2022-03-03 22:34:35 - train: epoch 0140, iter [04300, 05004], lr: 0.022263, loss: 1.6222
2022-03-03 22:35:09 - train: epoch 0140, iter [04400, 05004], lr: 0.022263, loss: 1.6356
2022-03-03 22:35:42 - train: epoch 0140, iter [04500, 05004], lr: 0.022263, loss: 1.5622
2022-03-03 22:36:15 - train: epoch 0140, iter [04600, 05004], lr: 0.022263, loss: 1.2771
2022-03-03 22:36:48 - train: epoch 0140, iter [04700, 05004], lr: 0.022263, loss: 1.4679
2022-03-03 22:37:22 - train: epoch 0140, iter [04800, 05004], lr: 0.022263, loss: 1.7512
2022-03-03 22:37:56 - train: epoch 0140, iter [04900, 05004], lr: 0.022263, loss: 1.4644
2022-03-03 22:38:27 - train: epoch 0140, iter [05000, 05004], lr: 0.022263, loss: 1.6244
2022-03-03 22:38:29 - train: epoch 140, train_loss: 1.4651
2022-03-03 22:39:43 - eval: epoch: 140, acc1: 67.718%, acc5: 88.476%, test_loss: 1.3059, per_image_load_time: 2.369ms, per_image_inference_time: 0.518ms
2022-03-03 22:39:43 - until epoch: 140, best_acc1: 68.246%
2022-03-03 22:39:43 - epoch 141 lr: 0.021596762663442216
2022-03-03 22:40:21 - train: epoch 0141, iter [00100, 05004], lr: 0.021597, loss: 1.4383
2022-03-03 22:40:55 - train: epoch 0141, iter [00200, 05004], lr: 0.021597, loss: 1.3405
2022-03-03 22:41:27 - train: epoch 0141, iter [00300, 05004], lr: 0.021597, loss: 1.5505
2022-03-03 22:42:01 - train: epoch 0141, iter [00400, 05004], lr: 0.021597, loss: 1.5083
2022-03-03 22:42:33 - train: epoch 0141, iter [00500, 05004], lr: 0.021597, loss: 1.3224
2022-03-03 22:43:07 - train: epoch 0141, iter [00600, 05004], lr: 0.021597, loss: 1.5939
2022-03-03 22:43:40 - train: epoch 0141, iter [00700, 05004], lr: 0.021597, loss: 1.5874
2022-03-03 22:44:14 - train: epoch 0141, iter [00800, 05004], lr: 0.021597, loss: 1.4430
2022-03-03 22:44:47 - train: epoch 0141, iter [00900, 05004], lr: 0.021597, loss: 1.4215
2022-03-03 22:45:20 - train: epoch 0141, iter [01000, 05004], lr: 0.021597, loss: 1.3686
2022-03-03 22:45:52 - train: epoch 0141, iter [01100, 05004], lr: 0.021597, loss: 1.4915
2022-03-03 22:46:27 - train: epoch 0141, iter [01200, 05004], lr: 0.021597, loss: 1.6711
2022-03-03 22:47:00 - train: epoch 0141, iter [01300, 05004], lr: 0.021597, loss: 1.6422
2022-03-03 22:47:33 - train: epoch 0141, iter [01400, 05004], lr: 0.021597, loss: 1.3690
2022-03-03 22:48:06 - train: epoch 0141, iter [01500, 05004], lr: 0.021597, loss: 1.4448
2022-03-03 22:48:40 - train: epoch 0141, iter [01600, 05004], lr: 0.021597, loss: 1.3223
2022-03-03 22:49:12 - train: epoch 0141, iter [01700, 05004], lr: 0.021597, loss: 1.2849
2022-03-03 22:49:46 - train: epoch 0141, iter [01800, 05004], lr: 0.021597, loss: 1.4779
2022-03-03 22:50:18 - train: epoch 0141, iter [01900, 05004], lr: 0.021597, loss: 1.3694
2022-03-03 22:50:51 - train: epoch 0141, iter [02000, 05004], lr: 0.021597, loss: 1.4395
2022-03-03 22:51:24 - train: epoch 0141, iter [02100, 05004], lr: 0.021597, loss: 1.2755
2022-03-03 22:51:56 - train: epoch 0141, iter [02200, 05004], lr: 0.021597, loss: 1.4866
2022-03-03 22:52:30 - train: epoch 0141, iter [02300, 05004], lr: 0.021597, loss: 1.4098
2022-03-03 22:53:03 - train: epoch 0141, iter [02400, 05004], lr: 0.021597, loss: 1.6212
2022-03-03 22:53:35 - train: epoch 0141, iter [02500, 05004], lr: 0.021597, loss: 1.7659
2022-03-03 22:54:08 - train: epoch 0141, iter [02600, 05004], lr: 0.021597, loss: 1.6965
2022-03-03 22:54:41 - train: epoch 0141, iter [02700, 05004], lr: 0.021597, loss: 1.4014
2022-03-03 22:55:14 - train: epoch 0141, iter [02800, 05004], lr: 0.021597, loss: 1.4258
2022-03-03 22:55:46 - train: epoch 0141, iter [02900, 05004], lr: 0.021597, loss: 1.4573
2022-03-03 22:56:20 - train: epoch 0141, iter [03000, 05004], lr: 0.021597, loss: 1.3311
2022-03-03 22:56:53 - train: epoch 0141, iter [03100, 05004], lr: 0.021597, loss: 1.6142
2022-03-03 22:57:26 - train: epoch 0141, iter [03200, 05004], lr: 0.021597, loss: 1.4766
2022-03-03 22:58:00 - train: epoch 0141, iter [03300, 05004], lr: 0.021597, loss: 1.6069
2022-03-03 22:58:33 - train: epoch 0141, iter [03400, 05004], lr: 0.021597, loss: 1.5935
2022-03-03 22:59:06 - train: epoch 0141, iter [03500, 05004], lr: 0.021597, loss: 1.6368
2022-03-03 22:59:39 - train: epoch 0141, iter [03600, 05004], lr: 0.021597, loss: 1.2675
2022-03-03 23:00:12 - train: epoch 0141, iter [03700, 05004], lr: 0.021597, loss: 1.5117
2022-03-03 23:00:46 - train: epoch 0141, iter [03800, 05004], lr: 0.021597, loss: 1.3983
2022-03-03 23:01:20 - train: epoch 0141, iter [03900, 05004], lr: 0.021597, loss: 1.4618
2022-03-03 23:01:53 - train: epoch 0141, iter [04000, 05004], lr: 0.021597, loss: 1.5456
2022-03-03 23:02:26 - train: epoch 0141, iter [04100, 05004], lr: 0.021597, loss: 1.3874
2022-03-03 23:03:00 - train: epoch 0141, iter [04200, 05004], lr: 0.021597, loss: 1.5530
2022-03-03 23:03:33 - train: epoch 0141, iter [04300, 05004], lr: 0.021597, loss: 1.6954
2022-03-03 23:04:06 - train: epoch 0141, iter [04400, 05004], lr: 0.021597, loss: 1.6664
2022-03-03 23:04:40 - train: epoch 0141, iter [04500, 05004], lr: 0.021597, loss: 1.5108
2022-03-03 23:05:13 - train: epoch 0141, iter [04600, 05004], lr: 0.021597, loss: 1.5719
2022-03-03 23:05:46 - train: epoch 0141, iter [04700, 05004], lr: 0.021597, loss: 1.6725
2022-03-03 23:06:20 - train: epoch 0141, iter [04800, 05004], lr: 0.021597, loss: 1.5906
2022-03-03 23:06:53 - train: epoch 0141, iter [04900, 05004], lr: 0.021597, loss: 1.5259
2022-03-03 23:07:25 - train: epoch 0141, iter [05000, 05004], lr: 0.021597, loss: 1.5658
2022-03-03 23:07:27 - train: epoch 141, train_loss: 1.4541
2022-03-03 23:08:40 - eval: epoch: 141, acc1: 67.804%, acc5: 88.668%, test_loss: 1.2962, per_image_load_time: 2.015ms, per_image_inference_time: 0.533ms
2022-03-03 23:08:41 - until epoch: 141, best_acc1: 68.246%
2022-03-03 23:08:41 - epoch 142 lr: 0.020937533765518185
2022-03-03 23:09:18 - train: epoch 0142, iter [00100, 05004], lr: 0.020938, loss: 1.5613
2022-03-03 23:09:52 - train: epoch 0142, iter [00200, 05004], lr: 0.020938, loss: 1.3961
2022-03-03 23:10:26 - train: epoch 0142, iter [00300, 05004], lr: 0.020938, loss: 1.3393
2022-03-03 23:11:00 - train: epoch 0142, iter [00400, 05004], lr: 0.020938, loss: 1.5105
2022-03-03 23:11:33 - train: epoch 0142, iter [00500, 05004], lr: 0.020938, loss: 1.3236
2022-03-03 23:12:06 - train: epoch 0142, iter [00600, 05004], lr: 0.020938, loss: 1.6295
2022-03-03 23:12:39 - train: epoch 0142, iter [00700, 05004], lr: 0.020938, loss: 1.4135
2022-03-03 23:13:12 - train: epoch 0142, iter [00800, 05004], lr: 0.020938, loss: 1.6093
2022-03-03 23:13:45 - train: epoch 0142, iter [00900, 05004], lr: 0.020938, loss: 1.2525
2022-03-03 23:14:19 - train: epoch 0142, iter [01000, 05004], lr: 0.020938, loss: 1.3724
2022-03-03 23:14:53 - train: epoch 0142, iter [01100, 05004], lr: 0.020938, loss: 1.2991
2022-03-03 23:15:26 - train: epoch 0142, iter [01200, 05004], lr: 0.020938, loss: 1.5195
2022-03-03 23:15:59 - train: epoch 0142, iter [01300, 05004], lr: 0.020938, loss: 1.4561
2022-03-03 23:16:33 - train: epoch 0142, iter [01400, 05004], lr: 0.020938, loss: 1.6186
2022-03-03 23:17:06 - train: epoch 0142, iter [01500, 05004], lr: 0.020938, loss: 1.6140
2022-03-03 23:17:40 - train: epoch 0142, iter [01600, 05004], lr: 0.020938, loss: 1.4584
2022-03-03 23:18:13 - train: epoch 0142, iter [01700, 05004], lr: 0.020938, loss: 1.3957
2022-03-03 23:18:47 - train: epoch 0142, iter [01800, 05004], lr: 0.020938, loss: 1.4894
2022-03-03 23:19:20 - train: epoch 0142, iter [01900, 05004], lr: 0.020938, loss: 1.3998
2022-03-03 23:19:54 - train: epoch 0142, iter [02000, 05004], lr: 0.020938, loss: 1.5601
2022-03-03 23:20:27 - train: epoch 0142, iter [02100, 05004], lr: 0.020938, loss: 1.3643
2022-03-03 23:21:01 - train: epoch 0142, iter [02200, 05004], lr: 0.020938, loss: 1.4737
2022-03-03 23:21:34 - train: epoch 0142, iter [02300, 05004], lr: 0.020938, loss: 1.4181
2022-03-03 23:22:07 - train: epoch 0142, iter [02400, 05004], lr: 0.020938, loss: 1.3314
2022-03-03 23:22:41 - train: epoch 0142, iter [02500, 05004], lr: 0.020938, loss: 1.4795
2022-03-03 23:23:15 - train: epoch 0142, iter [02600, 05004], lr: 0.020938, loss: 1.4106
2022-03-03 23:23:47 - train: epoch 0142, iter [02700, 05004], lr: 0.020938, loss: 1.4638
2022-03-03 23:24:20 - train: epoch 0142, iter [02800, 05004], lr: 0.020938, loss: 1.1525
2022-03-03 23:24:54 - train: epoch 0142, iter [02900, 05004], lr: 0.020938, loss: 1.2640
2022-03-03 23:25:27 - train: epoch 0142, iter [03000, 05004], lr: 0.020938, loss: 1.5646
2022-03-03 23:26:00 - train: epoch 0142, iter [03100, 05004], lr: 0.020938, loss: 1.6473
2022-03-03 23:26:33 - train: epoch 0142, iter [03200, 05004], lr: 0.020938, loss: 1.6657
2022-03-03 23:27:07 - train: epoch 0142, iter [03300, 05004], lr: 0.020938, loss: 1.3821
2022-03-03 23:27:40 - train: epoch 0142, iter [03400, 05004], lr: 0.020938, loss: 1.2624
2022-03-03 23:28:14 - train: epoch 0142, iter [03500, 05004], lr: 0.020938, loss: 1.5305
2022-03-03 23:28:47 - train: epoch 0142, iter [03600, 05004], lr: 0.020938, loss: 1.2243
2022-03-03 23:29:19 - train: epoch 0142, iter [03700, 05004], lr: 0.020938, loss: 1.5814
2022-03-03 23:29:52 - train: epoch 0142, iter [03800, 05004], lr: 0.020938, loss: 1.5355
2022-03-03 23:30:26 - train: epoch 0142, iter [03900, 05004], lr: 0.020938, loss: 1.3748
2022-03-03 23:31:00 - train: epoch 0142, iter [04000, 05004], lr: 0.020938, loss: 1.2791
2022-03-03 23:31:34 - train: epoch 0142, iter [04100, 05004], lr: 0.020938, loss: 1.4969
2022-03-03 23:32:06 - train: epoch 0142, iter [04200, 05004], lr: 0.020938, loss: 1.4104
2022-03-03 23:32:41 - train: epoch 0142, iter [04300, 05004], lr: 0.020938, loss: 1.3614
2022-03-03 23:33:14 - train: epoch 0142, iter [04400, 05004], lr: 0.020938, loss: 1.3139
2022-03-03 23:33:47 - train: epoch 0142, iter [04500, 05004], lr: 0.020938, loss: 1.3940
2022-03-03 23:34:21 - train: epoch 0142, iter [04600, 05004], lr: 0.020938, loss: 1.6703
2022-03-03 23:34:54 - train: epoch 0142, iter [04700, 05004], lr: 0.020938, loss: 1.7347
2022-03-03 23:35:28 - train: epoch 0142, iter [04800, 05004], lr: 0.020938, loss: 1.2320
2022-03-03 23:36:01 - train: epoch 0142, iter [04900, 05004], lr: 0.020938, loss: 1.5351
2022-03-03 23:36:33 - train: epoch 0142, iter [05000, 05004], lr: 0.020938, loss: 1.3797
2022-03-03 23:36:34 - train: epoch 142, train_loss: 1.4426
2022-03-03 23:37:47 - eval: epoch: 142, acc1: 68.430%, acc5: 88.764%, test_loss: 1.2866, per_image_load_time: 1.945ms, per_image_inference_time: 0.531ms
2022-03-03 23:37:48 - until epoch: 142, best_acc1: 68.430%
2022-03-03 23:37:48 - epoch 143 lr: 0.020285848032369137
2022-03-03 23:38:26 - train: epoch 0143, iter [00100, 05004], lr: 0.020286, loss: 1.4171
2022-03-03 23:38:59 - train: epoch 0143, iter [00200, 05004], lr: 0.020286, loss: 1.3214
2022-03-03 23:39:32 - train: epoch 0143, iter [00300, 05004], lr: 0.020286, loss: 1.2449
2022-03-03 23:40:06 - train: epoch 0143, iter [00400, 05004], lr: 0.020286, loss: 1.4549
2022-03-03 23:40:39 - train: epoch 0143, iter [00500, 05004], lr: 0.020286, loss: 1.5542
2022-03-03 23:41:11 - train: epoch 0143, iter [00600, 05004], lr: 0.020286, loss: 1.4243
2022-03-03 23:41:45 - train: epoch 0143, iter [00700, 05004], lr: 0.020286, loss: 1.3831
2022-03-03 23:42:18 - train: epoch 0143, iter [00800, 05004], lr: 0.020286, loss: 1.4373
2022-03-03 23:42:51 - train: epoch 0143, iter [00900, 05004], lr: 0.020286, loss: 1.4753
2022-03-03 23:43:25 - train: epoch 0143, iter [01000, 05004], lr: 0.020286, loss: 1.5102
2022-03-03 23:43:59 - train: epoch 0143, iter [01100, 05004], lr: 0.020286, loss: 1.3096
2022-03-03 23:44:32 - train: epoch 0143, iter [01200, 05004], lr: 0.020286, loss: 1.3553
2022-03-03 23:45:05 - train: epoch 0143, iter [01300, 05004], lr: 0.020286, loss: 1.3530
2022-03-03 23:45:38 - train: epoch 0143, iter [01400, 05004], lr: 0.020286, loss: 1.4076
2022-03-03 23:46:10 - train: epoch 0143, iter [01500, 05004], lr: 0.020286, loss: 1.4750
2022-03-03 23:46:44 - train: epoch 0143, iter [01600, 05004], lr: 0.020286, loss: 1.4340
2022-03-03 23:47:17 - train: epoch 0143, iter [01700, 05004], lr: 0.020286, loss: 1.4084
2022-03-03 23:47:51 - train: epoch 0143, iter [01800, 05004], lr: 0.020286, loss: 1.3784
2022-03-03 23:48:24 - train: epoch 0143, iter [01900, 05004], lr: 0.020286, loss: 1.2949
2022-03-03 23:48:57 - train: epoch 0143, iter [02000, 05004], lr: 0.020286, loss: 1.3107
2022-03-03 23:49:30 - train: epoch 0143, iter [02100, 05004], lr: 0.020286, loss: 1.4497
2022-03-03 23:50:04 - train: epoch 0143, iter [02200, 05004], lr: 0.020286, loss: 1.5864
2022-03-03 23:50:37 - train: epoch 0143, iter [02300, 05004], lr: 0.020286, loss: 1.6444
2022-03-03 23:51:11 - train: epoch 0143, iter [02400, 05004], lr: 0.020286, loss: 1.3958
2022-03-03 23:51:43 - train: epoch 0143, iter [02500, 05004], lr: 0.020286, loss: 1.4809
2022-03-03 23:52:17 - train: epoch 0143, iter [02600, 05004], lr: 0.020286, loss: 1.3456
2022-03-03 23:52:50 - train: epoch 0143, iter [02700, 05004], lr: 0.020286, loss: 1.3679
2022-03-03 23:53:23 - train: epoch 0143, iter [02800, 05004], lr: 0.020286, loss: 1.6130
2022-03-03 23:53:56 - train: epoch 0143, iter [02900, 05004], lr: 0.020286, loss: 1.4794
2022-03-03 23:54:29 - train: epoch 0143, iter [03000, 05004], lr: 0.020286, loss: 1.5544
2022-03-03 23:55:03 - train: epoch 0143, iter [03100, 05004], lr: 0.020286, loss: 1.2139
2022-03-03 23:55:36 - train: epoch 0143, iter [03200, 05004], lr: 0.020286, loss: 1.6760
2022-03-03 23:56:09 - train: epoch 0143, iter [03300, 05004], lr: 0.020286, loss: 1.3316
2022-03-03 23:56:41 - train: epoch 0143, iter [03400, 05004], lr: 0.020286, loss: 1.3270
2022-03-03 23:57:15 - train: epoch 0143, iter [03500, 05004], lr: 0.020286, loss: 1.5046
2022-03-03 23:57:48 - train: epoch 0143, iter [03600, 05004], lr: 0.020286, loss: 1.7056
2022-03-03 23:58:22 - train: epoch 0143, iter [03700, 05004], lr: 0.020286, loss: 1.5180
2022-03-03 23:58:55 - train: epoch 0143, iter [03800, 05004], lr: 0.020286, loss: 1.4953
2022-03-03 23:59:28 - train: epoch 0143, iter [03900, 05004], lr: 0.020286, loss: 1.1988
2022-03-04 00:00:01 - train: epoch 0143, iter [04000, 05004], lr: 0.020286, loss: 1.5612
2022-03-04 00:00:34 - train: epoch 0143, iter [04100, 05004], lr: 0.020286, loss: 1.4352
2022-03-04 00:01:07 - train: epoch 0143, iter [04200, 05004], lr: 0.020286, loss: 1.2228
2022-03-04 00:01:41 - train: epoch 0143, iter [04300, 05004], lr: 0.020286, loss: 1.6285
2022-03-04 00:02:13 - train: epoch 0143, iter [04400, 05004], lr: 0.020286, loss: 1.6164
2022-03-04 00:02:46 - train: epoch 0143, iter [04500, 05004], lr: 0.020286, loss: 1.4316
2022-03-04 00:03:19 - train: epoch 0143, iter [04600, 05004], lr: 0.020286, loss: 1.8191
2022-03-04 00:03:52 - train: epoch 0143, iter [04700, 05004], lr: 0.020286, loss: 1.4125
2022-03-04 00:04:26 - train: epoch 0143, iter [04800, 05004], lr: 0.020286, loss: 1.3144
2022-03-04 00:04:59 - train: epoch 0143, iter [04900, 05004], lr: 0.020286, loss: 1.2118
2022-03-04 00:05:31 - train: epoch 0143, iter [05000, 05004], lr: 0.020286, loss: 1.5130
2022-03-04 00:05:32 - train: epoch 143, train_loss: 1.4353
2022-03-04 00:06:45 - eval: epoch: 143, acc1: 68.900%, acc5: 89.136%, test_loss: 1.2466, per_image_load_time: 1.540ms, per_image_inference_time: 0.519ms
2022-03-04 00:06:46 - until epoch: 143, best_acc1: 68.900%
2022-03-04 00:06:46 - epoch 144 lr: 0.01964187460906444
2022-03-04 00:07:24 - train: epoch 0144, iter [00100, 05004], lr: 0.019642, loss: 1.2523
2022-03-04 00:07:57 - train: epoch 0144, iter [00200, 05004], lr: 0.019642, loss: 1.4219
2022-03-04 00:08:29 - train: epoch 0144, iter [00300, 05004], lr: 0.019642, loss: 1.4068
2022-03-04 00:09:02 - train: epoch 0144, iter [00400, 05004], lr: 0.019642, loss: 1.4568
2022-03-04 00:09:36 - train: epoch 0144, iter [00500, 05004], lr: 0.019642, loss: 1.5085
2022-03-04 00:10:09 - train: epoch 0144, iter [00600, 05004], lr: 0.019642, loss: 1.3569
2022-03-04 00:10:42 - train: epoch 0144, iter [00700, 05004], lr: 0.019642, loss: 1.3410
2022-03-04 00:11:15 - train: epoch 0144, iter [00800, 05004], lr: 0.019642, loss: 1.8266
2022-03-04 00:11:49 - train: epoch 0144, iter [00900, 05004], lr: 0.019642, loss: 1.3455
2022-03-04 00:12:22 - train: epoch 0144, iter [01000, 05004], lr: 0.019642, loss: 1.3774
2022-03-04 00:12:55 - train: epoch 0144, iter [01100, 05004], lr: 0.019642, loss: 1.5704
2022-03-04 00:13:28 - train: epoch 0144, iter [01200, 05004], lr: 0.019642, loss: 1.4076
2022-03-04 00:14:01 - train: epoch 0144, iter [01300, 05004], lr: 0.019642, loss: 1.5208
2022-03-04 00:14:35 - train: epoch 0144, iter [01400, 05004], lr: 0.019642, loss: 1.5247
2022-03-04 00:15:08 - train: epoch 0144, iter [01500, 05004], lr: 0.019642, loss: 1.4216
2022-03-04 00:15:41 - train: epoch 0144, iter [01600, 05004], lr: 0.019642, loss: 1.1872
2022-03-04 00:16:14 - train: epoch 0144, iter [01700, 05004], lr: 0.019642, loss: 1.2122
2022-03-04 00:16:48 - train: epoch 0144, iter [01800, 05004], lr: 0.019642, loss: 1.3543
2022-03-04 00:17:22 - train: epoch 0144, iter [01900, 05004], lr: 0.019642, loss: 1.2051
2022-03-04 00:17:56 - train: epoch 0144, iter [02000, 05004], lr: 0.019642, loss: 1.4361
2022-03-04 00:18:28 - train: epoch 0144, iter [02100, 05004], lr: 0.019642, loss: 1.4474
2022-03-04 00:19:02 - train: epoch 0144, iter [02200, 05004], lr: 0.019642, loss: 1.3256
2022-03-04 00:19:35 - train: epoch 0144, iter [02300, 05004], lr: 0.019642, loss: 1.1735
2022-03-04 00:20:09 - train: epoch 0144, iter [02400, 05004], lr: 0.019642, loss: 1.5780
2022-03-04 00:20:42 - train: epoch 0144, iter [02500, 05004], lr: 0.019642, loss: 1.2585
2022-03-04 00:21:15 - train: epoch 0144, iter [02600, 05004], lr: 0.019642, loss: 1.2898
2022-03-04 00:21:48 - train: epoch 0144, iter [02700, 05004], lr: 0.019642, loss: 1.2293
2022-03-04 00:22:21 - train: epoch 0144, iter [02800, 05004], lr: 0.019642, loss: 1.3702
2022-03-04 00:22:55 - train: epoch 0144, iter [02900, 05004], lr: 0.019642, loss: 1.2674
2022-03-04 00:23:29 - train: epoch 0144, iter [03000, 05004], lr: 0.019642, loss: 1.5656
2022-03-04 00:24:02 - train: epoch 0144, iter [03100, 05004], lr: 0.019642, loss: 1.3462
2022-03-04 00:24:35 - train: epoch 0144, iter [03200, 05004], lr: 0.019642, loss: 1.5205
2022-03-04 00:25:08 - train: epoch 0144, iter [03300, 05004], lr: 0.019642, loss: 1.4870
2022-03-04 00:25:41 - train: epoch 0144, iter [03400, 05004], lr: 0.019642, loss: 1.5443
2022-03-04 00:26:13 - train: epoch 0144, iter [03500, 05004], lr: 0.019642, loss: 1.5265
2022-03-04 00:26:47 - train: epoch 0144, iter [03600, 05004], lr: 0.019642, loss: 1.5420
2022-03-04 00:27:20 - train: epoch 0144, iter [03700, 05004], lr: 0.019642, loss: 1.3878
2022-03-04 00:27:54 - train: epoch 0144, iter [03800, 05004], lr: 0.019642, loss: 1.3206
2022-03-04 00:28:27 - train: epoch 0144, iter [03900, 05004], lr: 0.019642, loss: 1.1482
2022-03-04 00:29:01 - train: epoch 0144, iter [04000, 05004], lr: 0.019642, loss: 1.3625
2022-03-04 00:29:34 - train: epoch 0144, iter [04100, 05004], lr: 0.019642, loss: 1.6198
2022-03-04 00:30:07 - train: epoch 0144, iter [04200, 05004], lr: 0.019642, loss: 1.5100
2022-03-04 00:30:40 - train: epoch 0144, iter [04300, 05004], lr: 0.019642, loss: 1.4473
2022-03-04 00:31:14 - train: epoch 0144, iter [04400, 05004], lr: 0.019642, loss: 1.4338
2022-03-04 00:31:48 - train: epoch 0144, iter [04500, 05004], lr: 0.019642, loss: 1.3070
2022-03-04 00:32:20 - train: epoch 0144, iter [04600, 05004], lr: 0.019642, loss: 1.3456
2022-03-04 00:32:54 - train: epoch 0144, iter [04700, 05004], lr: 0.019642, loss: 1.3545
2022-03-04 00:33:28 - train: epoch 0144, iter [04800, 05004], lr: 0.019642, loss: 1.4485
2022-03-04 00:34:01 - train: epoch 0144, iter [04900, 05004], lr: 0.019642, loss: 1.6013
2022-03-04 00:34:33 - train: epoch 0144, iter [05000, 05004], lr: 0.019642, loss: 1.5560
2022-03-04 00:34:34 - train: epoch 144, train_loss: 1.4237
2022-03-04 00:35:47 - eval: epoch: 144, acc1: 68.876%, acc5: 89.270%, test_loss: 1.2522, per_image_load_time: 1.057ms, per_image_inference_time: 0.525ms
2022-03-04 00:35:48 - until epoch: 144, best_acc1: 68.900%
2022-03-04 00:35:48 - epoch 145 lr: 0.019005780638942982
2022-03-04 00:36:25 - train: epoch 0145, iter [00100, 05004], lr: 0.019006, loss: 1.0831
2022-03-04 00:36:59 - train: epoch 0145, iter [00200, 05004], lr: 0.019006, loss: 1.2898
2022-03-04 00:37:32 - train: epoch 0145, iter [00300, 05004], lr: 0.019006, loss: 1.3292
2022-03-04 00:38:04 - train: epoch 0145, iter [00400, 05004], lr: 0.019006, loss: 1.3022
2022-03-04 00:38:38 - train: epoch 0145, iter [00500, 05004], lr: 0.019006, loss: 1.6368
2022-03-04 00:39:12 - train: epoch 0145, iter [00600, 05004], lr: 0.019006, loss: 1.5299
2022-03-04 00:39:44 - train: epoch 0145, iter [00700, 05004], lr: 0.019006, loss: 1.5002
2022-03-04 00:40:18 - train: epoch 0145, iter [00800, 05004], lr: 0.019006, loss: 1.4193
2022-03-04 00:40:51 - train: epoch 0145, iter [00900, 05004], lr: 0.019006, loss: 1.3862
2022-03-04 00:41:25 - train: epoch 0145, iter [01000, 05004], lr: 0.019006, loss: 1.3978
2022-03-04 00:41:58 - train: epoch 0145, iter [01100, 05004], lr: 0.019006, loss: 1.3910
2022-03-04 00:42:31 - train: epoch 0145, iter [01200, 05004], lr: 0.019006, loss: 1.6677
2022-03-04 00:43:04 - train: epoch 0145, iter [01300, 05004], lr: 0.019006, loss: 1.5082
2022-03-04 00:43:37 - train: epoch 0145, iter [01400, 05004], lr: 0.019006, loss: 1.2887
2022-03-04 00:44:11 - train: epoch 0145, iter [01500, 05004], lr: 0.019006, loss: 1.6866
2022-03-04 00:44:43 - train: epoch 0145, iter [01600, 05004], lr: 0.019006, loss: 1.2887
2022-03-04 00:45:16 - train: epoch 0145, iter [01700, 05004], lr: 0.019006, loss: 1.7075
2022-03-04 00:45:50 - train: epoch 0145, iter [01800, 05004], lr: 0.019006, loss: 1.3827
2022-03-04 00:46:24 - train: epoch 0145, iter [01900, 05004], lr: 0.019006, loss: 1.3675
2022-03-04 00:46:58 - train: epoch 0145, iter [02000, 05004], lr: 0.019006, loss: 1.3907
2022-03-04 00:47:30 - train: epoch 0145, iter [02100, 05004], lr: 0.019006, loss: 1.5458
2022-03-04 00:48:04 - train: epoch 0145, iter [02200, 05004], lr: 0.019006, loss: 1.4189
2022-03-04 00:48:37 - train: epoch 0145, iter [02300, 05004], lr: 0.019006, loss: 1.4346
2022-03-04 00:49:10 - train: epoch 0145, iter [02400, 05004], lr: 0.019006, loss: 1.5558
2022-03-04 00:49:43 - train: epoch 0145, iter [02500, 05004], lr: 0.019006, loss: 1.2676
2022-03-04 00:50:16 - train: epoch 0145, iter [02600, 05004], lr: 0.019006, loss: 1.4479
2022-03-04 00:50:50 - train: epoch 0145, iter [02700, 05004], lr: 0.019006, loss: 1.4546
2022-03-04 00:51:22 - train: epoch 0145, iter [02800, 05004], lr: 0.019006, loss: 1.4925
2022-03-04 00:51:57 - train: epoch 0145, iter [02900, 05004], lr: 0.019006, loss: 1.6227
2022-03-04 00:52:30 - train: epoch 0145, iter [03000, 05004], lr: 0.019006, loss: 1.2440
2022-03-04 00:53:03 - train: epoch 0145, iter [03100, 05004], lr: 0.019006, loss: 1.6314
2022-03-04 00:53:36 - train: epoch 0145, iter [03200, 05004], lr: 0.019006, loss: 1.4794
2022-03-04 00:54:09 - train: epoch 0145, iter [03300, 05004], lr: 0.019006, loss: 1.6052
2022-03-04 00:54:43 - train: epoch 0145, iter [03400, 05004], lr: 0.019006, loss: 1.4349
2022-03-04 00:55:16 - train: epoch 0145, iter [03500, 05004], lr: 0.019006, loss: 1.1996
2022-03-04 00:55:49 - train: epoch 0145, iter [03600, 05004], lr: 0.019006, loss: 1.4578
2022-03-04 00:56:22 - train: epoch 0145, iter [03700, 05004], lr: 0.019006, loss: 1.2961
2022-03-04 00:56:55 - train: epoch 0145, iter [03800, 05004], lr: 0.019006, loss: 1.4185
2022-03-04 00:57:29 - train: epoch 0145, iter [03900, 05004], lr: 0.019006, loss: 1.4850
2022-03-04 00:58:02 - train: epoch 0145, iter [04000, 05004], lr: 0.019006, loss: 1.4227
2022-03-04 00:58:35 - train: epoch 0145, iter [04100, 05004], lr: 0.019006, loss: 1.5186
2022-03-04 00:59:08 - train: epoch 0145, iter [04200, 05004], lr: 0.019006, loss: 1.6502
2022-03-04 00:59:41 - train: epoch 0145, iter [04300, 05004], lr: 0.019006, loss: 1.3495
2022-03-04 01:00:15 - train: epoch 0145, iter [04400, 05004], lr: 0.019006, loss: 1.1908
2022-03-04 01:00:47 - train: epoch 0145, iter [04500, 05004], lr: 0.019006, loss: 1.3576
2022-03-04 01:01:21 - train: epoch 0145, iter [04600, 05004], lr: 0.019006, loss: 1.4693
2022-03-04 01:01:53 - train: epoch 0145, iter [04700, 05004], lr: 0.019006, loss: 1.4657
2022-03-04 01:02:26 - train: epoch 0145, iter [04800, 05004], lr: 0.019006, loss: 1.2790
2022-03-04 01:02:59 - train: epoch 0145, iter [04900, 05004], lr: 0.019006, loss: 1.6679
2022-03-04 01:03:31 - train: epoch 0145, iter [05000, 05004], lr: 0.019006, loss: 1.3702
2022-03-04 01:03:32 - train: epoch 145, train_loss: 1.4131
2022-03-04 01:04:46 - eval: epoch: 145, acc1: 68.708%, acc5: 89.256%, test_loss: 1.2579, per_image_load_time: 2.355ms, per_image_inference_time: 0.507ms
2022-03-04 01:04:47 - until epoch: 145, best_acc1: 68.900%
2022-03-04 01:04:47 - epoch 146 lr: 0.018377731220231144
2022-03-04 01:05:25 - train: epoch 0146, iter [00100, 05004], lr: 0.018378, loss: 1.2580
2022-03-04 01:05:58 - train: epoch 0146, iter [00200, 05004], lr: 0.018378, loss: 1.3896
2022-03-04 01:06:31 - train: epoch 0146, iter [00300, 05004], lr: 0.018378, loss: 1.4011
2022-03-04 01:07:04 - train: epoch 0146, iter [00400, 05004], lr: 0.018378, loss: 1.4289
2022-03-04 01:07:38 - train: epoch 0146, iter [00500, 05004], lr: 0.018378, loss: 1.2107
2022-03-04 01:08:10 - train: epoch 0146, iter [00600, 05004], lr: 0.018378, loss: 1.2415
2022-03-04 01:08:43 - train: epoch 0146, iter [00700, 05004], lr: 0.018378, loss: 1.5405
2022-03-04 01:09:16 - train: epoch 0146, iter [00800, 05004], lr: 0.018378, loss: 1.2403
2022-03-04 01:09:49 - train: epoch 0146, iter [00900, 05004], lr: 0.018378, loss: 1.3075
2022-03-04 01:10:23 - train: epoch 0146, iter [01000, 05004], lr: 0.018378, loss: 1.4322
2022-03-04 01:10:56 - train: epoch 0146, iter [01100, 05004], lr: 0.018378, loss: 1.2019
2022-03-04 01:11:29 - train: epoch 0146, iter [01200, 05004], lr: 0.018378, loss: 1.2383
2022-03-04 01:12:02 - train: epoch 0146, iter [01300, 05004], lr: 0.018378, loss: 1.1875
2022-03-04 01:12:36 - train: epoch 0146, iter [01400, 05004], lr: 0.018378, loss: 1.6378
2022-03-04 01:13:09 - train: epoch 0146, iter [01500, 05004], lr: 0.018378, loss: 1.3858
2022-03-04 01:13:42 - train: epoch 0146, iter [01600, 05004], lr: 0.018378, loss: 1.3975
2022-03-04 01:14:15 - train: epoch 0146, iter [01700, 05004], lr: 0.018378, loss: 1.4541
2022-03-04 01:14:48 - train: epoch 0146, iter [01800, 05004], lr: 0.018378, loss: 1.3346
2022-03-04 01:15:22 - train: epoch 0146, iter [01900, 05004], lr: 0.018378, loss: 1.2522
2022-03-04 01:15:55 - train: epoch 0146, iter [02000, 05004], lr: 0.018378, loss: 1.3898
2022-03-04 01:16:29 - train: epoch 0146, iter [02100, 05004], lr: 0.018378, loss: 1.2699
2022-03-04 01:17:02 - train: epoch 0146, iter [02200, 05004], lr: 0.018378, loss: 1.3357
2022-03-04 01:17:35 - train: epoch 0146, iter [02300, 05004], lr: 0.018378, loss: 1.3102
2022-03-04 01:18:09 - train: epoch 0146, iter [02400, 05004], lr: 0.018378, loss: 1.5513
2022-03-04 01:18:41 - train: epoch 0146, iter [02500, 05004], lr: 0.018378, loss: 1.5618
2022-03-04 01:19:16 - train: epoch 0146, iter [02600, 05004], lr: 0.018378, loss: 1.4886
2022-03-04 01:19:49 - train: epoch 0146, iter [02700, 05004], lr: 0.018378, loss: 1.4553
2022-03-04 01:20:21 - train: epoch 0146, iter [02800, 05004], lr: 0.018378, loss: 1.5167
2022-03-04 01:20:55 - train: epoch 0146, iter [02900, 05004], lr: 0.018378, loss: 1.3434
2022-03-04 01:21:29 - train: epoch 0146, iter [03000, 05004], lr: 0.018378, loss: 1.4637
2022-03-04 01:22:01 - train: epoch 0146, iter [03100, 05004], lr: 0.018378, loss: 1.4678
2022-03-04 01:22:34 - train: epoch 0146, iter [03200, 05004], lr: 0.018378, loss: 1.3159
2022-03-04 01:23:08 - train: epoch 0146, iter [03300, 05004], lr: 0.018378, loss: 1.4838
2022-03-04 01:23:41 - train: epoch 0146, iter [03400, 05004], lr: 0.018378, loss: 1.5627
2022-03-04 01:24:14 - train: epoch 0146, iter [03500, 05004], lr: 0.018378, loss: 1.2965
2022-03-04 01:24:47 - train: epoch 0146, iter [03600, 05004], lr: 0.018378, loss: 1.3349
2022-03-04 01:25:20 - train: epoch 0146, iter [03700, 05004], lr: 0.018378, loss: 1.4361
2022-03-04 01:25:53 - train: epoch 0146, iter [03800, 05004], lr: 0.018378, loss: 1.4604
2022-03-04 01:26:25 - train: epoch 0146, iter [03900, 05004], lr: 0.018378, loss: 1.5002
2022-03-04 01:26:59 - train: epoch 0146, iter [04000, 05004], lr: 0.018378, loss: 1.4423
2022-03-04 01:27:33 - train: epoch 0146, iter [04100, 05004], lr: 0.018378, loss: 1.2233
2022-03-04 01:28:06 - train: epoch 0146, iter [04200, 05004], lr: 0.018378, loss: 1.2987
2022-03-04 01:28:39 - train: epoch 0146, iter [04300, 05004], lr: 0.018378, loss: 1.6112
2022-03-04 01:29:12 - train: epoch 0146, iter [04400, 05004], lr: 0.018378, loss: 1.5760
2022-03-04 01:29:45 - train: epoch 0146, iter [04500, 05004], lr: 0.018378, loss: 1.2644
2022-03-04 01:30:18 - train: epoch 0146, iter [04600, 05004], lr: 0.018378, loss: 1.3923
2022-03-04 01:30:52 - train: epoch 0146, iter [04700, 05004], lr: 0.018378, loss: 1.3007
2022-03-04 01:31:25 - train: epoch 0146, iter [04800, 05004], lr: 0.018378, loss: 1.5455
2022-03-04 01:31:58 - train: epoch 0146, iter [04900, 05004], lr: 0.018378, loss: 1.7047
2022-03-04 01:32:29 - train: epoch 0146, iter [05000, 05004], lr: 0.018378, loss: 1.4328
2022-03-04 01:32:30 - train: epoch 146, train_loss: 1.3989
2022-03-04 01:33:43 - eval: epoch: 146, acc1: 69.904%, acc5: 89.796%, test_loss: 1.2151, per_image_load_time: 2.245ms, per_image_inference_time: 0.506ms
2022-03-04 01:33:44 - until epoch: 146, best_acc1: 69.904%
2022-03-04 01:33:44 - epoch 147 lr: 0.017757889363191483
2022-03-04 01:34:22 - train: epoch 0147, iter [00100, 05004], lr: 0.017758, loss: 1.4257
2022-03-04 01:34:54 - train: epoch 0147, iter [00200, 05004], lr: 0.017758, loss: 1.4197
2022-03-04 01:35:28 - train: epoch 0147, iter [00300, 05004], lr: 0.017758, loss: 1.4010
2022-03-04 01:36:01 - train: epoch 0147, iter [00400, 05004], lr: 0.017758, loss: 1.2266
2022-03-04 01:36:33 - train: epoch 0147, iter [00500, 05004], lr: 0.017758, loss: 1.4740
2022-03-04 01:37:07 - train: epoch 0147, iter [00600, 05004], lr: 0.017758, loss: 1.4360
2022-03-04 01:37:40 - train: epoch 0147, iter [00700, 05004], lr: 0.017758, loss: 1.1396
2022-03-04 01:38:13 - train: epoch 0147, iter [00800, 05004], lr: 0.017758, loss: 1.4271
2022-03-04 01:38:47 - train: epoch 0147, iter [00900, 05004], lr: 0.017758, loss: 1.3852
2022-03-04 01:39:20 - train: epoch 0147, iter [01000, 05004], lr: 0.017758, loss: 1.4923
2022-03-04 01:39:53 - train: epoch 0147, iter [01100, 05004], lr: 0.017758, loss: 1.3682
2022-03-04 01:40:26 - train: epoch 0147, iter [01200, 05004], lr: 0.017758, loss: 1.3344
2022-03-04 01:41:00 - train: epoch 0147, iter [01300, 05004], lr: 0.017758, loss: 1.6861
2022-03-04 01:41:33 - train: epoch 0147, iter [01400, 05004], lr: 0.017758, loss: 1.4716
2022-03-04 01:42:06 - train: epoch 0147, iter [01500, 05004], lr: 0.017758, loss: 1.4057
2022-03-04 01:42:40 - train: epoch 0147, iter [01600, 05004], lr: 0.017758, loss: 1.5524
2022-03-04 01:43:13 - train: epoch 0147, iter [01700, 05004], lr: 0.017758, loss: 1.5528
2022-03-04 01:43:46 - train: epoch 0147, iter [01800, 05004], lr: 0.017758, loss: 1.3074
2022-03-04 01:44:19 - train: epoch 0147, iter [01900, 05004], lr: 0.017758, loss: 1.4413
2022-03-04 01:44:52 - train: epoch 0147, iter [02000, 05004], lr: 0.017758, loss: 1.3427
2022-03-04 01:45:26 - train: epoch 0147, iter [02100, 05004], lr: 0.017758, loss: 1.3597
2022-03-04 01:45:59 - train: epoch 0147, iter [02200, 05004], lr: 0.017758, loss: 1.5297
2022-03-04 01:46:32 - train: epoch 0147, iter [02300, 05004], lr: 0.017758, loss: 1.4569
2022-03-04 01:47:06 - train: epoch 0147, iter [02400, 05004], lr: 0.017758, loss: 1.2445
2022-03-04 01:47:39 - train: epoch 0147, iter [02500, 05004], lr: 0.017758, loss: 1.5561
2022-03-04 01:48:12 - train: epoch 0147, iter [02600, 05004], lr: 0.017758, loss: 1.3665
2022-03-04 01:48:45 - train: epoch 0147, iter [02700, 05004], lr: 0.017758, loss: 1.4130
2022-03-04 01:49:18 - train: epoch 0147, iter [02800, 05004], lr: 0.017758, loss: 1.4070
2022-03-04 01:49:52 - train: epoch 0147, iter [02900, 05004], lr: 0.017758, loss: 1.3560
2022-03-04 01:50:24 - train: epoch 0147, iter [03000, 05004], lr: 0.017758, loss: 1.2784
2022-03-04 01:50:57 - train: epoch 0147, iter [03100, 05004], lr: 0.017758, loss: 1.3638
2022-03-04 01:51:30 - train: epoch 0147, iter [03200, 05004], lr: 0.017758, loss: 1.4336
2022-03-04 01:52:03 - train: epoch 0147, iter [03300, 05004], lr: 0.017758, loss: 1.5015
2022-03-04 01:52:37 - train: epoch 0147, iter [03400, 05004], lr: 0.017758, loss: 1.6255
2022-03-04 01:53:10 - train: epoch 0147, iter [03500, 05004], lr: 0.017758, loss: 1.4269
2022-03-04 01:53:43 - train: epoch 0147, iter [03600, 05004], lr: 0.017758, loss: 1.3389
2022-03-04 01:54:16 - train: epoch 0147, iter [03700, 05004], lr: 0.017758, loss: 1.3363
2022-03-04 01:54:49 - train: epoch 0147, iter [03800, 05004], lr: 0.017758, loss: 1.1942
2022-03-04 01:55:22 - train: epoch 0147, iter [03900, 05004], lr: 0.017758, loss: 1.6392
2022-03-04 01:55:55 - train: epoch 0147, iter [04000, 05004], lr: 0.017758, loss: 1.3877
2022-03-04 01:56:28 - train: epoch 0147, iter [04100, 05004], lr: 0.017758, loss: 1.2759
2022-03-04 01:57:01 - train: epoch 0147, iter [04200, 05004], lr: 0.017758, loss: 1.5463
2022-03-04 01:57:35 - train: epoch 0147, iter [04300, 05004], lr: 0.017758, loss: 1.3151
2022-03-04 01:58:08 - train: epoch 0147, iter [04400, 05004], lr: 0.017758, loss: 1.4020
2022-03-04 01:58:41 - train: epoch 0147, iter [04500, 05004], lr: 0.017758, loss: 1.5315
2022-03-04 01:59:14 - train: epoch 0147, iter [04600, 05004], lr: 0.017758, loss: 1.4171
2022-03-04 01:59:47 - train: epoch 0147, iter [04700, 05004], lr: 0.017758, loss: 1.5282
2022-03-04 02:00:21 - train: epoch 0147, iter [04800, 05004], lr: 0.017758, loss: 1.3159
2022-03-04 02:00:54 - train: epoch 0147, iter [04900, 05004], lr: 0.017758, loss: 1.1462
2022-03-04 02:01:25 - train: epoch 0147, iter [05000, 05004], lr: 0.017758, loss: 1.3298
2022-03-04 02:01:27 - train: epoch 147, train_loss: 1.3894
2022-03-04 02:02:40 - eval: epoch: 147, acc1: 69.630%, acc5: 89.728%, test_loss: 1.2133, per_image_load_time: 2.241ms, per_image_inference_time: 0.529ms
2022-03-04 02:02:41 - until epoch: 147, best_acc1: 69.904%
2022-03-04 02:02:41 - epoch 148 lr: 0.01714641594781347
2022-03-04 02:03:19 - train: epoch 0148, iter [00100, 05004], lr: 0.017146, loss: 1.4028
2022-03-04 02:03:52 - train: epoch 0148, iter [00200, 05004], lr: 0.017146, loss: 1.1608
2022-03-04 02:04:24 - train: epoch 0148, iter [00300, 05004], lr: 0.017146, loss: 1.3423
2022-03-04 02:04:58 - train: epoch 0148, iter [00400, 05004], lr: 0.017146, loss: 1.3877
2022-03-04 02:05:31 - train: epoch 0148, iter [00500, 05004], lr: 0.017146, loss: 1.3270
2022-03-04 02:06:04 - train: epoch 0148, iter [00600, 05004], lr: 0.017146, loss: 1.3260
2022-03-04 02:06:37 - train: epoch 0148, iter [00700, 05004], lr: 0.017146, loss: 1.1313
2022-03-04 02:07:10 - train: epoch 0148, iter [00800, 05004], lr: 0.017146, loss: 1.2791
2022-03-04 02:07:44 - train: epoch 0148, iter [00900, 05004], lr: 0.017146, loss: 1.2647
2022-03-04 02:08:17 - train: epoch 0148, iter [01000, 05004], lr: 0.017146, loss: 1.3028
2022-03-04 02:08:50 - train: epoch 0148, iter [01100, 05004], lr: 0.017146, loss: 1.1739
2022-03-04 02:09:23 - train: epoch 0148, iter [01200, 05004], lr: 0.017146, loss: 1.3390
2022-03-04 02:09:56 - train: epoch 0148, iter [01300, 05004], lr: 0.017146, loss: 1.2314
2022-03-04 02:10:29 - train: epoch 0148, iter [01400, 05004], lr: 0.017146, loss: 1.3101
2022-03-04 02:11:02 - train: epoch 0148, iter [01500, 05004], lr: 0.017146, loss: 1.2993
2022-03-04 02:11:36 - train: epoch 0148, iter [01600, 05004], lr: 0.017146, loss: 1.2926
2022-03-04 02:12:09 - train: epoch 0148, iter [01700, 05004], lr: 0.017146, loss: 1.5955
2022-03-04 02:12:42 - train: epoch 0148, iter [01800, 05004], lr: 0.017146, loss: 1.3627
2022-03-04 02:13:15 - train: epoch 0148, iter [01900, 05004], lr: 0.017146, loss: 1.2775
2022-03-04 02:13:48 - train: epoch 0148, iter [02000, 05004], lr: 0.017146, loss: 1.5577
2022-03-04 02:14:21 - train: epoch 0148, iter [02100, 05004], lr: 0.017146, loss: 1.3455
2022-03-04 02:14:54 - train: epoch 0148, iter [02200, 05004], lr: 0.017146, loss: 1.2363
2022-03-04 02:15:27 - train: epoch 0148, iter [02300, 05004], lr: 0.017146, loss: 1.4107
2022-03-04 02:16:01 - train: epoch 0148, iter [02400, 05004], lr: 0.017146, loss: 1.2835
2022-03-04 02:16:33 - train: epoch 0148, iter [02500, 05004], lr: 0.017146, loss: 1.4092
2022-03-04 02:17:07 - train: epoch 0148, iter [02600, 05004], lr: 0.017146, loss: 1.3263
2022-03-04 02:17:39 - train: epoch 0148, iter [02700, 05004], lr: 0.017146, loss: 1.3501
2022-03-04 02:18:13 - train: epoch 0148, iter [02800, 05004], lr: 0.017146, loss: 1.5026
2022-03-04 02:18:46 - train: epoch 0148, iter [02900, 05004], lr: 0.017146, loss: 1.4695
2022-03-04 02:19:18 - train: epoch 0148, iter [03000, 05004], lr: 0.017146, loss: 1.2360
2022-03-04 02:19:53 - train: epoch 0148, iter [03100, 05004], lr: 0.017146, loss: 1.3174
2022-03-04 02:20:26 - train: epoch 0148, iter [03200, 05004], lr: 0.017146, loss: 1.2943
2022-03-04 02:20:59 - train: epoch 0148, iter [03300, 05004], lr: 0.017146, loss: 1.2043
2022-03-04 02:21:32 - train: epoch 0148, iter [03400, 05004], lr: 0.017146, loss: 1.4663
2022-03-04 02:22:05 - train: epoch 0148, iter [03500, 05004], lr: 0.017146, loss: 1.3172
2022-03-04 02:22:39 - train: epoch 0148, iter [03600, 05004], lr: 0.017146, loss: 1.3141
2022-03-04 02:23:12 - train: epoch 0148, iter [03700, 05004], lr: 0.017146, loss: 1.3939
2022-03-04 02:23:45 - train: epoch 0148, iter [03800, 05004], lr: 0.017146, loss: 1.4394
2022-03-04 02:24:19 - train: epoch 0148, iter [03900, 05004], lr: 0.017146, loss: 1.2658
2022-03-04 02:24:53 - train: epoch 0148, iter [04000, 05004], lr: 0.017146, loss: 1.4396
2022-03-04 02:25:25 - train: epoch 0148, iter [04100, 05004], lr: 0.017146, loss: 1.3510
2022-03-04 02:25:58 - train: epoch 0148, iter [04200, 05004], lr: 0.017146, loss: 1.2306
2022-03-04 02:26:32 - train: epoch 0148, iter [04300, 05004], lr: 0.017146, loss: 1.4500
2022-03-04 02:27:05 - train: epoch 0148, iter [04400, 05004], lr: 0.017146, loss: 1.2106
2022-03-04 02:27:39 - train: epoch 0148, iter [04500, 05004], lr: 0.017146, loss: 1.3334
2022-03-04 02:28:11 - train: epoch 0148, iter [04600, 05004], lr: 0.017146, loss: 1.4134
2022-03-04 02:28:45 - train: epoch 0148, iter [04700, 05004], lr: 0.017146, loss: 1.3618
2022-03-04 02:29:18 - train: epoch 0148, iter [04800, 05004], lr: 0.017146, loss: 1.1260
2022-03-04 02:29:51 - train: epoch 0148, iter [04900, 05004], lr: 0.017146, loss: 1.5422
2022-03-04 02:30:23 - train: epoch 0148, iter [05000, 05004], lr: 0.017146, loss: 1.5429
2022-03-04 02:30:24 - train: epoch 148, train_loss: 1.3765
2022-03-04 02:31:38 - eval: epoch: 148, acc1: 69.940%, acc5: 89.860%, test_loss: 1.2019, per_image_load_time: 2.147ms, per_image_inference_time: 0.553ms
2022-03-04 02:31:38 - until epoch: 148, best_acc1: 69.940%
2022-03-04 02:31:38 - epoch 149 lr: 0.016543469682057107
2022-03-04 02:32:16 - train: epoch 0149, iter [00100, 05004], lr: 0.016543, loss: 1.4567
2022-03-04 02:32:49 - train: epoch 0149, iter [00200, 05004], lr: 0.016543, loss: 1.2817
2022-03-04 02:33:22 - train: epoch 0149, iter [00300, 05004], lr: 0.016543, loss: 1.4149
2022-03-04 02:33:55 - train: epoch 0149, iter [00400, 05004], lr: 0.016543, loss: 1.3190
2022-03-04 02:34:29 - train: epoch 0149, iter [00500, 05004], lr: 0.016543, loss: 1.1602
2022-03-04 02:35:02 - train: epoch 0149, iter [00600, 05004], lr: 0.016543, loss: 1.4014
2022-03-04 02:35:35 - train: epoch 0149, iter [00700, 05004], lr: 0.016543, loss: 1.3550
2022-03-04 02:36:08 - train: epoch 0149, iter [00800, 05004], lr: 0.016543, loss: 1.2898
2022-03-04 02:36:42 - train: epoch 0149, iter [00900, 05004], lr: 0.016543, loss: 1.3192
2022-03-04 02:37:16 - train: epoch 0149, iter [01000, 05004], lr: 0.016543, loss: 1.3797
2022-03-04 02:37:49 - train: epoch 0149, iter [01100, 05004], lr: 0.016543, loss: 1.3842
2022-03-04 02:38:23 - train: epoch 0149, iter [01200, 05004], lr: 0.016543, loss: 1.2781
2022-03-04 02:38:56 - train: epoch 0149, iter [01300, 05004], lr: 0.016543, loss: 1.3906
2022-03-04 02:39:29 - train: epoch 0149, iter [01400, 05004], lr: 0.016543, loss: 1.2030
2022-03-04 02:40:02 - train: epoch 0149, iter [01500, 05004], lr: 0.016543, loss: 1.2058
2022-03-04 02:40:36 - train: epoch 0149, iter [01600, 05004], lr: 0.016543, loss: 1.3812
2022-03-04 02:41:08 - train: epoch 0149, iter [01700, 05004], lr: 0.016543, loss: 1.1765
2022-03-04 02:41:42 - train: epoch 0149, iter [01800, 05004], lr: 0.016543, loss: 1.3859
2022-03-04 02:42:15 - train: epoch 0149, iter [01900, 05004], lr: 0.016543, loss: 1.2949
2022-03-04 02:42:48 - train: epoch 0149, iter [02000, 05004], lr: 0.016543, loss: 1.5380
2022-03-04 02:43:21 - train: epoch 0149, iter [02100, 05004], lr: 0.016543, loss: 1.4621
2022-03-04 02:43:55 - train: epoch 0149, iter [02200, 05004], lr: 0.016543, loss: 1.2435
2022-03-04 02:44:28 - train: epoch 0149, iter [02300, 05004], lr: 0.016543, loss: 1.3171
2022-03-04 02:45:01 - train: epoch 0149, iter [02400, 05004], lr: 0.016543, loss: 1.2647
2022-03-04 02:45:34 - train: epoch 0149, iter [02500, 05004], lr: 0.016543, loss: 1.2134
2022-03-04 02:46:07 - train: epoch 0149, iter [02600, 05004], lr: 0.016543, loss: 1.3888
2022-03-04 02:46:40 - train: epoch 0149, iter [02700, 05004], lr: 0.016543, loss: 1.6597
2022-03-04 02:47:14 - train: epoch 0149, iter [02800, 05004], lr: 0.016543, loss: 1.1988
2022-03-04 02:47:48 - train: epoch 0149, iter [02900, 05004], lr: 0.016543, loss: 1.3689
2022-03-04 02:48:20 - train: epoch 0149, iter [03000, 05004], lr: 0.016543, loss: 1.2395
2022-03-04 02:48:54 - train: epoch 0149, iter [03100, 05004], lr: 0.016543, loss: 1.3661
2022-03-04 02:49:27 - train: epoch 0149, iter [03200, 05004], lr: 0.016543, loss: 1.2620
2022-03-04 02:50:00 - train: epoch 0149, iter [03300, 05004], lr: 0.016543, loss: 1.4762
2022-03-04 02:50:34 - train: epoch 0149, iter [03400, 05004], lr: 0.016543, loss: 1.3439
2022-03-04 02:51:07 - train: epoch 0149, iter [03500, 05004], lr: 0.016543, loss: 1.3677
2022-03-04 02:51:40 - train: epoch 0149, iter [03600, 05004], lr: 0.016543, loss: 1.3438
2022-03-04 02:52:13 - train: epoch 0149, iter [03700, 05004], lr: 0.016543, loss: 1.2323
2022-03-04 02:52:47 - train: epoch 0149, iter [03800, 05004], lr: 0.016543, loss: 1.5119
2022-03-04 02:53:20 - train: epoch 0149, iter [03900, 05004], lr: 0.016543, loss: 1.6451
2022-03-04 02:53:54 - train: epoch 0149, iter [04000, 05004], lr: 0.016543, loss: 1.4727
2022-03-04 02:54:27 - train: epoch 0149, iter [04100, 05004], lr: 0.016543, loss: 1.4516
2022-03-04 02:55:01 - train: epoch 0149, iter [04200, 05004], lr: 0.016543, loss: 1.0018
2022-03-04 02:55:33 - train: epoch 0149, iter [04300, 05004], lr: 0.016543, loss: 1.3907
2022-03-04 02:56:07 - train: epoch 0149, iter [04400, 05004], lr: 0.016543, loss: 1.4843
2022-03-04 02:56:41 - train: epoch 0149, iter [04500, 05004], lr: 0.016543, loss: 1.2595
2022-03-04 02:57:14 - train: epoch 0149, iter [04600, 05004], lr: 0.016543, loss: 1.2148
2022-03-04 02:57:47 - train: epoch 0149, iter [04700, 05004], lr: 0.016543, loss: 1.4453
2022-03-04 02:58:21 - train: epoch 0149, iter [04800, 05004], lr: 0.016543, loss: 1.3892
2022-03-04 02:58:53 - train: epoch 0149, iter [04900, 05004], lr: 0.016543, loss: 1.3227
2022-03-04 02:59:26 - train: epoch 0149, iter [05000, 05004], lr: 0.016543, loss: 1.5057
2022-03-04 02:59:27 - train: epoch 149, train_loss: 1.3648
2022-03-04 03:00:40 - eval: epoch: 149, acc1: 69.894%, acc5: 89.478%, test_loss: 1.2213, per_image_load_time: 2.058ms, per_image_inference_time: 0.560ms
2022-03-04 03:00:41 - until epoch: 149, best_acc1: 69.940%
2022-03-04 03:00:41 - epoch 150 lr: 0.015949207060660137
2022-03-04 03:01:19 - train: epoch 0150, iter [00100, 05004], lr: 0.015949, loss: 1.1928
2022-03-04 03:01:51 - train: epoch 0150, iter [00200, 05004], lr: 0.015949, loss: 1.1977
2022-03-04 03:02:24 - train: epoch 0150, iter [00300, 05004], lr: 0.015949, loss: 1.2507
2022-03-04 03:02:58 - train: epoch 0150, iter [00400, 05004], lr: 0.015949, loss: 1.3688
2022-03-04 03:03:31 - train: epoch 0150, iter [00500, 05004], lr: 0.015949, loss: 1.2690
2022-03-04 03:04:04 - train: epoch 0150, iter [00600, 05004], lr: 0.015949, loss: 1.2886
2022-03-04 03:04:37 - train: epoch 0150, iter [00700, 05004], lr: 0.015949, loss: 1.3410
2022-03-04 03:05:11 - train: epoch 0150, iter [00800, 05004], lr: 0.015949, loss: 1.5241
2022-03-04 03:05:44 - train: epoch 0150, iter [00900, 05004], lr: 0.015949, loss: 1.4428
2022-03-04 03:06:17 - train: epoch 0150, iter [01000, 05004], lr: 0.015949, loss: 1.4841
2022-03-04 03:06:50 - train: epoch 0150, iter [01100, 05004], lr: 0.015949, loss: 1.5061
2022-03-04 03:07:23 - train: epoch 0150, iter [01200, 05004], lr: 0.015949, loss: 1.1815
2022-03-04 03:07:56 - train: epoch 0150, iter [01300, 05004], lr: 0.015949, loss: 1.4556
2022-03-04 03:08:30 - train: epoch 0150, iter [01400, 05004], lr: 0.015949, loss: 1.2988
2022-03-04 03:09:03 - train: epoch 0150, iter [01500, 05004], lr: 0.015949, loss: 1.5238
2022-03-04 03:09:36 - train: epoch 0150, iter [01600, 05004], lr: 0.015949, loss: 1.2353
2022-03-04 03:10:09 - train: epoch 0150, iter [01700, 05004], lr: 0.015949, loss: 1.3861
2022-03-04 03:10:42 - train: epoch 0150, iter [01800, 05004], lr: 0.015949, loss: 1.4390
2022-03-04 03:11:15 - train: epoch 0150, iter [01900, 05004], lr: 0.015949, loss: 1.1201
2022-03-04 03:11:49 - train: epoch 0150, iter [02000, 05004], lr: 0.015949, loss: 1.4667
2022-03-04 03:12:23 - train: epoch 0150, iter [02100, 05004], lr: 0.015949, loss: 1.2114
2022-03-04 03:12:56 - train: epoch 0150, iter [02200, 05004], lr: 0.015949, loss: 1.2793
2022-03-04 03:13:29 - train: epoch 0150, iter [02300, 05004], lr: 0.015949, loss: 1.4278
2022-03-04 03:14:02 - train: epoch 0150, iter [02400, 05004], lr: 0.015949, loss: 1.2856
2022-03-04 03:14:35 - train: epoch 0150, iter [02500, 05004], lr: 0.015949, loss: 1.2725
2022-03-04 03:15:09 - train: epoch 0150, iter [02600, 05004], lr: 0.015949, loss: 1.0958
2022-03-04 03:15:40 - train: epoch 0150, iter [02700, 05004], lr: 0.015949, loss: 1.2728
2022-03-04 03:16:13 - train: epoch 0150, iter [02800, 05004], lr: 0.015949, loss: 1.3699
2022-03-04 03:16:47 - train: epoch 0150, iter [02900, 05004], lr: 0.015949, loss: 1.2038
2022-03-04 03:17:21 - train: epoch 0150, iter [03000, 05004], lr: 0.015949, loss: 1.3744
2022-03-04 03:17:53 - train: epoch 0150, iter [03100, 05004], lr: 0.015949, loss: 1.2949
2022-03-04 03:18:27 - train: epoch 0150, iter [03200, 05004], lr: 0.015949, loss: 1.1915
2022-03-04 03:19:00 - train: epoch 0150, iter [03300, 05004], lr: 0.015949, loss: 1.4600
2022-03-04 03:19:34 - train: epoch 0150, iter [03400, 05004], lr: 0.015949, loss: 1.2917
2022-03-04 03:20:07 - train: epoch 0150, iter [03500, 05004], lr: 0.015949, loss: 1.3189
2022-03-04 03:20:40 - train: epoch 0150, iter [03600, 05004], lr: 0.015949, loss: 1.4943
2022-03-04 03:21:13 - train: epoch 0150, iter [03700, 05004], lr: 0.015949, loss: 1.4953
2022-03-04 03:21:46 - train: epoch 0150, iter [03800, 05004], lr: 0.015949, loss: 1.2244
2022-03-04 03:22:20 - train: epoch 0150, iter [03900, 05004], lr: 0.015949, loss: 1.4169
2022-03-04 03:22:53 - train: epoch 0150, iter [04000, 05004], lr: 0.015949, loss: 1.2883
2022-03-04 03:23:26 - train: epoch 0150, iter [04100, 05004], lr: 0.015949, loss: 1.3518
2022-03-04 03:24:00 - train: epoch 0150, iter [04200, 05004], lr: 0.015949, loss: 1.5654
2022-03-04 03:24:33 - train: epoch 0150, iter [04300, 05004], lr: 0.015949, loss: 1.3878
2022-03-04 03:25:06 - train: epoch 0150, iter [04400, 05004], lr: 0.015949, loss: 1.5104
2022-03-04 03:25:40 - train: epoch 0150, iter [04500, 05004], lr: 0.015949, loss: 1.4087
2022-03-04 03:26:13 - train: epoch 0150, iter [04600, 05004], lr: 0.015949, loss: 1.5674
2022-03-04 03:26:45 - train: epoch 0150, iter [04700, 05004], lr: 0.015949, loss: 1.3768
2022-03-04 03:27:19 - train: epoch 0150, iter [04800, 05004], lr: 0.015949, loss: 1.3618
2022-03-04 03:27:52 - train: epoch 0150, iter [04900, 05004], lr: 0.015949, loss: 1.2657
2022-03-04 03:28:24 - train: epoch 0150, iter [05000, 05004], lr: 0.015949, loss: 1.5479
2022-03-04 03:28:25 - train: epoch 150, train_loss: 1.3554
2022-03-04 03:29:39 - eval: epoch: 150, acc1: 69.606%, acc5: 89.668%, test_loss: 1.2258, per_image_load_time: 1.788ms, per_image_inference_time: 0.533ms
2022-03-04 03:29:39 - until epoch: 150, best_acc1: 69.940%
2022-03-04 03:29:39 - epoch 151 lr: 0.015363782324520032
2022-03-04 03:30:18 - train: epoch 0151, iter [00100, 05004], lr: 0.015364, loss: 1.4024
2022-03-04 03:30:51 - train: epoch 0151, iter [00200, 05004], lr: 0.015364, loss: 1.3275
2022-03-04 03:31:24 - train: epoch 0151, iter [00300, 05004], lr: 0.015364, loss: 1.3103
2022-03-04 03:31:56 - train: epoch 0151, iter [00400, 05004], lr: 0.015364, loss: 1.1678
2022-03-04 03:32:30 - train: epoch 0151, iter [00500, 05004], lr: 0.015364, loss: 1.3194
2022-03-04 03:33:02 - train: epoch 0151, iter [00600, 05004], lr: 0.015364, loss: 1.3699
2022-03-04 03:33:36 - train: epoch 0151, iter [00700, 05004], lr: 0.015364, loss: 1.4778
2022-03-04 03:34:09 - train: epoch 0151, iter [00800, 05004], lr: 0.015364, loss: 1.2949
2022-03-04 03:34:42 - train: epoch 0151, iter [00900, 05004], lr: 0.015364, loss: 1.3279
2022-03-04 03:35:15 - train: epoch 0151, iter [01000, 05004], lr: 0.015364, loss: 1.4045
2022-03-04 03:35:49 - train: epoch 0151, iter [01100, 05004], lr: 0.015364, loss: 1.2898
2022-03-04 03:36:22 - train: epoch 0151, iter [01200, 05004], lr: 0.015364, loss: 1.4302
2022-03-04 03:36:56 - train: epoch 0151, iter [01300, 05004], lr: 0.015364, loss: 1.2014
2022-03-04 03:37:29 - train: epoch 0151, iter [01400, 05004], lr: 0.015364, loss: 1.3017
2022-03-04 03:38:02 - train: epoch 0151, iter [01500, 05004], lr: 0.015364, loss: 1.1840
2022-03-04 03:38:35 - train: epoch 0151, iter [01600, 05004], lr: 0.015364, loss: 1.2575
2022-03-04 03:39:08 - train: epoch 0151, iter [01700, 05004], lr: 0.015364, loss: 1.2941
2022-03-04 03:39:42 - train: epoch 0151, iter [01800, 05004], lr: 0.015364, loss: 1.4314
2022-03-04 03:40:16 - train: epoch 0151, iter [01900, 05004], lr: 0.015364, loss: 1.3247
2022-03-04 03:40:49 - train: epoch 0151, iter [02000, 05004], lr: 0.015364, loss: 1.3118
2022-03-04 03:41:22 - train: epoch 0151, iter [02100, 05004], lr: 0.015364, loss: 1.1280
2022-03-04 03:41:55 - train: epoch 0151, iter [02200, 05004], lr: 0.015364, loss: 1.3360
2022-03-04 03:42:29 - train: epoch 0151, iter [02300, 05004], lr: 0.015364, loss: 1.3849
2022-03-04 03:43:02 - train: epoch 0151, iter [02400, 05004], lr: 0.015364, loss: 1.3035
2022-03-04 03:43:35 - train: epoch 0151, iter [02500, 05004], lr: 0.015364, loss: 1.3072
2022-03-04 03:44:07 - train: epoch 0151, iter [02600, 05004], lr: 0.015364, loss: 1.3870
2022-03-04 03:44:41 - train: epoch 0151, iter [02700, 05004], lr: 0.015364, loss: 1.2154
2022-03-04 03:45:14 - train: epoch 0151, iter [02800, 05004], lr: 0.015364, loss: 1.4544
2022-03-04 03:45:47 - train: epoch 0151, iter [02900, 05004], lr: 0.015364, loss: 1.3457
2022-03-04 03:46:21 - train: epoch 0151, iter [03000, 05004], lr: 0.015364, loss: 1.2322
2022-03-04 03:46:54 - train: epoch 0151, iter [03100, 05004], lr: 0.015364, loss: 1.3355
2022-03-04 03:47:28 - train: epoch 0151, iter [03200, 05004], lr: 0.015364, loss: 1.4065
2022-03-04 03:48:00 - train: epoch 0151, iter [03300, 05004], lr: 0.015364, loss: 1.2031
2022-03-04 03:48:34 - train: epoch 0151, iter [03400, 05004], lr: 0.015364, loss: 1.2590
2022-03-04 03:49:07 - train: epoch 0151, iter [03500, 05004], lr: 0.015364, loss: 1.3519
2022-03-04 03:49:40 - train: epoch 0151, iter [03600, 05004], lr: 0.015364, loss: 1.3482
2022-03-04 03:50:13 - train: epoch 0151, iter [03700, 05004], lr: 0.015364, loss: 1.2379
2022-03-04 03:50:47 - train: epoch 0151, iter [03800, 05004], lr: 0.015364, loss: 1.5537
2022-03-04 03:51:19 - train: epoch 0151, iter [03900, 05004], lr: 0.015364, loss: 1.5099
2022-03-04 03:51:52 - train: epoch 0151, iter [04000, 05004], lr: 0.015364, loss: 1.4484
2022-03-04 03:52:25 - train: epoch 0151, iter [04100, 05004], lr: 0.015364, loss: 1.4182
2022-03-04 03:52:59 - train: epoch 0151, iter [04200, 05004], lr: 0.015364, loss: 1.2585
2022-03-04 03:53:32 - train: epoch 0151, iter [04300, 05004], lr: 0.015364, loss: 1.3189
2022-03-04 03:54:05 - train: epoch 0151, iter [04400, 05004], lr: 0.015364, loss: 1.5227
2022-03-04 03:54:38 - train: epoch 0151, iter [04500, 05004], lr: 0.015364, loss: 1.3468
2022-03-04 03:55:11 - train: epoch 0151, iter [04600, 05004], lr: 0.015364, loss: 1.5148
2022-03-04 03:55:45 - train: epoch 0151, iter [04700, 05004], lr: 0.015364, loss: 1.1343
2022-03-04 03:56:18 - train: epoch 0151, iter [04800, 05004], lr: 0.015364, loss: 1.4637
2022-03-04 03:56:51 - train: epoch 0151, iter [04900, 05004], lr: 0.015364, loss: 1.4129
2022-03-04 03:57:23 - train: epoch 0151, iter [05000, 05004], lr: 0.015364, loss: 1.4447
2022-03-04 03:57:24 - train: epoch 151, train_loss: 1.3421
2022-03-04 03:58:38 - eval: epoch: 151, acc1: 70.720%, acc5: 90.040%, test_loss: 1.1834, per_image_load_time: 2.340ms, per_image_inference_time: 0.519ms
2022-03-04 03:58:39 - until epoch: 151, best_acc1: 70.720%
2022-03-04 03:58:39 - epoch 152 lr: 0.01478734742066054
2022-03-04 03:59:17 - train: epoch 0152, iter [00100, 05004], lr: 0.014787, loss: 1.3526
2022-03-04 03:59:49 - train: epoch 0152, iter [00200, 05004], lr: 0.014787, loss: 1.3540
2022-03-04 04:00:23 - train: epoch 0152, iter [00300, 05004], lr: 0.014787, loss: 1.4031
2022-03-04 04:00:56 - train: epoch 0152, iter [00400, 05004], lr: 0.014787, loss: 1.3450
2022-03-04 04:01:28 - train: epoch 0152, iter [00500, 05004], lr: 0.014787, loss: 1.3045
2022-03-04 04:02:01 - train: epoch 0152, iter [00600, 05004], lr: 0.014787, loss: 1.2197
2022-03-04 04:02:35 - train: epoch 0152, iter [00700, 05004], lr: 0.014787, loss: 1.2528
2022-03-04 04:03:08 - train: epoch 0152, iter [00800, 05004], lr: 0.014787, loss: 1.2354
2022-03-04 04:03:41 - train: epoch 0152, iter [00900, 05004], lr: 0.014787, loss: 1.4816
2022-03-04 04:04:15 - train: epoch 0152, iter [01000, 05004], lr: 0.014787, loss: 1.5045
2022-03-04 04:04:48 - train: epoch 0152, iter [01100, 05004], lr: 0.014787, loss: 1.4151
2022-03-04 04:05:21 - train: epoch 0152, iter [01200, 05004], lr: 0.014787, loss: 1.1697
2022-03-04 04:05:54 - train: epoch 0152, iter [01300, 05004], lr: 0.014787, loss: 1.3309
2022-03-04 04:06:27 - train: epoch 0152, iter [01400, 05004], lr: 0.014787, loss: 1.4123
2022-03-04 04:07:01 - train: epoch 0152, iter [01500, 05004], lr: 0.014787, loss: 1.2387
2022-03-04 04:07:33 - train: epoch 0152, iter [01600, 05004], lr: 0.014787, loss: 1.2241
2022-03-04 04:08:08 - train: epoch 0152, iter [01700, 05004], lr: 0.014787, loss: 1.4243
2022-03-04 04:08:41 - train: epoch 0152, iter [01800, 05004], lr: 0.014787, loss: 1.2143
2022-03-04 04:09:14 - train: epoch 0152, iter [01900, 05004], lr: 0.014787, loss: 1.1381
2022-03-04 04:09:47 - train: epoch 0152, iter [02000, 05004], lr: 0.014787, loss: 1.4261
2022-03-04 04:10:20 - train: epoch 0152, iter [02100, 05004], lr: 0.014787, loss: 1.1697
2022-03-04 04:10:53 - train: epoch 0152, iter [02200, 05004], lr: 0.014787, loss: 1.4273
2022-03-04 04:11:26 - train: epoch 0152, iter [02300, 05004], lr: 0.014787, loss: 1.3993
2022-03-04 04:12:00 - train: epoch 0152, iter [02400, 05004], lr: 0.014787, loss: 1.2033
2022-03-04 04:12:33 - train: epoch 0152, iter [02500, 05004], lr: 0.014787, loss: 1.5408
2022-03-04 04:13:06 - train: epoch 0152, iter [02600, 05004], lr: 0.014787, loss: 1.5094
2022-03-04 04:13:39 - train: epoch 0152, iter [02700, 05004], lr: 0.014787, loss: 1.4019
2022-03-04 04:14:12 - train: epoch 0152, iter [02800, 05004], lr: 0.014787, loss: 1.1975
2022-03-04 04:14:46 - train: epoch 0152, iter [02900, 05004], lr: 0.014787, loss: 1.3974
2022-03-04 04:15:19 - train: epoch 0152, iter [03000, 05004], lr: 0.014787, loss: 1.3424
2022-03-04 04:15:51 - train: epoch 0152, iter [03100, 05004], lr: 0.014787, loss: 1.2337
2022-03-04 04:16:25 - train: epoch 0152, iter [03200, 05004], lr: 0.014787, loss: 1.3740
2022-03-04 04:16:59 - train: epoch 0152, iter [03300, 05004], lr: 0.014787, loss: 1.4464
2022-03-04 04:17:31 - train: epoch 0152, iter [03400, 05004], lr: 0.014787, loss: 1.2759
2022-03-04 04:18:05 - train: epoch 0152, iter [03500, 05004], lr: 0.014787, loss: 1.1672
2022-03-04 04:18:38 - train: epoch 0152, iter [03600, 05004], lr: 0.014787, loss: 1.4240
2022-03-04 04:19:12 - train: epoch 0152, iter [03700, 05004], lr: 0.014787, loss: 1.3174
2022-03-04 04:19:44 - train: epoch 0152, iter [03800, 05004], lr: 0.014787, loss: 1.2129
2022-03-04 04:20:18 - train: epoch 0152, iter [03900, 05004], lr: 0.014787, loss: 1.4154
2022-03-04 04:20:51 - train: epoch 0152, iter [04000, 05004], lr: 0.014787, loss: 1.0926
2022-03-04 04:21:24 - train: epoch 0152, iter [04100, 05004], lr: 0.014787, loss: 1.4494
2022-03-04 04:21:57 - train: epoch 0152, iter [04200, 05004], lr: 0.014787, loss: 1.3260
2022-03-04 04:22:30 - train: epoch 0152, iter [04300, 05004], lr: 0.014787, loss: 1.4478
2022-03-04 04:23:03 - train: epoch 0152, iter [04400, 05004], lr: 0.014787, loss: 1.4342
2022-03-04 04:23:37 - train: epoch 0152, iter [04500, 05004], lr: 0.014787, loss: 1.3476
2022-03-04 04:24:10 - train: epoch 0152, iter [04600, 05004], lr: 0.014787, loss: 1.1664
2022-03-04 04:24:43 - train: epoch 0152, iter [04700, 05004], lr: 0.014787, loss: 1.2278
2022-03-04 04:25:17 - train: epoch 0152, iter [04800, 05004], lr: 0.014787, loss: 1.4446
2022-03-04 04:25:50 - train: epoch 0152, iter [04900, 05004], lr: 0.014787, loss: 1.4701
2022-03-04 04:26:23 - train: epoch 0152, iter [05000, 05004], lr: 0.014787, loss: 1.3320
2022-03-04 04:26:24 - train: epoch 152, train_loss: 1.3313
2022-03-04 04:27:38 - eval: epoch: 152, acc1: 70.776%, acc5: 90.222%, test_loss: 1.1725, per_image_load_time: 2.354ms, per_image_inference_time: 0.529ms
2022-03-04 04:27:39 - until epoch: 152, best_acc1: 70.776%
2022-03-04 04:27:39 - epoch 153 lr: 0.014220051962793951
2022-03-04 04:28:16 - train: epoch 0153, iter [00100, 05004], lr: 0.014220, loss: 1.3813
2022-03-04 04:28:50 - train: epoch 0153, iter [00200, 05004], lr: 0.014220, loss: 1.2059
2022-03-04 04:29:23 - train: epoch 0153, iter [00300, 05004], lr: 0.014220, loss: 1.3713
2022-03-04 04:29:56 - train: epoch 0153, iter [00400, 05004], lr: 0.014220, loss: 1.1932
2022-03-04 04:30:29 - train: epoch 0153, iter [00500, 05004], lr: 0.014220, loss: 1.4872
2022-03-04 04:31:03 - train: epoch 0153, iter [00600, 05004], lr: 0.014220, loss: 1.3615
2022-03-04 04:31:36 - train: epoch 0153, iter [00700, 05004], lr: 0.014220, loss: 1.3243
2022-03-04 04:32:09 - train: epoch 0153, iter [00800, 05004], lr: 0.014220, loss: 1.1915
2022-03-04 04:32:42 - train: epoch 0153, iter [00900, 05004], lr: 0.014220, loss: 1.3231
2022-03-04 04:33:16 - train: epoch 0153, iter [01000, 05004], lr: 0.014220, loss: 1.1897
2022-03-04 04:33:48 - train: epoch 0153, iter [01100, 05004], lr: 0.014220, loss: 1.3134
2022-03-04 04:34:22 - train: epoch 0153, iter [01200, 05004], lr: 0.014220, loss: 1.3386
2022-03-04 04:34:55 - train: epoch 0153, iter [01300, 05004], lr: 0.014220, loss: 1.4176
2022-03-04 04:35:28 - train: epoch 0153, iter [01400, 05004], lr: 0.014220, loss: 1.2695
2022-03-04 04:36:01 - train: epoch 0153, iter [01500, 05004], lr: 0.014220, loss: 1.1677
2022-03-04 04:36:34 - train: epoch 0153, iter [01600, 05004], lr: 0.014220, loss: 1.4025
2022-03-04 04:37:07 - train: epoch 0153, iter [01700, 05004], lr: 0.014220, loss: 1.2602
2022-03-04 04:37:40 - train: epoch 0153, iter [01800, 05004], lr: 0.014220, loss: 1.2407
2022-03-04 04:38:13 - train: epoch 0153, iter [01900, 05004], lr: 0.014220, loss: 1.1417
2022-03-04 04:38:45 - train: epoch 0153, iter [02000, 05004], lr: 0.014220, loss: 1.2113
2022-03-04 04:39:19 - train: epoch 0153, iter [02100, 05004], lr: 0.014220, loss: 1.3684
2022-03-04 04:39:53 - train: epoch 0153, iter [02200, 05004], lr: 0.014220, loss: 1.1761
2022-03-04 04:40:26 - train: epoch 0153, iter [02300, 05004], lr: 0.014220, loss: 1.2991
2022-03-04 04:41:00 - train: epoch 0153, iter [02400, 05004], lr: 0.014220, loss: 1.4173
2022-03-04 04:41:32 - train: epoch 0153, iter [02500, 05004], lr: 0.014220, loss: 1.3585
2022-03-04 04:42:06 - train: epoch 0153, iter [02600, 05004], lr: 0.014220, loss: 1.1021
2022-03-04 04:42:38 - train: epoch 0153, iter [02700, 05004], lr: 0.014220, loss: 1.3974
2022-03-04 04:43:12 - train: epoch 0153, iter [02800, 05004], lr: 0.014220, loss: 1.2665
2022-03-04 04:43:46 - train: epoch 0153, iter [02900, 05004], lr: 0.014220, loss: 1.2354
2022-03-04 04:44:20 - train: epoch 0153, iter [03000, 05004], lr: 0.014220, loss: 1.4963
2022-03-04 04:44:51 - train: epoch 0153, iter [03100, 05004], lr: 0.014220, loss: 1.3753
2022-03-04 04:45:25 - train: epoch 0153, iter [03200, 05004], lr: 0.014220, loss: 1.3795
2022-03-04 04:45:58 - train: epoch 0153, iter [03300, 05004], lr: 0.014220, loss: 1.3492
2022-03-04 04:46:31 - train: epoch 0153, iter [03400, 05004], lr: 0.014220, loss: 1.2011
2022-03-04 04:47:05 - train: epoch 0153, iter [03500, 05004], lr: 0.014220, loss: 1.2397
2022-03-04 04:47:38 - train: epoch 0153, iter [03600, 05004], lr: 0.014220, loss: 1.3498
2022-03-04 04:48:11 - train: epoch 0153, iter [03700, 05004], lr: 0.014220, loss: 1.3587
2022-03-04 04:48:44 - train: epoch 0153, iter [03800, 05004], lr: 0.014220, loss: 1.4146
2022-03-04 04:49:18 - train: epoch 0153, iter [03900, 05004], lr: 0.014220, loss: 1.2401
2022-03-04 04:49:50 - train: epoch 0153, iter [04000, 05004], lr: 0.014220, loss: 1.5917
2022-03-04 04:50:23 - train: epoch 0153, iter [04100, 05004], lr: 0.014220, loss: 1.2552
2022-03-04 04:50:56 - train: epoch 0153, iter [04200, 05004], lr: 0.014220, loss: 1.4658
2022-03-04 04:51:30 - train: epoch 0153, iter [04300, 05004], lr: 0.014220, loss: 1.3605
2022-03-04 04:52:03 - train: epoch 0153, iter [04400, 05004], lr: 0.014220, loss: 1.2512
2022-03-04 04:52:36 - train: epoch 0153, iter [04500, 05004], lr: 0.014220, loss: 1.2773
2022-03-04 04:53:09 - train: epoch 0153, iter [04600, 05004], lr: 0.014220, loss: 1.2526
2022-03-04 04:53:43 - train: epoch 0153, iter [04700, 05004], lr: 0.014220, loss: 1.3844
2022-03-04 04:54:16 - train: epoch 0153, iter [04800, 05004], lr: 0.014220, loss: 1.3184
2022-03-04 04:54:50 - train: epoch 0153, iter [04900, 05004], lr: 0.014220, loss: 1.1783
2022-03-04 04:55:21 - train: epoch 0153, iter [05000, 05004], lr: 0.014220, loss: 1.2224
2022-03-04 04:55:22 - train: epoch 153, train_loss: 1.3184
2022-03-04 04:56:36 - eval: epoch: 153, acc1: 71.222%, acc5: 90.488%, test_loss: 1.1511, per_image_load_time: 2.343ms, per_image_inference_time: 0.504ms
2022-03-04 04:56:37 - until epoch: 153, best_acc1: 71.222%
2022-03-04 04:56:37 - epoch 154 lr: 0.01366204319248885
2022-03-04 04:57:14 - train: epoch 0154, iter [00100, 05004], lr: 0.013662, loss: 1.3531
2022-03-04 04:57:48 - train: epoch 0154, iter [00200, 05004], lr: 0.013662, loss: 1.2528
2022-03-04 04:58:20 - train: epoch 0154, iter [00300, 05004], lr: 0.013662, loss: 1.3283
2022-03-04 04:58:54 - train: epoch 0154, iter [00400, 05004], lr: 0.013662, loss: 1.1587
2022-03-04 04:59:26 - train: epoch 0154, iter [00500, 05004], lr: 0.013662, loss: 1.4039
2022-03-04 05:00:00 - train: epoch 0154, iter [00600, 05004], lr: 0.013662, loss: 1.2273
2022-03-04 05:00:33 - train: epoch 0154, iter [00700, 05004], lr: 0.013662, loss: 1.2741
2022-03-04 05:01:06 - train: epoch 0154, iter [00800, 05004], lr: 0.013662, loss: 1.0420
2022-03-04 05:01:39 - train: epoch 0154, iter [00900, 05004], lr: 0.013662, loss: 1.4158
2022-03-04 05:02:12 - train: epoch 0154, iter [01000, 05004], lr: 0.013662, loss: 1.2688
2022-03-04 05:02:46 - train: epoch 0154, iter [01100, 05004], lr: 0.013662, loss: 1.3414
2022-03-04 05:03:19 - train: epoch 0154, iter [01200, 05004], lr: 0.013662, loss: 1.2659
2022-03-04 05:03:52 - train: epoch 0154, iter [01300, 05004], lr: 0.013662, loss: 1.0538
2022-03-04 05:04:26 - train: epoch 0154, iter [01400, 05004], lr: 0.013662, loss: 1.3588
2022-03-04 05:04:59 - train: epoch 0154, iter [01500, 05004], lr: 0.013662, loss: 1.3384
2022-03-04 05:05:32 - train: epoch 0154, iter [01600, 05004], lr: 0.013662, loss: 1.2534
2022-03-04 05:06:05 - train: epoch 0154, iter [01700, 05004], lr: 0.013662, loss: 1.3438
2022-03-04 05:06:38 - train: epoch 0154, iter [01800, 05004], lr: 0.013662, loss: 1.1369
2022-03-04 05:07:12 - train: epoch 0154, iter [01900, 05004], lr: 0.013662, loss: 1.3626
2022-03-04 05:07:46 - train: epoch 0154, iter [02000, 05004], lr: 0.013662, loss: 1.2159
2022-03-04 05:08:18 - train: epoch 0154, iter [02100, 05004], lr: 0.013662, loss: 1.2829
2022-03-04 05:08:51 - train: epoch 0154, iter [02200, 05004], lr: 0.013662, loss: 1.1903
2022-03-04 05:09:24 - train: epoch 0154, iter [02300, 05004], lr: 0.013662, loss: 1.2449
2022-03-04 05:09:57 - train: epoch 0154, iter [02400, 05004], lr: 0.013662, loss: 1.2395
2022-03-04 05:10:31 - train: epoch 0154, iter [02500, 05004], lr: 0.013662, loss: 1.4109
2022-03-04 05:11:05 - train: epoch 0154, iter [02600, 05004], lr: 0.013662, loss: 1.1518
2022-03-04 05:11:37 - train: epoch 0154, iter [02700, 05004], lr: 0.013662, loss: 1.2805
2022-03-04 05:12:11 - train: epoch 0154, iter [02800, 05004], lr: 0.013662, loss: 1.5613
2022-03-04 05:12:43 - train: epoch 0154, iter [02900, 05004], lr: 0.013662, loss: 1.3872
2022-03-04 05:13:17 - train: epoch 0154, iter [03000, 05004], lr: 0.013662, loss: 1.4019
2022-03-04 05:13:51 - train: epoch 0154, iter [03100, 05004], lr: 0.013662, loss: 1.2190
2022-03-04 05:14:24 - train: epoch 0154, iter [03200, 05004], lr: 0.013662, loss: 1.2824
2022-03-04 05:14:58 - train: epoch 0154, iter [03300, 05004], lr: 0.013662, loss: 1.2458
2022-03-04 05:15:30 - train: epoch 0154, iter [03400, 05004], lr: 0.013662, loss: 1.1609
2022-03-04 05:16:04 - train: epoch 0154, iter [03500, 05004], lr: 0.013662, loss: 1.1976
2022-03-04 05:16:37 - train: epoch 0154, iter [03600, 05004], lr: 0.013662, loss: 1.2371
2022-03-04 05:17:10 - train: epoch 0154, iter [03700, 05004], lr: 0.013662, loss: 1.2884
2022-03-04 05:17:44 - train: epoch 0154, iter [03800, 05004], lr: 0.013662, loss: 1.1960
2022-03-04 05:18:17 - train: epoch 0154, iter [03900, 05004], lr: 0.013662, loss: 1.3529
2022-03-04 05:18:50 - train: epoch 0154, iter [04000, 05004], lr: 0.013662, loss: 1.3667
2022-03-04 05:19:23 - train: epoch 0154, iter [04100, 05004], lr: 0.013662, loss: 1.1458
2022-03-04 05:19:56 - train: epoch 0154, iter [04200, 05004], lr: 0.013662, loss: 1.3216
2022-03-04 05:20:29 - train: epoch 0154, iter [04300, 05004], lr: 0.013662, loss: 1.4070
2022-03-04 05:21:02 - train: epoch 0154, iter [04400, 05004], lr: 0.013662, loss: 1.1469
2022-03-04 05:21:36 - train: epoch 0154, iter [04500, 05004], lr: 0.013662, loss: 1.2734
2022-03-04 05:22:08 - train: epoch 0154, iter [04600, 05004], lr: 0.013662, loss: 1.4024
2022-03-04 05:22:42 - train: epoch 0154, iter [04700, 05004], lr: 0.013662, loss: 1.3356
2022-03-04 05:23:15 - train: epoch 0154, iter [04800, 05004], lr: 0.013662, loss: 1.6314
2022-03-04 05:23:49 - train: epoch 0154, iter [04900, 05004], lr: 0.013662, loss: 1.7371
2022-03-04 05:24:20 - train: epoch 0154, iter [05000, 05004], lr: 0.013662, loss: 1.0409
2022-03-04 05:24:21 - train: epoch 154, train_loss: 1.3037
2022-03-04 05:25:35 - eval: epoch: 154, acc1: 70.988%, acc5: 90.400%, test_loss: 1.1630, per_image_load_time: 2.303ms, per_image_inference_time: 0.534ms
2022-03-04 05:25:35 - until epoch: 154, best_acc1: 71.222%
2022-03-04 05:25:35 - epoch 155 lr: 0.013113465940953495
2022-03-04 05:26:13 - train: epoch 0155, iter [00100, 05004], lr: 0.013113, loss: 1.3110
2022-03-04 05:26:47 - train: epoch 0155, iter [00200, 05004], lr: 0.013113, loss: 1.2862
2022-03-04 05:27:20 - train: epoch 0155, iter [00300, 05004], lr: 0.013113, loss: 1.2977
2022-03-04 05:27:53 - train: epoch 0155, iter [00400, 05004], lr: 0.013113, loss: 1.3379
2022-03-04 05:28:26 - train: epoch 0155, iter [00500, 05004], lr: 0.013113, loss: 1.3340
2022-03-04 05:28:58 - train: epoch 0155, iter [00600, 05004], lr: 0.013113, loss: 1.3358
2022-03-04 05:29:32 - train: epoch 0155, iter [00700, 05004], lr: 0.013113, loss: 1.1347
2022-03-04 05:30:05 - train: epoch 0155, iter [00800, 05004], lr: 0.013113, loss: 1.1383
2022-03-04 05:30:39 - train: epoch 0155, iter [00900, 05004], lr: 0.013113, loss: 1.3522
2022-03-04 05:31:12 - train: epoch 0155, iter [01000, 05004], lr: 0.013113, loss: 1.4346
2022-03-04 05:31:45 - train: epoch 0155, iter [01100, 05004], lr: 0.013113, loss: 1.3428
2022-03-04 05:32:18 - train: epoch 0155, iter [01200, 05004], lr: 0.013113, loss: 1.3028
2022-03-04 05:32:51 - train: epoch 0155, iter [01300, 05004], lr: 0.013113, loss: 1.2086
2022-03-04 05:33:25 - train: epoch 0155, iter [01400, 05004], lr: 0.013113, loss: 1.5114
2022-03-04 05:33:58 - train: epoch 0155, iter [01500, 05004], lr: 0.013113, loss: 1.2191
2022-03-04 05:34:31 - train: epoch 0155, iter [01600, 05004], lr: 0.013113, loss: 1.3976
2022-03-04 05:35:04 - train: epoch 0155, iter [01700, 05004], lr: 0.013113, loss: 1.2690
2022-03-04 05:35:37 - train: epoch 0155, iter [01800, 05004], lr: 0.013113, loss: 1.2983
2022-03-04 05:36:11 - train: epoch 0155, iter [01900, 05004], lr: 0.013113, loss: 1.0873
2022-03-04 05:36:45 - train: epoch 0155, iter [02000, 05004], lr: 0.013113, loss: 1.3657
2022-03-04 05:37:18 - train: epoch 0155, iter [02100, 05004], lr: 0.013113, loss: 1.1778
2022-03-04 05:37:51 - train: epoch 0155, iter [02200, 05004], lr: 0.013113, loss: 1.2376
2022-03-04 05:38:24 - train: epoch 0155, iter [02300, 05004], lr: 0.013113, loss: 1.2617
2022-03-04 05:38:58 - train: epoch 0155, iter [02400, 05004], lr: 0.013113, loss: 1.4005
2022-03-04 05:39:31 - train: epoch 0155, iter [02500, 05004], lr: 0.013113, loss: 1.2919
2022-03-04 05:40:05 - train: epoch 0155, iter [02600, 05004], lr: 0.013113, loss: 1.3389
2022-03-04 05:40:38 - train: epoch 0155, iter [02700, 05004], lr: 0.013113, loss: 1.3645
2022-03-04 05:41:11 - train: epoch 0155, iter [02800, 05004], lr: 0.013113, loss: 1.1441
2022-03-04 05:41:44 - train: epoch 0155, iter [02900, 05004], lr: 0.013113, loss: 1.2799
2022-03-04 05:42:17 - train: epoch 0155, iter [03000, 05004], lr: 0.013113, loss: 1.2048
2022-03-04 05:42:50 - train: epoch 0155, iter [03100, 05004], lr: 0.013113, loss: 1.1508
2022-03-04 05:43:23 - train: epoch 0155, iter [03200, 05004], lr: 0.013113, loss: 1.3162
2022-03-04 05:43:57 - train: epoch 0155, iter [03300, 05004], lr: 0.013113, loss: 1.4693
2022-03-04 05:44:29 - train: epoch 0155, iter [03400, 05004], lr: 0.013113, loss: 1.1941
2022-03-04 05:45:03 - train: epoch 0155, iter [03500, 05004], lr: 0.013113, loss: 1.3210
2022-03-04 05:45:36 - train: epoch 0155, iter [03600, 05004], lr: 0.013113, loss: 1.3465
2022-03-04 05:46:09 - train: epoch 0155, iter [03700, 05004], lr: 0.013113, loss: 1.1855
2022-03-04 05:46:43 - train: epoch 0155, iter [03800, 05004], lr: 0.013113, loss: 1.4677
2022-03-04 05:47:16 - train: epoch 0155, iter [03900, 05004], lr: 0.013113, loss: 1.1608
2022-03-04 05:47:49 - train: epoch 0155, iter [04000, 05004], lr: 0.013113, loss: 1.2850
2022-03-04 05:48:22 - train: epoch 0155, iter [04100, 05004], lr: 0.013113, loss: 1.3706
2022-03-04 05:48:55 - train: epoch 0155, iter [04200, 05004], lr: 0.013113, loss: 1.2276
2022-03-04 05:49:29 - train: epoch 0155, iter [04300, 05004], lr: 0.013113, loss: 1.4492
2022-03-04 05:50:02 - train: epoch 0155, iter [04400, 05004], lr: 0.013113, loss: 1.4631
2022-03-04 05:50:35 - train: epoch 0155, iter [04500, 05004], lr: 0.013113, loss: 1.1156
2022-03-04 05:51:09 - train: epoch 0155, iter [04600, 05004], lr: 0.013113, loss: 1.2924
2022-03-04 05:51:43 - train: epoch 0155, iter [04700, 05004], lr: 0.013113, loss: 1.4065
2022-03-04 05:52:17 - train: epoch 0155, iter [04800, 05004], lr: 0.013113, loss: 1.3635
2022-03-04 05:52:49 - train: epoch 0155, iter [04900, 05004], lr: 0.013113, loss: 1.3520
2022-03-04 05:53:21 - train: epoch 0155, iter [05000, 05004], lr: 0.013113, loss: 1.1887
2022-03-04 05:53:22 - train: epoch 155, train_loss: 1.2897
2022-03-04 05:54:37 - eval: epoch: 155, acc1: 71.196%, acc5: 90.378%, test_loss: 1.1576, per_image_load_time: 2.361ms, per_image_inference_time: 0.518ms
2022-03-04 05:54:37 - until epoch: 155, best_acc1: 71.222%
2022-03-04 05:54:37 - epoch 156 lr: 0.012574462591444941
2022-03-04 05:55:15 - train: epoch 0156, iter [00100, 05004], lr: 0.012574, loss: 1.3675
2022-03-04 05:55:48 - train: epoch 0156, iter [00200, 05004], lr: 0.012574, loss: 1.3201
2022-03-04 05:56:22 - train: epoch 0156, iter [00300, 05004], lr: 0.012574, loss: 1.1122
2022-03-04 05:56:55 - train: epoch 0156, iter [00400, 05004], lr: 0.012574, loss: 1.1279
2022-03-04 05:57:28 - train: epoch 0156, iter [00500, 05004], lr: 0.012574, loss: 1.2311
2022-03-04 05:58:02 - train: epoch 0156, iter [00600, 05004], lr: 0.012574, loss: 1.1746
2022-03-04 05:58:34 - train: epoch 0156, iter [00700, 05004], lr: 0.012574, loss: 1.1246
2022-03-04 05:59:08 - train: epoch 0156, iter [00800, 05004], lr: 0.012574, loss: 1.4468
2022-03-04 05:59:40 - train: epoch 0156, iter [00900, 05004], lr: 0.012574, loss: 1.4643
2022-03-04 06:00:13 - train: epoch 0156, iter [01000, 05004], lr: 0.012574, loss: 1.2350
2022-03-04 06:00:47 - train: epoch 0156, iter [01100, 05004], lr: 0.012574, loss: 1.2167
2022-03-04 06:01:20 - train: epoch 0156, iter [01200, 05004], lr: 0.012574, loss: 1.0111
2022-03-04 06:01:53 - train: epoch 0156, iter [01300, 05004], lr: 0.012574, loss: 1.2866
2022-03-04 06:02:27 - train: epoch 0156, iter [01400, 05004], lr: 0.012574, loss: 1.3654
2022-03-04 06:02:59 - train: epoch 0156, iter [01500, 05004], lr: 0.012574, loss: 1.2696
2022-03-04 06:03:32 - train: epoch 0156, iter [01600, 05004], lr: 0.012574, loss: 1.0828
2022-03-04 06:04:06 - train: epoch 0156, iter [01700, 05004], lr: 0.012574, loss: 1.3274
2022-03-04 06:04:38 - train: epoch 0156, iter [01800, 05004], lr: 0.012574, loss: 1.2043
2022-03-04 06:05:12 - train: epoch 0156, iter [01900, 05004], lr: 0.012574, loss: 1.0571
2022-03-04 06:05:44 - train: epoch 0156, iter [02000, 05004], lr: 0.012574, loss: 1.0523
2022-03-04 06:06:17 - train: epoch 0156, iter [02100, 05004], lr: 0.012574, loss: 1.4542
2022-03-04 06:06:50 - train: epoch 0156, iter [02200, 05004], lr: 0.012574, loss: 1.3976
2022-03-04 06:07:24 - train: epoch 0156, iter [02300, 05004], lr: 0.012574, loss: 1.2235
2022-03-04 06:07:57 - train: epoch 0156, iter [02400, 05004], lr: 0.012574, loss: 1.2473
2022-03-04 06:08:30 - train: epoch 0156, iter [02500, 05004], lr: 0.012574, loss: 1.1998
2022-03-04 06:09:04 - train: epoch 0156, iter [02600, 05004], lr: 0.012574, loss: 1.4879
2022-03-04 06:09:37 - train: epoch 0156, iter [02700, 05004], lr: 0.012574, loss: 1.1476
2022-03-04 06:10:11 - train: epoch 0156, iter [02800, 05004], lr: 0.012574, loss: 1.2679
2022-03-04 06:10:43 - train: epoch 0156, iter [02900, 05004], lr: 0.012574, loss: 1.2847
2022-03-04 06:11:16 - train: epoch 0156, iter [03000, 05004], lr: 0.012574, loss: 1.5016
2022-03-04 06:11:50 - train: epoch 0156, iter [03100, 05004], lr: 0.012574, loss: 1.2919
2022-03-04 06:12:23 - train: epoch 0156, iter [03200, 05004], lr: 0.012574, loss: 1.1102
2022-03-04 06:12:56 - train: epoch 0156, iter [03300, 05004], lr: 0.012574, loss: 1.1454
2022-03-04 06:13:30 - train: epoch 0156, iter [03400, 05004], lr: 0.012574, loss: 1.2545
2022-03-04 06:14:03 - train: epoch 0156, iter [03500, 05004], lr: 0.012574, loss: 1.3360
2022-03-04 06:14:36 - train: epoch 0156, iter [03600, 05004], lr: 0.012574, loss: 1.6036
2022-03-04 06:15:10 - train: epoch 0156, iter [03700, 05004], lr: 0.012574, loss: 1.3073
2022-03-04 06:15:42 - train: epoch 0156, iter [03800, 05004], lr: 0.012574, loss: 1.4417
2022-03-04 06:16:16 - train: epoch 0156, iter [03900, 05004], lr: 0.012574, loss: 1.3728
2022-03-04 06:16:49 - train: epoch 0156, iter [04000, 05004], lr: 0.012574, loss: 1.2715
2022-03-04 06:17:22 - train: epoch 0156, iter [04100, 05004], lr: 0.012574, loss: 1.4925
2022-03-04 06:17:55 - train: epoch 0156, iter [04200, 05004], lr: 0.012574, loss: 1.2814
2022-03-04 06:18:28 - train: epoch 0156, iter [04300, 05004], lr: 0.012574, loss: 1.2722
2022-03-04 06:19:02 - train: epoch 0156, iter [04400, 05004], lr: 0.012574, loss: 1.1529
2022-03-04 06:19:36 - train: epoch 0156, iter [04500, 05004], lr: 0.012574, loss: 1.2124
2022-03-04 06:20:09 - train: epoch 0156, iter [04600, 05004], lr: 0.012574, loss: 1.2517
2022-03-04 06:20:42 - train: epoch 0156, iter [04700, 05004], lr: 0.012574, loss: 1.2352
2022-03-04 06:21:15 - train: epoch 0156, iter [04800, 05004], lr: 0.012574, loss: 1.2048
2022-03-04 06:21:49 - train: epoch 0156, iter [04900, 05004], lr: 0.012574, loss: 1.4370
2022-03-04 06:22:20 - train: epoch 0156, iter [05000, 05004], lr: 0.012574, loss: 1.3198
2022-03-04 06:22:22 - train: epoch 156, train_loss: 1.2766
2022-03-04 06:23:36 - eval: epoch: 156, acc1: 71.128%, acc5: 90.402%, test_loss: 1.1602, per_image_load_time: 2.360ms, per_image_inference_time: 0.518ms
2022-03-04 06:23:36 - until epoch: 156, best_acc1: 71.222%
2022-03-04 06:23:36 - epoch 157 lr: 0.01204517304231343
2022-03-04 06:24:14 - train: epoch 0157, iter [00100, 05004], lr: 0.012045, loss: 1.3255
2022-03-04 06:24:48 - train: epoch 0157, iter [00200, 05004], lr: 0.012045, loss: 1.0906
2022-03-04 06:25:20 - train: epoch 0157, iter [00300, 05004], lr: 0.012045, loss: 1.1685
2022-03-04 06:25:54 - train: epoch 0157, iter [00400, 05004], lr: 0.012045, loss: 1.1878
2022-03-04 06:26:26 - train: epoch 0157, iter [00500, 05004], lr: 0.012045, loss: 1.1321
2022-03-04 06:27:00 - train: epoch 0157, iter [00600, 05004], lr: 0.012045, loss: 1.2631
2022-03-04 06:27:33 - train: epoch 0157, iter [00700, 05004], lr: 0.012045, loss: 1.2439
2022-03-04 06:28:07 - train: epoch 0157, iter [00800, 05004], lr: 0.012045, loss: 1.2754
2022-03-04 06:28:40 - train: epoch 0157, iter [00900, 05004], lr: 0.012045, loss: 1.2046
2022-03-04 06:29:12 - train: epoch 0157, iter [01000, 05004], lr: 0.012045, loss: 1.2263
2022-03-04 06:29:45 - train: epoch 0157, iter [01100, 05004], lr: 0.012045, loss: 1.1498
2022-03-04 06:30:19 - train: epoch 0157, iter [01200, 05004], lr: 0.012045, loss: 1.3578
2022-03-04 06:30:52 - train: epoch 0157, iter [01300, 05004], lr: 0.012045, loss: 1.3180
2022-03-04 06:31:25 - train: epoch 0157, iter [01400, 05004], lr: 0.012045, loss: 1.2057
2022-03-04 06:31:58 - train: epoch 0157, iter [01500, 05004], lr: 0.012045, loss: 1.3440
2022-03-04 06:32:32 - train: epoch 0157, iter [01600, 05004], lr: 0.012045, loss: 1.0736
2022-03-04 06:33:04 - train: epoch 0157, iter [01700, 05004], lr: 0.012045, loss: 1.0902
2022-03-04 06:33:37 - train: epoch 0157, iter [01800, 05004], lr: 0.012045, loss: 1.2392
2022-03-04 06:34:10 - train: epoch 0157, iter [01900, 05004], lr: 0.012045, loss: 1.3537
2022-03-04 06:34:44 - train: epoch 0157, iter [02000, 05004], lr: 0.012045, loss: 1.4841
2022-03-04 06:35:16 - train: epoch 0157, iter [02100, 05004], lr: 0.012045, loss: 1.3086
2022-03-04 06:35:50 - train: epoch 0157, iter [02200, 05004], lr: 0.012045, loss: 1.2514
2022-03-04 06:36:23 - train: epoch 0157, iter [02300, 05004], lr: 0.012045, loss: 1.1299
2022-03-04 06:36:56 - train: epoch 0157, iter [02400, 05004], lr: 0.012045, loss: 1.1162
2022-03-04 06:37:29 - train: epoch 0157, iter [02500, 05004], lr: 0.012045, loss: 1.1707
2022-03-04 06:38:02 - train: epoch 0157, iter [02600, 05004], lr: 0.012045, loss: 1.2199
2022-03-04 06:38:36 - train: epoch 0157, iter [02700, 05004], lr: 0.012045, loss: 1.3018
2022-03-04 06:39:09 - train: epoch 0157, iter [02800, 05004], lr: 0.012045, loss: 1.2199
2022-03-04 06:39:42 - train: epoch 0157, iter [02900, 05004], lr: 0.012045, loss: 1.0889
2022-03-04 06:40:15 - train: epoch 0157, iter [03000, 05004], lr: 0.012045, loss: 1.5008
2022-03-04 06:40:47 - train: epoch 0157, iter [03100, 05004], lr: 0.012045, loss: 1.2716
2022-03-04 06:41:21 - train: epoch 0157, iter [03200, 05004], lr: 0.012045, loss: 1.0407
2022-03-04 06:41:55 - train: epoch 0157, iter [03300, 05004], lr: 0.012045, loss: 1.4502
2022-03-04 06:42:28 - train: epoch 0157, iter [03400, 05004], lr: 0.012045, loss: 1.0746
2022-03-04 06:43:01 - train: epoch 0157, iter [03500, 05004], lr: 0.012045, loss: 1.4336
2022-03-04 06:43:34 - train: epoch 0157, iter [03600, 05004], lr: 0.012045, loss: 1.3167
2022-03-04 06:44:07 - train: epoch 0157, iter [03700, 05004], lr: 0.012045, loss: 1.0533
2022-03-04 06:44:40 - train: epoch 0157, iter [03800, 05004], lr: 0.012045, loss: 1.2343
2022-03-04 06:45:12 - train: epoch 0157, iter [03900, 05004], lr: 0.012045, loss: 1.3470
2022-03-04 06:45:46 - train: epoch 0157, iter [04000, 05004], lr: 0.012045, loss: 1.1778
2022-03-04 06:46:19 - train: epoch 0157, iter [04100, 05004], lr: 0.012045, loss: 1.1268
2022-03-04 06:46:53 - train: epoch 0157, iter [04200, 05004], lr: 0.012045, loss: 1.2328
2022-03-04 06:47:26 - train: epoch 0157, iter [04300, 05004], lr: 0.012045, loss: 1.3542
2022-03-04 06:47:59 - train: epoch 0157, iter [04400, 05004], lr: 0.012045, loss: 1.3116
2022-03-04 06:48:32 - train: epoch 0157, iter [04500, 05004], lr: 0.012045, loss: 1.2471
2022-03-04 06:49:05 - train: epoch 0157, iter [04600, 05004], lr: 0.012045, loss: 1.4316
2022-03-04 06:49:38 - train: epoch 0157, iter [04700, 05004], lr: 0.012045, loss: 1.2899
2022-03-04 06:50:12 - train: epoch 0157, iter [04800, 05004], lr: 0.012045, loss: 1.2635
2022-03-04 06:50:46 - train: epoch 0157, iter [04900, 05004], lr: 0.012045, loss: 1.3505
2022-03-04 06:51:18 - train: epoch 0157, iter [05000, 05004], lr: 0.012045, loss: 1.2501
2022-03-04 06:51:19 - train: epoch 157, train_loss: 1.2658
2022-03-04 06:52:32 - eval: epoch: 157, acc1: 71.686%, acc5: 90.822%, test_loss: 1.1300, per_image_load_time: 2.342ms, per_image_inference_time: 0.496ms
2022-03-04 06:52:33 - until epoch: 157, best_acc1: 71.686%
2022-03-04 06:52:33 - epoch 158 lr: 0.011525734670691701
2022-03-04 06:53:11 - train: epoch 0158, iter [00100, 05004], lr: 0.011526, loss: 1.3726
2022-03-04 06:53:44 - train: epoch 0158, iter [00200, 05004], lr: 0.011526, loss: 1.1480
2022-03-04 06:54:17 - train: epoch 0158, iter [00300, 05004], lr: 0.011526, loss: 1.1449
2022-03-04 06:54:50 - train: epoch 0158, iter [00400, 05004], lr: 0.011526, loss: 1.1135
2022-03-04 06:55:23 - train: epoch 0158, iter [00500, 05004], lr: 0.011526, loss: 1.1125
2022-03-04 06:55:56 - train: epoch 0158, iter [00600, 05004], lr: 0.011526, loss: 1.0914
2022-03-04 06:56:30 - train: epoch 0158, iter [00700, 05004], lr: 0.011526, loss: 1.1001
2022-03-04 06:57:03 - train: epoch 0158, iter [00800, 05004], lr: 0.011526, loss: 1.4298
2022-03-04 06:57:36 - train: epoch 0158, iter [00900, 05004], lr: 0.011526, loss: 1.3157
2022-03-04 06:58:09 - train: epoch 0158, iter [01000, 05004], lr: 0.011526, loss: 1.3213
2022-03-04 06:58:42 - train: epoch 0158, iter [01100, 05004], lr: 0.011526, loss: 1.0831
2022-03-04 06:59:15 - train: epoch 0158, iter [01200, 05004], lr: 0.011526, loss: 1.0753
2022-03-04 06:59:48 - train: epoch 0158, iter [01300, 05004], lr: 0.011526, loss: 1.3287
2022-03-04 07:00:21 - train: epoch 0158, iter [01400, 05004], lr: 0.011526, loss: 1.2149
2022-03-04 07:00:54 - train: epoch 0158, iter [01500, 05004], lr: 0.011526, loss: 1.2046
2022-03-04 07:01:27 - train: epoch 0158, iter [01600, 05004], lr: 0.011526, loss: 1.3274
2022-03-04 07:02:01 - train: epoch 0158, iter [01700, 05004], lr: 0.011526, loss: 1.2122
2022-03-04 07:02:34 - train: epoch 0158, iter [01800, 05004], lr: 0.011526, loss: 1.3525
2022-03-04 07:03:07 - train: epoch 0158, iter [01900, 05004], lr: 0.011526, loss: 1.3805
2022-03-04 07:03:39 - train: epoch 0158, iter [02000, 05004], lr: 0.011526, loss: 1.4669
2022-03-04 07:04:13 - train: epoch 0158, iter [02100, 05004], lr: 0.011526, loss: 1.2309
2022-03-04 07:04:46 - train: epoch 0158, iter [02200, 05004], lr: 0.011526, loss: 1.3311
2022-03-04 07:05:20 - train: epoch 0158, iter [02300, 05004], lr: 0.011526, loss: 1.2993
2022-03-04 07:05:53 - train: epoch 0158, iter [02400, 05004], lr: 0.011526, loss: 1.2656
2022-03-04 07:06:26 - train: epoch 0158, iter [02500, 05004], lr: 0.011526, loss: 1.2659
2022-03-04 07:07:00 - train: epoch 0158, iter [02600, 05004], lr: 0.011526, loss: 1.1845
2022-03-04 07:07:33 - train: epoch 0158, iter [02700, 05004], lr: 0.011526, loss: 1.1108
2022-03-04 07:08:06 - train: epoch 0158, iter [02800, 05004], lr: 0.011526, loss: 1.3410
2022-03-04 07:08:39 - train: epoch 0158, iter [02900, 05004], lr: 0.011526, loss: 1.2774
2022-03-04 07:09:12 - train: epoch 0158, iter [03000, 05004], lr: 0.011526, loss: 1.1712
2022-03-04 07:09:45 - train: epoch 0158, iter [03100, 05004], lr: 0.011526, loss: 1.2269
2022-03-04 07:10:19 - train: epoch 0158, iter [03200, 05004], lr: 0.011526, loss: 1.2019
2022-03-04 07:10:51 - train: epoch 0158, iter [03300, 05004], lr: 0.011526, loss: 1.0541
2022-03-04 07:11:24 - train: epoch 0158, iter [03400, 05004], lr: 0.011526, loss: 1.1121
2022-03-04 07:11:57 - train: epoch 0158, iter [03500, 05004], lr: 0.011526, loss: 1.2491
2022-03-04 07:12:30 - train: epoch 0158, iter [03600, 05004], lr: 0.011526, loss: 1.2301
2022-03-04 07:13:04 - train: epoch 0158, iter [03700, 05004], lr: 0.011526, loss: 1.3686
2022-03-04 07:13:37 - train: epoch 0158, iter [03800, 05004], lr: 0.011526, loss: 1.3864
2022-03-04 07:14:10 - train: epoch 0158, iter [03900, 05004], lr: 0.011526, loss: 1.3272
2022-03-04 07:14:44 - train: epoch 0158, iter [04000, 05004], lr: 0.011526, loss: 1.2319
2022-03-04 07:15:17 - train: epoch 0158, iter [04100, 05004], lr: 0.011526, loss: 1.2245
2022-03-04 07:15:49 - train: epoch 0158, iter [04200, 05004], lr: 0.011526, loss: 1.2876
2022-03-04 07:16:23 - train: epoch 0158, iter [04300, 05004], lr: 0.011526, loss: 1.5277
2022-03-04 07:16:56 - train: epoch 0158, iter [04400, 05004], lr: 0.011526, loss: 1.1524
2022-03-04 07:17:29 - train: epoch 0158, iter [04500, 05004], lr: 0.011526, loss: 1.2657
2022-03-04 07:18:03 - train: epoch 0158, iter [04600, 05004], lr: 0.011526, loss: 1.0073
2022-03-04 07:18:35 - train: epoch 0158, iter [04700, 05004], lr: 0.011526, loss: 1.1334
2022-03-04 07:19:09 - train: epoch 0158, iter [04800, 05004], lr: 0.011526, loss: 1.4747
2022-03-04 07:19:42 - train: epoch 0158, iter [04900, 05004], lr: 0.011526, loss: 1.1551
2022-03-04 07:20:13 - train: epoch 0158, iter [05000, 05004], lr: 0.011526, loss: 1.1834
2022-03-04 07:20:14 - train: epoch 158, train_loss: 1.2508
2022-03-04 07:21:29 - eval: epoch: 158, acc1: 71.676%, acc5: 90.704%, test_loss: 1.1359, per_image_load_time: 2.346ms, per_image_inference_time: 0.518ms
2022-03-04 07:21:29 - until epoch: 158, best_acc1: 71.686%
2022-03-04 07:21:29 - epoch 159 lr: 0.011016282296838887
2022-03-04 07:22:06 - train: epoch 0159, iter [00100, 05004], lr: 0.011016, loss: 1.1198
2022-03-04 07:22:40 - train: epoch 0159, iter [00200, 05004], lr: 0.011016, loss: 1.1631
2022-03-04 07:23:13 - train: epoch 0159, iter [00300, 05004], lr: 0.011016, loss: 1.4666
2022-03-04 07:23:46 - train: epoch 0159, iter [00400, 05004], lr: 0.011016, loss: 1.2822
2022-03-04 07:24:20 - train: epoch 0159, iter [00500, 05004], lr: 0.011016, loss: 1.3878
2022-03-04 07:24:52 - train: epoch 0159, iter [00600, 05004], lr: 0.011016, loss: 1.2074
2022-03-04 07:25:25 - train: epoch 0159, iter [00700, 05004], lr: 0.011016, loss: 1.3441
2022-03-04 07:25:58 - train: epoch 0159, iter [00800, 05004], lr: 0.011016, loss: 1.0416
2022-03-04 07:26:32 - train: epoch 0159, iter [00900, 05004], lr: 0.011016, loss: 1.0056
2022-03-04 07:27:05 - train: epoch 0159, iter [01000, 05004], lr: 0.011016, loss: 1.3176
2022-03-04 07:27:38 - train: epoch 0159, iter [01100, 05004], lr: 0.011016, loss: 1.2655
2022-03-04 07:28:11 - train: epoch 0159, iter [01200, 05004], lr: 0.011016, loss: 1.0734
2022-03-04 07:28:44 - train: epoch 0159, iter [01300, 05004], lr: 0.011016, loss: 1.4339
2022-03-04 07:29:17 - train: epoch 0159, iter [01400, 05004], lr: 0.011016, loss: 1.1827
2022-03-04 07:29:50 - train: epoch 0159, iter [01500, 05004], lr: 0.011016, loss: 1.3705
2022-03-04 07:30:24 - train: epoch 0159, iter [01600, 05004], lr: 0.011016, loss: 1.3188
2022-03-04 07:30:56 - train: epoch 0159, iter [01700, 05004], lr: 0.011016, loss: 1.2444
2022-03-04 07:31:30 - train: epoch 0159, iter [01800, 05004], lr: 0.011016, loss: 1.1351
2022-03-04 07:32:03 - train: epoch 0159, iter [01900, 05004], lr: 0.011016, loss: 1.0376
2022-03-04 07:32:37 - train: epoch 0159, iter [02000, 05004], lr: 0.011016, loss: 1.3611
2022-03-04 07:33:09 - train: epoch 0159, iter [02100, 05004], lr: 0.011016, loss: 1.0500
2022-03-04 07:33:42 - train: epoch 0159, iter [02200, 05004], lr: 0.011016, loss: 1.4249
2022-03-04 07:34:15 - train: epoch 0159, iter [02300, 05004], lr: 0.011016, loss: 1.0606
2022-03-04 07:34:48 - train: epoch 0159, iter [02400, 05004], lr: 0.011016, loss: 1.0839
2022-03-04 07:35:21 - train: epoch 0159, iter [02500, 05004], lr: 0.011016, loss: 1.0266
2022-03-04 07:35:54 - train: epoch 0159, iter [02600, 05004], lr: 0.011016, loss: 1.2236
2022-03-04 07:36:27 - train: epoch 0159, iter [02700, 05004], lr: 0.011016, loss: 1.1061
2022-03-04 07:37:01 - train: epoch 0159, iter [02800, 05004], lr: 0.011016, loss: 1.4553
2022-03-04 07:37:34 - train: epoch 0159, iter [02900, 05004], lr: 0.011016, loss: 1.3100
2022-03-04 07:38:07 - train: epoch 0159, iter [03000, 05004], lr: 0.011016, loss: 1.2677
2022-03-04 07:38:40 - train: epoch 0159, iter [03100, 05004], lr: 0.011016, loss: 1.1656
2022-03-04 07:39:13 - train: epoch 0159, iter [03200, 05004], lr: 0.011016, loss: 1.2672
2022-03-04 07:39:46 - train: epoch 0159, iter [03300, 05004], lr: 0.011016, loss: 1.3141
2022-03-04 07:40:20 - train: epoch 0159, iter [03400, 05004], lr: 0.011016, loss: 1.2777
2022-03-04 07:40:53 - train: epoch 0159, iter [03500, 05004], lr: 0.011016, loss: 1.2048
2022-03-04 07:41:26 - train: epoch 0159, iter [03600, 05004], lr: 0.011016, loss: 1.1631
2022-03-04 07:41:59 - train: epoch 0159, iter [03700, 05004], lr: 0.011016, loss: 1.3393
2022-03-04 07:42:32 - train: epoch 0159, iter [03800, 05004], lr: 0.011016, loss: 1.2301
2022-03-04 07:43:05 - train: epoch 0159, iter [03900, 05004], lr: 0.011016, loss: 1.3020
2022-03-04 07:43:38 - train: epoch 0159, iter [04000, 05004], lr: 0.011016, loss: 1.4303
2022-03-04 07:44:12 - train: epoch 0159, iter [04100, 05004], lr: 0.011016, loss: 1.2717
2022-03-04 07:44:45 - train: epoch 0159, iter [04200, 05004], lr: 0.011016, loss: 1.2777
2022-03-04 07:45:18 - train: epoch 0159, iter [04300, 05004], lr: 0.011016, loss: 1.2848
2022-03-04 07:45:50 - train: epoch 0159, iter [04400, 05004], lr: 0.011016, loss: 1.1699
2022-03-04 07:46:24 - train: epoch 0159, iter [04500, 05004], lr: 0.011016, loss: 1.4696
2022-03-04 07:46:57 - train: epoch 0159, iter [04600, 05004], lr: 0.011016, loss: 1.2735
2022-03-04 07:47:30 - train: epoch 0159, iter [04700, 05004], lr: 0.011016, loss: 1.0987
2022-03-04 07:48:04 - train: epoch 0159, iter [04800, 05004], lr: 0.011016, loss: 1.2942
2022-03-04 07:48:36 - train: epoch 0159, iter [04900, 05004], lr: 0.011016, loss: 1.2835
2022-03-04 07:49:08 - train: epoch 0159, iter [05000, 05004], lr: 0.011016, loss: 1.1275
2022-03-04 07:49:09 - train: epoch 159, train_loss: 1.2412
2022-03-04 07:50:24 - eval: epoch: 159, acc1: 71.058%, acc5: 90.228%, test_loss: 1.1626, per_image_load_time: 2.378ms, per_image_inference_time: 0.509ms
2022-03-04 07:50:24 - until epoch: 159, best_acc1: 71.686%
2022-03-04 07:50:24 - epoch 160 lr: 0.010516948149147755
2022-03-04 07:51:02 - train: epoch 0160, iter [00100, 05004], lr: 0.010517, loss: 1.1734
2022-03-04 07:51:35 - train: epoch 0160, iter [00200, 05004], lr: 0.010517, loss: 1.1439
2022-03-04 07:52:08 - train: epoch 0160, iter [00300, 05004], lr: 0.010517, loss: 1.3097
2022-03-04 07:52:41 - train: epoch 0160, iter [00400, 05004], lr: 0.010517, loss: 1.0270
2022-03-04 07:53:15 - train: epoch 0160, iter [00500, 05004], lr: 0.010517, loss: 1.3955
2022-03-04 07:53:48 - train: epoch 0160, iter [00600, 05004], lr: 0.010517, loss: 1.2702
2022-03-04 07:54:21 - train: epoch 0160, iter [00700, 05004], lr: 0.010517, loss: 1.2101
2022-03-04 07:54:55 - train: epoch 0160, iter [00800, 05004], lr: 0.010517, loss: 1.1604
2022-03-04 07:55:28 - train: epoch 0160, iter [00900, 05004], lr: 0.010517, loss: 1.3208
2022-03-04 07:56:00 - train: epoch 0160, iter [01000, 05004], lr: 0.010517, loss: 1.2252
2022-03-04 07:56:33 - train: epoch 0160, iter [01100, 05004], lr: 0.010517, loss: 1.4250
2022-03-04 07:57:06 - train: epoch 0160, iter [01200, 05004], lr: 0.010517, loss: 1.2670
2022-03-04 07:57:40 - train: epoch 0160, iter [01300, 05004], lr: 0.010517, loss: 1.3547
2022-03-04 07:58:13 - train: epoch 0160, iter [01400, 05004], lr: 0.010517, loss: 1.2186
2022-03-04 07:58:47 - train: epoch 0160, iter [01500, 05004], lr: 0.010517, loss: 1.0725
2022-03-04 07:59:19 - train: epoch 0160, iter [01600, 05004], lr: 0.010517, loss: 1.2451
2022-03-04 07:59:53 - train: epoch 0160, iter [01700, 05004], lr: 0.010517, loss: 1.1387
2022-03-04 08:00:25 - train: epoch 0160, iter [01800, 05004], lr: 0.010517, loss: 1.3321
2022-03-04 08:01:00 - train: epoch 0160, iter [01900, 05004], lr: 0.010517, loss: 1.2088
2022-03-04 08:01:31 - train: epoch 0160, iter [02000, 05004], lr: 0.010517, loss: 1.0352
2022-03-04 08:02:04 - train: epoch 0160, iter [02100, 05004], lr: 0.010517, loss: 1.4639
2022-03-04 08:02:38 - train: epoch 0160, iter [02200, 05004], lr: 0.010517, loss: 1.2269
2022-03-04 08:03:10 - train: epoch 0160, iter [02300, 05004], lr: 0.010517, loss: 1.1373
2022-03-04 08:03:44 - train: epoch 0160, iter [02400, 05004], lr: 0.010517, loss: 1.1278
2022-03-04 08:04:18 - train: epoch 0160, iter [02500, 05004], lr: 0.010517, loss: 1.1021
2022-03-04 08:04:51 - train: epoch 0160, iter [02600, 05004], lr: 0.010517, loss: 0.9397
2022-03-04 08:05:24 - train: epoch 0160, iter [02700, 05004], lr: 0.010517, loss: 1.0766
2022-03-04 08:05:57 - train: epoch 0160, iter [02800, 05004], lr: 0.010517, loss: 1.1913
2022-03-04 08:06:30 - train: epoch 0160, iter [02900, 05004], lr: 0.010517, loss: 1.1912
2022-03-04 08:07:03 - train: epoch 0160, iter [03000, 05004], lr: 0.010517, loss: 1.2864
2022-03-04 08:07:35 - train: epoch 0160, iter [03100, 05004], lr: 0.010517, loss: 1.2621
2022-03-04 08:08:09 - train: epoch 0160, iter [03200, 05004], lr: 0.010517, loss: 1.1143
2022-03-04 08:08:42 - train: epoch 0160, iter [03300, 05004], lr: 0.010517, loss: 1.2177
2022-03-04 08:09:16 - train: epoch 0160, iter [03400, 05004], lr: 0.010517, loss: 1.3523
2022-03-04 08:09:48 - train: epoch 0160, iter [03500, 05004], lr: 0.010517, loss: 1.1875
2022-03-04 08:10:21 - train: epoch 0160, iter [03600, 05004], lr: 0.010517, loss: 1.1866
2022-03-04 08:10:54 - train: epoch 0160, iter [03700, 05004], lr: 0.010517, loss: 1.2532
2022-03-04 08:11:27 - train: epoch 0160, iter [03800, 05004], lr: 0.010517, loss: 1.3941
2022-03-04 08:12:02 - train: epoch 0160, iter [03900, 05004], lr: 0.010517, loss: 1.3917
2022-03-04 08:12:33 - train: epoch 0160, iter [04000, 05004], lr: 0.010517, loss: 1.2812
2022-03-04 08:13:07 - train: epoch 0160, iter [04100, 05004], lr: 0.010517, loss: 1.3071
2022-03-04 08:13:41 - train: epoch 0160, iter [04200, 05004], lr: 0.010517, loss: 1.4219
2022-03-04 08:14:14 - train: epoch 0160, iter [04300, 05004], lr: 0.010517, loss: 0.9984
2022-03-04 08:14:47 - train: epoch 0160, iter [04400, 05004], lr: 0.010517, loss: 1.2807
2022-03-04 08:15:20 - train: epoch 0160, iter [04500, 05004], lr: 0.010517, loss: 1.3431
2022-03-04 08:15:53 - train: epoch 0160, iter [04600, 05004], lr: 0.010517, loss: 1.2067
2022-03-04 08:16:27 - train: epoch 0160, iter [04700, 05004], lr: 0.010517, loss: 1.2287
2022-03-04 08:17:00 - train: epoch 0160, iter [04800, 05004], lr: 0.010517, loss: 1.2505
2022-03-04 08:17:33 - train: epoch 0160, iter [04900, 05004], lr: 0.010517, loss: 1.1524
2022-03-04 08:18:05 - train: epoch 0160, iter [05000, 05004], lr: 0.010517, loss: 1.0916
2022-03-04 08:18:06 - train: epoch 160, train_loss: 1.2245
2022-03-04 08:19:21 - eval: epoch: 160, acc1: 72.234%, acc5: 90.994%, test_loss: 1.1155, per_image_load_time: 2.339ms, per_image_inference_time: 0.533ms
2022-03-04 08:19:21 - until epoch: 160, best_acc1: 72.234%
2022-03-04 08:19:21 - epoch 161 lr: 0.010027861829824953
2022-03-04 08:19:59 - train: epoch 0161, iter [00100, 05004], lr: 0.010028, loss: 1.2433
2022-03-04 08:20:31 - train: epoch 0161, iter [00200, 05004], lr: 0.010028, loss: 1.1374
2022-03-04 08:21:04 - train: epoch 0161, iter [00300, 05004], lr: 0.010028, loss: 1.0464
2022-03-04 08:21:38 - train: epoch 0161, iter [00400, 05004], lr: 0.010028, loss: 1.2057
2022-03-04 08:22:10 - train: epoch 0161, iter [00500, 05004], lr: 0.010028, loss: 1.2675
2022-03-04 08:22:43 - train: epoch 0161, iter [00600, 05004], lr: 0.010028, loss: 1.1343
2022-03-04 08:23:16 - train: epoch 0161, iter [00700, 05004], lr: 0.010028, loss: 1.1803
2022-03-04 08:23:49 - train: epoch 0161, iter [00800, 05004], lr: 0.010028, loss: 1.1490
2022-03-04 08:24:22 - train: epoch 0161, iter [00900, 05004], lr: 0.010028, loss: 1.0052
2022-03-04 08:24:56 - train: epoch 0161, iter [01000, 05004], lr: 0.010028, loss: 1.2743
2022-03-04 08:25:29 - train: epoch 0161, iter [01100, 05004], lr: 0.010028, loss: 1.1451
2022-03-04 08:26:02 - train: epoch 0161, iter [01200, 05004], lr: 0.010028, loss: 1.1080
2022-03-04 08:26:35 - train: epoch 0161, iter [01300, 05004], lr: 0.010028, loss: 1.1402
2022-03-04 08:27:09 - train: epoch 0161, iter [01400, 05004], lr: 0.010028, loss: 1.2842
2022-03-04 08:27:42 - train: epoch 0161, iter [01500, 05004], lr: 0.010028, loss: 1.1336
2022-03-04 08:28:15 - train: epoch 0161, iter [01600, 05004], lr: 0.010028, loss: 1.1482
2022-03-04 08:28:48 - train: epoch 0161, iter [01700, 05004], lr: 0.010028, loss: 1.1592
2022-03-04 08:29:21 - train: epoch 0161, iter [01800, 05004], lr: 0.010028, loss: 1.2186
2022-03-04 08:29:54 - train: epoch 0161, iter [01900, 05004], lr: 0.010028, loss: 1.1449
2022-03-04 08:30:27 - train: epoch 0161, iter [02000, 05004], lr: 0.010028, loss: 1.3030
2022-03-04 08:31:00 - train: epoch 0161, iter [02100, 05004], lr: 0.010028, loss: 1.2684
2022-03-04 08:31:34 - train: epoch 0161, iter [02200, 05004], lr: 0.010028, loss: 1.1167
2022-03-04 08:32:06 - train: epoch 0161, iter [02300, 05004], lr: 0.010028, loss: 1.1300
2022-03-04 08:32:39 - train: epoch 0161, iter [02400, 05004], lr: 0.010028, loss: 1.2745
2022-03-04 08:33:13 - train: epoch 0161, iter [02500, 05004], lr: 0.010028, loss: 1.3805
2022-03-04 08:33:47 - train: epoch 0161, iter [02600, 05004], lr: 0.010028, loss: 1.1990
2022-03-04 08:34:19 - train: epoch 0161, iter [02700, 05004], lr: 0.010028, loss: 1.1913
2022-03-04 08:34:52 - train: epoch 0161, iter [02800, 05004], lr: 0.010028, loss: 1.1365
2022-03-04 08:35:25 - train: epoch 0161, iter [02900, 05004], lr: 0.010028, loss: 1.2885
2022-03-04 08:35:59 - train: epoch 0161, iter [03000, 05004], lr: 0.010028, loss: 1.0054
2022-03-04 08:36:32 - train: epoch 0161, iter [03100, 05004], lr: 0.010028, loss: 1.2191
2022-03-04 08:37:05 - train: epoch 0161, iter [03200, 05004], lr: 0.010028, loss: 1.2731
2022-03-04 08:37:38 - train: epoch 0161, iter [03300, 05004], lr: 0.010028, loss: 1.0536
2022-03-04 08:38:11 - train: epoch 0161, iter [03400, 05004], lr: 0.010028, loss: 1.1076
2022-03-04 08:38:45 - train: epoch 0161, iter [03500, 05004], lr: 0.010028, loss: 1.3042
2022-03-04 08:39:18 - train: epoch 0161, iter [03600, 05004], lr: 0.010028, loss: 1.3232
2022-03-04 08:39:51 - train: epoch 0161, iter [03700, 05004], lr: 0.010028, loss: 1.2184
2022-03-04 08:40:24 - train: epoch 0161, iter [03800, 05004], lr: 0.010028, loss: 1.2911
2022-03-04 08:40:57 - train: epoch 0161, iter [03900, 05004], lr: 0.010028, loss: 1.1543
2022-03-04 08:41:30 - train: epoch 0161, iter [04000, 05004], lr: 0.010028, loss: 1.3967
2022-03-04 08:42:04 - train: epoch 0161, iter [04100, 05004], lr: 0.010028, loss: 1.1739
2022-03-04 08:42:37 - train: epoch 0161, iter [04200, 05004], lr: 0.010028, loss: 1.3279
2022-03-04 08:43:10 - train: epoch 0161, iter [04300, 05004], lr: 0.010028, loss: 1.2283
2022-03-04 08:43:43 - train: epoch 0161, iter [04400, 05004], lr: 0.010028, loss: 1.2948
2022-03-04 08:44:16 - train: epoch 0161, iter [04500, 05004], lr: 0.010028, loss: 1.0953
2022-03-04 08:44:49 - train: epoch 0161, iter [04600, 05004], lr: 0.010028, loss: 1.1466
2022-03-04 08:45:23 - train: epoch 0161, iter [04700, 05004], lr: 0.010028, loss: 1.2337
2022-03-04 08:45:56 - train: epoch 0161, iter [04800, 05004], lr: 0.010028, loss: 1.2963
2022-03-04 08:46:29 - train: epoch 0161, iter [04900, 05004], lr: 0.010028, loss: 1.0783
2022-03-04 08:47:01 - train: epoch 0161, iter [05000, 05004], lr: 0.010028, loss: 1.2081
2022-03-04 08:47:02 - train: epoch 161, train_loss: 1.2126
2022-03-04 08:48:16 - eval: epoch: 161, acc1: 72.276%, acc5: 91.104%, test_loss: 1.1040, per_image_load_time: 2.301ms, per_image_inference_time: 0.519ms
2022-03-04 08:48:17 - until epoch: 161, best_acc1: 72.276%
2022-03-04 08:48:17 - epoch 162 lr: 0.009549150281252633
2022-03-04 08:48:54 - train: epoch 0162, iter [00100, 05004], lr: 0.009549, loss: 1.1015
2022-03-04 08:49:27 - train: epoch 0162, iter [00200, 05004], lr: 0.009549, loss: 1.0633
2022-03-04 08:50:00 - train: epoch 0162, iter [00300, 05004], lr: 0.009549, loss: 1.3884
2022-03-04 08:50:34 - train: epoch 0162, iter [00400, 05004], lr: 0.009549, loss: 1.2512
2022-03-04 08:51:07 - train: epoch 0162, iter [00500, 05004], lr: 0.009549, loss: 1.3198
2022-03-04 08:51:40 - train: epoch 0162, iter [00600, 05004], lr: 0.009549, loss: 1.0824
2022-03-04 08:52:12 - train: epoch 0162, iter [00700, 05004], lr: 0.009549, loss: 1.1688
2022-03-04 08:52:46 - train: epoch 0162, iter [00800, 05004], lr: 0.009549, loss: 1.0206
2022-03-04 08:53:19 - train: epoch 0162, iter [00900, 05004], lr: 0.009549, loss: 1.3505
2022-03-04 08:53:52 - train: epoch 0162, iter [01000, 05004], lr: 0.009549, loss: 1.1622
2022-03-04 08:54:25 - train: epoch 0162, iter [01100, 05004], lr: 0.009549, loss: 1.3059
2022-03-04 08:54:57 - train: epoch 0162, iter [01200, 05004], lr: 0.009549, loss: 1.2791
2022-03-04 08:55:31 - train: epoch 0162, iter [01300, 05004], lr: 0.009549, loss: 1.2870
2022-03-04 08:56:04 - train: epoch 0162, iter [01400, 05004], lr: 0.009549, loss: 1.1538
2022-03-04 08:56:37 - train: epoch 0162, iter [01500, 05004], lr: 0.009549, loss: 1.1635
2022-03-04 08:57:10 - train: epoch 0162, iter [01600, 05004], lr: 0.009549, loss: 1.2138
2022-03-04 08:57:43 - train: epoch 0162, iter [01700, 05004], lr: 0.009549, loss: 1.1599
2022-03-04 08:58:16 - train: epoch 0162, iter [01800, 05004], lr: 0.009549, loss: 1.1512
2022-03-04 08:58:49 - train: epoch 0162, iter [01900, 05004], lr: 0.009549, loss: 1.3558
2022-03-04 08:59:24 - train: epoch 0162, iter [02000, 05004], lr: 0.009549, loss: 1.2312
2022-03-04 08:59:57 - train: epoch 0162, iter [02100, 05004], lr: 0.009549, loss: 1.1599
2022-03-04 09:00:29 - train: epoch 0162, iter [02200, 05004], lr: 0.009549, loss: 1.0570
2022-03-04 09:01:03 - train: epoch 0162, iter [02300, 05004], lr: 0.009549, loss: 1.3834
2022-03-04 09:01:36 - train: epoch 0162, iter [02400, 05004], lr: 0.009549, loss: 1.1638
2022-03-04 09:02:09 - train: epoch 0162, iter [02500, 05004], lr: 0.009549, loss: 1.3948
2022-03-04 09:02:43 - train: epoch 0162, iter [02600, 05004], lr: 0.009549, loss: 1.2347
2022-03-04 09:03:16 - train: epoch 0162, iter [02700, 05004], lr: 0.009549, loss: 1.0501
2022-03-04 09:03:49 - train: epoch 0162, iter [02800, 05004], lr: 0.009549, loss: 1.2384
2022-03-04 09:04:22 - train: epoch 0162, iter [02900, 05004], lr: 0.009549, loss: 1.0499
2022-03-04 09:04:56 - train: epoch 0162, iter [03000, 05004], lr: 0.009549, loss: 1.0293
2022-03-04 09:05:29 - train: epoch 0162, iter [03100, 05004], lr: 0.009549, loss: 1.2246
2022-03-04 09:06:02 - train: epoch 0162, iter [03200, 05004], lr: 0.009549, loss: 1.0803
2022-03-04 09:06:36 - train: epoch 0162, iter [03300, 05004], lr: 0.009549, loss: 1.2383
2022-03-04 09:07:08 - train: epoch 0162, iter [03400, 05004], lr: 0.009549, loss: 1.1492
2022-03-04 09:07:42 - train: epoch 0162, iter [03500, 05004], lr: 0.009549, loss: 1.1834
2022-03-04 09:08:15 - train: epoch 0162, iter [03600, 05004], lr: 0.009549, loss: 1.3438
2022-03-04 09:08:48 - train: epoch 0162, iter [03700, 05004], lr: 0.009549, loss: 1.1220
2022-03-04 09:09:22 - train: epoch 0162, iter [03800, 05004], lr: 0.009549, loss: 1.3533
2022-03-04 09:09:55 - train: epoch 0162, iter [03900, 05004], lr: 0.009549, loss: 1.2224
2022-03-04 09:10:28 - train: epoch 0162, iter [04000, 05004], lr: 0.009549, loss: 0.9665
2022-03-04 09:11:00 - train: epoch 0162, iter [04100, 05004], lr: 0.009549, loss: 1.3031
2022-03-04 09:11:34 - train: epoch 0162, iter [04200, 05004], lr: 0.009549, loss: 1.3692
2022-03-04 09:12:07 - train: epoch 0162, iter [04300, 05004], lr: 0.009549, loss: 1.2966
2022-03-04 09:12:40 - train: epoch 0162, iter [04400, 05004], lr: 0.009549, loss: 1.1875
2022-03-04 09:13:13 - train: epoch 0162, iter [04500, 05004], lr: 0.009549, loss: 1.3112
2022-03-04 09:13:47 - train: epoch 0162, iter [04600, 05004], lr: 0.009549, loss: 1.1922
2022-03-04 09:14:20 - train: epoch 0162, iter [04700, 05004], lr: 0.009549, loss: 1.0574
2022-03-04 09:14:54 - train: epoch 0162, iter [04800, 05004], lr: 0.009549, loss: 1.3995
2022-03-04 09:15:27 - train: epoch 0162, iter [04900, 05004], lr: 0.009549, loss: 1.3552
2022-03-04 09:15:59 - train: epoch 0162, iter [05000, 05004], lr: 0.009549, loss: 1.3207
2022-03-04 09:16:00 - train: epoch 162, train_loss: 1.1966
2022-03-04 09:17:14 - eval: epoch: 162, acc1: 72.390%, acc5: 91.076%, test_loss: 1.1081, per_image_load_time: 2.339ms, per_image_inference_time: 0.497ms
2022-03-04 09:17:15 - until epoch: 162, best_acc1: 72.390%
2022-03-04 09:17:15 - epoch 163 lr: 0.009080937753040646
2022-03-04 09:17:53 - train: epoch 0163, iter [00100, 05004], lr: 0.009081, loss: 1.3002
2022-03-04 09:18:26 - train: epoch 0163, iter [00200, 05004], lr: 0.009081, loss: 1.1177
2022-03-04 09:18:59 - train: epoch 0163, iter [00300, 05004], lr: 0.009081, loss: 1.2323
2022-03-04 09:19:32 - train: epoch 0163, iter [00400, 05004], lr: 0.009081, loss: 1.1868
2022-03-04 09:20:05 - train: epoch 0163, iter [00500, 05004], lr: 0.009081, loss: 1.1606
2022-03-04 09:20:38 - train: epoch 0163, iter [00600, 05004], lr: 0.009081, loss: 1.2593
2022-03-04 09:21:12 - train: epoch 0163, iter [00700, 05004], lr: 0.009081, loss: 1.1740
2022-03-04 09:21:45 - train: epoch 0163, iter [00800, 05004], lr: 0.009081, loss: 0.9811
2022-03-04 09:22:18 - train: epoch 0163, iter [00900, 05004], lr: 0.009081, loss: 1.1070
2022-03-04 09:22:52 - train: epoch 0163, iter [01000, 05004], lr: 0.009081, loss: 1.2435
2022-03-04 09:23:25 - train: epoch 0163, iter [01100, 05004], lr: 0.009081, loss: 1.1922
2022-03-04 09:23:59 - train: epoch 0163, iter [01200, 05004], lr: 0.009081, loss: 1.1511
2022-03-04 09:24:32 - train: epoch 0163, iter [01300, 05004], lr: 0.009081, loss: 1.0108
2022-03-04 09:25:06 - train: epoch 0163, iter [01400, 05004], lr: 0.009081, loss: 1.1375
2022-03-04 09:25:39 - train: epoch 0163, iter [01500, 05004], lr: 0.009081, loss: 1.1646
2022-03-04 09:26:12 - train: epoch 0163, iter [01600, 05004], lr: 0.009081, loss: 1.2436
2022-03-04 09:26:46 - train: epoch 0163, iter [01700, 05004], lr: 0.009081, loss: 1.0583
2022-03-04 09:27:19 - train: epoch 0163, iter [01800, 05004], lr: 0.009081, loss: 1.2912
2022-03-04 09:27:53 - train: epoch 0163, iter [01900, 05004], lr: 0.009081, loss: 1.3511
2022-03-04 09:28:26 - train: epoch 0163, iter [02000, 05004], lr: 0.009081, loss: 0.9240
2022-03-04 09:29:00 - train: epoch 0163, iter [02100, 05004], lr: 0.009081, loss: 1.1618
2022-03-04 09:29:33 - train: epoch 0163, iter [02200, 05004], lr: 0.009081, loss: 1.1253
2022-03-04 09:30:06 - train: epoch 0163, iter [02300, 05004], lr: 0.009081, loss: 1.1702
2022-03-04 09:30:39 - train: epoch 0163, iter [02400, 05004], lr: 0.009081, loss: 1.3261
2022-03-04 09:31:13 - train: epoch 0163, iter [02500, 05004], lr: 0.009081, loss: 1.1930
2022-03-04 09:31:46 - train: epoch 0163, iter [02600, 05004], lr: 0.009081, loss: 1.0559
2022-03-04 09:32:19 - train: epoch 0163, iter [02700, 05004], lr: 0.009081, loss: 1.3191
2022-03-04 09:32:52 - train: epoch 0163, iter [02800, 05004], lr: 0.009081, loss: 1.2100
2022-03-04 09:33:26 - train: epoch 0163, iter [02900, 05004], lr: 0.009081, loss: 0.9792
2022-03-04 09:33:59 - train: epoch 0163, iter [03000, 05004], lr: 0.009081, loss: 1.4248
2022-03-04 09:34:33 - train: epoch 0163, iter [03100, 05004], lr: 0.009081, loss: 1.1349
2022-03-04 09:35:06 - train: epoch 0163, iter [03200, 05004], lr: 0.009081, loss: 1.1206
2022-03-04 09:35:39 - train: epoch 0163, iter [03300, 05004], lr: 0.009081, loss: 1.1735
2022-03-04 09:36:13 - train: epoch 0163, iter [03400, 05004], lr: 0.009081, loss: 1.2323
2022-03-04 09:36:46 - train: epoch 0163, iter [03500, 05004], lr: 0.009081, loss: 1.2705
2022-03-04 09:37:19 - train: epoch 0163, iter [03600, 05004], lr: 0.009081, loss: 1.0871
2022-03-04 09:37:53 - train: epoch 0163, iter [03700, 05004], lr: 0.009081, loss: 1.1915
2022-03-04 09:38:26 - train: epoch 0163, iter [03800, 05004], lr: 0.009081, loss: 1.2000
2022-03-04 09:38:59 - train: epoch 0163, iter [03900, 05004], lr: 0.009081, loss: 1.2459
2022-03-04 09:39:32 - train: epoch 0163, iter [04000, 05004], lr: 0.009081, loss: 1.1092
2022-03-04 09:40:07 - train: epoch 0163, iter [04100, 05004], lr: 0.009081, loss: 1.0594
2022-03-04 09:40:40 - train: epoch 0163, iter [04200, 05004], lr: 0.009081, loss: 1.2173
2022-03-04 09:41:13 - train: epoch 0163, iter [04300, 05004], lr: 0.009081, loss: 1.1604
2022-03-04 09:41:47 - train: epoch 0163, iter [04400, 05004], lr: 0.009081, loss: 1.1832
2022-03-04 09:42:19 - train: epoch 0163, iter [04500, 05004], lr: 0.009081, loss: 1.0359
2022-03-04 09:42:53 - train: epoch 0163, iter [04600, 05004], lr: 0.009081, loss: 1.1019
2022-03-04 09:43:26 - train: epoch 0163, iter [04700, 05004], lr: 0.009081, loss: 1.2512
2022-03-04 09:44:00 - train: epoch 0163, iter [04800, 05004], lr: 0.009081, loss: 1.2225
2022-03-04 09:44:33 - train: epoch 0163, iter [04900, 05004], lr: 0.009081, loss: 1.1825
2022-03-04 09:45:05 - train: epoch 0163, iter [05000, 05004], lr: 0.009081, loss: 1.0157
2022-03-04 09:45:06 - train: epoch 163, train_loss: 1.1841
2022-03-04 09:46:20 - eval: epoch: 163, acc1: 72.056%, acc5: 90.882%, test_loss: 1.1168, per_image_load_time: 2.352ms, per_image_inference_time: 0.508ms
2022-03-04 09:46:21 - until epoch: 163, best_acc1: 72.390%
2022-03-04 09:46:21 - epoch 164 lr: 0.008623345769777514
2022-03-04 09:46:59 - train: epoch 0164, iter [00100, 05004], lr: 0.008623, loss: 1.0581
2022-03-04 09:47:32 - train: epoch 0164, iter [00200, 05004], lr: 0.008623, loss: 1.0088
2022-03-04 09:48:05 - train: epoch 0164, iter [00300, 05004], lr: 0.008623, loss: 1.3855
2022-03-04 09:48:38 - train: epoch 0164, iter [00400, 05004], lr: 0.008623, loss: 0.9799
2022-03-04 09:49:12 - train: epoch 0164, iter [00500, 05004], lr: 0.008623, loss: 1.0829
2022-03-04 09:49:44 - train: epoch 0164, iter [00600, 05004], lr: 0.008623, loss: 1.1767
2022-03-04 09:50:18 - train: epoch 0164, iter [00700, 05004], lr: 0.008623, loss: 1.0947
2022-03-04 09:50:51 - train: epoch 0164, iter [00800, 05004], lr: 0.008623, loss: 1.0812
2022-03-04 09:51:25 - train: epoch 0164, iter [00900, 05004], lr: 0.008623, loss: 1.1492
2022-03-04 09:51:57 - train: epoch 0164, iter [01000, 05004], lr: 0.008623, loss: 1.1654
2022-03-04 09:52:30 - train: epoch 0164, iter [01100, 05004], lr: 0.008623, loss: 1.1086
2022-03-04 09:53:04 - train: epoch 0164, iter [01200, 05004], lr: 0.008623, loss: 0.9278
2022-03-04 09:53:38 - train: epoch 0164, iter [01300, 05004], lr: 0.008623, loss: 1.0248
2022-03-04 09:54:11 - train: epoch 0164, iter [01400, 05004], lr: 0.008623, loss: 1.1575
2022-03-04 09:54:45 - train: epoch 0164, iter [01500, 05004], lr: 0.008623, loss: 1.3677
2022-03-04 09:55:17 - train: epoch 0164, iter [01600, 05004], lr: 0.008623, loss: 1.0882
2022-03-04 09:55:50 - train: epoch 0164, iter [01700, 05004], lr: 0.008623, loss: 1.2501
2022-03-04 09:56:24 - train: epoch 0164, iter [01800, 05004], lr: 0.008623, loss: 1.0667
2022-03-04 09:56:58 - train: epoch 0164, iter [01900, 05004], lr: 0.008623, loss: 1.0387
2022-03-04 09:57:31 - train: epoch 0164, iter [02000, 05004], lr: 0.008623, loss: 1.3189
2022-03-04 09:58:04 - train: epoch 0164, iter [02100, 05004], lr: 0.008623, loss: 1.2567
2022-03-04 09:58:38 - train: epoch 0164, iter [02200, 05004], lr: 0.008623, loss: 1.2054
2022-03-04 09:59:12 - train: epoch 0164, iter [02300, 05004], lr: 0.008623, loss: 1.1902
2022-03-04 09:59:45 - train: epoch 0164, iter [02400, 05004], lr: 0.008623, loss: 1.2446
2022-03-04 10:00:18 - train: epoch 0164, iter [02500, 05004], lr: 0.008623, loss: 1.2407
2022-03-04 10:00:51 - train: epoch 0164, iter [02600, 05004], lr: 0.008623, loss: 1.4173
2022-03-04 10:01:24 - train: epoch 0164, iter [02700, 05004], lr: 0.008623, loss: 1.2804
2022-03-04 10:01:58 - train: epoch 0164, iter [02800, 05004], lr: 0.008623, loss: 1.0490
2022-03-04 10:02:31 - train: epoch 0164, iter [02900, 05004], lr: 0.008623, loss: 0.9801
2022-03-04 10:03:05 - train: epoch 0164, iter [03000, 05004], lr: 0.008623, loss: 1.2375
2022-03-04 10:03:38 - train: epoch 0164, iter [03100, 05004], lr: 0.008623, loss: 1.2401
2022-03-04 10:04:12 - train: epoch 0164, iter [03200, 05004], lr: 0.008623, loss: 1.3375
2022-03-04 10:04:45 - train: epoch 0164, iter [03300, 05004], lr: 0.008623, loss: 1.2287
2022-03-04 10:05:19 - train: epoch 0164, iter [03400, 05004], lr: 0.008623, loss: 1.1564
2022-03-04 10:05:52 - train: epoch 0164, iter [03500, 05004], lr: 0.008623, loss: 1.1747
2022-03-04 10:06:25 - train: epoch 0164, iter [03600, 05004], lr: 0.008623, loss: 1.2188
2022-03-04 10:06:59 - train: epoch 0164, iter [03700, 05004], lr: 0.008623, loss: 1.2329
2022-03-04 10:07:33 - train: epoch 0164, iter [03800, 05004], lr: 0.008623, loss: 1.1225
2022-03-04 10:08:06 - train: epoch 0164, iter [03900, 05004], lr: 0.008623, loss: 1.0530
2022-03-04 10:08:39 - train: epoch 0164, iter [04000, 05004], lr: 0.008623, loss: 1.0949
2022-03-04 10:09:12 - train: epoch 0164, iter [04100, 05004], lr: 0.008623, loss: 1.1300
2022-03-04 10:09:46 - train: epoch 0164, iter [04200, 05004], lr: 0.008623, loss: 1.3474
2022-03-04 10:10:19 - train: epoch 0164, iter [04300, 05004], lr: 0.008623, loss: 1.1229
2022-03-04 10:10:53 - train: epoch 0164, iter [04400, 05004], lr: 0.008623, loss: 1.0973
2022-03-04 10:11:26 - train: epoch 0164, iter [04500, 05004], lr: 0.008623, loss: 1.3003
2022-03-04 10:12:00 - train: epoch 0164, iter [04600, 05004], lr: 0.008623, loss: 1.3184
2022-03-04 10:12:33 - train: epoch 0164, iter [04700, 05004], lr: 0.008623, loss: 1.1213
2022-03-04 10:13:07 - train: epoch 0164, iter [04800, 05004], lr: 0.008623, loss: 1.2722
2022-03-04 10:13:40 - train: epoch 0164, iter [04900, 05004], lr: 0.008623, loss: 1.1430
2022-03-04 10:14:13 - train: epoch 0164, iter [05000, 05004], lr: 0.008623, loss: 1.1042
2022-03-04 10:14:14 - train: epoch 164, train_loss: 1.1690
2022-03-04 10:15:28 - eval: epoch: 164, acc1: 72.794%, acc5: 91.250%, test_loss: 1.0832, per_image_load_time: 2.320ms, per_image_inference_time: 0.535ms
2022-03-04 10:15:29 - until epoch: 164, best_acc1: 72.794%
2022-03-04 10:15:29 - epoch 165 lr: 0.008176493099488664
2022-03-04 10:16:06 - train: epoch 0165, iter [00100, 05004], lr: 0.008176, loss: 1.0436
2022-03-04 10:16:40 - train: epoch 0165, iter [00200, 05004], lr: 0.008176, loss: 1.1827
2022-03-04 10:17:11 - train: epoch 0165, iter [00300, 05004], lr: 0.008176, loss: 1.3446
2022-03-04 10:17:44 - train: epoch 0165, iter [00400, 05004], lr: 0.008176, loss: 1.0874
2022-03-04 10:18:17 - train: epoch 0165, iter [00500, 05004], lr: 0.008176, loss: 1.0834
2022-03-04 10:18:51 - train: epoch 0165, iter [00600, 05004], lr: 0.008176, loss: 1.2176
2022-03-04 10:19:24 - train: epoch 0165, iter [00700, 05004], lr: 0.008176, loss: 1.3859
2022-03-04 10:19:57 - train: epoch 0165, iter [00800, 05004], lr: 0.008176, loss: 1.1440
2022-03-04 10:20:30 - train: epoch 0165, iter [00900, 05004], lr: 0.008176, loss: 1.1673
2022-03-04 10:21:03 - train: epoch 0165, iter [01000, 05004], lr: 0.008176, loss: 1.0285
2022-03-04 10:21:36 - train: epoch 0165, iter [01100, 05004], lr: 0.008176, loss: 1.0570
2022-03-04 10:22:09 - train: epoch 0165, iter [01200, 05004], lr: 0.008176, loss: 1.1487
2022-03-04 10:22:43 - train: epoch 0165, iter [01300, 05004], lr: 0.008176, loss: 1.1655
2022-03-04 10:23:16 - train: epoch 0165, iter [01400, 05004], lr: 0.008176, loss: 0.9751
2022-03-04 10:23:49 - train: epoch 0165, iter [01500, 05004], lr: 0.008176, loss: 1.1539
2022-03-04 10:24:22 - train: epoch 0165, iter [01600, 05004], lr: 0.008176, loss: 1.1094
2022-03-04 10:24:56 - train: epoch 0165, iter [01700, 05004], lr: 0.008176, loss: 1.2208
2022-03-04 10:25:28 - train: epoch 0165, iter [01800, 05004], lr: 0.008176, loss: 0.9602
2022-03-04 10:26:02 - train: epoch 0165, iter [01900, 05004], lr: 0.008176, loss: 1.0930
2022-03-04 10:26:35 - train: epoch 0165, iter [02000, 05004], lr: 0.008176, loss: 1.0388
2022-03-04 10:27:09 - train: epoch 0165, iter [02100, 05004], lr: 0.008176, loss: 1.0527
2022-03-04 10:27:42 - train: epoch 0165, iter [02200, 05004], lr: 0.008176, loss: 1.2846
2022-03-04 10:28:14 - train: epoch 0165, iter [02300, 05004], lr: 0.008176, loss: 1.1805
2022-03-04 10:28:49 - train: epoch 0165, iter [02400, 05004], lr: 0.008176, loss: 1.3608
2022-03-04 10:29:21 - train: epoch 0165, iter [02500, 05004], lr: 0.008176, loss: 1.2010
2022-03-04 10:29:55 - train: epoch 0165, iter [02600, 05004], lr: 0.008176, loss: 1.1885
2022-03-04 10:30:28 - train: epoch 0165, iter [02700, 05004], lr: 0.008176, loss: 1.2906
2022-03-04 10:31:02 - train: epoch 0165, iter [02800, 05004], lr: 0.008176, loss: 1.0994
2022-03-04 10:31:35 - train: epoch 0165, iter [02900, 05004], lr: 0.008176, loss: 1.3623
2022-03-04 10:32:09 - train: epoch 0165, iter [03000, 05004], lr: 0.008176, loss: 1.2263
2022-03-04 10:32:42 - train: epoch 0165, iter [03100, 05004], lr: 0.008176, loss: 1.1720
2022-03-04 10:33:15 - train: epoch 0165, iter [03200, 05004], lr: 0.008176, loss: 1.1397
2022-03-04 10:33:49 - train: epoch 0165, iter [03300, 05004], lr: 0.008176, loss: 1.1599
2022-03-04 10:34:22 - train: epoch 0165, iter [03400, 05004], lr: 0.008176, loss: 1.2447
2022-03-04 10:34:56 - train: epoch 0165, iter [03500, 05004], lr: 0.008176, loss: 1.1764
2022-03-04 10:35:28 - train: epoch 0165, iter [03600, 05004], lr: 0.008176, loss: 1.4319
2022-03-04 10:36:02 - train: epoch 0165, iter [03700, 05004], lr: 0.008176, loss: 1.0922
2022-03-04 10:36:35 - train: epoch 0165, iter [03800, 05004], lr: 0.008176, loss: 1.1638
2022-03-04 10:37:08 - train: epoch 0165, iter [03900, 05004], lr: 0.008176, loss: 1.0503
2022-03-04 10:37:41 - train: epoch 0165, iter [04000, 05004], lr: 0.008176, loss: 1.1513
2022-03-04 10:38:14 - train: epoch 0165, iter [04100, 05004], lr: 0.008176, loss: 1.0270
2022-03-04 10:38:47 - train: epoch 0165, iter [04200, 05004], lr: 0.008176, loss: 1.2925
2022-03-04 10:39:20 - train: epoch 0165, iter [04300, 05004], lr: 0.008176, loss: 1.1663
2022-03-04 10:39:54 - train: epoch 0165, iter [04400, 05004], lr: 0.008176, loss: 1.2088
2022-03-04 10:40:27 - train: epoch 0165, iter [04500, 05004], lr: 0.008176, loss: 1.0109
2022-03-04 10:40:59 - train: epoch 0165, iter [04600, 05004], lr: 0.008176, loss: 1.2203
2022-03-04 10:41:33 - train: epoch 0165, iter [04700, 05004], lr: 0.008176, loss: 1.2827
2022-03-04 10:42:06 - train: epoch 0165, iter [04800, 05004], lr: 0.008176, loss: 1.0703
2022-03-04 10:42:39 - train: epoch 0165, iter [04900, 05004], lr: 0.008176, loss: 0.9853
2022-03-04 10:43:12 - train: epoch 0165, iter [05000, 05004], lr: 0.008176, loss: 0.8718
2022-03-04 10:43:13 - train: epoch 165, train_loss: 1.1539
2022-03-04 10:44:27 - eval: epoch: 165, acc1: 72.538%, acc5: 91.146%, test_loss: 1.0999, per_image_load_time: 2.322ms, per_image_inference_time: 0.522ms
2022-03-04 10:44:27 - until epoch: 165, best_acc1: 72.794%
2022-03-04 10:44:27 - epoch 166 lr: 0.00774049572281027
2022-03-04 10:45:05 - train: epoch 0166, iter [00100, 05004], lr: 0.007740, loss: 0.9716
2022-03-04 10:45:38 - train: epoch 0166, iter [00200, 05004], lr: 0.007740, loss: 1.1083
2022-03-04 10:46:12 - train: epoch 0166, iter [00300, 05004], lr: 0.007740, loss: 1.3011
2022-03-04 10:46:44 - train: epoch 0166, iter [00400, 05004], lr: 0.007740, loss: 1.2646
2022-03-04 10:47:18 - train: epoch 0166, iter [00500, 05004], lr: 0.007740, loss: 1.1323
2022-03-04 10:47:51 - train: epoch 0166, iter [00600, 05004], lr: 0.007740, loss: 1.0659
2022-03-04 10:48:24 - train: epoch 0166, iter [00700, 05004], lr: 0.007740, loss: 1.0996
2022-03-04 10:48:57 - train: epoch 0166, iter [00800, 05004], lr: 0.007740, loss: 1.2925
2022-03-04 10:49:30 - train: epoch 0166, iter [00900, 05004], lr: 0.007740, loss: 1.1144
2022-03-04 10:50:04 - train: epoch 0166, iter [01000, 05004], lr: 0.007740, loss: 1.1062
2022-03-04 10:50:36 - train: epoch 0166, iter [01100, 05004], lr: 0.007740, loss: 1.1100
2022-03-04 10:51:10 - train: epoch 0166, iter [01200, 05004], lr: 0.007740, loss: 1.1593
2022-03-04 10:51:43 - train: epoch 0166, iter [01300, 05004], lr: 0.007740, loss: 1.0669
2022-03-04 10:52:16 - train: epoch 0166, iter [01400, 05004], lr: 0.007740, loss: 1.0814
2022-03-04 10:52:49 - train: epoch 0166, iter [01500, 05004], lr: 0.007740, loss: 1.1496
2022-03-04 10:53:23 - train: epoch 0166, iter [01600, 05004], lr: 0.007740, loss: 1.2987
2022-03-04 10:53:56 - train: epoch 0166, iter [01700, 05004], lr: 0.007740, loss: 1.2631
2022-03-04 10:54:29 - train: epoch 0166, iter [01800, 05004], lr: 0.007740, loss: 1.0707
2022-03-04 10:55:02 - train: epoch 0166, iter [01900, 05004], lr: 0.007740, loss: 1.1633
2022-03-04 10:55:36 - train: epoch 0166, iter [02000, 05004], lr: 0.007740, loss: 1.1821
2022-03-04 10:56:09 - train: epoch 0166, iter [02100, 05004], lr: 0.007740, loss: 0.9913
2022-03-04 10:56:42 - train: epoch 0166, iter [02200, 05004], lr: 0.007740, loss: 1.1322
2022-03-04 10:57:16 - train: epoch 0166, iter [02300, 05004], lr: 0.007740, loss: 1.1601
2022-03-04 10:57:48 - train: epoch 0166, iter [02400, 05004], lr: 0.007740, loss: 1.2505
2022-03-04 10:58:22 - train: epoch 0166, iter [02500, 05004], lr: 0.007740, loss: 1.1513
2022-03-04 10:58:55 - train: epoch 0166, iter [02600, 05004], lr: 0.007740, loss: 1.0476
2022-03-04 10:59:28 - train: epoch 0166, iter [02700, 05004], lr: 0.007740, loss: 1.1345
2022-03-04 11:00:01 - train: epoch 0166, iter [02800, 05004], lr: 0.007740, loss: 1.1677
2022-03-04 11:00:34 - train: epoch 0166, iter [02900, 05004], lr: 0.007740, loss: 1.2863
2022-03-04 11:01:08 - train: epoch 0166, iter [03000, 05004], lr: 0.007740, loss: 1.2914
2022-03-04 11:01:41 - train: epoch 0166, iter [03100, 05004], lr: 0.007740, loss: 1.3658
2022-03-04 11:02:14 - train: epoch 0166, iter [03200, 05004], lr: 0.007740, loss: 1.0513
2022-03-04 11:02:47 - train: epoch 0166, iter [03300, 05004], lr: 0.007740, loss: 1.0863
2022-03-04 11:03:21 - train: epoch 0166, iter [03400, 05004], lr: 0.007740, loss: 1.0960
2022-03-04 11:03:53 - train: epoch 0166, iter [03500, 05004], lr: 0.007740, loss: 1.2920
2022-03-04 11:04:26 - train: epoch 0166, iter [03600, 05004], lr: 0.007740, loss: 1.0569
2022-03-04 11:05:00 - train: epoch 0166, iter [03700, 05004], lr: 0.007740, loss: 0.9539
2022-03-04 11:05:33 - train: epoch 0166, iter [03800, 05004], lr: 0.007740, loss: 1.1872
2022-03-04 11:06:06 - train: epoch 0166, iter [03900, 05004], lr: 0.007740, loss: 1.0407
2022-03-04 11:06:40 - train: epoch 0166, iter [04000, 05004], lr: 0.007740, loss: 1.2178
2022-03-04 11:07:13 - train: epoch 0166, iter [04100, 05004], lr: 0.007740, loss: 1.0106
2022-03-04 11:07:47 - train: epoch 0166, iter [04200, 05004], lr: 0.007740, loss: 1.0845
2022-03-04 11:08:19 - train: epoch 0166, iter [04300, 05004], lr: 0.007740, loss: 1.2668
2022-03-04 11:08:53 - train: epoch 0166, iter [04400, 05004], lr: 0.007740, loss: 0.9679
2022-03-04 11:09:26 - train: epoch 0166, iter [04500, 05004], lr: 0.007740, loss: 1.1640
2022-03-04 11:09:59 - train: epoch 0166, iter [04600, 05004], lr: 0.007740, loss: 1.1684
2022-03-04 11:10:32 - train: epoch 0166, iter [04700, 05004], lr: 0.007740, loss: 1.1854
2022-03-04 11:11:05 - train: epoch 0166, iter [04800, 05004], lr: 0.007740, loss: 0.9970
2022-03-04 11:11:38 - train: epoch 0166, iter [04900, 05004], lr: 0.007740, loss: 1.0439
2022-03-04 11:12:11 - train: epoch 0166, iter [05000, 05004], lr: 0.007740, loss: 1.5249
2022-03-04 11:12:12 - train: epoch 166, train_loss: 1.1414
2022-03-04 11:13:25 - eval: epoch: 166, acc1: 73.450%, acc5: 91.666%, test_loss: 1.0570, per_image_load_time: 2.334ms, per_image_inference_time: 0.512ms
2022-03-04 11:13:26 - until epoch: 166, best_acc1: 73.450%
2022-03-04 11:13:26 - epoch 167 lr: 0.0073154668028864
2022-03-04 11:14:03 - train: epoch 0167, iter [00100, 05004], lr: 0.007315, loss: 1.2830
2022-03-04 11:14:35 - train: epoch 0167, iter [00200, 05004], lr: 0.007315, loss: 1.0391
2022-03-04 11:15:09 - train: epoch 0167, iter [00300, 05004], lr: 0.007315, loss: 1.1111
2022-03-04 11:15:43 - train: epoch 0167, iter [00400, 05004], lr: 0.007315, loss: 1.0003
2022-03-04 11:16:15 - train: epoch 0167, iter [00500, 05004], lr: 0.007315, loss: 1.1695
2022-03-04 11:16:48 - train: epoch 0167, iter [00600, 05004], lr: 0.007315, loss: 1.2121
2022-03-04 11:17:21 - train: epoch 0167, iter [00700, 05004], lr: 0.007315, loss: 1.0317
2022-03-04 11:17:54 - train: epoch 0167, iter [00800, 05004], lr: 0.007315, loss: 1.2795
2022-03-04 11:18:27 - train: epoch 0167, iter [00900, 05004], lr: 0.007315, loss: 1.0204
2022-03-04 11:19:00 - train: epoch 0167, iter [01000, 05004], lr: 0.007315, loss: 0.9774
2022-03-04 11:19:34 - train: epoch 0167, iter [01100, 05004], lr: 0.007315, loss: 0.9533
2022-03-04 11:20:07 - train: epoch 0167, iter [01200, 05004], lr: 0.007315, loss: 1.1421
2022-03-04 11:20:40 - train: epoch 0167, iter [01300, 05004], lr: 0.007315, loss: 1.2351
2022-03-04 11:21:13 - train: epoch 0167, iter [01400, 05004], lr: 0.007315, loss: 0.9706
2022-03-04 11:21:47 - train: epoch 0167, iter [01500, 05004], lr: 0.007315, loss: 1.1410
2022-03-04 11:22:20 - train: epoch 0167, iter [01600, 05004], lr: 0.007315, loss: 1.1283
2022-03-04 11:22:53 - train: epoch 0167, iter [01700, 05004], lr: 0.007315, loss: 0.9510
2022-03-04 11:23:26 - train: epoch 0167, iter [01800, 05004], lr: 0.007315, loss: 1.2168
2022-03-04 11:23:59 - train: epoch 0167, iter [01900, 05004], lr: 0.007315, loss: 1.3286
2022-03-04 11:24:33 - train: epoch 0167, iter [02000, 05004], lr: 0.007315, loss: 1.1531
2022-03-04 11:25:06 - train: epoch 0167, iter [02100, 05004], lr: 0.007315, loss: 0.9590
2022-03-04 11:25:39 - train: epoch 0167, iter [02200, 05004], lr: 0.007315, loss: 1.0292
2022-03-04 11:26:12 - train: epoch 0167, iter [02300, 05004], lr: 0.007315, loss: 1.3279
2022-03-04 11:26:45 - train: epoch 0167, iter [02400, 05004], lr: 0.007315, loss: 1.1173
2022-03-04 11:27:19 - train: epoch 0167, iter [02500, 05004], lr: 0.007315, loss: 1.0674
2022-03-04 11:27:52 - train: epoch 0167, iter [02600, 05004], lr: 0.007315, loss: 1.0764
2022-03-04 11:28:25 - train: epoch 0167, iter [02700, 05004], lr: 0.007315, loss: 1.2354
2022-03-04 11:28:59 - train: epoch 0167, iter [02800, 05004], lr: 0.007315, loss: 1.1730
2022-03-04 11:29:31 - train: epoch 0167, iter [02900, 05004], lr: 0.007315, loss: 1.0204
2022-03-04 11:30:05 - train: epoch 0167, iter [03000, 05004], lr: 0.007315, loss: 1.0038
2022-03-04 11:30:38 - train: epoch 0167, iter [03100, 05004], lr: 0.007315, loss: 1.0649
2022-03-04 11:31:11 - train: epoch 0167, iter [03200, 05004], lr: 0.007315, loss: 1.1182
2022-03-04 11:31:44 - train: epoch 0167, iter [03300, 05004], lr: 0.007315, loss: 1.2049
2022-03-04 11:32:18 - train: epoch 0167, iter [03400, 05004], lr: 0.007315, loss: 1.1435
2022-03-04 11:32:51 - train: epoch 0167, iter [03500, 05004], lr: 0.007315, loss: 1.1326
2022-03-04 11:33:25 - train: epoch 0167, iter [03600, 05004], lr: 0.007315, loss: 1.0273
2022-03-04 11:33:57 - train: epoch 0167, iter [03700, 05004], lr: 0.007315, loss: 1.1480
2022-03-04 11:34:31 - train: epoch 0167, iter [03800, 05004], lr: 0.007315, loss: 1.1522
2022-03-04 11:35:04 - train: epoch 0167, iter [03900, 05004], lr: 0.007315, loss: 1.0933
2022-03-04 11:35:37 - train: epoch 0167, iter [04000, 05004], lr: 0.007315, loss: 1.2655
2022-03-04 11:36:11 - train: epoch 0167, iter [04100, 05004], lr: 0.007315, loss: 1.0484
2022-03-04 11:36:43 - train: epoch 0167, iter [04200, 05004], lr: 0.007315, loss: 1.1244
2022-03-04 11:37:17 - train: epoch 0167, iter [04300, 05004], lr: 0.007315, loss: 1.1032
2022-03-04 11:37:50 - train: epoch 0167, iter [04400, 05004], lr: 0.007315, loss: 0.8646
2022-03-04 11:38:23 - train: epoch 0167, iter [04500, 05004], lr: 0.007315, loss: 0.8504
2022-03-04 11:38:57 - train: epoch 0167, iter [04600, 05004], lr: 0.007315, loss: 1.1001
2022-03-04 11:39:30 - train: epoch 0167, iter [04700, 05004], lr: 0.007315, loss: 1.3147
2022-03-04 11:40:03 - train: epoch 0167, iter [04800, 05004], lr: 0.007315, loss: 1.0525
2022-03-04 11:40:35 - train: epoch 0167, iter [04900, 05004], lr: 0.007315, loss: 1.1496
2022-03-04 11:41:08 - train: epoch 0167, iter [05000, 05004], lr: 0.007315, loss: 1.1222
2022-03-04 11:41:09 - train: epoch 167, train_loss: 1.1248
2022-03-04 11:42:23 - eval: epoch: 167, acc1: 73.578%, acc5: 91.662%, test_loss: 1.0563, per_image_load_time: 2.352ms, per_image_inference_time: 0.529ms
2022-03-04 11:42:24 - until epoch: 167, best_acc1: 73.578%
2022-03-04 11:42:24 - epoch 168 lr: 0.006901516655997537
2022-03-04 11:43:01 - train: epoch 0168, iter [00100, 05004], lr: 0.006902, loss: 0.9921
2022-03-04 11:43:35 - train: epoch 0168, iter [00200, 05004], lr: 0.006902, loss: 1.1406
2022-03-04 11:44:08 - train: epoch 0168, iter [00300, 05004], lr: 0.006902, loss: 1.2254
2022-03-04 11:44:41 - train: epoch 0168, iter [00400, 05004], lr: 0.006902, loss: 0.9421
2022-03-04 11:45:15 - train: epoch 0168, iter [00500, 05004], lr: 0.006902, loss: 1.0711
2022-03-04 11:45:48 - train: epoch 0168, iter [00600, 05004], lr: 0.006902, loss: 1.2226
2022-03-04 11:46:21 - train: epoch 0168, iter [00700, 05004], lr: 0.006902, loss: 1.0988
2022-03-04 11:46:55 - train: epoch 0168, iter [00800, 05004], lr: 0.006902, loss: 1.3194
2022-03-04 11:47:28 - train: epoch 0168, iter [00900, 05004], lr: 0.006902, loss: 1.0909
2022-03-04 11:48:01 - train: epoch 0168, iter [01000, 05004], lr: 0.006902, loss: 0.9426
2022-03-04 11:48:34 - train: epoch 0168, iter [01100, 05004], lr: 0.006902, loss: 1.0115
2022-03-04 11:49:07 - train: epoch 0168, iter [01200, 05004], lr: 0.006902, loss: 1.1437
2022-03-04 11:49:41 - train: epoch 0168, iter [01300, 05004], lr: 0.006902, loss: 1.1383
2022-03-04 11:50:15 - train: epoch 0168, iter [01400, 05004], lr: 0.006902, loss: 1.0115
2022-03-04 11:50:49 - train: epoch 0168, iter [01500, 05004], lr: 0.006902, loss: 1.0977
2022-03-04 11:51:22 - train: epoch 0168, iter [01600, 05004], lr: 0.006902, loss: 1.1687
2022-03-04 11:51:55 - train: epoch 0168, iter [01700, 05004], lr: 0.006902, loss: 1.1142
2022-03-04 11:52:28 - train: epoch 0168, iter [01800, 05004], lr: 0.006902, loss: 1.0433
2022-03-04 11:53:01 - train: epoch 0168, iter [01900, 05004], lr: 0.006902, loss: 1.1124
2022-03-04 11:53:34 - train: epoch 0168, iter [02000, 05004], lr: 0.006902, loss: 0.9894
2022-03-04 11:54:08 - train: epoch 0168, iter [02100, 05004], lr: 0.006902, loss: 1.0660
2022-03-04 11:54:41 - train: epoch 0168, iter [02200, 05004], lr: 0.006902, loss: 1.0613
2022-03-04 11:55:14 - train: epoch 0168, iter [02300, 05004], lr: 0.006902, loss: 1.0197
2022-03-04 11:55:48 - train: epoch 0168, iter [02400, 05004], lr: 0.006902, loss: 1.0954
2022-03-04 11:56:20 - train: epoch 0168, iter [02500, 05004], lr: 0.006902, loss: 1.2175
2022-03-04 11:56:54 - train: epoch 0168, iter [02600, 05004], lr: 0.006902, loss: 1.0374
2022-03-04 11:57:27 - train: epoch 0168, iter [02700, 05004], lr: 0.006902, loss: 1.0514
2022-03-04 11:58:00 - train: epoch 0168, iter [02800, 05004], lr: 0.006902, loss: 1.0895
2022-03-04 11:58:33 - train: epoch 0168, iter [02900, 05004], lr: 0.006902, loss: 1.2695
2022-03-04 11:59:07 - train: epoch 0168, iter [03000, 05004], lr: 0.006902, loss: 0.9861
2022-03-04 11:59:40 - train: epoch 0168, iter [03100, 05004], lr: 0.006902, loss: 1.1036
2022-03-04 12:00:13 - train: epoch 0168, iter [03200, 05004], lr: 0.006902, loss: 1.0678
2022-03-04 12:00:47 - train: epoch 0168, iter [03300, 05004], lr: 0.006902, loss: 0.9330
2022-03-04 12:01:20 - train: epoch 0168, iter [03400, 05004], lr: 0.006902, loss: 1.3105
2022-03-04 12:01:53 - train: epoch 0168, iter [03500, 05004], lr: 0.006902, loss: 1.0353
2022-03-04 12:02:26 - train: epoch 0168, iter [03600, 05004], lr: 0.006902, loss: 1.0865
2022-03-04 12:03:01 - train: epoch 0168, iter [03700, 05004], lr: 0.006902, loss: 1.0680
2022-03-04 12:03:34 - train: epoch 0168, iter [03800, 05004], lr: 0.006902, loss: 1.1423
2022-03-04 12:04:08 - train: epoch 0168, iter [03900, 05004], lr: 0.006902, loss: 1.2174
2022-03-04 12:04:41 - train: epoch 0168, iter [04000, 05004], lr: 0.006902, loss: 1.2950
2022-03-04 12:05:14 - train: epoch 0168, iter [04100, 05004], lr: 0.006902, loss: 1.2569
2022-03-04 12:05:48 - train: epoch 0168, iter [04200, 05004], lr: 0.006902, loss: 1.1226
2022-03-04 12:06:19 - train: epoch 0168, iter [04300, 05004], lr: 0.006902, loss: 1.0769
2022-03-04 12:06:53 - train: epoch 0168, iter [04400, 05004], lr: 0.006902, loss: 1.2281
2022-03-04 12:07:27 - train: epoch 0168, iter [04500, 05004], lr: 0.006902, loss: 1.3052
2022-03-04 12:08:00 - train: epoch 0168, iter [04600, 05004], lr: 0.006902, loss: 1.1277
2022-03-04 12:08:34 - train: epoch 0168, iter [04700, 05004], lr: 0.006902, loss: 1.0452
2022-03-04 12:09:07 - train: epoch 0168, iter [04800, 05004], lr: 0.006902, loss: 1.1290
2022-03-04 12:09:40 - train: epoch 0168, iter [04900, 05004], lr: 0.006902, loss: 1.1404
2022-03-04 12:10:12 - train: epoch 0168, iter [05000, 05004], lr: 0.006902, loss: 1.0634
2022-03-04 12:10:13 - train: epoch 168, train_loss: 1.1091
2022-03-04 12:11:27 - eval: epoch: 168, acc1: 73.748%, acc5: 91.904%, test_loss: 1.0488, per_image_load_time: 2.327ms, per_image_inference_time: 0.510ms
2022-03-04 12:11:27 - until epoch: 168, best_acc1: 73.748%
2022-03-04 12:11:27 - epoch 169 lr: 0.006498752722928042
2022-03-04 12:12:06 - train: epoch 0169, iter [00100, 05004], lr: 0.006499, loss: 1.1243
2022-03-04 12:12:39 - train: epoch 0169, iter [00200, 05004], lr: 0.006499, loss: 1.2471
2022-03-04 12:13:12 - train: epoch 0169, iter [00300, 05004], lr: 0.006499, loss: 1.2295
2022-03-04 12:13:44 - train: epoch 0169, iter [00400, 05004], lr: 0.006499, loss: 0.9595
2022-03-04 12:14:18 - train: epoch 0169, iter [00500, 05004], lr: 0.006499, loss: 1.1920
2022-03-04 12:14:51 - train: epoch 0169, iter [00600, 05004], lr: 0.006499, loss: 1.0762
2022-03-04 12:15:24 - train: epoch 0169, iter [00700, 05004], lr: 0.006499, loss: 1.1845
2022-03-04 12:15:56 - train: epoch 0169, iter [00800, 05004], lr: 0.006499, loss: 1.0716
2022-03-04 12:16:31 - train: epoch 0169, iter [00900, 05004], lr: 0.006499, loss: 1.3074
2022-03-04 12:17:04 - train: epoch 0169, iter [01000, 05004], lr: 0.006499, loss: 1.1094
2022-03-04 12:17:37 - train: epoch 0169, iter [01100, 05004], lr: 0.006499, loss: 1.2223
2022-03-04 12:18:11 - train: epoch 0169, iter [01200, 05004], lr: 0.006499, loss: 0.9282
2022-03-04 12:18:44 - train: epoch 0169, iter [01300, 05004], lr: 0.006499, loss: 1.0903
2022-03-04 12:19:17 - train: epoch 0169, iter [01400, 05004], lr: 0.006499, loss: 1.0837
2022-03-04 12:19:51 - train: epoch 0169, iter [01500, 05004], lr: 0.006499, loss: 0.9963
2022-03-04 12:20:24 - train: epoch 0169, iter [01600, 05004], lr: 0.006499, loss: 1.0519
2022-03-04 12:20:57 - train: epoch 0169, iter [01700, 05004], lr: 0.006499, loss: 1.1279
2022-03-04 12:21:30 - train: epoch 0169, iter [01800, 05004], lr: 0.006499, loss: 1.1835
2022-03-04 12:22:04 - train: epoch 0169, iter [01900, 05004], lr: 0.006499, loss: 1.1599
2022-03-04 12:22:37 - train: epoch 0169, iter [02000, 05004], lr: 0.006499, loss: 1.0622
2022-03-04 12:23:10 - train: epoch 0169, iter [02100, 05004], lr: 0.006499, loss: 1.1434
2022-03-04 12:23:44 - train: epoch 0169, iter [02200, 05004], lr: 0.006499, loss: 1.1339
2022-03-04 12:24:17 - train: epoch 0169, iter [02300, 05004], lr: 0.006499, loss: 1.1462
2022-03-04 12:24:50 - train: epoch 0169, iter [02400, 05004], lr: 0.006499, loss: 1.0048
2022-03-04 12:25:24 - train: epoch 0169, iter [02500, 05004], lr: 0.006499, loss: 1.0047
2022-03-04 12:25:57 - train: epoch 0169, iter [02600, 05004], lr: 0.006499, loss: 0.9910
2022-03-04 12:26:30 - train: epoch 0169, iter [02700, 05004], lr: 0.006499, loss: 0.9281
2022-03-04 12:27:04 - train: epoch 0169, iter [02800, 05004], lr: 0.006499, loss: 1.1264
2022-03-04 12:27:36 - train: epoch 0169, iter [02900, 05004], lr: 0.006499, loss: 1.0795
2022-03-04 12:28:10 - train: epoch 0169, iter [03000, 05004], lr: 0.006499, loss: 0.9170
2022-03-04 12:28:43 - train: epoch 0169, iter [03100, 05004], lr: 0.006499, loss: 0.9470
2022-03-04 12:29:16 - train: epoch 0169, iter [03200, 05004], lr: 0.006499, loss: 1.2802
2022-03-04 12:29:49 - train: epoch 0169, iter [03300, 05004], lr: 0.006499, loss: 0.9918
2022-03-04 12:30:23 - train: epoch 0169, iter [03400, 05004], lr: 0.006499, loss: 1.0224
2022-03-04 12:30:57 - train: epoch 0169, iter [03500, 05004], lr: 0.006499, loss: 1.0564
2022-03-04 12:31:30 - train: epoch 0169, iter [03600, 05004], lr: 0.006499, loss: 1.0978
2022-03-04 12:32:03 - train: epoch 0169, iter [03700, 05004], lr: 0.006499, loss: 1.1305
2022-03-04 12:32:36 - train: epoch 0169, iter [03800, 05004], lr: 0.006499, loss: 1.1772
2022-03-04 12:33:10 - train: epoch 0169, iter [03900, 05004], lr: 0.006499, loss: 0.9373
2022-03-04 12:33:42 - train: epoch 0169, iter [04000, 05004], lr: 0.006499, loss: 0.9698
2022-03-04 12:34:16 - train: epoch 0169, iter [04100, 05004], lr: 0.006499, loss: 0.9221
2022-03-04 12:34:49 - train: epoch 0169, iter [04200, 05004], lr: 0.006499, loss: 1.1670
2022-03-04 12:35:22 - train: epoch 0169, iter [04300, 05004], lr: 0.006499, loss: 0.9936
2022-03-04 12:35:55 - train: epoch 0169, iter [04400, 05004], lr: 0.006499, loss: 1.0959
2022-03-04 12:36:28 - train: epoch 0169, iter [04500, 05004], lr: 0.006499, loss: 1.3489
2022-03-04 12:37:02 - train: epoch 0169, iter [04600, 05004], lr: 0.006499, loss: 1.1433
2022-03-04 12:37:36 - train: epoch 0169, iter [04700, 05004], lr: 0.006499, loss: 0.9713
2022-03-04 12:38:09 - train: epoch 0169, iter [04800, 05004], lr: 0.006499, loss: 1.1222
2022-03-04 12:38:41 - train: epoch 0169, iter [04900, 05004], lr: 0.006499, loss: 0.9967
2022-03-04 12:39:14 - train: epoch 0169, iter [05000, 05004], lr: 0.006499, loss: 1.1889
2022-03-04 12:39:15 - train: epoch 169, train_loss: 1.0938
2022-03-04 12:40:29 - eval: epoch: 169, acc1: 74.000%, acc5: 91.952%, test_loss: 1.0400, per_image_load_time: 2.374ms, per_image_inference_time: 0.487ms
2022-03-04 12:40:30 - until epoch: 169, best_acc1: 74.000%
2022-03-04 12:40:30 - epoch 170 lr: 0.006107279541079769
2022-03-04 12:41:07 - train: epoch 0170, iter [00100, 05004], lr: 0.006107, loss: 1.1483
2022-03-04 12:41:42 - train: epoch 0170, iter [00200, 05004], lr: 0.006107, loss: 1.1533
2022-03-04 12:42:15 - train: epoch 0170, iter [00300, 05004], lr: 0.006107, loss: 1.0022
2022-03-04 12:42:47 - train: epoch 0170, iter [00400, 05004], lr: 0.006107, loss: 1.0782
2022-03-04 12:43:20 - train: epoch 0170, iter [00500, 05004], lr: 0.006107, loss: 1.1944
2022-03-04 12:43:53 - train: epoch 0170, iter [00600, 05004], lr: 0.006107, loss: 1.1776
2022-03-04 12:44:26 - train: epoch 0170, iter [00700, 05004], lr: 0.006107, loss: 0.8359
2022-03-04 12:44:59 - train: epoch 0170, iter [00800, 05004], lr: 0.006107, loss: 1.0927
2022-03-04 12:45:32 - train: epoch 0170, iter [00900, 05004], lr: 0.006107, loss: 1.1047
2022-03-04 12:46:05 - train: epoch 0170, iter [01000, 05004], lr: 0.006107, loss: 1.0451
2022-03-04 12:46:38 - train: epoch 0170, iter [01100, 05004], lr: 0.006107, loss: 0.9665
2022-03-04 12:47:11 - train: epoch 0170, iter [01200, 05004], lr: 0.006107, loss: 1.0523
2022-03-04 12:47:45 - train: epoch 0170, iter [01300, 05004], lr: 0.006107, loss: 1.1193
2022-03-04 12:48:18 - train: epoch 0170, iter [01400, 05004], lr: 0.006107, loss: 1.1731
2022-03-04 12:48:50 - train: epoch 0170, iter [01500, 05004], lr: 0.006107, loss: 1.0609
2022-03-04 12:49:24 - train: epoch 0170, iter [01600, 05004], lr: 0.006107, loss: 1.2094
2022-03-04 12:49:56 - train: epoch 0170, iter [01700, 05004], lr: 0.006107, loss: 1.3523
2022-03-04 12:50:30 - train: epoch 0170, iter [01800, 05004], lr: 0.006107, loss: 0.7990
2022-03-04 12:51:03 - train: epoch 0170, iter [01900, 05004], lr: 0.006107, loss: 1.4492
2022-03-04 12:51:36 - train: epoch 0170, iter [02000, 05004], lr: 0.006107, loss: 1.0349
2022-03-04 12:52:09 - train: epoch 0170, iter [02100, 05004], lr: 0.006107, loss: 1.0075
2022-03-04 12:52:44 - train: epoch 0170, iter [02200, 05004], lr: 0.006107, loss: 1.0198
2022-03-04 12:53:17 - train: epoch 0170, iter [02300, 05004], lr: 0.006107, loss: 1.0856
2022-03-04 12:53:50 - train: epoch 0170, iter [02400, 05004], lr: 0.006107, loss: 1.3130
2022-03-04 12:54:23 - train: epoch 0170, iter [02500, 05004], lr: 0.006107, loss: 1.0147
2022-03-04 12:54:56 - train: epoch 0170, iter [02600, 05004], lr: 0.006107, loss: 1.1085
2022-03-04 12:55:29 - train: epoch 0170, iter [02700, 05004], lr: 0.006107, loss: 0.9810
2022-03-04 12:56:03 - train: epoch 0170, iter [02800, 05004], lr: 0.006107, loss: 1.1253
2022-03-04 12:56:36 - train: epoch 0170, iter [02900, 05004], lr: 0.006107, loss: 1.1319
2022-03-04 12:57:09 - train: epoch 0170, iter [03000, 05004], lr: 0.006107, loss: 0.9565
2022-03-04 12:57:42 - train: epoch 0170, iter [03100, 05004], lr: 0.006107, loss: 0.9952
2022-03-04 12:58:16 - train: epoch 0170, iter [03200, 05004], lr: 0.006107, loss: 1.0949
2022-03-04 12:58:49 - train: epoch 0170, iter [03300, 05004], lr: 0.006107, loss: 1.0040
2022-03-04 12:59:23 - train: epoch 0170, iter [03400, 05004], lr: 0.006107, loss: 1.3409
2022-03-04 12:59:55 - train: epoch 0170, iter [03500, 05004], lr: 0.006107, loss: 1.1634
2022-03-04 13:00:28 - train: epoch 0170, iter [03600, 05004], lr: 0.006107, loss: 0.9314
2022-03-04 13:01:01 - train: epoch 0170, iter [03700, 05004], lr: 0.006107, loss: 1.2106
2022-03-04 13:01:35 - train: epoch 0170, iter [03800, 05004], lr: 0.006107, loss: 1.2924
2022-03-04 13:02:08 - train: epoch 0170, iter [03900, 05004], lr: 0.006107, loss: 1.3410
2022-03-04 13:02:41 - train: epoch 0170, iter [04000, 05004], lr: 0.006107, loss: 0.9809
2022-03-04 13:03:14 - train: epoch 0170, iter [04100, 05004], lr: 0.006107, loss: 0.9335
2022-03-04 13:03:48 - train: epoch 0170, iter [04200, 05004], lr: 0.006107, loss: 1.0975
2022-03-04 13:04:21 - train: epoch 0170, iter [04300, 05004], lr: 0.006107, loss: 1.2043
2022-03-04 13:04:54 - train: epoch 0170, iter [04400, 05004], lr: 0.006107, loss: 0.9853
2022-03-04 13:05:28 - train: epoch 0170, iter [04500, 05004], lr: 0.006107, loss: 1.0943
2022-03-04 13:06:00 - train: epoch 0170, iter [04600, 05004], lr: 0.006107, loss: 0.8806
2022-03-04 13:06:34 - train: epoch 0170, iter [04700, 05004], lr: 0.006107, loss: 1.2469
2022-03-04 13:07:07 - train: epoch 0170, iter [04800, 05004], lr: 0.006107, loss: 1.1811
2022-03-04 13:07:41 - train: epoch 0170, iter [04900, 05004], lr: 0.006107, loss: 1.1912
2022-03-04 13:08:13 - train: epoch 0170, iter [05000, 05004], lr: 0.006107, loss: 1.0741
2022-03-04 13:08:15 - train: epoch 170, train_loss: 1.0792
2022-03-04 13:09:28 - eval: epoch: 170, acc1: 74.062%, acc5: 91.980%, test_loss: 1.0348, per_image_load_time: 2.300ms, per_image_inference_time: 0.554ms
2022-03-04 13:09:29 - until epoch: 170, best_acc1: 74.062%
2022-03-04 13:09:29 - epoch 171 lr: 0.00572719871733951
2022-03-04 13:10:07 - train: epoch 0171, iter [00100, 05004], lr: 0.005727, loss: 0.9638
2022-03-04 13:10:39 - train: epoch 0171, iter [00200, 05004], lr: 0.005727, loss: 1.0642
2022-03-04 13:11:13 - train: epoch 0171, iter [00300, 05004], lr: 0.005727, loss: 0.9388
2022-03-04 13:11:46 - train: epoch 0171, iter [00400, 05004], lr: 0.005727, loss: 1.1968
2022-03-04 13:12:20 - train: epoch 0171, iter [00500, 05004], lr: 0.005727, loss: 0.9909
2022-03-04 13:12:53 - train: epoch 0171, iter [00600, 05004], lr: 0.005727, loss: 1.1343
2022-03-04 13:13:26 - train: epoch 0171, iter [00700, 05004], lr: 0.005727, loss: 1.0587
2022-03-04 13:14:01 - train: epoch 0171, iter [00800, 05004], lr: 0.005727, loss: 1.0141
2022-03-04 13:14:33 - train: epoch 0171, iter [00900, 05004], lr: 0.005727, loss: 0.8651
2022-03-04 13:15:06 - train: epoch 0171, iter [01000, 05004], lr: 0.005727, loss: 0.9278
2022-03-04 13:15:39 - train: epoch 0171, iter [01100, 05004], lr: 0.005727, loss: 0.8855
2022-03-04 13:16:12 - train: epoch 0171, iter [01200, 05004], lr: 0.005727, loss: 1.2906
2022-03-04 13:16:46 - train: epoch 0171, iter [01300, 05004], lr: 0.005727, loss: 1.0779
2022-03-04 13:17:20 - train: epoch 0171, iter [01400, 05004], lr: 0.005727, loss: 1.1086
2022-03-04 13:17:52 - train: epoch 0171, iter [01500, 05004], lr: 0.005727, loss: 0.9660
2022-03-04 13:18:26 - train: epoch 0171, iter [01600, 05004], lr: 0.005727, loss: 1.0945
2022-03-04 13:18:59 - train: epoch 0171, iter [01700, 05004], lr: 0.005727, loss: 1.0754
2022-03-04 13:19:33 - train: epoch 0171, iter [01800, 05004], lr: 0.005727, loss: 0.9833
2022-03-04 13:20:05 - train: epoch 0171, iter [01900, 05004], lr: 0.005727, loss: 1.0184
2022-03-04 13:20:39 - train: epoch 0171, iter [02000, 05004], lr: 0.005727, loss: 0.9700
2022-03-04 13:21:12 - train: epoch 0171, iter [02100, 05004], lr: 0.005727, loss: 1.0569
2022-03-04 13:21:45 - train: epoch 0171, iter [02200, 05004], lr: 0.005727, loss: 0.8787
2022-03-04 13:22:19 - train: epoch 0171, iter [02300, 05004], lr: 0.005727, loss: 1.0781
2022-03-04 13:22:52 - train: epoch 0171, iter [02400, 05004], lr: 0.005727, loss: 1.1607
2022-03-04 13:23:25 - train: epoch 0171, iter [02500, 05004], lr: 0.005727, loss: 0.9329
2022-03-04 13:23:58 - train: epoch 0171, iter [02600, 05004], lr: 0.005727, loss: 0.9178
2022-03-04 13:24:31 - train: epoch 0171, iter [02700, 05004], lr: 0.005727, loss: 1.1470
2022-03-04 13:25:04 - train: epoch 0171, iter [02800, 05004], lr: 0.005727, loss: 1.0733
2022-03-04 13:25:38 - train: epoch 0171, iter [02900, 05004], lr: 0.005727, loss: 1.0413
2022-03-04 13:26:10 - train: epoch 0171, iter [03000, 05004], lr: 0.005727, loss: 1.3067
2022-03-04 13:26:44 - train: epoch 0171, iter [03100, 05004], lr: 0.005727, loss: 1.0816
2022-03-04 13:27:17 - train: epoch 0171, iter [03200, 05004], lr: 0.005727, loss: 1.0591
2022-03-04 13:27:50 - train: epoch 0171, iter [03300, 05004], lr: 0.005727, loss: 0.9434
2022-03-04 13:28:23 - train: epoch 0171, iter [03400, 05004], lr: 0.005727, loss: 1.0916
2022-03-04 13:28:57 - train: epoch 0171, iter [03500, 05004], lr: 0.005727, loss: 1.0745
2022-03-04 13:29:30 - train: epoch 0171, iter [03600, 05004], lr: 0.005727, loss: 1.2283
2022-03-04 13:30:03 - train: epoch 0171, iter [03700, 05004], lr: 0.005727, loss: 1.2902
2022-03-04 13:30:37 - train: epoch 0171, iter [03800, 05004], lr: 0.005727, loss: 1.0938
2022-03-04 13:31:10 - train: epoch 0171, iter [03900, 05004], lr: 0.005727, loss: 1.0223
2022-03-04 13:31:43 - train: epoch 0171, iter [04000, 05004], lr: 0.005727, loss: 0.9369
2022-03-04 13:32:16 - train: epoch 0171, iter [04100, 05004], lr: 0.005727, loss: 1.1289
2022-03-04 13:32:49 - train: epoch 0171, iter [04200, 05004], lr: 0.005727, loss: 1.0058
2022-03-04 13:33:22 - train: epoch 0171, iter [04300, 05004], lr: 0.005727, loss: 1.2743
2022-03-04 13:33:55 - train: epoch 0171, iter [04400, 05004], lr: 0.005727, loss: 1.0300
2022-03-04 13:34:29 - train: epoch 0171, iter [04500, 05004], lr: 0.005727, loss: 1.2598
2022-03-04 13:35:02 - train: epoch 0171, iter [04600, 05004], lr: 0.005727, loss: 1.0027
2022-03-04 13:35:35 - train: epoch 0171, iter [04700, 05004], lr: 0.005727, loss: 1.1635
2022-03-04 13:36:08 - train: epoch 0171, iter [04800, 05004], lr: 0.005727, loss: 1.0604
2022-03-04 13:36:41 - train: epoch 0171, iter [04900, 05004], lr: 0.005727, loss: 1.1835
2022-03-04 13:37:13 - train: epoch 0171, iter [05000, 05004], lr: 0.005727, loss: 1.2412
2022-03-04 13:37:14 - train: epoch 171, train_loss: 1.0654
2022-03-04 13:38:28 - eval: epoch: 171, acc1: 74.284%, acc5: 92.104%, test_loss: 1.0308, per_image_load_time: 2.357ms, per_image_inference_time: 0.525ms
2022-03-04 13:38:29 - until epoch: 171, best_acc1: 74.284%
2022-03-04 13:38:29 - epoch 172 lr: 0.005358608901706802
2022-03-04 13:39:07 - train: epoch 0172, iter [00100, 05004], lr: 0.005359, loss: 1.1236
2022-03-04 13:39:40 - train: epoch 0172, iter [00200, 05004], lr: 0.005359, loss: 1.0539
2022-03-04 13:40:14 - train: epoch 0172, iter [00300, 05004], lr: 0.005359, loss: 1.1286
2022-03-04 13:40:47 - train: epoch 0172, iter [00400, 05004], lr: 0.005359, loss: 0.9482
2022-03-04 13:41:20 - train: epoch 0172, iter [00500, 05004], lr: 0.005359, loss: 1.1998
2022-03-04 13:41:53 - train: epoch 0172, iter [00600, 05004], lr: 0.005359, loss: 1.1306
2022-03-04 13:42:25 - train: epoch 0172, iter [00700, 05004], lr: 0.005359, loss: 0.8694
2022-03-04 13:42:59 - train: epoch 0172, iter [00800, 05004], lr: 0.005359, loss: 1.0784
2022-03-04 13:43:32 - train: epoch 0172, iter [00900, 05004], lr: 0.005359, loss: 0.9209
2022-03-04 13:44:05 - train: epoch 0172, iter [01000, 05004], lr: 0.005359, loss: 0.9011
2022-03-04 13:44:38 - train: epoch 0172, iter [01100, 05004], lr: 0.005359, loss: 1.1090
2022-03-04 13:45:12 - train: epoch 0172, iter [01200, 05004], lr: 0.005359, loss: 1.2413
2022-03-04 13:45:44 - train: epoch 0172, iter [01300, 05004], lr: 0.005359, loss: 0.9273
2022-03-04 13:46:19 - train: epoch 0172, iter [01400, 05004], lr: 0.005359, loss: 1.0853
2022-03-04 13:46:52 - train: epoch 0172, iter [01500, 05004], lr: 0.005359, loss: 1.0189
2022-03-04 13:47:25 - train: epoch 0172, iter [01600, 05004], lr: 0.005359, loss: 1.1216
2022-03-04 13:47:58 - train: epoch 0172, iter [01700, 05004], lr: 0.005359, loss: 1.2695
2022-03-04 13:48:32 - train: epoch 0172, iter [01800, 05004], lr: 0.005359, loss: 1.1674
2022-03-04 13:49:04 - train: epoch 0172, iter [01900, 05004], lr: 0.005359, loss: 0.8246
2022-03-04 13:49:38 - train: epoch 0172, iter [02000, 05004], lr: 0.005359, loss: 1.0539
2022-03-04 13:50:12 - train: epoch 0172, iter [02100, 05004], lr: 0.005359, loss: 1.0664
2022-03-04 13:50:45 - train: epoch 0172, iter [02200, 05004], lr: 0.005359, loss: 1.0906
2022-03-04 13:51:18 - train: epoch 0172, iter [02300, 05004], lr: 0.005359, loss: 1.0193
2022-03-04 13:51:51 - train: epoch 0172, iter [02400, 05004], lr: 0.005359, loss: 1.2703
2022-03-04 13:52:24 - train: epoch 0172, iter [02500, 05004], lr: 0.005359, loss: 0.9519
2022-03-04 13:52:57 - train: epoch 0172, iter [02600, 05004], lr: 0.005359, loss: 1.0271
2022-03-04 13:53:31 - train: epoch 0172, iter [02700, 05004], lr: 0.005359, loss: 0.9247
2022-03-04 13:54:04 - train: epoch 0172, iter [02800, 05004], lr: 0.005359, loss: 1.0482
2022-03-04 13:54:36 - train: epoch 0172, iter [02900, 05004], lr: 0.005359, loss: 1.0207
2022-03-04 13:55:10 - train: epoch 0172, iter [03000, 05004], lr: 0.005359, loss: 0.9459
2022-03-04 13:55:42 - train: epoch 0172, iter [03100, 05004], lr: 0.005359, loss: 1.3024
2022-03-04 13:56:15 - train: epoch 0172, iter [03200, 05004], lr: 0.005359, loss: 1.0456
2022-03-04 13:56:49 - train: epoch 0172, iter [03300, 05004], lr: 0.005359, loss: 1.0745
2022-03-04 13:57:22 - train: epoch 0172, iter [03400, 05004], lr: 0.005359, loss: 1.1261
2022-03-04 13:57:55 - train: epoch 0172, iter [03500, 05004], lr: 0.005359, loss: 0.9487
2022-03-04 13:58:29 - train: epoch 0172, iter [03600, 05004], lr: 0.005359, loss: 0.9819
2022-03-04 13:59:02 - train: epoch 0172, iter [03700, 05004], lr: 0.005359, loss: 0.9884
2022-03-04 13:59:35 - train: epoch 0172, iter [03800, 05004], lr: 0.005359, loss: 0.9399
2022-03-04 14:00:08 - train: epoch 0172, iter [03900, 05004], lr: 0.005359, loss: 0.9225
2022-03-04 14:00:41 - train: epoch 0172, iter [04000, 05004], lr: 0.005359, loss: 1.1538
2022-03-04 14:01:15 - train: epoch 0172, iter [04100, 05004], lr: 0.005359, loss: 1.2500
2022-03-04 14:01:48 - train: epoch 0172, iter [04200, 05004], lr: 0.005359, loss: 1.2025
2022-03-04 14:02:22 - train: epoch 0172, iter [04300, 05004], lr: 0.005359, loss: 0.9732
2022-03-04 14:02:55 - train: epoch 0172, iter [04400, 05004], lr: 0.005359, loss: 1.0405
2022-03-04 14:03:27 - train: epoch 0172, iter [04500, 05004], lr: 0.005359, loss: 1.1716
2022-03-04 14:04:00 - train: epoch 0172, iter [04600, 05004], lr: 0.005359, loss: 1.1376
2022-03-04 14:04:34 - train: epoch 0172, iter [04700, 05004], lr: 0.005359, loss: 1.1836
2022-03-04 14:05:07 - train: epoch 0172, iter [04800, 05004], lr: 0.005359, loss: 0.9742
2022-03-04 14:05:41 - train: epoch 0172, iter [04900, 05004], lr: 0.005359, loss: 1.0836
2022-03-04 14:06:12 - train: epoch 0172, iter [05000, 05004], lr: 0.005359, loss: 0.9555
2022-03-04 14:06:14 - train: epoch 172, train_loss: 1.0455
2022-03-04 14:07:28 - eval: epoch: 172, acc1: 74.610%, acc5: 92.372%, test_loss: 1.0167, per_image_load_time: 2.345ms, per_image_inference_time: 0.517ms
2022-03-04 14:07:28 - until epoch: 172, best_acc1: 74.610%
2022-03-04 14:07:28 - epoch 173 lr: 0.005001605761689398
2022-03-04 14:08:06 - train: epoch 0173, iter [00100, 05004], lr: 0.005002, loss: 1.1632
2022-03-04 14:08:39 - train: epoch 0173, iter [00200, 05004], lr: 0.005002, loss: 1.1583
2022-03-04 14:09:12 - train: epoch 0173, iter [00300, 05004], lr: 0.005002, loss: 0.9453
2022-03-04 14:09:46 - train: epoch 0173, iter [00400, 05004], lr: 0.005002, loss: 0.9633
2022-03-04 14:10:18 - train: epoch 0173, iter [00500, 05004], lr: 0.005002, loss: 1.0434
2022-03-04 14:10:52 - train: epoch 0173, iter [00600, 05004], lr: 0.005002, loss: 1.0307
2022-03-04 14:11:25 - train: epoch 0173, iter [00700, 05004], lr: 0.005002, loss: 0.7930
2022-03-04 14:11:57 - train: epoch 0173, iter [00800, 05004], lr: 0.005002, loss: 1.0132
2022-03-04 14:12:31 - train: epoch 0173, iter [00900, 05004], lr: 0.005002, loss: 1.1087
2022-03-04 14:13:04 - train: epoch 0173, iter [01000, 05004], lr: 0.005002, loss: 1.2711
2022-03-04 14:13:38 - train: epoch 0173, iter [01100, 05004], lr: 0.005002, loss: 0.9374
2022-03-04 14:14:11 - train: epoch 0173, iter [01200, 05004], lr: 0.005002, loss: 1.0867
2022-03-04 14:14:44 - train: epoch 0173, iter [01300, 05004], lr: 0.005002, loss: 1.1571
2022-03-04 14:15:17 - train: epoch 0173, iter [01400, 05004], lr: 0.005002, loss: 1.1831
2022-03-04 14:15:51 - train: epoch 0173, iter [01500, 05004], lr: 0.005002, loss: 0.9166
2022-03-04 14:16:24 - train: epoch 0173, iter [01600, 05004], lr: 0.005002, loss: 1.1034
2022-03-04 14:16:56 - train: epoch 0173, iter [01700, 05004], lr: 0.005002, loss: 0.9417
2022-03-04 14:17:30 - train: epoch 0173, iter [01800, 05004], lr: 0.005002, loss: 0.9547
2022-03-04 14:18:03 - train: epoch 0173, iter [01900, 05004], lr: 0.005002, loss: 1.0094
2022-03-04 14:18:37 - train: epoch 0173, iter [02000, 05004], lr: 0.005002, loss: 1.3724
2022-03-04 14:19:10 - train: epoch 0173, iter [02100, 05004], lr: 0.005002, loss: 0.9263
2022-03-04 14:19:43 - train: epoch 0173, iter [02200, 05004], lr: 0.005002, loss: 0.9819
2022-03-04 14:20:17 - train: epoch 0173, iter [02300, 05004], lr: 0.005002, loss: 1.0964
2022-03-04 14:20:51 - train: epoch 0173, iter [02400, 05004], lr: 0.005002, loss: 0.9603
2022-03-04 14:21:24 - train: epoch 0173, iter [02500, 05004], lr: 0.005002, loss: 0.9920
2022-03-04 14:21:57 - train: epoch 0173, iter [02600, 05004], lr: 0.005002, loss: 1.1636
2022-03-04 14:22:31 - train: epoch 0173, iter [02700, 05004], lr: 0.005002, loss: 1.0571
2022-03-04 14:23:04 - train: epoch 0173, iter [02800, 05004], lr: 0.005002, loss: 1.0723
2022-03-04 14:23:37 - train: epoch 0173, iter [02900, 05004], lr: 0.005002, loss: 1.0807
2022-03-04 14:24:10 - train: epoch 0173, iter [03000, 05004], lr: 0.005002, loss: 1.0913
2022-03-04 14:24:42 - train: epoch 0173, iter [03100, 05004], lr: 0.005002, loss: 1.0339
2022-03-04 14:25:16 - train: epoch 0173, iter [03200, 05004], lr: 0.005002, loss: 1.1958
2022-03-04 14:25:49 - train: epoch 0173, iter [03300, 05004], lr: 0.005002, loss: 1.0692
2022-03-04 14:26:23 - train: epoch 0173, iter [03400, 05004], lr: 0.005002, loss: 0.9675
2022-03-04 14:26:56 - train: epoch 0173, iter [03500, 05004], lr: 0.005002, loss: 1.1107
2022-03-04 14:27:29 - train: epoch 0173, iter [03600, 05004], lr: 0.005002, loss: 0.9911
2022-03-04 14:28:03 - train: epoch 0173, iter [03700, 05004], lr: 0.005002, loss: 1.0515
2022-03-04 14:28:36 - train: epoch 0173, iter [03800, 05004], lr: 0.005002, loss: 1.0455
2022-03-04 14:29:09 - train: epoch 0173, iter [03900, 05004], lr: 0.005002, loss: 0.8773
2022-03-04 14:29:42 - train: epoch 0173, iter [04000, 05004], lr: 0.005002, loss: 0.7912
2022-03-04 14:30:15 - train: epoch 0173, iter [04100, 05004], lr: 0.005002, loss: 1.0194
2022-03-04 14:30:49 - train: epoch 0173, iter [04200, 05004], lr: 0.005002, loss: 0.9918
2022-03-04 14:31:22 - train: epoch 0173, iter [04300, 05004], lr: 0.005002, loss: 0.9963
2022-03-04 14:31:55 - train: epoch 0173, iter [04400, 05004], lr: 0.005002, loss: 1.0067
2022-03-04 14:32:28 - train: epoch 0173, iter [04500, 05004], lr: 0.005002, loss: 1.0959
2022-03-04 14:33:01 - train: epoch 0173, iter [04600, 05004], lr: 0.005002, loss: 1.0191
2022-03-04 14:33:34 - train: epoch 0173, iter [04700, 05004], lr: 0.005002, loss: 1.0824
2022-03-04 14:34:07 - train: epoch 0173, iter [04800, 05004], lr: 0.005002, loss: 1.0462
2022-03-04 14:34:41 - train: epoch 0173, iter [04900, 05004], lr: 0.005002, loss: 1.0926
2022-03-04 14:35:12 - train: epoch 0173, iter [05000, 05004], lr: 0.005002, loss: 1.0674
2022-03-04 14:35:13 - train: epoch 173, train_loss: 1.0325
2022-03-04 14:36:27 - eval: epoch: 173, acc1: 74.656%, acc5: 92.312%, test_loss: 1.0099, per_image_load_time: 2.032ms, per_image_inference_time: 0.555ms
2022-03-04 14:36:28 - until epoch: 173, best_acc1: 74.656%
2022-03-04 14:36:28 - epoch 174 lr: 0.00465628195747273
2022-03-04 14:37:05 - train: epoch 0174, iter [00100, 05004], lr: 0.004656, loss: 0.8928
2022-03-04 14:37:38 - train: epoch 0174, iter [00200, 05004], lr: 0.004656, loss: 1.0537
2022-03-04 14:38:11 - train: epoch 0174, iter [00300, 05004], lr: 0.004656, loss: 1.0686
2022-03-04 14:38:44 - train: epoch 0174, iter [00400, 05004], lr: 0.004656, loss: 0.9488
2022-03-04 14:39:18 - train: epoch 0174, iter [00500, 05004], lr: 0.004656, loss: 1.0133
2022-03-04 14:39:51 - train: epoch 0174, iter [00600, 05004], lr: 0.004656, loss: 0.9868
2022-03-04 14:40:24 - train: epoch 0174, iter [00700, 05004], lr: 0.004656, loss: 0.9213
2022-03-04 14:40:57 - train: epoch 0174, iter [00800, 05004], lr: 0.004656, loss: 1.1095
2022-03-04 14:41:30 - train: epoch 0174, iter [00900, 05004], lr: 0.004656, loss: 0.9834
2022-03-04 14:42:03 - train: epoch 0174, iter [01000, 05004], lr: 0.004656, loss: 0.9432
2022-03-04 14:42:36 - train: epoch 0174, iter [01100, 05004], lr: 0.004656, loss: 1.0211
2022-03-04 14:43:10 - train: epoch 0174, iter [01200, 05004], lr: 0.004656, loss: 1.1332
2022-03-04 14:43:42 - train: epoch 0174, iter [01300, 05004], lr: 0.004656, loss: 1.0514
2022-03-04 14:44:16 - train: epoch 0174, iter [01400, 05004], lr: 0.004656, loss: 1.0740
2022-03-04 14:44:50 - train: epoch 0174, iter [01500, 05004], lr: 0.004656, loss: 1.0022
2022-03-04 14:45:23 - train: epoch 0174, iter [01600, 05004], lr: 0.004656, loss: 0.9099
2022-03-04 14:45:56 - train: epoch 0174, iter [01700, 05004], lr: 0.004656, loss: 0.8895
2022-03-04 14:46:29 - train: epoch 0174, iter [01800, 05004], lr: 0.004656, loss: 0.9707
2022-03-04 14:47:03 - train: epoch 0174, iter [01900, 05004], lr: 0.004656, loss: 1.0089
2022-03-04 14:47:36 - train: epoch 0174, iter [02000, 05004], lr: 0.004656, loss: 1.1725
2022-03-04 14:48:10 - train: epoch 0174, iter [02100, 05004], lr: 0.004656, loss: 0.9977
2022-03-04 14:48:43 - train: epoch 0174, iter [02200, 05004], lr: 0.004656, loss: 1.0476
2022-03-04 14:49:16 - train: epoch 0174, iter [02300, 05004], lr: 0.004656, loss: 1.1737
2022-03-04 14:49:49 - train: epoch 0174, iter [02400, 05004], lr: 0.004656, loss: 1.0198
2022-03-04 14:50:22 - train: epoch 0174, iter [02500, 05004], lr: 0.004656, loss: 0.9393
2022-03-04 14:50:56 - train: epoch 0174, iter [02600, 05004], lr: 0.004656, loss: 0.9209
2022-03-04 14:51:30 - train: epoch 0174, iter [02700, 05004], lr: 0.004656, loss: 0.8202
2022-03-04 14:52:03 - train: epoch 0174, iter [02800, 05004], lr: 0.004656, loss: 1.0956
2022-03-04 14:52:36 - train: epoch 0174, iter [02900, 05004], lr: 0.004656, loss: 1.0504
2022-03-04 14:53:09 - train: epoch 0174, iter [03000, 05004], lr: 0.004656, loss: 0.8407
2022-03-04 14:53:43 - train: epoch 0174, iter [03100, 05004], lr: 0.004656, loss: 1.0845
2022-03-04 14:54:15 - train: epoch 0174, iter [03200, 05004], lr: 0.004656, loss: 0.9556
2022-03-04 14:54:49 - train: epoch 0174, iter [03300, 05004], lr: 0.004656, loss: 1.0221
2022-03-04 14:55:22 - train: epoch 0174, iter [03400, 05004], lr: 0.004656, loss: 0.9069
2022-03-04 14:55:55 - train: epoch 0174, iter [03500, 05004], lr: 0.004656, loss: 1.0585
2022-03-04 14:56:28 - train: epoch 0174, iter [03600, 05004], lr: 0.004656, loss: 1.1814
2022-03-04 14:57:01 - train: epoch 0174, iter [03700, 05004], lr: 0.004656, loss: 0.9483
2022-03-04 14:57:34 - train: epoch 0174, iter [03800, 05004], lr: 0.004656, loss: 0.9481
2022-03-04 14:58:08 - train: epoch 0174, iter [03900, 05004], lr: 0.004656, loss: 1.1078
2022-03-04 14:58:41 - train: epoch 0174, iter [04000, 05004], lr: 0.004656, loss: 1.0820
2022-03-04 14:59:13 - train: epoch 0174, iter [04100, 05004], lr: 0.004656, loss: 0.9598
2022-03-04 14:59:46 - train: epoch 0174, iter [04200, 05004], lr: 0.004656, loss: 1.0682
2022-03-04 15:00:20 - train: epoch 0174, iter [04300, 05004], lr: 0.004656, loss: 1.0620
2022-03-04 15:00:53 - train: epoch 0174, iter [04400, 05004], lr: 0.004656, loss: 1.1362
2022-03-04 15:01:26 - train: epoch 0174, iter [04500, 05004], lr: 0.004656, loss: 1.1969
2022-03-04 15:01:59 - train: epoch 0174, iter [04600, 05004], lr: 0.004656, loss: 0.8435
2022-03-04 15:02:32 - train: epoch 0174, iter [04700, 05004], lr: 0.004656, loss: 1.3410
2022-03-04 15:03:05 - train: epoch 0174, iter [04800, 05004], lr: 0.004656, loss: 1.1975
2022-03-04 15:03:38 - train: epoch 0174, iter [04900, 05004], lr: 0.004656, loss: 1.0359
2022-03-04 15:04:10 - train: epoch 0174, iter [05000, 05004], lr: 0.004656, loss: 1.2132
2022-03-04 15:04:11 - train: epoch 174, train_loss: 1.0145
2022-03-04 15:05:26 - eval: epoch: 174, acc1: 74.824%, acc5: 92.344%, test_loss: 1.0072, per_image_load_time: 2.333ms, per_image_inference_time: 0.531ms
2022-03-04 15:05:26 - until epoch: 174, best_acc1: 74.824%
2022-03-04 15:05:26 - epoch 175 lr: 0.004322727117869951
2022-03-04 15:06:04 - train: epoch 0175, iter [00100, 05004], lr: 0.004323, loss: 1.0739
2022-03-04 15:06:37 - train: epoch 0175, iter [00200, 05004], lr: 0.004323, loss: 1.0529
2022-03-04 15:07:10 - train: epoch 0175, iter [00300, 05004], lr: 0.004323, loss: 0.8588
2022-03-04 15:07:44 - train: epoch 0175, iter [00400, 05004], lr: 0.004323, loss: 0.9628
2022-03-04 15:08:18 - train: epoch 0175, iter [00500, 05004], lr: 0.004323, loss: 0.9183
2022-03-04 15:08:51 - train: epoch 0175, iter [00600, 05004], lr: 0.004323, loss: 0.9046
2022-03-04 15:09:24 - train: epoch 0175, iter [00700, 05004], lr: 0.004323, loss: 1.1432
2022-03-04 15:09:57 - train: epoch 0175, iter [00800, 05004], lr: 0.004323, loss: 1.2214
2022-03-04 15:10:31 - train: epoch 0175, iter [00900, 05004], lr: 0.004323, loss: 1.0742
2022-03-04 15:11:03 - train: epoch 0175, iter [01000, 05004], lr: 0.004323, loss: 1.0614
2022-03-04 15:11:36 - train: epoch 0175, iter [01100, 05004], lr: 0.004323, loss: 0.8091
2022-03-04 15:12:10 - train: epoch 0175, iter [01200, 05004], lr: 0.004323, loss: 1.1415
2022-03-04 15:12:43 - train: epoch 0175, iter [01300, 05004], lr: 0.004323, loss: 1.0870
2022-03-04 15:13:16 - train: epoch 0175, iter [01400, 05004], lr: 0.004323, loss: 1.0929
2022-03-04 15:13:49 - train: epoch 0175, iter [01500, 05004], lr: 0.004323, loss: 1.0639
2022-03-04 15:14:22 - train: epoch 0175, iter [01600, 05004], lr: 0.004323, loss: 0.9603
2022-03-04 15:14:54 - train: epoch 0175, iter [01700, 05004], lr: 0.004323, loss: 1.0379
2022-03-04 15:15:27 - train: epoch 0175, iter [01800, 05004], lr: 0.004323, loss: 1.0775
2022-03-04 15:16:01 - train: epoch 0175, iter [01900, 05004], lr: 0.004323, loss: 0.8848
2022-03-04 15:16:34 - train: epoch 0175, iter [02000, 05004], lr: 0.004323, loss: 0.8903
2022-03-04 15:17:08 - train: epoch 0175, iter [02100, 05004], lr: 0.004323, loss: 0.8835
2022-03-04 15:17:40 - train: epoch 0175, iter [02200, 05004], lr: 0.004323, loss: 0.9047
2022-03-04 15:18:14 - train: epoch 0175, iter [02300, 05004], lr: 0.004323, loss: 1.0331
2022-03-04 15:18:47 - train: epoch 0175, iter [02400, 05004], lr: 0.004323, loss: 1.0568
2022-03-04 15:19:20 - train: epoch 0175, iter [02500, 05004], lr: 0.004323, loss: 1.0063
2022-03-04 15:19:53 - train: epoch 0175, iter [02600, 05004], lr: 0.004323, loss: 0.9613
2022-03-04 15:20:27 - train: epoch 0175, iter [02700, 05004], lr: 0.004323, loss: 1.0216
2022-03-04 15:20:59 - train: epoch 0175, iter [02800, 05004], lr: 0.004323, loss: 0.9338
2022-03-04 15:21:32 - train: epoch 0175, iter [02900, 05004], lr: 0.004323, loss: 1.0097
2022-03-04 15:22:05 - train: epoch 0175, iter [03000, 05004], lr: 0.004323, loss: 1.0346
2022-03-04 15:22:39 - train: epoch 0175, iter [03100, 05004], lr: 0.004323, loss: 0.9222
2022-03-04 15:23:12 - train: epoch 0175, iter [03200, 05004], lr: 0.004323, loss: 1.0571
2022-03-04 15:23:45 - train: epoch 0175, iter [03300, 05004], lr: 0.004323, loss: 0.9334
2022-03-04 15:24:18 - train: epoch 0175, iter [03400, 05004], lr: 0.004323, loss: 1.1440
2022-03-04 15:24:50 - train: epoch 0175, iter [03500, 05004], lr: 0.004323, loss: 0.8525
2022-03-04 15:25:24 - train: epoch 0175, iter [03600, 05004], lr: 0.004323, loss: 0.9452
2022-03-04 15:25:57 - train: epoch 0175, iter [03700, 05004], lr: 0.004323, loss: 1.1132
2022-03-04 15:26:30 - train: epoch 0175, iter [03800, 05004], lr: 0.004323, loss: 0.9073
2022-03-04 15:27:03 - train: epoch 0175, iter [03900, 05004], lr: 0.004323, loss: 1.0103
2022-03-04 15:27:37 - train: epoch 0175, iter [04000, 05004], lr: 0.004323, loss: 0.8879
2022-03-04 15:28:10 - train: epoch 0175, iter [04100, 05004], lr: 0.004323, loss: 1.1121
2022-03-04 15:28:43 - train: epoch 0175, iter [04200, 05004], lr: 0.004323, loss: 0.8396
2022-03-04 15:29:16 - train: epoch 0175, iter [04300, 05004], lr: 0.004323, loss: 0.8532
2022-03-04 15:29:50 - train: epoch 0175, iter [04400, 05004], lr: 0.004323, loss: 0.9274
2022-03-04 15:30:23 - train: epoch 0175, iter [04500, 05004], lr: 0.004323, loss: 1.0689
2022-03-04 15:30:56 - train: epoch 0175, iter [04600, 05004], lr: 0.004323, loss: 1.0184
2022-03-04 15:31:29 - train: epoch 0175, iter [04700, 05004], lr: 0.004323, loss: 1.0953
2022-03-04 15:32:03 - train: epoch 0175, iter [04800, 05004], lr: 0.004323, loss: 1.2272
2022-03-04 15:32:36 - train: epoch 0175, iter [04900, 05004], lr: 0.004323, loss: 1.1453
2022-03-04 15:33:08 - train: epoch 0175, iter [05000, 05004], lr: 0.004323, loss: 0.8442
2022-03-04 15:33:09 - train: epoch 175, train_loss: 0.9981
2022-03-04 15:34:23 - eval: epoch: 175, acc1: 75.008%, acc5: 92.502%, test_loss: 0.9973, per_image_load_time: 2.355ms, per_image_inference_time: 0.518ms
2022-03-04 15:34:24 - until epoch: 175, best_acc1: 75.008%
2022-03-04 15:34:24 - epoch 176 lr: 0.0040010278170587886
2022-03-04 15:35:02 - train: epoch 0176, iter [00100, 05004], lr: 0.004001, loss: 0.9113
2022-03-04 15:35:34 - train: epoch 0176, iter [00200, 05004], lr: 0.004001, loss: 0.8005
2022-03-04 15:36:08 - train: epoch 0176, iter [00300, 05004], lr: 0.004001, loss: 1.0389
2022-03-04 15:36:41 - train: epoch 0176, iter [00400, 05004], lr: 0.004001, loss: 0.9233
2022-03-04 15:37:15 - train: epoch 0176, iter [00500, 05004], lr: 0.004001, loss: 1.0244
2022-03-04 15:37:48 - train: epoch 0176, iter [00600, 05004], lr: 0.004001, loss: 1.0350
2022-03-04 15:38:21 - train: epoch 0176, iter [00700, 05004], lr: 0.004001, loss: 0.9428
2022-03-04 15:38:55 - train: epoch 0176, iter [00800, 05004], lr: 0.004001, loss: 1.1393
2022-03-04 15:39:28 - train: epoch 0176, iter [00900, 05004], lr: 0.004001, loss: 0.9885
2022-03-04 15:40:02 - train: epoch 0176, iter [01000, 05004], lr: 0.004001, loss: 0.8871
2022-03-04 15:40:35 - train: epoch 0176, iter [01100, 05004], lr: 0.004001, loss: 0.8528
2022-03-04 15:41:08 - train: epoch 0176, iter [01200, 05004], lr: 0.004001, loss: 1.0990
2022-03-04 15:41:42 - train: epoch 0176, iter [01300, 05004], lr: 0.004001, loss: 1.0078
2022-03-04 15:42:15 - train: epoch 0176, iter [01400, 05004], lr: 0.004001, loss: 0.9971
2022-03-04 15:42:49 - train: epoch 0176, iter [01500, 05004], lr: 0.004001, loss: 0.8502
2022-03-04 15:43:21 - train: epoch 0176, iter [01600, 05004], lr: 0.004001, loss: 0.9382
2022-03-04 15:43:54 - train: epoch 0176, iter [01700, 05004], lr: 0.004001, loss: 1.0549
2022-03-04 15:44:28 - train: epoch 0176, iter [01800, 05004], lr: 0.004001, loss: 1.0882
2022-03-04 15:45:01 - train: epoch 0176, iter [01900, 05004], lr: 0.004001, loss: 0.9254
2022-03-04 15:45:33 - train: epoch 0176, iter [02000, 05004], lr: 0.004001, loss: 0.9090
2022-03-04 15:46:07 - train: epoch 0176, iter [02100, 05004], lr: 0.004001, loss: 0.9228
2022-03-04 15:46:41 - train: epoch 0176, iter [02200, 05004], lr: 0.004001, loss: 0.9376
2022-03-04 15:47:14 - train: epoch 0176, iter [02300, 05004], lr: 0.004001, loss: 0.8760
2022-03-04 15:47:48 - train: epoch 0176, iter [02400, 05004], lr: 0.004001, loss: 0.9902
2022-03-04 15:48:21 - train: epoch 0176, iter [02500, 05004], lr: 0.004001, loss: 1.0763
2022-03-04 15:48:54 - train: epoch 0176, iter [02600, 05004], lr: 0.004001, loss: 0.8160
2022-03-04 15:49:28 - train: epoch 0176, iter [02700, 05004], lr: 0.004001, loss: 1.0220
2022-03-04 15:50:01 - train: epoch 0176, iter [02800, 05004], lr: 0.004001, loss: 0.9795
2022-03-04 15:50:33 - train: epoch 0176, iter [02900, 05004], lr: 0.004001, loss: 1.0733
2022-03-04 15:51:07 - train: epoch 0176, iter [03000, 05004], lr: 0.004001, loss: 0.8960
2022-03-04 15:51:40 - train: epoch 0176, iter [03100, 05004], lr: 0.004001, loss: 0.8784
2022-03-04 15:52:13 - train: epoch 0176, iter [03200, 05004], lr: 0.004001, loss: 0.9708
2022-03-04 15:52:47 - train: epoch 0176, iter [03300, 05004], lr: 0.004001, loss: 0.9343
2022-03-04 15:53:19 - train: epoch 0176, iter [03400, 05004], lr: 0.004001, loss: 1.2787
2022-03-04 15:53:53 - train: epoch 0176, iter [03500, 05004], lr: 0.004001, loss: 0.9845
2022-03-04 15:54:26 - train: epoch 0176, iter [03600, 05004], lr: 0.004001, loss: 1.1412
2022-03-04 15:54:59 - train: epoch 0176, iter [03700, 05004], lr: 0.004001, loss: 0.8399
2022-03-04 15:55:32 - train: epoch 0176, iter [03800, 05004], lr: 0.004001, loss: 1.0089
2022-03-04 15:56:05 - train: epoch 0176, iter [03900, 05004], lr: 0.004001, loss: 1.0267
2022-03-04 15:56:38 - train: epoch 0176, iter [04000, 05004], lr: 0.004001, loss: 1.1193
2022-03-04 15:57:11 - train: epoch 0176, iter [04100, 05004], lr: 0.004001, loss: 0.9240
2022-03-04 15:57:45 - train: epoch 0176, iter [04200, 05004], lr: 0.004001, loss: 0.9735
2022-03-04 15:58:18 - train: epoch 0176, iter [04300, 05004], lr: 0.004001, loss: 0.9837
2022-03-04 15:58:51 - train: epoch 0176, iter [04400, 05004], lr: 0.004001, loss: 0.9135
2022-03-04 15:59:25 - train: epoch 0176, iter [04500, 05004], lr: 0.004001, loss: 1.0028
2022-03-04 15:59:58 - train: epoch 0176, iter [04600, 05004], lr: 0.004001, loss: 0.8951
2022-03-04 16:00:32 - train: epoch 0176, iter [04700, 05004], lr: 0.004001, loss: 1.0452
2022-03-04 16:01:05 - train: epoch 0176, iter [04800, 05004], lr: 0.004001, loss: 0.9318
2022-03-04 16:01:38 - train: epoch 0176, iter [04900, 05004], lr: 0.004001, loss: 0.9549
2022-03-04 16:02:09 - train: epoch 0176, iter [05000, 05004], lr: 0.004001, loss: 1.0126
2022-03-04 16:02:10 - train: epoch 176, train_loss: 0.9796
2022-03-04 16:03:24 - eval: epoch: 176, acc1: 75.064%, acc5: 92.564%, test_loss: 0.9956, per_image_load_time: 2.350ms, per_image_inference_time: 0.508ms
2022-03-04 16:03:25 - until epoch: 176, best_acc1: 75.064%
2022-03-04 16:03:25 - epoch 177 lr: 0.003691267552111183
2022-03-04 16:04:03 - train: epoch 0177, iter [00100, 05004], lr: 0.003691, loss: 0.8584
2022-03-04 16:04:36 - train: epoch 0177, iter [00200, 05004], lr: 0.003691, loss: 1.0472
2022-03-04 16:05:09 - train: epoch 0177, iter [00300, 05004], lr: 0.003691, loss: 1.2041
2022-03-04 16:05:43 - train: epoch 0177, iter [00400, 05004], lr: 0.003691, loss: 1.1777
2022-03-04 16:06:16 - train: epoch 0177, iter [00500, 05004], lr: 0.003691, loss: 1.1169
2022-03-04 16:06:49 - train: epoch 0177, iter [00600, 05004], lr: 0.003691, loss: 0.8841
2022-03-04 16:07:22 - train: epoch 0177, iter [00700, 05004], lr: 0.003691, loss: 0.9333
2022-03-04 16:07:56 - train: epoch 0177, iter [00800, 05004], lr: 0.003691, loss: 1.0181
2022-03-04 16:08:29 - train: epoch 0177, iter [00900, 05004], lr: 0.003691, loss: 0.8695
2022-03-04 16:09:02 - train: epoch 0177, iter [01000, 05004], lr: 0.003691, loss: 1.0488
2022-03-04 16:09:35 - train: epoch 0177, iter [01100, 05004], lr: 0.003691, loss: 0.9301
2022-03-04 16:10:08 - train: epoch 0177, iter [01200, 05004], lr: 0.003691, loss: 0.9729
2022-03-04 16:10:41 - train: epoch 0177, iter [01300, 05004], lr: 0.003691, loss: 0.9901
2022-03-04 16:11:15 - train: epoch 0177, iter [01400, 05004], lr: 0.003691, loss: 0.9815
2022-03-04 16:11:48 - train: epoch 0177, iter [01500, 05004], lr: 0.003691, loss: 1.1475
2022-03-04 16:12:21 - train: epoch 0177, iter [01600, 05004], lr: 0.003691, loss: 0.9737
2022-03-04 16:12:55 - train: epoch 0177, iter [01700, 05004], lr: 0.003691, loss: 0.9826
2022-03-04 16:13:27 - train: epoch 0177, iter [01800, 05004], lr: 0.003691, loss: 1.0906
2022-03-04 16:14:00 - train: epoch 0177, iter [01900, 05004], lr: 0.003691, loss: 0.8501
2022-03-04 16:14:34 - train: epoch 0177, iter [02000, 05004], lr: 0.003691, loss: 0.8119
2022-03-04 16:15:07 - train: epoch 0177, iter [02100, 05004], lr: 0.003691, loss: 0.9892
2022-03-04 16:15:40 - train: epoch 0177, iter [02200, 05004], lr: 0.003691, loss: 1.1398
2022-03-04 16:16:13 - train: epoch 0177, iter [02300, 05004], lr: 0.003691, loss: 0.9140
2022-03-04 16:16:46 - train: epoch 0177, iter [02400, 05004], lr: 0.003691, loss: 1.0537
2022-03-04 16:17:19 - train: epoch 0177, iter [02500, 05004], lr: 0.003691, loss: 0.9668
2022-03-04 16:17:52 - train: epoch 0177, iter [02600, 05004], lr: 0.003691, loss: 0.8164
2022-03-04 16:18:25 - train: epoch 0177, iter [02700, 05004], lr: 0.003691, loss: 0.7851
2022-03-04 16:18:58 - train: epoch 0177, iter [02800, 05004], lr: 0.003691, loss: 0.8034
2022-03-04 16:19:31 - train: epoch 0177, iter [02900, 05004], lr: 0.003691, loss: 1.1107
2022-03-04 16:20:04 - train: epoch 0177, iter [03000, 05004], lr: 0.003691, loss: 0.8794
2022-03-04 16:20:38 - train: epoch 0177, iter [03100, 05004], lr: 0.003691, loss: 0.8750
2022-03-04 16:21:11 - train: epoch 0177, iter [03200, 05004], lr: 0.003691, loss: 0.9345
2022-03-04 16:21:45 - train: epoch 0177, iter [03300, 05004], lr: 0.003691, loss: 0.8681
2022-03-04 16:22:17 - train: epoch 0177, iter [03400, 05004], lr: 0.003691, loss: 1.0484
2022-03-04 16:22:51 - train: epoch 0177, iter [03500, 05004], lr: 0.003691, loss: 0.6981
2022-03-04 16:23:24 - train: epoch 0177, iter [03600, 05004], lr: 0.003691, loss: 1.0234
2022-03-04 16:23:58 - train: epoch 0177, iter [03700, 05004], lr: 0.003691, loss: 1.0071
2022-03-04 16:24:31 - train: epoch 0177, iter [03800, 05004], lr: 0.003691, loss: 1.0168
2022-03-04 16:25:04 - train: epoch 0177, iter [03900, 05004], lr: 0.003691, loss: 0.7476
2022-03-04 16:25:38 - train: epoch 0177, iter [04000, 05004], lr: 0.003691, loss: 0.9513
2022-03-04 16:26:11 - train: epoch 0177, iter [04100, 05004], lr: 0.003691, loss: 0.9859
2022-03-04 16:26:44 - train: epoch 0177, iter [04200, 05004], lr: 0.003691, loss: 1.2599
2022-03-04 16:27:17 - train: epoch 0177, iter [04300, 05004], lr: 0.003691, loss: 1.1637
2022-03-04 16:27:50 - train: epoch 0177, iter [04400, 05004], lr: 0.003691, loss: 0.8726
2022-03-04 16:28:24 - train: epoch 0177, iter [04500, 05004], lr: 0.003691, loss: 0.9499
2022-03-04 16:28:57 - train: epoch 0177, iter [04600, 05004], lr: 0.003691, loss: 1.1119
2022-03-04 16:29:30 - train: epoch 0177, iter [04700, 05004], lr: 0.003691, loss: 0.7836
2022-03-04 16:30:03 - train: epoch 0177, iter [04800, 05004], lr: 0.003691, loss: 1.0309
2022-03-04 16:30:37 - train: epoch 0177, iter [04900, 05004], lr: 0.003691, loss: 1.0506
2022-03-04 16:31:08 - train: epoch 0177, iter [05000, 05004], lr: 0.003691, loss: 1.1775
2022-03-04 16:31:09 - train: epoch 177, train_loss: 0.9653
2022-03-04 16:32:23 - eval: epoch: 177, acc1: 75.420%, acc5: 92.552%, test_loss: 0.9846, per_image_load_time: 2.352ms, per_image_inference_time: 0.500ms
2022-03-04 16:32:24 - until epoch: 177, best_acc1: 75.420%
2022-03-04 16:32:24 - epoch 178 lr: 0.003393526721321616
2022-03-04 16:33:01 - train: epoch 0178, iter [00100, 05004], lr: 0.003394, loss: 0.8265
2022-03-04 16:33:34 - train: epoch 0178, iter [00200, 05004], lr: 0.003394, loss: 1.0355
2022-03-04 16:34:07 - train: epoch 0178, iter [00300, 05004], lr: 0.003394, loss: 1.0825
2022-03-04 16:34:40 - train: epoch 0178, iter [00400, 05004], lr: 0.003394, loss: 0.9390
2022-03-04 16:35:13 - train: epoch 0178, iter [00500, 05004], lr: 0.003394, loss: 0.7618
2022-03-04 16:35:47 - train: epoch 0178, iter [00600, 05004], lr: 0.003394, loss: 0.9340
2022-03-04 16:36:20 - train: epoch 0178, iter [00700, 05004], lr: 0.003394, loss: 0.7393
2022-03-04 16:36:53 - train: epoch 0178, iter [00800, 05004], lr: 0.003394, loss: 1.0175
2022-03-04 16:37:27 - train: epoch 0178, iter [00900, 05004], lr: 0.003394, loss: 1.0591
2022-03-04 16:38:00 - train: epoch 0178, iter [01000, 05004], lr: 0.003394, loss: 0.8970
2022-03-04 16:38:34 - train: epoch 0178, iter [01100, 05004], lr: 0.003394, loss: 0.9114
2022-03-04 16:39:07 - train: epoch 0178, iter [01200, 05004], lr: 0.003394, loss: 0.9440
2022-03-04 16:39:40 - train: epoch 0178, iter [01300, 05004], lr: 0.003394, loss: 1.0163
2022-03-04 16:40:13 - train: epoch 0178, iter [01400, 05004], lr: 0.003394, loss: 0.9096
2022-03-04 16:40:47 - train: epoch 0178, iter [01500, 05004], lr: 0.003394, loss: 1.0503
2022-03-04 16:41:20 - train: epoch 0178, iter [01600, 05004], lr: 0.003394, loss: 0.8516
2022-03-04 16:41:54 - train: epoch 0178, iter [01700, 05004], lr: 0.003394, loss: 1.0104
2022-03-04 16:42:27 - train: epoch 0178, iter [01800, 05004], lr: 0.003394, loss: 0.9807
2022-03-04 16:43:00 - train: epoch 0178, iter [01900, 05004], lr: 0.003394, loss: 0.9735
2022-03-04 16:43:34 - train: epoch 0178, iter [02000, 05004], lr: 0.003394, loss: 0.9082
2022-03-04 16:44:07 - train: epoch 0178, iter [02100, 05004], lr: 0.003394, loss: 0.8408
2022-03-04 16:44:40 - train: epoch 0178, iter [02200, 05004], lr: 0.003394, loss: 0.8560
2022-03-04 16:45:13 - train: epoch 0178, iter [02300, 05004], lr: 0.003394, loss: 1.0133
2022-03-04 16:45:47 - train: epoch 0178, iter [02400, 05004], lr: 0.003394, loss: 0.9052
2022-03-04 16:46:20 - train: epoch 0178, iter [02500, 05004], lr: 0.003394, loss: 0.7457
2022-03-04 16:46:54 - train: epoch 0178, iter [02600, 05004], lr: 0.003394, loss: 0.9650
2022-03-04 16:47:27 - train: epoch 0178, iter [02700, 05004], lr: 0.003394, loss: 0.9693
2022-03-04 16:48:01 - train: epoch 0178, iter [02800, 05004], lr: 0.003394, loss: 0.7304
2022-03-04 16:48:34 - train: epoch 0178, iter [02900, 05004], lr: 0.003394, loss: 1.0047
2022-03-04 16:49:07 - train: epoch 0178, iter [03000, 05004], lr: 0.003394, loss: 0.9082
2022-03-04 16:49:40 - train: epoch 0178, iter [03100, 05004], lr: 0.003394, loss: 0.7887
2022-03-04 16:50:14 - train: epoch 0178, iter [03200, 05004], lr: 0.003394, loss: 0.9651
2022-03-04 16:50:46 - train: epoch 0178, iter [03300, 05004], lr: 0.003394, loss: 0.8363
2022-03-04 16:51:21 - train: epoch 0178, iter [03400, 05004], lr: 0.003394, loss: 0.9100
2022-03-04 16:51:53 - train: epoch 0178, iter [03500, 05004], lr: 0.003394, loss: 0.9697
2022-03-04 16:52:26 - train: epoch 0178, iter [03600, 05004], lr: 0.003394, loss: 0.8571
2022-03-04 16:52:59 - train: epoch 0178, iter [03700, 05004], lr: 0.003394, loss: 0.9324
2022-03-04 16:53:32 - train: epoch 0178, iter [03800, 05004], lr: 0.003394, loss: 1.0088
2022-03-04 16:54:05 - train: epoch 0178, iter [03900, 05004], lr: 0.003394, loss: 1.0917
2022-03-04 16:54:39 - train: epoch 0178, iter [04000, 05004], lr: 0.003394, loss: 0.9790
2022-03-04 16:55:12 - train: epoch 0178, iter [04100, 05004], lr: 0.003394, loss: 0.9802
2022-03-04 16:55:45 - train: epoch 0178, iter [04200, 05004], lr: 0.003394, loss: 0.9433
2022-03-04 16:56:18 - train: epoch 0178, iter [04300, 05004], lr: 0.003394, loss: 0.9515
2022-03-04 16:56:51 - train: epoch 0178, iter [04400, 05004], lr: 0.003394, loss: 0.9031
2022-03-04 16:57:25 - train: epoch 0178, iter [04500, 05004], lr: 0.003394, loss: 1.0826
2022-03-04 16:57:59 - train: epoch 0178, iter [04600, 05004], lr: 0.003394, loss: 0.9251
2022-03-04 16:58:31 - train: epoch 0178, iter [04700, 05004], lr: 0.003394, loss: 0.8995
2022-03-04 16:59:05 - train: epoch 0178, iter [04800, 05004], lr: 0.003394, loss: 0.7726
2022-03-04 16:59:39 - train: epoch 0178, iter [04900, 05004], lr: 0.003394, loss: 1.1094
2022-03-04 17:00:10 - train: epoch 0178, iter [05000, 05004], lr: 0.003394, loss: 0.9971
2022-03-04 17:00:12 - train: epoch 178, train_loss: 0.9455
2022-03-04 17:01:26 - eval: epoch: 178, acc1: 75.374%, acc5: 92.804%, test_loss: 0.9862, per_image_load_time: 2.347ms, per_image_inference_time: 0.537ms
2022-03-04 17:01:26 - until epoch: 178, best_acc1: 75.420%
2022-03-04 17:01:26 - epoch 179 lr: 0.0031078826033397846
2022-03-04 17:02:05 - train: epoch 0179, iter [00100, 05004], lr: 0.003108, loss: 1.0209
2022-03-04 17:02:37 - train: epoch 0179, iter [00200, 05004], lr: 0.003108, loss: 0.9253
2022-03-04 17:03:10 - train: epoch 0179, iter [00300, 05004], lr: 0.003108, loss: 1.1125
2022-03-04 17:03:43 - train: epoch 0179, iter [00400, 05004], lr: 0.003108, loss: 0.9304
2022-03-04 17:04:16 - train: epoch 0179, iter [00500, 05004], lr: 0.003108, loss: 0.9507
2022-03-04 17:04:49 - train: epoch 0179, iter [00600, 05004], lr: 0.003108, loss: 0.9874
2022-03-04 17:05:23 - train: epoch 0179, iter [00700, 05004], lr: 0.003108, loss: 0.7725
2022-03-04 17:05:56 - train: epoch 0179, iter [00800, 05004], lr: 0.003108, loss: 0.8421
2022-03-04 17:06:29 - train: epoch 0179, iter [00900, 05004], lr: 0.003108, loss: 0.8426
2022-03-04 17:07:02 - train: epoch 0179, iter [01000, 05004], lr: 0.003108, loss: 0.8532
2022-03-04 17:07:36 - train: epoch 0179, iter [01100, 05004], lr: 0.003108, loss: 0.8376
2022-03-04 17:08:10 - train: epoch 0179, iter [01200, 05004], lr: 0.003108, loss: 1.0234
2022-03-04 17:08:43 - train: epoch 0179, iter [01300, 05004], lr: 0.003108, loss: 0.9328
2022-03-04 17:09:16 - train: epoch 0179, iter [01400, 05004], lr: 0.003108, loss: 0.9343
2022-03-04 17:09:49 - train: epoch 0179, iter [01500, 05004], lr: 0.003108, loss: 1.0729
2022-03-04 17:10:23 - train: epoch 0179, iter [01600, 05004], lr: 0.003108, loss: 0.7610
2022-03-04 17:10:55 - train: epoch 0179, iter [01700, 05004], lr: 0.003108, loss: 0.9099
2022-03-04 17:11:29 - train: epoch 0179, iter [01800, 05004], lr: 0.003108, loss: 0.9606
2022-03-04 17:12:02 - train: epoch 0179, iter [01900, 05004], lr: 0.003108, loss: 0.8560
2022-03-04 17:12:35 - train: epoch 0179, iter [02000, 05004], lr: 0.003108, loss: 0.9955
2022-03-04 17:13:08 - train: epoch 0179, iter [02100, 05004], lr: 0.003108, loss: 0.8836
2022-03-04 17:13:41 - train: epoch 0179, iter [02200, 05004], lr: 0.003108, loss: 1.0587
2022-03-04 17:14:15 - train: epoch 0179, iter [02300, 05004], lr: 0.003108, loss: 0.8381
2022-03-04 17:14:47 - train: epoch 0179, iter [02400, 05004], lr: 0.003108, loss: 0.9562
2022-03-04 17:15:20 - train: epoch 0179, iter [02500, 05004], lr: 0.003108, loss: 1.1053
2022-03-04 17:15:53 - train: epoch 0179, iter [02600, 05004], lr: 0.003108, loss: 0.8476
2022-03-04 17:16:25 - train: epoch 0179, iter [02700, 05004], lr: 0.003108, loss: 1.1498
2022-03-04 17:16:59 - train: epoch 0179, iter [02800, 05004], lr: 0.003108, loss: 1.0146
2022-03-04 17:17:32 - train: epoch 0179, iter [02900, 05004], lr: 0.003108, loss: 1.0324
2022-03-04 17:18:05 - train: epoch 0179, iter [03000, 05004], lr: 0.003108, loss: 0.8220
2022-03-04 17:18:39 - train: epoch 0179, iter [03100, 05004], lr: 0.003108, loss: 1.0337
2022-03-04 17:19:11 - train: epoch 0179, iter [03200, 05004], lr: 0.003108, loss: 1.0488
2022-03-04 17:19:44 - train: epoch 0179, iter [03300, 05004], lr: 0.003108, loss: 1.0080
2022-03-04 17:20:18 - train: epoch 0179, iter [03400, 05004], lr: 0.003108, loss: 1.0986
2022-03-04 17:20:51 - train: epoch 0179, iter [03500, 05004], lr: 0.003108, loss: 0.9772
2022-03-04 17:21:24 - train: epoch 0179, iter [03600, 05004], lr: 0.003108, loss: 1.0524
2022-03-04 17:21:57 - train: epoch 0179, iter [03700, 05004], lr: 0.003108, loss: 0.8402
2022-03-04 17:22:30 - train: epoch 0179, iter [03800, 05004], lr: 0.003108, loss: 0.9146
2022-03-04 17:23:04 - train: epoch 0179, iter [03900, 05004], lr: 0.003108, loss: 1.0272
2022-03-04 17:23:37 - train: epoch 0179, iter [04000, 05004], lr: 0.003108, loss: 0.9550
2022-03-04 17:24:10 - train: epoch 0179, iter [04100, 05004], lr: 0.003108, loss: 0.9782
2022-03-04 17:24:43 - train: epoch 0179, iter [04200, 05004], lr: 0.003108, loss: 1.0387
2022-03-04 17:25:16 - train: epoch 0179, iter [04300, 05004], lr: 0.003108, loss: 0.8590
2022-03-04 17:25:50 - train: epoch 0179, iter [04400, 05004], lr: 0.003108, loss: 1.0795
2022-03-04 17:26:23 - train: epoch 0179, iter [04500, 05004], lr: 0.003108, loss: 0.9664
2022-03-04 17:26:56 - train: epoch 0179, iter [04600, 05004], lr: 0.003108, loss: 0.9584
2022-03-04 17:27:30 - train: epoch 0179, iter [04700, 05004], lr: 0.003108, loss: 0.9105
2022-03-04 17:28:03 - train: epoch 0179, iter [04800, 05004], lr: 0.003108, loss: 0.8353
2022-03-04 17:28:35 - train: epoch 0179, iter [04900, 05004], lr: 0.003108, loss: 0.9794
2022-03-04 17:29:08 - train: epoch 0179, iter [05000, 05004], lr: 0.003108, loss: 0.8732
2022-03-04 17:29:09 - train: epoch 179, train_loss: 0.9322
2022-03-04 17:30:23 - eval: epoch: 179, acc1: 75.534%, acc5: 92.614%, test_loss: 0.9812, per_image_load_time: 2.418ms, per_image_inference_time: 0.482ms
2022-03-04 17:30:24 - until epoch: 179, best_acc1: 75.534%
2022-03-04 17:30:24 - epoch 180 lr: 0.0028344093371128424
2022-03-04 17:31:02 - train: epoch 0180, iter [00100, 05004], lr: 0.002834, loss: 0.9089
2022-03-04 17:31:36 - train: epoch 0180, iter [00200, 05004], lr: 0.002834, loss: 0.9139
2022-03-04 17:32:09 - train: epoch 0180, iter [00300, 05004], lr: 0.002834, loss: 0.9007
2022-03-04 17:32:43 - train: epoch 0180, iter [00400, 05004], lr: 0.002834, loss: 0.8232
2022-03-04 17:33:15 - train: epoch 0180, iter [00500, 05004], lr: 0.002834, loss: 0.8668
2022-03-04 17:33:50 - train: epoch 0180, iter [00600, 05004], lr: 0.002834, loss: 0.8134
2022-03-04 17:34:23 - train: epoch 0180, iter [00700, 05004], lr: 0.002834, loss: 0.9396
2022-03-04 17:34:56 - train: epoch 0180, iter [00800, 05004], lr: 0.002834, loss: 0.7962
2022-03-04 17:35:29 - train: epoch 0180, iter [00900, 05004], lr: 0.002834, loss: 0.9125
2022-03-04 17:36:02 - train: epoch 0180, iter [01000, 05004], lr: 0.002834, loss: 0.8758
2022-03-04 17:36:36 - train: epoch 0180, iter [01100, 05004], lr: 0.002834, loss: 0.9650
2022-03-04 17:37:10 - train: epoch 0180, iter [01200, 05004], lr: 0.002834, loss: 0.9005
2022-03-04 17:37:42 - train: epoch 0180, iter [01300, 05004], lr: 0.002834, loss: 0.9687
2022-03-04 17:38:17 - train: epoch 0180, iter [01400, 05004], lr: 0.002834, loss: 0.8655
2022-03-04 17:38:51 - train: epoch 0180, iter [01500, 05004], lr: 0.002834, loss: 1.0658
2022-03-04 17:39:23 - train: epoch 0180, iter [01600, 05004], lr: 0.002834, loss: 0.8458
2022-03-04 17:39:56 - train: epoch 0180, iter [01700, 05004], lr: 0.002834, loss: 0.7925
2022-03-04 17:40:30 - train: epoch 0180, iter [01800, 05004], lr: 0.002834, loss: 0.9741
2022-03-04 17:41:04 - train: epoch 0180, iter [01900, 05004], lr: 0.002834, loss: 0.8157
2022-03-04 17:41:37 - train: epoch 0180, iter [02000, 05004], lr: 0.002834, loss: 0.9189
2022-03-04 17:42:11 - train: epoch 0180, iter [02100, 05004], lr: 0.002834, loss: 0.8629
2022-03-04 17:42:44 - train: epoch 0180, iter [02200, 05004], lr: 0.002834, loss: 0.8160
2022-03-04 17:43:17 - train: epoch 0180, iter [02300, 05004], lr: 0.002834, loss: 1.1978
2022-03-04 17:43:50 - train: epoch 0180, iter [02400, 05004], lr: 0.002834, loss: 1.0456
2022-03-04 17:44:24 - train: epoch 0180, iter [02500, 05004], lr: 0.002834, loss: 0.9410
2022-03-04 17:44:57 - train: epoch 0180, iter [02600, 05004], lr: 0.002834, loss: 0.9546
2022-03-04 17:45:31 - train: epoch 0180, iter [02700, 05004], lr: 0.002834, loss: 0.7324
2022-03-04 17:46:04 - train: epoch 0180, iter [02800, 05004], lr: 0.002834, loss: 0.8709
2022-03-04 17:46:37 - train: epoch 0180, iter [02900, 05004], lr: 0.002834, loss: 0.8036
2022-03-04 17:47:11 - train: epoch 0180, iter [03000, 05004], lr: 0.002834, loss: 0.8933
2022-03-04 17:47:44 - train: epoch 0180, iter [03100, 05004], lr: 0.002834, loss: 0.9617
2022-03-04 17:48:17 - train: epoch 0180, iter [03200, 05004], lr: 0.002834, loss: 0.9885
2022-03-04 17:48:50 - train: epoch 0180, iter [03300, 05004], lr: 0.002834, loss: 1.0939
2022-03-04 17:49:24 - train: epoch 0180, iter [03400, 05004], lr: 0.002834, loss: 1.0360
2022-03-04 17:49:57 - train: epoch 0180, iter [03500, 05004], lr: 0.002834, loss: 0.7796
2022-03-04 17:50:30 - train: epoch 0180, iter [03600, 05004], lr: 0.002834, loss: 0.9278
2022-03-04 17:51:03 - train: epoch 0180, iter [03700, 05004], lr: 0.002834, loss: 0.8713
2022-03-04 17:51:37 - train: epoch 0180, iter [03800, 05004], lr: 0.002834, loss: 1.0099
2022-03-04 17:52:11 - train: epoch 0180, iter [03900, 05004], lr: 0.002834, loss: 1.0144
2022-03-04 17:52:44 - train: epoch 0180, iter [04000, 05004], lr: 0.002834, loss: 0.8284
2022-03-04 17:53:17 - train: epoch 0180, iter [04100, 05004], lr: 0.002834, loss: 0.9479
2022-03-04 17:53:51 - train: epoch 0180, iter [04200, 05004], lr: 0.002834, loss: 0.8519
2022-03-04 17:54:24 - train: epoch 0180, iter [04300, 05004], lr: 0.002834, loss: 0.8453
2022-03-04 17:54:56 - train: epoch 0180, iter [04400, 05004], lr: 0.002834, loss: 0.7953
2022-03-04 17:55:30 - train: epoch 0180, iter [04500, 05004], lr: 0.002834, loss: 0.9739
2022-03-04 17:56:02 - train: epoch 0180, iter [04600, 05004], lr: 0.002834, loss: 0.9406
2022-03-04 17:56:36 - train: epoch 0180, iter [04700, 05004], lr: 0.002834, loss: 0.9829
2022-03-04 17:57:10 - train: epoch 0180, iter [04800, 05004], lr: 0.002834, loss: 0.8459
2022-03-04 17:57:43 - train: epoch 0180, iter [04900, 05004], lr: 0.002834, loss: 0.7890
2022-03-04 17:58:15 - train: epoch 0180, iter [05000, 05004], lr: 0.002834, loss: 0.9290
2022-03-04 17:58:16 - train: epoch 180, train_loss: 0.9131
2022-03-04 17:59:31 - eval: epoch: 180, acc1: 75.698%, acc5: 92.842%, test_loss: 0.9701, per_image_load_time: 2.366ms, per_image_inference_time: 0.510ms
2022-03-04 17:59:31 - until epoch: 180, best_acc1: 75.698%
2022-03-04 17:59:31 - epoch 181 lr: 0.002573177902642726
2022-03-04 18:00:10 - train: epoch 0181, iter [00100, 05004], lr: 0.002573, loss: 0.9928
2022-03-04 18:00:42 - train: epoch 0181, iter [00200, 05004], lr: 0.002573, loss: 0.8176
2022-03-04 18:01:16 - train: epoch 0181, iter [00300, 05004], lr: 0.002573, loss: 0.9264
2022-03-04 18:01:48 - train: epoch 0181, iter [00400, 05004], lr: 0.002573, loss: 0.7808
2022-03-04 18:02:22 - train: epoch 0181, iter [00500, 05004], lr: 0.002573, loss: 1.0249
2022-03-04 18:02:55 - train: epoch 0181, iter [00600, 05004], lr: 0.002573, loss: 0.7363
2022-03-04 18:03:28 - train: epoch 0181, iter [00700, 05004], lr: 0.002573, loss: 0.8442
2022-03-04 18:04:02 - train: epoch 0181, iter [00800, 05004], lr: 0.002573, loss: 1.0066
2022-03-04 18:04:35 - train: epoch 0181, iter [00900, 05004], lr: 0.002573, loss: 0.7517
2022-03-04 18:05:07 - train: epoch 0181, iter [01000, 05004], lr: 0.002573, loss: 1.0084
2022-03-04 18:05:41 - train: epoch 0181, iter [01100, 05004], lr: 0.002573, loss: 0.8681
2022-03-04 18:06:14 - train: epoch 0181, iter [01200, 05004], lr: 0.002573, loss: 0.7753
2022-03-04 18:06:48 - train: epoch 0181, iter [01300, 05004], lr: 0.002573, loss: 0.9101
2022-03-04 18:07:22 - train: epoch 0181, iter [01400, 05004], lr: 0.002573, loss: 0.8986
2022-03-04 18:07:54 - train: epoch 0181, iter [01500, 05004], lr: 0.002573, loss: 1.0552
2022-03-04 18:08:28 - train: epoch 0181, iter [01600, 05004], lr: 0.002573, loss: 0.8203
2022-03-04 18:09:01 - train: epoch 0181, iter [01700, 05004], lr: 0.002573, loss: 0.9180
2022-03-04 18:09:35 - train: epoch 0181, iter [01800, 05004], lr: 0.002573, loss: 0.8710
2022-03-04 18:10:08 - train: epoch 0181, iter [01900, 05004], lr: 0.002573, loss: 0.7883
2022-03-04 18:10:41 - train: epoch 0181, iter [02000, 05004], lr: 0.002573, loss: 1.0564
2022-03-04 18:11:14 - train: epoch 0181, iter [02100, 05004], lr: 0.002573, loss: 0.8243
2022-03-04 18:11:47 - train: epoch 0181, iter [02200, 05004], lr: 0.002573, loss: 0.9329
2022-03-04 18:12:20 - train: epoch 0181, iter [02300, 05004], lr: 0.002573, loss: 0.9285
2022-03-04 18:12:53 - train: epoch 0181, iter [02400, 05004], lr: 0.002573, loss: 0.7959
2022-03-04 18:13:27 - train: epoch 0181, iter [02500, 05004], lr: 0.002573, loss: 0.7759
2022-03-04 18:13:59 - train: epoch 0181, iter [02600, 05004], lr: 0.002573, loss: 0.7854
2022-03-04 18:14:33 - train: epoch 0181, iter [02700, 05004], lr: 0.002573, loss: 0.7642
2022-03-04 18:15:06 - train: epoch 0181, iter [02800, 05004], lr: 0.002573, loss: 0.8493
2022-03-04 18:15:40 - train: epoch 0181, iter [02900, 05004], lr: 0.002573, loss: 0.8068
2022-03-04 18:16:13 - train: epoch 0181, iter [03000, 05004], lr: 0.002573, loss: 0.8380
2022-03-04 18:16:46 - train: epoch 0181, iter [03100, 05004], lr: 0.002573, loss: 1.1358
2022-03-04 18:17:19 - train: epoch 0181, iter [03200, 05004], lr: 0.002573, loss: 0.6763
2022-03-04 18:17:52 - train: epoch 0181, iter [03300, 05004], lr: 0.002573, loss: 0.8830
2022-03-04 18:18:26 - train: epoch 0181, iter [03400, 05004], lr: 0.002573, loss: 0.9583
2022-03-04 18:18:59 - train: epoch 0181, iter [03500, 05004], lr: 0.002573, loss: 0.9182
2022-03-04 18:19:33 - train: epoch 0181, iter [03600, 05004], lr: 0.002573, loss: 1.0624
2022-03-04 18:20:06 - train: epoch 0181, iter [03700, 05004], lr: 0.002573, loss: 0.8773
2022-03-04 18:20:39 - train: epoch 0181, iter [03800, 05004], lr: 0.002573, loss: 0.9388
2022-03-04 18:21:12 - train: epoch 0181, iter [03900, 05004], lr: 0.002573, loss: 0.8790
2022-03-04 18:21:46 - train: epoch 0181, iter [04000, 05004], lr: 0.002573, loss: 0.7761
2022-03-04 18:22:19 - train: epoch 0181, iter [04100, 05004], lr: 0.002573, loss: 0.9064
2022-03-04 18:22:52 - train: epoch 0181, iter [04200, 05004], lr: 0.002573, loss: 0.8268
2022-03-04 18:23:25 - train: epoch 0181, iter [04300, 05004], lr: 0.002573, loss: 1.0992
2022-03-04 18:23:59 - train: epoch 0181, iter [04400, 05004], lr: 0.002573, loss: 0.8637
2022-03-04 18:24:32 - train: epoch 0181, iter [04500, 05004], lr: 0.002573, loss: 1.2032
2022-03-04 18:25:05 - train: epoch 0181, iter [04600, 05004], lr: 0.002573, loss: 0.8029
2022-03-04 18:25:39 - train: epoch 0181, iter [04700, 05004], lr: 0.002573, loss: 1.0746
2022-03-04 18:26:11 - train: epoch 0181, iter [04800, 05004], lr: 0.002573, loss: 0.8372
2022-03-04 18:26:45 - train: epoch 0181, iter [04900, 05004], lr: 0.002573, loss: 0.9237
2022-03-04 18:27:16 - train: epoch 0181, iter [05000, 05004], lr: 0.002573, loss: 0.9895
2022-03-04 18:27:17 - train: epoch 181, train_loss: 0.8962
2022-03-04 18:28:31 - eval: epoch: 181, acc1: 76.036%, acc5: 92.898%, test_loss: 0.9612, per_image_load_time: 1.423ms, per_image_inference_time: 0.525ms
2022-03-04 18:28:32 - until epoch: 181, best_acc1: 76.036%
2022-03-04 18:28:32 - epoch 182 lr: 0.002324256102563188
2022-03-04 18:29:10 - train: epoch 0182, iter [00100, 05004], lr: 0.002324, loss: 0.9180
2022-03-04 18:29:43 - train: epoch 0182, iter [00200, 05004], lr: 0.002324, loss: 1.0327
2022-03-04 18:30:17 - train: epoch 0182, iter [00300, 05004], lr: 0.002324, loss: 0.9617
2022-03-04 18:30:49 - train: epoch 0182, iter [00400, 05004], lr: 0.002324, loss: 0.7527
2022-03-04 18:31:22 - train: epoch 0182, iter [00500, 05004], lr: 0.002324, loss: 1.0669
2022-03-04 18:31:56 - train: epoch 0182, iter [00600, 05004], lr: 0.002324, loss: 0.9525
2022-03-04 18:32:28 - train: epoch 0182, iter [00700, 05004], lr: 0.002324, loss: 0.9169
2022-03-04 18:33:01 - train: epoch 0182, iter [00800, 05004], lr: 0.002324, loss: 1.0145
2022-03-04 18:33:35 - train: epoch 0182, iter [00900, 05004], lr: 0.002324, loss: 1.0683
2022-03-04 18:34:07 - train: epoch 0182, iter [01000, 05004], lr: 0.002324, loss: 0.8405
2022-03-04 18:34:40 - train: epoch 0182, iter [01100, 05004], lr: 0.002324, loss: 0.8303
2022-03-04 18:35:14 - train: epoch 0182, iter [01200, 05004], lr: 0.002324, loss: 0.9054
2022-03-04 18:35:47 - train: epoch 0182, iter [01300, 05004], lr: 0.002324, loss: 0.8362
2022-03-04 18:36:20 - train: epoch 0182, iter [01400, 05004], lr: 0.002324, loss: 1.1558
2022-03-04 18:36:53 - train: epoch 0182, iter [01500, 05004], lr: 0.002324, loss: 0.8395
2022-03-04 18:37:26 - train: epoch 0182, iter [01600, 05004], lr: 0.002324, loss: 1.0638
2022-03-04 18:37:59 - train: epoch 0182, iter [01700, 05004], lr: 0.002324, loss: 0.7897
2022-03-04 18:38:32 - train: epoch 0182, iter [01800, 05004], lr: 0.002324, loss: 0.6860
2022-03-04 18:39:06 - train: epoch 0182, iter [01900, 05004], lr: 0.002324, loss: 0.8032
2022-03-04 18:39:38 - train: epoch 0182, iter [02000, 05004], lr: 0.002324, loss: 1.0203
2022-03-04 18:40:12 - train: epoch 0182, iter [02100, 05004], lr: 0.002324, loss: 0.7667
2022-03-04 18:40:44 - train: epoch 0182, iter [02200, 05004], lr: 0.002324, loss: 0.9415
2022-03-04 18:41:18 - train: epoch 0182, iter [02300, 05004], lr: 0.002324, loss: 0.8783
2022-03-04 18:41:50 - train: epoch 0182, iter [02400, 05004], lr: 0.002324, loss: 0.7524
2022-03-04 18:42:24 - train: epoch 0182, iter [02500, 05004], lr: 0.002324, loss: 0.8306
2022-03-04 18:42:56 - train: epoch 0182, iter [02600, 05004], lr: 0.002324, loss: 0.6165
2022-03-04 18:43:28 - train: epoch 0182, iter [02700, 05004], lr: 0.002324, loss: 0.7033
2022-03-04 18:44:01 - train: epoch 0182, iter [02800, 05004], lr: 0.002324, loss: 0.9837
2022-03-04 18:44:34 - train: epoch 0182, iter [02900, 05004], lr: 0.002324, loss: 0.7291
2022-03-04 18:45:07 - train: epoch 0182, iter [03000, 05004], lr: 0.002324, loss: 0.8520
2022-03-04 18:45:40 - train: epoch 0182, iter [03100, 05004], lr: 0.002324, loss: 0.8653
2022-03-04 18:46:13 - train: epoch 0182, iter [03200, 05004], lr: 0.002324, loss: 0.8659
2022-03-04 18:46:45 - train: epoch 0182, iter [03300, 05004], lr: 0.002324, loss: 0.8749
2022-03-04 18:47:18 - train: epoch 0182, iter [03400, 05004], lr: 0.002324, loss: 0.8543
2022-03-04 18:47:51 - train: epoch 0182, iter [03500, 05004], lr: 0.002324, loss: 0.8156
2022-03-04 18:48:24 - train: epoch 0182, iter [03600, 05004], lr: 0.002324, loss: 0.9135
2022-03-04 18:48:57 - train: epoch 0182, iter [03700, 05004], lr: 0.002324, loss: 0.8238
2022-03-04 18:49:29 - train: epoch 0182, iter [03800, 05004], lr: 0.002324, loss: 1.1650
2022-03-04 18:50:02 - train: epoch 0182, iter [03900, 05004], lr: 0.002324, loss: 0.7967
2022-03-04 18:50:35 - train: epoch 0182, iter [04000, 05004], lr: 0.002324, loss: 0.8531
2022-03-04 18:51:08 - train: epoch 0182, iter [04100, 05004], lr: 0.002324, loss: 0.8477
2022-03-04 18:51:41 - train: epoch 0182, iter [04200, 05004], lr: 0.002324, loss: 0.8274
2022-03-04 18:52:14 - train: epoch 0182, iter [04300, 05004], lr: 0.002324, loss: 1.0119
2022-03-04 18:52:47 - train: epoch 0182, iter [04400, 05004], lr: 0.002324, loss: 0.8186
2022-03-04 18:53:19 - train: epoch 0182, iter [04500, 05004], lr: 0.002324, loss: 0.7046
2022-03-04 18:53:52 - train: epoch 0182, iter [04600, 05004], lr: 0.002324, loss: 0.8878
2022-03-04 18:54:25 - train: epoch 0182, iter [04700, 05004], lr: 0.002324, loss: 0.8691
2022-03-04 18:54:58 - train: epoch 0182, iter [04800, 05004], lr: 0.002324, loss: 0.8827
2022-03-04 18:55:31 - train: epoch 0182, iter [04900, 05004], lr: 0.002324, loss: 0.9301
2022-03-04 18:56:02 - train: epoch 0182, iter [05000, 05004], lr: 0.002324, loss: 0.8091
2022-03-04 18:56:03 - train: epoch 182, train_loss: 0.8806
2022-03-04 18:57:17 - eval: epoch: 182, acc1: 76.046%, acc5: 92.980%, test_loss: 0.9588, per_image_load_time: 2.404ms, per_image_inference_time: 0.464ms
2022-03-04 18:57:18 - until epoch: 182, best_acc1: 76.046%
2022-03-04 18:57:18 - epoch 183 lr: 0.002087708544541689
2022-03-04 18:57:56 - train: epoch 0183, iter [00100, 05004], lr: 0.002088, loss: 0.8931
2022-03-04 18:58:29 - train: epoch 0183, iter [00200, 05004], lr: 0.002088, loss: 0.6593
2022-03-04 18:59:01 - train: epoch 0183, iter [00300, 05004], lr: 0.002088, loss: 0.7360
2022-03-04 18:59:34 - train: epoch 0183, iter [00400, 05004], lr: 0.002088, loss: 0.7535
2022-03-04 19:00:07 - train: epoch 0183, iter [00500, 05004], lr: 0.002088, loss: 0.9398
2022-03-04 19:00:39 - train: epoch 0183, iter [00600, 05004], lr: 0.002088, loss: 0.7351
2022-03-04 19:01:12 - train: epoch 0183, iter [00700, 05004], lr: 0.002088, loss: 1.0435
2022-03-04 19:01:45 - train: epoch 0183, iter [00800, 05004], lr: 0.002088, loss: 0.8208
2022-03-04 19:02:17 - train: epoch 0183, iter [00900, 05004], lr: 0.002088, loss: 0.8494
2022-03-04 19:02:50 - train: epoch 0183, iter [01000, 05004], lr: 0.002088, loss: 0.9168
2022-03-04 19:03:23 - train: epoch 0183, iter [01100, 05004], lr: 0.002088, loss: 1.0050
2022-03-04 19:03:55 - train: epoch 0183, iter [01200, 05004], lr: 0.002088, loss: 0.7659
2022-03-04 19:04:28 - train: epoch 0183, iter [01300, 05004], lr: 0.002088, loss: 0.9464
2022-03-04 19:05:01 - train: epoch 0183, iter [01400, 05004], lr: 0.002088, loss: 0.9610
2022-03-04 19:05:33 - train: epoch 0183, iter [01500, 05004], lr: 0.002088, loss: 0.9431
2022-03-04 19:06:06 - train: epoch 0183, iter [01600, 05004], lr: 0.002088, loss: 0.8714
2022-03-04 19:06:39 - train: epoch 0183, iter [01700, 05004], lr: 0.002088, loss: 0.7245
2022-03-04 19:07:12 - train: epoch 0183, iter [01800, 05004], lr: 0.002088, loss: 0.8716
2022-03-04 19:07:45 - train: epoch 0183, iter [01900, 05004], lr: 0.002088, loss: 0.9132
2022-03-04 19:08:18 - train: epoch 0183, iter [02000, 05004], lr: 0.002088, loss: 0.7960
2022-03-04 19:08:51 - train: epoch 0183, iter [02100, 05004], lr: 0.002088, loss: 0.9597
2022-03-04 19:09:24 - train: epoch 0183, iter [02200, 05004], lr: 0.002088, loss: 1.0229
2022-03-04 19:09:56 - train: epoch 0183, iter [02300, 05004], lr: 0.002088, loss: 0.8357
2022-03-04 19:10:29 - train: epoch 0183, iter [02400, 05004], lr: 0.002088, loss: 0.9589
2022-03-04 19:11:02 - train: epoch 0183, iter [02500, 05004], lr: 0.002088, loss: 0.8220
2022-03-04 19:11:35 - train: epoch 0183, iter [02600, 05004], lr: 0.002088, loss: 0.9828
2022-03-04 19:12:08 - train: epoch 0183, iter [02700, 05004], lr: 0.002088, loss: 0.9362
2022-03-04 19:12:40 - train: epoch 0183, iter [02800, 05004], lr: 0.002088, loss: 0.8891
2022-03-04 19:13:13 - train: epoch 0183, iter [02900, 05004], lr: 0.002088, loss: 0.9962
2022-03-04 19:13:46 - train: epoch 0183, iter [03000, 05004], lr: 0.002088, loss: 0.9328
2022-03-04 19:14:19 - train: epoch 0183, iter [03100, 05004], lr: 0.002088, loss: 0.7909
2022-03-04 19:14:51 - train: epoch 0183, iter [03200, 05004], lr: 0.002088, loss: 0.7887
2022-03-04 19:15:24 - train: epoch 0183, iter [03300, 05004], lr: 0.002088, loss: 0.8812
2022-03-04 19:15:56 - train: epoch 0183, iter [03400, 05004], lr: 0.002088, loss: 0.7379
2022-03-04 19:16:29 - train: epoch 0183, iter [03500, 05004], lr: 0.002088, loss: 0.8664
2022-03-04 19:17:02 - train: epoch 0183, iter [03600, 05004], lr: 0.002088, loss: 0.8638
2022-03-04 19:17:35 - train: epoch 0183, iter [03700, 05004], lr: 0.002088, loss: 1.0676
2022-03-04 19:18:08 - train: epoch 0183, iter [03800, 05004], lr: 0.002088, loss: 0.5972
2022-03-04 19:18:41 - train: epoch 0183, iter [03900, 05004], lr: 0.002088, loss: 0.7282
2022-03-04 19:19:14 - train: epoch 0183, iter [04000, 05004], lr: 0.002088, loss: 1.0044
2022-03-04 19:19:47 - train: epoch 0183, iter [04100, 05004], lr: 0.002088, loss: 0.8204
2022-03-04 19:20:20 - train: epoch 0183, iter [04200, 05004], lr: 0.002088, loss: 0.9180
2022-03-04 19:20:53 - train: epoch 0183, iter [04300, 05004], lr: 0.002088, loss: 0.8627
2022-03-04 19:21:26 - train: epoch 0183, iter [04400, 05004], lr: 0.002088, loss: 0.9404
2022-03-04 19:21:59 - train: epoch 0183, iter [04500, 05004], lr: 0.002088, loss: 1.0053
2022-03-04 19:22:31 - train: epoch 0183, iter [04600, 05004], lr: 0.002088, loss: 0.9945
2022-03-04 19:23:05 - train: epoch 0183, iter [04700, 05004], lr: 0.002088, loss: 0.9339
2022-03-04 19:23:38 - train: epoch 0183, iter [04800, 05004], lr: 0.002088, loss: 0.9170
2022-03-04 19:24:11 - train: epoch 0183, iter [04900, 05004], lr: 0.002088, loss: 0.8149
2022-03-04 19:24:43 - train: epoch 0183, iter [05000, 05004], lr: 0.002088, loss: 1.0105
2022-03-04 19:24:44 - train: epoch 183, train_loss: 0.8672
2022-03-04 19:25:58 - eval: epoch: 183, acc1: 76.236%, acc5: 92.988%, test_loss: 0.9554, per_image_load_time: 2.337ms, per_image_inference_time: 0.507ms
2022-03-04 19:25:59 - until epoch: 183, best_acc1: 76.236%
2022-03-04 19:25:59 - epoch 184 lr: 0.0018635966245104663
2022-03-04 19:26:37 - train: epoch 0184, iter [00100, 05004], lr: 0.001864, loss: 0.8643
2022-03-04 19:27:10 - train: epoch 0184, iter [00200, 05004], lr: 0.001864, loss: 0.7786
2022-03-04 19:27:43 - train: epoch 0184, iter [00300, 05004], lr: 0.001864, loss: 0.7676
2022-03-04 19:28:16 - train: epoch 0184, iter [00400, 05004], lr: 0.001864, loss: 0.9737
2022-03-04 19:28:50 - train: epoch 0184, iter [00500, 05004], lr: 0.001864, loss: 0.8972
2022-03-04 19:29:22 - train: epoch 0184, iter [00600, 05004], lr: 0.001864, loss: 0.9235
2022-03-04 19:29:55 - train: epoch 0184, iter [00700, 05004], lr: 0.001864, loss: 0.8249
2022-03-04 19:30:28 - train: epoch 0184, iter [00800, 05004], lr: 0.001864, loss: 0.5924
2022-03-04 19:31:01 - train: epoch 0184, iter [00900, 05004], lr: 0.001864, loss: 0.9140
2022-03-04 19:31:35 - train: epoch 0184, iter [01000, 05004], lr: 0.001864, loss: 0.6182
2022-03-04 19:32:08 - train: epoch 0184, iter [01100, 05004], lr: 0.001864, loss: 0.9812
2022-03-04 19:32:41 - train: epoch 0184, iter [01200, 05004], lr: 0.001864, loss: 0.7441
2022-03-04 19:33:15 - train: epoch 0184, iter [01300, 05004], lr: 0.001864, loss: 0.7273
2022-03-04 19:33:48 - train: epoch 0184, iter [01400, 05004], lr: 0.001864, loss: 0.8020
2022-03-04 19:34:22 - train: epoch 0184, iter [01500, 05004], lr: 0.001864, loss: 0.8398
2022-03-04 19:34:54 - train: epoch 0184, iter [01600, 05004], lr: 0.001864, loss: 0.9006
2022-03-04 19:35:28 - train: epoch 0184, iter [01700, 05004], lr: 0.001864, loss: 0.7026
2022-03-04 19:36:01 - train: epoch 0184, iter [01800, 05004], lr: 0.001864, loss: 1.0498
2022-03-04 19:36:34 - train: epoch 0184, iter [01900, 05004], lr: 0.001864, loss: 0.5945
2022-03-04 19:37:06 - train: epoch 0184, iter [02000, 05004], lr: 0.001864, loss: 0.8104
2022-03-04 19:37:40 - train: epoch 0184, iter [02100, 05004], lr: 0.001864, loss: 0.7547
2022-03-04 19:38:12 - train: epoch 0184, iter [02200, 05004], lr: 0.001864, loss: 1.1390
2022-03-04 19:38:46 - train: epoch 0184, iter [02300, 05004], lr: 0.001864, loss: 1.0428
2022-03-04 19:39:18 - train: epoch 0184, iter [02400, 05004], lr: 0.001864, loss: 0.8056
2022-03-04 19:39:51 - train: epoch 0184, iter [02500, 05004], lr: 0.001864, loss: 0.7257
2022-03-04 19:40:24 - train: epoch 0184, iter [02600, 05004], lr: 0.001864, loss: 0.9227
2022-03-04 19:40:57 - train: epoch 0184, iter [02700, 05004], lr: 0.001864, loss: 0.9483
2022-03-04 19:41:30 - train: epoch 0184, iter [02800, 05004], lr: 0.001864, loss: 0.7819
2022-03-04 19:42:03 - train: epoch 0184, iter [02900, 05004], lr: 0.001864, loss: 0.8409
2022-03-04 19:42:36 - train: epoch 0184, iter [03000, 05004], lr: 0.001864, loss: 0.6881
2022-03-04 19:43:09 - train: epoch 0184, iter [03100, 05004], lr: 0.001864, loss: 0.9105
2022-03-04 19:43:42 - train: epoch 0184, iter [03200, 05004], lr: 0.001864, loss: 0.7622
2022-03-04 19:44:15 - train: epoch 0184, iter [03300, 05004], lr: 0.001864, loss: 0.8348
2022-03-04 19:44:48 - train: epoch 0184, iter [03400, 05004], lr: 0.001864, loss: 0.8725
2022-03-04 19:45:21 - train: epoch 0184, iter [03500, 05004], lr: 0.001864, loss: 0.7684
2022-03-04 19:45:54 - train: epoch 0184, iter [03600, 05004], lr: 0.001864, loss: 0.8734
2022-03-04 19:46:27 - train: epoch 0184, iter [03700, 05004], lr: 0.001864, loss: 0.7278
2022-03-04 19:47:00 - train: epoch 0184, iter [03800, 05004], lr: 0.001864, loss: 0.7317
2022-03-04 19:47:32 - train: epoch 0184, iter [03900, 05004], lr: 0.001864, loss: 0.8604
2022-03-04 19:48:06 - train: epoch 0184, iter [04000, 05004], lr: 0.001864, loss: 0.9326
2022-03-04 19:48:39 - train: epoch 0184, iter [04100, 05004], lr: 0.001864, loss: 0.8043
2022-03-04 19:49:12 - train: epoch 0184, iter [04200, 05004], lr: 0.001864, loss: 0.8706
2022-03-04 19:49:45 - train: epoch 0184, iter [04300, 05004], lr: 0.001864, loss: 0.8380
2022-03-04 19:50:18 - train: epoch 0184, iter [04400, 05004], lr: 0.001864, loss: 0.6726
2022-03-04 19:50:51 - train: epoch 0184, iter [04500, 05004], lr: 0.001864, loss: 0.9114
2022-03-04 19:51:25 - train: epoch 0184, iter [04600, 05004], lr: 0.001864, loss: 0.6949
2022-03-04 19:51:58 - train: epoch 0184, iter [04700, 05004], lr: 0.001864, loss: 0.8206
2022-03-04 19:52:31 - train: epoch 0184, iter [04800, 05004], lr: 0.001864, loss: 0.8821
2022-03-04 19:53:04 - train: epoch 0184, iter [04900, 05004], lr: 0.001864, loss: 0.6464
2022-03-04 19:53:36 - train: epoch 0184, iter [05000, 05004], lr: 0.001864, loss: 0.6625
2022-03-04 19:53:37 - train: epoch 184, train_loss: 0.8493
2022-03-04 19:54:51 - eval: epoch: 184, acc1: 76.310%, acc5: 93.004%, test_loss: 0.9554, per_image_load_time: 2.363ms, per_image_inference_time: 0.512ms
2022-03-04 19:54:52 - until epoch: 184, best_acc1: 76.310%
2022-03-04 19:54:52 - epoch 185 lr: 0.0016519785107311892
2022-03-04 19:55:31 - train: epoch 0185, iter [00100, 05004], lr: 0.001652, loss: 0.9060
2022-03-04 19:56:04 - train: epoch 0185, iter [00200, 05004], lr: 0.001652, loss: 0.7937
2022-03-04 19:56:37 - train: epoch 0185, iter [00300, 05004], lr: 0.001652, loss: 0.7850
2022-03-04 19:57:11 - train: epoch 0185, iter [00400, 05004], lr: 0.001652, loss: 0.8356
2022-03-04 19:57:43 - train: epoch 0185, iter [00500, 05004], lr: 0.001652, loss: 0.8307
2022-03-04 19:58:16 - train: epoch 0185, iter [00600, 05004], lr: 0.001652, loss: 0.9838
2022-03-04 19:58:50 - train: epoch 0185, iter [00700, 05004], lr: 0.001652, loss: 0.7939
2022-03-04 19:59:23 - train: epoch 0185, iter [00800, 05004], lr: 0.001652, loss: 0.7656
2022-03-04 19:59:57 - train: epoch 0185, iter [00900, 05004], lr: 0.001652, loss: 0.8588
2022-03-04 20:00:30 - train: epoch 0185, iter [01000, 05004], lr: 0.001652, loss: 0.7602
2022-03-04 20:01:03 - train: epoch 0185, iter [01100, 05004], lr: 0.001652, loss: 0.8386
2022-03-04 20:01:37 - train: epoch 0185, iter [01200, 05004], lr: 0.001652, loss: 0.7765
2022-03-04 20:02:11 - train: epoch 0185, iter [01300, 05004], lr: 0.001652, loss: 0.6936
2022-03-04 20:02:44 - train: epoch 0185, iter [01400, 05004], lr: 0.001652, loss: 0.7892
2022-03-04 20:03:17 - train: epoch 0185, iter [01500, 05004], lr: 0.001652, loss: 0.8955
2022-03-04 20:03:50 - train: epoch 0185, iter [01600, 05004], lr: 0.001652, loss: 0.8461
2022-03-04 20:04:24 - train: epoch 0185, iter [01700, 05004], lr: 0.001652, loss: 0.8686
2022-03-04 20:04:57 - train: epoch 0185, iter [01800, 05004], lr: 0.001652, loss: 0.8138
2022-03-04 20:05:31 - train: epoch 0185, iter [01900, 05004], lr: 0.001652, loss: 0.6676
2022-03-04 20:06:04 - train: epoch 0185, iter [02000, 05004], lr: 0.001652, loss: 0.7992
2022-03-04 20:06:37 - train: epoch 0185, iter [02100, 05004], lr: 0.001652, loss: 0.8726
2022-03-04 20:07:12 - train: epoch 0185, iter [02200, 05004], lr: 0.001652, loss: 0.9296
2022-03-04 20:07:44 - train: epoch 0185, iter [02300, 05004], lr: 0.001652, loss: 0.8202
2022-03-04 20:08:18 - train: epoch 0185, iter [02400, 05004], lr: 0.001652, loss: 0.8431
2022-03-04 20:08:51 - train: epoch 0185, iter [02500, 05004], lr: 0.001652, loss: 0.7815
2022-03-04 20:09:24 - train: epoch 0185, iter [02600, 05004], lr: 0.001652, loss: 0.5253
2022-03-04 20:09:58 - train: epoch 0185, iter [02700, 05004], lr: 0.001652, loss: 0.9740
2022-03-04 20:10:32 - train: epoch 0185, iter [02800, 05004], lr: 0.001652, loss: 0.8702
2022-03-04 20:11:04 - train: epoch 0185, iter [02900, 05004], lr: 0.001652, loss: 0.7330
2022-03-04 20:11:38 - train: epoch 0185, iter [03000, 05004], lr: 0.001652, loss: 0.8069
2022-03-04 20:12:11 - train: epoch 0185, iter [03100, 05004], lr: 0.001652, loss: 0.8675
2022-03-04 20:12:45 - train: epoch 0185, iter [03200, 05004], lr: 0.001652, loss: 0.8067
2022-03-04 20:13:18 - train: epoch 0185, iter [03300, 05004], lr: 0.001652, loss: 0.6918
2022-03-04 20:13:51 - train: epoch 0185, iter [03400, 05004], lr: 0.001652, loss: 0.6923
2022-03-04 20:14:24 - train: epoch 0185, iter [03500, 05004], lr: 0.001652, loss: 0.8919
2022-03-04 20:14:58 - train: epoch 0185, iter [03600, 05004], lr: 0.001652, loss: 1.0213
2022-03-04 20:15:31 - train: epoch 0185, iter [03700, 05004], lr: 0.001652, loss: 0.6815
2022-03-04 20:16:04 - train: epoch 0185, iter [03800, 05004], lr: 0.001652, loss: 0.8168
2022-03-04 20:16:38 - train: epoch 0185, iter [03900, 05004], lr: 0.001652, loss: 0.7501
2022-03-04 20:17:11 - train: epoch 0185, iter [04000, 05004], lr: 0.001652, loss: 0.7624
2022-03-04 20:17:44 - train: epoch 0185, iter [04100, 05004], lr: 0.001652, loss: 0.7519
2022-03-04 20:18:18 - train: epoch 0185, iter [04200, 05004], lr: 0.001652, loss: 0.8146
2022-03-04 20:18:51 - train: epoch 0185, iter [04300, 05004], lr: 0.001652, loss: 0.7001
2022-03-04 20:19:25 - train: epoch 0185, iter [04400, 05004], lr: 0.001652, loss: 0.8018
2022-03-04 20:19:58 - train: epoch 0185, iter [04500, 05004], lr: 0.001652, loss: 0.8137
2022-03-04 20:20:31 - train: epoch 0185, iter [04600, 05004], lr: 0.001652, loss: 0.8023
2022-03-04 20:21:05 - train: epoch 0185, iter [04700, 05004], lr: 0.001652, loss: 0.9208
2022-03-04 20:21:38 - train: epoch 0185, iter [04800, 05004], lr: 0.001652, loss: 0.9414
2022-03-04 20:22:10 - train: epoch 0185, iter [04900, 05004], lr: 0.001652, loss: 0.7896
2022-03-04 20:22:43 - train: epoch 0185, iter [05000, 05004], lr: 0.001652, loss: 0.9727
2022-03-04 20:22:44 - train: epoch 185, train_loss: 0.8334
2022-03-04 20:23:58 - eval: epoch: 185, acc1: 76.364%, acc5: 93.124%, test_loss: 0.9471, per_image_load_time: 2.360ms, per_image_inference_time: 0.497ms
2022-03-04 20:23:59 - until epoch: 185, best_acc1: 76.364%
2022-03-04 20:23:59 - epoch 186 lr: 0.0014529091286973996
2022-03-04 20:24:37 - train: epoch 0186, iter [00100, 05004], lr: 0.001453, loss: 0.8501
2022-03-04 20:25:09 - train: epoch 0186, iter [00200, 05004], lr: 0.001453, loss: 0.7774
2022-03-04 20:25:43 - train: epoch 0186, iter [00300, 05004], lr: 0.001453, loss: 0.8098
2022-03-04 20:26:16 - train: epoch 0186, iter [00400, 05004], lr: 0.001453, loss: 0.7854
2022-03-04 20:26:50 - train: epoch 0186, iter [00500, 05004], lr: 0.001453, loss: 0.8841
2022-03-04 20:27:23 - train: epoch 0186, iter [00600, 05004], lr: 0.001453, loss: 0.8458
2022-03-04 20:27:55 - train: epoch 0186, iter [00700, 05004], lr: 0.001453, loss: 0.8867
2022-03-04 20:28:29 - train: epoch 0186, iter [00800, 05004], lr: 0.001453, loss: 0.9130
2022-03-04 20:29:02 - train: epoch 0186, iter [00900, 05004], lr: 0.001453, loss: 0.9055
2022-03-04 20:29:35 - train: epoch 0186, iter [01000, 05004], lr: 0.001453, loss: 0.8307
2022-03-04 20:30:08 - train: epoch 0186, iter [01100, 05004], lr: 0.001453, loss: 0.7745
2022-03-04 20:30:41 - train: epoch 0186, iter [01200, 05004], lr: 0.001453, loss: 0.8130
2022-03-04 20:31:15 - train: epoch 0186, iter [01300, 05004], lr: 0.001453, loss: 0.6715
2022-03-04 20:31:47 - train: epoch 0186, iter [01400, 05004], lr: 0.001453, loss: 0.8015
2022-03-04 20:32:21 - train: epoch 0186, iter [01500, 05004], lr: 0.001453, loss: 0.8099
2022-03-04 20:32:53 - train: epoch 0186, iter [01600, 05004], lr: 0.001453, loss: 0.7018
2022-03-04 20:33:27 - train: epoch 0186, iter [01700, 05004], lr: 0.001453, loss: 0.5557
2022-03-04 20:34:01 - train: epoch 0186, iter [01800, 05004], lr: 0.001453, loss: 0.8091
2022-03-04 20:34:34 - train: epoch 0186, iter [01900, 05004], lr: 0.001453, loss: 0.8264
2022-03-04 20:35:06 - train: epoch 0186, iter [02000, 05004], lr: 0.001453, loss: 0.8153
2022-03-04 20:35:39 - train: epoch 0186, iter [02100, 05004], lr: 0.001453, loss: 0.9802
2022-03-04 20:36:13 - train: epoch 0186, iter [02200, 05004], lr: 0.001453, loss: 0.9183
2022-03-04 20:36:47 - train: epoch 0186, iter [02300, 05004], lr: 0.001453, loss: 0.7613
2022-03-04 20:37:20 - train: epoch 0186, iter [02400, 05004], lr: 0.001453, loss: 0.6192
2022-03-04 20:37:52 - train: epoch 0186, iter [02500, 05004], lr: 0.001453, loss: 0.8299
2022-03-04 20:38:26 - train: epoch 0186, iter [02600, 05004], lr: 0.001453, loss: 0.7829
2022-03-04 20:38:59 - train: epoch 0186, iter [02700, 05004], lr: 0.001453, loss: 0.8209
2022-03-04 20:39:32 - train: epoch 0186, iter [02800, 05004], lr: 0.001453, loss: 0.6951
2022-03-04 20:40:05 - train: epoch 0186, iter [02900, 05004], lr: 0.001453, loss: 0.8887
2022-03-04 20:40:38 - train: epoch 0186, iter [03000, 05004], lr: 0.001453, loss: 0.9949
2022-03-04 20:41:12 - train: epoch 0186, iter [03100, 05004], lr: 0.001453, loss: 0.7675
2022-03-04 20:41:44 - train: epoch 0186, iter [03200, 05004], lr: 0.001453, loss: 0.8443
2022-03-04 20:42:19 - train: epoch 0186, iter [03300, 05004], lr: 0.001453, loss: 0.8692
2022-03-04 20:42:51 - train: epoch 0186, iter [03400, 05004], lr: 0.001453, loss: 0.8885
2022-03-04 20:43:24 - train: epoch 0186, iter [03500, 05004], lr: 0.001453, loss: 0.8400
2022-03-04 20:43:58 - train: epoch 0186, iter [03600, 05004], lr: 0.001453, loss: 0.7409
2022-03-04 20:44:31 - train: epoch 0186, iter [03700, 05004], lr: 0.001453, loss: 0.7684
2022-03-04 20:45:04 - train: epoch 0186, iter [03800, 05004], lr: 0.001453, loss: 0.6811
2022-03-04 20:45:37 - train: epoch 0186, iter [03900, 05004], lr: 0.001453, loss: 0.8305
2022-03-04 20:46:10 - train: epoch 0186, iter [04000, 05004], lr: 0.001453, loss: 0.6281
2022-03-04 20:46:43 - train: epoch 0186, iter [04100, 05004], lr: 0.001453, loss: 0.8252
2022-03-04 20:47:16 - train: epoch 0186, iter [04200, 05004], lr: 0.001453, loss: 0.9410
2022-03-04 20:47:50 - train: epoch 0186, iter [04300, 05004], lr: 0.001453, loss: 0.9906
2022-03-04 20:48:23 - train: epoch 0186, iter [04400, 05004], lr: 0.001453, loss: 0.7946
2022-03-04 20:48:57 - train: epoch 0186, iter [04500, 05004], lr: 0.001453, loss: 0.8735
2022-03-04 20:49:30 - train: epoch 0186, iter [04600, 05004], lr: 0.001453, loss: 0.8355
2022-03-04 20:50:04 - train: epoch 0186, iter [04700, 05004], lr: 0.001453, loss: 0.9364
2022-03-04 20:50:36 - train: epoch 0186, iter [04800, 05004], lr: 0.001453, loss: 0.9737
2022-03-04 20:51:09 - train: epoch 0186, iter [04900, 05004], lr: 0.001453, loss: 0.7920
2022-03-04 20:51:41 - train: epoch 0186, iter [05000, 05004], lr: 0.001453, loss: 1.0839
2022-03-04 20:51:42 - train: epoch 186, train_loss: 0.8188
2022-03-04 20:52:56 - eval: epoch: 186, acc1: 76.584%, acc5: 93.188%, test_loss: 0.9389, per_image_load_time: 2.308ms, per_image_inference_time: 0.535ms
2022-03-04 20:52:57 - until epoch: 186, best_acc1: 76.584%
2022-03-04 20:52:57 - epoch 187 lr: 0.0012664401468786115
2022-03-04 20:53:35 - train: epoch 0187, iter [00100, 05004], lr: 0.001266, loss: 0.8020
2022-03-04 20:54:08 - train: epoch 0187, iter [00200, 05004], lr: 0.001266, loss: 0.7656
2022-03-04 20:54:41 - train: epoch 0187, iter [00300, 05004], lr: 0.001266, loss: 0.8084
2022-03-04 20:55:14 - train: epoch 0187, iter [00400, 05004], lr: 0.001266, loss: 0.7278
2022-03-04 20:55:46 - train: epoch 0187, iter [00500, 05004], lr: 0.001266, loss: 0.9081
2022-03-04 20:56:19 - train: epoch 0187, iter [00600, 05004], lr: 0.001266, loss: 0.8299
2022-03-04 20:56:52 - train: epoch 0187, iter [00700, 05004], lr: 0.001266, loss: 0.8769
2022-03-04 20:57:26 - train: epoch 0187, iter [00800, 05004], lr: 0.001266, loss: 0.8877
2022-03-04 20:57:59 - train: epoch 0187, iter [00900, 05004], lr: 0.001266, loss: 0.7416
2022-03-04 20:58:32 - train: epoch 0187, iter [01000, 05004], lr: 0.001266, loss: 0.8409
2022-03-04 20:59:05 - train: epoch 0187, iter [01100, 05004], lr: 0.001266, loss: 0.8304
2022-03-04 20:59:39 - train: epoch 0187, iter [01200, 05004], lr: 0.001266, loss: 0.8561
2022-03-04 21:00:11 - train: epoch 0187, iter [01300, 05004], lr: 0.001266, loss: 0.7722
2022-03-04 21:00:44 - train: epoch 0187, iter [01400, 05004], lr: 0.001266, loss: 0.8452
2022-03-04 21:01:17 - train: epoch 0187, iter [01500, 05004], lr: 0.001266, loss: 0.9409
2022-03-04 21:01:51 - train: epoch 0187, iter [01600, 05004], lr: 0.001266, loss: 0.6120
2022-03-04 21:02:24 - train: epoch 0187, iter [01700, 05004], lr: 0.001266, loss: 0.9972
2022-03-04 21:02:57 - train: epoch 0187, iter [01800, 05004], lr: 0.001266, loss: 0.8796
2022-03-04 21:03:30 - train: epoch 0187, iter [01900, 05004], lr: 0.001266, loss: 0.8773
2022-03-04 21:04:04 - train: epoch 0187, iter [02000, 05004], lr: 0.001266, loss: 0.8643
2022-03-04 21:04:37 - train: epoch 0187, iter [02100, 05004], lr: 0.001266, loss: 0.9458
2022-03-04 21:05:11 - train: epoch 0187, iter [02200, 05004], lr: 0.001266, loss: 0.9713
2022-03-04 21:05:44 - train: epoch 0187, iter [02300, 05004], lr: 0.001266, loss: 0.6259
2022-03-04 21:06:17 - train: epoch 0187, iter [02400, 05004], lr: 0.001266, loss: 0.9074
2022-03-04 21:06:50 - train: epoch 0187, iter [02500, 05004], lr: 0.001266, loss: 0.7232
2022-03-04 21:07:23 - train: epoch 0187, iter [02600, 05004], lr: 0.001266, loss: 0.8398
2022-03-04 21:07:56 - train: epoch 0187, iter [02700, 05004], lr: 0.001266, loss: 0.9544
2022-03-04 21:08:30 - train: epoch 0187, iter [02800, 05004], lr: 0.001266, loss: 0.8002
2022-03-04 21:09:03 - train: epoch 0187, iter [02900, 05004], lr: 0.001266, loss: 0.7936
2022-03-04 21:09:35 - train: epoch 0187, iter [03000, 05004], lr: 0.001266, loss: 0.7571
2022-03-04 21:10:09 - train: epoch 0187, iter [03100, 05004], lr: 0.001266, loss: 0.7370
2022-03-04 21:10:42 - train: epoch 0187, iter [03200, 05004], lr: 0.001266, loss: 0.7504
2022-03-04 21:11:15 - train: epoch 0187, iter [03300, 05004], lr: 0.001266, loss: 0.8625
2022-03-04 21:11:48 - train: epoch 0187, iter [03400, 05004], lr: 0.001266, loss: 0.8913
2022-03-04 21:12:22 - train: epoch 0187, iter [03500, 05004], lr: 0.001266, loss: 0.8802
2022-03-04 21:12:54 - train: epoch 0187, iter [03600, 05004], lr: 0.001266, loss: 1.0812
2022-03-04 21:13:27 - train: epoch 0187, iter [03700, 05004], lr: 0.001266, loss: 0.8239
2022-03-04 21:14:00 - train: epoch 0187, iter [03800, 05004], lr: 0.001266, loss: 0.8302
2022-03-04 21:14:33 - train: epoch 0187, iter [03900, 05004], lr: 0.001266, loss: 0.6890
2022-03-04 21:15:06 - train: epoch 0187, iter [04000, 05004], lr: 0.001266, loss: 0.8987
2022-03-04 21:15:39 - train: epoch 0187, iter [04100, 05004], lr: 0.001266, loss: 0.8262
2022-03-04 21:16:12 - train: epoch 0187, iter [04200, 05004], lr: 0.001266, loss: 0.9325
2022-03-04 21:16:45 - train: epoch 0187, iter [04300, 05004], lr: 0.001266, loss: 0.8610
2022-03-04 21:17:17 - train: epoch 0187, iter [04400, 05004], lr: 0.001266, loss: 0.7261
2022-03-04 21:17:51 - train: epoch 0187, iter [04500, 05004], lr: 0.001266, loss: 0.7841
2022-03-04 21:18:24 - train: epoch 0187, iter [04600, 05004], lr: 0.001266, loss: 0.8087
2022-03-04 21:18:57 - train: epoch 0187, iter [04700, 05004], lr: 0.001266, loss: 0.7282
2022-03-04 21:19:30 - train: epoch 0187, iter [04800, 05004], lr: 0.001266, loss: 0.8632
2022-03-04 21:20:03 - train: epoch 0187, iter [04900, 05004], lr: 0.001266, loss: 0.5631
2022-03-04 21:20:34 - train: epoch 0187, iter [05000, 05004], lr: 0.001266, loss: 0.5598
2022-03-04 21:20:35 - train: epoch 187, train_loss: 0.8020
2022-03-04 21:21:49 - eval: epoch: 187, acc1: 76.622%, acc5: 93.264%, test_loss: 0.9297, per_image_load_time: 2.317ms, per_image_inference_time: 0.533ms
2022-03-04 21:21:50 - until epoch: 187, best_acc1: 76.622%
2022-03-04 21:21:50 - epoch 188 lr: 0.0010926199633097156
2022-03-04 21:22:28 - train: epoch 0188, iter [00100, 05004], lr: 0.001093, loss: 0.7887
2022-03-04 21:23:01 - train: epoch 0188, iter [00200, 05004], lr: 0.001093, loss: 0.8675
2022-03-04 21:23:34 - train: epoch 0188, iter [00300, 05004], lr: 0.001093, loss: 0.5844
2022-03-04 21:24:07 - train: epoch 0188, iter [00400, 05004], lr: 0.001093, loss: 0.7416
2022-03-04 21:24:40 - train: epoch 0188, iter [00500, 05004], lr: 0.001093, loss: 0.7144
2022-03-04 21:25:13 - train: epoch 0188, iter [00600, 05004], lr: 0.001093, loss: 0.6754
2022-03-04 21:25:46 - train: epoch 0188, iter [00700, 05004], lr: 0.001093, loss: 0.7876
2022-03-04 21:26:20 - train: epoch 0188, iter [00800, 05004], lr: 0.001093, loss: 0.8141
2022-03-04 21:26:54 - train: epoch 0188, iter [00900, 05004], lr: 0.001093, loss: 0.7206
2022-03-04 21:27:28 - train: epoch 0188, iter [01000, 05004], lr: 0.001093, loss: 0.7755
2022-03-04 21:28:01 - train: epoch 0188, iter [01100, 05004], lr: 0.001093, loss: 0.8393
2022-03-04 21:28:35 - train: epoch 0188, iter [01200, 05004], lr: 0.001093, loss: 0.8097
2022-03-04 21:29:08 - train: epoch 0188, iter [01300, 05004], lr: 0.001093, loss: 0.6675
2022-03-04 21:29:42 - train: epoch 0188, iter [01400, 05004], lr: 0.001093, loss: 0.7729
2022-03-04 21:30:14 - train: epoch 0188, iter [01500, 05004], lr: 0.001093, loss: 0.7514
2022-03-04 21:30:48 - train: epoch 0188, iter [01600, 05004], lr: 0.001093, loss: 0.8878
2022-03-04 21:31:21 - train: epoch 0188, iter [01700, 05004], lr: 0.001093, loss: 0.7324
2022-03-04 21:31:55 - train: epoch 0188, iter [01800, 05004], lr: 0.001093, loss: 0.7201
2022-03-04 21:32:28 - train: epoch 0188, iter [01900, 05004], lr: 0.001093, loss: 0.8503
2022-03-04 21:33:01 - train: epoch 0188, iter [02000, 05004], lr: 0.001093, loss: 0.8337
2022-03-04 21:33:34 - train: epoch 0188, iter [02100, 05004], lr: 0.001093, loss: 0.6371
2022-03-04 21:34:07 - train: epoch 0188, iter [02200, 05004], lr: 0.001093, loss: 0.7381
2022-03-04 21:34:40 - train: epoch 0188, iter [02300, 05004], lr: 0.001093, loss: 0.8965
2022-03-04 21:35:13 - train: epoch 0188, iter [02400, 05004], lr: 0.001093, loss: 0.6478
2022-03-04 21:35:47 - train: epoch 0188, iter [02500, 05004], lr: 0.001093, loss: 0.9422
2022-03-04 21:36:20 - train: epoch 0188, iter [02600, 05004], lr: 0.001093, loss: 0.8293
2022-03-04 21:36:53 - train: epoch 0188, iter [02700, 05004], lr: 0.001093, loss: 0.9776
2022-03-04 21:37:26 - train: epoch 0188, iter [02800, 05004], lr: 0.001093, loss: 0.7453
2022-03-04 21:38:00 - train: epoch 0188, iter [02900, 05004], lr: 0.001093, loss: 0.7207
2022-03-04 21:38:33 - train: epoch 0188, iter [03000, 05004], lr: 0.001093, loss: 0.9130
2022-03-04 21:39:06 - train: epoch 0188, iter [03100, 05004], lr: 0.001093, loss: 0.7242
2022-03-04 21:39:39 - train: epoch 0188, iter [03200, 05004], lr: 0.001093, loss: 0.8986
2022-03-04 21:40:12 - train: epoch 0188, iter [03300, 05004], lr: 0.001093, loss: 0.7794
2022-03-04 21:40:45 - train: epoch 0188, iter [03400, 05004], lr: 0.001093, loss: 0.9119
2022-03-04 21:41:18 - train: epoch 0188, iter [03500, 05004], lr: 0.001093, loss: 0.7353
2022-03-04 21:41:52 - train: epoch 0188, iter [03600, 05004], lr: 0.001093, loss: 0.8576
2022-03-04 21:42:25 - train: epoch 0188, iter [03700, 05004], lr: 0.001093, loss: 0.8572
2022-03-04 21:42:58 - train: epoch 0188, iter [03800, 05004], lr: 0.001093, loss: 0.7033
2022-03-04 21:43:32 - train: epoch 0188, iter [03900, 05004], lr: 0.001093, loss: 0.6989
2022-03-04 21:44:04 - train: epoch 0188, iter [04000, 05004], lr: 0.001093, loss: 0.7089
2022-03-04 21:44:37 - train: epoch 0188, iter [04100, 05004], lr: 0.001093, loss: 0.9494
2022-03-04 21:45:11 - train: epoch 0188, iter [04200, 05004], lr: 0.001093, loss: 0.7194
2022-03-04 21:45:44 - train: epoch 0188, iter [04300, 05004], lr: 0.001093, loss: 0.7481
2022-03-04 21:46:18 - train: epoch 0188, iter [04400, 05004], lr: 0.001093, loss: 0.9789
2022-03-04 21:46:51 - train: epoch 0188, iter [04500, 05004], lr: 0.001093, loss: 0.8503
2022-03-04 21:47:24 - train: epoch 0188, iter [04600, 05004], lr: 0.001093, loss: 0.7804
2022-03-04 21:47:57 - train: epoch 0188, iter [04700, 05004], lr: 0.001093, loss: 0.7035
2022-03-04 21:48:30 - train: epoch 0188, iter [04800, 05004], lr: 0.001093, loss: 0.7903
2022-03-04 21:49:03 - train: epoch 0188, iter [04900, 05004], lr: 0.001093, loss: 0.8894
2022-03-04 21:49:36 - train: epoch 0188, iter [05000, 05004], lr: 0.001093, loss: 0.6795
2022-03-04 21:49:37 - train: epoch 188, train_loss: 0.7892
2022-03-04 21:50:51 - eval: epoch: 188, acc1: 76.764%, acc5: 93.210%, test_loss: 0.9341, per_image_load_time: 2.332ms, per_image_inference_time: 0.530ms
2022-03-04 21:50:52 - until epoch: 188, best_acc1: 76.764%
2022-03-04 21:50:52 - epoch 189 lr: 0.0009314936930293283
2022-03-04 21:51:29 - train: epoch 0189, iter [00100, 05004], lr: 0.000931, loss: 0.7976
2022-03-04 21:52:02 - train: epoch 0189, iter [00200, 05004], lr: 0.000931, loss: 0.8277
2022-03-04 21:52:35 - train: epoch 0189, iter [00300, 05004], lr: 0.000931, loss: 0.6649
2022-03-04 21:53:08 - train: epoch 0189, iter [00400, 05004], lr: 0.000931, loss: 0.6850
2022-03-04 21:53:41 - train: epoch 0189, iter [00500, 05004], lr: 0.000931, loss: 0.6978
2022-03-04 21:54:15 - train: epoch 0189, iter [00600, 05004], lr: 0.000931, loss: 0.8157
2022-03-04 21:54:47 - train: epoch 0189, iter [00700, 05004], lr: 0.000931, loss: 0.8961
2022-03-04 21:55:21 - train: epoch 0189, iter [00800, 05004], lr: 0.000931, loss: 0.7657
2022-03-04 21:55:54 - train: epoch 0189, iter [00900, 05004], lr: 0.000931, loss: 0.7518
2022-03-04 21:56:27 - train: epoch 0189, iter [01000, 05004], lr: 0.000931, loss: 0.8320
2022-03-04 21:57:00 - train: epoch 0189, iter [01100, 05004], lr: 0.000931, loss: 0.9737
2022-03-04 21:57:33 - train: epoch 0189, iter [01200, 05004], lr: 0.000931, loss: 0.7588
2022-03-04 21:58:06 - train: epoch 0189, iter [01300, 05004], lr: 0.000931, loss: 0.9175
2022-03-04 21:58:40 - train: epoch 0189, iter [01400, 05004], lr: 0.000931, loss: 0.8485
2022-03-04 21:59:13 - train: epoch 0189, iter [01500, 05004], lr: 0.000931, loss: 0.7700
2022-03-04 21:59:47 - train: epoch 0189, iter [01600, 05004], lr: 0.000931, loss: 0.6687
2022-03-04 22:00:21 - train: epoch 0189, iter [01700, 05004], lr: 0.000931, loss: 0.8166
2022-03-04 22:00:53 - train: epoch 0189, iter [01800, 05004], lr: 0.000931, loss: 0.8347
2022-03-04 22:01:27 - train: epoch 0189, iter [01900, 05004], lr: 0.000931, loss: 0.8250
2022-03-04 22:02:00 - train: epoch 0189, iter [02000, 05004], lr: 0.000931, loss: 0.7834
2022-03-04 22:02:32 - train: epoch 0189, iter [02100, 05004], lr: 0.000931, loss: 0.6921
2022-03-04 22:03:06 - train: epoch 0189, iter [02200, 05004], lr: 0.000931, loss: 0.6387
2022-03-04 22:03:40 - train: epoch 0189, iter [02300, 05004], lr: 0.000931, loss: 0.7202
2022-03-04 22:04:13 - train: epoch 0189, iter [02400, 05004], lr: 0.000931, loss: 0.6778
2022-03-04 22:04:46 - train: epoch 0189, iter [02500, 05004], lr: 0.000931, loss: 0.7964
2022-03-04 22:05:19 - train: epoch 0189, iter [02600, 05004], lr: 0.000931, loss: 0.8231
2022-03-04 22:05:52 - train: epoch 0189, iter [02700, 05004], lr: 0.000931, loss: 0.7338
2022-03-04 22:06:26 - train: epoch 0189, iter [02800, 05004], lr: 0.000931, loss: 0.7663
2022-03-04 22:06:59 - train: epoch 0189, iter [02900, 05004], lr: 0.000931, loss: 1.0186
2022-03-04 22:07:32 - train: epoch 0189, iter [03000, 05004], lr: 0.000931, loss: 0.7604
2022-03-04 22:08:05 - train: epoch 0189, iter [03100, 05004], lr: 0.000931, loss: 0.6774
2022-03-04 22:08:39 - train: epoch 0189, iter [03200, 05004], lr: 0.000931, loss: 0.6937
2022-03-04 22:09:13 - train: epoch 0189, iter [03300, 05004], lr: 0.000931, loss: 0.8688
2022-03-04 22:09:46 - train: epoch 0189, iter [03400, 05004], lr: 0.000931, loss: 0.7173
2022-03-04 22:10:19 - train: epoch 0189, iter [03500, 05004], lr: 0.000931, loss: 0.8754
2022-03-04 22:10:53 - train: epoch 0189, iter [03600, 05004], lr: 0.000931, loss: 0.7282
2022-03-04 22:11:25 - train: epoch 0189, iter [03700, 05004], lr: 0.000931, loss: 0.8497
2022-03-04 22:11:59 - train: epoch 0189, iter [03800, 05004], lr: 0.000931, loss: 0.5614
2022-03-04 22:12:32 - train: epoch 0189, iter [03900, 05004], lr: 0.000931, loss: 0.7747
2022-03-04 22:13:06 - train: epoch 0189, iter [04000, 05004], lr: 0.000931, loss: 0.9007
2022-03-04 22:13:39 - train: epoch 0189, iter [04100, 05004], lr: 0.000931, loss: 0.7376
2022-03-04 22:14:12 - train: epoch 0189, iter [04200, 05004], lr: 0.000931, loss: 0.8438
2022-03-04 22:14:45 - train: epoch 0189, iter [04300, 05004], lr: 0.000931, loss: 0.6200
2022-03-04 22:15:18 - train: epoch 0189, iter [04400, 05004], lr: 0.000931, loss: 0.8704
2022-03-04 22:15:51 - train: epoch 0189, iter [04500, 05004], lr: 0.000931, loss: 0.6622
2022-03-04 22:16:24 - train: epoch 0189, iter [04600, 05004], lr: 0.000931, loss: 0.6308
2022-03-04 22:16:58 - train: epoch 0189, iter [04700, 05004], lr: 0.000931, loss: 0.7707
2022-03-04 22:17:31 - train: epoch 0189, iter [04800, 05004], lr: 0.000931, loss: 0.9091
2022-03-04 22:18:04 - train: epoch 0189, iter [04900, 05004], lr: 0.000931, loss: 0.6887
2022-03-04 22:18:36 - train: epoch 0189, iter [05000, 05004], lr: 0.000931, loss: 0.8363
2022-03-04 22:18:37 - train: epoch 189, train_loss: 0.7783
2022-03-04 22:19:51 - eval: epoch: 189, acc1: 76.824%, acc5: 93.256%, test_loss: 0.9318, per_image_load_time: 2.360ms, per_image_inference_time: 0.523ms
2022-03-04 22:19:52 - until epoch: 189, best_acc1: 76.824%
2022-03-04 22:19:52 - epoch 190 lr: 0.000783103156370113
2022-03-04 22:20:29 - train: epoch 0190, iter [00100, 05004], lr: 0.000783, loss: 0.8341
2022-03-04 22:21:03 - train: epoch 0190, iter [00200, 05004], lr: 0.000783, loss: 0.6592
2022-03-04 22:21:36 - train: epoch 0190, iter [00300, 05004], lr: 0.000783, loss: 0.8249
2022-03-04 22:22:09 - train: epoch 0190, iter [00400, 05004], lr: 0.000783, loss: 0.9260
2022-03-04 22:22:43 - train: epoch 0190, iter [00500, 05004], lr: 0.000783, loss: 0.6222
2022-03-04 22:23:15 - train: epoch 0190, iter [00600, 05004], lr: 0.000783, loss: 0.7369
2022-03-04 22:23:48 - train: epoch 0190, iter [00700, 05004], lr: 0.000783, loss: 0.8537
2022-03-04 22:24:21 - train: epoch 0190, iter [00800, 05004], lr: 0.000783, loss: 0.6423
2022-03-04 22:24:55 - train: epoch 0190, iter [00900, 05004], lr: 0.000783, loss: 0.7750
2022-03-04 22:25:27 - train: epoch 0190, iter [01000, 05004], lr: 0.000783, loss: 0.7164
2022-03-04 22:26:01 - train: epoch 0190, iter [01100, 05004], lr: 0.000783, loss: 0.7038
2022-03-04 22:26:34 - train: epoch 0190, iter [01200, 05004], lr: 0.000783, loss: 0.6466
2022-03-04 22:27:07 - train: epoch 0190, iter [01300, 05004], lr: 0.000783, loss: 0.9349
2022-03-04 22:27:41 - train: epoch 0190, iter [01400, 05004], lr: 0.000783, loss: 0.7071
2022-03-04 22:28:14 - train: epoch 0190, iter [01500, 05004], lr: 0.000783, loss: 0.7017
2022-03-04 22:28:46 - train: epoch 0190, iter [01600, 05004], lr: 0.000783, loss: 0.7857
2022-03-04 22:29:20 - train: epoch 0190, iter [01700, 05004], lr: 0.000783, loss: 0.6877
2022-03-04 22:29:53 - train: epoch 0190, iter [01800, 05004], lr: 0.000783, loss: 0.7345
2022-03-04 22:30:27 - train: epoch 0190, iter [01900, 05004], lr: 0.000783, loss: 0.7710
2022-03-04 22:31:00 - train: epoch 0190, iter [02000, 05004], lr: 0.000783, loss: 0.7626
2022-03-04 22:31:32 - train: epoch 0190, iter [02100, 05004], lr: 0.000783, loss: 0.8405
2022-03-04 22:32:07 - train: epoch 0190, iter [02200, 05004], lr: 0.000783, loss: 0.6956
2022-03-04 22:32:39 - train: epoch 0190, iter [02300, 05004], lr: 0.000783, loss: 0.9218
2022-03-04 22:33:12 - train: epoch 0190, iter [02400, 05004], lr: 0.000783, loss: 0.8745
2022-03-04 22:33:45 - train: epoch 0190, iter [02500, 05004], lr: 0.000783, loss: 0.6868
2022-03-04 22:34:18 - train: epoch 0190, iter [02600, 05004], lr: 0.000783, loss: 0.7304
