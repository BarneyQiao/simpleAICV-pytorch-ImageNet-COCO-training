2022-07-25 07:10:42 - train: epoch 0017, iter [04500, 05004], lr: 0.192357, loss: 2.1983
2022-07-25 07:12:36 - train: epoch 0017, iter [04600, 05004], lr: 0.192332, loss: 1.9179
2022-07-25 07:14:45 - train: epoch 0017, iter [04700, 05004], lr: 0.192306, loss: 2.2663
2022-07-25 07:16:48 - train: epoch 0017, iter [04800, 05004], lr: 0.192281, loss: 2.0846
2022-07-25 07:18:51 - train: epoch 0017, iter [04900, 05004], lr: 0.192256, loss: 2.1573
2022-07-25 07:20:51 - train: epoch 0017, iter [05000, 05004], lr: 0.192230, loss: 1.9693
2022-07-25 07:20:54 - train: epoch 017, train_loss: 2.1512
2022-07-25 07:24:36 - eval: epoch: 017, acc1: 56.378%, acc5: 80.784%, test_loss: 1.8481, per_image_load_time: 5.524ms, per_image_inference_time: 0.911ms
2022-07-25 07:24:37 - until epoch: 017, best_acc1: 56.378%
2022-07-25 07:24:37 - epoch 018 lr: 0.192229
2022-07-25 07:26:55 - train: epoch 0018, iter [00100, 05004], lr: 0.192203, loss: 1.9535
2022-07-25 07:28:56 - train: epoch 0018, iter [00200, 05004], lr: 0.192178, loss: 2.3521
2022-07-25 07:30:56 - train: epoch 0018, iter [00300, 05004], lr: 0.192152, loss: 2.2797
2022-07-25 07:32:57 - train: epoch 0018, iter [00400, 05004], lr: 0.192126, loss: 2.5221
2022-07-25 07:34:59 - train: epoch 0018, iter [00500, 05004], lr: 0.192101, loss: 2.1037
2022-07-25 07:36:56 - train: epoch 0018, iter [00600, 05004], lr: 0.192075, loss: 2.2051
2022-07-25 07:39:03 - train: epoch 0018, iter [00700, 05004], lr: 0.192049, loss: 1.9306
2022-07-25 07:41:03 - train: epoch 0018, iter [00800, 05004], lr: 0.192023, loss: 2.1152
2022-07-25 07:43:04 - train: epoch 0018, iter [00900, 05004], lr: 0.191997, loss: 2.0945
2022-07-25 07:45:01 - train: epoch 0018, iter [01000, 05004], lr: 0.191972, loss: 2.0052
2022-07-25 07:47:06 - train: epoch 0018, iter [01100, 05004], lr: 0.191946, loss: 2.3329
2022-07-25 07:49:06 - train: epoch 0018, iter [01200, 05004], lr: 0.191920, loss: 2.0177
2022-07-25 07:51:04 - train: epoch 0018, iter [01300, 05004], lr: 0.191894, loss: 2.1979
2022-07-25 07:53:04 - train: epoch 0018, iter [01400, 05004], lr: 0.191867, loss: 2.1275
2022-07-25 07:55:05 - train: epoch 0018, iter [01500, 05004], lr: 0.191841, loss: 2.3449
2022-07-25 07:57:04 - train: epoch 0018, iter [01600, 05004], lr: 0.191815, loss: 2.1816
2022-07-25 07:59:00 - train: epoch 0018, iter [01700, 05004], lr: 0.191789, loss: 2.2053
2022-07-25 08:00:59 - train: epoch 0018, iter [01800, 05004], lr: 0.191763, loss: 2.1499
2022-07-25 08:02:55 - train: epoch 0018, iter [01900, 05004], lr: 0.191736, loss: 2.2780
2022-07-25 08:05:04 - train: epoch 0018, iter [02000, 05004], lr: 0.191710, loss: 2.3591
2022-07-25 08:07:13 - train: epoch 0018, iter [02100, 05004], lr: 0.191684, loss: 2.2225
2022-07-25 08:09:22 - train: epoch 0018, iter [02200, 05004], lr: 0.191657, loss: 2.2084
2022-07-25 08:11:16 - train: epoch 0018, iter [02300, 05004], lr: 0.191631, loss: 2.1630
2022-07-25 08:13:09 - train: epoch 0018, iter [02400, 05004], lr: 0.191604, loss: 1.9442
2022-07-25 08:15:04 - train: epoch 0018, iter [02500, 05004], lr: 0.191578, loss: 1.7970
2022-07-25 08:16:58 - train: epoch 0018, iter [02600, 05004], lr: 0.191551, loss: 2.0820
2022-07-25 08:18:59 - train: epoch 0018, iter [02700, 05004], lr: 0.191525, loss: 2.0627
2022-07-25 08:20:53 - train: epoch 0018, iter [02800, 05004], lr: 0.191498, loss: 1.9528
2022-07-25 08:22:50 - train: epoch 0018, iter [02900, 05004], lr: 0.191471, loss: 2.1411
2022-07-25 08:24:46 - train: epoch 0018, iter [03000, 05004], lr: 0.191445, loss: 2.0671
2022-07-25 08:26:41 - train: epoch 0018, iter [03100, 05004], lr: 0.191418, loss: 2.4884
2022-07-25 08:28:36 - train: epoch 0018, iter [03200, 05004], lr: 0.191391, loss: 2.1954
2022-07-25 08:30:33 - train: epoch 0018, iter [03300, 05004], lr: 0.191364, loss: 2.0386
2022-07-25 08:32:30 - train: epoch 0018, iter [03400, 05004], lr: 0.191337, loss: 1.9894
2022-07-25 08:34:25 - train: epoch 0018, iter [03500, 05004], lr: 0.191310, loss: 2.1006
2022-07-25 08:36:20 - train: epoch 0018, iter [03600, 05004], lr: 0.191283, loss: 2.1710
2022-07-25 08:38:18 - train: epoch 0018, iter [03700, 05004], lr: 0.191256, loss: 2.4762
2022-07-25 08:40:05 - train: epoch 0018, iter [03800, 05004], lr: 0.191229, loss: 2.5118
2022-07-25 08:42:05 - train: epoch 0018, iter [03900, 05004], lr: 0.191202, loss: 2.3724
2022-07-25 08:44:00 - train: epoch 0018, iter [04000, 05004], lr: 0.191175, loss: 2.2927
2022-07-25 08:45:55 - train: epoch 0018, iter [04100, 05004], lr: 0.191148, loss: 2.3413
2022-07-25 08:47:52 - train: epoch 0018, iter [04200, 05004], lr: 0.191121, loss: 2.1235
2022-07-25 08:49:48 - train: epoch 0018, iter [04300, 05004], lr: 0.191094, loss: 2.0917
2022-07-25 08:51:47 - train: epoch 0018, iter [04400, 05004], lr: 0.191066, loss: 2.0954
2022-07-25 08:53:48 - train: epoch 0018, iter [04500, 05004], lr: 0.191039, loss: 2.2612
2022-07-25 08:55:43 - train: epoch 0018, iter [04600, 05004], lr: 0.191012, loss: 2.2155
2022-07-25 08:57:35 - train: epoch 0018, iter [04700, 05004], lr: 0.190984, loss: 2.3851
2022-07-25 08:59:35 - train: epoch 0018, iter [04800, 05004], lr: 0.190957, loss: 2.0672
2022-07-25 09:01:41 - train: epoch 0018, iter [04900, 05004], lr: 0.190929, loss: 2.1871
2022-07-25 09:03:25 - train: epoch 0018, iter [05000, 05004], lr: 0.190902, loss: 2.1132
2022-07-25 09:03:29 - train: epoch 018, train_loss: 2.1379
2022-07-25 09:07:11 - eval: epoch: 018, acc1: 55.600%, acc5: 80.292%, test_loss: 1.8905, per_image_load_time: 5.310ms, per_image_inference_time: 0.757ms
2022-07-25 09:07:12 - until epoch: 018, best_acc1: 56.378%
2022-07-25 09:07:12 - epoch 019 lr: 0.190900
2022-07-25 09:09:26 - train: epoch 0019, iter [00100, 05004], lr: 0.190873, loss: 1.8380
2022-07-25 09:11:22 - train: epoch 0019, iter [00200, 05004], lr: 0.190845, loss: 2.0403
2022-07-25 09:13:19 - train: epoch 0019, iter [00300, 05004], lr: 0.190818, loss: 2.1362
2022-07-25 09:15:13 - train: epoch 0019, iter [00400, 05004], lr: 0.190790, loss: 1.9739
2022-07-25 09:17:06 - train: epoch 0019, iter [00500, 05004], lr: 0.190762, loss: 1.9420
2022-07-25 09:19:02 - train: epoch 0019, iter [00600, 05004], lr: 0.190735, loss: 2.1088
2022-07-25 09:21:06 - train: epoch 0019, iter [00700, 05004], lr: 0.190707, loss: 1.8739
2022-07-25 09:23:10 - train: epoch 0019, iter [00800, 05004], lr: 0.190679, loss: 2.2681
2022-07-25 09:25:16 - train: epoch 0019, iter [00900, 05004], lr: 0.190651, loss: 2.2951
2022-07-25 09:27:19 - train: epoch 0019, iter [01000, 05004], lr: 0.190623, loss: 2.1207
2022-07-25 09:29:21 - train: epoch 0019, iter [01100, 05004], lr: 0.190595, loss: 2.1006
2022-07-25 09:31:24 - train: epoch 0019, iter [01200, 05004], lr: 0.190567, loss: 2.1102
2022-07-25 09:33:24 - train: epoch 0019, iter [01300, 05004], lr: 0.190539, loss: 2.3563
2022-07-25 09:35:26 - train: epoch 0019, iter [01400, 05004], lr: 0.190511, loss: 1.9074
2022-07-25 09:37:24 - train: epoch 0019, iter [01500, 05004], lr: 0.190483, loss: 2.4272
2022-07-25 09:39:16 - train: epoch 0019, iter [01600, 05004], lr: 0.190455, loss: 1.9547
2022-07-25 09:41:14 - train: epoch 0019, iter [01700, 05004], lr: 0.190427, loss: 2.1699
2022-07-25 09:43:15 - train: epoch 0019, iter [01800, 05004], lr: 0.190398, loss: 2.0256
2022-07-25 09:45:24 - train: epoch 0019, iter [01900, 05004], lr: 0.190370, loss: 2.3297
2022-07-25 09:47:34 - train: epoch 0019, iter [02000, 05004], lr: 0.190342, loss: 2.0951
2022-07-25 09:49:35 - train: epoch 0019, iter [02100, 05004], lr: 0.190314, loss: 1.8254
2022-07-25 09:51:37 - train: epoch 0019, iter [02200, 05004], lr: 0.190285, loss: 2.1215
2022-07-25 09:53:39 - train: epoch 0019, iter [02300, 05004], lr: 0.190257, loss: 2.1481
2022-07-25 09:55:39 - train: epoch 0019, iter [02400, 05004], lr: 0.190228, loss: 2.2254
2022-07-25 09:57:41 - train: epoch 0019, iter [02500, 05004], lr: 0.190200, loss: 2.2092
2022-07-25 09:59:38 - train: epoch 0019, iter [02600, 05004], lr: 0.190171, loss: 2.0898
2022-07-25 10:01:45 - train: epoch 0019, iter [02700, 05004], lr: 0.190143, loss: 1.8847
2022-07-25 10:03:41 - train: epoch 0019, iter [02800, 05004], lr: 0.190114, loss: 2.1846
2022-07-25 10:05:44 - train: epoch 0019, iter [02900, 05004], lr: 0.190085, loss: 2.1505
2022-07-25 10:07:45 - train: epoch 0019, iter [03000, 05004], lr: 0.190057, loss: 2.1988
2022-07-25 10:09:44 - train: epoch 0019, iter [03100, 05004], lr: 0.190028, loss: 2.2777
2022-07-25 10:11:47 - train: epoch 0019, iter [03200, 05004], lr: 0.189999, loss: 1.8077
2022-07-25 10:13:50 - train: epoch 0019, iter [03300, 05004], lr: 0.189970, loss: 2.1175
2022-07-25 10:15:51 - train: epoch 0019, iter [03400, 05004], lr: 0.189941, loss: 2.1766
2022-07-25 10:17:53 - train: epoch 0019, iter [03500, 05004], lr: 0.189912, loss: 2.4034
2022-07-25 10:19:53 - train: epoch 0019, iter [03600, 05004], lr: 0.189883, loss: 1.7904
2022-07-25 10:21:55 - train: epoch 0019, iter [03700, 05004], lr: 0.189854, loss: 2.1829
2022-07-25 10:23:56 - train: epoch 0019, iter [03800, 05004], lr: 0.189825, loss: 2.3172
2022-07-25 10:25:56 - train: epoch 0019, iter [03900, 05004], lr: 0.189796, loss: 2.0618
2022-07-25 10:27:53 - train: epoch 0019, iter [04000, 05004], lr: 0.189767, loss: 2.0427
2022-07-25 10:29:57 - train: epoch 0019, iter [04100, 05004], lr: 0.189738, loss: 2.2404
2022-07-25 10:31:55 - train: epoch 0019, iter [04200, 05004], lr: 0.189709, loss: 2.0302
2022-07-25 10:33:49 - train: epoch 0019, iter [04300, 05004], lr: 0.189680, loss: 2.2820
2022-07-25 10:35:49 - train: epoch 0019, iter [04400, 05004], lr: 0.189650, loss: 2.2565
2022-07-25 10:37:44 - train: epoch 0019, iter [04500, 05004], lr: 0.189621, loss: 2.5313
2022-07-25 10:39:42 - train: epoch 0019, iter [04600, 05004], lr: 0.189592, loss: 2.0751
2022-07-25 10:41:40 - train: epoch 0019, iter [04700, 05004], lr: 0.189562, loss: 2.2863
2022-07-25 10:43:27 - train: epoch 0019, iter [04800, 05004], lr: 0.189533, loss: 2.0873
2022-07-25 10:45:39 - train: epoch 0019, iter [04900, 05004], lr: 0.189504, loss: 1.9848
2022-07-25 10:47:39 - train: epoch 0019, iter [05000, 05004], lr: 0.189474, loss: 2.2049
2022-07-25 10:47:44 - train: epoch 019, train_loss: 2.1248
2022-07-25 10:51:28 - eval: epoch: 019, acc1: 55.738%, acc5: 80.342%, test_loss: 1.8776, per_image_load_time: 7.250ms, per_image_inference_time: 0.895ms
2022-07-25 10:51:28 - until epoch: 019, best_acc1: 56.378%
2022-07-25 10:51:28 - epoch 020 lr: 0.189473
2022-07-25 10:53:54 - train: epoch 0020, iter [00100, 05004], lr: 0.189443, loss: 2.1454
2022-07-25 10:55:54 - train: epoch 0020, iter [00200, 05004], lr: 0.189414, loss: 1.9306
2022-07-25 10:58:01 - train: epoch 0020, iter [00300, 05004], lr: 0.189384, loss: 2.0255
2022-07-25 11:00:00 - train: epoch 0020, iter [00400, 05004], lr: 0.189355, loss: 1.9150
2022-07-25 11:01:59 - train: epoch 0020, iter [00500, 05004], lr: 0.189325, loss: 1.8952
2022-07-25 11:04:00 - train: epoch 0020, iter [00600, 05004], lr: 0.189295, loss: 2.0090
2022-07-25 11:06:05 - train: epoch 0020, iter [00700, 05004], lr: 0.189265, loss: 1.9434
2022-07-25 11:08:03 - train: epoch 0020, iter [00800, 05004], lr: 0.189236, loss: 2.0865
2022-07-25 11:10:06 - train: epoch 0020, iter [00900, 05004], lr: 0.189206, loss: 2.1797
2022-07-25 11:12:06 - train: epoch 0020, iter [01000, 05004], lr: 0.189176, loss: 2.0213
2022-07-25 11:14:13 - train: epoch 0020, iter [01100, 05004], lr: 0.189146, loss: 2.0678
2022-07-25 11:16:14 - train: epoch 0020, iter [01200, 05004], lr: 0.189116, loss: 1.9410
2022-07-25 11:18:21 - train: epoch 0020, iter [01300, 05004], lr: 0.189086, loss: 1.8671
2022-07-25 11:20:22 - train: epoch 0020, iter [01400, 05004], lr: 0.189056, loss: 2.2314
2022-07-25 11:22:22 - train: epoch 0020, iter [01500, 05004], lr: 0.189026, loss: 2.1589
2022-07-25 11:24:21 - train: epoch 0020, iter [01600, 05004], lr: 0.188996, loss: 2.1740
2022-07-25 11:26:20 - train: epoch 0020, iter [01700, 05004], lr: 0.188966, loss: 1.9728
2022-07-25 11:28:25 - train: epoch 0020, iter [01800, 05004], lr: 0.188935, loss: 2.1791
2022-07-25 11:30:23 - train: epoch 0020, iter [01900, 05004], lr: 0.188905, loss: 2.0736
2022-07-25 11:32:30 - train: epoch 0020, iter [02000, 05004], lr: 0.188875, loss: 1.9360
2022-07-25 11:34:28 - train: epoch 0020, iter [02100, 05004], lr: 0.188845, loss: 2.2428
2022-07-25 11:36:21 - train: epoch 0020, iter [02200, 05004], lr: 0.188814, loss: 2.1689
2022-07-25 11:38:14 - train: epoch 0020, iter [02300, 05004], lr: 0.188784, loss: 2.2351
2022-07-25 11:40:08 - train: epoch 0020, iter [02400, 05004], lr: 0.188753, loss: 2.2731
2022-07-25 11:42:03 - train: epoch 0020, iter [02500, 05004], lr: 0.188723, loss: 1.9864
2022-07-25 11:43:54 - train: epoch 0020, iter [02600, 05004], lr: 0.188692, loss: 1.9956
2022-07-25 11:45:45 - train: epoch 0020, iter [02700, 05004], lr: 0.188662, loss: 2.2160
2022-07-25 11:47:38 - train: epoch 0020, iter [02800, 05004], lr: 0.188631, loss: 2.4249
2022-07-25 11:49:26 - train: epoch 0020, iter [02900, 05004], lr: 0.188601, loss: 2.2293
2022-07-25 11:51:23 - train: epoch 0020, iter [03000, 05004], lr: 0.188570, loss: 2.2369
2022-07-25 11:53:14 - train: epoch 0020, iter [03100, 05004], lr: 0.188539, loss: 2.1471
2022-07-25 11:55:09 - train: epoch 0020, iter [03200, 05004], lr: 0.188509, loss: 2.4607
2022-07-25 11:57:09 - train: epoch 0020, iter [03300, 05004], lr: 0.188478, loss: 1.9204
2022-07-25 11:59:01 - train: epoch 0020, iter [03400, 05004], lr: 0.188447, loss: 2.1646
2022-07-25 12:01:01 - train: epoch 0020, iter [03500, 05004], lr: 0.188416, loss: 1.8443
2022-07-25 12:02:55 - train: epoch 0020, iter [03600, 05004], lr: 0.188385, loss: 1.9390
2022-07-25 12:04:49 - train: epoch 0020, iter [03700, 05004], lr: 0.188354, loss: 2.2362
2022-07-25 12:06:41 - train: epoch 0020, iter [03800, 05004], lr: 0.188323, loss: 2.1412
2022-07-25 12:08:36 - train: epoch 0020, iter [03900, 05004], lr: 0.188292, loss: 2.0940
2022-07-25 12:10:32 - train: epoch 0020, iter [04000, 05004], lr: 0.188261, loss: 2.0364
2022-07-25 12:12:25 - train: epoch 0020, iter [04100, 05004], lr: 0.188230, loss: 1.9680
2022-07-25 12:14:19 - train: epoch 0020, iter [04200, 05004], lr: 0.188199, loss: 2.0188
2022-07-25 12:16:15 - train: epoch 0020, iter [04300, 05004], lr: 0.188168, loss: 2.1728
2022-07-25 12:18:11 - train: epoch 0020, iter [04400, 05004], lr: 0.188137, loss: 2.1065
2022-07-25 12:20:08 - train: epoch 0020, iter [04500, 05004], lr: 0.188105, loss: 2.2739
2022-07-25 12:21:58 - train: epoch 0020, iter [04600, 05004], lr: 0.188074, loss: 2.0662
2022-07-25 12:23:54 - train: epoch 0020, iter [04700, 05004], lr: 0.188043, loss: 2.2397
2022-07-25 12:25:49 - train: epoch 0020, iter [04800, 05004], lr: 0.188011, loss: 2.0532
2022-07-25 12:27:30 - train: epoch 0020, iter [04900, 05004], lr: 0.187980, loss: 2.1049
2022-07-25 12:29:44 - train: epoch 0020, iter [05000, 05004], lr: 0.187949, loss: 1.8926
2022-07-25 12:29:49 - train: epoch 020, train_loss: 2.1127
2022-07-25 12:33:27 - eval: epoch: 020, acc1: 56.244%, acc5: 80.862%, test_loss: 1.8568, per_image_load_time: 4.250ms, per_image_inference_time: 0.864ms
2022-07-25 12:33:28 - until epoch: 020, best_acc1: 56.378%
2022-07-25 12:33:28 - epoch 021 lr: 0.187947
2022-07-25 12:35:40 - train: epoch 0021, iter [00100, 05004], lr: 0.187916, loss: 2.2156
2022-07-25 12:37:37 - train: epoch 0021, iter [00200, 05004], lr: 0.187884, loss: 2.2082
2022-07-25 12:39:31 - train: epoch 0021, iter [00300, 05004], lr: 0.187853, loss: 1.9997
2022-07-25 12:41:28 - train: epoch 0021, iter [00400, 05004], lr: 0.187821, loss: 2.0532
2022-07-25 12:43:20 - train: epoch 0021, iter [00500, 05004], lr: 0.187790, loss: 1.9254
2022-07-25 12:45:13 - train: epoch 0021, iter [00600, 05004], lr: 0.187758, loss: 1.9900
2022-07-25 12:47:04 - train: epoch 0021, iter [00700, 05004], lr: 0.187726, loss: 1.9606
2022-07-25 12:48:55 - train: epoch 0021, iter [00800, 05004], lr: 0.187695, loss: 2.2100
2022-07-25 12:50:45 - train: epoch 0021, iter [00900, 05004], lr: 0.187663, loss: 1.9452
2022-07-25 12:52:35 - train: epoch 0021, iter [01000, 05004], lr: 0.187631, loss: 2.0003
2022-07-25 12:54:33 - train: epoch 0021, iter [01100, 05004], lr: 0.187599, loss: 2.0909
2022-07-25 12:56:20 - train: epoch 0021, iter [01200, 05004], lr: 0.187567, loss: 1.8394
2022-07-25 12:58:10 - train: epoch 0021, iter [01300, 05004], lr: 0.187535, loss: 1.9363
2022-07-25 13:00:04 - train: epoch 0021, iter [01400, 05004], lr: 0.187503, loss: 2.0046
2022-07-25 13:01:51 - train: epoch 0021, iter [01500, 05004], lr: 0.187471, loss: 1.9243
2022-07-25 13:03:39 - train: epoch 0021, iter [01600, 05004], lr: 0.187439, loss: 1.9818
2022-07-25 13:05:30 - train: epoch 0021, iter [01700, 05004], lr: 0.187407, loss: 2.2723
2022-07-25 13:07:18 - train: epoch 0021, iter [01800, 05004], lr: 0.187375, loss: 1.7806
2022-07-25 13:09:07 - train: epoch 0021, iter [01900, 05004], lr: 0.187343, loss: 2.1988
2022-07-25 13:10:56 - train: epoch 0021, iter [02000, 05004], lr: 0.187311, loss: 2.1320
2022-07-25 13:12:43 - train: epoch 0021, iter [02100, 05004], lr: 0.187278, loss: 2.0276
2022-07-25 13:14:35 - train: epoch 0021, iter [02200, 05004], lr: 0.187246, loss: 2.0687
2022-07-25 13:16:27 - train: epoch 0021, iter [02300, 05004], lr: 0.187214, loss: 2.2529
2022-07-25 13:18:12 - train: epoch 0021, iter [02400, 05004], lr: 0.187181, loss: 2.0479
2022-07-25 13:20:05 - train: epoch 0021, iter [02500, 05004], lr: 0.187149, loss: 2.1976
2022-07-25 13:21:53 - train: epoch 0021, iter [02600, 05004], lr: 0.187117, loss: 2.3667
2022-07-25 13:23:40 - train: epoch 0021, iter [02700, 05004], lr: 0.187084, loss: 2.0057
2022-07-25 13:25:28 - train: epoch 0021, iter [02800, 05004], lr: 0.187052, loss: 1.9507
2022-07-25 13:27:15 - train: epoch 0021, iter [02900, 05004], lr: 0.187019, loss: 2.0386
2022-07-25 13:29:07 - train: epoch 0021, iter [03000, 05004], lr: 0.186987, loss: 2.2790
2022-07-25 13:30:55 - train: epoch 0021, iter [03100, 05004], lr: 0.186954, loss: 2.1340
2022-07-25 13:32:52 - train: epoch 0021, iter [03200, 05004], lr: 0.186921, loss: 1.9947
2022-07-25 13:34:39 - train: epoch 0021, iter [03300, 05004], lr: 0.186889, loss: 2.2393
2022-07-25 13:36:31 - train: epoch 0021, iter [03400, 05004], lr: 0.186856, loss: 2.1722
2022-07-25 13:38:22 - train: epoch 0021, iter [03500, 05004], lr: 0.186823, loss: 2.0555
2022-07-25 13:40:14 - train: epoch 0021, iter [03600, 05004], lr: 0.186790, loss: 2.0451
2022-07-25 13:42:05 - train: epoch 0021, iter [03700, 05004], lr: 0.186757, loss: 1.9850
2022-07-25 13:43:53 - train: epoch 0021, iter [03800, 05004], lr: 0.186725, loss: 2.1357
2022-07-25 13:45:47 - train: epoch 0021, iter [03900, 05004], lr: 0.186692, loss: 2.0981
2022-07-25 13:47:31 - train: epoch 0021, iter [04000, 05004], lr: 0.186659, loss: 2.3597
2022-07-25 13:49:25 - train: epoch 0021, iter [04100, 05004], lr: 0.186626, loss: 2.1521
2022-07-25 13:51:18 - train: epoch 0021, iter [04200, 05004], lr: 0.186593, loss: 2.0152
2022-07-25 13:53:15 - train: epoch 0021, iter [04300, 05004], lr: 0.186560, loss: 2.3260
2022-07-25 13:55:09 - train: epoch 0021, iter [04400, 05004], lr: 0.186526, loss: 2.1615
2022-07-25 13:57:03 - train: epoch 0021, iter [04500, 05004], lr: 0.186493, loss: 2.1019
2022-07-25 13:58:59 - train: epoch 0021, iter [04600, 05004], lr: 0.186460, loss: 1.9607
2022-07-25 14:00:50 - train: epoch 0021, iter [04700, 05004], lr: 0.186427, loss: 2.2733
2022-07-25 14:02:49 - train: epoch 0021, iter [04800, 05004], lr: 0.186394, loss: 2.2061
2022-07-25 14:04:45 - train: epoch 0021, iter [04900, 05004], lr: 0.186360, loss: 1.9201
2022-07-25 14:06:39 - train: epoch 0021, iter [05000, 05004], lr: 0.186327, loss: 2.1332
2022-07-25 14:06:45 - train: epoch 021, train_loss: 2.1012
2022-07-25 14:10:31 - eval: epoch: 021, acc1: 56.636%, acc5: 81.210%, test_loss: 1.8253, per_image_load_time: 6.544ms, per_image_inference_time: 0.834ms
2022-07-25 14:10:32 - until epoch: 021, best_acc1: 56.636%
2022-07-25 14:10:32 - epoch 022 lr: 0.186325
2022-07-25 14:12:47 - train: epoch 0022, iter [00100, 05004], lr: 0.186292, loss: 1.8328
2022-07-25 14:14:40 - train: epoch 0022, iter [00200, 05004], lr: 0.186259, loss: 2.0430
2022-07-25 14:16:35 - train: epoch 0022, iter [00300, 05004], lr: 0.186225, loss: 1.9283
2022-07-25 14:18:32 - train: epoch 0022, iter [00400, 05004], lr: 0.186192, loss: 2.0899
2022-07-25 14:20:30 - train: epoch 0022, iter [00500, 05004], lr: 0.186158, loss: 2.0256
2022-07-25 14:22:24 - train: epoch 0022, iter [00600, 05004], lr: 0.186125, loss: 1.9836
2022-07-25 14:24:22 - train: epoch 0022, iter [00700, 05004], lr: 0.186091, loss: 2.2928
2022-07-25 14:26:18 - train: epoch 0022, iter [00800, 05004], lr: 0.186058, loss: 2.1554
2022-07-25 14:28:14 - train: epoch 0022, iter [00900, 05004], lr: 0.186024, loss: 2.3732
2022-07-25 14:30:13 - train: epoch 0022, iter [01000, 05004], lr: 0.185990, loss: 2.0086
2022-07-25 14:32:08 - train: epoch 0022, iter [01100, 05004], lr: 0.185956, loss: 1.9618
2022-07-25 14:34:04 - train: epoch 0022, iter [01200, 05004], lr: 0.185923, loss: 1.7682
2022-07-25 14:35:56 - train: epoch 0022, iter [01300, 05004], lr: 0.185889, loss: 2.0626
2022-07-25 14:37:52 - train: epoch 0022, iter [01400, 05004], lr: 0.185855, loss: 2.1667
2022-07-25 14:39:49 - train: epoch 0022, iter [01500, 05004], lr: 0.185821, loss: 2.2034
2022-07-25 14:41:43 - train: epoch 0022, iter [01600, 05004], lr: 0.185787, loss: 1.8788
2022-07-25 14:43:39 - train: epoch 0022, iter [01700, 05004], lr: 0.185753, loss: 2.1757
2022-07-25 14:45:38 - train: epoch 0022, iter [01800, 05004], lr: 0.185719, loss: 2.4196
2022-07-25 14:47:35 - train: epoch 0022, iter [01900, 05004], lr: 0.185685, loss: 2.0831
2022-07-25 14:49:29 - train: epoch 0022, iter [02000, 05004], lr: 0.185651, loss: 2.1486
2022-07-25 14:51:27 - train: epoch 0022, iter [02100, 05004], lr: 0.185617, loss: 2.1320
2022-07-25 14:53:18 - train: epoch 0022, iter [02200, 05004], lr: 0.185583, loss: 1.8230
2022-07-25 14:55:14 - train: epoch 0022, iter [02300, 05004], lr: 0.185548, loss: 2.3669
2022-07-25 14:57:06 - train: epoch 0022, iter [02400, 05004], lr: 0.185514, loss: 2.3001
2022-07-25 14:58:59 - train: epoch 0022, iter [02500, 05004], lr: 0.185480, loss: 2.2251
2022-07-25 15:00:51 - train: epoch 0022, iter [02600, 05004], lr: 0.185446, loss: 2.0325
2022-07-25 15:02:45 - train: epoch 0022, iter [02700, 05004], lr: 0.185411, loss: 2.0087
2022-07-25 15:04:38 - train: epoch 0022, iter [02800, 05004], lr: 0.185377, loss: 2.3226
2022-07-25 15:06:32 - train: epoch 0022, iter [02900, 05004], lr: 0.185342, loss: 2.0917
2022-07-25 15:08:27 - train: epoch 0022, iter [03000, 05004], lr: 0.185308, loss: 2.0489
2022-07-25 15:10:20 - train: epoch 0022, iter [03100, 05004], lr: 0.185274, loss: 2.4182
2022-07-25 15:12:13 - train: epoch 0022, iter [03200, 05004], lr: 0.185239, loss: 2.0185
2022-07-25 15:14:08 - train: epoch 0022, iter [03300, 05004], lr: 0.185204, loss: 2.1727
2022-07-25 15:16:01 - train: epoch 0022, iter [03400, 05004], lr: 0.185170, loss: 2.0559
2022-07-25 15:17:58 - train: epoch 0022, iter [03500, 05004], lr: 0.185135, loss: 2.1632
2022-07-25 15:19:53 - train: epoch 0022, iter [03600, 05004], lr: 0.185100, loss: 2.0966
2022-07-25 15:21:49 - train: epoch 0022, iter [03700, 05004], lr: 0.185066, loss: 2.0962
2022-07-25 15:23:42 - train: epoch 0022, iter [03800, 05004], lr: 0.185031, loss: 2.3531
2022-07-25 15:25:33 - train: epoch 0022, iter [03900, 05004], lr: 0.184996, loss: 2.0928
2022-07-25 15:27:34 - train: epoch 0022, iter [04000, 05004], lr: 0.184961, loss: 2.0898
2022-07-25 15:29:26 - train: epoch 0022, iter [04100, 05004], lr: 0.184926, loss: 2.0366
2022-07-25 15:31:21 - train: epoch 0022, iter [04200, 05004], lr: 0.184892, loss: 2.2356
2022-07-25 15:33:13 - train: epoch 0022, iter [04300, 05004], lr: 0.184857, loss: 2.2197
2022-07-25 15:35:04 - train: epoch 0022, iter [04400, 05004], lr: 0.184822, loss: 2.0800
2022-07-25 15:37:02 - train: epoch 0022, iter [04500, 05004], lr: 0.184787, loss: 2.0520
2022-07-25 15:38:55 - train: epoch 0022, iter [04600, 05004], lr: 0.184752, loss: 2.4001
2022-07-25 15:40:45 - train: epoch 0022, iter [04700, 05004], lr: 0.184716, loss: 2.1189
2022-07-25 15:42:46 - train: epoch 0022, iter [04800, 05004], lr: 0.184681, loss: 1.9430
2022-07-25 15:44:36 - train: epoch 0022, iter [04900, 05004], lr: 0.184646, loss: 2.0947
2022-07-25 15:46:25 - train: epoch 0022, iter [05000, 05004], lr: 0.184611, loss: 1.8690
2022-07-25 15:46:29 - train: epoch 022, train_loss: 2.0913
2022-07-25 15:50:41 - eval: epoch: 022, acc1: 56.458%, acc5: 80.900%, test_loss: 1.8547, per_image_load_time: 8.038ms, per_image_inference_time: 0.758ms
2022-07-25 15:50:41 - until epoch: 022, best_acc1: 56.636%
2022-07-25 15:50:41 - epoch 023 lr: 0.184609
2022-07-25 15:52:55 - train: epoch 0023, iter [00100, 05004], lr: 0.184574, loss: 2.0814
2022-07-25 15:54:44 - train: epoch 0023, iter [00200, 05004], lr: 0.184539, loss: 2.0522
2022-07-25 15:56:36 - train: epoch 0023, iter [00300, 05004], lr: 0.184504, loss: 2.0402
2022-07-25 15:58:28 - train: epoch 0023, iter [00400, 05004], lr: 0.184468, loss: 2.1051
2022-07-25 16:00:20 - train: epoch 0023, iter [00500, 05004], lr: 0.184433, loss: 2.1192
2022-07-25 16:02:15 - train: epoch 0023, iter [00600, 05004], lr: 0.184398, loss: 1.9070
2022-07-25 16:04:10 - train: epoch 0023, iter [00700, 05004], lr: 0.184362, loss: 1.8463
2022-07-25 16:06:04 - train: epoch 0023, iter [00800, 05004], lr: 0.184327, loss: 2.2506
2022-07-25 16:07:56 - train: epoch 0023, iter [00900, 05004], lr: 0.184291, loss: 2.0965
2022-07-25 16:09:48 - train: epoch 0023, iter [01000, 05004], lr: 0.184255, loss: 1.9113
2022-07-25 16:11:40 - train: epoch 0023, iter [01100, 05004], lr: 0.184220, loss: 1.9661
2022-07-25 16:13:37 - train: epoch 0023, iter [01200, 05004], lr: 0.184184, loss: 1.9350
2022-07-25 16:15:28 - train: epoch 0023, iter [01300, 05004], lr: 0.184148, loss: 2.1052
2022-07-25 16:17:26 - train: epoch 0023, iter [01400, 05004], lr: 0.184113, loss: 2.1597
2022-07-25 16:19:19 - train: epoch 0023, iter [01500, 05004], lr: 0.184077, loss: 1.9040
2022-07-25 16:21:19 - train: epoch 0023, iter [01600, 05004], lr: 0.184041, loss: 2.1428
2022-07-25 16:23:09 - train: epoch 0023, iter [01700, 05004], lr: 0.184005, loss: 2.1036
2022-07-25 16:25:00 - train: epoch 0023, iter [01800, 05004], lr: 0.183969, loss: 2.0001
2022-07-25 16:26:51 - train: epoch 0023, iter [01900, 05004], lr: 0.183934, loss: 2.2510
2022-07-25 16:28:53 - train: epoch 0023, iter [02000, 05004], lr: 0.183898, loss: 1.8575
2022-07-25 16:30:47 - train: epoch 0023, iter [02100, 05004], lr: 0.183862, loss: 2.2095
2022-07-25 16:32:43 - train: epoch 0023, iter [02200, 05004], lr: 0.183826, loss: 1.8673
2022-07-25 16:34:43 - train: epoch 0023, iter [02300, 05004], lr: 0.183790, loss: 1.9245
2022-07-25 16:36:39 - train: epoch 0023, iter [02400, 05004], lr: 0.183753, loss: 2.0481
2022-07-25 16:38:35 - train: epoch 0023, iter [02500, 05004], lr: 0.183717, loss: 2.0830
2022-07-25 16:40:29 - train: epoch 0023, iter [02600, 05004], lr: 0.183681, loss: 2.1300
2022-07-25 16:42:23 - train: epoch 0023, iter [02700, 05004], lr: 0.183645, loss: 2.0274
2022-07-25 16:44:16 - train: epoch 0023, iter [02800, 05004], lr: 0.183609, loss: 2.0735
2022-07-25 16:46:08 - train: epoch 0023, iter [02900, 05004], lr: 0.183572, loss: 2.0432
2022-07-25 16:48:06 - train: epoch 0023, iter [03000, 05004], lr: 0.183536, loss: 2.2114
2022-07-25 16:50:01 - train: epoch 0023, iter [03100, 05004], lr: 0.183500, loss: 2.1546
2022-07-25 16:51:54 - train: epoch 0023, iter [03200, 05004], lr: 0.183463, loss: 2.0995
2022-07-25 16:53:44 - train: epoch 0023, iter [03300, 05004], lr: 0.183427, loss: 2.1378
2022-07-25 16:55:41 - train: epoch 0023, iter [03400, 05004], lr: 0.183391, loss: 2.1104
2022-07-25 16:57:31 - train: epoch 0023, iter [03500, 05004], lr: 0.183354, loss: 1.9922
2022-07-25 16:59:22 - train: epoch 0023, iter [03600, 05004], lr: 0.183318, loss: 1.8888
2022-07-25 17:01:23 - train: epoch 0023, iter [03700, 05004], lr: 0.183281, loss: 2.0579
2022-07-25 17:03:22 - train: epoch 0023, iter [03800, 05004], lr: 0.183244, loss: 2.2085
2022-07-25 17:05:15 - train: epoch 0023, iter [03900, 05004], lr: 0.183208, loss: 2.1198
2022-07-25 17:07:13 - train: epoch 0023, iter [04000, 05004], lr: 0.183171, loss: 1.9279
2022-07-25 17:09:08 - train: epoch 0023, iter [04100, 05004], lr: 0.183134, loss: 2.1411
2022-07-25 17:11:03 - train: epoch 0023, iter [04200, 05004], lr: 0.183098, loss: 1.9524
2022-07-25 17:13:00 - train: epoch 0023, iter [04300, 05004], lr: 0.183061, loss: 2.1513
2022-07-25 17:14:58 - train: epoch 0023, iter [04400, 05004], lr: 0.183024, loss: 1.9323
2022-07-25 17:16:53 - train: epoch 0023, iter [04500, 05004], lr: 0.182987, loss: 1.9207
2022-07-25 17:18:45 - train: epoch 0023, iter [04600, 05004], lr: 0.182950, loss: 2.2225
2022-07-25 17:20:41 - train: epoch 0023, iter [04700, 05004], lr: 0.182913, loss: 1.8688
2022-07-25 17:22:35 - train: epoch 0023, iter [04800, 05004], lr: 0.182876, loss: 2.0633
2022-07-25 17:24:33 - train: epoch 0023, iter [04900, 05004], lr: 0.182839, loss: 1.9717
2022-07-25 17:26:27 - train: epoch 0023, iter [05000, 05004], lr: 0.182802, loss: 2.1575
2022-07-25 17:26:31 - train: epoch 023, train_loss: 2.0802
2022-07-25 17:30:43 - eval: epoch: 023, acc1: 55.418%, acc5: 80.626%, test_loss: 1.8861, per_image_load_time: 8.813ms, per_image_inference_time: 0.782ms
2022-07-25 17:30:43 - until epoch: 023, best_acc1: 56.636%
2022-07-25 17:30:43 - epoch 024 lr: 0.182801
2022-07-25 17:33:00 - train: epoch 0024, iter [00100, 05004], lr: 0.182764, loss: 2.0488
2022-07-25 17:35:06 - train: epoch 0024, iter [00200, 05004], lr: 0.182727, loss: 2.0237
2022-07-25 17:37:10 - train: epoch 0024, iter [00300, 05004], lr: 0.182690, loss: 2.0064
2022-07-25 17:39:16 - train: epoch 0024, iter [00400, 05004], lr: 0.182652, loss: 2.2043
2022-07-25 17:41:11 - train: epoch 0024, iter [00500, 05004], lr: 0.182615, loss: 2.1002
2022-07-25 17:43:15 - train: epoch 0024, iter [00600, 05004], lr: 0.182578, loss: 2.2192
2022-07-25 17:45:17 - train: epoch 0024, iter [00700, 05004], lr: 0.182541, loss: 2.0208
2022-07-25 17:47:15 - train: epoch 0024, iter [00800, 05004], lr: 0.182503, loss: 1.9907
2022-07-25 17:49:11 - train: epoch 0024, iter [00900, 05004], lr: 0.182466, loss: 2.1051
2022-07-25 17:51:09 - train: epoch 0024, iter [01000, 05004], lr: 0.182429, loss: 1.9513
2022-07-25 17:53:04 - train: epoch 0024, iter [01100, 05004], lr: 0.182391, loss: 1.7815
2022-07-25 17:54:58 - train: epoch 0024, iter [01200, 05004], lr: 0.182354, loss: 2.0025
2022-07-25 17:56:52 - train: epoch 0024, iter [01300, 05004], lr: 0.182316, loss: 2.3905
2022-07-25 17:58:51 - train: epoch 0024, iter [01400, 05004], lr: 0.182279, loss: 1.9836
2022-07-25 18:00:44 - train: epoch 0024, iter [01500, 05004], lr: 0.182241, loss: 2.2222
2022-07-25 18:02:44 - train: epoch 0024, iter [01600, 05004], lr: 0.182203, loss: 1.9795
2022-07-25 18:04:39 - train: epoch 0024, iter [01700, 05004], lr: 0.182166, loss: 2.1082
2022-07-25 18:06:39 - train: epoch 0024, iter [01800, 05004], lr: 0.182128, loss: 2.0557
2022-07-25 18:08:32 - train: epoch 0024, iter [01900, 05004], lr: 0.182090, loss: 1.7815
2022-07-25 18:10:34 - train: epoch 0024, iter [02000, 05004], lr: 0.182053, loss: 2.2113
2022-07-25 18:12:30 - train: epoch 0024, iter [02100, 05004], lr: 0.182015, loss: 1.9765
2022-07-25 18:14:28 - train: epoch 0024, iter [02200, 05004], lr: 0.181977, loss: 1.7384
2022-07-25 18:16:26 - train: epoch 0024, iter [02300, 05004], lr: 0.181939, loss: 2.2467
2022-07-25 18:18:24 - train: epoch 0024, iter [02400, 05004], lr: 0.181901, loss: 2.1703
2022-07-25 18:20:16 - train: epoch 0024, iter [02500, 05004], lr: 0.181863, loss: 2.0839
2022-07-25 18:22:20 - train: epoch 0024, iter [02600, 05004], lr: 0.181825, loss: 2.2249
2022-07-25 18:24:17 - train: epoch 0024, iter [02700, 05004], lr: 0.181787, loss: 2.1945
2022-07-25 18:26:13 - train: epoch 0024, iter [02800, 05004], lr: 0.181749, loss: 2.2084
2022-07-25 18:28:08 - train: epoch 0024, iter [02900, 05004], lr: 0.181711, loss: 2.0725
2022-07-25 18:30:03 - train: epoch 0024, iter [03000, 05004], lr: 0.181673, loss: 1.8751
2022-07-25 18:32:05 - train: epoch 0024, iter [03100, 05004], lr: 0.181635, loss: 2.1175
2022-07-25 18:33:55 - train: epoch 0024, iter [03200, 05004], lr: 0.181597, loss: 2.2872
2022-07-25 18:35:47 - train: epoch 0024, iter [03300, 05004], lr: 0.181558, loss: 1.8521
2022-07-25 18:37:40 - train: epoch 0024, iter [03400, 05004], lr: 0.181520, loss: 2.1112
2022-07-25 18:39:34 - train: epoch 0024, iter [03500, 05004], lr: 0.181482, loss: 2.2004
2022-07-25 18:41:22 - train: epoch 0024, iter [03600, 05004], lr: 0.181444, loss: 2.2266
2022-07-25 18:43:19 - train: epoch 0024, iter [03700, 05004], lr: 0.181405, loss: 2.0299
2022-07-25 18:45:19 - train: epoch 0024, iter [03800, 05004], lr: 0.181367, loss: 1.9929
2022-07-25 18:47:10 - train: epoch 0024, iter [03900, 05004], lr: 0.181328, loss: 1.9485
2022-07-25 18:49:05 - train: epoch 0024, iter [04000, 05004], lr: 0.181290, loss: 1.9474
2022-07-25 18:51:01 - train: epoch 0024, iter [04100, 05004], lr: 0.181251, loss: 2.0761
2022-07-25 18:52:56 - train: epoch 0024, iter [04200, 05004], lr: 0.181213, loss: 1.8953
2022-07-25 18:54:50 - train: epoch 0024, iter [04300, 05004], lr: 0.181174, loss: 2.0545
2022-07-25 18:56:45 - train: epoch 0024, iter [04400, 05004], lr: 0.181136, loss: 2.0704
2022-07-25 18:58:43 - train: epoch 0024, iter [04500, 05004], lr: 0.181097, loss: 1.8221
2022-07-25 19:00:38 - train: epoch 0024, iter [04600, 05004], lr: 0.181058, loss: 2.1696
2022-07-25 19:02:32 - train: epoch 0024, iter [04700, 05004], lr: 0.181020, loss: 2.1334
2022-07-25 19:04:28 - train: epoch 0024, iter [04800, 05004], lr: 0.180981, loss: 1.8526
2022-07-25 19:06:31 - train: epoch 0024, iter [04900, 05004], lr: 0.180942, loss: 2.1758
2022-07-25 19:08:24 - train: epoch 0024, iter [05000, 05004], lr: 0.180903, loss: 2.2995
2022-07-25 19:08:28 - train: epoch 024, train_loss: 2.0685
2022-07-25 19:12:33 - eval: epoch: 024, acc1: 56.996%, acc5: 81.326%, test_loss: 1.8287, per_image_load_time: 8.012ms, per_image_inference_time: 0.780ms
2022-07-25 19:12:34 - until epoch: 024, best_acc1: 56.996%
2022-07-25 19:12:34 - epoch 025 lr: 0.180901
2022-07-25 19:15:08 - train: epoch 0025, iter [00100, 05004], lr: 0.180863, loss: 1.8738
2022-07-25 19:16:55 - train: epoch 0025, iter [00200, 05004], lr: 0.180824, loss: 1.8665
2022-07-25 19:18:52 - train: epoch 0025, iter [00300, 05004], lr: 0.180785, loss: 2.1383
2022-07-25 19:20:54 - train: epoch 0025, iter [00400, 05004], lr: 0.180746, loss: 1.9123
2022-07-25 19:22:58 - train: epoch 0025, iter [00500, 05004], lr: 0.180707, loss: 1.8729
2022-07-25 19:24:56 - train: epoch 0025, iter [00600, 05004], lr: 0.180668, loss: 2.0686
2022-07-25 19:26:55 - train: epoch 0025, iter [00700, 05004], lr: 0.180629, loss: 2.1220
2022-07-25 19:28:58 - train: epoch 0025, iter [00800, 05004], lr: 0.180590, loss: 1.9954
2022-07-25 19:31:00 - train: epoch 0025, iter [00900, 05004], lr: 0.180551, loss: 1.8988
2022-07-25 19:32:50 - train: epoch 0025, iter [01000, 05004], lr: 0.180511, loss: 2.1446
2022-07-25 19:34:47 - train: epoch 0025, iter [01100, 05004], lr: 0.180472, loss: 2.0966
2022-07-25 19:36:49 - train: epoch 0025, iter [01200, 05004], lr: 0.180433, loss: 2.2189
2022-07-25 19:38:42 - train: epoch 0025, iter [01300, 05004], lr: 0.180394, loss: 2.0500
2022-07-25 19:40:32 - train: epoch 0025, iter [01400, 05004], lr: 0.180354, loss: 1.9132
2022-07-25 19:42:26 - train: epoch 0025, iter [01500, 05004], lr: 0.180315, loss: 2.0576
2022-07-25 19:44:19 - train: epoch 0025, iter [01600, 05004], lr: 0.180276, loss: 1.9293
2022-07-25 19:46:16 - train: epoch 0025, iter [01700, 05004], lr: 0.180236, loss: 1.9127
2022-07-25 19:48:10 - train: epoch 0025, iter [01800, 05004], lr: 0.180197, loss: 1.8076
2022-07-25 19:50:05 - train: epoch 0025, iter [01900, 05004], lr: 0.180157, loss: 2.1388
2022-07-25 19:51:59 - train: epoch 0025, iter [02000, 05004], lr: 0.180118, loss: 2.0016
2022-07-25 19:53:55 - train: epoch 0025, iter [02100, 05004], lr: 0.180078, loss: 1.8636
2022-07-25 19:55:56 - train: epoch 0025, iter [02200, 05004], lr: 0.180039, loss: 1.9656
2022-07-25 19:57:50 - train: epoch 0025, iter [02300, 05004], lr: 0.179999, loss: 2.0584
2022-07-25 19:59:43 - train: epoch 0025, iter [02400, 05004], lr: 0.179959, loss: 2.0701
2022-07-25 20:01:39 - train: epoch 0025, iter [02500, 05004], lr: 0.179920, loss: 2.0670
2022-07-25 20:03:33 - train: epoch 0025, iter [02600, 05004], lr: 0.179880, loss: 2.1032
2022-07-25 20:05:29 - train: epoch 0025, iter [02700, 05004], lr: 0.179840, loss: 2.1042
2022-07-25 20:07:23 - train: epoch 0025, iter [02800, 05004], lr: 0.179800, loss: 2.0570
2022-07-25 20:09:18 - train: epoch 0025, iter [02900, 05004], lr: 0.179760, loss: 2.1326
2022-07-25 20:11:15 - train: epoch 0025, iter [03000, 05004], lr: 0.179721, loss: 2.1253
2022-07-25 20:13:12 - train: epoch 0025, iter [03100, 05004], lr: 0.179681, loss: 2.2182
2022-07-25 20:15:03 - train: epoch 0025, iter [03200, 05004], lr: 0.179641, loss: 1.9080
2022-07-25 20:17:02 - train: epoch 0025, iter [03300, 05004], lr: 0.179601, loss: 1.7993
2022-07-25 20:18:54 - train: epoch 0025, iter [03400, 05004], lr: 0.179561, loss: 2.0653
2022-07-25 20:20:47 - train: epoch 0025, iter [03500, 05004], lr: 0.179521, loss: 1.9125
2022-07-25 20:22:41 - train: epoch 0025, iter [03600, 05004], lr: 0.179481, loss: 1.9782
2022-07-25 20:24:39 - train: epoch 0025, iter [03700, 05004], lr: 0.179440, loss: 1.8027
2022-07-25 20:26:32 - train: epoch 0025, iter [03800, 05004], lr: 0.179400, loss: 2.0935
2022-07-25 20:28:28 - train: epoch 0025, iter [03900, 05004], lr: 0.179360, loss: 2.1519
2022-07-25 20:30:22 - train: epoch 0025, iter [04000, 05004], lr: 0.179320, loss: 2.2289
2022-07-25 20:32:19 - train: epoch 0025, iter [04100, 05004], lr: 0.179280, loss: 2.3454
2022-07-25 20:34:23 - train: epoch 0025, iter [04200, 05004], lr: 0.179239, loss: 2.0787
2022-07-25 20:36:22 - train: epoch 0025, iter [04300, 05004], lr: 0.179199, loss: 1.8402
2022-07-25 20:38:18 - train: epoch 0025, iter [04400, 05004], lr: 0.179159, loss: 1.9283
2022-07-25 20:40:11 - train: epoch 0025, iter [04500, 05004], lr: 0.179118, loss: 2.1019
2022-07-25 20:42:10 - train: epoch 0025, iter [04600, 05004], lr: 0.179078, loss: 1.9114
2022-07-25 20:44:07 - train: epoch 0025, iter [04700, 05004], lr: 0.179037, loss: 1.9637
2022-07-25 20:46:05 - train: epoch 0025, iter [04800, 05004], lr: 0.178997, loss: 1.6942
2022-07-25 20:48:03 - train: epoch 0025, iter [04900, 05004], lr: 0.178956, loss: 2.0484
2022-07-25 20:49:57 - train: epoch 0025, iter [05000, 05004], lr: 0.178916, loss: 2.3757
2022-07-25 20:50:00 - train: epoch 025, train_loss: 2.0575
2022-07-25 20:54:09 - eval: epoch: 025, acc1: 57.228%, acc5: 81.552%, test_loss: 1.8115, per_image_load_time: 7.638ms, per_image_inference_time: 0.862ms
2022-07-25 20:54:10 - until epoch: 025, best_acc1: 57.228%
2022-07-25 20:54:10 - epoch 026 lr: 0.178914
2022-07-25 20:56:43 - train: epoch 0026, iter [00100, 05004], lr: 0.178873, loss: 2.0239
2022-07-25 20:58:41 - train: epoch 0026, iter [00200, 05004], lr: 0.178833, loss: 1.7833
2022-07-25 21:00:34 - train: epoch 0026, iter [00300, 05004], lr: 0.178792, loss: 2.1827
2022-07-25 21:02:32 - train: epoch 0026, iter [00400, 05004], lr: 0.178751, loss: 2.0244
2022-07-25 21:04:29 - train: epoch 0026, iter [00500, 05004], lr: 0.178711, loss: 1.9476
2022-07-25 21:06:24 - train: epoch 0026, iter [00600, 05004], lr: 0.178670, loss: 2.4129
2022-07-25 21:08:17 - train: epoch 0026, iter [00700, 05004], lr: 0.178629, loss: 2.0440
2022-07-25 21:10:14 - train: epoch 0026, iter [00800, 05004], lr: 0.178588, loss: 1.8175
2022-07-25 21:12:14 - train: epoch 0026, iter [00900, 05004], lr: 0.178547, loss: 1.9408
2022-07-25 21:14:09 - train: epoch 0026, iter [01000, 05004], lr: 0.178506, loss: 2.1178
2022-07-25 21:16:05 - train: epoch 0026, iter [01100, 05004], lr: 0.178465, loss: 2.1112
2022-07-25 21:18:07 - train: epoch 0026, iter [01200, 05004], lr: 0.178424, loss: 2.1342
2022-07-25 21:20:02 - train: epoch 0026, iter [01300, 05004], lr: 0.178383, loss: 2.0013
2022-07-25 21:22:01 - train: epoch 0026, iter [01400, 05004], lr: 0.178342, loss: 2.2939
2022-07-25 21:24:00 - train: epoch 0026, iter [01500, 05004], lr: 0.178301, loss: 1.9697
2022-07-25 21:25:57 - train: epoch 0026, iter [01600, 05004], lr: 0.178260, loss: 2.0296
2022-07-25 21:28:05 - train: epoch 0026, iter [01700, 05004], lr: 0.178219, loss: 2.0742
2022-07-25 21:30:10 - train: epoch 0026, iter [01800, 05004], lr: 0.178178, loss: 2.0965
2022-07-25 21:32:11 - train: epoch 0026, iter [01900, 05004], lr: 0.178137, loss: 2.1257
2022-07-25 21:34:18 - train: epoch 0026, iter [02000, 05004], lr: 0.178095, loss: 2.0573
2022-07-25 21:36:22 - train: epoch 0026, iter [02100, 05004], lr: 0.178054, loss: 2.0112
2022-07-25 21:38:27 - train: epoch 0026, iter [02200, 05004], lr: 0.178013, loss: 1.9582
2022-07-25 21:40:32 - train: epoch 0026, iter [02300, 05004], lr: 0.177971, loss: 1.8555
2022-07-25 21:42:36 - train: epoch 0026, iter [02400, 05004], lr: 0.177930, loss: 2.3883
2022-07-25 21:44:33 - train: epoch 0026, iter [02500, 05004], lr: 0.177889, loss: 2.2077
2022-07-25 21:46:40 - train: epoch 0026, iter [02600, 05004], lr: 0.177847, loss: 2.1913
2022-07-25 21:48:43 - train: epoch 0026, iter [02700, 05004], lr: 0.177806, loss: 2.1295
2022-07-25 21:50:42 - train: epoch 0026, iter [02800, 05004], lr: 0.177764, loss: 1.9933
2022-07-25 21:52:43 - train: epoch 0026, iter [02900, 05004], lr: 0.177722, loss: 1.9490
2022-07-25 21:54:51 - train: epoch 0026, iter [03000, 05004], lr: 0.177681, loss: 2.1845
2022-07-25 21:56:55 - train: epoch 0026, iter [03100, 05004], lr: 0.177639, loss: 1.8800
2022-07-25 21:58:52 - train: epoch 0026, iter [03200, 05004], lr: 0.177598, loss: 2.0382
2022-07-25 22:00:54 - train: epoch 0026, iter [03300, 05004], lr: 0.177556, loss: 1.8937
2022-07-25 22:02:55 - train: epoch 0026, iter [03400, 05004], lr: 0.177514, loss: 2.2015
2022-07-25 22:04:58 - train: epoch 0026, iter [03500, 05004], lr: 0.177472, loss: 2.2900
2022-07-25 22:06:56 - train: epoch 0026, iter [03600, 05004], lr: 0.177431, loss: 1.8648
2022-07-25 22:08:56 - train: epoch 0026, iter [03700, 05004], lr: 0.177389, loss: 1.9206
2022-07-25 22:10:53 - train: epoch 0026, iter [03800, 05004], lr: 0.177347, loss: 2.2913
2022-07-25 22:12:54 - train: epoch 0026, iter [03900, 05004], lr: 0.177305, loss: 2.1703
2022-07-25 22:14:52 - train: epoch 0026, iter [04000, 05004], lr: 0.177263, loss: 2.1558
2022-07-25 22:16:54 - train: epoch 0026, iter [04100, 05004], lr: 0.177221, loss: 1.8586
2022-07-25 22:18:55 - train: epoch 0026, iter [04200, 05004], lr: 0.177179, loss: 2.1336
2022-07-25 22:20:51 - train: epoch 0026, iter [04300, 05004], lr: 0.177137, loss: 2.1658
2022-07-25 22:22:50 - train: epoch 0026, iter [04400, 05004], lr: 0.177095, loss: 2.2098
2022-07-25 22:24:54 - train: epoch 0026, iter [04500, 05004], lr: 0.177053, loss: 2.2298
2022-07-25 22:26:57 - train: epoch 0026, iter [04600, 05004], lr: 0.177011, loss: 2.0796
2022-07-25 22:29:04 - train: epoch 0026, iter [04700, 05004], lr: 0.176969, loss: 1.9003
2022-07-25 22:31:01 - train: epoch 0026, iter [04800, 05004], lr: 0.176926, loss: 1.9718
2022-07-25 22:33:02 - train: epoch 0026, iter [04900, 05004], lr: 0.176884, loss: 2.3258
2022-07-25 22:34:59 - train: epoch 0026, iter [05000, 05004], lr: 0.176842, loss: 2.2509
2022-07-25 22:35:02 - train: epoch 026, train_loss: 2.0484
2022-07-25 22:38:46 - eval: epoch: 026, acc1: 56.918%, acc5: 81.626%, test_loss: 1.8048, per_image_load_time: 7.763ms, per_image_inference_time: 0.793ms
2022-07-25 22:38:47 - until epoch: 026, best_acc1: 57.228%
2022-07-25 22:38:47 - epoch 027 lr: 0.176840
2022-07-25 22:41:29 - train: epoch 0027, iter [00100, 05004], lr: 0.176798, loss: 2.2458
2022-07-25 22:43:29 - train: epoch 0027, iter [00200, 05004], lr: 0.176755, loss: 2.2213
2022-07-25 22:45:34 - train: epoch 0027, iter [00300, 05004], lr: 0.176713, loss: 2.1901
2022-07-25 22:47:40 - train: epoch 0027, iter [00400, 05004], lr: 0.176671, loss: 2.1820
2022-07-25 22:49:41 - train: epoch 0027, iter [00500, 05004], lr: 0.176628, loss: 2.0909
2022-07-25 22:51:45 - train: epoch 0027, iter [00600, 05004], lr: 0.176586, loss: 2.1714
2022-07-25 22:53:46 - train: epoch 0027, iter [00700, 05004], lr: 0.176543, loss: 2.1385
2022-07-25 22:55:55 - train: epoch 0027, iter [00800, 05004], lr: 0.176501, loss: 1.9524
2022-07-25 22:57:57 - train: epoch 0027, iter [00900, 05004], lr: 0.176458, loss: 2.0392
2022-07-25 23:00:00 - train: epoch 0027, iter [01000, 05004], lr: 0.176416, loss: 2.1886
2022-07-25 23:01:59 - train: epoch 0027, iter [01100, 05004], lr: 0.176373, loss: 2.0114
2022-07-25 23:04:00 - train: epoch 0027, iter [01200, 05004], lr: 0.176330, loss: 2.0664
2022-07-25 23:06:02 - train: epoch 0027, iter [01300, 05004], lr: 0.176287, loss: 2.3317
2022-07-25 23:08:07 - train: epoch 0027, iter [01400, 05004], lr: 0.176245, loss: 2.0724
2022-07-25 23:10:10 - train: epoch 0027, iter [01500, 05004], lr: 0.176202, loss: 2.1338
2022-07-25 23:12:06 - train: epoch 0027, iter [01600, 05004], lr: 0.176159, loss: 2.1404
2022-07-25 23:14:07 - train: epoch 0027, iter [01700, 05004], lr: 0.176116, loss: 1.9477
2022-07-25 23:16:10 - train: epoch 0027, iter [01800, 05004], lr: 0.176073, loss: 1.9923
2022-07-25 23:18:10 - train: epoch 0027, iter [01900, 05004], lr: 0.176031, loss: 2.1768
2022-07-25 23:20:09 - train: epoch 0027, iter [02000, 05004], lr: 0.175988, loss: 1.9180
2022-07-25 23:22:13 - train: epoch 0027, iter [02100, 05004], lr: 0.175945, loss: 2.0651
2022-07-25 23:24:16 - train: epoch 0027, iter [02200, 05004], lr: 0.175902, loss: 2.2851
2022-07-25 23:26:16 - train: epoch 0027, iter [02300, 05004], lr: 0.175859, loss: 2.2431
2022-07-25 23:28:23 - train: epoch 0027, iter [02400, 05004], lr: 0.175815, loss: 2.1955
2022-07-25 23:30:27 - train: epoch 0027, iter [02500, 05004], lr: 0.175772, loss: 2.0612
2022-07-25 23:32:28 - train: epoch 0027, iter [02600, 05004], lr: 0.175729, loss: 2.2308
2022-07-25 23:34:31 - train: epoch 0027, iter [02700, 05004], lr: 0.175686, loss: 1.9170
2022-07-25 23:36:36 - train: epoch 0027, iter [02800, 05004], lr: 0.175643, loss: 2.1860
2022-07-25 23:38:34 - train: epoch 0027, iter [02900, 05004], lr: 0.175600, loss: 2.1548
2022-07-25 23:40:33 - train: epoch 0027, iter [03000, 05004], lr: 0.175556, loss: 2.0196
2022-07-25 23:42:28 - train: epoch 0027, iter [03100, 05004], lr: 0.175513, loss: 1.8526
2022-07-25 23:44:33 - train: epoch 0027, iter [03200, 05004], lr: 0.175470, loss: 2.0292
2022-07-25 23:46:37 - train: epoch 0027, iter [03300, 05004], lr: 0.175426, loss: 2.0837
2022-07-25 23:48:47 - train: epoch 0027, iter [03400, 05004], lr: 0.175383, loss: 1.8161
2022-07-25 23:50:51 - train: epoch 0027, iter [03500, 05004], lr: 0.175339, loss: 2.3500
2022-07-25 23:53:01 - train: epoch 0027, iter [03600, 05004], lr: 0.175296, loss: 2.1809
2022-07-25 23:55:06 - train: epoch 0027, iter [03700, 05004], lr: 0.175252, loss: 2.0593
2022-07-25 23:57:13 - train: epoch 0027, iter [03800, 05004], lr: 0.175209, loss: 1.8688
2022-07-25 23:59:21 - train: epoch 0027, iter [03900, 05004], lr: 0.175165, loss: 2.1175
2022-07-26 00:01:29 - train: epoch 0027, iter [04000, 05004], lr: 0.175122, loss: 2.1800
2022-07-26 00:03:35 - train: epoch 0027, iter [04100, 05004], lr: 0.175078, loss: 2.1267
2022-07-26 00:05:42 - train: epoch 0027, iter [04200, 05004], lr: 0.175034, loss: 1.9802
2022-07-26 00:07:56 - train: epoch 0027, iter [04300, 05004], lr: 0.174991, loss: 2.1460
2022-07-26 00:10:02 - train: epoch 0027, iter [04400, 05004], lr: 0.174947, loss: 2.0287
2022-07-26 00:12:13 - train: epoch 0027, iter [04500, 05004], lr: 0.174903, loss: 1.8096
2022-07-26 00:14:17 - train: epoch 0027, iter [04600, 05004], lr: 0.174859, loss: 1.9655
2022-07-26 00:16:25 - train: epoch 0027, iter [04700, 05004], lr: 0.174816, loss: 2.1502
2022-07-26 00:18:35 - train: epoch 0027, iter [04800, 05004], lr: 0.174772, loss: 2.1590
2022-07-26 00:20:38 - train: epoch 0027, iter [04900, 05004], lr: 0.174728, loss: 1.8284
2022-07-26 00:22:44 - train: epoch 0027, iter [05000, 05004], lr: 0.174684, loss: 1.7323
2022-07-26 00:22:48 - train: epoch 027, train_loss: 2.0383
2022-07-26 00:27:01 - eval: epoch: 027, acc1: 57.748%, acc5: 81.800%, test_loss: 1.7820, per_image_load_time: 5.556ms, per_image_inference_time: 0.846ms
2022-07-26 00:27:02 - until epoch: 027, best_acc1: 57.748%
2022-07-26 00:27:02 - epoch 028 lr: 0.174682
2022-07-26 00:29:26 - train: epoch 0028, iter [00100, 05004], lr: 0.174638, loss: 1.8690
2022-07-26 00:31:12 - train: epoch 0028, iter [00200, 05004], lr: 0.174594, loss: 1.8452
2022-07-26 00:33:13 - train: epoch 0028, iter [00300, 05004], lr: 0.174550, loss: 1.8308
2022-07-26 00:35:09 - train: epoch 0028, iter [00400, 05004], lr: 0.174506, loss: 1.9596
2022-07-26 00:37:03 - train: epoch 0028, iter [00500, 05004], lr: 0.174462, loss: 1.9847
2022-07-26 00:39:00 - train: epoch 0028, iter [00600, 05004], lr: 0.174418, loss: 2.3246
2022-07-26 00:40:52 - train: epoch 0028, iter [00700, 05004], lr: 0.174374, loss: 2.2018
2022-07-26 00:42:52 - train: epoch 0028, iter [00800, 05004], lr: 0.174330, loss: 1.8436
2022-07-26 00:44:55 - train: epoch 0028, iter [00900, 05004], lr: 0.174285, loss: 2.0665
2022-07-26 00:46:49 - train: epoch 0028, iter [01000, 05004], lr: 0.174241, loss: 2.0734
2022-07-26 00:48:49 - train: epoch 0028, iter [01100, 05004], lr: 0.174197, loss: 2.0208
2022-07-26 00:50:46 - train: epoch 0028, iter [01200, 05004], lr: 0.174152, loss: 2.1082
2022-07-26 00:52:43 - train: epoch 0028, iter [01300, 05004], lr: 0.174108, loss: 1.7577
2022-07-26 00:54:38 - train: epoch 0028, iter [01400, 05004], lr: 0.174064, loss: 2.3295
2022-07-26 00:56:37 - train: epoch 0028, iter [01500, 05004], lr: 0.174019, loss: 1.9647
2022-07-26 00:58:31 - train: epoch 0028, iter [01600, 05004], lr: 0.173975, loss: 1.9536
2022-07-26 01:00:22 - train: epoch 0028, iter [01700, 05004], lr: 0.173930, loss: 1.9351
2022-07-26 01:02:33 - train: epoch 0028, iter [01800, 05004], lr: 0.173886, loss: 1.9340
2022-07-26 01:04:30 - train: epoch 0028, iter [01900, 05004], lr: 0.173841, loss: 2.1133
2022-07-26 01:06:32 - train: epoch 0028, iter [02000, 05004], lr: 0.173797, loss: 2.2131
2022-07-26 01:08:32 - train: epoch 0028, iter [02100, 05004], lr: 0.173752, loss: 2.2274
2022-07-26 01:10:33 - train: epoch 0028, iter [02200, 05004], lr: 0.173707, loss: 2.2637
2022-07-26 01:12:31 - train: epoch 0028, iter [02300, 05004], lr: 0.173663, loss: 2.2078
2022-07-26 01:14:35 - train: epoch 0028, iter [02400, 05004], lr: 0.173618, loss: 2.3006
2022-07-26 01:16:35 - train: epoch 0028, iter [02500, 05004], lr: 0.173573, loss: 1.9537
2022-07-26 01:18:38 - train: epoch 0028, iter [02600, 05004], lr: 0.173529, loss: 1.8544
2022-07-26 01:20:40 - train: epoch 0028, iter [02700, 05004], lr: 0.173484, loss: 2.1917
2022-07-26 01:22:38 - train: epoch 0028, iter [02800, 05004], lr: 0.173439, loss: 2.0902
2022-07-26 01:24:38 - train: epoch 0028, iter [02900, 05004], lr: 0.173394, loss: 1.9880
2022-07-26 01:26:34 - train: epoch 0028, iter [03000, 05004], lr: 0.173349, loss: 2.0238
2022-07-26 01:28:37 - train: epoch 0028, iter [03100, 05004], lr: 0.173304, loss: 2.1395
2022-07-26 01:30:37 - train: epoch 0028, iter [03200, 05004], lr: 0.173259, loss: 1.9495
2022-07-26 01:32:40 - train: epoch 0028, iter [03300, 05004], lr: 0.173214, loss: 1.8691
2022-07-26 01:34:38 - train: epoch 0028, iter [03400, 05004], lr: 0.173169, loss: 2.1248
2022-07-26 01:36:34 - train: epoch 0028, iter [03500, 05004], lr: 0.173124, loss: 2.1007
2022-07-26 01:38:40 - train: epoch 0028, iter [03600, 05004], lr: 0.173079, loss: 2.1076
2022-07-26 01:40:38 - train: epoch 0028, iter [03700, 05004], lr: 0.173034, loss: 1.9544
2022-07-26 01:42:36 - train: epoch 0028, iter [03800, 05004], lr: 0.172989, loss: 1.8618
2022-07-26 01:44:40 - train: epoch 0028, iter [03900, 05004], lr: 0.172944, loss: 2.1587
2022-07-26 01:46:39 - train: epoch 0028, iter [04000, 05004], lr: 0.172898, loss: 1.9665
2022-07-26 01:48:33 - train: epoch 0028, iter [04100, 05004], lr: 0.172853, loss: 2.0372
2022-07-26 01:50:36 - train: epoch 0028, iter [04200, 05004], lr: 0.172808, loss: 2.1212
2022-07-26 01:52:35 - train: epoch 0028, iter [04300, 05004], lr: 0.172762, loss: 1.9078
2022-07-26 01:54:32 - train: epoch 0028, iter [04400, 05004], lr: 0.172717, loss: 2.1176
2022-07-26 01:56:32 - train: epoch 0028, iter [04500, 05004], lr: 0.172672, loss: 2.1397
2022-07-26 01:58:34 - train: epoch 0028, iter [04600, 05004], lr: 0.172626, loss: 1.9280
2022-07-26 02:00:36 - train: epoch 0028, iter [04700, 05004], lr: 0.172581, loss: 2.1376
2022-07-26 02:02:35 - train: epoch 0028, iter [04800, 05004], lr: 0.172535, loss: 1.9927
2022-07-26 02:04:36 - train: epoch 0028, iter [04900, 05004], lr: 0.172490, loss: 2.0159
2022-07-26 02:06:34 - train: epoch 0028, iter [05000, 05004], lr: 0.172444, loss: 1.9738
2022-07-26 02:06:38 - train: epoch 028, train_loss: 2.0299
2022-07-26 02:10:43 - eval: epoch: 028, acc1: 57.726%, acc5: 81.640%, test_loss: 1.7936, per_image_load_time: 5.283ms, per_image_inference_time: 0.857ms
2022-07-26 02:10:43 - until epoch: 028, best_acc1: 57.748%
2022-07-26 02:10:43 - epoch 029 lr: 0.172442
2022-07-26 02:13:06 - train: epoch 0029, iter [00100, 05004], lr: 0.172397, loss: 1.7733
2022-07-26 02:15:05 - train: epoch 0029, iter [00200, 05004], lr: 0.172351, loss: 2.0485
2022-07-26 02:17:11 - train: epoch 0029, iter [00300, 05004], lr: 0.172306, loss: 2.0558
2022-07-26 02:19:11 - train: epoch 0029, iter [00400, 05004], lr: 0.172260, loss: 2.0516
2022-07-26 02:21:13 - train: epoch 0029, iter [00500, 05004], lr: 0.172214, loss: 2.0094
2022-07-26 02:23:15 - train: epoch 0029, iter [00600, 05004], lr: 0.172169, loss: 2.1949
2022-07-26 02:25:12 - train: epoch 0029, iter [00700, 05004], lr: 0.172123, loss: 1.9119
2022-07-26 02:27:15 - train: epoch 0029, iter [00800, 05004], lr: 0.172077, loss: 1.9370
2022-07-26 02:29:14 - train: epoch 0029, iter [00900, 05004], lr: 0.172031, loss: 1.9916
2022-07-26 02:31:18 - train: epoch 0029, iter [01000, 05004], lr: 0.171985, loss: 2.0413
2022-07-26 02:33:14 - train: epoch 0029, iter [01100, 05004], lr: 0.171939, loss: 1.7409
2022-07-26 02:35:17 - train: epoch 0029, iter [01200, 05004], lr: 0.171894, loss: 1.9089
2022-07-26 02:37:19 - train: epoch 0029, iter [01300, 05004], lr: 0.171848, loss: 2.0410
2022-07-26 02:39:18 - train: epoch 0029, iter [01400, 05004], lr: 0.171802, loss: 1.9839
2022-07-26 02:41:20 - train: epoch 0029, iter [01500, 05004], lr: 0.171756, loss: 2.2022
2022-07-26 02:43:17 - train: epoch 0029, iter [01600, 05004], lr: 0.171710, loss: 2.2268
2022-07-26 02:45:18 - train: epoch 0029, iter [01700, 05004], lr: 0.171664, loss: 2.0346
2022-07-26 02:47:16 - train: epoch 0029, iter [01800, 05004], lr: 0.171617, loss: 2.0769
2022-07-26 02:49:12 - train: epoch 0029, iter [01900, 05004], lr: 0.171571, loss: 1.9402
2022-07-26 02:51:13 - train: epoch 0029, iter [02000, 05004], lr: 0.171525, loss: 2.2249
2022-07-26 02:53:10 - train: epoch 0029, iter [02100, 05004], lr: 0.171479, loss: 1.8187
2022-07-26 02:55:16 - train: epoch 0029, iter [02200, 05004], lr: 0.171433, loss: 2.0567
2022-07-26 02:57:14 - train: epoch 0029, iter [02300, 05004], lr: 0.171386, loss: 1.9971
2022-07-26 02:59:18 - train: epoch 0029, iter [02400, 05004], lr: 0.171340, loss: 2.0837
2022-07-26 03:01:18 - train: epoch 0029, iter [02500, 05004], lr: 0.171294, loss: 1.8736
2022-07-26 03:03:17 - train: epoch 0029, iter [02600, 05004], lr: 0.171247, loss: 2.1860
2022-07-26 03:05:13 - train: epoch 0029, iter [02700, 05004], lr: 0.171201, loss: 2.1204
2022-07-26 03:07:12 - train: epoch 0029, iter [02800, 05004], lr: 0.171155, loss: 1.9864
2022-07-26 03:09:15 - train: epoch 0029, iter [02900, 05004], lr: 0.171108, loss: 2.0787
2022-07-26 03:11:20 - train: epoch 0029, iter [03000, 05004], lr: 0.171062, loss: 1.8772
2022-07-26 03:13:17 - train: epoch 0029, iter [03100, 05004], lr: 0.171015, loss: 2.0095
2022-07-26 03:15:19 - train: epoch 0029, iter [03200, 05004], lr: 0.170969, loss: 2.2138
2022-07-26 03:17:20 - train: epoch 0029, iter [03300, 05004], lr: 0.170922, loss: 1.9712
2022-07-26 03:19:22 - train: epoch 0029, iter [03400, 05004], lr: 0.170875, loss: 2.1230
2022-07-26 03:21:24 - train: epoch 0029, iter [03500, 05004], lr: 0.170829, loss: 2.1106
2022-07-26 03:23:21 - train: epoch 0029, iter [03600, 05004], lr: 0.170782, loss: 1.9783
2022-07-26 03:25:23 - train: epoch 0029, iter [03700, 05004], lr: 0.170735, loss: 2.1970
2022-07-26 03:27:21 - train: epoch 0029, iter [03800, 05004], lr: 0.170689, loss: 1.8721
2022-07-26 03:29:25 - train: epoch 0029, iter [03900, 05004], lr: 0.170642, loss: 2.0240
2022-07-26 03:31:22 - train: epoch 0029, iter [04000, 05004], lr: 0.170595, loss: 2.1030
2022-07-26 03:33:25 - train: epoch 0029, iter [04100, 05004], lr: 0.170548, loss: 2.1634
2022-07-26 03:35:29 - train: epoch 0029, iter [04200, 05004], lr: 0.170501, loss: 1.9082
2022-07-26 03:37:27 - train: epoch 0029, iter [04300, 05004], lr: 0.170455, loss: 1.8373
2022-07-26 03:39:25 - train: epoch 0029, iter [04400, 05004], lr: 0.170408, loss: 2.0338
2022-07-26 03:41:31 - train: epoch 0029, iter [04500, 05004], lr: 0.170361, loss: 2.1289
2022-07-26 03:43:24 - train: epoch 0029, iter [04600, 05004], lr: 0.170314, loss: 2.1768
2022-07-26 03:45:23 - train: epoch 0029, iter [04700, 05004], lr: 0.170267, loss: 1.9476
2022-07-26 03:47:17 - train: epoch 0029, iter [04800, 05004], lr: 0.170220, loss: 2.0107
2022-07-26 03:49:21 - train: epoch 0029, iter [04900, 05004], lr: 0.170173, loss: 2.1232
2022-07-26 03:51:16 - train: epoch 0029, iter [05000, 05004], lr: 0.170126, loss: 1.7786
2022-07-26 03:51:20 - train: epoch 029, train_loss: 2.0190
2022-07-26 03:55:22 - eval: epoch: 029, acc1: 58.630%, acc5: 82.622%, test_loss: 1.7364, per_image_load_time: 8.467ms, per_image_inference_time: 0.839ms
2022-07-26 03:55:22 - until epoch: 029, best_acc1: 58.630%
2022-07-26 03:55:22 - epoch 030 lr: 0.170123
2022-07-26 03:57:31 - train: epoch 0030, iter [00100, 05004], lr: 0.170077, loss: 2.0064
2022-07-26 03:59:49 - train: epoch 0030, iter [00200, 05004], lr: 0.170029, loss: 2.0628
2022-07-26 04:01:47 - train: epoch 0030, iter [00300, 05004], lr: 0.169982, loss: 1.8988
2022-07-26 04:03:45 - train: epoch 0030, iter [00400, 05004], lr: 0.169935, loss: 1.8220
2022-07-26 04:05:38 - train: epoch 0030, iter [00500, 05004], lr: 0.169888, loss: 1.9346
2022-07-26 04:07:36 - train: epoch 0030, iter [00600, 05004], lr: 0.169840, loss: 1.9850
2022-07-26 04:09:35 - train: epoch 0030, iter [00700, 05004], lr: 0.169793, loss: 2.0397
2022-07-26 04:11:34 - train: epoch 0030, iter [00800, 05004], lr: 0.169746, loss: 2.0263
2022-07-26 04:13:28 - train: epoch 0030, iter [00900, 05004], lr: 0.169698, loss: 2.1701
2022-07-26 04:15:23 - train: epoch 0030, iter [01000, 05004], lr: 0.169651, loss: 1.8350
2022-07-26 04:17:15 - train: epoch 0030, iter [01100, 05004], lr: 0.169604, loss: 1.8354
2022-07-26 04:19:16 - train: epoch 0030, iter [01200, 05004], lr: 0.169556, loss: 1.9794
2022-07-26 04:21:14 - train: epoch 0030, iter [01300, 05004], lr: 0.169509, loss: 1.8873
2022-07-26 04:23:05 - train: epoch 0030, iter [01400, 05004], lr: 0.169461, loss: 1.9384
2022-07-26 04:25:00 - train: epoch 0030, iter [01500, 05004], lr: 0.169414, loss: 2.0555
2022-07-26 04:26:59 - train: epoch 0030, iter [01600, 05004], lr: 0.169366, loss: 1.9799
2022-07-26 04:28:55 - train: epoch 0030, iter [01700, 05004], lr: 0.169318, loss: 2.1268
2022-07-26 04:30:51 - train: epoch 0030, iter [01800, 05004], lr: 0.169271, loss: 1.9673
2022-07-26 04:32:44 - train: epoch 0030, iter [01900, 05004], lr: 0.169223, loss: 2.0385
2022-07-26 04:34:41 - train: epoch 0030, iter [02000, 05004], lr: 0.169175, loss: 1.9360
2022-07-26 04:36:41 - train: epoch 0030, iter [02100, 05004], lr: 0.169128, loss: 2.1331
2022-07-26 04:38:36 - train: epoch 0030, iter [02200, 05004], lr: 0.169080, loss: 2.0174
2022-07-26 04:40:35 - train: epoch 0030, iter [02300, 05004], lr: 0.169032, loss: 1.9867
2022-07-26 04:42:30 - train: epoch 0030, iter [02400, 05004], lr: 0.168984, loss: 2.0525
2022-07-26 04:44:25 - train: epoch 0030, iter [02500, 05004], lr: 0.168936, loss: 2.0978
2022-07-26 04:46:22 - train: epoch 0030, iter [02600, 05004], lr: 0.168888, loss: 1.9423
2022-07-26 04:48:17 - train: epoch 0030, iter [02700, 05004], lr: 0.168840, loss: 2.0646
2022-07-26 04:50:10 - train: epoch 0030, iter [02800, 05004], lr: 0.168793, loss: 2.1259
2022-07-26 04:52:03 - train: epoch 0030, iter [02900, 05004], lr: 0.168745, loss: 1.9368
2022-07-26 04:54:02 - train: epoch 0030, iter [03000, 05004], lr: 0.168697, loss: 1.9635
2022-07-26 04:55:57 - train: epoch 0030, iter [03100, 05004], lr: 0.168649, loss: 2.1553
2022-07-26 04:57:52 - train: epoch 0030, iter [03200, 05004], lr: 0.168600, loss: 1.8145
2022-07-26 04:59:51 - train: epoch 0030, iter [03300, 05004], lr: 0.168552, loss: 2.1040
2022-07-26 05:01:44 - train: epoch 0030, iter [03400, 05004], lr: 0.168504, loss: 1.9383
2022-07-26 05:03:45 - train: epoch 0030, iter [03500, 05004], lr: 0.168456, loss: 2.1537
2022-07-26 05:05:39 - train: epoch 0030, iter [03600, 05004], lr: 0.168408, loss: 2.0660
2022-07-26 05:07:36 - train: epoch 0030, iter [03700, 05004], lr: 0.168360, loss: 2.0432
2022-07-26 05:09:30 - train: epoch 0030, iter [03800, 05004], lr: 0.168311, loss: 2.0136
2022-07-26 05:11:31 - train: epoch 0030, iter [03900, 05004], lr: 0.168263, loss: 2.0185
2022-07-26 05:13:20 - train: epoch 0030, iter [04000, 05004], lr: 0.168215, loss: 1.8477
2022-07-26 05:15:17 - train: epoch 0030, iter [04100, 05004], lr: 0.168166, loss: 2.1038
2022-07-26 05:17:15 - train: epoch 0030, iter [04200, 05004], lr: 0.168118, loss: 2.2779
2022-07-26 05:19:12 - train: epoch 0030, iter [04300, 05004], lr: 0.168070, loss: 1.9401
2022-07-26 05:21:08 - train: epoch 0030, iter [04400, 05004], lr: 0.168021, loss: 2.0220
2022-07-26 05:23:02 - train: epoch 0030, iter [04500, 05004], lr: 0.167973, loss: 2.0900
2022-07-26 05:24:55 - train: epoch 0030, iter [04600, 05004], lr: 0.167924, loss: 2.0884
2022-07-26 05:26:50 - train: epoch 0030, iter [04700, 05004], lr: 0.167876, loss: 1.9107
2022-07-26 05:28:52 - train: epoch 0030, iter [04800, 05004], lr: 0.167827, loss: 2.1685
2022-07-26 05:30:47 - train: epoch 0030, iter [04900, 05004], lr: 0.167779, loss: 1.9855
2022-07-26 05:32:40 - train: epoch 0030, iter [05000, 05004], lr: 0.167730, loss: 2.0847
2022-07-26 05:32:43 - train: epoch 030, train_loss: 2.0118
2022-07-26 05:36:41 - eval: epoch: 030, acc1: 58.206%, acc5: 82.362%, test_loss: 1.7476, per_image_load_time: 2.061ms, per_image_inference_time: 0.777ms
2022-07-26 05:36:42 - until epoch: 030, best_acc1: 58.630%
2022-07-26 05:36:42 - epoch 031 lr: 0.167728
2022-07-26 05:38:56 - train: epoch 0031, iter [00100, 05004], lr: 0.167680, loss: 2.2038
2022-07-26 05:40:51 - train: epoch 0031, iter [00200, 05004], lr: 0.167631, loss: 1.9379
2022-07-26 05:42:59 - train: epoch 0031, iter [00300, 05004], lr: 0.167582, loss: 1.8933
2022-07-26 05:44:58 - train: epoch 0031, iter [00400, 05004], lr: 0.167533, loss: 1.9103
2022-07-26 05:46:50 - train: epoch 0031, iter [00500, 05004], lr: 0.167485, loss: 1.8792
2022-07-26 05:48:52 - train: epoch 0031, iter [00600, 05004], lr: 0.167436, loss: 1.8179
2022-07-26 05:50:50 - train: epoch 0031, iter [00700, 05004], lr: 0.167387, loss: 2.0384
2022-07-26 05:52:45 - train: epoch 0031, iter [00800, 05004], lr: 0.167338, loss: 2.0619
2022-07-26 05:54:40 - train: epoch 0031, iter [00900, 05004], lr: 0.167289, loss: 2.0178
2022-07-26 05:56:39 - train: epoch 0031, iter [01000, 05004], lr: 0.167240, loss: 2.2235
2022-07-26 05:58:34 - train: epoch 0031, iter [01100, 05004], lr: 0.167192, loss: 1.8581
2022-07-26 06:00:29 - train: epoch 0031, iter [01200, 05004], lr: 0.167143, loss: 2.0030
2022-07-26 06:02:25 - train: epoch 0031, iter [01300, 05004], lr: 0.167094, loss: 1.5925
2022-07-26 06:04:18 - train: epoch 0031, iter [01400, 05004], lr: 0.167045, loss: 1.8688
2022-07-26 06:06:15 - train: epoch 0031, iter [01500, 05004], lr: 0.166996, loss: 2.2193
2022-07-26 06:08:08 - train: epoch 0031, iter [01600, 05004], lr: 0.166946, loss: 1.8839
2022-07-26 06:10:03 - train: epoch 0031, iter [01700, 05004], lr: 0.166897, loss: 1.7912
2022-07-26 06:12:00 - train: epoch 0031, iter [01800, 05004], lr: 0.166848, loss: 1.7645
2022-07-26 06:13:59 - train: epoch 0031, iter [01900, 05004], lr: 0.166799, loss: 1.8258
2022-07-26 06:15:54 - train: epoch 0031, iter [02000, 05004], lr: 0.166750, loss: 1.8700
2022-07-26 06:17:49 - train: epoch 0031, iter [02100, 05004], lr: 0.166701, loss: 1.9503
2022-07-26 06:19:49 - train: epoch 0031, iter [02200, 05004], lr: 0.166651, loss: 1.7531
2022-07-26 06:21:46 - train: epoch 0031, iter [02300, 05004], lr: 0.166602, loss: 1.8092
2022-07-26 06:23:44 - train: epoch 0031, iter [02400, 05004], lr: 0.166553, loss: 2.2914
2022-07-26 06:25:39 - train: epoch 0031, iter [02500, 05004], lr: 0.166503, loss: 2.1897
2022-07-26 06:27:31 - train: epoch 0031, iter [02600, 05004], lr: 0.166454, loss: 1.9578
2022-07-26 06:29:29 - train: epoch 0031, iter [02700, 05004], lr: 0.166405, loss: 2.1892
2022-07-26 06:31:24 - train: epoch 0031, iter [02800, 05004], lr: 0.166355, loss: 2.2425
2022-07-26 06:33:24 - train: epoch 0031, iter [02900, 05004], lr: 0.166306, loss: 2.0030
2022-07-26 06:35:15 - train: epoch 0031, iter [03000, 05004], lr: 0.166256, loss: 2.1404
2022-07-26 06:37:15 - train: epoch 0031, iter [03100, 05004], lr: 0.166207, loss: 2.0310
2022-07-26 06:39:07 - train: epoch 0031, iter [03200, 05004], lr: 0.166157, loss: 2.2979
2022-07-26 06:41:06 - train: epoch 0031, iter [03300, 05004], lr: 0.166108, loss: 1.9738
2022-07-26 06:42:58 - train: epoch 0031, iter [03400, 05004], lr: 0.166058, loss: 2.1014
2022-07-26 06:44:53 - train: epoch 0031, iter [03500, 05004], lr: 0.166008, loss: 2.0615
2022-07-26 06:46:52 - train: epoch 0031, iter [03600, 05004], lr: 0.165959, loss: 2.0759
2022-07-26 06:48:52 - train: epoch 0031, iter [03700, 05004], lr: 0.165909, loss: 1.7820
2022-07-26 06:50:47 - train: epoch 0031, iter [03800, 05004], lr: 0.165859, loss: 2.1931
2022-07-26 06:52:44 - train: epoch 0031, iter [03900, 05004], lr: 0.165810, loss: 1.9903
2022-07-26 06:54:46 - train: epoch 0031, iter [04000, 05004], lr: 0.165760, loss: 1.9586
2022-07-26 06:56:40 - train: epoch 0031, iter [04100, 05004], lr: 0.165710, loss: 2.0964
2022-07-26 06:58:36 - train: epoch 0031, iter [04200, 05004], lr: 0.165660, loss: 2.1074
2022-07-26 07:00:35 - train: epoch 0031, iter [04300, 05004], lr: 0.165610, loss: 2.0911
2022-07-26 07:02:34 - train: epoch 0031, iter [04400, 05004], lr: 0.165561, loss: 2.0738
2022-07-26 07:04:33 - train: epoch 0031, iter [04500, 05004], lr: 0.165511, loss: 2.1173
2022-07-26 07:06:25 - train: epoch 0031, iter [04600, 05004], lr: 0.165461, loss: 2.3346
2022-07-26 07:08:21 - train: epoch 0031, iter [04700, 05004], lr: 0.165411, loss: 1.7099
2022-07-26 07:10:18 - train: epoch 0031, iter [04800, 05004], lr: 0.165361, loss: 1.9083
