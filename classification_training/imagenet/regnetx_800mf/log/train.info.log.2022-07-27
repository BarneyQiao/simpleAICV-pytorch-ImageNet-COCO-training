2022-07-27 07:12:36 - train: epoch 0045, iter [04600, 05004], lr: 0.124807, loss: 2.0287
2022-07-27 07:14:38 - train: epoch 0045, iter [04700, 05004], lr: 0.124743, loss: 1.6577
2022-07-27 07:16:37 - train: epoch 0045, iter [04800, 05004], lr: 0.124679, loss: 1.7079
2022-07-27 07:18:29 - train: epoch 0045, iter [04900, 05004], lr: 0.124615, loss: 1.9645
2022-07-27 07:20:24 - train: epoch 0045, iter [05000, 05004], lr: 0.124551, loss: 1.8170
2022-07-27 07:20:28 - train: epoch 045, train_loss: 1.8542
2022-07-27 07:24:29 - eval: epoch: 045, acc1: 61.382%, acc5: 84.690%, test_loss: 1.5939, per_image_load_time: 5.385ms, per_image_inference_time: 0.756ms
2022-07-27 07:24:30 - until epoch: 045, best_acc1: 61.382%
2022-07-27 07:24:30 - epoch 046 lr: 0.124548
2022-07-27 07:26:38 - train: epoch 0046, iter [00100, 05004], lr: 0.124484, loss: 1.5782
2022-07-27 07:28:32 - train: epoch 0046, iter [00200, 05004], lr: 0.124420, loss: 1.8230
2022-07-27 07:30:22 - train: epoch 0046, iter [00300, 05004], lr: 0.124356, loss: 1.7108
2022-07-27 07:32:11 - train: epoch 0046, iter [00400, 05004], lr: 0.124292, loss: 1.8195
2022-07-27 07:34:06 - train: epoch 0046, iter [00500, 05004], lr: 0.124228, loss: 1.8252
2022-07-27 07:35:58 - train: epoch 0046, iter [00600, 05004], lr: 0.124164, loss: 1.7524
2022-07-27 07:37:58 - train: epoch 0046, iter [00700, 05004], lr: 0.124100, loss: 1.5104
2022-07-27 07:39:55 - train: epoch 0046, iter [00800, 05004], lr: 0.124036, loss: 1.9527
2022-07-27 07:41:55 - train: epoch 0046, iter [00900, 05004], lr: 0.123972, loss: 1.7095
2022-07-27 07:43:52 - train: epoch 0046, iter [01000, 05004], lr: 0.123907, loss: 1.5801
2022-07-27 07:45:49 - train: epoch 0046, iter [01100, 05004], lr: 0.123843, loss: 1.9736
2022-07-27 07:47:44 - train: epoch 0046, iter [01200, 05004], lr: 0.123779, loss: 1.6612
2022-07-27 07:49:43 - train: epoch 0046, iter [01300, 05004], lr: 0.123715, loss: 2.0380
2022-07-27 07:51:50 - train: epoch 0046, iter [01400, 05004], lr: 0.123651, loss: 1.6746
2022-07-27 07:54:00 - train: epoch 0046, iter [01500, 05004], lr: 0.123586, loss: 1.6685
2022-07-27 07:55:54 - train: epoch 0046, iter [01600, 05004], lr: 0.123522, loss: 1.7894
2022-07-27 07:58:01 - train: epoch 0046, iter [01700, 05004], lr: 0.123458, loss: 1.8711
2022-07-27 08:00:06 - train: epoch 0046, iter [01800, 05004], lr: 0.123394, loss: 2.0009
2022-07-27 08:02:07 - train: epoch 0046, iter [01900, 05004], lr: 0.123329, loss: 1.7717
2022-07-27 08:04:15 - train: epoch 0046, iter [02000, 05004], lr: 0.123265, loss: 1.8781
2022-07-27 08:06:19 - train: epoch 0046, iter [02100, 05004], lr: 0.123201, loss: 2.1044
2022-07-27 08:08:19 - train: epoch 0046, iter [02200, 05004], lr: 0.123137, loss: 1.7343
2022-07-27 08:10:27 - train: epoch 0046, iter [02300, 05004], lr: 0.123072, loss: 2.1306
2022-07-27 08:12:28 - train: epoch 0046, iter [02400, 05004], lr: 0.123008, loss: 1.8411
2022-07-27 08:14:31 - train: epoch 0046, iter [02500, 05004], lr: 0.122944, loss: 1.8722
2022-07-27 08:16:31 - train: epoch 0046, iter [02600, 05004], lr: 0.122879, loss: 1.9996
2022-07-27 08:18:38 - train: epoch 0046, iter [02700, 05004], lr: 0.122815, loss: 1.6128
2022-07-27 08:20:38 - train: epoch 0046, iter [02800, 05004], lr: 0.122751, loss: 1.7406
2022-07-27 08:22:39 - train: epoch 0046, iter [02900, 05004], lr: 0.122686, loss: 1.9256
2022-07-27 08:24:42 - train: epoch 0046, iter [03000, 05004], lr: 0.122622, loss: 1.7361
2022-07-27 08:26:45 - train: epoch 0046, iter [03100, 05004], lr: 0.122558, loss: 1.7324
2022-07-27 08:28:47 - train: epoch 0046, iter [03200, 05004], lr: 0.122493, loss: 2.0458
2022-07-27 08:30:47 - train: epoch 0046, iter [03300, 05004], lr: 0.122429, loss: 1.8979
2022-07-27 08:32:49 - train: epoch 0046, iter [03400, 05004], lr: 0.122364, loss: 1.9564
2022-07-27 08:34:48 - train: epoch 0046, iter [03500, 05004], lr: 0.122300, loss: 1.9709
2022-07-27 08:36:48 - train: epoch 0046, iter [03600, 05004], lr: 0.122236, loss: 1.8511
2022-07-27 08:38:51 - train: epoch 0046, iter [03700, 05004], lr: 0.122171, loss: 1.5664
2022-07-27 08:40:49 - train: epoch 0046, iter [03800, 05004], lr: 0.122107, loss: 1.7351
2022-07-27 08:42:52 - train: epoch 0046, iter [03900, 05004], lr: 0.122042, loss: 1.7502
2022-07-27 08:44:51 - train: epoch 0046, iter [04000, 05004], lr: 0.121978, loss: 1.8498
2022-07-27 08:46:57 - train: epoch 0046, iter [04100, 05004], lr: 0.121913, loss: 1.9252
2022-07-27 08:48:58 - train: epoch 0046, iter [04200, 05004], lr: 0.121849, loss: 1.7327
2022-07-27 08:51:01 - train: epoch 0046, iter [04300, 05004], lr: 0.121784, loss: 2.0191
2022-07-27 08:53:01 - train: epoch 0046, iter [04400, 05004], lr: 0.121720, loss: 1.9230
2022-07-27 08:55:01 - train: epoch 0046, iter [04500, 05004], lr: 0.121655, loss: 1.7658
2022-07-27 08:57:04 - train: epoch 0046, iter [04600, 05004], lr: 0.121591, loss: 1.9135
2022-07-27 08:59:07 - train: epoch 0046, iter [04700, 05004], lr: 0.121526, loss: 1.8562
2022-07-27 09:01:08 - train: epoch 0046, iter [04800, 05004], lr: 0.121462, loss: 1.7478
2022-07-27 09:03:07 - train: epoch 0046, iter [04900, 05004], lr: 0.121397, loss: 2.0344
2022-07-27 09:05:09 - train: epoch 0046, iter [05000, 05004], lr: 0.121333, loss: 1.7625
2022-07-27 09:05:13 - train: epoch 046, train_loss: 1.8445
2022-07-27 09:09:12 - eval: epoch: 046, acc1: 61.468%, acc5: 84.622%, test_loss: 1.6009, per_image_load_time: 6.807ms, per_image_inference_time: 0.871ms
2022-07-27 09:09:12 - until epoch: 046, best_acc1: 61.468%
2022-07-27 09:09:12 - epoch 047 lr: 0.121329
2022-07-27 09:11:37 - train: epoch 0047, iter [00100, 05004], lr: 0.121265, loss: 1.8367
2022-07-27 09:13:42 - train: epoch 0047, iter [00200, 05004], lr: 0.121201, loss: 1.8227
2022-07-27 09:15:39 - train: epoch 0047, iter [00300, 05004], lr: 0.121136, loss: 1.5106
2022-07-27 09:17:37 - train: epoch 0047, iter [00400, 05004], lr: 0.121072, loss: 1.7304
2022-07-27 09:19:37 - train: epoch 0047, iter [00500, 05004], lr: 0.121007, loss: 1.6970
2022-07-27 09:21:43 - train: epoch 0047, iter [00600, 05004], lr: 0.120942, loss: 1.8632
2022-07-27 09:23:45 - train: epoch 0047, iter [00700, 05004], lr: 0.120878, loss: 1.8958
2022-07-27 09:25:47 - train: epoch 0047, iter [00800, 05004], lr: 0.120813, loss: 1.8869
2022-07-27 09:27:47 - train: epoch 0047, iter [00900, 05004], lr: 0.120749, loss: 1.8678
2022-07-27 09:29:51 - train: epoch 0047, iter [01000, 05004], lr: 0.120684, loss: 1.7146
2022-07-27 09:31:52 - train: epoch 0047, iter [01100, 05004], lr: 0.120619, loss: 1.9901
2022-07-27 09:33:51 - train: epoch 0047, iter [01200, 05004], lr: 0.120555, loss: 1.7182
2022-07-27 09:36:01 - train: epoch 0047, iter [01300, 05004], lr: 0.120490, loss: 1.7442
2022-07-27 09:38:01 - train: epoch 0047, iter [01400, 05004], lr: 0.120425, loss: 1.7461
2022-07-27 09:40:03 - train: epoch 0047, iter [01500, 05004], lr: 0.120360, loss: 1.8644
2022-07-27 09:42:08 - train: epoch 0047, iter [01600, 05004], lr: 0.120296, loss: 1.7970
2022-07-27 09:44:05 - train: epoch 0047, iter [01700, 05004], lr: 0.120231, loss: 1.7080
2022-07-27 09:46:08 - train: epoch 0047, iter [01800, 05004], lr: 0.120166, loss: 1.8453
2022-07-27 09:48:17 - train: epoch 0047, iter [01900, 05004], lr: 0.120102, loss: 1.8123
2022-07-27 09:50:25 - train: epoch 0047, iter [02000, 05004], lr: 0.120037, loss: 1.8727
2022-07-27 09:52:28 - train: epoch 0047, iter [02100, 05004], lr: 0.119972, loss: 1.9594
2022-07-27 09:54:31 - train: epoch 0047, iter [02200, 05004], lr: 0.119907, loss: 1.9299
2022-07-27 09:56:35 - train: epoch 0047, iter [02300, 05004], lr: 0.119843, loss: 1.7386
2022-07-27 09:58:32 - train: epoch 0047, iter [02400, 05004], lr: 0.119778, loss: 1.7528
2022-07-27 10:00:32 - train: epoch 0047, iter [02500, 05004], lr: 0.119713, loss: 1.8561
2022-07-27 10:02:37 - train: epoch 0047, iter [02600, 05004], lr: 0.119648, loss: 2.0985
2022-07-27 10:04:39 - train: epoch 0047, iter [02700, 05004], lr: 0.119583, loss: 1.7335
2022-07-27 10:06:41 - train: epoch 0047, iter [02800, 05004], lr: 0.119519, loss: 1.9145
2022-07-27 10:08:40 - train: epoch 0047, iter [02900, 05004], lr: 0.119454, loss: 1.8143
2022-07-27 10:10:38 - train: epoch 0047, iter [03000, 05004], lr: 0.119389, loss: 1.7898
2022-07-27 10:12:39 - train: epoch 0047, iter [03100, 05004], lr: 0.119324, loss: 1.7776
2022-07-27 10:14:39 - train: epoch 0047, iter [03200, 05004], lr: 0.119259, loss: 1.9602
2022-07-27 10:16:40 - train: epoch 0047, iter [03300, 05004], lr: 0.119194, loss: 1.5954
2022-07-27 10:18:37 - train: epoch 0047, iter [03400, 05004], lr: 0.119130, loss: 1.6481
2022-07-27 10:20:38 - train: epoch 0047, iter [03500, 05004], lr: 0.119065, loss: 2.1893
2022-07-27 10:22:37 - train: epoch 0047, iter [03600, 05004], lr: 0.119000, loss: 1.8227
2022-07-27 10:24:39 - train: epoch 0047, iter [03700, 05004], lr: 0.118935, loss: 1.7665
2022-07-27 10:26:43 - train: epoch 0047, iter [03800, 05004], lr: 0.118870, loss: 1.6837
2022-07-27 10:28:48 - train: epoch 0047, iter [03900, 05004], lr: 0.118805, loss: 1.8880
2022-07-27 10:30:47 - train: epoch 0047, iter [04000, 05004], lr: 0.118740, loss: 1.9295
2022-07-27 10:32:52 - train: epoch 0047, iter [04100, 05004], lr: 0.118675, loss: 1.9910
2022-07-27 10:34:51 - train: epoch 0047, iter [04200, 05004], lr: 0.118610, loss: 1.7642
2022-07-27 10:36:54 - train: epoch 0047, iter [04300, 05004], lr: 0.118545, loss: 1.6744
2022-07-27 10:38:57 - train: epoch 0047, iter [04400, 05004], lr: 0.118480, loss: 1.7693
2022-07-27 10:40:58 - train: epoch 0047, iter [04500, 05004], lr: 0.118416, loss: 1.8317
2022-07-27 10:42:57 - train: epoch 0047, iter [04600, 05004], lr: 0.118351, loss: 1.6972
2022-07-27 10:44:54 - train: epoch 0047, iter [04700, 05004], lr: 0.118286, loss: 2.0909
2022-07-27 10:46:59 - train: epoch 0047, iter [04800, 05004], lr: 0.118221, loss: 1.6815
2022-07-27 10:48:51 - train: epoch 0047, iter [04900, 05004], lr: 0.118156, loss: 1.9880
2022-07-27 10:50:44 - train: epoch 0047, iter [05000, 05004], lr: 0.118091, loss: 1.7829
2022-07-27 10:50:47 - train: epoch 047, train_loss: 1.8292
2022-07-27 10:54:50 - eval: epoch: 047, acc1: 62.154%, acc5: 84.862%, test_loss: 1.5718, per_image_load_time: 8.654ms, per_image_inference_time: 0.793ms
2022-07-27 10:54:51 - until epoch: 047, best_acc1: 62.154%
2022-07-27 10:54:51 - epoch 048 lr: 0.118087
2022-07-27 10:56:55 - train: epoch 0048, iter [00100, 05004], lr: 0.118023, loss: 1.7685
2022-07-27 10:58:43 - train: epoch 0048, iter [00200, 05004], lr: 0.117958, loss: 1.9752
2022-07-27 11:00:36 - train: epoch 0048, iter [00300, 05004], lr: 0.117893, loss: 1.7488
2022-07-27 11:02:32 - train: epoch 0048, iter [00400, 05004], lr: 0.117828, loss: 1.8999
2022-07-27 11:04:33 - train: epoch 0048, iter [00500, 05004], lr: 0.117763, loss: 1.8621
2022-07-27 11:06:28 - train: epoch 0048, iter [00600, 05004], lr: 0.117698, loss: 1.8864
2022-07-27 11:08:27 - train: epoch 0048, iter [00700, 05004], lr: 0.117633, loss: 1.7824
2022-07-27 11:10:28 - train: epoch 0048, iter [00800, 05004], lr: 0.117568, loss: 1.8211
2022-07-27 11:12:29 - train: epoch 0048, iter [00900, 05004], lr: 0.117503, loss: 2.0432
2022-07-27 11:14:29 - train: epoch 0048, iter [01000, 05004], lr: 0.117438, loss: 1.7076
2022-07-27 11:16:24 - train: epoch 0048, iter [01100, 05004], lr: 0.117373, loss: 2.1297
2022-07-27 11:18:22 - train: epoch 0048, iter [01200, 05004], lr: 0.117308, loss: 1.8035
2022-07-27 11:20:19 - train: epoch 0048, iter [01300, 05004], lr: 0.117242, loss: 1.5916
2022-07-27 11:22:15 - train: epoch 0048, iter [01400, 05004], lr: 0.117177, loss: 1.8327
2022-07-27 11:24:15 - train: epoch 0048, iter [01500, 05004], lr: 0.117112, loss: 1.8888
2022-07-27 11:26:08 - train: epoch 0048, iter [01600, 05004], lr: 0.117047, loss: 1.8657
2022-07-27 11:28:18 - train: epoch 0048, iter [01700, 05004], lr: 0.116982, loss: 1.9167
2022-07-27 11:30:11 - train: epoch 0048, iter [01800, 05004], lr: 0.116917, loss: 1.8754
2022-07-27 11:32:14 - train: epoch 0048, iter [01900, 05004], lr: 0.116852, loss: 1.8972
2022-07-27 11:34:18 - train: epoch 0048, iter [02000, 05004], lr: 0.116787, loss: 2.0214
2022-07-27 11:36:09 - train: epoch 0048, iter [02100, 05004], lr: 0.116721, loss: 1.6948
2022-07-27 11:38:07 - train: epoch 0048, iter [02200, 05004], lr: 0.116656, loss: 2.0304
2022-07-27 11:40:05 - train: epoch 0048, iter [02300, 05004], lr: 0.116591, loss: 1.8003
2022-07-27 11:42:09 - train: epoch 0048, iter [02400, 05004], lr: 0.116526, loss: 2.0655
2022-07-27 11:44:07 - train: epoch 0048, iter [02500, 05004], lr: 0.116461, loss: 1.9329
2022-07-27 11:46:03 - train: epoch 0048, iter [02600, 05004], lr: 0.116396, loss: 1.9297
2022-07-27 11:48:05 - train: epoch 0048, iter [02700, 05004], lr: 0.116330, loss: 1.8697
2022-07-27 11:50:03 - train: epoch 0048, iter [02800, 05004], lr: 0.116265, loss: 1.6624
2022-07-27 11:52:02 - train: epoch 0048, iter [02900, 05004], lr: 0.116200, loss: 1.9138
2022-07-27 11:53:59 - train: epoch 0048, iter [03000, 05004], lr: 0.116135, loss: 1.7677
2022-07-27 11:55:59 - train: epoch 0048, iter [03100, 05004], lr: 0.116070, loss: 1.8922
2022-07-27 11:57:56 - train: epoch 0048, iter [03200, 05004], lr: 0.116004, loss: 1.6528
2022-07-27 11:59:53 - train: epoch 0048, iter [03300, 05004], lr: 0.115939, loss: 2.0668
2022-07-27 12:01:49 - train: epoch 0048, iter [03400, 05004], lr: 0.115874, loss: 1.7563
2022-07-27 12:03:44 - train: epoch 0048, iter [03500, 05004], lr: 0.115809, loss: 2.0892
2022-07-27 12:05:44 - train: epoch 0048, iter [03600, 05004], lr: 0.115743, loss: 1.9417
2022-07-27 12:07:48 - train: epoch 0048, iter [03700, 05004], lr: 0.115678, loss: 1.7427
2022-07-27 12:09:45 - train: epoch 0048, iter [03800, 05004], lr: 0.115613, loss: 1.7612
2022-07-27 12:11:47 - train: epoch 0048, iter [03900, 05004], lr: 0.115547, loss: 2.1049
2022-07-27 12:13:51 - train: epoch 0048, iter [04000, 05004], lr: 0.115482, loss: 1.6397
2022-07-27 12:15:50 - train: epoch 0048, iter [04100, 05004], lr: 0.115417, loss: 2.0109
2022-07-27 12:17:45 - train: epoch 0048, iter [04200, 05004], lr: 0.115352, loss: 1.6792
2022-07-27 12:19:42 - train: epoch 0048, iter [04300, 05004], lr: 0.115286, loss: 1.8447
2022-07-27 12:21:40 - train: epoch 0048, iter [04400, 05004], lr: 0.115221, loss: 1.7674
2022-07-27 12:23:41 - train: epoch 0048, iter [04500, 05004], lr: 0.115156, loss: 1.8173
2022-07-27 12:25:40 - train: epoch 0048, iter [04600, 05004], lr: 0.115090, loss: 1.7135
2022-07-27 12:27:36 - train: epoch 0048, iter [04700, 05004], lr: 0.115025, loss: 1.9884
2022-07-27 12:29:34 - train: epoch 0048, iter [04800, 05004], lr: 0.114960, loss: 1.8637
2022-07-27 12:31:32 - train: epoch 0048, iter [04900, 05004], lr: 0.114894, loss: 1.8520
2022-07-27 12:33:28 - train: epoch 0048, iter [05000, 05004], lr: 0.114829, loss: 1.9011
2022-07-27 12:33:31 - train: epoch 048, train_loss: 1.8210
2022-07-27 12:37:44 - eval: epoch: 048, acc1: 61.678%, acc5: 84.582%, test_loss: 1.5942, per_image_load_time: 9.044ms, per_image_inference_time: 0.751ms
2022-07-27 12:37:44 - until epoch: 048, best_acc1: 62.154%
2022-07-27 12:37:44 - epoch 049 lr: 0.114826
2022-07-27 12:40:00 - train: epoch 0049, iter [00100, 05004], lr: 0.114761, loss: 1.7286
2022-07-27 12:41:53 - train: epoch 0049, iter [00200, 05004], lr: 0.114696, loss: 1.7369
2022-07-27 12:43:51 - train: epoch 0049, iter [00300, 05004], lr: 0.114630, loss: 2.0268
2022-07-27 12:45:43 - train: epoch 0049, iter [00400, 05004], lr: 0.114565, loss: 1.7873
2022-07-27 12:47:43 - train: epoch 0049, iter [00500, 05004], lr: 0.114500, loss: 1.8192
2022-07-27 12:49:40 - train: epoch 0049, iter [00600, 05004], lr: 0.114434, loss: 1.6875
2022-07-27 12:51:36 - train: epoch 0049, iter [00700, 05004], lr: 0.114369, loss: 1.7122
2022-07-27 12:53:32 - train: epoch 0049, iter [00800, 05004], lr: 0.114303, loss: 2.1647
2022-07-27 12:55:34 - train: epoch 0049, iter [00900, 05004], lr: 0.114238, loss: 1.6239
2022-07-27 12:57:31 - train: epoch 0049, iter [01000, 05004], lr: 0.114172, loss: 1.7199
2022-07-27 12:59:26 - train: epoch 0049, iter [01100, 05004], lr: 0.114107, loss: 1.6324
2022-07-27 13:01:22 - train: epoch 0049, iter [01200, 05004], lr: 0.114042, loss: 1.8051
2022-07-27 13:03:19 - train: epoch 0049, iter [01300, 05004], lr: 0.113976, loss: 2.0535
2022-07-27 13:05:18 - train: epoch 0049, iter [01400, 05004], lr: 0.113911, loss: 1.9741
2022-07-27 13:07:15 - train: epoch 0049, iter [01500, 05004], lr: 0.113845, loss: 1.8050
2022-07-27 13:09:15 - train: epoch 0049, iter [01600, 05004], lr: 0.113780, loss: 1.8489
2022-07-27 13:11:14 - train: epoch 0049, iter [01700, 05004], lr: 0.113714, loss: 1.6536
2022-07-27 13:13:22 - train: epoch 0049, iter [01800, 05004], lr: 0.113649, loss: 1.7429
2022-07-27 13:15:18 - train: epoch 0049, iter [01900, 05004], lr: 0.113583, loss: 1.8300
2022-07-27 13:17:17 - train: epoch 0049, iter [02000, 05004], lr: 0.113518, loss: 1.8380
2022-07-27 13:19:15 - train: epoch 0049, iter [02100, 05004], lr: 0.113453, loss: 1.7351
2022-07-27 13:21:16 - train: epoch 0049, iter [02200, 05004], lr: 0.113387, loss: 1.6838
2022-07-27 13:23:12 - train: epoch 0049, iter [02300, 05004], lr: 0.113322, loss: 1.7872
2022-07-27 13:25:11 - train: epoch 0049, iter [02400, 05004], lr: 0.113256, loss: 1.8139
2022-07-27 13:27:04 - train: epoch 0049, iter [02500, 05004], lr: 0.113191, loss: 1.8462
2022-07-27 13:29:02 - train: epoch 0049, iter [02600, 05004], lr: 0.113125, loss: 1.9372
2022-07-27 13:31:00 - train: epoch 0049, iter [02700, 05004], lr: 0.113059, loss: 1.8184
2022-07-27 13:32:54 - train: epoch 0049, iter [02800, 05004], lr: 0.112994, loss: 1.8384
2022-07-27 13:34:45 - train: epoch 0049, iter [02900, 05004], lr: 0.112928, loss: 1.7889
2022-07-27 13:36:47 - train: epoch 0049, iter [03000, 05004], lr: 0.112863, loss: 1.9175
2022-07-27 13:38:42 - train: epoch 0049, iter [03100, 05004], lr: 0.112797, loss: 1.8762
2022-07-27 13:40:35 - train: epoch 0049, iter [03200, 05004], lr: 0.112732, loss: 1.9594
2022-07-27 13:42:37 - train: epoch 0049, iter [03300, 05004], lr: 0.112666, loss: 1.8426
2022-07-27 13:44:34 - train: epoch 0049, iter [03400, 05004], lr: 0.112601, loss: 1.6836
2022-07-27 13:46:29 - train: epoch 0049, iter [03500, 05004], lr: 0.112535, loss: 1.9440
2022-07-27 13:48:24 - train: epoch 0049, iter [03600, 05004], lr: 0.112470, loss: 1.8605
2022-07-27 13:50:24 - train: epoch 0049, iter [03700, 05004], lr: 0.112404, loss: 1.6544
2022-07-27 13:52:18 - train: epoch 0049, iter [03800, 05004], lr: 0.112338, loss: 1.9096
2022-07-27 13:54:20 - train: epoch 0049, iter [03900, 05004], lr: 0.112273, loss: 1.9824
2022-07-27 13:56:15 - train: epoch 0049, iter [04000, 05004], lr: 0.112207, loss: 1.6474
2022-07-27 13:58:19 - train: epoch 0049, iter [04100, 05004], lr: 0.112142, loss: 1.6812
2022-07-27 14:00:13 - train: epoch 0049, iter [04200, 05004], lr: 0.112076, loss: 1.9439
2022-07-27 14:02:13 - train: epoch 0049, iter [04300, 05004], lr: 0.112010, loss: 2.0031
2022-07-27 14:04:08 - train: epoch 0049, iter [04400, 05004], lr: 0.111945, loss: 1.8366
2022-07-27 14:06:00 - train: epoch 0049, iter [04500, 05004], lr: 0.111879, loss: 1.7756
2022-07-27 14:07:52 - train: epoch 0049, iter [04600, 05004], lr: 0.111814, loss: 1.6976
2022-07-27 14:09:47 - train: epoch 0049, iter [04700, 05004], lr: 0.111748, loss: 1.9822
2022-07-27 14:11:44 - train: epoch 0049, iter [04800, 05004], lr: 0.111682, loss: 1.8265
2022-07-27 14:13:35 - train: epoch 0049, iter [04900, 05004], lr: 0.111617, loss: 1.6938
2022-07-27 14:15:26 - train: epoch 0049, iter [05000, 05004], lr: 0.111551, loss: 1.7971
2022-07-27 14:15:29 - train: epoch 049, train_loss: 1.8056
2022-07-27 14:19:25 - eval: epoch: 049, acc1: 62.972%, acc5: 85.690%, test_loss: 1.5235, per_image_load_time: 2.441ms, per_image_inference_time: 0.792ms
2022-07-27 14:19:25 - until epoch: 049, best_acc1: 62.972%
2022-07-27 14:19:25 - epoch 050 lr: 0.111548
2022-07-27 14:21:47 - train: epoch 0050, iter [00100, 05004], lr: 0.111483, loss: 1.7554
2022-07-27 14:23:50 - train: epoch 0050, iter [00200, 05004], lr: 0.111417, loss: 1.6216
2022-07-27 14:25:51 - train: epoch 0050, iter [00300, 05004], lr: 0.111352, loss: 1.6585
2022-07-27 14:27:45 - train: epoch 0050, iter [00400, 05004], lr: 0.111286, loss: 1.6354
2022-07-27 14:29:35 - train: epoch 0050, iter [00500, 05004], lr: 0.111220, loss: 1.6388
2022-07-27 14:31:41 - train: epoch 0050, iter [00600, 05004], lr: 0.111155, loss: 1.9911
2022-07-27 14:33:45 - train: epoch 0050, iter [00700, 05004], lr: 0.111089, loss: 1.5381
2022-07-27 14:35:53 - train: epoch 0050, iter [00800, 05004], lr: 0.111023, loss: 1.3800
2022-07-27 14:37:53 - train: epoch 0050, iter [00900, 05004], lr: 0.110957, loss: 1.7383
2022-07-27 14:39:53 - train: epoch 0050, iter [01000, 05004], lr: 0.110892, loss: 1.8864
2022-07-27 14:41:54 - train: epoch 0050, iter [01100, 05004], lr: 0.110826, loss: 1.9651
2022-07-27 14:44:00 - train: epoch 0050, iter [01200, 05004], lr: 0.110760, loss: 1.6513
2022-07-27 14:46:04 - train: epoch 0050, iter [01300, 05004], lr: 0.110695, loss: 1.6801
2022-07-27 14:48:09 - train: epoch 0050, iter [01400, 05004], lr: 0.110629, loss: 1.9300
2022-07-27 14:50:13 - train: epoch 0050, iter [01500, 05004], lr: 0.110563, loss: 1.7525
2022-07-27 14:52:27 - train: epoch 0050, iter [01600, 05004], lr: 0.110498, loss: 1.8103
2022-07-27 14:54:23 - train: epoch 0050, iter [01700, 05004], lr: 0.110432, loss: 1.9449
2022-07-27 14:56:25 - train: epoch 0050, iter [01800, 05004], lr: 0.110366, loss: 1.9260
2022-07-27 14:58:41 - train: epoch 0050, iter [01900, 05004], lr: 0.110300, loss: 1.6885
2022-07-27 15:00:37 - train: epoch 0050, iter [02000, 05004], lr: 0.110235, loss: 1.5877
2022-07-27 15:02:42 - train: epoch 0050, iter [02100, 05004], lr: 0.110169, loss: 1.5934
2022-07-27 15:04:48 - train: epoch 0050, iter [02200, 05004], lr: 0.110103, loss: 1.9396
2022-07-27 15:06:55 - train: epoch 0050, iter [02300, 05004], lr: 0.110037, loss: 1.6597
2022-07-27 15:09:05 - train: epoch 0050, iter [02400, 05004], lr: 0.109972, loss: 1.9276
2022-07-27 15:11:11 - train: epoch 0050, iter [02500, 05004], lr: 0.109906, loss: 1.5856
2022-07-27 15:13:14 - train: epoch 0050, iter [02600, 05004], lr: 0.109840, loss: 1.6274
2022-07-27 15:15:17 - train: epoch 0050, iter [02700, 05004], lr: 0.109774, loss: 1.7546
2022-07-27 15:17:24 - train: epoch 0050, iter [02800, 05004], lr: 0.109709, loss: 1.7841
2022-07-27 15:19:26 - train: epoch 0050, iter [02900, 05004], lr: 0.109643, loss: 2.1479
2022-07-27 15:21:32 - train: epoch 0050, iter [03000, 05004], lr: 0.109577, loss: 1.8102
2022-07-27 15:23:34 - train: epoch 0050, iter [03100, 05004], lr: 0.109511, loss: 1.9941
2022-07-27 15:25:31 - train: epoch 0050, iter [03200, 05004], lr: 0.109445, loss: 1.8819
2022-07-27 15:27:22 - train: epoch 0050, iter [03300, 05004], lr: 0.109380, loss: 1.6718
2022-07-27 15:29:17 - train: epoch 0050, iter [03400, 05004], lr: 0.109314, loss: 1.7554
2022-07-27 15:31:09 - train: epoch 0050, iter [03500, 05004], lr: 0.109248, loss: 1.7626
2022-07-27 15:33:05 - train: epoch 0050, iter [03600, 05004], lr: 0.109182, loss: 1.8500
2022-07-27 15:35:02 - train: epoch 0050, iter [03700, 05004], lr: 0.109116, loss: 2.0881
2022-07-27 15:36:58 - train: epoch 0050, iter [03800, 05004], lr: 0.109051, loss: 1.6522
2022-07-27 15:38:54 - train: epoch 0050, iter [03900, 05004], lr: 0.108985, loss: 1.6798
2022-07-27 15:40:55 - train: epoch 0050, iter [04000, 05004], lr: 0.108919, loss: 1.9057
2022-07-27 15:42:52 - train: epoch 0050, iter [04100, 05004], lr: 0.108853, loss: 1.8771
2022-07-27 15:44:50 - train: epoch 0050, iter [04200, 05004], lr: 0.108787, loss: 1.9186
2022-07-27 15:46:45 - train: epoch 0050, iter [04300, 05004], lr: 0.108721, loss: 1.9602
2022-07-27 15:48:39 - train: epoch 0050, iter [04400, 05004], lr: 0.108656, loss: 1.7647
2022-07-27 15:50:32 - train: epoch 0050, iter [04500, 05004], lr: 0.108590, loss: 1.7432
2022-07-27 15:52:35 - train: epoch 0050, iter [04600, 05004], lr: 0.108524, loss: 1.9577
2022-07-27 15:54:41 - train: epoch 0050, iter [04700, 05004], lr: 0.108458, loss: 1.8731
2022-07-27 15:56:40 - train: epoch 0050, iter [04800, 05004], lr: 0.108392, loss: 1.7695
2022-07-27 15:58:39 - train: epoch 0050, iter [04900, 05004], lr: 0.108326, loss: 1.6427
2022-07-27 16:00:38 - train: epoch 0050, iter [05000, 05004], lr: 0.108261, loss: 1.6795
2022-07-27 16:00:41 - train: epoch 050, train_loss: 1.7942
2022-07-27 16:04:42 - eval: epoch: 050, acc1: 62.654%, acc5: 85.292%, test_loss: 1.5552, per_image_load_time: 5.182ms, per_image_inference_time: 0.840ms
2022-07-27 16:04:42 - until epoch: 050, best_acc1: 62.972%
2022-07-27 16:04:42 - epoch 051 lr: 0.108257
2022-07-27 16:07:07 - train: epoch 0051, iter [00100, 05004], lr: 0.108192, loss: 1.9932
2022-07-27 16:09:03 - train: epoch 0051, iter [00200, 05004], lr: 0.108126, loss: 1.9558
2022-07-27 16:11:07 - train: epoch 0051, iter [00300, 05004], lr: 0.108060, loss: 1.7636
2022-07-27 16:13:07 - train: epoch 0051, iter [00400, 05004], lr: 0.107994, loss: 1.4812
2022-07-27 16:15:11 - train: epoch 0051, iter [00500, 05004], lr: 0.107929, loss: 1.8469
2022-07-27 16:17:12 - train: epoch 0051, iter [00600, 05004], lr: 0.107863, loss: 1.7758
2022-07-27 16:19:14 - train: epoch 0051, iter [00700, 05004], lr: 0.107797, loss: 1.8378
2022-07-27 16:21:11 - train: epoch 0051, iter [00800, 05004], lr: 0.107731, loss: 1.9165
2022-07-27 16:23:18 - train: epoch 0051, iter [00900, 05004], lr: 0.107665, loss: 1.6704
2022-07-27 16:25:15 - train: epoch 0051, iter [01000, 05004], lr: 0.107599, loss: 2.0249
2022-07-27 16:27:17 - train: epoch 0051, iter [01100, 05004], lr: 0.107533, loss: 1.8935
2022-07-27 16:29:22 - train: epoch 0051, iter [01200, 05004], lr: 0.107467, loss: 1.5043
2022-07-27 16:31:24 - train: epoch 0051, iter [01300, 05004], lr: 0.107401, loss: 1.5950
2022-07-27 16:33:25 - train: epoch 0051, iter [01400, 05004], lr: 0.107336, loss: 1.7783
2022-07-27 16:35:30 - train: epoch 0051, iter [01500, 05004], lr: 0.107270, loss: 1.7208
2022-07-27 16:37:39 - train: epoch 0051, iter [01600, 05004], lr: 0.107204, loss: 1.5766
2022-07-27 16:39:42 - train: epoch 0051, iter [01700, 05004], lr: 0.107138, loss: 1.7238
2022-07-27 16:41:28 - train: epoch 0051, iter [01800, 05004], lr: 0.107072, loss: 1.7794
2022-07-27 16:43:45 - train: epoch 0051, iter [01900, 05004], lr: 0.107006, loss: 1.6442
2022-07-27 16:45:45 - train: epoch 0051, iter [02000, 05004], lr: 0.106940, loss: 1.7816
2022-07-27 16:47:49 - train: epoch 0051, iter [02100, 05004], lr: 0.106874, loss: 1.9075
2022-07-27 16:49:49 - train: epoch 0051, iter [02200, 05004], lr: 0.106808, loss: 1.7054
2022-07-27 16:51:53 - train: epoch 0051, iter [02300, 05004], lr: 0.106742, loss: 1.8768
2022-07-27 16:53:50 - train: epoch 0051, iter [02400, 05004], lr: 0.106676, loss: 1.6459
2022-07-27 16:55:49 - train: epoch 0051, iter [02500, 05004], lr: 0.106610, loss: 1.7397
2022-07-27 16:57:52 - train: epoch 0051, iter [02600, 05004], lr: 0.106544, loss: 1.7534
2022-07-27 16:59:55 - train: epoch 0051, iter [02700, 05004], lr: 0.106478, loss: 1.8290
2022-07-27 17:01:48 - train: epoch 0051, iter [02800, 05004], lr: 0.106413, loss: 1.6643
2022-07-27 17:03:50 - train: epoch 0051, iter [02900, 05004], lr: 0.106347, loss: 1.8205
2022-07-27 17:05:51 - train: epoch 0051, iter [03000, 05004], lr: 0.106281, loss: 1.7489
2022-07-27 17:07:50 - train: epoch 0051, iter [03100, 05004], lr: 0.106215, loss: 1.6379
2022-07-27 17:09:50 - train: epoch 0051, iter [03200, 05004], lr: 0.106149, loss: 1.8304
2022-07-27 17:11:50 - train: epoch 0051, iter [03300, 05004], lr: 0.106083, loss: 1.9944
2022-07-27 17:13:46 - train: epoch 0051, iter [03400, 05004], lr: 0.106017, loss: 1.8719
2022-07-27 17:15:41 - train: epoch 0051, iter [03500, 05004], lr: 0.105951, loss: 1.7376
2022-07-27 17:17:30 - train: epoch 0051, iter [03600, 05004], lr: 0.105885, loss: 1.6723
2022-07-27 17:19:26 - train: epoch 0051, iter [03700, 05004], lr: 0.105819, loss: 1.9083
2022-07-27 17:21:17 - train: epoch 0051, iter [03800, 05004], lr: 0.105753, loss: 1.8262
2022-07-27 17:23:08 - train: epoch 0051, iter [03900, 05004], lr: 0.105687, loss: 1.9062
2022-07-27 17:25:03 - train: epoch 0051, iter [04000, 05004], lr: 0.105621, loss: 1.7296
2022-07-27 17:27:04 - train: epoch 0051, iter [04100, 05004], lr: 0.105555, loss: 1.7925
2022-07-27 17:28:59 - train: epoch 0051, iter [04200, 05004], lr: 0.105489, loss: 1.8271
2022-07-27 17:30:54 - train: epoch 0051, iter [04300, 05004], lr: 0.105423, loss: 1.5649
2022-07-27 17:32:48 - train: epoch 0051, iter [04400, 05004], lr: 0.105357, loss: 1.9914
2022-07-27 17:34:42 - train: epoch 0051, iter [04500, 05004], lr: 0.105291, loss: 1.7688
2022-07-27 17:36:37 - train: epoch 0051, iter [04600, 05004], lr: 0.105225, loss: 2.0895
2022-07-27 17:38:30 - train: epoch 0051, iter [04700, 05004], lr: 0.105159, loss: 1.6583
2022-07-27 17:40:26 - train: epoch 0051, iter [04800, 05004], lr: 0.105093, loss: 1.8536
2022-07-27 17:42:18 - train: epoch 0051, iter [04900, 05004], lr: 0.105027, loss: 1.8846
2022-07-27 17:44:10 - train: epoch 0051, iter [05000, 05004], lr: 0.104961, loss: 1.7955
2022-07-27 17:44:14 - train: epoch 051, train_loss: 1.7809
2022-07-27 17:48:06 - eval: epoch: 051, acc1: 63.432%, acc5: 85.606%, test_loss: 1.5144, per_image_load_time: 8.018ms, per_image_inference_time: 0.799ms
2022-07-27 17:48:06 - until epoch: 051, best_acc1: 63.432%
2022-07-27 17:48:06 - epoch 052 lr: 0.104958
2022-07-27 17:50:20 - train: epoch 0052, iter [00100, 05004], lr: 0.104892, loss: 1.6055
2022-07-27 17:52:08 - train: epoch 0052, iter [00200, 05004], lr: 0.104826, loss: 1.8660
2022-07-27 17:53:57 - train: epoch 0052, iter [00300, 05004], lr: 0.104760, loss: 1.7187
2022-07-27 17:55:47 - train: epoch 0052, iter [00400, 05004], lr: 0.104694, loss: 1.8886
2022-07-27 17:57:42 - train: epoch 0052, iter [00500, 05004], lr: 0.104628, loss: 1.7571
2022-07-27 17:59:35 - train: epoch 0052, iter [00600, 05004], lr: 0.104562, loss: 1.4945
2022-07-27 18:01:26 - train: epoch 0052, iter [00700, 05004], lr: 0.104496, loss: 1.8115
2022-07-27 18:03:26 - train: epoch 0052, iter [00800, 05004], lr: 0.104430, loss: 1.7358
2022-07-27 18:05:25 - train: epoch 0052, iter [00900, 05004], lr: 0.104364, loss: 1.7538
2022-07-27 18:07:20 - train: epoch 0052, iter [01000, 05004], lr: 0.104298, loss: 1.8564
2022-07-27 18:09:17 - train: epoch 0052, iter [01100, 05004], lr: 0.104232, loss: 1.7595
2022-07-27 18:11:12 - train: epoch 0052, iter [01200, 05004], lr: 0.104166, loss: 1.5847
2022-07-27 18:13:04 - train: epoch 0052, iter [01300, 05004], lr: 0.104100, loss: 1.7120
2022-07-27 18:14:53 - train: epoch 0052, iter [01400, 05004], lr: 0.104034, loss: 1.9212
2022-07-27 18:16:49 - train: epoch 0052, iter [01500, 05004], lr: 0.103968, loss: 1.7505
2022-07-27 18:18:45 - train: epoch 0052, iter [01600, 05004], lr: 0.103902, loss: 1.6398
2022-07-27 18:20:37 - train: epoch 0052, iter [01700, 05004], lr: 0.103836, loss: 1.5353
2022-07-27 18:22:31 - train: epoch 0052, iter [01800, 05004], lr: 0.103770, loss: 1.7637
2022-07-27 18:24:25 - train: epoch 0052, iter [01900, 05004], lr: 0.103704, loss: 1.5237
2022-07-27 18:26:09 - train: epoch 0052, iter [02000, 05004], lr: 0.103638, loss: 1.9961
2022-07-27 18:28:18 - train: epoch 0052, iter [02100, 05004], lr: 0.103572, loss: 1.7719
2022-07-27 18:30:11 - train: epoch 0052, iter [02200, 05004], lr: 0.103506, loss: 1.8227
2022-07-27 18:32:12 - train: epoch 0052, iter [02300, 05004], lr: 0.103440, loss: 1.6024
2022-07-27 18:34:08 - train: epoch 0052, iter [02400, 05004], lr: 0.103374, loss: 1.4184
2022-07-27 18:36:05 - train: epoch 0052, iter [02500, 05004], lr: 0.103308, loss: 1.6510
2022-07-27 18:38:04 - train: epoch 0052, iter [02600, 05004], lr: 0.103242, loss: 1.6957
2022-07-27 18:39:59 - train: epoch 0052, iter [02700, 05004], lr: 0.103176, loss: 2.0004
2022-07-27 18:41:51 - train: epoch 0052, iter [02800, 05004], lr: 0.103110, loss: 1.7106
2022-07-27 18:43:38 - train: epoch 0052, iter [02900, 05004], lr: 0.103043, loss: 1.5814
2022-07-27 18:45:30 - train: epoch 0052, iter [03000, 05004], lr: 0.102977, loss: 1.6731
2022-07-27 18:47:24 - train: epoch 0052, iter [03100, 05004], lr: 0.102911, loss: 2.0470
2022-07-27 18:49:14 - train: epoch 0052, iter [03200, 05004], lr: 0.102845, loss: 1.6062
2022-07-27 18:51:14 - train: epoch 0052, iter [03300, 05004], lr: 0.102779, loss: 1.6903
2022-07-27 18:53:12 - train: epoch 0052, iter [03400, 05004], lr: 0.102713, loss: 1.9908
2022-07-27 18:55:12 - train: epoch 0052, iter [03500, 05004], lr: 0.102647, loss: 1.9706
2022-07-27 18:57:12 - train: epoch 0052, iter [03600, 05004], lr: 0.102581, loss: 1.9589
2022-07-27 18:59:15 - train: epoch 0052, iter [03700, 05004], lr: 0.102515, loss: 1.9171
2022-07-27 19:01:14 - train: epoch 0052, iter [03800, 05004], lr: 0.102449, loss: 1.6994
2022-07-27 19:03:15 - train: epoch 0052, iter [03900, 05004], lr: 0.102383, loss: 1.5953
2022-07-27 19:05:17 - train: epoch 0052, iter [04000, 05004], lr: 0.102317, loss: 1.9556
2022-07-27 19:07:16 - train: epoch 0052, iter [04100, 05004], lr: 0.102251, loss: 1.4711
2022-07-27 19:09:20 - train: epoch 0052, iter [04200, 05004], lr: 0.102185, loss: 1.8627
2022-07-27 19:11:20 - train: epoch 0052, iter [04300, 05004], lr: 0.102119, loss: 1.7777
2022-07-27 19:13:21 - train: epoch 0052, iter [04400, 05004], lr: 0.102052, loss: 1.8095
2022-07-27 19:15:25 - train: epoch 0052, iter [04500, 05004], lr: 0.101986, loss: 1.7916
2022-07-27 19:17:32 - train: epoch 0052, iter [04600, 05004], lr: 0.101920, loss: 1.6320
2022-07-27 19:19:29 - train: epoch 0052, iter [04700, 05004], lr: 0.101854, loss: 1.8412
2022-07-27 19:21:34 - train: epoch 0052, iter [04800, 05004], lr: 0.101788, loss: 1.5374
2022-07-27 19:23:36 - train: epoch 0052, iter [04900, 05004], lr: 0.101722, loss: 1.9207
2022-07-27 19:25:35 - train: epoch 0052, iter [05000, 05004], lr: 0.101656, loss: 1.7144
2022-07-27 19:25:38 - train: epoch 052, train_loss: 1.7682
2022-07-27 19:29:38 - eval: epoch: 052, acc1: 63.296%, acc5: 85.546%, test_loss: 1.5222, per_image_load_time: 7.814ms, per_image_inference_time: 0.852ms
2022-07-27 19:29:38 - until epoch: 052, best_acc1: 63.432%
2022-07-27 19:29:38 - epoch 053 lr: 0.101653
2022-07-27 19:31:57 - train: epoch 0053, iter [00100, 05004], lr: 0.101587, loss: 1.8573
2022-07-27 19:33:52 - train: epoch 0053, iter [00200, 05004], lr: 0.101521, loss: 1.7083
2022-07-27 19:35:47 - train: epoch 0053, iter [00300, 05004], lr: 0.101455, loss: 1.7449
2022-07-27 19:37:46 - train: epoch 0053, iter [00400, 05004], lr: 0.101389, loss: 1.7549
2022-07-27 19:39:50 - train: epoch 0053, iter [00500, 05004], lr: 0.101323, loss: 1.7451
2022-07-27 19:41:48 - train: epoch 0053, iter [00600, 05004], lr: 0.101257, loss: 1.4593
2022-07-27 19:43:52 - train: epoch 0053, iter [00700, 05004], lr: 0.101191, loss: 1.8910
2022-07-27 19:45:56 - train: epoch 0053, iter [00800, 05004], lr: 0.101125, loss: 1.7954
2022-07-27 19:48:01 - train: epoch 0053, iter [00900, 05004], lr: 0.101059, loss: 1.7636
2022-07-27 19:50:03 - train: epoch 0053, iter [01000, 05004], lr: 0.100993, loss: 1.6887
2022-07-27 19:52:06 - train: epoch 0053, iter [01100, 05004], lr: 0.100927, loss: 1.7975
2022-07-27 19:54:09 - train: epoch 0053, iter [01200, 05004], lr: 0.100860, loss: 1.3363
2022-07-27 19:56:17 - train: epoch 0053, iter [01300, 05004], lr: 0.100794, loss: 1.7762
2022-07-27 19:58:19 - train: epoch 0053, iter [01400, 05004], lr: 0.100728, loss: 1.9740
2022-07-27 20:00:22 - train: epoch 0053, iter [01500, 05004], lr: 0.100662, loss: 1.8174
2022-07-27 20:02:26 - train: epoch 0053, iter [01600, 05004], lr: 0.100596, loss: 1.9636
2022-07-27 20:04:25 - train: epoch 0053, iter [01700, 05004], lr: 0.100530, loss: 1.7671
2022-07-27 20:06:25 - train: epoch 0053, iter [01800, 05004], lr: 0.100464, loss: 1.7696
2022-07-27 20:08:29 - train: epoch 0053, iter [01900, 05004], lr: 0.100398, loss: 1.6708
2022-07-27 20:10:27 - train: epoch 0053, iter [02000, 05004], lr: 0.100332, loss: 1.8275
2022-07-27 20:12:13 - train: epoch 0053, iter [02100, 05004], lr: 0.100266, loss: 2.0241
2022-07-27 20:14:28 - train: epoch 0053, iter [02200, 05004], lr: 0.100200, loss: 1.5629
2022-07-27 20:16:27 - train: epoch 0053, iter [02300, 05004], lr: 0.100133, loss: 1.6964
2022-07-27 20:18:34 - train: epoch 0053, iter [02400, 05004], lr: 0.100067, loss: 1.6472
2022-07-27 20:20:38 - train: epoch 0053, iter [02500, 05004], lr: 0.100001, loss: 1.9772
2022-07-27 20:22:48 - train: epoch 0053, iter [02600, 05004], lr: 0.099935, loss: 2.0423
2022-07-27 20:24:52 - train: epoch 0053, iter [02700, 05004], lr: 0.099869, loss: 1.8623
2022-07-27 20:26:57 - train: epoch 0053, iter [02800, 05004], lr: 0.099803, loss: 1.8439
2022-07-27 20:29:00 - train: epoch 0053, iter [02900, 05004], lr: 0.099737, loss: 1.5837
2022-07-27 20:31:01 - train: epoch 0053, iter [03000, 05004], lr: 0.099671, loss: 1.6778
2022-07-27 20:33:01 - train: epoch 0053, iter [03100, 05004], lr: 0.099605, loss: 1.8508
2022-07-27 20:35:01 - train: epoch 0053, iter [03200, 05004], lr: 0.099539, loss: 1.7561
2022-07-27 20:37:06 - train: epoch 0053, iter [03300, 05004], lr: 0.099473, loss: 1.9352
2022-07-27 20:39:07 - train: epoch 0053, iter [03400, 05004], lr: 0.099407, loss: 1.5994
2022-07-27 20:41:09 - train: epoch 0053, iter [03500, 05004], lr: 0.099340, loss: 1.7168
2022-07-27 20:43:11 - train: epoch 0053, iter [03600, 05004], lr: 0.099274, loss: 1.8938
2022-07-27 20:45:14 - train: epoch 0053, iter [03700, 05004], lr: 0.099208, loss: 1.8149
2022-07-27 20:47:19 - train: epoch 0053, iter [03800, 05004], lr: 0.099142, loss: 1.4575
2022-07-27 20:49:22 - train: epoch 0053, iter [03900, 05004], lr: 0.099076, loss: 1.9980
2022-07-27 20:51:23 - train: epoch 0053, iter [04000, 05004], lr: 0.099010, loss: 1.9216
2022-07-27 20:53:29 - train: epoch 0053, iter [04100, 05004], lr: 0.098944, loss: 1.7836
2022-07-27 20:55:30 - train: epoch 0053, iter [04200, 05004], lr: 0.098878, loss: 2.0850
2022-07-27 20:57:29 - train: epoch 0053, iter [04300, 05004], lr: 0.098812, loss: 1.8831
2022-07-27 20:59:35 - train: epoch 0053, iter [04400, 05004], lr: 0.098746, loss: 1.7444
2022-07-27 21:01:37 - train: epoch 0053, iter [04500, 05004], lr: 0.098680, loss: 1.8719
2022-07-27 21:03:40 - train: epoch 0053, iter [04600, 05004], lr: 0.098614, loss: 1.4487
2022-07-27 21:05:39 - train: epoch 0053, iter [04700, 05004], lr: 0.098547, loss: 1.9074
2022-07-27 21:07:43 - train: epoch 0053, iter [04800, 05004], lr: 0.098481, loss: 2.1271
2022-07-27 21:09:45 - train: epoch 0053, iter [04900, 05004], lr: 0.098415, loss: 1.6225
2022-07-27 21:11:44 - train: epoch 0053, iter [05000, 05004], lr: 0.098349, loss: 1.6575
2022-07-27 21:11:50 - train: epoch 053, train_loss: 1.7520
2022-07-27 21:15:51 - eval: epoch: 053, acc1: 63.228%, acc5: 85.828%, test_loss: 1.5238, per_image_load_time: 7.428ms, per_image_inference_time: 0.898ms
2022-07-27 21:15:51 - until epoch: 053, best_acc1: 63.432%
2022-07-27 21:15:51 - epoch 054 lr: 0.098346
2022-07-27 21:18:14 - train: epoch 0054, iter [00100, 05004], lr: 0.098281, loss: 1.5817
2022-07-27 21:20:15 - train: epoch 0054, iter [00200, 05004], lr: 0.098214, loss: 1.6857
2022-07-27 21:22:21 - train: epoch 0054, iter [00300, 05004], lr: 0.098148, loss: 1.7421
2022-07-27 21:24:20 - train: epoch 0054, iter [00400, 05004], lr: 0.098082, loss: 1.6864
2022-07-27 21:26:25 - train: epoch 0054, iter [00500, 05004], lr: 0.098016, loss: 1.6870
2022-07-27 21:28:23 - train: epoch 0054, iter [00600, 05004], lr: 0.097950, loss: 1.6696
2022-07-27 21:30:30 - train: epoch 0054, iter [00700, 05004], lr: 0.097884, loss: 1.7756
2022-07-27 21:32:33 - train: epoch 0054, iter [00800, 05004], lr: 0.097818, loss: 1.7036
2022-07-27 21:34:34 - train: epoch 0054, iter [00900, 05004], lr: 0.097752, loss: 1.4536
2022-07-27 21:36:43 - train: epoch 0054, iter [01000, 05004], lr: 0.097686, loss: 1.4303
2022-07-27 21:38:43 - train: epoch 0054, iter [01100, 05004], lr: 0.097620, loss: 1.6380
2022-07-27 21:40:48 - train: epoch 0054, iter [01200, 05004], lr: 0.097554, loss: 1.9691
2022-07-27 21:42:50 - train: epoch 0054, iter [01300, 05004], lr: 0.097488, loss: 1.7970
2022-07-27 21:44:51 - train: epoch 0054, iter [01400, 05004], lr: 0.097422, loss: 1.7632
2022-07-27 21:46:50 - train: epoch 0054, iter [01500, 05004], lr: 0.097356, loss: 1.6950
2022-07-27 21:48:51 - train: epoch 0054, iter [01600, 05004], lr: 0.097289, loss: 1.5676
2022-07-27 21:50:52 - train: epoch 0054, iter [01700, 05004], lr: 0.097223, loss: 1.7400
2022-07-27 21:52:57 - train: epoch 0054, iter [01800, 05004], lr: 0.097157, loss: 1.7604
2022-07-27 21:55:00 - train: epoch 0054, iter [01900, 05004], lr: 0.097091, loss: 1.9274
2022-07-27 21:57:00 - train: epoch 0054, iter [02000, 05004], lr: 0.097025, loss: 1.7954
2022-07-27 21:59:02 - train: epoch 0054, iter [02100, 05004], lr: 0.096959, loss: 1.5016
2022-07-27 22:00:52 - train: epoch 0054, iter [02200, 05004], lr: 0.096893, loss: 1.7232
2022-07-27 22:02:51 - train: epoch 0054, iter [02300, 05004], lr: 0.096827, loss: 1.5444
2022-07-27 22:04:49 - train: epoch 0054, iter [02400, 05004], lr: 0.096761, loss: 1.7807
2022-07-27 22:06:56 - train: epoch 0054, iter [02500, 05004], lr: 0.096695, loss: 1.8108
2022-07-27 22:08:57 - train: epoch 0054, iter [02600, 05004], lr: 0.096629, loss: 1.7068
2022-07-27 22:11:01 - train: epoch 0054, iter [02700, 05004], lr: 0.096563, loss: 1.6647
2022-07-27 22:12:59 - train: epoch 0054, iter [02800, 05004], lr: 0.096497, loss: 1.9633
2022-07-27 22:14:59 - train: epoch 0054, iter [02900, 05004], lr: 0.096431, loss: 1.6083
2022-07-27 22:16:59 - train: epoch 0054, iter [03000, 05004], lr: 0.096365, loss: 1.8369
2022-07-27 22:19:03 - train: epoch 0054, iter [03100, 05004], lr: 0.096299, loss: 1.9144
2022-07-27 22:21:01 - train: epoch 0054, iter [03200, 05004], lr: 0.096233, loss: 1.8748
2022-07-27 22:23:00 - train: epoch 0054, iter [03300, 05004], lr: 0.096167, loss: 1.8715
2022-07-27 22:25:03 - train: epoch 0054, iter [03400, 05004], lr: 0.096101, loss: 1.6485
2022-07-27 22:27:01 - train: epoch 0054, iter [03500, 05004], lr: 0.096035, loss: 1.9438
2022-07-27 22:29:08 - train: epoch 0054, iter [03600, 05004], lr: 0.095969, loss: 1.8092
2022-07-27 22:31:05 - train: epoch 0054, iter [03700, 05004], lr: 0.095902, loss: 1.6364
2022-07-27 22:33:07 - train: epoch 0054, iter [03800, 05004], lr: 0.095836, loss: 1.4872
2022-07-27 22:35:04 - train: epoch 0054, iter [03900, 05004], lr: 0.095770, loss: 1.5414
2022-07-27 22:37:10 - train: epoch 0054, iter [04000, 05004], lr: 0.095704, loss: 1.7846
2022-07-27 22:39:11 - train: epoch 0054, iter [04100, 05004], lr: 0.095638, loss: 1.7706
2022-07-27 22:41:18 - train: epoch 0054, iter [04200, 05004], lr: 0.095572, loss: 1.8190
2022-07-27 22:43:15 - train: epoch 0054, iter [04300, 05004], lr: 0.095506, loss: 1.7500
2022-07-27 22:45:20 - train: epoch 0054, iter [04400, 05004], lr: 0.095440, loss: 1.6338
2022-07-27 22:47:24 - train: epoch 0054, iter [04500, 05004], lr: 0.095374, loss: 1.5417
2022-07-27 22:49:23 - train: epoch 0054, iter [04600, 05004], lr: 0.095308, loss: 1.7888
2022-07-27 22:51:31 - train: epoch 0054, iter [04700, 05004], lr: 0.095242, loss: 2.1299
2022-07-27 22:53:28 - train: epoch 0054, iter [04800, 05004], lr: 0.095176, loss: 1.6569
2022-07-27 22:55:32 - train: epoch 0054, iter [04900, 05004], lr: 0.095110, loss: 1.7111
2022-07-27 22:57:33 - train: epoch 0054, iter [05000, 05004], lr: 0.095044, loss: 1.6694
2022-07-27 22:57:37 - train: epoch 054, train_loss: 1.7406
2022-07-27 23:01:41 - eval: epoch: 054, acc1: 63.312%, acc5: 85.848%, test_loss: 1.5188, per_image_load_time: 8.135ms, per_image_inference_time: 0.871ms
2022-07-27 23:01:41 - until epoch: 054, best_acc1: 63.432%
2022-07-27 23:01:41 - epoch 055 lr: 0.095041
2022-07-27 23:04:07 - train: epoch 0055, iter [00100, 05004], lr: 0.094976, loss: 1.7428
2022-07-27 23:06:06 - train: epoch 0055, iter [00200, 05004], lr: 0.094910, loss: 1.5977
2022-07-27 23:08:04 - train: epoch 0055, iter [00300, 05004], lr: 0.094844, loss: 1.4930
2022-07-27 23:10:03 - train: epoch 0055, iter [00400, 05004], lr: 0.094778, loss: 1.5537
2022-07-27 23:12:05 - train: epoch 0055, iter [00500, 05004], lr: 0.094712, loss: 1.5774
2022-07-27 23:14:08 - train: epoch 0055, iter [00600, 05004], lr: 0.094646, loss: 1.6490
2022-07-27 23:16:15 - train: epoch 0055, iter [00700, 05004], lr: 0.094580, loss: 1.6295
2022-07-27 23:18:14 - train: epoch 0055, iter [00800, 05004], lr: 0.094514, loss: 1.6582
2022-07-27 23:20:16 - train: epoch 0055, iter [00900, 05004], lr: 0.094448, loss: 1.6387
2022-07-27 23:22:15 - train: epoch 0055, iter [01000, 05004], lr: 0.094382, loss: 1.8914
2022-07-27 23:24:17 - train: epoch 0055, iter [01100, 05004], lr: 0.094316, loss: 1.6806
2022-07-27 23:26:22 - train: epoch 0055, iter [01200, 05004], lr: 0.094250, loss: 1.6649
2022-07-27 23:28:33 - train: epoch 0055, iter [01300, 05004], lr: 0.094184, loss: 1.8884
2022-07-27 23:30:36 - train: epoch 0055, iter [01400, 05004], lr: 0.094118, loss: 1.7921
2022-07-27 23:32:39 - train: epoch 0055, iter [01500, 05004], lr: 0.094052, loss: 1.5670
2022-07-27 23:34:46 - train: epoch 0055, iter [01600, 05004], lr: 0.093986, loss: 1.8830
2022-07-27 23:36:48 - train: epoch 0055, iter [01700, 05004], lr: 0.093920, loss: 1.7886
2022-07-27 23:38:52 - train: epoch 0055, iter [01800, 05004], lr: 0.093854, loss: 1.7798
2022-07-27 23:40:51 - train: epoch 0055, iter [01900, 05004], lr: 0.093788, loss: 1.8219
2022-07-27 23:43:01 - train: epoch 0055, iter [02000, 05004], lr: 0.093722, loss: 1.6360
2022-07-27 23:45:00 - train: epoch 0055, iter [02100, 05004], lr: 0.093656, loss: 1.6536
2022-07-27 23:47:05 - train: epoch 0055, iter [02200, 05004], lr: 0.093590, loss: 1.8360
2022-07-27 23:48:59 - train: epoch 0055, iter [02300, 05004], lr: 0.093524, loss: 1.7599
2022-07-27 23:51:20 - train: epoch 0055, iter [02400, 05004], lr: 0.093458, loss: 1.5109
2022-07-27 23:53:15 - train: epoch 0055, iter [02500, 05004], lr: 0.093392, loss: 1.6434
2022-07-27 23:55:19 - train: epoch 0055, iter [02600, 05004], lr: 0.093326, loss: 1.4900
2022-07-27 23:57:24 - train: epoch 0055, iter [02700, 05004], lr: 0.093260, loss: 1.4718
2022-07-27 23:59:26 - train: epoch 0055, iter [02800, 05004], lr: 0.093194, loss: 1.8248
2022-07-28 00:01:26 - train: epoch 0055, iter [02900, 05004], lr: 0.093129, loss: 1.7609
2022-07-28 00:03:23 - train: epoch 0055, iter [03000, 05004], lr: 0.093063, loss: 1.7909
2022-07-28 00:05:24 - train: epoch 0055, iter [03100, 05004], lr: 0.092997, loss: 1.8271
2022-07-28 00:07:28 - train: epoch 0055, iter [03200, 05004], lr: 0.092931, loss: 1.6379
2022-07-28 00:09:26 - train: epoch 0055, iter [03300, 05004], lr: 0.092865, loss: 1.4746
2022-07-28 00:11:29 - train: epoch 0055, iter [03400, 05004], lr: 0.092799, loss: 1.8231
2022-07-28 00:13:23 - train: epoch 0055, iter [03500, 05004], lr: 0.092733, loss: 1.6559
2022-07-28 00:15:27 - train: epoch 0055, iter [03600, 05004], lr: 0.092667, loss: 1.8289
2022-07-28 00:17:31 - train: epoch 0055, iter [03700, 05004], lr: 0.092601, loss: 1.7089
2022-07-28 00:19:33 - train: epoch 0055, iter [03800, 05004], lr: 0.092535, loss: 1.7222
2022-07-28 00:21:33 - train: epoch 0055, iter [03900, 05004], lr: 0.092469, loss: 2.0805
2022-07-28 00:23:36 - train: epoch 0055, iter [04000, 05004], lr: 0.092403, loss: 1.5240
2022-07-28 00:25:38 - train: epoch 0055, iter [04100, 05004], lr: 0.092338, loss: 1.6711
2022-07-28 00:27:40 - train: epoch 0055, iter [04200, 05004], lr: 0.092272, loss: 1.8306
2022-07-28 00:29:38 - train: epoch 0055, iter [04300, 05004], lr: 0.092206, loss: 1.7653
2022-07-28 00:31:46 - train: epoch 0055, iter [04400, 05004], lr: 0.092140, loss: 1.7733
2022-07-28 00:33:46 - train: epoch 0055, iter [04500, 05004], lr: 0.092074, loss: 1.7762
2022-07-28 00:35:43 - train: epoch 0055, iter [04600, 05004], lr: 0.092008, loss: 1.8083
2022-07-28 00:37:41 - train: epoch 0055, iter [04700, 05004], lr: 0.091942, loss: 1.9761
2022-07-28 00:39:44 - train: epoch 0055, iter [04800, 05004], lr: 0.091876, loss: 1.8315
2022-07-28 00:41:45 - train: epoch 0055, iter [04900, 05004], lr: 0.091811, loss: 1.7593
2022-07-28 00:43:41 - train: epoch 0055, iter [05000, 05004], lr: 0.091745, loss: 1.8256
2022-07-28 00:43:45 - train: epoch 055, train_loss: 1.7261
2022-07-28 00:47:46 - eval: epoch: 055, acc1: 63.938%, acc5: 86.218%, test_loss: 1.4761, per_image_load_time: 7.007ms, per_image_inference_time: 0.899ms
2022-07-28 00:47:47 - until epoch: 055, best_acc1: 63.938%
2022-07-28 00:47:47 - epoch 056 lr: 0.091741
2022-07-28 00:50:08 - train: epoch 0056, iter [00100, 05004], lr: 0.091676, loss: 1.5696
2022-07-28 00:52:06 - train: epoch 0056, iter [00200, 05004], lr: 0.091610, loss: 1.6659
2022-07-28 00:54:10 - train: epoch 0056, iter [00300, 05004], lr: 0.091545, loss: 1.5097
2022-07-28 00:56:14 - train: epoch 0056, iter [00400, 05004], lr: 0.091479, loss: 1.6651
2022-07-28 00:58:17 - train: epoch 0056, iter [00500, 05004], lr: 0.091413, loss: 1.6635
2022-07-28 01:00:14 - train: epoch 0056, iter [00600, 05004], lr: 0.091347, loss: 1.7319
2022-07-28 01:02:15 - train: epoch 0056, iter [00700, 05004], lr: 0.091281, loss: 1.7395
2022-07-28 01:04:19 - train: epoch 0056, iter [00800, 05004], lr: 0.091215, loss: 1.7668
2022-07-28 01:06:18 - train: epoch 0056, iter [00900, 05004], lr: 0.091149, loss: 1.7951
2022-07-28 01:08:18 - train: epoch 0056, iter [01000, 05004], lr: 0.091084, loss: 1.7578
2022-07-28 01:10:17 - train: epoch 0056, iter [01100, 05004], lr: 0.091018, loss: 1.5201
2022-07-28 01:12:17 - train: epoch 0056, iter [01200, 05004], lr: 0.090952, loss: 1.5219
2022-07-28 01:14:12 - train: epoch 0056, iter [01300, 05004], lr: 0.090886, loss: 1.8189
2022-07-28 01:16:12 - train: epoch 0056, iter [01400, 05004], lr: 0.090820, loss: 1.6943
2022-07-28 01:18:20 - train: epoch 0056, iter [01500, 05004], lr: 0.090755, loss: 1.8902
2022-07-28 01:20:21 - train: epoch 0056, iter [01600, 05004], lr: 0.090689, loss: 1.5340
2022-07-28 01:22:25 - train: epoch 0056, iter [01700, 05004], lr: 0.090623, loss: 1.7099
2022-07-28 01:24:25 - train: epoch 0056, iter [01800, 05004], lr: 0.090557, loss: 1.8878
2022-07-28 01:26:31 - train: epoch 0056, iter [01900, 05004], lr: 0.090491, loss: 1.8512
2022-07-28 01:28:29 - train: epoch 0056, iter [02000, 05004], lr: 0.090426, loss: 1.7085
2022-07-28 01:30:33 - train: epoch 0056, iter [02100, 05004], lr: 0.090360, loss: 1.6894
2022-07-28 01:32:33 - train: epoch 0056, iter [02200, 05004], lr: 0.090294, loss: 1.8715
2022-07-28 01:34:29 - train: epoch 0056, iter [02300, 05004], lr: 0.090228, loss: 1.7301
2022-07-28 01:36:16 - train: epoch 0056, iter [02400, 05004], lr: 0.090163, loss: 1.7523
2022-07-28 01:38:40 - train: epoch 0056, iter [02500, 05004], lr: 0.090097, loss: 2.0362
2022-07-28 01:40:37 - train: epoch 0056, iter [02600, 05004], lr: 0.090031, loss: 1.8196
2022-07-28 01:42:47 - train: epoch 0056, iter [02700, 05004], lr: 0.089965, loss: 1.6654
2022-07-28 01:44:49 - train: epoch 0056, iter [02800, 05004], lr: 0.089899, loss: 1.5805
2022-07-28 01:46:53 - train: epoch 0056, iter [02900, 05004], lr: 0.089834, loss: 1.7689
2022-07-28 01:48:57 - train: epoch 0056, iter [03000, 05004], lr: 0.089768, loss: 1.8171
2022-07-28 01:51:00 - train: epoch 0056, iter [03100, 05004], lr: 0.089702, loss: 1.7479
2022-07-28 01:53:01 - train: epoch 0056, iter [03200, 05004], lr: 0.089637, loss: 1.6692
2022-07-28 01:55:06 - train: epoch 0056, iter [03300, 05004], lr: 0.089571, loss: 1.7951
2022-07-28 01:57:10 - train: epoch 0056, iter [03400, 05004], lr: 0.089505, loss: 1.4372
2022-07-28 01:59:09 - train: epoch 0056, iter [03500, 05004], lr: 0.089439, loss: 1.6683
2022-07-28 02:01:14 - train: epoch 0056, iter [03600, 05004], lr: 0.089374, loss: 1.4439
2022-07-28 02:03:18 - train: epoch 0056, iter [03700, 05004], lr: 0.089308, loss: 1.7807
2022-07-28 02:05:19 - train: epoch 0056, iter [03800, 05004], lr: 0.089242, loss: 1.6680
2022-07-28 02:07:26 - train: epoch 0056, iter [03900, 05004], lr: 0.089177, loss: 1.7890
2022-07-28 02:09:26 - train: epoch 0056, iter [04000, 05004], lr: 0.089111, loss: 1.8390
2022-07-28 02:11:27 - train: epoch 0056, iter [04100, 05004], lr: 0.089045, loss: 1.7550
2022-07-28 02:13:31 - train: epoch 0056, iter [04200, 05004], lr: 0.088979, loss: 1.6452
2022-07-28 02:15:34 - train: epoch 0056, iter [04300, 05004], lr: 0.088914, loss: 1.6822
2022-07-28 02:17:38 - train: epoch 0056, iter [04400, 05004], lr: 0.088848, loss: 1.7737
2022-07-28 02:19:41 - train: epoch 0056, iter [04500, 05004], lr: 0.088782, loss: 1.8451
2022-07-28 02:21:44 - train: epoch 0056, iter [04600, 05004], lr: 0.088717, loss: 1.8360
2022-07-28 02:23:48 - train: epoch 0056, iter [04700, 05004], lr: 0.088651, loss: 1.6832
2022-07-28 02:25:47 - train: epoch 0056, iter [04800, 05004], lr: 0.088585, loss: 1.9014
2022-07-28 02:27:49 - train: epoch 0056, iter [04900, 05004], lr: 0.088520, loss: 1.7076
2022-07-28 02:29:52 - train: epoch 0056, iter [05000, 05004], lr: 0.088454, loss: 1.7146
2022-07-28 02:29:55 - train: epoch 056, train_loss: 1.7142
2022-07-28 02:33:56 - eval: epoch: 056, acc1: 64.126%, acc5: 86.452%, test_loss: 1.4708, per_image_load_time: 6.565ms, per_image_inference_time: 0.864ms
2022-07-28 02:33:56 - until epoch: 056, best_acc1: 64.126%
2022-07-28 02:33:56 - epoch 057 lr: 0.088451
2022-07-28 02:36:18 - train: epoch 0057, iter [00100, 05004], lr: 0.088386, loss: 1.5321
2022-07-28 02:38:16 - train: epoch 0057, iter [00200, 05004], lr: 0.088320, loss: 1.6048
2022-07-28 02:40:18 - train: epoch 0057, iter [00300, 05004], lr: 0.088255, loss: 1.8156
2022-07-28 02:42:19 - train: epoch 0057, iter [00400, 05004], lr: 0.088189, loss: 1.6799
2022-07-28 02:44:26 - train: epoch 0057, iter [00500, 05004], lr: 0.088123, loss: 1.5450
2022-07-28 02:46:30 - train: epoch 0057, iter [00600, 05004], lr: 0.088058, loss: 1.7311
2022-07-28 02:48:32 - train: epoch 0057, iter [00700, 05004], lr: 0.087992, loss: 1.5168
2022-07-28 02:50:32 - train: epoch 0057, iter [00800, 05004], lr: 0.087927, loss: 1.6896
2022-07-28 02:52:32 - train: epoch 0057, iter [00900, 05004], lr: 0.087861, loss: 1.6627
2022-07-28 02:54:40 - train: epoch 0057, iter [01000, 05004], lr: 0.087795, loss: 1.4231
2022-07-28 02:56:43 - train: epoch 0057, iter [01100, 05004], lr: 0.087730, loss: 1.6083
2022-07-28 02:58:46 - train: epoch 0057, iter [01200, 05004], lr: 0.087664, loss: 1.5203
2022-07-28 03:00:43 - train: epoch 0057, iter [01300, 05004], lr: 0.087599, loss: 2.0519
2022-07-28 03:02:45 - train: epoch 0057, iter [01400, 05004], lr: 0.087533, loss: 1.7487
2022-07-28 03:04:47 - train: epoch 0057, iter [01500, 05004], lr: 0.087467, loss: 1.8771
2022-07-28 03:06:46 - train: epoch 0057, iter [01600, 05004], lr: 0.087402, loss: 1.6838
2022-07-28 03:08:46 - train: epoch 0057, iter [01700, 05004], lr: 0.087336, loss: 1.7378
2022-07-28 03:10:54 - train: epoch 0057, iter [01800, 05004], lr: 0.087271, loss: 1.8001
2022-07-28 03:13:01 - train: epoch 0057, iter [01900, 05004], lr: 0.087205, loss: 1.8362
2022-07-28 03:15:02 - train: epoch 0057, iter [02000, 05004], lr: 0.087140, loss: 1.7300
2022-07-28 03:17:04 - train: epoch 0057, iter [02100, 05004], lr: 0.087074, loss: 1.6699
2022-07-28 03:19:12 - train: epoch 0057, iter [02200, 05004], lr: 0.087009, loss: 1.6861
2022-07-28 03:21:16 - train: epoch 0057, iter [02300, 05004], lr: 0.086943, loss: 1.7118
2022-07-28 03:23:15 - train: epoch 0057, iter [02400, 05004], lr: 0.086878, loss: 1.7194
2022-07-28 03:25:26 - train: epoch 0057, iter [02500, 05004], lr: 0.086812, loss: 2.1002
2022-07-28 03:27:29 - train: epoch 0057, iter [02600, 05004], lr: 0.086747, loss: 1.5334
2022-07-28 03:29:34 - train: epoch 0057, iter [02700, 05004], lr: 0.086681, loss: 1.4821
2022-07-28 03:31:37 - train: epoch 0057, iter [02800, 05004], lr: 0.086616, loss: 1.5973
2022-07-28 03:33:41 - train: epoch 0057, iter [02900, 05004], lr: 0.086550, loss: 1.6701
2022-07-28 03:35:42 - train: epoch 0057, iter [03000, 05004], lr: 0.086485, loss: 1.8750
2022-07-28 03:37:49 - train: epoch 0057, iter [03100, 05004], lr: 0.086419, loss: 1.9666
2022-07-28 03:39:42 - train: epoch 0057, iter [03200, 05004], lr: 0.086354, loss: 1.6792
2022-07-28 03:41:43 - train: epoch 0057, iter [03300, 05004], lr: 0.086288, loss: 1.6310
2022-07-28 03:43:44 - train: epoch 0057, iter [03400, 05004], lr: 0.086223, loss: 1.8037
2022-07-28 03:45:45 - train: epoch 0057, iter [03500, 05004], lr: 0.086157, loss: 1.9484
2022-07-28 03:47:48 - train: epoch 0057, iter [03600, 05004], lr: 0.086092, loss: 1.7315
2022-07-28 03:49:49 - train: epoch 0057, iter [03700, 05004], lr: 0.086026, loss: 1.8480
2022-07-28 03:51:51 - train: epoch 0057, iter [03800, 05004], lr: 0.085961, loss: 1.5397
2022-07-28 03:53:56 - train: epoch 0057, iter [03900, 05004], lr: 0.085896, loss: 1.8697
2022-07-28 03:55:55 - train: epoch 0057, iter [04000, 05004], lr: 0.085830, loss: 1.6266
2022-07-28 03:57:56 - train: epoch 0057, iter [04100, 05004], lr: 0.085765, loss: 1.7640
2022-07-28 03:59:57 - train: epoch 0057, iter [04200, 05004], lr: 0.085699, loss: 1.7132
2022-07-28 04:01:57 - train: epoch 0057, iter [04300, 05004], lr: 0.085634, loss: 1.4433
2022-07-28 04:04:04 - train: epoch 0057, iter [04400, 05004], lr: 0.085568, loss: 1.7376
2022-07-28 04:06:06 - train: epoch 0057, iter [04500, 05004], lr: 0.085503, loss: 1.6698
2022-07-28 04:08:10 - train: epoch 0057, iter [04600, 05004], lr: 0.085438, loss: 1.7549
2022-07-28 04:10:13 - train: epoch 0057, iter [04700, 05004], lr: 0.085372, loss: 1.6006
2022-07-28 04:12:16 - train: epoch 0057, iter [04800, 05004], lr: 0.085307, loss: 1.8701
2022-07-28 04:14:19 - train: epoch 0057, iter [04900, 05004], lr: 0.085242, loss: 1.7584
2022-07-28 04:16:18 - train: epoch 0057, iter [05000, 05004], lr: 0.085176, loss: 1.5832
2022-07-28 04:16:22 - train: epoch 057, train_loss: 1.6987
2022-07-28 04:20:23 - eval: epoch: 057, acc1: 64.228%, acc5: 86.286%, test_loss: 1.4749, per_image_load_time: 8.384ms, per_image_inference_time: 0.809ms
2022-07-28 04:20:23 - until epoch: 057, best_acc1: 64.228%
2022-07-28 04:20:23 - epoch 058 lr: 0.085173
2022-07-28 04:22:41 - train: epoch 0058, iter [00100, 05004], lr: 0.085108, loss: 1.7892
2022-07-28 04:24:48 - train: epoch 0058, iter [00200, 05004], lr: 0.085043, loss: 1.7295
2022-07-28 04:26:47 - train: epoch 0058, iter [00300, 05004], lr: 0.084978, loss: 1.6622
2022-07-28 04:28:48 - train: epoch 0058, iter [00400, 05004], lr: 0.084912, loss: 1.6599
2022-07-28 04:30:55 - train: epoch 0058, iter [00500, 05004], lr: 0.084847, loss: 1.5440
2022-07-28 04:32:58 - train: epoch 0058, iter [00600, 05004], lr: 0.084782, loss: 1.8424
2022-07-28 04:35:00 - train: epoch 0058, iter [00700, 05004], lr: 0.084716, loss: 1.6479
2022-07-28 04:36:58 - train: epoch 0058, iter [00800, 05004], lr: 0.084651, loss: 1.6388
2022-07-28 04:39:00 - train: epoch 0058, iter [00900, 05004], lr: 0.084586, loss: 1.4044
2022-07-28 04:41:05 - train: epoch 0058, iter [01000, 05004], lr: 0.084520, loss: 1.7387
2022-07-28 04:43:12 - train: epoch 0058, iter [01100, 05004], lr: 0.084455, loss: 1.5017
2022-07-28 04:45:14 - train: epoch 0058, iter [01200, 05004], lr: 0.084390, loss: 1.5889
2022-07-28 04:47:20 - train: epoch 0058, iter [01300, 05004], lr: 0.084325, loss: 1.9795
2022-07-28 04:49:23 - train: epoch 0058, iter [01400, 05004], lr: 0.084259, loss: 1.6870
2022-07-28 04:51:25 - train: epoch 0058, iter [01500, 05004], lr: 0.084194, loss: 1.5479
2022-07-28 04:53:21 - train: epoch 0058, iter [01600, 05004], lr: 0.084129, loss: 1.5465
2022-07-28 04:55:30 - train: epoch 0058, iter [01700, 05004], lr: 0.084064, loss: 1.7191
2022-07-28 04:57:28 - train: epoch 0058, iter [01800, 05004], lr: 0.083998, loss: 1.9090
2022-07-28 04:59:30 - train: epoch 0058, iter [01900, 05004], lr: 0.083933, loss: 1.9278
2022-07-28 05:01:33 - train: epoch 0058, iter [02000, 05004], lr: 0.083868, loss: 1.8160
2022-07-28 05:03:35 - train: epoch 0058, iter [02100, 05004], lr: 0.083803, loss: 1.6199
2022-07-28 05:05:39 - train: epoch 0058, iter [02200, 05004], lr: 0.083737, loss: 1.7253
2022-07-28 05:07:47 - train: epoch 0058, iter [02300, 05004], lr: 0.083672, loss: 1.5856
2022-07-28 05:09:55 - train: epoch 0058, iter [02400, 05004], lr: 0.083607, loss: 1.8873
2022-07-28 05:11:58 - train: epoch 0058, iter [02500, 05004], lr: 0.083542, loss: 1.8131
2022-07-28 05:14:17 - train: epoch 0058, iter [02600, 05004], lr: 0.083477, loss: 1.6753
2022-07-28 05:16:11 - train: epoch 0058, iter [02700, 05004], lr: 0.083411, loss: 1.8467
2022-07-28 05:18:21 - train: epoch 0058, iter [02800, 05004], lr: 0.083346, loss: 1.5278
2022-07-28 05:20:23 - train: epoch 0058, iter [02900, 05004], lr: 0.083281, loss: 1.4987
2022-07-28 05:22:24 - train: epoch 0058, iter [03000, 05004], lr: 0.083216, loss: 1.7618
2022-07-28 05:24:30 - train: epoch 0058, iter [03100, 05004], lr: 0.083151, loss: 1.7084
2022-07-28 05:26:36 - train: epoch 0058, iter [03200, 05004], lr: 0.083086, loss: 1.5009
2022-07-28 05:28:37 - train: epoch 0058, iter [03300, 05004], lr: 0.083021, loss: 1.5898
2022-07-28 05:30:38 - train: epoch 0058, iter [03400, 05004], lr: 0.082955, loss: 1.7520
2022-07-28 05:32:43 - train: epoch 0058, iter [03500, 05004], lr: 0.082890, loss: 1.8041
2022-07-28 05:34:41 - train: epoch 0058, iter [03600, 05004], lr: 0.082825, loss: 1.7170
2022-07-28 05:36:46 - train: epoch 0058, iter [03700, 05004], lr: 0.082760, loss: 1.6581
2022-07-28 05:38:48 - train: epoch 0058, iter [03800, 05004], lr: 0.082695, loss: 1.7330
2022-07-28 05:40:52 - train: epoch 0058, iter [03900, 05004], lr: 0.082630, loss: 1.6045
2022-07-28 05:42:58 - train: epoch 0058, iter [04000, 05004], lr: 0.082565, loss: 1.4959
2022-07-28 05:45:02 - train: epoch 0058, iter [04100, 05004], lr: 0.082500, loss: 1.5610
2022-07-28 05:46:56 - train: epoch 0058, iter [04200, 05004], lr: 0.082435, loss: 1.7632
2022-07-28 05:49:02 - train: epoch 0058, iter [04300, 05004], lr: 0.082370, loss: 1.8636
2022-07-28 05:51:07 - train: epoch 0058, iter [04400, 05004], lr: 0.082305, loss: 1.4616
2022-07-28 05:53:09 - train: epoch 0058, iter [04500, 05004], lr: 0.082240, loss: 1.5618
2022-07-28 05:55:13 - train: epoch 0058, iter [04600, 05004], lr: 0.082175, loss: 1.6292
2022-07-28 05:57:18 - train: epoch 0058, iter [04700, 05004], lr: 0.082110, loss: 1.6357
2022-07-28 05:59:26 - train: epoch 0058, iter [04800, 05004], lr: 0.082045, loss: 1.5033
2022-07-28 06:01:24 - train: epoch 0058, iter [04900, 05004], lr: 0.081980, loss: 1.6353
2022-07-28 06:03:25 - train: epoch 0058, iter [05000, 05004], lr: 0.081915, loss: 1.4495
2022-07-28 06:03:28 - train: epoch 058, train_loss: 1.6839
2022-07-28 06:07:40 - eval: epoch: 058, acc1: 64.810%, acc5: 86.668%, test_loss: 1.4496, per_image_load_time: 8.822ms, per_image_inference_time: 0.808ms
2022-07-28 06:07:40 - until epoch: 058, best_acc1: 64.810%
2022-07-28 06:07:40 - epoch 059 lr: 0.081911
2022-07-28 06:10:06 - train: epoch 0059, iter [00100, 05004], lr: 0.081847, loss: 1.4566
2022-07-28 06:12:10 - train: epoch 0059, iter [00200, 05004], lr: 0.081782, loss: 1.5149
2022-07-28 06:14:14 - train: epoch 0059, iter [00300, 05004], lr: 0.081717, loss: 1.5293
2022-07-28 06:16:23 - train: epoch 0059, iter [00400, 05004], lr: 0.081652, loss: 1.7497
2022-07-28 06:18:25 - train: epoch 0059, iter [00500, 05004], lr: 0.081587, loss: 1.7962
2022-07-28 06:20:26 - train: epoch 0059, iter [00600, 05004], lr: 0.081522, loss: 1.5400
2022-07-28 06:22:28 - train: epoch 0059, iter [00700, 05004], lr: 0.081457, loss: 1.5210
2022-07-28 06:24:33 - train: epoch 0059, iter [00800, 05004], lr: 0.081392, loss: 1.8229
2022-07-28 06:26:32 - train: epoch 0059, iter [00900, 05004], lr: 0.081327, loss: 1.7010
2022-07-28 06:28:34 - train: epoch 0059, iter [01000, 05004], lr: 0.081262, loss: 1.8401
2022-07-28 06:30:47 - train: epoch 0059, iter [01100, 05004], lr: 0.081197, loss: 1.9881
2022-07-28 06:32:55 - train: epoch 0059, iter [01200, 05004], lr: 0.081133, loss: 1.3432
2022-07-28 06:35:04 - train: epoch 0059, iter [01300, 05004], lr: 0.081068, loss: 1.9210
2022-07-28 06:37:15 - train: epoch 0059, iter [01400, 05004], lr: 0.081003, loss: 1.8186
2022-07-28 06:39:23 - train: epoch 0059, iter [01500, 05004], lr: 0.080938, loss: 1.6450
2022-07-28 06:41:33 - train: epoch 0059, iter [01600, 05004], lr: 0.080873, loss: 1.6460
2022-07-28 06:43:40 - train: epoch 0059, iter [01700, 05004], lr: 0.080808, loss: 1.7755
2022-07-28 06:45:49 - train: epoch 0059, iter [01800, 05004], lr: 0.080743, loss: 1.5957
2022-07-28 06:47:57 - train: epoch 0059, iter [01900, 05004], lr: 0.080678, loss: 1.6282
2022-07-28 06:50:11 - train: epoch 0059, iter [02000, 05004], lr: 0.080614, loss: 1.5172
2022-07-28 06:52:20 - train: epoch 0059, iter [02100, 05004], lr: 0.080549, loss: 1.8047
2022-07-28 06:54:25 - train: epoch 0059, iter [02200, 05004], lr: 0.080484, loss: 1.6388
2022-07-28 06:56:36 - train: epoch 0059, iter [02300, 05004], lr: 0.080419, loss: 1.4216
2022-07-28 06:58:45 - train: epoch 0059, iter [02400, 05004], lr: 0.080354, loss: 1.8890
2022-07-28 07:00:44 - train: epoch 0059, iter [02500, 05004], lr: 0.080290, loss: 1.9625
2022-07-28 07:02:49 - train: epoch 0059, iter [02600, 05004], lr: 0.080225, loss: 1.4997
2022-07-28 07:04:49 - train: epoch 0059, iter [02700, 05004], lr: 0.080160, loss: 1.6448
2022-07-28 07:07:01 - train: epoch 0059, iter [02800, 05004], lr: 0.080095, loss: 1.6675
2022-07-28 07:09:06 - train: epoch 0059, iter [02900, 05004], lr: 0.080031, loss: 1.6564
2022-07-28 07:11:18 - train: epoch 0059, iter [03000, 05004], lr: 0.079966, loss: 1.8787
