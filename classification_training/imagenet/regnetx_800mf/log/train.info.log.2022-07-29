2022-07-29 07:15:03 - train: epoch 0073, iter [04600, 05004], lr: 0.037491, loss: 1.5871
2022-07-29 07:16:56 - train: epoch 0073, iter [04700, 05004], lr: 0.037439, loss: 1.3227
2022-07-29 07:18:48 - train: epoch 0073, iter [04800, 05004], lr: 0.037387, loss: 1.4056
2022-07-29 07:20:43 - train: epoch 0073, iter [04900, 05004], lr: 0.037336, loss: 1.3927
2022-07-29 07:22:39 - train: epoch 0073, iter [05000, 05004], lr: 0.037284, loss: 1.4643
2022-07-29 07:22:43 - train: epoch 073, train_loss: 1.4225
2022-07-29 07:26:39 - eval: epoch: 073, acc1: 68.778%, acc5: 88.904%, test_loss: 1.2777, per_image_load_time: 1.987ms, per_image_inference_time: 0.780ms
2022-07-29 07:26:39 - until epoch: 073, best_acc1: 68.778%
2022-07-29 07:26:39 - epoch 074 lr: 0.037282
2022-07-29 07:28:50 - train: epoch 0074, iter [00100, 05004], lr: 0.037231, loss: 1.2756
2022-07-29 07:30:42 - train: epoch 0074, iter [00200, 05004], lr: 0.037179, loss: 1.1900
2022-07-29 07:32:32 - train: epoch 0074, iter [00300, 05004], lr: 0.037128, loss: 1.3799
2022-07-29 07:34:22 - train: epoch 0074, iter [00400, 05004], lr: 0.037077, loss: 1.5100
2022-07-29 07:36:20 - train: epoch 0074, iter [00500, 05004], lr: 0.037025, loss: 1.2746
2022-07-29 07:38:07 - train: epoch 0074, iter [00600, 05004], lr: 0.036974, loss: 1.4177
2022-07-29 07:39:59 - train: epoch 0074, iter [00700, 05004], lr: 0.036923, loss: 1.3255
2022-07-29 07:41:50 - train: epoch 0074, iter [00800, 05004], lr: 0.036871, loss: 1.4609
2022-07-29 07:43:47 - train: epoch 0074, iter [00900, 05004], lr: 0.036820, loss: 1.3119
2022-07-29 07:45:34 - train: epoch 0074, iter [01000, 05004], lr: 0.036769, loss: 1.5199
2022-07-29 07:47:30 - train: epoch 0074, iter [01100, 05004], lr: 0.036718, loss: 1.4675
2022-07-29 07:49:27 - train: epoch 0074, iter [01200, 05004], lr: 0.036667, loss: 1.4423
2022-07-29 07:51:16 - train: epoch 0074, iter [01300, 05004], lr: 0.036616, loss: 1.3673
2022-07-29 07:53:13 - train: epoch 0074, iter [01400, 05004], lr: 0.036564, loss: 1.4472
2022-07-29 07:55:04 - train: epoch 0074, iter [01500, 05004], lr: 0.036513, loss: 1.5274
2022-07-29 07:56:55 - train: epoch 0074, iter [01600, 05004], lr: 0.036462, loss: 1.3516
2022-07-29 07:58:53 - train: epoch 0074, iter [01700, 05004], lr: 0.036411, loss: 1.3867
2022-07-29 08:00:48 - train: epoch 0074, iter [01800, 05004], lr: 0.036360, loss: 1.3104
2022-07-29 08:02:42 - train: epoch 0074, iter [01900, 05004], lr: 0.036309, loss: 1.2648
2022-07-29 08:04:39 - train: epoch 0074, iter [02000, 05004], lr: 0.036258, loss: 1.1923
2022-07-29 08:06:35 - train: epoch 0074, iter [02100, 05004], lr: 0.036208, loss: 1.4465
2022-07-29 08:08:26 - train: epoch 0074, iter [02200, 05004], lr: 0.036157, loss: 1.6021
2022-07-29 08:10:22 - train: epoch 0074, iter [02300, 05004], lr: 0.036106, loss: 1.5251
2022-07-29 08:12:15 - train: epoch 0074, iter [02400, 05004], lr: 0.036055, loss: 1.2514
2022-07-29 08:14:10 - train: epoch 0074, iter [02500, 05004], lr: 0.036004, loss: 1.2390
2022-07-29 08:16:01 - train: epoch 0074, iter [02600, 05004], lr: 0.035953, loss: 1.2603
2022-07-29 08:17:48 - train: epoch 0074, iter [02700, 05004], lr: 0.035903, loss: 1.3144
2022-07-29 08:19:34 - train: epoch 0074, iter [02800, 05004], lr: 0.035852, loss: 1.5415
2022-07-29 08:21:24 - train: epoch 0074, iter [02900, 05004], lr: 0.035801, loss: 1.4331
2022-07-29 08:23:17 - train: epoch 0074, iter [03000, 05004], lr: 0.035751, loss: 1.4157
2022-07-29 08:25:11 - train: epoch 0074, iter [03100, 05004], lr: 0.035700, loss: 1.3983
2022-07-29 08:27:04 - train: epoch 0074, iter [03200, 05004], lr: 0.035649, loss: 1.3189
2022-07-29 08:28:56 - train: epoch 0074, iter [03300, 05004], lr: 0.035599, loss: 1.4304
2022-07-29 08:30:56 - train: epoch 0074, iter [03400, 05004], lr: 0.035548, loss: 1.4534
2022-07-29 08:32:53 - train: epoch 0074, iter [03500, 05004], lr: 0.035498, loss: 1.1878
2022-07-29 08:34:53 - train: epoch 0074, iter [03600, 05004], lr: 0.035447, loss: 1.6012
2022-07-29 08:36:49 - train: epoch 0074, iter [03700, 05004], lr: 0.035397, loss: 1.5601
2022-07-29 08:38:53 - train: epoch 0074, iter [03800, 05004], lr: 0.035346, loss: 1.3262
2022-07-29 08:40:53 - train: epoch 0074, iter [03900, 05004], lr: 0.035296, loss: 1.2498
2022-07-29 08:42:45 - train: epoch 0074, iter [04000, 05004], lr: 0.035246, loss: 1.5111
2022-07-29 08:45:06 - train: epoch 0074, iter [04100, 05004], lr: 0.035195, loss: 1.4579
2022-07-29 08:47:02 - train: epoch 0074, iter [04200, 05004], lr: 0.035145, loss: 1.6591
2022-07-29 08:49:18 - train: epoch 0074, iter [04300, 05004], lr: 0.035095, loss: 1.3163
2022-07-29 08:51:24 - train: epoch 0074, iter [04400, 05004], lr: 0.035044, loss: 1.2194
2022-07-29 08:53:25 - train: epoch 0074, iter [04500, 05004], lr: 0.034994, loss: 1.4362
2022-07-29 08:55:30 - train: epoch 0074, iter [04600, 05004], lr: 0.034944, loss: 1.4580
2022-07-29 08:57:32 - train: epoch 0074, iter [04700, 05004], lr: 0.034894, loss: 1.3204
2022-07-29 08:59:33 - train: epoch 0074, iter [04800, 05004], lr: 0.034844, loss: 1.4504
2022-07-29 09:01:33 - train: epoch 0074, iter [04900, 05004], lr: 0.034794, loss: 1.6177
2022-07-29 09:03:31 - train: epoch 0074, iter [05000, 05004], lr: 0.034743, loss: 1.5546
2022-07-29 09:03:35 - train: epoch 074, train_loss: 1.4032
2022-07-29 09:07:40 - eval: epoch: 074, acc1: 68.686%, acc5: 89.000%, test_loss: 1.2671, per_image_load_time: 3.500ms, per_image_inference_time: 0.777ms
2022-07-29 09:07:40 - until epoch: 074, best_acc1: 68.778%
2022-07-29 09:07:40 - epoch 075 lr: 0.034741
2022-07-29 09:09:51 - train: epoch 0075, iter [00100, 05004], lr: 0.034691, loss: 1.4244
2022-07-29 09:11:48 - train: epoch 0075, iter [00200, 05004], lr: 0.034641, loss: 1.2428
2022-07-29 09:13:43 - train: epoch 0075, iter [00300, 05004], lr: 0.034591, loss: 1.2700
2022-07-29 09:15:39 - train: epoch 0075, iter [00400, 05004], lr: 0.034541, loss: 1.2882
2022-07-29 09:17:33 - train: epoch 0075, iter [00500, 05004], lr: 0.034491, loss: 1.3537
2022-07-29 09:19:30 - train: epoch 0075, iter [00600, 05004], lr: 0.034441, loss: 1.2001
2022-07-29 09:21:24 - train: epoch 0075, iter [00700, 05004], lr: 0.034392, loss: 1.5818
2022-07-29 09:23:20 - train: epoch 0075, iter [00800, 05004], lr: 0.034342, loss: 1.3280
2022-07-29 09:25:14 - train: epoch 0075, iter [00900, 05004], lr: 0.034292, loss: 1.4318
2022-07-29 09:27:04 - train: epoch 0075, iter [01000, 05004], lr: 0.034242, loss: 1.2646
2022-07-29 09:28:58 - train: epoch 0075, iter [01100, 05004], lr: 0.034192, loss: 1.3763
2022-07-29 09:30:52 - train: epoch 0075, iter [01200, 05004], lr: 0.034143, loss: 1.2800
2022-07-29 09:32:49 - train: epoch 0075, iter [01300, 05004], lr: 0.034093, loss: 1.2690
2022-07-29 09:34:51 - train: epoch 0075, iter [01400, 05004], lr: 0.034043, loss: 1.2507
2022-07-29 09:36:54 - train: epoch 0075, iter [01500, 05004], lr: 0.033994, loss: 1.3534
2022-07-29 09:38:54 - train: epoch 0075, iter [01600, 05004], lr: 0.033944, loss: 1.2110
2022-07-29 09:40:48 - train: epoch 0075, iter [01700, 05004], lr: 0.033894, loss: 1.3104
2022-07-29 09:42:40 - train: epoch 0075, iter [01800, 05004], lr: 0.033845, loss: 1.4365
2022-07-29 09:44:34 - train: epoch 0075, iter [01900, 05004], lr: 0.033795, loss: 1.2506
2022-07-29 09:46:27 - train: epoch 0075, iter [02000, 05004], lr: 0.033746, loss: 1.4495
2022-07-29 09:48:19 - train: epoch 0075, iter [02100, 05004], lr: 0.033696, loss: 1.2810
2022-07-29 09:50:16 - train: epoch 0075, iter [02200, 05004], lr: 0.033647, loss: 1.4287
2022-07-29 09:52:07 - train: epoch 0075, iter [02300, 05004], lr: 0.033597, loss: 1.1193
2022-07-29 09:54:01 - train: epoch 0075, iter [02400, 05004], lr: 0.033548, loss: 1.3294
2022-07-29 09:55:51 - train: epoch 0075, iter [02500, 05004], lr: 0.033499, loss: 1.4064
2022-07-29 09:57:44 - train: epoch 0075, iter [02600, 05004], lr: 0.033449, loss: 1.2552
2022-07-29 09:59:37 - train: epoch 0075, iter [02700, 05004], lr: 0.033400, loss: 1.4032
2022-07-29 10:01:35 - train: epoch 0075, iter [02800, 05004], lr: 0.033351, loss: 1.2049
2022-07-29 10:03:26 - train: epoch 0075, iter [02900, 05004], lr: 0.033301, loss: 1.3709
2022-07-29 10:05:17 - train: epoch 0075, iter [03000, 05004], lr: 0.033252, loss: 1.5791
2022-07-29 10:07:11 - train: epoch 0075, iter [03100, 05004], lr: 0.033203, loss: 1.3525
2022-07-29 10:09:01 - train: epoch 0075, iter [03200, 05004], lr: 0.033154, loss: 1.3969
2022-07-29 10:10:55 - train: epoch 0075, iter [03300, 05004], lr: 0.033105, loss: 1.3071
2022-07-29 10:12:50 - train: epoch 0075, iter [03400, 05004], lr: 0.033056, loss: 1.4335
2022-07-29 10:14:40 - train: epoch 0075, iter [03500, 05004], lr: 0.033006, loss: 1.4669
2022-07-29 10:16:30 - train: epoch 0075, iter [03600, 05004], lr: 0.032957, loss: 1.5473
2022-07-29 10:18:22 - train: epoch 0075, iter [03700, 05004], lr: 0.032908, loss: 1.5156
2022-07-29 10:20:13 - train: epoch 0075, iter [03800, 05004], lr: 0.032859, loss: 1.3394
2022-07-29 10:22:06 - train: epoch 0075, iter [03900, 05004], lr: 0.032810, loss: 1.3572
2022-07-29 10:24:00 - train: epoch 0075, iter [04000, 05004], lr: 0.032761, loss: 1.2626
2022-07-29 10:26:23 - train: epoch 0075, iter [04100, 05004], lr: 0.032713, loss: 1.3044
2022-07-29 10:28:23 - train: epoch 0075, iter [04200, 05004], lr: 0.032664, loss: 1.3175
2022-07-29 10:30:38 - train: epoch 0075, iter [04300, 05004], lr: 0.032615, loss: 1.5295
2022-07-29 10:32:42 - train: epoch 0075, iter [04400, 05004], lr: 0.032566, loss: 1.3354
2022-07-29 10:34:44 - train: epoch 0075, iter [04500, 05004], lr: 0.032517, loss: 1.3618
2022-07-29 10:36:45 - train: epoch 0075, iter [04600, 05004], lr: 0.032469, loss: 1.2652
2022-07-29 10:38:39 - train: epoch 0075, iter [04700, 05004], lr: 0.032420, loss: 1.6827
2022-07-29 10:40:37 - train: epoch 0075, iter [04800, 05004], lr: 0.032371, loss: 1.2490
2022-07-29 10:42:35 - train: epoch 0075, iter [04900, 05004], lr: 0.032322, loss: 1.3977
2022-07-29 10:44:32 - train: epoch 0075, iter [05000, 05004], lr: 0.032274, loss: 1.3847
2022-07-29 10:44:36 - train: epoch 075, train_loss: 1.3814
2022-07-29 10:48:37 - eval: epoch: 075, acc1: 69.234%, acc5: 89.270%, test_loss: 1.2559, per_image_load_time: 4.849ms, per_image_inference_time: 0.848ms
2022-07-29 10:48:37 - until epoch: 075, best_acc1: 69.234%
2022-07-29 10:48:37 - epoch 076 lr: 0.032271
2022-07-29 10:50:59 - train: epoch 0076, iter [00100, 05004], lr: 0.032223, loss: 1.2650
2022-07-29 10:52:59 - train: epoch 0076, iter [00200, 05004], lr: 0.032175, loss: 1.3123
2022-07-29 10:55:07 - train: epoch 0076, iter [00300, 05004], lr: 0.032126, loss: 1.3972
2022-07-29 10:57:03 - train: epoch 0076, iter [00400, 05004], lr: 0.032078, loss: 1.3482
2022-07-29 10:59:06 - train: epoch 0076, iter [00500, 05004], lr: 0.032029, loss: 1.2890
2022-07-29 11:01:05 - train: epoch 0076, iter [00600, 05004], lr: 0.031981, loss: 1.3584
2022-07-29 11:03:06 - train: epoch 0076, iter [00700, 05004], lr: 0.031932, loss: 1.3184
2022-07-29 11:05:04 - train: epoch 0076, iter [00800, 05004], lr: 0.031884, loss: 1.3198
2022-07-29 11:07:06 - train: epoch 0076, iter [00900, 05004], lr: 0.031835, loss: 1.2459
2022-07-29 11:09:05 - train: epoch 0076, iter [01000, 05004], lr: 0.031787, loss: 1.3198
2022-07-29 11:11:11 - train: epoch 0076, iter [01100, 05004], lr: 0.031739, loss: 1.2617
2022-07-29 11:13:08 - train: epoch 0076, iter [01200, 05004], lr: 0.031691, loss: 1.1622
2022-07-29 11:15:11 - train: epoch 0076, iter [01300, 05004], lr: 0.031642, loss: 1.3084
2022-07-29 11:17:13 - train: epoch 0076, iter [01400, 05004], lr: 0.031594, loss: 1.3417
2022-07-29 11:19:19 - train: epoch 0076, iter [01500, 05004], lr: 0.031546, loss: 1.3783
2022-07-29 11:21:17 - train: epoch 0076, iter [01600, 05004], lr: 0.031498, loss: 1.3666
2022-07-29 11:23:18 - train: epoch 0076, iter [01700, 05004], lr: 0.031450, loss: 1.3177
2022-07-29 11:25:13 - train: epoch 0076, iter [01800, 05004], lr: 0.031401, loss: 1.2209
2022-07-29 11:27:17 - train: epoch 0076, iter [01900, 05004], lr: 0.031353, loss: 1.4031
2022-07-29 11:29:08 - train: epoch 0076, iter [02000, 05004], lr: 0.031305, loss: 1.4508
2022-07-29 11:31:04 - train: epoch 0076, iter [02100, 05004], lr: 0.031257, loss: 1.3124
2022-07-29 11:32:58 - train: epoch 0076, iter [02200, 05004], lr: 0.031209, loss: 1.4215
2022-07-29 11:34:55 - train: epoch 0076, iter [02300, 05004], lr: 0.031161, loss: 1.3419
2022-07-29 11:36:58 - train: epoch 0076, iter [02400, 05004], lr: 0.031114, loss: 1.4985
2022-07-29 11:38:53 - train: epoch 0076, iter [02500, 05004], lr: 0.031066, loss: 1.3842
2022-07-29 11:40:56 - train: epoch 0076, iter [02600, 05004], lr: 0.031018, loss: 1.5768
2022-07-29 11:42:51 - train: epoch 0076, iter [02700, 05004], lr: 0.030970, loss: 1.3386
2022-07-29 11:44:53 - train: epoch 0076, iter [02800, 05004], lr: 0.030922, loss: 1.3066
2022-07-29 11:46:53 - train: epoch 0076, iter [02900, 05004], lr: 0.030874, loss: 1.4904
2022-07-29 11:48:53 - train: epoch 0076, iter [03000, 05004], lr: 0.030827, loss: 1.2724
2022-07-29 11:50:44 - train: epoch 0076, iter [03100, 05004], lr: 0.030779, loss: 1.3412
2022-07-29 11:52:38 - train: epoch 0076, iter [03200, 05004], lr: 0.030731, loss: 1.2588
2022-07-29 11:54:37 - train: epoch 0076, iter [03300, 05004], lr: 0.030684, loss: 1.3963
2022-07-29 11:56:39 - train: epoch 0076, iter [03400, 05004], lr: 0.030636, loss: 1.3183
2022-07-29 11:58:35 - train: epoch 0076, iter [03500, 05004], lr: 0.030588, loss: 1.2542
2022-07-29 12:00:35 - train: epoch 0076, iter [03600, 05004], lr: 0.030541, loss: 1.1440
2022-07-29 12:02:31 - train: epoch 0076, iter [03700, 05004], lr: 0.030493, loss: 1.2758
2022-07-29 12:04:28 - train: epoch 0076, iter [03800, 05004], lr: 0.030446, loss: 1.3886
2022-07-29 12:06:25 - train: epoch 0076, iter [03900, 05004], lr: 0.030398, loss: 1.3214
2022-07-29 12:08:25 - train: epoch 0076, iter [04000, 05004], lr: 0.030351, loss: 1.1931
2022-07-29 12:10:16 - train: epoch 0076, iter [04100, 05004], lr: 0.030303, loss: 1.2622
2022-07-29 12:12:21 - train: epoch 0076, iter [04200, 05004], lr: 0.030256, loss: 1.4689
2022-07-29 12:14:15 - train: epoch 0076, iter [04300, 05004], lr: 0.030209, loss: 1.3761
2022-07-29 12:16:13 - train: epoch 0076, iter [04400, 05004], lr: 0.030161, loss: 1.3023
2022-07-29 12:18:12 - train: epoch 0076, iter [04500, 05004], lr: 0.030114, loss: 1.5758
2022-07-29 12:20:02 - train: epoch 0076, iter [04600, 05004], lr: 0.030067, loss: 1.2957
2022-07-29 12:22:00 - train: epoch 0076, iter [04700, 05004], lr: 0.030020, loss: 1.3972
2022-07-29 12:23:51 - train: epoch 0076, iter [04800, 05004], lr: 0.029972, loss: 1.2282
2022-07-29 12:25:48 - train: epoch 0076, iter [04900, 05004], lr: 0.029925, loss: 1.4825
2022-07-29 12:27:39 - train: epoch 0076, iter [05000, 05004], lr: 0.029878, loss: 1.5198
2022-07-29 12:27:43 - train: epoch 076, train_loss: 1.3598
2022-07-29 12:31:46 - eval: epoch: 076, acc1: 69.280%, acc5: 89.420%, test_loss: 1.2417, per_image_load_time: 8.171ms, per_image_inference_time: 0.830ms
2022-07-29 12:31:47 - until epoch: 076, best_acc1: 69.280%
2022-07-29 12:31:47 - epoch 077 lr: 0.029876
2022-07-29 12:34:00 - train: epoch 0077, iter [00100, 05004], lr: 0.029829, loss: 1.4463
2022-07-29 12:35:53 - train: epoch 0077, iter [00200, 05004], lr: 0.029782, loss: 1.3833
2022-07-29 12:37:48 - train: epoch 0077, iter [00300, 05004], lr: 0.029735, loss: 1.2701
2022-07-29 12:39:48 - train: epoch 0077, iter [00400, 05004], lr: 0.029688, loss: 1.2301
2022-07-29 12:41:45 - train: epoch 0077, iter [00500, 05004], lr: 0.029641, loss: 1.2284
2022-07-29 12:43:37 - train: epoch 0077, iter [00600, 05004], lr: 0.029594, loss: 1.1747
2022-07-29 12:45:32 - train: epoch 0077, iter [00700, 05004], lr: 0.029547, loss: 1.3813
2022-07-29 12:47:29 - train: epoch 0077, iter [00800, 05004], lr: 0.029500, loss: 1.3394
2022-07-29 12:49:26 - train: epoch 0077, iter [00900, 05004], lr: 0.029454, loss: 1.2532
2022-07-29 12:51:21 - train: epoch 0077, iter [01000, 05004], lr: 0.029407, loss: 1.2004
2022-07-29 12:53:17 - train: epoch 0077, iter [01100, 05004], lr: 0.029360, loss: 1.3317
2022-07-29 12:55:13 - train: epoch 0077, iter [01200, 05004], lr: 0.029313, loss: 1.4517
2022-07-29 12:57:06 - train: epoch 0077, iter [01300, 05004], lr: 0.029266, loss: 1.2835
2022-07-29 12:59:02 - train: epoch 0077, iter [01400, 05004], lr: 0.029220, loss: 1.2808
2022-07-29 13:00:57 - train: epoch 0077, iter [01500, 05004], lr: 0.029173, loss: 1.3844
2022-07-29 13:02:55 - train: epoch 0077, iter [01600, 05004], lr: 0.029126, loss: 1.3415
2022-07-29 13:04:55 - train: epoch 0077, iter [01700, 05004], lr: 0.029080, loss: 1.3826
2022-07-29 13:06:49 - train: epoch 0077, iter [01800, 05004], lr: 0.029033, loss: 1.2369
2022-07-29 13:08:46 - train: epoch 0077, iter [01900, 05004], lr: 0.028987, loss: 1.2186
2022-07-29 13:10:41 - train: epoch 0077, iter [02000, 05004], lr: 0.028940, loss: 1.2836
2022-07-29 13:12:38 - train: epoch 0077, iter [02100, 05004], lr: 0.028894, loss: 1.4618
2022-07-29 13:14:38 - train: epoch 0077, iter [02200, 05004], lr: 0.028847, loss: 1.2056
2022-07-29 13:16:35 - train: epoch 0077, iter [02300, 05004], lr: 0.028801, loss: 1.4899
2022-07-29 13:18:32 - train: epoch 0077, iter [02400, 05004], lr: 0.028754, loss: 1.3058
2022-07-29 13:20:29 - train: epoch 0077, iter [02500, 05004], lr: 0.028708, loss: 1.2128
2022-07-29 13:22:26 - train: epoch 0077, iter [02600, 05004], lr: 0.028662, loss: 1.1414
2022-07-29 13:24:24 - train: epoch 0077, iter [02700, 05004], lr: 0.028615, loss: 1.3480
2022-07-29 13:26:22 - train: epoch 0077, iter [02800, 05004], lr: 0.028569, loss: 1.3768
2022-07-29 13:28:17 - train: epoch 0077, iter [02900, 05004], lr: 0.028523, loss: 1.4094
2022-07-29 13:30:16 - train: epoch 0077, iter [03000, 05004], lr: 0.028477, loss: 1.3155
2022-07-29 13:32:10 - train: epoch 0077, iter [03100, 05004], lr: 0.028431, loss: 1.3889
2022-07-29 13:34:06 - train: epoch 0077, iter [03200, 05004], lr: 0.028384, loss: 1.4627
2022-07-29 13:36:01 - train: epoch 0077, iter [03300, 05004], lr: 0.028338, loss: 1.2644
2022-07-29 13:37:59 - train: epoch 0077, iter [03400, 05004], lr: 0.028292, loss: 1.4733
2022-07-29 13:39:53 - train: epoch 0077, iter [03500, 05004], lr: 0.028246, loss: 1.2908
2022-07-29 13:41:47 - train: epoch 0077, iter [03600, 05004], lr: 0.028200, loss: 1.2671
2022-07-29 13:43:41 - train: epoch 0077, iter [03700, 05004], lr: 0.028154, loss: 1.2356
2022-07-29 13:45:45 - train: epoch 0077, iter [03800, 05004], lr: 0.028108, loss: 1.2576
2022-07-29 13:47:34 - train: epoch 0077, iter [03900, 05004], lr: 0.028062, loss: 1.3797
2022-07-29 13:49:29 - train: epoch 0077, iter [04000, 05004], lr: 0.028016, loss: 1.3379
2022-07-29 13:51:14 - train: epoch 0077, iter [04100, 05004], lr: 0.027971, loss: 1.3979
2022-07-29 13:53:19 - train: epoch 0077, iter [04200, 05004], lr: 0.027925, loss: 1.4315
2022-07-29 13:55:25 - train: epoch 0077, iter [04300, 05004], lr: 0.027879, loss: 1.4134
2022-07-29 13:57:22 - train: epoch 0077, iter [04400, 05004], lr: 0.027833, loss: 1.2716
2022-07-29 13:59:26 - train: epoch 0077, iter [04500, 05004], lr: 0.027787, loss: 1.3747
2022-07-29 14:01:27 - train: epoch 0077, iter [04600, 05004], lr: 0.027742, loss: 1.2217
2022-07-29 14:03:25 - train: epoch 0077, iter [04700, 05004], lr: 0.027696, loss: 1.3453
2022-07-29 14:05:24 - train: epoch 0077, iter [04800, 05004], lr: 0.027650, loss: 1.2228
2022-07-29 14:07:22 - train: epoch 0077, iter [04900, 05004], lr: 0.027605, loss: 1.4778
2022-07-29 14:09:12 - train: epoch 0077, iter [05000, 05004], lr: 0.027559, loss: 1.4337
2022-07-29 14:09:16 - train: epoch 077, train_loss: 1.3407
2022-07-29 14:13:10 - eval: epoch: 077, acc1: 69.846%, acc5: 89.644%, test_loss: 1.2212, per_image_load_time: 6.579ms, per_image_inference_time: 0.809ms
2022-07-29 14:13:10 - until epoch: 077, best_acc1: 69.846%
2022-07-29 14:13:10 - epoch 078 lr: 0.027557
2022-07-29 14:15:22 - train: epoch 0078, iter [00100, 05004], lr: 0.027512, loss: 1.3849
2022-07-29 14:17:20 - train: epoch 0078, iter [00200, 05004], lr: 0.027466, loss: 1.5265
2022-07-29 14:19:14 - train: epoch 0078, iter [00300, 05004], lr: 0.027421, loss: 1.2226
2022-07-29 14:21:10 - train: epoch 0078, iter [00400, 05004], lr: 0.027376, loss: 1.2100
2022-07-29 14:23:02 - train: epoch 0078, iter [00500, 05004], lr: 0.027330, loss: 1.0597
2022-07-29 14:24:58 - train: epoch 0078, iter [00600, 05004], lr: 0.027285, loss: 1.3697
2022-07-29 14:26:51 - train: epoch 0078, iter [00700, 05004], lr: 0.027239, loss: 1.4168
2022-07-29 14:28:49 - train: epoch 0078, iter [00800, 05004], lr: 0.027194, loss: 1.3767
2022-07-29 14:30:44 - train: epoch 0078, iter [00900, 05004], lr: 0.027149, loss: 1.4114
2022-07-29 14:32:41 - train: epoch 0078, iter [01000, 05004], lr: 0.027103, loss: 1.2947
2022-07-29 14:34:33 - train: epoch 0078, iter [01100, 05004], lr: 0.027058, loss: 1.1944
2022-07-29 14:36:31 - train: epoch 0078, iter [01200, 05004], lr: 0.027013, loss: 1.2247
2022-07-29 14:38:25 - train: epoch 0078, iter [01300, 05004], lr: 0.026968, loss: 1.2281
2022-07-29 14:40:19 - train: epoch 0078, iter [01400, 05004], lr: 0.026923, loss: 1.1562
2022-07-29 14:42:17 - train: epoch 0078, iter [01500, 05004], lr: 0.026878, loss: 1.2635
2022-07-29 14:44:16 - train: epoch 0078, iter [01600, 05004], lr: 0.026833, loss: 1.3475
2022-07-29 14:46:13 - train: epoch 0078, iter [01700, 05004], lr: 0.026788, loss: 1.2726
2022-07-29 14:48:12 - train: epoch 0078, iter [01800, 05004], lr: 0.026743, loss: 1.4313
2022-07-29 14:50:08 - train: epoch 0078, iter [01900, 05004], lr: 0.026698, loss: 1.4387
2022-07-29 14:52:00 - train: epoch 0078, iter [02000, 05004], lr: 0.026653, loss: 1.2119
2022-07-29 14:53:56 - train: epoch 0078, iter [02100, 05004], lr: 0.026608, loss: 1.5652
2022-07-29 14:55:57 - train: epoch 0078, iter [02200, 05004], lr: 0.026563, loss: 1.4124
2022-07-29 14:57:51 - train: epoch 0078, iter [02300, 05004], lr: 0.026518, loss: 1.2357
2022-07-29 14:59:54 - train: epoch 0078, iter [02400, 05004], lr: 0.026473, loss: 1.2781
2022-07-29 15:01:44 - train: epoch 0078, iter [02500, 05004], lr: 0.026429, loss: 1.2828
2022-07-29 15:03:45 - train: epoch 0078, iter [02600, 05004], lr: 0.026384, loss: 1.2320
2022-07-29 15:05:43 - train: epoch 0078, iter [02700, 05004], lr: 0.026339, loss: 1.2228
2022-07-29 15:07:40 - train: epoch 0078, iter [02800, 05004], lr: 0.026294, loss: 1.4492
2022-07-29 15:09:38 - train: epoch 0078, iter [02900, 05004], lr: 0.026250, loss: 1.2964
2022-07-29 15:11:33 - train: epoch 0078, iter [03000, 05004], lr: 0.026205, loss: 1.3002
2022-07-29 15:13:28 - train: epoch 0078, iter [03100, 05004], lr: 0.026161, loss: 1.4472
2022-07-29 15:15:29 - train: epoch 0078, iter [03200, 05004], lr: 0.026116, loss: 1.4755
2022-07-29 15:17:21 - train: epoch 0078, iter [03300, 05004], lr: 0.026071, loss: 1.5344
2022-07-29 15:19:13 - train: epoch 0078, iter [03400, 05004], lr: 0.026027, loss: 1.3383
2022-07-29 15:21:09 - train: epoch 0078, iter [03500, 05004], lr: 0.025983, loss: 1.3816
2022-07-29 15:23:05 - train: epoch 0078, iter [03600, 05004], lr: 0.025938, loss: 1.4251
2022-07-29 15:25:02 - train: epoch 0078, iter [03700, 05004], lr: 0.025894, loss: 1.2899
2022-07-29 15:26:53 - train: epoch 0078, iter [03800, 05004], lr: 0.025849, loss: 1.2478
2022-07-29 15:28:49 - train: epoch 0078, iter [03900, 05004], lr: 0.025805, loss: 1.3384
2022-07-29 15:30:46 - train: epoch 0078, iter [04000, 05004], lr: 0.025761, loss: 1.4643
2022-07-29 15:32:42 - train: epoch 0078, iter [04100, 05004], lr: 0.025716, loss: 1.2822
2022-07-29 15:34:38 - train: epoch 0078, iter [04200, 05004], lr: 0.025672, loss: 1.5409
2022-07-29 15:36:24 - train: epoch 0078, iter [04300, 05004], lr: 0.025628, loss: 1.5316
2022-07-29 15:38:34 - train: epoch 0078, iter [04400, 05004], lr: 0.025584, loss: 1.4407
2022-07-29 15:40:23 - train: epoch 0078, iter [04500, 05004], lr: 0.025540, loss: 1.3557
2022-07-29 15:42:21 - train: epoch 0078, iter [04600, 05004], lr: 0.025496, loss: 1.1685
2022-07-29 15:44:15 - train: epoch 0078, iter [04700, 05004], lr: 0.025452, loss: 1.3489
2022-07-29 15:46:03 - train: epoch 0078, iter [04800, 05004], lr: 0.025408, loss: 1.5233
2022-07-29 15:48:00 - train: epoch 0078, iter [04900, 05004], lr: 0.025364, loss: 1.2616
2022-07-29 15:49:44 - train: epoch 0078, iter [05000, 05004], lr: 0.025320, loss: 1.4172
2022-07-29 15:49:47 - train: epoch 078, train_loss: 1.3174
2022-07-29 15:53:38 - eval: epoch: 078, acc1: 69.914%, acc5: 89.866%, test_loss: 1.2162, per_image_load_time: 2.723ms, per_image_inference_time: 0.747ms
2022-07-29 15:53:39 - until epoch: 078, best_acc1: 69.914%
2022-07-29 15:53:39 - epoch 079 lr: 0.025317
2022-07-29 15:55:50 - train: epoch 0079, iter [00100, 05004], lr: 0.025274, loss: 1.3673
2022-07-29 15:57:44 - train: epoch 0079, iter [00200, 05004], lr: 0.025230, loss: 1.2504
2022-07-29 15:59:37 - train: epoch 0079, iter [00300, 05004], lr: 0.025186, loss: 1.0799
2022-07-29 16:01:28 - train: epoch 0079, iter [00400, 05004], lr: 0.025142, loss: 1.2496
2022-07-29 16:03:26 - train: epoch 0079, iter [00500, 05004], lr: 0.025099, loss: 1.2243
2022-07-29 16:05:22 - train: epoch 0079, iter [00600, 05004], lr: 0.025055, loss: 1.1356
2022-07-29 16:07:22 - train: epoch 0079, iter [00700, 05004], lr: 0.025011, loss: 1.4231
2022-07-29 16:09:20 - train: epoch 0079, iter [00800, 05004], lr: 0.024967, loss: 1.2627
2022-07-29 16:11:22 - train: epoch 0079, iter [00900, 05004], lr: 0.024924, loss: 1.3867
2022-07-29 16:13:21 - train: epoch 0079, iter [01000, 05004], lr: 0.024880, loss: 1.3561
2022-07-29 16:15:15 - train: epoch 0079, iter [01100, 05004], lr: 0.024836, loss: 1.3234
2022-07-29 16:17:10 - train: epoch 0079, iter [01200, 05004], lr: 0.024793, loss: 1.5148
2022-07-29 16:19:07 - train: epoch 0079, iter [01300, 05004], lr: 0.024749, loss: 0.9906
2022-07-29 16:21:11 - train: epoch 0079, iter [01400, 05004], lr: 0.024706, loss: 1.5864
2022-07-29 16:23:09 - train: epoch 0079, iter [01500, 05004], lr: 0.024662, loss: 1.2139
2022-07-29 16:25:04 - train: epoch 0079, iter [01600, 05004], lr: 0.024619, loss: 1.0819
2022-07-29 16:27:14 - train: epoch 0079, iter [01700, 05004], lr: 0.024575, loss: 1.2464
2022-07-29 16:29:13 - train: epoch 0079, iter [01800, 05004], lr: 0.024532, loss: 1.4084
2022-07-29 16:31:12 - train: epoch 0079, iter [01900, 05004], lr: 0.024489, loss: 1.0840
2022-07-29 16:33:11 - train: epoch 0079, iter [02000, 05004], lr: 0.024445, loss: 1.4639
2022-07-29 16:35:14 - train: epoch 0079, iter [02100, 05004], lr: 0.024402, loss: 1.3541
2022-07-29 16:37:12 - train: epoch 0079, iter [02200, 05004], lr: 0.024359, loss: 1.2450
2022-07-29 16:39:11 - train: epoch 0079, iter [02300, 05004], lr: 0.024316, loss: 1.2322
2022-07-29 16:41:09 - train: epoch 0079, iter [02400, 05004], lr: 0.024273, loss: 1.3696
2022-07-29 16:43:07 - train: epoch 0079, iter [02500, 05004], lr: 0.024229, loss: 1.2501
2022-07-29 16:45:07 - train: epoch 0079, iter [02600, 05004], lr: 0.024186, loss: 1.2944
2022-07-29 16:47:04 - train: epoch 0079, iter [02700, 05004], lr: 0.024143, loss: 1.1660
2022-07-29 16:49:05 - train: epoch 0079, iter [02800, 05004], lr: 0.024100, loss: 1.1640
2022-07-29 16:50:59 - train: epoch 0079, iter [02900, 05004], lr: 0.024057, loss: 1.3420
2022-07-29 16:53:00 - train: epoch 0079, iter [03000, 05004], lr: 0.024014, loss: 1.3838
2022-07-29 16:55:00 - train: epoch 0079, iter [03100, 05004], lr: 0.023971, loss: 1.2443
2022-07-29 16:57:09 - train: epoch 0079, iter [03200, 05004], lr: 0.023928, loss: 1.3299
2022-07-29 16:58:59 - train: epoch 0079, iter [03300, 05004], lr: 0.023885, loss: 1.1992
2022-07-29 17:01:02 - train: epoch 0079, iter [03400, 05004], lr: 0.023843, loss: 1.1475
2022-07-29 17:02:58 - train: epoch 0079, iter [03500, 05004], lr: 0.023800, loss: 1.3862
2022-07-29 17:04:58 - train: epoch 0079, iter [03600, 05004], lr: 0.023757, loss: 1.2518
2022-07-29 17:06:57 - train: epoch 0079, iter [03700, 05004], lr: 0.023714, loss: 1.1887
2022-07-29 17:08:54 - train: epoch 0079, iter [03800, 05004], lr: 0.023672, loss: 1.1666
2022-07-29 17:10:53 - train: epoch 0079, iter [03900, 05004], lr: 0.023629, loss: 1.2743
2022-07-29 17:12:56 - train: epoch 0079, iter [04000, 05004], lr: 0.023586, loss: 1.1754
2022-07-29 17:14:58 - train: epoch 0079, iter [04100, 05004], lr: 0.023544, loss: 1.4025
2022-07-29 17:16:55 - train: epoch 0079, iter [04200, 05004], lr: 0.023501, loss: 1.2131
2022-07-29 17:18:55 - train: epoch 0079, iter [04300, 05004], lr: 0.023458, loss: 1.1143
2022-07-29 17:20:38 - train: epoch 0079, iter [04400, 05004], lr: 0.023416, loss: 1.2462
2022-07-29 17:22:51 - train: epoch 0079, iter [04500, 05004], lr: 0.023373, loss: 1.5371
2022-07-29 17:24:39 - train: epoch 0079, iter [04600, 05004], lr: 0.023331, loss: 1.4181
2022-07-29 17:26:42 - train: epoch 0079, iter [04700, 05004], lr: 0.023289, loss: 1.3150
2022-07-29 17:28:43 - train: epoch 0079, iter [04800, 05004], lr: 0.023246, loss: 1.1735
2022-07-29 17:30:47 - train: epoch 0079, iter [04900, 05004], lr: 0.023204, loss: 1.1291
2022-07-29 17:32:42 - train: epoch 0079, iter [05000, 05004], lr: 0.023162, loss: 1.1630
2022-07-29 17:32:46 - train: epoch 079, train_loss: 1.2916
2022-07-29 17:36:50 - eval: epoch: 079, acc1: 70.360%, acc5: 90.098%, test_loss: 1.1944, per_image_load_time: 8.452ms, per_image_inference_time: 0.763ms
2022-07-29 17:36:50 - until epoch: 079, best_acc1: 70.360%
2022-07-29 17:36:50 - epoch 080 lr: 0.023159
2022-07-29 17:39:05 - train: epoch 0080, iter [00100, 05004], lr: 0.023118, loss: 1.1520
2022-07-29 17:41:05 - train: epoch 0080, iter [00200, 05004], lr: 0.023075, loss: 1.3895
2022-07-29 17:43:00 - train: epoch 0080, iter [00300, 05004], lr: 0.023033, loss: 1.3538
2022-07-29 17:44:55 - train: epoch 0080, iter [00400, 05004], lr: 0.022991, loss: 1.2463
2022-07-29 17:46:54 - train: epoch 0080, iter [00500, 05004], lr: 0.022949, loss: 1.1634
2022-07-29 17:48:51 - train: epoch 0080, iter [00600, 05004], lr: 0.022907, loss: 1.2705
2022-07-29 17:50:47 - train: epoch 0080, iter [00700, 05004], lr: 0.022865, loss: 1.2096
2022-07-29 17:52:42 - train: epoch 0080, iter [00800, 05004], lr: 0.022823, loss: 1.0416
2022-07-29 17:54:40 - train: epoch 0080, iter [00900, 05004], lr: 0.022781, loss: 1.1806
2022-07-29 17:56:33 - train: epoch 0080, iter [01000, 05004], lr: 0.022739, loss: 1.2383
2022-07-29 17:58:31 - train: epoch 0080, iter [01100, 05004], lr: 0.022697, loss: 1.2106
2022-07-29 18:00:28 - train: epoch 0080, iter [01200, 05004], lr: 0.022655, loss: 1.1418
2022-07-29 18:02:25 - train: epoch 0080, iter [01300, 05004], lr: 0.022613, loss: 1.2508
2022-07-29 18:04:18 - train: epoch 0080, iter [01400, 05004], lr: 0.022571, loss: 1.2031
2022-07-29 18:06:14 - train: epoch 0080, iter [01500, 05004], lr: 0.022529, loss: 1.1401
2022-07-29 18:08:03 - train: epoch 0080, iter [01600, 05004], lr: 0.022488, loss: 1.2549
2022-07-29 18:09:56 - train: epoch 0080, iter [01700, 05004], lr: 0.022446, loss: 1.4271
2022-07-29 18:11:49 - train: epoch 0080, iter [01800, 05004], lr: 0.022404, loss: 1.4300
2022-07-29 18:13:44 - train: epoch 0080, iter [01900, 05004], lr: 0.022362, loss: 1.2328
2022-07-29 18:15:33 - train: epoch 0080, iter [02000, 05004], lr: 0.022321, loss: 1.2767
2022-07-29 18:17:24 - train: epoch 0080, iter [02100, 05004], lr: 0.022279, loss: 1.5503
2022-07-29 18:19:18 - train: epoch 0080, iter [02200, 05004], lr: 0.022238, loss: 1.2200
2022-07-29 18:21:09 - train: epoch 0080, iter [02300, 05004], lr: 0.022196, loss: 1.2563
2022-07-29 18:22:59 - train: epoch 0080, iter [02400, 05004], lr: 0.022155, loss: 1.3569
2022-07-29 18:24:53 - train: epoch 0080, iter [02500, 05004], lr: 0.022113, loss: 1.2176
2022-07-29 18:26:44 - train: epoch 0080, iter [02600, 05004], lr: 0.022072, loss: 1.2917
2022-07-29 18:28:39 - train: epoch 0080, iter [02700, 05004], lr: 0.022030, loss: 1.2229
2022-07-29 18:30:29 - train: epoch 0080, iter [02800, 05004], lr: 0.021989, loss: 1.3952
2022-07-29 18:32:20 - train: epoch 0080, iter [02900, 05004], lr: 0.021948, loss: 1.3793
2022-07-29 18:34:10 - train: epoch 0080, iter [03000, 05004], lr: 0.021906, loss: 1.1705
2022-07-29 18:36:08 - train: epoch 0080, iter [03100, 05004], lr: 0.021865, loss: 1.2442
2022-07-29 18:37:56 - train: epoch 0080, iter [03200, 05004], lr: 0.021824, loss: 1.0222
2022-07-29 18:39:54 - train: epoch 0080, iter [03300, 05004], lr: 0.021783, loss: 1.3433
2022-07-29 18:41:44 - train: epoch 0080, iter [03400, 05004], lr: 0.021741, loss: 1.0791
2022-07-29 18:43:39 - train: epoch 0080, iter [03500, 05004], lr: 0.021700, loss: 1.2769
2022-07-29 18:45:32 - train: epoch 0080, iter [03600, 05004], lr: 0.021659, loss: 1.3455
2022-07-29 18:47:23 - train: epoch 0080, iter [03700, 05004], lr: 0.021618, loss: 1.2019
2022-07-29 18:49:13 - train: epoch 0080, iter [03800, 05004], lr: 0.021577, loss: 1.2626
2022-07-29 18:51:05 - train: epoch 0080, iter [03900, 05004], lr: 0.021536, loss: 1.3360
2022-07-29 18:52:59 - train: epoch 0080, iter [04000, 05004], lr: 0.021495, loss: 1.4216
2022-07-29 18:54:51 - train: epoch 0080, iter [04100, 05004], lr: 0.021454, loss: 1.3386
2022-07-29 18:56:58 - train: epoch 0080, iter [04200, 05004], lr: 0.021413, loss: 1.2515
2022-07-29 18:58:55 - train: epoch 0080, iter [04300, 05004], lr: 0.021373, loss: 1.5943
2022-07-29 19:00:43 - train: epoch 0080, iter [04400, 05004], lr: 0.021332, loss: 1.1727
2022-07-29 19:02:34 - train: epoch 0080, iter [04500, 05004], lr: 0.021291, loss: 1.2467
2022-07-29 19:04:38 - train: epoch 0080, iter [04600, 05004], lr: 0.021250, loss: 1.3536
2022-07-29 19:06:24 - train: epoch 0080, iter [04700, 05004], lr: 0.021210, loss: 1.1968
2022-07-29 19:08:26 - train: epoch 0080, iter [04800, 05004], lr: 0.021169, loss: 1.3292
2022-07-29 19:10:29 - train: epoch 0080, iter [04900, 05004], lr: 0.021128, loss: 1.4970
2022-07-29 19:12:25 - train: epoch 0080, iter [05000, 05004], lr: 0.021088, loss: 1.2343
2022-07-29 19:12:29 - train: epoch 080, train_loss: 1.2687
2022-07-29 19:16:12 - eval: epoch: 080, acc1: 70.934%, acc5: 90.240%, test_loss: 1.1816, per_image_load_time: 7.575ms, per_image_inference_time: 0.879ms
2022-07-29 19:16:13 - until epoch: 080, best_acc1: 70.934%
2022-07-29 19:16:13 - epoch 081 lr: 0.021086
2022-07-29 19:18:29 - train: epoch 0081, iter [00100, 05004], lr: 0.021045, loss: 1.1840
2022-07-29 19:20:29 - train: epoch 0081, iter [00200, 05004], lr: 0.021005, loss: 1.1402
2022-07-29 19:22:25 - train: epoch 0081, iter [00300, 05004], lr: 0.020964, loss: 1.2195
2022-07-29 19:24:26 - train: epoch 0081, iter [00400, 05004], lr: 0.020924, loss: 1.1478
2022-07-29 19:26:22 - train: epoch 0081, iter [00500, 05004], lr: 0.020883, loss: 1.5419
2022-07-29 19:28:23 - train: epoch 0081, iter [00600, 05004], lr: 0.020843, loss: 1.1350
2022-07-29 19:30:21 - train: epoch 0081, iter [00700, 05004], lr: 0.020803, loss: 1.2863
2022-07-29 19:32:16 - train: epoch 0081, iter [00800, 05004], lr: 0.020762, loss: 1.1994
2022-07-29 19:34:11 - train: epoch 0081, iter [00900, 05004], lr: 0.020722, loss: 1.1335
2022-07-29 19:36:07 - train: epoch 0081, iter [01000, 05004], lr: 0.020682, loss: 1.3824
2022-07-29 19:38:02 - train: epoch 0081, iter [01100, 05004], lr: 0.020642, loss: 1.1315
2022-07-29 19:40:00 - train: epoch 0081, iter [01200, 05004], lr: 0.020601, loss: 1.4599
2022-07-29 19:41:53 - train: epoch 0081, iter [01300, 05004], lr: 0.020561, loss: 1.1014
2022-07-29 19:43:49 - train: epoch 0081, iter [01400, 05004], lr: 0.020521, loss: 1.1032
2022-07-29 19:45:45 - train: epoch 0081, iter [01500, 05004], lr: 0.020481, loss: 1.4871
2022-07-29 19:47:43 - train: epoch 0081, iter [01600, 05004], lr: 0.020441, loss: 1.0399
2022-07-29 19:49:35 - train: epoch 0081, iter [01700, 05004], lr: 0.020401, loss: 1.1377
2022-07-29 19:51:34 - train: epoch 0081, iter [01800, 05004], lr: 0.020361, loss: 1.1969
2022-07-29 19:53:35 - train: epoch 0081, iter [01900, 05004], lr: 0.020321, loss: 1.2385
2022-07-29 19:55:30 - train: epoch 0081, iter [02000, 05004], lr: 0.020281, loss: 1.2820
2022-07-29 19:57:25 - train: epoch 0081, iter [02100, 05004], lr: 0.020241, loss: 1.2235
2022-07-29 19:59:16 - train: epoch 0081, iter [02200, 05004], lr: 0.020201, loss: 1.1525
2022-07-29 20:01:12 - train: epoch 0081, iter [02300, 05004], lr: 0.020162, loss: 1.2012
2022-07-29 20:03:07 - train: epoch 0081, iter [02400, 05004], lr: 0.020122, loss: 1.1714
2022-07-29 20:05:06 - train: epoch 0081, iter [02500, 05004], lr: 0.020082, loss: 1.4119
2022-07-29 20:07:02 - train: epoch 0081, iter [02600, 05004], lr: 0.020042, loss: 1.2736
2022-07-29 20:08:55 - train: epoch 0081, iter [02700, 05004], lr: 0.020003, loss: 1.3546
2022-07-29 20:10:53 - train: epoch 0081, iter [02800, 05004], lr: 0.019963, loss: 1.0625
2022-07-29 20:12:42 - train: epoch 0081, iter [02900, 05004], lr: 0.019923, loss: 1.1272
2022-07-29 20:14:35 - train: epoch 0081, iter [03000, 05004], lr: 0.019884, loss: 1.3543
2022-07-29 20:16:33 - train: epoch 0081, iter [03100, 05004], lr: 0.019844, loss: 1.3078
2022-07-29 20:18:25 - train: epoch 0081, iter [03200, 05004], lr: 0.019805, loss: 1.0895
2022-07-29 20:20:18 - train: epoch 0081, iter [03300, 05004], lr: 0.019765, loss: 1.1390
2022-07-29 20:22:15 - train: epoch 0081, iter [03400, 05004], lr: 0.019726, loss: 1.2185
2022-07-29 20:24:05 - train: epoch 0081, iter [03500, 05004], lr: 0.019687, loss: 1.2451
2022-07-29 20:26:08 - train: epoch 0081, iter [03600, 05004], lr: 0.019647, loss: 1.0742
2022-07-29 20:27:58 - train: epoch 0081, iter [03700, 05004], lr: 0.019608, loss: 1.2634
2022-07-29 20:29:55 - train: epoch 0081, iter [03800, 05004], lr: 0.019569, loss: 1.4227
2022-07-29 20:31:51 - train: epoch 0081, iter [03900, 05004], lr: 0.019529, loss: 1.3125
2022-07-29 20:33:49 - train: epoch 0081, iter [04000, 05004], lr: 0.019490, loss: 1.1462
2022-07-29 20:35:45 - train: epoch 0081, iter [04100, 05004], lr: 0.019451, loss: 1.1924
2022-07-29 20:37:44 - train: epoch 0081, iter [04200, 05004], lr: 0.019412, loss: 1.3793
2022-07-29 20:39:39 - train: epoch 0081, iter [04300, 05004], lr: 0.019373, loss: 1.1639
2022-07-29 20:41:36 - train: epoch 0081, iter [04400, 05004], lr: 0.019334, loss: 1.3706
2022-07-29 20:43:31 - train: epoch 0081, iter [04500, 05004], lr: 0.019295, loss: 1.1634
2022-07-29 20:45:21 - train: epoch 0081, iter [04600, 05004], lr: 0.019256, loss: 1.3132
2022-07-29 20:47:33 - train: epoch 0081, iter [04700, 05004], lr: 0.019217, loss: 1.3781
2022-07-29 20:49:22 - train: epoch 0081, iter [04800, 05004], lr: 0.019178, loss: 1.3027
2022-07-29 20:51:23 - train: epoch 0081, iter [04900, 05004], lr: 0.019139, loss: 1.1717
2022-07-29 20:53:18 - train: epoch 0081, iter [05000, 05004], lr: 0.019100, loss: 1.1305
2022-07-29 20:53:22 - train: epoch 081, train_loss: 1.2448
2022-07-29 20:57:00 - eval: epoch: 081, acc1: 70.946%, acc5: 90.348%, test_loss: 1.1726, per_image_load_time: 7.240ms, per_image_inference_time: 0.880ms
2022-07-29 20:57:01 - until epoch: 081, best_acc1: 70.946%
2022-07-29 20:57:01 - epoch 082 lr: 0.019098
2022-07-29 20:59:17 - train: epoch 0082, iter [00100, 05004], lr: 0.019059, loss: 1.0876
2022-07-29 21:01:15 - train: epoch 0082, iter [00200, 05004], lr: 0.019021, loss: 1.2469
2022-07-29 21:03:07 - train: epoch 0082, iter [00300, 05004], lr: 0.018982, loss: 1.2694
2022-07-29 21:05:05 - train: epoch 0082, iter [00400, 05004], lr: 0.018943, loss: 1.2038
2022-07-29 21:06:57 - train: epoch 0082, iter [00500, 05004], lr: 0.018905, loss: 1.3287
2022-07-29 21:08:56 - train: epoch 0082, iter [00600, 05004], lr: 0.018866, loss: 1.1241
2022-07-29 21:10:52 - train: epoch 0082, iter [00700, 05004], lr: 0.018827, loss: 1.2759
2022-07-29 21:12:43 - train: epoch 0082, iter [00800, 05004], lr: 0.018789, loss: 1.3216
2022-07-29 21:14:38 - train: epoch 0082, iter [00900, 05004], lr: 0.018750, loss: 1.2862
2022-07-29 21:16:33 - train: epoch 0082, iter [01000, 05004], lr: 0.018712, loss: 1.1796
2022-07-29 21:18:30 - train: epoch 0082, iter [01100, 05004], lr: 0.018673, loss: 1.2991
2022-07-29 21:20:26 - train: epoch 0082, iter [01200, 05004], lr: 0.018635, loss: 1.2653
2022-07-29 21:22:25 - train: epoch 0082, iter [01300, 05004], lr: 0.018596, loss: 1.2950
2022-07-29 21:24:23 - train: epoch 0082, iter [01400, 05004], lr: 0.018558, loss: 1.2890
2022-07-29 21:26:21 - train: epoch 0082, iter [01500, 05004], lr: 0.018520, loss: 1.1558
2022-07-29 21:28:17 - train: epoch 0082, iter [01600, 05004], lr: 0.018481, loss: 1.1586
2022-07-29 21:30:13 - train: epoch 0082, iter [01700, 05004], lr: 0.018443, loss: 1.1063
2022-07-29 21:32:07 - train: epoch 0082, iter [01800, 05004], lr: 0.018405, loss: 1.1434
2022-07-29 21:33:58 - train: epoch 0082, iter [01900, 05004], lr: 0.018367, loss: 1.2224
2022-07-29 21:35:55 - train: epoch 0082, iter [02000, 05004], lr: 0.018329, loss: 1.1151
2022-07-29 21:37:47 - train: epoch 0082, iter [02100, 05004], lr: 0.018290, loss: 1.2158
2022-07-29 21:39:43 - train: epoch 0082, iter [02200, 05004], lr: 0.018252, loss: 1.3182
2022-07-29 21:41:38 - train: epoch 0082, iter [02300, 05004], lr: 0.018214, loss: 1.2499
2022-07-29 21:43:34 - train: epoch 0082, iter [02400, 05004], lr: 0.018176, loss: 1.5454
2022-07-29 21:45:30 - train: epoch 0082, iter [02500, 05004], lr: 0.018138, loss: 1.0756
2022-07-29 21:47:22 - train: epoch 0082, iter [02600, 05004], lr: 0.018100, loss: 1.1540
2022-07-29 21:49:20 - train: epoch 0082, iter [02700, 05004], lr: 0.018062, loss: 1.2195
2022-07-29 21:51:14 - train: epoch 0082, iter [02800, 05004], lr: 0.018025, loss: 1.2493
2022-07-29 21:53:10 - train: epoch 0082, iter [02900, 05004], lr: 0.017987, loss: 1.1470
2022-07-29 21:55:06 - train: epoch 0082, iter [03000, 05004], lr: 0.017949, loss: 1.3320
2022-07-29 21:56:59 - train: epoch 0082, iter [03100, 05004], lr: 0.017911, loss: 1.2256
2022-07-29 21:58:54 - train: epoch 0082, iter [03200, 05004], lr: 0.017873, loss: 1.4494
2022-07-29 22:00:49 - train: epoch 0082, iter [03300, 05004], lr: 0.017836, loss: 1.1684
2022-07-29 22:02:38 - train: epoch 0082, iter [03400, 05004], lr: 0.017798, loss: 1.2232
2022-07-29 22:04:37 - train: epoch 0082, iter [03500, 05004], lr: 0.017761, loss: 1.1191
2022-07-29 22:06:30 - train: epoch 0082, iter [03600, 05004], lr: 0.017723, loss: 1.1812
2022-07-29 22:08:23 - train: epoch 0082, iter [03700, 05004], lr: 0.017685, loss: 1.2807
2022-07-29 22:10:22 - train: epoch 0082, iter [03800, 05004], lr: 0.017648, loss: 1.3647
2022-07-29 22:12:20 - train: epoch 0082, iter [03900, 05004], lr: 0.017610, loss: 1.2657
2022-07-29 22:14:12 - train: epoch 0082, iter [04000, 05004], lr: 0.017573, loss: 1.0972
2022-07-29 22:16:06 - train: epoch 0082, iter [04100, 05004], lr: 0.017536, loss: 1.2029
2022-07-29 22:17:59 - train: epoch 0082, iter [04200, 05004], lr: 0.017498, loss: 1.2150
2022-07-29 22:19:54 - train: epoch 0082, iter [04300, 05004], lr: 0.017461, loss: 1.1570
2022-07-29 22:21:50 - train: epoch 0082, iter [04400, 05004], lr: 0.017424, loss: 1.0471
2022-07-29 22:23:44 - train: epoch 0082, iter [04500, 05004], lr: 0.017386, loss: 1.1273
2022-07-29 22:25:40 - train: epoch 0082, iter [04600, 05004], lr: 0.017349, loss: 1.2940
2022-07-29 22:27:36 - train: epoch 0082, iter [04700, 05004], lr: 0.017312, loss: 1.0780
2022-07-29 22:29:32 - train: epoch 0082, iter [04800, 05004], lr: 0.017275, loss: 1.1041
2022-07-29 22:31:46 - train: epoch 0082, iter [04900, 05004], lr: 0.017238, loss: 1.3014
2022-07-29 22:33:34 - train: epoch 0082, iter [05000, 05004], lr: 0.017201, loss: 1.3594
2022-07-29 22:33:39 - train: epoch 082, train_loss: 1.2212
2022-07-29 22:37:20 - eval: epoch: 082, acc1: 71.282%, acc5: 90.436%, test_loss: 1.1630, per_image_load_time: 7.501ms, per_image_inference_time: 0.869ms
2022-07-29 22:37:21 - until epoch: 082, best_acc1: 71.282%
2022-07-29 22:37:21 - epoch 083 lr: 0.017199
2022-07-29 22:39:41 - train: epoch 0083, iter [00100, 05004], lr: 0.017162, loss: 1.1639
2022-07-29 22:41:46 - train: epoch 0083, iter [00200, 05004], lr: 0.017125, loss: 0.9787
2022-07-29 22:43:43 - train: epoch 0083, iter [00300, 05004], lr: 0.017088, loss: 1.3018
2022-07-29 22:45:41 - train: epoch 0083, iter [00400, 05004], lr: 0.017051, loss: 1.1905
2022-07-29 22:47:36 - train: epoch 0083, iter [00500, 05004], lr: 0.017014, loss: 1.0749
2022-07-29 22:49:30 - train: epoch 0083, iter [00600, 05004], lr: 0.016977, loss: 1.2007
2022-07-29 22:51:28 - train: epoch 0083, iter [00700, 05004], lr: 0.016941, loss: 0.9620
2022-07-29 22:53:23 - train: epoch 0083, iter [00800, 05004], lr: 0.016904, loss: 1.1016
2022-07-29 22:55:24 - train: epoch 0083, iter [00900, 05004], lr: 0.016867, loss: 1.3601
2022-07-29 22:57:21 - train: epoch 0083, iter [01000, 05004], lr: 0.016830, loss: 1.0787
2022-07-29 22:59:19 - train: epoch 0083, iter [01100, 05004], lr: 0.016794, loss: 1.1940
2022-07-29 23:01:19 - train: epoch 0083, iter [01200, 05004], lr: 0.016757, loss: 1.0631
2022-07-29 23:03:13 - train: epoch 0083, iter [01300, 05004], lr: 0.016720, loss: 1.1171
2022-07-29 23:05:17 - train: epoch 0083, iter [01400, 05004], lr: 0.016684, loss: 1.4458
2022-07-29 23:07:15 - train: epoch 0083, iter [01500, 05004], lr: 0.016647, loss: 0.9829
2022-07-29 23:09:13 - train: epoch 0083, iter [01600, 05004], lr: 0.016611, loss: 1.1778
2022-07-29 23:11:07 - train: epoch 0083, iter [01700, 05004], lr: 0.016574, loss: 1.1611
2022-07-29 23:13:01 - train: epoch 0083, iter [01800, 05004], lr: 0.016538, loss: 1.2995
2022-07-29 23:14:56 - train: epoch 0083, iter [01900, 05004], lr: 0.016502, loss: 1.1404
2022-07-29 23:16:55 - train: epoch 0083, iter [02000, 05004], lr: 0.016465, loss: 0.9160
2022-07-29 23:18:55 - train: epoch 0083, iter [02100, 05004], lr: 0.016429, loss: 1.0856
2022-07-29 23:20:50 - train: epoch 0083, iter [02200, 05004], lr: 0.016393, loss: 1.0694
2022-07-29 23:22:40 - train: epoch 0083, iter [02300, 05004], lr: 0.016356, loss: 1.0726
2022-07-29 23:24:35 - train: epoch 0083, iter [02400, 05004], lr: 0.016320, loss: 1.1567
2022-07-29 23:26:28 - train: epoch 0083, iter [02500, 05004], lr: 0.016284, loss: 1.0890
2022-07-29 23:28:22 - train: epoch 0083, iter [02600, 05004], lr: 0.016248, loss: 1.2469
2022-07-29 23:30:19 - train: epoch 0083, iter [02700, 05004], lr: 0.016212, loss: 1.1995
2022-07-29 23:32:13 - train: epoch 0083, iter [02800, 05004], lr: 0.016176, loss: 1.0568
2022-07-29 23:34:05 - train: epoch 0083, iter [02900, 05004], lr: 0.016140, loss: 1.3256
2022-07-29 23:36:01 - train: epoch 0083, iter [03000, 05004], lr: 0.016104, loss: 1.2939
2022-07-29 23:37:57 - train: epoch 0083, iter [03100, 05004], lr: 0.016068, loss: 1.1403
2022-07-29 23:39:52 - train: epoch 0083, iter [03200, 05004], lr: 0.016032, loss: 1.0525
2022-07-29 23:41:49 - train: epoch 0083, iter [03300, 05004], lr: 0.015996, loss: 1.2795
2022-07-29 23:43:45 - train: epoch 0083, iter [03400, 05004], lr: 0.015960, loss: 1.0882
2022-07-29 23:45:40 - train: epoch 0083, iter [03500, 05004], lr: 0.015924, loss: 1.2906
2022-07-29 23:47:34 - train: epoch 0083, iter [03600, 05004], lr: 0.015889, loss: 1.2351
2022-07-29 23:49:26 - train: epoch 0083, iter [03700, 05004], lr: 0.015853, loss: 1.1343
2022-07-29 23:51:24 - train: epoch 0083, iter [03800, 05004], lr: 0.015817, loss: 1.2247
2022-07-29 23:53:25 - train: epoch 0083, iter [03900, 05004], lr: 0.015782, loss: 1.1628
2022-07-29 23:55:26 - train: epoch 0083, iter [04000, 05004], lr: 0.015746, loss: 1.4461
2022-07-29 23:57:23 - train: epoch 0083, iter [04100, 05004], lr: 0.015710, loss: 1.1772
2022-07-29 23:59:16 - train: epoch 0083, iter [04200, 05004], lr: 0.015675, loss: 1.4449
2022-07-30 00:01:16 - train: epoch 0083, iter [04300, 05004], lr: 0.015639, loss: 1.1700
2022-07-30 00:03:10 - train: epoch 0083, iter [04400, 05004], lr: 0.015604, loss: 1.2313
2022-07-30 00:05:07 - train: epoch 0083, iter [04500, 05004], lr: 0.015568, loss: 1.2498
2022-07-30 00:07:03 - train: epoch 0083, iter [04600, 05004], lr: 0.015533, loss: 1.1449
2022-07-30 00:09:05 - train: epoch 0083, iter [04700, 05004], lr: 0.015498, loss: 1.6693
2022-07-30 00:11:00 - train: epoch 0083, iter [04800, 05004], lr: 0.015462, loss: 1.2590
2022-07-30 00:13:02 - train: epoch 0083, iter [04900, 05004], lr: 0.015427, loss: 1.1083
2022-07-30 00:15:09 - train: epoch 0083, iter [05000, 05004], lr: 0.015392, loss: 1.1419
2022-07-30 00:15:11 - train: epoch 083, train_loss: 1.1985
2022-07-30 00:18:58 - eval: epoch: 083, acc1: 71.636%, acc5: 90.380%, test_loss: 1.1495, per_image_load_time: 7.684ms, per_image_inference_time: 0.887ms
2022-07-30 00:18:59 - until epoch: 083, best_acc1: 71.636%
2022-07-30 00:18:59 - epoch 084 lr: 0.015390
2022-07-30 00:21:15 - train: epoch 0084, iter [00100, 05004], lr: 0.015355, loss: 1.1308
2022-07-30 00:23:15 - train: epoch 0084, iter [00200, 05004], lr: 0.015320, loss: 1.1165
2022-07-30 00:25:21 - train: epoch 0084, iter [00300, 05004], lr: 0.015285, loss: 1.0519
2022-07-30 00:27:22 - train: epoch 0084, iter [00400, 05004], lr: 0.015250, loss: 1.1325
2022-07-30 00:29:28 - train: epoch 0084, iter [00500, 05004], lr: 0.015215, loss: 1.2172
2022-07-30 00:31:25 - train: epoch 0084, iter [00600, 05004], lr: 0.015180, loss: 1.3038
2022-07-30 00:33:27 - train: epoch 0084, iter [00700, 05004], lr: 0.015145, loss: 1.1562
2022-07-30 00:35:28 - train: epoch 0084, iter [00800, 05004], lr: 0.015110, loss: 1.4245
2022-07-30 00:37:29 - train: epoch 0084, iter [00900, 05004], lr: 0.015075, loss: 1.2221
2022-07-30 00:39:27 - train: epoch 0084, iter [01000, 05004], lr: 0.015040, loss: 1.2749
2022-07-30 00:41:29 - train: epoch 0084, iter [01100, 05004], lr: 0.015005, loss: 1.0598
2022-07-30 00:43:29 - train: epoch 0084, iter [01200, 05004], lr: 0.014970, loss: 1.3985
2022-07-30 00:45:28 - train: epoch 0084, iter [01300, 05004], lr: 0.014936, loss: 1.2815
2022-07-30 00:47:29 - train: epoch 0084, iter [01400, 05004], lr: 0.014901, loss: 1.2408
2022-07-30 00:49:31 - train: epoch 0084, iter [01500, 05004], lr: 0.014866, loss: 1.1647
2022-07-30 00:51:30 - train: epoch 0084, iter [01600, 05004], lr: 0.014832, loss: 1.2430
2022-07-30 00:53:29 - train: epoch 0084, iter [01700, 05004], lr: 0.014797, loss: 1.3296
2022-07-30 00:55:25 - train: epoch 0084, iter [01800, 05004], lr: 0.014762, loss: 1.1531
2022-07-30 00:57:27 - train: epoch 0084, iter [01900, 05004], lr: 0.014728, loss: 1.3859
2022-07-30 00:59:24 - train: epoch 0084, iter [02000, 05004], lr: 0.014693, loss: 1.0594
2022-07-30 01:01:23 - train: epoch 0084, iter [02100, 05004], lr: 0.014659, loss: 1.1281
2022-07-30 01:03:23 - train: epoch 0084, iter [02200, 05004], lr: 0.014624, loss: 1.0054
2022-07-30 01:05:28 - train: epoch 0084, iter [02300, 05004], lr: 0.014590, loss: 1.0432
2022-07-30 01:07:26 - train: epoch 0084, iter [02400, 05004], lr: 0.014556, loss: 1.1069
2022-07-30 01:09:24 - train: epoch 0084, iter [02500, 05004], lr: 0.014521, loss: 1.3803
2022-07-30 01:11:22 - train: epoch 0084, iter [02600, 05004], lr: 0.014487, loss: 1.2271
2022-07-30 01:13:18 - train: epoch 0084, iter [02700, 05004], lr: 0.014453, loss: 1.0521
2022-07-30 01:15:14 - train: epoch 0084, iter [02800, 05004], lr: 0.014419, loss: 1.0688
2022-07-30 01:17:16 - train: epoch 0084, iter [02900, 05004], lr: 0.014385, loss: 1.1071
2022-07-30 01:19:13 - train: epoch 0084, iter [03000, 05004], lr: 0.014350, loss: 1.2550
2022-07-30 01:21:13 - train: epoch 0084, iter [03100, 05004], lr: 0.014316, loss: 1.2406
2022-07-30 01:23:16 - train: epoch 0084, iter [03200, 05004], lr: 0.014282, loss: 1.3207
2022-07-30 01:25:19 - train: epoch 0084, iter [03300, 05004], lr: 0.014248, loss: 1.1342
2022-07-30 01:27:18 - train: epoch 0084, iter [03400, 05004], lr: 0.014214, loss: 1.1495
2022-07-30 01:29:17 - train: epoch 0084, iter [03500, 05004], lr: 0.014180, loss: 1.2576
2022-07-30 01:31:08 - train: epoch 0084, iter [03600, 05004], lr: 0.014146, loss: 1.1655
2022-07-30 01:33:09 - train: epoch 0084, iter [03700, 05004], lr: 0.014113, loss: 1.1953
2022-07-30 01:35:13 - train: epoch 0084, iter [03800, 05004], lr: 0.014079, loss: 1.2191
2022-07-30 01:37:16 - train: epoch 0084, iter [03900, 05004], lr: 0.014045, loss: 1.1820
2022-07-30 01:39:14 - train: epoch 0084, iter [04000, 05004], lr: 0.014011, loss: 1.0238
2022-07-30 01:41:21 - train: epoch 0084, iter [04100, 05004], lr: 0.013977, loss: 1.0674
2022-07-30 01:43:19 - train: epoch 0084, iter [04200, 05004], lr: 0.013944, loss: 1.1650
2022-07-30 01:45:19 - train: epoch 0084, iter [04300, 05004], lr: 0.013910, loss: 1.3111
2022-07-30 01:47:16 - train: epoch 0084, iter [04400, 05004], lr: 0.013877, loss: 1.4242
2022-07-30 01:49:14 - train: epoch 0084, iter [04500, 05004], lr: 0.013843, loss: 1.3905
2022-07-30 01:51:14 - train: epoch 0084, iter [04600, 05004], lr: 0.013809, loss: 1.0940
2022-07-30 01:53:09 - train: epoch 0084, iter [04700, 05004], lr: 0.013776, loss: 1.1899
2022-07-30 01:55:00 - train: epoch 0084, iter [04800, 05004], lr: 0.013742, loss: 1.1480
2022-07-30 01:57:12 - train: epoch 0084, iter [04900, 05004], lr: 0.013709, loss: 1.2027
2022-07-30 01:59:00 - train: epoch 0084, iter [05000, 05004], lr: 0.013676, loss: 1.1515
2022-07-30 01:59:06 - train: epoch 084, train_loss: 1.1675
2022-07-30 02:02:53 - eval: epoch: 084, acc1: 72.004%, acc5: 90.758%, test_loss: 1.1329, per_image_load_time: 6.493ms, per_image_inference_time: 0.863ms
2022-07-30 02:02:54 - until epoch: 084, best_acc1: 72.004%
2022-07-30 02:02:54 - epoch 085 lr: 0.013674
2022-07-30 02:05:10 - train: epoch 0085, iter [00100, 05004], lr: 0.013641, loss: 1.0202
2022-07-30 02:07:06 - train: epoch 0085, iter [00200, 05004], lr: 0.013608, loss: 1.1189
2022-07-30 02:09:08 - train: epoch 0085, iter [00300, 05004], lr: 0.013574, loss: 1.1572
2022-07-30 02:11:06 - train: epoch 0085, iter [00400, 05004], lr: 0.013541, loss: 1.1371
2022-07-30 02:13:07 - train: epoch 0085, iter [00500, 05004], lr: 0.013508, loss: 1.1624
2022-07-30 02:15:07 - train: epoch 0085, iter [00600, 05004], lr: 0.013475, loss: 0.8412
2022-07-30 02:17:00 - train: epoch 0085, iter [00700, 05004], lr: 0.013442, loss: 1.3090
2022-07-30 02:18:56 - train: epoch 0085, iter [00800, 05004], lr: 0.013409, loss: 1.0300
2022-07-30 02:20:52 - train: epoch 0085, iter [00900, 05004], lr: 0.013376, loss: 1.1249
2022-07-30 02:22:52 - train: epoch 0085, iter [01000, 05004], lr: 0.013343, loss: 1.0326
2022-07-30 02:24:52 - train: epoch 0085, iter [01100, 05004], lr: 0.013310, loss: 1.1932
2022-07-30 02:26:53 - train: epoch 0085, iter [01200, 05004], lr: 0.013277, loss: 1.1152
2022-07-30 02:28:55 - train: epoch 0085, iter [01300, 05004], lr: 0.013244, loss: 1.2136
2022-07-30 02:30:48 - train: epoch 0085, iter [01400, 05004], lr: 0.013211, loss: 1.1007
2022-07-30 02:32:48 - train: epoch 0085, iter [01500, 05004], lr: 0.013178, loss: 1.3860
2022-07-30 02:34:46 - train: epoch 0085, iter [01600, 05004], lr: 0.013145, loss: 0.9929
2022-07-30 02:36:41 - train: epoch 0085, iter [01700, 05004], lr: 0.013113, loss: 1.3106
2022-07-30 02:38:39 - train: epoch 0085, iter [01800, 05004], lr: 0.013080, loss: 1.0144
2022-07-30 02:40:39 - train: epoch 0085, iter [01900, 05004], lr: 0.013047, loss: 1.2315
2022-07-30 02:42:35 - train: epoch 0085, iter [02000, 05004], lr: 0.013015, loss: 1.0613
2022-07-30 02:44:32 - train: epoch 0085, iter [02100, 05004], lr: 0.012982, loss: 1.0133
2022-07-30 02:46:27 - train: epoch 0085, iter [02200, 05004], lr: 0.012950, loss: 1.2387
2022-07-30 02:48:27 - train: epoch 0085, iter [02300, 05004], lr: 0.012917, loss: 1.1308
2022-07-30 02:50:27 - train: epoch 0085, iter [02400, 05004], lr: 0.012885, loss: 1.1905
2022-07-30 02:52:25 - train: epoch 0085, iter [02500, 05004], lr: 0.012852, loss: 1.3644
2022-07-30 02:54:22 - train: epoch 0085, iter [02600, 05004], lr: 0.012820, loss: 1.2090
2022-07-30 02:56:23 - train: epoch 0085, iter [02700, 05004], lr: 0.012787, loss: 1.2069
2022-07-30 02:58:19 - train: epoch 0085, iter [02800, 05004], lr: 0.012755, loss: 1.2876
2022-07-30 03:00:20 - train: epoch 0085, iter [02900, 05004], lr: 0.012723, loss: 1.2200
2022-07-30 03:02:15 - train: epoch 0085, iter [03000, 05004], lr: 0.012691, loss: 1.2016
2022-07-30 03:04:20 - train: epoch 0085, iter [03100, 05004], lr: 0.012658, loss: 1.1972
2022-07-30 03:06:18 - train: epoch 0085, iter [03200, 05004], lr: 0.012626, loss: 1.1773
2022-07-30 03:08:13 - train: epoch 0085, iter [03300, 05004], lr: 0.012594, loss: 1.2059
2022-07-30 03:10:09 - train: epoch 0085, iter [03400, 05004], lr: 0.012562, loss: 1.3557
2022-07-30 03:12:12 - train: epoch 0085, iter [03500, 05004], lr: 0.012530, loss: 1.3674
2022-07-30 03:14:09 - train: epoch 0085, iter [03600, 05004], lr: 0.012498, loss: 1.2310
2022-07-30 03:16:07 - train: epoch 0085, iter [03700, 05004], lr: 0.012466, loss: 1.0339
2022-07-30 03:18:02 - train: epoch 0085, iter [03800, 05004], lr: 0.012434, loss: 1.2451
2022-07-30 03:20:01 - train: epoch 0085, iter [03900, 05004], lr: 0.012402, loss: 0.9918
2022-07-30 03:22:00 - train: epoch 0085, iter [04000, 05004], lr: 0.012370, loss: 1.3976
2022-07-30 03:23:58 - train: epoch 0085, iter [04100, 05004], lr: 0.012339, loss: 1.1970
2022-07-30 03:25:50 - train: epoch 0085, iter [04200, 05004], lr: 0.012307, loss: 1.0999
2022-07-30 03:27:50 - train: epoch 0085, iter [04300, 05004], lr: 0.012275, loss: 1.0904
2022-07-30 03:29:47 - train: epoch 0085, iter [04400, 05004], lr: 0.012243, loss: 1.2647
2022-07-30 03:31:46 - train: epoch 0085, iter [04500, 05004], lr: 0.012212, loss: 1.2904
2022-07-30 03:33:40 - train: epoch 0085, iter [04600, 05004], lr: 0.012180, loss: 1.2545
2022-07-30 03:35:38 - train: epoch 0085, iter [04700, 05004], lr: 0.012148, loss: 1.5000
2022-07-30 03:37:36 - train: epoch 0085, iter [04800, 05004], lr: 0.012117, loss: 1.1291
2022-07-30 03:39:25 - train: epoch 0085, iter [04900, 05004], lr: 0.012085, loss: 1.1590
2022-07-30 03:41:25 - train: epoch 0085, iter [05000, 05004], lr: 0.012054, loss: 0.9266
2022-07-30 03:41:30 - train: epoch 085, train_loss: 1.1456
2022-07-30 03:45:11 - eval: epoch: 085, acc1: 72.270%, acc5: 90.838%, test_loss: 1.1254, per_image_load_time: 7.657ms, per_image_inference_time: 0.809ms
2022-07-30 03:45:12 - until epoch: 085, best_acc1: 72.270%
2022-07-30 03:45:12 - epoch 086 lr: 0.012052
2022-07-30 03:47:27 - train: epoch 0086, iter [00100, 05004], lr: 0.012021, loss: 1.1522
2022-07-30 03:49:23 - train: epoch 0086, iter [00200, 05004], lr: 0.011990, loss: 1.0627
2022-07-30 03:51:18 - train: epoch 0086, iter [00300, 05004], lr: 0.011958, loss: 1.4130
2022-07-30 03:53:11 - train: epoch 0086, iter [00400, 05004], lr: 0.011927, loss: 1.2493
2022-07-30 03:55:08 - train: epoch 0086, iter [00500, 05004], lr: 0.011896, loss: 1.0688
2022-07-30 03:57:08 - train: epoch 0086, iter [00600, 05004], lr: 0.011865, loss: 1.1951
2022-07-30 03:59:01 - train: epoch 0086, iter [00700, 05004], lr: 0.011833, loss: 0.9471
2022-07-30 04:00:59 - train: epoch 0086, iter [00800, 05004], lr: 0.011802, loss: 1.2351
2022-07-30 04:02:56 - train: epoch 0086, iter [00900, 05004], lr: 0.011771, loss: 1.0004
2022-07-30 04:04:47 - train: epoch 0086, iter [01000, 05004], lr: 0.011740, loss: 1.1374
2022-07-30 04:06:47 - train: epoch 0086, iter [01100, 05004], lr: 0.011709, loss: 1.0671
2022-07-30 04:08:41 - train: epoch 0086, iter [01200, 05004], lr: 0.011678, loss: 1.0295
2022-07-30 04:10:36 - train: epoch 0086, iter [01300, 05004], lr: 0.011647, loss: 1.2491
2022-07-30 04:12:33 - train: epoch 0086, iter [01400, 05004], lr: 0.011616, loss: 1.1186
2022-07-30 04:14:34 - train: epoch 0086, iter [01500, 05004], lr: 0.011585, loss: 0.9197
2022-07-30 04:16:29 - train: epoch 0086, iter [01600, 05004], lr: 0.011554, loss: 1.0579
2022-07-30 04:18:26 - train: epoch 0086, iter [01700, 05004], lr: 0.011523, loss: 1.1679
2022-07-30 04:20:19 - train: epoch 0086, iter [01800, 05004], lr: 0.011493, loss: 1.2011
2022-07-30 04:22:16 - train: epoch 0086, iter [01900, 05004], lr: 0.011462, loss: 1.2273
2022-07-30 04:24:15 - train: epoch 0086, iter [02000, 05004], lr: 0.011431, loss: 1.3003
2022-07-30 04:26:07 - train: epoch 0086, iter [02100, 05004], lr: 0.011401, loss: 0.8041
2022-07-30 04:28:04 - train: epoch 0086, iter [02200, 05004], lr: 0.011370, loss: 1.0499
2022-07-30 04:29:58 - train: epoch 0086, iter [02300, 05004], lr: 0.011339, loss: 1.1962
2022-07-30 04:32:00 - train: epoch 0086, iter [02400, 05004], lr: 0.011309, loss: 1.0709
2022-07-30 04:34:00 - train: epoch 0086, iter [02500, 05004], lr: 0.011278, loss: 1.1183
2022-07-30 04:35:51 - train: epoch 0086, iter [02600, 05004], lr: 0.011248, loss: 1.1147
2022-07-30 04:37:49 - train: epoch 0086, iter [02700, 05004], lr: 0.011217, loss: 1.0783
2022-07-30 04:39:46 - train: epoch 0086, iter [02800, 05004], lr: 0.011187, loss: 1.1154
2022-07-30 04:41:48 - train: epoch 0086, iter [02900, 05004], lr: 0.011157, loss: 0.9466
2022-07-30 04:43:45 - train: epoch 0086, iter [03000, 05004], lr: 0.011126, loss: 0.9419
2022-07-30 04:45:41 - train: epoch 0086, iter [03100, 05004], lr: 0.011096, loss: 1.1083
2022-07-30 04:47:39 - train: epoch 0086, iter [03200, 05004], lr: 0.011066, loss: 1.1579
2022-07-30 04:49:38 - train: epoch 0086, iter [03300, 05004], lr: 0.011036, loss: 1.1025
2022-07-30 04:51:35 - train: epoch 0086, iter [03400, 05004], lr: 0.011005, loss: 1.0768
2022-07-30 04:53:34 - train: epoch 0086, iter [03500, 05004], lr: 0.010975, loss: 1.2712
2022-07-30 04:55:34 - train: epoch 0086, iter [03600, 05004], lr: 0.010945, loss: 1.1440
2022-07-30 04:57:43 - train: epoch 0086, iter [03700, 05004], lr: 0.010915, loss: 1.1731
2022-07-30 04:59:44 - train: epoch 0086, iter [03800, 05004], lr: 0.010885, loss: 1.0126
2022-07-30 05:01:44 - train: epoch 0086, iter [03900, 05004], lr: 0.010855, loss: 1.1978
2022-07-30 05:03:47 - train: epoch 0086, iter [04000, 05004], lr: 0.010825, loss: 1.0729
2022-07-30 05:05:53 - train: epoch 0086, iter [04100, 05004], lr: 0.010795, loss: 1.1060
2022-07-30 05:07:51 - train: epoch 0086, iter [04200, 05004], lr: 0.010766, loss: 1.2624
2022-07-30 05:09:56 - train: epoch 0086, iter [04300, 05004], lr: 0.010736, loss: 1.3323
2022-07-30 05:11:56 - train: epoch 0086, iter [04400, 05004], lr: 0.010706, loss: 1.1454
2022-07-30 05:13:54 - train: epoch 0086, iter [04500, 05004], lr: 0.010676, loss: 0.9398
2022-07-30 05:16:01 - train: epoch 0086, iter [04600, 05004], lr: 0.010647, loss: 1.0706
2022-07-30 05:18:01 - train: epoch 0086, iter [04700, 05004], lr: 0.010617, loss: 1.1267
2022-07-30 05:20:11 - train: epoch 0086, iter [04800, 05004], lr: 0.010587, loss: 1.1356
2022-07-30 05:22:13 - train: epoch 0086, iter [04900, 05004], lr: 0.010558, loss: 1.1458
2022-07-30 05:24:01 - train: epoch 0086, iter [05000, 05004], lr: 0.010528, loss: 1.1297
2022-07-30 05:24:07 - train: epoch 086, train_loss: 1.1176
2022-07-30 05:28:00 - eval: epoch: 086, acc1: 72.428%, acc5: 91.054%, test_loss: 1.1150, per_image_load_time: 7.544ms, per_image_inference_time: 0.834ms
2022-07-30 05:28:00 - until epoch: 086, best_acc1: 72.428%
2022-07-30 05:28:00 - epoch 087 lr: 0.010527
2022-07-30 05:30:24 - train: epoch 0087, iter [00100, 05004], lr: 0.010498, loss: 1.0396
2022-07-30 05:32:30 - train: epoch 0087, iter [00200, 05004], lr: 0.010468, loss: 0.9968
2022-07-30 05:34:33 - train: epoch 0087, iter [00300, 05004], lr: 0.010439, loss: 1.0721
2022-07-30 05:36:36 - train: epoch 0087, iter [00400, 05004], lr: 0.010409, loss: 1.1998
2022-07-30 05:38:31 - train: epoch 0087, iter [00500, 05004], lr: 0.010380, loss: 0.9186
2022-07-30 05:40:37 - train: epoch 0087, iter [00600, 05004], lr: 0.010351, loss: 1.1243
2022-07-30 05:42:39 - train: epoch 0087, iter [00700, 05004], lr: 0.010321, loss: 1.0506
2022-07-30 05:44:43 - train: epoch 0087, iter [00800, 05004], lr: 0.010292, loss: 1.0648
2022-07-30 05:46:43 - train: epoch 0087, iter [00900, 05004], lr: 0.010263, loss: 1.0465
2022-07-30 05:48:50 - train: epoch 0087, iter [01000, 05004], lr: 0.010234, loss: 1.0139
2022-07-30 05:50:44 - train: epoch 0087, iter [01100, 05004], lr: 0.010205, loss: 1.0895
2022-07-30 05:52:49 - train: epoch 0087, iter [01200, 05004], lr: 0.010176, loss: 1.0764
2022-07-30 05:54:51 - train: epoch 0087, iter [01300, 05004], lr: 0.010147, loss: 1.2628
2022-07-30 05:56:54 - train: epoch 0087, iter [01400, 05004], lr: 0.010118, loss: 1.0079
2022-07-30 05:58:55 - train: epoch 0087, iter [01500, 05004], lr: 0.010089, loss: 1.0668
2022-07-30 06:00:57 - train: epoch 0087, iter [01600, 05004], lr: 0.010060, loss: 0.9683
2022-07-30 06:03:00 - train: epoch 0087, iter [01700, 05004], lr: 0.010031, loss: 1.2102
2022-07-30 06:04:59 - train: epoch 0087, iter [01800, 05004], lr: 0.010002, loss: 1.3121
2022-07-30 06:07:00 - train: epoch 0087, iter [01900, 05004], lr: 0.009973, loss: 1.2396
2022-07-30 06:08:59 - train: epoch 0087, iter [02000, 05004], lr: 0.009945, loss: 1.1423
2022-07-30 06:11:05 - train: epoch 0087, iter [02100, 05004], lr: 0.009916, loss: 1.1180
2022-07-30 06:13:09 - train: epoch 0087, iter [02200, 05004], lr: 0.009887, loss: 1.0119
2022-07-30 06:15:12 - train: epoch 0087, iter [02300, 05004], lr: 0.009859, loss: 1.4037
2022-07-30 06:17:13 - train: epoch 0087, iter [02400, 05004], lr: 0.009830, loss: 0.9318
2022-07-30 06:19:15 - train: epoch 0087, iter [02500, 05004], lr: 0.009801, loss: 0.9080
2022-07-30 06:21:18 - train: epoch 0087, iter [02600, 05004], lr: 0.009773, loss: 0.9039
2022-07-30 06:23:21 - train: epoch 0087, iter [02700, 05004], lr: 0.009744, loss: 1.0014
2022-07-30 06:25:24 - train: epoch 0087, iter [02800, 05004], lr: 0.009716, loss: 1.0061
2022-07-30 06:27:26 - train: epoch 0087, iter [02900, 05004], lr: 0.009688, loss: 0.9994
2022-07-30 06:29:30 - train: epoch 0087, iter [03000, 05004], lr: 0.009659, loss: 1.1877
2022-07-30 06:31:30 - train: epoch 0087, iter [03100, 05004], lr: 0.009631, loss: 1.1395
2022-07-30 06:33:38 - train: epoch 0087, iter [03200, 05004], lr: 0.009603, loss: 1.0249
2022-07-30 06:35:42 - train: epoch 0087, iter [03300, 05004], lr: 0.009574, loss: 1.0699
2022-07-30 06:37:47 - train: epoch 0087, iter [03400, 05004], lr: 0.009546, loss: 1.0650
2022-07-30 06:39:43 - train: epoch 0087, iter [03500, 05004], lr: 0.009518, loss: 0.9807
2022-07-30 06:41:44 - train: epoch 0087, iter [03600, 05004], lr: 0.009490, loss: 0.9104
2022-07-30 06:43:51 - train: epoch 0087, iter [03700, 05004], lr: 0.009462, loss: 1.1267
2022-07-30 06:45:50 - train: epoch 0087, iter [03800, 05004], lr: 0.009434, loss: 1.0094
2022-07-30 06:47:53 - train: epoch 0087, iter [03900, 05004], lr: 0.009406, loss: 1.1074
2022-07-30 06:49:54 - train: epoch 0087, iter [04000, 05004], lr: 0.009378, loss: 1.1486
2022-07-30 06:51:58 - train: epoch 0087, iter [04100, 05004], lr: 0.009350, loss: 1.0873
2022-07-30 06:54:00 - train: epoch 0087, iter [04200, 05004], lr: 0.009322, loss: 1.1592
2022-07-30 06:56:00 - train: epoch 0087, iter [04300, 05004], lr: 0.009294, loss: 1.0524
2022-07-30 06:58:05 - train: epoch 0087, iter [04400, 05004], lr: 0.009266, loss: 1.0592
2022-07-30 07:00:08 - train: epoch 0087, iter [04500, 05004], lr: 0.009239, loss: 1.0540
2022-07-30 07:02:11 - train: epoch 0087, iter [04600, 05004], lr: 0.009211, loss: 1.2338
2022-07-30 07:04:15 - train: epoch 0087, iter [04700, 05004], lr: 0.009183, loss: 1.0506
2022-07-30 07:06:22 - train: epoch 0087, iter [04800, 05004], lr: 0.009156, loss: 1.0120
2022-07-30 07:08:26 - train: epoch 0087, iter [04900, 05004], lr: 0.009128, loss: 0.9978
2022-07-30 07:10:08 - train: epoch 0087, iter [05000, 05004], lr: 0.009100, loss: 1.0852
2022-07-30 07:10:12 - train: epoch 087, train_loss: 1.0916
2022-07-30 07:14:25 - eval: epoch: 087, acc1: 72.824%, acc5: 91.280%, test_loss: 1.1067, per_image_load_time: 4.780ms, per_image_inference_time: 0.877ms
2022-07-30 07:14:25 - until epoch: 087, best_acc1: 72.824%
2022-07-30 07:14:25 - epoch 088 lr: 0.009099
