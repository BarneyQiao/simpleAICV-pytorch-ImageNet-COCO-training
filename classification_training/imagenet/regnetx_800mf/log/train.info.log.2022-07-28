2022-07-28 07:13:26 - train: epoch 0059, iter [03100, 05004], lr: 0.079901, loss: 1.6126
2022-07-28 07:15:39 - train: epoch 0059, iter [03200, 05004], lr: 0.079836, loss: 1.9143
2022-07-28 07:17:46 - train: epoch 0059, iter [03300, 05004], lr: 0.079772, loss: 1.5021
2022-07-28 07:19:56 - train: epoch 0059, iter [03400, 05004], lr: 0.079707, loss: 2.0580
2022-07-28 07:22:03 - train: epoch 0059, iter [03500, 05004], lr: 0.079642, loss: 1.6765
2022-07-28 07:24:12 - train: epoch 0059, iter [03600, 05004], lr: 0.079577, loss: 1.8169
2022-07-28 07:26:20 - train: epoch 0059, iter [03700, 05004], lr: 0.079513, loss: 1.5512
2022-07-28 07:28:24 - train: epoch 0059, iter [03800, 05004], lr: 0.079448, loss: 1.7830
2022-07-28 07:30:30 - train: epoch 0059, iter [03900, 05004], lr: 0.079383, loss: 1.6551
2022-07-28 07:32:39 - train: epoch 0059, iter [04000, 05004], lr: 0.079319, loss: 1.7987
2022-07-28 07:34:36 - train: epoch 0059, iter [04100, 05004], lr: 0.079254, loss: 1.5555
2022-07-28 07:36:36 - train: epoch 0059, iter [04200, 05004], lr: 0.079189, loss: 1.6029
2022-07-28 07:38:30 - train: epoch 0059, iter [04300, 05004], lr: 0.079125, loss: 1.8603
2022-07-28 07:40:28 - train: epoch 0059, iter [04400, 05004], lr: 0.079060, loss: 1.5884
2022-07-28 07:42:28 - train: epoch 0059, iter [04500, 05004], lr: 0.078996, loss: 1.6968
2022-07-28 07:44:26 - train: epoch 0059, iter [04600, 05004], lr: 0.078931, loss: 1.8572
2022-07-28 07:46:23 - train: epoch 0059, iter [04700, 05004], lr: 0.078866, loss: 1.5811
2022-07-28 07:48:21 - train: epoch 0059, iter [04800, 05004], lr: 0.078802, loss: 1.6565
2022-07-28 07:50:22 - train: epoch 0059, iter [04900, 05004], lr: 0.078737, loss: 1.8442
2022-07-28 07:52:15 - train: epoch 0059, iter [05000, 05004], lr: 0.078673, loss: 1.8732
2022-07-28 07:52:18 - train: epoch 059, train_loss: 1.6694
2022-07-28 07:56:24 - eval: epoch: 059, acc1: 64.802%, acc5: 86.756%, test_loss: 1.4453, per_image_load_time: 8.550ms, per_image_inference_time: 0.791ms
2022-07-28 07:56:24 - until epoch: 059, best_acc1: 64.810%
2022-07-28 07:56:24 - epoch 060 lr: 0.078669
2022-07-28 07:58:41 - train: epoch 0060, iter [00100, 05004], lr: 0.078605, loss: 1.6062
2022-07-28 08:00:32 - train: epoch 0060, iter [00200, 05004], lr: 0.078541, loss: 1.5313
2022-07-28 08:02:29 - train: epoch 0060, iter [00300, 05004], lr: 0.078476, loss: 1.4663
2022-07-28 08:04:22 - train: epoch 0060, iter [00400, 05004], lr: 0.078412, loss: 1.8031
2022-07-28 08:06:24 - train: epoch 0060, iter [00500, 05004], lr: 0.078347, loss: 1.7683
2022-07-28 08:08:21 - train: epoch 0060, iter [00600, 05004], lr: 0.078283, loss: 1.6973
2022-07-28 08:10:18 - train: epoch 0060, iter [00700, 05004], lr: 0.078218, loss: 1.6982
2022-07-28 08:12:18 - train: epoch 0060, iter [00800, 05004], lr: 0.078154, loss: 1.6780
2022-07-28 08:14:16 - train: epoch 0060, iter [00900, 05004], lr: 0.078089, loss: 1.5943
2022-07-28 08:16:19 - train: epoch 0060, iter [01000, 05004], lr: 0.078025, loss: 1.2576
2022-07-28 08:18:16 - train: epoch 0060, iter [01100, 05004], lr: 0.077960, loss: 1.6022
2022-07-28 08:20:15 - train: epoch 0060, iter [01200, 05004], lr: 0.077896, loss: 1.3993
2022-07-28 08:22:12 - train: epoch 0060, iter [01300, 05004], lr: 0.077831, loss: 1.5653
2022-07-28 08:24:14 - train: epoch 0060, iter [01400, 05004], lr: 0.077767, loss: 1.7685
2022-07-28 08:26:13 - train: epoch 0060, iter [01500, 05004], lr: 0.077703, loss: 1.9144
2022-07-28 08:28:15 - train: epoch 0060, iter [01600, 05004], lr: 0.077638, loss: 1.5198
2022-07-28 08:30:29 - train: epoch 0060, iter [01700, 05004], lr: 0.077574, loss: 1.7388
2022-07-28 08:32:38 - train: epoch 0060, iter [01800, 05004], lr: 0.077509, loss: 1.5621
2022-07-28 08:34:48 - train: epoch 0060, iter [01900, 05004], lr: 0.077445, loss: 1.9721
2022-07-28 08:37:01 - train: epoch 0060, iter [02000, 05004], lr: 0.077381, loss: 1.6090
2022-07-28 08:39:11 - train: epoch 0060, iter [02100, 05004], lr: 0.077316, loss: 1.6813
2022-07-28 08:41:25 - train: epoch 0060, iter [02200, 05004], lr: 0.077252, loss: 1.8126
2022-07-28 08:43:30 - train: epoch 0060, iter [02300, 05004], lr: 0.077188, loss: 1.5159
2022-07-28 08:45:34 - train: epoch 0060, iter [02400, 05004], lr: 0.077123, loss: 1.6318
2022-07-28 08:47:43 - train: epoch 0060, iter [02500, 05004], lr: 0.077059, loss: 1.5349
2022-07-28 08:49:42 - train: epoch 0060, iter [02600, 05004], lr: 0.076995, loss: 1.7422
2022-07-28 08:52:05 - train: epoch 0060, iter [02700, 05004], lr: 0.076930, loss: 1.7929
2022-07-28 08:54:14 - train: epoch 0060, iter [02800, 05004], lr: 0.076866, loss: 1.8796
2022-07-28 08:56:21 - train: epoch 0060, iter [02900, 05004], lr: 0.076802, loss: 1.6886
2022-07-28 08:58:27 - train: epoch 0060, iter [03000, 05004], lr: 0.076737, loss: 1.8489
2022-07-28 09:00:33 - train: epoch 0060, iter [03100, 05004], lr: 0.076673, loss: 1.7517
2022-07-28 09:02:46 - train: epoch 0060, iter [03200, 05004], lr: 0.076609, loss: 1.5931
2022-07-28 09:04:53 - train: epoch 0060, iter [03300, 05004], lr: 0.076545, loss: 1.3728
2022-07-28 09:07:00 - train: epoch 0060, iter [03400, 05004], lr: 0.076480, loss: 1.7680
2022-07-28 09:09:08 - train: epoch 0060, iter [03500, 05004], lr: 0.076416, loss: 1.7745
2022-07-28 09:11:16 - train: epoch 0060, iter [03600, 05004], lr: 0.076352, loss: 1.9390
2022-07-28 09:13:12 - train: epoch 0060, iter [03700, 05004], lr: 0.076288, loss: 1.6725
2022-07-28 09:15:13 - train: epoch 0060, iter [03800, 05004], lr: 0.076224, loss: 1.8494
2022-07-28 09:17:15 - train: epoch 0060, iter [03900, 05004], lr: 0.076159, loss: 1.7706
2022-07-28 09:19:16 - train: epoch 0060, iter [04000, 05004], lr: 0.076095, loss: 1.7280
2022-07-28 09:21:13 - train: epoch 0060, iter [04100, 05004], lr: 0.076031, loss: 1.7482
2022-07-28 09:23:17 - train: epoch 0060, iter [04200, 05004], lr: 0.075967, loss: 1.5620
2022-07-28 09:25:19 - train: epoch 0060, iter [04300, 05004], lr: 0.075903, loss: 1.5178
2022-07-28 09:27:19 - train: epoch 0060, iter [04400, 05004], lr: 0.075839, loss: 1.8756
2022-07-28 09:29:15 - train: epoch 0060, iter [04500, 05004], lr: 0.075774, loss: 1.7458
2022-07-28 09:31:17 - train: epoch 0060, iter [04600, 05004], lr: 0.075710, loss: 1.5867
2022-07-28 09:33:19 - train: epoch 0060, iter [04700, 05004], lr: 0.075646, loss: 1.4930
2022-07-28 09:35:16 - train: epoch 0060, iter [04800, 05004], lr: 0.075582, loss: 1.8193
2022-07-28 09:37:12 - train: epoch 0060, iter [04900, 05004], lr: 0.075518, loss: 1.5122
2022-07-28 09:39:05 - train: epoch 0060, iter [05000, 05004], lr: 0.075454, loss: 1.6020
2022-07-28 09:39:08 - train: epoch 060, train_loss: 1.6544
2022-07-28 09:43:09 - eval: epoch: 060, acc1: 64.880%, acc5: 86.928%, test_loss: 1.4347, per_image_load_time: 8.577ms, per_image_inference_time: 0.786ms
2022-07-28 09:43:09 - until epoch: 060, best_acc1: 64.880%
2022-07-28 09:43:09 - epoch 061 lr: 0.075451
2022-07-28 09:45:19 - train: epoch 0061, iter [00100, 05004], lr: 0.075387, loss: 1.5278
2022-07-28 09:47:22 - train: epoch 0061, iter [00200, 05004], lr: 0.075323, loss: 1.5738
2022-07-28 09:49:20 - train: epoch 0061, iter [00300, 05004], lr: 0.075259, loss: 1.4566
2022-07-28 09:51:18 - train: epoch 0061, iter [00400, 05004], lr: 0.075195, loss: 1.8068
2022-07-28 09:53:16 - train: epoch 0061, iter [00500, 05004], lr: 0.075131, loss: 1.6963
2022-07-28 09:55:05 - train: epoch 0061, iter [00600, 05004], lr: 0.075067, loss: 1.6526
2022-07-28 09:57:06 - train: epoch 0061, iter [00700, 05004], lr: 0.075003, loss: 1.3243
2022-07-28 09:59:07 - train: epoch 0061, iter [00800, 05004], lr: 0.074939, loss: 1.5686
2022-07-28 10:01:03 - train: epoch 0061, iter [00900, 05004], lr: 0.074875, loss: 1.4773
2022-07-28 10:03:06 - train: epoch 0061, iter [01000, 05004], lr: 0.074811, loss: 1.5581
2022-07-28 10:05:05 - train: epoch 0061, iter [01100, 05004], lr: 0.074747, loss: 1.4033
2022-07-28 10:07:09 - train: epoch 0061, iter [01200, 05004], lr: 0.074683, loss: 1.3768
2022-07-28 10:09:03 - train: epoch 0061, iter [01300, 05004], lr: 0.074620, loss: 1.4680
2022-07-28 10:11:04 - train: epoch 0061, iter [01400, 05004], lr: 0.074556, loss: 1.6709
2022-07-28 10:12:58 - train: epoch 0061, iter [01500, 05004], lr: 0.074492, loss: 1.6107
2022-07-28 10:14:59 - train: epoch 0061, iter [01600, 05004], lr: 0.074428, loss: 1.4119
2022-07-28 10:16:54 - train: epoch 0061, iter [01700, 05004], lr: 0.074364, loss: 1.7073
2022-07-28 10:19:04 - train: epoch 0061, iter [01800, 05004], lr: 0.074300, loss: 1.6529
2022-07-28 10:20:56 - train: epoch 0061, iter [01900, 05004], lr: 0.074236, loss: 2.0541
2022-07-28 10:22:52 - train: epoch 0061, iter [02000, 05004], lr: 0.074172, loss: 1.5018
2022-07-28 10:24:44 - train: epoch 0061, iter [02100, 05004], lr: 0.074109, loss: 1.7092
2022-07-28 10:26:34 - train: epoch 0061, iter [02200, 05004], lr: 0.074045, loss: 1.5769
2022-07-28 10:28:27 - train: epoch 0061, iter [02300, 05004], lr: 0.073981, loss: 1.5733
2022-07-28 10:30:19 - train: epoch 0061, iter [02400, 05004], lr: 0.073917, loss: 1.5472
2022-07-28 10:32:20 - train: epoch 0061, iter [02500, 05004], lr: 0.073853, loss: 1.7186
2022-07-28 10:34:19 - train: epoch 0061, iter [02600, 05004], lr: 0.073790, loss: 1.7848
2022-07-28 10:36:17 - train: epoch 0061, iter [02700, 05004], lr: 0.073726, loss: 1.7613
2022-07-28 10:38:20 - train: epoch 0061, iter [02800, 05004], lr: 0.073662, loss: 1.5691
2022-07-28 10:40:33 - train: epoch 0061, iter [02900, 05004], lr: 0.073598, loss: 1.8568
2022-07-28 10:42:37 - train: epoch 0061, iter [03000, 05004], lr: 0.073534, loss: 1.5716
2022-07-28 10:44:48 - train: epoch 0061, iter [03100, 05004], lr: 0.073471, loss: 1.5507
2022-07-28 10:46:51 - train: epoch 0061, iter [03200, 05004], lr: 0.073407, loss: 1.7129
2022-07-28 10:48:49 - train: epoch 0061, iter [03300, 05004], lr: 0.073343, loss: 1.7818
2022-07-28 10:50:46 - train: epoch 0061, iter [03400, 05004], lr: 0.073280, loss: 1.4731
2022-07-28 10:52:43 - train: epoch 0061, iter [03500, 05004], lr: 0.073216, loss: 1.7123
2022-07-28 10:54:46 - train: epoch 0061, iter [03600, 05004], lr: 0.073152, loss: 1.5960
2022-07-28 10:56:45 - train: epoch 0061, iter [03700, 05004], lr: 0.073089, loss: 1.6122
2022-07-28 10:58:37 - train: epoch 0061, iter [03800, 05004], lr: 0.073025, loss: 1.6691
2022-07-28 11:00:33 - train: epoch 0061, iter [03900, 05004], lr: 0.072961, loss: 1.5241
2022-07-28 11:02:25 - train: epoch 0061, iter [04000, 05004], lr: 0.072898, loss: 1.6226
2022-07-28 11:04:25 - train: epoch 0061, iter [04100, 05004], lr: 0.072834, loss: 1.7240
2022-07-28 11:06:24 - train: epoch 0061, iter [04200, 05004], lr: 0.072771, loss: 1.5964
2022-07-28 11:08:26 - train: epoch 0061, iter [04300, 05004], lr: 0.072707, loss: 1.7297
2022-07-28 11:10:27 - train: epoch 0061, iter [04400, 05004], lr: 0.072643, loss: 1.7104
2022-07-28 11:12:30 - train: epoch 0061, iter [04500, 05004], lr: 0.072580, loss: 1.6921
2022-07-28 11:14:26 - train: epoch 0061, iter [04600, 05004], lr: 0.072516, loss: 1.6337
2022-07-28 11:16:18 - train: epoch 0061, iter [04700, 05004], lr: 0.072453, loss: 1.6504
2022-07-28 11:18:08 - train: epoch 0061, iter [04800, 05004], lr: 0.072389, loss: 1.7568
2022-07-28 11:19:56 - train: epoch 0061, iter [04900, 05004], lr: 0.072326, loss: 1.4333
2022-07-28 11:21:38 - train: epoch 0061, iter [05000, 05004], lr: 0.072262, loss: 1.8983
2022-07-28 11:21:41 - train: epoch 061, train_loss: 1.6381
2022-07-28 11:25:56 - eval: epoch: 061, acc1: 65.188%, acc5: 87.084%, test_loss: 1.4284, per_image_load_time: 1.904ms, per_image_inference_time: 0.700ms
2022-07-28 11:25:56 - until epoch: 061, best_acc1: 65.188%
2022-07-28 11:25:56 - epoch 062 lr: 0.072259
2022-07-28 11:28:08 - train: epoch 0062, iter [00100, 05004], lr: 0.072196, loss: 1.7837
2022-07-28 11:30:04 - train: epoch 0062, iter [00200, 05004], lr: 0.072133, loss: 1.6793
2022-07-28 11:32:00 - train: epoch 0062, iter [00300, 05004], lr: 0.072069, loss: 1.6195
2022-07-28 11:33:55 - train: epoch 0062, iter [00400, 05004], lr: 0.072006, loss: 1.6404
2022-07-28 11:35:47 - train: epoch 0062, iter [00500, 05004], lr: 0.071942, loss: 1.5976
2022-07-28 11:37:37 - train: epoch 0062, iter [00600, 05004], lr: 0.071879, loss: 1.3966
2022-07-28 11:39:33 - train: epoch 0062, iter [00700, 05004], lr: 0.071816, loss: 1.5222
2022-07-28 11:41:27 - train: epoch 0062, iter [00800, 05004], lr: 0.071752, loss: 1.5458
2022-07-28 11:43:23 - train: epoch 0062, iter [00900, 05004], lr: 0.071689, loss: 1.7621
2022-07-28 11:45:13 - train: epoch 0062, iter [01000, 05004], lr: 0.071625, loss: 1.5470
2022-07-28 11:47:11 - train: epoch 0062, iter [01100, 05004], lr: 0.071562, loss: 1.6384
2022-07-28 11:49:05 - train: epoch 0062, iter [01200, 05004], lr: 0.071499, loss: 1.7102
2022-07-28 11:51:00 - train: epoch 0062, iter [01300, 05004], lr: 0.071435, loss: 1.4379
2022-07-28 11:52:55 - train: epoch 0062, iter [01400, 05004], lr: 0.071372, loss: 1.5644
2022-07-28 11:54:52 - train: epoch 0062, iter [01500, 05004], lr: 0.071309, loss: 1.5601
2022-07-28 11:56:45 - train: epoch 0062, iter [01600, 05004], lr: 0.071245, loss: 1.8111
2022-07-28 11:58:35 - train: epoch 0062, iter [01700, 05004], lr: 0.071182, loss: 1.5472
2022-07-28 12:00:33 - train: epoch 0062, iter [01800, 05004], lr: 0.071119, loss: 1.7626
2022-07-28 12:02:35 - train: epoch 0062, iter [01900, 05004], lr: 0.071056, loss: 1.4543
2022-07-28 12:04:35 - train: epoch 0062, iter [02000, 05004], lr: 0.070992, loss: 1.7125
2022-07-28 12:06:38 - train: epoch 0062, iter [02100, 05004], lr: 0.070929, loss: 1.7037
2022-07-28 12:08:30 - train: epoch 0062, iter [02200, 05004], lr: 0.070866, loss: 1.3719
2022-07-28 12:10:25 - train: epoch 0062, iter [02300, 05004], lr: 0.070803, loss: 1.6654
2022-07-28 12:12:23 - train: epoch 0062, iter [02400, 05004], lr: 0.070739, loss: 1.4710
2022-07-28 12:14:23 - train: epoch 0062, iter [02500, 05004], lr: 0.070676, loss: 1.7920
2022-07-28 12:16:14 - train: epoch 0062, iter [02600, 05004], lr: 0.070613, loss: 1.5963
2022-07-28 12:18:10 - train: epoch 0062, iter [02700, 05004], lr: 0.070550, loss: 1.6283
2022-07-28 12:20:04 - train: epoch 0062, iter [02800, 05004], lr: 0.070487, loss: 1.8941
2022-07-28 12:22:14 - train: epoch 0062, iter [02900, 05004], lr: 0.070424, loss: 1.5702
2022-07-28 12:24:14 - train: epoch 0062, iter [03000, 05004], lr: 0.070361, loss: 1.7173
2022-07-28 12:26:21 - train: epoch 0062, iter [03100, 05004], lr: 0.070297, loss: 1.6627
2022-07-28 12:28:27 - train: epoch 0062, iter [03200, 05004], lr: 0.070234, loss: 1.5874
2022-07-28 12:30:28 - train: epoch 0062, iter [03300, 05004], lr: 0.070171, loss: 1.4504
2022-07-28 12:32:23 - train: epoch 0062, iter [03400, 05004], lr: 0.070108, loss: 1.5710
2022-07-28 12:34:18 - train: epoch 0062, iter [03500, 05004], lr: 0.070045, loss: 1.6287
2022-07-28 12:36:17 - train: epoch 0062, iter [03600, 05004], lr: 0.069982, loss: 1.7494
2022-07-28 12:38:09 - train: epoch 0062, iter [03700, 05004], lr: 0.069919, loss: 1.6324
2022-07-28 12:40:04 - train: epoch 0062, iter [03800, 05004], lr: 0.069856, loss: 1.7038
2022-07-28 12:41:56 - train: epoch 0062, iter [03900, 05004], lr: 0.069793, loss: 1.8169
2022-07-28 12:43:56 - train: epoch 0062, iter [04000, 05004], lr: 0.069730, loss: 1.3962
2022-07-28 12:46:03 - train: epoch 0062, iter [04100, 05004], lr: 0.069667, loss: 1.7925
2022-07-28 12:48:07 - train: epoch 0062, iter [04200, 05004], lr: 0.069604, loss: 1.5251
2022-07-28 12:50:07 - train: epoch 0062, iter [04300, 05004], lr: 0.069541, loss: 1.5244
2022-07-28 12:52:10 - train: epoch 0062, iter [04400, 05004], lr: 0.069478, loss: 1.4837
2022-07-28 12:54:10 - train: epoch 0062, iter [04500, 05004], lr: 0.069415, loss: 1.4574
2022-07-28 12:56:11 - train: epoch 0062, iter [04600, 05004], lr: 0.069352, loss: 1.5156
2022-07-28 12:58:11 - train: epoch 0062, iter [04700, 05004], lr: 0.069289, loss: 1.6529
2022-07-28 13:00:12 - train: epoch 0062, iter [04800, 05004], lr: 0.069227, loss: 1.6483
2022-07-28 13:02:05 - train: epoch 0062, iter [04900, 05004], lr: 0.069164, loss: 1.6314
2022-07-28 13:03:58 - train: epoch 0062, iter [05000, 05004], lr: 0.069101, loss: 1.6647
2022-07-28 13:04:01 - train: epoch 062, train_loss: 1.6228
2022-07-28 13:07:55 - eval: epoch: 062, acc1: 64.940%, acc5: 86.746%, test_loss: 1.4300, per_image_load_time: 8.299ms, per_image_inference_time: 0.783ms
2022-07-28 13:07:56 - until epoch: 062, best_acc1: 65.188%
2022-07-28 13:07:56 - epoch 063 lr: 0.069098
2022-07-28 13:10:10 - train: epoch 0063, iter [00100, 05004], lr: 0.069035, loss: 1.5883
2022-07-28 13:12:10 - train: epoch 0063, iter [00200, 05004], lr: 0.068973, loss: 1.6417
2022-07-28 13:14:01 - train: epoch 0063, iter [00300, 05004], lr: 0.068910, loss: 1.5523
2022-07-28 13:15:59 - train: epoch 0063, iter [00400, 05004], lr: 0.068847, loss: 1.6394
2022-07-28 13:17:54 - train: epoch 0063, iter [00500, 05004], lr: 0.068784, loss: 1.5588
2022-07-28 13:19:49 - train: epoch 0063, iter [00600, 05004], lr: 0.068721, loss: 1.5723
2022-07-28 13:21:48 - train: epoch 0063, iter [00700, 05004], lr: 0.068659, loss: 1.6704
2022-07-28 13:23:47 - train: epoch 0063, iter [00800, 05004], lr: 0.068596, loss: 1.5159
2022-07-28 13:25:49 - train: epoch 0063, iter [00900, 05004], lr: 0.068533, loss: 1.8638
2022-07-28 13:27:49 - train: epoch 0063, iter [01000, 05004], lr: 0.068470, loss: 1.8454
2022-07-28 13:29:49 - train: epoch 0063, iter [01100, 05004], lr: 0.068408, loss: 1.3499
2022-07-28 13:31:47 - train: epoch 0063, iter [01200, 05004], lr: 0.068345, loss: 1.5943
2022-07-28 13:33:47 - train: epoch 0063, iter [01300, 05004], lr: 0.068282, loss: 1.3431
2022-07-28 13:35:44 - train: epoch 0063, iter [01400, 05004], lr: 0.068220, loss: 1.8996
2022-07-28 13:37:42 - train: epoch 0063, iter [01500, 05004], lr: 0.068157, loss: 1.5475
2022-07-28 13:39:42 - train: epoch 0063, iter [01600, 05004], lr: 0.068094, loss: 1.6006
2022-07-28 13:41:36 - train: epoch 0063, iter [01700, 05004], lr: 0.068032, loss: 1.4857
2022-07-28 13:43:38 - train: epoch 0063, iter [01800, 05004], lr: 0.067969, loss: 1.6343
2022-07-28 13:45:48 - train: epoch 0063, iter [01900, 05004], lr: 0.067907, loss: 1.8549
2022-07-28 13:47:53 - train: epoch 0063, iter [02000, 05004], lr: 0.067844, loss: 1.4619
2022-07-28 13:49:49 - train: epoch 0063, iter [02100, 05004], lr: 0.067781, loss: 1.5043
2022-07-28 13:51:53 - train: epoch 0063, iter [02200, 05004], lr: 0.067719, loss: 1.7892
2022-07-28 13:54:02 - train: epoch 0063, iter [02300, 05004], lr: 0.067656, loss: 1.7686
2022-07-28 13:56:12 - train: epoch 0063, iter [02400, 05004], lr: 0.067594, loss: 1.8224
2022-07-28 13:58:17 - train: epoch 0063, iter [02500, 05004], lr: 0.067531, loss: 1.5144
2022-07-28 14:00:20 - train: epoch 0063, iter [02600, 05004], lr: 0.067469, loss: 1.5245
2022-07-28 14:02:20 - train: epoch 0063, iter [02700, 05004], lr: 0.067406, loss: 1.6292
2022-07-28 14:04:19 - train: epoch 0063, iter [02800, 05004], lr: 0.067344, loss: 1.6761
2022-07-28 14:06:08 - train: epoch 0063, iter [02900, 05004], lr: 0.067281, loss: 1.7939
2022-07-28 14:08:24 - train: epoch 0063, iter [03000, 05004], lr: 0.067219, loss: 1.7193
2022-07-28 14:10:29 - train: epoch 0063, iter [03100, 05004], lr: 0.067157, loss: 1.9233
2022-07-28 14:12:40 - train: epoch 0063, iter [03200, 05004], lr: 0.067094, loss: 1.6523
2022-07-28 14:14:43 - train: epoch 0063, iter [03300, 05004], lr: 0.067032, loss: 1.5330
2022-07-28 14:16:53 - train: epoch 0063, iter [03400, 05004], lr: 0.066969, loss: 1.4092
2022-07-28 14:18:55 - train: epoch 0063, iter [03500, 05004], lr: 0.066907, loss: 1.7997
2022-07-28 14:20:58 - train: epoch 0063, iter [03600, 05004], lr: 0.066845, loss: 1.5969
2022-07-28 14:23:01 - train: epoch 0063, iter [03700, 05004], lr: 0.066782, loss: 1.4886
2022-07-28 14:24:57 - train: epoch 0063, iter [03800, 05004], lr: 0.066720, loss: 1.6391
2022-07-28 14:26:55 - train: epoch 0063, iter [03900, 05004], lr: 0.066658, loss: 1.4870
2022-07-28 14:28:53 - train: epoch 0063, iter [04000, 05004], lr: 0.066595, loss: 1.4978
2022-07-28 14:30:56 - train: epoch 0063, iter [04100, 05004], lr: 0.066533, loss: 1.6784
2022-07-28 14:33:04 - train: epoch 0063, iter [04200, 05004], lr: 0.066471, loss: 1.5420
2022-07-28 14:35:01 - train: epoch 0063, iter [04300, 05004], lr: 0.066409, loss: 1.5530
2022-07-28 14:37:00 - train: epoch 0063, iter [04400, 05004], lr: 0.066346, loss: 1.5241
2022-07-28 14:39:03 - train: epoch 0063, iter [04500, 05004], lr: 0.066284, loss: 1.7798
2022-07-28 14:41:04 - train: epoch 0063, iter [04600, 05004], lr: 0.066222, loss: 1.4302
2022-07-28 14:43:06 - train: epoch 0063, iter [04700, 05004], lr: 0.066160, loss: 1.5863
2022-07-28 14:45:03 - train: epoch 0063, iter [04800, 05004], lr: 0.066097, loss: 1.6641
2022-07-28 14:47:06 - train: epoch 0063, iter [04900, 05004], lr: 0.066035, loss: 1.7334
2022-07-28 14:49:02 - train: epoch 0063, iter [05000, 05004], lr: 0.065973, loss: 1.6013
2022-07-28 14:49:05 - train: epoch 063, train_loss: 1.6037
2022-07-28 14:53:05 - eval: epoch: 063, acc1: 65.774%, acc5: 87.224%, test_loss: 1.4047, per_image_load_time: 7.987ms, per_image_inference_time: 0.811ms
2022-07-28 14:53:05 - until epoch: 063, best_acc1: 65.774%
2022-07-28 14:53:05 - epoch 064 lr: 0.065970
2022-07-28 14:55:27 - train: epoch 0064, iter [00100, 05004], lr: 0.065909, loss: 1.3516
2022-07-28 14:57:25 - train: epoch 0064, iter [00200, 05004], lr: 0.065846, loss: 1.5409
2022-07-28 14:59:28 - train: epoch 0064, iter [00300, 05004], lr: 0.065784, loss: 1.5088
2022-07-28 15:01:26 - train: epoch 0064, iter [00400, 05004], lr: 0.065722, loss: 1.4669
2022-07-28 15:03:24 - train: epoch 0064, iter [00500, 05004], lr: 0.065660, loss: 1.4066
2022-07-28 15:05:29 - train: epoch 0064, iter [00600, 05004], lr: 0.065598, loss: 1.5659
2022-07-28 15:07:31 - train: epoch 0064, iter [00700, 05004], lr: 0.065536, loss: 1.7198
2022-07-28 15:09:31 - train: epoch 0064, iter [00800, 05004], lr: 0.065474, loss: 1.4992
2022-07-28 15:11:36 - train: epoch 0064, iter [00900, 05004], lr: 0.065412, loss: 1.5051
2022-07-28 15:13:32 - train: epoch 0064, iter [01000, 05004], lr: 0.065350, loss: 1.6037
2022-07-28 15:15:29 - train: epoch 0064, iter [01100, 05004], lr: 0.065288, loss: 1.6646
2022-07-28 15:17:30 - train: epoch 0064, iter [01200, 05004], lr: 0.065226, loss: 1.5417
2022-07-28 15:19:33 - train: epoch 0064, iter [01300, 05004], lr: 0.065164, loss: 1.5201
2022-07-28 15:21:30 - train: epoch 0064, iter [01400, 05004], lr: 0.065102, loss: 1.6844
2022-07-28 15:23:32 - train: epoch 0064, iter [01500, 05004], lr: 0.065040, loss: 1.7672
2022-07-28 15:25:34 - train: epoch 0064, iter [01600, 05004], lr: 0.064978, loss: 1.6040
2022-07-28 15:27:24 - train: epoch 0064, iter [01700, 05004], lr: 0.064916, loss: 1.5245
2022-07-28 15:29:27 - train: epoch 0064, iter [01800, 05004], lr: 0.064855, loss: 1.4019
2022-07-28 15:31:25 - train: epoch 0064, iter [01900, 05004], lr: 0.064793, loss: 1.5369
2022-07-28 15:33:20 - train: epoch 0064, iter [02000, 05004], lr: 0.064731, loss: 1.4705
2022-07-28 15:35:24 - train: epoch 0064, iter [02100, 05004], lr: 0.064669, loss: 1.6680
2022-07-28 15:37:21 - train: epoch 0064, iter [02200, 05004], lr: 0.064607, loss: 1.7013
2022-07-28 15:39:17 - train: epoch 0064, iter [02300, 05004], lr: 0.064545, loss: 1.7605
2022-07-28 15:41:14 - train: epoch 0064, iter [02400, 05004], lr: 0.064484, loss: 1.5488
2022-07-28 15:43:10 - train: epoch 0064, iter [02500, 05004], lr: 0.064422, loss: 1.6834
2022-07-28 15:45:11 - train: epoch 0064, iter [02600, 05004], lr: 0.064360, loss: 1.5480
2022-07-28 15:47:09 - train: epoch 0064, iter [02700, 05004], lr: 0.064298, loss: 1.5781
2022-07-28 15:49:08 - train: epoch 0064, iter [02800, 05004], lr: 0.064237, loss: 1.5535
2022-07-28 15:50:58 - train: epoch 0064, iter [02900, 05004], lr: 0.064175, loss: 1.8590
2022-07-28 15:53:05 - train: epoch 0064, iter [03000, 05004], lr: 0.064113, loss: 1.4064
2022-07-28 15:55:12 - train: epoch 0064, iter [03100, 05004], lr: 0.064052, loss: 1.6591
2022-07-28 15:57:09 - train: epoch 0064, iter [03200, 05004], lr: 0.063990, loss: 1.6981
2022-07-28 15:59:06 - train: epoch 0064, iter [03300, 05004], lr: 0.063928, loss: 1.4446
2022-07-28 16:01:09 - train: epoch 0064, iter [03400, 05004], lr: 0.063867, loss: 1.5455
2022-07-28 16:03:07 - train: epoch 0064, iter [03500, 05004], lr: 0.063805, loss: 1.6088
2022-07-28 16:05:08 - train: epoch 0064, iter [03600, 05004], lr: 0.063743, loss: 1.6990
2022-07-28 16:07:07 - train: epoch 0064, iter [03700, 05004], lr: 0.063682, loss: 1.5754
2022-07-28 16:09:07 - train: epoch 0064, iter [03800, 05004], lr: 0.063620, loss: 1.5621
2022-07-28 16:10:59 - train: epoch 0064, iter [03900, 05004], lr: 0.063559, loss: 1.6419
2022-07-28 16:12:55 - train: epoch 0064, iter [04000, 05004], lr: 0.063497, loss: 1.5239
2022-07-28 16:14:46 - train: epoch 0064, iter [04100, 05004], lr: 0.063436, loss: 1.5731
2022-07-28 16:16:35 - train: epoch 0064, iter [04200, 05004], lr: 0.063374, loss: 1.4349
2022-07-28 16:18:34 - train: epoch 0064, iter [04300, 05004], lr: 0.063313, loss: 1.5929
2022-07-28 16:20:34 - train: epoch 0064, iter [04400, 05004], lr: 0.063251, loss: 1.3795
2022-07-28 16:22:28 - train: epoch 0064, iter [04500, 05004], lr: 0.063190, loss: 1.5368
2022-07-28 16:24:26 - train: epoch 0064, iter [04600, 05004], lr: 0.063128, loss: 1.7575
2022-07-28 16:26:24 - train: epoch 0064, iter [04700, 05004], lr: 0.063067, loss: 1.8994
2022-07-28 16:28:23 - train: epoch 0064, iter [04800, 05004], lr: 0.063005, loss: 1.6671
2022-07-28 16:30:19 - train: epoch 0064, iter [04900, 05004], lr: 0.062944, loss: 1.5584
2022-07-28 16:32:10 - train: epoch 0064, iter [05000, 05004], lr: 0.062883, loss: 1.4297
2022-07-28 16:32:13 - train: epoch 064, train_loss: 1.5914
2022-07-28 16:36:08 - eval: epoch: 064, acc1: 66.286%, acc5: 87.684%, test_loss: 1.3745, per_image_load_time: 7.794ms, per_image_inference_time: 0.791ms
2022-07-28 16:36:09 - until epoch: 064, best_acc1: 66.286%
2022-07-28 16:36:09 - epoch 065 lr: 0.062880
2022-07-28 16:38:20 - train: epoch 0065, iter [00100, 05004], lr: 0.062819, loss: 1.5102
2022-07-28 16:40:07 - train: epoch 0065, iter [00200, 05004], lr: 0.062758, loss: 1.6392
2022-07-28 16:41:50 - train: epoch 0065, iter [00300, 05004], lr: 0.062696, loss: 1.5874
2022-07-28 16:43:41 - train: epoch 0065, iter [00400, 05004], lr: 0.062635, loss: 1.5007
2022-07-28 16:45:33 - train: epoch 0065, iter [00500, 05004], lr: 0.062574, loss: 1.5399
2022-07-28 16:47:21 - train: epoch 0065, iter [00600, 05004], lr: 0.062512, loss: 1.4963
2022-07-28 16:49:12 - train: epoch 0065, iter [00700, 05004], lr: 0.062451, loss: 1.6187
2022-07-28 16:50:59 - train: epoch 0065, iter [00800, 05004], lr: 0.062390, loss: 1.4336
2022-07-28 16:52:52 - train: epoch 0065, iter [00900, 05004], lr: 0.062329, loss: 1.6271
2022-07-28 16:54:45 - train: epoch 0065, iter [01000, 05004], lr: 0.062267, loss: 1.7897
2022-07-28 16:56:33 - train: epoch 0065, iter [01100, 05004], lr: 0.062206, loss: 1.5658
2022-07-28 16:58:21 - train: epoch 0065, iter [01200, 05004], lr: 0.062145, loss: 1.8525
2022-07-28 17:00:24 - train: epoch 0065, iter [01300, 05004], lr: 0.062084, loss: 1.5327
2022-07-28 17:02:23 - train: epoch 0065, iter [01400, 05004], lr: 0.062023, loss: 1.3422
2022-07-28 17:04:21 - train: epoch 0065, iter [01500, 05004], lr: 0.061962, loss: 1.5674
2022-07-28 17:06:31 - train: epoch 0065, iter [01600, 05004], lr: 0.061901, loss: 1.6190
2022-07-28 17:08:30 - train: epoch 0065, iter [01700, 05004], lr: 0.061839, loss: 1.6741
2022-07-28 17:10:32 - train: epoch 0065, iter [01800, 05004], lr: 0.061778, loss: 1.4461
2022-07-28 17:12:26 - train: epoch 0065, iter [01900, 05004], lr: 0.061717, loss: 1.3888
2022-07-28 17:14:25 - train: epoch 0065, iter [02000, 05004], lr: 0.061656, loss: 1.7861
2022-07-28 17:16:22 - train: epoch 0065, iter [02100, 05004], lr: 0.061595, loss: 1.5644
2022-07-28 17:18:19 - train: epoch 0065, iter [02200, 05004], lr: 0.061534, loss: 1.5877
2022-07-28 17:20:19 - train: epoch 0065, iter [02300, 05004], lr: 0.061473, loss: 1.4539
2022-07-28 17:22:14 - train: epoch 0065, iter [02400, 05004], lr: 0.061412, loss: 1.5925
2022-07-28 17:24:12 - train: epoch 0065, iter [02500, 05004], lr: 0.061351, loss: 1.6330
2022-07-28 17:26:14 - train: epoch 0065, iter [02600, 05004], lr: 0.061290, loss: 1.5045
2022-07-28 17:28:11 - train: epoch 0065, iter [02700, 05004], lr: 0.061229, loss: 1.6457
2022-07-28 17:30:07 - train: epoch 0065, iter [02800, 05004], lr: 0.061169, loss: 1.6931
2022-07-28 17:32:02 - train: epoch 0065, iter [02900, 05004], lr: 0.061108, loss: 1.5908
2022-07-28 17:33:50 - train: epoch 0065, iter [03000, 05004], lr: 0.061047, loss: 1.4446
2022-07-28 17:35:53 - train: epoch 0065, iter [03100, 05004], lr: 0.060986, loss: 1.5304
2022-07-28 17:38:01 - train: epoch 0065, iter [03200, 05004], lr: 0.060925, loss: 1.6788
2022-07-28 17:39:52 - train: epoch 0065, iter [03300, 05004], lr: 0.060864, loss: 1.5813
2022-07-28 17:41:45 - train: epoch 0065, iter [03400, 05004], lr: 0.060803, loss: 1.5967
2022-07-28 17:43:46 - train: epoch 0065, iter [03500, 05004], lr: 0.060743, loss: 1.8402
2022-07-28 17:45:46 - train: epoch 0065, iter [03600, 05004], lr: 0.060682, loss: 1.7630
2022-07-28 17:47:37 - train: epoch 0065, iter [03700, 05004], lr: 0.060621, loss: 1.4199
2022-07-28 17:49:39 - train: epoch 0065, iter [03800, 05004], lr: 0.060560, loss: 1.5657
2022-07-28 17:51:42 - train: epoch 0065, iter [03900, 05004], lr: 0.060500, loss: 1.6749
2022-07-28 17:53:37 - train: epoch 0065, iter [04000, 05004], lr: 0.060439, loss: 1.5028
2022-07-28 17:55:34 - train: epoch 0065, iter [04100, 05004], lr: 0.060378, loss: 1.6376
2022-07-28 17:57:32 - train: epoch 0065, iter [04200, 05004], lr: 0.060318, loss: 1.6899
2022-07-28 17:59:24 - train: epoch 0065, iter [04300, 05004], lr: 0.060257, loss: 1.4772
2022-07-28 18:01:16 - train: epoch 0065, iter [04400, 05004], lr: 0.060196, loss: 1.7307
2022-07-28 18:03:08 - train: epoch 0065, iter [04500, 05004], lr: 0.060136, loss: 1.7318
2022-07-28 18:05:01 - train: epoch 0065, iter [04600, 05004], lr: 0.060075, loss: 1.6433
2022-07-28 18:06:58 - train: epoch 0065, iter [04700, 05004], lr: 0.060015, loss: 1.5776
2022-07-28 18:08:45 - train: epoch 0065, iter [04800, 05004], lr: 0.059954, loss: 1.4731
2022-07-28 18:10:39 - train: epoch 0065, iter [04900, 05004], lr: 0.059893, loss: 1.3704
2022-07-28 18:12:31 - train: epoch 0065, iter [05000, 05004], lr: 0.059833, loss: 1.6530
2022-07-28 18:12:35 - train: epoch 065, train_loss: 1.5752
2022-07-28 18:16:34 - eval: epoch: 065, acc1: 65.308%, acc5: 86.758%, test_loss: 1.4393, per_image_load_time: 8.370ms, per_image_inference_time: 0.778ms
2022-07-28 18:16:34 - until epoch: 065, best_acc1: 66.286%
2022-07-28 18:16:34 - epoch 066 lr: 0.059830
2022-07-28 18:18:48 - train: epoch 0066, iter [00100, 05004], lr: 0.059770, loss: 1.4871
2022-07-28 18:20:42 - train: epoch 0066, iter [00200, 05004], lr: 0.059709, loss: 1.6303
2022-07-28 18:22:41 - train: epoch 0066, iter [00300, 05004], lr: 0.059649, loss: 1.4461
2022-07-28 18:24:47 - train: epoch 0066, iter [00400, 05004], lr: 0.059589, loss: 1.3496
2022-07-28 18:26:43 - train: epoch 0066, iter [00500, 05004], lr: 0.059528, loss: 1.5623
2022-07-28 18:28:40 - train: epoch 0066, iter [00600, 05004], lr: 0.059468, loss: 1.4541
2022-07-28 18:30:32 - train: epoch 0066, iter [00700, 05004], lr: 0.059407, loss: 1.5013
2022-07-28 18:32:27 - train: epoch 0066, iter [00800, 05004], lr: 0.059347, loss: 1.5843
2022-07-28 18:34:19 - train: epoch 0066, iter [00900, 05004], lr: 0.059286, loss: 1.6088
2022-07-28 18:36:13 - train: epoch 0066, iter [01000, 05004], lr: 0.059226, loss: 1.6452
2022-07-28 18:38:05 - train: epoch 0066, iter [01100, 05004], lr: 0.059166, loss: 1.4993
2022-07-28 18:40:00 - train: epoch 0066, iter [01200, 05004], lr: 0.059105, loss: 1.7071
2022-07-28 18:41:52 - train: epoch 0066, iter [01300, 05004], lr: 0.059045, loss: 1.5853
2022-07-28 18:43:47 - train: epoch 0066, iter [01400, 05004], lr: 0.058985, loss: 1.4972
2022-07-28 18:45:44 - train: epoch 0066, iter [01500, 05004], lr: 0.058925, loss: 1.5111
2022-07-28 18:47:42 - train: epoch 0066, iter [01600, 05004], lr: 0.058864, loss: 1.5190
2022-07-28 18:49:38 - train: epoch 0066, iter [01700, 05004], lr: 0.058804, loss: 1.6012
2022-07-28 18:51:36 - train: epoch 0066, iter [01800, 05004], lr: 0.058744, loss: 1.4350
2022-07-28 18:53:37 - train: epoch 0066, iter [01900, 05004], lr: 0.058684, loss: 1.6087
2022-07-28 18:55:49 - train: epoch 0066, iter [02000, 05004], lr: 0.058624, loss: 1.3175
2022-07-28 18:57:49 - train: epoch 0066, iter [02100, 05004], lr: 0.058563, loss: 1.5576
2022-07-28 18:59:54 - train: epoch 0066, iter [02200, 05004], lr: 0.058503, loss: 1.2345
2022-07-28 19:01:54 - train: epoch 0066, iter [02300, 05004], lr: 0.058443, loss: 1.5418
2022-07-28 19:04:03 - train: epoch 0066, iter [02400, 05004], lr: 0.058383, loss: 1.4109
2022-07-28 19:05:57 - train: epoch 0066, iter [02500, 05004], lr: 0.058323, loss: 1.8137
2022-07-28 19:07:52 - train: epoch 0066, iter [02600, 05004], lr: 0.058263, loss: 1.4240
2022-07-28 19:09:48 - train: epoch 0066, iter [02700, 05004], lr: 0.058203, loss: 1.6558
2022-07-28 19:11:45 - train: epoch 0066, iter [02800, 05004], lr: 0.058143, loss: 1.5201
2022-07-28 19:13:35 - train: epoch 0066, iter [02900, 05004], lr: 0.058083, loss: 1.5818
2022-07-28 19:15:40 - train: epoch 0066, iter [03000, 05004], lr: 0.058023, loss: 1.6617
2022-07-28 19:17:41 - train: epoch 0066, iter [03100, 05004], lr: 0.057963, loss: 1.7500
2022-07-28 19:19:32 - train: epoch 0066, iter [03200, 05004], lr: 0.057903, loss: 1.5629
2022-07-28 19:21:47 - train: epoch 0066, iter [03300, 05004], lr: 0.057843, loss: 1.5028
2022-07-28 19:23:45 - train: epoch 0066, iter [03400, 05004], lr: 0.057783, loss: 1.7344
2022-07-28 19:25:37 - train: epoch 0066, iter [03500, 05004], lr: 0.057723, loss: 1.6931
2022-07-28 19:27:26 - train: epoch 0066, iter [03600, 05004], lr: 0.057663, loss: 1.6202
2022-07-28 19:29:17 - train: epoch 0066, iter [03700, 05004], lr: 0.057603, loss: 1.6357
2022-07-28 19:31:18 - train: epoch 0066, iter [03800, 05004], lr: 0.057544, loss: 1.4960
2022-07-28 19:33:08 - train: epoch 0066, iter [03900, 05004], lr: 0.057484, loss: 1.4032
2022-07-28 19:35:03 - train: epoch 0066, iter [04000, 05004], lr: 0.057424, loss: 1.6636
2022-07-28 19:37:01 - train: epoch 0066, iter [04100, 05004], lr: 0.057364, loss: 1.6783
2022-07-28 19:38:54 - train: epoch 0066, iter [04200, 05004], lr: 0.057304, loss: 1.4094
2022-07-28 19:40:48 - train: epoch 0066, iter [04300, 05004], lr: 0.057245, loss: 1.4258
2022-07-28 19:42:45 - train: epoch 0066, iter [04400, 05004], lr: 0.057185, loss: 1.4337
2022-07-28 19:44:33 - train: epoch 0066, iter [04500, 05004], lr: 0.057125, loss: 1.6657
2022-07-28 19:46:30 - train: epoch 0066, iter [04600, 05004], lr: 0.057066, loss: 1.7296
2022-07-28 19:48:20 - train: epoch 0066, iter [04700, 05004], lr: 0.057006, loss: 1.2909
2022-07-28 19:50:16 - train: epoch 0066, iter [04800, 05004], lr: 0.056946, loss: 1.5420
2022-07-28 19:52:05 - train: epoch 0066, iter [04900, 05004], lr: 0.056887, loss: 1.4741
2022-07-28 19:53:56 - train: epoch 0066, iter [05000, 05004], lr: 0.056827, loss: 1.4603
2022-07-28 19:53:59 - train: epoch 066, train_loss: 1.5570
2022-07-28 19:57:58 - eval: epoch: 066, acc1: 66.286%, acc5: 87.686%, test_loss: 1.3737, per_image_load_time: 8.037ms, per_image_inference_time: 0.762ms
2022-07-28 19:57:59 - until epoch: 066, best_acc1: 66.286%
2022-07-28 19:57:59 - epoch 067 lr: 0.056824
2022-07-28 20:00:09 - train: epoch 0067, iter [00100, 05004], lr: 0.056765, loss: 1.6002
2022-07-28 20:02:04 - train: epoch 0067, iter [00200, 05004], lr: 0.056705, loss: 1.6452
2022-07-28 20:03:58 - train: epoch 0067, iter [00300, 05004], lr: 0.056646, loss: 1.3916
2022-07-28 20:05:51 - train: epoch 0067, iter [00400, 05004], lr: 0.056586, loss: 1.4406
2022-07-28 20:07:41 - train: epoch 0067, iter [00500, 05004], lr: 0.056527, loss: 1.4104
2022-07-28 20:09:38 - train: epoch 0067, iter [00600, 05004], lr: 0.056467, loss: 1.4836
2022-07-28 20:11:30 - train: epoch 0067, iter [00700, 05004], lr: 0.056408, loss: 1.5042
2022-07-28 20:13:23 - train: epoch 0067, iter [00800, 05004], lr: 0.056348, loss: 1.7328
2022-07-28 20:15:20 - train: epoch 0067, iter [00900, 05004], lr: 0.056289, loss: 1.7591
2022-07-28 20:17:11 - train: epoch 0067, iter [01000, 05004], lr: 0.056229, loss: 1.5255
2022-07-28 20:19:04 - train: epoch 0067, iter [01100, 05004], lr: 0.056170, loss: 1.5846
2022-07-28 20:20:58 - train: epoch 0067, iter [01200, 05004], lr: 0.056111, loss: 1.5673
2022-07-28 20:22:48 - train: epoch 0067, iter [01300, 05004], lr: 0.056051, loss: 1.7138
2022-07-28 20:24:41 - train: epoch 0067, iter [01400, 05004], lr: 0.055992, loss: 1.5964
2022-07-28 20:26:36 - train: epoch 0067, iter [01500, 05004], lr: 0.055933, loss: 1.3524
2022-07-28 20:28:26 - train: epoch 0067, iter [01600, 05004], lr: 0.055873, loss: 1.5789
2022-07-28 20:30:20 - train: epoch 0067, iter [01700, 05004], lr: 0.055814, loss: 1.3718
2022-07-28 20:32:13 - train: epoch 0067, iter [01800, 05004], lr: 0.055755, loss: 1.6548
2022-07-28 20:34:05 - train: epoch 0067, iter [01900, 05004], lr: 0.055695, loss: 1.5307
2022-07-28 20:35:57 - train: epoch 0067, iter [02000, 05004], lr: 0.055636, loss: 1.5710
2022-07-28 20:37:51 - train: epoch 0067, iter [02100, 05004], lr: 0.055577, loss: 1.3478
2022-07-28 20:39:43 - train: epoch 0067, iter [02200, 05004], lr: 0.055518, loss: 1.4905
2022-07-28 20:41:34 - train: epoch 0067, iter [02300, 05004], lr: 0.055459, loss: 1.4099
2022-07-28 20:43:27 - train: epoch 0067, iter [02400, 05004], lr: 0.055399, loss: 1.5145
2022-07-28 20:45:16 - train: epoch 0067, iter [02500, 05004], lr: 0.055340, loss: 1.5633
2022-07-28 20:47:07 - train: epoch 0067, iter [02600, 05004], lr: 0.055281, loss: 1.4503
2022-07-28 20:48:59 - train: epoch 0067, iter [02700, 05004], lr: 0.055222, loss: 1.5140
2022-07-28 20:50:49 - train: epoch 0067, iter [02800, 05004], lr: 0.055163, loss: 1.6991
2022-07-28 20:52:42 - train: epoch 0067, iter [02900, 05004], lr: 0.055104, loss: 1.6319
2022-07-28 20:54:34 - train: epoch 0067, iter [03000, 05004], lr: 0.055045, loss: 1.4917
2022-07-28 20:56:22 - train: epoch 0067, iter [03100, 05004], lr: 0.054986, loss: 1.3692
2022-07-28 20:58:17 - train: epoch 0067, iter [03200, 05004], lr: 0.054927, loss: 1.6239
2022-07-28 21:00:01 - train: epoch 0067, iter [03300, 05004], lr: 0.054868, loss: 1.3538
2022-07-28 21:02:03 - train: epoch 0067, iter [03400, 05004], lr: 0.054809, loss: 1.5372
2022-07-28 21:03:57 - train: epoch 0067, iter [03500, 05004], lr: 0.054750, loss: 1.6446
2022-07-28 21:05:53 - train: epoch 0067, iter [03600, 05004], lr: 0.054691, loss: 1.5423
2022-07-28 21:07:50 - train: epoch 0067, iter [03700, 05004], lr: 0.054632, loss: 1.5501
2022-07-28 21:09:47 - train: epoch 0067, iter [03800, 05004], lr: 0.054573, loss: 1.6213
2022-07-28 21:11:48 - train: epoch 0067, iter [03900, 05004], lr: 0.054514, loss: 1.6981
2022-07-28 21:13:43 - train: epoch 0067, iter [04000, 05004], lr: 0.054456, loss: 1.7035
2022-07-28 21:15:38 - train: epoch 0067, iter [04100, 05004], lr: 0.054397, loss: 1.4109
2022-07-28 21:17:26 - train: epoch 0067, iter [04200, 05004], lr: 0.054338, loss: 1.5862
2022-07-28 21:19:17 - train: epoch 0067, iter [04300, 05004], lr: 0.054279, loss: 1.3232
2022-07-28 21:21:10 - train: epoch 0067, iter [04400, 05004], lr: 0.054220, loss: 1.5469
2022-07-28 21:23:07 - train: epoch 0067, iter [04500, 05004], lr: 0.054162, loss: 1.5084
2022-07-28 21:24:59 - train: epoch 0067, iter [04600, 05004], lr: 0.054103, loss: 1.3766
2022-07-28 21:26:46 - train: epoch 0067, iter [04700, 05004], lr: 0.054044, loss: 1.5046
2022-07-28 21:28:35 - train: epoch 0067, iter [04800, 05004], lr: 0.053986, loss: 1.3794
2022-07-28 21:30:21 - train: epoch 0067, iter [04900, 05004], lr: 0.053927, loss: 1.5731
2022-07-28 21:32:06 - train: epoch 0067, iter [05000, 05004], lr: 0.053868, loss: 1.6711
2022-07-28 21:32:10 - train: epoch 067, train_loss: 1.5380
2022-07-28 21:36:20 - eval: epoch: 067, acc1: 66.948%, acc5: 88.132%, test_loss: 1.3511, per_image_load_time: 8.222ms, per_image_inference_time: 0.769ms
2022-07-28 21:36:21 - until epoch: 067, best_acc1: 66.948%
2022-07-28 21:36:21 - epoch 068 lr: 0.053865
2022-07-28 21:38:38 - train: epoch 0068, iter [00100, 05004], lr: 0.053807, loss: 1.5645
2022-07-28 21:40:31 - train: epoch 0068, iter [00200, 05004], lr: 0.053749, loss: 1.4950
2022-07-28 21:42:31 - train: epoch 0068, iter [00300, 05004], lr: 0.053690, loss: 1.3950
2022-07-28 21:44:24 - train: epoch 0068, iter [00400, 05004], lr: 0.053632, loss: 1.4275
2022-07-28 21:46:25 - train: epoch 0068, iter [00500, 05004], lr: 0.053573, loss: 1.3815
2022-07-28 21:48:17 - train: epoch 0068, iter [00600, 05004], lr: 0.053514, loss: 1.6015
2022-07-28 21:50:12 - train: epoch 0068, iter [00700, 05004], lr: 0.053456, loss: 1.6630
2022-07-28 21:52:03 - train: epoch 0068, iter [00800, 05004], lr: 0.053397, loss: 1.3452
2022-07-28 21:53:51 - train: epoch 0068, iter [00900, 05004], lr: 0.053339, loss: 1.5149
2022-07-28 21:55:43 - train: epoch 0068, iter [01000, 05004], lr: 0.053281, loss: 1.5860
2022-07-28 21:57:30 - train: epoch 0068, iter [01100, 05004], lr: 0.053222, loss: 1.6854
2022-07-28 21:59:26 - train: epoch 0068, iter [01200, 05004], lr: 0.053164, loss: 1.5079
2022-07-28 22:01:16 - train: epoch 0068, iter [01300, 05004], lr: 0.053105, loss: 1.4483
2022-07-28 22:03:06 - train: epoch 0068, iter [01400, 05004], lr: 0.053047, loss: 1.6111
2022-07-28 22:04:51 - train: epoch 0068, iter [01500, 05004], lr: 0.052989, loss: 1.5672
2022-07-28 22:06:41 - train: epoch 0068, iter [01600, 05004], lr: 0.052930, loss: 1.5305
2022-07-28 22:08:33 - train: epoch 0068, iter [01700, 05004], lr: 0.052872, loss: 1.6871
2022-07-28 22:10:24 - train: epoch 0068, iter [01800, 05004], lr: 0.052814, loss: 1.4674
2022-07-28 22:12:15 - train: epoch 0068, iter [01900, 05004], lr: 0.052756, loss: 1.4945
2022-07-28 22:14:11 - train: epoch 0068, iter [02000, 05004], lr: 0.052697, loss: 1.6091
2022-07-28 22:15:57 - train: epoch 0068, iter [02100, 05004], lr: 0.052639, loss: 1.5864
2022-07-28 22:17:52 - train: epoch 0068, iter [02200, 05004], lr: 0.052581, loss: 1.4102
2022-07-28 22:19:42 - train: epoch 0068, iter [02300, 05004], lr: 0.052523, loss: 1.5493
2022-07-28 22:21:27 - train: epoch 0068, iter [02400, 05004], lr: 0.052465, loss: 1.3935
2022-07-28 22:23:15 - train: epoch 0068, iter [02500, 05004], lr: 0.052406, loss: 1.6367
2022-07-28 22:25:04 - train: epoch 0068, iter [02600, 05004], lr: 0.052348, loss: 1.3912
2022-07-28 22:26:55 - train: epoch 0068, iter [02700, 05004], lr: 0.052290, loss: 1.3559
2022-07-28 22:28:42 - train: epoch 0068, iter [02800, 05004], lr: 0.052232, loss: 1.7707
2022-07-28 22:30:30 - train: epoch 0068, iter [02900, 05004], lr: 0.052174, loss: 1.8041
2022-07-28 22:32:18 - train: epoch 0068, iter [03000, 05004], lr: 0.052116, loss: 1.4649
2022-07-28 22:34:07 - train: epoch 0068, iter [03100, 05004], lr: 0.052058, loss: 1.3044
2022-07-28 22:35:57 - train: epoch 0068, iter [03200, 05004], lr: 0.052000, loss: 1.6019
2022-07-28 22:37:41 - train: epoch 0068, iter [03300, 05004], lr: 0.051942, loss: 1.5214
2022-07-28 22:39:30 - train: epoch 0068, iter [03400, 05004], lr: 0.051884, loss: 1.4345
2022-07-28 22:41:19 - train: epoch 0068, iter [03500, 05004], lr: 0.051826, loss: 1.3263
2022-07-28 22:43:11 - train: epoch 0068, iter [03600, 05004], lr: 0.051768, loss: 1.5278
2022-07-28 22:45:21 - train: epoch 0068, iter [03700, 05004], lr: 0.051710, loss: 1.5137
2022-07-28 22:47:20 - train: epoch 0068, iter [03800, 05004], lr: 0.051653, loss: 1.6361
2022-07-28 22:49:24 - train: epoch 0068, iter [03900, 05004], lr: 0.051595, loss: 1.6392
2022-07-28 22:51:26 - train: epoch 0068, iter [04000, 05004], lr: 0.051537, loss: 1.4781
2022-07-28 22:53:29 - train: epoch 0068, iter [04100, 05004], lr: 0.051479, loss: 1.3376
2022-07-28 22:55:28 - train: epoch 0068, iter [04200, 05004], lr: 0.051421, loss: 1.6323
2022-07-28 22:57:27 - train: epoch 0068, iter [04300, 05004], lr: 0.051364, loss: 1.4449
2022-07-28 22:59:29 - train: epoch 0068, iter [04400, 05004], lr: 0.051306, loss: 1.4656
2022-07-28 23:01:30 - train: epoch 0068, iter [04500, 05004], lr: 0.051248, loss: 1.5918
2022-07-28 23:03:29 - train: epoch 0068, iter [04600, 05004], lr: 0.051190, loss: 1.5896
2022-07-28 23:05:15 - train: epoch 0068, iter [04700, 05004], lr: 0.051133, loss: 1.7448
2022-07-28 23:07:08 - train: epoch 0068, iter [04800, 05004], lr: 0.051075, loss: 1.6665
2022-07-28 23:09:01 - train: epoch 0068, iter [04900, 05004], lr: 0.051018, loss: 1.5307
2022-07-28 23:10:51 - train: epoch 0068, iter [05000, 05004], lr: 0.050960, loss: 1.4692
2022-07-28 23:10:55 - train: epoch 068, train_loss: 1.5209
2022-07-28 23:14:48 - eval: epoch: 068, acc1: 67.458%, acc5: 88.322%, test_loss: 1.3214, per_image_load_time: 7.943ms, per_image_inference_time: 0.698ms
2022-07-28 23:14:48 - until epoch: 068, best_acc1: 67.458%
2022-07-28 23:14:48 - epoch 069 lr: 0.050957
2022-07-28 23:17:12 - train: epoch 0069, iter [00100, 05004], lr: 0.050900, loss: 1.5188
2022-07-28 23:19:11 - train: epoch 0069, iter [00200, 05004], lr: 0.050843, loss: 1.5203
2022-07-28 23:21:17 - train: epoch 0069, iter [00300, 05004], lr: 0.050785, loss: 1.5192
2022-07-28 23:23:19 - train: epoch 0069, iter [00400, 05004], lr: 0.050727, loss: 1.4366
2022-07-28 23:25:23 - train: epoch 0069, iter [00500, 05004], lr: 0.050670, loss: 1.4816
2022-07-28 23:27:17 - train: epoch 0069, iter [00600, 05004], lr: 0.050612, loss: 1.3243
2022-07-28 23:29:18 - train: epoch 0069, iter [00700, 05004], lr: 0.050555, loss: 1.5854
2022-07-28 23:31:14 - train: epoch 0069, iter [00800, 05004], lr: 0.050498, loss: 1.7410
2022-07-28 23:33:08 - train: epoch 0069, iter [00900, 05004], lr: 0.050440, loss: 1.4864
2022-07-28 23:35:01 - train: epoch 0069, iter [01000, 05004], lr: 0.050383, loss: 1.4451
2022-07-28 23:36:55 - train: epoch 0069, iter [01100, 05004], lr: 0.050325, loss: 1.3905
2022-07-28 23:38:46 - train: epoch 0069, iter [01200, 05004], lr: 0.050268, loss: 1.2871
2022-07-28 23:40:33 - train: epoch 0069, iter [01300, 05004], lr: 0.050211, loss: 1.5213
2022-07-28 23:42:24 - train: epoch 0069, iter [01400, 05004], lr: 0.050153, loss: 1.4754
2022-07-28 23:44:10 - train: epoch 0069, iter [01500, 05004], lr: 0.050096, loss: 1.5762
2022-07-28 23:46:06 - train: epoch 0069, iter [01600, 05004], lr: 0.050039, loss: 1.5297
2022-07-28 23:47:52 - train: epoch 0069, iter [01700, 05004], lr: 0.049982, loss: 1.2354
2022-07-28 23:49:41 - train: epoch 0069, iter [01800, 05004], lr: 0.049924, loss: 1.3685
2022-07-28 23:51:30 - train: epoch 0069, iter [01900, 05004], lr: 0.049867, loss: 1.5309
2022-07-28 23:53:18 - train: epoch 0069, iter [02000, 05004], lr: 0.049810, loss: 1.3991
2022-07-28 23:55:04 - train: epoch 0069, iter [02100, 05004], lr: 0.049753, loss: 1.4422
2022-07-28 23:56:55 - train: epoch 0069, iter [02200, 05004], lr: 0.049696, loss: 1.5837
2022-07-28 23:58:44 - train: epoch 0069, iter [02300, 05004], lr: 0.049639, loss: 1.3816
2022-07-29 00:00:31 - train: epoch 0069, iter [02400, 05004], lr: 0.049582, loss: 1.5234
2022-07-29 00:02:22 - train: epoch 0069, iter [02500, 05004], lr: 0.049525, loss: 1.3471
2022-07-29 00:04:06 - train: epoch 0069, iter [02600, 05004], lr: 0.049468, loss: 1.5531
2022-07-29 00:05:58 - train: epoch 0069, iter [02700, 05004], lr: 0.049411, loss: 1.6586
2022-07-29 00:07:48 - train: epoch 0069, iter [02800, 05004], lr: 0.049354, loss: 1.4774
2022-07-29 00:09:38 - train: epoch 0069, iter [02900, 05004], lr: 0.049297, loss: 1.5020
2022-07-29 00:11:28 - train: epoch 0069, iter [03000, 05004], lr: 0.049240, loss: 1.5613
2022-07-29 00:13:11 - train: epoch 0069, iter [03100, 05004], lr: 0.049183, loss: 1.6466
2022-07-29 00:15:03 - train: epoch 0069, iter [03200, 05004], lr: 0.049126, loss: 1.4411
2022-07-29 00:16:56 - train: epoch 0069, iter [03300, 05004], lr: 0.049069, loss: 1.3982
2022-07-29 00:18:49 - train: epoch 0069, iter [03400, 05004], lr: 0.049012, loss: 1.5823
2022-07-29 00:20:42 - train: epoch 0069, iter [03500, 05004], lr: 0.048955, loss: 1.4222
2022-07-29 00:22:36 - train: epoch 0069, iter [03600, 05004], lr: 0.048898, loss: 1.4661
2022-07-29 00:24:17 - train: epoch 0069, iter [03700, 05004], lr: 0.048842, loss: 1.6682
2022-07-29 00:26:16 - train: epoch 0069, iter [03800, 05004], lr: 0.048785, loss: 1.5332
2022-07-29 00:28:18 - train: epoch 0069, iter [03900, 05004], lr: 0.048728, loss: 1.4960
2022-07-29 00:30:08 - train: epoch 0069, iter [04000, 05004], lr: 0.048671, loss: 1.6752
2022-07-29 00:31:59 - train: epoch 0069, iter [04100, 05004], lr: 0.048615, loss: 1.6467
2022-07-29 00:33:49 - train: epoch 0069, iter [04200, 05004], lr: 0.048558, loss: 1.6431
2022-07-29 00:35:40 - train: epoch 0069, iter [04300, 05004], lr: 0.048501, loss: 1.5740
2022-07-29 00:37:30 - train: epoch 0069, iter [04400, 05004], lr: 0.048445, loss: 1.4691
2022-07-29 00:39:19 - train: epoch 0069, iter [04500, 05004], lr: 0.048388, loss: 1.5872
2022-07-29 00:41:08 - train: epoch 0069, iter [04600, 05004], lr: 0.048331, loss: 1.7840
2022-07-29 00:42:56 - train: epoch 0069, iter [04700, 05004], lr: 0.048275, loss: 1.4307
2022-07-29 00:44:47 - train: epoch 0069, iter [04800, 05004], lr: 0.048218, loss: 1.6792
2022-07-29 00:46:38 - train: epoch 0069, iter [04900, 05004], lr: 0.048162, loss: 1.4945
2022-07-29 00:48:25 - train: epoch 0069, iter [05000, 05004], lr: 0.048105, loss: 1.4322
2022-07-29 00:48:30 - train: epoch 069, train_loss: 1.5027
2022-07-29 00:52:23 - eval: epoch: 069, acc1: 67.458%, acc5: 88.390%, test_loss: 1.3276, per_image_load_time: 5.656ms, per_image_inference_time: 0.739ms
2022-07-29 00:52:23 - until epoch: 069, best_acc1: 67.458%
2022-07-29 00:52:23 - epoch 070 lr: 0.048102
2022-07-29 00:54:34 - train: epoch 0070, iter [00100, 05004], lr: 0.048047, loss: 1.5020
2022-07-29 00:56:22 - train: epoch 0070, iter [00200, 05004], lr: 0.047990, loss: 1.5167
2022-07-29 00:58:15 - train: epoch 0070, iter [00300, 05004], lr: 0.047934, loss: 1.5536
2022-07-29 01:00:03 - train: epoch 0070, iter [00400, 05004], lr: 0.047877, loss: 1.3091
2022-07-29 01:01:53 - train: epoch 0070, iter [00500, 05004], lr: 0.047821, loss: 1.6750
2022-07-29 01:03:46 - train: epoch 0070, iter [00600, 05004], lr: 0.047765, loss: 1.4919
2022-07-29 01:05:38 - train: epoch 0070, iter [00700, 05004], lr: 0.047708, loss: 1.4892
2022-07-29 01:07:27 - train: epoch 0070, iter [00800, 05004], lr: 0.047652, loss: 1.5930
2022-07-29 01:09:18 - train: epoch 0070, iter [00900, 05004], lr: 0.047596, loss: 1.3552
2022-07-29 01:11:07 - train: epoch 0070, iter [01000, 05004], lr: 0.047539, loss: 1.4960
2022-07-29 01:12:56 - train: epoch 0070, iter [01100, 05004], lr: 0.047483, loss: 1.6726
2022-07-29 01:14:44 - train: epoch 0070, iter [01200, 05004], lr: 0.047427, loss: 1.3599
2022-07-29 01:16:31 - train: epoch 0070, iter [01300, 05004], lr: 0.047371, loss: 1.3363
2022-07-29 01:18:20 - train: epoch 0070, iter [01400, 05004], lr: 0.047314, loss: 1.5058
2022-07-29 01:20:10 - train: epoch 0070, iter [01500, 05004], lr: 0.047258, loss: 1.3770
2022-07-29 01:21:54 - train: epoch 0070, iter [01600, 05004], lr: 0.047202, loss: 1.4394
2022-07-29 01:23:46 - train: epoch 0070, iter [01700, 05004], lr: 0.047146, loss: 1.4011
2022-07-29 01:25:34 - train: epoch 0070, iter [01800, 05004], lr: 0.047090, loss: 1.4351
2022-07-29 01:27:17 - train: epoch 0070, iter [01900, 05004], lr: 0.047034, loss: 1.4605
2022-07-29 01:29:03 - train: epoch 0070, iter [02000, 05004], lr: 0.046978, loss: 1.4076
2022-07-29 01:30:50 - train: epoch 0070, iter [02100, 05004], lr: 0.046922, loss: 1.6014
2022-07-29 01:32:35 - train: epoch 0070, iter [02200, 05004], lr: 0.046866, loss: 1.4263
2022-07-29 01:34:21 - train: epoch 0070, iter [02300, 05004], lr: 0.046810, loss: 1.5613
2022-07-29 01:36:16 - train: epoch 0070, iter [02400, 05004], lr: 0.046754, loss: 1.4008
2022-07-29 01:37:58 - train: epoch 0070, iter [02500, 05004], lr: 0.046698, loss: 1.5000
2022-07-29 01:39:40 - train: epoch 0070, iter [02600, 05004], lr: 0.046642, loss: 1.4918
2022-07-29 01:41:30 - train: epoch 0070, iter [02700, 05004], lr: 0.046586, loss: 1.5176
2022-07-29 01:43:20 - train: epoch 0070, iter [02800, 05004], lr: 0.046530, loss: 1.6375
2022-07-29 01:45:07 - train: epoch 0070, iter [02900, 05004], lr: 0.046474, loss: 1.6213
2022-07-29 01:46:54 - train: epoch 0070, iter [03000, 05004], lr: 0.046419, loss: 1.4187
2022-07-29 01:48:39 - train: epoch 0070, iter [03100, 05004], lr: 0.046363, loss: 1.4902
2022-07-29 01:50:22 - train: epoch 0070, iter [03200, 05004], lr: 0.046307, loss: 1.8089
2022-07-29 01:52:11 - train: epoch 0070, iter [03300, 05004], lr: 0.046251, loss: 1.3436
2022-07-29 01:53:58 - train: epoch 0070, iter [03400, 05004], lr: 0.046196, loss: 1.5358
2022-07-29 01:55:43 - train: epoch 0070, iter [03500, 05004], lr: 0.046140, loss: 1.4676
2022-07-29 01:57:29 - train: epoch 0070, iter [03600, 05004], lr: 0.046084, loss: 1.5027
2022-07-29 01:59:20 - train: epoch 0070, iter [03700, 05004], lr: 0.046029, loss: 1.3922
2022-07-29 02:01:09 - train: epoch 0070, iter [03800, 05004], lr: 0.045973, loss: 1.4331
2022-07-29 02:03:16 - train: epoch 0070, iter [03900, 05004], lr: 0.045917, loss: 1.3738
2022-07-29 02:05:02 - train: epoch 0070, iter [04000, 05004], lr: 0.045862, loss: 1.7532
2022-07-29 02:07:01 - train: epoch 0070, iter [04100, 05004], lr: 0.045806, loss: 1.4806
2022-07-29 02:08:53 - train: epoch 0070, iter [04200, 05004], lr: 0.045751, loss: 1.5055
2022-07-29 02:10:41 - train: epoch 0070, iter [04300, 05004], lr: 0.045695, loss: 1.4359
2022-07-29 02:12:29 - train: epoch 0070, iter [04400, 05004], lr: 0.045640, loss: 1.5611
2022-07-29 02:14:15 - train: epoch 0070, iter [04500, 05004], lr: 0.045584, loss: 1.4464
2022-07-29 02:16:01 - train: epoch 0070, iter [04600, 05004], lr: 0.045529, loss: 1.5924
2022-07-29 02:17:48 - train: epoch 0070, iter [04700, 05004], lr: 0.045473, loss: 1.6556
2022-07-29 02:19:33 - train: epoch 0070, iter [04800, 05004], lr: 0.045418, loss: 1.4123
2022-07-29 02:21:25 - train: epoch 0070, iter [04900, 05004], lr: 0.045363, loss: 1.4806
2022-07-29 02:23:08 - train: epoch 0070, iter [05000, 05004], lr: 0.045307, loss: 1.5002
2022-07-29 02:23:12 - train: epoch 070, train_loss: 1.4844
2022-07-29 02:27:04 - eval: epoch: 070, acc1: 67.840%, acc5: 88.560%, test_loss: 1.3216, per_image_load_time: 8.130ms, per_image_inference_time: 0.729ms
2022-07-29 02:27:05 - until epoch: 070, best_acc1: 67.840%
2022-07-29 02:27:05 - epoch 071 lr: 0.045305
2022-07-29 02:29:13 - train: epoch 0071, iter [00100, 05004], lr: 0.045250, loss: 1.2341
2022-07-29 02:31:04 - train: epoch 0071, iter [00200, 05004], lr: 0.045195, loss: 1.4137
2022-07-29 02:32:58 - train: epoch 0071, iter [00300, 05004], lr: 0.045139, loss: 1.4738
2022-07-29 02:34:46 - train: epoch 0071, iter [00400, 05004], lr: 0.045084, loss: 1.5444
2022-07-29 02:36:39 - train: epoch 0071, iter [00500, 05004], lr: 0.045029, loss: 1.6743
2022-07-29 02:38:25 - train: epoch 0071, iter [00600, 05004], lr: 0.044974, loss: 1.6656
2022-07-29 02:40:11 - train: epoch 0071, iter [00700, 05004], lr: 0.044918, loss: 1.5023
2022-07-29 02:41:56 - train: epoch 0071, iter [00800, 05004], lr: 0.044863, loss: 1.6032
2022-07-29 02:43:43 - train: epoch 0071, iter [00900, 05004], lr: 0.044808, loss: 1.5765
2022-07-29 02:45:27 - train: epoch 0071, iter [01000, 05004], lr: 0.044753, loss: 1.5081
2022-07-29 02:47:16 - train: epoch 0071, iter [01100, 05004], lr: 0.044698, loss: 1.4922
2022-07-29 02:49:02 - train: epoch 0071, iter [01200, 05004], lr: 0.044643, loss: 1.5920
2022-07-29 02:50:51 - train: epoch 0071, iter [01300, 05004], lr: 0.044588, loss: 1.5102
2022-07-29 02:52:37 - train: epoch 0071, iter [01400, 05004], lr: 0.044533, loss: 1.3050
2022-07-29 02:54:24 - train: epoch 0071, iter [01500, 05004], lr: 0.044478, loss: 1.2825
2022-07-29 02:56:20 - train: epoch 0071, iter [01600, 05004], lr: 0.044423, loss: 1.3593
2022-07-29 02:58:14 - train: epoch 0071, iter [01700, 05004], lr: 0.044368, loss: 1.5374
2022-07-29 03:00:08 - train: epoch 0071, iter [01800, 05004], lr: 0.044313, loss: 1.5386
2022-07-29 03:01:59 - train: epoch 0071, iter [01900, 05004], lr: 0.044258, loss: 1.4967
2022-07-29 03:03:49 - train: epoch 0071, iter [02000, 05004], lr: 0.044203, loss: 1.4948
2022-07-29 03:05:42 - train: epoch 0071, iter [02100, 05004], lr: 0.044149, loss: 1.4194
2022-07-29 03:07:43 - train: epoch 0071, iter [02200, 05004], lr: 0.044094, loss: 1.4038
2022-07-29 03:09:47 - train: epoch 0071, iter [02300, 05004], lr: 0.044039, loss: 1.4175
2022-07-29 03:11:48 - train: epoch 0071, iter [02400, 05004], lr: 0.043984, loss: 1.4222
2022-07-29 03:13:48 - train: epoch 0071, iter [02500, 05004], lr: 0.043930, loss: 1.4987
2022-07-29 03:15:47 - train: epoch 0071, iter [02600, 05004], lr: 0.043875, loss: 1.3526
2022-07-29 03:17:45 - train: epoch 0071, iter [02700, 05004], lr: 0.043820, loss: 1.5622
2022-07-29 03:19:45 - train: epoch 0071, iter [02800, 05004], lr: 0.043766, loss: 1.4853
2022-07-29 03:21:46 - train: epoch 0071, iter [02900, 05004], lr: 0.043711, loss: 1.3239
2022-07-29 03:23:48 - train: epoch 0071, iter [03000, 05004], lr: 0.043656, loss: 1.5215
2022-07-29 03:25:48 - train: epoch 0071, iter [03100, 05004], lr: 0.043602, loss: 1.3367
2022-07-29 03:27:49 - train: epoch 0071, iter [03200, 05004], lr: 0.043547, loss: 1.3630
2022-07-29 03:29:46 - train: epoch 0071, iter [03300, 05004], lr: 0.043493, loss: 1.4788
2022-07-29 03:31:45 - train: epoch 0071, iter [03400, 05004], lr: 0.043438, loss: 1.3262
2022-07-29 03:33:43 - train: epoch 0071, iter [03500, 05004], lr: 0.043384, loss: 1.4878
2022-07-29 03:35:45 - train: epoch 0071, iter [03600, 05004], lr: 0.043329, loss: 1.6700
2022-07-29 03:37:37 - train: epoch 0071, iter [03700, 05004], lr: 0.043275, loss: 1.5918
2022-07-29 03:39:27 - train: epoch 0071, iter [03800, 05004], lr: 0.043220, loss: 1.4803
2022-07-29 03:41:44 - train: epoch 0071, iter [03900, 05004], lr: 0.043166, loss: 1.5428
2022-07-29 03:43:35 - train: epoch 0071, iter [04000, 05004], lr: 0.043112, loss: 1.4626
2022-07-29 03:45:44 - train: epoch 0071, iter [04100, 05004], lr: 0.043057, loss: 1.4887
2022-07-29 03:47:42 - train: epoch 0071, iter [04200, 05004], lr: 0.043003, loss: 1.5872
2022-07-29 03:49:41 - train: epoch 0071, iter [04300, 05004], lr: 0.042949, loss: 1.4227
2022-07-29 03:51:36 - train: epoch 0071, iter [04400, 05004], lr: 0.042894, loss: 1.5672
2022-07-29 03:53:33 - train: epoch 0071, iter [04500, 05004], lr: 0.042840, loss: 1.4501
2022-07-29 03:55:28 - train: epoch 0071, iter [04600, 05004], lr: 0.042786, loss: 1.4305
2022-07-29 03:57:25 - train: epoch 0071, iter [04700, 05004], lr: 0.042732, loss: 1.4220
2022-07-29 03:59:20 - train: epoch 0071, iter [04800, 05004], lr: 0.042678, loss: 1.4809
2022-07-29 04:01:17 - train: epoch 0071, iter [04900, 05004], lr: 0.042623, loss: 1.2860
2022-07-29 04:03:08 - train: epoch 0071, iter [05000, 05004], lr: 0.042569, loss: 1.5184
2022-07-29 04:03:12 - train: epoch 071, train_loss: 1.4634
2022-07-29 04:07:01 - eval: epoch: 071, acc1: 68.024%, acc5: 88.580%, test_loss: 1.2985, per_image_load_time: 8.116ms, per_image_inference_time: 0.778ms
2022-07-29 04:07:01 - until epoch: 071, best_acc1: 68.024%
2022-07-29 04:07:01 - epoch 072 lr: 0.042567
2022-07-29 04:09:10 - train: epoch 0072, iter [00100, 05004], lr: 0.042513, loss: 1.4150
2022-07-29 04:11:07 - train: epoch 0072, iter [00200, 05004], lr: 0.042459, loss: 1.2683
2022-07-29 04:13:00 - train: epoch 0072, iter [00300, 05004], lr: 0.042405, loss: 1.2657
2022-07-29 04:14:54 - train: epoch 0072, iter [00400, 05004], lr: 0.042351, loss: 1.4853
2022-07-29 04:16:44 - train: epoch 0072, iter [00500, 05004], lr: 0.042297, loss: 1.4765
2022-07-29 04:18:42 - train: epoch 0072, iter [00600, 05004], lr: 0.042243, loss: 1.2865
2022-07-29 04:20:32 - train: epoch 0072, iter [00700, 05004], lr: 0.042189, loss: 1.3648
2022-07-29 04:22:27 - train: epoch 0072, iter [00800, 05004], lr: 0.042135, loss: 1.6315
2022-07-29 04:24:18 - train: epoch 0072, iter [00900, 05004], lr: 0.042081, loss: 1.3327
2022-07-29 04:26:13 - train: epoch 0072, iter [01000, 05004], lr: 0.042027, loss: 1.4392
2022-07-29 04:28:13 - train: epoch 0072, iter [01100, 05004], lr: 0.041974, loss: 1.4473
2022-07-29 04:30:08 - train: epoch 0072, iter [01200, 05004], lr: 0.041920, loss: 1.3030
2022-07-29 04:32:01 - train: epoch 0072, iter [01300, 05004], lr: 0.041866, loss: 1.5053
2022-07-29 04:33:53 - train: epoch 0072, iter [01400, 05004], lr: 0.041812, loss: 1.3469
2022-07-29 04:35:46 - train: epoch 0072, iter [01500, 05004], lr: 0.041758, loss: 1.6244
2022-07-29 04:37:43 - train: epoch 0072, iter [01600, 05004], lr: 0.041705, loss: 1.3949
2022-07-29 04:39:37 - train: epoch 0072, iter [01700, 05004], lr: 0.041651, loss: 1.5305
2022-07-29 04:41:30 - train: epoch 0072, iter [01800, 05004], lr: 0.041597, loss: 1.3553
2022-07-29 04:43:26 - train: epoch 0072, iter [01900, 05004], lr: 0.041544, loss: 1.3998
2022-07-29 04:45:23 - train: epoch 0072, iter [02000, 05004], lr: 0.041490, loss: 1.4641
2022-07-29 04:47:13 - train: epoch 0072, iter [02100, 05004], lr: 0.041437, loss: 1.6031
2022-07-29 04:49:05 - train: epoch 0072, iter [02200, 05004], lr: 0.041383, loss: 1.5264
2022-07-29 04:51:03 - train: epoch 0072, iter [02300, 05004], lr: 0.041330, loss: 1.4983
2022-07-29 04:52:53 - train: epoch 0072, iter [02400, 05004], lr: 0.041276, loss: 1.4843
2022-07-29 04:54:47 - train: epoch 0072, iter [02500, 05004], lr: 0.041223, loss: 1.4318
2022-07-29 04:56:40 - train: epoch 0072, iter [02600, 05004], lr: 0.041169, loss: 1.4584
2022-07-29 04:58:35 - train: epoch 0072, iter [02700, 05004], lr: 0.041116, loss: 1.4252
2022-07-29 05:00:29 - train: epoch 0072, iter [02800, 05004], lr: 0.041062, loss: 1.4558
2022-07-29 05:02:23 - train: epoch 0072, iter [02900, 05004], lr: 0.041009, loss: 1.2372
2022-07-29 05:04:16 - train: epoch 0072, iter [03000, 05004], lr: 0.040956, loss: 1.6595
2022-07-29 05:06:13 - train: epoch 0072, iter [03100, 05004], lr: 0.040902, loss: 1.3671
2022-07-29 05:08:03 - train: epoch 0072, iter [03200, 05004], lr: 0.040849, loss: 1.5479
2022-07-29 05:09:58 - train: epoch 0072, iter [03300, 05004], lr: 0.040796, loss: 1.4843
2022-07-29 05:11:53 - train: epoch 0072, iter [03400, 05004], lr: 0.040742, loss: 1.5805
2022-07-29 05:13:43 - train: epoch 0072, iter [03500, 05004], lr: 0.040689, loss: 1.5189
2022-07-29 05:15:34 - train: epoch 0072, iter [03600, 05004], lr: 0.040636, loss: 1.5055
2022-07-29 05:17:32 - train: epoch 0072, iter [03700, 05004], lr: 0.040583, loss: 1.7107
2022-07-29 05:19:28 - train: epoch 0072, iter [03800, 05004], lr: 0.040530, loss: 1.3820
2022-07-29 05:21:13 - train: epoch 0072, iter [03900, 05004], lr: 0.040477, loss: 1.4717
2022-07-29 05:23:20 - train: epoch 0072, iter [04000, 05004], lr: 0.040423, loss: 1.3644
2022-07-29 05:25:09 - train: epoch 0072, iter [04100, 05004], lr: 0.040370, loss: 1.5932
2022-07-29 05:27:13 - train: epoch 0072, iter [04200, 05004], lr: 0.040317, loss: 1.3998
2022-07-29 05:29:14 - train: epoch 0072, iter [04300, 05004], lr: 0.040264, loss: 1.3784
2022-07-29 05:31:09 - train: epoch 0072, iter [04400, 05004], lr: 0.040211, loss: 1.3000
2022-07-29 05:33:03 - train: epoch 0072, iter [04500, 05004], lr: 0.040158, loss: 1.4763
2022-07-29 05:35:00 - train: epoch 0072, iter [04600, 05004], lr: 0.040105, loss: 1.3214
2022-07-29 05:37:02 - train: epoch 0072, iter [04700, 05004], lr: 0.040053, loss: 1.4595
2022-07-29 05:38:58 - train: epoch 0072, iter [04800, 05004], lr: 0.040000, loss: 1.6885
2022-07-29 05:40:54 - train: epoch 0072, iter [04900, 05004], lr: 0.039947, loss: 1.3491
2022-07-29 05:42:47 - train: epoch 0072, iter [05000, 05004], lr: 0.039894, loss: 1.2530
2022-07-29 05:42:51 - train: epoch 072, train_loss: 1.4430
2022-07-29 05:46:41 - eval: epoch: 072, acc1: 68.480%, acc5: 88.832%, test_loss: 1.2815, per_image_load_time: 8.155ms, per_image_inference_time: 0.795ms
2022-07-29 05:46:42 - until epoch: 072, best_acc1: 68.480%
2022-07-29 05:46:42 - epoch 073 lr: 0.039891
2022-07-29 05:48:59 - train: epoch 0073, iter [00100, 05004], lr: 0.039839, loss: 1.5086
2022-07-29 05:50:58 - train: epoch 0073, iter [00200, 05004], lr: 0.039786, loss: 1.3411
2022-07-29 05:52:48 - train: epoch 0073, iter [00300, 05004], lr: 0.039734, loss: 1.4922
2022-07-29 05:54:37 - train: epoch 0073, iter [00400, 05004], lr: 0.039681, loss: 1.1852
2022-07-29 05:56:32 - train: epoch 0073, iter [00500, 05004], lr: 0.039628, loss: 1.4357
2022-07-29 05:58:29 - train: epoch 0073, iter [00600, 05004], lr: 0.039575, loss: 1.3902
2022-07-29 06:00:24 - train: epoch 0073, iter [00700, 05004], lr: 0.039523, loss: 1.3313
2022-07-29 06:02:17 - train: epoch 0073, iter [00800, 05004], lr: 0.039470, loss: 1.3730
2022-07-29 06:04:14 - train: epoch 0073, iter [00900, 05004], lr: 0.039418, loss: 1.2742
2022-07-29 06:06:09 - train: epoch 0073, iter [01000, 05004], lr: 0.039365, loss: 1.3652
2022-07-29 06:08:00 - train: epoch 0073, iter [01100, 05004], lr: 0.039313, loss: 1.2988
2022-07-29 06:09:55 - train: epoch 0073, iter [01200, 05004], lr: 0.039260, loss: 1.5369
2022-07-29 06:11:52 - train: epoch 0073, iter [01300, 05004], lr: 0.039208, loss: 1.5808
2022-07-29 06:13:39 - train: epoch 0073, iter [01400, 05004], lr: 0.039155, loss: 1.2644
2022-07-29 06:15:31 - train: epoch 0073, iter [01500, 05004], lr: 0.039103, loss: 1.4075
2022-07-29 06:17:27 - train: epoch 0073, iter [01600, 05004], lr: 0.039050, loss: 1.4568
2022-07-29 06:19:21 - train: epoch 0073, iter [01700, 05004], lr: 0.038998, loss: 1.4527
2022-07-29 06:21:17 - train: epoch 0073, iter [01800, 05004], lr: 0.038945, loss: 1.4220
2022-07-29 06:23:09 - train: epoch 0073, iter [01900, 05004], lr: 0.038893, loss: 1.5602
2022-07-29 06:25:05 - train: epoch 0073, iter [02000, 05004], lr: 0.038841, loss: 1.2443
2022-07-29 06:26:57 - train: epoch 0073, iter [02100, 05004], lr: 0.038789, loss: 1.3089
2022-07-29 06:28:58 - train: epoch 0073, iter [02200, 05004], lr: 0.038736, loss: 1.4159
2022-07-29 06:30:46 - train: epoch 0073, iter [02300, 05004], lr: 0.038684, loss: 1.2346
2022-07-29 06:32:44 - train: epoch 0073, iter [02400, 05004], lr: 0.038632, loss: 1.4333
2022-07-29 06:34:38 - train: epoch 0073, iter [02500, 05004], lr: 0.038580, loss: 1.4518
2022-07-29 06:36:33 - train: epoch 0073, iter [02600, 05004], lr: 0.038528, loss: 1.2514
2022-07-29 06:38:29 - train: epoch 0073, iter [02700, 05004], lr: 0.038476, loss: 1.3318
2022-07-29 06:40:21 - train: epoch 0073, iter [02800, 05004], lr: 0.038423, loss: 1.2678
2022-07-29 06:42:13 - train: epoch 0073, iter [02900, 05004], lr: 0.038371, loss: 1.5897
2022-07-29 06:44:08 - train: epoch 0073, iter [03000, 05004], lr: 0.038319, loss: 1.1952
2022-07-29 06:46:06 - train: epoch 0073, iter [03100, 05004], lr: 0.038267, loss: 1.3221
2022-07-29 06:47:59 - train: epoch 0073, iter [03200, 05004], lr: 0.038215, loss: 1.2687
2022-07-29 06:49:54 - train: epoch 0073, iter [03300, 05004], lr: 0.038163, loss: 1.5270
2022-07-29 06:51:50 - train: epoch 0073, iter [03400, 05004], lr: 0.038111, loss: 1.3913
2022-07-29 06:53:46 - train: epoch 0073, iter [03500, 05004], lr: 0.038060, loss: 1.3792
2022-07-29 06:55:44 - train: epoch 0073, iter [03600, 05004], lr: 0.038008, loss: 1.3405
2022-07-29 06:57:34 - train: epoch 0073, iter [03700, 05004], lr: 0.037956, loss: 1.3415
2022-07-29 06:59:27 - train: epoch 0073, iter [03800, 05004], lr: 0.037904, loss: 1.4026
2022-07-29 07:01:22 - train: epoch 0073, iter [03900, 05004], lr: 0.037852, loss: 1.5015
2022-07-29 07:03:18 - train: epoch 0073, iter [04000, 05004], lr: 0.037801, loss: 1.5745
2022-07-29 07:05:22 - train: epoch 0073, iter [04100, 05004], lr: 0.037749, loss: 1.5417
2022-07-29 07:07:07 - train: epoch 0073, iter [04200, 05004], lr: 0.037697, loss: 1.4901
2022-07-29 07:09:11 - train: epoch 0073, iter [04300, 05004], lr: 0.037645, loss: 1.4664
2022-07-29 07:11:10 - train: epoch 0073, iter [04400, 05004], lr: 0.037594, loss: 1.4935
2022-07-29 07:13:05 - train: epoch 0073, iter [04500, 05004], lr: 0.037542, loss: 1.3596
