2022-07-26 07:12:14 - train: epoch 0031, iter [04900, 05004], lr: 0.165311, loss: 1.8657
2022-07-26 07:14:08 - train: epoch 0031, iter [05000, 05004], lr: 0.165261, loss: 1.9752
2022-07-26 07:14:11 - train: epoch 031, train_loss: 2.0000
2022-07-26 07:18:05 - eval: epoch: 031, acc1: 59.658%, acc5: 83.398%, test_loss: 1.6845, per_image_load_time: 2.431ms, per_image_inference_time: 0.774ms
2022-07-26 07:18:05 - until epoch: 031, best_acc1: 59.658%
2022-07-26 07:18:05 - epoch 032 lr: 0.165258
2022-07-26 07:20:12 - train: epoch 0032, iter [00100, 05004], lr: 0.165208, loss: 1.9055
2022-07-26 07:22:00 - train: epoch 0032, iter [00200, 05004], lr: 0.165158, loss: 2.0873
2022-07-26 07:23:41 - train: epoch 0032, iter [00300, 05004], lr: 0.165108, loss: 1.6727
2022-07-26 07:25:59 - train: epoch 0032, iter [00400, 05004], lr: 0.165058, loss: 2.0414
2022-07-26 07:27:55 - train: epoch 0032, iter [00500, 05004], lr: 0.165008, loss: 2.1249
2022-07-26 07:29:57 - train: epoch 0032, iter [00600, 05004], lr: 0.164958, loss: 1.9379
2022-07-26 07:31:54 - train: epoch 0032, iter [00700, 05004], lr: 0.164907, loss: 1.9603
2022-07-26 07:33:48 - train: epoch 0032, iter [00800, 05004], lr: 0.164857, loss: 1.7906
2022-07-26 07:35:49 - train: epoch 0032, iter [00900, 05004], lr: 0.164807, loss: 2.0347
2022-07-26 07:37:46 - train: epoch 0032, iter [01000, 05004], lr: 0.164756, loss: 1.9822
2022-07-26 07:39:38 - train: epoch 0032, iter [01100, 05004], lr: 0.164706, loss: 1.9913
2022-07-26 07:41:37 - train: epoch 0032, iter [01200, 05004], lr: 0.164656, loss: 2.1096
2022-07-26 07:43:30 - train: epoch 0032, iter [01300, 05004], lr: 0.164605, loss: 1.7985
2022-07-26 07:45:22 - train: epoch 0032, iter [01400, 05004], lr: 0.164555, loss: 2.2874
2022-07-26 07:47:16 - train: epoch 0032, iter [01500, 05004], lr: 0.164504, loss: 1.9388
2022-07-26 07:49:04 - train: epoch 0032, iter [01600, 05004], lr: 0.164454, loss: 1.9417
2022-07-26 07:50:57 - train: epoch 0032, iter [01700, 05004], lr: 0.164403, loss: 1.9288
2022-07-26 07:52:50 - train: epoch 0032, iter [01800, 05004], lr: 0.164353, loss: 2.3522
2022-07-26 07:54:44 - train: epoch 0032, iter [01900, 05004], lr: 0.164302, loss: 1.7676
2022-07-26 07:56:34 - train: epoch 0032, iter [02000, 05004], lr: 0.164251, loss: 1.9871
2022-07-26 07:58:25 - train: epoch 0032, iter [02100, 05004], lr: 0.164201, loss: 2.0064
2022-07-26 08:00:17 - train: epoch 0032, iter [02200, 05004], lr: 0.164150, loss: 1.8181
2022-07-26 08:02:09 - train: epoch 0032, iter [02300, 05004], lr: 0.164099, loss: 1.8515
2022-07-26 08:03:59 - train: epoch 0032, iter [02400, 05004], lr: 0.164049, loss: 1.8348
2022-07-26 08:05:55 - train: epoch 0032, iter [02500, 05004], lr: 0.163998, loss: 2.2165
2022-07-26 08:07:44 - train: epoch 0032, iter [02600, 05004], lr: 0.163947, loss: 1.7729
2022-07-26 08:09:34 - train: epoch 0032, iter [02700, 05004], lr: 0.163896, loss: 1.9568
2022-07-26 08:11:25 - train: epoch 0032, iter [02800, 05004], lr: 0.163845, loss: 2.0957
2022-07-26 08:13:16 - train: epoch 0032, iter [02900, 05004], lr: 0.163795, loss: 2.0682
2022-07-26 08:15:07 - train: epoch 0032, iter [03000, 05004], lr: 0.163744, loss: 1.7851
2022-07-26 08:16:56 - train: epoch 0032, iter [03100, 05004], lr: 0.163693, loss: 2.1244
2022-07-26 08:18:49 - train: epoch 0032, iter [03200, 05004], lr: 0.163642, loss: 2.2426
2022-07-26 08:20:45 - train: epoch 0032, iter [03300, 05004], lr: 0.163591, loss: 2.0789
2022-07-26 08:22:36 - train: epoch 0032, iter [03400, 05004], lr: 0.163540, loss: 1.7275
2022-07-26 08:24:29 - train: epoch 0032, iter [03500, 05004], lr: 0.163489, loss: 1.8094
2022-07-26 08:26:22 - train: epoch 0032, iter [03600, 05004], lr: 0.163438, loss: 2.2179
2022-07-26 08:28:17 - train: epoch 0032, iter [03700, 05004], lr: 0.163387, loss: 1.9117
2022-07-26 08:30:10 - train: epoch 0032, iter [03800, 05004], lr: 0.163335, loss: 1.7887
2022-07-26 08:32:01 - train: epoch 0032, iter [03900, 05004], lr: 0.163284, loss: 1.8843
2022-07-26 08:33:54 - train: epoch 0032, iter [04000, 05004], lr: 0.163233, loss: 1.9899
2022-07-26 08:35:47 - train: epoch 0032, iter [04100, 05004], lr: 0.163182, loss: 1.9237
2022-07-26 08:37:40 - train: epoch 0032, iter [04200, 05004], lr: 0.163131, loss: 1.8071
2022-07-26 08:39:29 - train: epoch 0032, iter [04300, 05004], lr: 0.163079, loss: 2.3002
2022-07-26 08:41:24 - train: epoch 0032, iter [04400, 05004], lr: 0.163028, loss: 1.9220
2022-07-26 08:43:16 - train: epoch 0032, iter [04500, 05004], lr: 0.162977, loss: 2.0323
2022-07-26 08:45:14 - train: epoch 0032, iter [04600, 05004], lr: 0.162925, loss: 1.9972
2022-07-26 08:47:11 - train: epoch 0032, iter [04700, 05004], lr: 0.162874, loss: 2.0977
2022-07-26 08:49:16 - train: epoch 0032, iter [04800, 05004], lr: 0.162823, loss: 1.9114
2022-07-26 08:51:11 - train: epoch 0032, iter [04900, 05004], lr: 0.162771, loss: 1.9671
2022-07-26 08:53:07 - train: epoch 0032, iter [05000, 05004], lr: 0.162720, loss: 2.0582
2022-07-26 08:53:11 - train: epoch 032, train_loss: 1.9898
2022-07-26 08:57:06 - eval: epoch: 032, acc1: 59.288%, acc5: 82.892%, test_loss: 1.7057, per_image_load_time: 7.935ms, per_image_inference_time: 0.746ms
2022-07-26 08:57:06 - until epoch: 032, best_acc1: 59.658%
2022-07-26 08:57:06 - epoch 033 lr: 0.162717
2022-07-26 08:59:13 - train: epoch 0033, iter [00100, 05004], lr: 0.162666, loss: 1.8083
2022-07-26 09:01:12 - train: epoch 0033, iter [00200, 05004], lr: 0.162615, loss: 2.2157
2022-07-26 09:03:02 - train: epoch 0033, iter [00300, 05004], lr: 0.162563, loss: 1.9173
2022-07-26 09:04:58 - train: epoch 0033, iter [00400, 05004], lr: 0.162512, loss: 1.7071
2022-07-26 09:07:05 - train: epoch 0033, iter [00500, 05004], lr: 0.162460, loss: 1.8023
2022-07-26 09:09:01 - train: epoch 0033, iter [00600, 05004], lr: 0.162408, loss: 1.9057
2022-07-26 09:11:02 - train: epoch 0033, iter [00700, 05004], lr: 0.162357, loss: 2.1074
2022-07-26 09:13:05 - train: epoch 0033, iter [00800, 05004], lr: 0.162305, loss: 2.0617
2022-07-26 09:14:59 - train: epoch 0033, iter [00900, 05004], lr: 0.162253, loss: 1.9827
2022-07-26 09:17:02 - train: epoch 0033, iter [01000, 05004], lr: 0.162202, loss: 1.9367
2022-07-26 09:18:57 - train: epoch 0033, iter [01100, 05004], lr: 0.162150, loss: 1.8038
2022-07-26 09:20:53 - train: epoch 0033, iter [01200, 05004], lr: 0.162098, loss: 2.1864
2022-07-26 09:22:49 - train: epoch 0033, iter [01300, 05004], lr: 0.162046, loss: 1.8761
2022-07-26 09:24:43 - train: epoch 0033, iter [01400, 05004], lr: 0.161994, loss: 1.9501
2022-07-26 09:26:41 - train: epoch 0033, iter [01500, 05004], lr: 0.161942, loss: 2.2980
2022-07-26 09:28:34 - train: epoch 0033, iter [01600, 05004], lr: 0.161891, loss: 2.0330
2022-07-26 09:30:30 - train: epoch 0033, iter [01700, 05004], lr: 0.161839, loss: 1.9779
2022-07-26 09:32:24 - train: epoch 0033, iter [01800, 05004], lr: 0.161787, loss: 1.9999
2022-07-26 09:34:16 - train: epoch 0033, iter [01900, 05004], lr: 0.161735, loss: 2.0529
2022-07-26 09:36:07 - train: epoch 0033, iter [02000, 05004], lr: 0.161683, loss: 1.9895
2022-07-26 09:38:00 - train: epoch 0033, iter [02100, 05004], lr: 0.161631, loss: 2.0378
2022-07-26 09:39:56 - train: epoch 0033, iter [02200, 05004], lr: 0.161579, loss: 2.0239
2022-07-26 09:42:01 - train: epoch 0033, iter [02300, 05004], lr: 0.161527, loss: 2.0617
2022-07-26 09:43:57 - train: epoch 0033, iter [02400, 05004], lr: 0.161474, loss: 2.1542
2022-07-26 09:45:50 - train: epoch 0033, iter [02500, 05004], lr: 0.161422, loss: 1.9891
2022-07-26 09:47:48 - train: epoch 0033, iter [02600, 05004], lr: 0.161370, loss: 1.8881
2022-07-26 09:49:44 - train: epoch 0033, iter [02700, 05004], lr: 0.161318, loss: 1.8647
2022-07-26 09:51:40 - train: epoch 0033, iter [02800, 05004], lr: 0.161266, loss: 2.1667
2022-07-26 09:53:31 - train: epoch 0033, iter [02900, 05004], lr: 0.161213, loss: 2.1094
2022-07-26 09:55:30 - train: epoch 0033, iter [03000, 05004], lr: 0.161161, loss: 2.0380
2022-07-26 09:57:30 - train: epoch 0033, iter [03100, 05004], lr: 0.161109, loss: 1.9935
2022-07-26 09:59:22 - train: epoch 0033, iter [03200, 05004], lr: 0.161057, loss: 2.0433
2022-07-26 10:01:18 - train: epoch 0033, iter [03300, 05004], lr: 0.161004, loss: 1.7244
2022-07-26 10:03:11 - train: epoch 0033, iter [03400, 05004], lr: 0.160952, loss: 1.9332
2022-07-26 10:05:10 - train: epoch 0033, iter [03500, 05004], lr: 0.160899, loss: 2.0970
2022-07-26 10:07:02 - train: epoch 0033, iter [03600, 05004], lr: 0.160847, loss: 2.0121
2022-07-26 10:09:00 - train: epoch 0033, iter [03700, 05004], lr: 0.160795, loss: 1.8449
2022-07-26 10:10:55 - train: epoch 0033, iter [03800, 05004], lr: 0.160742, loss: 2.0730
2022-07-26 10:12:51 - train: epoch 0033, iter [03900, 05004], lr: 0.160690, loss: 2.2109
2022-07-26 10:14:46 - train: epoch 0033, iter [04000, 05004], lr: 0.160637, loss: 2.1532
2022-07-26 10:16:42 - train: epoch 0033, iter [04100, 05004], lr: 0.160584, loss: 1.9441
2022-07-26 10:18:41 - train: epoch 0033, iter [04200, 05004], lr: 0.160532, loss: 2.0261
2022-07-26 10:20:33 - train: epoch 0033, iter [04300, 05004], lr: 0.160479, loss: 1.9125
2022-07-26 10:22:28 - train: epoch 0033, iter [04400, 05004], lr: 0.160427, loss: 2.0514
2022-07-26 10:24:25 - train: epoch 0033, iter [04500, 05004], lr: 0.160374, loss: 2.2266
2022-07-26 10:26:25 - train: epoch 0033, iter [04600, 05004], lr: 0.160321, loss: 2.0652
2022-07-26 10:28:24 - train: epoch 0033, iter [04700, 05004], lr: 0.160269, loss: 1.8752
2022-07-26 10:30:20 - train: epoch 0033, iter [04800, 05004], lr: 0.160216, loss: 2.2774
2022-07-26 10:32:17 - train: epoch 0033, iter [04900, 05004], lr: 0.160163, loss: 1.6651
2022-07-26 10:34:10 - train: epoch 0033, iter [05000, 05004], lr: 0.160110, loss: 2.2209
2022-07-26 10:34:14 - train: epoch 033, train_loss: 1.9802
2022-07-26 10:38:00 - eval: epoch: 033, acc1: 58.522%, acc5: 82.688%, test_loss: 1.7382, per_image_load_time: 3.586ms, per_image_inference_time: 0.784ms
2022-07-26 10:38:01 - until epoch: 033, best_acc1: 59.658%
2022-07-26 10:38:01 - epoch 034 lr: 0.160108
2022-07-26 10:40:18 - train: epoch 0034, iter [00100, 05004], lr: 0.160055, loss: 1.8434
2022-07-26 10:42:10 - train: epoch 0034, iter [00200, 05004], lr: 0.160002, loss: 1.8262
2022-07-26 10:44:06 - train: epoch 0034, iter [00300, 05004], lr: 0.159950, loss: 1.9107
2022-07-26 10:46:03 - train: epoch 0034, iter [00400, 05004], lr: 0.159897, loss: 1.8083
2022-07-26 10:47:58 - train: epoch 0034, iter [00500, 05004], lr: 0.159844, loss: 1.8594
2022-07-26 10:50:00 - train: epoch 0034, iter [00600, 05004], lr: 0.159791, loss: 1.9407
2022-07-26 10:51:53 - train: epoch 0034, iter [00700, 05004], lr: 0.159738, loss: 1.8141
2022-07-26 10:53:49 - train: epoch 0034, iter [00800, 05004], lr: 0.159685, loss: 2.0076
2022-07-26 10:55:48 - train: epoch 0034, iter [00900, 05004], lr: 0.159632, loss: 1.9800
2022-07-26 10:57:51 - train: epoch 0034, iter [01000, 05004], lr: 0.159579, loss: 1.8816
2022-07-26 10:59:52 - train: epoch 0034, iter [01100, 05004], lr: 0.159526, loss: 1.8466
2022-07-26 11:01:49 - train: epoch 0034, iter [01200, 05004], lr: 0.159472, loss: 2.1914
2022-07-26 11:03:40 - train: epoch 0034, iter [01300, 05004], lr: 0.159419, loss: 1.7300
2022-07-26 11:05:34 - train: epoch 0034, iter [01400, 05004], lr: 0.159366, loss: 2.0434
2022-07-26 11:07:34 - train: epoch 0034, iter [01500, 05004], lr: 0.159313, loss: 1.8950
2022-07-26 11:09:31 - train: epoch 0034, iter [01600, 05004], lr: 0.159260, loss: 2.0033
2022-07-26 11:11:30 - train: epoch 0034, iter [01700, 05004], lr: 0.159206, loss: 2.0904
2022-07-26 11:13:20 - train: epoch 0034, iter [01800, 05004], lr: 0.159153, loss: 2.1395
2022-07-26 11:15:22 - train: epoch 0034, iter [01900, 05004], lr: 0.159100, loss: 1.9921
2022-07-26 11:17:21 - train: epoch 0034, iter [02000, 05004], lr: 0.159047, loss: 1.9367
2022-07-26 11:19:14 - train: epoch 0034, iter [02100, 05004], lr: 0.158993, loss: 1.9610
2022-07-26 11:21:15 - train: epoch 0034, iter [02200, 05004], lr: 0.158940, loss: 1.7610
2022-07-26 11:23:11 - train: epoch 0034, iter [02300, 05004], lr: 0.158886, loss: 1.9655
2022-07-26 11:25:04 - train: epoch 0034, iter [02400, 05004], lr: 0.158833, loss: 1.7342
2022-07-26 11:27:01 - train: epoch 0034, iter [02500, 05004], lr: 0.158780, loss: 1.9506
2022-07-26 11:28:55 - train: epoch 0034, iter [02600, 05004], lr: 0.158726, loss: 1.9284
2022-07-26 11:30:54 - train: epoch 0034, iter [02700, 05004], lr: 0.158673, loss: 2.0360
2022-07-26 11:32:54 - train: epoch 0034, iter [02800, 05004], lr: 0.158619, loss: 1.7851
2022-07-26 11:34:51 - train: epoch 0034, iter [02900, 05004], lr: 0.158566, loss: 1.9193
2022-07-26 11:36:49 - train: epoch 0034, iter [03000, 05004], lr: 0.158512, loss: 1.8956
2022-07-26 11:38:40 - train: epoch 0034, iter [03100, 05004], lr: 0.158458, loss: 1.9781
2022-07-26 11:40:41 - train: epoch 0034, iter [03200, 05004], lr: 0.158405, loss: 2.1158
2022-07-26 11:42:40 - train: epoch 0034, iter [03300, 05004], lr: 0.158351, loss: 1.9328
2022-07-26 11:44:35 - train: epoch 0034, iter [03400, 05004], lr: 0.158297, loss: 2.0005
2022-07-26 11:46:33 - train: epoch 0034, iter [03500, 05004], lr: 0.158244, loss: 1.7959
2022-07-26 11:48:28 - train: epoch 0034, iter [03600, 05004], lr: 0.158190, loss: 1.7106
2022-07-26 11:50:24 - train: epoch 0034, iter [03700, 05004], lr: 0.158136, loss: 2.1869
2022-07-26 11:52:17 - train: epoch 0034, iter [03800, 05004], lr: 0.158082, loss: 1.9602
2022-07-26 11:54:11 - train: epoch 0034, iter [03900, 05004], lr: 0.158029, loss: 2.1231
2022-07-26 11:56:08 - train: epoch 0034, iter [04000, 05004], lr: 0.157975, loss: 1.8245
2022-07-26 11:58:11 - train: epoch 0034, iter [04100, 05004], lr: 0.157921, loss: 2.1014
2022-07-26 12:00:03 - train: epoch 0034, iter [04200, 05004], lr: 0.157867, loss: 1.7838
2022-07-26 12:02:08 - train: epoch 0034, iter [04300, 05004], lr: 0.157813, loss: 2.1306
2022-07-26 12:04:02 - train: epoch 0034, iter [04400, 05004], lr: 0.157759, loss: 1.9626
2022-07-26 12:06:02 - train: epoch 0034, iter [04500, 05004], lr: 0.157705, loss: 1.8985
2022-07-26 12:07:53 - train: epoch 0034, iter [04600, 05004], lr: 0.157651, loss: 2.1056
2022-07-26 12:09:55 - train: epoch 0034, iter [04700, 05004], lr: 0.157597, loss: 1.9948
2022-07-26 12:11:49 - train: epoch 0034, iter [04800, 05004], lr: 0.157543, loss: 1.9771
2022-07-26 12:13:45 - train: epoch 0034, iter [04900, 05004], lr: 0.157489, loss: 2.1016
2022-07-26 12:15:39 - train: epoch 0034, iter [05000, 05004], lr: 0.157435, loss: 1.9800
2022-07-26 12:15:42 - train: epoch 034, train_loss: 1.9713
2022-07-26 12:19:43 - eval: epoch: 034, acc1: 59.216%, acc5: 83.228%, test_loss: 1.7054, per_image_load_time: 4.138ms, per_image_inference_time: 0.775ms
2022-07-26 12:19:43 - until epoch: 034, best_acc1: 59.658%
2022-07-26 12:19:43 - epoch 035 lr: 0.157432
2022-07-26 12:21:50 - train: epoch 0035, iter [00100, 05004], lr: 0.157379, loss: 1.7498
2022-07-26 12:23:48 - train: epoch 0035, iter [00200, 05004], lr: 0.157325, loss: 1.7232
2022-07-26 12:25:46 - train: epoch 0035, iter [00300, 05004], lr: 0.157270, loss: 1.9597
2022-07-26 12:27:45 - train: epoch 0035, iter [00400, 05004], lr: 0.157216, loss: 1.8258
2022-07-26 12:29:40 - train: epoch 0035, iter [00500, 05004], lr: 0.157162, loss: 2.0650
2022-07-26 12:31:36 - train: epoch 0035, iter [00600, 05004], lr: 0.157108, loss: 2.0647
2022-07-26 12:33:42 - train: epoch 0035, iter [00700, 05004], lr: 0.157054, loss: 1.9575
2022-07-26 12:35:34 - train: epoch 0035, iter [00800, 05004], lr: 0.156999, loss: 1.9110
2022-07-26 12:37:33 - train: epoch 0035, iter [00900, 05004], lr: 0.156945, loss: 1.9365
2022-07-26 12:39:35 - train: epoch 0035, iter [01000, 05004], lr: 0.156891, loss: 1.8692
2022-07-26 12:41:31 - train: epoch 0035, iter [01100, 05004], lr: 0.156836, loss: 1.9977
2022-07-26 12:43:27 - train: epoch 0035, iter [01200, 05004], lr: 0.156782, loss: 2.0812
2022-07-26 12:45:27 - train: epoch 0035, iter [01300, 05004], lr: 0.156727, loss: 1.7603
2022-07-26 12:47:22 - train: epoch 0035, iter [01400, 05004], lr: 0.156673, loss: 1.8574
2022-07-26 12:49:22 - train: epoch 0035, iter [01500, 05004], lr: 0.156619, loss: 2.0219
2022-07-26 12:51:21 - train: epoch 0035, iter [01600, 05004], lr: 0.156564, loss: 2.0337
2022-07-26 12:53:13 - train: epoch 0035, iter [01700, 05004], lr: 0.156510, loss: 1.9697
2022-07-26 12:55:08 - train: epoch 0035, iter [01800, 05004], lr: 0.156455, loss: 2.0847
2022-07-26 12:57:01 - train: epoch 0035, iter [01900, 05004], lr: 0.156400, loss: 1.9302
2022-07-26 12:59:01 - train: epoch 0035, iter [02000, 05004], lr: 0.156346, loss: 2.0853
2022-07-26 13:00:57 - train: epoch 0035, iter [02100, 05004], lr: 0.156291, loss: 1.7528
2022-07-26 13:02:54 - train: epoch 0035, iter [02200, 05004], lr: 0.156237, loss: 1.9512
2022-07-26 13:04:51 - train: epoch 0035, iter [02300, 05004], lr: 0.156182, loss: 1.8694
2022-07-26 13:06:47 - train: epoch 0035, iter [02400, 05004], lr: 0.156127, loss: 2.1826
2022-07-26 13:08:43 - train: epoch 0035, iter [02500, 05004], lr: 0.156073, loss: 2.0893
2022-07-26 13:10:40 - train: epoch 0035, iter [02600, 05004], lr: 0.156018, loss: 2.0602
2022-07-26 13:12:35 - train: epoch 0035, iter [02700, 05004], lr: 0.155963, loss: 1.7217
2022-07-26 13:14:31 - train: epoch 0035, iter [02800, 05004], lr: 0.155908, loss: 1.8584
2022-07-26 13:16:26 - train: epoch 0035, iter [02900, 05004], lr: 0.155854, loss: 1.8710
2022-07-26 13:18:24 - train: epoch 0035, iter [03000, 05004], lr: 0.155799, loss: 1.9236
2022-07-26 13:20:20 - train: epoch 0035, iter [03100, 05004], lr: 0.155744, loss: 1.8213
2022-07-26 13:22:18 - train: epoch 0035, iter [03200, 05004], lr: 0.155689, loss: 1.9291
2022-07-26 13:24:13 - train: epoch 0035, iter [03300, 05004], lr: 0.155634, loss: 1.7735
2022-07-26 13:26:06 - train: epoch 0035, iter [03400, 05004], lr: 0.155579, loss: 2.0151
2022-07-26 13:28:05 - train: epoch 0035, iter [03500, 05004], lr: 0.155524, loss: 1.7255
2022-07-26 13:30:07 - train: epoch 0035, iter [03600, 05004], lr: 0.155469, loss: 1.9439
2022-07-26 13:32:00 - train: epoch 0035, iter [03700, 05004], lr: 0.155414, loss: 1.9712
2022-07-26 13:34:04 - train: epoch 0035, iter [03800, 05004], lr: 0.155359, loss: 1.8047
2022-07-26 13:36:06 - train: epoch 0035, iter [03900, 05004], lr: 0.155304, loss: 1.8387
2022-07-26 13:38:02 - train: epoch 0035, iter [04000, 05004], lr: 0.155249, loss: 1.6120
2022-07-26 13:40:04 - train: epoch 0035, iter [04100, 05004], lr: 0.155194, loss: 1.9798
2022-07-26 13:42:09 - train: epoch 0035, iter [04200, 05004], lr: 0.155139, loss: 1.9389
2022-07-26 13:44:14 - train: epoch 0035, iter [04300, 05004], lr: 0.155084, loss: 1.9745
2022-07-26 13:46:12 - train: epoch 0035, iter [04400, 05004], lr: 0.155029, loss: 2.1705
2022-07-26 13:48:13 - train: epoch 0035, iter [04500, 05004], lr: 0.154973, loss: 2.0864
2022-07-26 13:50:11 - train: epoch 0035, iter [04600, 05004], lr: 0.154918, loss: 1.9562
2022-07-26 13:52:14 - train: epoch 0035, iter [04700, 05004], lr: 0.154863, loss: 1.9130
2022-07-26 13:54:16 - train: epoch 0035, iter [04800, 05004], lr: 0.154808, loss: 2.0874
2022-07-26 13:56:21 - train: epoch 0035, iter [04900, 05004], lr: 0.154752, loss: 1.9878
2022-07-26 13:58:15 - train: epoch 0035, iter [05000, 05004], lr: 0.154697, loss: 1.8866
2022-07-26 13:58:19 - train: epoch 035, train_loss: 1.9597
2022-07-26 14:02:17 - eval: epoch: 035, acc1: 60.012%, acc5: 83.536%, test_loss: 1.6798, per_image_load_time: 3.461ms, per_image_inference_time: 0.868ms
2022-07-26 14:02:17 - until epoch: 035, best_acc1: 60.012%
2022-07-26 14:02:17 - epoch 036 lr: 0.154694
2022-07-26 14:04:35 - train: epoch 0036, iter [00100, 05004], lr: 0.154639, loss: 1.9571
2022-07-26 14:06:36 - train: epoch 0036, iter [00200, 05004], lr: 0.154584, loss: 1.7608
2022-07-26 14:08:36 - train: epoch 0036, iter [00300, 05004], lr: 0.154529, loss: 1.7874
2022-07-26 14:10:37 - train: epoch 0036, iter [00400, 05004], lr: 0.154473, loss: 1.7925
2022-07-26 14:12:40 - train: epoch 0036, iter [00500, 05004], lr: 0.154418, loss: 1.9864
2022-07-26 14:14:33 - train: epoch 0036, iter [00600, 05004], lr: 0.154362, loss: 1.8871
2022-07-26 14:16:49 - train: epoch 0036, iter [00700, 05004], lr: 0.154307, loss: 1.8611
2022-07-26 14:18:50 - train: epoch 0036, iter [00800, 05004], lr: 0.154251, loss: 1.6222
2022-07-26 14:20:56 - train: epoch 0036, iter [00900, 05004], lr: 0.154196, loss: 2.0028
2022-07-26 14:22:58 - train: epoch 0036, iter [01000, 05004], lr: 0.154140, loss: 1.8430
2022-07-26 14:25:04 - train: epoch 0036, iter [01100, 05004], lr: 0.154085, loss: 1.9182
2022-07-26 14:27:06 - train: epoch 0036, iter [01200, 05004], lr: 0.154029, loss: 1.9268
2022-07-26 14:29:05 - train: epoch 0036, iter [01300, 05004], lr: 0.153974, loss: 1.8634
2022-07-26 14:31:06 - train: epoch 0036, iter [01400, 05004], lr: 0.153918, loss: 1.9134
2022-07-26 14:33:10 - train: epoch 0036, iter [01500, 05004], lr: 0.153862, loss: 2.1656
2022-07-26 14:35:17 - train: epoch 0036, iter [01600, 05004], lr: 0.153807, loss: 1.9782
2022-07-26 14:37:14 - train: epoch 0036, iter [01700, 05004], lr: 0.153751, loss: 1.9091
2022-07-26 14:39:12 - train: epoch 0036, iter [01800, 05004], lr: 0.153695, loss: 1.7065
2022-07-26 14:41:16 - train: epoch 0036, iter [01900, 05004], lr: 0.153639, loss: 1.8201
2022-07-26 14:43:10 - train: epoch 0036, iter [02000, 05004], lr: 0.153584, loss: 1.6239
2022-07-26 14:45:13 - train: epoch 0036, iter [02100, 05004], lr: 0.153528, loss: 1.8525
2022-07-26 14:47:13 - train: epoch 0036, iter [02200, 05004], lr: 0.153472, loss: 2.1852
2022-07-26 14:49:13 - train: epoch 0036, iter [02300, 05004], lr: 0.153416, loss: 2.2961
2022-07-26 14:51:14 - train: epoch 0036, iter [02400, 05004], lr: 0.153360, loss: 2.2686
2022-07-26 14:53:18 - train: epoch 0036, iter [02500, 05004], lr: 0.153304, loss: 1.7606
2022-07-26 14:55:24 - train: epoch 0036, iter [02600, 05004], lr: 0.153248, loss: 2.2249
2022-07-26 14:57:24 - train: epoch 0036, iter [02700, 05004], lr: 0.153192, loss: 1.6865
2022-07-26 14:59:24 - train: epoch 0036, iter [02800, 05004], lr: 0.153136, loss: 2.0162
2022-07-26 15:01:27 - train: epoch 0036, iter [02900, 05004], lr: 0.153080, loss: 2.0065
2022-07-26 15:03:24 - train: epoch 0036, iter [03000, 05004], lr: 0.153024, loss: 2.0823
2022-07-26 15:05:27 - train: epoch 0036, iter [03100, 05004], lr: 0.152968, loss: 1.9858
2022-07-26 15:07:29 - train: epoch 0036, iter [03200, 05004], lr: 0.152912, loss: 2.1763
2022-07-26 15:09:31 - train: epoch 0036, iter [03300, 05004], lr: 0.152856, loss: 1.7395
2022-07-26 15:11:35 - train: epoch 0036, iter [03400, 05004], lr: 0.152800, loss: 1.8121
2022-07-26 15:13:36 - train: epoch 0036, iter [03500, 05004], lr: 0.152744, loss: 2.0518
2022-07-26 15:15:36 - train: epoch 0036, iter [03600, 05004], lr: 0.152688, loss: 2.1079
2022-07-26 15:17:39 - train: epoch 0036, iter [03700, 05004], lr: 0.152632, loss: 1.8823
2022-07-26 15:19:40 - train: epoch 0036, iter [03800, 05004], lr: 0.152575, loss: 2.0387
2022-07-26 15:21:39 - train: epoch 0036, iter [03900, 05004], lr: 0.152519, loss: 2.0040
2022-07-26 15:23:42 - train: epoch 0036, iter [04000, 05004], lr: 0.152463, loss: 2.0743
2022-07-26 15:25:42 - train: epoch 0036, iter [04100, 05004], lr: 0.152407, loss: 1.8510
2022-07-26 15:27:42 - train: epoch 0036, iter [04200, 05004], lr: 0.152350, loss: 1.9332
2022-07-26 15:29:46 - train: epoch 0036, iter [04300, 05004], lr: 0.152294, loss: 1.7783
2022-07-26 15:31:42 - train: epoch 0036, iter [04400, 05004], lr: 0.152238, loss: 1.8583
2022-07-26 15:33:51 - train: epoch 0036, iter [04500, 05004], lr: 0.152181, loss: 1.8826
2022-07-26 15:35:49 - train: epoch 0036, iter [04600, 05004], lr: 0.152125, loss: 1.9432
2022-07-26 15:37:48 - train: epoch 0036, iter [04700, 05004], lr: 0.152069, loss: 1.8458
2022-07-26 15:39:50 - train: epoch 0036, iter [04800, 05004], lr: 0.152012, loss: 1.8530
2022-07-26 15:41:54 - train: epoch 0036, iter [04900, 05004], lr: 0.151956, loss: 1.9880
2022-07-26 15:43:47 - train: epoch 0036, iter [05000, 05004], lr: 0.151899, loss: 2.0335
2022-07-26 15:43:52 - train: epoch 036, train_loss: 1.9504
2022-07-26 15:48:01 - eval: epoch: 036, acc1: 59.524%, acc5: 82.978%, test_loss: 1.6994, per_image_load_time: 8.360ms, per_image_inference_time: 0.833ms
2022-07-26 15:48:01 - until epoch: 036, best_acc1: 60.012%
2022-07-26 15:48:01 - epoch 037 lr: 0.151896
2022-07-26 15:50:21 - train: epoch 0037, iter [00100, 05004], lr: 0.151840, loss: 1.6724
2022-07-26 15:52:26 - train: epoch 0037, iter [00200, 05004], lr: 0.151784, loss: 1.7365
2022-07-26 15:54:31 - train: epoch 0037, iter [00300, 05004], lr: 0.151727, loss: 1.7100
2022-07-26 15:56:33 - train: epoch 0037, iter [00400, 05004], lr: 0.151671, loss: 1.6995
2022-07-26 15:58:33 - train: epoch 0037, iter [00500, 05004], lr: 0.151614, loss: 1.8096
2022-07-26 16:00:36 - train: epoch 0037, iter [00600, 05004], lr: 0.151558, loss: 2.1434
2022-07-26 16:02:26 - train: epoch 0037, iter [00700, 05004], lr: 0.151501, loss: 1.9026
2022-07-26 16:04:44 - train: epoch 0037, iter [00800, 05004], lr: 0.151444, loss: 2.0967
2022-07-26 16:06:38 - train: epoch 0037, iter [00900, 05004], lr: 0.151388, loss: 2.1519
2022-07-26 16:08:48 - train: epoch 0037, iter [01000, 05004], lr: 0.151331, loss: 1.8891
2022-07-26 16:10:50 - train: epoch 0037, iter [01100, 05004], lr: 0.151274, loss: 2.0376
2022-07-26 16:12:53 - train: epoch 0037, iter [01200, 05004], lr: 0.151217, loss: 2.1836
2022-07-26 16:14:58 - train: epoch 0037, iter [01300, 05004], lr: 0.151161, loss: 1.9601
2022-07-26 16:17:00 - train: epoch 0037, iter [01400, 05004], lr: 0.151104, loss: 2.1432
2022-07-26 16:19:01 - train: epoch 0037, iter [01500, 05004], lr: 0.151047, loss: 1.8071
2022-07-26 16:21:02 - train: epoch 0037, iter [01600, 05004], lr: 0.150990, loss: 1.6997
2022-07-26 16:23:06 - train: epoch 0037, iter [01700, 05004], lr: 0.150933, loss: 2.1356
2022-07-26 16:25:03 - train: epoch 0037, iter [01800, 05004], lr: 0.150876, loss: 1.8327
2022-07-26 16:27:09 - train: epoch 0037, iter [01900, 05004], lr: 0.150820, loss: 2.0351
2022-07-26 16:29:07 - train: epoch 0037, iter [02000, 05004], lr: 0.150763, loss: 1.9080
2022-07-26 16:31:05 - train: epoch 0037, iter [02100, 05004], lr: 0.150706, loss: 1.8541
2022-07-26 16:33:05 - train: epoch 0037, iter [02200, 05004], lr: 0.150649, loss: 1.8719
2022-07-26 16:35:04 - train: epoch 0037, iter [02300, 05004], lr: 0.150592, loss: 1.8605
2022-07-26 16:37:07 - train: epoch 0037, iter [02400, 05004], lr: 0.150535, loss: 1.8411
2022-07-26 16:39:09 - train: epoch 0037, iter [02500, 05004], lr: 0.150478, loss: 2.1467
2022-07-26 16:41:14 - train: epoch 0037, iter [02600, 05004], lr: 0.150421, loss: 2.1925
2022-07-26 16:43:15 - train: epoch 0037, iter [02700, 05004], lr: 0.150364, loss: 2.1248
2022-07-26 16:45:15 - train: epoch 0037, iter [02800, 05004], lr: 0.150306, loss: 2.0014
2022-07-26 16:47:25 - train: epoch 0037, iter [02900, 05004], lr: 0.150249, loss: 1.9057
2022-07-26 16:49:26 - train: epoch 0037, iter [03000, 05004], lr: 0.150192, loss: 2.1836
2022-07-26 16:51:25 - train: epoch 0037, iter [03100, 05004], lr: 0.150135, loss: 1.8262
2022-07-26 16:53:30 - train: epoch 0037, iter [03200, 05004], lr: 0.150078, loss: 1.9857
2022-07-26 16:55:30 - train: epoch 0037, iter [03300, 05004], lr: 0.150021, loss: 1.9107
2022-07-26 16:57:30 - train: epoch 0037, iter [03400, 05004], lr: 0.149963, loss: 1.6678
2022-07-26 16:59:32 - train: epoch 0037, iter [03500, 05004], lr: 0.149906, loss: 1.9463
2022-07-26 17:01:31 - train: epoch 0037, iter [03600, 05004], lr: 0.149849, loss: 2.1720
2022-07-26 17:03:31 - train: epoch 0037, iter [03700, 05004], lr: 0.149792, loss: 2.0891
2022-07-26 17:05:35 - train: epoch 0037, iter [03800, 05004], lr: 0.149734, loss: 1.8725
2022-07-26 17:07:35 - train: epoch 0037, iter [03900, 05004], lr: 0.149677, loss: 1.8560
2022-07-26 17:09:40 - train: epoch 0037, iter [04000, 05004], lr: 0.149619, loss: 1.9827
2022-07-26 17:11:42 - train: epoch 0037, iter [04100, 05004], lr: 0.149562, loss: 1.9378
2022-07-26 17:13:45 - train: epoch 0037, iter [04200, 05004], lr: 0.149505, loss: 2.0853
2022-07-26 17:15:43 - train: epoch 0037, iter [04300, 05004], lr: 0.149447, loss: 1.7369
2022-07-26 17:17:44 - train: epoch 0037, iter [04400, 05004], lr: 0.149390, loss: 1.8733
2022-07-26 17:19:42 - train: epoch 0037, iter [04500, 05004], lr: 0.149332, loss: 1.7563
2022-07-26 17:21:44 - train: epoch 0037, iter [04600, 05004], lr: 0.149275, loss: 1.7950
2022-07-26 17:23:43 - train: epoch 0037, iter [04700, 05004], lr: 0.149217, loss: 1.9912
2022-07-26 17:25:45 - train: epoch 0037, iter [04800, 05004], lr: 0.149160, loss: 1.8369
2022-07-26 17:27:44 - train: epoch 0037, iter [04900, 05004], lr: 0.149102, loss: 2.0012
2022-07-26 17:29:42 - train: epoch 0037, iter [05000, 05004], lr: 0.149045, loss: 1.7891
2022-07-26 17:29:46 - train: epoch 037, train_loss: 1.9421
2022-07-26 17:33:56 - eval: epoch: 037, acc1: 59.966%, acc5: 83.736%, test_loss: 1.6711, per_image_load_time: 5.059ms, per_image_inference_time: 0.856ms
2022-07-26 17:33:56 - until epoch: 037, best_acc1: 60.012%
2022-07-26 17:33:56 - epoch 038 lr: 0.149042
2022-07-26 17:36:15 - train: epoch 0038, iter [00100, 05004], lr: 0.148985, loss: 1.8116
2022-07-26 17:38:19 - train: epoch 0038, iter [00200, 05004], lr: 0.148927, loss: 1.8756
2022-07-26 17:40:16 - train: epoch 0038, iter [00300, 05004], lr: 0.148869, loss: 1.7535
2022-07-26 17:42:23 - train: epoch 0038, iter [00400, 05004], lr: 0.148812, loss: 1.7087
2022-07-26 17:44:28 - train: epoch 0038, iter [00500, 05004], lr: 0.148754, loss: 1.7200
2022-07-26 17:46:27 - train: epoch 0038, iter [00600, 05004], lr: 0.148696, loss: 1.9585
2022-07-26 17:48:22 - train: epoch 0038, iter [00700, 05004], lr: 0.148639, loss: 1.7299
2022-07-26 17:50:29 - train: epoch 0038, iter [00800, 05004], lr: 0.148581, loss: 1.8370
2022-07-26 17:52:31 - train: epoch 0038, iter [00900, 05004], lr: 0.148523, loss: 1.9281
2022-07-26 17:54:36 - train: epoch 0038, iter [01000, 05004], lr: 0.148465, loss: 2.0730
2022-07-26 17:56:47 - train: epoch 0038, iter [01100, 05004], lr: 0.148408, loss: 1.8625
2022-07-26 17:58:50 - train: epoch 0038, iter [01200, 05004], lr: 0.148350, loss: 1.9964
2022-07-26 18:00:49 - train: epoch 0038, iter [01300, 05004], lr: 0.148292, loss: 1.9910
2022-07-26 18:02:45 - train: epoch 0038, iter [01400, 05004], lr: 0.148234, loss: 1.9588
2022-07-26 18:04:47 - train: epoch 0038, iter [01500, 05004], lr: 0.148176, loss: 1.7405
2022-07-26 18:06:46 - train: epoch 0038, iter [01600, 05004], lr: 0.148118, loss: 2.2150
2022-07-26 18:08:49 - train: epoch 0038, iter [01700, 05004], lr: 0.148060, loss: 2.1891
2022-07-26 18:10:47 - train: epoch 0038, iter [01800, 05004], lr: 0.148002, loss: 1.9646
2022-07-26 18:12:46 - train: epoch 0038, iter [01900, 05004], lr: 0.147944, loss: 1.9794
2022-07-26 18:14:49 - train: epoch 0038, iter [02000, 05004], lr: 0.147886, loss: 1.9007
2022-07-26 18:16:51 - train: epoch 0038, iter [02100, 05004], lr: 0.147828, loss: 2.0480
2022-07-26 18:18:51 - train: epoch 0038, iter [02200, 05004], lr: 0.147770, loss: 2.0938
2022-07-26 18:20:48 - train: epoch 0038, iter [02300, 05004], lr: 0.147712, loss: 2.0992
2022-07-26 18:22:46 - train: epoch 0038, iter [02400, 05004], lr: 0.147654, loss: 2.1593
2022-07-26 18:24:47 - train: epoch 0038, iter [02500, 05004], lr: 0.147596, loss: 1.8194
2022-07-26 18:26:46 - train: epoch 0038, iter [02600, 05004], lr: 0.147538, loss: 2.0930
2022-07-26 18:28:53 - train: epoch 0038, iter [02700, 05004], lr: 0.147480, loss: 2.0109
2022-07-26 18:30:54 - train: epoch 0038, iter [02800, 05004], lr: 0.147421, loss: 2.0833
2022-07-26 18:33:03 - train: epoch 0038, iter [02900, 05004], lr: 0.147363, loss: 1.9691
2022-07-26 18:35:01 - train: epoch 0038, iter [03000, 05004], lr: 0.147305, loss: 1.9434
2022-07-26 18:37:01 - train: epoch 0038, iter [03100, 05004], lr: 0.147247, loss: 1.8994
2022-07-26 18:39:05 - train: epoch 0038, iter [03200, 05004], lr: 0.147189, loss: 1.7162
2022-07-26 18:41:06 - train: epoch 0038, iter [03300, 05004], lr: 0.147130, loss: 1.8760
2022-07-26 18:43:03 - train: epoch 0038, iter [03400, 05004], lr: 0.147072, loss: 1.7786
2022-07-26 18:45:07 - train: epoch 0038, iter [03500, 05004], lr: 0.147014, loss: 1.8743
2022-07-26 18:47:07 - train: epoch 0038, iter [03600, 05004], lr: 0.146955, loss: 2.1314
2022-07-26 18:49:09 - train: epoch 0038, iter [03700, 05004], lr: 0.146897, loss: 1.7981
2022-07-26 18:51:12 - train: epoch 0038, iter [03800, 05004], lr: 0.146839, loss: 2.0940
2022-07-26 18:53:13 - train: epoch 0038, iter [03900, 05004], lr: 0.146780, loss: 1.9964
2022-07-26 18:55:20 - train: epoch 0038, iter [04000, 05004], lr: 0.146722, loss: 2.0256
2022-07-26 18:57:15 - train: epoch 0038, iter [04100, 05004], lr: 0.146663, loss: 1.9690
2022-07-26 18:59:18 - train: epoch 0038, iter [04200, 05004], lr: 0.146605, loss: 1.8404
2022-07-26 19:01:20 - train: epoch 0038, iter [04300, 05004], lr: 0.146546, loss: 2.0811
2022-07-26 19:03:25 - train: epoch 0038, iter [04400, 05004], lr: 0.146488, loss: 2.0241
2022-07-26 19:05:26 - train: epoch 0038, iter [04500, 05004], lr: 0.146429, loss: 2.1190
2022-07-26 19:07:31 - train: epoch 0038, iter [04600, 05004], lr: 0.146371, loss: 1.9194
2022-07-26 19:09:37 - train: epoch 0038, iter [04700, 05004], lr: 0.146312, loss: 1.9070
2022-07-26 19:11:39 - train: epoch 0038, iter [04800, 05004], lr: 0.146254, loss: 2.0472
2022-07-26 19:13:38 - train: epoch 0038, iter [04900, 05004], lr: 0.146195, loss: 1.9430
2022-07-26 19:15:32 - train: epoch 0038, iter [05000, 05004], lr: 0.146136, loss: 1.7311
2022-07-26 19:15:36 - train: epoch 038, train_loss: 1.9317
2022-07-26 19:19:47 - eval: epoch: 038, acc1: 59.074%, acc5: 83.094%, test_loss: 1.7092, per_image_load_time: 5.601ms, per_image_inference_time: 0.876ms
2022-07-26 19:19:48 - until epoch: 038, best_acc1: 60.012%
2022-07-26 19:19:48 - epoch 039 lr: 0.146134
2022-07-26 19:22:05 - train: epoch 0039, iter [00100, 05004], lr: 0.146075, loss: 2.0776
2022-07-26 19:24:06 - train: epoch 0039, iter [00200, 05004], lr: 0.146017, loss: 2.0278
2022-07-26 19:26:09 - train: epoch 0039, iter [00300, 05004], lr: 0.145958, loss: 1.7747
2022-07-26 19:28:09 - train: epoch 0039, iter [00400, 05004], lr: 0.145899, loss: 1.9023
2022-07-26 19:30:12 - train: epoch 0039, iter [00500, 05004], lr: 0.145841, loss: 1.8186
2022-07-26 19:32:12 - train: epoch 0039, iter [00600, 05004], lr: 0.145782, loss: 1.8930
2022-07-26 19:34:20 - train: epoch 0039, iter [00700, 05004], lr: 0.145723, loss: 2.1046
2022-07-26 19:36:22 - train: epoch 0039, iter [00800, 05004], lr: 0.145664, loss: 1.6907
2022-07-26 19:38:33 - train: epoch 0039, iter [00900, 05004], lr: 0.145606, loss: 1.9673
2022-07-26 19:40:36 - train: epoch 0039, iter [01000, 05004], lr: 0.145547, loss: 1.9316
2022-07-26 19:42:39 - train: epoch 0039, iter [01100, 05004], lr: 0.145488, loss: 1.9401
2022-07-26 19:44:40 - train: epoch 0039, iter [01200, 05004], lr: 0.145429, loss: 2.0556
2022-07-26 19:46:44 - train: epoch 0039, iter [01300, 05004], lr: 0.145370, loss: 1.8565
2022-07-26 19:48:47 - train: epoch 0039, iter [01400, 05004], lr: 0.145311, loss: 2.0274
2022-07-26 19:50:50 - train: epoch 0039, iter [01500, 05004], lr: 0.145252, loss: 2.0523
2022-07-26 19:52:53 - train: epoch 0039, iter [01600, 05004], lr: 0.145193, loss: 2.1136
2022-07-26 19:54:53 - train: epoch 0039, iter [01700, 05004], lr: 0.145134, loss: 1.8768
2022-07-26 19:56:51 - train: epoch 0039, iter [01800, 05004], lr: 0.145075, loss: 1.9311
2022-07-26 19:58:55 - train: epoch 0039, iter [01900, 05004], lr: 0.145016, loss: 1.7591
2022-07-26 20:00:54 - train: epoch 0039, iter [02000, 05004], lr: 0.144957, loss: 1.8997
2022-07-26 20:02:55 - train: epoch 0039, iter [02100, 05004], lr: 0.144898, loss: 1.9999
2022-07-26 20:04:53 - train: epoch 0039, iter [02200, 05004], lr: 0.144839, loss: 1.9323
2022-07-26 20:06:56 - train: epoch 0039, iter [02300, 05004], lr: 0.144780, loss: 2.0877
2022-07-26 20:08:56 - train: epoch 0039, iter [02400, 05004], lr: 0.144721, loss: 1.9871
2022-07-26 20:10:57 - train: epoch 0039, iter [02500, 05004], lr: 0.144662, loss: 1.8049
2022-07-26 20:12:54 - train: epoch 0039, iter [02600, 05004], lr: 0.144603, loss: 1.8381
2022-07-26 20:14:55 - train: epoch 0039, iter [02700, 05004], lr: 0.144544, loss: 2.0861
2022-07-26 20:16:53 - train: epoch 0039, iter [02800, 05004], lr: 0.144485, loss: 1.8741
2022-07-26 20:19:00 - train: epoch 0039, iter [02900, 05004], lr: 0.144425, loss: 1.8365
2022-07-26 20:21:02 - train: epoch 0039, iter [03000, 05004], lr: 0.144366, loss: 1.9640
2022-07-26 20:23:04 - train: epoch 0039, iter [03100, 05004], lr: 0.144307, loss: 1.7378
2022-07-26 20:25:05 - train: epoch 0039, iter [03200, 05004], lr: 0.144248, loss: 2.0798
2022-07-26 20:27:01 - train: epoch 0039, iter [03300, 05004], lr: 0.144188, loss: 1.9049
2022-07-26 20:29:02 - train: epoch 0039, iter [03400, 05004], lr: 0.144129, loss: 1.9829
2022-07-26 20:31:05 - train: epoch 0039, iter [03500, 05004], lr: 0.144070, loss: 2.1704
2022-07-26 20:33:05 - train: epoch 0039, iter [03600, 05004], lr: 0.144010, loss: 2.1305
2022-07-26 20:35:03 - train: epoch 0039, iter [03700, 05004], lr: 0.143951, loss: 1.6969
2022-07-26 20:37:07 - train: epoch 0039, iter [03800, 05004], lr: 0.143892, loss: 1.6590
2022-07-26 20:39:09 - train: epoch 0039, iter [03900, 05004], lr: 0.143832, loss: 1.8800
2022-07-26 20:41:08 - train: epoch 0039, iter [04000, 05004], lr: 0.143773, loss: 1.7925
2022-07-26 20:43:15 - train: epoch 0039, iter [04100, 05004], lr: 0.143714, loss: 1.9612
2022-07-26 20:45:13 - train: epoch 0039, iter [04200, 05004], lr: 0.143654, loss: 1.8435
2022-07-26 20:47:16 - train: epoch 0039, iter [04300, 05004], lr: 0.143595, loss: 1.9442
2022-07-26 20:49:21 - train: epoch 0039, iter [04400, 05004], lr: 0.143535, loss: 1.8055
2022-07-26 20:51:26 - train: epoch 0039, iter [04500, 05004], lr: 0.143476, loss: 1.9390
2022-07-26 20:53:25 - train: epoch 0039, iter [04600, 05004], lr: 0.143416, loss: 2.0654
2022-07-26 20:55:25 - train: epoch 0039, iter [04700, 05004], lr: 0.143357, loss: 1.8127
2022-07-26 20:57:26 - train: epoch 0039, iter [04800, 05004], lr: 0.143297, loss: 1.7591
2022-07-26 20:59:32 - train: epoch 0039, iter [04900, 05004], lr: 0.143237, loss: 1.8911
2022-07-26 21:01:31 - train: epoch 0039, iter [05000, 05004], lr: 0.143178, loss: 1.7209
2022-07-26 21:01:35 - train: epoch 039, train_loss: 1.9209
2022-07-26 21:05:31 - eval: epoch: 039, acc1: 60.538%, acc5: 84.384%, test_loss: 1.6207, per_image_load_time: 8.060ms, per_image_inference_time: 0.835ms
2022-07-26 21:05:32 - until epoch: 039, best_acc1: 60.538%
2022-07-26 21:05:32 - epoch 040 lr: 0.143175
2022-07-26 21:07:46 - train: epoch 0040, iter [00100, 05004], lr: 0.143116, loss: 2.2220
2022-07-26 21:09:44 - train: epoch 0040, iter [00200, 05004], lr: 0.143056, loss: 2.1512
2022-07-26 21:11:37 - train: epoch 0040, iter [00300, 05004], lr: 0.142997, loss: 2.0428
2022-07-26 21:13:31 - train: epoch 0040, iter [00400, 05004], lr: 0.142937, loss: 1.8001
2022-07-26 21:15:28 - train: epoch 0040, iter [00500, 05004], lr: 0.142877, loss: 1.8937
2022-07-26 21:17:25 - train: epoch 0040, iter [00600, 05004], lr: 0.142817, loss: 2.0760
2022-07-26 21:19:27 - train: epoch 0040, iter [00700, 05004], lr: 0.142758, loss: 1.8911
2022-07-26 21:21:22 - train: epoch 0040, iter [00800, 05004], lr: 0.142698, loss: 1.8624
2022-07-26 21:23:31 - train: epoch 0040, iter [00900, 05004], lr: 0.142638, loss: 1.6665
2022-07-26 21:25:26 - train: epoch 0040, iter [01000, 05004], lr: 0.142578, loss: 1.7254
2022-07-26 21:27:26 - train: epoch 0040, iter [01100, 05004], lr: 0.142519, loss: 1.6050
2022-07-26 21:29:28 - train: epoch 0040, iter [01200, 05004], lr: 0.142459, loss: 1.6577
2022-07-26 21:31:22 - train: epoch 0040, iter [01300, 05004], lr: 0.142399, loss: 1.6442
2022-07-26 21:33:18 - train: epoch 0040, iter [01400, 05004], lr: 0.142339, loss: 1.9490
2022-07-26 21:35:15 - train: epoch 0040, iter [01500, 05004], lr: 0.142279, loss: 2.1243
2022-07-26 21:37:12 - train: epoch 0040, iter [01600, 05004], lr: 0.142219, loss: 1.9687
2022-07-26 21:39:12 - train: epoch 0040, iter [01700, 05004], lr: 0.142159, loss: 1.6998
2022-07-26 21:41:13 - train: epoch 0040, iter [01800, 05004], lr: 0.142099, loss: 1.9185
2022-07-26 21:43:15 - train: epoch 0040, iter [01900, 05004], lr: 0.142039, loss: 1.9691
2022-07-26 21:45:14 - train: epoch 0040, iter [02000, 05004], lr: 0.141980, loss: 2.0382
2022-07-26 21:47:17 - train: epoch 0040, iter [02100, 05004], lr: 0.141920, loss: 1.7254
2022-07-26 21:49:19 - train: epoch 0040, iter [02200, 05004], lr: 0.141860, loss: 1.7428
2022-07-26 21:51:15 - train: epoch 0040, iter [02300, 05004], lr: 0.141799, loss: 1.8550
2022-07-26 21:53:17 - train: epoch 0040, iter [02400, 05004], lr: 0.141739, loss: 2.0413
2022-07-26 21:55:15 - train: epoch 0040, iter [02500, 05004], lr: 0.141679, loss: 2.0042
2022-07-26 21:57:16 - train: epoch 0040, iter [02600, 05004], lr: 0.141619, loss: 2.0086
2022-07-26 21:59:18 - train: epoch 0040, iter [02700, 05004], lr: 0.141559, loss: 2.0231
2022-07-26 22:01:14 - train: epoch 0040, iter [02800, 05004], lr: 0.141499, loss: 2.0989
2022-07-26 22:03:17 - train: epoch 0040, iter [02900, 05004], lr: 0.141439, loss: 2.1623
2022-07-26 22:05:11 - train: epoch 0040, iter [03000, 05004], lr: 0.141379, loss: 1.9338
2022-07-26 22:07:11 - train: epoch 0040, iter [03100, 05004], lr: 0.141319, loss: 2.2124
2022-07-26 22:09:10 - train: epoch 0040, iter [03200, 05004], lr: 0.141258, loss: 2.1146
2022-07-26 22:11:14 - train: epoch 0040, iter [03300, 05004], lr: 0.141198, loss: 2.0653
2022-07-26 22:13:13 - train: epoch 0040, iter [03400, 05004], lr: 0.141138, loss: 2.1548
2022-07-26 22:15:08 - train: epoch 0040, iter [03500, 05004], lr: 0.141078, loss: 2.0075
2022-07-26 22:17:10 - train: epoch 0040, iter [03600, 05004], lr: 0.141017, loss: 1.7689
2022-07-26 22:19:08 - train: epoch 0040, iter [03700, 05004], lr: 0.140957, loss: 1.9486
2022-07-26 22:21:11 - train: epoch 0040, iter [03800, 05004], lr: 0.140897, loss: 1.8830
2022-07-26 22:23:11 - train: epoch 0040, iter [03900, 05004], lr: 0.140837, loss: 1.9359
2022-07-26 22:25:23 - train: epoch 0040, iter [04000, 05004], lr: 0.140776, loss: 1.9627
2022-07-26 22:27:26 - train: epoch 0040, iter [04100, 05004], lr: 0.140716, loss: 1.8421
2022-07-26 22:29:28 - train: epoch 0040, iter [04200, 05004], lr: 0.140656, loss: 1.7021
2022-07-26 22:31:30 - train: epoch 0040, iter [04300, 05004], lr: 0.140595, loss: 2.0277
2022-07-26 22:33:35 - train: epoch 0040, iter [04400, 05004], lr: 0.140535, loss: 1.8638
2022-07-26 22:35:35 - train: epoch 0040, iter [04500, 05004], lr: 0.140474, loss: 1.7793
2022-07-26 22:37:40 - train: epoch 0040, iter [04600, 05004], lr: 0.140414, loss: 1.8531
2022-07-26 22:39:41 - train: epoch 0040, iter [04700, 05004], lr: 0.140353, loss: 1.9017
2022-07-26 22:41:47 - train: epoch 0040, iter [04800, 05004], lr: 0.140293, loss: 1.8519
2022-07-26 22:43:48 - train: epoch 0040, iter [04900, 05004], lr: 0.140232, loss: 1.8810
2022-07-26 22:45:49 - train: epoch 0040, iter [05000, 05004], lr: 0.140172, loss: 2.0123
2022-07-26 22:45:52 - train: epoch 040, train_loss: 1.9089
2022-07-26 22:50:06 - eval: epoch: 040, acc1: 59.014%, acc5: 82.768%, test_loss: 1.7136, per_image_load_time: 8.640ms, per_image_inference_time: 0.899ms
2022-07-26 22:50:07 - until epoch: 040, best_acc1: 60.538%
2022-07-26 22:50:07 - epoch 041 lr: 0.140169
2022-07-26 22:52:33 - train: epoch 0041, iter [00100, 05004], lr: 0.140109, loss: 2.0985
2022-07-26 22:54:39 - train: epoch 0041, iter [00200, 05004], lr: 0.140048, loss: 2.1759
2022-07-26 22:56:46 - train: epoch 0041, iter [00300, 05004], lr: 0.139988, loss: 1.9434
2022-07-26 22:58:50 - train: epoch 0041, iter [00400, 05004], lr: 0.139927, loss: 1.7907
2022-07-26 23:00:54 - train: epoch 0041, iter [00500, 05004], lr: 0.139867, loss: 1.5834
2022-07-26 23:03:00 - train: epoch 0041, iter [00600, 05004], lr: 0.139806, loss: 1.8330
2022-07-26 23:05:06 - train: epoch 0041, iter [00700, 05004], lr: 0.139745, loss: 1.8262
2022-07-26 23:07:08 - train: epoch 0041, iter [00800, 05004], lr: 0.139685, loss: 1.9271
2022-07-26 23:09:01 - train: epoch 0041, iter [00900, 05004], lr: 0.139624, loss: 1.7038
2022-07-26 23:11:11 - train: epoch 0041, iter [01000, 05004], lr: 0.139563, loss: 2.1504
2022-07-26 23:13:10 - train: epoch 0041, iter [01100, 05004], lr: 0.139503, loss: 1.7412
2022-07-26 23:15:13 - train: epoch 0041, iter [01200, 05004], lr: 0.139442, loss: 1.7473
2022-07-26 23:17:17 - train: epoch 0041, iter [01300, 05004], lr: 0.139381, loss: 1.6393
2022-07-26 23:19:19 - train: epoch 0041, iter [01400, 05004], lr: 0.139321, loss: 1.7701
2022-07-26 23:21:19 - train: epoch 0041, iter [01500, 05004], lr: 0.139260, loss: 2.0409
2022-07-26 23:23:26 - train: epoch 0041, iter [01600, 05004], lr: 0.139199, loss: 1.8428
2022-07-26 23:25:23 - train: epoch 0041, iter [01700, 05004], lr: 0.139138, loss: 1.8630
2022-07-26 23:27:26 - train: epoch 0041, iter [01800, 05004], lr: 0.139077, loss: 1.7379
2022-07-26 23:29:25 - train: epoch 0041, iter [01900, 05004], lr: 0.139017, loss: 1.8428
2022-07-26 23:31:26 - train: epoch 0041, iter [02000, 05004], lr: 0.138956, loss: 1.6285
2022-07-26 23:33:26 - train: epoch 0041, iter [02100, 05004], lr: 0.138895, loss: 1.7477
2022-07-26 23:35:23 - train: epoch 0041, iter [02200, 05004], lr: 0.138834, loss: 1.5630
2022-07-26 23:37:18 - train: epoch 0041, iter [02300, 05004], lr: 0.138773, loss: 1.8764
2022-07-26 23:39:17 - train: epoch 0041, iter [02400, 05004], lr: 0.138712, loss: 2.0737
2022-07-26 23:41:12 - train: epoch 0041, iter [02500, 05004], lr: 0.138651, loss: 1.8691
2022-07-26 23:43:07 - train: epoch 0041, iter [02600, 05004], lr: 0.138590, loss: 2.1516
2022-07-26 23:45:07 - train: epoch 0041, iter [02700, 05004], lr: 0.138529, loss: 2.0943
2022-07-26 23:47:06 - train: epoch 0041, iter [02800, 05004], lr: 0.138468, loss: 2.0155
2022-07-26 23:49:13 - train: epoch 0041, iter [02900, 05004], lr: 0.138407, loss: 1.9100
2022-07-26 23:51:12 - train: epoch 0041, iter [03000, 05004], lr: 0.138346, loss: 1.9992
2022-07-26 23:53:13 - train: epoch 0041, iter [03100, 05004], lr: 0.138285, loss: 1.8003
2022-07-26 23:55:16 - train: epoch 0041, iter [03200, 05004], lr: 0.138224, loss: 1.8085
2022-07-26 23:57:16 - train: epoch 0041, iter [03300, 05004], lr: 0.138163, loss: 1.8431
2022-07-26 23:59:14 - train: epoch 0041, iter [03400, 05004], lr: 0.138102, loss: 1.6577
2022-07-27 00:01:12 - train: epoch 0041, iter [03500, 05004], lr: 0.138041, loss: 1.9400
2022-07-27 00:03:10 - train: epoch 0041, iter [03600, 05004], lr: 0.137980, loss: 2.0309
2022-07-27 00:05:08 - train: epoch 0041, iter [03700, 05004], lr: 0.137919, loss: 1.8626
2022-07-27 00:07:07 - train: epoch 0041, iter [03800, 05004], lr: 0.137857, loss: 1.7740
2022-07-27 00:09:08 - train: epoch 0041, iter [03900, 05004], lr: 0.137796, loss: 1.8919
2022-07-27 00:11:04 - train: epoch 0041, iter [04000, 05004], lr: 0.137735, loss: 1.7280
2022-07-27 00:13:06 - train: epoch 0041, iter [04100, 05004], lr: 0.137674, loss: 1.8197
2022-07-27 00:15:01 - train: epoch 0041, iter [04200, 05004], lr: 0.137613, loss: 1.9760
2022-07-27 00:17:06 - train: epoch 0041, iter [04300, 05004], lr: 0.137551, loss: 1.9108
2022-07-27 00:19:08 - train: epoch 0041, iter [04400, 05004], lr: 0.137490, loss: 1.6243
2022-07-27 00:21:09 - train: epoch 0041, iter [04500, 05004], lr: 0.137429, loss: 1.9645
2022-07-27 00:23:06 - train: epoch 0041, iter [04600, 05004], lr: 0.137368, loss: 2.0310
2022-07-27 00:25:08 - train: epoch 0041, iter [04700, 05004], lr: 0.137306, loss: 1.9166
2022-07-27 00:27:07 - train: epoch 0041, iter [04800, 05004], lr: 0.137245, loss: 1.9972
2022-07-27 00:29:11 - train: epoch 0041, iter [04900, 05004], lr: 0.137184, loss: 2.1092
2022-07-27 00:31:06 - train: epoch 0041, iter [05000, 05004], lr: 0.137122, loss: 2.0024
2022-07-27 00:31:10 - train: epoch 041, train_loss: 1.8999
2022-07-27 00:35:25 - eval: epoch: 041, acc1: 60.788%, acc5: 84.076%, test_loss: 1.6332, per_image_load_time: 8.788ms, per_image_inference_time: 0.875ms
2022-07-27 00:35:25 - until epoch: 041, best_acc1: 60.788%
2022-07-27 00:35:25 - epoch 042 lr: 0.137119
2022-07-27 00:37:45 - train: epoch 0042, iter [00100, 05004], lr: 0.137058, loss: 1.5964
2022-07-27 00:39:48 - train: epoch 0042, iter [00200, 05004], lr: 0.136997, loss: 1.8704
2022-07-27 00:41:47 - train: epoch 0042, iter [00300, 05004], lr: 0.136936, loss: 2.0222
2022-07-27 00:43:48 - train: epoch 0042, iter [00400, 05004], lr: 0.136874, loss: 1.5769
2022-07-27 00:45:46 - train: epoch 0042, iter [00500, 05004], lr: 0.136813, loss: 1.8693
2022-07-27 00:47:43 - train: epoch 0042, iter [00600, 05004], lr: 0.136751, loss: 1.8549
2022-07-27 00:49:43 - train: epoch 0042, iter [00700, 05004], lr: 0.136690, loss: 1.9329
2022-07-27 00:51:41 - train: epoch 0042, iter [00800, 05004], lr: 0.136628, loss: 1.8907
2022-07-27 00:53:41 - train: epoch 0042, iter [00900, 05004], lr: 0.136567, loss: 1.9621
2022-07-27 00:55:33 - train: epoch 0042, iter [01000, 05004], lr: 0.136505, loss: 2.0546
2022-07-27 00:57:42 - train: epoch 0042, iter [01100, 05004], lr: 0.136444, loss: 1.7939
2022-07-27 00:59:34 - train: epoch 0042, iter [01200, 05004], lr: 0.136382, loss: 1.6559
2022-07-27 01:01:36 - train: epoch 0042, iter [01300, 05004], lr: 0.136321, loss: 1.9166
2022-07-27 01:03:36 - train: epoch 0042, iter [01400, 05004], lr: 0.136259, loss: 2.0026
2022-07-27 01:05:31 - train: epoch 0042, iter [01500, 05004], lr: 0.136197, loss: 1.8688
2022-07-27 01:07:29 - train: epoch 0042, iter [01600, 05004], lr: 0.136136, loss: 1.9748
2022-07-27 01:09:26 - train: epoch 0042, iter [01700, 05004], lr: 0.136074, loss: 1.9388
2022-07-27 01:11:25 - train: epoch 0042, iter [01800, 05004], lr: 0.136013, loss: 1.8326
2022-07-27 01:13:27 - train: epoch 0042, iter [01900, 05004], lr: 0.135951, loss: 2.1468
2022-07-27 01:15:23 - train: epoch 0042, iter [02000, 05004], lr: 0.135889, loss: 1.8503
2022-07-27 01:17:21 - train: epoch 0042, iter [02100, 05004], lr: 0.135828, loss: 1.6382
2022-07-27 01:19:21 - train: epoch 0042, iter [02200, 05004], lr: 0.135766, loss: 1.9491
2022-07-27 01:21:13 - train: epoch 0042, iter [02300, 05004], lr: 0.135704, loss: 1.9255
2022-07-27 01:23:10 - train: epoch 0042, iter [02400, 05004], lr: 0.135642, loss: 1.9779
2022-07-27 01:25:02 - train: epoch 0042, iter [02500, 05004], lr: 0.135581, loss: 1.8647
2022-07-27 01:27:02 - train: epoch 0042, iter [02600, 05004], lr: 0.135519, loss: 1.9828
2022-07-27 01:28:59 - train: epoch 0042, iter [02700, 05004], lr: 0.135457, loss: 1.7961
2022-07-27 01:30:59 - train: epoch 0042, iter [02800, 05004], lr: 0.135395, loss: 1.6639
2022-07-27 01:32:58 - train: epoch 0042, iter [02900, 05004], lr: 0.135333, loss: 1.7418
2022-07-27 01:34:56 - train: epoch 0042, iter [03000, 05004], lr: 0.135272, loss: 1.7560
2022-07-27 01:37:01 - train: epoch 0042, iter [03100, 05004], lr: 0.135210, loss: 1.7867
2022-07-27 01:38:57 - train: epoch 0042, iter [03200, 05004], lr: 0.135148, loss: 2.0406
2022-07-27 01:40:53 - train: epoch 0042, iter [03300, 05004], lr: 0.135086, loss: 2.0924
2022-07-27 01:42:49 - train: epoch 0042, iter [03400, 05004], lr: 0.135024, loss: 1.7097
2022-07-27 01:44:50 - train: epoch 0042, iter [03500, 05004], lr: 0.134962, loss: 1.9296
2022-07-27 01:46:47 - train: epoch 0042, iter [03600, 05004], lr: 0.134900, loss: 2.1968
2022-07-27 01:48:42 - train: epoch 0042, iter [03700, 05004], lr: 0.134838, loss: 1.9400
2022-07-27 01:50:40 - train: epoch 0042, iter [03800, 05004], lr: 0.134776, loss: 1.7446
2022-07-27 01:52:36 - train: epoch 0042, iter [03900, 05004], lr: 0.134714, loss: 1.9308
2022-07-27 01:54:33 - train: epoch 0042, iter [04000, 05004], lr: 0.134652, loss: 1.8731
2022-07-27 01:56:32 - train: epoch 0042, iter [04100, 05004], lr: 0.134590, loss: 1.8867
2022-07-27 01:58:31 - train: epoch 0042, iter [04200, 05004], lr: 0.134528, loss: 1.9774
2022-07-27 02:00:30 - train: epoch 0042, iter [04300, 05004], lr: 0.134466, loss: 1.7146
2022-07-27 02:02:30 - train: epoch 0042, iter [04400, 05004], lr: 0.134404, loss: 1.7308
2022-07-27 02:04:29 - train: epoch 0042, iter [04500, 05004], lr: 0.134342, loss: 1.8366
2022-07-27 02:06:23 - train: epoch 0042, iter [04600, 05004], lr: 0.134280, loss: 1.9781
2022-07-27 02:08:20 - train: epoch 0042, iter [04700, 05004], lr: 0.134218, loss: 1.9771
2022-07-27 02:10:20 - train: epoch 0042, iter [04800, 05004], lr: 0.134156, loss: 1.8754
2022-07-27 02:12:16 - train: epoch 0042, iter [04900, 05004], lr: 0.134094, loss: 1.9256
2022-07-27 02:14:08 - train: epoch 0042, iter [05000, 05004], lr: 0.134032, loss: 1.9770
2022-07-27 02:14:12 - train: epoch 042, train_loss: 1.8886
2022-07-27 02:18:05 - eval: epoch: 042, acc1: 60.692%, acc5: 84.084%, test_loss: 1.6317, per_image_load_time: 8.203ms, per_image_inference_time: 0.773ms
2022-07-27 02:18:06 - until epoch: 042, best_acc1: 60.788%
2022-07-27 02:18:06 - epoch 043 lr: 0.134029
2022-07-27 02:20:23 - train: epoch 0043, iter [00100, 05004], lr: 0.133967, loss: 2.1490
2022-07-27 02:22:22 - train: epoch 0043, iter [00200, 05004], lr: 0.133905, loss: 2.1006
2022-07-27 02:24:16 - train: epoch 0043, iter [00300, 05004], lr: 0.133843, loss: 1.4570
2022-07-27 02:26:14 - train: epoch 0043, iter [00400, 05004], lr: 0.133781, loss: 1.7233
2022-07-27 02:28:08 - train: epoch 0043, iter [00500, 05004], lr: 0.133718, loss: 1.7519
2022-07-27 02:30:08 - train: epoch 0043, iter [00600, 05004], lr: 0.133656, loss: 1.9374
2022-07-27 02:32:02 - train: epoch 0043, iter [00700, 05004], lr: 0.133594, loss: 1.9623
2022-07-27 02:34:01 - train: epoch 0043, iter [00800, 05004], lr: 0.133532, loss: 2.0076
2022-07-27 02:35:54 - train: epoch 0043, iter [00900, 05004], lr: 0.133469, loss: 1.8685
2022-07-27 02:37:56 - train: epoch 0043, iter [01000, 05004], lr: 0.133407, loss: 1.9140
2022-07-27 02:39:52 - train: epoch 0043, iter [01100, 05004], lr: 0.133345, loss: 1.8055
2022-07-27 02:41:55 - train: epoch 0043, iter [01200, 05004], lr: 0.133283, loss: 1.6621
2022-07-27 02:43:48 - train: epoch 0043, iter [01300, 05004], lr: 0.133220, loss: 1.8147
2022-07-27 02:45:50 - train: epoch 0043, iter [01400, 05004], lr: 0.133158, loss: 1.7594
2022-07-27 02:47:46 - train: epoch 0043, iter [01500, 05004], lr: 0.133096, loss: 1.8301
2022-07-27 02:49:44 - train: epoch 0043, iter [01600, 05004], lr: 0.133033, loss: 1.8999
2022-07-27 02:51:42 - train: epoch 0043, iter [01700, 05004], lr: 0.132971, loss: 2.1065
2022-07-27 02:53:40 - train: epoch 0043, iter [01800, 05004], lr: 0.132908, loss: 1.9200
2022-07-27 02:55:36 - train: epoch 0043, iter [01900, 05004], lr: 0.132846, loss: 1.9974
2022-07-27 02:57:34 - train: epoch 0043, iter [02000, 05004], lr: 0.132784, loss: 1.9183
2022-07-27 02:59:31 - train: epoch 0043, iter [02100, 05004], lr: 0.132721, loss: 1.6822
2022-07-27 03:01:26 - train: epoch 0043, iter [02200, 05004], lr: 0.132659, loss: 1.8614
2022-07-27 03:03:27 - train: epoch 0043, iter [02300, 05004], lr: 0.132596, loss: 2.1241
2022-07-27 03:05:23 - train: epoch 0043, iter [02400, 05004], lr: 0.132534, loss: 1.7310
2022-07-27 03:07:19 - train: epoch 0043, iter [02500, 05004], lr: 0.132471, loss: 2.0543
2022-07-27 03:09:12 - train: epoch 0043, iter [02600, 05004], lr: 0.132409, loss: 1.7695
2022-07-27 03:11:12 - train: epoch 0043, iter [02700, 05004], lr: 0.132346, loss: 2.0406
2022-07-27 03:13:15 - train: epoch 0043, iter [02800, 05004], lr: 0.132284, loss: 1.7831
2022-07-27 03:15:10 - train: epoch 0043, iter [02900, 05004], lr: 0.132221, loss: 1.8786
2022-07-27 03:17:07 - train: epoch 0043, iter [03000, 05004], lr: 0.132158, loss: 2.0720
2022-07-27 03:19:01 - train: epoch 0043, iter [03100, 05004], lr: 0.132096, loss: 2.1679
2022-07-27 03:21:04 - train: epoch 0043, iter [03200, 05004], lr: 0.132033, loss: 1.9690
2022-07-27 03:23:04 - train: epoch 0043, iter [03300, 05004], lr: 0.131971, loss: 1.8896
2022-07-27 03:25:02 - train: epoch 0043, iter [03400, 05004], lr: 0.131908, loss: 1.8589
2022-07-27 03:26:59 - train: epoch 0043, iter [03500, 05004], lr: 0.131845, loss: 1.8232
2022-07-27 03:28:57 - train: epoch 0043, iter [03600, 05004], lr: 0.131783, loss: 1.9847
2022-07-27 03:30:49 - train: epoch 0043, iter [03700, 05004], lr: 0.131720, loss: 1.8014
2022-07-27 03:32:52 - train: epoch 0043, iter [03800, 05004], lr: 0.131657, loss: 2.1049
2022-07-27 03:34:48 - train: epoch 0043, iter [03900, 05004], lr: 0.131595, loss: 1.9661
2022-07-27 03:36:45 - train: epoch 0043, iter [04000, 05004], lr: 0.131532, loss: 1.8280
2022-07-27 03:38:46 - train: epoch 0043, iter [04100, 05004], lr: 0.131469, loss: 2.0436
2022-07-27 03:40:42 - train: epoch 0043, iter [04200, 05004], lr: 0.131407, loss: 1.9513
2022-07-27 03:42:40 - train: epoch 0043, iter [04300, 05004], lr: 0.131344, loss: 2.0456
2022-07-27 03:44:35 - train: epoch 0043, iter [04400, 05004], lr: 0.131281, loss: 1.8501
2022-07-27 03:46:32 - train: epoch 0043, iter [04500, 05004], lr: 0.131218, loss: 1.8663
2022-07-27 03:48:29 - train: epoch 0043, iter [04600, 05004], lr: 0.131156, loss: 1.7965
2022-07-27 03:50:22 - train: epoch 0043, iter [04700, 05004], lr: 0.131093, loss: 1.9640
2022-07-27 03:52:21 - train: epoch 0043, iter [04800, 05004], lr: 0.131030, loss: 1.8755
2022-07-27 03:54:15 - train: epoch 0043, iter [04900, 05004], lr: 0.130967, loss: 1.9295
2022-07-27 03:56:12 - train: epoch 0043, iter [05000, 05004], lr: 0.130904, loss: 2.0522
2022-07-27 03:56:15 - train: epoch 043, train_loss: 1.8766
2022-07-27 04:00:15 - eval: epoch: 043, acc1: 61.346%, acc5: 84.258%, test_loss: 1.6179, per_image_load_time: 2.237ms, per_image_inference_time: 0.743ms
2022-07-27 04:00:15 - until epoch: 043, best_acc1: 61.346%
2022-07-27 04:00:15 - epoch 044 lr: 0.130901
2022-07-27 04:02:39 - train: epoch 0044, iter [00100, 05004], lr: 0.130839, loss: 1.6425
2022-07-27 04:04:36 - train: epoch 0044, iter [00200, 05004], lr: 0.130776, loss: 1.8187
2022-07-27 04:06:37 - train: epoch 0044, iter [00300, 05004], lr: 0.130713, loss: 1.7890
2022-07-27 04:08:40 - train: epoch 0044, iter [00400, 05004], lr: 0.130650, loss: 1.8476
2022-07-27 04:10:42 - train: epoch 0044, iter [00500, 05004], lr: 0.130587, loss: 1.7785
2022-07-27 04:12:40 - train: epoch 0044, iter [00600, 05004], lr: 0.130524, loss: 1.7029
2022-07-27 04:14:34 - train: epoch 0044, iter [00700, 05004], lr: 0.130461, loss: 1.7188
2022-07-27 04:16:35 - train: epoch 0044, iter [00800, 05004], lr: 0.130398, loss: 1.9293
2022-07-27 04:18:31 - train: epoch 0044, iter [00900, 05004], lr: 0.130335, loss: 1.7083
2022-07-27 04:20:27 - train: epoch 0044, iter [01000, 05004], lr: 0.130273, loss: 1.8458
2022-07-27 04:22:23 - train: epoch 0044, iter [01100, 05004], lr: 0.130210, loss: 1.6975
2022-07-27 04:24:26 - train: epoch 0044, iter [01200, 05004], lr: 0.130147, loss: 1.8208
2022-07-27 04:26:32 - train: epoch 0044, iter [01300, 05004], lr: 0.130084, loss: 1.9229
2022-07-27 04:28:28 - train: epoch 0044, iter [01400, 05004], lr: 0.130020, loss: 1.7836
2022-07-27 04:30:25 - train: epoch 0044, iter [01500, 05004], lr: 0.129957, loss: 1.9327
2022-07-27 04:32:23 - train: epoch 0044, iter [01600, 05004], lr: 0.129894, loss: 1.7254
2022-07-27 04:34:16 - train: epoch 0044, iter [01700, 05004], lr: 0.129831, loss: 1.7253
2022-07-27 04:36:14 - train: epoch 0044, iter [01800, 05004], lr: 0.129768, loss: 1.7270
2022-07-27 04:38:07 - train: epoch 0044, iter [01900, 05004], lr: 0.129705, loss: 2.0609
2022-07-27 04:40:05 - train: epoch 0044, iter [02000, 05004], lr: 0.129642, loss: 1.7942
2022-07-27 04:41:58 - train: epoch 0044, iter [02100, 05004], lr: 0.129579, loss: 2.1668
2022-07-27 04:43:58 - train: epoch 0044, iter [02200, 05004], lr: 0.129516, loss: 1.8444
2022-07-27 04:46:02 - train: epoch 0044, iter [02300, 05004], lr: 0.129453, loss: 2.0373
2022-07-27 04:47:53 - train: epoch 0044, iter [02400, 05004], lr: 0.129389, loss: 2.0448
2022-07-27 04:49:53 - train: epoch 0044, iter [02500, 05004], lr: 0.129326, loss: 1.8223
2022-07-27 04:51:44 - train: epoch 0044, iter [02600, 05004], lr: 0.129263, loss: 1.8167
2022-07-27 04:53:40 - train: epoch 0044, iter [02700, 05004], lr: 0.129200, loss: 1.7878
2022-07-27 04:55:40 - train: epoch 0044, iter [02800, 05004], lr: 0.129137, loss: 1.7632
2022-07-27 04:57:34 - train: epoch 0044, iter [02900, 05004], lr: 0.129073, loss: 1.6755
2022-07-27 04:59:34 - train: epoch 0044, iter [03000, 05004], lr: 0.129010, loss: 1.7393
2022-07-27 05:01:28 - train: epoch 0044, iter [03100, 05004], lr: 0.128947, loss: 1.8826
2022-07-27 05:03:27 - train: epoch 0044, iter [03200, 05004], lr: 0.128884, loss: 1.7456
2022-07-27 05:05:22 - train: epoch 0044, iter [03300, 05004], lr: 0.128820, loss: 1.9974
2022-07-27 05:07:16 - train: epoch 0044, iter [03400, 05004], lr: 0.128757, loss: 2.0331
2022-07-27 05:09:16 - train: epoch 0044, iter [03500, 05004], lr: 0.128694, loss: 1.8780
2022-07-27 05:11:16 - train: epoch 0044, iter [03600, 05004], lr: 0.128631, loss: 1.9481
2022-07-27 05:13:12 - train: epoch 0044, iter [03700, 05004], lr: 0.128567, loss: 1.8009
2022-07-27 05:15:06 - train: epoch 0044, iter [03800, 05004], lr: 0.128504, loss: 1.6579
2022-07-27 05:17:06 - train: epoch 0044, iter [03900, 05004], lr: 0.128441, loss: 1.9260
2022-07-27 05:19:04 - train: epoch 0044, iter [04000, 05004], lr: 0.128377, loss: 1.8627
2022-07-27 05:20:56 - train: epoch 0044, iter [04100, 05004], lr: 0.128314, loss: 1.7045
2022-07-27 05:22:58 - train: epoch 0044, iter [04200, 05004], lr: 0.128250, loss: 1.8100
2022-07-27 05:24:56 - train: epoch 0044, iter [04300, 05004], lr: 0.128187, loss: 1.8221
2022-07-27 05:26:49 - train: epoch 0044, iter [04400, 05004], lr: 0.128124, loss: 1.7943
2022-07-27 05:28:44 - train: epoch 0044, iter [04500, 05004], lr: 0.128060, loss: 1.9047
2022-07-27 05:30:41 - train: epoch 0044, iter [04600, 05004], lr: 0.127997, loss: 1.8884
2022-07-27 05:32:39 - train: epoch 0044, iter [04700, 05004], lr: 0.127933, loss: 2.0242
2022-07-27 05:34:34 - train: epoch 0044, iter [04800, 05004], lr: 0.127870, loss: 1.9081
2022-07-27 05:36:34 - train: epoch 0044, iter [04900, 05004], lr: 0.127806, loss: 1.9049
2022-07-27 05:38:27 - train: epoch 0044, iter [05000, 05004], lr: 0.127743, loss: 1.9297
2022-07-27 05:38:31 - train: epoch 044, train_loss: 1.8660
2022-07-27 05:42:32 - eval: epoch: 044, acc1: 60.840%, acc5: 84.092%, test_loss: 1.6294, per_image_load_time: 8.546ms, per_image_inference_time: 0.808ms
2022-07-27 05:42:33 - until epoch: 044, best_acc1: 61.346%
2022-07-27 05:42:33 - epoch 045 lr: 0.127740
2022-07-27 05:44:56 - train: epoch 0045, iter [00100, 05004], lr: 0.127677, loss: 1.6628
2022-07-27 05:46:55 - train: epoch 0045, iter [00200, 05004], lr: 0.127613, loss: 1.8478
2022-07-27 05:48:50 - train: epoch 0045, iter [00300, 05004], lr: 0.127550, loss: 1.8643
2022-07-27 05:50:48 - train: epoch 0045, iter [00400, 05004], lr: 0.127486, loss: 1.8781
2022-07-27 05:52:49 - train: epoch 0045, iter [00500, 05004], lr: 0.127423, loss: 2.0989
2022-07-27 05:54:48 - train: epoch 0045, iter [00600, 05004], lr: 0.127359, loss: 1.9694
2022-07-27 05:56:44 - train: epoch 0045, iter [00700, 05004], lr: 0.127296, loss: 1.6150
2022-07-27 05:58:52 - train: epoch 0045, iter [00800, 05004], lr: 0.127232, loss: 1.7086
2022-07-27 06:00:47 - train: epoch 0045, iter [00900, 05004], lr: 0.127168, loss: 1.6167
2022-07-27 06:02:46 - train: epoch 0045, iter [01000, 05004], lr: 0.127105, loss: 1.9705
2022-07-27 06:04:42 - train: epoch 0045, iter [01100, 05004], lr: 0.127041, loss: 1.9003
2022-07-27 06:06:39 - train: epoch 0045, iter [01200, 05004], lr: 0.126978, loss: 2.1154
2022-07-27 06:08:46 - train: epoch 0045, iter [01300, 05004], lr: 0.126914, loss: 1.8405
2022-07-27 06:10:38 - train: epoch 0045, iter [01400, 05004], lr: 0.126850, loss: 1.7591
2022-07-27 06:12:36 - train: epoch 0045, iter [01500, 05004], lr: 0.126787, loss: 1.9407
2022-07-27 06:14:34 - train: epoch 0045, iter [01600, 05004], lr: 0.126723, loss: 1.6479
2022-07-27 06:16:33 - train: epoch 0045, iter [01700, 05004], lr: 0.126659, loss: 1.7472
2022-07-27 06:18:29 - train: epoch 0045, iter [01800, 05004], lr: 0.126595, loss: 1.8100
2022-07-27 06:20:26 - train: epoch 0045, iter [01900, 05004], lr: 0.126532, loss: 1.6300
2022-07-27 06:22:23 - train: epoch 0045, iter [02000, 05004], lr: 0.126468, loss: 2.0083
2022-07-27 06:24:18 - train: epoch 0045, iter [02100, 05004], lr: 0.126404, loss: 1.9752
2022-07-27 06:26:15 - train: epoch 0045, iter [02200, 05004], lr: 0.126341, loss: 2.1430
2022-07-27 06:28:08 - train: epoch 0045, iter [02300, 05004], lr: 0.126277, loss: 1.7752
2022-07-27 06:30:02 - train: epoch 0045, iter [02400, 05004], lr: 0.126213, loss: 1.7600
2022-07-27 06:31:59 - train: epoch 0045, iter [02500, 05004], lr: 0.126149, loss: 1.7905
2022-07-27 06:33:54 - train: epoch 0045, iter [02600, 05004], lr: 0.126085, loss: 1.9368
2022-07-27 06:35:47 - train: epoch 0045, iter [02700, 05004], lr: 0.126022, loss: 1.8412
2022-07-27 06:37:49 - train: epoch 0045, iter [02800, 05004], lr: 0.125958, loss: 1.7544
2022-07-27 06:39:42 - train: epoch 0045, iter [02900, 05004], lr: 0.125894, loss: 2.0626
2022-07-27 06:41:46 - train: epoch 0045, iter [03000, 05004], lr: 0.125830, loss: 2.0180
2022-07-27 06:43:39 - train: epoch 0045, iter [03100, 05004], lr: 0.125766, loss: 1.9757
2022-07-27 06:45:32 - train: epoch 0045, iter [03200, 05004], lr: 0.125702, loss: 2.1446
2022-07-27 06:47:30 - train: epoch 0045, iter [03300, 05004], lr: 0.125639, loss: 1.8332
2022-07-27 06:49:29 - train: epoch 0045, iter [03400, 05004], lr: 0.125575, loss: 1.7055
2022-07-27 06:51:24 - train: epoch 0045, iter [03500, 05004], lr: 0.125511, loss: 2.0711
2022-07-27 06:53:21 - train: epoch 0045, iter [03600, 05004], lr: 0.125447, loss: 1.9195
2022-07-27 06:55:16 - train: epoch 0045, iter [03700, 05004], lr: 0.125383, loss: 1.6237
2022-07-27 06:57:13 - train: epoch 0045, iter [03800, 05004], lr: 0.125319, loss: 1.8060
2022-07-27 06:59:11 - train: epoch 0045, iter [03900, 05004], lr: 0.125255, loss: 1.8172
2022-07-27 07:01:00 - train: epoch 0045, iter [04000, 05004], lr: 0.125191, loss: 1.8398
2022-07-27 07:02:57 - train: epoch 0045, iter [04100, 05004], lr: 0.125127, loss: 1.8892
2022-07-27 07:04:49 - train: epoch 0045, iter [04200, 05004], lr: 0.125063, loss: 2.1556
2022-07-27 07:06:49 - train: epoch 0045, iter [04300, 05004], lr: 0.124999, loss: 1.7151
2022-07-27 07:08:46 - train: epoch 0045, iter [04400, 05004], lr: 0.124935, loss: 1.8577
2022-07-27 07:10:41 - train: epoch 0045, iter [04500, 05004], lr: 0.124871, loss: 1.7808
