2022-07-30 07:16:52 - train: epoch 0088, iter [00100, 05004], lr: 0.009072, loss: 1.0967
2022-07-30 07:18:55 - train: epoch 0088, iter [00200, 05004], lr: 0.009044, loss: 1.0465
2022-07-30 07:20:59 - train: epoch 0088, iter [00300, 05004], lr: 0.009017, loss: 1.0998
2022-07-30 07:23:04 - train: epoch 0088, iter [00400, 05004], lr: 0.008989, loss: 1.2644
2022-07-30 07:25:06 - train: epoch 0088, iter [00500, 05004], lr: 0.008962, loss: 1.1276
2022-07-30 07:27:07 - train: epoch 0088, iter [00600, 05004], lr: 0.008935, loss: 1.0745
2022-07-30 07:29:13 - train: epoch 0088, iter [00700, 05004], lr: 0.008908, loss: 0.9640
2022-07-30 07:31:13 - train: epoch 0088, iter [00800, 05004], lr: 0.008880, loss: 1.1859
2022-07-30 07:33:17 - train: epoch 0088, iter [00900, 05004], lr: 0.008853, loss: 1.0817
2022-07-30 07:35:21 - train: epoch 0088, iter [01000, 05004], lr: 0.008826, loss: 1.1004
2022-07-30 07:37:24 - train: epoch 0088, iter [01100, 05004], lr: 0.008799, loss: 1.0045
2022-07-30 07:39:28 - train: epoch 0088, iter [01200, 05004], lr: 0.008772, loss: 1.1739
2022-07-30 07:41:27 - train: epoch 0088, iter [01300, 05004], lr: 0.008745, loss: 1.1444
2022-07-30 07:43:28 - train: epoch 0088, iter [01400, 05004], lr: 0.008718, loss: 1.0964
2022-07-30 07:45:28 - train: epoch 0088, iter [01500, 05004], lr: 0.008691, loss: 1.0745
2022-07-30 07:47:31 - train: epoch 0088, iter [01600, 05004], lr: 0.008664, loss: 0.8029
2022-07-30 07:49:30 - train: epoch 0088, iter [01700, 05004], lr: 0.008637, loss: 1.0473
2022-07-30 07:51:31 - train: epoch 0088, iter [01800, 05004], lr: 0.008610, loss: 0.9037
2022-07-30 07:53:33 - train: epoch 0088, iter [01900, 05004], lr: 0.008583, loss: 0.9926
2022-07-30 07:55:32 - train: epoch 0088, iter [02000, 05004], lr: 0.008556, loss: 1.0688
2022-07-30 07:57:37 - train: epoch 0088, iter [02100, 05004], lr: 0.008530, loss: 0.9286
2022-07-30 07:59:41 - train: epoch 0088, iter [02200, 05004], lr: 0.008503, loss: 1.1350
2022-07-30 08:01:42 - train: epoch 0088, iter [02300, 05004], lr: 0.008476, loss: 1.1592
2022-07-30 08:03:46 - train: epoch 0088, iter [02400, 05004], lr: 0.008450, loss: 1.1417
2022-07-30 08:05:51 - train: epoch 0088, iter [02500, 05004], lr: 0.008423, loss: 1.2460
2022-07-30 08:07:54 - train: epoch 0088, iter [02600, 05004], lr: 0.008397, loss: 1.0181
2022-07-30 08:09:53 - train: epoch 0088, iter [02700, 05004], lr: 0.008370, loss: 1.1683
2022-07-30 08:11:51 - train: epoch 0088, iter [02800, 05004], lr: 0.008344, loss: 0.9652
2022-07-30 08:13:58 - train: epoch 0088, iter [02900, 05004], lr: 0.008317, loss: 0.9346
2022-07-30 08:16:00 - train: epoch 0088, iter [03000, 05004], lr: 0.008291, loss: 1.2142
2022-07-30 08:18:01 - train: epoch 0088, iter [03100, 05004], lr: 0.008265, loss: 1.0175
2022-07-30 08:20:05 - train: epoch 0088, iter [03200, 05004], lr: 0.008238, loss: 1.0640
2022-07-30 08:22:07 - train: epoch 0088, iter [03300, 05004], lr: 0.008212, loss: 0.9126
2022-07-30 08:24:17 - train: epoch 0088, iter [03400, 05004], lr: 0.008186, loss: 0.9543
2022-07-30 08:26:16 - train: epoch 0088, iter [03500, 05004], lr: 0.008160, loss: 1.0218
2022-07-30 08:28:24 - train: epoch 0088, iter [03600, 05004], lr: 0.008134, loss: 1.0555
2022-07-30 08:30:28 - train: epoch 0088, iter [03700, 05004], lr: 0.008108, loss: 1.0546
2022-07-30 08:32:28 - train: epoch 0088, iter [03800, 05004], lr: 0.008081, loss: 1.2070
2022-07-30 08:34:31 - train: epoch 0088, iter [03900, 05004], lr: 0.008055, loss: 1.0026
2022-07-30 08:36:40 - train: epoch 0088, iter [04000, 05004], lr: 0.008029, loss: 0.9806
2022-07-30 08:38:36 - train: epoch 0088, iter [04100, 05004], lr: 0.008004, loss: 1.0764
2022-07-30 08:40:41 - train: epoch 0088, iter [04200, 05004], lr: 0.007978, loss: 1.0871
2022-07-30 08:42:43 - train: epoch 0088, iter [04300, 05004], lr: 0.007952, loss: 0.9868
2022-07-30 08:44:43 - train: epoch 0088, iter [04400, 05004], lr: 0.007926, loss: 0.9347
2022-07-30 08:46:42 - train: epoch 0088, iter [04500, 05004], lr: 0.007900, loss: 0.9756
2022-07-30 08:48:39 - train: epoch 0088, iter [04600, 05004], lr: 0.007875, loss: 1.1729
2022-07-30 08:50:40 - train: epoch 0088, iter [04700, 05004], lr: 0.007849, loss: 1.1115
2022-07-30 08:52:46 - train: epoch 0088, iter [04800, 05004], lr: 0.007823, loss: 1.0909
2022-07-30 08:54:51 - train: epoch 0088, iter [04900, 05004], lr: 0.007798, loss: 0.9772
2022-07-30 08:56:47 - train: epoch 0088, iter [05000, 05004], lr: 0.007772, loss: 1.1536
2022-07-30 08:56:49 - train: epoch 088, train_loss: 1.0670
2022-07-30 09:01:06 - eval: epoch: 088, acc1: 72.974%, acc5: 91.406%, test_loss: 1.0886, per_image_load_time: 5.290ms, per_image_inference_time: 0.885ms
2022-07-30 09:01:06 - until epoch: 088, best_acc1: 72.974%
2022-07-30 09:01:06 - epoch 089 lr: 0.007771
2022-07-30 09:03:32 - train: epoch 0089, iter [00100, 05004], lr: 0.007746, loss: 1.1552
2022-07-30 09:05:34 - train: epoch 0089, iter [00200, 05004], lr: 0.007720, loss: 0.8106
2022-07-30 09:07:41 - train: epoch 0089, iter [00300, 05004], lr: 0.007695, loss: 1.1058
2022-07-30 09:09:39 - train: epoch 0089, iter [00400, 05004], lr: 0.007669, loss: 0.9452
2022-07-30 09:11:36 - train: epoch 0089, iter [00500, 05004], lr: 0.007644, loss: 0.9893
2022-07-30 09:13:35 - train: epoch 0089, iter [00600, 05004], lr: 0.007618, loss: 1.0142
2022-07-30 09:15:27 - train: epoch 0089, iter [00700, 05004], lr: 0.007593, loss: 0.9984
2022-07-30 09:17:26 - train: epoch 0089, iter [00800, 05004], lr: 0.007568, loss: 1.1140
2022-07-30 09:19:26 - train: epoch 0089, iter [00900, 05004], lr: 0.007543, loss: 0.9774
2022-07-30 09:21:20 - train: epoch 0089, iter [01000, 05004], lr: 0.007518, loss: 1.1816
2022-07-30 09:23:18 - train: epoch 0089, iter [01100, 05004], lr: 0.007493, loss: 0.9090
2022-07-30 09:25:15 - train: epoch 0089, iter [01200, 05004], lr: 0.007467, loss: 1.2432
2022-07-30 09:27:12 - train: epoch 0089, iter [01300, 05004], lr: 0.007442, loss: 0.9249
2022-07-30 09:29:07 - train: epoch 0089, iter [01400, 05004], lr: 0.007417, loss: 1.1816
2022-07-30 09:31:05 - train: epoch 0089, iter [01500, 05004], lr: 0.007392, loss: 1.1175
2022-07-30 09:32:57 - train: epoch 0089, iter [01600, 05004], lr: 0.007368, loss: 0.9170
2022-07-30 09:34:57 - train: epoch 0089, iter [01700, 05004], lr: 0.007343, loss: 1.0851
2022-07-30 09:36:55 - train: epoch 0089, iter [01800, 05004], lr: 0.007318, loss: 1.1373
2022-07-30 09:38:57 - train: epoch 0089, iter [01900, 05004], lr: 0.007293, loss: 0.9118
2022-07-30 09:40:59 - train: epoch 0089, iter [02000, 05004], lr: 0.007268, loss: 0.9248
2022-07-30 09:42:55 - train: epoch 0089, iter [02100, 05004], lr: 0.007244, loss: 1.1587
2022-07-30 09:44:56 - train: epoch 0089, iter [02200, 05004], lr: 0.007219, loss: 1.1501
2022-07-30 09:46:51 - train: epoch 0089, iter [02300, 05004], lr: 0.007194, loss: 0.9792
2022-07-30 09:48:53 - train: epoch 0089, iter [02400, 05004], lr: 0.007170, loss: 0.9532
2022-07-30 09:50:51 - train: epoch 0089, iter [02500, 05004], lr: 0.007145, loss: 1.1511
2022-07-30 09:52:55 - train: epoch 0089, iter [02600, 05004], lr: 0.007121, loss: 0.9226
2022-07-30 09:54:54 - train: epoch 0089, iter [02700, 05004], lr: 0.007096, loss: 0.9765
2022-07-30 09:56:55 - train: epoch 0089, iter [02800, 05004], lr: 0.007072, loss: 1.1817
2022-07-30 09:58:54 - train: epoch 0089, iter [02900, 05004], lr: 0.007047, loss: 1.0382
2022-07-30 10:00:52 - train: epoch 0089, iter [03000, 05004], lr: 0.007023, loss: 1.1165
2022-07-30 10:02:56 - train: epoch 0089, iter [03100, 05004], lr: 0.006999, loss: 1.1322
2022-07-30 10:04:52 - train: epoch 0089, iter [03200, 05004], lr: 0.006974, loss: 1.1193
2022-07-30 10:06:55 - train: epoch 0089, iter [03300, 05004], lr: 0.006950, loss: 1.0300
2022-07-30 10:08:54 - train: epoch 0089, iter [03400, 05004], lr: 0.006926, loss: 1.1573
2022-07-30 10:10:58 - train: epoch 0089, iter [03500, 05004], lr: 0.006902, loss: 0.8308
2022-07-30 10:12:57 - train: epoch 0089, iter [03600, 05004], lr: 0.006878, loss: 1.0588
2022-07-30 10:14:57 - train: epoch 0089, iter [03700, 05004], lr: 0.006854, loss: 1.2466
2022-07-30 10:16:56 - train: epoch 0089, iter [03800, 05004], lr: 0.006830, loss: 1.0365
2022-07-30 10:18:59 - train: epoch 0089, iter [03900, 05004], lr: 0.006806, loss: 1.2989
2022-07-30 10:20:55 - train: epoch 0089, iter [04000, 05004], lr: 0.006782, loss: 1.0724
2022-07-30 10:22:55 - train: epoch 0089, iter [04100, 05004], lr: 0.006758, loss: 1.1995
2022-07-30 10:24:53 - train: epoch 0089, iter [04200, 05004], lr: 0.006734, loss: 1.0524
2022-07-30 10:26:46 - train: epoch 0089, iter [04300, 05004], lr: 0.006710, loss: 1.1527
2022-07-30 10:28:45 - train: epoch 0089, iter [04400, 05004], lr: 0.006686, loss: 0.9821
2022-07-30 10:30:38 - train: epoch 0089, iter [04500, 05004], lr: 0.006663, loss: 0.9230
2022-07-30 10:32:40 - train: epoch 0089, iter [04600, 05004], lr: 0.006639, loss: 1.0714
2022-07-30 10:34:36 - train: epoch 0089, iter [04700, 05004], lr: 0.006615, loss: 0.9200
2022-07-30 10:36:34 - train: epoch 0089, iter [04800, 05004], lr: 0.006592, loss: 1.0215
2022-07-30 10:38:29 - train: epoch 0089, iter [04900, 05004], lr: 0.006568, loss: 1.0046
2022-07-30 10:40:23 - train: epoch 0089, iter [05000, 05004], lr: 0.006544, loss: 0.8868
2022-07-30 10:40:25 - train: epoch 089, train_loss: 1.0426
2022-07-30 10:44:35 - eval: epoch: 089, acc1: 73.326%, acc5: 91.450%, test_loss: 1.0824, per_image_load_time: 8.668ms, per_image_inference_time: 0.778ms
2022-07-30 10:44:35 - until epoch: 089, best_acc1: 73.326%
2022-07-30 10:44:35 - epoch 090 lr: 0.006543
2022-07-30 10:46:50 - train: epoch 0090, iter [00100, 05004], lr: 0.006520, loss: 0.9671
2022-07-30 10:48:45 - train: epoch 0090, iter [00200, 05004], lr: 0.006497, loss: 0.8491
2022-07-30 10:50:44 - train: epoch 0090, iter [00300, 05004], lr: 0.006473, loss: 0.9672
2022-07-30 10:52:39 - train: epoch 0090, iter [00400, 05004], lr: 0.006450, loss: 1.0011
2022-07-30 10:54:39 - train: epoch 0090, iter [00500, 05004], lr: 0.006426, loss: 1.0814
2022-07-30 10:56:32 - train: epoch 0090, iter [00600, 05004], lr: 0.006403, loss: 1.3291
2022-07-30 10:58:36 - train: epoch 0090, iter [00700, 05004], lr: 0.006380, loss: 1.0059
2022-07-30 11:00:30 - train: epoch 0090, iter [00800, 05004], lr: 0.006357, loss: 1.1300
2022-07-30 11:02:25 - train: epoch 0090, iter [00900, 05004], lr: 0.006334, loss: 1.0892
2022-07-30 11:04:22 - train: epoch 0090, iter [01000, 05004], lr: 0.006310, loss: 0.9283
2022-07-30 11:06:17 - train: epoch 0090, iter [01100, 05004], lr: 0.006287, loss: 1.0186
2022-07-30 11:08:14 - train: epoch 0090, iter [01200, 05004], lr: 0.006264, loss: 0.8848
2022-07-30 11:10:16 - train: epoch 0090, iter [01300, 05004], lr: 0.006241, loss: 1.0997
2022-07-30 11:12:15 - train: epoch 0090, iter [01400, 05004], lr: 0.006218, loss: 0.9754
2022-07-30 11:14:15 - train: epoch 0090, iter [01500, 05004], lr: 0.006195, loss: 1.0388
2022-07-30 11:16:19 - train: epoch 0090, iter [01600, 05004], lr: 0.006173, loss: 0.9698
2022-07-30 11:18:16 - train: epoch 0090, iter [01700, 05004], lr: 0.006150, loss: 0.8751
2022-07-30 11:20:11 - train: epoch 0090, iter [01800, 05004], lr: 0.006127, loss: 1.1627
2022-07-30 11:22:12 - train: epoch 0090, iter [01900, 05004], lr: 0.006104, loss: 0.8786
2022-07-30 11:24:06 - train: epoch 0090, iter [02000, 05004], lr: 0.006081, loss: 1.0168
2022-07-30 11:26:06 - train: epoch 0090, iter [02100, 05004], lr: 0.006059, loss: 1.2055
2022-07-30 11:28:06 - train: epoch 0090, iter [02200, 05004], lr: 0.006036, loss: 1.1612
2022-07-30 11:30:02 - train: epoch 0090, iter [02300, 05004], lr: 0.006014, loss: 1.1252
2022-07-30 11:32:07 - train: epoch 0090, iter [02400, 05004], lr: 0.005991, loss: 1.1524
2022-07-30 11:34:08 - train: epoch 0090, iter [02500, 05004], lr: 0.005969, loss: 0.9242
2022-07-30 11:36:05 - train: epoch 0090, iter [02600, 05004], lr: 0.005946, loss: 0.9681
2022-07-30 11:38:02 - train: epoch 0090, iter [02700, 05004], lr: 0.005924, loss: 1.1290
2022-07-30 11:40:03 - train: epoch 0090, iter [02800, 05004], lr: 0.005901, loss: 0.8957
2022-07-30 11:41:58 - train: epoch 0090, iter [02900, 05004], lr: 0.005879, loss: 1.0729
2022-07-30 11:44:01 - train: epoch 0090, iter [03000, 05004], lr: 0.005857, loss: 1.0154
2022-07-30 11:46:07 - train: epoch 0090, iter [03100, 05004], lr: 0.005834, loss: 0.8145
2022-07-30 11:48:04 - train: epoch 0090, iter [03200, 05004], lr: 0.005812, loss: 1.0500
2022-07-30 11:49:59 - train: epoch 0090, iter [03300, 05004], lr: 0.005790, loss: 1.2425
2022-07-30 11:52:00 - train: epoch 0090, iter [03400, 05004], lr: 0.005768, loss: 0.9552
2022-07-30 11:54:00 - train: epoch 0090, iter [03500, 05004], lr: 0.005746, loss: 1.0451
2022-07-30 11:55:56 - train: epoch 0090, iter [03600, 05004], lr: 0.005724, loss: 0.9897
2022-07-30 11:57:53 - train: epoch 0090, iter [03700, 05004], lr: 0.005702, loss: 0.9826
2022-07-30 11:59:46 - train: epoch 0090, iter [03800, 05004], lr: 0.005680, loss: 1.0329
2022-07-30 12:01:46 - train: epoch 0090, iter [03900, 05004], lr: 0.005658, loss: 0.9435
2022-07-30 12:03:43 - train: epoch 0090, iter [04000, 05004], lr: 0.005636, loss: 1.0356
2022-07-30 12:05:43 - train: epoch 0090, iter [04100, 05004], lr: 0.005614, loss: 1.3438
2022-07-30 12:07:47 - train: epoch 0090, iter [04200, 05004], lr: 0.005592, loss: 1.0551
2022-07-30 12:09:46 - train: epoch 0090, iter [04300, 05004], lr: 0.005570, loss: 0.9825
2022-07-30 12:11:40 - train: epoch 0090, iter [04400, 05004], lr: 0.005549, loss: 1.1363
2022-07-30 12:13:36 - train: epoch 0090, iter [04500, 05004], lr: 0.005527, loss: 1.1209
2022-07-30 12:15:39 - train: epoch 0090, iter [04600, 05004], lr: 0.005505, loss: 0.9621
2022-07-30 12:17:36 - train: epoch 0090, iter [04700, 05004], lr: 0.005484, loss: 0.8387
2022-07-30 12:19:32 - train: epoch 0090, iter [04800, 05004], lr: 0.005462, loss: 1.0852
2022-07-30 12:21:29 - train: epoch 0090, iter [04900, 05004], lr: 0.005441, loss: 0.8853
2022-07-30 12:23:25 - train: epoch 0090, iter [05000, 05004], lr: 0.005419, loss: 1.0331
2022-07-30 12:23:28 - train: epoch 090, train_loss: 1.0184
2022-07-30 12:27:39 - eval: epoch: 090, acc1: 73.470%, acc5: 91.512%, test_loss: 1.0726, per_image_load_time: 8.713ms, per_image_inference_time: 0.798ms
2022-07-30 12:27:39 - until epoch: 090, best_acc1: 73.470%
2022-07-30 12:27:39 - epoch 091 lr: 0.005418
2022-07-30 12:29:57 - train: epoch 0091, iter [00100, 05004], lr: 0.005397, loss: 0.8580
2022-07-30 12:31:57 - train: epoch 0091, iter [00200, 05004], lr: 0.005375, loss: 0.8295
2022-07-30 12:33:59 - train: epoch 0091, iter [00300, 05004], lr: 0.005354, loss: 0.9185
2022-07-30 12:35:59 - train: epoch 0091, iter [00400, 05004], lr: 0.005333, loss: 1.0588
2022-07-30 12:37:46 - train: epoch 0091, iter [00500, 05004], lr: 0.005312, loss: 0.9284
2022-07-30 12:39:40 - train: epoch 0091, iter [00600, 05004], lr: 0.005290, loss: 0.9630
2022-07-30 12:41:40 - train: epoch 0091, iter [00700, 05004], lr: 0.005269, loss: 1.1735
2022-07-30 12:43:40 - train: epoch 0091, iter [00800, 05004], lr: 0.005248, loss: 0.8637
2022-07-30 12:45:39 - train: epoch 0091, iter [00900, 05004], lr: 0.005227, loss: 0.9080
2022-07-30 12:47:36 - train: epoch 0091, iter [01000, 05004], lr: 0.005206, loss: 0.9555
2022-07-30 12:49:40 - train: epoch 0091, iter [01100, 05004], lr: 0.005185, loss: 0.9323
2022-07-30 12:51:39 - train: epoch 0091, iter [01200, 05004], lr: 0.005164, loss: 1.1654
2022-07-30 12:53:44 - train: epoch 0091, iter [01300, 05004], lr: 0.005143, loss: 1.0871
2022-07-30 12:55:42 - train: epoch 0091, iter [01400, 05004], lr: 0.005122, loss: 0.9862
2022-07-30 12:57:44 - train: epoch 0091, iter [01500, 05004], lr: 0.005101, loss: 0.9972
2022-07-30 12:59:46 - train: epoch 0091, iter [01600, 05004], lr: 0.005080, loss: 0.8763
2022-07-30 13:01:45 - train: epoch 0091, iter [01700, 05004], lr: 0.005059, loss: 0.9837
2022-07-30 13:03:45 - train: epoch 0091, iter [01800, 05004], lr: 0.005039, loss: 1.0044
2022-07-30 13:05:48 - train: epoch 0091, iter [01900, 05004], lr: 0.005018, loss: 0.9988
2022-07-30 13:07:50 - train: epoch 0091, iter [02000, 05004], lr: 0.004997, loss: 0.9084
2022-07-30 13:09:48 - train: epoch 0091, iter [02100, 05004], lr: 0.004977, loss: 0.8843
2022-07-30 13:11:47 - train: epoch 0091, iter [02200, 05004], lr: 0.004956, loss: 1.1174
2022-07-30 13:13:51 - train: epoch 0091, iter [02300, 05004], lr: 0.004936, loss: 0.9467
2022-07-30 13:15:50 - train: epoch 0091, iter [02400, 05004], lr: 0.004915, loss: 1.0165
2022-07-30 13:17:56 - train: epoch 0091, iter [02500, 05004], lr: 0.004895, loss: 0.9348
2022-07-30 13:19:53 - train: epoch 0091, iter [02600, 05004], lr: 0.004874, loss: 1.0236
2022-07-30 13:21:59 - train: epoch 0091, iter [02700, 05004], lr: 0.004854, loss: 0.8958
2022-07-30 13:23:55 - train: epoch 0091, iter [02800, 05004], lr: 0.004834, loss: 0.9019
2022-07-30 13:25:53 - train: epoch 0091, iter [02900, 05004], lr: 0.004813, loss: 1.0817
2022-07-30 13:27:53 - train: epoch 0091, iter [03000, 05004], lr: 0.004793, loss: 1.0691
2022-07-30 13:29:52 - train: epoch 0091, iter [03100, 05004], lr: 0.004773, loss: 0.9427
2022-07-30 13:31:50 - train: epoch 0091, iter [03200, 05004], lr: 0.004753, loss: 1.0937
2022-07-30 13:33:54 - train: epoch 0091, iter [03300, 05004], lr: 0.004733, loss: 0.9340
2022-07-30 13:35:53 - train: epoch 0091, iter [03400, 05004], lr: 0.004713, loss: 0.9275
2022-07-30 13:37:53 - train: epoch 0091, iter [03500, 05004], lr: 0.004693, loss: 1.0111
2022-07-30 13:39:56 - train: epoch 0091, iter [03600, 05004], lr: 0.004673, loss: 1.0413
2022-07-30 13:41:50 - train: epoch 0091, iter [03700, 05004], lr: 0.004653, loss: 1.1232
2022-07-30 13:43:52 - train: epoch 0091, iter [03800, 05004], lr: 0.004633, loss: 1.0531
2022-07-30 13:45:47 - train: epoch 0091, iter [03900, 05004], lr: 0.004613, loss: 0.9895
2022-07-30 13:47:50 - train: epoch 0091, iter [04000, 05004], lr: 0.004593, loss: 0.9350
2022-07-30 13:49:45 - train: epoch 0091, iter [04100, 05004], lr: 0.004573, loss: 1.0578
2022-07-30 13:51:45 - train: epoch 0091, iter [04200, 05004], lr: 0.004554, loss: 1.0391
2022-07-30 13:53:48 - train: epoch 0091, iter [04300, 05004], lr: 0.004534, loss: 0.9375
2022-07-30 13:55:49 - train: epoch 0091, iter [04400, 05004], lr: 0.004514, loss: 0.9012
2022-07-30 13:57:45 - train: epoch 0091, iter [04500, 05004], lr: 0.004495, loss: 0.9655
2022-07-30 13:59:40 - train: epoch 0091, iter [04600, 05004], lr: 0.004475, loss: 0.9824
2022-07-30 14:01:45 - train: epoch 0091, iter [04700, 05004], lr: 0.004456, loss: 1.0271
2022-07-30 14:03:42 - train: epoch 0091, iter [04800, 05004], lr: 0.004436, loss: 0.8991
2022-07-30 14:05:38 - train: epoch 0091, iter [04900, 05004], lr: 0.004417, loss: 0.8883
2022-07-30 14:07:25 - train: epoch 0091, iter [05000, 05004], lr: 0.004397, loss: 1.2191
2022-07-30 14:07:30 - train: epoch 091, train_loss: 0.9939
2022-07-30 14:11:39 - eval: epoch: 091, acc1: 73.628%, acc5: 91.680%, test_loss: 1.0722, per_image_load_time: 8.602ms, per_image_inference_time: 0.870ms
2022-07-30 14:11:40 - until epoch: 091, best_acc1: 73.628%
2022-07-30 14:11:40 - epoch 092 lr: 0.004396
2022-07-30 14:14:02 - train: epoch 0092, iter [00100, 05004], lr: 0.004377, loss: 0.8441
2022-07-30 14:16:00 - train: epoch 0092, iter [00200, 05004], lr: 0.004358, loss: 0.9804
2022-07-30 14:17:54 - train: epoch 0092, iter [00300, 05004], lr: 0.004338, loss: 1.2881
2022-07-30 14:19:47 - train: epoch 0092, iter [00400, 05004], lr: 0.004319, loss: 1.0188
2022-07-30 14:21:37 - train: epoch 0092, iter [00500, 05004], lr: 0.004300, loss: 0.8930
2022-07-30 14:23:31 - train: epoch 0092, iter [00600, 05004], lr: 0.004281, loss: 0.9450
2022-07-30 14:25:21 - train: epoch 0092, iter [00700, 05004], lr: 0.004262, loss: 0.9945
2022-07-30 14:27:08 - train: epoch 0092, iter [00800, 05004], lr: 0.004243, loss: 1.0647
2022-07-30 14:29:01 - train: epoch 0092, iter [00900, 05004], lr: 0.004224, loss: 1.0751
2022-07-30 14:30:47 - train: epoch 0092, iter [01000, 05004], lr: 0.004205, loss: 0.8994
2022-07-30 14:32:35 - train: epoch 0092, iter [01100, 05004], lr: 0.004186, loss: 0.8013
2022-07-30 14:34:20 - train: epoch 0092, iter [01200, 05004], lr: 0.004167, loss: 0.9013
2022-07-30 14:36:08 - train: epoch 0092, iter [01300, 05004], lr: 0.004148, loss: 0.9603
2022-07-30 14:37:57 - train: epoch 0092, iter [01400, 05004], lr: 0.004129, loss: 0.8135
2022-07-30 14:39:47 - train: epoch 0092, iter [01500, 05004], lr: 0.004110, loss: 0.9344
2022-07-30 14:41:35 - train: epoch 0092, iter [01600, 05004], lr: 0.004092, loss: 0.8646
2022-07-30 14:43:27 - train: epoch 0092, iter [01700, 05004], lr: 0.004073, loss: 1.0040
2022-07-30 14:45:17 - train: epoch 0092, iter [01800, 05004], lr: 0.004054, loss: 1.1036
2022-07-30 14:47:07 - train: epoch 0092, iter [01900, 05004], lr: 0.004036, loss: 1.0130
2022-07-30 14:48:52 - train: epoch 0092, iter [02000, 05004], lr: 0.004017, loss: 0.8852
2022-07-30 14:50:44 - train: epoch 0092, iter [02100, 05004], lr: 0.003999, loss: 1.0014
2022-07-30 14:52:36 - train: epoch 0092, iter [02200, 05004], lr: 0.003980, loss: 0.9375
2022-07-30 14:54:25 - train: epoch 0092, iter [02300, 05004], lr: 0.003962, loss: 1.0313
2022-07-30 14:56:13 - train: epoch 0092, iter [02400, 05004], lr: 0.003943, loss: 0.9159
2022-07-30 14:58:01 - train: epoch 0092, iter [02500, 05004], lr: 0.003925, loss: 0.8233
2022-07-30 14:59:54 - train: epoch 0092, iter [02600, 05004], lr: 0.003907, loss: 0.9601
2022-07-30 15:01:45 - train: epoch 0092, iter [02700, 05004], lr: 0.003888, loss: 1.0511
2022-07-30 15:03:37 - train: epoch 0092, iter [02800, 05004], lr: 0.003870, loss: 1.0098
2022-07-30 15:05:24 - train: epoch 0092, iter [02900, 05004], lr: 0.003852, loss: 1.0825
2022-07-30 15:07:11 - train: epoch 0092, iter [03000, 05004], lr: 0.003834, loss: 0.8221
2022-07-30 15:09:02 - train: epoch 0092, iter [03100, 05004], lr: 0.003816, loss: 0.9654
2022-07-30 15:10:57 - train: epoch 0092, iter [03200, 05004], lr: 0.003798, loss: 0.8731
2022-07-30 15:12:45 - train: epoch 0092, iter [03300, 05004], lr: 0.003780, loss: 1.0198
2022-07-30 15:14:37 - train: epoch 0092, iter [03400, 05004], lr: 0.003762, loss: 1.1272
2022-07-30 15:16:29 - train: epoch 0092, iter [03500, 05004], lr: 0.003744, loss: 0.8940
2022-07-30 15:18:20 - train: epoch 0092, iter [03600, 05004], lr: 0.003726, loss: 0.8824
2022-07-30 15:20:11 - train: epoch 0092, iter [03700, 05004], lr: 0.003708, loss: 0.8915
2022-07-30 15:22:01 - train: epoch 0092, iter [03800, 05004], lr: 0.003690, loss: 0.9496
2022-07-30 15:23:53 - train: epoch 0092, iter [03900, 05004], lr: 0.003672, loss: 1.1070
2022-07-30 15:25:41 - train: epoch 0092, iter [04000, 05004], lr: 0.003655, loss: 0.9507
2022-07-30 15:27:41 - train: epoch 0092, iter [04100, 05004], lr: 0.003637, loss: 0.9525
2022-07-30 15:29:35 - train: epoch 0092, iter [04200, 05004], lr: 0.003619, loss: 1.0008
2022-07-30 15:31:30 - train: epoch 0092, iter [04300, 05004], lr: 0.003602, loss: 0.8158
2022-07-30 15:33:26 - train: epoch 0092, iter [04400, 05004], lr: 0.003584, loss: 1.1052
2022-07-30 15:35:25 - train: epoch 0092, iter [04500, 05004], lr: 0.003567, loss: 0.8900
2022-07-30 15:37:24 - train: epoch 0092, iter [04600, 05004], lr: 0.003549, loss: 1.0967
2022-07-30 15:39:22 - train: epoch 0092, iter [04700, 05004], lr: 0.003532, loss: 0.8526
2022-07-30 15:41:20 - train: epoch 0092, iter [04800, 05004], lr: 0.003514, loss: 0.9785
2022-07-30 15:43:20 - train: epoch 0092, iter [04900, 05004], lr: 0.003497, loss: 0.8709
2022-07-30 15:45:16 - train: epoch 0092, iter [05000, 05004], lr: 0.003480, loss: 0.7437
2022-07-30 15:45:20 - train: epoch 092, train_loss: 0.9740
2022-07-30 15:49:51 - eval: epoch: 092, acc1: 73.912%, acc5: 91.800%, test_loss: 1.0632, per_image_load_time: 4.169ms, per_image_inference_time: 0.774ms
2022-07-30 15:49:52 - until epoch: 092, best_acc1: 73.912%
2022-07-30 15:49:52 - epoch 093 lr: 0.003479
2022-07-30 15:51:59 - train: epoch 0093, iter [00100, 05004], lr: 0.003462, loss: 0.9442
2022-07-30 15:53:59 - train: epoch 0093, iter [00200, 05004], lr: 0.003445, loss: 0.7218
2022-07-30 15:55:53 - train: epoch 0093, iter [00300, 05004], lr: 0.003427, loss: 0.9071
2022-07-30 15:57:55 - train: epoch 0093, iter [00400, 05004], lr: 0.003410, loss: 0.8556
2022-07-30 15:59:55 - train: epoch 0093, iter [00500, 05004], lr: 0.003393, loss: 1.2268
2022-07-30 16:01:56 - train: epoch 0093, iter [00600, 05004], lr: 0.003376, loss: 1.0652
2022-07-30 16:03:58 - train: epoch 0093, iter [00700, 05004], lr: 0.003359, loss: 0.9909
2022-07-30 16:05:54 - train: epoch 0093, iter [00800, 05004], lr: 0.003342, loss: 0.8623
2022-07-30 16:07:49 - train: epoch 0093, iter [00900, 05004], lr: 0.003325, loss: 0.9160
2022-07-30 16:09:53 - train: epoch 0093, iter [01000, 05004], lr: 0.003308, loss: 0.7076
2022-07-30 16:11:50 - train: epoch 0093, iter [01100, 05004], lr: 0.003292, loss: 0.8507
2022-07-30 16:13:52 - train: epoch 0093, iter [01200, 05004], lr: 0.003275, loss: 0.8555
2022-07-30 16:15:51 - train: epoch 0093, iter [01300, 05004], lr: 0.003258, loss: 0.9560
2022-07-30 16:17:53 - train: epoch 0093, iter [01400, 05004], lr: 0.003241, loss: 1.1468
2022-07-30 16:19:50 - train: epoch 0093, iter [01500, 05004], lr: 0.003225, loss: 1.0005
2022-07-30 16:21:45 - train: epoch 0093, iter [01600, 05004], lr: 0.003208, loss: 0.9039
2022-07-30 16:23:47 - train: epoch 0093, iter [01700, 05004], lr: 0.003191, loss: 0.9527
2022-07-30 16:25:48 - train: epoch 0093, iter [01800, 05004], lr: 0.003175, loss: 1.1408
2022-07-30 16:27:46 - train: epoch 0093, iter [01900, 05004], lr: 0.003158, loss: 0.9489
2022-07-30 16:29:44 - train: epoch 0093, iter [02000, 05004], lr: 0.003142, loss: 0.8878
2022-07-30 16:31:48 - train: epoch 0093, iter [02100, 05004], lr: 0.003126, loss: 1.0129
2022-07-30 16:33:44 - train: epoch 0093, iter [02200, 05004], lr: 0.003109, loss: 0.9987
2022-07-30 16:35:48 - train: epoch 0093, iter [02300, 05004], lr: 0.003093, loss: 1.0282
2022-07-30 16:37:49 - train: epoch 0093, iter [02400, 05004], lr: 0.003077, loss: 0.9035
2022-07-30 16:39:50 - train: epoch 0093, iter [02500, 05004], lr: 0.003060, loss: 0.9563
2022-07-30 16:41:56 - train: epoch 0093, iter [02600, 05004], lr: 0.003044, loss: 0.9999
2022-07-30 16:44:01 - train: epoch 0093, iter [02700, 05004], lr: 0.003028, loss: 0.7753
2022-07-30 16:45:59 - train: epoch 0093, iter [02800, 05004], lr: 0.003012, loss: 0.8608
2022-07-30 16:47:54 - train: epoch 0093, iter [02900, 05004], lr: 0.002996, loss: 0.7831
2022-07-30 16:49:54 - train: epoch 0093, iter [03000, 05004], lr: 0.002980, loss: 0.9162
2022-07-30 16:51:54 - train: epoch 0093, iter [03100, 05004], lr: 0.002964, loss: 0.8861
2022-07-30 16:53:54 - train: epoch 0093, iter [03200, 05004], lr: 0.002948, loss: 0.8259
2022-07-30 16:55:55 - train: epoch 0093, iter [03300, 05004], lr: 0.002932, loss: 0.9443
2022-07-30 16:57:49 - train: epoch 0093, iter [03400, 05004], lr: 0.002916, loss: 1.0549
2022-07-30 16:59:49 - train: epoch 0093, iter [03500, 05004], lr: 0.002900, loss: 0.9468
2022-07-30 17:01:52 - train: epoch 0093, iter [03600, 05004], lr: 0.002884, loss: 1.0275
2022-07-30 17:03:48 - train: epoch 0093, iter [03700, 05004], lr: 0.002869, loss: 0.9568
2022-07-30 17:05:45 - train: epoch 0093, iter [03800, 05004], lr: 0.002853, loss: 0.7966
2022-07-30 17:07:43 - train: epoch 0093, iter [03900, 05004], lr: 0.002837, loss: 1.0104
2022-07-30 17:09:45 - train: epoch 0093, iter [04000, 05004], lr: 0.002822, loss: 1.1182
2022-07-30 17:11:45 - train: epoch 0093, iter [04100, 05004], lr: 0.002806, loss: 0.9313
2022-07-30 17:13:46 - train: epoch 0093, iter [04200, 05004], lr: 0.002791, loss: 0.9042
2022-07-30 17:15:39 - train: epoch 0093, iter [04300, 05004], lr: 0.002775, loss: 1.0489
2022-07-30 17:17:39 - train: epoch 0093, iter [04400, 05004], lr: 0.002760, loss: 0.8061
2022-07-30 17:19:32 - train: epoch 0093, iter [04500, 05004], lr: 0.002744, loss: 1.0429
2022-07-30 17:21:32 - train: epoch 0093, iter [04600, 05004], lr: 0.002729, loss: 0.7950
2022-07-30 17:23:38 - train: epoch 0093, iter [04700, 05004], lr: 0.002714, loss: 1.0040
2022-07-30 17:25:29 - train: epoch 0093, iter [04800, 05004], lr: 0.002698, loss: 0.8824
2022-07-30 17:27:29 - train: epoch 0093, iter [04900, 05004], lr: 0.002683, loss: 0.9211
2022-07-30 17:29:29 - train: epoch 0093, iter [05000, 05004], lr: 0.002668, loss: 1.0912
2022-07-30 17:29:32 - train: epoch 093, train_loss: 0.9529
2022-07-30 17:33:58 - eval: epoch: 093, acc1: 74.130%, acc5: 91.858%, test_loss: 1.0568, per_image_load_time: 9.329ms, per_image_inference_time: 0.769ms
2022-07-30 17:33:59 - until epoch: 093, best_acc1: 74.130%
2022-07-30 17:33:59 - epoch 094 lr: 0.002667
2022-07-30 17:36:14 - train: epoch 0094, iter [00100, 05004], lr: 0.002652, loss: 0.8173
2022-07-30 17:38:12 - train: epoch 0094, iter [00200, 05004], lr: 0.002637, loss: 0.9169
2022-07-30 17:40:14 - train: epoch 0094, iter [00300, 05004], lr: 0.002622, loss: 1.0621
2022-07-30 17:42:14 - train: epoch 0094, iter [00400, 05004], lr: 0.002607, loss: 1.0576
2022-07-30 17:44:16 - train: epoch 0094, iter [00500, 05004], lr: 0.002592, loss: 0.8828
2022-07-30 17:46:18 - train: epoch 0094, iter [00600, 05004], lr: 0.002577, loss: 0.7991
2022-07-30 17:48:21 - train: epoch 0094, iter [00700, 05004], lr: 0.002562, loss: 0.8437
2022-07-30 17:50:18 - train: epoch 0094, iter [00800, 05004], lr: 0.002547, loss: 0.7796
2022-07-30 17:52:21 - train: epoch 0094, iter [00900, 05004], lr: 0.002533, loss: 0.9027
2022-07-30 17:54:23 - train: epoch 0094, iter [01000, 05004], lr: 0.002518, loss: 0.9188
2022-07-30 17:56:22 - train: epoch 0094, iter [01100, 05004], lr: 0.002503, loss: 0.9857
2022-07-30 17:58:26 - train: epoch 0094, iter [01200, 05004], lr: 0.002488, loss: 0.9752
2022-07-30 18:00:29 - train: epoch 0094, iter [01300, 05004], lr: 0.002474, loss: 0.8802
2022-07-30 18:02:29 - train: epoch 0094, iter [01400, 05004], lr: 0.002459, loss: 1.0285
2022-07-30 18:04:25 - train: epoch 0094, iter [01500, 05004], lr: 0.002445, loss: 1.1255
2022-07-30 18:06:27 - train: epoch 0094, iter [01600, 05004], lr: 0.002430, loss: 1.2507
2022-07-30 18:08:26 - train: epoch 0094, iter [01700, 05004], lr: 0.002416, loss: 0.7875
2022-07-30 18:10:31 - train: epoch 0094, iter [01800, 05004], lr: 0.002401, loss: 0.8699
2022-07-30 18:12:32 - train: epoch 0094, iter [01900, 05004], lr: 0.002387, loss: 0.8914
2022-07-30 18:14:33 - train: epoch 0094, iter [02000, 05004], lr: 0.002373, loss: 0.7085
2022-07-30 18:16:32 - train: epoch 0094, iter [02100, 05004], lr: 0.002358, loss: 0.8359
2022-07-30 18:18:31 - train: epoch 0094, iter [02200, 05004], lr: 0.002344, loss: 1.0318
2022-07-30 18:20:33 - train: epoch 0094, iter [02300, 05004], lr: 0.002330, loss: 0.8969
2022-07-30 18:22:40 - train: epoch 0094, iter [02400, 05004], lr: 0.002316, loss: 0.9597
2022-07-30 18:24:38 - train: epoch 0094, iter [02500, 05004], lr: 0.002302, loss: 0.9780
2022-07-30 18:26:39 - train: epoch 0094, iter [02600, 05004], lr: 0.002288, loss: 1.0075
2022-07-30 18:28:36 - train: epoch 0094, iter [02700, 05004], lr: 0.002273, loss: 0.8004
2022-07-30 18:30:36 - train: epoch 0094, iter [02800, 05004], lr: 0.002260, loss: 1.0738
2022-07-30 18:32:36 - train: epoch 0094, iter [02900, 05004], lr: 0.002246, loss: 0.8804
2022-07-30 18:34:39 - train: epoch 0094, iter [03000, 05004], lr: 0.002232, loss: 1.0701
2022-07-30 18:36:36 - train: epoch 0094, iter [03100, 05004], lr: 0.002218, loss: 0.9054
2022-07-30 18:38:36 - train: epoch 0094, iter [03200, 05004], lr: 0.002204, loss: 0.9735
2022-07-30 18:40:39 - train: epoch 0094, iter [03300, 05004], lr: 0.002190, loss: 1.0383
2022-07-30 18:42:38 - train: epoch 0094, iter [03400, 05004], lr: 0.002176, loss: 0.8483
2022-07-30 18:44:38 - train: epoch 0094, iter [03500, 05004], lr: 0.002163, loss: 1.0685
2022-07-30 18:46:37 - train: epoch 0094, iter [03600, 05004], lr: 0.002149, loss: 0.8218
2022-07-30 18:48:40 - train: epoch 0094, iter [03700, 05004], lr: 0.002136, loss: 1.0095
2022-07-30 18:50:41 - train: epoch 0094, iter [03800, 05004], lr: 0.002122, loss: 0.8092
2022-07-30 18:52:44 - train: epoch 0094, iter [03900, 05004], lr: 0.002108, loss: 0.7457
2022-07-30 18:54:49 - train: epoch 0094, iter [04000, 05004], lr: 0.002095, loss: 1.1248
2022-07-30 18:56:47 - train: epoch 0094, iter [04100, 05004], lr: 0.002082, loss: 1.0239
2022-07-30 18:58:46 - train: epoch 0094, iter [04200, 05004], lr: 0.002068, loss: 0.7976
2022-07-30 19:00:50 - train: epoch 0094, iter [04300, 05004], lr: 0.002055, loss: 0.9027
2022-07-30 19:02:52 - train: epoch 0094, iter [04400, 05004], lr: 0.002041, loss: 0.8715
2022-07-30 19:04:50 - train: epoch 0094, iter [04500, 05004], lr: 0.002028, loss: 0.9511
2022-07-30 19:06:48 - train: epoch 0094, iter [04600, 05004], lr: 0.002015, loss: 1.0883
2022-07-30 19:08:46 - train: epoch 0094, iter [04700, 05004], lr: 0.002002, loss: 1.0481
2022-07-30 19:10:42 - train: epoch 0094, iter [04800, 05004], lr: 0.001989, loss: 0.7161
2022-07-30 19:12:49 - train: epoch 0094, iter [04900, 05004], lr: 0.001976, loss: 0.9683
2022-07-30 19:14:41 - train: epoch 0094, iter [05000, 05004], lr: 0.001963, loss: 0.9649
2022-07-30 19:14:44 - train: epoch 094, train_loss: 0.9370
2022-07-30 19:19:07 - eval: epoch: 094, acc1: 74.282%, acc5: 91.926%, test_loss: 1.0515, per_image_load_time: 9.377ms, per_image_inference_time: 0.683ms
2022-07-30 19:19:08 - until epoch: 094, best_acc1: 74.282%
2022-07-30 19:19:08 - epoch 095 lr: 0.001962
2022-07-30 19:21:29 - train: epoch 0095, iter [00100, 05004], lr: 0.001949, loss: 0.8350
2022-07-30 19:23:29 - train: epoch 0095, iter [00200, 05004], lr: 0.001936, loss: 0.9043
2022-07-30 19:25:31 - train: epoch 0095, iter [00300, 05004], lr: 0.001923, loss: 0.8614
2022-07-30 19:27:31 - train: epoch 0095, iter [00400, 05004], lr: 0.001910, loss: 0.9613
2022-07-30 19:29:26 - train: epoch 0095, iter [00500, 05004], lr: 0.001897, loss: 0.8278
2022-07-30 19:31:18 - train: epoch 0095, iter [00600, 05004], lr: 0.001885, loss: 0.7701
2022-07-30 19:33:10 - train: epoch 0095, iter [00700, 05004], lr: 0.001872, loss: 0.9937
2022-07-30 19:35:01 - train: epoch 0095, iter [00800, 05004], lr: 0.001859, loss: 0.8619
2022-07-30 19:36:53 - train: epoch 0095, iter [00900, 05004], lr: 0.001846, loss: 0.9170
2022-07-30 19:38:46 - train: epoch 0095, iter [01000, 05004], lr: 0.001834, loss: 1.0552
2022-07-30 19:40:33 - train: epoch 0095, iter [01100, 05004], lr: 0.001821, loss: 0.6968
2022-07-30 19:42:29 - train: epoch 0095, iter [01200, 05004], lr: 0.001809, loss: 0.9562
2022-07-30 19:44:24 - train: epoch 0095, iter [01300, 05004], lr: 0.001796, loss: 0.9215
2022-07-30 19:46:13 - train: epoch 0095, iter [01400, 05004], lr: 0.001784, loss: 0.8625
2022-07-30 19:48:05 - train: epoch 0095, iter [01500, 05004], lr: 0.001771, loss: 0.9299
2022-07-30 19:49:59 - train: epoch 0095, iter [01600, 05004], lr: 0.001759, loss: 0.6458
2022-07-30 19:51:50 - train: epoch 0095, iter [01700, 05004], lr: 0.001747, loss: 0.9045
2022-07-30 19:53:45 - train: epoch 0095, iter [01800, 05004], lr: 0.001734, loss: 1.1255
2022-07-30 19:55:38 - train: epoch 0095, iter [01900, 05004], lr: 0.001722, loss: 0.9137
2022-07-30 19:57:29 - train: epoch 0095, iter [02000, 05004], lr: 0.001710, loss: 0.9630
2022-07-30 19:59:23 - train: epoch 0095, iter [02100, 05004], lr: 0.001698, loss: 0.8143
2022-07-30 20:01:15 - train: epoch 0095, iter [02200, 05004], lr: 0.001686, loss: 0.7586
2022-07-30 20:03:10 - train: epoch 0095, iter [02300, 05004], lr: 0.001674, loss: 0.9013
2022-07-30 20:05:05 - train: epoch 0095, iter [02400, 05004], lr: 0.001662, loss: 0.8939
2022-07-30 20:06:57 - train: epoch 0095, iter [02500, 05004], lr: 0.001650, loss: 0.8046
2022-07-30 20:08:51 - train: epoch 0095, iter [02600, 05004], lr: 0.001638, loss: 0.7804
2022-07-30 20:10:45 - train: epoch 0095, iter [02700, 05004], lr: 0.001626, loss: 1.0490
2022-07-30 20:12:34 - train: epoch 0095, iter [02800, 05004], lr: 0.001614, loss: 0.7255
2022-07-30 20:14:34 - train: epoch 0095, iter [02900, 05004], lr: 0.001602, loss: 0.9040
2022-07-30 20:16:25 - train: epoch 0095, iter [03000, 05004], lr: 0.001590, loss: 1.1477
2022-07-30 20:18:18 - train: epoch 0095, iter [03100, 05004], lr: 0.001579, loss: 0.8144
2022-07-30 20:20:08 - train: epoch 0095, iter [03200, 05004], lr: 0.001567, loss: 1.0404
2022-07-30 20:22:05 - train: epoch 0095, iter [03300, 05004], lr: 0.001555, loss: 0.9949
2022-07-30 20:23:58 - train: epoch 0095, iter [03400, 05004], lr: 0.001544, loss: 0.8715
2022-07-30 20:25:59 - train: epoch 0095, iter [03500, 05004], lr: 0.001532, loss: 0.9424
2022-07-30 20:27:53 - train: epoch 0095, iter [03600, 05004], lr: 0.001521, loss: 0.7874
2022-07-30 20:29:53 - train: epoch 0095, iter [03700, 05004], lr: 0.001509, loss: 1.0062
2022-07-30 20:31:54 - train: epoch 0095, iter [03800, 05004], lr: 0.001498, loss: 0.7991
2022-07-30 20:33:50 - train: epoch 0095, iter [03900, 05004], lr: 0.001487, loss: 0.9928
2022-07-30 20:35:48 - train: epoch 0095, iter [04000, 05004], lr: 0.001475, loss: 0.7882
2022-07-30 20:37:46 - train: epoch 0095, iter [04100, 05004], lr: 0.001464, loss: 1.0744
2022-07-30 20:39:45 - train: epoch 0095, iter [04200, 05004], lr: 0.001453, loss: 0.9372
2022-07-30 20:41:48 - train: epoch 0095, iter [04300, 05004], lr: 0.001442, loss: 0.9243
2022-07-30 20:43:40 - train: epoch 0095, iter [04400, 05004], lr: 0.001430, loss: 0.8612
2022-07-30 20:45:45 - train: epoch 0095, iter [04500, 05004], lr: 0.001419, loss: 0.8362
2022-07-30 20:47:41 - train: epoch 0095, iter [04600, 05004], lr: 0.001408, loss: 1.0175
2022-07-30 20:49:43 - train: epoch 0095, iter [04700, 05004], lr: 0.001397, loss: 0.8088
2022-07-30 20:51:37 - train: epoch 0095, iter [04800, 05004], lr: 0.001386, loss: 0.8115
2022-07-30 20:53:37 - train: epoch 0095, iter [04900, 05004], lr: 0.001375, loss: 0.7886
2022-07-30 20:55:32 - train: epoch 0095, iter [05000, 05004], lr: 0.001364, loss: 0.7022
2022-07-30 20:55:37 - train: epoch 095, train_loss: 0.9211
2022-07-30 20:59:56 - eval: epoch: 095, acc1: 74.282%, acc5: 92.004%, test_loss: 1.0467, per_image_load_time: 8.052ms, per_image_inference_time: 0.736ms
2022-07-30 20:59:56 - until epoch: 095, best_acc1: 74.282%
2022-07-30 20:59:56 - epoch 096 lr: 0.001364
2022-07-30 21:02:03 - train: epoch 0096, iter [00100, 05004], lr: 0.001353, loss: 0.9593
2022-07-30 21:04:00 - train: epoch 0096, iter [00200, 05004], lr: 0.001342, loss: 0.9785
2022-07-30 21:05:55 - train: epoch 0096, iter [00300, 05004], lr: 0.001331, loss: 1.0444
2022-07-30 21:07:55 - train: epoch 0096, iter [00400, 05004], lr: 0.001321, loss: 0.6730
2022-07-30 21:09:52 - train: epoch 0096, iter [00500, 05004], lr: 0.001310, loss: 0.9609
2022-07-30 21:11:47 - train: epoch 0096, iter [00600, 05004], lr: 0.001299, loss: 0.9828
2022-07-30 21:13:43 - train: epoch 0096, iter [00700, 05004], lr: 0.001289, loss: 0.8137
2022-07-30 21:15:42 - train: epoch 0096, iter [00800, 05004], lr: 0.001278, loss: 0.8725
2022-07-30 21:17:41 - train: epoch 0096, iter [00900, 05004], lr: 0.001268, loss: 0.9336
2022-07-30 21:19:37 - train: epoch 0096, iter [01000, 05004], lr: 0.001257, loss: 0.8869
2022-07-30 21:21:35 - train: epoch 0096, iter [01100, 05004], lr: 0.001247, loss: 0.9806
2022-07-30 21:23:34 - train: epoch 0096, iter [01200, 05004], lr: 0.001236, loss: 0.8439
2022-07-30 21:25:27 - train: epoch 0096, iter [01300, 05004], lr: 0.001226, loss: 0.9943
2022-07-30 21:27:26 - train: epoch 0096, iter [01400, 05004], lr: 0.001216, loss: 0.9165
2022-07-30 21:29:25 - train: epoch 0096, iter [01500, 05004], lr: 0.001206, loss: 0.8992
2022-07-30 21:31:20 - train: epoch 0096, iter [01600, 05004], lr: 0.001195, loss: 0.7016
2022-07-30 21:33:21 - train: epoch 0096, iter [01700, 05004], lr: 0.001185, loss: 0.7336
2022-07-30 21:35:25 - train: epoch 0096, iter [01800, 05004], lr: 0.001175, loss: 0.9710
2022-07-30 21:37:22 - train: epoch 0096, iter [01900, 05004], lr: 0.001165, loss: 1.0930
2022-07-30 21:39:19 - train: epoch 0096, iter [02000, 05004], lr: 0.001155, loss: 0.7751
2022-07-30 21:41:15 - train: epoch 0096, iter [02100, 05004], lr: 0.001145, loss: 0.7174
2022-07-30 21:43:15 - train: epoch 0096, iter [02200, 05004], lr: 0.001135, loss: 0.7338
2022-07-30 21:45:10 - train: epoch 0096, iter [02300, 05004], lr: 0.001125, loss: 0.8372
2022-07-30 21:47:09 - train: epoch 0096, iter [02400, 05004], lr: 0.001115, loss: 0.8432
2022-07-30 21:49:02 - train: epoch 0096, iter [02500, 05004], lr: 0.001105, loss: 0.8606
2022-07-30 21:51:03 - train: epoch 0096, iter [02600, 05004], lr: 0.001096, loss: 0.8982
2022-07-30 21:52:58 - train: epoch 0096, iter [02700, 05004], lr: 0.001086, loss: 1.0327
2022-07-30 21:55:01 - train: epoch 0096, iter [02800, 05004], lr: 0.001076, loss: 1.0648
2022-07-30 21:57:02 - train: epoch 0096, iter [02900, 05004], lr: 0.001067, loss: 1.0745
2022-07-30 21:59:02 - train: epoch 0096, iter [03000, 05004], lr: 0.001057, loss: 0.8413
2022-07-30 22:01:00 - train: epoch 0096, iter [03100, 05004], lr: 0.001047, loss: 0.9636
2022-07-30 22:03:02 - train: epoch 0096, iter [03200, 05004], lr: 0.001038, loss: 0.9599
2022-07-30 22:04:59 - train: epoch 0096, iter [03300, 05004], lr: 0.001028, loss: 0.9235
2022-07-30 22:06:54 - train: epoch 0096, iter [03400, 05004], lr: 0.001019, loss: 0.8869
2022-07-30 22:08:49 - train: epoch 0096, iter [03500, 05004], lr: 0.001010, loss: 0.8464
2022-07-30 22:10:51 - train: epoch 0096, iter [03600, 05004], lr: 0.001000, loss: 0.9467
2022-07-30 22:12:50 - train: epoch 0096, iter [03700, 05004], lr: 0.000991, loss: 0.9111
2022-07-30 22:14:48 - train: epoch 0096, iter [03800, 05004], lr: 0.000982, loss: 0.8570
2022-07-30 22:16:40 - train: epoch 0096, iter [03900, 05004], lr: 0.000972, loss: 0.8382
2022-07-30 22:18:42 - train: epoch 0096, iter [04000, 05004], lr: 0.000963, loss: 1.0368
2022-07-30 22:20:37 - train: epoch 0096, iter [04100, 05004], lr: 0.000954, loss: 0.8418
2022-07-30 22:22:36 - train: epoch 0096, iter [04200, 05004], lr: 0.000945, loss: 0.7434
2022-07-30 22:24:32 - train: epoch 0096, iter [04300, 05004], lr: 0.000936, loss: 0.9447
2022-07-30 22:26:29 - train: epoch 0096, iter [04400, 05004], lr: 0.000927, loss: 0.7537
2022-07-30 22:28:32 - train: epoch 0096, iter [04500, 05004], lr: 0.000918, loss: 1.0339
2022-07-30 22:30:29 - train: epoch 0096, iter [04600, 05004], lr: 0.000909, loss: 0.7743
2022-07-30 22:32:25 - train: epoch 0096, iter [04700, 05004], lr: 0.000900, loss: 0.9041
2022-07-30 22:34:27 - train: epoch 0096, iter [04800, 05004], lr: 0.000891, loss: 0.9181
2022-07-30 22:36:25 - train: epoch 0096, iter [04900, 05004], lr: 0.000883, loss: 0.9939
2022-07-30 22:38:18 - train: epoch 0096, iter [05000, 05004], lr: 0.000874, loss: 0.9027
2022-07-30 22:38:22 - train: epoch 096, train_loss: 0.9088
2022-07-30 22:42:28 - eval: epoch: 096, acc1: 74.428%, acc5: 92.022%, test_loss: 1.0433, per_image_load_time: 5.296ms, per_image_inference_time: 0.762ms
2022-07-30 22:42:29 - until epoch: 096, best_acc1: 74.428%
2022-07-30 22:42:29 - epoch 097 lr: 0.000874
2022-07-30 22:44:38 - train: epoch 0097, iter [00100, 05004], lr: 0.000865, loss: 0.9805
2022-07-30 22:46:41 - train: epoch 0097, iter [00200, 05004], lr: 0.000856, loss: 0.7594
2022-07-30 22:48:34 - train: epoch 0097, iter [00300, 05004], lr: 0.000848, loss: 0.8119
2022-07-30 22:50:39 - train: epoch 0097, iter [00400, 05004], lr: 0.000839, loss: 0.9795
2022-07-30 22:52:36 - train: epoch 0097, iter [00500, 05004], lr: 0.000831, loss: 0.9468
2022-07-30 22:54:42 - train: epoch 0097, iter [00600, 05004], lr: 0.000822, loss: 0.9926
2022-07-30 22:56:38 - train: epoch 0097, iter [00700, 05004], lr: 0.000814, loss: 0.9690
2022-07-30 22:58:38 - train: epoch 0097, iter [00800, 05004], lr: 0.000805, loss: 0.7112
2022-07-30 23:00:32 - train: epoch 0097, iter [00900, 05004], lr: 0.000797, loss: 0.8783
2022-07-30 23:02:26 - train: epoch 0097, iter [01000, 05004], lr: 0.000789, loss: 0.8447
2022-07-30 23:04:27 - train: epoch 0097, iter [01100, 05004], lr: 0.000780, loss: 0.8689
2022-07-30 23:06:17 - train: epoch 0097, iter [01200, 05004], lr: 0.000772, loss: 0.9677
2022-07-30 23:08:13 - train: epoch 0097, iter [01300, 05004], lr: 0.000764, loss: 0.9087
2022-07-30 23:10:14 - train: epoch 0097, iter [01400, 05004], lr: 0.000756, loss: 0.8463
2022-07-30 23:12:11 - train: epoch 0097, iter [01500, 05004], lr: 0.000748, loss: 0.9412
2022-07-30 23:14:09 - train: epoch 0097, iter [01600, 05004], lr: 0.000740, loss: 0.8656
2022-07-30 23:16:03 - train: epoch 0097, iter [01700, 05004], lr: 0.000732, loss: 0.8164
2022-07-30 23:18:01 - train: epoch 0097, iter [01800, 05004], lr: 0.000724, loss: 0.9031
2022-07-30 23:19:57 - train: epoch 0097, iter [01900, 05004], lr: 0.000716, loss: 0.8706
2022-07-30 23:21:52 - train: epoch 0097, iter [02000, 05004], lr: 0.000708, loss: 0.8145
2022-07-30 23:23:50 - train: epoch 0097, iter [02100, 05004], lr: 0.000700, loss: 0.8644
2022-07-30 23:25:59 - train: epoch 0097, iter [02200, 05004], lr: 0.000692, loss: 0.8821
2022-07-30 23:28:08 - train: epoch 0097, iter [02300, 05004], lr: 0.000685, loss: 0.9249
2022-07-30 23:30:12 - train: epoch 0097, iter [02400, 05004], lr: 0.000677, loss: 0.8457
2022-07-30 23:32:18 - train: epoch 0097, iter [02500, 05004], lr: 0.000669, loss: 0.7713
2022-07-30 23:34:32 - train: epoch 0097, iter [02600, 05004], lr: 0.000662, loss: 0.8781
2022-07-30 23:36:42 - train: epoch 0097, iter [02700, 05004], lr: 0.000654, loss: 0.8098
2022-07-30 23:38:53 - train: epoch 0097, iter [02800, 05004], lr: 0.000647, loss: 0.8832
2022-07-30 23:41:02 - train: epoch 0097, iter [02900, 05004], lr: 0.000639, loss: 0.7709
2022-07-30 23:43:09 - train: epoch 0097, iter [03000, 05004], lr: 0.000632, loss: 1.0445
2022-07-30 23:45:13 - train: epoch 0097, iter [03100, 05004], lr: 0.000624, loss: 0.8694
2022-07-30 23:47:21 - train: epoch 0097, iter [03200, 05004], lr: 0.000617, loss: 0.8531
2022-07-30 23:49:29 - train: epoch 0097, iter [03300, 05004], lr: 0.000610, loss: 0.9864
2022-07-30 23:51:34 - train: epoch 0097, iter [03400, 05004], lr: 0.000602, loss: 0.9529
2022-07-30 23:53:41 - train: epoch 0097, iter [03500, 05004], lr: 0.000595, loss: 0.8074
2022-07-30 23:55:57 - train: epoch 0097, iter [03600, 05004], lr: 0.000588, loss: 0.7340
2022-07-30 23:58:01 - train: epoch 0097, iter [03700, 05004], lr: 0.000581, loss: 0.8036
2022-07-31 00:00:09 - train: epoch 0097, iter [03800, 05004], lr: 0.000574, loss: 0.8556
2022-07-31 00:02:14 - train: epoch 0097, iter [03900, 05004], lr: 0.000567, loss: 0.9054
2022-07-31 00:04:20 - train: epoch 0097, iter [04000, 05004], lr: 0.000560, loss: 0.8450
2022-07-31 00:06:25 - train: epoch 0097, iter [04100, 05004], lr: 0.000553, loss: 0.9887
2022-07-31 00:08:33 - train: epoch 0097, iter [04200, 05004], lr: 0.000546, loss: 0.8036
2022-07-31 00:10:40 - train: epoch 0097, iter [04300, 05004], lr: 0.000539, loss: 0.9499
2022-07-31 00:12:49 - train: epoch 0097, iter [04400, 05004], lr: 0.000532, loss: 0.9313
2022-07-31 00:14:57 - train: epoch 0097, iter [04500, 05004], lr: 0.000525, loss: 0.9921
2022-07-31 00:17:00 - train: epoch 0097, iter [04600, 05004], lr: 0.000519, loss: 0.8638
2022-07-31 00:19:11 - train: epoch 0097, iter [04700, 05004], lr: 0.000512, loss: 0.8291
2022-07-31 00:21:22 - train: epoch 0097, iter [04800, 05004], lr: 0.000505, loss: 0.8864
2022-07-31 00:23:30 - train: epoch 0097, iter [04900, 05004], lr: 0.000499, loss: 0.9894
2022-07-31 00:25:18 - train: epoch 0097, iter [05000, 05004], lr: 0.000492, loss: 0.9091
2022-07-31 00:25:22 - train: epoch 097, train_loss: 0.8990
2022-07-31 00:29:21 - eval: epoch: 097, acc1: 74.520%, acc5: 92.034%, test_loss: 1.0393, per_image_load_time: 7.981ms, per_image_inference_time: 0.833ms
2022-07-31 00:29:21 - until epoch: 097, best_acc1: 74.520%
2022-07-31 00:29:21 - epoch 098 lr: 0.000492
2022-07-31 00:31:55 - train: epoch 0098, iter [00100, 05004], lr: 0.000485, loss: 1.0659
2022-07-31 00:34:03 - train: epoch 0098, iter [00200, 05004], lr: 0.000479, loss: 0.8302
2022-07-31 00:36:21 - train: epoch 0098, iter [00300, 05004], lr: 0.000472, loss: 0.9677
2022-07-31 00:38:32 - train: epoch 0098, iter [00400, 05004], lr: 0.000466, loss: 0.9629
2022-07-31 00:40:40 - train: epoch 0098, iter [00500, 05004], lr: 0.000460, loss: 0.8335
2022-07-31 00:42:47 - train: epoch 0098, iter [00600, 05004], lr: 0.000453, loss: 0.9966
2022-07-31 00:44:54 - train: epoch 0098, iter [00700, 05004], lr: 0.000447, loss: 0.9610
2022-07-31 00:47:03 - train: epoch 0098, iter [00800, 05004], lr: 0.000441, loss: 1.0638
2022-07-31 00:49:09 - train: epoch 0098, iter [00900, 05004], lr: 0.000435, loss: 1.0224
2022-07-31 00:51:16 - train: epoch 0098, iter [01000, 05004], lr: 0.000428, loss: 0.9235
2022-07-31 00:53:30 - train: epoch 0098, iter [01100, 05004], lr: 0.000422, loss: 0.9335
2022-07-31 00:55:37 - train: epoch 0098, iter [01200, 05004], lr: 0.000416, loss: 0.7600
2022-07-31 00:57:42 - train: epoch 0098, iter [01300, 05004], lr: 0.000410, loss: 0.9163
2022-07-31 00:59:44 - train: epoch 0098, iter [01400, 05004], lr: 0.000404, loss: 0.8940
2022-07-31 01:01:56 - train: epoch 0098, iter [01500, 05004], lr: 0.000398, loss: 0.8376
2022-07-31 01:04:00 - train: epoch 0098, iter [01600, 05004], lr: 0.000393, loss: 1.0843
2022-07-31 01:06:04 - train: epoch 0098, iter [01700, 05004], lr: 0.000387, loss: 0.9225
2022-07-31 01:08:07 - train: epoch 0098, iter [01800, 05004], lr: 0.000381, loss: 1.0543
2022-07-31 01:10:14 - train: epoch 0098, iter [01900, 05004], lr: 0.000375, loss: 0.7984
2022-07-31 01:12:18 - train: epoch 0098, iter [02000, 05004], lr: 0.000369, loss: 0.9040
2022-07-31 01:14:32 - train: epoch 0098, iter [02100, 05004], lr: 0.000364, loss: 0.8833
2022-07-31 01:16:38 - train: epoch 0098, iter [02200, 05004], lr: 0.000358, loss: 0.9001
2022-07-31 01:18:46 - train: epoch 0098, iter [02300, 05004], lr: 0.000353, loss: 0.9523
2022-07-31 01:20:53 - train: epoch 0098, iter [02400, 05004], lr: 0.000347, loss: 0.8982
2022-07-31 01:23:02 - train: epoch 0098, iter [02500, 05004], lr: 0.000342, loss: 0.9314
2022-07-31 01:25:04 - train: epoch 0098, iter [02600, 05004], lr: 0.000336, loss: 0.7977
2022-07-31 01:27:11 - train: epoch 0098, iter [02700, 05004], lr: 0.000331, loss: 0.9170
2022-07-31 01:29:15 - train: epoch 0098, iter [02800, 05004], lr: 0.000325, loss: 1.1166
2022-07-31 01:31:19 - train: epoch 0098, iter [02900, 05004], lr: 0.000320, loss: 0.8264
2022-07-31 01:33:26 - train: epoch 0098, iter [03000, 05004], lr: 0.000315, loss: 0.7997
2022-07-31 01:35:39 - train: epoch 0098, iter [03100, 05004], lr: 0.000310, loss: 0.8376
2022-07-31 01:37:39 - train: epoch 0098, iter [03200, 05004], lr: 0.000305, loss: 0.8864
2022-07-31 01:39:45 - train: epoch 0098, iter [03300, 05004], lr: 0.000299, loss: 0.8434
2022-07-31 01:41:54 - train: epoch 0098, iter [03400, 05004], lr: 0.000294, loss: 0.9927
2022-07-31 01:43:55 - train: epoch 0098, iter [03500, 05004], lr: 0.000289, loss: 0.8877
2022-07-31 01:45:53 - train: epoch 0098, iter [03600, 05004], lr: 0.000284, loss: 0.9423
2022-07-31 01:48:02 - train: epoch 0098, iter [03700, 05004], lr: 0.000279, loss: 0.9747
2022-07-31 01:50:04 - train: epoch 0098, iter [03800, 05004], lr: 0.000274, loss: 0.9524
2022-07-31 01:52:10 - train: epoch 0098, iter [03900, 05004], lr: 0.000270, loss: 0.7391
2022-07-31 01:54:08 - train: epoch 0098, iter [04000, 05004], lr: 0.000265, loss: 0.8860
2022-07-31 01:56:03 - train: epoch 0098, iter [04100, 05004], lr: 0.000260, loss: 0.8873
2022-07-31 01:58:03 - train: epoch 0098, iter [04200, 05004], lr: 0.000255, loss: 0.8481
2022-07-31 02:00:01 - train: epoch 0098, iter [04300, 05004], lr: 0.000250, loss: 0.6907
2022-07-31 02:01:59 - train: epoch 0098, iter [04400, 05004], lr: 0.000246, loss: 1.0466
2022-07-31 02:03:49 - train: epoch 0098, iter [04500, 05004], lr: 0.000241, loss: 0.8890
2022-07-31 02:05:48 - train: epoch 0098, iter [04600, 05004], lr: 0.000237, loss: 0.9711
2022-07-31 02:07:53 - train: epoch 0098, iter [04700, 05004], lr: 0.000232, loss: 0.8351
2022-07-31 02:09:55 - train: epoch 0098, iter [04800, 05004], lr: 0.000228, loss: 0.7781
2022-07-31 02:11:59 - train: epoch 0098, iter [04900, 05004], lr: 0.000223, loss: 0.8768
2022-07-31 02:14:05 - train: epoch 0098, iter [05000, 05004], lr: 0.000219, loss: 0.7807
2022-07-31 02:14:09 - train: epoch 098, train_loss: 0.8937
2022-07-31 02:18:45 - eval: epoch: 098, acc1: 74.488%, acc5: 92.144%, test_loss: 1.0404, per_image_load_time: 6.843ms, per_image_inference_time: 0.847ms
2022-07-31 02:18:46 - until epoch: 098, best_acc1: 74.520%
2022-07-31 02:18:46 - epoch 099 lr: 0.000219
2022-07-31 02:21:13 - train: epoch 0099, iter [00100, 05004], lr: 0.000214, loss: 1.1529
2022-07-31 02:23:19 - train: epoch 0099, iter [00200, 05004], lr: 0.000210, loss: 0.8281
2022-07-31 02:25:26 - train: epoch 0099, iter [00300, 05004], lr: 0.000206, loss: 0.8000
2022-07-31 02:27:30 - train: epoch 0099, iter [00400, 05004], lr: 0.000202, loss: 0.9728
2022-07-31 02:29:38 - train: epoch 0099, iter [00500, 05004], lr: 0.000197, loss: 0.8124
2022-07-31 02:31:36 - train: epoch 0099, iter [00600, 05004], lr: 0.000193, loss: 0.8159
2022-07-31 02:33:45 - train: epoch 0099, iter [00700, 05004], lr: 0.000189, loss: 0.9908
2022-07-31 02:35:50 - train: epoch 0099, iter [00800, 05004], lr: 0.000185, loss: 0.9061
2022-07-31 02:37:57 - train: epoch 0099, iter [00900, 05004], lr: 0.000181, loss: 0.9222
2022-07-31 02:40:00 - train: epoch 0099, iter [01000, 05004], lr: 0.000177, loss: 0.7598
2022-07-31 02:42:03 - train: epoch 0099, iter [01100, 05004], lr: 0.000173, loss: 0.8766
2022-07-31 02:44:12 - train: epoch 0099, iter [01200, 05004], lr: 0.000169, loss: 0.8292
2022-07-31 02:46:22 - train: epoch 0099, iter [01300, 05004], lr: 0.000166, loss: 0.8941
2022-07-31 02:48:32 - train: epoch 0099, iter [01400, 05004], lr: 0.000162, loss: 0.8686
2022-07-31 02:50:39 - train: epoch 0099, iter [01500, 05004], lr: 0.000158, loss: 0.8380
2022-07-31 02:52:38 - train: epoch 0099, iter [01600, 05004], lr: 0.000154, loss: 1.0208
2022-07-31 02:54:43 - train: epoch 0099, iter [01700, 05004], lr: 0.000151, loss: 0.9013
2022-07-31 02:56:49 - train: epoch 0099, iter [01800, 05004], lr: 0.000147, loss: 1.0675
2022-07-31 02:58:57 - train: epoch 0099, iter [01900, 05004], lr: 0.000144, loss: 0.8383
2022-07-31 03:01:01 - train: epoch 0099, iter [02000, 05004], lr: 0.000140, loss: 1.0457
2022-07-31 03:03:11 - train: epoch 0099, iter [02100, 05004], lr: 0.000137, loss: 0.8350
2022-07-31 03:05:17 - train: epoch 0099, iter [02200, 05004], lr: 0.000133, loss: 0.8373
2022-07-31 03:07:24 - train: epoch 0099, iter [02300, 05004], lr: 0.000130, loss: 0.8639
2022-07-31 03:09:29 - train: epoch 0099, iter [02400, 05004], lr: 0.000126, loss: 1.1712
2022-07-31 03:11:39 - train: epoch 0099, iter [02500, 05004], lr: 0.000123, loss: 0.8802
2022-07-31 03:13:50 - train: epoch 0099, iter [02600, 05004], lr: 0.000120, loss: 0.6842
2022-07-31 03:15:53 - train: epoch 0099, iter [02700, 05004], lr: 0.000117, loss: 0.8990
2022-07-31 03:17:56 - train: epoch 0099, iter [02800, 05004], lr: 0.000113, loss: 0.9120
2022-07-31 03:20:03 - train: epoch 0099, iter [02900, 05004], lr: 0.000110, loss: 0.8505
2022-07-31 03:22:03 - train: epoch 0099, iter [03000, 05004], lr: 0.000107, loss: 1.1000
2022-07-31 03:24:10 - train: epoch 0099, iter [03100, 05004], lr: 0.000104, loss: 0.6798
2022-07-31 03:26:12 - train: epoch 0099, iter [03200, 05004], lr: 0.000101, loss: 0.8715
2022-07-31 03:28:20 - train: epoch 0099, iter [03300, 05004], lr: 0.000098, loss: 0.7962
2022-07-31 03:30:20 - train: epoch 0099, iter [03400, 05004], lr: 0.000095, loss: 0.8672
2022-07-31 03:32:22 - train: epoch 0099, iter [03500, 05004], lr: 0.000092, loss: 0.9299
2022-07-31 03:34:30 - train: epoch 0099, iter [03600, 05004], lr: 0.000090, loss: 0.8290
2022-07-31 03:36:40 - train: epoch 0099, iter [03700, 05004], lr: 0.000087, loss: 0.7782
2022-07-31 03:38:47 - train: epoch 0099, iter [03800, 05004], lr: 0.000084, loss: 1.0325
2022-07-31 03:40:54 - train: epoch 0099, iter [03900, 05004], lr: 0.000081, loss: 0.9016
2022-07-31 03:42:56 - train: epoch 0099, iter [04000, 05004], lr: 0.000079, loss: 0.8478
2022-07-31 03:45:07 - train: epoch 0099, iter [04100, 05004], lr: 0.000076, loss: 0.8255
2022-07-31 03:47:09 - train: epoch 0099, iter [04200, 05004], lr: 0.000074, loss: 0.9529
2022-07-31 03:49:14 - train: epoch 0099, iter [04300, 05004], lr: 0.000071, loss: 0.8548
2022-07-31 03:51:19 - train: epoch 0099, iter [04400, 05004], lr: 0.000069, loss: 0.8335
2022-07-31 03:53:24 - train: epoch 0099, iter [04500, 05004], lr: 0.000066, loss: 0.9778
2022-07-31 03:55:23 - train: epoch 0099, iter [04600, 05004], lr: 0.000064, loss: 0.9718
2022-07-31 03:57:28 - train: epoch 0099, iter [04700, 05004], lr: 0.000062, loss: 0.8308
2022-07-31 03:59:27 - train: epoch 0099, iter [04800, 05004], lr: 0.000059, loss: 1.1460
2022-07-31 04:01:31 - train: epoch 0099, iter [04900, 05004], lr: 0.000057, loss: 0.9219
2022-07-31 04:03:38 - train: epoch 0099, iter [05000, 05004], lr: 0.000055, loss: 0.8161
2022-07-31 04:03:40 - train: epoch 099, train_loss: 0.8871
2022-07-31 04:08:26 - eval: epoch: 099, acc1: 74.502%, acc5: 92.094%, test_loss: 1.0390, per_image_load_time: 4.978ms, per_image_inference_time: 0.810ms
2022-07-31 04:08:27 - until epoch: 099, best_acc1: 74.520%
2022-07-31 04:08:27 - epoch 100 lr: 0.000055
2022-07-31 04:10:57 - train: epoch 0100, iter [00100, 05004], lr: 0.000053, loss: 0.9714
2022-07-31 04:13:11 - train: epoch 0100, iter [00200, 05004], lr: 0.000050, loss: 0.8485
2022-07-31 04:15:25 - train: epoch 0100, iter [00300, 05004], lr: 0.000048, loss: 0.8291
2022-07-31 04:17:28 - train: epoch 0100, iter [00400, 05004], lr: 0.000046, loss: 0.8199
2022-07-31 04:19:39 - train: epoch 0100, iter [00500, 05004], lr: 0.000044, loss: 1.0630
2022-07-31 04:21:44 - train: epoch 0100, iter [00600, 05004], lr: 0.000042, loss: 0.9874
2022-07-31 04:23:52 - train: epoch 0100, iter [00700, 05004], lr: 0.000040, loss: 0.8097
2022-07-31 04:26:02 - train: epoch 0100, iter [00800, 05004], lr: 0.000039, loss: 0.8344
2022-07-31 04:28:06 - train: epoch 0100, iter [00900, 05004], lr: 0.000037, loss: 0.8754
2022-07-31 04:30:14 - train: epoch 0100, iter [01000, 05004], lr: 0.000035, loss: 0.7559
2022-07-31 04:32:18 - train: epoch 0100, iter [01100, 05004], lr: 0.000033, loss: 0.7685
2022-07-31 04:34:24 - train: epoch 0100, iter [01200, 05004], lr: 0.000032, loss: 0.6939
2022-07-31 04:36:24 - train: epoch 0100, iter [01300, 05004], lr: 0.000030, loss: 0.9366
2022-07-31 04:38:31 - train: epoch 0100, iter [01400, 05004], lr: 0.000028, loss: 1.0378
2022-07-31 04:40:37 - train: epoch 0100, iter [01500, 05004], lr: 0.000027, loss: 1.0192
2022-07-31 04:42:44 - train: epoch 0100, iter [01600, 05004], lr: 0.000025, loss: 0.9066
2022-07-31 04:44:51 - train: epoch 0100, iter [01700, 05004], lr: 0.000024, loss: 0.7585
2022-07-31 04:46:58 - train: epoch 0100, iter [01800, 05004], lr: 0.000022, loss: 0.7703
2022-07-31 04:48:59 - train: epoch 0100, iter [01900, 05004], lr: 0.000021, loss: 0.9981
2022-07-31 04:51:07 - train: epoch 0100, iter [02000, 05004], lr: 0.000020, loss: 0.8668
2022-07-31 04:53:13 - train: epoch 0100, iter [02100, 05004], lr: 0.000018, loss: 0.8432
2022-07-31 04:55:23 - train: epoch 0100, iter [02200, 05004], lr: 0.000017, loss: 0.9746
2022-07-31 04:57:28 - train: epoch 0100, iter [02300, 05004], lr: 0.000016, loss: 0.8354
2022-07-31 04:59:34 - train: epoch 0100, iter [02400, 05004], lr: 0.000015, loss: 0.8151
2022-07-31 05:01:41 - train: epoch 0100, iter [02500, 05004], lr: 0.000014, loss: 0.8941
2022-07-31 05:03:41 - train: epoch 0100, iter [02600, 05004], lr: 0.000013, loss: 0.7597
2022-07-31 05:05:42 - train: epoch 0100, iter [02700, 05004], lr: 0.000012, loss: 0.8197
2022-07-31 05:07:43 - train: epoch 0100, iter [02800, 05004], lr: 0.000011, loss: 0.8246
2022-07-31 05:09:41 - train: epoch 0100, iter [02900, 05004], lr: 0.000010, loss: 0.9600
2022-07-31 05:11:42 - train: epoch 0100, iter [03000, 05004], lr: 0.000009, loss: 0.9010
2022-07-31 05:13:39 - train: epoch 0100, iter [03100, 05004], lr: 0.000008, loss: 0.7628
2022-07-31 05:15:38 - train: epoch 0100, iter [03200, 05004], lr: 0.000007, loss: 1.0413
2022-07-31 05:17:38 - train: epoch 0100, iter [03300, 05004], lr: 0.000006, loss: 1.0048
2022-07-31 05:19:43 - train: epoch 0100, iter [03400, 05004], lr: 0.000006, loss: 0.8938
2022-07-31 05:21:44 - train: epoch 0100, iter [03500, 05004], lr: 0.000005, loss: 0.7291
2022-07-31 05:23:42 - train: epoch 0100, iter [03600, 05004], lr: 0.000004, loss: 0.8181
2022-07-31 05:25:45 - train: epoch 0100, iter [03700, 05004], lr: 0.000004, loss: 0.8626
2022-07-31 05:27:51 - train: epoch 0100, iter [03800, 05004], lr: 0.000003, loss: 0.8272
2022-07-31 05:29:48 - train: epoch 0100, iter [03900, 05004], lr: 0.000003, loss: 1.0878
2022-07-31 05:31:50 - train: epoch 0100, iter [04000, 05004], lr: 0.000002, loss: 0.8401
2022-07-31 05:33:50 - train: epoch 0100, iter [04100, 05004], lr: 0.000002, loss: 1.0179
2022-07-31 05:35:46 - train: epoch 0100, iter [04200, 05004], lr: 0.000001, loss: 0.7431
2022-07-31 05:37:51 - train: epoch 0100, iter [04300, 05004], lr: 0.000001, loss: 1.0575
2022-07-31 05:39:54 - train: epoch 0100, iter [04400, 05004], lr: 0.000001, loss: 0.9766
2022-07-31 05:41:55 - train: epoch 0100, iter [04500, 05004], lr: 0.000001, loss: 0.8652
2022-07-31 05:43:57 - train: epoch 0100, iter [04600, 05004], lr: 0.000000, loss: 0.8872
2022-07-31 05:45:59 - train: epoch 0100, iter [04700, 05004], lr: 0.000000, loss: 0.9200
2022-07-31 05:48:00 - train: epoch 0100, iter [04800, 05004], lr: 0.000000, loss: 0.9188
2022-07-31 05:50:08 - train: epoch 0100, iter [04900, 05004], lr: 0.000000, loss: 0.8470
2022-07-31 05:52:04 - train: epoch 0100, iter [05000, 05004], lr: 0.000000, loss: 0.9699
2022-07-31 05:52:06 - train: epoch 100, train_loss: 0.8863
2022-07-31 05:56:30 - eval: epoch: 100, acc1: 74.482%, acc5: 92.056%, test_loss: 1.0398, per_image_load_time: 3.633ms, per_image_inference_time: 0.823ms
2022-07-31 05:56:30 - until epoch: 100, best_acc1: 74.520%
2022-07-31 05:56:30 - train done. model: RegNetX_800MF, train time: 166.772 hours, best_acc1: 74.520%
