2022-02-04 21:31:12 - network: resnet50
2022-02-04 21:31:12 - num_classes: 1000
2022-02-04 21:31:12 - input_image_size: 224
2022-02-04 21:31:12 - scale: 1.1428571428571428
2022-02-04 21:31:12 - trained_model_path: 
2022-02-04 21:31:12 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-04 21:31:12 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5c21ff4580>
2022-02-04 21:31:12 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5c21ff4850>
2022-02-04 21:31:12 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5c21ff4880>
2022-02-04 21:31:12 - seed: 0
2022-02-04 21:31:12 - batch_size: 256
2022-02-04 21:31:12 - num_workers: 16
2022-02-04 21:31:12 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-04 21:31:12 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-04 21:31:12 - epochs: 100
2022-02-04 21:31:12 - print_interval: 100
2022-02-04 21:31:12 - distributed: True
2022-02-04 21:31:12 - sync_bn: False
2022-02-04 21:31:12 - apex: True
2022-02-04 21:31:12 - gpus_type: NVIDIA RTX A5000
2022-02-04 21:31:12 - gpus_num: 2
2022-02-04 21:31:12 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5c00ed2270>
2022-02-04 21:31:16 - --------------------parameters--------------------
2022-02-04 21:31:16 - name: conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-02-04 21:31:16 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-02-04 21:31:16 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-02-04 21:31:16 - name: fc.weight, grad: True
2022-02-04 21:31:16 - name: fc.bias, grad: True
2022-02-04 21:31:16 - --------------------buffers--------------------
2022-02-04 21:31:16 - name: conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-04 21:31:16 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-02-04 21:31:16 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-02-04 21:31:16 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-01-28 23:52:00 - epoch 001 lr: 0.1
2022-01-28 23:52:39 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9065
2022-01-28 23:53:14 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.9048
2022-01-28 23:53:48 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.8952
2022-01-28 23:54:23 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.8715
2022-01-28 23:54:57 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.7673
2022-01-28 23:55:32 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.6241
2022-01-28 23:56:05 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.6512
2022-01-28 23:56:39 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.5660
2022-01-28 23:57:13 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.4316
2022-01-28 23:57:48 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.3669
2022-01-28 23:58:20 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.2373
2022-01-28 23:58:55 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 6.1196
2022-01-28 23:59:28 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 6.0990
2022-01-29 00:00:03 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 6.0058
2022-01-29 00:00:36 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.8831
2022-01-29 00:01:11 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.8960
2022-01-29 00:01:45 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.6459
2022-01-29 00:02:19 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.6989
2022-01-29 00:02:53 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.5863
2022-01-29 00:03:27 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.5216
2022-01-29 00:04:00 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.4425
2022-01-29 00:04:34 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.3856
2022-01-29 00:05:07 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 5.2839
2022-01-29 00:05:41 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 5.2094
2022-01-29 00:06:14 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.2439
2022-01-29 00:06:48 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.3052
2022-01-29 00:07:21 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.2433
2022-01-29 00:07:55 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 5.0648
2022-01-29 00:08:28 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.9267
2022-01-29 00:09:02 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.0494
2022-01-29 00:09:36 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.1011
2022-01-29 00:10:09 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 5.0097
2022-01-29 00:10:42 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.7794
2022-01-29 00:11:14 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.7505
2022-01-29 00:11:48 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.6809
2022-01-29 00:12:21 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.7841
2022-01-29 00:12:54 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.7893
2022-01-29 00:13:28 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.5763
2022-01-29 00:14:02 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.6732
2022-01-29 00:14:35 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.6463
2022-01-29 00:15:08 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.6533
2022-01-29 00:15:43 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.5122
2022-01-29 00:16:18 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.4140
2022-01-29 00:16:52 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.2682
2022-01-29 00:17:27 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.5004
2022-01-29 00:18:04 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.6425
2022-01-29 00:18:38 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.2701
2022-01-29 00:19:13 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.6074
2022-01-29 00:19:47 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.3340
2022-01-29 00:20:21 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.1549
2022-01-29 00:20:22 - train: epoch 001, train_loss: 5.3951
2022-01-29 00:21:40 - eval: epoch: 001, acc1: 14.216%, acc5: 33.014%, test_loss: 4.7258, per_image_load_time: 2.683ms, per_image_inference_time: 0.342ms
2022-01-29 00:21:40 - epoch 002 lr: 0.1
2022-01-29 00:22:20 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.2431
2022-01-29 00:22:52 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.0581
2022-01-29 00:23:24 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.3006
2022-01-29 00:24:00 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.2174
2022-01-29 00:24:33 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.9464
2022-01-29 00:25:05 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 3.9469
2022-01-29 00:25:37 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.1974
2022-01-29 00:26:10 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.8989
2022-01-29 00:26:42 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.6263
2022-01-29 00:27:16 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.1528
2022-01-29 00:27:48 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.0987
2022-01-29 00:28:21 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 3.9432
2022-01-29 00:28:53 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.9625
2022-01-29 00:29:26 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 3.9682
2022-01-29 00:29:58 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 3.8480
2022-01-29 00:30:31 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.7694
2022-01-29 00:31:03 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.7856
2022-01-29 00:31:36 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.9379
2022-01-29 00:32:09 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.6595
2022-01-29 00:32:41 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.4631
2022-01-29 00:33:15 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.7415
2022-01-29 00:33:49 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.4193
2022-01-29 00:34:21 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.6330
2022-01-29 00:34:54 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.4891
2022-01-29 00:35:29 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.5487
2022-01-29 00:36:03 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.4801
2022-01-29 00:36:37 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.7966
2022-01-29 00:37:09 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.6121
2022-01-29 00:37:43 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.5405
2022-01-29 00:38:14 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.3682
2022-01-29 00:38:48 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.4706
2022-01-29 00:39:21 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.5072
2022-01-29 00:39:53 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.4441
2022-01-29 00:40:26 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.6074
2022-01-29 00:40:59 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.4585
2022-01-29 00:41:31 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.5244
2022-01-29 00:42:04 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.5838
2022-01-29 00:42:38 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.1968
2022-01-29 00:43:16 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.4702
2022-01-29 00:43:49 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.3940
2022-01-29 00:44:26 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.4172
2022-01-29 00:44:58 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.3936
2022-01-29 00:45:32 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.3455
2022-01-29 00:46:04 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.1408
2022-01-29 00:46:39 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.2132
2022-01-29 00:47:12 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.1449
2022-01-29 00:47:44 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.3390
2022-01-29 00:48:18 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.3344
2022-01-29 00:48:51 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.2183
2022-01-29 00:49:23 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.1849
2022-01-29 00:49:24 - train: epoch 002, train_loss: 3.6591
2022-01-29 00:50:41 - eval: epoch: 002, acc1: 32.110%, acc5: 58.220%, test_loss: 3.6451, per_image_load_time: 2.670ms, per_image_inference_time: 0.348ms
2022-01-29 00:50:42 - epoch 003 lr: 0.1
2022-01-29 00:51:21 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.3578
2022-01-29 00:51:54 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.2528
2022-01-29 00:52:26 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.1503
2022-01-29 00:52:58 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.2374
2022-01-29 00:53:39 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.3656
2022-01-29 00:54:11 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.1316
2022-01-29 00:54:43 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.4218
2022-01-29 00:55:15 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.2563
2022-01-29 00:55:47 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.0550
2022-01-29 00:56:20 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.1403
2022-01-29 00:56:51 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.0861
2022-01-29 00:57:22 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.1531
2022-01-29 00:57:56 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.0824
2022-01-29 00:58:28 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.0417
2022-01-29 00:59:00 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.3449
2022-01-29 00:59:31 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 2.9999
2022-01-29 01:00:05 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.0785
2022-01-29 01:00:37 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 2.9620
2022-01-29 01:01:10 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.1388
2022-01-29 01:01:42 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.3453
2022-01-29 01:02:15 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.2914
2022-01-29 01:02:46 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.5757
2022-01-29 01:03:19 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.0368
2022-01-29 01:03:51 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 2.9620
2022-01-29 01:04:24 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.1093
2022-01-29 01:04:55 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.0797
2022-01-29 01:05:28 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.3992
2022-01-29 01:06:00 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.0134
2022-01-29 01:06:33 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.0698
2022-01-29 01:07:04 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.1307
2022-01-29 01:07:37 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.0986
2022-01-29 01:08:09 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.0218
2022-01-29 01:08:42 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.1024
2022-01-29 01:09:16 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.2256
2022-01-29 01:09:52 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.8298
2022-01-29 01:10:26 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 2.9199
2022-01-29 01:10:59 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 2.9023
2022-01-29 01:11:32 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.0661
2022-01-29 01:12:06 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.1393
2022-01-29 01:12:39 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.0030
2022-01-29 01:13:12 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.0343
2022-01-29 01:13:44 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.9297
2022-01-29 01:14:17 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.6960
2022-01-29 01:14:51 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.8568
2022-01-29 01:15:23 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 2.9901
2022-01-29 01:15:56 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 2.9772
2022-01-29 01:16:29 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.7972
2022-01-29 01:17:02 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.0017
2022-01-29 01:17:35 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.0731
2022-01-29 01:18:08 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 2.9766
2022-01-29 01:18:09 - train: epoch 003, train_loss: 3.0590
2022-01-29 01:19:28 - eval: epoch: 003, acc1: 39.320%, acc5: 66.304%, test_loss: 2.7399, per_image_load_time: 2.755ms, per_image_inference_time: 0.350ms
2022-01-29 01:19:29 - epoch 004 lr: 0.1
2022-01-29 01:20:08 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 2.8593
2022-01-29 01:20:39 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.8100
2022-01-29 01:21:12 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.8880
2022-01-29 01:21:45 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.7915
2022-01-29 01:22:17 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.6686
2022-01-29 01:22:49 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.0951
2022-01-29 01:23:22 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 2.9665
2022-01-29 01:23:54 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.7248
2022-01-29 01:24:26 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.6212
2022-01-29 01:24:58 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.8209
2022-01-29 01:25:30 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 2.9631
2022-01-29 01:26:02 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.6118
2022-01-29 01:26:33 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.6156
2022-01-29 01:27:06 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 2.8885
2022-01-29 01:27:38 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 2.8905
2022-01-29 01:28:11 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.6911
2022-01-29 01:28:42 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.9329
2022-01-29 01:29:14 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 2.9794
2022-01-29 01:29:45 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 2.8747
2022-01-29 01:30:18 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.7464
2022-01-29 01:30:50 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.8142
2022-01-29 01:31:22 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.7490
2022-01-29 01:31:53 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.7309
2022-01-29 01:32:24 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.6640
2022-01-29 01:32:56 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.8065
2022-01-29 01:33:27 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.9101
2022-01-29 01:33:59 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.6311
2022-01-29 01:34:31 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.7835
2022-01-29 01:35:03 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.6556
2022-01-29 01:35:35 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.6896
2022-01-29 01:36:07 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.7930
2022-01-29 01:36:39 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.7276
2022-01-29 01:37:11 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.9088
2022-01-29 01:37:42 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.8946
2022-01-29 01:38:15 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.6602
2022-01-29 01:38:47 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.5839
2022-01-29 01:39:19 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.7469
2022-01-29 01:39:52 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.6553
2022-01-29 01:40:23 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.7063
2022-01-29 01:40:56 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.4768
2022-01-29 01:41:28 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.7382
2022-01-29 01:42:00 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.5416
2022-01-29 01:42:33 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.5042
2022-01-29 01:43:05 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.5072
2022-01-29 01:43:37 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.1802
2022-01-29 01:44:09 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.7217
2022-01-29 01:44:41 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.5819
2022-01-29 01:45:13 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.5456
2022-01-29 01:45:46 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.7536
2022-01-29 01:46:17 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.8175
2022-01-29 01:46:18 - train: epoch 004, train_loss: 2.7861
2022-01-29 01:47:32 - eval: epoch: 004, acc1: 42.596%, acc5: 69.160%, test_loss: 2.5612, per_image_load_time: 2.557ms, per_image_inference_time: 0.339ms
2022-01-29 01:47:33 - epoch 005 lr: 0.1
2022-01-29 01:48:09 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.7141
2022-01-29 01:48:41 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.7910
2022-01-29 01:49:13 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.8247
2022-01-29 01:49:44 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.6986
2022-01-29 01:50:16 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.4777
2022-01-29 01:50:46 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.7695
2022-01-29 01:51:18 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.6785
2022-01-29 01:51:49 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 2.9823
2022-01-29 01:52:21 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.6462
2022-01-29 01:52:51 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.7832
2022-01-29 01:53:23 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.7179
2022-01-29 01:53:55 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.7074
2022-01-29 01:54:27 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.6229
2022-01-29 01:54:58 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.8029
2022-01-29 01:55:30 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.4237
2022-01-29 01:56:01 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.4920
2022-01-29 01:56:33 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.5356
2022-01-29 01:57:04 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.7291
2022-01-29 01:57:35 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.5041
2022-01-29 01:58:07 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.6396
2022-01-29 01:58:38 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.3957
2022-01-29 01:59:10 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.6005
2022-01-29 01:59:42 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.4317
2022-01-29 02:00:13 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.4959
2022-01-29 02:00:45 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.7047
2022-01-29 02:01:16 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.7408
2022-01-29 02:01:47 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.6858
2022-01-29 02:02:19 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.6202
2022-01-29 02:02:50 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.5664
2022-01-29 02:03:22 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.5433
2022-01-29 02:03:53 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.5655
2022-01-29 02:04:26 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.7247
2022-01-29 02:04:59 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.4139
2022-01-29 02:05:31 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.4945
2022-01-29 02:06:03 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.6580
2022-01-29 02:06:35 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.6672
2022-01-29 02:07:07 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.6276
2022-01-29 02:07:40 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.4442
2022-01-29 02:08:12 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.7808
2022-01-29 02:08:45 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.5623
2022-01-29 02:09:17 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.5300
2022-01-29 02:09:50 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.5992
2022-01-29 02:10:22 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.5007
2022-01-29 02:10:54 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.6404
2022-01-29 02:11:27 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.5222
2022-01-29 02:12:00 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.5153
2022-01-29 02:12:33 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.4043
2022-01-29 02:13:05 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.4548
2022-01-29 02:13:38 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.6095
2022-01-29 02:14:10 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.4322
2022-01-29 02:14:11 - train: epoch 005, train_loss: 2.6309
2022-01-29 02:15:29 - eval: epoch: 005, acc1: 44.838%, acc5: 71.580%, test_loss: 2.4189, per_image_load_time: 2.686ms, per_image_inference_time: 0.349ms
2022-01-29 02:15:30 - epoch 006 lr: 0.1
2022-01-29 02:16:07 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.4670
2022-01-29 02:16:39 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.6387
2022-01-29 02:17:13 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.3734
2022-01-29 02:17:44 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.7220
2022-01-29 02:18:17 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.4729
2022-01-29 02:18:49 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.5667
2022-01-29 02:19:22 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.6155
2022-01-29 02:19:55 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.5472
2022-01-29 02:20:27 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.5006
2022-01-29 02:20:59 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.4576
2022-01-29 02:21:32 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.4687
2022-01-29 02:22:04 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.6533
2022-01-29 02:22:37 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.6556
2022-01-29 02:23:10 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.7093
2022-01-29 02:23:43 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.6482
2022-01-29 02:24:15 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.4027
2022-01-29 02:24:47 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.7300
2022-01-29 02:25:19 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.7350
2022-01-29 02:25:53 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.5702
2022-01-29 02:26:26 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.6959
2022-01-29 02:26:59 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.5376
2022-01-29 02:27:31 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.4541
2022-01-29 02:28:05 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.3649
2022-01-29 02:28:36 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.5540
2022-01-29 02:29:08 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.7214
2022-01-29 02:29:41 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.4657
2022-01-29 02:30:14 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.5693
2022-01-29 02:30:47 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.3767
2022-01-29 02:31:19 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.5996
2022-01-29 02:31:52 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.4956
2022-01-29 02:32:24 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.2961
2022-01-29 02:32:56 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.4842
2022-01-29 02:33:28 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.3184
2022-01-29 02:34:01 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.7351
2022-01-29 02:34:33 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.4947
2022-01-29 02:35:06 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.5910
2022-01-29 02:35:39 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.5390
2022-01-29 02:36:11 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.3286
2022-01-29 02:36:44 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.3685
2022-01-29 02:37:18 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.7647
2022-01-29 02:37:52 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.4582
2022-01-29 02:38:25 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.3834
2022-01-29 02:38:58 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.4608
2022-01-29 02:39:33 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.4044
2022-01-29 02:40:06 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.5110
2022-01-29 02:40:40 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.4613
2022-01-29 02:41:15 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.4996
2022-01-29 02:41:47 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.5529
2022-01-29 02:42:22 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.5081
2022-01-29 02:42:54 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.3876
2022-01-29 02:42:55 - train: epoch 006, train_loss: 2.5320
2022-01-29 02:44:11 - eval: epoch: 006, acc1: 45.072%, acc5: 71.402%, test_loss: 2.4302, per_image_load_time: 2.609ms, per_image_inference_time: 0.353ms
2022-01-29 02:44:12 - epoch 007 lr: 0.1
2022-01-29 02:44:50 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.3864
2022-01-29 02:45:23 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.6042
2022-01-29 02:45:57 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.6830
2022-01-29 02:46:29 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.5505
2022-01-29 02:47:02 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.3724
2022-01-29 02:47:34 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.5842
2022-01-29 02:48:07 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.4231
2022-01-29 02:48:40 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.4579
2022-01-29 02:49:13 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.4223
2022-01-29 02:49:46 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.5047
2022-01-29 02:50:19 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.2873
2022-01-29 02:50:51 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.4847
2022-01-29 02:51:23 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.4488
2022-01-29 02:52:00 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.5145
2022-01-29 02:52:32 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.5663
2022-01-29 02:53:05 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.3855
2022-01-29 02:53:38 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.5462
2022-01-29 02:54:10 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.3885
2022-01-29 02:54:42 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.5708
2022-01-29 02:55:15 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.3436
2022-01-29 02:55:48 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.5427
2022-01-29 02:56:20 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.3126
2022-01-29 02:56:54 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.5504
2022-01-29 02:57:25 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.5735
2022-01-29 02:57:59 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.3983
2022-01-29 02:58:31 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.4438
2022-01-29 02:59:03 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.3790
2022-01-29 02:59:35 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.3799
2022-01-29 03:00:08 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.2903
2022-01-29 03:00:38 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.4193
2022-01-29 03:01:10 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.3286
2022-01-29 03:01:43 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.3823
2022-01-29 03:02:14 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.6966
2022-01-29 03:02:46 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.2078
2022-01-29 03:03:20 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.6742
2022-01-29 03:03:54 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.2950
2022-01-29 03:04:26 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.4426
2022-01-29 03:04:59 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.6135
2022-01-29 03:05:32 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.2253
2022-01-29 03:06:05 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.4788
2022-01-29 03:06:39 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.3636
2022-01-29 03:07:12 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.1845
2022-01-29 03:07:46 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.5465
2022-01-29 03:08:19 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.2372
2022-01-29 03:08:53 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.6494
2022-01-29 03:09:26 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.4372
2022-01-29 03:10:01 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.6220
2022-01-29 03:10:35 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.6865
2022-01-29 03:11:11 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.3994
2022-01-29 03:11:45 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.3882
2022-01-29 03:11:46 - train: epoch 007, train_loss: 2.4575
2022-01-29 03:13:06 - eval: epoch: 007, acc1: 47.440%, acc5: 74.182%, test_loss: 2.2751, per_image_load_time: 2.808ms, per_image_inference_time: 0.345ms
2022-01-29 03:13:07 - epoch 008 lr: 0.1
2022-01-29 03:13:47 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.4366
2022-01-29 03:14:20 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.5174
2022-01-29 03:14:53 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.2602
2022-01-29 03:15:26 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.3272
2022-01-29 03:15:58 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.3537
2022-01-29 03:16:31 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.4394
2022-01-29 03:17:04 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.7486
2022-01-29 03:17:37 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.2388
2022-01-29 03:18:09 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.4008
2022-01-29 03:18:42 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.3762
2022-01-29 03:19:15 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.2204
2022-01-29 03:19:47 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.2117
2022-01-29 03:20:19 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.5006
2022-01-29 03:20:51 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.4062
2022-01-29 03:21:24 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.4403
2022-01-29 03:21:57 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.4218
2022-01-29 03:22:29 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.3571
2022-01-29 03:23:02 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.5401
2022-01-29 03:23:35 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.1821
2022-01-29 03:24:07 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.5249
2022-01-29 03:24:41 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.4530
2022-01-29 03:25:14 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.3229
2022-01-29 03:25:47 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.4269
2022-01-29 03:26:19 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.3542
2022-01-29 03:26:51 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.4590
2022-01-29 03:27:24 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.4808
2022-01-29 03:27:58 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.5111
2022-01-29 03:28:31 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.5163
2022-01-29 03:29:02 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.4998
2022-01-29 03:29:35 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.5374
2022-01-29 03:30:08 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.3925
2022-01-29 03:30:40 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.6246
2022-01-29 03:31:12 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.5505
2022-01-29 03:31:45 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.4949
2022-01-29 03:32:17 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.3998
2022-01-29 03:32:51 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.5320
2022-01-29 03:33:23 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.3406
2022-01-29 03:33:57 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.3639
2022-01-29 03:34:29 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.4967
2022-01-29 03:35:02 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.6529
2022-01-29 03:35:34 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.3890
2022-01-29 03:36:07 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.3669
2022-01-29 03:36:39 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.1118
2022-01-29 03:37:12 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.3680
2022-01-29 03:37:44 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.5434
2022-01-29 03:38:16 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.4619
2022-01-29 03:38:50 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.2886
2022-01-29 03:39:23 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.5029
2022-01-29 03:39:56 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.4871
2022-01-29 03:40:27 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.3400
2022-01-29 03:40:29 - train: epoch 008, train_loss: 2.4037
2022-01-29 03:41:46 - eval: epoch: 008, acc1: 48.176%, acc5: 74.980%, test_loss: 2.2347, per_image_load_time: 2.661ms, per_image_inference_time: 0.359ms
2022-01-29 03:41:47 - epoch 009 lr: 0.1
2022-01-29 03:42:25 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.0893
2022-01-29 03:42:58 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.1946
2022-01-29 03:43:30 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.1062
2022-01-29 03:44:03 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.5524
2022-01-29 03:44:36 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.4217
2022-01-29 03:45:07 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.4174
2022-01-29 03:45:40 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.2574
2022-01-29 03:46:12 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.1887
2022-01-29 03:46:46 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.2538
2022-01-29 03:47:19 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.2438
2022-01-29 03:47:52 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.6615
2022-01-29 03:48:24 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.4609
2022-01-29 03:48:56 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.5377
2022-01-29 03:49:29 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.0962
2022-01-29 03:50:01 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.2658
2022-01-29 03:50:35 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.3463
2022-01-29 03:51:08 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.5926
2022-01-29 03:51:40 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.3277
2022-01-29 03:52:14 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.1040
2022-01-29 03:52:46 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.0895
2022-01-29 03:53:19 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.4474
2022-01-29 03:53:50 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.3403
2022-01-29 03:54:23 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.0346
2022-01-29 03:54:55 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.3179
2022-01-29 03:55:27 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.3144
2022-01-29 03:56:00 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.3733
2022-01-29 03:56:32 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.3054
2022-01-29 03:57:04 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.4297
2022-01-29 03:57:37 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.1058
2022-01-29 03:58:09 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.2837
2022-01-29 03:58:41 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.3950
2022-01-29 03:59:15 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.4678
2022-01-29 03:59:47 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.3779
2022-01-29 04:00:20 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.5244
2022-01-29 04:00:53 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.4937
2022-01-29 04:01:25 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.3527
2022-01-29 04:01:57 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.5203
2022-01-29 04:02:27 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.5548
2022-01-29 04:02:58 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.0685
2022-01-29 04:03:31 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.5445
2022-01-29 04:04:04 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.4272
2022-01-29 04:04:37 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.3214
2022-01-29 04:05:10 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.4691
2022-01-29 04:05:44 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.3701
2022-01-29 04:06:16 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.2252
2022-01-29 04:06:49 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.4597
2022-01-29 04:07:22 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.5446
2022-01-29 04:07:56 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.5388
2022-01-29 04:08:31 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.4155
2022-01-29 04:09:03 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.3621
2022-01-29 04:09:04 - train: epoch 009, train_loss: 2.3641
2022-01-29 04:10:21 - eval: epoch: 009, acc1: 50.996%, acc5: 77.006%, test_loss: 2.0985, per_image_load_time: 2.624ms, per_image_inference_time: 0.356ms
2022-01-29 04:10:22 - epoch 010 lr: 0.1
2022-01-29 04:11:00 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.2602
2022-01-29 04:11:33 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.4379
2022-01-29 04:12:06 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.2923
2022-01-29 04:12:38 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.4513
2022-01-29 04:13:10 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.2673
2022-01-29 04:13:43 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.4092
2022-01-29 04:14:15 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.4095
2022-01-29 04:14:48 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.1681
2022-01-29 04:15:21 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.2362
2022-01-29 04:15:53 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.1762
2022-01-29 04:16:26 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.3419
2022-01-29 04:16:59 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.1062
2022-01-29 04:17:33 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.2173
2022-01-29 04:18:05 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.3984
2022-01-29 04:18:37 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.1382
2022-01-29 04:19:09 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.4638
2022-01-29 04:19:43 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.4823
2022-01-29 04:20:14 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.2388
2022-01-29 04:20:47 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.3515
2022-01-29 04:21:18 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.4038
2022-01-29 04:21:51 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.1339
2022-01-29 04:22:24 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.4602
2022-01-29 04:22:55 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.5359
2022-01-29 04:23:28 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.4585
2022-01-29 04:24:00 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.3289
2022-01-29 04:24:32 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.4222
2022-01-29 04:25:04 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.1278
2022-01-29 04:25:38 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.3797
2022-01-29 04:26:11 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.3824
2022-01-29 04:26:43 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.3923
2022-01-29 04:27:15 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.4856
2022-01-29 04:27:48 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.2295
2022-01-29 04:28:21 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.4974
2022-01-29 04:28:53 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.5009
2022-01-29 04:29:26 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.6154
2022-01-29 04:30:01 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.5166
2022-01-29 04:30:33 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.1941
2022-01-29 04:31:05 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.3614
2022-01-29 04:31:38 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 1.9165
2022-01-29 04:32:11 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.2360
2022-01-29 04:32:43 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.2496
2022-01-29 04:33:16 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.4112
2022-01-29 04:33:48 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.2942
2022-01-29 04:34:21 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.2903
2022-01-29 04:34:53 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.1862
2022-01-29 04:35:28 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.3553
2022-01-29 04:36:00 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.3490
2022-01-29 04:36:33 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.2968
2022-01-29 04:37:06 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.1755
2022-01-29 04:37:38 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.1910
2022-01-29 04:37:39 - train: epoch 010, train_loss: 2.3313
2022-01-29 04:38:53 - eval: epoch: 010, acc1: 46.940%, acc5: 73.286%, test_loss: 2.3322, per_image_load_time: 2.334ms, per_image_inference_time: 0.363ms
2022-01-29 04:38:54 - epoch 011 lr: 0.1
2022-01-29 04:39:31 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.0655
2022-01-29 04:40:04 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.4147
2022-01-29 04:40:37 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.2188
2022-01-29 04:41:08 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.5414
2022-01-29 04:41:41 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.3091
2022-01-29 04:42:13 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.2849
2022-01-29 04:42:46 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.3289
2022-01-29 04:43:18 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.3016
2022-01-29 04:43:51 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.4126
2022-01-29 04:44:24 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.2281
2022-01-29 04:44:57 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.4181
2022-01-29 04:45:29 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.5816
2022-01-29 04:46:02 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.5259
2022-01-29 04:46:33 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.4046
2022-01-29 04:47:07 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.2168
2022-01-29 04:47:38 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.3426
2022-01-29 04:48:10 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.3755
2022-01-29 04:48:43 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.2283
2022-01-29 04:49:16 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.1559
2022-01-29 04:49:48 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.4787
2022-01-29 04:50:20 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.3809
2022-01-29 04:50:52 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.1162
2022-01-29 04:51:24 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.5667
2022-01-29 04:51:57 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.1464
2022-01-29 04:52:29 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.4523
2022-01-29 04:53:01 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.3179
2022-01-29 04:53:34 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.3035
2022-01-29 04:54:07 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 1.9962
2022-01-29 04:54:39 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.3439
2022-01-29 04:55:11 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.4890
2022-01-29 04:55:44 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.3323
2022-01-29 04:56:15 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.1240
2022-01-29 04:56:48 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.3017
2022-01-29 04:57:20 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.3471
2022-01-29 04:57:53 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.2729
2022-01-29 04:58:24 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.3123
2022-01-29 04:58:57 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.3641
2022-01-29 04:59:28 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.1072
2022-01-29 05:00:00 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.4453
2022-01-29 05:00:31 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.2764
2022-01-29 05:01:03 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.1456
2022-01-29 05:01:35 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.1745
2022-01-29 05:02:07 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.4512
2022-01-29 05:02:41 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.3275
2022-01-29 05:03:15 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.0953
2022-01-29 05:03:48 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.2068
2022-01-29 05:04:22 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.0565
2022-01-29 05:04:55 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.0447
2022-01-29 05:05:27 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.2198
2022-01-29 05:06:00 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.2789
2022-01-29 05:06:01 - train: epoch 011, train_loss: 2.3044
2022-01-29 05:07:17 - eval: epoch: 011, acc1: 52.330%, acc5: 78.248%, test_loss: 2.0231, per_image_load_time: 2.563ms, per_image_inference_time: 0.360ms
2022-01-29 05:07:18 - epoch 012 lr: 0.1
2022-01-29 05:07:56 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.1819
2022-01-29 05:08:29 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.2319
2022-01-29 05:09:02 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.2881
2022-01-29 05:09:34 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.3243
2022-01-29 05:10:07 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.4408
2022-01-29 05:10:40 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.0481
2022-01-29 05:11:13 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.0994
2022-01-29 05:11:48 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.2629
2022-01-29 05:12:21 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.3658
2022-01-29 05:12:53 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.0314
2022-01-29 05:13:27 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.5639
2022-01-29 05:13:59 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.0619
2022-01-29 05:14:31 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.2817
2022-01-29 05:15:03 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.4266
2022-01-29 05:15:36 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.0564
2022-01-29 05:16:08 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.3030
2022-01-29 05:16:40 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.1405
2022-01-29 05:17:12 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.3029
2022-01-29 05:17:45 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.3558
2022-01-29 05:18:17 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.4805
2022-01-29 05:18:50 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.2367
2022-01-29 05:19:21 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.5077
2022-01-29 05:19:56 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.3005
2022-01-29 05:20:27 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.2767
2022-01-29 05:21:00 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.1749
2022-01-29 05:21:32 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.0109
2022-01-29 05:22:04 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.1858
2022-01-29 05:22:36 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.2078
2022-01-29 05:23:09 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.1324
2022-01-29 05:23:40 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.1247
2022-01-29 05:24:14 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.4028
2022-01-29 05:24:45 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.0095
2022-01-29 05:25:17 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.3386
2022-01-29 05:25:49 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.2861
2022-01-29 05:26:22 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.3868
2022-01-29 05:26:54 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.2826
2022-01-29 05:27:26 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.2410
2022-01-29 05:27:59 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.3333
2022-01-29 05:28:32 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.1886
2022-01-29 05:29:05 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.1716
2022-01-29 05:29:37 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.0938
2022-01-29 05:30:09 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.0339
2022-01-29 05:30:42 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.3390
2022-01-29 05:31:15 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.1338
2022-01-29 05:31:47 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.3103
2022-01-29 05:32:20 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.6499
2022-01-29 05:32:53 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.2150
2022-01-29 05:33:25 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.3099
2022-01-29 05:33:58 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.3629
2022-01-29 05:34:31 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.0284
2022-01-29 05:34:32 - train: epoch 012, train_loss: 2.2831
2022-01-29 05:35:47 - eval: epoch: 012, acc1: 51.624%, acc5: 77.498%, test_loss: 2.0639, per_image_load_time: 2.540ms, per_image_inference_time: 0.371ms
2022-01-29 05:35:48 - epoch 013 lr: 0.1
2022-01-29 05:36:25 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 1.9924
2022-01-29 05:36:58 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.1751
2022-01-29 05:37:32 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.1839
2022-01-29 05:38:04 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.2233
2022-01-29 05:38:36 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.1676
2022-01-29 05:39:08 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.3614
2022-01-29 05:39:42 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.0850
2022-01-29 05:40:14 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.4248
2022-01-29 05:40:47 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.1387
2022-01-29 05:41:18 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.2566
2022-01-29 05:41:51 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.2510
2022-01-29 05:42:23 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.4055
2022-01-29 05:42:55 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.3524
2022-01-29 05:43:28 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.0797
2022-01-29 05:44:00 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.3880
2022-01-29 05:44:31 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.0299
2022-01-29 05:45:04 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.3344
2022-01-29 05:45:37 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.3612
2022-01-29 05:46:10 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.2937
2022-01-29 05:46:45 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.4624
2022-01-29 05:47:16 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.4977
2022-01-29 05:47:51 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.1169
2022-01-29 05:48:23 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.3144
2022-01-29 05:48:56 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.2369
2022-01-29 05:49:29 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.2123
2022-01-29 05:50:02 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.1842
2022-01-29 05:50:36 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.1205
2022-01-29 05:51:08 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.2863
2022-01-29 05:51:42 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.3099
2022-01-29 05:52:14 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.1532
2022-01-29 05:52:45 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.0646
2022-01-29 05:53:20 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.2979
2022-01-29 05:53:56 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.0713
2022-01-29 05:54:27 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.2603
2022-01-29 05:55:00 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.1315
2022-01-29 05:55:35 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.5558
2022-01-29 05:56:09 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 1.9811
2022-01-29 05:56:44 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.4384
2022-01-29 05:57:18 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.3927
2022-01-29 05:57:52 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.2545
2022-01-29 05:58:24 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.1174
2022-01-29 05:58:59 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.1936
2022-01-29 05:59:31 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.2434
2022-01-29 06:00:05 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.1233
2022-01-29 06:00:39 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.2618
2022-01-29 06:01:10 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.2921
2022-01-29 06:01:42 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.2849
2022-01-29 06:02:12 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.3569
2022-01-29 06:02:44 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.3109
2022-01-29 06:03:19 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.4377
2022-01-29 06:03:20 - train: epoch 013, train_loss: 2.2647
2022-01-29 06:04:43 - eval: epoch: 013, acc1: 48.186%, acc5: 74.316%, test_loss: 2.2648, per_image_load_time: 2.880ms, per_image_inference_time: 0.352ms
2022-01-29 06:04:43 - epoch 014 lr: 0.1
2022-01-29 06:05:25 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.2527
2022-01-29 06:05:57 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.3317
2022-01-29 06:06:32 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 1.9424
2022-01-29 06:07:06 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.1868
2022-01-29 06:07:38 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.1469
2022-01-29 06:08:11 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.2531
2022-01-29 06:08:46 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.2664
2022-01-29 06:09:18 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.2071
2022-01-29 06:09:51 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.2834
2022-01-29 06:10:24 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.4689
2022-01-29 06:10:56 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.1061
2022-01-29 06:11:29 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.2756
2022-01-29 06:12:04 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.2718
2022-01-29 06:12:37 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.3011
2022-01-29 06:13:10 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.2990
2022-01-29 06:13:42 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.2518
2022-01-29 06:14:14 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.4807
2022-01-29 06:14:47 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.4366
2022-01-29 06:15:21 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.1625
2022-01-29 06:15:51 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.2500
2022-01-29 06:16:25 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.4024
2022-01-29 06:16:59 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.2400
2022-01-29 06:17:31 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.2816
2022-01-29 06:18:03 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.4398
2022-01-29 06:18:35 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.1809
2022-01-29 06:19:07 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.1365
2022-01-29 06:19:40 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.2196
2022-01-29 06:20:13 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.5916
2022-01-29 06:20:49 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.2409
2022-01-29 06:21:20 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.3428
2022-01-29 06:21:53 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.0760
2022-01-29 06:22:25 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.2610
2022-01-29 06:22:58 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.1259
2022-01-29 06:23:31 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.2625
2022-01-29 06:24:03 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.1763
2022-01-29 06:24:36 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.1187
2022-01-29 06:25:08 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.2343
2022-01-29 06:25:41 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.4108
2022-01-29 06:26:14 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.0944
2022-01-29 06:26:48 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.3217
2022-01-29 06:27:21 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.1948
2022-01-29 06:27:54 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.0978
2022-01-29 06:28:26 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.0404
2022-01-29 06:28:59 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.0923
2022-01-29 06:29:31 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.2203
2022-01-29 06:30:04 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.1224
2022-01-29 06:30:36 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.2063
2022-01-29 06:31:11 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.0702
2022-01-29 06:31:42 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 1.9886
2022-01-29 06:32:17 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.2043
2022-01-29 06:32:18 - train: epoch 014, train_loss: 2.2497
2022-01-29 06:33:34 - eval: epoch: 014, acc1: 45.722%, acc5: 72.082%, test_loss: 2.4167, per_image_load_time: 2.602ms, per_image_inference_time: 0.355ms
2022-01-29 06:33:35 - epoch 015 lr: 0.1
2022-01-29 06:34:13 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.0607
2022-01-29 06:34:47 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.3662
2022-01-29 06:35:18 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.3236
2022-01-29 06:35:50 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.1922
2022-01-29 06:36:22 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.2874
2022-01-29 06:36:55 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.3506
2022-01-29 06:37:28 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.1615
2022-01-29 06:38:00 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 1.9949
2022-01-29 06:38:34 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.1698
2022-01-29 06:39:06 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.2493
2022-01-29 06:39:38 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.1467
2022-01-29 06:40:11 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.2741
2022-01-29 06:40:44 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.5930
2022-01-29 06:41:18 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.0486
2022-01-29 06:41:50 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 1.8942
2022-01-29 06:42:23 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.2959
2022-01-29 06:42:55 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.5283
2022-01-29 06:43:27 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.0902
2022-01-29 06:44:00 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.1428
2022-01-29 06:44:32 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.1817
2022-01-29 06:45:05 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.0641
2022-01-29 06:45:38 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.4190
2022-01-29 06:46:10 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 1.9798
2022-01-29 06:46:42 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.4021
2022-01-29 06:47:14 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.2099
2022-01-29 06:47:48 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.0660
2022-01-29 06:48:21 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.2746
2022-01-29 06:48:53 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.3042
2022-01-29 06:49:25 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.2453
2022-01-29 06:49:57 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.1406
2022-01-29 06:50:29 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.2064
2022-01-29 06:51:02 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.1205
2022-01-29 06:51:34 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.0398
2022-01-29 06:52:07 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.2115
2022-01-29 06:52:38 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.3538
2022-01-29 06:53:10 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.3373
2022-01-29 06:53:42 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.1239
2022-01-29 06:54:15 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.1501
2022-01-29 06:54:47 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.4324
2022-01-29 06:55:20 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.2331
2022-01-29 06:55:53 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.3991
2022-01-29 06:56:25 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.0274
2022-01-29 06:56:58 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.2009
2022-01-29 06:57:31 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.2483
2022-01-29 06:58:02 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.2437
2022-01-29 06:58:35 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.1292
2022-01-29 06:59:07 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.3510
2022-01-29 06:59:40 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.1841
2022-01-29 07:00:13 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.1723
2022-01-29 07:00:44 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.2575
2022-01-29 07:00:45 - train: epoch 015, train_loss: 2.2383
2022-01-29 07:01:59 - eval: epoch: 015, acc1: 52.252%, acc5: 77.454%, test_loss: 2.0502, per_image_load_time: 2.498ms, per_image_inference_time: 0.376ms
2022-01-29 07:02:00 - epoch 016 lr: 0.1
2022-01-29 07:02:38 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.2662
2022-01-29 07:03:10 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.0192
2022-01-29 07:03:42 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.2665
2022-01-29 07:04:13 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.4072
2022-01-29 07:04:44 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 1.9499
2022-01-29 07:05:16 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.3503
2022-01-29 07:05:47 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.0255
2022-01-29 07:06:20 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.2218
2022-01-29 07:06:53 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.3349
2022-01-29 07:07:29 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.0773
2022-01-29 07:08:03 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.1458
2022-01-29 07:08:37 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.1204
2022-01-29 07:09:12 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.4511
2022-01-29 07:09:47 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.0975
2022-01-29 07:10:20 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.2788
2022-01-29 07:10:56 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.3850
2022-01-29 07:11:30 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.1161
2022-01-29 07:12:07 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.1428
2022-01-29 07:12:40 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.1002
2022-01-29 07:13:14 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 1.9382
2022-01-29 07:13:50 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.3327
2022-01-29 07:14:26 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.2873
2022-01-29 07:15:04 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.4311
2022-01-29 07:15:38 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.4035
2022-01-29 07:16:13 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.1099
2022-01-29 07:16:50 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.4790
2022-01-29 07:17:24 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.2324
2022-01-29 07:17:58 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.0710
2022-01-29 07:18:34 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.2669
2022-01-29 07:19:10 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.4458
2022-01-29 07:19:45 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.4850
2022-01-29 07:20:19 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.3323
2022-01-29 07:20:56 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.3112
2022-01-29 07:21:33 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.1146
2022-01-29 07:22:09 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.0353
2022-01-29 07:22:44 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.1032
2022-01-29 07:23:19 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.2728
2022-01-29 07:23:54 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.6395
2022-01-29 07:24:29 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.2654
2022-01-29 07:25:06 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.3516
2022-01-29 07:25:40 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.1858
2022-01-29 07:26:18 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.1798
2022-01-29 07:26:52 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.1054
2022-01-29 07:27:27 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.0815
2022-01-29 07:28:00 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.2781
2022-01-29 07:28:37 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.1664
2022-01-29 07:29:11 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.5094
2022-01-29 07:29:48 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.1406
2022-01-29 07:30:26 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.2479
2022-01-29 07:31:01 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.3283
2022-01-29 07:31:02 - train: epoch 016, train_loss: 2.2254
2022-01-29 07:32:21 - eval: epoch: 016, acc1: 53.518%, acc5: 79.120%, test_loss: 1.9776, per_image_load_time: 2.677ms, per_image_inference_time: 0.378ms
2022-01-29 07:32:22 - epoch 017 lr: 0.1
2022-01-29 07:33:03 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.1399
2022-01-29 07:33:36 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.3506
2022-01-29 07:34:12 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.4368
2022-01-29 07:34:45 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 1.9603
2022-01-29 07:35:19 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.1932
2022-01-29 07:35:55 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.4689
2022-01-29 07:36:28 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.2583
2022-01-29 07:37:01 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.1311
2022-01-29 07:37:38 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.1630
2022-01-29 07:38:13 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.0895
2022-01-29 07:38:46 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.6198
2022-01-29 07:39:22 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.3337
2022-01-29 07:39:58 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.2035
2022-01-29 07:40:31 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.2692
2022-01-29 07:41:06 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 1.8991
2022-01-29 07:41:41 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.0149
2022-01-29 07:42:16 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.1552
2022-01-29 07:42:52 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.1692
2022-01-29 07:43:26 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.1995
2022-01-29 07:44:01 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.3230
2022-01-29 07:44:35 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.2551
2022-01-29 07:45:09 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.2246
2022-01-29 07:45:44 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.0777
2022-01-29 07:46:17 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.1379
2022-01-29 07:46:51 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.3344
2022-01-29 07:47:26 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.1391
2022-01-29 07:48:00 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.1570
2022-01-29 07:48:34 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.4808
2022-01-29 07:49:08 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.3863
2022-01-29 07:49:44 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.0659
2022-01-29 07:50:18 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.3525
2022-01-29 07:50:53 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.1427
2022-01-29 07:51:25 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.2724
2022-01-29 07:51:57 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.0508
2022-01-29 07:52:33 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.2829
2022-01-29 07:53:06 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.4061
2022-01-29 07:53:40 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.1405
2022-01-29 07:54:12 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.3936
2022-01-29 07:54:47 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.0895
2022-01-29 07:55:23 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.0453
2022-01-29 07:55:57 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.2042
2022-01-29 07:56:31 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.2878
2022-01-29 07:57:04 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.3357
2022-01-29 07:57:38 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.2232
2022-01-29 07:58:12 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.2317
2022-01-29 07:58:45 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.1793
2022-01-29 07:59:19 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.2856
2022-01-29 07:59:53 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.2495
2022-01-29 08:00:28 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.0764
2022-01-29 08:01:02 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.0392
2022-01-29 08:01:03 - train: epoch 017, train_loss: 2.2147
2022-01-29 08:02:21 - eval: epoch: 017, acc1: 52.944%, acc5: 78.706%, test_loss: 2.0100, per_image_load_time: 2.695ms, per_image_inference_time: 0.363ms
2022-01-29 08:02:22 - epoch 018 lr: 0.1
2022-01-29 08:03:02 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.1858
2022-01-29 08:03:34 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.2775
2022-01-29 08:04:08 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.3366
2022-01-29 08:04:43 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.4264
2022-01-29 08:05:15 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.1095
2022-01-29 08:05:50 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.3016
2022-01-29 08:06:23 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 1.9666
2022-01-29 08:06:56 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.1362
2022-01-29 08:07:30 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.3719
2022-01-29 08:08:03 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.0562
2022-01-29 08:08:37 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.4389
2022-01-29 08:09:10 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.2065
2022-01-29 08:09:43 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.5382
2022-01-29 08:10:17 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.0728
2022-01-29 08:10:50 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.4825
2022-01-29 08:11:23 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.1917
2022-01-29 08:11:58 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.1323
2022-01-29 08:12:31 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 1.9895
2022-01-29 08:13:05 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.2300
2022-01-29 08:13:39 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.4819
2022-01-29 08:14:12 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.3579
2022-01-29 08:14:44 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.1665
2022-01-29 08:15:16 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.2287
2022-01-29 08:15:48 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.0695
2022-01-29 08:16:22 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 1.9118
2022-01-29 08:16:57 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.0701
2022-01-29 08:17:30 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.3569
2022-01-29 08:18:06 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 1.9128
2022-01-29 08:18:38 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.2985
2022-01-29 08:19:12 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.1627
2022-01-29 08:19:47 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.6094
2022-01-29 08:20:21 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.0818
2022-01-29 08:20:55 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.1350
2022-01-29 08:21:30 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.1230
2022-01-29 08:22:03 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.4354
2022-01-29 08:22:40 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.2515
2022-01-29 08:23:17 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.5971
2022-01-29 08:23:51 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.3127
2022-01-29 08:24:26 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.3370
2022-01-29 08:25:01 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.0867
2022-01-29 08:25:36 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.1556
2022-01-29 08:26:10 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.2514
2022-01-29 08:26:48 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.0250
2022-01-29 08:27:23 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.4845
2022-01-29 08:27:56 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.1245
2022-01-29 08:28:29 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.0657
2022-01-29 08:29:03 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.4315
2022-01-29 08:29:39 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.2810
2022-01-29 08:30:14 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.2260
2022-01-29 08:30:47 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.3030
2022-01-29 08:30:49 - train: epoch 018, train_loss: 2.2044
2022-01-29 08:32:08 - eval: epoch: 018, acc1: 50.118%, acc5: 76.070%, test_loss: 2.1605, per_image_load_time: 2.711ms, per_image_inference_time: 0.361ms
2022-01-29 08:32:09 - epoch 019 lr: 0.1
2022-01-29 08:32:48 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.0561
2022-01-29 08:33:20 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.3673
2022-01-29 08:33:53 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.3930
2022-01-29 08:34:26 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.2018
2022-01-29 08:35:02 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.1963
2022-01-29 08:35:34 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.0992
2022-01-29 08:36:08 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 1.9301
2022-01-29 08:36:43 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.3370
2022-01-29 08:37:15 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.2111
2022-01-29 08:37:51 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.3301
2022-01-29 08:38:26 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.1818
2022-01-29 08:39:01 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.2003
2022-01-29 08:39:33 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.2862
2022-01-29 08:40:06 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.0265
2022-01-29 08:40:39 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.5137
2022-01-29 08:41:11 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.1536
2022-01-29 08:41:43 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.2677
2022-01-29 08:42:16 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.1525
2022-01-29 08:42:50 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.2576
2022-01-29 08:43:23 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.0261
2022-01-29 08:43:55 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.0662
2022-01-29 08:44:28 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.2230
2022-01-29 08:45:02 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.1687
2022-01-29 08:45:36 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.3061
2022-01-29 08:46:09 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.0866
2022-01-29 08:46:41 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.2547
2022-01-29 08:47:14 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.3139
2022-01-29 08:47:52 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.2784
2022-01-29 08:48:25 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.1290
2022-01-29 08:48:59 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.3715
2022-01-29 08:49:31 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.2457
2022-01-29 08:50:07 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.8730
2022-01-29 08:50:40 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.1389
2022-01-29 08:51:13 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.2449
2022-01-29 08:51:46 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.2366
2022-01-29 08:52:20 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.0074
2022-01-29 08:52:53 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.2623
2022-01-29 08:53:25 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.3368
2022-01-29 08:53:57 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.2038
2022-01-29 08:54:31 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 1.9820
2022-01-29 08:55:08 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.3143
2022-01-29 08:55:40 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.2306
2022-01-29 08:56:15 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.0088
2022-01-29 08:56:48 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.2761
2022-01-29 08:57:21 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.4606
2022-01-29 08:57:54 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.1024
2022-01-29 08:58:27 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.0806
2022-01-29 08:59:00 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.2669
2022-01-29 08:59:34 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.2062
2022-01-29 09:00:06 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.2662
2022-01-29 09:00:07 - train: epoch 019, train_loss: 2.1998
2022-01-29 09:01:22 - eval: epoch: 019, acc1: 52.558%, acc5: 78.048%, test_loss: 2.0224, per_image_load_time: 2.531ms, per_image_inference_time: 0.380ms
2022-01-29 09:01:23 - epoch 020 lr: 0.1
2022-01-29 09:02:02 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.2110
2022-01-29 09:02:35 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 1.9320
2022-01-29 09:03:08 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.2211
2022-01-29 09:03:40 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.0401
2022-01-29 09:04:15 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.1217
2022-01-29 09:04:47 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.3113
2022-01-29 09:05:22 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 1.9998
2022-01-29 09:05:55 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.2579
2022-01-29 09:06:28 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.4923
2022-01-29 09:07:06 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.2570
2022-01-29 09:07:39 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.1388
2022-01-29 09:08:11 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 1.9745
2022-01-29 09:08:47 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.1187
2022-01-29 09:09:20 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.4100
2022-01-29 09:09:54 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.2459
2022-01-29 09:10:26 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.0395
2022-01-29 09:11:01 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 1.9802
2022-01-29 09:11:33 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.2424
2022-01-29 09:12:06 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 1.9963
2022-01-29 09:12:40 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.1744
2022-01-29 09:13:10 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.3745
2022-01-29 09:13:43 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 1.9926
2022-01-29 09:14:16 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.0594
2022-01-29 09:14:48 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.3065
2022-01-29 09:15:20 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.1225
2022-01-29 09:15:54 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.0136
2022-01-29 09:16:27 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.3127
2022-01-29 09:16:59 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.2717
2022-01-29 09:17:31 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.3691
2022-01-29 09:18:03 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.0716
2022-01-29 09:18:35 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.2485
2022-01-29 09:19:08 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.3057
2022-01-29 09:19:40 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.0583
2022-01-29 09:20:14 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.2863
2022-01-29 09:20:45 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.0722
2022-01-29 09:21:18 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.2290
2022-01-29 09:21:51 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.0576
2022-01-29 09:22:24 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.0764
2022-01-29 09:22:56 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.3175
2022-01-29 09:23:28 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.0760
2022-01-29 09:24:01 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.0729
2022-01-29 09:24:33 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.1977
2022-01-29 09:25:06 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.2980
2022-01-29 09:25:39 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.2310
2022-01-29 09:26:12 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.2112
2022-01-29 09:26:45 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.3045
2022-01-29 09:27:17 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.0696
2022-01-29 09:27:50 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.1785
2022-01-29 09:28:26 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.3052
2022-01-29 09:28:59 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.1078
2022-01-29 09:29:00 - train: epoch 020, train_loss: 2.1888
2022-01-29 09:30:14 - eval: epoch: 020, acc1: 52.180%, acc5: 77.432%, test_loss: 2.0648, per_image_load_time: 2.467ms, per_image_inference_time: 0.387ms
2022-01-29 09:30:15 - epoch 021 lr: 0.1
2022-01-29 09:30:52 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.0891
2022-01-29 09:31:26 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.2386
2022-01-29 09:31:59 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.9327
2022-01-29 09:32:32 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.3551
2022-01-29 09:33:04 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.0722
2022-01-29 09:33:37 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.0878
2022-01-29 09:34:09 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.0730
2022-01-29 09:34:42 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.3242
2022-01-29 09:35:15 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.2052
2022-01-29 09:35:47 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.0577
2022-01-29 09:36:20 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.1424
2022-01-29 09:36:53 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.0846
2022-01-29 09:37:25 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.0852
2022-01-29 09:37:57 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.0960
2022-01-29 09:38:30 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.1051
2022-01-29 09:39:03 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.1492
2022-01-29 09:39:35 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.1753
2022-01-29 09:40:07 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 1.9815
2022-01-29 09:40:41 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.1620
2022-01-29 09:41:12 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.4253
2022-01-29 09:41:45 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.0115
2022-01-29 09:42:18 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.2079
2022-01-29 09:42:50 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.0822
2022-01-29 09:43:24 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.1305
2022-01-29 09:43:57 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.2007
2022-01-29 09:44:29 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.4344
2022-01-29 09:45:01 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.0744
2022-01-29 09:45:34 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.1941
2022-01-29 09:46:07 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.1389
2022-01-29 09:46:40 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.4881
2022-01-29 09:47:11 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.2196
2022-01-29 09:47:45 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.1421
2022-01-29 09:48:19 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.4168
2022-01-29 09:48:50 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.3601
2022-01-29 09:49:23 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.0399
2022-01-29 09:49:56 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.1629
2022-01-29 09:50:29 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.2107
2022-01-29 09:51:02 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.0819
2022-01-29 09:51:35 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.0019
2022-01-29 09:52:07 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.3993
2022-01-29 09:52:41 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.1262
2022-01-29 09:53:14 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.1244
2022-01-29 09:53:46 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.1272
2022-01-29 09:54:19 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.2142
2022-01-29 09:54:52 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.3260
2022-01-29 09:55:26 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.0633
2022-01-29 09:55:59 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.3830
2022-01-29 09:56:32 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.2913
2022-01-29 09:57:05 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.0473
2022-01-29 09:57:38 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.0822
2022-01-29 09:57:39 - train: epoch 021, train_loss: 2.1836
2022-01-29 09:58:55 - eval: epoch: 021, acc1: 54.074%, acc5: 79.726%, test_loss: 1.9340, per_image_load_time: 2.574ms, per_image_inference_time: 0.372ms
2022-01-29 09:58:56 - epoch 022 lr: 0.1
2022-01-29 09:59:32 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.0027
2022-01-29 10:00:04 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.0185
2022-01-29 10:00:37 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 1.9657
2022-01-29 10:01:11 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 1.9536
2022-01-29 10:01:43 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.1162
2022-01-29 10:02:15 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.1965
2022-01-29 10:02:48 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.3487
2022-01-29 10:03:20 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.3547
2022-01-29 10:03:52 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.3047
2022-01-29 10:04:24 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.1736
2022-01-29 10:04:57 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.2439
2022-01-29 10:05:29 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.8467
2022-01-29 10:06:00 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.2506
2022-01-29 10:06:31 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.2029
2022-01-29 10:07:03 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.1761
2022-01-29 10:07:35 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.0328
2022-01-29 10:08:06 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 1.9936
2022-01-29 10:08:38 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.3353
2022-01-29 10:09:11 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.0246
2022-01-29 10:09:44 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.1776
2022-01-29 10:10:17 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.2977
2022-01-29 10:10:50 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 1.9883
2022-01-29 10:11:21 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.3278
2022-01-29 10:11:54 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.1792
2022-01-29 10:12:27 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.2314
2022-01-29 10:12:59 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 1.9620
2022-01-29 10:13:31 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.0151
2022-01-29 10:14:04 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.4303
2022-01-29 10:14:37 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.0709
2022-01-29 10:15:09 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.2247
2022-01-29 10:15:41 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.3637
2022-01-29 10:16:14 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.1690
2022-01-29 10:16:46 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.1169
2022-01-29 10:17:18 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.0603
2022-01-29 10:17:51 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.3218
2022-01-29 10:18:23 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.2764
2022-01-29 10:18:55 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.2741
2022-01-29 10:19:27 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.3798
2022-01-29 10:20:00 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.1487
2022-01-29 10:20:31 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.2567
2022-01-29 10:21:03 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.1576
2022-01-29 10:21:36 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.0922
2022-01-29 10:22:09 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.4138
2022-01-29 10:22:41 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.2961
2022-01-29 10:23:14 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.1111
2022-01-29 10:23:44 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.4695
2022-01-29 10:24:18 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.2934
2022-01-29 10:24:50 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.0087
2022-01-29 10:25:23 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.0028
2022-01-29 10:25:55 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.0835
2022-01-29 10:25:56 - train: epoch 022, train_loss: 2.1806
2022-01-29 10:27:10 - eval: epoch: 022, acc1: 46.558%, acc5: 73.086%, test_loss: 2.3638, per_image_load_time: 1.419ms, per_image_inference_time: 0.380ms
2022-01-29 10:27:10 - epoch 023 lr: 0.1
2022-01-29 10:27:47 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 1.9614
2022-01-29 10:28:20 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 1.8821
2022-01-29 10:28:52 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.0825
2022-01-29 10:29:25 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.1583
2022-01-29 10:29:58 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.2498
2022-01-29 10:30:30 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.1194
2022-01-29 10:31:02 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 1.9813
2022-01-29 10:31:34 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.0046
2022-01-29 10:32:06 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.2012
2022-01-29 10:32:39 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.0350
2022-01-29 10:33:11 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.3030
2022-01-29 10:33:43 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.0733
2022-01-29 10:34:17 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.0643
2022-01-29 10:34:49 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.1971
2022-01-29 10:35:20 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 1.9706
2022-01-29 10:35:52 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.1542
2022-01-29 10:36:24 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.3129
2022-01-29 10:36:57 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.0120
2022-01-29 10:37:29 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.1550
2022-01-29 10:38:01 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.9071
2022-01-29 10:38:33 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.2635
2022-01-29 10:39:06 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.8984
2022-01-29 10:39:38 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 1.9846
2022-01-29 10:40:10 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.2632
2022-01-29 10:40:43 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.2989
2022-01-29 10:41:15 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.2115
2022-01-29 10:41:48 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.1597
2022-01-29 10:42:20 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.2559
2022-01-29 10:42:51 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.2231
2022-01-29 10:43:24 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.3137
2022-01-29 10:43:56 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.3096
2022-01-29 10:44:28 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.3967
2022-01-29 10:45:01 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.2905
2022-01-29 10:45:34 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.2697
2022-01-29 10:46:05 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.0909
2022-01-29 10:46:38 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 1.9758
2022-01-29 10:47:10 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.2130
2022-01-29 10:47:42 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.2183
2022-01-29 10:48:14 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.2188
2022-01-29 10:48:46 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.1672
2022-01-29 10:49:19 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.0956
2022-01-29 10:49:50 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.0268
2022-01-29 10:50:22 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 1.9509
2022-01-29 10:50:55 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 1.8758
2022-01-29 10:51:27 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.0029
2022-01-29 10:52:00 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.3547
2022-01-29 10:52:31 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 1.9041
2022-01-29 10:53:04 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.0951
2022-01-29 10:53:37 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.1343
2022-01-29 10:54:09 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.3011
2022-01-29 10:54:10 - train: epoch 023, train_loss: 2.1734
2022-01-29 10:55:23 - eval: epoch: 023, acc1: 55.346%, acc5: 80.484%, test_loss: 1.8819, per_image_load_time: 2.471ms, per_image_inference_time: 0.364ms
2022-01-29 10:55:24 - epoch 024 lr: 0.1
2022-01-29 10:56:01 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.0909
2022-01-29 10:56:34 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.1580
2022-01-29 10:57:06 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.0992
2022-01-29 10:57:38 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.2415
2022-01-29 10:58:09 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.0957
2022-01-29 10:58:42 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 1.9740
2022-01-29 10:59:14 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.1417
2022-01-29 10:59:46 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.1233
2022-01-29 11:00:17 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.1757
2022-01-29 11:00:49 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.0729
2022-01-29 11:01:21 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.8597
2022-01-29 11:01:53 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 1.9791
2022-01-29 11:02:24 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.5099
2022-01-29 11:02:55 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.0410
2022-01-29 11:03:28 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.3348
2022-01-29 11:04:00 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.1926
2022-01-29 11:04:31 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.0726
2022-01-29 11:05:03 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.3388
2022-01-29 11:05:34 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 1.9694
2022-01-29 11:06:07 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.0867
2022-01-29 11:06:38 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.1906
2022-01-29 11:07:09 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.0051
2022-01-29 11:07:39 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.1732
2022-01-29 11:08:11 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.1110
2022-01-29 11:08:43 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.1557
2022-01-29 11:09:16 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.0502
2022-01-29 11:09:49 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.3057
2022-01-29 11:10:21 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.2811
2022-01-29 11:10:54 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.2469
2022-01-29 11:11:26 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.0478
2022-01-29 11:11:59 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.0938
2022-01-29 11:12:33 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.2712
2022-01-29 11:13:05 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.8787
2022-01-29 11:13:40 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.1781
2022-01-29 11:14:12 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 1.9981
2022-01-29 11:14:45 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.2334
2022-01-29 11:15:18 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.1691
2022-01-29 11:15:52 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.4112
2022-01-29 11:16:24 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.0907
2022-01-29 11:16:57 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.1595
2022-01-29 11:17:29 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.1864
2022-01-29 11:18:03 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.2240
2022-01-29 11:18:36 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.1603
2022-01-29 11:19:09 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.2555
2022-01-29 11:19:41 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 1.9831
2022-01-29 11:20:15 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.2372
2022-01-29 11:20:48 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.0662
2022-01-29 11:21:23 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.0808
2022-01-29 11:21:57 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.1265
2022-01-29 11:22:29 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.3059
2022-01-29 11:22:30 - train: epoch 024, train_loss: 2.1686
2022-01-29 11:23:46 - eval: epoch: 024, acc1: 52.688%, acc5: 78.168%, test_loss: 2.0219, per_image_load_time: 2.597ms, per_image_inference_time: 0.365ms
2022-01-29 11:23:47 - epoch 025 lr: 0.1
2022-01-29 11:24:25 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 1.9398
2022-01-29 11:24:58 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 1.9502
2022-01-29 11:25:30 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.0159
2022-01-29 11:26:02 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.1114
2022-01-29 11:26:34 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 1.9605
2022-01-29 11:27:07 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.1293
2022-01-29 11:27:39 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.2831
2022-01-29 11:28:12 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.0828
2022-01-29 11:28:44 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 1.9704
2022-01-29 11:29:17 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.0633
2022-01-29 11:29:48 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.1555
2022-01-29 11:30:21 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.1633
2022-01-29 11:30:53 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.2297
2022-01-29 11:31:24 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.2849
2022-01-29 11:31:57 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.2025
2022-01-29 11:32:29 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 1.8519
2022-01-29 11:33:01 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.1049
2022-01-29 11:33:34 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 1.9514
2022-01-29 11:34:05 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.0107
2022-01-29 11:34:38 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.1183
2022-01-29 11:35:12 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.1516
2022-01-29 11:35:45 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 1.9162
2022-01-29 11:36:17 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.1719
2022-01-29 11:36:49 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 1.9896
2022-01-29 11:37:22 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.2434
2022-01-29 11:37:55 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.3672
2022-01-29 11:38:28 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.2099
2022-01-29 11:39:00 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.2180
2022-01-29 11:39:32 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.3176
2022-01-29 11:40:06 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.4215
2022-01-29 11:40:38 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.0881
2022-01-29 11:41:11 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.3469
2022-01-29 11:41:43 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.1674
2022-01-29 11:42:15 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.2794
2022-01-29 11:42:48 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.0024
2022-01-29 11:43:22 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.3576
2022-01-29 11:43:55 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.1142
2022-01-29 11:44:28 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.1808
2022-01-29 11:45:00 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.3181
2022-01-29 11:45:33 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.3261
2022-01-29 11:46:05 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.4383
2022-01-29 11:46:38 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.1300
2022-01-29 11:47:11 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.0450
2022-01-29 11:47:44 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.0918
2022-01-29 11:48:16 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.1219
2022-01-29 11:48:48 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.1897
2022-01-29 11:49:22 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 1.9632
2022-01-29 11:49:55 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.0215
2022-01-29 11:50:27 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.1266
2022-01-29 11:50:59 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.3847
2022-01-29 11:51:00 - train: epoch 025, train_loss: 2.1665
2022-01-29 11:52:15 - eval: epoch: 025, acc1: 52.704%, acc5: 78.162%, test_loss: 2.0191, per_image_load_time: 2.526ms, per_image_inference_time: 0.377ms
2022-01-29 11:52:16 - epoch 026 lr: 0.1
2022-01-29 11:52:54 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 1.9262
2022-01-29 11:53:26 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.9450
2022-01-29 11:53:58 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.0497
2022-01-29 11:54:31 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.1557
2022-01-29 11:55:03 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.2368
2022-01-29 11:55:35 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.2742
2022-01-29 11:56:06 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.0586
2022-01-29 11:56:39 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 1.9287
2022-01-29 11:57:12 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.3969
2022-01-29 11:57:43 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.0511
2022-01-29 11:58:16 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.0418
2022-01-29 11:58:48 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.1418
2022-01-29 11:59:20 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.0498
2022-01-29 11:59:54 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.1868
2022-01-29 12:00:26 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.0808
2022-01-29 12:00:58 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.3710
2022-01-29 12:01:31 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.0636
2022-01-29 12:02:03 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.2931
2022-01-29 12:02:36 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.4335
2022-01-29 12:03:08 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.2830
2022-01-29 12:03:39 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.2071
2022-01-29 12:04:12 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.2347
2022-01-29 12:04:43 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.0497
2022-01-29 12:05:18 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.3683
2022-01-29 12:05:49 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.2653
2022-01-29 12:06:22 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.2632
2022-01-29 12:06:55 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.1418
2022-01-29 12:07:27 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.0710
2022-01-29 12:08:00 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.2778
2022-01-29 12:08:32 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.0893
2022-01-29 12:09:05 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.0663
2022-01-29 12:09:39 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.1716
2022-01-29 12:10:11 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.0321
2022-01-29 12:10:43 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.3951
2022-01-29 12:11:16 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.2210
2022-01-29 12:11:50 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 1.9212
2022-01-29 12:12:22 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.2524
2022-01-29 12:12:55 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.3881
2022-01-29 12:13:27 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.3608
2022-01-29 12:14:01 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.3645
2022-01-29 12:14:33 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.1518
2022-01-29 12:15:06 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.2924
2022-01-29 12:15:38 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.3149
2022-01-29 12:16:11 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.3565
2022-01-29 12:16:44 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.3750
2022-01-29 12:17:16 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.0907
2022-01-29 12:17:50 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.0533
2022-01-29 12:18:22 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.1482
2022-01-29 12:18:56 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.1670
2022-01-29 12:19:28 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.3580
2022-01-29 12:19:29 - train: epoch 026, train_loss: 2.1600
2022-01-29 12:20:44 - eval: epoch: 026, acc1: 53.872%, acc5: 79.344%, test_loss: 1.9543, per_image_load_time: 2.556ms, per_image_inference_time: 0.379ms
2022-01-29 12:20:45 - epoch 027 lr: 0.1
2022-01-29 12:21:24 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.3364
2022-01-29 12:21:56 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.0458
2022-01-29 12:22:30 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.1613
2022-01-29 12:23:02 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.2632
2022-01-29 12:23:33 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.1966
2022-01-29 12:24:07 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.3822
2022-01-29 12:24:40 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.3045
2022-01-29 12:25:12 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.0990
2022-01-29 12:25:45 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.0477
2022-01-29 12:26:19 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.2759
2022-01-29 12:26:50 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.0641
2022-01-29 12:27:24 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.4170
2022-01-29 12:27:57 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.1846
2022-01-29 12:28:30 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.3786
2022-01-29 12:29:03 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.2273
2022-01-29 12:29:35 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.1872
2022-01-29 12:30:07 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.0996
2022-01-29 12:30:40 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.1039
2022-01-29 12:31:13 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.3202
2022-01-29 12:31:45 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.1697
2022-01-29 12:32:18 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.3973
2022-01-29 12:32:50 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.2128
2022-01-29 12:33:24 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.4255
2022-01-29 12:33:56 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.1496
2022-01-29 12:34:28 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.0583
2022-01-29 12:35:01 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.3919
2022-01-29 12:35:34 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.1617
2022-01-29 12:36:07 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.2807
2022-01-29 12:36:40 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.1130
2022-01-29 12:37:12 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.1413
2022-01-29 12:37:44 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.0572
2022-01-29 12:38:17 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.0635
2022-01-29 12:38:50 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.2391
2022-01-29 12:39:23 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.1934
2022-01-29 12:39:56 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.4809
2022-01-29 12:40:28 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.0776
2022-01-29 12:41:02 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.0715
2022-01-29 12:41:34 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 1.8849
2022-01-29 12:42:07 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 1.9780
2022-01-29 12:42:39 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.1111
2022-01-29 12:43:12 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.0832
2022-01-29 12:43:45 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.2129
2022-01-29 12:44:18 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 1.9912
2022-01-29 12:44:51 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.1031
2022-01-29 12:45:23 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.0088
2022-01-29 12:45:56 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.1444
2022-01-29 12:46:30 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.1974
2022-01-29 12:47:02 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.2871
2022-01-29 12:47:37 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.2754
2022-01-29 12:48:09 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.7915
2022-01-29 12:48:10 - train: epoch 027, train_loss: 2.1577
2022-01-29 12:49:24 - eval: epoch: 027, acc1: 55.692%, acc5: 80.598%, test_loss: 1.8781, per_image_load_time: 2.454ms, per_image_inference_time: 0.373ms
2022-01-29 12:49:25 - epoch 028 lr: 0.1
2022-01-29 12:50:03 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 1.9229
2022-01-29 12:50:36 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.0975
2022-01-29 12:51:09 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.0556
2022-01-29 12:51:41 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.0234
2022-01-29 12:52:14 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.0389
2022-01-29 12:52:46 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.2939
2022-01-29 12:53:19 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.4453
2022-01-29 12:53:52 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.8658
2022-01-29 12:54:24 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 1.9233
2022-01-29 12:54:56 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.1954
2022-01-29 12:55:30 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.1221
2022-01-29 12:56:02 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 1.9672
2022-01-29 12:56:34 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.1152
2022-01-29 12:57:06 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.5236
2022-01-29 12:57:39 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.0983
2022-01-29 12:58:12 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.0606
2022-01-29 12:58:44 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.0574
2022-01-29 12:59:16 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.0306
2022-01-29 12:59:48 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.0534
2022-01-29 13:00:21 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.3499
2022-01-29 13:00:53 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.1242
2022-01-29 13:01:26 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.2410
2022-01-29 13:01:58 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.2392
2022-01-29 13:02:32 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.5147
2022-01-29 13:03:05 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.0640
2022-01-29 13:03:38 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 1.9311
2022-01-29 13:04:10 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.1265
2022-01-29 13:04:43 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.1681
2022-01-29 13:05:17 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.1503
2022-01-29 13:05:49 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.2090
2022-01-29 13:06:21 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.3995
2022-01-29 13:06:54 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.1162
2022-01-29 13:07:28 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.1902
2022-01-29 13:08:00 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.0615
2022-01-29 13:08:32 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.1363
2022-01-29 13:09:04 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 1.9898
2022-01-29 13:09:37 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.1634
2022-01-29 13:10:10 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.9056
2022-01-29 13:10:42 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.0996
2022-01-29 13:11:15 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.2561
2022-01-29 13:11:47 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.2740
2022-01-29 13:12:21 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.3340
2022-01-29 13:12:53 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.0774
2022-01-29 13:13:26 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.1885
2022-01-29 13:13:59 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.1473
2022-01-29 13:14:31 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.1185
2022-01-29 13:15:05 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.2115
2022-01-29 13:15:38 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.0581
2022-01-29 13:16:11 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 1.8937
2022-01-29 13:16:43 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.1449
2022-01-29 13:16:44 - train: epoch 028, train_loss: 2.1512
2022-01-29 13:18:01 - eval: epoch: 028, acc1: 51.832%, acc5: 77.208%, test_loss: 2.0791, per_image_load_time: 2.598ms, per_image_inference_time: 0.371ms
2022-01-29 13:18:02 - epoch 029 lr: 0.1
2022-01-29 13:18:40 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.3750
2022-01-29 13:19:13 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.1940
2022-01-29 13:19:46 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.2483
2022-01-29 13:20:19 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.1062
2022-01-29 13:20:51 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.1644
2022-01-29 13:21:24 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.3928
2022-01-29 13:21:56 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.6269
2022-01-29 13:22:29 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.2550
2022-01-29 13:23:01 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.1455
2022-01-29 13:23:34 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 1.9229
2022-01-29 13:24:06 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.1484
2022-01-29 13:24:40 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.2652
2022-01-29 13:25:12 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.0868
2022-01-29 13:25:44 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.4078
2022-01-29 13:26:17 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.2143
2022-01-29 13:26:51 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.1185
2022-01-29 13:27:22 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.1757
2022-01-29 13:27:55 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.3155
2022-01-29 13:28:27 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.0026
2022-01-29 13:29:00 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.3256
2022-01-29 13:29:32 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.0460
2022-01-29 13:30:05 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.1940
2022-01-29 13:30:38 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.1915
2022-01-29 13:31:10 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.0338
2022-01-29 13:31:43 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.0358
2022-01-29 13:32:16 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.1043
2022-01-29 13:32:47 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.0199
2022-01-29 13:33:20 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.1361
2022-01-29 13:33:52 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.0761
2022-01-29 13:34:26 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.1419
2022-01-29 13:34:58 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.1619
2022-01-29 13:35:30 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.1982
2022-01-29 13:36:03 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.1225
2022-01-29 13:36:37 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 1.9385
2022-01-29 13:37:09 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.1816
2022-01-29 13:37:41 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.0629
2022-01-29 13:38:13 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.0340
2022-01-29 13:38:45 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.2028
2022-01-29 13:39:18 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 1.9314
2022-01-29 13:39:51 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.1305
2022-01-29 13:40:23 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.1654
2022-01-29 13:40:56 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.0218
2022-01-29 13:41:28 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.1877
2022-01-29 13:42:02 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.1038
2022-01-29 13:42:34 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.2382
2022-01-29 13:43:06 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.4708
2022-01-29 13:43:39 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 1.9645
2022-01-29 13:44:12 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.1955
2022-01-29 13:44:45 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.5065
2022-01-29 13:45:17 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.8312
2022-01-29 13:45:18 - train: epoch 029, train_loss: 2.1489
2022-01-29 13:46:33 - eval: epoch: 029, acc1: 54.186%, acc5: 79.246%, test_loss: 1.9561, per_image_load_time: 2.509ms, per_image_inference_time: 0.381ms
2022-01-29 13:46:34 - epoch 030 lr: 0.1
2022-01-29 13:47:12 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.3230
2022-01-29 13:47:45 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.2017
2022-01-29 13:48:17 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.0536
2022-01-29 13:48:49 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 1.8667
2022-01-29 13:49:22 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.4342
2022-01-29 13:49:54 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 1.9958
2022-01-29 13:50:26 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.0106
2022-01-29 13:50:59 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.2095
2022-01-29 13:51:31 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.1984
2022-01-29 13:52:04 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 1.8427
2022-01-29 13:52:36 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.0134
2022-01-29 13:53:09 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.2187
2022-01-29 13:53:41 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 1.9095
2022-01-29 13:54:12 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.0642
2022-01-29 13:54:45 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.1889
2022-01-29 13:55:17 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.1051
2022-01-29 13:55:49 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.3409
2022-01-29 13:56:23 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.1219
2022-01-29 13:56:55 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.2271
2022-01-29 13:57:28 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.1477
2022-01-29 13:58:01 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.2511
2022-01-29 13:58:34 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.1046
2022-01-29 13:59:07 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.0693
2022-01-29 13:59:39 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.4009
2022-01-29 14:00:12 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.2425
2022-01-29 14:00:45 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.0999
2022-01-29 14:01:18 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.0945
2022-01-29 14:01:51 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.1063
2022-01-29 14:02:22 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.2177
2022-01-29 14:02:56 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.3419
2022-01-29 14:03:28 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.2036
2022-01-29 14:04:01 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.0303
2022-01-29 14:04:35 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.3620
2022-01-29 14:05:07 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.0929
2022-01-29 14:05:38 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.2128
2022-01-29 14:06:11 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 1.9904
2022-01-29 14:06:44 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.0784
2022-01-29 14:07:17 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.3121
2022-01-29 14:07:49 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.2176
2022-01-29 14:08:21 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 1.9429
2022-01-29 14:08:53 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.0454
2022-01-29 14:09:26 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.3992
2022-01-29 14:09:58 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.0864
2022-01-29 14:10:30 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.1476
2022-01-29 14:11:02 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.2332
2022-01-29 14:11:35 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 1.9461
2022-01-29 14:12:08 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.1537
2022-01-29 14:12:40 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.1394
2022-01-29 14:13:12 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.3203
2022-01-29 14:13:45 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.2002
2022-01-29 14:13:46 - train: epoch 030, train_loss: 2.1501
2022-01-29 14:15:00 - eval: epoch: 030, acc1: 55.062%, acc5: 80.188%, test_loss: 1.8874, per_image_load_time: 2.500ms, per_image_inference_time: 0.369ms
2022-01-29 14:15:01 - epoch 031 lr: 0.010000000000000002
2022-01-29 14:15:37 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 1.9375
2022-01-29 14:16:11 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.6747
2022-01-29 14:16:43 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.7481
2022-01-29 14:17:14 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.8413
2022-01-29 14:17:46 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.6901
2022-01-29 14:18:18 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.6056
2022-01-29 14:18:49 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.6332
2022-01-29 14:19:22 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.6035
2022-01-29 14:19:53 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.6663
2022-01-29 14:20:26 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 1.8347
2022-01-29 14:20:58 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 1.8882
2022-01-29 14:21:30 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.7000
2022-01-29 14:22:02 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.4762
2022-01-29 14:22:33 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.6321
2022-01-29 14:23:05 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.7636
2022-01-29 14:23:37 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.4921
2022-01-29 14:24:08 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.5285
2022-01-29 14:24:40 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.6288
2022-01-29 14:25:13 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.7095
2022-01-29 14:25:43 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.6373
2022-01-29 14:26:16 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.4357
2022-01-29 14:26:47 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.4502
2022-01-29 14:27:19 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.4359
2022-01-29 14:27:50 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.5494
2022-01-29 14:28:22 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.4645
2022-01-29 14:28:54 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.5844
2022-01-29 14:29:26 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.6508
2022-01-29 14:29:59 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.8090
2022-01-29 14:30:31 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.6311
2022-01-29 14:31:02 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.6551
2022-01-29 14:31:33 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.4040
2022-01-29 14:32:04 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.6912
2022-01-29 14:32:36 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.6838
2022-01-29 14:33:07 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.7099
2022-01-29 14:33:40 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.6995
2022-01-29 14:34:10 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.6350
2022-01-29 14:34:42 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.4666
2022-01-29 14:35:13 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.4358
2022-01-29 14:35:45 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.5367
2022-01-29 14:36:18 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.5199
2022-01-29 14:36:49 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.5102
2022-01-29 14:37:21 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.6415
2022-01-29 14:37:54 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.4678
2022-01-29 14:38:26 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.6498
2022-01-29 14:38:58 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.6662
2022-01-29 14:39:30 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.5776
2022-01-29 14:40:02 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.5879
2022-01-29 14:40:35 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.4180
2022-01-29 14:41:07 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.4975
2022-01-29 14:41:39 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.4585
2022-01-29 14:41:40 - train: epoch 031, train_loss: 1.6059
2022-01-29 14:42:54 - eval: epoch: 031, acc1: 68.738%, acc5: 89.032%, test_loss: 1.2579, per_image_load_time: 2.450ms, per_image_inference_time: 0.367ms
2022-01-29 14:42:54 - epoch 032 lr: 0.010000000000000002
2022-01-29 14:43:32 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.4889
2022-01-29 14:44:03 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.5880
2022-01-29 14:44:36 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.5418
2022-01-29 14:45:08 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.7165
2022-01-29 14:45:39 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.5837
2022-01-29 14:46:10 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.5310
2022-01-29 14:46:42 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.5194
2022-01-29 14:47:13 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.5774
2022-01-29 14:47:46 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.4234
2022-01-29 14:48:19 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.6273
2022-01-29 14:48:52 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.5484
2022-01-29 14:49:24 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.4325
2022-01-29 14:49:56 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.4801
2022-01-29 14:50:28 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.6263
2022-01-29 14:51:00 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.5410
2022-01-29 14:51:32 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.5880
2022-01-29 14:52:04 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.5935
2022-01-29 14:52:37 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.7604
2022-01-29 14:53:09 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.3659
2022-01-29 14:53:42 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.4749
2022-01-29 14:54:14 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.3957
2022-01-29 14:54:46 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.3811
2022-01-29 14:55:18 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.5305
2022-01-29 14:55:50 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.4138
2022-01-29 14:56:23 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.6911
2022-01-29 14:56:56 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.2850
2022-01-29 14:57:28 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.4485
2022-01-29 14:58:01 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.6052
2022-01-29 14:58:33 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.2849
2022-01-29 14:59:05 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.2823
2022-01-29 14:59:37 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.4746
2022-01-29 15:00:10 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.4565
2022-01-29 15:00:41 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.3817
2022-01-29 15:01:14 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.2900
2022-01-29 15:01:46 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.5280
2022-01-29 15:02:19 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.5005
2022-01-29 15:02:51 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.4937
2022-01-29 15:03:24 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.3428
2022-01-29 15:03:55 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.5247
2022-01-29 15:04:28 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.4144
2022-01-29 15:05:00 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.4122
2022-01-29 15:05:33 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.3269
2022-01-29 15:06:04 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.6786
2022-01-29 15:06:37 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.4445
2022-01-29 15:07:09 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.6986
2022-01-29 15:07:41 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.4612
2022-01-29 15:08:13 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.5692
2022-01-29 15:08:46 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.3722
2022-01-29 15:09:18 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.6816
2022-01-29 15:09:49 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.4661
2022-01-29 15:09:51 - train: epoch 032, train_loss: 1.4867
2022-01-29 15:11:03 - eval: epoch: 032, acc1: 69.676%, acc5: 89.588%, test_loss: 1.2112, per_image_load_time: 2.473ms, per_image_inference_time: 0.372ms
2022-01-29 15:11:04 - epoch 033 lr: 0.010000000000000002
2022-01-29 15:11:42 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.2890
2022-01-29 15:12:13 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.5718
2022-01-29 15:12:45 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.3518
2022-01-29 15:13:17 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.3764
2022-01-29 15:13:49 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.5146
2022-01-29 15:14:20 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.2760
2022-01-29 15:14:52 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.5266
2022-01-29 15:15:24 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.5187
2022-01-29 15:15:55 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.4834
2022-01-29 15:16:28 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.5170
2022-01-29 15:16:59 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.4373
2022-01-29 15:17:30 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.4362
2022-01-29 15:18:02 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.3463
2022-01-29 15:18:34 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.6202
2022-01-29 15:19:05 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.6531
2022-01-29 15:19:37 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.5276
2022-01-29 15:20:08 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.2587
2022-01-29 15:20:40 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.6468
2022-01-29 15:21:12 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.5118
2022-01-29 15:21:44 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.2710
2022-01-29 15:22:16 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.5406
2022-01-29 15:22:47 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.4913
2022-01-29 15:23:20 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.3749
2022-01-29 15:23:52 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.7178
2022-01-29 15:24:23 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.4558
2022-01-29 15:24:56 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.2610
2022-01-29 15:25:27 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.5860
2022-01-29 15:25:59 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.3466
2022-01-29 15:26:31 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.5323
2022-01-29 15:27:03 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.4932
2022-01-29 15:27:35 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.4883
2022-01-29 15:28:07 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.4420
2022-01-29 15:28:38 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.5312
2022-01-29 15:29:11 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.3617
2022-01-29 15:29:42 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.4479
2022-01-29 15:30:14 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.6918
2022-01-29 15:30:46 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.4412
2022-01-29 15:31:18 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.3197
2022-01-29 15:31:50 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.4937
2022-01-29 15:32:22 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.5117
2022-01-29 15:32:54 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.4188
2022-01-29 15:33:27 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.3013
2022-01-29 15:33:59 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.6334
2022-01-29 15:34:30 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.4964
2022-01-29 15:35:02 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.5933
2022-01-29 15:35:34 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.3329
2022-01-29 15:36:07 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.2595
2022-01-29 15:36:39 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.7634
2022-01-29 15:37:12 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.2723
2022-01-29 15:37:44 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.4089
2022-01-29 15:37:45 - train: epoch 033, train_loss: 1.4384
2022-01-29 15:38:58 - eval: epoch: 033, acc1: 70.136%, acc5: 89.958%, test_loss: 1.1846, per_image_load_time: 2.459ms, per_image_inference_time: 0.378ms
2022-01-29 15:38:59 - epoch 034 lr: 0.010000000000000002
2022-01-29 15:39:36 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.3643
2022-01-29 15:40:08 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.3637
2022-01-29 15:40:39 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.4094
2022-01-29 15:41:10 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.2233
2022-01-29 15:41:42 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.4640
2022-01-29 15:42:13 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.6664
2022-01-29 15:42:45 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.3738
2022-01-29 15:43:15 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.3638
2022-01-29 15:43:48 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.3548
2022-01-29 15:44:20 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.3653
2022-01-29 15:44:52 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.4194
2022-01-29 15:45:24 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.3864
2022-01-29 15:45:56 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.3320
2022-01-29 15:46:27 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.4563
2022-01-29 15:46:59 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.2908
2022-01-29 15:47:31 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.2953
2022-01-29 15:48:03 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.3682
2022-01-29 15:48:36 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.5862
2022-01-29 15:49:08 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.4973
2022-01-29 15:49:40 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.4635
2022-01-29 15:50:11 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.5735
2022-01-29 15:50:44 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.3050
2022-01-29 15:51:16 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.4990
2022-01-29 15:51:48 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.3240
2022-01-29 15:52:20 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.2957
2022-01-29 15:52:53 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.5009
2022-01-29 15:53:25 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.4416
2022-01-29 15:53:57 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.1906
2022-01-29 15:54:29 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.1625
2022-01-29 15:55:00 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.2724
2022-01-29 15:55:32 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.3748
2022-01-29 15:56:04 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.4016
2022-01-29 15:56:36 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.3911
2022-01-29 15:57:07 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.5155
2022-01-29 15:57:40 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.2744
2022-01-29 15:58:11 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.2454
2022-01-29 15:58:43 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.3190
2022-01-29 15:59:15 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.3610
2022-01-29 15:59:47 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.4581
2022-01-29 16:00:18 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.2282
2022-01-29 16:00:50 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.3949
2022-01-29 16:01:22 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.3600
2022-01-29 16:01:53 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.3823
2022-01-29 16:02:25 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.4461
2022-01-29 16:02:57 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.5248
2022-01-29 16:03:29 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.5056
2022-01-29 16:04:00 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.3927
2022-01-29 16:04:33 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.3930
2022-01-29 16:05:05 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.3497
2022-01-29 16:05:36 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.3029
2022-01-29 16:05:38 - train: epoch 034, train_loss: 1.4104
2022-01-29 16:06:51 - eval: epoch: 034, acc1: 69.976%, acc5: 89.816%, test_loss: 1.1986, per_image_load_time: 2.447ms, per_image_inference_time: 0.366ms
2022-01-29 16:06:52 - epoch 035 lr: 0.010000000000000002
2022-01-29 16:07:29 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.2434
2022-01-29 16:08:01 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.1580
2022-01-29 16:08:33 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.5918
2022-01-29 16:09:05 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.3274
2022-01-29 16:09:37 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.3895
2022-01-29 16:10:08 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.3036
2022-01-29 16:10:40 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.2797
2022-01-29 16:11:11 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.3879
2022-01-29 16:11:43 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.4305
2022-01-29 16:12:16 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.5057
2022-01-29 16:12:48 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.5747
2022-01-29 16:13:20 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.4130
2022-01-29 16:13:53 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.4249
2022-01-29 16:14:25 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.4763
2022-01-29 16:14:57 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.4959
2022-01-29 16:15:29 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.4072
2022-01-29 16:16:02 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.2642
2022-01-29 16:16:35 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.3868
2022-01-29 16:17:07 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.4455
2022-01-29 16:17:40 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.3769
2022-01-29 16:18:12 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.3443
2022-01-29 16:18:44 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.5344
2022-01-29 16:19:17 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.3654
2022-01-29 16:19:48 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.4977
2022-01-29 16:20:22 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.3393
2022-01-29 16:20:53 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.5806
2022-01-29 16:21:26 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.5293
2022-01-29 16:21:58 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.3275
2022-01-29 16:22:30 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.2953
2022-01-29 16:23:03 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.4518
2022-01-29 16:23:35 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.4090
2022-01-29 16:24:07 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.2958
2022-01-29 16:24:40 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.2756
2022-01-29 16:25:12 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.4374
2022-01-29 16:25:44 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.1746
2022-01-29 16:26:16 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.4373
2022-01-29 16:26:50 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.1855
2022-01-29 16:27:21 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.4228
2022-01-29 16:27:54 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.4584
2022-01-29 16:28:26 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.1873
2022-01-29 16:28:59 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.6539
2022-01-29 16:29:31 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.2993
2022-01-29 16:30:04 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.3565
2022-01-29 16:30:36 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.4771
2022-01-29 16:31:10 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.5049
2022-01-29 16:31:42 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.3506
2022-01-29 16:32:15 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.6968
2022-01-29 16:32:48 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.5856
2022-01-29 16:33:21 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.4253
2022-01-29 16:33:53 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.3221
2022-01-29 16:33:54 - train: epoch 035, train_loss: 1.3898
2022-01-29 16:35:08 - eval: epoch: 035, acc1: 70.658%, acc5: 90.128%, test_loss: 1.1714, per_image_load_time: 2.471ms, per_image_inference_time: 0.377ms
2022-01-29 16:35:09 - epoch 036 lr: 0.010000000000000002
2022-01-29 16:35:46 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.5338
2022-01-29 16:36:19 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.1451
2022-01-29 16:36:51 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.2554
2022-01-29 16:37:23 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.3959
2022-01-29 16:37:54 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.4060
2022-01-29 16:38:27 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.2894
2022-01-29 16:38:59 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.2300
2022-01-29 16:39:32 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.1572
2022-01-29 16:40:03 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.3532
2022-01-29 16:40:36 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.3865
2022-01-29 16:41:08 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.5211
2022-01-29 16:41:41 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.3825
2022-01-29 16:42:12 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.3774
2022-01-29 16:42:45 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.5513
2022-01-29 16:43:17 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.5877
2022-01-29 16:43:50 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.3619
2022-01-29 16:44:22 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.3709
2022-01-29 16:44:54 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.2244
2022-01-29 16:45:26 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.3541
2022-01-29 16:45:58 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.3562
2022-01-29 16:46:31 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.4336
2022-01-29 16:47:03 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.3482
2022-01-29 16:47:35 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.4619
2022-01-29 16:48:07 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.5215
2022-01-29 16:48:39 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.2011
2022-01-29 16:49:11 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.4205
2022-01-29 16:49:43 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.3075
2022-01-29 16:50:16 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.2550
2022-01-29 16:50:48 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.2474
2022-01-29 16:51:20 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.4986
2022-01-29 16:51:52 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.3705
2022-01-29 16:52:24 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.3811
2022-01-29 16:52:56 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.2479
2022-01-29 16:53:29 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.2955
2022-01-29 16:54:01 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.3219
2022-01-29 16:54:33 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.4564
2022-01-29 16:55:05 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.4160
2022-01-29 16:55:38 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.2150
2022-01-29 16:56:11 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.3475
2022-01-29 16:56:43 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.4493
2022-01-29 16:57:15 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.3183
2022-01-29 16:57:47 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.3627
2022-01-29 16:58:20 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.2903
2022-01-29 16:58:52 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.4644
2022-01-29 16:59:26 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.1519
2022-01-29 16:59:58 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.2594
2022-01-29 17:00:32 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.3231
2022-01-29 17:01:04 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.4282
2022-01-29 17:01:38 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.3584
2022-01-29 17:02:10 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.4080
2022-01-29 17:02:11 - train: epoch 036, train_loss: 1.3754
2022-01-29 17:03:26 - eval: epoch: 036, acc1: 70.514%, acc5: 90.116%, test_loss: 1.1751, per_image_load_time: 2.535ms, per_image_inference_time: 0.373ms
2022-01-29 17:03:27 - epoch 037 lr: 0.010000000000000002
2022-01-29 17:04:04 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.1873
2022-01-29 17:04:37 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.1670
2022-01-29 17:05:09 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.2786
2022-01-29 17:05:42 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.4672
2022-01-29 17:06:14 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.2938
2022-01-29 17:06:46 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.4772
2022-01-29 17:07:18 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.4853
2022-01-29 17:07:50 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.3449
2022-01-29 17:08:24 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.6569
2022-01-29 17:08:56 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.2804
2022-01-29 17:09:29 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.3917
2022-01-29 17:10:01 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.3482
2022-01-29 17:10:33 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.3331
2022-01-29 17:11:05 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.4169
2022-01-29 17:11:38 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.3085
2022-01-29 17:12:10 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.1658
2022-01-29 17:12:42 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.6371
2022-01-29 17:13:14 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.4496
2022-01-29 17:13:46 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.4539
2022-01-29 17:14:18 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.3911
2022-01-29 17:14:51 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.4057
2022-01-29 17:15:24 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.3692
2022-01-29 17:15:56 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.1949
2022-01-29 17:16:28 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.3716
2022-01-29 17:17:00 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.3023
2022-01-29 17:17:33 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.5896
2022-01-29 17:18:05 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.4620
2022-01-29 17:18:38 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.3962
2022-01-29 17:19:10 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.4896
2022-01-29 17:19:42 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.4138
2022-01-29 17:20:15 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.3263
2022-01-29 17:20:47 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.4360
2022-01-29 17:21:20 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.2093
2022-01-29 17:21:52 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.2372
2022-01-29 17:22:25 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.3842
2022-01-29 17:22:57 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.5199
2022-01-29 17:23:29 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.4649
2022-01-29 17:24:02 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.2902
2022-01-29 17:24:34 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.5481
2022-01-29 17:25:07 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.2395
2022-01-29 17:25:39 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.4025
2022-01-29 17:26:11 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.5481
2022-01-29 17:26:44 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.4034
2022-01-29 17:27:15 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.3179
2022-01-29 17:27:48 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.2531
2022-01-29 17:28:20 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.3747
2022-01-29 17:28:54 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.3465
2022-01-29 17:29:26 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.3390
2022-01-29 17:29:59 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.3649
2022-01-29 17:30:31 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.3484
2022-01-29 17:30:32 - train: epoch 037, train_loss: 1.3696
2022-01-29 17:31:46 - eval: epoch: 037, acc1: 70.926%, acc5: 90.198%, test_loss: 1.1662, per_image_load_time: 2.480ms, per_image_inference_time: 0.391ms
2022-01-29 17:31:47 - epoch 038 lr: 0.010000000000000002
2022-01-29 17:32:23 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.3146
2022-01-29 17:32:55 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.0607
2022-01-29 17:33:27 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.0233
2022-01-29 17:33:59 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.2796
2022-01-29 17:34:32 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.2459
2022-01-29 17:35:04 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.3794
2022-01-29 17:35:37 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.2168
2022-01-29 17:36:09 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.2718
2022-01-29 17:36:41 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.3127
2022-01-29 17:37:14 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.5805
2022-01-29 17:37:46 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.2818
2022-01-29 17:38:19 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.2902
2022-01-29 17:38:51 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.5166
2022-01-29 17:39:23 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.4083
2022-01-29 17:39:56 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.3784
2022-01-29 17:40:28 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.4011
2022-01-29 17:41:01 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.4883
2022-01-29 17:41:33 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.4532
2022-01-29 17:42:05 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.4075
2022-01-29 17:42:37 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.3355
2022-01-29 17:43:10 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.4258
2022-01-29 17:43:42 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.2571
2022-01-29 17:44:14 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.4162
2022-01-29 17:44:47 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.5854
2022-01-29 17:45:19 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.2759
2022-01-29 17:45:51 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.3667
2022-01-29 17:46:24 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.4499
2022-01-29 17:46:56 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.4715
2022-01-29 17:47:29 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.4992
2022-01-29 17:48:01 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.3373
2022-01-29 17:48:34 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.3441
2022-01-29 17:49:06 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.1128
2022-01-29 17:49:38 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.1387
2022-01-29 17:50:11 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.2949
2022-01-29 17:50:43 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.4518
2022-01-29 17:51:15 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.4555
2022-01-29 17:51:47 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.1580
2022-01-29 17:52:20 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.4731
2022-01-29 17:52:53 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.3834
2022-01-29 17:53:25 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.3240
2022-01-29 17:53:58 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.3347
2022-01-29 17:54:30 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.1923
2022-01-29 17:55:02 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.4532
2022-01-29 17:55:35 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.4529
2022-01-29 17:56:08 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.3778
2022-01-29 17:56:41 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.4965
2022-01-29 17:57:14 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.2113
2022-01-29 17:57:46 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.4171
2022-01-29 17:58:19 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.3800
2022-01-29 17:58:50 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.1331
2022-01-29 17:58:51 - train: epoch 038, train_loss: 1.3623
2022-01-29 18:00:04 - eval: epoch: 038, acc1: 70.332%, acc5: 89.860%, test_loss: 1.1900, per_image_load_time: 2.470ms, per_image_inference_time: 0.378ms
2022-01-29 18:00:05 - epoch 039 lr: 0.010000000000000002
2022-01-29 18:00:42 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.4041
2022-01-29 18:01:14 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.6069
2022-01-29 18:01:46 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.1747
2022-01-29 18:02:18 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.3878
2022-01-29 18:02:49 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.2336
2022-01-29 18:03:21 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.1713
2022-01-29 18:03:52 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.4746
2022-01-29 18:04:25 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.3470
2022-01-29 18:04:56 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.4614
2022-01-29 18:05:28 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.3217
2022-01-29 18:06:00 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.4401
2022-01-29 18:06:32 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.3311
2022-01-29 18:07:03 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.6318
2022-01-29 18:07:34 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.5039
2022-01-29 18:08:06 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.3252
2022-01-29 18:08:38 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.5139
2022-01-29 18:09:10 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.1502
2022-01-29 18:09:42 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.3965
2022-01-29 18:10:13 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.1555
2022-01-29 18:10:45 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.2247
2022-01-29 18:11:16 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.3563
2022-01-29 18:11:48 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.3368
2022-01-29 18:12:19 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.5951
2022-01-29 18:12:50 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.4521
2022-01-29 18:13:22 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.3248
2022-01-29 18:13:54 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.3592
2022-01-29 18:14:26 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.5509
2022-01-29 18:14:57 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.2262
2022-01-29 18:15:30 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.1607
2022-01-29 18:16:02 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.4775
2022-01-29 18:16:33 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.2881
2022-01-29 18:17:05 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.3871
2022-01-29 18:17:36 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.4568
2022-01-29 18:18:09 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.4979
2022-01-29 18:18:41 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.5118
2022-01-29 18:19:13 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.5445
2022-01-29 18:19:45 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.3255
2022-01-29 18:20:17 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.1883
2022-01-29 18:20:48 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.5259
2022-01-29 18:21:21 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.4044
2022-01-29 18:21:52 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.4614
2022-01-29 18:22:24 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.3475
2022-01-29 18:22:56 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.3805
2022-01-29 18:23:28 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.1374
2022-01-29 18:23:59 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.2689
2022-01-29 18:24:31 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.5334
2022-01-29 18:25:02 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.3818
2022-01-29 18:25:33 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.3597
2022-01-29 18:26:04 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.2275
2022-01-29 18:26:35 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.2923
2022-01-29 18:26:36 - train: epoch 039, train_loss: 1.3596
2022-01-29 18:27:46 - eval: epoch: 039, acc1: 70.068%, acc5: 89.906%, test_loss: 1.1897, per_image_load_time: 0.841ms, per_image_inference_time: 0.371ms
2022-01-29 18:27:47 - epoch 040 lr: 0.010000000000000002
2022-01-29 18:28:24 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.5949
2022-01-29 18:28:56 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.5823
2022-01-29 18:29:27 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.4218
2022-01-29 18:29:58 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.3114
2022-01-29 18:30:29 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.2746
2022-01-29 18:31:01 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.4585
2022-01-29 18:31:32 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.3553
2022-01-29 18:32:03 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.4725
2022-01-29 18:32:34 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.2028
2022-01-29 18:33:06 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.0524
2022-01-29 18:33:37 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.3037
2022-01-29 18:34:09 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.2902
2022-01-29 18:34:39 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.2051
2022-01-29 18:35:10 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.2146
2022-01-29 18:35:41 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.5465
2022-01-29 18:36:13 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.4021
2022-01-29 18:36:44 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.4332
2022-01-29 18:37:14 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.1536
2022-01-29 18:37:45 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.3252
2022-01-29 18:38:17 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.3365
2022-01-29 18:38:48 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.3118
2022-01-29 18:39:19 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.2184
2022-01-29 18:39:50 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.2664
2022-01-29 18:40:22 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.3727
2022-01-29 18:40:53 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.4077
2022-01-29 18:41:24 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.2575
2022-01-29 18:41:55 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.4225
2022-01-29 18:42:26 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.3489
2022-01-29 18:42:57 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.6017
2022-01-29 18:43:28 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.5645
2022-01-29 18:44:00 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.2703
2022-01-29 18:44:30 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.5252
2022-01-29 18:45:01 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.3989
2022-01-29 18:45:33 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.3809
2022-01-29 18:46:04 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.4842
2022-01-29 18:46:34 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.3657
2022-01-29 18:47:06 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.3335
2022-01-29 18:47:37 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.2647
2022-01-29 18:48:07 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.4543
2022-01-29 18:48:39 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.4539
2022-01-29 18:49:11 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.5359
2022-01-29 18:49:41 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.2364
2022-01-29 18:50:13 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.4399
2022-01-29 18:50:44 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.4148
2022-01-29 18:51:16 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.1801
2022-01-29 18:51:47 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.3777
2022-01-29 18:52:19 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.4209
2022-01-29 18:52:50 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.2315
2022-01-29 18:53:22 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.5185
2022-01-29 18:53:52 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.4557
2022-01-29 18:53:53 - train: epoch 040, train_loss: 1.3577
2022-01-29 18:55:05 - eval: epoch: 040, acc1: 70.304%, acc5: 90.158%, test_loss: 1.1755, per_image_load_time: 2.419ms, per_image_inference_time: 0.367ms
2022-01-29 18:55:06 - epoch 041 lr: 0.010000000000000002
2022-01-29 18:55:43 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.4050
2022-01-29 18:56:14 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.4817
2022-01-29 18:56:44 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.3032
2022-01-29 18:57:16 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.3498
2022-01-29 18:57:46 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.0901
2022-01-29 18:58:18 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.4164
2022-01-29 18:58:49 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.2639
2022-01-29 18:59:20 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.0248
2022-01-29 18:59:51 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.1835
2022-01-29 19:00:23 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.5664
2022-01-29 19:00:54 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.2430
2022-01-29 19:01:24 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.1106
2022-01-29 19:01:55 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.3484
2022-01-29 19:02:27 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.4682
2022-01-29 19:02:57 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.6645
2022-01-29 19:03:28 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.1477
2022-01-29 19:03:59 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.3614
2022-01-29 19:04:31 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.3250
2022-01-29 19:05:01 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.3352
2022-01-29 19:05:33 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.3234
2022-01-29 19:06:04 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.2085
2022-01-29 19:06:34 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.2236
2022-01-29 19:07:06 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.1366
2022-01-29 19:07:38 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.5129
2022-01-29 19:08:08 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.2868
2022-01-29 19:08:39 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.4909
2022-01-29 19:09:11 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.5949
2022-01-29 19:09:42 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.3085
2022-01-29 19:10:14 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.3858
2022-01-29 19:10:44 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.4198
2022-01-29 19:11:16 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.5053
2022-01-29 19:11:47 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.2258
2022-01-29 19:12:19 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.1993
2022-01-29 19:12:50 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.2785
2022-01-29 19:13:22 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.4339
2022-01-29 19:13:53 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.4037
2022-01-29 19:14:23 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.3603
2022-01-29 19:14:54 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.2418
2022-01-29 19:15:26 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.4465
2022-01-29 19:15:57 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.2472
2022-01-29 19:16:29 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.3832
2022-01-29 19:16:59 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.2889
2022-01-29 19:17:30 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.4170
2022-01-29 19:18:02 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.2522
2022-01-29 19:18:33 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.3847
2022-01-29 19:19:04 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.3392
2022-01-29 19:19:37 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.6088
2022-01-29 19:20:08 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.3690
2022-01-29 19:20:41 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.3604
2022-01-29 19:21:12 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.4100
2022-01-29 19:21:13 - train: epoch 041, train_loss: 1.3594
2022-01-29 19:22:25 - eval: epoch: 041, acc1: 70.694%, acc5: 90.200%, test_loss: 1.1696, per_image_load_time: 1.115ms, per_image_inference_time: 0.403ms
2022-01-29 19:22:26 - epoch 042 lr: 0.010000000000000002
2022-01-29 19:23:04 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.0599
2022-01-29 19:23:35 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.3999
2022-01-29 19:24:08 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.3872
2022-01-29 19:24:39 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.1487
2022-01-29 19:25:10 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.4445
2022-01-29 19:25:43 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.1834
2022-01-29 19:26:14 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.2540
2022-01-29 19:26:46 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.3024
2022-01-29 19:27:18 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.5166
2022-01-29 19:27:49 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.4342
2022-01-29 19:28:21 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.1557
2022-01-29 19:28:53 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.3262
2022-01-29 19:29:25 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.4501
2022-01-29 19:29:57 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.6307
2022-01-29 19:30:28 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.2830
2022-01-29 19:31:01 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.4685
2022-01-29 19:31:33 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.3592
2022-01-29 19:32:03 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.3572
2022-01-29 19:32:35 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.3802
2022-01-29 19:33:06 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.2547
2022-01-29 19:33:37 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.1883
2022-01-29 19:34:10 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.2003
2022-01-29 19:34:41 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.3038
2022-01-29 19:35:13 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.6254
2022-01-29 19:35:45 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.4541
2022-01-29 19:36:17 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.4835
2022-01-29 19:36:48 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.2042
2022-01-29 19:37:20 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.3329
2022-01-29 19:37:52 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.3536
2022-01-29 19:38:24 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.4046
2022-01-29 19:38:55 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.3077
2022-01-29 19:39:27 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.3571
2022-01-29 19:39:59 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.4454
2022-01-29 19:40:31 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.2620
2022-01-29 19:41:02 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.2577
2022-01-29 19:41:34 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.4941
2022-01-29 19:42:05 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.4071
2022-01-29 19:42:36 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.2450
2022-01-29 19:43:08 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.2396
2022-01-29 19:43:39 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.4658
2022-01-29 19:44:12 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.5003
2022-01-29 19:44:43 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.4596
2022-01-29 19:45:16 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.1752
2022-01-29 19:45:46 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.2981
2022-01-29 19:46:18 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.3737
2022-01-29 19:46:51 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.6531
2022-01-29 19:47:22 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.1914
2022-01-29 19:47:54 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.5064
2022-01-29 19:48:26 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.4555
2022-01-29 19:48:58 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.3123
2022-01-29 19:48:59 - train: epoch 042, train_loss: 1.3584
2022-01-29 19:50:12 - eval: epoch: 042, acc1: 70.098%, acc5: 89.830%, test_loss: 1.1968, per_image_load_time: 2.404ms, per_image_inference_time: 0.401ms
2022-01-29 19:50:13 - epoch 043 lr: 0.010000000000000002
2022-01-29 19:50:49 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.3687
2022-01-29 19:51:22 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.4274
2022-01-29 19:51:55 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.0873
2022-01-29 19:52:27 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.2350
2022-01-29 19:52:59 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.3517
2022-01-29 19:53:31 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.2502
2022-01-29 19:54:03 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.5409
2022-01-29 19:54:34 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.4739
2022-01-29 19:55:06 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.3054
2022-01-29 19:55:38 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.4715
2022-01-29 19:56:10 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.4373
2022-01-29 19:56:42 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.4786
2022-01-29 19:57:14 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.4383
2022-01-29 19:57:46 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.3761
2022-01-29 19:58:17 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.2022
2022-01-29 19:58:49 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.3458
2022-01-29 19:59:21 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.4645
2022-01-29 19:59:53 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.4092
2022-01-29 20:00:25 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.3887
2022-01-29 20:00:57 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.1664
2022-01-29 20:01:28 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.3702
2022-01-29 20:02:00 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.3982
2022-01-29 20:02:31 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.5662
2022-01-29 20:03:04 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.2336
2022-01-29 20:03:34 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.5476
2022-01-29 20:04:06 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.0827
2022-01-29 20:04:39 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.4151
2022-01-29 20:05:11 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.2606
2022-01-29 20:05:43 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.3957
2022-01-29 20:06:14 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.5983
2022-01-29 20:06:46 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.5756
2022-01-29 20:07:18 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.4288
2022-01-29 20:07:50 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.4378
2022-01-29 20:08:22 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.4146
2022-01-29 20:08:53 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.3851
2022-01-29 20:09:25 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.3470
2022-01-29 20:09:57 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.4221
2022-01-29 20:10:29 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.4583
2022-01-29 20:11:01 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.4505
2022-01-29 20:11:32 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.3597
2022-01-29 20:12:04 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.6087
2022-01-29 20:12:35 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.3848
2022-01-29 20:13:06 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.3850
2022-01-29 20:13:38 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.3057
2022-01-29 20:14:09 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.3755
2022-01-29 20:14:41 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.3368
2022-01-29 20:15:13 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.2728
2022-01-29 20:15:46 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.3056
2022-01-29 20:16:18 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.3627
2022-01-29 20:16:49 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.2428
2022-01-29 20:16:51 - train: epoch 043, train_loss: 1.3570
2022-01-29 20:18:03 - eval: epoch: 043, acc1: 69.854%, acc5: 89.914%, test_loss: 1.1928, per_image_load_time: 2.431ms, per_image_inference_time: 0.382ms
2022-01-29 20:18:04 - epoch 044 lr: 0.010000000000000002
2022-01-29 20:18:42 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.3820
2022-01-29 20:19:13 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.4714
2022-01-29 20:19:46 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.4324
2022-01-29 20:20:17 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.0584
2022-01-29 20:20:49 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.2753
2022-01-29 20:21:21 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.1335
2022-01-29 20:21:53 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.1106
2022-01-29 20:22:25 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.2396
2022-01-29 20:22:56 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.3073
2022-01-29 20:23:28 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.3763
2022-01-29 20:24:00 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.3237
2022-01-29 20:24:32 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.2323
2022-01-29 20:25:03 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.4036
2022-01-29 20:25:36 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.2278
2022-01-29 20:26:08 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.3079
2022-01-29 20:26:39 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.2461
2022-01-29 20:27:11 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.3135
2022-01-29 20:27:42 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.2717
2022-01-29 20:28:14 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.4850
2022-01-29 20:28:46 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.2378
2022-01-29 20:29:18 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.4766
2022-01-29 20:29:50 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.3130
2022-01-29 20:30:21 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.4415
2022-01-29 20:30:53 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.3947
2022-01-29 20:31:25 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.3973
2022-01-29 20:31:56 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.4666
2022-01-29 20:32:29 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.5278
2022-01-29 20:33:00 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.0794
2022-01-29 20:33:32 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.1982
2022-01-29 20:34:05 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.3044
2022-01-29 20:34:36 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.4564
2022-01-29 20:35:08 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.0887
2022-01-29 20:35:39 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.2878
2022-01-29 20:36:11 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.5756
2022-01-29 20:36:43 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.2068
2022-01-29 20:37:14 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.5510
2022-01-29 20:37:46 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.4302
2022-01-29 20:38:18 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.0936
2022-01-29 20:38:50 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.4115
2022-01-29 20:39:22 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.5829
2022-01-29 20:39:53 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.2140
2022-01-29 20:40:26 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.3205
2022-01-29 20:40:57 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.5003
2022-01-29 20:41:29 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.2019
2022-01-29 20:42:01 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.3612
2022-01-29 20:42:33 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.2804
2022-01-29 20:43:05 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.4051
2022-01-29 20:43:37 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.5710
2022-01-29 20:44:09 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.2116
2022-01-29 20:44:41 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.3662
2022-01-29 20:44:42 - train: epoch 044, train_loss: 1.3585
2022-01-29 20:45:54 - eval: epoch: 044, acc1: 69.954%, acc5: 89.790%, test_loss: 1.2039, per_image_load_time: 1.850ms, per_image_inference_time: 0.386ms
2022-01-29 20:45:55 - epoch 045 lr: 0.010000000000000002
2022-01-29 20:46:33 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.2106
2022-01-29 20:47:05 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.4376
2022-01-29 20:47:36 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.4572
2022-01-29 20:48:07 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.2102
2022-01-29 20:48:39 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.4182
2022-01-29 20:49:12 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.3820
2022-01-29 20:49:45 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.1481
2022-01-29 20:50:16 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.3396
2022-01-29 20:50:48 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.3883
2022-01-29 20:51:21 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.2624
2022-01-29 20:51:53 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.4998
2022-01-29 20:52:26 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.3787
2022-01-29 20:52:58 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.4405
2022-01-29 20:53:30 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.2377
2022-01-29 20:54:02 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.5003
2022-01-29 20:54:34 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.3158
2022-01-29 20:55:07 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.1543
2022-01-29 20:55:39 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.2089
2022-01-29 20:56:12 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.3804
2022-01-29 20:56:44 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.4206
2022-01-29 20:57:16 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.5341
2022-01-29 20:57:48 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.4146
2022-01-29 20:58:20 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.2821
2022-01-29 20:58:52 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.4533
2022-01-29 20:59:24 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.3696
2022-01-29 20:59:57 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.4216
2022-01-29 21:00:29 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.2474
2022-01-29 21:01:02 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.3045
2022-01-29 21:01:34 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.3989
2022-01-29 21:02:06 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.4985
2022-01-29 21:02:39 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.3697
2022-01-29 21:03:10 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.4594
2022-01-29 21:03:43 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.2831
2022-01-29 21:04:14 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.2241
2022-01-29 21:04:47 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.4841
2022-01-29 21:05:18 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.4398
2022-01-29 21:05:50 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.3895
2022-01-29 21:06:23 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.2687
2022-01-29 21:06:56 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.5257
2022-01-29 21:07:28 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.4362
2022-01-29 21:08:00 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.4220
2022-01-29 21:08:33 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.4443
2022-01-29 21:09:05 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.6287
2022-01-29 21:09:37 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.4863
2022-01-29 21:10:11 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.2657
2022-01-29 21:10:44 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.4434
2022-01-29 21:11:17 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.4995
2022-01-29 21:11:49 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.2836
2022-01-29 21:12:22 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.3645
2022-01-29 21:12:55 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.4037
2022-01-29 21:12:56 - train: epoch 045, train_loss: 1.3550
2022-01-29 21:14:12 - eval: epoch: 045, acc1: 69.566%, acc5: 89.664%, test_loss: 1.2149, per_image_load_time: 2.559ms, per_image_inference_time: 0.374ms
2022-01-29 21:14:13 - epoch 046 lr: 0.010000000000000002
2022-01-29 21:14:51 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.1678
2022-01-29 21:15:24 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.2258
2022-01-29 21:15:57 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.3928
2022-01-29 21:16:29 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.5545
2022-01-29 21:17:01 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.1548
2022-01-29 21:17:34 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.4891
2022-01-29 21:18:06 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.2873
2022-01-29 21:18:39 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.4076
2022-01-29 21:19:11 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.4412
2022-01-29 21:19:43 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.1738
2022-01-29 21:20:16 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.3799
2022-01-29 21:20:48 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.3555
2022-01-29 21:21:20 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.4068
2022-01-29 21:21:53 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.4967
2022-01-29 21:22:25 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.2781
2022-01-29 21:22:58 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.3828
2022-01-29 21:23:30 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.2146
2022-01-29 21:24:02 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.4186
2022-01-29 21:24:35 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.1307
2022-01-29 21:25:07 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.2716
2022-01-29 21:25:39 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.3861
2022-01-29 21:26:12 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.1936
2022-01-29 21:26:44 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.5219
2022-01-29 21:27:17 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.3562
2022-01-29 21:27:49 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.4068
2022-01-29 21:28:22 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.4590
2022-01-29 21:28:54 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.2428
2022-01-29 21:29:26 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.0885
2022-01-29 21:29:59 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.4192
2022-01-29 21:30:32 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.1964
2022-01-29 21:31:04 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.1909
2022-01-29 21:31:36 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.5988
2022-01-29 21:32:08 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.3931
2022-01-29 21:32:41 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.3679
2022-01-29 21:33:14 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.4066
2022-01-29 21:33:46 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.2598
2022-01-29 21:34:20 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.2267
2022-01-29 21:34:52 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.4920
2022-01-29 21:35:27 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.3508
2022-01-29 21:35:59 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.2295
2022-01-29 21:36:33 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.5247
2022-01-29 21:37:06 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.2579
2022-01-29 21:37:39 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.5702
2022-01-29 21:38:11 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.4262
2022-01-29 21:38:45 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.2902
2022-01-29 21:39:18 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.4771
2022-01-29 21:39:52 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.2771
2022-01-29 21:40:25 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.3161
2022-01-29 21:40:59 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.4526
2022-01-29 21:41:32 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.3065
2022-01-29 21:41:33 - train: epoch 046, train_loss: 1.3550
2022-01-29 21:42:49 - eval: epoch: 046, acc1: 70.382%, acc5: 89.922%, test_loss: 1.1888, per_image_load_time: 2.581ms, per_image_inference_time: 0.374ms
2022-01-29 21:42:50 - epoch 047 lr: 0.010000000000000002
2022-01-29 21:43:28 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.2713
2022-01-29 21:44:01 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.4028
2022-01-29 21:44:34 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.2321
2022-01-29 21:45:06 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.2420
2022-01-29 21:45:39 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.4006
2022-01-29 21:46:11 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.3267
2022-01-29 21:46:43 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.4532
2022-01-29 21:47:15 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.1628
2022-01-29 21:47:47 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.4325
2022-01-29 21:48:20 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.3311
2022-01-29 21:48:52 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.3921
2022-01-29 21:49:23 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.2657
2022-01-29 21:49:56 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.3857
2022-01-29 21:50:29 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.3766
2022-01-29 21:51:01 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.2527
2022-01-29 21:51:33 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.2342
2022-01-29 21:52:04 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.2029
2022-01-29 21:52:38 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.3293
2022-01-29 21:53:09 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.2439
2022-01-29 21:53:41 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.3217
2022-01-29 21:54:12 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.5679
2022-01-29 21:54:44 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.5385
2022-01-29 21:55:17 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.3514
2022-01-29 21:55:50 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.1582
2022-01-29 21:56:22 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.3841
2022-01-29 21:56:55 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.5902
2022-01-29 21:57:27 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.3768
2022-01-29 21:57:59 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.5450
2022-01-29 21:58:31 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.2939
2022-01-29 21:59:04 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.4581
2022-01-29 21:59:35 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.3094
2022-01-29 22:00:07 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.6166
2022-01-29 22:00:39 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.2276
2022-01-29 22:01:09 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.3156
2022-01-29 22:01:41 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.4633
2022-01-29 22:02:14 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.4056
2022-01-29 22:02:45 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.4996
2022-01-29 22:03:17 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.3509
2022-01-29 22:03:49 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.2491
2022-01-29 22:04:22 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.3983
2022-01-29 22:04:55 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.4620
2022-01-29 22:05:26 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.3147
2022-01-29 22:06:00 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.2511
2022-01-29 22:06:31 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.3618
2022-01-29 22:07:03 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.4557
2022-01-29 22:07:35 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.3821
2022-01-29 22:08:07 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.4168
2022-01-29 22:08:39 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.2730
2022-01-29 22:09:11 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.4265
2022-01-29 22:09:43 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.2116
2022-01-29 22:09:44 - train: epoch 047, train_loss: 1.3557
2022-01-29 22:10:58 - eval: epoch: 047, acc1: 70.008%, acc5: 90.034%, test_loss: 1.1904, per_image_load_time: 2.487ms, per_image_inference_time: 0.372ms
2022-01-29 22:10:58 - epoch 048 lr: 0.010000000000000002
2022-01-29 22:11:36 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.5409
2022-01-29 22:12:07 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.6136
2022-01-29 22:12:38 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.3358
2022-01-29 22:13:10 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.3814
2022-01-29 22:13:42 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.2076
2022-01-29 22:14:13 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.3324
2022-01-29 22:14:45 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.4080
2022-01-29 22:15:17 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.4426
2022-01-29 22:15:48 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.5035
2022-01-29 22:16:19 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.3624
2022-01-29 22:16:52 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.3298
2022-01-29 22:17:23 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.4693
2022-01-29 22:17:55 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.1761
2022-01-29 22:18:25 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.2603
2022-01-29 22:18:58 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.4570
2022-01-29 22:19:29 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.3123
2022-01-29 22:20:01 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.4783
2022-01-29 22:20:33 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.4031
2022-01-29 22:21:04 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.4534
2022-01-29 22:21:36 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.5609
2022-01-29 22:22:08 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.3425
2022-01-29 22:22:39 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.5029
2022-01-29 22:23:10 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.1772
2022-01-29 22:23:41 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.4101
2022-01-29 22:24:13 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.3947
2022-01-29 22:24:45 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.5728
2022-01-29 22:25:16 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.5105
2022-01-29 22:25:47 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.2665
2022-01-29 22:26:19 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.4320
2022-01-29 22:26:51 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.3067
2022-01-29 22:27:22 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.3019
2022-01-29 22:27:54 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.1976
2022-01-29 22:28:25 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.4647
2022-01-29 22:28:57 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.2417
2022-01-29 22:29:29 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.5296
2022-01-29 22:30:00 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.3493
2022-01-29 22:30:33 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.4494
2022-01-29 22:31:05 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.2969
2022-01-29 22:31:37 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.4505
2022-01-29 22:32:09 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.1774
2022-01-29 22:32:41 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.5536
2022-01-29 22:33:13 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.3528
2022-01-29 22:33:45 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.3169
2022-01-29 22:34:17 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.2886
2022-01-29 22:34:49 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.3691
2022-01-29 22:35:21 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.2824
2022-01-29 22:35:53 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.3875
2022-01-29 22:36:25 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.6938
2022-01-29 22:36:58 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.4664
2022-01-29 22:37:29 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.3639
2022-01-29 22:37:30 - train: epoch 048, train_loss: 1.3566
2022-01-29 22:38:42 - eval: epoch: 048, acc1: 69.810%, acc5: 89.658%, test_loss: 1.2099, per_image_load_time: 2.417ms, per_image_inference_time: 0.371ms
2022-01-29 22:38:43 - epoch 049 lr: 0.010000000000000002
2022-01-29 22:39:20 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.4819
2022-01-29 22:39:51 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.2665
2022-01-29 22:40:22 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.4904
2022-01-29 22:40:54 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.4398
2022-01-29 22:41:25 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.3551
2022-01-29 22:41:56 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.3075
2022-01-29 22:42:26 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.3451
2022-01-29 22:42:58 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.6886
2022-01-29 22:43:30 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.2297
2022-01-29 22:44:01 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.3647
2022-01-29 22:44:32 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.2362
2022-01-29 22:45:04 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.0861
2022-01-29 22:45:35 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.4881
2022-01-29 22:46:07 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.4615
2022-01-29 22:46:37 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.3329
2022-01-29 22:47:10 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.4595
2022-01-29 22:47:41 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.4754
2022-01-29 22:48:13 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.3054
2022-01-29 22:48:44 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.3700
2022-01-29 22:49:16 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.2082
2022-01-29 22:49:47 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.2506
2022-01-29 22:50:19 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.3426
2022-01-29 22:50:50 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.2632
2022-01-29 22:51:21 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.4502
2022-01-29 22:51:53 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.3110
2022-01-29 22:52:23 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.3502
2022-01-29 22:52:53 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.2002
2022-01-29 22:53:24 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.3329
2022-01-29 22:53:56 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.4984
2022-01-29 22:54:27 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.2947
2022-01-29 22:54:59 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.4074
2022-01-29 22:55:31 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.3862
2022-01-29 22:56:03 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.3976
2022-01-29 22:56:34 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.5520
2022-01-29 22:57:05 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.3959
2022-01-29 22:57:37 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.5717
2022-01-29 22:58:09 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.4020
2022-01-29 22:58:40 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.5705
2022-01-29 22:59:11 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.4583
2022-01-29 22:59:43 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.3290
2022-01-29 23:00:14 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.2624
2022-01-29 23:00:45 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.4533
2022-01-29 23:01:18 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.6294
2022-01-29 23:01:49 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.3697
2022-01-29 23:02:21 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.2271
2022-01-29 23:02:51 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.3770
2022-01-29 23:03:24 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.4563
2022-01-29 23:03:56 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.2282
2022-01-29 23:04:28 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.2193
2022-01-29 23:05:00 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.2893
2022-01-29 23:05:01 - train: epoch 049, train_loss: 1.3513
2022-01-29 23:06:13 - eval: epoch: 049, acc1: 70.186%, acc5: 89.814%, test_loss: 1.1884, per_image_load_time: 2.436ms, per_image_inference_time: 0.381ms
2022-01-29 23:06:14 - epoch 050 lr: 0.010000000000000002
2022-01-29 23:06:51 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.5425
2022-01-29 23:07:22 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.3011
2022-01-29 23:07:54 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.3906
2022-01-29 23:08:25 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.1725
2022-01-29 23:08:57 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.4328
2022-01-29 23:09:29 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.3936
2022-01-29 23:10:00 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.2140
2022-01-29 23:10:31 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.1363
2022-01-29 23:11:03 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.2115
2022-01-29 23:11:34 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.4659
2022-01-29 23:12:06 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.4619
2022-01-29 23:12:37 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.3324
2022-01-29 23:13:08 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.1977
2022-01-29 23:13:40 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.4278
2022-01-29 23:14:12 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.3607
2022-01-29 23:14:44 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.2581
2022-01-29 23:15:16 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.4338
2022-01-29 23:15:47 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.3095
2022-01-29 23:16:19 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.3155
2022-01-29 23:16:50 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.3830
2022-01-29 23:17:21 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.1523
2022-01-29 23:17:53 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.4164
2022-01-29 23:18:25 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.2860
2022-01-29 23:18:56 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.3004
2022-01-29 23:19:28 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.3938
2022-01-29 23:19:58 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.2733
2022-01-29 23:20:30 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.3946
2022-01-29 23:21:01 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.5493
2022-01-29 23:21:32 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.5333
2022-01-29 23:22:04 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.4269
2022-01-29 23:22:36 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.2704
2022-01-29 23:23:07 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.3601
2022-01-29 23:23:38 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.2795
2022-01-29 23:24:10 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.1782
2022-01-29 23:24:41 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.3594
2022-01-29 23:25:13 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.2786
2022-01-29 23:25:44 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.4404
2022-01-29 23:26:16 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.2578
2022-01-29 23:26:47 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.1767
2022-01-29 23:27:19 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.4863
2022-01-29 23:27:50 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.3073
2022-01-29 23:28:22 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.3596
2022-01-29 23:28:53 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.2881
2022-01-29 23:29:24 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.2993
2022-01-29 23:29:56 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.3002
2022-01-29 23:30:28 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.3919
2022-01-29 23:30:59 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.3685
2022-01-29 23:31:31 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.1457
2022-01-29 23:32:03 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.2281
2022-01-29 23:32:34 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.2527
2022-01-29 23:32:35 - train: epoch 050, train_loss: 1.3490
2022-01-29 23:33:50 - eval: epoch: 050, acc1: 70.058%, acc5: 89.924%, test_loss: 1.1980, per_image_load_time: 2.467ms, per_image_inference_time: 0.385ms
2022-01-29 23:33:51 - epoch 051 lr: 0.010000000000000002
2022-01-29 23:34:28 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.5197
2022-01-29 23:35:00 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.6230
2022-01-29 23:35:33 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.3865
2022-01-29 23:36:05 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.0366
2022-01-29 23:36:37 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.3714
2022-01-29 23:37:09 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.3912
2022-01-29 23:37:41 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.2449
2022-01-29 23:38:13 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.5510
2022-01-29 23:38:45 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.2198
2022-01-29 23:39:18 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.5772
2022-01-29 23:39:50 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.4655
2022-01-29 23:40:22 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.2784
2022-01-29 23:40:54 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 0.9881
2022-01-29 23:41:27 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.2888
2022-01-29 23:41:59 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.3283
2022-01-29 23:42:32 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.2278
2022-01-29 23:43:03 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.4416
2022-01-29 23:43:35 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.3499
2022-01-29 23:44:06 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.1317
2022-01-29 23:44:38 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.3141
2022-01-29 23:45:10 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.3693
2022-01-29 23:45:42 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.3164
2022-01-29 23:46:13 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.3394
2022-01-29 23:46:45 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.3718
2022-01-29 23:47:18 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.3039
2022-01-29 23:47:51 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.2190
2022-01-29 23:48:23 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.3805
2022-01-29 23:48:56 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.2410
2022-01-29 23:49:28 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.3560
2022-01-29 23:50:00 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.2508
2022-01-29 23:50:32 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.2230
2022-01-29 23:51:04 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.3062
2022-01-29 23:51:37 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.4226
