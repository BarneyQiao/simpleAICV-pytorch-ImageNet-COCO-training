2022-06-25 21:24:13 - network: resnet50
2022-06-25 21:24:13 - num_classes: 1000
2022-06-25 21:24:13 - input_image_size: 224
2022-06-25 21:24:13 - scale: 1.1428571428571428
2022-06-25 21:24:13 - trained_model_path: 
2022-06-25 21:24:13 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-06-25 21:24:13 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-06-25 21:24:13 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f08ad2ada30>
2022-06-25 21:24:13 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f08ad2add00>
2022-06-25 21:24:13 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f08ad2add30>
2022-06-25 21:24:13 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f08ad2add90>
2022-06-25 21:24:13 - seed: 0
2022-06-25 21:24:13 - batch_size: 256
2022-06-25 21:24:13 - num_workers: 16
2022-06-25 21:24:13 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-06-25 21:24:13 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-06-25 21:24:13 - epochs: 100
2022-06-25 21:24:13 - print_interval: 100
2022-06-25 21:24:13 - sync_bn: False
2022-06-25 21:24:13 - apex: True
2022-06-25 21:24:13 - use_ema_model: False
2022-06-25 21:24:13 - ema_model_decay: 0.9999
2022-06-25 21:24:13 - gpus_type: NVIDIA RTX A5000
2022-06-25 21:24:13 - gpus_num: 2
2022-06-25 21:24:13 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f08760cdbb0>
2022-06-25 21:24:13 - --------------------parameters--------------------
2022-06-25 21:24:13 - name: conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-06-25 21:24:13 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-06-25 21:24:13 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-06-25 21:24:13 - name: fc.weight, grad: True
2022-06-25 21:24:13 - name: fc.bias, grad: True
2022-06-25 21:24:13 - --------------------buffers--------------------
2022-06-25 21:24:13 - name: conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-06-25 21:24:13 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-06-25 21:24:13 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-06-25 21:24:13 - -----------no weight decay layers--------------
2022-06-25 21:24:13 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-25 21:24:13 - -------------weight decay layers---------------
2022-06-25 21:24:13 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer1.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer2.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer3.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:13 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-25 21:24:14 - epoch 001 lr: 0.100000
2022-06-25 21:24:54 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9280
2022-06-25 21:25:28 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.8955
2022-06-25 21:26:03 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.8709
2022-06-25 21:26:35 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.8746
2022-06-25 21:27:09 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.7700
2022-06-25 21:27:41 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.5816
2022-06-25 21:28:15 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.6301
2022-06-25 21:28:48 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.4758
2022-06-25 21:29:22 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.3725
2022-06-25 21:29:55 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.3843
2022-06-25 21:30:28 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.2352
2022-06-25 21:31:02 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 6.0974
2022-06-25 21:31:35 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 6.1549
2022-06-25 21:32:09 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 6.0276
2022-06-25 21:32:43 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.8788
2022-06-25 21:33:16 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.8831
2022-06-25 21:33:49 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.6916
2022-06-25 21:34:22 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.7106
2022-06-25 21:34:57 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.5471
2022-06-25 21:35:30 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.5365
2022-06-25 21:36:02 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.4436
2022-06-25 21:36:37 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.3345
2022-06-25 21:37:10 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 5.2351
2022-06-25 21:37:43 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 5.2312
2022-06-25 21:38:16 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.2601
2022-06-25 21:38:50 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.3554
2022-06-25 21:39:24 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.2254
2022-06-25 21:39:57 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 5.0549
2022-06-25 21:40:30 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.9404
2022-06-25 21:41:04 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.0760
2022-06-25 21:41:38 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.1854
2022-06-25 21:42:11 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.9872
2022-06-25 21:42:45 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.7503
2022-06-25 21:43:18 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.7337
2022-06-25 21:43:51 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.7577
2022-06-25 21:44:25 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.7364
2022-06-25 21:44:59 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.8296
2022-06-25 21:45:32 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.6597
2022-06-25 21:46:05 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.7299
2022-06-25 21:46:39 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.6267
2022-06-25 21:47:12 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.6857
2022-06-25 21:47:45 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.5577
2022-06-25 21:48:18 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.4626
2022-06-25 21:48:52 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.2412
2022-06-25 21:49:26 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.4088
2022-06-25 21:50:00 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.6928
2022-06-25 21:50:34 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.2514
2022-06-25 21:51:07 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.5205
2022-06-25 21:51:40 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.2984
2022-06-25 21:52:13 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.2355
2022-06-25 21:52:14 - train: epoch 001, train_loss: 5.3974
2022-06-25 21:53:29 - eval: epoch: 001, acc1: 15.742%, acc5: 35.054%, test_loss: 4.4744, per_image_load_time: 2.456ms, per_image_inference_time: 0.461ms
2022-06-25 21:53:30 - until epoch: 001, best_acc1: 15.742%
2022-06-25 21:53:30 - epoch 002 lr: 0.100000
2022-06-25 21:54:09 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.2817
2022-06-25 21:54:42 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.0024
2022-06-25 21:55:15 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.2973
2022-06-25 21:55:48 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.1397
2022-06-25 21:56:21 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.9921
2022-06-25 21:56:54 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.0141
2022-06-25 21:57:27 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.2194
2022-06-25 21:58:01 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.8221
2022-06-25 21:58:34 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.6268
2022-06-25 21:59:08 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.1318
2022-06-25 21:59:41 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.1485
2022-06-25 22:00:14 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 3.9316
2022-06-25 22:00:47 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.9088
2022-06-25 22:01:22 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.0040
2022-06-25 22:01:56 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 3.8847
2022-06-25 22:02:30 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.8094
2022-06-25 22:03:03 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.8972
2022-06-25 22:03:37 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.9823
2022-06-25 22:04:10 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.6913
2022-06-25 22:04:44 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.4803
2022-06-25 22:05:16 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.7318
2022-06-25 22:05:50 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.4581
2022-06-25 22:06:24 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.6838
2022-06-25 22:06:58 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.5658
2022-06-25 22:07:31 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.5775
2022-06-25 22:08:05 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.5611
2022-06-25 22:08:38 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.7953
2022-06-25 22:09:12 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.6145
2022-06-25 22:09:45 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.5784
2022-06-25 22:10:19 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.5004
2022-06-25 22:10:52 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.5581
2022-06-25 22:11:25 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.5362
2022-06-25 22:11:59 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.5159
2022-06-25 22:12:33 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.7043
2022-06-25 22:13:07 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.4188
2022-06-25 22:13:41 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.5353
2022-06-25 22:14:14 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.6307
2022-06-25 22:14:48 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.2449
2022-06-25 22:15:23 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.4004
2022-06-25 22:15:57 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.4426
2022-06-25 22:16:31 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.4957
2022-06-25 22:17:05 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.3803
2022-06-25 22:17:38 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.4016
2022-06-25 22:18:11 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.2517
2022-06-25 22:18:45 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.2140
2022-06-25 22:19:20 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.2439
2022-06-25 22:19:53 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.3958
2022-06-25 22:20:26 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.4223
2022-06-25 22:21:01 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.1981
2022-06-25 22:21:33 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.2085
2022-06-25 22:21:34 - train: epoch 002, train_loss: 3.6825
2022-06-25 22:22:49 - eval: epoch: 002, acc1: 31.642%, acc5: 57.544%, test_loss: 3.2046, per_image_load_time: 2.383ms, per_image_inference_time: 0.521ms
2022-06-25 22:22:49 - until epoch: 002, best_acc1: 31.642%
2022-06-25 22:22:49 - epoch 003 lr: 0.100000
2022-06-25 22:23:28 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.3304
2022-06-25 22:24:02 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.3127
2022-06-25 22:24:36 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.1536
2022-06-25 22:25:10 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.2412
2022-06-25 22:25:44 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.3943
2022-06-25 22:26:18 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.1797
2022-06-25 22:26:52 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.4501
2022-06-25 22:27:26 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.3157
2022-06-25 22:28:01 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.0948
2022-06-25 22:28:34 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.1970
2022-06-25 22:29:08 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.0659
2022-06-25 22:29:42 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.1741
2022-06-25 22:30:17 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.1280
2022-06-25 22:30:50 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.0643
2022-06-25 22:31:24 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.3757
2022-06-25 22:31:58 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.1028
2022-06-25 22:32:33 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.0846
2022-06-25 22:33:07 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.0190
2022-06-25 22:33:41 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.2046
2022-06-25 22:34:16 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.3937
2022-06-25 22:34:50 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.3459
2022-06-25 22:35:25 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.6052
2022-06-25 22:35:58 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.0120
2022-06-25 22:36:32 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.0107
2022-06-25 22:37:07 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.0854
2022-06-25 22:37:42 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.1185
2022-06-25 22:38:16 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.3959
2022-06-25 22:38:50 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.0783
2022-06-25 22:39:23 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.0246
2022-06-25 22:39:58 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.1639
2022-06-25 22:40:33 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.1667
2022-06-25 22:41:07 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.1580
2022-06-25 22:41:41 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.0161
2022-06-25 22:42:15 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.2077
2022-06-25 22:42:50 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.8218
2022-06-25 22:43:23 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 2.8671
2022-06-25 22:43:58 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 2.9408
2022-06-25 22:44:32 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.0407
2022-06-25 22:45:07 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.1870
2022-06-25 22:45:40 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 2.9722
2022-06-25 22:46:15 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 2.9525
2022-06-25 22:46:49 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.8622
2022-06-25 22:47:24 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.6051
2022-06-25 22:47:58 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.7966
2022-06-25 22:48:32 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 2.9293
2022-06-25 22:49:06 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 2.9972
2022-06-25 22:49:41 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.7738
2022-06-25 22:50:16 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 2.9944
2022-06-25 22:50:50 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.0057
2022-06-25 22:51:24 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.0090
2022-06-25 22:51:25 - train: epoch 003, train_loss: 3.0683
2022-06-25 22:52:40 - eval: epoch: 003, acc1: 35.170%, acc5: 60.918%, test_loss: 3.9140, per_image_load_time: 2.418ms, per_image_inference_time: 0.486ms
2022-06-25 22:52:40 - until epoch: 003, best_acc1: 35.170%
2022-06-25 22:52:40 - epoch 004 lr: 0.100000
2022-06-25 22:53:19 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 2.8327
2022-06-25 22:53:53 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.7850
2022-06-25 22:54:27 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.9000
2022-06-25 22:55:00 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.7900
2022-06-25 22:55:35 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.7150
2022-06-25 22:56:08 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.0473
2022-06-25 22:56:42 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 2.9496
2022-06-25 22:57:16 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.7326
2022-06-25 22:57:49 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.6277
2022-06-25 22:58:23 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.7928
2022-06-25 22:58:57 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 2.9479
2022-06-25 22:59:30 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.5516
2022-06-25 23:00:05 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.6162
2022-06-25 23:00:39 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 2.9228
2022-06-25 23:01:12 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 2.9527
2022-06-25 23:01:46 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.6754
2022-06-25 23:02:21 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.9559
2022-06-25 23:02:56 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 2.9799
2022-06-25 23:03:28 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 2.8788
2022-06-25 23:04:04 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.7730
2022-06-25 23:04:37 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.8624
2022-06-25 23:05:11 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.7199
2022-06-25 23:05:46 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.6204
2022-06-25 23:06:20 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.6184
2022-06-25 23:06:54 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.6835
2022-06-25 23:07:28 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.7982
2022-06-25 23:08:02 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.6181
2022-06-25 23:08:37 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.7334
2022-06-25 23:09:10 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.6861
2022-06-25 23:09:45 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.7346
2022-06-25 23:10:19 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.8370
2022-06-25 23:10:54 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.6431
2022-06-25 23:11:28 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.8461
2022-06-25 23:12:03 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.7674
2022-06-25 23:12:37 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.6265
2022-06-25 23:13:11 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.5227
2022-06-25 23:13:45 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.7148
2022-06-25 23:14:20 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.6830
2022-06-25 23:14:54 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.6917
2022-06-25 23:15:28 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.4159
2022-06-25 23:16:02 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.8049
2022-06-25 23:16:37 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.5419
2022-06-25 23:17:11 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.5155
2022-06-25 23:17:45 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.5029
2022-06-25 23:18:20 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.1032
2022-06-25 23:18:54 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.6219
2022-06-25 23:19:28 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.6100
2022-06-25 23:20:03 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.6133
2022-06-25 23:20:36 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.8167
2022-06-25 23:21:10 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.7974
2022-06-25 23:21:11 - train: epoch 004, train_loss: 2.7708
2022-06-25 23:22:26 - eval: epoch: 004, acc1: 37.388%, acc5: 63.160%, test_loss: 2.9406, per_image_load_time: 1.087ms, per_image_inference_time: 0.520ms
2022-06-25 23:22:27 - until epoch: 004, best_acc1: 37.388%
2022-06-25 23:22:27 - epoch 005 lr: 0.100000
2022-06-25 23:23:06 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.7071
2022-06-25 23:23:39 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.7192
2022-06-25 23:24:13 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.7260
2022-06-25 23:24:46 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.7576
2022-06-25 23:25:20 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.4342
2022-06-25 23:25:53 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.5732
2022-06-25 23:26:27 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.7606
2022-06-25 23:27:01 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 2.7893
2022-06-25 23:27:35 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.6632
2022-06-25 23:28:08 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.5843
2022-06-25 23:28:43 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.7316
2022-06-25 23:29:17 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.7073
2022-06-25 23:29:50 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.5999
2022-06-25 23:30:26 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.7410
2022-06-25 23:30:58 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.4467
2022-06-25 23:31:32 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.4947
2022-06-25 23:32:06 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.4796
2022-06-25 23:32:41 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.6238
2022-06-25 23:33:15 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.5388
2022-06-25 23:33:49 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.5767
2022-06-25 23:34:23 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.4067
2022-06-25 23:34:57 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.4356
2022-06-25 23:35:32 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.2757
2022-06-25 23:36:06 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.5682
2022-06-25 23:36:39 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.6361
2022-06-25 23:37:13 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.7802
2022-06-25 23:37:47 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.6803
2022-06-25 23:38:21 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.6178
2022-06-25 23:38:56 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.4743
2022-06-25 23:39:30 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.5446
2022-06-25 23:40:04 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.5731
2022-06-25 23:40:38 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.7223
2022-06-25 23:41:12 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.4272
2022-06-25 23:41:47 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.4384
2022-06-25 23:42:21 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.5021
2022-06-25 23:42:55 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.6356
2022-06-25 23:43:29 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.4519
2022-06-25 23:44:03 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.3629
2022-06-25 23:44:38 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.9364
2022-06-25 23:45:12 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.4826
2022-06-25 23:45:46 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.5284
2022-06-25 23:46:21 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.6304
2022-06-25 23:46:55 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.4244
2022-06-25 23:47:29 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.5978
2022-06-25 23:48:03 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.5246
2022-06-25 23:48:37 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.5201
2022-06-25 23:49:12 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.2306
2022-06-25 23:49:46 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.3642
2022-06-25 23:50:20 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.6760
2022-06-25 23:50:53 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.4582
2022-06-25 23:50:54 - train: epoch 005, train_loss: 2.5877
2022-06-25 23:52:08 - eval: epoch: 005, acc1: 37.406%, acc5: 60.886%, test_loss: 6.3961, per_image_load_time: 1.204ms, per_image_inference_time: 0.521ms
2022-06-25 23:52:09 - until epoch: 005, best_acc1: 37.406%
2022-06-25 23:52:09 - epoch 006 lr: 0.100000
2022-06-25 23:52:47 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.5072
2022-06-25 23:53:20 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.6045
2022-06-25 23:53:55 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.2285
2022-06-25 23:54:30 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.5501
2022-06-25 23:55:04 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.4487
2022-06-25 23:55:37 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.5508
2022-06-25 23:56:13 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.5073
2022-06-25 23:56:47 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.5768
2022-06-25 23:57:21 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.3060
2022-06-25 23:57:54 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.4312
2022-06-25 23:58:28 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.4411
2022-06-25 23:59:03 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.5486
2022-06-25 23:59:37 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.6082
2022-06-26 00:00:11 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.6018
2022-06-26 00:00:46 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.6940
2022-06-26 00:01:19 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.3148
2022-06-26 00:01:54 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.6040
2022-06-26 00:02:27 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.5833
2022-06-26 00:03:01 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.3004
2022-06-26 00:03:36 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.6800
2022-06-26 00:04:10 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.5804
2022-06-26 00:04:44 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.3754
2022-06-26 00:05:18 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.3531
2022-06-26 00:05:51 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.4870
2022-06-26 00:06:26 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.6609
2022-06-26 00:07:00 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.4260
2022-06-26 00:07:35 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.5786
2022-06-26 00:08:09 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.3156
2022-06-26 00:08:43 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.6575
2022-06-26 00:09:17 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.4838
2022-06-26 00:09:50 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.2487
2022-06-26 00:10:24 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.3809
2022-06-26 00:10:59 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.3321
2022-06-26 00:11:34 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.6616
2022-06-26 00:12:08 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.4980
2022-06-26 00:12:42 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.4645
2022-06-26 00:13:15 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.4125
2022-06-26 00:13:50 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.3487
2022-06-26 00:14:24 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.3551
2022-06-26 00:14:59 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.7062
2022-06-26 00:15:32 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.4083
2022-06-26 00:16:06 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.2832
2022-06-26 00:16:41 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.4222
2022-06-26 00:17:15 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.3765
2022-06-26 00:17:50 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.4929
2022-06-26 00:18:23 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.4792
2022-06-26 00:18:58 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.3506
2022-06-26 00:19:32 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.4287
2022-06-26 00:20:06 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.4701
2022-06-26 00:20:39 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.3327
2022-06-26 00:20:40 - train: epoch 006, train_loss: 2.4634
2022-06-26 00:21:56 - eval: epoch: 006, acc1: 48.166%, acc5: 74.030%, test_loss: 2.2838, per_image_load_time: 2.030ms, per_image_inference_time: 0.538ms
2022-06-26 00:21:56 - until epoch: 006, best_acc1: 48.166%
2022-06-26 00:21:56 - epoch 007 lr: 0.100000
2022-06-26 00:22:36 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.2373
2022-06-26 00:23:08 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.6261
2022-06-26 00:23:43 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.6326
2022-06-26 00:24:17 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.5317
2022-06-26 00:24:51 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.3471
2022-06-26 00:25:25 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.5525
2022-06-26 00:25:59 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.4047
2022-06-26 00:26:32 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.3967
2022-06-26 00:27:06 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.3973
2022-06-26 00:27:40 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.4303
2022-06-26 00:28:14 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.2340
2022-06-26 00:28:49 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.4105
2022-06-26 00:29:23 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.3611
2022-06-26 00:29:58 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.3614
2022-06-26 00:30:32 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.5043
2022-06-26 00:31:06 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.3714
2022-06-26 00:31:39 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.3602
2022-06-26 00:32:14 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.3105
2022-06-26 00:32:48 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.3498
2022-06-26 00:33:22 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.2353
2022-06-26 00:33:56 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.4850
2022-06-26 00:34:30 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.2479
2022-06-26 00:35:04 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.4958
2022-06-26 00:35:38 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.5277
2022-06-26 00:36:13 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.4659
2022-06-26 00:36:46 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.2684
2022-06-26 00:37:20 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.2652
2022-06-26 00:37:54 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.3667
2022-06-26 00:38:28 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.3120
2022-06-26 00:39:01 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.3992
2022-06-26 00:39:36 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.1628
2022-06-26 00:40:11 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.3248
2022-06-26 00:40:45 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.6195
2022-06-26 00:41:19 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.2629
2022-06-26 00:41:53 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.4779
2022-06-26 00:42:28 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.2310
2022-06-26 00:43:01 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.3260
2022-06-26 00:43:35 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.5676
2022-06-26 00:44:10 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.2645
2022-06-26 00:44:44 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.4924
2022-06-26 00:45:18 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.3633
2022-06-26 00:45:52 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.3276
2022-06-26 00:46:27 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.4490
2022-06-26 00:47:01 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.1547
2022-06-26 00:47:35 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.5805
2022-06-26 00:48:09 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.4144
2022-06-26 00:48:43 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.4940
2022-06-26 00:49:17 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.5660
2022-06-26 00:49:52 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.3439
2022-06-26 00:50:25 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.2692
2022-06-26 00:50:26 - train: epoch 007, train_loss: 2.3819
2022-06-26 00:51:41 - eval: epoch: 007, acc1: 44.992%, acc5: 71.298%, test_loss: 2.4348, per_image_load_time: 2.400ms, per_image_inference_time: 0.497ms
2022-06-26 00:51:42 - until epoch: 007, best_acc1: 48.166%
2022-06-26 00:51:42 - epoch 008 lr: 0.100000
2022-06-26 00:52:20 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.2636
2022-06-26 00:52:54 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.3653
2022-06-26 00:53:27 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.0425
2022-06-26 00:54:01 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.2148
2022-06-26 00:54:34 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.1706
2022-06-26 00:55:09 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.2143
2022-06-26 00:55:42 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.6198
2022-06-26 00:56:17 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.1843
2022-06-26 00:56:50 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.3993
2022-06-26 00:57:24 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.3527
2022-06-26 00:57:58 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.1691
2022-06-26 00:58:33 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.1401
2022-06-26 00:59:06 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.2692
2022-06-26 00:59:40 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.2837
2022-06-26 01:00:15 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.3645
2022-06-26 01:00:50 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.4329
2022-06-26 01:01:24 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.3228
2022-06-26 01:01:58 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.4362
2022-06-26 01:02:31 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.1396
2022-06-26 01:03:06 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.3281
2022-06-26 01:03:40 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.2746
2022-06-26 01:04:15 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.2888
2022-06-26 01:04:49 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.3501
2022-06-26 01:05:23 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.3416
2022-06-26 01:05:57 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.3015
2022-06-26 01:06:31 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.4260
2022-06-26 01:07:05 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.4246
2022-06-26 01:07:39 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.4455
2022-06-26 01:08:14 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.2380
2022-06-26 01:08:48 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.4016
2022-06-26 01:09:23 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.3159
2022-06-26 01:09:57 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.6746
2022-06-26 01:10:32 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.4790
2022-06-26 01:11:05 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.4283
2022-06-26 01:11:39 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.2951
2022-06-26 01:12:14 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.4502
2022-06-26 01:12:48 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.3033
2022-06-26 01:13:22 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.1461
2022-06-26 01:13:57 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.4381
2022-06-26 01:14:31 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.5603
2022-06-26 01:15:05 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.1896
2022-06-26 01:15:39 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.2352
2022-06-26 01:16:15 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.0917
2022-06-26 01:16:48 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.2218
2022-06-26 01:17:24 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.4419
2022-06-26 01:17:57 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.3786
2022-06-26 01:18:31 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.1764
2022-06-26 01:19:06 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.3634
2022-06-26 01:19:40 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.3602
2022-06-26 01:20:13 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.3289
2022-06-26 01:20:14 - train: epoch 008, train_loss: 2.3066
2022-06-26 01:21:29 - eval: epoch: 008, acc1: 52.072%, acc5: 77.348%, test_loss: 2.0640, per_image_load_time: 2.377ms, per_image_inference_time: 0.500ms
2022-06-26 01:21:30 - until epoch: 008, best_acc1: 52.072%
2022-06-26 01:21:30 - epoch 009 lr: 0.100000
2022-06-26 01:22:08 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 1.9418
2022-06-26 01:22:42 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.0810
2022-06-26 01:23:16 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 1.9651
2022-06-26 01:23:50 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.4704
2022-06-26 01:24:23 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.3435
2022-06-26 01:24:56 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.2406
2022-06-26 01:25:31 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.1796
2022-06-26 01:26:06 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.1154
2022-06-26 01:26:39 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.0342
2022-06-26 01:27:13 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.0754
2022-06-26 01:27:47 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.4995
2022-06-26 01:28:22 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.3619
2022-06-26 01:28:55 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.4840
2022-06-26 01:29:30 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 1.9361
2022-06-26 01:30:04 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.1089
2022-06-26 01:30:38 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.1954
2022-06-26 01:31:13 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.4955
2022-06-26 01:31:48 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.1992
2022-06-26 01:32:22 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.0121
2022-06-26 01:32:56 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 1.9699
2022-06-26 01:33:31 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.2417
2022-06-26 01:34:06 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.3042
2022-06-26 01:34:39 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.0608
2022-06-26 01:35:14 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.1549
2022-06-26 01:35:48 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.1673
2022-06-26 01:36:22 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.2319
2022-06-26 01:36:57 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.2827
2022-06-26 01:37:31 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.2808
2022-06-26 01:38:06 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 1.9818
2022-06-26 01:38:39 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.1461
2022-06-26 01:39:14 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.3321
2022-06-26 01:39:48 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.2216
2022-06-26 01:40:22 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.1915
2022-06-26 01:40:57 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.3786
2022-06-26 01:41:30 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.4195
2022-06-26 01:42:06 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.2658
2022-06-26 01:42:39 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.4759
2022-06-26 01:43:14 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.4636
2022-06-26 01:43:48 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 1.8963
2022-06-26 01:44:22 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.4526
2022-06-26 01:44:56 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.1944
2022-06-26 01:45:31 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.1866
2022-06-26 01:46:05 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.4085
2022-06-26 01:46:40 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.2933
2022-06-26 01:47:14 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.1474
2022-06-26 01:47:48 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.3868
2022-06-26 01:48:22 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.4898
2022-06-26 01:48:57 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.5093
2022-06-26 01:49:31 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.1958
2022-06-26 01:50:04 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.1749
2022-06-26 01:50:05 - train: epoch 009, train_loss: 2.2582
2022-06-26 01:51:19 - eval: epoch: 009, acc1: 52.108%, acc5: 77.264%, test_loss: 2.1058, per_image_load_time: 2.251ms, per_image_inference_time: 0.530ms
2022-06-26 01:51:20 - until epoch: 009, best_acc1: 52.108%
2022-06-26 01:51:20 - epoch 010 lr: 0.100000
2022-06-26 01:51:58 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.1179
2022-06-26 01:52:32 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.2775
2022-06-26 01:53:06 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.2040
2022-06-26 01:53:40 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.3368
2022-06-26 01:54:13 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.1184
2022-06-26 01:54:47 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.2813
2022-06-26 01:55:21 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.3305
2022-06-26 01:55:55 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.0501
2022-06-26 01:56:29 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.0503
2022-06-26 01:57:04 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.0017
2022-06-26 01:57:38 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.2516
2022-06-26 01:58:12 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.0202
2022-06-26 01:58:47 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.0730
2022-06-26 01:59:20 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.2844
2022-06-26 01:59:55 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 1.9847
2022-06-26 02:00:29 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.3135
2022-06-26 02:01:03 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.3259
2022-06-26 02:01:38 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.2476
2022-06-26 02:02:11 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.2653
2022-06-26 02:02:46 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.2865
2022-06-26 02:03:20 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.1431
2022-06-26 02:03:55 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.4254
2022-06-26 02:04:29 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.4024
2022-06-26 02:05:02 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.3860
2022-06-26 02:05:37 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.2276
2022-06-26 02:06:10 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.3936
2022-06-26 02:06:46 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 1.9805
2022-06-26 02:07:20 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.2375
2022-06-26 02:07:54 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.2435
2022-06-26 02:08:29 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.2412
2022-06-26 02:09:03 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.4822
2022-06-26 02:09:37 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.2489
2022-06-26 02:10:12 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.3612
2022-06-26 02:10:46 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.4068
2022-06-26 02:11:20 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.4651
2022-06-26 02:11:54 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.3351
2022-06-26 02:12:29 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.0601
2022-06-26 02:13:03 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.1790
2022-06-26 02:13:37 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 1.9728
2022-06-26 02:14:11 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.1908
2022-06-26 02:14:45 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.0388
2022-06-26 02:15:20 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.2726
2022-06-26 02:15:54 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.2202
2022-06-26 02:16:29 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.1719
2022-06-26 02:17:03 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.0721
2022-06-26 02:17:37 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.2185
2022-06-26 02:18:12 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.1411
2022-06-26 02:18:46 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.2007
2022-06-26 02:19:21 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.1245
2022-06-26 02:19:54 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 1.9939
2022-06-26 02:19:55 - train: epoch 010, train_loss: 2.2146
2022-06-26 02:21:10 - eval: epoch: 010, acc1: 51.886%, acc5: 77.358%, test_loss: 2.0690, per_image_load_time: 2.426ms, per_image_inference_time: 0.493ms
2022-06-26 02:21:11 - until epoch: 010, best_acc1: 52.108%
2022-06-26 02:21:11 - epoch 011 lr: 0.100000
2022-06-26 02:21:49 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 1.9500
2022-06-26 02:22:23 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.2920
2022-06-26 02:22:57 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.0043
2022-06-26 02:23:30 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.2936
2022-06-26 02:24:05 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.1688
2022-06-26 02:24:38 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.1900
2022-06-26 02:25:12 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.1983
2022-06-26 02:25:47 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.1621
2022-06-26 02:26:19 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.3507
2022-06-26 02:26:53 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.2070
2022-06-26 02:27:29 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.2713
2022-06-26 02:28:03 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.5601
2022-06-26 02:28:37 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.3924
2022-06-26 02:29:11 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.2426
2022-06-26 02:29:45 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.0788
2022-06-26 02:30:19 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.3096
2022-06-26 02:30:53 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.1980
2022-06-26 02:31:28 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.0597
2022-06-26 02:32:01 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 1.9659
2022-06-26 02:32:35 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.3506
2022-06-26 02:33:09 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.1965
2022-06-26 02:33:43 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.0192
2022-06-26 02:34:17 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.4427
2022-06-26 02:34:51 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.0202
2022-06-26 02:35:25 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.4681
2022-06-26 02:36:00 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.1868
2022-06-26 02:36:34 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.2462
2022-06-26 02:37:08 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 1.8904
2022-06-26 02:37:42 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.2577
2022-06-26 02:38:16 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.4595
2022-06-26 02:38:50 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.2380
2022-06-26 02:39:24 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 1.9529
2022-06-26 02:39:58 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.2928
2022-06-26 02:40:33 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.0923
2022-06-26 02:41:06 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.1927
2022-06-26 02:41:41 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.1651
2022-06-26 02:42:15 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.2180
2022-06-26 02:42:49 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 1.9365
2022-06-26 02:43:23 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.2448
2022-06-26 02:43:58 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.0997
2022-06-26 02:44:32 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 1.8995
2022-06-26 02:45:06 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.1166
2022-06-26 02:45:41 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.2901
2022-06-26 02:46:15 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.0652
2022-06-26 02:46:48 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.0748
2022-06-26 02:47:23 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.0657
2022-06-26 02:47:57 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 1.9938
2022-06-26 02:48:31 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 1.9958
2022-06-26 02:49:06 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.0267
2022-06-26 02:49:39 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.1640
2022-06-26 02:49:40 - train: epoch 011, train_loss: 2.1807
2022-06-26 02:50:55 - eval: epoch: 011, acc1: 52.036%, acc5: 77.368%, test_loss: 2.0748, per_image_load_time: 2.391ms, per_image_inference_time: 0.487ms
2022-06-26 02:50:55 - until epoch: 011, best_acc1: 52.108%
2022-06-26 02:50:55 - epoch 012 lr: 0.100000
2022-06-26 02:51:34 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.0649
2022-06-26 02:52:07 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.1078
2022-06-26 02:52:42 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.1393
2022-06-26 02:53:16 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.1590
2022-06-26 02:53:50 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.3151
2022-06-26 02:54:23 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 1.9679
2022-06-26 02:54:58 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.0610
2022-06-26 02:55:31 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.1845
2022-06-26 02:56:05 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.2888
2022-06-26 02:56:39 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 1.9264
2022-06-26 02:57:13 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.4008
2022-06-26 02:57:47 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 1.9651
2022-06-26 02:58:21 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.1801
2022-06-26 02:58:55 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.3248
2022-06-26 02:59:29 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 1.9849
2022-06-26 03:00:03 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.1463
2022-06-26 03:00:37 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.0365
2022-06-26 03:01:11 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.1533
2022-06-26 03:01:45 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.1983
2022-06-26 03:02:19 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.3604
2022-06-26 03:02:53 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.1784
2022-06-26 03:03:27 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.2962
2022-06-26 03:04:01 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.1503
2022-06-26 03:04:35 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.2398
2022-06-26 03:05:09 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.0276
2022-06-26 03:05:44 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 1.9031
2022-06-26 03:06:18 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.0167
2022-06-26 03:06:52 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.1626
2022-06-26 03:07:26 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 1.9773
2022-06-26 03:08:00 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.0797
2022-06-26 03:08:34 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.2710
2022-06-26 03:09:08 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 1.9343
2022-06-26 03:09:42 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.1590
2022-06-26 03:10:16 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.1329
2022-06-26 03:10:50 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.3737
2022-06-26 03:11:24 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.0924
2022-06-26 03:11:58 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.1224
2022-06-26 03:12:33 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.1159
2022-06-26 03:13:06 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.0357
2022-06-26 03:13:40 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.0995
2022-06-26 03:14:14 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 1.9618
2022-06-26 03:14:48 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.0050
2022-06-26 03:15:23 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.3069
2022-06-26 03:15:57 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.0303
2022-06-26 03:16:30 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 1.9869
2022-06-26 03:17:05 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.4355
2022-06-26 03:17:39 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.1601
2022-06-26 03:18:14 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.2999
2022-06-26 03:18:48 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.2096
2022-06-26 03:19:21 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 1.7966
2022-06-26 03:19:22 - train: epoch 012, train_loss: 2.1536
2022-06-26 03:20:37 - eval: epoch: 012, acc1: 53.694%, acc5: 79.096%, test_loss: 1.9683, per_image_load_time: 2.402ms, per_image_inference_time: 0.462ms
2022-06-26 03:20:37 - until epoch: 012, best_acc1: 53.694%
2022-06-26 03:20:37 - epoch 013 lr: 0.100000
2022-06-26 03:21:16 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 1.9856
2022-06-26 03:21:50 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.1342
2022-06-26 03:22:24 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.0452
2022-06-26 03:22:57 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.0536
2022-06-26 03:23:31 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.1353
2022-06-26 03:24:05 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.1942
2022-06-26 03:24:39 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.0030
2022-06-26 03:25:13 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.2072
2022-06-26 03:25:46 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.0585
2022-06-26 03:26:20 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.1681
2022-06-26 03:26:54 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.1356
2022-06-26 03:27:29 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.2816
2022-06-26 03:28:03 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.1669
2022-06-26 03:28:37 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 1.9098
2022-06-26 03:29:12 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.2073
2022-06-26 03:29:46 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 1.8742
2022-06-26 03:30:20 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.1682
2022-06-26 03:30:54 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.1729
2022-06-26 03:31:28 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.0810
2022-06-26 03:32:02 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.4359
2022-06-26 03:32:37 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.3169
2022-06-26 03:33:11 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.0729
2022-06-26 03:33:46 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.1676
2022-06-26 03:34:20 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.1910
2022-06-26 03:34:55 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.0399
2022-06-26 03:35:28 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.0903
2022-06-26 03:36:02 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.0435
2022-06-26 03:36:38 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.0998
2022-06-26 03:37:12 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.1727
2022-06-26 03:37:46 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.1104
2022-06-26 03:38:20 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 1.8842
2022-06-26 03:38:54 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.1833
2022-06-26 03:39:29 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.0267
2022-06-26 03:40:02 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.0823
2022-06-26 03:40:38 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.0257
2022-06-26 03:41:12 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.3870
2022-06-26 03:41:46 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 1.8786
2022-06-26 03:42:20 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.2825
2022-06-26 03:42:54 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.2457
2022-06-26 03:43:28 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.1449
2022-06-26 03:44:03 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.0188
2022-06-26 03:44:37 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.1433
2022-06-26 03:45:11 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.0380
2022-06-26 03:45:45 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 1.9596
2022-06-26 03:46:20 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.0386
2022-06-26 03:46:55 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.1988
2022-06-26 03:47:29 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.2347
2022-06-26 03:48:03 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.3065
2022-06-26 03:48:39 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.1657
2022-06-26 03:49:11 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.2722
2022-06-26 03:49:13 - train: epoch 013, train_loss: 2.1312
2022-06-26 03:50:28 - eval: epoch: 013, acc1: 50.940%, acc5: 75.492%, test_loss: 2.6619, per_image_load_time: 2.417ms, per_image_inference_time: 0.512ms
2022-06-26 03:50:28 - until epoch: 013, best_acc1: 53.694%
2022-06-26 03:50:28 - epoch 014 lr: 0.100000
2022-06-26 03:51:08 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.0797
2022-06-26 03:51:41 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.1681
2022-06-26 03:52:15 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 1.8418
2022-06-26 03:52:48 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.0019
2022-06-26 03:53:22 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.0185
2022-06-26 03:53:56 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 1.9936
2022-06-26 03:54:30 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.2022
2022-06-26 03:55:04 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.0423
2022-06-26 03:55:38 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.1104
2022-06-26 03:56:13 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.3465
2022-06-26 03:56:47 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.0155
2022-06-26 03:57:21 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.2298
2022-06-26 03:57:55 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.1155
2022-06-26 03:58:29 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.1767
2022-06-26 03:59:03 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.2494
2022-06-26 03:59:38 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.1724
2022-06-26 04:00:12 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.2638
2022-06-26 04:00:46 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.2800
2022-06-26 04:01:20 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 1.9578
2022-06-26 04:01:54 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.0575
2022-06-26 04:02:28 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.2789
2022-06-26 04:03:02 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.1556
2022-06-26 04:03:36 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.0650
2022-06-26 04:04:10 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.2850
2022-06-26 04:04:45 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 1.9372
2022-06-26 04:05:18 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.0944
2022-06-26 04:05:53 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 1.9802
2022-06-26 04:06:26 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.4683
2022-06-26 04:07:01 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.0374
2022-06-26 04:07:36 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.1945
2022-06-26 04:08:09 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 1.9428
2022-06-26 04:08:44 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.0179
2022-06-26 04:09:18 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.0606
2022-06-26 04:09:52 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 1.9501
2022-06-26 04:10:27 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 1.9775
2022-06-26 04:11:01 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 1.9139
2022-06-26 04:11:35 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 1.9820
2022-06-26 04:12:09 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.2708
2022-06-26 04:12:42 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 1.9579
2022-06-26 04:13:18 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.2666
2022-06-26 04:13:52 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 1.9507
2022-06-26 04:14:26 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 1.9398
2022-06-26 04:15:00 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 1.8864
2022-06-26 04:15:34 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 1.9321
2022-06-26 04:16:09 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.0452
2022-06-26 04:16:43 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 1.9390
2022-06-26 04:17:17 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 1.9364
2022-06-26 04:17:52 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 1.9391
2022-06-26 04:18:26 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 1.9815
2022-06-26 04:18:59 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.1761
2022-06-26 04:19:00 - train: epoch 014, train_loss: 2.1075
2022-06-26 04:20:16 - eval: epoch: 014, acc1: 52.512%, acc5: 78.060%, test_loss: 2.1155, per_image_load_time: 2.243ms, per_image_inference_time: 0.489ms
2022-06-26 04:20:16 - until epoch: 014, best_acc1: 53.694%
2022-06-26 04:20:16 - epoch 015 lr: 0.100000
2022-06-26 04:20:55 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 1.8505
2022-06-26 04:21:29 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.2480
2022-06-26 04:22:02 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.2104
2022-06-26 04:22:36 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.1160
2022-06-26 04:23:10 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.1679
2022-06-26 04:23:44 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.1764
2022-06-26 04:24:18 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.0510
2022-06-26 04:24:52 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 1.8401
2022-06-26 04:25:26 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.0063
2022-06-26 04:26:00 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.1713
2022-06-26 04:26:34 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 1.9739
2022-06-26 04:27:08 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.1084
2022-06-26 04:27:42 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.3586
2022-06-26 04:28:16 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 1.9810
2022-06-26 04:28:50 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 1.8816
2022-06-26 04:29:25 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.1190
2022-06-26 04:29:59 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.2088
2022-06-26 04:30:33 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 1.9715
2022-06-26 04:31:07 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 1.9220
2022-06-26 04:31:41 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.0838
2022-06-26 04:32:15 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 1.9328
2022-06-26 04:32:50 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.3192
2022-06-26 04:33:24 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 1.9677
2022-06-26 04:33:59 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.1879
2022-06-26 04:34:33 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.0961
2022-06-26 04:35:07 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 1.9880
2022-06-26 04:35:41 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.1914
2022-06-26 04:36:16 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.1137
2022-06-26 04:36:50 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.0700
2022-06-26 04:37:24 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.0005
2022-06-26 04:37:59 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.0760
2022-06-26 04:38:33 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.0018
2022-06-26 04:39:07 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 1.9482
2022-06-26 04:39:41 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.1306
2022-06-26 04:40:15 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.2308
2022-06-26 04:40:50 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.1847
2022-06-26 04:41:24 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.0200
2022-06-26 04:41:59 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 1.9895
2022-06-26 04:42:33 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.2053
2022-06-26 04:43:08 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.1321
2022-06-26 04:43:43 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.2791
2022-06-26 04:44:18 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 1.7186
2022-06-26 04:44:51 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.0754
2022-06-26 04:45:26 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.1478
2022-06-26 04:46:01 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.2433
2022-06-26 04:46:34 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 1.9993
2022-06-26 04:47:09 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.1699
2022-06-26 04:47:44 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.0605
2022-06-26 04:48:17 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.0810
2022-06-26 04:48:50 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.1507
2022-06-26 04:48:52 - train: epoch 015, train_loss: 2.0929
2022-06-26 04:50:06 - eval: epoch: 015, acc1: 54.718%, acc5: 79.450%, test_loss: 1.9432, per_image_load_time: 2.049ms, per_image_inference_time: 0.539ms
2022-06-26 04:50:07 - until epoch: 015, best_acc1: 54.718%
2022-06-26 04:50:07 - epoch 016 lr: 0.100000
2022-06-26 04:50:46 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 1.9328
2022-06-26 04:51:19 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 1.9200
2022-06-26 04:51:52 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.1959
2022-06-26 04:52:26 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.2467
2022-06-26 04:53:00 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 1.8476
2022-06-26 04:53:34 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.2377
2022-06-26 04:54:08 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 1.8059
2022-06-26 04:54:42 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.1152
2022-06-26 04:55:15 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.1672
2022-06-26 04:55:49 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 1.9270
2022-06-26 04:56:24 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 1.9662
2022-06-26 04:56:57 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 1.9705
2022-06-26 04:57:31 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.1622
2022-06-26 04:58:05 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.0173
2022-06-26 04:58:39 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.1291
2022-06-26 04:59:14 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.1415
2022-06-26 04:59:48 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 1.9202
2022-06-26 05:00:22 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 1.9529
2022-06-26 05:00:56 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.0893
2022-06-26 05:01:30 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 1.8061
2022-06-26 05:02:04 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.2167
2022-06-26 05:02:39 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.1333
2022-06-26 05:03:12 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.4076
2022-06-26 05:03:45 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.2897
2022-06-26 05:04:19 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.0872
2022-06-26 05:04:53 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.2235
2022-06-26 05:05:26 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 1.9900
2022-06-26 05:06:01 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 1.9465
2022-06-26 05:06:36 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.2509
2022-06-26 05:07:10 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.3743
2022-06-26 05:07:44 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.1899
2022-06-26 05:08:18 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.3012
2022-06-26 05:08:52 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.0665
2022-06-26 05:09:25 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.0257
2022-06-26 05:09:59 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.2041
2022-06-26 05:10:33 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 1.9165
2022-06-26 05:11:07 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.1972
2022-06-26 05:11:41 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.3903
2022-06-26 05:12:15 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.1841
2022-06-26 05:12:49 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.1276
2022-06-26 05:13:23 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.1043
2022-06-26 05:13:57 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.1635
2022-06-26 05:14:31 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 1.9352
2022-06-26 05:15:06 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 1.9137
2022-06-26 05:15:39 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.2022
2022-06-26 05:16:14 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.0029
2022-06-26 05:16:48 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.3951
2022-06-26 05:17:23 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.1033
2022-06-26 05:17:56 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.1311
2022-06-26 05:18:29 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.1906
2022-06-26 05:18:31 - train: epoch 016, train_loss: 2.0790
2022-06-26 05:19:45 - eval: epoch: 016, acc1: 52.656%, acc5: 78.186%, test_loss: 2.0345, per_image_load_time: 1.490ms, per_image_inference_time: 0.521ms
2022-06-26 05:19:45 - until epoch: 016, best_acc1: 54.718%
2022-06-26 05:19:45 - epoch 017 lr: 0.100000
2022-06-26 05:20:24 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 1.9613
2022-06-26 05:20:58 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.0920
2022-06-26 05:21:31 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.2307
2022-06-26 05:22:05 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 1.8823
2022-06-26 05:22:39 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.1289
2022-06-26 05:23:13 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.3543
2022-06-26 05:23:48 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.1242
2022-06-26 05:24:22 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 1.8647
2022-06-26 05:24:56 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.0840
2022-06-26 05:25:30 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.0155
2022-06-26 05:26:04 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.3744
2022-06-26 05:26:37 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.1840
2022-06-26 05:27:11 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.1035
2022-06-26 05:27:46 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.1234
2022-06-26 05:28:19 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 1.7414
2022-06-26 05:28:53 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 1.9949
2022-06-26 05:29:28 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.0067
2022-06-26 05:30:02 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.0871
2022-06-26 05:30:36 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 1.9360
2022-06-26 05:31:10 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.2405
2022-06-26 05:31:44 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.1109
2022-06-26 05:32:19 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.0053
2022-06-26 05:32:53 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 1.9992
2022-06-26 05:33:28 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 1.9955
2022-06-26 05:34:02 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.2415
2022-06-26 05:34:37 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.0028
2022-06-26 05:35:11 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.0025
2022-06-26 05:35:45 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.3506
2022-06-26 05:36:19 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.2558
2022-06-26 05:36:53 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 1.8611
2022-06-26 05:37:28 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.1572
2022-06-26 05:38:02 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 1.9704
2022-06-26 05:38:36 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.1626
2022-06-26 05:39:10 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 1.8335
2022-06-26 05:39:44 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.1744
2022-06-26 05:40:18 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.3474
2022-06-26 05:40:52 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.1340
2022-06-26 05:41:27 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.2150
2022-06-26 05:42:01 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.0580
2022-06-26 05:42:35 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 1.9251
2022-06-26 05:43:09 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.0991
2022-06-26 05:43:44 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.0075
2022-06-26 05:44:19 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.0720
2022-06-26 05:44:54 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 1.9887
2022-06-26 05:45:27 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.2617
2022-06-26 05:46:02 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.0297
2022-06-26 05:46:37 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.2273
2022-06-26 05:47:10 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.1874
2022-06-26 05:47:44 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 1.9270
2022-06-26 05:48:18 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 1.8606
2022-06-26 05:48:19 - train: epoch 017, train_loss: 2.0628
2022-06-26 05:49:35 - eval: epoch: 017, acc1: 48.256%, acc5: 71.960%, test_loss: 3.5540, per_image_load_time: 2.460ms, per_image_inference_time: 0.488ms
2022-06-26 05:49:35 - until epoch: 017, best_acc1: 54.718%
2022-06-26 05:49:36 - epoch 018 lr: 0.100000
2022-06-26 05:50:14 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.1046
2022-06-26 05:50:47 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.0641
2022-06-26 05:51:22 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.2020
2022-06-26 05:51:55 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.1512
2022-06-26 05:52:29 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 1.9426
2022-06-26 05:53:03 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.2105
2022-06-26 05:53:37 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 1.8002
2022-06-26 05:54:11 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 1.9924
2022-06-26 05:54:45 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.0915
2022-06-26 05:55:19 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 1.9138
2022-06-26 05:55:53 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.3136
2022-06-26 05:56:28 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 1.9882
2022-06-26 05:57:02 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.3267
2022-06-26 05:57:36 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.0095
2022-06-26 05:58:10 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.2650
2022-06-26 05:58:44 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.0810
2022-06-26 05:59:18 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 1.9755
2022-06-26 05:59:52 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 1.8179
2022-06-26 06:00:26 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.1233
2022-06-26 06:01:01 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.2107
2022-06-26 06:01:34 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.2816
2022-06-26 06:02:09 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.0230
2022-06-26 06:02:43 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.0475
2022-06-26 06:03:17 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 1.8321
2022-06-26 06:03:51 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 1.8651
2022-06-26 06:04:25 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 1.9366
2022-06-26 06:05:00 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.2316
2022-06-26 06:05:35 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 1.8269
2022-06-26 06:06:08 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.1057
2022-06-26 06:06:43 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.0893
2022-06-26 06:07:16 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.3311
2022-06-26 06:07:51 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 1.9031
2022-06-26 06:08:25 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 1.9078
2022-06-26 06:09:00 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.0579
2022-06-26 06:09:34 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.3150
2022-06-26 06:10:07 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.1120
2022-06-26 06:10:41 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.4425
2022-06-26 06:11:16 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.1222
2022-06-26 06:11:51 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.0996
2022-06-26 06:12:25 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 1.9731
2022-06-26 06:12:59 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 1.9909
2022-06-26 06:13:33 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.0432
2022-06-26 06:14:07 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 1.8819
2022-06-26 06:14:42 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.2209
2022-06-26 06:15:16 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.0579
2022-06-26 06:15:50 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 1.9596
2022-06-26 06:16:25 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.3393
2022-06-26 06:16:59 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.1068
2022-06-26 06:17:34 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.0045
2022-06-26 06:18:06 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.1149
2022-06-26 06:18:07 - train: epoch 018, train_loss: 2.0481
2022-06-26 06:19:22 - eval: epoch: 018, acc1: 50.864%, acc5: 76.108%, test_loss: 2.1536, per_image_load_time: 2.348ms, per_image_inference_time: 0.513ms
2022-06-26 06:19:22 - until epoch: 018, best_acc1: 54.718%
2022-06-26 06:19:22 - epoch 019 lr: 0.100000
2022-06-26 06:20:02 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 1.8991
2022-06-26 06:20:35 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.2051
2022-06-26 06:21:09 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.3082
2022-06-26 06:21:43 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.0353
2022-06-26 06:22:16 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.0836
2022-06-26 06:22:51 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.0102
2022-06-26 06:23:25 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 1.7912
2022-06-26 06:23:58 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.2244
2022-06-26 06:24:32 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.0544
2022-06-26 06:25:06 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.1621
2022-06-26 06:25:40 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.0027
2022-06-26 06:26:14 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.0537
2022-06-26 06:26:48 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.2272
2022-06-26 06:27:21 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 1.9845
2022-06-26 06:27:55 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.3525
2022-06-26 06:28:30 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 1.9809
2022-06-26 06:29:03 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.0556
2022-06-26 06:29:37 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 1.9616
2022-06-26 06:30:11 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.2450
2022-06-26 06:30:46 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 1.9878
2022-06-26 06:31:20 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 1.8296
2022-06-26 06:31:54 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.0048
2022-06-26 06:32:28 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 1.9992
2022-06-26 06:33:02 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.0644
2022-06-26 06:33:35 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 1.9585
2022-06-26 06:34:11 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.1313
2022-06-26 06:34:44 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.0785
2022-06-26 06:35:19 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.0509
2022-06-26 06:35:53 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.0419
2022-06-26 06:36:27 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.2866
2022-06-26 06:37:00 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.1298
2022-06-26 06:37:35 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.8291
2022-06-26 06:38:10 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 1.9995
2022-06-26 06:38:44 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.1796
2022-06-26 06:39:18 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.0616
2022-06-26 06:39:51 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 1.8637
2022-06-26 06:40:27 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.1205
2022-06-26 06:41:02 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.1507
2022-06-26 06:41:35 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.0465
2022-06-26 06:42:10 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 1.9932
2022-06-26 06:42:44 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.0915
2022-06-26 06:43:19 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.0167
2022-06-26 06:43:53 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 1.8935
2022-06-26 06:44:28 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.1755
2022-06-26 06:45:02 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.2938
2022-06-26 06:45:35 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 1.9437
2022-06-26 06:46:10 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 1.8878
2022-06-26 06:46:44 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.0596
2022-06-26 06:47:18 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.0455
2022-06-26 06:47:51 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.0798
2022-06-26 06:47:52 - train: epoch 019, train_loss: 2.0426
2022-06-26 06:49:07 - eval: epoch: 019, acc1: 56.320%, acc5: 80.898%, test_loss: 1.8440, per_image_load_time: 2.260ms, per_image_inference_time: 0.516ms
2022-06-26 06:49:07 - until epoch: 019, best_acc1: 56.320%
2022-06-26 06:49:07 - epoch 020 lr: 0.100000
2022-06-26 06:49:45 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.1136
2022-06-26 06:50:19 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 1.7992
2022-06-26 06:50:53 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.1009
2022-06-26 06:51:27 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 1.8427
2022-06-26 06:52:00 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 1.9622
2022-06-26 06:52:35 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.1386
2022-06-26 06:53:09 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 1.8427
2022-06-26 06:53:43 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.1366
2022-06-26 06:54:17 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.3757
2022-06-26 06:54:50 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.1418
2022-06-26 06:55:26 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 1.9711
2022-06-26 06:56:00 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 1.7378
2022-06-26 06:56:34 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 1.9405
2022-06-26 06:57:09 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.2383
2022-06-26 06:57:42 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.1313
2022-06-26 06:58:17 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 1.8919
2022-06-26 06:58:51 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 1.7580
2022-06-26 06:59:26 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.0308
2022-06-26 06:59:59 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 1.8271
2022-06-26 07:00:33 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.0135
2022-06-26 07:01:08 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.1280
2022-06-26 07:01:42 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 1.8528
2022-06-26 07:02:17 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 1.8619
2022-06-26 07:02:50 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.2432
2022-06-26 07:03:24 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 1.9967
2022-06-26 07:03:58 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 1.8636
2022-06-26 07:04:32 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.1108
2022-06-26 07:05:07 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.0101
2022-06-26 07:05:41 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.1828
2022-06-26 07:06:16 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 1.9992
2022-06-26 07:06:49 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.1034
2022-06-26 07:07:24 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.2247
2022-06-26 07:07:59 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 1.8649
2022-06-26 07:08:33 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.1999
2022-06-26 07:09:08 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 1.8550
2022-06-26 07:09:41 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.0623
2022-06-26 07:10:16 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 1.9003
2022-06-26 07:10:50 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 1.9775
2022-06-26 07:11:24 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.2319
2022-06-26 07:11:59 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 1.9258
2022-06-26 07:12:33 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 1.8657
2022-06-26 07:13:07 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 1.9883
2022-06-26 07:13:43 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.0798
2022-06-26 07:14:17 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.0360
2022-06-26 07:14:51 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.1296
2022-06-26 07:15:26 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.0802
2022-06-26 07:16:00 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 1.9868
2022-06-26 07:16:34 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.0188
2022-06-26 07:17:09 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.1711
2022-06-26 07:17:42 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 1.8730
2022-06-26 07:17:43 - train: epoch 020, train_loss: 2.0269
2022-06-26 07:18:58 - eval: epoch: 020, acc1: 54.490%, acc5: 79.536%, test_loss: 1.9435, per_image_load_time: 2.417ms, per_image_inference_time: 0.497ms
2022-06-26 07:18:59 - until epoch: 020, best_acc1: 56.320%
2022-06-26 07:18:59 - epoch 021 lr: 0.100000
2022-06-26 07:19:38 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 1.8707
2022-06-26 07:20:11 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.1348
2022-06-26 07:20:44 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.7020
2022-06-26 07:21:18 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.2108
2022-06-26 07:21:53 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 1.8960
2022-06-26 07:22:27 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 1.8346
2022-06-26 07:23:01 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 1.8295
2022-06-26 07:23:33 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.1145
2022-06-26 07:24:08 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.0440
2022-06-26 07:24:42 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 1.8846
2022-06-26 07:25:16 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 1.9418
2022-06-26 07:25:49 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 1.9422
2022-06-26 07:26:23 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 1.9068
2022-06-26 07:26:58 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.0310
2022-06-26 07:27:32 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 1.7964
2022-06-26 07:28:05 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.0904
2022-06-26 07:28:40 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.0355
2022-06-26 07:29:14 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 1.8430
2022-06-26 07:29:49 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.1830
2022-06-26 07:30:23 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.2698
2022-06-26 07:30:56 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 1.7696
2022-06-26 07:31:31 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.0034
2022-06-26 07:32:06 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 1.9213
2022-06-26 07:32:39 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 1.9069
2022-06-26 07:33:14 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.0392
2022-06-26 07:33:49 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.3028
2022-06-26 07:34:23 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 1.9168
2022-06-26 07:34:57 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.0169
2022-06-26 07:35:31 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 1.9116
2022-06-26 07:36:06 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.2139
2022-06-26 07:36:40 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.1255
2022-06-26 07:37:13 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.0733
2022-06-26 07:37:47 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.2342
2022-06-26 07:38:21 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.2010
2022-06-26 07:38:55 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 1.9365
2022-06-26 07:39:29 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 1.9552
2022-06-26 07:40:03 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.0672
2022-06-26 07:40:37 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 1.9699
2022-06-26 07:41:12 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 1.9034
2022-06-26 07:41:47 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.2161
2022-06-26 07:42:21 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.0507
2022-06-26 07:42:56 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.0166
2022-06-26 07:43:30 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 1.9039
2022-06-26 07:44:04 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.0708
2022-06-26 07:44:38 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.1230
2022-06-26 07:45:12 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 1.8958
2022-06-26 07:45:46 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.1758
2022-06-26 07:46:21 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.2052
2022-06-26 07:46:55 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 1.8051
2022-06-26 07:47:28 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 1.9688
2022-06-26 07:47:29 - train: epoch 021, train_loss: 2.0196
2022-06-26 07:48:43 - eval: epoch: 021, acc1: 55.728%, acc5: 80.690%, test_loss: 1.8669, per_image_load_time: 2.264ms, per_image_inference_time: 0.517ms
2022-06-26 07:48:44 - until epoch: 021, best_acc1: 56.320%
2022-06-26 07:48:44 - epoch 022 lr: 0.100000
2022-06-26 07:49:22 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 1.7809
2022-06-26 07:49:55 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 1.8206
2022-06-26 07:50:29 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 1.6193
2022-06-26 07:51:02 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 1.8919
2022-06-26 07:51:35 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.1046
2022-06-26 07:52:08 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.0897
2022-06-26 07:52:41 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 1.9933
2022-06-26 07:53:13 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.0447
2022-06-26 07:53:46 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.0667
2022-06-26 07:54:17 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.0602
2022-06-26 07:54:52 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.0095
2022-06-26 07:55:26 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.6697
2022-06-26 07:55:59 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 1.9363
2022-06-26 07:56:32 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 1.9763
2022-06-26 07:57:04 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 1.9737
2022-06-26 07:57:38 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 1.7631
2022-06-26 07:58:12 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 1.8809
2022-06-26 07:58:44 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.1358
2022-06-26 07:59:17 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 1.8965
2022-06-26 07:59:52 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.1116
2022-06-26 08:00:25 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.1566
2022-06-26 08:01:00 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 1.8370
2022-06-26 08:01:34 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.1617
2022-06-26 08:02:08 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 1.9835
2022-06-26 08:02:41 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.1125
2022-06-26 08:03:15 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 1.8137
2022-06-26 08:03:48 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 1.8136
2022-06-26 08:04:22 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.2553
2022-06-26 08:04:56 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 1.8252
2022-06-26 08:05:30 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.0691
2022-06-26 08:06:04 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.1756
2022-06-26 08:06:38 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.0864
2022-06-26 08:07:12 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 1.9116
2022-06-26 08:07:46 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 1.7843
2022-06-26 08:08:21 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.0792
2022-06-26 08:08:54 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.0790
2022-06-26 08:09:28 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.2002
2022-06-26 08:10:01 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.1578
2022-06-26 08:10:35 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 1.9081
2022-06-26 08:11:09 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.0749
2022-06-26 08:11:41 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 1.9438
2022-06-26 08:12:16 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 1.9050
2022-06-26 08:12:50 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.2550
2022-06-26 08:13:23 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.0563
2022-06-26 08:13:56 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 1.9817
2022-06-26 08:14:30 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.2161
2022-06-26 08:15:03 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.1470
2022-06-26 08:15:37 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 1.8552
2022-06-26 08:16:11 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 1.9033
2022-06-26 08:16:44 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 1.9478
2022-06-26 08:16:45 - train: epoch 022, train_loss: 2.0113
2022-06-26 08:18:00 - eval: epoch: 022, acc1: 48.676%, acc5: 73.748%, test_loss: 2.3325, per_image_load_time: 2.436ms, per_image_inference_time: 0.495ms
2022-06-26 08:18:00 - until epoch: 022, best_acc1: 56.320%
2022-06-26 08:18:00 - epoch 023 lr: 0.100000
2022-06-26 08:18:39 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 1.8354
2022-06-26 08:19:13 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 1.5981
2022-06-26 08:19:45 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 1.8820
2022-06-26 08:20:19 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 1.9717
2022-06-26 08:20:54 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.0714
2022-06-26 08:21:28 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 1.8742
2022-06-26 08:22:02 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 1.7842
2022-06-26 08:22:36 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 1.9010
2022-06-26 08:23:10 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 1.9815
2022-06-26 08:23:44 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 1.8811
2022-06-26 08:24:18 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.1102
2022-06-26 08:24:53 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 1.8296
2022-06-26 08:25:27 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 1.9697
2022-06-26 08:26:02 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.0398
2022-06-26 08:26:35 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 1.8134
2022-06-26 08:27:10 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 1.9734
2022-06-26 08:27:43 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.0614
2022-06-26 08:28:17 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 1.8693
2022-06-26 08:28:52 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.0726
2022-06-26 08:29:26 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.6633
2022-06-26 08:30:00 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.1569
2022-06-26 08:30:35 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.7591
2022-06-26 08:31:08 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 1.8378
2022-06-26 08:31:43 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 1.9702
2022-06-26 08:32:17 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.0161
2022-06-26 08:32:52 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.0774
2022-06-26 08:33:26 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.0489
2022-06-26 08:34:01 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 1.9709
2022-06-26 08:34:35 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.1196
2022-06-26 08:35:09 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.2014
2022-06-26 08:35:44 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.0798
2022-06-26 08:36:18 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.1619
2022-06-26 08:36:52 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.0703
2022-06-26 08:37:26 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.1684
2022-06-26 08:38:00 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 1.9206
2022-06-26 08:38:35 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 1.7927
2022-06-26 08:39:08 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.1003
2022-06-26 08:39:43 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.0492
2022-06-26 08:40:18 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 1.9356
2022-06-26 08:40:52 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.0068
2022-06-26 08:41:26 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 1.9106
2022-06-26 08:42:00 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 1.9041
2022-06-26 08:42:35 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 1.8155
2022-06-26 08:43:09 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 1.8013
2022-06-26 08:43:43 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 1.9121
2022-06-26 08:44:17 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.1423
2022-06-26 08:44:51 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 1.7297
2022-06-26 08:45:25 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 1.9679
2022-06-26 08:46:00 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 1.8235
2022-06-26 08:46:33 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.0054
2022-06-26 08:46:34 - train: epoch 023, train_loss: 2.0028
2022-06-26 08:47:49 - eval: epoch: 023, acc1: 52.562%, acc5: 77.736%, test_loss: 2.0537, per_image_load_time: 1.789ms, per_image_inference_time: 0.509ms
2022-06-26 08:47:49 - until epoch: 023, best_acc1: 56.320%
2022-06-26 08:47:49 - epoch 024 lr: 0.100000
2022-06-26 08:48:27 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 1.8988
2022-06-26 08:49:02 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.0882
2022-06-26 08:49:35 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 1.9202
2022-06-26 08:50:10 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.0832
2022-06-26 08:50:43 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 1.9102
2022-06-26 08:51:17 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 1.8102
2022-06-26 08:51:51 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 1.9031
2022-06-26 08:52:25 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 1.9189
2022-06-26 08:52:59 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 1.9352
2022-06-26 08:53:33 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 1.9163
2022-06-26 08:54:07 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.6879
2022-06-26 08:54:41 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 1.8535
2022-06-26 08:55:15 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.2841
2022-06-26 08:55:49 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 1.8410
2022-06-26 08:56:23 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.2273
2022-06-26 08:56:58 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.0673
2022-06-26 08:57:32 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 1.9420
2022-06-26 08:58:06 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.2328
2022-06-26 08:58:40 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 1.8095
2022-06-26 08:59:14 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.1188
2022-06-26 08:59:49 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 1.9703
2022-06-26 09:00:23 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 1.8416
2022-06-26 09:00:57 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.0266
2022-06-26 09:01:31 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 1.9358
2022-06-26 09:02:05 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 1.9561
2022-06-26 09:02:39 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 1.9211
2022-06-26 09:03:14 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.2083
2022-06-26 09:03:48 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.0773
2022-06-26 09:04:21 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.0063
2022-06-26 09:04:56 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 1.8947
2022-06-26 09:05:31 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 1.9110
2022-06-26 09:06:05 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.1878
2022-06-26 09:06:40 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.7088
2022-06-26 09:07:14 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.0851
2022-06-26 09:07:48 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 1.9576
2022-06-26 09:08:23 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.1000
2022-06-26 09:08:58 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 1.8997
2022-06-26 09:09:31 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.2254
2022-06-26 09:10:06 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.0409
2022-06-26 09:10:40 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.1064
2022-06-26 09:11:14 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 1.9236
2022-06-26 09:11:49 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.0483
2022-06-26 09:12:24 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.1128
2022-06-26 09:12:58 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.0765
2022-06-26 09:13:33 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 1.8815
2022-06-26 09:14:08 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.0772
2022-06-26 09:14:42 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 1.9748
2022-06-26 09:15:16 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 1.9119
2022-06-26 09:15:51 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 1.9813
2022-06-26 09:16:23 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.0292
2022-06-26 09:16:24 - train: epoch 024, train_loss: 1.9978
2022-06-26 09:17:39 - eval: epoch: 024, acc1: 55.962%, acc5: 80.766%, test_loss: 1.8658, per_image_load_time: 2.300ms, per_image_inference_time: 0.490ms
2022-06-26 09:17:39 - until epoch: 024, best_acc1: 56.320%
2022-06-26 09:17:39 - epoch 025 lr: 0.100000
2022-06-26 09:18:19 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 1.7644
2022-06-26 09:18:52 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 1.8101
2022-06-26 09:19:27 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 1.7780
2022-06-26 09:20:01 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 1.9088
2022-06-26 09:20:35 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 1.8856
2022-06-26 09:21:10 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 1.9899
2022-06-26 09:21:44 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.0022
2022-06-26 09:22:19 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 1.9237
2022-06-26 09:22:53 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 1.8965
2022-06-26 09:23:27 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 1.9853
2022-06-26 09:24:02 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 1.8802
2022-06-26 09:24:37 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.0344
2022-06-26 09:25:10 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.0447
2022-06-26 09:25:45 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.0854
2022-06-26 09:26:21 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 1.9871
2022-06-26 09:26:55 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 1.7308
2022-06-26 09:27:28 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 1.9368
2022-06-26 09:28:03 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 1.8017
2022-06-26 09:28:38 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 1.8636
2022-06-26 09:29:13 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 1.9455
2022-06-26 09:29:47 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 1.8529
2022-06-26 09:30:22 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 1.7924
2022-06-26 09:30:56 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.0467
2022-06-26 09:31:31 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 1.8076
2022-06-26 09:32:06 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 1.9592
2022-06-26 09:32:40 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.1559
2022-06-26 09:33:15 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.0375
2022-06-26 09:33:48 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 1.9994
2022-06-26 09:34:24 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.2346
2022-06-26 09:34:58 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.1469
2022-06-26 09:35:32 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 1.9185
2022-06-26 09:36:07 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.3132
2022-06-26 09:36:42 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 1.8959
2022-06-26 09:37:16 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.0805
2022-06-26 09:37:51 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 1.7324
2022-06-26 09:38:25 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.1683
2022-06-26 09:38:59 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 1.9633
2022-06-26 09:39:33 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.0042
2022-06-26 09:40:07 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.2352
2022-06-26 09:40:41 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.0798
2022-06-26 09:41:16 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.2421
2022-06-26 09:41:50 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.0034
2022-06-26 09:42:25 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 1.9204
2022-06-26 09:42:59 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 1.9171
2022-06-26 09:43:34 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 1.8536
2022-06-26 09:44:09 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.0339
2022-06-26 09:44:44 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 1.9600
2022-06-26 09:45:18 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 1.7922
2022-06-26 09:45:52 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 1.8928
2022-06-26 09:46:26 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.1829
2022-06-26 09:46:27 - train: epoch 025, train_loss: 1.9902
2022-06-26 09:47:42 - eval: epoch: 025, acc1: 57.714%, acc5: 81.892%, test_loss: 1.7720, per_image_load_time: 1.682ms, per_image_inference_time: 0.526ms
2022-06-26 09:47:43 - until epoch: 025, best_acc1: 57.714%
2022-06-26 09:47:43 - epoch 026 lr: 0.100000
2022-06-26 09:48:22 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 1.8337
2022-06-26 09:48:55 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.7679
2022-06-26 09:49:29 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.0523
2022-06-26 09:50:02 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 1.9230
2022-06-26 09:50:37 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 1.9782
2022-06-26 09:51:11 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.1051
2022-06-26 09:51:45 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 1.9022
2022-06-26 09:52:19 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 1.7433
2022-06-26 09:52:53 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.1883
2022-06-26 09:53:28 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 1.7977
2022-06-26 09:54:02 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 1.9105
2022-06-26 09:54:37 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.0049
2022-06-26 09:55:10 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 1.8190
2022-06-26 09:55:45 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.0449
2022-06-26 09:56:20 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 1.9311
2022-06-26 09:56:55 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.1012
2022-06-26 09:57:28 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 1.9085
2022-06-26 09:58:03 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.0336
2022-06-26 09:58:38 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.3170
2022-06-26 09:59:13 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.1260
2022-06-26 09:59:47 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.0393
2022-06-26 10:00:21 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 1.9670
2022-06-26 10:00:56 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 1.9386
2022-06-26 10:01:31 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.1660
2022-06-26 10:02:05 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.0634
2022-06-26 10:02:40 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.0377
2022-06-26 10:03:13 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 1.9358
2022-06-26 10:03:48 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 1.8654
2022-06-26 10:04:23 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.1105
2022-06-26 10:04:56 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 1.9531
2022-06-26 10:05:31 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 1.9505
2022-06-26 10:06:06 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 1.9692
2022-06-26 10:06:39 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 1.9274
2022-06-26 10:07:13 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.2258
2022-06-26 10:07:47 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.0798
2022-06-26 10:08:22 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 1.8210
2022-06-26 10:08:56 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.0504
2022-06-26 10:09:30 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.1326
2022-06-26 10:10:04 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.1059
2022-06-26 10:10:38 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.2512
2022-06-26 10:11:12 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.0409
2022-06-26 10:11:47 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.1415
2022-06-26 10:12:20 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.0149
2022-06-26 10:12:54 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.1241
2022-06-26 10:13:27 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.2152
2022-06-26 10:14:02 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.0389
2022-06-26 10:14:35 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 1.9265
2022-06-26 10:15:09 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 1.8817
2022-06-26 10:15:43 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.0100
2022-06-26 10:16:16 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.1678
2022-06-26 10:16:17 - train: epoch 026, train_loss: 1.9844
2022-06-26 10:17:32 - eval: epoch: 026, acc1: 57.978%, acc5: 82.080%, test_loss: 1.7685, per_image_load_time: 1.757ms, per_image_inference_time: 0.484ms
2022-06-26 10:17:33 - until epoch: 026, best_acc1: 57.978%
2022-06-26 10:17:33 - epoch 027 lr: 0.100000
2022-06-26 10:18:12 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.0275
2022-06-26 10:18:47 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 1.9426
2022-06-26 10:19:20 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 1.8883
2022-06-26 10:19:54 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.1474
2022-06-26 10:20:28 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 1.9962
2022-06-26 10:21:02 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.1331
2022-06-26 10:21:37 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.0969
2022-06-26 10:22:09 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.0071
2022-06-26 10:22:44 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 1.9014
2022-06-26 10:23:17 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 1.9831
2022-06-26 10:23:51 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 1.9827
2022-06-26 10:24:24 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.2300
2022-06-26 10:24:59 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.0455
2022-06-26 10:25:33 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.1731
2022-06-26 10:26:07 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.0168
2022-06-26 10:26:41 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 1.9779
2022-06-26 10:27:15 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.0030
2022-06-26 10:27:48 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 1.9501
2022-06-26 10:28:22 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.1104
2022-06-26 10:28:56 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 1.9675
2022-06-26 10:29:31 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.2546
2022-06-26 10:30:05 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.1399
2022-06-26 10:30:39 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.2879
2022-06-26 10:31:13 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.0279
2022-06-26 10:31:47 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 1.7750
2022-06-26 10:32:20 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.1443
2022-06-26 10:32:55 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 1.9805
2022-06-26 10:33:28 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.1564
2022-06-26 10:34:02 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.0502
2022-06-26 10:34:36 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 1.8683
2022-06-26 10:35:10 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 1.8716
2022-06-26 10:35:43 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 1.8665
2022-06-26 10:36:18 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.1117
2022-06-26 10:36:51 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 1.9173
2022-06-26 10:37:25 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.3079
2022-06-26 10:37:59 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 1.9247
2022-06-26 10:38:33 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.0233
2022-06-26 10:39:08 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 1.7658
2022-06-26 10:39:41 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 1.7807
2022-06-26 10:40:16 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.0701
2022-06-26 10:40:50 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 1.9954
2022-06-26 10:41:24 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 1.9901
2022-06-26 10:41:59 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 1.9056
2022-06-26 10:42:33 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.0224
2022-06-26 10:43:07 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 1.7954
2022-06-26 10:43:41 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 1.9071
2022-06-26 10:44:15 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 1.9570
2022-06-26 10:44:50 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.1659
2022-06-26 10:45:24 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.1225
2022-06-26 10:45:58 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.7338
2022-06-26 10:45:59 - train: epoch 027, train_loss: 1.9768
2022-06-26 10:47:14 - eval: epoch: 027, acc1: 59.122%, acc5: 83.054%, test_loss: 1.7120, per_image_load_time: 2.041ms, per_image_inference_time: 0.538ms
2022-06-26 10:47:14 - until epoch: 027, best_acc1: 59.122%
2022-06-26 10:47:14 - epoch 028 lr: 0.100000
2022-06-26 10:47:53 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 1.7125
2022-06-26 10:48:27 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 1.8893
2022-06-26 10:49:01 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 1.9007
2022-06-26 10:49:35 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 1.8276
2022-06-26 10:50:09 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 1.7958
2022-06-26 10:50:43 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.1520
2022-06-26 10:51:17 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.2156
2022-06-26 10:51:51 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.6723
2022-06-26 10:52:24 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 1.8403
2022-06-26 10:52:58 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 1.9900
2022-06-26 10:53:33 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.0473
2022-06-26 10:54:06 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 1.7759
2022-06-26 10:54:40 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 1.9308
2022-06-26 10:55:15 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.2384
2022-06-26 10:55:50 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 1.9760
2022-06-26 10:56:23 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 1.9814
2022-06-26 10:56:57 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 1.9033
2022-06-26 10:57:31 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 1.8230
2022-06-26 10:58:06 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 1.9917
2022-06-26 10:58:41 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.0262
2022-06-26 10:59:15 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 1.9454
2022-06-26 10:59:49 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.0406
2022-06-26 11:00:24 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.2071
2022-06-26 11:00:58 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.1428
2022-06-26 11:01:33 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 1.9990
2022-06-26 11:02:06 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 1.7677
2022-06-26 11:02:40 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 1.9913
2022-06-26 11:03:16 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 1.8754
2022-06-26 11:03:49 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 1.9811
2022-06-26 11:04:24 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.0943
2022-06-26 11:04:58 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.1681
2022-06-26 11:05:32 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 1.9846
2022-06-26 11:06:05 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.0815
2022-06-26 11:06:39 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 1.8990
2022-06-26 11:07:14 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 1.9772
2022-06-26 11:07:49 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 1.8516
2022-06-26 11:08:22 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 1.9528
2022-06-26 11:08:56 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.7075
2022-06-26 11:09:30 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 1.9697
2022-06-26 11:10:04 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.0799
2022-06-26 11:10:38 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.1255
2022-06-26 11:11:12 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.0775
2022-06-26 11:11:46 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 1.8689
2022-06-26 11:12:22 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 1.9616
2022-06-26 11:12:55 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.1237
2022-06-26 11:13:29 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.0310
2022-06-26 11:14:04 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.0873
2022-06-26 11:14:37 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 1.8516
2022-06-26 11:15:12 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 1.7973
2022-06-26 11:15:45 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 1.8831
2022-06-26 11:15:46 - train: epoch 028, train_loss: 1.9687
2022-06-26 11:17:01 - eval: epoch: 028, acc1: 57.874%, acc5: 81.924%, test_loss: 1.7761, per_image_load_time: 2.362ms, per_image_inference_time: 0.538ms
2022-06-26 11:17:02 - until epoch: 028, best_acc1: 59.122%
2022-06-26 11:17:02 - epoch 029 lr: 0.100000
2022-06-26 11:17:41 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.0391
2022-06-26 11:18:15 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.0234
2022-06-26 11:18:51 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.1431
2022-06-26 11:19:25 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 1.9680
2022-06-26 11:19:59 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 1.9375
2022-06-26 11:20:33 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.1302
2022-06-26 11:21:07 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.5037
2022-06-26 11:21:42 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.0846
2022-06-26 11:22:16 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 1.8495
2022-06-26 11:22:51 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 1.7840
2022-06-26 11:23:25 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 1.8837
2022-06-26 11:23:59 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.0132
2022-06-26 11:24:32 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 1.8660
2022-06-26 11:25:06 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.1169
2022-06-26 11:25:40 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 1.9713
2022-06-26 11:26:14 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 1.8764
2022-06-26 11:26:48 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.0640
2022-06-26 11:27:22 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.0248
2022-06-26 11:27:55 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 1.8030
2022-06-26 11:28:31 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.1187
2022-06-26 11:29:04 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 1.9924
2022-06-26 11:29:39 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.0223
2022-06-26 11:30:12 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.0175
2022-06-26 11:30:46 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 1.8803
2022-06-26 11:31:20 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 1.9013
2022-06-26 11:31:53 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.0101
2022-06-26 11:32:27 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 1.9280
2022-06-26 11:33:02 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 1.8629
2022-06-26 11:33:35 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 1.8769
2022-06-26 11:34:10 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 1.8659
2022-06-26 11:34:44 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 1.9616
2022-06-26 11:35:18 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 1.9871
2022-06-26 11:35:53 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 1.9043
2022-06-26 11:36:28 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 1.7326
2022-06-26 11:37:01 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 1.9927
2022-06-26 11:37:36 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 1.7961
2022-06-26 11:38:09 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 1.8674
2022-06-26 11:38:43 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.1337
2022-06-26 11:39:18 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 1.7866
2022-06-26 11:39:52 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 1.8267
2022-06-26 11:40:26 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 1.8407
2022-06-26 11:41:01 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 1.8648
2022-06-26 11:41:34 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.0310
2022-06-26 11:42:09 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 1.9874
2022-06-26 11:42:43 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.0982
2022-06-26 11:43:17 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.1730
2022-06-26 11:43:51 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 1.8062
2022-06-26 11:44:25 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.0213
2022-06-26 11:44:59 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.2793
2022-06-26 11:45:32 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.7269
2022-06-26 11:45:33 - train: epoch 029, train_loss: 1.9650
2022-06-26 11:46:48 - eval: epoch: 029, acc1: 55.658%, acc5: 79.982%, test_loss: 1.8964, per_image_load_time: 1.730ms, per_image_inference_time: 0.514ms
2022-06-26 11:46:48 - until epoch: 029, best_acc1: 59.122%
2022-06-26 11:46:48 - epoch 030 lr: 0.100000
2022-06-26 11:47:26 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.0283
2022-06-26 11:48:00 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.0486
2022-06-26 11:48:34 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 1.8368
2022-06-26 11:49:06 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 1.6455
2022-06-26 11:49:41 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.2933
2022-06-26 11:50:14 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 1.8206
2022-06-26 11:50:47 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 1.9142
2022-06-26 11:51:21 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.0129
2022-06-26 11:51:55 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.0022
2022-06-26 11:52:28 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 1.6395
2022-06-26 11:53:02 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 1.7550
2022-06-26 11:53:35 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.0278
2022-06-26 11:54:10 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 1.8212
2022-06-26 11:54:44 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 1.8908
2022-06-26 11:55:18 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 1.9239
2022-06-26 11:55:52 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 1.9459
2022-06-26 11:56:25 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.1986
2022-06-26 11:56:59 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 1.9411
2022-06-26 11:57:34 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.1016
2022-06-26 11:58:08 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 1.9064
2022-06-26 11:58:42 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 1.9742
2022-06-26 11:59:16 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 1.9355
2022-06-26 11:59:50 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 1.9788
2022-06-26 12:00:23 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.2175
2022-06-26 12:00:57 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 1.9927
2022-06-26 12:01:31 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 1.9407
2022-06-26 12:02:05 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 1.8813
2022-06-26 12:02:39 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.0528
2022-06-26 12:03:12 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 1.9838
2022-06-26 12:03:46 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.0854
2022-06-26 12:04:20 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 1.8938
2022-06-26 12:04:54 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 1.9409
2022-06-26 12:05:28 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.1443
2022-06-26 12:06:01 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 1.9566
2022-06-26 12:06:37 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.0777
2022-06-26 12:07:10 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 1.8191
2022-06-26 12:07:44 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.0319
2022-06-26 12:08:17 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.0470
2022-06-26 12:08:51 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.0594
2022-06-26 12:09:25 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 1.8561
2022-06-26 12:09:59 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 1.8313
2022-06-26 12:10:33 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.1249
2022-06-26 12:11:07 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.0166
2022-06-26 12:11:41 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 1.9828
2022-06-26 12:12:15 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.0809
2022-06-26 12:12:49 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 1.7397
2022-06-26 12:13:23 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 1.8556
2022-06-26 12:13:57 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 1.9288
2022-06-26 12:14:31 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.2006
2022-06-26 12:15:03 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.0224
2022-06-26 12:15:04 - train: epoch 030, train_loss: 1.9641
2022-06-26 12:16:18 - eval: epoch: 030, acc1: 59.096%, acc5: 83.216%, test_loss: 1.7074, per_image_load_time: 1.332ms, per_image_inference_time: 0.506ms
2022-06-26 12:16:19 - until epoch: 030, best_acc1: 59.122%
2022-06-26 12:16:19 - epoch 031 lr: 0.010000
2022-06-26 12:16:58 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 1.8270
2022-06-26 12:17:31 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.5765
2022-06-26 12:18:05 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.5972
2022-06-26 12:18:37 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.7747
2022-06-26 12:19:11 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.5251
2022-06-26 12:19:45 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.4955
2022-06-26 12:20:19 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.5443
2022-06-26 12:20:54 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.5319
2022-06-26 12:21:28 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.4689
2022-06-26 12:22:01 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 1.6829
2022-06-26 12:22:35 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 1.8096
2022-06-26 12:23:10 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.5292
2022-06-26 12:23:44 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.3084
2022-06-26 12:24:19 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.5128
2022-06-26 12:24:52 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.5654
2022-06-26 12:25:26 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.3770
2022-06-26 12:25:59 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.3561
2022-06-26 12:26:34 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.4211
2022-06-26 12:27:07 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.5573
2022-06-26 12:27:41 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.4580
2022-06-26 12:28:15 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.3042
2022-06-26 12:28:49 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.2721
2022-06-26 12:29:24 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.3129
2022-06-26 12:29:58 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.4263
2022-06-26 12:30:32 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.4194
2022-06-26 12:31:06 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.5203
2022-06-26 12:31:40 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.5081
2022-06-26 12:32:14 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.7385
2022-06-26 12:32:49 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.5371
2022-06-26 12:33:23 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.5583
2022-06-26 12:33:57 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.3225
2022-06-26 12:34:31 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.6055
2022-06-26 12:35:05 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.5195
2022-06-26 12:35:40 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.5061
2022-06-26 12:36:14 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.6395
2022-06-26 12:36:47 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.4472
2022-06-26 12:37:21 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.3655
2022-06-26 12:37:56 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.3205
2022-06-26 12:38:30 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.2539
2022-06-26 12:39:04 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.3555
2022-06-26 12:39:39 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.3797
2022-06-26 12:40:13 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.4709
2022-06-26 12:40:46 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.3679
2022-06-26 12:41:20 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.5299
2022-06-26 12:41:54 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.5976
2022-06-26 12:42:29 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.4171
2022-06-26 12:43:02 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.4747
2022-06-26 12:43:37 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.3051
2022-06-26 12:44:12 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.3642
2022-06-26 12:44:45 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.4059
2022-06-26 12:44:46 - train: epoch 031, train_loss: 1.4826
2022-06-26 12:46:01 - eval: epoch: 031, acc1: 70.372%, acc5: 89.844%, test_loss: 1.1967, per_image_load_time: 2.419ms, per_image_inference_time: 0.522ms
2022-06-26 12:46:02 - until epoch: 031, best_acc1: 70.372%
2022-06-26 12:46:02 - epoch 032 lr: 0.010000
2022-06-26 12:46:41 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.3104
2022-06-26 12:47:16 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.4028
2022-06-26 12:47:49 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.5016
2022-06-26 12:48:23 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.5538
2022-06-26 12:48:58 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.4776
2022-06-26 12:49:31 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.3582
2022-06-26 12:50:06 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.4010
2022-06-26 12:50:38 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.3221
2022-06-26 12:51:14 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.3813
2022-06-26 12:51:49 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.4880
2022-06-26 12:52:22 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.4751
2022-06-26 12:52:56 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.3843
2022-06-26 12:53:32 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.3024
2022-06-26 12:54:06 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.5032
2022-06-26 12:54:40 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.3503
2022-06-26 12:55:13 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.4737
2022-06-26 12:55:49 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.5002
2022-06-26 12:56:24 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.7077
2022-06-26 12:56:57 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.2619
2022-06-26 12:57:32 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.3585
2022-06-26 12:58:06 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.3497
2022-06-26 12:58:41 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.2523
2022-06-26 12:59:17 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.3461
2022-06-26 12:59:51 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.2989
2022-06-26 13:00:26 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.4842
2022-06-26 13:01:00 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.1241
2022-06-26 13:01:34 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.3535
2022-06-26 13:02:10 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.4467
2022-06-26 13:02:45 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.0841
2022-06-26 13:03:20 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.1013
2022-06-26 13:03:54 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.4012
2022-06-26 13:04:30 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.3571
2022-06-26 13:05:03 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.2891
2022-06-26 13:05:39 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.1912
2022-06-26 13:06:14 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.3996
2022-06-26 13:06:49 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.4277
2022-06-26 13:07:24 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.3978
2022-06-26 13:07:58 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.1700
2022-06-26 13:08:33 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.3942
2022-06-26 13:09:07 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.2571
2022-06-26 13:09:42 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.3452
2022-06-26 13:10:15 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.1833
2022-06-26 13:10:51 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.5839
2022-06-26 13:11:25 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.3333
2022-06-26 13:12:00 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.6333
2022-06-26 13:12:35 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.3939
2022-06-26 13:13:09 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.4473
2022-06-26 13:13:44 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.2285
2022-06-26 13:14:19 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.5027
2022-06-26 13:14:53 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.3947
2022-06-26 13:14:54 - train: epoch 032, train_loss: 1.3591
2022-06-26 13:16:11 - eval: epoch: 032, acc1: 71.146%, acc5: 90.284%, test_loss: 1.1542, per_image_load_time: 2.400ms, per_image_inference_time: 0.521ms
2022-06-26 13:16:11 - until epoch: 032, best_acc1: 71.146%
2022-06-26 13:16:11 - epoch 033 lr: 0.010000
2022-06-26 13:16:51 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.1539
2022-06-26 13:17:26 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.4913
2022-06-26 13:18:00 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.1894
2022-06-26 13:18:35 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.1991
2022-06-26 13:19:10 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.4380
2022-06-26 13:19:44 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.1953
2022-06-26 13:20:20 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.3731
2022-06-26 13:20:54 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.3422
2022-06-26 13:21:27 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.3456
2022-06-26 13:22:02 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.3446
2022-06-26 13:22:38 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.2861
2022-06-26 13:23:12 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.2407
2022-06-26 13:23:47 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.1553
2022-06-26 13:24:22 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.4302
2022-06-26 13:24:57 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.5204
2022-06-26 13:25:31 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.4225
2022-06-26 13:26:06 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.0663
2022-06-26 13:26:41 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.5382
2022-06-26 13:27:16 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.3466
2022-06-26 13:27:51 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.1987
2022-06-26 13:28:26 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.3700
2022-06-26 13:29:00 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.3527
2022-06-26 13:29:35 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.2218
2022-06-26 13:30:10 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.5573
2022-06-26 13:30:45 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.3636
2022-06-26 13:31:19 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.1853
2022-06-26 13:31:53 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.4753
2022-06-26 13:32:28 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.2748
2022-06-26 13:33:02 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.3741
2022-06-26 13:33:37 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.4018
2022-06-26 13:34:12 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.3511
2022-06-26 13:34:46 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.2675
2022-06-26 13:35:20 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.4684
2022-06-26 13:35:55 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.2807
2022-06-26 13:36:29 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.3103
2022-06-26 13:37:04 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.4603
2022-06-26 13:37:39 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.3218
2022-06-26 13:38:14 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.1646
2022-06-26 13:38:48 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.3775
2022-06-26 13:39:24 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.3326
2022-06-26 13:39:59 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.2923
2022-06-26 13:40:34 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.1870
2022-06-26 13:41:09 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.4000
2022-06-26 13:41:44 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.3855
2022-06-26 13:42:18 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.4598
2022-06-26 13:42:53 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.2258
2022-06-26 13:43:27 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.1518
2022-06-26 13:44:03 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.6323
2022-06-26 13:44:39 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.1495
2022-06-26 13:45:12 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.2916
2022-06-26 13:45:13 - train: epoch 033, train_loss: 1.3074
2022-06-26 13:46:30 - eval: epoch: 033, acc1: 71.866%, acc5: 90.672%, test_loss: 1.1270, per_image_load_time: 1.316ms, per_image_inference_time: 0.501ms
2022-06-26 13:46:30 - until epoch: 033, best_acc1: 71.866%
2022-06-26 13:46:30 - epoch 034 lr: 0.010000
2022-06-26 13:47:10 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.2517
2022-06-26 13:47:44 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.2892
2022-06-26 13:48:18 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.2849
2022-06-26 13:48:54 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.0663
2022-06-26 13:49:30 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.3691
2022-06-26 13:50:05 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.5409
2022-06-26 13:50:39 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.2473
2022-06-26 13:51:15 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.2234
2022-06-26 13:51:52 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.2372
2022-06-26 13:52:27 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.2100
2022-06-26 13:53:02 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.2223
2022-06-26 13:53:37 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.1874
2022-06-26 13:54:11 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.2459
2022-06-26 13:54:45 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.3085
2022-06-26 13:55:20 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.2121
2022-06-26 13:55:55 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.1336
2022-06-26 13:56:30 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.1774
2022-06-26 13:57:06 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.4227
2022-06-26 13:57:39 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.3509
2022-06-26 13:58:14 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.3235
2022-06-26 13:58:49 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.4400
2022-06-26 13:59:23 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.1928
2022-06-26 13:59:59 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.3765
2022-06-26 14:00:33 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.1462
2022-06-26 14:01:08 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.2450
2022-06-26 14:01:42 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.3210
2022-06-26 14:02:18 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.2639
2022-06-26 14:03:09 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.0939
2022-06-26 14:03:55 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.0906
2022-06-26 14:04:28 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.1230
2022-06-26 14:05:04 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.1131
2022-06-26 14:05:37 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.2722
2022-06-26 14:06:12 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.1777
2022-06-26 14:06:46 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.2932
2022-06-26 14:07:21 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.1486
2022-06-26 14:07:55 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.1040
2022-06-26 14:08:30 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.1762
2022-06-26 14:09:04 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.2748
2022-06-26 14:09:38 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.3166
2022-06-26 14:10:12 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.1122
2022-06-26 14:10:47 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.2568
2022-06-26 14:11:21 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.2314
2022-06-26 14:11:56 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.3330
2022-06-26 14:12:30 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.3563
2022-06-26 14:13:05 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.4859
2022-06-26 14:13:39 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.3967
2022-06-26 14:14:13 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.2303
2022-06-26 14:14:47 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.1371
2022-06-26 14:15:22 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.2459
2022-06-26 14:15:55 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.0727
2022-06-26 14:15:56 - train: epoch 034, train_loss: 1.2756
2022-06-26 14:17:11 - eval: epoch: 034, acc1: 71.706%, acc5: 90.690%, test_loss: 1.1274, per_image_load_time: 2.235ms, per_image_inference_time: 0.483ms
2022-06-26 14:17:11 - until epoch: 034, best_acc1: 71.866%
2022-06-26 14:17:11 - epoch 035 lr: 0.010000
2022-06-26 14:17:51 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.1448
2022-06-26 14:18:24 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 0.9844
2022-06-26 14:18:59 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.4858
2022-06-26 14:19:33 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.1100
2022-06-26 14:20:06 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.2924
2022-06-26 14:20:41 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.2193
2022-06-26 14:21:14 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.2128
2022-06-26 14:21:49 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.2062
2022-06-26 14:22:22 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.4388
2022-06-26 14:22:57 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.4203
2022-06-26 14:23:31 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.4477
2022-06-26 14:24:05 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.1986
2022-06-26 14:24:40 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.2971
2022-06-26 14:25:14 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.2923
2022-06-26 14:25:47 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.3248
2022-06-26 14:26:22 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.2837
2022-06-26 14:26:56 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.1300
2022-06-26 14:27:30 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.3291
2022-06-26 14:28:05 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.3173
2022-06-26 14:28:39 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.1877
2022-06-26 14:29:14 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.1826
2022-06-26 14:29:48 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.3547
2022-06-26 14:30:21 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.2474
2022-06-26 14:30:57 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.2693
2022-06-26 14:31:30 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.1474
2022-06-26 14:32:04 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.3933
2022-06-26 14:32:39 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.4214
2022-06-26 14:33:13 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.2200
2022-06-26 14:33:47 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.2341
2022-06-26 14:34:22 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.4143
2022-06-26 14:34:56 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.2657
2022-06-26 14:35:30 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.0979
2022-06-26 14:36:05 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.1394
2022-06-26 14:36:39 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.1991
2022-06-26 14:37:13 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.0150
2022-06-26 14:37:48 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.2247
2022-06-26 14:38:21 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.0777
2022-06-26 14:38:57 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.2589
2022-06-26 14:39:31 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.3969
2022-06-26 14:40:05 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.0856
2022-06-26 14:40:40 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.4754
2022-06-26 14:41:14 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.1772
2022-06-26 14:41:48 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.3370
2022-06-26 14:42:23 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.3100
2022-06-26 14:42:58 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.3709
2022-06-26 14:43:31 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.1673
2022-06-26 14:44:07 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.4990
2022-06-26 14:44:42 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.4793
2022-06-26 14:45:16 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.3299
2022-06-26 14:45:50 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.2167
2022-06-26 14:45:51 - train: epoch 035, train_loss: 1.2528
2022-06-26 14:47:07 - eval: epoch: 035, acc1: 71.468%, acc5: 90.674%, test_loss: 1.1362, per_image_load_time: 2.425ms, per_image_inference_time: 0.494ms
2022-06-26 14:47:07 - until epoch: 035, best_acc1: 71.866%
2022-06-26 15:48:49 - epoch 036 lr: 0.010000
2022-06-26 15:49:32 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.3379
2022-06-26 15:50:43 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.2265
2022-06-26 15:51:19 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.1361
2022-06-26 15:51:55 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.2629
2022-06-26 15:52:32 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.0461
2022-06-26 15:53:09 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.2298
2022-06-26 15:53:45 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.0682
2022-06-26 15:54:20 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.2599
2022-06-26 15:54:55 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.1786
2022-06-26 15:55:31 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.1574
2022-06-26 15:56:07 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.1270
2022-06-26 15:56:42 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.0542
2022-06-26 15:57:17 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.3027
2022-06-26 15:57:51 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.5011
2022-06-26 15:58:25 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.2511
2022-06-26 15:58:59 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.2692
2022-06-26 15:59:33 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.1800
2022-06-26 16:00:06 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.2328
2022-06-26 16:00:40 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.1437
2022-06-26 16:01:15 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.3237
2022-06-26 16:01:49 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.2855
2022-06-26 16:02:23 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.2942
2022-06-26 16:02:57 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.3420
2022-06-26 16:03:31 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.3031
2022-06-26 16:04:05 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.1281
2022-06-26 16:04:38 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.1055
2022-06-26 16:05:13 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.2543
2022-06-26 16:05:47 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.0359
2022-06-26 16:06:21 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.3253
2022-06-26 16:06:55 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.2167
2022-06-26 16:07:29 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.3994
2022-06-26 16:08:04 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.1456
2022-06-26 16:08:38 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.2073
2022-06-26 16:09:11 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.2388
2022-06-26 16:09:46 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.3450
2022-06-26 16:10:20 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.1574
2022-06-26 16:10:55 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.2985
2022-06-26 16:11:29 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.4349
2022-06-26 16:12:04 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.3028
2022-06-26 16:12:38 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.1776
2022-06-26 16:13:13 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.3062
2022-06-26 16:13:47 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.1849
2022-06-26 16:14:21 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.2405
2022-06-26 16:14:56 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.2337
2022-06-26 16:15:30 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.2487
2022-06-26 16:16:04 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.1872
2022-06-26 16:16:38 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.0662
2022-06-26 16:17:12 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.1146
2022-06-26 16:17:45 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.1086
2022-06-26 16:17:46 - train: epoch 036, train_loss: 1.2353
2022-06-26 16:19:02 - eval: epoch: 036, acc1: 72.220%, acc5: 90.796%, test_loss: 1.1171, per_image_load_time: 1.977ms, per_image_inference_time: 0.538ms
2022-06-26 16:19:02 - until epoch: 036, best_acc1: 72.220%
2022-06-26 17:37:29 - epoch 037 lr: 0.010000
2022-06-26 17:38:08 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.0882
2022-06-26 17:38:41 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.1156
2022-06-26 17:39:14 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.3129
2022-06-26 17:39:49 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.0182
2022-06-26 17:40:23 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.0690
2022-06-26 17:40:56 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.2180
2022-06-26 17:41:30 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.3923
2022-06-26 17:42:03 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.0627
2022-06-26 17:42:36 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.4064
2022-06-26 17:43:09 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.2715
2022-06-26 17:43:44 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.1849
2022-06-26 17:44:16 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.1970
2022-06-26 17:44:51 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.3024
2022-06-26 17:45:25 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.3979
2022-06-26 17:45:58 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.2491
2022-06-26 17:46:32 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.0901
2022-06-26 17:47:06 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.2418
2022-06-26 17:47:40 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.0834
2022-06-26 17:48:14 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.0986
2022-06-26 17:48:48 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.1424
2022-06-26 17:49:21 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.2735
2022-06-26 17:49:56 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.0454
2022-06-26 17:50:30 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.2167
2022-06-26 17:51:04 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.3251
2022-06-26 17:51:38 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.1883
2022-06-26 17:52:11 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.3911
2022-06-26 17:52:45 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.4400
2022-06-26 17:53:19 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.2068
2022-06-26 17:53:53 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.1522
2022-06-26 17:54:27 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.4549
2022-06-26 17:55:01 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.1559
2022-06-26 17:55:34 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.2071
2022-06-26 17:56:09 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.1272
2022-06-26 17:56:42 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.1553
2022-06-26 17:57:17 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.2428
2022-06-26 17:57:50 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.2894
2022-06-26 17:58:24 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.3290
2022-06-26 17:58:56 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.3123
2022-06-26 17:59:30 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.3456
2022-06-26 18:00:04 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.2583
2022-06-26 18:00:37 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.2235
2022-06-26 18:01:12 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.3684
2022-06-26 18:01:45 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.3030
2022-06-26 18:02:19 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.2140
2022-06-26 18:02:53 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.1348
2022-06-26 18:03:27 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.1162
2022-06-26 18:04:00 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.2613
2022-06-26 18:04:34 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.2588
2022-06-26 18:05:08 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.2462
2022-06-26 18:05:41 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.2307
2022-06-26 18:05:42 - train: epoch 037, train_loss: 1.2234
2022-06-26 18:06:58 - eval: epoch: 037, acc1: 72.274%, acc5: 91.038%, test_loss: 1.1049, per_image_load_time: 2.429ms, per_image_inference_time: 0.494ms
2022-06-26 18:06:58 - until epoch: 037, best_acc1: 72.274%
2022-06-26 18:06:58 - epoch 038 lr: 0.010000
2022-06-26 18:07:37 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.3672
2022-06-26 18:08:11 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 0.9948
2022-06-26 18:08:45 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 0.9592
2022-06-26 18:09:19 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.1317
2022-06-26 18:09:54 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.0621
2022-06-26 18:10:28 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.1538
2022-06-26 18:11:02 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.0894
2022-06-26 18:11:36 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.0791
2022-06-26 18:12:11 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.1849
2022-06-26 18:12:44 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.4205
2022-06-26 18:13:19 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.1517
2022-06-26 18:13:52 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.3134
2022-06-26 18:14:27 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.2166
2022-06-26 18:15:00 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.2118
2022-06-26 18:15:34 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.2011
2022-06-26 18:16:08 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.2070
2022-06-26 18:16:41 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.3489
2022-06-26 18:17:15 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.4931
2022-06-26 18:17:49 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.2017
2022-06-26 18:18:23 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.4239
2022-06-26 18:18:57 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.1405
2022-06-26 18:19:32 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.2491
2022-06-26 18:20:06 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.2431
2022-06-26 18:20:41 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.4415
2022-06-26 18:21:15 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.0694
2022-06-26 18:21:49 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.2417
2022-06-26 18:22:23 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.2419
2022-06-26 18:22:57 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.3504
2022-06-26 18:23:32 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.1462
2022-06-26 18:24:06 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.2035
2022-06-26 18:24:39 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.2392
2022-06-26 18:25:14 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 0.9452
2022-06-26 18:25:48 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.2517
2022-06-26 18:26:22 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.2681
2022-06-26 18:26:57 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.1041
2022-06-26 18:27:31 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.2638
2022-06-26 18:28:05 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.1492
2022-06-26 18:28:39 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.3565
2022-06-26 18:29:14 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.2767
2022-06-26 18:29:47 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.0615
2022-06-26 18:30:22 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.2706
2022-06-26 18:30:56 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.1347
2022-06-26 18:31:30 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.1499
2022-06-26 18:32:05 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.3376
2022-06-26 18:32:39 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.3800
2022-06-26 18:33:13 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.2543
2022-06-26 18:33:48 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.2224
2022-06-26 18:34:21 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.1509
2022-06-26 18:34:56 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.0420
2022-06-26 18:35:28 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.0272
2022-06-26 18:35:30 - train: epoch 038, train_loss: 1.2115
2022-06-26 18:36:45 - eval: epoch: 038, acc1: 72.596%, acc5: 91.070%, test_loss: 1.0973, per_image_load_time: 2.211ms, per_image_inference_time: 0.512ms
2022-06-26 18:36:45 - until epoch: 038, best_acc1: 72.596%
2022-06-26 18:36:45 - epoch 039 lr: 0.010000
2022-06-26 18:37:24 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.1811
2022-06-26 18:37:58 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.2687
2022-06-26 18:38:32 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.0857
2022-06-26 18:39:06 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.0937
2022-06-26 18:39:40 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.3105
2022-06-26 18:40:14 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.1949
2022-06-26 18:40:47 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.4720
2022-06-26 18:41:22 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.0458
2022-06-26 18:41:56 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.2504
2022-06-26 18:42:30 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.0959
2022-06-26 18:43:03 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.2256
2022-06-26 18:43:38 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.2709
2022-06-26 18:44:12 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.2436
2022-06-26 18:44:45 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.2843
2022-06-26 18:45:20 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.1827
2022-06-26 18:45:53 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.0277
2022-06-26 18:46:28 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.0810
2022-06-26 18:47:03 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.2083
2022-06-26 18:47:37 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 0.9584
2022-06-26 18:48:11 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.2017
2022-06-26 18:48:45 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.2960
2022-06-26 18:49:19 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.2806
2022-06-26 18:49:52 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.4151
2022-06-26 18:50:26 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.2187
2022-06-26 18:51:01 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.2416
2022-06-26 18:51:35 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.1155
2022-06-26 18:52:08 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.2297
2022-06-26 18:52:43 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.1701
2022-06-26 18:53:17 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.0422
2022-06-26 18:53:51 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.3915
2022-06-26 18:54:26 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 0.9931
2022-06-26 18:55:00 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.4032
2022-06-26 18:55:34 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.2899
2022-06-26 18:56:08 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.2428
2022-06-26 18:56:41 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.2562
2022-06-26 18:57:15 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.2165
2022-06-26 18:57:50 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.1404
2022-06-26 18:58:25 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.1404
2022-06-26 18:58:58 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.1259
2022-06-26 18:59:34 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.0753
2022-06-26 19:00:07 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.3809
2022-06-26 19:00:41 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.2772
2022-06-26 19:01:15 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.2743
2022-06-26 19:01:49 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.0898
2022-06-26 19:02:24 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.1944
2022-06-26 19:02:58 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.3410
2022-06-26 19:03:32 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.1749
2022-06-26 19:04:06 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.1505
2022-06-26 19:04:40 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.2034
2022-06-26 19:05:14 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.1628
2022-06-26 19:05:15 - train: epoch 039, train_loss: 1.2055
2022-06-26 19:06:30 - eval: epoch: 039, acc1: 72.256%, acc5: 91.068%, test_loss: 1.1023, per_image_load_time: 2.346ms, per_image_inference_time: 0.505ms
2022-06-26 19:06:30 - until epoch: 039, best_acc1: 72.596%
2022-06-26 19:06:30 - epoch 040 lr: 0.010000
2022-06-26 19:07:10 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.2473
2022-06-26 19:07:43 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.2833
2022-06-26 19:08:17 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.1202
2022-06-26 19:08:51 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.0758
2022-06-26 19:09:25 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.1355
2022-06-26 19:09:59 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.1844
2022-06-26 19:10:33 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.1851
2022-06-26 19:11:07 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.3489
2022-06-26 19:11:42 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.1271
2022-06-26 19:12:16 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.1123
2022-06-26 19:12:50 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.0314
2022-06-26 19:13:24 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.0377
2022-06-26 19:13:58 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 0.9934
2022-06-26 19:14:32 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.2425
2022-06-26 19:15:06 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.1483
2022-06-26 19:15:39 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.3268
2022-06-26 19:16:14 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.2217
2022-06-26 19:16:47 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.1731
2022-06-26 19:17:21 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.3300
2022-06-26 19:17:55 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.2185
2022-06-26 19:18:30 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.0595
2022-06-26 19:19:03 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.2045
2022-06-26 19:19:37 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.1094
2022-06-26 19:20:12 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.2302
2022-06-26 19:20:47 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.5207
2022-06-26 19:21:20 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.3485
2022-06-26 19:21:55 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.4486
2022-06-26 19:22:29 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.1274
2022-06-26 19:23:03 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.4300
2022-06-26 19:23:37 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.2745
2022-06-26 19:24:12 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.2516
2022-06-26 19:24:45 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.4049
2022-06-26 19:25:20 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.1990
2022-06-26 19:25:55 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.1458
2022-06-26 19:26:29 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.2758
2022-06-26 19:27:02 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.2806
2022-06-26 19:27:36 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.1717
2022-06-26 19:28:11 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.0946
2022-06-26 19:28:45 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.1830
2022-06-26 19:29:19 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.1811
2022-06-26 19:29:54 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.3539
2022-06-26 19:30:28 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.1402
2022-06-26 19:31:02 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.2381
2022-06-26 19:31:38 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.0658
2022-06-26 19:32:11 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.1564
2022-06-26 19:32:46 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.3857
2022-06-26 19:33:22 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.1870
2022-06-26 19:33:56 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.0259
2022-06-26 19:34:32 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.2440
2022-06-26 19:35:05 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.1007
2022-06-26 19:35:06 - train: epoch 040, train_loss: 1.2007
2022-06-26 19:36:22 - eval: epoch: 040, acc1: 72.192%, acc5: 91.026%, test_loss: 1.1087, per_image_load_time: 2.371ms, per_image_inference_time: 0.500ms
2022-06-26 19:36:23 - until epoch: 040, best_acc1: 72.596%
2022-06-26 19:36:23 - epoch 041 lr: 0.010000
2022-06-26 19:37:02 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.3133
2022-06-26 19:37:37 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.4043
2022-06-26 19:38:10 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.2357
2022-06-26 19:38:46 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.2518
2022-06-26 19:39:19 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.0437
2022-06-26 19:39:55 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.1578
2022-06-26 19:40:28 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.0827
2022-06-26 19:41:03 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.0780
2022-06-26 19:41:37 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 0.9949
2022-06-26 19:42:12 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.2800
2022-06-26 19:42:46 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.0555
2022-06-26 19:43:20 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.1985
2022-06-26 19:43:54 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 0.9921
2022-06-26 19:44:29 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.1015
2022-06-26 19:45:03 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.3147
2022-06-26 19:45:38 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.1778
2022-06-26 19:46:13 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.3477
2022-06-26 19:46:47 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.3523
2022-06-26 19:47:22 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.3575
2022-06-26 19:47:58 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 0.9595
2022-06-26 19:48:32 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.2265
2022-06-26 19:49:07 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.0762
2022-06-26 19:49:42 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.1088
2022-06-26 19:50:16 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.4216
2022-06-26 19:50:51 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.2071
2022-06-26 19:51:25 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.2941
2022-06-26 19:52:00 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.4308
2022-06-26 19:52:35 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.1434
2022-06-26 19:53:09 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.0814
2022-06-26 19:53:45 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.2012
2022-06-26 19:54:19 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.1966
2022-06-26 19:54:54 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.1064
2022-06-26 19:55:29 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.0998
2022-06-26 19:56:04 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.0640
2022-06-26 19:56:38 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.2596
2022-06-26 19:57:12 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.3042
2022-06-26 19:57:46 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.3567
2022-06-26 19:58:20 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.2419
2022-06-26 19:58:55 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.2179
2022-06-26 19:59:30 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.1837
2022-06-26 20:00:03 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.2784
2022-06-26 20:00:38 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.1618
2022-06-26 20:01:13 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.1349
2022-06-26 20:01:47 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.1851
2022-06-26 20:02:22 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.1960
2022-06-26 20:02:56 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.2675
2022-06-26 20:03:31 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.2963
2022-06-26 20:04:05 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.1196
2022-06-26 20:04:40 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.3618
2022-06-26 20:05:12 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.0947
2022-06-26 20:05:13 - train: epoch 041, train_loss: 1.1987
2022-06-26 20:06:30 - eval: epoch: 041, acc1: 72.228%, acc5: 91.034%, test_loss: 1.1068, per_image_load_time: 2.421ms, per_image_inference_time: 0.476ms
2022-06-26 20:06:30 - until epoch: 041, best_acc1: 72.596%
2022-06-26 20:06:30 - epoch 042 lr: 0.010000
2022-06-26 20:07:08 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.0459
2022-06-26 20:07:43 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.3083
2022-06-26 20:08:17 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.1948
2022-06-26 20:08:51 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.1366
2022-06-26 20:09:25 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.2714
2022-06-26 20:09:59 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.0102
2022-06-26 20:10:33 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.3185
2022-06-26 20:11:07 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.2200
2022-06-26 20:11:41 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.2792
2022-06-26 20:12:15 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.2766
2022-06-26 20:12:49 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.2187
2022-06-26 20:13:23 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.1496
2022-06-26 20:13:58 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.3066
2022-06-26 20:14:31 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.5102
2022-06-26 20:15:07 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 0.8736
2022-06-26 20:15:42 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.2818
2022-06-26 20:16:16 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.1674
2022-06-26 20:16:50 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.1108
2022-06-26 20:17:24 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.3403
2022-06-26 20:17:59 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 0.9439
2022-06-26 20:18:33 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.0760
2022-06-26 20:19:07 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.2894
2022-06-26 20:19:41 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.2620
2022-06-26 20:20:17 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.2918
2022-06-26 20:20:50 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.1360
2022-06-26 20:21:25 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.3611
2022-06-26 20:21:59 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.0539
2022-06-26 20:22:33 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.1438
2022-06-26 20:23:07 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.1881
2022-06-26 20:23:42 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.0693
2022-06-26 20:24:17 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.2543
2022-06-26 20:24:51 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.2301
2022-06-26 20:25:25 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.1910
2022-06-26 20:25:59 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.1700
2022-06-26 20:26:34 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.3988
2022-06-26 20:27:09 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.4462
2022-06-26 20:27:43 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.1472
2022-06-26 20:28:18 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 0.9353
2022-06-26 20:28:52 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.1017
2022-06-26 20:29:27 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.2415
2022-06-26 20:30:02 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.2710
2022-06-26 20:30:37 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.1521
2022-06-26 20:31:11 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.0682
2022-06-26 20:31:45 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.0769
2022-06-26 20:32:20 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.1738
2022-06-26 20:32:54 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.2023
2022-06-26 20:33:27 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.2896
2022-06-26 20:34:02 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.1524
2022-06-26 20:34:36 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.2328
2022-06-26 20:35:09 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.3382
2022-06-26 20:35:10 - train: epoch 042, train_loss: 1.1941
2022-06-26 20:36:25 - eval: epoch: 042, acc1: 71.386%, acc5: 90.674%, test_loss: 1.1406, per_image_load_time: 2.389ms, per_image_inference_time: 0.496ms
2022-06-26 20:36:25 - until epoch: 042, best_acc1: 72.596%
2022-06-26 20:36:25 - epoch 043 lr: 0.010000
2022-06-26 20:37:05 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.3812
2022-06-26 20:37:38 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.3454
2022-06-26 20:38:12 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.0804
2022-06-26 20:38:46 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.0848
2022-06-26 20:39:20 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.0925
2022-06-26 20:39:53 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.0188
2022-06-26 20:40:27 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.3360
2022-06-26 20:41:01 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.3459
2022-06-26 20:41:36 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.1146
2022-06-26 20:42:10 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.1812
2022-06-26 20:42:44 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.2678
2022-06-26 20:43:18 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.1519
2022-06-26 20:43:52 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.3220
2022-06-26 20:44:26 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.1196
2022-06-26 20:45:00 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.0726
2022-06-26 20:45:34 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.0510
2022-06-26 20:46:09 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.3995
2022-06-26 20:46:42 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.2692
2022-06-26 20:47:18 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.2377
2022-06-26 20:47:51 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.0131
2022-06-26 20:48:24 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.1510
2022-06-26 20:48:59 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.1522
2022-06-26 20:49:32 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.1596
2022-06-26 20:50:07 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 0.9780
2022-06-26 20:50:43 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.3823
2022-06-26 20:51:16 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.2000
2022-06-26 20:51:51 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.3288
2022-06-26 20:52:25 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.0200
2022-06-26 20:53:00 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.3053
2022-06-26 20:53:34 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.2191
2022-06-26 20:54:08 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.4495
2022-06-26 20:54:43 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.2305
2022-06-26 20:55:17 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.2020
2022-06-26 20:55:51 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.1504
2022-06-26 20:56:24 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.1855
2022-06-26 20:56:59 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.2448
2022-06-26 20:57:34 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.2301
2022-06-26 20:58:08 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.2125
2022-06-26 20:58:42 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 0.9939
2022-06-26 20:59:17 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.2828
2022-06-26 20:59:51 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.2829
2022-06-26 21:00:25 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.2744
2022-06-26 21:00:59 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.2506
2022-06-26 21:01:33 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.1292
2022-06-26 21:02:09 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.2851
2022-06-26 21:02:43 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.2752
2022-06-26 21:03:17 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.3586
2022-06-26 21:03:51 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.0917
2022-06-26 21:04:25 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.0701
2022-06-26 21:04:58 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.0523
2022-06-26 21:04:59 - train: epoch 043, train_loss: 1.1912
2022-06-26 21:06:14 - eval: epoch: 043, acc1: 71.670%, acc5: 90.732%, test_loss: 1.1283, per_image_load_time: 2.400ms, per_image_inference_time: 0.486ms
2022-06-26 21:06:15 - until epoch: 043, best_acc1: 72.596%
2022-06-26 21:06:15 - epoch 044 lr: 0.010000
2022-06-26 21:06:54 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.1848
2022-06-26 21:07:28 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.1715
2022-06-26 21:08:02 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.2341
2022-06-26 21:08:35 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.1299
2022-06-26 21:09:09 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.0883
2022-06-26 21:09:43 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 0.9659
2022-06-26 21:10:16 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.0250
2022-06-26 21:10:50 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.1954
2022-06-26 21:11:23 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.1430
2022-06-26 21:11:57 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.0325
2022-06-26 21:12:32 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.1452
2022-06-26 21:13:06 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.0368
2022-06-26 21:13:40 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.3356
2022-06-26 21:14:14 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.2233
2022-06-26 21:14:48 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.1365
2022-06-26 21:15:22 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.0971
2022-06-26 21:15:57 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.1681
2022-06-26 21:16:30 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.0739
2022-06-26 21:17:05 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.2426
2022-06-26 21:17:38 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.1249
2022-06-26 21:18:11 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.4597
2022-06-26 21:18:46 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.2332
2022-06-26 21:19:20 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.3211
2022-06-26 21:19:54 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.1441
2022-06-26 21:20:29 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.2286
2022-06-26 21:21:02 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.2867
2022-06-26 21:21:36 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.1735
2022-06-26 21:22:11 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.0996
2022-06-26 21:22:44 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.0104
2022-06-26 21:23:18 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.1420
2022-06-26 21:23:52 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.1763
2022-06-26 21:24:26 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.0479
2022-06-26 21:25:00 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.3124
2022-06-26 21:25:34 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.3997
2022-06-26 21:26:09 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.3407
2022-06-26 21:26:43 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.3061
2022-06-26 21:27:16 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.1100
2022-06-26 21:27:50 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 0.9799
2022-06-26 21:28:24 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.2039
2022-06-26 21:28:57 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.3958
2022-06-26 21:29:31 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.0866
2022-06-26 21:30:05 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.2543
2022-06-26 21:30:38 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.1914
2022-06-26 21:31:13 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.2612
2022-06-26 21:31:46 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.3901
2022-06-26 21:32:21 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.1306
2022-06-26 21:32:55 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.2541
2022-06-26 21:33:28 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.1796
2022-06-26 21:34:02 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.2143
2022-06-26 21:34:35 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.2191
2022-06-26 21:34:36 - train: epoch 044, train_loss: 1.1913
2022-06-26 21:35:51 - eval: epoch: 044, acc1: 71.536%, acc5: 90.624%, test_loss: 1.1419, per_image_load_time: 1.777ms, per_image_inference_time: 0.487ms
2022-06-26 21:35:51 - until epoch: 044, best_acc1: 72.596%
2022-06-26 21:35:51 - epoch 045 lr: 0.010000
2022-06-26 21:36:29 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.0406
2022-06-26 21:37:04 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 0.9549
2022-06-26 21:37:38 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.1479
2022-06-26 21:38:11 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.0782
2022-06-26 21:38:46 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.2896
2022-06-26 21:39:20 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.2885
2022-06-26 21:39:55 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.0123
2022-06-26 21:40:28 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.0594
2022-06-26 21:41:03 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.1890
2022-06-26 21:41:37 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.1304
2022-06-26 21:42:11 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.3577
2022-06-26 21:42:44 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.0262
2022-06-26 21:43:17 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.1946
2022-06-26 21:43:52 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.1239
2022-06-26 21:44:26 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.2108
2022-06-26 21:45:01 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 0.9316
2022-06-26 21:45:34 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.1518
2022-06-26 21:46:08 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.0612
2022-06-26 21:46:43 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.1734
2022-06-26 21:47:18 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.2977
2022-06-26 21:47:51 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.1680
2022-06-26 21:48:25 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.3260
2022-06-26 21:48:59 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.0739
2022-06-26 21:49:33 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.3094
2022-06-26 21:50:07 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.1713
2022-06-26 21:50:41 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.2421
2022-06-26 21:51:14 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 0.9235
2022-06-26 21:51:50 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.1622
2022-06-26 21:52:23 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.2707
2022-06-26 21:52:58 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.1825
2022-06-26 21:53:31 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.2071
2022-06-26 21:54:06 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.3677
2022-06-26 21:54:40 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.0887
2022-06-26 21:55:15 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 0.9786
2022-06-26 21:55:49 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.3208
2022-06-26 21:56:23 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.1938
2022-06-26 21:56:57 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.2180
2022-06-26 21:57:31 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.2642
2022-06-26 21:58:06 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.4072
2022-06-26 21:58:39 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.3274
2022-06-26 21:59:13 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.3715
2022-06-26 21:59:48 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.3486
2022-06-26 22:00:22 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.4285
2022-06-26 22:00:57 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.1513
2022-06-26 22:01:31 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.0335
2022-06-26 22:02:05 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.2539
2022-06-26 22:02:40 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.3214
2022-06-26 22:03:13 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.2322
2022-06-26 22:03:47 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.3400
2022-06-26 22:04:21 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.1012
2022-06-26 22:04:22 - train: epoch 045, train_loss: 1.1892
2022-06-26 22:05:37 - eval: epoch: 045, acc1: 71.472%, acc5: 90.608%, test_loss: 1.1457, per_image_load_time: 2.355ms, per_image_inference_time: 0.506ms
2022-06-26 22:05:37 - until epoch: 045, best_acc1: 72.596%
2022-06-26 22:05:37 - epoch 046 lr: 0.010000
2022-06-26 22:06:16 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.1006
2022-06-26 22:06:50 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 0.9802
2022-06-26 22:07:24 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.1059
2022-06-26 22:07:57 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 0.8918
2022-06-26 22:08:32 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 0.9552
2022-06-26 22:09:06 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.5109
2022-06-26 22:09:40 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.0595
2022-06-26 22:10:14 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.2771
2022-06-26 22:10:48 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.1807
2022-06-26 22:11:21 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.0161
2022-06-26 22:11:56 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.1339
2022-06-26 22:12:30 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.2907
2022-06-26 22:13:04 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.2799
2022-06-26 22:13:38 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.2254
2022-06-26 22:14:13 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.0826
2022-06-26 22:14:47 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.3460
2022-06-26 22:15:22 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.2041
2022-06-26 22:15:56 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.4017
2022-06-26 22:16:30 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.2282
2022-06-26 22:17:04 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.1599
2022-06-26 22:17:38 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.1514
2022-06-26 22:18:13 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.1361
2022-06-26 22:18:47 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.3497
2022-06-26 22:19:22 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 0.9943
2022-06-26 22:19:56 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.0499
2022-06-26 22:20:30 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.2974
2022-06-26 22:21:04 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.2989
2022-06-26 22:21:37 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 0.9235
2022-06-26 22:22:12 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.4185
2022-06-26 22:24:48 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.1143
2022-06-26 22:25:22 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 0.9336
2022-06-26 22:25:56 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.1877
2022-06-26 22:26:29 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.3255
2022-06-26 22:27:03 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.5131
2022-06-26 22:27:38 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.3359
2022-06-26 22:28:11 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.2959
2022-06-26 22:28:44 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 0.8829
2022-06-26 22:29:19 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.1638
2022-06-26 22:29:54 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.2135
2022-06-26 22:30:27 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.0695
2022-06-26 22:31:01 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.2266
2022-06-26 22:31:36 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.1736
2022-06-26 22:32:10 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.3417
2022-06-26 22:32:44 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.4252
2022-06-26 22:33:18 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.1600
2022-06-26 22:33:51 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.4141
2022-06-26 22:34:25 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.1458
2022-06-26 22:35:00 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.0225
2022-06-26 22:35:34 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.4963
2022-06-26 22:36:06 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.2242
2022-06-26 22:36:08 - train: epoch 046, train_loss: 1.1872
2022-06-26 22:37:23 - eval: epoch: 046, acc1: 71.264%, acc5: 90.426%, test_loss: 1.1579, per_image_load_time: 2.422ms, per_image_inference_time: 0.512ms
2022-06-26 22:37:24 - until epoch: 046, best_acc1: 72.596%
2022-06-26 22:37:24 - epoch 047 lr: 0.010000
2022-06-26 22:38:03 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.1273
2022-06-26 22:38:35 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.4034
2022-06-26 22:39:10 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.0248
2022-06-26 22:39:44 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.1615
2022-06-26 22:40:17 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.1480
2022-06-26 22:40:52 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.1296
2022-06-26 22:41:26 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.1015
2022-06-26 22:42:00 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 0.9477
2022-06-26 22:42:34 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.4147
2022-06-26 22:43:07 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.3662
2022-06-26 22:43:41 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.1338
2022-06-26 22:44:14 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.0680
2022-06-26 22:44:48 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.3581
2022-06-26 22:45:22 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.2487
2022-06-26 22:45:56 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.2521
2022-06-26 22:46:30 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.1147
2022-06-26 22:47:04 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 0.9436
2022-06-26 22:47:38 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.2473
2022-06-26 22:48:11 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.1681
2022-06-26 22:48:44 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.1275
2022-06-26 22:49:18 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.2643
2022-06-26 22:49:51 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.3425
2022-06-26 22:50:25 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.1543
2022-06-26 22:50:58 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.1428
2022-06-26 22:51:33 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.2445
2022-06-26 22:52:07 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.4230
2022-06-26 22:52:41 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.1566
2022-06-26 22:53:15 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.3943
2022-06-26 22:53:49 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.3032
2022-06-26 22:54:24 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.1768
2022-06-26 22:54:58 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.2939
2022-06-26 22:55:32 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.3631
2022-06-26 22:56:06 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.2365
2022-06-26 22:56:40 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 0.9724
2022-06-26 22:57:14 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.1879
2022-06-26 22:57:48 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.2246
2022-06-26 22:58:22 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.2166
2022-06-26 22:58:56 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.2077
2022-06-26 22:59:31 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.2191
2022-06-26 23:00:04 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.2717
2022-06-26 23:00:39 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.2567
2022-06-26 23:01:13 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.0853
2022-06-26 23:01:47 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.0234
2022-06-26 23:02:21 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.3425
2022-06-26 23:02:56 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.1470
2022-06-26 23:03:29 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.2220
2022-06-26 23:04:03 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.4857
2022-06-26 23:04:38 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.0513
2022-06-26 23:05:12 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.3007
2022-06-26 23:05:44 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.2081
2022-06-26 23:05:45 - train: epoch 047, train_loss: 1.1857
2022-06-26 23:07:01 - eval: epoch: 047, acc1: 71.636%, acc5: 90.740%, test_loss: 1.1327, per_image_load_time: 2.374ms, per_image_inference_time: 0.496ms
2022-06-26 23:07:01 - until epoch: 047, best_acc1: 72.596%
2022-06-26 23:07:01 - epoch 048 lr: 0.010000
2022-06-26 23:07:40 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.2583
2022-06-26 23:08:13 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.3899
2022-06-26 23:08:46 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.2648
2022-06-26 23:09:20 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.2846
2022-06-26 23:09:54 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.1756
2022-06-26 23:10:28 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.2811
2022-06-26 23:11:01 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.0042
2022-06-26 23:11:35 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.2036
2022-06-26 23:12:10 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.3371
2022-06-26 23:12:43 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.3000
2022-06-26 23:13:18 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.1911
2022-06-26 23:13:51 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.3731
2022-06-26 23:14:26 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.0919
2022-06-26 23:14:59 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.0653
2022-06-26 23:15:33 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.1949
2022-06-26 23:16:07 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.1233
2022-06-26 23:16:41 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.2753
2022-06-26 23:17:14 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.0855
2022-06-26 23:17:49 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.2886
2022-06-26 23:18:22 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.2048
2022-06-26 23:18:56 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.1868
2022-06-26 23:19:30 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.3991
2022-06-26 23:20:05 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.1918
2022-06-26 23:20:39 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.2368
2022-06-26 23:21:12 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.2703
2022-06-26 23:21:46 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.1450
2022-06-26 23:22:21 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.2758
2022-06-26 23:22:55 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.2112
2022-06-26 23:23:28 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.2947
2022-06-26 23:24:02 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.1989
2022-06-26 23:24:36 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.2568
2022-06-26 23:25:11 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 0.8323
2022-06-26 23:25:44 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.3842
2022-06-26 23:26:18 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.1860
2022-06-26 23:26:52 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.2482
2022-06-26 23:27:26 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.2520
2022-06-26 23:28:00 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.4051
2022-06-26 23:28:34 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.1818
2022-06-26 23:29:08 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.4082
2022-06-26 23:29:43 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.2385
2022-06-26 23:30:17 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.4141
2022-06-26 23:30:51 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.1882
2022-06-26 23:31:24 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.1931
2022-06-26 23:31:59 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.4765
2022-06-26 23:32:33 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.3288
2022-06-26 23:33:07 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.2767
2022-06-26 23:33:41 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.3089
2022-06-26 23:34:16 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.4025
2022-06-26 23:34:50 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.2406
2022-06-26 23:35:22 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.3405
2022-06-26 23:35:23 - train: epoch 048, train_loss: 1.1829
2022-06-26 23:36:38 - eval: epoch: 048, acc1: 71.470%, acc5: 90.626%, test_loss: 1.1383, per_image_load_time: 2.300ms, per_image_inference_time: 0.517ms
2022-06-26 23:36:39 - until epoch: 048, best_acc1: 72.596%
2022-06-26 23:36:39 - epoch 049 lr: 0.010000
2022-06-26 23:37:18 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.2314
2022-06-26 23:37:51 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.0113
2022-06-26 23:38:25 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.1728
2022-06-26 23:38:59 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.0961
2022-06-26 23:39:33 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.0538
2022-06-26 23:40:06 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.1306
2022-06-26 23:40:41 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.2516
2022-06-26 23:41:15 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.2414
2022-06-26 23:41:48 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.1321
2022-06-26 23:42:23 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.3183
2022-06-26 23:42:56 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.0447
2022-06-26 23:43:31 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.0372
2022-06-26 23:44:04 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.3132
2022-06-26 23:44:38 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.3612
2022-06-26 23:45:13 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 0.8438
2022-06-26 23:45:46 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.0283
2022-06-26 23:46:20 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.1969
2022-06-26 23:46:55 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.1945
2022-06-26 23:47:28 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.0631
2022-06-26 23:48:03 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.3042
2022-06-26 23:48:37 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.1964
2022-06-26 23:49:11 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.1713
2022-06-26 23:49:44 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.0681
2022-06-26 23:50:19 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.2323
2022-06-26 23:50:53 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.1690
2022-06-26 23:51:28 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.1694
2022-06-26 23:52:01 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 0.9635
2022-06-26 23:52:35 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.2470
2022-06-26 23:53:10 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.3028
2022-06-26 23:53:45 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.1750
2022-06-26 23:54:19 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.1931
2022-06-26 23:54:52 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.2802
2022-06-26 23:55:27 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.1281
2022-06-26 23:56:00 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.1220
2022-06-26 23:56:34 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.3082
2022-06-26 23:57:08 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.2950
2022-06-26 23:57:43 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 0.9630
2022-06-26 23:58:17 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.2222
2022-06-26 23:58:52 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.1855
2022-06-26 23:59:25 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.2088
2022-06-26 23:59:59 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.0871
2022-06-27 00:00:33 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.2453
2022-06-27 00:01:07 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.4479
2022-06-27 00:01:42 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.0453
2022-06-27 00:02:15 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.1998
2022-06-27 00:02:49 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.2952
2022-06-27 00:03:23 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.5682
2022-06-27 00:03:57 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.1684
2022-06-27 00:04:32 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.0192
2022-06-27 00:05:04 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.0347
2022-06-27 00:05:05 - train: epoch 049, train_loss: 1.1778
2022-06-27 00:06:20 - eval: epoch: 049, acc1: 71.016%, acc5: 90.232%, test_loss: 1.1717, per_image_load_time: 2.353ms, per_image_inference_time: 0.509ms
2022-06-27 00:06:21 - until epoch: 049, best_acc1: 72.596%
2022-06-27 00:06:21 - epoch 050 lr: 0.010000
2022-06-27 00:07:00 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.2442
2022-06-27 00:07:33 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.1591
2022-06-27 00:08:07 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.1720
2022-06-27 00:08:40 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.0105
2022-06-27 00:09:14 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.1393
2022-06-27 00:09:49 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.2616
2022-06-27 00:10:23 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.0549
2022-06-27 00:10:57 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.0524
2022-06-27 00:11:31 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.1287
2022-06-27 00:12:05 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.1752
2022-06-27 00:12:38 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.1824
2022-06-27 00:13:12 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.1788
2022-06-27 00:13:46 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 0.9864
2022-06-27 00:14:21 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 0.9490
2022-06-27 00:14:54 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.0703
2022-06-27 00:15:29 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.2025
2022-06-27 00:16:03 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.1348
2022-06-27 00:16:36 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.2851
2022-06-27 00:17:10 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.1888
2022-06-27 00:17:44 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.2055
2022-06-27 00:18:18 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.2525
2022-06-27 00:18:52 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.1631
2022-06-27 00:19:26 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.1562
2022-06-27 00:20:00 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.3119
2022-06-27 00:20:33 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.1019
2022-06-27 00:21:07 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.0509
2022-06-27 00:21:41 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.1896
2022-06-27 00:22:15 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.2557
2022-06-27 00:22:49 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.1920
2022-06-27 00:23:24 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.1070
2022-06-27 00:23:57 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.1423
2022-06-27 00:24:32 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.1209
2022-06-27 00:25:06 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 0.9334
2022-06-27 00:25:41 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.0486
2022-06-27 00:26:14 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.2706
2022-06-27 00:26:48 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.1438
2022-06-27 00:27:22 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.3738
2022-06-27 00:27:56 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.2057
2022-06-27 00:28:30 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 0.9545
2022-06-27 00:29:05 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.3312
2022-06-27 00:29:39 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.2084
2022-06-27 00:30:13 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.1689
2022-06-27 00:30:47 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.1202
2022-06-27 00:31:22 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.2194
2022-06-27 00:31:56 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.1800
2022-06-27 00:32:30 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.1570
2022-06-27 00:33:04 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.1992
2022-06-27 00:33:38 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.2825
2022-06-27 00:34:13 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.0921
2022-06-27 00:34:45 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.0176
2022-06-27 00:34:46 - train: epoch 050, train_loss: 1.1765
2022-06-27 00:36:02 - eval: epoch: 050, acc1: 71.592%, acc5: 90.846%, test_loss: 1.1380, per_image_load_time: 2.470ms, per_image_inference_time: 0.471ms
2022-06-27 00:36:03 - until epoch: 050, best_acc1: 72.596%
2022-06-27 00:36:03 - epoch 051 lr: 0.010000
2022-06-27 00:36:43 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.2803
2022-06-27 00:37:17 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.2753
2022-06-27 00:37:50 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.1505
2022-06-27 00:38:25 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.0101
2022-06-27 00:38:58 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.2088
2022-06-27 00:39:32 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 0.9875
2022-06-27 00:40:07 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.2260
2022-06-27 00:40:40 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.3613
2022-06-27 00:41:15 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.1430
2022-06-27 00:41:49 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.3400
2022-06-27 00:42:24 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.2999
2022-06-27 00:42:58 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.0085
2022-06-27 00:43:32 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.0075
2022-06-27 00:44:06 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 0.9258
2022-06-27 00:44:40 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 0.9723
2022-06-27 00:45:14 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 0.9955
2022-06-27 00:45:48 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.2593
2022-06-27 00:46:22 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.2437
2022-06-27 00:46:55 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.0476
2022-06-27 00:47:29 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.0524
2022-06-27 00:48:02 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.0874
2022-06-27 00:48:37 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.2077
2022-06-27 00:49:11 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.0595
2022-06-27 00:49:45 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.2287
2022-06-27 00:50:19 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.2159
2022-06-27 00:50:53 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.1349
2022-06-27 00:51:27 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.0985
2022-06-27 00:52:01 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 0.9861
2022-06-27 00:52:35 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.1738
2022-06-27 00:53:09 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.1091
2022-06-27 00:53:43 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.1977
2022-06-27 00:54:18 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.0679
2022-06-27 00:54:51 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.3903
2022-06-27 00:55:25 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.2510
2022-06-27 00:55:59 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.1422
2022-06-27 00:56:32 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.2107
2022-06-27 00:57:07 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.4550
2022-06-27 00:57:41 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.1386
2022-06-27 00:58:15 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.4042
2022-06-27 00:58:49 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.1880
2022-06-27 00:59:24 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.3036
2022-06-27 00:59:58 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.4152
2022-06-27 01:00:31 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.2003
2022-06-27 01:01:04 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.2744
2022-06-27 01:01:39 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.1563
2022-06-27 01:02:13 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.3306
2022-06-27 01:02:47 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.1355
2022-06-27 01:03:21 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.3158
2022-06-27 01:03:56 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.3511
2022-06-27 01:04:28 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.1815
2022-06-27 01:04:29 - train: epoch 051, train_loss: 1.1741
2022-06-27 01:05:44 - eval: epoch: 051, acc1: 71.220%, acc5: 90.348%, test_loss: 1.1630, per_image_load_time: 2.420ms, per_image_inference_time: 0.472ms
2022-06-27 01:05:44 - until epoch: 051, best_acc1: 72.596%
2022-06-27 01:05:44 - epoch 052 lr: 0.010000
2022-06-27 01:06:23 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.2195
2022-06-27 01:06:56 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.3758
2022-06-27 01:07:30 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.1483
2022-06-27 01:08:04 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.1974
2022-06-27 01:08:39 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.2650
2022-06-27 01:09:13 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.2714
2022-06-27 01:09:46 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.2343
2022-06-27 01:10:20 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.1519
2022-06-27 01:10:54 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.2472
2022-06-27 01:11:28 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.2277
2022-06-27 01:12:02 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.1708
2022-06-27 01:12:36 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 0.9027
2022-06-27 01:13:10 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.0127
2022-06-27 01:13:44 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.1672
2022-06-27 01:14:19 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 0.9956
2022-06-27 01:14:52 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.0792
2022-06-27 01:15:27 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.0869
2022-06-27 01:16:00 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.0656
2022-06-27 01:16:33 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.1059
2022-06-27 01:17:08 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.2202
2022-06-27 01:17:42 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.1664
2022-06-27 01:18:17 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.2989
2022-06-27 01:18:51 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.0534
2022-06-27 01:19:26 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.0601
2022-06-27 01:20:00 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.0937
2022-06-27 01:20:34 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.1041
2022-06-27 01:21:06 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.1178
2022-06-27 01:21:40 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.1579
2022-06-27 01:22:14 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.1136
2022-06-27 01:22:47 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.2121
2022-06-27 01:23:21 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.2119
2022-06-27 01:23:55 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.3912
2022-06-27 01:24:28 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.1616
2022-06-27 01:25:02 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.2956
2022-06-27 01:25:36 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.1297
2022-06-27 01:26:09 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.3455
2022-06-27 01:26:44 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.3276
2022-06-27 01:27:17 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.1124
2022-06-27 01:27:50 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.0918
2022-06-27 01:28:24 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.3779
2022-06-27 01:28:58 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.1680
2022-06-27 01:29:32 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.3474
2022-06-27 01:30:05 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.1517
2022-06-27 01:30:39 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.1976
2022-06-27 01:31:13 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.1723
2022-06-27 01:31:47 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.1506
2022-06-27 01:32:21 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.2181
2022-06-27 01:32:55 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.0738
2022-06-27 01:33:29 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.1033
2022-06-27 01:34:02 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.2506
2022-06-27 01:34:03 - train: epoch 052, train_loss: 1.1720
2022-06-27 01:35:19 - eval: epoch: 052, acc1: 71.360%, acc5: 90.468%, test_loss: 1.1608, per_image_load_time: 2.451ms, per_image_inference_time: 0.472ms
2022-06-27 01:35:19 - until epoch: 052, best_acc1: 72.596%
2022-06-27 01:35:19 - epoch 053 lr: 0.010000
2022-06-27 01:35:58 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.2074
2022-06-27 01:36:33 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 0.9983
2022-06-27 01:37:06 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.1317
2022-06-27 01:37:41 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.3259
2022-06-27 01:38:19 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.2977
2022-06-27 01:38:54 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.0254
2022-06-27 01:39:28 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.0704
2022-06-27 01:40:03 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.2217
2022-06-27 01:40:37 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.0477
2022-06-27 01:41:11 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.0861
2022-06-27 01:41:44 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.0079
2022-06-27 01:42:18 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.1314
2022-06-27 01:42:52 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.2076
2022-06-27 01:43:27 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.2489
2022-06-27 01:44:00 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.0649
2022-06-27 01:44:34 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.2792
2022-06-27 01:45:07 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.3919
2022-06-27 01:45:41 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.2345
2022-06-27 01:46:15 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.0594
2022-06-27 01:46:49 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.3060
2022-06-27 01:47:22 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.2891
2022-06-27 01:47:56 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.2145
2022-06-27 01:48:29 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.0397
2022-06-27 01:49:03 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.1828
2022-06-27 01:49:37 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.2927
2022-06-27 01:50:10 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.1711
2022-06-27 01:50:44 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.1451
2022-06-27 01:51:18 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.2512
2022-06-27 01:51:53 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.0043
2022-06-27 01:52:27 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.0607
2022-06-27 01:53:01 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.3239
2022-06-27 01:53:35 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.2440
2022-06-27 01:54:09 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.1307
2022-06-27 01:54:43 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.1625
2022-06-27 01:55:17 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.2466
2022-06-27 01:55:50 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.3678
2022-06-27 01:56:25 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.3467
2022-06-27 01:56:59 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.1278
2022-06-27 01:57:32 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.2256
2022-06-27 01:58:06 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.1605
2022-06-27 01:58:40 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.1322
2022-06-27 01:59:14 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.2129
2022-06-27 01:59:48 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.3377
2022-06-27 02:00:22 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.2279
2022-06-27 02:00:55 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.3511
2022-06-27 02:01:30 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 0.9996
2022-06-27 02:02:02 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.2700
2022-06-27 02:02:37 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.3990
2022-06-27 02:03:11 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.2570
2022-06-27 02:03:43 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.0488
2022-06-27 02:03:44 - train: epoch 053, train_loss: 1.1685
2022-06-27 02:05:01 - eval: epoch: 053, acc1: 71.676%, acc5: 90.780%, test_loss: 1.1335, per_image_load_time: 2.446ms, per_image_inference_time: 0.477ms
2022-06-27 02:05:01 - until epoch: 053, best_acc1: 72.596%
2022-06-27 02:05:01 - epoch 054 lr: 0.010000
2022-06-27 02:05:40 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.0929
2022-06-27 02:06:13 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.0567
2022-06-27 02:06:47 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.0698
2022-06-27 02:07:21 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 0.9513
2022-06-27 02:07:55 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.2303
2022-06-27 02:08:30 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.1920
2022-06-27 02:09:04 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.3766
2022-06-27 02:09:39 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.2860
2022-06-27 02:10:13 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 0.9137
2022-06-27 02:10:46 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 0.9225
2022-06-27 02:11:20 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.0932
2022-06-27 02:11:54 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.2497
2022-06-27 02:12:28 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.0610
2022-06-27 02:13:01 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.2529
2022-06-27 02:13:35 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.1865
2022-06-27 02:14:09 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.0650
2022-06-27 02:14:42 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 0.9391
2022-06-27 02:15:16 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 0.9991
2022-06-27 02:15:50 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.1535
2022-06-27 02:16:23 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.2065
2022-06-27 02:16:57 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.1874
2022-06-27 02:17:31 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.0861
2022-06-27 02:18:04 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 0.9825
2022-06-27 02:18:38 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.2748
2022-06-27 02:19:12 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.2002
2022-06-27 02:19:44 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.2110
2022-06-27 02:20:20 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.1030
2022-06-27 02:20:53 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.3634
2022-06-27 02:21:27 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.1241
2022-06-27 02:22:01 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.3845
2022-06-27 02:22:34 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.1285
2022-06-27 02:23:08 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.3628
2022-06-27 02:23:42 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.0530
2022-06-27 02:24:16 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.2316
2022-06-27 02:24:50 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.1511
2022-06-27 02:25:23 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.0954
2022-06-27 02:25:57 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.0479
2022-06-27 02:26:31 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.1043
2022-06-27 02:27:04 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.1089
2022-06-27 02:27:38 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.0465
2022-06-27 02:28:12 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.2852
2022-06-27 02:28:46 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.1124
2022-06-27 02:29:19 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.2914
2022-06-27 02:29:53 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.1747
2022-06-27 02:30:27 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.0864
2022-06-27 02:31:01 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.3570
2022-06-27 02:31:34 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.2448
2022-06-27 02:32:08 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.2111
2022-06-27 02:32:41 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.1610
2022-06-27 02:33:14 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.0973
2022-06-27 02:33:15 - train: epoch 054, train_loss: 1.1648
2022-06-27 02:34:30 - eval: epoch: 054, acc1: 71.280%, acc5: 90.686%, test_loss: 1.1526, per_image_load_time: 2.292ms, per_image_inference_time: 0.501ms
2022-06-27 02:34:30 - until epoch: 054, best_acc1: 72.596%
2022-06-27 02:34:30 - epoch 055 lr: 0.010000
2022-06-27 02:35:09 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.0765
2022-06-27 02:35:42 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.0104
2022-06-27 02:36:16 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.0878
2022-06-27 02:36:50 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 0.8866
2022-06-27 02:37:23 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 0.9943
2022-06-27 02:37:56 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.0468
2022-06-27 02:38:31 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.1862
2022-06-27 02:39:04 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.0820
2022-06-27 02:39:40 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.2146
2022-06-27 02:40:13 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.2230
2022-06-27 02:40:47 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.2271
2022-06-27 02:41:21 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.2651
2022-06-27 02:41:54 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.2243
2022-06-27 02:42:29 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.0904
2022-06-27 02:43:02 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.1582
2022-06-27 02:43:37 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.1660
2022-06-27 02:44:11 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.0007
2022-06-27 02:44:45 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.0653
2022-06-27 02:45:19 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.1423
2022-06-27 02:45:53 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.0530
2022-06-27 02:46:27 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 0.8990
2022-06-27 02:47:02 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.2635
2022-06-27 02:47:35 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.0770
2022-06-27 02:48:09 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.0039
2022-06-27 02:48:43 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.1949
2022-06-27 02:49:17 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.1845
2022-06-27 02:49:52 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.0859
2022-06-27 02:50:26 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.2757
2022-06-27 02:51:00 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 0.9759
2022-06-27 02:51:34 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.2665
2022-06-27 02:52:08 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.3023
2022-06-27 02:52:43 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.0308
2022-06-27 02:53:16 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 0.9109
2022-06-27 02:53:51 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.0303
2022-06-27 02:54:24 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.0156
2022-06-27 02:54:58 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.2598
2022-06-27 02:55:32 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.0675
2022-06-27 02:56:06 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.3194
2022-06-27 02:56:40 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.5293
2022-06-27 02:57:14 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.1121
2022-06-27 02:57:49 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.1281
2022-06-27 02:58:23 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.3108
2022-06-27 02:58:56 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.3904
2022-06-27 02:59:31 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.3285
2022-06-27 03:00:06 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.1168
2022-06-27 03:00:40 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.1632
2022-06-27 03:01:14 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.2208
2022-06-27 03:01:49 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.0890
2022-06-27 03:02:22 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.2241
2022-06-27 03:02:55 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.3551
2022-06-27 03:02:57 - train: epoch 055, train_loss: 1.1646
2022-06-27 03:04:12 - eval: epoch: 055, acc1: 71.886%, acc5: 90.800%, test_loss: 1.1321, per_image_load_time: 1.776ms, per_image_inference_time: 0.508ms
2022-06-27 03:04:12 - until epoch: 055, best_acc1: 72.596%
2022-06-27 03:04:12 - epoch 056 lr: 0.010000
2022-06-27 03:04:52 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.1826
2022-06-27 03:05:26 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.1213
2022-06-27 03:06:00 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.1186
2022-06-27 03:06:34 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.1052
2022-06-27 03:07:09 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.0904
2022-06-27 03:07:43 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.1347
2022-06-27 03:08:17 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.2877
2022-06-27 03:08:51 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.0333
2022-06-27 03:09:25 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.3495
2022-06-27 03:09:59 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.2330
2022-06-27 03:10:33 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.0392
2022-06-27 03:11:07 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.1560
2022-06-27 03:11:41 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.3770
2022-06-27 03:12:14 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 0.9874
2022-06-27 03:12:49 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.2693
2022-06-27 03:13:22 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.0258
2022-06-27 03:13:56 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.1733
2022-06-27 03:14:30 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.2349
2022-06-27 03:15:05 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.2623
2022-06-27 03:15:38 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.1685
2022-06-27 03:16:11 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.1728
2022-06-27 03:16:46 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.2357
2022-06-27 03:17:19 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.1951
2022-06-27 03:17:53 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.1359
2022-06-27 03:18:26 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.4330
2022-06-27 03:19:00 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.3407
2022-06-27 03:19:34 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.0648
2022-06-27 03:20:07 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.0413
2022-06-27 03:20:41 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.1213
2022-06-27 03:21:15 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.3662
2022-06-27 03:21:48 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.0699
2022-06-27 03:22:22 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.1581
2022-06-27 03:22:56 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.1701
2022-06-27 03:23:29 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.0604
2022-06-27 03:24:03 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.0803
2022-06-27 03:24:37 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 0.9465
2022-06-27 03:25:09 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.2817
2022-06-27 03:25:43 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.2396
2022-06-27 03:26:17 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.5532
2022-06-27 03:26:51 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.1767
2022-06-27 03:27:25 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.3115
2022-06-27 03:27:58 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.0965
2022-06-27 03:28:32 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.2421
2022-06-27 03:29:06 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.2175
2022-06-27 03:29:39 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.0092
2022-06-27 03:30:12 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.2074
2022-06-27 03:30:46 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.1306
2022-06-27 03:31:19 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.0767
2022-06-27 03:31:53 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.2211
2022-06-27 03:32:25 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.2240
2022-06-27 03:32:26 - train: epoch 056, train_loss: 1.1595
2022-06-27 03:33:42 - eval: epoch: 056, acc1: 71.670%, acc5: 90.580%, test_loss: 1.1444, per_image_load_time: 2.379ms, per_image_inference_time: 0.496ms
2022-06-27 03:33:42 - until epoch: 056, best_acc1: 72.596%
2022-06-27 03:33:42 - epoch 057 lr: 0.010000
2022-06-27 03:34:22 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.1738
2022-06-27 03:34:55 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.1361
2022-06-27 03:35:30 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.1162
2022-06-27 03:36:03 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.2787
2022-06-27 03:36:36 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.2368
2022-06-27 03:37:10 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.1981
2022-06-27 03:37:44 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 0.9871
2022-06-27 03:38:17 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.2249
2022-06-27 03:38:51 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.0255
2022-06-27 03:39:25 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.0094
2022-06-27 03:39:59 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 0.9573
2022-06-27 03:40:32 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.1306
2022-06-27 03:41:06 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.0966
2022-06-27 03:41:39 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.1515
2022-06-27 03:42:13 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.0529
2022-06-27 03:42:46 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.1535
2022-06-27 03:43:20 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.1753
2022-06-27 03:43:54 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.2194
2022-06-27 03:44:28 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 0.9990
2022-06-27 03:45:01 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.3198
2022-06-27 03:45:34 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 0.9931
2022-06-27 03:46:08 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.0521
2022-06-27 03:46:41 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.0732
2022-06-27 03:47:15 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.0937
2022-06-27 03:47:48 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.3242
2022-06-27 03:48:21 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.0963
2022-06-27 03:48:56 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.0849
2022-06-27 03:49:29 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 0.9882
2022-06-27 03:50:02 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.1867
2022-06-27 03:50:36 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.2941
2022-06-27 03:51:09 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.1620
2022-06-27 03:51:43 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.3309
2022-06-27 03:52:16 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.2703
2022-06-27 03:52:50 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.1609
2022-06-27 03:53:24 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.2482
2022-06-27 03:53:57 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.0836
2022-06-27 03:54:31 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.0613
2022-06-27 03:55:04 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.0554
2022-06-27 03:55:38 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.1551
2022-06-27 03:56:11 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.0742
2022-06-27 03:56:45 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.1745
2022-06-27 03:57:19 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.1385
2022-06-27 03:57:52 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.1339
2022-06-27 03:58:26 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.1583
2022-06-27 03:59:00 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.2110
2022-06-27 03:59:33 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.1468
2022-06-27 04:00:07 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.1788
2022-06-27 04:00:40 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.4643
2022-06-27 04:01:13 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.4103
2022-06-27 04:01:45 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.0905
2022-06-27 04:01:46 - train: epoch 057, train_loss: 1.1575
2022-06-27 04:03:02 - eval: epoch: 057, acc1: 72.242%, acc5: 90.978%, test_loss: 1.1240, per_image_load_time: 2.436ms, per_image_inference_time: 0.469ms
2022-06-27 04:03:02 - until epoch: 057, best_acc1: 72.596%
2022-06-27 04:03:02 - epoch 058 lr: 0.010000
2022-06-27 04:03:41 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.1245
2022-06-27 04:04:15 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.0097
2022-06-27 04:04:48 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.0818
2022-06-27 04:05:22 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.1905
2022-06-27 04:05:55 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 0.9828
2022-06-27 04:06:29 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.3063
2022-06-27 04:07:03 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.1493
2022-06-27 04:07:36 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.1943
2022-06-27 04:08:10 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.0528
2022-06-27 04:08:44 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.2326
2022-06-27 04:09:18 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.0526
2022-06-27 04:09:51 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.0379
2022-06-27 04:10:25 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.2669
2022-06-27 04:10:58 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.0985
2022-06-27 04:11:32 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.0235
2022-06-27 04:12:06 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.2288
2022-06-27 04:12:40 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.1125
2022-06-27 04:13:13 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.2602
2022-06-27 04:13:47 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.1089
2022-06-27 04:14:21 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.2062
2022-06-27 04:14:54 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.2015
2022-06-27 04:15:28 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.1005
2022-06-27 04:16:02 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.3585
2022-06-27 04:16:35 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.0709
2022-06-27 04:17:09 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.2649
2022-06-27 04:17:42 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.0260
2022-06-27 04:18:16 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.5773
2022-06-27 04:18:49 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.1374
2022-06-27 04:19:24 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.0116
2022-06-27 04:19:56 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.1087
2022-06-27 04:20:29 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.2358
2022-06-27 04:21:03 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.0871
2022-06-27 04:21:36 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.0516
2022-06-27 04:22:10 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.1405
2022-06-27 04:22:44 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.1250
2022-06-27 04:23:17 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.1668
2022-06-27 04:23:50 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.1276
2022-06-27 04:24:24 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.0094
2022-06-27 04:24:59 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.1920
2022-06-27 04:25:32 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.1650
2022-06-27 04:26:06 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.2249
2022-06-27 04:26:39 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.1134
2022-06-27 04:27:14 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.0656
2022-06-27 04:27:47 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.2212
2022-06-27 04:28:20 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.0867
2022-06-27 04:28:55 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.1406
2022-06-27 04:29:28 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.2677
2022-06-27 04:30:01 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.1039
2022-06-27 04:30:35 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.1431
2022-06-27 04:31:07 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 0.9348
2022-06-27 04:31:09 - train: epoch 058, train_loss: 1.1501
2022-06-27 04:32:24 - eval: epoch: 058, acc1: 71.386%, acc5: 90.638%, test_loss: 1.1509, per_image_load_time: 2.399ms, per_image_inference_time: 0.481ms
2022-06-27 04:32:24 - until epoch: 058, best_acc1: 72.596%
2022-06-27 04:32:24 - epoch 059 lr: 0.010000
2022-06-27 04:33:03 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.1189
2022-06-27 04:33:36 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 0.8432
2022-06-27 04:34:09 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.0679
2022-06-27 04:34:43 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.2576
2022-06-27 04:35:16 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.0575
2022-06-27 04:35:50 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 0.9640
2022-06-27 04:36:23 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.1599
2022-06-27 04:36:57 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 0.9759
2022-06-27 04:37:30 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.3716
2022-06-27 04:38:03 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.1826
2022-06-27 04:38:37 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 1.4564
2022-06-27 04:39:10 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 0.9514
2022-06-27 04:39:44 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.2902
2022-06-27 04:40:18 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.2822
2022-06-27 04:40:52 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.1054
2022-06-27 04:41:25 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.0606
2022-06-27 04:41:58 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.0505
2022-06-27 04:42:32 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.0503
2022-06-27 04:43:05 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.0667
2022-06-27 04:43:40 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 0.9988
2022-06-27 04:44:13 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.3262
2022-06-27 04:44:46 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.1298
2022-06-27 04:45:20 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.1687
2022-06-27 04:45:52 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.2055
2022-06-27 04:46:26 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.3789
2022-06-27 04:47:00 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.1048
2022-06-27 04:47:33 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.0442
2022-06-27 04:48:07 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.2510
2022-06-27 04:48:40 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.0069
2022-06-27 04:49:14 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 1.3344
2022-06-27 04:49:47 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.0814
2022-06-27 04:50:22 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.3028
2022-06-27 04:50:55 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.2933
2022-06-27 04:51:28 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 1.4681
2022-06-27 04:52:02 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.1372
2022-06-27 04:52:36 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.1811
2022-06-27 04:53:10 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.1101
2022-06-27 04:53:43 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.2006
2022-06-27 04:54:17 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 0.9701
2022-06-27 04:54:51 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 1.3427
2022-06-27 04:55:24 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.0902
2022-06-27 04:55:57 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.2345
2022-06-27 04:56:31 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.2118
2022-06-27 04:57:05 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.2417
2022-06-27 04:57:39 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.2538
2022-06-27 04:58:12 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.4964
2022-06-27 04:58:46 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.1618
2022-06-27 04:59:20 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.0385
2022-06-27 04:59:53 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.1527
2022-06-27 05:00:25 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.2861
2022-06-27 05:00:26 - train: epoch 059, train_loss: 1.1486
2022-06-27 05:02:01 - eval: epoch: 059, acc1: 70.980%, acc5: 90.326%, test_loss: 1.1641, per_image_load_time: 2.360ms, per_image_inference_time: 0.472ms
2022-06-27 05:02:01 - until epoch: 059, best_acc1: 72.596%
2022-06-27 05:02:01 - epoch 060 lr: 0.010000
2022-06-27 05:02:42 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.0689
2022-06-27 05:03:14 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.0683
2022-06-27 05:03:48 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.0420
2022-06-27 05:04:21 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.2981
2022-06-27 05:04:55 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.2203
2022-06-27 05:05:29 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.0228
2022-06-27 05:06:03 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.1480
2022-06-27 05:06:36 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.2923
2022-06-27 05:07:10 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.0267
2022-06-27 05:07:43 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 0.8408
2022-06-27 05:08:17 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.1216
2022-06-27 05:08:51 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 0.9092
2022-06-27 05:09:24 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.1703
2022-06-27 05:09:58 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.2680
2022-06-27 05:10:31 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.3139
2022-06-27 05:11:05 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.0229
2022-06-27 05:11:38 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.0566
2022-06-27 05:12:13 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.1993
2022-06-27 05:12:46 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.3144
2022-06-27 05:13:19 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.2677
2022-06-27 05:13:53 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.3748
2022-06-27 05:14:27 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.1458
2022-06-27 05:15:00 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.0501
2022-06-27 05:15:34 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.0152
2022-06-27 05:16:07 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.1095
2022-06-27 05:16:40 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.2178
2022-06-27 05:17:14 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.0324
2022-06-27 05:17:47 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.1634
2022-06-27 05:18:21 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.1043
2022-06-27 05:18:55 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.2453
2022-06-27 05:19:28 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.1745
2022-06-27 05:20:02 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.2057
2022-06-27 05:20:35 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 0.9250
2022-06-27 05:21:09 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.1949
2022-06-27 05:21:42 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.2028
2022-06-27 05:22:15 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.2068
2022-06-27 05:22:48 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.4868
2022-06-27 05:23:22 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.1915
2022-06-27 05:23:56 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.1191
2022-06-27 05:24:30 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.1319
2022-06-27 05:25:04 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.0780
2022-06-27 05:25:36 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.1575
2022-06-27 05:26:10 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.0914
2022-06-27 05:26:45 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.2423
2022-06-27 05:27:17 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.2265
2022-06-27 05:27:50 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.0689
2022-06-27 05:28:23 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.0599
2022-06-27 05:28:57 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.0616
2022-06-27 05:29:31 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.0782
2022-06-27 05:30:04 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.2169
2022-06-27 05:30:05 - train: epoch 060, train_loss: 1.1451
2022-06-27 05:31:20 - eval: epoch: 060, acc1: 71.752%, acc5: 90.884%, test_loss: 1.1356, per_image_load_time: 2.391ms, per_image_inference_time: 0.466ms
2022-06-27 05:31:20 - until epoch: 060, best_acc1: 72.596%
2022-06-27 05:31:20 - epoch 061 lr: 0.001000
2022-06-27 05:31:59 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 0.8291
2022-06-27 05:32:33 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.1311
2022-06-27 05:33:06 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 0.7235
2022-06-27 05:33:40 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.1354
2022-06-27 05:34:13 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.0708
2022-06-27 05:34:47 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.0095
2022-06-27 05:35:21 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 0.7151
2022-06-27 05:35:54 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 0.9868
2022-06-27 05:36:27 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.1997
2022-06-27 05:37:01 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 0.9622
2022-06-27 05:37:35 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 0.7819
2022-06-27 05:38:09 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 0.9413
2022-06-27 05:38:43 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.0344
2022-06-27 05:39:17 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 0.8951
2022-06-27 05:39:50 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.0005
2022-06-27 05:40:23 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 0.8765
2022-06-27 05:40:56 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 0.9184
2022-06-27 05:41:30 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.0275
2022-06-27 05:42:03 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 0.9516
2022-06-27 05:42:37 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.0295
2022-06-27 05:43:10 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.2162
2022-06-27 05:43:44 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 0.8901
2022-06-27 05:44:17 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.2440
2022-06-27 05:44:51 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 0.7887
2022-06-27 05:45:25 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 0.8792
2022-06-27 05:45:58 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 0.8870
2022-06-27 05:46:32 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.1039
2022-06-27 05:47:05 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 0.8362
2022-06-27 05:47:40 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.1090
2022-06-27 05:48:13 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 0.8806
2022-06-27 05:48:46 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 0.9378
2022-06-27 05:49:20 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.0018
2022-06-27 05:49:53 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 0.9189
2022-06-27 05:50:26 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.0582
2022-06-27 05:51:00 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.0406
2022-06-27 05:51:34 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 0.9747
2022-06-27 05:52:07 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 0.9194
2022-06-27 05:52:41 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 0.9374
2022-06-27 05:53:15 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 0.9165
2022-06-27 05:53:49 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 0.8483
2022-06-27 05:54:21 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.2248
2022-06-27 05:54:56 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 0.9076
2022-06-27 05:55:29 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 0.7927
2022-06-27 05:56:02 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 0.8030
2022-06-27 05:56:36 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 0.8982
2022-06-27 05:57:10 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 0.8197
2022-06-27 05:57:43 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 0.9934
2022-06-27 05:58:17 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.0006
2022-06-27 05:58:52 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.0039
2022-06-27 05:59:23 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.0915
2022-06-27 05:59:25 - train: epoch 061, train_loss: 0.9708
2022-06-27 06:00:39 - eval: epoch: 061, acc1: 75.158%, acc5: 92.388%, test_loss: 0.9928, per_image_load_time: 2.352ms, per_image_inference_time: 0.524ms
2022-06-27 06:00:40 - until epoch: 061, best_acc1: 75.158%
2022-06-27 06:00:40 - epoch 062 lr: 0.001000
2022-06-27 06:01:18 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 0.9534
2022-06-27 06:01:51 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.0605
2022-06-27 06:02:25 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 0.8573
2022-06-27 06:02:58 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 0.8818
2022-06-27 06:03:33 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 0.7863
2022-06-27 06:04:07 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 0.8316
2022-06-27 06:04:39 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 0.8468
2022-06-27 06:05:15 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 0.9712
2022-06-27 06:05:49 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.0762
2022-06-27 06:06:23 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 0.9545
2022-06-27 06:06:57 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 0.9191
2022-06-27 06:07:31 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.1060
2022-06-27 06:08:04 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 0.9745
2022-06-27 06:08:38 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 0.9305
2022-06-27 06:09:13 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 0.8993
2022-06-27 06:09:48 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 0.8989
2022-06-27 06:10:21 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 0.7999
2022-06-27 06:10:55 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.0482
2022-06-27 06:11:30 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 0.7564
2022-06-27 06:12:04 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 0.9670
2022-06-27 06:12:38 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.0616
2022-06-27 06:13:11 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 0.6883
2022-06-27 06:13:44 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.0402
2022-06-27 06:14:20 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 0.9514
2022-06-27 06:14:54 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.0838
2022-06-27 06:15:27 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 0.7254
2022-06-27 06:16:02 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 0.8325
2022-06-27 06:16:36 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 0.9797
2022-06-27 06:17:10 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 0.9461
2022-06-27 06:17:44 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.0368
2022-06-27 06:18:18 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 0.9520
2022-06-27 06:18:52 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 0.8675
2022-06-27 06:19:26 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 0.8081
2022-06-27 06:20:00 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.0252
2022-06-27 06:20:34 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 0.7969
2022-06-27 06:21:07 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 0.8001
2022-06-27 06:21:41 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 0.8491
2022-06-27 06:22:15 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 0.8705
2022-06-27 06:22:49 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 0.9434
2022-06-27 06:23:24 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 0.7499
2022-06-27 06:23:58 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 0.9050
2022-06-27 06:24:32 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 0.6588
2022-06-27 06:25:06 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 0.8218
2022-06-27 06:25:40 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 0.8845
2022-06-27 06:26:14 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 0.8671
2022-06-27 06:26:48 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 0.8008
2022-06-27 06:27:23 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 0.9997
2022-06-27 06:27:55 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.0173
2022-06-27 06:28:30 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 0.9105
2022-06-27 06:29:02 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 0.9797
2022-06-27 06:29:04 - train: epoch 062, train_loss: 0.9238
2022-06-27 06:30:18 - eval: epoch: 062, acc1: 75.448%, acc5: 92.600%, test_loss: 0.9749, per_image_load_time: 2.328ms, per_image_inference_time: 0.510ms
2022-06-27 06:30:19 - until epoch: 062, best_acc1: 75.448%
2022-06-27 06:30:19 - epoch 063 lr: 0.001000
2022-06-27 06:30:57 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 0.9968
2022-06-27 06:31:30 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 0.8103
2022-06-27 06:32:05 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 0.9416
2022-06-27 06:32:41 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 0.9874
2022-06-27 06:33:15 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 0.9406
2022-06-27 06:33:49 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 0.8804
2022-06-27 06:34:22 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 0.9767
2022-06-27 06:34:56 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.0376
2022-06-27 06:35:30 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 0.8621
2022-06-27 06:36:05 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 0.9909
2022-06-27 06:36:39 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 0.8394
2022-06-27 06:37:14 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 0.8516
2022-06-27 06:37:48 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 0.7443
2022-06-27 06:38:23 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.0355
2022-06-27 06:38:56 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 0.8610
2022-06-27 06:39:31 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 0.9434
2022-06-27 06:40:05 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 0.9661
2022-06-27 06:40:38 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 0.8202
2022-06-27 06:41:13 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 0.9772
2022-06-27 06:41:47 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 0.8081
2022-06-27 06:42:20 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 0.8130
2022-06-27 06:42:54 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.1548
2022-06-27 06:43:29 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 0.9944
2022-06-27 06:44:03 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 0.7879
2022-06-27 06:44:37 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 0.7654
2022-06-27 06:45:11 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 0.9421
2022-06-27 06:45:45 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.0854
2022-06-27 06:46:20 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.0028
2022-06-27 06:46:54 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 0.9832
2022-06-27 06:47:28 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.0587
2022-06-27 06:48:02 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 0.9840
2022-06-27 06:48:36 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 0.8894
2022-06-27 06:49:09 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 0.9750
2022-06-27 06:49:44 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 0.8293
2022-06-27 06:50:18 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 0.9149
2022-06-27 06:50:53 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 0.8602
2022-06-27 06:51:27 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 0.8495
2022-06-27 06:52:01 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 0.9396
2022-06-27 06:52:35 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 0.8607
2022-06-27 06:53:09 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 0.7572
2022-06-27 06:53:43 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.0481
2022-06-27 06:54:17 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.0758
2022-06-27 06:54:52 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.0202
2022-06-27 06:55:25 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 0.8344
2022-06-27 06:56:00 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 0.9560
2022-06-27 06:56:34 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 0.8958
2022-06-27 06:57:08 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 0.7404
2022-06-27 06:57:42 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 0.9726
2022-06-27 06:58:17 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 0.9750
2022-06-27 06:58:49 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 0.9589
2022-06-27 06:58:50 - train: epoch 063, train_loss: 0.9008
2022-06-27 07:00:06 - eval: epoch: 063, acc1: 75.582%, acc5: 92.660%, test_loss: 0.9721, per_image_load_time: 2.358ms, per_image_inference_time: 0.482ms
2022-06-27 07:00:06 - until epoch: 063, best_acc1: 75.582%
2022-06-27 07:00:06 - epoch 064 lr: 0.001000
2022-06-27 07:00:44 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 0.8239
2022-06-27 07:01:18 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 0.9309
2022-06-27 07:01:52 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 0.9888
2022-06-27 07:02:26 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 0.8280
2022-06-27 07:03:00 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 0.8706
2022-06-27 07:03:34 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 0.9608
2022-06-27 07:04:08 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.1351
2022-06-27 07:04:42 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 0.8490
2022-06-27 07:05:16 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 0.8760
2022-06-27 07:05:49 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 0.8838
2022-06-27 07:06:23 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 0.9094
2022-06-27 07:06:57 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 0.9840
2022-06-27 07:07:33 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 0.9453
2022-06-27 07:08:05 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.1275
2022-06-27 07:08:39 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 0.8985
2022-06-27 07:09:13 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 0.9027
2022-06-27 07:09:46 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 0.8557
2022-06-27 07:10:20 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 0.7698
2022-06-27 07:10:55 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 0.8746
2022-06-27 07:11:28 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 0.8995
2022-06-27 07:12:02 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 0.9332
2022-06-27 07:12:36 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 0.9691
2022-06-27 07:13:09 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.0871
2022-06-27 07:13:44 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 0.9183
2022-06-27 07:14:18 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 0.9227
2022-06-27 07:14:51 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 0.7658
2022-06-27 07:15:25 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 0.8499
2022-06-27 07:15:59 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 0.7839
2022-06-27 07:16:33 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 0.9878
2022-06-27 07:17:07 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 0.7621
2022-06-27 07:17:40 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 0.7525
2022-06-27 07:18:15 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.1584
2022-06-27 07:18:49 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.0408
2022-06-27 07:19:23 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 0.8919
2022-06-27 07:19:56 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 0.9400
2022-06-27 07:20:31 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 0.7292
2022-06-27 07:21:04 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 0.6649
2022-06-27 07:21:38 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 0.9408
2022-06-27 07:22:13 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 0.8046
2022-06-27 07:22:46 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 0.8346
2022-06-27 07:23:20 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 0.9985
2022-06-27 07:23:53 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 0.7916
2022-06-27 07:24:28 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 0.9555
2022-06-27 07:25:01 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 0.8016
2022-06-27 07:25:36 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 0.8527
2022-06-27 07:26:10 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 0.9008
2022-06-27 07:26:43 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.0494
2022-06-27 07:27:17 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.0288
2022-06-27 07:27:51 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 0.8806
2022-06-27 07:28:24 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 0.7812
2022-06-27 07:28:25 - train: epoch 064, train_loss: 0.8880
2022-06-27 07:29:39 - eval: epoch: 064, acc1: 75.666%, acc5: 92.760%, test_loss: 0.9680, per_image_load_time: 2.344ms, per_image_inference_time: 0.510ms
2022-06-27 07:29:40 - until epoch: 064, best_acc1: 75.666%
2022-06-27 07:29:40 - epoch 065 lr: 0.001000
2022-06-27 07:30:19 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 0.8099
2022-06-27 07:30:52 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 0.9663
2022-06-27 07:31:26 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 0.8352
2022-06-27 07:32:00 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 0.7952
2022-06-27 07:32:33 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.1034
2022-06-27 07:33:07 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.1230
2022-06-27 07:33:41 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 0.9231
2022-06-27 07:34:15 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 0.8785
2022-06-27 07:34:51 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 0.9030
2022-06-27 07:35:25 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 0.8695
2022-06-27 07:35:58 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 0.9202
2022-06-27 07:36:33 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 0.9418
2022-06-27 07:37:07 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 0.8669
2022-06-27 07:37:41 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 0.7955
2022-06-27 07:38:15 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 0.8494
2022-06-27 07:38:49 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 0.9319
2022-06-27 07:39:22 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 0.9207
2022-06-27 07:39:57 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 0.7386
2022-06-27 07:40:31 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 0.7709
2022-06-27 07:41:04 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 0.8161
2022-06-27 07:41:39 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 0.7943
2022-06-27 07:42:13 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 0.9587
2022-06-27 07:42:47 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 0.9027
2022-06-27 07:43:21 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 0.8858
2022-06-27 07:43:54 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 0.7635
2022-06-27 07:44:29 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.0183
2022-06-27 07:45:03 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 0.9852
2022-06-27 07:45:37 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 0.9175
2022-06-27 07:46:11 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 0.8414
2022-06-27 07:46:45 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 0.8572
2022-06-27 07:47:20 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.0351
2022-06-27 07:47:52 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 0.9701
2022-06-27 07:48:27 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 0.7848
2022-06-27 07:49:02 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 0.8590
2022-06-27 07:49:36 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 0.9510
2022-06-27 07:50:10 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 0.7587
2022-06-27 07:50:44 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 0.8613
2022-06-27 07:51:18 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 0.8877
2022-06-27 07:51:52 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 0.9845
2022-06-27 07:52:27 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 0.9281
2022-06-27 07:53:00 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 0.8289
2022-06-27 07:53:35 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 0.9461
2022-06-27 07:54:09 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 0.7828
2022-06-27 07:54:42 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 0.7603
2022-06-27 07:55:17 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 0.8591
2022-06-27 07:55:52 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 0.7655
2022-06-27 07:56:27 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 0.7845
2022-06-27 07:57:01 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 0.7291
2022-06-27 07:57:34 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 0.7581
2022-06-27 07:58:07 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 0.8589
2022-06-27 07:58:08 - train: epoch 065, train_loss: 0.8752
2022-06-27 07:59:23 - eval: epoch: 065, acc1: 75.662%, acc5: 92.748%, test_loss: 0.9671, per_image_load_time: 2.380ms, per_image_inference_time: 0.484ms
2022-06-27 07:59:23 - until epoch: 065, best_acc1: 75.666%
2022-06-27 07:59:23 - epoch 066 lr: 0.001000
2022-06-27 08:00:02 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 0.8783
2022-06-27 08:00:36 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 0.8915
2022-06-27 08:01:09 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 0.8419
2022-06-27 08:01:43 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 0.7873
2022-06-27 08:02:16 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 0.8884
2022-06-27 08:02:50 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 0.7984
2022-06-27 08:03:24 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 0.8514
2022-06-27 08:03:58 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.0577
2022-06-27 08:04:31 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 0.7851
2022-06-27 08:05:05 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 0.6117
2022-06-27 08:05:38 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 0.8701
2022-06-27 08:06:13 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 0.9834
2022-06-27 08:06:47 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 0.8229
2022-06-27 08:07:21 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 0.8523
2022-06-27 08:07:55 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 0.8776
2022-06-27 08:08:30 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.0425
2022-06-27 08:09:04 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 0.7346
2022-06-27 08:09:38 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 0.7121
2022-06-27 08:10:11 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.0220
2022-06-27 08:10:46 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 0.8403
2022-06-27 08:11:19 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 0.9561
2022-06-27 08:11:54 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 0.6719
2022-06-27 08:12:28 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 0.8278
2022-06-27 08:13:02 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.0292
2022-06-27 08:13:36 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 0.8468
2022-06-27 08:14:10 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 0.7455
2022-06-27 08:14:44 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.0309
2022-06-27 08:15:18 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 0.8910
2022-06-27 08:15:54 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 0.9055
2022-06-27 08:16:27 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 0.9060
2022-06-27 08:17:02 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.0864
2022-06-27 08:17:36 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 0.7632
2022-06-27 08:18:11 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 0.8250
2022-06-27 08:18:45 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.0575
2022-06-27 08:19:19 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 0.9216
2022-06-27 08:19:53 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.0090
2022-06-27 08:20:27 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 0.7651
2022-06-27 08:21:01 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 0.8320
2022-06-27 08:21:35 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 0.9487
2022-06-27 08:22:09 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 0.7357
2022-06-27 08:22:44 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 0.9764
2022-06-27 08:23:17 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 0.7922
2022-06-27 08:23:52 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 0.7687
2022-06-27 08:24:25 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 0.8289
2022-06-27 08:25:00 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.0550
2022-06-27 08:25:34 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.0846
2022-06-27 08:26:09 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 0.6599
2022-06-27 08:26:43 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 0.8911
2022-06-27 08:27:17 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 0.7150
2022-06-27 08:27:49 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 0.8626
2022-06-27 08:27:51 - train: epoch 066, train_loss: 0.8660
2022-06-27 08:29:05 - eval: epoch: 066, acc1: 75.702%, acc5: 92.770%, test_loss: 0.9635, per_image_load_time: 2.354ms, per_image_inference_time: 0.497ms
2022-06-27 08:29:05 - until epoch: 066, best_acc1: 75.702%
2022-06-27 08:29:05 - epoch 067 lr: 0.001000
2022-06-27 08:29:43 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 0.8541
2022-06-27 08:30:18 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 0.8480
2022-06-27 08:30:52 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 0.8791
2022-06-27 08:31:25 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 0.8199
2022-06-27 08:31:59 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 0.8285
2022-06-27 08:32:33 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 0.8837
2022-06-27 08:33:06 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 0.7665
2022-06-27 08:33:40 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 0.7682
2022-06-27 08:34:15 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.0448
2022-06-27 08:34:49 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 0.8346
2022-06-27 08:35:23 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 0.7686
2022-06-27 08:35:58 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 0.9125
2022-06-27 08:36:31 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 0.9457
2022-06-27 08:37:05 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 0.7611
2022-06-27 08:37:40 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 0.7092
2022-06-27 08:38:14 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 0.8058
2022-06-27 08:38:48 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 0.7184
2022-06-27 08:39:22 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.1475
2022-06-27 08:39:56 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 0.9071
2022-06-27 08:40:30 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 0.8987
2022-06-27 08:41:05 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 0.8238
2022-06-27 08:41:39 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 0.8300
2022-06-27 08:42:13 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 0.9551
2022-06-27 08:42:46 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 0.6677
2022-06-27 08:43:21 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 0.8485
2022-06-27 08:43:55 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 0.8275
2022-06-27 08:44:29 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 0.7236
2022-06-27 08:45:04 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 0.9513
2022-06-27 08:45:38 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 0.9194
2022-06-27 08:46:12 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 0.7599
2022-06-27 08:46:46 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 0.7409
2022-06-27 08:47:20 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 0.9092
2022-06-27 08:47:54 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 0.7031
2022-06-27 08:48:28 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 0.9904
2022-06-27 08:49:03 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 0.8188
2022-06-27 08:49:37 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 0.8667
2022-06-27 08:50:11 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 0.8717
2022-06-27 08:50:45 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.0202
2022-06-27 08:51:19 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 0.8037
2022-06-27 08:51:52 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 0.8123
2022-06-27 08:52:27 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.0442
2022-06-27 08:53:01 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.0363
2022-06-27 08:53:35 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 0.7756
2022-06-27 08:54:10 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 0.7109
2022-06-27 08:54:44 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 0.7714
2022-06-27 08:55:18 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 0.7556
2022-06-27 08:55:52 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.0049
2022-06-27 08:56:26 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 0.7124
2022-06-27 08:57:01 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 0.7706
2022-06-27 08:57:33 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 0.8652
2022-06-27 08:57:34 - train: epoch 067, train_loss: 0.8599
2022-06-27 08:58:50 - eval: epoch: 067, acc1: 75.710%, acc5: 92.702%, test_loss: 0.9667, per_image_load_time: 2.404ms, per_image_inference_time: 0.487ms
2022-06-27 08:58:50 - until epoch: 067, best_acc1: 75.710%
2022-06-27 08:58:50 - epoch 068 lr: 0.001000
2022-06-27 08:59:29 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.0349
2022-06-27 09:00:03 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 0.8672
2022-06-27 09:00:36 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 0.9045
2022-06-27 09:01:10 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 0.8317
2022-06-27 09:01:43 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 0.9144
2022-06-27 09:02:17 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 0.9861
2022-06-27 09:02:51 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 0.9255
2022-06-27 09:03:25 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 0.7712
2022-06-27 09:03:59 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 0.7837
2022-06-27 09:04:32 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 0.6767
2022-06-27 09:05:06 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 0.8705
2022-06-27 09:05:40 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 0.6138
2022-06-27 09:06:14 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 0.8061
2022-06-27 09:06:48 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 0.8008
2022-06-27 09:07:21 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 0.9213
2022-06-27 09:07:56 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 0.9870
2022-06-27 09:08:29 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 0.9387
2022-06-27 09:09:04 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 0.8731
2022-06-27 09:09:38 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 0.8991
2022-06-27 09:10:12 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.0156
2022-06-27 09:10:45 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 0.9434
2022-06-27 09:11:20 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 0.8252
2022-06-27 09:11:53 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 0.9431
2022-06-27 09:12:27 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 0.9170
2022-06-27 09:13:01 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 0.7841
2022-06-27 09:13:35 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 0.7786
2022-06-27 09:14:09 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 0.7490
2022-06-27 09:14:43 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 0.8237
2022-06-27 09:15:17 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 0.8703
2022-06-27 09:15:51 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 0.8670
2022-06-27 09:16:25 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 0.7441
2022-06-27 09:16:58 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 0.7734
2022-06-27 09:17:32 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 0.7254
2022-06-27 09:18:06 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 0.8704
2022-06-27 09:18:41 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 0.8620
2022-06-27 09:19:14 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 0.8111
2022-06-27 09:19:48 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 0.9575
2022-06-27 09:20:22 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 0.9264
2022-06-27 09:20:57 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 0.8247
2022-06-27 09:21:30 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 0.8665
2022-06-27 09:22:05 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 0.7677
2022-06-27 09:22:39 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 0.7769
2022-06-27 09:23:13 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 0.7191
2022-06-27 09:23:47 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 0.8227
2022-06-27 09:24:20 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 0.8031
2022-06-27 09:24:55 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.0718
2022-06-27 09:25:29 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 0.9054
2022-06-27 09:26:03 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 0.8196
2022-06-27 09:26:37 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 0.7089
2022-06-27 09:27:10 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 0.7216
2022-06-27 09:27:11 - train: epoch 068, train_loss: 0.8529
2022-06-27 09:28:26 - eval: epoch: 068, acc1: 75.802%, acc5: 92.728%, test_loss: 0.9627, per_image_load_time: 2.281ms, per_image_inference_time: 0.501ms
2022-06-27 09:28:26 - until epoch: 068, best_acc1: 75.802%
2022-06-27 09:28:26 - epoch 069 lr: 0.001000
2022-06-27 09:29:04 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 0.8960
2022-06-27 09:29:39 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.0613
2022-06-27 09:30:12 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 0.8498
2022-06-27 09:30:46 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 0.9937
2022-06-27 09:31:20 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 0.8249
2022-06-27 09:31:54 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 0.7841
2022-06-27 09:32:27 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 0.7316
2022-06-27 09:33:02 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 0.8905
2022-06-27 09:33:35 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 0.7425
2022-06-27 09:34:10 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 0.8164
2022-06-27 09:34:42 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 0.8253
2022-06-27 09:35:16 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 0.8115
2022-06-27 09:35:50 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.0277
2022-06-27 09:36:24 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.0440
2022-06-27 09:36:58 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 0.7305
2022-06-27 09:37:32 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 0.8487
2022-06-27 09:38:06 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 0.7346
2022-06-27 09:38:40 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 0.9037
2022-06-27 09:39:13 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 0.8474
2022-06-27 09:39:47 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 0.7824
2022-06-27 09:40:21 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 0.8104
2022-06-27 09:40:54 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 0.8686
2022-06-27 09:41:28 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 0.6910
2022-06-27 09:42:02 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 0.8946
2022-06-27 09:42:35 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 0.8572
2022-06-27 09:43:09 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 0.6558
2022-06-27 09:43:43 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 0.9520
2022-06-27 09:44:17 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 0.8971
2022-06-27 09:44:51 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 0.9016
2022-06-27 09:45:26 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 0.7943
2022-06-27 09:46:00 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 0.9282
2022-06-27 09:46:33 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 0.7016
2022-06-27 09:47:08 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 0.9250
2022-06-27 09:47:41 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 0.9048
2022-06-27 09:48:16 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 0.7243
2022-06-27 09:48:50 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 0.8170
2022-06-27 09:49:24 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 0.8519
2022-06-27 09:49:58 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 0.9423
2022-06-27 09:50:32 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 0.8366
2022-06-27 09:51:06 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 0.8347
2022-06-27 09:51:41 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 0.8042
2022-06-27 09:52:14 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 0.7528
2022-06-27 09:52:48 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 0.7698
2022-06-27 09:53:22 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.0032
2022-06-27 09:53:57 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 0.8320
2022-06-27 09:54:30 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 0.9560
2022-06-27 09:55:05 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 0.9795
2022-06-27 09:55:39 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 0.9991
2022-06-27 09:56:13 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 0.7269
2022-06-27 09:56:46 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 0.9014
2022-06-27 09:56:47 - train: epoch 069, train_loss: 0.8459
2022-06-27 09:58:02 - eval: epoch: 069, acc1: 75.812%, acc5: 92.780%, test_loss: 0.9637, per_image_load_time: 2.147ms, per_image_inference_time: 0.493ms
2022-06-27 09:58:02 - until epoch: 069, best_acc1: 75.812%
2022-06-27 09:58:02 - epoch 070 lr: 0.001000
2022-06-27 09:58:42 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 0.8509
2022-06-27 09:59:15 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 0.8877
2022-06-27 09:59:49 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 0.9249
2022-06-27 10:00:23 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 0.8169
2022-06-27 10:00:57 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 0.9455
2022-06-27 10:01:31 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 0.8021
2022-06-27 10:02:05 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 0.8559
2022-06-27 10:02:39 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 0.8471
2022-06-27 10:03:14 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 0.7988
2022-06-27 10:03:48 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 0.8215
2022-06-27 10:04:20 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.0228
2022-06-27 10:04:54 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 0.7554
2022-06-27 10:05:28 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 0.8641
2022-06-27 10:06:02 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 0.8313
2022-06-27 10:06:37 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 0.7699
2022-06-27 10:07:10 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 0.8415
2022-06-27 10:07:44 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 0.7573
2022-06-27 10:08:18 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 0.7473
2022-06-27 10:08:51 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 0.9217
2022-06-27 10:09:26 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 0.8660
2022-06-27 10:10:00 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 0.8928
2022-06-27 10:10:33 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 0.7630
2022-06-27 10:11:08 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 0.8920
2022-06-27 10:11:42 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.0009
2022-06-27 10:12:17 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 0.7315
2022-06-27 10:12:50 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 0.9352
2022-06-27 10:13:24 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 0.8346
2022-06-27 10:13:58 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 0.9173
2022-06-27 10:14:32 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 0.8024
2022-06-27 10:15:06 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 0.7789
2022-06-27 10:15:40 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 0.8118
2022-06-27 10:16:13 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 0.9507
2022-06-27 10:16:47 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 0.7459
2022-06-27 10:17:21 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 0.7771
2022-06-27 10:17:56 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 0.9110
2022-06-27 10:18:31 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 0.8489
2022-06-27 10:19:05 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 0.8811
2022-06-27 10:19:38 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 0.6794
2022-06-27 10:20:13 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 0.7860
2022-06-27 10:20:47 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 0.8951
2022-06-27 10:21:19 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 0.8383
2022-06-27 10:21:55 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 0.7177
2022-06-27 10:22:28 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 0.8369
2022-06-27 10:23:03 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 0.7850
2022-06-27 10:23:36 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 0.8434
2022-06-27 10:24:11 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 0.9245
2022-06-27 10:24:45 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 0.7124
2022-06-27 10:25:19 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 0.7257
2022-06-27 10:25:54 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 0.7305
2022-06-27 10:26:26 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 0.8321
2022-06-27 10:26:27 - train: epoch 070, train_loss: 0.8399
2022-06-27 10:27:41 - eval: epoch: 070, acc1: 75.946%, acc5: 92.904%, test_loss: 0.9603, per_image_load_time: 2.237ms, per_image_inference_time: 0.507ms
2022-06-27 10:27:42 - until epoch: 070, best_acc1: 75.946%
2022-06-27 10:27:42 - epoch 071 lr: 0.001000
2022-06-27 10:28:20 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 0.6476
2022-06-27 10:28:54 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 0.7261
2022-06-27 10:29:28 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 0.8824
2022-06-27 10:30:02 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 0.9123
2022-06-27 10:30:35 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 0.7779
2022-06-27 10:31:09 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 0.9250
2022-06-27 10:31:43 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 0.8325
2022-06-27 10:32:16 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 0.8929
2022-06-27 10:32:50 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 0.8411
2022-06-27 10:33:23 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 0.9517
2022-06-27 10:33:58 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 0.8787
2022-06-27 10:34:32 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 0.8344
2022-06-27 10:35:05 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 0.6976
2022-06-27 10:35:39 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 0.7710
2022-06-27 10:36:12 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 0.7820
2022-06-27 10:36:46 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 0.8429
2022-06-27 10:37:20 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 0.7522
2022-06-27 10:37:54 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 0.9674
2022-06-27 10:38:27 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 0.7871
2022-06-27 10:39:01 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 0.7834
2022-06-27 10:39:36 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 0.8503
2022-06-27 10:40:09 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 0.8247
2022-06-27 10:40:43 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 0.6660
2022-06-27 10:41:16 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 0.7821
2022-06-27 10:41:49 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 0.8333
2022-06-27 10:42:24 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 0.8173
2022-06-27 10:42:58 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.0164
2022-06-27 10:43:32 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 0.8585
2022-06-27 10:44:05 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 0.8456
2022-06-27 10:44:39 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 0.9700
2022-06-27 10:45:13 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 0.7726
2022-06-27 10:45:47 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 0.8486
2022-06-27 10:46:21 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 0.8218
2022-06-27 10:46:55 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 0.9271
2022-06-27 10:47:28 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 0.8877
2022-06-27 10:48:02 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 0.8153
2022-06-27 10:48:37 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 0.8759
2022-06-27 10:49:10 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 0.9345
2022-06-27 10:49:44 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 0.8858
2022-06-27 10:50:18 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 0.7973
2022-06-27 10:50:52 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 0.8264
2022-06-27 10:51:27 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.0276
2022-06-27 10:52:00 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 0.7420
2022-06-27 10:52:34 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 0.8926
2022-06-27 10:53:08 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 0.9384
2022-06-27 10:53:41 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 0.7088
2022-06-27 10:54:15 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 0.6618
2022-06-27 10:54:50 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 0.8502
2022-06-27 10:55:24 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 0.8510
2022-06-27 10:55:57 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 0.8285
2022-06-27 10:55:58 - train: epoch 071, train_loss: 0.8318
2022-06-27 10:57:13 - eval: epoch: 071, acc1: 75.898%, acc5: 92.798%, test_loss: 0.9607, per_image_load_time: 2.280ms, per_image_inference_time: 0.515ms
2022-06-27 10:57:13 - until epoch: 071, best_acc1: 75.946%
2022-06-27 10:57:13 - epoch 072 lr: 0.001000
2022-06-27 10:57:53 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 0.8705
2022-06-27 10:58:26 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 0.8305
2022-06-27 10:59:00 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 0.8840
2022-06-27 10:59:34 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 0.8006
2022-06-27 11:00:08 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 0.6373
2022-06-27 11:00:41 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 0.8879
2022-06-27 11:01:15 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 0.7148
2022-06-27 11:01:48 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 0.8467
2022-06-27 11:02:22 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 0.6342
2022-06-27 11:02:56 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 0.7015
2022-06-27 11:03:30 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 0.8839
2022-06-27 11:04:04 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 0.8283
2022-06-27 11:04:38 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 0.9545
2022-06-27 11:05:11 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 0.7773
2022-06-27 11:05:46 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 0.7817
2022-06-27 11:06:20 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 0.9335
2022-06-27 11:06:54 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 0.8989
2022-06-27 11:07:27 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 0.7041
2022-06-27 11:08:02 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 0.7832
2022-06-27 11:08:35 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 0.7018
2022-06-27 11:09:10 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 0.9571
2022-06-27 11:09:44 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 0.7951
2022-06-27 11:10:18 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 0.9667
2022-06-27 11:10:51 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 0.9316
2022-06-27 11:11:26 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 0.7262
2022-06-27 11:11:59 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 0.7716
2022-06-27 11:12:34 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 0.7819
2022-06-27 11:13:08 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 0.8786
2022-06-27 11:13:42 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 0.7745
2022-06-27 11:14:17 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 0.8798
2022-06-27 11:14:51 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 0.9791
2022-06-27 11:15:25 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 0.8828
2022-06-27 11:15:58 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 0.8479
2022-06-27 11:16:33 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 0.7762
2022-06-27 11:17:07 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 0.8115
2022-06-27 11:17:41 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 0.8783
2022-06-27 11:18:15 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 0.9033
2022-06-27 11:18:49 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 0.8293
2022-06-27 11:19:23 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 0.7140
2022-06-27 11:19:58 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 0.7632
2022-06-27 11:20:32 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 0.8821
2022-06-27 11:21:06 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 0.6833
2022-06-27 11:21:40 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 0.8487
2022-06-27 11:22:14 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 0.8159
2022-06-27 11:22:48 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 0.8725
2022-06-27 11:23:23 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 0.7584
2022-06-27 11:23:57 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 0.7956
2022-06-27 11:24:31 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 0.8954
2022-06-27 11:25:05 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 0.8501
2022-06-27 11:25:38 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 0.8857
2022-06-27 11:25:39 - train: epoch 072, train_loss: 0.8280
2022-06-27 11:26:55 - eval: epoch: 072, acc1: 75.772%, acc5: 92.744%, test_loss: 0.9619, per_image_load_time: 2.444ms, per_image_inference_time: 0.468ms
2022-06-27 11:26:55 - until epoch: 072, best_acc1: 75.946%
2022-06-27 11:26:55 - epoch 073 lr: 0.001000
2022-06-27 11:27:34 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 0.9294
2022-06-27 11:28:09 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 0.7798
2022-06-27 11:28:41 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 0.9018
2022-06-27 11:29:15 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 0.6008
2022-06-27 11:29:49 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 0.6748
2022-06-27 11:30:23 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 0.8640
2022-06-27 11:30:57 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 0.8102
2022-06-27 11:31:31 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 0.7994
2022-06-27 11:32:06 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 0.8156
2022-06-27 11:32:39 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 0.7369
2022-06-27 11:33:13 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 0.7065
2022-06-27 11:33:47 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 0.8061
2022-06-27 11:34:21 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 0.8187
2022-06-27 11:34:55 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 0.6621
2022-06-27 11:35:30 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 0.9348
2022-06-27 11:36:04 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 0.6921
2022-06-27 11:36:38 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 0.9588
2022-06-27 11:37:12 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 0.7404
2022-06-27 11:37:46 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 0.9045
2022-06-27 11:38:21 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 0.7440
2022-06-27 11:38:54 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 0.7581
2022-06-27 11:39:29 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 0.8471
2022-06-27 11:40:03 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 0.6852
2022-06-27 11:40:37 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 0.8882
2022-06-27 11:41:11 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 0.8310
2022-06-27 11:41:45 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 0.6728
2022-06-27 11:42:20 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 0.9696
2022-06-27 11:42:54 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 0.9326
2022-06-27 11:43:28 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 0.7980
2022-06-27 11:44:02 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 0.7946
2022-06-27 11:44:36 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 0.8199
2022-06-27 11:45:10 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 0.9143
2022-06-27 11:45:44 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 0.7550
2022-06-27 11:46:19 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 0.7807
2022-06-27 11:46:53 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 0.6885
2022-06-27 11:47:27 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 0.8861
2022-06-27 11:48:01 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 0.8422
2022-06-27 11:48:35 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 0.8906
2022-06-27 11:49:10 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 0.7617
2022-06-27 11:49:44 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 0.7943
2022-06-27 11:50:18 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 0.9405
2022-06-27 11:50:52 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 0.9910
2022-06-27 11:51:27 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 0.9136
2022-06-27 11:52:01 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 0.8821
2022-06-27 11:52:35 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 0.7866
2022-06-27 11:53:10 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.0224
2022-06-27 11:53:44 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 0.7070
2022-06-27 11:54:18 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 0.8009
2022-06-27 11:54:53 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 0.7589
2022-06-27 11:55:25 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 0.9936
2022-06-27 11:55:26 - train: epoch 073, train_loss: 0.8230
2022-06-27 11:56:42 - eval: epoch: 073, acc1: 75.914%, acc5: 92.828%, test_loss: 0.9613, per_image_load_time: 2.407ms, per_image_inference_time: 0.476ms
2022-06-27 11:56:42 - until epoch: 073, best_acc1: 75.946%
2022-06-27 11:56:42 - epoch 074 lr: 0.001000
2022-06-27 11:57:21 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 0.7966
2022-06-27 11:57:55 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 0.9335
2022-06-27 11:58:29 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 0.8365
2022-06-27 11:59:03 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 0.9563
2022-06-27 11:59:36 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 0.7866
2022-06-27 12:00:11 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 0.7564
2022-06-27 12:00:45 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 0.6062
2022-06-27 12:01:18 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.0425
2022-06-27 12:01:53 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 0.7232
2022-06-27 12:02:28 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 0.9382
2022-06-27 12:03:02 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 0.7104
2022-06-27 12:03:36 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 0.7136
2022-06-27 12:04:10 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 0.8464
2022-06-27 12:04:44 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 0.7969
2022-06-27 12:05:18 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 0.7998
2022-06-27 12:05:52 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 0.8273
2022-06-27 12:06:26 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 0.9025
2022-06-27 12:07:00 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 0.9387
2022-06-27 12:07:34 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 0.8554
2022-06-27 12:08:08 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 0.6693
2022-06-27 12:08:43 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 0.8202
2022-06-27 12:09:16 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.0648
2022-06-27 12:09:50 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 0.9107
2022-06-27 12:10:24 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 0.8494
2022-06-27 12:10:58 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 0.8100
2022-06-27 12:11:32 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 0.6600
2022-06-27 12:12:06 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 0.7297
2022-06-27 12:12:40 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.1361
2022-06-27 12:13:13 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 0.9014
2022-06-27 12:13:47 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 0.8502
2022-06-27 12:14:21 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 0.8606
2022-06-27 12:14:55 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 0.8386
2022-06-27 12:15:29 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 0.7391
2022-06-27 12:16:03 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 0.8549
2022-06-27 12:16:37 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 0.6421
2022-06-27 12:17:12 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 0.9817
2022-06-27 12:17:45 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 0.8688
2022-06-27 12:18:20 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 0.8151
2022-06-27 12:18:54 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 0.8377
2022-06-27 12:19:28 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 0.9334
2022-06-27 12:20:02 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 0.8531
2022-06-27 12:20:36 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 0.8690
2022-06-27 12:21:11 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 0.9186
2022-06-27 12:21:46 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 0.7592
2022-06-27 12:22:19 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 0.6267
2022-06-27 12:22:53 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 0.8843
2022-06-27 12:23:27 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 0.7595
2022-06-27 12:24:01 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 0.8825
2022-06-27 12:24:36 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 0.9665
2022-06-27 12:25:09 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 0.7915
2022-06-27 12:25:10 - train: epoch 074, train_loss: 0.8204
2022-06-27 12:26:26 - eval: epoch: 074, acc1: 75.826%, acc5: 92.834%, test_loss: 0.9633, per_image_load_time: 2.403ms, per_image_inference_time: 0.498ms
2022-06-27 12:26:26 - until epoch: 074, best_acc1: 75.946%
2022-06-27 12:26:26 - epoch 075 lr: 0.001000
2022-06-27 12:27:05 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 0.7810
2022-06-27 12:27:39 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 0.8770
2022-06-27 12:28:12 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 0.7086
2022-06-27 12:28:46 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 0.7471
2022-06-27 12:29:20 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 0.7503
2022-06-27 12:29:54 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 0.7830
2022-06-27 12:30:28 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 0.8161
2022-06-27 12:31:02 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 0.7666
2022-06-27 12:31:35 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 0.9379
2022-06-27 12:32:09 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 0.7435
2022-06-27 12:32:43 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 0.9846
2022-06-27 12:33:18 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 0.8327
2022-06-27 12:33:52 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 0.7878
2022-06-27 12:34:26 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 0.8340
2022-06-27 12:34:59 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 0.8357
2022-06-27 12:35:33 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 0.7320
2022-06-27 12:36:07 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 0.9350
2022-06-27 12:36:41 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 0.8727
2022-06-27 12:37:15 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 0.6947
2022-06-27 12:37:49 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 0.6871
2022-06-27 12:38:23 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 0.8997
2022-06-27 12:38:58 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 0.6723
2022-06-27 12:39:32 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 0.6478
2022-06-27 12:40:06 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 0.7622
2022-06-27 12:40:41 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 0.7307
2022-06-27 12:41:14 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 0.8555
2022-06-27 12:41:50 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 0.8312
2022-06-27 12:42:24 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 0.8422
2022-06-27 12:42:58 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 0.7996
2022-06-27 12:43:32 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 0.9902
2022-06-27 12:44:06 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 0.7230
2022-06-27 12:44:40 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 0.6939
2022-06-27 12:45:15 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 0.8813
2022-06-27 12:45:49 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 0.8521
2022-06-27 12:46:23 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 0.7929
2022-06-27 12:46:58 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 0.9048
2022-06-27 12:47:32 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.0171
2022-06-27 12:48:06 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 0.7407
2022-06-27 12:48:40 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 0.7555
2022-06-27 12:49:15 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 0.6060
2022-06-27 12:49:49 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 0.7343
2022-06-27 12:50:23 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 0.8020
2022-06-27 12:50:56 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 0.9685
2022-06-27 12:51:31 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 0.8232
2022-06-27 12:52:06 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 0.7697
2022-06-27 12:52:40 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 0.7387
2022-06-27 12:53:15 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 0.9000
2022-06-27 12:53:49 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 0.6825
2022-06-27 12:54:23 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 0.6826
2022-06-27 12:54:56 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 0.7313
2022-06-27 12:54:57 - train: epoch 075, train_loss: 0.8157
2022-06-27 12:56:13 - eval: epoch: 075, acc1: 75.858%, acc5: 92.866%, test_loss: 0.9597, per_image_load_time: 2.351ms, per_image_inference_time: 0.484ms
2022-06-27 12:56:13 - until epoch: 075, best_acc1: 75.946%
2022-06-27 12:56:13 - epoch 076 lr: 0.001000
2022-06-27 12:56:53 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 0.8384
2022-06-27 12:57:26 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 0.7743
2022-06-27 12:58:00 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 0.8773
2022-06-27 12:58:34 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 0.9355
2022-06-27 12:59:08 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 0.7365
2022-06-27 12:59:42 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 0.8074
2022-06-27 13:00:16 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 0.7616
2022-06-27 13:00:50 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 0.8125
2022-06-27 13:01:24 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 0.9012
2022-06-27 13:01:58 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 0.8116
2022-06-27 13:02:33 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 0.7030
2022-06-27 13:03:06 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 0.6460
2022-06-27 13:03:41 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 0.7926
2022-06-27 13:04:15 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 0.6881
2022-06-27 13:04:50 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 0.8122
2022-06-27 13:05:24 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 0.7877
2022-06-27 13:05:58 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 0.7554
2022-06-27 13:06:31 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 0.6280
2022-06-27 13:07:05 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 0.8572
2022-06-27 13:07:39 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 0.8481
2022-06-27 13:08:13 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 0.8685
2022-06-27 13:08:48 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 0.8720
2022-06-27 13:09:22 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 0.7622
2022-06-27 13:09:56 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 0.8794
2022-06-27 13:10:30 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 0.7239
2022-06-27 13:11:04 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 0.9210
2022-06-27 13:11:38 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 0.7703
2022-06-27 13:12:13 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 0.9478
2022-06-27 13:12:47 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 0.6748
2022-06-27 13:13:22 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 0.6356
2022-06-27 13:13:56 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 0.9346
2022-06-27 13:14:30 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 0.7517
2022-06-27 13:15:04 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 0.7431
2022-06-27 13:15:38 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 0.8704
2022-06-27 13:16:13 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 0.7940
2022-06-27 13:16:48 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 0.9339
2022-06-27 13:17:22 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 0.9469
2022-06-27 13:17:56 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 0.8054
2022-06-27 13:18:30 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 0.7294
2022-06-27 13:19:05 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 0.8280
2022-06-27 13:19:39 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 0.7126
2022-06-27 13:20:14 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 0.7305
2022-06-27 13:20:48 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 0.9985
2022-06-27 13:21:22 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 0.8044
2022-06-27 13:21:56 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 0.7958
2022-06-27 13:22:31 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 0.8530
2022-06-27 13:23:05 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.0553
2022-06-27 13:23:39 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 0.7306
2022-06-27 13:24:14 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 0.8050
2022-06-27 13:24:47 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.1116
2022-06-27 13:24:48 - train: epoch 076, train_loss: 0.8095
2022-06-27 13:26:03 - eval: epoch: 076, acc1: 75.852%, acc5: 92.844%, test_loss: 0.9633, per_image_load_time: 2.361ms, per_image_inference_time: 0.475ms
2022-06-27 13:26:04 - until epoch: 076, best_acc1: 75.946%
2022-06-27 13:26:04 - epoch 077 lr: 0.001000
2022-06-27 13:26:43 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 0.8865
2022-06-27 13:27:16 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 0.8466
2022-06-27 13:27:51 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 0.8174
2022-06-27 13:28:25 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 0.7326
2022-06-27 13:28:59 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 0.7731
2022-06-27 13:29:33 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 0.6543
2022-06-27 13:30:07 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 0.9212
2022-06-27 13:30:40 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 0.8330
2022-06-27 13:31:15 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.0179
2022-06-27 13:31:48 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 0.8952
2022-06-27 13:32:23 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 0.8491
2022-06-27 13:32:57 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 0.8838
2022-06-27 13:33:31 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 0.7158
2022-06-27 13:34:05 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 0.8785
2022-06-27 13:34:40 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 0.7705
2022-06-27 13:35:13 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 0.8500
2022-06-27 13:35:47 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 0.7814
2022-06-27 13:36:22 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 0.7676
2022-06-27 13:36:55 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 0.7696
2022-06-27 13:37:30 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 0.8572
2022-06-27 13:38:04 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 0.9319
2022-06-27 13:38:38 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 0.9467
2022-06-27 13:39:12 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 0.9262
2022-06-27 13:39:46 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 0.7755
2022-06-27 13:40:20 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 0.8241
2022-06-27 13:40:55 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 0.6697
2022-06-27 13:41:28 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 0.7793
2022-06-27 13:42:03 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 0.9582
2022-06-27 13:42:37 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 0.8186
2022-06-27 13:43:10 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 0.8337
2022-06-27 13:43:45 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 0.8446
2022-06-27 13:44:19 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 0.9568
2022-06-27 13:44:53 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 0.6695
2022-06-27 13:45:27 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 0.7798
2022-06-27 13:46:02 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 0.7907
2022-06-27 13:46:36 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 0.7838
2022-06-27 13:47:10 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 0.8576
2022-06-27 13:47:44 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 0.5717
2022-06-27 13:48:18 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 0.7169
2022-06-27 13:48:52 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 0.8427
2022-06-27 13:49:27 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 0.8353
2022-06-27 13:50:01 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 0.7714
2022-06-27 13:50:35 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 0.8218
2022-06-27 13:51:10 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 0.8210
2022-06-27 13:51:44 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 0.9433
2022-06-27 13:52:18 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 0.8143
2022-06-27 13:52:52 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 0.8515
2022-06-27 13:53:26 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 0.8335
2022-06-27 13:54:00 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 0.8127
2022-06-27 13:54:33 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 0.7928
2022-06-27 13:54:35 - train: epoch 077, train_loss: 0.8078
2022-06-27 13:55:51 - eval: epoch: 077, acc1: 76.008%, acc5: 92.830%, test_loss: 0.9608, per_image_load_time: 2.439ms, per_image_inference_time: 0.503ms
2022-06-27 13:55:52 - until epoch: 077, best_acc1: 76.008%
2022-06-27 13:55:52 - epoch 078 lr: 0.001000
2022-06-27 13:56:31 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 0.8743
2022-06-27 13:57:04 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.0291
2022-06-27 13:57:38 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 0.8907
2022-06-27 13:58:13 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 0.7919
2022-06-27 13:58:47 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 0.7141
2022-06-27 13:59:21 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 0.6323
2022-06-27 13:59:54 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 0.9664
2022-06-27 14:00:29 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 0.7635
2022-06-27 14:01:03 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 0.7710
2022-06-27 14:01:37 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 0.8019
2022-06-27 14:02:11 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 0.7759
2022-06-27 14:02:46 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 0.8387
2022-06-27 14:03:20 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 0.7267
2022-06-27 14:03:54 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 0.7242
2022-06-27 14:04:28 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 0.9646
2022-06-27 14:05:03 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 0.7677
2022-06-27 14:05:37 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 0.8861
2022-06-27 14:06:11 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 0.8644
2022-06-27 14:06:45 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 0.9083
2022-06-27 14:07:19 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 0.7075
2022-06-27 14:07:53 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.0540
2022-06-27 14:08:28 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 0.9579
2022-06-27 14:09:02 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 0.6543
2022-06-27 14:09:36 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 0.9246
2022-06-27 14:10:11 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.0495
2022-06-27 14:10:45 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 0.7738
2022-06-27 14:11:19 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 0.8705
2022-06-27 14:11:53 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 0.8000
2022-06-27 14:12:28 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 0.6831
2022-06-27 14:13:01 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 0.7927
2022-06-27 14:13:36 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 0.6883
2022-06-27 14:14:10 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 0.9016
2022-06-27 14:14:44 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 0.9143
2022-06-27 14:15:18 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 0.8397
2022-06-27 14:15:52 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 0.9952
2022-06-27 14:16:26 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 0.7818
2022-06-27 14:17:00 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 0.9417
2022-06-27 14:17:34 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 0.7833
2022-06-27 14:18:08 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 0.7712
2022-06-27 14:18:42 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 0.7899
2022-06-27 14:19:17 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 0.6967
2022-06-27 14:19:51 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 0.8886
2022-06-27 14:20:24 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 0.8897
2022-06-27 14:20:59 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 0.8078
2022-06-27 14:21:34 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 0.8837
2022-06-27 14:22:07 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 0.7327
2022-06-27 14:22:42 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 0.8986
2022-06-27 14:23:16 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 0.9104
2022-06-27 14:23:50 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 0.7125
2022-06-27 14:24:23 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 0.6473
2022-06-27 14:24:24 - train: epoch 078, train_loss: 0.8031
2022-06-27 14:25:40 - eval: epoch: 078, acc1: 75.930%, acc5: 92.838%, test_loss: 0.9614, per_image_load_time: 1.792ms, per_image_inference_time: 0.510ms
2022-06-27 14:25:40 - until epoch: 078, best_acc1: 76.008%
2022-06-27 14:25:40 - epoch 079 lr: 0.001000
2022-06-27 14:26:19 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 0.8096
2022-06-27 14:26:51 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 0.7258
2022-06-27 14:27:25 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 0.7334
2022-06-27 14:27:59 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 0.8189
2022-06-27 14:28:34 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 0.8447
2022-06-27 14:29:07 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 0.9706
2022-06-27 14:29:41 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 0.8901
2022-06-27 14:30:14 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 0.9347
2022-06-27 14:30:49 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 0.7627
2022-06-27 14:31:23 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 0.8956
2022-06-27 14:31:57 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 0.8937
2022-06-27 14:32:32 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 0.8627
2022-06-27 14:33:06 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 0.7015
2022-06-27 14:33:39 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 0.7734
2022-06-27 14:34:12 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 0.7163
2022-06-27 14:34:44 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 0.7028
2022-06-27 14:35:18 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 0.7163
2022-06-27 14:35:50 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 0.8340
2022-06-27 14:36:24 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 0.8925
2022-06-27 14:36:56 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 0.9000
2022-06-27 14:37:29 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 0.6820
2022-06-27 14:38:02 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 0.9504
2022-06-27 14:38:36 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 0.7830
2022-06-27 14:39:11 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 0.8481
2022-06-27 14:39:44 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 0.8546
2022-06-27 14:40:19 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 0.8324
2022-06-27 14:40:53 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 0.6247
2022-06-27 14:41:25 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 0.7510
2022-06-27 14:41:58 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 0.7978
2022-06-27 14:42:31 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 0.9146
2022-06-27 14:43:04 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 0.8474
2022-06-27 14:43:36 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 0.7572
2022-06-27 14:44:09 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 0.8219
2022-06-27 14:44:42 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 0.8363
2022-06-27 14:45:14 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 0.8575
2022-06-27 14:45:47 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 0.7142
2022-06-27 14:46:19 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 0.8369
2022-06-27 14:46:52 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 0.7972
2022-06-27 14:47:25 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 0.8222
2022-06-27 14:47:57 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 0.6557
2022-06-27 14:48:30 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 0.9051
2022-06-27 14:49:03 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 0.7489
2022-06-27 14:49:35 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 0.8217
2022-06-27 14:50:09 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 0.9109
2022-06-27 14:50:42 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 0.9286
2022-06-27 14:51:15 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 0.7924
2022-06-27 14:51:48 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 0.7076
2022-06-27 14:52:21 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 0.7656
2022-06-27 14:52:54 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 0.7460
2022-06-27 14:53:26 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 0.6755
2022-06-27 14:53:27 - train: epoch 079, train_loss: 0.7989
2022-06-27 14:54:41 - eval: epoch: 079, acc1: 75.854%, acc5: 92.858%, test_loss: 0.9645, per_image_load_time: 2.364ms, per_image_inference_time: 0.450ms
2022-06-27 14:54:42 - until epoch: 079, best_acc1: 76.008%
2022-06-27 14:54:42 - epoch 080 lr: 0.001000
2022-06-27 14:55:20 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 0.6938
2022-06-27 14:55:53 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 0.9172
2022-06-27 14:56:26 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 0.7700
2022-06-27 14:56:59 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 0.6778
2022-06-27 14:57:33 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 0.8748
2022-06-27 14:58:06 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 0.7647
2022-06-27 14:58:40 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 0.7012
2022-06-27 14:59:13 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 0.7711
2022-06-27 14:59:46 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 0.7906
2022-06-27 15:00:17 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 0.7736
2022-06-27 15:00:50 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 0.8007
2022-06-27 15:01:22 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 0.8452
2022-06-27 15:01:55 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 0.6569
2022-06-27 15:02:26 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 0.8316
2022-06-27 15:02:59 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 0.5965
2022-06-27 15:03:31 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 0.8549
2022-06-27 15:04:04 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 0.7752
2022-06-27 15:04:35 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 0.7836
2022-06-27 15:05:08 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 0.8662
2022-06-27 15:05:41 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 0.6355
2022-06-27 15:06:15 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 0.7981
2022-06-27 15:06:49 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 0.7628
2022-06-27 15:07:23 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 0.7129
2022-06-27 15:07:56 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 0.8866
2022-06-27 15:08:31 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 0.7012
2022-06-27 15:09:04 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 0.6861
2022-06-27 15:09:38 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 0.7611
2022-06-27 15:10:11 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 0.6844
2022-06-27 15:10:45 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 0.9730
2022-06-27 15:11:19 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 0.7590
2022-06-27 15:11:53 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 0.7302
2022-06-27 15:12:26 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 0.7628
2022-06-27 15:13:00 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 0.9881
2022-06-27 15:13:33 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 0.6716
2022-06-27 15:14:07 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 0.7961
2022-06-27 15:14:41 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 0.7548
2022-06-27 15:15:14 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 0.7758
2022-06-27 15:15:48 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 0.9165
2022-06-27 15:16:21 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 0.7114
2022-06-27 15:16:55 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 0.8929
2022-06-27 15:17:28 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 0.9691
2022-06-27 15:18:02 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 0.6961
2022-06-27 15:18:36 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 0.8510
2022-06-27 15:19:10 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 0.7633
2022-06-27 15:19:43 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 0.8244
2022-06-27 15:20:17 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 0.8593
2022-06-27 15:20:50 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 0.8174
2022-06-27 15:21:25 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 0.7224
2022-06-27 15:21:58 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.0948
2022-06-27 15:22:31 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 0.7671
2022-06-27 15:22:32 - train: epoch 080, train_loss: 0.7953
2022-06-27 15:23:48 - eval: epoch: 080, acc1: 75.918%, acc5: 92.818%, test_loss: 0.9618, per_image_load_time: 2.410ms, per_image_inference_time: 0.509ms
2022-06-27 15:23:49 - until epoch: 080, best_acc1: 76.008%
2022-06-27 15:23:49 - epoch 081 lr: 0.001000
2022-06-27 15:24:28 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 0.6878
2022-06-27 15:25:02 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 0.7992
2022-06-27 15:25:36 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 0.7983
2022-06-27 15:26:10 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 0.6901
2022-06-27 15:26:44 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 0.9280
2022-06-27 15:27:17 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 0.6592
2022-06-27 15:27:51 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 0.7097
2022-06-27 15:28:24 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 0.7385
2022-06-27 15:28:58 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 0.7146
2022-06-27 15:29:33 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 0.7974
2022-06-27 15:30:06 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 0.8147
2022-06-27 15:30:40 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 0.8989
2022-06-27 15:31:13 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 0.7727
2022-06-27 15:31:47 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 0.5811
2022-06-27 15:32:21 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 0.9042
2022-06-27 15:32:55 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 0.8128
2022-06-27 15:33:29 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 0.8474
2022-06-27 15:34:02 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 0.8071
2022-06-27 15:34:35 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 0.8319
2022-06-27 15:35:08 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 0.8322
2022-06-27 15:35:41 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 0.9329
2022-06-27 15:36:13 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 0.6127
2022-06-27 15:36:47 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 0.5863
2022-06-27 15:37:20 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 0.8792
2022-06-27 15:37:54 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 0.8467
2022-06-27 15:38:27 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.0225
2022-06-27 15:39:00 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 0.8975
2022-06-27 15:39:34 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 0.8866
2022-06-27 15:40:07 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 0.8056
2022-06-27 15:40:41 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 0.6817
2022-06-27 15:41:14 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 0.7075
2022-06-27 15:41:48 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 0.7304
2022-06-27 15:42:22 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 0.7730
2022-06-27 15:42:55 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 0.8456
2022-06-27 15:43:29 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 0.8938
2022-06-27 15:44:02 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 0.7085
2022-06-27 15:44:36 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 0.6317
2022-06-27 15:45:10 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 0.7912
2022-06-27 15:45:44 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 0.7106
2022-06-27 15:46:17 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 0.7154
2022-06-27 15:46:52 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 0.7857
2022-06-27 15:47:27 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 0.8406
2022-06-27 15:48:01 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 0.8599
2022-06-27 15:48:36 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 0.8607
2022-06-27 15:49:09 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 0.8378
2022-06-27 15:49:44 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 0.7766
2022-06-27 15:50:18 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 0.7590
2022-06-27 15:50:53 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 0.6982
2022-06-27 15:51:26 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 0.9535
2022-06-27 15:51:58 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 0.6233
2022-06-27 15:51:59 - train: epoch 081, train_loss: 0.7929
2022-06-27 15:53:12 - eval: epoch: 081, acc1: 75.682%, acc5: 92.750%, test_loss: 0.9677, per_image_load_time: 2.338ms, per_image_inference_time: 0.450ms
2022-06-27 15:53:12 - until epoch: 081, best_acc1: 76.008%
2022-06-27 15:53:12 - epoch 082 lr: 0.001000
2022-06-27 15:53:51 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 0.7097
2022-06-27 15:54:22 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 0.7027
2022-06-27 15:54:56 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 0.7212
2022-06-27 15:55:29 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 0.9648
2022-06-27 15:56:01 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 0.8158
2022-06-27 15:56:35 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 0.6790
2022-06-27 15:57:09 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 0.8149
2022-06-27 15:57:42 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 0.8782
2022-06-27 15:58:16 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 0.9162
2022-06-27 15:58:49 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 0.7413
2022-06-27 15:59:22 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 0.8458
2022-06-27 15:59:56 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 0.7295
2022-06-27 16:00:30 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 0.8425
2022-06-27 16:01:04 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 0.8805
2022-06-27 16:01:37 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 0.7588
2022-06-27 16:02:11 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 0.8030
2022-06-27 16:02:43 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 0.8017
2022-06-27 16:03:16 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 0.7580
2022-06-27 16:03:48 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 0.7859
2022-06-27 16:04:21 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 0.6722
2022-06-27 16:04:53 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 0.7888
2022-06-27 16:05:25 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 0.7633
2022-06-27 16:05:58 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 0.8464
2022-06-27 16:06:31 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 0.9370
2022-06-27 16:07:05 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 0.8318
2022-06-27 16:07:37 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 0.7760
2022-06-27 16:08:11 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 0.8217
2022-06-27 16:08:45 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 0.6262
2022-06-27 16:09:18 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 0.7061
2022-06-27 16:09:53 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 0.7185
2022-06-27 16:10:26 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 0.9704
2022-06-27 16:11:00 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 1.0152
2022-06-27 16:11:34 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 0.6425
2022-06-27 16:12:07 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 0.8143
2022-06-27 16:12:41 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 0.6410
2022-06-27 16:13:15 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 0.6882
2022-06-27 16:13:50 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 0.7185
2022-06-27 16:14:22 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 0.8833
2022-06-27 16:14:57 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 0.7037
2022-06-27 16:15:30 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 0.7361
2022-06-27 16:16:04 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 0.9075
2022-06-27 16:16:37 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 0.8781
2022-06-27 16:17:11 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 0.6409
2022-06-27 16:17:45 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 0.6242
2022-06-27 16:18:19 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 0.5790
2022-06-27 16:18:52 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 0.9438
2022-06-27 16:19:27 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 0.7504
2022-06-27 16:20:01 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 0.7960
2022-06-27 16:20:34 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 0.8693
2022-06-27 16:21:07 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 0.8503
2022-06-27 16:21:08 - train: epoch 082, train_loss: 0.7888
2022-06-27 16:22:24 - eval: epoch: 082, acc1: 75.850%, acc5: 92.896%, test_loss: 0.9678, per_image_load_time: 2.441ms, per_image_inference_time: 0.465ms
2022-06-27 16:22:24 - until epoch: 082, best_acc1: 76.008%
2022-06-27 16:22:24 - epoch 083 lr: 0.001000
2022-06-27 16:23:03 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 0.8332
2022-06-27 16:23:38 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 0.7227
2022-06-27 16:24:11 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 0.6387
2022-06-27 16:24:44 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 0.7254
2022-06-27 16:25:17 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 0.8898
2022-06-27 16:25:48 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 0.7914
2022-06-27 16:26:20 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 0.7314
2022-06-27 16:26:52 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 0.7709
2022-06-27 16:27:25 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 0.7218
2022-06-27 16:27:57 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 0.8836
2022-06-27 16:28:30 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 0.7302
2022-06-27 16:29:03 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 0.6725
2022-06-27 16:29:36 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 0.9280
2022-06-27 16:30:09 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 0.8704
2022-06-27 16:30:42 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 0.6282
2022-06-27 16:31:15 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 0.7679
2022-06-27 16:31:49 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 0.9006
2022-06-27 16:32:22 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 0.9602
2022-06-27 16:32:55 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 0.8409
2022-06-27 16:33:29 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 0.6585
2022-06-27 16:34:01 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 0.7093
2022-06-27 16:34:35 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 0.6201
2022-06-27 16:35:08 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 0.8223
2022-06-27 16:35:41 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 0.7575
2022-06-27 16:36:14 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 0.6926
2022-06-27 16:36:47 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 0.7731
2022-06-27 16:37:20 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 0.7857
2022-06-27 16:37:54 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 0.8749
2022-06-27 16:38:28 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 0.8676
2022-06-27 16:39:01 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 0.7821
2022-06-27 16:39:33 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 0.7399
2022-06-27 16:40:08 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 0.8049
2022-06-27 16:40:42 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 0.8793
2022-06-27 16:41:15 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 0.7789
2022-06-27 16:41:49 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 0.7927
2022-06-27 16:42:22 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 0.8765
2022-06-27 16:42:55 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 0.6867
2022-06-27 16:43:28 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 0.7599
2022-06-27 16:44:02 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 0.8873
2022-06-27 16:44:35 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 0.9906
2022-06-27 16:45:09 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 0.7621
2022-06-27 16:45:41 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 0.9125
2022-06-27 16:46:15 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 0.6757
2022-06-27 16:46:48 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 0.7820
2022-06-27 16:47:22 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 0.8669
2022-06-27 16:47:55 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 0.7787
2022-06-27 16:48:29 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.0627
2022-06-27 16:49:02 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 0.9440
2022-06-27 16:49:36 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 0.8046
2022-06-27 16:50:08 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 0.6754
2022-06-27 16:50:09 - train: epoch 083, train_loss: 0.7857
2022-06-27 16:51:24 - eval: epoch: 083, acc1: 75.888%, acc5: 92.812%, test_loss: 0.9668, per_image_load_time: 2.303ms, per_image_inference_time: 0.468ms
2022-06-27 16:51:24 - until epoch: 083, best_acc1: 76.008%
2022-06-27 16:51:24 - epoch 084 lr: 0.001000
2022-06-27 16:52:03 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 0.5703
2022-06-27 16:52:37 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 0.9167
2022-06-27 16:53:10 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 0.7002
2022-06-27 16:53:43 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 0.7016
2022-06-27 16:54:18 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 0.7202
2022-06-27 16:54:50 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 0.7351
2022-06-27 16:55:24 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 0.9178
2022-06-27 16:55:57 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 0.9627
2022-06-27 16:56:30 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 0.7602
2022-06-27 16:57:04 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 0.7459
2022-06-27 16:57:37 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 0.6936
2022-06-27 16:58:10 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 0.8284
2022-06-27 16:58:43 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 0.8315
2022-06-27 16:59:17 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 0.8787
2022-06-27 16:59:51 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 0.6928
2022-06-27 17:00:24 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 0.6778
2022-06-27 17:00:58 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 0.8875
2022-06-27 17:01:31 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 0.6776
2022-06-27 17:02:05 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 0.8278
2022-06-27 17:02:38 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 0.6712
2022-06-27 17:03:12 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 0.5984
2022-06-27 17:03:45 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 0.7022
2022-06-27 17:04:20 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 0.8522
2022-06-27 17:04:53 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 0.6676
2022-06-27 17:05:27 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 0.7866
2022-06-27 17:06:01 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 0.7660
2022-06-27 17:06:34 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 0.6982
2022-06-27 17:07:08 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 0.9034
2022-06-27 17:07:43 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 0.8226
2022-06-27 17:08:16 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 0.8820
2022-06-27 17:08:50 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 0.8001
2022-06-27 17:09:23 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 0.6229
2022-06-27 17:09:58 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 0.8299
2022-06-27 17:10:31 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 0.6752
2022-06-27 17:11:05 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 0.7421
2022-06-27 17:11:38 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 0.7481
2022-06-27 17:12:12 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 0.9862
2022-06-27 17:12:46 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 0.8697
2022-06-27 17:13:19 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 0.7448
2022-06-27 17:13:52 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 0.7398
2022-06-27 17:14:28 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 0.8035
2022-06-27 17:15:00 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 0.9629
2022-06-27 17:15:34 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 0.9740
2022-06-27 17:16:07 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 0.8836
2022-06-27 17:16:42 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 0.7885
2022-06-27 17:17:15 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 0.8010
2022-06-27 17:17:47 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 0.9002
2022-06-27 17:18:20 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 0.7270
2022-06-27 17:18:54 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 0.7129
2022-06-27 17:19:27 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 0.9314
2022-06-27 17:19:28 - train: epoch 084, train_loss: 0.7833
2022-06-27 17:20:42 - eval: epoch: 084, acc1: 75.896%, acc5: 92.814%, test_loss: 0.9689, per_image_load_time: 2.425ms, per_image_inference_time: 0.455ms
2022-06-27 17:20:43 - until epoch: 084, best_acc1: 76.008%
2022-06-27 17:20:43 - epoch 085 lr: 0.001000
2022-06-27 17:21:22 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 0.6404
2022-06-27 17:21:56 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 0.7169
2022-06-27 17:22:30 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 0.7899
2022-06-27 17:23:04 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 0.8440
2022-06-27 17:23:38 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 0.8229
2022-06-27 17:24:12 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 0.6755
2022-06-27 17:24:45 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 0.7569
2022-06-27 17:25:19 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 0.8148
2022-06-27 17:25:52 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 0.7217
2022-06-27 17:26:26 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 0.7875
2022-06-27 17:27:00 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 0.8180
2022-06-27 17:27:33 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 0.5730
2022-06-27 17:28:07 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 0.7472
2022-06-27 17:28:41 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 0.8083
2022-06-27 17:29:15 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 0.7104
2022-06-27 17:29:48 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 0.7818
2022-06-27 17:30:22 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 0.9711
2022-06-27 17:30:56 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 0.5566
2022-06-27 17:31:29 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 0.6901
2022-06-27 17:32:03 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 0.6471
2022-06-27 17:32:37 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 0.6740
2022-06-27 17:33:11 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 0.6953
2022-06-27 17:33:44 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 0.8538
2022-06-27 17:34:18 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 0.8433
2022-06-27 17:34:52 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 0.8399
2022-06-27 17:35:26 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 0.9039
2022-06-27 17:35:59 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 0.7743
2022-06-27 17:36:32 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 0.8725
2022-06-27 17:37:06 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 0.9047
