2022-01-29 23:52:09 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.4436
2022-01-29 23:52:41 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.3276
2022-01-29 23:53:13 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.3103
2022-01-29 23:53:45 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.4484
2022-01-29 23:54:18 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.3286
2022-01-29 23:54:49 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.4204
2022-01-29 23:55:22 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.3329
2022-01-29 23:55:54 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.4508
2022-01-29 23:56:26 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.5557
2022-01-29 23:56:59 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.2903
2022-01-29 23:57:30 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.3790
2022-01-29 23:58:02 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.1885
2022-01-29 23:58:35 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.4342
2022-01-29 23:59:07 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.3900
2022-01-29 23:59:40 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.2787
2022-01-30 00:00:12 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.4564
2022-01-30 00:00:44 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.2149
2022-01-30 00:00:45 - train: epoch 051, train_loss: 1.3479
2022-01-30 00:01:59 - eval: epoch: 051, acc1: 70.078%, acc5: 89.884%, test_loss: 1.1990, per_image_load_time: 2.476ms, per_image_inference_time: 0.375ms
2022-01-30 00:02:00 - epoch 052 lr: 0.010000000000000002
2022-01-30 00:02:37 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.1634
2022-01-30 00:03:09 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.3394
2022-01-30 00:03:41 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.4653
2022-01-30 00:04:13 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.4101
2022-01-30 00:04:47 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.4471
2022-01-30 00:05:17 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.2738
2022-01-30 00:05:50 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.3336
2022-01-30 00:06:22 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.4444
2022-01-30 00:06:54 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.3571
2022-01-30 00:07:26 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.5100
2022-01-30 00:07:58 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.3041
2022-01-30 00:08:31 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.2339
2022-01-30 00:09:03 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.2252
2022-01-30 00:09:35 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.6394
2022-01-30 00:10:07 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.1651
2022-01-30 00:10:40 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.2185
2022-01-30 00:11:12 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.2169
2022-01-30 00:11:45 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.2126
2022-01-30 00:12:16 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.3533
2022-01-30 00:12:48 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.4377
2022-01-30 00:13:19 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.2315
2022-01-30 00:13:51 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.4255
2022-01-30 00:14:24 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.1849
2022-01-30 00:14:56 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.1941
2022-01-30 00:15:29 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.2745
2022-01-30 00:16:01 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.1216
2022-01-30 00:16:32 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.2372
2022-01-30 00:17:04 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.3382
2022-01-30 00:17:36 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.1463
2022-01-30 00:18:08 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.3475
2022-01-30 00:18:41 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.3045
2022-01-30 00:19:12 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.5127
2022-01-30 00:19:44 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.3899
2022-01-30 00:20:15 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.4515
2022-01-30 00:20:47 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.4180
2022-01-30 00:21:18 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.3945
2022-01-30 00:21:51 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.5120
2022-01-30 00:22:23 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.2460
2022-01-30 00:22:55 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.2988
2022-01-30 00:23:27 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.4545
2022-01-30 00:24:00 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.1686
2022-01-30 00:24:33 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.2463
2022-01-30 00:25:05 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.3706
2022-01-30 00:25:38 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.3841
2022-01-30 00:26:10 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.2073
2022-01-30 00:26:42 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.5033
2022-01-30 00:27:15 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.3754
2022-01-30 00:27:48 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.1363
2022-01-30 00:28:20 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.1870
2022-01-30 00:28:53 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.2561
2022-01-30 00:28:54 - train: epoch 052, train_loss: 1.3434
2022-01-30 00:30:08 - eval: epoch: 052, acc1: 68.106%, acc5: 88.592%, test_loss: 1.2914, per_image_load_time: 2.450ms, per_image_inference_time: 0.384ms
2022-01-30 00:30:09 - epoch 053 lr: 0.010000000000000002
2022-01-30 00:30:47 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.2482
2022-01-30 00:31:20 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.4984
2022-01-30 00:31:52 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.2838
2022-01-30 00:32:24 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.4018
2022-01-30 00:32:56 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.4479
2022-01-30 00:33:28 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.3805
2022-01-30 00:34:00 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.1720
2022-01-30 00:34:33 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.4664
2022-01-30 00:35:06 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.2655
2022-01-30 00:35:37 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.2817
2022-01-30 00:36:09 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.3879
2022-01-30 00:36:41 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.2467
2022-01-30 00:37:13 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.3376
2022-01-30 00:37:46 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.5490
2022-01-30 00:38:17 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.3590
2022-01-30 00:38:49 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.5356
2022-01-30 00:39:19 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.5348
2022-01-30 00:39:52 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.4052
2022-01-30 00:40:24 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.1362
2022-01-30 00:40:56 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.3974
2022-01-30 00:41:28 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.3816
2022-01-30 00:42:01 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.2920
2022-01-30 00:42:33 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.3264
2022-01-30 00:43:05 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.4521
2022-01-30 00:43:37 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.5075
2022-01-30 00:44:08 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.3806
2022-01-30 00:44:41 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.5504
2022-01-30 00:45:13 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.4469
2022-01-30 00:45:45 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.1943
2022-01-30 00:46:17 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.1914
2022-01-30 00:46:50 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.5853
2022-01-30 00:47:22 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.5091
2022-01-30 00:47:53 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.3275
2022-01-30 00:48:26 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.5403
2022-01-30 00:48:59 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.4514
2022-01-30 00:49:31 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.4973
2022-01-30 00:50:03 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.5078
2022-01-30 00:50:35 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.3043
2022-01-30 00:51:07 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.4956
2022-01-30 00:51:40 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.4687
2022-01-30 00:52:12 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.4301
2022-01-30 00:52:44 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.4078
2022-01-30 00:53:16 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.6642
2022-01-30 00:53:49 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.2813
2022-01-30 00:54:21 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.3494
2022-01-30 00:54:53 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.3152
2022-01-30 00:55:26 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.4634
2022-01-30 00:55:58 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.4726
2022-01-30 00:56:31 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.1813
2022-01-30 00:57:03 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.3090
2022-01-30 00:57:04 - train: epoch 053, train_loss: 1.3420
2022-01-30 00:58:18 - eval: epoch: 053, acc1: 70.096%, acc5: 90.032%, test_loss: 1.1898, per_image_load_time: 2.488ms, per_image_inference_time: 0.385ms
2022-01-30 00:58:19 - epoch 054 lr: 0.010000000000000002
2022-01-30 00:58:57 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.1788
2022-01-30 00:59:29 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.6409
2022-01-30 01:00:01 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.3289
2022-01-30 01:00:32 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.1945
2022-01-30 01:01:05 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.3141
2022-01-30 01:01:37 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.2103
2022-01-30 01:02:09 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.3847
2022-01-30 01:02:41 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.3864
2022-01-30 01:03:13 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.0827
2022-01-30 01:03:45 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.1592
2022-01-30 01:04:17 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.1907
2022-01-30 01:04:48 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.4502
2022-01-30 01:05:19 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.2926
2022-01-30 01:05:53 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.2919
2022-01-30 01:06:24 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.4232
2022-01-30 01:06:55 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.0108
2022-01-30 01:07:27 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.3703
2022-01-30 01:07:58 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.2820
2022-01-30 01:08:31 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.6742
2022-01-30 01:09:02 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.3563
2022-01-30 01:09:35 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.1685
2022-01-30 01:10:07 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.3144
2022-01-30 01:10:39 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.3782
2022-01-30 01:11:11 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.2213
2022-01-30 01:11:43 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.3816
2022-01-30 01:12:15 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.2171
2022-01-30 01:12:47 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.3865
2022-01-30 01:13:19 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.5799
2022-01-30 01:13:50 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.2450
2022-01-30 01:14:22 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.4203
2022-01-30 01:14:54 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.2340
2022-01-30 01:15:26 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.5127
2022-01-30 01:15:57 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.3247
2022-01-30 01:16:29 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.3957
2022-01-30 01:17:01 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.4284
2022-01-30 01:17:32 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.3260
2022-01-30 01:18:04 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.2426
2022-01-30 01:18:35 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.3347
2022-01-30 01:19:07 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.2922
2022-01-30 01:19:39 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.1857
2022-01-30 01:20:10 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.4455
2022-01-30 01:20:42 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.3562
2022-01-30 01:21:13 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.3560
2022-01-30 01:21:46 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.2087
2022-01-30 01:22:17 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.1982
2022-01-30 01:22:49 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.3748
2022-01-30 01:23:20 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.5669
2022-01-30 01:23:54 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.5516
2022-01-30 01:24:26 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.1924
2022-01-30 01:24:58 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.4085
2022-01-30 01:24:59 - train: epoch 054, train_loss: 1.3426
2022-01-30 01:26:12 - eval: epoch: 054, acc1: 70.088%, acc5: 89.822%, test_loss: 1.2035, per_image_load_time: 2.478ms, per_image_inference_time: 0.368ms
2022-01-30 01:26:13 - epoch 055 lr: 0.010000000000000002
2022-01-30 01:26:50 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.3626
2022-01-30 01:27:22 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.1455
2022-01-30 01:27:54 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.0765
2022-01-30 01:28:26 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.2467
2022-01-30 01:28:57 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.1179
2022-01-30 01:29:29 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.2799
2022-01-30 01:30:01 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.3810
2022-01-30 01:30:32 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.1617
2022-01-30 01:31:04 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.2823
2022-01-30 01:31:35 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.3162
2022-01-30 01:32:06 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.2907
2022-01-30 01:32:37 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.3598
2022-01-30 01:33:09 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.3942
2022-01-30 01:33:39 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.1948
2022-01-30 01:34:10 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.2948
2022-01-30 01:34:43 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.4357
2022-01-30 01:35:15 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.3627
2022-01-30 01:35:46 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.3824
2022-01-30 01:36:18 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.3740
2022-01-30 01:36:50 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.2912
2022-01-30 01:37:22 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.1075
2022-01-30 01:37:53 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.4667
2022-01-30 01:38:25 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.2238
2022-01-30 01:38:56 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.1671
2022-01-30 01:39:28 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.3089
2022-01-30 01:40:00 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.2462
2022-01-30 01:40:31 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.1667
2022-01-30 01:41:02 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.3008
2022-01-30 01:41:34 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.4686
2022-01-30 01:42:05 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.2571
2022-01-30 01:42:37 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.4459
2022-01-30 01:43:08 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.2889
2022-01-30 01:43:40 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.1166
2022-01-30 01:44:12 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.3049
2022-01-30 01:44:44 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.1596
2022-01-30 01:45:15 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.4075
2022-01-30 01:45:47 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.3459
2022-01-30 01:46:18 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.4397
2022-01-30 01:46:50 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.6818
2022-01-30 01:47:22 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.3553
2022-01-30 01:47:54 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.3284
2022-01-30 01:48:26 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.3499
2022-01-30 01:48:58 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.4767
2022-01-30 01:49:29 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.6000
2022-01-30 01:50:01 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.3555
2022-01-30 01:50:33 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.4421
2022-01-30 01:51:05 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.1563
2022-01-30 01:51:37 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.4047
2022-01-30 01:52:11 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.3048
2022-01-30 01:52:42 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.2982
2022-01-30 01:52:43 - train: epoch 055, train_loss: 1.3364
2022-01-30 01:53:56 - eval: epoch: 055, acc1: 70.374%, acc5: 90.024%, test_loss: 1.1852, per_image_load_time: 2.462ms, per_image_inference_time: 0.367ms
2022-01-30 01:53:57 - epoch 056 lr: 0.010000000000000002
2022-01-30 01:54:33 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.4105
2022-01-30 01:55:05 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.5147
2022-01-30 01:55:37 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.2840
2022-01-30 01:56:09 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.3190
2022-01-30 01:56:41 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.2822
2022-01-30 01:57:12 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.3747
2022-01-30 01:57:44 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.3381
2022-01-30 01:58:15 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.5134
2022-01-30 01:58:47 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.4246
2022-01-30 01:59:19 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.3707
2022-01-30 01:59:50 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.2354
2022-01-30 02:00:21 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.2838
2022-01-30 02:00:52 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.4168
2022-01-30 02:01:24 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.3018
2022-01-30 02:01:56 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.5295
2022-01-30 02:02:27 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.0869
2022-01-30 02:02:59 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.4291
2022-01-30 02:03:31 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.5470
2022-01-30 02:04:03 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.1950
2022-01-30 02:04:34 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.2793
2022-01-30 02:05:06 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.3476
2022-01-30 02:05:37 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.4626
2022-01-30 02:06:08 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.4553
2022-01-30 02:06:40 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.3881
2022-01-30 02:07:12 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.5428
2022-01-30 02:07:44 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.2731
2022-01-30 02:08:15 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.2963
2022-01-30 02:08:47 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.2488
2022-01-30 02:09:19 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.5184
2022-01-30 02:09:51 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.4578
2022-01-30 02:10:22 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.1766
2022-01-30 02:10:54 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.2485
2022-01-30 02:11:25 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.6057
2022-01-30 02:11:57 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.2164
2022-01-30 02:12:28 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.3660
2022-01-30 02:12:59 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.1667
2022-01-30 02:13:30 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.2142
2022-01-30 02:14:02 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.2062
2022-01-30 02:14:33 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.5770
2022-01-30 02:15:05 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.3004
2022-01-30 02:15:36 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.5443
2022-01-30 02:16:06 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.3433
2022-01-30 02:16:38 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.2227
2022-01-30 02:17:09 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.4234
2022-01-30 02:17:41 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.3392
2022-01-30 02:18:12 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.4068
2022-01-30 02:18:43 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.4311
2022-01-30 02:19:15 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.4790
2022-01-30 02:19:47 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.3596
2022-01-30 02:20:19 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.3475
2022-01-30 02:20:20 - train: epoch 056, train_loss: 1.3329
2022-01-30 02:21:32 - eval: epoch: 056, acc1: 69.940%, acc5: 89.890%, test_loss: 1.2027, per_image_load_time: 2.418ms, per_image_inference_time: 0.368ms
2022-01-30 02:21:33 - epoch 057 lr: 0.010000000000000002
2022-01-30 02:22:09 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.4153
2022-01-30 02:22:40 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.2425
2022-01-30 02:23:12 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.1876
2022-01-30 02:23:43 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.2843
2022-01-30 02:24:14 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.1129
2022-01-30 02:24:45 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.4522
2022-01-30 02:25:14 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.1520
2022-01-30 02:25:45 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.3486
2022-01-30 02:26:17 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.3935
2022-01-30 02:26:48 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.2652
2022-01-30 02:27:19 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.2079
2022-01-30 02:27:51 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.3487
2022-01-30 02:28:21 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.3255
2022-01-30 02:28:53 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.4089
2022-01-30 02:29:24 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.4209
2022-01-30 02:29:55 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.4424
2022-01-30 02:30:26 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.5319
2022-01-30 02:30:57 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.4538
2022-01-30 02:31:28 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.2854
2022-01-30 02:31:59 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.2650
2022-01-30 02:32:30 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.3151
2022-01-30 02:33:01 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.2845
2022-01-30 02:33:32 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.3237
2022-01-30 02:34:03 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.2048
2022-01-30 02:34:34 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.3356
2022-01-30 02:35:06 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.2299
2022-01-30 02:35:37 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.0805
2022-01-30 02:36:07 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.0884
2022-01-30 02:36:39 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.4265
2022-01-30 02:37:10 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.4429
2022-01-30 02:37:41 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.6103
2022-01-30 02:38:12 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.4352
2022-01-30 02:38:44 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.3328
2022-01-30 02:39:15 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.3257
2022-01-30 02:39:47 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.3997
2022-01-30 02:40:18 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.2687
2022-01-30 02:40:49 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.3087
2022-01-30 02:41:20 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.2690
2022-01-30 02:41:51 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.4940
2022-01-30 02:42:23 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.3328
2022-01-30 02:42:54 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.3784
2022-01-30 02:43:25 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.3726
2022-01-30 02:43:57 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.3058
2022-01-30 02:44:28 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.3054
2022-01-30 02:44:59 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.4397
2022-01-30 02:45:30 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.4838
2022-01-30 02:46:02 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.2399
2022-01-30 02:46:34 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.5119
2022-01-30 02:47:05 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.5601
2022-01-30 02:47:36 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.4928
2022-01-30 02:47:37 - train: epoch 057, train_loss: 1.3322
2022-01-30 02:48:49 - eval: epoch: 057, acc1: 69.538%, acc5: 89.558%, test_loss: 1.2258, per_image_load_time: 2.415ms, per_image_inference_time: 0.368ms
2022-01-30 02:48:50 - epoch 058 lr: 0.010000000000000002
2022-01-30 02:49:25 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.2822
2022-01-30 02:49:57 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.1500
2022-01-30 02:50:28 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.3967
2022-01-30 02:50:59 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.2908
2022-01-30 02:51:31 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.2120
2022-01-30 02:52:02 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.5381
2022-01-30 02:52:33 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.3343
2022-01-30 02:53:04 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.1772
2022-01-30 02:53:35 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.1745
2022-01-30 02:54:06 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.3746
2022-01-30 02:54:37 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.1057
2022-01-30 02:55:09 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.2408
2022-01-30 02:55:40 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.3980
2022-01-30 02:56:11 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.2997
2022-01-30 02:56:42 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.3490
2022-01-30 02:57:13 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.2946
2022-01-30 02:57:45 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.5271
2022-01-30 02:58:16 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.3519
2022-01-30 02:58:47 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.5011
2022-01-30 02:59:18 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.3750
2022-01-30 02:59:50 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.2302
2022-01-30 03:00:21 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.1667
2022-01-30 03:00:52 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.2986
2022-01-30 03:01:23 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.3290
2022-01-30 03:01:54 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.4222
2022-01-30 03:02:25 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.2099
2022-01-30 03:02:56 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.4841
2022-01-30 03:03:27 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.2105
2022-01-30 03:03:59 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.2879
2022-01-30 03:04:30 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.4110
2022-01-30 03:05:01 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.2001
2022-01-30 03:05:32 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.2250
2022-01-30 03:06:03 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.2702
2022-01-30 03:06:35 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.2489
2022-01-30 03:07:06 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.2898
2022-01-30 03:07:36 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.2405
2022-01-30 03:08:07 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.2998
2022-01-30 03:08:39 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.4179
2022-01-30 03:09:10 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.3444
2022-01-30 03:09:41 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.5363
2022-01-30 03:10:13 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.4195
2022-01-30 03:10:43 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.1199
2022-01-30 03:11:15 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.5451
2022-01-30 03:11:47 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.1662
2022-01-30 03:12:18 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.4126
2022-01-30 03:12:51 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.2085
2022-01-30 03:13:22 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.4033
2022-01-30 03:13:54 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.4074
2022-01-30 03:14:25 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.3035
2022-01-30 03:14:57 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.2664
2022-01-30 03:14:58 - train: epoch 058, train_loss: 1.3278
2022-01-30 03:16:09 - eval: epoch: 058, acc1: 70.592%, acc5: 90.154%, test_loss: 1.1791, per_image_load_time: 2.347ms, per_image_inference_time: 0.375ms
2022-01-30 03:16:10 - epoch 059 lr: 0.010000000000000002
2022-01-30 03:16:46 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.4690
2022-01-30 03:17:17 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.2725
2022-01-30 03:17:48 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.3471
2022-01-30 03:18:21 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.3339
2022-01-30 03:18:54 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.6089
2022-01-30 03:19:26 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.2596
2022-01-30 03:19:58 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.2103
2022-01-30 03:20:31 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.3131
2022-01-30 03:21:03 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.2968
2022-01-30 03:21:36 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.3156
2022-01-30 03:22:08 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 1.4181
2022-01-30 03:22:41 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.1574
2022-01-30 03:23:12 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.3610
2022-01-30 03:23:45 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.4648
2022-01-30 03:24:18 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.4243
2022-01-30 03:24:50 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.2425
2022-01-30 03:25:22 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.4027
2022-01-30 03:25:55 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.2725
2022-01-30 03:26:27 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.2952
2022-01-30 03:26:59 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.3314
2022-01-30 03:27:31 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.4454
2022-01-30 03:28:03 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.5369
2022-01-30 03:28:36 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.2334
2022-01-30 03:29:07 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.3989
2022-01-30 03:29:40 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.3181
2022-01-30 03:30:12 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.2801
2022-01-30 03:30:44 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.2243
2022-01-30 03:31:17 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.6357
2022-01-30 03:31:49 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.2226
2022-01-30 03:32:21 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 1.6956
2022-01-30 03:32:53 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.2130
2022-01-30 03:33:26 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.3127
2022-01-30 03:33:57 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.3855
2022-01-30 03:34:30 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 1.7196
2022-01-30 03:35:02 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.1114
2022-01-30 03:35:35 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.2812
2022-01-30 03:36:08 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.0989
2022-01-30 03:36:40 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.3903
2022-01-30 03:37:13 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.2600
2022-01-30 03:37:45 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 1.6086
2022-01-30 03:38:18 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.4725
2022-01-30 03:38:50 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.4496
2022-01-30 03:39:22 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.5723
2022-01-30 03:39:55 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.5823
2022-01-30 03:40:28 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.3911
2022-01-30 03:41:01 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.3567
2022-01-30 03:41:33 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.2931
2022-01-30 03:42:06 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.1962
2022-01-30 03:42:38 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.4893
2022-01-30 03:43:10 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.4312
2022-01-30 03:43:11 - train: epoch 059, train_loss: 1.3267
2022-01-30 03:44:27 - eval: epoch: 059, acc1: 69.930%, acc5: 89.852%, test_loss: 1.2058, per_image_load_time: 2.581ms, per_image_inference_time: 0.359ms
2022-01-30 03:44:28 - epoch 060 lr: 0.010000000000000002
2022-01-30 03:45:06 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.1626
2022-01-30 03:45:39 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.3930
2022-01-30 03:46:11 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.2479
2022-01-30 03:46:43 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.3785
2022-01-30 03:47:16 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.4704
2022-01-30 03:47:49 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.4130
2022-01-30 03:48:21 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.3330
2022-01-30 03:48:54 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.4728
2022-01-30 03:49:27 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.0735
2022-01-30 03:50:00 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.0411
2022-01-30 03:50:32 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.1015
2022-01-30 03:51:05 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.2965
2022-01-30 03:51:37 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.2542
2022-01-30 03:52:09 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.3164
2022-01-30 03:52:41 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.2466
2022-01-30 03:53:14 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.3754
2022-01-30 03:53:46 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.2633
2022-01-30 03:54:18 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.3410
2022-01-30 03:54:51 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.5295
2022-01-30 03:55:24 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.2629
2022-01-30 03:55:56 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.3717
2022-01-30 03:56:29 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.4268
2022-01-30 03:57:01 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.1332
2022-01-30 03:57:34 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.2585
2022-01-30 03:58:06 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.3498
2022-01-30 03:58:39 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.3947
2022-01-30 03:59:12 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.3580
2022-01-30 03:59:44 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.2427
2022-01-30 04:00:17 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.3714
2022-01-30 04:00:49 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.5675
2022-01-30 04:01:22 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.4700
2022-01-30 04:01:55 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.2965
2022-01-30 04:02:27 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.1642
2022-01-30 04:03:00 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.2674
2022-01-30 04:03:33 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.3950
2022-01-30 04:04:05 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.2526
2022-01-30 04:04:38 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.3733
2022-01-30 04:05:11 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.3543
2022-01-30 04:05:44 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.6737
2022-01-30 04:06:17 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.3371
2022-01-30 04:06:49 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.5617
2022-01-30 04:07:22 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.4655
2022-01-30 04:07:55 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.3532
2022-01-30 04:08:28 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.3972
2022-01-30 04:09:00 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.3469
2022-01-30 04:09:34 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.2853
2022-01-30 04:10:06 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.3590
2022-01-30 04:10:40 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.2132
2022-01-30 04:11:12 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.3019
2022-01-30 04:11:44 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.3949
2022-01-30 04:11:45 - train: epoch 060, train_loss: 1.3229
2022-01-30 04:13:00 - eval: epoch: 060, acc1: 69.908%, acc5: 89.920%, test_loss: 1.2018, per_image_load_time: 2.545ms, per_image_inference_time: 0.371ms
2022-01-30 04:13:01 - epoch 061 lr: 0.0010000000000000002
2022-01-30 04:13:39 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.1691
2022-01-30 04:14:12 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.2167
2022-01-30 04:14:44 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 0.9968
2022-01-30 04:15:16 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.4272
2022-01-30 04:15:48 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.1656
2022-01-30 04:16:19 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.2217
2022-01-30 04:16:51 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 0.9446
2022-01-30 04:17:22 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.2134
2022-01-30 04:17:54 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.0718
2022-01-30 04:18:26 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 0.9602
2022-01-30 04:19:00 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.0611
2022-01-30 04:19:32 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.0741
2022-01-30 04:20:05 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.0608
2022-01-30 04:20:37 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.0189
2022-01-30 04:21:09 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.1731
2022-01-30 04:21:41 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.0027
2022-01-30 04:22:14 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.0996
2022-01-30 04:22:46 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.1205
2022-01-30 04:23:18 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.2407
2022-01-30 04:23:51 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.0201
2022-01-30 04:24:22 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.4337
2022-01-30 04:24:55 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.0100
2022-01-30 04:25:26 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.0998
2022-01-30 04:25:59 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 0.8785
2022-01-30 04:26:31 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.0140
2022-01-30 04:27:03 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.2832
2022-01-30 04:27:36 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.2748
2022-01-30 04:28:08 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.1336
2022-01-30 04:28:40 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.2477
2022-01-30 04:29:12 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.0482
2022-01-30 04:29:45 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.2338
2022-01-30 04:30:17 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.1794
2022-01-30 04:30:49 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.1063
2022-01-30 04:31:21 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 0.9011
2022-01-30 04:31:54 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.0899
2022-01-30 04:32:26 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.1002
2022-01-30 04:32:58 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.2307
2022-01-30 04:33:30 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.0490
2022-01-30 04:34:02 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.0981
2022-01-30 04:34:35 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.1289
2022-01-30 04:35:07 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.2757
2022-01-30 04:35:40 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.0293
2022-01-30 04:36:11 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.2446
2022-01-30 04:36:44 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.0795
2022-01-30 04:37:16 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.0136
2022-01-30 04:37:49 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.1921
2022-01-30 04:38:21 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.0145
2022-01-30 04:38:54 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.0705
2022-01-30 04:39:26 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.1783
2022-01-30 04:39:59 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.1554
2022-01-30 04:40:00 - train: epoch 061, train_loss: 1.1167
2022-01-30 04:41:14 - eval: epoch: 061, acc1: 74.544%, acc5: 92.218%, test_loss: 1.0034, per_image_load_time: 2.500ms, per_image_inference_time: 0.368ms
2022-01-30 04:41:15 - epoch 062 lr: 0.0010000000000000002
2022-01-30 04:41:52 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.1713
2022-01-30 04:42:24 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.1908
2022-01-30 04:42:57 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.0953
2022-01-30 04:43:29 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 0.8971
2022-01-30 04:44:02 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 0.9716
2022-01-30 04:44:34 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.0368
2022-01-30 04:45:06 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.2666
2022-01-30 04:45:38 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.0938
2022-01-30 04:46:10 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 0.9800
2022-01-30 04:46:43 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.2099
2022-01-30 04:47:15 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 0.9724
2022-01-30 04:47:48 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.2577
2022-01-30 04:48:19 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.1438
2022-01-30 04:48:52 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.0835
2022-01-30 04:49:24 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.1008
2022-01-30 04:49:57 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.1946
2022-01-30 04:50:29 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.2140
2022-01-30 04:51:01 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.0065
2022-01-30 04:51:33 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.0225
2022-01-30 04:52:06 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.0444
2022-01-30 04:52:38 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.1271
2022-01-30 04:53:11 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 0.9554
2022-01-30 04:53:43 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.1086
2022-01-30 04:54:15 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 0.9403
2022-01-30 04:54:48 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.1586
2022-01-30 04:55:20 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 0.9747
2022-01-30 04:55:52 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.0223
2022-01-30 04:56:24 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.1623
2022-01-30 04:56:57 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.1588
2022-01-30 04:57:29 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.1355
2022-01-30 04:58:02 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.1481
2022-01-30 04:58:33 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 0.9180
2022-01-30 04:59:06 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.2388
2022-01-30 04:59:38 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.1868
2022-01-30 05:00:10 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.1308
2022-01-30 05:00:42 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.1662
2022-01-30 05:01:14 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.0678
2022-01-30 05:01:46 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.0407
2022-01-30 05:02:19 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 0.9954
2022-01-30 05:02:50 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 0.8577
2022-01-30 05:03:23 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.0760
2022-01-30 05:03:55 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 0.9096
2022-01-30 05:04:28 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.0323
2022-01-30 05:05:00 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 0.9490
2022-01-30 05:05:32 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 0.9141
2022-01-30 05:06:05 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 0.8870
2022-01-30 05:06:38 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 0.9885
2022-01-30 05:07:11 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 0.9613
2022-01-30 05:07:43 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 0.9805
2022-01-30 05:08:15 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.0496
2022-01-30 05:08:17 - train: epoch 062, train_loss: 1.0620
2022-01-30 05:09:30 - eval: epoch: 062, acc1: 74.896%, acc5: 92.400%, test_loss: 0.9876, per_image_load_time: 2.457ms, per_image_inference_time: 0.380ms
2022-01-30 05:09:31 - epoch 063 lr: 0.0010000000000000002
2022-01-30 05:10:07 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 0.9865
2022-01-30 05:10:39 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 0.9872
2022-01-30 05:11:11 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.1736
2022-01-30 05:11:42 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.0834
2022-01-30 05:12:14 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 0.9380
2022-01-30 05:12:46 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.0764
2022-01-30 05:13:19 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.0378
2022-01-30 05:13:51 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.0722
2022-01-30 05:14:24 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.0320
2022-01-30 05:14:57 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.0720
2022-01-30 05:15:29 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 0.9003
2022-01-30 05:16:02 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 0.9357
2022-01-30 05:16:35 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.0993
2022-01-30 05:17:07 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.2752
2022-01-30 05:17:39 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.1279
2022-01-30 05:18:12 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.0207
2022-01-30 05:18:44 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 0.9149
2022-01-30 05:19:17 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.1622
2022-01-30 05:19:49 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.1385
2022-01-30 05:20:22 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 0.9459
2022-01-30 05:20:54 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 0.9278
2022-01-30 05:21:27 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.2727
2022-01-30 05:21:59 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.1106
2022-01-30 05:22:32 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.1415
2022-01-30 05:23:05 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.0010
2022-01-30 05:23:38 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 0.9576
2022-01-30 05:24:10 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.2202
2022-01-30 05:24:43 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.0228
2022-01-30 05:25:15 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.0572
2022-01-30 05:25:48 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.0774
2022-01-30 05:26:20 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.3342
2022-01-30 05:26:53 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.1324
2022-01-30 05:27:26 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.0302
2022-01-30 05:27:58 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 0.8970
2022-01-30 05:28:31 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.1409
2022-01-30 05:29:03 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 0.9584
2022-01-30 05:29:36 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.1868
2022-01-30 05:30:09 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.2633
2022-01-30 05:30:41 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 0.8143
2022-01-30 05:31:14 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 0.8043
2022-01-30 05:31:47 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.1669
2022-01-30 05:32:19 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 0.9911
2022-01-30 05:32:52 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.0737
2022-01-30 05:33:24 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.0641
2022-01-30 05:33:57 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.0071
2022-01-30 05:34:28 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 0.9651
2022-01-30 05:35:01 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 0.9517
2022-01-30 05:35:33 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.0175
2022-01-30 05:36:04 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.0281
2022-01-30 05:36:36 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 0.9881
2022-01-30 05:36:38 - train: epoch 063, train_loss: 1.0419
2022-01-30 05:37:52 - eval: epoch: 063, acc1: 75.260%, acc5: 92.548%, test_loss: 0.9802, per_image_load_time: 2.494ms, per_image_inference_time: 0.377ms
2022-01-30 05:37:53 - epoch 064 lr: 0.0010000000000000002
2022-01-30 05:38:30 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.0270
2022-01-30 05:39:01 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.0967
2022-01-30 05:39:33 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 0.8468
2022-01-30 05:40:05 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.1783
2022-01-30 05:40:37 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 0.9567
2022-01-30 05:41:10 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.1512
2022-01-30 05:41:41 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.1257
2022-01-30 05:42:13 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 0.9406
2022-01-30 05:42:45 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.0354
2022-01-30 05:43:17 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.0588
2022-01-30 05:43:49 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 0.9731
2022-01-30 05:44:21 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 0.9009
2022-01-30 05:44:53 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 0.9903
2022-01-30 05:45:25 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.2493
2022-01-30 05:45:57 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.0609
2022-01-30 05:46:30 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 0.9568
2022-01-30 05:47:01 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 0.9132
2022-01-30 05:47:34 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 0.9750
2022-01-30 05:48:05 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 0.9322
2022-01-30 05:48:38 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 0.9873
2022-01-30 05:49:10 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 0.9955
2022-01-30 05:49:42 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.1027
2022-01-30 05:50:14 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.1379
2022-01-30 05:50:46 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.0049
2022-01-30 05:51:18 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 0.9267
2022-01-30 05:51:50 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 0.9089
2022-01-30 05:52:22 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 0.9931
2022-01-30 05:52:54 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 0.9533
2022-01-30 05:53:25 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.2593
2022-01-30 05:53:58 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.1171
2022-01-30 05:54:29 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 0.9083
2022-01-30 05:55:01 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 0.9861
2022-01-30 05:55:33 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.0028
2022-01-30 05:56:06 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.1861
2022-01-30 05:56:38 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 0.9299
2022-01-30 05:57:10 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 0.9053
2022-01-30 05:57:43 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 0.7918
2022-01-30 05:58:15 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.1555
2022-01-30 05:58:46 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 0.9137
2022-01-30 05:59:18 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 0.8729
2022-01-30 05:59:50 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.0879
2022-01-30 06:00:22 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 0.9479
2022-01-30 06:00:54 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.2175
2022-01-30 06:01:26 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.0483
2022-01-30 06:01:58 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 0.9159
2022-01-30 06:02:30 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.2425
2022-01-30 06:03:02 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.3380
2022-01-30 06:03:35 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.0373
2022-01-30 06:04:07 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.2464
2022-01-30 06:04:38 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 0.8903
2022-01-30 06:04:39 - train: epoch 064, train_loss: 1.0260
2022-01-30 06:05:53 - eval: epoch: 064, acc1: 75.280%, acc5: 92.622%, test_loss: 0.9750, per_image_load_time: 2.497ms, per_image_inference_time: 0.359ms
2022-01-30 06:05:54 - epoch 065 lr: 0.0010000000000000002
2022-01-30 06:06:31 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.0597
2022-01-30 06:07:02 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.0067
2022-01-30 06:07:33 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.0130
2022-01-30 06:08:04 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.0939
2022-01-30 06:08:35 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 0.9175
2022-01-30 06:09:07 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.0362
2022-01-30 06:09:40 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 0.9221
2022-01-30 06:10:12 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.0210
2022-01-30 06:10:45 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 0.9764
2022-01-30 06:11:16 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 0.9647
2022-01-30 06:11:48 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 0.9868
2022-01-30 06:12:20 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.2224
2022-01-30 06:12:53 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 0.9945
2022-01-30 06:13:24 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 0.8175
2022-01-30 06:13:56 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.0504
2022-01-30 06:14:29 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.0503
2022-01-30 06:15:01 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 0.9852
2022-01-30 06:15:33 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 0.9564
2022-01-30 06:16:05 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 0.9228
2022-01-30 06:16:37 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 0.9694
2022-01-30 06:17:09 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 0.9605
2022-01-30 06:17:40 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.1982
2022-01-30 06:18:12 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 0.9436
2022-01-30 06:18:44 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 0.9693
2022-01-30 06:19:16 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.0468
2022-01-30 06:19:48 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.1085
2022-01-30 06:20:19 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.0334
2022-01-30 06:20:51 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 0.8747
2022-01-30 06:21:22 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.0947
2022-01-30 06:21:54 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 0.9294
2022-01-30 06:22:25 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.0364
2022-01-30 06:22:58 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.0699
2022-01-30 06:23:30 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 0.9745
2022-01-30 06:24:02 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 0.9035
2022-01-30 06:24:34 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.2512
2022-01-30 06:25:05 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.0956
2022-01-30 06:25:37 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 0.8412
2022-01-30 06:26:08 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 0.8516
2022-01-30 06:26:40 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.0838
2022-01-30 06:27:12 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.0983
2022-01-30 06:27:43 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 0.9177
2022-01-30 06:28:14 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.0518
2022-01-30 06:28:46 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 0.9490
2022-01-30 06:29:18 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 0.9507
2022-01-30 06:29:50 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 0.9921
2022-01-30 06:30:21 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.0697
2022-01-30 06:30:54 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.0444
2022-01-30 06:31:27 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 0.7589
2022-01-30 06:31:59 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 0.9024
2022-01-30 06:32:30 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.0666
2022-01-30 06:32:31 - train: epoch 065, train_loss: 1.0134
2022-01-30 06:33:43 - eval: epoch: 065, acc1: 75.318%, acc5: 92.586%, test_loss: 0.9778, per_image_load_time: 2.438ms, per_image_inference_time: 0.366ms
2022-01-30 06:33:44 - epoch 066 lr: 0.0010000000000000002
2022-01-30 06:34:21 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 0.9596
2022-01-30 06:34:53 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.1053
2022-01-30 06:35:24 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 0.8134
2022-01-30 06:35:56 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 0.8220
2022-01-30 06:36:27 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.0316
2022-01-30 06:36:58 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 0.9811
2022-01-30 06:37:30 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 0.9542
2022-01-30 06:38:01 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.2427
2022-01-30 06:38:33 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.1454
2022-01-30 06:39:06 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.0576
2022-01-30 06:39:37 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.0468
2022-01-30 06:40:10 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.1278
2022-01-30 06:40:42 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.0506
2022-01-30 06:41:14 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 0.8134
2022-01-30 06:41:46 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.0303
2022-01-30 06:42:18 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 0.9763
2022-01-30 06:42:51 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 0.9468
2022-01-30 06:43:23 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 0.8834
2022-01-30 06:43:55 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.1381
2022-01-30 06:44:27 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.1451
2022-01-30 06:44:59 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.0206
2022-01-30 06:45:31 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 0.9669
2022-01-30 06:46:04 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.1820
2022-01-30 06:46:36 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 0.8794
2022-01-30 06:47:07 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 0.9925
2022-01-30 06:47:40 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 0.8719
2022-01-30 06:48:12 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.1479
2022-01-30 06:48:44 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.1223
2022-01-30 06:49:17 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.0086
2022-01-30 06:49:49 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 0.9542
2022-01-30 06:50:22 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.1404
2022-01-30 06:50:53 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 0.8588
2022-01-30 06:51:25 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 0.8966
2022-01-30 06:51:57 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.2101
2022-01-30 06:52:28 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.0745
2022-01-30 06:53:00 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.1146
2022-01-30 06:53:32 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.0972
2022-01-30 06:54:05 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 0.9006
2022-01-30 06:54:37 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 0.8923
2022-01-30 06:55:08 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.1626
2022-01-30 06:55:41 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 0.8384
2022-01-30 06:56:12 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 0.7658
2022-01-30 06:56:44 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 0.8500
2022-01-30 06:57:16 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 0.9936
2022-01-30 06:57:48 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 0.9986
2022-01-30 06:58:21 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.1786
2022-01-30 06:58:53 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 0.9161
2022-01-30 06:59:26 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 0.9558
2022-01-30 06:59:59 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.0044
2022-01-30 07:00:30 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 0.9906
2022-01-30 07:00:31 - train: epoch 066, train_loss: 1.0040
2022-01-30 07:01:43 - eval: epoch: 066, acc1: 75.370%, acc5: 92.648%, test_loss: 0.9707, per_image_load_time: 0.990ms, per_image_inference_time: 0.421ms
2022-01-30 07:01:44 - epoch 067 lr: 0.0010000000000000002
2022-01-30 07:02:21 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 0.8538
2022-01-30 07:02:54 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 0.9170
2022-01-30 07:03:26 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.3444
2022-01-30 07:03:58 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.0746
2022-01-30 07:04:30 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 0.9874
2022-01-30 07:05:03 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 0.9434
2022-01-30 07:05:34 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.1427
2022-01-30 07:06:06 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.0722
2022-01-30 07:06:38 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.0497
2022-01-30 07:07:10 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 0.8520
2022-01-30 07:07:42 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 0.9865
2022-01-30 07:08:14 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 0.9972
2022-01-30 07:08:46 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.0789
2022-01-30 07:09:18 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.0253
2022-01-30 07:09:51 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.0540
2022-01-30 07:10:22 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.0780
2022-01-30 07:10:55 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 0.8941
2022-01-30 07:11:27 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.1715
2022-01-30 07:11:59 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 0.8404
2022-01-30 07:12:31 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.1692
2022-01-30 07:13:03 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 0.8951
2022-01-30 07:13:36 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.0369
2022-01-30 07:14:08 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 0.9529
2022-01-30 07:14:40 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 0.9449
2022-01-30 07:15:12 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 0.9434
2022-01-30 07:15:44 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 0.8960
2022-01-30 07:16:16 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 0.9120
2022-01-30 07:16:49 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.0628
2022-01-30 07:17:20 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 0.9576
2022-01-30 07:17:53 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.0450
2022-01-30 07:18:25 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 0.8020
2022-01-30 07:18:57 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.2174
2022-01-30 07:19:30 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 0.9105
2022-01-30 07:20:01 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.0060
2022-01-30 07:20:33 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 0.8999
2022-01-30 07:21:06 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 0.8976
2022-01-30 07:21:38 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.1158
2022-01-30 07:22:09 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.0434
2022-01-30 07:22:41 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 0.9779
2022-01-30 07:23:12 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.1916
2022-01-30 07:23:43 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.1847
2022-01-30 07:24:15 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.1877
2022-01-30 07:24:46 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 0.8630
2022-01-30 07:25:18 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 0.9584
2022-01-30 07:25:50 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 0.8813
2022-01-30 07:26:21 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 0.7509
2022-01-30 07:26:54 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 0.9115
2022-01-30 07:27:25 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 0.8015
2022-01-30 07:27:57 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 0.9924
2022-01-30 07:28:28 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.0157
2022-01-30 07:28:30 - train: epoch 067, train_loss: 0.9954
2022-01-30 07:29:41 - eval: epoch: 067, acc1: 75.474%, acc5: 92.680%, test_loss: 0.9700, per_image_load_time: 2.331ms, per_image_inference_time: 0.405ms
2022-01-30 07:29:42 - epoch 068 lr: 0.0010000000000000002
2022-01-30 07:30:19 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.0487
2022-01-30 07:30:51 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.0325
2022-01-30 07:31:22 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.0843
2022-01-30 07:31:54 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.0152
2022-01-30 07:32:25 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.0063
2022-01-30 07:32:57 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.0244
2022-01-30 07:33:28 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.2641
2022-01-30 07:33:59 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 0.9837
2022-01-30 07:34:31 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 0.9325
2022-01-30 07:35:02 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 0.9823
2022-01-30 07:35:34 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.1379
2022-01-30 07:36:05 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 0.8987
2022-01-30 07:36:36 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 0.8088
2022-01-30 07:37:08 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.0578
2022-01-30 07:37:40 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.1108
2022-01-30 07:38:11 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.0692
2022-01-30 07:38:43 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.2081
2022-01-30 07:39:14 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 0.9918
2022-01-30 07:39:46 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.0043
2022-01-30 07:40:18 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.0004
2022-01-30 07:40:49 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.0097
2022-01-30 07:41:20 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.0548
2022-01-30 07:41:52 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 0.8169
2022-01-30 07:42:24 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.0908
2022-01-30 07:42:55 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 0.9925
2022-01-30 07:43:28 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.0003
2022-01-30 07:43:58 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.0219
2022-01-30 07:44:30 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.1418
2022-01-30 07:45:01 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.0837
2022-01-30 07:45:33 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.1412
2022-01-30 07:46:04 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 0.8837
2022-01-30 07:46:36 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.0542
2022-01-30 07:47:07 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.0067
2022-01-30 07:47:39 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 0.8680
2022-01-30 07:48:11 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.0334
2022-01-30 07:48:42 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 0.9259
2022-01-30 07:49:14 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.0586
2022-01-30 07:49:45 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.0286
2022-01-30 07:50:16 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.0063
2022-01-30 07:50:48 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 0.9982
2022-01-30 07:51:19 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 0.8682
2022-01-30 07:51:50 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.1507
2022-01-30 07:52:20 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 1.1880
2022-01-30 07:52:52 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 0.9595
2022-01-30 07:53:23 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.0238
2022-01-30 07:53:56 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.0887
2022-01-30 07:54:28 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.2302
2022-01-30 07:55:00 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.0764
2022-01-30 07:55:32 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.0797
2022-01-30 07:56:04 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 0.9460
2022-01-30 07:56:05 - train: epoch 068, train_loss: 0.9888
2022-01-30 07:57:18 - eval: epoch: 068, acc1: 75.478%, acc5: 92.718%, test_loss: 0.9657, per_image_load_time: 2.470ms, per_image_inference_time: 0.377ms
2022-01-30 07:57:19 - epoch 069 lr: 0.0010000000000000002
2022-01-30 07:57:56 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.1609
2022-01-30 07:58:27 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.1433
2022-01-30 07:58:59 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.1302
2022-01-30 07:59:30 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 0.9614
2022-01-30 08:00:02 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 0.8765
2022-01-30 08:00:33 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 0.8789
2022-01-30 08:01:04 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 0.9723
2022-01-30 08:01:36 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 0.9748
2022-01-30 08:02:08 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.0212
2022-01-30 08:02:39 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 0.9911
2022-01-30 08:03:10 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 0.9101
2022-01-30 08:03:42 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.0309
2022-01-30 08:04:14 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.1572
2022-01-30 08:04:46 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.0585
2022-01-30 08:05:17 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 0.9960
2022-01-30 08:05:49 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.0058
2022-01-30 08:06:21 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 0.9183
2022-01-30 08:06:52 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 0.8224
2022-01-30 08:07:24 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 0.8728
2022-01-30 08:07:55 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 0.8147
2022-01-30 08:08:27 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.0108
2022-01-30 08:08:59 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.0138
2022-01-30 08:09:31 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.0449
2022-01-30 08:10:02 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.0269
2022-01-30 08:10:34 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 0.9867
2022-01-30 08:11:06 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.0819
2022-01-30 08:11:38 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.2432
2022-01-30 08:12:10 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.2015
2022-01-30 08:12:42 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 0.8660
2022-01-30 08:13:14 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 0.9334
2022-01-30 08:13:46 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 0.8830
2022-01-30 08:14:19 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 0.9208
2022-01-30 08:14:51 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 0.8708
2022-01-30 08:15:23 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 0.8155
2022-01-30 08:15:55 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 0.9049
2022-01-30 08:16:28 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 0.8935
2022-01-30 08:17:00 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.0937
2022-01-30 08:17:32 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.0383
2022-01-30 08:18:05 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.0740
2022-01-30 08:18:37 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.0787
2022-01-30 08:19:09 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.0819
2022-01-30 08:19:41 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 0.8569
2022-01-30 08:20:13 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.0132
2022-01-30 08:20:44 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.0198
2022-01-30 08:21:16 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 0.9206
2022-01-30 08:21:48 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.1080
2022-01-30 08:22:20 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 0.9951
2022-01-30 08:22:52 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 0.9424
2022-01-30 08:23:24 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 0.8738
2022-01-30 08:23:55 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.0320
2022-01-30 08:23:56 - train: epoch 069, train_loss: 0.9793
2022-01-30 08:25:08 - eval: epoch: 069, acc1: 75.408%, acc5: 92.712%, test_loss: 0.9658, per_image_load_time: 2.409ms, per_image_inference_time: 0.377ms
2022-01-30 08:25:09 - epoch 070 lr: 0.0010000000000000002
2022-01-30 08:25:46 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.0273
2022-01-30 08:26:17 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.0420
2022-01-30 08:26:49 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.1337
2022-01-30 08:27:20 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 0.9389
2022-01-30 08:27:52 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.1291
2022-01-30 08:28:23 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 0.8883
2022-01-30 08:28:55 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.0243
2022-01-30 08:29:26 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 0.9355
2022-01-30 08:29:57 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.0297
2022-01-30 08:30:28 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 0.9732
2022-01-30 08:31:01 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.2352
2022-01-30 08:31:32 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 0.8293
2022-01-30 08:32:04 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 0.9252
2022-01-30 08:32:36 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.0200
2022-01-30 08:33:08 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 0.8817
2022-01-30 08:33:40 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.0121
2022-01-30 08:34:12 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.0867
2022-01-30 08:34:44 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 0.9466
2022-01-30 08:35:16 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 0.9724
2022-01-30 08:35:48 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 0.9451
2022-01-30 08:36:20 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 0.9944
2022-01-30 08:36:53 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.0272
2022-01-30 08:37:24 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 0.9464
2022-01-30 08:37:57 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.0891
2022-01-30 08:38:28 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.0274
2022-01-30 08:39:00 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 0.9288
2022-01-30 08:39:32 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 0.9549
2022-01-30 08:40:04 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.0383
2022-01-30 08:40:36 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.1778
2022-01-30 08:41:08 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 0.9971
2022-01-30 08:41:40 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 0.9501
2022-01-30 08:42:12 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.0853
2022-01-30 08:42:44 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 0.8814
2022-01-30 08:43:17 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.0047
2022-01-30 08:43:48 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 0.9919
2022-01-30 08:44:20 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.0641
2022-01-30 08:44:53 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 0.9804
2022-01-30 08:45:24 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 0.9037
2022-01-30 08:45:55 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 0.8754
2022-01-30 08:46:25 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 0.8346
2022-01-30 08:46:57 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 0.9124
2022-01-30 08:47:30 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.0123
2022-01-30 08:48:02 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 0.9398
2022-01-30 08:48:34 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.0693
2022-01-30 08:49:06 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 0.9648
2022-01-30 08:49:38 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.1209
2022-01-30 08:50:11 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.0407
2022-01-30 08:50:43 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.0230
2022-01-30 08:51:16 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.0513
2022-01-30 08:51:48 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 0.9360
2022-01-30 08:51:49 - train: epoch 070, train_loss: 0.9744
2022-01-30 08:53:03 - eval: epoch: 070, acc1: 75.538%, acc5: 92.656%, test_loss: 0.9645, per_image_load_time: 2.515ms, per_image_inference_time: 0.370ms
2022-01-30 08:53:04 - epoch 071 lr: 0.0010000000000000002
2022-01-30 08:53:41 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 0.8620
2022-01-30 08:54:13 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.0078
2022-01-30 08:54:45 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 0.8252
2022-01-30 08:55:18 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 0.9799
2022-01-30 08:55:49 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 0.9393
2022-01-30 08:56:21 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.0596
2022-01-30 08:56:53 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.1139
2022-01-30 08:57:25 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 0.9563
2022-01-30 08:57:57 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.0203
2022-01-30 08:58:28 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.1119
2022-01-30 08:59:01 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.0458
2022-01-30 08:59:32 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 0.9083
2022-01-30 09:00:04 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 0.9344
2022-01-30 09:00:36 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 0.9454
2022-01-30 09:01:08 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 0.8377
2022-01-30 09:01:40 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 0.9679
2022-01-30 09:02:12 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 0.9629
2022-01-30 09:02:43 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 0.9552
2022-01-30 09:03:15 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 0.8973
2022-01-30 09:03:47 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.0917
2022-01-30 09:04:19 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 0.7936
2022-01-30 09:04:51 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 0.8615
2022-01-30 09:05:23 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 0.8433
2022-01-30 09:05:55 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 0.9045
2022-01-30 09:06:27 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.0436
2022-01-30 09:06:59 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 0.8665
2022-01-30 09:07:31 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 0.9860
2022-01-30 09:08:02 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.0613
2022-01-30 09:08:35 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 0.9882
2022-01-30 09:09:06 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 0.9761
2022-01-30 09:09:38 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 0.9628
2022-01-30 09:10:10 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 0.8646
2022-01-30 09:10:42 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 0.8724
2022-01-30 09:11:14 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 0.8624
2022-01-30 09:11:45 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.0377
2022-01-30 09:12:17 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.1451
2022-01-30 09:12:49 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.0240
2022-01-30 09:13:22 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 0.9547
2022-01-30 09:13:54 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.0414
2022-01-30 09:14:25 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.1038
2022-01-30 09:14:58 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.0193
2022-01-30 09:15:30 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.1123
2022-01-30 09:16:02 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.0100
2022-01-30 09:16:34 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 0.9570
2022-01-30 09:17:06 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 0.9218
2022-01-30 09:17:38 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.0803
2022-01-30 09:18:10 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 0.8315
2022-01-30 09:18:42 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 0.8700
2022-01-30 09:19:14 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 0.7248
2022-01-30 09:19:46 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 0.8697
2022-01-30 09:19:47 - train: epoch 071, train_loss: 0.9679
2022-01-30 09:20:59 - eval: epoch: 071, acc1: 75.528%, acc5: 92.692%, test_loss: 0.9617, per_image_load_time: 2.442ms, per_image_inference_time: 0.382ms
2022-01-30 09:21:00 - epoch 072 lr: 0.0010000000000000002
2022-01-30 09:21:38 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.0052
2022-01-30 09:22:10 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 0.8143
2022-01-30 09:22:42 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 0.8506
2022-01-30 09:23:14 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 0.8518
2022-01-30 09:23:46 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 0.9172
2022-01-30 09:24:19 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 0.9913
2022-01-30 09:24:50 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 0.9394
2022-01-30 09:25:23 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.1276
2022-01-30 09:25:55 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 0.8550
2022-01-30 09:26:28 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 0.8964
2022-01-30 09:27:00 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 0.9702
2022-01-30 09:27:32 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 0.8489
2022-01-30 09:28:05 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 0.9506
2022-01-30 09:28:37 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.0488
2022-01-30 09:29:09 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 0.9410
2022-01-30 09:29:42 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.0168
2022-01-30 09:30:15 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 0.8882
2022-01-30 09:30:48 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 0.8206
2022-01-30 09:31:21 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 0.8322
2022-01-30 09:31:53 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.0097
2022-01-30 09:32:26 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 0.9767
2022-01-30 09:32:59 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.0498
2022-01-30 09:33:31 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.1143
2022-01-30 09:34:03 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 0.8269
2022-01-30 09:34:37 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 0.8221
2022-01-30 09:35:09 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 0.8228
2022-01-30 09:35:42 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 0.9762
2022-01-30 09:36:15 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.0592
2022-01-30 09:36:47 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 0.9295
2022-01-30 09:37:20 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 0.8995
2022-01-30 09:37:52 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.1055
2022-01-30 09:38:24 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 0.8679
2022-01-30 09:38:56 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 0.9261
2022-01-30 09:39:29 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 0.9612
2022-01-30 09:40:00 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 0.9341
2022-01-30 09:40:32 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 0.8548
2022-01-30 09:41:03 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.0523
2022-01-30 09:41:35 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 0.9105
2022-01-30 09:42:06 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.0733
2022-01-30 09:42:39 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 0.9280
2022-01-30 09:43:12 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.0563
2022-01-30 09:43:44 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 0.9879
2022-01-30 09:44:17 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 0.9837
2022-01-30 09:44:50 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 0.8287
2022-01-30 09:45:23 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 0.9666
2022-01-30 09:45:55 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 0.9972
2022-01-30 09:46:28 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 0.9770
2022-01-30 09:47:00 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.1208
2022-01-30 09:47:33 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 0.9735
2022-01-30 09:48:05 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.0473
2022-01-30 09:48:06 - train: epoch 072, train_loss: 0.9631
2022-01-30 09:49:19 - eval: epoch: 072, acc1: 75.570%, acc5: 92.712%, test_loss: 0.9634, per_image_load_time: 2.497ms, per_image_inference_time: 0.362ms
2022-01-30 09:49:21 - epoch 073 lr: 0.0010000000000000002
2022-01-30 09:49:57 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.1331
2022-01-30 09:50:29 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 0.9886
2022-01-30 09:51:01 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.1409
2022-01-30 09:51:32 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 0.6995
2022-01-30 09:52:05 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 0.8672
2022-01-30 09:52:36 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 0.8870
2022-01-30 09:53:08 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 0.9890
2022-01-30 09:53:39 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 0.8706
2022-01-30 09:54:10 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 0.8296
2022-01-30 09:54:42 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 0.8632
2022-01-30 09:55:13 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 0.9845
2022-01-30 09:55:44 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 0.9813
2022-01-30 09:56:16 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 0.9640
2022-01-30 09:56:47 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 0.8972
2022-01-30 09:57:19 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 0.9202
2022-01-30 09:57:50 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 0.9727
2022-01-30 09:58:21 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.1639
2022-01-30 09:58:52 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 0.8325
2022-01-30 09:59:24 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.0502
2022-01-30 09:59:56 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 0.7721
2022-01-30 10:00:27 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 0.9430
2022-01-30 10:00:58 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.0991
2022-01-30 10:01:29 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.0241
2022-01-30 10:02:01 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 0.9743
2022-01-30 10:02:32 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.0518
2022-01-30 10:03:03 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 0.9776
2022-01-30 10:03:34 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 0.9624
2022-01-30 10:04:06 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 0.9901
2022-01-30 10:04:37 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.0639
2022-01-30 10:05:08 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 0.8076
2022-01-30 10:05:40 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 0.8753
2022-01-30 10:06:10 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 0.9205
2022-01-30 10:06:42 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 0.9010
2022-01-30 10:07:13 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 0.9339
2022-01-30 10:07:44 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 0.9622
2022-01-30 10:08:15 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 0.8165
2022-01-30 10:08:46 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 0.9815
2022-01-30 10:09:18 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.2126
2022-01-30 10:09:50 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 0.9568
2022-01-30 10:10:20 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.0279
2022-01-30 10:10:52 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 0.9615
2022-01-30 10:11:23 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.0409
2022-01-30 10:11:54 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 0.9312
2022-01-30 10:12:26 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.0337
2022-01-30 10:12:58 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 0.9066
2022-01-30 10:13:29 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.1830
2022-01-30 10:14:01 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 0.7718
2022-01-30 10:14:32 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 0.9942
2022-01-30 10:15:05 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 0.8432
2022-01-30 10:15:37 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.0682
2022-01-30 10:15:38 - train: epoch 073, train_loss: 0.9572
2022-01-30 10:16:50 - eval: epoch: 073, acc1: 75.638%, acc5: 92.822%, test_loss: 0.9613, per_image_load_time: 2.449ms, per_image_inference_time: 0.366ms
2022-01-30 10:16:51 - epoch 074 lr: 0.0010000000000000002
2022-01-30 10:17:28 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 0.8734
2022-01-30 10:17:59 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 0.9955
2022-01-30 10:18:30 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.0067
2022-01-30 10:19:02 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.0518
2022-01-30 10:19:32 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 0.9962
2022-01-30 10:20:04 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.0253
2022-01-30 10:20:35 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 0.9580
2022-01-30 10:21:07 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.1146
2022-01-30 10:21:38 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 0.9778
2022-01-30 10:22:10 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 0.9932
2022-01-30 10:22:40 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.0776
2022-01-30 10:23:12 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 0.9330
2022-01-30 10:23:43 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.0667
2022-01-30 10:24:14 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 0.9209
2022-01-30 10:24:46 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 0.9343
2022-01-30 10:25:17 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 0.8006
2022-01-30 10:25:48 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.0232
2022-01-30 10:26:20 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.2848
2022-01-30 10:26:51 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.1577
2022-01-30 10:27:22 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 0.7326
2022-01-30 10:27:53 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 0.9157
2022-01-30 10:28:24 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.2314
2022-01-30 10:28:55 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.0035
2022-01-30 10:29:26 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 0.9425
2022-01-30 10:29:57 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 0.9010
2022-01-30 10:30:28 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 0.8063
2022-01-30 10:31:00 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 0.8863
2022-01-30 10:31:31 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.1817
2022-01-30 10:32:03 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 0.8996
2022-01-30 10:32:33 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 0.9571
2022-01-30 10:33:05 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 0.9076
2022-01-30 10:33:36 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 0.8832
2022-01-30 10:34:08 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.0074
2022-01-30 10:34:38 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 0.9592
2022-01-30 10:35:09 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 0.7621
2022-01-30 10:35:40 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 0.9788
2022-01-30 10:36:11 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.0216
2022-01-30 10:36:43 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 0.8853
2022-01-30 10:37:14 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 0.7881
2022-01-30 10:37:46 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 0.8965
2022-01-30 10:38:17 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.1430
2022-01-30 10:38:49 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 0.9008
2022-01-30 10:39:21 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 0.8511
2022-01-30 10:39:53 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 0.8644
2022-01-30 10:40:24 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 0.7885
2022-01-30 10:40:56 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.0541
2022-01-30 10:41:28 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 0.9820
2022-01-30 10:42:00 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 0.9969
2022-01-30 10:42:33 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.2151
2022-01-30 10:43:04 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.0306
2022-01-30 10:43:06 - train: epoch 074, train_loss: 0.9523
2022-01-30 10:44:20 - eval: epoch: 074, acc1: 75.728%, acc5: 92.768%, test_loss: 0.9573, per_image_load_time: 2.541ms, per_image_inference_time: 0.362ms
2022-01-30 10:44:21 - epoch 075 lr: 0.0010000000000000002
2022-01-30 10:44:59 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 0.9709
2022-01-30 10:45:31 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 0.9460
2022-01-30 10:46:03 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 0.8728
2022-01-30 10:46:34 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.0074
2022-01-30 10:47:07 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 0.8248
2022-01-30 10:47:39 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 0.9034
2022-01-30 10:48:10 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.0328
2022-01-30 10:48:44 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.0349
2022-01-30 10:49:16 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.1693
2022-01-30 10:49:48 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 0.8019
2022-01-30 10:50:21 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.0136
2022-01-30 10:50:54 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 0.9892
2022-01-30 10:51:26 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 0.8289
2022-01-30 10:51:59 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 0.9317
2022-01-30 10:52:32 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.0946
2022-01-30 10:53:04 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 0.7552
2022-01-30 10:53:36 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 0.9144
2022-01-30 10:54:09 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 0.9356
2022-01-30 10:54:42 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 0.8709
2022-01-30 10:55:14 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.0183
2022-01-30 10:55:47 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 0.9460
2022-01-30 10:56:19 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 0.9708
2022-01-30 10:56:51 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 1.0232
2022-01-30 10:57:24 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 0.9993
2022-01-30 10:57:56 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.0596
2022-01-30 10:58:28 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 0.9681
2022-01-30 10:59:00 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 0.7812
2022-01-30 10:59:33 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 0.8463
2022-01-30 11:00:04 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 0.9211
2022-01-30 11:00:37 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.0699
2022-01-30 11:01:09 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.0276
2022-01-30 11:01:41 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 0.8423
2022-01-30 11:02:13 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 0.9322
2022-01-30 11:02:45 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 0.7628
2022-01-30 11:03:17 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 0.8444
2022-01-30 11:03:49 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 0.9568
2022-01-30 11:04:21 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 0.9500
2022-01-30 11:04:53 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 0.9816
2022-01-30 11:05:26 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.1234
2022-01-30 11:05:58 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 0.8658
2022-01-30 11:06:30 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 0.6741
2022-01-30 11:07:02 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.1242
2022-01-30 11:07:33 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.1674
2022-01-30 11:08:06 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 0.7694
2022-01-30 11:08:38 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.0068
2022-01-30 11:09:11 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 0.7885
2022-01-30 11:09:43 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.0544
2022-01-30 11:10:15 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 0.9352
2022-01-30 11:10:48 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 0.8520
2022-01-30 11:11:19 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.0862
2022-01-30 11:11:20 - train: epoch 075, train_loss: 0.9493
2022-01-30 11:12:33 - eval: epoch: 075, acc1: 75.674%, acc5: 92.776%, test_loss: 0.9582, per_image_load_time: 2.466ms, per_image_inference_time: 0.369ms
2022-01-30 11:12:33 - epoch 076 lr: 0.0010000000000000002
2022-01-30 11:13:10 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 0.7798
2022-01-30 11:13:43 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 0.9515
2022-01-30 11:14:15 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.0252
2022-01-30 11:14:46 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.0631
2022-01-30 11:15:19 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 0.9630
2022-01-30 11:15:51 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 0.9481
2022-01-30 11:16:23 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 0.9451
2022-01-30 11:16:55 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 0.8605
2022-01-30 11:17:26 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 0.8107
2022-01-30 11:17:59 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 0.7375
2022-01-30 11:18:30 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 0.8995
2022-01-30 11:19:03 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 0.9936
2022-01-30 11:19:34 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 0.9485
2022-01-30 11:20:06 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 0.8666
2022-01-30 11:20:38 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 0.8220
2022-01-30 11:21:09 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 0.8659
2022-01-30 11:21:41 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 0.9709
2022-01-30 11:22:12 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 0.7477
2022-01-30 11:22:44 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.0201
2022-01-30 11:23:15 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 0.9435
2022-01-30 11:23:45 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.0369
2022-01-30 11:24:17 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.1287
2022-01-30 11:24:48 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 0.8853
2022-01-30 11:25:20 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.1094
2022-01-30 11:25:51 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 0.9833
2022-01-30 11:26:22 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.1113
2022-01-30 11:26:53 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.0536
2022-01-30 11:27:24 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 0.8794
2022-01-30 11:27:56 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 0.9453
2022-01-30 11:28:27 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 0.8105
2022-01-30 11:28:58 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 0.9919
2022-01-30 11:29:29 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.0080
2022-01-30 11:30:00 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 0.9820
2022-01-30 11:30:31 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.0429
2022-01-30 11:31:03 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 0.8490
2022-01-30 11:31:34 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 0.8608
2022-01-30 11:32:05 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 0.8769
2022-01-30 11:32:36 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 0.7998
2022-01-30 11:33:08 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 0.8356
2022-01-30 11:33:40 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 0.9316
2022-01-30 11:34:12 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 0.8105
2022-01-30 11:34:43 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.1009
2022-01-30 11:35:15 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 0.9345
2022-01-30 11:35:47 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 0.9135
2022-01-30 11:36:19 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 0.8876
2022-01-30 11:36:51 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 0.8790
2022-01-30 11:37:23 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.0153
2022-01-30 11:37:55 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 0.7442
2022-01-30 11:38:27 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 0.9491
2022-01-30 11:38:59 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.0023
2022-01-30 11:39:00 - train: epoch 076, train_loss: 0.9439
2022-01-30 11:40:13 - eval: epoch: 076, acc1: 75.776%, acc5: 92.820%, test_loss: 0.9613, per_image_load_time: 2.472ms, per_image_inference_time: 0.368ms
2022-01-30 11:40:15 - epoch 077 lr: 0.0010000000000000002
2022-01-30 11:40:52 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 0.9954
2022-01-30 11:41:23 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 1.0465
2022-01-30 11:41:54 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 0.7186
2022-01-30 11:42:26 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.0823
2022-01-30 11:42:57 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 0.8125
2022-01-30 11:43:28 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 0.6965
2022-01-30 11:44:00 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 0.9549
2022-01-30 11:44:32 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.1036
2022-01-30 11:45:03 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 0.9829
2022-01-30 11:45:34 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 0.7799
2022-01-30 11:46:06 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 0.9055
2022-01-30 11:46:37 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 1.0167
2022-01-30 11:47:09 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 0.7353
2022-01-30 11:47:40 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.0794
2022-01-30 11:48:12 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 0.8253
2022-01-30 11:48:42 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 0.9358
2022-01-30 11:49:14 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.1443
2022-01-30 11:49:46 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 0.8251
2022-01-30 11:50:17 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 0.9131
2022-01-30 11:50:48 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 0.7912
2022-01-30 11:51:19 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 0.9289
2022-01-30 11:51:50 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 0.9769
2022-01-30 11:52:22 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.0420
2022-01-30 11:52:53 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 0.9381
2022-01-30 11:53:25 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.0596
2022-01-30 11:53:56 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 0.7036
2022-01-30 11:54:28 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 0.9937
2022-01-30 11:54:59 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.0547
2022-01-30 11:55:31 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.2308
2022-01-30 11:56:02 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 0.7602
2022-01-30 11:56:34 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 0.9778
2022-01-30 11:57:05 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.0434
2022-01-30 11:57:36 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 0.7967
2022-01-30 11:58:08 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.1339
2022-01-30 11:58:39 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 0.7995
2022-01-30 11:59:11 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 0.8658
2022-01-30 11:59:43 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 1.0463
2022-01-30 12:00:14 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 0.9582
2022-01-30 12:00:46 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 1.0587
2022-01-30 12:01:18 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 0.9217
2022-01-30 12:01:50 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 0.9979
2022-01-30 12:02:21 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.0016
2022-01-30 12:02:53 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 0.8552
2022-01-30 12:03:26 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 0.9005
2022-01-30 12:03:58 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.0183
2022-01-30 12:04:30 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 0.8975
2022-01-30 12:05:02 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 0.7441
2022-01-30 12:05:34 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 0.9146
2022-01-30 12:06:08 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.1204
2022-01-30 12:06:39 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.1324
2022-01-30 12:06:40 - train: epoch 077, train_loss: 0.9383
2022-01-30 12:07:54 - eval: epoch: 077, acc1: 75.836%, acc5: 92.744%, test_loss: 0.9602, per_image_load_time: 2.502ms, per_image_inference_time: 0.365ms
2022-01-30 12:07:55 - epoch 078 lr: 0.0010000000000000002
2022-01-30 12:08:33 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 0.9112
2022-01-30 12:09:05 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.0160
2022-01-30 12:09:37 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.0780
2022-01-30 12:10:09 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 0.9301
2022-01-30 12:10:41 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 0.8894
2022-01-30 12:11:12 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 0.8876
2022-01-30 12:11:44 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.0240
2022-01-30 12:12:16 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.1558
2022-01-30 12:12:48 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 0.9768
2022-01-30 12:13:19 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 0.9523
2022-01-30 12:13:51 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 0.7522
2022-01-30 12:14:22 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 0.7249
2022-01-30 12:14:53 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 0.9865
2022-01-30 12:15:25 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 0.8059
2022-01-30 12:15:57 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 0.8497
2022-01-30 12:16:28 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 0.9761
2022-01-30 12:16:59 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 0.9342
2022-01-30 12:17:31 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 0.9064
2022-01-30 12:18:02 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 0.7548
2022-01-30 12:18:35 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 0.8206
2022-01-30 12:19:05 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.1095
2022-01-30 12:19:37 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 0.8632
2022-01-30 12:20:08 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 1.0402
2022-01-30 12:20:40 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 0.9612
2022-01-30 12:21:11 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 0.9189
2022-01-30 12:21:42 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 0.8658
2022-01-30 12:22:13 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 0.8943
2022-01-30 12:22:44 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 0.9965
2022-01-30 12:23:15 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 0.9193
2022-01-30 12:23:47 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 0.8888
2022-01-30 12:24:18 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.0758
2022-01-30 12:24:49 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 0.9305
2022-01-30 12:25:20 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 0.9563
2022-01-30 12:25:51 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 0.9597
2022-01-30 12:26:23 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 0.9247
2022-01-30 12:26:54 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.0517
2022-01-30 12:27:25 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 0.7734
2022-01-30 12:27:56 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 0.9401
2022-01-30 12:28:27 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 0.9409
2022-01-30 12:28:58 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 0.8987
2022-01-30 12:29:30 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.0203
2022-01-30 12:30:01 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.0891
2022-01-30 12:30:31 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 0.9563
2022-01-30 12:31:02 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 0.9314
2022-01-30 12:31:34 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.0866
2022-01-30 12:32:05 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 0.8335
2022-01-30 12:32:36 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 0.9344
2022-01-30 12:33:08 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 1.1468
2022-01-30 12:33:41 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 0.9469
2022-01-30 12:34:11 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 0.9690
2022-01-30 12:34:13 - train: epoch 078, train_loss: 0.9307
2022-01-30 12:35:28 - eval: epoch: 078, acc1: 75.616%, acc5: 92.756%, test_loss: 0.9626, per_image_load_time: 2.580ms, per_image_inference_time: 0.355ms
2022-01-30 12:35:29 - epoch 079 lr: 0.0010000000000000002
2022-01-30 12:36:06 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 0.8931
2022-01-30 12:36:38 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 0.8743
2022-01-30 12:37:10 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 0.9838
2022-01-30 12:37:42 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 0.9638
2022-01-30 12:38:14 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 0.9929
2022-01-30 12:38:46 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 0.9454
2022-01-30 12:39:17 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 0.8868
2022-01-30 12:39:49 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 0.9925
2022-01-30 12:40:21 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 0.9003
2022-01-30 12:40:53 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 0.8573
2022-01-30 12:41:24 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 0.9475
2022-01-30 12:41:56 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.1922
2022-01-30 12:42:27 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 0.7874
2022-01-30 12:42:59 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 0.9132
2022-01-30 12:43:31 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 0.7900
2022-01-30 12:44:03 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 0.8410
2022-01-30 12:44:34 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 0.7957
2022-01-30 12:45:06 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 0.8349
2022-01-30 12:45:38 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.0247
2022-01-30 12:46:09 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.0591
2022-01-30 12:46:41 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 0.7888
2022-01-30 12:47:12 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 0.9970
2022-01-30 12:47:44 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 0.7764
2022-01-30 12:48:15 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.0151
2022-01-30 12:48:47 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 0.9207
2022-01-30 12:49:18 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 0.8747
2022-01-30 12:49:50 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 0.8107
2022-01-30 12:50:21 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 0.8469
2022-01-30 12:50:53 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 0.7647
2022-01-30 12:51:25 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.0081
2022-01-30 12:51:57 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 0.9864
2022-01-30 12:52:28 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.0995
2022-01-30 12:53:00 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 0.8954
2022-01-30 12:53:33 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 0.7872
2022-01-30 12:54:04 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.0979
2022-01-30 12:54:35 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 0.8548
2022-01-30 12:55:06 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 0.8686
2022-01-30 12:55:37 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 0.9591
2022-01-30 12:56:09 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 0.7983
2022-01-30 12:56:40 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 0.7957
2022-01-30 12:57:12 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.0893
2022-01-30 12:57:43 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 0.8895
2022-01-30 12:58:15 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 0.6704
2022-01-30 12:58:47 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.0616
2022-01-30 12:59:19 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.1089
2022-01-30 12:59:51 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.0855
2022-01-30 13:00:23 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 0.9849
2022-01-30 13:00:55 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.0146
2022-01-30 13:01:27 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.1409
2022-01-30 13:02:00 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 0.7994
2022-01-30 13:02:01 - train: epoch 079, train_loss: 0.9295
2022-01-30 13:03:14 - eval: epoch: 079, acc1: 75.654%, acc5: 92.896%, test_loss: 0.9597, per_image_load_time: 2.417ms, per_image_inference_time: 0.379ms
2022-01-30 13:03:15 - epoch 080 lr: 0.0010000000000000002
2022-01-30 13:03:51 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 0.8023
2022-01-30 13:04:24 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 0.8946
2022-01-30 13:04:56 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.0097
2022-01-30 13:05:27 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 0.8190
2022-01-30 13:05:59 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 0.9826
2022-01-30 13:06:31 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 0.8312
2022-01-30 13:07:02 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 0.8733
2022-01-30 13:07:34 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 0.8383
2022-01-30 13:08:06 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 0.9924
2022-01-30 13:08:37 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 0.8297
2022-01-30 13:09:09 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 0.9112
2022-01-30 13:09:41 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 0.8737
2022-01-30 13:10:12 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 0.8902
2022-01-30 13:10:44 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 0.9370
2022-01-30 13:11:16 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 0.8787
2022-01-30 13:11:48 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 0.8957
2022-01-30 13:12:20 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 1.0193
2022-01-30 13:12:51 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 0.9866
2022-01-30 13:13:23 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 0.9068
2022-01-30 13:13:55 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 1.0410
2022-01-30 13:14:28 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 0.9141
2022-01-30 13:14:59 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 0.9951
2022-01-30 13:15:31 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 0.7833
2022-01-30 13:16:03 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 0.8975
2022-01-30 13:16:35 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 0.8467
2022-01-30 13:17:07 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 0.8695
2022-01-30 13:17:39 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 0.9601
2022-01-30 13:18:11 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 1.0608
2022-01-30 13:18:43 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 0.8128
2022-01-30 13:19:16 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 0.9289
2022-01-30 13:19:46 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 1.0945
2022-01-30 13:20:19 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 0.6594
2022-01-30 13:20:51 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 0.9175
2022-01-30 13:21:23 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 0.8923
2022-01-30 13:21:55 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 0.7623
2022-01-30 13:22:27 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 0.9029
2022-01-30 13:22:59 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 0.8664
2022-01-30 13:23:32 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 0.9769
2022-01-30 13:24:04 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 0.8767
2022-01-30 13:24:36 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 1.0563
2022-01-30 13:25:08 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 1.0047
2022-01-30 13:25:40 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 0.8884
2022-01-30 13:26:13 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 0.9948
2022-01-30 13:26:45 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 0.9761
2022-01-30 13:27:18 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 0.9106
2022-01-30 13:27:49 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 1.0158
2022-01-30 13:28:23 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 0.9635
2022-01-30 13:28:55 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 0.9397
2022-01-30 13:29:29 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.0565
2022-01-30 13:30:01 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 0.7732
2022-01-30 13:30:02 - train: epoch 080, train_loss: 0.9267
2022-01-30 13:31:17 - eval: epoch: 080, acc1: 75.752%, acc5: 92.812%, test_loss: 0.9610, per_image_load_time: 2.451ms, per_image_inference_time: 0.386ms
2022-01-30 13:31:18 - epoch 081 lr: 0.0010000000000000002
2022-01-30 13:31:56 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 0.7601
2022-01-30 13:32:28 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 0.9697
2022-01-30 13:33:00 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 0.8068
2022-01-30 13:33:34 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 1.0139
2022-01-30 13:34:06 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 0.9837
2022-01-30 13:34:39 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 0.9014
2022-01-30 13:35:09 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 0.8773
2022-01-30 13:35:42 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 1.0222
2022-01-30 13:36:14 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 0.8085
2022-01-30 13:36:46 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 0.9163
2022-01-30 13:37:18 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 0.8873
2022-01-30 13:37:50 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 0.9552
2022-01-30 13:38:22 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 0.8273
2022-01-30 13:38:54 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 0.6583
2022-01-30 13:39:26 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 0.9506
2022-01-30 13:39:58 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 0.8418
2022-01-30 13:40:31 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 1.0435
2022-01-30 13:41:03 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 0.8479
2022-01-30 13:41:35 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 0.8789
2022-01-30 13:42:07 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 0.9581
2022-01-30 13:42:39 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 0.9133
2022-01-30 13:43:11 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 0.8957
2022-01-30 13:43:42 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 0.9072
2022-01-30 13:44:15 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 0.8917
2022-01-30 13:44:47 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 1.0629
2022-01-30 13:45:19 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.0049
2022-01-30 13:45:50 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 0.9361
2022-01-30 13:46:23 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 0.8531
2022-01-30 13:46:54 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 0.8270
2022-01-30 13:47:27 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 0.8900
2022-01-30 13:47:58 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 0.8695
2022-01-30 13:48:30 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 0.9073
2022-01-30 13:49:02 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 0.9885
2022-01-30 13:49:35 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 0.9901
2022-01-30 13:50:06 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 1.0158
2022-01-30 13:50:39 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 0.8299
2022-01-30 13:51:10 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 1.0137
2022-01-30 13:51:42 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 0.8721
2022-01-30 13:52:14 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 0.9646
2022-01-30 13:52:46 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 0.7224
2022-01-30 13:53:18 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 0.9963
2022-01-30 13:53:49 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 0.8700
2022-01-30 13:54:21 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 0.8946
2022-01-30 13:54:54 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 1.0180
2022-01-30 13:55:25 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 0.9158
2022-01-30 13:55:57 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 0.8726
2022-01-30 13:56:29 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 0.9561
2022-01-30 13:57:02 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 0.9380
2022-01-30 13:57:34 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 0.8666
2022-01-30 13:58:05 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 0.7264
2022-01-30 13:58:06 - train: epoch 081, train_loss: 0.9220
2022-01-30 13:59:18 - eval: epoch: 081, acc1: 75.850%, acc5: 92.954%, test_loss: 0.9601, per_image_load_time: 2.377ms, per_image_inference_time: 0.399ms
2022-01-30 13:59:19 - epoch 082 lr: 0.0010000000000000002
2022-01-30 13:59:55 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 0.7460
2022-01-30 14:00:26 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 0.8419
2022-01-30 14:00:58 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 0.9468
2022-01-30 14:01:30 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 1.0741
2022-01-30 14:02:01 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 0.8957
2022-01-30 14:02:33 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 0.7758
2022-01-30 14:03:04 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 1.0259
2022-01-30 14:03:36 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 0.9803
2022-01-30 14:04:07 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 1.0535
2022-01-30 14:04:38 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 1.0104
2022-01-30 14:05:10 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 1.1054
2022-01-30 14:05:41 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 0.8992
2022-01-30 14:06:13 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 1.0082
2022-01-30 14:06:44 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 0.8167
2022-01-30 14:07:15 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 0.8506
2022-01-30 14:07:46 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 0.9248
2022-01-30 14:08:16 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 0.8749
2022-01-30 14:08:47 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 0.7427
2022-01-30 14:09:19 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 0.9414
2022-01-30 14:09:50 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 0.8192
2022-01-30 14:10:22 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 0.8451
2022-01-30 14:10:53 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 1.0268
2022-01-30 14:11:26 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 0.9445
2022-01-30 14:11:56 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 1.0450
2022-01-30 14:12:28 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 0.8610
2022-01-30 14:12:59 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 0.9538
2022-01-30 14:13:30 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 0.9109
2022-01-30 14:14:01 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 0.8774
2022-01-30 14:14:33 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 0.8758
2022-01-30 14:15:04 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 0.8515
2022-01-30 14:15:35 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 0.7778
2022-01-30 14:16:06 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 0.9990
2022-01-30 14:16:38 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 0.8297
2022-01-30 14:17:08 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 0.8317
2022-01-30 14:17:40 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 0.8733
2022-01-30 14:18:11 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 0.8653
2022-01-30 14:18:42 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 0.8528
2022-01-30 14:19:13 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 1.1730
2022-01-30 14:19:44 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 0.9801
2022-01-30 14:20:15 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 1.0320
2022-01-30 14:20:46 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 0.8540
2022-01-30 14:21:17 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 1.0012
2022-01-30 14:21:49 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 0.7715
2022-01-30 14:22:20 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 0.8650
2022-01-30 14:22:52 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 0.9607
2022-01-30 14:23:23 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 1.0731
2022-01-30 14:23:54 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 0.8807
2022-01-30 14:24:25 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 0.8674
2022-01-30 14:24:57 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 0.8489
2022-01-30 14:25:28 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 1.0071
2022-01-30 14:25:29 - train: epoch 082, train_loss: 0.9184
2022-01-30 14:26:41 - eval: epoch: 082, acc1: 75.708%, acc5: 92.878%, test_loss: 0.9613, per_image_load_time: 2.466ms, per_image_inference_time: 0.362ms
2022-01-30 14:26:42 - epoch 083 lr: 0.0010000000000000002
2022-01-30 14:27:18 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 0.8135
2022-01-30 14:27:49 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 0.7818
2022-01-30 14:28:21 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 0.9718
2022-01-30 14:28:52 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 1.1235
2022-01-30 14:29:23 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 0.8139
2022-01-30 14:29:54 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 0.8707
2022-01-30 14:30:25 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 0.8585
2022-01-30 14:30:56 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 0.8842
2022-01-30 14:31:27 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 0.9832
2022-01-30 14:31:58 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 1.0328
2022-01-30 14:32:29 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 0.9519
2022-01-30 14:33:00 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 0.8302
2022-01-30 14:33:31 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 0.7757
2022-01-30 14:34:03 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 1.0293
2022-01-30 14:34:34 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 0.8432
2022-01-30 14:35:05 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 0.9163
2022-01-30 14:35:36 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 1.0276
2022-01-30 14:36:07 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 1.0649
2022-01-30 14:36:38 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 0.7238
2022-01-30 14:37:09 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 0.6795
2022-01-30 14:37:41 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 0.8519
2022-01-30 14:38:12 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 0.8779
2022-01-30 14:38:43 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 0.9662
2022-01-30 14:39:14 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 0.8847
2022-01-30 14:39:45 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 0.9247
2022-01-30 14:40:16 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 0.8226
2022-01-30 14:40:48 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 0.9010
2022-01-30 14:41:19 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 0.8755
2022-01-30 14:41:50 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 0.8374
2022-01-30 14:42:22 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 0.9911
2022-01-30 14:42:53 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 0.8219
2022-01-30 14:43:25 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 0.8309
2022-01-30 14:43:56 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 1.1861
2022-01-30 14:44:28 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 0.9728
2022-01-30 14:44:59 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 0.9323
2022-01-30 14:45:31 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 0.9549
2022-01-30 14:46:02 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 0.9385
2022-01-30 14:46:34 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 0.8338
2022-01-30 14:47:05 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 0.8588
2022-01-30 14:47:37 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 1.0728
2022-01-30 14:48:09 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 0.9033
2022-01-30 14:48:41 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 1.1261
2022-01-30 14:49:12 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 0.9240
2022-01-30 14:49:44 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 0.7291
2022-01-30 14:50:16 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 1.0342
2022-01-30 14:50:48 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 1.1178
2022-01-30 14:51:20 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.0504
2022-01-30 14:51:52 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 1.0122
2022-01-30 14:52:24 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 1.1964
2022-01-30 14:52:55 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 1.0354
2022-01-30 14:52:56 - train: epoch 083, train_loss: 0.9152
2022-01-30 14:54:09 - eval: epoch: 083, acc1: 75.822%, acc5: 92.732%, test_loss: 0.9620, per_image_load_time: 2.442ms, per_image_inference_time: 0.375ms
2022-01-30 14:54:10 - epoch 084 lr: 0.0010000000000000002
2022-01-30 14:54:47 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 0.8397
2022-01-30 14:55:19 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 0.9776
2022-01-30 14:55:50 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 0.7977
2022-01-30 14:56:20 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 0.8982
2022-01-30 14:56:52 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 1.0183
2022-01-30 14:57:23 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 1.0859
2022-01-30 14:57:54 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 0.9450
2022-01-30 14:58:24 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 1.0166
2022-01-30 14:58:56 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 0.9116
2022-01-30 14:59:27 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 1.0234
2022-01-30 14:59:59 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 0.8642
2022-01-30 15:00:29 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 1.1033
2022-01-30 15:01:00 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 0.8480
2022-01-30 15:01:30 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 0.9139
2022-01-30 15:02:02 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 0.9518
2022-01-30 15:02:34 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 0.7863
2022-01-30 15:03:05 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 0.9527
2022-01-30 15:03:37 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 0.9273
2022-01-30 15:04:09 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 0.9576
2022-01-30 15:04:40 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 0.9515
2022-01-30 15:05:12 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 0.8748
2022-01-30 15:05:44 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 0.7899
2022-01-30 15:06:15 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 0.9193
2022-01-30 15:06:47 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 0.7919
2022-01-30 15:07:19 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 0.9537
2022-01-30 15:07:50 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 1.0662
2022-01-30 15:08:22 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 0.9034
2022-01-30 15:08:53 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 1.0463
2022-01-30 15:09:25 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 0.8365
2022-01-30 15:09:57 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 0.9457
2022-01-30 15:10:28 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 0.8227
2022-01-30 15:11:00 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 0.8015
2022-01-30 15:11:31 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 0.8946
2022-01-30 15:12:03 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 0.8337
2022-01-30 15:12:35 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 0.7423
2022-01-30 15:13:06 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 0.8870
2022-01-30 15:13:37 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 1.0103
2022-01-30 15:14:09 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 1.0411
2022-01-30 15:14:41 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 0.9749
2022-01-30 15:15:13 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 1.0566
2022-01-30 15:15:46 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 0.9442
2022-01-30 15:16:17 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 0.9716
2022-01-30 15:16:49 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 0.8734
2022-01-30 15:17:20 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.0717
2022-01-30 15:17:53 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 0.8865
2022-01-30 15:18:24 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 0.9001
2022-01-30 15:18:56 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 1.1969
2022-01-30 15:19:27 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 0.8217
2022-01-30 15:19:59 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 0.7282
2022-01-30 15:20:30 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 1.0409
2022-01-30 15:20:32 - train: epoch 084, train_loss: 0.9116
2022-01-30 15:21:46 - eval: epoch: 084, acc1: 75.934%, acc5: 92.884%, test_loss: 0.9597, per_image_load_time: 2.405ms, per_image_inference_time: 0.377ms
2022-01-30 15:21:47 - epoch 085 lr: 0.0010000000000000002
2022-01-30 15:22:24 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 0.7524
2022-01-30 15:22:56 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 0.6905
2022-01-30 15:23:28 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 1.0145
2022-01-30 15:23:59 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 0.8329
2022-01-30 15:24:30 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 1.0910
2022-01-30 15:25:02 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 0.7316
2022-01-30 15:25:32 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 0.8140
2022-01-30 15:26:05 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 0.8141
2022-01-30 15:26:36 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 0.8437
2022-01-30 15:27:07 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 1.0185
2022-01-30 15:27:39 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 0.8907
2022-01-30 15:28:11 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 0.8623
2022-01-30 15:28:42 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 0.9113
2022-01-30 15:29:13 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 0.9207
2022-01-30 15:29:45 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 0.7312
2022-01-30 15:30:16 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 0.9315
2022-01-30 15:30:47 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 1.1096
2022-01-30 15:31:19 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 0.8210
2022-01-30 15:31:51 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 0.8188
2022-01-30 15:32:22 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 0.8409
2022-01-30 15:32:54 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 0.6953
2022-01-30 15:33:25 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 0.9776
2022-01-30 15:33:56 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 0.7321
2022-01-30 15:34:29 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 0.8938
2022-01-30 15:35:01 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 1.0914
2022-01-30 15:35:32 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 0.9865
2022-01-30 15:36:02 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 0.8854
2022-01-30 15:36:35 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 1.0177
2022-01-30 15:37:06 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 1.0542
2022-01-30 15:37:38 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 1.0370
2022-01-30 15:38:09 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 0.8483
2022-01-30 15:38:41 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 0.8880
2022-01-30 15:39:11 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 0.8809
2022-01-30 15:39:43 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 1.0121
2022-01-30 15:40:15 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 0.9892
2022-01-30 15:40:46 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 0.9434
2022-01-30 15:41:21 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 0.7996
2022-01-30 15:41:59 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 0.8829
2022-01-30 15:42:41 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 0.8523
2022-01-30 15:43:33 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 1.1197
2022-01-30 15:44:05 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 0.9106
2022-01-30 15:44:35 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 1.0287
2022-01-30 15:45:07 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 0.8603
2022-01-30 15:46:00 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 0.8683
2022-01-30 15:46:42 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 1.0186
2022-01-30 15:47:19 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 0.9697
2022-01-30 15:47:55 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.0398
2022-01-30 15:48:34 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 0.7771
2022-01-30 15:49:30 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 0.8960
2022-01-30 15:50:10 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 0.8157
2022-01-30 15:50:11 - train: epoch 085, train_loss: 0.9091
2022-01-30 15:52:01 - eval: epoch: 085, acc1: 75.694%, acc5: 92.738%, test_loss: 0.9667, per_image_load_time: 3.927ms, per_image_inference_time: 0.361ms
2022-01-30 15:52:01 - epoch 086 lr: 0.0010000000000000002
2022-01-30 15:52:48 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 0.7634
2022-01-30 15:53:38 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 0.8374
2022-01-30 15:54:30 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 0.9956
2022-01-30 15:55:07 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 0.8836
2022-01-30 15:55:57 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 0.8691
2022-01-30 15:56:43 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 0.8340
2022-01-30 15:57:27 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 0.7977
2022-01-30 15:57:58 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 1.0354
2022-01-30 15:58:28 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 0.9921
2022-01-30 15:58:59 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 0.9589
2022-01-30 15:59:31 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 0.9273
2022-01-30 16:00:12 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 0.8234
2022-01-30 16:02:53 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 0.9597
2022-01-30 16:05:07 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 0.7494
2022-01-30 16:07:11 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 0.7612
2022-01-30 16:09:55 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 0.9465
2022-01-30 16:13:26 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 0.8509
2022-01-30 16:17:15 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 0.8474
2022-01-30 16:21:58 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 1.0171
2022-01-30 16:24:45 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 0.9119
2022-01-30 16:26:10 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 0.8329
2022-01-30 16:27:28 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 0.9154
2022-01-30 16:28:59 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 0.8415
2022-01-30 16:30:05 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 0.8009
2022-01-30 16:31:52 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 0.8836
2022-01-30 16:33:12 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 0.9403
2022-01-30 16:34:31 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 0.7996
2022-01-30 16:36:03 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 1.0462
2022-01-30 16:36:51 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 0.7566
2022-01-30 16:39:55 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 0.9487
2022-01-30 16:41:23 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 0.8614
2022-01-30 16:42:59 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 0.9987
2022-01-30 16:43:59 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 0.9687
2022-01-30 16:44:52 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 0.9251
2022-01-30 16:46:24 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 0.8113
2022-01-30 16:48:17 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 0.9001
2022-01-30 16:49:54 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 1.0463
2022-01-30 16:50:32 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 0.9167
2022-01-30 16:51:03 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 0.9015
2022-01-30 16:51:35 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 0.9620
2022-01-30 16:52:17 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 0.9622
2022-01-30 16:53:17 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 0.9438
2022-01-30 16:53:56 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 0.9032
2022-01-30 16:54:46 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 0.8927
2022-01-30 16:55:58 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 0.8692
2022-01-30 16:56:28 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 0.8516
2022-01-30 16:57:16 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 0.8664
2022-01-30 16:58:22 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 0.9446
2022-01-30 16:59:12 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 0.9124
2022-01-30 16:59:42 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 0.8896
2022-01-30 16:59:44 - train: epoch 086, train_loss: 0.9040
2022-01-30 17:00:56 - eval: epoch: 086, acc1: 75.734%, acc5: 92.772%, test_loss: 0.9699, per_image_load_time: 2.375ms, per_image_inference_time: 0.384ms
2022-01-30 17:00:56 - epoch 087 lr: 0.0010000000000000002
2022-01-30 17:01:33 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 1.0139
2022-01-30 17:02:04 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 0.7194
2022-01-30 17:02:35 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 1.0091
2022-01-30 17:03:15 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 0.9987
2022-01-30 17:04:27 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 0.7207
2022-01-30 17:05:21 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 0.8870
2022-01-30 17:05:56 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 0.7303
2022-01-30 17:06:28 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 1.0535
2022-01-30 17:06:59 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 0.9210
2022-01-30 17:07:31 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 0.8214
2022-01-30 17:08:08 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 0.9576
2022-01-30 17:08:39 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 0.9354
2022-01-30 17:09:10 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 0.9690
2022-01-30 17:09:41 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 0.9397
2022-01-30 17:10:13 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 0.8424
2022-01-30 17:10:57 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 0.7906
2022-01-30 17:11:28 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 1.1029
2022-01-30 17:11:59 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 0.9440
2022-01-30 17:12:31 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 0.9185
2022-01-30 17:13:03 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 0.8846
2022-01-30 17:13:35 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 1.0352
2022-01-30 17:14:07 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 0.8634
2022-01-30 17:14:39 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 0.9900
2022-01-30 17:15:11 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 0.9170
2022-01-30 17:15:42 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 0.8223
2022-01-30 17:16:13 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 0.9032
2022-01-30 17:16:45 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 0.8275
2022-01-30 17:17:17 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 0.7476
2022-01-30 17:17:51 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 0.7888
2022-01-30 17:18:24 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 0.8664
2022-01-30 17:18:55 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 0.9541
2022-01-30 17:19:26 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 0.8241
2022-01-30 17:19:58 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 0.9205
2022-01-30 17:20:29 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 0.9346
2022-01-30 17:21:01 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 0.8211
2022-01-30 17:21:33 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 0.8049
2022-01-30 17:22:04 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 0.8391
2022-01-30 17:22:46 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 0.8971
2022-01-30 17:23:24 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 0.8826
2022-01-30 17:23:55 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 0.9318
2022-01-30 17:24:27 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 1.0115
2022-01-30 17:24:59 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 0.9769
2022-01-30 17:25:31 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 0.9925
2022-01-30 17:26:02 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 0.9345
2022-01-30 17:26:59 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 0.9802
2022-01-30 17:27:31 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 0.8777
2022-01-30 17:28:03 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 0.9218
2022-01-30 17:28:36 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 0.8655
2022-01-30 17:29:08 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 0.8307
2022-01-30 17:29:39 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 0.8720
2022-01-30 17:29:41 - train: epoch 087, train_loss: 0.9005
2022-01-30 17:30:52 - eval: epoch: 087, acc1: 75.590%, acc5: 92.734%, test_loss: 0.9662, per_image_load_time: 1.516ms, per_image_inference_time: 0.430ms
2022-01-30 17:30:52 - epoch 088 lr: 0.0010000000000000002
2022-01-30 17:31:29 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 0.8775
2022-01-30 17:32:01 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 0.8397
2022-01-30 17:32:32 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 0.9707
2022-01-30 17:33:04 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 0.9057
2022-01-30 17:33:35 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 0.7939
2022-01-30 17:34:08 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 0.8723
2022-01-30 17:34:39 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 0.8433
2022-01-30 17:35:11 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 0.8633
2022-01-30 17:35:42 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 0.9400
2022-01-30 17:36:14 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 0.9132
2022-01-30 17:36:46 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 0.8847
2022-01-30 17:37:17 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 0.8996
2022-01-30 17:37:49 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 0.9527
2022-01-30 17:38:21 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 0.8474
2022-01-30 17:38:53 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 0.9459
2022-01-30 17:39:25 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 0.7706
2022-01-30 17:39:57 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 0.9593
2022-01-30 17:40:29 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 0.8982
2022-01-30 17:41:00 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 0.9160
2022-01-30 17:41:33 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 0.7150
2022-01-30 17:42:04 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 0.9133
2022-01-30 17:42:36 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 0.7885
2022-01-30 17:43:08 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 0.7268
2022-01-30 17:43:40 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 0.8724
2022-01-30 17:44:12 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 0.9208
2022-01-30 17:44:44 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 0.9727
2022-01-30 17:45:15 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 0.9014
2022-01-30 17:45:47 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 0.8434
2022-01-30 17:46:18 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 0.8597
2022-01-30 17:46:50 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 0.9487
2022-01-30 17:47:22 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 0.8247
2022-01-30 17:47:53 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 0.7596
2022-01-30 17:48:25 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 0.8147
2022-01-30 17:48:57 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 0.7677
2022-01-30 17:49:29 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 0.7869
2022-01-30 17:50:00 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 0.9568
2022-01-30 17:50:32 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 0.9394
2022-01-30 17:51:04 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 0.9006
2022-01-30 17:51:36 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 1.0407
2022-01-30 17:52:08 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 0.7654
2022-01-30 17:52:41 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 0.9820
2022-01-30 17:53:11 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 0.9941
2022-01-30 17:53:44 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 0.8391
2022-01-30 17:54:15 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 0.8441
2022-01-30 17:54:48 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 0.9241
2022-01-30 17:55:20 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 0.9986
2022-01-30 17:55:52 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 0.9688
2022-01-30 17:56:23 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 0.7641
2022-01-30 17:56:56 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 0.7411
2022-01-30 17:57:27 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 0.9158
2022-01-30 17:57:28 - train: epoch 088, train_loss: 0.8992
2022-01-30 17:58:39 - eval: epoch: 088, acc1: 75.628%, acc5: 92.808%, test_loss: 0.9678, per_image_load_time: 1.933ms, per_image_inference_time: 0.456ms
2022-01-30 17:58:40 - epoch 089 lr: 0.0010000000000000002
2022-01-30 17:59:17 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 1.0797
2022-01-30 17:59:48 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 0.7512
2022-01-30 18:00:20 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 0.8824
2022-01-30 18:00:53 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 0.7822
2022-01-30 18:01:24 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 0.9232
2022-01-30 18:01:56 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 0.8290
2022-01-30 18:02:28 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 1.0191
2022-01-30 18:03:00 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 1.0434
2022-01-30 18:03:31 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 1.0197
2022-01-30 18:04:04 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 1.0327
2022-01-30 18:04:36 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 0.8374
2022-01-30 18:05:07 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 0.9691
2022-01-30 18:05:39 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 0.9138
2022-01-30 18:06:11 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 0.9511
2022-01-30 18:06:42 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 0.9551
2022-01-30 18:07:14 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 0.9479
2022-01-30 18:07:47 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 0.9305
2022-01-30 18:08:18 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 0.8827
2022-01-30 18:08:50 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 0.8382
2022-01-30 18:09:23 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 0.8149
2022-01-30 18:09:54 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 1.0046
2022-01-30 18:10:26 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 0.9037
2022-01-30 18:10:58 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 0.8862
2022-01-30 18:11:29 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 0.9305
2022-01-30 18:12:01 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 0.8435
2022-01-30 18:12:33 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 0.9242
2022-01-30 18:13:04 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 0.8180
2022-01-30 18:13:36 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 0.9736
2022-01-30 18:14:08 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 0.8603
2022-01-30 18:14:39 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 0.8600
2022-01-30 18:15:12 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 1.0193
2022-01-30 18:15:43 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 0.9460
2022-01-30 18:16:15 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 1.0093
2022-01-30 18:16:47 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 0.9407
2022-01-30 18:17:18 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 0.7733
2022-01-30 18:17:50 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 0.8202
2022-01-30 18:18:22 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 1.0848
2022-01-30 18:18:54 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 0.8608
2022-01-30 18:19:27 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 0.9148
2022-01-30 18:19:58 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 0.7784
2022-01-30 18:20:30 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 1.1816
2022-01-30 18:21:01 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 0.8752
2022-01-30 18:21:33 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 0.9303
2022-01-30 18:22:03 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 0.8379
2022-01-30 18:22:35 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 0.8203
2022-01-30 18:23:07 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 0.8719
2022-01-30 18:23:38 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 0.8976
2022-01-30 18:24:10 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 0.9048
2022-01-30 18:24:42 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 0.8331
2022-01-30 18:25:13 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 0.7302
2022-01-30 18:25:14 - train: epoch 089, train_loss: 0.8965
2022-01-30 18:26:26 - eval: epoch: 089, acc1: 75.684%, acc5: 92.882%, test_loss: 0.9627, per_image_load_time: 1.499ms, per_image_inference_time: 0.435ms
2022-01-30 18:33:10 - epoch 090 lr: 0.0010000000000000002
2022-01-30 18:33:48 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 0.8305
2022-01-30 18:34:19 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 0.8715
2022-01-30 18:34:51 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 0.7604
2022-01-30 18:35:21 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 0.6609
2022-01-30 18:35:54 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 0.9136
2022-01-30 18:36:26 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 1.0302
2022-01-30 18:36:58 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 0.8784
2022-01-30 18:37:28 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 0.8548
2022-01-30 18:38:02 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 0.7551
2022-01-30 18:38:34 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 0.8650
2022-01-30 18:39:06 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 0.9384
2022-01-30 18:39:38 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 0.7690
2022-01-30 18:40:10 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 0.8485
2022-01-30 18:40:42 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 0.8828
2022-01-30 18:41:14 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 1.0211
2022-01-30 18:41:47 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 0.8687
2022-01-30 18:42:18 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 0.7720
2022-01-30 18:42:50 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 0.8709
2022-01-30 18:43:22 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 0.7943
2022-01-30 18:43:55 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 0.9231
2022-01-30 18:44:27 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 1.0074
2022-01-30 18:44:59 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 0.9256
2022-01-30 18:45:32 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 0.9037
2022-01-30 18:46:04 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 0.8851
2022-01-30 18:46:37 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 1.0064
2022-01-30 18:47:09 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 0.8854
2022-01-30 18:47:41 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 1.0124
2022-01-30 18:48:14 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 0.8408
2022-01-30 18:48:46 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 0.7948
2022-01-30 18:49:18 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 0.8040
2022-01-30 18:49:51 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 0.7212
2022-01-30 18:50:23 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 0.9036
2022-01-30 18:50:55 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 0.8512
2022-01-30 18:51:26 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 0.9954
2022-01-30 18:51:59 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 0.9556
2022-01-30 18:52:31 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 0.8506
2022-01-30 18:53:02 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 0.9510
2022-01-30 18:53:34 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 1.0048
2022-01-30 18:54:07 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 0.8958
2022-01-30 18:54:38 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 0.8225
2022-01-30 18:55:10 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 1.1068
2022-01-30 18:55:43 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 0.8752
2022-01-30 18:56:16 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 0.7684
2022-01-30 18:56:48 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 0.8927
2022-01-30 18:57:20 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 0.8654
2022-01-30 18:57:52 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 0.8322
2022-01-30 18:58:26 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 0.9472
2022-01-30 18:58:58 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 1.0017
2022-01-30 18:59:31 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 0.8399
2022-01-30 19:00:03 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 0.7291
2022-01-30 19:00:04 - train: epoch 090, train_loss: 0.8913
2022-01-30 19:01:17 - eval: epoch: 090, acc1: 75.846%, acc5: 92.818%, test_loss: 0.9638, per_image_load_time: 0.974ms, per_image_inference_time: 0.423ms
2022-01-30 19:01:17 - epoch 091 lr: 0.00010000000000000003
2022-01-30 19:01:55 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 0.7746
2022-01-30 19:02:28 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 0.9212
2022-01-30 19:03:00 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 0.8063
2022-01-30 19:03:33 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 0.8899
2022-01-30 19:04:04 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 0.7454
2022-01-30 19:04:36 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 0.8066
2022-01-30 19:05:09 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 0.8888
2022-01-30 19:05:41 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 0.6721
2022-01-30 19:06:13 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 0.9453
2022-01-30 19:06:45 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 0.8085
2022-01-30 19:07:18 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 0.7080
2022-01-30 19:07:51 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 1.0111
2022-01-30 19:08:23 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 0.8734
2022-01-30 19:08:55 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 0.8688
2022-01-30 19:09:27 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 0.8175
2022-01-30 19:09:59 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 0.9317
2022-01-30 19:10:30 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 0.9365
2022-01-30 19:11:03 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 0.7728
2022-01-30 19:11:34 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 0.9496
2022-01-30 19:12:05 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 0.6827
2022-01-30 19:12:38 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 0.8598
2022-01-30 19:13:09 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 0.8528
2022-01-30 19:13:41 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 1.1082
2022-01-30 19:14:13 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 0.7060
2022-01-30 19:14:45 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 0.7801
2022-01-30 19:15:18 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 0.8958
2022-01-30 19:15:49 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 1.0049
2022-01-30 19:16:21 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 0.8416
2022-01-30 19:16:53 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 1.0249
2022-01-30 19:17:26 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 0.9007
2022-01-30 19:17:58 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 0.7677
2022-01-30 19:18:30 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 0.8865
2022-01-30 19:19:02 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 0.7908
2022-01-30 19:19:34 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 0.7134
2022-01-30 19:20:07 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 0.8978
2022-01-30 19:20:39 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 0.8944
2022-01-30 19:21:11 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 0.8840
2022-01-30 19:21:43 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 0.7569
2022-01-30 19:22:15 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 0.8119
2022-01-30 19:22:47 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 0.7904
2022-01-30 19:23:20 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 0.8243
2022-01-30 19:23:52 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 0.9783
2022-01-30 19:24:24 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 0.7800
2022-01-30 19:24:57 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 0.6392
2022-01-30 19:25:28 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 0.7638
2022-01-30 19:26:00 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 0.9377
2022-01-30 19:26:32 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 0.8109
2022-01-30 19:27:04 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 1.0018
2022-01-30 19:27:36 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 0.7636
2022-01-30 19:28:07 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 0.9478
2022-01-30 19:28:08 - train: epoch 091, train_loss: 0.8598
2022-01-30 19:29:20 - eval: epoch: 091, acc1: 76.064%, acc5: 93.050%, test_loss: 0.9471, per_image_load_time: 0.746ms, per_image_inference_time: 0.398ms
2022-01-30 19:29:21 - epoch 092 lr: 0.00010000000000000003
2022-01-30 19:29:58 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 0.9471
2022-01-30 19:30:30 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 0.8675
2022-01-30 19:31:02 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 0.8646
2022-01-30 19:31:34 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 0.8056
2022-01-30 19:32:05 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 0.9368
2022-01-30 19:32:37 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 0.8513
2022-01-30 19:33:08 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 0.8893
2022-01-30 19:33:40 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 0.8383
2022-01-30 19:34:11 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 1.0265
2022-01-30 19:34:42 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 0.8995
2022-01-30 19:35:14 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 0.8311
2022-01-30 19:35:46 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 0.7866
2022-01-30 19:36:17 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 0.7278
2022-01-30 19:36:48 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 0.6779
2022-01-30 19:37:21 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 0.8125
2022-01-30 19:37:52 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 0.8332
2022-01-30 19:38:23 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 0.8730
2022-01-30 19:38:56 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 0.8728
2022-01-30 19:39:28 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 0.7484
2022-01-30 19:39:59 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 0.9000
2022-01-30 19:40:31 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 1.0225
2022-01-30 19:41:02 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 0.8759
2022-01-30 19:41:34 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 0.8823
2022-01-30 19:42:06 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 0.7224
2022-01-30 19:42:37 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 0.8838
2022-01-30 19:43:10 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 0.8406
2022-01-30 19:43:41 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 0.8879
2022-01-30 19:44:13 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 0.8456
2022-01-30 19:44:45 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 0.8385
2022-01-30 19:45:16 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 0.8482
2022-01-30 19:45:48 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 0.9096
2022-01-30 19:46:20 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 0.9460
2022-01-30 19:46:53 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 0.9529
2022-01-30 19:47:24 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 1.0863
2022-01-30 19:47:56 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 0.7743
2022-01-30 19:48:27 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 0.8580
2022-01-30 19:48:59 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 0.8191
2022-01-30 19:49:31 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 0.7572
2022-01-30 19:50:03 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 0.8006
2022-01-30 19:50:35 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 0.8265
2022-01-30 19:51:07 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 0.7591
2022-01-30 19:51:39 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 0.9411
2022-01-30 19:52:10 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 0.8393
2022-01-30 19:52:42 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 0.9288
2022-01-30 19:53:15 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 0.7254
2022-01-30 19:53:47 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 0.7477
2022-01-30 19:54:18 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 0.6616
2022-01-30 19:54:51 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 0.8596
2022-01-30 19:55:23 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 0.6622
2022-01-30 19:55:54 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 0.8014
2022-01-30 19:55:56 - train: epoch 092, train_loss: 0.8495
2022-01-30 19:57:08 - eval: epoch: 092, acc1: 76.204%, acc5: 93.100%, test_loss: 0.9471, per_image_load_time: 0.946ms, per_image_inference_time: 0.381ms
2022-01-30 19:57:09 - epoch 093 lr: 0.00010000000000000003
2022-01-30 19:57:46 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 0.8958
2022-01-30 19:58:17 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 0.8205
2022-01-30 19:58:50 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 0.8260
2022-01-30 19:59:22 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 0.8540
2022-01-30 19:59:54 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 0.9459
2022-01-30 20:00:25 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 0.8535
2022-01-30 20:00:57 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 0.8085
2022-01-30 20:01:28 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 0.7193
2022-01-30 20:02:00 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 0.8969
2022-01-30 20:02:32 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 0.6509
2022-01-30 20:03:03 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 0.8716
2022-01-30 20:03:36 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 0.8441
2022-01-30 20:04:08 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 0.7288
2022-01-30 20:04:41 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 0.8126
2022-01-30 20:05:14 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 0.9808
2022-01-30 20:05:46 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 0.8633
2022-01-30 20:06:19 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 0.8922
2022-01-30 20:06:52 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 1.0459
2022-01-30 20:07:24 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 0.8156
2022-01-30 20:07:57 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 0.7439
2022-01-30 20:08:29 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 0.7144
2022-01-30 20:09:01 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 1.1061
2022-01-30 20:09:33 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 0.8301
2022-01-30 20:10:05 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 0.6958
2022-01-30 20:10:38 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 0.9640
2022-01-30 20:11:09 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 0.9231
2022-01-30 20:11:41 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 0.8430
2022-01-30 20:12:13 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 0.8846
2022-01-30 20:12:44 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 0.8394
2022-01-30 20:13:16 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 0.8852
2022-01-30 20:13:48 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 0.8196
2022-01-30 20:14:20 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 0.7894
2022-01-30 20:14:53 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 0.7613
2022-01-30 20:15:25 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 1.0561
2022-01-30 20:15:57 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 0.8042
2022-01-30 20:16:29 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 0.8747
2022-01-30 20:17:02 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 0.8440
2022-01-30 20:17:34 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 0.7740
2022-01-30 20:18:05 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 0.9304
2022-01-30 20:18:39 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 0.8849
2022-01-30 20:19:11 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 0.9472
2022-01-30 20:19:43 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 1.0020
2022-01-30 20:20:15 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 0.9551
2022-01-30 20:20:48 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 0.9370
2022-01-30 20:21:20 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 0.7492
2022-01-30 20:21:52 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 0.7845
2022-01-30 20:22:24 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 1.0003
2022-01-30 20:22:57 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 0.7957
2022-01-30 20:23:30 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 0.8468
2022-01-30 20:24:00 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 1.0164
2022-01-30 20:24:02 - train: epoch 093, train_loss: 0.8471
2022-01-30 20:25:14 - eval: epoch: 093, acc1: 76.200%, acc5: 93.082%, test_loss: 0.9449, per_image_load_time: 0.817ms, per_image_inference_time: 0.390ms
2022-01-30 20:25:15 - epoch 094 lr: 0.00010000000000000003
2022-01-30 20:25:53 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 0.8526
2022-01-30 20:26:25 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 0.7307
2022-01-30 20:26:56 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 0.8993
2022-01-30 20:27:28 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 1.0580
2022-01-30 20:28:00 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 0.8413
2022-01-30 20:28:31 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 0.7120
2022-01-30 20:29:03 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 0.8268
2022-01-30 20:29:35 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 0.7017
2022-01-30 20:30:07 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 0.7815
2022-01-30 20:30:39 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 0.8411
2022-01-30 20:31:11 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 1.0234
2022-01-30 20:31:42 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 0.8257
2022-01-30 20:32:15 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 0.7456
2022-01-30 20:32:47 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 0.9664
2022-01-30 20:33:18 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 0.9133
2022-01-30 20:33:50 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.0026
2022-01-30 20:34:22 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 0.7670
2022-01-30 20:34:54 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 0.7392
2022-01-30 20:35:27 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 1.0323
2022-01-30 20:35:58 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 0.7047
2022-01-30 20:36:30 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 0.6717
2022-01-30 20:37:02 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 0.7829
2022-01-30 20:37:34 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 0.8768
2022-01-30 20:38:06 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 0.8225
2022-01-30 20:38:38 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 0.7879
2022-01-30 20:39:09 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 0.8194
2022-01-30 20:39:41 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 0.7956
2022-01-30 20:40:14 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 0.8082
2022-01-30 20:40:46 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 0.8523
2022-01-30 20:41:18 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 0.9438
2022-01-30 20:41:51 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 0.8712
2022-01-30 20:42:22 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 0.8015
2022-01-30 20:42:54 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 0.8546
2022-01-30 20:43:27 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 0.8352
2022-01-30 20:43:59 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 0.9766
2022-01-30 20:44:31 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 0.7327
2022-01-30 20:45:03 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 0.9919
2022-01-30 20:45:35 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 0.9248
2022-01-30 20:46:08 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 0.8517
2022-01-30 20:46:39 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 0.8423
2022-01-30 20:47:12 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 0.9368
2022-01-30 20:47:43 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 0.8739
2022-01-30 20:48:15 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 0.8650
2022-01-30 20:48:47 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 0.9860
2022-01-30 20:49:20 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 0.7787
2022-01-30 20:49:52 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 0.8232
2022-01-30 20:50:25 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 1.0062
2022-01-30 20:50:57 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 0.6216
2022-01-30 20:51:30 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 0.8666
2022-01-30 20:52:01 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 0.8786
2022-01-30 20:52:03 - train: epoch 094, train_loss: 0.8420
2022-01-30 20:53:16 - eval: epoch: 094, acc1: 76.198%, acc5: 93.108%, test_loss: 0.9444, per_image_load_time: 0.608ms, per_image_inference_time: 0.389ms
2022-01-30 20:53:16 - epoch 095 lr: 0.00010000000000000003
2022-01-30 20:53:55 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 0.8951
2022-01-30 20:54:26 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 0.9115
2022-01-30 20:54:58 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 0.7534
2022-01-30 20:55:30 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 0.7537
2022-01-30 20:56:02 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 1.0707
2022-01-30 20:56:34 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 0.8111
2022-01-30 20:57:05 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 1.0301
2022-01-30 20:57:38 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 0.8894
2022-01-30 20:58:08 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 0.9534
2022-01-30 20:58:39 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 0.7504
2022-01-30 20:59:11 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 0.9125
2022-01-30 20:59:42 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 0.6658
2022-01-30 21:00:15 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 0.8594
2022-01-30 21:00:47 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 0.9571
2022-01-30 21:01:19 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 0.8068
2022-01-30 21:01:51 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 0.8704
2022-01-30 21:02:23 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 0.8969
2022-01-30 21:02:55 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 0.8227
2022-01-30 21:03:27 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 0.8784
2022-01-30 21:03:59 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 0.8024
2022-01-30 21:04:31 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 0.9341
2022-01-30 21:05:03 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 0.8077
2022-01-30 21:05:36 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 0.8258
2022-01-30 21:06:08 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 0.9157
2022-01-30 21:06:40 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 0.8566
2022-01-30 21:07:13 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 0.9227
2022-01-30 21:07:46 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 0.9081
2022-01-30 21:08:18 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 0.6772
2022-01-30 21:08:49 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 0.7990
2022-01-30 21:09:21 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 0.8824
2022-01-30 21:09:54 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 0.9657
2022-01-30 21:10:25 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 0.8072
2022-01-30 21:10:57 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 0.8383
2022-01-30 21:11:29 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 0.6917
2022-01-30 21:12:02 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 0.8294
2022-01-30 21:12:34 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 0.9084
2022-01-30 21:13:06 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 0.8241
2022-01-30 21:13:38 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 0.7763
2022-01-30 21:14:10 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 0.8277
2022-01-30 21:14:43 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 0.6546
2022-01-30 21:15:15 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 0.7776
2022-01-30 21:15:48 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 0.7023
2022-01-30 21:16:20 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 1.0138
2022-01-30 21:16:53 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 0.8508
2022-01-30 21:17:26 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 0.7600
2022-01-30 21:17:59 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 0.8094
2022-01-30 21:18:33 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 0.8575
2022-01-30 21:19:06 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 0.6920
2022-01-30 21:19:40 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 0.7258
2022-01-30 21:20:13 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 0.8307
2022-01-30 21:20:14 - train: epoch 095, train_loss: 0.8413
2022-01-30 21:21:29 - eval: epoch: 095, acc1: 76.218%, acc5: 93.048%, test_loss: 0.9452, per_image_load_time: 0.588ms, per_image_inference_time: 0.360ms
2022-01-30 21:21:30 - epoch 096 lr: 0.00010000000000000003
2022-01-30 21:22:09 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 1.0058
2022-01-30 21:22:41 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 0.7688
2022-01-30 21:23:13 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 0.8812
2022-01-30 21:23:45 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 0.8094
2022-01-30 21:24:19 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 0.9258
2022-01-30 21:24:51 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 0.8670
2022-01-30 21:25:23 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 0.8337
2022-01-30 21:25:56 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 0.7270
2022-01-30 21:26:28 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 0.9485
2022-01-30 21:27:01 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 0.9513
2022-01-30 21:27:32 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 0.8959
2022-01-30 21:28:04 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 0.8082
2022-01-30 21:28:37 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 0.8409
2022-01-30 21:29:08 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 0.8579
2022-01-30 21:29:40 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 0.7490
2022-01-30 21:30:12 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 0.7311
2022-01-30 21:30:44 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 0.8767
2022-01-30 21:31:15 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 0.8485
2022-01-30 21:31:46 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 1.0312
2022-01-30 21:32:18 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 0.8008
2022-01-30 21:32:49 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 0.8312
2022-01-30 21:33:21 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 0.6421
2022-01-30 21:33:52 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 0.9120
2022-01-30 21:34:23 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 0.8108
2022-01-30 21:34:54 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 0.8882
2022-01-30 21:35:26 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 0.7947
2022-01-30 21:35:57 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 0.7829
2022-01-30 21:36:28 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 0.7737
2022-01-30 21:37:00 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 0.9298
2022-01-30 21:37:31 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 0.7706
2022-01-30 21:38:02 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 1.0063
2022-01-30 21:38:34 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 0.9402
2022-01-30 21:39:05 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 0.8427
2022-01-30 21:39:36 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 0.8152
2022-01-30 21:40:07 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 0.7539
2022-01-30 21:40:40 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 0.6927
2022-01-30 21:41:11 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 0.9779
2022-01-30 21:41:42 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 0.7063
2022-01-30 21:42:13 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 0.8047
2022-01-30 21:42:46 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 0.9052
2022-01-30 21:43:17 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 0.9838
2022-01-30 21:43:49 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 0.8051
2022-01-30 21:44:21 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 0.7305
2022-01-30 21:44:53 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 0.7883
2022-01-30 21:45:25 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 1.0101
2022-01-30 21:45:57 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 0.8561
2022-01-30 21:46:30 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 0.8976
2022-01-30 21:47:02 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 0.8658
2022-01-30 21:47:37 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 0.8780
2022-01-30 21:48:11 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 0.9401
2022-01-30 21:48:12 - train: epoch 096, train_loss: 0.8376
2022-01-30 21:49:26 - eval: epoch: 096, acc1: 76.226%, acc5: 93.102%, test_loss: 0.9433, per_image_load_time: 0.521ms, per_image_inference_time: 0.349ms
2022-01-30 21:49:27 - epoch 097 lr: 0.00010000000000000003
2022-01-30 21:50:05 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 0.9752
2022-01-30 21:50:36 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 0.8080
2022-01-30 21:51:08 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 0.8255
2022-01-30 21:51:40 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 0.9846
2022-01-30 21:52:11 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 0.8245
2022-01-30 21:52:43 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 0.9854
2022-01-30 21:53:14 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 0.7237
2022-01-30 21:53:46 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 0.8456
2022-01-30 21:54:17 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 0.7693
2022-01-30 21:54:49 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 0.9187
2022-01-30 21:55:21 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 0.6580
2022-01-30 21:55:52 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 0.7308
2022-01-30 21:56:23 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 0.7488
2022-01-30 21:56:55 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 0.7823
2022-01-30 21:57:26 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 0.7269
2022-01-30 21:57:58 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 0.8500
2022-01-30 21:58:30 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 0.6917
2022-01-30 21:59:02 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 0.8274
2022-01-30 21:59:34 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 0.7000
2022-01-30 22:00:05 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 0.7235
2022-01-30 22:00:35 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 0.8814
2022-01-30 22:01:05 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 0.8888
2022-01-30 22:01:36 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 0.6930
2022-01-30 22:02:06 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 0.8666
2022-01-30 22:02:37 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 1.0155
2022-01-30 22:03:07 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 0.9373
2022-01-30 22:03:40 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 0.8532
2022-01-30 22:04:11 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 0.8517
2022-01-30 22:04:44 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 0.8151
2022-01-30 22:05:15 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 1.0105
2022-01-30 22:05:47 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 0.8384
2022-01-30 22:06:19 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 1.0012
2022-01-30 22:06:49 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 0.9382
2022-01-30 22:07:21 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 0.7584
2022-01-30 22:07:52 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 0.8214
2022-01-30 22:08:25 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 0.9410
2022-01-30 22:08:56 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 0.7976
2022-01-30 22:09:28 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 0.6917
2022-01-30 22:09:59 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 0.7434
2022-01-30 22:10:30 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 0.8837
2022-01-30 22:11:02 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 0.9103
2022-01-30 22:11:34 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 0.7204
2022-01-30 22:12:05 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 0.6049
2022-01-30 22:12:37 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 0.8476
2022-01-30 22:13:09 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 0.8999
2022-01-30 22:13:42 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 0.9034
2022-01-30 22:14:15 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 0.8330
2022-01-30 22:14:47 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 0.8422
2022-01-30 22:15:21 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 1.0728
2022-01-30 22:15:54 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 0.7616
2022-01-30 22:15:55 - train: epoch 097, train_loss: 0.8353
2022-01-30 22:17:10 - eval: epoch: 097, acc1: 76.256%, acc5: 93.104%, test_loss: 0.9421, per_image_load_time: 0.918ms, per_image_inference_time: 0.350ms
2022-01-30 22:17:11 - epoch 098 lr: 0.00010000000000000003
2022-01-30 22:17:47 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 0.9539
2022-01-30 22:18:19 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 0.8142
2022-01-30 22:18:51 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 0.9521
2022-01-30 22:19:23 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 0.7585
2022-01-30 22:19:54 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 0.9330
2022-01-30 22:20:26 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 0.7348
2022-01-30 22:20:58 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 0.8921
2022-01-30 22:21:29 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 0.7537
2022-01-30 22:22:01 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 0.9086
2022-01-30 22:22:32 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 0.7960
2022-01-30 22:23:04 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 0.8501
2022-01-30 22:23:36 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 0.7871
2022-01-30 22:24:08 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 0.8112
2022-01-30 22:24:39 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 0.8903
2022-01-30 22:25:11 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 0.8071
2022-01-30 22:25:42 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 0.8394
2022-01-30 22:26:13 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 0.9320
2022-01-30 22:26:45 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 0.8179
2022-01-30 22:27:17 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 0.7910
2022-01-30 22:27:48 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 0.9498
2022-01-30 22:28:20 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 0.8764
2022-01-30 22:28:51 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 0.7843
2022-01-30 22:29:23 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 0.8032
2022-01-30 22:29:54 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 0.9391
2022-01-30 22:30:26 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 0.8990
2022-01-30 22:30:57 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 0.9102
2022-01-30 22:31:28 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 0.9867
2022-01-30 22:32:00 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 0.8178
2022-01-30 22:32:32 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 0.9743
2022-01-30 22:33:04 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 0.9396
2022-01-30 22:33:36 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 0.7014
2022-01-30 22:34:07 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 0.7449
2022-01-30 22:34:39 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 0.9098
2022-01-30 22:35:10 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 0.7812
2022-01-30 22:35:41 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 0.8894
2022-01-30 22:36:13 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 0.9792
2022-01-30 22:36:44 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 0.8004
2022-01-30 22:37:16 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 0.8651
2022-01-30 22:37:48 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 0.7773
2022-01-30 22:38:21 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 0.9668
2022-01-30 22:38:52 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 0.6270
2022-01-30 22:39:23 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 0.9330
2022-01-30 22:39:55 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 0.7058
2022-01-30 22:40:28 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 0.9414
2022-01-30 22:40:59 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 1.0031
2022-01-30 22:41:32 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 0.8549
2022-01-30 22:42:05 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 0.9426
2022-01-30 22:42:38 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 0.6092
2022-01-30 22:43:10 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 0.8712
2022-01-30 22:43:43 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 0.8028
2022-01-30 22:43:44 - train: epoch 098, train_loss: 0.8354
2022-01-30 22:45:01 - eval: epoch: 098, acc1: 76.248%, acc5: 93.114%, test_loss: 0.9431, per_image_load_time: 0.754ms, per_image_inference_time: 0.341ms
2022-01-30 22:45:01 - epoch 099 lr: 0.00010000000000000003
2022-01-30 22:45:39 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 0.8433
2022-01-30 22:46:11 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 0.6809
2022-01-30 22:46:43 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 0.8654
2022-01-30 22:47:17 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 0.8797
2022-01-30 22:47:49 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 0.9976
2022-01-30 22:48:20 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 0.9396
2022-01-30 22:48:54 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 0.8130
2022-01-30 22:49:26 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 1.0128
2022-01-30 22:49:59 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 0.8205
2022-01-30 22:50:31 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 0.7631
2022-01-30 22:51:03 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 0.8029
2022-01-30 22:51:36 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 0.8358
2022-01-30 22:52:09 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 0.8072
2022-01-30 22:52:44 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 0.8100
2022-01-30 22:53:16 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 0.7967
2022-01-30 22:53:50 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 0.8543
2022-01-30 22:54:24 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 0.8255
2022-01-30 22:54:56 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 0.9032
2022-01-30 22:55:30 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 0.7966
2022-01-30 22:56:03 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 0.8939
2022-01-30 22:56:37 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 0.7442
2022-01-30 22:57:09 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 0.7899
2022-01-30 22:57:41 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 0.8336
2022-01-30 22:58:12 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 1.0435
2022-01-30 22:58:43 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 0.6218
2022-01-30 22:59:17 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 0.8148
2022-01-30 22:59:49 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 0.8766
2022-01-30 23:00:23 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 0.8308
2022-01-30 23:00:55 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 0.7578
2022-01-30 23:01:27 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 0.8458
2022-01-30 23:02:01 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 0.7140
2022-01-30 23:02:33 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 0.7709
2022-01-30 23:03:07 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 0.8253
2022-01-30 23:03:40 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 1.0476
2022-01-30 23:04:12 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 0.8290
2022-01-30 23:04:45 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 0.7395
2022-01-30 23:05:17 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 0.6342
2022-01-30 23:05:51 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 0.7631
2022-01-30 23:06:24 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 0.7978
2022-01-30 23:06:58 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 0.8494
2022-01-30 23:07:30 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 0.6193
2022-01-30 23:08:04 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 0.8762
2022-01-30 23:08:37 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 0.9900
2022-01-30 23:09:10 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 0.8973
2022-01-30 23:09:43 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 0.9171
2022-01-30 23:10:18 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 0.9541
2022-01-30 23:10:53 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 0.8541
2022-01-30 23:11:28 - train: epoch 0099, iter [04800, 05004], lr: 0.000100, loss: 0.8021
2022-01-30 23:12:02 - train: epoch 0099, iter [04900, 05004], lr: 0.000100, loss: 0.9441
2022-01-30 23:12:37 - train: epoch 0099, iter [05000, 05004], lr: 0.000100, loss: 0.8639
2022-01-30 23:12:39 - train: epoch 099, train_loss: 0.8347
2022-01-30 23:13:57 - eval: epoch: 099, acc1: 76.232%, acc5: 93.076%, test_loss: 0.9435, per_image_load_time: 0.628ms, per_image_inference_time: 0.353ms
2022-01-30 23:13:58 - epoch 100 lr: 0.00010000000000000003
2022-01-30 23:14:36 - train: epoch 0100, iter [00100, 05004], lr: 0.000100, loss: 0.9185
2022-01-30 23:15:09 - train: epoch 0100, iter [00200, 05004], lr: 0.000100, loss: 0.7957
2022-01-30 23:15:42 - train: epoch 0100, iter [00300, 05004], lr: 0.000100, loss: 0.6899
2022-01-30 23:16:15 - train: epoch 0100, iter [00400, 05004], lr: 0.000100, loss: 0.8091
2022-01-30 23:16:48 - train: epoch 0100, iter [00500, 05004], lr: 0.000100, loss: 0.7674
2022-01-30 23:17:20 - train: epoch 0100, iter [00600, 05004], lr: 0.000100, loss: 0.9545
2022-01-30 23:17:53 - train: epoch 0100, iter [00700, 05004], lr: 0.000100, loss: 0.7444
2022-01-30 23:18:25 - train: epoch 0100, iter [00800, 05004], lr: 0.000100, loss: 0.7676
2022-01-30 23:18:58 - train: epoch 0100, iter [00900, 05004], lr: 0.000100, loss: 0.6793
2022-01-30 23:19:31 - train: epoch 0100, iter [01000, 05004], lr: 0.000100, loss: 0.7594
2022-01-30 23:20:05 - train: epoch 0100, iter [01100, 05004], lr: 0.000100, loss: 0.6615
2022-01-30 23:20:37 - train: epoch 0100, iter [01200, 05004], lr: 0.000100, loss: 1.0005
2022-01-30 23:21:10 - train: epoch 0100, iter [01300, 05004], lr: 0.000100, loss: 0.7729
2022-01-30 23:21:43 - train: epoch 0100, iter [01400, 05004], lr: 0.000100, loss: 0.8482
2022-01-30 23:22:15 - train: epoch 0100, iter [01500, 05004], lr: 0.000100, loss: 0.7483
2022-01-30 23:22:48 - train: epoch 0100, iter [01600, 05004], lr: 0.000100, loss: 0.7265
2022-01-30 23:23:21 - train: epoch 0100, iter [01700, 05004], lr: 0.000100, loss: 0.7552
2022-01-30 23:23:55 - train: epoch 0100, iter [01800, 05004], lr: 0.000100, loss: 0.8753
2022-01-30 23:24:27 - train: epoch 0100, iter [01900, 05004], lr: 0.000100, loss: 0.7330
2022-01-30 23:25:01 - train: epoch 0100, iter [02000, 05004], lr: 0.000100, loss: 0.6978
2022-01-30 23:25:33 - train: epoch 0100, iter [02100, 05004], lr: 0.000100, loss: 0.7541
2022-01-30 23:26:06 - train: epoch 0100, iter [02200, 05004], lr: 0.000100, loss: 0.7325
2022-01-30 23:26:39 - train: epoch 0100, iter [02300, 05004], lr: 0.000100, loss: 0.9121
2022-01-30 23:27:13 - train: epoch 0100, iter [02400, 05004], lr: 0.000100, loss: 0.9466
2022-01-30 23:27:47 - train: epoch 0100, iter [02500, 05004], lr: 0.000100, loss: 0.8081
2022-01-30 23:28:18 - train: epoch 0100, iter [02600, 05004], lr: 0.000100, loss: 0.6674
2022-01-30 23:28:52 - train: epoch 0100, iter [02700, 05004], lr: 0.000100, loss: 0.7799
2022-01-30 23:29:24 - train: epoch 0100, iter [02800, 05004], lr: 0.000100, loss: 0.7629
2022-01-30 23:29:56 - train: epoch 0100, iter [02900, 05004], lr: 0.000100, loss: 0.8285
2022-01-30 23:30:29 - train: epoch 0100, iter [03000, 05004], lr: 0.000100, loss: 0.8027
2022-01-30 23:31:03 - train: epoch 0100, iter [03100, 05004], lr: 0.000100, loss: 0.7598
2022-01-30 23:31:36 - train: epoch 0100, iter [03200, 05004], lr: 0.000100, loss: 0.7922
2022-01-30 23:32:09 - train: epoch 0100, iter [03300, 05004], lr: 0.000100, loss: 1.0324
2022-01-30 23:32:41 - train: epoch 0100, iter [03400, 05004], lr: 0.000100, loss: 0.9359
2022-01-30 23:33:16 - train: epoch 0100, iter [03500, 05004], lr: 0.000100, loss: 0.7838
2022-01-30 23:33:48 - train: epoch 0100, iter [03600, 05004], lr: 0.000100, loss: 0.8795
2022-01-30 23:34:23 - train: epoch 0100, iter [03700, 05004], lr: 0.000100, loss: 0.8406
2022-01-30 23:34:55 - train: epoch 0100, iter [03800, 05004], lr: 0.000100, loss: 0.7989
2022-01-30 23:35:28 - train: epoch 0100, iter [03900, 05004], lr: 0.000100, loss: 0.7682
2022-01-30 23:36:01 - train: epoch 0100, iter [04000, 05004], lr: 0.000100, loss: 0.8265
2022-01-30 23:36:34 - train: epoch 0100, iter [04100, 05004], lr: 0.000100, loss: 0.8074
2022-01-30 23:37:08 - train: epoch 0100, iter [04200, 05004], lr: 0.000100, loss: 0.8516
2022-01-30 23:37:41 - train: epoch 0100, iter [04300, 05004], lr: 0.000100, loss: 0.8455
2022-01-30 23:38:15 - train: epoch 0100, iter [04400, 05004], lr: 0.000100, loss: 0.8788
2022-01-30 23:38:49 - train: epoch 0100, iter [04500, 05004], lr: 0.000100, loss: 0.6958
2022-01-30 23:39:24 - train: epoch 0100, iter [04600, 05004], lr: 0.000100, loss: 0.8756
2022-01-30 23:39:58 - train: epoch 0100, iter [04700, 05004], lr: 0.000100, loss: 1.0288
2022-01-30 23:40:33 - train: epoch 0100, iter [04800, 05004], lr: 0.000100, loss: 0.8038
2022-01-30 23:41:09 - train: epoch 0100, iter [04900, 05004], lr: 0.000100, loss: 0.7555
2022-01-30 23:41:44 - train: epoch 0100, iter [05000, 05004], lr: 0.000100, loss: 0.8743
2022-01-30 23:41:45 - train: epoch 100, train_loss: 0.8352
2022-01-30 23:43:02 - eval: epoch: 100, acc1: 76.322%, acc5: 93.056%, test_loss: 0.9446, per_image_load_time: 1.089ms, per_image_inference_time: 0.354ms
2022-01-30 23:43:03 - train done. model: resnet50, train time: 47.718 hours, best_acc1: 76.322%
