2022-03-12 22:40:02 - network: yoloxsbackbone
2022-03-12 22:40:02 - num_classes: 1000
2022-03-12 22:40:02 - input_image_size: 256
2022-03-12 22:40:02 - scale: 1.1428571428571428
2022-03-12 22:40:02 - trained_model_path: 
2022-03-12 22:40:02 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-12 22:40:02 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fcaa2967f70>
2022-03-12 22:40:02 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fca85305280>
2022-03-12 22:40:02 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fca853052b0>
2022-03-12 22:40:02 - seed: 0
2022-03-12 22:40:02 - batch_size: 256
2022-03-12 22:40:02 - num_workers: 16
2022-03-12 22:40:02 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-12 22:40:02 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-03-12 22:40:02 - epochs: 100
2022-03-12 22:40:02 - print_interval: 100
2022-03-12 22:40:02 - distributed: True
2022-03-12 22:40:02 - sync_bn: False
2022-03-12 22:40:02 - apex: True
2022-03-12 22:40:02 - gpus_type: NVIDIA RTX A5000
2022-03-12 22:40:02 - gpus_num: 2
2022-03-12 22:40:02 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fca84e4abf0>
2022-03-12 22:40:02 - --------------------parameters--------------------
2022-03-12 22:40:02 - name: conv.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: conv.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: conv.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer1.0.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer1.0.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer1.0.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer1.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer1.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer1.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer1.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer1.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer1.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer2.0.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer2.0.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer2.0.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer3.0.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer3.0.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer3.0.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer4.0.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer4.0.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer4.0.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer4.2.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer4.2.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer4.2.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: layer4.2.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-12 22:40:02 - name: layer4.2.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-12 22:40:02 - name: layer4.2.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-12 22:40:02 - name: fc.weight, grad: True
2022-03-12 22:40:02 - name: fc.bias, grad: True
2022-03-12 22:40:02 - --------------------buffers--------------------
2022-03-12 22:40:02 - name: conv.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: conv.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: conv.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer1.0.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer1.0.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer1.0.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer1.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer1.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer1.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer1.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer1.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer1.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer2.0.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer2.0.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer2.0.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer2.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer3.0.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer3.0.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer3.0.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer3.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer4.0.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer4.0.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer4.0.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer4.2.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer4.2.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer4.2.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - name: layer4.2.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-12 22:40:02 - name: layer4.2.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-12 22:40:02 - name: layer4.2.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-12 22:40:02 - epoch 001 lr: 0.1
2022-03-12 22:40:42 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8828
2022-03-12 22:41:18 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.7898
2022-03-12 22:41:53 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.7230
2022-03-12 22:42:25 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.6941
2022-03-12 22:42:58 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.5801
2022-03-12 22:43:34 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.3901
2022-03-12 22:44:07 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.3151
2022-03-12 22:44:42 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.0759
2022-03-12 22:45:16 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.9454
2022-03-12 22:45:50 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.8681
2022-03-12 22:46:24 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.8166
2022-03-12 22:46:58 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.5612
2022-03-12 22:47:30 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.5291
2022-03-12 22:48:05 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.5638
2022-03-12 22:48:39 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.4134
2022-03-12 22:49:13 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.4692
2022-03-12 22:49:48 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.2021
2022-03-12 22:50:23 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.2722
2022-03-12 22:50:57 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.2094
2022-03-12 22:51:30 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.1286
2022-03-12 22:52:03 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.0417
2022-03-12 22:52:39 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.0830
2022-03-12 22:53:12 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.7855
2022-03-12 22:53:46 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.9479
2022-03-12 22:54:20 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.9230
2022-03-12 22:54:55 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.0024
2022-03-12 22:55:29 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 4.9926
2022-03-12 22:56:03 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.7196
2022-03-12 22:56:35 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.6487
2022-03-12 22:57:11 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.7845
2022-03-12 22:57:45 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.8019
2022-03-12 22:58:19 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.7401
2022-03-12 22:58:54 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.4910
2022-03-12 22:59:27 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.5299
2022-03-12 23:00:03 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.5933
2022-03-12 23:00:37 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.6242
2022-03-12 23:01:10 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.6844
2022-03-12 23:01:42 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.5493
2022-03-12 23:02:16 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.6014
2022-03-12 23:02:50 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.4206
2022-03-12 23:03:25 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.5127
2022-03-12 23:03:58 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.4013
2022-03-12 23:04:33 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.3761
2022-03-12 23:05:07 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.2058
2022-03-12 23:05:38 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.5136
2022-03-12 23:06:13 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.5829
2022-03-12 23:06:47 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.2686
2022-03-12 23:07:20 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.6129
2022-03-12 23:07:55 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.3665
2022-03-12 23:08:27 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.1593
2022-03-12 23:08:28 - train: epoch 001, train_loss: 5.1405
2022-03-12 23:09:48 - eval: epoch: 001, acc1: 17.552%, acc5: 39.228%, test_loss: 4.1307, per_image_load_time: 2.949ms, per_image_inference_time: 0.157ms
2022-03-12 23:09:48 - until epoch: 001, best_acc1: 17.552%
2022-03-12 23:09:48 - epoch 002 lr: 0.1
2022-03-12 23:10:26 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.3652
2022-03-12 23:11:00 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 3.9902
2022-03-12 23:11:35 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.4243
2022-03-12 23:12:08 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.2465
2022-03-12 23:12:43 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.0561
2022-03-12 23:13:18 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.0392
2022-03-12 23:13:52 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.3632
2022-03-12 23:14:26 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.9570
2022-03-12 23:14:57 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.7835
2022-03-12 23:15:32 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.3953
2022-03-12 23:16:05 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.2074
2022-03-12 23:16:39 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.1115
2022-03-12 23:17:13 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.0719
2022-03-12 23:17:48 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.1674
2022-03-12 23:18:22 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.1955
2022-03-12 23:18:57 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.9377
2022-03-12 23:19:28 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.9663
2022-03-12 23:20:02 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.1051
2022-03-12 23:20:36 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.9663
2022-03-12 23:21:11 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.6562
2022-03-12 23:21:45 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.9441
2022-03-12 23:22:19 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.6149
2022-03-12 23:22:54 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.9960
2022-03-12 23:23:28 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.7077
2022-03-12 23:24:01 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.8059
2022-03-12 23:24:33 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.8003
2022-03-12 23:25:07 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 4.0300
2022-03-12 23:25:41 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.9784
2022-03-12 23:26:15 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.7191
2022-03-12 23:26:50 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.7017
2022-03-12 23:27:25 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.7267
2022-03-12 23:27:58 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.8732
2022-03-12 23:28:32 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.7393
2022-03-12 23:29:04 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.8757
2022-03-12 23:29:38 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.7420
2022-03-12 23:30:12 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.6890
2022-03-12 23:30:46 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.8265
2022-03-12 23:31:22 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.5489
2022-03-12 23:31:55 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.8379
2022-03-12 23:32:30 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.6976
2022-03-12 23:33:04 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.8432
2022-03-12 23:33:35 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.6311
2022-03-12 23:34:10 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.5723
2022-03-12 23:34:45 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.4962
2022-03-12 23:35:20 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.5960
2022-03-12 23:35:53 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.6875
2022-03-12 23:36:28 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.5819
2022-03-12 23:37:01 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.6654
2022-03-12 23:37:35 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.4970
2022-03-12 23:38:06 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.5537
2022-03-12 23:38:07 - train: epoch 002, train_loss: 3.8909
2022-03-12 23:39:24 - eval: epoch: 002, acc1: 28.132%, acc5: 53.562%, test_loss: 3.3795, per_image_load_time: 1.150ms, per_image_inference_time: 0.169ms
2022-03-12 23:39:24 - until epoch: 002, best_acc1: 28.132%
2022-03-12 23:39:24 - epoch 003 lr: 0.1
2022-03-12 23:40:06 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.6472
2022-03-12 23:40:41 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.6613
2022-03-12 23:41:14 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.5175
2022-03-12 23:41:49 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.5443
2022-03-12 23:42:22 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.7904
2022-03-12 23:42:55 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.5816
2022-03-12 23:43:29 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.7759
2022-03-12 23:44:03 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.6620
2022-03-12 23:44:37 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.5384
2022-03-12 23:45:11 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.4480
2022-03-12 23:45:44 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.3977
2022-03-12 23:46:18 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.6214
2022-03-12 23:46:53 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.5127
2022-03-12 23:47:24 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.4972
2022-03-12 23:47:58 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.6940
2022-03-12 23:48:32 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.4486
2022-03-12 23:49:07 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.4287
2022-03-12 23:49:42 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.3656
2022-03-12 23:50:16 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.5899
2022-03-12 23:50:50 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.7763
2022-03-12 23:51:24 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.6524
2022-03-12 23:51:56 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.8008
2022-03-12 23:52:30 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.3789
2022-03-12 23:53:04 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.4736
2022-03-12 23:53:37 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.6151
2022-03-12 23:54:10 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.4822
2022-03-12 23:54:45 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.7036
2022-03-12 23:55:19 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.2452
2022-03-12 23:55:54 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.4850
2022-03-12 23:56:26 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.7006
2022-03-12 23:57:00 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.6654
2022-03-12 23:57:33 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.5011
2022-03-12 23:58:06 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.3808
2022-03-12 23:58:41 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.6264
2022-03-12 23:59:14 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.3361
2022-03-12 23:59:49 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.3127
2022-03-13 00:00:22 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.4210
2022-03-13 00:00:55 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.5330
2022-03-13 00:01:27 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.6901
2022-03-13 00:02:01 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.2762
2022-03-13 00:02:35 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.4602
2022-03-13 00:03:08 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.4330
2022-03-13 00:03:42 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 3.1640
2022-03-13 00:04:16 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.2114
2022-03-13 00:04:49 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.3998
2022-03-13 00:05:23 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.2483
2022-03-13 00:05:54 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.2929
2022-03-13 00:06:29 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.4262
2022-03-13 00:07:02 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.4073
2022-03-13 00:07:34 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.4270
2022-03-13 00:07:35 - train: epoch 003, train_loss: 3.4767
2022-03-13 00:08:53 - eval: epoch: 003, acc1: 31.810%, acc5: 58.120%, test_loss: 3.1514, per_image_load_time: 2.887ms, per_image_inference_time: 0.154ms
2022-03-13 00:08:53 - until epoch: 003, best_acc1: 31.810%
2022-03-13 00:08:53 - epoch 004 lr: 0.1
2022-03-13 00:09:35 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.3780
2022-03-13 00:10:07 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.2298
2022-03-13 00:10:40 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.2272
2022-03-13 00:11:15 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 3.3096
2022-03-13 00:11:48 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.2994
2022-03-13 00:12:23 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.5069
2022-03-13 00:12:56 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.3259
2022-03-13 00:13:31 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 3.2020
2022-03-13 00:14:05 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.9754
2022-03-13 00:14:37 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.3793
2022-03-13 00:15:10 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.3524
2022-03-13 00:15:45 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 3.1384
2022-03-13 00:16:19 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 3.1511
2022-03-13 00:16:52 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.3465
2022-03-13 00:17:26 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.4145
2022-03-13 00:18:00 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 3.1344
2022-03-13 00:18:34 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.3721
2022-03-13 00:19:06 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.5377
2022-03-13 00:19:39 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.3151
2022-03-13 00:20:13 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.2439
2022-03-13 00:20:47 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.2431
2022-03-13 00:21:20 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.2362
2022-03-13 00:21:54 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 3.0821
2022-03-13 00:22:29 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 3.0763
2022-03-13 00:23:01 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 3.2225
2022-03-13 00:23:34 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.3436
2022-03-13 00:24:07 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 3.0305
2022-03-13 00:24:41 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.1412
2022-03-13 00:25:16 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 3.2281
2022-03-13 00:25:49 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 3.2040
2022-03-13 00:26:23 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.2966
2022-03-13 00:26:57 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 3.2598
2022-03-13 00:27:31 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.2857
2022-03-13 00:28:05 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 3.2401
2022-03-13 00:28:36 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 3.2000
2022-03-13 00:29:11 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.9801
2022-03-13 00:29:44 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.2085
2022-03-13 00:30:19 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 3.2271
2022-03-13 00:30:52 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 3.0588
2022-03-13 00:31:27 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 3.0200
2022-03-13 00:31:59 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 3.1032
2022-03-13 00:32:33 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 3.1232
2022-03-13 00:33:04 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.9725
2022-03-13 00:33:39 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.9714
2022-03-13 00:34:12 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.7194
2022-03-13 00:34:46 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 3.1582
2022-03-13 00:35:19 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 3.1115
2022-03-13 00:35:54 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 3.0078
2022-03-13 00:36:25 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 3.1798
2022-03-13 00:36:58 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 3.2817
2022-03-13 00:36:59 - train: epoch 004, train_loss: 3.2552
2022-03-13 00:38:14 - eval: epoch: 004, acc1: 35.852%, acc5: 62.196%, test_loss: 2.9249, per_image_load_time: 1.992ms, per_image_inference_time: 0.146ms
2022-03-13 00:38:14 - until epoch: 004, best_acc1: 35.852%
2022-03-13 00:38:14 - epoch 005 lr: 0.1
2022-03-13 00:38:54 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.2152
2022-03-13 00:39:29 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.0822
2022-03-13 00:40:03 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.2628
2022-03-13 00:40:37 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.1471
2022-03-13 00:41:11 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.9741
2022-03-13 00:41:44 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 3.1012
2022-03-13 00:42:16 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 3.1331
2022-03-13 00:42:50 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.3730
2022-03-13 00:43:25 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 3.1484
2022-03-13 00:43:58 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.1394
2022-03-13 00:44:32 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.1686
2022-03-13 00:45:06 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 3.1870
2022-03-13 00:45:40 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 3.1609
2022-03-13 00:46:14 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.3561
2022-03-13 00:46:46 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.9960
2022-03-13 00:47:19 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 3.0382
2022-03-13 00:47:54 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.9764
2022-03-13 00:48:27 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 3.1841
2022-03-13 00:49:02 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.9067
2022-03-13 00:49:35 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 3.1248
2022-03-13 00:50:10 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.9775
2022-03-13 00:50:43 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 3.0814
2022-03-13 00:51:16 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.9755
2022-03-13 00:51:49 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 3.0063
2022-03-13 00:52:23 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 3.1535
2022-03-13 00:52:55 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.3516
2022-03-13 00:53:30 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.1585
2022-03-13 00:54:02 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 3.1628
2022-03-13 00:54:37 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.9077
2022-03-13 00:55:09 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 3.2023
2022-03-13 00:55:42 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.1760
2022-03-13 00:56:15 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 3.1846
2022-03-13 00:56:48 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.9666
2022-03-13 00:57:22 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 3.1230
2022-03-13 00:57:55 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 3.0622
2022-03-13 00:58:28 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 3.1196
2022-03-13 00:59:03 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.9445
2022-03-13 00:59:36 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 3.0016
2022-03-13 01:00:08 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.3843
2022-03-13 01:00:41 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 3.0449
2022-03-13 01:01:16 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 3.0275
2022-03-13 01:01:49 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 3.1259
2022-03-13 01:02:23 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.9837
2022-03-13 01:02:57 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 3.2368
2022-03-13 01:03:31 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 3.1686
2022-03-13 01:04:03 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 3.0922
2022-03-13 01:04:37 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.8370
2022-03-13 01:05:08 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.9502
2022-03-13 01:05:43 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 3.2345
2022-03-13 01:06:14 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.8726
2022-03-13 01:06:15 - train: epoch 005, train_loss: 3.1145
2022-03-13 01:07:33 - eval: epoch: 005, acc1: 37.078%, acc5: 63.960%, test_loss: 2.8310, per_image_load_time: 1.528ms, per_image_inference_time: 0.155ms
2022-03-13 01:07:34 - until epoch: 005, best_acc1: 37.078%
2022-03-13 01:07:34 - epoch 006 lr: 0.1
2022-03-13 01:08:15 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.9843
2022-03-13 01:08:49 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 3.0815
2022-03-13 01:09:20 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.7903
2022-03-13 01:09:54 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.1249
2022-03-13 01:10:27 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 3.0757
2022-03-13 01:11:02 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.9266
2022-03-13 01:11:35 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.0947
2022-03-13 01:12:10 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 3.1980
2022-03-13 01:12:43 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.9944
2022-03-13 01:13:17 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.9383
2022-03-13 01:13:48 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.8717
2022-03-13 01:14:23 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 3.1360
2022-03-13 01:14:56 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.1395
2022-03-13 01:15:30 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 3.0602
2022-03-13 01:16:03 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 3.1823
2022-03-13 01:16:38 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.8993
2022-03-13 01:17:10 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 3.0769
2022-03-13 01:17:45 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 3.1165
2022-03-13 01:18:16 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.9113
2022-03-13 01:18:50 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 3.1510
2022-03-13 01:19:23 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.9947
2022-03-13 01:19:57 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.8963
2022-03-13 01:20:30 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.7873
2022-03-13 01:21:04 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 3.0012
2022-03-13 01:21:37 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 3.2372
2022-03-13 01:22:12 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.9539
2022-03-13 01:22:43 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 3.1300
2022-03-13 01:23:18 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.7909
2022-03-13 01:23:50 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 3.1050
2022-03-13 01:24:24 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.8451
2022-03-13 01:24:56 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.7936
2022-03-13 01:25:31 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.9032
2022-03-13 01:26:04 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.8115
2022-03-13 01:26:37 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 3.2127
2022-03-13 01:27:10 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 3.0586
2022-03-13 01:27:42 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 3.0897
2022-03-13 01:28:15 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.9759
2022-03-13 01:28:50 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.8547
2022-03-13 01:29:23 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.8235
2022-03-13 01:29:57 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.2390
2022-03-13 01:30:31 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.9132
2022-03-13 01:31:05 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.9939
2022-03-13 01:31:37 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.8848
2022-03-13 01:32:09 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.9783
2022-03-13 01:32:43 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 3.0669
2022-03-13 01:33:17 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 3.0002
2022-03-13 01:33:51 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 3.1314
2022-03-13 01:34:25 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.9566
2022-03-13 01:34:59 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 3.0964
2022-03-13 01:35:31 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.8908
2022-03-13 01:35:32 - train: epoch 006, train_loss: 3.0167
2022-03-13 01:36:46 - eval: epoch: 006, acc1: 39.268%, acc5: 66.304%, test_loss: 2.7186, per_image_load_time: 2.431ms, per_image_inference_time: 0.153ms
2022-03-13 01:36:47 - until epoch: 006, best_acc1: 39.268%
2022-03-13 01:36:47 - epoch 007 lr: 0.1
2022-03-13 01:37:27 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.7896
2022-03-13 01:38:02 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 3.2677
2022-03-13 01:38:36 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 3.3393
2022-03-13 01:39:10 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 3.0021
2022-03-13 01:39:43 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.9627
2022-03-13 01:40:18 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 3.0925
2022-03-13 01:40:50 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.9989
2022-03-13 01:41:22 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 3.0661
2022-03-13 01:41:57 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 3.0059
2022-03-13 01:42:29 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.8840
2022-03-13 01:43:04 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.8235
2022-03-13 01:43:37 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.8827
2022-03-13 01:44:11 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.8356
2022-03-13 01:44:46 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.9535
2022-03-13 01:45:19 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 3.0282
2022-03-13 01:45:50 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.9156
2022-03-13 01:46:24 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.9775
2022-03-13 01:46:58 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.7887
2022-03-13 01:47:32 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.9801
2022-03-13 01:48:05 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.7171
2022-03-13 01:48:40 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 3.0527
2022-03-13 01:49:13 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.8174
2022-03-13 01:49:47 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 3.0509
2022-03-13 01:50:18 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.9861
2022-03-13 01:50:52 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.8327
2022-03-13 01:51:25 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.8406
2022-03-13 01:51:59 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.8992
2022-03-13 01:52:32 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.9650
2022-03-13 01:53:07 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.8682
2022-03-13 01:53:40 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 3.1060
2022-03-13 01:54:15 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.7542
2022-03-13 01:54:46 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.8563
2022-03-13 01:55:20 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 3.2577
2022-03-13 01:55:53 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.8259
2022-03-13 01:56:27 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.9073
2022-03-13 01:57:01 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.8105
2022-03-13 01:57:34 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 3.0805
2022-03-13 01:58:08 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 3.1355
2022-03-13 01:58:41 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.8746
2022-03-13 01:59:15 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.9739
2022-03-13 01:59:46 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.7780
2022-03-13 02:00:20 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.6920
2022-03-13 02:00:54 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 3.1160
2022-03-13 02:01:28 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.8029
2022-03-13 02:02:03 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 3.0430
2022-03-13 02:02:35 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.7768
2022-03-13 02:03:10 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.9813
2022-03-13 02:03:43 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 3.1911
2022-03-13 02:04:15 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.8243
2022-03-13 02:04:48 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.9599
2022-03-13 02:04:49 - train: epoch 007, train_loss: 2.9416
2022-03-13 02:06:05 - eval: epoch: 007, acc1: 39.430%, acc5: 66.902%, test_loss: 2.6798, per_image_load_time: 2.500ms, per_image_inference_time: 0.151ms
2022-03-13 02:06:05 - until epoch: 007, best_acc1: 39.430%
2022-03-13 02:06:05 - epoch 008 lr: 0.1
2022-03-13 02:06:45 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.8899
2022-03-13 02:07:19 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.8926
2022-03-13 02:07:52 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.6484
2022-03-13 02:08:25 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.7519
2022-03-13 02:08:57 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.6592
2022-03-13 02:09:30 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.8737
2022-03-13 02:10:04 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 3.1809
2022-03-13 02:10:37 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.7782
2022-03-13 02:11:11 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.9230
2022-03-13 02:11:45 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.9299
2022-03-13 02:12:19 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.7754
2022-03-13 02:12:51 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.7009
2022-03-13 02:13:23 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 3.0167
2022-03-13 02:13:57 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.7647
2022-03-13 02:14:30 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 3.0331
2022-03-13 02:15:05 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.9316
2022-03-13 02:15:39 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.7258
2022-03-13 02:16:12 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 3.0648
2022-03-13 02:16:45 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.7236
2022-03-13 02:17:18 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.9397
2022-03-13 02:17:51 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.8702
2022-03-13 02:18:25 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.8123
2022-03-13 02:18:58 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.8933
2022-03-13 02:19:32 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.7977
2022-03-13 02:20:04 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.9007
2022-03-13 02:20:39 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.8978
2022-03-13 02:21:13 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 3.0692
2022-03-13 02:21:48 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.9223
2022-03-13 02:22:18 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.9373
2022-03-13 02:22:52 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 3.0836
2022-03-13 02:23:25 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.8489
2022-03-13 02:24:00 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 3.0338
2022-03-13 02:24:32 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 3.0484
2022-03-13 02:25:07 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 3.0857
2022-03-13 02:25:40 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.8361
2022-03-13 02:26:15 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.9836
2022-03-13 02:26:47 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.8979
2022-03-13 02:27:19 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.7741
2022-03-13 02:27:51 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.8977
2022-03-13 02:28:26 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 3.2552
2022-03-13 02:28:58 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.7675
2022-03-13 02:29:32 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.8264
2022-03-13 02:30:07 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.6237
2022-03-13 02:30:41 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.8295
2022-03-13 02:31:13 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 3.0539
2022-03-13 02:31:45 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 3.0108
2022-03-13 02:32:18 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.8011
2022-03-13 02:32:51 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.8617
2022-03-13 02:33:24 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.9135
2022-03-13 02:33:57 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.7675
2022-03-13 02:33:58 - train: epoch 008, train_loss: 2.8839
2022-03-13 02:35:15 - eval: epoch: 008, acc1: 42.072%, acc5: 69.064%, test_loss: 2.5549, per_image_load_time: 1.056ms, per_image_inference_time: 0.156ms
2022-03-13 02:35:15 - until epoch: 008, best_acc1: 42.072%
2022-03-13 02:35:15 - epoch 009 lr: 0.1
2022-03-13 02:35:53 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.6291
2022-03-13 02:36:26 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.7213
2022-03-13 02:36:58 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.6516
2022-03-13 02:37:33 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 3.0302
2022-03-13 02:38:06 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.8628
2022-03-13 02:38:40 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.9182
2022-03-13 02:39:14 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.8147
2022-03-13 02:39:48 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.6044
2022-03-13 02:40:21 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.8169
2022-03-13 02:40:53 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.5946
2022-03-13 02:41:26 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 3.1095
2022-03-13 02:42:00 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.9771
2022-03-13 02:42:32 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.9060
2022-03-13 02:43:06 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.6270
2022-03-13 02:43:40 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.7072
2022-03-13 02:44:13 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.8600
2022-03-13 02:44:46 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.9458
2022-03-13 02:45:18 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.7351
2022-03-13 02:45:50 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.6202
2022-03-13 02:46:25 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.6723
2022-03-13 02:46:59 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.9158
2022-03-13 02:47:33 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 3.0239
2022-03-13 02:48:07 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.5578
2022-03-13 02:48:40 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.7423
2022-03-13 02:49:14 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.6032
2022-03-13 02:49:45 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.8795
2022-03-13 02:50:19 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.7926
2022-03-13 02:50:52 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.9306
2022-03-13 02:51:26 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.4413
2022-03-13 02:52:00 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.8167
2022-03-13 02:52:33 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.8612
2022-03-13 02:53:08 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.8443
2022-03-13 02:53:41 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.8539
2022-03-13 02:54:12 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.9686
2022-03-13 02:54:46 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.9195
2022-03-13 02:55:19 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.7518
2022-03-13 02:55:53 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.9491
2022-03-13 02:56:26 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.9965
2022-03-13 02:57:00 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.6304
2022-03-13 02:57:33 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 3.0039
2022-03-13 02:58:07 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.9205
2022-03-13 02:58:38 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.7392
2022-03-13 02:59:13 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.9658
2022-03-13 02:59:45 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.8836
2022-03-13 03:00:19 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.9626
2022-03-13 03:00:52 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.8156
2022-03-13 03:01:26 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.8869
2022-03-13 03:02:00 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 3.0358
2022-03-13 03:02:34 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.9153
2022-03-13 03:03:04 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.6957
2022-03-13 03:03:05 - train: epoch 009, train_loss: 2.8371
2022-03-13 03:04:19 - eval: epoch: 009, acc1: 42.404%, acc5: 69.488%, test_loss: 2.5375, per_image_load_time: 2.013ms, per_image_inference_time: 0.165ms
2022-03-13 03:04:20 - until epoch: 009, best_acc1: 42.404%
2022-03-13 03:04:20 - epoch 010 lr: 0.1
2022-03-13 03:04:59 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.8380
2022-03-13 03:05:32 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.9359
2022-03-13 03:06:06 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.8769
2022-03-13 03:06:40 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.8721
2022-03-13 03:07:14 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.6503
2022-03-13 03:07:45 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.9342
2022-03-13 03:08:19 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.8469
2022-03-13 03:08:53 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.7350
2022-03-13 03:09:27 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.6368
2022-03-13 03:10:00 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.5656
2022-03-13 03:10:35 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.8633
2022-03-13 03:11:08 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.6501
2022-03-13 03:11:42 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.6835
2022-03-13 03:12:14 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.8466
2022-03-13 03:12:46 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.5413
2022-03-13 03:13:20 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.9158
2022-03-13 03:13:54 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 3.0677
2022-03-13 03:14:27 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.7446
2022-03-13 03:15:01 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.8190
2022-03-13 03:15:35 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.8254
2022-03-13 03:16:09 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.6347
2022-03-13 03:16:41 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 3.0041
2022-03-13 03:17:12 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.9103
2022-03-13 03:17:47 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 3.0544
2022-03-13 03:18:20 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.8850
2022-03-13 03:18:53 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.7940
2022-03-13 03:19:27 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.6255
2022-03-13 03:20:00 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.8185
2022-03-13 03:20:34 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.9000
2022-03-13 03:21:08 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.6929
2022-03-13 03:21:39 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.8774
2022-03-13 03:22:13 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.8121
2022-03-13 03:22:46 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.8635
2022-03-13 03:23:20 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.9878
2022-03-13 03:23:53 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.9589
2022-03-13 03:24:27 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 3.0023
2022-03-13 03:24:59 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.7158
2022-03-13 03:25:34 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.8293
2022-03-13 03:26:06 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.4663
2022-03-13 03:26:38 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.7694
2022-03-13 03:27:13 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.7492
2022-03-13 03:27:45 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 3.0052
2022-03-13 03:28:19 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.8428
2022-03-13 03:28:52 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.7860
2022-03-13 03:29:26 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.6262
2022-03-13 03:29:59 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.8133
2022-03-13 03:30:32 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.7578
2022-03-13 03:31:04 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.7043
2022-03-13 03:31:38 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.4973
2022-03-13 03:32:10 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.6024
2022-03-13 03:32:11 - train: epoch 010, train_loss: 2.7953
2022-03-13 03:33:27 - eval: epoch: 010, acc1: 41.976%, acc5: 68.964%, test_loss: 2.5519, per_image_load_time: 1.460ms, per_image_inference_time: 0.143ms
2022-03-13 03:33:27 - until epoch: 010, best_acc1: 42.404%
2022-03-13 03:33:27 - epoch 011 lr: 0.1
2022-03-13 03:34:06 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.6023
2022-03-13 03:34:40 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.9012
2022-03-13 03:35:11 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.5724
2022-03-13 03:35:43 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.8643
2022-03-13 03:36:17 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.7461
2022-03-13 03:36:51 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.7641
2022-03-13 03:37:24 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.7629
2022-03-13 03:37:58 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.7953
2022-03-13 03:38:31 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.8828
2022-03-13 03:39:06 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.6419
2022-03-13 03:39:36 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.8622
2022-03-13 03:40:09 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 3.0472
2022-03-13 03:40:43 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.8050
2022-03-13 03:41:17 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.7953
2022-03-13 03:41:50 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.5702
2022-03-13 03:42:24 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.7513
2022-03-13 03:42:57 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.7445
2022-03-13 03:43:31 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.6932
2022-03-13 03:44:03 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.7389
2022-03-13 03:44:35 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.9128
2022-03-13 03:45:08 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.8306
2022-03-13 03:45:42 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.7503
2022-03-13 03:46:15 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.9211
2022-03-13 03:46:48 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.5914
2022-03-13 03:47:22 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.9941
2022-03-13 03:47:55 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.7231
2022-03-13 03:48:27 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.7204
2022-03-13 03:49:00 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.5176
2022-03-13 03:49:33 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.8385
2022-03-13 03:50:07 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 3.0809
2022-03-13 03:50:40 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.8081
2022-03-13 03:51:14 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.4606
2022-03-13 03:51:47 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.8798
2022-03-13 03:52:21 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.7708
2022-03-13 03:52:53 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.8524
2022-03-13 03:53:25 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.8475
2022-03-13 03:53:59 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 3.0105
2022-03-13 03:54:33 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.3941
2022-03-13 03:55:06 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.7970
2022-03-13 03:55:40 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.7634
2022-03-13 03:56:13 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.5966
2022-03-13 03:56:46 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.6630
2022-03-13 03:57:20 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.8169
2022-03-13 03:57:50 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.7138
2022-03-13 03:58:24 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.4523
2022-03-13 03:58:58 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.6998
2022-03-13 03:59:32 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.4471
2022-03-13 04:00:05 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.5578
2022-03-13 04:00:38 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.5528
2022-03-13 04:01:10 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.7223
2022-03-13 04:01:11 - train: epoch 011, train_loss: 2.7631
2022-03-13 04:02:24 - eval: epoch: 011, acc1: 43.828%, acc5: 70.586%, test_loss: 2.4613, per_image_load_time: 1.230ms, per_image_inference_time: 0.157ms
2022-03-13 04:02:24 - until epoch: 011, best_acc1: 43.828%
2022-03-13 04:02:24 - epoch 012 lr: 0.1
2022-03-13 04:03:04 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.6985
2022-03-13 04:03:37 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.5909
2022-03-13 04:04:12 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.7353
2022-03-13 04:04:45 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.8784
2022-03-13 04:05:18 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.9822
2022-03-13 04:05:52 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.4321
2022-03-13 04:06:25 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.6364
2022-03-13 04:06:56 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.7486
2022-03-13 04:07:31 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.8517
2022-03-13 04:08:04 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.4964
2022-03-13 04:08:38 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 3.1101
2022-03-13 04:09:11 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.6092
2022-03-13 04:09:45 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.6704
2022-03-13 04:10:19 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.7450
2022-03-13 04:10:51 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.5603
2022-03-13 04:11:23 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.6466
2022-03-13 04:11:56 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.6288
2022-03-13 04:12:30 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.7116
2022-03-13 04:13:02 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.7960
2022-03-13 04:13:36 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.9318
2022-03-13 04:14:10 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.8390
2022-03-13 04:14:43 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.8530
2022-03-13 04:15:17 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.7124
2022-03-13 04:15:48 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.7035
2022-03-13 04:16:22 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.5091
2022-03-13 04:16:56 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.5157
2022-03-13 04:17:28 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.7568
2022-03-13 04:18:02 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.7384
2022-03-13 04:18:35 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.6307
2022-03-13 04:19:09 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.6369
2022-03-13 04:19:43 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.8440
2022-03-13 04:20:14 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.4515
2022-03-13 04:20:48 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.6259
2022-03-13 04:21:21 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.7729
2022-03-13 04:21:54 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.9393
2022-03-13 04:22:29 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.7715
2022-03-13 04:23:02 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.7555
2022-03-13 04:23:36 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.6741
2022-03-13 04:24:10 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.6297
2022-03-13 04:24:43 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.6713
2022-03-13 04:25:15 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.6670
2022-03-13 04:25:49 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.6034
2022-03-13 04:26:22 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.7477
2022-03-13 04:26:57 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.5672
2022-03-13 04:27:29 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.6711
2022-03-13 04:28:04 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 3.0856
2022-03-13 04:28:37 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.5652
2022-03-13 04:29:09 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.8800
2022-03-13 04:29:41 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.7726
2022-03-13 04:30:13 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.4318
2022-03-13 04:30:14 - train: epoch 012, train_loss: 2.7335
2022-03-13 04:31:30 - eval: epoch: 012, acc1: 44.118%, acc5: 70.940%, test_loss: 2.4426, per_image_load_time: 2.263ms, per_image_inference_time: 0.142ms
2022-03-13 04:31:30 - until epoch: 012, best_acc1: 44.118%
2022-03-13 04:31:30 - epoch 013 lr: 0.1
2022-03-13 04:32:09 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.4676
2022-03-13 04:32:43 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.7130
2022-03-13 04:33:16 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.6501
2022-03-13 04:33:47 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.5707
2022-03-13 04:34:20 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.6330
2022-03-13 04:34:53 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.7920
2022-03-13 04:35:27 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.4474
2022-03-13 04:36:00 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.9090
2022-03-13 04:36:34 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.6762
2022-03-13 04:37:07 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.7915
2022-03-13 04:37:40 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.7708
2022-03-13 04:38:12 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.9296
2022-03-13 04:38:44 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.7640
2022-03-13 04:39:19 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.6758
2022-03-13 04:39:52 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.8412
2022-03-13 04:40:26 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.4618
2022-03-13 04:40:59 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.8394
2022-03-13 04:41:33 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.6282
2022-03-13 04:42:06 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.6902
2022-03-13 04:42:38 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.9261
2022-03-13 04:43:11 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.8611
2022-03-13 04:43:45 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.6154
2022-03-13 04:44:18 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.6324
2022-03-13 04:44:51 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.7622
2022-03-13 04:45:24 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.6856
2022-03-13 04:45:59 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.5266
2022-03-13 04:46:32 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.5518
2022-03-13 04:47:05 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.8317
2022-03-13 04:47:35 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.7281
2022-03-13 04:48:10 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.6788
2022-03-13 04:48:42 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.5746
2022-03-13 04:49:16 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.6647
2022-03-13 04:49:49 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.5761
2022-03-13 04:50:23 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.6846
2022-03-13 04:50:55 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.5909
2022-03-13 04:51:29 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.9414
2022-03-13 04:52:00 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.4371
2022-03-13 04:52:34 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.8360
2022-03-13 04:53:07 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.8106
2022-03-13 04:53:41 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.6965
2022-03-13 04:54:14 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.6952
2022-03-13 04:54:48 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.7232
2022-03-13 04:55:20 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.7227
2022-03-13 04:55:53 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.5587
2022-03-13 04:56:24 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.7927
2022-03-13 04:56:59 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.8190
2022-03-13 04:57:31 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.8364
2022-03-13 04:58:04 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.7086
2022-03-13 04:58:38 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.8449
2022-03-13 04:59:10 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.7737
2022-03-13 04:59:11 - train: epoch 013, train_loss: 2.7107
2022-03-13 05:00:26 - eval: epoch: 013, acc1: 45.698%, acc5: 72.190%, test_loss: 2.3647, per_image_load_time: 1.928ms, per_image_inference_time: 0.160ms
2022-03-13 05:00:26 - until epoch: 013, best_acc1: 45.698%
2022-03-13 05:00:26 - epoch 014 lr: 0.1
2022-03-13 05:01:03 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.6258
2022-03-13 05:01:36 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.8758
2022-03-13 05:02:10 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.3627
2022-03-13 05:02:44 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.4930
2022-03-13 05:03:18 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.6205
2022-03-13 05:03:51 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.7990
2022-03-13 05:04:24 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.5826
2022-03-13 05:04:56 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.6632
2022-03-13 05:05:28 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.8800
2022-03-13 05:06:02 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.8267
2022-03-13 05:06:35 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.6550
2022-03-13 05:07:09 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.8157
2022-03-13 05:07:43 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.7235
2022-03-13 05:08:16 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.6978
2022-03-13 05:08:51 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.9134
2022-03-13 05:09:24 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.6266
2022-03-13 05:09:56 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.7577
2022-03-13 05:10:29 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.8874
2022-03-13 05:11:02 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.4858
2022-03-13 05:11:36 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.6479
2022-03-13 05:12:10 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.9095
2022-03-13 05:12:43 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.6861
2022-03-13 05:13:16 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.7114
2022-03-13 05:13:49 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.8792
2022-03-13 05:14:21 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.6409
2022-03-13 05:14:53 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.8060
2022-03-13 05:15:27 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.6748
2022-03-13 05:15:59 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.9423
2022-03-13 05:16:33 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.6435
2022-03-13 05:17:06 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.8297
2022-03-13 05:17:39 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.6318
2022-03-13 05:18:11 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.5847
2022-03-13 05:18:44 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.4713
2022-03-13 05:19:17 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.6430
2022-03-13 05:19:50 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.7547
2022-03-13 05:20:22 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.5252
2022-03-13 05:20:56 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.5091
2022-03-13 05:21:30 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.8440
2022-03-13 05:22:02 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.5708
2022-03-13 05:22:36 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.9077
2022-03-13 05:23:07 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.6387
2022-03-13 05:23:40 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.5522
2022-03-13 05:24:12 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.5306
2022-03-13 05:24:46 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.4464
2022-03-13 05:25:20 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.7821
2022-03-13 05:25:52 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.6178
2022-03-13 05:26:26 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.6574
2022-03-13 05:26:59 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.5670
2022-03-13 05:27:31 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.4709
2022-03-13 05:28:02 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.6636
2022-03-13 05:28:03 - train: epoch 014, train_loss: 2.6891
2022-03-13 05:29:19 - eval: epoch: 014, acc1: 44.168%, acc5: 71.428%, test_loss: 2.4380, per_image_load_time: 1.405ms, per_image_inference_time: 0.154ms
2022-03-13 05:29:19 - until epoch: 014, best_acc1: 45.698%
2022-03-13 05:29:19 - epoch 015 lr: 0.1
2022-03-13 05:29:58 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.5258
2022-03-13 05:30:33 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.7961
2022-03-13 05:31:06 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.8869
2022-03-13 05:31:38 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.6190
2022-03-13 05:32:10 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.5742
2022-03-13 05:32:43 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.8140
2022-03-13 05:33:17 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.5984
2022-03-13 05:33:51 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.3441
2022-03-13 05:34:25 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.5658
2022-03-13 05:34:58 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.8054
2022-03-13 05:35:31 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.5666
2022-03-13 05:36:05 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.7658
2022-03-13 05:36:37 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.8745
2022-03-13 05:37:09 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.6181
2022-03-13 05:37:43 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.4500
2022-03-13 05:38:17 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.6423
2022-03-13 05:38:51 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.8651
2022-03-13 05:39:24 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.4752
2022-03-13 05:39:58 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.6585
2022-03-13 05:40:32 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.5480
2022-03-13 05:41:05 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.4537
2022-03-13 05:41:36 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.6932
2022-03-13 05:42:10 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.5422
2022-03-13 05:42:43 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.7487
2022-03-13 05:43:18 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.7846
2022-03-13 05:43:51 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.3834
2022-03-13 05:44:24 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.6583
2022-03-13 05:44:58 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.8162
2022-03-13 05:45:30 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.5548
2022-03-13 05:46:02 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.4078
2022-03-13 05:46:35 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.5871
2022-03-13 05:47:08 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.6872
2022-03-13 05:47:41 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.5493
2022-03-13 05:48:16 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.7268
2022-03-13 05:48:49 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.9417
2022-03-13 05:49:22 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.7079
2022-03-13 05:49:55 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.5489
2022-03-13 05:50:27 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.5963
2022-03-13 05:51:00 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.7957
2022-03-13 05:51:34 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.6505
2022-03-13 05:52:08 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.8726
2022-03-13 05:52:40 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.3905
2022-03-13 05:53:14 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.7451
2022-03-13 05:53:48 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.5183
2022-03-13 05:54:20 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.6485
2022-03-13 05:54:52 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.6324
2022-03-13 05:55:25 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.9078
2022-03-13 05:55:59 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.6951
2022-03-13 05:56:32 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.7483
2022-03-13 05:57:04 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.8571
2022-03-13 05:57:05 - train: epoch 015, train_loss: 2.6726
2022-03-13 05:58:21 - eval: epoch: 015, acc1: 45.424%, acc5: 72.108%, test_loss: 2.3666, per_image_load_time: 0.988ms, per_image_inference_time: 0.145ms
2022-03-13 05:58:21 - until epoch: 015, best_acc1: 45.698%
2022-03-13 05:58:21 - epoch 016 lr: 0.1
2022-03-13 05:58:58 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.5653
2022-03-13 05:59:32 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.3535
2022-03-13 06:00:05 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.5982
2022-03-13 06:00:38 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.9431
2022-03-13 06:01:12 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.3930
2022-03-13 06:01:46 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.7633
2022-03-13 06:02:20 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.3745
2022-03-13 06:02:53 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.6959
2022-03-13 06:03:25 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.8620
2022-03-13 06:03:57 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.5123
2022-03-13 06:04:31 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.5518
2022-03-13 06:05:04 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.4574
2022-03-13 06:05:38 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.8029
2022-03-13 06:06:11 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.6268
2022-03-13 06:06:45 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.7330
2022-03-13 06:07:17 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.7086
2022-03-13 06:07:50 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.6257
2022-03-13 06:08:22 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.6235
2022-03-13 06:08:55 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.7071
2022-03-13 06:09:30 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.3336
2022-03-13 06:10:04 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.7238
2022-03-13 06:10:37 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.7207
2022-03-13 06:11:10 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.7781
2022-03-13 06:11:44 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.8043
2022-03-13 06:12:16 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.6062
2022-03-13 06:12:48 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.8107
2022-03-13 06:13:20 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.5988
2022-03-13 06:13:55 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.3976
2022-03-13 06:14:28 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.6914
2022-03-13 06:15:02 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.8956
2022-03-13 06:15:35 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.7827
2022-03-13 06:16:09 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.6711
2022-03-13 06:16:42 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.8695
2022-03-13 06:17:14 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.6818
2022-03-13 06:17:47 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.5263
2022-03-13 06:18:22 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.4843
2022-03-13 06:18:54 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.7675
2022-03-13 06:19:29 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 3.0816
2022-03-13 06:20:01 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.6988
2022-03-13 06:20:35 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.6723
2022-03-13 06:21:08 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.6416
2022-03-13 06:21:39 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.7393
2022-03-13 06:22:13 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.4613
2022-03-13 06:22:47 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.5719
2022-03-13 06:23:20 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.7134
2022-03-13 06:23:53 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.5976
2022-03-13 06:24:28 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.9227
2022-03-13 06:25:00 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.4981
2022-03-13 06:25:33 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.8062
2022-03-13 06:26:03 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.7246
2022-03-13 06:26:04 - train: epoch 016, train_loss: 2.6566
2022-03-13 06:27:19 - eval: epoch: 016, acc1: 45.830%, acc5: 72.260%, test_loss: 2.3580, per_image_load_time: 2.639ms, per_image_inference_time: 0.153ms
2022-03-13 06:27:19 - until epoch: 016, best_acc1: 45.830%
2022-03-13 06:27:19 - epoch 017 lr: 0.1
2022-03-13 06:27:57 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.5084
2022-03-13 06:28:31 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.8316
2022-03-13 06:29:04 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.8405
2022-03-13 06:29:37 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.2931
2022-03-13 06:30:10 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.6808
2022-03-13 06:30:42 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.9164
2022-03-13 06:31:16 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.6793
2022-03-13 06:31:48 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.6235
2022-03-13 06:32:22 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.6058
2022-03-13 06:32:56 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.6393
2022-03-13 06:33:28 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 3.0135
2022-03-13 06:34:02 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.7392
2022-03-13 06:34:34 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.7131
2022-03-13 06:35:07 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.6390
2022-03-13 06:35:39 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.3039
2022-03-13 06:36:13 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.4204
2022-03-13 06:36:47 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.5592
2022-03-13 06:37:20 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.8056
2022-03-13 06:37:53 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.4566
2022-03-13 06:38:26 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.8392
2022-03-13 06:38:59 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.6632
2022-03-13 06:39:30 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.5469
2022-03-13 06:40:05 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.5512
2022-03-13 06:40:37 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.5132
2022-03-13 06:41:11 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.8849
2022-03-13 06:41:44 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.5573
2022-03-13 06:42:19 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.5483
2022-03-13 06:42:51 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.9299
2022-03-13 06:43:25 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.9092
2022-03-13 06:43:56 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.3341
2022-03-13 06:44:30 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.8464
2022-03-13 06:45:03 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.5190
2022-03-13 06:45:36 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.7381
2022-03-13 06:46:09 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.5431
2022-03-13 06:46:43 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.6216
2022-03-13 06:47:17 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.8225
2022-03-13 06:47:50 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.6096
2022-03-13 06:48:23 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.8076
2022-03-13 06:48:55 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.5100
2022-03-13 06:49:28 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.4628
2022-03-13 06:50:02 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.6797
2022-03-13 06:50:35 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.6053
2022-03-13 06:51:09 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.6046
2022-03-13 06:51:42 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.6148
2022-03-13 06:52:15 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.6975
2022-03-13 06:52:47 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.6067
2022-03-13 06:53:21 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.7417
2022-03-13 06:53:53 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.7851
2022-03-13 06:54:28 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.5213
2022-03-13 06:54:59 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.5276
2022-03-13 06:55:00 - train: epoch 017, train_loss: 2.6431
2022-03-13 06:56:17 - eval: epoch: 017, acc1: 45.896%, acc5: 72.804%, test_loss: 2.3437, per_image_load_time: 0.870ms, per_image_inference_time: 0.143ms
2022-03-13 06:56:17 - until epoch: 017, best_acc1: 45.896%
2022-03-13 06:56:17 - epoch 018 lr: 0.1
2022-03-13 06:56:55 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.5807
2022-03-13 06:57:28 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.5208
2022-03-13 06:58:01 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.8929
2022-03-13 06:58:35 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.7732
2022-03-13 06:59:08 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.4658
2022-03-13 06:59:42 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.8060
2022-03-13 07:00:15 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.4063
2022-03-13 07:00:49 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.5846
2022-03-13 07:01:22 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.6162
2022-03-13 07:01:54 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.5335
2022-03-13 07:02:26 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.8921
2022-03-13 07:03:00 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.7606
2022-03-13 07:03:34 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.9045
2022-03-13 07:04:08 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.7231
2022-03-13 07:04:41 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.7609
2022-03-13 07:05:15 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.5553
2022-03-13 07:05:48 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.4984
2022-03-13 07:06:20 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.4502
2022-03-13 07:06:53 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.5797
2022-03-13 07:07:28 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.9945
2022-03-13 07:08:01 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.8314
2022-03-13 07:08:35 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.6530
2022-03-13 07:09:08 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.7214
2022-03-13 07:09:42 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.4764
2022-03-13 07:10:15 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.3185
2022-03-13 07:10:47 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.3223
2022-03-13 07:11:20 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.8344
2022-03-13 07:11:54 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.4491
2022-03-13 07:12:26 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.6001
2022-03-13 07:13:00 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.4143
2022-03-13 07:13:33 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.9064
2022-03-13 07:14:07 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.4293
2022-03-13 07:14:40 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.6010
2022-03-13 07:15:13 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.5680
2022-03-13 07:15:45 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.6867
2022-03-13 07:16:19 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.7503
2022-03-13 07:16:52 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 3.0007
2022-03-13 07:17:26 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.6702
2022-03-13 07:17:59 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.6039
2022-03-13 07:18:33 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.5563
2022-03-13 07:19:06 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.4964
2022-03-13 07:19:38 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.5750
2022-03-13 07:20:10 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.3258
2022-03-13 07:20:43 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.6972
2022-03-13 07:21:16 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.6209
2022-03-13 07:21:51 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.4220
2022-03-13 07:22:23 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.8194
2022-03-13 07:22:58 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.7516
2022-03-13 07:23:30 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.5808
2022-03-13 07:24:02 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.7509
2022-03-13 07:24:03 - train: epoch 018, train_loss: 2.6285
2022-03-13 07:25:16 - eval: epoch: 018, acc1: 46.226%, acc5: 72.822%, test_loss: 2.3370, per_image_load_time: 1.883ms, per_image_inference_time: 0.148ms
2022-03-13 07:25:17 - until epoch: 018, best_acc1: 46.226%
2022-03-13 07:25:17 - epoch 019 lr: 0.1
2022-03-13 07:25:55 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.4493
2022-03-13 07:26:29 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.8669
2022-03-13 07:27:02 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.7941
2022-03-13 07:27:35 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.4745
2022-03-13 07:28:09 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.6145
2022-03-13 07:28:39 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.6535
2022-03-13 07:29:13 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.2706
2022-03-13 07:29:46 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.9384
2022-03-13 07:30:20 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.6247
2022-03-13 07:30:53 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.7322
2022-03-13 07:31:25 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.4321
2022-03-13 07:31:59 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.6105
2022-03-13 07:32:31 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.7186
2022-03-13 07:33:04 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.5986
2022-03-13 07:33:36 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 3.0649
2022-03-13 07:34:10 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.5358
2022-03-13 07:34:43 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.7430
2022-03-13 07:35:16 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.4935
2022-03-13 07:35:49 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.8601
2022-03-13 07:36:24 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.6145
2022-03-13 07:36:55 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.5977
2022-03-13 07:37:27 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.4961
2022-03-13 07:38:00 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.5612
2022-03-13 07:38:34 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.7671
2022-03-13 07:39:07 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.5438
2022-03-13 07:39:41 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.6642
2022-03-13 07:40:12 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.7591
2022-03-13 07:40:47 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.6520
2022-03-13 07:41:19 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.6913
2022-03-13 07:41:51 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.8971
2022-03-13 07:42:24 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.7053
2022-03-13 07:42:57 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.3820
2022-03-13 07:43:30 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.5927
2022-03-13 07:44:04 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.6966
2022-03-13 07:44:37 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.6524
2022-03-13 07:45:10 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.5166
2022-03-13 07:45:44 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.6694
2022-03-13 07:46:15 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.7452
2022-03-13 07:46:48 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.4715
2022-03-13 07:47:21 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.5787
2022-03-13 07:47:55 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.6311
2022-03-13 07:48:27 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.5780
2022-03-13 07:49:01 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.4579
2022-03-13 07:49:34 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.6850
2022-03-13 07:50:07 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.9894
2022-03-13 07:50:40 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.7010
2022-03-13 07:51:10 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.6425
2022-03-13 07:51:46 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.6455
2022-03-13 07:52:18 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.6505
2022-03-13 07:52:50 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.4989
2022-03-13 07:52:51 - train: epoch 019, train_loss: 2.6205
2022-03-13 07:54:07 - eval: epoch: 019, acc1: 46.198%, acc5: 72.914%, test_loss: 2.3292, per_image_load_time: 0.945ms, per_image_inference_time: 0.145ms
2022-03-13 07:54:07 - until epoch: 019, best_acc1: 46.226%
2022-03-13 07:54:07 - epoch 020 lr: 0.1
2022-03-13 07:54:47 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.7991
2022-03-13 07:55:18 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.4250
2022-03-13 07:55:51 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.6890
2022-03-13 07:56:24 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.2910
2022-03-13 07:56:58 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.5411
2022-03-13 07:57:32 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.8259
2022-03-13 07:58:05 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.4935
2022-03-13 07:58:39 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.6055
2022-03-13 07:59:12 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.9124
2022-03-13 07:59:45 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.7224
2022-03-13 08:00:18 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.4223
2022-03-13 08:00:51 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.3039
2022-03-13 08:01:26 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.5225
2022-03-13 08:01:58 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.7943
2022-03-13 08:02:32 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.6546
2022-03-13 08:03:04 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.4062
2022-03-13 08:03:38 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.3574
2022-03-13 08:04:11 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.5652
2022-03-13 08:04:43 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.4409
2022-03-13 08:05:15 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.5791
2022-03-13 08:05:49 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.7891
2022-03-13 08:06:22 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.5467
2022-03-13 08:06:57 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.6074
2022-03-13 08:07:29 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.7225
2022-03-13 08:08:03 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.5310
2022-03-13 08:08:35 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.4205
2022-03-13 08:09:06 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.5834
2022-03-13 08:09:40 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.7157
2022-03-13 08:10:13 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.7981
2022-03-13 08:10:46 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.5255
2022-03-13 08:11:19 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.7370
2022-03-13 08:11:52 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.8445
2022-03-13 08:12:25 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.5243
2022-03-13 08:12:58 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.5972
2022-03-13 08:13:30 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.4879
2022-03-13 08:14:03 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.5607
2022-03-13 08:14:37 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.4637
2022-03-13 08:15:09 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.5443
2022-03-13 08:15:42 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.7017
2022-03-13 08:16:15 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.4625
2022-03-13 08:16:49 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.3635
2022-03-13 08:17:22 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.5798
2022-03-13 08:17:54 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.7300
2022-03-13 08:18:26 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.6547
2022-03-13 08:19:00 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.6681
2022-03-13 08:19:33 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.7873
2022-03-13 08:20:06 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.5070
2022-03-13 08:20:39 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.7120
2022-03-13 08:21:13 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.7110
2022-03-13 08:21:45 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.3735
2022-03-13 08:21:46 - train: epoch 020, train_loss: 2.6067
2022-03-13 08:22:59 - eval: epoch: 020, acc1: 47.444%, acc5: 73.994%, test_loss: 2.2736, per_image_load_time: 2.028ms, per_image_inference_time: 0.168ms
2022-03-13 08:22:59 - until epoch: 020, best_acc1: 47.444%
2022-03-13 08:22:59 - epoch 021 lr: 0.1
2022-03-13 08:23:38 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.4717
2022-03-13 08:24:13 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.6193
2022-03-13 08:24:46 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.2733
2022-03-13 08:25:19 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.6224
2022-03-13 08:25:52 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.3495
2022-03-13 08:26:25 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.4826
2022-03-13 08:26:57 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.3985
2022-03-13 08:27:31 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.8269
2022-03-13 08:28:04 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.5924
2022-03-13 08:28:37 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.3251
2022-03-13 08:29:10 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.4739
2022-03-13 08:29:44 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.4962
2022-03-13 08:30:17 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.5406
2022-03-13 08:30:50 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.5800
2022-03-13 08:31:21 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.3652
2022-03-13 08:31:54 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.5935
2022-03-13 08:32:27 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.5829
2022-03-13 08:33:00 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.4496
2022-03-13 08:33:34 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.6971
2022-03-13 08:34:06 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.8189
2022-03-13 08:34:40 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.3977
2022-03-13 08:35:13 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.7297
2022-03-13 08:35:45 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.3274
2022-03-13 08:36:17 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.5346
2022-03-13 08:36:50 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.5109
2022-03-13 08:37:24 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.8633
2022-03-13 08:37:58 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.5011
2022-03-13 08:38:30 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.6792
2022-03-13 08:39:03 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.5728
2022-03-13 08:39:36 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.8214
2022-03-13 08:40:07 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.7683
2022-03-13 08:40:41 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.6263
2022-03-13 08:41:13 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.9067
2022-03-13 08:41:47 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.7480
2022-03-13 08:42:19 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.4688
2022-03-13 08:42:53 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.5762
2022-03-13 08:43:26 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.5434
2022-03-13 08:43:59 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.4506
2022-03-13 08:44:31 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.6057
2022-03-13 08:45:03 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.7983
2022-03-13 08:45:36 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.5000
2022-03-13 08:46:09 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.6676
2022-03-13 08:46:42 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.5498
2022-03-13 08:47:15 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.6233
2022-03-13 08:47:49 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.7000
2022-03-13 08:48:21 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.5383
2022-03-13 08:48:53 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.6170
2022-03-13 08:49:25 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.6704
2022-03-13 08:49:58 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.3810
2022-03-13 08:50:30 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.4635
2022-03-13 08:50:31 - train: epoch 021, train_loss: 2.5975
2022-03-13 08:51:46 - eval: epoch: 021, acc1: 46.700%, acc5: 73.584%, test_loss: 2.2921, per_image_load_time: 0.994ms, per_image_inference_time: 0.141ms
2022-03-13 08:51:47 - until epoch: 021, best_acc1: 47.444%
2022-03-13 08:51:47 - epoch 022 lr: 0.1
2022-03-13 08:52:25 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.2678
2022-03-13 08:52:58 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.5037
2022-03-13 08:53:31 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.2489
2022-03-13 08:54:04 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.4973
2022-03-13 08:54:36 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.5315
2022-03-13 08:55:10 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.5936
2022-03-13 08:55:43 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.7374
2022-03-13 08:56:17 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.7667
2022-03-13 08:56:49 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.7951
2022-03-13 08:57:23 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.6250
2022-03-13 08:57:54 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.6412
2022-03-13 08:58:27 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.4026
2022-03-13 08:59:00 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.6176
2022-03-13 08:59:33 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.6416
2022-03-13 09:00:07 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.5547
2022-03-13 09:00:40 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.4989
2022-03-13 09:01:13 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.4346
2022-03-13 09:01:47 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.7862
2022-03-13 09:02:19 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.4982
2022-03-13 09:02:52 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.5456
2022-03-13 09:03:24 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.6192
2022-03-13 09:03:58 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.3179
2022-03-13 09:04:30 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.6750
2022-03-13 09:05:04 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.5897
2022-03-13 09:05:36 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.5023
2022-03-13 09:06:10 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.3988
2022-03-13 09:06:42 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.4267
2022-03-13 09:07:15 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.9971
2022-03-13 09:07:48 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.4244
2022-03-13 09:08:22 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.7236
2022-03-13 09:08:55 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.8038
2022-03-13 09:09:29 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.6111
2022-03-13 09:10:02 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.6586
2022-03-13 09:10:36 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.5225
2022-03-13 09:11:08 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.6953
2022-03-13 09:11:41 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.7312
2022-03-13 09:12:14 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.5370
2022-03-13 09:12:47 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.7617
2022-03-13 09:13:21 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.5828
2022-03-13 09:13:54 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.7009
2022-03-13 09:14:28 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.5020
2022-03-13 09:15:01 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.5115
2022-03-13 09:15:35 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.6753
2022-03-13 09:16:06 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.6245
2022-03-13 09:16:41 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.5317
2022-03-13 09:17:13 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.7304
2022-03-13 09:17:47 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.6775
2022-03-13 09:18:20 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.3521
2022-03-13 09:18:54 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.4298
2022-03-13 09:19:25 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.4087
2022-03-13 09:19:26 - train: epoch 022, train_loss: 2.5913
2022-03-13 09:20:42 - eval: epoch: 022, acc1: 46.860%, acc5: 73.418%, test_loss: 2.2965, per_image_load_time: 0.920ms, per_image_inference_time: 0.160ms
2022-03-13 09:20:42 - until epoch: 022, best_acc1: 47.444%
2022-03-13 09:20:42 - epoch 023 lr: 0.1
2022-03-13 09:21:22 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.4060
2022-03-13 09:21:56 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.2587
2022-03-13 09:22:30 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.3988
2022-03-13 09:23:05 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.7456
2022-03-13 09:23:37 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.5878
2022-03-13 09:24:11 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.5264
2022-03-13 09:24:43 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.3580
2022-03-13 09:25:16 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.4130
2022-03-13 09:25:49 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.5903
2022-03-13 09:26:23 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.4656
2022-03-13 09:26:57 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.7007
2022-03-13 09:27:30 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.3965
2022-03-13 09:28:05 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.6125
2022-03-13 09:28:39 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.6497
2022-03-13 09:29:10 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.4515
2022-03-13 09:29:45 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.5119
2022-03-13 09:30:19 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.7561
2022-03-13 09:30:53 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.4534
2022-03-13 09:31:27 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.6752
2022-03-13 09:32:01 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.1780
2022-03-13 09:32:36 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.7815
2022-03-13 09:33:09 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.1960
2022-03-13 09:33:42 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.4401
2022-03-13 09:34:15 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.6859
2022-03-13 09:34:50 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.6773
2022-03-13 09:35:25 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.5597
2022-03-13 09:36:00 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.6155
2022-03-13 09:36:35 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.5895
2022-03-13 09:37:09 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.8156
2022-03-13 09:37:44 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.7827
2022-03-13 09:38:17 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.5909
2022-03-13 09:38:52 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.7072
2022-03-13 09:39:26 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.6988
2022-03-13 09:40:01 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.7634
2022-03-13 09:40:35 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.4715
2022-03-13 09:41:10 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.4584
2022-03-13 09:41:46 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.5595
2022-03-13 09:42:20 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.5165
2022-03-13 09:42:54 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.5987
2022-03-13 09:43:27 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.5274
2022-03-13 09:44:02 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.4456
2022-03-13 09:44:36 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.5242
2022-03-13 09:45:10 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.3634
2022-03-13 09:45:44 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.4375
2022-03-13 09:46:19 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.3421
2022-03-13 09:47:04 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.7350
2022-03-13 09:47:43 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.3207
2022-03-13 09:48:35 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.4786
2022-03-13 09:49:24 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.5817
2022-03-13 09:50:07 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.7325
2022-03-13 09:50:08 - train: epoch 023, train_loss: 2.5821
2022-03-13 09:51:45 - eval: epoch: 023, acc1: 47.430%, acc5: 73.882%, test_loss: 2.2798, per_image_load_time: 3.074ms, per_image_inference_time: 0.196ms
2022-03-13 09:51:45 - until epoch: 023, best_acc1: 47.444%
2022-03-13 09:51:45 - epoch 024 lr: 0.1
2022-03-13 09:52:55 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.5851
2022-03-13 09:54:06 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.5935
2022-03-13 09:55:14 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.5370
2022-03-13 09:56:19 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.6208
2022-03-13 09:57:07 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.5821
2022-03-13 09:57:41 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.3302
2022-03-13 09:58:16 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.5002
2022-03-13 09:58:49 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.5617
2022-03-13 09:59:23 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.6284
2022-03-13 09:59:58 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.5864
2022-03-13 10:00:28 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.2682
2022-03-13 10:01:03 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.3693
2022-03-13 10:01:36 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.9880
2022-03-13 10:02:11 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.4688
2022-03-13 10:02:45 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.8444
2022-03-13 10:03:20 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.6250
2022-03-13 10:03:54 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.5068
2022-03-13 10:04:29 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.7436
2022-03-13 10:05:00 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.2938
2022-03-13 10:05:34 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.5870
2022-03-13 10:06:08 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.6618
2022-03-13 10:06:42 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.4021
2022-03-13 10:07:17 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.5478
2022-03-13 10:07:51 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.5610
2022-03-13 10:08:26 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.5456
2022-03-13 10:09:00 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.5198
2022-03-13 10:09:32 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.7238
2022-03-13 10:10:06 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.7181
2022-03-13 10:10:40 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.7731
2022-03-13 10:11:15 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.4978
2022-03-13 10:11:48 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.3984
2022-03-13 10:12:24 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.7312
2022-03-13 10:12:58 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.3181
2022-03-13 10:13:32 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.6444
2022-03-13 10:14:05 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.4314
2022-03-13 10:14:37 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.4741
2022-03-13 10:15:12 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.4759
2022-03-13 10:15:47 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.8160
2022-03-13 10:16:21 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.6286
2022-03-13 10:16:56 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.5610
2022-03-13 10:17:30 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.5609
2022-03-13 10:18:04 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.4609
2022-03-13 10:18:37 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.5425
2022-03-13 10:19:09 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.6483
2022-03-13 10:19:44 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.3950
2022-03-13 10:20:18 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.6226
2022-03-13 10:20:52 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.6405
2022-03-13 10:21:26 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.5738
2022-03-13 10:21:59 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.6093
2022-03-13 10:22:31 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.6386
2022-03-13 10:22:32 - train: epoch 024, train_loss: 2.5752
2022-03-13 10:23:46 - eval: epoch: 024, acc1: 47.334%, acc5: 74.100%, test_loss: 2.2679, per_image_load_time: 2.289ms, per_image_inference_time: 0.167ms
2022-03-13 10:23:46 - until epoch: 024, best_acc1: 47.444%
2022-03-13 10:23:46 - epoch 025 lr: 0.1
2022-03-13 10:24:25 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.5201
2022-03-13 10:24:59 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.3134
2022-03-13 10:25:34 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.4149
2022-03-13 10:26:07 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.7240
2022-03-13 10:26:41 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.4938
2022-03-13 10:27:15 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.5709
2022-03-13 10:27:48 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.5662
2022-03-13 10:28:19 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.5432
2022-03-13 10:28:53 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.4715
2022-03-13 10:29:27 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.4652
2022-03-13 10:30:00 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.5999
2022-03-13 10:30:35 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.5413
2022-03-13 10:31:08 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.4759
2022-03-13 10:31:43 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.6316
2022-03-13 10:32:15 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.5403
2022-03-13 10:32:46 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.3487
2022-03-13 10:33:21 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.6797
2022-03-13 10:33:54 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.3982
2022-03-13 10:34:28 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.4385
2022-03-13 10:35:01 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.6161
2022-03-13 10:35:35 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.4581
2022-03-13 10:36:09 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.3133
2022-03-13 10:36:42 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.5963
2022-03-13 10:37:13 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.3542
2022-03-13 10:37:48 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.6592
2022-03-13 10:38:21 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.6673
2022-03-13 10:38:55 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.6473
2022-03-13 10:39:27 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.3936
2022-03-13 10:40:01 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.6132
2022-03-13 10:40:35 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.7748
2022-03-13 10:41:09 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.6313
2022-03-13 10:41:39 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.7296
2022-03-13 10:42:13 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.6465
2022-03-13 10:42:47 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.5952
2022-03-13 10:43:21 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.3344
2022-03-13 10:43:54 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.7583
2022-03-13 10:44:29 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.5856
2022-03-13 10:45:02 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.6639
2022-03-13 10:45:35 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.7531
2022-03-13 10:46:07 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.7254
2022-03-13 10:46:40 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.8124
2022-03-13 10:47:12 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.6152
2022-03-13 10:47:46 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.4675
2022-03-13 10:48:20 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.4564
2022-03-13 10:48:53 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.4781
2022-03-13 10:49:27 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.5477
2022-03-13 10:50:00 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.6170
2022-03-13 10:50:34 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.3495
2022-03-13 10:51:05 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.4637
2022-03-13 10:51:37 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.5901
2022-03-13 10:51:38 - train: epoch 025, train_loss: 2.5687
2022-03-13 10:52:55 - eval: epoch: 025, acc1: 46.816%, acc5: 73.446%, test_loss: 2.3097, per_image_load_time: 0.839ms, per_image_inference_time: 0.145ms
2022-03-13 10:52:55 - until epoch: 025, best_acc1: 47.444%
2022-03-13 10:52:55 - epoch 026 lr: 0.1
2022-03-13 10:53:34 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.3282
2022-03-13 10:54:08 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.4528
2022-03-13 10:54:42 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.5022
2022-03-13 10:55:13 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.4078
2022-03-13 10:55:46 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.5215
2022-03-13 10:56:20 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.5262
2022-03-13 10:56:53 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.5392
2022-03-13 10:57:27 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.3492
2022-03-13 10:58:00 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.8896
2022-03-13 10:58:33 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.4687
2022-03-13 10:59:08 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.5562
2022-03-13 10:59:39 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.5823
2022-03-13 11:00:13 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.4713
2022-03-13 11:00:46 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.6116
2022-03-13 11:01:20 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.5924
2022-03-13 11:01:53 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.7195
2022-03-13 11:02:26 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.5171
2022-03-13 11:03:00 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.7280
2022-03-13 11:03:34 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.9150
2022-03-13 11:04:05 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.7638
2022-03-13 11:04:39 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.6997
2022-03-13 11:05:12 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.5787
2022-03-13 11:05:47 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.5218
2022-03-13 11:06:20 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.7863
2022-03-13 11:06:55 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.6892
2022-03-13 11:07:28 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.5287
2022-03-13 11:08:02 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.5231
2022-03-13 11:08:34 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.4964
2022-03-13 11:09:08 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.6970
2022-03-13 11:09:42 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.5791
2022-03-13 11:10:16 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.4923
2022-03-13 11:10:50 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.5469
2022-03-13 11:11:24 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.4503
2022-03-13 11:11:58 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.6713
2022-03-13 11:12:32 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.5527
2022-03-13 11:13:05 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.4890
2022-03-13 11:13:38 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.6541
2022-03-13 11:14:13 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.7218
2022-03-13 11:14:47 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.8219
2022-03-13 11:15:21 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.8409
2022-03-13 11:15:55 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.6108
2022-03-13 11:16:30 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.7124
2022-03-13 11:17:04 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.7505
2022-03-13 11:17:36 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.7447
2022-03-13 11:18:09 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.8928
2022-03-13 11:18:44 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.5310
2022-03-13 11:19:18 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.4207
2022-03-13 11:19:53 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.4432
2022-03-13 11:20:27 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.6321
2022-03-13 11:20:59 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.8918
2022-03-13 11:21:00 - train: epoch 026, train_loss: 2.5617
2022-03-13 11:22:15 - eval: epoch: 026, acc1: 48.172%, acc5: 74.890%, test_loss: 2.2254, per_image_load_time: 1.985ms, per_image_inference_time: 0.161ms
2022-03-13 11:22:15 - until epoch: 026, best_acc1: 48.172%
2022-03-13 11:22:15 - epoch 027 lr: 0.1
2022-03-13 11:22:53 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.6816
2022-03-13 11:23:28 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.4277
2022-03-13 11:24:02 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.7250
2022-03-13 11:24:35 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.8624
2022-03-13 11:25:10 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.6813
2022-03-13 11:25:44 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.6926
2022-03-13 11:26:19 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.7996
2022-03-13 11:26:50 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.5793
2022-03-13 11:27:23 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.4538
2022-03-13 11:27:58 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.4812
2022-03-13 11:28:32 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.5221
2022-03-13 11:29:05 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.7617
2022-03-13 11:29:40 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.6840
2022-03-13 11:30:14 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.8487
2022-03-13 11:30:49 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.5746
2022-03-13 11:31:21 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.6157
2022-03-13 11:31:55 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.4330
2022-03-13 11:32:29 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.6032
2022-03-13 11:33:03 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.6835
2022-03-13 11:33:37 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.5566
2022-03-13 11:34:11 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.7124
2022-03-13 11:34:46 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.6755
2022-03-13 11:35:19 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.7953
2022-03-13 11:35:54 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.4244
2022-03-13 11:36:25 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.5293
2022-03-13 11:37:00 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.7891
2022-03-13 11:37:35 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.5327
2022-03-13 11:38:08 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.5985
2022-03-13 11:38:42 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.6339
2022-03-13 11:39:17 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.4199
2022-03-13 11:39:51 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.4554
2022-03-13 11:40:25 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.3709
2022-03-13 11:40:57 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.5850
2022-03-13 11:41:33 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.6319
2022-03-13 11:42:06 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.8183
2022-03-13 11:42:41 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.3925
2022-03-13 11:43:14 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.5021
2022-03-13 11:43:48 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.3590
2022-03-13 11:44:23 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.4838
2022-03-13 11:44:58 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.6720
2022-03-13 11:45:30 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.6020
2022-03-13 11:46:04 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.5855
2022-03-13 11:46:37 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.4821
2022-03-13 11:47:13 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.5518
2022-03-13 11:47:46 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.4226
2022-03-13 11:48:20 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.4677
2022-03-13 11:48:54 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.6216
2022-03-13 11:49:29 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.7905
2022-03-13 11:50:00 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.6351
2022-03-13 11:50:32 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.1856
2022-03-13 11:50:33 - train: epoch 027, train_loss: 2.5582
2022-03-13 11:51:50 - eval: epoch: 027, acc1: 47.996%, acc5: 74.268%, test_loss: 2.2419, per_image_load_time: 0.965ms, per_image_inference_time: 0.151ms
2022-03-13 11:51:50 - until epoch: 027, best_acc1: 48.172%
2022-03-13 11:51:50 - epoch 028 lr: 0.1
2022-03-13 11:52:29 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.3231
2022-03-13 11:53:03 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.4915
2022-03-13 11:53:38 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.5067
2022-03-13 11:54:11 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.4628
2022-03-13 11:54:42 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.4642
2022-03-13 11:55:18 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.6138
2022-03-13 11:55:50 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.7885
2022-03-13 11:56:22 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.2752
2022-03-13 11:56:54 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.3274
2022-03-13 11:57:26 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.5268
2022-03-13 11:57:58 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.6193
2022-03-13 11:58:30 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.3241
2022-03-13 11:59:02 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.5287
2022-03-13 11:59:34 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.8084
2022-03-13 12:00:06 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.5559
2022-03-13 12:00:39 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.4516
2022-03-13 12:01:10 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.5041
2022-03-13 12:01:44 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.4345
2022-03-13 12:02:15 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.4316
2022-03-13 12:02:48 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.7741
2022-03-13 12:03:19 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.6459
2022-03-13 12:03:51 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.6495
2022-03-13 12:04:24 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.6540
2022-03-13 12:04:55 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.7569
2022-03-13 12:05:28 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.4229
2022-03-13 12:06:01 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.3456
2022-03-13 12:06:33 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.5289
2022-03-13 12:07:05 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.5642
2022-03-13 12:07:38 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.5908
2022-03-13 12:08:09 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.6206
2022-03-13 12:08:42 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.7049
2022-03-13 12:09:14 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.4904
2022-03-13 12:09:46 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.6433
2022-03-13 12:10:18 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.3946
2022-03-13 12:10:50 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.4569
2022-03-13 12:11:22 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.3767
2022-03-13 12:11:53 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.5374
2022-03-13 12:12:26 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.2253
2022-03-13 12:12:58 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.5252
2022-03-13 12:13:30 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.7085
2022-03-13 12:14:02 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.6345
2022-03-13 12:14:35 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.6718
2022-03-13 12:15:06 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.4744
2022-03-13 12:15:39 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.5510
2022-03-13 12:16:11 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.5496
2022-03-13 12:16:44 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.6182
2022-03-13 12:17:15 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.6371
2022-03-13 12:17:47 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.4934
2022-03-13 12:18:20 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.4531
2022-03-13 12:18:50 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.4585
2022-03-13 12:18:51 - train: epoch 028, train_loss: 2.5482
2022-03-13 12:20:03 - eval: epoch: 028, acc1: 48.192%, acc5: 74.454%, test_loss: 2.2303, per_image_load_time: 1.177ms, per_image_inference_time: 0.168ms
2022-03-13 12:20:04 - until epoch: 028, best_acc1: 48.192%
2022-03-13 12:20:04 - epoch 029 lr: 0.1
2022-03-13 12:20:41 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.5763
2022-03-13 12:21:13 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.4909
2022-03-13 12:21:46 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.6492
2022-03-13 12:22:17 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.4799
2022-03-13 12:22:50 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.5230
2022-03-13 12:23:21 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.8496
2022-03-13 12:23:54 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.1911
2022-03-13 12:24:26 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.6324
2022-03-13 12:24:58 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.4212
2022-03-13 12:25:30 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.4200
2022-03-13 12:26:03 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.4882
2022-03-13 12:26:34 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.6247
2022-03-13 12:27:07 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.4931
2022-03-13 12:27:39 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.7605
2022-03-13 12:28:12 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.6962
2022-03-13 12:28:44 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.5120
2022-03-13 12:29:16 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.5633
2022-03-13 12:29:48 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.6751
2022-03-13 12:30:21 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.3571
2022-03-13 12:30:53 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.7907
2022-03-13 12:31:26 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.4933
2022-03-13 12:31:58 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.6666
2022-03-13 12:32:31 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.5972
2022-03-13 12:33:02 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.4526
2022-03-13 12:33:35 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.5313
2022-03-13 12:34:08 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.5752
2022-03-13 12:34:40 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.5177
2022-03-13 12:35:12 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.6004
2022-03-13 12:35:45 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.4548
2022-03-13 12:36:17 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.5289
2022-03-13 12:36:49 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.4742
2022-03-13 12:37:21 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.5206
2022-03-13 12:37:54 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.3735
2022-03-13 12:38:25 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.4569
2022-03-13 12:38:58 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.6488
2022-03-13 12:39:30 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.5617
2022-03-13 12:40:03 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.5173
2022-03-13 12:40:34 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.4949
2022-03-13 12:41:07 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.3737
2022-03-13 12:41:39 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.5556
2022-03-13 12:42:12 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.4972
2022-03-13 12:42:43 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.3740
2022-03-13 12:43:16 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.6032
2022-03-13 12:43:48 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.5360
2022-03-13 12:44:20 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.6641
2022-03-13 12:44:52 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.6810
2022-03-13 12:45:24 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.4428
2022-03-13 12:45:57 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.6028
2022-03-13 12:46:30 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.9351
2022-03-13 12:47:00 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.2279
2022-03-13 12:47:01 - train: epoch 029, train_loss: 2.5440
2022-03-13 12:48:15 - eval: epoch: 029, acc1: 49.100%, acc5: 75.180%, test_loss: 2.1829, per_image_load_time: 0.821ms, per_image_inference_time: 0.164ms
2022-03-13 12:48:15 - until epoch: 029, best_acc1: 49.100%
2022-03-13 12:48:15 - epoch 030 lr: 0.1
2022-03-13 12:48:53 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.6716
2022-03-13 12:49:25 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.5784
2022-03-13 12:49:58 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.5078
2022-03-13 12:50:30 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.3173
2022-03-13 12:51:02 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.8411
2022-03-13 12:51:35 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.3905
2022-03-13 12:52:07 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.6671
2022-03-13 12:52:40 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.6168
2022-03-13 12:53:12 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.5673
2022-03-13 12:53:45 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.4256
2022-03-13 12:54:17 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.2479
2022-03-13 12:54:50 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.4644
2022-03-13 12:55:23 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.3529
2022-03-13 12:55:56 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.5148
2022-03-13 12:56:28 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.4741
2022-03-13 12:57:01 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.4715
2022-03-13 12:57:34 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.7820
2022-03-13 12:58:06 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.6317
2022-03-13 12:58:39 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.7238
2022-03-13 12:59:11 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.4969
2022-03-13 12:59:44 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.6800
2022-03-13 13:00:17 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.5220
2022-03-13 13:00:50 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.5355
2022-03-13 13:01:22 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.7784
2022-03-13 13:01:54 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.5625
2022-03-13 13:02:27 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.5331
2022-03-13 13:03:00 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.4479
2022-03-13 13:03:33 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.5047
2022-03-13 13:04:05 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.4631
2022-03-13 13:04:39 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.6800
2022-03-13 13:05:11 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.4631
2022-03-13 13:05:44 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.4511
2022-03-13 13:06:16 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.7298
2022-03-13 13:06:49 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.5470
2022-03-13 13:07:22 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.5756
2022-03-13 13:07:55 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.3650
2022-03-13 13:08:27 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.4386
2022-03-13 13:09:00 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.7865
2022-03-13 13:09:33 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.6381
2022-03-13 13:10:06 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.3043
2022-03-13 13:10:38 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.3956
2022-03-13 13:11:11 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.7361
2022-03-13 13:11:44 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.5633
2022-03-13 13:12:17 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.4687
2022-03-13 13:12:49 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.7379
2022-03-13 13:13:22 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.3505
2022-03-13 13:13:54 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.6110
2022-03-13 13:14:25 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.6911
2022-03-13 13:14:58 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.6750
2022-03-13 13:15:29 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.6733
2022-03-13 13:15:29 - train: epoch 030, train_loss: 2.5423
2022-03-13 13:16:43 - eval: epoch: 030, acc1: 47.594%, acc5: 74.020%, test_loss: 2.2787, per_image_load_time: 2.694ms, per_image_inference_time: 0.161ms
2022-03-13 13:16:43 - until epoch: 030, best_acc1: 49.100%
2022-03-13 13:16:43 - epoch 031 lr: 0.010000000000000002
2022-03-13 13:17:20 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.3840
2022-03-13 13:17:52 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.1799
2022-03-13 13:18:25 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 2.0563
2022-03-13 13:18:56 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.2754
2022-03-13 13:19:28 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.9501
2022-03-13 13:20:00 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 2.0481
2022-03-13 13:20:32 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 2.0813
2022-03-13 13:21:04 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 2.0502
2022-03-13 13:21:37 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 2.0402
2022-03-13 13:22:09 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.3732
2022-03-13 13:22:42 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.2446
2022-03-13 13:23:14 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 2.1310
2022-03-13 13:23:47 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.8448
2022-03-13 13:24:19 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 2.1086
2022-03-13 13:24:52 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.1548
2022-03-13 13:25:24 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.9138
2022-03-13 13:25:56 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.9240
2022-03-13 13:26:28 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 2.0793
2022-03-13 13:27:01 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 2.1423
2022-03-13 13:27:34 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 2.1516
2022-03-13 13:28:06 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.9803
2022-03-13 13:28:39 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.8730
2022-03-13 13:29:11 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.8598
2022-03-13 13:29:43 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.9853
2022-03-13 13:30:15 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.9219
2022-03-13 13:30:48 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.9348
2022-03-13 13:31:20 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 2.0497
2022-03-13 13:31:53 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.3090
2022-03-13 13:32:27 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 2.0239
2022-03-13 13:32:59 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.1038
2022-03-13 13:33:32 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.8420
2022-03-13 13:34:05 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 2.1524
2022-03-13 13:34:37 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 2.0251
2022-03-13 13:35:11 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.1271
2022-03-13 13:35:44 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 2.0610
2022-03-13 13:36:17 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.9274
2022-03-13 13:36:50 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.9586
2022-03-13 13:37:23 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.8520
2022-03-13 13:37:55 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 2.0662
2022-03-13 13:38:29 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.9377
2022-03-13 13:39:02 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.9910
2022-03-13 13:39:36 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 2.0732
2022-03-13 13:40:08 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.8975
2022-03-13 13:40:42 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 2.0403
2022-03-13 13:41:14 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 2.0680
2022-03-13 13:41:47 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 2.0971
2022-03-13 13:42:20 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 2.0140
2022-03-13 13:42:53 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.8755
2022-03-13 13:43:27 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.7760
2022-03-13 13:43:58 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 2.0112
2022-03-13 13:43:58 - train: epoch 031, train_loss: 2.0369
2022-03-13 13:45:10 - eval: epoch: 031, acc1: 60.668%, acc5: 83.654%, test_loss: 1.6252, per_image_load_time: 2.165ms, per_image_inference_time: 0.192ms
2022-03-13 13:45:10 - until epoch: 031, best_acc1: 60.668%
2022-03-13 13:45:10 - epoch 032 lr: 0.010000000000000002
2022-03-13 13:45:49 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.9118
2022-03-13 13:46:22 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 2.0539
2022-03-13 13:46:54 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.9741
2022-03-13 13:47:27 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 2.1765
2022-03-13 13:48:00 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.9795
2022-03-13 13:48:33 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 2.0991
2022-03-13 13:49:06 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.9794
2022-03-13 13:49:37 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.9843
2022-03-13 13:50:10 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.9028
2022-03-13 13:50:43 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 2.0189
2022-03-13 13:51:16 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 2.0134
2022-03-13 13:51:49 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.9910
2022-03-13 13:52:21 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.8607
2022-03-13 13:52:54 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 2.0398
2022-03-13 13:53:27 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 2.0619
2022-03-13 13:54:00 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 2.1127
2022-03-13 13:54:34 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 2.0914
2022-03-13 13:55:06 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.2172
2022-03-13 13:55:38 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.8030
2022-03-13 13:56:12 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.8599
2022-03-13 13:56:44 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.7455
2022-03-13 13:57:18 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.7819
2022-03-13 13:57:50 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 2.0276
2022-03-13 13:58:23 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.8623
2022-03-13 13:58:56 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 2.0884
2022-03-13 13:59:30 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.7354
2022-03-13 14:00:02 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.9416
2022-03-13 14:00:35 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 2.0390
2022-03-13 14:01:08 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.7953
2022-03-13 14:01:41 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.7272
2022-03-13 14:02:13 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.9606
2022-03-13 14:02:47 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.9109
2022-03-13 14:03:20 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.7368
2022-03-13 14:03:53 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.7381
2022-03-13 14:04:25 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.8859
2022-03-13 14:04:59 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.9732
2022-03-13 14:05:32 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 2.0437
2022-03-13 14:06:06 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.7549
2022-03-13 14:06:37 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.8829
2022-03-13 14:07:10 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.7730
2022-03-13 14:07:44 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.9444
2022-03-13 14:08:17 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.7930
2022-03-13 14:08:50 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 2.1200
2022-03-13 14:09:24 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.8441
2022-03-13 14:09:56 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.9763
2022-03-13 14:10:28 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.8561
2022-03-13 14:11:01 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 2.1040
2022-03-13 14:11:34 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.7784
2022-03-13 14:12:07 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 2.0594
2022-03-13 14:12:38 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.9366
2022-03-13 14:12:39 - train: epoch 032, train_loss: 1.9233
2022-03-13 14:13:52 - eval: epoch: 032, acc1: 61.732%, acc5: 84.206%, test_loss: 1.5824, per_image_load_time: 2.411ms, per_image_inference_time: 0.185ms
2022-03-13 14:13:53 - until epoch: 032, best_acc1: 61.732%
2022-03-13 14:13:53 - epoch 033 lr: 0.010000000000000002
2022-03-13 14:14:31 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.7013
2022-03-13 14:15:04 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.9127
2022-03-13 14:15:36 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.8267
2022-03-13 14:16:10 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.6836
2022-03-13 14:16:42 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.9796
2022-03-13 14:17:16 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.7274
2022-03-13 14:17:48 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.9579
2022-03-13 14:18:22 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 2.0548
2022-03-13 14:18:54 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.9857
2022-03-13 14:19:26 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.9468
2022-03-13 14:19:59 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.8068
2022-03-13 14:20:32 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.8936
2022-03-13 14:21:05 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.7752
2022-03-13 14:21:38 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 2.0331
2022-03-13 14:22:11 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 2.2336
2022-03-13 14:22:44 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 2.0188
2022-03-13 14:23:17 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.7689
2022-03-13 14:23:50 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 2.0565
2022-03-13 14:24:22 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.9212
2022-03-13 14:24:55 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.7474
2022-03-13 14:25:28 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.8423
2022-03-13 14:26:00 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 2.0172
2022-03-13 14:26:34 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.6747
2022-03-13 14:27:07 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 2.1164
2022-03-13 14:27:40 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.8394
2022-03-13 14:28:12 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.7805
2022-03-13 14:28:45 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 2.1052
2022-03-13 14:29:18 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.8481
2022-03-13 14:29:51 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.9847
2022-03-13 14:30:24 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.9281
2022-03-13 14:30:58 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.8450
2022-03-13 14:31:30 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.7856
2022-03-13 14:32:03 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.9503
2022-03-13 14:32:36 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.7850
2022-03-13 14:33:09 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.9499
2022-03-13 14:33:41 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 2.1102
2022-03-13 14:34:15 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.8928
2022-03-13 14:34:48 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.7190
2022-03-13 14:35:21 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.9961
2022-03-13 14:35:54 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.9784
2022-03-13 14:36:27 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.8702
2022-03-13 14:37:01 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.7595
2022-03-13 14:37:34 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.9627
2022-03-13 14:38:07 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 2.0282
2022-03-13 14:38:39 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 2.0678
2022-03-13 14:39:12 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.6850
2022-03-13 14:39:45 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.7553
2022-03-13 14:40:18 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.1905
2022-03-13 14:40:51 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.6542
2022-03-13 14:41:22 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.8853
2022-03-13 14:41:23 - train: epoch 033, train_loss: 1.8806
2022-03-13 14:42:37 - eval: epoch: 033, acc1: 62.078%, acc5: 84.432%, test_loss: 1.5606, per_image_load_time: 2.567ms, per_image_inference_time: 0.179ms
2022-03-13 14:42:37 - until epoch: 033, best_acc1: 62.078%
2022-03-13 14:42:37 - epoch 034 lr: 0.010000000000000002
2022-03-13 14:43:16 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.7515
2022-03-13 14:43:49 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.7762
2022-03-13 14:44:21 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.8849
2022-03-13 14:44:54 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.5517
2022-03-13 14:45:26 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.7635
2022-03-13 14:45:59 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 2.0785
2022-03-13 14:46:32 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.7448
2022-03-13 14:47:06 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.7690
2022-03-13 14:47:37 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.8501
2022-03-13 14:48:11 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.8264
2022-03-13 14:48:43 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.7987
2022-03-13 14:49:17 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.8885
2022-03-13 14:49:49 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.8461
2022-03-13 14:50:23 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.8120
2022-03-13 14:50:55 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.7449
2022-03-13 14:51:29 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.7323
2022-03-13 14:52:01 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.8053
2022-03-13 14:52:34 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 2.1318
2022-03-13 14:53:07 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.9887
2022-03-13 14:53:41 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.9657
2022-03-13 14:54:13 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 2.1588
2022-03-13 14:54:47 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.7755
2022-03-13 14:55:20 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 2.0405
2022-03-13 14:55:53 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.7878
2022-03-13 14:56:26 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.9206
2022-03-13 14:56:59 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.8959
2022-03-13 14:57:31 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.8396
2022-03-13 14:58:05 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.7022
2022-03-13 14:58:37 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.5821
2022-03-13 14:59:11 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.6365
2022-03-13 14:59:44 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.9451
2022-03-13 15:00:17 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.9328
2022-03-13 15:00:50 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.9233
2022-03-13 15:01:23 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.9301
2022-03-13 15:01:57 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.6938
2022-03-13 15:02:30 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.7095
2022-03-13 15:03:04 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.6673
2022-03-13 15:03:36 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.7772
2022-03-13 15:04:09 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.9540
2022-03-13 15:04:43 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.6566
2022-03-13 15:05:16 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.8515
2022-03-13 15:05:49 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.8045
2022-03-13 15:06:22 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.8872
2022-03-13 15:06:55 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.8882
2022-03-13 15:07:28 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.9744
2022-03-13 15:08:01 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.9013
2022-03-13 15:08:34 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.9255
2022-03-13 15:09:07 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.8982
2022-03-13 15:09:39 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.8341
2022-03-13 15:10:11 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.7052
2022-03-13 15:10:11 - train: epoch 034, train_loss: 1.8580
2022-03-13 15:11:25 - eval: epoch: 034, acc1: 62.438%, acc5: 84.830%, test_loss: 1.5438, per_image_load_time: 2.477ms, per_image_inference_time: 0.193ms
2022-03-13 15:11:25 - until epoch: 034, best_acc1: 62.438%
2022-03-13 15:11:25 - epoch 035 lr: 0.010000000000000002
2022-03-13 15:12:02 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.7032
2022-03-13 15:12:36 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.6739
2022-03-13 15:13:08 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 2.0274
2022-03-13 15:13:40 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.7674
2022-03-13 15:14:13 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.8538
2022-03-13 15:14:47 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.7508
2022-03-13 15:15:19 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.8351
2022-03-13 15:15:52 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.8471
2022-03-13 15:16:25 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.9906
2022-03-13 15:16:58 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 2.0723
2022-03-13 15:17:31 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.9444
2022-03-13 15:18:04 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.7760
2022-03-13 15:18:36 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.9556
2022-03-13 15:19:09 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.6926
2022-03-13 15:19:42 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.9493
2022-03-13 15:20:16 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.8533
2022-03-13 15:20:48 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.6635
2022-03-13 15:21:21 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.8759
2022-03-13 15:21:53 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.9019
2022-03-13 15:22:27 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.9268
2022-03-13 15:22:59 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.8083
2022-03-13 15:23:33 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.8925
2022-03-13 15:24:06 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.8153
2022-03-13 15:24:38 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 2.0144
2022-03-13 15:25:11 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.8143
2022-03-13 15:25:45 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.8986
2022-03-13 15:26:17 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 2.0230
2022-03-13 15:26:51 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.7698
2022-03-13 15:27:23 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.8422
2022-03-13 15:27:57 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.9028
2022-03-13 15:28:29 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.8586
2022-03-13 15:29:03 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.6996
2022-03-13 15:29:35 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.7647
2022-03-13 15:30:08 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.8766
2022-03-13 15:30:41 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.5537
2022-03-13 15:31:15 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.8508
2022-03-13 15:31:47 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.6789
2022-03-13 15:32:21 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.8213
2022-03-13 15:32:53 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.9506
2022-03-13 15:33:26 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.6359
2022-03-13 15:33:59 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 2.1194
2022-03-13 15:34:32 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.6631
2022-03-13 15:35:05 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.8750
2022-03-13 15:35:37 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.8696
2022-03-13 15:36:10 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 2.0105
2022-03-13 15:36:44 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.7481
2022-03-13 15:37:16 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 2.1365
2022-03-13 15:37:49 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 2.1056
2022-03-13 15:38:22 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.8993
2022-03-13 15:38:53 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.6476
2022-03-13 15:38:54 - train: epoch 035, train_loss: 1.8434
2022-03-13 15:40:08 - eval: epoch: 035, acc1: 62.650%, acc5: 84.870%, test_loss: 1.5350, per_image_load_time: 1.743ms, per_image_inference_time: 0.170ms
2022-03-13 15:40:08 - until epoch: 035, best_acc1: 62.650%
2022-03-13 15:40:08 - epoch 036 lr: 0.010000000000000002
2022-03-13 15:40:46 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.9951
2022-03-13 15:41:19 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.5375
2022-03-13 15:41:53 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.7529
2022-03-13 15:42:25 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.8174
2022-03-13 15:42:58 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.9413
2022-03-13 15:43:32 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.7796
2022-03-13 15:44:04 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.7271
2022-03-13 15:44:38 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.6955
2022-03-13 15:45:09 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.8002
2022-03-13 15:45:42 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.8873
2022-03-13 15:46:15 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.9406
2022-03-13 15:46:48 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.7990
2022-03-13 15:47:21 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.9242
2022-03-13 15:47:54 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 2.0706
2022-03-13 15:48:26 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.9919
2022-03-13 15:49:00 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.9759
2022-03-13 15:49:32 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.8214
2022-03-13 15:50:05 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.7114
2022-03-13 15:50:37 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.7768
2022-03-13 15:51:09 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.6806
2022-03-13 15:51:43 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.8587
2022-03-13 15:52:16 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.7841
2022-03-13 15:52:49 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.8224
2022-03-13 15:53:22 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 2.0377
2022-03-13 15:53:55 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.6577
2022-03-13 15:54:29 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.8801
2022-03-13 15:55:00 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.7907
2022-03-13 15:55:34 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.7632
2022-03-13 15:56:07 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.6799
2022-03-13 15:56:40 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 2.0224
2022-03-13 15:57:13 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.8707
2022-03-13 15:57:47 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.7872
2022-03-13 15:58:19 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.7299
2022-03-13 15:58:53 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.8028
2022-03-13 15:59:25 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.8711
2022-03-13 15:59:58 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.9962
2022-03-13 16:00:32 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.8252
2022-03-13 16:01:04 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.7923
2022-03-13 16:01:38 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.7556
2022-03-13 16:02:11 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.9069
2022-03-13 16:02:44 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.7543
2022-03-13 16:03:17 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.7562
2022-03-13 16:03:50 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.8491
2022-03-13 16:04:22 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.8765
2022-03-13 16:04:55 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.6207
2022-03-13 16:05:28 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.6484
2022-03-13 16:06:00 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.8243
2022-03-13 16:06:34 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.7957
2022-03-13 16:07:06 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.7634
2022-03-13 16:07:38 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.7733
2022-03-13 16:07:39 - train: epoch 036, train_loss: 1.8333
2022-03-13 16:08:52 - eval: epoch: 036, acc1: 62.692%, acc5: 84.832%, test_loss: 1.5359, per_image_load_time: 1.962ms, per_image_inference_time: 0.168ms
2022-03-13 16:08:52 - until epoch: 036, best_acc1: 62.692%
2022-03-13 16:08:52 - epoch 037 lr: 0.010000000000000002
2022-03-13 16:09:30 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.6446
2022-03-13 16:10:02 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.5087
2022-03-13 16:10:36 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.8011
2022-03-13 16:11:09 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.8350
2022-03-13 16:11:41 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.7993
2022-03-13 16:12:14 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.9352
2022-03-13 16:12:48 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.8756
2022-03-13 16:13:20 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.8374
2022-03-13 16:13:52 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.1332
2022-03-13 16:14:26 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.6846
2022-03-13 16:14:59 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.8841
2022-03-13 16:15:31 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.8012
2022-03-13 16:16:04 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.8491
2022-03-13 16:16:37 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.9507
2022-03-13 16:17:10 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.7508
2022-03-13 16:17:42 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.5904
2022-03-13 16:18:16 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 2.0357
2022-03-13 16:18:49 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.8676
2022-03-13 16:19:21 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.9193
2022-03-13 16:19:55 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.9205
2022-03-13 16:20:27 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.8628
2022-03-13 16:21:00 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.7718
2022-03-13 16:21:33 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.6655
2022-03-13 16:22:06 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.8325
2022-03-13 16:22:38 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.8142
2022-03-13 16:23:11 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 2.0413
2022-03-13 16:23:44 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.9899
2022-03-13 16:24:18 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.8267
2022-03-13 16:24:51 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.8706
2022-03-13 16:25:23 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.9737
2022-03-13 16:25:57 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.7716
2022-03-13 16:26:29 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.8999
2022-03-13 16:27:02 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.7424
2022-03-13 16:27:35 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.6139
2022-03-13 16:28:08 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.8842
2022-03-13 16:28:40 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.9377
2022-03-13 16:29:13 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.9145
2022-03-13 16:29:45 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.7832
2022-03-13 16:30:17 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 2.0483
2022-03-13 16:30:50 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.7925
2022-03-13 16:31:22 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.7887
2022-03-13 16:31:55 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 2.0423
2022-03-13 16:32:27 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.7636
2022-03-13 16:32:59 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.7602
2022-03-13 16:33:32 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.7896
2022-03-13 16:34:05 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.8835
2022-03-13 16:34:39 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.8594
2022-03-13 16:35:10 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.8581
2022-03-13 16:35:44 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.8416
2022-03-13 16:36:15 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.8025
2022-03-13 16:36:16 - train: epoch 037, train_loss: 1.8286
2022-03-13 16:37:29 - eval: epoch: 037, acc1: 62.720%, acc5: 85.164%, test_loss: 1.5267, per_image_load_time: 2.395ms, per_image_inference_time: 0.172ms
2022-03-13 16:37:29 - until epoch: 037, best_acc1: 62.720%
2022-03-13 16:37:29 - epoch 038 lr: 0.010000000000000002
2022-03-13 16:38:08 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.7899
2022-03-13 16:38:40 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.6475
2022-03-13 16:39:11 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.6094
2022-03-13 16:39:45 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.6739
2022-03-13 16:40:18 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.6639
2022-03-13 16:40:51 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.9741
2022-03-13 16:41:23 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.4981
2022-03-13 16:41:56 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.6630
2022-03-13 16:42:28 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.7468
2022-03-13 16:43:01 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 2.0256
2022-03-13 16:43:34 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.8402
2022-03-13 16:44:06 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.8342
2022-03-13 16:44:40 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 2.0252
2022-03-13 16:45:12 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.8309
2022-03-13 16:45:45 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.8275
2022-03-13 16:46:17 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.9202
2022-03-13 16:46:50 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.9999
2022-03-13 16:47:23 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.8551
2022-03-13 16:47:56 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.8948
2022-03-13 16:48:29 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.7776
2022-03-13 16:49:01 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.8214
2022-03-13 16:49:34 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.6665
2022-03-13 16:50:07 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.9110
2022-03-13 16:50:40 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 2.0959
2022-03-13 16:51:13 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.7578
2022-03-13 16:51:46 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.8301
2022-03-13 16:52:19 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.9787
2022-03-13 16:52:52 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 2.0617
2022-03-13 16:53:25 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.8109
2022-03-13 16:53:57 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.7897
2022-03-13 16:54:31 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.7424
2022-03-13 16:55:03 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.5712
2022-03-13 16:55:36 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.5763
2022-03-13 16:56:08 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.7853
2022-03-13 16:56:42 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.8595
2022-03-13 16:57:14 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.8887
2022-03-13 16:57:47 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.5801
2022-03-13 16:58:21 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.9044
2022-03-13 16:58:54 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.7509
2022-03-13 16:59:27 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.9296
2022-03-13 17:00:00 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.7637
2022-03-13 17:00:33 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.6465
2022-03-13 17:01:05 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.9516
2022-03-13 17:01:38 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.7848
2022-03-13 17:02:12 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.8885
2022-03-13 17:02:46 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 2.0295
2022-03-13 17:03:19 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.7062
2022-03-13 17:03:52 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.8664
2022-03-13 17:04:24 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.7678
2022-03-13 17:04:55 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.7700
2022-03-13 17:04:56 - train: epoch 038, train_loss: 1.8249
2022-03-13 17:06:10 - eval: epoch: 038, acc1: 62.890%, acc5: 85.166%, test_loss: 1.5269, per_image_load_time: 0.973ms, per_image_inference_time: 0.169ms
2022-03-13 17:06:10 - until epoch: 038, best_acc1: 62.890%
2022-03-13 17:06:10 - epoch 039 lr: 0.010000000000000002
2022-03-13 17:06:48 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.9522
2022-03-13 17:07:21 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 2.0060
2022-03-13 17:07:54 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.5623
2022-03-13 17:08:27 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.7663
2022-03-13 17:08:59 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.7872
2022-03-13 17:09:31 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.7126
2022-03-13 17:10:04 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.9818
2022-03-13 17:10:37 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.8050
2022-03-13 17:11:10 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.8342
2022-03-13 17:11:43 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.6781
2022-03-13 17:12:16 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.9359
2022-03-13 17:12:47 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.8899
2022-03-13 17:13:21 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 2.0771
2022-03-13 17:13:53 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.8088
2022-03-13 17:14:26 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.7254
2022-03-13 17:14:59 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.9476
2022-03-13 17:15:32 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.5258
2022-03-13 17:16:04 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.8781
2022-03-13 17:16:38 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.5934
2022-03-13 17:17:10 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.7551
2022-03-13 17:17:43 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.8429
2022-03-13 17:18:16 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.6979
2022-03-13 17:18:49 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 2.0575
2022-03-13 17:19:22 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.9895
2022-03-13 17:19:55 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.7506
2022-03-13 17:20:27 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.8800
2022-03-13 17:21:00 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.9540
2022-03-13 17:21:33 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.8071
2022-03-13 17:22:06 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.6293
2022-03-13 17:22:38 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.9425
2022-03-13 17:23:12 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.7752
2022-03-13 17:23:44 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.8604
2022-03-13 17:24:17 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.8490
2022-03-13 17:24:50 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.8893
2022-03-13 17:25:23 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 2.0395
2022-03-13 17:25:56 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.9212
2022-03-13 17:26:28 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.7353
2022-03-13 17:27:01 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.6383
2022-03-13 17:27:34 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.8118
2022-03-13 17:28:07 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.8032
2022-03-13 17:28:40 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.7740
2022-03-13 17:29:13 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.8965
2022-03-13 17:29:45 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.7936
2022-03-13 17:30:18 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.5220
2022-03-13 17:30:51 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.6731
2022-03-13 17:31:25 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 2.0264
2022-03-13 17:31:57 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.8804
2022-03-13 17:32:31 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.8422
2022-03-13 17:33:03 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.7428
2022-03-13 17:33:34 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.7313
2022-03-13 17:33:35 - train: epoch 039, train_loss: 1.8239
2022-03-13 17:34:48 - eval: epoch: 039, acc1: 62.532%, acc5: 84.932%, test_loss: 1.5314, per_image_load_time: 0.988ms, per_image_inference_time: 0.177ms
2022-03-13 17:34:48 - until epoch: 039, best_acc1: 62.890%
2022-03-13 17:34:48 - epoch 040 lr: 0.010000000000000002
2022-03-13 17:35:27 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 2.1133
2022-03-13 17:36:00 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 2.0200
2022-03-13 17:36:34 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.9035
2022-03-13 17:37:05 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.7562
2022-03-13 17:37:38 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.7358
2022-03-13 17:38:11 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.9492
2022-03-13 17:38:44 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.8340
2022-03-13 17:39:16 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 2.0794
2022-03-13 17:39:49 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.7262
2022-03-13 17:40:22 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.4847
2022-03-13 17:40:55 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.7394
2022-03-13 17:41:27 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.7933
2022-03-13 17:42:00 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.7036
2022-03-13 17:42:33 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.7038
2022-03-13 17:43:06 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 2.0455
2022-03-13 17:43:39 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.8523
2022-03-13 17:44:11 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.9334
2022-03-13 17:44:44 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.7367
2022-03-13 17:45:17 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.7827
2022-03-13 17:45:50 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.8798
2022-03-13 17:46:23 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.8197
2022-03-13 17:46:57 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.6466
2022-03-13 17:47:28 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.7572
2022-03-13 17:48:02 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.9140
2022-03-13 17:48:34 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.9114
2022-03-13 17:49:08 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.7922
2022-03-13 17:49:40 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.9627
2022-03-13 17:50:13 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.7748
2022-03-13 17:50:45 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 2.1516
2022-03-13 17:51:19 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.8646
2022-03-13 17:51:52 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.7566
2022-03-13 17:52:24 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 2.0121
2022-03-13 17:52:57 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.9043
2022-03-13 17:53:31 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.7621
2022-03-13 17:54:03 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.8648
2022-03-13 17:54:37 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.8515
2022-03-13 17:55:09 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.7620
2022-03-13 17:55:42 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.6956
2022-03-13 17:56:14 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.7582
2022-03-13 17:56:48 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.9728
2022-03-13 17:57:20 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.9455
2022-03-13 17:57:54 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.7135
2022-03-13 17:58:26 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.8301
2022-03-13 17:58:59 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.8010
2022-03-13 17:59:32 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.6624
2022-03-13 18:00:05 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.8558
2022-03-13 18:00:38 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.7780
2022-03-13 18:01:09 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.7623
2022-03-13 18:01:43 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.9316
2022-03-13 18:02:14 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.9013
2022-03-13 18:02:15 - train: epoch 040, train_loss: 1.8222
2022-03-13 18:03:28 - eval: epoch: 040, acc1: 62.536%, acc5: 84.896%, test_loss: 1.5337, per_image_load_time: 0.876ms, per_image_inference_time: 0.163ms
2022-03-13 18:03:28 - until epoch: 040, best_acc1: 62.890%
2022-03-13 18:03:28 - epoch 041 lr: 0.010000000000000002
2022-03-13 18:04:07 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.9205
2022-03-13 18:04:40 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.9229
2022-03-13 18:05:13 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.7530
2022-03-13 18:05:45 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.8679
2022-03-13 18:06:16 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.5790
2022-03-13 18:06:49 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.9276
2022-03-13 18:07:19 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.8050
2022-03-13 18:07:51 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.5214
2022-03-13 18:08:23 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.5226
2022-03-13 18:08:54 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.9900
2022-03-13 18:09:25 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.7794
2022-03-13 18:09:56 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.5287
2022-03-13 18:10:28 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.8858
2022-03-13 18:11:00 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.9571
2022-03-13 18:11:32 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 2.2376
2022-03-13 18:12:04 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.5511
2022-03-13 18:12:35 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.8186
2022-03-13 18:13:08 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.7779
2022-03-13 18:13:39 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.9053
2022-03-13 18:14:11 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.8372
2022-03-13 18:14:43 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.6349
2022-03-13 18:15:15 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.6489
2022-03-13 18:15:46 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.6021
2022-03-13 18:16:18 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 2.0725
2022-03-13 18:16:50 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.7298
2022-03-13 18:17:22 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.9840
2022-03-13 18:17:53 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 2.1069
2022-03-13 18:18:25 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.8961
2022-03-13 18:18:58 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.9344
2022-03-13 18:19:29 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.9171
2022-03-13 18:20:02 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.9185
2022-03-13 18:20:33 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.8038
2022-03-13 18:21:04 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.6481
2022-03-13 18:21:36 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.7132
2022-03-13 18:22:08 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.9092
2022-03-13 18:22:39 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.9538
2022-03-13 18:23:11 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.8600
2022-03-13 18:23:44 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.7487
2022-03-13 18:24:16 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.8339
2022-03-13 18:24:47 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.6962
2022-03-13 18:25:19 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.8001
2022-03-13 18:25:50 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.7765
2022-03-13 18:26:23 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.9668
2022-03-13 18:26:54 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.7970
2022-03-13 18:27:27 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.7351
2022-03-13 18:27:59 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.9065
2022-03-13 18:28:32 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 2.0373
2022-03-13 18:29:04 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.7375
2022-03-13 18:29:36 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.7583
2022-03-13 18:30:07 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.9238
2022-03-13 18:30:07 - train: epoch 041, train_loss: 1.8248
2022-03-13 18:31:19 - eval: epoch: 041, acc1: 62.652%, acc5: 84.898%, test_loss: 1.5316, per_image_load_time: 0.910ms, per_image_inference_time: 0.150ms
2022-03-13 18:31:19 - until epoch: 041, best_acc1: 62.890%
2022-03-13 18:31:19 - epoch 042 lr: 0.010000000000000002
2022-03-13 18:31:56 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.5497
2022-03-13 18:32:27 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.6947
2022-03-13 18:32:59 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.8577
2022-03-13 18:33:31 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.5057
2022-03-13 18:34:03 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.8728
2022-03-13 18:34:36 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.6714
2022-03-13 18:35:06 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.8689
2022-03-13 18:35:38 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.7260
2022-03-13 18:36:09 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 2.0747
2022-03-13 18:36:41 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 2.0035
2022-03-13 18:37:13 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.5905
2022-03-13 18:37:45 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.6852
2022-03-13 18:38:16 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.7833
2022-03-13 18:38:48 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 2.1684
2022-03-13 18:39:19 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.7132
2022-03-13 18:39:52 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.8125
2022-03-13 18:40:23 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.8496
2022-03-13 18:40:55 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.9050
2022-03-13 18:41:26 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.9595
2022-03-13 18:41:58 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.6296
2022-03-13 18:42:28 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.6140
2022-03-13 18:43:02 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.7086
2022-03-13 18:43:32 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.6722
2022-03-13 18:44:04 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 2.0857
2022-03-13 18:44:36 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.9790
2022-03-13 18:45:08 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.9669
2022-03-13 18:45:39 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.6249
2022-03-13 18:46:11 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.7027
2022-03-13 18:46:43 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.8385
2022-03-13 18:47:15 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.9452
2022-03-13 18:47:46 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.8171
2022-03-13 18:48:18 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.9999
2022-03-13 18:48:50 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.9635
2022-03-13 18:49:21 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.6512
2022-03-13 18:49:53 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.8370
2022-03-13 18:50:25 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.8675
2022-03-13 18:50:56 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.7808
2022-03-13 18:51:29 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.6730
2022-03-13 18:52:00 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.6603
2022-03-13 18:52:32 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.9082
2022-03-13 18:53:03 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 2.0150
2022-03-13 18:53:35 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.9411
2022-03-13 18:54:06 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.7524
2022-03-13 18:54:38 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.6580
2022-03-13 18:55:09 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.8371
2022-03-13 18:55:41 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 2.0507
2022-03-13 18:56:13 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.7211
2022-03-13 18:56:45 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.9799
2022-03-13 18:57:17 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.7932
2022-03-13 18:57:47 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.7195
2022-03-13 18:57:47 - train: epoch 042, train_loss: 1.8248
2022-03-13 18:58:59 - eval: epoch: 042, acc1: 62.488%, acc5: 84.924%, test_loss: 1.5331, per_image_load_time: 1.637ms, per_image_inference_time: 0.146ms
2022-03-13 18:58:59 - until epoch: 042, best_acc1: 62.890%
2022-03-13 18:58:59 - epoch 043 lr: 0.010000000000000002
2022-03-13 18:59:35 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.7433
2022-03-13 19:00:07 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.8925
2022-03-13 19:00:38 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.5989
2022-03-13 19:01:10 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.7693
2022-03-13 19:01:42 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.8345
2022-03-13 19:02:13 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.7899
2022-03-13 19:02:45 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.8012
2022-03-13 19:03:16 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.9813
2022-03-13 19:03:47 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.7807
2022-03-13 19:04:18 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 2.0123
2022-03-13 19:04:50 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.9390
2022-03-13 19:05:22 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.8557
2022-03-13 19:05:53 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.9069
2022-03-13 19:06:25 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.8030
2022-03-13 19:06:56 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.6704
2022-03-13 19:07:28 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.9133
2022-03-13 19:07:59 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.9243
2022-03-13 19:08:31 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.9798
2022-03-13 19:09:03 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.8458
2022-03-13 19:09:35 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.6483
2022-03-13 19:10:06 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.8087
2022-03-13 19:10:37 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.9240
2022-03-13 19:11:08 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.8829
2022-03-13 19:11:40 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.7992
2022-03-13 19:12:12 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.9903
2022-03-13 19:12:44 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.6124
2022-03-13 19:13:15 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.9029
2022-03-13 19:13:47 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.6618
2022-03-13 19:14:19 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.9033
2022-03-13 19:14:50 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 2.0465
2022-03-13 19:15:21 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 2.0236
2022-03-13 19:15:53 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.9029
2022-03-13 19:16:25 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.9156
2022-03-13 19:16:57 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.9156
2022-03-13 19:17:27 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.8597
2022-03-13 19:18:00 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.7946
2022-03-13 19:18:31 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.8695
2022-03-13 19:19:03 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.9815
2022-03-13 19:19:34 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.9763
2022-03-13 19:20:06 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.9089
2022-03-13 19:20:37 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 2.0782
2022-03-13 19:21:09 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.8863
2022-03-13 19:21:41 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.8882
2022-03-13 19:22:12 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.8023
2022-03-13 19:22:44 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.7698
2022-03-13 19:23:15 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.8302
2022-03-13 19:23:48 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.8014
2022-03-13 19:24:18 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.7749
2022-03-13 19:24:51 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.8160
2022-03-13 19:25:20 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.6539
2022-03-13 19:25:21 - train: epoch 043, train_loss: 1.8220
2022-03-13 19:26:33 - eval: epoch: 043, acc1: 62.468%, acc5: 84.888%, test_loss: 1.5338, per_image_load_time: 0.962ms, per_image_inference_time: 0.163ms
2022-03-13 19:26:33 - until epoch: 043, best_acc1: 62.890%
2022-03-13 19:26:33 - epoch 044 lr: 0.010000000000000002
2022-03-13 19:27:10 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.8794
2022-03-13 19:27:41 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.9007
2022-03-13 19:28:13 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.8263
2022-03-13 19:28:45 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.4774
2022-03-13 19:29:16 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.7083
2022-03-13 19:29:47 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.5370
2022-03-13 19:30:18 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.5990
2022-03-13 19:30:50 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.7749
2022-03-13 19:31:22 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.8517
2022-03-13 19:31:53 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.7563
2022-03-13 19:32:25 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.6561
2022-03-13 19:32:56 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.6701
2022-03-13 19:33:28 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.8277
2022-03-13 19:33:59 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.6401
2022-03-13 19:34:30 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.7669
2022-03-13 19:35:02 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.7189
2022-03-13 19:35:34 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.7098
2022-03-13 19:36:05 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.7382
2022-03-13 19:36:36 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.9072
2022-03-13 19:37:08 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.8957
2022-03-13 19:37:40 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 2.0401
2022-03-13 19:38:10 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.8454
2022-03-13 19:38:43 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.9277
2022-03-13 19:39:14 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.8795
2022-03-13 19:39:46 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.8883
2022-03-13 19:40:17 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.9311
2022-03-13 19:40:48 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.9882
2022-03-13 19:41:20 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.6068
2022-03-13 19:41:51 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.7046
2022-03-13 19:42:22 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.8216
2022-03-13 19:42:54 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.8615
2022-03-13 19:43:25 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.6250
2022-03-13 19:43:56 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.8142
2022-03-13 19:44:27 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.9453
2022-03-13 19:45:00 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.7629
2022-03-13 19:45:30 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.8093
2022-03-13 19:46:02 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.8658
2022-03-13 19:46:33 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.5190
2022-03-13 19:47:04 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.9654
2022-03-13 19:47:36 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 2.0196
2022-03-13 19:48:07 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.7766
2022-03-13 19:48:38 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.9120
2022-03-13 19:49:10 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.9218
2022-03-13 19:49:41 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.6395
2022-03-13 19:50:12 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.9778
2022-03-13 19:50:44 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.7538
2022-03-13 19:51:15 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.8635
2022-03-13 19:51:46 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 2.1051
2022-03-13 19:52:18 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.6506
2022-03-13 19:52:48 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.8016
2022-03-13 19:52:48 - train: epoch 044, train_loss: 1.8219
2022-03-13 19:53:59 - eval: epoch: 044, acc1: 62.600%, acc5: 84.788%, test_loss: 1.5413, per_image_load_time: 2.568ms, per_image_inference_time: 0.156ms
2022-03-13 19:54:00 - until epoch: 044, best_acc1: 62.890%
2022-03-13 19:54:00 - epoch 045 lr: 0.010000000000000002
2022-03-13 19:54:36 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.6170
2022-03-13 19:55:07 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.8640
2022-03-13 19:55:39 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.8330
2022-03-13 19:56:09 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.6130
2022-03-13 19:56:40 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.9367
2022-03-13 19:57:11 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.9252
2022-03-13 19:57:42 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.4628
2022-03-13 19:58:13 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.7953
2022-03-13 19:58:45 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.9011
2022-03-13 19:59:16 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.8904
2022-03-13 19:59:48 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.9570
2022-03-13 20:00:18 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.8214
2022-03-13 20:00:50 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.9174
2022-03-13 20:01:22 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.8287
2022-03-13 20:01:54 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.7926
2022-03-13 20:02:25 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.7198
2022-03-13 20:02:57 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.6773
2022-03-13 20:03:28 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.7084
2022-03-13 20:04:00 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 2.0417
2022-03-13 20:04:31 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 2.0339
2022-03-13 20:05:03 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.9290
2022-03-13 20:05:34 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.9382
2022-03-13 20:06:06 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.6236
2022-03-13 20:06:37 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.9737
2022-03-13 20:07:08 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.8538
2022-03-13 20:07:39 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.7820
2022-03-13 20:08:11 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.7012
2022-03-13 20:08:42 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.6876
2022-03-13 20:09:13 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.9448
2022-03-13 20:09:44 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 2.0211
2022-03-13 20:10:16 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.7370
2022-03-13 20:10:47 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.9197
2022-03-13 20:11:18 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.8650
2022-03-13 20:11:50 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.5686
2022-03-13 20:12:21 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 2.0515
2022-03-13 20:12:52 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.9465
2022-03-13 20:13:24 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.7650
2022-03-13 20:13:55 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.6707
2022-03-13 20:14:27 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.9096
2022-03-13 20:14:58 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.9332
2022-03-13 20:15:29 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.6876
2022-03-13 20:16:00 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 2.0708
2022-03-13 20:16:32 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.9473
2022-03-13 20:17:03 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 2.0129
2022-03-13 20:17:35 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.7922
2022-03-13 20:18:06 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 2.0105
2022-03-13 20:18:38 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.7345
2022-03-13 20:19:09 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.6193
2022-03-13 20:19:40 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.7902
2022-03-13 20:20:10 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.6934
2022-03-13 20:20:11 - train: epoch 045, train_loss: 1.8191
2022-03-13 20:21:21 - eval: epoch: 045, acc1: 62.220%, acc5: 84.742%, test_loss: 1.5460, per_image_load_time: 2.141ms, per_image_inference_time: 0.153ms
2022-03-13 20:21:21 - until epoch: 045, best_acc1: 62.890%
2022-03-13 20:21:21 - epoch 046 lr: 0.010000000000000002
2022-03-13 20:21:57 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.4829
2022-03-13 20:22:30 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.6293
2022-03-13 20:23:01 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.7769
2022-03-13 20:23:33 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.9302
2022-03-13 20:24:03 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.6467
2022-03-13 20:24:34 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.8599
2022-03-13 20:25:06 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.7078
2022-03-13 20:25:38 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.9824
2022-03-13 20:26:09 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.7975
2022-03-13 20:26:40 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.6595
2022-03-13 20:27:10 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.8503
2022-03-13 20:27:43 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.8137
2022-03-13 20:28:13 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.7890
2022-03-13 20:28:45 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.9764
2022-03-13 20:29:16 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.7843
2022-03-13 20:29:48 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.8372
2022-03-13 20:30:19 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.7124
2022-03-13 20:30:51 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.7728
2022-03-13 20:31:22 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.6023
2022-03-13 20:31:53 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.7797
2022-03-13 20:32:25 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.9036
2022-03-13 20:32:57 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.6063
2022-03-13 20:33:27 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 2.0360
2022-03-13 20:33:58 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.8014
2022-03-13 20:34:31 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.8117
2022-03-13 20:35:02 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.9755
2022-03-13 20:35:34 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.7636
2022-03-13 20:36:04 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.6943
2022-03-13 20:36:36 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.8392
2022-03-13 20:37:07 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.6749
2022-03-13 20:37:39 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.6382
2022-03-13 20:38:10 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.9988
2022-03-13 20:38:42 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.9028
2022-03-13 20:39:12 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.7644
2022-03-13 20:39:45 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.8636
2022-03-13 20:40:16 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.7284
2022-03-13 20:40:47 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.6822
2022-03-13 20:41:18 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.9540
2022-03-13 20:41:50 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.7808
2022-03-13 20:42:21 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.6727
2022-03-13 20:42:53 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 2.0175
2022-03-13 20:43:23 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.6697
2022-03-13 20:43:55 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 2.0876
2022-03-13 20:44:26 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.9850
2022-03-13 20:44:58 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.7555
2022-03-13 20:45:29 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.8970
2022-03-13 20:46:01 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.7385
2022-03-13 20:46:32 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.7920
2022-03-13 20:47:04 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.9307
2022-03-13 20:47:33 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.8352
2022-03-13 20:47:34 - train: epoch 046, train_loss: 1.8198
2022-03-13 20:48:45 - eval: epoch: 046, acc1: 62.588%, acc5: 84.864%, test_loss: 1.5313, per_image_load_time: 2.271ms, per_image_inference_time: 0.155ms
2022-03-13 20:48:45 - until epoch: 046, best_acc1: 62.890%
2022-03-13 20:48:45 - epoch 047 lr: 0.010000000000000002
2022-03-13 20:49:21 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.8260
2022-03-13 20:49:52 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.9648
2022-03-13 20:50:24 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.6774
2022-03-13 20:50:56 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.6244
2022-03-13 20:51:26 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.8506
2022-03-13 20:51:58 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.9487
2022-03-13 20:52:29 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.8810
2022-03-13 20:53:01 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.6785
2022-03-13 20:53:32 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.9998
2022-03-13 20:54:04 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.8607
2022-03-13 20:54:35 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.8545
2022-03-13 20:55:06 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.7074
2022-03-13 20:55:37 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.9305
2022-03-13 20:56:09 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.8643
2022-03-13 20:56:40 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.7326
2022-03-13 20:57:12 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.7170
2022-03-13 20:57:43 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.7136
2022-03-13 20:58:14 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.8784
2022-03-13 20:58:45 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.7695
2022-03-13 20:59:18 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.8001
2022-03-13 20:59:49 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.9360
2022-03-13 21:00:21 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.8934
2022-03-13 21:00:51 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.8598
2022-03-13 21:01:23 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.5732
2022-03-13 21:01:54 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.8666
2022-03-13 21:02:26 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 2.1043
2022-03-13 21:02:57 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.7383
2022-03-13 21:03:29 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 2.0206
2022-03-13 21:04:00 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.8135
2022-03-13 21:04:32 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.8342
2022-03-13 21:05:03 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.7338
2022-03-13 21:05:36 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 2.1634
2022-03-13 21:06:06 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.7483
2022-03-13 21:06:38 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.8491
2022-03-13 21:07:09 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.9858
2022-03-13 21:07:41 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.9513
2022-03-13 21:08:11 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 2.0323
2022-03-13 21:08:43 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.8299
2022-03-13 21:09:14 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.6765
2022-03-13 21:09:45 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.8872
2022-03-13 21:10:17 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.8336
2022-03-13 21:10:47 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.8561
2022-03-13 21:11:20 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.7317
2022-03-13 21:11:50 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.8472
2022-03-13 21:12:23 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.9256
2022-03-13 21:12:54 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.8578
2022-03-13 21:13:25 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.8268
2022-03-13 21:13:56 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.7865
2022-03-13 21:14:28 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.9390
2022-03-13 21:14:58 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.6318
2022-03-13 21:14:59 - train: epoch 047, train_loss: 1.8199
2022-03-13 21:16:09 - eval: epoch: 047, acc1: 62.702%, acc5: 84.960%, test_loss: 1.5292, per_image_load_time: 2.149ms, per_image_inference_time: 0.157ms
2022-03-13 21:16:09 - until epoch: 047, best_acc1: 62.890%
2022-03-13 21:16:09 - epoch 048 lr: 0.010000000000000002
2022-03-13 21:16:45 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.9783
2022-03-13 21:17:17 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.2361
2022-03-13 21:17:48 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.7823
2022-03-13 21:18:20 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.8095
2022-03-13 21:18:51 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.6451
2022-03-13 21:19:23 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.8282
2022-03-13 21:19:55 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.7845
2022-03-13 21:20:26 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.9388
2022-03-13 21:20:57 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 2.0446
2022-03-13 21:21:29 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.8948
2022-03-13 21:22:02 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.8750
2022-03-13 21:22:33 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.8261
2022-03-13 21:23:05 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.5167
2022-03-13 21:23:37 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.7731
2022-03-13 21:24:09 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.9443
2022-03-13 21:24:41 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.7729
2022-03-13 21:25:13 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.9618
2022-03-13 21:25:45 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.8522
2022-03-13 21:26:18 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 2.0023
2022-03-13 21:26:50 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 2.0244
2022-03-13 21:27:21 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.7697
2022-03-13 21:27:53 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.9665
2022-03-13 21:28:24 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.6900
2022-03-13 21:28:57 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.8805
2022-03-13 21:29:29 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.8230
2022-03-13 21:30:01 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 2.0812
2022-03-13 21:30:33 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.9914
2022-03-13 21:31:05 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.7476
2022-03-13 21:31:38 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.8562
2022-03-13 21:32:09 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.8042
2022-03-13 21:32:41 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.8194
2022-03-13 21:33:14 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.6736
2022-03-13 21:33:46 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.9529
2022-03-13 21:34:18 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.7317
2022-03-13 21:34:50 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 2.0939
2022-03-13 21:35:21 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.8663
2022-03-13 21:35:54 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.8287
2022-03-13 21:36:25 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.8678
2022-03-13 21:36:58 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.9200
2022-03-13 21:37:30 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.6279
2022-03-13 21:38:02 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.9867
2022-03-13 21:38:34 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.7575
2022-03-13 21:39:05 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.7447
2022-03-13 21:39:37 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.6844
2022-03-13 21:40:10 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.8415
2022-03-13 21:40:41 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.8267
2022-03-13 21:41:13 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.8771
2022-03-13 21:41:46 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 2.1113
2022-03-13 21:42:18 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.9653
2022-03-13 21:42:48 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.7956
2022-03-13 21:42:49 - train: epoch 048, train_loss: 1.8209
2022-03-13 21:44:01 - eval: epoch: 048, acc1: 62.660%, acc5: 84.998%, test_loss: 1.5294, per_image_load_time: 2.360ms, per_image_inference_time: 0.160ms
2022-03-13 21:44:01 - until epoch: 048, best_acc1: 62.890%
2022-03-13 21:44:01 - epoch 049 lr: 0.010000000000000002
2022-03-13 21:44:38 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 2.0130
2022-03-13 21:45:09 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.7310
2022-03-13 21:45:41 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.8723
2022-03-13 21:46:12 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.8966
2022-03-13 21:46:45 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.6926
2022-03-13 21:47:16 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.6959
2022-03-13 21:47:49 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.8432
2022-03-13 21:48:20 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 2.1364
2022-03-13 21:48:52 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.6362
2022-03-13 21:49:24 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.7904
2022-03-13 21:49:57 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.6600
2022-03-13 21:50:28 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.6274
2022-03-13 21:51:00 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.9943
2022-03-13 21:51:32 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.9284
2022-03-13 21:52:04 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.6836
2022-03-13 21:52:36 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.9817
2022-03-13 21:53:08 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.9706
2022-03-13 21:53:39 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.6778
2022-03-13 21:54:11 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.7114
2022-03-13 21:54:42 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.7718
2022-03-13 21:55:14 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.8423
2022-03-13 21:55:47 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.7181
2022-03-13 21:56:18 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.7131
2022-03-13 21:56:51 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.9644
2022-03-13 21:57:22 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.9136
2022-03-13 21:57:54 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.7794
2022-03-13 21:58:27 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.5879
2022-03-13 21:58:58 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.7150
2022-03-13 21:59:31 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.9202
2022-03-13 22:00:02 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.8155
2022-03-13 22:00:35 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.9353
2022-03-13 22:01:06 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.9168
2022-03-13 22:01:39 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.8481
2022-03-13 22:02:10 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.9382
2022-03-13 22:02:43 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.9285
2022-03-13 22:03:14 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.9597
2022-03-13 22:03:47 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.9230
2022-03-13 22:04:18 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.9818
2022-03-13 22:04:51 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 2.0782
2022-03-13 22:05:23 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.7663
2022-03-13 22:05:55 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.8354
2022-03-13 22:06:26 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.9633
2022-03-13 22:06:58 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 2.0735
2022-03-13 22:07:31 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.8770
2022-03-13 22:08:03 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.8480
2022-03-13 22:08:35 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.9556
2022-03-13 22:09:07 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.9578
2022-03-13 22:09:38 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.6716
2022-03-13 22:10:11 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.6689
2022-03-13 22:10:40 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.7300
2022-03-13 22:10:41 - train: epoch 049, train_loss: 1.8152
2022-03-13 22:11:53 - eval: epoch: 049, acc1: 62.520%, acc5: 84.908%, test_loss: 1.5335, per_image_load_time: 1.676ms, per_image_inference_time: 0.158ms
2022-03-13 22:11:53 - until epoch: 049, best_acc1: 62.890%
2022-03-13 22:11:53 - epoch 050 lr: 0.010000000000000002
2022-03-13 22:12:30 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 2.0394
2022-03-13 22:13:03 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.8300
2022-03-13 22:13:34 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.7934
2022-03-13 22:14:07 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.7129
2022-03-13 22:14:38 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.9050
2022-03-13 22:15:10 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.9907
2022-03-13 22:15:43 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.5441
2022-03-13 22:16:14 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.5839
2022-03-13 22:16:47 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.6120
2022-03-13 22:17:18 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.9885
2022-03-13 22:17:52 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.9070
2022-03-13 22:18:23 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.7937
2022-03-13 22:18:55 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.6218
2022-03-13 22:19:27 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.8595
2022-03-13 22:20:00 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.7821
2022-03-13 22:20:31 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.6701
2022-03-13 22:21:04 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.8058
2022-03-13 22:21:35 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.8732
2022-03-13 22:22:07 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.7044
2022-03-13 22:22:39 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.7846
2022-03-13 22:23:12 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.4873
2022-03-13 22:23:45 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.7951
2022-03-13 22:24:17 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.8017
2022-03-13 22:24:48 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.8109
2022-03-13 22:25:21 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.9246
2022-03-13 22:25:53 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.6077
2022-03-13 22:26:25 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.9855
2022-03-13 22:26:56 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 2.0962
2022-03-13 22:27:29 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 2.0083
2022-03-13 22:28:01 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 2.0336
2022-03-13 22:28:33 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.7992
2022-03-13 22:29:05 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.7504
2022-03-13 22:29:38 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.7058
2022-03-13 22:30:10 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.7166
2022-03-13 22:30:42 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.8144
2022-03-13 22:31:14 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.7978
2022-03-13 22:31:46 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.9589
2022-03-13 22:32:18 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.6203
2022-03-13 22:32:50 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.5578
2022-03-13 22:33:21 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.9466
2022-03-13 22:33:53 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.7063
2022-03-13 22:34:25 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.8902
2022-03-13 22:34:58 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.7509
2022-03-13 22:35:30 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.6873
2022-03-13 22:36:02 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.7544
2022-03-13 22:36:34 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.8582
2022-03-13 22:37:06 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.8676
2022-03-13 22:37:37 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.5960
2022-03-13 22:38:09 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.6505
2022-03-13 22:38:39 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.8005
2022-03-13 22:38:40 - train: epoch 050, train_loss: 1.8136
2022-03-13 22:39:52 - eval: epoch: 050, acc1: 62.816%, acc5: 85.074%, test_loss: 1.5230, per_image_load_time: 1.378ms, per_image_inference_time: 0.176ms
2022-03-13 22:39:53 - until epoch: 050, best_acc1: 62.890%
2022-03-13 22:39:53 - epoch 051 lr: 0.010000000000000002
