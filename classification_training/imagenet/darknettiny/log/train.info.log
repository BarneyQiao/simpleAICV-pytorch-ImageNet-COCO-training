2022-02-28 07:30:36 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 2.2357
2022-02-28 07:31:08 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 2.5218
2022-02-28 07:31:42 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 2.2234
2022-02-28 07:32:15 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 2.4796
2022-02-28 07:32:48 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 2.4993
2022-02-28 07:33:22 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 2.3815
2022-02-28 07:33:55 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 2.3295
2022-02-28 07:34:27 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 2.3866
2022-02-28 07:35:02 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 2.3137
2022-02-28 07:35:35 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 2.2188
2022-02-28 07:36:08 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 2.4373
2022-02-28 07:36:41 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 2.4386
2022-02-28 07:37:15 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 2.4249
2022-02-28 07:37:47 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 2.5013
2022-02-28 07:38:21 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 2.0368
2022-02-28 07:38:53 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 2.4084
2022-02-28 07:39:26 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 2.3646
2022-02-28 07:40:00 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 2.7639
2022-02-28 07:40:33 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 2.4063
2022-02-28 07:41:06 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 2.2374
2022-02-28 07:41:40 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 2.5544
2022-02-28 07:42:13 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 2.3732
2022-02-28 07:42:45 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 2.0240
2022-02-28 07:42:47 - train: epoch 079, train_loss: 2.3987
2022-02-28 07:44:02 - eval: epoch: 079, acc1: 54.128%, acc5: 77.202%, test_loss: 2.0658, per_image_load_time: 2.431ms, per_image_inference_time: 0.141ms
2022-02-28 07:44:02 - until epoch: 079, best_acc1: 54.172%
2022-02-28 07:44:02 - epoch 080 lr: 0.0010000000000000002
2022-02-28 07:44:40 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 2.1653
2022-02-28 07:45:14 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 2.4819
2022-02-28 07:45:45 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 2.2801
2022-02-28 07:46:19 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 2.4798
2022-02-28 07:46:53 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 2.2341
2022-02-28 07:47:26 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 2.4421
2022-02-28 07:47:58 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 2.3777
2022-02-28 07:48:32 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 2.0173
2022-02-28 07:49:04 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 2.2699
2022-02-28 07:49:37 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 2.3661
2022-02-28 07:50:09 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 2.3545
2022-02-28 07:50:43 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 2.3888
2022-02-28 07:51:16 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 2.3051
2022-02-28 07:51:50 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 2.3955
2022-02-28 07:52:24 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 2.3428
2022-02-28 07:52:58 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 2.4806
2022-02-28 07:53:30 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 2.5451
2022-02-28 07:54:05 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 2.4828
2022-02-28 07:54:39 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 2.4743
2022-02-28 07:55:12 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 2.3401
2022-02-28 07:55:45 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 2.5467
2022-02-28 07:56:18 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 2.6224
2022-02-28 07:56:51 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 2.5111
2022-02-28 07:57:26 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 2.5721
2022-02-28 07:57:58 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 2.4383
2022-02-28 07:58:32 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 2.3206
2022-02-28 07:59:03 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 2.3805
2022-02-28 07:59:37 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 2.4491
2022-02-28 08:00:10 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 2.6339
2022-02-28 08:00:44 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 2.3671
2022-02-28 08:01:16 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 2.2067
2022-02-28 08:01:50 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 2.1907
2022-02-28 08:02:23 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 2.3579
2022-02-28 08:02:57 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 2.3129
2022-02-28 08:03:29 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 2.3630
2022-02-28 08:04:03 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 2.3869
2022-02-28 08:04:36 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 2.5281
2022-02-28 08:05:10 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 2.4362
2022-02-28 08:05:42 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 2.1979
2022-02-28 08:06:16 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 2.6620
2022-02-28 08:06:49 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 2.5324
2022-02-28 08:07:23 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 2.3808
2022-02-28 08:07:55 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 2.6477
2022-02-28 08:08:29 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 2.3310
2022-02-28 08:09:03 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 2.3591
2022-02-28 08:09:36 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 2.4516
2022-02-28 08:10:09 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 2.6791
2022-02-28 08:10:42 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 2.4166
2022-02-28 08:11:16 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 2.7853
2022-02-28 08:11:49 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 2.3486
2022-02-28 08:11:51 - train: epoch 080, train_loss: 2.3984
2022-02-28 08:13:08 - eval: epoch: 080, acc1: 54.306%, acc5: 77.266%, test_loss: 2.0618, per_image_load_time: 2.812ms, per_image_inference_time: 0.129ms
2022-02-28 08:13:08 - until epoch: 080, best_acc1: 54.306%
2022-02-28 22:20:39 - epoch 081 lr: 0.0010000000000000002
2022-02-28 22:21:16 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 2.3617
2022-02-28 22:21:48 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 2.1789
2022-02-28 22:22:20 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 2.5435
2022-02-28 22:22:53 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 2.2191
2022-02-28 22:23:24 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 2.6538
2022-02-28 22:23:58 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 2.4622
2022-02-28 22:24:29 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 2.3313
2022-02-28 22:25:02 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 2.3835
2022-02-28 22:25:33 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 2.4728
2022-02-28 22:26:06 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 2.5791
2022-02-28 22:26:38 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 2.3062
2022-02-28 22:27:10 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 2.3720
2022-02-28 22:27:42 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 2.3046
2022-02-28 22:28:14 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 2.1491
2022-02-28 22:28:46 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 2.4275
2022-02-28 22:29:18 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 2.3083
2022-02-28 22:29:51 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 2.3566
2022-02-28 22:30:26 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 2.4176
2022-02-28 22:30:58 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 2.3230
2022-02-28 22:31:31 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 2.4333
2022-02-28 22:32:03 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 2.2906
2022-02-28 22:32:35 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 2.2492
2022-02-28 22:33:08 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 2.3729
2022-02-28 22:33:39 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 2.2696
2022-02-28 22:34:13 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 2.4763
2022-02-28 22:34:44 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 2.6280
2022-02-28 22:35:16 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 2.4548
2022-02-28 22:35:48 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 2.3355
2022-02-28 22:36:21 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 2.2595
2022-02-28 22:36:53 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 2.2822
2022-02-28 22:37:26 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 2.3655
2022-02-28 22:37:58 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 2.3757
2022-02-28 22:38:31 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 2.2149
2022-02-28 22:39:03 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 2.5475
2022-02-28 22:39:36 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 2.4705
2022-02-28 22:40:08 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 2.2803
2022-02-28 22:40:42 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 2.4005
2022-02-28 22:41:13 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 2.4188
2022-02-28 22:41:46 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 2.6600
2022-02-28 22:42:19 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 2.6261
2022-02-28 22:42:51 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 2.2924
2022-02-28 22:43:23 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 2.5494
2022-02-28 22:43:56 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 2.4966
2022-02-28 22:44:28 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 2.6554
2022-02-28 22:45:01 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 2.3150
2022-02-28 22:45:33 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 2.3927
2022-02-28 22:46:07 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 2.6990
2022-02-28 22:46:40 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 2.3486
2022-02-28 22:47:14 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 2.3221
2022-02-28 22:47:44 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 2.1431
2022-02-28 22:47:45 - train: epoch 081, train_loss: 2.3982
2022-02-28 22:49:01 - eval: epoch: 081, acc1: 54.068%, acc5: 77.228%, test_loss: 2.0604, per_image_load_time: 2.852ms, per_image_inference_time: 0.113ms
2022-02-28 22:49:01 - until epoch: 081, best_acc1: 54.306%
2022-02-28 22:49:01 - epoch 082 lr: 0.0010000000000000002
2022-02-28 22:49:39 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 2.3531
2022-02-28 22:50:12 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 2.3778
2022-02-28 22:50:43 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 2.3664
2022-02-28 22:51:16 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 2.6832
2022-02-28 22:51:47 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 2.3971
2022-02-28 22:52:21 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 2.4654
2022-02-28 22:52:53 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 2.3590
2022-02-28 22:53:25 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 2.3960
2022-02-28 22:53:58 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 2.5299
2022-02-28 22:54:30 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 2.5370
2022-02-28 22:55:03 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 2.5460
2022-02-28 22:55:35 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 2.3866
2022-02-28 22:56:07 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 2.4612
2022-02-28 22:56:39 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 2.3128
2022-02-28 22:57:13 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 2.3929
2022-02-28 22:57:44 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 2.2543
2022-02-28 22:58:18 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 2.3306
2022-02-28 22:58:49 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 2.2622
2022-02-28 22:59:23 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 2.4399
2022-02-28 22:59:54 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 2.2609
2022-02-28 23:00:27 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 2.4834
2022-02-28 23:00:59 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 2.4080
2022-02-28 23:01:32 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 2.3615
2022-02-28 23:02:04 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 2.5238
2022-02-28 23:02:37 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 2.4706
2022-02-28 23:03:09 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 2.2390
2022-02-28 23:03:42 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 2.1871
2022-02-28 23:04:13 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 2.0512
2022-02-28 23:04:46 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 2.2994
2022-02-28 23:05:18 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 2.2089
2022-02-28 23:05:52 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 2.5011
2022-02-28 23:06:24 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 2.4905
2022-02-28 23:06:57 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 2.4220
2022-02-28 23:07:28 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 2.2439
2022-02-28 23:08:01 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 2.3341
2022-02-28 23:08:33 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 2.3870
2022-02-28 23:09:06 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 2.2959
2022-02-28 23:09:38 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 2.3898
2022-02-28 23:10:10 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 2.2901
2022-02-28 23:10:44 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 2.2270
2022-02-28 23:11:15 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 2.6283
2022-02-28 23:11:47 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 2.5016
2022-02-28 23:12:20 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 2.2924
2022-02-28 23:12:52 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 2.3279
2022-02-28 23:13:24 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 2.3683
2022-02-28 23:13:58 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 2.4298
2022-02-28 23:14:29 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 2.3696
2022-02-28 23:15:03 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 2.2715
2022-02-28 23:15:35 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 2.3155
2022-02-28 23:16:06 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 2.3119
2022-02-28 23:16:07 - train: epoch 082, train_loss: 2.3964
2022-02-28 23:17:21 - eval: epoch: 082, acc1: 54.038%, acc5: 77.308%, test_loss: 2.0629, per_image_load_time: 2.525ms, per_image_inference_time: 0.117ms
2022-02-28 23:17:21 - until epoch: 082, best_acc1: 54.306%
2022-02-28 23:17:21 - epoch 083 lr: 0.0010000000000000002
2022-02-28 23:17:57 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 2.5155
2022-02-28 23:18:30 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 2.3068
2022-02-28 23:19:03 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 2.4741
2022-02-28 23:19:35 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 2.4299
2022-02-28 23:20:08 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 2.4019
2022-02-28 23:20:40 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 2.3345
2022-02-28 23:21:12 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 2.5426
2022-02-28 23:21:44 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 2.2440
2022-02-28 23:22:17 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 2.2979
2022-02-28 23:22:49 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 2.5216
2022-02-28 23:23:21 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 2.5634
2022-02-28 23:23:55 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 2.4344
2022-02-28 23:24:26 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 2.3771
2022-02-28 23:24:58 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 2.4366
2022-02-28 23:25:30 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 2.4019
2022-02-28 23:26:02 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 2.4758
2022-02-28 23:26:35 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 2.4146
2022-02-28 23:27:08 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 2.3979
2022-02-28 23:27:39 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 2.3564
2022-02-28 23:28:12 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 2.2555
2022-02-28 23:28:44 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 2.0661
2022-02-28 23:29:16 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 2.2132
2022-02-28 23:29:48 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 2.5832
2022-02-28 23:30:21 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 2.2890
2022-02-28 23:30:53 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 2.5005
2022-02-28 23:31:26 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 2.3280
2022-02-28 23:31:57 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 2.4736
2022-02-28 23:32:31 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 2.2724
2022-02-28 23:33:03 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 2.5448
2022-02-28 23:33:36 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 2.3964
2022-02-28 23:34:09 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 2.1988
2022-02-28 23:34:42 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 2.1910
2022-02-28 23:35:15 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 2.4750
2022-02-28 23:35:48 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 2.3719
2022-02-28 23:36:22 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 2.3639
2022-02-28 23:36:54 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 2.3369
2022-02-28 23:37:27 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 2.2496
2022-02-28 23:37:59 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 2.2569
2022-02-28 23:38:32 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 2.6674
2022-02-28 23:39:04 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 2.5928
2022-02-28 23:39:36 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 2.4449
2022-02-28 23:40:09 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 2.6047
2022-02-28 23:40:41 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 2.3268
2022-02-28 23:41:12 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 2.3469
2022-02-28 23:41:45 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 2.3385
2022-02-28 23:42:18 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 2.4488
2022-02-28 23:42:48 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 2.8154
2022-02-28 23:43:22 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 2.5301
2022-02-28 23:43:54 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 2.4494
2022-02-28 23:44:26 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 2.3462
2022-02-28 23:44:26 - train: epoch 083, train_loss: 2.3954
2022-02-28 23:45:38 - eval: epoch: 083, acc1: 54.208%, acc5: 77.330%, test_loss: 2.0600, per_image_load_time: 1.296ms, per_image_inference_time: 0.120ms
2022-02-28 23:45:38 - until epoch: 083, best_acc1: 54.306%
2022-02-28 23:45:38 - epoch 084 lr: 0.0010000000000000002
2022-02-28 23:46:16 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 2.5195
2022-02-28 23:46:47 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 2.4761
2022-02-28 23:47:18 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 2.1790
2022-02-28 23:47:51 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 2.6101
2022-02-28 23:48:23 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 2.6426
2022-02-28 23:48:55 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 2.6668
2022-02-28 23:49:26 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 2.4756
2022-02-28 23:49:58 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 2.6441
2022-02-28 23:50:31 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 2.4134
2022-02-28 23:51:04 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 2.6117
2022-02-28 23:51:35 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 2.2778
2022-02-28 23:52:09 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 2.5024
2022-02-28 23:52:41 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 2.3792
2022-02-28 23:53:14 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 2.2079
2022-02-28 23:53:47 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 2.3114
2022-02-28 23:54:21 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 2.4975
2022-02-28 23:54:53 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 2.6055
2022-02-28 23:55:26 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 2.4064
2022-02-28 23:55:58 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 2.1975
2022-02-28 23:56:32 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 2.3700
2022-02-28 23:57:04 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 2.5339
2022-02-28 23:57:37 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 2.2019
2022-02-28 23:58:10 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 2.6771
2022-02-28 23:58:43 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 2.2134
2022-02-28 23:59:17 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 2.4970
2022-02-28 23:59:49 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 2.2838
2022-03-01 00:00:22 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 2.4110
2022-03-01 00:00:56 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 2.4297
2022-03-01 00:01:29 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 2.3466
2022-03-01 00:02:02 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 2.4445
2022-03-01 00:02:35 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 2.4982
2022-03-01 00:03:08 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 2.4358
2022-03-01 00:03:41 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 2.5208
2022-03-01 00:04:15 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 2.2887
2022-03-01 00:04:47 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 2.4297
2022-03-01 00:05:21 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 2.3552
2022-03-01 00:05:53 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 2.6821
2022-03-01 00:06:26 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 2.2375
2022-03-01 00:06:59 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 2.4905
2022-03-01 00:07:33 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 2.3618
2022-03-01 00:08:06 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 2.2230
2022-03-01 00:08:38 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 2.5660
2022-03-01 00:09:12 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 2.5299
2022-03-01 00:09:46 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 2.7249
2022-03-01 00:10:18 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 2.0463
2022-03-01 00:10:53 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 2.3587
2022-03-01 00:11:24 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 2.4289
2022-03-01 00:11:58 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 2.2196
2022-03-01 00:12:31 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 2.0497
2022-03-01 00:13:04 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 2.2731
2022-03-01 00:13:04 - train: epoch 084, train_loss: 2.3965
2022-03-01 00:14:18 - eval: epoch: 084, acc1: 54.114%, acc5: 77.320%, test_loss: 2.0635, per_image_load_time: 0.625ms, per_image_inference_time: 0.123ms
2022-03-01 00:14:18 - until epoch: 084, best_acc1: 54.306%
2022-03-01 00:14:18 - epoch 085 lr: 0.0010000000000000002
2022-03-01 00:14:56 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 2.1830
2022-03-01 00:15:28 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 2.4305
2022-03-01 00:16:02 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 2.6257
2022-03-01 00:16:35 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 2.6544
2022-03-01 00:17:08 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 2.6086
2022-03-01 00:17:39 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 2.2200
2022-03-01 00:18:12 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 2.4877
2022-03-01 00:18:45 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 2.3139
2022-03-01 00:19:18 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 2.2178
2022-03-01 00:19:52 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 2.4069
2022-03-01 00:20:24 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 2.5820
2022-03-01 00:20:56 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 2.2332
2022-03-01 00:21:31 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 2.3349
2022-03-01 00:22:03 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 2.3392
2022-03-01 00:22:36 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 2.3169
2022-03-01 00:23:10 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 2.3312
2022-03-01 00:23:42 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 2.8485
2022-03-01 00:24:16 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 2.1908
2022-03-01 00:24:48 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 2.0688
2022-03-01 00:25:21 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 2.2035
2022-03-01 00:25:55 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 2.1714
2022-03-01 00:26:28 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 2.3816
2022-03-01 00:27:00 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 2.3575
2022-03-01 00:27:33 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 2.6011
2022-03-01 00:28:07 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 2.7245
2022-03-01 00:28:39 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 2.3937
2022-03-01 00:29:13 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 2.1691
2022-03-01 00:29:45 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 2.4872
2022-03-01 00:30:19 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 2.4005
2022-03-01 00:30:51 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 2.6621
2022-03-01 00:31:25 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 2.5118
2022-03-01 00:31:57 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 2.5544
2022-03-01 00:32:30 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 2.4010
2022-03-01 00:33:03 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 2.5469
2022-03-01 00:33:37 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 2.5531
2022-03-01 00:34:09 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 2.4289
2022-03-01 00:34:43 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 2.0882
2022-03-01 00:35:15 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 2.3923
2022-03-01 00:35:49 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 2.3906
2022-03-01 00:36:21 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 2.4840
2022-03-01 00:36:55 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 2.4016
2022-03-01 00:37:28 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 2.6058
2022-03-01 00:38:02 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 2.1313
2022-03-01 00:38:33 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 2.4275
2022-03-01 00:39:08 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 2.4092
2022-03-01 00:39:40 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 2.5417
2022-03-01 00:40:13 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 2.6743
2022-03-01 00:40:47 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 2.2284
2022-03-01 00:41:20 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 2.4472
2022-03-01 00:41:52 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 2.1719
2022-03-01 00:41:53 - train: epoch 085, train_loss: 2.3950
2022-03-01 00:43:07 - eval: epoch: 085, acc1: 54.238%, acc5: 77.236%, test_loss: 2.0647, per_image_load_time: 1.199ms, per_image_inference_time: 0.129ms
2022-03-01 00:43:07 - until epoch: 085, best_acc1: 54.306%
2022-03-01 00:43:07 - epoch 086 lr: 0.0010000000000000002
2022-03-01 00:43:45 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 2.4479
2022-03-01 00:44:18 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 2.3874
2022-03-01 00:44:51 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 2.4082
2022-03-01 00:45:24 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 2.5680
2022-03-01 00:45:57 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 2.4693
2022-03-01 00:46:31 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 2.2102
2022-03-01 00:47:02 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 2.3058
2022-03-01 00:47:37 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 2.4614
2022-03-01 00:48:09 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 2.5163
2022-03-01 00:48:42 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 2.3849
2022-03-01 00:49:15 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 2.5224
2022-03-01 00:49:48 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 2.3618
2022-03-01 00:50:21 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 2.3301
2022-03-01 00:50:54 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 2.2540
2022-03-01 00:51:27 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 2.1914
2022-03-01 00:51:59 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 2.4448
2022-03-01 00:52:32 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 2.3928
2022-03-01 00:53:07 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 2.4241
2022-03-01 00:53:39 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 2.3235
2022-03-01 00:54:13 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 2.4869
2022-03-01 00:54:44 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 2.1443
2022-03-01 00:55:18 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 2.3410
2022-03-01 00:55:50 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 2.3973
2022-03-01 00:56:24 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 2.3186
2022-03-01 00:56:55 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 2.4818
2022-03-01 00:57:30 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 2.4343
2022-03-01 00:58:02 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 2.4250
2022-03-01 00:58:35 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 2.4473
2022-03-01 00:59:08 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 2.1985
2022-03-01 00:59:41 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 2.0923
2022-03-01 01:00:13 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 2.4365
2022-03-01 01:00:46 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 2.6304
2022-03-01 01:01:19 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 2.3491
2022-03-01 01:01:53 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 2.6218
2022-03-01 01:02:26 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 2.6023
2022-03-01 01:02:59 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 2.5027
2022-03-01 01:03:33 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 2.4669
2022-03-01 01:04:05 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 2.3308
2022-03-01 01:04:38 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 2.5613
2022-03-01 01:05:10 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 2.4919
2022-03-01 01:05:44 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 2.2561
2022-03-01 01:06:17 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 2.4096
2022-03-01 01:06:50 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 2.4019
2022-03-01 01:07:24 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 2.4024
2022-03-01 01:07:57 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 2.3209
2022-03-01 01:08:30 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 2.3663
2022-03-01 01:09:04 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 2.5094
2022-03-01 01:09:36 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 2.3621
2022-03-01 01:10:10 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 2.3653
2022-03-01 01:10:42 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 2.3945
2022-03-01 01:10:43 - train: epoch 086, train_loss: 2.3931
2022-03-01 01:11:58 - eval: epoch: 086, acc1: 54.122%, acc5: 77.232%, test_loss: 2.0601, per_image_load_time: 1.227ms, per_image_inference_time: 0.123ms
2022-03-01 01:11:58 - until epoch: 086, best_acc1: 54.306%
2022-03-01 01:11:58 - epoch 087 lr: 0.0010000000000000002
2022-03-01 01:12:36 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 2.4195
2022-03-01 01:13:10 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 2.3768
2022-03-01 01:13:41 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 2.4274
2022-03-01 01:14:14 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 2.4364
2022-03-01 01:14:46 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 2.3803
2022-03-01 01:15:20 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 2.6605
2022-03-01 01:15:52 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 2.3867
2022-03-01 01:16:25 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 2.2475
2022-03-01 01:16:58 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 2.2544
2022-03-01 01:17:32 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 2.2434
2022-03-01 01:18:04 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 2.4228
2022-03-01 01:18:38 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 2.4817
2022-03-01 01:19:10 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 2.6499
2022-03-01 01:19:44 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 2.2033
2022-03-01 01:20:16 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 2.3553
2022-03-01 01:20:49 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 2.3652
2022-03-01 01:21:22 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 2.6859
2022-03-01 01:21:55 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 2.5014
2022-03-01 01:22:28 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 2.4508
2022-03-01 01:23:02 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 2.4664
2022-03-01 01:23:35 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 2.4076
2022-03-01 01:24:08 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 2.4430
2022-03-01 01:24:40 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 2.4983
2022-03-01 01:25:14 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 2.3711
2022-03-01 01:25:46 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 2.3654
2022-03-01 01:26:20 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 2.3532
2022-03-01 01:26:53 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 2.3501
2022-03-01 01:27:25 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 2.2632
2022-03-01 01:27:57 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 2.3727
2022-03-01 01:28:30 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 2.4567
2022-03-01 01:29:04 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 2.3571
2022-03-01 01:29:36 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 2.4806
2022-03-01 01:30:09 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 2.4374
2022-03-01 01:30:43 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 2.2211
2022-03-01 01:31:15 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 2.4258
2022-03-01 01:31:48 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 2.2657
2022-03-01 01:32:21 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 2.2191
2022-03-01 01:32:55 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 2.3456
2022-03-01 01:33:27 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 2.3396
2022-03-01 01:34:01 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 2.3031
2022-03-01 01:34:34 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 2.4069
2022-03-01 01:35:06 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 2.2856
2022-03-01 01:35:40 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 2.3536
2022-03-01 01:36:12 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 2.4056
2022-03-01 01:36:47 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 2.2896
2022-03-01 01:37:19 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 2.5558
2022-03-01 01:37:53 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 2.4178
2022-03-01 01:38:25 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 2.2647
2022-03-01 01:38:59 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 2.2907
2022-03-01 01:39:30 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 2.5451
2022-03-01 01:39:31 - train: epoch 087, train_loss: 2.3911
2022-03-01 01:40:45 - eval: epoch: 087, acc1: 54.082%, acc5: 77.306%, test_loss: 2.0581, per_image_load_time: 1.595ms, per_image_inference_time: 0.136ms
2022-03-01 01:40:45 - until epoch: 087, best_acc1: 54.306%
2022-03-01 01:40:45 - epoch 088 lr: 0.0010000000000000002
2022-03-01 01:41:23 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 2.3833
2022-03-01 01:41:58 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 2.4091
2022-03-01 01:42:29 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 2.6096
2022-03-01 01:43:02 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 2.5199
2022-03-01 01:43:34 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 2.5444
2022-03-01 01:44:08 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 2.5350
2022-03-01 01:44:42 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 2.5114
2022-03-01 01:45:14 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 2.4914
2022-03-01 01:45:47 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 2.3344
2022-03-01 01:46:20 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 2.3830
2022-03-01 01:46:52 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 2.4926
2022-03-01 01:47:26 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 2.3429
2022-03-01 01:47:57 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 2.5204
2022-03-01 01:48:31 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 2.2196
2022-03-01 01:49:04 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 2.3344
2022-03-01 01:49:38 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 2.3162
2022-03-01 01:50:10 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 2.5514
2022-03-01 01:50:43 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 2.2160
2022-03-01 01:51:17 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 2.3200
2022-03-01 01:51:50 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 2.3767
2022-03-01 01:52:23 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 2.4041
2022-03-01 01:52:55 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 2.5003
2022-03-01 01:53:29 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 2.3631
2022-03-01 01:54:01 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 2.6773
2022-03-01 01:54:34 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 2.6847
2022-03-01 01:55:08 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 2.4068
2022-03-01 01:55:42 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 2.4586
2022-03-01 01:56:14 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 2.7741
2022-03-01 01:56:48 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 2.1885
2022-03-01 01:57:19 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 2.5739
2022-03-01 01:57:53 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 2.1635
2022-03-01 01:58:26 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 2.3436
2022-03-01 01:58:59 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 2.2680
2022-03-01 01:59:34 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 2.2126
2022-03-01 02:00:07 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 2.3853
2022-03-01 02:00:39 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 2.3668
2022-03-01 02:01:12 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 2.5125
2022-03-01 02:01:46 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 2.4507
2022-03-01 02:02:18 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 2.3218
2022-03-01 02:02:52 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 2.2162
2022-03-01 02:03:25 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 2.4023
2022-03-01 02:03:59 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 2.2896
2022-03-01 02:04:32 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 2.1679
2022-03-01 02:05:06 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 2.1685
2022-03-01 02:05:38 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 2.4644
2022-03-01 02:06:12 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 2.7068
2022-03-01 02:06:45 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 2.3526
2022-03-01 02:07:19 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 2.3473
2022-03-01 02:07:53 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 2.3668
2022-03-01 02:08:25 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 2.5805
2022-03-01 02:08:26 - train: epoch 088, train_loss: 2.3915
2022-03-01 02:09:41 - eval: epoch: 088, acc1: 54.268%, acc5: 77.438%, test_loss: 2.0576, per_image_load_time: 1.805ms, per_image_inference_time: 0.135ms
2022-03-01 02:09:41 - until epoch: 088, best_acc1: 54.306%
2022-03-01 02:09:41 - epoch 089 lr: 0.0010000000000000002
2022-03-01 02:10:20 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 2.5567
2022-03-01 02:10:52 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 2.1884
2022-03-01 02:11:26 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 2.3825
2022-03-01 02:11:58 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 2.3843
2022-03-01 02:12:31 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 2.3942
2022-03-01 02:13:04 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 2.3738
2022-03-01 02:13:36 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 2.6106
2022-03-01 02:14:09 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 2.6400
2022-03-01 02:14:42 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 2.2950
2022-03-01 02:15:13 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 2.4676
2022-03-01 02:15:47 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 2.3272
2022-03-01 02:16:20 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 2.7081
2022-03-01 02:16:53 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 2.3334
2022-03-01 02:17:25 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 2.5643
2022-03-01 02:17:59 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 2.4053
2022-03-01 02:18:32 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 2.2043
2022-03-01 02:19:05 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 2.4897
2022-03-01 02:19:37 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 2.5984
2022-03-01 02:20:11 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 2.3378
2022-03-01 02:20:43 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 2.2608
2022-03-01 02:21:17 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 2.3964
2022-03-01 02:21:50 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 2.2837
2022-03-01 02:22:23 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 2.3166
2022-03-01 02:22:56 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 2.6799
2022-03-01 02:23:28 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 2.4299
2022-03-01 02:24:01 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 2.3013
2022-03-01 02:24:35 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 2.3290
2022-03-01 02:25:07 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 2.4458
2022-03-01 02:25:41 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 2.8036
2022-03-01 02:26:13 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 2.2720
2022-03-01 02:26:47 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 2.4554
2022-03-01 02:27:18 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 2.6444
2022-03-01 02:27:52 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 2.6093
2022-03-01 02:28:25 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 2.2659
2022-03-01 02:28:58 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 2.3909
2022-03-01 02:29:30 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 2.3191
2022-03-01 02:30:04 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 2.3856
2022-03-01 02:30:36 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 2.3030
2022-03-01 02:31:10 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 2.4771
2022-03-01 02:31:43 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 2.4456
2022-03-01 02:32:16 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 2.4532
2022-03-01 02:32:50 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 2.3742
2022-03-01 02:33:23 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 2.5566
2022-03-01 02:33:56 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 2.4791
2022-03-01 02:34:29 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 2.1883
2022-03-01 02:35:01 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 2.3688
2022-03-01 02:35:34 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 2.4314
2022-03-01 02:36:07 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 2.2846
2022-03-01 02:36:41 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 2.6308
2022-03-01 02:37:13 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 2.1333
2022-03-01 02:37:14 - train: epoch 089, train_loss: 2.3914
2022-03-01 02:38:28 - eval: epoch: 089, acc1: 54.084%, acc5: 77.196%, test_loss: 2.0637, per_image_load_time: 1.274ms, per_image_inference_time: 0.124ms
2022-03-01 02:38:28 - until epoch: 089, best_acc1: 54.306%
2022-03-01 02:38:28 - epoch 090 lr: 0.0010000000000000002
2022-03-01 02:39:06 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 2.5051
2022-03-01 02:39:38 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 2.3951
2022-03-01 02:40:11 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 2.1014
2022-03-01 02:40:44 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 2.2789
2022-03-01 02:41:18 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 2.5339
2022-03-01 02:41:50 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 2.6545
2022-03-01 02:42:23 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 2.3856
2022-03-01 02:42:56 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 2.2734
2022-03-01 02:43:28 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 2.2831
2022-03-01 02:44:01 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 2.3291
2022-03-01 02:44:34 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 2.5323
2022-03-01 02:45:06 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 2.2610
2022-03-01 02:45:39 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 2.3658
2022-03-01 02:46:12 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 2.2564
2022-03-01 02:46:46 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 2.5796
2022-03-01 02:47:18 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 2.4475
2022-03-01 02:47:52 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 2.1443
2022-03-01 02:48:24 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 2.4220
2022-03-01 02:48:58 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 2.1840
2022-03-01 02:49:30 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 2.5530
2022-03-01 02:50:03 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 2.3135
2022-03-01 02:50:36 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 2.4178
2022-03-01 02:51:10 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 2.5165
2022-03-01 02:51:43 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 2.4113
2022-03-01 02:52:17 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 2.0363
2022-03-01 02:52:49 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 2.1341
2022-03-01 02:53:23 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 2.1903
2022-03-01 02:53:56 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 2.6832
2022-03-01 02:54:30 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 2.5684
2022-03-01 02:55:02 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 2.3239
2022-03-01 02:55:36 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 2.2428
2022-03-01 02:56:07 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 2.3974
2022-03-01 02:56:41 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 2.7610
2022-03-01 02:57:14 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 2.4258
2022-03-01 02:57:46 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 2.1982
2022-03-01 02:58:20 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 2.2475
2022-03-01 02:58:53 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 2.4929
2022-03-01 02:59:27 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 2.3642
2022-03-01 03:00:00 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 2.1001
2022-03-01 03:00:33 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 2.3498
2022-03-01 03:01:06 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 2.5699
2022-03-01 03:01:39 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 2.5827
2022-03-01 03:02:13 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 2.3889
2022-03-01 03:02:46 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 2.4269
2022-03-01 03:03:20 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 2.3125
2022-03-01 03:03:52 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 2.3737
2022-03-01 03:04:27 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 2.1633
2022-03-01 03:04:59 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 2.6070
2022-03-01 03:05:34 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 2.1979
2022-03-01 03:06:05 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 2.3314
2022-03-01 03:06:06 - train: epoch 090, train_loss: 2.3889
2022-03-01 03:07:20 - eval: epoch: 090, acc1: 54.120%, acc5: 77.270%, test_loss: 2.0605, per_image_load_time: 1.584ms, per_image_inference_time: 0.145ms
2022-03-01 03:07:20 - until epoch: 090, best_acc1: 54.306%
2022-03-01 03:07:20 - epoch 091 lr: 0.00010000000000000003
2022-03-01 03:07:57 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 2.1339
2022-03-01 03:08:30 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 2.3000
2022-03-01 03:09:05 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 2.3011
2022-03-01 03:09:37 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 2.6927
2022-03-01 03:10:11 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 2.2101
2022-03-01 03:10:42 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 2.2839
2022-03-01 03:11:16 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 2.1504
2022-03-01 03:11:48 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 1.9373
2022-03-01 03:12:21 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 2.3280
2022-03-01 03:12:54 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 2.1900
2022-03-01 03:13:27 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 2.2043
2022-03-01 03:14:00 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 2.3687
2022-03-01 03:14:34 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 2.1726
2022-03-01 03:15:05 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 2.3608
2022-03-01 03:15:38 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 2.2568
2022-03-01 03:16:12 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 2.4467
2022-03-01 03:16:46 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 2.3639
2022-03-01 03:17:18 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 2.4434
2022-03-01 03:17:51 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 2.3465
2022-03-01 03:18:24 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 2.2598
2022-03-01 03:18:57 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 2.3628
2022-03-01 03:19:31 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 2.4494
2022-03-01 03:20:03 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 2.2420
2022-03-01 03:20:36 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 2.5751
2022-03-01 03:21:09 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 2.2207
2022-03-01 03:21:42 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 2.1626
2022-03-01 03:22:15 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 2.4539
2022-03-01 03:22:48 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 2.4434
2022-03-01 03:23:21 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 2.3257
2022-03-01 03:23:54 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 2.4683
2022-03-01 03:24:27 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 2.5050
2022-03-01 03:25:01 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 2.6575
2022-03-01 03:25:33 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 2.4573
2022-03-01 03:26:07 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 2.4687
2022-03-01 03:26:40 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 2.3374
2022-03-01 03:27:12 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 2.4924
2022-03-01 03:27:45 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 2.5246
2022-03-01 03:28:17 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 2.5109
2022-03-01 03:28:51 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 2.1606
2022-03-01 03:29:23 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 2.2921
2022-03-01 03:29:57 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 2.1644
2022-03-01 03:30:30 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 2.6379
2022-03-01 03:31:03 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 2.1870
2022-03-01 03:31:36 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 2.2206
2022-03-01 03:32:09 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 2.0662
2022-03-01 03:32:41 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 2.4908
2022-03-01 03:33:15 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 2.4435
2022-03-01 03:33:48 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 2.2791
2022-03-01 03:34:22 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 2.1793
2022-03-01 03:34:54 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 2.6241
2022-03-01 03:34:55 - train: epoch 091, train_loss: 2.3685
2022-03-01 03:36:09 - eval: epoch: 091, acc1: 54.722%, acc5: 77.578%, test_loss: 2.0394, per_image_load_time: 1.016ms, per_image_inference_time: 0.147ms
2022-03-01 03:36:09 - until epoch: 091, best_acc1: 54.722%
2022-03-01 03:36:09 - epoch 092 lr: 0.00010000000000000003
2022-03-01 03:36:47 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 2.3685
2022-03-01 03:37:20 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 2.4964
2022-03-01 03:37:53 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 2.6190
2022-03-01 03:38:26 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 2.3093
2022-03-01 03:38:58 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 2.6142
2022-03-01 03:39:31 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 2.5733
2022-03-01 03:40:04 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 2.5112
2022-03-01 03:40:37 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 2.3255
2022-03-01 03:41:10 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 2.4637
2022-03-01 03:41:44 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 2.3905
2022-03-01 03:42:16 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 2.2170
2022-03-01 03:42:49 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 2.2852
2022-03-01 03:43:22 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 2.2038
2022-03-01 03:43:56 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 2.3691
2022-03-01 03:44:28 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 2.0992
2022-03-01 03:45:01 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 2.2463
2022-03-01 03:45:34 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 2.2783
2022-03-01 03:46:07 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 2.5282
2022-03-01 03:46:40 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 2.2047
2022-03-01 03:47:13 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 2.4635
2022-03-01 03:47:46 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 2.4050
2022-03-01 03:48:19 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 2.5318
2022-03-01 03:48:53 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 2.4404
2022-03-01 03:49:24 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 2.1777
2022-03-01 03:49:59 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 2.2406
2022-03-01 03:50:31 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 2.2038
2022-03-01 03:51:05 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 2.4384
2022-03-01 03:51:38 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 2.3724
2022-03-01 03:52:11 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 2.6655
2022-03-01 03:52:44 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 2.4051
2022-03-01 03:53:17 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 2.4878
2022-03-01 03:53:51 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 2.2632
2022-03-01 03:54:23 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 2.2101
2022-03-01 03:54:56 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 2.5902
2022-03-01 03:55:29 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 2.5064
2022-03-01 03:56:03 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 2.1579
2022-03-01 03:56:35 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 2.3812
2022-03-01 03:57:09 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 2.2918
2022-03-01 03:57:43 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 2.3550
2022-03-01 03:58:16 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 2.3146
2022-03-01 03:58:49 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 2.1431
2022-03-01 03:59:22 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 2.4694
2022-03-01 03:59:55 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 2.4266
2022-03-01 04:00:28 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 2.2941
2022-03-01 04:01:01 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 2.2790
2022-03-01 04:01:34 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 2.4485
2022-03-01 04:02:07 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 2.3484
2022-03-01 04:02:42 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 2.3942
2022-03-01 04:03:14 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 2.1737
2022-03-01 04:03:47 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 2.2740
2022-03-01 04:03:48 - train: epoch 092, train_loss: 2.3632
2022-03-01 04:05:03 - eval: epoch: 092, acc1: 54.716%, acc5: 77.634%, test_loss: 2.0398, per_image_load_time: 0.905ms, per_image_inference_time: 0.125ms
2022-03-01 04:05:03 - until epoch: 092, best_acc1: 54.722%
2022-03-01 04:05:03 - epoch 093 lr: 0.00010000000000000003
2022-03-01 04:05:41 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 2.2875
2022-03-01 04:06:12 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 2.1217
2022-03-01 04:06:46 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 2.4181
2022-03-01 04:07:19 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 2.4395
2022-03-01 04:07:52 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 2.3934
2022-03-01 04:08:26 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 2.4312
2022-03-01 04:08:58 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 2.3514
2022-03-01 04:09:31 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 2.3342
2022-03-01 04:10:03 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 2.4153
2022-03-01 04:10:36 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 2.2342
2022-03-01 04:11:09 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 2.4870
2022-03-01 04:11:41 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 2.3635
2022-03-01 04:12:15 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 2.2548
2022-03-01 04:12:46 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 2.3267
2022-03-01 04:13:19 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 2.3918
2022-03-01 04:13:53 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 2.2597
2022-03-01 04:14:27 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 2.4316
2022-03-01 04:14:58 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 2.4454
2022-03-01 04:15:32 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 2.0950
2022-03-01 04:16:04 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 2.2963
2022-03-01 04:16:38 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 2.2480
2022-03-01 04:17:10 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 2.3337
2022-03-01 04:17:45 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 2.3593
2022-03-01 04:18:16 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 2.2201
2022-03-01 04:18:50 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 2.1398
2022-03-01 04:19:22 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 2.6369
2022-03-01 04:19:57 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 2.3586
2022-03-01 04:20:29 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 2.3359
2022-03-01 04:21:02 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 2.1823
2022-03-01 04:21:35 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 2.2431
2022-03-01 04:22:08 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 2.6255
2022-03-01 04:22:40 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 2.2776
2022-03-01 04:23:14 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 2.2266
2022-03-01 04:23:47 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 2.6045
2022-03-01 04:24:21 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 2.2569
2022-03-01 04:24:53 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 2.4506
2022-03-01 04:25:25 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 2.3558
2022-03-01 04:25:57 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 2.3906
2022-03-01 04:26:31 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 2.3483
2022-03-01 04:27:03 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 2.6849
2022-03-01 04:27:37 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 2.4916
2022-03-01 04:28:10 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 2.4889
2022-03-01 04:28:43 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 2.4639
2022-03-01 04:29:16 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 2.2806
2022-03-01 04:29:48 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 2.2479
2022-03-01 04:30:22 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 2.1070
2022-03-01 04:30:55 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 2.8446
2022-03-01 04:31:28 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 2.3905
2022-03-01 04:32:01 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 2.1672
2022-03-01 04:32:32 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 2.2932
2022-03-01 04:32:33 - train: epoch 093, train_loss: 2.3617
2022-03-01 04:33:48 - eval: epoch: 093, acc1: 54.692%, acc5: 77.710%, test_loss: 2.0368, per_image_load_time: 1.569ms, per_image_inference_time: 0.127ms
2022-03-01 04:33:48 - until epoch: 093, best_acc1: 54.722%
2022-03-01 04:33:48 - epoch 094 lr: 0.00010000000000000003
2022-03-01 04:34:26 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 2.2251
2022-03-01 04:34:59 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 2.3163
2022-03-01 04:35:31 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 2.4122
2022-03-01 04:36:05 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 2.3484
2022-03-01 04:36:37 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 2.2683
2022-03-01 04:37:10 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 2.1534
2022-03-01 04:37:43 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 2.5738
2022-03-01 04:38:16 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 2.3974
2022-03-01 04:38:48 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 2.4041
2022-03-01 04:39:20 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 2.3233
2022-03-01 04:39:52 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 2.4820
2022-03-01 04:40:25 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 2.3807
2022-03-01 04:40:58 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 2.1653
2022-03-01 04:41:32 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 2.2602
2022-03-01 04:42:03 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 2.2776
2022-03-01 04:42:37 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 2.5462
2022-03-01 04:43:10 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 2.1540
2022-03-01 04:43:42 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 2.1642
2022-03-01 04:44:16 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 2.2973
2022-03-01 04:44:48 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 2.1028
2022-03-01 04:45:21 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 2.2199
2022-03-01 04:45:55 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 2.2900
2022-03-01 04:46:28 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 2.2128
2022-03-01 04:47:02 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 2.4150
2022-03-01 04:47:34 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 2.4210
2022-03-01 04:48:08 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 2.2635
2022-03-01 04:48:41 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 2.3910
2022-03-01 04:49:15 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 2.7286
2022-03-01 04:49:47 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 2.1469
2022-03-01 04:50:21 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 2.4356
2022-03-01 04:50:53 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 2.5031
2022-03-01 04:51:26 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 2.3926
2022-03-01 04:51:59 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 2.1714
2022-03-01 04:52:33 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 2.4449
2022-03-01 04:53:06 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 2.7415
2022-03-01 04:53:40 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 2.4661
2022-03-01 04:54:12 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 2.4840
2022-03-01 04:54:45 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 2.2473
2022-03-01 04:55:18 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 2.3975
2022-03-01 04:55:51 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 2.4910
2022-03-01 04:56:24 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 2.5291
2022-03-01 04:56:59 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 2.4932
2022-03-01 04:57:30 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 2.4778
2022-03-01 04:58:04 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 2.6118
2022-03-01 04:58:38 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 2.3668
2022-03-01 04:59:13 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 2.3443
2022-03-01 04:59:45 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 2.2711
2022-03-01 05:00:19 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 2.2876
2022-03-01 05:00:51 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 2.4333
2022-03-01 05:01:24 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 2.1317
2022-03-01 05:01:25 - train: epoch 094, train_loss: 2.3624
2022-03-01 05:02:39 - eval: epoch: 094, acc1: 54.710%, acc5: 77.672%, test_loss: 2.0357, per_image_load_time: 0.969ms, per_image_inference_time: 0.135ms
2022-03-01 05:02:39 - until epoch: 094, best_acc1: 54.722%
2022-03-01 05:02:39 - epoch 095 lr: 0.00010000000000000003
2022-03-01 05:03:18 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 2.5584
2022-03-01 05:03:49 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 2.4947
2022-03-01 05:04:23 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 2.2569
2022-03-01 05:04:55 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 2.3637
2022-03-01 05:05:28 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 2.4664
2022-03-01 05:06:00 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 2.5810
2022-03-01 05:06:33 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 2.6369
2022-03-01 05:07:07 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 2.4257
2022-03-01 05:07:39 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 2.7198
2022-03-01 05:08:12 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 2.4018
2022-03-01 05:08:45 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 2.3593
2022-03-01 05:09:17 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 2.4660
2022-03-01 05:09:51 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 2.3489
2022-03-01 05:10:24 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 2.2397
2022-03-01 05:10:57 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 2.3466
2022-03-01 05:11:31 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 2.2613
2022-03-01 05:12:03 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 2.5321
2022-03-01 05:12:37 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 2.5108
2022-03-01 05:13:09 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 2.2141
2022-03-01 05:13:43 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 2.3218
2022-03-01 05:14:15 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 2.3849
2022-03-01 05:14:49 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 2.2050
2022-03-01 05:15:22 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 2.3127
2022-03-01 05:15:56 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 2.5419
2022-03-01 05:16:28 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 2.2487
2022-03-01 05:17:01 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 2.3191
2022-03-01 05:17:33 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 2.1690
2022-03-01 05:18:08 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 2.2957
2022-03-01 05:18:41 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 2.4877
2022-03-01 05:19:15 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 2.5977
2022-03-01 05:19:47 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 2.4248
2022-03-01 05:20:21 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 2.3271
2022-03-01 05:20:52 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 2.3062
2022-03-01 05:21:27 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 2.3335
2022-03-01 05:22:00 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 2.1969
2022-03-01 05:22:33 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 2.1558
2022-03-01 05:23:07 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 2.2846
2022-03-01 05:23:39 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 2.3654
2022-03-01 05:24:13 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 2.4995
2022-03-01 05:24:45 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 2.3707
2022-03-01 05:25:19 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 2.2861
2022-03-01 05:25:52 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 2.4088
2022-03-01 05:26:26 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 2.3548
2022-03-01 05:26:59 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 2.5576
2022-03-01 05:27:33 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 2.3713
2022-03-01 05:28:05 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 2.3065
2022-03-01 05:28:38 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 2.5035
2022-03-01 05:29:10 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 2.3569
2022-03-01 05:29:44 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 2.0875
2022-03-01 05:30:16 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 2.2112
2022-03-01 05:30:17 - train: epoch 095, train_loss: 2.3601
2022-03-01 05:31:33 - eval: epoch: 095, acc1: 54.706%, acc5: 77.686%, test_loss: 2.0359, per_image_load_time: 1.109ms, per_image_inference_time: 0.108ms
2022-03-01 05:31:33 - until epoch: 095, best_acc1: 54.722%
2022-03-01 05:31:33 - epoch 096 lr: 0.00010000000000000003
2022-03-01 05:32:11 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 2.5047
2022-03-01 05:32:44 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 2.1199
2022-03-01 05:33:17 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 2.2384
2022-03-01 05:33:48 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 2.1452
2022-03-01 05:34:22 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 2.1779
2022-03-01 05:34:53 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 2.5308
2022-03-01 05:35:26 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 2.2072
2022-03-01 05:35:58 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 2.1379
2022-03-01 05:36:32 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 2.2021
2022-03-01 05:37:04 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 2.5907
2022-03-01 05:37:38 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 2.2764
2022-03-01 05:38:10 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 2.1468
2022-03-01 05:38:43 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 2.3385
2022-03-01 05:39:15 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 2.3329
2022-03-01 05:39:49 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 2.1855
2022-03-01 05:40:21 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 2.1619
2022-03-01 05:40:54 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 2.1161
2022-03-01 05:41:26 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 2.4587
2022-03-01 05:41:59 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 2.6409
2022-03-01 05:42:31 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 2.4029
2022-03-01 05:43:05 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 2.4080
2022-03-01 05:43:37 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 1.9674
2022-03-01 05:44:10 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 2.1600
2022-03-01 05:44:42 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 2.3151
2022-03-01 05:45:15 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 2.3823
2022-03-01 05:45:47 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 2.1594
2022-03-01 05:46:20 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 2.5345
2022-03-01 05:46:52 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 2.3785
2022-03-01 05:47:25 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 2.6372
2022-03-01 05:47:57 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 2.6352
2022-03-01 05:48:30 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 2.5205
2022-03-01 05:49:03 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 2.5395
2022-03-01 05:49:35 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 2.4019
2022-03-01 05:50:08 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 2.3357
2022-03-01 05:50:41 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 2.1100
2022-03-01 05:51:12 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 2.1271
2022-03-01 05:51:46 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 2.2902
2022-03-01 05:52:19 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 2.0787
2022-03-01 05:52:52 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 2.3746
2022-03-01 05:53:24 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 2.3140
2022-03-01 05:53:57 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 2.2840
2022-03-01 05:54:29 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 2.3551
2022-03-01 05:55:01 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 2.0657
2022-03-01 05:55:34 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 2.2147
2022-03-01 05:56:06 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 2.3464
2022-03-01 05:56:39 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 2.0750
2022-03-01 05:57:12 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 2.4122
2022-03-01 05:57:46 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 2.4192
2022-03-01 05:58:18 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 2.7344
2022-03-01 05:58:50 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 2.4258
2022-03-01 05:58:51 - train: epoch 096, train_loss: 2.3604
2022-03-01 06:00:06 - eval: epoch: 096, acc1: 54.648%, acc5: 77.656%, test_loss: 2.0335, per_image_load_time: 0.622ms, per_image_inference_time: 0.114ms
2022-03-01 06:00:06 - until epoch: 096, best_acc1: 54.722%
2022-03-01 06:00:06 - epoch 097 lr: 0.00010000000000000003
2022-03-01 06:00:44 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 2.4711
2022-03-01 06:01:16 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 2.2539
2022-03-01 06:01:49 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 2.3095
2022-03-01 06:02:22 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 2.5541
2022-03-01 06:02:54 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 2.5962
2022-03-01 06:03:27 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 2.3514
2022-03-01 06:03:58 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 2.2590
2022-03-01 06:04:32 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 2.3307
2022-03-01 06:05:04 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 2.0769
2022-03-01 06:05:37 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 2.5718
2022-03-01 06:06:09 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 2.2640
2022-03-01 06:06:42 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 2.5324
2022-03-01 06:07:15 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 2.3376
2022-03-01 06:07:48 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 2.4319
2022-03-01 06:08:20 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 2.4576
2022-03-01 06:08:53 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 2.1317
2022-03-01 06:09:25 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 2.3572
2022-03-01 06:09:58 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 2.2741
2022-03-01 06:10:31 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 2.1656
2022-03-01 06:11:03 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 2.3478
2022-03-01 06:11:36 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 2.3361
2022-03-01 06:12:08 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 2.3395
2022-03-01 06:12:40 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 2.1851
2022-03-01 06:13:14 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 2.3162
2022-03-01 06:13:46 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 2.4373
2022-03-01 06:14:19 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 2.6000
2022-03-01 06:14:52 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 2.2667
2022-03-01 06:15:23 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 2.3565
2022-03-01 06:15:56 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 2.2044
2022-03-01 06:16:29 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 2.4512
2022-03-01 06:17:02 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 2.2914
2022-03-01 06:17:35 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 2.6238
2022-03-01 06:18:08 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 2.3766
2022-03-01 06:18:40 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 2.4251
2022-03-01 06:19:12 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 2.2501
2022-03-01 06:19:45 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 2.4479
2022-03-01 06:20:16 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 2.2943
2022-03-01 06:20:50 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 2.2298
2022-03-01 06:21:21 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 2.3127
2022-03-01 06:21:54 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 2.5104
2022-03-01 06:22:26 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 2.5088
2022-03-01 06:23:00 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 2.3060
2022-03-01 06:23:33 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 2.4020
2022-03-01 06:24:05 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 2.1202
2022-03-01 06:24:38 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 2.3254
2022-03-01 06:25:10 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 2.2572
2022-03-01 06:25:43 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 2.2969
2022-03-01 06:26:16 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 2.4791
2022-03-01 06:26:50 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 2.3761
2022-03-01 06:27:21 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 2.1214
2022-03-01 06:27:22 - train: epoch 097, train_loss: 2.3604
2022-03-01 06:28:36 - eval: epoch: 097, acc1: 54.790%, acc5: 77.712%, test_loss: 2.0336, per_image_load_time: 0.683ms, per_image_inference_time: 0.117ms
2022-03-01 06:28:37 - until epoch: 097, best_acc1: 54.790%
2022-03-01 06:28:37 - epoch 098 lr: 0.00010000000000000003
2022-03-01 06:29:14 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 2.6795
2022-03-01 06:29:46 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 2.5583
2022-03-01 06:30:20 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 2.3057
2022-03-01 06:30:52 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 2.3610
2022-03-01 06:31:25 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 2.4977
2022-03-01 06:31:58 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 2.4239
2022-03-01 06:32:32 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 2.2876
2022-03-01 06:33:03 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 2.4572
2022-03-01 06:33:37 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 2.2593
2022-03-01 06:34:09 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 2.2130
2022-03-01 06:34:42 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 2.1558
2022-03-01 06:35:15 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 2.4636
2022-03-01 06:35:48 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 2.3742
2022-03-01 06:36:22 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 2.5272
2022-03-01 06:36:54 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 2.2809
2022-03-01 06:37:27 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 2.3843
2022-03-01 06:38:01 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 2.4104
2022-03-01 06:38:34 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 2.2714
2022-03-01 06:39:07 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 2.3485
2022-03-01 06:39:39 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 2.3982
2022-03-01 06:40:12 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 2.5772
2022-03-01 06:40:45 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 2.4165
2022-03-01 06:41:17 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 2.2088
2022-03-01 06:41:51 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 2.3537
2022-03-01 06:42:24 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 2.4259
2022-03-01 06:42:57 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 2.3262
2022-03-01 06:43:30 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 2.3956
2022-03-01 06:44:03 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 2.4148
2022-03-01 06:44:36 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 2.3991
2022-03-01 06:45:10 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 2.2885
2022-03-01 06:45:41 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 2.4732
2022-03-01 06:46:14 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 2.1620
2022-03-01 06:46:46 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 2.3125
2022-03-01 06:47:20 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 2.6072
2022-03-01 06:47:51 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 2.2066
2022-03-01 06:48:25 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 2.6067
2022-03-01 06:48:57 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 2.3027
2022-03-01 06:49:31 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 2.1369
2022-03-01 06:50:03 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 2.4680
2022-03-01 06:50:37 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 2.5320
2022-03-01 06:51:09 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 2.5393
2022-03-01 06:51:43 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 2.4194
2022-03-01 06:52:16 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 2.2386
2022-03-01 06:52:49 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 2.5742
2022-03-01 06:53:21 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 2.2680
2022-03-01 06:53:56 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 2.3442
2022-03-01 06:54:28 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 2.1587
2022-03-01 06:55:01 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 2.3234
2022-03-01 06:55:35 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 2.3256
2022-03-01 06:56:08 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 2.3877
2022-03-01 06:56:09 - train: epoch 098, train_loss: 2.3596
2022-03-01 06:57:23 - eval: epoch: 098, acc1: 54.754%, acc5: 77.744%, test_loss: 2.0331, per_image_load_time: 1.077ms, per_image_inference_time: 0.133ms
2022-03-01 06:57:23 - until epoch: 098, best_acc1: 54.790%
2022-03-01 06:57:23 - epoch 099 lr: 0.00010000000000000003
2022-03-01 06:58:01 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 2.4210
2022-03-01 06:58:34 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 2.4110
2022-03-01 06:59:07 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 2.2708
2022-03-01 06:59:39 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 2.3326
2022-03-01 07:00:14 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 2.5136
2022-03-01 07:00:45 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 2.3718
2022-03-01 07:01:19 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 2.4596
2022-03-01 07:01:51 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 2.2868
2022-03-01 07:02:24 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 2.3858
2022-03-01 07:02:56 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 2.4059
2022-03-01 07:03:29 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 2.3383
2022-03-01 07:04:03 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 2.2761
2022-03-01 07:04:36 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 2.3499
2022-03-01 07:05:08 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 2.3637
2022-03-01 07:05:42 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 2.5127
2022-03-01 07:06:13 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 2.2213
2022-03-01 07:06:47 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 2.0966
2022-03-01 07:07:18 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 2.3601
2022-03-01 07:07:52 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 2.4914
2022-03-01 07:08:25 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 2.1350
2022-03-01 07:08:57 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 2.1960
2022-03-01 07:09:31 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 2.1118
2022-03-01 07:10:03 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 2.2130
2022-03-01 07:10:37 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 2.6933
2022-03-01 07:11:09 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 2.3027
2022-03-01 07:11:42 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 2.3638
2022-03-01 07:12:15 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 2.4179
2022-03-01 07:12:48 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 2.3903
2022-03-01 07:13:21 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 2.3159
2022-03-01 07:13:54 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 2.5683
2022-03-01 07:14:27 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 2.2560
2022-03-01 07:15:00 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 2.5794
2022-03-01 07:15:32 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 2.4218
2022-03-01 07:16:06 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 2.3747
2022-03-01 07:16:38 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 2.3967
2022-03-01 07:17:11 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 2.2434
2022-03-01 07:17:44 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 2.0834
2022-03-01 07:18:17 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 2.4221
2022-03-01 07:18:51 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 2.3172
2022-03-01 07:19:23 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 2.3594
2022-03-01 07:19:56 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 2.1988
2022-03-01 07:20:29 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 2.4995
2022-03-01 07:21:02 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 2.2828
2022-03-01 07:21:36 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 2.3472
2022-03-01 07:22:08 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 2.4281
2022-03-01 07:22:42 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 2.5153
2022-03-01 07:23:15 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 2.2282
2022-03-01 07:23:49 - train: epoch 0099, iter [04800, 05004], lr: 0.000100, loss: 2.4531
2022-03-01 07:24:21 - train: epoch 0099, iter [04900, 05004], lr: 0.000100, loss: 2.4017
2022-03-01 07:24:53 - train: epoch 0099, iter [05000, 05004], lr: 0.000100, loss: 2.5109
2022-03-01 07:24:54 - train: epoch 099, train_loss: 2.3609
2022-03-01 07:26:08 - eval: epoch: 099, acc1: 54.694%, acc5: 77.726%, test_loss: 2.0348, per_image_load_time: 2.155ms, per_image_inference_time: 0.137ms
2022-03-01 07:26:08 - until epoch: 099, best_acc1: 54.790%
2022-03-01 07:26:08 - epoch 100 lr: 0.00010000000000000003
2022-03-01 07:26:46 - train: epoch 0100, iter [00100, 05004], lr: 0.000100, loss: 2.5014
2022-03-01 07:27:19 - train: epoch 0100, iter [00200, 05004], lr: 0.000100, loss: 2.4506
2022-03-01 07:27:52 - train: epoch 0100, iter [00300, 05004], lr: 0.000100, loss: 2.3479
2022-03-01 07:28:25 - train: epoch 0100, iter [00400, 05004], lr: 0.000100, loss: 2.1416
2022-03-01 07:28:58 - train: epoch 0100, iter [00500, 05004], lr: 0.000100, loss: 2.3141
2022-03-01 07:29:30 - train: epoch 0100, iter [00600, 05004], lr: 0.000100, loss: 2.4562
2022-03-01 07:30:03 - train: epoch 0100, iter [00700, 05004], lr: 0.000100, loss: 2.3112
2022-03-01 07:30:35 - train: epoch 0100, iter [00800, 05004], lr: 0.000100, loss: 2.3724
2022-03-01 07:31:08 - train: epoch 0100, iter [00900, 05004], lr: 0.000100, loss: 2.1295
2022-03-01 07:31:39 - train: epoch 0100, iter [01000, 05004], lr: 0.000100, loss: 2.2397
2022-03-01 07:32:13 - train: epoch 0100, iter [01100, 05004], lr: 0.000100, loss: 2.2711
2022-03-01 07:32:45 - train: epoch 0100, iter [01200, 05004], lr: 0.000100, loss: 2.3281
2022-03-01 07:33:18 - train: epoch 0100, iter [01300, 05004], lr: 0.000100, loss: 2.4525
2022-03-01 07:33:50 - train: epoch 0100, iter [01400, 05004], lr: 0.000100, loss: 2.5449
2022-03-01 07:34:25 - train: epoch 0100, iter [01500, 05004], lr: 0.000100, loss: 2.3902
2022-03-01 07:34:57 - train: epoch 0100, iter [01600, 05004], lr: 0.000100, loss: 2.1845
2022-03-01 07:35:31 - train: epoch 0100, iter [01700, 05004], lr: 0.000100, loss: 2.1801
2022-03-01 07:36:03 - train: epoch 0100, iter [01800, 05004], lr: 0.000100, loss: 2.4283
2022-03-01 07:36:37 - train: epoch 0100, iter [01900, 05004], lr: 0.000100, loss: 2.3355
2022-03-01 07:37:09 - train: epoch 0100, iter [02000, 05004], lr: 0.000100, loss: 2.4973
2022-03-01 07:37:41 - train: epoch 0100, iter [02100, 05004], lr: 0.000100, loss: 2.1648
2022-03-01 07:38:14 - train: epoch 0100, iter [02200, 05004], lr: 0.000100, loss: 2.5011
2022-03-01 07:38:47 - train: epoch 0100, iter [02300, 05004], lr: 0.000100, loss: 2.1933
2022-03-01 07:39:19 - train: epoch 0100, iter [02400, 05004], lr: 0.000100, loss: 2.4109
2022-03-01 07:39:51 - train: epoch 0100, iter [02500, 05004], lr: 0.000100, loss: 2.5022
2022-03-01 07:40:25 - train: epoch 0100, iter [02600, 05004], lr: 0.000100, loss: 2.2475
2022-03-01 07:40:59 - train: epoch 0100, iter [02700, 05004], lr: 0.000100, loss: 2.2770
2022-03-01 07:41:31 - train: epoch 0100, iter [02800, 05004], lr: 0.000100, loss: 2.3860
2022-03-01 07:42:05 - train: epoch 0100, iter [02900, 05004], lr: 0.000100, loss: 2.5502
2022-03-01 07:42:37 - train: epoch 0100, iter [03000, 05004], lr: 0.000100, loss: 2.1965
2022-03-01 07:43:11 - train: epoch 0100, iter [03100, 05004], lr: 0.000100, loss: 2.4385
2022-03-01 07:43:44 - train: epoch 0100, iter [03200, 05004], lr: 0.000100, loss: 2.6392
2022-03-01 07:44:17 - train: epoch 0100, iter [03300, 05004], lr: 0.000100, loss: 2.4632
2022-03-01 07:44:50 - train: epoch 0100, iter [03400, 05004], lr: 0.000100, loss: 2.3851
2022-03-01 07:45:23 - train: epoch 0100, iter [03500, 05004], lr: 0.000100, loss: 2.1218
2022-03-01 07:45:57 - train: epoch 0100, iter [03600, 05004], lr: 0.000100, loss: 2.4896
2022-03-01 07:46:29 - train: epoch 0100, iter [03700, 05004], lr: 0.000100, loss: 2.3421
2022-03-01 07:47:03 - train: epoch 0100, iter [03800, 05004], lr: 0.000100, loss: 2.2525
2022-03-01 07:47:35 - train: epoch 0100, iter [03900, 05004], lr: 0.000100, loss: 2.4417
2022-03-01 07:48:09 - train: epoch 0100, iter [04000, 05004], lr: 0.000100, loss: 2.4549
2022-03-01 07:48:43 - train: epoch 0100, iter [04100, 05004], lr: 0.000100, loss: 2.4223
2022-03-01 07:49:15 - train: epoch 0100, iter [04200, 05004], lr: 0.000100, loss: 2.4130
2022-03-01 07:49:50 - train: epoch 0100, iter [04300, 05004], lr: 0.000100, loss: 2.2046
2022-03-01 07:50:22 - train: epoch 0100, iter [04400, 05004], lr: 0.000100, loss: 2.5367
2022-03-01 07:50:55 - train: epoch 0100, iter [04500, 05004], lr: 0.000100, loss: 2.1563
2022-03-01 07:51:29 - train: epoch 0100, iter [04600, 05004], lr: 0.000100, loss: 2.3320
2022-03-01 07:52:03 - train: epoch 0100, iter [04700, 05004], lr: 0.000100, loss: 2.3972
2022-03-01 07:52:36 - train: epoch 0100, iter [04800, 05004], lr: 0.000100, loss: 2.3261
2022-03-01 07:53:09 - train: epoch 0100, iter [04900, 05004], lr: 0.000100, loss: 2.3243
2022-03-01 07:53:42 - train: epoch 0100, iter [05000, 05004], lr: 0.000100, loss: 2.4114
2022-03-01 07:53:43 - train: epoch 100, train_loss: 2.3576
2022-03-01 07:54:57 - eval: epoch: 100, acc1: 54.724%, acc5: 77.714%, test_loss: 2.0345, per_image_load_time: 1.230ms, per_image_inference_time: 0.139ms
2022-03-01 07:54:57 - until epoch: 100, best_acc1: 54.790%
2022-03-01 07:54:57 - train done. model: darknettiny, train time: 48.251 hours, best_acc1: 54.790%
