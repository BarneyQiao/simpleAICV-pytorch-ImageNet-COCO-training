2022-02-26 07:57:15 - network: darknettiny
2022-02-26 07:57:15 - num_classes: 1000
2022-02-26 07:57:15 - input_image_size: 256
2022-02-26 07:57:15 - scale: 1.1428571428571428
2022-02-26 07:57:15 - trained_model_path: 
2022-02-26 07:57:15 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-26 07:57:15 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f97577dffa0>
2022-02-26 07:57:15 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f973f1d42b0>
2022-02-26 07:57:15 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f973f1d42e0>
2022-02-26 07:57:15 - seed: 0
2022-02-26 07:57:15 - batch_size: 256
2022-02-26 07:57:15 - num_workers: 16
2022-02-26 07:57:15 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-26 07:57:15 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-26 07:57:15 - epochs: 100
2022-02-26 07:57:15 - print_interval: 100
2022-02-26 07:57:15 - distributed: True
2022-02-26 07:57:15 - sync_bn: False
2022-02-26 07:57:15 - apex: True
2022-02-26 07:57:15 - gpus_type: NVIDIA GeForce RTX 3090
2022-02-26 07:57:15 - gpus_num: 2
2022-02-26 07:57:15 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f973e119d70>
2022-02-26 07:57:19 - --------------------parameters--------------------
2022-02-26 07:57:19 - name: conv1.layer.0.weight, grad: True
2022-02-26 07:57:19 - name: conv1.layer.1.weight, grad: True
2022-02-26 07:57:19 - name: conv1.layer.1.bias, grad: True
2022-02-26 07:57:19 - name: conv2.layer.0.weight, grad: True
2022-02-26 07:57:19 - name: conv2.layer.1.weight, grad: True
2022-02-26 07:57:19 - name: conv2.layer.1.bias, grad: True
2022-02-26 07:57:19 - name: conv3.layer.0.weight, grad: True
2022-02-26 07:57:19 - name: conv3.layer.1.weight, grad: True
2022-02-26 07:57:19 - name: conv3.layer.1.bias, grad: True
2022-02-26 07:57:19 - name: conv4.layer.0.weight, grad: True
2022-02-26 07:57:19 - name: conv4.layer.1.weight, grad: True
2022-02-26 07:57:19 - name: conv4.layer.1.bias, grad: True
2022-02-26 07:57:19 - name: conv5.layer.0.weight, grad: True
2022-02-26 07:57:19 - name: conv5.layer.1.weight, grad: True
2022-02-26 07:57:19 - name: conv5.layer.1.bias, grad: True
2022-02-26 07:57:19 - name: conv6.layer.0.weight, grad: True
2022-02-26 07:57:19 - name: conv6.layer.1.weight, grad: True
2022-02-26 07:57:19 - name: conv6.layer.1.bias, grad: True
2022-02-26 07:57:19 - name: fc.weight, grad: True
2022-02-26 07:57:19 - name: fc.bias, grad: True
2022-02-26 07:57:19 - --------------------buffers--------------------
2022-02-26 07:57:19 - name: conv1.layer.1.running_mean, grad: False
2022-02-26 07:57:19 - name: conv1.layer.1.running_var, grad: False
2022-02-26 07:57:19 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-02-26 07:57:19 - name: conv2.layer.1.running_mean, grad: False
2022-02-26 07:57:19 - name: conv2.layer.1.running_var, grad: False
2022-02-26 07:57:19 - name: conv2.layer.1.num_batches_tracked, grad: False
2022-02-26 07:57:19 - name: conv3.layer.1.running_mean, grad: False
2022-02-26 07:57:19 - name: conv3.layer.1.running_var, grad: False
2022-02-26 07:57:19 - name: conv3.layer.1.num_batches_tracked, grad: False
2022-02-26 07:57:19 - name: conv4.layer.1.running_mean, grad: False
2022-02-26 07:57:19 - name: conv4.layer.1.running_var, grad: False
2022-02-26 07:57:19 - name: conv4.layer.1.num_batches_tracked, grad: False
2022-02-26 07:57:19 - name: conv5.layer.1.running_mean, grad: False
2022-02-26 07:57:19 - name: conv5.layer.1.running_var, grad: False
2022-02-26 07:57:19 - name: conv5.layer.1.num_batches_tracked, grad: False
2022-02-26 07:57:19 - name: conv6.layer.1.running_mean, grad: False
2022-02-26 07:57:19 - name: conv6.layer.1.running_var, grad: False
2022-02-26 07:57:19 - name: conv6.layer.1.num_batches_tracked, grad: False
2022-02-26 07:57:19 - epoch 001 lr: 0.1
2022-02-26 07:57:58 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.7098
2022-02-26 07:58:31 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.4621
2022-02-26 07:59:05 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.3612
2022-02-26 07:59:38 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.1237
2022-02-26 08:00:12 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 5.9729
2022-02-26 08:00:45 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 5.7055
2022-02-26 08:01:19 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 5.7513
2022-02-26 08:01:52 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 5.7244
2022-02-26 08:02:26 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.6960
2022-02-26 08:03:00 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.4780
2022-02-26 08:03:33 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.5394
2022-02-26 08:04:06 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.4302
2022-02-26 08:04:40 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.2743
2022-02-26 08:05:13 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.3182
2022-02-26 08:05:47 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.1792
2022-02-26 08:06:20 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.2318
2022-02-26 08:06:55 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 4.9891
2022-02-26 08:07:28 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.0940
2022-02-26 08:08:01 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 4.9055
2022-02-26 08:08:34 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 4.9263
2022-02-26 08:09:08 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 4.8530
2022-02-26 08:09:41 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 4.8496
2022-02-26 08:10:15 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.6919
2022-02-26 08:10:48 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.7135
2022-02-26 08:11:22 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.8116
2022-02-26 08:11:55 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 4.8834
2022-02-26 08:12:29 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 4.7819
2022-02-26 08:13:02 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.5945
2022-02-26 08:13:36 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.4537
2022-02-26 08:14:09 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.6280
2022-02-26 08:14:43 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.6890
2022-02-26 08:15:16 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.4855
2022-02-26 08:15:49 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.4486
2022-02-26 08:16:23 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.4644
2022-02-26 08:16:56 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.4700
2022-02-26 08:17:30 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.4042
2022-02-26 08:18:03 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.6119
2022-02-26 08:18:37 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.2290
2022-02-26 08:19:10 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.4068
2022-02-26 08:19:44 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.4110
2022-02-26 08:20:17 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.5518
2022-02-26 08:20:52 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.2786
2022-02-26 08:21:25 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.2002
2022-02-26 08:21:59 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.0742
2022-02-26 08:22:32 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.2827
2022-02-26 08:23:07 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.5342
2022-02-26 08:23:40 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.2507
2022-02-26 08:24:15 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.5697
2022-02-26 08:24:48 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.1717
2022-02-26 08:25:21 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.0751
2022-02-26 08:25:22 - train: epoch 001, train_loss: 4.9190
2022-02-26 08:26:36 - eval: epoch: 001, acc1: 20.684%, acc5: 42.974%, test_loss: 3.9969, per_image_load_time: 2.761ms, per_image_inference_time: 0.119ms
2022-02-26 08:26:36 - until epoch: 001, best_acc1: 20.684%
2022-02-26 08:26:36 - epoch 002 lr: 0.1
2022-02-26 08:27:15 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.2154
2022-02-26 08:27:48 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 3.9039
2022-02-26 08:28:21 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.3229
2022-02-26 08:28:55 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.2276
2022-02-26 08:29:29 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.9292
2022-02-26 08:30:02 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 3.9892
2022-02-26 08:30:36 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.2103
2022-02-26 08:31:09 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.9082
2022-02-26 08:31:43 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.7711
2022-02-26 08:32:17 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.2281
2022-02-26 08:32:50 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.1059
2022-02-26 08:33:23 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.0027
2022-02-26 08:33:57 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.9080
2022-02-26 08:34:31 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.1159
2022-02-26 08:35:05 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.1092
2022-02-26 08:35:37 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.9012
2022-02-26 08:36:11 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.0457
2022-02-26 08:36:44 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.0380
2022-02-26 08:37:18 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.8085
2022-02-26 08:37:51 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.7700
2022-02-26 08:38:25 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.9472
2022-02-26 08:38:59 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.7126
2022-02-26 08:39:33 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.9046
2022-02-26 08:40:06 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.6879
2022-02-26 08:40:41 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.8072
2022-02-26 08:41:13 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.7326
2022-02-26 08:41:47 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 4.0432
2022-02-26 08:42:20 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.8830
2022-02-26 08:42:54 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.8610
2022-02-26 08:43:27 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.7419
2022-02-26 08:44:01 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.6988
2022-02-26 08:44:34 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.8107
2022-02-26 08:45:08 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.8412
2022-02-26 08:45:41 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.9286
2022-02-26 08:46:15 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.7091
2022-02-26 08:46:48 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.8048
2022-02-26 08:47:22 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.8503
2022-02-26 08:47:56 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.6050
2022-02-26 08:48:30 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.7501
2022-02-26 08:49:03 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.6499
2022-02-26 08:49:38 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.8030
2022-02-26 08:50:10 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.5545
2022-02-26 08:50:45 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.5967
2022-02-26 08:51:19 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.7502
2022-02-26 08:51:53 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.5201
2022-02-26 08:52:26 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.6749
2022-02-26 08:53:01 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.7296
2022-02-26 08:53:35 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.7266
2022-02-26 08:54:09 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.6303
2022-02-26 08:54:43 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.6810
2022-02-26 08:54:44 - train: epoch 002, train_loss: 3.8785
2022-02-26 08:55:59 - eval: epoch: 002, acc1: 27.360%, acc5: 51.022%, test_loss: 3.6127, per_image_load_time: 1.786ms, per_image_inference_time: 0.112ms
2022-02-26 08:55:59 - until epoch: 002, best_acc1: 27.360%
2022-02-26 08:55:59 - epoch 003 lr: 0.1
2022-02-26 08:56:37 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.6759
2022-02-26 08:57:11 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.6890
2022-02-26 08:57:44 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.6342
2022-02-26 08:58:18 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.5594
2022-02-26 08:58:50 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.7622
2022-02-26 08:59:24 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.6564
2022-02-26 08:59:57 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.8202
2022-02-26 09:00:31 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.7805
2022-02-26 09:01:04 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.5650
2022-02-26 09:01:38 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.7107
2022-02-26 09:02:11 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.4575
2022-02-26 09:02:45 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.6832
2022-02-26 09:03:18 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.5894
2022-02-26 09:03:51 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.4780
2022-02-26 09:04:25 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.9075
2022-02-26 09:04:58 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.5927
2022-02-26 09:05:31 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.4758
2022-02-26 09:06:05 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.5388
2022-02-26 09:06:39 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.6412
2022-02-26 09:07:12 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.8271
2022-02-26 09:07:46 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.7037
2022-02-26 09:08:21 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.9486
2022-02-26 09:08:53 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.5440
2022-02-26 09:09:27 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.4982
2022-02-26 09:09:59 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.7480
2022-02-26 09:10:33 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.7360
2022-02-26 09:11:07 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.8581
2022-02-26 09:11:41 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.4196
2022-02-26 09:12:14 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.5578
2022-02-26 09:12:49 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.6942
2022-02-26 09:13:22 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.8316
2022-02-26 09:13:56 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.4528
2022-02-26 09:14:28 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.5721
2022-02-26 09:15:02 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.6902
2022-02-26 09:15:36 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.2754
2022-02-26 09:16:09 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.4932
2022-02-26 09:16:43 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.6150
2022-02-26 09:17:17 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.6518
2022-02-26 09:17:51 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.8500
2022-02-26 09:18:24 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.4984
2022-02-26 09:18:57 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.5694
2022-02-26 09:19:32 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.5230
2022-02-26 09:20:06 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 3.3212
2022-02-26 09:20:41 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.4289
2022-02-26 09:21:14 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.5779
2022-02-26 09:21:49 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.5017
2022-02-26 09:22:22 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.3699
2022-02-26 09:22:57 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.6357
2022-02-26 09:23:31 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.6338
2022-02-26 09:24:05 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.5426
2022-02-26 09:24:05 - train: epoch 003, train_loss: 3.5983
2022-02-26 09:25:22 - eval: epoch: 003, acc1: 31.602%, acc5: 56.032%, test_loss: 3.3229, per_image_load_time: 1.219ms, per_image_inference_time: 0.107ms
2022-02-26 09:25:22 - until epoch: 003, best_acc1: 31.602%
2022-02-26 09:25:22 - epoch 004 lr: 0.1
2022-02-26 09:26:01 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.5965
2022-02-26 09:26:34 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.4376
2022-02-26 09:27:07 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.4391
2022-02-26 09:27:39 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 3.3857
2022-02-26 09:28:14 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.4448
2022-02-26 09:28:46 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.7315
2022-02-26 09:29:20 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.5646
2022-02-26 09:29:52 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 3.3872
2022-02-26 09:30:25 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 3.1824
2022-02-26 09:30:57 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.5596
2022-02-26 09:31:33 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.6322
2022-02-26 09:32:06 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 3.4157
2022-02-26 09:32:40 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 3.3193
2022-02-26 09:33:12 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.5620
2022-02-26 09:33:47 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.5788
2022-02-26 09:34:19 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 3.3797
2022-02-26 09:34:52 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.5472
2022-02-26 09:35:25 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.6603
2022-02-26 09:36:00 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.6199
2022-02-26 09:36:33 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.4224
2022-02-26 09:37:07 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.6054
2022-02-26 09:37:39 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.3871
2022-02-26 09:38:13 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 3.2740
2022-02-26 09:38:46 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 3.2018
2022-02-26 09:39:20 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 3.3333
2022-02-26 09:39:54 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.5006
2022-02-26 09:40:25 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 3.3618
2022-02-26 09:41:01 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.4724
2022-02-26 09:41:33 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 3.4733
2022-02-26 09:42:06 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 3.5078
2022-02-26 09:42:40 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.5188
2022-02-26 09:43:12 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 3.4928
2022-02-26 09:43:46 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.6134
2022-02-26 09:44:18 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 3.4440
2022-02-26 09:44:52 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 3.3173
2022-02-26 09:45:25 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 3.1961
2022-02-26 09:45:58 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.3733
2022-02-26 09:46:33 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 3.2405
2022-02-26 09:47:05 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 3.2868
2022-02-26 09:47:38 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 3.1236
2022-02-26 09:48:12 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 3.5372
2022-02-26 09:48:44 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 3.4012
2022-02-26 09:49:17 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 3.3209
2022-02-26 09:49:50 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 3.2428
2022-02-26 09:50:24 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.9492
2022-02-26 09:50:56 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 3.4690
2022-02-26 09:51:30 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 3.3983
2022-02-26 09:52:03 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 3.2913
2022-02-26 09:52:36 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 3.4296
2022-02-26 09:53:08 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 3.5127
2022-02-26 09:53:09 - train: epoch 004, train_loss: 3.4653
2022-02-26 09:54:23 - eval: epoch: 004, acc1: 32.548%, acc5: 56.784%, test_loss: 3.2933, per_image_load_time: 2.782ms, per_image_inference_time: 0.109ms
2022-02-26 09:54:23 - until epoch: 004, best_acc1: 32.548%
2022-02-26 09:54:23 - epoch 005 lr: 0.1
2022-02-26 09:55:01 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.4462
2022-02-26 09:55:34 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.3134
2022-02-26 09:56:08 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.6102
2022-02-26 09:56:40 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.4379
2022-02-26 09:57:14 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 3.2231
2022-02-26 09:57:46 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 3.3254
2022-02-26 09:58:20 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 3.5213
2022-02-26 09:58:53 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.5410
2022-02-26 09:59:26 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 3.3279
2022-02-26 09:59:59 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.4319
2022-02-26 10:00:33 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.5247
2022-02-26 10:01:06 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 3.4080
2022-02-26 10:01:39 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 3.3392
2022-02-26 10:02:11 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.3703
2022-02-26 10:02:46 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 3.2314
2022-02-26 10:03:18 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 3.1319
2022-02-26 10:03:51 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 3.3179
2022-02-26 10:04:25 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 3.5568
2022-02-26 10:04:58 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 3.1531
2022-02-26 10:05:31 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 3.2862
2022-02-26 10:06:04 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 3.1651
2022-02-26 10:06:38 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 3.4416
2022-02-26 10:07:11 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 3.2403
2022-02-26 10:07:44 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 3.3108
2022-02-26 10:08:18 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 3.3328
2022-02-26 10:08:50 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.5183
2022-02-26 10:09:24 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.5819
2022-02-26 10:09:57 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 3.4017
2022-02-26 10:10:31 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 3.2192
2022-02-26 10:11:05 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 3.2679
2022-02-26 10:11:37 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.3532
2022-02-26 10:12:11 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 3.4877
2022-02-26 10:12:44 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 3.2247
2022-02-26 10:13:18 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 3.2548
2022-02-26 10:13:51 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 3.4293
2022-02-26 10:14:24 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 3.4337
2022-02-26 10:14:57 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 3.3064
2022-02-26 10:15:31 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 3.3813
2022-02-26 10:16:04 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.6698
2022-02-26 10:16:38 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 3.3965
2022-02-26 10:17:10 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 3.3718
2022-02-26 10:17:44 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 3.3421
2022-02-26 10:18:17 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 3.3475
2022-02-26 10:18:51 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 3.4225
2022-02-26 10:19:24 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 3.5327
2022-02-26 10:19:58 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 3.3469
2022-02-26 10:20:31 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 3.1440
2022-02-26 10:21:05 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 3.1888
2022-02-26 10:21:38 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 3.4776
2022-02-26 10:22:10 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 3.4010
2022-02-26 10:22:11 - train: epoch 005, train_loss: 3.3869
2022-02-26 10:23:29 - eval: epoch: 005, acc1: 32.310%, acc5: 57.384%, test_loss: 3.2620, per_image_load_time: 2.907ms, per_image_inference_time: 0.106ms
2022-02-26 10:23:29 - until epoch: 005, best_acc1: 32.548%
2022-02-26 10:23:29 - epoch 006 lr: 0.1
2022-02-26 10:24:08 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 3.3152
2022-02-26 10:24:41 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 3.3944
2022-02-26 10:25:13 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 3.2357
2022-02-26 10:25:45 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.3964
2022-02-26 10:26:18 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 3.2620
2022-02-26 10:26:50 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 3.4125
2022-02-26 10:27:23 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.3235
2022-02-26 10:27:54 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 3.2633
2022-02-26 10:28:28 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 3.2978
2022-02-26 10:28:59 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 3.3330
2022-02-26 10:29:31 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 3.2262
2022-02-26 10:30:04 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 3.3934
2022-02-26 10:30:37 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.5036
2022-02-26 10:31:08 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 3.3768
2022-02-26 10:31:41 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 3.4243
2022-02-26 10:32:12 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 3.1237
2022-02-26 10:32:46 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 3.4240
2022-02-26 10:33:18 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 3.4383
2022-02-26 10:33:51 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 3.2910
2022-02-26 10:34:24 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 3.5286
2022-02-26 10:34:55 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 3.2821
2022-02-26 10:35:29 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 3.1386
2022-02-26 10:36:00 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 3.1768
2022-02-26 10:36:34 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 3.3667
2022-02-26 10:37:05 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 3.5812
2022-02-26 10:37:39 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 3.2890
2022-02-26 10:38:10 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 3.4068
2022-02-26 10:38:43 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 3.0783
2022-02-26 10:39:15 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 3.4078
2022-02-26 10:39:48 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 3.1511
2022-02-26 10:40:20 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 3.0226
2022-02-26 10:40:53 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 3.2524
2022-02-26 10:41:24 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 3.1196
2022-02-26 10:41:58 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 3.4452
2022-02-26 10:42:29 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 3.4049
2022-02-26 10:43:02 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 3.5051
2022-02-26 10:43:34 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 3.3608
2022-02-26 10:44:08 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 3.2042
2022-02-26 10:44:39 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 3.1494
2022-02-26 10:45:13 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.6903
2022-02-26 10:45:44 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 3.3119
2022-02-26 10:46:18 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 3.2601
2022-02-26 10:46:49 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 3.1196
2022-02-26 10:47:23 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 3.3153
2022-02-26 10:47:55 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 3.3305
2022-02-26 10:48:28 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 3.3720
2022-02-26 10:49:00 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 3.4115
2022-02-26 10:49:34 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 3.4105
2022-02-26 10:50:06 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 3.4465
2022-02-26 10:50:39 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 3.2494
2022-02-26 10:50:39 - train: epoch 006, train_loss: 3.3410
2022-02-26 10:51:52 - eval: epoch: 006, acc1: 34.020%, acc5: 58.730%, test_loss: 3.1811, per_image_load_time: 2.479ms, per_image_inference_time: 0.124ms
2022-02-26 10:51:52 - until epoch: 006, best_acc1: 34.020%
2022-02-26 10:51:52 - epoch 007 lr: 0.1
2022-02-26 10:52:29 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 3.1565
2022-02-26 10:53:02 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 3.5460
2022-02-26 10:53:35 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 3.6818
2022-02-26 10:54:07 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 3.2560
2022-02-26 10:54:39 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 3.2072
2022-02-26 10:55:11 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 3.4301
2022-02-26 10:55:44 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 3.3272
2022-02-26 10:56:16 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 3.4114
2022-02-26 10:56:49 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 3.4017
2022-02-26 10:57:21 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 3.3013
2022-02-26 10:57:53 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 3.2079
2022-02-26 10:58:26 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 3.2547
2022-02-26 10:58:58 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 3.2007
2022-02-26 10:59:31 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 3.3952
2022-02-26 11:00:03 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 3.3724
2022-02-26 11:00:37 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 3.3142
2022-02-26 11:01:09 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 3.3932
2022-02-26 11:01:42 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 3.1936
2022-02-26 11:02:14 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 3.4009
2022-02-26 11:02:47 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 3.1353
2022-02-26 11:03:20 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 3.4035
2022-02-26 11:03:52 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 3.1897
2022-02-26 11:04:25 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 3.2988
2022-02-26 11:04:57 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 3.3717
2022-02-26 11:05:30 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 3.1871
2022-02-26 11:06:02 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 3.1866
2022-02-26 11:06:36 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 3.2242
2022-02-26 11:07:07 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 3.2745
2022-02-26 11:07:41 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 3.3790
2022-02-26 11:08:12 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 3.4168
2022-02-26 11:08:46 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 3.1466
2022-02-26 11:09:18 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 3.1791
2022-02-26 11:09:51 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 3.4640
2022-02-26 11:10:23 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 3.2602
2022-02-26 11:10:57 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 3.3811
2022-02-26 11:11:31 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 3.0896
2022-02-26 11:12:03 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 3.4181
2022-02-26 11:12:37 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 3.5522
2022-02-26 11:13:10 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 3.2577
2022-02-26 11:13:44 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 3.4332
2022-02-26 11:14:17 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 3.2415
2022-02-26 11:14:51 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 3.1562
2022-02-26 11:15:22 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 3.3960
2022-02-26 11:15:56 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 3.1924
2022-02-26 11:16:27 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 3.5410
2022-02-26 11:17:01 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 3.3539
2022-02-26 11:17:33 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 3.5179
2022-02-26 11:18:07 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 3.6140
2022-02-26 11:18:39 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 3.3519
2022-02-26 11:19:11 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 3.3525
2022-02-26 11:19:12 - train: epoch 007, train_loss: 3.3061
2022-02-26 11:20:27 - eval: epoch: 007, acc1: 34.366%, acc5: 59.092%, test_loss: 3.1348, per_image_load_time: 2.798ms, per_image_inference_time: 0.107ms
2022-02-26 11:20:27 - until epoch: 007, best_acc1: 34.366%
2022-02-26 11:20:27 - epoch 008 lr: 0.1
2022-02-26 11:21:04 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 3.1562
2022-02-26 11:21:36 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 3.5090
2022-02-26 11:22:09 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 3.0378
2022-02-26 11:22:40 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.9931
2022-02-26 11:23:13 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 3.2211
2022-02-26 11:23:45 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 3.2310
2022-02-26 11:24:18 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 3.4493
2022-02-26 11:24:50 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 3.2017
2022-02-26 11:25:23 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 3.1686
2022-02-26 11:25:55 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 3.2728
2022-02-26 11:26:28 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 3.0734
2022-02-26 11:27:01 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 3.0144
2022-02-26 11:27:34 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 3.3263
2022-02-26 11:28:06 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 3.3228
2022-02-26 11:28:38 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 3.2932
2022-02-26 11:29:11 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 3.2925
2022-02-26 11:29:44 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 3.1996
2022-02-26 11:30:16 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 3.4690
2022-02-26 11:30:49 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 3.0706
2022-02-26 11:31:21 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 3.4435
2022-02-26 11:31:54 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 3.2518
2022-02-26 11:32:26 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 3.1784
2022-02-26 11:33:00 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 3.2996
2022-02-26 11:33:31 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 3.1514
2022-02-26 11:34:04 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 3.3229
2022-02-26 11:34:37 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 3.3199
2022-02-26 11:35:09 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 3.4587
2022-02-26 11:35:43 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 3.4061
2022-02-26 11:36:14 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 3.3400
2022-02-26 11:36:48 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 3.4025
2022-02-26 11:37:20 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 3.2642
2022-02-26 11:37:53 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 3.5912
2022-02-26 11:38:25 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 3.6116
2022-02-26 11:38:58 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 3.3398
2022-02-26 11:39:30 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 3.3356
2022-02-26 11:40:03 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 3.2791
2022-02-26 11:40:35 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 3.2322
2022-02-26 11:41:09 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 3.2244
2022-02-26 11:41:40 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 3.3447
2022-02-26 11:42:14 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 3.6196
2022-02-26 11:42:45 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 3.2778
2022-02-26 11:43:19 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 3.2866
2022-02-26 11:43:51 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 3.1227
2022-02-26 11:44:25 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 3.2904
2022-02-26 11:44:56 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 3.4802
2022-02-26 11:45:29 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 3.4703
2022-02-26 11:46:01 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 3.1446
2022-02-26 11:46:34 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 3.2610
2022-02-26 11:47:07 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 3.3775
2022-02-26 11:47:38 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 3.1203
2022-02-26 11:47:39 - train: epoch 008, train_loss: 3.2827
2022-02-26 11:48:52 - eval: epoch: 008, acc1: 36.958%, acc5: 62.198%, test_loss: 2.9753, per_image_load_time: 1.505ms, per_image_inference_time: 0.108ms
2022-02-26 11:48:52 - until epoch: 008, best_acc1: 36.958%
2022-02-26 11:48:52 - epoch 009 lr: 0.1
2022-02-26 11:49:30 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 3.0578
2022-02-26 11:50:02 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 3.0752
2022-02-26 11:50:34 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.9824
2022-02-26 11:51:06 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 3.4470
2022-02-26 11:51:39 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 3.2726
2022-02-26 11:52:11 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 3.2843
2022-02-26 11:52:44 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 3.2323
2022-02-26 11:53:16 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 3.0943
2022-02-26 11:53:49 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 3.1396
2022-02-26 11:54:21 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 3.2162
2022-02-26 11:54:54 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 3.5244
2022-02-26 11:55:26 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 3.3560
2022-02-26 11:55:58 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 3.3637
2022-02-26 11:56:30 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 3.0063
2022-02-26 11:57:04 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 3.0640
2022-02-26 11:57:36 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 3.3098
2022-02-26 11:58:08 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 3.4904
2022-02-26 11:58:41 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 3.0840
2022-02-26 11:59:14 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 3.0387
2022-02-26 11:59:47 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 3.0424
2022-02-26 12:00:20 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 3.5890
2022-02-26 12:00:51 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 3.4458
2022-02-26 12:01:25 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 3.0321
2022-02-26 12:01:59 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 3.2346
2022-02-26 12:02:33 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 3.0041
2022-02-26 12:03:07 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 3.3726
2022-02-26 12:03:40 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 3.0986
2022-02-26 12:04:13 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 3.1963
2022-02-26 12:04:45 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.9550
2022-02-26 12:05:19 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 3.2949
2022-02-26 12:05:51 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 3.3236
2022-02-26 12:06:25 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 3.3499
2022-02-26 12:06:56 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 3.3330
2022-02-26 12:07:30 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 3.4111
2022-02-26 12:08:02 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 3.4253
2022-02-26 12:08:35 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 3.1021
2022-02-26 12:09:06 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 3.3825
2022-02-26 12:09:39 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 3.5480
2022-02-26 12:10:13 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.9169
2022-02-26 12:10:45 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 3.4439
2022-02-26 12:11:18 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 3.3465
2022-02-26 12:11:50 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 3.1679
2022-02-26 12:12:23 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 3.4367
2022-02-26 12:12:56 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 3.3352
2022-02-26 12:13:29 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 3.2884
2022-02-26 12:14:01 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 3.3169
2022-02-26 12:14:35 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 3.3897
2022-02-26 12:15:06 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 3.4793
2022-02-26 12:15:40 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 3.3844
2022-02-26 12:16:10 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 3.1733
2022-02-26 12:16:11 - train: epoch 009, train_loss: 3.2636
2022-02-26 12:17:24 - eval: epoch: 009, acc1: 36.094%, acc5: 60.914%, test_loss: 3.0511, per_image_load_time: 1.286ms, per_image_inference_time: 0.123ms
2022-02-26 12:17:24 - until epoch: 009, best_acc1: 36.958%
2022-02-26 12:17:24 - epoch 010 lr: 0.1
2022-02-26 12:18:02 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 3.2204
2022-02-26 12:18:34 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 3.3820
2022-02-26 12:19:07 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 3.3152
2022-02-26 12:19:40 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 3.3156
2022-02-26 12:20:12 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 3.2135
2022-02-26 12:20:45 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 3.4230
2022-02-26 12:21:18 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 3.3707
2022-02-26 12:21:50 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 3.1831
2022-02-26 12:22:23 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 3.0736
2022-02-26 12:22:55 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 3.1272
2022-02-26 12:23:27 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 3.2781
2022-02-26 12:23:59 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 3.0442
2022-02-26 12:24:33 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 3.0982
2022-02-26 12:25:05 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 3.2841
2022-02-26 12:25:38 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.9646
2022-02-26 12:26:10 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 3.1730
2022-02-26 12:26:43 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 3.3659
2022-02-26 12:27:15 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 3.2063
2022-02-26 12:27:48 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 3.2238
2022-02-26 12:28:20 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 3.3024
2022-02-26 12:28:53 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 3.2458
2022-02-26 12:29:25 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 3.4048
2022-02-26 12:29:58 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 3.4815
2022-02-26 12:30:30 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 3.4043
2022-02-26 12:31:04 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 3.3155
2022-02-26 12:31:35 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 3.2732
2022-02-26 12:32:09 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 3.1586
2022-02-26 12:32:41 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 3.2812
2022-02-26 12:33:16 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 3.3956
2022-02-26 12:33:49 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 3.1849
2022-02-26 12:34:22 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 3.4410
2022-02-26 12:34:55 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 3.4301
2022-02-26 12:35:29 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 3.3274
2022-02-26 12:36:01 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 3.3817
2022-02-26 12:36:36 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 3.3826
2022-02-26 12:37:08 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 3.4441
2022-02-26 12:37:43 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 3.1305
2022-02-26 12:38:15 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 3.2704
2022-02-26 12:38:49 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 3.0633
2022-02-26 12:39:22 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 3.2089
2022-02-26 12:39:57 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 3.2217
2022-02-26 12:40:29 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 3.3110
2022-02-26 12:41:03 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 3.0936
2022-02-26 12:41:36 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 3.1475
2022-02-26 12:42:10 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 3.2189
2022-02-26 12:42:43 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 3.3461
2022-02-26 12:43:17 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 3.2455
2022-02-26 12:43:50 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 3.1652
2022-02-26 12:44:25 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 3.1506
2022-02-26 12:44:56 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 3.1219
2022-02-26 12:44:58 - train: epoch 010, train_loss: 3.2455
2022-02-26 12:46:13 - eval: epoch: 010, acc1: 36.232%, acc5: 61.474%, test_loss: 3.0269, per_image_load_time: 2.322ms, per_image_inference_time: 0.116ms
2022-02-26 12:46:13 - until epoch: 010, best_acc1: 36.958%
2022-02-26 12:46:13 - epoch 011 lr: 0.1
2022-02-26 12:46:52 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 3.1582
2022-02-26 12:47:25 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 3.3448
2022-02-26 12:47:59 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 3.0812
2022-02-26 12:48:32 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 3.3694
2022-02-26 12:49:06 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 3.1849
2022-02-26 12:49:38 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 3.1834
2022-02-26 12:50:12 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 3.1749
2022-02-26 12:50:45 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 3.1628
2022-02-26 12:51:19 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 3.3315
2022-02-26 12:51:53 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 3.2435
2022-02-26 12:52:28 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 3.3938
2022-02-26 12:53:01 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 3.5894
2022-02-26 12:53:35 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 3.3071
2022-02-26 12:54:08 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 3.1491
2022-02-26 12:54:43 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.9416
2022-02-26 12:55:15 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 3.2570
2022-02-26 12:55:49 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 3.3701
2022-02-26 12:56:24 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 3.1977
2022-02-26 12:56:57 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 3.0897
2022-02-26 12:57:31 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 3.3097
2022-02-26 12:58:04 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 3.2441
2022-02-26 12:58:39 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 3.2217
2022-02-26 12:59:11 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 3.4445
2022-02-26 12:59:46 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 3.1713
2022-02-26 13:00:18 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 3.4573
2022-02-26 13:00:53 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 3.2973
2022-02-26 13:01:26 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 3.1770
2022-02-26 13:02:00 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.9617
2022-02-26 13:02:33 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 3.1589
2022-02-26 13:03:07 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 3.4655
2022-02-26 13:03:41 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 3.2726
2022-02-26 13:04:15 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 3.0528
2022-02-26 13:04:47 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 3.4127
2022-02-26 13:05:22 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 3.2302
2022-02-26 13:05:55 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 3.2445
2022-02-26 13:06:29 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 3.3259
2022-02-26 13:07:02 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 3.5040
2022-02-26 13:07:36 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.9009
2022-02-26 13:08:09 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 3.3066
2022-02-26 13:08:42 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 3.1904
2022-02-26 13:09:16 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.9905
2022-02-26 13:09:51 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 3.1896
2022-02-26 13:10:24 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 3.3735
2022-02-26 13:10:57 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 3.2162
2022-02-26 13:11:31 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 3.0147
2022-02-26 13:12:05 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 3.1894
2022-02-26 13:12:38 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.9041
2022-02-26 13:13:13 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 3.0764
2022-02-26 13:13:46 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 3.1811
2022-02-26 13:14:20 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 3.1021
2022-02-26 13:14:20 - train: epoch 011, train_loss: 3.2339
2022-02-26 13:15:36 - eval: epoch: 011, acc1: 37.406%, acc5: 62.640%, test_loss: 2.9542, per_image_load_time: 2.841ms, per_image_inference_time: 0.111ms
2022-02-26 13:15:36 - until epoch: 011, best_acc1: 37.406%
2022-02-26 13:15:36 - epoch 012 lr: 0.1
2022-02-26 13:16:14 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 3.2525
2022-02-26 13:16:48 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 3.0289
2022-02-26 13:17:21 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 3.1349
2022-02-26 13:17:55 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 3.2230
2022-02-26 13:18:28 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 3.5567
2022-02-26 13:19:02 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 3.1337
2022-02-26 13:19:35 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 3.0248
2022-02-26 13:20:09 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 3.2471
2022-02-26 13:20:41 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 3.3570
2022-02-26 13:21:16 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.9461
2022-02-26 13:21:48 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 3.5678
2022-02-26 13:22:22 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 3.0150
2022-02-26 13:22:55 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 3.1586
2022-02-26 13:23:29 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 3.2008
2022-02-26 13:24:01 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.9937
2022-02-26 13:24:34 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 3.0946
2022-02-26 13:25:09 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 3.0757
2022-02-26 13:25:42 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 3.2101
2022-02-26 13:26:16 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 3.3863
2022-02-26 13:26:49 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 3.4176
2022-02-26 13:27:23 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 3.2155
2022-02-26 13:27:56 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 3.4180
2022-02-26 13:28:30 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 3.1113
2022-02-26 13:29:03 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 3.3007
2022-02-26 13:29:36 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 3.0186
2022-02-26 13:30:09 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 3.0891
2022-02-26 13:30:43 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 3.2197
2022-02-26 13:31:16 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 3.2515
2022-02-26 13:31:50 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.9186
2022-02-26 13:32:25 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 3.0674
2022-02-26 13:32:57 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 3.3918
2022-02-26 13:33:32 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.9297
2022-02-26 13:34:04 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 3.2506
2022-02-26 13:34:37 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 3.1837
2022-02-26 13:35:10 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 3.3178
2022-02-26 13:35:45 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 3.3464
2022-02-26 13:36:18 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 3.2585
2022-02-26 13:36:52 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 3.2763
2022-02-26 13:37:25 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 3.1411
2022-02-26 13:37:58 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 3.1888
2022-02-26 13:38:32 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 3.1706
2022-02-26 13:39:06 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 3.0854
2022-02-26 13:39:39 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 3.2330
2022-02-26 13:40:13 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 3.0818
2022-02-26 13:40:46 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 3.2156
2022-02-26 13:41:20 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 3.4447
2022-02-26 13:41:53 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 3.1091
2022-02-26 13:42:28 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 3.3268
2022-02-26 13:43:01 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 3.2209
2022-02-26 13:43:34 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.9143
2022-02-26 13:43:35 - train: epoch 012, train_loss: 3.2219
2022-02-26 13:44:51 - eval: epoch: 012, acc1: 35.528%, acc5: 60.906%, test_loss: 3.0765, per_image_load_time: 2.872ms, per_image_inference_time: 0.106ms
2022-02-26 13:44:51 - until epoch: 012, best_acc1: 37.406%
2022-02-26 13:44:51 - epoch 013 lr: 0.1
2022-02-26 13:45:30 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 3.0900
2022-02-26 13:46:03 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 3.2285
2022-02-26 13:46:37 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 3.0447
2022-02-26 13:47:08 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 3.1526
2022-02-26 13:47:41 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 3.1773
2022-02-26 13:48:14 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 3.3679
2022-02-26 13:48:48 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 3.0268
2022-02-26 13:49:21 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 3.3147
2022-02-26 13:49:54 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 3.2319
2022-02-26 13:50:28 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 3.2851
2022-02-26 13:51:01 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 3.1775
2022-02-26 13:51:34 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 3.3526
2022-02-26 13:52:08 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 3.3592
2022-02-26 13:52:41 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 3.1825
2022-02-26 13:53:14 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 3.2732
2022-02-26 13:53:47 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 3.0472
2022-02-26 13:54:20 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 3.2136
2022-02-26 13:54:55 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 3.1289
2022-02-26 13:55:28 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 3.1348
2022-02-26 13:56:02 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 3.4017
2022-02-26 13:56:35 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 3.5530
2022-02-26 13:57:09 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 3.1068
2022-02-26 13:57:41 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 3.1480
2022-02-26 13:58:15 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 3.2870
2022-02-26 13:58:49 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 3.1482
2022-02-26 13:59:22 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 3.0142
2022-02-26 13:59:55 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 3.1775
2022-02-26 14:00:29 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 3.3082
2022-02-26 14:01:02 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 3.1955
2022-02-26 14:01:35 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 3.2416
2022-02-26 14:02:09 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 3.0541
2022-02-26 14:02:43 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 3.2230
2022-02-26 14:03:15 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 3.1090
2022-02-26 14:03:49 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 3.0988
2022-02-26 14:04:22 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 3.0423
2022-02-26 14:04:56 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 3.3465
2022-02-26 14:05:29 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 3.0717
2022-02-26 14:06:01 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 3.3186
2022-02-26 14:06:35 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 3.2819
2022-02-26 14:07:09 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 3.1994
2022-02-26 14:07:43 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 3.1357
2022-02-26 14:08:17 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 3.1591
2022-02-26 14:08:49 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 3.2250
2022-02-26 14:09:24 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 3.1621
2022-02-26 14:09:56 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 3.2034
2022-02-26 14:10:31 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 3.1582
2022-02-26 14:11:04 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 3.2058
2022-02-26 14:11:39 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 3.2177
2022-02-26 14:12:11 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 3.2273
2022-02-26 14:12:45 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 3.2695
2022-02-26 14:12:45 - train: epoch 013, train_loss: 3.2114
2022-02-26 14:13:59 - eval: epoch: 013, acc1: 37.578%, acc5: 62.564%, test_loss: 2.9544, per_image_load_time: 1.246ms, per_image_inference_time: 0.119ms
2022-02-26 14:13:59 - until epoch: 013, best_acc1: 37.578%
2022-02-26 14:13:59 - epoch 014 lr: 0.1
2022-02-26 14:14:37 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 3.0994
2022-02-26 14:15:10 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 3.4135
2022-02-26 14:15:44 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.8977
2022-02-26 14:16:17 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 3.0483
2022-02-26 14:16:50 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 3.0960
2022-02-26 14:17:23 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 3.1127
2022-02-26 14:17:55 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 3.2265
2022-02-26 14:18:29 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 3.1843
2022-02-26 14:19:02 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 3.2743
2022-02-26 14:19:35 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 3.2145
2022-02-26 14:20:09 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 3.0621
2022-02-26 14:20:43 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 3.1363
2022-02-26 14:21:17 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 3.1003
2022-02-26 14:21:48 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 3.2424
2022-02-26 14:22:22 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 3.3723
2022-02-26 14:22:56 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 3.1415
2022-02-26 14:23:29 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 3.3792
2022-02-26 14:24:02 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 3.4454
2022-02-26 14:24:36 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 3.0103
2022-02-26 14:25:08 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 3.1362
2022-02-26 14:25:42 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 3.2686
2022-02-26 14:26:16 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 3.2779
2022-02-26 14:26:49 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 3.1624
2022-02-26 14:27:24 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 3.3570
2022-02-26 14:27:56 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 3.2361
2022-02-26 14:28:30 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 3.2494
2022-02-26 14:29:02 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 3.1561
2022-02-26 14:29:36 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 3.4252
2022-02-26 14:30:09 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.9959
2022-02-26 14:30:43 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 3.4532
2022-02-26 14:31:16 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 3.2096
2022-02-26 14:31:49 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 3.1155
2022-02-26 14:32:23 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 3.0346
2022-02-26 14:32:57 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 3.1045
2022-02-26 14:33:31 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 3.2431
2022-02-26 14:34:02 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 3.1595
2022-02-26 14:34:37 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 3.1268
2022-02-26 14:35:09 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 3.4811
2022-02-26 14:35:42 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 3.1899
2022-02-26 14:36:17 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 3.3898
2022-02-26 14:36:50 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 3.1716
2022-02-26 14:37:24 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 3.0300
2022-02-26 14:37:57 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 3.1666
2022-02-26 14:38:31 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 3.0944
2022-02-26 14:39:05 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 3.1555
2022-02-26 14:39:39 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 3.1404
2022-02-26 14:40:11 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 3.2717
2022-02-26 14:40:45 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 3.1505
2022-02-26 14:41:19 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 3.0459
2022-02-26 14:41:52 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 3.3388
2022-02-26 14:41:53 - train: epoch 014, train_loss: 3.2038
2022-02-26 14:43:07 - eval: epoch: 014, acc1: 37.120%, acc5: 62.378%, test_loss: 2.9736, per_image_load_time: 1.456ms, per_image_inference_time: 0.104ms
2022-02-26 14:43:08 - until epoch: 014, best_acc1: 37.578%
2022-02-26 14:43:08 - epoch 015 lr: 0.1
2022-02-26 14:43:46 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 3.0431
2022-02-26 14:44:20 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 3.3333
2022-02-26 14:44:52 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 3.3962
2022-02-26 14:45:26 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 3.1978
2022-02-26 14:45:59 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 3.2148
2022-02-26 14:46:32 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 3.1979
2022-02-26 14:47:05 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 3.1654
2022-02-26 14:47:38 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 3.0287
2022-02-26 14:48:12 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 3.0538
2022-02-26 14:48:44 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 3.1850
2022-02-26 14:49:18 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 3.1456
2022-02-26 14:49:51 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 3.2226
2022-02-26 14:50:24 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 3.3177
2022-02-26 14:50:57 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 3.1813
2022-02-26 14:51:31 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.9469
2022-02-26 14:52:03 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 3.0921
2022-02-26 14:52:38 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 3.5068
2022-02-26 14:53:10 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.9702
2022-02-26 14:53:44 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.9684
2022-02-26 14:54:17 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 3.0879
2022-02-26 14:54:51 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 3.0821
2022-02-26 14:55:24 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 3.3810
2022-02-26 14:55:57 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.9093
2022-02-26 14:56:30 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 3.1526
2022-02-26 14:57:04 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 3.3462
2022-02-26 14:57:36 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.9153
2022-02-26 14:58:11 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 3.1375
2022-02-26 14:58:43 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 3.3545
2022-02-26 14:59:17 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 3.2202
2022-02-26 14:59:50 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.9323
2022-02-26 15:00:24 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 3.0623
2022-02-26 15:00:56 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 3.1108
2022-02-26 15:01:30 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 3.0265
2022-02-26 15:02:03 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 3.1925
2022-02-26 15:02:37 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 3.3717
2022-02-26 15:03:10 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 3.1533
2022-02-26 15:03:44 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 3.2253
2022-02-26 15:04:16 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 3.1981
2022-02-26 15:04:51 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 3.4136
2022-02-26 15:05:23 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 3.2665
2022-02-26 15:05:57 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 3.2303
2022-02-26 15:06:29 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.9126
2022-02-26 15:07:03 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 3.1022
2022-02-26 15:07:35 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 3.0980
2022-02-26 15:08:10 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 3.2860
2022-02-26 15:08:42 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 3.1562
2022-02-26 15:09:16 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 3.3670
2022-02-26 15:09:48 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 3.1668
2022-02-26 15:10:22 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 3.2932
2022-02-26 15:10:54 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 3.3413
2022-02-26 15:10:55 - train: epoch 015, train_loss: 3.1979
2022-02-26 15:12:10 - eval: epoch: 015, acc1: 36.700%, acc5: 61.988%, test_loss: 3.0185, per_image_load_time: 2.806ms, per_image_inference_time: 0.102ms
2022-02-26 15:12:10 - until epoch: 015, best_acc1: 37.578%
2022-02-26 15:12:10 - epoch 016 lr: 0.1
2022-02-26 15:12:48 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 3.1089
2022-02-26 15:13:21 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 3.0025
2022-02-26 15:13:54 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 3.2183
2022-02-26 15:14:27 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 3.5151
2022-02-26 15:15:00 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.9798
2022-02-26 15:15:34 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 3.4527
2022-02-26 15:16:06 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 3.0029
2022-02-26 15:16:40 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 3.1724
2022-02-26 15:17:12 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 3.2764
2022-02-26 15:17:46 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.9494
2022-02-26 15:18:18 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 3.0639
2022-02-26 15:18:52 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 3.0732
2022-02-26 15:19:25 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 3.2555
2022-02-26 15:19:58 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.9936
2022-02-26 15:20:31 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 3.2483
2022-02-26 15:21:04 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 3.2940
2022-02-26 15:21:38 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 3.1051
2022-02-26 15:22:11 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 3.1706
2022-02-26 15:22:44 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 3.1613
2022-02-26 15:23:18 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.7952
2022-02-26 15:23:52 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 3.2440
2022-02-26 15:24:25 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 3.2336
2022-02-26 15:24:58 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 3.4204
2022-02-26 15:25:31 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 3.3293
2022-02-26 15:26:05 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.9850
2022-02-26 15:26:38 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 3.2456
2022-02-26 15:27:12 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 3.1600
2022-02-26 15:27:45 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.9345
2022-02-26 15:28:18 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 3.2062
2022-02-26 15:28:51 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 3.4579
2022-02-26 15:29:24 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 3.2658
2022-02-26 15:29:58 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 3.2149
2022-02-26 15:30:31 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 3.3099
2022-02-26 15:31:04 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 3.2075
2022-02-26 15:31:36 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 3.0696
2022-02-26 15:32:10 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 3.0851
2022-02-26 15:32:43 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 3.2393
2022-02-26 15:33:16 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 3.5869
2022-02-26 15:33:49 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 3.0958
2022-02-26 15:34:23 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 3.2372
2022-02-26 15:34:55 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 3.0796
2022-02-26 15:35:29 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 3.1827
2022-02-26 15:36:02 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 3.0061
2022-02-26 15:36:37 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 3.0906
2022-02-26 15:37:09 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 3.2881
2022-02-26 15:37:43 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 3.0366
2022-02-26 15:38:16 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 3.3925
2022-02-26 15:38:49 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 3.0932
2022-02-26 15:39:23 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 3.2833
2022-02-26 15:39:54 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 3.2607
2022-02-26 15:39:56 - train: epoch 016, train_loss: 3.1899
2022-02-26 15:41:11 - eval: epoch: 016, acc1: 35.592%, acc5: 60.490%, test_loss: 3.0889, per_image_load_time: 2.803ms, per_image_inference_time: 0.101ms
2022-02-26 15:41:11 - until epoch: 016, best_acc1: 37.578%
2022-02-26 15:41:11 - epoch 017 lr: 0.1
2022-02-26 15:41:49 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 3.0204
2022-02-26 15:42:23 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 3.3581
2022-02-26 15:42:56 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 3.3274
2022-02-26 15:43:29 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.9447
2022-02-26 15:44:02 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 3.0893
2022-02-26 15:44:36 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 3.5010
2022-02-26 15:45:08 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 3.1859
2022-02-26 15:45:42 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 3.0734
2022-02-26 15:46:15 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 3.2035
2022-02-26 15:46:48 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 3.0410
2022-02-26 15:47:21 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 3.4349
2022-02-26 15:47:54 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 3.4381
2022-02-26 15:48:27 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 3.1792
2022-02-26 15:49:01 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 3.3667
2022-02-26 15:49:34 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.9990
2022-02-26 15:50:07 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 3.0560
2022-02-26 15:50:39 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 3.2289
2022-02-26 15:51:13 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 3.2974
2022-02-26 15:51:46 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.9619
2022-02-26 15:52:19 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 3.2641
2022-02-26 15:52:52 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 3.2773
2022-02-26 15:53:24 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 3.0779
2022-02-26 15:53:58 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 3.1809
2022-02-26 15:54:31 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 3.0083
2022-02-26 15:55:05 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 3.2854
2022-02-26 15:55:37 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 3.1657
2022-02-26 15:56:11 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 3.0346
2022-02-26 15:56:44 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 3.5004
2022-02-26 15:57:17 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 3.3510
2022-02-26 15:57:51 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.8484
2022-02-26 15:58:23 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 3.3415
2022-02-26 15:58:57 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 3.0214
2022-02-26 15:59:29 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 3.2565
2022-02-26 16:00:03 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 3.0307
2022-02-26 16:00:36 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 3.2679
2022-02-26 16:01:10 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 3.3733
2022-02-26 16:01:42 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 3.1068
2022-02-26 16:02:15 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 3.3996
2022-02-26 16:02:48 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 3.0684
2022-02-26 16:03:20 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 3.0891
2022-02-26 16:03:54 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 3.2177
2022-02-26 16:04:26 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 3.1495
2022-02-26 16:05:00 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 3.2002
2022-02-26 16:05:32 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 3.3174
2022-02-26 16:06:06 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 3.3168
2022-02-26 16:06:38 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 3.2168
2022-02-26 16:07:12 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 3.2746
2022-02-26 16:07:44 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 3.1206
2022-02-26 16:08:18 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.9906
2022-02-26 16:08:49 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.9582
2022-02-26 16:08:51 - train: epoch 017, train_loss: 3.1826
2022-02-26 16:10:06 - eval: epoch: 017, acc1: 37.842%, acc5: 63.044%, test_loss: 2.9357, per_image_load_time: 2.640ms, per_image_inference_time: 0.129ms
2022-02-26 16:10:06 - until epoch: 017, best_acc1: 37.842%
2022-02-26 16:10:06 - epoch 018 lr: 0.1
2022-02-26 16:10:44 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 3.2144
2022-02-26 16:11:17 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 3.1848
2022-02-26 16:11:50 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 3.3115
2022-02-26 16:12:24 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 3.2502
2022-02-26 16:12:57 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 3.1140
2022-02-26 16:13:31 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 3.3146
2022-02-26 16:14:05 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.9429
2022-02-26 16:14:40 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 3.1432
2022-02-26 16:15:14 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 3.2188
2022-02-26 16:15:48 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 3.1008
2022-02-26 16:16:21 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 3.4943
2022-02-26 16:16:55 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 3.0381
2022-02-26 16:17:27 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 3.4049
2022-02-26 16:18:00 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 3.1823
2022-02-26 16:18:34 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 3.2286
2022-02-26 16:19:07 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 3.1972
2022-02-26 16:19:41 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.9852
2022-02-26 16:20:13 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.9321
2022-02-26 16:20:47 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 3.3235
2022-02-26 16:21:21 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 3.6461
2022-02-26 16:21:53 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 3.4821
2022-02-26 16:22:27 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 3.2092
2022-02-26 16:22:59 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 3.1451
2022-02-26 16:23:33 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.9670
2022-02-26 16:24:05 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.8376
2022-02-26 16:24:38 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.9880
2022-02-26 16:25:12 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 3.3451
2022-02-26 16:25:45 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.9299
2022-02-26 16:26:18 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 3.2431
2022-02-26 16:26:51 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 3.0302
2022-02-26 16:27:24 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 3.5340
2022-02-26 16:27:57 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 3.0884
2022-02-26 16:28:30 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 3.1855
2022-02-26 16:29:03 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 3.1423
2022-02-26 16:29:36 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 3.2371
2022-02-26 16:30:09 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 3.2829
2022-02-26 16:30:42 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 3.6086
2022-02-26 16:31:14 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 3.2741
2022-02-26 16:31:48 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 3.1451
2022-02-26 16:32:21 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 3.1116
2022-02-26 16:32:55 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 3.0984
2022-02-26 16:33:28 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 3.1772
2022-02-26 16:34:01 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.9964
2022-02-26 16:34:35 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 3.2905
2022-02-26 16:35:08 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 3.2432
2022-02-26 16:35:42 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 3.0024
2022-02-26 16:36:14 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 3.3984
2022-02-26 16:36:48 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 3.2348
2022-02-26 16:37:21 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 3.2934
2022-02-26 16:37:53 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 3.2662
2022-02-26 16:37:54 - train: epoch 018, train_loss: 3.1751
2022-02-26 16:39:07 - eval: epoch: 018, acc1: 37.738%, acc5: 63.158%, test_loss: 2.9366, per_image_load_time: 2.743ms, per_image_inference_time: 0.108ms
2022-02-26 16:39:08 - until epoch: 018, best_acc1: 37.842%
2022-02-26 16:39:08 - epoch 019 lr: 0.1
2022-02-26 16:39:46 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 3.0224
2022-02-26 16:40:18 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 3.3998
2022-02-26 16:40:53 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 3.4192
2022-02-26 16:41:25 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 3.0448
2022-02-26 16:41:58 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 3.0609
2022-02-26 16:42:31 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 3.0757
2022-02-26 16:43:04 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.9437
2022-02-26 16:43:37 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 3.3383
2022-02-26 16:44:11 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 3.1508
2022-02-26 16:44:44 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 3.4792
2022-02-26 16:45:17 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.9078
2022-02-26 16:45:50 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 3.1865
2022-02-26 16:46:22 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 3.2170
2022-02-26 16:46:57 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 3.1070
2022-02-26 16:47:29 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 3.5260
2022-02-26 16:48:02 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 3.0993
2022-02-26 16:48:36 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 3.2550
2022-02-26 16:49:09 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 3.0778
2022-02-26 16:49:41 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 3.3662
2022-02-26 16:50:15 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 3.2399
2022-02-26 16:50:47 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 3.1750
2022-02-26 16:51:21 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 3.1923
2022-02-26 16:51:54 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 3.2106
2022-02-26 16:52:28 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 3.3549
2022-02-26 16:53:00 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 3.0611
2022-02-26 16:53:34 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 3.2057
2022-02-26 16:54:07 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 3.2488
2022-02-26 16:54:39 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 3.2009
2022-02-26 16:55:13 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 3.0723
2022-02-26 16:55:45 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 3.4390
2022-02-26 16:56:19 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 3.2271
2022-02-26 16:56:51 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.9455
2022-02-26 16:57:25 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 3.0975
2022-02-26 16:57:57 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 3.2299
2022-02-26 16:58:31 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 3.3122
2022-02-26 16:59:03 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 3.1372
2022-02-26 16:59:37 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 3.3206
2022-02-26 17:00:10 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 3.3462
2022-02-26 17:00:45 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 3.1516
2022-02-26 17:01:17 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 3.0235
2022-02-26 17:01:51 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 3.1724
2022-02-26 17:02:22 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 3.2287
2022-02-26 17:02:58 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 3.1361
2022-02-26 17:03:29 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 3.2441
2022-02-26 17:04:04 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 3.5416
2022-02-26 17:04:38 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 3.0865
2022-02-26 17:05:11 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 3.1593
2022-02-26 17:05:46 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 3.2374
2022-02-26 17:06:19 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 3.0138
2022-02-26 17:06:52 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 3.2655
2022-02-26 17:06:53 - train: epoch 019, train_loss: 3.1737
2022-02-26 17:08:07 - eval: epoch: 019, acc1: 38.540%, acc5: 63.738%, test_loss: 2.8787, per_image_load_time: 2.703ms, per_image_inference_time: 0.119ms
2022-02-26 17:08:07 - until epoch: 019, best_acc1: 38.540%
2022-02-26 17:08:07 - epoch 020 lr: 0.1
2022-02-26 17:08:45 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 3.4220
2022-02-26 17:09:17 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.9596
2022-02-26 17:09:51 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 3.3503
2022-02-26 17:10:24 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.9957
2022-02-26 17:10:58 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 3.1346
2022-02-26 17:11:30 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 3.3267
2022-02-26 17:12:03 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 3.1431
2022-02-26 17:12:38 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 3.1355
2022-02-26 17:13:10 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 3.3403
2022-02-26 17:13:44 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 3.2095
2022-02-26 17:14:16 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 3.0142
2022-02-26 17:14:49 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.9835
2022-02-26 17:15:23 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 3.2123
2022-02-26 17:15:56 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 3.3344
2022-02-26 17:16:30 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 3.1120
2022-02-26 17:17:04 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.9634
2022-02-26 17:17:37 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.8551
2022-02-26 17:18:11 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 3.2230
2022-02-26 17:18:44 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 3.0830
2022-02-26 17:19:18 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 3.1080
2022-02-26 17:19:50 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 3.3110
2022-02-26 17:20:24 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 3.0767
2022-02-26 17:20:57 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 3.0449
2022-02-26 17:21:31 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 3.2026
2022-02-26 17:22:04 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 3.0658
2022-02-26 17:22:39 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.9781
2022-02-26 17:23:11 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 3.2172
2022-02-26 17:23:45 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 3.2612
2022-02-26 17:24:16 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 3.3523
2022-02-26 17:24:50 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 3.0846
2022-02-26 17:25:23 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 3.1965
2022-02-26 17:25:57 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 3.3682
2022-02-26 17:26:30 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 3.0819
2022-02-26 17:27:03 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 3.1501
2022-02-26 17:27:38 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 3.0225
2022-02-26 17:28:10 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 3.0738
2022-02-26 17:28:44 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 3.0677
2022-02-26 17:29:18 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.9992
2022-02-26 17:29:51 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 3.2915
2022-02-26 17:30:25 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 3.0554
2022-02-26 17:30:57 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 3.0417
2022-02-26 17:31:31 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 3.0020
2022-02-26 17:32:03 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 3.2767
2022-02-26 17:32:38 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 3.1360
2022-02-26 17:33:10 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 3.1480
2022-02-26 17:33:44 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 3.2275
2022-02-26 17:34:17 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.9606
2022-02-26 17:34:51 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 3.2031
2022-02-26 17:35:24 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 3.2549
2022-02-26 17:35:56 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.9793
2022-02-26 17:35:57 - train: epoch 020, train_loss: 3.1636
2022-02-26 17:37:12 - eval: epoch: 020, acc1: 37.678%, acc5: 62.732%, test_loss: 2.9504, per_image_load_time: 2.509ms, per_image_inference_time: 0.107ms
2022-02-26 17:37:12 - until epoch: 020, best_acc1: 38.540%
2022-02-26 17:37:12 - epoch 021 lr: 0.1
2022-02-26 17:37:50 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 3.0526
2022-02-26 17:38:25 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 3.1504
2022-02-26 17:38:57 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.7280
2022-02-26 17:39:30 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 3.2118
2022-02-26 17:40:02 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.9634
2022-02-26 17:40:37 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 3.0208
2022-02-26 17:41:09 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 3.0014
2022-02-26 17:41:43 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 3.2753
2022-02-26 17:42:16 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 3.3261
2022-02-26 17:42:48 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 3.0133
2022-02-26 17:43:22 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 3.0579
2022-02-26 17:43:55 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 3.0817
2022-02-26 17:44:28 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 3.1210
2022-02-26 17:45:01 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 3.1496
2022-02-26 17:45:35 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.9744
2022-02-26 17:46:08 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 3.1927
2022-02-26 17:46:40 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 3.1792
2022-02-26 17:47:13 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 3.1199
2022-02-26 17:47:46 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 3.2123
2022-02-26 17:48:19 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 3.2850
2022-02-26 17:48:53 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.8784
2022-02-26 17:49:25 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 3.3189
2022-02-26 17:49:59 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.9728
2022-02-26 17:50:32 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 3.1580
2022-02-26 17:51:05 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 3.1089
2022-02-26 17:51:38 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 3.4700
2022-02-26 17:52:12 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.9911
2022-02-26 17:52:44 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 3.2010
2022-02-26 17:53:19 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 3.1227
2022-02-26 17:53:51 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 3.4167
2022-02-26 17:54:25 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 3.3171
2022-02-26 17:55:01 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 3.2099
2022-02-26 17:55:34 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 3.4762
2022-02-26 17:56:09 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 3.2993
2022-02-26 17:56:43 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 3.0655
2022-02-26 17:57:18 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 3.1081
2022-02-26 17:57:50 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 3.1324
2022-02-26 17:58:24 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 3.1439
2022-02-26 17:58:58 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 3.0030
2022-02-26 17:59:31 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 3.3347
2022-02-26 18:00:05 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.9140
2022-02-26 18:00:37 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 3.1798
2022-02-26 18:01:10 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 3.0687
2022-02-26 18:01:44 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 3.1549
2022-02-26 18:02:17 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 3.3130
2022-02-26 18:02:50 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 3.0937
2022-02-26 18:03:24 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 3.2453
2022-02-26 18:03:57 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 3.2558
2022-02-26 18:04:32 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 3.0220
2022-02-26 18:05:03 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 3.0687
2022-02-26 18:05:05 - train: epoch 021, train_loss: 3.1608
2022-02-26 18:06:19 - eval: epoch: 021, acc1: 38.268%, acc5: 63.818%, test_loss: 2.8850, per_image_load_time: 2.263ms, per_image_inference_time: 0.123ms
2022-02-26 18:06:19 - until epoch: 021, best_acc1: 38.540%
2022-02-26 18:06:19 - epoch 022 lr: 0.1
2022-02-26 18:06:57 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.8922
2022-02-26 18:07:31 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 3.1476
2022-02-26 18:08:04 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.8321
2022-02-26 18:08:37 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 3.0404
2022-02-26 18:09:10 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 3.1783
2022-02-26 18:09:45 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 3.1379
2022-02-26 18:10:18 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 3.2654
2022-02-26 18:10:52 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 3.3043
2022-02-26 18:11:24 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 3.3357
2022-02-26 18:11:59 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 3.0906
2022-02-26 18:12:31 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 3.2020
2022-02-26 18:13:07 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.8357
2022-02-26 18:13:40 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 3.1318
2022-02-26 18:14:14 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 3.3912
2022-02-26 18:14:47 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 3.1438
2022-02-26 18:15:21 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 3.0766
2022-02-26 18:15:54 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.8302
2022-02-26 18:16:28 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 3.5698
2022-02-26 18:17:02 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.9962
2022-02-26 18:17:36 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 3.2198
2022-02-26 18:18:08 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 3.1978
2022-02-26 18:18:42 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.9373
2022-02-26 18:19:16 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 3.3108
2022-02-26 18:19:50 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 3.1650
2022-02-26 18:20:23 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 3.0830
2022-02-26 18:20:57 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.9716
2022-02-26 18:21:30 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 3.0355
2022-02-26 18:22:04 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 3.5525
2022-02-26 18:22:36 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.9745
2022-02-26 18:23:11 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 3.1755
2022-02-26 18:23:44 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 3.4169
2022-02-26 18:24:16 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 3.1189
2022-02-26 18:24:51 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 3.1892
2022-02-26 18:25:24 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 3.0723
2022-02-26 18:25:58 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 3.2238
2022-02-26 18:26:31 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 3.1650
2022-02-26 18:27:05 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 3.2566
2022-02-26 18:27:39 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 3.1958
2022-02-26 18:28:12 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 3.0838
2022-02-26 18:28:47 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 3.2375
2022-02-26 18:29:19 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 3.0046
2022-02-26 18:29:53 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 3.2393
2022-02-26 18:30:26 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 3.3536
2022-02-26 18:31:00 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 3.1460
2022-02-26 18:31:33 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 3.0946
2022-02-26 18:32:07 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 3.3208
2022-02-26 18:32:40 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 3.2359
2022-02-26 18:33:15 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.9008
2022-02-26 18:33:48 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 3.0596
2022-02-26 18:34:21 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 3.0096
2022-02-26 18:34:22 - train: epoch 022, train_loss: 3.1584
2022-02-26 18:35:37 - eval: epoch: 022, acc1: 37.660%, acc5: 63.024%, test_loss: 2.9470, per_image_load_time: 2.754ms, per_image_inference_time: 0.116ms
2022-02-26 18:35:37 - until epoch: 022, best_acc1: 38.540%
2022-02-26 18:35:37 - epoch 023 lr: 0.1
2022-02-26 18:36:16 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.9829
2022-02-26 18:36:49 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.8611
2022-02-26 18:37:22 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 3.0234
2022-02-26 18:37:56 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 3.3752
2022-02-26 18:38:29 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 3.1236
2022-02-26 18:39:03 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 3.1325
2022-02-26 18:39:35 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 3.0534
2022-02-26 18:40:08 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 3.0692
2022-02-26 18:40:42 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 3.1764
2022-02-26 18:41:16 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.9059
2022-02-26 18:41:49 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 3.2054
2022-02-26 18:42:22 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.8445
2022-02-26 18:42:56 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.9996
2022-02-26 18:43:30 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 3.1143
2022-02-26 18:44:03 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 3.0101
2022-02-26 18:44:37 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 3.0742
2022-02-26 18:45:12 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 3.2778
2022-02-26 18:45:47 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 3.0974
2022-02-26 18:46:22 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 3.2323
2022-02-26 18:46:56 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.8377
2022-02-26 18:47:32 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 3.2434
2022-02-26 18:48:08 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.9119
2022-02-26 18:48:42 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.9609
2022-02-26 18:49:14 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 3.2587
2022-02-26 18:49:49 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 3.1985
2022-02-26 18:50:24 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 3.1128
2022-02-26 18:50:56 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 3.1398
2022-02-26 18:51:31 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 3.1691
2022-02-26 18:52:04 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 3.2211
2022-02-26 18:52:37 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 3.3094
2022-02-26 18:53:11 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 3.0723
2022-02-26 18:53:45 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 3.2596
2022-02-26 18:54:17 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 3.2562
2022-02-26 18:54:52 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 3.3721
2022-02-26 18:55:24 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 3.0272
2022-02-26 18:55:58 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 3.0282
2022-02-26 18:56:31 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 3.1976
2022-02-26 18:57:06 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 3.1336
2022-02-26 18:57:38 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 3.1722
2022-02-26 18:58:13 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 3.1372
2022-02-26 18:58:45 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.9983
2022-02-26 18:59:19 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.8372
2022-02-26 18:59:53 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.9912
2022-02-26 19:00:26 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 3.0377
2022-02-26 19:01:00 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 3.0767
2022-02-26 19:01:33 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 3.0814
2022-02-26 19:02:07 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.9734
2022-02-26 19:02:40 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 3.0590
2022-02-26 19:03:16 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 3.0556
2022-02-26 19:03:47 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 3.1741
2022-02-26 19:03:48 - train: epoch 023, train_loss: 3.1532
2022-02-26 19:05:03 - eval: epoch: 023, acc1: 38.250%, acc5: 63.776%, test_loss: 2.8921, per_image_load_time: 2.785ms, per_image_inference_time: 0.120ms
2022-02-26 19:05:03 - until epoch: 023, best_acc1: 38.540%
2022-02-26 19:05:03 - epoch 024 lr: 0.1
2022-02-26 19:05:42 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 3.0565
2022-02-26 19:06:14 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 3.1488
2022-02-26 19:06:47 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 3.1512
2022-02-26 19:07:21 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 3.1464
2022-02-26 19:07:54 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 3.1656
2022-02-26 19:08:28 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.8596
2022-02-26 19:09:01 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 3.2455
2022-02-26 19:09:35 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 3.0711
2022-02-26 19:10:07 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 3.1392
2022-02-26 19:10:41 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 3.0770
2022-02-26 19:11:13 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.9737
2022-02-26 19:11:48 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.9776
2022-02-26 19:12:20 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 3.5084
2022-02-26 19:12:55 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 3.0041
2022-02-26 19:13:28 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 3.3923
2022-02-26 19:14:02 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 3.1395
2022-02-26 19:14:35 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 3.2102
2022-02-26 19:15:09 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 3.1814
2022-02-26 19:15:41 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.8879
2022-02-26 19:16:15 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 3.1763
2022-02-26 19:16:49 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 3.1277
2022-02-26 19:17:21 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 3.0098
2022-02-26 19:17:55 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 3.2184
2022-02-26 19:18:29 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 3.1133
2022-02-26 19:19:01 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 3.0698
2022-02-26 19:19:36 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 3.0637
2022-02-26 19:20:08 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 3.1680
2022-02-26 19:20:41 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 3.2649
2022-02-26 19:21:15 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 3.3860
2022-02-26 19:21:48 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 3.0826
2022-02-26 19:22:21 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 3.0909
2022-02-26 19:22:55 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 3.3123
2022-02-26 19:23:29 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.9808
2022-02-26 19:24:02 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 3.1563
2022-02-26 19:24:36 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 3.0197
2022-02-26 19:25:08 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 3.1584
2022-02-26 19:25:42 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 3.1546
2022-02-26 19:26:15 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 3.2934
2022-02-26 19:26:48 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 3.2779
2022-02-26 19:27:21 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 3.0541
2022-02-26 19:27:56 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 3.0919
2022-02-26 19:28:29 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 3.1388
2022-02-26 19:29:02 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 3.1181
2022-02-26 19:29:37 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 3.2006
2022-02-26 19:30:09 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.9379
2022-02-26 19:30:42 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 3.2729
2022-02-26 19:31:17 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 3.0968
2022-02-26 19:31:50 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 3.0807
2022-02-26 19:32:24 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 3.2986
2022-02-26 19:32:56 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 3.3424
2022-02-26 19:32:57 - train: epoch 024, train_loss: 3.1499
2022-02-26 19:34:12 - eval: epoch: 024, acc1: 36.706%, acc5: 62.298%, test_loss: 2.9898, per_image_load_time: 2.071ms, per_image_inference_time: 0.121ms
2022-02-26 19:34:12 - until epoch: 024, best_acc1: 38.540%
2022-02-26 23:57:02 - epoch 025 lr: 0.1
2022-02-26 23:57:40 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 3.0153
2022-02-26 23:58:14 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.7778
2022-02-26 23:58:46 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 3.1410
2022-02-26 23:59:19 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 3.2429
2022-02-26 23:59:51 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.9214
2022-02-27 00:00:25 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 3.1758
2022-02-27 00:00:56 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 3.2721
2022-02-27 00:01:30 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 3.1411
2022-02-27 00:02:03 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.9414
2022-02-27 00:02:36 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 3.1397
2022-02-27 00:03:10 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 3.1294
2022-02-27 00:03:42 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 3.2168
2022-02-27 00:04:16 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.9081
2022-02-27 00:04:48 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 3.2138
2022-02-27 00:05:23 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.9329
2022-02-27 00:05:55 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.8325
2022-02-27 00:06:29 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 3.1237
2022-02-27 00:07:01 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.9874
2022-02-27 00:07:35 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 3.1152
2022-02-27 00:08:08 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 3.1699
2022-02-27 00:08:42 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 3.0021
2022-02-27 00:09:15 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.8258
2022-02-27 00:09:49 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 3.1716
2022-02-27 00:10:21 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 3.0358
2022-02-27 00:10:55 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 3.2277
2022-02-27 00:11:27 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 3.1861
2022-02-27 00:12:02 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 3.3931
2022-02-27 00:12:35 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 3.1738
2022-02-27 00:13:08 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 3.2436
2022-02-27 00:13:41 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 3.2387
2022-02-27 00:14:14 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 3.2254
2022-02-27 00:14:47 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.9775
2022-02-27 00:15:21 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 3.0422
2022-02-27 00:15:53 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 3.2185
2022-02-27 00:16:28 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.9832
2022-02-27 00:17:00 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.9718
2022-02-27 00:17:33 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 3.0406
2022-02-27 00:18:06 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 3.1374
2022-02-27 00:18:40 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 3.5307
2022-02-27 00:19:13 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 3.1359
2022-02-27 00:19:46 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 3.3669
2022-02-27 00:20:21 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 3.2308
2022-02-27 00:20:53 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 3.1170
2022-02-27 00:21:26 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.9812
2022-02-27 00:22:00 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 3.1476
2022-02-27 00:22:32 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 3.1001
2022-02-27 00:23:06 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 3.0315
2022-02-27 00:23:39 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 3.1339
2022-02-27 00:24:14 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 3.3639
2022-02-27 00:24:45 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 3.2555
2022-02-27 00:24:46 - train: epoch 025, train_loss: 3.1469
2022-02-27 00:25:59 - eval: epoch: 025, acc1: 39.360%, acc5: 64.850%, test_loss: 2.8420, per_image_load_time: 1.975ms, per_image_inference_time: 0.105ms
2022-02-27 00:25:59 - until epoch: 025, best_acc1: 39.360%
2022-02-27 00:25:59 - epoch 026 lr: 0.1
2022-02-27 00:26:38 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 3.3132
2022-02-27 00:27:10 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 3.2269
2022-02-27 00:27:43 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 3.1118
2022-02-27 00:28:16 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 3.0988
2022-02-27 00:28:50 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 3.1800
2022-02-27 00:29:21 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 3.2676
2022-02-27 00:29:55 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 3.1542
2022-02-27 00:30:28 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.8492
2022-02-27 00:31:00 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 3.1673
2022-02-27 00:31:35 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 3.2615
2022-02-27 00:32:07 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 3.0617
2022-02-27 00:32:39 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 3.1398
2022-02-27 00:33:13 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.9336
2022-02-27 00:33:45 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 3.0655
2022-02-27 00:34:19 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 3.1208
2022-02-27 00:34:52 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 3.3638
2022-02-27 00:35:26 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 3.3165
2022-02-27 00:35:58 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 3.2613
2022-02-27 00:36:32 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 3.2573
2022-02-27 00:37:06 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 3.1117
2022-02-27 00:37:37 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 3.1970
2022-02-27 00:38:12 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 3.1584
2022-02-27 00:38:44 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 3.0929
2022-02-27 00:39:17 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 3.2562
2022-02-27 00:39:50 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 3.1856
2022-02-27 00:40:23 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 3.1637
2022-02-27 00:40:57 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 3.2286
2022-02-27 00:41:30 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 3.0429
2022-02-27 00:42:04 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 3.2392
2022-02-27 00:42:38 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 3.2286
2022-02-27 00:43:12 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 3.1283
2022-02-27 00:43:47 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 3.1256
2022-02-27 00:44:19 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 3.0127
2022-02-27 00:44:54 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 3.0973
2022-02-27 00:45:26 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 3.1649
2022-02-27 00:46:00 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 3.1348
2022-02-27 00:46:32 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 3.0362
2022-02-27 00:47:06 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 3.3257
2022-02-27 00:47:39 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 3.2890
2022-02-27 00:48:13 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 3.1900
2022-02-27 00:48:45 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.9391
2022-02-27 00:49:20 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 3.2096
2022-02-27 00:49:52 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 3.1046
2022-02-27 00:50:26 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 3.2696
2022-02-27 00:50:58 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 3.3280
2022-02-27 00:51:32 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 3.1481
2022-02-27 00:52:06 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 3.1449
2022-02-27 00:52:39 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 3.1147
2022-02-27 00:53:13 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 3.4001
2022-02-27 00:53:46 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 3.2407
2022-02-27 00:53:46 - train: epoch 026, train_loss: 3.1444
2022-02-27 00:55:00 - eval: epoch: 026, acc1: 37.424%, acc5: 63.000%, test_loss: 2.9542, per_image_load_time: 2.714ms, per_image_inference_time: 0.129ms
2022-02-27 00:55:01 - until epoch: 026, best_acc1: 39.360%
2022-02-27 00:55:01 - epoch 027 lr: 0.1
2022-02-27 00:55:39 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 3.2368
2022-02-27 00:56:12 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 3.2561
2022-02-27 00:56:45 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 3.0828
2022-02-27 00:57:19 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 3.2560
2022-02-27 00:57:52 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 3.2822
2022-02-27 00:58:25 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 3.2442
2022-02-27 00:58:58 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 3.2199
2022-02-27 00:59:32 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 3.1758
2022-02-27 01:00:03 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 3.0497
2022-02-27 01:00:39 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 3.2276
2022-02-27 01:01:11 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 3.1205
2022-02-27 01:01:45 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 3.3087
2022-02-27 01:02:18 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 3.3758
2022-02-27 01:02:52 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 3.4340
2022-02-27 01:03:24 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 3.2037
2022-02-27 01:03:58 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 3.2063
2022-02-27 01:04:31 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 3.1248
2022-02-27 01:05:05 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 3.1987
2022-02-27 01:05:38 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 3.2742
2022-02-27 01:06:12 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 3.0690
2022-02-27 01:06:45 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 3.2666
2022-02-27 01:07:18 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 3.4131
2022-02-27 01:07:51 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 3.4053
2022-02-27 01:08:26 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 3.2374
2022-02-27 01:08:59 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 3.0748
2022-02-27 01:09:33 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 3.0243
2022-02-27 01:10:05 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.9751
2022-02-27 01:10:39 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.9610
2022-02-27 01:11:12 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 3.1863
2022-02-27 01:11:46 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.9289
2022-02-27 01:12:19 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.9743
2022-02-27 01:12:52 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.9882
2022-02-27 01:13:25 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 3.2612
2022-02-27 01:13:59 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 3.0361
2022-02-27 01:14:33 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 3.2915
2022-02-27 01:15:07 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 3.2297
2022-02-27 01:15:40 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 3.0520
2022-02-27 01:16:13 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 3.1136
2022-02-27 01:16:47 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 3.0258
2022-02-27 01:17:20 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 3.2714
2022-02-27 01:17:54 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 3.0875
2022-02-27 01:18:26 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 3.0068
2022-02-27 01:19:01 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 3.2076
2022-02-27 01:19:33 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 3.2417
2022-02-27 01:20:07 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 3.0062
2022-02-27 01:20:40 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 3.0682
2022-02-27 01:21:13 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 3.4057
2022-02-27 01:21:47 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 3.3560
2022-02-27 01:22:21 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 3.0778
2022-02-27 01:22:53 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.7541
2022-02-27 01:22:54 - train: epoch 027, train_loss: 3.1422
2022-02-27 01:24:10 - eval: epoch: 027, acc1: 38.384%, acc5: 64.016%, test_loss: 2.8898, per_image_load_time: 2.826ms, per_image_inference_time: 0.101ms
2022-02-27 01:24:10 - until epoch: 027, best_acc1: 39.360%
2022-02-27 01:24:10 - epoch 028 lr: 0.1
2022-02-27 01:24:49 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.9067
2022-02-27 01:25:20 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.9955
2022-02-27 01:25:54 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 3.0673
2022-02-27 01:26:27 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 3.1974
2022-02-27 01:27:01 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 3.1091
2022-02-27 01:27:34 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 3.3190
2022-02-27 01:28:08 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 3.3032
2022-02-27 01:28:40 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 3.2806
2022-02-27 01:29:14 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.9195
2022-02-27 01:29:46 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 3.3335
2022-02-27 01:30:20 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 3.0912
2022-02-27 01:30:53 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 3.0743
2022-02-27 01:31:28 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 3.0298
2022-02-27 01:32:02 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 3.3700
2022-02-27 01:32:37 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 3.1859
2022-02-27 01:33:11 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 3.2104
2022-02-27 01:33:46 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 3.3521
2022-02-27 01:34:20 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 3.0223
2022-02-27 01:34:54 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 3.2018
2022-02-27 01:35:26 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 3.2135
2022-02-27 01:36:00 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 3.4491
2022-02-27 01:36:34 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 3.2733
2022-02-27 01:37:07 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 3.1826
2022-02-27 01:37:41 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 3.4082
2022-02-27 01:38:14 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 3.2652
2022-02-27 01:38:48 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.9147
2022-02-27 01:39:21 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 3.1185
2022-02-27 01:39:54 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 3.0822
2022-02-27 01:40:28 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 3.0649
2022-02-27 01:41:01 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 3.0956
2022-02-27 01:41:35 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 3.3767
2022-02-27 01:42:07 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 3.0066
2022-02-27 01:42:41 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 3.1092
2022-02-27 01:43:15 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 3.0650
2022-02-27 01:43:47 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.9676
2022-02-27 01:44:21 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 3.0653
2022-02-27 01:44:53 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 3.2561
2022-02-27 01:45:27 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 3.0407
2022-02-27 01:46:01 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 3.3550
2022-02-27 01:46:34 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 3.1375
2022-02-27 01:47:06 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 3.0498
2022-02-27 01:47:39 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 3.1256
2022-02-27 01:48:13 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 3.1501
2022-02-27 01:48:46 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.9942
2022-02-27 01:49:21 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 3.1039
2022-02-27 01:49:53 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 3.2105
2022-02-27 01:50:27 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 3.2625
2022-02-27 01:51:00 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 3.1687
2022-02-27 01:51:34 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 3.0376
2022-02-27 01:52:06 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.9080
2022-02-27 01:52:06 - train: epoch 028, train_loss: 3.1416
2022-02-27 01:53:21 - eval: epoch: 028, acc1: 38.860%, acc5: 64.436%, test_loss: 2.8782, per_image_load_time: 2.213ms, per_image_inference_time: 0.093ms
2022-02-27 01:53:21 - until epoch: 028, best_acc1: 39.360%
2022-02-27 01:53:21 - epoch 029 lr: 0.1
2022-02-27 01:53:58 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 3.0833
2022-02-27 01:54:33 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 3.2378
2022-02-27 01:55:05 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 3.1318
2022-02-27 01:55:37 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 3.1911
2022-02-27 01:56:11 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 3.0642
2022-02-27 01:56:43 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 3.2371
2022-02-27 01:57:17 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.9902
2022-02-27 01:57:50 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 3.0704
2022-02-27 01:58:22 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 3.0458
2022-02-27 01:58:56 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 3.2811
2022-02-27 01:59:28 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 3.1568
2022-02-27 02:00:03 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 3.1482
2022-02-27 02:00:36 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 3.1381
2022-02-27 02:01:09 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 3.2317
2022-02-27 02:01:42 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 3.2428
2022-02-27 02:02:16 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 3.4670
2022-02-27 02:02:49 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 3.0704
2022-02-27 02:03:22 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 3.0921
2022-02-27 02:03:55 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.9369
2022-02-27 02:04:28 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 3.1304
2022-02-27 02:05:01 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 3.0962
2022-02-27 02:05:34 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.9955
2022-02-27 02:06:07 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 3.0798
2022-02-27 02:06:41 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 3.1348
2022-02-27 02:07:14 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 3.1323
2022-02-27 02:07:48 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 3.3812
2022-02-27 02:08:20 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 3.0549
2022-02-27 02:08:54 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 3.0679
2022-02-27 02:09:27 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.9208
2022-02-27 02:10:00 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 3.1177
2022-02-27 02:10:34 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 3.0462
2022-02-27 02:11:07 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 3.1887
2022-02-27 02:11:40 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.9655
2022-02-27 02:12:13 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 3.0013
2022-02-27 02:12:46 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 3.4698
2022-02-27 02:13:20 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 3.1275
2022-02-27 02:13:53 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 3.0598
2022-02-27 02:14:26 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.9957
2022-02-27 02:15:00 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.9979
2022-02-27 02:15:34 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 3.4518
2022-02-27 02:16:06 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 3.0671
2022-02-27 02:16:41 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.9712
2022-02-27 02:17:13 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 3.0831
2022-02-27 02:17:47 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.9822
2022-02-27 02:18:19 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 3.3025
2022-02-27 02:18:53 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 3.1606
2022-02-27 02:19:25 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 3.0364
2022-02-27 02:19:59 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 3.1005
2022-02-27 02:20:32 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 3.3040
2022-02-27 02:21:05 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.8308
2022-02-27 02:21:05 - train: epoch 029, train_loss: 3.1377
2022-02-27 02:22:21 - eval: epoch: 029, acc1: 38.448%, acc5: 63.766%, test_loss: 2.8839, per_image_load_time: 1.909ms, per_image_inference_time: 0.120ms
2022-02-27 02:22:21 - until epoch: 029, best_acc1: 39.360%
2022-02-27 02:22:21 - epoch 030 lr: 0.1
2022-02-27 02:23:02 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 3.3039
2022-02-27 02:23:35 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 3.1579
2022-02-27 02:24:10 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 3.0515
2022-02-27 02:24:44 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 3.0298
2022-02-27 02:25:18 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 3.3230
2022-02-27 02:25:51 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 3.0904
2022-02-27 02:26:24 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 3.3038
2022-02-27 02:26:58 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 3.2337
2022-02-27 02:27:30 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 3.1668
2022-02-27 02:28:04 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 3.0015
2022-02-27 02:28:38 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 3.2312
2022-02-27 02:29:10 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.9801
2022-02-27 02:29:44 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 3.0446
2022-02-27 02:30:16 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.9852
2022-02-27 02:30:49 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 3.0022
2022-02-27 02:31:22 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 3.1469
2022-02-27 02:31:55 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 3.3707
2022-02-27 02:32:29 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.9683
2022-02-27 02:33:01 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 3.0222
2022-02-27 02:33:35 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 3.1348
2022-02-27 02:34:07 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 3.1050
2022-02-27 02:34:40 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 3.3630
2022-02-27 02:35:14 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 3.1865
2022-02-27 02:35:47 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 3.2342
2022-02-27 02:36:19 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 3.2893
2022-02-27 02:36:53 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 3.1534
2022-02-27 02:37:26 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 3.0390
2022-02-27 02:37:58 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 3.1576
2022-02-27 02:38:32 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 3.0394
2022-02-27 02:39:04 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 3.1262
2022-02-27 02:39:38 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 3.2800
2022-02-27 02:40:10 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 3.0453
2022-02-27 02:40:43 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 3.3744
2022-02-27 02:41:16 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 3.0442
2022-02-27 02:41:49 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 3.0956
2022-02-27 02:42:23 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 3.1072
2022-02-27 02:42:56 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 3.0809
2022-02-27 02:43:29 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 3.1216
2022-02-27 02:44:01 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.9465
2022-02-27 02:44:35 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 3.0525
2022-02-27 02:45:07 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.9771
2022-02-27 02:45:42 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 3.2432
2022-02-27 02:46:14 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 3.0601
2022-02-27 02:46:48 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 3.1935
2022-02-27 02:47:20 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 3.3167
2022-02-27 02:47:54 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.8583
2022-02-27 02:48:26 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 3.1574
2022-02-27 02:49:00 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 3.1754
2022-02-27 02:49:33 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 3.0876
2022-02-27 02:50:05 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 3.2814
2022-02-27 02:50:06 - train: epoch 030, train_loss: 3.1375
2022-02-27 02:51:19 - eval: epoch: 030, acc1: 39.128%, acc5: 64.434%, test_loss: 2.8581, per_image_load_time: 1.320ms, per_image_inference_time: 0.115ms
2022-02-27 02:51:19 - until epoch: 030, best_acc1: 39.360%
2022-02-27 02:51:19 - epoch 031 lr: 0.010000000000000002
2022-02-27 02:51:57 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 3.0145
2022-02-27 02:52:29 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.7891
2022-02-27 02:53:03 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 2.7058
2022-02-27 02:53:35 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.7649
2022-02-27 02:54:10 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 2.7464
2022-02-27 02:54:42 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 2.6366
2022-02-27 02:55:16 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 2.8575
2022-02-27 02:55:48 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 2.9461
2022-02-27 02:56:22 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 2.8545
2022-02-27 02:56:56 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.9040
2022-02-27 02:57:29 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.7703
2022-02-27 02:58:02 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 2.9163
2022-02-27 02:58:36 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 2.6784
2022-02-27 02:59:09 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 2.5230
2022-02-27 02:59:43 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.8867
2022-02-27 03:00:15 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 2.6043
2022-02-27 03:00:49 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 2.7119
2022-02-27 03:01:21 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 2.5507
2022-02-27 03:01:55 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 2.6809
2022-02-27 03:02:28 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 2.7986
2022-02-27 03:03:01 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 2.5364
2022-02-27 03:03:35 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 2.6868
2022-02-27 03:04:09 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 2.5752
2022-02-27 03:04:42 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 2.7030
2022-02-27 03:05:16 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 2.7695
2022-02-27 03:05:48 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 2.7713
2022-02-27 03:06:22 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 2.8565
2022-02-27 03:06:55 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 3.0460
2022-02-27 03:07:30 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 2.5407
2022-02-27 03:08:02 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 3.0647
2022-02-27 03:08:36 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 2.6590
2022-02-27 03:09:08 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 2.9433
2022-02-27 03:09:42 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 2.6549
2022-02-27 03:10:15 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.7520
2022-02-27 03:10:50 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 2.9098
2022-02-27 03:11:23 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 2.7185
2022-02-27 03:11:57 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 2.7098
2022-02-27 03:12:30 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 2.6528
2022-02-27 03:13:05 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 2.7053
2022-02-27 03:13:38 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 2.6329
2022-02-27 03:14:12 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 2.7133
2022-02-27 03:14:45 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 2.7960
2022-02-27 03:15:19 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 2.5865
2022-02-27 03:15:53 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 2.9410
2022-02-27 03:16:27 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 2.7292
2022-02-27 03:17:00 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 2.8948
2022-02-27 03:17:33 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 2.6157
2022-02-27 03:18:08 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 2.6569
2022-02-27 03:18:41 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 2.6278
2022-02-27 03:19:14 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 2.7827
2022-02-27 03:19:15 - train: epoch 031, train_loss: 2.7340
2022-02-27 03:20:29 - eval: epoch: 031, acc1: 49.522%, acc5: 73.396%, test_loss: 2.3099, per_image_load_time: 2.230ms, per_image_inference_time: 0.106ms
2022-02-27 03:20:29 - until epoch: 031, best_acc1: 49.522%
2022-02-27 03:20:29 - epoch 032 lr: 0.010000000000000002
2022-02-27 03:21:08 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 2.6133
2022-02-27 03:21:42 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 2.5857
2022-02-27 03:22:14 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 2.4892
2022-02-27 03:22:48 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 2.6263
2022-02-27 03:23:22 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 2.7439
2022-02-27 03:23:54 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 2.6612
2022-02-27 03:24:28 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 2.5915
2022-02-27 03:25:01 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 2.5869
2022-02-27 03:25:34 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 2.7796
2022-02-27 03:26:08 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 2.6914
2022-02-27 03:26:41 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 2.6570
2022-02-27 03:27:14 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 2.7380
2022-02-27 03:27:49 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 2.6755
2022-02-27 03:28:22 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 2.8849
2022-02-27 03:28:56 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 2.5696
2022-02-27 03:29:29 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 2.7350
2022-02-27 03:30:03 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 2.7930
2022-02-27 03:30:35 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.9809
2022-02-27 03:31:09 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 2.2847
2022-02-27 03:31:41 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 2.6421
2022-02-27 03:32:15 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 2.5169
2022-02-27 03:32:48 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 2.6362
2022-02-27 03:33:22 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 2.5592
2022-02-27 03:33:57 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 2.6745
2022-02-27 03:34:29 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 2.5152
2022-02-27 03:35:04 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 2.2062
2022-02-27 03:35:37 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 2.7737
2022-02-27 03:36:10 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 2.6364
2022-02-27 03:36:44 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 2.5016
2022-02-27 03:37:17 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 2.6377
2022-02-27 03:37:50 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 2.7657
2022-02-27 03:38:23 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 2.8371
2022-02-27 03:38:57 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 2.7314
2022-02-27 03:39:29 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 2.3454
2022-02-27 03:40:03 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 2.6657
2022-02-27 03:40:36 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 2.7004
2022-02-27 03:41:10 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 2.7639
2022-02-27 03:41:43 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 2.5958
2022-02-27 03:42:16 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 2.6025
2022-02-27 03:42:50 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 2.7336
2022-02-27 03:43:24 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 2.6068
2022-02-27 03:43:57 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 2.5376
2022-02-27 03:44:30 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 2.6991
2022-02-27 03:45:05 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 2.7328
2022-02-27 03:45:37 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 2.7194
2022-02-27 03:46:11 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 2.5213
2022-02-27 03:46:44 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 2.6759
2022-02-27 03:47:19 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 2.5514
2022-02-27 03:47:51 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 2.5799
2022-02-27 03:48:25 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 2.6699
2022-02-27 03:48:25 - train: epoch 032, train_loss: 2.6564
2022-02-27 03:49:41 - eval: epoch: 032, acc1: 49.964%, acc5: 73.816%, test_loss: 2.2846, per_image_load_time: 2.836ms, per_image_inference_time: 0.107ms
2022-02-27 03:49:41 - until epoch: 032, best_acc1: 49.964%
2022-02-27 03:49:41 - epoch 033 lr: 0.010000000000000002
2022-02-27 03:50:20 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 2.4369
2022-02-27 03:50:52 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 2.7232
2022-02-27 03:51:26 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 2.5172
2022-02-27 03:51:59 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 2.4786
2022-02-27 03:52:32 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 2.7490
2022-02-27 03:53:06 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 2.5959
2022-02-27 03:53:39 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 2.9298
2022-02-27 03:54:13 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 2.6291
2022-02-27 03:54:47 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 2.6795
2022-02-27 03:55:19 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 2.7751
2022-02-27 03:55:54 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 2.6121
2022-02-27 03:56:26 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 2.9241
2022-02-27 03:57:00 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 2.5680
2022-02-27 03:57:34 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 2.6216
2022-02-27 03:58:08 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 2.8203
2022-02-27 03:58:41 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 2.7564
2022-02-27 03:59:14 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 2.6257
2022-02-27 03:59:48 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 2.6484
2022-02-27 04:00:20 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 2.5176
2022-02-27 04:00:54 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 2.6099
2022-02-27 04:01:27 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 2.5478
2022-02-27 04:02:01 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 2.6363
2022-02-27 04:02:34 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 2.6024
2022-02-27 04:03:07 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 2.7026
2022-02-27 04:03:42 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 2.5976
2022-02-27 04:04:17 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 2.4927
2022-02-27 04:04:51 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 2.7048
2022-02-27 04:05:26 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 2.8226
2022-02-27 04:05:59 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 2.7618
2022-02-27 04:06:34 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 2.6595
2022-02-27 04:07:09 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 2.6487
2022-02-27 04:07:41 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 2.5996
2022-02-27 04:08:14 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 2.6484
2022-02-27 04:08:47 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 2.5352
2022-02-27 04:09:20 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 2.7013
2022-02-27 04:09:54 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 2.6877
2022-02-27 04:10:27 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 2.5953
2022-02-27 04:11:01 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 2.7949
2022-02-27 04:11:34 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 2.7604
2022-02-27 04:12:08 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 2.6311
2022-02-27 04:12:41 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 2.5946
2022-02-27 04:13:16 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 2.5276
2022-02-27 04:13:48 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 2.7527
2022-02-27 04:14:21 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 2.6005
2022-02-27 04:14:54 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 2.8004
2022-02-27 04:15:28 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 2.5983
2022-02-27 04:16:01 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 2.6107
2022-02-27 04:16:35 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.7991
2022-02-27 04:17:07 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 2.4866
2022-02-27 04:17:40 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 2.6891
2022-02-27 04:17:41 - train: epoch 033, train_loss: 2.6299
2022-02-27 04:18:56 - eval: epoch: 033, acc1: 50.348%, acc5: 74.180%, test_loss: 2.2594, per_image_load_time: 2.806ms, per_image_inference_time: 0.107ms
2022-02-27 04:18:56 - until epoch: 033, best_acc1: 50.348%
2022-02-27 04:18:56 - epoch 034 lr: 0.010000000000000002
2022-02-27 04:19:34 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 2.6368
2022-02-27 04:20:06 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 2.4000
2022-02-27 04:20:40 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 2.5312
2022-02-27 04:21:13 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 2.5718
2022-02-27 04:21:46 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 2.4547
2022-02-27 04:22:18 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 2.7853
2022-02-27 04:22:51 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 2.4089
2022-02-27 04:23:24 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 2.9050
2022-02-27 04:23:57 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 2.5369
2022-02-27 04:24:30 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 2.6075
2022-02-27 04:25:04 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 2.7125
2022-02-27 04:25:36 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 2.9335
2022-02-27 04:26:10 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 2.5450
2022-02-27 04:26:42 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 2.5557
2022-02-27 04:27:15 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 2.5791
2022-02-27 04:27:47 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 2.5426
2022-02-27 04:28:20 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 2.5358
2022-02-27 04:28:54 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 2.9446
2022-02-27 04:29:27 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 2.6461
2022-02-27 04:30:00 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 2.3560
2022-02-27 04:30:32 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 2.6816
2022-02-27 04:31:06 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 2.2779
2022-02-27 04:31:38 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 2.7438
2022-02-27 04:32:12 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 2.3308
2022-02-27 04:32:44 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 2.8878
2022-02-27 04:33:18 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 2.9785
2022-02-27 04:33:51 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 2.6921
2022-02-27 04:34:25 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 2.3891
2022-02-27 04:34:57 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 2.4681
2022-02-27 04:35:32 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 2.3597
2022-02-27 04:36:04 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 2.6090
2022-02-27 04:36:38 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 2.5369
2022-02-27 04:37:11 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 2.4314
2022-02-27 04:37:45 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 2.6967
2022-02-27 04:38:17 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 2.6394
2022-02-27 04:38:50 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 2.3434
2022-02-27 04:39:24 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 2.4911
2022-02-27 04:39:56 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 2.5962
2022-02-27 04:40:30 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 2.6882
2022-02-27 04:41:02 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 2.5620
2022-02-27 04:41:36 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 2.7041
2022-02-27 04:42:08 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 2.5321
2022-02-27 04:42:42 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 2.7420
2022-02-27 04:43:15 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 2.5382
2022-02-27 04:43:48 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 2.5931
2022-02-27 04:44:22 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 2.8499
2022-02-27 04:44:54 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 2.7638
2022-02-27 04:45:29 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 2.7700
2022-02-27 04:46:01 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 2.6380
2022-02-27 04:46:34 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 2.5135
2022-02-27 04:46:35 - train: epoch 034, train_loss: 2.6167
2022-02-27 04:47:48 - eval: epoch: 034, acc1: 50.556%, acc5: 74.442%, test_loss: 2.2439, per_image_load_time: 1.356ms, per_image_inference_time: 0.115ms
2022-02-27 04:47:48 - until epoch: 034, best_acc1: 50.556%
2022-02-27 04:47:48 - epoch 035 lr: 0.010000000000000002
2022-02-27 04:48:26 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 2.3957
2022-02-27 04:48:59 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 2.4503
2022-02-27 04:49:33 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 2.6227
2022-02-27 04:50:05 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 2.6371
2022-02-27 04:50:38 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 2.3860
2022-02-27 04:51:11 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 2.7074
2022-02-27 04:51:44 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 2.4411
2022-02-27 04:52:18 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 2.5801
2022-02-27 04:52:50 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 2.5702
2022-02-27 04:53:24 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 2.5416
2022-02-27 04:53:57 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 2.6983
2022-02-27 04:54:32 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 2.5842
2022-02-27 04:55:05 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 2.8251
2022-02-27 04:55:40 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 2.6336
2022-02-27 04:56:14 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 2.6356
2022-02-27 04:56:50 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 2.6126
2022-02-27 04:57:23 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 2.4463
2022-02-27 04:57:58 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 2.6456
2022-02-27 04:58:31 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 2.6056
2022-02-27 04:59:04 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 2.7015
2022-02-27 04:59:37 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 2.4236
2022-02-27 05:00:11 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 2.8154
2022-02-27 05:00:43 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 2.7278
2022-02-27 05:01:17 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 2.7555
2022-02-27 05:01:50 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 2.5872
2022-02-27 05:02:23 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 2.6753
2022-02-27 05:02:57 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 2.4220
2022-02-27 05:03:30 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 2.5703
2022-02-27 05:04:03 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 2.6791
2022-02-27 05:04:36 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 2.4094
2022-02-27 05:05:09 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 2.6028
2022-02-27 05:05:43 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 2.3970
2022-02-27 05:06:16 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 2.7475
2022-02-27 05:06:49 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 2.7678
2022-02-27 05:07:22 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 2.2442
2022-02-27 05:07:56 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 2.8071
2022-02-27 05:08:29 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 2.6029
2022-02-27 05:09:02 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 2.4696
2022-02-27 05:09:37 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 2.6458
2022-02-27 05:10:10 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 2.3362
2022-02-27 05:10:44 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 2.7657
2022-02-27 05:11:17 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 2.3364
2022-02-27 05:11:51 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 2.5697
2022-02-27 05:12:24 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 2.8826
2022-02-27 05:12:58 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 2.7118
2022-02-27 05:13:30 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 2.6144
2022-02-27 05:14:04 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 2.9261
2022-02-27 05:14:38 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 2.7454
2022-02-27 05:15:11 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 2.7946
2022-02-27 05:15:44 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 2.5053
2022-02-27 05:15:45 - train: epoch 035, train_loss: 2.6110
2022-02-27 05:16:59 - eval: epoch: 035, acc1: 50.668%, acc5: 74.670%, test_loss: 2.2444, per_image_load_time: 1.989ms, per_image_inference_time: 0.111ms
2022-02-27 05:16:59 - until epoch: 035, best_acc1: 50.668%
2022-02-27 05:16:59 - epoch 036 lr: 0.010000000000000002
2022-02-27 05:17:37 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 2.6783
2022-02-27 05:18:10 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 2.5070
2022-02-27 05:18:43 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 2.3501
2022-02-27 05:19:16 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 2.6577
2022-02-27 05:19:49 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 2.5870
2022-02-27 05:20:23 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 2.5536
2022-02-27 05:20:54 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 2.5937
2022-02-27 05:21:28 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 2.3737
2022-02-27 05:22:01 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 2.5991
2022-02-27 05:22:35 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 2.4399
2022-02-27 05:23:06 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 2.5723
2022-02-27 05:23:39 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 2.6830
2022-02-27 05:24:12 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 2.5941
2022-02-27 05:24:46 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 2.7769
2022-02-27 05:25:19 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 2.6887
2022-02-27 05:25:53 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 2.6672
2022-02-27 05:26:25 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 2.6224
2022-02-27 05:26:59 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 2.3508
2022-02-27 05:27:31 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 2.6513
2022-02-27 05:28:06 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 2.4752
2022-02-27 05:28:40 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 2.5549
2022-02-27 05:29:12 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 2.7997
2022-02-27 05:29:45 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 2.6707
2022-02-27 05:30:18 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 2.7175
2022-02-27 05:30:52 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 2.4199
2022-02-27 05:31:26 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 2.6312
2022-02-27 05:31:58 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 2.4706
2022-02-27 05:32:31 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 2.5023
2022-02-27 05:33:05 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 2.5955
2022-02-27 05:33:38 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 2.6602
2022-02-27 05:34:11 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 2.6594
2022-02-27 05:34:45 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 2.4857
2022-02-27 05:35:19 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 2.6372
2022-02-27 05:35:51 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 2.5283
2022-02-27 05:36:25 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 2.7510
2022-02-27 05:36:58 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 2.8399
2022-02-27 05:37:32 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 2.6066
2022-02-27 05:38:05 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 2.7567
2022-02-27 05:38:39 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 2.5460
2022-02-27 05:39:11 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 2.7404
2022-02-27 05:39:45 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 2.6372
2022-02-27 05:40:17 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 2.5857
2022-02-27 05:40:52 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 2.5370
2022-02-27 05:41:23 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 2.5710
2022-02-27 05:41:57 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 2.3741
2022-02-27 05:42:30 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 2.4840
2022-02-27 05:43:03 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 2.7087
2022-02-27 05:43:38 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 2.6555
2022-02-27 05:44:10 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 2.5140
2022-02-27 05:44:43 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 2.5962
2022-02-27 05:44:44 - train: epoch 036, train_loss: 2.6057
2022-02-27 05:46:01 - eval: epoch: 036, acc1: 50.660%, acc5: 74.470%, test_loss: 2.2403, per_image_load_time: 2.589ms, per_image_inference_time: 0.100ms
2022-02-27 05:46:01 - until epoch: 036, best_acc1: 50.668%
2022-02-27 05:46:01 - epoch 037 lr: 0.010000000000000002
2022-02-27 05:46:41 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 2.4765
2022-02-27 05:47:16 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 2.3375
2022-02-27 05:47:49 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 2.6247
2022-02-27 05:48:24 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 2.3707
2022-02-27 05:48:57 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 2.6799
2022-02-27 05:49:31 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 2.7601
2022-02-27 05:50:03 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 2.5557
2022-02-27 05:50:36 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 2.7014
2022-02-27 05:51:09 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.8304
2022-02-27 05:51:43 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 2.2969
2022-02-27 05:52:16 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 2.9183
2022-02-27 05:52:50 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 2.6927
2022-02-27 05:53:22 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 2.7620
2022-02-27 05:53:56 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 2.9201
2022-02-27 05:54:29 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 2.2359
2022-02-27 05:55:02 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 2.3351
2022-02-27 05:55:35 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 2.6578
2022-02-27 05:56:09 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 2.7066
2022-02-27 05:56:41 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 2.5639
2022-02-27 05:57:15 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 2.5008
2022-02-27 05:57:47 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 2.6217
2022-02-27 05:58:22 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 2.4741
2022-02-27 05:58:54 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 2.4541
2022-02-27 05:59:28 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 2.6147
2022-02-27 06:00:01 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 2.7068
2022-02-27 06:00:35 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 2.7551
2022-02-27 06:01:07 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 2.8058
2022-02-27 06:01:41 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 2.8087
2022-02-27 06:02:14 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 2.5788
2022-02-27 06:02:47 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 2.6900
2022-02-27 06:03:20 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 2.4421
2022-02-27 06:03:54 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 2.5073
2022-02-27 06:04:27 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 2.5684
2022-02-27 06:05:00 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 2.4884
2022-02-27 06:05:34 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 2.6652
2022-02-27 06:06:07 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 2.5916
2022-02-27 06:06:40 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 2.5911
2022-02-27 06:07:14 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 2.5952
2022-02-27 06:07:47 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 2.6211
2022-02-27 06:08:21 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 2.6749
2022-02-27 06:08:54 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 2.5471
2022-02-27 06:09:27 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 2.7914
2022-02-27 06:10:00 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 2.6300
2022-02-27 06:10:34 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 2.5683
2022-02-27 06:11:05 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 2.6663
2022-02-27 06:11:40 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 2.6598
2022-02-27 06:12:13 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 2.7296
2022-02-27 06:12:45 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 2.5039
2022-02-27 06:13:19 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 2.4599
2022-02-27 06:13:51 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 2.4533
2022-02-27 06:13:52 - train: epoch 037, train_loss: 2.6052
2022-02-27 06:15:07 - eval: epoch: 037, acc1: 50.650%, acc5: 74.440%, test_loss: 2.2440, per_image_load_time: 1.795ms, per_image_inference_time: 0.113ms
2022-02-27 06:15:07 - until epoch: 037, best_acc1: 50.668%
2022-02-27 06:15:07 - epoch 038 lr: 0.010000000000000002
2022-02-27 06:15:45 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 2.5605
2022-02-27 06:16:18 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 2.6346
2022-02-27 06:16:51 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 2.5178
2022-02-27 06:17:24 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 2.5217
2022-02-27 06:17:57 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 2.6103
2022-02-27 06:18:30 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 3.0588
2022-02-27 06:19:03 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 2.4939
2022-02-27 06:19:37 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 2.5902
2022-02-27 06:20:08 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 2.6883
2022-02-27 06:20:42 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 2.6576
2022-02-27 06:21:15 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 2.6026
2022-02-27 06:21:49 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 2.7668
2022-02-27 06:22:22 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 2.6213
2022-02-27 06:22:55 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 2.3926
2022-02-27 06:23:28 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 2.3707
2022-02-27 06:24:02 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 2.8617
2022-02-27 06:24:34 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 2.8284
2022-02-27 06:25:07 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 2.6043
2022-02-27 06:25:40 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 2.5641
2022-02-27 06:26:13 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 2.5266
2022-02-27 06:26:47 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 2.5160
2022-02-27 06:27:19 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 2.5647
2022-02-27 06:27:53 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 2.9420
2022-02-27 06:28:26 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 2.7721
2022-02-27 06:29:00 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 2.5561
2022-02-27 06:29:32 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 2.6165
2022-02-27 06:30:07 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 2.4583
2022-02-27 06:30:39 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 2.8193
2022-02-27 06:31:14 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 2.5688
2022-02-27 06:31:46 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 2.5960
2022-02-27 06:32:20 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 2.5214
2022-02-27 06:32:52 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 2.3517
2022-02-27 06:33:26 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 2.4558
2022-02-27 06:33:59 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 2.4890
2022-02-27 06:34:33 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 2.7446
2022-02-27 06:35:05 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 2.6710
2022-02-27 06:35:40 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 2.3852
2022-02-27 06:36:12 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 2.8735
2022-02-27 06:36:48 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 2.5349
2022-02-27 06:37:22 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 2.7572
2022-02-27 06:37:55 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 2.4644
2022-02-27 06:38:29 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 2.4733
2022-02-27 06:39:03 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 2.7201
2022-02-27 06:39:36 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 2.6642
2022-02-27 06:40:09 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 2.7996
2022-02-27 06:40:42 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 2.5491
2022-02-27 06:41:15 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 2.3684
2022-02-27 06:41:48 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 2.6038
2022-02-27 06:42:21 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 2.4793
2022-02-27 06:42:52 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 2.4266
2022-02-27 06:42:53 - train: epoch 038, train_loss: 2.6056
2022-02-27 06:44:08 - eval: epoch: 038, acc1: 50.232%, acc5: 74.316%, test_loss: 2.2552, per_image_load_time: 2.823ms, per_image_inference_time: 0.097ms
2022-02-27 06:44:08 - until epoch: 038, best_acc1: 50.668%
2022-02-27 11:56:18 - epoch 039 lr: 0.010000000000000002
2022-02-27 11:56:56 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 2.6318
2022-02-27 11:57:31 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 2.5195
2022-02-27 11:58:03 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 2.4317
2022-02-27 11:58:36 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 2.4660
2022-02-27 11:59:09 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 2.6477
2022-02-27 11:59:42 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 2.4847
2022-02-27 12:00:17 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 2.7511
2022-02-27 12:00:51 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 2.5562
2022-02-27 12:01:23 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 2.6664
2022-02-27 12:01:57 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 2.5378
2022-02-27 12:02:30 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 2.7547
2022-02-27 12:03:04 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 2.6122
2022-02-27 12:03:37 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 2.6795
2022-02-27 12:04:11 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 2.7046
2022-02-27 12:04:43 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 2.6169
2022-02-27 12:05:17 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 2.5972
2022-02-27 12:05:50 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 2.4775
2022-02-27 12:06:25 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 2.4993
2022-02-27 12:06:58 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 2.2682
2022-02-27 12:07:31 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 2.5896
2022-02-27 12:08:04 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 2.7697
2022-02-27 12:08:38 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 2.4283
2022-02-27 12:09:10 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 2.8190
2022-02-27 12:09:45 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 2.7023
2022-02-27 12:10:17 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 2.5688
2022-02-27 12:10:51 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 2.6898
2022-02-27 12:11:24 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 2.7120
2022-02-27 12:11:59 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 2.5871
2022-02-27 12:12:31 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 2.3652
2022-02-27 12:13:05 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 2.8027
2022-02-27 12:13:37 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 2.5707
2022-02-27 12:14:12 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 2.6375
2022-02-27 12:14:44 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 2.6232
2022-02-27 12:15:18 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 2.7313
2022-02-27 12:15:50 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 2.7777
2022-02-27 12:16:24 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 2.7428
2022-02-27 12:16:58 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 2.3937
2022-02-27 12:17:31 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 2.4500
2022-02-27 12:18:03 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 2.7764
2022-02-27 12:18:37 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 2.5978
2022-02-27 12:19:10 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 2.7788
2022-02-27 12:19:44 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 2.6171
2022-02-27 12:20:17 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 2.5033
2022-02-27 12:20:50 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 2.4929
2022-02-27 12:21:24 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 2.6646
2022-02-27 12:21:58 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 2.6820
2022-02-27 12:22:32 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 2.7578
2022-02-27 12:23:06 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 2.4630
2022-02-27 12:23:40 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 2.5550
2022-02-27 12:24:11 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 2.5733
2022-02-27 12:24:12 - train: epoch 039, train_loss: 2.6087
2022-02-27 12:25:26 - eval: epoch: 039, acc1: 49.972%, acc5: 73.984%, test_loss: 2.2644, per_image_load_time: 2.760ms, per_image_inference_time: 0.128ms
2022-02-27 12:25:26 - until epoch: 039, best_acc1: 50.668%
2022-02-27 12:25:26 - epoch 040 lr: 0.010000000000000002
2022-02-27 12:26:04 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 2.8602
2022-02-27 12:26:38 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 2.7352
2022-02-27 12:27:11 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 2.7090
2022-02-27 12:27:43 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 2.4049
2022-02-27 12:28:16 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 2.4998
2022-02-27 12:28:49 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 2.4704
2022-02-27 12:29:22 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 2.7035
2022-02-27 12:29:55 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 2.8214
2022-02-27 12:30:27 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 2.5167
2022-02-27 12:31:01 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 2.4673
2022-02-27 12:31:34 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 2.4020
2022-02-27 12:32:08 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 2.6566
2022-02-27 12:32:40 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 2.5012
2022-02-27 12:33:14 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 2.5755
2022-02-27 12:33:45 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 2.5116
2022-02-27 12:34:20 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 2.8528
2022-02-27 12:34:51 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 2.5837
2022-02-27 12:35:25 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 2.5802
2022-02-27 12:35:57 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 2.6410
2022-02-27 12:36:31 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 2.6688
2022-02-27 12:37:03 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 2.2668
2022-02-27 12:37:37 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 2.3920
2022-02-27 12:38:09 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 2.5896
2022-02-27 12:38:42 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 2.6254
2022-02-27 12:39:15 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 2.7212
2022-02-27 12:39:49 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 2.7114
2022-02-27 12:40:21 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 2.7336
2022-02-27 12:40:55 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 2.6751
2022-02-27 12:41:27 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 2.7091
2022-02-27 12:42:01 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 2.4747
2022-02-27 12:42:33 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 2.7235
2022-02-27 12:43:08 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 2.7156
2022-02-27 12:43:39 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 2.6219
2022-02-27 12:44:13 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 2.5510
2022-02-27 12:44:45 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 2.6978
2022-02-27 12:45:18 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 2.6161
2022-02-27 12:45:50 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 2.4559
2022-02-27 12:46:24 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 2.6345
2022-02-27 12:46:56 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 2.5476
2022-02-27 12:47:31 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 2.5379
2022-02-27 12:48:03 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 2.3771
2022-02-27 12:48:38 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 2.6122
2022-02-27 12:49:10 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 2.4905
2022-02-27 12:49:43 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 2.3894
2022-02-27 12:50:16 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 2.5042
2022-02-27 12:50:51 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 2.6828
2022-02-27 12:51:24 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 2.5742
2022-02-27 12:51:58 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 2.5477
2022-02-27 12:52:31 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 2.6501
2022-02-27 12:53:03 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 2.4749
2022-02-27 12:53:04 - train: epoch 040, train_loss: 2.6101
2022-02-27 12:54:18 - eval: epoch: 040, acc1: 50.470%, acc5: 74.198%, test_loss: 2.2623, per_image_load_time: 2.756ms, per_image_inference_time: 0.140ms
2022-02-27 12:54:18 - until epoch: 040, best_acc1: 50.668%
2022-02-27 12:54:18 - epoch 041 lr: 0.010000000000000002
2022-02-27 12:54:56 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 2.6804
2022-02-27 12:55:30 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 2.8625
2022-02-27 12:56:03 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 2.6348
2022-02-27 12:56:34 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 2.6714
2022-02-27 12:57:08 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 2.3503
2022-02-27 12:57:41 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 2.7272
2022-02-27 12:58:14 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 2.5651
2022-02-27 12:58:46 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 2.3643
2022-02-27 12:59:21 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 2.3086
2022-02-27 12:59:54 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 2.7726
2022-02-27 13:00:28 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 2.7091
2022-02-27 13:01:00 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 2.5225
2022-02-27 13:01:34 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 2.5234
2022-02-27 13:02:08 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 2.5980
2022-02-27 13:02:40 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 2.9597
2022-02-27 13:03:13 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 2.6286
2022-02-27 13:03:47 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 2.6691
2022-02-27 13:04:21 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 2.4698
2022-02-27 13:04:54 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 2.6509
2022-02-27 13:05:28 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 2.4667
2022-02-27 13:06:01 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 2.4133
2022-02-27 13:06:35 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 2.4516
2022-02-27 13:07:07 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 2.5177
2022-02-27 13:07:40 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 2.6507
2022-02-27 13:08:13 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 2.6263
2022-02-27 13:08:46 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 2.7805
2022-02-27 13:09:19 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 2.6700
2022-02-27 13:09:53 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 2.5966
2022-02-27 13:10:25 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 2.6658
2022-02-27 13:10:59 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 2.7055
2022-02-27 13:11:31 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 2.5671
2022-02-27 13:12:04 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 2.6781
2022-02-27 13:12:37 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 2.6350
2022-02-27 13:13:11 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 2.3552
2022-02-27 13:13:44 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 2.4918
2022-02-27 13:14:17 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 2.7496
2022-02-27 13:14:52 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 2.7905
2022-02-27 13:15:24 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 2.7485
2022-02-27 13:15:58 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 2.6312
2022-02-27 13:16:29 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 2.4644
2022-02-27 13:17:03 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 2.6167
2022-02-27 13:17:35 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 2.5770
2022-02-27 13:18:09 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 2.3821
2022-02-27 13:18:40 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 2.4122
2022-02-27 13:19:13 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 2.5740
2022-02-27 13:19:46 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 2.6440
2022-02-27 13:20:20 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 2.6199
2022-02-27 13:20:53 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 2.5752
2022-02-27 13:21:26 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 2.6730
2022-02-27 13:21:59 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 2.6554
2022-02-27 13:22:00 - train: epoch 041, train_loss: 2.6104
2022-02-27 13:23:14 - eval: epoch: 041, acc1: 50.174%, acc5: 74.166%, test_loss: 2.2607, per_image_load_time: 2.771ms, per_image_inference_time: 0.131ms
2022-02-27 13:23:14 - until epoch: 041, best_acc1: 50.668%
2022-02-27 13:23:14 - epoch 042 lr: 0.010000000000000002
2022-02-27 13:23:51 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 2.3165
2022-02-27 13:24:24 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 2.5888
2022-02-27 13:24:58 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 2.6837
2022-02-27 13:25:31 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 2.3534
2022-02-27 13:26:03 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 2.7028
2022-02-27 13:26:37 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 2.4297
2022-02-27 13:27:10 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 2.7982
2022-02-27 13:27:43 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 2.6822
2022-02-27 13:28:14 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 2.6265
2022-02-27 13:28:49 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 2.8795
2022-02-27 13:29:22 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 2.3617
2022-02-27 13:29:55 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 2.4515
2022-02-27 13:30:28 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 2.4659
2022-02-27 13:31:02 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 2.8360
2022-02-27 13:31:34 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 2.4444
2022-02-27 13:32:08 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 2.6430
2022-02-27 13:32:41 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 2.6113
2022-02-27 13:33:14 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 2.5468
2022-02-27 13:33:48 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 2.7797
2022-02-27 13:34:21 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 2.7745
2022-02-27 13:34:55 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 2.3163
2022-02-27 13:35:27 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 2.6601
2022-02-27 13:36:01 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 2.5935
2022-02-27 13:36:34 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 2.5817
2022-02-27 13:37:08 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 2.7568
2022-02-27 13:37:41 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 2.6429
2022-02-27 13:38:14 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 2.4581
2022-02-27 13:38:48 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 2.6038
2022-02-27 13:39:22 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 2.6982
2022-02-27 13:39:54 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 2.5858
2022-02-27 13:40:29 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 2.6741
2022-02-27 13:41:00 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 2.8477
2022-02-27 13:41:35 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 2.7670
2022-02-27 13:42:09 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 2.2449
2022-02-27 13:42:42 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 2.7142
2022-02-27 13:43:15 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 2.5422
2022-02-27 13:43:48 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 2.5342
2022-02-27 13:44:22 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 2.3627
2022-02-27 13:44:56 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 2.5884
2022-02-27 13:45:30 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 2.3446
2022-02-27 13:46:02 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 2.7099
2022-02-27 13:46:37 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 2.6548
2022-02-27 13:47:10 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 2.4877
2022-02-27 13:47:44 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 2.5590
2022-02-27 13:48:17 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 2.4592
2022-02-27 13:48:52 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 2.6757
2022-02-27 13:49:24 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 2.4844
2022-02-27 13:49:58 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 2.6108
2022-02-27 13:50:32 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 2.4426
2022-02-27 13:51:05 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 2.7236
2022-02-27 13:51:05 - train: epoch 042, train_loss: 2.6144
2022-02-27 13:52:20 - eval: epoch: 042, acc1: 50.180%, acc5: 74.036%, test_loss: 2.2714, per_image_load_time: 2.727ms, per_image_inference_time: 0.133ms
2022-02-27 13:52:20 - until epoch: 042, best_acc1: 50.668%
2022-02-27 13:52:20 - epoch 043 lr: 0.010000000000000002
2022-02-27 13:52:59 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 2.7176
2022-02-27 13:53:33 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 2.9454
2022-02-27 13:54:06 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 2.4413
2022-02-27 13:54:39 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 2.5932
2022-02-27 13:55:13 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 2.4167
2022-02-27 13:55:47 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 2.6837
2022-02-27 13:56:20 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 2.7099
2022-02-27 13:56:54 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 2.7449
2022-02-27 13:57:26 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 2.5394
2022-02-27 13:58:00 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 2.6184
2022-02-27 13:58:34 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 2.7297
2022-02-27 13:59:07 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 2.4671
2022-02-27 13:59:41 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 2.7150
2022-02-27 14:00:14 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 2.5882
2022-02-27 14:00:48 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 2.4263
2022-02-27 14:01:21 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 2.5758
2022-02-27 14:01:54 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 2.6620
2022-02-27 14:02:28 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 2.9931
2022-02-27 14:03:02 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 2.8033
2022-02-27 14:03:35 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 2.3664
2022-02-27 14:04:08 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 2.3596
2022-02-27 14:04:40 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 2.7377
2022-02-27 14:05:14 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 2.7844
2022-02-27 14:05:46 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 2.2961
2022-02-27 14:06:20 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 3.0618
2022-02-27 14:06:52 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 2.5062
2022-02-27 14:07:25 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 2.7076
2022-02-27 14:07:58 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 2.2608
2022-02-27 14:08:32 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 2.7416
2022-02-27 14:09:05 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 2.7900
2022-02-27 14:09:38 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 2.6626
2022-02-27 14:10:11 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 2.6978
2022-02-27 14:10:43 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 2.5964
2022-02-27 14:11:18 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 2.6076
2022-02-27 14:11:50 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 2.6097
2022-02-27 14:12:23 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 2.7232
2022-02-27 14:12:56 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 2.5172
2022-02-27 14:13:29 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 2.8282
2022-02-27 14:14:02 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 2.5747
2022-02-27 14:14:35 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 2.6630
2022-02-27 14:15:09 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 2.7411
2022-02-27 14:15:42 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 2.6465
2022-02-27 14:16:15 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 2.6595
2022-02-27 14:16:49 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 2.7977
2022-02-27 14:17:22 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 2.5539
2022-02-27 14:17:55 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 2.7526
2022-02-27 14:18:29 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 2.7342
2022-02-27 14:19:02 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 2.4758
2022-02-27 14:19:36 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 2.5555
2022-02-27 14:20:08 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 2.6174
2022-02-27 14:20:09 - train: epoch 043, train_loss: 2.6160
2022-02-27 14:21:25 - eval: epoch: 043, acc1: 50.092%, acc5: 74.232%, test_loss: 2.2640, per_image_load_time: 2.856ms, per_image_inference_time: 0.117ms
2022-02-27 14:21:25 - until epoch: 043, best_acc1: 50.668%
2022-02-27 14:21:25 - epoch 044 lr: 0.010000000000000002
2022-02-27 14:22:04 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 2.6500
2022-02-27 14:22:37 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 2.7300
2022-02-27 14:23:11 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 2.5011
2022-02-27 14:23:43 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 2.2980
2022-02-27 14:24:17 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 2.6033
2022-02-27 14:24:49 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 2.2592
2022-02-27 14:25:24 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 2.5905
2022-02-27 14:25:57 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 2.8327
2022-02-27 14:26:30 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 2.6064
2022-02-27 14:27:03 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 2.6187
2022-02-27 14:27:37 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 2.4277
2022-02-27 14:28:10 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 2.5730
2022-02-27 14:28:43 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 2.3816
2022-02-27 14:29:17 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 2.4757
2022-02-27 14:29:49 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 2.3738
2022-02-27 14:30:23 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 2.5189
2022-02-27 14:30:55 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 2.6028
2022-02-27 14:31:29 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 2.5501
2022-02-27 14:32:01 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 2.7259
2022-02-27 14:32:35 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 2.4207
2022-02-27 14:33:09 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 2.9824
2022-02-27 14:33:42 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 2.7019
2022-02-27 14:34:16 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 2.6800
2022-02-27 14:34:48 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 2.8473
2022-02-27 14:35:21 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 2.6521
2022-02-27 14:35:54 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 2.9115
2022-02-27 14:36:28 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 2.4312
2022-02-27 14:37:01 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 2.4564
2022-02-27 14:37:35 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 2.3831
2022-02-27 14:38:07 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 2.4551
2022-02-27 14:38:40 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 2.7474
2022-02-27 14:39:14 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 2.4832
2022-02-27 14:39:47 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 2.6578
2022-02-27 14:40:21 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 2.6823
2022-02-27 14:40:55 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 2.3856
2022-02-27 14:41:27 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 2.7153
2022-02-27 14:42:01 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 2.6422
2022-02-27 14:42:34 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 2.2985
2022-02-27 14:43:08 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 2.4892
2022-02-27 14:43:41 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 2.6559
2022-02-27 14:44:15 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 2.4797
2022-02-27 14:44:48 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 2.6915
2022-02-27 14:45:22 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 2.7462
2022-02-27 14:45:54 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 2.7552
2022-02-27 14:46:29 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 2.7431
2022-02-27 14:47:02 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 2.6441
2022-02-27 14:47:36 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 2.7737
2022-02-27 14:48:08 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 2.6508
2022-02-27 14:48:43 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 2.4169
2022-02-27 14:49:15 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 2.5515
2022-02-27 14:49:16 - train: epoch 044, train_loss: 2.6186
2022-02-27 14:50:32 - eval: epoch: 044, acc1: 50.296%, acc5: 74.132%, test_loss: 2.2647, per_image_load_time: 2.815ms, per_image_inference_time: 0.120ms
2022-02-27 14:50:32 - until epoch: 044, best_acc1: 50.668%
2022-02-27 14:50:32 - epoch 045 lr: 0.010000000000000002
2022-02-27 14:51:10 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 2.6644
2022-02-27 14:51:44 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 2.5658
2022-02-27 14:52:17 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 2.5353
2022-02-27 14:52:50 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 2.6114
2022-02-27 14:53:22 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 2.7639
2022-02-27 14:53:56 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 2.8239
2022-02-27 14:54:28 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 2.5427
2022-02-27 14:55:02 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 2.5356
2022-02-27 14:55:34 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 2.4693
2022-02-27 14:56:08 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 2.5732
2022-02-27 14:56:41 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 2.5758
2022-02-27 14:57:14 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 2.7573
2022-02-27 14:57:47 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 2.5275
2022-02-27 14:58:21 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 2.7371
2022-02-27 14:58:53 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 2.6166
2022-02-27 14:59:26 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 2.4402
2022-02-27 15:00:00 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 2.5330
2022-02-27 15:00:33 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 2.6548
2022-02-27 15:01:07 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 2.6126
2022-02-27 15:01:39 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 2.6944
2022-02-27 15:02:13 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 2.6802
2022-02-27 15:02:46 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 2.7170
2022-02-27 15:03:19 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 2.3758
2022-02-27 15:03:52 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 2.6456
2022-02-27 15:04:26 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 2.5873
2022-02-27 15:04:58 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 2.5650
2022-02-27 15:05:32 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 2.4489
2022-02-27 15:06:05 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 2.3641
2022-02-27 15:06:38 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 2.8659
2022-02-27 15:07:11 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 2.7234
2022-02-27 15:07:45 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 2.5098
2022-02-27 15:08:17 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 2.9085
2022-02-27 15:08:51 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 2.7576
2022-02-27 15:09:24 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 2.4364
2022-02-27 15:09:57 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 2.6768
2022-02-27 15:10:32 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 2.7021
2022-02-27 15:11:04 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 2.5181
2022-02-27 15:11:38 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 2.5876
2022-02-27 15:12:10 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 2.5835
2022-02-27 15:12:43 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 2.9379
2022-02-27 15:13:17 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 2.6349
2022-02-27 15:13:51 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 2.7630
2022-02-27 15:14:23 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 2.7512
2022-02-27 15:14:56 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 2.8784
2022-02-27 15:15:29 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 2.4001
2022-02-27 15:16:03 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 2.6811
2022-02-27 15:16:36 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 2.7098
2022-02-27 15:17:10 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 2.4421
2022-02-27 15:17:43 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 2.6736
2022-02-27 15:18:15 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 2.4222
2022-02-27 15:18:16 - train: epoch 045, train_loss: 2.6166
2022-02-27 15:19:30 - eval: epoch: 045, acc1: 49.994%, acc5: 74.022%, test_loss: 2.2742, per_image_load_time: 2.638ms, per_image_inference_time: 0.125ms
2022-02-27 15:19:30 - until epoch: 045, best_acc1: 50.668%
2022-02-27 15:19:30 - epoch 046 lr: 0.010000000000000002
2022-02-27 15:20:09 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 2.3794
2022-02-27 15:20:42 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 2.4929
2022-02-27 15:21:15 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 2.4314
2022-02-27 15:21:48 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 2.6398
2022-02-27 15:22:21 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 2.5457
2022-02-27 15:22:53 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 2.6514
2022-02-27 15:23:27 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 2.5958
2022-02-27 15:24:01 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 2.7813
2022-02-27 15:24:33 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 2.4449
2022-02-27 15:25:06 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 2.6237
2022-02-27 15:25:38 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 2.5329
2022-02-27 15:26:13 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 2.4296
2022-02-27 15:26:45 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 2.8617
2022-02-27 15:27:19 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 2.5262
2022-02-27 15:27:51 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 2.5953
2022-02-27 15:28:25 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 2.6904
2022-02-27 15:28:57 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 2.5928
2022-02-27 15:29:31 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 2.8820
2022-02-27 15:30:04 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 2.3763
2022-02-27 15:30:38 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 2.3898
2022-02-27 15:31:10 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 2.6435
2022-02-27 15:31:44 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 2.3307
2022-02-27 15:32:17 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 2.7577
2022-02-27 15:32:50 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 2.6464
2022-02-27 15:33:23 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 2.5792
2022-02-27 15:33:56 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 2.8658
2022-02-27 15:34:30 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 2.5953
2022-02-27 15:35:03 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 2.6729
2022-02-27 15:35:37 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 2.5425
2022-02-27 15:36:09 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 2.3995
2022-02-27 15:36:43 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 2.3095
2022-02-27 15:37:16 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 2.4796
2022-02-27 15:37:51 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 2.7957
2022-02-27 15:38:23 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 2.5203
2022-02-27 15:38:58 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 2.8144
2022-02-27 15:39:31 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 2.6958
2022-02-27 15:40:06 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 2.5499
2022-02-27 15:40:38 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 2.4804
2022-02-27 15:41:12 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 2.3969
2022-02-27 15:41:47 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 2.5019
2022-02-27 15:42:19 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 2.7982
2022-02-27 15:42:53 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 2.5034
2022-02-27 15:43:26 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 2.6401
2022-02-27 15:44:01 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 2.8829
2022-02-27 15:44:33 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 2.6380
2022-02-27 15:45:08 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 2.6976
2022-02-27 15:45:41 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 2.5338
2022-02-27 15:46:16 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 2.2522
2022-02-27 15:46:48 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 2.6004
2022-02-27 15:47:21 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 2.4840
2022-02-27 15:47:22 - train: epoch 046, train_loss: 2.6179
2022-02-27 15:48:37 - eval: epoch: 046, acc1: 50.018%, acc5: 74.028%, test_loss: 2.2803, per_image_load_time: 2.616ms, per_image_inference_time: 0.128ms
2022-02-27 15:48:37 - until epoch: 046, best_acc1: 50.668%
2022-02-27 15:48:37 - epoch 047 lr: 0.010000000000000002
2022-02-27 15:49:16 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 2.4887
2022-02-27 15:49:48 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 2.7750
2022-02-27 15:50:22 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 2.2126
2022-02-27 15:50:55 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 2.3634
2022-02-27 15:51:28 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 2.6432
2022-02-27 15:52:02 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 2.7233
2022-02-27 15:52:34 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 2.6650
2022-02-27 15:53:09 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 2.5176
2022-02-27 15:53:41 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 2.8043
2022-02-27 15:54:15 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 2.5833
2022-02-27 15:54:48 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 2.9563
2022-02-27 15:55:22 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 2.7019
2022-02-27 15:55:55 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 2.6545
2022-02-27 15:56:27 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 2.6979
2022-02-27 15:57:02 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 2.8331
2022-02-27 15:57:34 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 2.5360
2022-02-27 15:58:09 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 2.4739
2022-02-27 15:58:41 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 2.5816
2022-02-27 15:59:16 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 2.5783
2022-02-27 15:59:49 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 2.6889
2022-02-27 16:00:23 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 2.8613
2022-02-27 16:00:56 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 2.7474
2022-02-27 16:01:30 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 2.7034
2022-02-27 16:02:02 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 2.5109
2022-02-27 16:02:36 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 2.6050
2022-02-27 16:03:09 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 2.6359
2022-02-27 16:03:44 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 2.5124
2022-02-27 16:04:16 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 2.6934
2022-02-27 16:04:50 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 2.5607
2022-02-27 16:05:24 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 2.7504
2022-02-27 16:05:58 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 2.5769
2022-02-27 16:06:30 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 2.8614
2022-02-27 16:07:04 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 2.4420
2022-02-27 16:07:37 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 2.3865
2022-02-27 16:08:10 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 2.7024
2022-02-27 16:08:45 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 2.5674
2022-02-27 16:09:17 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 2.8125
2022-02-27 16:09:50 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 2.7189
2022-02-27 16:10:25 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 2.6027
2022-02-27 16:10:58 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 2.6012
2022-02-27 16:11:31 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 2.5690
2022-02-27 16:12:04 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 2.6219
2022-02-27 16:12:39 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 2.5521
2022-02-27 16:13:12 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 2.5832
2022-02-27 16:13:46 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 2.5709
2022-02-27 16:14:19 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 2.5329
2022-02-27 16:14:52 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 2.8820
2022-02-27 16:15:26 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 2.5060
2022-02-27 16:15:59 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 2.7977
2022-02-27 16:16:33 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 2.7062
2022-02-27 16:16:34 - train: epoch 047, train_loss: 2.6176
2022-02-27 16:17:49 - eval: epoch: 047, acc1: 49.770%, acc5: 73.896%, test_loss: 2.2756, per_image_load_time: 2.786ms, per_image_inference_time: 0.120ms
2022-02-27 16:17:49 - until epoch: 047, best_acc1: 50.668%
2022-02-27 16:17:49 - epoch 048 lr: 0.010000000000000002
2022-02-27 16:18:28 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 2.7795
2022-02-27 16:19:02 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.8811
2022-02-27 16:19:36 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 2.5467
2022-02-27 16:20:08 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 2.6363
2022-02-27 16:20:41 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 2.5833
2022-02-27 16:21:15 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 2.6904
2022-02-27 16:21:48 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 2.5165
2022-02-27 16:22:22 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 2.5399
2022-02-27 16:22:54 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 2.7035
2022-02-27 16:23:29 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 2.7738
2022-02-27 16:24:02 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 2.8021
2022-02-27 16:24:34 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 2.7050
2022-02-27 16:25:08 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 2.4244
2022-02-27 16:25:41 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 2.6795
2022-02-27 16:26:15 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 2.6090
2022-02-27 16:26:48 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 2.6700
2022-02-27 16:27:22 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 2.6760
2022-02-27 16:27:55 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 2.6128
2022-02-27 16:28:29 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 2.5635
2022-02-27 16:29:02 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 2.7824
2022-02-27 16:29:36 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 2.5239
2022-02-27 16:30:09 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 2.7647
2022-02-27 16:30:43 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 2.5411
2022-02-27 16:31:16 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 2.6146
2022-02-27 16:31:50 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 2.6452
2022-02-27 16:32:23 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 2.5837
2022-02-27 16:32:57 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 2.6333
2022-02-27 16:33:31 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 2.7573
2022-02-27 16:34:06 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 2.5617
2022-02-27 16:34:39 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 2.6227
2022-02-27 16:35:14 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 3.0159
2022-02-27 16:35:47 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 2.2654
2022-02-27 16:36:21 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 2.7055
2022-02-27 16:36:54 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 2.8936
2022-02-27 16:37:28 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 2.6813
2022-02-27 16:38:01 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 2.4486
2022-02-27 16:38:36 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 2.4842
2022-02-27 16:39:09 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 2.6101
2022-02-27 16:39:43 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 2.6706
2022-02-27 16:40:16 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 2.5214
2022-02-27 16:40:50 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 2.6752
2022-02-27 16:41:22 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 2.5845
2022-02-27 16:41:57 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 2.5622
2022-02-27 16:42:29 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 2.5831
2022-02-27 16:43:03 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 2.6535
2022-02-27 16:43:36 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 2.7092
2022-02-27 16:44:09 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 2.8217
2022-02-27 16:44:42 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 2.8514
2022-02-27 16:45:16 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 2.5579
2022-02-27 16:45:49 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 2.5798
2022-02-27 16:45:50 - train: epoch 048, train_loss: 2.6193
2022-02-27 16:47:06 - eval: epoch: 048, acc1: 49.786%, acc5: 73.624%, test_loss: 2.2831, per_image_load_time: 2.829ms, per_image_inference_time: 0.126ms
2022-02-27 16:47:06 - until epoch: 048, best_acc1: 50.668%
2022-02-27 16:47:06 - epoch 049 lr: 0.010000000000000002
2022-02-27 16:47:45 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 2.6097
2022-02-27 16:48:19 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 2.3712
2022-02-27 16:48:52 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 2.6845
2022-02-27 16:49:25 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 2.7215
2022-02-27 16:49:58 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 2.6052
2022-02-27 16:50:32 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 2.5806
2022-02-27 16:51:05 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 2.6435
2022-02-27 16:51:39 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 2.7707
2022-02-27 16:52:11 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 2.6214
2022-02-27 16:52:45 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 2.5561
2022-02-27 16:53:19 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 2.5263
2022-02-27 16:53:52 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 2.5385
2022-02-27 16:54:26 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 2.8653
2022-02-27 16:54:58 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 2.9278
2022-02-27 16:55:32 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 2.4022
2022-02-27 16:56:05 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 2.7707
2022-02-27 16:56:40 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 2.7124
2022-02-27 16:57:12 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 2.5328
2022-02-27 16:57:45 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 2.5045
2022-02-27 16:58:19 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 2.5800
2022-02-27 16:58:52 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 2.4499
2022-02-27 16:59:26 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 2.6299
2022-02-27 16:59:59 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 2.5579
2022-02-27 17:00:33 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 2.8940
2022-02-27 17:01:06 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 2.6585
2022-02-27 17:01:41 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 2.7016
2022-02-27 17:02:13 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 2.7047
2022-02-27 17:02:47 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 2.6035
2022-02-27 17:03:20 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 2.5675
2022-02-27 17:03:54 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 2.7324
2022-02-27 17:04:28 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 2.6450
2022-02-27 17:05:02 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 2.7682
2022-02-27 17:05:34 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 2.4405
2022-02-27 17:06:08 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 2.7580
2022-02-27 17:06:41 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 2.5193
2022-02-27 17:07:14 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 2.8344
2022-02-27 17:07:47 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 2.5394
2022-02-27 17:08:22 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 2.8260
2022-02-27 17:08:55 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 2.6351
2022-02-27 17:09:30 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 2.3820
2022-02-27 17:10:02 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 2.5813
2022-02-27 17:10:36 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 2.6986
2022-02-27 17:11:09 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 2.7435
2022-02-27 17:11:44 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 2.6996
2022-02-27 17:12:17 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 2.4994
2022-02-27 17:12:49 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 2.7939
2022-02-27 17:13:23 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 2.6116
2022-02-27 17:13:58 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 2.5817
2022-02-27 17:14:31 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 2.5203
2022-02-27 17:15:04 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 2.6840
2022-02-27 17:15:04 - train: epoch 049, train_loss: 2.6193
2022-02-27 17:16:21 - eval: epoch: 049, acc1: 49.826%, acc5: 73.872%, test_loss: 2.2858, per_image_load_time: 2.814ms, per_image_inference_time: 0.130ms
2022-02-27 17:16:21 - until epoch: 049, best_acc1: 50.668%
2022-02-27 17:16:21 - epoch 050 lr: 0.010000000000000002
2022-02-27 17:16:59 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 2.7626
2022-02-27 17:17:34 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 2.6652
2022-02-27 17:18:06 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 2.6406
2022-02-27 17:18:40 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 2.5246
2022-02-27 17:19:13 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 2.7312
2022-02-27 17:19:47 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 2.8570
2022-02-27 17:20:20 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 2.3829
2022-02-27 17:20:53 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 2.4151
2022-02-27 17:21:26 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 2.6868
2022-02-27 17:22:00 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 2.7538
2022-02-27 17:22:33 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 2.7979
2022-02-27 17:23:06 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 2.6952
2022-02-27 17:23:39 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 2.5755
2022-02-27 17:24:13 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 2.6798
2022-02-27 17:24:45 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 2.4359
2022-02-27 17:25:20 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 2.5808
2022-02-27 17:25:53 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 2.6565
2022-02-27 17:26:27 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 2.9132
2022-02-27 17:27:00 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 2.6850
2022-02-27 17:27:34 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 2.4508
2022-02-27 17:28:07 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 2.5068
2022-02-27 17:28:41 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 2.7690
2022-02-27 17:29:13 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 2.6023
2022-02-27 17:29:47 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 2.7908
2022-02-27 17:30:21 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 2.5125
2022-02-27 17:30:53 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 2.3906
2022-02-27 17:31:27 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 2.5738
2022-02-27 17:32:00 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 2.7493
2022-02-27 17:32:33 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 2.8923
2022-02-27 17:33:08 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 2.7899
2022-02-27 17:33:40 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 2.8326
2022-02-27 17:34:15 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 2.5687
2022-02-27 17:34:48 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 2.4324
2022-02-27 17:35:21 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 2.4896
2022-02-27 17:35:54 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 2.8237
2022-02-27 17:36:28 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 2.6728
2022-02-27 17:37:00 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 3.0122
2022-02-27 17:37:34 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 2.4342
2022-02-27 17:38:06 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 2.4588
2022-02-27 17:38:40 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 2.5619
2022-02-27 17:39:14 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 2.6448
2022-02-27 17:39:46 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 2.9024
2022-02-27 17:40:20 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 2.6539
2022-02-27 17:40:53 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 2.7410
2022-02-27 17:41:27 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 2.5710
2022-02-27 17:42:00 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 2.6855
2022-02-27 17:42:34 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 2.6626
2022-02-27 17:43:06 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 2.4860
2022-02-27 17:43:41 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 2.4977
2022-02-27 17:44:13 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 2.4581
2022-02-27 17:44:13 - train: epoch 050, train_loss: 2.6193
2022-02-27 17:45:30 - eval: epoch: 050, acc1: 49.556%, acc5: 73.868%, test_loss: 2.2900, per_image_load_time: 2.865ms, per_image_inference_time: 0.119ms
2022-02-27 17:45:30 - until epoch: 050, best_acc1: 50.668%
2022-02-27 17:45:30 - epoch 051 lr: 0.010000000000000002
2022-02-27 17:46:09 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 2.7062
2022-02-27 17:46:42 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 2.7344
2022-02-27 17:47:15 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 2.6835
2022-02-27 17:47:48 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 2.3098
2022-02-27 17:48:22 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 2.6977
2022-02-27 17:48:55 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 2.5542
2022-02-27 17:49:28 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 2.5161
2022-02-27 17:50:00 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 2.9014
2022-02-27 17:50:33 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 2.6586
2022-02-27 17:51:06 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 3.0313
2022-02-27 17:51:39 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 2.6555
2022-02-27 17:52:13 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 2.4957
2022-02-27 17:52:45 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 2.4233
2022-02-27 17:53:18 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 2.5396
2022-02-27 17:53:53 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 2.4880
2022-02-27 17:54:25 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 2.3095
2022-02-27 17:54:58 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 2.6329
2022-02-27 17:55:30 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 2.4932
2022-02-27 17:56:04 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 2.4896
2022-02-27 17:56:37 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 2.5443
2022-02-27 17:57:10 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 2.6483
2022-02-27 17:57:43 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 2.4962
2022-02-27 17:58:16 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 2.7616
2022-02-27 17:58:50 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 2.6926
2022-02-27 17:59:23 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 2.5454
2022-02-27 17:59:57 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 2.6302
2022-02-27 18:00:31 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 2.5498
2022-02-27 18:01:03 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 2.5534
2022-02-27 18:01:38 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 2.6802
2022-02-27 18:02:10 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 2.4348
2022-02-27 18:02:44 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 2.4807
2022-02-27 18:03:16 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 2.5036
2022-02-27 18:03:49 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 2.7669
2022-02-27 18:04:22 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 2.5956
2022-02-27 18:04:56 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 2.4833
2022-02-27 18:05:28 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 2.6187
2022-02-27 18:06:03 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 2.7685
2022-02-27 18:06:35 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 2.5625
2022-02-27 18:07:09 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 2.8447
2022-02-27 18:07:42 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 2.7994
2022-02-27 18:08:16 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 2.5980
2022-02-27 18:08:48 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 2.7324
2022-02-27 18:09:21 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 2.5715
2022-02-27 18:09:55 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 2.7236
2022-02-27 18:10:28 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 2.3078
2022-02-27 18:11:03 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 2.7137
2022-02-27 18:11:35 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 2.8382
2022-02-27 18:12:09 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 2.6500
2022-02-27 18:12:42 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 2.6666
2022-02-27 18:13:15 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 2.5691
2022-02-27 18:13:16 - train: epoch 051, train_loss: 2.6213
2022-02-27 18:14:31 - eval: epoch: 051, acc1: 50.126%, acc5: 74.202%, test_loss: 2.2632, per_image_load_time: 1.094ms, per_image_inference_time: 0.120ms
2022-02-27 18:14:31 - until epoch: 051, best_acc1: 50.668%
2022-02-27 18:14:31 - epoch 052 lr: 0.010000000000000002
2022-02-27 18:15:09 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 2.6167
2022-02-27 18:15:42 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 2.6848
2022-02-27 18:16:16 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 2.5266
2022-02-27 18:16:49 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 2.6318
2022-02-27 18:17:23 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 2.7949
2022-02-27 18:17:56 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 2.5804
2022-02-27 18:18:30 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 2.7303
2022-02-27 18:19:03 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 2.5374
2022-02-27 18:19:36 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 2.5716
2022-02-27 18:20:10 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 2.7926
2022-02-27 18:20:43 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 2.5612
2022-02-27 18:21:17 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 2.4490
2022-02-27 18:21:50 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 2.6480
2022-02-27 18:22:23 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 2.7276
2022-02-27 18:22:57 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 2.4561
2022-02-27 18:23:30 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 2.3141
2022-02-27 18:24:04 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 2.4476
2022-02-27 18:24:37 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 2.3748
2022-02-27 18:25:11 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 2.5035
2022-02-27 18:25:44 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 2.9702
2022-02-27 18:26:17 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 2.6130
2022-02-27 18:26:51 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 2.6955
2022-02-27 18:27:24 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 2.5265
2022-02-27 18:27:58 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 2.4322
2022-02-27 18:28:30 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 2.4720
2022-02-27 18:29:04 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 2.5043
2022-02-27 18:29:36 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 2.6703
2022-02-27 18:30:10 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 2.4535
2022-02-27 18:30:43 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 2.4942
2022-02-27 18:31:17 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 2.5541
2022-02-27 18:31:50 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 2.5884
2022-02-27 18:32:24 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 2.5882
2022-02-27 18:32:58 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 2.5703
2022-02-27 18:33:31 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 2.6036
2022-02-27 18:34:05 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 2.9542
2022-02-27 18:34:38 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 2.7611
2022-02-27 18:35:12 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 2.7427
2022-02-27 18:35:45 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 2.6301
2022-02-27 18:36:19 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 2.4684
2022-02-27 18:36:52 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 2.6110
2022-02-27 18:37:27 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 2.3968
2022-02-27 18:37:59 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 2.6012
2022-02-27 18:38:32 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 2.6533
2022-02-27 18:39:05 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 2.5697
2022-02-27 18:39:39 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 2.7211
2022-02-27 18:40:12 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 2.5625
2022-02-27 18:40:46 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 2.6056
2022-02-27 18:41:19 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 2.3529
2022-02-27 18:41:53 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 2.5509
2022-02-27 18:42:25 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 2.5238
2022-02-27 18:42:26 - train: epoch 052, train_loss: 2.6175
2022-02-27 18:43:42 - eval: epoch: 052, acc1: 49.502%, acc5: 73.630%, test_loss: 2.2983, per_image_load_time: 2.582ms, per_image_inference_time: 0.129ms
2022-02-27 18:43:42 - until epoch: 052, best_acc1: 50.668%
2022-02-27 18:43:42 - epoch 053 lr: 0.010000000000000002
2022-02-27 18:44:19 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 2.5178
2022-02-27 18:44:52 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 2.7850
2022-02-27 18:45:26 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 2.7881
2022-02-27 18:46:00 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 2.8330
2022-02-27 18:46:32 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 2.6465
2022-02-27 18:47:07 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 2.6612
2022-02-27 18:47:38 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 2.5714
2022-02-27 18:48:12 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 2.6362
2022-02-27 18:48:46 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 2.5745
2022-02-27 18:49:18 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 2.4875
2022-02-27 18:49:52 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 2.5956
2022-02-27 18:50:24 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 2.2986
2022-02-27 18:50:58 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 2.6466
2022-02-27 18:51:30 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 2.7644
2022-02-27 18:52:04 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 2.4652
2022-02-27 18:52:37 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 2.9344
2022-02-27 18:53:10 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 2.7903
2022-02-27 18:53:44 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 2.6323
2022-02-27 18:54:17 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 2.6290
2022-02-27 18:54:50 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 2.7774
2022-02-27 18:55:23 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 2.8732
2022-02-27 18:55:57 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 2.7850
2022-02-27 18:56:30 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 2.5765
2022-02-27 18:57:04 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 2.6206
2022-02-27 18:57:36 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 2.7526
2022-02-27 18:58:10 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 2.7108
2022-02-27 18:58:44 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 2.7017
2022-02-27 18:59:17 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 2.7418
2022-02-27 18:59:51 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 2.5116
2022-02-27 19:00:23 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 2.5257
2022-02-27 19:00:57 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 2.8384
2022-02-27 19:01:30 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 2.7439
2022-02-27 19:02:05 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 2.8246
2022-02-27 19:02:38 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 2.7133
2022-02-27 19:03:12 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 2.5906
2022-02-27 19:03:44 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 2.8350
2022-02-27 19:04:19 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 2.8572
2022-02-27 19:04:51 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 2.5709
2022-02-27 19:05:25 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 2.7460
2022-02-27 19:05:58 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 2.7474
2022-02-27 19:06:32 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 2.7947
2022-02-27 19:07:04 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 2.7273
2022-02-27 19:07:37 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 2.6317
2022-02-27 19:08:10 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 2.7232
2022-02-27 19:08:44 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 2.7600
2022-02-27 19:09:17 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 2.6152
2022-02-27 19:09:51 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 2.5806
2022-02-27 19:10:24 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 2.8216
2022-02-27 19:10:57 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 2.7690
2022-02-27 19:11:30 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 2.5648
2022-02-27 19:11:31 - train: epoch 053, train_loss: 2.6165
2022-02-27 19:12:46 - eval: epoch: 053, acc1: 49.848%, acc5: 73.986%, test_loss: 2.2810, per_image_load_time: 2.743ms, per_image_inference_time: 0.116ms
2022-02-27 19:12:46 - until epoch: 053, best_acc1: 50.668%
2022-02-27 19:12:46 - epoch 054 lr: 0.010000000000000002
2022-02-27 19:13:25 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 2.5596
2022-02-27 19:13:58 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 2.7408
2022-02-27 19:14:31 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 2.6359
2022-02-27 19:15:05 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 2.6877
2022-02-27 19:15:38 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 2.8238
2022-02-27 19:16:11 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 2.6359
2022-02-27 19:16:44 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 3.0191
2022-02-27 19:17:18 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 2.8672
2022-02-27 19:17:51 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 2.3260
2022-02-27 19:18:25 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 2.5507
2022-02-27 19:18:58 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 2.4973
2022-02-27 19:19:32 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 2.7593
2022-02-27 19:20:04 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 2.5410
2022-02-27 19:20:38 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 2.7966
2022-02-27 19:21:10 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 2.7665
2022-02-27 19:21:45 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 2.5804
2022-02-27 19:22:17 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 2.5001
2022-02-27 19:22:51 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 2.3763
2022-02-27 19:23:23 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 2.7037
2022-02-27 19:23:56 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 2.8319
2022-02-27 19:24:29 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 2.4762
2022-02-27 19:25:03 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 2.8030
2022-02-27 19:25:35 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 2.3723
2022-02-27 19:26:08 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 2.6994
2022-02-27 19:26:41 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 2.5766
2022-02-27 19:27:13 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 2.6910
2022-02-27 19:27:46 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 2.5981
2022-02-27 19:28:21 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 2.8665
2022-02-27 19:28:54 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 2.4807
2022-02-27 19:29:27 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 2.8992
2022-02-27 19:29:59 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 2.6484
2022-02-27 19:30:33 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 2.7679
2022-02-27 19:31:06 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 2.5934
2022-02-27 19:31:39 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 2.6332
2022-02-27 19:32:12 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 2.7045
2022-02-27 19:32:45 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 2.7497
2022-02-27 19:33:17 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 2.4571
2022-02-27 19:33:49 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 2.4275
2022-02-27 19:34:22 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 2.5656
2022-02-27 19:34:55 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 2.5802
2022-02-27 19:35:27 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 2.5815
2022-02-27 19:36:00 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 2.7210
2022-02-27 19:36:33 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 2.7049
2022-02-27 19:37:07 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 2.4704
2022-02-27 19:37:39 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 2.3887
2022-02-27 19:38:13 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 2.7267
2022-02-27 19:38:47 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 3.0012
2022-02-27 19:39:19 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 2.7371
2022-02-27 19:39:54 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 2.6146
2022-02-27 19:40:26 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 2.5592
2022-02-27 19:40:27 - train: epoch 054, train_loss: 2.6196
2022-02-27 19:41:42 - eval: epoch: 054, acc1: 49.914%, acc5: 73.880%, test_loss: 2.2776, per_image_load_time: 2.276ms, per_image_inference_time: 0.126ms
2022-02-27 19:41:42 - until epoch: 054, best_acc1: 50.668%
2022-02-27 19:41:42 - epoch 055 lr: 0.010000000000000002
2022-02-27 19:42:20 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 2.4105
2022-02-27 19:42:54 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 2.6915
2022-02-27 19:43:26 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 2.4084
2022-02-27 19:43:59 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 2.6485
2022-02-27 19:44:32 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 2.5722
2022-02-27 19:45:04 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 2.6349
2022-02-27 19:45:37 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 2.6376
2022-02-27 19:46:09 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 2.3938
2022-02-27 19:46:42 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 2.5459
2022-02-27 19:47:15 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 2.5117
2022-02-27 19:47:48 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 2.4952
2022-02-27 19:48:20 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 2.6055
2022-02-27 19:48:54 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 2.7290
2022-02-27 19:49:26 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 2.5964
2022-02-27 19:50:01 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 2.5933
2022-02-27 19:50:33 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 2.8916
2022-02-27 19:51:07 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 2.9281
2022-02-27 19:51:38 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 2.7466
2022-02-27 19:52:12 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 2.7736
2022-02-27 19:52:44 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 2.6015
2022-02-27 19:53:18 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 2.4620
2022-02-27 19:53:50 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 2.8801
2022-02-27 19:54:24 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 2.3772
2022-02-27 19:54:56 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 2.5695
2022-02-27 19:55:29 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 2.3842
2022-02-27 19:56:01 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 2.3498
2022-02-27 19:56:35 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 2.5329
2022-02-27 19:57:07 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 2.8157
2022-02-27 19:57:40 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 2.6358
2022-02-27 19:58:13 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 2.6094
2022-02-27 19:58:46 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 2.4710
2022-02-27 19:59:18 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 2.5528
2022-02-27 19:59:52 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 2.4395
2022-02-27 20:00:26 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 2.5927
2022-02-27 20:00:58 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 2.6991
2022-02-27 20:01:33 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 2.6957
2022-02-27 20:02:05 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 2.4759
2022-02-27 20:02:39 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 2.6193
2022-02-27 20:03:12 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 2.9673
2022-02-27 20:03:45 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 2.7371
2022-02-27 20:04:19 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 2.4161
2022-02-27 20:04:52 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 2.8602
2022-02-27 20:05:26 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 2.7827
2022-02-27 20:05:58 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 2.5888
2022-02-27 20:06:31 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 2.6253
2022-02-27 20:07:05 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 2.5403
2022-02-27 20:07:38 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 2.7908
2022-02-27 20:08:11 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 2.7582
2022-02-27 20:08:45 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 2.7780
2022-02-27 20:09:18 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 2.6363
2022-02-27 20:09:19 - train: epoch 055, train_loss: 2.6184
2022-02-27 20:10:34 - eval: epoch: 055, acc1: 49.764%, acc5: 74.144%, test_loss: 2.2722, per_image_load_time: 2.755ms, per_image_inference_time: 0.122ms
2022-02-27 20:10:35 - until epoch: 055, best_acc1: 50.668%
2022-02-27 20:10:35 - epoch 056 lr: 0.010000000000000002
2022-02-27 20:11:13 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 2.4871
2022-02-27 20:11:47 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 2.5804
2022-02-27 20:12:20 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 2.4792
2022-02-27 20:12:53 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 2.7879
2022-02-27 20:13:27 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 2.5622
2022-02-27 20:14:00 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 2.5859
2022-02-27 20:14:34 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 2.5687
2022-02-27 20:15:07 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 2.7962
2022-02-27 20:15:40 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 2.6475
2022-02-27 20:16:14 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 2.5043
2022-02-27 20:16:48 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 2.5613
2022-02-27 20:17:21 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 2.6198
2022-02-27 20:17:54 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 2.7160
2022-02-27 20:18:27 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 2.6705
2022-02-27 20:19:01 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 2.9635
2022-02-27 20:19:34 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 2.3839
2022-02-27 20:20:08 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 2.8132
2022-02-27 20:20:41 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 2.8175
2022-02-27 20:21:14 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 2.7182
2022-02-27 20:21:48 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 2.5351
2022-02-27 20:22:22 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 2.8608
2022-02-27 20:22:56 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 2.9145
2022-02-27 20:23:28 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 2.6040
2022-02-27 20:24:02 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 2.6076
2022-02-27 20:24:34 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 2.7526
2022-02-27 20:25:09 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 2.5463
2022-02-27 20:25:41 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 2.5780
2022-02-27 20:26:15 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 2.5435
2022-02-27 20:26:47 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 2.8404
2022-02-27 20:27:20 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 2.9669
2022-02-27 20:27:53 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 2.6571
2022-02-27 20:28:27 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 2.5478
2022-02-27 20:29:00 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 2.9966
2022-02-27 20:29:34 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 2.6608
2022-02-27 20:30:06 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 2.6857
2022-02-27 20:30:39 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 2.3482
2022-02-27 20:31:11 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 2.5592
2022-02-27 20:31:45 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 2.6943
2022-02-27 20:32:17 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 2.8306
2022-02-27 20:32:50 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 2.7097
2022-02-27 20:33:22 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 2.7641
2022-02-27 20:33:55 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 2.6587
2022-02-27 20:34:28 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 2.6701
2022-02-27 20:35:01 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 2.7169
2022-02-27 20:35:34 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 2.6209
2022-02-27 20:36:06 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 2.5201
2022-02-27 20:36:39 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 2.5930
2022-02-27 20:37:11 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 2.7041
2022-02-27 20:37:44 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 2.7218
2022-02-27 20:38:17 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 2.5039
2022-02-27 20:38:18 - train: epoch 056, train_loss: 2.6184
2022-02-27 20:39:33 - eval: epoch: 056, acc1: 50.290%, acc5: 74.294%, test_loss: 2.2676, per_image_load_time: 2.748ms, per_image_inference_time: 0.116ms
2022-02-27 20:39:33 - until epoch: 056, best_acc1: 50.668%
2022-02-27 20:39:33 - epoch 057 lr: 0.010000000000000002
2022-02-27 20:40:10 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 2.5345
2022-02-27 20:40:44 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 2.5849
2022-02-27 20:41:17 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 2.6733
2022-02-27 20:41:49 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 2.6374
2022-02-27 20:42:22 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 2.5684
2022-02-27 20:42:55 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 2.6937
2022-02-27 20:43:29 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 2.5784
2022-02-27 20:44:00 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 2.6712
2022-02-27 20:44:34 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 2.6706
2022-02-27 20:45:05 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 2.5659
2022-02-27 20:45:39 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 2.5262
2022-02-27 20:46:10 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 2.7342
2022-02-27 20:46:43 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 2.7925
2022-02-27 20:47:15 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 2.7020
2022-02-27 20:47:47 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 2.8163
2022-02-27 20:48:20 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 2.8058
2022-02-27 20:48:52 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 2.5248
2022-02-27 20:49:25 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 2.7563
2022-02-27 20:49:57 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 2.6788
2022-02-27 20:50:29 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 2.5576
2022-02-27 20:51:03 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 2.5727
2022-02-27 20:51:35 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 2.7187
2022-02-27 20:52:10 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 2.8018
2022-02-27 20:52:42 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 2.5386
2022-02-27 20:53:16 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 3.0135
2022-02-27 20:53:48 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 2.5282
2022-02-27 20:54:22 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 2.3335
2022-02-27 20:54:54 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 2.3542
2022-02-27 20:55:27 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 2.4617
2022-02-27 20:55:59 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 2.8971
2022-02-27 20:56:34 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 2.7287
2022-02-27 20:57:05 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 2.8187
2022-02-27 20:57:40 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 2.6003
2022-02-27 20:58:12 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 2.7540
2022-02-27 20:58:46 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 2.8365
2022-02-27 20:59:18 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 2.8112
2022-02-27 20:59:51 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 2.7600
2022-02-27 21:00:23 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 2.5605
2022-02-27 21:00:56 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 2.8495
2022-02-27 21:01:28 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 2.5730
2022-02-27 21:02:02 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 2.6618
2022-02-27 21:02:33 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 2.5724
2022-02-27 21:03:07 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 2.4800
2022-02-27 21:03:39 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 2.7624
2022-02-27 21:04:13 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 2.6730
2022-02-27 21:04:45 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 2.4707
2022-02-27 21:05:18 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 2.7433
2022-02-27 21:05:51 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 2.7997
2022-02-27 21:06:24 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 2.9291
2022-02-27 21:06:54 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 2.8982
2022-02-27 21:06:55 - train: epoch 057, train_loss: 2.6180
2022-02-27 21:08:08 - eval: epoch: 057, acc1: 49.378%, acc5: 73.314%, test_loss: 2.3135, per_image_load_time: 1.135ms, per_image_inference_time: 0.128ms
2022-02-27 21:08:08 - until epoch: 057, best_acc1: 50.668%
2022-02-27 21:08:08 - epoch 058 lr: 0.010000000000000002
2022-02-27 21:08:45 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 2.7767
2022-02-27 21:09:18 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 2.5404
2022-02-27 21:09:51 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 2.7371
2022-02-27 21:10:23 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 2.6788
2022-02-27 21:10:56 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 2.3671
2022-02-27 21:11:28 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 2.8842
2022-02-27 21:12:01 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 2.7771
2022-02-27 21:12:33 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 2.6680
2022-02-27 21:13:05 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 2.4676
2022-02-27 21:13:38 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 2.7315
2022-02-27 21:14:11 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 2.4858
2022-02-27 21:14:43 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 2.4643
2022-02-27 21:15:17 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 2.6716
2022-02-27 21:15:48 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 2.7881
2022-02-27 21:16:22 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 2.3322
2022-02-27 21:16:54 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 2.5476
2022-02-27 21:17:28 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 2.7483
2022-02-27 21:18:00 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 2.7063
2022-02-27 21:18:33 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 2.7372
2022-02-27 21:19:06 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 3.0331
2022-02-27 21:19:39 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 2.4955
2022-02-27 21:20:13 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 2.4811
2022-02-27 21:20:45 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 2.5942
2022-02-27 21:21:18 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 2.8020
2022-02-27 21:21:52 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 2.6758
2022-02-27 21:22:24 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 2.5742
2022-02-27 21:22:59 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 2.8452
2022-02-27 21:23:30 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 2.7053
2022-02-27 21:24:04 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 2.4398
2022-02-27 21:24:37 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 2.7261
2022-02-27 21:25:11 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 2.7472
2022-02-27 21:25:43 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 2.4684
2022-02-27 21:26:17 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 2.5065
2022-02-27 21:26:49 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 2.5664
2022-02-27 21:27:24 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 2.6151
2022-02-27 21:27:55 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 2.2709
2022-02-27 21:28:29 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 2.6265
2022-02-27 21:29:02 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 2.5443
2022-02-27 21:29:35 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 2.6923
2022-02-27 21:30:07 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 2.6038
2022-02-27 21:30:41 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 2.6106
2022-02-27 21:31:13 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 2.5535
2022-02-27 21:31:47 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 2.5719
2022-02-27 21:32:19 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 2.5570
2022-02-27 21:32:53 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 2.6701
2022-02-27 21:33:24 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 2.4992
2022-02-27 21:33:58 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 2.6378
2022-02-27 21:34:30 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 2.5413
2022-02-27 21:35:04 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 2.4868
2022-02-27 21:35:35 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 2.5993
2022-02-27 21:35:36 - train: epoch 058, train_loss: 2.6157
2022-02-27 21:36:50 - eval: epoch: 058, acc1: 50.158%, acc5: 74.304%, test_loss: 2.2628, per_image_load_time: 0.842ms, per_image_inference_time: 0.110ms
2022-02-27 21:36:50 - until epoch: 058, best_acc1: 50.668%
2022-02-27 21:36:50 - epoch 059 lr: 0.010000000000000002
2022-02-27 21:37:28 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 2.6419
2022-02-27 21:38:01 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 2.7004
2022-02-27 21:38:32 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 2.6414
2022-02-27 21:39:05 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 2.7108
2022-02-27 21:39:38 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 2.6063
2022-02-27 21:40:10 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 2.4816
2022-02-27 21:40:43 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 2.4318
2022-02-27 21:41:15 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 2.7381
2022-02-27 21:41:49 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 2.7453
2022-02-27 21:42:21 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 2.6342
2022-02-27 21:42:53 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 2.8807
2022-02-27 21:43:25 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 2.3665
2022-02-27 21:43:59 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 2.8694
2022-02-27 21:44:30 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 2.9811
2022-02-27 21:45:04 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 2.4882
2022-02-27 21:45:36 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 2.5506
2022-02-27 21:46:09 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 2.6309
2022-02-27 21:46:41 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 2.5404
2022-02-27 21:47:15 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 2.3475
2022-02-27 21:47:47 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 2.5368
2022-02-27 21:48:21 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 2.7875
2022-02-27 21:48:53 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 2.6888
2022-02-27 21:49:26 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 2.7623
2022-02-27 21:49:59 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 2.7907
2022-02-27 21:50:32 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 2.7852
2022-02-27 21:51:05 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 2.6243
2022-02-27 21:51:38 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 2.5609
2022-02-27 21:52:10 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 2.7352
2022-02-27 21:52:44 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 2.5067
2022-02-27 21:53:16 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 2.9998
2022-02-27 21:53:48 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 2.5007
2022-02-27 21:54:22 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 2.6456
2022-02-27 21:54:54 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 2.5726
2022-02-27 21:55:28 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 3.0195
2022-02-27 21:55:59 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 2.4871
2022-02-27 21:56:31 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 2.6007
2022-02-27 21:57:06 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 2.6483
2022-02-27 21:57:37 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 2.4927
2022-02-27 21:58:11 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 2.3905
2022-02-27 21:58:44 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 2.7916
2022-02-27 21:59:17 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 2.5440
2022-02-27 21:59:49 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 2.6958
2022-02-27 22:00:24 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 2.7562
2022-02-27 22:00:55 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 2.4318
2022-02-27 22:01:29 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 2.6774
2022-02-27 22:02:03 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 2.8117
2022-02-27 22:02:34 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 2.4795
2022-02-27 22:03:07 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 2.6090
2022-02-27 22:03:41 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 2.8761
2022-02-27 22:04:13 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 2.6926
2022-02-27 22:04:14 - train: epoch 059, train_loss: 2.6168
2022-02-27 22:05:29 - eval: epoch: 059, acc1: 49.944%, acc5: 74.082%, test_loss: 2.2756, per_image_load_time: 2.785ms, per_image_inference_time: 0.108ms
2022-02-27 22:05:29 - until epoch: 059, best_acc1: 50.668%
2022-02-27 22:05:29 - epoch 060 lr: 0.010000000000000002
2022-02-27 22:06:06 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 2.5837
2022-02-27 22:06:38 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 2.6524
2022-02-27 22:07:10 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 2.5565
2022-02-27 22:07:42 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 2.8259
2022-02-27 22:08:15 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 2.6271
2022-02-27 22:08:48 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 2.7869
2022-02-27 22:09:21 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 2.7147
2022-02-27 22:09:53 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 2.7571
2022-02-27 22:10:27 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 2.5910
2022-02-27 22:10:59 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 2.2087
2022-02-27 22:11:31 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 2.5731
2022-02-27 22:12:03 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 2.4539
2022-02-27 22:12:35 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 2.6567
2022-02-27 22:13:07 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 2.5890
2022-02-27 22:13:39 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 2.6656
2022-02-27 22:14:12 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 2.6978
2022-02-27 22:14:44 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 2.5425
2022-02-27 22:15:16 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 2.7037
2022-02-27 22:15:49 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 2.8875
2022-02-27 22:16:21 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 2.4434
2022-02-27 22:16:55 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 2.7196
2022-02-27 22:17:27 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 2.7717
2022-02-27 22:18:00 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 2.5511
2022-02-27 22:18:32 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 2.6303
2022-02-27 22:19:05 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 2.6985
2022-02-27 22:19:37 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 2.5583
2022-02-27 22:20:10 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 2.8171
2022-02-27 22:20:42 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 2.8722
2022-02-27 22:21:15 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 2.5088
2022-02-27 22:21:47 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 2.7302
2022-02-27 22:22:20 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 2.7032
2022-02-27 22:22:53 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 2.5996
2022-02-27 22:23:26 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 2.5379
2022-02-27 22:23:58 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 2.6456
2022-02-27 22:24:32 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 2.7443
2022-02-27 22:25:05 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 2.7309
2022-02-27 22:25:38 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 2.6109
2022-02-27 22:26:10 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 2.7038
2022-02-27 22:26:44 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 2.6628
2022-02-27 22:27:16 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 2.7714
2022-02-27 22:27:49 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 2.8425
2022-02-27 22:28:22 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 2.7459
2022-02-27 22:28:56 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 2.6189
2022-02-27 22:29:28 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 2.7628
2022-02-27 22:30:00 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 2.5128
2022-02-27 22:30:32 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 2.3193
2022-02-27 22:31:06 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 2.6017
2022-02-27 22:31:39 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 2.6405
2022-02-27 22:32:11 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 2.6008
2022-02-27 22:32:42 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 2.6949
2022-02-27 22:32:44 - train: epoch 060, train_loss: 2.6147
2022-02-27 22:33:59 - eval: epoch: 060, acc1: 49.330%, acc5: 73.888%, test_loss: 2.3028, per_image_load_time: 2.773ms, per_image_inference_time: 0.121ms
2022-02-27 22:33:59 - until epoch: 060, best_acc1: 50.668%
2022-02-27 22:33:59 - epoch 061 lr: 0.0010000000000000002
2022-02-27 22:34:36 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 2.2219
2022-02-27 22:35:10 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 2.5729
2022-02-27 22:35:41 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 2.3788
2022-02-27 22:36:15 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 2.5030
2022-02-27 22:36:47 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 2.3958
2022-02-27 22:37:21 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 2.5210
2022-02-27 22:37:54 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 2.4116
2022-02-27 22:38:26 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 2.2651
2022-02-27 22:39:00 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 2.5566
2022-02-27 22:39:33 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 2.5895
2022-02-27 22:40:06 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 2.5311
2022-02-27 22:40:38 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 2.2041
2022-02-27 22:41:11 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 2.4838
2022-02-27 22:41:43 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 2.2526
2022-02-27 22:42:17 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 2.4021
2022-02-27 22:42:48 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 2.3947
2022-02-27 22:43:21 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 2.6482
2022-02-27 22:43:54 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 2.3812
2022-02-27 22:44:27 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 2.6797
2022-02-27 22:45:00 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 2.4932
2022-02-27 22:45:31 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 2.6422
2022-02-27 22:46:05 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 2.5280
2022-02-27 22:46:38 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 2.2216
2022-02-27 22:47:09 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 2.1527
2022-02-27 22:47:43 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 2.3782
2022-02-27 22:48:15 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 2.4821
2022-02-27 22:48:48 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 2.5362
2022-02-27 22:49:20 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 2.3899
2022-02-27 22:49:52 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 2.6138
2022-02-27 22:50:26 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 2.5070
2022-02-27 22:50:59 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 2.4390
2022-02-27 22:51:31 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 2.3562
2022-02-27 22:52:04 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 2.8203
2022-02-27 22:52:36 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 2.4324
2022-02-27 22:53:09 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 2.6945
2022-02-27 22:53:43 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 2.3438
2022-02-27 22:54:14 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 2.4138
2022-02-27 22:54:48 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 2.4039
2022-02-27 22:55:20 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 2.5420
2022-02-27 22:55:53 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 2.6992
2022-02-27 22:56:26 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 2.5128
2022-02-27 22:56:59 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 2.3734
2022-02-27 22:57:30 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 2.3825
2022-02-27 22:58:05 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 2.4142
2022-02-27 22:58:37 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 2.4960
2022-02-27 22:59:10 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 2.3043
2022-02-27 22:59:43 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 2.3008
2022-02-27 23:00:16 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 2.3314
2022-02-27 23:00:49 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 2.3751
2022-02-27 23:01:21 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 2.7125
2022-02-27 23:01:22 - train: epoch 061, train_loss: 2.4645
2022-02-27 23:02:36 - eval: epoch: 061, acc1: 53.390%, acc5: 76.530%, test_loss: 2.1115, per_image_load_time: 2.229ms, per_image_inference_time: 0.133ms
2022-02-27 23:02:37 - until epoch: 061, best_acc1: 53.390%
2022-02-27 23:02:37 - epoch 062 lr: 0.0010000000000000002
2022-02-27 23:03:14 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 2.5123
2022-02-27 23:03:47 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 2.6291
2022-02-27 23:04:19 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 2.6630
2022-02-27 23:04:52 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 2.4980
2022-02-27 23:05:25 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 2.4890
2022-02-27 23:05:58 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 2.1690
2022-02-27 23:06:30 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 2.4755
2022-02-27 23:07:03 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 2.7685
2022-02-27 23:07:35 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 2.4602
2022-02-27 23:08:09 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 2.5860
2022-02-27 23:08:41 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 2.3270
2022-02-27 23:09:14 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 2.4471
2022-02-27 23:09:46 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 2.3291
2022-02-27 23:10:19 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 2.4405
2022-02-27 23:10:52 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 2.4934
2022-02-27 23:11:25 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 2.5767
2022-02-27 23:11:57 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 2.4645
2022-02-27 23:12:31 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 2.2242
2022-02-27 23:13:03 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 2.4994
2022-02-27 23:13:36 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 2.7745
2022-02-27 23:14:09 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 2.5902
2022-02-27 23:14:42 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 2.2667
2022-02-27 23:15:14 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 2.3377
2022-02-27 23:15:48 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 2.4163
2022-02-27 23:16:19 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 2.6811
2022-02-27 23:16:53 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 2.1183
2022-02-27 23:17:25 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 2.3959
2022-02-27 23:17:58 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 2.5665
2022-02-27 23:18:30 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 2.5119
2022-02-27 23:19:04 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 2.6791
2022-02-27 23:19:36 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 2.4151
2022-02-27 23:20:09 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 2.3600
2022-02-27 23:20:41 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 2.4766
2022-02-27 23:21:15 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 2.4921
2022-02-27 23:21:48 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 2.6053
2022-02-27 23:22:21 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 2.7244
2022-02-27 23:22:53 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 2.5041
2022-02-27 23:23:26 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 2.3773
2022-02-27 23:23:59 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 2.4476
2022-02-27 23:24:31 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 2.1368
2022-02-27 23:25:05 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 2.4619
2022-02-27 23:25:37 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 2.2195
2022-02-27 23:26:10 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 2.6207
2022-02-27 23:26:42 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 2.5773
2022-02-27 23:27:16 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 2.3807
2022-02-27 23:27:48 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 2.3953
2022-02-27 23:28:22 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 2.2581
2022-02-27 23:28:55 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 2.3577
2022-02-27 23:29:29 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 2.1519
2022-02-27 23:30:01 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 2.6466
2022-02-27 23:30:02 - train: epoch 062, train_loss: 2.4413
2022-02-27 23:31:16 - eval: epoch: 062, acc1: 53.464%, acc5: 76.836%, test_loss: 2.0957, per_image_load_time: 1.546ms, per_image_inference_time: 0.138ms
2022-02-27 23:31:16 - until epoch: 062, best_acc1: 53.464%
2022-02-27 23:31:16 - epoch 063 lr: 0.0010000000000000002
2022-02-27 23:31:53 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 2.4368
2022-02-27 23:32:26 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 2.4414
2022-02-27 23:32:59 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 2.6171
2022-02-27 23:33:33 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 2.4906
2022-02-27 23:34:04 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 2.2239
2022-02-27 23:34:37 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 2.5898
2022-02-27 23:35:09 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 2.3975
2022-02-27 23:35:41 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 2.6003
2022-02-27 23:36:14 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 2.5580
2022-02-27 23:36:48 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 2.6088
2022-02-27 23:37:19 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 2.3794
2022-02-27 23:37:52 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 2.6634
2022-02-27 23:38:25 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 2.1026
2022-02-27 23:38:58 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 2.5479
2022-02-27 23:39:31 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 2.3142
2022-02-27 23:40:04 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 2.2853
2022-02-27 23:40:36 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 2.3440
2022-02-27 23:41:10 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 2.5302
2022-02-27 23:41:42 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 2.4265
2022-02-27 23:42:14 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 2.1676
2022-02-27 23:42:46 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 2.3181
2022-02-27 23:43:21 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 2.8084
2022-02-27 23:43:53 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 2.6343
2022-02-27 23:44:26 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 2.4830
2022-02-27 23:44:59 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 2.2602
2022-02-27 23:45:32 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 2.2776
2022-02-27 23:46:05 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 2.6069
2022-02-27 23:46:38 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 2.4028
2022-02-27 23:47:10 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 2.4966
2022-02-27 23:47:43 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 2.6290
2022-02-27 23:48:15 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 2.6895
2022-02-27 23:48:48 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 2.4686
2022-02-27 23:49:22 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 2.4188
2022-02-27 23:49:56 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 2.4215
2022-02-27 23:50:28 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 2.5818
2022-02-27 23:51:01 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 2.4534
2022-02-27 23:51:34 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 2.5054
2022-02-27 23:52:08 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 2.4629
2022-02-27 23:52:40 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 2.3617
2022-02-27 23:53:14 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 2.2385
2022-02-27 23:53:48 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 2.6003
2022-02-27 23:54:21 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 2.4826
2022-02-27 23:54:54 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 2.6759
2022-02-27 23:55:27 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 2.3195
2022-02-27 23:55:59 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 2.5105
2022-02-27 23:56:33 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 2.4520
2022-02-27 23:57:07 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 2.4649
2022-02-27 23:57:40 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 2.3998
2022-02-27 23:58:14 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 2.4559
2022-02-27 23:58:47 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 2.4547
2022-02-27 23:58:48 - train: epoch 063, train_loss: 2.4348
2022-02-28 00:00:04 - eval: epoch: 063, acc1: 53.642%, acc5: 76.916%, test_loss: 2.0882, per_image_load_time: 2.799ms, per_image_inference_time: 0.138ms
2022-02-28 00:00:04 - until epoch: 063, best_acc1: 53.642%
2022-02-28 00:00:04 - epoch 064 lr: 0.0010000000000000002
2022-02-28 00:00:43 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 2.2570
2022-02-28 00:01:15 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 2.4624
2022-02-28 00:01:48 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 2.2215
2022-02-28 00:02:20 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 2.1643
2022-02-28 00:02:54 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 2.3131
2022-02-28 00:03:26 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 2.4456
2022-02-28 00:04:00 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 2.5578
2022-02-28 00:04:33 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 2.2910
2022-02-28 00:05:07 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 2.4540
2022-02-28 00:05:40 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 2.3701
2022-02-28 00:06:13 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 2.4838
2022-02-28 00:06:46 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 2.2130
2022-02-28 00:07:20 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 2.3966
2022-02-28 00:07:53 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 2.7046
2022-02-28 00:08:26 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 2.5639
2022-02-28 00:08:59 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 2.2570
2022-02-28 00:09:32 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 2.2888
2022-02-28 00:10:06 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 2.3088
2022-02-28 00:10:39 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 2.5739
2022-02-28 00:11:11 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 2.2558
2022-02-28 00:11:44 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 2.5499
2022-02-28 00:12:17 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 2.4015
2022-02-28 00:12:51 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 2.4549
2022-02-28 00:13:23 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 2.4849
2022-02-28 00:13:56 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 2.4466
2022-02-28 00:14:29 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 2.0801
2022-02-28 00:15:03 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 2.3509
2022-02-28 00:15:35 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 2.2299
2022-02-28 00:16:09 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 2.3372
2022-02-28 00:16:42 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 2.3132
2022-02-28 00:17:16 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 2.5264
2022-02-28 00:17:49 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 2.4457
2022-02-28 00:18:22 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 2.1672
2022-02-28 00:18:55 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 2.5511
2022-02-28 00:19:28 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 2.4633
2022-02-28 00:20:02 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 2.2504
2022-02-28 00:20:36 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 2.2161
2022-02-28 00:21:09 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 2.6091
2022-02-28 00:21:42 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 2.2126
2022-02-28 00:22:16 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 2.4590
2022-02-28 00:22:48 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 2.6408
2022-02-28 00:23:22 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 2.3060
2022-02-28 00:23:54 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 2.5167
2022-02-28 00:24:28 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 2.2128
2022-02-28 00:25:00 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 2.3273
2022-02-28 00:25:34 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 2.7740
2022-02-28 00:26:07 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 2.7287
2022-02-28 00:26:40 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 2.5367
2022-02-28 00:27:13 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 2.5415
2022-02-28 00:27:46 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 2.2783
2022-02-28 00:27:47 - train: epoch 064, train_loss: 2.4283
2022-02-28 00:29:02 - eval: epoch: 064, acc1: 53.670%, acc5: 77.062%, test_loss: 2.0814, per_image_load_time: 1.360ms, per_image_inference_time: 0.135ms
2022-02-28 00:29:02 - until epoch: 064, best_acc1: 53.670%
2022-02-28 00:29:02 - epoch 065 lr: 0.0010000000000000002
2022-02-28 00:29:40 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 2.5345
2022-02-28 00:30:12 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 2.2463
2022-02-28 00:30:45 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 2.2904
2022-02-28 00:31:18 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 2.3164
2022-02-28 00:31:52 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 2.4180
2022-02-28 00:32:26 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 2.5397
2022-02-28 00:32:58 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 2.5803
2022-02-28 00:33:31 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 2.4533
2022-02-28 00:34:05 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 2.4622
2022-02-28 00:34:37 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 2.6339
2022-02-28 00:35:10 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 2.3818
2022-02-28 00:35:43 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 2.7897
2022-02-28 00:36:16 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 2.3724
2022-02-28 00:36:48 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 2.3371
2022-02-28 00:37:22 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 2.4720
2022-02-28 00:37:55 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 2.4010
2022-02-28 00:38:28 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 2.4039
2022-02-28 00:39:00 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 2.2201
2022-02-28 00:39:34 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 2.0594
2022-02-28 00:40:06 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 2.3850
2022-02-28 00:40:39 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 2.4316
2022-02-28 00:41:12 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 2.4535
2022-02-28 00:41:45 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 2.1956
2022-02-28 00:42:18 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 2.3362
2022-02-28 00:42:51 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 2.4890
2022-02-28 00:43:24 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 2.5581
2022-02-28 00:43:58 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 2.4515
2022-02-28 00:44:30 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 2.4211
2022-02-28 00:45:03 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 2.2782
2022-02-28 00:45:35 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 2.4266
2022-02-28 00:46:08 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 2.4545
2022-02-28 00:46:42 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 2.5364
2022-02-28 00:47:15 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 2.3587
2022-02-28 00:47:49 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 2.1828
2022-02-28 00:48:21 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 2.6215
2022-02-28 00:48:54 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 2.4312
2022-02-28 00:49:27 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 2.1752
2022-02-28 00:50:00 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 2.2502
2022-02-28 00:50:34 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 2.5582
2022-02-28 00:51:07 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 2.3804
2022-02-28 00:51:39 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 2.4715
2022-02-28 00:52:12 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 2.2825
2022-02-28 00:52:45 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 2.4769
2022-02-28 00:53:19 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 2.3710
2022-02-28 00:53:51 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 2.5049
2022-02-28 00:54:24 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 2.4208
2022-02-28 00:54:57 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 2.3080
2022-02-28 00:55:31 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 2.2384
2022-02-28 00:56:04 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 2.3977
2022-02-28 00:56:36 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 2.2829
2022-02-28 00:56:37 - train: epoch 065, train_loss: 2.4227
2022-02-28 00:57:52 - eval: epoch: 065, acc1: 53.910%, acc5: 77.022%, test_loss: 2.0795, per_image_load_time: 1.576ms, per_image_inference_time: 0.120ms
2022-02-28 00:57:52 - until epoch: 065, best_acc1: 53.910%
2022-02-28 00:57:52 - epoch 066 lr: 0.0010000000000000002
2022-02-28 00:58:29 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 2.2063
2022-02-28 00:59:02 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 2.6501
2022-02-28 00:59:36 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 2.3420
2022-02-28 01:00:09 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 2.1527
2022-02-28 01:00:42 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 2.5040
2022-02-28 01:01:14 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 2.4037
2022-02-28 01:01:48 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 2.5466
2022-02-28 01:02:19 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 2.6158
2022-02-28 01:02:53 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 2.4374
2022-02-28 01:03:24 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 2.5006
2022-02-28 01:03:58 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 2.5304
2022-02-28 01:04:31 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 2.6891
2022-02-28 01:05:05 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 2.5481
2022-02-28 01:05:37 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 2.4006
2022-02-28 01:06:11 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 2.4383
2022-02-28 01:06:43 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 2.6818
2022-02-28 01:07:17 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 2.2486
2022-02-28 01:07:49 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 2.3710
2022-02-28 01:08:23 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 2.5796
2022-02-28 01:08:55 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 2.2205
2022-02-28 01:09:28 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 2.4261
2022-02-28 01:10:00 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 2.4876
2022-02-28 01:10:34 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 2.6180
2022-02-28 01:11:06 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 2.3886
2022-02-28 01:11:40 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 2.5420
2022-02-28 01:12:12 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 2.2821
2022-02-28 01:12:46 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 2.4099
2022-02-28 01:13:18 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 2.3108
2022-02-28 01:13:52 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 2.3782
2022-02-28 01:14:25 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 2.3881
2022-02-28 01:14:58 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 2.3289
2022-02-28 01:15:30 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 2.4754
2022-02-28 01:16:04 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 2.3083
2022-02-28 01:16:36 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 2.4705
2022-02-28 01:17:10 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 2.6118
2022-02-28 01:17:42 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 2.3986
2022-02-28 01:18:15 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 2.2914
2022-02-28 01:18:49 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 2.4381
2022-02-28 01:19:21 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 2.3499
2022-02-28 01:19:56 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 2.4108
2022-02-28 01:20:27 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 2.3780
2022-02-28 01:21:01 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 2.3263
2022-02-28 01:21:34 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 2.3260
2022-02-28 01:22:09 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 2.3814
2022-02-28 01:22:41 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 2.4825
2022-02-28 01:23:16 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 2.6696
2022-02-28 01:23:48 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 2.3357
2022-02-28 01:24:23 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 2.5368
2022-02-28 01:24:56 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 2.5803
2022-02-28 01:25:28 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 2.3037
2022-02-28 01:25:29 - train: epoch 066, train_loss: 2.4206
2022-02-28 01:26:45 - eval: epoch: 066, acc1: 53.994%, acc5: 77.082%, test_loss: 2.0789, per_image_load_time: 2.797ms, per_image_inference_time: 0.133ms
2022-02-28 01:26:45 - until epoch: 066, best_acc1: 53.994%
2022-02-28 01:26:45 - epoch 067 lr: 0.0010000000000000002
2022-02-28 01:27:23 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 2.5201
2022-02-28 01:27:57 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 2.4594
2022-02-28 01:28:29 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 2.5005
2022-02-28 01:29:03 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 2.1961
2022-02-28 01:29:36 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 2.3932
2022-02-28 01:30:09 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 2.4491
2022-02-28 01:30:41 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 2.3454
2022-02-28 01:31:15 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 2.6782
2022-02-28 01:31:48 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 2.7387
2022-02-28 01:32:21 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 2.2695
2022-02-28 01:32:54 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 2.3904
2022-02-28 01:33:26 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 2.4918
2022-02-28 01:33:59 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 2.4137
2022-02-28 01:34:32 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 2.6544
2022-02-28 01:35:07 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 2.0525
2022-02-28 01:35:39 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 2.2690
2022-02-28 01:36:12 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 2.3877
2022-02-28 01:36:46 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 2.5242
2022-02-28 01:37:18 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 2.5099
2022-02-28 01:37:52 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 2.4831
2022-02-28 01:38:25 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 2.3927
2022-02-28 01:38:59 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 2.3301
2022-02-28 01:39:31 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 2.5473
2022-02-28 01:40:06 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 2.2499
2022-02-28 01:40:38 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 2.4558
2022-02-28 01:41:12 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 2.4756
2022-02-28 01:41:45 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 2.2998
2022-02-28 01:42:18 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 2.5728
2022-02-28 01:42:51 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 2.4001
2022-02-28 01:43:23 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 2.3941
2022-02-28 01:43:57 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 2.3479
2022-02-28 01:44:31 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 2.4719
2022-02-28 01:45:03 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 2.0992
2022-02-28 01:45:37 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 2.5309
2022-02-28 01:46:10 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 2.4232
2022-02-28 01:46:44 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 2.3913
2022-02-28 01:47:16 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 2.5971
2022-02-28 01:47:50 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 2.4212
2022-02-28 01:48:23 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 2.7105
2022-02-28 01:48:57 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 2.3085
2022-02-28 01:49:29 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 2.4423
2022-02-28 01:50:03 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 2.2033
2022-02-28 01:50:35 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 2.1317
2022-02-28 01:51:09 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 2.3800
2022-02-28 01:51:43 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 2.4565
2022-02-28 01:52:16 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 2.1387
2022-02-28 01:52:50 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 2.4283
2022-02-28 01:53:23 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 2.0952
2022-02-28 01:53:57 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 2.3705
2022-02-28 01:54:29 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 2.4117
2022-02-28 01:54:31 - train: epoch 067, train_loss: 2.4163
2022-02-28 01:55:46 - eval: epoch: 067, acc1: 53.912%, acc5: 77.062%, test_loss: 2.0795, per_image_load_time: 1.632ms, per_image_inference_time: 0.127ms
2022-02-28 01:55:46 - until epoch: 067, best_acc1: 53.994%
2022-02-28 01:55:46 - epoch 068 lr: 0.0010000000000000002
2022-02-28 01:56:23 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 2.6848
2022-02-28 01:56:56 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 2.3170
2022-02-28 01:57:30 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 2.5861
2022-02-28 01:58:03 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 2.1733
2022-02-28 01:58:36 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 2.3086
2022-02-28 01:59:10 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 2.4889
2022-02-28 01:59:43 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 2.3599
2022-02-28 02:00:16 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 2.3299
2022-02-28 02:00:50 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 2.4628
2022-02-28 02:01:22 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 2.5540
2022-02-28 02:01:57 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 2.6673
2022-02-28 02:02:28 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 2.4041
2022-02-28 02:03:02 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 2.2425
2022-02-28 02:03:34 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 2.4113
2022-02-28 02:04:08 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 2.5264
2022-02-28 02:04:41 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 2.3662
2022-02-28 02:05:15 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 2.4712
2022-02-28 02:05:47 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 2.5234
2022-02-28 02:06:21 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 2.4443
2022-02-28 02:06:54 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 2.5880
2022-02-28 02:07:28 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 2.3145
2022-02-28 02:08:00 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 2.4387
2022-02-28 02:08:35 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 2.4608
2022-02-28 02:09:07 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 2.6245
2022-02-28 02:09:41 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 2.3906
2022-02-28 02:10:13 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 2.1794
2022-02-28 02:10:47 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 2.4231
2022-02-28 02:11:20 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 2.4159
2022-02-28 02:11:53 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 2.4761
2022-02-28 02:12:26 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 2.5922
2022-02-28 02:13:01 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 2.2364
2022-02-28 02:13:33 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 2.3794
2022-02-28 02:14:08 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 2.6221
2022-02-28 02:14:40 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 2.3278
2022-02-28 02:15:14 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 2.2807
2022-02-28 02:15:46 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 2.4387
2022-02-28 02:16:20 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 2.4764
2022-02-28 02:16:53 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 2.5848
2022-02-28 02:17:27 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 2.5064
2022-02-28 02:18:00 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 2.3085
2022-02-28 02:18:34 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 2.2230
2022-02-28 02:19:07 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 2.3144
2022-02-28 02:19:40 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 2.5019
2022-02-28 02:20:13 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 2.2515
2022-02-28 02:20:47 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 2.5614
2022-02-28 02:21:19 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 2.3822
2022-02-28 02:21:54 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 2.6025
2022-02-28 02:22:26 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 2.4677
2022-02-28 02:23:03 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 2.4351
2022-02-28 02:23:35 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 2.5543
2022-02-28 02:23:36 - train: epoch 068, train_loss: 2.4152
2022-02-28 02:24:51 - eval: epoch: 068, acc1: 53.936%, acc5: 77.198%, test_loss: 2.0758, per_image_load_time: 1.035ms, per_image_inference_time: 0.127ms
2022-02-28 02:24:51 - until epoch: 068, best_acc1: 53.994%
2022-02-28 02:24:51 - epoch 069 lr: 0.0010000000000000002
2022-02-28 02:25:28 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 2.6861
2022-02-28 02:26:02 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 2.6589
2022-02-28 02:26:35 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 2.3761
2022-02-28 02:27:08 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 2.1417
2022-02-28 02:27:41 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 2.3922
2022-02-28 02:28:13 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 2.1595
2022-02-28 02:28:47 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 2.3647
2022-02-28 02:29:19 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 2.2650
2022-02-28 02:29:53 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 2.1784
2022-02-28 02:30:26 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 2.3813
2022-02-28 02:31:00 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 2.4254
2022-02-28 02:31:32 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 2.2120
2022-02-28 02:32:06 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 2.6352
2022-02-28 02:32:38 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 2.0866
2022-02-28 02:33:12 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 2.5135
2022-02-28 02:33:45 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 2.5150
2022-02-28 02:34:19 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 2.4649
2022-02-28 02:34:51 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 2.4661
2022-02-28 02:35:24 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 2.4006
2022-02-28 02:35:56 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 2.3893
2022-02-28 02:36:31 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 2.3916
2022-02-28 02:37:03 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 2.4796
2022-02-28 02:37:36 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 2.2733
2022-02-28 02:38:09 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 2.4485
2022-02-28 02:38:43 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 2.3246
2022-02-28 02:39:16 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 2.3141
2022-02-28 02:39:50 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 2.4696
2022-02-28 02:40:22 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 2.5368
2022-02-28 02:40:55 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 2.3578
2022-02-28 02:41:28 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 2.4015
2022-02-28 02:42:01 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 2.5437
2022-02-28 02:42:34 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 2.4037
2022-02-28 02:43:08 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 2.4505
2022-02-28 02:43:40 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 2.4000
2022-02-28 02:44:13 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 2.3289
2022-02-28 02:44:46 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.8990
2022-02-28 02:45:19 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 2.4749
2022-02-28 02:45:52 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 2.4868
2022-02-28 02:46:24 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 2.4991
2022-02-28 02:46:58 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 2.5694
2022-02-28 02:47:30 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 2.3306
2022-02-28 02:48:03 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 2.3435
2022-02-28 02:48:37 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 2.5353
2022-02-28 02:49:09 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 2.4519
2022-02-28 02:49:43 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 2.5178
2022-02-28 02:50:15 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 2.4129
2022-02-28 02:50:49 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 2.5734
2022-02-28 02:51:21 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 2.2852
2022-02-28 02:51:56 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 2.3397
2022-02-28 02:52:28 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 2.4730
2022-02-28 02:52:30 - train: epoch 069, train_loss: 2.4143
2022-02-28 02:53:45 - eval: epoch: 069, acc1: 54.060%, acc5: 77.082%, test_loss: 2.0744, per_image_load_time: 2.756ms, per_image_inference_time: 0.121ms
2022-02-28 02:53:45 - until epoch: 069, best_acc1: 54.060%
2022-02-28 02:53:45 - epoch 070 lr: 0.0010000000000000002
2022-02-28 02:54:24 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 2.5032
2022-02-28 02:54:56 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 2.4982
2022-02-28 02:55:30 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 2.6501
2022-02-28 02:56:03 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 2.3767
2022-02-28 02:56:35 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 2.3838
2022-02-28 02:57:08 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 2.3052
2022-02-28 02:57:42 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 2.1959
2022-02-28 02:58:15 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 2.3890
2022-02-28 02:58:48 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 2.2465
2022-02-28 02:59:21 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 2.3513
2022-02-28 02:59:55 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 2.6449
2022-02-28 03:00:27 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 2.3186
2022-02-28 03:00:59 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 2.3019
2022-02-28 03:01:33 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 2.2965
2022-02-28 03:02:06 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 2.3668
2022-02-28 03:02:38 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 2.3909
2022-02-28 03:03:12 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 2.4433
2022-02-28 03:03:45 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 2.3302
2022-02-28 03:04:19 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 2.4761
2022-02-28 03:04:51 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 2.5024
2022-02-28 03:05:25 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 2.5675
2022-02-28 03:05:58 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 2.5670
2022-02-28 03:06:32 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 2.8639
2022-02-28 03:07:04 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 2.4366
2022-02-28 03:07:38 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 2.2283
2022-02-28 03:08:11 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 2.1358
2022-02-28 03:08:44 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 2.5761
2022-02-28 03:09:17 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 2.6169
2022-02-28 03:09:50 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 2.4048
2022-02-28 03:10:24 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 2.3415
2022-02-28 03:10:56 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 2.3566
2022-02-28 03:11:30 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 2.6316
2022-02-28 03:12:02 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 2.4405
2022-02-28 03:12:36 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 2.4573
2022-02-28 03:13:09 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 2.5919
2022-02-28 03:13:42 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 2.5706
2022-02-28 03:14:15 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 2.3030
2022-02-28 03:14:47 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 2.4656
2022-02-28 03:15:21 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 2.3476
2022-02-28 03:15:53 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 2.2308
2022-02-28 03:16:27 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 2.3354
2022-02-28 03:16:59 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 2.4523
2022-02-28 03:17:34 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 2.4736
2022-02-28 03:18:06 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 2.3987
2022-02-28 03:18:39 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 2.5779
2022-02-28 03:19:12 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 2.5477
2022-02-28 03:19:47 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 2.2993
2022-02-28 03:20:19 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 2.3403
2022-02-28 03:20:54 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 2.4287
2022-02-28 03:21:25 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 2.3963
2022-02-28 03:21:27 - train: epoch 070, train_loss: 2.4121
2022-02-28 03:22:43 - eval: epoch: 070, acc1: 53.900%, acc5: 77.104%, test_loss: 2.0737, per_image_load_time: 2.751ms, per_image_inference_time: 0.127ms
2022-02-28 03:22:43 - until epoch: 070, best_acc1: 54.060%
2022-02-28 03:22:43 - epoch 071 lr: 0.0010000000000000002
2022-02-28 03:23:20 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 2.1509
2022-02-28 03:23:54 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 2.3031
2022-02-28 03:24:26 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 2.7009
2022-02-28 03:25:00 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 2.5804
2022-02-28 03:25:32 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 2.5852
2022-02-28 03:26:06 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 2.4524
2022-02-28 03:26:40 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 2.3494
2022-02-28 03:27:12 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 2.3891
2022-02-28 03:27:45 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 2.5620
2022-02-28 03:28:18 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 2.3871
2022-02-28 03:28:52 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 2.6685
2022-02-28 03:29:25 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 2.3410
2022-02-28 03:29:58 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 2.4272
2022-02-28 03:30:32 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 2.4422
2022-02-28 03:31:05 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 2.3475
2022-02-28 03:31:38 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 2.1187
2022-02-28 03:32:11 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 2.2456
2022-02-28 03:32:45 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 2.5448
2022-02-28 03:33:17 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 2.3628
2022-02-28 03:33:51 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 2.3593
2022-02-28 03:34:25 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 2.2502
2022-02-28 03:34:58 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 2.3342
2022-02-28 03:35:31 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 2.3418
2022-02-28 03:36:05 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 2.2330
2022-02-28 03:36:38 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 2.5002
2022-02-28 03:37:11 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 2.1951
2022-02-28 03:37:44 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 2.4776
2022-02-28 03:38:17 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 2.4868
2022-02-28 03:38:51 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 2.3190
2022-02-28 03:39:24 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 2.5140
2022-02-28 03:39:57 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 2.1771
2022-02-28 03:40:31 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 2.5140
2022-02-28 03:41:04 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 2.4032
2022-02-28 03:41:38 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 2.4256
2022-02-28 03:42:10 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 2.5531
2022-02-28 03:42:44 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 2.2759
2022-02-28 03:43:17 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 2.4019
2022-02-28 03:43:51 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 2.4080
2022-02-28 03:44:23 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 2.4590
2022-02-28 03:44:57 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 2.3615
2022-02-28 03:45:29 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 2.3660
2022-02-28 03:46:03 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 2.4811
2022-02-28 03:46:36 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 2.1641
2022-02-28 03:47:10 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 2.5614
2022-02-28 03:47:43 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 2.4417
2022-02-28 03:48:17 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 2.6116
2022-02-28 03:48:49 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 2.3501
2022-02-28 03:49:23 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 2.3283
2022-02-28 03:49:56 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 2.2454
2022-02-28 03:50:29 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 2.4061
2022-02-28 03:50:30 - train: epoch 071, train_loss: 2.4083
2022-02-28 03:51:46 - eval: epoch: 071, acc1: 54.034%, acc5: 77.144%, test_loss: 2.0708, per_image_load_time: 2.666ms, per_image_inference_time: 0.137ms
2022-02-28 03:51:46 - until epoch: 071, best_acc1: 54.060%
2022-02-28 03:51:46 - epoch 072 lr: 0.0010000000000000002
2022-02-28 03:52:24 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 2.6885
2022-02-28 03:52:59 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 2.3725
2022-02-28 03:53:32 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 2.2902
2022-02-28 03:54:05 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 2.4575
2022-02-28 03:54:37 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 2.2134
2022-02-28 03:55:12 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 2.3626
2022-02-28 03:55:43 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 2.5214
2022-02-28 03:56:17 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 2.5289
2022-02-28 03:56:49 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 2.1201
2022-02-28 03:57:22 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 2.3239
2022-02-28 03:57:56 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 2.6010
2022-02-28 03:58:28 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 2.2618
2022-02-28 03:59:03 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 2.4399
2022-02-28 03:59:35 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 2.4398
2022-02-28 04:00:09 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 2.4437
2022-02-28 04:00:42 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 2.1935
2022-02-28 04:01:16 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 2.2326
2022-02-28 04:01:48 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 2.3719
2022-02-28 04:02:22 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 2.5332
2022-02-28 04:02:55 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 2.5041
2022-02-28 04:03:27 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 2.5324
2022-02-28 04:04:01 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 2.5946
2022-02-28 04:04:35 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 2.5262
2022-02-28 04:05:07 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 2.4088
2022-02-28 04:05:41 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 2.4142
2022-02-28 04:06:14 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 2.3607
2022-02-28 04:06:48 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 2.5154
2022-02-28 04:07:21 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 2.4308
2022-02-28 04:07:54 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 2.2122
2022-02-28 04:08:26 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 2.5112
2022-02-28 04:09:00 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 2.5794
2022-02-28 04:09:33 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 2.2750
2022-02-28 04:10:06 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 2.4092
2022-02-28 04:10:40 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 2.8073
2022-02-28 04:11:13 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 2.6055
2022-02-28 04:11:46 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 2.4175
2022-02-28 04:12:19 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 2.6613
2022-02-28 04:12:53 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 2.4515
2022-02-28 04:13:25 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 2.4581
2022-02-28 04:14:00 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 2.3656
2022-02-28 04:14:32 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 2.4586
2022-02-28 04:15:06 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 2.1989
2022-02-28 04:15:40 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 2.1491
2022-02-28 04:16:14 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 2.4375
2022-02-28 04:16:47 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 2.5706
2022-02-28 04:17:20 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 2.2491
2022-02-28 04:17:53 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 2.3933
2022-02-28 04:18:28 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 2.4877
2022-02-28 04:19:00 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 2.3525
2022-02-28 04:19:33 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 2.3828
2022-02-28 04:19:34 - train: epoch 072, train_loss: 2.4108
2022-02-28 04:20:50 - eval: epoch: 072, acc1: 54.072%, acc5: 77.170%, test_loss: 2.0694, per_image_load_time: 2.533ms, per_image_inference_time: 0.139ms
2022-02-28 04:20:50 - until epoch: 072, best_acc1: 54.072%
2022-02-28 04:20:50 - epoch 073 lr: 0.0010000000000000002
2022-02-28 04:21:28 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 2.6683
2022-02-28 04:22:01 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 2.5164
2022-02-28 04:22:33 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 2.4160
2022-02-28 04:23:07 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 2.2227
2022-02-28 04:23:39 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 2.4456
2022-02-28 04:24:12 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 2.2226
2022-02-28 04:24:46 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 2.3974
2022-02-28 04:25:19 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 2.2232
2022-02-28 04:25:51 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 2.3057
2022-02-28 04:26:25 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 2.4785
2022-02-28 04:26:58 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 2.6727
2022-02-28 04:27:31 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 2.5962
2022-02-28 04:28:05 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 2.6029
2022-02-28 04:28:37 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 2.3190
2022-02-28 04:29:11 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 2.4220
2022-02-28 04:29:43 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 2.3942
2022-02-28 04:30:16 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 2.7835
2022-02-28 04:30:50 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 2.0241
2022-02-28 04:31:24 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 2.5562
2022-02-28 04:31:56 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 2.2686
2022-02-28 04:32:30 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 2.2392
2022-02-28 04:33:04 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 2.2611
2022-02-28 04:33:36 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 2.2982
2022-02-28 04:34:10 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 2.4956
2022-02-28 04:34:44 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 2.4166
2022-02-28 04:35:16 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 2.3916
2022-02-28 04:35:50 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 2.3512
2022-02-28 04:36:23 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 2.4440
2022-02-28 04:36:57 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 2.4775
2022-02-28 04:37:29 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 2.1463
2022-02-28 04:38:03 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 2.4430
2022-02-28 04:38:35 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 2.2305
2022-02-28 04:39:09 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 2.4445
2022-02-28 04:39:43 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 2.4980
2022-02-28 04:40:16 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 2.5046
2022-02-28 04:40:48 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 2.3160
2022-02-28 04:41:22 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 2.4033
2022-02-28 04:41:54 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 2.4425
2022-02-28 04:42:29 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 2.3211
2022-02-28 04:43:02 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 2.4846
2022-02-28 04:43:36 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 2.4057
2022-02-28 04:44:08 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 2.5143
2022-02-28 04:44:43 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 2.2973
2022-02-28 04:45:16 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 2.5249
2022-02-28 04:45:48 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 2.2517
2022-02-28 04:46:22 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 2.5849
2022-02-28 04:46:56 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 2.2476
2022-02-28 04:47:30 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 2.1767
2022-02-28 04:48:03 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 2.3978
2022-02-28 04:48:36 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 2.5387
2022-02-28 04:48:37 - train: epoch 073, train_loss: 2.4054
2022-02-28 04:49:53 - eval: epoch: 073, acc1: 54.172%, acc5: 77.252%, test_loss: 2.0681, per_image_load_time: 2.761ms, per_image_inference_time: 0.123ms
2022-02-28 04:49:53 - until epoch: 073, best_acc1: 54.172%
2022-02-28 04:49:53 - epoch 074 lr: 0.0010000000000000002
2022-02-28 04:50:30 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 2.4543
2022-02-28 04:51:04 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 2.4804
2022-02-28 04:51:38 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 2.3903
2022-02-28 04:52:09 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 2.6342
2022-02-28 04:52:43 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 2.3451
2022-02-28 04:53:17 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 2.3919
2022-02-28 04:53:50 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 2.3449
2022-02-28 04:54:23 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 2.5075
2022-02-28 04:54:56 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 2.1447
2022-02-28 04:55:29 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 2.6100
2022-02-28 04:56:02 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 2.5554
2022-02-28 04:56:36 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 2.3496
2022-02-28 04:57:08 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 2.3511
2022-02-28 04:57:43 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 2.2946
2022-02-28 04:58:14 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 2.5828
2022-02-28 04:58:49 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 2.1602
2022-02-28 04:59:21 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 2.3780
2022-02-28 04:59:56 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 2.4789
2022-02-28 05:00:28 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 2.5134
2022-02-28 05:01:02 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 2.3332
2022-02-28 05:01:35 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 2.3319
2022-02-28 05:02:08 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 2.9301
2022-02-28 05:02:41 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 2.4736
2022-02-28 05:03:15 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 2.5571
2022-02-28 05:03:47 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 2.2548
2022-02-28 05:04:22 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 2.2721
2022-02-28 05:04:54 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 2.4632
2022-02-28 05:05:27 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 2.5595
2022-02-28 05:06:00 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 2.5240
2022-02-28 05:06:34 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 2.4471
2022-02-28 05:07:06 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 2.4843
2022-02-28 05:07:40 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 2.3048
2022-02-28 05:08:12 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 2.4333
2022-02-28 05:08:47 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 2.3630
2022-02-28 05:09:19 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 2.2855
2022-02-28 05:09:53 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 2.4674
2022-02-28 05:10:26 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 2.5378
2022-02-28 05:11:00 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 2.1044
2022-02-28 05:11:32 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 2.2395
2022-02-28 05:12:06 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 2.4776
2022-02-28 05:12:39 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 2.3654
2022-02-28 05:13:14 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 2.2450
2022-02-28 05:13:46 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 2.5432
2022-02-28 05:14:20 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 2.2554
2022-02-28 05:14:52 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 2.3385
2022-02-28 05:15:27 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 2.4842
2022-02-28 05:16:00 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 2.3292
2022-02-28 05:16:34 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 2.3930
2022-02-28 05:17:07 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 2.6476
2022-02-28 05:17:40 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 2.5900
2022-02-28 05:17:42 - train: epoch 074, train_loss: 2.4053
2022-02-28 05:18:58 - eval: epoch: 074, acc1: 54.058%, acc5: 77.234%, test_loss: 2.0660, per_image_load_time: 2.702ms, per_image_inference_time: 0.129ms
2022-02-28 05:18:58 - until epoch: 074, best_acc1: 54.172%
2022-02-28 05:18:58 - epoch 075 lr: 0.0010000000000000002
2022-02-28 05:19:36 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 2.4768
2022-02-28 05:20:10 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 2.5653
2022-02-28 05:20:42 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 2.4716
2022-02-28 05:21:16 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 2.3339
2022-02-28 05:21:49 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 2.3505
2022-02-28 05:22:22 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 2.5263
2022-02-28 05:22:55 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 2.6042
2022-02-28 05:23:30 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 2.3142
2022-02-28 05:24:02 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 2.4348
2022-02-28 05:24:35 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 2.0251
2022-02-28 05:25:08 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 2.4677
2022-02-28 05:25:42 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 2.2579
2022-02-28 05:26:14 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 2.2300
2022-02-28 05:26:47 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 2.0348
2022-02-28 05:27:20 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 2.5152
2022-02-28 05:27:53 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 2.1617
2022-02-28 05:28:25 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 2.4932
2022-02-28 05:28:59 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 2.3377
2022-02-28 05:29:32 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 2.4834
2022-02-28 05:30:06 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 2.1242
2022-02-28 05:30:38 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 2.0870
2022-02-28 05:31:12 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 2.3221
2022-02-28 05:31:44 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 2.2237
2022-02-28 05:32:18 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 2.4281
2022-02-28 05:32:51 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 2.2827
2022-02-28 05:33:24 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 2.5507
2022-02-28 05:33:58 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 2.1805
2022-02-28 05:34:31 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 2.3572
2022-02-28 05:35:04 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 2.5168
2022-02-28 05:35:37 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 2.4974
2022-02-28 05:36:10 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 2.5596
2022-02-28 05:36:44 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 2.4384
2022-02-28 05:37:17 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 2.3126
2022-02-28 05:37:50 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 2.4363
2022-02-28 05:38:24 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 2.3400
2022-02-28 05:38:57 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 2.6302
2022-02-28 05:39:30 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 2.5001
2022-02-28 05:40:04 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 2.1833
2022-02-28 05:40:36 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 2.3527
2022-02-28 05:41:09 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 2.1709
2022-02-28 05:41:43 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 2.3363
2022-02-28 05:42:16 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 2.4873
2022-02-28 05:42:49 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 2.5297
2022-02-28 05:43:22 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 2.2118
2022-02-28 05:43:56 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 2.3763
2022-02-28 05:44:30 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 2.2373
2022-02-28 05:45:04 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 2.6633
2022-02-28 05:45:37 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 2.3639
2022-02-28 05:46:11 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 2.4060
2022-02-28 05:46:43 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 2.3889
2022-02-28 05:46:45 - train: epoch 075, train_loss: 2.4035
2022-02-28 05:48:02 - eval: epoch: 075, acc1: 54.164%, acc5: 77.180%, test_loss: 2.0663, per_image_load_time: 2.777ms, per_image_inference_time: 0.127ms
2022-02-28 05:48:02 - until epoch: 075, best_acc1: 54.172%
2022-02-28 05:48:02 - epoch 076 lr: 0.0010000000000000002
2022-02-28 05:48:39 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 2.3779
2022-02-28 05:49:14 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 2.4219
2022-02-28 05:49:46 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 2.4066
2022-02-28 05:50:20 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 2.4665
2022-02-28 05:50:52 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 2.4444
2022-02-28 05:51:26 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 2.3572
2022-02-28 05:51:58 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 2.2541
2022-02-28 05:52:32 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 2.3673
2022-02-28 05:53:04 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 2.3645
2022-02-28 05:53:38 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 2.4661
2022-02-28 05:54:10 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 2.4258
2022-02-28 05:54:43 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 2.2795
2022-02-28 05:55:17 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 2.3576
2022-02-28 05:55:49 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 2.3340
2022-02-28 05:56:23 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 2.4471
2022-02-28 05:56:56 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 2.3865
2022-02-28 05:57:30 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 2.3822
2022-02-28 05:58:02 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 2.3035
2022-02-28 05:58:35 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 2.4606
2022-02-28 05:59:08 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 2.4379
2022-02-28 05:59:41 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 2.5270
2022-02-28 06:00:13 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 2.3817
2022-02-28 06:00:47 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 2.3072
2022-02-28 06:01:20 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 2.7063
2022-02-28 06:01:53 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 2.2530
2022-02-28 06:02:26 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 2.6491
2022-02-28 06:02:59 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 2.2496
2022-02-28 06:03:32 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 2.4071
2022-02-28 06:04:05 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 2.5080
2022-02-28 06:04:39 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 2.4387
2022-02-28 06:05:11 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 2.4398
2022-02-28 06:05:45 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 2.4326
2022-02-28 06:06:17 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 2.1678
2022-02-28 06:06:52 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 2.3440
2022-02-28 06:07:24 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 2.2624
2022-02-28 06:07:58 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 2.3212
2022-02-28 06:08:32 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 2.4408
2022-02-28 06:09:05 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 2.4814
2022-02-28 06:09:37 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 2.5176
2022-02-28 06:10:11 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 2.5278
2022-02-28 06:10:44 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 2.5490
2022-02-28 06:11:18 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 2.4670
2022-02-28 06:11:51 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 2.4871
2022-02-28 06:12:24 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 2.4642
2022-02-28 06:12:57 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 2.5805
2022-02-28 06:13:31 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 2.3487
2022-02-28 06:14:04 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 2.4870
2022-02-28 06:14:37 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 2.2967
2022-02-28 06:15:11 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 2.4181
2022-02-28 06:15:44 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 2.5121
2022-02-28 06:15:45 - train: epoch 076, train_loss: 2.4005
2022-02-28 06:17:01 - eval: epoch: 076, acc1: 53.924%, acc5: 77.246%, test_loss: 2.0700, per_image_load_time: 1.455ms, per_image_inference_time: 0.128ms
2022-02-28 06:17:01 - until epoch: 076, best_acc1: 54.172%
2022-02-28 06:17:01 - epoch 077 lr: 0.0010000000000000002
2022-02-28 06:17:39 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 2.3599
2022-02-28 06:18:12 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 2.5615
2022-02-28 06:18:44 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 2.3437
2022-02-28 06:19:18 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 2.4182
2022-02-28 06:19:50 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 2.3808
2022-02-28 06:20:24 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 2.2041
2022-02-28 06:20:56 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 2.5205
2022-02-28 06:21:30 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 2.4658
2022-02-28 06:22:03 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 2.4537
2022-02-28 06:22:35 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 2.1839
2022-02-28 06:23:09 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 2.4943
2022-02-28 06:23:43 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 2.6751
2022-02-28 06:24:16 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 2.4428
2022-02-28 06:24:49 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 2.2183
2022-02-28 06:25:21 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 2.2129
2022-02-28 06:25:55 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 2.4472
2022-02-28 06:26:29 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 2.5964
2022-02-28 06:27:01 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 2.1831
2022-02-28 06:27:34 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 2.1784
2022-02-28 06:28:06 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 2.4623
2022-02-28 06:28:40 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 2.6645
2022-02-28 06:29:13 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 2.2045
2022-02-28 06:29:47 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 2.4600
2022-02-28 06:30:20 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 2.3758
2022-02-28 06:30:54 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 2.6051
2022-02-28 06:31:26 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 2.1557
2022-02-28 06:32:00 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 2.1770
2022-02-28 06:32:33 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 2.4308
2022-02-28 06:33:07 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 2.5290
2022-02-28 06:33:40 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 2.3336
2022-02-28 06:34:12 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 2.4733
2022-02-28 06:34:46 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 2.5074
2022-02-28 06:35:18 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 2.1924
2022-02-28 06:35:52 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 2.7017
2022-02-28 06:36:25 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 2.2027
2022-02-28 06:36:58 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 2.3101
2022-02-28 06:37:31 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 2.3960
2022-02-28 06:38:05 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 2.4015
2022-02-28 06:38:37 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 2.2465
2022-02-28 06:39:11 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 2.4974
2022-02-28 06:39:43 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 2.2175
2022-02-28 06:40:17 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 2.4374
2022-02-28 06:40:50 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 2.3011
2022-02-28 06:41:24 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 2.4287
2022-02-28 06:41:57 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 2.5600
2022-02-28 06:42:31 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 2.3988
2022-02-28 06:43:03 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 2.3317
2022-02-28 06:43:37 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 2.4029
2022-02-28 06:44:09 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 2.6140
2022-02-28 06:44:41 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 2.3905
2022-02-28 06:44:43 - train: epoch 077, train_loss: 2.4005
2022-02-28 06:45:58 - eval: epoch: 077, acc1: 54.160%, acc5: 77.274%, test_loss: 2.0602, per_image_load_time: 1.584ms, per_image_inference_time: 0.146ms
2022-02-28 06:45:58 - until epoch: 077, best_acc1: 54.172%
2022-02-28 06:45:58 - epoch 078 lr: 0.0010000000000000002
2022-02-28 06:46:36 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 2.2946
2022-02-28 06:47:10 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 2.4674
2022-02-28 06:47:42 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 2.3119
2022-02-28 06:48:16 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 2.4219
2022-02-28 06:48:49 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 2.2067
2022-02-28 06:49:22 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 2.3714
2022-02-28 06:49:54 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 2.4027
2022-02-28 06:50:29 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 2.4141
2022-02-28 06:51:01 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 2.3299
2022-02-28 06:51:35 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 2.3980
2022-02-28 06:52:08 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 2.1547
2022-02-28 06:52:41 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 2.1137
2022-02-28 06:53:14 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 2.1655
2022-02-28 06:53:48 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 2.0564
2022-02-28 06:54:20 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 2.4211
2022-02-28 06:54:54 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 2.4567
2022-02-28 06:55:27 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 2.5741
2022-02-28 06:56:02 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 2.3448
2022-02-28 06:56:34 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 2.3663
2022-02-28 06:57:08 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 2.2970
2022-02-28 06:57:41 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 2.4747
2022-02-28 06:58:15 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 2.2566
2022-02-28 06:58:47 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 2.2190
2022-02-28 06:59:21 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 2.3337
2022-02-28 06:59:53 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 2.3754
2022-02-28 07:00:27 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 2.3154
2022-02-28 07:01:00 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 2.1922
2022-02-28 07:01:34 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 2.2859
2022-02-28 07:02:07 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 2.3865
2022-02-28 07:02:40 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 2.4793
2022-02-28 07:03:13 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 2.3687
2022-02-28 07:03:46 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 2.5790
2022-02-28 07:04:19 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 2.5692
2022-02-28 07:04:53 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 2.2139
2022-02-28 07:05:26 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 2.3678
2022-02-28 07:06:00 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 2.5745
2022-02-28 07:06:32 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 2.3907
2022-02-28 07:07:06 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 2.0735
2022-02-28 07:07:38 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 2.3127
2022-02-28 07:08:13 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 2.5174
2022-02-28 07:08:45 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 2.2868
2022-02-28 07:09:18 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 2.4393
2022-02-28 07:09:51 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 2.4425
2022-02-28 07:10:24 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 2.3264
2022-02-28 07:10:58 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 2.3674
2022-02-28 07:11:31 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 2.1859
2022-02-28 07:12:04 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 2.2188
2022-02-28 07:12:37 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 2.6087
2022-02-28 07:13:12 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 2.3887
2022-02-28 07:13:44 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 2.3991
2022-02-28 07:13:46 - train: epoch 078, train_loss: 2.3990
2022-02-28 07:15:02 - eval: epoch: 078, acc1: 53.882%, acc5: 77.278%, test_loss: 2.0636, per_image_load_time: 2.780ms, per_image_inference_time: 0.146ms
2022-02-28 07:15:02 - until epoch: 078, best_acc1: 54.172%
2022-02-28 07:15:02 - epoch 079 lr: 0.0010000000000000002
2022-02-28 07:15:40 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 2.3084
2022-02-28 07:16:13 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 2.5035
2022-02-28 07:16:47 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 2.3971
2022-02-28 07:17:19 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 2.3212
2022-02-28 07:17:53 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 2.4059
2022-02-28 07:18:25 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 2.3024
2022-02-28 07:19:00 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 2.3987
2022-02-28 07:19:32 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 2.3987
2022-02-28 07:20:06 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 2.5149
2022-02-28 07:20:39 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 2.3549
2022-02-28 07:21:11 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 2.5100
2022-02-28 07:21:44 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 2.3199
2022-02-28 07:22:18 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 2.3294
2022-02-28 07:22:51 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 2.4438
2022-02-28 07:23:24 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 2.6153
2022-02-28 07:23:57 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 2.0193
2022-02-28 07:24:31 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 2.4077
2022-02-28 07:25:04 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 2.4899
2022-02-28 07:25:38 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 2.3313
2022-02-28 07:26:10 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 2.6927
2022-02-28 07:26:43 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 2.4078
2022-02-28 07:27:17 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 2.4017
2022-02-28 07:27:51 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 2.2363
2022-02-28 07:28:23 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 2.2098
2022-02-28 07:28:57 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 2.2919
2022-02-28 07:29:29 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 2.2746
2022-02-28 07:30:03 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 2.2180
