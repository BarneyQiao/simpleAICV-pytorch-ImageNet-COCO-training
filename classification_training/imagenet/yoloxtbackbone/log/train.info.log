2022-03-11 23:12:32 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 2.1104
2022-03-11 23:13:05 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.7814
2022-03-11 23:13:37 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.8735
2022-03-11 23:14:09 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.6061
2022-03-11 23:14:41 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.9686
2022-03-11 23:15:13 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.8479
2022-03-11 23:15:47 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.8054
2022-03-11 23:16:19 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.6152
2022-03-11 23:16:51 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.6546
2022-03-11 23:17:23 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.6575
2022-03-11 23:17:56 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.7223
2022-03-11 23:18:28 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.8102
2022-03-11 23:19:00 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.6354
2022-03-11 23:19:31 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.7683
2022-03-11 23:20:03 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.7702
2022-03-11 23:20:35 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 2.0453
2022-03-11 23:21:08 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.7534
2022-03-11 23:21:40 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.8972
2022-03-11 23:22:12 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.6692
2022-03-11 23:22:44 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.8430
2022-03-11 23:23:17 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.6826
2022-03-11 23:23:49 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.6245
2022-03-11 23:24:22 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.7780
2022-03-11 23:24:53 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.7946
2022-03-11 23:25:26 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.7930
2022-03-11 23:25:58 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.8716
2022-03-11 23:26:33 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.7683
2022-03-11 23:27:05 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.7748
2022-03-11 23:27:38 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.7445
2022-03-11 23:28:10 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.6963
2022-03-11 23:28:43 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.5725
2022-03-11 23:29:16 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.9241
2022-03-11 23:29:49 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.6781
2022-03-11 23:30:22 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.9987
2022-03-11 23:30:54 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.7867
2022-03-11 23:31:27 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.7989
2022-03-11 23:32:00 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.6175
2022-03-11 23:32:33 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.9233
2022-03-11 23:33:05 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.7030
2022-03-11 23:33:38 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.6236
2022-03-11 23:34:10 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.9853
2022-03-11 23:34:43 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.7203
2022-03-11 23:35:16 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.6355
2022-03-11 23:35:49 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.7583
2022-03-11 23:36:22 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.7045
2022-03-11 23:36:55 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.6698
2022-03-11 23:37:26 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.9712
2022-03-11 23:37:28 - train: epoch 061, train_loss: 1.7727
2022-03-11 23:38:43 - eval: epoch: 061, acc1: 64.044%, acc5: 85.694%, test_loss: 1.4694, per_image_load_time: 2.689ms, per_image_inference_time: 0.163ms
2022-03-11 23:38:43 - until epoch: 061, best_acc1: 64.044%
2022-03-11 23:38:43 - epoch 062 lr: 0.0010000000000000002
2022-03-11 23:39:20 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.9326
2022-03-11 23:39:53 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.9130
2022-03-11 23:40:26 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.8710
2022-03-11 23:40:58 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.7095
2022-03-11 23:41:32 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.8376
2022-03-11 23:42:05 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.5242
2022-03-11 23:42:37 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.7411
2022-03-11 23:43:10 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.8546
2022-03-11 23:43:42 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.9843
2022-03-11 23:44:15 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.6743
2022-03-11 23:44:47 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.7906
2022-03-11 23:45:20 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.7354
2022-03-11 23:45:52 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.7930
2022-03-11 23:46:24 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.6537
2022-03-11 23:46:57 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.8822
2022-03-11 23:47:29 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.8627
2022-03-11 23:48:01 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.8041
2022-03-11 23:48:34 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.6559
2022-03-11 23:49:06 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.6286
2022-03-11 23:49:39 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.8887
2022-03-11 23:50:12 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.8652
2022-03-11 23:50:44 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.6013
2022-03-11 23:51:17 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.6294
2022-03-11 23:51:49 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.6939
2022-03-11 23:52:22 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.7988
2022-03-11 23:52:54 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.7397
2022-03-11 23:53:26 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.5890
2022-03-11 23:53:58 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.8875
2022-03-11 23:54:31 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.7677
2022-03-11 23:55:03 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.7800
2022-03-11 23:55:36 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.6988
2022-03-11 23:56:08 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.6049
2022-03-11 23:56:41 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.8371
2022-03-11 23:57:13 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.7729
2022-03-11 23:57:45 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.8131
2022-03-11 23:58:17 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.7897
2022-03-11 23:58:49 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.7093
2022-03-11 23:59:22 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.6689
2022-03-11 23:59:55 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.8012
2022-03-12 00:00:27 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.4522
2022-03-12 00:01:01 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.8428
2022-03-12 00:01:33 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 1.5699
2022-03-12 00:02:06 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.7395
2022-03-12 00:02:38 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.7343
2022-03-12 00:03:12 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.7101
2022-03-12 00:03:44 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 1.5409
2022-03-12 00:04:17 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.6716
2022-03-12 00:04:49 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.6643
2022-03-12 00:05:22 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.6567
2022-03-12 00:05:53 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.6852
2022-03-12 00:05:55 - train: epoch 062, train_loss: 1.7321
2022-03-12 00:07:10 - eval: epoch: 062, acc1: 64.410%, acc5: 85.992%, test_loss: 1.4516, per_image_load_time: 2.663ms, per_image_inference_time: 0.170ms
2022-03-12 00:07:10 - until epoch: 062, best_acc1: 64.410%
2022-03-12 00:07:10 - epoch 063 lr: 0.0010000000000000002
2022-03-12 00:07:47 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 1.8930
2022-03-12 00:08:20 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.7453
2022-03-12 00:08:52 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.8496
2022-03-12 00:09:24 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.6479
2022-03-12 00:09:58 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.6136
2022-03-12 00:10:29 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.7646
2022-03-12 00:11:03 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.7217
2022-03-12 00:11:34 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.7799
2022-03-12 00:12:07 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 2.0728
2022-03-12 00:12:40 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.8315
2022-03-12 00:13:12 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.5491
2022-03-12 00:13:44 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.7711
2022-03-12 00:14:18 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.4441
2022-03-12 00:14:50 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.9421
2022-03-12 00:15:23 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.6346
2022-03-12 00:15:55 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.7087
2022-03-12 00:16:27 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.5969
2022-03-12 00:16:59 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.6846
2022-03-12 00:17:32 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.9103
2022-03-12 00:18:05 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 1.7155
2022-03-12 00:18:37 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.6472
2022-03-12 00:19:10 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.9599
2022-03-12 00:19:43 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 2.0230
2022-03-12 00:20:15 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.6977
2022-03-12 00:20:48 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.6761
2022-03-12 00:21:21 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.5785
2022-03-12 00:21:53 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.9022
2022-03-12 00:22:27 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.8459
2022-03-12 00:22:59 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.6749
2022-03-12 00:23:32 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.9595
2022-03-12 00:24:05 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.9050
2022-03-12 00:24:38 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.6954
2022-03-12 00:25:10 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.5969
2022-03-12 00:25:44 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.5602
2022-03-12 00:26:16 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.9765
2022-03-12 00:26:49 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.7809
2022-03-12 00:27:22 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.9083
2022-03-12 00:27:54 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.7497
2022-03-12 00:28:28 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 1.5200
2022-03-12 00:29:00 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 1.5361
2022-03-12 00:29:33 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.6751
2022-03-12 00:30:06 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.7827
2022-03-12 00:30:39 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.8164
2022-03-12 00:31:12 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.5975
2022-03-12 00:31:45 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.8605
2022-03-12 00:32:17 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.8162
2022-03-12 00:32:51 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.6804
2022-03-12 00:33:24 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.6150
2022-03-12 00:33:57 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.5481
2022-03-12 00:34:28 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.7880
2022-03-12 00:34:31 - train: epoch 063, train_loss: 1.7179
2022-03-12 00:35:45 - eval: epoch: 063, acc1: 64.658%, acc5: 86.094%, test_loss: 1.4435, per_image_load_time: 2.666ms, per_image_inference_time: 0.158ms
2022-03-12 00:35:45 - until epoch: 063, best_acc1: 64.658%
2022-03-12 00:35:45 - epoch 064 lr: 0.0010000000000000002
2022-03-12 00:36:22 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.6096
2022-03-12 00:36:56 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.7616
2022-03-12 00:37:28 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.5962
2022-03-12 00:38:01 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.7767
2022-03-12 00:38:33 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.5214
2022-03-12 00:39:06 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.9808
2022-03-12 00:39:38 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.8692
2022-03-12 00:40:11 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 1.6248
2022-03-12 00:40:43 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.6118
2022-03-12 00:41:16 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.6035
2022-03-12 00:41:48 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.6036
2022-03-12 00:42:20 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.5257
2022-03-12 00:42:53 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.8991
2022-03-12 00:43:25 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 2.0098
2022-03-12 00:43:58 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.7210
2022-03-12 00:44:31 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.4681
2022-03-12 00:45:02 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.7461
2022-03-12 00:45:36 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.5415
2022-03-12 00:46:07 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.7255
2022-03-12 00:46:40 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.5922
2022-03-12 00:47:13 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 1.7382
2022-03-12 00:47:45 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.8100
2022-03-12 00:48:18 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.8693
2022-03-12 00:48:51 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.6233
2022-03-12 00:49:24 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 1.6823
2022-03-12 00:49:58 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.6081
2022-03-12 00:50:29 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.6812
2022-03-12 00:51:04 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.5882
2022-03-12 00:51:36 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.7545
2022-03-12 00:52:10 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.7433
2022-03-12 00:52:42 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.6789
2022-03-12 00:53:16 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.6333
2022-03-12 00:53:48 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.6945
2022-03-12 00:54:22 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.8668
2022-03-12 00:54:53 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.5921
2022-03-12 00:55:27 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.7562
2022-03-12 00:56:00 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 1.4768
2022-03-12 00:56:34 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.7075
2022-03-12 00:57:05 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.7455
2022-03-12 00:57:39 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.7136
2022-03-12 00:58:11 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.6654
2022-03-12 00:58:43 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 1.4989
2022-03-12 00:59:16 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.7802
2022-03-12 00:59:49 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.6129
2022-03-12 01:00:22 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 1.5142
2022-03-12 01:00:54 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.8178
2022-03-12 01:01:28 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 2.0900
2022-03-12 01:01:59 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.7354
2022-03-12 01:02:32 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.9979
2022-03-12 01:03:04 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.5552
2022-03-12 01:03:06 - train: epoch 064, train_loss: 1.7101
2022-03-12 01:04:21 - eval: epoch: 064, acc1: 64.806%, acc5: 86.100%, test_loss: 1.4392, per_image_load_time: 2.679ms, per_image_inference_time: 0.148ms
2022-03-12 01:04:21 - until epoch: 064, best_acc1: 64.806%
2022-03-12 01:04:21 - epoch 065 lr: 0.0010000000000000002
2022-03-12 01:04:59 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.7314
2022-03-12 01:05:32 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.8125
2022-03-12 01:06:05 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.6564
2022-03-12 01:06:36 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.6276
2022-03-12 01:07:09 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.7715
2022-03-12 01:07:41 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.7314
2022-03-12 01:08:15 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.7936
2022-03-12 01:08:46 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.6510
2022-03-12 01:09:19 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.8212
2022-03-12 01:09:51 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.7262
2022-03-12 01:10:24 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.6611
2022-03-12 01:10:57 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.9258
2022-03-12 01:11:30 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.8128
2022-03-12 01:12:03 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 1.5183
2022-03-12 01:12:35 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.8293
2022-03-12 01:13:07 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.8979
2022-03-12 01:13:39 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.7851
2022-03-12 01:14:12 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.5295
2022-03-12 01:14:45 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 1.5041
2022-03-12 01:15:18 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.7295
2022-03-12 01:15:50 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.5821
2022-03-12 01:16:23 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.7359
2022-03-12 01:16:55 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.7736
2022-03-12 01:17:28 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.7504
2022-03-12 01:18:00 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.8041
2022-03-12 01:18:33 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.7953
2022-03-12 01:19:06 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.7898
2022-03-12 01:19:39 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.7423
2022-03-12 01:20:11 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.6942
2022-03-12 01:20:44 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.6221
2022-03-12 01:21:16 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.7892
2022-03-12 01:21:49 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.8694
2022-03-12 01:22:21 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.8072
2022-03-12 01:22:54 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.6914
2022-03-12 01:23:26 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.8827
2022-03-12 01:23:59 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.6383
2022-03-12 01:24:32 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.5898
2022-03-12 01:25:04 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.7178
2022-03-12 01:25:37 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.9147
2022-03-12 01:26:09 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.7569
2022-03-12 01:26:42 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.7210
2022-03-12 01:27:14 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.7774
2022-03-12 01:27:47 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.6578
2022-03-12 01:28:19 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.7118
2022-03-12 01:28:51 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.7178
2022-03-12 01:29:24 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.7771
2022-03-12 01:29:57 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.6657
2022-03-12 01:30:29 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 1.3743
2022-03-12 01:31:03 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.7398
2022-03-12 01:31:33 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.5141
2022-03-12 01:31:36 - train: epoch 065, train_loss: 1.7040
2022-03-12 01:32:49 - eval: epoch: 065, acc1: 64.760%, acc5: 86.220%, test_loss: 1.4330, per_image_load_time: 2.659ms, per_image_inference_time: 0.150ms
2022-03-12 01:32:49 - until epoch: 065, best_acc1: 64.806%
2022-03-12 01:32:49 - epoch 066 lr: 0.0010000000000000002
2022-03-12 01:33:27 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.6689
2022-03-12 01:33:59 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.9177
2022-03-12 01:34:31 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 1.5844
2022-03-12 01:35:04 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 1.3242
2022-03-12 01:35:36 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.7963
2022-03-12 01:36:08 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.9137
2022-03-12 01:36:40 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.5362
2022-03-12 01:37:12 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.7551
2022-03-12 01:37:45 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.5808
2022-03-12 01:38:17 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.6370
2022-03-12 01:38:49 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.7442
2022-03-12 01:39:21 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.8657
2022-03-12 01:39:54 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.7333
2022-03-12 01:40:26 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 1.5852
2022-03-12 01:41:00 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.8483
2022-03-12 01:41:31 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.6260
2022-03-12 01:42:05 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.6721
2022-03-12 01:42:37 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.4733
2022-03-12 01:43:10 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.9583
2022-03-12 01:43:43 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.4799
2022-03-12 01:44:14 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.8168
2022-03-12 01:44:47 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 1.4145
2022-03-12 01:45:20 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.7957
2022-03-12 01:45:53 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.7660
2022-03-12 01:46:25 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.6183
2022-03-12 01:46:57 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.6204
2022-03-12 01:47:30 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.7714
2022-03-12 01:48:03 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.5540
2022-03-12 01:48:35 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.8065
2022-03-12 01:49:09 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.6379
2022-03-12 01:49:40 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.8087
2022-03-12 01:50:13 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.8416
2022-03-12 01:50:46 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.6700
2022-03-12 01:51:18 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.8459
2022-03-12 01:51:51 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.6890
2022-03-12 01:52:24 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.7583
2022-03-12 01:52:56 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.5972
2022-03-12 01:53:29 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.8932
2022-03-12 01:54:02 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.6328
2022-03-12 01:54:34 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.7798
2022-03-12 01:55:07 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 1.6415
2022-03-12 01:55:39 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 1.6694
2022-03-12 01:56:12 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.5884
2022-03-12 01:56:45 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.7748
2022-03-12 01:57:17 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.9065
2022-03-12 01:57:50 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.9322
2022-03-12 01:58:23 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.5179
2022-03-12 01:58:56 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.7093
2022-03-12 01:59:28 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.4716
2022-03-12 02:00:01 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 1.6571
2022-03-12 02:00:03 - train: epoch 066, train_loss: 1.6966
2022-03-12 02:01:17 - eval: epoch: 066, acc1: 64.900%, acc5: 86.272%, test_loss: 1.4296, per_image_load_time: 1.818ms, per_image_inference_time: 0.160ms
2022-03-12 02:01:17 - until epoch: 066, best_acc1: 64.900%
2022-03-12 02:01:17 - epoch 067 lr: 0.0010000000000000002
2022-03-12 02:01:55 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.5796
2022-03-12 02:02:29 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.7172
2022-03-12 02:03:00 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.5696
2022-03-12 02:03:33 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.6503
2022-03-12 02:04:05 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.6170
2022-03-12 02:04:38 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 1.6763
2022-03-12 02:05:09 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.5846
2022-03-12 02:05:42 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.5815
2022-03-12 02:06:14 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 2.0452
2022-03-12 02:06:48 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 1.7356
2022-03-12 02:07:20 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 1.7057
2022-03-12 02:07:53 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.5940
2022-03-12 02:08:25 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.7771
2022-03-12 02:08:58 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.8301
2022-03-12 02:09:30 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.6364
2022-03-12 02:10:03 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.8275
2022-03-12 02:10:36 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.5391
2022-03-12 02:11:08 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.9384
2022-03-12 02:11:41 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.6653
2022-03-12 02:12:13 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.6961
2022-03-12 02:12:46 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 1.4387
2022-03-12 02:13:18 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.6193
2022-03-12 02:13:52 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.6966
2022-03-12 02:14:24 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 1.5264
2022-03-12 02:14:57 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 1.7342
2022-03-12 02:15:29 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.6779
2022-03-12 02:16:02 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.6355
2022-03-12 02:16:35 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.7658
2022-03-12 02:17:07 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.7407
2022-03-12 02:17:40 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.6402
2022-03-12 02:18:12 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 1.4590
2022-03-12 02:18:45 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.7646
2022-03-12 02:19:18 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.4766
2022-03-12 02:19:50 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.7227
2022-03-12 02:20:23 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.7056
2022-03-12 02:20:56 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.6972
2022-03-12 02:21:28 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.8969
2022-03-12 02:22:01 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.8612
2022-03-12 02:22:33 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 1.9272
2022-03-12 02:23:06 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.6986
2022-03-12 02:23:39 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.7150
2022-03-12 02:24:11 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.7209
2022-03-12 02:24:45 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.5061
2022-03-12 02:25:17 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.5369
2022-03-12 02:25:50 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 1.5313
2022-03-12 02:26:23 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.5643
2022-03-12 02:26:57 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.7409
2022-03-12 02:27:28 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.4568
2022-03-12 02:28:02 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.8191
2022-03-12 02:28:33 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.8182
2022-03-12 02:28:36 - train: epoch 067, train_loss: 1.6936
2022-03-12 02:29:50 - eval: epoch: 067, acc1: 64.890%, acc5: 86.382%, test_loss: 1.4267, per_image_load_time: 2.674ms, per_image_inference_time: 0.165ms
2022-03-12 02:29:50 - until epoch: 067, best_acc1: 64.900%
2022-03-12 02:29:50 - epoch 068 lr: 0.0010000000000000002
2022-03-12 02:30:27 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.7351
2022-03-12 02:31:00 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.7325
2022-03-12 02:31:33 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.6804
2022-03-12 02:32:05 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.5810
2022-03-12 02:32:37 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.5963
2022-03-12 02:33:09 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.8614
2022-03-12 02:33:42 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.7724
2022-03-12 02:34:14 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.6485
2022-03-12 02:34:47 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.7663
2022-03-12 02:35:19 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.6694
2022-03-12 02:35:51 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.8426
2022-03-12 02:36:24 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 1.5154
2022-03-12 02:36:56 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.6447
2022-03-12 02:37:30 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.7715
2022-03-12 02:38:02 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.7827
2022-03-12 02:38:34 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.5851
2022-03-12 02:39:06 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.7521
2022-03-12 02:39:39 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.6749
2022-03-12 02:40:11 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.7239
2022-03-12 02:40:43 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.6809
2022-03-12 02:41:16 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.6724
2022-03-12 02:41:48 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.6358
2022-03-12 02:42:21 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 1.5314
2022-03-12 02:42:53 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.6873
2022-03-12 02:43:26 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.7354
2022-03-12 02:43:58 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.5536
2022-03-12 02:44:31 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.7976
2022-03-12 02:45:03 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.8380
2022-03-12 02:45:36 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.7968
2022-03-12 02:46:08 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.7495
2022-03-12 02:46:41 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.7036
2022-03-12 02:47:13 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.7953
2022-03-12 02:47:46 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.7119
2022-03-12 02:48:18 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 1.7216
2022-03-12 02:48:50 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.6528
2022-03-12 02:49:23 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.6776
2022-03-12 02:49:56 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.7423
2022-03-12 02:50:28 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.8071
2022-03-12 02:51:01 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.8995
2022-03-12 02:51:33 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.8225
2022-03-12 02:52:06 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.4973
2022-03-12 02:52:38 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.7509
2022-03-12 02:53:11 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 1.7563
2022-03-12 02:53:44 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.5338
2022-03-12 02:54:16 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.7130
2022-03-12 02:54:49 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.6696
2022-03-12 02:55:21 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.9795
2022-03-12 02:55:54 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.7360
2022-03-12 02:56:27 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.7389
2022-03-12 02:56:59 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.7120
2022-03-12 02:57:02 - train: epoch 068, train_loss: 1.6893
2022-03-12 02:58:17 - eval: epoch: 068, acc1: 64.942%, acc5: 86.358%, test_loss: 1.4247, per_image_load_time: 2.682ms, per_image_inference_time: 0.159ms
2022-03-12 02:58:17 - until epoch: 068, best_acc1: 64.942%
2022-03-12 02:58:17 - epoch 069 lr: 0.0010000000000000002
2022-03-12 02:58:54 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.9445
2022-03-12 02:59:28 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.8124
2022-03-12 03:00:00 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.7458
2022-03-12 03:00:32 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.7418
2022-03-12 03:01:05 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.8357
2022-03-12 03:01:37 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 1.6141
2022-03-12 03:02:11 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.7951
2022-03-12 03:02:42 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.6536
2022-03-12 03:03:15 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.5886
2022-03-12 03:03:47 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.6470
2022-03-12 03:04:21 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.6384
2022-03-12 03:04:54 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.4829
2022-03-12 03:05:27 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.9134
2022-03-12 03:05:59 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.5358
2022-03-12 03:06:32 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.6100
2022-03-12 03:07:05 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.7788
2022-03-12 03:07:38 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.7505
2022-03-12 03:08:10 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.7496
2022-03-12 03:08:43 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.7045
2022-03-12 03:09:15 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 1.5926
2022-03-12 03:09:49 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.8271
2022-03-12 03:10:21 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.7374
2022-03-12 03:10:54 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.5641
2022-03-12 03:11:27 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.7145
2022-03-12 03:12:00 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.5700
2022-03-12 03:12:31 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.6405
2022-03-12 03:13:04 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.8146
2022-03-12 03:13:38 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.7312
2022-03-12 03:14:10 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 1.5261
2022-03-12 03:14:42 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.5818
2022-03-12 03:15:15 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 2.0698
2022-03-12 03:15:48 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 1.5837
2022-03-12 03:16:20 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.6097
2022-03-12 03:16:53 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 1.5017
2022-03-12 03:17:25 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 1.5503
2022-03-12 03:17:58 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.4376
2022-03-12 03:18:31 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.7505
2022-03-12 03:19:04 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.6476
2022-03-12 03:19:36 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.7761
2022-03-12 03:20:09 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.7996
2022-03-12 03:20:41 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.7098
2022-03-12 03:21:15 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.9106
2022-03-12 03:21:47 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.7221
2022-03-12 03:22:20 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.7934
2022-03-12 03:22:52 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.7873
2022-03-12 03:23:26 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.9964
2022-03-12 03:23:58 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.7743
2022-03-12 03:24:32 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.7202
2022-03-12 03:25:04 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 1.5000
2022-03-12 03:25:36 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.6670
2022-03-12 03:25:39 - train: epoch 069, train_loss: 1.6853
2022-03-12 03:26:53 - eval: epoch: 069, acc1: 65.062%, acc5: 86.396%, test_loss: 1.4229, per_image_load_time: 2.038ms, per_image_inference_time: 0.158ms
2022-03-12 03:26:53 - until epoch: 069, best_acc1: 65.062%
2022-03-12 03:26:53 - epoch 070 lr: 0.0010000000000000002
2022-03-12 03:27:30 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.8996
2022-03-12 03:28:03 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.6341
2022-03-12 03:28:35 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.6783
2022-03-12 03:29:08 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.6343
2022-03-12 03:29:40 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.9652
2022-03-12 03:30:14 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.5932
2022-03-12 03:30:46 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.6123
2022-03-12 03:31:20 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.5807
2022-03-12 03:31:51 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.5603
2022-03-12 03:32:24 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.5823
2022-03-12 03:32:57 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.6962
2022-03-12 03:33:30 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 1.6723
2022-03-12 03:34:01 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.6982
2022-03-12 03:34:34 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.4802
2022-03-12 03:35:07 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 1.6288
2022-03-12 03:35:39 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.7613
2022-03-12 03:36:13 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.5826
2022-03-12 03:36:45 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.4717
2022-03-12 03:37:18 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.5919
2022-03-12 03:37:51 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.6730
2022-03-12 03:38:23 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 1.7735
2022-03-12 03:38:56 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.6646
2022-03-12 03:39:29 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.9318
2022-03-12 03:40:02 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.6488
2022-03-12 03:40:34 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.7641
2022-03-12 03:41:07 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.5655
2022-03-12 03:41:39 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.7664
2022-03-12 03:42:12 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.7770
2022-03-12 03:42:44 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.7377
2022-03-12 03:43:17 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.7629
2022-03-12 03:43:50 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.6662
2022-03-12 03:44:22 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.8727
2022-03-12 03:44:55 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 1.6015
2022-03-12 03:45:28 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.6561
2022-03-12 03:46:01 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.5947
2022-03-12 03:46:34 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.5733
2022-03-12 03:47:07 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.6495
2022-03-12 03:47:39 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.6342
2022-03-12 03:48:12 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 1.6626
2022-03-12 03:48:45 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.7494
2022-03-12 03:49:18 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 1.7180
2022-03-12 03:49:50 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.8259
2022-03-12 03:50:24 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.7766
2022-03-12 03:50:56 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.6464
2022-03-12 03:51:29 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.5129
2022-03-12 03:52:02 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.7129
2022-03-12 03:52:34 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.6583
2022-03-12 03:53:08 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.7197
2022-03-12 03:53:41 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.8039
2022-03-12 03:54:13 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.7680
2022-03-12 03:54:15 - train: epoch 070, train_loss: 1.6800
2022-03-12 03:55:29 - eval: epoch: 070, acc1: 65.184%, acc5: 86.352%, test_loss: 1.4180, per_image_load_time: 2.660ms, per_image_inference_time: 0.163ms
2022-03-12 03:55:29 - until epoch: 070, best_acc1: 65.184%
2022-03-12 03:55:29 - epoch 071 lr: 0.0010000000000000002
2022-03-12 03:56:07 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.5022
2022-03-12 03:56:40 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.5981
2022-03-12 03:57:12 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.8809
2022-03-12 03:57:45 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.7661
2022-03-12 03:58:17 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.8270
2022-03-12 03:58:48 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.8475
2022-03-12 03:59:22 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.5641
2022-03-12 03:59:55 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.7225
2022-03-12 04:00:27 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.6389
2022-03-12 04:01:00 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.6476
2022-03-12 04:01:32 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.7589
2022-03-12 04:02:05 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.4614
2022-03-12 04:02:38 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.7038
2022-03-12 04:03:11 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.5579
2022-03-12 04:03:43 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 1.5852
2022-03-12 04:04:16 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 1.5562
2022-03-12 04:04:49 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.6284
2022-03-12 04:05:21 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.7379
2022-03-12 04:05:53 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 1.6188
2022-03-12 04:06:26 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.7702
2022-03-12 04:06:59 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.7303
2022-03-12 04:07:32 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 1.3392
2022-03-12 04:08:05 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 1.5049
2022-03-12 04:08:38 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.6500
2022-03-12 04:09:11 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.7841
2022-03-12 04:09:43 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.6406
2022-03-12 04:10:16 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.6201
2022-03-12 04:10:49 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.8598
2022-03-12 04:11:22 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.5816
2022-03-12 04:11:55 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.9284
2022-03-12 04:12:27 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.5676
2022-03-12 04:13:01 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.7327
2022-03-12 04:13:34 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.8190
2022-03-12 04:14:07 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.3905
2022-03-12 04:14:40 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.6993
2022-03-12 04:15:13 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.5830
2022-03-12 04:15:46 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.8011
2022-03-12 04:16:19 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.5909
2022-03-12 04:16:51 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.6721
2022-03-12 04:17:25 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.6437
2022-03-12 04:17:57 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.6405
2022-03-12 04:18:31 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.7902
2022-03-12 04:19:03 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.5298
2022-03-12 04:19:36 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.8092
2022-03-12 04:20:09 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.8077
2022-03-12 04:20:41 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.7886
2022-03-12 04:21:15 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.6548
2022-03-12 04:21:47 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 1.7586
2022-03-12 04:22:21 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 1.4507
2022-03-12 04:22:52 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.6329
2022-03-12 04:22:55 - train: epoch 071, train_loss: 1.6769
2022-03-12 04:24:10 - eval: epoch: 071, acc1: 65.168%, acc5: 86.442%, test_loss: 1.4164, per_image_load_time: 2.658ms, per_image_inference_time: 0.147ms
2022-03-12 04:24:10 - until epoch: 071, best_acc1: 65.184%
2022-03-12 04:24:10 - epoch 072 lr: 0.0010000000000000002
2022-03-12 04:24:47 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.8166
2022-03-12 04:25:21 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.6025
2022-03-12 04:25:53 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.5496
2022-03-12 04:26:25 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.7120
2022-03-12 04:26:57 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.6352
2022-03-12 04:27:30 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.5119
2022-03-12 04:28:02 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.9058
2022-03-12 04:28:34 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.9179
2022-03-12 04:29:07 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.6938
2022-03-12 04:29:40 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.5550
2022-03-12 04:30:13 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 1.7568
2022-03-12 04:30:45 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 1.5911
2022-03-12 04:31:18 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.8057
2022-03-12 04:31:49 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.6142
2022-03-12 04:32:23 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.7101
2022-03-12 04:32:54 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.5637
2022-03-12 04:33:28 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 1.5147
2022-03-12 04:34:00 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.5154
2022-03-12 04:34:32 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.7936
2022-03-12 04:35:04 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.5442
2022-03-12 04:35:37 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.8962
2022-03-12 04:36:09 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.8591
2022-03-12 04:36:43 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.8376
2022-03-12 04:37:15 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 1.5212
2022-03-12 04:37:48 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.7284
2022-03-12 04:38:20 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 1.5568
2022-03-12 04:38:52 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 1.7435
2022-03-12 04:39:25 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.7706
2022-03-12 04:39:58 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.5713
2022-03-12 04:40:30 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.6065
2022-03-12 04:41:02 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.6516
2022-03-12 04:41:35 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.6689
2022-03-12 04:42:09 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.6617
2022-03-12 04:42:41 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 1.8083
2022-03-12 04:43:13 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 1.4406
2022-03-12 04:43:47 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.6494
2022-03-12 04:44:19 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.8421
2022-03-12 04:44:53 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.4471
2022-03-12 04:45:24 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.6236
2022-03-12 04:45:58 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.6137
2022-03-12 04:46:30 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.8160
2022-03-12 04:47:03 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.5220
2022-03-12 04:47:36 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.6299
2022-03-12 04:48:08 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 1.6901
2022-03-12 04:48:41 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.6936
2022-03-12 04:49:14 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.4431
2022-03-12 04:49:47 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.6944
2022-03-12 04:50:19 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.7126
2022-03-12 04:50:53 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.6494
2022-03-12 04:51:25 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.6585
2022-03-12 04:51:27 - train: epoch 072, train_loss: 1.6733
2022-03-12 04:52:43 - eval: epoch: 072, acc1: 65.122%, acc5: 86.446%, test_loss: 1.4186, per_image_load_time: 2.809ms, per_image_inference_time: 0.135ms
2022-03-12 04:52:43 - until epoch: 072, best_acc1: 65.184%
2022-03-12 09:00:07 - epoch 073 lr: 0.0010000000000000002
2022-03-12 09:00:44 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.9716
2022-03-12 09:01:18 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.6803
2022-03-12 09:01:49 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.8679
2022-03-12 09:02:23 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 1.4625
2022-03-12 09:02:55 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.5068
2022-03-12 09:03:28 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.6531
2022-03-12 09:04:02 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.7589
2022-03-12 09:04:34 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.6503
2022-03-12 09:05:07 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.5525
2022-03-12 09:05:40 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 1.5555
2022-03-12 09:06:13 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.7474
2022-03-12 09:06:45 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.6399
2022-03-12 09:07:19 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.6077
2022-03-12 09:07:51 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.6948
2022-03-12 09:08:25 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.6367
2022-03-12 09:08:57 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.7403
2022-03-12 09:09:30 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.8860
2022-03-12 09:10:02 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 1.5158
2022-03-12 09:10:35 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.5955
2022-03-12 09:11:08 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.5438
2022-03-12 09:11:41 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.6696
2022-03-12 09:12:14 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.5527
2022-03-12 09:12:46 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.5783
2022-03-12 09:13:20 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.9221
2022-03-12 09:13:52 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.8455
2022-03-12 09:14:26 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 1.5641
2022-03-12 09:14:59 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.6465
2022-03-12 09:15:31 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.5850
2022-03-12 09:16:04 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.8425
2022-03-12 09:16:37 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 1.6658
2022-03-12 09:17:09 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.7869
2022-03-12 09:17:42 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 1.4743
2022-03-12 09:18:14 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 1.7768
2022-03-12 09:18:48 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 1.5066
2022-03-12 09:19:21 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.6558
2022-03-12 09:19:53 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 1.5914
2022-03-12 09:20:27 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.6473
2022-03-12 09:20:59 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.8403
2022-03-12 09:21:33 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.6768
2022-03-12 09:22:04 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.9015
2022-03-12 09:22:38 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 1.5459
2022-03-12 09:23:10 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.8786
2022-03-12 09:23:44 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.7720
2022-03-12 09:24:16 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.6858
2022-03-12 09:24:49 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.5475
2022-03-12 09:25:22 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.7893
2022-03-12 09:25:55 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.4273
2022-03-12 09:26:28 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.5618
2022-03-12 09:27:01 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.7919
2022-03-12 09:27:32 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.6875
2022-03-12 09:27:33 - train: epoch 073, train_loss: 1.6731
2022-03-12 09:28:47 - eval: epoch: 073, acc1: 65.234%, acc5: 86.414%, test_loss: 1.4160, per_image_load_time: 2.574ms, per_image_inference_time: 0.182ms
2022-03-12 09:28:47 - until epoch: 073, best_acc1: 65.234%
2022-03-12 09:28:47 - epoch 074 lr: 0.0010000000000000002
2022-03-12 09:29:26 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.8581
2022-03-12 09:29:58 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 1.9788
2022-03-12 09:30:31 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.5745
2022-03-12 09:31:04 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.9534
2022-03-12 09:31:36 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.5545
2022-03-12 09:32:09 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.6514
2022-03-12 09:32:41 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 1.5544
2022-03-12 09:33:15 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.8620
2022-03-12 09:33:47 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.6112
2022-03-12 09:34:21 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.8801
2022-03-12 09:34:53 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.6252
2022-03-12 09:35:26 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 1.7429
2022-03-12 09:35:59 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.5965
2022-03-12 09:36:33 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 1.6207
2022-03-12 09:37:05 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.7282
2022-03-12 09:37:37 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 1.5967
2022-03-12 09:38:10 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.8727
2022-03-12 09:38:43 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.8498
2022-03-12 09:39:17 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.6579
2022-03-12 09:39:50 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 1.5602
2022-03-12 09:40:24 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.6541
2022-03-12 09:40:56 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.9842
2022-03-12 09:41:30 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.8216
2022-03-12 09:42:02 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.6861
2022-03-12 09:42:36 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.5380
2022-03-12 09:43:09 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.6464
2022-03-12 09:43:41 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 1.7320
2022-03-12 09:44:15 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.7656
2022-03-12 09:44:49 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.7361
2022-03-12 09:45:22 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.7051
2022-03-12 09:45:55 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.6875
2022-03-12 09:46:29 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 1.6900
2022-03-12 09:47:04 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.7065
2022-03-12 09:47:36 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.6040
2022-03-12 09:48:10 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.6350
2022-03-12 09:48:43 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 1.8209
2022-03-12 09:49:16 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.7333
2022-03-12 09:49:50 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.6103
2022-03-12 09:50:23 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 1.5395
2022-03-12 09:50:57 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 1.7798
2022-03-12 09:51:29 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.6019
2022-03-12 09:52:02 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.6951
2022-03-12 09:52:35 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.7716
2022-03-12 09:53:09 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 1.5415
2022-03-12 09:53:40 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 1.7608
2022-03-12 09:54:13 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.7942
2022-03-12 09:54:47 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.4104
2022-03-12 09:55:20 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.6537
2022-03-12 09:55:53 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.7650
2022-03-12 09:56:25 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.7131
2022-03-12 09:56:25 - train: epoch 074, train_loss: 1.6680
2022-03-12 09:57:39 - eval: epoch: 074, acc1: 65.316%, acc5: 86.400%, test_loss: 1.4108, per_image_load_time: 2.655ms, per_image_inference_time: 0.177ms
2022-03-12 09:57:39 - until epoch: 074, best_acc1: 65.316%
2022-03-12 09:57:39 - epoch 075 lr: 0.0010000000000000002
2022-03-12 09:58:18 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 1.7543
2022-03-12 09:58:51 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 1.6008
2022-03-12 09:59:24 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.6926
2022-03-12 09:59:58 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.6451
2022-03-12 10:00:30 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.6573
2022-03-12 10:01:03 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.6255
2022-03-12 10:01:36 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.6957
2022-03-12 10:02:09 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.5045
2022-03-12 10:02:42 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.6739
2022-03-12 10:03:15 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 1.4795
2022-03-12 10:03:48 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.8130
2022-03-12 10:04:21 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.4554
2022-03-12 10:04:54 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.7253
2022-03-12 10:05:27 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.5995
2022-03-12 10:06:01 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.7862
2022-03-12 10:06:33 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 1.5096
2022-03-12 10:07:07 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 1.5537
2022-03-12 10:07:40 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.7839
2022-03-12 10:08:13 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 1.6759
2022-03-12 10:08:46 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.6717
2022-03-12 10:09:20 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.5452
2022-03-12 10:09:53 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.6960
2022-03-12 10:10:27 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 1.7307
2022-03-12 10:11:00 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.7140
2022-03-12 10:11:33 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.6861
2022-03-12 10:12:06 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.7320
2022-03-12 10:12:39 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 1.4526
2022-03-12 10:13:13 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.6914
2022-03-12 10:13:45 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.8179
2022-03-12 10:14:19 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.7459
2022-03-12 10:14:52 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.7244
2022-03-12 10:15:25 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.6829
2022-03-12 10:15:59 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.8845
2022-03-12 10:16:32 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 1.5931
2022-03-12 10:17:06 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.6631
2022-03-12 10:17:39 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.7572
2022-03-12 10:18:12 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.5856
2022-03-12 10:18:46 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 1.5265
2022-03-12 10:19:19 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.6509
2022-03-12 10:19:52 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.6209
2022-03-12 10:20:25 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 1.6630
2022-03-12 10:20:59 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.7974
2022-03-12 10:21:32 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.7699
2022-03-12 10:22:06 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 1.5614
2022-03-12 10:22:39 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.7309
2022-03-12 10:23:13 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 1.5176
2022-03-12 10:23:46 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.8538
2022-03-12 10:24:19 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.6713
2022-03-12 10:24:52 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.6702
2022-03-12 10:25:24 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.6122
2022-03-12 10:25:24 - train: epoch 075, train_loss: 1.6674
2022-03-12 10:26:38 - eval: epoch: 075, acc1: 65.298%, acc5: 86.470%, test_loss: 1.4122, per_image_load_time: 2.672ms, per_image_inference_time: 0.158ms
2022-03-12 10:26:38 - until epoch: 075, best_acc1: 65.316%
2022-03-12 10:26:38 - epoch 076 lr: 0.0010000000000000002
2022-03-12 10:27:16 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 1.5105
2022-03-12 10:27:49 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.6390
2022-03-12 10:28:22 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.6308
2022-03-12 10:28:55 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.6161
2022-03-12 10:29:29 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.6204
2022-03-12 10:30:01 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.4987
2022-03-12 10:30:35 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.7839
2022-03-12 10:31:07 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.7491
2022-03-12 10:31:41 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 1.7759
2022-03-12 10:32:13 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.7000
2022-03-12 10:32:47 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.4086
2022-03-12 10:33:19 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.5950
2022-03-12 10:33:53 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.6495
2022-03-12 10:34:26 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 1.7310
2022-03-12 10:34:59 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.6346
2022-03-12 10:35:32 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.6027
2022-03-12 10:36:06 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.6344
2022-03-12 10:36:38 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 1.4590
2022-03-12 10:37:11 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.6856
2022-03-12 10:37:45 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 1.6519
2022-03-12 10:38:17 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.9258
2022-03-12 10:38:50 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.5913
2022-03-12 10:39:24 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 1.6146
2022-03-12 10:39:56 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.8258
2022-03-12 10:40:29 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 1.5926
2022-03-12 10:41:03 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.8783
2022-03-12 10:41:36 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.7162
2022-03-12 10:42:09 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 1.5224
2022-03-12 10:42:43 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 1.7011
2022-03-12 10:43:16 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 1.5592
2022-03-12 10:43:49 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 1.8045
2022-03-12 10:44:22 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.7314
2022-03-12 10:44:54 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.4594
2022-03-12 10:45:28 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.8088
2022-03-12 10:46:01 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 1.5903
2022-03-12 10:46:34 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 1.5440
2022-03-12 10:47:07 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 1.4264
2022-03-12 10:47:41 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 1.5131
2022-03-12 10:48:14 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 1.5660
2022-03-12 10:48:47 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 1.7712
2022-03-12 10:49:21 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 1.6620
2022-03-12 10:49:54 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.7920
2022-03-12 10:50:28 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 1.7960
2022-03-12 10:51:00 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 1.6708
2022-03-12 10:51:34 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 1.7084
2022-03-12 10:52:07 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 1.6409
2022-03-12 10:52:39 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.9528
2022-03-12 10:53:14 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 1.6204
2022-03-12 10:53:46 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 1.7602
2022-03-12 10:54:18 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.7649
2022-03-12 10:54:19 - train: epoch 076, train_loss: 1.6649
2022-03-12 10:55:31 - eval: epoch: 076, acc1: 65.418%, acc5: 86.660%, test_loss: 1.4078, per_image_load_time: 1.431ms, per_image_inference_time: 0.163ms
2022-03-12 10:55:31 - until epoch: 076, best_acc1: 65.418%
2022-03-12 10:55:31 - epoch 077 lr: 0.0010000000000000002
2022-03-12 10:56:10 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 1.7200
2022-03-12 10:56:43 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 2.0716
2022-03-12 10:57:16 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 1.4387
2022-03-12 10:57:48 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.6963
2022-03-12 10:58:21 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 1.7106
2022-03-12 10:58:54 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 1.4731
2022-03-12 10:59:28 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 1.7082
2022-03-12 11:00:00 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.6617
2022-03-12 11:00:34 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.7080
2022-03-12 11:01:06 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 1.7286
2022-03-12 11:01:39 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 1.6928
2022-03-12 11:02:11 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 1.8310
2022-03-12 11:02:45 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 1.4423
2022-03-12 11:03:15 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.6003
2022-03-12 11:03:49 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 1.6868
2022-03-12 11:04:20 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 1.5756
2022-03-12 11:04:54 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.7740
2022-03-12 11:05:26 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 1.6953
2022-03-12 11:05:59 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 1.5608
2022-03-12 11:06:32 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 1.5712
2022-03-12 11:07:06 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 1.7725
2022-03-12 11:07:37 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 1.4581
2022-03-12 11:08:11 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.8266
2022-03-12 11:08:44 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 1.5925
2022-03-12 11:09:17 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.7809
2022-03-12 11:09:49 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 1.6185
2022-03-12 11:10:22 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 1.6562
2022-03-12 11:10:55 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.7884
2022-03-12 11:11:28 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.8118
2022-03-12 11:12:00 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 1.6138
2022-03-12 11:12:33 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 1.5710
2022-03-12 11:13:06 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.8669
2022-03-12 11:13:39 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 1.6976
2022-03-12 11:14:12 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.7012
2022-03-12 11:14:45 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.6332
2022-03-12 11:15:18 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 1.5312
2022-03-12 11:15:51 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 1.6777
2022-03-12 11:16:23 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 1.5487
2022-03-12 11:16:56 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 1.5531
2022-03-12 11:17:29 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 1.7951
2022-03-12 11:18:02 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 1.6030
2022-03-12 11:18:35 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.7184
2022-03-12 11:19:08 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 1.6518
2022-03-12 11:19:41 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 1.5882
2022-03-12 11:20:13 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.9906
2022-03-12 11:20:47 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 1.5667
2022-03-12 11:21:19 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 1.5281
2022-03-12 11:21:53 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 1.5448
2022-03-12 11:22:25 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.9383
2022-03-12 11:22:57 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.6515
2022-03-12 11:22:57 - train: epoch 077, train_loss: 1.6638
2022-03-12 11:24:12 - eval: epoch: 077, acc1: 65.334%, acc5: 86.640%, test_loss: 1.4069, per_image_load_time: 2.687ms, per_image_inference_time: 0.163ms
2022-03-12 11:24:12 - until epoch: 077, best_acc1: 65.418%
2022-03-12 11:24:12 - epoch 078 lr: 0.0010000000000000002
2022-03-12 11:24:50 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 1.6760
2022-03-12 11:25:23 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.8285
2022-03-12 11:25:54 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.5098
2022-03-12 11:26:27 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 1.6710
2022-03-12 11:27:00 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.4606
2022-03-12 11:27:33 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 1.4222
2022-03-12 11:28:05 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.8114
2022-03-12 11:28:39 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.8959
2022-03-12 11:29:11 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 1.8337
2022-03-12 11:29:44 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 1.6857
2022-03-12 11:30:18 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 1.3853
2022-03-12 11:30:49 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 1.6520
2022-03-12 11:31:23 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 1.6502
2022-03-12 11:31:55 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 1.5069
2022-03-12 11:32:29 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 1.6375
2022-03-12 11:33:02 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 1.6773
2022-03-12 11:33:35 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 1.8276
2022-03-12 11:34:08 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 1.6336
2022-03-12 11:34:40 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 1.7434
2022-03-12 11:35:14 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 1.6013
2022-03-12 11:35:46 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.8818
2022-03-12 11:36:19 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 1.7232
2022-03-12 11:36:53 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 1.6744
2022-03-12 11:37:25 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 1.7710
2022-03-12 11:37:59 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.6518
2022-03-12 11:38:32 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 1.7667
2022-03-12 11:39:06 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 1.5947
2022-03-12 11:39:38 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 1.6922
2022-03-12 11:40:12 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 1.5620
2022-03-12 11:40:44 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 1.6734
2022-03-12 11:41:19 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.7422
2022-03-12 11:41:52 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 1.7583
2022-03-12 11:42:25 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.7547
2022-03-12 11:42:57 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 1.4513
2022-03-12 11:43:31 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 1.6890
2022-03-12 11:44:04 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.8166
2022-03-12 11:44:38 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 1.8954
2022-03-12 11:45:11 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 1.6063
2022-03-12 11:45:44 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 1.6464
2022-03-12 11:46:17 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 1.5455
2022-03-12 11:46:51 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.7402
2022-03-12 11:47:24 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.8829
2022-03-12 11:47:57 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 1.7537
2022-03-12 11:48:30 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 1.7148
2022-03-12 11:49:03 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.8582
2022-03-12 11:49:37 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 1.4713
2022-03-12 11:50:10 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 1.6552
2022-03-12 11:50:44 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 1.8144
2022-03-12 11:51:16 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 1.6058
2022-03-12 11:51:48 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 1.8105
2022-03-12 11:51:49 - train: epoch 078, train_loss: 1.6589
2022-03-12 11:53:02 - eval: epoch: 078, acc1: 65.512%, acc5: 86.664%, test_loss: 1.4043, per_image_load_time: 1.597ms, per_image_inference_time: 0.162ms
2022-03-12 11:53:02 - until epoch: 078, best_acc1: 65.512%
2022-03-12 11:53:02 - epoch 079 lr: 0.0010000000000000002
2022-03-12 11:53:40 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 1.5591
2022-03-12 11:54:13 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 1.6174
2022-03-12 11:54:46 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 1.7005
2022-03-12 11:55:19 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 1.6798
2022-03-12 11:55:51 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 1.6373
2022-03-12 11:56:24 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 1.6391
2022-03-12 11:56:57 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 1.6015
2022-03-12 11:57:29 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 1.7263
2022-03-12 11:58:02 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 1.6493
2022-03-12 11:58:35 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 1.7721
2022-03-12 11:59:08 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 1.8937
2022-03-12 11:59:41 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.7549
2022-03-12 12:00:14 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 1.5218
2022-03-12 12:00:46 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 1.6981
2022-03-12 12:01:20 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 1.6813
2022-03-12 12:01:53 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 1.5578
2022-03-12 12:02:27 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 1.7851
2022-03-12 12:02:59 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 1.6496
2022-03-12 12:03:33 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.6143
2022-03-12 12:04:05 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.9086
2022-03-12 12:04:38 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 1.3661
2022-03-12 12:05:10 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 1.5191
2022-03-12 12:05:43 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 1.5363
2022-03-12 12:06:15 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.5323
2022-03-12 12:06:47 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 1.6769
2022-03-12 12:07:21 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 1.5049
2022-03-12 12:07:54 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 1.3758
2022-03-12 12:08:26 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 1.5538
2022-03-12 12:08:59 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 1.6868
2022-03-12 12:09:32 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.6627
2022-03-12 12:10:05 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 1.4784
2022-03-12 12:10:38 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.8733
2022-03-12 12:11:11 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 1.7846
2022-03-12 12:11:43 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 1.6377
2022-03-12 12:12:17 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.7693
2022-03-12 12:12:50 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 1.4899
2022-03-12 12:13:22 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 1.6198
2022-03-12 12:13:56 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 1.8003
2022-03-12 12:14:29 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 1.6782
2022-03-12 12:15:02 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 1.6144
2022-03-12 12:15:35 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.6197
2022-03-12 12:16:08 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.4981
2022-03-12 12:16:42 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 1.4434
2022-03-12 12:17:14 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.8647
2022-03-12 12:17:47 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.8192
2022-03-12 12:18:21 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.6947
2022-03-12 12:18:53 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 1.6816
2022-03-12 12:19:26 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.8772
2022-03-12 12:20:00 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.6464
2022-03-12 12:20:31 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 1.5138
2022-03-12 12:20:31 - train: epoch 079, train_loss: 1.6586
2022-03-12 12:21:44 - eval: epoch: 079, acc1: 65.464%, acc5: 86.674%, test_loss: 1.4057, per_image_load_time: 2.683ms, per_image_inference_time: 0.160ms
2022-03-12 12:21:45 - until epoch: 079, best_acc1: 65.512%
2022-03-12 12:21:45 - epoch 080 lr: 0.0010000000000000002
2022-03-12 12:22:23 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 1.6568
2022-03-12 12:22:55 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 1.7148
2022-03-12 12:23:27 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.4961
2022-03-12 12:24:01 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 1.6658
2022-03-12 12:24:33 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 1.3626
2022-03-12 12:25:05 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 1.6072
2022-03-12 12:25:37 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 1.5874
2022-03-12 12:26:10 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 1.4226
2022-03-12 12:26:41 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 1.5916
2022-03-12 12:27:13 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 1.5528
2022-03-12 12:27:47 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 1.5303
2022-03-12 12:28:19 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 1.6916
2022-03-12 12:28:51 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 1.6058
2022-03-12 12:29:23 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 1.6542
2022-03-12 12:29:56 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 1.5610
2022-03-12 12:30:27 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 1.5960
2022-03-12 12:30:59 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 1.6873
2022-03-12 12:31:32 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 1.7883
2022-03-12 12:32:04 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 1.5456
2022-03-12 12:32:37 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 1.7044
2022-03-12 12:33:10 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 1.8094
2022-03-12 12:33:42 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 1.6947
2022-03-12 12:34:13 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 1.6356
2022-03-12 12:34:46 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 1.8557
2022-03-12 12:35:18 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 1.5263
2022-03-12 12:35:51 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 1.8562
2022-03-12 12:36:23 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 1.5228
2022-03-12 12:36:55 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 1.7097
2022-03-12 12:37:27 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 1.7494
2022-03-12 12:38:00 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 1.5677
2022-03-12 12:38:31 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 1.6473
2022-03-12 12:39:04 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 1.2874
2022-03-12 12:39:36 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 1.5755
2022-03-12 12:40:09 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 1.5451
2022-03-12 12:40:41 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 1.5290
2022-03-12 12:41:14 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 1.6696
2022-03-12 12:41:46 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 1.6891
2022-03-12 12:42:19 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 1.5950
2022-03-12 12:42:50 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 1.4672
2022-03-12 12:43:22 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 1.8018
2022-03-12 12:43:55 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 1.8588
2022-03-12 12:44:27 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 1.6395
2022-03-12 12:45:00 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 1.8592
2022-03-12 12:45:31 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 1.5969
2022-03-12 12:46:04 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 1.6264
2022-03-12 12:46:36 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 1.6871
2022-03-12 12:47:09 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 1.7140
2022-03-12 12:47:40 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 1.6498
2022-03-12 12:48:13 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.8052
2022-03-12 12:48:44 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 1.6302
2022-03-12 12:48:44 - train: epoch 080, train_loss: 1.6577
2022-03-12 12:49:57 - eval: epoch: 080, acc1: 65.592%, acc5: 86.614%, test_loss: 1.4023, per_image_load_time: 2.025ms, per_image_inference_time: 0.178ms
2022-03-12 12:49:57 - until epoch: 080, best_acc1: 65.592%
2022-03-12 12:49:57 - epoch 081 lr: 0.0010000000000000002
2022-03-12 12:50:35 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 1.6962
2022-03-12 12:51:07 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 1.4384
2022-03-12 12:51:39 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 1.6594
2022-03-12 12:52:13 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 1.4847
2022-03-12 12:52:45 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 1.9385
2022-03-12 12:53:17 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 1.5449
2022-03-12 12:53:50 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 1.6296
2022-03-12 12:54:21 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 1.7369
2022-03-12 12:54:53 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 1.8250
2022-03-12 12:55:26 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 1.7340
2022-03-12 12:55:59 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 1.5596
2022-03-12 12:56:31 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 1.7909
2022-03-12 12:57:04 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 1.4948
2022-03-12 12:57:37 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 1.3779
2022-03-12 12:58:10 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 1.6893
2022-03-12 12:58:42 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 1.6755
2022-03-12 12:59:16 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 1.5808
2022-03-12 12:59:47 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 1.7181
2022-03-12 13:00:21 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 1.5933
2022-03-12 13:00:53 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 1.8911
2022-03-12 13:01:26 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 1.6916
2022-03-12 13:01:57 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 1.5843
2022-03-12 13:02:30 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 1.5981
2022-03-12 13:03:02 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 1.6205
2022-03-12 13:03:35 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 1.6934
2022-03-12 13:04:08 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.7357
2022-03-12 13:04:40 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 1.6160
2022-03-12 13:05:12 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 1.6102
2022-03-12 13:05:45 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 1.5016
2022-03-12 13:06:17 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 1.7656
2022-03-12 13:06:49 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 1.6758
2022-03-12 13:07:22 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 1.5839
2022-03-12 13:07:55 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 1.5365
2022-03-12 13:08:28 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 1.8459
2022-03-12 13:09:00 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 1.7726
2022-03-12 13:09:32 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 1.6137
2022-03-12 13:10:06 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 1.7075
2022-03-12 13:10:39 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 1.6110
2022-03-12 13:11:11 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 1.7965
2022-03-12 13:11:44 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 1.7937
2022-03-12 13:12:16 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 1.6537
2022-03-12 13:12:49 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 1.7089
2022-03-12 13:13:21 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 1.7531
2022-03-12 13:13:54 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 1.6136
2022-03-12 13:14:26 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 1.6332
2022-03-12 13:14:59 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 1.7641
2022-03-12 13:15:32 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 1.8699
2022-03-12 13:16:05 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 1.5254
2022-03-12 13:16:37 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 1.5936
2022-03-12 13:17:08 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 1.5018
2022-03-12 13:17:09 - train: epoch 081, train_loss: 1.6550
2022-03-12 13:18:22 - eval: epoch: 081, acc1: 65.434%, acc5: 86.662%, test_loss: 1.4034, per_image_load_time: 1.750ms, per_image_inference_time: 0.163ms
2022-03-12 13:18:22 - until epoch: 081, best_acc1: 65.592%
2022-03-12 13:18:22 - epoch 082 lr: 0.0010000000000000002
2022-03-12 13:18:59 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 1.4596
2022-03-12 13:19:32 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 1.6118
2022-03-12 13:20:05 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 1.6151
2022-03-12 13:20:36 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 1.7031
2022-03-12 13:21:09 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 1.8431
2022-03-12 13:21:41 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 1.5962
2022-03-12 13:22:14 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 1.8691
2022-03-12 13:22:46 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 1.5343
2022-03-12 13:23:18 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 1.7536
2022-03-12 13:23:51 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 1.6509
2022-03-12 13:24:23 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 1.7912
2022-03-12 13:24:56 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 1.9963
2022-03-12 13:25:29 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 1.8291
2022-03-12 13:26:02 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 1.6012
2022-03-12 13:26:34 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 1.6783
2022-03-12 13:27:07 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 1.6572
2022-03-12 13:27:40 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 1.5878
2022-03-12 13:28:12 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 1.6994
2022-03-12 13:28:45 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 1.3304
2022-03-12 13:29:18 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 1.4799
2022-03-12 13:29:50 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 1.8368
2022-03-12 13:30:23 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 1.7645
2022-03-12 13:30:56 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 1.5857
2022-03-12 13:31:28 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 1.7679
2022-03-12 13:32:00 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 1.4796
2022-03-12 13:32:33 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 1.5940
2022-03-12 13:33:05 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 1.6130
2022-03-12 13:33:38 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 1.5988
2022-03-12 13:34:10 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 1.5477
2022-03-12 13:34:43 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 1.5839
2022-03-12 13:35:16 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 1.7973
2022-03-12 13:35:48 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 1.7286
2022-03-12 13:36:21 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 1.6894
2022-03-12 13:36:54 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 1.7939
2022-03-12 13:37:26 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 1.5863
2022-03-12 13:37:59 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 1.6266
2022-03-12 13:38:31 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 1.4462
2022-03-12 13:39:03 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 1.5214
2022-03-12 13:39:36 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 1.6902
2022-03-12 13:40:08 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 1.6555
2022-03-12 13:40:41 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 1.6988
2022-03-12 13:41:13 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 1.7979
2022-03-12 13:41:47 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 1.5010
2022-03-12 13:42:18 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 1.5651
2022-03-12 13:42:52 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 1.5859
2022-03-12 13:43:23 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 1.6372
2022-03-12 13:43:55 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 1.4796
2022-03-12 13:44:28 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 1.5343
2022-03-12 13:45:01 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 1.7966
2022-03-12 13:45:32 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 1.5719
2022-03-12 13:45:32 - train: epoch 082, train_loss: 1.6542
2022-03-12 13:46:45 - eval: epoch: 082, acc1: 65.448%, acc5: 86.678%, test_loss: 1.4037, per_image_load_time: 1.433ms, per_image_inference_time: 0.160ms
2022-03-12 13:46:45 - until epoch: 082, best_acc1: 65.592%
2022-03-12 13:46:45 - epoch 083 lr: 0.0010000000000000002
2022-03-12 13:47:23 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 1.3819
2022-03-12 13:47:55 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 1.7211
2022-03-12 13:48:28 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 1.6273
2022-03-12 13:49:01 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 1.6853
2022-03-12 13:49:32 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 1.4943
2022-03-12 13:50:05 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 1.5904
2022-03-12 13:50:38 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 1.7116
2022-03-12 13:51:10 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 1.5420
2022-03-12 13:51:43 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 1.7605
2022-03-12 13:52:16 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 1.8132
2022-03-12 13:52:48 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 1.7088
2022-03-12 13:53:22 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 1.8318
2022-03-12 13:53:53 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 1.8139
2022-03-12 13:54:26 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 1.7236
2022-03-12 13:55:00 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 1.5618
2022-03-12 13:55:33 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 1.6790
2022-03-12 13:56:05 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 1.6625
2022-03-12 13:56:38 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 1.8422
2022-03-12 13:57:10 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 1.5671
2022-03-12 13:57:44 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 1.3593
2022-03-12 13:58:15 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 1.7568
2022-03-12 13:58:48 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 1.6881
2022-03-12 13:59:20 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 1.6683
2022-03-12 13:59:53 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 1.4759
2022-03-12 14:00:25 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 1.6113
2022-03-12 14:00:57 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 1.6259
2022-03-12 14:01:30 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 1.6725
2022-03-12 14:02:02 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 1.6179
2022-03-12 14:02:35 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 1.7611
2022-03-12 14:03:06 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 1.6684
2022-03-12 14:03:40 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 1.5993
2022-03-12 14:04:11 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 1.5925
2022-03-12 14:04:45 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 1.7992
2022-03-12 14:05:18 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 1.6366
2022-03-12 14:05:50 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 1.6224
2022-03-12 14:06:23 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 1.4760
2022-03-12 14:06:54 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 1.5379
2022-03-12 14:07:27 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 1.5289
2022-03-12 14:07:59 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 1.8911
2022-03-12 14:08:33 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 1.6989
2022-03-12 14:09:05 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 1.5538
2022-03-12 14:09:38 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 2.0734
2022-03-12 14:10:09 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 1.7086
2022-03-12 14:10:42 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 1.6753
2022-03-12 14:11:15 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 1.4813
2022-03-12 14:11:48 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 1.7915
2022-03-12 14:12:19 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.9422
2022-03-12 14:12:51 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 1.6609
2022-03-12 14:13:24 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 1.6255
2022-03-12 14:13:55 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 1.4667
2022-03-12 14:13:56 - train: epoch 083, train_loss: 1.6539
2022-03-12 14:15:09 - eval: epoch: 083, acc1: 65.474%, acc5: 86.614%, test_loss: 1.4047, per_image_load_time: 1.624ms, per_image_inference_time: 0.143ms
2022-03-12 14:15:09 - until epoch: 083, best_acc1: 65.592%
2022-03-12 14:15:09 - epoch 084 lr: 0.0010000000000000002
2022-03-12 14:15:46 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 1.7579
2022-03-12 14:16:19 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 1.6242
2022-03-12 14:16:51 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 1.4948
2022-03-12 14:17:23 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 1.6135
2022-03-12 14:17:56 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 1.8851
2022-03-12 14:18:28 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 1.9498
2022-03-12 14:19:01 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 1.6409
2022-03-12 14:19:33 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 1.8696
2022-03-12 14:20:06 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 1.7888
2022-03-12 14:20:38 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 1.5304
2022-03-12 14:21:10 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 1.6422
2022-03-12 14:21:43 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 1.8706
2022-03-12 14:22:15 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 1.7059
2022-03-12 14:22:48 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 1.7063
2022-03-12 14:23:20 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 1.8390
2022-03-12 14:23:53 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 1.5551
2022-03-12 14:24:25 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 1.8628
2022-03-12 14:24:59 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 1.6798
2022-03-12 14:25:30 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 1.6208
2022-03-12 14:26:03 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 1.5746
2022-03-12 14:26:36 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 1.4790
2022-03-12 14:27:09 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 1.6321
2022-03-12 14:27:41 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 1.7539
2022-03-12 14:28:14 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 1.5377
2022-03-12 14:28:46 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 1.7136
2022-03-12 14:29:19 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 1.5337
2022-03-12 14:29:51 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 1.7376
2022-03-12 14:30:24 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 1.6491
2022-03-12 14:30:56 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 1.9153
2022-03-12 14:31:30 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 1.7943
2022-03-12 14:32:02 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 1.5014
2022-03-12 14:32:35 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 1.6035
2022-03-12 14:33:07 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 1.5941
2022-03-12 14:33:41 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 1.6083
2022-03-12 14:34:13 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 1.5536
2022-03-12 14:34:46 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 1.6863
2022-03-12 14:35:17 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 1.9432
2022-03-12 14:35:51 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 1.7410
2022-03-12 14:36:24 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 1.6924
2022-03-12 14:36:57 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 1.4961
2022-03-12 14:37:29 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 1.5800
2022-03-12 14:38:02 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 1.5933
2022-03-12 14:38:34 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 1.6144
2022-03-12 14:39:07 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.9512
2022-03-12 14:39:39 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 1.4437
2022-03-12 14:40:13 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 1.6439
2022-03-12 14:40:45 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 1.6805
2022-03-12 14:41:19 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 1.5899
2022-03-12 14:41:52 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 1.6035
2022-03-12 14:42:24 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 1.7691
2022-03-12 14:42:24 - train: epoch 084, train_loss: 1.6527
2022-03-12 14:43:38 - eval: epoch: 084, acc1: 65.754%, acc5: 86.784%, test_loss: 1.3956, per_image_load_time: 2.709ms, per_image_inference_time: 0.161ms
2022-03-12 14:43:39 - until epoch: 084, best_acc1: 65.754%
2022-03-12 14:43:39 - epoch 085 lr: 0.0010000000000000002
2022-03-12 14:44:17 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 1.5869
2022-03-12 14:44:50 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 1.5762
2022-03-12 14:45:22 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 1.8723
2022-03-12 14:45:56 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 1.6730
2022-03-12 14:46:27 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 1.7055
2022-03-12 14:46:59 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 1.3946
2022-03-12 14:47:31 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 1.6598
2022-03-12 14:48:04 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 1.6365
2022-03-12 14:48:36 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 1.6461
2022-03-12 14:49:09 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 1.7165
2022-03-12 14:49:40 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 1.6663
2022-03-12 14:50:13 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 1.7706
2022-03-12 14:50:46 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 1.8478
2022-03-12 14:51:19 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 1.5810
2022-03-12 14:51:50 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 1.6825
2022-03-12 14:52:23 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 1.6475
2022-03-12 14:52:56 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 1.8482
2022-03-12 14:53:28 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 1.3356
2022-03-12 14:54:00 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 1.6034
2022-03-12 14:54:32 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 1.5863
2022-03-12 14:55:05 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 1.4863
2022-03-12 14:55:38 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 1.6594
2022-03-12 14:56:10 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 1.5312
2022-03-12 14:56:43 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 1.7587
2022-03-12 14:57:15 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 1.6656
2022-03-12 14:57:48 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 1.7718
2022-03-12 14:58:19 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 1.5385
2022-03-12 14:58:52 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 1.8366
2022-03-12 14:59:24 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 1.6966
2022-03-12 14:59:57 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 1.7549
2022-03-12 15:00:29 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 1.8178
2022-03-12 15:01:02 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 1.6122
2022-03-12 15:01:34 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 1.7878
2022-03-12 15:02:06 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 1.7279
2022-03-12 15:02:39 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 1.7205
2022-03-12 15:03:11 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 1.8813
2022-03-12 15:03:44 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 1.4841
2022-03-12 15:04:15 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 1.5190
2022-03-12 15:04:48 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 1.6103
2022-03-12 15:05:20 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 1.8614
2022-03-12 15:05:53 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 1.6363
2022-03-12 15:06:24 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 1.7445
2022-03-12 15:06:57 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 1.5988
2022-03-12 15:07:29 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 1.6633
2022-03-12 15:08:02 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 1.8210
2022-03-12 15:08:34 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 1.6481
2022-03-12 15:09:07 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.9720
2022-03-12 15:09:38 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 1.4425
2022-03-12 15:10:12 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 1.8039
2022-03-12 15:10:41 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 1.4249
2022-03-12 15:10:42 - train: epoch 085, train_loss: 1.6486
2022-03-12 15:11:55 - eval: epoch: 085, acc1: 65.540%, acc5: 86.684%, test_loss: 1.3998, per_image_load_time: 1.507ms, per_image_inference_time: 0.160ms
2022-03-12 15:11:55 - until epoch: 085, best_acc1: 65.754%
2022-03-12 15:11:55 - epoch 086 lr: 0.0010000000000000002
2022-03-12 15:12:33 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 1.5278
2022-03-12 15:13:05 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 1.4433
2022-03-12 15:13:37 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 1.6387
2022-03-12 15:14:09 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 1.9054
2022-03-12 15:14:42 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 1.6798
2022-03-12 15:15:14 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 1.7006
2022-03-12 15:15:46 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 1.5702
2022-03-12 15:16:19 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 1.7175
2022-03-12 15:16:51 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 1.7774
2022-03-12 15:17:24 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 1.6715
2022-03-12 15:17:56 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 1.8142
2022-03-12 15:18:29 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 1.4846
2022-03-12 15:19:01 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 1.7513
2022-03-12 15:19:33 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 1.6398
2022-03-12 15:20:05 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 1.4225
2022-03-12 15:20:38 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 1.4354
2022-03-12 15:21:10 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 1.6765
2022-03-12 15:21:44 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 1.7499
2022-03-12 15:22:16 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 1.6493
2022-03-12 15:22:49 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 1.8101
2022-03-12 15:23:21 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 1.5081
2022-03-12 15:23:54 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 1.7886
2022-03-12 15:24:26 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 1.6289
2022-03-12 15:25:00 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 1.3459
2022-03-12 15:25:30 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 1.9071
2022-03-12 15:26:05 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 1.6493
2022-03-12 15:26:36 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 1.5981
2022-03-12 15:27:10 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 1.8258
2022-03-12 15:27:42 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 1.4266
2022-03-12 15:28:15 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 1.6329
2022-03-12 15:28:47 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 1.6688
2022-03-12 15:29:19 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 1.7039
2022-03-12 15:29:53 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 1.7730
2022-03-12 15:30:24 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 1.7054
2022-03-12 15:30:57 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 1.7044
2022-03-12 15:31:30 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 1.5641
2022-03-12 15:32:03 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 1.6607
2022-03-12 15:32:34 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 1.6990
2022-03-12 15:33:07 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 1.6142
2022-03-12 15:33:39 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 1.7851
2022-03-12 15:34:12 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 1.5881
2022-03-12 15:34:45 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 1.9226
2022-03-12 15:35:17 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 1.7817
2022-03-12 15:35:50 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 1.6147
2022-03-12 15:36:23 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 1.4706
2022-03-12 15:36:55 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 1.5690
2022-03-12 15:37:29 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 1.5575
2022-03-12 15:38:01 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 1.7529
2022-03-12 15:38:34 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 1.5372
2022-03-12 15:39:05 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 1.5051
2022-03-12 15:39:05 - train: epoch 086, train_loss: 1.6463
2022-03-12 15:40:18 - eval: epoch: 086, acc1: 65.474%, acc5: 86.576%, test_loss: 1.4025, per_image_load_time: 1.293ms, per_image_inference_time: 0.166ms
2022-03-12 15:40:18 - until epoch: 086, best_acc1: 65.754%
2022-03-12 15:40:18 - epoch 087 lr: 0.0010000000000000002
2022-03-12 15:40:56 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 1.4765
2022-03-12 15:41:29 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 1.4328
2022-03-12 15:42:01 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 1.5075
2022-03-12 15:42:33 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 1.9415
2022-03-12 15:43:05 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 1.4488
2022-03-12 15:43:38 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 1.8032
2022-03-12 15:44:11 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 1.5777
2022-03-12 15:44:42 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 1.4989
2022-03-12 15:45:15 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 1.6760
2022-03-12 15:45:47 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 1.5401
2022-03-12 15:46:19 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 1.5923
2022-03-12 15:46:52 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 1.7208
2022-03-12 15:47:24 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 2.0010
2022-03-12 15:47:57 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 1.3626
2022-03-12 15:48:30 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 1.4417
2022-03-12 15:49:02 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 1.5756
2022-03-12 15:49:35 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 1.9242
2022-03-12 15:50:07 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 1.8470
2022-03-12 15:50:39 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 1.7490
2022-03-12 15:51:11 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 1.6982
2022-03-12 15:51:45 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 1.8892
2022-03-12 15:52:17 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 1.6599
2022-03-12 15:52:51 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 1.6346
2022-03-12 15:53:22 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 1.5896
2022-03-12 15:53:56 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 1.6417
2022-03-12 15:54:27 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 1.5618
2022-03-12 15:55:00 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 1.6333
2022-03-12 15:55:33 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 1.4287
2022-03-12 15:56:06 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 1.5012
2022-03-12 15:56:38 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 1.7627
2022-03-12 15:57:11 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 1.4937
2022-03-12 15:57:44 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 1.6648
2022-03-12 15:58:16 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 1.6138
2022-03-12 15:58:50 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 1.4588
2022-03-12 15:59:22 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 1.4234
2022-03-12 15:59:54 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 1.5273
2022-03-12 16:00:26 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 1.7004
2022-03-12 16:00:59 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 1.5863
2022-03-12 16:01:31 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 1.6619
2022-03-12 16:02:04 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 1.6624
2022-03-12 16:02:36 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 1.8674
2022-03-12 16:03:09 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 1.5607
2022-03-12 16:03:41 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 1.6539
2022-03-12 16:04:14 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 1.6879
2022-03-12 16:04:46 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 1.6131
2022-03-12 16:05:20 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 1.6247
2022-03-12 16:05:52 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 1.5677
2022-03-12 16:06:25 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 1.5543
2022-03-12 16:06:57 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 1.8208
2022-03-12 16:07:28 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 1.4821
2022-03-12 16:07:29 - train: epoch 087, train_loss: 1.6455
2022-03-12 16:08:42 - eval: epoch: 087, acc1: 65.558%, acc5: 86.674%, test_loss: 1.4015, per_image_load_time: 2.662ms, per_image_inference_time: 0.154ms
2022-03-12 16:08:42 - until epoch: 087, best_acc1: 65.754%
2022-03-12 16:08:42 - epoch 088 lr: 0.0010000000000000002
2022-03-12 16:09:20 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 1.5569
2022-03-12 16:09:53 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 1.6518
2022-03-12 16:10:25 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 1.7449
2022-03-12 16:10:58 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 1.7449
2022-03-12 16:11:30 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 1.9060
2022-03-12 16:12:02 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 1.6739
2022-03-12 16:12:35 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 1.6752
2022-03-12 16:13:06 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 1.8155
2022-03-12 16:13:39 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 1.6608
2022-03-12 16:14:11 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 1.5191
2022-03-12 16:14:43 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 1.5741
2022-03-12 16:15:17 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 1.6922
2022-03-12 16:15:48 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 1.7034
2022-03-12 16:16:21 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 1.4629
2022-03-12 16:16:53 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 1.6808
2022-03-12 16:17:26 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 1.6792
2022-03-12 16:17:58 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 1.6496
2022-03-12 16:18:31 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 1.4739
2022-03-12 16:19:03 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 1.6272
2022-03-12 16:19:35 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 1.5469
2022-03-12 16:20:07 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 1.8032
2022-03-12 16:20:40 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 1.7117
2022-03-12 16:21:12 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 1.5022
2022-03-12 16:21:45 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 1.6938
2022-03-12 16:22:17 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 1.7587
2022-03-12 16:22:50 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 1.7490
2022-03-12 16:23:23 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 1.7619
2022-03-12 16:23:55 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 1.7708
2022-03-12 16:24:28 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 1.5391
2022-03-12 16:25:02 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 1.8353
2022-03-12 16:25:34 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 1.4635
2022-03-12 16:26:06 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 1.7300
2022-03-12 16:26:38 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 1.3720
2022-03-12 16:27:11 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 1.4397
2022-03-12 16:27:43 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 1.5887
2022-03-12 16:28:16 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 1.6072
2022-03-12 16:28:49 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 1.6961
2022-03-12 16:29:21 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 1.7563
2022-03-12 16:29:55 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 1.6878
2022-03-12 16:30:27 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 1.3783
2022-03-12 16:31:00 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 1.8043
2022-03-12 16:31:32 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 1.6939
2022-03-12 16:32:05 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 1.6206
2022-03-12 16:32:37 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 1.4391
2022-03-12 16:33:10 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 1.6368
2022-03-12 16:33:42 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 1.9569
2022-03-12 16:34:14 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 1.8193
2022-03-12 16:34:47 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 1.6566
2022-03-12 16:35:20 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 1.5408
2022-03-12 16:35:50 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 1.6810
2022-03-12 16:35:51 - train: epoch 088, train_loss: 1.6470
2022-03-12 16:37:04 - eval: epoch: 088, acc1: 65.628%, acc5: 86.668%, test_loss: 1.3983, per_image_load_time: 1.463ms, per_image_inference_time: 0.177ms
2022-03-12 16:37:04 - until epoch: 088, best_acc1: 65.754%
2022-03-12 16:37:04 - epoch 089 lr: 0.0010000000000000002
2022-03-12 16:37:42 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 1.8276
2022-03-12 16:38:14 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 1.4099
2022-03-12 16:38:47 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 1.8950
2022-03-12 16:39:20 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 1.6071
2022-03-12 16:39:52 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 1.4930
2022-03-12 16:40:25 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 1.6160
2022-03-12 16:40:57 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 1.7593
2022-03-12 16:41:30 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 1.8499
2022-03-12 16:42:02 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 1.3674
2022-03-12 16:42:35 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 1.8190
2022-03-12 16:43:07 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 1.5478
2022-03-12 16:43:40 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 1.8600
2022-03-12 16:44:12 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 1.7091
2022-03-12 16:44:44 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 1.7471
2022-03-12 16:45:18 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 1.7576
2022-03-12 16:45:51 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 1.4341
2022-03-12 16:46:24 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 1.6892
2022-03-12 16:46:58 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 1.7801
2022-03-12 16:47:32 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 1.5614
2022-03-12 16:48:04 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 1.7232
2022-03-12 16:48:37 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 1.6885
2022-03-12 16:49:08 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 1.6571
2022-03-12 16:49:42 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 1.7029
2022-03-12 16:50:13 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 1.8232
2022-03-12 16:50:46 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 1.8017
2022-03-12 16:51:18 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 1.4648
2022-03-12 16:51:51 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 1.6406
2022-03-12 16:52:22 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 1.9023
2022-03-12 16:52:55 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 1.6133
2022-03-12 16:53:26 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 1.6736
2022-03-12 16:53:59 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 1.6558
2022-03-12 16:54:31 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 1.7732
2022-03-12 16:55:04 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 1.8129
2022-03-12 16:55:35 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 1.6115
2022-03-12 16:56:08 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 1.4689
2022-03-12 16:56:39 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 1.8318
2022-03-12 16:57:12 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 1.6655
2022-03-12 16:57:43 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 1.6136
2022-03-12 16:58:17 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 1.6604
2022-03-12 16:58:48 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 1.5728
2022-03-12 16:59:21 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 1.6380
2022-03-12 16:59:53 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 1.6053
2022-03-12 17:00:26 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 1.6686
2022-03-12 17:00:57 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 1.7113
2022-03-12 17:01:30 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 1.5592
2022-03-12 17:02:01 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 1.7246
2022-03-12 17:02:34 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 1.4513
2022-03-12 17:03:06 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 1.4876
2022-03-12 17:03:39 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 1.6638
2022-03-12 17:04:10 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 1.3260
2022-03-12 17:04:10 - train: epoch 089, train_loss: 1.6436
2022-03-12 17:05:24 - eval: epoch: 089, acc1: 65.704%, acc5: 86.828%, test_loss: 1.3958, per_image_load_time: 1.928ms, per_image_inference_time: 0.170ms
2022-03-12 17:05:24 - until epoch: 089, best_acc1: 65.754%
2022-03-12 17:05:24 - epoch 090 lr: 0.0010000000000000002
2022-03-12 17:06:02 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 1.6090
2022-03-12 17:06:34 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 1.6195
2022-03-12 17:07:08 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 1.5153
2022-03-12 17:07:40 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 1.6663
2022-03-12 17:08:13 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 1.6063
2022-03-12 17:08:45 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 1.9508
2022-03-12 17:09:19 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 1.6561
2022-03-12 17:09:50 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 1.8028
2022-03-12 17:10:24 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 1.5295
2022-03-12 17:10:57 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 1.4782
2022-03-12 17:11:30 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 1.6596
2022-03-12 17:12:02 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 1.6046
2022-03-12 17:12:35 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 1.6890
2022-03-12 17:13:07 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 1.5771
2022-03-12 17:13:40 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 1.8714
2022-03-12 17:14:13 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 1.5467
2022-03-12 17:14:44 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 1.4377
2022-03-12 17:15:17 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 1.4812
2022-03-12 17:15:49 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 1.6828
2022-03-12 17:16:22 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 1.6924
2022-03-12 17:16:55 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 1.6627
2022-03-12 17:17:28 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 1.7704
2022-03-12 17:18:01 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 1.7750
2022-03-12 17:18:34 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 1.5625
2022-03-12 17:19:08 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 1.6329
2022-03-12 17:19:40 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 1.4529
2022-03-12 17:20:13 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 1.5427
2022-03-12 17:20:45 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 1.6903
2022-03-12 17:21:18 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 1.7040
2022-03-12 17:21:50 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 1.5755
2022-03-12 17:22:23 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 1.4820
2022-03-12 17:22:55 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 1.6830
2022-03-12 17:23:28 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 1.9241
2022-03-12 17:24:00 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 1.6880
2022-03-12 17:24:33 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 1.6979
2022-03-12 17:25:05 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 1.4844
2022-03-12 17:25:39 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 1.7881
2022-03-12 17:26:10 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 1.5732
2022-03-12 17:26:43 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 1.5542
2022-03-12 17:27:18 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 1.5287
2022-03-12 17:27:53 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 1.8504
2022-03-12 17:28:27 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 1.7584
2022-03-12 17:29:03 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 1.5326
2022-03-12 17:29:37 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 1.6331
2022-03-12 17:30:12 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 1.6008
2022-03-12 17:30:47 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 1.5915
2022-03-12 17:31:20 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 1.6279
2022-03-12 17:31:53 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 1.8822
2022-03-12 17:32:28 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 1.4332
2022-03-12 17:32:59 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 1.6388
2022-03-12 17:33:00 - train: epoch 090, train_loss: 1.6430
2022-03-12 17:34:17 - eval: epoch: 090, acc1: 65.728%, acc5: 86.746%, test_loss: 1.3942, per_image_load_time: 2.248ms, per_image_inference_time: 0.157ms
2022-03-12 17:34:17 - until epoch: 090, best_acc1: 65.754%
2022-03-12 17:34:17 - epoch 091 lr: 0.00010000000000000003
2022-03-12 17:34:57 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 1.4898
2022-03-12 17:35:32 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 1.5825
2022-03-12 17:36:03 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 1.7058
2022-03-12 17:36:39 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 1.5228
2022-03-12 17:37:12 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 1.5808
2022-03-12 17:37:47 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 1.5817
2022-03-12 17:38:21 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 1.7387
2022-03-12 17:38:56 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 1.4515
2022-03-12 17:39:29 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 1.6623
2022-03-12 17:40:04 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 1.5795
2022-03-12 17:40:36 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 1.5103
2022-03-12 17:41:11 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 1.6647
2022-03-12 17:41:45 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 1.6732
2022-03-12 17:42:20 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 1.6761
2022-03-12 17:42:53 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 1.4740
2022-03-12 17:43:28 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 1.6153
2022-03-12 17:44:02 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 1.6100
2022-03-12 17:44:37 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 1.6724
2022-03-12 17:45:09 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 1.6083
2022-03-12 17:45:44 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 1.4911
2022-03-12 17:46:18 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 1.5408
2022-03-12 17:46:53 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 1.6805
2022-03-12 17:47:26 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 1.6617
2022-03-12 17:48:01 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 1.4308
2022-03-12 17:48:36 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 1.6694
2022-03-12 17:49:11 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 1.5872
2022-03-12 17:49:43 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 1.4871
2022-03-12 17:50:17 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 1.6878
2022-03-12 17:50:52 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 1.5726
2022-03-12 17:51:26 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 1.7765
2022-03-12 17:52:00 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 1.5961
2022-03-12 17:52:35 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 1.7988
2022-03-12 17:53:10 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 1.6084
2022-03-12 17:53:43 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 1.7167
2022-03-12 17:54:18 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 1.8782
2022-03-12 17:54:51 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 1.6837
2022-03-12 17:55:27 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 1.6597
2022-03-12 17:56:01 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 1.6382
2022-03-12 17:56:36 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 1.6375
2022-03-12 17:57:10 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 1.6299
2022-03-12 17:57:45 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 1.6426
2022-03-12 17:58:19 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 1.7069
2022-03-12 17:58:54 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 1.6805
2022-03-12 17:59:26 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 1.4125
2022-03-12 18:00:00 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 1.5315
2022-03-12 18:00:34 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 1.5070
2022-03-12 18:01:09 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 1.6345
2022-03-12 18:01:44 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 1.5565
2022-03-12 18:02:20 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 1.6616
2022-03-12 18:02:53 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 1.7304
2022-03-12 18:02:54 - train: epoch 091, train_loss: 1.6132
2022-03-12 18:04:11 - eval: epoch: 091, acc1: 66.180%, acc5: 87.160%, test_loss: 1.3727, per_image_load_time: 2.454ms, per_image_inference_time: 0.148ms
2022-03-12 18:04:11 - until epoch: 091, best_acc1: 66.180%
2022-03-12 18:04:11 - epoch 092 lr: 0.00010000000000000003
2022-03-12 18:04:53 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 1.6640
2022-03-12 18:05:28 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 1.7298
2022-03-12 18:06:03 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 1.7837
2022-03-12 18:06:38 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 1.6593
2022-03-12 18:07:12 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 1.5561
2022-03-12 18:07:47 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 1.6574
2022-03-12 18:08:22 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 1.6981
2022-03-12 18:08:54 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 1.5881
2022-03-12 18:09:29 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 1.5311
2022-03-12 18:10:04 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 1.4537
2022-03-12 18:10:40 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 1.4828
2022-03-12 18:11:14 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 1.4357
2022-03-12 18:11:50 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 1.5719
2022-03-12 18:12:23 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 1.4446
2022-03-12 18:12:59 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 1.5261
2022-03-12 18:13:30 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 1.5666
2022-03-12 18:14:06 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 1.5098
2022-03-12 18:14:39 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 1.6026
2022-03-12 18:15:14 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 1.5416
2022-03-12 18:15:48 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 1.6395
2022-03-12 18:16:23 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 1.4964
2022-03-12 18:16:57 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 1.4842
2022-03-12 18:17:33 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 1.6219
2022-03-12 18:18:05 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 1.5063
2022-03-12 18:18:40 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 1.5590
2022-03-12 18:19:15 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 1.6400
2022-03-12 18:19:50 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 1.6449
2022-03-12 18:20:24 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 1.5293
2022-03-12 18:20:59 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 1.6204
2022-03-12 18:21:34 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 1.6085
2022-03-12 18:22:08 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 1.7074
2022-03-12 18:22:41 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 1.6639
2022-03-12 18:23:16 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 1.5284
2022-03-12 18:23:51 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 1.7195
2022-03-12 18:24:26 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 1.5370
2022-03-12 18:25:00 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 1.4710
2022-03-12 18:25:36 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 1.5692
2022-03-12 18:26:10 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 1.6933
2022-03-12 18:26:46 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 1.7337
2022-03-12 18:27:18 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 1.5730
2022-03-12 18:27:52 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 1.3938
2022-03-12 18:28:26 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 1.6086
2022-03-12 18:29:01 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 1.4956
2022-03-12 18:29:37 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 1.7348
2022-03-12 18:30:11 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 1.5071
2022-03-12 18:30:46 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 1.6196
2022-03-12 18:31:20 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 1.4427
2022-03-12 18:31:54 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 1.6147
2022-03-12 18:32:28 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 1.4737
2022-03-12 18:33:02 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 1.5477
2022-03-12 18:33:02 - train: epoch 092, train_loss: 1.6030
2022-03-12 18:34:23 - eval: epoch: 092, acc1: 66.160%, acc5: 87.116%, test_loss: 1.3718, per_image_load_time: 1.705ms, per_image_inference_time: 0.156ms
2022-03-12 18:34:24 - until epoch: 092, best_acc1: 66.180%
2022-03-12 18:34:24 - epoch 093 lr: 0.00010000000000000003
2022-03-12 18:35:06 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 1.6066
2022-03-12 18:35:41 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 1.6810
2022-03-12 18:36:16 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 1.5414
2022-03-12 18:36:48 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 1.6711
2022-03-12 18:37:24 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 1.8279
2022-03-12 18:37:59 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 1.6450
2022-03-12 18:38:34 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 1.5941
2022-03-12 18:39:08 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 1.6055
2022-03-12 18:39:43 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 1.7505
2022-03-12 18:40:18 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 1.3437
2022-03-12 18:40:53 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 1.6714
2022-03-12 18:41:26 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 1.6113
2022-03-12 18:42:00 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 1.5743
2022-03-12 18:42:35 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 1.6081
2022-03-12 18:43:09 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 1.7876
2022-03-12 18:43:44 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 1.5857
2022-03-12 18:44:18 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 1.6228
2022-03-12 18:44:53 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 1.5445
2022-03-12 18:45:28 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 1.3534
2022-03-12 18:46:01 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 1.6269
2022-03-12 18:46:34 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 1.5638
2022-03-12 18:47:07 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 1.6498
2022-03-12 18:47:39 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 1.6508
2022-03-12 18:48:12 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 1.5332
2022-03-12 18:48:46 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 1.5105
2022-03-12 18:49:19 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 1.9980
2022-03-12 18:49:54 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 1.5575
2022-03-12 18:50:26 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 1.6412
2022-03-12 18:50:59 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 1.3298
2022-03-12 18:51:34 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 1.3988
2022-03-12 18:52:08 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 1.5955
2022-03-12 18:52:42 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 1.5684
2022-03-12 18:53:15 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 1.5852
2022-03-12 18:53:50 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 1.6661
2022-03-12 18:54:24 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 1.4509
2022-03-12 18:54:57 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 1.8322
2022-03-12 18:55:31 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 1.6326
2022-03-12 18:56:03 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 1.4537
2022-03-12 18:56:38 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 1.4717
2022-03-12 18:57:11 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 1.6737
2022-03-12 18:57:46 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 1.6782
2022-03-12 18:58:19 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 1.6756
2022-03-12 18:58:52 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 1.6699
2022-03-12 18:59:25 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 1.4596
2022-03-12 19:00:00 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 1.7187
2022-03-12 19:00:32 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 1.3446
2022-03-12 19:01:05 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 1.7411
2022-03-12 19:01:36 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 1.5328
2022-03-12 19:02:11 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 1.5035
2022-03-12 19:02:43 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 1.6378
2022-03-12 19:02:44 - train: epoch 093, train_loss: 1.6006
2022-03-12 19:04:01 - eval: epoch: 093, acc1: 66.216%, acc5: 87.088%, test_loss: 1.3690, per_image_load_time: 2.796ms, per_image_inference_time: 0.165ms
2022-03-12 19:04:01 - until epoch: 093, best_acc1: 66.216%
2022-03-12 19:04:01 - epoch 094 lr: 0.00010000000000000003
2022-03-12 19:04:40 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 1.6832
2022-03-12 19:05:15 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 1.6626
2022-03-12 19:05:48 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 1.6052
2022-03-12 19:06:21 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 1.7636
2022-03-12 19:06:54 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 1.5081
2022-03-12 19:07:29 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 1.4738
2022-03-12 19:08:02 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 1.6290
2022-03-12 19:08:37 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 1.5471
2022-03-12 19:09:10 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 1.5319
2022-03-12 19:09:45 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 1.5404
2022-03-12 19:10:19 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 1.7597
2022-03-12 19:10:53 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 1.6098
2022-03-12 19:11:26 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 1.5994
2022-03-12 19:11:59 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 1.3519
2022-03-12 19:12:33 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 1.5512
2022-03-12 19:13:07 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.8378
2022-03-12 19:13:41 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 1.4072
2022-03-12 19:14:14 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 1.5372
2022-03-12 19:14:49 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 1.6064
2022-03-12 19:15:22 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 1.3993
2022-03-12 19:15:56 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 1.5190
2022-03-12 19:16:29 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 1.6213
2022-03-12 19:17:02 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 1.5020
2022-03-12 19:17:37 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 1.5503
2022-03-12 19:18:11 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 1.5164
2022-03-12 19:18:45 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 1.5474
2022-03-12 19:19:19 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 1.5716
2022-03-12 19:19:53 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 1.6408
2022-03-12 19:20:26 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 1.5018
2022-03-12 19:21:00 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 1.6945
2022-03-12 19:21:33 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 1.7820
2022-03-12 19:22:06 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 1.6323
2022-03-12 19:22:40 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 1.5638
2022-03-12 19:23:15 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 1.5139
2022-03-12 19:23:49 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 1.8921
2022-03-12 19:24:23 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 1.6723
2022-03-12 19:24:57 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 1.6371
2022-03-12 19:25:30 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 1.5992
2022-03-12 19:26:05 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 1.5715
2022-03-12 19:26:38 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 1.6332
2022-03-12 19:27:12 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 1.7615
2022-03-12 19:27:45 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 1.5817
2022-03-12 19:28:20 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 1.8068
2022-03-12 19:28:53 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 1.5761
2022-03-12 19:29:28 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 1.5667
2022-03-12 19:30:01 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 1.6956
2022-03-12 19:30:33 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 1.8149
2022-03-12 19:31:07 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 1.4673
2022-03-12 19:31:39 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 1.6975
2022-03-12 19:32:10 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 1.6085
2022-03-12 19:32:11 - train: epoch 094, train_loss: 1.6015
2022-03-12 19:33:24 - eval: epoch: 094, acc1: 66.256%, acc5: 87.092%, test_loss: 1.3681, per_image_load_time: 2.381ms, per_image_inference_time: 0.147ms
2022-03-12 19:33:24 - until epoch: 094, best_acc1: 66.256%
2022-03-12 19:33:24 - epoch 095 lr: 0.00010000000000000003
2022-03-12 19:34:04 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 1.5909
2022-03-12 19:34:39 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 1.5606
2022-03-12 19:35:12 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 1.6655
2022-03-12 19:35:44 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 1.7068
2022-03-12 19:36:17 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 1.5818
2022-03-12 19:36:52 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 1.4927
2022-03-12 19:37:24 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 1.6204
2022-03-12 19:37:57 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 1.5507
2022-03-12 19:38:29 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 1.7024
2022-03-12 19:39:01 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 1.6491
2022-03-12 19:39:35 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 1.6592
2022-03-12 19:40:08 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 1.4472
2022-03-12 19:40:41 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 1.5102
2022-03-12 19:41:12 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 1.7788
2022-03-12 19:41:45 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 1.5863
2022-03-12 19:42:19 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 1.5003
2022-03-12 19:42:51 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 1.5869
2022-03-12 19:43:29 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 1.6706
2022-03-12 19:44:01 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 1.6057
2022-03-12 19:44:36 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 1.6079
2022-03-12 19:45:12 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 1.5205
2022-03-12 19:45:45 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 1.3331
2022-03-12 19:46:21 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 1.5991
2022-03-12 19:46:55 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 1.5020
2022-03-12 19:47:29 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 1.5223
2022-03-12 19:48:01 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 1.6521
2022-03-12 19:48:36 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 1.5683
2022-03-12 19:49:10 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 1.4111
2022-03-12 19:49:44 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 1.7281
2022-03-12 19:50:19 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 1.8296
2022-03-12 19:50:53 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 1.6328
2022-03-12 19:51:28 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 1.6253
2022-03-12 19:52:01 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 1.6104
2022-03-12 19:52:33 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 1.3907
2022-03-12 19:53:08 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 1.4632
2022-03-12 19:53:42 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 1.5740
2022-03-12 19:54:18 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 1.5351
2022-03-12 19:54:52 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 1.4635
2022-03-12 19:55:27 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 1.8057
2022-03-12 19:56:02 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 1.4606
2022-03-12 19:56:36 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 1.6505
2022-03-12 19:57:11 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 1.5658
2022-03-12 19:57:45 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 1.6903
2022-03-12 19:58:18 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 1.8322
2022-03-12 19:58:52 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 1.5123
2022-03-12 19:59:25 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 1.5229
2022-03-12 20:00:00 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 1.4719
2022-03-12 20:00:35 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 1.4651
2022-03-12 20:01:10 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 1.5017
2022-03-12 20:01:43 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 1.3542
2022-03-12 20:01:43 - train: epoch 095, train_loss: 1.5972
2022-03-12 20:03:00 - eval: epoch: 095, acc1: 66.292%, acc5: 87.070%, test_loss: 1.3674, per_image_load_time: 2.750ms, per_image_inference_time: 0.173ms
2022-03-12 20:03:00 - until epoch: 095, best_acc1: 66.292%
2022-03-12 20:03:00 - epoch 096 lr: 0.00010000000000000003
2022-03-12 20:03:42 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 1.6086
2022-03-12 20:04:16 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 1.5656
2022-03-12 20:04:52 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 1.5180
2022-03-12 20:05:26 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 1.5088
2022-03-12 20:06:02 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 1.6613
2022-03-12 20:06:36 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 1.5712
2022-03-12 20:07:11 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 1.5555
2022-03-12 20:07:43 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 1.4031
2022-03-12 20:08:18 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 1.3347
2022-03-12 20:08:51 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 1.6360
2022-03-12 20:09:25 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 1.7496
2022-03-12 20:10:00 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 1.4247
2022-03-12 20:10:34 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 1.6633
2022-03-12 20:11:09 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 1.6975
2022-03-12 20:11:44 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 1.6198
2022-03-12 20:12:17 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 1.4467
2022-03-12 20:12:52 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 1.3358
2022-03-12 20:13:24 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 1.6880
2022-03-12 20:13:58 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 1.6043
2022-03-12 20:14:31 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 1.4619
2022-03-12 20:15:03 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 1.7191
2022-03-12 20:15:35 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 1.3346
2022-03-12 20:16:09 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 1.5477
2022-03-12 20:16:41 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 1.5636
2022-03-12 20:17:13 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 1.6150
2022-03-12 20:17:46 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 1.4781
2022-03-12 20:18:19 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 1.5889
2022-03-12 20:18:51 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 1.5868
2022-03-12 20:19:24 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 1.6190
2022-03-12 20:19:57 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 1.7109
2022-03-12 20:20:29 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 1.6634
2022-03-12 20:21:04 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 1.6892
2022-03-12 20:21:36 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 1.8009
2022-03-12 20:22:09 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 1.4140
2022-03-12 20:22:41 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 1.4340
2022-03-12 20:23:15 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 1.6691
2022-03-12 20:23:47 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 1.7529
2022-03-12 20:24:20 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 1.4375
2022-03-12 20:24:52 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 1.5503
2022-03-12 20:25:25 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 1.7208
2022-03-12 20:25:58 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 1.5127
2022-03-12 20:26:31 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 1.3004
2022-03-12 20:27:03 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 1.5035
2022-03-12 20:27:36 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 1.5742
2022-03-12 20:28:09 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 1.6564
2022-03-12 20:28:41 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 1.4278
2022-03-12 20:29:14 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 1.5170
2022-03-12 20:29:47 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 1.6183
2022-03-12 20:30:20 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 1.6043
2022-03-12 20:30:51 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 1.6354
2022-03-12 20:30:52 - train: epoch 096, train_loss: 1.5983
2022-03-12 20:32:06 - eval: epoch: 096, acc1: 66.280%, acc5: 87.190%, test_loss: 1.3650, per_image_load_time: 2.733ms, per_image_inference_time: 0.148ms
2022-03-12 20:32:06 - until epoch: 096, best_acc1: 66.292%
2022-03-12 20:32:06 - epoch 097 lr: 0.00010000000000000003
2022-03-12 20:32:44 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 1.6132
2022-03-12 20:33:17 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 1.6733
2022-03-12 20:33:49 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 1.5454
2022-03-12 20:34:22 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 1.6030
2022-03-12 20:34:54 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 1.6082
2022-03-12 20:35:27 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 1.6022
2022-03-12 20:35:57 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 1.4658
2022-03-12 20:36:30 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 1.5476
2022-03-12 20:37:02 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 1.5480
2022-03-12 20:37:35 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 1.7052
2022-03-12 20:38:06 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 1.4508
2022-03-12 20:38:39 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 1.5170
2022-03-12 20:39:12 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 1.4757
2022-03-12 20:39:44 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 1.5315
2022-03-12 20:40:17 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 1.5840
2022-03-12 20:40:50 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 1.3293
2022-03-12 20:41:22 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 1.6053
2022-03-12 20:41:55 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 1.5919
2022-03-12 20:42:28 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 1.5663
2022-03-12 20:43:00 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 1.4223
2022-03-12 20:43:33 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 1.6470
2022-03-12 20:44:06 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 1.3341
2022-03-12 20:44:38 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 1.5227
2022-03-12 20:45:11 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 1.5459
2022-03-12 20:45:44 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 1.5350
2022-03-12 20:46:16 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 1.7109
2022-03-12 20:46:50 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 1.7358
2022-03-12 20:47:22 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 1.4103
2022-03-12 20:47:54 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 1.5543
2022-03-12 20:48:27 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 1.7855
2022-03-12 20:48:59 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 1.4871
2022-03-12 20:49:33 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 1.8687
2022-03-12 20:50:05 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 1.7032
2022-03-12 20:50:37 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 1.7640
2022-03-12 20:51:10 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 1.6407
2022-03-12 20:51:43 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 1.5533
2022-03-12 20:52:15 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 1.3991
2022-03-12 20:52:49 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 1.4610
2022-03-12 20:53:20 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 1.4969
2022-03-12 20:53:53 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 1.7358
2022-03-12 20:54:26 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 1.5488
2022-03-12 20:55:00 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 1.4919
2022-03-12 20:55:32 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 1.6722
2022-03-12 20:56:05 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 1.5508
2022-03-12 20:56:37 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 1.5912
2022-03-12 20:57:10 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 1.5269
2022-03-12 20:57:44 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 1.5521
2022-03-12 20:58:15 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 1.5325
2022-03-12 20:58:49 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 1.6631
2022-03-12 20:59:20 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 1.3167
2022-03-12 20:59:21 - train: epoch 097, train_loss: 1.5970
2022-03-12 21:00:33 - eval: epoch: 097, acc1: 66.300%, acc5: 87.122%, test_loss: 1.3649, per_image_load_time: 2.671ms, per_image_inference_time: 0.155ms
2022-03-12 21:00:34 - until epoch: 097, best_acc1: 66.300%
2022-03-12 21:00:34 - epoch 098 lr: 0.00010000000000000003
2022-03-12 21:01:11 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 1.6267
2022-03-12 21:01:43 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 1.6573
2022-03-12 21:02:15 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 1.7007
2022-03-12 21:02:48 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 1.6518
2022-03-12 21:03:20 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 1.6382
2022-03-12 21:03:53 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 1.6161
2022-03-12 21:04:26 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 1.7148
2022-03-12 21:04:58 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 1.8107
2022-03-12 21:05:31 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 1.5808
2022-03-12 21:06:03 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 1.6856
2022-03-12 21:06:37 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 1.4936
2022-03-12 21:07:09 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 1.6370
2022-03-12 21:07:41 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 1.4408
2022-03-12 21:08:13 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 1.6739
2022-03-12 21:08:46 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 1.4785
2022-03-12 21:09:19 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 1.6160
2022-03-12 21:09:51 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 1.4630
2022-03-12 21:10:24 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 1.5560
2022-03-12 21:10:56 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 1.4474
2022-03-12 21:11:29 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 1.6032
2022-03-12 21:12:01 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 1.7847
2022-03-12 21:12:34 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 1.4637
2022-03-12 21:13:05 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 1.4388
2022-03-12 21:13:38 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 1.6842
2022-03-12 21:14:10 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 1.7136
2022-03-12 21:14:42 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 1.6648
2022-03-12 21:15:15 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 1.6677
2022-03-12 21:15:47 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 1.6210
2022-03-12 21:16:20 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 1.4592
2022-03-12 21:16:53 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 1.5840
2022-03-12 21:17:25 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 1.6621
2022-03-12 21:17:58 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 1.4115
2022-03-12 21:18:29 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 1.5718
2022-03-12 21:19:03 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 1.6350
2022-03-12 21:19:35 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 1.6871
2022-03-12 21:20:07 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 1.8021
2022-03-12 21:20:40 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 1.5363
2022-03-12 21:21:11 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 1.4650
2022-03-12 21:21:44 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 1.4473
2022-03-12 21:22:16 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 1.5429
2022-03-12 21:22:49 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 1.7952
2022-03-12 21:23:21 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 1.6982
2022-03-12 21:23:54 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 1.5480
2022-03-12 21:24:25 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 1.6653
2022-03-12 21:24:58 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 1.4906
2022-03-12 21:25:30 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 1.5702
2022-03-12 21:26:03 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 1.6448
2022-03-12 21:26:35 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 1.5228
2022-03-12 21:27:07 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 1.4914
2022-03-12 21:27:38 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 1.5881
2022-03-12 21:27:39 - train: epoch 098, train_loss: 1.5978
2022-03-12 21:28:52 - eval: epoch: 098, acc1: 66.262%, acc5: 87.108%, test_loss: 1.3658, per_image_load_time: 2.007ms, per_image_inference_time: 0.165ms
2022-03-12 21:28:52 - until epoch: 098, best_acc1: 66.300%
2022-03-12 21:28:52 - epoch 099 lr: 0.00010000000000000003
2022-03-12 21:29:31 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 1.5413
2022-03-12 21:30:03 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 1.6581
2022-03-12 21:30:35 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 1.4309
2022-03-12 21:31:08 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 1.8181
2022-03-12 21:31:39 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 1.6762
2022-03-12 21:32:11 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 1.7582
2022-03-12 21:32:44 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 1.7187
2022-03-12 21:33:16 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 1.7071
2022-03-12 21:33:48 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 1.6573
2022-03-12 21:34:20 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 1.6445
2022-03-12 21:34:54 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 1.5448
2022-03-12 21:35:26 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 1.4301
2022-03-12 21:36:00 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 1.4553
2022-03-12 21:36:32 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 1.6721
2022-03-12 21:37:04 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 1.4593
2022-03-12 21:37:37 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 1.6510
2022-03-12 21:38:09 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 1.5622
2022-03-12 21:38:42 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 1.5924
2022-03-12 21:39:14 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 1.3380
2022-03-12 21:39:47 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 1.4375
2022-03-12 21:40:19 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 1.5522
2022-03-12 21:40:52 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 1.5967
2022-03-12 21:41:25 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 1.4538
2022-03-12 21:41:58 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 1.7601
2022-03-12 21:42:30 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 1.6335
2022-03-12 21:43:03 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 1.4909
2022-03-12 21:43:36 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 1.6718
2022-03-12 21:44:10 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 1.6519
2022-03-12 21:44:43 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 1.6259
2022-03-12 21:45:16 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 1.6371
2022-03-12 21:45:49 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 1.4249
2022-03-12 21:46:21 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 1.6165
2022-03-12 21:46:55 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 1.6493
2022-03-12 21:47:28 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 1.4995
2022-03-12 21:48:01 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 1.5894
2022-03-12 21:48:33 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 1.3634
2022-03-12 21:49:07 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 1.4195
2022-03-12 21:49:39 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 1.4544
2022-03-12 21:50:13 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 1.5471
2022-03-12 21:50:45 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 1.5889
2022-03-12 21:51:19 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 1.5081
2022-03-12 21:51:52 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 1.7514
2022-03-12 21:52:25 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 1.5719
2022-03-12 21:52:57 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 1.5015
2022-03-12 21:53:30 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 1.6658
2022-03-12 21:54:03 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 1.8935
2022-03-12 21:54:37 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 1.4632
2022-03-12 21:55:10 - train: epoch 0099, iter [04800, 05004], lr: 0.000100, loss: 1.6280
2022-03-12 21:55:43 - train: epoch 0099, iter [04900, 05004], lr: 0.000100, loss: 1.6558
2022-03-12 21:56:15 - train: epoch 0099, iter [05000, 05004], lr: 0.000100, loss: 1.7465
2022-03-12 21:56:16 - train: epoch 099, train_loss: 1.5959
2022-03-12 21:57:32 - eval: epoch: 099, acc1: 66.316%, acc5: 87.130%, test_loss: 1.3664, per_image_load_time: 2.446ms, per_image_inference_time: 0.152ms
2022-03-12 21:57:32 - until epoch: 099, best_acc1: 66.316%
2022-03-12 21:57:32 - epoch 100 lr: 0.00010000000000000003
2022-03-12 21:58:13 - train: epoch 0100, iter [00100, 05004], lr: 0.000100, loss: 1.7681
2022-03-12 21:58:46 - train: epoch 0100, iter [00200, 05004], lr: 0.000100, loss: 1.8028
2022-03-12 21:59:22 - train: epoch 0100, iter [00300, 05004], lr: 0.000100, loss: 1.6437
2022-03-12 21:59:57 - train: epoch 0100, iter [00400, 05004], lr: 0.000100, loss: 1.5476
2022-03-12 22:00:32 - train: epoch 0100, iter [00500, 05004], lr: 0.000100, loss: 1.7188
2022-03-12 22:01:07 - train: epoch 0100, iter [00600, 05004], lr: 0.000100, loss: 1.6654
2022-03-12 22:01:43 - train: epoch 0100, iter [00700, 05004], lr: 0.000100, loss: 1.4531
2022-03-12 22:02:15 - train: epoch 0100, iter [00800, 05004], lr: 0.000100, loss: 1.6674
2022-03-12 22:02:50 - train: epoch 0100, iter [00900, 05004], lr: 0.000100, loss: 1.5611
2022-03-12 22:03:23 - train: epoch 0100, iter [01000, 05004], lr: 0.000100, loss: 1.6500
2022-03-12 22:03:58 - train: epoch 0100, iter [01100, 05004], lr: 0.000100, loss: 1.4279
2022-03-12 22:04:32 - train: epoch 0100, iter [01200, 05004], lr: 0.000100, loss: 1.7528
2022-03-12 22:05:06 - train: epoch 0100, iter [01300, 05004], lr: 0.000100, loss: 1.7430
2022-03-12 22:05:39 - train: epoch 0100, iter [01400, 05004], lr: 0.000100, loss: 1.6635
2022-03-12 22:06:15 - train: epoch 0100, iter [01500, 05004], lr: 0.000100, loss: 1.7284
2022-03-12 22:06:47 - train: epoch 0100, iter [01600, 05004], lr: 0.000100, loss: 1.4422
2022-03-12 22:07:21 - train: epoch 0100, iter [01700, 05004], lr: 0.000100, loss: 1.4566
2022-03-12 22:07:53 - train: epoch 0100, iter [01800, 05004], lr: 0.000100, loss: 1.4681
2022-03-12 22:08:29 - train: epoch 0100, iter [01900, 05004], lr: 0.000100, loss: 1.4022
2022-03-12 22:09:02 - train: epoch 0100, iter [02000, 05004], lr: 0.000100, loss: 1.8447
2022-03-12 22:09:36 - train: epoch 0100, iter [02100, 05004], lr: 0.000100, loss: 1.5655
2022-03-12 22:10:10 - train: epoch 0100, iter [02200, 05004], lr: 0.000100, loss: 1.7045
2022-03-12 22:10:44 - train: epoch 0100, iter [02300, 05004], lr: 0.000100, loss: 1.6787
2022-03-12 22:11:17 - train: epoch 0100, iter [02400, 05004], lr: 0.000100, loss: 1.6075
2022-03-12 22:11:50 - train: epoch 0100, iter [02500, 05004], lr: 0.000100, loss: 1.7166
2022-03-12 22:12:24 - train: epoch 0100, iter [02600, 05004], lr: 0.000100, loss: 1.6280
2022-03-12 22:12:58 - train: epoch 0100, iter [02700, 05004], lr: 0.000100, loss: 1.4709
2022-03-12 22:13:31 - train: epoch 0100, iter [02800, 05004], lr: 0.000100, loss: 1.5528
2022-03-12 22:14:05 - train: epoch 0100, iter [02900, 05004], lr: 0.000100, loss: 1.6053
2022-03-12 22:14:39 - train: epoch 0100, iter [03000, 05004], lr: 0.000100, loss: 1.6503
2022-03-12 22:15:14 - train: epoch 0100, iter [03100, 05004], lr: 0.000100, loss: 1.6993
2022-03-12 22:15:47 - train: epoch 0100, iter [03200, 05004], lr: 0.000100, loss: 1.9275
2022-03-12 22:16:19 - train: epoch 0100, iter [03300, 05004], lr: 0.000100, loss: 1.5548
2022-03-12 22:16:53 - train: epoch 0100, iter [03400, 05004], lr: 0.000100, loss: 1.8002
2022-03-12 22:17:28 - train: epoch 0100, iter [03500, 05004], lr: 0.000100, loss: 1.4113
2022-03-12 22:18:02 - train: epoch 0100, iter [03600, 05004], lr: 0.000100, loss: 1.5689
2022-03-12 22:18:36 - train: epoch 0100, iter [03700, 05004], lr: 0.000100, loss: 1.4590
2022-03-12 22:19:10 - train: epoch 0100, iter [03800, 05004], lr: 0.000100, loss: 1.5486
2022-03-12 22:19:44 - train: epoch 0100, iter [03900, 05004], lr: 0.000100, loss: 1.6600
2022-03-12 22:20:17 - train: epoch 0100, iter [04000, 05004], lr: 0.000100, loss: 1.5937
2022-03-12 22:20:50 - train: epoch 0100, iter [04100, 05004], lr: 0.000100, loss: 1.6640
2022-03-12 22:21:24 - train: epoch 0100, iter [04200, 05004], lr: 0.000100, loss: 1.6875
2022-03-12 22:21:57 - train: epoch 0100, iter [04300, 05004], lr: 0.000100, loss: 1.5203
2022-03-12 22:22:31 - train: epoch 0100, iter [04400, 05004], lr: 0.000100, loss: 1.6661
2022-03-12 22:23:06 - train: epoch 0100, iter [04500, 05004], lr: 0.000100, loss: 1.4430
2022-03-12 22:23:40 - train: epoch 0100, iter [04600, 05004], lr: 0.000100, loss: 1.6426
2022-03-12 22:24:14 - train: epoch 0100, iter [04700, 05004], lr: 0.000100, loss: 1.8067
2022-03-12 22:24:48 - train: epoch 0100, iter [04800, 05004], lr: 0.000100, loss: 1.3795
2022-03-12 22:25:21 - train: epoch 0100, iter [04900, 05004], lr: 0.000100, loss: 1.4420
2022-03-12 22:25:52 - train: epoch 0100, iter [05000, 05004], lr: 0.000100, loss: 1.6963
2022-03-12 22:25:53 - train: epoch 100, train_loss: 1.5949
2022-03-12 22:27:11 - eval: epoch: 100, acc1: 66.334%, acc5: 87.108%, test_loss: 1.3650, per_image_load_time: 2.723ms, per_image_inference_time: 0.151ms
2022-03-12 22:27:11 - until epoch: 100, best_acc1: 66.334%
2022-03-12 22:27:11 - train done. model: yoloxtbackbone, train time: 48.010 hours, best_acc1: 66.334%
