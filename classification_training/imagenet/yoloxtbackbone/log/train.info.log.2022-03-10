2022-03-09 23:46:58 - network: yoloxtbackbone
2022-03-09 23:46:58 - num_classes: 1000
2022-03-09 23:46:58 - input_image_size: 256
2022-03-09 23:46:58 - scale: 1.1428571428571428
2022-03-09 23:46:58 - trained_model_path: 
2022-03-09 23:46:58 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-09 23:46:58 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f367c81df70>
2022-03-09 23:46:58 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f365f73f280>
2022-03-09 23:46:58 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f365f73f2b0>
2022-03-09 23:46:58 - seed: 0
2022-03-09 23:46:58 - batch_size: 256
2022-03-09 23:46:58 - num_workers: 16
2022-03-09 23:46:58 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-09 23:46:58 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-03-09 23:46:58 - epochs: 100
2022-03-09 23:46:58 - print_interval: 100
2022-03-09 23:46:58 - distributed: True
2022-03-09 23:46:58 - sync_bn: False
2022-03-09 23:46:58 - apex: True
2022-03-09 23:46:58 - gpus_type: NVIDIA RTX A5000
2022-03-09 23:46:58 - gpus_num: 2
2022-03-09 23:46:58 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f365f285730>
2022-03-09 23:46:58 - --------------------parameters--------------------
2022-03-09 23:46:58 - name: conv.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: conv.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: conv.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer1.0.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer1.0.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer1.0.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer1.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer1.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer1.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer1.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer1.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer1.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer2.0.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer2.0.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer2.0.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer3.0.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer3.0.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer3.0.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer4.0.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer4.0.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer4.0.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer4.2.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer4.2.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer4.2.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: layer4.2.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-09 23:46:58 - name: layer4.2.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-09 23:46:58 - name: layer4.2.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-09 23:46:58 - name: fc.weight, grad: True
2022-03-09 23:46:58 - name: fc.bias, grad: True
2022-03-09 23:46:58 - --------------------buffers--------------------
2022-03-09 23:46:58 - name: conv.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: conv.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: conv.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer1.0.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer1.0.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer1.0.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer1.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer1.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer1.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer1.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer1.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer1.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer2.0.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer2.0.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer2.0.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer2.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer3.0.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer3.0.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer3.0.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer3.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer4.0.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer4.0.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer4.0.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer4.2.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer4.2.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer4.2.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - name: layer4.2.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-09 23:46:58 - name: layer4.2.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-09 23:46:58 - name: layer4.2.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-09 23:46:58 - epoch 001 lr: 0.1
2022-03-09 23:47:37 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9027
2022-03-09 23:48:10 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.7380
2022-03-09 23:48:43 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.7350
2022-03-09 23:49:16 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.5868
2022-03-09 23:49:50 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.3770
2022-03-09 23:50:23 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.1626
2022-03-09 23:50:57 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.1466
2022-03-09 23:51:31 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 5.9787
2022-03-09 23:52:04 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.8509
2022-03-09 23:52:38 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.8028
2022-03-09 23:53:11 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.7290
2022-03-09 23:53:46 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.6217
2022-03-09 23:54:18 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.4974
2022-03-09 23:54:52 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.5383
2022-03-09 23:55:24 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.3998
2022-03-09 23:55:56 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.4331
2022-03-09 23:56:30 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.2567
2022-03-09 23:57:03 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.2994
2022-03-09 23:57:37 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.1805
2022-03-09 23:58:09 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.0900
2022-03-09 23:58:42 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.1043
2022-03-09 23:59:15 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.0977
2022-03-09 23:59:49 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.8644
2022-03-10 00:00:22 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.8876
2022-03-10 00:00:56 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.9700
2022-03-10 00:01:29 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 4.9810
2022-03-10 00:02:02 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.0466
2022-03-10 00:02:36 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.8920
2022-03-10 00:03:08 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.7487
2022-03-10 00:03:41 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.9259
2022-03-10 00:04:14 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.9483
2022-03-10 00:04:47 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.8106
2022-03-10 00:05:20 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.6058
2022-03-10 00:05:53 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.5479
2022-03-10 00:06:25 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.7308
2022-03-10 00:06:59 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.7092
2022-03-10 00:07:31 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.7602
2022-03-10 00:08:05 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.5465
2022-03-10 00:08:38 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.6647
2022-03-10 00:09:12 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.4127
2022-03-10 00:09:45 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.6211
2022-03-10 00:10:18 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.3658
2022-03-10 00:10:51 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.4314
2022-03-10 00:11:24 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.3233
2022-03-10 00:11:57 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.5717
2022-03-10 00:12:30 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.5956
2022-03-10 00:13:03 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.3856
2022-03-10 00:13:37 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.6253
2022-03-10 00:14:11 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.4390
2022-03-10 00:14:43 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.2414
2022-03-10 00:14:44 - train: epoch 001, train_loss: 5.1562
2022-03-10 00:15:58 - eval: epoch: 001, acc1: 16.386%, acc5: 37.240%, test_loss: 4.2079, per_image_load_time: 2.742ms, per_image_inference_time: 0.139ms
2022-03-10 00:15:58 - until epoch: 001, best_acc1: 16.386%
2022-03-10 00:15:58 - epoch 002 lr: 0.1
2022-03-10 00:16:37 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.4176
2022-03-10 00:17:10 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.2129
2022-03-10 00:17:45 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.4925
2022-03-10 00:18:18 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.3698
2022-03-10 00:18:51 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.0784
2022-03-10 00:19:25 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.0788
2022-03-10 00:19:58 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.3164
2022-03-10 00:20:32 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 4.0492
2022-03-10 00:21:07 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.9286
2022-03-10 00:21:40 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.3806
2022-03-10 00:22:13 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.1559
2022-03-10 00:22:47 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.1373
2022-03-10 00:23:22 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.1233
2022-03-10 00:23:55 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.3337
2022-03-10 00:24:28 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.3579
2022-03-10 00:25:02 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 4.0253
2022-03-10 00:25:36 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.0948
2022-03-10 00:26:10 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.0722
2022-03-10 00:26:44 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 4.0375
2022-03-10 00:27:18 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.7808
2022-03-10 00:27:52 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 4.0417
2022-03-10 00:28:25 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.7658
2022-03-10 00:28:59 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 4.1134
2022-03-10 00:29:32 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.8681
2022-03-10 00:30:06 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.9006
2022-03-10 00:30:40 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.8339
2022-03-10 00:31:14 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 4.0912
2022-03-10 00:31:48 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 4.1390
2022-03-10 00:32:23 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.8606
2022-03-10 00:32:55 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.8396
2022-03-10 00:33:31 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.8333
2022-03-10 00:34:04 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.9918
2022-03-10 00:34:38 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.8719
2022-03-10 00:35:12 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.9869
2022-03-10 00:35:45 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.8077
2022-03-10 00:36:19 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.8757
2022-03-10 00:36:54 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.9265
2022-03-10 00:37:28 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.7560
2022-03-10 00:38:02 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.9229
2022-03-10 00:38:36 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.7846
2022-03-10 00:39:11 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.9639
2022-03-10 00:39:45 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.6754
2022-03-10 00:40:19 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.7571
2022-03-10 00:40:52 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.6491
2022-03-10 00:41:27 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.7114
2022-03-10 00:42:00 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.7813
2022-03-10 00:42:35 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.8104
2022-03-10 00:43:09 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.7590
2022-03-10 00:43:43 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.5631
2022-03-10 00:44:16 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.7894
2022-03-10 00:44:17 - train: epoch 002, train_loss: 3.9906
2022-03-10 00:45:33 - eval: epoch: 002, acc1: 25.488%, acc5: 50.744%, test_loss: 3.5283, per_image_load_time: 2.137ms, per_image_inference_time: 0.161ms
2022-03-10 00:45:33 - until epoch: 002, best_acc1: 25.488%
2022-03-10 00:45:33 - epoch 003 lr: 0.1
2022-03-10 00:46:12 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.7366
2022-03-10 00:46:46 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.7710
2022-03-10 00:47:18 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.7003
2022-03-10 00:47:51 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.7243
2022-03-10 00:48:24 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.8951
2022-03-10 00:48:59 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.5161
2022-03-10 00:49:31 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.9457
2022-03-10 00:50:05 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.8119
2022-03-10 00:50:37 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.6225
2022-03-10 00:51:11 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.7931
2022-03-10 00:51:45 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.5095
2022-03-10 00:52:19 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.6291
2022-03-10 00:52:52 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.5855
2022-03-10 00:53:26 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.6252
2022-03-10 00:54:00 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.8303
2022-03-10 00:54:33 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.6195
2022-03-10 00:55:07 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.5074
2022-03-10 00:55:41 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.5017
2022-03-10 00:56:14 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.7159
2022-03-10 00:56:48 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.9565
2022-03-10 00:57:22 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.7985
2022-03-10 00:57:55 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.9590
2022-03-10 00:58:28 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.5519
2022-03-10 00:59:03 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.6776
2022-03-10 00:59:36 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.7394
2022-03-10 01:00:10 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.5350
2022-03-10 01:00:44 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.8135
2022-03-10 01:01:18 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.4354
2022-03-10 01:01:51 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.5027
2022-03-10 01:02:24 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.7481
2022-03-10 01:02:58 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.7534
2022-03-10 01:03:31 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.6276
2022-03-10 01:04:06 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.4444
2022-03-10 01:04:38 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.6548
2022-03-10 01:05:12 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.3921
2022-03-10 01:05:45 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.5279
2022-03-10 01:06:19 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.6534
2022-03-10 01:06:53 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.6486
2022-03-10 01:07:26 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.7889
2022-03-10 01:08:00 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.4702
2022-03-10 01:08:33 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.5989
2022-03-10 01:09:07 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.5078
2022-03-10 01:09:40 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 3.1641
2022-03-10 01:10:15 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.2922
2022-03-10 01:10:48 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.5200
2022-03-10 01:11:22 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.4524
2022-03-10 01:11:54 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.3979
2022-03-10 01:12:29 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.4995
2022-03-10 01:13:02 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.6380
2022-03-10 01:13:35 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.6250
2022-03-10 01:13:36 - train: epoch 003, train_loss: 3.6084
2022-03-10 01:14:50 - eval: epoch: 003, acc1: 29.630%, acc5: 55.660%, test_loss: 3.2678, per_image_load_time: 2.015ms, per_image_inference_time: 0.145ms
2022-03-10 01:14:50 - until epoch: 003, best_acc1: 29.630%
2022-03-10 01:14:50 - epoch 004 lr: 0.1
2022-03-10 01:15:28 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.4864
2022-03-10 01:16:02 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.4926
2022-03-10 01:16:36 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.4510
2022-03-10 01:17:08 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 3.3435
2022-03-10 01:17:41 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.3707
2022-03-10 01:18:15 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.7027
2022-03-10 01:18:50 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.5671
2022-03-10 01:19:22 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 3.3196
2022-03-10 01:19:56 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 3.2671
2022-03-10 01:20:28 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.4151
2022-03-10 01:21:02 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.5117
2022-03-10 01:21:36 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 3.2214
2022-03-10 01:22:10 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 3.3042
2022-03-10 01:22:43 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.4281
2022-03-10 01:23:17 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.5495
2022-03-10 01:23:51 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 3.3108
2022-03-10 01:24:25 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.4630
2022-03-10 01:24:58 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.5688
2022-03-10 01:25:32 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.5661
2022-03-10 01:26:06 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.3801
2022-03-10 01:26:39 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.3681
2022-03-10 01:27:14 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.3588
2022-03-10 01:27:47 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 3.2423
2022-03-10 01:28:22 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 3.2251
2022-03-10 01:28:54 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 3.2162
2022-03-10 01:29:28 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.4730
2022-03-10 01:30:02 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 3.2706
2022-03-10 01:30:35 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.2456
2022-03-10 01:31:10 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 3.3204
2022-03-10 01:31:43 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 3.2453
2022-03-10 01:32:17 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.4169
2022-03-10 01:32:51 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 3.3692
2022-03-10 01:33:25 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.4165
2022-03-10 01:33:58 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 3.4182
2022-03-10 01:34:32 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 3.3296
2022-03-10 01:35:06 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 3.1738
2022-03-10 01:35:40 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.4030
2022-03-10 01:36:13 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 3.2912
2022-03-10 01:36:48 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 3.1832
2022-03-10 01:37:21 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 3.0806
2022-03-10 01:37:55 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 3.2939
2022-03-10 01:38:28 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 3.2379
2022-03-10 01:39:02 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 3.2215
2022-03-10 01:39:36 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 3.0271
2022-03-10 01:40:11 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.9370
2022-03-10 01:40:44 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 3.3385
2022-03-10 01:41:19 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 3.2666
2022-03-10 01:41:52 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 3.1942
2022-03-10 01:42:27 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 3.2535
2022-03-10 01:43:01 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 3.4475
2022-03-10 01:43:02 - train: epoch 004, train_loss: 3.3893
2022-03-10 01:44:17 - eval: epoch: 004, acc1: 32.358%, acc5: 58.874%, test_loss: 3.1140, per_image_load_time: 2.466ms, per_image_inference_time: 0.163ms
2022-03-10 01:44:17 - until epoch: 004, best_acc1: 32.358%
2022-03-10 01:44:17 - epoch 005 lr: 0.1
2022-03-10 01:44:56 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.3014
2022-03-10 01:45:30 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.3497
2022-03-10 01:46:04 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.4856
2022-03-10 01:46:36 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.3181
2022-03-10 01:47:10 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 3.1776
2022-03-10 01:47:43 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 3.2803
2022-03-10 01:48:16 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 3.4802
2022-03-10 01:48:51 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.6211
2022-03-10 01:49:23 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 3.2542
2022-03-10 01:49:57 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.3633
2022-03-10 01:50:31 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.3606
2022-03-10 01:51:04 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 3.3298
2022-03-10 01:51:38 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 3.3081
2022-03-10 01:52:12 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.3707
2022-03-10 01:52:46 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 3.1206
2022-03-10 01:53:20 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 3.1301
2022-03-10 01:53:53 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 3.1752
2022-03-10 01:54:27 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 3.2030
2022-03-10 01:55:01 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.9813
2022-03-10 01:55:34 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 3.0919
2022-03-10 01:56:08 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 3.0924
2022-03-10 01:56:42 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 3.2342
2022-03-10 01:57:14 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 3.1765
2022-03-10 01:57:49 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 3.1415
2022-03-10 01:58:20 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 3.2385
2022-03-10 01:58:54 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.5750
2022-03-10 01:59:28 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.4269
2022-03-10 02:00:02 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 3.2071
2022-03-10 02:00:35 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 3.0780
2022-03-10 02:01:09 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 3.2744
2022-03-10 02:01:44 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.3206
2022-03-10 02:02:17 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 3.3136
2022-03-10 02:02:51 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 3.1053
2022-03-10 02:03:26 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 3.1151
2022-03-10 02:03:59 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 3.2227
2022-03-10 02:04:32 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 3.2356
2022-03-10 02:05:06 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 3.2304
2022-03-10 02:05:40 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 3.1833
2022-03-10 02:06:15 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.5124
2022-03-10 02:06:47 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 3.1294
2022-03-10 02:07:22 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 3.2140
2022-03-10 02:07:56 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 3.2738
2022-03-10 02:08:30 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 3.1812
2022-03-10 02:09:05 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 3.2860
2022-03-10 02:09:38 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 3.3914
2022-03-10 02:10:12 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 3.1619
2022-03-10 02:10:46 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.9746
2022-03-10 02:11:20 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 3.0844
2022-03-10 02:11:56 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 3.2461
2022-03-10 02:12:28 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 3.1687
2022-03-10 02:12:29 - train: epoch 005, train_loss: 3.2515
2022-03-10 02:13:45 - eval: epoch: 005, acc1: 33.634%, acc5: 60.236%, test_loss: 3.0345, per_image_load_time: 2.327ms, per_image_inference_time: 0.157ms
2022-03-10 02:13:45 - until epoch: 005, best_acc1: 33.634%
2022-03-10 02:13:45 - epoch 006 lr: 0.1
2022-03-10 02:14:24 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 3.1074
2022-03-10 02:14:57 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 3.0452
2022-03-10 02:15:31 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.9013
2022-03-10 02:16:03 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.2928
2022-03-10 02:16:37 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 3.2074
2022-03-10 02:17:11 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 3.2515
2022-03-10 02:17:44 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.1827
2022-03-10 02:18:17 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 3.1543
2022-03-10 02:18:51 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 3.0285
2022-03-10 02:19:24 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 3.0996
2022-03-10 02:19:57 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 3.0638
2022-03-10 02:20:30 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 3.3650
2022-03-10 02:21:04 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.3328
2022-03-10 02:21:37 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 3.1717
2022-03-10 02:22:11 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 3.3516
2022-03-10 02:22:44 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.9616
2022-03-10 02:23:18 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 3.2662
2022-03-10 02:23:51 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 3.2802
2022-03-10 02:24:25 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 3.0770
2022-03-10 02:24:58 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 3.3272
2022-03-10 02:25:32 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 3.1308
2022-03-10 02:26:05 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.9962
2022-03-10 02:26:39 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 3.0308
2022-03-10 02:27:13 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 3.1434
2022-03-10 02:27:46 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 3.3731
2022-03-10 02:28:20 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 3.1635
2022-03-10 02:28:53 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 3.2738
2022-03-10 02:29:26 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 3.0595
2022-03-10 02:29:59 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 3.1898
2022-03-10 02:30:33 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.9878
2022-03-10 02:31:06 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.9698
2022-03-10 02:31:40 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.9266
2022-03-10 02:32:13 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 3.0668
2022-03-10 02:32:47 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 3.2120
2022-03-10 02:33:20 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 3.1122
2022-03-10 02:33:53 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 3.1793
2022-03-10 02:34:26 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 3.1233
2022-03-10 02:35:00 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.9063
2022-03-10 02:35:33 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.9722
2022-03-10 02:36:06 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.3940
2022-03-10 02:36:40 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 3.0651
2022-03-10 02:37:13 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 3.1404
2022-03-10 02:37:46 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.9634
2022-03-10 02:38:20 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 3.1853
2022-03-10 02:38:53 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 3.1112
2022-03-10 02:39:27 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 3.1122
2022-03-10 02:40:00 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 3.2404
2022-03-10 02:40:32 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 3.1128
2022-03-10 02:41:08 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 3.2437
2022-03-10 02:41:40 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 3.0085
2022-03-10 02:41:41 - train: epoch 006, train_loss: 3.1580
2022-03-10 02:42:55 - eval: epoch: 006, acc1: 36.418%, acc5: 63.544%, test_loss: 2.8630, per_image_load_time: 2.659ms, per_image_inference_time: 0.155ms
2022-03-10 02:42:55 - until epoch: 006, best_acc1: 36.418%
2022-03-10 08:13:16 - epoch 007 lr: 0.1
2022-03-10 08:13:54 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 3.0995
2022-03-10 08:14:28 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 3.2936
2022-03-10 08:15:02 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 3.2873
2022-03-10 08:15:35 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 3.1126
2022-03-10 08:16:09 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 3.0769
2022-03-10 08:16:42 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 3.0743
2022-03-10 08:17:16 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 3.0747
2022-03-10 08:17:49 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 3.2135
2022-03-10 08:18:23 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 3.3027
2022-03-10 08:18:55 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 3.0994
2022-03-10 08:19:30 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 3.1189
2022-03-10 08:20:04 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 3.1833
2022-03-10 08:20:37 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.9175
2022-03-10 08:21:11 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 3.2246
2022-03-10 08:21:43 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 3.2316
2022-03-10 08:22:17 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 3.1632
2022-03-10 08:22:51 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 3.1079
2022-03-10 08:23:24 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 3.0473
2022-03-10 08:23:58 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 3.1528
2022-03-10 08:24:31 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 3.0410
2022-03-10 08:25:05 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 3.3817
2022-03-10 08:25:37 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.8807
2022-03-10 08:26:12 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 3.1965
2022-03-10 08:26:45 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 3.4119
2022-03-10 08:27:20 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 3.0922
2022-03-10 08:27:53 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 3.0700
2022-03-10 08:28:27 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 3.0191
2022-03-10 08:29:01 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.9837
2022-03-10 08:29:35 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.8655
2022-03-10 08:30:08 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 3.0109
2022-03-10 08:30:43 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.9456
2022-03-10 08:31:15 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 3.0174
2022-03-10 08:31:49 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 3.3278
2022-03-10 08:32:22 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 3.0547
2022-03-10 08:32:56 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 3.1827
2022-03-10 08:33:29 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.9933
2022-03-10 08:34:04 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 3.2367
2022-03-10 08:34:37 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 3.2342
2022-03-10 08:35:11 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 3.1185
2022-03-10 08:35:44 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 3.1965
2022-03-10 08:36:18 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.9414
2022-03-10 08:36:51 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.9637
2022-03-10 08:37:25 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 3.1488
2022-03-10 08:37:58 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.9177
2022-03-10 08:38:33 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 3.0719
2022-03-10 08:39:06 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 3.3293
2022-03-10 08:39:41 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 3.0204
2022-03-10 08:40:14 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 3.2768
2022-03-10 08:40:49 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 3.0379
2022-03-10 08:41:22 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 3.1116
2022-03-10 08:41:23 - train: epoch 007, train_loss: 3.0850
2022-03-10 08:42:37 - eval: epoch: 007, acc1: 36.442%, acc5: 63.426%, test_loss: 2.8723, per_image_load_time: 1.755ms, per_image_inference_time: 0.168ms
2022-03-10 08:42:37 - until epoch: 007, best_acc1: 36.442%
2022-03-10 08:42:37 - epoch 008 lr: 0.1
2022-03-10 08:43:15 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 3.1348
2022-03-10 08:43:50 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 3.1183
2022-03-10 08:44:22 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.9506
2022-03-10 08:44:57 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.9478
2022-03-10 08:45:29 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.8740
2022-03-10 08:46:03 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 3.0243
2022-03-10 08:46:36 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 3.2060
2022-03-10 08:47:09 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.7751
2022-03-10 08:47:42 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 3.0292
2022-03-10 08:48:16 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 3.3207
2022-03-10 08:48:50 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.7927
2022-03-10 08:49:23 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.9894
2022-03-10 08:49:57 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 3.1692
2022-03-10 08:50:30 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.9273
2022-03-10 08:51:03 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 3.0537
2022-03-10 08:51:38 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 3.1494
2022-03-10 08:52:11 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 3.0355
2022-03-10 08:52:44 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.9916
2022-03-10 08:53:18 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 3.1594
2022-03-10 08:53:52 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.9860
2022-03-10 08:54:26 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 3.0439
2022-03-10 08:54:59 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.9771
2022-03-10 08:55:32 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 3.0976
2022-03-10 08:56:06 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.9397
2022-03-10 08:56:40 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.9929
2022-03-10 08:57:13 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.9273
2022-03-10 08:57:47 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 3.2165
2022-03-10 08:58:20 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 3.0819
2022-03-10 08:58:54 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.9973
2022-03-10 08:59:26 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 3.1153
2022-03-10 09:00:01 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 3.0805
2022-03-10 09:00:34 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 3.1763
2022-03-10 09:01:08 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 3.0609
2022-03-10 09:01:41 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 3.0677
2022-03-10 09:02:15 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 3.0890
2022-03-10 09:02:48 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 3.1457
2022-03-10 09:03:22 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.9456
2022-03-10 09:03:55 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 3.1891
2022-03-10 09:04:30 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 3.0293
2022-03-10 09:05:03 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 3.2779
2022-03-10 09:05:38 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.9003
2022-03-10 09:06:11 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.9391
2022-03-10 09:06:45 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.9832
2022-03-10 09:07:18 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 3.0555
2022-03-10 09:07:52 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 3.0123
2022-03-10 09:08:25 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 3.0968
2022-03-10 09:08:59 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 3.1811
2022-03-10 09:09:32 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 3.0612
2022-03-10 09:10:07 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.8417
2022-03-10 09:10:39 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.8536
2022-03-10 09:10:40 - train: epoch 008, train_loss: 3.0309
2022-03-10 09:11:56 - eval: epoch: 008, acc1: 38.960%, acc5: 66.532%, test_loss: 2.7011, per_image_load_time: 2.596ms, per_image_inference_time: 0.139ms
2022-03-10 09:11:56 - until epoch: 008, best_acc1: 38.960%
2022-03-10 09:11:56 - epoch 009 lr: 0.1
2022-03-10 09:12:34 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.8618
2022-03-10 09:13:09 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.8537
2022-03-10 09:13:41 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.8283
2022-03-10 09:14:15 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 3.4407
2022-03-10 09:14:48 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 3.0369
2022-03-10 09:15:22 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.9470
2022-03-10 09:15:56 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.7854
2022-03-10 09:16:29 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.8160
2022-03-10 09:17:03 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.8902
2022-03-10 09:17:37 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.8808
2022-03-10 09:18:10 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 3.2699
2022-03-10 09:18:44 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 3.1855
2022-03-10 09:19:17 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 3.1501
2022-03-10 09:19:50 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.8366
2022-03-10 09:20:24 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.8723
2022-03-10 09:20:58 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 3.0511
2022-03-10 09:21:32 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.9973
2022-03-10 09:22:06 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.7998
2022-03-10 09:22:39 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.8174
2022-03-10 09:23:13 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.9008
2022-03-10 09:23:47 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 3.1639
2022-03-10 09:24:21 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 3.0588
2022-03-10 09:24:54 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.7890
2022-03-10 09:25:27 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 3.0299
2022-03-10 09:26:00 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.8896
2022-03-10 09:26:34 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 3.0115
2022-03-10 09:27:07 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.8963
2022-03-10 09:27:41 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 3.1314
2022-03-10 09:28:15 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.6737
2022-03-10 09:28:48 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 3.0489
2022-03-10 09:29:22 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 3.0481
2022-03-10 09:29:55 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 3.1068
2022-03-10 09:30:29 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 3.0577
2022-03-10 09:31:02 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 3.0817
2022-03-10 09:31:35 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 3.0014
2022-03-10 09:32:09 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.7749
2022-03-10 09:32:43 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.9333
2022-03-10 09:33:16 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 3.0843
2022-03-10 09:33:50 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.8324
2022-03-10 09:34:24 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 3.1956
2022-03-10 09:34:57 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.9510
2022-03-10 09:35:31 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.9189
2022-03-10 09:36:04 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.8597
2022-03-10 09:36:39 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.9117
2022-03-10 09:37:11 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 3.1797
2022-03-10 09:37:45 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 3.0061
2022-03-10 09:38:19 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 3.1234
2022-03-10 09:38:52 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 3.0601
2022-03-10 09:39:27 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 3.0438
2022-03-10 09:40:00 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.8152
2022-03-10 09:40:00 - train: epoch 009, train_loss: 2.9867
2022-03-10 09:41:15 - eval: epoch: 009, acc1: 38.080%, acc5: 65.300%, test_loss: 2.7566, per_image_load_time: 2.230ms, per_image_inference_time: 0.144ms
2022-03-10 09:41:15 - until epoch: 009, best_acc1: 38.960%
2022-03-10 09:41:15 - epoch 010 lr: 0.1
2022-03-10 09:41:54 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.9328
2022-03-10 09:42:27 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.9967
2022-03-10 09:43:01 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 3.1543
2022-03-10 09:43:35 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.9764
2022-03-10 09:44:08 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.9185
2022-03-10 09:44:41 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 3.1832
2022-03-10 09:45:15 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.9314
2022-03-10 09:45:49 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.8746
2022-03-10 09:46:22 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.7931
2022-03-10 09:46:56 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.8830
2022-03-10 09:47:29 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.9295
2022-03-10 09:48:03 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.9285
2022-03-10 09:48:37 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.8120
2022-03-10 09:49:09 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 3.1160
2022-03-10 09:49:44 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.8279
2022-03-10 09:50:18 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 3.0296
2022-03-10 09:50:50 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 3.2335
2022-03-10 09:51:24 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.5762
2022-03-10 09:51:58 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.9986
2022-03-10 09:52:32 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 3.0540
2022-03-10 09:53:05 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.7252
2022-03-10 09:53:38 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 3.1607
2022-03-10 09:54:12 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.9450
2022-03-10 09:54:46 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 3.2740
2022-03-10 09:55:20 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 3.0369
2022-03-10 09:55:53 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 3.0542
2022-03-10 09:56:27 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.6081
2022-03-10 09:57:01 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.9289
2022-03-10 09:57:33 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 3.1188
2022-03-10 09:58:07 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 3.0628
2022-03-10 09:58:41 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 3.0890
2022-03-10 09:59:15 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.9491
2022-03-10 09:59:49 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 3.0683
2022-03-10 10:00:23 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 3.0238
2022-03-10 10:00:56 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.9793
2022-03-10 10:01:30 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 3.1939
2022-03-10 10:02:03 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.9576
2022-03-10 10:02:36 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.9637
2022-03-10 10:03:11 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.6879
2022-03-10 10:03:44 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.7845
2022-03-10 10:04:18 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.9534
2022-03-10 10:04:51 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.8517
2022-03-10 10:05:25 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.9296
2022-03-10 10:05:58 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.6926
2022-03-10 10:06:34 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.9419
2022-03-10 10:07:07 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.6710
2022-03-10 10:07:40 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 3.1093
2022-03-10 10:08:17 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.8130
2022-03-10 10:08:57 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.7226
2022-03-10 10:09:30 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.9462
2022-03-10 10:09:31 - train: epoch 010, train_loss: 2.9472
2022-03-10 10:10:52 - eval: epoch: 010, acc1: 40.632%, acc5: 67.656%, test_loss: 2.6348, per_image_load_time: 2.489ms, per_image_inference_time: 0.149ms
2022-03-10 10:10:52 - until epoch: 010, best_acc1: 40.632%
2022-03-10 23:12:38 - epoch 011 lr: 0.1
2022-03-10 23:13:16 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.9395
2022-03-10 23:13:49 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 3.0281
2022-03-10 23:14:22 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.9718
2022-03-10 23:14:56 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.8615
2022-03-10 23:15:30 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.7261
2022-03-10 23:16:03 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.8253
2022-03-10 23:16:36 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 3.0898
2022-03-10 23:17:10 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 3.0078
2022-03-10 23:17:43 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 3.0031
2022-03-10 23:18:17 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.7016
2022-03-10 23:18:49 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.9867
2022-03-10 23:19:24 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 3.0765
2022-03-10 23:19:57 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 3.1140
2022-03-10 23:20:30 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.9625
2022-03-10 23:21:03 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.8573
2022-03-10 23:21:36 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.9590
2022-03-10 23:22:09 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 3.0744
2022-03-10 23:22:41 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.7009
2022-03-10 23:23:14 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.7185
2022-03-10 23:23:46 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.9246
2022-03-10 23:24:20 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.7441
2022-03-10 23:24:52 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.8319
2022-03-10 23:25:25 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.9938
2022-03-10 23:25:58 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.7815
2022-03-10 23:26:31 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.8178
2022-03-10 23:27:03 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 3.0364
2022-03-10 23:27:36 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.8701
2022-03-10 23:28:09 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.7477
2022-03-10 23:28:43 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.9653
2022-03-10 23:29:14 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 3.2047
2022-03-10 23:29:48 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.8850
2022-03-10 23:30:20 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.8376
2022-03-10 23:30:52 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 3.2673
2022-03-10 23:31:26 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.8280
2022-03-10 23:31:58 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 3.0786
2022-03-10 23:32:31 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 3.0351
2022-03-10 23:33:04 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 3.1146
2022-03-10 23:33:36 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.6984
2022-03-10 23:34:09 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 3.2175
2022-03-10 23:34:43 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.8867
2022-03-10 23:35:16 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.8095
2022-03-10 23:35:48 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.7413
2022-03-10 23:36:21 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.8396
2022-03-10 23:36:54 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.9329
2022-03-10 23:37:27 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.6365
2022-03-10 23:38:00 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.9222
2022-03-10 23:38:34 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.5451
2022-03-10 23:39:07 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.8499
2022-03-10 23:39:40 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.8212
2022-03-10 23:40:12 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.6314
2022-03-10 23:40:13 - train: epoch 011, train_loss: 2.9167
2022-03-10 23:41:27 - eval: epoch: 011, acc1: 41.526%, acc5: 68.606%, test_loss: 2.5871, per_image_load_time: 1.983ms, per_image_inference_time: 0.138ms
2022-03-10 23:41:27 - until epoch: 011, best_acc1: 41.526%
2022-03-10 23:41:27 - epoch 012 lr: 0.1
2022-03-10 23:42:04 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.8906
2022-03-10 23:42:37 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.7950
2022-03-10 23:43:10 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.7923
2022-03-10 23:43:42 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 3.0795
2022-03-10 23:44:15 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 3.0923
2022-03-10 23:44:47 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.7250
2022-03-10 23:45:20 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.8191
2022-03-10 23:45:52 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.8856
2022-03-10 23:46:25 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.9422
2022-03-10 23:46:58 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.7632
2022-03-10 23:47:30 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 3.1030
2022-03-10 23:48:03 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.9639
2022-03-10 23:48:36 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.6832
2022-03-10 23:49:09 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.9337
2022-03-10 23:49:41 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.5618
2022-03-10 23:50:14 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.9983
2022-03-10 23:50:46 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.9206
2022-03-10 23:51:19 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.8590
2022-03-10 23:51:52 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.9902
2022-03-10 23:52:25 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 3.1703
2022-03-10 23:52:57 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.8534
2022-03-10 23:53:30 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.8998
2022-03-10 23:54:03 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.8412
2022-03-10 23:54:36 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.8117
2022-03-10 23:55:08 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.7451
2022-03-10 23:55:40 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.7136
2022-03-10 23:56:13 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.8858
2022-03-10 23:56:46 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.8821
2022-03-10 23:57:18 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.8131
2022-03-10 23:57:51 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.3382
2022-03-10 23:58:23 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.9489
2022-03-10 23:58:56 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.7761
2022-03-10 23:59:29 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.9349
2022-03-11 00:00:02 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.9296
2022-03-11 00:00:34 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.7760
2022-03-11 00:01:08 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.7514
2022-03-11 00:01:40 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.8821
2022-03-11 00:02:13 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.8703
2022-03-11 00:02:45 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.6219
2022-03-11 00:03:18 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.8420
2022-03-11 00:03:52 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.7614
2022-03-11 00:04:25 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.6244
2022-03-11 00:04:58 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.9083
2022-03-11 00:05:29 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.6523
2022-03-11 00:06:02 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.9157
2022-03-11 00:06:37 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 3.1432
2022-03-11 00:07:11 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.9990
2022-03-11 00:07:43 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 3.0164
2022-03-11 00:08:17 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.6871
2022-03-11 00:08:48 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.6168
2022-03-11 00:08:49 - train: epoch 012, train_loss: 2.8874
2022-03-11 00:10:03 - eval: epoch: 012, acc1: 41.246%, acc5: 68.258%, test_loss: 2.5995, per_image_load_time: 2.671ms, per_image_inference_time: 0.149ms
2022-03-11 00:10:03 - until epoch: 012, best_acc1: 41.526%
2022-03-11 00:10:03 - epoch 013 lr: 0.1
2022-03-11 00:10:41 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.9688
2022-03-11 00:11:13 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.7432
2022-03-11 00:11:47 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.7839
2022-03-11 00:12:20 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.7699
2022-03-11 00:12:53 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.8231
2022-03-11 00:13:24 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.7840
2022-03-11 00:13:57 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.6272
2022-03-11 00:14:30 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.8921
2022-03-11 00:15:03 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 3.0183
2022-03-11 00:15:36 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.8833
2022-03-11 00:16:09 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.7664
2022-03-11 00:16:41 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.9903
2022-03-11 00:17:14 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.8332
2022-03-11 00:17:47 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.7812
2022-03-11 00:18:21 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 3.1511
2022-03-11 00:18:53 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.5930
2022-03-11 00:19:26 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.7727
2022-03-11 00:20:00 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.6209
2022-03-11 00:20:33 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.8955
2022-03-11 00:21:05 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 3.0849
2022-03-11 00:21:38 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 3.1412
2022-03-11 00:22:11 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.8053
2022-03-11 00:22:43 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.8820
2022-03-11 00:23:17 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 3.0727
2022-03-11 00:23:49 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.8089
2022-03-11 00:24:22 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.7583
2022-03-11 00:24:56 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.7164
2022-03-11 00:25:28 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.9291
2022-03-11 00:26:00 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.7022
2022-03-11 00:26:33 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.8608
2022-03-11 00:27:07 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.9810
2022-03-11 00:27:39 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 3.0104
2022-03-11 00:28:12 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.8681
2022-03-11 00:28:44 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.8565
2022-03-11 00:29:18 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.7214
2022-03-11 00:29:50 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 3.0520
2022-03-11 00:30:24 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.4910
2022-03-11 00:30:56 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.9012
2022-03-11 00:31:30 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.9478
2022-03-11 00:32:03 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.9034
2022-03-11 00:32:35 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.6935
2022-03-11 00:33:08 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.8643
2022-03-11 00:33:40 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 3.0154
2022-03-11 00:34:13 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.7807
2022-03-11 00:34:46 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.7306
2022-03-11 00:35:20 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.7936
2022-03-11 00:35:52 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.8476
2022-03-11 00:36:26 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 3.0137
2022-03-11 00:36:58 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 3.1239
2022-03-11 00:37:30 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.9353
2022-03-11 00:37:31 - train: epoch 013, train_loss: 2.8669
2022-03-11 00:38:43 - eval: epoch: 013, acc1: 41.918%, acc5: 69.060%, test_loss: 2.5626, per_image_load_time: 2.286ms, per_image_inference_time: 0.148ms
2022-03-11 00:38:43 - until epoch: 013, best_acc1: 41.918%
2022-03-11 00:38:43 - epoch 014 lr: 0.1
2022-03-11 00:39:21 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.7729
2022-03-11 00:39:54 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 3.0745
2022-03-11 00:40:26 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.7278
2022-03-11 00:40:59 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.8079
2022-03-11 00:41:31 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.7158
2022-03-11 00:42:04 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.8317
2022-03-11 00:42:37 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.6039
2022-03-11 00:43:10 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.8871
2022-03-11 00:43:42 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.8370
2022-03-11 00:44:15 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 3.1840
2022-03-11 00:44:48 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.7588
2022-03-11 00:45:21 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.8472
2022-03-11 00:45:54 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.8142
2022-03-11 00:46:26 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.7823
2022-03-11 00:46:59 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.9915
2022-03-11 00:47:32 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.8463
2022-03-11 00:48:04 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.8395
2022-03-11 00:48:38 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 3.0628
2022-03-11 00:49:11 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.5701
2022-03-11 00:49:43 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.8247
2022-03-11 00:50:16 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 3.1334
2022-03-11 00:50:49 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.9670
2022-03-11 00:51:22 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.7502
2022-03-11 00:51:54 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.8484
2022-03-11 00:52:28 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.8102
2022-03-11 00:53:01 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.6508
2022-03-11 00:53:33 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.7850
2022-03-11 00:54:06 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.9719
2022-03-11 00:54:38 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.8822
2022-03-11 00:55:12 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.8560
2022-03-11 00:55:44 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.8244
2022-03-11 00:56:17 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.8147
2022-03-11 00:56:50 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.7363
2022-03-11 00:57:24 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.6999
2022-03-11 00:57:56 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.6773
2022-03-11 00:58:29 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.6751
2022-03-11 00:59:01 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.8141
2022-03-11 00:59:34 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 3.0298
2022-03-11 01:00:07 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.9370
2022-03-11 01:00:40 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.8085
2022-03-11 01:01:12 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 3.0358
2022-03-11 01:01:46 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.8449
2022-03-11 01:02:19 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.8546
2022-03-11 01:02:52 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.7839
2022-03-11 01:03:25 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.7416
2022-03-11 01:03:58 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.6771
2022-03-11 01:04:30 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.8558
2022-03-11 01:05:03 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.8482
2022-03-11 01:05:36 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.7461
2022-03-11 01:06:08 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.8176
2022-03-11 01:06:09 - train: epoch 014, train_loss: 2.8423
2022-03-11 01:07:23 - eval: epoch: 014, acc1: 41.786%, acc5: 69.154%, test_loss: 2.5617, per_image_load_time: 1.379ms, per_image_inference_time: 0.132ms
2022-03-11 01:07:23 - until epoch: 014, best_acc1: 41.918%
2022-03-11 01:07:23 - epoch 015 lr: 0.1
2022-03-11 01:08:01 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.6098
2022-03-11 01:08:34 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.8538
2022-03-11 01:09:06 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.9351
2022-03-11 01:09:39 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.9051
2022-03-11 01:10:11 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.9633
2022-03-11 01:10:45 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 3.0059
2022-03-11 01:11:16 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.8626
2022-03-11 01:11:50 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.7974
2022-03-11 01:12:22 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.7314
2022-03-11 01:12:55 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.7203
2022-03-11 01:13:28 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.9698
2022-03-11 01:14:00 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.8153
2022-03-11 01:14:34 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.9818
2022-03-11 01:15:06 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.9927
2022-03-11 01:15:40 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.6903
2022-03-11 01:16:12 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.7759
2022-03-11 01:16:45 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.7884
2022-03-11 01:17:18 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.7623
2022-03-11 01:17:51 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.8556
2022-03-11 01:18:24 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.5709
2022-03-11 01:18:57 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.6190
2022-03-11 01:19:30 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.7798
2022-03-11 01:20:03 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.6974
2022-03-11 01:20:35 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.7338
2022-03-11 01:21:10 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 3.0146
2022-03-11 01:21:42 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.6698
2022-03-11 01:22:15 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 3.0886
2022-03-11 01:22:48 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.8359
2022-03-11 01:23:21 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.8592
2022-03-11 01:23:54 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.6215
2022-03-11 01:24:27 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.6368
2022-03-11 01:24:59 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.8698
2022-03-11 01:25:33 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.8051
2022-03-11 01:26:05 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.7403
2022-03-11 01:26:38 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.9713
2022-03-11 01:27:11 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.9730
2022-03-11 01:27:44 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.6074
2022-03-11 01:28:18 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.8604
2022-03-11 01:28:50 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.8301
2022-03-11 01:29:23 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.8167
2022-03-11 01:29:56 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.7193
2022-03-11 01:30:29 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.6383
2022-03-11 01:31:01 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.8669
2022-03-11 01:31:35 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.7573
2022-03-11 01:32:08 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.8837
2022-03-11 01:32:42 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.8748
2022-03-11 01:33:14 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.9077
2022-03-11 01:33:48 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.7268
2022-03-11 01:34:20 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.8341
2022-03-11 01:34:52 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.9164
2022-03-11 01:34:53 - train: epoch 015, train_loss: 2.8240
2022-03-11 01:36:11 - eval: epoch: 015, acc1: 41.932%, acc5: 68.580%, test_loss: 2.5699, per_image_load_time: 2.551ms, per_image_inference_time: 0.131ms
2022-03-11 01:36:11 - until epoch: 015, best_acc1: 41.932%
2022-03-11 01:36:11 - epoch 016 lr: 0.1
2022-03-11 01:36:49 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.7102
2022-03-11 01:37:21 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.7133
2022-03-11 01:37:54 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.7521
2022-03-11 01:38:27 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.9447
2022-03-11 01:39:00 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.6798
2022-03-11 01:39:33 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.7211
2022-03-11 01:40:05 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.5960
2022-03-11 01:40:37 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.9491
2022-03-11 01:41:09 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 3.0051
2022-03-11 01:41:42 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.7972
2022-03-11 01:42:14 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.8579
2022-03-11 01:42:48 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.6686
2022-03-11 01:43:21 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.8979
2022-03-11 01:43:54 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.6858
2022-03-11 01:44:26 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 3.0040
2022-03-11 01:44:59 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.6855
2022-03-11 01:45:32 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.8469
2022-03-11 01:46:05 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 3.0532
2022-03-11 01:46:37 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.7741
2022-03-11 01:47:10 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.4522
2022-03-11 01:47:43 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.9957
2022-03-11 01:48:15 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.9432
2022-03-11 01:48:48 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.9983
2022-03-11 01:49:21 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.8932
2022-03-11 01:49:54 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.8060
2022-03-11 01:50:26 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.8241
2022-03-11 01:51:00 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.6911
2022-03-11 01:51:32 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.8741
2022-03-11 01:52:05 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.7971
2022-03-11 01:52:38 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.9505
2022-03-11 01:53:11 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.8368
2022-03-11 01:53:44 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 3.0019
2022-03-11 01:54:19 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.8411
2022-03-11 01:54:52 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.6075
2022-03-11 01:55:26 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.9083
2022-03-11 01:55:59 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.7129
2022-03-11 01:56:34 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 3.0353
2022-03-11 01:57:06 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 3.0152
2022-03-11 01:57:41 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.8720
2022-03-11 01:58:15 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.9109
2022-03-11 01:58:47 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.9223
2022-03-11 01:59:21 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.5915
2022-03-11 01:59:54 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.8334
2022-03-11 02:00:28 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.7607
2022-03-11 02:01:02 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.8019
2022-03-11 02:01:35 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.7021
2022-03-11 02:02:09 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 3.1078
2022-03-11 02:02:42 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.7041
2022-03-11 02:03:16 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.8168
2022-03-11 02:03:48 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.8773
2022-03-11 02:03:49 - train: epoch 016, train_loss: 2.8097
2022-03-11 02:05:03 - eval: epoch: 016, acc1: 40.810%, acc5: 67.314%, test_loss: 2.6556, per_image_load_time: 2.125ms, per_image_inference_time: 0.130ms
2022-03-11 02:05:04 - until epoch: 016, best_acc1: 41.932%
2022-03-11 02:05:04 - epoch 017 lr: 0.1
2022-03-11 02:05:41 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.7924
2022-03-11 02:06:14 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.9778
2022-03-11 02:06:48 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 3.1019
2022-03-11 02:07:21 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.3558
2022-03-11 02:07:55 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.7930
2022-03-11 02:08:28 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 3.1860
2022-03-11 02:09:02 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.8519
2022-03-11 02:09:35 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.8375
2022-03-11 02:10:09 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.5873
2022-03-11 02:10:42 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.8015
2022-03-11 02:11:16 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.9310
2022-03-11 02:11:48 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.8758
2022-03-11 02:12:22 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.9362
2022-03-11 02:12:55 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.9093
2022-03-11 02:13:28 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.6776
2022-03-11 02:14:01 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.6891
2022-03-11 02:14:35 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.7565
2022-03-11 02:15:07 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.8253
2022-03-11 02:15:40 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.6831
2022-03-11 02:16:14 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.7204
2022-03-11 02:16:46 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.7094
2022-03-11 02:17:20 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.4980
2022-03-11 02:17:53 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.9032
2022-03-11 02:18:26 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.8286
2022-03-11 02:18:59 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.7767
2022-03-11 02:19:32 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.5810
2022-03-11 02:20:05 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.9859
2022-03-11 02:20:39 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.6935
2022-03-11 02:21:12 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 3.1586
2022-03-11 02:21:45 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.8051
2022-03-11 02:22:18 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.7884
2022-03-11 02:22:51 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.5721
2022-03-11 02:23:25 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.9333
2022-03-11 02:23:58 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.5902
2022-03-11 02:24:31 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.6575
2022-03-11 02:25:04 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.9210
2022-03-11 02:25:38 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.8003
2022-03-11 02:26:11 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.8990
2022-03-11 02:26:45 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.6704
2022-03-11 02:27:18 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.9516
2022-03-11 02:27:51 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.8643
2022-03-11 02:28:24 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.8097
2022-03-11 02:28:58 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.7962
2022-03-11 02:29:31 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.8800
2022-03-11 02:30:04 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.8205
2022-03-11 02:30:37 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.7068
2022-03-11 02:31:10 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.8105
2022-03-11 02:31:43 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.8633
2022-03-11 02:32:16 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.7813
2022-03-11 02:32:48 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.5430
2022-03-11 02:32:49 - train: epoch 017, train_loss: 2.7938
2022-03-11 02:34:05 - eval: epoch: 017, acc1: 40.794%, acc5: 68.354%, test_loss: 2.6005, per_image_load_time: 2.733ms, per_image_inference_time: 0.148ms
2022-03-11 02:34:06 - until epoch: 017, best_acc1: 41.932%
2022-03-11 02:34:06 - epoch 018 lr: 0.1
2022-03-11 02:34:43 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.7481
2022-03-11 02:35:16 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 3.0328
2022-03-11 02:35:48 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.9603
2022-03-11 02:36:21 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.9241
2022-03-11 02:36:53 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.7623
2022-03-11 02:37:25 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.8476
2022-03-11 02:37:58 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.7055
2022-03-11 02:38:30 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.8774
2022-03-11 02:39:03 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.8700
2022-03-11 02:39:35 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.8137
2022-03-11 02:40:08 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 3.1765
2022-03-11 02:40:40 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.6127
2022-03-11 02:41:14 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.9790
2022-03-11 02:41:46 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.8697
2022-03-11 02:42:18 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.8757
2022-03-11 02:42:51 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.7212
2022-03-11 02:43:24 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.7893
2022-03-11 02:43:56 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.7415
2022-03-11 02:44:29 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.6612
2022-03-11 02:45:01 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 3.1669
2022-03-11 02:45:34 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 3.0347
2022-03-11 02:46:07 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.6972
2022-03-11 02:46:41 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.9078
2022-03-11 02:47:13 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.6292
2022-03-11 02:47:46 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.4993
2022-03-11 02:48:18 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.7892
2022-03-11 02:48:50 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.9240
2022-03-11 02:49:24 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.7296
2022-03-11 02:49:56 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.6348
2022-03-11 02:50:29 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.6814
2022-03-11 02:51:01 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 3.1668
2022-03-11 02:51:34 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.7328
2022-03-11 02:52:07 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.6334
2022-03-11 02:52:39 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.7940
2022-03-11 02:53:12 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.7772
2022-03-11 02:53:45 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.7326
2022-03-11 02:54:18 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 3.0977
2022-03-11 02:54:50 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.7678
2022-03-11 02:55:23 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.8789
2022-03-11 02:55:55 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.8680
2022-03-11 02:56:28 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.8380
2022-03-11 02:57:00 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.7048
2022-03-11 02:57:33 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.6855
2022-03-11 02:58:06 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 3.0254
2022-03-11 02:58:40 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.7187
2022-03-11 02:59:12 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.9152
2022-03-11 02:59:46 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.8351
2022-03-11 03:00:19 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.7736
2022-03-11 03:00:52 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.8314
2022-03-11 03:01:24 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.7847
2022-03-11 03:01:25 - train: epoch 018, train_loss: 2.7808
2022-03-11 03:02:38 - eval: epoch: 018, acc1: 43.480%, acc5: 70.552%, test_loss: 2.4693, per_image_load_time: 1.922ms, per_image_inference_time: 0.136ms
2022-03-11 03:02:38 - until epoch: 018, best_acc1: 43.480%
2022-03-11 03:02:38 - epoch 019 lr: 0.1
2022-03-11 03:03:17 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.7138
2022-03-11 03:03:49 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.8553
2022-03-11 03:04:21 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.6604
2022-03-11 03:04:54 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.6710
2022-03-11 03:05:27 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.7172
2022-03-11 03:06:00 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.7909
2022-03-11 03:06:32 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.5348
2022-03-11 03:07:05 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.7868
2022-03-11 03:07:38 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.9025
2022-03-11 03:08:10 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.8736
2022-03-11 03:08:44 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.7845
2022-03-11 03:09:15 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.6768
2022-03-11 03:09:49 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.7286
2022-03-11 03:10:22 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.5269
2022-03-11 03:10:55 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.9782
2022-03-11 03:11:27 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.4095
2022-03-11 03:12:00 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 3.0065
2022-03-11 03:12:33 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.7798
2022-03-11 03:13:06 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 3.1684
2022-03-11 03:13:38 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.6455
2022-03-11 03:14:11 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.6302
2022-03-11 03:14:44 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.6908
2022-03-11 03:15:17 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.9000
2022-03-11 03:15:49 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.9965
2022-03-11 03:16:22 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.7537
2022-03-11 03:16:55 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.8900
2022-03-11 03:17:28 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.7551
2022-03-11 03:18:01 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.7526
2022-03-11 03:18:34 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.7249
2022-03-11 03:19:06 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.9310
2022-03-11 03:19:39 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.7051
2022-03-11 03:20:11 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.5044
2022-03-11 03:20:44 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.8914
2022-03-11 03:21:17 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.8575
2022-03-11 03:21:50 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.8137
2022-03-11 03:22:22 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.4970
2022-03-11 03:22:55 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.7639
2022-03-11 03:23:28 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 3.0513
2022-03-11 03:24:01 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.6876
2022-03-11 03:24:33 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.6577
2022-03-11 03:25:07 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.8416
2022-03-11 03:25:39 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.6642
2022-03-11 03:26:12 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.7513
2022-03-11 03:26:45 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.8931
2022-03-11 03:27:18 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.9649
2022-03-11 03:27:51 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.4961
2022-03-11 03:28:23 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.7355
2022-03-11 03:28:56 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.5850
2022-03-11 03:29:29 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.6852
2022-03-11 03:30:01 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.8609
2022-03-11 03:30:02 - train: epoch 019, train_loss: 2.7668
2022-03-11 03:31:16 - eval: epoch: 019, acc1: 43.180%, acc5: 70.300%, test_loss: 2.4949, per_image_load_time: 2.744ms, per_image_inference_time: 0.146ms
2022-03-11 03:31:16 - until epoch: 019, best_acc1: 43.480%
2022-03-11 03:31:16 - epoch 020 lr: 0.1
2022-03-11 03:31:53 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.8386
2022-03-11 03:32:27 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.3858
2022-03-11 03:32:58 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.6265
2022-03-11 03:33:31 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.5129
2022-03-11 03:34:03 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.5347
2022-03-11 03:34:37 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.8638
2022-03-11 03:35:09 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.5353
2022-03-11 03:35:41 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.7280
2022-03-11 03:36:14 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 3.0015
2022-03-11 03:36:48 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.6787
2022-03-11 03:37:20 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.6807
2022-03-11 03:37:52 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.8376
2022-03-11 03:38:25 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.7109
2022-03-11 03:38:59 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.8459
2022-03-11 03:39:30 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.6118
2022-03-11 03:40:04 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.8779
2022-03-11 03:40:36 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.6815
2022-03-11 03:41:09 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.7767
2022-03-11 03:41:41 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.4189
2022-03-11 03:42:14 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.6554
2022-03-11 03:42:47 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.7347
2022-03-11 03:43:19 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.6652
2022-03-11 03:43:52 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.6640
2022-03-11 03:44:25 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.9385
2022-03-11 03:44:57 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.6279
2022-03-11 03:45:30 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.5932
2022-03-11 03:46:02 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.6569
2022-03-11 03:46:35 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 3.0588
2022-03-11 03:47:07 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.8150
2022-03-11 03:47:40 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.7897
2022-03-11 03:48:13 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 3.0079
2022-03-11 03:48:46 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.9222
2022-03-11 03:49:18 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.6004
2022-03-11 03:49:51 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.6409
2022-03-11 03:50:23 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.5583
2022-03-11 03:50:57 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.8823
2022-03-11 03:51:29 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.5642
2022-03-11 03:52:02 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.8118
2022-03-11 03:52:35 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.9637
2022-03-11 03:53:08 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.6566
2022-03-11 03:53:40 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.8030
2022-03-11 03:54:14 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.7962
2022-03-11 03:54:45 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.8449
2022-03-11 03:55:19 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.7055
2022-03-11 03:55:50 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.6561
2022-03-11 03:56:23 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.5393
2022-03-11 03:56:56 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.6319
2022-03-11 03:57:29 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.7755
2022-03-11 03:58:02 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.7943
2022-03-11 03:58:34 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.5240
2022-03-11 03:58:36 - train: epoch 020, train_loss: 2.7566
2022-03-11 03:59:49 - eval: epoch: 020, acc1: 43.132%, acc5: 70.002%, test_loss: 2.5032, per_image_load_time: 2.711ms, per_image_inference_time: 0.128ms
2022-03-11 03:59:49 - until epoch: 020, best_acc1: 43.480%
2022-03-11 03:59:49 - epoch 021 lr: 0.1
2022-03-11 04:00:27 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.7394
2022-03-11 04:00:59 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.8438
2022-03-11 04:01:32 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.2325
2022-03-11 04:02:04 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.8389
2022-03-11 04:02:36 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.5365
2022-03-11 04:03:08 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.4553
2022-03-11 04:03:41 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.5906
2022-03-11 04:04:14 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.8035
2022-03-11 04:04:46 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.5959
2022-03-11 04:05:18 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.6674
2022-03-11 04:05:52 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.5393
2022-03-11 04:06:23 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.6608
2022-03-11 04:06:57 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.6871
2022-03-11 04:07:29 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.7526
2022-03-11 04:08:02 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.7368
2022-03-11 04:08:35 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.5395
2022-03-11 04:09:08 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.9157
2022-03-11 04:09:40 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.8018
2022-03-11 04:10:12 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.8159
2022-03-11 04:10:45 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.7305
2022-03-11 04:11:18 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.5127
2022-03-11 04:11:50 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 3.0555
2022-03-11 04:12:22 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.8981
2022-03-11 04:12:55 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.7428
2022-03-11 04:13:28 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.6867
2022-03-11 04:14:01 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.6997
2022-03-11 04:14:33 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.6752
2022-03-11 04:15:05 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.7309
2022-03-11 04:15:38 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.8932
2022-03-11 04:16:10 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.8624
2022-03-11 04:16:43 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.8386
2022-03-11 04:17:16 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.6605
2022-03-11 04:17:47 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 3.0435
2022-03-11 04:18:20 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.9234
2022-03-11 04:18:53 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.9326
2022-03-11 04:19:26 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.9978
2022-03-11 04:19:58 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.7426
2022-03-11 04:20:31 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.8515
2022-03-11 04:21:04 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.9075
2022-03-11 04:21:35 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.9100
2022-03-11 04:22:09 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.7214
2022-03-11 04:22:40 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.9569
2022-03-11 04:23:14 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.6091
2022-03-11 04:23:46 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.7498
2022-03-11 04:24:19 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.7859
2022-03-11 04:24:51 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.6651
2022-03-11 04:25:24 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.8729
2022-03-11 04:25:56 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.9320
2022-03-11 04:26:30 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.5956
2022-03-11 04:27:01 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.5694
2022-03-11 04:27:03 - train: epoch 021, train_loss: 2.7480
2022-03-11 04:28:16 - eval: epoch: 021, acc1: 41.658%, acc5: 69.218%, test_loss: 2.5641, per_image_load_time: 1.608ms, per_image_inference_time: 0.147ms
2022-03-11 04:28:16 - until epoch: 021, best_acc1: 43.480%
2022-03-11 04:28:16 - epoch 022 lr: 0.1
2022-03-11 04:28:52 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.4735
2022-03-11 04:29:25 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.6416
2022-03-11 04:29:58 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.4932
2022-03-11 04:30:30 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.5987
2022-03-11 04:31:03 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.7379
2022-03-11 04:31:36 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.6738
2022-03-11 04:32:08 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.8899
2022-03-11 04:32:42 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.9106
2022-03-11 04:33:14 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.6156
2022-03-11 04:33:46 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.6970
2022-03-11 04:34:19 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.8194
2022-03-11 04:34:52 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.4718
2022-03-11 04:35:25 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 3.0333
2022-03-11 04:35:58 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.8758
2022-03-11 04:36:31 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.8731
2022-03-11 04:37:04 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.5194
2022-03-11 04:37:37 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.7890
2022-03-11 04:38:10 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.9942
2022-03-11 04:38:42 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.7101
2022-03-11 04:39:14 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.6494
2022-03-11 04:39:47 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.7264
2022-03-11 04:40:20 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.5047
2022-03-11 04:40:53 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 3.0238
2022-03-11 04:41:26 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.6775
2022-03-11 04:41:59 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.6129
2022-03-11 04:42:31 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.7185
2022-03-11 04:43:04 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.5723
2022-03-11 04:43:36 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 3.0317
2022-03-11 04:44:10 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.7635
2022-03-11 04:44:42 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 3.0555
2022-03-11 04:45:15 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 3.0958
2022-03-11 04:45:47 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.6103
2022-03-11 04:46:20 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.6655
2022-03-11 04:46:53 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.7360
2022-03-11 04:47:26 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 3.0482
2022-03-11 04:47:58 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.6868
2022-03-11 04:48:31 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.9955
2022-03-11 04:49:04 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.8944
2022-03-11 04:49:37 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.6300
2022-03-11 04:50:10 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.7614
2022-03-11 04:50:42 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.7062
2022-03-11 04:51:16 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.7651
2022-03-11 04:51:48 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.8162
2022-03-11 04:52:21 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.5910
2022-03-11 04:52:54 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.7105
2022-03-11 04:53:27 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.8761
2022-03-11 04:53:59 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.7926
2022-03-11 04:54:34 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.7966
2022-03-11 04:55:06 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.8154
2022-03-11 04:55:38 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.6391
2022-03-11 04:55:39 - train: epoch 022, train_loss: 2.7384
2022-03-11 04:56:54 - eval: epoch: 022, acc1: 43.374%, acc5: 70.332%, test_loss: 2.4878, per_image_load_time: 2.768ms, per_image_inference_time: 0.142ms
2022-03-11 04:56:54 - until epoch: 022, best_acc1: 43.480%
2022-03-11 04:56:54 - epoch 023 lr: 0.1
2022-03-11 04:57:32 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.6711
2022-03-11 04:58:04 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.6109
2022-03-11 04:58:36 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.6165
2022-03-11 04:59:08 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.7535
2022-03-11 04:59:40 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.8531
2022-03-11 05:00:12 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.7799
2022-03-11 05:00:44 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.7149
2022-03-11 05:01:17 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.7435
2022-03-11 05:01:49 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.8474
2022-03-11 05:02:22 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.7749
2022-03-11 05:02:54 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.7516
2022-03-11 05:03:27 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.4555
2022-03-11 05:03:58 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.7204
2022-03-11 05:04:32 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.5958
2022-03-11 05:05:03 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.5444
2022-03-11 05:05:36 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.4487
2022-03-11 05:06:08 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.8399
2022-03-11 05:06:41 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.5256
2022-03-11 05:07:12 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.7934
2022-03-11 05:07:45 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.5783
2022-03-11 05:08:18 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.7333
2022-03-11 05:08:50 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.3917
2022-03-11 05:09:22 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.7278
2022-03-11 05:09:54 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.8651
2022-03-11 05:10:27 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.9326
2022-03-11 05:10:59 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.7219
2022-03-11 05:11:33 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.8430
2022-03-11 05:12:04 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.7954
2022-03-11 05:12:38 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.7585
2022-03-11 05:13:09 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.6333
2022-03-11 05:13:42 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.8554
2022-03-11 05:14:14 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.7227
2022-03-11 05:14:47 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.9507
2022-03-11 05:15:19 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.8465
2022-03-11 05:15:52 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.6475
2022-03-11 05:16:24 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.6444
2022-03-11 05:16:56 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.6768
2022-03-11 05:17:29 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.6695
2022-03-11 05:18:01 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.6451
2022-03-11 05:18:34 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.7722
2022-03-11 05:19:06 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.6599
2022-03-11 05:19:38 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.5443
2022-03-11 05:20:10 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.6776
2022-03-11 05:20:43 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.6730
2022-03-11 05:21:16 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.6806
2022-03-11 05:21:48 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.6491
2022-03-11 05:22:20 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.6302
2022-03-11 05:22:53 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.6795
2022-03-11 05:23:26 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.5809
2022-03-11 05:23:58 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.7609
2022-03-11 05:23:59 - train: epoch 023, train_loss: 2.7289
2022-03-11 05:25:12 - eval: epoch: 023, acc1: 43.496%, acc5: 70.326%, test_loss: 2.4724, per_image_load_time: 2.118ms, per_image_inference_time: 0.150ms
2022-03-11 05:25:12 - until epoch: 023, best_acc1: 43.496%
2022-03-11 05:25:12 - epoch 024 lr: 0.1
2022-03-11 05:25:50 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.8637
2022-03-11 05:26:22 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.5939
2022-03-11 05:26:54 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.7102
2022-03-11 05:27:27 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.7051
2022-03-11 05:27:59 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.7554
2022-03-11 05:28:31 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.6146
2022-03-11 05:29:04 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.8759
2022-03-11 05:29:36 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.7164
2022-03-11 05:30:08 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.7554
2022-03-11 05:30:40 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.5722
2022-03-11 05:31:12 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.5842
2022-03-11 05:31:45 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.4790
2022-03-11 05:32:17 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.8309
2022-03-11 05:32:50 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.4619
2022-03-11 05:33:22 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.9204
2022-03-11 05:33:54 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.6492
2022-03-11 05:34:26 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.7289
2022-03-11 05:34:59 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.8218
2022-03-11 05:35:31 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.3919
2022-03-11 05:36:04 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.8774
2022-03-11 05:36:35 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.7426
2022-03-11 05:37:09 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.5748
2022-03-11 05:37:40 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.7896
2022-03-11 05:38:13 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.6808
2022-03-11 05:38:44 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.6717
2022-03-11 05:39:17 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.8438
2022-03-11 05:39:49 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.8025
2022-03-11 05:40:22 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.7028
2022-03-11 05:40:54 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.7324
2022-03-11 05:41:26 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.6858
2022-03-11 05:41:58 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.4352
2022-03-11 05:42:31 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.8233
2022-03-11 05:43:03 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.5197
2022-03-11 05:43:36 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.6864
2022-03-11 05:44:08 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.5416
2022-03-11 05:44:41 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.6256
2022-03-11 05:45:13 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.4937
2022-03-11 05:45:46 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.7260
2022-03-11 05:46:18 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.6006
2022-03-11 05:46:51 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.5986
2022-03-11 05:47:22 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.6126
2022-03-11 05:47:56 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.6641
2022-03-11 05:48:27 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.5500
2022-03-11 05:49:01 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.6150
2022-03-11 05:49:32 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.6084
2022-03-11 05:50:05 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.8360
2022-03-11 05:50:37 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.7674
2022-03-11 05:51:11 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.6172
2022-03-11 05:51:43 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.6341
2022-03-11 05:52:14 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.6872
2022-03-11 05:52:15 - train: epoch 024, train_loss: 2.7233
2022-03-11 05:53:28 - eval: epoch: 024, acc1: 43.350%, acc5: 70.236%, test_loss: 2.4935, per_image_load_time: 2.636ms, per_image_inference_time: 0.150ms
2022-03-11 05:53:28 - until epoch: 024, best_acc1: 43.496%
2022-03-11 05:53:28 - epoch 025 lr: 0.1
2022-03-11 05:54:06 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.6349
2022-03-11 05:54:39 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.4590
2022-03-11 05:55:11 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.6536
2022-03-11 05:55:43 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.9683
2022-03-11 05:56:16 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.5949
2022-03-11 05:56:48 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.9308
2022-03-11 05:57:20 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.5879
2022-03-11 05:57:53 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.7362
2022-03-11 05:58:25 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.4872
2022-03-11 05:58:58 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.5872
2022-03-11 05:59:30 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.6346
2022-03-11 06:00:04 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.7207
2022-03-11 06:00:35 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.4570
2022-03-11 06:01:08 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.7672
2022-03-11 06:01:41 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.6965
2022-03-11 06:02:13 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.7221
2022-03-11 06:02:46 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.5464
2022-03-11 06:03:19 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.6482
2022-03-11 06:03:51 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.5977
2022-03-11 06:04:25 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.8788
2022-03-11 06:04:57 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.6496
2022-03-11 06:05:30 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.6151
2022-03-11 06:06:03 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.6270
2022-03-11 06:06:35 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.6509
2022-03-11 06:07:08 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.7212
2022-03-11 06:07:41 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.6438
2022-03-11 06:08:14 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.8469
2022-03-11 06:08:46 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.6264
2022-03-11 06:09:19 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.9097
2022-03-11 06:09:51 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.7129
2022-03-11 06:10:24 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.8196
2022-03-11 06:10:56 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.6208
2022-03-11 06:11:29 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.6310
2022-03-11 06:12:01 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.7712
2022-03-11 06:12:35 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.5098
2022-03-11 06:13:07 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.6730
2022-03-11 06:13:39 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.7931
2022-03-11 06:14:11 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.7333
2022-03-11 06:14:44 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.7422
2022-03-11 06:15:17 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.8067
2022-03-11 06:15:50 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.8392
2022-03-11 06:16:22 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.7307
2022-03-11 06:16:55 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.5978
2022-03-11 06:17:28 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.5762
2022-03-11 06:18:00 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.8021
2022-03-11 06:18:33 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.3853
2022-03-11 06:19:06 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.7952
2022-03-11 06:19:39 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.4922
2022-03-11 06:20:12 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.7844
2022-03-11 06:20:43 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.7280
2022-03-11 06:20:44 - train: epoch 025, train_loss: 2.7164
2022-03-11 06:21:58 - eval: epoch: 025, acc1: 45.040%, acc5: 72.104%, test_loss: 2.3884, per_image_load_time: 2.699ms, per_image_inference_time: 0.144ms
2022-03-11 06:21:58 - until epoch: 025, best_acc1: 45.040%
2022-03-11 06:21:58 - epoch 026 lr: 0.1
2022-03-11 06:22:36 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.7985
2022-03-11 06:23:08 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.6128
2022-03-11 06:23:41 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.8506
2022-03-11 06:24:13 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.5179
2022-03-11 06:24:45 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.7478
2022-03-11 06:25:18 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.8026
2022-03-11 06:25:51 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.8928
2022-03-11 06:26:23 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.6250
2022-03-11 06:26:55 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.7489
2022-03-11 06:27:27 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.6627
2022-03-11 06:28:00 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.7171
2022-03-11 06:28:33 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.6376
2022-03-11 06:29:05 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.4408
2022-03-11 06:29:37 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.8481
2022-03-11 06:30:10 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.6455
2022-03-11 06:30:42 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.8377
2022-03-11 06:31:15 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.7974
2022-03-11 06:31:48 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.9166
2022-03-11 06:32:21 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.9152
2022-03-11 06:32:53 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.6942
2022-03-11 06:33:25 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.8154
2022-03-11 06:33:58 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.7722
2022-03-11 06:34:31 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.7677
2022-03-11 06:35:03 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.8133
2022-03-11 06:35:36 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.7955
2022-03-11 06:36:09 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.6735
2022-03-11 06:36:41 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.9371
2022-03-11 06:37:14 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.7435
2022-03-11 06:37:47 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.8871
2022-03-11 06:38:19 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.7238
2022-03-11 06:38:52 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.7025
2022-03-11 06:39:24 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.7232
2022-03-11 06:39:56 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.7990
2022-03-11 06:40:29 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.7054
2022-03-11 06:41:01 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.7135
2022-03-11 06:41:33 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.8557
2022-03-11 06:42:06 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.6519
2022-03-11 06:42:38 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.8850
2022-03-11 06:43:11 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.9372
2022-03-11 06:43:43 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.7823
2022-03-11 06:44:16 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.7940
2022-03-11 06:44:49 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.8780
2022-03-11 06:45:22 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.9162
2022-03-11 06:45:54 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.7559
2022-03-11 06:46:27 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 3.0076
2022-03-11 06:46:59 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.8348
2022-03-11 06:47:32 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.7428
2022-03-11 06:48:05 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.5845
2022-03-11 06:48:38 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.8035
2022-03-11 06:49:10 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.7126
2022-03-11 06:49:11 - train: epoch 026, train_loss: 2.7105
2022-03-11 06:50:24 - eval: epoch: 026, acc1: 45.082%, acc5: 71.918%, test_loss: 2.3884, per_image_load_time: 2.732ms, per_image_inference_time: 0.143ms
2022-03-11 06:50:24 - until epoch: 026, best_acc1: 45.082%
2022-03-11 06:50:24 - epoch 027 lr: 0.1
2022-03-11 06:51:02 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.9415
2022-03-11 06:51:34 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.4750
2022-03-11 06:52:07 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.7169
2022-03-11 06:52:39 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.7794
2022-03-11 06:53:12 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.9587
2022-03-11 06:53:44 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.8604
2022-03-11 06:54:16 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.7469
2022-03-11 06:54:49 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.6442
2022-03-11 06:55:22 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.7214
2022-03-11 06:55:54 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.9166
2022-03-11 06:56:27 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.5348
2022-03-11 06:57:00 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 3.0141
2022-03-11 06:57:32 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.7507
2022-03-11 06:58:05 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.8674
2022-03-11 06:58:37 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.5410
2022-03-11 06:59:10 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.6789
2022-03-11 06:59:42 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.8097
2022-03-11 07:00:15 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.7686
2022-03-11 07:00:47 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.7353
2022-03-11 07:01:20 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.6163
2022-03-11 07:01:53 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.8320
2022-03-11 07:02:26 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.9590
2022-03-11 07:02:59 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.6911
2022-03-11 07:03:32 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.7358
2022-03-11 07:04:04 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.7615
2022-03-11 07:04:37 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.7720
2022-03-11 07:05:09 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.6758
2022-03-11 07:05:42 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.8208
2022-03-11 07:06:14 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.7560
2022-03-11 07:06:47 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.5733
2022-03-11 07:07:19 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.4935
2022-03-11 07:07:52 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.6291
2022-03-11 07:08:25 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.7427
2022-03-11 07:08:57 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.5996
2022-03-11 07:09:30 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.8988
2022-03-11 07:10:03 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.6697
2022-03-11 07:10:35 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.5721
2022-03-11 07:11:08 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.4269
2022-03-11 07:11:41 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.5797
2022-03-11 07:12:13 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.8805
2022-03-11 07:12:46 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.5897
2022-03-11 07:13:19 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.7760
2022-03-11 07:13:52 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.7860
2022-03-11 07:14:24 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.8102
2022-03-11 07:14:57 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.6500
2022-03-11 07:15:30 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.5909
2022-03-11 07:16:03 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.7273
2022-03-11 07:16:35 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.8570
2022-03-11 07:17:09 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.6493
2022-03-11 07:17:40 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.5246
2022-03-11 07:17:41 - train: epoch 027, train_loss: 2.7026
2022-03-11 07:18:55 - eval: epoch: 027, acc1: 43.824%, acc5: 70.946%, test_loss: 2.4487, per_image_load_time: 1.757ms, per_image_inference_time: 0.154ms
2022-03-11 07:18:55 - until epoch: 027, best_acc1: 45.082%
2022-03-11 07:18:55 - epoch 028 lr: 0.1
2022-03-11 07:19:33 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.5972
2022-03-11 07:20:05 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.6474
2022-03-11 07:20:37 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.5189
2022-03-11 07:21:10 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.6483
2022-03-11 07:21:43 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.4713
2022-03-11 07:22:16 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.8598
2022-03-11 07:22:48 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.9315
2022-03-11 07:23:21 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.5557
2022-03-11 07:23:52 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.6016
2022-03-11 07:24:25 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.7666
2022-03-11 07:24:58 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.7213
2022-03-11 07:25:31 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.5900
2022-03-11 07:26:02 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.5987
2022-03-11 07:26:35 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.9760
2022-03-11 07:27:07 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.7091
2022-03-11 07:27:40 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.7368
2022-03-11 07:28:13 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.6742
2022-03-11 07:28:45 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.6525
2022-03-11 07:29:17 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.8030
2022-03-11 07:29:50 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.8310
2022-03-11 07:30:22 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.8805
2022-03-11 07:30:55 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.8811
2022-03-11 07:31:27 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.8897
2022-03-11 07:32:00 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.8961
2022-03-11 07:32:32 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.7429
2022-03-11 07:33:05 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.7734
2022-03-11 07:33:38 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.7146
2022-03-11 07:34:10 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.6661
2022-03-11 07:34:43 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.8377
2022-03-11 07:35:15 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.7428
2022-03-11 07:35:48 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.9373
2022-03-11 07:36:21 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.4182
2022-03-11 07:36:53 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.6578
2022-03-11 07:37:26 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.7684
2022-03-11 07:37:58 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.8495
2022-03-11 07:38:31 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.7346
2022-03-11 07:39:03 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.8007
2022-03-11 07:39:36 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.5094
2022-03-11 07:40:08 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.8110
2022-03-11 07:40:42 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.8494
2022-03-11 07:41:13 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.7557
2022-03-11 07:41:46 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.6644
2022-03-11 07:42:19 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.5445
2022-03-11 07:42:52 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.5961
2022-03-11 07:43:24 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.9422
2022-03-11 07:43:58 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.8266
2022-03-11 07:44:29 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.9402
2022-03-11 07:45:03 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.6847
2022-03-11 07:45:36 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.8530
2022-03-11 07:46:07 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.4117
2022-03-11 07:46:08 - train: epoch 028, train_loss: 2.6952
2022-03-11 07:47:22 - eval: epoch: 028, acc1: 44.414%, acc5: 71.544%, test_loss: 2.4257, per_image_load_time: 2.705ms, per_image_inference_time: 0.141ms
2022-03-11 07:47:22 - until epoch: 028, best_acc1: 45.082%
2022-03-11 07:47:22 - epoch 029 lr: 0.1
2022-03-11 07:48:00 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.4664
2022-03-11 07:48:33 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.6574
2022-03-11 07:49:04 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.7612
2022-03-11 07:49:38 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.6774
2022-03-11 07:50:11 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.6808
2022-03-11 07:50:44 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.7400
2022-03-11 07:51:16 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.5319
2022-03-11 07:51:48 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.6911
2022-03-11 07:52:20 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.6151
2022-03-11 07:52:53 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.6524
2022-03-11 07:53:25 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.7271
2022-03-11 07:53:58 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.7231
2022-03-11 07:54:31 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.7404
2022-03-11 07:55:03 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.6401
2022-03-11 07:55:36 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.8560
2022-03-11 07:56:09 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.8786
2022-03-11 07:56:41 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.5549
2022-03-11 07:57:14 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.8068
2022-03-11 07:57:46 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.4714
2022-03-11 07:58:19 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.6725
2022-03-11 07:58:51 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.7368
2022-03-11 07:59:24 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.8841
2022-03-11 07:59:57 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.6363
2022-03-11 08:00:30 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.5359
2022-03-11 08:01:03 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.6409
2022-03-11 08:01:35 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.7391
2022-03-11 08:02:08 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.6680
2022-03-11 08:02:40 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.7151
2022-03-11 08:03:13 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.6063
2022-03-11 08:03:45 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.7232
2022-03-11 08:04:19 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.8142
2022-03-11 08:04:51 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.7550
2022-03-11 08:05:23 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.5202
2022-03-11 08:05:57 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.6555
2022-03-11 08:06:30 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.9609
2022-03-11 08:07:02 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.7371
2022-03-11 08:07:34 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.7026
2022-03-11 08:08:06 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.6482
2022-03-11 08:08:40 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.4662
2022-03-11 08:09:13 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.8685
2022-03-11 08:09:46 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.7253
2022-03-11 08:10:19 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.4609
2022-03-11 08:10:51 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.6428
2022-03-11 08:11:24 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.6677
2022-03-11 08:11:57 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.8199
2022-03-11 08:12:30 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.5970
2022-03-11 08:13:01 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.5706
2022-03-11 08:13:35 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.8198
2022-03-11 08:14:08 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.9022
2022-03-11 08:14:39 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.4207
2022-03-11 08:14:41 - train: epoch 029, train_loss: 2.6939
2022-03-11 08:15:53 - eval: epoch: 029, acc1: 44.206%, acc5: 71.240%, test_loss: 2.4343, per_image_load_time: 2.677ms, per_image_inference_time: 0.144ms
2022-03-11 08:15:53 - until epoch: 029, best_acc1: 45.082%
2022-03-11 08:15:53 - epoch 030 lr: 0.1
2022-03-11 08:16:31 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.7616
2022-03-11 08:17:04 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.9254
2022-03-11 08:17:36 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.5642
2022-03-11 08:18:08 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.5537
2022-03-11 08:18:40 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.9530
2022-03-11 08:19:13 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.6948
2022-03-11 08:19:46 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.6931
2022-03-11 08:20:18 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.8612
2022-03-11 08:20:51 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.8823
2022-03-11 08:21:23 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.4735
2022-03-11 08:21:56 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.4847
2022-03-11 08:22:29 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.5257
2022-03-11 08:23:01 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.4126
2022-03-11 08:23:34 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.5394
2022-03-11 08:24:06 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.6181
2022-03-11 08:24:39 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.6739
2022-03-11 08:25:11 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.9562
2022-03-11 08:25:44 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.6227
2022-03-11 08:26:18 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.5998
2022-03-11 08:26:50 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.6244
2022-03-11 08:27:23 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.7872
2022-03-11 08:27:55 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.7565
2022-03-11 08:28:29 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.5829
2022-03-11 08:29:01 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.7008
2022-03-11 08:29:34 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.7552
2022-03-11 08:30:07 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.6046
2022-03-11 08:30:39 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.5470
2022-03-11 08:31:12 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.7437
2022-03-11 08:31:45 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.5868
2022-03-11 08:32:17 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.7755
2022-03-11 08:32:51 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.7390
2022-03-11 08:33:22 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.7170
2022-03-11 08:33:55 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 3.0538
2022-03-11 08:34:28 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.6118
2022-03-11 08:35:02 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.6602
2022-03-11 08:35:34 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.7335
2022-03-11 08:36:08 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.5529
2022-03-11 08:36:40 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.6727
2022-03-11 08:37:13 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.5814
2022-03-11 08:37:46 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.4579
2022-03-11 08:38:19 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.6413
2022-03-11 08:38:51 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.8396
2022-03-11 08:39:25 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.5137
2022-03-11 08:39:57 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.6446
2022-03-11 08:40:30 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.6217
2022-03-11 08:41:03 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.6157
2022-03-11 08:41:35 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.5917
2022-03-11 08:42:09 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.7238
2022-03-11 08:42:41 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.8443
2022-03-11 08:43:13 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.8508
2022-03-11 08:43:14 - train: epoch 030, train_loss: 2.6878
2022-03-11 08:44:28 - eval: epoch: 030, acc1: 44.022%, acc5: 70.902%, test_loss: 2.4472, per_image_load_time: 1.940ms, per_image_inference_time: 0.172ms
2022-03-11 08:44:28 - until epoch: 030, best_acc1: 45.082%
2022-03-11 08:44:28 - epoch 031 lr: 0.010000000000000002
2022-03-11 08:45:06 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.6921
2022-03-11 08:45:39 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.3168
2022-03-11 08:46:11 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 2.0964
2022-03-11 08:46:43 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.1629
2022-03-11 08:47:16 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 2.2238
2022-03-11 08:47:47 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 2.3425
2022-03-11 08:48:19 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 2.1464
2022-03-11 08:48:52 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 2.3464
2022-03-11 08:49:25 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 2.2024
2022-03-11 08:49:57 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.4583
2022-03-11 08:50:30 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.2227
2022-03-11 08:51:03 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 2.2852
2022-03-11 08:51:35 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 2.0269
2022-03-11 08:52:08 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 2.0223
2022-03-11 08:52:40 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.2443
2022-03-11 08:53:13 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 2.1344
2022-03-11 08:53:45 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 2.0075
2022-03-11 08:54:18 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.8691
2022-03-11 08:54:50 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 2.0786
2022-03-11 08:55:23 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 2.2289
2022-03-11 08:55:55 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 2.0814
2022-03-11 08:56:28 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 2.0615
2022-03-11 08:57:01 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 2.0015
2022-03-11 08:57:32 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 2.1320
2022-03-11 08:58:06 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 2.1876
2022-03-11 08:58:38 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 2.1213
2022-03-11 08:59:10 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 2.2110
2022-03-11 08:59:44 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.4542
2022-03-11 09:00:16 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 2.2788
2022-03-11 09:00:49 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.2229
2022-03-11 09:01:21 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 2.1481
2022-03-11 09:01:54 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 2.3326
2022-03-11 09:02:26 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 2.0409
2022-03-11 09:02:59 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.1990
2022-03-11 09:03:31 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 2.2174
2022-03-11 09:04:05 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 2.2113
2022-03-11 09:04:36 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.9990
2022-03-11 09:05:10 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 2.0026
2022-03-11 09:05:42 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 2.2014
2022-03-11 09:06:14 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 2.0133
2022-03-11 09:06:47 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 2.0266
2022-03-11 09:07:19 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 2.1574
2022-03-11 09:07:52 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.9697
2022-03-11 09:08:25 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 2.1149
2022-03-11 09:08:57 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 2.0952
2022-03-11 09:09:30 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 2.1203
2022-03-11 09:10:03 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.9268
2022-03-11 09:10:36 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 2.1358
2022-03-11 09:11:08 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 2.1508
2022-03-11 09:11:40 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 2.2009
2022-03-11 09:11:41 - train: epoch 031, train_loss: 2.1874
2022-03-11 09:12:54 - eval: epoch: 031, acc1: 57.934%, acc5: 81.388%, test_loss: 1.7644, per_image_load_time: 1.814ms, per_image_inference_time: 0.146ms
2022-03-11 09:12:54 - until epoch: 031, best_acc1: 57.934%
2022-03-11 09:12:54 - epoch 032 lr: 0.010000000000000002
2022-03-11 09:13:32 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.9759
2022-03-11 09:14:05 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.9193
2022-03-11 09:14:37 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.8416
2022-03-11 09:15:09 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 2.1505
2022-03-11 09:15:41 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 2.0920
2022-03-11 09:16:13 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 2.2819
2022-03-11 09:16:46 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 2.2013
2022-03-11 09:17:18 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 2.0146
2022-03-11 09:17:51 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 2.1759
2022-03-11 09:18:23 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 2.1252
2022-03-11 09:18:56 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 2.1169
2022-03-11 09:19:28 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 2.0369
2022-03-11 09:20:01 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.9617
2022-03-11 09:20:33 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 2.2545
2022-03-11 09:21:06 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 2.1747
2022-03-11 09:21:38 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 2.3380
2022-03-11 09:22:11 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 2.0252
2022-03-11 09:22:44 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.5882
2022-03-11 09:23:16 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.7827
2022-03-11 09:23:49 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.9964
2022-03-11 09:24:21 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 2.0202
2022-03-11 09:24:54 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 2.0913
2022-03-11 09:25:26 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.9534
2022-03-11 09:25:59 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 2.2168
2022-03-11 09:26:31 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 2.0558
2022-03-11 09:27:05 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.8664
2022-03-11 09:27:37 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 2.1160
2022-03-11 09:28:10 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 2.0875
2022-03-11 09:28:42 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 2.0501
2022-03-11 09:29:15 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.9620
2022-03-11 09:29:47 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 2.0355
2022-03-11 09:30:20 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 2.1582
2022-03-11 09:30:52 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.8131
2022-03-11 09:31:25 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.7074
2022-03-11 09:31:57 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 2.1054
2022-03-11 09:32:31 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 2.1989
2022-03-11 09:33:03 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 2.1357
2022-03-11 09:33:36 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.8933
2022-03-11 09:34:08 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.9708
2022-03-11 09:34:42 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.9082
2022-03-11 09:35:14 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.9870
2022-03-11 09:35:47 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.8171
2022-03-11 09:36:19 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 2.2068
2022-03-11 09:36:52 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.8400
2022-03-11 09:37:23 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 2.0488
2022-03-11 09:37:56 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.9081
2022-03-11 09:38:28 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 2.0127
2022-03-11 09:39:00 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.9565
2022-03-11 09:39:34 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 2.0682
2022-03-11 09:40:06 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 2.0170
2022-03-11 09:40:07 - train: epoch 032, train_loss: 2.0784
2022-03-11 09:41:21 - eval: epoch: 032, acc1: 58.978%, acc5: 82.254%, test_loss: 1.7145, per_image_load_time: 2.687ms, per_image_inference_time: 0.153ms
2022-03-11 09:41:21 - until epoch: 032, best_acc1: 58.978%
2022-03-11 09:41:21 - epoch 033 lr: 0.010000000000000002
2022-03-11 09:41:58 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.7497
2022-03-11 09:42:30 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.9696
2022-03-11 09:43:03 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.9448
2022-03-11 09:43:36 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.9514
2022-03-11 09:44:09 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 2.0432
2022-03-11 09:44:42 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.8353
2022-03-11 09:45:16 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 2.3152
2022-03-11 09:45:49 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 2.1824
2022-03-11 09:46:22 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 2.2180
2022-03-11 09:46:54 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 2.1982
2022-03-11 09:47:28 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 2.1328
2022-03-11 09:48:02 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 2.1909
2022-03-11 09:48:34 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 2.0769
2022-03-11 09:49:07 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 2.1051
2022-03-11 09:49:40 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 2.2418
2022-03-11 09:50:13 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 2.3631
2022-03-11 09:50:46 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.7950
2022-03-11 09:51:20 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 2.0769
2022-03-11 09:51:52 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.9195
2022-03-11 09:52:26 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.9524
2022-03-11 09:53:00 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.9407
2022-03-11 09:53:34 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 2.0844
2022-03-11 09:54:06 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 2.0432
2022-03-11 09:54:41 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 2.1995
2022-03-11 09:55:13 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 2.0149
2022-03-11 09:55:47 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 2.0373
2022-03-11 09:56:20 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 2.0517
2022-03-11 09:56:54 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 2.1219
2022-03-11 09:57:26 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 2.0132
2022-03-11 09:58:00 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 2.1425
2022-03-11 09:58:34 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 2.0965
2022-03-11 09:59:07 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.9869
2022-03-11 09:59:40 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.9471
2022-03-11 10:00:13 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.9197
2022-03-11 10:00:46 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 2.0516
2022-03-11 10:01:20 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 2.1876
2022-03-11 10:01:53 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 2.0889
2022-03-11 10:02:27 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.8704
2022-03-11 10:03:00 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 2.3439
2022-03-11 10:03:34 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 2.2032
2022-03-11 10:04:06 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.9941
2022-03-11 10:04:41 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.9971
2022-03-11 10:05:14 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 2.2046
2022-03-11 10:05:47 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 2.0485
2022-03-11 10:06:20 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 2.2926
2022-03-11 10:06:54 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 2.1482
2022-03-11 10:07:27 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 2.0240
2022-03-11 10:08:01 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.3542
2022-03-11 10:08:34 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.9620
2022-03-11 10:09:06 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.9840
2022-03-11 10:09:08 - train: epoch 033, train_loss: 2.0392
2022-03-11 10:10:23 - eval: epoch: 033, acc1: 59.156%, acc5: 82.474%, test_loss: 1.6947, per_image_load_time: 2.521ms, per_image_inference_time: 0.173ms
2022-03-11 10:10:23 - until epoch: 033, best_acc1: 59.156%
2022-03-11 10:10:23 - epoch 034 lr: 0.010000000000000002
2022-03-11 10:11:01 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.8972
2022-03-11 10:11:34 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 2.0243
2022-03-11 10:12:06 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.9138
2022-03-11 10:12:39 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.7697
2022-03-11 10:13:13 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.9825
2022-03-11 10:13:45 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 2.2045
2022-03-11 10:14:19 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.9290
2022-03-11 10:14:51 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 2.0890
2022-03-11 10:15:24 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.9304
2022-03-11 10:15:57 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 2.0305
2022-03-11 10:16:30 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 2.0248
2022-03-11 10:17:03 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 2.1056
2022-03-11 10:17:37 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.9337
2022-03-11 10:18:10 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 2.1221
2022-03-11 10:18:42 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 2.2003
2022-03-11 10:19:15 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 2.1772
2022-03-11 10:19:48 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.9816
2022-03-11 10:20:20 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 2.2445
2022-03-11 10:20:53 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 2.3544
2022-03-11 10:21:26 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 2.0261
2022-03-11 10:21:58 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 2.0634
2022-03-11 10:22:32 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.7567
2022-03-11 10:23:04 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 2.1279
2022-03-11 10:23:38 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.8036
2022-03-11 10:24:10 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 2.0973
2022-03-11 10:24:44 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 2.1834
2022-03-11 10:25:17 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 2.1840
2022-03-11 10:25:50 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.8673
2022-03-11 10:26:22 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 2.0126
2022-03-11 10:26:56 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 2.0420
2022-03-11 10:27:28 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.9816
2022-03-11 10:28:03 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 2.0042
2022-03-11 10:28:36 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.8623
2022-03-11 10:29:10 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 2.1597
2022-03-11 10:29:43 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.8117
2022-03-11 10:30:16 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.7987
2022-03-11 10:30:50 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.8398
2022-03-11 10:31:22 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.9120
2022-03-11 10:31:56 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 2.2773
2022-03-11 10:32:29 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.7731
2022-03-11 10:33:03 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 2.0325
2022-03-11 10:33:36 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.8582
2022-03-11 10:34:09 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 2.0462
2022-03-11 10:34:41 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.7966
2022-03-11 10:35:15 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 2.1664
2022-03-11 10:35:49 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 2.2824
2022-03-11 10:36:21 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 2.1617
2022-03-11 10:36:55 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 2.2071
2022-03-11 10:37:28 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 2.1039
2022-03-11 10:38:00 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.9579
2022-03-11 10:38:02 - train: epoch 034, train_loss: 2.0169
2022-03-11 10:39:15 - eval: epoch: 034, acc1: 59.544%, acc5: 82.432%, test_loss: 1.6867, per_image_load_time: 1.768ms, per_image_inference_time: 0.166ms
2022-03-11 10:39:15 - until epoch: 034, best_acc1: 59.544%
2022-03-11 10:39:15 - epoch 035 lr: 0.010000000000000002
2022-03-11 10:39:54 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.7199
2022-03-11 10:40:27 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.8289
2022-03-11 10:41:00 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 2.0532
2022-03-11 10:41:32 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 2.0341
2022-03-11 10:42:06 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.9710
2022-03-11 10:42:38 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 2.1173
2022-03-11 10:43:12 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 2.0466
2022-03-11 10:43:45 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 2.2261
2022-03-11 10:44:18 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 2.1696
2022-03-11 10:44:52 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.8357
2022-03-11 10:45:25 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 2.1149
2022-03-11 10:45:58 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.9676
2022-03-11 10:46:30 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.9973
2022-03-11 10:47:04 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 2.1284
2022-03-11 10:47:37 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.9220
2022-03-11 10:48:11 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.8498
2022-03-11 10:48:44 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.8121
2022-03-11 10:49:18 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 2.1689
2022-03-11 10:49:51 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 2.0061
2022-03-11 10:50:24 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 2.0780
2022-03-11 10:50:57 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.8561
2022-03-11 10:51:31 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 2.1110
2022-03-11 10:52:04 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.9735
2022-03-11 10:52:38 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 2.0797
2022-03-11 10:53:11 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 2.0564
2022-03-11 10:53:44 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.8943
2022-03-11 10:54:17 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 2.0004
2022-03-11 10:54:50 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.9511
2022-03-11 10:55:23 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.9703
2022-03-11 10:55:58 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.8906
2022-03-11 10:56:31 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 2.1720
2022-03-11 10:57:06 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.9392
2022-03-11 10:57:39 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.8827
2022-03-11 10:58:12 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 2.1382
2022-03-11 10:58:46 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.7237
2022-03-11 10:59:21 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.9344
2022-03-11 10:59:54 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 2.0418
2022-03-11 11:00:28 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.9153
2022-03-11 11:01:00 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.9512
2022-03-11 11:01:34 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.9353
2022-03-11 11:02:07 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 2.2074
2022-03-11 11:02:40 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.8042
2022-03-11 11:03:13 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.9907
2022-03-11 11:03:46 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 2.2737
2022-03-11 11:04:20 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 2.0128
2022-03-11 11:04:53 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.9697
2022-03-11 11:05:27 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 2.2298
2022-03-11 11:06:01 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 2.2544
2022-03-11 11:06:34 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 2.0580
2022-03-11 11:07:07 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.9889
2022-03-11 11:07:08 - train: epoch 035, train_loss: 2.0064
2022-03-11 11:08:23 - eval: epoch: 035, acc1: 59.748%, acc5: 82.744%, test_loss: 1.6763, per_image_load_time: 1.381ms, per_image_inference_time: 0.150ms
2022-03-11 11:08:23 - until epoch: 035, best_acc1: 59.748%
2022-03-11 11:08:23 - epoch 036 lr: 0.010000000000000002
2022-03-11 11:09:01 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.9133
2022-03-11 11:09:35 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.8154
2022-03-11 11:10:07 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.8902
2022-03-11 11:10:40 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.8781
2022-03-11 11:11:12 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.9092
2022-03-11 11:11:45 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.6862
2022-03-11 11:12:19 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.9768
2022-03-11 11:12:52 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.7204
2022-03-11 11:13:25 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 2.0432
2022-03-11 11:13:58 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 2.0753
2022-03-11 11:14:31 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 2.0795
2022-03-11 11:15:04 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.8587
2022-03-11 11:15:37 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 2.0060
2022-03-11 11:16:10 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.9679
2022-03-11 11:16:42 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 2.1415
2022-03-11 11:17:15 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 2.2930
2022-03-11 11:17:47 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.9177
2022-03-11 11:18:21 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 2.0270
2022-03-11 11:18:54 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 2.0831
2022-03-11 11:19:26 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.8641
2022-03-11 11:20:02 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.9688
2022-03-11 11:20:35 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 2.0027
2022-03-11 11:21:08 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.9481
2022-03-11 11:21:41 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 2.0486
2022-03-11 11:22:15 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.6884
2022-03-11 11:22:49 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 2.0963
2022-03-11 11:23:21 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.9752
2022-03-11 11:23:55 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.8689
2022-03-11 11:24:28 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.7491
2022-03-11 11:25:02 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.9320
2022-03-11 11:25:35 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 2.0521
2022-03-11 11:26:08 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 2.0542
2022-03-11 11:26:41 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.9842
2022-03-11 11:27:15 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.8004
2022-03-11 11:27:48 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 2.1136
2022-03-11 11:28:22 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 2.1032
2022-03-11 11:28:56 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.9897
2022-03-11 11:29:29 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 2.0369
2022-03-11 11:30:03 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.9825
2022-03-11 11:30:35 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 2.0629
2022-03-11 11:31:09 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.9401
2022-03-11 11:31:42 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 2.1051
2022-03-11 11:32:16 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 2.0157
2022-03-11 11:32:49 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 2.0629
2022-03-11 11:33:23 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.9400
2022-03-11 11:33:56 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.8635
2022-03-11 11:34:30 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.9541
2022-03-11 11:35:04 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.9625
2022-03-11 11:35:38 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.9584
2022-03-11 11:36:10 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.8821
2022-03-11 11:36:11 - train: epoch 036, train_loss: 1.9959
2022-03-11 11:37:26 - eval: epoch: 036, acc1: 59.620%, acc5: 82.930%, test_loss: 1.6723, per_image_load_time: 2.665ms, per_image_inference_time: 0.151ms
2022-03-11 11:37:26 - until epoch: 036, best_acc1: 59.748%
2022-03-11 11:37:26 - epoch 037 lr: 0.010000000000000002
2022-03-11 11:38:04 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 2.0113
2022-03-11 11:38:37 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.9193
2022-03-11 11:39:11 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 2.0646
2022-03-11 11:39:43 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.8986
2022-03-11 11:40:16 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 2.0484
2022-03-11 11:40:48 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 2.2308
2022-03-11 11:41:21 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 2.1221
2022-03-11 11:41:54 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 2.0422
2022-03-11 11:42:26 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.1512
2022-03-11 11:43:00 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 2.0100
2022-03-11 11:43:33 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 2.1348
2022-03-11 11:44:06 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 2.0893
2022-03-11 11:44:39 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 2.1233
2022-03-11 11:45:12 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 2.1609
2022-03-11 11:45:45 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.9240
2022-03-11 11:46:18 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.6848
2022-03-11 11:46:52 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 2.1407
2022-03-11 11:47:24 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 2.1804
2022-03-11 11:47:57 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.9814
2022-03-11 11:48:30 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.9472
2022-03-11 11:49:03 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.8595
2022-03-11 11:49:35 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.8923
2022-03-11 11:50:09 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.8575
2022-03-11 11:50:42 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 2.0217
2022-03-11 11:51:15 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.9490
2022-03-11 11:51:48 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 2.2716
2022-03-11 11:52:22 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 2.2193
2022-03-11 11:52:54 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.9411
2022-03-11 11:53:27 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.9738
2022-03-11 11:54:00 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 2.1275
2022-03-11 11:54:34 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.9638
2022-03-11 11:55:06 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 2.0351
2022-03-11 11:55:40 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.9087
2022-03-11 11:56:13 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.7669
2022-03-11 11:56:47 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 2.0136
2022-03-11 11:57:19 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 2.0541
2022-03-11 11:57:53 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.9444
2022-03-11 11:58:26 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 2.0165
2022-03-11 11:58:59 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.9195
2022-03-11 11:59:32 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.8403
2022-03-11 12:00:05 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.8489
2022-03-11 12:00:38 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 2.1805
2022-03-11 12:01:12 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 2.0554
2022-03-11 12:01:44 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.9772
2022-03-11 12:02:18 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.8325
2022-03-11 12:02:51 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 2.0963
2022-03-11 12:03:26 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 2.0491
2022-03-11 12:03:58 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.8133
2022-03-11 12:04:31 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.9852
2022-03-11 12:05:04 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 2.1496
2022-03-11 12:05:05 - train: epoch 037, train_loss: 1.9927
2022-03-11 12:06:20 - eval: epoch: 037, acc1: 59.780%, acc5: 83.030%, test_loss: 1.6648, per_image_load_time: 2.733ms, per_image_inference_time: 0.161ms
2022-03-11 12:06:20 - until epoch: 037, best_acc1: 59.780%
2022-03-11 12:06:20 - epoch 038 lr: 0.010000000000000002
2022-03-11 12:06:57 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.8140
2022-03-11 12:07:31 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 2.0700
2022-03-11 12:08:03 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.8275
2022-03-11 12:08:37 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.7472
2022-03-11 12:09:10 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.8243
2022-03-11 12:09:43 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 2.1968
2022-03-11 12:10:15 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.7884
2022-03-11 12:10:49 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.8818
2022-03-11 12:11:21 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.8803
2022-03-11 12:11:54 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 2.1858
2022-03-11 12:12:27 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 2.0057
2022-03-11 12:13:01 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 2.0731
2022-03-11 12:13:33 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 2.0509
2022-03-11 12:14:07 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.9572
2022-03-11 12:14:40 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.9066
2022-03-11 12:15:13 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 2.3822
2022-03-11 12:15:46 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 2.0914
2022-03-11 12:16:18 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 2.0132
2022-03-11 12:16:51 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 2.0182
2022-03-11 12:17:24 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 2.0028
2022-03-11 12:17:57 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 2.0522
2022-03-11 12:18:31 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.9699
2022-03-11 12:19:03 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 2.0867
2022-03-11 12:19:37 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 2.0870
2022-03-11 12:20:11 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.8990
2022-03-11 12:20:43 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.9437
2022-03-11 12:21:17 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.9929
2022-03-11 12:21:50 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 2.1218
2022-03-11 12:22:25 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 2.1066
2022-03-11 12:22:57 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 2.0233
2022-03-11 12:23:29 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 2.0584
2022-03-11 12:24:02 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.6556
2022-03-11 12:24:36 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.7887
2022-03-11 12:25:09 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.7787
2022-03-11 12:25:43 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.9209
2022-03-11 12:26:15 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.8911
2022-03-11 12:26:49 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.8698
2022-03-11 12:27:21 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 2.1337
2022-03-11 12:27:55 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.9589
2022-03-11 12:28:27 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 2.0835
2022-03-11 12:29:01 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 2.0356
2022-03-11 12:29:34 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.7297
2022-03-11 12:30:07 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 2.0852
2022-03-11 12:30:40 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.9866
2022-03-11 12:31:14 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 2.0385
2022-03-11 12:31:46 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.8638
2022-03-11 12:32:20 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.9204
2022-03-11 12:32:53 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.9410
2022-03-11 12:33:27 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 2.0579
2022-03-11 12:33:59 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.7600
2022-03-11 12:34:01 - train: epoch 038, train_loss: 1.9912
2022-03-11 12:35:16 - eval: epoch: 038, acc1: 60.038%, acc5: 83.036%, test_loss: 1.6589, per_image_load_time: 2.340ms, per_image_inference_time: 0.164ms
2022-03-11 12:35:16 - until epoch: 038, best_acc1: 60.038%
2022-03-11 12:35:16 - epoch 039 lr: 0.010000000000000002
2022-03-11 12:35:54 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.9159
2022-03-11 12:36:27 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 2.2420
2022-03-11 12:37:01 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.9432
2022-03-11 12:37:34 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.9890
2022-03-11 12:38:07 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 2.0665
2022-03-11 12:38:39 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.7949
2022-03-11 12:39:12 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 2.1025
2022-03-11 12:39:45 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 2.0021
2022-03-11 12:40:18 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.9578
2022-03-11 12:40:51 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.7849
2022-03-11 12:41:25 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 2.0623
2022-03-11 12:41:58 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 2.1763
2022-03-11 12:42:30 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.8751
2022-03-11 12:43:04 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 2.0590
2022-03-11 12:43:36 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 2.0449
2022-03-11 12:44:10 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.8792
2022-03-11 12:44:43 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.9372
2022-03-11 12:45:17 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.9875
2022-03-11 12:45:50 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.6234
2022-03-11 12:46:23 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.8581
2022-03-11 12:46:56 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 2.1626
2022-03-11 12:47:29 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.8725
2022-03-11 12:48:02 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 2.4655
2022-03-11 12:48:35 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 2.0358
2022-03-11 12:49:08 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.7940
2022-03-11 12:49:41 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.9342
2022-03-11 12:50:13 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 2.0266
2022-03-11 12:50:47 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.9493
2022-03-11 12:51:20 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.9827
2022-03-11 12:51:54 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 2.2130
2022-03-11 12:52:27 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.9699
2022-03-11 12:53:00 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 2.0935
2022-03-11 12:53:32 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.9643
2022-03-11 12:54:06 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.9689
2022-03-11 12:54:39 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 2.0650
2022-03-11 12:55:13 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.9266
2022-03-11 12:55:46 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.8389
2022-03-11 12:56:19 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.8948
2022-03-11 12:56:52 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.9862
2022-03-11 12:57:26 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.8881
2022-03-11 12:57:57 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.8536
2022-03-11 12:58:30 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 2.1165
2022-03-11 12:59:04 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.9360
2022-03-11 12:59:37 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.9137
2022-03-11 13:00:10 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.9124
2022-03-11 13:00:43 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 2.2216
2022-03-11 13:01:16 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.8489
2022-03-11 13:01:49 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.9198
2022-03-11 13:02:21 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.8074
2022-03-11 13:02:53 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.8157
2022-03-11 13:02:55 - train: epoch 039, train_loss: 1.9898
2022-03-11 13:04:09 - eval: epoch: 039, acc1: 59.728%, acc5: 82.948%, test_loss: 1.6666, per_image_load_time: 2.271ms, per_image_inference_time: 0.154ms
2022-03-11 13:04:09 - until epoch: 039, best_acc1: 60.038%
2022-03-11 13:04:09 - epoch 040 lr: 0.010000000000000002
2022-03-11 13:04:47 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 2.1775
2022-03-11 13:05:21 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 2.0861
2022-03-11 13:05:54 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 2.3264
2022-03-11 13:06:26 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.8259
2022-03-11 13:07:00 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.9687
2022-03-11 13:07:32 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.8221
2022-03-11 13:08:05 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 2.0012
2022-03-11 13:08:39 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 2.1390
2022-03-11 13:09:11 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.8160
2022-03-11 13:09:45 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.8831
2022-03-11 13:10:18 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.8020
2022-03-11 13:10:50 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.9535
2022-03-11 13:11:23 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.9977
2022-03-11 13:11:56 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.9978
2022-03-11 13:12:28 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 2.1387
2022-03-11 13:13:01 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.9925
2022-03-11 13:13:34 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.9312
2022-03-11 13:14:08 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.9758
2022-03-11 13:14:40 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.7948
2022-03-11 13:15:13 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 2.1582
2022-03-11 13:15:46 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.9574
2022-03-11 13:16:20 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.7779
2022-03-11 13:16:53 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 2.1263
2022-03-11 13:17:26 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 2.0331
2022-03-11 13:17:58 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 2.0734
2022-03-11 13:18:32 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.9348
2022-03-11 13:19:05 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 2.0814
2022-03-11 13:19:38 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 2.1274
2022-03-11 13:20:11 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 2.1954
2022-03-11 13:20:45 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.9168
2022-03-11 13:21:19 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 2.1423
2022-03-11 13:21:51 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 2.2168
2022-03-11 13:22:24 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.8742
2022-03-11 13:22:56 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 2.0828
2022-03-11 13:23:30 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.9753
2022-03-11 13:24:02 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 2.1213
2022-03-11 13:24:36 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.9249
2022-03-11 13:25:09 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.8934
2022-03-11 13:25:43 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 2.0731
2022-03-11 13:26:16 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 2.0008
2022-03-11 13:26:49 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.9822
2022-03-11 13:27:22 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.9481
2022-03-11 13:27:56 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 2.0779
2022-03-11 13:28:29 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 2.2246
2022-03-11 13:29:03 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 2.0274
2022-03-11 13:29:37 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.9484
2022-03-11 13:30:11 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.9980
2022-03-11 13:30:44 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.9682
2022-03-11 13:31:18 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.9307
2022-03-11 13:31:51 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.8844
2022-03-11 13:31:52 - train: epoch 040, train_loss: 1.9869
2022-03-11 13:33:07 - eval: epoch: 040, acc1: 59.858%, acc5: 82.952%, test_loss: 1.6666, per_image_load_time: 2.050ms, per_image_inference_time: 0.157ms
2022-03-11 13:33:07 - until epoch: 040, best_acc1: 60.038%
2022-03-11 13:33:07 - epoch 041 lr: 0.010000000000000002
2022-03-11 13:33:45 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 2.1924
2022-03-11 13:34:19 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 2.2811
2022-03-11 13:34:52 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.9708
2022-03-11 13:35:24 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 2.0071
2022-03-11 13:35:57 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.8467
2022-03-11 13:36:30 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.8930
2022-03-11 13:37:03 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.8304
2022-03-11 13:37:36 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.7144
2022-03-11 13:38:09 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.7678
2022-03-11 13:38:42 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 2.0911
2022-03-11 13:39:15 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.9357
2022-03-11 13:39:49 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.6825
2022-03-11 13:40:20 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.9419
2022-03-11 13:40:55 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.9244
2022-03-11 13:41:27 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 2.3624
2022-03-11 13:42:00 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 2.0547
2022-03-11 13:42:34 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.7016
2022-03-11 13:43:07 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.8764
2022-03-11 13:43:40 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 2.1257
2022-03-11 13:44:12 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.8549
2022-03-11 13:44:46 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.8563
2022-03-11 13:45:19 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.9657
2022-03-11 13:45:52 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.8185
2022-03-11 13:46:25 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 2.2020
2022-03-11 13:46:59 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.9575
2022-03-11 13:47:32 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 2.2400
2022-03-11 13:48:04 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 2.2812
2022-03-11 13:48:37 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.9562
2022-03-11 13:49:10 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.9894
2022-03-11 13:49:43 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 2.1308
2022-03-11 13:50:16 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 2.0753
2022-03-11 13:50:48 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.9321
2022-03-11 13:51:22 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.7844
2022-03-11 13:51:54 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.9863
2022-03-11 13:52:27 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.8560
2022-03-11 13:52:59 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 2.1116
2022-03-11 13:53:33 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.9990
2022-03-11 13:54:05 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.8682
2022-03-11 13:54:38 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.9788
2022-03-11 13:55:10 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.7249
2022-03-11 13:55:43 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 2.0899
2022-03-11 13:56:16 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.9100
2022-03-11 13:56:49 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.8386
2022-03-11 13:57:22 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.9489
2022-03-11 13:57:56 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 2.0179
2022-03-11 13:58:28 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 2.0677
2022-03-11 13:59:02 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 2.0108
2022-03-11 13:59:34 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.9790
2022-03-11 14:00:08 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 2.1244
2022-03-11 14:00:39 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.9372
2022-03-11 14:00:41 - train: epoch 041, train_loss: 1.9901
2022-03-11 14:01:54 - eval: epoch: 041, acc1: 59.912%, acc5: 83.030%, test_loss: 1.6609, per_image_load_time: 1.333ms, per_image_inference_time: 0.155ms
2022-03-11 14:01:54 - until epoch: 041, best_acc1: 60.038%
2022-03-11 14:01:54 - epoch 042 lr: 0.010000000000000002
2022-03-11 14:02:31 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.7253
2022-03-11 14:03:04 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.7638
2022-03-11 14:03:37 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 2.0449
2022-03-11 14:04:09 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.9104
2022-03-11 14:04:42 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.9242
2022-03-11 14:05:15 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.9962
2022-03-11 14:05:47 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 2.2717
2022-03-11 14:06:20 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.9691
2022-03-11 14:06:53 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.9771
2022-03-11 14:07:27 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 2.1071
2022-03-11 14:08:01 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.8839
2022-03-11 14:08:33 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.7492
2022-03-11 14:09:08 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.9774
2022-03-11 14:09:40 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 2.1225
2022-03-11 14:10:13 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.9068
2022-03-11 14:10:45 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.8389
2022-03-11 14:11:18 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 2.0693
2022-03-11 14:11:51 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.9877
2022-03-11 14:12:24 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 2.1843
2022-03-11 14:12:56 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.8367
2022-03-11 14:13:28 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.7792
2022-03-11 14:14:02 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.9073
2022-03-11 14:14:34 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.8567
2022-03-11 14:15:08 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 2.1052
2022-03-11 14:15:40 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 2.0450
2022-03-11 14:16:13 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.9852
2022-03-11 14:16:46 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.8014
2022-03-11 14:17:19 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.8807
2022-03-11 14:17:51 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.9915
2022-03-11 14:18:24 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.9934
2022-03-11 14:18:57 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 2.0958
2022-03-11 14:19:29 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 2.1004
2022-03-11 14:20:01 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 2.2849
2022-03-11 14:20:34 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.8282
2022-03-11 14:21:07 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.9584
2022-03-11 14:21:39 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.9136
2022-03-11 14:22:12 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 2.0690
2022-03-11 14:22:45 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.7858
2022-03-11 14:23:17 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 2.0296
2022-03-11 14:23:50 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.7991
2022-03-11 14:24:22 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 2.2127
2022-03-11 14:24:55 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 2.2958
2022-03-11 14:25:28 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.9064
2022-03-11 14:26:01 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.9760
2022-03-11 14:26:33 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.8439
2022-03-11 14:27:07 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.9828
2022-03-11 14:27:39 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 2.0950
2022-03-11 14:28:12 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.9441
2022-03-11 14:28:46 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.8855
2022-03-11 14:29:18 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 2.1042
2022-03-11 14:29:19 - train: epoch 042, train_loss: 1.9910
2022-03-11 14:30:33 - eval: epoch: 042, acc1: 59.318%, acc5: 82.796%, test_loss: 1.6803, per_image_load_time: 1.891ms, per_image_inference_time: 0.171ms
2022-03-11 14:30:33 - until epoch: 042, best_acc1: 60.038%
2022-03-11 14:30:33 - epoch 043 lr: 0.010000000000000002
2022-03-11 14:31:11 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 2.0129
2022-03-11 14:31:45 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 2.0029
2022-03-11 14:32:17 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.7595
2022-03-11 14:32:50 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.8155
2022-03-11 14:33:23 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.8708
2022-03-11 14:33:55 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.9286
2022-03-11 14:34:28 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.7986
2022-03-11 14:35:01 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 2.0089
2022-03-11 14:35:33 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.9048
2022-03-11 14:36:07 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 2.0815
2022-03-11 14:36:40 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 2.2822
2022-03-11 14:37:13 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.9012
2022-03-11 14:37:47 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 2.0381
2022-03-11 14:38:21 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.9723
2022-03-11 14:38:53 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.9400
2022-03-11 14:39:28 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.8348
2022-03-11 14:39:59 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 2.1283
2022-03-11 14:40:33 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 2.1136
2022-03-11 14:41:06 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.9968
2022-03-11 14:41:40 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.8125
2022-03-11 14:42:13 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.7305
2022-03-11 14:42:46 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 2.1095
2022-03-11 14:43:20 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 2.2572
2022-03-11 14:43:54 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.6606
2022-03-11 14:44:28 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.9805
2022-03-11 14:45:02 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.7885
2022-03-11 14:45:37 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 2.0869
2022-03-11 14:46:11 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.6767
2022-03-11 14:46:45 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 2.0689
2022-03-11 14:47:18 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 2.3974
2022-03-11 14:47:53 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 2.3056
2022-03-11 14:48:27 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.9864
2022-03-11 14:49:01 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 2.0655
2022-03-11 14:49:35 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 2.0967
2022-03-11 14:50:08 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 2.0307
2022-03-11 14:50:43 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.9007
2022-03-11 14:51:16 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.9126
2022-03-11 14:51:49 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 2.1681
2022-03-11 14:52:23 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 2.0361
2022-03-11 14:52:56 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 2.1290
2022-03-11 14:53:30 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 2.1825
2022-03-11 14:54:03 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 2.1305
2022-03-11 14:54:38 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 2.1016
2022-03-11 14:55:10 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 2.0829
2022-03-11 14:55:44 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.8974
2022-03-11 14:56:17 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 2.0029
2022-03-11 14:56:51 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.9467
2022-03-11 14:57:25 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.9054
2022-03-11 14:57:59 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 2.0236
2022-03-11 14:58:32 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.9434
2022-03-11 14:58:34 - train: epoch 043, train_loss: 1.9878
2022-03-11 14:59:50 - eval: epoch: 043, acc1: 59.622%, acc5: 83.046%, test_loss: 1.6678, per_image_load_time: 2.818ms, per_image_inference_time: 0.145ms
2022-03-11 14:59:50 - until epoch: 043, best_acc1: 60.038%
2022-03-11 14:59:50 - epoch 044 lr: 0.010000000000000002
2022-03-11 15:00:29 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.9416
2022-03-11 15:01:03 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 2.0642
2022-03-11 15:01:36 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.8240
2022-03-11 15:02:09 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.6084
2022-03-11 15:02:43 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 2.0017
2022-03-11 15:03:15 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.7803
2022-03-11 15:03:50 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 2.0401
2022-03-11 15:04:22 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.9895
2022-03-11 15:04:56 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.7712
2022-03-11 15:05:30 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.8751
2022-03-11 15:06:03 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.7173
2022-03-11 15:06:37 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.8246
2022-03-11 15:07:10 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.9272
2022-03-11 15:07:44 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.7572
2022-03-11 15:08:17 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 2.0497
2022-03-11 15:08:51 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.9697
2022-03-11 15:09:24 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.9816
2022-03-11 15:09:59 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.8971
2022-03-11 15:10:31 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 2.2604
2022-03-11 15:11:05 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.8227
2022-03-11 15:11:38 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 2.1886
2022-03-11 15:12:12 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 2.0266
2022-03-11 15:12:46 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.9859
2022-03-11 15:13:19 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.9530
2022-03-11 15:13:53 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.9491
2022-03-11 15:14:27 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 2.0612
2022-03-11 15:15:00 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.9749
2022-03-11 15:15:33 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.7310
2022-03-11 15:16:07 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.9063
2022-03-11 15:16:40 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.7109
2022-03-11 15:17:14 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 2.0589
2022-03-11 15:17:45 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.9393
2022-03-11 15:18:19 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 2.0557
2022-03-11 15:18:52 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 2.0740
2022-03-11 15:19:25 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.9277
2022-03-11 15:19:58 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.9318
2022-03-11 15:20:32 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.9806
2022-03-11 15:21:04 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.6844
2022-03-11 15:21:38 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 2.1755
2022-03-11 15:22:10 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 2.0918
2022-03-11 15:22:44 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.9396
2022-03-11 15:23:17 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.8906
2022-03-11 15:23:51 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.8303
2022-03-11 15:24:24 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.9368
2022-03-11 15:24:58 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 2.3189
2022-03-11 15:25:31 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.8786
2022-03-11 15:26:06 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 2.0057
2022-03-11 15:26:39 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 2.0073
2022-03-11 15:27:13 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 2.0591
2022-03-11 15:27:45 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.9678
2022-03-11 15:27:47 - train: epoch 044, train_loss: 1.9882
2022-03-11 15:29:02 - eval: epoch: 044, acc1: 59.788%, acc5: 82.882%, test_loss: 1.6744, per_image_load_time: 2.792ms, per_image_inference_time: 0.135ms
2022-03-11 15:29:02 - until epoch: 044, best_acc1: 60.038%
2022-03-11 15:29:02 - epoch 045 lr: 0.010000000000000002
2022-03-11 15:29:40 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.9167
2022-03-11 15:30:13 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.8704
2022-03-11 15:30:46 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.8167
2022-03-11 15:31:20 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.9931
2022-03-11 15:31:52 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 2.2689
2022-03-11 15:32:26 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 2.2311
2022-03-11 15:32:59 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.7758
2022-03-11 15:33:31 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.9335
2022-03-11 15:34:05 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.9017
2022-03-11 15:34:37 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.9434
2022-03-11 15:35:11 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 2.1780
2022-03-11 15:35:43 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.9736
2022-03-11 15:36:17 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 2.1836
2022-03-11 15:36:50 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.9765
2022-03-11 15:37:24 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.8410
2022-03-11 15:37:56 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.8365
2022-03-11 15:38:30 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.8070
2022-03-11 15:39:03 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.8602
2022-03-11 15:39:42 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.9351
2022-03-11 15:40:14 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 2.1091
2022-03-11 15:40:48 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.9656
2022-03-11 15:41:22 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 2.0054
2022-03-11 15:41:55 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.7194
2022-03-11 15:42:32 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.8550
2022-03-11 15:43:05 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.9312
2022-03-11 15:43:39 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.9869
2022-03-11 15:44:11 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.9770
2022-03-11 15:44:45 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.8039
2022-03-11 15:45:17 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.9674
2022-03-11 15:45:51 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 2.1143
2022-03-11 15:46:23 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 2.0392
2022-03-11 15:46:57 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 2.0442
2022-03-11 15:47:30 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 2.0297
2022-03-11 15:48:08 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 2.1484
2022-03-11 15:48:45 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 2.0351
2022-03-11 15:49:21 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.9233
2022-03-11 15:49:58 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.9983
2022-03-11 15:50:34 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 2.0986
2022-03-11 15:51:11 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.9606
2022-03-11 15:51:45 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.8884
2022-03-11 15:52:19 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.9052
2022-03-11 15:52:55 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 2.1629
2022-03-11 15:53:29 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 2.1705
2022-03-11 15:54:05 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 2.0297
2022-03-11 15:54:41 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.9285
2022-03-11 15:55:16 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 2.1548
2022-03-11 15:55:50 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.9918
2022-03-11 15:56:26 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.8221
2022-03-11 15:56:59 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 2.0784
2022-03-11 15:57:35 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.8853
2022-03-11 15:57:36 - train: epoch 045, train_loss: 1.9890
2022-03-11 15:58:54 - eval: epoch: 045, acc1: 59.560%, acc5: 83.062%, test_loss: 1.6715, per_image_load_time: 2.216ms, per_image_inference_time: 0.164ms
2022-03-11 15:58:54 - until epoch: 045, best_acc1: 60.038%
2022-03-11 15:58:54 - epoch 046 lr: 0.010000000000000002
2022-03-11 15:59:32 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 2.0263
2022-03-11 16:00:06 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.8601
2022-03-11 16:00:39 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.9002
2022-03-11 16:01:12 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 2.1051
2022-03-11 16:01:44 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.8080
2022-03-11 16:02:17 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 2.0257
2022-03-11 16:02:50 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 2.0096
2022-03-11 16:03:24 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 2.2170
2022-03-11 16:03:56 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.8670
2022-03-11 16:04:30 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.9342
2022-03-11 16:05:01 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 2.0038
2022-03-11 16:05:36 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.8871
2022-03-11 16:06:09 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.9617
2022-03-11 16:06:42 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 2.0542
2022-03-11 16:07:15 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.8459
2022-03-11 16:07:48 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 2.0237
2022-03-11 16:08:20 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 2.0071
2022-03-11 16:08:54 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 2.1020
2022-03-11 16:09:27 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.7957
2022-03-11 16:10:00 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.9227
2022-03-11 16:10:33 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 2.1699
2022-03-11 16:11:07 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.7773
2022-03-11 16:11:39 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 2.1574
2022-03-11 16:12:13 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.9017
2022-03-11 16:12:46 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 2.0360
2022-03-11 16:13:19 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 2.1042
2022-03-11 16:13:52 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.8501
2022-03-11 16:14:25 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.8156
2022-03-11 16:14:58 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 2.2127
2022-03-11 16:15:31 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.7489
2022-03-11 16:16:04 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.8254
2022-03-11 16:16:36 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.9648
2022-03-11 16:17:10 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 2.0386
2022-03-11 16:17:44 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 2.1198
2022-03-11 16:18:17 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.9936
2022-03-11 16:18:50 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 2.1659
2022-03-11 16:19:23 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.8551
2022-03-11 16:19:56 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.9565
2022-03-11 16:20:29 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.9357
2022-03-11 16:21:03 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.9050
2022-03-11 16:21:35 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 2.1896
2022-03-11 16:22:09 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.7246
2022-03-11 16:22:42 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 2.2602
2022-03-11 16:23:14 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 2.1926
2022-03-11 16:23:47 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.9543
2022-03-11 16:24:21 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.9563
2022-03-11 16:24:53 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.8430
2022-03-11 16:25:26 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.9088
2022-03-11 16:25:59 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 2.0970
2022-03-11 16:26:30 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.9746
2022-03-11 16:26:31 - train: epoch 046, train_loss: 1.9847
2022-03-11 16:27:45 - eval: epoch: 046, acc1: 59.466%, acc5: 83.006%, test_loss: 1.6726, per_image_load_time: 2.386ms, per_image_inference_time: 0.166ms
2022-03-11 16:27:45 - until epoch: 046, best_acc1: 60.038%
2022-03-11 16:27:45 - epoch 047 lr: 0.010000000000000002
2022-03-11 16:28:23 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.7882
2022-03-11 16:28:56 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 2.0447
2022-03-11 16:29:29 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.8270
2022-03-11 16:30:01 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.7289
2022-03-11 16:30:34 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.8872
2022-03-11 16:31:07 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 2.2172
2022-03-11 16:31:39 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.9779
2022-03-11 16:32:12 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 2.0909
2022-03-11 16:32:44 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 2.1076
2022-03-11 16:33:17 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 2.0092
2022-03-11 16:33:49 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 2.2443
2022-03-11 16:34:21 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.9335
2022-03-11 16:34:55 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.9222
2022-03-11 16:35:28 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.7358
2022-03-11 16:36:00 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 2.0350
2022-03-11 16:36:33 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.9611
2022-03-11 16:37:06 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.7919
2022-03-11 16:37:39 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.9990
2022-03-11 16:38:11 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.8151
2022-03-11 16:38:43 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.9865
2022-03-11 16:39:16 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 2.1098
2022-03-11 16:39:50 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 2.1234
2022-03-11 16:40:22 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 2.1003
2022-03-11 16:40:56 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.9811
2022-03-11 16:41:28 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 2.0123
2022-03-11 16:42:01 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 2.1183
2022-03-11 16:42:33 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 2.1265
2022-03-11 16:43:07 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 2.1682
2022-03-11 16:43:39 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 2.0618
2022-03-11 16:44:13 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 2.2128
2022-03-11 16:44:45 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.9462
2022-03-11 16:45:20 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 2.1104
2022-03-11 16:45:51 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 2.0027
2022-03-11 16:46:24 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.8414
2022-03-11 16:46:57 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.9815
2022-03-11 16:47:31 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 2.1450
2022-03-11 16:48:04 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 2.0166
2022-03-11 16:48:36 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.8355
2022-03-11 16:49:10 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 2.0199
2022-03-11 16:49:43 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 2.1596
2022-03-11 16:50:16 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 2.1208
2022-03-11 16:50:49 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.8210
2022-03-11 16:51:22 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.9820
2022-03-11 16:51:55 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.8865
2022-03-11 16:52:29 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 2.0329
2022-03-11 16:53:01 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 2.0102
2022-03-11 16:53:34 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 2.2120
2022-03-11 16:54:07 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.8341
2022-03-11 16:54:40 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 2.0647
2022-03-11 16:55:12 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 2.0221
2022-03-11 16:55:13 - train: epoch 047, train_loss: 1.9875
2022-03-11 16:56:27 - eval: epoch: 047, acc1: 59.680%, acc5: 83.038%, test_loss: 1.6649, per_image_load_time: 1.281ms, per_image_inference_time: 0.167ms
2022-03-11 16:56:27 - until epoch: 047, best_acc1: 60.038%
2022-03-11 16:56:27 - epoch 048 lr: 0.010000000000000002
2022-03-11 16:57:05 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.9597
2022-03-11 16:57:38 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.2661
2022-03-11 16:58:11 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.9588
2022-03-11 16:58:43 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.9014
2022-03-11 16:59:16 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.9213
2022-03-11 16:59:48 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.9710
2022-03-11 17:00:21 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.9456
2022-03-11 17:00:53 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 2.0292
2022-03-11 17:01:26 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 2.2173
2022-03-11 17:01:59 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 2.0259
2022-03-11 17:02:31 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 2.1003
2022-03-11 17:03:04 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 2.0351
2022-03-11 17:03:37 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.9159
2022-03-11 17:04:10 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 2.0611
2022-03-11 17:04:42 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.9591
2022-03-11 17:05:15 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 2.0008
2022-03-11 17:05:48 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.8014
2022-03-11 17:06:22 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.9862
2022-03-11 17:06:54 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 2.0198
2022-03-11 17:07:27 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.9796
2022-03-11 17:08:00 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 2.0900
2022-03-11 17:08:33 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 2.1466
2022-03-11 17:09:05 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.8026
2022-03-11 17:09:39 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 2.1074
2022-03-11 17:10:11 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 2.0884
2022-03-11 17:10:44 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 2.0593
2022-03-11 17:11:18 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 2.1741
2022-03-11 17:11:50 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.9398
2022-03-11 17:12:23 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 2.0992
2022-03-11 17:12:55 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.9430
2022-03-11 17:13:29 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.9517
2022-03-11 17:14:01 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.8893
2022-03-11 17:14:35 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 2.0670
2022-03-11 17:15:07 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.9945
2022-03-11 17:15:41 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 2.2476
2022-03-11 17:16:12 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 2.1013
2022-03-11 17:16:46 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 2.1052
2022-03-11 17:17:18 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 2.0169
2022-03-11 17:17:52 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 2.0201
2022-03-11 17:18:25 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.7019
2022-03-11 17:18:59 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 2.2007
2022-03-11 17:19:31 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.7994
2022-03-11 17:20:04 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 2.1848
2022-03-11 17:20:36 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.8730
2022-03-11 17:21:10 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 2.0191
2022-03-11 17:21:42 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 2.0740
2022-03-11 17:22:16 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 2.0189
2022-03-11 17:22:54 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 2.3871
2022-03-11 17:23:33 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.9270
2022-03-11 17:24:10 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.9192
2022-03-11 17:24:12 - train: epoch 048, train_loss: 1.9853
2022-03-11 17:25:30 - eval: epoch: 048, acc1: 59.758%, acc5: 82.956%, test_loss: 1.6696, per_image_load_time: 1.903ms, per_image_inference_time: 0.145ms
2022-03-11 17:25:30 - until epoch: 048, best_acc1: 60.038%
2022-03-11 17:25:30 - epoch 049 lr: 0.010000000000000002
2022-03-11 17:26:08 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.9702
2022-03-11 17:26:41 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.7922
2022-03-11 17:27:13 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 2.1608
2022-03-11 17:27:47 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 2.0288
2022-03-11 17:28:18 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 2.0315
2022-03-11 17:28:51 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.7699
2022-03-11 17:29:24 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.8466
2022-03-11 17:29:57 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 2.1773
2022-03-11 17:30:29 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.7854
2022-03-11 17:31:02 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 2.0644
2022-03-11 17:31:36 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.9640
2022-03-11 17:32:09 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.8031
2022-03-11 17:32:40 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 2.0612
2022-03-11 17:33:14 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 2.1235
2022-03-11 17:33:47 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 2.0490
2022-03-11 17:34:20 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 2.0269
2022-03-11 17:34:53 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.8601
2022-03-11 17:35:26 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.8426
2022-03-11 17:35:59 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.9390
2022-03-11 17:36:32 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.9464
2022-03-11 17:37:05 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 2.0124
2022-03-11 17:37:38 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 2.1043
2022-03-11 17:38:10 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.8612
2022-03-11 17:38:44 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 2.1041
2022-03-11 17:39:17 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.9435
2022-03-11 17:39:49 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.9733
2022-03-11 17:40:24 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.9105
2022-03-11 17:40:56 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 2.0371
2022-03-11 17:41:29 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.9936
2022-03-11 17:42:01 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.8763
2022-03-11 17:42:34 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.9137
2022-03-11 17:43:07 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 2.1777
2022-03-11 17:43:40 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.8940
2022-03-11 17:44:13 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 2.2442
2022-03-11 17:44:47 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.9094
2022-03-11 17:45:19 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 2.2661
2022-03-11 17:45:52 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.8228
2022-03-11 17:46:26 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 2.0769
2022-03-11 17:46:59 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 2.1787
2022-03-11 17:47:32 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.9105
2022-03-11 17:48:05 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 2.0719
2022-03-11 17:48:39 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 2.0697
2022-03-11 17:49:12 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 2.1231
2022-03-11 17:49:44 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 2.0397
2022-03-11 17:50:17 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.8808
2022-03-11 17:50:52 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 2.2026
2022-03-11 17:51:25 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 2.0737
2022-03-11 17:51:58 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 2.0525
2022-03-11 17:52:31 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.8891
2022-03-11 17:53:04 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.9160
2022-03-11 17:53:06 - train: epoch 049, train_loss: 1.9834
2022-03-11 17:54:22 - eval: epoch: 049, acc1: 59.682%, acc5: 82.904%, test_loss: 1.6799, per_image_load_time: 2.748ms, per_image_inference_time: 0.183ms
2022-03-11 17:54:22 - until epoch: 049, best_acc1: 60.038%
2022-03-11 17:54:22 - epoch 050 lr: 0.010000000000000002
2022-03-11 17:55:00 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.9382
2022-03-11 17:55:33 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 2.1370
2022-03-11 17:56:07 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 2.1046
2022-03-11 17:56:40 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.8137
2022-03-11 17:57:13 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.8744
2022-03-11 17:57:46 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 2.1943
2022-03-11 17:58:20 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.8817
2022-03-11 17:58:52 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.8692
2022-03-11 17:59:25 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 2.2043
2022-03-11 17:59:58 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 2.0251
2022-03-11 18:00:31 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.9006
2022-03-11 18:01:04 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.8757
2022-03-11 18:01:37 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.8406
2022-03-11 18:02:09 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.8943
2022-03-11 18:02:44 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.8250
2022-03-11 18:03:16 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 2.0431
2022-03-11 18:03:49 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 2.0182
2022-03-11 18:04:23 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 2.0848
2022-03-11 18:04:56 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 2.0920
2022-03-11 18:05:30 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.9914
2022-03-11 18:06:03 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.8827
2022-03-11 18:06:36 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 2.0019
2022-03-11 18:07:09 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.6988
2022-03-11 18:07:42 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 2.0543
2022-03-11 18:08:15 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.7980
2022-03-11 18:08:49 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.8442
2022-03-11 18:09:21 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.9785
2022-03-11 18:09:55 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 2.0639
2022-03-11 18:10:28 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 2.3306
2022-03-11 18:11:02 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 2.1691
2022-03-11 18:11:35 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.9083
2022-03-11 18:12:07 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 2.0514
2022-03-11 18:12:41 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.8913
2022-03-11 18:13:13 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 2.1651
2022-03-11 18:13:47 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.9130
2022-03-11 18:14:20 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 2.0352
2022-03-11 18:14:53 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 2.2572
2022-03-11 18:15:27 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.8881
2022-03-11 18:15:59 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.8077
2022-03-11 18:16:31 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 2.0256
2022-03-11 18:17:03 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.8719
2022-03-11 18:17:38 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 2.0145
2022-03-11 18:18:10 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 2.1275
2022-03-11 18:18:43 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.9545
2022-03-11 18:19:17 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.9657
2022-03-11 18:19:50 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 2.0977
2022-03-11 18:20:23 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 2.1391
2022-03-11 18:20:55 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.9768
2022-03-11 18:21:30 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.9923
2022-03-11 18:22:02 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.9270
2022-03-11 18:22:04 - train: epoch 050, train_loss: 1.9806
2022-03-11 18:23:18 - eval: epoch: 050, acc1: 59.786%, acc5: 83.166%, test_loss: 1.6668, per_image_load_time: 1.435ms, per_image_inference_time: 0.149ms
2022-03-11 18:23:18 - until epoch: 050, best_acc1: 60.038%
2022-03-11 18:23:18 - epoch 051 lr: 0.010000000000000002
2022-03-11 18:23:56 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 2.1958
2022-03-11 18:24:29 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 2.3008
2022-03-11 18:25:02 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.9132
2022-03-11 18:25:35 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.7113
2022-03-11 18:26:09 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 2.1379
2022-03-11 18:26:40 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.9144
2022-03-11 18:27:13 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 2.0904
2022-03-11 18:27:45 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 2.0618
2022-03-11 18:28:19 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 2.1394
2022-03-11 18:28:51 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 2.3823
2022-03-11 18:29:24 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 2.0915
2022-03-11 18:29:57 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.8111
2022-03-11 18:30:31 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.8275
2022-03-11 18:31:04 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.9791
2022-03-11 18:31:36 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.9787
2022-03-11 18:32:10 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.7922
2022-03-11 18:32:42 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 2.1377
2022-03-11 18:33:16 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.9813
2022-03-11 18:33:49 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.7885
2022-03-11 18:34:22 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 2.0695
2022-03-11 18:34:55 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 2.0457
2022-03-11 18:35:29 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.9913
2022-03-11 18:36:01 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.9913
2022-03-11 18:36:34 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.9005
2022-03-11 18:37:08 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.8574
2022-03-11 18:37:40 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.8589
2022-03-11 18:38:13 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.8970
2022-03-11 18:38:45 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.9924
2022-03-11 18:39:19 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 2.0225
2022-03-11 18:39:52 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.9759
2022-03-11 18:40:25 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 2.1209
2022-03-11 18:40:58 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 2.0138
2022-03-11 18:41:31 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 2.0590
2022-03-11 18:42:04 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 2.0279
2022-03-11 18:42:36 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.7961
2022-03-11 18:43:09 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.9491
2022-03-11 18:43:43 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 2.0732
2022-03-11 18:44:15 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.8999
2022-03-11 18:44:49 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 2.0643
2022-03-11 18:45:22 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.9584
2022-03-11 18:45:55 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.9259
2022-03-11 18:46:29 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 2.0080
2022-03-11 18:47:03 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.9704
2022-03-11 18:47:35 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 2.0404
2022-03-11 18:48:09 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.8314
2022-03-11 18:48:42 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 2.0035
2022-03-11 18:49:16 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 2.0935
2022-03-11 18:49:48 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 2.0867
2022-03-11 18:50:22 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 2.1713
2022-03-11 18:50:54 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.9146
2022-03-11 18:50:56 - train: epoch 051, train_loss: 1.9786
2022-03-11 18:52:11 - eval: epoch: 051, acc1: 59.618%, acc5: 82.780%, test_loss: 1.6801, per_image_load_time: 1.588ms, per_image_inference_time: 0.138ms
2022-03-11 18:52:11 - until epoch: 051, best_acc1: 60.038%
2022-03-11 18:52:11 - epoch 052 lr: 0.010000000000000002
2022-03-11 18:52:50 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 2.1319
2022-03-11 18:53:24 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 2.0526
2022-03-11 18:53:56 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 2.0564
2022-03-11 18:54:27 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 2.1803
2022-03-11 18:55:00 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.9962
2022-03-11 18:55:34 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.9298
2022-03-11 18:56:06 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 2.1431
2022-03-11 18:56:39 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.7861
2022-03-11 18:57:11 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 2.0919
2022-03-11 18:57:44 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 2.1235
2022-03-11 18:58:17 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.9181
2022-03-11 18:58:50 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.8482
2022-03-11 18:59:23 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.7968
2022-03-11 18:59:56 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 2.2165
2022-03-11 19:00:30 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.8726
2022-03-11 19:01:03 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.7698
2022-03-11 19:01:36 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.9092
2022-03-11 19:02:09 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.9476
2022-03-11 19:02:43 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.8468
2022-03-11 19:03:17 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.9512
2022-03-11 19:03:49 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 2.0367
2022-03-11 19:04:24 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 2.1076
2022-03-11 19:04:56 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.9320
2022-03-11 19:05:30 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.6894
2022-03-11 19:06:03 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 2.0542
2022-03-11 19:06:37 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.8096
2022-03-11 19:07:09 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.9247
2022-03-11 19:07:43 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 2.0087
2022-03-11 19:08:16 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.8556
2022-03-11 19:08:49 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.8054
2022-03-11 19:09:22 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 2.1335
2022-03-11 19:09:56 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.9764
2022-03-11 19:10:28 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.9156
2022-03-11 19:11:02 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.9959
2022-03-11 19:11:35 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.8648
2022-03-11 19:12:09 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 2.1045
2022-03-11 19:12:42 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.9680
2022-03-11 19:13:15 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.8521
2022-03-11 19:13:48 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.7569
2022-03-11 19:14:22 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 2.0608
2022-03-11 19:14:55 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.7556
2022-03-11 19:15:29 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 2.0061
2022-03-11 19:16:02 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.8964
2022-03-11 19:16:35 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 2.1442
2022-03-11 19:17:08 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.9841
2022-03-11 19:17:43 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.7804
2022-03-11 19:18:15 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.9238
2022-03-11 19:18:49 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.6290
2022-03-11 19:19:23 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.9303
2022-03-11 19:19:56 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.8296
2022-03-11 19:19:58 - train: epoch 052, train_loss: 1.9774
2022-03-11 19:21:16 - eval: epoch: 052, acc1: 60.072%, acc5: 83.074%, test_loss: 1.6628, per_image_load_time: 1.046ms, per_image_inference_time: 0.144ms
2022-03-11 19:21:16 - until epoch: 052, best_acc1: 60.072%
2022-03-11 19:21:16 - epoch 053 lr: 0.010000000000000002
2022-03-11 19:21:53 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 2.1430
2022-03-11 19:22:26 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.9468
2022-03-11 19:22:58 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.9786
2022-03-11 19:23:31 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 2.1094
2022-03-11 19:24:03 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 2.0517
2022-03-11 19:24:36 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 2.0330
2022-03-11 19:25:08 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.9688
2022-03-11 19:25:41 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 2.0156
2022-03-11 19:26:12 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.9000
2022-03-11 19:26:45 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 2.0214
2022-03-11 19:27:17 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.9994
2022-03-11 19:27:50 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.8104
2022-03-11 19:28:22 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 2.0024
2022-03-11 19:28:55 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 2.2035
2022-03-11 19:29:27 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.9245
2022-03-11 19:29:59 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 2.3329
2022-03-11 19:30:30 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 2.2256
2022-03-11 19:31:01 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.9468
2022-03-11 19:31:34 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.8591
2022-03-11 19:32:06 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 2.0829
2022-03-11 19:32:38 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 2.1684
2022-03-11 19:33:10 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.8359
2022-03-11 19:33:42 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 2.0911
2022-03-11 19:34:14 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.8617
2022-03-11 19:34:47 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 2.1895
2022-03-11 19:35:17 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 2.2063
2022-03-11 19:35:50 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.9790
2022-03-11 19:36:22 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 2.1632
2022-03-11 19:36:55 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.9795
2022-03-11 19:37:26 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.9866
2022-03-11 19:37:59 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 2.1074
2022-03-11 19:38:30 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 2.1214
2022-03-11 19:39:04 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.9875
2022-03-11 19:39:35 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.8273
2022-03-11 19:40:07 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.8955
2022-03-11 19:40:39 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 2.0485
2022-03-11 19:41:11 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 2.1868
2022-03-11 19:41:43 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 2.0043
2022-03-11 19:42:16 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 2.2931
2022-03-11 19:42:47 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.8173
2022-03-11 19:43:20 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 2.0405
2022-03-11 19:43:53 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 2.0118
2022-03-11 19:44:26 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 2.2111
2022-03-11 19:45:00 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.9323
2022-03-11 19:45:36 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 2.2009
2022-03-11 19:46:16 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.8250
2022-03-11 19:46:55 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.8385
2022-03-11 19:47:27 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 2.3243
2022-03-11 19:48:00 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.9005
2022-03-11 19:48:31 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.7898
2022-03-11 19:48:34 - train: epoch 053, train_loss: 1.9743
2022-03-11 19:49:49 - eval: epoch: 053, acc1: 59.906%, acc5: 82.912%, test_loss: 1.6639, per_image_load_time: 2.722ms, per_image_inference_time: 0.142ms
2022-03-11 19:49:49 - until epoch: 053, best_acc1: 60.072%
2022-03-11 19:49:49 - epoch 054 lr: 0.010000000000000002
2022-03-11 19:50:27 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.9224
2022-03-11 19:50:59 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 2.0189
2022-03-11 19:51:32 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.9742
2022-03-11 19:52:05 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.8429
2022-03-11 19:52:37 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 2.0975
2022-03-11 19:53:10 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 2.1367
2022-03-11 19:53:42 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 2.3700
2022-03-11 19:54:15 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 2.0650
2022-03-11 19:54:47 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.7916
2022-03-11 19:55:20 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.9123
2022-03-11 19:55:52 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.9838
2022-03-11 19:56:24 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 2.0434
2022-03-11 19:56:55 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.9552
2022-03-11 19:57:27 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 2.0520
2022-03-11 19:57:59 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.9781
2022-03-11 19:58:32 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.8666
2022-03-11 19:59:04 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.8332
2022-03-11 19:59:38 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.9128
2022-03-11 20:00:09 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 2.1374
2022-03-11 20:00:42 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.8932
2022-03-11 20:01:15 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.7902
2022-03-11 20:01:47 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.9379
2022-03-11 20:02:20 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.7460
2022-03-11 20:02:53 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 2.2369
2022-03-11 20:03:25 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.9809
2022-03-11 20:03:57 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.8394
2022-03-11 20:04:31 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.9550
2022-03-11 20:05:03 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 2.0925
2022-03-11 20:05:35 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.7013
2022-03-11 20:06:08 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 2.0826
2022-03-11 20:06:40 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 2.2314
2022-03-11 20:07:13 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 2.0970
2022-03-11 20:07:45 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 2.0646
2022-03-11 20:08:18 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.7423
2022-03-11 20:08:50 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.9028
2022-03-11 20:09:24 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 2.0717
2022-03-11 20:09:56 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.7814
2022-03-11 20:10:29 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.8697
2022-03-11 20:11:01 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 2.1460
2022-03-11 20:11:33 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.9833
2022-03-11 20:12:06 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.9190
2022-03-11 20:12:40 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 2.0361
2022-03-11 20:13:12 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 2.0572
2022-03-11 20:13:45 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.7597
2022-03-11 20:14:17 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 2.0120
2022-03-11 20:14:50 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.9254
2022-03-11 20:15:23 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 2.2589
2022-03-11 20:15:56 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 2.0829
2022-03-11 20:16:29 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.7889
2022-03-11 20:17:01 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.9790
2022-03-11 20:17:03 - train: epoch 054, train_loss: 1.9709
2022-03-11 20:18:18 - eval: epoch: 054, acc1: 59.482%, acc5: 83.084%, test_loss: 1.6662, per_image_load_time: 2.708ms, per_image_inference_time: 0.143ms
2022-03-11 20:18:18 - until epoch: 054, best_acc1: 60.072%
2022-03-11 20:18:18 - epoch 055 lr: 0.010000000000000002
2022-03-11 20:18:56 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 2.0495
2022-03-11 20:19:29 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 2.1151
2022-03-11 20:20:01 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.7879
2022-03-11 20:20:34 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.8765
2022-03-11 20:21:07 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.7149
2022-03-11 20:21:40 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 2.0814
2022-03-11 20:22:12 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.8276
2022-03-11 20:22:45 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.8222
2022-03-11 20:23:17 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.9822
2022-03-11 20:23:50 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.9284
2022-03-11 20:24:23 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.9749
2022-03-11 20:24:56 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.8470
2022-03-11 20:25:28 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.9860
2022-03-11 20:26:02 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.9246
2022-03-11 20:26:34 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.9298
2022-03-11 20:27:07 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.9613
2022-03-11 20:27:39 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 2.2296
2022-03-11 20:28:12 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 2.0288
2022-03-11 20:28:44 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 2.3191
2022-03-11 20:29:17 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.8729
2022-03-11 20:29:49 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.7842
2022-03-11 20:30:21 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.9990
2022-03-11 20:30:54 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.8199
2022-03-11 20:31:26 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.9988
2022-03-11 20:31:59 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 2.0697
2022-03-11 20:32:31 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.7891
2022-03-11 20:33:04 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.8981
2022-03-11 20:33:36 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 2.0525
2022-03-11 20:34:08 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 2.0465
2022-03-11 20:34:40 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 2.0847
2022-03-11 20:35:13 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.9123
2022-03-11 20:35:45 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.9147
2022-03-11 20:36:18 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.7198
2022-03-11 20:36:48 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 2.0136
2022-03-11 20:37:22 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.9823
2022-03-11 20:37:54 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 2.0522
2022-03-11 20:38:27 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.9412
2022-03-11 20:38:59 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.9470
2022-03-11 20:39:32 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 2.1503
2022-03-11 20:40:04 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.8918
2022-03-11 20:40:37 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.9073
2022-03-11 20:41:09 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 2.1485
2022-03-11 20:41:41 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 2.0495
2022-03-11 20:42:13 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 2.1021
2022-03-11 20:42:46 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.7680
2022-03-11 20:43:19 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.7766
2022-03-11 20:43:52 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.9628
2022-03-11 20:44:24 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 2.0501
2022-03-11 20:44:57 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 2.0580
2022-03-11 20:45:28 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.8830
2022-03-11 20:45:31 - train: epoch 055, train_loss: 1.9686
2022-03-11 20:46:45 - eval: epoch: 055, acc1: 59.812%, acc5: 83.184%, test_loss: 1.6559, per_image_load_time: 1.210ms, per_image_inference_time: 0.152ms
2022-03-11 20:46:45 - until epoch: 055, best_acc1: 60.072%
2022-03-11 20:46:45 - epoch 056 lr: 0.010000000000000002
2022-03-11 20:47:23 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.9195
2022-03-11 20:47:56 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 2.0951
2022-03-11 20:48:28 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.8239
2022-03-11 20:49:01 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.9959
2022-03-11 20:49:35 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.9757
2022-03-11 20:50:08 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.8610
2022-03-11 20:50:41 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.9603
2022-03-11 20:51:16 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 2.0462
2022-03-11 20:51:49 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 2.0219
2022-03-11 20:52:23 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.7720
2022-03-11 20:52:56 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.7077
2022-03-11 20:53:30 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.9160
2022-03-11 20:54:03 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 2.0552
2022-03-11 20:54:37 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.9014
2022-03-11 20:55:10 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 2.4261
2022-03-11 20:55:45 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.8447
2022-03-11 20:56:18 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 2.0865
2022-03-11 20:56:53 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 2.2524
2022-03-11 20:57:26 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.9375
2022-03-11 20:57:59 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.9996
2022-03-11 20:58:31 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 2.0313
2022-03-11 20:59:05 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 2.2218
2022-03-11 20:59:39 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.8749
2022-03-11 21:00:14 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 2.0117
2022-03-11 21:00:47 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 2.2377
2022-03-11 21:01:23 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.9013
2022-03-11 21:01:56 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.9723
2022-03-11 21:02:30 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.8580
2022-03-11 21:03:04 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.9602
2022-03-11 21:03:39 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 2.0966
2022-03-11 21:04:13 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.7910
2022-03-11 21:04:47 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 2.0022
2022-03-11 21:05:21 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.9577
2022-03-11 21:05:55 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 2.0492
2022-03-11 21:06:29 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.9170
2022-03-11 21:07:04 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.8570
2022-03-11 21:07:37 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.8916
2022-03-11 21:08:12 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 2.0792
2022-03-11 21:08:47 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 2.0908
2022-03-11 21:09:20 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.9982
2022-03-11 21:09:55 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 2.1135
2022-03-11 21:10:29 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.9253
2022-03-11 21:11:03 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.9837
2022-03-11 21:11:37 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 2.0349
2022-03-11 21:12:12 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 2.0479
2022-03-11 21:12:46 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.9727
2022-03-11 21:13:21 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 2.0089
2022-03-11 21:13:55 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 2.0267
2022-03-11 21:14:30 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 2.0919
2022-03-11 21:15:03 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.8020
2022-03-11 21:15:06 - train: epoch 056, train_loss: 1.9677
2022-03-11 21:16:22 - eval: epoch: 056, acc1: 60.080%, acc5: 83.244%, test_loss: 1.6494, per_image_load_time: 1.701ms, per_image_inference_time: 0.165ms
2022-03-11 21:16:22 - until epoch: 056, best_acc1: 60.080%
2022-03-11 21:16:22 - epoch 057 lr: 0.010000000000000002
2022-03-11 21:17:01 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 2.0779
2022-03-11 21:17:35 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 2.0529
2022-03-11 21:18:09 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 2.2049
2022-03-11 21:18:41 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.8667
2022-03-11 21:19:16 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.8805
2022-03-11 21:19:48 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 2.1276
2022-03-11 21:20:22 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.7324
2022-03-11 21:20:55 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.7838
2022-03-11 21:21:28 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 2.0466
2022-03-11 21:22:01 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.7425
2022-03-11 21:22:36 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.7517
2022-03-11 21:23:09 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 2.0393
2022-03-11 21:23:42 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 2.0511
2022-03-11 21:24:16 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 2.0531
2022-03-11 21:24:50 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 2.1226
2022-03-11 21:25:23 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.9145
2022-03-11 21:25:57 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.9784
2022-03-11 21:26:31 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 2.1083
2022-03-11 21:27:06 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 2.0343
2022-03-11 21:27:39 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.9057
2022-03-11 21:28:13 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.8860
2022-03-11 21:28:45 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 2.0541
2022-03-11 21:29:19 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.9575
2022-03-11 21:29:50 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.9695
2022-03-11 21:30:24 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 2.3458
2022-03-11 21:30:57 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.9555
2022-03-11 21:31:29 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.8394
2022-03-11 21:32:02 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.8029
2022-03-11 21:32:34 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 2.0109
2022-03-11 21:33:07 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 2.1596
2022-03-11 21:33:40 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 2.1192
2022-03-11 21:34:12 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 2.1397
2022-03-11 21:34:45 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 2.0737
2022-03-11 21:35:17 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 2.0439
2022-03-11 21:35:51 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 2.1650
2022-03-11 21:36:24 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 2.1642
2022-03-11 21:36:56 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.7491
2022-03-11 21:37:28 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 2.0014
2022-03-11 21:38:01 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 2.1985
2022-03-11 21:38:33 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.8207
2022-03-11 21:39:07 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 2.1270
2022-03-11 21:39:39 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.9479
2022-03-11 21:40:11 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.7927
2022-03-11 21:40:45 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 2.1121
2022-03-11 21:41:17 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 2.1318
2022-03-11 21:41:50 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.9470
2022-03-11 21:42:23 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 2.2330
2022-03-11 21:42:56 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 2.1739
2022-03-11 21:43:28 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 2.3321
2022-03-11 21:44:00 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.8276
2022-03-11 21:44:02 - train: epoch 057, train_loss: 1.9638
2022-03-11 21:45:17 - eval: epoch: 057, acc1: 60.320%, acc5: 83.288%, test_loss: 1.6488, per_image_load_time: 1.705ms, per_image_inference_time: 0.179ms
2022-03-11 21:45:17 - until epoch: 057, best_acc1: 60.320%
2022-03-11 21:45:17 - epoch 058 lr: 0.010000000000000002
2022-03-11 21:45:54 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.9876
2022-03-11 21:46:26 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 2.0006
2022-03-11 21:47:00 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 2.0272
2022-03-11 21:47:31 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.8213
2022-03-11 21:48:04 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.7628
2022-03-11 21:48:36 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 2.1101
2022-03-11 21:49:09 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.9465
2022-03-11 21:49:42 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 2.0241
2022-03-11 21:50:14 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.9084
2022-03-11 21:50:47 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.8548
2022-03-11 21:51:19 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.6404
2022-03-11 21:51:52 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.7680
2022-03-11 21:52:24 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 2.1045
2022-03-11 21:52:57 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 2.0153
2022-03-11 21:53:30 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 2.0195
2022-03-11 21:54:02 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.7972
2022-03-11 21:54:35 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 2.1975
2022-03-11 21:55:07 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 2.2124
2022-03-11 21:55:40 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 2.1537
2022-03-11 21:56:12 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 2.0973
2022-03-11 21:56:45 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.9960
2022-03-11 21:57:17 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.6977
2022-03-11 21:57:51 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 2.1441
2022-03-11 21:58:31 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 2.0663
2022-03-11 21:59:06 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 2.1459
2022-03-11 21:59:39 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.9260
2022-03-11 22:00:11 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 2.2292
2022-03-11 22:00:42 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.8504
2022-03-11 22:01:15 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.7991
2022-03-11 22:01:48 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 2.0262
2022-03-11 22:02:20 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.9307
2022-03-11 22:02:53 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.8355
2022-03-11 22:03:24 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.9164
2022-03-11 22:03:57 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.9881
2022-03-11 22:04:29 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.7938
2022-03-11 22:05:02 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 2.0568
2022-03-11 22:05:34 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 2.0515
2022-03-11 22:06:06 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 2.0060
2022-03-11 22:06:38 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.8814
2022-03-11 22:07:10 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 2.3116
2022-03-11 22:07:43 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 2.0030
2022-03-11 22:08:14 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.9291
2022-03-11 22:08:47 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 2.0918
2022-03-11 22:09:19 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 2.0724
2022-03-11 22:09:51 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.8930
2022-03-11 22:10:24 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.9240
2022-03-11 22:10:56 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 2.0166
2022-03-11 22:11:29 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.9574
2022-03-11 22:12:01 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.8215
2022-03-11 22:12:33 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.8106
2022-03-11 22:12:35 - train: epoch 058, train_loss: 1.9653
2022-03-11 22:13:49 - eval: epoch: 058, acc1: 60.108%, acc5: 83.324%, test_loss: 1.6378, per_image_load_time: 2.633ms, per_image_inference_time: 0.140ms
2022-03-11 22:13:49 - until epoch: 058, best_acc1: 60.320%
2022-03-11 22:13:49 - epoch 059 lr: 0.010000000000000002
2022-03-11 22:14:27 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 2.0492
2022-03-11 22:15:00 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.7781
2022-03-11 22:15:31 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.9435
2022-03-11 22:16:02 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 2.0228
2022-03-11 22:16:36 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.8393
2022-03-11 22:17:08 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.9906
2022-03-11 22:17:40 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 2.0013
2022-03-11 22:18:12 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.8846
2022-03-11 22:18:44 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.9602
2022-03-11 22:19:15 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 2.0081
2022-03-11 22:19:47 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 2.2565
2022-03-11 22:20:20 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.8867
2022-03-11 22:20:52 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 2.0706
2022-03-11 22:21:24 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 2.2348
2022-03-11 22:21:55 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.8644
2022-03-11 22:22:27 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.9207
2022-03-11 22:22:59 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 2.0690
2022-03-11 22:23:31 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.8904
2022-03-11 22:24:02 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.8721
2022-03-11 22:24:35 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.7736
2022-03-11 22:25:07 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 2.1361
2022-03-11 22:25:39 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.8986
2022-03-11 22:26:11 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.9549
2022-03-11 22:26:43 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 2.1353
2022-03-11 22:27:15 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 2.1737
2022-03-11 22:27:48 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.8795
2022-03-11 22:28:18 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.9355
2022-03-11 22:28:52 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.9838
2022-03-11 22:29:23 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.8113
2022-03-11 22:29:56 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 2.1973
2022-03-11 22:30:28 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.8081
2022-03-11 22:31:00 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 2.0344
2022-03-11 22:31:30 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 2.0236
2022-03-11 22:32:03 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 2.2448
2022-03-11 22:32:34 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.7072
2022-03-11 22:33:07 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 2.1166
2022-03-11 22:33:39 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 2.0465
2022-03-11 22:34:11 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.8827
2022-03-11 22:34:43 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.6453
2022-03-11 22:35:15 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 2.1526
2022-03-11 22:35:47 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.7754
2022-03-11 22:36:19 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.8721
2022-03-11 22:36:51 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 2.1453
2022-03-11 22:37:23 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.7836
2022-03-11 22:37:56 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.8388
2022-03-11 22:38:27 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 2.0895
2022-03-11 22:39:00 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.9525
2022-03-11 22:39:31 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.8845
2022-03-11 22:40:04 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.9871
2022-03-11 22:40:35 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 2.2959
2022-03-11 22:40:38 - train: epoch 059, train_loss: 1.9606
2022-03-11 22:41:51 - eval: epoch: 059, acc1: 59.590%, acc5: 83.052%, test_loss: 1.6595, per_image_load_time: 2.673ms, per_image_inference_time: 0.152ms
2022-03-11 22:41:52 - until epoch: 059, best_acc1: 60.320%
2022-03-11 22:41:52 - epoch 060 lr: 0.010000000000000002
2022-03-11 22:42:30 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.8930
2022-03-11 22:43:02 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 2.0362
2022-03-11 22:43:33 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 2.0038
2022-03-11 22:44:06 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 2.0543
2022-03-11 22:44:39 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 2.1192
2022-03-11 22:45:12 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 2.0586
2022-03-11 22:45:44 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 2.0175
2022-03-11 22:46:17 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 2.1049
2022-03-11 22:46:49 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.7501
2022-03-11 22:47:22 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.6349
2022-03-11 22:47:55 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.9338
2022-03-11 22:48:29 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.8994
2022-03-11 22:49:01 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.8298
2022-03-11 22:49:33 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 2.0073
2022-03-11 22:50:06 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.8682
2022-03-11 22:50:39 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.9808
2022-03-11 22:51:11 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 2.0816
2022-03-11 22:51:44 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 2.0147
2022-03-11 22:52:17 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 2.1038
2022-03-11 22:52:50 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.9075
2022-03-11 22:53:22 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.9878
2022-03-11 22:53:55 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 2.0091
2022-03-11 22:54:28 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.7967
2022-03-11 22:55:00 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.8482
2022-03-11 22:55:33 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 2.0062
2022-03-11 22:56:05 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.9335
2022-03-11 22:56:38 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.9320
2022-03-11 22:57:10 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.8754
2022-03-11 22:57:43 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.9884
2022-03-11 22:58:15 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 2.0391
2022-03-11 22:58:48 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 2.0886
2022-03-11 22:59:19 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.8532
2022-03-11 22:59:52 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.8749
2022-03-11 23:00:24 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.9196
2022-03-11 23:00:57 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 2.1132
2022-03-11 23:01:28 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 2.0544
2022-03-11 23:02:01 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 2.0397
2022-03-11 23:02:33 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.9027
2022-03-11 23:03:06 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.9522
2022-03-11 23:03:38 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.8360
2022-03-11 23:04:11 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 2.2202
2022-03-11 23:04:42 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 2.0760
2022-03-11 23:05:15 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.8861
2022-03-11 23:05:47 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 2.1353
2022-03-11 23:06:20 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 2.0874
2022-03-11 23:06:52 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.8245
2022-03-11 23:07:25 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.8587
2022-03-11 23:07:57 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.7695
2022-03-11 23:08:30 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 2.0151
2022-03-11 23:09:01 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.9715
2022-03-11 23:09:03 - train: epoch 060, train_loss: 1.9576
2022-03-11 23:10:17 - eval: epoch: 060, acc1: 59.504%, acc5: 83.006%, test_loss: 1.6760, per_image_load_time: 2.703ms, per_image_inference_time: 0.142ms
2022-03-11 23:10:17 - until epoch: 060, best_acc1: 60.320%
2022-03-11 23:10:17 - epoch 061 lr: 0.0010000000000000002
2022-03-11 23:10:54 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.7081
2022-03-11 23:11:28 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.8889
2022-03-11 23:12:00 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.8153
