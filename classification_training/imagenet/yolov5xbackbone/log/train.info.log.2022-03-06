2022-03-04 08:25:30 - network: yolov5xbackbone
2022-03-04 08:25:30 - num_classes: 1000
2022-03-04 08:25:30 - input_image_size: 256
2022-03-04 08:25:30 - scale: 1.1428571428571428
2022-03-04 08:25:30 - trained_model_path: 
2022-03-04 08:25:30 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-04 08:25:30 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fbdcdbe8970>
2022-03-04 08:25:30 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fbdcdbe8c40>
2022-03-04 08:25:30 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fbdcdbe8c70>
2022-03-04 08:25:30 - seed: 0
2022-03-04 08:25:30 - batch_size: 128
2022-03-04 08:25:30 - num_workers: 16
2022-03-04 08:25:30 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-04 08:25:30 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-03-04 08:25:30 - epochs: 100
2022-03-04 08:25:30 - print_interval: 100
2022-03-04 08:25:30 - distributed: True
2022-03-04 08:25:30 - sync_bn: False
2022-03-04 08:25:30 - apex: True
2022-03-04 08:25:30 - gpus_type: NVIDIA RTX A5000
2022-03-04 08:25:30 - gpus_num: 2
2022-03-04 08:25:30 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fbdad1b1870>
2022-03-04 08:25:30 - --------------------parameters--------------------
2022-03-04 08:25:30 - name: conv.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: conv.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: conv.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.conv1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.conv1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.conv1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.conv2.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.conv2.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.conv2.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.conv3.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.conv3.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.conv3.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.4.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.4.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.4.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.4.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.4.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.4.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.5.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.5.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.5.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.5.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.5.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.5.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.6.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.6.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.6.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.6.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.6.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.6.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.7.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.7.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.7.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.7.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.7.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.7.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.2.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.2.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.2.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.conv1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.conv1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.conv1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.conv2.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.conv2.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.conv2.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.conv3.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.conv3.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.conv3.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.4.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.4.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.5.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.5.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.6.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.6.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.6.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.6.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.6.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.6.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.7.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.7.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.7.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.7.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.7.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.7.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.8.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.8.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.8.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.8.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.8.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.8.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.9.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.9.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.9.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.9.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.9.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.9.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.10.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.10.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.10.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.10.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.10.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.10.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.11.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.11.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.11.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.11.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.11.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.11.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.4.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.4.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.4.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.conv1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.conv1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.conv1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.conv2.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.conv2.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.conv2.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.conv3.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.conv3.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.conv3.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: middle_layers.6.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.6.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: middle_layers.6.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: sppf.conv1.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: sppf.conv1.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: sppf.conv1.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: sppf.conv2.layer.0.weight, grad: True
2022-03-04 08:25:30 - name: sppf.conv2.layer.1.weight, grad: True
2022-03-04 08:25:30 - name: sppf.conv2.layer.1.bias, grad: True
2022-03-04 08:25:30 - name: fc.weight, grad: True
2022-03-04 08:25:30 - name: fc.bias, grad: True
2022-03-04 08:25:30 - --------------------buffers--------------------
2022-03-04 08:25:30 - name: conv.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: conv.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: conv.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.conv1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.conv1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.conv2.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.conv2.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.conv3.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.conv3.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.4.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.4.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.4.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.4.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.5.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.5.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.5.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.5.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.6.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.6.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.6.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.6.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.7.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.7.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.7.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.7.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.1.bottlenecks.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.2.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.2.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.2.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.conv1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.conv1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.conv2.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.conv2.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.conv3.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.conv3.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.6.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.6.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.6.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.6.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.7.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.7.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.7.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.7.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.8.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.8.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.8.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.8.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.8.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.8.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.9.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.9.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.9.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.9.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.9.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.9.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.10.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.10.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.10.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.10.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.10.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.10.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.11.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.11.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.11.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.11.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.11.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.3.bottlenecks.11.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.4.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.4.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.4.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.conv1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.conv1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.conv1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.conv2.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.conv2.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.conv2.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.conv3.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.conv3.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.conv3.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.5.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: middle_layers.6.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: middle_layers.6.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: middle_layers.6.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: sppf.conv1.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: sppf.conv1.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: sppf.conv1.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - name: sppf.conv2.layer.1.running_mean, grad: False
2022-03-04 08:25:30 - name: sppf.conv2.layer.1.running_var, grad: False
2022-03-04 08:25:30 - name: sppf.conv2.layer.1.num_batches_tracked, grad: False
2022-03-04 08:25:30 - epoch 001 lr: 0.1
2022-03-04 08:26:06 - train: epoch 0001, iter [00100, 10009], lr: 0.100000, loss: 7.1960
2022-03-04 08:26:39 - train: epoch 0001, iter [00200, 10009], lr: 0.100000, loss: 7.0046
2022-03-04 08:27:11 - train: epoch 0001, iter [00300, 10009], lr: 0.100000, loss: 6.9415
2022-03-04 08:27:43 - train: epoch 0001, iter [00400, 10009], lr: 0.100000, loss: 6.9158
2022-03-04 08:28:15 - train: epoch 0001, iter [00500, 10009], lr: 0.100000, loss: 6.8978
2022-03-04 08:28:48 - train: epoch 0001, iter [00600, 10009], lr: 0.100000, loss: 6.8805
2022-03-04 08:29:20 - train: epoch 0001, iter [00700, 10009], lr: 0.100000, loss: 6.8908
2022-03-04 08:29:53 - train: epoch 0001, iter [00800, 10009], lr: 0.100000, loss: 6.8836
2022-03-04 08:30:25 - train: epoch 0001, iter [00900, 10009], lr: 0.100000, loss: 6.8242
2022-03-04 08:30:58 - train: epoch 0001, iter [01000, 10009], lr: 0.100000, loss: 6.8581
2022-03-04 08:31:30 - train: epoch 0001, iter [01100, 10009], lr: 0.100000, loss: 6.8058
2022-03-04 08:32:03 - train: epoch 0001, iter [01200, 10009], lr: 0.100000, loss: 6.7089
2022-03-04 08:32:35 - train: epoch 0001, iter [01300, 10009], lr: 0.100000, loss: 6.7561
2022-03-04 08:33:08 - train: epoch 0001, iter [01400, 10009], lr: 0.100000, loss: 6.7425
2022-03-04 08:33:40 - train: epoch 0001, iter [01500, 10009], lr: 0.100000, loss: 6.7634
2022-03-04 08:34:13 - train: epoch 0001, iter [01600, 10009], lr: 0.100000, loss: 6.6655
2022-03-04 08:34:46 - train: epoch 0001, iter [01700, 10009], lr: 0.100000, loss: 6.5341
2022-03-04 08:35:18 - train: epoch 0001, iter [01800, 10009], lr: 0.100000, loss: 6.5350
2022-03-04 08:35:51 - train: epoch 0001, iter [01900, 10009], lr: 0.100000, loss: 6.5907
2022-03-04 08:36:24 - train: epoch 0001, iter [02000, 10009], lr: 0.100000, loss: 6.4295
2022-03-04 08:36:56 - train: epoch 0001, iter [02100, 10009], lr: 0.100000, loss: 6.4023
2022-03-04 08:37:29 - train: epoch 0001, iter [02200, 10009], lr: 0.100000, loss: 6.5111
2022-03-04 08:38:01 - train: epoch 0001, iter [02300, 10009], lr: 0.100000, loss: 6.4538
2022-03-04 08:38:34 - train: epoch 0001, iter [02400, 10009], lr: 0.100000, loss: 6.2313
2022-03-04 08:39:06 - train: epoch 0001, iter [02500, 10009], lr: 0.100000, loss: 6.4178
2022-03-04 08:39:39 - train: epoch 0001, iter [02600, 10009], lr: 0.100000, loss: 6.2081
2022-03-04 08:40:12 - train: epoch 0001, iter [02700, 10009], lr: 0.100000, loss: 6.3618
2022-03-04 08:40:44 - train: epoch 0001, iter [02800, 10009], lr: 0.100000, loss: 6.0601
2022-03-04 08:41:17 - train: epoch 0001, iter [02900, 10009], lr: 0.100000, loss: 6.0758
2022-03-04 08:41:49 - train: epoch 0001, iter [03000, 10009], lr: 0.100000, loss: 6.1026
2022-03-04 08:42:22 - train: epoch 0001, iter [03100, 10009], lr: 0.100000, loss: 6.3824
2022-03-04 08:42:55 - train: epoch 0001, iter [03200, 10009], lr: 0.100000, loss: 6.1109
2022-03-04 08:43:27 - train: epoch 0001, iter [03300, 10009], lr: 0.100000, loss: 6.0770
2022-03-04 08:44:00 - train: epoch 0001, iter [03400, 10009], lr: 0.100000, loss: 6.0600
2022-03-04 08:44:32 - train: epoch 0001, iter [03500, 10009], lr: 0.100000, loss: 6.0312
2022-03-04 08:45:05 - train: epoch 0001, iter [03600, 10009], lr: 0.100000, loss: 5.8131
2022-03-04 08:45:38 - train: epoch 0001, iter [03700, 10009], lr: 0.100000, loss: 5.6313
2022-03-04 08:46:10 - train: epoch 0001, iter [03800, 10009], lr: 0.100000, loss: 5.6076
2022-03-04 08:46:43 - train: epoch 0001, iter [03900, 10009], lr: 0.100000, loss: 5.8496
2022-03-04 08:47:15 - train: epoch 0001, iter [04000, 10009], lr: 0.100000, loss: 5.6898
2022-03-04 08:47:48 - train: epoch 0001, iter [04100, 10009], lr: 0.100000, loss: 5.5764
2022-03-04 08:48:21 - train: epoch 0001, iter [04200, 10009], lr: 0.100000, loss: 5.7240
2022-03-04 08:48:53 - train: epoch 0001, iter [04300, 10009], lr: 0.100000, loss: 5.6746
2022-03-04 08:49:26 - train: epoch 0001, iter [04400, 10009], lr: 0.100000, loss: 5.6334
2022-03-04 08:49:58 - train: epoch 0001, iter [04500, 10009], lr: 0.100000, loss: 5.2808
2022-03-04 08:50:31 - train: epoch 0001, iter [04600, 10009], lr: 0.100000, loss: 5.5162
2022-03-04 08:51:04 - train: epoch 0001, iter [04700, 10009], lr: 0.100000, loss: 5.5107
2022-03-04 08:51:36 - train: epoch 0001, iter [04800, 10009], lr: 0.100000, loss: 5.2984
2022-03-04 08:52:09 - train: epoch 0001, iter [04900, 10009], lr: 0.100000, loss: 5.4301
2022-03-04 08:52:41 - train: epoch 0001, iter [05000, 10009], lr: 0.100000, loss: 5.4251
2022-03-04 08:53:14 - train: epoch 0001, iter [05100, 10009], lr: 0.100000, loss: 5.2514
2022-03-04 08:53:47 - train: epoch 0001, iter [05200, 10009], lr: 0.100000, loss: 5.5323
2022-03-04 08:54:19 - train: epoch 0001, iter [05300, 10009], lr: 0.100000, loss: 5.2664
2022-03-04 08:54:52 - train: epoch 0001, iter [05400, 10009], lr: 0.100000, loss: 5.4872
2022-03-04 08:55:24 - train: epoch 0001, iter [05500, 10009], lr: 0.100000, loss: 5.2836
2022-03-04 08:55:57 - train: epoch 0001, iter [05600, 10009], lr: 0.100000, loss: 5.1267
2022-03-04 08:56:30 - train: epoch 0001, iter [05700, 10009], lr: 0.100000, loss: 4.9539
2022-03-04 08:57:02 - train: epoch 0001, iter [05800, 10009], lr: 0.100000, loss: 5.0227
2022-03-04 08:57:35 - train: epoch 0001, iter [05900, 10009], lr: 0.100000, loss: 4.9376
2022-03-04 08:58:07 - train: epoch 0001, iter [06000, 10009], lr: 0.100000, loss: 5.2412
2022-03-04 08:58:40 - train: epoch 0001, iter [06100, 10009], lr: 0.100000, loss: 5.0979
2022-03-04 08:59:12 - train: epoch 0001, iter [06200, 10009], lr: 0.100000, loss: 5.3193
2022-03-04 08:59:45 - train: epoch 0001, iter [06300, 10009], lr: 0.100000, loss: 4.7948
2022-03-04 09:00:18 - train: epoch 0001, iter [06400, 10009], lr: 0.100000, loss: 5.2562
2022-03-04 09:00:50 - train: epoch 0001, iter [06500, 10009], lr: 0.100000, loss: 4.8318
2022-03-04 09:01:23 - train: epoch 0001, iter [06600, 10009], lr: 0.100000, loss: 4.8967
2022-03-04 09:01:55 - train: epoch 0001, iter [06700, 10009], lr: 0.100000, loss: 4.7699
2022-03-04 09:02:28 - train: epoch 0001, iter [06800, 10009], lr: 0.100000, loss: 4.9095
2022-03-04 09:03:01 - train: epoch 0001, iter [06900, 10009], lr: 0.100000, loss: 4.9814
2022-03-04 09:03:33 - train: epoch 0001, iter [07000, 10009], lr: 0.100000, loss: 4.8032
2022-03-04 09:04:06 - train: epoch 0001, iter [07100, 10009], lr: 0.100000, loss: 4.5475
2022-03-04 09:04:38 - train: epoch 0001, iter [07200, 10009], lr: 0.100000, loss: 4.8449
2022-03-04 09:05:11 - train: epoch 0001, iter [07300, 10009], lr: 0.100000, loss: 4.9008
2022-03-04 09:05:44 - train: epoch 0001, iter [07400, 10009], lr: 0.100000, loss: 4.9624
2022-03-04 09:06:17 - train: epoch 0001, iter [07500, 10009], lr: 0.100000, loss: 4.9269
2022-03-04 09:06:49 - train: epoch 0001, iter [07600, 10009], lr: 0.100000, loss: 4.8128
2022-03-04 09:07:22 - train: epoch 0001, iter [07700, 10009], lr: 0.100000, loss: 4.8711
2022-03-04 09:07:54 - train: epoch 0001, iter [07800, 10009], lr: 0.100000, loss: 4.9138
2022-03-04 09:08:27 - train: epoch 0001, iter [07900, 10009], lr: 0.100000, loss: 4.6215
2022-03-04 09:09:00 - train: epoch 0001, iter [08000, 10009], lr: 0.100000, loss: 4.6873
2022-03-04 09:09:32 - train: epoch 0001, iter [08100, 10009], lr: 0.100000, loss: 4.4471
2022-03-04 09:10:05 - train: epoch 0001, iter [08200, 10009], lr: 0.100000, loss: 5.2035
2022-03-04 09:10:38 - train: epoch 0001, iter [08300, 10009], lr: 0.100000, loss: 4.7636
2022-03-04 09:11:10 - train: epoch 0001, iter [08400, 10009], lr: 0.100000, loss: 4.8480
2022-03-04 09:11:43 - train: epoch 0001, iter [08500, 10009], lr: 0.100000, loss: 4.8913
2022-03-04 09:12:15 - train: epoch 0001, iter [08600, 10009], lr: 0.100000, loss: 4.5588
2022-03-04 09:12:48 - train: epoch 0001, iter [08700, 10009], lr: 0.100000, loss: 4.5662
2022-03-04 09:13:21 - train: epoch 0001, iter [08800, 10009], lr: 0.100000, loss: 4.5099
2022-03-04 09:13:53 - train: epoch 0001, iter [08900, 10009], lr: 0.100000, loss: 4.3916
2022-03-04 09:14:26 - train: epoch 0001, iter [09000, 10009], lr: 0.100000, loss: 4.6352
2022-03-04 09:14:59 - train: epoch 0001, iter [09100, 10009], lr: 0.100000, loss: 4.5401
2022-03-04 09:15:31 - train: epoch 0001, iter [09200, 10009], lr: 0.100000, loss: 4.8577
2022-03-04 09:16:04 - train: epoch 0001, iter [09300, 10009], lr: 0.100000, loss: 4.4389
2022-03-04 09:16:36 - train: epoch 0001, iter [09400, 10009], lr: 0.100000, loss: 4.5661
2022-03-04 09:17:09 - train: epoch 0001, iter [09500, 10009], lr: 0.100000, loss: 4.2726
2022-03-04 09:17:42 - train: epoch 0001, iter [09600, 10009], lr: 0.100000, loss: 4.6973
2022-03-04 09:18:15 - train: epoch 0001, iter [09700, 10009], lr: 0.100000, loss: 4.4956
2022-03-04 09:18:47 - train: epoch 0001, iter [09800, 10009], lr: 0.100000, loss: 4.6988
2022-03-04 09:19:20 - train: epoch 0001, iter [09900, 10009], lr: 0.100000, loss: 4.5144
2022-03-04 09:19:53 - train: epoch 0001, iter [10000, 10009], lr: 0.100000, loss: 4.5071
2022-03-04 09:19:56 - train: epoch 001, train_loss: 5.5536
2022-03-04 09:21:10 - eval: epoch: 001, acc1: 18.286%, acc5: 38.846%, test_loss: 4.2247, per_image_load_time: 1.050ms, per_image_inference_time: 1.537ms
2022-03-04 09:21:11 - until epoch: 001, best_acc1: 18.286%
2022-03-04 09:21:11 - epoch 002 lr: 0.1
2022-03-04 09:21:47 - train: epoch 0002, iter [00100, 10009], lr: 0.100000, loss: 4.4988
2022-03-04 09:22:19 - train: epoch 0002, iter [00200, 10009], lr: 0.100000, loss: 4.4099
2022-03-04 09:22:52 - train: epoch 0002, iter [00300, 10009], lr: 0.100000, loss: 4.5304
2022-03-04 09:23:24 - train: epoch 0002, iter [00400, 10009], lr: 0.100000, loss: 4.5943
2022-03-04 09:23:57 - train: epoch 0002, iter [00500, 10009], lr: 0.100000, loss: 4.5980
2022-03-04 09:24:29 - train: epoch 0002, iter [00600, 10009], lr: 0.100000, loss: 4.3354
2022-03-04 09:25:02 - train: epoch 0002, iter [00700, 10009], lr: 0.100000, loss: 4.4535
2022-03-04 09:25:34 - train: epoch 0002, iter [00800, 10009], lr: 0.100000, loss: 4.4456
2022-03-04 09:26:07 - train: epoch 0002, iter [00900, 10009], lr: 0.100000, loss: 4.1325
2022-03-04 09:26:39 - train: epoch 0002, iter [01000, 10009], lr: 0.100000, loss: 4.3585
2022-03-04 09:27:12 - train: epoch 0002, iter [01100, 10009], lr: 0.100000, loss: 4.2815
2022-03-04 09:27:44 - train: epoch 0002, iter [01200, 10009], lr: 0.100000, loss: 3.8114
2022-03-04 09:28:17 - train: epoch 0002, iter [01300, 10009], lr: 0.100000, loss: 4.2026
2022-03-04 09:28:50 - train: epoch 0002, iter [01400, 10009], lr: 0.100000, loss: 4.3073
2022-03-04 09:29:22 - train: epoch 0002, iter [01500, 10009], lr: 0.100000, loss: 4.2458
2022-03-04 09:29:55 - train: epoch 0002, iter [01600, 10009], lr: 0.100000, loss: 4.0423
2022-03-04 09:30:27 - train: epoch 0002, iter [01700, 10009], lr: 0.100000, loss: 4.1840
2022-03-04 09:31:00 - train: epoch 0002, iter [01800, 10009], lr: 0.100000, loss: 4.2522
2022-03-04 09:31:32 - train: epoch 0002, iter [01900, 10009], lr: 0.100000, loss: 4.3741
2022-03-04 09:32:05 - train: epoch 0002, iter [02000, 10009], lr: 0.100000, loss: 4.6286
2022-03-04 09:32:38 - train: epoch 0002, iter [02100, 10009], lr: 0.100000, loss: 4.1710
2022-03-04 09:33:10 - train: epoch 0002, iter [02200, 10009], lr: 0.100000, loss: 4.3190
2022-03-04 09:33:43 - train: epoch 0002, iter [02300, 10009], lr: 0.100000, loss: 4.2975
2022-03-04 09:34:15 - train: epoch 0002, iter [02400, 10009], lr: 0.100000, loss: 3.9477
2022-03-04 09:34:48 - train: epoch 0002, iter [02500, 10009], lr: 0.100000, loss: 4.0890
2022-03-04 09:35:20 - train: epoch 0002, iter [02600, 10009], lr: 0.100000, loss: 4.4818
2022-03-04 09:35:53 - train: epoch 0002, iter [02700, 10009], lr: 0.100000, loss: 4.3193
2022-03-04 09:36:26 - train: epoch 0002, iter [02800, 10009], lr: 0.100000, loss: 4.3444
2022-03-04 09:36:58 - train: epoch 0002, iter [02900, 10009], lr: 0.100000, loss: 4.1656
2022-03-04 09:37:31 - train: epoch 0002, iter [03000, 10009], lr: 0.100000, loss: 4.2569
2022-03-04 09:38:03 - train: epoch 0002, iter [03100, 10009], lr: 0.100000, loss: 4.3789
2022-03-04 09:38:36 - train: epoch 0002, iter [03200, 10009], lr: 0.100000, loss: 3.9081
2022-03-04 09:39:09 - train: epoch 0002, iter [03300, 10009], lr: 0.100000, loss: 3.9503
2022-03-04 09:39:41 - train: epoch 0002, iter [03400, 10009], lr: 0.100000, loss: 4.5713
2022-03-04 09:40:14 - train: epoch 0002, iter [03500, 10009], lr: 0.100000, loss: 4.2374
2022-03-04 09:40:46 - train: epoch 0002, iter [03600, 10009], lr: 0.100000, loss: 4.2016
2022-03-04 09:41:19 - train: epoch 0002, iter [03700, 10009], lr: 0.100000, loss: 3.9230
2022-03-04 09:41:52 - train: epoch 0002, iter [03800, 10009], lr: 0.100000, loss: 3.9715
2022-03-04 09:42:24 - train: epoch 0002, iter [03900, 10009], lr: 0.100000, loss: 3.9503
2022-03-04 09:42:57 - train: epoch 0002, iter [04000, 10009], lr: 0.100000, loss: 3.7792
2022-03-04 09:43:30 - train: epoch 0002, iter [04100, 10009], lr: 0.100000, loss: 3.8444
2022-03-04 09:44:02 - train: epoch 0002, iter [04200, 10009], lr: 0.100000, loss: 4.0777
2022-03-04 09:44:35 - train: epoch 0002, iter [04300, 10009], lr: 0.100000, loss: 3.9274
2022-03-04 09:45:08 - train: epoch 0002, iter [04400, 10009], lr: 0.100000, loss: 3.9566
2022-03-04 09:45:40 - train: epoch 0002, iter [04500, 10009], lr: 0.100000, loss: 4.4209
2022-03-04 09:46:13 - train: epoch 0002, iter [04600, 10009], lr: 0.100000, loss: 4.3217
2022-03-04 09:46:45 - train: epoch 0002, iter [04700, 10009], lr: 0.100000, loss: 3.6413
2022-03-04 09:47:18 - train: epoch 0002, iter [04800, 10009], lr: 0.100000, loss: 4.0302
2022-03-04 09:47:51 - train: epoch 0002, iter [04900, 10009], lr: 0.100000, loss: 4.1169
2022-03-04 09:48:23 - train: epoch 0002, iter [05000, 10009], lr: 0.100000, loss: 4.1062
2022-03-04 09:48:56 - train: epoch 0002, iter [05100, 10009], lr: 0.100000, loss: 3.8533
2022-03-04 09:49:29 - train: epoch 0002, iter [05200, 10009], lr: 0.100000, loss: 4.0369
2022-03-04 09:50:01 - train: epoch 0002, iter [05300, 10009], lr: 0.100000, loss: 3.7883
2022-03-04 09:50:34 - train: epoch 0002, iter [05400, 10009], lr: 0.100000, loss: 4.0852
2022-03-04 09:51:07 - train: epoch 0002, iter [05500, 10009], lr: 0.100000, loss: 4.0224
2022-03-04 09:51:39 - train: epoch 0002, iter [05600, 10009], lr: 0.100000, loss: 4.2176
2022-03-04 09:52:12 - train: epoch 0002, iter [05700, 10009], lr: 0.100000, loss: 4.1390
2022-03-04 09:52:45 - train: epoch 0002, iter [05800, 10009], lr: 0.100000, loss: 4.3231
2022-03-04 09:53:17 - train: epoch 0002, iter [05900, 10009], lr: 0.100000, loss: 3.9820
2022-03-04 09:53:50 - train: epoch 0002, iter [06000, 10009], lr: 0.100000, loss: 3.8934
2022-03-04 09:54:22 - train: epoch 0002, iter [06100, 10009], lr: 0.100000, loss: 3.7396
2022-03-04 09:54:55 - train: epoch 0002, iter [06200, 10009], lr: 0.100000, loss: 3.9579
2022-03-04 09:55:28 - train: epoch 0002, iter [06300, 10009], lr: 0.100000, loss: 3.8581
2022-03-04 09:56:00 - train: epoch 0002, iter [06400, 10009], lr: 0.100000, loss: 3.9804
2022-03-04 09:56:33 - train: epoch 0002, iter [06500, 10009], lr: 0.100000, loss: 4.2327
2022-03-04 09:57:05 - train: epoch 0002, iter [06600, 10009], lr: 0.100000, loss: 3.8273
2022-03-04 09:57:38 - train: epoch 0002, iter [06700, 10009], lr: 0.100000, loss: 4.0936
2022-03-04 09:58:11 - train: epoch 0002, iter [06800, 10009], lr: 0.100000, loss: 3.8071
2022-03-04 09:58:43 - train: epoch 0002, iter [06900, 10009], lr: 0.100000, loss: 3.8794
2022-03-04 09:59:16 - train: epoch 0002, iter [07000, 10009], lr: 0.100000, loss: 3.7752
2022-03-04 09:59:49 - train: epoch 0002, iter [07100, 10009], lr: 0.100000, loss: 4.1085
2022-03-04 10:00:21 - train: epoch 0002, iter [07200, 10009], lr: 0.100000, loss: 3.9579
2022-03-04 10:00:54 - train: epoch 0002, iter [07300, 10009], lr: 0.100000, loss: 4.2911
2022-03-04 10:01:27 - train: epoch 0002, iter [07400, 10009], lr: 0.100000, loss: 4.0321
2022-03-04 10:01:59 - train: epoch 0002, iter [07500, 10009], lr: 0.100000, loss: 3.7690
2022-03-04 10:02:32 - train: epoch 0002, iter [07600, 10009], lr: 0.100000, loss: 3.8653
2022-03-04 10:03:04 - train: epoch 0002, iter [07700, 10009], lr: 0.100000, loss: 3.9678
2022-03-04 10:03:37 - train: epoch 0002, iter [07800, 10009], lr: 0.100000, loss: 3.8720
2022-03-04 10:04:10 - train: epoch 0002, iter [07900, 10009], lr: 0.100000, loss: 3.7679
2022-03-04 10:04:42 - train: epoch 0002, iter [08000, 10009], lr: 0.100000, loss: 3.7769
2022-03-04 10:05:15 - train: epoch 0002, iter [08100, 10009], lr: 0.100000, loss: 3.8814
2022-03-04 10:05:48 - train: epoch 0002, iter [08200, 10009], lr: 0.100000, loss: 4.1534
2022-03-04 10:06:20 - train: epoch 0002, iter [08300, 10009], lr: 0.100000, loss: 3.7615
2022-03-04 10:06:53 - train: epoch 0002, iter [08400, 10009], lr: 0.100000, loss: 3.6746
2022-03-04 10:07:26 - train: epoch 0002, iter [08500, 10009], lr: 0.100000, loss: 3.7722
2022-03-04 10:07:58 - train: epoch 0002, iter [08600, 10009], lr: 0.100000, loss: 3.5918
2022-03-04 10:08:31 - train: epoch 0002, iter [08700, 10009], lr: 0.100000, loss: 3.6509
2022-03-04 10:09:04 - train: epoch 0002, iter [08800, 10009], lr: 0.100000, loss: 3.8396
2022-03-04 10:09:36 - train: epoch 0002, iter [08900, 10009], lr: 0.100000, loss: 3.9996
2022-03-04 10:10:09 - train: epoch 0002, iter [09000, 10009], lr: 0.100000, loss: 3.5898
2022-03-04 10:10:41 - train: epoch 0002, iter [09100, 10009], lr: 0.100000, loss: 3.6441
2022-03-04 10:11:14 - train: epoch 0002, iter [09200, 10009], lr: 0.100000, loss: 3.9629
2022-03-04 10:11:47 - train: epoch 0002, iter [09300, 10009], lr: 0.100000, loss: 3.8101
2022-03-04 10:12:19 - train: epoch 0002, iter [09400, 10009], lr: 0.100000, loss: 3.7046
2022-03-04 10:12:52 - train: epoch 0002, iter [09500, 10009], lr: 0.100000, loss: 3.9479
2022-03-04 10:13:24 - train: epoch 0002, iter [09600, 10009], lr: 0.100000, loss: 3.8931
2022-03-04 10:13:57 - train: epoch 0002, iter [09700, 10009], lr: 0.100000, loss: 3.8875
2022-03-04 10:14:30 - train: epoch 0002, iter [09800, 10009], lr: 0.100000, loss: 3.9286
2022-03-04 10:15:02 - train: epoch 0002, iter [09900, 10009], lr: 0.100000, loss: 3.7705
2022-03-04 10:15:35 - train: epoch 0002, iter [10000, 10009], lr: 0.100000, loss: 3.8497
2022-03-04 10:15:38 - train: epoch 002, train_loss: 4.0257
2022-03-04 10:16:52 - eval: epoch: 002, acc1: 28.372%, acc5: 53.228%, test_loss: 3.4643, per_image_load_time: 0.798ms, per_image_inference_time: 1.553ms
2022-03-04 10:16:53 - until epoch: 002, best_acc1: 28.372%
2022-03-04 10:16:53 - epoch 003 lr: 0.1
2022-03-04 10:17:29 - train: epoch 0003, iter [00100, 10009], lr: 0.100000, loss: 3.4417
2022-03-04 10:18:01 - train: epoch 0003, iter [00200, 10009], lr: 0.100000, loss: 3.6903
2022-03-04 10:18:34 - train: epoch 0003, iter [00300, 10009], lr: 0.100000, loss: 3.7241
2022-03-04 10:19:07 - train: epoch 0003, iter [00400, 10009], lr: 0.100000, loss: 3.8048
2022-03-04 10:19:39 - train: epoch 0003, iter [00500, 10009], lr: 0.100000, loss: 3.8212
2022-03-04 10:20:12 - train: epoch 0003, iter [00600, 10009], lr: 0.100000, loss: 3.4333
2022-03-04 10:20:45 - train: epoch 0003, iter [00700, 10009], lr: 0.100000, loss: 3.5493
2022-03-04 10:21:17 - train: epoch 0003, iter [00800, 10009], lr: 0.100000, loss: 3.8088
2022-03-04 10:21:50 - train: epoch 0003, iter [00900, 10009], lr: 0.100000, loss: 3.5285
2022-03-04 10:22:23 - train: epoch 0003, iter [01000, 10009], lr: 0.100000, loss: 3.8207
2022-03-04 10:22:55 - train: epoch 0003, iter [01100, 10009], lr: 0.100000, loss: 3.3331
2022-03-04 10:23:28 - train: epoch 0003, iter [01200, 10009], lr: 0.100000, loss: 3.6448
2022-03-04 10:24:01 - train: epoch 0003, iter [01300, 10009], lr: 0.100000, loss: 4.0047
2022-03-04 10:24:33 - train: epoch 0003, iter [01400, 10009], lr: 0.100000, loss: 3.6134
2022-03-04 10:25:06 - train: epoch 0003, iter [01500, 10009], lr: 0.100000, loss: 3.9026
2022-03-04 10:25:39 - train: epoch 0003, iter [01600, 10009], lr: 0.100000, loss: 3.5788
2022-03-04 10:26:11 - train: epoch 0003, iter [01700, 10009], lr: 0.100000, loss: 3.8001
2022-03-04 10:26:44 - train: epoch 0003, iter [01800, 10009], lr: 0.100000, loss: 3.7797
2022-03-04 10:27:17 - train: epoch 0003, iter [01900, 10009], lr: 0.100000, loss: 3.4443
2022-03-04 10:27:50 - train: epoch 0003, iter [02000, 10009], lr: 0.100000, loss: 3.4147
2022-03-04 10:28:22 - train: epoch 0003, iter [02100, 10009], lr: 0.100000, loss: 3.5784
2022-03-04 10:28:55 - train: epoch 0003, iter [02200, 10009], lr: 0.100000, loss: 3.5121
2022-03-04 10:29:27 - train: epoch 0003, iter [02300, 10009], lr: 0.100000, loss: 3.4497
2022-03-04 10:30:00 - train: epoch 0003, iter [02400, 10009], lr: 0.100000, loss: 3.8037
2022-03-04 10:30:33 - train: epoch 0003, iter [02500, 10009], lr: 0.100000, loss: 3.7877
2022-03-04 10:31:05 - train: epoch 0003, iter [02600, 10009], lr: 0.100000, loss: 3.4733
2022-03-04 10:31:38 - train: epoch 0003, iter [02700, 10009], lr: 0.100000, loss: 3.5510
2022-03-04 10:32:10 - train: epoch 0003, iter [02800, 10009], lr: 0.100000, loss: 3.3361
2022-03-04 10:32:43 - train: epoch 0003, iter [02900, 10009], lr: 0.100000, loss: 3.6745
2022-03-04 10:33:16 - train: epoch 0003, iter [03000, 10009], lr: 0.100000, loss: 3.9140
2022-03-04 10:33:48 - train: epoch 0003, iter [03100, 10009], lr: 0.100000, loss: 3.6667
2022-03-04 10:34:21 - train: epoch 0003, iter [03200, 10009], lr: 0.100000, loss: 3.8407
2022-03-04 10:34:54 - train: epoch 0003, iter [03300, 10009], lr: 0.100000, loss: 3.6643
2022-03-04 10:35:26 - train: epoch 0003, iter [03400, 10009], lr: 0.100000, loss: 3.4528
2022-03-04 10:35:59 - train: epoch 0003, iter [03500, 10009], lr: 0.100000, loss: 3.7193
2022-03-04 10:36:32 - train: epoch 0003, iter [03600, 10009], lr: 0.100000, loss: 3.4461
2022-03-04 10:37:04 - train: epoch 0003, iter [03700, 10009], lr: 0.100000, loss: 3.6783
2022-03-04 10:37:37 - train: epoch 0003, iter [03800, 10009], lr: 0.100000, loss: 3.5831
2022-03-04 10:38:10 - train: epoch 0003, iter [03900, 10009], lr: 0.100000, loss: 3.6863
2022-03-04 10:38:42 - train: epoch 0003, iter [04000, 10009], lr: 0.100000, loss: 3.5292
2022-03-04 10:39:15 - train: epoch 0003, iter [04100, 10009], lr: 0.100000, loss: 3.3369
2022-03-04 10:39:48 - train: epoch 0003, iter [04200, 10009], lr: 0.100000, loss: 3.4430
2022-03-04 10:40:21 - train: epoch 0003, iter [04300, 10009], lr: 0.100000, loss: 3.5703
2022-03-04 10:40:53 - train: epoch 0003, iter [04400, 10009], lr: 0.100000, loss: 4.0453
2022-03-04 10:41:26 - train: epoch 0003, iter [04500, 10009], lr: 0.100000, loss: 3.7405
2022-03-04 10:41:58 - train: epoch 0003, iter [04600, 10009], lr: 0.100000, loss: 3.5522
2022-03-04 10:42:31 - train: epoch 0003, iter [04700, 10009], lr: 0.100000, loss: 3.6782
2022-03-04 10:43:04 - train: epoch 0003, iter [04800, 10009], lr: 0.100000, loss: 3.4874
2022-03-04 10:43:36 - train: epoch 0003, iter [04900, 10009], lr: 0.100000, loss: 3.2562
2022-03-04 10:44:09 - train: epoch 0003, iter [05000, 10009], lr: 0.100000, loss: 3.4013
2022-03-04 10:44:42 - train: epoch 0003, iter [05100, 10009], lr: 0.100000, loss: 3.6789
2022-03-04 10:45:14 - train: epoch 0003, iter [05200, 10009], lr: 0.100000, loss: 3.6666
2022-03-04 10:45:47 - train: epoch 0003, iter [05300, 10009], lr: 0.100000, loss: 3.3034
2022-03-04 10:46:19 - train: epoch 0003, iter [05400, 10009], lr: 0.100000, loss: 3.8798
2022-03-04 10:46:52 - train: epoch 0003, iter [05500, 10009], lr: 0.100000, loss: 3.2551
2022-03-04 10:47:25 - train: epoch 0003, iter [05600, 10009], lr: 0.100000, loss: 3.3712
2022-03-04 10:47:57 - train: epoch 0003, iter [05700, 10009], lr: 0.100000, loss: 3.4867
2022-03-04 10:48:30 - train: epoch 0003, iter [05800, 10009], lr: 0.100000, loss: 3.3676
2022-03-04 10:49:02 - train: epoch 0003, iter [05900, 10009], lr: 0.100000, loss: 3.2180
2022-03-04 10:49:35 - train: epoch 0003, iter [06000, 10009], lr: 0.100000, loss: 3.1888
2022-03-04 10:50:07 - train: epoch 0003, iter [06100, 10009], lr: 0.100000, loss: 3.1223
2022-03-04 10:50:40 - train: epoch 0003, iter [06200, 10009], lr: 0.100000, loss: 3.8190
2022-03-04 10:51:13 - train: epoch 0003, iter [06300, 10009], lr: 0.100000, loss: 3.1743
2022-03-04 10:51:45 - train: epoch 0003, iter [06400, 10009], lr: 0.100000, loss: 3.5793
2022-03-04 10:52:18 - train: epoch 0003, iter [06500, 10009], lr: 0.100000, loss: 3.5819
2022-03-04 10:52:50 - train: epoch 0003, iter [06600, 10009], lr: 0.100000, loss: 3.5018
2022-03-04 10:53:23 - train: epoch 0003, iter [06700, 10009], lr: 0.100000, loss: 3.6511
2022-03-04 10:53:56 - train: epoch 0003, iter [06800, 10009], lr: 0.100000, loss: 3.7393
2022-03-04 10:54:28 - train: epoch 0003, iter [06900, 10009], lr: 0.100000, loss: 3.3554
2022-03-04 10:55:01 - train: epoch 0003, iter [07000, 10009], lr: 0.100000, loss: 3.3190
2022-03-04 10:55:34 - train: epoch 0003, iter [07100, 10009], lr: 0.100000, loss: 3.2381
2022-03-04 10:56:06 - train: epoch 0003, iter [07200, 10009], lr: 0.100000, loss: 3.3026
2022-03-04 10:56:39 - train: epoch 0003, iter [07300, 10009], lr: 0.100000, loss: 3.7123
2022-03-04 10:57:12 - train: epoch 0003, iter [07400, 10009], lr: 0.100000, loss: 3.7556
2022-03-04 10:57:44 - train: epoch 0003, iter [07500, 10009], lr: 0.100000, loss: 3.8101
2022-03-04 10:58:17 - train: epoch 0003, iter [07600, 10009], lr: 0.100000, loss: 3.4407
2022-03-04 10:58:50 - train: epoch 0003, iter [07700, 10009], lr: 0.100000, loss: 3.4983
2022-03-04 10:59:22 - train: epoch 0003, iter [07800, 10009], lr: 0.100000, loss: 3.7881
2022-03-04 10:59:55 - train: epoch 0003, iter [07900, 10009], lr: 0.100000, loss: 3.2794
2022-03-04 11:00:27 - train: epoch 0003, iter [08000, 10009], lr: 0.100000, loss: 3.1133
2022-03-04 11:01:00 - train: epoch 0003, iter [08100, 10009], lr: 0.100000, loss: 3.7042
2022-03-04 11:01:33 - train: epoch 0003, iter [08200, 10009], lr: 0.100000, loss: 3.4272
2022-03-04 11:02:05 - train: epoch 0003, iter [08300, 10009], lr: 0.100000, loss: 3.8317
2022-03-04 11:02:38 - train: epoch 0003, iter [08400, 10009], lr: 0.100000, loss: 3.1588
2022-03-04 11:03:11 - train: epoch 0003, iter [08500, 10009], lr: 0.100000, loss: 3.5900
2022-03-04 11:03:43 - train: epoch 0003, iter [08600, 10009], lr: 0.100000, loss: 3.3881
2022-03-04 11:04:16 - train: epoch 0003, iter [08700, 10009], lr: 0.100000, loss: 3.6650
2022-03-04 11:04:49 - train: epoch 0003, iter [08800, 10009], lr: 0.100000, loss: 3.1622
2022-03-04 11:05:21 - train: epoch 0003, iter [08900, 10009], lr: 0.100000, loss: 3.3396
2022-03-04 11:05:54 - train: epoch 0003, iter [09000, 10009], lr: 0.100000, loss: 3.4839
2022-03-04 11:06:27 - train: epoch 0003, iter [09100, 10009], lr: 0.100000, loss: 3.3851
2022-03-04 11:06:59 - train: epoch 0003, iter [09200, 10009], lr: 0.100000, loss: 3.3951
2022-03-04 11:07:32 - train: epoch 0003, iter [09300, 10009], lr: 0.100000, loss: 3.6960
2022-03-04 11:08:04 - train: epoch 0003, iter [09400, 10009], lr: 0.100000, loss: 3.5179
2022-03-04 11:08:37 - train: epoch 0003, iter [09500, 10009], lr: 0.100000, loss: 3.2818
2022-03-04 11:09:10 - train: epoch 0003, iter [09600, 10009], lr: 0.100000, loss: 3.4810
2022-03-04 11:09:42 - train: epoch 0003, iter [09700, 10009], lr: 0.100000, loss: 3.4609
2022-03-04 11:10:15 - train: epoch 0003, iter [09800, 10009], lr: 0.100000, loss: 3.6816
2022-03-04 11:10:48 - train: epoch 0003, iter [09900, 10009], lr: 0.100000, loss: 3.2253
2022-03-04 11:11:20 - train: epoch 0003, iter [10000, 10009], lr: 0.100000, loss: 3.4113
2022-03-04 11:11:23 - train: epoch 003, train_loss: 3.5409
2022-03-04 11:12:37 - eval: epoch: 003, acc1: 34.268%, acc5: 59.762%, test_loss: 3.0886, per_image_load_time: 1.168ms, per_image_inference_time: 1.554ms
2022-03-04 11:12:38 - until epoch: 003, best_acc1: 34.268%
2022-03-04 11:12:38 - epoch 004 lr: 0.1
2022-03-04 11:13:14 - train: epoch 0004, iter [00100, 10009], lr: 0.100000, loss: 3.2243
2022-03-04 11:13:46 - train: epoch 0004, iter [00200, 10009], lr: 0.100000, loss: 3.3064
2022-03-04 11:14:19 - train: epoch 0004, iter [00300, 10009], lr: 0.100000, loss: 3.3353
2022-03-04 11:14:51 - train: epoch 0004, iter [00400, 10009], lr: 0.100000, loss: 3.5050
2022-03-04 11:15:24 - train: epoch 0004, iter [00500, 10009], lr: 0.100000, loss: 3.4095
2022-03-04 11:15:56 - train: epoch 0004, iter [00600, 10009], lr: 0.100000, loss: 3.1963
2022-03-04 11:16:29 - train: epoch 0004, iter [00700, 10009], lr: 0.100000, loss: 3.3589
2022-03-04 11:17:02 - train: epoch 0004, iter [00800, 10009], lr: 0.100000, loss: 3.4476
2022-03-04 11:17:34 - train: epoch 0004, iter [00900, 10009], lr: 0.100000, loss: 3.3040
2022-03-04 11:18:07 - train: epoch 0004, iter [01000, 10009], lr: 0.100000, loss: 3.0158
2022-03-04 11:18:39 - train: epoch 0004, iter [01100, 10009], lr: 0.100000, loss: 3.4789
2022-03-04 11:19:12 - train: epoch 0004, iter [01200, 10009], lr: 0.100000, loss: 3.7092
2022-03-04 11:19:45 - train: epoch 0004, iter [01300, 10009], lr: 0.100000, loss: 3.2953
2022-03-04 11:20:17 - train: epoch 0004, iter [01400, 10009], lr: 0.100000, loss: 3.5703
2022-03-04 11:20:50 - train: epoch 0004, iter [01500, 10009], lr: 0.100000, loss: 3.5091
2022-03-04 11:21:23 - train: epoch 0004, iter [01600, 10009], lr: 0.100000, loss: 3.3126
2022-03-04 11:21:55 - train: epoch 0004, iter [01700, 10009], lr: 0.100000, loss: 3.2830
2022-03-04 11:22:28 - train: epoch 0004, iter [01800, 10009], lr: 0.100000, loss: 3.0710
2022-03-04 11:23:01 - train: epoch 0004, iter [01900, 10009], lr: 0.100000, loss: 3.4253
2022-03-04 11:23:33 - train: epoch 0004, iter [02000, 10009], lr: 0.100000, loss: 3.3578
2022-03-04 11:24:06 - train: epoch 0004, iter [02100, 10009], lr: 0.100000, loss: 3.6520
2022-03-04 11:24:39 - train: epoch 0004, iter [02200, 10009], lr: 0.100000, loss: 3.5278
2022-03-04 11:25:11 - train: epoch 0004, iter [02300, 10009], lr: 0.100000, loss: 3.7055
2022-03-04 11:25:44 - train: epoch 0004, iter [02400, 10009], lr: 0.100000, loss: 3.0016
2022-03-04 11:26:17 - train: epoch 0004, iter [02500, 10009], lr: 0.100000, loss: 3.2285
2022-03-04 11:26:49 - train: epoch 0004, iter [02600, 10009], lr: 0.100000, loss: 3.4362
2022-03-04 11:27:22 - train: epoch 0004, iter [02700, 10009], lr: 0.100000, loss: 3.2492
2022-03-04 11:27:55 - train: epoch 0004, iter [02800, 10009], lr: 0.100000, loss: 3.3071
2022-03-04 11:28:28 - train: epoch 0004, iter [02900, 10009], lr: 0.100000, loss: 3.2441
2022-03-04 11:29:00 - train: epoch 0004, iter [03000, 10009], lr: 0.100000, loss: 3.2320
2022-03-04 11:29:33 - train: epoch 0004, iter [03100, 10009], lr: 0.100000, loss: 3.6583
2022-03-04 11:30:05 - train: epoch 0004, iter [03200, 10009], lr: 0.100000, loss: 3.0907
2022-03-04 11:30:38 - train: epoch 0004, iter [03300, 10009], lr: 0.100000, loss: 3.5210
2022-03-04 11:31:11 - train: epoch 0004, iter [03400, 10009], lr: 0.100000, loss: 3.3269
2022-03-04 11:31:43 - train: epoch 0004, iter [03500, 10009], lr: 0.100000, loss: 3.2569
2022-03-04 11:32:16 - train: epoch 0004, iter [03600, 10009], lr: 0.100000, loss: 3.2588
2022-03-04 11:32:49 - train: epoch 0004, iter [03700, 10009], lr: 0.100000, loss: 3.1560
2022-03-04 11:33:21 - train: epoch 0004, iter [03800, 10009], lr: 0.100000, loss: 3.3384
2022-03-04 11:33:54 - train: epoch 0004, iter [03900, 10009], lr: 0.100000, loss: 3.3030
2022-03-04 11:34:26 - train: epoch 0004, iter [04000, 10009], lr: 0.100000, loss: 3.1824
2022-03-04 11:34:59 - train: epoch 0004, iter [04100, 10009], lr: 0.100000, loss: 3.5348
2022-03-04 11:35:32 - train: epoch 0004, iter [04200, 10009], lr: 0.100000, loss: 3.3318
2022-03-04 11:36:04 - train: epoch 0004, iter [04300, 10009], lr: 0.100000, loss: 3.6247
2022-03-04 11:36:37 - train: epoch 0004, iter [04400, 10009], lr: 0.100000, loss: 3.1011
2022-03-04 11:37:10 - train: epoch 0004, iter [04500, 10009], lr: 0.100000, loss: 3.5051
2022-03-04 11:37:42 - train: epoch 0004, iter [04600, 10009], lr: 0.100000, loss: 3.2094
2022-03-04 11:38:15 - train: epoch 0004, iter [04700, 10009], lr: 0.100000, loss: 3.1548
2022-03-04 11:38:48 - train: epoch 0004, iter [04800, 10009], lr: 0.100000, loss: 3.2186
2022-03-04 11:39:20 - train: epoch 0004, iter [04900, 10009], lr: 0.100000, loss: 3.4516
2022-03-04 11:39:53 - train: epoch 0004, iter [05000, 10009], lr: 0.100000, loss: 3.2952
2022-03-04 11:40:25 - train: epoch 0004, iter [05100, 10009], lr: 0.100000, loss: 3.2797
2022-03-04 11:40:58 - train: epoch 0004, iter [05200, 10009], lr: 0.100000, loss: 3.1217
2022-03-04 11:41:31 - train: epoch 0004, iter [05300, 10009], lr: 0.100000, loss: 3.3319
2022-03-04 11:42:03 - train: epoch 0004, iter [05400, 10009], lr: 0.100000, loss: 3.3167
2022-03-04 11:42:36 - train: epoch 0004, iter [05500, 10009], lr: 0.100000, loss: 3.3793
2022-03-04 11:43:09 - train: epoch 0004, iter [05600, 10009], lr: 0.100000, loss: 2.8984
2022-03-04 11:43:42 - train: epoch 0004, iter [05700, 10009], lr: 0.100000, loss: 3.3751
2022-03-04 11:44:14 - train: epoch 0004, iter [05800, 10009], lr: 0.100000, loss: 3.2662
2022-03-04 11:44:47 - train: epoch 0004, iter [05900, 10009], lr: 0.100000, loss: 3.3538
2022-03-04 11:45:19 - train: epoch 0004, iter [06000, 10009], lr: 0.100000, loss: 3.3639
2022-03-04 11:45:52 - train: epoch 0004, iter [06100, 10009], lr: 0.100000, loss: 3.5252
2022-03-04 11:46:25 - train: epoch 0004, iter [06200, 10009], lr: 0.100000, loss: 3.2321
2022-03-04 11:46:58 - train: epoch 0004, iter [06300, 10009], lr: 0.100000, loss: 3.6897
2022-03-04 11:47:30 - train: epoch 0004, iter [06400, 10009], lr: 0.100000, loss: 3.3236
2022-03-04 11:48:03 - train: epoch 0004, iter [06500, 10009], lr: 0.100000, loss: 3.1774
2022-03-04 11:48:35 - train: epoch 0004, iter [06600, 10009], lr: 0.100000, loss: 3.6409
2022-03-04 11:49:08 - train: epoch 0004, iter [06700, 10009], lr: 0.100000, loss: 3.1761
2022-03-04 11:49:41 - train: epoch 0004, iter [06800, 10009], lr: 0.100000, loss: 3.3915
2022-03-04 11:50:14 - train: epoch 0004, iter [06900, 10009], lr: 0.100000, loss: 3.2322
2022-03-04 11:50:46 - train: epoch 0004, iter [07000, 10009], lr: 0.100000, loss: 3.3842
2022-03-04 11:51:19 - train: epoch 0004, iter [07100, 10009], lr: 0.100000, loss: 3.3231
2022-03-04 11:51:52 - train: epoch 0004, iter [07200, 10009], lr: 0.100000, loss: 2.9472
2022-03-04 11:52:24 - train: epoch 0004, iter [07300, 10009], lr: 0.100000, loss: 2.9466
2022-03-04 11:52:57 - train: epoch 0004, iter [07400, 10009], lr: 0.100000, loss: 3.3477
2022-03-04 11:53:30 - train: epoch 0004, iter [07500, 10009], lr: 0.100000, loss: 3.4959
2022-03-04 11:54:03 - train: epoch 0004, iter [07600, 10009], lr: 0.100000, loss: 3.2647
2022-03-04 11:54:35 - train: epoch 0004, iter [07700, 10009], lr: 0.100000, loss: 3.4352
2022-03-04 11:55:08 - train: epoch 0004, iter [07800, 10009], lr: 0.100000, loss: 3.0946
2022-03-04 11:55:41 - train: epoch 0004, iter [07900, 10009], lr: 0.100000, loss: 3.1725
2022-03-04 11:56:13 - train: epoch 0004, iter [08000, 10009], lr: 0.100000, loss: 3.1336
2022-03-04 11:56:46 - train: epoch 0004, iter [08100, 10009], lr: 0.100000, loss: 3.5984
2022-03-04 11:57:19 - train: epoch 0004, iter [08200, 10009], lr: 0.100000, loss: 3.3202
2022-03-04 11:57:52 - train: epoch 0004, iter [08300, 10009], lr: 0.100000, loss: 3.1305
2022-03-04 11:58:24 - train: epoch 0004, iter [08400, 10009], lr: 0.100000, loss: 3.6823
2022-03-04 11:58:57 - train: epoch 0004, iter [08500, 10009], lr: 0.100000, loss: 3.5398
2022-03-04 11:59:30 - train: epoch 0004, iter [08600, 10009], lr: 0.100000, loss: 3.2615
2022-03-04 12:00:02 - train: epoch 0004, iter [08700, 10009], lr: 0.100000, loss: 3.4608
2022-03-04 12:00:35 - train: epoch 0004, iter [08800, 10009], lr: 0.100000, loss: 3.2444
2022-03-04 12:01:07 - train: epoch 0004, iter [08900, 10009], lr: 0.100000, loss: 3.0608
2022-03-04 12:01:40 - train: epoch 0004, iter [09000, 10009], lr: 0.100000, loss: 2.6864
2022-03-04 12:02:13 - train: epoch 0004, iter [09100, 10009], lr: 0.100000, loss: 3.2778
2022-03-04 12:02:45 - train: epoch 0004, iter [09200, 10009], lr: 0.100000, loss: 3.4940
2022-03-04 12:03:18 - train: epoch 0004, iter [09300, 10009], lr: 0.100000, loss: 3.1438
2022-03-04 12:03:51 - train: epoch 0004, iter [09400, 10009], lr: 0.100000, loss: 3.2727
2022-03-04 12:04:23 - train: epoch 0004, iter [09500, 10009], lr: 0.100000, loss: 3.0215
2022-03-04 12:04:56 - train: epoch 0004, iter [09600, 10009], lr: 0.100000, loss: 3.0520
2022-03-04 12:05:29 - train: epoch 0004, iter [09700, 10009], lr: 0.100000, loss: 3.3160
2022-03-04 12:06:01 - train: epoch 0004, iter [09800, 10009], lr: 0.100000, loss: 3.2703
2022-03-04 12:06:34 - train: epoch 0004, iter [09900, 10009], lr: 0.100000, loss: 2.8758
2022-03-04 12:07:07 - train: epoch 0004, iter [10000, 10009], lr: 0.100000, loss: 3.2409
2022-03-04 12:07:10 - train: epoch 004, train_loss: 3.2986
2022-03-04 12:08:24 - eval: epoch: 004, acc1: 37.986%, acc5: 63.654%, test_loss: 2.8785, per_image_load_time: 1.089ms, per_image_inference_time: 1.554ms
2022-03-04 12:08:25 - until epoch: 004, best_acc1: 37.986%
2022-03-04 12:08:25 - epoch 005 lr: 0.1
2022-03-04 12:09:01 - train: epoch 0005, iter [00100, 10009], lr: 0.100000, loss: 3.3989
2022-03-04 12:09:34 - train: epoch 0005, iter [00200, 10009], lr: 0.100000, loss: 3.2371
2022-03-04 12:10:07 - train: epoch 0005, iter [00300, 10009], lr: 0.100000, loss: 3.0625
2022-03-04 12:10:40 - train: epoch 0005, iter [00400, 10009], lr: 0.100000, loss: 3.2359
2022-03-04 12:11:12 - train: epoch 0005, iter [00500, 10009], lr: 0.100000, loss: 3.2586
2022-03-04 12:11:45 - train: epoch 0005, iter [00600, 10009], lr: 0.100000, loss: 3.2066
2022-03-04 12:12:18 - train: epoch 0005, iter [00700, 10009], lr: 0.100000, loss: 3.3268
2022-03-04 12:12:51 - train: epoch 0005, iter [00800, 10009], lr: 0.100000, loss: 3.2137
2022-03-04 12:13:23 - train: epoch 0005, iter [00900, 10009], lr: 0.100000, loss: 3.2338
2022-03-04 12:13:56 - train: epoch 0005, iter [01000, 10009], lr: 0.100000, loss: 3.3353
2022-03-04 12:14:29 - train: epoch 0005, iter [01100, 10009], lr: 0.100000, loss: 2.8245
2022-03-04 12:15:02 - train: epoch 0005, iter [01200, 10009], lr: 0.100000, loss: 3.1620
2022-03-04 12:15:35 - train: epoch 0005, iter [01300, 10009], lr: 0.100000, loss: 3.5518
2022-03-04 12:16:08 - train: epoch 0005, iter [01400, 10009], lr: 0.100000, loss: 3.1891
2022-03-04 12:16:41 - train: epoch 0005, iter [01500, 10009], lr: 0.100000, loss: 3.1883
2022-03-04 12:17:13 - train: epoch 0005, iter [01600, 10009], lr: 0.100000, loss: 3.3840
2022-03-04 12:17:46 - train: epoch 0005, iter [01700, 10009], lr: 0.100000, loss: 2.7638
2022-03-04 12:18:19 - train: epoch 0005, iter [01800, 10009], lr: 0.100000, loss: 3.3299
2022-03-04 12:18:52 - train: epoch 0005, iter [01900, 10009], lr: 0.100000, loss: 3.2151
2022-03-04 12:19:25 - train: epoch 0005, iter [02000, 10009], lr: 0.100000, loss: 3.0351
2022-03-04 12:19:58 - train: epoch 0005, iter [02100, 10009], lr: 0.100000, loss: 3.4201
2022-03-04 12:20:31 - train: epoch 0005, iter [02200, 10009], lr: 0.100000, loss: 2.9512
2022-03-04 12:21:03 - train: epoch 0005, iter [02300, 10009], lr: 0.100000, loss: 3.1246
2022-03-04 12:21:36 - train: epoch 0005, iter [02400, 10009], lr: 0.100000, loss: 3.3847
2022-03-04 12:22:09 - train: epoch 0005, iter [02500, 10009], lr: 0.100000, loss: 3.2265
2022-03-04 12:22:42 - train: epoch 0005, iter [02600, 10009], lr: 0.100000, loss: 2.8809
2022-03-04 12:23:15 - train: epoch 0005, iter [02700, 10009], lr: 0.100000, loss: 3.1120
2022-03-04 12:23:47 - train: epoch 0005, iter [02800, 10009], lr: 0.100000, loss: 3.1176
2022-03-04 12:24:20 - train: epoch 0005, iter [02900, 10009], lr: 0.100000, loss: 3.5291
2022-03-04 12:24:53 - train: epoch 0005, iter [03000, 10009], lr: 0.100000, loss: 3.0806
2022-03-04 12:25:26 - train: epoch 0005, iter [03100, 10009], lr: 0.100000, loss: 3.4828
2022-03-04 12:25:59 - train: epoch 0005, iter [03200, 10009], lr: 0.100000, loss: 3.3571
2022-03-04 12:26:32 - train: epoch 0005, iter [03300, 10009], lr: 0.100000, loss: 3.7792
2022-03-04 12:27:04 - train: epoch 0005, iter [03400, 10009], lr: 0.100000, loss: 3.0448
2022-03-04 12:27:37 - train: epoch 0005, iter [03500, 10009], lr: 0.100000, loss: 3.1558
2022-03-04 12:28:10 - train: epoch 0005, iter [03600, 10009], lr: 0.100000, loss: 3.0682
2022-03-04 12:28:43 - train: epoch 0005, iter [03700, 10009], lr: 0.100000, loss: 2.9135
2022-03-04 12:29:16 - train: epoch 0005, iter [03800, 10009], lr: 0.100000, loss: 3.3079
2022-03-04 12:29:49 - train: epoch 0005, iter [03900, 10009], lr: 0.100000, loss: 3.3547
2022-03-04 12:30:21 - train: epoch 0005, iter [04000, 10009], lr: 0.100000, loss: 3.3670
2022-03-04 12:30:54 - train: epoch 0005, iter [04100, 10009], lr: 0.100000, loss: 3.2455
2022-03-04 12:31:27 - train: epoch 0005, iter [04200, 10009], lr: 0.100000, loss: 3.0768
2022-03-04 12:32:00 - train: epoch 0005, iter [04300, 10009], lr: 0.100000, loss: 3.0554
2022-03-04 12:32:33 - train: epoch 0005, iter [04400, 10009], lr: 0.100000, loss: 3.0125
2022-03-04 12:33:05 - train: epoch 0005, iter [04500, 10009], lr: 0.100000, loss: 3.3207
2022-03-04 12:33:38 - train: epoch 0005, iter [04600, 10009], lr: 0.100000, loss: 3.3847
2022-03-04 12:34:11 - train: epoch 0005, iter [04700, 10009], lr: 0.100000, loss: 3.0659
2022-03-04 12:34:44 - train: epoch 0005, iter [04800, 10009], lr: 0.100000, loss: 3.4590
2022-03-04 12:35:16 - train: epoch 0005, iter [04900, 10009], lr: 0.100000, loss: 3.0814
2022-03-04 12:35:49 - train: epoch 0005, iter [05000, 10009], lr: 0.100000, loss: 3.1665
2022-03-04 12:36:22 - train: epoch 0005, iter [05100, 10009], lr: 0.100000, loss: 3.2808
2022-03-04 12:36:55 - train: epoch 0005, iter [05200, 10009], lr: 0.100000, loss: 3.3649
2022-03-04 12:37:28 - train: epoch 0005, iter [05300, 10009], lr: 0.100000, loss: 2.6905
2022-03-04 12:38:00 - train: epoch 0005, iter [05400, 10009], lr: 0.100000, loss: 3.1386
2022-03-04 12:38:33 - train: epoch 0005, iter [05500, 10009], lr: 0.100000, loss: 3.0812
2022-03-04 12:39:06 - train: epoch 0005, iter [05600, 10009], lr: 0.100000, loss: 3.3668
2022-03-04 12:39:39 - train: epoch 0005, iter [05700, 10009], lr: 0.100000, loss: 3.1703
2022-03-04 12:40:11 - train: epoch 0005, iter [05800, 10009], lr: 0.100000, loss: 2.8594
2022-03-04 12:40:44 - train: epoch 0005, iter [05900, 10009], lr: 0.100000, loss: 3.1354
2022-03-04 12:41:17 - train: epoch 0005, iter [06000, 10009], lr: 0.100000, loss: 2.8686
2022-03-04 12:41:50 - train: epoch 0005, iter [06100, 10009], lr: 0.100000, loss: 3.4162
2022-03-04 12:42:22 - train: epoch 0005, iter [06200, 10009], lr: 0.100000, loss: 3.2419
2022-03-04 12:42:55 - train: epoch 0005, iter [06300, 10009], lr: 0.100000, loss: 2.5440
2022-03-04 12:43:28 - train: epoch 0005, iter [06400, 10009], lr: 0.100000, loss: 3.0864
2022-03-04 12:44:00 - train: epoch 0005, iter [06500, 10009], lr: 0.100000, loss: 3.1289
2022-03-04 12:44:33 - train: epoch 0005, iter [06600, 10009], lr: 0.100000, loss: 2.9878
2022-03-04 12:45:06 - train: epoch 0005, iter [06700, 10009], lr: 0.100000, loss: 3.2558
2022-03-04 12:45:39 - train: epoch 0005, iter [06800, 10009], lr: 0.100000, loss: 3.0358
2022-03-04 12:46:11 - train: epoch 0005, iter [06900, 10009], lr: 0.100000, loss: 2.8719
2022-03-04 12:46:44 - train: epoch 0005, iter [07000, 10009], lr: 0.100000, loss: 2.9018
2022-03-04 12:47:17 - train: epoch 0005, iter [07100, 10009], lr: 0.100000, loss: 3.1930
2022-03-04 12:47:50 - train: epoch 0005, iter [07200, 10009], lr: 0.100000, loss: 2.9766
2022-03-04 12:48:22 - train: epoch 0005, iter [07300, 10009], lr: 0.100000, loss: 3.3003
2022-03-04 12:48:55 - train: epoch 0005, iter [07400, 10009], lr: 0.100000, loss: 3.0471
2022-03-04 12:49:28 - train: epoch 0005, iter [07500, 10009], lr: 0.100000, loss: 2.9632
2022-03-04 12:50:01 - train: epoch 0005, iter [07600, 10009], lr: 0.100000, loss: 3.1667
2022-03-04 12:50:33 - train: epoch 0005, iter [07700, 10009], lr: 0.100000, loss: 3.2742
2022-03-04 12:51:06 - train: epoch 0005, iter [07800, 10009], lr: 0.100000, loss: 3.2719
2022-03-04 12:51:39 - train: epoch 0005, iter [07900, 10009], lr: 0.100000, loss: 3.3206
2022-03-04 12:52:12 - train: epoch 0005, iter [08000, 10009], lr: 0.100000, loss: 3.0568
2022-03-04 12:52:44 - train: epoch 0005, iter [08100, 10009], lr: 0.100000, loss: 2.9639
2022-03-04 12:53:17 - train: epoch 0005, iter [08200, 10009], lr: 0.100000, loss: 3.4079
2022-03-04 12:53:50 - train: epoch 0005, iter [08300, 10009], lr: 0.100000, loss: 2.8670
2022-03-04 12:54:22 - train: epoch 0005, iter [08400, 10009], lr: 0.100000, loss: 3.1330
2022-03-04 12:54:55 - train: epoch 0005, iter [08500, 10009], lr: 0.100000, loss: 2.8538
2022-03-04 12:55:28 - train: epoch 0005, iter [08600, 10009], lr: 0.100000, loss: 3.1091
2022-03-04 12:56:00 - train: epoch 0005, iter [08700, 10009], lr: 0.100000, loss: 3.0816
2022-03-04 12:56:33 - train: epoch 0005, iter [08800, 10009], lr: 0.100000, loss: 2.7678
2022-03-04 12:57:06 - train: epoch 0005, iter [08900, 10009], lr: 0.100000, loss: 3.0444
2022-03-04 12:57:38 - train: epoch 0005, iter [09000, 10009], lr: 0.100000, loss: 3.2142
2022-03-04 12:58:11 - train: epoch 0005, iter [09100, 10009], lr: 0.100000, loss: 3.3346
2022-03-04 12:58:44 - train: epoch 0005, iter [09200, 10009], lr: 0.100000, loss: 3.2124
2022-03-04 12:59:17 - train: epoch 0005, iter [09300, 10009], lr: 0.100000, loss: 3.2065
2022-03-04 12:59:49 - train: epoch 0005, iter [09400, 10009], lr: 0.100000, loss: 3.0572
2022-03-04 13:00:22 - train: epoch 0005, iter [09500, 10009], lr: 0.100000, loss: 2.8129
2022-03-04 13:00:55 - train: epoch 0005, iter [09600, 10009], lr: 0.100000, loss: 2.7412
2022-03-04 13:01:27 - train: epoch 0005, iter [09700, 10009], lr: 0.100000, loss: 2.6481
2022-03-04 13:02:00 - train: epoch 0005, iter [09800, 10009], lr: 0.100000, loss: 3.2861
2022-03-04 13:02:33 - train: epoch 0005, iter [09900, 10009], lr: 0.100000, loss: 3.2527
2022-03-04 13:03:06 - train: epoch 0005, iter [10000, 10009], lr: 0.100000, loss: 2.8124
2022-03-04 13:03:09 - train: epoch 005, train_loss: 3.1585
2022-03-04 13:04:24 - eval: epoch: 005, acc1: 39.894%, acc5: 66.320%, test_loss: 2.7298, per_image_load_time: 0.915ms, per_image_inference_time: 1.543ms
2022-03-04 13:04:25 - until epoch: 005, best_acc1: 39.894%
2022-03-04 13:04:25 - epoch 006 lr: 0.1
2022-03-04 13:05:01 - train: epoch 0006, iter [00100, 10009], lr: 0.100000, loss: 2.7616
2022-03-04 13:05:33 - train: epoch 0006, iter [00200, 10009], lr: 0.100000, loss: 3.0481
2022-03-04 13:06:06 - train: epoch 0006, iter [00300, 10009], lr: 0.100000, loss: 3.0186
2022-03-04 13:06:38 - train: epoch 0006, iter [00400, 10009], lr: 0.100000, loss: 3.3635
2022-03-04 13:07:11 - train: epoch 0006, iter [00500, 10009], lr: 0.100000, loss: 2.6973
2022-03-04 13:07:43 - train: epoch 0006, iter [00600, 10009], lr: 0.100000, loss: 2.9746
2022-03-04 13:08:15 - train: epoch 0006, iter [00700, 10009], lr: 0.100000, loss: 3.0652
2022-03-04 13:08:48 - train: epoch 0006, iter [00800, 10009], lr: 0.100000, loss: 3.0789
2022-03-04 13:09:20 - train: epoch 0006, iter [00900, 10009], lr: 0.100000, loss: 2.9612
2022-03-04 13:09:53 - train: epoch 0006, iter [01000, 10009], lr: 0.100000, loss: 2.9857
2022-03-04 13:10:25 - train: epoch 0006, iter [01100, 10009], lr: 0.100000, loss: 3.2538
2022-03-04 13:10:58 - train: epoch 0006, iter [01200, 10009], lr: 0.100000, loss: 2.9230
2022-03-04 13:11:30 - train: epoch 0006, iter [01300, 10009], lr: 0.100000, loss: 2.9713
2022-03-04 13:12:03 - train: epoch 0006, iter [01400, 10009], lr: 0.100000, loss: 3.2213
2022-03-04 13:12:35 - train: epoch 0006, iter [01500, 10009], lr: 0.100000, loss: 3.3098
2022-03-04 13:13:08 - train: epoch 0006, iter [01600, 10009], lr: 0.100000, loss: 3.3786
2022-03-04 13:13:41 - train: epoch 0006, iter [01700, 10009], lr: 0.100000, loss: 2.8309
2022-03-04 13:14:13 - train: epoch 0006, iter [01800, 10009], lr: 0.100000, loss: 2.8378
2022-03-04 13:14:46 - train: epoch 0006, iter [01900, 10009], lr: 0.100000, loss: 3.0430
2022-03-04 13:15:18 - train: epoch 0006, iter [02000, 10009], lr: 0.100000, loss: 3.0027
2022-03-04 13:15:51 - train: epoch 0006, iter [02100, 10009], lr: 0.100000, loss: 3.0016
2022-03-04 13:16:23 - train: epoch 0006, iter [02200, 10009], lr: 0.100000, loss: 2.9783
2022-03-04 13:16:56 - train: epoch 0006, iter [02300, 10009], lr: 0.100000, loss: 3.0582
2022-03-04 13:17:28 - train: epoch 0006, iter [02400, 10009], lr: 0.100000, loss: 3.5890
2022-03-04 13:18:01 - train: epoch 0006, iter [02500, 10009], lr: 0.100000, loss: 2.8885
2022-03-04 13:18:33 - train: epoch 0006, iter [02600, 10009], lr: 0.100000, loss: 3.0821
2022-03-04 13:19:06 - train: epoch 0006, iter [02700, 10009], lr: 0.100000, loss: 2.8851
2022-03-04 13:19:38 - train: epoch 0006, iter [02800, 10009], lr: 0.100000, loss: 2.8132
2022-03-04 13:20:11 - train: epoch 0006, iter [02900, 10009], lr: 0.100000, loss: 3.0514
2022-03-04 13:20:43 - train: epoch 0006, iter [03000, 10009], lr: 0.100000, loss: 3.4281
2022-03-04 13:21:16 - train: epoch 0006, iter [03100, 10009], lr: 0.100000, loss: 3.2541
2022-03-04 13:21:48 - train: epoch 0006, iter [03200, 10009], lr: 0.100000, loss: 2.7430
2022-03-04 13:22:21 - train: epoch 0006, iter [03300, 10009], lr: 0.100000, loss: 3.2639
2022-03-04 13:22:53 - train: epoch 0006, iter [03400, 10009], lr: 0.100000, loss: 3.6723
2022-03-04 13:23:26 - train: epoch 0006, iter [03500, 10009], lr: 0.100000, loss: 3.1429
2022-03-04 13:23:58 - train: epoch 0006, iter [03600, 10009], lr: 0.100000, loss: 3.1392
2022-03-04 13:24:31 - train: epoch 0006, iter [03700, 10009], lr: 0.100000, loss: 3.0865
2022-03-04 13:25:03 - train: epoch 0006, iter [03800, 10009], lr: 0.100000, loss: 2.9466
2022-03-04 13:25:36 - train: epoch 0006, iter [03900, 10009], lr: 0.100000, loss: 2.9543
2022-03-04 13:26:08 - train: epoch 0006, iter [04000, 10009], lr: 0.100000, loss: 3.2188
2022-03-04 13:26:41 - train: epoch 0006, iter [04100, 10009], lr: 0.100000, loss: 3.0518
2022-03-04 13:27:14 - train: epoch 0006, iter [04200, 10009], lr: 0.100000, loss: 2.9864
2022-03-04 13:27:46 - train: epoch 0006, iter [04300, 10009], lr: 0.100000, loss: 3.2065
2022-03-04 13:28:19 - train: epoch 0006, iter [04400, 10009], lr: 0.100000, loss: 2.8523
2022-03-04 13:28:51 - train: epoch 0006, iter [04500, 10009], lr: 0.100000, loss: 3.4612
2022-03-04 13:29:24 - train: epoch 0006, iter [04600, 10009], lr: 0.100000, loss: 2.9262
2022-03-04 13:29:57 - train: epoch 0006, iter [04700, 10009], lr: 0.100000, loss: 3.1511
2022-03-04 13:30:29 - train: epoch 0006, iter [04800, 10009], lr: 0.100000, loss: 2.9762
2022-03-04 13:31:02 - train: epoch 0006, iter [04900, 10009], lr: 0.100000, loss: 2.9996
2022-03-04 13:31:35 - train: epoch 0006, iter [05000, 10009], lr: 0.100000, loss: 3.0025
2022-03-04 13:32:07 - train: epoch 0006, iter [05100, 10009], lr: 0.100000, loss: 2.9742
2022-03-04 13:32:40 - train: epoch 0006, iter [05200, 10009], lr: 0.100000, loss: 2.8628
2022-03-04 13:33:13 - train: epoch 0006, iter [05300, 10009], lr: 0.100000, loss: 2.9357
2022-03-04 13:33:45 - train: epoch 0006, iter [05400, 10009], lr: 0.100000, loss: 2.9419
2022-03-04 13:34:18 - train: epoch 0006, iter [05500, 10009], lr: 0.100000, loss: 3.2356
2022-03-04 13:34:51 - train: epoch 0006, iter [05600, 10009], lr: 0.100000, loss: 2.6820
2022-03-04 13:35:23 - train: epoch 0006, iter [05700, 10009], lr: 0.100000, loss: 3.0022
2022-03-04 13:35:56 - train: epoch 0006, iter [05800, 10009], lr: 0.100000, loss: 3.0526
2022-03-04 13:36:29 - train: epoch 0006, iter [05900, 10009], lr: 0.100000, loss: 3.2155
2022-03-04 13:37:01 - train: epoch 0006, iter [06000, 10009], lr: 0.100000, loss: 3.0434
2022-03-04 13:37:34 - train: epoch 0006, iter [06100, 10009], lr: 0.100000, loss: 3.0274
2022-03-04 13:38:07 - train: epoch 0006, iter [06200, 10009], lr: 0.100000, loss: 2.8880
2022-03-04 13:38:40 - train: epoch 0006, iter [06300, 10009], lr: 0.100000, loss: 2.8111
2022-03-04 13:39:12 - train: epoch 0006, iter [06400, 10009], lr: 0.100000, loss: 3.0459
2022-03-04 13:39:45 - train: epoch 0006, iter [06500, 10009], lr: 0.100000, loss: 3.3395
2022-03-04 13:40:17 - train: epoch 0006, iter [06600, 10009], lr: 0.100000, loss: 3.0431
2022-03-04 13:40:50 - train: epoch 0006, iter [06700, 10009], lr: 0.100000, loss: 3.0585
2022-03-04 13:41:23 - train: epoch 0006, iter [06800, 10009], lr: 0.100000, loss: 3.1706
2022-03-04 13:41:56 - train: epoch 0006, iter [06900, 10009], lr: 0.100000, loss: 2.7001
2022-03-04 13:42:28 - train: epoch 0006, iter [07000, 10009], lr: 0.100000, loss: 3.2181
2022-03-04 13:43:01 - train: epoch 0006, iter [07100, 10009], lr: 0.100000, loss: 3.0290
2022-03-04 13:43:34 - train: epoch 0006, iter [07200, 10009], lr: 0.100000, loss: 3.3232
2022-03-04 13:44:06 - train: epoch 0006, iter [07300, 10009], lr: 0.100000, loss: 3.0144
2022-03-04 13:44:39 - train: epoch 0006, iter [07400, 10009], lr: 0.100000, loss: 2.9442
2022-03-04 13:45:11 - train: epoch 0006, iter [07500, 10009], lr: 0.100000, loss: 3.5327
2022-03-04 13:45:44 - train: epoch 0006, iter [07600, 10009], lr: 0.100000, loss: 3.0032
2022-03-04 13:46:17 - train: epoch 0006, iter [07700, 10009], lr: 0.100000, loss: 3.0394
2022-03-04 13:46:49 - train: epoch 0006, iter [07800, 10009], lr: 0.100000, loss: 3.2713
2022-03-04 13:47:22 - train: epoch 0006, iter [07900, 10009], lr: 0.100000, loss: 2.8927
2022-03-04 13:47:55 - train: epoch 0006, iter [08000, 10009], lr: 0.100000, loss: 3.0301
2022-03-04 13:48:28 - train: epoch 0006, iter [08100, 10009], lr: 0.100000, loss: 2.9840
2022-03-04 13:49:00 - train: epoch 0006, iter [08200, 10009], lr: 0.100000, loss: 2.7977
2022-03-04 13:49:33 - train: epoch 0006, iter [08300, 10009], lr: 0.100000, loss: 3.0305
2022-03-04 13:50:06 - train: epoch 0006, iter [08400, 10009], lr: 0.100000, loss: 3.0692
2022-03-04 13:50:39 - train: epoch 0006, iter [08500, 10009], lr: 0.100000, loss: 2.7283
2022-03-04 13:51:11 - train: epoch 0006, iter [08600, 10009], lr: 0.100000, loss: 2.8337
2022-03-04 13:51:44 - train: epoch 0006, iter [08700, 10009], lr: 0.100000, loss: 2.8697
2022-03-04 13:52:17 - train: epoch 0006, iter [08800, 10009], lr: 0.100000, loss: 3.1483
2022-03-04 13:52:50 - train: epoch 0006, iter [08900, 10009], lr: 0.100000, loss: 2.9018
2022-03-04 13:53:22 - train: epoch 0006, iter [09000, 10009], lr: 0.100000, loss: 2.8791
2022-03-04 13:53:55 - train: epoch 0006, iter [09100, 10009], lr: 0.100000, loss: 2.7249
2022-03-04 13:54:28 - train: epoch 0006, iter [09200, 10009], lr: 0.100000, loss: 3.1644
2022-03-04 13:55:00 - train: epoch 0006, iter [09300, 10009], lr: 0.100000, loss: 3.0284
2022-03-04 13:55:33 - train: epoch 0006, iter [09400, 10009], lr: 0.100000, loss: 3.1229
2022-03-04 13:56:06 - train: epoch 0006, iter [09500, 10009], lr: 0.100000, loss: 3.1640
2022-03-04 13:56:38 - train: epoch 0006, iter [09600, 10009], lr: 0.100000, loss: 2.9663
2022-03-04 13:57:11 - train: epoch 0006, iter [09700, 10009], lr: 0.100000, loss: 2.9533
2022-03-04 13:57:43 - train: epoch 0006, iter [09800, 10009], lr: 0.100000, loss: 3.1399
2022-03-04 13:58:16 - train: epoch 0006, iter [09900, 10009], lr: 0.100000, loss: 3.1104
2022-03-04 13:58:49 - train: epoch 0006, iter [10000, 10009], lr: 0.100000, loss: 2.7913
2022-03-04 13:58:52 - train: epoch 006, train_loss: 3.0663
2022-03-04 14:00:07 - eval: epoch: 006, acc1: 40.778%, acc5: 67.066%, test_loss: 2.6867, per_image_load_time: 0.997ms, per_image_inference_time: 1.558ms
2022-03-04 14:00:08 - until epoch: 006, best_acc1: 40.778%
2022-03-04 14:00:08 - epoch 007 lr: 0.1
2022-03-04 14:00:44 - train: epoch 0007, iter [00100, 10009], lr: 0.100000, loss: 2.8793
2022-03-04 14:01:16 - train: epoch 0007, iter [00200, 10009], lr: 0.100000, loss: 2.8356
2022-03-04 14:01:49 - train: epoch 0007, iter [00300, 10009], lr: 0.100000, loss: 2.6873
2022-03-04 14:02:21 - train: epoch 0007, iter [00400, 10009], lr: 0.100000, loss: 3.2788
2022-03-04 14:02:54 - train: epoch 0007, iter [00500, 10009], lr: 0.100000, loss: 2.9243
2022-03-04 14:03:26 - train: epoch 0007, iter [00600, 10009], lr: 0.100000, loss: 3.3646
2022-03-04 14:03:59 - train: epoch 0007, iter [00700, 10009], lr: 0.100000, loss: 2.9904
2022-03-04 14:04:31 - train: epoch 0007, iter [00800, 10009], lr: 0.100000, loss: 2.9138
2022-03-04 14:05:04 - train: epoch 0007, iter [00900, 10009], lr: 0.100000, loss: 2.9426
2022-03-04 14:05:36 - train: epoch 0007, iter [01000, 10009], lr: 0.100000, loss: 2.8808
2022-03-04 14:06:09 - train: epoch 0007, iter [01100, 10009], lr: 0.100000, loss: 3.0667
2022-03-04 14:06:41 - train: epoch 0007, iter [01200, 10009], lr: 0.100000, loss: 3.0590
2022-03-04 14:07:14 - train: epoch 0007, iter [01300, 10009], lr: 0.100000, loss: 2.8143
2022-03-04 14:07:46 - train: epoch 0007, iter [01400, 10009], lr: 0.100000, loss: 3.1269
2022-03-04 14:08:19 - train: epoch 0007, iter [01500, 10009], lr: 0.100000, loss: 3.6034
2022-03-04 14:08:51 - train: epoch 0007, iter [01600, 10009], lr: 0.100000, loss: 3.1587
2022-03-04 14:09:24 - train: epoch 0007, iter [01700, 10009], lr: 0.100000, loss: 3.0138
2022-03-04 14:09:56 - train: epoch 0007, iter [01800, 10009], lr: 0.100000, loss: 3.1664
2022-03-04 14:10:29 - train: epoch 0007, iter [01900, 10009], lr: 0.100000, loss: 3.0062
2022-03-04 14:11:01 - train: epoch 0007, iter [02000, 10009], lr: 0.100000, loss: 3.1963
2022-03-04 14:11:34 - train: epoch 0007, iter [02100, 10009], lr: 0.100000, loss: 3.0227
2022-03-04 14:12:06 - train: epoch 0007, iter [02200, 10009], lr: 0.100000, loss: 2.9030
2022-03-04 14:12:39 - train: epoch 0007, iter [02300, 10009], lr: 0.100000, loss: 2.9793
2022-03-04 14:13:11 - train: epoch 0007, iter [02400, 10009], lr: 0.100000, loss: 3.2541
2022-03-04 14:13:44 - train: epoch 0007, iter [02500, 10009], lr: 0.100000, loss: 3.3523
2022-03-04 14:14:17 - train: epoch 0007, iter [02600, 10009], lr: 0.100000, loss: 2.8629
2022-03-04 14:14:49 - train: epoch 0007, iter [02700, 10009], lr: 0.100000, loss: 3.1003
2022-03-04 14:15:22 - train: epoch 0007, iter [02800, 10009], lr: 0.100000, loss: 2.9545
2022-03-04 14:15:55 - train: epoch 0007, iter [02900, 10009], lr: 0.100000, loss: 2.6839
2022-03-04 14:16:28 - train: epoch 0007, iter [03000, 10009], lr: 0.100000, loss: 3.0056
2022-03-04 14:17:01 - train: epoch 0007, iter [03100, 10009], lr: 0.100000, loss: 2.9867
2022-03-04 14:17:34 - train: epoch 0007, iter [03200, 10009], lr: 0.100000, loss: 3.2942
2022-03-04 14:18:07 - train: epoch 0007, iter [03300, 10009], lr: 0.100000, loss: 2.8373
2022-03-04 14:18:40 - train: epoch 0007, iter [03400, 10009], lr: 0.100000, loss: 3.0716
2022-03-04 14:19:13 - train: epoch 0007, iter [03500, 10009], lr: 0.100000, loss: 2.7164
2022-03-04 14:19:46 - train: epoch 0007, iter [03600, 10009], lr: 0.100000, loss: 2.9307
2022-03-04 14:20:19 - train: epoch 0007, iter [03700, 10009], lr: 0.100000, loss: 3.0012
2022-03-04 14:20:52 - train: epoch 0007, iter [03800, 10009], lr: 0.100000, loss: 2.9639
2022-03-04 14:21:25 - train: epoch 0007, iter [03900, 10009], lr: 0.100000, loss: 3.0316
2022-03-04 14:21:58 - train: epoch 0007, iter [04000, 10009], lr: 0.100000, loss: 2.7018
2022-03-04 14:22:31 - train: epoch 0007, iter [04100, 10009], lr: 0.100000, loss: 2.9530
2022-03-04 14:23:04 - train: epoch 0007, iter [04200, 10009], lr: 0.100000, loss: 3.3765
2022-03-04 14:23:37 - train: epoch 0007, iter [04300, 10009], lr: 0.100000, loss: 2.4271
2022-03-04 14:24:10 - train: epoch 0007, iter [04400, 10009], lr: 0.100000, loss: 2.8321
2022-03-04 14:24:43 - train: epoch 0007, iter [04500, 10009], lr: 0.100000, loss: 2.7496
2022-03-04 14:25:16 - train: epoch 0007, iter [04600, 10009], lr: 0.100000, loss: 3.2622
2022-03-04 14:25:49 - train: epoch 0007, iter [04700, 10009], lr: 0.100000, loss: 2.9406
2022-03-04 14:26:22 - train: epoch 0007, iter [04800, 10009], lr: 0.100000, loss: 3.0690
2022-03-04 14:26:55 - train: epoch 0007, iter [04900, 10009], lr: 0.100000, loss: 2.8159
2022-03-04 14:27:28 - train: epoch 0007, iter [05000, 10009], lr: 0.100000, loss: 2.8515
2022-03-04 14:28:01 - train: epoch 0007, iter [05100, 10009], lr: 0.100000, loss: 3.4113
2022-03-04 14:28:34 - train: epoch 0007, iter [05200, 10009], lr: 0.100000, loss: 2.9539
2022-03-04 14:29:07 - train: epoch 0007, iter [05300, 10009], lr: 0.100000, loss: 2.9100
2022-03-04 14:29:40 - train: epoch 0007, iter [05400, 10009], lr: 0.100000, loss: 2.8049
2022-03-04 14:30:13 - train: epoch 0007, iter [05500, 10009], lr: 0.100000, loss: 3.2033
2022-03-04 14:30:46 - train: epoch 0007, iter [05600, 10009], lr: 0.100000, loss: 3.2849
2022-03-04 14:31:18 - train: epoch 0007, iter [05700, 10009], lr: 0.100000, loss: 3.2251
2022-03-04 14:31:51 - train: epoch 0007, iter [05800, 10009], lr: 0.100000, loss: 2.8983
2022-03-04 14:32:24 - train: epoch 0007, iter [05900, 10009], lr: 0.100000, loss: 2.6346
2022-03-04 14:32:57 - train: epoch 0007, iter [06000, 10009], lr: 0.100000, loss: 3.2097
2022-03-04 14:33:30 - train: epoch 0007, iter [06100, 10009], lr: 0.100000, loss: 2.7703
2022-03-04 14:34:03 - train: epoch 0007, iter [06200, 10009], lr: 0.100000, loss: 2.7630
2022-03-04 14:34:35 - train: epoch 0007, iter [06300, 10009], lr: 0.100000, loss: 3.0423
2022-03-04 14:35:08 - train: epoch 0007, iter [06400, 10009], lr: 0.100000, loss: 2.7082
2022-03-04 14:35:41 - train: epoch 0007, iter [06500, 10009], lr: 0.100000, loss: 2.8475
2022-03-04 14:36:14 - train: epoch 0007, iter [06600, 10009], lr: 0.100000, loss: 3.3149
2022-03-04 14:36:46 - train: epoch 0007, iter [06700, 10009], lr: 0.100000, loss: 2.7078
2022-03-04 14:37:19 - train: epoch 0007, iter [06800, 10009], lr: 0.100000, loss: 2.9098
2022-03-04 14:37:52 - train: epoch 0007, iter [06900, 10009], lr: 0.100000, loss: 3.0669
2022-03-04 14:38:24 - train: epoch 0007, iter [07000, 10009], lr: 0.100000, loss: 3.1477
2022-03-04 14:38:57 - train: epoch 0007, iter [07100, 10009], lr: 0.100000, loss: 2.8186
2022-03-04 14:39:30 - train: epoch 0007, iter [07200, 10009], lr: 0.100000, loss: 2.6864
2022-03-04 14:40:03 - train: epoch 0007, iter [07300, 10009], lr: 0.100000, loss: 2.8092
2022-03-04 14:40:35 - train: epoch 0007, iter [07400, 10009], lr: 0.100000, loss: 2.9541
2022-03-04 14:41:08 - train: epoch 0007, iter [07500, 10009], lr: 0.100000, loss: 3.3384
2022-03-04 14:41:41 - train: epoch 0007, iter [07600, 10009], lr: 0.100000, loss: 3.1709
2022-03-04 14:42:14 - train: epoch 0007, iter [07700, 10009], lr: 0.100000, loss: 2.5723
2022-03-04 14:42:47 - train: epoch 0007, iter [07800, 10009], lr: 0.100000, loss: 3.1053
2022-03-04 14:43:20 - train: epoch 0007, iter [07900, 10009], lr: 0.100000, loss: 3.0900
2022-03-04 14:43:53 - train: epoch 0007, iter [08000, 10009], lr: 0.100000, loss: 3.0276
2022-03-04 14:44:25 - train: epoch 0007, iter [08100, 10009], lr: 0.100000, loss: 2.9504
2022-03-04 14:44:58 - train: epoch 0007, iter [08200, 10009], lr: 0.100000, loss: 2.7643
2022-03-04 14:45:31 - train: epoch 0007, iter [08300, 10009], lr: 0.100000, loss: 2.9882
2022-03-04 14:46:04 - train: epoch 0007, iter [08400, 10009], lr: 0.100000, loss: 2.9396
2022-03-04 14:46:37 - train: epoch 0007, iter [08500, 10009], lr: 0.100000, loss: 3.0273
2022-03-04 14:47:10 - train: epoch 0007, iter [08600, 10009], lr: 0.100000, loss: 3.0962
2022-03-04 14:47:43 - train: epoch 0007, iter [08700, 10009], lr: 0.100000, loss: 3.1245
2022-03-04 14:48:15 - train: epoch 0007, iter [08800, 10009], lr: 0.100000, loss: 3.1608
2022-03-04 14:48:48 - train: epoch 0007, iter [08900, 10009], lr: 0.100000, loss: 3.0101
2022-03-04 14:49:21 - train: epoch 0007, iter [09000, 10009], lr: 0.100000, loss: 3.0669
2022-03-04 14:49:54 - train: epoch 0007, iter [09100, 10009], lr: 0.100000, loss: 2.8355
2022-03-04 14:50:27 - train: epoch 0007, iter [09200, 10009], lr: 0.100000, loss: 3.0733
2022-03-04 14:51:00 - train: epoch 0007, iter [09300, 10009], lr: 0.100000, loss: 2.8727
2022-03-04 14:51:33 - train: epoch 0007, iter [09400, 10009], lr: 0.100000, loss: 2.7927
2022-03-04 14:52:06 - train: epoch 0007, iter [09500, 10009], lr: 0.100000, loss: 3.4582
2022-03-04 14:52:38 - train: epoch 0007, iter [09600, 10009], lr: 0.100000, loss: 3.3569
2022-03-04 14:53:11 - train: epoch 0007, iter [09700, 10009], lr: 0.100000, loss: 3.0677
2022-03-04 14:53:44 - train: epoch 0007, iter [09800, 10009], lr: 0.100000, loss: 3.0168
2022-03-04 14:54:17 - train: epoch 0007, iter [09900, 10009], lr: 0.100000, loss: 3.0456
2022-03-04 14:54:50 - train: epoch 0007, iter [10000, 10009], lr: 0.100000, loss: 3.2300
2022-03-04 14:54:53 - train: epoch 007, train_loss: 2.9961
2022-03-04 14:56:08 - eval: epoch: 007, acc1: 41.094%, acc5: 67.434%, test_loss: 2.6622, per_image_load_time: 1.328ms, per_image_inference_time: 1.545ms
2022-03-04 14:56:09 - until epoch: 007, best_acc1: 41.094%
2022-03-04 14:56:09 - epoch 008 lr: 0.1
2022-03-04 14:56:45 - train: epoch 0008, iter [00100, 10009], lr: 0.100000, loss: 2.5303
2022-03-04 14:57:18 - train: epoch 0008, iter [00200, 10009], lr: 0.100000, loss: 3.0494
2022-03-04 14:57:51 - train: epoch 0008, iter [00300, 10009], lr: 0.100000, loss: 2.4145
2022-03-04 14:58:23 - train: epoch 0008, iter [00400, 10009], lr: 0.100000, loss: 3.0791
2022-03-04 14:58:56 - train: epoch 0008, iter [00500, 10009], lr: 0.100000, loss: 2.8687
2022-03-04 14:59:28 - train: epoch 0008, iter [00600, 10009], lr: 0.100000, loss: 2.9992
2022-03-04 15:00:01 - train: epoch 0008, iter [00700, 10009], lr: 0.100000, loss: 3.0319
2022-03-04 15:00:34 - train: epoch 0008, iter [00800, 10009], lr: 0.100000, loss: 3.0157
2022-03-04 15:01:06 - train: epoch 0008, iter [00900, 10009], lr: 0.100000, loss: 2.7407
2022-03-04 15:01:39 - train: epoch 0008, iter [01000, 10009], lr: 0.100000, loss: 3.0703
2022-03-04 15:02:12 - train: epoch 0008, iter [01100, 10009], lr: 0.100000, loss: 2.9520
2022-03-04 15:02:44 - train: epoch 0008, iter [01200, 10009], lr: 0.100000, loss: 2.9332
2022-03-04 15:03:17 - train: epoch 0008, iter [01300, 10009], lr: 0.100000, loss: 3.0333
2022-03-04 15:03:50 - train: epoch 0008, iter [01400, 10009], lr: 0.100000, loss: 2.9645
2022-03-04 15:04:22 - train: epoch 0008, iter [01500, 10009], lr: 0.100000, loss: 2.6869
2022-03-04 15:04:55 - train: epoch 0008, iter [01600, 10009], lr: 0.100000, loss: 2.6736
2022-03-04 15:05:27 - train: epoch 0008, iter [01700, 10009], lr: 0.100000, loss: 2.8287
2022-03-04 15:06:00 - train: epoch 0008, iter [01800, 10009], lr: 0.100000, loss: 2.9513
2022-03-04 15:06:32 - train: epoch 0008, iter [01900, 10009], lr: 0.100000, loss: 2.8482
2022-03-04 15:07:05 - train: epoch 0008, iter [02000, 10009], lr: 0.100000, loss: 3.1395
2022-03-04 15:07:38 - train: epoch 0008, iter [02100, 10009], lr: 0.100000, loss: 3.1074
2022-03-04 15:08:11 - train: epoch 0008, iter [02200, 10009], lr: 0.100000, loss: 3.0001
2022-03-04 15:08:43 - train: epoch 0008, iter [02300, 10009], lr: 0.100000, loss: 3.4589
2022-03-04 15:09:16 - train: epoch 0008, iter [02400, 10009], lr: 0.100000, loss: 2.9538
2022-03-04 15:09:49 - train: epoch 0008, iter [02500, 10009], lr: 0.100000, loss: 2.8772
2022-03-04 15:10:21 - train: epoch 0008, iter [02600, 10009], lr: 0.100000, loss: 2.9791
2022-03-04 15:10:54 - train: epoch 0008, iter [02700, 10009], lr: 0.100000, loss: 3.1077
2022-03-04 15:11:27 - train: epoch 0008, iter [02800, 10009], lr: 0.100000, loss: 2.6666
2022-03-04 15:12:00 - train: epoch 0008, iter [02900, 10009], lr: 0.100000, loss: 3.0622
2022-03-04 15:12:33 - train: epoch 0008, iter [03000, 10009], lr: 0.100000, loss: 2.7961
2022-03-04 15:13:06 - train: epoch 0008, iter [03100, 10009], lr: 0.100000, loss: 2.8496
2022-03-04 15:13:38 - train: epoch 0008, iter [03200, 10009], lr: 0.100000, loss: 3.1387
2022-03-04 15:14:11 - train: epoch 0008, iter [03300, 10009], lr: 0.100000, loss: 2.9581
2022-03-04 15:14:44 - train: epoch 0008, iter [03400, 10009], lr: 0.100000, loss: 2.9670
2022-03-04 15:15:17 - train: epoch 0008, iter [03500, 10009], lr: 0.100000, loss: 2.8806
2022-03-04 15:15:50 - train: epoch 0008, iter [03600, 10009], lr: 0.100000, loss: 2.9788
2022-03-04 15:16:22 - train: epoch 0008, iter [03700, 10009], lr: 0.100000, loss: 3.0166
2022-03-04 15:16:55 - train: epoch 0008, iter [03800, 10009], lr: 0.100000, loss: 3.1263
2022-03-04 15:17:28 - train: epoch 0008, iter [03900, 10009], lr: 0.100000, loss: 3.0058
2022-03-04 15:18:01 - train: epoch 0008, iter [04000, 10009], lr: 0.100000, loss: 3.3501
2022-03-04 15:18:34 - train: epoch 0008, iter [04100, 10009], lr: 0.100000, loss: 3.1964
2022-03-04 15:19:06 - train: epoch 0008, iter [04200, 10009], lr: 0.100000, loss: 2.9420
2022-03-04 15:19:39 - train: epoch 0008, iter [04300, 10009], lr: 0.100000, loss: 3.2553
2022-03-04 15:20:12 - train: epoch 0008, iter [04400, 10009], lr: 0.100000, loss: 2.7334
2022-03-04 15:20:45 - train: epoch 0008, iter [04500, 10009], lr: 0.100000, loss: 2.7442
2022-03-04 15:21:18 - train: epoch 0008, iter [04600, 10009], lr: 0.100000, loss: 3.1739
2022-03-04 15:21:51 - train: epoch 0008, iter [04700, 10009], lr: 0.100000, loss: 2.6972
2022-03-04 15:22:24 - train: epoch 0008, iter [04800, 10009], lr: 0.100000, loss: 2.8433
2022-03-04 15:22:56 - train: epoch 0008, iter [04900, 10009], lr: 0.100000, loss: 2.6859
2022-03-04 15:23:29 - train: epoch 0008, iter [05000, 10009], lr: 0.100000, loss: 2.8209
2022-03-04 15:24:01 - train: epoch 0008, iter [05100, 10009], lr: 0.100000, loss: 3.2037
2022-03-04 15:24:34 - train: epoch 0008, iter [05200, 10009], lr: 0.100000, loss: 2.7605
2022-03-04 15:25:07 - train: epoch 0008, iter [05300, 10009], lr: 0.100000, loss: 3.0904
2022-03-04 15:25:39 - train: epoch 0008, iter [05400, 10009], lr: 0.100000, loss: 3.0864
2022-03-04 15:26:12 - train: epoch 0008, iter [05500, 10009], lr: 0.100000, loss: 3.5525
2022-03-04 15:26:45 - train: epoch 0008, iter [05600, 10009], lr: 0.100000, loss: 2.8377
2022-03-04 15:27:17 - train: epoch 0008, iter [05700, 10009], lr: 0.100000, loss: 3.1035
2022-03-04 15:27:50 - train: epoch 0008, iter [05800, 10009], lr: 0.100000, loss: 2.8240
2022-03-04 15:28:23 - train: epoch 0008, iter [05900, 10009], lr: 0.100000, loss: 2.8670
2022-03-04 15:28:56 - train: epoch 0008, iter [06000, 10009], lr: 0.100000, loss: 3.0915
2022-03-04 15:29:29 - train: epoch 0008, iter [06100, 10009], lr: 0.100000, loss: 2.7534
2022-03-04 15:30:01 - train: epoch 0008, iter [06200, 10009], lr: 0.100000, loss: 2.9253
2022-03-04 15:30:34 - train: epoch 0008, iter [06300, 10009], lr: 0.100000, loss: 3.3137
2022-03-04 15:31:07 - train: epoch 0008, iter [06400, 10009], lr: 0.100000, loss: 3.1360
2022-03-04 15:31:40 - train: epoch 0008, iter [06500, 10009], lr: 0.100000, loss: 3.0777
2022-03-04 15:32:12 - train: epoch 0008, iter [06600, 10009], lr: 0.100000, loss: 3.3682
2022-03-04 15:32:45 - train: epoch 0008, iter [06700, 10009], lr: 0.100000, loss: 3.0442
2022-03-04 15:33:18 - train: epoch 0008, iter [06800, 10009], lr: 0.100000, loss: 2.9034
2022-03-04 15:33:51 - train: epoch 0008, iter [06900, 10009], lr: 0.100000, loss: 2.7796
2022-03-04 15:34:24 - train: epoch 0008, iter [07000, 10009], lr: 0.100000, loss: 3.0487
2022-03-04 15:34:56 - train: epoch 0008, iter [07100, 10009], lr: 0.100000, loss: 3.1630
2022-03-04 15:35:29 - train: epoch 0008, iter [07200, 10009], lr: 0.100000, loss: 3.1814
2022-03-04 15:36:02 - train: epoch 0008, iter [07300, 10009], lr: 0.100000, loss: 2.9948
2022-03-04 15:36:35 - train: epoch 0008, iter [07400, 10009], lr: 0.100000, loss: 2.9855
2022-03-04 15:37:08 - train: epoch 0008, iter [07500, 10009], lr: 0.100000, loss: 2.8816
2022-03-04 15:37:41 - train: epoch 0008, iter [07600, 10009], lr: 0.100000, loss: 2.6836
2022-03-04 15:38:13 - train: epoch 0008, iter [07700, 10009], lr: 0.100000, loss: 2.9781
2022-03-04 15:38:46 - train: epoch 0008, iter [07800, 10009], lr: 0.100000, loss: 3.0127
2022-03-04 15:39:19 - train: epoch 0008, iter [07900, 10009], lr: 0.100000, loss: 3.0711
2022-03-04 15:39:52 - train: epoch 0008, iter [08000, 10009], lr: 0.100000, loss: 3.5511
2022-03-04 15:40:25 - train: epoch 0008, iter [08100, 10009], lr: 0.100000, loss: 2.8847
2022-03-04 15:40:58 - train: epoch 0008, iter [08200, 10009], lr: 0.100000, loss: 2.6499
2022-03-04 15:41:31 - train: epoch 0008, iter [08300, 10009], lr: 0.100000, loss: 2.7757
2022-03-04 15:42:04 - train: epoch 0008, iter [08400, 10009], lr: 0.100000, loss: 3.1239
2022-03-04 15:42:37 - train: epoch 0008, iter [08500, 10009], lr: 0.100000, loss: 2.7418
2022-03-04 15:43:10 - train: epoch 0008, iter [08600, 10009], lr: 0.100000, loss: 2.9975
2022-03-04 15:43:43 - train: epoch 0008, iter [08700, 10009], lr: 0.100000, loss: 3.1484
2022-03-04 15:44:15 - train: epoch 0008, iter [08800, 10009], lr: 0.100000, loss: 3.1875
2022-03-04 15:44:48 - train: epoch 0008, iter [08900, 10009], lr: 0.100000, loss: 2.6537
2022-03-04 15:45:21 - train: epoch 0008, iter [09000, 10009], lr: 0.100000, loss: 3.0942
2022-03-04 15:45:54 - train: epoch 0008, iter [09100, 10009], lr: 0.100000, loss: 2.7634
2022-03-04 15:46:27 - train: epoch 0008, iter [09200, 10009], lr: 0.100000, loss: 2.8765
2022-03-04 15:47:00 - train: epoch 0008, iter [09300, 10009], lr: 0.100000, loss: 3.4077
2022-03-04 15:47:33 - train: epoch 0008, iter [09400, 10009], lr: 0.100000, loss: 2.8263
2022-03-04 15:48:05 - train: epoch 0008, iter [09500, 10009], lr: 0.100000, loss: 3.1812
2022-03-04 15:48:38 - train: epoch 0008, iter [09600, 10009], lr: 0.100000, loss: 2.8982
2022-03-04 15:49:11 - train: epoch 0008, iter [09700, 10009], lr: 0.100000, loss: 2.7108
2022-03-04 15:49:44 - train: epoch 0008, iter [09800, 10009], lr: 0.100000, loss: 3.0992
2022-03-04 15:50:17 - train: epoch 0008, iter [09900, 10009], lr: 0.100000, loss: 2.7537
2022-03-04 15:50:50 - train: epoch 0008, iter [10000, 10009], lr: 0.100000, loss: 2.9710
2022-03-04 15:50:53 - train: epoch 008, train_loss: 2.9439
2022-03-04 15:52:08 - eval: epoch: 008, acc1: 42.950%, acc5: 69.008%, test_loss: 2.5578, per_image_load_time: 1.326ms, per_image_inference_time: 1.547ms
2022-03-04 15:52:09 - until epoch: 008, best_acc1: 42.950%
2022-03-04 15:52:09 - epoch 009 lr: 0.1
2022-03-04 15:52:45 - train: epoch 0009, iter [00100, 10009], lr: 0.100000, loss: 2.6052
2022-03-04 15:53:18 - train: epoch 0009, iter [00200, 10009], lr: 0.100000, loss: 2.3210
2022-03-04 15:53:50 - train: epoch 0009, iter [00300, 10009], lr: 0.100000, loss: 2.8775
2022-03-04 15:54:23 - train: epoch 0009, iter [00400, 10009], lr: 0.100000, loss: 2.9637
2022-03-04 15:54:55 - train: epoch 0009, iter [00500, 10009], lr: 0.100000, loss: 3.3188
2022-03-04 15:55:28 - train: epoch 0009, iter [00600, 10009], lr: 0.100000, loss: 2.7202
2022-03-04 15:56:00 - train: epoch 0009, iter [00700, 10009], lr: 0.100000, loss: 3.0094
2022-03-04 15:56:33 - train: epoch 0009, iter [00800, 10009], lr: 0.100000, loss: 3.0536
2022-03-04 15:57:06 - train: epoch 0009, iter [00900, 10009], lr: 0.100000, loss: 2.9847
2022-03-04 15:57:38 - train: epoch 0009, iter [01000, 10009], lr: 0.100000, loss: 3.0554
2022-03-04 15:58:11 - train: epoch 0009, iter [01100, 10009], lr: 0.100000, loss: 2.8200
2022-03-04 15:58:44 - train: epoch 0009, iter [01200, 10009], lr: 0.100000, loss: 2.9932
2022-03-04 15:59:16 - train: epoch 0009, iter [01300, 10009], lr: 0.100000, loss: 3.2086
2022-03-04 15:59:49 - train: epoch 0009, iter [01400, 10009], lr: 0.100000, loss: 2.5948
2022-03-04 16:00:22 - train: epoch 0009, iter [01500, 10009], lr: 0.100000, loss: 2.7807
2022-03-04 16:00:54 - train: epoch 0009, iter [01600, 10009], lr: 0.100000, loss: 2.9132
2022-03-04 16:01:27 - train: epoch 0009, iter [01700, 10009], lr: 0.100000, loss: 2.8629
2022-03-04 16:02:00 - train: epoch 0009, iter [01800, 10009], lr: 0.100000, loss: 2.4159
2022-03-04 16:02:32 - train: epoch 0009, iter [01900, 10009], lr: 0.100000, loss: 2.5771
2022-03-04 16:03:05 - train: epoch 0009, iter [02000, 10009], lr: 0.100000, loss: 2.6852
2022-03-04 16:03:38 - train: epoch 0009, iter [02100, 10009], lr: 0.100000, loss: 3.1083
2022-03-04 16:04:10 - train: epoch 0009, iter [02200, 10009], lr: 0.100000, loss: 3.0391
2022-03-04 16:04:43 - train: epoch 0009, iter [02300, 10009], lr: 0.100000, loss: 2.9671
2022-03-04 16:05:16 - train: epoch 0009, iter [02400, 10009], lr: 0.100000, loss: 2.9600
2022-03-04 16:05:49 - train: epoch 0009, iter [02500, 10009], lr: 0.100000, loss: 2.6591
2022-03-04 16:06:21 - train: epoch 0009, iter [02600, 10009], lr: 0.100000, loss: 3.1562
2022-03-04 16:06:54 - train: epoch 0009, iter [02700, 10009], lr: 0.100000, loss: 2.9240
2022-03-04 16:07:26 - train: epoch 0009, iter [02800, 10009], lr: 0.100000, loss: 2.8387
2022-03-04 16:07:59 - train: epoch 0009, iter [02900, 10009], lr: 0.100000, loss: 3.2073
2022-03-04 16:08:32 - train: epoch 0009, iter [03000, 10009], lr: 0.100000, loss: 2.9344
2022-03-04 16:09:05 - train: epoch 0009, iter [03100, 10009], lr: 0.100000, loss: 3.1360
2022-03-04 16:09:37 - train: epoch 0009, iter [03200, 10009], lr: 0.100000, loss: 2.7059
2022-03-04 16:10:10 - train: epoch 0009, iter [03300, 10009], lr: 0.100000, loss: 2.7071
2022-03-04 16:10:43 - train: epoch 0009, iter [03400, 10009], lr: 0.100000, loss: 3.0772
2022-03-04 16:11:16 - train: epoch 0009, iter [03500, 10009], lr: 0.100000, loss: 2.7537
2022-03-04 16:11:49 - train: epoch 0009, iter [03600, 10009], lr: 0.100000, loss: 2.9558
2022-03-04 16:12:22 - train: epoch 0009, iter [03700, 10009], lr: 0.100000, loss: 2.4844
2022-03-04 16:12:55 - train: epoch 0009, iter [03800, 10009], lr: 0.100000, loss: 2.5513
2022-03-04 16:13:27 - train: epoch 0009, iter [03900, 10009], lr: 0.100000, loss: 3.0013
2022-03-04 16:14:00 - train: epoch 0009, iter [04000, 10009], lr: 0.100000, loss: 2.7048
2022-03-04 16:14:33 - train: epoch 0009, iter [04100, 10009], lr: 0.100000, loss: 2.8506
2022-03-04 16:15:05 - train: epoch 0009, iter [04200, 10009], lr: 0.100000, loss: 2.7777
2022-03-04 16:15:38 - train: epoch 0009, iter [04300, 10009], lr: 0.100000, loss: 2.7745
2022-03-04 16:16:11 - train: epoch 0009, iter [04400, 10009], lr: 0.100000, loss: 3.0253
2022-03-04 16:16:43 - train: epoch 0009, iter [04500, 10009], lr: 0.100000, loss: 3.2114
2022-03-04 16:17:16 - train: epoch 0009, iter [04600, 10009], lr: 0.100000, loss: 3.1477
2022-03-04 16:17:49 - train: epoch 0009, iter [04700, 10009], lr: 0.100000, loss: 2.9219
2022-03-04 16:18:22 - train: epoch 0009, iter [04800, 10009], lr: 0.100000, loss: 2.8742
2022-03-04 16:18:54 - train: epoch 0009, iter [04900, 10009], lr: 0.100000, loss: 2.6908
2022-03-04 16:19:27 - train: epoch 0009, iter [05000, 10009], lr: 0.100000, loss: 2.6548
2022-03-04 16:20:00 - train: epoch 0009, iter [05100, 10009], lr: 0.100000, loss: 2.5738
2022-03-04 16:20:33 - train: epoch 0009, iter [05200, 10009], lr: 0.100000, loss: 3.2252
2022-03-04 16:21:06 - train: epoch 0009, iter [05300, 10009], lr: 0.100000, loss: 2.8178
2022-03-04 16:21:39 - train: epoch 0009, iter [05400, 10009], lr: 0.100000, loss: 2.9089
2022-03-04 16:22:11 - train: epoch 0009, iter [05500, 10009], lr: 0.100000, loss: 3.1843
2022-03-04 16:22:44 - train: epoch 0009, iter [05600, 10009], lr: 0.100000, loss: 3.1176
2022-03-04 16:23:17 - train: epoch 0009, iter [05700, 10009], lr: 0.100000, loss: 3.1446
2022-03-04 16:23:50 - train: epoch 0009, iter [05800, 10009], lr: 0.100000, loss: 2.4482
2022-03-04 16:24:23 - train: epoch 0009, iter [05900, 10009], lr: 0.100000, loss: 3.0562
2022-03-04 16:24:55 - train: epoch 0009, iter [06000, 10009], lr: 0.100000, loss: 3.2564
2022-03-04 16:25:28 - train: epoch 0009, iter [06100, 10009], lr: 0.100000, loss: 2.7176
2022-03-04 16:26:01 - train: epoch 0009, iter [06200, 10009], lr: 0.100000, loss: 2.9430
2022-03-04 16:26:34 - train: epoch 0009, iter [06300, 10009], lr: 0.100000, loss: 2.8755
2022-03-04 16:27:07 - train: epoch 0009, iter [06400, 10009], lr: 0.100000, loss: 2.8804
2022-03-04 16:27:40 - train: epoch 0009, iter [06500, 10009], lr: 0.100000, loss: 2.8173
2022-03-04 16:28:13 - train: epoch 0009, iter [06600, 10009], lr: 0.100000, loss: 2.7746
2022-03-04 16:28:46 - train: epoch 0009, iter [06700, 10009], lr: 0.100000, loss: 2.9862
2022-03-04 16:29:19 - train: epoch 0009, iter [06800, 10009], lr: 0.100000, loss: 2.9857
2022-03-04 16:29:51 - train: epoch 0009, iter [06900, 10009], lr: 0.100000, loss: 3.0321
2022-03-04 16:30:24 - train: epoch 0009, iter [07000, 10009], lr: 0.100000, loss: 3.1268
2022-03-04 16:30:57 - train: epoch 0009, iter [07100, 10009], lr: 0.100000, loss: 3.1673
2022-03-04 16:31:30 - train: epoch 0009, iter [07200, 10009], lr: 0.100000, loss: 2.7285
2022-03-04 16:32:03 - train: epoch 0009, iter [07300, 10009], lr: 0.100000, loss: 2.8216
2022-03-04 16:32:36 - train: epoch 0009, iter [07400, 10009], lr: 0.100000, loss: 3.1737
2022-03-04 16:33:08 - train: epoch 0009, iter [07500, 10009], lr: 0.100000, loss: 2.5694
2022-03-04 16:33:41 - train: epoch 0009, iter [07600, 10009], lr: 0.100000, loss: 3.1912
2022-03-04 16:34:14 - train: epoch 0009, iter [07700, 10009], lr: 0.100000, loss: 2.9412
2022-03-04 16:34:47 - train: epoch 0009, iter [07800, 10009], lr: 0.100000, loss: 2.8547
2022-03-04 16:35:20 - train: epoch 0009, iter [07900, 10009], lr: 0.100000, loss: 2.7700
2022-03-04 16:35:52 - train: epoch 0009, iter [08000, 10009], lr: 0.100000, loss: 2.7344
2022-03-04 16:36:25 - train: epoch 0009, iter [08100, 10009], lr: 0.100000, loss: 3.2677
2022-03-04 16:36:58 - train: epoch 0009, iter [08200, 10009], lr: 0.100000, loss: 3.0185
2022-03-04 16:37:30 - train: epoch 0009, iter [08300, 10009], lr: 0.100000, loss: 3.1824
2022-03-04 16:38:03 - train: epoch 0009, iter [08400, 10009], lr: 0.100000, loss: 2.8195
2022-03-04 16:38:36 - train: epoch 0009, iter [08500, 10009], lr: 0.100000, loss: 2.9115
2022-03-04 16:39:08 - train: epoch 0009, iter [08600, 10009], lr: 0.100000, loss: 2.9048
2022-03-04 16:39:41 - train: epoch 0009, iter [08700, 10009], lr: 0.100000, loss: 2.7825
2022-03-04 16:40:14 - train: epoch 0009, iter [08800, 10009], lr: 0.100000, loss: 2.9670
2022-03-04 16:40:46 - train: epoch 0009, iter [08900, 10009], lr: 0.100000, loss: 2.8216
2022-03-04 16:41:19 - train: epoch 0009, iter [09000, 10009], lr: 0.100000, loss: 3.2581
2022-03-04 16:41:52 - train: epoch 0009, iter [09100, 10009], lr: 0.100000, loss: 2.9034
2022-03-04 16:42:24 - train: epoch 0009, iter [09200, 10009], lr: 0.100000, loss: 2.7855
2022-03-04 16:42:57 - train: epoch 0009, iter [09300, 10009], lr: 0.100000, loss: 2.9605
2022-03-04 16:43:30 - train: epoch 0009, iter [09400, 10009], lr: 0.100000, loss: 3.3882
2022-03-04 16:44:03 - train: epoch 0009, iter [09500, 10009], lr: 0.100000, loss: 3.0479
2022-03-04 16:44:35 - train: epoch 0009, iter [09600, 10009], lr: 0.100000, loss: 3.0923
2022-03-04 16:45:08 - train: epoch 0009, iter [09700, 10009], lr: 0.100000, loss: 2.6644
2022-03-04 16:45:40 - train: epoch 0009, iter [09800, 10009], lr: 0.100000, loss: 3.4160
2022-03-04 16:46:13 - train: epoch 0009, iter [09900, 10009], lr: 0.100000, loss: 2.8989
2022-03-04 16:46:46 - train: epoch 0009, iter [10000, 10009], lr: 0.100000, loss: 2.7196
2022-03-04 16:46:49 - train: epoch 009, train_loss: 2.9021
2022-03-04 16:48:05 - eval: epoch: 009, acc1: 43.122%, acc5: 69.572%, test_loss: 2.5518, per_image_load_time: 1.022ms, per_image_inference_time: 1.527ms
2022-03-04 16:48:06 - until epoch: 009, best_acc1: 43.122%
2022-03-04 16:48:06 - epoch 010 lr: 0.1
2022-03-04 16:48:42 - train: epoch 0010, iter [00100, 10009], lr: 0.100000, loss: 2.8180
2022-03-04 16:49:14 - train: epoch 0010, iter [00200, 10009], lr: 0.100000, loss: 2.8658
2022-03-04 16:49:46 - train: epoch 0010, iter [00300, 10009], lr: 0.100000, loss: 2.8776
2022-03-04 16:50:19 - train: epoch 0010, iter [00400, 10009], lr: 0.100000, loss: 2.7598
2022-03-04 16:50:51 - train: epoch 0010, iter [00500, 10009], lr: 0.100000, loss: 2.8607
2022-03-04 16:51:24 - train: epoch 0010, iter [00600, 10009], lr: 0.100000, loss: 3.3045
2022-03-04 16:51:56 - train: epoch 0010, iter [00700, 10009], lr: 0.100000, loss: 3.1890
2022-03-04 16:52:29 - train: epoch 0010, iter [00800, 10009], lr: 0.100000, loss: 2.6595
2022-03-04 16:53:02 - train: epoch 0010, iter [00900, 10009], lr: 0.100000, loss: 2.7760
2022-03-04 16:53:34 - train: epoch 0010, iter [01000, 10009], lr: 0.100000, loss: 2.7092
2022-03-04 16:54:07 - train: epoch 0010, iter [01100, 10009], lr: 0.100000, loss: 2.6961
2022-03-04 16:54:40 - train: epoch 0010, iter [01200, 10009], lr: 0.100000, loss: 2.8658
2022-03-04 16:55:12 - train: epoch 0010, iter [01300, 10009], lr: 0.100000, loss: 2.8186
2022-03-04 16:55:45 - train: epoch 0010, iter [01400, 10009], lr: 0.100000, loss: 3.0778
2022-03-04 16:56:17 - train: epoch 0010, iter [01500, 10009], lr: 0.100000, loss: 2.9058
2022-03-04 16:56:50 - train: epoch 0010, iter [01600, 10009], lr: 0.100000, loss: 2.5770
2022-03-04 16:57:23 - train: epoch 0010, iter [01700, 10009], lr: 0.100000, loss: 2.9029
2022-03-04 16:57:55 - train: epoch 0010, iter [01800, 10009], lr: 0.100000, loss: 2.6312
2022-03-04 16:58:28 - train: epoch 0010, iter [01900, 10009], lr: 0.100000, loss: 3.0438
2022-03-04 16:59:01 - train: epoch 0010, iter [02000, 10009], lr: 0.100000, loss: 2.5656
2022-03-04 16:59:33 - train: epoch 0010, iter [02100, 10009], lr: 0.100000, loss: 3.1432
2022-03-04 17:00:06 - train: epoch 0010, iter [02200, 10009], lr: 0.100000, loss: 3.0427
2022-03-04 17:00:39 - train: epoch 0010, iter [02300, 10009], lr: 0.100000, loss: 2.9190
2022-03-04 17:01:12 - train: epoch 0010, iter [02400, 10009], lr: 0.100000, loss: 2.7851
2022-03-04 17:01:44 - train: epoch 0010, iter [02500, 10009], lr: 0.100000, loss: 2.7508
2022-03-04 17:02:17 - train: epoch 0010, iter [02600, 10009], lr: 0.100000, loss: 2.6212
2022-03-04 17:02:50 - train: epoch 0010, iter [02700, 10009], lr: 0.100000, loss: 2.8309
2022-03-04 17:03:23 - train: epoch 0010, iter [02800, 10009], lr: 0.100000, loss: 3.1477
2022-03-04 17:03:56 - train: epoch 0010, iter [02900, 10009], lr: 0.100000, loss: 2.9355
2022-03-04 17:04:28 - train: epoch 0010, iter [03000, 10009], lr: 0.100000, loss: 2.6622
2022-03-04 17:05:01 - train: epoch 0010, iter [03100, 10009], lr: 0.100000, loss: 3.0360
2022-03-04 17:05:34 - train: epoch 0010, iter [03200, 10009], lr: 0.100000, loss: 2.8011
2022-03-04 17:06:07 - train: epoch 0010, iter [03300, 10009], lr: 0.100000, loss: 2.8998
2022-03-04 17:06:40 - train: epoch 0010, iter [03400, 10009], lr: 0.100000, loss: 2.8932
2022-03-04 17:07:12 - train: epoch 0010, iter [03500, 10009], lr: 0.100000, loss: 2.8356
2022-03-04 17:07:45 - train: epoch 0010, iter [03600, 10009], lr: 0.100000, loss: 2.5837
2022-03-04 17:08:18 - train: epoch 0010, iter [03700, 10009], lr: 0.100000, loss: 2.7539
2022-03-04 17:08:51 - train: epoch 0010, iter [03800, 10009], lr: 0.100000, loss: 3.0868
2022-03-04 17:09:24 - train: epoch 0010, iter [03900, 10009], lr: 0.100000, loss: 2.8367
2022-03-04 17:09:57 - train: epoch 0010, iter [04000, 10009], lr: 0.100000, loss: 2.5809
2022-03-04 17:10:30 - train: epoch 0010, iter [04100, 10009], lr: 0.100000, loss: 2.9614
2022-03-04 17:11:03 - train: epoch 0010, iter [04200, 10009], lr: 0.100000, loss: 2.6101
2022-03-04 17:11:35 - train: epoch 0010, iter [04300, 10009], lr: 0.100000, loss: 2.6500
2022-03-04 17:12:08 - train: epoch 0010, iter [04400, 10009], lr: 0.100000, loss: 2.8582
2022-03-04 17:12:41 - train: epoch 0010, iter [04500, 10009], lr: 0.100000, loss: 2.7466
2022-03-04 17:13:14 - train: epoch 0010, iter [04600, 10009], lr: 0.100000, loss: 2.9545
2022-03-04 17:13:47 - train: epoch 0010, iter [04700, 10009], lr: 0.100000, loss: 2.8393
2022-03-04 17:14:20 - train: epoch 0010, iter [04800, 10009], lr: 0.100000, loss: 2.7858
2022-03-04 17:14:52 - train: epoch 0010, iter [04900, 10009], lr: 0.100000, loss: 3.0879
2022-03-04 17:15:25 - train: epoch 0010, iter [05000, 10009], lr: 0.100000, loss: 2.8057
2022-03-04 17:15:58 - train: epoch 0010, iter [05100, 10009], lr: 0.100000, loss: 2.9817
2022-03-04 17:16:31 - train: epoch 0010, iter [05200, 10009], lr: 0.100000, loss: 2.8659
2022-03-04 17:17:04 - train: epoch 0010, iter [05300, 10009], lr: 0.100000, loss: 2.6742
2022-03-04 17:17:36 - train: epoch 0010, iter [05400, 10009], lr: 0.100000, loss: 2.8518
2022-03-04 17:18:09 - train: epoch 0010, iter [05500, 10009], lr: 0.100000, loss: 3.1995
2022-03-04 17:18:42 - train: epoch 0010, iter [05600, 10009], lr: 0.100000, loss: 2.8084
2022-03-04 17:19:14 - train: epoch 0010, iter [05700, 10009], lr: 0.100000, loss: 2.8881
2022-03-04 17:19:47 - train: epoch 0010, iter [05800, 10009], lr: 0.100000, loss: 3.1358
2022-03-04 17:20:20 - train: epoch 0010, iter [05900, 10009], lr: 0.100000, loss: 2.8392
2022-03-04 17:20:53 - train: epoch 0010, iter [06000, 10009], lr: 0.100000, loss: 2.7717
2022-03-04 17:21:25 - train: epoch 0010, iter [06100, 10009], lr: 0.100000, loss: 2.8183
2022-03-04 17:21:58 - train: epoch 0010, iter [06200, 10009], lr: 0.100000, loss: 3.0848
2022-03-04 17:22:31 - train: epoch 0010, iter [06300, 10009], lr: 0.100000, loss: 3.1173
2022-03-04 17:23:03 - train: epoch 0010, iter [06400, 10009], lr: 0.100000, loss: 3.1005
2022-03-04 17:23:36 - train: epoch 0010, iter [06500, 10009], lr: 0.100000, loss: 2.8399
2022-03-04 17:24:09 - train: epoch 0010, iter [06600, 10009], lr: 0.100000, loss: 2.8954
2022-03-04 17:24:42 - train: epoch 0010, iter [06700, 10009], lr: 0.100000, loss: 2.5701
2022-03-04 17:25:14 - train: epoch 0010, iter [06800, 10009], lr: 0.100000, loss: 3.1681
2022-03-04 17:25:47 - train: epoch 0010, iter [06900, 10009], lr: 0.100000, loss: 3.0633
2022-03-04 17:26:20 - train: epoch 0010, iter [07000, 10009], lr: 0.100000, loss: 2.7463
2022-03-04 17:26:53 - train: epoch 0010, iter [07100, 10009], lr: 0.100000, loss: 2.8221
2022-03-04 17:27:25 - train: epoch 0010, iter [07200, 10009], lr: 0.100000, loss: 3.0541
2022-03-04 17:27:58 - train: epoch 0010, iter [07300, 10009], lr: 0.100000, loss: 2.4851
2022-03-04 17:28:31 - train: epoch 0010, iter [07400, 10009], lr: 0.100000, loss: 2.9744
2022-03-04 17:29:03 - train: epoch 0010, iter [07500, 10009], lr: 0.100000, loss: 3.3142
2022-03-04 17:29:36 - train: epoch 0010, iter [07600, 10009], lr: 0.100000, loss: 2.9253
2022-03-04 17:30:09 - train: epoch 0010, iter [07700, 10009], lr: 0.100000, loss: 3.0743
2022-03-04 17:30:42 - train: epoch 0010, iter [07800, 10009], lr: 0.100000, loss: 2.3395
2022-03-04 17:31:14 - train: epoch 0010, iter [07900, 10009], lr: 0.100000, loss: 2.9589
2022-03-04 17:31:47 - train: epoch 0010, iter [08000, 10009], lr: 0.100000, loss: 2.6984
2022-03-04 17:32:20 - train: epoch 0010, iter [08100, 10009], lr: 0.100000, loss: 2.9153
2022-03-04 17:32:53 - train: epoch 0010, iter [08200, 10009], lr: 0.100000, loss: 2.8430
2022-03-04 17:33:25 - train: epoch 0010, iter [08300, 10009], lr: 0.100000, loss: 2.6937
2022-03-04 17:33:58 - train: epoch 0010, iter [08400, 10009], lr: 0.100000, loss: 2.8223
2022-03-04 17:34:31 - train: epoch 0010, iter [08500, 10009], lr: 0.100000, loss: 2.7720
2022-03-04 17:35:03 - train: epoch 0010, iter [08600, 10009], lr: 0.100000, loss: 2.8701
2022-03-04 17:35:36 - train: epoch 0010, iter [08700, 10009], lr: 0.100000, loss: 2.9776
2022-03-04 17:36:09 - train: epoch 0010, iter [08800, 10009], lr: 0.100000, loss: 2.8946
2022-03-04 17:36:42 - train: epoch 0010, iter [08900, 10009], lr: 0.100000, loss: 2.8400
2022-03-04 17:37:14 - train: epoch 0010, iter [09000, 10009], lr: 0.100000, loss: 2.9011
2022-03-04 17:37:47 - train: epoch 0010, iter [09100, 10009], lr: 0.100000, loss: 2.8649
2022-03-04 17:38:20 - train: epoch 0010, iter [09200, 10009], lr: 0.100000, loss: 3.0194
2022-03-04 17:38:52 - train: epoch 0010, iter [09300, 10009], lr: 0.100000, loss: 2.5703
2022-03-04 17:39:25 - train: epoch 0010, iter [09400, 10009], lr: 0.100000, loss: 3.0815
2022-03-04 17:39:58 - train: epoch 0010, iter [09500, 10009], lr: 0.100000, loss: 2.8425
2022-03-04 17:40:30 - train: epoch 0010, iter [09600, 10009], lr: 0.100000, loss: 3.0543
2022-03-04 17:41:03 - train: epoch 0010, iter [09700, 10009], lr: 0.100000, loss: 3.0850
2022-03-04 17:41:36 - train: epoch 0010, iter [09800, 10009], lr: 0.100000, loss: 2.9852
2022-03-04 17:42:09 - train: epoch 0010, iter [09900, 10009], lr: 0.100000, loss: 3.0115
2022-03-04 17:42:41 - train: epoch 0010, iter [10000, 10009], lr: 0.100000, loss: 2.6272
2022-03-04 17:42:45 - train: epoch 010, train_loss: 2.8678
2022-03-04 17:44:00 - eval: epoch: 010, acc1: 43.620%, acc5: 69.852%, test_loss: 2.5170, per_image_load_time: 0.800ms, per_image_inference_time: 1.539ms
2022-03-04 17:44:01 - until epoch: 010, best_acc1: 43.620%
2022-03-04 17:44:01 - epoch 011 lr: 0.1
2022-03-04 17:44:37 - train: epoch 0011, iter [00100, 10009], lr: 0.100000, loss: 3.0520
2022-03-04 17:45:09 - train: epoch 0011, iter [00200, 10009], lr: 0.100000, loss: 2.7779
2022-03-04 17:45:42 - train: epoch 0011, iter [00300, 10009], lr: 0.100000, loss: 2.6172
2022-03-04 17:46:14 - train: epoch 0011, iter [00400, 10009], lr: 0.100000, loss: 2.9543
2022-03-04 17:46:47 - train: epoch 0011, iter [00500, 10009], lr: 0.100000, loss: 2.9473
2022-03-04 17:47:19 - train: epoch 0011, iter [00600, 10009], lr: 0.100000, loss: 2.6371
2022-03-04 17:47:52 - train: epoch 0011, iter [00700, 10009], lr: 0.100000, loss: 2.8133
2022-03-04 17:48:24 - train: epoch 0011, iter [00800, 10009], lr: 0.100000, loss: 3.0595
2022-03-04 17:48:57 - train: epoch 0011, iter [00900, 10009], lr: 0.100000, loss: 2.9706
2022-03-04 17:49:30 - train: epoch 0011, iter [01000, 10009], lr: 0.100000, loss: 2.9335
2022-03-04 17:50:02 - train: epoch 0011, iter [01100, 10009], lr: 0.100000, loss: 2.6471
2022-03-04 17:50:35 - train: epoch 0011, iter [01200, 10009], lr: 0.100000, loss: 3.0328
2022-03-04 17:51:07 - train: epoch 0011, iter [01300, 10009], lr: 0.100000, loss: 2.7746
2022-03-04 17:51:40 - train: epoch 0011, iter [01400, 10009], lr: 0.100000, loss: 2.8816
2022-03-04 17:52:13 - train: epoch 0011, iter [01500, 10009], lr: 0.100000, loss: 2.7566
2022-03-04 17:52:45 - train: epoch 0011, iter [01600, 10009], lr: 0.100000, loss: 2.8545
2022-03-04 17:53:18 - train: epoch 0011, iter [01700, 10009], lr: 0.100000, loss: 2.8759
2022-03-04 17:53:50 - train: epoch 0011, iter [01800, 10009], lr: 0.100000, loss: 2.6589
2022-03-04 17:54:23 - train: epoch 0011, iter [01900, 10009], lr: 0.100000, loss: 2.4392
2022-03-04 17:54:56 - train: epoch 0011, iter [02000, 10009], lr: 0.100000, loss: 2.7519
2022-03-04 17:55:28 - train: epoch 0011, iter [02100, 10009], lr: 0.100000, loss: 2.9851
2022-03-04 17:56:01 - train: epoch 0011, iter [02200, 10009], lr: 0.100000, loss: 2.7844
2022-03-04 17:56:33 - train: epoch 0011, iter [02300, 10009], lr: 0.100000, loss: 3.0516
2022-03-04 17:57:06 - train: epoch 0011, iter [02400, 10009], lr: 0.100000, loss: 3.0157
2022-03-04 17:57:38 - train: epoch 0011, iter [02500, 10009], lr: 0.100000, loss: 3.1990
2022-03-04 17:58:11 - train: epoch 0011, iter [02600, 10009], lr: 0.100000, loss: 3.1922
2022-03-04 17:58:43 - train: epoch 0011, iter [02700, 10009], lr: 0.100000, loss: 2.5878
2022-03-04 17:59:16 - train: epoch 0011, iter [02800, 10009], lr: 0.100000, loss: 2.7816
2022-03-04 17:59:49 - train: epoch 0011, iter [02900, 10009], lr: 0.100000, loss: 2.7681
2022-03-04 18:00:21 - train: epoch 0011, iter [03000, 10009], lr: 0.100000, loss: 3.0068
2022-03-04 18:00:54 - train: epoch 0011, iter [03100, 10009], lr: 0.100000, loss: 3.0180
2022-03-04 18:01:26 - train: epoch 0011, iter [03200, 10009], lr: 0.100000, loss: 2.8705
2022-03-04 18:01:59 - train: epoch 0011, iter [03300, 10009], lr: 0.100000, loss: 3.0198
2022-03-04 18:02:32 - train: epoch 0011, iter [03400, 10009], lr: 0.100000, loss: 2.8223
2022-03-04 18:03:04 - train: epoch 0011, iter [03500, 10009], lr: 0.100000, loss: 2.7105
2022-03-04 18:03:37 - train: epoch 0011, iter [03600, 10009], lr: 0.100000, loss: 2.9469
2022-03-04 18:04:10 - train: epoch 0011, iter [03700, 10009], lr: 0.100000, loss: 3.0252
2022-03-04 18:04:42 - train: epoch 0011, iter [03800, 10009], lr: 0.100000, loss: 3.0954
2022-03-04 18:05:15 - train: epoch 0011, iter [03900, 10009], lr: 0.100000, loss: 3.0338
2022-03-04 18:05:48 - train: epoch 0011, iter [04000, 10009], lr: 0.100000, loss: 2.8150
2022-03-04 18:06:20 - train: epoch 0011, iter [04100, 10009], lr: 0.100000, loss: 2.7177
2022-03-04 18:06:53 - train: epoch 0011, iter [04200, 10009], lr: 0.100000, loss: 2.9092
2022-03-04 18:07:26 - train: epoch 0011, iter [04300, 10009], lr: 0.100000, loss: 2.8186
2022-03-04 18:07:58 - train: epoch 0011, iter [04400, 10009], lr: 0.100000, loss: 2.7964
2022-03-04 18:08:31 - train: epoch 0011, iter [04500, 10009], lr: 0.100000, loss: 2.5531
2022-03-04 18:09:03 - train: epoch 0011, iter [04600, 10009], lr: 0.100000, loss: 2.9601
2022-03-04 18:09:36 - train: epoch 0011, iter [04700, 10009], lr: 0.100000, loss: 2.6643
2022-03-04 18:10:09 - train: epoch 0011, iter [04800, 10009], lr: 0.100000, loss: 2.7872
2022-03-04 18:10:41 - train: epoch 0011, iter [04900, 10009], lr: 0.100000, loss: 2.9237
2022-03-04 18:11:14 - train: epoch 0011, iter [05000, 10009], lr: 0.100000, loss: 2.9347
2022-03-04 18:11:47 - train: epoch 0011, iter [05100, 10009], lr: 0.100000, loss: 2.8397
2022-03-04 18:12:19 - train: epoch 0011, iter [05200, 10009], lr: 0.100000, loss: 2.5919
2022-03-04 18:12:52 - train: epoch 0011, iter [05300, 10009], lr: 0.100000, loss: 2.9368
2022-03-04 18:13:25 - train: epoch 0011, iter [05400, 10009], lr: 0.100000, loss: 2.5831
2022-03-04 18:13:58 - train: epoch 0011, iter [05500, 10009], lr: 0.100000, loss: 2.6142
2022-03-04 18:14:31 - train: epoch 0011, iter [05600, 10009], lr: 0.100000, loss: 2.5410
2022-03-04 18:15:03 - train: epoch 0011, iter [05700, 10009], lr: 0.100000, loss: 2.5029
2022-03-04 18:15:36 - train: epoch 0011, iter [05800, 10009], lr: 0.100000, loss: 2.4702
2022-03-04 18:16:09 - train: epoch 0011, iter [05900, 10009], lr: 0.100000, loss: 3.4826
2022-03-04 18:16:41 - train: epoch 0011, iter [06000, 10009], lr: 0.100000, loss: 3.3119
2022-03-04 18:17:14 - train: epoch 0011, iter [06100, 10009], lr: 0.100000, loss: 2.8091
2022-03-04 18:17:47 - train: epoch 0011, iter [06200, 10009], lr: 0.100000, loss: 2.9626
2022-03-04 18:18:20 - train: epoch 0011, iter [06300, 10009], lr: 0.100000, loss: 2.9678
2022-03-04 18:18:52 - train: epoch 0011, iter [06400, 10009], lr: 0.100000, loss: 2.6434
2022-03-04 18:19:25 - train: epoch 0011, iter [06500, 10009], lr: 0.100000, loss: 3.0468
2022-03-04 18:19:58 - train: epoch 0011, iter [06600, 10009], lr: 0.100000, loss: 3.0308
2022-03-04 18:20:30 - train: epoch 0011, iter [06700, 10009], lr: 0.100000, loss: 2.3357
2022-03-04 18:21:03 - train: epoch 0011, iter [06800, 10009], lr: 0.100000, loss: 3.0842
2022-03-04 18:21:36 - train: epoch 0011, iter [06900, 10009], lr: 0.100000, loss: 3.0245
2022-03-04 18:22:09 - train: epoch 0011, iter [07000, 10009], lr: 0.100000, loss: 3.0202
2022-03-04 18:22:41 - train: epoch 0011, iter [07100, 10009], lr: 0.100000, loss: 3.3078
2022-03-04 18:23:14 - train: epoch 0011, iter [07200, 10009], lr: 0.100000, loss: 2.6786
2022-03-04 18:23:47 - train: epoch 0011, iter [07300, 10009], lr: 0.100000, loss: 2.7394
2022-03-04 18:24:19 - train: epoch 0011, iter [07400, 10009], lr: 0.100000, loss: 2.8393
2022-03-04 18:24:52 - train: epoch 0011, iter [07500, 10009], lr: 0.100000, loss: 2.7513
2022-03-04 18:25:25 - train: epoch 0011, iter [07600, 10009], lr: 0.100000, loss: 2.7869
2022-03-04 18:25:57 - train: epoch 0011, iter [07700, 10009], lr: 0.100000, loss: 2.7132
2022-03-04 18:26:30 - train: epoch 0011, iter [07800, 10009], lr: 0.100000, loss: 2.6665
2022-03-04 18:27:03 - train: epoch 0011, iter [07900, 10009], lr: 0.100000, loss: 2.8875
2022-03-04 18:27:36 - train: epoch 0011, iter [08000, 10009], lr: 0.100000, loss: 2.7245
2022-03-04 18:28:08 - train: epoch 0011, iter [08100, 10009], lr: 0.100000, loss: 3.1383
2022-03-04 18:28:41 - train: epoch 0011, iter [08200, 10009], lr: 0.100000, loss: 2.5591
2022-03-04 18:29:14 - train: epoch 0011, iter [08300, 10009], lr: 0.100000, loss: 2.8545
2022-03-04 18:29:46 - train: epoch 0011, iter [08400, 10009], lr: 0.100000, loss: 2.8853
2022-03-04 18:30:19 - train: epoch 0011, iter [08500, 10009], lr: 0.100000, loss: 3.1004
2022-03-04 18:30:52 - train: epoch 0011, iter [08600, 10009], lr: 0.100000, loss: 3.0066
2022-03-04 18:31:24 - train: epoch 0011, iter [08700, 10009], lr: 0.100000, loss: 2.7199
2022-03-04 18:31:57 - train: epoch 0011, iter [08800, 10009], lr: 0.100000, loss: 2.9759
2022-03-04 18:32:30 - train: epoch 0011, iter [08900, 10009], lr: 0.100000, loss: 2.9774
2022-03-04 18:33:03 - train: epoch 0011, iter [09000, 10009], lr: 0.100000, loss: 2.7700
2022-03-04 18:33:35 - train: epoch 0011, iter [09100, 10009], lr: 0.100000, loss: 2.8975
2022-03-04 18:34:08 - train: epoch 0011, iter [09200, 10009], lr: 0.100000, loss: 2.9526
2022-03-04 18:34:41 - train: epoch 0011, iter [09300, 10009], lr: 0.100000, loss: 2.9617
2022-03-04 18:35:13 - train: epoch 0011, iter [09400, 10009], lr: 0.100000, loss: 2.4683
2022-03-04 18:35:46 - train: epoch 0011, iter [09500, 10009], lr: 0.100000, loss: 3.3092
2022-03-04 18:36:19 - train: epoch 0011, iter [09600, 10009], lr: 0.100000, loss: 2.7868
2022-03-04 18:36:51 - train: epoch 0011, iter [09700, 10009], lr: 0.100000, loss: 2.8340
2022-03-04 18:37:24 - train: epoch 0011, iter [09800, 10009], lr: 0.100000, loss: 2.6955
2022-03-04 18:37:57 - train: epoch 0011, iter [09900, 10009], lr: 0.100000, loss: 2.8748
2022-03-04 18:38:30 - train: epoch 0011, iter [10000, 10009], lr: 0.100000, loss: 3.1460
2022-03-04 18:38:33 - train: epoch 011, train_loss: 2.8406
2022-03-04 18:39:48 - eval: epoch: 011, acc1: 44.874%, acc5: 71.024%, test_loss: 2.4484, per_image_load_time: 1.073ms, per_image_inference_time: 1.553ms
2022-03-04 18:39:48 - until epoch: 011, best_acc1: 44.874%
2022-03-04 18:39:48 - epoch 012 lr: 0.1
2022-03-04 18:40:24 - train: epoch 0012, iter [00100, 10009], lr: 0.100000, loss: 2.7366
2022-03-04 18:40:57 - train: epoch 0012, iter [00200, 10009], lr: 0.100000, loss: 2.9215
2022-03-04 18:41:29 - train: epoch 0012, iter [00300, 10009], lr: 0.100000, loss: 2.7966
2022-03-04 18:42:02 - train: epoch 0012, iter [00400, 10009], lr: 0.100000, loss: 2.9944
2022-03-04 18:42:34 - train: epoch 0012, iter [00500, 10009], lr: 0.100000, loss: 3.2453
2022-03-04 18:43:07 - train: epoch 0012, iter [00600, 10009], lr: 0.100000, loss: 2.8701
2022-03-04 18:43:39 - train: epoch 0012, iter [00700, 10009], lr: 0.100000, loss: 2.7803
2022-03-04 18:44:12 - train: epoch 0012, iter [00800, 10009], lr: 0.100000, loss: 2.8963
2022-03-04 18:44:44 - train: epoch 0012, iter [00900, 10009], lr: 0.100000, loss: 2.9705
2022-03-04 18:45:17 - train: epoch 0012, iter [01000, 10009], lr: 0.100000, loss: 2.5119
2022-03-04 18:45:49 - train: epoch 0012, iter [01100, 10009], lr: 0.100000, loss: 2.7085
2022-03-04 18:46:21 - train: epoch 0012, iter [01200, 10009], lr: 0.100000, loss: 2.4151
2022-03-04 18:46:54 - train: epoch 0012, iter [01300, 10009], lr: 0.100000, loss: 2.8283
2022-03-04 18:47:27 - train: epoch 0012, iter [01400, 10009], lr: 0.100000, loss: 2.6512
2022-03-04 18:47:59 - train: epoch 0012, iter [01500, 10009], lr: 0.100000, loss: 2.7411
2022-03-04 18:48:32 - train: epoch 0012, iter [01600, 10009], lr: 0.100000, loss: 2.5438
2022-03-04 18:49:04 - train: epoch 0012, iter [01700, 10009], lr: 0.100000, loss: 2.5484
2022-03-04 18:49:37 - train: epoch 0012, iter [01800, 10009], lr: 0.100000, loss: 3.3642
2022-03-04 18:50:09 - train: epoch 0012, iter [01900, 10009], lr: 0.100000, loss: 3.0618
2022-03-04 18:50:42 - train: epoch 0012, iter [02000, 10009], lr: 0.100000, loss: 2.5613
2022-03-04 18:51:14 - train: epoch 0012, iter [02100, 10009], lr: 0.100000, loss: 2.9367
2022-03-04 18:51:47 - train: epoch 0012, iter [02200, 10009], lr: 0.100000, loss: 3.1924
2022-03-04 18:52:19 - train: epoch 0012, iter [02300, 10009], lr: 0.100000, loss: 3.1226
2022-03-04 18:52:52 - train: epoch 0012, iter [02400, 10009], lr: 0.100000, loss: 2.5588
2022-03-04 18:53:24 - train: epoch 0012, iter [02500, 10009], lr: 0.100000, loss: 2.5567
2022-03-04 18:53:57 - train: epoch 0012, iter [02600, 10009], lr: 0.100000, loss: 2.8272
2022-03-04 18:54:29 - train: epoch 0012, iter [02700, 10009], lr: 0.100000, loss: 2.8827
2022-03-04 18:55:02 - train: epoch 0012, iter [02800, 10009], lr: 0.100000, loss: 2.8599
2022-03-04 18:55:34 - train: epoch 0012, iter [02900, 10009], lr: 0.100000, loss: 2.9163
2022-03-04 18:56:07 - train: epoch 0012, iter [03000, 10009], lr: 0.100000, loss: 2.3827
2022-03-04 18:56:40 - train: epoch 0012, iter [03100, 10009], lr: 0.100000, loss: 2.7580
2022-03-04 18:57:12 - train: epoch 0012, iter [03200, 10009], lr: 0.100000, loss: 2.9184
2022-03-04 18:57:45 - train: epoch 0012, iter [03300, 10009], lr: 0.100000, loss: 2.8737
2022-03-04 18:58:17 - train: epoch 0012, iter [03400, 10009], lr: 0.100000, loss: 2.7425
2022-03-04 18:58:50 - train: epoch 0012, iter [03500, 10009], lr: 0.100000, loss: 2.9580
2022-03-04 18:59:22 - train: epoch 0012, iter [03600, 10009], lr: 0.100000, loss: 2.6421
2022-03-04 18:59:55 - train: epoch 0012, iter [03700, 10009], lr: 0.100000, loss: 2.6219
2022-03-04 19:00:27 - train: epoch 0012, iter [03800, 10009], lr: 0.100000, loss: 2.9552
2022-03-04 19:01:00 - train: epoch 0012, iter [03900, 10009], lr: 0.100000, loss: 2.9005
2022-03-04 19:01:32 - train: epoch 0012, iter [04000, 10009], lr: 0.100000, loss: 3.2459
2022-03-04 19:02:05 - train: epoch 0012, iter [04100, 10009], lr: 0.100000, loss: 2.8974
2022-03-04 19:02:38 - train: epoch 0012, iter [04200, 10009], lr: 0.100000, loss: 3.0454
2022-03-04 19:03:10 - train: epoch 0012, iter [04300, 10009], lr: 0.100000, loss: 2.7154
2022-03-04 19:03:43 - train: epoch 0012, iter [04400, 10009], lr: 0.100000, loss: 3.0455
2022-03-04 19:04:15 - train: epoch 0012, iter [04500, 10009], lr: 0.100000, loss: 2.6992
2022-03-04 19:04:48 - train: epoch 0012, iter [04600, 10009], lr: 0.100000, loss: 2.9051
2022-03-04 19:05:21 - train: epoch 0012, iter [04700, 10009], lr: 0.100000, loss: 2.9834
2022-03-04 19:05:53 - train: epoch 0012, iter [04800, 10009], lr: 0.100000, loss: 2.8152
2022-03-04 19:06:26 - train: epoch 0012, iter [04900, 10009], lr: 0.100000, loss: 2.1676
2022-03-04 19:06:58 - train: epoch 0012, iter [05000, 10009], lr: 0.100000, loss: 2.6489
2022-03-04 19:07:31 - train: epoch 0012, iter [05100, 10009], lr: 0.100000, loss: 3.0690
2022-03-04 19:08:04 - train: epoch 0012, iter [05200, 10009], lr: 0.100000, loss: 2.6069
2022-03-04 19:08:36 - train: epoch 0012, iter [05300, 10009], lr: 0.100000, loss: 2.3659
2022-03-04 19:09:09 - train: epoch 0012, iter [05400, 10009], lr: 0.100000, loss: 2.8259
2022-03-04 19:09:42 - train: epoch 0012, iter [05500, 10009], lr: 0.100000, loss: 3.0301
2022-03-04 19:10:14 - train: epoch 0012, iter [05600, 10009], lr: 0.100000, loss: 2.6640
2022-03-04 19:10:47 - train: epoch 0012, iter [05700, 10009], lr: 0.100000, loss: 3.0874
2022-03-04 19:11:19 - train: epoch 0012, iter [05800, 10009], lr: 0.100000, loss: 3.1407
2022-03-04 19:11:52 - train: epoch 0012, iter [05900, 10009], lr: 0.100000, loss: 2.8315
2022-03-04 19:12:25 - train: epoch 0012, iter [06000, 10009], lr: 0.100000, loss: 2.9152
2022-03-04 19:12:57 - train: epoch 0012, iter [06100, 10009], lr: 0.100000, loss: 2.5657
2022-03-04 19:13:30 - train: epoch 0012, iter [06200, 10009], lr: 0.100000, loss: 2.2898
2022-03-04 19:14:03 - train: epoch 0012, iter [06300, 10009], lr: 0.100000, loss: 2.6194
2022-03-04 19:14:35 - train: epoch 0012, iter [06400, 10009], lr: 0.100000, loss: 2.3294
2022-03-04 19:15:08 - train: epoch 0012, iter [06500, 10009], lr: 0.100000, loss: 2.7295
2022-03-04 19:15:40 - train: epoch 0012, iter [06600, 10009], lr: 0.100000, loss: 2.6409
2022-03-04 19:16:13 - train: epoch 0012, iter [06700, 10009], lr: 0.100000, loss: 3.0570
2022-03-04 19:16:46 - train: epoch 0012, iter [06800, 10009], lr: 0.100000, loss: 2.7313
2022-03-04 19:17:19 - train: epoch 0012, iter [06900, 10009], lr: 0.100000, loss: 2.6469
2022-03-04 19:17:51 - train: epoch 0012, iter [07000, 10009], lr: 0.100000, loss: 2.8079
2022-03-04 19:18:24 - train: epoch 0012, iter [07100, 10009], lr: 0.100000, loss: 2.6451
2022-03-04 19:18:56 - train: epoch 0012, iter [07200, 10009], lr: 0.100000, loss: 2.5960
2022-03-04 19:19:29 - train: epoch 0012, iter [07300, 10009], lr: 0.100000, loss: 2.9476
2022-03-04 19:20:02 - train: epoch 0012, iter [07400, 10009], lr: 0.100000, loss: 2.9776
2022-03-04 19:20:34 - train: epoch 0012, iter [07500, 10009], lr: 0.100000, loss: 2.6616
2022-03-04 19:21:07 - train: epoch 0012, iter [07600, 10009], lr: 0.100000, loss: 2.8802
2022-03-04 19:21:40 - train: epoch 0012, iter [07700, 10009], lr: 0.100000, loss: 2.7774
2022-03-04 19:22:12 - train: epoch 0012, iter [07800, 10009], lr: 0.100000, loss: 2.4766
2022-03-04 19:22:45 - train: epoch 0012, iter [07900, 10009], lr: 0.100000, loss: 2.9847
2022-03-04 19:23:18 - train: epoch 0012, iter [08000, 10009], lr: 0.100000, loss: 2.7005
2022-03-04 19:23:50 - train: epoch 0012, iter [08100, 10009], lr: 0.100000, loss: 2.9308
2022-03-04 19:24:23 - train: epoch 0012, iter [08200, 10009], lr: 0.100000, loss: 2.5916
2022-03-04 19:24:56 - train: epoch 0012, iter [08300, 10009], lr: 0.100000, loss: 2.7191
2022-03-04 19:25:28 - train: epoch 0012, iter [08400, 10009], lr: 0.100000, loss: 2.7687
2022-03-04 19:26:01 - train: epoch 0012, iter [08500, 10009], lr: 0.100000, loss: 2.7846
2022-03-04 19:26:33 - train: epoch 0012, iter [08600, 10009], lr: 0.100000, loss: 3.0136
2022-03-04 19:27:06 - train: epoch 0012, iter [08700, 10009], lr: 0.100000, loss: 2.7491
2022-03-04 19:27:39 - train: epoch 0012, iter [08800, 10009], lr: 0.100000, loss: 2.6865
2022-03-04 19:28:11 - train: epoch 0012, iter [08900, 10009], lr: 0.100000, loss: 2.9220
2022-03-04 19:28:44 - train: epoch 0012, iter [09000, 10009], lr: 0.100000, loss: 2.5959
2022-03-04 19:29:17 - train: epoch 0012, iter [09100, 10009], lr: 0.100000, loss: 2.5341
2022-03-04 19:29:49 - train: epoch 0012, iter [09200, 10009], lr: 0.100000, loss: 2.6975
2022-03-04 19:30:22 - train: epoch 0012, iter [09300, 10009], lr: 0.100000, loss: 2.6300
2022-03-04 19:30:55 - train: epoch 0012, iter [09400, 10009], lr: 0.100000, loss: 2.7258
2022-03-04 19:31:27 - train: epoch 0012, iter [09500, 10009], lr: 0.100000, loss: 2.8309
2022-03-04 19:32:00 - train: epoch 0012, iter [09600, 10009], lr: 0.100000, loss: 2.7461
2022-03-04 19:32:33 - train: epoch 0012, iter [09700, 10009], lr: 0.100000, loss: 3.0535
2022-03-04 19:33:06 - train: epoch 0012, iter [09800, 10009], lr: 0.100000, loss: 2.6104
2022-03-04 19:33:38 - train: epoch 0012, iter [09900, 10009], lr: 0.100000, loss: 2.8136
2022-03-04 19:34:11 - train: epoch 0012, iter [10000, 10009], lr: 0.100000, loss: 2.5094
2022-03-04 19:34:14 - train: epoch 012, train_loss: 2.8143
2022-03-04 19:35:29 - eval: epoch: 012, acc1: 45.708%, acc5: 71.774%, test_loss: 2.4009, per_image_load_time: 1.181ms, per_image_inference_time: 1.549ms
2022-03-04 19:35:30 - until epoch: 012, best_acc1: 45.708%
2022-03-04 19:35:30 - epoch 013 lr: 0.1
2022-03-04 19:36:05 - train: epoch 0013, iter [00100, 10009], lr: 0.100000, loss: 2.6104
2022-03-04 19:36:38 - train: epoch 0013, iter [00200, 10009], lr: 0.100000, loss: 2.6703
2022-03-04 19:37:10 - train: epoch 0013, iter [00300, 10009], lr: 0.100000, loss: 2.9027
2022-03-04 19:37:43 - train: epoch 0013, iter [00400, 10009], lr: 0.100000, loss: 2.5101
2022-03-04 19:38:15 - train: epoch 0013, iter [00500, 10009], lr: 0.100000, loss: 2.6899
2022-03-04 19:38:48 - train: epoch 0013, iter [00600, 10009], lr: 0.100000, loss: 2.9283
2022-03-04 19:39:20 - train: epoch 0013, iter [00700, 10009], lr: 0.100000, loss: 2.6881
2022-03-04 19:39:53 - train: epoch 0013, iter [00800, 10009], lr: 0.100000, loss: 2.9836
2022-03-04 19:40:25 - train: epoch 0013, iter [00900, 10009], lr: 0.100000, loss: 3.2432
2022-03-04 19:40:58 - train: epoch 0013, iter [01000, 10009], lr: 0.100000, loss: 2.8475
2022-03-04 19:41:30 - train: epoch 0013, iter [01100, 10009], lr: 0.100000, loss: 2.9505
2022-03-04 19:42:02 - train: epoch 0013, iter [01200, 10009], lr: 0.100000, loss: 3.2864
2022-03-04 19:42:35 - train: epoch 0013, iter [01300, 10009], lr: 0.100000, loss: 2.9978
2022-03-04 19:43:07 - train: epoch 0013, iter [01400, 10009], lr: 0.100000, loss: 2.5551
2022-03-04 19:43:40 - train: epoch 0013, iter [01500, 10009], lr: 0.100000, loss: 2.7542
2022-03-04 19:44:12 - train: epoch 0013, iter [01600, 10009], lr: 0.100000, loss: 3.0852
2022-03-04 19:44:45 - train: epoch 0013, iter [01700, 10009], lr: 0.100000, loss: 2.6068
2022-03-04 19:45:17 - train: epoch 0013, iter [01800, 10009], lr: 0.100000, loss: 2.9321
2022-03-04 19:45:50 - train: epoch 0013, iter [01900, 10009], lr: 0.100000, loss: 2.5554
2022-03-04 19:46:22 - train: epoch 0013, iter [02000, 10009], lr: 0.100000, loss: 2.8492
2022-03-04 19:46:55 - train: epoch 0013, iter [02100, 10009], lr: 0.100000, loss: 2.6076
2022-03-04 19:47:27 - train: epoch 0013, iter [02200, 10009], lr: 0.100000, loss: 2.6775
2022-03-04 19:48:00 - train: epoch 0013, iter [02300, 10009], lr: 0.100000, loss: 3.2182
2022-03-04 19:48:33 - train: epoch 0013, iter [02400, 10009], lr: 0.100000, loss: 3.0311
2022-03-04 19:49:05 - train: epoch 0013, iter [02500, 10009], lr: 0.100000, loss: 2.7963
2022-03-04 19:49:38 - train: epoch 0013, iter [02600, 10009], lr: 0.100000, loss: 2.6059
2022-03-04 19:50:10 - train: epoch 0013, iter [02700, 10009], lr: 0.100000, loss: 2.7178
2022-03-04 19:50:43 - train: epoch 0013, iter [02800, 10009], lr: 0.100000, loss: 2.9767
2022-03-04 19:51:15 - train: epoch 0013, iter [02900, 10009], lr: 0.100000, loss: 2.7916
2022-03-04 19:51:48 - train: epoch 0013, iter [03000, 10009], lr: 0.100000, loss: 2.8785
2022-03-04 19:52:20 - train: epoch 0013, iter [03100, 10009], lr: 0.100000, loss: 2.9420
2022-03-04 19:52:53 - train: epoch 0013, iter [03200, 10009], lr: 0.100000, loss: 2.4262
2022-03-04 19:53:26 - train: epoch 0013, iter [03300, 10009], lr: 0.100000, loss: 2.8922
2022-03-04 19:53:58 - train: epoch 0013, iter [03400, 10009], lr: 0.100000, loss: 2.5286
2022-03-04 19:54:31 - train: epoch 0013, iter [03500, 10009], lr: 0.100000, loss: 3.1986
2022-03-04 19:55:03 - train: epoch 0013, iter [03600, 10009], lr: 0.100000, loss: 2.6726
2022-03-04 19:55:36 - train: epoch 0013, iter [03700, 10009], lr: 0.100000, loss: 2.6643
2022-03-04 19:56:08 - train: epoch 0013, iter [03800, 10009], lr: 0.100000, loss: 2.7916
2022-03-04 19:56:41 - train: epoch 0013, iter [03900, 10009], lr: 0.100000, loss: 2.2929
2022-03-04 19:57:14 - train: epoch 0013, iter [04000, 10009], lr: 0.100000, loss: 2.8938
2022-03-04 19:57:46 - train: epoch 0013, iter [04100, 10009], lr: 0.100000, loss: 2.4590
2022-03-04 19:58:19 - train: epoch 0013, iter [04200, 10009], lr: 0.100000, loss: 3.1761
2022-03-04 19:58:52 - train: epoch 0013, iter [04300, 10009], lr: 0.100000, loss: 2.6021
2022-03-04 19:59:24 - train: epoch 0013, iter [04400, 10009], lr: 0.100000, loss: 2.8536
2022-03-04 19:59:57 - train: epoch 0013, iter [04500, 10009], lr: 0.100000, loss: 2.5435
2022-03-04 20:00:29 - train: epoch 0013, iter [04600, 10009], lr: 0.100000, loss: 2.7319
2022-03-04 20:01:02 - train: epoch 0013, iter [04700, 10009], lr: 0.100000, loss: 2.8256
2022-03-04 20:01:34 - train: epoch 0013, iter [04800, 10009], lr: 0.100000, loss: 3.0029
2022-03-04 20:02:07 - train: epoch 0013, iter [04900, 10009], lr: 0.100000, loss: 2.6308
2022-03-04 20:02:40 - train: epoch 0013, iter [05000, 10009], lr: 0.100000, loss: 2.8662
2022-03-04 20:03:12 - train: epoch 0013, iter [05100, 10009], lr: 0.100000, loss: 2.6506
2022-03-04 20:03:45 - train: epoch 0013, iter [05200, 10009], lr: 0.100000, loss: 2.7286
2022-03-04 20:04:17 - train: epoch 0013, iter [05300, 10009], lr: 0.100000, loss: 2.5476
2022-03-04 20:04:50 - train: epoch 0013, iter [05400, 10009], lr: 0.100000, loss: 2.4283
2022-03-04 20:05:23 - train: epoch 0013, iter [05500, 10009], lr: 0.100000, loss: 2.7824
2022-03-04 20:05:55 - train: epoch 0013, iter [05600, 10009], lr: 0.100000, loss: 2.6953
2022-03-04 20:06:28 - train: epoch 0013, iter [05700, 10009], lr: 0.100000, loss: 2.6974
2022-03-04 20:07:00 - train: epoch 0013, iter [05800, 10009], lr: 0.100000, loss: 2.7489
2022-03-04 20:07:33 - train: epoch 0013, iter [05900, 10009], lr: 0.100000, loss: 2.7813
2022-03-04 20:08:06 - train: epoch 0013, iter [06000, 10009], lr: 0.100000, loss: 2.7663
2022-03-04 20:08:38 - train: epoch 0013, iter [06100, 10009], lr: 0.100000, loss: 2.5672
2022-03-04 20:09:11 - train: epoch 0013, iter [06200, 10009], lr: 0.100000, loss: 2.4578
2022-03-04 20:09:43 - train: epoch 0013, iter [06300, 10009], lr: 0.100000, loss: 2.8082
2022-03-04 20:10:16 - train: epoch 0013, iter [06400, 10009], lr: 0.100000, loss: 2.8688
2022-03-04 20:10:49 - train: epoch 0013, iter [06500, 10009], lr: 0.100000, loss: 2.6156
2022-03-04 20:11:21 - train: epoch 0013, iter [06600, 10009], lr: 0.100000, loss: 2.2696
2022-03-04 20:11:54 - train: epoch 0013, iter [06700, 10009], lr: 0.100000, loss: 2.8027
2022-03-04 20:12:27 - train: epoch 0013, iter [06800, 10009], lr: 0.100000, loss: 2.8354
2022-03-04 20:12:59 - train: epoch 0013, iter [06900, 10009], lr: 0.100000, loss: 2.8264
2022-03-04 20:13:32 - train: epoch 0013, iter [07000, 10009], lr: 0.100000, loss: 2.4667
2022-03-04 20:14:04 - train: epoch 0013, iter [07100, 10009], lr: 0.100000, loss: 2.8446
2022-03-04 20:14:37 - train: epoch 0013, iter [07200, 10009], lr: 0.100000, loss: 2.6940
2022-03-04 20:15:09 - train: epoch 0013, iter [07300, 10009], lr: 0.100000, loss: 2.8834
2022-03-04 20:15:42 - train: epoch 0013, iter [07400, 10009], lr: 0.100000, loss: 2.6189
2022-03-04 20:16:14 - train: epoch 0013, iter [07500, 10009], lr: 0.100000, loss: 2.8588
2022-03-04 20:16:47 - train: epoch 0013, iter [07600, 10009], lr: 0.100000, loss: 2.8948
2022-03-04 20:17:20 - train: epoch 0013, iter [07700, 10009], lr: 0.100000, loss: 2.4796
2022-03-04 20:17:52 - train: epoch 0013, iter [07800, 10009], lr: 0.100000, loss: 2.8070
2022-03-04 20:18:25 - train: epoch 0013, iter [07900, 10009], lr: 0.100000, loss: 3.1057
2022-03-04 20:18:57 - train: epoch 0013, iter [08000, 10009], lr: 0.100000, loss: 2.5760
2022-03-04 20:19:30 - train: epoch 0013, iter [08100, 10009], lr: 0.100000, loss: 2.9308
2022-03-04 20:20:02 - train: epoch 0013, iter [08200, 10009], lr: 0.100000, loss: 2.8324
2022-03-04 20:20:35 - train: epoch 0013, iter [08300, 10009], lr: 0.100000, loss: 2.5684
2022-03-04 20:21:07 - train: epoch 0013, iter [08400, 10009], lr: 0.100000, loss: 2.6634
2022-03-04 20:21:40 - train: epoch 0013, iter [08500, 10009], lr: 0.100000, loss: 3.1522
2022-03-04 20:22:13 - train: epoch 0013, iter [08600, 10009], lr: 0.100000, loss: 2.9274
2022-03-04 20:22:45 - train: epoch 0013, iter [08700, 10009], lr: 0.100000, loss: 2.7257
2022-03-04 20:23:18 - train: epoch 0013, iter [08800, 10009], lr: 0.100000, loss: 2.7312
2022-03-04 20:23:50 - train: epoch 0013, iter [08900, 10009], lr: 0.100000, loss: 2.7257
2022-03-04 20:24:23 - train: epoch 0013, iter [09000, 10009], lr: 0.100000, loss: 2.5644
2022-03-04 20:24:55 - train: epoch 0013, iter [09100, 10009], lr: 0.100000, loss: 2.8453
2022-03-04 20:25:28 - train: epoch 0013, iter [09200, 10009], lr: 0.100000, loss: 2.9260
2022-03-04 20:26:00 - train: epoch 0013, iter [09300, 10009], lr: 0.100000, loss: 2.8851
2022-03-04 20:26:33 - train: epoch 0013, iter [09400, 10009], lr: 0.100000, loss: 2.7129
2022-03-04 20:27:05 - train: epoch 0013, iter [09500, 10009], lr: 0.100000, loss: 2.6504
2022-03-04 20:27:38 - train: epoch 0013, iter [09600, 10009], lr: 0.100000, loss: 2.7389
2022-03-04 20:28:11 - train: epoch 0013, iter [09700, 10009], lr: 0.100000, loss: 2.7203
2022-03-04 20:28:43 - train: epoch 0013, iter [09800, 10009], lr: 0.100000, loss: 2.8817
2022-03-04 20:29:16 - train: epoch 0013, iter [09900, 10009], lr: 0.100000, loss: 2.4982
2022-03-04 20:29:48 - train: epoch 0013, iter [10000, 10009], lr: 0.100000, loss: 2.6695
2022-03-04 20:29:52 - train: epoch 013, train_loss: 2.7967
2022-03-04 20:31:06 - eval: epoch: 013, acc1: 46.312%, acc5: 72.536%, test_loss: 2.3684, per_image_load_time: 1.222ms, per_image_inference_time: 1.569ms
2022-03-04 20:31:07 - until epoch: 013, best_acc1: 46.312%
2022-03-04 20:31:07 - epoch 014 lr: 0.1
2022-03-04 20:31:43 - train: epoch 0014, iter [00100, 10009], lr: 0.100000, loss: 2.6864
2022-03-04 20:32:16 - train: epoch 0014, iter [00200, 10009], lr: 0.100000, loss: 2.8150
2022-03-04 20:32:48 - train: epoch 0014, iter [00300, 10009], lr: 0.100000, loss: 3.1099
2022-03-04 20:33:21 - train: epoch 0014, iter [00400, 10009], lr: 0.100000, loss: 2.8056
2022-03-04 20:33:53 - train: epoch 0014, iter [00500, 10009], lr: 0.100000, loss: 2.6563
2022-03-04 20:34:25 - train: epoch 0014, iter [00600, 10009], lr: 0.100000, loss: 2.6846
2022-03-04 20:34:58 - train: epoch 0014, iter [00700, 10009], lr: 0.100000, loss: 2.5761
2022-03-04 20:35:31 - train: epoch 0014, iter [00800, 10009], lr: 0.100000, loss: 2.8293
2022-03-04 20:36:03 - train: epoch 0014, iter [00900, 10009], lr: 0.100000, loss: 2.8636
2022-03-04 20:36:36 - train: epoch 0014, iter [01000, 10009], lr: 0.100000, loss: 2.7710
2022-03-04 20:37:09 - train: epoch 0014, iter [01100, 10009], lr: 0.100000, loss: 2.6109
2022-03-04 20:37:41 - train: epoch 0014, iter [01200, 10009], lr: 0.100000, loss: 2.8606
2022-03-04 20:38:14 - train: epoch 0014, iter [01300, 10009], lr: 0.100000, loss: 2.6253
2022-03-04 20:38:46 - train: epoch 0014, iter [01400, 10009], lr: 0.100000, loss: 2.5632
2022-03-04 20:39:19 - train: epoch 0014, iter [01500, 10009], lr: 0.100000, loss: 2.6983
2022-03-04 20:39:52 - train: epoch 0014, iter [01600, 10009], lr: 0.100000, loss: 2.7070
2022-03-04 20:40:24 - train: epoch 0014, iter [01700, 10009], lr: 0.100000, loss: 2.5534
2022-03-04 20:40:57 - train: epoch 0014, iter [01800, 10009], lr: 0.100000, loss: 2.6856
2022-03-04 20:41:29 - train: epoch 0014, iter [01900, 10009], lr: 0.100000, loss: 2.9646
2022-03-04 20:42:02 - train: epoch 0014, iter [02000, 10009], lr: 0.100000, loss: 2.4926
2022-03-04 20:42:35 - train: epoch 0014, iter [02100, 10009], lr: 0.100000, loss: 2.7889
2022-03-04 20:43:07 - train: epoch 0014, iter [02200, 10009], lr: 0.100000, loss: 2.7671
2022-03-04 20:43:40 - train: epoch 0014, iter [02300, 10009], lr: 0.100000, loss: 2.8697
2022-03-04 20:44:13 - train: epoch 0014, iter [02400, 10009], lr: 0.100000, loss: 2.5801
2022-03-04 20:44:46 - train: epoch 0014, iter [02500, 10009], lr: 0.100000, loss: 2.5324
2022-03-04 20:45:18 - train: epoch 0014, iter [02600, 10009], lr: 0.100000, loss: 2.5951
2022-03-04 20:45:51 - train: epoch 0014, iter [02700, 10009], lr: 0.100000, loss: 3.3200
2022-03-04 20:46:24 - train: epoch 0014, iter [02800, 10009], lr: 0.100000, loss: 2.6129
2022-03-04 20:46:56 - train: epoch 0014, iter [02900, 10009], lr: 0.100000, loss: 2.8103
2022-03-04 20:47:29 - train: epoch 0014, iter [03000, 10009], lr: 0.100000, loss: 2.6425
2022-03-04 20:48:02 - train: epoch 0014, iter [03100, 10009], lr: 0.100000, loss: 2.6511
2022-03-04 20:48:34 - train: epoch 0014, iter [03200, 10009], lr: 0.100000, loss: 2.7949
2022-03-04 20:49:07 - train: epoch 0014, iter [03300, 10009], lr: 0.100000, loss: 2.7024
2022-03-04 20:49:40 - train: epoch 0014, iter [03400, 10009], lr: 0.100000, loss: 2.8647
2022-03-04 20:50:13 - train: epoch 0014, iter [03500, 10009], lr: 0.100000, loss: 2.4090
2022-03-04 20:50:46 - train: epoch 0014, iter [03600, 10009], lr: 0.100000, loss: 2.8631
2022-03-04 20:51:19 - train: epoch 0014, iter [03700, 10009], lr: 0.100000, loss: 2.5988
2022-03-04 20:51:52 - train: epoch 0014, iter [03800, 10009], lr: 0.100000, loss: 2.5348
2022-03-04 20:52:25 - train: epoch 0014, iter [03900, 10009], lr: 0.100000, loss: 2.6553
2022-03-04 20:52:58 - train: epoch 0014, iter [04000, 10009], lr: 0.100000, loss: 2.3978
2022-03-04 20:53:31 - train: epoch 0014, iter [04100, 10009], lr: 0.100000, loss: 2.6058
2022-03-04 20:54:03 - train: epoch 0014, iter [04200, 10009], lr: 0.100000, loss: 3.0615
2022-03-04 20:54:36 - train: epoch 0014, iter [04300, 10009], lr: 0.100000, loss: 2.8305
2022-03-04 20:55:09 - train: epoch 0014, iter [04400, 10009], lr: 0.100000, loss: 2.4594
2022-03-04 20:55:42 - train: epoch 0014, iter [04500, 10009], lr: 0.100000, loss: 2.8145
2022-03-04 20:56:15 - train: epoch 0014, iter [04600, 10009], lr: 0.100000, loss: 2.6870
2022-03-04 20:56:48 - train: epoch 0014, iter [04700, 10009], lr: 0.100000, loss: 2.7209
2022-03-04 20:57:21 - train: epoch 0014, iter [04800, 10009], lr: 0.100000, loss: 2.9166
2022-03-04 20:57:54 - train: epoch 0014, iter [04900, 10009], lr: 0.100000, loss: 2.8191
2022-03-04 20:58:26 - train: epoch 0014, iter [05000, 10009], lr: 0.100000, loss: 2.9764
2022-03-04 20:58:59 - train: epoch 0014, iter [05100, 10009], lr: 0.100000, loss: 3.0703
2022-03-04 20:59:32 - train: epoch 0014, iter [05200, 10009], lr: 0.100000, loss: 2.3325
2022-03-04 21:00:04 - train: epoch 0014, iter [05300, 10009], lr: 0.100000, loss: 3.0093
2022-03-04 21:00:37 - train: epoch 0014, iter [05400, 10009], lr: 0.100000, loss: 2.6700
2022-03-04 21:01:10 - train: epoch 0014, iter [05500, 10009], lr: 0.100000, loss: 2.8635
2022-03-04 21:01:43 - train: epoch 0014, iter [05600, 10009], lr: 0.100000, loss: 3.0426
2022-03-04 21:02:15 - train: epoch 0014, iter [05700, 10009], lr: 0.100000, loss: 2.9048
2022-03-04 21:02:48 - train: epoch 0014, iter [05800, 10009], lr: 0.100000, loss: 2.8179
2022-03-04 21:03:21 - train: epoch 0014, iter [05900, 10009], lr: 0.100000, loss: 2.6684
2022-03-04 21:03:53 - train: epoch 0014, iter [06000, 10009], lr: 0.100000, loss: 3.0496
2022-03-04 21:04:26 - train: epoch 0014, iter [06100, 10009], lr: 0.100000, loss: 2.7168
2022-03-04 21:04:59 - train: epoch 0014, iter [06200, 10009], lr: 0.100000, loss: 2.8270
2022-03-04 21:05:32 - train: epoch 0014, iter [06300, 10009], lr: 0.100000, loss: 2.9462
2022-03-04 21:06:05 - train: epoch 0014, iter [06400, 10009], lr: 0.100000, loss: 2.3434
2022-03-04 21:06:37 - train: epoch 0014, iter [06500, 10009], lr: 0.100000, loss: 2.8800
2022-03-04 21:07:10 - train: epoch 0014, iter [06600, 10009], lr: 0.100000, loss: 2.3486
2022-03-04 21:07:42 - train: epoch 0014, iter [06700, 10009], lr: 0.100000, loss: 2.6871
2022-03-04 21:08:15 - train: epoch 0014, iter [06800, 10009], lr: 0.100000, loss: 2.7537
2022-03-04 21:08:48 - train: epoch 0014, iter [06900, 10009], lr: 0.100000, loss: 2.7316
2022-03-04 21:09:21 - train: epoch 0014, iter [07000, 10009], lr: 0.100000, loss: 2.7588
2022-03-04 21:09:53 - train: epoch 0014, iter [07100, 10009], lr: 0.100000, loss: 2.8997
2022-03-04 21:10:26 - train: epoch 0014, iter [07200, 10009], lr: 0.100000, loss: 2.7247
2022-03-04 21:10:59 - train: epoch 0014, iter [07300, 10009], lr: 0.100000, loss: 2.8459
2022-03-04 21:11:31 - train: epoch 0014, iter [07400, 10009], lr: 0.100000, loss: 2.4561
2022-03-04 21:12:04 - train: epoch 0014, iter [07500, 10009], lr: 0.100000, loss: 2.7692
2022-03-04 21:12:37 - train: epoch 0014, iter [07600, 10009], lr: 0.100000, loss: 2.8743
2022-03-04 21:13:09 - train: epoch 0014, iter [07700, 10009], lr: 0.100000, loss: 3.1508
2022-03-04 21:13:42 - train: epoch 0014, iter [07800, 10009], lr: 0.100000, loss: 3.0389
2022-03-04 21:14:15 - train: epoch 0014, iter [07900, 10009], lr: 0.100000, loss: 2.9567
2022-03-04 21:14:47 - train: epoch 0014, iter [08000, 10009], lr: 0.100000, loss: 2.9597
2022-03-04 21:15:20 - train: epoch 0014, iter [08100, 10009], lr: 0.100000, loss: 3.0206
2022-03-04 21:15:53 - train: epoch 0014, iter [08200, 10009], lr: 0.100000, loss: 2.7484
2022-03-04 21:16:26 - train: epoch 0014, iter [08300, 10009], lr: 0.100000, loss: 2.7270
2022-03-04 21:16:58 - train: epoch 0014, iter [08400, 10009], lr: 0.100000, loss: 2.9865
2022-03-04 21:17:31 - train: epoch 0014, iter [08500, 10009], lr: 0.100000, loss: 2.7015
2022-03-04 21:18:04 - train: epoch 0014, iter [08600, 10009], lr: 0.100000, loss: 2.3396
2022-03-04 21:18:36 - train: epoch 0014, iter [08700, 10009], lr: 0.100000, loss: 2.7618
2022-03-04 21:19:09 - train: epoch 0014, iter [08800, 10009], lr: 0.100000, loss: 2.6715
2022-03-04 21:19:42 - train: epoch 0014, iter [08900, 10009], lr: 0.100000, loss: 2.6992
2022-03-04 21:20:14 - train: epoch 0014, iter [09000, 10009], lr: 0.100000, loss: 3.0861
2022-03-04 21:20:47 - train: epoch 0014, iter [09100, 10009], lr: 0.100000, loss: 2.4417
2022-03-04 21:21:20 - train: epoch 0014, iter [09200, 10009], lr: 0.100000, loss: 2.9324
2022-03-04 21:21:53 - train: epoch 0014, iter [09300, 10009], lr: 0.100000, loss: 3.5836
2022-03-04 21:22:26 - train: epoch 0014, iter [09400, 10009], lr: 0.100000, loss: 2.9278
2022-03-04 21:22:58 - train: epoch 0014, iter [09500, 10009], lr: 0.100000, loss: 3.0008
2022-03-04 21:23:31 - train: epoch 0014, iter [09600, 10009], lr: 0.100000, loss: 2.6703
2022-03-04 21:24:04 - train: epoch 0014, iter [09700, 10009], lr: 0.100000, loss: 2.9131
2022-03-04 21:24:37 - train: epoch 0014, iter [09800, 10009], lr: 0.100000, loss: 2.6823
2022-03-04 21:25:09 - train: epoch 0014, iter [09900, 10009], lr: 0.100000, loss: 2.7596
2022-03-04 21:25:42 - train: epoch 0014, iter [10000, 10009], lr: 0.100000, loss: 2.5796
2022-03-04 21:25:46 - train: epoch 014, train_loss: 2.7756
2022-03-04 21:26:59 - eval: epoch: 014, acc1: 45.380%, acc5: 71.642%, test_loss: 2.4280, per_image_load_time: 0.815ms, per_image_inference_time: 1.548ms
2022-03-04 21:27:00 - until epoch: 014, best_acc1: 46.312%
2022-03-04 21:27:00 - epoch 015 lr: 0.1
2022-03-04 21:27:36 - train: epoch 0015, iter [00100, 10009], lr: 0.100000, loss: 2.7058
2022-03-04 21:28:09 - train: epoch 0015, iter [00200, 10009], lr: 0.100000, loss: 2.6648
2022-03-04 21:28:41 - train: epoch 0015, iter [00300, 10009], lr: 0.100000, loss: 2.6823
2022-03-04 21:29:14 - train: epoch 0015, iter [00400, 10009], lr: 0.100000, loss: 2.9113
2022-03-04 21:29:46 - train: epoch 0015, iter [00500, 10009], lr: 0.100000, loss: 2.6045
2022-03-04 21:30:19 - train: epoch 0015, iter [00600, 10009], lr: 0.100000, loss: 2.9792
2022-03-04 21:30:52 - train: epoch 0015, iter [00700, 10009], lr: 0.100000, loss: 2.7762
2022-03-04 21:31:24 - train: epoch 0015, iter [00800, 10009], lr: 0.100000, loss: 3.3469
2022-03-04 21:31:57 - train: epoch 0015, iter [00900, 10009], lr: 0.100000, loss: 2.9008
2022-03-04 21:32:29 - train: epoch 0015, iter [01000, 10009], lr: 0.100000, loss: 2.9293
2022-03-04 21:33:02 - train: epoch 0015, iter [01100, 10009], lr: 0.100000, loss: 2.3744
2022-03-04 21:33:35 - train: epoch 0015, iter [01200, 10009], lr: 0.100000, loss: 2.7783
2022-03-04 21:34:07 - train: epoch 0015, iter [01300, 10009], lr: 0.100000, loss: 2.6641
2022-03-04 21:34:40 - train: epoch 0015, iter [01400, 10009], lr: 0.100000, loss: 3.0722
2022-03-04 21:35:12 - train: epoch 0015, iter [01500, 10009], lr: 0.100000, loss: 2.3659
2022-03-04 21:35:45 - train: epoch 0015, iter [01600, 10009], lr: 0.100000, loss: 2.5636
2022-03-04 21:36:17 - train: epoch 0015, iter [01700, 10009], lr: 0.100000, loss: 2.7387
2022-03-04 21:36:50 - train: epoch 0015, iter [01800, 10009], lr: 0.100000, loss: 3.0566
2022-03-04 21:37:23 - train: epoch 0015, iter [01900, 10009], lr: 0.100000, loss: 2.7033
2022-03-04 21:37:55 - train: epoch 0015, iter [02000, 10009], lr: 0.100000, loss: 2.9034
2022-03-04 21:38:28 - train: epoch 0015, iter [02100, 10009], lr: 0.100000, loss: 2.6455
2022-03-04 21:39:01 - train: epoch 0015, iter [02200, 10009], lr: 0.100000, loss: 2.4181
2022-03-04 21:39:34 - train: epoch 0015, iter [02300, 10009], lr: 0.100000, loss: 2.5394
2022-03-04 21:40:07 - train: epoch 0015, iter [02400, 10009], lr: 0.100000, loss: 2.9390
2022-03-04 21:40:39 - train: epoch 0015, iter [02500, 10009], lr: 0.100000, loss: 2.8581
2022-03-04 21:41:12 - train: epoch 0015, iter [02600, 10009], lr: 0.100000, loss: 2.9517
2022-03-04 21:41:45 - train: epoch 0015, iter [02700, 10009], lr: 0.100000, loss: 2.6870
2022-03-04 21:42:17 - train: epoch 0015, iter [02800, 10009], lr: 0.100000, loss: 2.9202
2022-03-04 21:42:50 - train: epoch 0015, iter [02900, 10009], lr: 0.100000, loss: 2.4932
2022-03-04 21:43:23 - train: epoch 0015, iter [03000, 10009], lr: 0.100000, loss: 2.7425
2022-03-04 21:43:55 - train: epoch 0015, iter [03100, 10009], lr: 0.100000, loss: 2.7364
2022-03-04 21:44:28 - train: epoch 0015, iter [03200, 10009], lr: 0.100000, loss: 2.7527
2022-03-04 21:45:01 - train: epoch 0015, iter [03300, 10009], lr: 0.100000, loss: 3.2470
2022-03-04 21:45:33 - train: epoch 0015, iter [03400, 10009], lr: 0.100000, loss: 3.0519
2022-03-04 21:46:06 - train: epoch 0015, iter [03500, 10009], lr: 0.100000, loss: 2.8496
2022-03-04 21:46:39 - train: epoch 0015, iter [03600, 10009], lr: 0.100000, loss: 2.4995
2022-03-04 21:47:11 - train: epoch 0015, iter [03700, 10009], lr: 0.100000, loss: 3.0847
2022-03-04 21:47:44 - train: epoch 0015, iter [03800, 10009], lr: 0.100000, loss: 2.5769
2022-03-04 21:48:17 - train: epoch 0015, iter [03900, 10009], lr: 0.100000, loss: 3.1128
2022-03-04 21:48:49 - train: epoch 0015, iter [04000, 10009], lr: 0.100000, loss: 2.5735
2022-03-04 21:49:22 - train: epoch 0015, iter [04100, 10009], lr: 0.100000, loss: 2.7417
2022-03-04 21:49:55 - train: epoch 0015, iter [04200, 10009], lr: 0.100000, loss: 2.8845
2022-03-04 21:50:27 - train: epoch 0015, iter [04300, 10009], lr: 0.100000, loss: 2.5299
2022-03-04 21:51:00 - train: epoch 0015, iter [04400, 10009], lr: 0.100000, loss: 2.5257
2022-03-04 21:51:33 - train: epoch 0015, iter [04500, 10009], lr: 0.100000, loss: 2.8774
2022-03-04 21:52:06 - train: epoch 0015, iter [04600, 10009], lr: 0.100000, loss: 2.8620
2022-03-04 21:52:38 - train: epoch 0015, iter [04700, 10009], lr: 0.100000, loss: 2.7072
2022-03-04 21:53:11 - train: epoch 0015, iter [04800, 10009], lr: 0.100000, loss: 3.0591
2022-03-04 21:53:44 - train: epoch 0015, iter [04900, 10009], lr: 0.100000, loss: 3.3119
2022-03-04 21:54:17 - train: epoch 0015, iter [05000, 10009], lr: 0.100000, loss: 3.1589
2022-03-04 21:54:50 - train: epoch 0015, iter [05100, 10009], lr: 0.100000, loss: 2.8907
2022-03-04 21:55:22 - train: epoch 0015, iter [05200, 10009], lr: 0.100000, loss: 2.6042
2022-03-04 21:55:55 - train: epoch 0015, iter [05300, 10009], lr: 0.100000, loss: 2.3970
2022-03-04 21:56:28 - train: epoch 0015, iter [05400, 10009], lr: 0.100000, loss: 2.8644
2022-03-04 21:57:01 - train: epoch 0015, iter [05500, 10009], lr: 0.100000, loss: 2.7095
2022-03-04 21:57:34 - train: epoch 0015, iter [05600, 10009], lr: 0.100000, loss: 3.1017
2022-03-04 21:58:06 - train: epoch 0015, iter [05700, 10009], lr: 0.100000, loss: 2.7248
2022-03-04 21:58:39 - train: epoch 0015, iter [05800, 10009], lr: 0.100000, loss: 2.7928
2022-03-04 21:59:12 - train: epoch 0015, iter [05900, 10009], lr: 0.100000, loss: 3.0615
2022-03-04 21:59:45 - train: epoch 0015, iter [06000, 10009], lr: 0.100000, loss: 2.6888
2022-03-04 22:00:18 - train: epoch 0015, iter [06100, 10009], lr: 0.100000, loss: 2.8073
2022-03-04 22:00:51 - train: epoch 0015, iter [06200, 10009], lr: 0.100000, loss: 2.7625
2022-03-04 22:01:23 - train: epoch 0015, iter [06300, 10009], lr: 0.100000, loss: 3.0829
2022-03-04 22:01:56 - train: epoch 0015, iter [06400, 10009], lr: 0.100000, loss: 2.6538
2022-03-04 22:02:29 - train: epoch 0015, iter [06500, 10009], lr: 0.100000, loss: 2.5127
2022-03-04 22:03:02 - train: epoch 0015, iter [06600, 10009], lr: 0.100000, loss: 2.7675
2022-03-04 22:03:35 - train: epoch 0015, iter [06700, 10009], lr: 0.100000, loss: 2.8189
2022-03-04 22:04:08 - train: epoch 0015, iter [06800, 10009], lr: 0.100000, loss: 2.7392
2022-03-04 22:04:41 - train: epoch 0015, iter [06900, 10009], lr: 0.100000, loss: 2.7423
2022-03-04 22:05:14 - train: epoch 0015, iter [07000, 10009], lr: 0.100000, loss: 2.5957
2022-03-04 22:05:47 - train: epoch 0015, iter [07100, 10009], lr: 0.100000, loss: 2.7804
2022-03-04 22:06:19 - train: epoch 0015, iter [07200, 10009], lr: 0.100000, loss: 2.9226
2022-03-04 22:06:52 - train: epoch 0015, iter [07300, 10009], lr: 0.100000, loss: 2.8488
2022-03-04 22:07:25 - train: epoch 0015, iter [07400, 10009], lr: 0.100000, loss: 2.5861
2022-03-04 22:07:58 - train: epoch 0015, iter [07500, 10009], lr: 0.100000, loss: 2.4438
2022-03-04 22:08:31 - train: epoch 0015, iter [07600, 10009], lr: 0.100000, loss: 2.8163
2022-03-04 22:09:04 - train: epoch 0015, iter [07700, 10009], lr: 0.100000, loss: 2.8731
2022-03-04 22:09:37 - train: epoch 0015, iter [07800, 10009], lr: 0.100000, loss: 3.0457
2022-03-04 22:10:10 - train: epoch 0015, iter [07900, 10009], lr: 0.100000, loss: 3.0921
2022-03-04 22:10:43 - train: epoch 0015, iter [08000, 10009], lr: 0.100000, loss: 2.7015
2022-03-04 22:11:16 - train: epoch 0015, iter [08100, 10009], lr: 0.100000, loss: 2.5063
2022-03-04 22:11:49 - train: epoch 0015, iter [08200, 10009], lr: 0.100000, loss: 2.7852
2022-03-04 22:12:22 - train: epoch 0015, iter [08300, 10009], lr: 0.100000, loss: 2.9768
2022-03-04 22:12:54 - train: epoch 0015, iter [08400, 10009], lr: 0.100000, loss: 2.7124
2022-03-04 22:13:27 - train: epoch 0015, iter [08500, 10009], lr: 0.100000, loss: 2.7941
2022-03-04 22:14:00 - train: epoch 0015, iter [08600, 10009], lr: 0.100000, loss: 2.3842
2022-03-04 22:14:33 - train: epoch 0015, iter [08700, 10009], lr: 0.100000, loss: 3.0835
2022-03-04 22:15:06 - train: epoch 0015, iter [08800, 10009], lr: 0.100000, loss: 2.5744
2022-03-04 22:15:38 - train: epoch 0015, iter [08900, 10009], lr: 0.100000, loss: 2.8850
2022-03-04 22:16:11 - train: epoch 0015, iter [09000, 10009], lr: 0.100000, loss: 2.4807
2022-03-04 22:16:44 - train: epoch 0015, iter [09100, 10009], lr: 0.100000, loss: 2.8647
2022-03-04 22:17:17 - train: epoch 0015, iter [09200, 10009], lr: 0.100000, loss: 2.7939
2022-03-04 22:17:49 - train: epoch 0015, iter [09300, 10009], lr: 0.100000, loss: 2.9470
2022-03-04 22:18:22 - train: epoch 0015, iter [09400, 10009], lr: 0.100000, loss: 2.9018
2022-03-04 22:18:55 - train: epoch 0015, iter [09500, 10009], lr: 0.100000, loss: 2.4642
2022-03-04 22:19:28 - train: epoch 0015, iter [09600, 10009], lr: 0.100000, loss: 2.7910
2022-03-04 22:20:01 - train: epoch 0015, iter [09700, 10009], lr: 0.100000, loss: 2.5673
2022-03-04 22:20:34 - train: epoch 0015, iter [09800, 10009], lr: 0.100000, loss: 2.6241
2022-03-04 22:21:07 - train: epoch 0015, iter [09900, 10009], lr: 0.100000, loss: 2.9257
2022-03-04 22:21:40 - train: epoch 0015, iter [10000, 10009], lr: 0.100000, loss: 2.5850
2022-03-04 22:21:43 - train: epoch 015, train_loss: 2.7584
2022-03-04 22:23:00 - eval: epoch: 015, acc1: 46.606%, acc5: 72.066%, test_loss: 2.3697, per_image_load_time: 1.378ms, per_image_inference_time: 1.540ms
2022-03-04 22:23:00 - until epoch: 015, best_acc1: 46.606%
2022-03-05 09:56:07 - epoch 016 lr: 0.1
2022-03-05 09:56:43 - train: epoch 0016, iter [00100, 10009], lr: 0.100000, loss: 2.7765
2022-03-05 09:57:16 - train: epoch 0016, iter [00200, 10009], lr: 0.100000, loss: 2.6949
2022-03-05 09:57:48 - train: epoch 0016, iter [00300, 10009], lr: 0.100000, loss: 2.3532
2022-03-05 09:58:21 - train: epoch 0016, iter [00400, 10009], lr: 0.100000, loss: 2.7276
2022-03-05 09:58:53 - train: epoch 0016, iter [00500, 10009], lr: 0.100000, loss: 2.9453
2022-03-05 09:59:26 - train: epoch 0016, iter [00600, 10009], lr: 0.100000, loss: 3.2078
2022-03-05 09:59:58 - train: epoch 0016, iter [00700, 10009], lr: 0.100000, loss: 2.8044
2022-03-05 10:00:31 - train: epoch 0016, iter [00800, 10009], lr: 0.100000, loss: 2.6132
2022-03-05 10:01:04 - train: epoch 0016, iter [00900, 10009], lr: 0.100000, loss: 2.7850
2022-03-05 10:01:36 - train: epoch 0016, iter [01000, 10009], lr: 0.100000, loss: 2.5192
2022-03-05 10:02:09 - train: epoch 0016, iter [01100, 10009], lr: 0.100000, loss: 2.7211
2022-03-05 10:02:41 - train: epoch 0016, iter [01200, 10009], lr: 0.100000, loss: 2.5625
2022-03-05 10:03:14 - train: epoch 0016, iter [01300, 10009], lr: 0.100000, loss: 2.6939
2022-03-05 10:03:46 - train: epoch 0016, iter [01400, 10009], lr: 0.100000, loss: 2.8264
2022-03-05 10:04:19 - train: epoch 0016, iter [01500, 10009], lr: 0.100000, loss: 2.9830
2022-03-05 10:04:51 - train: epoch 0016, iter [01600, 10009], lr: 0.100000, loss: 2.4160
2022-03-05 10:05:24 - train: epoch 0016, iter [01700, 10009], lr: 0.100000, loss: 2.9627
2022-03-05 10:05:57 - train: epoch 0016, iter [01800, 10009], lr: 0.100000, loss: 3.0656
2022-03-05 10:06:29 - train: epoch 0016, iter [01900, 10009], lr: 0.100000, loss: 2.7235
2022-03-05 10:07:02 - train: epoch 0016, iter [02000, 10009], lr: 0.100000, loss: 2.5618
2022-03-05 10:07:35 - train: epoch 0016, iter [02100, 10009], lr: 0.100000, loss: 2.9078
2022-03-05 10:08:08 - train: epoch 0016, iter [02200, 10009], lr: 0.100000, loss: 2.7106
2022-03-05 10:08:40 - train: epoch 0016, iter [02300, 10009], lr: 0.100000, loss: 2.8298
2022-03-05 10:09:13 - train: epoch 0016, iter [02400, 10009], lr: 0.100000, loss: 2.5136
2022-03-05 10:09:46 - train: epoch 0016, iter [02500, 10009], lr: 0.100000, loss: 2.5033
2022-03-05 10:10:19 - train: epoch 0016, iter [02600, 10009], lr: 0.100000, loss: 2.5758
2022-03-05 10:10:51 - train: epoch 0016, iter [02700, 10009], lr: 0.100000, loss: 3.1036
2022-03-05 10:11:24 - train: epoch 0016, iter [02800, 10009], lr: 0.100000, loss: 2.5753
2022-03-05 10:11:57 - train: epoch 0016, iter [02900, 10009], lr: 0.100000, loss: 2.8156
2022-03-05 10:12:29 - train: epoch 0016, iter [03000, 10009], lr: 0.100000, loss: 3.0039
2022-03-05 10:13:02 - train: epoch 0016, iter [03100, 10009], lr: 0.100000, loss: 3.0734
2022-03-05 10:13:35 - train: epoch 0016, iter [03200, 10009], lr: 0.100000, loss: 2.8640
2022-03-05 10:14:07 - train: epoch 0016, iter [03300, 10009], lr: 0.100000, loss: 2.8053
2022-03-05 10:14:40 - train: epoch 0016, iter [03400, 10009], lr: 0.100000, loss: 2.9666
2022-03-05 10:15:13 - train: epoch 0016, iter [03500, 10009], lr: 0.100000, loss: 2.8113
2022-03-05 10:15:45 - train: epoch 0016, iter [03600, 10009], lr: 0.100000, loss: 2.8969
2022-03-05 10:16:18 - train: epoch 0016, iter [03700, 10009], lr: 0.100000, loss: 2.5429
2022-03-05 10:16:51 - train: epoch 0016, iter [03800, 10009], lr: 0.100000, loss: 2.7094
2022-03-05 10:17:24 - train: epoch 0016, iter [03900, 10009], lr: 0.100000, loss: 2.6709
2022-03-05 10:17:56 - train: epoch 0016, iter [04000, 10009], lr: 0.100000, loss: 2.0966
2022-03-05 10:18:29 - train: epoch 0016, iter [04100, 10009], lr: 0.100000, loss: 2.9316
2022-03-05 10:19:01 - train: epoch 0016, iter [04200, 10009], lr: 0.100000, loss: 2.7568
2022-03-05 10:19:34 - train: epoch 0016, iter [04300, 10009], lr: 0.100000, loss: 2.7185
2022-03-05 10:20:07 - train: epoch 0016, iter [04400, 10009], lr: 0.100000, loss: 2.6497
2022-03-05 10:20:39 - train: epoch 0016, iter [04500, 10009], lr: 0.100000, loss: 2.9956
2022-03-05 10:21:12 - train: epoch 0016, iter [04600, 10009], lr: 0.100000, loss: 2.8070
2022-03-05 10:21:45 - train: epoch 0016, iter [04700, 10009], lr: 0.100000, loss: 2.2031
2022-03-05 10:22:17 - train: epoch 0016, iter [04800, 10009], lr: 0.100000, loss: 3.0143
2022-03-05 10:22:50 - train: epoch 0016, iter [04900, 10009], lr: 0.100000, loss: 2.7988
2022-03-05 10:23:23 - train: epoch 0016, iter [05000, 10009], lr: 0.100000, loss: 2.6699
2022-03-05 10:23:55 - train: epoch 0016, iter [05100, 10009], lr: 0.100000, loss: 2.3582
2022-03-05 10:24:28 - train: epoch 0016, iter [05200, 10009], lr: 0.100000, loss: 2.6378
2022-03-05 10:25:01 - train: epoch 0016, iter [05300, 10009], lr: 0.100000, loss: 2.9256
2022-03-05 10:25:33 - train: epoch 0016, iter [05400, 10009], lr: 0.100000, loss: 2.6280
2022-03-05 10:26:06 - train: epoch 0016, iter [05500, 10009], lr: 0.100000, loss: 2.8763
2022-03-05 10:26:39 - train: epoch 0016, iter [05600, 10009], lr: 0.100000, loss: 2.7492
2022-03-05 10:27:12 - train: epoch 0016, iter [05700, 10009], lr: 0.100000, loss: 2.5244
2022-03-05 10:27:44 - train: epoch 0016, iter [05800, 10009], lr: 0.100000, loss: 2.4746
2022-03-05 10:28:17 - train: epoch 0016, iter [05900, 10009], lr: 0.100000, loss: 2.7337
2022-03-05 10:28:50 - train: epoch 0016, iter [06000, 10009], lr: 0.100000, loss: 2.7805
2022-03-05 10:29:22 - train: epoch 0016, iter [06100, 10009], lr: 0.100000, loss: 2.5808
2022-03-05 10:29:55 - train: epoch 0016, iter [06200, 10009], lr: 0.100000, loss: 2.6135
2022-03-05 10:30:28 - train: epoch 0016, iter [06300, 10009], lr: 0.100000, loss: 2.4868
2022-03-05 10:31:01 - train: epoch 0016, iter [06400, 10009], lr: 0.100000, loss: 2.9535
2022-03-05 10:31:34 - train: epoch 0016, iter [06500, 10009], lr: 0.100000, loss: 2.8870
2022-03-05 10:32:07 - train: epoch 0016, iter [06600, 10009], lr: 0.100000, loss: 2.8002
2022-03-05 10:32:40 - train: epoch 0016, iter [06700, 10009], lr: 0.100000, loss: 2.9025
2022-03-05 10:33:12 - train: epoch 0016, iter [06800, 10009], lr: 0.100000, loss: 2.7177
2022-03-05 10:33:45 - train: epoch 0016, iter [06900, 10009], lr: 0.100000, loss: 2.7762
2022-03-05 10:34:18 - train: epoch 0016, iter [07000, 10009], lr: 0.100000, loss: 2.6974
2022-03-05 10:34:51 - train: epoch 0016, iter [07100, 10009], lr: 0.100000, loss: 2.6702
2022-03-05 10:35:24 - train: epoch 0016, iter [07200, 10009], lr: 0.100000, loss: 2.6650
2022-03-05 10:35:57 - train: epoch 0016, iter [07300, 10009], lr: 0.100000, loss: 2.6692
2022-03-05 10:36:30 - train: epoch 0016, iter [07400, 10009], lr: 0.100000, loss: 2.9735
2022-03-05 10:37:03 - train: epoch 0016, iter [07500, 10009], lr: 0.100000, loss: 3.4945
2022-03-05 10:37:36 - train: epoch 0016, iter [07600, 10009], lr: 0.100000, loss: 2.8824
2022-03-05 10:38:09 - train: epoch 0016, iter [07700, 10009], lr: 0.100000, loss: 2.6930
2022-03-05 10:38:42 - train: epoch 0016, iter [07800, 10009], lr: 0.100000, loss: 2.5769
2022-03-05 10:39:15 - train: epoch 0016, iter [07900, 10009], lr: 0.100000, loss: 2.7105
2022-03-05 10:39:48 - train: epoch 0016, iter [08000, 10009], lr: 0.100000, loss: 3.1991
2022-03-05 10:40:21 - train: epoch 0016, iter [08100, 10009], lr: 0.100000, loss: 3.0021
2022-03-05 10:40:54 - train: epoch 0016, iter [08200, 10009], lr: 0.100000, loss: 2.5732
2022-03-05 10:41:27 - train: epoch 0016, iter [08300, 10009], lr: 0.100000, loss: 2.7027
2022-03-05 10:42:00 - train: epoch 0016, iter [08400, 10009], lr: 0.100000, loss: 2.7060
2022-03-05 10:42:33 - train: epoch 0016, iter [08500, 10009], lr: 0.100000, loss: 2.9428
2022-03-05 10:43:06 - train: epoch 0016, iter [08600, 10009], lr: 0.100000, loss: 2.6813
2022-03-05 10:43:39 - train: epoch 0016, iter [08700, 10009], lr: 0.100000, loss: 2.5599
2022-03-05 10:44:12 - train: epoch 0016, iter [08800, 10009], lr: 0.100000, loss: 2.3661
2022-03-05 10:44:45 - train: epoch 0016, iter [08900, 10009], lr: 0.100000, loss: 2.6807
2022-03-05 10:45:17 - train: epoch 0016, iter [09000, 10009], lr: 0.100000, loss: 2.6747
2022-03-05 10:45:51 - train: epoch 0016, iter [09100, 10009], lr: 0.100000, loss: 2.8899
2022-03-05 10:46:23 - train: epoch 0016, iter [09200, 10009], lr: 0.100000, loss: 2.5912
2022-03-05 10:46:56 - train: epoch 0016, iter [09300, 10009], lr: 0.100000, loss: 2.9266
2022-03-05 10:47:29 - train: epoch 0016, iter [09400, 10009], lr: 0.100000, loss: 2.8927
2022-03-05 10:48:02 - train: epoch 0016, iter [09500, 10009], lr: 0.100000, loss: 2.9611
2022-03-05 10:48:35 - train: epoch 0016, iter [09600, 10009], lr: 0.100000, loss: 2.8260
2022-03-05 10:49:08 - train: epoch 0016, iter [09700, 10009], lr: 0.100000, loss: 3.0072
2022-03-05 10:49:41 - train: epoch 0016, iter [09800, 10009], lr: 0.100000, loss: 3.0593
2022-03-05 10:50:14 - train: epoch 0016, iter [09900, 10009], lr: 0.100000, loss: 2.6069
2022-03-05 10:50:47 - train: epoch 0016, iter [10000, 10009], lr: 0.100000, loss: 2.5284
2022-03-05 10:50:50 - train: epoch 016, train_loss: 2.7448
2022-03-05 10:52:05 - eval: epoch: 016, acc1: 46.600%, acc5: 72.542%, test_loss: 2.3596, per_image_load_time: 1.051ms, per_image_inference_time: 1.566ms
2022-03-05 10:52:05 - until epoch: 016, best_acc1: 46.606%
2022-03-05 10:52:05 - epoch 017 lr: 0.1
2022-03-05 10:52:41 - train: epoch 0017, iter [00100, 10009], lr: 0.100000, loss: 2.9385
2022-03-05 10:53:14 - train: epoch 0017, iter [00200, 10009], lr: 0.100000, loss: 2.8600
2022-03-05 10:53:47 - train: epoch 0017, iter [00300, 10009], lr: 0.100000, loss: 2.9497
2022-03-05 10:54:20 - train: epoch 0017, iter [00400, 10009], lr: 0.100000, loss: 2.6423
2022-03-05 10:54:52 - train: epoch 0017, iter [00500, 10009], lr: 0.100000, loss: 2.9985
2022-03-05 10:55:25 - train: epoch 0017, iter [00600, 10009], lr: 0.100000, loss: 3.3141
2022-03-05 10:55:58 - train: epoch 0017, iter [00700, 10009], lr: 0.100000, loss: 2.5534
2022-03-05 10:56:31 - train: epoch 0017, iter [00800, 10009], lr: 0.100000, loss: 2.3295
2022-03-05 10:57:04 - train: epoch 0017, iter [00900, 10009], lr: 0.100000, loss: 3.0581
2022-03-05 10:57:37 - train: epoch 0017, iter [01000, 10009], lr: 0.100000, loss: 2.6571
2022-03-05 10:58:10 - train: epoch 0017, iter [01100, 10009], lr: 0.100000, loss: 2.9440
2022-03-05 10:58:43 - train: epoch 0017, iter [01200, 10009], lr: 0.100000, loss: 2.5809
2022-03-05 10:59:16 - train: epoch 0017, iter [01300, 10009], lr: 0.100000, loss: 2.4207
2022-03-05 10:59:49 - train: epoch 0017, iter [01400, 10009], lr: 0.100000, loss: 2.6690
2022-03-05 11:00:21 - train: epoch 0017, iter [01500, 10009], lr: 0.100000, loss: 3.1684
2022-03-05 11:00:54 - train: epoch 0017, iter [01600, 10009], lr: 0.100000, loss: 2.6094
2022-03-05 11:01:27 - train: epoch 0017, iter [01700, 10009], lr: 0.100000, loss: 2.2372
2022-03-05 11:02:00 - train: epoch 0017, iter [01800, 10009], lr: 0.100000, loss: 2.3510
2022-03-05 11:02:33 - train: epoch 0017, iter [01900, 10009], lr: 0.100000, loss: 2.8217
2022-03-05 11:03:05 - train: epoch 0017, iter [02000, 10009], lr: 0.100000, loss: 2.8883
2022-03-05 11:03:38 - train: epoch 0017, iter [02100, 10009], lr: 0.100000, loss: 2.5514
2022-03-05 11:04:11 - train: epoch 0017, iter [02200, 10009], lr: 0.100000, loss: 3.0534
2022-03-05 11:04:44 - train: epoch 0017, iter [02300, 10009], lr: 0.100000, loss: 2.9150
2022-03-05 11:05:16 - train: epoch 0017, iter [02400, 10009], lr: 0.100000, loss: 2.4252
2022-03-05 11:05:49 - train: epoch 0017, iter [02500, 10009], lr: 0.100000, loss: 3.0271
2022-03-05 11:06:22 - train: epoch 0017, iter [02600, 10009], lr: 0.100000, loss: 2.7120
2022-03-05 11:06:54 - train: epoch 0017, iter [02700, 10009], lr: 0.100000, loss: 3.0171
2022-03-05 11:07:27 - train: epoch 0017, iter [02800, 10009], lr: 0.100000, loss: 2.9746
2022-03-05 11:08:00 - train: epoch 0017, iter [02900, 10009], lr: 0.100000, loss: 2.9118
2022-03-05 11:08:32 - train: epoch 0017, iter [03000, 10009], lr: 0.100000, loss: 2.6672
2022-03-05 11:09:05 - train: epoch 0017, iter [03100, 10009], lr: 0.100000, loss: 2.3818
2022-03-05 11:09:38 - train: epoch 0017, iter [03200, 10009], lr: 0.100000, loss: 2.6055
2022-03-05 11:10:10 - train: epoch 0017, iter [03300, 10009], lr: 0.100000, loss: 2.4975
2022-03-05 11:10:43 - train: epoch 0017, iter [03400, 10009], lr: 0.100000, loss: 2.6728
2022-03-05 11:11:16 - train: epoch 0017, iter [03500, 10009], lr: 0.100000, loss: 2.4948
2022-03-05 11:11:48 - train: epoch 0017, iter [03600, 10009], lr: 0.100000, loss: 2.9408
2022-03-05 11:12:21 - train: epoch 0017, iter [03700, 10009], lr: 0.100000, loss: 2.4106
2022-03-05 11:12:54 - train: epoch 0017, iter [03800, 10009], lr: 0.100000, loss: 2.5455
2022-03-05 11:13:26 - train: epoch 0017, iter [03900, 10009], lr: 0.100000, loss: 2.8901
2022-03-05 11:13:59 - train: epoch 0017, iter [04000, 10009], lr: 0.100000, loss: 2.8404
2022-03-05 11:14:32 - train: epoch 0017, iter [04100, 10009], lr: 0.100000, loss: 2.7498
2022-03-05 11:15:05 - train: epoch 0017, iter [04200, 10009], lr: 0.100000, loss: 2.7492
2022-03-05 11:15:37 - train: epoch 0017, iter [04300, 10009], lr: 0.100000, loss: 2.8750
2022-03-05 11:16:10 - train: epoch 0017, iter [04400, 10009], lr: 0.100000, loss: 2.5792
2022-03-05 11:16:43 - train: epoch 0017, iter [04500, 10009], lr: 0.100000, loss: 2.7236
2022-03-05 11:17:15 - train: epoch 0017, iter [04600, 10009], lr: 0.100000, loss: 2.5904
2022-03-05 11:17:48 - train: epoch 0017, iter [04700, 10009], lr: 0.100000, loss: 2.5228
2022-03-05 11:18:21 - train: epoch 0017, iter [04800, 10009], lr: 0.100000, loss: 2.5991
2022-03-05 11:18:53 - train: epoch 0017, iter [04900, 10009], lr: 0.100000, loss: 2.7093
2022-03-05 11:19:26 - train: epoch 0017, iter [05000, 10009], lr: 0.100000, loss: 2.5563
2022-03-05 11:19:58 - train: epoch 0017, iter [05100, 10009], lr: 0.100000, loss: 2.7418
2022-03-05 11:20:31 - train: epoch 0017, iter [05200, 10009], lr: 0.100000, loss: 2.5424
2022-03-05 11:21:04 - train: epoch 0017, iter [05300, 10009], lr: 0.100000, loss: 2.7027
2022-03-05 11:21:36 - train: epoch 0017, iter [05400, 10009], lr: 0.100000, loss: 2.5813
2022-03-05 11:22:09 - train: epoch 0017, iter [05500, 10009], lr: 0.100000, loss: 2.6257
2022-03-05 11:22:42 - train: epoch 0017, iter [05600, 10009], lr: 0.100000, loss: 2.6359
2022-03-05 11:23:14 - train: epoch 0017, iter [05700, 10009], lr: 0.100000, loss: 2.5896
2022-03-05 11:23:47 - train: epoch 0017, iter [05800, 10009], lr: 0.100000, loss: 2.9992
2022-03-05 11:24:20 - train: epoch 0017, iter [05900, 10009], lr: 0.100000, loss: 2.6961
2022-03-05 11:24:52 - train: epoch 0017, iter [06000, 10009], lr: 0.100000, loss: 2.5230
2022-03-05 11:25:25 - train: epoch 0017, iter [06100, 10009], lr: 0.100000, loss: 2.3736
2022-03-05 11:25:58 - train: epoch 0017, iter [06200, 10009], lr: 0.100000, loss: 2.8542
2022-03-05 11:26:31 - train: epoch 0017, iter [06300, 10009], lr: 0.100000, loss: 2.9423
2022-03-05 11:27:03 - train: epoch 0017, iter [06400, 10009], lr: 0.100000, loss: 2.4198
2022-03-05 11:27:36 - train: epoch 0017, iter [06500, 10009], lr: 0.100000, loss: 2.5776
2022-03-05 11:28:09 - train: epoch 0017, iter [06600, 10009], lr: 0.100000, loss: 2.8993
2022-03-05 11:28:41 - train: epoch 0017, iter [06700, 10009], lr: 0.100000, loss: 2.5637
2022-03-05 11:29:14 - train: epoch 0017, iter [06800, 10009], lr: 0.100000, loss: 2.7443
2022-03-05 11:29:47 - train: epoch 0017, iter [06900, 10009], lr: 0.100000, loss: 2.7019
2022-03-05 11:30:19 - train: epoch 0017, iter [07000, 10009], lr: 0.100000, loss: 2.6836
2022-03-05 11:30:52 - train: epoch 0017, iter [07100, 10009], lr: 0.100000, loss: 2.5934
2022-03-05 11:31:25 - train: epoch 0017, iter [07200, 10009], lr: 0.100000, loss: 2.7714
2022-03-05 11:31:57 - train: epoch 0017, iter [07300, 10009], lr: 0.100000, loss: 2.3908
2022-03-05 11:32:30 - train: epoch 0017, iter [07400, 10009], lr: 0.100000, loss: 2.6605
2022-03-05 11:33:03 - train: epoch 0017, iter [07500, 10009], lr: 0.100000, loss: 3.1543
2022-03-05 11:33:35 - train: epoch 0017, iter [07600, 10009], lr: 0.100000, loss: 3.0820
2022-03-05 11:34:08 - train: epoch 0017, iter [07700, 10009], lr: 0.100000, loss: 3.0196
2022-03-05 11:34:41 - train: epoch 0017, iter [07800, 10009], lr: 0.100000, loss: 2.7211
2022-03-05 11:35:14 - train: epoch 0017, iter [07900, 10009], lr: 0.100000, loss: 2.9535
2022-03-05 11:35:46 - train: epoch 0017, iter [08000, 10009], lr: 0.100000, loss: 2.8152
2022-03-05 11:36:19 - train: epoch 0017, iter [08100, 10009], lr: 0.100000, loss: 2.6012
2022-03-05 11:36:52 - train: epoch 0017, iter [08200, 10009], lr: 0.100000, loss: 2.9430
2022-03-05 11:37:24 - train: epoch 0017, iter [08300, 10009], lr: 0.100000, loss: 2.7852
2022-03-05 11:37:57 - train: epoch 0017, iter [08400, 10009], lr: 0.100000, loss: 2.6035
2022-03-05 11:38:30 - train: epoch 0017, iter [08500, 10009], lr: 0.100000, loss: 2.9206
2022-03-05 11:39:02 - train: epoch 0017, iter [08600, 10009], lr: 0.100000, loss: 2.8454
2022-03-05 11:39:35 - train: epoch 0017, iter [08700, 10009], lr: 0.100000, loss: 2.9201
2022-03-05 11:40:08 - train: epoch 0017, iter [08800, 10009], lr: 0.100000, loss: 2.8080
2022-03-05 11:40:40 - train: epoch 0017, iter [08900, 10009], lr: 0.100000, loss: 2.8099
2022-03-05 11:41:13 - train: epoch 0017, iter [09000, 10009], lr: 0.100000, loss: 2.8056
2022-03-05 11:41:46 - train: epoch 0017, iter [09100, 10009], lr: 0.100000, loss: 2.7511
2022-03-05 11:42:18 - train: epoch 0017, iter [09200, 10009], lr: 0.100000, loss: 2.8373
2022-03-05 11:42:51 - train: epoch 0017, iter [09300, 10009], lr: 0.100000, loss: 2.7810
2022-03-05 11:43:24 - train: epoch 0017, iter [09400, 10009], lr: 0.100000, loss: 3.1507
2022-03-05 11:43:56 - train: epoch 0017, iter [09500, 10009], lr: 0.100000, loss: 2.8792
2022-03-05 11:44:29 - train: epoch 0017, iter [09600, 10009], lr: 0.100000, loss: 2.8585
2022-03-05 11:45:02 - train: epoch 0017, iter [09700, 10009], lr: 0.100000, loss: 2.7240
2022-03-05 11:45:34 - train: epoch 0017, iter [09800, 10009], lr: 0.100000, loss: 2.6319
2022-03-05 11:46:07 - train: epoch 0017, iter [09900, 10009], lr: 0.100000, loss: 2.7834
2022-03-05 11:46:40 - train: epoch 0017, iter [10000, 10009], lr: 0.100000, loss: 2.6338
2022-03-05 11:46:43 - train: epoch 017, train_loss: 2.7325
2022-03-05 11:47:58 - eval: epoch: 017, acc1: 47.312%, acc5: 73.392%, test_loss: 2.3172, per_image_load_time: 1.260ms, per_image_inference_time: 1.577ms
2022-03-05 11:47:59 - until epoch: 017, best_acc1: 47.312%
2022-03-05 11:47:59 - epoch 018 lr: 0.1
2022-03-05 11:48:34 - train: epoch 0018, iter [00100, 10009], lr: 0.100000, loss: 2.4161
2022-03-05 11:49:07 - train: epoch 0018, iter [00200, 10009], lr: 0.100000, loss: 2.6630
2022-03-05 11:49:40 - train: epoch 0018, iter [00300, 10009], lr: 0.100000, loss: 2.6097
2022-03-05 11:50:13 - train: epoch 0018, iter [00400, 10009], lr: 0.100000, loss: 2.9881
2022-03-05 11:50:45 - train: epoch 0018, iter [00500, 10009], lr: 0.100000, loss: 2.8850
2022-03-05 11:51:18 - train: epoch 0018, iter [00600, 10009], lr: 0.100000, loss: 2.8717
2022-03-05 11:51:50 - train: epoch 0018, iter [00700, 10009], lr: 0.100000, loss: 2.8482
2022-03-05 11:52:23 - train: epoch 0018, iter [00800, 10009], lr: 0.100000, loss: 3.0237
2022-03-05 11:52:56 - train: epoch 0018, iter [00900, 10009], lr: 0.100000, loss: 2.8810
2022-03-05 11:53:29 - train: epoch 0018, iter [01000, 10009], lr: 0.100000, loss: 2.7887
2022-03-05 11:54:01 - train: epoch 0018, iter [01100, 10009], lr: 0.100000, loss: 2.7536
2022-03-05 11:54:34 - train: epoch 0018, iter [01200, 10009], lr: 0.100000, loss: 2.7513
2022-03-05 11:55:07 - train: epoch 0018, iter [01300, 10009], lr: 0.100000, loss: 2.5802
2022-03-05 11:55:39 - train: epoch 0018, iter [01400, 10009], lr: 0.100000, loss: 2.5136
2022-03-05 11:56:12 - train: epoch 0018, iter [01500, 10009], lr: 0.100000, loss: 2.7571
2022-03-05 11:56:45 - train: epoch 0018, iter [01600, 10009], lr: 0.100000, loss: 2.8508
2022-03-05 11:57:17 - train: epoch 0018, iter [01700, 10009], lr: 0.100000, loss: 2.4598
2022-03-05 11:57:50 - train: epoch 0018, iter [01800, 10009], lr: 0.100000, loss: 2.6295
2022-03-05 11:58:23 - train: epoch 0018, iter [01900, 10009], lr: 0.100000, loss: 2.6277
2022-03-05 11:58:55 - train: epoch 0018, iter [02000, 10009], lr: 0.100000, loss: 2.5520
2022-03-05 11:59:28 - train: epoch 0018, iter [02100, 10009], lr: 0.100000, loss: 2.8847
2022-03-05 12:00:01 - train: epoch 0018, iter [02200, 10009], lr: 0.100000, loss: 3.0114
2022-03-05 12:00:33 - train: epoch 0018, iter [02300, 10009], lr: 0.100000, loss: 2.8418
2022-03-05 12:01:06 - train: epoch 0018, iter [02400, 10009], lr: 0.100000, loss: 2.6709
2022-03-05 12:01:39 - train: epoch 0018, iter [02500, 10009], lr: 0.100000, loss: 2.5170
2022-03-05 12:02:11 - train: epoch 0018, iter [02600, 10009], lr: 0.100000, loss: 3.1563
2022-03-05 12:02:44 - train: epoch 0018, iter [02700, 10009], lr: 0.100000, loss: 2.9473
2022-03-05 12:03:17 - train: epoch 0018, iter [02800, 10009], lr: 0.100000, loss: 2.7370
2022-03-05 12:03:49 - train: epoch 0018, iter [02900, 10009], lr: 0.100000, loss: 2.8521
2022-03-05 12:04:22 - train: epoch 0018, iter [03000, 10009], lr: 0.100000, loss: 3.1073
2022-03-05 12:04:55 - train: epoch 0018, iter [03100, 10009], lr: 0.100000, loss: 2.9330
2022-03-05 12:05:27 - train: epoch 0018, iter [03200, 10009], lr: 0.100000, loss: 2.4780
2022-03-05 12:06:00 - train: epoch 0018, iter [03300, 10009], lr: 0.100000, loss: 2.7027
2022-03-05 12:06:33 - train: epoch 0018, iter [03400, 10009], lr: 0.100000, loss: 2.7021
2022-03-05 12:07:05 - train: epoch 0018, iter [03500, 10009], lr: 0.100000, loss: 2.3369
2022-03-05 12:07:38 - train: epoch 0018, iter [03600, 10009], lr: 0.100000, loss: 2.7084
2022-03-05 12:08:10 - train: epoch 0018, iter [03700, 10009], lr: 0.100000, loss: 2.8794
2022-03-05 12:08:43 - train: epoch 0018, iter [03800, 10009], lr: 0.100000, loss: 2.8251
2022-03-05 12:09:16 - train: epoch 0018, iter [03900, 10009], lr: 0.100000, loss: 2.4479
2022-03-05 12:09:48 - train: epoch 0018, iter [04000, 10009], lr: 0.100000, loss: 3.1108
2022-03-05 12:10:21 - train: epoch 0018, iter [04100, 10009], lr: 0.100000, loss: 2.5271
2022-03-05 12:10:54 - train: epoch 0018, iter [04200, 10009], lr: 0.100000, loss: 3.0187
2022-03-05 12:11:26 - train: epoch 0018, iter [04300, 10009], lr: 0.100000, loss: 2.7990
2022-03-05 12:11:59 - train: epoch 0018, iter [04400, 10009], lr: 0.100000, loss: 2.7039
2022-03-05 12:12:31 - train: epoch 0018, iter [04500, 10009], lr: 0.100000, loss: 2.9025
2022-03-05 12:13:04 - train: epoch 0018, iter [04600, 10009], lr: 0.100000, loss: 3.0871
2022-03-05 12:13:36 - train: epoch 0018, iter [04700, 10009], lr: 0.100000, loss: 2.8839
2022-03-05 12:14:09 - train: epoch 0018, iter [04800, 10009], lr: 0.100000, loss: 2.5611
2022-03-05 12:14:42 - train: epoch 0018, iter [04900, 10009], lr: 0.100000, loss: 2.9700
2022-03-05 12:15:14 - train: epoch 0018, iter [05000, 10009], lr: 0.100000, loss: 2.6990
2022-03-05 12:15:47 - train: epoch 0018, iter [05100, 10009], lr: 0.100000, loss: 2.7806
2022-03-05 12:16:20 - train: epoch 0018, iter [05200, 10009], lr: 0.100000, loss: 2.8050
2022-03-05 12:16:52 - train: epoch 0018, iter [05300, 10009], lr: 0.100000, loss: 2.8974
2022-03-05 12:17:25 - train: epoch 0018, iter [05400, 10009], lr: 0.100000, loss: 2.6113
2022-03-05 12:17:58 - train: epoch 0018, iter [05500, 10009], lr: 0.100000, loss: 2.5503
2022-03-05 12:18:30 - train: epoch 0018, iter [05600, 10009], lr: 0.100000, loss: 2.5512
2022-03-05 12:19:03 - train: epoch 0018, iter [05700, 10009], lr: 0.100000, loss: 2.9746
2022-03-05 12:19:35 - train: epoch 0018, iter [05800, 10009], lr: 0.100000, loss: 2.6677
2022-03-05 12:20:08 - train: epoch 0018, iter [05900, 10009], lr: 0.100000, loss: 2.5991
2022-03-05 12:20:41 - train: epoch 0018, iter [06000, 10009], lr: 0.100000, loss: 2.4893
2022-03-05 12:21:14 - train: epoch 0018, iter [06100, 10009], lr: 0.100000, loss: 2.7983
2022-03-05 12:21:46 - train: epoch 0018, iter [06200, 10009], lr: 0.100000, loss: 2.9903
2022-03-05 12:22:19 - train: epoch 0018, iter [06300, 10009], lr: 0.100000, loss: 2.8336
2022-03-05 12:22:52 - train: epoch 0018, iter [06400, 10009], lr: 0.100000, loss: 2.7727
2022-03-05 12:23:24 - train: epoch 0018, iter [06500, 10009], lr: 0.100000, loss: 2.6897
2022-03-05 12:23:57 - train: epoch 0018, iter [06600, 10009], lr: 0.100000, loss: 2.6725
2022-03-05 12:24:30 - train: epoch 0018, iter [06700, 10009], lr: 0.100000, loss: 2.6742
2022-03-05 12:25:02 - train: epoch 0018, iter [06800, 10009], lr: 0.100000, loss: 2.3845
2022-03-05 12:25:35 - train: epoch 0018, iter [06900, 10009], lr: 0.100000, loss: 2.8536
2022-03-05 12:26:08 - train: epoch 0018, iter [07000, 10009], lr: 0.100000, loss: 2.3709
2022-03-05 12:26:41 - train: epoch 0018, iter [07100, 10009], lr: 0.100000, loss: 2.7146
2022-03-05 12:27:13 - train: epoch 0018, iter [07200, 10009], lr: 0.100000, loss: 2.7069
2022-03-05 12:27:46 - train: epoch 0018, iter [07300, 10009], lr: 0.100000, loss: 2.8376
2022-03-05 12:28:19 - train: epoch 0018, iter [07400, 10009], lr: 0.100000, loss: 2.8972
2022-03-05 12:28:51 - train: epoch 0018, iter [07500, 10009], lr: 0.100000, loss: 2.4460
2022-03-05 12:29:24 - train: epoch 0018, iter [07600, 10009], lr: 0.100000, loss: 3.2700
2022-03-05 12:29:57 - train: epoch 0018, iter [07700, 10009], lr: 0.100000, loss: 2.6699
2022-03-05 12:30:30 - train: epoch 0018, iter [07800, 10009], lr: 0.100000, loss: 2.4855
2022-03-05 12:31:02 - train: epoch 0018, iter [07900, 10009], lr: 0.100000, loss: 2.9444
2022-03-05 12:31:35 - train: epoch 0018, iter [08000, 10009], lr: 0.100000, loss: 2.7943
2022-03-05 12:32:08 - train: epoch 0018, iter [08100, 10009], lr: 0.100000, loss: 2.8838
2022-03-05 12:32:40 - train: epoch 0018, iter [08200, 10009], lr: 0.100000, loss: 2.6856
2022-03-05 12:33:13 - train: epoch 0018, iter [08300, 10009], lr: 0.100000, loss: 2.9546
2022-03-05 12:33:46 - train: epoch 0018, iter [08400, 10009], lr: 0.100000, loss: 2.6065
2022-03-05 12:34:19 - train: epoch 0018, iter [08500, 10009], lr: 0.100000, loss: 2.5359
2022-03-05 12:34:51 - train: epoch 0018, iter [08600, 10009], lr: 0.100000, loss: 2.6291
2022-03-05 12:35:24 - train: epoch 0018, iter [08700, 10009], lr: 0.100000, loss: 2.4109
2022-03-05 12:35:57 - train: epoch 0018, iter [08800, 10009], lr: 0.100000, loss: 2.6587
2022-03-05 12:36:29 - train: epoch 0018, iter [08900, 10009], lr: 0.100000, loss: 2.6388
2022-03-05 12:37:02 - train: epoch 0018, iter [09000, 10009], lr: 0.100000, loss: 2.7806
2022-03-05 12:37:35 - train: epoch 0018, iter [09100, 10009], lr: 0.100000, loss: 2.7950
2022-03-05 12:38:07 - train: epoch 0018, iter [09200, 10009], lr: 0.100000, loss: 2.7716
2022-03-05 12:38:40 - train: epoch 0018, iter [09300, 10009], lr: 0.100000, loss: 2.2980
2022-03-05 12:39:13 - train: epoch 0018, iter [09400, 10009], lr: 0.100000, loss: 3.0147
2022-03-05 12:39:45 - train: epoch 0018, iter [09500, 10009], lr: 0.100000, loss: 2.7547
2022-03-05 12:40:18 - train: epoch 0018, iter [09600, 10009], lr: 0.100000, loss: 2.4736
2022-03-05 12:40:51 - train: epoch 0018, iter [09700, 10009], lr: 0.100000, loss: 2.1889
2022-03-05 12:41:24 - train: epoch 0018, iter [09800, 10009], lr: 0.100000, loss: 2.7951
2022-03-05 12:41:56 - train: epoch 0018, iter [09900, 10009], lr: 0.100000, loss: 2.5040
2022-03-05 12:42:29 - train: epoch 0018, iter [10000, 10009], lr: 0.100000, loss: 2.8012
2022-03-05 12:42:32 - train: epoch 018, train_loss: 2.7192
2022-03-05 12:43:48 - eval: epoch: 018, acc1: 47.070%, acc5: 73.434%, test_loss: 2.3168, per_image_load_time: 1.341ms, per_image_inference_time: 1.551ms
2022-03-05 12:43:49 - until epoch: 018, best_acc1: 47.312%
2022-03-05 13:32:19 - epoch 019 lr: 0.1
2022-03-05 13:32:55 - train: epoch 0019, iter [00100, 10009], lr: 0.100000, loss: 2.8685
2022-03-05 13:33:27 - train: epoch 0019, iter [00200, 10009], lr: 0.100000, loss: 2.7043
2022-03-05 13:34:00 - train: epoch 0019, iter [00300, 10009], lr: 0.100000, loss: 2.3486
2022-03-05 13:34:32 - train: epoch 0019, iter [00400, 10009], lr: 0.100000, loss: 2.8875
2022-03-05 13:35:05 - train: epoch 0019, iter [00500, 10009], lr: 0.100000, loss: 2.7696
2022-03-05 13:35:37 - train: epoch 0019, iter [00600, 10009], lr: 0.100000, loss: 2.6334
2022-03-05 13:36:10 - train: epoch 0019, iter [00700, 10009], lr: 0.100000, loss: 2.5979
2022-03-05 13:36:42 - train: epoch 0019, iter [00800, 10009], lr: 0.100000, loss: 2.5636
2022-03-05 13:37:15 - train: epoch 0019, iter [00900, 10009], lr: 0.100000, loss: 2.6610
2022-03-05 13:37:47 - train: epoch 0019, iter [01000, 10009], lr: 0.100000, loss: 2.4850
2022-03-05 13:38:20 - train: epoch 0019, iter [01100, 10009], lr: 0.100000, loss: 2.4260
2022-03-05 13:38:52 - train: epoch 0019, iter [01200, 10009], lr: 0.100000, loss: 2.6684
2022-03-05 13:39:25 - train: epoch 0019, iter [01300, 10009], lr: 0.100000, loss: 3.0734
2022-03-05 13:39:57 - train: epoch 0019, iter [01400, 10009], lr: 0.100000, loss: 2.3714
2022-03-05 13:40:30 - train: epoch 0019, iter [01500, 10009], lr: 0.100000, loss: 2.8883
2022-03-05 13:41:02 - train: epoch 0019, iter [01600, 10009], lr: 0.100000, loss: 2.7846
2022-03-05 13:41:35 - train: epoch 0019, iter [01700, 10009], lr: 0.100000, loss: 2.8137
2022-03-05 13:42:07 - train: epoch 0019, iter [01800, 10009], lr: 0.100000, loss: 2.4840
2022-03-05 13:42:40 - train: epoch 0019, iter [01900, 10009], lr: 0.100000, loss: 2.9145
2022-03-05 13:43:12 - train: epoch 0019, iter [02000, 10009], lr: 0.100000, loss: 2.6494
2022-03-05 13:43:45 - train: epoch 0019, iter [02100, 10009], lr: 0.100000, loss: 2.6435
2022-03-05 13:44:18 - train: epoch 0019, iter [02200, 10009], lr: 0.100000, loss: 2.3738
2022-03-05 13:44:50 - train: epoch 0019, iter [02300, 10009], lr: 0.100000, loss: 2.6107
2022-03-05 13:45:23 - train: epoch 0019, iter [02400, 10009], lr: 0.100000, loss: 2.5529
2022-03-05 13:45:55 - train: epoch 0019, iter [02500, 10009], lr: 0.100000, loss: 2.6644
2022-03-05 13:46:28 - train: epoch 0019, iter [02600, 10009], lr: 0.100000, loss: 3.0362
2022-03-05 13:47:01 - train: epoch 0019, iter [02700, 10009], lr: 0.100000, loss: 2.5477
2022-03-05 13:47:33 - train: epoch 0019, iter [02800, 10009], lr: 0.100000, loss: 2.4759
2022-03-05 13:48:06 - train: epoch 0019, iter [02900, 10009], lr: 0.100000, loss: 2.4619
2022-03-05 13:48:38 - train: epoch 0019, iter [03000, 10009], lr: 0.100000, loss: 3.0002
2022-03-05 13:49:11 - train: epoch 0019, iter [03100, 10009], lr: 0.100000, loss: 2.6368
2022-03-05 13:49:44 - train: epoch 0019, iter [03200, 10009], lr: 0.100000, loss: 2.5549
2022-03-05 13:50:16 - train: epoch 0019, iter [03300, 10009], lr: 0.100000, loss: 2.8081
2022-03-05 13:50:49 - train: epoch 0019, iter [03400, 10009], lr: 0.100000, loss: 2.9169
2022-03-05 13:51:21 - train: epoch 0019, iter [03500, 10009], lr: 0.100000, loss: 2.7814
2022-03-05 13:51:54 - train: epoch 0019, iter [03600, 10009], lr: 0.100000, loss: 2.2769
2022-03-05 13:52:27 - train: epoch 0019, iter [03700, 10009], lr: 0.100000, loss: 2.9142
2022-03-05 13:52:59 - train: epoch 0019, iter [03800, 10009], lr: 0.100000, loss: 3.0503
2022-03-05 13:53:32 - train: epoch 0019, iter [03900, 10009], lr: 0.100000, loss: 2.5192
2022-03-05 13:54:04 - train: epoch 0019, iter [04000, 10009], lr: 0.100000, loss: 2.8054
2022-03-05 13:54:37 - train: epoch 0019, iter [04100, 10009], lr: 0.100000, loss: 2.5197
2022-03-05 13:55:10 - train: epoch 0019, iter [04200, 10009], lr: 0.100000, loss: 2.5958
2022-03-05 13:55:42 - train: epoch 0019, iter [04300, 10009], lr: 0.100000, loss: 2.6198
2022-03-05 13:56:15 - train: epoch 0019, iter [04400, 10009], lr: 0.100000, loss: 2.7157
2022-03-05 13:56:48 - train: epoch 0019, iter [04500, 10009], lr: 0.100000, loss: 2.6818
2022-03-05 13:57:20 - train: epoch 0019, iter [04600, 10009], lr: 0.100000, loss: 2.5828
2022-03-05 13:57:53 - train: epoch 0019, iter [04700, 10009], lr: 0.100000, loss: 2.7960
2022-03-05 13:58:26 - train: epoch 0019, iter [04800, 10009], lr: 0.100000, loss: 2.9045
2022-03-05 13:58:58 - train: epoch 0019, iter [04900, 10009], lr: 0.100000, loss: 2.7504
2022-03-05 13:59:31 - train: epoch 0019, iter [05000, 10009], lr: 0.100000, loss: 2.8040
2022-03-05 14:00:04 - train: epoch 0019, iter [05100, 10009], lr: 0.100000, loss: 3.2148
2022-03-05 14:00:37 - train: epoch 0019, iter [05200, 10009], lr: 0.100000, loss: 2.6286
2022-03-05 14:01:09 - train: epoch 0019, iter [05300, 10009], lr: 0.100000, loss: 2.7907
2022-03-05 14:01:42 - train: epoch 0019, iter [05400, 10009], lr: 0.100000, loss: 2.8614
2022-03-05 14:02:15 - train: epoch 0019, iter [05500, 10009], lr: 0.100000, loss: 2.5880
2022-03-05 14:02:48 - train: epoch 0019, iter [05600, 10009], lr: 0.100000, loss: 2.9668
2022-03-05 14:03:21 - train: epoch 0019, iter [05700, 10009], lr: 0.100000, loss: 2.8387
2022-03-05 14:03:53 - train: epoch 0019, iter [05800, 10009], lr: 0.100000, loss: 2.5830
2022-03-05 14:04:26 - train: epoch 0019, iter [05900, 10009], lr: 0.100000, loss: 2.7088
2022-03-05 14:04:59 - train: epoch 0019, iter [06000, 10009], lr: 0.100000, loss: 2.8635
2022-03-05 14:05:32 - train: epoch 0019, iter [06100, 10009], lr: 0.100000, loss: 2.8586
2022-03-05 14:06:04 - train: epoch 0019, iter [06200, 10009], lr: 0.100000, loss: 2.5827
2022-03-05 14:06:37 - train: epoch 0019, iter [06300, 10009], lr: 0.100000, loss: 2.8121
2022-03-05 14:07:10 - train: epoch 0019, iter [06400, 10009], lr: 0.100000, loss: 2.2907
2022-03-05 14:07:42 - train: epoch 0019, iter [06500, 10009], lr: 0.100000, loss: 3.1337
2022-03-05 14:08:15 - train: epoch 0019, iter [06600, 10009], lr: 0.100000, loss: 2.8202
2022-03-05 14:08:48 - train: epoch 0019, iter [06700, 10009], lr: 0.100000, loss: 2.8240
2022-03-05 14:09:21 - train: epoch 0019, iter [06800, 10009], lr: 0.100000, loss: 2.7093
2022-03-05 14:09:53 - train: epoch 0019, iter [06900, 10009], lr: 0.100000, loss: 2.6014
2022-03-05 14:10:26 - train: epoch 0019, iter [07000, 10009], lr: 0.100000, loss: 2.7828
2022-03-05 14:10:59 - train: epoch 0019, iter [07100, 10009], lr: 0.100000, loss: 2.5130
2022-03-05 14:11:31 - train: epoch 0019, iter [07200, 10009], lr: 0.100000, loss: 2.4912
2022-03-05 14:12:04 - train: epoch 0019, iter [07300, 10009], lr: 0.100000, loss: 2.7729
2022-03-05 14:12:37 - train: epoch 0019, iter [07400, 10009], lr: 0.100000, loss: 2.9359
2022-03-05 14:13:10 - train: epoch 0019, iter [07500, 10009], lr: 0.100000, loss: 2.9930
2022-03-05 14:13:42 - train: epoch 0019, iter [07600, 10009], lr: 0.100000, loss: 2.6308
2022-03-05 14:14:15 - train: epoch 0019, iter [07700, 10009], lr: 0.100000, loss: 2.6578
2022-03-05 14:14:48 - train: epoch 0019, iter [07800, 10009], lr: 0.100000, loss: 2.5008
2022-03-05 14:15:21 - train: epoch 0019, iter [07900, 10009], lr: 0.100000, loss: 2.3855
2022-03-05 14:15:53 - train: epoch 0019, iter [08000, 10009], lr: 0.100000, loss: 2.9886
2022-03-05 14:16:26 - train: epoch 0019, iter [08100, 10009], lr: 0.100000, loss: 2.8980
2022-03-05 14:16:59 - train: epoch 0019, iter [08200, 10009], lr: 0.100000, loss: 2.8858
2022-03-05 14:17:31 - train: epoch 0019, iter [08300, 10009], lr: 0.100000, loss: 2.9031
2022-03-05 14:18:04 - train: epoch 0019, iter [08400, 10009], lr: 0.100000, loss: 2.7829
2022-03-05 14:18:36 - train: epoch 0019, iter [08500, 10009], lr: 0.100000, loss: 3.0046
2022-03-05 14:19:09 - train: epoch 0019, iter [08600, 10009], lr: 0.100000, loss: 2.6163
2022-03-05 14:19:42 - train: epoch 0019, iter [08700, 10009], lr: 0.100000, loss: 3.1384
2022-03-05 14:20:14 - train: epoch 0019, iter [08800, 10009], lr: 0.100000, loss: 2.8164
2022-03-05 14:20:47 - train: epoch 0019, iter [08900, 10009], lr: 0.100000, loss: 2.9449
2022-03-05 14:21:20 - train: epoch 0019, iter [09000, 10009], lr: 0.100000, loss: 3.1953
2022-03-05 14:21:52 - train: epoch 0019, iter [09100, 10009], lr: 0.100000, loss: 2.5148
2022-03-05 14:22:25 - train: epoch 0019, iter [09200, 10009], lr: 0.100000, loss: 2.8636
2022-03-05 14:22:58 - train: epoch 0019, iter [09300, 10009], lr: 0.100000, loss: 2.8528
2022-03-05 14:23:31 - train: epoch 0019, iter [09400, 10009], lr: 0.100000, loss: 2.9267
2022-03-05 14:24:03 - train: epoch 0019, iter [09500, 10009], lr: 0.100000, loss: 2.8519
2022-03-05 14:24:36 - train: epoch 0019, iter [09600, 10009], lr: 0.100000, loss: 2.7371
2022-03-05 14:25:09 - train: epoch 0019, iter [09700, 10009], lr: 0.100000, loss: 2.9580
2022-03-05 14:25:41 - train: epoch 0019, iter [09800, 10009], lr: 0.100000, loss: 2.6696
2022-03-05 14:26:14 - train: epoch 0019, iter [09900, 10009], lr: 0.100000, loss: 2.4489
2022-03-05 14:26:47 - train: epoch 0019, iter [10000, 10009], lr: 0.100000, loss: 2.7544
2022-03-05 14:26:50 - train: epoch 019, train_loss: 2.7107
2022-03-05 14:28:05 - eval: epoch: 019, acc1: 47.862%, acc5: 73.766%, test_loss: 2.3053, per_image_load_time: 0.625ms, per_image_inference_time: 1.579ms
2022-03-05 14:28:05 - until epoch: 019, best_acc1: 47.862%
2022-03-05 14:28:05 - epoch 020 lr: 0.1
2022-03-05 14:28:42 - train: epoch 0020, iter [00100, 10009], lr: 0.100000, loss: 2.9447
2022-03-05 14:29:15 - train: epoch 0020, iter [00200, 10009], lr: 0.100000, loss: 2.4977
2022-03-05 14:29:48 - train: epoch 0020, iter [00300, 10009], lr: 0.100000, loss: 2.5400
2022-03-05 14:30:21 - train: epoch 0020, iter [00400, 10009], lr: 0.100000, loss: 2.8095
2022-03-05 14:30:53 - train: epoch 0020, iter [00500, 10009], lr: 0.100000, loss: 2.3475
2022-03-05 14:31:26 - train: epoch 0020, iter [00600, 10009], lr: 0.100000, loss: 2.8029
2022-03-05 14:31:59 - train: epoch 0020, iter [00700, 10009], lr: 0.100000, loss: 2.5520
2022-03-05 14:32:32 - train: epoch 0020, iter [00800, 10009], lr: 0.100000, loss: 2.7117
2022-03-05 14:33:05 - train: epoch 0020, iter [00900, 10009], lr: 0.100000, loss: 2.4637
2022-03-05 14:33:37 - train: epoch 0020, iter [01000, 10009], lr: 0.100000, loss: 2.4623
2022-03-05 14:34:10 - train: epoch 0020, iter [01100, 10009], lr: 0.100000, loss: 2.7972
2022-03-05 14:34:43 - train: epoch 0020, iter [01200, 10009], lr: 0.100000, loss: 2.5281
2022-03-05 14:35:16 - train: epoch 0020, iter [01300, 10009], lr: 0.100000, loss: 2.8351
2022-03-05 14:35:49 - train: epoch 0020, iter [01400, 10009], lr: 0.100000, loss: 2.7145
2022-03-05 14:36:21 - train: epoch 0020, iter [01500, 10009], lr: 0.100000, loss: 2.7287
2022-03-05 14:36:54 - train: epoch 0020, iter [01600, 10009], lr: 0.100000, loss: 2.4890
2022-03-05 14:37:27 - train: epoch 0020, iter [01700, 10009], lr: 0.100000, loss: 2.5512
2022-03-05 14:38:00 - train: epoch 0020, iter [01800, 10009], lr: 0.100000, loss: 2.6192
2022-03-05 14:38:32 - train: epoch 0020, iter [01900, 10009], lr: 0.100000, loss: 2.3369
2022-03-05 14:39:05 - train: epoch 0020, iter [02000, 10009], lr: 0.100000, loss: 2.9241
2022-03-05 14:39:38 - train: epoch 0020, iter [02100, 10009], lr: 0.100000, loss: 2.5711
2022-03-05 14:40:11 - train: epoch 0020, iter [02200, 10009], lr: 0.100000, loss: 2.5662
2022-03-05 14:40:43 - train: epoch 0020, iter [02300, 10009], lr: 0.100000, loss: 2.6618
2022-03-05 14:41:16 - train: epoch 0020, iter [02400, 10009], lr: 0.100000, loss: 2.7017
2022-03-05 14:41:49 - train: epoch 0020, iter [02500, 10009], lr: 0.100000, loss: 2.6354
2022-03-05 14:42:22 - train: epoch 0020, iter [02600, 10009], lr: 0.100000, loss: 2.8168
2022-03-05 14:42:54 - train: epoch 0020, iter [02700, 10009], lr: 0.100000, loss: 2.8837
2022-03-05 14:43:27 - train: epoch 0020, iter [02800, 10009], lr: 0.100000, loss: 3.0245
2022-03-05 14:44:00 - train: epoch 0020, iter [02900, 10009], lr: 0.100000, loss: 2.7971
2022-03-05 14:44:33 - train: epoch 0020, iter [03000, 10009], lr: 0.100000, loss: 2.7383
2022-03-05 14:45:06 - train: epoch 0020, iter [03100, 10009], lr: 0.100000, loss: 2.8473
2022-03-05 14:45:38 - train: epoch 0020, iter [03200, 10009], lr: 0.100000, loss: 3.1129
2022-03-05 14:46:11 - train: epoch 0020, iter [03300, 10009], lr: 0.100000, loss: 2.5225
2022-03-05 14:46:44 - train: epoch 0020, iter [03400, 10009], lr: 0.100000, loss: 2.6969
2022-03-05 14:47:17 - train: epoch 0020, iter [03500, 10009], lr: 0.100000, loss: 2.7208
2022-03-05 14:47:49 - train: epoch 0020, iter [03600, 10009], lr: 0.100000, loss: 3.0871
2022-03-05 14:48:22 - train: epoch 0020, iter [03700, 10009], lr: 0.100000, loss: 2.1687
2022-03-05 14:48:55 - train: epoch 0020, iter [03800, 10009], lr: 0.100000, loss: 2.6219
2022-03-05 14:49:28 - train: epoch 0020, iter [03900, 10009], lr: 0.100000, loss: 2.5593
2022-03-05 14:50:01 - train: epoch 0020, iter [04000, 10009], lr: 0.100000, loss: 2.6838
2022-03-05 14:50:33 - train: epoch 0020, iter [04100, 10009], lr: 0.100000, loss: 2.7468
2022-03-05 14:51:06 - train: epoch 0020, iter [04200, 10009], lr: 0.100000, loss: 2.9106
2022-03-05 14:51:39 - train: epoch 0020, iter [04300, 10009], lr: 0.100000, loss: 2.8198
2022-03-05 14:52:11 - train: epoch 0020, iter [04400, 10009], lr: 0.100000, loss: 2.3187
2022-03-05 14:52:44 - train: epoch 0020, iter [04500, 10009], lr: 0.100000, loss: 2.8188
2022-03-05 14:53:17 - train: epoch 0020, iter [04600, 10009], lr: 0.100000, loss: 2.7628
2022-03-05 14:53:50 - train: epoch 0020, iter [04700, 10009], lr: 0.100000, loss: 2.7317
2022-03-05 14:54:22 - train: epoch 0020, iter [04800, 10009], lr: 0.100000, loss: 2.7605
2022-03-05 14:54:55 - train: epoch 0020, iter [04900, 10009], lr: 0.100000, loss: 2.9122
2022-03-05 14:55:28 - train: epoch 0020, iter [05000, 10009], lr: 0.100000, loss: 2.5640
2022-03-05 14:56:00 - train: epoch 0020, iter [05100, 10009], lr: 0.100000, loss: 2.4786
2022-03-05 14:56:33 - train: epoch 0020, iter [05200, 10009], lr: 0.100000, loss: 2.6200
2022-03-05 14:57:06 - train: epoch 0020, iter [05300, 10009], lr: 0.100000, loss: 2.9125
2022-03-05 14:57:39 - train: epoch 0020, iter [05400, 10009], lr: 0.100000, loss: 2.5116
2022-03-05 14:58:12 - train: epoch 0020, iter [05500, 10009], lr: 0.100000, loss: 2.3657
2022-03-05 14:58:45 - train: epoch 0020, iter [05600, 10009], lr: 0.100000, loss: 3.1686
2022-03-05 14:59:18 - train: epoch 0020, iter [05700, 10009], lr: 0.100000, loss: 2.4818
2022-03-05 14:59:50 - train: epoch 0020, iter [05800, 10009], lr: 0.100000, loss: 2.8748
2022-03-05 15:00:23 - train: epoch 0020, iter [05900, 10009], lr: 0.100000, loss: 2.8593
2022-03-05 15:00:56 - train: epoch 0020, iter [06000, 10009], lr: 0.100000, loss: 2.4446
2022-03-05 15:01:29 - train: epoch 0020, iter [06100, 10009], lr: 0.100000, loss: 2.7895
2022-03-05 15:02:02 - train: epoch 0020, iter [06200, 10009], lr: 0.100000, loss: 3.0647
2022-03-05 15:02:35 - train: epoch 0020, iter [06300, 10009], lr: 0.100000, loss: 2.7642
2022-03-05 15:03:08 - train: epoch 0020, iter [06400, 10009], lr: 0.100000, loss: 2.5918
2022-03-05 15:03:41 - train: epoch 0020, iter [06500, 10009], lr: 0.100000, loss: 2.4480
2022-03-05 15:04:14 - train: epoch 0020, iter [06600, 10009], lr: 0.100000, loss: 2.5916
2022-03-05 15:04:46 - train: epoch 0020, iter [06700, 10009], lr: 0.100000, loss: 2.8910
2022-03-05 15:05:19 - train: epoch 0020, iter [06800, 10009], lr: 0.100000, loss: 2.8854
2022-03-05 15:05:52 - train: epoch 0020, iter [06900, 10009], lr: 0.100000, loss: 2.9598
2022-03-05 15:06:25 - train: epoch 0020, iter [07000, 10009], lr: 0.100000, loss: 2.5344
2022-03-05 15:06:58 - train: epoch 0020, iter [07100, 10009], lr: 0.100000, loss: 2.5392
2022-03-05 15:07:31 - train: epoch 0020, iter [07200, 10009], lr: 0.100000, loss: 2.8302
2022-03-05 15:08:04 - train: epoch 0020, iter [07300, 10009], lr: 0.100000, loss: 2.6728
2022-03-05 15:08:36 - train: epoch 0020, iter [07400, 10009], lr: 0.100000, loss: 2.6721
2022-03-05 15:09:09 - train: epoch 0020, iter [07500, 10009], lr: 0.100000, loss: 2.7865
2022-03-05 15:09:42 - train: epoch 0020, iter [07600, 10009], lr: 0.100000, loss: 2.7975
2022-03-05 15:10:15 - train: epoch 0020, iter [07700, 10009], lr: 0.100000, loss: 2.8347
2022-03-05 15:10:48 - train: epoch 0020, iter [07800, 10009], lr: 0.100000, loss: 3.1362
2022-03-05 15:11:21 - train: epoch 0020, iter [07900, 10009], lr: 0.100000, loss: 2.7191
2022-03-05 15:11:54 - train: epoch 0020, iter [08000, 10009], lr: 0.100000, loss: 2.6083
2022-03-05 15:12:26 - train: epoch 0020, iter [08100, 10009], lr: 0.100000, loss: 2.7004
2022-03-05 15:12:59 - train: epoch 0020, iter [08200, 10009], lr: 0.100000, loss: 2.4553
2022-03-05 15:13:32 - train: epoch 0020, iter [08300, 10009], lr: 0.100000, loss: 2.7715
2022-03-05 15:14:05 - train: epoch 0020, iter [08400, 10009], lr: 0.100000, loss: 2.7690
2022-03-05 15:14:38 - train: epoch 0020, iter [08500, 10009], lr: 0.100000, loss: 2.5497
2022-03-05 15:15:11 - train: epoch 0020, iter [08600, 10009], lr: 0.100000, loss: 2.9505
2022-03-05 15:15:44 - train: epoch 0020, iter [08700, 10009], lr: 0.100000, loss: 2.5341
2022-03-05 15:16:17 - train: epoch 0020, iter [08800, 10009], lr: 0.100000, loss: 2.8082
2022-03-05 15:16:49 - train: epoch 0020, iter [08900, 10009], lr: 0.100000, loss: 2.5624
2022-03-05 15:17:22 - train: epoch 0020, iter [09000, 10009], lr: 0.100000, loss: 2.5156
2022-03-05 15:17:55 - train: epoch 0020, iter [09100, 10009], lr: 0.100000, loss: 2.6386
2022-03-05 15:18:28 - train: epoch 0020, iter [09200, 10009], lr: 0.100000, loss: 2.9903
2022-03-05 15:19:01 - train: epoch 0020, iter [09300, 10009], lr: 0.100000, loss: 2.9855
2022-03-05 15:19:34 - train: epoch 0020, iter [09400, 10009], lr: 0.100000, loss: 2.4996
2022-03-05 15:20:06 - train: epoch 0020, iter [09500, 10009], lr: 0.100000, loss: 2.7771
2022-03-05 15:20:39 - train: epoch 0020, iter [09600, 10009], lr: 0.100000, loss: 2.5798
2022-03-05 15:21:12 - train: epoch 0020, iter [09700, 10009], lr: 0.100000, loss: 2.5478
2022-03-05 15:21:45 - train: epoch 0020, iter [09800, 10009], lr: 0.100000, loss: 2.4001
2022-03-05 15:22:18 - train: epoch 0020, iter [09900, 10009], lr: 0.100000, loss: 2.7803
2022-03-05 15:22:51 - train: epoch 0020, iter [10000, 10009], lr: 0.100000, loss: 2.7387
2022-03-05 15:22:54 - train: epoch 020, train_loss: 2.7016
2022-03-05 15:24:20 - eval: epoch: 020, acc1: 47.684%, acc5: 73.198%, test_loss: 2.3126, per_image_load_time: 1.699ms, per_image_inference_time: 1.541ms
2022-03-05 15:24:21 - until epoch: 020, best_acc1: 47.862%
2022-03-05 15:24:21 - epoch 021 lr: 0.1
2022-03-05 15:24:57 - train: epoch 0021, iter [00100, 10009], lr: 0.100000, loss: 2.5055
2022-03-05 15:25:30 - train: epoch 0021, iter [00200, 10009], lr: 0.100000, loss: 2.6163
2022-03-05 15:26:03 - train: epoch 0021, iter [00300, 10009], lr: 0.100000, loss: 2.5443
2022-03-05 15:26:35 - train: epoch 0021, iter [00400, 10009], lr: 0.100000, loss: 2.6269
2022-03-05 15:27:08 - train: epoch 0021, iter [00500, 10009], lr: 0.100000, loss: 2.5881
2022-03-05 15:27:41 - train: epoch 0021, iter [00600, 10009], lr: 0.100000, loss: 2.1953
2022-03-05 15:28:13 - train: epoch 0021, iter [00700, 10009], lr: 0.100000, loss: 2.2729
2022-03-05 15:28:46 - train: epoch 0021, iter [00800, 10009], lr: 0.100000, loss: 2.4714
2022-03-05 15:29:19 - train: epoch 0021, iter [00900, 10009], lr: 0.100000, loss: 2.9625
2022-03-05 15:29:51 - train: epoch 0021, iter [01000, 10009], lr: 0.100000, loss: 2.3660
2022-03-05 15:30:24 - train: epoch 0021, iter [01100, 10009], lr: 0.100000, loss: 2.7337
2022-03-05 15:30:57 - train: epoch 0021, iter [01200, 10009], lr: 0.100000, loss: 2.5562
2022-03-05 15:31:29 - train: epoch 0021, iter [01300, 10009], lr: 0.100000, loss: 2.9386
2022-03-05 15:32:02 - train: epoch 0021, iter [01400, 10009], lr: 0.100000, loss: 2.7417
2022-03-05 15:32:35 - train: epoch 0021, iter [01500, 10009], lr: 0.100000, loss: 3.0158
2022-03-05 15:33:07 - train: epoch 0021, iter [01600, 10009], lr: 0.100000, loss: 2.8981
2022-03-05 15:33:40 - train: epoch 0021, iter [01700, 10009], lr: 0.100000, loss: 2.6272
2022-03-05 15:34:13 - train: epoch 0021, iter [01800, 10009], lr: 0.100000, loss: 2.7175
2022-03-05 15:34:46 - train: epoch 0021, iter [01900, 10009], lr: 0.100000, loss: 2.7739
2022-03-05 15:35:19 - train: epoch 0021, iter [02000, 10009], lr: 0.100000, loss: 2.8008
2022-03-05 15:35:52 - train: epoch 0021, iter [02100, 10009], lr: 0.100000, loss: 2.6882
2022-03-05 15:36:24 - train: epoch 0021, iter [02200, 10009], lr: 0.100000, loss: 2.3988
2022-03-05 15:36:57 - train: epoch 0021, iter [02300, 10009], lr: 0.100000, loss: 2.8339
2022-03-05 15:37:30 - train: epoch 0021, iter [02400, 10009], lr: 0.100000, loss: 2.9169
2022-03-05 15:38:03 - train: epoch 0021, iter [02500, 10009], lr: 0.100000, loss: 2.6258
2022-03-05 15:38:36 - train: epoch 0021, iter [02600, 10009], lr: 0.100000, loss: 2.6043
2022-03-05 15:39:08 - train: epoch 0021, iter [02700, 10009], lr: 0.100000, loss: 2.7173
2022-03-05 15:39:41 - train: epoch 0021, iter [02800, 10009], lr: 0.100000, loss: 2.5230
2022-03-05 15:40:14 - train: epoch 0021, iter [02900, 10009], lr: 0.100000, loss: 2.8320
2022-03-05 15:40:47 - train: epoch 0021, iter [03000, 10009], lr: 0.100000, loss: 2.4617
2022-03-05 15:41:19 - train: epoch 0021, iter [03100, 10009], lr: 0.100000, loss: 2.8682
2022-03-05 15:41:52 - train: epoch 0021, iter [03200, 10009], lr: 0.100000, loss: 2.1660
2022-03-05 15:42:25 - train: epoch 0021, iter [03300, 10009], lr: 0.100000, loss: 2.3247
2022-03-05 15:42:58 - train: epoch 0021, iter [03400, 10009], lr: 0.100000, loss: 2.6850
2022-03-05 15:43:31 - train: epoch 0021, iter [03500, 10009], lr: 0.100000, loss: 2.7178
2022-03-05 15:44:04 - train: epoch 0021, iter [03600, 10009], lr: 0.100000, loss: 2.7051
2022-03-05 15:44:36 - train: epoch 0021, iter [03700, 10009], lr: 0.100000, loss: 2.9585
2022-03-05 15:45:09 - train: epoch 0021, iter [03800, 10009], lr: 0.100000, loss: 2.8014
2022-03-05 15:45:42 - train: epoch 0021, iter [03900, 10009], lr: 0.100000, loss: 2.9228
2022-03-05 15:46:15 - train: epoch 0021, iter [04000, 10009], lr: 0.100000, loss: 2.8683
2022-03-05 15:46:48 - train: epoch 0021, iter [04100, 10009], lr: 0.100000, loss: 2.9504
2022-03-05 15:47:21 - train: epoch 0021, iter [04200, 10009], lr: 0.100000, loss: 2.5392
2022-03-05 15:47:54 - train: epoch 0021, iter [04300, 10009], lr: 0.100000, loss: 2.5355
2022-03-05 15:48:27 - train: epoch 0021, iter [04400, 10009], lr: 0.100000, loss: 2.8103
2022-03-05 15:48:59 - train: epoch 0021, iter [04500, 10009], lr: 0.100000, loss: 2.4606
2022-03-05 15:49:32 - train: epoch 0021, iter [04600, 10009], lr: 0.100000, loss: 2.9061
2022-03-05 15:50:05 - train: epoch 0021, iter [04700, 10009], lr: 0.100000, loss: 2.7748
2022-03-05 15:50:38 - train: epoch 0021, iter [04800, 10009], lr: 0.100000, loss: 2.6020
2022-03-05 15:51:10 - train: epoch 0021, iter [04900, 10009], lr: 0.100000, loss: 2.7514
2022-03-05 15:51:43 - train: epoch 0021, iter [05000, 10009], lr: 0.100000, loss: 2.9698
2022-03-05 15:52:16 - train: epoch 0021, iter [05100, 10009], lr: 0.100000, loss: 3.2633
2022-03-05 15:52:49 - train: epoch 0021, iter [05200, 10009], lr: 0.100000, loss: 2.8080
2022-03-05 15:53:22 - train: epoch 0021, iter [05300, 10009], lr: 0.100000, loss: 2.5714
2022-03-05 15:53:55 - train: epoch 0021, iter [05400, 10009], lr: 0.100000, loss: 2.7069
2022-03-05 15:54:28 - train: epoch 0021, iter [05500, 10009], lr: 0.100000, loss: 2.5262
2022-03-05 15:55:00 - train: epoch 0021, iter [05600, 10009], lr: 0.100000, loss: 2.7500
2022-03-05 15:55:33 - train: epoch 0021, iter [05700, 10009], lr: 0.100000, loss: 2.6178
2022-03-05 15:56:06 - train: epoch 0021, iter [05800, 10009], lr: 0.100000, loss: 2.5943
2022-03-05 15:56:39 - train: epoch 0021, iter [05900, 10009], lr: 0.100000, loss: 2.6409
2022-03-05 15:57:12 - train: epoch 0021, iter [06000, 10009], lr: 0.100000, loss: 2.7579
2022-03-05 15:57:45 - train: epoch 0021, iter [06100, 10009], lr: 0.100000, loss: 2.4842
2022-03-05 15:58:18 - train: epoch 0021, iter [06200, 10009], lr: 0.100000, loss: 2.5643
2022-03-05 15:58:51 - train: epoch 0021, iter [06300, 10009], lr: 0.100000, loss: 2.5330
2022-03-05 15:59:24 - train: epoch 0021, iter [06400, 10009], lr: 0.100000, loss: 2.6351
2022-03-05 15:59:56 - train: epoch 0021, iter [06500, 10009], lr: 0.100000, loss: 2.5958
2022-03-05 16:00:29 - train: epoch 0021, iter [06600, 10009], lr: 0.100000, loss: 3.0603
2022-03-05 16:01:02 - train: epoch 0021, iter [06700, 10009], lr: 0.100000, loss: 2.3920
2022-03-05 16:01:35 - train: epoch 0021, iter [06800, 10009], lr: 0.100000, loss: 2.9713
2022-03-05 16:02:08 - train: epoch 0021, iter [06900, 10009], lr: 0.100000, loss: 2.8586
2022-03-05 16:02:41 - train: epoch 0021, iter [07000, 10009], lr: 0.100000, loss: 2.6433
2022-03-05 16:03:14 - train: epoch 0021, iter [07100, 10009], lr: 0.100000, loss: 2.4504
2022-03-05 16:03:46 - train: epoch 0021, iter [07200, 10009], lr: 0.100000, loss: 2.5598
2022-03-05 16:04:19 - train: epoch 0021, iter [07300, 10009], lr: 0.100000, loss: 2.1924
2022-03-05 16:04:52 - train: epoch 0021, iter [07400, 10009], lr: 0.100000, loss: 2.8126
2022-03-05 16:05:25 - train: epoch 0021, iter [07500, 10009], lr: 0.100000, loss: 2.7091
2022-03-05 16:05:58 - train: epoch 0021, iter [07600, 10009], lr: 0.100000, loss: 2.7330
2022-03-05 16:06:31 - train: epoch 0021, iter [07700, 10009], lr: 0.100000, loss: 2.8776
2022-03-05 16:07:04 - train: epoch 0021, iter [07800, 10009], lr: 0.100000, loss: 2.7932
2022-03-05 16:07:37 - train: epoch 0021, iter [07900, 10009], lr: 0.100000, loss: 2.9171
2022-03-05 16:08:10 - train: epoch 0021, iter [08000, 10009], lr: 0.100000, loss: 3.3252
2022-03-05 16:08:43 - train: epoch 0021, iter [08100, 10009], lr: 0.100000, loss: 2.6288
2022-03-05 16:09:15 - train: epoch 0021, iter [08200, 10009], lr: 0.100000, loss: 2.8805
2022-03-05 16:09:48 - train: epoch 0021, iter [08300, 10009], lr: 0.100000, loss: 2.8267
2022-03-05 16:10:21 - train: epoch 0021, iter [08400, 10009], lr: 0.100000, loss: 2.8382
2022-03-05 16:10:54 - train: epoch 0021, iter [08500, 10009], lr: 0.100000, loss: 2.6973
2022-03-05 16:11:27 - train: epoch 0021, iter [08600, 10009], lr: 0.100000, loss: 2.7141
2022-03-05 16:12:00 - train: epoch 0021, iter [08700, 10009], lr: 0.100000, loss: 2.7389
2022-03-05 16:12:33 - train: epoch 0021, iter [08800, 10009], lr: 0.100000, loss: 2.6993
2022-03-05 16:13:06 - train: epoch 0021, iter [08900, 10009], lr: 0.100000, loss: 2.6771
2022-03-05 16:13:38 - train: epoch 0021, iter [09000, 10009], lr: 0.100000, loss: 2.5391
2022-03-05 16:14:11 - train: epoch 0021, iter [09100, 10009], lr: 0.100000, loss: 3.1094
2022-03-05 16:14:44 - train: epoch 0021, iter [09200, 10009], lr: 0.100000, loss: 2.4884
2022-03-05 16:15:17 - train: epoch 0021, iter [09300, 10009], lr: 0.100000, loss: 2.5624
2022-03-05 16:15:50 - train: epoch 0021, iter [09400, 10009], lr: 0.100000, loss: 3.0338
2022-03-05 16:16:23 - train: epoch 0021, iter [09500, 10009], lr: 0.100000, loss: 2.8035
2022-03-05 16:16:55 - train: epoch 0021, iter [09600, 10009], lr: 0.100000, loss: 2.6392
2022-03-05 16:17:28 - train: epoch 0021, iter [09700, 10009], lr: 0.100000, loss: 2.7262
2022-03-05 16:18:01 - train: epoch 0021, iter [09800, 10009], lr: 0.100000, loss: 2.7191
2022-03-05 16:18:34 - train: epoch 0021, iter [09900, 10009], lr: 0.100000, loss: 2.4198
2022-03-05 16:19:07 - train: epoch 0021, iter [10000, 10009], lr: 0.100000, loss: 2.4187
2022-03-05 16:19:10 - train: epoch 021, train_loss: 2.6921
2022-03-05 16:20:25 - eval: epoch: 021, acc1: 47.006%, acc5: 73.322%, test_loss: 2.3316, per_image_load_time: 1.339ms, per_image_inference_time: 1.539ms
2022-03-05 16:20:26 - until epoch: 021, best_acc1: 47.862%
2022-03-05 16:20:26 - epoch 022 lr: 0.1
2022-03-05 16:21:02 - train: epoch 0022, iter [00100, 10009], lr: 0.100000, loss: 2.5546
2022-03-05 16:21:35 - train: epoch 0022, iter [00200, 10009], lr: 0.100000, loss: 2.7386
2022-03-05 16:22:08 - train: epoch 0022, iter [00300, 10009], lr: 0.100000, loss: 2.4675
2022-03-05 16:22:41 - train: epoch 0022, iter [00400, 10009], lr: 0.100000, loss: 2.9212
2022-03-05 16:23:14 - train: epoch 0022, iter [00500, 10009], lr: 0.100000, loss: 2.7331
2022-03-05 16:23:47 - train: epoch 0022, iter [00600, 10009], lr: 0.100000, loss: 2.6135
2022-03-05 16:24:19 - train: epoch 0022, iter [00700, 10009], lr: 0.100000, loss: 2.8490
2022-03-05 16:24:52 - train: epoch 0022, iter [00800, 10009], lr: 0.100000, loss: 2.6386
2022-03-05 16:25:25 - train: epoch 0022, iter [00900, 10009], lr: 0.100000, loss: 2.5946
2022-03-05 16:25:58 - train: epoch 0022, iter [01000, 10009], lr: 0.100000, loss: 2.8041
2022-03-05 16:26:31 - train: epoch 0022, iter [01100, 10009], lr: 0.100000, loss: 2.9957
2022-03-05 16:27:04 - train: epoch 0022, iter [01200, 10009], lr: 0.100000, loss: 2.5997
2022-03-05 16:27:36 - train: epoch 0022, iter [01300, 10009], lr: 0.100000, loss: 2.8212
2022-03-05 16:28:09 - train: epoch 0022, iter [01400, 10009], lr: 0.100000, loss: 2.5880
2022-03-05 16:28:42 - train: epoch 0022, iter [01500, 10009], lr: 0.100000, loss: 2.8623
2022-03-05 16:29:15 - train: epoch 0022, iter [01600, 10009], lr: 0.100000, loss: 3.0135
2022-03-05 16:29:48 - train: epoch 0022, iter [01700, 10009], lr: 0.100000, loss: 2.9345
2022-03-05 16:30:21 - train: epoch 0022, iter [01800, 10009], lr: 0.100000, loss: 2.6054
2022-03-05 16:30:54 - train: epoch 0022, iter [01900, 10009], lr: 0.100000, loss: 2.5759
2022-03-05 16:31:27 - train: epoch 0022, iter [02000, 10009], lr: 0.100000, loss: 2.6329
2022-03-05 16:31:59 - train: epoch 0022, iter [02100, 10009], lr: 0.100000, loss: 2.8446
2022-03-05 16:32:32 - train: epoch 0022, iter [02200, 10009], lr: 0.100000, loss: 2.5813
2022-03-05 16:33:05 - train: epoch 0022, iter [02300, 10009], lr: 0.100000, loss: 2.4275
2022-03-05 16:33:38 - train: epoch 0022, iter [02400, 10009], lr: 0.100000, loss: 2.5578
2022-03-05 16:34:11 - train: epoch 0022, iter [02500, 10009], lr: 0.100000, loss: 2.7011
2022-03-05 16:34:44 - train: epoch 0022, iter [02600, 10009], lr: 0.100000, loss: 2.7725
2022-03-05 16:35:16 - train: epoch 0022, iter [02700, 10009], lr: 0.100000, loss: 2.8662
2022-03-05 16:35:49 - train: epoch 0022, iter [02800, 10009], lr: 0.100000, loss: 2.6037
2022-03-05 16:36:22 - train: epoch 0022, iter [02900, 10009], lr: 0.100000, loss: 2.3377
2022-03-05 16:36:55 - train: epoch 0022, iter [03000, 10009], lr: 0.100000, loss: 2.8754
2022-03-05 16:37:28 - train: epoch 0022, iter [03100, 10009], lr: 0.100000, loss: 2.8614
2022-03-05 16:38:01 - train: epoch 0022, iter [03200, 10009], lr: 0.100000, loss: 2.5760
2022-03-05 16:38:34 - train: epoch 0022, iter [03300, 10009], lr: 0.100000, loss: 2.4570
2022-03-05 16:39:07 - train: epoch 0022, iter [03400, 10009], lr: 0.100000, loss: 2.9599
2022-03-05 16:39:40 - train: epoch 0022, iter [03500, 10009], lr: 0.100000, loss: 2.5193
2022-03-05 16:40:12 - train: epoch 0022, iter [03600, 10009], lr: 0.100000, loss: 3.1966
2022-03-05 16:40:45 - train: epoch 0022, iter [03700, 10009], lr: 0.100000, loss: 2.2613
2022-03-05 16:41:18 - train: epoch 0022, iter [03800, 10009], lr: 0.100000, loss: 2.7872
2022-03-05 16:41:51 - train: epoch 0022, iter [03900, 10009], lr: 0.100000, loss: 2.5194
2022-03-05 16:42:24 - train: epoch 0022, iter [04000, 10009], lr: 0.100000, loss: 3.0099
2022-03-05 16:42:57 - train: epoch 0022, iter [04100, 10009], lr: 0.100000, loss: 2.7381
2022-03-05 16:43:30 - train: epoch 0022, iter [04200, 10009], lr: 0.100000, loss: 2.4018
2022-03-05 16:44:02 - train: epoch 0022, iter [04300, 10009], lr: 0.100000, loss: 2.5891
2022-03-05 16:44:35 - train: epoch 0022, iter [04400, 10009], lr: 0.100000, loss: 2.4028
2022-03-05 16:45:08 - train: epoch 0022, iter [04500, 10009], lr: 0.100000, loss: 2.5167
2022-03-05 16:45:41 - train: epoch 0022, iter [04600, 10009], lr: 0.100000, loss: 2.8194
2022-03-05 16:46:14 - train: epoch 0022, iter [04700, 10009], lr: 0.100000, loss: 2.0933
2022-03-05 16:46:47 - train: epoch 0022, iter [04800, 10009], lr: 0.100000, loss: 2.8403
2022-03-05 16:47:20 - train: epoch 0022, iter [04900, 10009], lr: 0.100000, loss: 2.7617
2022-03-05 16:47:53 - train: epoch 0022, iter [05000, 10009], lr: 0.100000, loss: 2.3974
2022-03-05 16:48:26 - train: epoch 0022, iter [05100, 10009], lr: 0.100000, loss: 3.1789
2022-03-05 16:48:58 - train: epoch 0022, iter [05200, 10009], lr: 0.100000, loss: 2.4494
2022-03-05 16:49:31 - train: epoch 0022, iter [05300, 10009], lr: 0.100000, loss: 2.5261
2022-03-05 16:50:04 - train: epoch 0022, iter [05400, 10009], lr: 0.100000, loss: 2.3415
2022-03-05 16:50:37 - train: epoch 0022, iter [05500, 10009], lr: 0.100000, loss: 2.5929
2022-03-05 16:51:10 - train: epoch 0022, iter [05600, 10009], lr: 0.100000, loss: 2.9574
2022-03-05 16:51:43 - train: epoch 0022, iter [05700, 10009], lr: 0.100000, loss: 2.5187
2022-03-05 16:52:16 - train: epoch 0022, iter [05800, 10009], lr: 0.100000, loss: 2.3141
2022-03-05 16:52:49 - train: epoch 0022, iter [05900, 10009], lr: 0.100000, loss: 2.5001
2022-03-05 16:53:22 - train: epoch 0022, iter [06000, 10009], lr: 0.100000, loss: 3.1458
2022-03-05 16:53:55 - train: epoch 0022, iter [06100, 10009], lr: 0.100000, loss: 2.0469
2022-03-05 16:54:28 - train: epoch 0022, iter [06200, 10009], lr: 0.100000, loss: 2.7717
2022-03-05 16:55:00 - train: epoch 0022, iter [06300, 10009], lr: 0.100000, loss: 2.4675
2022-03-05 16:55:33 - train: epoch 0022, iter [06400, 10009], lr: 0.100000, loss: 2.5973
2022-03-05 16:56:06 - train: epoch 0022, iter [06500, 10009], lr: 0.100000, loss: 2.8793
2022-03-05 16:56:39 - train: epoch 0022, iter [06600, 10009], lr: 0.100000, loss: 2.5409
2022-03-05 16:57:12 - train: epoch 0022, iter [06700, 10009], lr: 0.100000, loss: 2.8941
2022-03-05 16:57:45 - train: epoch 0022, iter [06800, 10009], lr: 0.100000, loss: 2.7008
2022-03-05 16:58:18 - train: epoch 0022, iter [06900, 10009], lr: 0.100000, loss: 2.7352
2022-03-05 16:58:50 - train: epoch 0022, iter [07000, 10009], lr: 0.100000, loss: 2.8857
2022-03-05 16:59:23 - train: epoch 0022, iter [07100, 10009], lr: 0.100000, loss: 2.4201
2022-03-05 16:59:56 - train: epoch 0022, iter [07200, 10009], lr: 0.100000, loss: 2.4409
2022-03-05 17:00:29 - train: epoch 0022, iter [07300, 10009], lr: 0.100000, loss: 2.5078
2022-03-05 17:01:02 - train: epoch 0022, iter [07400, 10009], lr: 0.100000, loss: 3.0337
2022-03-05 17:01:35 - train: epoch 0022, iter [07500, 10009], lr: 0.100000, loss: 2.6808
2022-03-05 17:02:08 - train: epoch 0022, iter [07600, 10009], lr: 0.100000, loss: 2.9816
2022-03-05 17:02:40 - train: epoch 0022, iter [07700, 10009], lr: 0.100000, loss: 2.9717
2022-03-05 17:03:13 - train: epoch 0022, iter [07800, 10009], lr: 0.100000, loss: 2.6917
2022-03-05 17:03:46 - train: epoch 0022, iter [07900, 10009], lr: 0.100000, loss: 2.6852
2022-03-05 17:04:19 - train: epoch 0022, iter [08000, 10009], lr: 0.100000, loss: 2.6828
2022-03-05 17:04:52 - train: epoch 0022, iter [08100, 10009], lr: 0.100000, loss: 2.7033
2022-03-05 17:05:24 - train: epoch 0022, iter [08200, 10009], lr: 0.100000, loss: 2.5774
2022-03-05 17:05:57 - train: epoch 0022, iter [08300, 10009], lr: 0.100000, loss: 2.6425
2022-03-05 17:06:30 - train: epoch 0022, iter [08400, 10009], lr: 0.100000, loss: 3.0686
2022-03-05 17:07:03 - train: epoch 0022, iter [08500, 10009], lr: 0.100000, loss: 2.6999
2022-03-05 17:07:35 - train: epoch 0022, iter [08600, 10009], lr: 0.100000, loss: 2.8006
2022-03-05 17:08:08 - train: epoch 0022, iter [08700, 10009], lr: 0.100000, loss: 2.7613
2022-03-05 17:08:41 - train: epoch 0022, iter [08800, 10009], lr: 0.100000, loss: 2.7836
2022-03-05 17:09:14 - train: epoch 0022, iter [08900, 10009], lr: 0.100000, loss: 2.8664
2022-03-05 17:09:47 - train: epoch 0022, iter [09000, 10009], lr: 0.100000, loss: 2.5352
2022-03-05 17:10:19 - train: epoch 0022, iter [09100, 10009], lr: 0.100000, loss: 2.9383
2022-03-05 17:10:52 - train: epoch 0022, iter [09200, 10009], lr: 0.100000, loss: 2.7552
2022-03-05 17:11:25 - train: epoch 0022, iter [09300, 10009], lr: 0.100000, loss: 2.4701
2022-03-05 17:11:58 - train: epoch 0022, iter [09400, 10009], lr: 0.100000, loss: 2.4783
2022-03-05 17:12:31 - train: epoch 0022, iter [09500, 10009], lr: 0.100000, loss: 2.6625
2022-03-05 17:13:03 - train: epoch 0022, iter [09600, 10009], lr: 0.100000, loss: 2.5618
2022-03-05 17:13:36 - train: epoch 0022, iter [09700, 10009], lr: 0.100000, loss: 2.4977
2022-03-05 17:14:09 - train: epoch 0022, iter [09800, 10009], lr: 0.100000, loss: 2.6526
2022-03-05 17:14:42 - train: epoch 0022, iter [09900, 10009], lr: 0.100000, loss: 2.5983
2022-03-05 17:15:15 - train: epoch 0022, iter [10000, 10009], lr: 0.100000, loss: 2.8457
2022-03-05 17:15:18 - train: epoch 022, train_loss: 2.6864
2022-03-05 17:16:33 - eval: epoch: 022, acc1: 47.256%, acc5: 73.366%, test_loss: 2.3092, per_image_load_time: 1.315ms, per_image_inference_time: 1.564ms
2022-03-05 17:16:33 - until epoch: 022, best_acc1: 47.862%
2022-03-05 17:16:33 - epoch 023 lr: 0.1
2022-03-05 17:17:10 - train: epoch 0023, iter [00100, 10009], lr: 0.100000, loss: 2.3045
2022-03-05 17:17:43 - train: epoch 0023, iter [00200, 10009], lr: 0.100000, loss: 2.4947
2022-03-05 17:18:15 - train: epoch 0023, iter [00300, 10009], lr: 0.100000, loss: 2.1243
2022-03-05 17:18:48 - train: epoch 0023, iter [00400, 10009], lr: 0.100000, loss: 2.5827
2022-03-05 17:19:21 - train: epoch 0023, iter [00500, 10009], lr: 0.100000, loss: 2.9314
2022-03-05 17:19:54 - train: epoch 0023, iter [00600, 10009], lr: 0.100000, loss: 2.6841
2022-03-05 17:20:27 - train: epoch 0023, iter [00700, 10009], lr: 0.100000, loss: 2.6803
2022-03-05 17:20:59 - train: epoch 0023, iter [00800, 10009], lr: 0.100000, loss: 2.6849
2022-03-05 17:21:32 - train: epoch 0023, iter [00900, 10009], lr: 0.100000, loss: 2.5686
2022-03-05 17:22:05 - train: epoch 0023, iter [01000, 10009], lr: 0.100000, loss: 2.9101
2022-03-05 17:22:37 - train: epoch 0023, iter [01100, 10009], lr: 0.100000, loss: 2.5229
2022-03-05 17:23:10 - train: epoch 0023, iter [01200, 10009], lr: 0.100000, loss: 2.5561
2022-03-05 17:23:43 - train: epoch 0023, iter [01300, 10009], lr: 0.100000, loss: 2.2654
2022-03-05 17:24:16 - train: epoch 0023, iter [01400, 10009], lr: 0.100000, loss: 2.5746
2022-03-05 17:24:48 - train: epoch 0023, iter [01500, 10009], lr: 0.100000, loss: 2.5787
2022-03-05 17:25:21 - train: epoch 0023, iter [01600, 10009], lr: 0.100000, loss: 2.9424
2022-03-05 17:25:54 - train: epoch 0023, iter [01700, 10009], lr: 0.100000, loss: 2.6704
2022-03-05 17:26:27 - train: epoch 0023, iter [01800, 10009], lr: 0.100000, loss: 2.7970
2022-03-05 17:27:00 - train: epoch 0023, iter [01900, 10009], lr: 0.100000, loss: 2.9466
2022-03-05 17:27:32 - train: epoch 0023, iter [02000, 10009], lr: 0.100000, loss: 2.8633
2022-03-05 17:28:05 - train: epoch 0023, iter [02100, 10009], lr: 0.100000, loss: 2.7626
2022-03-05 17:28:38 - train: epoch 0023, iter [02200, 10009], lr: 0.100000, loss: 2.6664
2022-03-05 17:29:11 - train: epoch 0023, iter [02300, 10009], lr: 0.100000, loss: 2.9373
2022-03-05 17:29:43 - train: epoch 0023, iter [02400, 10009], lr: 0.100000, loss: 2.5159
2022-03-05 17:30:16 - train: epoch 0023, iter [02500, 10009], lr: 0.100000, loss: 2.2099
2022-03-05 17:30:49 - train: epoch 0023, iter [02600, 10009], lr: 0.100000, loss: 2.3288
2022-03-05 17:31:22 - train: epoch 0023, iter [02700, 10009], lr: 0.100000, loss: 2.2927
2022-03-05 17:31:55 - train: epoch 0023, iter [02800, 10009], lr: 0.100000, loss: 3.1043
2022-03-05 17:32:27 - train: epoch 0023, iter [02900, 10009], lr: 0.100000, loss: 3.2527
2022-03-05 17:33:00 - train: epoch 0023, iter [03000, 10009], lr: 0.100000, loss: 2.5044
2022-03-05 17:33:33 - train: epoch 0023, iter [03100, 10009], lr: 0.100000, loss: 2.7404
2022-03-05 17:34:06 - train: epoch 0023, iter [03200, 10009], lr: 0.100000, loss: 2.6404
2022-03-05 17:34:39 - train: epoch 0023, iter [03300, 10009], lr: 0.100000, loss: 2.8932
2022-03-05 17:35:11 - train: epoch 0023, iter [03400, 10009], lr: 0.100000, loss: 2.8604
2022-03-05 17:35:44 - train: epoch 0023, iter [03500, 10009], lr: 0.100000, loss: 2.3880
2022-03-05 17:36:17 - train: epoch 0023, iter [03600, 10009], lr: 0.100000, loss: 2.8935
2022-03-05 17:36:50 - train: epoch 0023, iter [03700, 10009], lr: 0.100000, loss: 2.6586
2022-03-05 17:37:23 - train: epoch 0023, iter [03800, 10009], lr: 0.100000, loss: 2.9251
2022-03-05 17:37:55 - train: epoch 0023, iter [03900, 10009], lr: 0.100000, loss: 2.5106
2022-03-05 17:38:28 - train: epoch 0023, iter [04000, 10009], lr: 0.100000, loss: 2.2649
2022-03-05 17:39:01 - train: epoch 0023, iter [04100, 10009], lr: 0.100000, loss: 2.8591
2022-03-05 17:39:34 - train: epoch 0023, iter [04200, 10009], lr: 0.100000, loss: 2.6204
2022-03-05 17:40:07 - train: epoch 0023, iter [04300, 10009], lr: 0.100000, loss: 2.8437
2022-03-05 17:40:39 - train: epoch 0023, iter [04400, 10009], lr: 0.100000, loss: 2.3021
2022-03-05 17:41:12 - train: epoch 0023, iter [04500, 10009], lr: 0.100000, loss: 2.4572
2022-03-05 17:41:45 - train: epoch 0023, iter [04600, 10009], lr: 0.100000, loss: 2.8104
2022-03-05 17:42:18 - train: epoch 0023, iter [04700, 10009], lr: 0.100000, loss: 2.6214
2022-03-05 17:42:51 - train: epoch 0023, iter [04800, 10009], lr: 0.100000, loss: 2.6984
2022-03-05 17:43:23 - train: epoch 0023, iter [04900, 10009], lr: 0.100000, loss: 2.4348
2022-03-05 17:43:56 - train: epoch 0023, iter [05000, 10009], lr: 0.100000, loss: 2.5684
2022-03-05 17:44:29 - train: epoch 0023, iter [05100, 10009], lr: 0.100000, loss: 2.7760
2022-03-05 17:45:02 - train: epoch 0023, iter [05200, 10009], lr: 0.100000, loss: 2.6528
2022-03-05 17:45:35 - train: epoch 0023, iter [05300, 10009], lr: 0.100000, loss: 2.3946
2022-03-05 17:46:08 - train: epoch 0023, iter [05400, 10009], lr: 0.100000, loss: 2.7065
2022-03-05 17:46:40 - train: epoch 0023, iter [05500, 10009], lr: 0.100000, loss: 2.4015
2022-03-05 17:47:13 - train: epoch 0023, iter [05600, 10009], lr: 0.100000, loss: 2.9169
2022-03-05 17:47:46 - train: epoch 0023, iter [05700, 10009], lr: 0.100000, loss: 2.6231
2022-03-05 17:48:19 - train: epoch 0023, iter [05800, 10009], lr: 0.100000, loss: 3.2428
2022-03-05 17:48:51 - train: epoch 0023, iter [05900, 10009], lr: 0.100000, loss: 2.5620
2022-03-05 17:49:24 - train: epoch 0023, iter [06000, 10009], lr: 0.100000, loss: 2.8034
2022-03-05 17:49:57 - train: epoch 0023, iter [06100, 10009], lr: 0.100000, loss: 2.6374
2022-03-05 17:50:30 - train: epoch 0023, iter [06200, 10009], lr: 0.100000, loss: 2.7431
2022-03-05 17:51:02 - train: epoch 0023, iter [06300, 10009], lr: 0.100000, loss: 2.6565
2022-03-05 17:51:35 - train: epoch 0023, iter [06400, 10009], lr: 0.100000, loss: 2.7897
2022-03-05 17:52:08 - train: epoch 0023, iter [06500, 10009], lr: 0.100000, loss: 3.1224
2022-03-05 17:52:41 - train: epoch 0023, iter [06600, 10009], lr: 0.100000, loss: 2.8384
2022-03-05 17:53:14 - train: epoch 0023, iter [06700, 10009], lr: 0.100000, loss: 2.6808
2022-03-05 17:53:46 - train: epoch 0023, iter [06800, 10009], lr: 0.100000, loss: 2.5643
2022-03-05 17:54:19 - train: epoch 0023, iter [06900, 10009], lr: 0.100000, loss: 3.2057
2022-03-05 17:54:52 - train: epoch 0023, iter [07000, 10009], lr: 0.100000, loss: 2.6402
2022-03-05 17:55:25 - train: epoch 0023, iter [07100, 10009], lr: 0.100000, loss: 3.0271
2022-03-05 17:55:58 - train: epoch 0023, iter [07200, 10009], lr: 0.100000, loss: 2.5885
2022-03-05 17:56:30 - train: epoch 0023, iter [07300, 10009], lr: 0.100000, loss: 2.8121
2022-03-05 17:57:03 - train: epoch 0023, iter [07400, 10009], lr: 0.100000, loss: 2.6307
2022-03-05 17:57:36 - train: epoch 0023, iter [07500, 10009], lr: 0.100000, loss: 2.6876
2022-03-05 17:58:09 - train: epoch 0023, iter [07600, 10009], lr: 0.100000, loss: 2.8727
2022-03-05 17:58:42 - train: epoch 0023, iter [07700, 10009], lr: 0.100000, loss: 2.8643
2022-03-05 17:59:14 - train: epoch 0023, iter [07800, 10009], lr: 0.100000, loss: 2.4494
2022-03-05 17:59:47 - train: epoch 0023, iter [07900, 10009], lr: 0.100000, loss: 2.6691
2022-03-05 18:00:20 - train: epoch 0023, iter [08000, 10009], lr: 0.100000, loss: 2.4543
2022-03-05 18:00:53 - train: epoch 0023, iter [08100, 10009], lr: 0.100000, loss: 3.1852
2022-03-05 18:01:25 - train: epoch 0023, iter [08200, 10009], lr: 0.100000, loss: 2.6542
2022-03-05 18:01:58 - train: epoch 0023, iter [08300, 10009], lr: 0.100000, loss: 2.6124
2022-03-05 18:02:31 - train: epoch 0023, iter [08400, 10009], lr: 0.100000, loss: 2.3751
2022-03-05 18:03:04 - train: epoch 0023, iter [08500, 10009], lr: 0.100000, loss: 2.2218
2022-03-05 18:03:37 - train: epoch 0023, iter [08600, 10009], lr: 0.100000, loss: 2.5832
2022-03-05 18:04:09 - train: epoch 0023, iter [08700, 10009], lr: 0.100000, loss: 2.5783
2022-03-05 18:04:42 - train: epoch 0023, iter [08800, 10009], lr: 0.100000, loss: 2.3952
2022-03-05 18:05:15 - train: epoch 0023, iter [08900, 10009], lr: 0.100000, loss: 2.5783
2022-03-05 18:05:48 - train: epoch 0023, iter [09000, 10009], lr: 0.100000, loss: 2.4750
2022-03-05 18:06:21 - train: epoch 0023, iter [09100, 10009], lr: 0.100000, loss: 2.7518
2022-03-05 18:06:53 - train: epoch 0023, iter [09200, 10009], lr: 0.100000, loss: 2.6741
2022-03-05 18:07:26 - train: epoch 0023, iter [09300, 10009], lr: 0.100000, loss: 3.0890
2022-03-05 18:07:59 - train: epoch 0023, iter [09400, 10009], lr: 0.100000, loss: 2.4246
2022-03-05 18:08:32 - train: epoch 0023, iter [09500, 10009], lr: 0.100000, loss: 2.7722
2022-03-05 18:09:05 - train: epoch 0023, iter [09600, 10009], lr: 0.100000, loss: 2.5433
2022-03-05 18:09:37 - train: epoch 0023, iter [09700, 10009], lr: 0.100000, loss: 2.6304
2022-03-05 18:10:10 - train: epoch 0023, iter [09800, 10009], lr: 0.100000, loss: 2.5182
2022-03-05 18:10:43 - train: epoch 0023, iter [09900, 10009], lr: 0.100000, loss: 2.6043
2022-03-05 18:11:16 - train: epoch 0023, iter [10000, 10009], lr: 0.100000, loss: 2.7546
2022-03-05 18:11:19 - train: epoch 023, train_loss: 2.6799
2022-03-05 18:12:33 - eval: epoch: 023, acc1: 47.426%, acc5: 73.748%, test_loss: 2.3021, per_image_load_time: 0.954ms, per_image_inference_time: 1.573ms
2022-03-05 18:12:34 - until epoch: 023, best_acc1: 47.862%
2022-03-05 18:12:34 - epoch 024 lr: 0.1
2022-03-05 18:13:11 - train: epoch 0024, iter [00100, 10009], lr: 0.100000, loss: 2.8329
2022-03-05 18:13:43 - train: epoch 0024, iter [00200, 10009], lr: 0.100000, loss: 2.9847
2022-03-05 18:14:16 - train: epoch 0024, iter [00300, 10009], lr: 0.100000, loss: 2.5185
2022-03-05 18:14:49 - train: epoch 0024, iter [00400, 10009], lr: 0.100000, loss: 2.6640
2022-03-05 18:15:22 - train: epoch 0024, iter [00500, 10009], lr: 0.100000, loss: 2.9487
2022-03-05 18:15:55 - train: epoch 0024, iter [00600, 10009], lr: 0.100000, loss: 2.9091
2022-03-05 18:16:27 - train: epoch 0024, iter [00700, 10009], lr: 0.100000, loss: 2.4065
2022-03-05 18:17:00 - train: epoch 0024, iter [00800, 10009], lr: 0.100000, loss: 2.6271
2022-03-05 18:17:33 - train: epoch 0024, iter [00900, 10009], lr: 0.100000, loss: 2.6424
2022-03-05 18:18:06 - train: epoch 0024, iter [01000, 10009], lr: 0.100000, loss: 2.6803
2022-03-05 18:18:39 - train: epoch 0024, iter [01100, 10009], lr: 0.100000, loss: 2.5397
2022-03-05 18:19:11 - train: epoch 0024, iter [01200, 10009], lr: 0.100000, loss: 2.6049
2022-03-05 18:19:44 - train: epoch 0024, iter [01300, 10009], lr: 0.100000, loss: 2.4892
2022-03-05 18:20:17 - train: epoch 0024, iter [01400, 10009], lr: 0.100000, loss: 3.0270
2022-03-05 18:20:50 - train: epoch 0024, iter [01500, 10009], lr: 0.100000, loss: 2.4291
2022-03-05 18:21:23 - train: epoch 0024, iter [01600, 10009], lr: 0.100000, loss: 2.5854
2022-03-05 18:21:56 - train: epoch 0024, iter [01700, 10009], lr: 0.100000, loss: 2.7074
2022-03-05 18:22:28 - train: epoch 0024, iter [01800, 10009], lr: 0.100000, loss: 2.5198
2022-03-05 18:23:01 - train: epoch 0024, iter [01900, 10009], lr: 0.100000, loss: 2.6242
2022-03-05 18:23:34 - train: epoch 0024, iter [02000, 10009], lr: 0.100000, loss: 2.6765
2022-03-05 18:24:06 - train: epoch 0024, iter [02100, 10009], lr: 0.100000, loss: 2.4501
2022-03-05 18:24:39 - train: epoch 0024, iter [02200, 10009], lr: 0.100000, loss: 2.6881
2022-03-05 18:25:12 - train: epoch 0024, iter [02300, 10009], lr: 0.100000, loss: 2.7425
2022-03-05 18:25:45 - train: epoch 0024, iter [02400, 10009], lr: 0.100000, loss: 2.6383
2022-03-05 18:26:17 - train: epoch 0024, iter [02500, 10009], lr: 0.100000, loss: 2.7075
2022-03-05 18:26:50 - train: epoch 0024, iter [02600, 10009], lr: 0.100000, loss: 2.6970
2022-03-05 18:27:23 - train: epoch 0024, iter [02700, 10009], lr: 0.100000, loss: 2.8124
2022-03-05 18:27:56 - train: epoch 0024, iter [02800, 10009], lr: 0.100000, loss: 2.5882
2022-03-05 18:28:28 - train: epoch 0024, iter [02900, 10009], lr: 0.100000, loss: 3.0965
2022-03-05 18:29:01 - train: epoch 0024, iter [03000, 10009], lr: 0.100000, loss: 2.6605
2022-03-05 18:29:34 - train: epoch 0024, iter [03100, 10009], lr: 0.100000, loss: 2.6964
2022-03-05 18:30:07 - train: epoch 0024, iter [03200, 10009], lr: 0.100000, loss: 2.6874
2022-03-05 18:30:40 - train: epoch 0024, iter [03300, 10009], lr: 0.100000, loss: 2.8892
2022-03-05 18:31:13 - train: epoch 0024, iter [03400, 10009], lr: 0.100000, loss: 2.5277
2022-03-05 18:31:45 - train: epoch 0024, iter [03500, 10009], lr: 0.100000, loss: 2.4749
2022-03-05 18:32:18 - train: epoch 0024, iter [03600, 10009], lr: 0.100000, loss: 3.0412
2022-03-05 18:32:51 - train: epoch 0024, iter [03700, 10009], lr: 0.100000, loss: 2.7672
2022-03-05 18:33:24 - train: epoch 0024, iter [03800, 10009], lr: 0.100000, loss: 2.7343
2022-03-05 18:33:57 - train: epoch 0024, iter [03900, 10009], lr: 0.100000, loss: 2.3747
2022-03-05 18:34:29 - train: epoch 0024, iter [04000, 10009], lr: 0.100000, loss: 2.5564
2022-03-05 18:35:02 - train: epoch 0024, iter [04100, 10009], lr: 0.100000, loss: 2.5859
2022-03-05 18:35:35 - train: epoch 0024, iter [04200, 10009], lr: 0.100000, loss: 2.5388
2022-03-05 18:36:08 - train: epoch 0024, iter [04300, 10009], lr: 0.100000, loss: 2.8803
2022-03-05 18:36:41 - train: epoch 0024, iter [04400, 10009], lr: 0.100000, loss: 2.5286
2022-03-05 18:37:13 - train: epoch 0024, iter [04500, 10009], lr: 0.100000, loss: 2.4840
2022-03-05 18:37:46 - train: epoch 0024, iter [04600, 10009], lr: 0.100000, loss: 2.3766
2022-03-05 18:38:19 - train: epoch 0024, iter [04700, 10009], lr: 0.100000, loss: 2.5648
2022-03-05 18:38:52 - train: epoch 0024, iter [04800, 10009], lr: 0.100000, loss: 2.7608
2022-03-05 18:39:25 - train: epoch 0024, iter [04900, 10009], lr: 0.100000, loss: 2.1951
2022-03-05 18:39:57 - train: epoch 0024, iter [05000, 10009], lr: 0.100000, loss: 2.5729
2022-03-05 18:40:30 - train: epoch 0024, iter [05100, 10009], lr: 0.100000, loss: 2.5661
2022-03-05 18:41:03 - train: epoch 0024, iter [05200, 10009], lr: 0.100000, loss: 3.1922
2022-03-05 18:41:36 - train: epoch 0024, iter [05300, 10009], lr: 0.100000, loss: 2.4904
2022-03-05 18:42:08 - train: epoch 0024, iter [05400, 10009], lr: 0.100000, loss: 2.7786
2022-03-05 18:42:41 - train: epoch 0024, iter [05500, 10009], lr: 0.100000, loss: 2.7773
2022-03-05 18:43:14 - train: epoch 0024, iter [05600, 10009], lr: 0.100000, loss: 2.6846
2022-03-05 18:43:47 - train: epoch 0024, iter [05700, 10009], lr: 0.100000, loss: 2.5942
2022-03-05 18:44:20 - train: epoch 0024, iter [05800, 10009], lr: 0.100000, loss: 2.7897
2022-03-05 18:44:53 - train: epoch 0024, iter [05900, 10009], lr: 0.100000, loss: 2.5848
2022-03-05 18:45:25 - train: epoch 0024, iter [06000, 10009], lr: 0.100000, loss: 2.7584
2022-03-05 18:45:58 - train: epoch 0024, iter [06100, 10009], lr: 0.100000, loss: 2.9189
2022-03-05 18:46:31 - train: epoch 0024, iter [06200, 10009], lr: 0.100000, loss: 2.4421
2022-03-05 18:47:04 - train: epoch 0024, iter [06300, 10009], lr: 0.100000, loss: 2.4444
2022-03-05 18:47:36 - train: epoch 0024, iter [06400, 10009], lr: 0.100000, loss: 3.0172
2022-03-05 18:48:09 - train: epoch 0024, iter [06500, 10009], lr: 0.100000, loss: 2.8722
2022-03-05 18:48:42 - train: epoch 0024, iter [06600, 10009], lr: 0.100000, loss: 2.4272
2022-03-05 18:49:15 - train: epoch 0024, iter [06700, 10009], lr: 0.100000, loss: 2.7002
2022-03-05 18:49:48 - train: epoch 0024, iter [06800, 10009], lr: 0.100000, loss: 2.5776
2022-03-05 18:50:20 - train: epoch 0024, iter [06900, 10009], lr: 0.100000, loss: 2.8189
2022-03-05 18:50:53 - train: epoch 0024, iter [07000, 10009], lr: 0.100000, loss: 2.5202
2022-03-05 18:51:26 - train: epoch 0024, iter [07100, 10009], lr: 0.100000, loss: 2.4583
2022-03-05 18:51:59 - train: epoch 0024, iter [07200, 10009], lr: 0.100000, loss: 2.5567
2022-03-05 18:52:31 - train: epoch 0024, iter [07300, 10009], lr: 0.100000, loss: 2.6831
2022-03-05 18:53:04 - train: epoch 0024, iter [07400, 10009], lr: 0.100000, loss: 2.7969
2022-03-05 18:53:37 - train: epoch 0024, iter [07500, 10009], lr: 0.100000, loss: 2.7765
2022-03-05 18:54:10 - train: epoch 0024, iter [07600, 10009], lr: 0.100000, loss: 2.7886
2022-03-05 18:54:43 - train: epoch 0024, iter [07700, 10009], lr: 0.100000, loss: 2.4309
2022-03-05 18:55:15 - train: epoch 0024, iter [07800, 10009], lr: 0.100000, loss: 2.8612
2022-03-05 18:55:48 - train: epoch 0024, iter [07900, 10009], lr: 0.100000, loss: 2.5864
2022-03-05 18:56:21 - train: epoch 0024, iter [08000, 10009], lr: 0.100000, loss: 2.5295
2022-03-05 18:56:54 - train: epoch 0024, iter [08100, 10009], lr: 0.100000, loss: 2.7565
2022-03-05 18:57:27 - train: epoch 0024, iter [08200, 10009], lr: 0.100000, loss: 2.5897
2022-03-05 18:57:59 - train: epoch 0024, iter [08300, 10009], lr: 0.100000, loss: 2.6216
2022-03-05 18:58:32 - train: epoch 0024, iter [08400, 10009], lr: 0.100000, loss: 2.4393
2022-03-05 18:59:05 - train: epoch 0024, iter [08500, 10009], lr: 0.100000, loss: 2.7864
2022-03-05 18:59:37 - train: epoch 0024, iter [08600, 10009], lr: 0.100000, loss: 2.6749
2022-03-05 19:00:10 - train: epoch 0024, iter [08700, 10009], lr: 0.100000, loss: 2.4748
2022-03-05 19:00:43 - train: epoch 0024, iter [08800, 10009], lr: 0.100000, loss: 2.4901
2022-03-05 19:01:15 - train: epoch 0024, iter [08900, 10009], lr: 0.100000, loss: 2.8164
2022-03-05 19:01:48 - train: epoch 0024, iter [09000, 10009], lr: 0.100000, loss: 2.4078
2022-03-05 19:02:21 - train: epoch 0024, iter [09100, 10009], lr: 0.100000, loss: 2.8702
2022-03-05 19:02:54 - train: epoch 0024, iter [09200, 10009], lr: 0.100000, loss: 2.5957
2022-03-05 19:03:26 - train: epoch 0024, iter [09300, 10009], lr: 0.100000, loss: 2.3542
2022-03-05 19:03:59 - train: epoch 0024, iter [09400, 10009], lr: 0.100000, loss: 2.6857
2022-03-05 19:04:32 - train: epoch 0024, iter [09500, 10009], lr: 0.100000, loss: 2.5620
2022-03-05 19:05:04 - train: epoch 0024, iter [09600, 10009], lr: 0.100000, loss: 2.5220
2022-03-05 19:05:37 - train: epoch 0024, iter [09700, 10009], lr: 0.100000, loss: 2.6549
2022-03-05 19:06:10 - train: epoch 0024, iter [09800, 10009], lr: 0.100000, loss: 2.8395
2022-03-05 19:06:42 - train: epoch 0024, iter [09900, 10009], lr: 0.100000, loss: 2.7820
2022-03-05 19:07:15 - train: epoch 0024, iter [10000, 10009], lr: 0.100000, loss: 2.9244
2022-03-05 19:07:18 - train: epoch 024, train_loss: 2.6738
2022-03-05 19:08:35 - eval: epoch: 024, acc1: 47.644%, acc5: 73.568%, test_loss: 2.3052, per_image_load_time: 1.418ms, per_image_inference_time: 1.541ms
2022-03-05 19:08:36 - until epoch: 024, best_acc1: 47.862%
2022-03-05 19:08:36 - epoch 025 lr: 0.1
2022-03-05 19:09:12 - train: epoch 0025, iter [00100, 10009], lr: 0.100000, loss: 2.4195
2022-03-05 19:09:45 - train: epoch 0025, iter [00200, 10009], lr: 0.100000, loss: 2.8213
2022-03-05 19:10:18 - train: epoch 0025, iter [00300, 10009], lr: 0.100000, loss: 2.5503
2022-03-05 19:10:50 - train: epoch 0025, iter [00400, 10009], lr: 0.100000, loss: 2.4159
2022-03-05 19:11:23 - train: epoch 0025, iter [00500, 10009], lr: 0.100000, loss: 2.9650
2022-03-05 19:11:56 - train: epoch 0025, iter [00600, 10009], lr: 0.100000, loss: 2.4023
2022-03-05 19:12:29 - train: epoch 0025, iter [00700, 10009], lr: 0.100000, loss: 2.8782
2022-03-05 19:13:02 - train: epoch 0025, iter [00800, 10009], lr: 0.100000, loss: 2.8278
2022-03-05 19:13:35 - train: epoch 0025, iter [00900, 10009], lr: 0.100000, loss: 2.5453
2022-03-05 19:14:08 - train: epoch 0025, iter [01000, 10009], lr: 0.100000, loss: 2.6846
2022-03-05 19:14:41 - train: epoch 0025, iter [01100, 10009], lr: 0.100000, loss: 2.2380
2022-03-05 19:15:14 - train: epoch 0025, iter [01200, 10009], lr: 0.100000, loss: 2.8044
2022-03-05 19:15:47 - train: epoch 0025, iter [01300, 10009], lr: 0.100000, loss: 2.7134
2022-03-05 19:16:19 - train: epoch 0025, iter [01400, 10009], lr: 0.100000, loss: 2.7709
2022-03-05 19:16:52 - train: epoch 0025, iter [01500, 10009], lr: 0.100000, loss: 2.8202
2022-03-05 19:17:25 - train: epoch 0025, iter [01600, 10009], lr: 0.100000, loss: 2.7821
2022-03-05 19:17:58 - train: epoch 0025, iter [01700, 10009], lr: 0.100000, loss: 2.4837
2022-03-05 19:18:31 - train: epoch 0025, iter [01800, 10009], lr: 0.100000, loss: 2.2613
2022-03-05 19:19:03 - train: epoch 0025, iter [01900, 10009], lr: 0.100000, loss: 2.2963
2022-03-05 19:19:36 - train: epoch 0025, iter [02000, 10009], lr: 0.100000, loss: 2.4128
2022-03-05 19:20:09 - train: epoch 0025, iter [02100, 10009], lr: 0.100000, loss: 3.0979
2022-03-05 19:20:42 - train: epoch 0025, iter [02200, 10009], lr: 0.100000, loss: 2.9395
2022-03-05 19:21:15 - train: epoch 0025, iter [02300, 10009], lr: 0.100000, loss: 2.9408
2022-03-05 19:21:48 - train: epoch 0025, iter [02400, 10009], lr: 0.100000, loss: 2.6358
2022-03-05 19:22:21 - train: epoch 0025, iter [02500, 10009], lr: 0.100000, loss: 2.7077
2022-03-05 19:22:54 - train: epoch 0025, iter [02600, 10009], lr: 0.100000, loss: 2.4301
2022-03-05 19:23:27 - train: epoch 0025, iter [02700, 10009], lr: 0.100000, loss: 2.4922
2022-03-05 19:24:00 - train: epoch 0025, iter [02800, 10009], lr: 0.100000, loss: 3.0535
2022-03-05 19:24:33 - train: epoch 0025, iter [02900, 10009], lr: 0.100000, loss: 2.6316
2022-03-05 19:25:06 - train: epoch 0025, iter [03000, 10009], lr: 0.100000, loss: 2.4482
2022-03-05 19:25:39 - train: epoch 0025, iter [03100, 10009], lr: 0.100000, loss: 2.6689
2022-03-05 19:26:12 - train: epoch 0025, iter [03200, 10009], lr: 0.100000, loss: 2.5785
2022-03-05 19:26:45 - train: epoch 0025, iter [03300, 10009], lr: 0.100000, loss: 2.6172
2022-03-05 19:27:17 - train: epoch 0025, iter [03400, 10009], lr: 0.100000, loss: 2.7143
2022-03-05 19:27:50 - train: epoch 0025, iter [03500, 10009], lr: 0.100000, loss: 2.4889
2022-03-05 19:28:23 - train: epoch 0025, iter [03600, 10009], lr: 0.100000, loss: 2.5774
2022-03-05 19:28:56 - train: epoch 0025, iter [03700, 10009], lr: 0.100000, loss: 2.3391
2022-03-05 19:29:29 - train: epoch 0025, iter [03800, 10009], lr: 0.100000, loss: 2.5547
2022-03-05 19:30:02 - train: epoch 0025, iter [03900, 10009], lr: 0.100000, loss: 2.6434
2022-03-05 19:30:35 - train: epoch 0025, iter [04000, 10009], lr: 0.100000, loss: 2.7005
2022-03-05 19:31:08 - train: epoch 0025, iter [04100, 10009], lr: 0.100000, loss: 2.3769
2022-03-05 19:31:40 - train: epoch 0025, iter [04200, 10009], lr: 0.100000, loss: 2.6064
2022-03-05 19:32:13 - train: epoch 0025, iter [04300, 10009], lr: 0.100000, loss: 2.5349
2022-03-05 19:32:46 - train: epoch 0025, iter [04400, 10009], lr: 0.100000, loss: 2.3776
2022-03-05 19:33:19 - train: epoch 0025, iter [04500, 10009], lr: 0.100000, loss: 2.6281
2022-03-05 19:33:52 - train: epoch 0025, iter [04600, 10009], lr: 0.100000, loss: 2.5429
2022-03-05 19:34:25 - train: epoch 0025, iter [04700, 10009], lr: 0.100000, loss: 2.6147
2022-03-05 19:34:58 - train: epoch 0025, iter [04800, 10009], lr: 0.100000, loss: 2.6521
2022-03-05 19:35:31 - train: epoch 0025, iter [04900, 10009], lr: 0.100000, loss: 2.5487
2022-03-05 19:36:03 - train: epoch 0025, iter [05000, 10009], lr: 0.100000, loss: 2.9592
2022-03-05 19:36:36 - train: epoch 0025, iter [05100, 10009], lr: 0.100000, loss: 2.7914
2022-03-05 19:37:09 - train: epoch 0025, iter [05200, 10009], lr: 0.100000, loss: 2.5061
2022-03-05 19:37:42 - train: epoch 0025, iter [05300, 10009], lr: 0.100000, loss: 2.5175
2022-03-05 19:38:15 - train: epoch 0025, iter [05400, 10009], lr: 0.100000, loss: 2.8021
2022-03-05 19:38:48 - train: epoch 0025, iter [05500, 10009], lr: 0.100000, loss: 2.6896
2022-03-05 19:39:21 - train: epoch 0025, iter [05600, 10009], lr: 0.100000, loss: 2.4734
2022-03-05 19:39:54 - train: epoch 0025, iter [05700, 10009], lr: 0.100000, loss: 2.6032
2022-03-05 19:40:27 - train: epoch 0025, iter [05800, 10009], lr: 0.100000, loss: 2.7540
2022-03-05 19:41:00 - train: epoch 0025, iter [05900, 10009], lr: 0.100000, loss: 2.5903
2022-03-05 19:41:33 - train: epoch 0025, iter [06000, 10009], lr: 0.100000, loss: 2.6013
2022-03-05 19:42:06 - train: epoch 0025, iter [06100, 10009], lr: 0.100000, loss: 2.4572
2022-03-05 19:42:39 - train: epoch 0025, iter [06200, 10009], lr: 0.100000, loss: 2.7639
2022-03-05 19:43:12 - train: epoch 0025, iter [06300, 10009], lr: 0.100000, loss: 2.4506
2022-03-05 19:43:45 - train: epoch 0025, iter [06400, 10009], lr: 0.100000, loss: 2.6790
2022-03-05 19:44:18 - train: epoch 0025, iter [06500, 10009], lr: 0.100000, loss: 2.7936
2022-03-05 19:44:51 - train: epoch 0025, iter [06600, 10009], lr: 0.100000, loss: 2.8327
2022-03-05 19:45:24 - train: epoch 0025, iter [06700, 10009], lr: 0.100000, loss: 3.0321
2022-03-05 19:45:57 - train: epoch 0025, iter [06800, 10009], lr: 0.100000, loss: 2.7592
2022-03-05 19:46:30 - train: epoch 0025, iter [06900, 10009], lr: 0.100000, loss: 3.0350
2022-03-05 19:47:03 - train: epoch 0025, iter [07000, 10009], lr: 0.100000, loss: 2.5966
2022-03-05 19:47:36 - train: epoch 0025, iter [07100, 10009], lr: 0.100000, loss: 2.6785
2022-03-05 19:48:09 - train: epoch 0025, iter [07200, 10009], lr: 0.100000, loss: 2.8707
2022-03-05 19:48:42 - train: epoch 0025, iter [07300, 10009], lr: 0.100000, loss: 2.5647
2022-03-05 19:49:15 - train: epoch 0025, iter [07400, 10009], lr: 0.100000, loss: 2.9071
2022-03-05 19:49:47 - train: epoch 0025, iter [07500, 10009], lr: 0.100000, loss: 2.7212
2022-03-05 19:50:20 - train: epoch 0025, iter [07600, 10009], lr: 0.100000, loss: 2.9970
2022-03-05 19:50:53 - train: epoch 0025, iter [07700, 10009], lr: 0.100000, loss: 2.5149
2022-03-05 19:51:26 - train: epoch 0025, iter [07800, 10009], lr: 0.100000, loss: 3.1618
2022-03-05 19:51:59 - train: epoch 0025, iter [07900, 10009], lr: 0.100000, loss: 2.8478
2022-03-05 19:52:31 - train: epoch 0025, iter [08000, 10009], lr: 0.100000, loss: 2.7168
2022-03-05 19:53:04 - train: epoch 0025, iter [08100, 10009], lr: 0.100000, loss: 2.7716
2022-03-05 19:53:37 - train: epoch 0025, iter [08200, 10009], lr: 0.100000, loss: 3.0330
2022-03-05 19:54:10 - train: epoch 0025, iter [08300, 10009], lr: 0.100000, loss: 2.4278
2022-03-05 19:54:43 - train: epoch 0025, iter [08400, 10009], lr: 0.100000, loss: 2.9798
2022-03-05 19:55:16 - train: epoch 0025, iter [08500, 10009], lr: 0.100000, loss: 2.7090
2022-03-05 19:55:49 - train: epoch 0025, iter [08600, 10009], lr: 0.100000, loss: 2.5824
2022-03-05 19:56:22 - train: epoch 0025, iter [08700, 10009], lr: 0.100000, loss: 2.2186
2022-03-05 19:56:55 - train: epoch 0025, iter [08800, 10009], lr: 0.100000, loss: 2.4865
2022-03-05 19:57:28 - train: epoch 0025, iter [08900, 10009], lr: 0.100000, loss: 2.4474
2022-03-05 19:58:00 - train: epoch 0025, iter [09000, 10009], lr: 0.100000, loss: 2.6728
2022-03-05 19:58:33 - train: epoch 0025, iter [09100, 10009], lr: 0.100000, loss: 2.7448
2022-03-05 19:59:06 - train: epoch 0025, iter [09200, 10009], lr: 0.100000, loss: 2.7316
2022-03-05 19:59:39 - train: epoch 0025, iter [09300, 10009], lr: 0.100000, loss: 2.7875
2022-03-05 20:00:12 - train: epoch 0025, iter [09400, 10009], lr: 0.100000, loss: 2.7142
2022-03-05 20:00:45 - train: epoch 0025, iter [09500, 10009], lr: 0.100000, loss: 2.8412
2022-03-05 20:01:18 - train: epoch 0025, iter [09600, 10009], lr: 0.100000, loss: 2.6652
2022-03-05 20:01:51 - train: epoch 0025, iter [09700, 10009], lr: 0.100000, loss: 2.3203
2022-03-05 20:02:24 - train: epoch 0025, iter [09800, 10009], lr: 0.100000, loss: 2.9449
2022-03-05 20:02:57 - train: epoch 0025, iter [09900, 10009], lr: 0.100000, loss: 2.6796
2022-03-05 20:03:30 - train: epoch 0025, iter [10000, 10009], lr: 0.100000, loss: 2.7604
2022-03-05 20:03:33 - train: epoch 025, train_loss: 2.6661
2022-03-05 20:04:54 - eval: epoch: 025, acc1: 47.306%, acc5: 73.550%, test_loss: 2.3128, per_image_load_time: 1.135ms, per_image_inference_time: 1.558ms
2022-03-05 20:04:54 - until epoch: 025, best_acc1: 47.862%
2022-03-05 20:04:54 - epoch 026 lr: 0.1
2022-03-05 20:05:31 - train: epoch 0026, iter [00100, 10009], lr: 0.100000, loss: 2.3397
2022-03-05 20:06:04 - train: epoch 0026, iter [00200, 10009], lr: 0.100000, loss: 2.3899
2022-03-05 20:06:37 - train: epoch 0026, iter [00300, 10009], lr: 0.100000, loss: 2.3790
2022-03-05 20:07:09 - train: epoch 0026, iter [00400, 10009], lr: 0.100000, loss: 2.3799
2022-03-05 20:07:42 - train: epoch 0026, iter [00500, 10009], lr: 0.100000, loss: 2.9010
2022-03-05 20:08:15 - train: epoch 0026, iter [00600, 10009], lr: 0.100000, loss: 2.7580
2022-03-05 20:08:47 - train: epoch 0026, iter [00700, 10009], lr: 0.100000, loss: 2.8031
2022-03-05 20:09:20 - train: epoch 0026, iter [00800, 10009], lr: 0.100000, loss: 2.6219
2022-03-05 20:09:53 - train: epoch 0026, iter [00900, 10009], lr: 0.100000, loss: 2.7599
2022-03-05 20:10:26 - train: epoch 0026, iter [01000, 10009], lr: 0.100000, loss: 2.7939
2022-03-05 20:10:58 - train: epoch 0026, iter [01100, 10009], lr: 0.100000, loss: 2.4130
2022-03-05 20:11:31 - train: epoch 0026, iter [01200, 10009], lr: 0.100000, loss: 3.0875
2022-03-05 20:12:04 - train: epoch 0026, iter [01300, 10009], lr: 0.100000, loss: 2.4843
2022-03-05 20:12:36 - train: epoch 0026, iter [01400, 10009], lr: 0.100000, loss: 2.9537
2022-03-05 20:13:09 - train: epoch 0026, iter [01500, 10009], lr: 0.100000, loss: 2.9354
2022-03-05 20:13:42 - train: epoch 0026, iter [01600, 10009], lr: 0.100000, loss: 2.3855
2022-03-05 20:14:15 - train: epoch 0026, iter [01700, 10009], lr: 0.100000, loss: 2.9236
2022-03-05 20:14:47 - train: epoch 0026, iter [01800, 10009], lr: 0.100000, loss: 2.5650
2022-03-05 20:15:20 - train: epoch 0026, iter [01900, 10009], lr: 0.100000, loss: 2.7732
2022-03-05 20:15:53 - train: epoch 0026, iter [02000, 10009], lr: 0.100000, loss: 2.7072
2022-03-05 20:16:26 - train: epoch 0026, iter [02100, 10009], lr: 0.100000, loss: 2.5249
2022-03-05 20:16:59 - train: epoch 0026, iter [02200, 10009], lr: 0.100000, loss: 2.8540
2022-03-05 20:17:31 - train: epoch 0026, iter [02300, 10009], lr: 0.100000, loss: 2.9159
2022-03-05 20:18:04 - train: epoch 0026, iter [02400, 10009], lr: 0.100000, loss: 2.6744
2022-03-05 20:18:37 - train: epoch 0026, iter [02500, 10009], lr: 0.100000, loss: 2.9003
2022-03-05 20:19:10 - train: epoch 0026, iter [02600, 10009], lr: 0.100000, loss: 2.8910
2022-03-05 20:19:43 - train: epoch 0026, iter [02700, 10009], lr: 0.100000, loss: 2.4490
2022-03-05 20:20:16 - train: epoch 0026, iter [02800, 10009], lr: 0.100000, loss: 2.8014
2022-03-05 20:20:48 - train: epoch 0026, iter [02900, 10009], lr: 0.100000, loss: 2.4241
2022-03-05 20:21:21 - train: epoch 0026, iter [03000, 10009], lr: 0.100000, loss: 2.4962
2022-03-05 20:21:54 - train: epoch 0026, iter [03100, 10009], lr: 0.100000, loss: 2.4166
2022-03-05 20:22:27 - train: epoch 0026, iter [03200, 10009], lr: 0.100000, loss: 2.9606
2022-03-05 20:23:00 - train: epoch 0026, iter [03300, 10009], lr: 0.100000, loss: 2.6278
2022-03-05 20:23:33 - train: epoch 0026, iter [03400, 10009], lr: 0.100000, loss: 2.5465
2022-03-05 20:24:06 - train: epoch 0026, iter [03500, 10009], lr: 0.100000, loss: 2.3255
2022-03-05 20:24:38 - train: epoch 0026, iter [03600, 10009], lr: 0.100000, loss: 3.0310
2022-03-05 20:25:11 - train: epoch 0026, iter [03700, 10009], lr: 0.100000, loss: 2.6366
2022-03-05 20:25:44 - train: epoch 0026, iter [03800, 10009], lr: 0.100000, loss: 2.8738
2022-03-05 20:26:17 - train: epoch 0026, iter [03900, 10009], lr: 0.100000, loss: 2.7904
2022-03-05 20:26:50 - train: epoch 0026, iter [04000, 10009], lr: 0.100000, loss: 2.5837
2022-03-05 20:27:23 - train: epoch 0026, iter [04100, 10009], lr: 0.100000, loss: 2.3778
2022-03-05 20:27:56 - train: epoch 0026, iter [04200, 10009], lr: 0.100000, loss: 2.7441
2022-03-05 20:28:28 - train: epoch 0026, iter [04300, 10009], lr: 0.100000, loss: 2.6441
2022-03-05 20:29:01 - train: epoch 0026, iter [04400, 10009], lr: 0.100000, loss: 2.7147
2022-03-05 20:29:34 - train: epoch 0026, iter [04500, 10009], lr: 0.100000, loss: 2.5297
2022-03-05 20:30:07 - train: epoch 0026, iter [04600, 10009], lr: 0.100000, loss: 2.8965
2022-03-05 20:30:40 - train: epoch 0026, iter [04700, 10009], lr: 0.100000, loss: 2.6563
2022-03-05 20:31:13 - train: epoch 0026, iter [04800, 10009], lr: 0.100000, loss: 2.9162
2022-03-05 20:31:46 - train: epoch 0026, iter [04900, 10009], lr: 0.100000, loss: 2.5281
2022-03-05 20:32:19 - train: epoch 0026, iter [05000, 10009], lr: 0.100000, loss: 2.6140
2022-03-05 20:32:51 - train: epoch 0026, iter [05100, 10009], lr: 0.100000, loss: 2.9102
2022-03-05 20:33:24 - train: epoch 0026, iter [05200, 10009], lr: 0.100000, loss: 2.7804
2022-03-05 20:33:57 - train: epoch 0026, iter [05300, 10009], lr: 0.100000, loss: 2.6277
2022-03-05 20:34:30 - train: epoch 0026, iter [05400, 10009], lr: 0.100000, loss: 3.0349
2022-03-05 20:35:03 - train: epoch 0026, iter [05500, 10009], lr: 0.100000, loss: 2.2886
2022-03-05 20:35:36 - train: epoch 0026, iter [05600, 10009], lr: 0.100000, loss: 2.4592
2022-03-05 20:36:09 - train: epoch 0026, iter [05700, 10009], lr: 0.100000, loss: 2.8165
2022-03-05 20:36:41 - train: epoch 0026, iter [05800, 10009], lr: 0.100000, loss: 2.8440
2022-03-05 20:37:14 - train: epoch 0026, iter [05900, 10009], lr: 0.100000, loss: 3.1623
2022-03-05 20:37:47 - train: epoch 0026, iter [06000, 10009], lr: 0.100000, loss: 2.8025
2022-03-05 20:38:20 - train: epoch 0026, iter [06100, 10009], lr: 0.100000, loss: 2.7851
2022-03-05 20:38:53 - train: epoch 0026, iter [06200, 10009], lr: 0.100000, loss: 2.2063
2022-03-05 20:39:26 - train: epoch 0026, iter [06300, 10009], lr: 0.100000, loss: 2.7493
2022-03-05 20:39:59 - train: epoch 0026, iter [06400, 10009], lr: 0.100000, loss: 2.9265
2022-03-05 20:40:32 - train: epoch 0026, iter [06500, 10009], lr: 0.100000, loss: 2.8908
2022-03-05 20:41:04 - train: epoch 0026, iter [06600, 10009], lr: 0.100000, loss: 2.6545
2022-03-05 20:41:37 - train: epoch 0026, iter [06700, 10009], lr: 0.100000, loss: 3.0243
2022-03-05 20:42:10 - train: epoch 0026, iter [06800, 10009], lr: 0.100000, loss: 2.7894
2022-03-05 20:42:43 - train: epoch 0026, iter [06900, 10009], lr: 0.100000, loss: 2.6319
2022-03-05 20:43:16 - train: epoch 0026, iter [07000, 10009], lr: 0.100000, loss: 2.7726
2022-03-05 20:43:48 - train: epoch 0026, iter [07100, 10009], lr: 0.100000, loss: 2.8603
2022-03-05 20:44:21 - train: epoch 0026, iter [07200, 10009], lr: 0.100000, loss: 2.5710
2022-03-05 20:44:54 - train: epoch 0026, iter [07300, 10009], lr: 0.100000, loss: 2.8851
2022-03-05 20:45:27 - train: epoch 0026, iter [07400, 10009], lr: 0.100000, loss: 3.0269
2022-03-05 20:45:59 - train: epoch 0026, iter [07500, 10009], lr: 0.100000, loss: 2.8894
2022-03-05 20:46:32 - train: epoch 0026, iter [07600, 10009], lr: 0.100000, loss: 2.6264
2022-03-05 20:47:05 - train: epoch 0026, iter [07700, 10009], lr: 0.100000, loss: 2.8713
2022-03-05 20:47:38 - train: epoch 0026, iter [07800, 10009], lr: 0.100000, loss: 3.0475
2022-03-05 20:48:10 - train: epoch 0026, iter [07900, 10009], lr: 0.100000, loss: 3.0207
2022-03-05 20:48:43 - train: epoch 0026, iter [08000, 10009], lr: 0.100000, loss: 3.2012
2022-03-05 20:49:16 - train: epoch 0026, iter [08100, 10009], lr: 0.100000, loss: 2.4623
2022-03-05 20:49:49 - train: epoch 0026, iter [08200, 10009], lr: 0.100000, loss: 2.7044
2022-03-05 20:50:21 - train: epoch 0026, iter [08300, 10009], lr: 0.100000, loss: 2.8368
2022-03-05 20:50:54 - train: epoch 0026, iter [08400, 10009], lr: 0.100000, loss: 2.7930
2022-03-05 20:51:27 - train: epoch 0026, iter [08500, 10009], lr: 0.100000, loss: 2.4050
2022-03-05 20:52:00 - train: epoch 0026, iter [08600, 10009], lr: 0.100000, loss: 2.9424
2022-03-05 20:52:32 - train: epoch 0026, iter [08700, 10009], lr: 0.100000, loss: 2.6101
2022-03-05 20:53:05 - train: epoch 0026, iter [08800, 10009], lr: 0.100000, loss: 2.7396
2022-03-05 20:53:38 - train: epoch 0026, iter [08900, 10009], lr: 0.100000, loss: 2.3421
2022-03-05 20:54:11 - train: epoch 0026, iter [09000, 10009], lr: 0.100000, loss: 2.7512
2022-03-05 20:54:43 - train: epoch 0026, iter [09100, 10009], lr: 0.100000, loss: 2.5064
2022-03-05 20:55:16 - train: epoch 0026, iter [09200, 10009], lr: 0.100000, loss: 2.6553
2022-03-05 20:55:49 - train: epoch 0026, iter [09300, 10009], lr: 0.100000, loss: 2.4223
2022-03-05 20:56:22 - train: epoch 0026, iter [09400, 10009], lr: 0.100000, loss: 2.5448
2022-03-05 20:56:54 - train: epoch 0026, iter [09500, 10009], lr: 0.100000, loss: 2.5077
2022-03-05 20:57:27 - train: epoch 0026, iter [09600, 10009], lr: 0.100000, loss: 2.6003
2022-03-05 20:58:00 - train: epoch 0026, iter [09700, 10009], lr: 0.100000, loss: 2.5032
2022-03-05 20:58:32 - train: epoch 0026, iter [09800, 10009], lr: 0.100000, loss: 2.8942
2022-03-05 20:59:05 - train: epoch 0026, iter [09900, 10009], lr: 0.100000, loss: 2.5844
2022-03-05 20:59:38 - train: epoch 0026, iter [10000, 10009], lr: 0.100000, loss: 2.5724
2022-03-05 20:59:41 - train: epoch 026, train_loss: 2.6628
2022-03-05 21:00:56 - eval: epoch: 026, acc1: 47.630%, acc5: 73.892%, test_loss: 2.2936, per_image_load_time: 1.066ms, per_image_inference_time: 1.526ms
2022-03-05 21:00:57 - until epoch: 026, best_acc1: 47.862%
2022-03-05 21:00:57 - epoch 027 lr: 0.1
2022-03-05 21:01:33 - train: epoch 0027, iter [00100, 10009], lr: 0.100000, loss: 2.6634
2022-03-05 21:02:06 - train: epoch 0027, iter [00200, 10009], lr: 0.100000, loss: 2.6906
2022-03-05 21:02:38 - train: epoch 0027, iter [00300, 10009], lr: 0.100000, loss: 2.5620
2022-03-05 21:03:11 - train: epoch 0027, iter [00400, 10009], lr: 0.100000, loss: 2.7585
2022-03-05 21:03:44 - train: epoch 0027, iter [00500, 10009], lr: 0.100000, loss: 2.7397
2022-03-05 21:04:16 - train: epoch 0027, iter [00600, 10009], lr: 0.100000, loss: 2.6215
2022-03-05 21:04:49 - train: epoch 0027, iter [00700, 10009], lr: 0.100000, loss: 2.7077
2022-03-05 21:05:22 - train: epoch 0027, iter [00800, 10009], lr: 0.100000, loss: 2.2693
2022-03-05 21:05:55 - train: epoch 0027, iter [00900, 10009], lr: 0.100000, loss: 2.5002
2022-03-05 21:06:28 - train: epoch 0027, iter [01000, 10009], lr: 0.100000, loss: 2.5113
2022-03-05 21:07:00 - train: epoch 0027, iter [01100, 10009], lr: 0.100000, loss: 2.3521
2022-03-05 21:07:33 - train: epoch 0027, iter [01200, 10009], lr: 0.100000, loss: 2.8097
2022-03-05 21:08:06 - train: epoch 0027, iter [01300, 10009], lr: 0.100000, loss: 2.5093
2022-03-05 21:08:39 - train: epoch 0027, iter [01400, 10009], lr: 0.100000, loss: 2.5031
2022-03-05 21:09:12 - train: epoch 0027, iter [01500, 10009], lr: 0.100000, loss: 2.5398
2022-03-05 21:09:44 - train: epoch 0027, iter [01600, 10009], lr: 0.100000, loss: 2.7573
2022-03-05 21:10:17 - train: epoch 0027, iter [01700, 10009], lr: 0.100000, loss: 2.7987
2022-03-05 21:10:50 - train: epoch 0027, iter [01800, 10009], lr: 0.100000, loss: 2.7414
2022-03-05 21:11:23 - train: epoch 0027, iter [01900, 10009], lr: 0.100000, loss: 2.7942
2022-03-05 21:11:55 - train: epoch 0027, iter [02000, 10009], lr: 0.100000, loss: 2.7350
2022-03-05 21:12:28 - train: epoch 0027, iter [02100, 10009], lr: 0.100000, loss: 2.5550
2022-03-05 21:13:01 - train: epoch 0027, iter [02200, 10009], lr: 0.100000, loss: 2.5534
2022-03-05 21:13:34 - train: epoch 0027, iter [02300, 10009], lr: 0.100000, loss: 2.6746
2022-03-05 21:14:06 - train: epoch 0027, iter [02400, 10009], lr: 0.100000, loss: 2.5751
2022-03-05 21:14:39 - train: epoch 0027, iter [02500, 10009], lr: 0.100000, loss: 2.8064
2022-03-05 21:15:12 - train: epoch 0027, iter [02600, 10009], lr: 0.100000, loss: 2.6794
2022-03-05 21:15:45 - train: epoch 0027, iter [02700, 10009], lr: 0.100000, loss: 2.8793
2022-03-05 21:16:18 - train: epoch 0027, iter [02800, 10009], lr: 0.100000, loss: 2.6302
2022-03-05 21:16:50 - train: epoch 0027, iter [02900, 10009], lr: 0.100000, loss: 2.5218
2022-03-05 21:17:23 - train: epoch 0027, iter [03000, 10009], lr: 0.100000, loss: 2.6021
2022-03-05 21:17:56 - train: epoch 0027, iter [03100, 10009], lr: 0.100000, loss: 2.5727
2022-03-05 21:18:29 - train: epoch 0027, iter [03200, 10009], lr: 0.100000, loss: 2.6922
2022-03-05 21:19:01 - train: epoch 0027, iter [03300, 10009], lr: 0.100000, loss: 2.6644
2022-03-05 21:19:34 - train: epoch 0027, iter [03400, 10009], lr: 0.100000, loss: 2.4686
2022-03-05 21:20:07 - train: epoch 0027, iter [03500, 10009], lr: 0.100000, loss: 2.2621
2022-03-05 21:20:40 - train: epoch 0027, iter [03600, 10009], lr: 0.100000, loss: 2.6847
2022-03-05 21:21:12 - train: epoch 0027, iter [03700, 10009], lr: 0.100000, loss: 2.6371
2022-03-05 21:21:45 - train: epoch 0027, iter [03800, 10009], lr: 0.100000, loss: 2.5140
2022-03-05 21:22:18 - train: epoch 0027, iter [03900, 10009], lr: 0.100000, loss: 2.5494
2022-03-05 21:22:51 - train: epoch 0027, iter [04000, 10009], lr: 0.100000, loss: 2.3194
2022-03-05 21:23:23 - train: epoch 0027, iter [04100, 10009], lr: 0.100000, loss: 2.7192
2022-03-05 21:23:56 - train: epoch 0027, iter [04200, 10009], lr: 0.100000, loss: 2.3222
2022-03-05 21:24:29 - train: epoch 0027, iter [04300, 10009], lr: 0.100000, loss: 2.7578
2022-03-05 21:25:02 - train: epoch 0027, iter [04400, 10009], lr: 0.100000, loss: 2.9100
2022-03-05 21:25:34 - train: epoch 0027, iter [04500, 10009], lr: 0.100000, loss: 2.5076
2022-03-05 21:26:07 - train: epoch 0027, iter [04600, 10009], lr: 0.100000, loss: 2.8370
2022-03-05 21:26:40 - train: epoch 0027, iter [04700, 10009], lr: 0.100000, loss: 2.5999
2022-03-05 21:27:13 - train: epoch 0027, iter [04800, 10009], lr: 0.100000, loss: 2.4862
2022-03-05 21:27:45 - train: epoch 0027, iter [04900, 10009], lr: 0.100000, loss: 2.5803
2022-03-05 21:28:18 - train: epoch 0027, iter [05000, 10009], lr: 0.100000, loss: 2.4949
2022-03-05 21:28:51 - train: epoch 0027, iter [05100, 10009], lr: 0.100000, loss: 2.4878
2022-03-05 21:29:24 - train: epoch 0027, iter [05200, 10009], lr: 0.100000, loss: 2.6335
2022-03-05 21:29:56 - train: epoch 0027, iter [05300, 10009], lr: 0.100000, loss: 2.7427
2022-03-05 21:30:29 - train: epoch 0027, iter [05400, 10009], lr: 0.100000, loss: 2.6630
2022-03-05 21:31:02 - train: epoch 0027, iter [05500, 10009], lr: 0.100000, loss: 2.3417
2022-03-05 21:31:35 - train: epoch 0027, iter [05600, 10009], lr: 0.100000, loss: 2.5032
2022-03-05 21:32:08 - train: epoch 0027, iter [05700, 10009], lr: 0.100000, loss: 2.1953
2022-03-05 21:32:40 - train: epoch 0027, iter [05800, 10009], lr: 0.100000, loss: 2.7927
2022-03-05 21:33:13 - train: epoch 0027, iter [05900, 10009], lr: 0.100000, loss: 2.5505
2022-03-05 21:33:46 - train: epoch 0027, iter [06000, 10009], lr: 0.100000, loss: 2.5711
2022-03-05 21:34:19 - train: epoch 0027, iter [06100, 10009], lr: 0.100000, loss: 2.8812
2022-03-05 21:34:51 - train: epoch 0027, iter [06200, 10009], lr: 0.100000, loss: 2.4258
2022-03-05 21:35:24 - train: epoch 0027, iter [06300, 10009], lr: 0.100000, loss: 2.6947
2022-03-05 21:35:57 - train: epoch 0027, iter [06400, 10009], lr: 0.100000, loss: 2.7755
2022-03-05 21:36:30 - train: epoch 0027, iter [06500, 10009], lr: 0.100000, loss: 2.4756
2022-03-05 21:37:03 - train: epoch 0027, iter [06600, 10009], lr: 0.100000, loss: 2.7586
2022-03-05 21:37:35 - train: epoch 0027, iter [06700, 10009], lr: 0.100000, loss: 2.5821
2022-03-05 21:38:08 - train: epoch 0027, iter [06800, 10009], lr: 0.100000, loss: 2.9280
2022-03-05 21:38:41 - train: epoch 0027, iter [06900, 10009], lr: 0.100000, loss: 2.7620
2022-03-05 21:39:14 - train: epoch 0027, iter [07000, 10009], lr: 0.100000, loss: 2.8545
2022-03-05 21:39:46 - train: epoch 0027, iter [07100, 10009], lr: 0.100000, loss: 2.8594
2022-03-05 21:40:19 - train: epoch 0027, iter [07200, 10009], lr: 0.100000, loss: 2.8756
2022-03-05 21:40:52 - train: epoch 0027, iter [07300, 10009], lr: 0.100000, loss: 2.6118
2022-03-05 21:41:25 - train: epoch 0027, iter [07400, 10009], lr: 0.100000, loss: 2.4020
2022-03-05 21:41:58 - train: epoch 0027, iter [07500, 10009], lr: 0.100000, loss: 2.4680
2022-03-05 21:42:30 - train: epoch 0027, iter [07600, 10009], lr: 0.100000, loss: 2.5881
2022-03-05 21:43:03 - train: epoch 0027, iter [07700, 10009], lr: 0.100000, loss: 2.8306
2022-03-05 21:43:36 - train: epoch 0027, iter [07800, 10009], lr: 0.100000, loss: 2.4707
2022-03-05 21:44:09 - train: epoch 0027, iter [07900, 10009], lr: 0.100000, loss: 2.7133
2022-03-05 21:44:42 - train: epoch 0027, iter [08000, 10009], lr: 0.100000, loss: 2.5068
2022-03-05 21:45:14 - train: epoch 0027, iter [08100, 10009], lr: 0.100000, loss: 2.6934
2022-03-05 21:45:47 - train: epoch 0027, iter [08200, 10009], lr: 0.100000, loss: 2.5118
2022-03-05 21:46:20 - train: epoch 0027, iter [08300, 10009], lr: 0.100000, loss: 3.0300
2022-03-05 21:46:53 - train: epoch 0027, iter [08400, 10009], lr: 0.100000, loss: 2.7856
2022-03-05 21:47:26 - train: epoch 0027, iter [08500, 10009], lr: 0.100000, loss: 2.2190
2022-03-05 21:47:58 - train: epoch 0027, iter [08600, 10009], lr: 0.100000, loss: 2.8753
2022-03-05 21:48:31 - train: epoch 0027, iter [08700, 10009], lr: 0.100000, loss: 2.7774
2022-03-05 21:49:04 - train: epoch 0027, iter [08800, 10009], lr: 0.100000, loss: 2.4213
2022-03-05 21:49:36 - train: epoch 0027, iter [08900, 10009], lr: 0.100000, loss: 2.9979
2022-03-05 21:50:09 - train: epoch 0027, iter [09000, 10009], lr: 0.100000, loss: 2.2178
2022-03-05 21:50:42 - train: epoch 0027, iter [09100, 10009], lr: 0.100000, loss: 3.0882
2022-03-05 21:51:15 - train: epoch 0027, iter [09200, 10009], lr: 0.100000, loss: 2.6638
2022-03-05 21:51:48 - train: epoch 0027, iter [09300, 10009], lr: 0.100000, loss: 2.5705
2022-03-05 21:52:20 - train: epoch 0027, iter [09400, 10009], lr: 0.100000, loss: 2.9186
2022-03-05 21:52:53 - train: epoch 0027, iter [09500, 10009], lr: 0.100000, loss: 2.4823
2022-03-05 21:53:26 - train: epoch 0027, iter [09600, 10009], lr: 0.100000, loss: 2.8410
2022-03-05 21:53:59 - train: epoch 0027, iter [09700, 10009], lr: 0.100000, loss: 2.5163
2022-03-05 21:54:31 - train: epoch 0027, iter [09800, 10009], lr: 0.100000, loss: 2.5434
2022-03-05 21:55:04 - train: epoch 0027, iter [09900, 10009], lr: 0.100000, loss: 2.4583
2022-03-05 21:55:37 - train: epoch 0027, iter [10000, 10009], lr: 0.100000, loss: 2.3051
2022-03-05 21:55:40 - train: epoch 027, train_loss: 2.6540
2022-03-05 21:56:55 - eval: epoch: 027, acc1: 48.518%, acc5: 74.116%, test_loss: 2.2712, per_image_load_time: 1.267ms, per_image_inference_time: 1.539ms
2022-03-05 21:56:56 - until epoch: 027, best_acc1: 48.518%
2022-03-05 21:56:56 - epoch 028 lr: 0.1
2022-03-05 21:57:32 - train: epoch 0028, iter [00100, 10009], lr: 0.100000, loss: 2.4946
2022-03-05 21:58:05 - train: epoch 0028, iter [00200, 10009], lr: 0.100000, loss: 2.2637
2022-03-05 21:58:38 - train: epoch 0028, iter [00300, 10009], lr: 0.100000, loss: 2.7350
2022-03-05 21:59:10 - train: epoch 0028, iter [00400, 10009], lr: 0.100000, loss: 2.3891
2022-03-05 21:59:43 - train: epoch 0028, iter [00500, 10009], lr: 0.100000, loss: 2.4883
2022-03-05 22:00:16 - train: epoch 0028, iter [00600, 10009], lr: 0.100000, loss: 2.8576
2022-03-05 22:00:48 - train: epoch 0028, iter [00700, 10009], lr: 0.100000, loss: 2.9689
2022-03-05 22:01:21 - train: epoch 0028, iter [00800, 10009], lr: 0.100000, loss: 2.4319
2022-03-05 22:01:53 - train: epoch 0028, iter [00900, 10009], lr: 0.100000, loss: 2.5197
2022-03-05 22:02:26 - train: epoch 0028, iter [01000, 10009], lr: 0.100000, loss: 2.5675
2022-03-05 22:02:59 - train: epoch 0028, iter [01100, 10009], lr: 0.100000, loss: 2.6565
2022-03-05 22:03:31 - train: epoch 0028, iter [01200, 10009], lr: 0.100000, loss: 2.9287
2022-03-05 22:04:04 - train: epoch 0028, iter [01300, 10009], lr: 0.100000, loss: 2.8368
2022-03-05 22:04:37 - train: epoch 0028, iter [01400, 10009], lr: 0.100000, loss: 2.9935
2022-03-05 22:05:10 - train: epoch 0028, iter [01500, 10009], lr: 0.100000, loss: 2.6736
2022-03-05 22:05:42 - train: epoch 0028, iter [01600, 10009], lr: 0.100000, loss: 2.1847
2022-03-05 22:06:15 - train: epoch 0028, iter [01700, 10009], lr: 0.100000, loss: 2.5646
2022-03-05 22:06:48 - train: epoch 0028, iter [01800, 10009], lr: 0.100000, loss: 2.8316
2022-03-05 22:07:20 - train: epoch 0028, iter [01900, 10009], lr: 0.100000, loss: 2.6752
2022-03-05 22:07:53 - train: epoch 0028, iter [02000, 10009], lr: 0.100000, loss: 2.9803
2022-03-05 22:08:26 - train: epoch 0028, iter [02100, 10009], lr: 0.100000, loss: 2.8488
2022-03-05 22:08:59 - train: epoch 0028, iter [02200, 10009], lr: 0.100000, loss: 2.5945
2022-03-05 22:09:31 - train: epoch 0028, iter [02300, 10009], lr: 0.100000, loss: 2.6507
2022-03-05 22:10:04 - train: epoch 0028, iter [02400, 10009], lr: 0.100000, loss: 2.5942
2022-03-05 22:10:37 - train: epoch 0028, iter [02500, 10009], lr: 0.100000, loss: 2.8067
2022-03-05 22:11:09 - train: epoch 0028, iter [02600, 10009], lr: 0.100000, loss: 2.7734
2022-03-05 22:11:42 - train: epoch 0028, iter [02700, 10009], lr: 0.100000, loss: 2.5776
2022-03-05 22:12:15 - train: epoch 0028, iter [02800, 10009], lr: 0.100000, loss: 2.8239
2022-03-05 22:12:47 - train: epoch 0028, iter [02900, 10009], lr: 0.100000, loss: 2.7311
2022-03-05 22:13:20 - train: epoch 0028, iter [03000, 10009], lr: 0.100000, loss: 2.6688
2022-03-05 22:13:53 - train: epoch 0028, iter [03100, 10009], lr: 0.100000, loss: 2.5598
2022-03-05 22:14:26 - train: epoch 0028, iter [03200, 10009], lr: 0.100000, loss: 2.7852
2022-03-05 22:14:58 - train: epoch 0028, iter [03300, 10009], lr: 0.100000, loss: 2.7270
2022-03-05 22:15:31 - train: epoch 0028, iter [03400, 10009], lr: 0.100000, loss: 2.6335
2022-03-05 22:16:04 - train: epoch 0028, iter [03500, 10009], lr: 0.100000, loss: 2.5842
2022-03-05 22:16:36 - train: epoch 0028, iter [03600, 10009], lr: 0.100000, loss: 2.8971
2022-03-05 22:17:09 - train: epoch 0028, iter [03700, 10009], lr: 0.100000, loss: 2.7341
2022-03-05 22:17:42 - train: epoch 0028, iter [03800, 10009], lr: 0.100000, loss: 2.4179
2022-03-05 22:18:15 - train: epoch 0028, iter [03900, 10009], lr: 0.100000, loss: 2.6054
2022-03-05 22:18:47 - train: epoch 0028, iter [04000, 10009], lr: 0.100000, loss: 2.8704
2022-03-05 22:19:20 - train: epoch 0028, iter [04100, 10009], lr: 0.100000, loss: 2.5830
2022-03-05 22:19:53 - train: epoch 0028, iter [04200, 10009], lr: 0.100000, loss: 2.5968
2022-03-05 22:20:26 - train: epoch 0028, iter [04300, 10009], lr: 0.100000, loss: 2.5495
2022-03-05 22:20:58 - train: epoch 0028, iter [04400, 10009], lr: 0.100000, loss: 2.7874
2022-03-05 22:21:31 - train: epoch 0028, iter [04500, 10009], lr: 0.100000, loss: 2.5863
2022-03-05 22:22:04 - train: epoch 0028, iter [04600, 10009], lr: 0.100000, loss: 2.8591
2022-03-05 22:22:37 - train: epoch 0028, iter [04700, 10009], lr: 0.100000, loss: 2.4646
2022-03-05 22:23:09 - train: epoch 0028, iter [04800, 10009], lr: 0.100000, loss: 2.9719
2022-03-05 22:23:42 - train: epoch 0028, iter [04900, 10009], lr: 0.100000, loss: 2.2654
2022-03-05 22:24:15 - train: epoch 0028, iter [05000, 10009], lr: 0.100000, loss: 2.5229
2022-03-05 22:24:48 - train: epoch 0028, iter [05100, 10009], lr: 0.100000, loss: 2.5969
2022-03-05 22:25:20 - train: epoch 0028, iter [05200, 10009], lr: 0.100000, loss: 2.5864
2022-03-05 22:25:53 - train: epoch 0028, iter [05300, 10009], lr: 0.100000, loss: 2.5502
2022-03-05 22:26:26 - train: epoch 0028, iter [05400, 10009], lr: 0.100000, loss: 2.6392
2022-03-05 22:26:58 - train: epoch 0028, iter [05500, 10009], lr: 0.100000, loss: 2.7796
2022-03-05 22:27:31 - train: epoch 0028, iter [05600, 10009], lr: 0.100000, loss: 2.6315
2022-03-05 22:28:04 - train: epoch 0028, iter [05700, 10009], lr: 0.100000, loss: 2.4963
2022-03-05 22:28:37 - train: epoch 0028, iter [05800, 10009], lr: 0.100000, loss: 2.6231
2022-03-05 22:29:09 - train: epoch 0028, iter [05900, 10009], lr: 0.100000, loss: 2.9645
2022-03-05 22:29:42 - train: epoch 0028, iter [06000, 10009], lr: 0.100000, loss: 2.7545
2022-03-05 22:30:14 - train: epoch 0028, iter [06100, 10009], lr: 0.100000, loss: 2.8170
2022-03-05 22:30:47 - train: epoch 0028, iter [06200, 10009], lr: 0.100000, loss: 2.7799
2022-03-05 22:31:20 - train: epoch 0028, iter [06300, 10009], lr: 0.100000, loss: 2.5137
2022-03-05 22:31:53 - train: epoch 0028, iter [06400, 10009], lr: 0.100000, loss: 2.4828
2022-03-05 22:32:26 - train: epoch 0028, iter [06500, 10009], lr: 0.100000, loss: 2.4805
2022-03-05 22:32:59 - train: epoch 0028, iter [06600, 10009], lr: 0.100000, loss: 2.6326
2022-03-05 22:33:31 - train: epoch 0028, iter [06700, 10009], lr: 0.100000, loss: 2.9348
2022-03-05 22:34:04 - train: epoch 0028, iter [06800, 10009], lr: 0.100000, loss: 3.0577
2022-03-05 22:34:37 - train: epoch 0028, iter [06900, 10009], lr: 0.100000, loss: 2.7154
2022-03-05 22:35:10 - train: epoch 0028, iter [07000, 10009], lr: 0.100000, loss: 2.7128
2022-03-05 22:35:43 - train: epoch 0028, iter [07100, 10009], lr: 0.100000, loss: 2.4779
2022-03-05 22:36:16 - train: epoch 0028, iter [07200, 10009], lr: 0.100000, loss: 2.5584
2022-03-05 22:36:49 - train: epoch 0028, iter [07300, 10009], lr: 0.100000, loss: 2.6234
2022-03-05 22:37:21 - train: epoch 0028, iter [07400, 10009], lr: 0.100000, loss: 2.8675
2022-03-05 22:37:54 - train: epoch 0028, iter [07500, 10009], lr: 0.100000, loss: 2.7468
2022-03-05 22:38:27 - train: epoch 0028, iter [07600, 10009], lr: 0.100000, loss: 2.2504
2022-03-05 22:39:00 - train: epoch 0028, iter [07700, 10009], lr: 0.100000, loss: 3.0469
2022-03-05 22:39:32 - train: epoch 0028, iter [07800, 10009], lr: 0.100000, loss: 2.8087
2022-03-05 22:40:05 - train: epoch 0028, iter [07900, 10009], lr: 0.100000, loss: 3.0190
2022-03-05 22:40:37 - train: epoch 0028, iter [08000, 10009], lr: 0.100000, loss: 2.6174
2022-03-05 22:41:10 - train: epoch 0028, iter [08100, 10009], lr: 0.100000, loss: 2.6358
2022-03-05 22:41:43 - train: epoch 0028, iter [08200, 10009], lr: 0.100000, loss: 2.3629
2022-03-05 22:42:16 - train: epoch 0028, iter [08300, 10009], lr: 0.100000, loss: 2.5281
2022-03-05 22:42:48 - train: epoch 0028, iter [08400, 10009], lr: 0.100000, loss: 2.6610
2022-03-05 22:43:21 - train: epoch 0028, iter [08500, 10009], lr: 0.100000, loss: 3.2069
2022-03-05 22:43:54 - train: epoch 0028, iter [08600, 10009], lr: 0.100000, loss: 2.4281
2022-03-05 22:44:26 - train: epoch 0028, iter [08700, 10009], lr: 0.100000, loss: 3.1965
2022-03-05 22:44:59 - train: epoch 0028, iter [08800, 10009], lr: 0.100000, loss: 2.4457
2022-03-05 22:45:32 - train: epoch 0028, iter [08900, 10009], lr: 0.100000, loss: 2.6970
2022-03-05 22:46:05 - train: epoch 0028, iter [09000, 10009], lr: 0.100000, loss: 2.7999
2022-03-05 22:46:37 - train: epoch 0028, iter [09100, 10009], lr: 0.100000, loss: 2.7120
2022-03-05 22:47:10 - train: epoch 0028, iter [09200, 10009], lr: 0.100000, loss: 2.6559
2022-03-05 22:47:43 - train: epoch 0028, iter [09300, 10009], lr: 0.100000, loss: 2.4393
2022-03-05 22:48:15 - train: epoch 0028, iter [09400, 10009], lr: 0.100000, loss: 2.6044
2022-03-05 22:48:48 - train: epoch 0028, iter [09500, 10009], lr: 0.100000, loss: 2.6541
2022-03-05 22:49:21 - train: epoch 0028, iter [09600, 10009], lr: 0.100000, loss: 2.6868
2022-03-05 22:49:54 - train: epoch 0028, iter [09700, 10009], lr: 0.100000, loss: 2.6475
2022-03-05 22:50:26 - train: epoch 0028, iter [09800, 10009], lr: 0.100000, loss: 2.5029
2022-03-05 22:50:59 - train: epoch 0028, iter [09900, 10009], lr: 0.100000, loss: 2.6916
2022-03-05 22:51:32 - train: epoch 0028, iter [10000, 10009], lr: 0.100000, loss: 2.2438
2022-03-05 22:51:35 - train: epoch 028, train_loss: 2.6534
2022-03-05 22:52:51 - eval: epoch: 028, acc1: 48.512%, acc5: 74.634%, test_loss: 2.2467, per_image_load_time: 1.055ms, per_image_inference_time: 1.566ms
2022-03-05 22:52:51 - until epoch: 028, best_acc1: 48.518%
2022-03-05 22:52:51 - epoch 029 lr: 0.1
2022-03-05 22:53:27 - train: epoch 0029, iter [00100, 10009], lr: 0.100000, loss: 2.4984
2022-03-05 22:54:00 - train: epoch 0029, iter [00200, 10009], lr: 0.100000, loss: 2.7004
2022-03-05 22:54:33 - train: epoch 0029, iter [00300, 10009], lr: 0.100000, loss: 2.6310
2022-03-05 22:55:05 - train: epoch 0029, iter [00400, 10009], lr: 0.100000, loss: 2.4999
2022-03-05 22:55:38 - train: epoch 0029, iter [00500, 10009], lr: 0.100000, loss: 2.6299
2022-03-05 22:56:11 - train: epoch 0029, iter [00600, 10009], lr: 0.100000, loss: 3.1360
2022-03-05 22:56:44 - train: epoch 0029, iter [00700, 10009], lr: 0.100000, loss: 2.7413
2022-03-05 22:57:17 - train: epoch 0029, iter [00800, 10009], lr: 0.100000, loss: 2.5899
2022-03-05 22:57:49 - train: epoch 0029, iter [00900, 10009], lr: 0.100000, loss: 2.6997
2022-03-05 22:58:22 - train: epoch 0029, iter [01000, 10009], lr: 0.100000, loss: 2.4595
2022-03-05 22:58:55 - train: epoch 0029, iter [01100, 10009], lr: 0.100000, loss: 2.6510
2022-03-05 22:59:28 - train: epoch 0029, iter [01200, 10009], lr: 0.100000, loss: 2.6612
2022-03-05 23:00:01 - train: epoch 0029, iter [01300, 10009], lr: 0.100000, loss: 2.5622
2022-03-05 23:00:34 - train: epoch 0029, iter [01400, 10009], lr: 0.100000, loss: 2.4192
2022-03-05 23:01:06 - train: epoch 0029, iter [01500, 10009], lr: 0.100000, loss: 2.9335
2022-03-05 23:01:39 - train: epoch 0029, iter [01600, 10009], lr: 0.100000, loss: 2.7443
2022-03-05 23:02:12 - train: epoch 0029, iter [01700, 10009], lr: 0.100000, loss: 2.5266
2022-03-05 23:02:45 - train: epoch 0029, iter [01800, 10009], lr: 0.100000, loss: 3.0265
2022-03-05 23:03:17 - train: epoch 0029, iter [01900, 10009], lr: 0.100000, loss: 2.9081
2022-03-05 23:03:50 - train: epoch 0029, iter [02000, 10009], lr: 0.100000, loss: 2.6869
2022-03-05 23:04:23 - train: epoch 0029, iter [02100, 10009], lr: 0.100000, loss: 3.1308
2022-03-05 23:04:56 - train: epoch 0029, iter [02200, 10009], lr: 0.100000, loss: 2.8162
2022-03-05 23:05:28 - train: epoch 0029, iter [02300, 10009], lr: 0.100000, loss: 2.6279
2022-03-05 23:06:01 - train: epoch 0029, iter [02400, 10009], lr: 0.100000, loss: 2.7345
2022-03-05 23:06:34 - train: epoch 0029, iter [02500, 10009], lr: 0.100000, loss: 2.8935
2022-03-05 23:07:07 - train: epoch 0029, iter [02600, 10009], lr: 0.100000, loss: 2.2941
2022-03-05 23:07:40 - train: epoch 0029, iter [02700, 10009], lr: 0.100000, loss: 2.6351
2022-03-05 23:08:12 - train: epoch 0029, iter [02800, 10009], lr: 0.100000, loss: 2.8419
2022-03-05 23:08:45 - train: epoch 0029, iter [02900, 10009], lr: 0.100000, loss: 2.2711
2022-03-05 23:09:18 - train: epoch 0029, iter [03000, 10009], lr: 0.100000, loss: 2.9539
2022-03-05 23:09:51 - train: epoch 0029, iter [03100, 10009], lr: 0.100000, loss: 2.5939
2022-03-05 23:10:23 - train: epoch 0029, iter [03200, 10009], lr: 0.100000, loss: 2.3420
2022-03-05 23:10:56 - train: epoch 0029, iter [03300, 10009], lr: 0.100000, loss: 2.7186
2022-03-05 23:11:29 - train: epoch 0029, iter [03400, 10009], lr: 0.100000, loss: 2.7478
2022-03-05 23:12:01 - train: epoch 0029, iter [03500, 10009], lr: 0.100000, loss: 2.6781
2022-03-05 23:12:34 - train: epoch 0029, iter [03600, 10009], lr: 0.100000, loss: 2.3690
2022-03-05 23:13:07 - train: epoch 0029, iter [03700, 10009], lr: 0.100000, loss: 2.4494
2022-03-05 23:13:39 - train: epoch 0029, iter [03800, 10009], lr: 0.100000, loss: 2.8079
2022-03-05 23:14:12 - train: epoch 0029, iter [03900, 10009], lr: 0.100000, loss: 2.8278
2022-03-05 23:14:45 - train: epoch 0029, iter [04000, 10009], lr: 0.100000, loss: 2.3566
2022-03-05 23:15:18 - train: epoch 0029, iter [04100, 10009], lr: 0.100000, loss: 2.4317
2022-03-05 23:15:51 - train: epoch 0029, iter [04200, 10009], lr: 0.100000, loss: 2.7268
2022-03-05 23:16:23 - train: epoch 0029, iter [04300, 10009], lr: 0.100000, loss: 2.6468
2022-03-05 23:16:56 - train: epoch 0029, iter [04400, 10009], lr: 0.100000, loss: 2.8264
2022-03-05 23:17:29 - train: epoch 0029, iter [04500, 10009], lr: 0.100000, loss: 2.8621
2022-03-05 23:18:01 - train: epoch 0029, iter [04600, 10009], lr: 0.100000, loss: 2.5401
2022-03-05 23:18:34 - train: epoch 0029, iter [04700, 10009], lr: 0.100000, loss: 2.3483
2022-03-05 23:19:07 - train: epoch 0029, iter [04800, 10009], lr: 0.100000, loss: 2.4683
2022-03-05 23:19:40 - train: epoch 0029, iter [04900, 10009], lr: 0.100000, loss: 2.6412
2022-03-05 23:20:13 - train: epoch 0029, iter [05000, 10009], lr: 0.100000, loss: 2.2711
2022-03-05 23:20:45 - train: epoch 0029, iter [05100, 10009], lr: 0.100000, loss: 2.3920
2022-03-05 23:21:18 - train: epoch 0029, iter [05200, 10009], lr: 0.100000, loss: 2.9545
2022-03-05 23:21:51 - train: epoch 0029, iter [05300, 10009], lr: 0.100000, loss: 2.7262
2022-03-05 23:22:24 - train: epoch 0029, iter [05400, 10009], lr: 0.100000, loss: 2.4867
2022-03-05 23:22:56 - train: epoch 0029, iter [05500, 10009], lr: 0.100000, loss: 2.6308
2022-03-05 23:23:29 - train: epoch 0029, iter [05600, 10009], lr: 0.100000, loss: 2.8699
2022-03-05 23:24:02 - train: epoch 0029, iter [05700, 10009], lr: 0.100000, loss: 2.7232
2022-03-05 23:24:35 - train: epoch 0029, iter [05800, 10009], lr: 0.100000, loss: 2.5001
2022-03-05 23:25:07 - train: epoch 0029, iter [05900, 10009], lr: 0.100000, loss: 2.6086
2022-03-05 23:25:40 - train: epoch 0029, iter [06000, 10009], lr: 0.100000, loss: 2.6184
2022-03-05 23:26:13 - train: epoch 0029, iter [06100, 10009], lr: 0.100000, loss: 3.2003
2022-03-05 23:26:46 - train: epoch 0029, iter [06200, 10009], lr: 0.100000, loss: 2.8745
2022-03-05 23:27:19 - train: epoch 0029, iter [06300, 10009], lr: 0.100000, loss: 2.5723
2022-03-05 23:27:52 - train: epoch 0029, iter [06400, 10009], lr: 0.100000, loss: 2.5918
2022-03-05 23:28:24 - train: epoch 0029, iter [06500, 10009], lr: 0.100000, loss: 2.4791
2022-03-05 23:28:57 - train: epoch 0029, iter [06600, 10009], lr: 0.100000, loss: 2.6329
2022-03-05 23:29:30 - train: epoch 0029, iter [06700, 10009], lr: 0.100000, loss: 2.5832
2022-03-05 23:30:03 - train: epoch 0029, iter [06800, 10009], lr: 0.100000, loss: 2.2360
2022-03-05 23:30:36 - train: epoch 0029, iter [06900, 10009], lr: 0.100000, loss: 2.9224
2022-03-05 23:31:09 - train: epoch 0029, iter [07000, 10009], lr: 0.100000, loss: 2.5585
2022-03-05 23:31:42 - train: epoch 0029, iter [07100, 10009], lr: 0.100000, loss: 2.2902
2022-03-05 23:32:14 - train: epoch 0029, iter [07200, 10009], lr: 0.100000, loss: 2.6213
2022-03-05 23:32:47 - train: epoch 0029, iter [07300, 10009], lr: 0.100000, loss: 2.5355
2022-03-05 23:33:20 - train: epoch 0029, iter [07400, 10009], lr: 0.100000, loss: 2.6749
2022-03-05 23:33:53 - train: epoch 0029, iter [07500, 10009], lr: 0.100000, loss: 2.5483
2022-03-05 23:34:26 - train: epoch 0029, iter [07600, 10009], lr: 0.100000, loss: 2.3608
2022-03-05 23:34:58 - train: epoch 0029, iter [07700, 10009], lr: 0.100000, loss: 2.7403
2022-03-05 23:35:31 - train: epoch 0029, iter [07800, 10009], lr: 0.100000, loss: 2.5365
2022-03-05 23:36:04 - train: epoch 0029, iter [07900, 10009], lr: 0.100000, loss: 2.9007
2022-03-05 23:36:37 - train: epoch 0029, iter [08000, 10009], lr: 0.100000, loss: 2.7492
2022-03-05 23:37:10 - train: epoch 0029, iter [08100, 10009], lr: 0.100000, loss: 2.8982
2022-03-05 23:37:43 - train: epoch 0029, iter [08200, 10009], lr: 0.100000, loss: 2.4347
2022-03-05 23:38:15 - train: epoch 0029, iter [08300, 10009], lr: 0.100000, loss: 2.5083
2022-03-05 23:38:48 - train: epoch 0029, iter [08400, 10009], lr: 0.100000, loss: 2.6125
2022-03-05 23:39:21 - train: epoch 0029, iter [08500, 10009], lr: 0.100000, loss: 2.5035
2022-03-05 23:39:54 - train: epoch 0029, iter [08600, 10009], lr: 0.100000, loss: 2.5952
2022-03-05 23:40:27 - train: epoch 0029, iter [08700, 10009], lr: 0.100000, loss: 2.5079
2022-03-05 23:41:00 - train: epoch 0029, iter [08800, 10009], lr: 0.100000, loss: 2.3690
2022-03-05 23:41:33 - train: epoch 0029, iter [08900, 10009], lr: 0.100000, loss: 2.7383
2022-03-05 23:42:06 - train: epoch 0029, iter [09000, 10009], lr: 0.100000, loss: 2.6759
2022-03-05 23:42:38 - train: epoch 0029, iter [09100, 10009], lr: 0.100000, loss: 2.4106
2022-03-05 23:43:11 - train: epoch 0029, iter [09200, 10009], lr: 0.100000, loss: 2.7430
2022-03-05 23:43:44 - train: epoch 0029, iter [09300, 10009], lr: 0.100000, loss: 2.5812
2022-03-05 23:44:17 - train: epoch 0029, iter [09400, 10009], lr: 0.100000, loss: 2.1271
2022-03-05 23:44:50 - train: epoch 0029, iter [09500, 10009], lr: 0.100000, loss: 2.5748
2022-03-05 23:45:23 - train: epoch 0029, iter [09600, 10009], lr: 0.100000, loss: 2.6514
2022-03-05 23:45:56 - train: epoch 0029, iter [09700, 10009], lr: 0.100000, loss: 2.5717
2022-03-05 23:46:29 - train: epoch 0029, iter [09800, 10009], lr: 0.100000, loss: 2.9187
2022-03-05 23:47:02 - train: epoch 0029, iter [09900, 10009], lr: 0.100000, loss: 2.2791
2022-03-05 23:47:34 - train: epoch 0029, iter [10000, 10009], lr: 0.100000, loss: 2.5290
2022-03-05 23:47:38 - train: epoch 029, train_loss: 2.6470
2022-03-05 23:48:53 - eval: epoch: 029, acc1: 48.318%, acc5: 74.176%, test_loss: 2.2639, per_image_load_time: 1.023ms, per_image_inference_time: 1.553ms
2022-03-05 23:48:54 - until epoch: 029, best_acc1: 48.518%
2022-03-05 23:48:54 - epoch 030 lr: 0.1
2022-03-05 23:49:30 - train: epoch 0030, iter [00100, 10009], lr: 0.100000, loss: 2.4833
2022-03-05 23:50:03 - train: epoch 0030, iter [00200, 10009], lr: 0.100000, loss: 2.6769
2022-03-05 23:50:35 - train: epoch 0030, iter [00300, 10009], lr: 0.100000, loss: 2.3533
2022-03-05 23:51:08 - train: epoch 0030, iter [00400, 10009], lr: 0.100000, loss: 2.5718
2022-03-05 23:51:41 - train: epoch 0030, iter [00500, 10009], lr: 0.100000, loss: 2.5949
2022-03-05 23:52:14 - train: epoch 0030, iter [00600, 10009], lr: 0.100000, loss: 2.5626
2022-03-05 23:52:46 - train: epoch 0030, iter [00700, 10009], lr: 0.100000, loss: 2.5861
2022-03-05 23:53:19 - train: epoch 0030, iter [00800, 10009], lr: 0.100000, loss: 2.6698
2022-03-05 23:53:52 - train: epoch 0030, iter [00900, 10009], lr: 0.100000, loss: 2.6457
2022-03-05 23:54:25 - train: epoch 0030, iter [01000, 10009], lr: 0.100000, loss: 2.7107
2022-03-05 23:54:57 - train: epoch 0030, iter [01100, 10009], lr: 0.100000, loss: 3.0604
2022-03-05 23:55:30 - train: epoch 0030, iter [01200, 10009], lr: 0.100000, loss: 2.4133
2022-03-05 23:56:03 - train: epoch 0030, iter [01300, 10009], lr: 0.100000, loss: 2.6405
2022-03-05 23:56:36 - train: epoch 0030, iter [01400, 10009], lr: 0.100000, loss: 2.6838
2022-03-05 23:57:08 - train: epoch 0030, iter [01500, 10009], lr: 0.100000, loss: 2.6333
2022-03-05 23:57:41 - train: epoch 0030, iter [01600, 10009], lr: 0.100000, loss: 2.7893
2022-03-05 23:58:14 - train: epoch 0030, iter [01700, 10009], lr: 0.100000, loss: 2.5483
2022-03-05 23:58:47 - train: epoch 0030, iter [01800, 10009], lr: 0.100000, loss: 3.1734
2022-03-05 23:59:20 - train: epoch 0030, iter [01900, 10009], lr: 0.100000, loss: 2.6982
2022-03-05 23:59:52 - train: epoch 0030, iter [02000, 10009], lr: 0.100000, loss: 2.6018
2022-03-06 00:00:25 - train: epoch 0030, iter [02100, 10009], lr: 0.100000, loss: 2.3008
2022-03-06 00:00:58 - train: epoch 0030, iter [02200, 10009], lr: 0.100000, loss: 2.5706
2022-03-06 00:01:31 - train: epoch 0030, iter [02300, 10009], lr: 0.100000, loss: 2.5954
2022-03-06 00:02:03 - train: epoch 0030, iter [02400, 10009], lr: 0.100000, loss: 2.4173
2022-03-06 00:02:36 - train: epoch 0030, iter [02500, 10009], lr: 0.100000, loss: 2.3412
2022-03-06 00:03:09 - train: epoch 0030, iter [02600, 10009], lr: 0.100000, loss: 2.5765
2022-03-06 00:03:42 - train: epoch 0030, iter [02700, 10009], lr: 0.100000, loss: 2.4987
2022-03-06 00:04:15 - train: epoch 0030, iter [02800, 10009], lr: 0.100000, loss: 2.7781
2022-03-06 00:04:47 - train: epoch 0030, iter [02900, 10009], lr: 0.100000, loss: 2.4519
2022-03-06 00:05:20 - train: epoch 0030, iter [03000, 10009], lr: 0.100000, loss: 2.5755
2022-03-06 00:05:53 - train: epoch 0030, iter [03100, 10009], lr: 0.100000, loss: 2.8749
2022-03-06 00:06:26 - train: epoch 0030, iter [03200, 10009], lr: 0.100000, loss: 2.7177
2022-03-06 00:06:58 - train: epoch 0030, iter [03300, 10009], lr: 0.100000, loss: 2.2867
2022-03-06 00:07:31 - train: epoch 0030, iter [03400, 10009], lr: 0.100000, loss: 2.6391
2022-03-06 00:08:04 - train: epoch 0030, iter [03500, 10009], lr: 0.100000, loss: 2.6597
2022-03-06 00:08:37 - train: epoch 0030, iter [03600, 10009], lr: 0.100000, loss: 2.5943
2022-03-06 00:09:10 - train: epoch 0030, iter [03700, 10009], lr: 0.100000, loss: 2.7939
2022-03-06 00:09:43 - train: epoch 0030, iter [03800, 10009], lr: 0.100000, loss: 2.6259
2022-03-06 00:10:15 - train: epoch 0030, iter [03900, 10009], lr: 0.100000, loss: 2.8541
2022-03-06 00:10:48 - train: epoch 0030, iter [04000, 10009], lr: 0.100000, loss: 2.8268
2022-03-06 00:11:21 - train: epoch 0030, iter [04100, 10009], lr: 0.100000, loss: 2.5463
2022-03-06 00:11:54 - train: epoch 0030, iter [04200, 10009], lr: 0.100000, loss: 2.9916
2022-03-06 00:12:27 - train: epoch 0030, iter [04300, 10009], lr: 0.100000, loss: 2.7962
2022-03-06 00:13:00 - train: epoch 0030, iter [04400, 10009], lr: 0.100000, loss: 2.5593
2022-03-06 00:13:32 - train: epoch 0030, iter [04500, 10009], lr: 0.100000, loss: 2.7078
2022-03-06 00:14:05 - train: epoch 0030, iter [04600, 10009], lr: 0.100000, loss: 2.4477
2022-03-06 00:14:38 - train: epoch 0030, iter [04700, 10009], lr: 0.100000, loss: 2.6345
2022-03-06 00:15:10 - train: epoch 0030, iter [04800, 10009], lr: 0.100000, loss: 2.4512
2022-03-06 00:15:43 - train: epoch 0030, iter [04900, 10009], lr: 0.100000, loss: 2.9930
2022-03-06 00:16:16 - train: epoch 0030, iter [05000, 10009], lr: 0.100000, loss: 3.2966
2022-03-06 00:16:49 - train: epoch 0030, iter [05100, 10009], lr: 0.100000, loss: 2.7937
2022-03-06 00:17:21 - train: epoch 0030, iter [05200, 10009], lr: 0.100000, loss: 2.3384
2022-03-06 00:17:54 - train: epoch 0030, iter [05300, 10009], lr: 0.100000, loss: 2.2791
2022-03-06 00:18:27 - train: epoch 0030, iter [05400, 10009], lr: 0.100000, loss: 2.6505
2022-03-06 00:19:00 - train: epoch 0030, iter [05500, 10009], lr: 0.100000, loss: 2.4894
2022-03-06 00:19:32 - train: epoch 0030, iter [05600, 10009], lr: 0.100000, loss: 2.7394
2022-03-06 00:20:05 - train: epoch 0030, iter [05700, 10009], lr: 0.100000, loss: 2.3506
2022-03-06 00:20:38 - train: epoch 0030, iter [05800, 10009], lr: 0.100000, loss: 2.7655
2022-03-06 00:21:10 - train: epoch 0030, iter [05900, 10009], lr: 0.100000, loss: 2.6251
2022-03-06 00:21:43 - train: epoch 0030, iter [06000, 10009], lr: 0.100000, loss: 2.8233
2022-03-06 00:22:16 - train: epoch 0030, iter [06100, 10009], lr: 0.100000, loss: 2.4822
2022-03-06 00:22:48 - train: epoch 0030, iter [06200, 10009], lr: 0.100000, loss: 2.8167
2022-03-06 00:23:21 - train: epoch 0030, iter [06300, 10009], lr: 0.100000, loss: 2.6377
2022-03-06 00:23:54 - train: epoch 0030, iter [06400, 10009], lr: 0.100000, loss: 2.3634
2022-03-06 00:24:27 - train: epoch 0030, iter [06500, 10009], lr: 0.100000, loss: 2.7943
2022-03-06 00:25:00 - train: epoch 0030, iter [06600, 10009], lr: 0.100000, loss: 2.8178
2022-03-06 00:25:32 - train: epoch 0030, iter [06700, 10009], lr: 0.100000, loss: 2.6187
2022-03-06 00:26:05 - train: epoch 0030, iter [06800, 10009], lr: 0.100000, loss: 2.3887
2022-03-06 00:26:38 - train: epoch 0030, iter [06900, 10009], lr: 0.100000, loss: 2.5537
2022-03-06 00:27:10 - train: epoch 0030, iter [07000, 10009], lr: 0.100000, loss: 2.7629
2022-03-06 00:27:43 - train: epoch 0030, iter [07100, 10009], lr: 0.100000, loss: 2.6973
2022-03-06 00:28:16 - train: epoch 0030, iter [07200, 10009], lr: 0.100000, loss: 2.5504
2022-03-06 00:28:48 - train: epoch 0030, iter [07300, 10009], lr: 0.100000, loss: 2.9416
2022-03-06 00:29:21 - train: epoch 0030, iter [07400, 10009], lr: 0.100000, loss: 2.7652
2022-03-06 00:29:54 - train: epoch 0030, iter [07500, 10009], lr: 0.100000, loss: 2.5664
2022-03-06 00:30:27 - train: epoch 0030, iter [07600, 10009], lr: 0.100000, loss: 2.4880
2022-03-06 00:30:59 - train: epoch 0030, iter [07700, 10009], lr: 0.100000, loss: 2.7294
2022-03-06 00:31:32 - train: epoch 0030, iter [07800, 10009], lr: 0.100000, loss: 2.4876
2022-03-06 00:32:05 - train: epoch 0030, iter [07900, 10009], lr: 0.100000, loss: 2.6961
2022-03-06 00:32:38 - train: epoch 0030, iter [08000, 10009], lr: 0.100000, loss: 2.4872
2022-03-06 00:33:11 - train: epoch 0030, iter [08100, 10009], lr: 0.100000, loss: 2.5625
2022-03-06 00:33:44 - train: epoch 0030, iter [08200, 10009], lr: 0.100000, loss: 2.6471
2022-03-06 00:34:17 - train: epoch 0030, iter [08300, 10009], lr: 0.100000, loss: 2.3873
2022-03-06 00:34:50 - train: epoch 0030, iter [08400, 10009], lr: 0.100000, loss: 2.9030
2022-03-06 00:35:22 - train: epoch 0030, iter [08500, 10009], lr: 0.100000, loss: 2.6439
2022-03-06 00:35:55 - train: epoch 0030, iter [08600, 10009], lr: 0.100000, loss: 2.5678
2022-03-06 00:36:28 - train: epoch 0030, iter [08700, 10009], lr: 0.100000, loss: 2.8249
2022-03-06 00:37:01 - train: epoch 0030, iter [08800, 10009], lr: 0.100000, loss: 2.8184
2022-03-06 00:37:34 - train: epoch 0030, iter [08900, 10009], lr: 0.100000, loss: 2.7877
2022-03-06 00:38:07 - train: epoch 0030, iter [09000, 10009], lr: 0.100000, loss: 2.6290
2022-03-06 00:38:40 - train: epoch 0030, iter [09100, 10009], lr: 0.100000, loss: 2.7184
2022-03-06 00:39:12 - train: epoch 0030, iter [09200, 10009], lr: 0.100000, loss: 2.3981
2022-03-06 00:39:45 - train: epoch 0030, iter [09300, 10009], lr: 0.100000, loss: 2.7741
2022-03-06 00:40:18 - train: epoch 0030, iter [09400, 10009], lr: 0.100000, loss: 2.5333
2022-03-06 00:40:51 - train: epoch 0030, iter [09500, 10009], lr: 0.100000, loss: 2.6733
2022-03-06 00:41:24 - train: epoch 0030, iter [09600, 10009], lr: 0.100000, loss: 2.7563
2022-03-06 00:41:56 - train: epoch 0030, iter [09700, 10009], lr: 0.100000, loss: 2.8195
2022-03-06 00:42:29 - train: epoch 0030, iter [09800, 10009], lr: 0.100000, loss: 2.3307
2022-03-06 00:43:02 - train: epoch 0030, iter [09900, 10009], lr: 0.100000, loss: 2.7434
2022-03-06 00:43:35 - train: epoch 0030, iter [10000, 10009], lr: 0.100000, loss: 2.7706
2022-03-06 00:43:38 - train: epoch 030, train_loss: 2.6421
2022-03-06 00:44:53 - eval: epoch: 030, acc1: 48.270%, acc5: 74.336%, test_loss: 2.2649, per_image_load_time: 1.182ms, per_image_inference_time: 1.551ms
2022-03-06 00:44:54 - until epoch: 030, best_acc1: 48.518%
2022-03-06 00:44:54 - epoch 031 lr: 0.010000000000000002
2022-03-06 00:45:30 - train: epoch 0031, iter [00100, 10009], lr: 0.010000, loss: 2.6481
2022-03-06 00:46:03 - train: epoch 0031, iter [00200, 10009], lr: 0.010000, loss: 2.3949
2022-03-06 00:46:36 - train: epoch 0031, iter [00300, 10009], lr: 0.010000, loss: 2.1348
2022-03-06 00:47:08 - train: epoch 0031, iter [00400, 10009], lr: 0.010000, loss: 2.0653
2022-03-06 00:47:41 - train: epoch 0031, iter [00500, 10009], lr: 0.010000, loss: 2.3897
2022-03-06 00:48:14 - train: epoch 0031, iter [00600, 10009], lr: 0.010000, loss: 2.1090
2022-03-06 00:48:46 - train: epoch 0031, iter [00700, 10009], lr: 0.010000, loss: 2.3582
2022-03-06 00:49:19 - train: epoch 0031, iter [00800, 10009], lr: 0.010000, loss: 2.0528
2022-03-06 00:49:52 - train: epoch 0031, iter [00900, 10009], lr: 0.010000, loss: 2.0992
2022-03-06 00:50:24 - train: epoch 0031, iter [01000, 10009], lr: 0.010000, loss: 2.3438
2022-03-06 00:50:57 - train: epoch 0031, iter [01100, 10009], lr: 0.010000, loss: 2.2004
2022-03-06 00:51:30 - train: epoch 0031, iter [01200, 10009], lr: 0.010000, loss: 1.9826
2022-03-06 00:52:02 - train: epoch 0031, iter [01300, 10009], lr: 0.010000, loss: 1.7380
2022-03-06 00:52:35 - train: epoch 0031, iter [01400, 10009], lr: 0.010000, loss: 2.0824
2022-03-06 00:53:07 - train: epoch 0031, iter [01500, 10009], lr: 0.010000, loss: 2.2156
2022-03-06 00:53:40 - train: epoch 0031, iter [01600, 10009], lr: 0.010000, loss: 2.0868
2022-03-06 00:54:13 - train: epoch 0031, iter [01700, 10009], lr: 0.010000, loss: 2.1273
2022-03-06 00:54:45 - train: epoch 0031, iter [01800, 10009], lr: 0.010000, loss: 2.0296
2022-03-06 00:55:18 - train: epoch 0031, iter [01900, 10009], lr: 0.010000, loss: 2.1659
2022-03-06 00:55:51 - train: epoch 0031, iter [02000, 10009], lr: 0.010000, loss: 2.1665
2022-03-06 00:56:24 - train: epoch 0031, iter [02100, 10009], lr: 0.010000, loss: 2.0697
2022-03-06 00:56:56 - train: epoch 0031, iter [02200, 10009], lr: 0.010000, loss: 1.8810
2022-03-06 00:57:29 - train: epoch 0031, iter [02300, 10009], lr: 0.010000, loss: 2.0257
2022-03-06 00:58:02 - train: epoch 0031, iter [02400, 10009], lr: 0.010000, loss: 2.0048
2022-03-06 00:58:34 - train: epoch 0031, iter [02500, 10009], lr: 0.010000, loss: 2.3144
2022-03-06 00:59:07 - train: epoch 0031, iter [02600, 10009], lr: 0.010000, loss: 1.8183
2022-03-06 00:59:40 - train: epoch 0031, iter [02700, 10009], lr: 0.010000, loss: 2.0542
2022-03-06 01:00:12 - train: epoch 0031, iter [02800, 10009], lr: 0.010000, loss: 1.9528
2022-03-06 01:00:45 - train: epoch 0031, iter [02900, 10009], lr: 0.010000, loss: 2.2782
2022-03-06 01:01:18 - train: epoch 0031, iter [03000, 10009], lr: 0.010000, loss: 2.0837
2022-03-06 01:01:51 - train: epoch 0031, iter [03100, 10009], lr: 0.010000, loss: 2.1651
2022-03-06 01:02:23 - train: epoch 0031, iter [03200, 10009], lr: 0.010000, loss: 1.6314
2022-03-06 01:02:56 - train: epoch 0031, iter [03300, 10009], lr: 0.010000, loss: 1.9253
2022-03-06 01:03:29 - train: epoch 0031, iter [03400, 10009], lr: 0.010000, loss: 1.6499
2022-03-06 01:04:02 - train: epoch 0031, iter [03500, 10009], lr: 0.010000, loss: 1.9323
2022-03-06 01:04:34 - train: epoch 0031, iter [03600, 10009], lr: 0.010000, loss: 1.5783
2022-03-06 01:05:07 - train: epoch 0031, iter [03700, 10009], lr: 0.010000, loss: 1.8462
2022-03-06 01:05:40 - train: epoch 0031, iter [03800, 10009], lr: 0.010000, loss: 1.9146
2022-03-06 01:06:12 - train: epoch 0031, iter [03900, 10009], lr: 0.010000, loss: 2.1216
2022-03-06 01:06:45 - train: epoch 0031, iter [04000, 10009], lr: 0.010000, loss: 1.6000
2022-03-06 01:07:18 - train: epoch 0031, iter [04100, 10009], lr: 0.010000, loss: 2.0801
2022-03-06 01:07:50 - train: epoch 0031, iter [04200, 10009], lr: 0.010000, loss: 1.7452
2022-03-06 01:08:23 - train: epoch 0031, iter [04300, 10009], lr: 0.010000, loss: 2.0090
2022-03-06 01:08:56 - train: epoch 0031, iter [04400, 10009], lr: 0.010000, loss: 1.7807
2022-03-06 01:09:29 - train: epoch 0031, iter [04500, 10009], lr: 0.010000, loss: 1.9002
2022-03-06 01:10:01 - train: epoch 0031, iter [04600, 10009], lr: 0.010000, loss: 1.6688
2022-03-06 01:10:34 - train: epoch 0031, iter [04700, 10009], lr: 0.010000, loss: 1.9649
2022-03-06 01:11:07 - train: epoch 0031, iter [04800, 10009], lr: 0.010000, loss: 1.9846
2022-03-06 01:11:39 - train: epoch 0031, iter [04900, 10009], lr: 0.010000, loss: 1.7690
2022-03-06 01:12:12 - train: epoch 0031, iter [05000, 10009], lr: 0.010000, loss: 1.7465
2022-03-06 01:12:45 - train: epoch 0031, iter [05100, 10009], lr: 0.010000, loss: 2.0549
2022-03-06 01:13:18 - train: epoch 0031, iter [05200, 10009], lr: 0.010000, loss: 1.9088
2022-03-06 01:13:50 - train: epoch 0031, iter [05300, 10009], lr: 0.010000, loss: 1.9200
2022-03-06 01:14:23 - train: epoch 0031, iter [05400, 10009], lr: 0.010000, loss: 2.0676
2022-03-06 01:14:56 - train: epoch 0031, iter [05500, 10009], lr: 0.010000, loss: 1.8365
2022-03-06 01:15:28 - train: epoch 0031, iter [05600, 10009], lr: 0.010000, loss: 2.3010
2022-03-06 01:16:01 - train: epoch 0031, iter [05700, 10009], lr: 0.010000, loss: 1.8874
2022-03-06 01:16:34 - train: epoch 0031, iter [05800, 10009], lr: 0.010000, loss: 2.0005
2022-03-06 01:17:07 - train: epoch 0031, iter [05900, 10009], lr: 0.010000, loss: 1.8259
2022-03-06 01:17:39 - train: epoch 0031, iter [06000, 10009], lr: 0.010000, loss: 2.3184
2022-03-06 01:18:12 - train: epoch 0031, iter [06100, 10009], lr: 0.010000, loss: 1.7455
2022-03-06 01:18:45 - train: epoch 0031, iter [06200, 10009], lr: 0.010000, loss: 1.9317
2022-03-06 01:19:17 - train: epoch 0031, iter [06300, 10009], lr: 0.010000, loss: 1.8507
2022-03-06 01:19:50 - train: epoch 0031, iter [06400, 10009], lr: 0.010000, loss: 1.8301
2022-03-06 01:20:23 - train: epoch 0031, iter [06500, 10009], lr: 0.010000, loss: 1.8053
2022-03-06 01:20:56 - train: epoch 0031, iter [06600, 10009], lr: 0.010000, loss: 1.9614
2022-03-06 01:21:29 - train: epoch 0031, iter [06700, 10009], lr: 0.010000, loss: 1.9179
2022-03-06 01:22:02 - train: epoch 0031, iter [06800, 10009], lr: 0.010000, loss: 2.2061
2022-03-06 01:22:34 - train: epoch 0031, iter [06900, 10009], lr: 0.010000, loss: 2.3008
2022-03-06 01:23:07 - train: epoch 0031, iter [07000, 10009], lr: 0.010000, loss: 2.2296
2022-03-06 01:23:40 - train: epoch 0031, iter [07100, 10009], lr: 0.010000, loss: 1.7229
2022-03-06 01:24:13 - train: epoch 0031, iter [07200, 10009], lr: 0.010000, loss: 2.3163
2022-03-06 01:24:46 - train: epoch 0031, iter [07300, 10009], lr: 0.010000, loss: 2.2830
2022-03-06 01:25:18 - train: epoch 0031, iter [07400, 10009], lr: 0.010000, loss: 1.8742
2022-03-06 01:25:51 - train: epoch 0031, iter [07500, 10009], lr: 0.010000, loss: 1.9456
2022-03-06 01:26:24 - train: epoch 0031, iter [07600, 10009], lr: 0.010000, loss: 1.9332
2022-03-06 01:26:57 - train: epoch 0031, iter [07700, 10009], lr: 0.010000, loss: 1.9871
2022-03-06 01:27:29 - train: epoch 0031, iter [07800, 10009], lr: 0.010000, loss: 1.5434
2022-03-06 01:28:02 - train: epoch 0031, iter [07900, 10009], lr: 0.010000, loss: 1.7786
2022-03-06 01:28:35 - train: epoch 0031, iter [08000, 10009], lr: 0.010000, loss: 1.7284
2022-03-06 01:29:08 - train: epoch 0031, iter [08100, 10009], lr: 0.010000, loss: 1.9763
2022-03-06 01:29:41 - train: epoch 0031, iter [08200, 10009], lr: 0.010000, loss: 1.7171
2022-03-06 01:30:13 - train: epoch 0031, iter [08300, 10009], lr: 0.010000, loss: 1.7753
2022-03-06 01:30:46 - train: epoch 0031, iter [08400, 10009], lr: 0.010000, loss: 1.8604
2022-03-06 01:31:19 - train: epoch 0031, iter [08500, 10009], lr: 0.010000, loss: 2.1495
2022-03-06 01:31:51 - train: epoch 0031, iter [08600, 10009], lr: 0.010000, loss: 1.6852
2022-03-06 01:32:24 - train: epoch 0031, iter [08700, 10009], lr: 0.010000, loss: 1.5675
2022-03-06 01:32:57 - train: epoch 0031, iter [08800, 10009], lr: 0.010000, loss: 1.6828
2022-03-06 01:33:30 - train: epoch 0031, iter [08900, 10009], lr: 0.010000, loss: 2.0902
2022-03-06 01:34:03 - train: epoch 0031, iter [09000, 10009], lr: 0.010000, loss: 1.6074
2022-03-06 01:34:35 - train: epoch 0031, iter [09100, 10009], lr: 0.010000, loss: 2.3932
2022-03-06 01:35:08 - train: epoch 0031, iter [09200, 10009], lr: 0.010000, loss: 2.2350
2022-03-06 01:35:41 - train: epoch 0031, iter [09300, 10009], lr: 0.010000, loss: 2.2219
2022-03-06 01:36:13 - train: epoch 0031, iter [09400, 10009], lr: 0.010000, loss: 1.8255
2022-03-06 01:36:46 - train: epoch 0031, iter [09500, 10009], lr: 0.010000, loss: 1.6901
2022-03-06 01:37:19 - train: epoch 0031, iter [09600, 10009], lr: 0.010000, loss: 1.6535
2022-03-06 01:37:52 - train: epoch 0031, iter [09700, 10009], lr: 0.010000, loss: 1.9396
2022-03-06 01:38:24 - train: epoch 0031, iter [09800, 10009], lr: 0.010000, loss: 2.0009
2022-03-06 01:38:57 - train: epoch 0031, iter [09900, 10009], lr: 0.010000, loss: 1.6067
2022-03-06 01:39:30 - train: epoch 0031, iter [10000, 10009], lr: 0.010000, loss: 1.8890
2022-03-06 01:39:33 - train: epoch 031, train_loss: 1.9995
2022-03-06 01:40:48 - eval: epoch: 031, acc1: 63.190%, acc5: 84.938%, test_loss: 1.5350, per_image_load_time: 0.994ms, per_image_inference_time: 1.556ms
2022-03-06 01:40:49 - until epoch: 031, best_acc1: 63.190%
2022-03-06 01:40:49 - epoch 032 lr: 0.010000000000000002
2022-03-06 01:41:25 - train: epoch 0032, iter [00100, 10009], lr: 0.010000, loss: 2.0820
2022-03-06 01:41:58 - train: epoch 0032, iter [00200, 10009], lr: 0.010000, loss: 1.5411
2022-03-06 01:42:30 - train: epoch 0032, iter [00300, 10009], lr: 0.010000, loss: 1.8701
2022-03-06 01:43:03 - train: epoch 0032, iter [00400, 10009], lr: 0.010000, loss: 1.9940
2022-03-06 01:43:36 - train: epoch 0032, iter [00500, 10009], lr: 0.010000, loss: 1.9528
2022-03-06 01:44:09 - train: epoch 0032, iter [00600, 10009], lr: 0.010000, loss: 1.5091
2022-03-06 01:44:41 - train: epoch 0032, iter [00700, 10009], lr: 0.010000, loss: 1.9285
2022-03-06 01:45:14 - train: epoch 0032, iter [00800, 10009], lr: 0.010000, loss: 1.9424
2022-03-06 01:45:47 - train: epoch 0032, iter [00900, 10009], lr: 0.010000, loss: 1.8015
2022-03-06 01:46:19 - train: epoch 0032, iter [01000, 10009], lr: 0.010000, loss: 1.6991
2022-03-06 01:46:52 - train: epoch 0032, iter [01100, 10009], lr: 0.010000, loss: 2.1041
2022-03-06 01:47:25 - train: epoch 0032, iter [01200, 10009], lr: 0.010000, loss: 1.8461
2022-03-06 01:47:58 - train: epoch 0032, iter [01300, 10009], lr: 0.010000, loss: 1.8845
2022-03-06 01:48:30 - train: epoch 0032, iter [01400, 10009], lr: 0.010000, loss: 1.7982
2022-03-06 01:49:03 - train: epoch 0032, iter [01500, 10009], lr: 0.010000, loss: 1.6533
2022-03-06 01:49:36 - train: epoch 0032, iter [01600, 10009], lr: 0.010000, loss: 1.7014
2022-03-06 01:50:09 - train: epoch 0032, iter [01700, 10009], lr: 0.010000, loss: 2.0483
2022-03-06 01:50:41 - train: epoch 0032, iter [01800, 10009], lr: 0.010000, loss: 1.8518
2022-03-06 01:51:14 - train: epoch 0032, iter [01900, 10009], lr: 0.010000, loss: 2.0466
2022-03-06 01:51:46 - train: epoch 0032, iter [02000, 10009], lr: 0.010000, loss: 1.7135
2022-03-06 01:52:19 - train: epoch 0032, iter [02100, 10009], lr: 0.010000, loss: 1.7979
2022-03-06 01:52:52 - train: epoch 0032, iter [02200, 10009], lr: 0.010000, loss: 2.0362
2022-03-06 01:53:25 - train: epoch 0032, iter [02300, 10009], lr: 0.010000, loss: 2.2102
2022-03-06 01:53:57 - train: epoch 0032, iter [02400, 10009], lr: 0.010000, loss: 1.8304
2022-03-06 01:54:30 - train: epoch 0032, iter [02500, 10009], lr: 0.010000, loss: 2.0263
2022-03-06 01:55:03 - train: epoch 0032, iter [02600, 10009], lr: 0.010000, loss: 1.7781
2022-03-06 01:55:35 - train: epoch 0032, iter [02700, 10009], lr: 0.010000, loss: 1.6584
2022-03-06 01:56:08 - train: epoch 0032, iter [02800, 10009], lr: 0.010000, loss: 2.1889
2022-03-06 01:56:41 - train: epoch 0032, iter [02900, 10009], lr: 0.010000, loss: 1.3438
2022-03-06 01:57:13 - train: epoch 0032, iter [03000, 10009], lr: 0.010000, loss: 1.9470
2022-03-06 01:57:46 - train: epoch 0032, iter [03100, 10009], lr: 0.010000, loss: 2.0145
2022-03-06 01:58:19 - train: epoch 0032, iter [03200, 10009], lr: 0.010000, loss: 2.1134
2022-03-06 01:58:51 - train: epoch 0032, iter [03300, 10009], lr: 0.010000, loss: 1.7136
2022-03-06 01:59:24 - train: epoch 0032, iter [03400, 10009], lr: 0.010000, loss: 1.8031
2022-03-06 01:59:57 - train: epoch 0032, iter [03500, 10009], lr: 0.010000, loss: 2.0257
2022-03-06 02:00:29 - train: epoch 0032, iter [03600, 10009], lr: 0.010000, loss: 2.0755
2022-03-06 02:01:02 - train: epoch 0032, iter [03700, 10009], lr: 0.010000, loss: 1.6852
2022-03-06 02:01:35 - train: epoch 0032, iter [03800, 10009], lr: 0.010000, loss: 1.6691
2022-03-06 02:02:07 - train: epoch 0032, iter [03900, 10009], lr: 0.010000, loss: 1.8733
2022-03-06 02:02:40 - train: epoch 0032, iter [04000, 10009], lr: 0.010000, loss: 1.8084
2022-03-06 02:03:13 - train: epoch 0032, iter [04100, 10009], lr: 0.010000, loss: 2.0105
2022-03-06 02:03:46 - train: epoch 0032, iter [04200, 10009], lr: 0.010000, loss: 2.1424
2022-03-06 02:04:18 - train: epoch 0032, iter [04300, 10009], lr: 0.010000, loss: 1.8007
2022-03-06 02:04:51 - train: epoch 0032, iter [04400, 10009], lr: 0.010000, loss: 2.2628
2022-03-06 02:05:24 - train: epoch 0032, iter [04500, 10009], lr: 0.010000, loss: 1.8527
2022-03-06 02:05:57 - train: epoch 0032, iter [04600, 10009], lr: 0.010000, loss: 1.9525
2022-03-06 02:06:29 - train: epoch 0032, iter [04700, 10009], lr: 0.010000, loss: 1.6554
2022-03-06 02:07:02 - train: epoch 0032, iter [04800, 10009], lr: 0.010000, loss: 1.7821
2022-03-06 02:07:35 - train: epoch 0032, iter [04900, 10009], lr: 0.010000, loss: 1.9099
2022-03-06 02:08:07 - train: epoch 0032, iter [05000, 10009], lr: 0.010000, loss: 1.9396
2022-03-06 02:08:40 - train: epoch 0032, iter [05100, 10009], lr: 0.010000, loss: 1.7066
2022-03-06 02:09:13 - train: epoch 0032, iter [05200, 10009], lr: 0.010000, loss: 1.8043
2022-03-06 02:09:45 - train: epoch 0032, iter [05300, 10009], lr: 0.010000, loss: 1.8422
2022-03-06 02:10:18 - train: epoch 0032, iter [05400, 10009], lr: 0.010000, loss: 1.9787
2022-03-06 02:10:51 - train: epoch 0032, iter [05500, 10009], lr: 0.010000, loss: 1.8456
2022-03-06 02:11:24 - train: epoch 0032, iter [05600, 10009], lr: 0.010000, loss: 1.5619
2022-03-06 02:11:57 - train: epoch 0032, iter [05700, 10009], lr: 0.010000, loss: 1.9096
2022-03-06 02:12:29 - train: epoch 0032, iter [05800, 10009], lr: 0.010000, loss: 1.7251
2022-03-06 02:13:02 - train: epoch 0032, iter [05900, 10009], lr: 0.010000, loss: 1.5262
2022-03-06 02:13:35 - train: epoch 0032, iter [06000, 10009], lr: 0.010000, loss: 1.6579
2022-03-06 02:14:07 - train: epoch 0032, iter [06100, 10009], lr: 0.010000, loss: 1.6273
2022-03-06 02:14:40 - train: epoch 0032, iter [06200, 10009], lr: 0.010000, loss: 1.8748
2022-03-06 02:15:13 - train: epoch 0032, iter [06300, 10009], lr: 0.010000, loss: 1.4703
2022-03-06 02:15:46 - train: epoch 0032, iter [06400, 10009], lr: 0.010000, loss: 1.9969
2022-03-06 02:16:19 - train: epoch 0032, iter [06500, 10009], lr: 0.010000, loss: 1.6075
2022-03-06 02:16:51 - train: epoch 0032, iter [06600, 10009], lr: 0.010000, loss: 1.4697
2022-03-06 02:17:24 - train: epoch 0032, iter [06700, 10009], lr: 0.010000, loss: 1.7231
2022-03-06 02:17:57 - train: epoch 0032, iter [06800, 10009], lr: 0.010000, loss: 1.8480
2022-03-06 02:18:30 - train: epoch 0032, iter [06900, 10009], lr: 0.010000, loss: 1.4176
2022-03-06 02:19:03 - train: epoch 0032, iter [07000, 10009], lr: 0.010000, loss: 1.7125
2022-03-06 02:19:36 - train: epoch 0032, iter [07100, 10009], lr: 0.010000, loss: 1.8689
2022-03-06 02:20:08 - train: epoch 0032, iter [07200, 10009], lr: 0.010000, loss: 1.9073
2022-03-06 02:20:41 - train: epoch 0032, iter [07300, 10009], lr: 0.010000, loss: 1.5948
2022-03-06 02:21:14 - train: epoch 0032, iter [07400, 10009], lr: 0.010000, loss: 1.7312
2022-03-06 02:21:47 - train: epoch 0032, iter [07500, 10009], lr: 0.010000, loss: 1.8506
2022-03-06 02:22:20 - train: epoch 0032, iter [07600, 10009], lr: 0.010000, loss: 1.7811
2022-03-06 02:22:52 - train: epoch 0032, iter [07700, 10009], lr: 0.010000, loss: 2.0813
2022-03-06 02:23:25 - train: epoch 0032, iter [07800, 10009], lr: 0.010000, loss: 1.8407
2022-03-06 02:23:58 - train: epoch 0032, iter [07900, 10009], lr: 0.010000, loss: 1.4744
2022-03-06 02:24:31 - train: epoch 0032, iter [08000, 10009], lr: 0.010000, loss: 2.0244
2022-03-06 02:25:04 - train: epoch 0032, iter [08100, 10009], lr: 0.010000, loss: 2.1041
2022-03-06 02:25:36 - train: epoch 0032, iter [08200, 10009], lr: 0.010000, loss: 1.9978
2022-03-06 02:26:09 - train: epoch 0032, iter [08300, 10009], lr: 0.010000, loss: 2.0453
2022-03-06 02:26:42 - train: epoch 0032, iter [08400, 10009], lr: 0.010000, loss: 1.6934
2022-03-06 02:27:15 - train: epoch 0032, iter [08500, 10009], lr: 0.010000, loss: 2.1866
2022-03-06 02:27:48 - train: epoch 0032, iter [08600, 10009], lr: 0.010000, loss: 1.9534
2022-03-06 02:28:20 - train: epoch 0032, iter [08700, 10009], lr: 0.010000, loss: 2.1030
2022-03-06 02:28:53 - train: epoch 0032, iter [08800, 10009], lr: 0.010000, loss: 1.7007
2022-03-06 02:29:26 - train: epoch 0032, iter [08900, 10009], lr: 0.010000, loss: 1.8953
2022-03-06 02:29:59 - train: epoch 0032, iter [09000, 10009], lr: 0.010000, loss: 1.9078
2022-03-06 02:30:31 - train: epoch 0032, iter [09100, 10009], lr: 0.010000, loss: 1.5587
2022-03-06 02:31:04 - train: epoch 0032, iter [09200, 10009], lr: 0.010000, loss: 1.8760
2022-03-06 02:31:37 - train: epoch 0032, iter [09300, 10009], lr: 0.010000, loss: 1.6443
2022-03-06 02:32:09 - train: epoch 0032, iter [09400, 10009], lr: 0.010000, loss: 2.0395
2022-03-06 02:32:42 - train: epoch 0032, iter [09500, 10009], lr: 0.010000, loss: 1.8986
2022-03-06 02:33:15 - train: epoch 0032, iter [09600, 10009], lr: 0.010000, loss: 1.7118
2022-03-06 02:33:48 - train: epoch 0032, iter [09700, 10009], lr: 0.010000, loss: 1.6475
2022-03-06 02:34:20 - train: epoch 0032, iter [09800, 10009], lr: 0.010000, loss: 2.0486
2022-03-06 02:34:53 - train: epoch 0032, iter [09900, 10009], lr: 0.010000, loss: 1.7534
2022-03-06 02:35:26 - train: epoch 0032, iter [10000, 10009], lr: 0.010000, loss: 1.5473
2022-03-06 02:35:29 - train: epoch 032, train_loss: 1.8506
2022-03-06 02:36:44 - eval: epoch: 032, acc1: 64.316%, acc5: 85.772%, test_loss: 1.4723, per_image_load_time: 1.176ms, per_image_inference_time: 1.584ms
2022-03-06 02:36:45 - until epoch: 032, best_acc1: 64.316%
2022-03-06 02:36:45 - epoch 033 lr: 0.010000000000000002
2022-03-06 02:37:21 - train: epoch 0033, iter [00100, 10009], lr: 0.010000, loss: 1.4554
2022-03-06 02:37:54 - train: epoch 0033, iter [00200, 10009], lr: 0.010000, loss: 1.7508
2022-03-06 02:38:27 - train: epoch 0033, iter [00300, 10009], lr: 0.010000, loss: 1.2885
2022-03-06 02:39:00 - train: epoch 0033, iter [00400, 10009], lr: 0.010000, loss: 1.9446
2022-03-06 02:39:32 - train: epoch 0033, iter [00500, 10009], lr: 0.010000, loss: 1.9369
2022-03-06 02:40:05 - train: epoch 0033, iter [00600, 10009], lr: 0.010000, loss: 1.8020
2022-03-06 02:40:38 - train: epoch 0033, iter [00700, 10009], lr: 0.010000, loss: 1.8668
2022-03-06 02:41:10 - train: epoch 0033, iter [00800, 10009], lr: 0.010000, loss: 1.6509
2022-03-06 02:41:43 - train: epoch 0033, iter [00900, 10009], lr: 0.010000, loss: 2.2396
2022-03-06 02:42:15 - train: epoch 0033, iter [01000, 10009], lr: 0.010000, loss: 2.1563
2022-03-06 02:42:48 - train: epoch 0033, iter [01100, 10009], lr: 0.010000, loss: 1.7440
2022-03-06 02:43:21 - train: epoch 0033, iter [01200, 10009], lr: 0.010000, loss: 1.5564
2022-03-06 02:43:54 - train: epoch 0033, iter [01300, 10009], lr: 0.010000, loss: 1.7191
2022-03-06 02:44:26 - train: epoch 0033, iter [01400, 10009], lr: 0.010000, loss: 2.3738
2022-03-06 02:44:59 - train: epoch 0033, iter [01500, 10009], lr: 0.010000, loss: 1.5373
2022-03-06 02:45:31 - train: epoch 0033, iter [01600, 10009], lr: 0.010000, loss: 2.0402
2022-03-06 02:46:04 - train: epoch 0033, iter [01700, 10009], lr: 0.010000, loss: 2.0088
2022-03-06 02:46:37 - train: epoch 0033, iter [01800, 10009], lr: 0.010000, loss: 1.6118
2022-03-06 02:47:09 - train: epoch 0033, iter [01900, 10009], lr: 0.010000, loss: 1.8211
2022-03-06 02:47:42 - train: epoch 0033, iter [02000, 10009], lr: 0.010000, loss: 1.8280
2022-03-06 02:48:15 - train: epoch 0033, iter [02100, 10009], lr: 0.010000, loss: 1.6423
2022-03-06 02:48:47 - train: epoch 0033, iter [02200, 10009], lr: 0.010000, loss: 1.6943
2022-03-06 02:49:20 - train: epoch 0033, iter [02300, 10009], lr: 0.010000, loss: 2.0291
2022-03-06 02:49:53 - train: epoch 0033, iter [02400, 10009], lr: 0.010000, loss: 1.9109
2022-03-06 02:50:25 - train: epoch 0033, iter [02500, 10009], lr: 0.010000, loss: 1.5840
2022-03-06 02:50:58 - train: epoch 0033, iter [02600, 10009], lr: 0.010000, loss: 2.0847
2022-03-06 02:51:31 - train: epoch 0033, iter [02700, 10009], lr: 0.010000, loss: 1.9616
2022-03-06 02:52:03 - train: epoch 0033, iter [02800, 10009], lr: 0.010000, loss: 1.7345
2022-03-06 02:52:36 - train: epoch 0033, iter [02900, 10009], lr: 0.010000, loss: 1.8666
2022-03-06 02:53:09 - train: epoch 0033, iter [03000, 10009], lr: 0.010000, loss: 1.9432
2022-03-06 02:53:41 - train: epoch 0033, iter [03100, 10009], lr: 0.010000, loss: 1.3453
2022-03-06 02:54:14 - train: epoch 0033, iter [03200, 10009], lr: 0.010000, loss: 1.8761
2022-03-06 02:54:47 - train: epoch 0033, iter [03300, 10009], lr: 0.010000, loss: 1.6660
2022-03-06 02:55:19 - train: epoch 0033, iter [03400, 10009], lr: 0.010000, loss: 1.7891
2022-03-06 02:55:52 - train: epoch 0033, iter [03500, 10009], lr: 0.010000, loss: 1.3576
2022-03-06 02:56:25 - train: epoch 0033, iter [03600, 10009], lr: 0.010000, loss: 1.8629
2022-03-06 02:56:57 - train: epoch 0033, iter [03700, 10009], lr: 0.010000, loss: 2.0235
2022-03-06 02:57:30 - train: epoch 0033, iter [03800, 10009], lr: 0.010000, loss: 1.9632
2022-03-06 02:58:03 - train: epoch 0033, iter [03900, 10009], lr: 0.010000, loss: 1.9534
2022-03-06 02:58:35 - train: epoch 0033, iter [04000, 10009], lr: 0.010000, loss: 1.7653
2022-03-06 02:59:08 - train: epoch 0033, iter [04100, 10009], lr: 0.010000, loss: 1.9772
2022-03-06 02:59:41 - train: epoch 0033, iter [04200, 10009], lr: 0.010000, loss: 1.5901
2022-03-06 03:00:13 - train: epoch 0033, iter [04300, 10009], lr: 0.010000, loss: 1.5938
2022-03-06 03:00:46 - train: epoch 0033, iter [04400, 10009], lr: 0.010000, loss: 1.7526
2022-03-06 03:01:19 - train: epoch 0033, iter [04500, 10009], lr: 0.010000, loss: 1.9766
2022-03-06 03:01:51 - train: epoch 0033, iter [04600, 10009], lr: 0.010000, loss: 1.7283
2022-03-06 03:02:24 - train: epoch 0033, iter [04700, 10009], lr: 0.010000, loss: 2.0375
2022-03-06 03:02:57 - train: epoch 0033, iter [04800, 10009], lr: 0.010000, loss: 1.8965
2022-03-06 03:03:29 - train: epoch 0033, iter [04900, 10009], lr: 0.010000, loss: 1.9224
2022-03-06 03:04:02 - train: epoch 0033, iter [05000, 10009], lr: 0.010000, loss: 1.6179
2022-03-06 03:04:35 - train: epoch 0033, iter [05100, 10009], lr: 0.010000, loss: 1.7790
2022-03-06 03:05:07 - train: epoch 0033, iter [05200, 10009], lr: 0.010000, loss: 1.4836
2022-03-06 03:05:40 - train: epoch 0033, iter [05300, 10009], lr: 0.010000, loss: 2.1793
2022-03-06 03:06:13 - train: epoch 0033, iter [05400, 10009], lr: 0.010000, loss: 1.7869
2022-03-06 03:06:45 - train: epoch 0033, iter [05500, 10009], lr: 0.010000, loss: 1.6121
2022-03-06 03:07:18 - train: epoch 0033, iter [05600, 10009], lr: 0.010000, loss: 1.9668
2022-03-06 03:07:51 - train: epoch 0033, iter [05700, 10009], lr: 0.010000, loss: 1.9688
2022-03-06 03:08:23 - train: epoch 0033, iter [05800, 10009], lr: 0.010000, loss: 1.7274
2022-03-06 03:08:56 - train: epoch 0033, iter [05900, 10009], lr: 0.010000, loss: 1.3628
2022-03-06 03:09:29 - train: epoch 0033, iter [06000, 10009], lr: 0.010000, loss: 1.7796
2022-03-06 03:10:01 - train: epoch 0033, iter [06100, 10009], lr: 0.010000, loss: 2.0160
2022-03-06 03:10:34 - train: epoch 0033, iter [06200, 10009], lr: 0.010000, loss: 1.7514
2022-03-06 03:11:07 - train: epoch 0033, iter [06300, 10009], lr: 0.010000, loss: 1.8165
2022-03-06 03:11:40 - train: epoch 0033, iter [06400, 10009], lr: 0.010000, loss: 1.6832
2022-03-06 03:12:12 - train: epoch 0033, iter [06500, 10009], lr: 0.010000, loss: 1.4249
2022-03-06 03:12:45 - train: epoch 0033, iter [06600, 10009], lr: 0.010000, loss: 1.4879
2022-03-06 03:13:18 - train: epoch 0033, iter [06700, 10009], lr: 0.010000, loss: 1.6698
2022-03-06 03:13:50 - train: epoch 0033, iter [06800, 10009], lr: 0.010000, loss: 1.7782
2022-03-06 03:14:23 - train: epoch 0033, iter [06900, 10009], lr: 0.010000, loss: 1.7959
2022-03-06 03:14:56 - train: epoch 0033, iter [07000, 10009], lr: 0.010000, loss: 1.8039
2022-03-06 03:15:28 - train: epoch 0033, iter [07100, 10009], lr: 0.010000, loss: 1.8858
2022-03-06 03:16:01 - train: epoch 0033, iter [07200, 10009], lr: 0.010000, loss: 1.9725
2022-03-06 03:16:34 - train: epoch 0033, iter [07300, 10009], lr: 0.010000, loss: 1.6814
2022-03-06 03:17:07 - train: epoch 0033, iter [07400, 10009], lr: 0.010000, loss: 1.4464
2022-03-06 03:17:39 - train: epoch 0033, iter [07500, 10009], lr: 0.010000, loss: 1.9963
2022-03-06 03:18:12 - train: epoch 0033, iter [07600, 10009], lr: 0.010000, loss: 1.9281
2022-03-06 03:18:45 - train: epoch 0033, iter [07700, 10009], lr: 0.010000, loss: 1.9025
2022-03-06 03:19:18 - train: epoch 0033, iter [07800, 10009], lr: 0.010000, loss: 2.0088
2022-03-06 03:19:51 - train: epoch 0033, iter [07900, 10009], lr: 0.010000, loss: 1.7231
2022-03-06 03:20:23 - train: epoch 0033, iter [08000, 10009], lr: 0.010000, loss: 1.9587
2022-03-06 03:20:56 - train: epoch 0033, iter [08100, 10009], lr: 0.010000, loss: 1.9100
2022-03-06 03:21:29 - train: epoch 0033, iter [08200, 10009], lr: 0.010000, loss: 1.9040
2022-03-06 03:22:02 - train: epoch 0033, iter [08300, 10009], lr: 0.010000, loss: 2.0530
2022-03-06 03:22:35 - train: epoch 0033, iter [08400, 10009], lr: 0.010000, loss: 1.8328
2022-03-06 03:23:08 - train: epoch 0033, iter [08500, 10009], lr: 0.010000, loss: 1.6970
2022-03-06 03:23:41 - train: epoch 0033, iter [08600, 10009], lr: 0.010000, loss: 1.8807
2022-03-06 03:24:14 - train: epoch 0033, iter [08700, 10009], lr: 0.010000, loss: 1.6745
2022-03-06 03:24:47 - train: epoch 0033, iter [08800, 10009], lr: 0.010000, loss: 1.7608
2022-03-06 03:25:20 - train: epoch 0033, iter [08900, 10009], lr: 0.010000, loss: 1.5979
2022-03-06 03:25:52 - train: epoch 0033, iter [09000, 10009], lr: 0.010000, loss: 1.5776
2022-03-06 03:26:25 - train: epoch 0033, iter [09100, 10009], lr: 0.010000, loss: 2.0480
2022-03-06 03:26:58 - train: epoch 0033, iter [09200, 10009], lr: 0.010000, loss: 1.7064
2022-03-06 03:27:31 - train: epoch 0033, iter [09300, 10009], lr: 0.010000, loss: 1.8040
2022-03-06 03:28:04 - train: epoch 0033, iter [09400, 10009], lr: 0.010000, loss: 1.6439
2022-03-06 03:28:37 - train: epoch 0033, iter [09500, 10009], lr: 0.010000, loss: 1.6098
2022-03-06 03:29:10 - train: epoch 0033, iter [09600, 10009], lr: 0.010000, loss: 1.8523
2022-03-06 03:29:43 - train: epoch 0033, iter [09700, 10009], lr: 0.010000, loss: 2.1296
2022-03-06 03:30:16 - train: epoch 0033, iter [09800, 10009], lr: 0.010000, loss: 1.8899
2022-03-06 03:30:49 - train: epoch 0033, iter [09900, 10009], lr: 0.010000, loss: 1.7158
2022-03-06 03:31:22 - train: epoch 0033, iter [10000, 10009], lr: 0.010000, loss: 1.7413
2022-03-06 03:31:25 - train: epoch 033, train_loss: 1.8008
2022-03-06 03:32:41 - eval: epoch: 033, acc1: 64.846%, acc5: 86.060%, test_loss: 1.4523, per_image_load_time: 0.861ms, per_image_inference_time: 1.576ms
2022-03-06 03:32:42 - until epoch: 033, best_acc1: 64.846%
2022-03-06 03:32:42 - epoch 034 lr: 0.010000000000000002
2022-03-06 03:33:18 - train: epoch 0034, iter [00100, 10009], lr: 0.010000, loss: 1.8917
2022-03-06 03:33:51 - train: epoch 0034, iter [00200, 10009], lr: 0.010000, loss: 1.4343
2022-03-06 03:34:24 - train: epoch 0034, iter [00300, 10009], lr: 0.010000, loss: 1.8410
2022-03-06 03:34:57 - train: epoch 0034, iter [00400, 10009], lr: 0.010000, loss: 1.7163
2022-03-06 03:35:30 - train: epoch 0034, iter [00500, 10009], lr: 0.010000, loss: 1.8065
2022-03-06 03:36:03 - train: epoch 0034, iter [00600, 10009], lr: 0.010000, loss: 1.6336
2022-03-06 03:36:36 - train: epoch 0034, iter [00700, 10009], lr: 0.010000, loss: 1.7229
2022-03-06 03:37:09 - train: epoch 0034, iter [00800, 10009], lr: 0.010000, loss: 1.6449
2022-03-06 03:37:42 - train: epoch 0034, iter [00900, 10009], lr: 0.010000, loss: 2.1133
2022-03-06 03:38:15 - train: epoch 0034, iter [01000, 10009], lr: 0.010000, loss: 1.8168
2022-03-06 03:38:48 - train: epoch 0034, iter [01100, 10009], lr: 0.010000, loss: 1.9229
2022-03-06 03:39:21 - train: epoch 0034, iter [01200, 10009], lr: 0.010000, loss: 1.9320
2022-03-06 03:39:54 - train: epoch 0034, iter [01300, 10009], lr: 0.010000, loss: 1.9244
2022-03-06 03:40:26 - train: epoch 0034, iter [01400, 10009], lr: 0.010000, loss: 1.6769
2022-03-06 03:40:59 - train: epoch 0034, iter [01500, 10009], lr: 0.010000, loss: 2.0083
2022-03-06 03:41:32 - train: epoch 0034, iter [01600, 10009], lr: 0.010000, loss: 1.8338
2022-03-06 03:42:05 - train: epoch 0034, iter [01700, 10009], lr: 0.010000, loss: 1.4544
2022-03-06 03:42:38 - train: epoch 0034, iter [01800, 10009], lr: 0.010000, loss: 1.9220
2022-03-06 03:43:11 - train: epoch 0034, iter [01900, 10009], lr: 0.010000, loss: 2.1136
2022-03-06 03:43:44 - train: epoch 0034, iter [02000, 10009], lr: 0.010000, loss: 1.5919
2022-03-06 03:44:16 - train: epoch 0034, iter [02100, 10009], lr: 0.010000, loss: 1.6361
2022-03-06 03:44:50 - train: epoch 0034, iter [02200, 10009], lr: 0.010000, loss: 1.6158
2022-03-06 03:45:22 - train: epoch 0034, iter [02300, 10009], lr: 0.010000, loss: 2.0760
2022-03-06 03:45:55 - train: epoch 0034, iter [02400, 10009], lr: 0.010000, loss: 1.8731
2022-03-06 03:46:28 - train: epoch 0034, iter [02500, 10009], lr: 0.010000, loss: 2.0439
2022-03-06 03:47:01 - train: epoch 0034, iter [02600, 10009], lr: 0.010000, loss: 1.4893
2022-03-06 03:47:34 - train: epoch 0034, iter [02700, 10009], lr: 0.010000, loss: 1.7733
2022-03-06 03:48:06 - train: epoch 0034, iter [02800, 10009], lr: 0.010000, loss: 1.7522
2022-03-06 03:48:39 - train: epoch 0034, iter [02900, 10009], lr: 0.010000, loss: 1.9565
2022-03-06 03:49:12 - train: epoch 0034, iter [03000, 10009], lr: 0.010000, loss: 1.6047
2022-03-06 03:49:45 - train: epoch 0034, iter [03100, 10009], lr: 0.010000, loss: 1.6416
2022-03-06 03:50:17 - train: epoch 0034, iter [03200, 10009], lr: 0.010000, loss: 1.8476
2022-03-06 03:50:50 - train: epoch 0034, iter [03300, 10009], lr: 0.010000, loss: 2.0013
2022-03-06 03:51:23 - train: epoch 0034, iter [03400, 10009], lr: 0.010000, loss: 1.8673
2022-03-06 03:51:56 - train: epoch 0034, iter [03500, 10009], lr: 0.010000, loss: 1.7071
2022-03-06 03:52:28 - train: epoch 0034, iter [03600, 10009], lr: 0.010000, loss: 2.2684
2022-03-06 03:53:01 - train: epoch 0034, iter [03700, 10009], lr: 0.010000, loss: 1.6968
2022-03-06 03:53:34 - train: epoch 0034, iter [03800, 10009], lr: 0.010000, loss: 1.8429
2022-03-06 03:54:07 - train: epoch 0034, iter [03900, 10009], lr: 0.010000, loss: 1.8752
2022-03-06 03:54:40 - train: epoch 0034, iter [04000, 10009], lr: 0.010000, loss: 1.7850
2022-03-06 03:55:13 - train: epoch 0034, iter [04100, 10009], lr: 0.010000, loss: 1.7955
2022-03-06 03:55:45 - train: epoch 0034, iter [04200, 10009], lr: 0.010000, loss: 1.6359
2022-03-06 03:56:18 - train: epoch 0034, iter [04300, 10009], lr: 0.010000, loss: 1.7949
2022-03-06 03:56:51 - train: epoch 0034, iter [04400, 10009], lr: 0.010000, loss: 1.4398
2022-03-06 03:57:24 - train: epoch 0034, iter [04500, 10009], lr: 0.010000, loss: 1.6484
2022-03-06 03:57:56 - train: epoch 0034, iter [04600, 10009], lr: 0.010000, loss: 2.0857
2022-03-06 03:58:29 - train: epoch 0034, iter [04700, 10009], lr: 0.010000, loss: 1.9074
2022-03-06 03:59:01 - train: epoch 0034, iter [04800, 10009], lr: 0.010000, loss: 1.8472
2022-03-06 03:59:34 - train: epoch 0034, iter [04900, 10009], lr: 0.010000, loss: 1.9376
2022-03-06 04:00:07 - train: epoch 0034, iter [05000, 10009], lr: 0.010000, loss: 1.9435
2022-03-06 04:00:39 - train: epoch 0034, iter [05100, 10009], lr: 0.010000, loss: 1.6934
2022-03-06 04:01:12 - train: epoch 0034, iter [05200, 10009], lr: 0.010000, loss: 1.6013
2022-03-06 04:01:45 - train: epoch 0034, iter [05300, 10009], lr: 0.010000, loss: 1.9611
2022-03-06 04:02:17 - train: epoch 0034, iter [05400, 10009], lr: 0.010000, loss: 1.9220
2022-03-06 04:02:50 - train: epoch 0034, iter [05500, 10009], lr: 0.010000, loss: 1.7027
2022-03-06 04:03:23 - train: epoch 0034, iter [05600, 10009], lr: 0.010000, loss: 1.5106
2022-03-06 04:03:55 - train: epoch 0034, iter [05700, 10009], lr: 0.010000, loss: 1.4562
2022-03-06 04:04:28 - train: epoch 0034, iter [05800, 10009], lr: 0.010000, loss: 1.6544
2022-03-06 04:05:01 - train: epoch 0034, iter [05900, 10009], lr: 0.010000, loss: 1.4683
2022-03-06 04:05:33 - train: epoch 0034, iter [06000, 10009], lr: 0.010000, loss: 1.5529
2022-03-06 04:06:06 - train: epoch 0034, iter [06100, 10009], lr: 0.010000, loss: 1.4868
2022-03-06 04:06:38 - train: epoch 0034, iter [06200, 10009], lr: 0.010000, loss: 1.8327
2022-03-06 04:07:11 - train: epoch 0034, iter [06300, 10009], lr: 0.010000, loss: 1.6462
2022-03-06 04:07:44 - train: epoch 0034, iter [06400, 10009], lr: 0.010000, loss: 1.8661
2022-03-06 04:08:16 - train: epoch 0034, iter [06500, 10009], lr: 0.010000, loss: 1.9666
2022-03-06 04:08:49 - train: epoch 0034, iter [06600, 10009], lr: 0.010000, loss: 1.7055
2022-03-06 04:09:22 - train: epoch 0034, iter [06700, 10009], lr: 0.010000, loss: 1.4544
2022-03-06 04:09:54 - train: epoch 0034, iter [06800, 10009], lr: 0.010000, loss: 2.0159
2022-03-06 04:10:27 - train: epoch 0034, iter [06900, 10009], lr: 0.010000, loss: 1.6892
2022-03-06 04:11:00 - train: epoch 0034, iter [07000, 10009], lr: 0.010000, loss: 1.6207
2022-03-06 04:11:32 - train: epoch 0034, iter [07100, 10009], lr: 0.010000, loss: 1.7101
2022-03-06 04:12:05 - train: epoch 0034, iter [07200, 10009], lr: 0.010000, loss: 1.2816
2022-03-06 04:12:38 - train: epoch 0034, iter [07300, 10009], lr: 0.010000, loss: 1.8100
2022-03-06 04:13:11 - train: epoch 0034, iter [07400, 10009], lr: 0.010000, loss: 1.8428
2022-03-06 04:13:43 - train: epoch 0034, iter [07500, 10009], lr: 0.010000, loss: 1.9253
2022-03-06 04:14:16 - train: epoch 0034, iter [07600, 10009], lr: 0.010000, loss: 1.6612
2022-03-06 04:14:49 - train: epoch 0034, iter [07700, 10009], lr: 0.010000, loss: 1.5724
2022-03-06 04:15:22 - train: epoch 0034, iter [07800, 10009], lr: 0.010000, loss: 2.0482
2022-03-06 04:15:55 - train: epoch 0034, iter [07900, 10009], lr: 0.010000, loss: 1.4874
2022-03-06 04:16:28 - train: epoch 0034, iter [08000, 10009], lr: 0.010000, loss: 1.7239
2022-03-06 04:17:00 - train: epoch 0034, iter [08100, 10009], lr: 0.010000, loss: 1.5513
2022-03-06 04:17:33 - train: epoch 0034, iter [08200, 10009], lr: 0.010000, loss: 2.2718
2022-03-06 04:18:06 - train: epoch 0034, iter [08300, 10009], lr: 0.010000, loss: 1.7739
2022-03-06 04:18:39 - train: epoch 0034, iter [08400, 10009], lr: 0.010000, loss: 1.7251
2022-03-06 04:19:12 - train: epoch 0034, iter [08500, 10009], lr: 0.010000, loss: 1.7500
2022-03-06 04:19:45 - train: epoch 0034, iter [08600, 10009], lr: 0.010000, loss: 1.9946
2022-03-06 04:20:17 - train: epoch 0034, iter [08700, 10009], lr: 0.010000, loss: 1.8915
2022-03-06 04:20:50 - train: epoch 0034, iter [08800, 10009], lr: 0.010000, loss: 1.8935
2022-03-06 04:21:23 - train: epoch 0034, iter [08900, 10009], lr: 0.010000, loss: 1.9682
2022-03-06 04:21:56 - train: epoch 0034, iter [09000, 10009], lr: 0.010000, loss: 1.6372
2022-03-06 04:22:29 - train: epoch 0034, iter [09100, 10009], lr: 0.010000, loss: 1.8155
2022-03-06 04:23:02 - train: epoch 0034, iter [09200, 10009], lr: 0.010000, loss: 1.7893
2022-03-06 04:23:34 - train: epoch 0034, iter [09300, 10009], lr: 0.010000, loss: 1.8717
2022-03-06 04:24:07 - train: epoch 0034, iter [09400, 10009], lr: 0.010000, loss: 1.7382
2022-03-06 04:24:40 - train: epoch 0034, iter [09500, 10009], lr: 0.010000, loss: 1.9665
2022-03-06 04:25:13 - train: epoch 0034, iter [09600, 10009], lr: 0.010000, loss: 1.7679
2022-03-06 04:25:45 - train: epoch 0034, iter [09700, 10009], lr: 0.010000, loss: 1.8069
2022-03-06 04:26:18 - train: epoch 0034, iter [09800, 10009], lr: 0.010000, loss: 1.6656
2022-03-06 04:26:51 - train: epoch 0034, iter [09900, 10009], lr: 0.010000, loss: 1.5868
2022-03-06 04:27:23 - train: epoch 0034, iter [10000, 10009], lr: 0.010000, loss: 1.6171
2022-03-06 04:27:27 - train: epoch 034, train_loss: 1.7825
2022-03-06 04:28:43 - eval: epoch: 034, acc1: 64.958%, acc5: 86.264%, test_loss: 1.4472, per_image_load_time: 0.796ms, per_image_inference_time: 1.550ms
2022-03-06 04:28:44 - until epoch: 034, best_acc1: 64.958%
2022-03-06 04:28:44 - epoch 035 lr: 0.010000000000000002
2022-03-06 04:29:20 - train: epoch 0035, iter [00100, 10009], lr: 0.010000, loss: 1.8938
2022-03-06 04:29:53 - train: epoch 0035, iter [00200, 10009], lr: 0.010000, loss: 1.6682
2022-03-06 04:30:25 - train: epoch 0035, iter [00300, 10009], lr: 0.010000, loss: 1.7075
2022-03-06 04:30:58 - train: epoch 0035, iter [00400, 10009], lr: 0.010000, loss: 1.6929
2022-03-06 04:31:30 - train: epoch 0035, iter [00500, 10009], lr: 0.010000, loss: 1.6678
2022-03-06 04:32:03 - train: epoch 0035, iter [00600, 10009], lr: 0.010000, loss: 1.6764
2022-03-06 04:32:35 - train: epoch 0035, iter [00700, 10009], lr: 0.010000, loss: 1.7914
2022-03-06 04:33:08 - train: epoch 0035, iter [00800, 10009], lr: 0.010000, loss: 1.7280
2022-03-06 04:33:40 - train: epoch 0035, iter [00900, 10009], lr: 0.010000, loss: 1.7395
2022-03-06 04:34:13 - train: epoch 0035, iter [01000, 10009], lr: 0.010000, loss: 2.0106
2022-03-06 04:34:45 - train: epoch 0035, iter [01100, 10009], lr: 0.010000, loss: 1.6607
2022-03-06 04:35:17 - train: epoch 0035, iter [01200, 10009], lr: 0.010000, loss: 1.7956
2022-03-06 04:35:50 - train: epoch 0035, iter [01300, 10009], lr: 0.010000, loss: 1.9285
2022-03-06 04:36:23 - train: epoch 0035, iter [01400, 10009], lr: 0.010000, loss: 1.6218
2022-03-06 04:36:55 - train: epoch 0035, iter [01500, 10009], lr: 0.010000, loss: 1.5257
2022-03-06 04:37:28 - train: epoch 0035, iter [01600, 10009], lr: 0.010000, loss: 1.6784
2022-03-06 04:38:00 - train: epoch 0035, iter [01700, 10009], lr: 0.010000, loss: 1.9886
2022-03-06 04:38:33 - train: epoch 0035, iter [01800, 10009], lr: 0.010000, loss: 1.8138
2022-03-06 04:39:06 - train: epoch 0035, iter [01900, 10009], lr: 0.010000, loss: 1.7427
2022-03-06 04:39:38 - train: epoch 0035, iter [02000, 10009], lr: 0.010000, loss: 1.9491
2022-03-06 04:40:11 - train: epoch 0035, iter [02100, 10009], lr: 0.010000, loss: 1.6374
2022-03-06 04:40:44 - train: epoch 0035, iter [02200, 10009], lr: 0.010000, loss: 1.8890
2022-03-06 04:41:16 - train: epoch 0035, iter [02300, 10009], lr: 0.010000, loss: 1.5934
2022-03-06 04:41:49 - train: epoch 0035, iter [02400, 10009], lr: 0.010000, loss: 1.6284
2022-03-06 04:42:22 - train: epoch 0035, iter [02500, 10009], lr: 0.010000, loss: 1.8390
2022-03-06 04:42:54 - train: epoch 0035, iter [02600, 10009], lr: 0.010000, loss: 1.6225
2022-03-06 04:43:27 - train: epoch 0035, iter [02700, 10009], lr: 0.010000, loss: 1.7150
2022-03-06 04:44:00 - train: epoch 0035, iter [02800, 10009], lr: 0.010000, loss: 1.7473
2022-03-06 04:44:32 - train: epoch 0035, iter [02900, 10009], lr: 0.010000, loss: 1.5991
2022-03-06 04:45:05 - train: epoch 0035, iter [03000, 10009], lr: 0.010000, loss: 2.0327
2022-03-06 04:45:37 - train: epoch 0035, iter [03100, 10009], lr: 0.010000, loss: 1.7470
2022-03-06 04:46:10 - train: epoch 0035, iter [03200, 10009], lr: 0.010000, loss: 1.7069
2022-03-06 04:46:43 - train: epoch 0035, iter [03300, 10009], lr: 0.010000, loss: 1.8734
2022-03-06 04:47:16 - train: epoch 0035, iter [03400, 10009], lr: 0.010000, loss: 1.4781
2022-03-06 04:47:48 - train: epoch 0035, iter [03500, 10009], lr: 0.010000, loss: 1.7763
2022-03-06 04:48:21 - train: epoch 0035, iter [03600, 10009], lr: 0.010000, loss: 1.7353
2022-03-06 04:48:53 - train: epoch 0035, iter [03700, 10009], lr: 0.010000, loss: 1.9634
2022-03-06 04:49:26 - train: epoch 0035, iter [03800, 10009], lr: 0.010000, loss: 2.0113
2022-03-06 04:49:59 - train: epoch 0035, iter [03900, 10009], lr: 0.010000, loss: 1.9170
2022-03-06 04:50:32 - train: epoch 0035, iter [04000, 10009], lr: 0.010000, loss: 1.9130
2022-03-06 04:51:04 - train: epoch 0035, iter [04100, 10009], lr: 0.010000, loss: 2.2725
2022-03-06 04:51:37 - train: epoch 0035, iter [04200, 10009], lr: 0.010000, loss: 1.6772
2022-03-06 04:52:10 - train: epoch 0035, iter [04300, 10009], lr: 0.010000, loss: 1.6488
2022-03-06 04:52:42 - train: epoch 0035, iter [04400, 10009], lr: 0.010000, loss: 2.1051
2022-03-06 04:53:15 - train: epoch 0035, iter [04500, 10009], lr: 0.010000, loss: 1.9553
2022-03-06 04:53:48 - train: epoch 0035, iter [04600, 10009], lr: 0.010000, loss: 1.6303
2022-03-06 04:54:20 - train: epoch 0035, iter [04700, 10009], lr: 0.010000, loss: 1.6895
2022-03-06 04:54:53 - train: epoch 0035, iter [04800, 10009], lr: 0.010000, loss: 2.1739
2022-03-06 04:55:26 - train: epoch 0035, iter [04900, 10009], lr: 0.010000, loss: 1.8436
2022-03-06 04:55:58 - train: epoch 0035, iter [05000, 10009], lr: 0.010000, loss: 1.8041
2022-03-06 04:56:31 - train: epoch 0035, iter [05100, 10009], lr: 0.010000, loss: 1.8484
2022-03-06 04:57:04 - train: epoch 0035, iter [05200, 10009], lr: 0.010000, loss: 2.0511
2022-03-06 04:57:37 - train: epoch 0035, iter [05300, 10009], lr: 0.010000, loss: 1.6046
2022-03-06 04:58:09 - train: epoch 0035, iter [05400, 10009], lr: 0.010000, loss: 1.9673
2022-03-06 04:58:42 - train: epoch 0035, iter [05500, 10009], lr: 0.010000, loss: 1.7878
2022-03-06 04:59:15 - train: epoch 0035, iter [05600, 10009], lr: 0.010000, loss: 2.1428
2022-03-06 04:59:47 - train: epoch 0035, iter [05700, 10009], lr: 0.010000, loss: 1.7161
2022-03-06 05:00:20 - train: epoch 0035, iter [05800, 10009], lr: 0.010000, loss: 2.0612
2022-03-06 05:00:53 - train: epoch 0035, iter [05900, 10009], lr: 0.010000, loss: 1.8005
2022-03-06 05:01:25 - train: epoch 0035, iter [06000, 10009], lr: 0.010000, loss: 1.8805
2022-03-06 05:01:58 - train: epoch 0035, iter [06100, 10009], lr: 0.010000, loss: 1.7548
2022-03-06 05:02:31 - train: epoch 0035, iter [06200, 10009], lr: 0.010000, loss: 1.9548
2022-03-06 05:03:04 - train: epoch 0035, iter [06300, 10009], lr: 0.010000, loss: 1.6989
2022-03-06 05:03:36 - train: epoch 0035, iter [06400, 10009], lr: 0.010000, loss: 1.6283
2022-03-06 05:04:09 - train: epoch 0035, iter [06500, 10009], lr: 0.010000, loss: 1.6826
2022-03-06 05:04:42 - train: epoch 0035, iter [06600, 10009], lr: 0.010000, loss: 1.6209
2022-03-06 05:05:14 - train: epoch 0035, iter [06700, 10009], lr: 0.010000, loss: 1.6723
2022-03-06 05:05:47 - train: epoch 0035, iter [06800, 10009], lr: 0.010000, loss: 1.8083
2022-03-06 05:06:20 - train: epoch 0035, iter [06900, 10009], lr: 0.010000, loss: 1.8744
2022-03-06 05:06:53 - train: epoch 0035, iter [07000, 10009], lr: 0.010000, loss: 1.9897
2022-03-06 05:07:26 - train: epoch 0035, iter [07100, 10009], lr: 0.010000, loss: 1.6642
2022-03-06 05:07:58 - train: epoch 0035, iter [07200, 10009], lr: 0.010000, loss: 1.6463
2022-03-06 05:08:31 - train: epoch 0035, iter [07300, 10009], lr: 0.010000, loss: 1.7956
2022-03-06 05:09:04 - train: epoch 0035, iter [07400, 10009], lr: 0.010000, loss: 1.6480
2022-03-06 05:09:37 - train: epoch 0035, iter [07500, 10009], lr: 0.010000, loss: 1.8969
2022-03-06 05:10:10 - train: epoch 0035, iter [07600, 10009], lr: 0.010000, loss: 2.0373
2022-03-06 05:10:42 - train: epoch 0035, iter [07700, 10009], lr: 0.010000, loss: 1.7105
2022-03-06 05:11:15 - train: epoch 0035, iter [07800, 10009], lr: 0.010000, loss: 1.8643
2022-03-06 05:11:48 - train: epoch 0035, iter [07900, 10009], lr: 0.010000, loss: 2.0588
2022-03-06 05:12:21 - train: epoch 0035, iter [08000, 10009], lr: 0.010000, loss: 1.6988
2022-03-06 05:12:53 - train: epoch 0035, iter [08100, 10009], lr: 0.010000, loss: 1.8349
2022-03-06 05:13:26 - train: epoch 0035, iter [08200, 10009], lr: 0.010000, loss: 1.8638
2022-03-06 05:13:59 - train: epoch 0035, iter [08300, 10009], lr: 0.010000, loss: 1.7866
2022-03-06 05:14:32 - train: epoch 0035, iter [08400, 10009], lr: 0.010000, loss: 1.7138
2022-03-06 05:15:05 - train: epoch 0035, iter [08500, 10009], lr: 0.010000, loss: 1.8983
2022-03-06 05:15:38 - train: epoch 0035, iter [08600, 10009], lr: 0.010000, loss: 1.7968
2022-03-06 05:16:10 - train: epoch 0035, iter [08700, 10009], lr: 0.010000, loss: 1.9300
2022-03-06 05:16:43 - train: epoch 0035, iter [08800, 10009], lr: 0.010000, loss: 1.7692
2022-03-06 05:17:16 - train: epoch 0035, iter [08900, 10009], lr: 0.010000, loss: 2.0532
2022-03-06 05:17:49 - train: epoch 0035, iter [09000, 10009], lr: 0.010000, loss: 1.6105
2022-03-06 05:18:22 - train: epoch 0035, iter [09100, 10009], lr: 0.010000, loss: 1.9005
2022-03-06 05:18:55 - train: epoch 0035, iter [09200, 10009], lr: 0.010000, loss: 2.0383
2022-03-06 05:19:27 - train: epoch 0035, iter [09300, 10009], lr: 0.010000, loss: 1.9831
2022-03-06 05:20:00 - train: epoch 0035, iter [09400, 10009], lr: 0.010000, loss: 1.9174
2022-03-06 05:20:33 - train: epoch 0035, iter [09500, 10009], lr: 0.010000, loss: 2.0937
2022-03-06 05:21:06 - train: epoch 0035, iter [09600, 10009], lr: 0.010000, loss: 1.7833
2022-03-06 05:21:39 - train: epoch 0035, iter [09700, 10009], lr: 0.010000, loss: 1.8710
2022-03-06 05:22:12 - train: epoch 0035, iter [09800, 10009], lr: 0.010000, loss: 1.9428
2022-03-06 05:22:44 - train: epoch 0035, iter [09900, 10009], lr: 0.010000, loss: 1.6171
2022-03-06 05:23:17 - train: epoch 0035, iter [10000, 10009], lr: 0.010000, loss: 1.6096
2022-03-06 05:23:21 - train: epoch 035, train_loss: 1.7738
2022-03-06 05:24:36 - eval: epoch: 035, acc1: 65.026%, acc5: 86.278%, test_loss: 1.4439, per_image_load_time: 0.929ms, per_image_inference_time: 1.567ms
2022-03-06 05:24:37 - until epoch: 035, best_acc1: 65.026%
2022-03-06 05:24:37 - epoch 036 lr: 0.010000000000000002
2022-03-06 05:25:13 - train: epoch 0036, iter [00100, 10009], lr: 0.010000, loss: 1.3259
2022-03-06 05:25:46 - train: epoch 0036, iter [00200, 10009], lr: 0.010000, loss: 1.5534
2022-03-06 05:26:19 - train: epoch 0036, iter [00300, 10009], lr: 0.010000, loss: 1.8477
2022-03-06 05:26:52 - train: epoch 0036, iter [00400, 10009], lr: 0.010000, loss: 1.5517
2022-03-06 05:27:25 - train: epoch 0036, iter [00500, 10009], lr: 0.010000, loss: 1.3969
2022-03-06 05:27:58 - train: epoch 0036, iter [00600, 10009], lr: 0.010000, loss: 1.6542
2022-03-06 05:28:31 - train: epoch 0036, iter [00700, 10009], lr: 0.010000, loss: 1.3834
2022-03-06 05:29:03 - train: epoch 0036, iter [00800, 10009], lr: 0.010000, loss: 1.6949
2022-03-06 05:29:36 - train: epoch 0036, iter [00900, 10009], lr: 0.010000, loss: 1.8537
2022-03-06 05:30:09 - train: epoch 0036, iter [01000, 10009], lr: 0.010000, loss: 1.6802
2022-03-06 05:30:42 - train: epoch 0036, iter [01100, 10009], lr: 0.010000, loss: 1.4769
2022-03-06 05:31:15 - train: epoch 0036, iter [01200, 10009], lr: 0.010000, loss: 1.5082
2022-03-06 05:31:47 - train: epoch 0036, iter [01300, 10009], lr: 0.010000, loss: 1.9201
2022-03-06 05:32:20 - train: epoch 0036, iter [01400, 10009], lr: 0.010000, loss: 1.6582
2022-03-06 05:32:53 - train: epoch 0036, iter [01500, 10009], lr: 0.010000, loss: 1.5016
2022-03-06 05:33:26 - train: epoch 0036, iter [01600, 10009], lr: 0.010000, loss: 1.7418
2022-03-06 05:33:59 - train: epoch 0036, iter [01700, 10009], lr: 0.010000, loss: 1.5530
2022-03-06 05:34:32 - train: epoch 0036, iter [01800, 10009], lr: 0.010000, loss: 2.0351
2022-03-06 05:35:05 - train: epoch 0036, iter [01900, 10009], lr: 0.010000, loss: 1.5855
2022-03-06 05:35:38 - train: epoch 0036, iter [02000, 10009], lr: 0.010000, loss: 1.7169
2022-03-06 05:36:10 - train: epoch 0036, iter [02100, 10009], lr: 0.010000, loss: 1.8090
2022-03-06 05:36:43 - train: epoch 0036, iter [02200, 10009], lr: 0.010000, loss: 1.8892
2022-03-06 05:37:16 - train: epoch 0036, iter [02300, 10009], lr: 0.010000, loss: 1.9432
2022-03-06 05:37:49 - train: epoch 0036, iter [02400, 10009], lr: 0.010000, loss: 1.8397
2022-03-06 05:38:22 - train: epoch 0036, iter [02500, 10009], lr: 0.010000, loss: 1.5684
2022-03-06 05:38:54 - train: epoch 0036, iter [02600, 10009], lr: 0.010000, loss: 1.6733
2022-03-06 05:39:27 - train: epoch 0036, iter [02700, 10009], lr: 0.010000, loss: 1.7859
2022-03-06 05:40:00 - train: epoch 0036, iter [02800, 10009], lr: 0.010000, loss: 1.9445
2022-03-06 05:40:33 - train: epoch 0036, iter [02900, 10009], lr: 0.010000, loss: 1.6450
2022-03-06 05:41:05 - train: epoch 0036, iter [03000, 10009], lr: 0.010000, loss: 2.1189
2022-03-06 05:41:38 - train: epoch 0036, iter [03100, 10009], lr: 0.010000, loss: 2.0035
2022-03-06 05:42:11 - train: epoch 0036, iter [03200, 10009], lr: 0.010000, loss: 1.9015
2022-03-06 05:42:44 - train: epoch 0036, iter [03300, 10009], lr: 0.010000, loss: 1.7152
2022-03-06 05:43:17 - train: epoch 0036, iter [03400, 10009], lr: 0.010000, loss: 1.9231
2022-03-06 05:43:50 - train: epoch 0036, iter [03500, 10009], lr: 0.010000, loss: 1.7659
2022-03-06 05:44:22 - train: epoch 0036, iter [03600, 10009], lr: 0.010000, loss: 1.7389
2022-03-06 05:44:55 - train: epoch 0036, iter [03700, 10009], lr: 0.010000, loss: 1.8245
2022-03-06 05:45:28 - train: epoch 0036, iter [03800, 10009], lr: 0.010000, loss: 1.8232
2022-03-06 05:46:01 - train: epoch 0036, iter [03900, 10009], lr: 0.010000, loss: 1.6831
2022-03-06 05:46:34 - train: epoch 0036, iter [04000, 10009], lr: 0.010000, loss: 1.8784
2022-03-06 05:47:06 - train: epoch 0036, iter [04100, 10009], lr: 0.010000, loss: 1.5931
2022-03-06 05:47:39 - train: epoch 0036, iter [04200, 10009], lr: 0.010000, loss: 1.5798
2022-03-06 05:48:12 - train: epoch 0036, iter [04300, 10009], lr: 0.010000, loss: 1.9221
2022-03-06 05:48:45 - train: epoch 0036, iter [04400, 10009], lr: 0.010000, loss: 1.8380
2022-03-06 05:49:18 - train: epoch 0036, iter [04500, 10009], lr: 0.010000, loss: 1.8212
2022-03-06 05:49:50 - train: epoch 0036, iter [04600, 10009], lr: 0.010000, loss: 2.1625
2022-03-06 05:50:23 - train: epoch 0036, iter [04700, 10009], lr: 0.010000, loss: 2.1335
2022-03-06 05:50:56 - train: epoch 0036, iter [04800, 10009], lr: 0.010000, loss: 1.7581
2022-03-06 05:51:29 - train: epoch 0036, iter [04900, 10009], lr: 0.010000, loss: 1.8234
2022-03-06 05:52:02 - train: epoch 0036, iter [05000, 10009], lr: 0.010000, loss: 1.4136
2022-03-06 05:52:34 - train: epoch 0036, iter [05100, 10009], lr: 0.010000, loss: 1.8050
2022-03-06 05:53:07 - train: epoch 0036, iter [05200, 10009], lr: 0.010000, loss: 1.9392
2022-03-06 05:53:40 - train: epoch 0036, iter [05300, 10009], lr: 0.010000, loss: 1.4940
2022-03-06 05:54:13 - train: epoch 0036, iter [05400, 10009], lr: 0.010000, loss: 1.6231
2022-03-06 05:54:45 - train: epoch 0036, iter [05500, 10009], lr: 0.010000, loss: 2.0420
2022-03-06 05:55:18 - train: epoch 0036, iter [05600, 10009], lr: 0.010000, loss: 1.6297
2022-03-06 05:55:51 - train: epoch 0036, iter [05700, 10009], lr: 0.010000, loss: 1.8120
2022-03-06 05:56:23 - train: epoch 0036, iter [05800, 10009], lr: 0.010000, loss: 1.3185
2022-03-06 05:56:56 - train: epoch 0036, iter [05900, 10009], lr: 0.010000, loss: 1.6486
2022-03-06 05:57:29 - train: epoch 0036, iter [06000, 10009], lr: 0.010000, loss: 2.0210
2022-03-06 05:58:02 - train: epoch 0036, iter [06100, 10009], lr: 0.010000, loss: 1.8011
2022-03-06 05:58:35 - train: epoch 0036, iter [06200, 10009], lr: 0.010000, loss: 1.6994
2022-03-06 05:59:07 - train: epoch 0036, iter [06300, 10009], lr: 0.010000, loss: 1.7866
2022-03-06 05:59:40 - train: epoch 0036, iter [06400, 10009], lr: 0.010000, loss: 1.6494
2022-03-06 06:00:13 - train: epoch 0036, iter [06500, 10009], lr: 0.010000, loss: 1.6139
2022-03-06 06:00:46 - train: epoch 0036, iter [06600, 10009], lr: 0.010000, loss: 2.1126
2022-03-06 06:01:18 - train: epoch 0036, iter [06700, 10009], lr: 0.010000, loss: 1.7902
2022-03-06 06:01:51 - train: epoch 0036, iter [06800, 10009], lr: 0.010000, loss: 1.5796
2022-03-06 06:02:24 - train: epoch 0036, iter [06900, 10009], lr: 0.010000, loss: 1.7279
2022-03-06 06:02:57 - train: epoch 0036, iter [07000, 10009], lr: 0.010000, loss: 2.3786
2022-03-06 06:03:30 - train: epoch 0036, iter [07100, 10009], lr: 0.010000, loss: 1.5115
2022-03-06 06:04:02 - train: epoch 0036, iter [07200, 10009], lr: 0.010000, loss: 2.1700
2022-03-06 06:04:35 - train: epoch 0036, iter [07300, 10009], lr: 0.010000, loss: 1.7048
2022-03-06 06:05:08 - train: epoch 0036, iter [07400, 10009], lr: 0.010000, loss: 1.7192
2022-03-06 06:05:40 - train: epoch 0036, iter [07500, 10009], lr: 0.010000, loss: 1.8717
2022-03-06 06:06:13 - train: epoch 0036, iter [07600, 10009], lr: 0.010000, loss: 2.1301
2022-03-06 06:06:46 - train: epoch 0036, iter [07700, 10009], lr: 0.010000, loss: 2.0063
2022-03-06 06:07:19 - train: epoch 0036, iter [07800, 10009], lr: 0.010000, loss: 2.0592
2022-03-06 06:07:51 - train: epoch 0036, iter [07900, 10009], lr: 0.010000, loss: 2.1505
2022-03-06 06:08:24 - train: epoch 0036, iter [08000, 10009], lr: 0.010000, loss: 1.9061
2022-03-06 06:08:57 - train: epoch 0036, iter [08100, 10009], lr: 0.010000, loss: 1.8484
2022-03-06 06:09:30 - train: epoch 0036, iter [08200, 10009], lr: 0.010000, loss: 1.6385
2022-03-06 06:10:02 - train: epoch 0036, iter [08300, 10009], lr: 0.010000, loss: 2.2775
2022-03-06 06:10:35 - train: epoch 0036, iter [08400, 10009], lr: 0.010000, loss: 1.9320
2022-03-06 06:11:08 - train: epoch 0036, iter [08500, 10009], lr: 0.010000, loss: 1.9903
2022-03-06 06:11:41 - train: epoch 0036, iter [08600, 10009], lr: 0.010000, loss: 1.7209
2022-03-06 06:12:13 - train: epoch 0036, iter [08700, 10009], lr: 0.010000, loss: 1.4970
2022-03-06 06:12:46 - train: epoch 0036, iter [08800, 10009], lr: 0.010000, loss: 1.7209
2022-03-06 06:13:19 - train: epoch 0036, iter [08900, 10009], lr: 0.010000, loss: 1.8755
2022-03-06 06:13:52 - train: epoch 0036, iter [09000, 10009], lr: 0.010000, loss: 1.7314
2022-03-06 06:14:24 - train: epoch 0036, iter [09100, 10009], lr: 0.010000, loss: 1.8918
2022-03-06 06:14:57 - train: epoch 0036, iter [09200, 10009], lr: 0.010000, loss: 1.8542
2022-03-06 06:15:30 - train: epoch 0036, iter [09300, 10009], lr: 0.010000, loss: 2.0769
2022-03-06 06:16:02 - train: epoch 0036, iter [09400, 10009], lr: 0.010000, loss: 1.8476
2022-03-06 06:16:35 - train: epoch 0036, iter [09500, 10009], lr: 0.010000, loss: 1.9728
2022-03-06 06:17:08 - train: epoch 0036, iter [09600, 10009], lr: 0.010000, loss: 2.0593
2022-03-06 06:17:41 - train: epoch 0036, iter [09700, 10009], lr: 0.010000, loss: 1.7280
2022-03-06 06:18:13 - train: epoch 0036, iter [09800, 10009], lr: 0.010000, loss: 1.6466
2022-03-06 06:18:46 - train: epoch 0036, iter [09900, 10009], lr: 0.010000, loss: 1.8740
2022-03-06 06:19:19 - train: epoch 0036, iter [10000, 10009], lr: 0.010000, loss: 1.5914
2022-03-06 06:19:22 - train: epoch 036, train_loss: 1.7692
2022-03-06 06:20:38 - eval: epoch: 036, acc1: 65.088%, acc5: 86.258%, test_loss: 1.4392, per_image_load_time: 0.981ms, per_image_inference_time: 1.570ms
2022-03-06 06:20:39 - until epoch: 036, best_acc1: 65.088%
2022-03-06 06:20:39 - epoch 037 lr: 0.010000000000000002
2022-03-06 06:21:15 - train: epoch 0037, iter [00100, 10009], lr: 0.010000, loss: 1.5576
2022-03-06 06:21:47 - train: epoch 0037, iter [00200, 10009], lr: 0.010000, loss: 1.4667
2022-03-06 06:22:20 - train: epoch 0037, iter [00300, 10009], lr: 0.010000, loss: 1.6654
2022-03-06 06:22:53 - train: epoch 0037, iter [00400, 10009], lr: 0.010000, loss: 1.4570
2022-03-06 06:23:25 - train: epoch 0037, iter [00500, 10009], lr: 0.010000, loss: 1.5471
2022-03-06 06:23:58 - train: epoch 0037, iter [00600, 10009], lr: 0.010000, loss: 1.6229
2022-03-06 06:24:31 - train: epoch 0037, iter [00700, 10009], lr: 0.010000, loss: 1.6530
2022-03-06 06:25:04 - train: epoch 0037, iter [00800, 10009], lr: 0.010000, loss: 1.5866
2022-03-06 06:25:36 - train: epoch 0037, iter [00900, 10009], lr: 0.010000, loss: 1.6911
2022-03-06 06:26:09 - train: epoch 0037, iter [01000, 10009], lr: 0.010000, loss: 1.9907
2022-03-06 06:26:42 - train: epoch 0037, iter [01100, 10009], lr: 0.010000, loss: 1.6333
2022-03-06 06:27:15 - train: epoch 0037, iter [01200, 10009], lr: 0.010000, loss: 1.9825
2022-03-06 06:27:48 - train: epoch 0037, iter [01300, 10009], lr: 0.010000, loss: 1.7533
2022-03-06 06:28:20 - train: epoch 0037, iter [01400, 10009], lr: 0.010000, loss: 1.5783
2022-03-06 06:28:53 - train: epoch 0037, iter [01500, 10009], lr: 0.010000, loss: 1.8549
2022-03-06 06:29:26 - train: epoch 0037, iter [01600, 10009], lr: 0.010000, loss: 1.7384
2022-03-06 06:29:59 - train: epoch 0037, iter [01700, 10009], lr: 0.010000, loss: 1.7673
2022-03-06 06:30:32 - train: epoch 0037, iter [01800, 10009], lr: 0.010000, loss: 1.9928
2022-03-06 06:31:04 - train: epoch 0037, iter [01900, 10009], lr: 0.010000, loss: 2.0322
2022-03-06 06:31:37 - train: epoch 0037, iter [02000, 10009], lr: 0.010000, loss: 1.8267
2022-03-06 06:32:10 - train: epoch 0037, iter [02100, 10009], lr: 0.010000, loss: 1.8299
2022-03-06 06:32:42 - train: epoch 0037, iter [02200, 10009], lr: 0.010000, loss: 1.7872
2022-03-06 06:33:15 - train: epoch 0037, iter [02300, 10009], lr: 0.010000, loss: 1.9857
2022-03-06 06:33:48 - train: epoch 0037, iter [02400, 10009], lr: 0.010000, loss: 1.6372
2022-03-06 06:34:21 - train: epoch 0037, iter [02500, 10009], lr: 0.010000, loss: 2.0097
2022-03-06 06:34:53 - train: epoch 0037, iter [02600, 10009], lr: 0.010000, loss: 1.8130
2022-03-06 06:35:26 - train: epoch 0037, iter [02700, 10009], lr: 0.010000, loss: 1.8870
2022-03-06 06:35:59 - train: epoch 0037, iter [02800, 10009], lr: 0.010000, loss: 1.9015
2022-03-06 06:36:32 - train: epoch 0037, iter [02900, 10009], lr: 0.010000, loss: 1.4487
2022-03-06 06:37:04 - train: epoch 0037, iter [03000, 10009], lr: 0.010000, loss: 1.9473
2022-03-06 06:37:37 - train: epoch 0037, iter [03100, 10009], lr: 0.010000, loss: 1.9805
2022-03-06 06:38:10 - train: epoch 0037, iter [03200, 10009], lr: 0.010000, loss: 1.4689
2022-03-06 06:38:42 - train: epoch 0037, iter [03300, 10009], lr: 0.010000, loss: 1.9755
2022-03-06 06:39:15 - train: epoch 0037, iter [03400, 10009], lr: 0.010000, loss: 1.9459
2022-03-06 06:39:48 - train: epoch 0037, iter [03500, 10009], lr: 0.010000, loss: 1.8915
2022-03-06 06:40:21 - train: epoch 0037, iter [03600, 10009], lr: 0.010000, loss: 1.9168
2022-03-06 06:40:54 - train: epoch 0037, iter [03700, 10009], lr: 0.010000, loss: 1.7342
2022-03-06 06:41:26 - train: epoch 0037, iter [03800, 10009], lr: 0.010000, loss: 1.3928
2022-03-06 06:41:59 - train: epoch 0037, iter [03900, 10009], lr: 0.010000, loss: 1.7544
2022-03-06 06:42:32 - train: epoch 0037, iter [04000, 10009], lr: 0.010000, loss: 1.7212
2022-03-06 06:43:04 - train: epoch 0037, iter [04100, 10009], lr: 0.010000, loss: 1.7725
2022-03-06 06:43:37 - train: epoch 0037, iter [04200, 10009], lr: 0.010000, loss: 1.6337
2022-03-06 06:44:10 - train: epoch 0037, iter [04300, 10009], lr: 0.010000, loss: 1.6036
2022-03-06 06:44:43 - train: epoch 0037, iter [04400, 10009], lr: 0.010000, loss: 1.7164
2022-03-06 06:45:15 - train: epoch 0037, iter [04500, 10009], lr: 0.010000, loss: 1.4116
2022-03-06 06:45:48 - train: epoch 0037, iter [04600, 10009], lr: 0.010000, loss: 1.8644
2022-03-06 06:46:21 - train: epoch 0037, iter [04700, 10009], lr: 0.010000, loss: 1.9138
2022-03-06 06:46:54 - train: epoch 0037, iter [04800, 10009], lr: 0.010000, loss: 1.5467
2022-03-06 06:47:26 - train: epoch 0037, iter [04900, 10009], lr: 0.010000, loss: 1.9164
2022-03-06 06:47:59 - train: epoch 0037, iter [05000, 10009], lr: 0.010000, loss: 1.4392
2022-03-06 06:48:32 - train: epoch 0037, iter [05100, 10009], lr: 0.010000, loss: 1.6286
2022-03-06 06:49:05 - train: epoch 0037, iter [05200, 10009], lr: 0.010000, loss: 2.1330
2022-03-06 06:49:38 - train: epoch 0037, iter [05300, 10009], lr: 0.010000, loss: 1.6991
2022-03-06 06:50:11 - train: epoch 0037, iter [05400, 10009], lr: 0.010000, loss: 1.9662
2022-03-06 06:50:43 - train: epoch 0037, iter [05500, 10009], lr: 0.010000, loss: 2.0161
2022-03-06 06:51:16 - train: epoch 0037, iter [05600, 10009], lr: 0.010000, loss: 1.8327
2022-03-06 06:51:49 - train: epoch 0037, iter [05700, 10009], lr: 0.010000, loss: 1.7761
2022-03-06 06:52:22 - train: epoch 0037, iter [05800, 10009], lr: 0.010000, loss: 1.8534
2022-03-06 06:52:54 - train: epoch 0037, iter [05900, 10009], lr: 0.010000, loss: 1.8271
2022-03-06 06:53:27 - train: epoch 0037, iter [06000, 10009], lr: 0.010000, loss: 2.1270
2022-03-06 06:54:00 - train: epoch 0037, iter [06100, 10009], lr: 0.010000, loss: 1.7085
2022-03-06 06:54:33 - train: epoch 0037, iter [06200, 10009], lr: 0.010000, loss: 2.1178
2022-03-06 06:55:06 - train: epoch 0037, iter [06300, 10009], lr: 0.010000, loss: 2.0804
2022-03-06 06:55:39 - train: epoch 0037, iter [06400, 10009], lr: 0.010000, loss: 2.0956
2022-03-06 06:56:11 - train: epoch 0037, iter [06500, 10009], lr: 0.010000, loss: 2.1254
2022-03-06 06:56:44 - train: epoch 0037, iter [06600, 10009], lr: 0.010000, loss: 1.8832
2022-03-06 06:57:17 - train: epoch 0037, iter [06700, 10009], lr: 0.010000, loss: 2.1462
2022-03-06 06:57:50 - train: epoch 0037, iter [06800, 10009], lr: 0.010000, loss: 1.6415
2022-03-06 06:58:23 - train: epoch 0037, iter [06900, 10009], lr: 0.010000, loss: 2.0405
2022-03-06 06:58:56 - train: epoch 0037, iter [07000, 10009], lr: 0.010000, loss: 1.6965
2022-03-06 06:59:29 - train: epoch 0037, iter [07100, 10009], lr: 0.010000, loss: 1.5559
2022-03-06 07:00:02 - train: epoch 0037, iter [07200, 10009], lr: 0.010000, loss: 1.9941
2022-03-06 07:00:34 - train: epoch 0037, iter [07300, 10009], lr: 0.010000, loss: 1.5280
2022-03-06 07:01:07 - train: epoch 0037, iter [07400, 10009], lr: 0.010000, loss: 1.6789
2022-03-06 07:01:40 - train: epoch 0037, iter [07500, 10009], lr: 0.010000, loss: 1.4742
2022-03-06 07:02:13 - train: epoch 0037, iter [07600, 10009], lr: 0.010000, loss: 1.9660
2022-03-06 07:02:46 - train: epoch 0037, iter [07700, 10009], lr: 0.010000, loss: 1.6052
2022-03-06 07:03:18 - train: epoch 0037, iter [07800, 10009], lr: 0.010000, loss: 1.9004
2022-03-06 07:03:51 - train: epoch 0037, iter [07900, 10009], lr: 0.010000, loss: 1.8023
2022-03-06 07:04:24 - train: epoch 0037, iter [08000, 10009], lr: 0.010000, loss: 1.8807
2022-03-06 07:04:57 - train: epoch 0037, iter [08100, 10009], lr: 0.010000, loss: 1.7124
2022-03-06 07:05:29 - train: epoch 0037, iter [08200, 10009], lr: 0.010000, loss: 1.9190
2022-03-06 07:06:02 - train: epoch 0037, iter [08300, 10009], lr: 0.010000, loss: 1.6200
2022-03-06 07:06:35 - train: epoch 0037, iter [08400, 10009], lr: 0.010000, loss: 2.0837
2022-03-06 07:07:08 - train: epoch 0037, iter [08500, 10009], lr: 0.010000, loss: 1.8249
2022-03-06 07:07:40 - train: epoch 0037, iter [08600, 10009], lr: 0.010000, loss: 1.7390
2022-03-06 07:08:13 - train: epoch 0037, iter [08700, 10009], lr: 0.010000, loss: 1.9387
2022-03-06 07:08:46 - train: epoch 0037, iter [08800, 10009], lr: 0.010000, loss: 1.5943
2022-03-06 07:09:18 - train: epoch 0037, iter [08900, 10009], lr: 0.010000, loss: 1.7285
2022-03-06 07:09:51 - train: epoch 0037, iter [09000, 10009], lr: 0.010000, loss: 1.6903
2022-03-06 07:10:24 - train: epoch 0037, iter [09100, 10009], lr: 0.010000, loss: 1.7032
2022-03-06 07:10:57 - train: epoch 0037, iter [09200, 10009], lr: 0.010000, loss: 1.7500
2022-03-06 07:11:29 - train: epoch 0037, iter [09300, 10009], lr: 0.010000, loss: 2.0052
2022-03-06 07:12:02 - train: epoch 0037, iter [09400, 10009], lr: 0.010000, loss: 1.9235
2022-03-06 07:12:35 - train: epoch 0037, iter [09500, 10009], lr: 0.010000, loss: 1.7389
2022-03-06 07:13:07 - train: epoch 0037, iter [09600, 10009], lr: 0.010000, loss: 1.8001
2022-03-06 07:13:40 - train: epoch 0037, iter [09700, 10009], lr: 0.010000, loss: 1.8759
2022-03-06 07:14:13 - train: epoch 0037, iter [09800, 10009], lr: 0.010000, loss: 1.6129
2022-03-06 07:14:46 - train: epoch 0037, iter [09900, 10009], lr: 0.010000, loss: 2.1157
2022-03-06 07:15:19 - train: epoch 0037, iter [10000, 10009], lr: 0.010000, loss: 1.6222
2022-03-06 07:15:23 - train: epoch 037, train_loss: 1.7715
2022-03-06 07:16:39 - eval: epoch: 037, acc1: 65.284%, acc5: 86.324%, test_loss: 1.4393, per_image_load_time: 1.270ms, per_image_inference_time: 1.568ms
2022-03-06 07:16:39 - until epoch: 037, best_acc1: 65.284%
2022-03-06 07:16:39 - epoch 038 lr: 0.010000000000000002
2022-03-06 07:17:16 - train: epoch 0038, iter [00100, 10009], lr: 0.010000, loss: 1.6518
2022-03-06 07:17:49 - train: epoch 0038, iter [00200, 10009], lr: 0.010000, loss: 1.5730
2022-03-06 07:18:22 - train: epoch 0038, iter [00300, 10009], lr: 0.010000, loss: 1.2577
2022-03-06 07:18:54 - train: epoch 0038, iter [00400, 10009], lr: 0.010000, loss: 1.6524
2022-03-06 07:19:27 - train: epoch 0038, iter [00500, 10009], lr: 0.010000, loss: 1.8267
2022-03-06 07:20:00 - train: epoch 0038, iter [00600, 10009], lr: 0.010000, loss: 1.6114
2022-03-06 07:20:33 - train: epoch 0038, iter [00700, 10009], lr: 0.010000, loss: 1.7362
2022-03-06 07:21:06 - train: epoch 0038, iter [00800, 10009], lr: 0.010000, loss: 1.4845
2022-03-06 07:21:38 - train: epoch 0038, iter [00900, 10009], lr: 0.010000, loss: 1.5725
2022-03-06 07:22:11 - train: epoch 0038, iter [01000, 10009], lr: 0.010000, loss: 1.7547
2022-03-06 07:22:44 - train: epoch 0038, iter [01100, 10009], lr: 0.010000, loss: 2.0177
2022-03-06 07:23:16 - train: epoch 0038, iter [01200, 10009], lr: 0.010000, loss: 2.1422
2022-03-06 07:23:49 - train: epoch 0038, iter [01300, 10009], lr: 0.010000, loss: 2.0453
2022-03-06 07:24:21 - train: epoch 0038, iter [01400, 10009], lr: 0.010000, loss: 1.9047
2022-03-06 07:24:54 - train: epoch 0038, iter [01500, 10009], lr: 0.010000, loss: 1.5897
2022-03-06 07:25:27 - train: epoch 0038, iter [01600, 10009], lr: 0.010000, loss: 1.5235
2022-03-06 07:25:59 - train: epoch 0038, iter [01700, 10009], lr: 0.010000, loss: 1.6987
2022-03-06 07:26:32 - train: epoch 0038, iter [01800, 10009], lr: 0.010000, loss: 1.5001
2022-03-06 07:27:05 - train: epoch 0038, iter [01900, 10009], lr: 0.010000, loss: 1.7846
2022-03-06 07:27:37 - train: epoch 0038, iter [02000, 10009], lr: 0.010000, loss: 1.9781
2022-03-06 07:28:10 - train: epoch 0038, iter [02100, 10009], lr: 0.010000, loss: 1.4547
2022-03-06 07:28:42 - train: epoch 0038, iter [02200, 10009], lr: 0.010000, loss: 1.8416
2022-03-06 07:29:15 - train: epoch 0038, iter [02300, 10009], lr: 0.010000, loss: 1.6404
2022-03-06 07:29:48 - train: epoch 0038, iter [02400, 10009], lr: 0.010000, loss: 1.7132
2022-03-06 07:30:21 - train: epoch 0038, iter [02500, 10009], lr: 0.010000, loss: 1.9738
2022-03-06 07:30:53 - train: epoch 0038, iter [02600, 10009], lr: 0.010000, loss: 1.6577
2022-03-06 07:31:26 - train: epoch 0038, iter [02700, 10009], lr: 0.010000, loss: 1.6677
2022-03-06 07:31:59 - train: epoch 0038, iter [02800, 10009], lr: 0.010000, loss: 1.8656
2022-03-06 07:32:31 - train: epoch 0038, iter [02900, 10009], lr: 0.010000, loss: 1.5694
2022-03-06 07:33:04 - train: epoch 0038, iter [03000, 10009], lr: 0.010000, loss: 2.1921
2022-03-06 07:33:37 - train: epoch 0038, iter [03100, 10009], lr: 0.010000, loss: 1.7148
2022-03-06 07:34:10 - train: epoch 0038, iter [03200, 10009], lr: 0.010000, loss: 1.6169
2022-03-06 07:34:43 - train: epoch 0038, iter [03300, 10009], lr: 0.010000, loss: 1.5498
2022-03-06 07:35:15 - train: epoch 0038, iter [03400, 10009], lr: 0.010000, loss: 1.9392
2022-03-06 07:35:48 - train: epoch 0038, iter [03500, 10009], lr: 0.010000, loss: 2.2664
2022-03-06 07:36:21 - train: epoch 0038, iter [03600, 10009], lr: 0.010000, loss: 1.8969
2022-03-06 07:36:54 - train: epoch 0038, iter [03700, 10009], lr: 0.010000, loss: 1.4865
2022-03-06 07:37:27 - train: epoch 0038, iter [03800, 10009], lr: 0.010000, loss: 1.7693
2022-03-06 07:38:00 - train: epoch 0038, iter [03900, 10009], lr: 0.010000, loss: 2.0184
2022-03-06 07:38:32 - train: epoch 0038, iter [04000, 10009], lr: 0.010000, loss: 1.4458
2022-03-06 07:39:05 - train: epoch 0038, iter [04100, 10009], lr: 0.010000, loss: 1.7557
2022-03-06 07:39:38 - train: epoch 0038, iter [04200, 10009], lr: 0.010000, loss: 1.6716
2022-03-06 07:40:11 - train: epoch 0038, iter [04300, 10009], lr: 0.010000, loss: 1.6371
2022-03-06 07:40:44 - train: epoch 0038, iter [04400, 10009], lr: 0.010000, loss: 1.5950
2022-03-06 07:41:17 - train: epoch 0038, iter [04500, 10009], lr: 0.010000, loss: 1.8723
2022-03-06 07:41:50 - train: epoch 0038, iter [04600, 10009], lr: 0.010000, loss: 1.7871
2022-03-06 07:42:22 - train: epoch 0038, iter [04700, 10009], lr: 0.010000, loss: 1.9132
2022-03-06 07:42:55 - train: epoch 0038, iter [04800, 10009], lr: 0.010000, loss: 1.5813
2022-03-06 07:43:28 - train: epoch 0038, iter [04900, 10009], lr: 0.010000, loss: 1.7787
2022-03-06 07:44:01 - train: epoch 0038, iter [05000, 10009], lr: 0.010000, loss: 1.7261
2022-03-06 07:44:34 - train: epoch 0038, iter [05100, 10009], lr: 0.010000, loss: 1.8591
2022-03-06 07:45:06 - train: epoch 0038, iter [05200, 10009], lr: 0.010000, loss: 1.5765
2022-03-06 07:45:39 - train: epoch 0038, iter [05300, 10009], lr: 0.010000, loss: 1.9232
2022-03-06 07:46:12 - train: epoch 0038, iter [05400, 10009], lr: 0.010000, loss: 1.6111
2022-03-06 07:46:45 - train: epoch 0038, iter [05500, 10009], lr: 0.010000, loss: 1.6230
2022-03-06 07:47:18 - train: epoch 0038, iter [05600, 10009], lr: 0.010000, loss: 1.8958
2022-03-06 07:47:51 - train: epoch 0038, iter [05700, 10009], lr: 0.010000, loss: 1.9005
2022-03-06 07:48:24 - train: epoch 0038, iter [05800, 10009], lr: 0.010000, loss: 1.9173
2022-03-06 07:48:57 - train: epoch 0038, iter [05900, 10009], lr: 0.010000, loss: 1.8137
2022-03-06 07:49:29 - train: epoch 0038, iter [06000, 10009], lr: 0.010000, loss: 1.7474
2022-03-06 07:50:02 - train: epoch 0038, iter [06100, 10009], lr: 0.010000, loss: 1.4002
2022-03-06 07:50:35 - train: epoch 0038, iter [06200, 10009], lr: 0.010000, loss: 1.7962
2022-03-06 07:51:08 - train: epoch 0038, iter [06300, 10009], lr: 0.010000, loss: 1.6838
2022-03-06 07:51:41 - train: epoch 0038, iter [06400, 10009], lr: 0.010000, loss: 1.5551
2022-03-06 07:52:14 - train: epoch 0038, iter [06500, 10009], lr: 0.010000, loss: 1.7077
2022-03-06 07:52:47 - train: epoch 0038, iter [06600, 10009], lr: 0.010000, loss: 1.7623
2022-03-06 07:53:20 - train: epoch 0038, iter [06700, 10009], lr: 0.010000, loss: 1.5510
2022-03-06 07:53:53 - train: epoch 0038, iter [06800, 10009], lr: 0.010000, loss: 1.8322
2022-03-06 07:54:26 - train: epoch 0038, iter [06900, 10009], lr: 0.010000, loss: 1.7577
2022-03-06 07:54:58 - train: epoch 0038, iter [07000, 10009], lr: 0.010000, loss: 1.8923
2022-03-06 07:55:31 - train: epoch 0038, iter [07100, 10009], lr: 0.010000, loss: 1.4304
2022-03-06 07:56:04 - train: epoch 0038, iter [07200, 10009], lr: 0.010000, loss: 1.7225
2022-03-06 07:56:37 - train: epoch 0038, iter [07300, 10009], lr: 0.010000, loss: 1.8661
2022-03-06 07:57:10 - train: epoch 0038, iter [07400, 10009], lr: 0.010000, loss: 1.7517
2022-03-06 07:57:43 - train: epoch 0038, iter [07500, 10009], lr: 0.010000, loss: 1.6447
2022-03-06 07:58:16 - train: epoch 0038, iter [07600, 10009], lr: 0.010000, loss: 2.0537
2022-03-06 07:58:49 - train: epoch 0038, iter [07700, 10009], lr: 0.010000, loss: 2.1183
2022-03-06 07:59:21 - train: epoch 0038, iter [07800, 10009], lr: 0.010000, loss: 2.0023
2022-03-06 07:59:54 - train: epoch 0038, iter [07900, 10009], lr: 0.010000, loss: 1.8631
2022-03-06 08:00:27 - train: epoch 0038, iter [08000, 10009], lr: 0.010000, loss: 1.5457
2022-03-06 08:01:00 - train: epoch 0038, iter [08100, 10009], lr: 0.010000, loss: 1.8533
2022-03-06 08:01:33 - train: epoch 0038, iter [08200, 10009], lr: 0.010000, loss: 1.4513
2022-03-06 08:02:06 - train: epoch 0038, iter [08300, 10009], lr: 0.010000, loss: 1.4151
2022-03-06 08:02:39 - train: epoch 0038, iter [08400, 10009], lr: 0.010000, loss: 1.5192
2022-03-06 08:03:12 - train: epoch 0038, iter [08500, 10009], lr: 0.010000, loss: 1.5104
2022-03-06 08:03:45 - train: epoch 0038, iter [08600, 10009], lr: 0.010000, loss: 1.7500
2022-03-06 08:04:18 - train: epoch 0038, iter [08700, 10009], lr: 0.010000, loss: 1.7146
2022-03-06 08:04:50 - train: epoch 0038, iter [08800, 10009], lr: 0.010000, loss: 1.9873
2022-03-06 08:05:23 - train: epoch 0038, iter [08900, 10009], lr: 0.010000, loss: 1.6928
2022-03-06 08:05:56 - train: epoch 0038, iter [09000, 10009], lr: 0.010000, loss: 1.6635
2022-03-06 08:06:29 - train: epoch 0038, iter [09100, 10009], lr: 0.010000, loss: 1.5295
2022-03-06 08:07:02 - train: epoch 0038, iter [09200, 10009], lr: 0.010000, loss: 1.9283
2022-03-06 08:07:35 - train: epoch 0038, iter [09300, 10009], lr: 0.010000, loss: 1.9178
2022-03-06 08:08:08 - train: epoch 0038, iter [09400, 10009], lr: 0.010000, loss: 1.7421
2022-03-06 08:08:41 - train: epoch 0038, iter [09500, 10009], lr: 0.010000, loss: 1.6401
2022-03-06 08:09:14 - train: epoch 0038, iter [09600, 10009], lr: 0.010000, loss: 1.6733
2022-03-06 08:09:46 - train: epoch 0038, iter [09700, 10009], lr: 0.010000, loss: 1.5793
2022-03-06 08:10:19 - train: epoch 0038, iter [09800, 10009], lr: 0.010000, loss: 1.9208
2022-03-06 08:10:52 - train: epoch 0038, iter [09900, 10009], lr: 0.010000, loss: 1.8636
2022-03-06 08:11:25 - train: epoch 0038, iter [10000, 10009], lr: 0.010000, loss: 1.4349
2022-03-06 08:11:29 - train: epoch 038, train_loss: 1.7678
2022-03-06 08:12:44 - eval: epoch: 038, acc1: 64.852%, acc5: 86.262%, test_loss: 1.4469, per_image_load_time: 0.660ms, per_image_inference_time: 1.556ms
2022-03-06 08:12:45 - until epoch: 038, best_acc1: 65.284%
2022-03-06 08:12:45 - epoch 039 lr: 0.010000000000000002
2022-03-06 08:13:21 - train: epoch 0039, iter [00100, 10009], lr: 0.010000, loss: 2.2385
2022-03-06 08:13:54 - train: epoch 0039, iter [00200, 10009], lr: 0.010000, loss: 1.4442
2022-03-06 08:14:27 - train: epoch 0039, iter [00300, 10009], lr: 0.010000, loss: 2.0045
2022-03-06 08:14:59 - train: epoch 0039, iter [00400, 10009], lr: 0.010000, loss: 1.9590
2022-03-06 08:15:32 - train: epoch 0039, iter [00500, 10009], lr: 0.010000, loss: 1.6977
2022-03-06 08:16:05 - train: epoch 0039, iter [00600, 10009], lr: 0.010000, loss: 1.3260
2022-03-06 08:16:38 - train: epoch 0039, iter [00700, 10009], lr: 0.010000, loss: 1.7860
2022-03-06 08:17:11 - train: epoch 0039, iter [00800, 10009], lr: 0.010000, loss: 1.6339
2022-03-06 08:17:43 - train: epoch 0039, iter [00900, 10009], lr: 0.010000, loss: 1.7120
2022-03-06 08:18:16 - train: epoch 0039, iter [01000, 10009], lr: 0.010000, loss: 1.6766
2022-03-06 08:18:49 - train: epoch 0039, iter [01100, 10009], lr: 0.010000, loss: 1.7044
2022-03-06 08:19:22 - train: epoch 0039, iter [01200, 10009], lr: 0.010000, loss: 1.4925
2022-03-06 08:19:55 - train: epoch 0039, iter [01300, 10009], lr: 0.010000, loss: 1.7970
2022-03-06 08:20:28 - train: epoch 0039, iter [01400, 10009], lr: 0.010000, loss: 2.0710
2022-03-06 08:21:01 - train: epoch 0039, iter [01500, 10009], lr: 0.010000, loss: 1.7372
2022-03-06 08:21:33 - train: epoch 0039, iter [01600, 10009], lr: 0.010000, loss: 1.8245
2022-03-06 08:22:06 - train: epoch 0039, iter [01700, 10009], lr: 0.010000, loss: 1.9404
2022-03-06 08:22:39 - train: epoch 0039, iter [01800, 10009], lr: 0.010000, loss: 1.6603
2022-03-06 08:23:12 - train: epoch 0039, iter [01900, 10009], lr: 0.010000, loss: 1.7813
2022-03-06 08:23:45 - train: epoch 0039, iter [02000, 10009], lr: 0.010000, loss: 1.4738
2022-03-06 08:24:18 - train: epoch 0039, iter [02100, 10009], lr: 0.010000, loss: 2.0212
2022-03-06 08:24:51 - train: epoch 0039, iter [02200, 10009], lr: 0.010000, loss: 1.7397
2022-03-06 08:25:24 - train: epoch 0039, iter [02300, 10009], lr: 0.010000, loss: 1.8026
2022-03-06 08:25:57 - train: epoch 0039, iter [02400, 10009], lr: 0.010000, loss: 1.6674
2022-03-06 08:26:30 - train: epoch 0039, iter [02500, 10009], lr: 0.010000, loss: 1.9252
2022-03-06 08:27:03 - train: epoch 0039, iter [02600, 10009], lr: 0.010000, loss: 1.9265
2022-03-06 08:27:36 - train: epoch 0039, iter [02700, 10009], lr: 0.010000, loss: 1.9171
2022-03-06 08:28:08 - train: epoch 0039, iter [02800, 10009], lr: 0.010000, loss: 2.0118
2022-03-06 08:28:41 - train: epoch 0039, iter [02900, 10009], lr: 0.010000, loss: 2.2122
2022-03-06 08:29:14 - train: epoch 0039, iter [03000, 10009], lr: 0.010000, loss: 1.7413
2022-03-06 08:29:47 - train: epoch 0039, iter [03100, 10009], lr: 0.010000, loss: 1.7095
2022-03-06 08:30:20 - train: epoch 0039, iter [03200, 10009], lr: 0.010000, loss: 1.8481
2022-03-06 08:30:53 - train: epoch 0039, iter [03300, 10009], lr: 0.010000, loss: 1.6496
2022-03-06 08:31:26 - train: epoch 0039, iter [03400, 10009], lr: 0.010000, loss: 1.4228
2022-03-06 08:31:59 - train: epoch 0039, iter [03500, 10009], lr: 0.010000, loss: 1.9518
2022-03-06 08:32:32 - train: epoch 0039, iter [03600, 10009], lr: 0.010000, loss: 2.0526
2022-03-06 08:33:04 - train: epoch 0039, iter [03700, 10009], lr: 0.010000, loss: 1.8273
2022-03-06 08:33:37 - train: epoch 0039, iter [03800, 10009], lr: 0.010000, loss: 1.3516
2022-03-06 08:34:10 - train: epoch 0039, iter [03900, 10009], lr: 0.010000, loss: 1.7166
2022-03-06 08:34:43 - train: epoch 0039, iter [04000, 10009], lr: 0.010000, loss: 1.5981
2022-03-06 08:35:15 - train: epoch 0039, iter [04100, 10009], lr: 0.010000, loss: 2.1169
2022-03-06 08:35:48 - train: epoch 0039, iter [04200, 10009], lr: 0.010000, loss: 1.8199
2022-03-06 08:36:21 - train: epoch 0039, iter [04300, 10009], lr: 0.010000, loss: 1.6538
2022-03-06 08:36:54 - train: epoch 0039, iter [04400, 10009], lr: 0.010000, loss: 1.4804
2022-03-06 08:37:26 - train: epoch 0039, iter [04500, 10009], lr: 0.010000, loss: 1.7469
2022-03-06 08:37:59 - train: epoch 0039, iter [04600, 10009], lr: 0.010000, loss: 2.2638
2022-03-06 08:38:32 - train: epoch 0039, iter [04700, 10009], lr: 0.010000, loss: 1.4605
2022-03-06 08:39:05 - train: epoch 0039, iter [04800, 10009], lr: 0.010000, loss: 1.8857
2022-03-06 08:39:38 - train: epoch 0039, iter [04900, 10009], lr: 0.010000, loss: 1.8611
2022-03-06 08:40:11 - train: epoch 0039, iter [05000, 10009], lr: 0.010000, loss: 1.6518
2022-03-06 08:40:43 - train: epoch 0039, iter [05100, 10009], lr: 0.010000, loss: 1.5674
2022-03-06 08:41:16 - train: epoch 0039, iter [05200, 10009], lr: 0.010000, loss: 1.6731
2022-03-06 08:41:49 - train: epoch 0039, iter [05300, 10009], lr: 0.010000, loss: 1.8565
2022-03-06 08:42:22 - train: epoch 0039, iter [05400, 10009], lr: 0.010000, loss: 1.6986
2022-03-06 08:42:55 - train: epoch 0039, iter [05500, 10009], lr: 0.010000, loss: 1.4677
2022-03-06 08:43:28 - train: epoch 0039, iter [05600, 10009], lr: 0.010000, loss: 1.8702
2022-03-06 08:44:01 - train: epoch 0039, iter [05700, 10009], lr: 0.010000, loss: 1.6589
2022-03-06 08:44:33 - train: epoch 0039, iter [05800, 10009], lr: 0.010000, loss: 1.8971
2022-03-06 08:45:06 - train: epoch 0039, iter [05900, 10009], lr: 0.010000, loss: 1.5406
2022-03-06 08:45:39 - train: epoch 0039, iter [06000, 10009], lr: 0.010000, loss: 1.8186
2022-03-06 08:46:12 - train: epoch 0039, iter [06100, 10009], lr: 0.010000, loss: 1.8837
2022-03-06 08:46:45 - train: epoch 0039, iter [06200, 10009], lr: 0.010000, loss: 1.6965
2022-03-06 08:47:17 - train: epoch 0039, iter [06300, 10009], lr: 0.010000, loss: 1.8145
2022-03-06 08:47:50 - train: epoch 0039, iter [06400, 10009], lr: 0.010000, loss: 1.7235
2022-03-06 08:48:23 - train: epoch 0039, iter [06500, 10009], lr: 0.010000, loss: 1.8634
2022-03-06 08:48:56 - train: epoch 0039, iter [06600, 10009], lr: 0.010000, loss: 1.7495
2022-03-06 08:49:29 - train: epoch 0039, iter [06700, 10009], lr: 0.010000, loss: 1.9423
2022-03-06 08:50:02 - train: epoch 0039, iter [06800, 10009], lr: 0.010000, loss: 1.9246
2022-03-06 08:50:35 - train: epoch 0039, iter [06900, 10009], lr: 0.010000, loss: 1.7031
2022-03-06 08:51:07 - train: epoch 0039, iter [07000, 10009], lr: 0.010000, loss: 1.8617
2022-03-06 08:51:40 - train: epoch 0039, iter [07100, 10009], lr: 0.010000, loss: 1.7408
2022-03-06 08:52:13 - train: epoch 0039, iter [07200, 10009], lr: 0.010000, loss: 1.7784
2022-03-06 08:52:46 - train: epoch 0039, iter [07300, 10009], lr: 0.010000, loss: 2.0385
2022-03-06 08:53:19 - train: epoch 0039, iter [07400, 10009], lr: 0.010000, loss: 1.6993
2022-03-06 08:53:51 - train: epoch 0039, iter [07500, 10009], lr: 0.010000, loss: 1.5162
2022-03-06 08:54:24 - train: epoch 0039, iter [07600, 10009], lr: 0.010000, loss: 1.6467
2022-03-06 08:54:57 - train: epoch 0039, iter [07700, 10009], lr: 0.010000, loss: 1.6533
2022-03-06 08:55:30 - train: epoch 0039, iter [07800, 10009], lr: 0.010000, loss: 1.5150
2022-03-06 08:56:02 - train: epoch 0039, iter [07900, 10009], lr: 0.010000, loss: 1.9228
2022-03-06 08:56:35 - train: epoch 0039, iter [08000, 10009], lr: 0.010000, loss: 1.5889
2022-03-06 08:57:08 - train: epoch 0039, iter [08100, 10009], lr: 0.010000, loss: 1.4340
2022-03-06 08:57:41 - train: epoch 0039, iter [08200, 10009], lr: 0.010000, loss: 1.9183
2022-03-06 08:58:14 - train: epoch 0039, iter [08300, 10009], lr: 0.010000, loss: 1.8107
2022-03-06 08:58:46 - train: epoch 0039, iter [08400, 10009], lr: 0.010000, loss: 1.9459
2022-03-06 08:59:19 - train: epoch 0039, iter [08500, 10009], lr: 0.010000, loss: 1.8269
2022-03-06 08:59:52 - train: epoch 0039, iter [08600, 10009], lr: 0.010000, loss: 1.5357
2022-03-06 09:00:25 - train: epoch 0039, iter [08700, 10009], lr: 0.010000, loss: 1.6607
2022-03-06 09:00:58 - train: epoch 0039, iter [08800, 10009], lr: 0.010000, loss: 1.3907
2022-03-06 09:01:31 - train: epoch 0039, iter [08900, 10009], lr: 0.010000, loss: 1.8867
2022-03-06 09:02:04 - train: epoch 0039, iter [09000, 10009], lr: 0.010000, loss: 1.6182
2022-03-06 09:02:37 - train: epoch 0039, iter [09100, 10009], lr: 0.010000, loss: 1.6294
2022-03-06 09:03:10 - train: epoch 0039, iter [09200, 10009], lr: 0.010000, loss: 2.0381
2022-03-06 09:03:43 - train: epoch 0039, iter [09300, 10009], lr: 0.010000, loss: 1.7761
2022-03-06 09:04:16 - train: epoch 0039, iter [09400, 10009], lr: 0.010000, loss: 1.9333
2022-03-06 09:04:49 - train: epoch 0039, iter [09500, 10009], lr: 0.010000, loss: 1.8156
2022-03-06 09:05:22 - train: epoch 0039, iter [09600, 10009], lr: 0.010000, loss: 1.6770
2022-03-06 09:05:55 - train: epoch 0039, iter [09700, 10009], lr: 0.010000, loss: 2.0221
2022-03-06 09:06:27 - train: epoch 0039, iter [09800, 10009], lr: 0.010000, loss: 1.6174
2022-03-06 09:07:00 - train: epoch 0039, iter [09900, 10009], lr: 0.010000, loss: 1.7063
2022-03-06 09:07:33 - train: epoch 0039, iter [10000, 10009], lr: 0.010000, loss: 1.4620
2022-03-06 09:07:37 - train: epoch 039, train_loss: 1.7638
2022-03-06 09:08:53 - eval: epoch: 039, acc1: 64.496%, acc5: 86.168%, test_loss: 1.4504, per_image_load_time: 0.747ms, per_image_inference_time: 1.565ms
2022-03-06 09:08:54 - until epoch: 039, best_acc1: 65.284%
2022-03-06 09:08:54 - epoch 040 lr: 0.010000000000000002
2022-03-06 09:09:30 - train: epoch 0040, iter [00100, 10009], lr: 0.010000, loss: 1.7927
2022-03-06 09:10:03 - train: epoch 0040, iter [00200, 10009], lr: 0.010000, loss: 1.8277
2022-03-06 09:10:36 - train: epoch 0040, iter [00300, 10009], lr: 0.010000, loss: 1.5900
2022-03-06 09:11:09 - train: epoch 0040, iter [00400, 10009], lr: 0.010000, loss: 2.0201
2022-03-06 09:11:42 - train: epoch 0040, iter [00500, 10009], lr: 0.010000, loss: 1.6952
2022-03-06 09:12:14 - train: epoch 0040, iter [00600, 10009], lr: 0.010000, loss: 1.9289
2022-03-06 09:12:47 - train: epoch 0040, iter [00700, 10009], lr: 0.010000, loss: 1.7528
2022-03-06 09:13:20 - train: epoch 0040, iter [00800, 10009], lr: 0.010000, loss: 1.8270
2022-03-06 09:13:53 - train: epoch 0040, iter [00900, 10009], lr: 0.010000, loss: 1.9750
2022-03-06 09:14:25 - train: epoch 0040, iter [01000, 10009], lr: 0.010000, loss: 1.7256
2022-03-06 09:14:58 - train: epoch 0040, iter [01100, 10009], lr: 0.010000, loss: 1.6113
2022-03-06 09:15:31 - train: epoch 0040, iter [01200, 10009], lr: 0.010000, loss: 1.8624
2022-03-06 09:16:03 - train: epoch 0040, iter [01300, 10009], lr: 0.010000, loss: 1.9365
2022-03-06 09:16:36 - train: epoch 0040, iter [01400, 10009], lr: 0.010000, loss: 1.7535
2022-03-06 09:17:09 - train: epoch 0040, iter [01500, 10009], lr: 0.010000, loss: 1.9346
2022-03-06 09:17:41 - train: epoch 0040, iter [01600, 10009], lr: 0.010000, loss: 2.0376
2022-03-06 09:18:14 - train: epoch 0040, iter [01700, 10009], lr: 0.010000, loss: 1.5688
2022-03-06 09:18:47 - train: epoch 0040, iter [01800, 10009], lr: 0.010000, loss: 1.5700
2022-03-06 09:19:20 - train: epoch 0040, iter [01900, 10009], lr: 0.010000, loss: 1.7105
2022-03-06 09:19:52 - train: epoch 0040, iter [02000, 10009], lr: 0.010000, loss: 1.6379
2022-03-06 09:20:25 - train: epoch 0040, iter [02100, 10009], lr: 0.010000, loss: 1.7947
2022-03-06 09:20:58 - train: epoch 0040, iter [02200, 10009], lr: 0.010000, loss: 1.4903
2022-03-06 09:21:31 - train: epoch 0040, iter [02300, 10009], lr: 0.010000, loss: 1.9182
2022-03-06 09:22:03 - train: epoch 0040, iter [02400, 10009], lr: 0.010000, loss: 1.7465
2022-03-06 09:22:36 - train: epoch 0040, iter [02500, 10009], lr: 0.010000, loss: 1.7296
2022-03-06 09:23:09 - train: epoch 0040, iter [02600, 10009], lr: 0.010000, loss: 1.5361
2022-03-06 09:23:42 - train: epoch 0040, iter [02700, 10009], lr: 0.010000, loss: 1.8496
2022-03-06 09:24:14 - train: epoch 0040, iter [02800, 10009], lr: 0.010000, loss: 1.8184
2022-03-06 09:24:47 - train: epoch 0040, iter [02900, 10009], lr: 0.010000, loss: 1.7304
2022-03-06 09:25:20 - train: epoch 0040, iter [03000, 10009], lr: 0.010000, loss: 1.6537
2022-03-06 09:25:53 - train: epoch 0040, iter [03100, 10009], lr: 0.010000, loss: 1.6747
2022-03-06 09:26:25 - train: epoch 0040, iter [03200, 10009], lr: 0.010000, loss: 1.8695
2022-03-06 09:26:58 - train: epoch 0040, iter [03300, 10009], lr: 0.010000, loss: 1.7188
2022-03-06 09:27:31 - train: epoch 0040, iter [03400, 10009], lr: 0.010000, loss: 1.8042
2022-03-06 09:28:04 - train: epoch 0040, iter [03500, 10009], lr: 0.010000, loss: 1.8722
2022-03-06 09:28:36 - train: epoch 0040, iter [03600, 10009], lr: 0.010000, loss: 1.5249
2022-03-06 09:29:09 - train: epoch 0040, iter [03700, 10009], lr: 0.010000, loss: 1.9907
2022-03-06 09:29:42 - train: epoch 0040, iter [03800, 10009], lr: 0.010000, loss: 1.4546
2022-03-06 09:30:14 - train: epoch 0040, iter [03900, 10009], lr: 0.010000, loss: 1.6199
2022-03-06 09:30:47 - train: epoch 0040, iter [04000, 10009], lr: 0.010000, loss: 1.7673
2022-03-06 09:31:20 - train: epoch 0040, iter [04100, 10009], lr: 0.010000, loss: 1.9696
2022-03-06 09:31:53 - train: epoch 0040, iter [04200, 10009], lr: 0.010000, loss: 1.7660
2022-03-06 09:32:25 - train: epoch 0040, iter [04300, 10009], lr: 0.010000, loss: 1.4842
2022-03-06 09:32:58 - train: epoch 0040, iter [04400, 10009], lr: 0.010000, loss: 1.6612
2022-03-06 09:33:31 - train: epoch 0040, iter [04500, 10009], lr: 0.010000, loss: 2.0860
2022-03-06 09:34:03 - train: epoch 0040, iter [04600, 10009], lr: 0.010000, loss: 1.7812
2022-03-06 09:34:36 - train: epoch 0040, iter [04700, 10009], lr: 0.010000, loss: 2.1441
2022-03-06 09:35:09 - train: epoch 0040, iter [04800, 10009], lr: 0.010000, loss: 1.7483
2022-03-06 09:35:41 - train: epoch 0040, iter [04900, 10009], lr: 0.010000, loss: 1.8796
2022-03-06 09:36:14 - train: epoch 0040, iter [05000, 10009], lr: 0.010000, loss: 2.0749
2022-03-06 09:36:46 - train: epoch 0040, iter [05100, 10009], lr: 0.010000, loss: 1.7947
2022-03-06 09:37:19 - train: epoch 0040, iter [05200, 10009], lr: 0.010000, loss: 1.7723
2022-03-06 09:37:52 - train: epoch 0040, iter [05300, 10009], lr: 0.010000, loss: 1.6024
2022-03-06 09:38:24 - train: epoch 0040, iter [05400, 10009], lr: 0.010000, loss: 1.8593
2022-03-06 09:38:57 - train: epoch 0040, iter [05500, 10009], lr: 0.010000, loss: 1.7211
2022-03-06 09:39:29 - train: epoch 0040, iter [05600, 10009], lr: 0.010000, loss: 1.7848
2022-03-06 09:40:02 - train: epoch 0040, iter [05700, 10009], lr: 0.010000, loss: 2.2740
2022-03-06 09:40:34 - train: epoch 0040, iter [05800, 10009], lr: 0.010000, loss: 2.1015
2022-03-06 09:41:07 - train: epoch 0040, iter [05900, 10009], lr: 0.010000, loss: 1.5201
2022-03-06 09:41:40 - train: epoch 0040, iter [06000, 10009], lr: 0.010000, loss: 1.6652
2022-03-06 09:42:13 - train: epoch 0040, iter [06100, 10009], lr: 0.010000, loss: 1.8532
2022-03-06 09:42:45 - train: epoch 0040, iter [06200, 10009], lr: 0.010000, loss: 1.7327
2022-03-06 09:43:18 - train: epoch 0040, iter [06300, 10009], lr: 0.010000, loss: 1.6791
2022-03-06 09:43:51 - train: epoch 0040, iter [06400, 10009], lr: 0.010000, loss: 1.8946
2022-03-06 09:44:24 - train: epoch 0040, iter [06500, 10009], lr: 0.010000, loss: 1.3893
2022-03-06 09:44:57 - train: epoch 0040, iter [06600, 10009], lr: 0.010000, loss: 1.5896
2022-03-06 09:45:29 - train: epoch 0040, iter [06700, 10009], lr: 0.010000, loss: 1.8064
2022-03-06 09:46:02 - train: epoch 0040, iter [06800, 10009], lr: 0.010000, loss: 1.6276
2022-03-06 09:46:35 - train: epoch 0040, iter [06900, 10009], lr: 0.010000, loss: 1.8729
2022-03-06 09:47:08 - train: epoch 0040, iter [07000, 10009], lr: 0.010000, loss: 1.7389
2022-03-06 09:47:41 - train: epoch 0040, iter [07100, 10009], lr: 0.010000, loss: 1.7484
2022-03-06 09:48:14 - train: epoch 0040, iter [07200, 10009], lr: 0.010000, loss: 1.5282
2022-03-06 09:48:47 - train: epoch 0040, iter [07300, 10009], lr: 0.010000, loss: 1.9769
2022-03-06 09:49:20 - train: epoch 0040, iter [07400, 10009], lr: 0.010000, loss: 1.6616
2022-03-06 09:49:53 - train: epoch 0040, iter [07500, 10009], lr: 0.010000, loss: 1.7153
2022-03-06 09:50:26 - train: epoch 0040, iter [07600, 10009], lr: 0.010000, loss: 1.9146
2022-03-06 09:50:58 - train: epoch 0040, iter [07700, 10009], lr: 0.010000, loss: 1.5326
2022-03-06 09:51:31 - train: epoch 0040, iter [07800, 10009], lr: 0.010000, loss: 1.8170
2022-03-06 09:52:04 - train: epoch 0040, iter [07900, 10009], lr: 0.010000, loss: 1.7780
2022-03-06 09:52:37 - train: epoch 0040, iter [08000, 10009], lr: 0.010000, loss: 1.8537
2022-03-06 09:53:10 - train: epoch 0040, iter [08100, 10009], lr: 0.010000, loss: 1.5647
2022-03-06 09:53:43 - train: epoch 0040, iter [08200, 10009], lr: 0.010000, loss: 1.4257
2022-03-06 09:54:16 - train: epoch 0040, iter [08300, 10009], lr: 0.010000, loss: 1.6616
2022-03-06 09:54:49 - train: epoch 0040, iter [08400, 10009], lr: 0.010000, loss: 1.7543
2022-03-06 09:55:22 - train: epoch 0040, iter [08500, 10009], lr: 0.010000, loss: 1.3815
2022-03-06 09:55:54 - train: epoch 0040, iter [08600, 10009], lr: 0.010000, loss: 1.9771
2022-03-06 09:56:27 - train: epoch 0040, iter [08700, 10009], lr: 0.010000, loss: 1.8008
2022-03-06 09:57:00 - train: epoch 0040, iter [08800, 10009], lr: 0.010000, loss: 1.6987
2022-03-06 09:57:33 - train: epoch 0040, iter [08900, 10009], lr: 0.010000, loss: 1.8453
2022-03-06 09:58:06 - train: epoch 0040, iter [09000, 10009], lr: 0.010000, loss: 1.4644
2022-03-06 09:58:39 - train: epoch 0040, iter [09100, 10009], lr: 0.010000, loss: 1.5001
2022-03-06 09:59:11 - train: epoch 0040, iter [09200, 10009], lr: 0.010000, loss: 1.7156
2022-03-06 09:59:44 - train: epoch 0040, iter [09300, 10009], lr: 0.010000, loss: 1.5823
2022-03-06 10:00:17 - train: epoch 0040, iter [09400, 10009], lr: 0.010000, loss: 1.9475
2022-03-06 10:00:50 - train: epoch 0040, iter [09500, 10009], lr: 0.010000, loss: 1.8684
2022-03-06 10:01:22 - train: epoch 0040, iter [09600, 10009], lr: 0.010000, loss: 1.6296
2022-03-06 10:01:55 - train: epoch 0040, iter [09700, 10009], lr: 0.010000, loss: 1.3969
2022-03-06 10:02:28 - train: epoch 0040, iter [09800, 10009], lr: 0.010000, loss: 1.6707
2022-03-06 10:03:01 - train: epoch 0040, iter [09900, 10009], lr: 0.010000, loss: 1.6862
2022-03-06 10:03:34 - train: epoch 0040, iter [10000, 10009], lr: 0.010000, loss: 1.6092
2022-03-06 10:03:37 - train: epoch 040, train_loss: 1.7622
2022-03-06 10:04:53 - eval: epoch: 040, acc1: 64.874%, acc5: 86.190%, test_loss: 1.4511, per_image_load_time: 0.924ms, per_image_inference_time: 1.545ms
2022-03-06 10:04:54 - until epoch: 040, best_acc1: 65.284%
2022-03-06 10:56:07 - epoch 041 lr: 0.010000000000000002
2022-03-06 10:56:31 - train: epoch 0041, iter [00100, 10009], lr: 0.010000, loss: 2.0903
2022-03-06 10:56:50 - train: epoch 0041, iter [00200, 10009], lr: 0.010000, loss: 2.1878
2022-03-06 10:57:10 - train: epoch 0041, iter [00300, 10009], lr: 0.010000, loss: 1.7706
2022-03-06 10:57:30 - train: epoch 0041, iter [00400, 10009], lr: 0.010000, loss: 1.6855
2022-03-06 10:57:49 - train: epoch 0041, iter [00500, 10009], lr: 0.010000, loss: 1.6056
2022-03-06 10:58:09 - train: epoch 0041, iter [00600, 10009], lr: 0.010000, loss: 1.7125
2022-03-06 10:58:29 - train: epoch 0041, iter [00700, 10009], lr: 0.010000, loss: 2.1945
2022-03-06 10:58:49 - train: epoch 0041, iter [00800, 10009], lr: 0.010000, loss: 1.6443
2022-03-06 10:59:09 - train: epoch 0041, iter [00900, 10009], lr: 0.010000, loss: 1.8199
2022-03-06 10:59:29 - train: epoch 0041, iter [01000, 10009], lr: 0.010000, loss: 1.3682
2022-03-06 10:59:48 - train: epoch 0041, iter [01100, 10009], lr: 0.010000, loss: 1.7966
2022-03-06 11:00:08 - train: epoch 0041, iter [01200, 10009], lr: 0.010000, loss: 1.5851
2022-03-06 11:00:28 - train: epoch 0041, iter [01300, 10009], lr: 0.010000, loss: 1.9213
2022-03-06 11:00:48 - train: epoch 0041, iter [01400, 10009], lr: 0.010000, loss: 1.6405
2022-03-06 11:01:08 - train: epoch 0041, iter [01500, 10009], lr: 0.010000, loss: 1.6755
2022-03-06 11:01:28 - train: epoch 0041, iter [01600, 10009], lr: 0.010000, loss: 1.4417
2022-03-06 11:01:47 - train: epoch 0041, iter [01700, 10009], lr: 0.010000, loss: 1.6322
2022-03-06 11:02:07 - train: epoch 0041, iter [01800, 10009], lr: 0.010000, loss: 1.4896
2022-03-06 11:02:27 - train: epoch 0041, iter [01900, 10009], lr: 0.010000, loss: 2.0146
2022-03-06 11:02:47 - train: epoch 0041, iter [02000, 10009], lr: 0.010000, loss: 1.9838
2022-03-06 11:03:07 - train: epoch 0041, iter [02100, 10009], lr: 0.010000, loss: 2.0011
2022-03-06 11:03:26 - train: epoch 0041, iter [02200, 10009], lr: 0.010000, loss: 1.6659
2022-03-06 11:03:46 - train: epoch 0041, iter [02300, 10009], lr: 0.010000, loss: 1.8748
2022-03-06 11:04:06 - train: epoch 0041, iter [02400, 10009], lr: 0.010000, loss: 1.4357
2022-03-06 11:04:26 - train: epoch 0041, iter [02500, 10009], lr: 0.010000, loss: 1.6335
2022-03-06 11:04:45 - train: epoch 0041, iter [02600, 10009], lr: 0.010000, loss: 1.3966
2022-03-06 11:05:05 - train: epoch 0041, iter [02700, 10009], lr: 0.010000, loss: 1.9223
2022-03-06 11:05:25 - train: epoch 0041, iter [02800, 10009], lr: 0.010000, loss: 1.9143
2022-03-06 11:05:45 - train: epoch 0041, iter [02900, 10009], lr: 0.010000, loss: 1.8956
2022-03-06 11:06:05 - train: epoch 0041, iter [03000, 10009], lr: 0.010000, loss: 2.1038
2022-03-06 11:06:25 - train: epoch 0041, iter [03100, 10009], lr: 0.010000, loss: 2.0364
2022-03-06 11:06:45 - train: epoch 0041, iter [03200, 10009], lr: 0.010000, loss: 2.0934
2022-03-06 11:07:04 - train: epoch 0041, iter [03300, 10009], lr: 0.010000, loss: 1.9237
2022-03-06 11:07:24 - train: epoch 0041, iter [03400, 10009], lr: 0.010000, loss: 1.9523
2022-03-06 11:07:44 - train: epoch 0041, iter [03500, 10009], lr: 0.010000, loss: 1.9727
2022-03-06 11:08:04 - train: epoch 0041, iter [03600, 10009], lr: 0.010000, loss: 1.7063
2022-03-06 11:08:24 - train: epoch 0041, iter [03700, 10009], lr: 0.010000, loss: 1.8722
2022-03-06 11:08:44 - train: epoch 0041, iter [03800, 10009], lr: 0.010000, loss: 1.7909
2022-03-06 11:09:03 - train: epoch 0041, iter [03900, 10009], lr: 0.010000, loss: 1.9267
2022-03-06 11:09:23 - train: epoch 0041, iter [04000, 10009], lr: 0.010000, loss: 1.9344
2022-03-06 11:09:43 - train: epoch 0041, iter [04100, 10009], lr: 0.010000, loss: 1.6446
2022-03-06 11:10:03 - train: epoch 0041, iter [04200, 10009], lr: 0.010000, loss: 1.7267
2022-03-06 11:10:23 - train: epoch 0041, iter [04300, 10009], lr: 0.010000, loss: 1.7880
2022-03-06 11:10:42 - train: epoch 0041, iter [04400, 10009], lr: 0.010000, loss: 1.6358
2022-03-06 11:11:02 - train: epoch 0041, iter [04500, 10009], lr: 0.010000, loss: 1.7435
2022-03-06 11:11:22 - train: epoch 0041, iter [04600, 10009], lr: 0.010000, loss: 1.7815
2022-03-06 11:11:42 - train: epoch 0041, iter [04700, 10009], lr: 0.010000, loss: 1.9028
2022-03-06 11:12:02 - train: epoch 0041, iter [04800, 10009], lr: 0.010000, loss: 2.0665
2022-03-06 11:12:22 - train: epoch 0041, iter [04900, 10009], lr: 0.010000, loss: 1.5235
2022-03-06 11:12:42 - train: epoch 0041, iter [05000, 10009], lr: 0.010000, loss: 1.6726
2022-03-06 11:13:02 - train: epoch 0041, iter [05100, 10009], lr: 0.010000, loss: 1.5459
2022-03-06 11:13:21 - train: epoch 0041, iter [05200, 10009], lr: 0.010000, loss: 2.1687
2022-03-06 11:13:41 - train: epoch 0041, iter [05300, 10009], lr: 0.010000, loss: 1.8117
2022-03-06 11:14:01 - train: epoch 0041, iter [05400, 10009], lr: 0.010000, loss: 1.7725
2022-03-06 11:14:21 - train: epoch 0041, iter [05500, 10009], lr: 0.010000, loss: 1.8674
2022-03-06 11:14:41 - train: epoch 0041, iter [05600, 10009], lr: 0.010000, loss: 1.9131
2022-03-06 11:15:00 - train: epoch 0041, iter [05700, 10009], lr: 0.010000, loss: 1.8228
2022-03-06 11:15:20 - train: epoch 0041, iter [05800, 10009], lr: 0.010000, loss: 1.7248
2022-03-06 11:15:40 - train: epoch 0041, iter [05900, 10009], lr: 0.010000, loss: 1.9703
2022-03-06 11:16:00 - train: epoch 0041, iter [06000, 10009], lr: 0.010000, loss: 1.8574
2022-03-06 11:16:20 - train: epoch 0041, iter [06100, 10009], lr: 0.010000, loss: 1.6850
2022-03-06 11:16:39 - train: epoch 0041, iter [06200, 10009], lr: 0.010000, loss: 2.0143
2022-03-06 11:16:59 - train: epoch 0041, iter [06300, 10009], lr: 0.010000, loss: 1.5685
2022-03-06 11:17:19 - train: epoch 0041, iter [06400, 10009], lr: 0.010000, loss: 1.4899
2022-03-06 11:17:39 - train: epoch 0041, iter [06500, 10009], lr: 0.010000, loss: 1.9831
2022-03-06 11:17:59 - train: epoch 0041, iter [06600, 10009], lr: 0.010000, loss: 1.7587
2022-03-06 11:18:19 - train: epoch 0041, iter [06700, 10009], lr: 0.010000, loss: 1.8487
2022-03-06 11:18:38 - train: epoch 0041, iter [06800, 10009], lr: 0.010000, loss: 1.6973
2022-03-06 11:18:58 - train: epoch 0041, iter [06900, 10009], lr: 0.010000, loss: 1.3358
2022-03-06 11:19:18 - train: epoch 0041, iter [07000, 10009], lr: 0.010000, loss: 1.9004
2022-03-06 11:19:38 - train: epoch 0041, iter [07100, 10009], lr: 0.010000, loss: 1.9715
2022-03-06 11:19:58 - train: epoch 0041, iter [07200, 10009], lr: 0.010000, loss: 1.6842
2022-03-06 11:20:17 - train: epoch 0041, iter [07300, 10009], lr: 0.010000, loss: 1.4974
2022-03-06 11:20:37 - train: epoch 0041, iter [07400, 10009], lr: 0.010000, loss: 1.7002
2022-03-06 11:20:57 - train: epoch 0041, iter [07500, 10009], lr: 0.010000, loss: 1.8021
2022-03-06 11:21:17 - train: epoch 0041, iter [07600, 10009], lr: 0.010000, loss: 1.9885
2022-03-06 11:21:37 - train: epoch 0041, iter [07700, 10009], lr: 0.010000, loss: 1.3695
2022-03-06 11:21:57 - train: epoch 0041, iter [07800, 10009], lr: 0.010000, loss: 1.6036
2022-03-06 11:22:17 - train: epoch 0041, iter [07900, 10009], lr: 0.010000, loss: 1.9813
2022-03-06 11:22:36 - train: epoch 0041, iter [08000, 10009], lr: 0.010000, loss: 1.5935
2022-03-06 11:22:56 - train: epoch 0041, iter [08100, 10009], lr: 0.010000, loss: 1.8470
2022-03-06 11:23:16 - train: epoch 0041, iter [08200, 10009], lr: 0.010000, loss: 1.6469
2022-03-06 11:23:36 - train: epoch 0041, iter [08300, 10009], lr: 0.010000, loss: 2.0768
2022-03-06 11:23:56 - train: epoch 0041, iter [08400, 10009], lr: 0.010000, loss: 1.6903
2022-03-06 11:24:16 - train: epoch 0041, iter [08500, 10009], lr: 0.010000, loss: 1.8742
2022-03-06 11:24:36 - train: epoch 0041, iter [08600, 10009], lr: 0.010000, loss: 1.6310
2022-03-06 11:24:56 - train: epoch 0041, iter [08700, 10009], lr: 0.010000, loss: 1.5534
2022-03-06 11:25:16 - train: epoch 0041, iter [08800, 10009], lr: 0.010000, loss: 1.7050
2022-03-06 11:25:35 - train: epoch 0041, iter [08900, 10009], lr: 0.010000, loss: 1.8564
2022-03-06 11:25:55 - train: epoch 0041, iter [09000, 10009], lr: 0.010000, loss: 1.7553
2022-03-06 11:26:15 - train: epoch 0041, iter [09100, 10009], lr: 0.010000, loss: 2.0049
2022-03-06 11:26:35 - train: epoch 0041, iter [09200, 10009], lr: 0.010000, loss: 1.7726
2022-03-06 11:26:54 - train: epoch 0041, iter [09300, 10009], lr: 0.010000, loss: 1.8815
2022-03-06 11:27:14 - train: epoch 0041, iter [09400, 10009], lr: 0.010000, loss: 1.7576
2022-03-06 11:27:34 - train: epoch 0041, iter [09500, 10009], lr: 0.010000, loss: 1.6636
2022-03-06 11:27:54 - train: epoch 0041, iter [09600, 10009], lr: 0.010000, loss: 1.5939
2022-03-06 11:28:14 - train: epoch 0041, iter [09700, 10009], lr: 0.010000, loss: 2.2966
2022-03-06 11:28:34 - train: epoch 0041, iter [09800, 10009], lr: 0.010000, loss: 1.7324
2022-03-06 11:28:53 - train: epoch 0041, iter [09900, 10009], lr: 0.010000, loss: 1.6645
2022-03-06 11:29:13 - train: epoch 0041, iter [10000, 10009], lr: 0.010000, loss: 1.9552
2022-03-06 11:29:15 - train: epoch 041, train_loss: 1.7592
2022-03-06 11:30:29 - eval: epoch: 041, acc1: 65.286%, acc5: 86.468%, test_loss: 1.4349, per_image_load_time: 1.169ms, per_image_inference_time: 0.981ms
2022-03-06 11:30:30 - until epoch: 041, best_acc1: 65.286%
2022-03-06 11:30:30 - epoch 042 lr: 0.010000000000000002
2022-03-06 11:30:54 - train: epoch 0042, iter [00100, 10009], lr: 0.010000, loss: 1.7154
2022-03-06 11:31:13 - train: epoch 0042, iter [00200, 10009], lr: 0.010000, loss: 1.4627
2022-03-06 11:31:33 - train: epoch 0042, iter [00300, 10009], lr: 0.010000, loss: 1.4691
2022-03-06 11:31:53 - train: epoch 0042, iter [00400, 10009], lr: 0.010000, loss: 1.9061
2022-03-06 11:32:13 - train: epoch 0042, iter [00500, 10009], lr: 0.010000, loss: 1.7203
2022-03-06 11:32:32 - train: epoch 0042, iter [00600, 10009], lr: 0.010000, loss: 1.6100
2022-03-06 11:32:52 - train: epoch 0042, iter [00700, 10009], lr: 0.010000, loss: 1.6250
2022-03-06 11:33:12 - train: epoch 0042, iter [00800, 10009], lr: 0.010000, loss: 1.6456
2022-03-06 11:33:32 - train: epoch 0042, iter [00900, 10009], lr: 0.010000, loss: 1.8397
2022-03-06 11:33:51 - train: epoch 0042, iter [01000, 10009], lr: 0.010000, loss: 1.8696
2022-03-06 11:34:11 - train: epoch 0042, iter [01100, 10009], lr: 0.010000, loss: 1.8513
2022-03-06 11:34:31 - train: epoch 0042, iter [01200, 10009], lr: 0.010000, loss: 1.6108
2022-03-06 11:34:51 - train: epoch 0042, iter [01300, 10009], lr: 0.010000, loss: 1.4949
2022-03-06 11:35:10 - train: epoch 0042, iter [01400, 10009], lr: 0.010000, loss: 2.1488
2022-03-06 11:35:30 - train: epoch 0042, iter [01500, 10009], lr: 0.010000, loss: 1.8276
2022-03-06 11:35:50 - train: epoch 0042, iter [01600, 10009], lr: 0.010000, loss: 1.8426
2022-03-06 11:36:10 - train: epoch 0042, iter [01700, 10009], lr: 0.010000, loss: 1.7261
2022-03-06 11:36:29 - train: epoch 0042, iter [01800, 10009], lr: 0.010000, loss: 1.9918
2022-03-06 11:36:49 - train: epoch 0042, iter [01900, 10009], lr: 0.010000, loss: 1.9580
2022-03-06 11:37:09 - train: epoch 0042, iter [02000, 10009], lr: 0.010000, loss: 2.0337
2022-03-06 11:37:28 - train: epoch 0042, iter [02100, 10009], lr: 0.010000, loss: 1.8777
2022-03-06 11:37:48 - train: epoch 0042, iter [02200, 10009], lr: 0.010000, loss: 1.8594
2022-03-06 11:38:08 - train: epoch 0042, iter [02300, 10009], lr: 0.010000, loss: 1.8211
2022-03-06 11:38:28 - train: epoch 0042, iter [02400, 10009], lr: 0.010000, loss: 1.4835
2022-03-06 11:38:47 - train: epoch 0042, iter [02500, 10009], lr: 0.010000, loss: 1.7167
2022-03-06 11:39:07 - train: epoch 0042, iter [02600, 10009], lr: 0.010000, loss: 1.6233
2022-03-06 11:39:27 - train: epoch 0042, iter [02700, 10009], lr: 0.010000, loss: 1.7559
2022-03-06 11:39:47 - train: epoch 0042, iter [02800, 10009], lr: 0.010000, loss: 2.0283
2022-03-06 11:40:07 - train: epoch 0042, iter [02900, 10009], lr: 0.010000, loss: 1.8914
2022-03-06 11:40:27 - train: epoch 0042, iter [03000, 10009], lr: 0.010000, loss: 1.6488
2022-03-06 11:40:46 - train: epoch 0042, iter [03100, 10009], lr: 0.010000, loss: 1.5662
2022-03-06 11:41:06 - train: epoch 0042, iter [03200, 10009], lr: 0.010000, loss: 1.8670
2022-03-06 11:41:26 - train: epoch 0042, iter [03300, 10009], lr: 0.010000, loss: 1.7953
2022-03-06 11:41:46 - train: epoch 0042, iter [03400, 10009], lr: 0.010000, loss: 1.6821
2022-03-06 11:42:06 - train: epoch 0042, iter [03500, 10009], lr: 0.010000, loss: 1.7736
2022-03-06 11:42:26 - train: epoch 0042, iter [03600, 10009], lr: 0.010000, loss: 1.5230
2022-03-06 11:42:46 - train: epoch 0042, iter [03700, 10009], lr: 0.010000, loss: 1.5966
2022-03-06 11:43:06 - train: epoch 0042, iter [03800, 10009], lr: 0.010000, loss: 1.5301
2022-03-06 11:43:26 - train: epoch 0042, iter [03900, 10009], lr: 0.010000, loss: 1.6003
2022-03-06 11:43:46 - train: epoch 0042, iter [04000, 10009], lr: 0.010000, loss: 1.5192
2022-03-06 11:44:06 - train: epoch 0042, iter [04100, 10009], lr: 0.010000, loss: 1.3730
2022-03-06 11:44:26 - train: epoch 0042, iter [04200, 10009], lr: 0.010000, loss: 1.6654
2022-03-06 11:44:45 - train: epoch 0042, iter [04300, 10009], lr: 0.010000, loss: 1.6895
2022-03-06 11:45:05 - train: epoch 0042, iter [04400, 10009], lr: 0.010000, loss: 1.5112
2022-03-06 11:45:25 - train: epoch 0042, iter [04500, 10009], lr: 0.010000, loss: 1.6577
2022-03-06 11:45:45 - train: epoch 0042, iter [04600, 10009], lr: 0.010000, loss: 1.7662
2022-03-06 11:46:05 - train: epoch 0042, iter [04700, 10009], lr: 0.010000, loss: 1.4409
2022-03-06 11:46:25 - train: epoch 0042, iter [04800, 10009], lr: 0.010000, loss: 1.8145
2022-03-06 11:46:45 - train: epoch 0042, iter [04900, 10009], lr: 0.010000, loss: 1.7142
2022-03-06 11:47:05 - train: epoch 0042, iter [05000, 10009], lr: 0.010000, loss: 1.8441
2022-03-06 11:47:25 - train: epoch 0042, iter [05100, 10009], lr: 0.010000, loss: 2.0604
2022-03-06 11:47:45 - train: epoch 0042, iter [05200, 10009], lr: 0.010000, loss: 1.8939
2022-03-06 11:48:05 - train: epoch 0042, iter [05300, 10009], lr: 0.010000, loss: 1.8269
2022-03-06 11:48:25 - train: epoch 0042, iter [05400, 10009], lr: 0.010000, loss: 1.7926
2022-03-06 11:48:45 - train: epoch 0042, iter [05500, 10009], lr: 0.010000, loss: 2.0461
2022-03-06 11:49:04 - train: epoch 0042, iter [05600, 10009], lr: 0.010000, loss: 1.6240
2022-03-06 11:49:24 - train: epoch 0042, iter [05700, 10009], lr: 0.010000, loss: 1.3703
2022-03-06 11:49:44 - train: epoch 0042, iter [05800, 10009], lr: 0.010000, loss: 1.8815
2022-03-06 11:50:04 - train: epoch 0042, iter [05900, 10009], lr: 0.010000, loss: 1.5559
2022-03-06 11:50:24 - train: epoch 0042, iter [06000, 10009], lr: 0.010000, loss: 1.7693
2022-03-06 11:50:44 - train: epoch 0042, iter [06100, 10009], lr: 0.010000, loss: 1.7377
2022-03-06 11:51:04 - train: epoch 0042, iter [06200, 10009], lr: 0.010000, loss: 2.0084
2022-03-06 11:51:24 - train: epoch 0042, iter [06300, 10009], lr: 0.010000, loss: 1.8897
2022-03-06 11:51:44 - train: epoch 0042, iter [06400, 10009], lr: 0.010000, loss: 1.9346
2022-03-06 11:52:04 - train: epoch 0042, iter [06500, 10009], lr: 0.010000, loss: 1.9835
2022-03-06 11:52:24 - train: epoch 0042, iter [06600, 10009], lr: 0.010000, loss: 2.0190
2022-03-06 11:52:44 - train: epoch 0042, iter [06700, 10009], lr: 0.010000, loss: 1.5850
2022-03-06 11:53:04 - train: epoch 0042, iter [06800, 10009], lr: 0.010000, loss: 1.7083
2022-03-06 11:53:24 - train: epoch 0042, iter [06900, 10009], lr: 0.010000, loss: 1.7684
2022-03-06 11:53:44 - train: epoch 0042, iter [07000, 10009], lr: 0.010000, loss: 1.4971
2022-03-06 11:54:04 - train: epoch 0042, iter [07100, 10009], lr: 0.010000, loss: 2.0125
2022-03-06 11:54:23 - train: epoch 0042, iter [07200, 10009], lr: 0.010000, loss: 2.1279
2022-03-06 11:54:43 - train: epoch 0042, iter [07300, 10009], lr: 0.010000, loss: 1.5569
2022-03-06 11:55:03 - train: epoch 0042, iter [07400, 10009], lr: 0.010000, loss: 1.6991
2022-03-06 11:55:23 - train: epoch 0042, iter [07500, 10009], lr: 0.010000, loss: 1.7025
2022-03-06 11:55:43 - train: epoch 0042, iter [07600, 10009], lr: 0.010000, loss: 1.3887
2022-03-06 11:56:03 - train: epoch 0042, iter [07700, 10009], lr: 0.010000, loss: 1.6773
2022-03-06 11:56:23 - train: epoch 0042, iter [07800, 10009], lr: 0.010000, loss: 1.7108
2022-03-06 11:56:43 - train: epoch 0042, iter [07900, 10009], lr: 0.010000, loss: 1.6290
2022-03-06 11:57:03 - train: epoch 0042, iter [08000, 10009], lr: 0.010000, loss: 1.7543
2022-03-06 11:57:23 - train: epoch 0042, iter [08100, 10009], lr: 0.010000, loss: 1.8353
2022-03-06 11:57:43 - train: epoch 0042, iter [08200, 10009], lr: 0.010000, loss: 2.0641
2022-03-06 11:58:03 - train: epoch 0042, iter [08300, 10009], lr: 0.010000, loss: 1.9015
2022-03-06 11:58:23 - train: epoch 0042, iter [08400, 10009], lr: 0.010000, loss: 1.7628
2022-03-06 11:58:43 - train: epoch 0042, iter [08500, 10009], lr: 0.010000, loss: 1.7351
2022-03-06 11:59:03 - train: epoch 0042, iter [08600, 10009], lr: 0.010000, loss: 1.7322
2022-03-06 11:59:23 - train: epoch 0042, iter [08700, 10009], lr: 0.010000, loss: 1.7090
2022-03-06 11:59:43 - train: epoch 0042, iter [08800, 10009], lr: 0.010000, loss: 1.6226
2022-03-06 12:00:03 - train: epoch 0042, iter [08900, 10009], lr: 0.010000, loss: 1.6218
2022-03-06 12:00:23 - train: epoch 0042, iter [09000, 10009], lr: 0.010000, loss: 1.5826
2022-03-06 12:00:43 - train: epoch 0042, iter [09100, 10009], lr: 0.010000, loss: 1.6809
2022-03-06 12:01:03 - train: epoch 0042, iter [09200, 10009], lr: 0.010000, loss: 2.0408
2022-03-06 12:01:22 - train: epoch 0042, iter [09300, 10009], lr: 0.010000, loss: 1.5405
2022-03-06 12:01:42 - train: epoch 0042, iter [09400, 10009], lr: 0.010000, loss: 1.9410
2022-03-06 12:02:02 - train: epoch 0042, iter [09500, 10009], lr: 0.010000, loss: 1.5700
2022-03-06 12:02:22 - train: epoch 0042, iter [09600, 10009], lr: 0.010000, loss: 1.7882
2022-03-06 12:02:42 - train: epoch 0042, iter [09700, 10009], lr: 0.010000, loss: 1.8893
2022-03-06 12:03:02 - train: epoch 0042, iter [09800, 10009], lr: 0.010000, loss: 2.1690
2022-03-06 12:03:22 - train: epoch 0042, iter [09900, 10009], lr: 0.010000, loss: 1.6409
2022-03-06 12:03:42 - train: epoch 0042, iter [10000, 10009], lr: 0.010000, loss: 1.8557
2022-03-06 12:03:44 - train: epoch 042, train_loss: 1.7516
2022-03-06 12:04:58 - eval: epoch: 042, acc1: 65.190%, acc5: 86.366%, test_loss: 1.4350, per_image_load_time: 1.663ms, per_image_inference_time: 0.961ms
2022-03-06 12:04:59 - until epoch: 042, best_acc1: 65.286%
2022-03-06 12:04:59 - epoch 043 lr: 0.010000000000000002
2022-03-06 12:05:23 - train: epoch 0043, iter [00100, 10009], lr: 0.010000, loss: 1.6293
2022-03-06 12:05:43 - train: epoch 0043, iter [00200, 10009], lr: 0.010000, loss: 1.8979
2022-03-06 12:06:03 - train: epoch 0043, iter [00300, 10009], lr: 0.010000, loss: 1.6654
2022-03-06 12:06:22 - train: epoch 0043, iter [00400, 10009], lr: 0.010000, loss: 1.7956
2022-03-06 12:06:42 - train: epoch 0043, iter [00500, 10009], lr: 0.010000, loss: 1.6643
2022-03-06 12:07:02 - train: epoch 0043, iter [00600, 10009], lr: 0.010000, loss: 1.4336
2022-03-06 12:07:21 - train: epoch 0043, iter [00700, 10009], lr: 0.010000, loss: 1.5037
2022-03-06 12:07:41 - train: epoch 0043, iter [00800, 10009], lr: 0.010000, loss: 1.7842
2022-03-06 12:08:01 - train: epoch 0043, iter [00900, 10009], lr: 0.010000, loss: 1.7713
2022-03-06 12:08:20 - train: epoch 0043, iter [01000, 10009], lr: 0.010000, loss: 1.4245
2022-03-06 12:08:40 - train: epoch 0043, iter [01100, 10009], lr: 0.010000, loss: 1.7613
2022-03-06 12:09:00 - train: epoch 0043, iter [01200, 10009], lr: 0.010000, loss: 1.6587
2022-03-06 12:09:19 - train: epoch 0043, iter [01300, 10009], lr: 0.010000, loss: 1.5327
2022-03-06 12:09:39 - train: epoch 0043, iter [01400, 10009], lr: 0.010000, loss: 1.6496
2022-03-06 12:09:59 - train: epoch 0043, iter [01500, 10009], lr: 0.010000, loss: 1.8648
2022-03-06 12:10:18 - train: epoch 0043, iter [01600, 10009], lr: 0.010000, loss: 2.0237
2022-03-06 12:10:38 - train: epoch 0043, iter [01700, 10009], lr: 0.010000, loss: 1.7770
2022-03-06 12:10:57 - train: epoch 0043, iter [01800, 10009], lr: 0.010000, loss: 1.5911
2022-03-06 12:11:17 - train: epoch 0043, iter [01900, 10009], lr: 0.010000, loss: 1.5200
2022-03-06 12:11:37 - train: epoch 0043, iter [02000, 10009], lr: 0.010000, loss: 1.8872
2022-03-06 12:11:56 - train: epoch 0043, iter [02100, 10009], lr: 0.010000, loss: 1.4285
2022-03-06 12:12:16 - train: epoch 0043, iter [02200, 10009], lr: 0.010000, loss: 1.8153
2022-03-06 12:12:36 - train: epoch 0043, iter [02300, 10009], lr: 0.010000, loss: 1.5221
2022-03-06 12:12:55 - train: epoch 0043, iter [02400, 10009], lr: 0.010000, loss: 1.4560
2022-03-06 12:13:15 - train: epoch 0043, iter [02500, 10009], lr: 0.010000, loss: 1.5957
2022-03-06 12:13:35 - train: epoch 0043, iter [02600, 10009], lr: 0.010000, loss: 2.0372
2022-03-06 12:13:54 - train: epoch 0043, iter [02700, 10009], lr: 0.010000, loss: 1.7104
2022-03-06 12:14:14 - train: epoch 0043, iter [02800, 10009], lr: 0.010000, loss: 1.7751
2022-03-06 12:14:33 - train: epoch 0043, iter [02900, 10009], lr: 0.010000, loss: 1.7335
2022-03-06 12:14:53 - train: epoch 0043, iter [03000, 10009], lr: 0.010000, loss: 1.5545
2022-03-06 12:15:12 - train: epoch 0043, iter [03100, 10009], lr: 0.010000, loss: 1.7817
2022-03-06 12:15:32 - train: epoch 0043, iter [03200, 10009], lr: 0.010000, loss: 1.6294
2022-03-06 12:15:52 - train: epoch 0043, iter [03300, 10009], lr: 0.010000, loss: 1.8029
2022-03-06 12:16:11 - train: epoch 0043, iter [03400, 10009], lr: 0.010000, loss: 1.9227
2022-03-06 12:16:31 - train: epoch 0043, iter [03500, 10009], lr: 0.010000, loss: 1.8109
2022-03-06 12:16:51 - train: epoch 0043, iter [03600, 10009], lr: 0.010000, loss: 2.0556
2022-03-06 12:17:11 - train: epoch 0043, iter [03700, 10009], lr: 0.010000, loss: 1.6153
2022-03-06 12:17:30 - train: epoch 0043, iter [03800, 10009], lr: 0.010000, loss: 1.8335
2022-03-06 12:17:50 - train: epoch 0043, iter [03900, 10009], lr: 0.010000, loss: 1.6359
2022-03-06 12:18:10 - train: epoch 0043, iter [04000, 10009], lr: 0.010000, loss: 1.7579
2022-03-06 12:18:30 - train: epoch 0043, iter [04100, 10009], lr: 0.010000, loss: 1.5968
2022-03-06 12:18:49 - train: epoch 0043, iter [04200, 10009], lr: 0.010000, loss: 1.9255
2022-03-06 12:19:09 - train: epoch 0043, iter [04300, 10009], lr: 0.010000, loss: 1.6312
2022-03-06 12:19:28 - train: epoch 0043, iter [04400, 10009], lr: 0.010000, loss: 1.5383
2022-03-06 12:19:48 - train: epoch 0043, iter [04500, 10009], lr: 0.010000, loss: 1.7830
2022-03-06 12:20:08 - train: epoch 0043, iter [04600, 10009], lr: 0.010000, loss: 2.0807
2022-03-06 12:20:28 - train: epoch 0043, iter [04700, 10009], lr: 0.010000, loss: 1.6038
2022-03-06 12:20:47 - train: epoch 0043, iter [04800, 10009], lr: 0.010000, loss: 1.3951
2022-03-06 12:21:07 - train: epoch 0043, iter [04900, 10009], lr: 0.010000, loss: 1.6425
2022-03-06 12:21:27 - train: epoch 0043, iter [05000, 10009], lr: 0.010000, loss: 1.7981
2022-03-06 12:21:46 - train: epoch 0043, iter [05100, 10009], lr: 0.010000, loss: 1.7190
2022-03-06 12:22:06 - train: epoch 0043, iter [05200, 10009], lr: 0.010000, loss: 1.4784
2022-03-06 12:22:26 - train: epoch 0043, iter [05300, 10009], lr: 0.010000, loss: 1.9793
2022-03-06 12:22:46 - train: epoch 0043, iter [05400, 10009], lr: 0.010000, loss: 1.7661
2022-03-06 12:23:05 - train: epoch 0043, iter [05500, 10009], lr: 0.010000, loss: 1.5594
2022-03-06 12:23:25 - train: epoch 0043, iter [05600, 10009], lr: 0.010000, loss: 1.7016
2022-03-06 12:23:45 - train: epoch 0043, iter [05700, 10009], lr: 0.010000, loss: 1.6750
2022-03-06 12:24:04 - train: epoch 0043, iter [05800, 10009], lr: 0.010000, loss: 1.9487
2022-03-06 12:24:24 - train: epoch 0043, iter [05900, 10009], lr: 0.010000, loss: 1.9952
2022-03-06 12:24:50 - train: epoch 0043, iter [06000, 10009], lr: 0.010000, loss: 1.6137
2022-03-06 12:25:09 - train: epoch 0043, iter [06100, 10009], lr: 0.010000, loss: 1.8923
2022-03-06 12:25:29 - train: epoch 0043, iter [06200, 10009], lr: 0.010000, loss: 1.7743
2022-03-06 12:25:48 - train: epoch 0043, iter [06300, 10009], lr: 0.010000, loss: 1.6121
2022-03-06 12:26:08 - train: epoch 0043, iter [06400, 10009], lr: 0.010000, loss: 1.6897
2022-03-06 12:26:28 - train: epoch 0043, iter [06500, 10009], lr: 0.010000, loss: 1.4023
2022-03-06 12:26:47 - train: epoch 0043, iter [06600, 10009], lr: 0.010000, loss: 1.9896
2022-03-06 12:27:07 - train: epoch 0043, iter [06700, 10009], lr: 0.010000, loss: 1.8087
2022-03-06 12:27:26 - train: epoch 0043, iter [06800, 10009], lr: 0.010000, loss: 1.9955
2022-03-06 12:27:46 - train: epoch 0043, iter [06900, 10009], lr: 0.010000, loss: 1.5326
2022-03-06 12:28:06 - train: epoch 0043, iter [07000, 10009], lr: 0.010000, loss: 1.4592
2022-03-06 12:28:25 - train: epoch 0043, iter [07100, 10009], lr: 0.010000, loss: 1.7118
2022-03-06 12:28:45 - train: epoch 0043, iter [07200, 10009], lr: 0.010000, loss: 1.9553
2022-03-06 12:29:04 - train: epoch 0043, iter [07300, 10009], lr: 0.010000, loss: 1.7257
2022-03-06 12:29:24 - train: epoch 0043, iter [07400, 10009], lr: 0.010000, loss: 1.7421
2022-03-06 12:29:44 - train: epoch 0043, iter [07500, 10009], lr: 0.010000, loss: 1.4397
2022-03-06 12:30:03 - train: epoch 0043, iter [07600, 10009], lr: 0.010000, loss: 1.9277
2022-03-06 12:30:23 - train: epoch 0043, iter [07700, 10009], lr: 0.010000, loss: 1.6857
2022-03-06 12:30:43 - train: epoch 0043, iter [07800, 10009], lr: 0.010000, loss: 1.4750
2022-03-06 12:31:03 - train: epoch 0043, iter [07900, 10009], lr: 0.010000, loss: 1.6230
2022-03-06 12:31:22 - train: epoch 0043, iter [08000, 10009], lr: 0.010000, loss: 2.2644
2022-03-06 12:31:42 - train: epoch 0043, iter [08100, 10009], lr: 0.010000, loss: 1.8131
2022-03-06 12:32:02 - train: epoch 0043, iter [08200, 10009], lr: 0.010000, loss: 1.7066
2022-03-06 12:32:22 - train: epoch 0043, iter [08300, 10009], lr: 0.010000, loss: 1.8714
2022-03-06 12:32:41 - train: epoch 0043, iter [08400, 10009], lr: 0.010000, loss: 1.6809
2022-03-06 12:33:01 - train: epoch 0043, iter [08500, 10009], lr: 0.010000, loss: 2.0313
2022-03-06 12:33:21 - train: epoch 0043, iter [08600, 10009], lr: 0.010000, loss: 1.6796
2022-03-06 12:33:41 - train: epoch 0043, iter [08700, 10009], lr: 0.010000, loss: 2.0103
2022-03-06 12:34:00 - train: epoch 0043, iter [08800, 10009], lr: 0.010000, loss: 1.7060
2022-03-06 12:34:20 - train: epoch 0043, iter [08900, 10009], lr: 0.010000, loss: 1.6038
2022-03-06 12:34:40 - train: epoch 0043, iter [09000, 10009], lr: 0.010000, loss: 1.8382
2022-03-06 12:35:00 - train: epoch 0043, iter [09100, 10009], lr: 0.010000, loss: 1.8835
2022-03-06 12:35:19 - train: epoch 0043, iter [09200, 10009], lr: 0.010000, loss: 1.7457
2022-03-06 12:35:39 - train: epoch 0043, iter [09300, 10009], lr: 0.010000, loss: 1.6189
2022-03-06 12:35:59 - train: epoch 0043, iter [09400, 10009], lr: 0.010000, loss: 1.9599
2022-03-06 12:36:19 - train: epoch 0043, iter [09500, 10009], lr: 0.010000, loss: 2.1354
2022-03-06 12:36:38 - train: epoch 0043, iter [09600, 10009], lr: 0.010000, loss: 1.4978
2022-03-06 12:36:58 - train: epoch 0043, iter [09700, 10009], lr: 0.010000, loss: 1.8169
2022-03-06 12:37:18 - train: epoch 0043, iter [09800, 10009], lr: 0.010000, loss: 1.7812
2022-03-06 12:37:38 - train: epoch 0043, iter [09900, 10009], lr: 0.010000, loss: 1.7692
2022-03-06 12:37:58 - train: epoch 0043, iter [10000, 10009], lr: 0.010000, loss: 1.6196
2022-03-06 12:38:00 - train: epoch 043, train_loss: 1.7479
2022-03-06 12:39:14 - eval: epoch: 043, acc1: 65.078%, acc5: 86.614%, test_loss: 1.4257, per_image_load_time: 1.912ms, per_image_inference_time: 0.897ms
2022-03-06 12:39:15 - until epoch: 043, best_acc1: 65.286%
2022-03-06 12:39:15 - epoch 044 lr: 0.010000000000000002
2022-03-06 12:39:39 - train: epoch 0044, iter [00100, 10009], lr: 0.010000, loss: 1.6325
2022-03-06 12:39:59 - train: epoch 0044, iter [00200, 10009], lr: 0.010000, loss: 1.8069
2022-03-06 12:40:18 - train: epoch 0044, iter [00300, 10009], lr: 0.010000, loss: 1.3908
2022-03-06 12:40:38 - train: epoch 0044, iter [00400, 10009], lr: 0.010000, loss: 1.9458
2022-03-06 12:40:58 - train: epoch 0044, iter [00500, 10009], lr: 0.010000, loss: 1.7860
2022-03-06 12:41:17 - train: epoch 0044, iter [00600, 10009], lr: 0.010000, loss: 1.7387
2022-03-06 12:41:37 - train: epoch 0044, iter [00700, 10009], lr: 0.010000, loss: 1.4844
2022-03-06 12:41:57 - train: epoch 0044, iter [00800, 10009], lr: 0.010000, loss: 1.7255
2022-03-06 12:42:17 - train: epoch 0044, iter [00900, 10009], lr: 0.010000, loss: 1.6232
2022-03-06 12:42:36 - train: epoch 0044, iter [01000, 10009], lr: 0.010000, loss: 1.7470
2022-03-06 12:42:56 - train: epoch 0044, iter [01100, 10009], lr: 0.010000, loss: 1.8798
2022-03-06 12:43:16 - train: epoch 0044, iter [01200, 10009], lr: 0.010000, loss: 1.3650
2022-03-06 12:43:35 - train: epoch 0044, iter [01300, 10009], lr: 0.010000, loss: 1.8203
2022-03-06 12:43:55 - train: epoch 0044, iter [01400, 10009], lr: 0.010000, loss: 1.6901
2022-03-06 12:44:15 - train: epoch 0044, iter [01500, 10009], lr: 0.010000, loss: 1.7354
2022-03-06 12:44:35 - train: epoch 0044, iter [01600, 10009], lr: 0.010000, loss: 1.7156
2022-03-06 12:44:55 - train: epoch 0044, iter [01700, 10009], lr: 0.010000, loss: 1.9014
2022-03-06 12:45:14 - train: epoch 0044, iter [01800, 10009], lr: 0.010000, loss: 1.6492
2022-03-06 12:45:34 - train: epoch 0044, iter [01900, 10009], lr: 0.010000, loss: 1.4849
2022-03-06 12:45:54 - train: epoch 0044, iter [02000, 10009], lr: 0.010000, loss: 1.6280
2022-03-06 12:46:13 - train: epoch 0044, iter [02100, 10009], lr: 0.010000, loss: 1.7114
2022-03-06 12:46:33 - train: epoch 0044, iter [02200, 10009], lr: 0.010000, loss: 1.5335
2022-03-06 12:46:53 - train: epoch 0044, iter [02300, 10009], lr: 0.010000, loss: 2.1840
2022-03-06 12:47:13 - train: epoch 0044, iter [02400, 10009], lr: 0.010000, loss: 1.8039
2022-03-06 12:47:32 - train: epoch 0044, iter [02500, 10009], lr: 0.010000, loss: 1.6283
2022-03-06 12:47:52 - train: epoch 0044, iter [02600, 10009], lr: 0.010000, loss: 1.3845
2022-03-06 12:48:12 - train: epoch 0044, iter [02700, 10009], lr: 0.010000, loss: 2.0246
2022-03-06 12:48:31 - train: epoch 0044, iter [02800, 10009], lr: 0.010000, loss: 1.3427
2022-03-06 12:48:51 - train: epoch 0044, iter [02900, 10009], lr: 0.010000, loss: 1.6653
2022-03-06 12:49:11 - train: epoch 0044, iter [03000, 10009], lr: 0.010000, loss: 1.9901
2022-03-06 12:49:31 - train: epoch 0044, iter [03100, 10009], lr: 0.010000, loss: 1.6840
2022-03-06 12:49:51 - train: epoch 0044, iter [03200, 10009], lr: 0.010000, loss: 1.7604
2022-03-06 12:50:10 - train: epoch 0044, iter [03300, 10009], lr: 0.010000, loss: 1.5693
2022-03-06 12:50:30 - train: epoch 0044, iter [03400, 10009], lr: 0.010000, loss: 1.7061
2022-03-06 12:50:50 - train: epoch 0044, iter [03500, 10009], lr: 0.010000, loss: 2.1444
2022-03-06 12:51:10 - train: epoch 0044, iter [03600, 10009], lr: 0.010000, loss: 1.9704
2022-03-06 12:51:29 - train: epoch 0044, iter [03700, 10009], lr: 0.010000, loss: 1.5809
2022-03-06 12:51:49 - train: epoch 0044, iter [03800, 10009], lr: 0.010000, loss: 1.7549
2022-03-06 12:52:09 - train: epoch 0044, iter [03900, 10009], lr: 0.010000, loss: 1.9347
2022-03-06 12:52:29 - train: epoch 0044, iter [04000, 10009], lr: 0.010000, loss: 1.5920
2022-03-06 12:52:49 - train: epoch 0044, iter [04100, 10009], lr: 0.010000, loss: 1.6527
2022-03-06 12:53:09 - train: epoch 0044, iter [04200, 10009], lr: 0.010000, loss: 1.9111
2022-03-06 12:53:29 - train: epoch 0044, iter [04300, 10009], lr: 0.010000, loss: 2.0501
2022-03-06 12:53:49 - train: epoch 0044, iter [04400, 10009], lr: 0.010000, loss: 1.7031
2022-03-06 12:54:09 - train: epoch 0044, iter [04500, 10009], lr: 0.010000, loss: 1.6092
2022-03-06 12:54:28 - train: epoch 0044, iter [04600, 10009], lr: 0.010000, loss: 2.0519
2022-03-06 12:54:48 - train: epoch 0044, iter [04700, 10009], lr: 0.010000, loss: 1.6591
2022-03-06 12:55:08 - train: epoch 0044, iter [04800, 10009], lr: 0.010000, loss: 1.7310
2022-03-06 12:55:28 - train: epoch 0044, iter [04900, 10009], lr: 0.010000, loss: 2.0769
2022-03-06 12:55:48 - train: epoch 0044, iter [05000, 10009], lr: 0.010000, loss: 1.5902
2022-03-06 12:56:08 - train: epoch 0044, iter [05100, 10009], lr: 0.010000, loss: 1.7040
2022-03-06 12:56:28 - train: epoch 0044, iter [05200, 10009], lr: 0.010000, loss: 1.7205
2022-03-06 12:56:48 - train: epoch 0044, iter [05300, 10009], lr: 0.010000, loss: 1.6914
2022-03-06 12:57:08 - train: epoch 0044, iter [05400, 10009], lr: 0.010000, loss: 1.8704
2022-03-06 12:57:28 - train: epoch 0044, iter [05500, 10009], lr: 0.010000, loss: 1.6146
2022-03-06 12:57:48 - train: epoch 0044, iter [05600, 10009], lr: 0.010000, loss: 1.4980
2022-03-06 12:58:08 - train: epoch 0044, iter [05700, 10009], lr: 0.010000, loss: 1.7750
2022-03-06 12:58:28 - train: epoch 0044, iter [05800, 10009], lr: 0.010000, loss: 1.5399
2022-03-06 12:58:48 - train: epoch 0044, iter [05900, 10009], lr: 0.010000, loss: 1.8691
2022-03-06 12:59:09 - train: epoch 0044, iter [06000, 10009], lr: 0.010000, loss: 1.6388
2022-03-06 12:59:29 - train: epoch 0044, iter [06100, 10009], lr: 0.010000, loss: 1.5907
2022-03-06 12:59:49 - train: epoch 0044, iter [06200, 10009], lr: 0.010000, loss: 1.4073
2022-03-06 13:00:09 - train: epoch 0044, iter [06300, 10009], lr: 0.010000, loss: 1.7653
2022-03-06 13:00:28 - train: epoch 0044, iter [06400, 10009], lr: 0.010000, loss: 1.4993
2022-03-06 13:00:48 - train: epoch 0044, iter [06500, 10009], lr: 0.010000, loss: 1.8295
2022-03-06 13:01:08 - train: epoch 0044, iter [06600, 10009], lr: 0.010000, loss: 1.6266
2022-03-06 13:01:28 - train: epoch 0044, iter [06700, 10009], lr: 0.010000, loss: 1.9833
2022-03-06 13:01:48 - train: epoch 0044, iter [06800, 10009], lr: 0.010000, loss: 1.9733
2022-03-06 13:02:08 - train: epoch 0044, iter [06900, 10009], lr: 0.010000, loss: 2.0414
2022-03-06 13:02:28 - train: epoch 0044, iter [07000, 10009], lr: 0.010000, loss: 1.7544
2022-03-06 13:02:48 - train: epoch 0044, iter [07100, 10009], lr: 0.010000, loss: 1.7792
2022-03-06 13:03:09 - train: epoch 0044, iter [07200, 10009], lr: 0.010000, loss: 2.0042
2022-03-06 13:03:29 - train: epoch 0044, iter [07300, 10009], lr: 0.010000, loss: 1.7165
2022-03-06 13:03:49 - train: epoch 0044, iter [07400, 10009], lr: 0.010000, loss: 1.8841
2022-03-06 13:04:09 - train: epoch 0044, iter [07500, 10009], lr: 0.010000, loss: 2.0244
2022-03-06 13:04:29 - train: epoch 0044, iter [07600, 10009], lr: 0.010000, loss: 1.7261
2022-03-06 13:04:49 - train: epoch 0044, iter [07700, 10009], lr: 0.010000, loss: 1.8998
2022-03-06 13:05:09 - train: epoch 0044, iter [07800, 10009], lr: 0.010000, loss: 1.7224
2022-03-06 13:05:29 - train: epoch 0044, iter [07900, 10009], lr: 0.010000, loss: 1.6190
2022-03-06 13:05:49 - train: epoch 0044, iter [08000, 10009], lr: 0.010000, loss: 1.6160
2022-03-06 13:06:09 - train: epoch 0044, iter [08100, 10009], lr: 0.010000, loss: 1.8806
2022-03-06 13:06:29 - train: epoch 0044, iter [08200, 10009], lr: 0.010000, loss: 1.7267
2022-03-06 13:06:50 - train: epoch 0044, iter [08300, 10009], lr: 0.010000, loss: 1.6512
2022-03-06 13:07:09 - train: epoch 0044, iter [08400, 10009], lr: 0.010000, loss: 1.9202
2022-03-06 13:07:29 - train: epoch 0044, iter [08500, 10009], lr: 0.010000, loss: 1.5573
2022-03-06 13:07:49 - train: epoch 0044, iter [08600, 10009], lr: 0.010000, loss: 1.7991
2022-03-06 13:08:09 - train: epoch 0044, iter [08700, 10009], lr: 0.010000, loss: 1.4932
2022-03-06 13:08:29 - train: epoch 0044, iter [08800, 10009], lr: 0.010000, loss: 1.8075
2022-03-06 13:08:49 - train: epoch 0044, iter [08900, 10009], lr: 0.010000, loss: 1.5150
2022-03-06 13:09:09 - train: epoch 0044, iter [09000, 10009], lr: 0.010000, loss: 1.7439
2022-03-06 13:09:29 - train: epoch 0044, iter [09100, 10009], lr: 0.010000, loss: 1.8370
2022-03-06 13:09:49 - train: epoch 0044, iter [09200, 10009], lr: 0.010000, loss: 1.8006
2022-03-06 13:10:09 - train: epoch 0044, iter [09300, 10009], lr: 0.010000, loss: 1.7307
2022-03-06 13:10:29 - train: epoch 0044, iter [09400, 10009], lr: 0.010000, loss: 1.9425
2022-03-06 13:10:49 - train: epoch 0044, iter [09500, 10009], lr: 0.010000, loss: 1.3853
2022-03-06 13:11:09 - train: epoch 0044, iter [09600, 10009], lr: 0.010000, loss: 1.8255
2022-03-06 13:11:29 - train: epoch 0044, iter [09700, 10009], lr: 0.010000, loss: 1.7712
2022-03-06 13:11:49 - train: epoch 0044, iter [09800, 10009], lr: 0.010000, loss: 1.7182
2022-03-06 13:12:08 - train: epoch 0044, iter [09900, 10009], lr: 0.010000, loss: 1.7736
2022-03-06 13:12:28 - train: epoch 0044, iter [10000, 10009], lr: 0.010000, loss: 1.7477
2022-03-06 13:12:31 - train: epoch 044, train_loss: 1.7401
2022-03-06 13:13:46 - eval: epoch: 044, acc1: 65.132%, acc5: 86.556%, test_loss: 1.4309, per_image_load_time: 1.619ms, per_image_inference_time: 0.858ms
2022-03-06 13:13:47 - until epoch: 044, best_acc1: 65.286%
2022-03-06 13:13:47 - epoch 045 lr: 0.010000000000000002
2022-03-06 13:14:10 - train: epoch 0045, iter [00100, 10009], lr: 0.010000, loss: 1.7147
2022-03-06 13:14:30 - train: epoch 0045, iter [00200, 10009], lr: 0.010000, loss: 1.5957
2022-03-06 13:14:50 - train: epoch 0045, iter [00300, 10009], lr: 0.010000, loss: 1.5109
2022-03-06 13:15:10 - train: epoch 0045, iter [00400, 10009], lr: 0.010000, loss: 1.9206
2022-03-06 13:15:30 - train: epoch 0045, iter [00500, 10009], lr: 0.010000, loss: 1.5855
2022-03-06 13:15:49 - train: epoch 0045, iter [00600, 10009], lr: 0.010000, loss: 1.7288
2022-03-06 13:16:09 - train: epoch 0045, iter [00700, 10009], lr: 0.010000, loss: 1.7361
2022-03-06 13:16:29 - train: epoch 0045, iter [00800, 10009], lr: 0.010000, loss: 1.4899
2022-03-06 13:16:49 - train: epoch 0045, iter [00900, 10009], lr: 0.010000, loss: 1.5880
2022-03-06 13:17:09 - train: epoch 0045, iter [01000, 10009], lr: 0.010000, loss: 2.2386
2022-03-06 13:17:29 - train: epoch 0045, iter [01100, 10009], lr: 0.010000, loss: 1.5399
2022-03-06 13:17:49 - train: epoch 0045, iter [01200, 10009], lr: 0.010000, loss: 1.5967
2022-03-06 13:18:08 - train: epoch 0045, iter [01300, 10009], lr: 0.010000, loss: 1.4933
2022-03-06 13:18:28 - train: epoch 0045, iter [01400, 10009], lr: 0.010000, loss: 1.4129
2022-03-06 13:18:48 - train: epoch 0045, iter [01500, 10009], lr: 0.010000, loss: 1.7433
2022-03-06 13:19:08 - train: epoch 0045, iter [01600, 10009], lr: 0.010000, loss: 1.5367
2022-03-06 13:19:27 - train: epoch 0045, iter [01700, 10009], lr: 0.010000, loss: 1.6123
2022-03-06 13:19:47 - train: epoch 0045, iter [01800, 10009], lr: 0.010000, loss: 1.7807
2022-03-06 13:20:07 - train: epoch 0045, iter [01900, 10009], lr: 0.010000, loss: 1.9969
2022-03-06 13:20:27 - train: epoch 0045, iter [02000, 10009], lr: 0.010000, loss: 2.1213
2022-03-06 13:20:46 - train: epoch 0045, iter [02100, 10009], lr: 0.010000, loss: 1.6999
2022-03-06 13:21:06 - train: epoch 0045, iter [02200, 10009], lr: 0.010000, loss: 1.9458
2022-03-06 13:21:26 - train: epoch 0045, iter [02300, 10009], lr: 0.010000, loss: 1.6786
2022-03-06 13:21:46 - train: epoch 0045, iter [02400, 10009], lr: 0.010000, loss: 1.7380
2022-03-06 13:22:06 - train: epoch 0045, iter [02500, 10009], lr: 0.010000, loss: 1.7649
2022-03-06 13:22:26 - train: epoch 0045, iter [02600, 10009], lr: 0.010000, loss: 1.5878
2022-03-06 13:22:45 - train: epoch 0045, iter [02700, 10009], lr: 0.010000, loss: 1.9058
2022-03-06 13:23:05 - train: epoch 0045, iter [02800, 10009], lr: 0.010000, loss: 1.7142
2022-03-06 13:23:25 - train: epoch 0045, iter [02900, 10009], lr: 0.010000, loss: 1.7176
2022-03-06 13:23:45 - train: epoch 0045, iter [03000, 10009], lr: 0.010000, loss: 1.5397
2022-03-06 13:24:04 - train: epoch 0045, iter [03100, 10009], lr: 0.010000, loss: 2.1709
2022-03-06 13:24:24 - train: epoch 0045, iter [03200, 10009], lr: 0.010000, loss: 1.5017
2022-03-06 13:24:44 - train: epoch 0045, iter [03300, 10009], lr: 0.010000, loss: 1.7448
2022-03-06 13:25:04 - train: epoch 0045, iter [03400, 10009], lr: 0.010000, loss: 1.4792
2022-03-06 13:25:24 - train: epoch 0045, iter [03500, 10009], lr: 0.010000, loss: 1.7746
2022-03-06 13:25:43 - train: epoch 0045, iter [03600, 10009], lr: 0.010000, loss: 1.8281
2022-03-06 13:26:03 - train: epoch 0045, iter [03700, 10009], lr: 0.010000, loss: 1.6686
2022-03-06 13:26:23 - train: epoch 0045, iter [03800, 10009], lr: 0.010000, loss: 1.6707
2022-03-06 13:26:42 - train: epoch 0045, iter [03900, 10009], lr: 0.010000, loss: 1.5595
2022-03-06 13:27:04 - train: epoch 0045, iter [04000, 10009], lr: 0.010000, loss: 1.8448
2022-03-06 13:27:24 - train: epoch 0045, iter [04100, 10009], lr: 0.010000, loss: 1.5914
2022-03-06 13:27:51 - train: epoch 0045, iter [04200, 10009], lr: 0.010000, loss: 1.6780
2022-03-06 13:28:13 - train: epoch 0045, iter [04300, 10009], lr: 0.010000, loss: 2.0153
2022-03-06 13:28:37 - train: epoch 0045, iter [04400, 10009], lr: 0.010000, loss: 1.9762
2022-03-06 13:28:57 - train: epoch 0045, iter [04500, 10009], lr: 0.010000, loss: 2.0536
2022-03-06 13:29:17 - train: epoch 0045, iter [04600, 10009], lr: 0.010000, loss: 1.7375
2022-03-06 13:29:37 - train: epoch 0045, iter [04700, 10009], lr: 0.010000, loss: 2.0205
2022-03-06 13:29:56 - train: epoch 0045, iter [04800, 10009], lr: 0.010000, loss: 1.6106
2022-03-06 13:30:16 - train: epoch 0045, iter [04900, 10009], lr: 0.010000, loss: 1.8361
2022-03-06 13:30:36 - train: epoch 0045, iter [05000, 10009], lr: 0.010000, loss: 1.8282
2022-03-06 13:30:56 - train: epoch 0045, iter [05100, 10009], lr: 0.010000, loss: 1.6621
2022-03-06 13:31:15 - train: epoch 0045, iter [05200, 10009], lr: 0.010000, loss: 1.6298
2022-03-06 13:31:35 - train: epoch 0045, iter [05300, 10009], lr: 0.010000, loss: 1.9096
2022-03-06 13:32:07 - train: epoch 0045, iter [05400, 10009], lr: 0.010000, loss: 1.6109
2022-03-06 13:32:31 - train: epoch 0045, iter [05500, 10009], lr: 0.010000, loss: 1.3993
2022-03-06 13:32:51 - train: epoch 0045, iter [05600, 10009], lr: 0.010000, loss: 1.6291
2022-03-06 13:33:11 - train: epoch 0045, iter [05700, 10009], lr: 0.010000, loss: 1.8787
2022-03-06 13:33:31 - train: epoch 0045, iter [05800, 10009], lr: 0.010000, loss: 1.9912
2022-03-06 13:33:50 - train: epoch 0045, iter [05900, 10009], lr: 0.010000, loss: 1.9158
2022-03-06 13:34:10 - train: epoch 0045, iter [06000, 10009], lr: 0.010000, loss: 1.8978
2022-03-06 13:34:30 - train: epoch 0045, iter [06100, 10009], lr: 0.010000, loss: 1.8793
2022-03-06 13:34:50 - train: epoch 0045, iter [06200, 10009], lr: 0.010000, loss: 1.7359
2022-03-06 13:35:09 - train: epoch 0045, iter [06300, 10009], lr: 0.010000, loss: 1.6501
2022-03-06 13:35:29 - train: epoch 0045, iter [06400, 10009], lr: 0.010000, loss: 1.8549
2022-03-06 13:35:49 - train: epoch 0045, iter [06500, 10009], lr: 0.010000, loss: 1.6865
2022-03-06 13:36:09 - train: epoch 0045, iter [06600, 10009], lr: 0.010000, loss: 1.5220
2022-03-06 13:36:28 - train: epoch 0045, iter [06700, 10009], lr: 0.010000, loss: 1.8114
2022-03-06 13:36:48 - train: epoch 0045, iter [06800, 10009], lr: 0.010000, loss: 1.5818
2022-03-06 13:37:19 - train: epoch 0045, iter [06900, 10009], lr: 0.010000, loss: 1.9652
2022-03-06 13:37:55 - train: epoch 0045, iter [07000, 10009], lr: 0.010000, loss: 1.9757
2022-03-06 13:38:26 - train: epoch 0045, iter [07100, 10009], lr: 0.010000, loss: 1.7490
2022-03-06 13:38:46 - train: epoch 0045, iter [07200, 10009], lr: 0.010000, loss: 2.1121
2022-03-06 13:39:05 - train: epoch 0045, iter [07300, 10009], lr: 0.010000, loss: 1.6163
2022-03-06 13:39:25 - train: epoch 0045, iter [07400, 10009], lr: 0.010000, loss: 1.8095
2022-03-06 13:39:45 - train: epoch 0045, iter [07500, 10009], lr: 0.010000, loss: 1.7453
2022-03-06 13:40:05 - train: epoch 0045, iter [07600, 10009], lr: 0.010000, loss: 1.7611
2022-03-06 13:40:24 - train: epoch 0045, iter [07700, 10009], lr: 0.010000, loss: 1.9325
2022-03-06 13:40:44 - train: epoch 0045, iter [07800, 10009], lr: 0.010000, loss: 1.7424
2022-03-06 13:41:04 - train: epoch 0045, iter [07900, 10009], lr: 0.010000, loss: 1.6425
2022-03-06 13:41:24 - train: epoch 0045, iter [08000, 10009], lr: 0.010000, loss: 1.5938
2022-03-06 13:41:44 - train: epoch 0045, iter [08100, 10009], lr: 0.010000, loss: 1.6335
2022-03-06 13:42:03 - train: epoch 0045, iter [08200, 10009], lr: 0.010000, loss: 1.6763
2022-03-06 13:42:23 - train: epoch 0045, iter [08300, 10009], lr: 0.010000, loss: 1.8666
2022-03-06 13:42:43 - train: epoch 0045, iter [08400, 10009], lr: 0.010000, loss: 2.1812
2022-03-06 13:43:03 - train: epoch 0045, iter [08500, 10009], lr: 0.010000, loss: 1.5402
2022-03-06 13:43:22 - train: epoch 0045, iter [08600, 10009], lr: 0.010000, loss: 1.9031
2022-03-06 13:43:42 - train: epoch 0045, iter [08700, 10009], lr: 0.010000, loss: 1.6691
2022-03-06 13:44:02 - train: epoch 0045, iter [08800, 10009], lr: 0.010000, loss: 2.2452
2022-03-06 13:44:22 - train: epoch 0045, iter [08900, 10009], lr: 0.010000, loss: 2.4015
2022-03-06 13:44:41 - train: epoch 0045, iter [09000, 10009], lr: 0.010000, loss: 1.6631
2022-03-06 13:45:01 - train: epoch 0045, iter [09100, 10009], lr: 0.010000, loss: 2.2065
2022-03-06 13:45:21 - train: epoch 0045, iter [09200, 10009], lr: 0.010000, loss: 2.0364
2022-03-06 13:45:41 - train: epoch 0045, iter [09300, 10009], lr: 0.010000, loss: 1.6321
2022-03-06 13:46:00 - train: epoch 0045, iter [09400, 10009], lr: 0.010000, loss: 1.7626
2022-03-06 13:46:20 - train: epoch 0045, iter [09500, 10009], lr: 0.010000, loss: 1.7306
2022-03-06 13:46:40 - train: epoch 0045, iter [09600, 10009], lr: 0.010000, loss: 1.4913
2022-03-06 13:47:13 - train: epoch 0045, iter [09700, 10009], lr: 0.010000, loss: 1.7442
2022-03-06 13:47:47 - train: epoch 0045, iter [09800, 10009], lr: 0.010000, loss: 2.0027
2022-03-06 13:48:06 - train: epoch 0045, iter [09900, 10009], lr: 0.010000, loss: 1.6155
2022-03-06 13:48:26 - train: epoch 0045, iter [10000, 10009], lr: 0.010000, loss: 1.5905
2022-03-06 13:48:28 - train: epoch 045, train_loss: 1.7372
2022-03-06 13:49:43 - eval: epoch: 045, acc1: 65.194%, acc5: 86.614%, test_loss: 1.4259, per_image_load_time: 1.959ms, per_image_inference_time: 0.830ms
2022-03-06 13:49:44 - until epoch: 045, best_acc1: 65.286%
2022-03-06 13:49:44 - epoch 046 lr: 0.010000000000000002
2022-03-06 13:50:08 - train: epoch 0046, iter [00100, 10009], lr: 0.010000, loss: 1.8885
2022-03-06 13:50:28 - train: epoch 0046, iter [00200, 10009], lr: 0.010000, loss: 1.5472
2022-03-06 13:50:48 - train: epoch 0046, iter [00300, 10009], lr: 0.010000, loss: 1.8534
2022-03-06 13:51:08 - train: epoch 0046, iter [00400, 10009], lr: 0.010000, loss: 1.5363
2022-03-06 13:51:27 - train: epoch 0046, iter [00500, 10009], lr: 0.010000, loss: 1.8868
2022-03-06 13:51:47 - train: epoch 0046, iter [00600, 10009], lr: 0.010000, loss: 1.6072
2022-03-06 13:52:07 - train: epoch 0046, iter [00700, 10009], lr: 0.010000, loss: 1.8651
2022-03-06 13:52:27 - train: epoch 0046, iter [00800, 10009], lr: 0.010000, loss: 1.8579
2022-03-06 13:52:47 - train: epoch 0046, iter [00900, 10009], lr: 0.010000, loss: 1.4496
2022-03-06 13:53:07 - train: epoch 0046, iter [01000, 10009], lr: 0.010000, loss: 1.7294
2022-03-06 13:53:27 - train: epoch 0046, iter [01100, 10009], lr: 0.010000, loss: 1.9710
2022-03-06 13:53:47 - train: epoch 0046, iter [01200, 10009], lr: 0.010000, loss: 1.4440
2022-03-06 13:54:07 - train: epoch 0046, iter [01300, 10009], lr: 0.010000, loss: 1.7878
2022-03-06 13:54:27 - train: epoch 0046, iter [01400, 10009], lr: 0.010000, loss: 1.4537
2022-03-06 13:54:47 - train: epoch 0046, iter [01500, 10009], lr: 0.010000, loss: 1.6841
2022-03-06 13:55:07 - train: epoch 0046, iter [01600, 10009], lr: 0.010000, loss: 1.5922
2022-03-06 13:55:27 - train: epoch 0046, iter [01700, 10009], lr: 0.010000, loss: 1.7097
2022-03-06 13:55:47 - train: epoch 0046, iter [01800, 10009], lr: 0.010000, loss: 2.0726
2022-03-06 13:56:07 - train: epoch 0046, iter [01900, 10009], lr: 0.010000, loss: 1.7307
2022-03-06 13:56:27 - train: epoch 0046, iter [02000, 10009], lr: 0.010000, loss: 1.5111
2022-03-06 13:56:47 - train: epoch 0046, iter [02100, 10009], lr: 0.010000, loss: 1.7167
2022-03-06 13:57:07 - train: epoch 0046, iter [02200, 10009], lr: 0.010000, loss: 1.8914
2022-03-06 13:57:26 - train: epoch 0046, iter [02300, 10009], lr: 0.010000, loss: 1.5947
2022-03-06 13:57:46 - train: epoch 0046, iter [02400, 10009], lr: 0.010000, loss: 1.6223
2022-03-06 13:58:06 - train: epoch 0046, iter [02500, 10009], lr: 0.010000, loss: 1.5473
2022-03-06 13:58:26 - train: epoch 0046, iter [02600, 10009], lr: 0.010000, loss: 1.7460
2022-03-06 13:58:46 - train: epoch 0046, iter [02700, 10009], lr: 0.010000, loss: 1.7822
2022-03-06 13:59:06 - train: epoch 0046, iter [02800, 10009], lr: 0.010000, loss: 1.8990
2022-03-06 13:59:26 - train: epoch 0046, iter [02900, 10009], lr: 0.010000, loss: 1.5444
2022-03-06 13:59:46 - train: epoch 0046, iter [03000, 10009], lr: 0.010000, loss: 1.8294
2022-03-06 14:00:06 - train: epoch 0046, iter [03100, 10009], lr: 0.010000, loss: 1.5048
2022-03-06 14:00:26 - train: epoch 0046, iter [03200, 10009], lr: 0.010000, loss: 1.7215
2022-03-06 14:00:46 - train: epoch 0046, iter [03300, 10009], lr: 0.010000, loss: 1.9196
2022-03-06 14:01:06 - train: epoch 0046, iter [03400, 10009], lr: 0.010000, loss: 1.8338
2022-03-06 14:01:26 - train: epoch 0046, iter [03500, 10009], lr: 0.010000, loss: 1.8950
2022-03-06 14:01:53 - train: epoch 0046, iter [03600, 10009], lr: 0.010000, loss: 1.6627
2022-03-06 14:02:13 - train: epoch 0046, iter [03700, 10009], lr: 0.010000, loss: 1.5507
2022-03-06 14:02:33 - train: epoch 0046, iter [03800, 10009], lr: 0.010000, loss: 1.5948
2022-03-06 14:02:53 - train: epoch 0046, iter [03900, 10009], lr: 0.010000, loss: 1.9558
2022-03-06 14:03:13 - train: epoch 0046, iter [04000, 10009], lr: 0.010000, loss: 1.5304
2022-03-06 14:03:33 - train: epoch 0046, iter [04100, 10009], lr: 0.010000, loss: 1.5042
2022-03-06 14:04:01 - train: epoch 0046, iter [04200, 10009], lr: 0.010000, loss: 1.8034
2022-03-06 14:04:31 - train: epoch 0046, iter [04300, 10009], lr: 0.010000, loss: 1.8327
2022-03-06 14:05:09 - train: epoch 0046, iter [04400, 10009], lr: 0.010000, loss: 1.7751
2022-03-06 14:05:46 - train: epoch 0046, iter [04500, 10009], lr: 0.010000, loss: 1.3157
2022-03-06 14:06:06 - train: epoch 0046, iter [04600, 10009], lr: 0.010000, loss: 1.9090
2022-03-06 14:06:26 - train: epoch 0046, iter [04700, 10009], lr: 0.010000, loss: 1.7942
2022-03-06 14:06:46 - train: epoch 0046, iter [04800, 10009], lr: 0.010000, loss: 1.4827
2022-03-06 14:07:06 - train: epoch 0046, iter [04900, 10009], lr: 0.010000, loss: 1.5623
2022-03-06 14:07:26 - train: epoch 0046, iter [05000, 10009], lr: 0.010000, loss: 1.6020
2022-03-06 14:07:46 - train: epoch 0046, iter [05100, 10009], lr: 0.010000, loss: 1.7527
2022-03-06 14:08:06 - train: epoch 0046, iter [05200, 10009], lr: 0.010000, loss: 1.7680
2022-03-06 14:08:25 - train: epoch 0046, iter [05300, 10009], lr: 0.010000, loss: 1.5128
2022-03-06 14:08:45 - train: epoch 0046, iter [05400, 10009], lr: 0.010000, loss: 1.6816
2022-03-06 14:09:05 - train: epoch 0046, iter [05500, 10009], lr: 0.010000, loss: 2.0895
2022-03-06 14:09:25 - train: epoch 0046, iter [05600, 10009], lr: 0.010000, loss: 1.6286
2022-03-06 14:09:45 - train: epoch 0046, iter [05700, 10009], lr: 0.010000, loss: 1.7215
2022-03-06 14:10:05 - train: epoch 0046, iter [05800, 10009], lr: 0.010000, loss: 1.7338
2022-03-06 14:10:25 - train: epoch 0046, iter [05900, 10009], lr: 0.010000, loss: 1.9206
2022-03-06 14:10:45 - train: epoch 0046, iter [06000, 10009], lr: 0.010000, loss: 1.4983
2022-03-06 14:11:05 - train: epoch 0046, iter [06100, 10009], lr: 0.010000, loss: 1.6346
2022-03-06 14:11:25 - train: epoch 0046, iter [06200, 10009], lr: 0.010000, loss: 1.6006
2022-03-06 14:11:45 - train: epoch 0046, iter [06300, 10009], lr: 0.010000, loss: 1.7248
2022-03-06 14:12:05 - train: epoch 0046, iter [06400, 10009], lr: 0.010000, loss: 2.1224
2022-03-06 14:12:25 - train: epoch 0046, iter [06500, 10009], lr: 0.010000, loss: 1.6272
2022-03-06 14:12:45 - train: epoch 0046, iter [06600, 10009], lr: 0.010000, loss: 1.8419
2022-03-06 14:13:05 - train: epoch 0046, iter [06700, 10009], lr: 0.010000, loss: 1.5884
2022-03-06 14:13:25 - train: epoch 0046, iter [06800, 10009], lr: 0.010000, loss: 1.8189
2022-03-06 14:13:45 - train: epoch 0046, iter [06900, 10009], lr: 0.010000, loss: 1.9794
2022-03-06 14:14:05 - train: epoch 0046, iter [07000, 10009], lr: 0.010000, loss: 1.7612
2022-03-06 14:14:25 - train: epoch 0046, iter [07100, 10009], lr: 0.010000, loss: 1.6513
2022-03-06 14:14:45 - train: epoch 0046, iter [07200, 10009], lr: 0.010000, loss: 1.7540
2022-03-06 14:15:05 - train: epoch 0046, iter [07300, 10009], lr: 0.010000, loss: 2.0121
2022-03-06 14:15:25 - train: epoch 0046, iter [07400, 10009], lr: 0.010000, loss: 1.6209
2022-03-06 14:15:45 - train: epoch 0046, iter [07500, 10009], lr: 0.010000, loss: 1.7986
2022-03-06 14:16:05 - train: epoch 0046, iter [07600, 10009], lr: 0.010000, loss: 1.7609
2022-03-06 14:16:25 - train: epoch 0046, iter [07700, 10009], lr: 0.010000, loss: 1.3933
2022-03-06 14:16:45 - train: epoch 0046, iter [07800, 10009], lr: 0.010000, loss: 1.8594
2022-03-06 14:17:05 - train: epoch 0046, iter [07900, 10009], lr: 0.010000, loss: 1.4695
2022-03-06 14:17:25 - train: epoch 0046, iter [08000, 10009], lr: 0.010000, loss: 1.8480
2022-03-06 14:17:45 - train: epoch 0046, iter [08100, 10009], lr: 0.010000, loss: 1.7921
2022-03-06 14:18:05 - train: epoch 0046, iter [08200, 10009], lr: 0.010000, loss: 1.7992
2022-03-06 14:18:30 - train: epoch 0046, iter [08300, 10009], lr: 0.010000, loss: 2.0251
2022-03-06 14:18:50 - train: epoch 0046, iter [08400, 10009], lr: 0.010000, loss: 1.6408
2022-03-06 14:19:10 - train: epoch 0046, iter [08500, 10009], lr: 0.010000, loss: 1.8510
2022-03-06 14:19:30 - train: epoch 0046, iter [08600, 10009], lr: 0.010000, loss: 1.9197
2022-03-06 14:19:50 - train: epoch 0046, iter [08700, 10009], lr: 0.010000, loss: 2.2229
2022-03-06 14:20:10 - train: epoch 0046, iter [08800, 10009], lr: 0.010000, loss: 1.8214
2022-03-06 14:20:30 - train: epoch 0046, iter [08900, 10009], lr: 0.010000, loss: 1.8258
2022-03-06 14:20:50 - train: epoch 0046, iter [09000, 10009], lr: 0.010000, loss: 1.7635
2022-03-06 14:21:10 - train: epoch 0046, iter [09100, 10009], lr: 0.010000, loss: 1.6940
2022-03-06 14:21:38 - train: epoch 0046, iter [09200, 10009], lr: 0.010000, loss: 1.8324
2022-03-06 14:22:17 - train: epoch 0046, iter [09300, 10009], lr: 0.010000, loss: 1.9684
2022-03-06 14:22:44 - train: epoch 0046, iter [09400, 10009], lr: 0.010000, loss: 1.6855
2022-03-06 14:23:04 - train: epoch 0046, iter [09500, 10009], lr: 0.010000, loss: 1.7169
2022-03-06 14:23:24 - train: epoch 0046, iter [09600, 10009], lr: 0.010000, loss: 1.1997
2022-03-06 14:23:44 - train: epoch 0046, iter [09700, 10009], lr: 0.010000, loss: 1.6148
2022-03-06 14:24:04 - train: epoch 0046, iter [09800, 10009], lr: 0.010000, loss: 2.0639
2022-03-06 14:24:24 - train: epoch 0046, iter [09900, 10009], lr: 0.010000, loss: 1.5346
2022-03-06 14:24:44 - train: epoch 0046, iter [10000, 10009], lr: 0.010000, loss: 1.9277
2022-03-06 14:24:46 - train: epoch 046, train_loss: 1.7288
2022-03-06 14:26:00 - eval: epoch: 046, acc1: 65.436%, acc5: 86.544%, test_loss: 1.4251, per_image_load_time: 1.023ms, per_image_inference_time: 0.803ms
2022-03-06 14:26:01 - until epoch: 046, best_acc1: 65.436%
2022-03-06 14:26:01 - epoch 047 lr: 0.010000000000000002
2022-03-06 14:26:24 - train: epoch 0047, iter [00100, 10009], lr: 0.010000, loss: 1.7033
2022-03-06 14:26:44 - train: epoch 0047, iter [00200, 10009], lr: 0.010000, loss: 1.5301
2022-03-06 14:27:04 - train: epoch 0047, iter [00300, 10009], lr: 0.010000, loss: 1.8430
2022-03-06 14:27:24 - train: epoch 0047, iter [00400, 10009], lr: 0.010000, loss: 2.0748
2022-03-06 14:27:43 - train: epoch 0047, iter [00500, 10009], lr: 0.010000, loss: 1.7865
2022-03-06 14:28:03 - train: epoch 0047, iter [00600, 10009], lr: 0.010000, loss: 1.6099
2022-03-06 14:28:23 - train: epoch 0047, iter [00700, 10009], lr: 0.010000, loss: 1.4739
2022-03-06 14:28:42 - train: epoch 0047, iter [00800, 10009], lr: 0.010000, loss: 1.3895
2022-03-06 14:29:02 - train: epoch 0047, iter [00900, 10009], lr: 0.010000, loss: 1.8184
2022-03-06 14:29:22 - train: epoch 0047, iter [01000, 10009], lr: 0.010000, loss: 1.9932
2022-03-06 14:29:42 - train: epoch 0047, iter [01100, 10009], lr: 0.010000, loss: 1.7137
2022-03-06 14:30:01 - train: epoch 0047, iter [01200, 10009], lr: 0.010000, loss: 1.9576
2022-03-06 14:30:21 - train: epoch 0047, iter [01300, 10009], lr: 0.010000, loss: 1.8278
2022-03-06 14:30:41 - train: epoch 0047, iter [01400, 10009], lr: 0.010000, loss: 1.8375
2022-03-06 14:31:01 - train: epoch 0047, iter [01500, 10009], lr: 0.010000, loss: 1.6729
2022-03-06 14:31:20 - train: epoch 0047, iter [01600, 10009], lr: 0.010000, loss: 1.9669
2022-03-06 14:31:40 - train: epoch 0047, iter [01700, 10009], lr: 0.010000, loss: 1.5457
2022-03-06 14:32:00 - train: epoch 0047, iter [01800, 10009], lr: 0.010000, loss: 1.8813
2022-03-06 14:32:20 - train: epoch 0047, iter [01900, 10009], lr: 0.010000, loss: 1.8935
2022-03-06 14:32:39 - train: epoch 0047, iter [02000, 10009], lr: 0.010000, loss: 1.8913
2022-03-06 14:32:59 - train: epoch 0047, iter [02100, 10009], lr: 0.010000, loss: 1.8662
2022-03-06 14:33:19 - train: epoch 0047, iter [02200, 10009], lr: 0.010000, loss: 1.7923
2022-03-06 14:33:39 - train: epoch 0047, iter [02300, 10009], lr: 0.010000, loss: 1.8122
2022-03-06 14:33:58 - train: epoch 0047, iter [02400, 10009], lr: 0.010000, loss: 1.6056
2022-03-06 14:34:18 - train: epoch 0047, iter [02500, 10009], lr: 0.010000, loss: 1.6270
2022-03-06 14:34:38 - train: epoch 0047, iter [02600, 10009], lr: 0.010000, loss: 1.7532
2022-03-06 14:34:58 - train: epoch 0047, iter [02700, 10009], lr: 0.010000, loss: 2.2233
2022-03-06 14:35:17 - train: epoch 0047, iter [02800, 10009], lr: 0.010000, loss: 1.7453
2022-03-06 14:35:37 - train: epoch 0047, iter [02900, 10009], lr: 0.010000, loss: 1.4849
2022-03-06 14:35:57 - train: epoch 0047, iter [03000, 10009], lr: 0.010000, loss: 1.4914
2022-03-06 14:36:17 - train: epoch 0047, iter [03100, 10009], lr: 0.010000, loss: 2.0192
2022-03-06 14:36:36 - train: epoch 0047, iter [03200, 10009], lr: 0.010000, loss: 1.7645
2022-03-06 14:36:56 - train: epoch 0047, iter [03300, 10009], lr: 0.010000, loss: 1.6418
2022-03-06 14:37:22 - train: epoch 0047, iter [03400, 10009], lr: 0.010000, loss: 1.6867
2022-03-06 14:37:42 - train: epoch 0047, iter [03500, 10009], lr: 0.010000, loss: 1.9068
2022-03-06 14:38:02 - train: epoch 0047, iter [03600, 10009], lr: 0.010000, loss: 1.7090
2022-03-06 14:38:22 - train: epoch 0047, iter [03700, 10009], lr: 0.010000, loss: 1.8253
2022-03-06 14:38:41 - train: epoch 0047, iter [03800, 10009], lr: 0.010000, loss: 1.6017
2022-03-06 14:39:01 - train: epoch 0047, iter [03900, 10009], lr: 0.010000, loss: 1.8371
2022-03-06 14:39:37 - train: epoch 0047, iter [04000, 10009], lr: 0.010000, loss: 1.5122
2022-03-06 14:40:02 - train: epoch 0047, iter [04100, 10009], lr: 0.010000, loss: 1.8374
2022-03-06 14:40:22 - train: epoch 0047, iter [04200, 10009], lr: 0.010000, loss: 1.9290
2022-03-06 14:40:42 - train: epoch 0047, iter [04300, 10009], lr: 0.010000, loss: 1.6064
2022-03-06 14:41:02 - train: epoch 0047, iter [04400, 10009], lr: 0.010000, loss: 1.8144
2022-03-06 14:41:22 - train: epoch 0047, iter [04500, 10009], lr: 0.010000, loss: 1.6026
2022-03-06 14:41:42 - train: epoch 0047, iter [04600, 10009], lr: 0.010000, loss: 2.0760
2022-03-06 14:42:02 - train: epoch 0047, iter [04700, 10009], lr: 0.010000, loss: 1.4845
2022-03-06 14:42:22 - train: epoch 0047, iter [04800, 10009], lr: 0.010000, loss: 1.5354
2022-03-06 14:42:41 - train: epoch 0047, iter [04900, 10009], lr: 0.010000, loss: 1.2405
2022-03-06 14:43:01 - train: epoch 0047, iter [05000, 10009], lr: 0.010000, loss: 1.7826
2022-03-06 14:43:21 - train: epoch 0047, iter [05100, 10009], lr: 0.010000, loss: 1.7899
2022-03-06 14:43:41 - train: epoch 0047, iter [05200, 10009], lr: 0.010000, loss: 2.1804
2022-03-06 14:44:01 - train: epoch 0047, iter [05300, 10009], lr: 0.010000, loss: 1.6663
2022-03-06 14:44:21 - train: epoch 0047, iter [05400, 10009], lr: 0.010000, loss: 1.5375
2022-03-06 14:44:40 - train: epoch 0047, iter [05500, 10009], lr: 0.010000, loss: 2.1022
2022-03-06 14:45:00 - train: epoch 0047, iter [05600, 10009], lr: 0.010000, loss: 1.5717
2022-03-06 14:45:20 - train: epoch 0047, iter [05700, 10009], lr: 0.010000, loss: 1.8068
2022-03-06 14:45:40 - train: epoch 0047, iter [05800, 10009], lr: 0.010000, loss: 1.8724
2022-03-06 14:45:59 - train: epoch 0047, iter [05900, 10009], lr: 0.010000, loss: 1.5285
2022-03-06 14:46:19 - train: epoch 0047, iter [06000, 10009], lr: 0.010000, loss: 1.9234
2022-03-06 14:46:39 - train: epoch 0047, iter [06100, 10009], lr: 0.010000, loss: 1.7986
2022-03-06 14:46:58 - train: epoch 0047, iter [06200, 10009], lr: 0.010000, loss: 1.5486
2022-03-06 14:47:18 - train: epoch 0047, iter [06300, 10009], lr: 0.010000, loss: 2.0416
2022-03-06 14:47:38 - train: epoch 0047, iter [06400, 10009], lr: 0.010000, loss: 1.6913
2022-03-06 14:47:58 - train: epoch 0047, iter [06500, 10009], lr: 0.010000, loss: 1.9749
2022-03-06 14:48:18 - train: epoch 0047, iter [06600, 10009], lr: 0.010000, loss: 1.8248
2022-03-06 14:48:37 - train: epoch 0047, iter [06700, 10009], lr: 0.010000, loss: 1.6065
2022-03-06 14:48:57 - train: epoch 0047, iter [06800, 10009], lr: 0.010000, loss: 1.7873
2022-03-06 14:49:17 - train: epoch 0047, iter [06900, 10009], lr: 0.010000, loss: 1.5551
2022-03-06 14:49:37 - train: epoch 0047, iter [07000, 10009], lr: 0.010000, loss: 2.0749
2022-03-06 14:49:56 - train: epoch 0047, iter [07100, 10009], lr: 0.010000, loss: 2.1131
2022-03-06 14:50:16 - train: epoch 0047, iter [07200, 10009], lr: 0.010000, loss: 1.7934
2022-03-06 14:50:36 - train: epoch 0047, iter [07300, 10009], lr: 0.010000, loss: 1.7334
2022-03-06 14:50:56 - train: epoch 0047, iter [07400, 10009], lr: 0.010000, loss: 1.7907
2022-03-06 14:51:16 - train: epoch 0047, iter [07500, 10009], lr: 0.010000, loss: 1.7357
2022-03-06 14:51:36 - train: epoch 0047, iter [07600, 10009], lr: 0.010000, loss: 1.8557
2022-03-06 14:51:56 - train: epoch 0047, iter [07700, 10009], lr: 0.010000, loss: 1.9915
2022-03-06 14:52:16 - train: epoch 0047, iter [07800, 10009], lr: 0.010000, loss: 1.7634
2022-03-06 14:52:35 - train: epoch 0047, iter [07900, 10009], lr: 0.010000, loss: 1.7050
2022-03-06 14:52:55 - train: epoch 0047, iter [08000, 10009], lr: 0.010000, loss: 1.8542
2022-03-06 14:53:15 - train: epoch 0047, iter [08100, 10009], lr: 0.010000, loss: 1.6851
2022-03-06 14:53:35 - train: epoch 0047, iter [08200, 10009], lr: 0.010000, loss: 1.6799
2022-03-06 14:53:55 - train: epoch 0047, iter [08300, 10009], lr: 0.010000, loss: 1.8075
2022-03-06 14:54:15 - train: epoch 0047, iter [08400, 10009], lr: 0.010000, loss: 1.7159
2022-03-06 14:54:34 - train: epoch 0047, iter [08500, 10009], lr: 0.010000, loss: 1.8182
2022-03-06 14:54:54 - train: epoch 0047, iter [08600, 10009], lr: 0.010000, loss: 1.7194
2022-03-06 14:55:14 - train: epoch 0047, iter [08700, 10009], lr: 0.010000, loss: 1.9286
2022-03-06 14:55:34 - train: epoch 0047, iter [08800, 10009], lr: 0.010000, loss: 1.6168
2022-03-06 14:55:54 - train: epoch 0047, iter [08900, 10009], lr: 0.010000, loss: 1.8719
2022-03-06 14:56:14 - train: epoch 0047, iter [09000, 10009], lr: 0.010000, loss: 1.7510
2022-03-06 14:56:34 - train: epoch 0047, iter [09100, 10009], lr: 0.010000, loss: 2.0050
2022-03-06 14:56:53 - train: epoch 0047, iter [09200, 10009], lr: 0.010000, loss: 1.8266
2022-03-06 14:57:13 - train: epoch 0047, iter [09300, 10009], lr: 0.010000, loss: 1.8115
2022-03-06 14:57:33 - train: epoch 0047, iter [09400, 10009], lr: 0.010000, loss: 2.2363
2022-03-06 14:57:53 - train: epoch 0047, iter [09500, 10009], lr: 0.010000, loss: 1.6725
2022-03-06 14:58:13 - train: epoch 0047, iter [09600, 10009], lr: 0.010000, loss: 1.6004
2022-03-06 14:58:33 - train: epoch 0047, iter [09700, 10009], lr: 0.010000, loss: 1.6980
2022-03-06 14:58:53 - train: epoch 0047, iter [09800, 10009], lr: 0.010000, loss: 1.9423
2022-03-06 14:59:13 - train: epoch 0047, iter [09900, 10009], lr: 0.010000, loss: 2.1766
2022-03-06 14:59:33 - train: epoch 0047, iter [10000, 10009], lr: 0.010000, loss: 1.7241
2022-03-06 14:59:35 - train: epoch 047, train_loss: 1.7242
2022-03-06 15:00:49 - eval: epoch: 047, acc1: 65.596%, acc5: 86.800%, test_loss: 1.4164, per_image_load_time: 1.789ms, per_image_inference_time: 0.853ms
2022-03-06 15:00:50 - until epoch: 047, best_acc1: 65.596%
2022-03-06 15:00:50 - epoch 048 lr: 0.010000000000000002
2022-03-06 15:01:14 - train: epoch 0048, iter [00100, 10009], lr: 0.010000, loss: 1.5045
2022-03-06 15:01:34 - train: epoch 0048, iter [00200, 10009], lr: 0.010000, loss: 1.6728
2022-03-06 15:01:54 - train: epoch 0048, iter [00300, 10009], lr: 0.010000, loss: 1.3785
2022-03-06 15:02:14 - train: epoch 0048, iter [00400, 10009], lr: 0.010000, loss: 1.9660
2022-03-06 15:02:34 - train: epoch 0048, iter [00500, 10009], lr: 0.010000, loss: 1.9414
2022-03-06 15:02:54 - train: epoch 0048, iter [00600, 10009], lr: 0.010000, loss: 1.7741
2022-03-06 15:03:14 - train: epoch 0048, iter [00700, 10009], lr: 0.010000, loss: 1.8193
2022-03-06 15:03:34 - train: epoch 0048, iter [00800, 10009], lr: 0.010000, loss: 1.7371
2022-03-06 15:03:54 - train: epoch 0048, iter [00900, 10009], lr: 0.010000, loss: 1.6967
2022-03-06 15:04:14 - train: epoch 0048, iter [01000, 10009], lr: 0.010000, loss: 1.8085
2022-03-06 15:04:34 - train: epoch 0048, iter [01100, 10009], lr: 0.010000, loss: 1.9290
2022-03-06 15:04:54 - train: epoch 0048, iter [01200, 10009], lr: 0.010000, loss: 2.0438
2022-03-06 15:05:14 - train: epoch 0048, iter [01300, 10009], lr: 0.010000, loss: 1.5221
2022-03-06 15:05:34 - train: epoch 0048, iter [01400, 10009], lr: 0.010000, loss: 1.4846
2022-03-06 15:05:54 - train: epoch 0048, iter [01500, 10009], lr: 0.010000, loss: 1.6967
2022-03-06 15:06:15 - train: epoch 0048, iter [01600, 10009], lr: 0.010000, loss: 1.6303
2022-03-06 15:06:35 - train: epoch 0048, iter [01700, 10009], lr: 0.010000, loss: 1.7832
2022-03-06 15:06:55 - train: epoch 0048, iter [01800, 10009], lr: 0.010000, loss: 1.8937
2022-03-06 15:07:15 - train: epoch 0048, iter [01900, 10009], lr: 0.010000, loss: 1.6662
2022-03-06 15:07:35 - train: epoch 0048, iter [02000, 10009], lr: 0.010000, loss: 2.0709
2022-03-06 15:07:55 - train: epoch 0048, iter [02100, 10009], lr: 0.010000, loss: 1.7264
2022-03-06 15:08:15 - train: epoch 0048, iter [02200, 10009], lr: 0.010000, loss: 1.8886
2022-03-06 15:08:34 - train: epoch 0048, iter [02300, 10009], lr: 0.010000, loss: 1.7626
2022-03-06 15:08:55 - train: epoch 0048, iter [02400, 10009], lr: 0.010000, loss: 1.7306
2022-03-06 15:09:15 - train: epoch 0048, iter [02500, 10009], lr: 0.010000, loss: 1.6838
2022-03-06 15:09:35 - train: epoch 0048, iter [02600, 10009], lr: 0.010000, loss: 1.7161
2022-03-06 15:09:55 - train: epoch 0048, iter [02700, 10009], lr: 0.010000, loss: 1.6231
2022-03-06 15:10:15 - train: epoch 0048, iter [02800, 10009], lr: 0.010000, loss: 1.5900
2022-03-06 15:10:34 - train: epoch 0048, iter [02900, 10009], lr: 0.010000, loss: 1.8221
2022-03-06 15:10:54 - train: epoch 0048, iter [03000, 10009], lr: 0.010000, loss: 1.8954
2022-03-06 15:11:14 - train: epoch 0048, iter [03100, 10009], lr: 0.010000, loss: 1.4236
2022-03-06 15:11:34 - train: epoch 0048, iter [03200, 10009], lr: 0.010000, loss: 1.8043
2022-03-06 15:11:54 - train: epoch 0048, iter [03300, 10009], lr: 0.010000, loss: 1.6641
2022-03-06 15:12:14 - train: epoch 0048, iter [03400, 10009], lr: 0.010000, loss: 1.7984
2022-03-06 15:12:34 - train: epoch 0048, iter [03500, 10009], lr: 0.010000, loss: 1.9280
2022-03-06 15:12:54 - train: epoch 0048, iter [03600, 10009], lr: 0.010000, loss: 1.4494
2022-03-06 15:13:14 - train: epoch 0048, iter [03700, 10009], lr: 0.010000, loss: 1.4526
2022-03-06 15:13:34 - train: epoch 0048, iter [03800, 10009], lr: 0.010000, loss: 1.4411
2022-03-06 15:13:54 - train: epoch 0048, iter [03900, 10009], lr: 0.010000, loss: 1.7944
2022-03-06 15:14:14 - train: epoch 0048, iter [04000, 10009], lr: 0.010000, loss: 1.9490
2022-03-06 15:14:34 - train: epoch 0048, iter [04100, 10009], lr: 0.010000, loss: 1.7119
2022-03-06 15:14:54 - train: epoch 0048, iter [04200, 10009], lr: 0.010000, loss: 1.4775
2022-03-06 15:15:14 - train: epoch 0048, iter [04300, 10009], lr: 0.010000, loss: 1.3561
2022-03-06 15:15:34 - train: epoch 0048, iter [04400, 10009], lr: 0.010000, loss: 1.9595
2022-03-06 15:15:54 - train: epoch 0048, iter [04500, 10009], lr: 0.010000, loss: 1.6922
2022-03-06 15:16:14 - train: epoch 0048, iter [04600, 10009], lr: 0.010000, loss: 1.4944
2022-03-06 15:16:34 - train: epoch 0048, iter [04700, 10009], lr: 0.010000, loss: 1.5691
2022-03-06 15:16:54 - train: epoch 0048, iter [04800, 10009], lr: 0.010000, loss: 1.9726
2022-03-06 15:17:15 - train: epoch 0048, iter [04900, 10009], lr: 0.010000, loss: 1.6840
2022-03-06 15:17:35 - train: epoch 0048, iter [05000, 10009], lr: 0.010000, loss: 2.0248
2022-03-06 15:17:55 - train: epoch 0048, iter [05100, 10009], lr: 0.010000, loss: 1.7788
2022-03-06 15:18:15 - train: epoch 0048, iter [05200, 10009], lr: 0.010000, loss: 1.6012
2022-03-06 15:18:35 - train: epoch 0048, iter [05300, 10009], lr: 0.010000, loss: 1.5205
2022-03-06 15:18:55 - train: epoch 0048, iter [05400, 10009], lr: 0.010000, loss: 2.2121
2022-03-06 15:19:15 - train: epoch 0048, iter [05500, 10009], lr: 0.010000, loss: 1.8459
2022-03-06 15:19:35 - train: epoch 0048, iter [05600, 10009], lr: 0.010000, loss: 1.5131
2022-03-06 15:19:55 - train: epoch 0048, iter [05700, 10009], lr: 0.010000, loss: 1.7003
2022-03-06 15:20:15 - train: epoch 0048, iter [05800, 10009], lr: 0.010000, loss: 1.9297
2022-03-06 15:20:35 - train: epoch 0048, iter [05900, 10009], lr: 0.010000, loss: 1.4336
2022-03-06 15:20:55 - train: epoch 0048, iter [06000, 10009], lr: 0.010000, loss: 1.5778
2022-03-06 15:21:15 - train: epoch 0048, iter [06100, 10009], lr: 0.010000, loss: 1.6363
2022-03-06 15:21:35 - train: epoch 0048, iter [06200, 10009], lr: 0.010000, loss: 1.4031
2022-03-06 15:21:55 - train: epoch 0048, iter [06300, 10009], lr: 0.010000, loss: 1.7452
2022-03-06 15:22:15 - train: epoch 0048, iter [06400, 10009], lr: 0.010000, loss: 1.2961
2022-03-06 15:22:35 - train: epoch 0048, iter [06500, 10009], lr: 0.010000, loss: 2.0279
2022-03-06 15:22:55 - train: epoch 0048, iter [06600, 10009], lr: 0.010000, loss: 1.8577
2022-03-06 15:23:15 - train: epoch 0048, iter [06700, 10009], lr: 0.010000, loss: 1.8654
2022-03-06 15:23:35 - train: epoch 0048, iter [06800, 10009], lr: 0.010000, loss: 2.1172
2022-03-06 15:23:55 - train: epoch 0048, iter [06900, 10009], lr: 0.010000, loss: 1.7472
2022-03-06 15:24:15 - train: epoch 0048, iter [07000, 10009], lr: 0.010000, loss: 1.6256
2022-03-06 15:24:35 - train: epoch 0048, iter [07100, 10009], lr: 0.010000, loss: 1.6886
2022-03-06 15:24:55 - train: epoch 0048, iter [07200, 10009], lr: 0.010000, loss: 1.9440
2022-03-06 15:25:15 - train: epoch 0048, iter [07300, 10009], lr: 0.010000, loss: 1.7957
2022-03-06 15:25:35 - train: epoch 0048, iter [07400, 10009], lr: 0.010000, loss: 1.8897
2022-03-06 15:25:55 - train: epoch 0048, iter [07500, 10009], lr: 0.010000, loss: 1.5656
2022-03-06 15:26:14 - train: epoch 0048, iter [07600, 10009], lr: 0.010000, loss: 1.6193
2022-03-06 15:26:35 - train: epoch 0048, iter [07700, 10009], lr: 0.010000, loss: 1.9808
2022-03-06 15:26:55 - train: epoch 0048, iter [07800, 10009], lr: 0.010000, loss: 1.9032
2022-03-06 15:27:15 - train: epoch 0048, iter [07900, 10009], lr: 0.010000, loss: 2.1433
2022-03-06 15:27:35 - train: epoch 0048, iter [08000, 10009], lr: 0.010000, loss: 2.0443
2022-03-06 15:27:55 - train: epoch 0048, iter [08100, 10009], lr: 0.010000, loss: 1.5821
2022-03-06 15:28:15 - train: epoch 0048, iter [08200, 10009], lr: 0.010000, loss: 1.7216
2022-03-06 15:28:35 - train: epoch 0048, iter [08300, 10009], lr: 0.010000, loss: 1.8333
2022-03-06 15:28:55 - train: epoch 0048, iter [08400, 10009], lr: 0.010000, loss: 1.6586
2022-03-06 15:29:15 - train: epoch 0048, iter [08500, 10009], lr: 0.010000, loss: 1.6509
2022-03-06 15:29:35 - train: epoch 0048, iter [08600, 10009], lr: 0.010000, loss: 1.6773
2022-03-06 15:29:55 - train: epoch 0048, iter [08700, 10009], lr: 0.010000, loss: 1.6569
2022-03-06 15:30:15 - train: epoch 0048, iter [08800, 10009], lr: 0.010000, loss: 1.8137
2022-03-06 15:30:35 - train: epoch 0048, iter [08900, 10009], lr: 0.010000, loss: 1.8334
2022-03-06 15:30:55 - train: epoch 0048, iter [09000, 10009], lr: 0.010000, loss: 1.6962
2022-03-06 15:31:15 - train: epoch 0048, iter [09100, 10009], lr: 0.010000, loss: 1.4315
2022-03-06 15:31:35 - train: epoch 0048, iter [09200, 10009], lr: 0.010000, loss: 1.8958
2022-03-06 15:31:55 - train: epoch 0048, iter [09300, 10009], lr: 0.010000, loss: 2.1835
2022-03-06 15:32:15 - train: epoch 0048, iter [09400, 10009], lr: 0.010000, loss: 1.7072
2022-03-06 15:32:35 - train: epoch 0048, iter [09500, 10009], lr: 0.010000, loss: 1.7245
2022-03-06 15:32:55 - train: epoch 0048, iter [09600, 10009], lr: 0.010000, loss: 2.0247
2022-03-06 15:33:15 - train: epoch 0048, iter [09700, 10009], lr: 0.010000, loss: 1.8663
2022-03-06 15:33:35 - train: epoch 0048, iter [09800, 10009], lr: 0.010000, loss: 1.7020
2022-03-06 15:33:55 - train: epoch 0048, iter [09900, 10009], lr: 0.010000, loss: 1.8330
2022-03-06 15:34:15 - train: epoch 0048, iter [10000, 10009], lr: 0.010000, loss: 1.7273
2022-03-06 15:34:17 - train: epoch 048, train_loss: 1.7208
2022-03-06 15:35:33 - eval: epoch: 048, acc1: 65.664%, acc5: 86.818%, test_loss: 1.4057, per_image_load_time: 2.091ms, per_image_inference_time: 0.804ms
2022-03-06 15:35:34 - until epoch: 048, best_acc1: 65.664%
2022-03-06 15:35:34 - epoch 049 lr: 0.010000000000000002
2022-03-06 15:35:57 - train: epoch 0049, iter [00100, 10009], lr: 0.010000, loss: 1.3569
2022-03-06 15:36:16 - train: epoch 0049, iter [00200, 10009], lr: 0.010000, loss: 1.7875
2022-03-06 15:36:36 - train: epoch 0049, iter [00300, 10009], lr: 0.010000, loss: 1.6562
2022-03-06 15:36:55 - train: epoch 0049, iter [00400, 10009], lr: 0.010000, loss: 1.3321
2022-03-06 15:37:15 - train: epoch 0049, iter [00500, 10009], lr: 0.010000, loss: 1.5192
2022-03-06 15:37:34 - train: epoch 0049, iter [00600, 10009], lr: 0.010000, loss: 1.9121
2022-03-06 15:37:54 - train: epoch 0049, iter [00700, 10009], lr: 0.010000, loss: 1.7542
2022-03-06 15:38:13 - train: epoch 0049, iter [00800, 10009], lr: 0.010000, loss: 1.6982
2022-03-06 15:38:33 - train: epoch 0049, iter [00900, 10009], lr: 0.010000, loss: 2.1202
2022-03-06 15:38:53 - train: epoch 0049, iter [01000, 10009], lr: 0.010000, loss: 1.6323
2022-03-06 15:39:12 - train: epoch 0049, iter [01100, 10009], lr: 0.010000, loss: 1.3976
2022-03-06 15:39:32 - train: epoch 0049, iter [01200, 10009], lr: 0.010000, loss: 1.3784
2022-03-06 15:39:52 - train: epoch 0049, iter [01300, 10009], lr: 0.010000, loss: 1.6643
2022-03-06 15:40:11 - train: epoch 0049, iter [01400, 10009], lr: 0.010000, loss: 1.6736
2022-03-06 15:40:31 - train: epoch 0049, iter [01500, 10009], lr: 0.010000, loss: 1.6869
2022-03-06 15:40:51 - train: epoch 0049, iter [01600, 10009], lr: 0.010000, loss: 1.8919
2022-03-06 15:41:10 - train: epoch 0049, iter [01700, 10009], lr: 0.010000, loss: 1.6958
2022-03-06 15:41:30 - train: epoch 0049, iter [01800, 10009], lr: 0.010000, loss: 1.6216
2022-03-06 15:41:50 - train: epoch 0049, iter [01900, 10009], lr: 0.010000, loss: 1.8304
2022-03-06 15:42:09 - train: epoch 0049, iter [02000, 10009], lr: 0.010000, loss: 1.4512
2022-03-06 15:42:29 - train: epoch 0049, iter [02100, 10009], lr: 0.010000, loss: 1.6306
2022-03-06 15:42:49 - train: epoch 0049, iter [02200, 10009], lr: 0.010000, loss: 1.4109
2022-03-06 15:43:08 - train: epoch 0049, iter [02300, 10009], lr: 0.010000, loss: 1.9066
2022-03-06 15:43:28 - train: epoch 0049, iter [02400, 10009], lr: 0.010000, loss: 1.6338
2022-03-06 15:43:48 - train: epoch 0049, iter [02500, 10009], lr: 0.010000, loss: 1.9859
2022-03-06 15:44:08 - train: epoch 0049, iter [02600, 10009], lr: 0.010000, loss: 1.8330
2022-03-06 15:44:27 - train: epoch 0049, iter [02700, 10009], lr: 0.010000, loss: 1.7019
2022-03-06 15:44:47 - train: epoch 0049, iter [02800, 10009], lr: 0.010000, loss: 1.8753
2022-03-06 15:45:07 - train: epoch 0049, iter [02900, 10009], lr: 0.010000, loss: 1.6041
2022-03-06 15:45:27 - train: epoch 0049, iter [03000, 10009], lr: 0.010000, loss: 1.6777
2022-03-06 15:45:46 - train: epoch 0049, iter [03100, 10009], lr: 0.010000, loss: 1.7343
2022-03-06 15:46:06 - train: epoch 0049, iter [03200, 10009], lr: 0.010000, loss: 1.4993
2022-03-06 15:46:26 - train: epoch 0049, iter [03300, 10009], lr: 0.010000, loss: 1.8705
2022-03-06 15:46:46 - train: epoch 0049, iter [03400, 10009], lr: 0.010000, loss: 1.6443
2022-03-06 15:47:06 - train: epoch 0049, iter [03500, 10009], lr: 0.010000, loss: 1.6617
2022-03-06 15:47:25 - train: epoch 0049, iter [03600, 10009], lr: 0.010000, loss: 1.6602
2022-03-06 15:47:45 - train: epoch 0049, iter [03700, 10009], lr: 0.010000, loss: 1.5688
2022-03-06 15:48:05 - train: epoch 0049, iter [03800, 10009], lr: 0.010000, loss: 1.8574
2022-03-06 15:48:25 - train: epoch 0049, iter [03900, 10009], lr: 0.010000, loss: 1.6303
2022-03-06 15:48:45 - train: epoch 0049, iter [04000, 10009], lr: 0.010000, loss: 1.7661
2022-03-06 15:49:05 - train: epoch 0049, iter [04100, 10009], lr: 0.010000, loss: 1.7593
2022-03-06 15:49:24 - train: epoch 0049, iter [04200, 10009], lr: 0.010000, loss: 1.6105
2022-03-06 15:49:44 - train: epoch 0049, iter [04300, 10009], lr: 0.010000, loss: 1.6906
2022-03-06 15:50:04 - train: epoch 0049, iter [04400, 10009], lr: 0.010000, loss: 1.8390
2022-03-06 15:50:24 - train: epoch 0049, iter [04500, 10009], lr: 0.010000, loss: 1.6869
2022-03-06 15:50:44 - train: epoch 0049, iter [04600, 10009], lr: 0.010000, loss: 1.8702
2022-03-06 15:51:04 - train: epoch 0049, iter [04700, 10009], lr: 0.010000, loss: 1.7041
2022-03-06 15:51:24 - train: epoch 0049, iter [04800, 10009], lr: 0.010000, loss: 1.7764
2022-03-06 15:51:43 - train: epoch 0049, iter [04900, 10009], lr: 0.010000, loss: 1.8660
2022-03-06 15:52:03 - train: epoch 0049, iter [05000, 10009], lr: 0.010000, loss: 1.5895
2022-03-06 15:52:23 - train: epoch 0049, iter [05100, 10009], lr: 0.010000, loss: 1.8598
2022-03-06 15:52:43 - train: epoch 0049, iter [05200, 10009], lr: 0.010000, loss: 1.5597
2022-03-06 15:53:03 - train: epoch 0049, iter [05300, 10009], lr: 0.010000, loss: 1.5542
2022-03-06 15:53:23 - train: epoch 0049, iter [05400, 10009], lr: 0.010000, loss: 1.6790
2022-03-06 15:53:42 - train: epoch 0049, iter [05500, 10009], lr: 0.010000, loss: 1.8657
2022-03-06 15:54:02 - train: epoch 0049, iter [05600, 10009], lr: 0.010000, loss: 1.7260
2022-03-06 15:54:22 - train: epoch 0049, iter [05700, 10009], lr: 0.010000, loss: 1.8917
2022-03-06 15:54:42 - train: epoch 0049, iter [05800, 10009], lr: 0.010000, loss: 1.6645
2022-03-06 15:55:02 - train: epoch 0049, iter [05900, 10009], lr: 0.010000, loss: 2.0499
2022-03-06 15:55:22 - train: epoch 0049, iter [06000, 10009], lr: 0.010000, loss: 1.7442
2022-03-06 15:55:41 - train: epoch 0049, iter [06100, 10009], lr: 0.010000, loss: 1.8037
2022-03-06 15:56:01 - train: epoch 0049, iter [06200, 10009], lr: 0.010000, loss: 1.5164
2022-03-06 15:56:21 - train: epoch 0049, iter [06300, 10009], lr: 0.010000, loss: 1.7979
2022-03-06 15:56:41 - train: epoch 0049, iter [06400, 10009], lr: 0.010000, loss: 1.7319
2022-03-06 15:57:01 - train: epoch 0049, iter [06500, 10009], lr: 0.010000, loss: 1.7516
2022-03-06 15:57:21 - train: epoch 0049, iter [06600, 10009], lr: 0.010000, loss: 1.7336
2022-03-06 15:57:40 - train: epoch 0049, iter [06700, 10009], lr: 0.010000, loss: 1.4538
2022-03-06 15:58:00 - train: epoch 0049, iter [06800, 10009], lr: 0.010000, loss: 1.9431
2022-03-06 15:58:20 - train: epoch 0049, iter [06900, 10009], lr: 0.010000, loss: 1.7826
2022-03-06 15:58:40 - train: epoch 0049, iter [07000, 10009], lr: 0.010000, loss: 1.7488
2022-03-06 15:59:00 - train: epoch 0049, iter [07100, 10009], lr: 0.010000, loss: 1.8418
2022-03-06 15:59:20 - train: epoch 0049, iter [07200, 10009], lr: 0.010000, loss: 1.8192
2022-03-06 15:59:39 - train: epoch 0049, iter [07300, 10009], lr: 0.010000, loss: 1.4308
2022-03-06 15:59:59 - train: epoch 0049, iter [07400, 10009], lr: 0.010000, loss: 1.2836
2022-03-06 16:00:19 - train: epoch 0049, iter [07500, 10009], lr: 0.010000, loss: 1.6963
2022-03-06 16:00:39 - train: epoch 0049, iter [07600, 10009], lr: 0.010000, loss: 1.7524
2022-03-06 16:00:59 - train: epoch 0049, iter [07700, 10009], lr: 0.010000, loss: 1.7311
2022-03-06 16:01:19 - train: epoch 0049, iter [07800, 10009], lr: 0.010000, loss: 1.8372
2022-03-06 16:01:38 - train: epoch 0049, iter [07900, 10009], lr: 0.010000, loss: 1.7985
2022-03-06 16:01:58 - train: epoch 0049, iter [08000, 10009], lr: 0.010000, loss: 1.5983
2022-03-06 16:02:18 - train: epoch 0049, iter [08100, 10009], lr: 0.010000, loss: 1.9024
2022-03-06 16:02:38 - train: epoch 0049, iter [08200, 10009], lr: 0.010000, loss: 1.9379
2022-03-06 16:02:58 - train: epoch 0049, iter [08300, 10009], lr: 0.010000, loss: 1.9967
2022-03-06 16:03:18 - train: epoch 0049, iter [08400, 10009], lr: 0.010000, loss: 1.7300
2022-03-06 16:03:37 - train: epoch 0049, iter [08500, 10009], lr: 0.010000, loss: 1.7133
2022-03-06 16:03:57 - train: epoch 0049, iter [08600, 10009], lr: 0.010000, loss: 1.8894
2022-03-06 16:04:17 - train: epoch 0049, iter [08700, 10009], lr: 0.010000, loss: 1.3804
2022-03-06 16:04:37 - train: epoch 0049, iter [08800, 10009], lr: 0.010000, loss: 1.6787
2022-03-06 16:04:57 - train: epoch 0049, iter [08900, 10009], lr: 0.010000, loss: 1.5346
2022-03-06 16:05:16 - train: epoch 0049, iter [09000, 10009], lr: 0.010000, loss: 1.5349
2022-03-06 16:05:36 - train: epoch 0049, iter [09100, 10009], lr: 0.010000, loss: 1.6569
2022-03-06 16:05:56 - train: epoch 0049, iter [09200, 10009], lr: 0.010000, loss: 1.6772
2022-03-06 16:06:16 - train: epoch 0049, iter [09300, 10009], lr: 0.010000, loss: 1.9273
2022-03-06 16:06:35 - train: epoch 0049, iter [09400, 10009], lr: 0.010000, loss: 2.1363
2022-03-06 16:06:55 - train: epoch 0049, iter [09500, 10009], lr: 0.010000, loss: 1.4757
2022-03-06 16:07:15 - train: epoch 0049, iter [09600, 10009], lr: 0.010000, loss: 1.7864
2022-03-06 16:07:35 - train: epoch 0049, iter [09700, 10009], lr: 0.010000, loss: 1.6957
2022-03-06 16:07:54 - train: epoch 0049, iter [09800, 10009], lr: 0.010000, loss: 1.5003
2022-03-06 16:08:14 - train: epoch 0049, iter [09900, 10009], lr: 0.010000, loss: 1.8351
2022-03-06 16:08:34 - train: epoch 0049, iter [10000, 10009], lr: 0.010000, loss: 1.8699
2022-03-06 16:08:36 - train: epoch 049, train_loss: 1.7124
2022-03-06 16:09:51 - eval: epoch: 049, acc1: 65.646%, acc5: 86.822%, test_loss: 1.4073, per_image_load_time: 2.046ms, per_image_inference_time: 0.818ms
2022-03-06 16:09:51 - until epoch: 049, best_acc1: 65.664%
2022-03-06 16:09:51 - epoch 050 lr: 0.010000000000000002
2022-03-06 16:10:15 - train: epoch 0050, iter [00100, 10009], lr: 0.010000, loss: 1.7076
2022-03-06 16:10:34 - train: epoch 0050, iter [00200, 10009], lr: 0.010000, loss: 1.5931
2022-03-06 16:10:54 - train: epoch 0050, iter [00300, 10009], lr: 0.010000, loss: 1.8351
2022-03-06 16:11:13 - train: epoch 0050, iter [00400, 10009], lr: 0.010000, loss: 1.7955
2022-03-06 16:11:33 - train: epoch 0050, iter [00500, 10009], lr: 0.010000, loss: 1.4753
2022-03-06 16:11:52 - train: epoch 0050, iter [00600, 10009], lr: 0.010000, loss: 1.7561
2022-03-06 16:12:12 - train: epoch 0050, iter [00700, 10009], lr: 0.010000, loss: 1.7328
2022-03-06 16:12:31 - train: epoch 0050, iter [00800, 10009], lr: 0.010000, loss: 1.6936
2022-03-06 16:12:51 - train: epoch 0050, iter [00900, 10009], lr: 0.010000, loss: 1.5949
2022-03-06 16:13:11 - train: epoch 0050, iter [01000, 10009], lr: 0.010000, loss: 1.4786
2022-03-06 16:13:30 - train: epoch 0050, iter [01100, 10009], lr: 0.010000, loss: 1.5594
2022-03-06 16:13:50 - train: epoch 0050, iter [01200, 10009], lr: 0.010000, loss: 1.8839
2022-03-06 16:14:10 - train: epoch 0050, iter [01300, 10009], lr: 0.010000, loss: 1.7071
2022-03-06 16:14:29 - train: epoch 0050, iter [01400, 10009], lr: 0.010000, loss: 1.5455
2022-03-06 16:14:49 - train: epoch 0050, iter [01500, 10009], lr: 0.010000, loss: 1.8719
2022-03-06 16:15:08 - train: epoch 0050, iter [01600, 10009], lr: 0.010000, loss: 1.6079
2022-03-06 16:15:28 - train: epoch 0050, iter [01700, 10009], lr: 0.010000, loss: 1.6663
2022-03-06 16:15:48 - train: epoch 0050, iter [01800, 10009], lr: 0.010000, loss: 1.7535
2022-03-06 16:16:07 - train: epoch 0050, iter [01900, 10009], lr: 0.010000, loss: 1.4865
2022-03-06 16:16:27 - train: epoch 0050, iter [02000, 10009], lr: 0.010000, loss: 1.7341
2022-03-06 16:16:47 - train: epoch 0050, iter [02100, 10009], lr: 0.010000, loss: 1.6289
2022-03-06 16:17:06 - train: epoch 0050, iter [02200, 10009], lr: 0.010000, loss: 1.7510
2022-03-06 16:17:26 - train: epoch 0050, iter [02300, 10009], lr: 0.010000, loss: 1.5438
2022-03-06 16:17:46 - train: epoch 0050, iter [02400, 10009], lr: 0.010000, loss: 1.6859
2022-03-06 16:18:05 - train: epoch 0050, iter [02500, 10009], lr: 0.010000, loss: 1.5701
2022-03-06 16:18:25 - train: epoch 0050, iter [02600, 10009], lr: 0.010000, loss: 1.7217
2022-03-06 16:18:45 - train: epoch 0050, iter [02700, 10009], lr: 0.010000, loss: 1.9687
2022-03-06 16:19:04 - train: epoch 0050, iter [02800, 10009], lr: 0.010000, loss: 1.6548
2022-03-06 16:19:24 - train: epoch 0050, iter [02900, 10009], lr: 0.010000, loss: 1.8115
2022-03-06 16:19:44 - train: epoch 0050, iter [03000, 10009], lr: 0.010000, loss: 1.7488
2022-03-06 16:20:03 - train: epoch 0050, iter [03100, 10009], lr: 0.010000, loss: 1.7243
2022-03-06 16:20:23 - train: epoch 0050, iter [03200, 10009], lr: 0.010000, loss: 1.6831
2022-03-06 16:20:42 - train: epoch 0050, iter [03300, 10009], lr: 0.010000, loss: 1.5695
2022-03-06 16:21:02 - train: epoch 0050, iter [03400, 10009], lr: 0.010000, loss: 1.4980
2022-03-06 16:21:22 - train: epoch 0050, iter [03500, 10009], lr: 0.010000, loss: 1.5393
2022-03-06 16:21:41 - train: epoch 0050, iter [03600, 10009], lr: 0.010000, loss: 1.8046
2022-03-06 16:22:01 - train: epoch 0050, iter [03700, 10009], lr: 0.010000, loss: 1.8549
2022-03-06 16:22:21 - train: epoch 0050, iter [03800, 10009], lr: 0.010000, loss: 1.5797
2022-03-06 16:22:40 - train: epoch 0050, iter [03900, 10009], lr: 0.010000, loss: 1.9082
2022-03-06 16:23:00 - train: epoch 0050, iter [04000, 10009], lr: 0.010000, loss: 1.5925
2022-03-06 16:23:20 - train: epoch 0050, iter [04100, 10009], lr: 0.010000, loss: 1.7483
2022-03-06 16:23:40 - train: epoch 0050, iter [04200, 10009], lr: 0.010000, loss: 1.6251
2022-03-06 16:24:00 - train: epoch 0050, iter [04300, 10009], lr: 0.010000, loss: 1.6038
2022-03-06 16:24:20 - train: epoch 0050, iter [04400, 10009], lr: 0.010000, loss: 1.7993
2022-03-06 16:24:40 - train: epoch 0050, iter [04500, 10009], lr: 0.010000, loss: 1.8596
2022-03-06 16:25:00 - train: epoch 0050, iter [04600, 10009], lr: 0.010000, loss: 1.4490
2022-03-06 16:25:19 - train: epoch 0050, iter [04700, 10009], lr: 0.010000, loss: 1.4171
2022-03-06 16:25:39 - train: epoch 0050, iter [04800, 10009], lr: 0.010000, loss: 1.9134
2022-03-06 16:25:59 - train: epoch 0050, iter [04900, 10009], lr: 0.010000, loss: 1.5380
2022-03-06 16:26:19 - train: epoch 0050, iter [05000, 10009], lr: 0.010000, loss: 1.6273
2022-03-06 16:26:39 - train: epoch 0050, iter [05100, 10009], lr: 0.010000, loss: 2.0096
2022-03-06 16:26:59 - train: epoch 0050, iter [05200, 10009], lr: 0.010000, loss: 1.8462
2022-03-06 16:27:19 - train: epoch 0050, iter [05300, 10009], lr: 0.010000, loss: 1.5540
2022-03-06 16:27:39 - train: epoch 0050, iter [05400, 10009], lr: 0.010000, loss: 1.5321
2022-03-06 16:27:59 - train: epoch 0050, iter [05500, 10009], lr: 0.010000, loss: 1.6431
2022-03-06 16:28:19 - train: epoch 0050, iter [05600, 10009], lr: 0.010000, loss: 1.8983
2022-03-06 16:28:39 - train: epoch 0050, iter [05700, 10009], lr: 0.010000, loss: 1.5197
2022-03-06 16:28:59 - train: epoch 0050, iter [05800, 10009], lr: 0.010000, loss: 1.8240
2022-03-06 16:29:18 - train: epoch 0050, iter [05900, 10009], lr: 0.010000, loss: 1.5464
2022-03-06 16:29:38 - train: epoch 0050, iter [06000, 10009], lr: 0.010000, loss: 1.8202
2022-03-06 16:29:58 - train: epoch 0050, iter [06100, 10009], lr: 0.010000, loss: 1.7308
2022-03-06 16:30:18 - train: epoch 0050, iter [06200, 10009], lr: 0.010000, loss: 1.7545
2022-03-06 16:30:38 - train: epoch 0050, iter [06300, 10009], lr: 0.010000, loss: 1.4282
2022-03-06 16:30:57 - train: epoch 0050, iter [06400, 10009], lr: 0.010000, loss: 1.5190
2022-03-06 16:31:17 - train: epoch 0050, iter [06500, 10009], lr: 0.010000, loss: 1.5058
2022-03-06 16:31:37 - train: epoch 0050, iter [06600, 10009], lr: 0.010000, loss: 1.4153
2022-03-06 16:31:57 - train: epoch 0050, iter [06700, 10009], lr: 0.010000, loss: 2.1726
2022-03-06 16:32:17 - train: epoch 0050, iter [06800, 10009], lr: 0.010000, loss: 2.0828
2022-03-06 16:32:37 - train: epoch 0050, iter [06900, 10009], lr: 0.010000, loss: 2.0482
2022-03-06 16:32:56 - train: epoch 0050, iter [07000, 10009], lr: 0.010000, loss: 1.8857
2022-03-06 16:33:16 - train: epoch 0050, iter [07100, 10009], lr: 0.010000, loss: 1.5454
2022-03-06 16:33:36 - train: epoch 0050, iter [07200, 10009], lr: 0.010000, loss: 1.7520
2022-03-06 16:33:56 - train: epoch 0050, iter [07300, 10009], lr: 0.010000, loss: 1.2958
2022-03-06 16:34:16 - train: epoch 0050, iter [07400, 10009], lr: 0.010000, loss: 2.0129
2022-03-06 16:34:36 - train: epoch 0050, iter [07500, 10009], lr: 0.010000, loss: 1.8401
2022-03-06 16:34:55 - train: epoch 0050, iter [07600, 10009], lr: 0.010000, loss: 1.8312
2022-03-06 16:35:15 - train: epoch 0050, iter [07700, 10009], lr: 0.010000, loss: 1.8577
2022-03-06 16:35:35 - train: epoch 0050, iter [07800, 10009], lr: 0.010000, loss: 1.7023
2022-03-06 16:35:55 - train: epoch 0050, iter [07900, 10009], lr: 0.010000, loss: 2.0884
2022-03-06 16:36:15 - train: epoch 0050, iter [08000, 10009], lr: 0.010000, loss: 1.6526
2022-03-06 16:36:34 - train: epoch 0050, iter [08100, 10009], lr: 0.010000, loss: 1.4914
2022-03-06 16:36:54 - train: epoch 0050, iter [08200, 10009], lr: 0.010000, loss: 1.5591
2022-03-06 16:37:14 - train: epoch 0050, iter [08300, 10009], lr: 0.010000, loss: 1.5647
2022-03-06 16:37:34 - train: epoch 0050, iter [08400, 10009], lr: 0.010000, loss: 1.7341
2022-03-06 16:37:54 - train: epoch 0050, iter [08500, 10009], lr: 0.010000, loss: 2.1266
2022-03-06 16:38:13 - train: epoch 0050, iter [08600, 10009], lr: 0.010000, loss: 1.9567
2022-03-06 16:38:33 - train: epoch 0050, iter [08700, 10009], lr: 0.010000, loss: 1.4435
2022-03-06 16:38:53 - train: epoch 0050, iter [08800, 10009], lr: 0.010000, loss: 1.9318
2022-03-06 16:39:13 - train: epoch 0050, iter [08900, 10009], lr: 0.010000, loss: 1.4172
2022-03-06 16:39:33 - train: epoch 0050, iter [09000, 10009], lr: 0.010000, loss: 1.6553
2022-03-06 16:39:53 - train: epoch 0050, iter [09100, 10009], lr: 0.010000, loss: 1.9203
2022-03-06 16:40:13 - train: epoch 0050, iter [09200, 10009], lr: 0.010000, loss: 1.5467
2022-03-06 16:40:32 - train: epoch 0050, iter [09300, 10009], lr: 0.010000, loss: 1.7326
2022-03-06 16:40:52 - train: epoch 0050, iter [09400, 10009], lr: 0.010000, loss: 1.8785
2022-03-06 16:41:12 - train: epoch 0050, iter [09500, 10009], lr: 0.010000, loss: 1.9472
2022-03-06 16:41:32 - train: epoch 0050, iter [09600, 10009], lr: 0.010000, loss: 2.0216
2022-03-06 16:41:52 - train: epoch 0050, iter [09700, 10009], lr: 0.010000, loss: 1.4566
2022-03-06 16:42:12 - train: epoch 0050, iter [09800, 10009], lr: 0.010000, loss: 1.5196
2022-03-06 16:42:32 - train: epoch 0050, iter [09900, 10009], lr: 0.010000, loss: 2.0207
2022-03-06 16:42:52 - train: epoch 0050, iter [10000, 10009], lr: 0.010000, loss: 1.6945
2022-03-06 16:42:54 - train: epoch 050, train_loss: 1.7104
2022-03-06 16:44:09 - eval: epoch: 050, acc1: 65.700%, acc5: 86.930%, test_loss: 1.4069, per_image_load_time: 1.455ms, per_image_inference_time: 0.881ms
2022-03-06 16:44:10 - until epoch: 050, best_acc1: 65.700%
2022-03-06 16:44:10 - epoch 051 lr: 0.010000000000000002
2022-03-06 16:44:33 - train: epoch 0051, iter [00100, 10009], lr: 0.010000, loss: 1.6353
2022-03-06 16:44:53 - train: epoch 0051, iter [00200, 10009], lr: 0.010000, loss: 1.5702
2022-03-06 16:45:12 - train: epoch 0051, iter [00300, 10009], lr: 0.010000, loss: 1.6493
2022-03-06 16:45:32 - train: epoch 0051, iter [00400, 10009], lr: 0.010000, loss: 1.7405
2022-03-06 16:45:51 - train: epoch 0051, iter [00500, 10009], lr: 0.010000, loss: 1.7373
2022-03-06 16:46:11 - train: epoch 0051, iter [00600, 10009], lr: 0.010000, loss: 1.6219
2022-03-06 16:46:31 - train: epoch 0051, iter [00700, 10009], lr: 0.010000, loss: 1.4506
2022-03-06 16:46:50 - train: epoch 0051, iter [00800, 10009], lr: 0.010000, loss: 1.3940
2022-03-06 16:47:10 - train: epoch 0051, iter [00900, 10009], lr: 0.010000, loss: 1.8401
2022-03-06 16:47:30 - train: epoch 0051, iter [01000, 10009], lr: 0.010000, loss: 1.7006
2022-03-06 16:47:49 - train: epoch 0051, iter [01100, 10009], lr: 0.010000, loss: 1.5391
2022-03-06 16:48:09 - train: epoch 0051, iter [01200, 10009], lr: 0.010000, loss: 1.5993
2022-03-06 16:48:28 - train: epoch 0051, iter [01300, 10009], lr: 0.010000, loss: 1.7130
2022-03-06 16:48:48 - train: epoch 0051, iter [01400, 10009], lr: 0.010000, loss: 1.6999
2022-03-06 16:49:07 - train: epoch 0051, iter [01500, 10009], lr: 0.010000, loss: 1.7848
2022-03-06 16:49:27 - train: epoch 0051, iter [01600, 10009], lr: 0.010000, loss: 2.0797
2022-03-06 16:49:46 - train: epoch 0051, iter [01700, 10009], lr: 0.010000, loss: 1.8633
2022-03-06 16:50:06 - train: epoch 0051, iter [01800, 10009], lr: 0.010000, loss: 1.5791
2022-03-06 16:50:26 - train: epoch 0051, iter [01900, 10009], lr: 0.010000, loss: 1.4881
2022-03-06 16:50:45 - train: epoch 0051, iter [02000, 10009], lr: 0.010000, loss: 2.2305
2022-03-06 16:51:05 - train: epoch 0051, iter [02100, 10009], lr: 0.010000, loss: 1.3835
2022-03-06 16:51:24 - train: epoch 0051, iter [02200, 10009], lr: 0.010000, loss: 1.7652
2022-03-06 16:51:44 - train: epoch 0051, iter [02300, 10009], lr: 0.010000, loss: 1.8671
2022-03-06 16:52:04 - train: epoch 0051, iter [02400, 10009], lr: 0.010000, loss: 1.7786
2022-03-06 16:52:23 - train: epoch 0051, iter [02500, 10009], lr: 0.010000, loss: 1.9690
2022-03-06 16:52:43 - train: epoch 0051, iter [02600, 10009], lr: 0.010000, loss: 1.4533
2022-03-06 16:53:03 - train: epoch 0051, iter [02700, 10009], lr: 0.010000, loss: 1.7636
2022-03-06 16:53:22 - train: epoch 0051, iter [02800, 10009], lr: 0.010000, loss: 1.5602
2022-03-06 16:53:42 - train: epoch 0051, iter [02900, 10009], lr: 0.010000, loss: 1.3996
2022-03-06 16:54:02 - train: epoch 0051, iter [03000, 10009], lr: 0.010000, loss: 1.5555
2022-03-06 16:54:22 - train: epoch 0051, iter [03100, 10009], lr: 0.010000, loss: 1.8887
2022-03-06 16:54:41 - train: epoch 0051, iter [03200, 10009], lr: 0.010000, loss: 1.5489
2022-03-06 16:55:01 - train: epoch 0051, iter [03300, 10009], lr: 0.010000, loss: 1.7257
2022-03-06 16:55:21 - train: epoch 0051, iter [03400, 10009], lr: 0.010000, loss: 1.9227
2022-03-06 16:55:40 - train: epoch 0051, iter [03500, 10009], lr: 0.010000, loss: 1.5982
2022-03-06 16:56:00 - train: epoch 0051, iter [03600, 10009], lr: 0.010000, loss: 1.8918
2022-03-06 16:56:20 - train: epoch 0051, iter [03700, 10009], lr: 0.010000, loss: 1.7465
2022-03-06 16:56:40 - train: epoch 0051, iter [03800, 10009], lr: 0.010000, loss: 1.6901
2022-03-06 16:56:59 - train: epoch 0051, iter [03900, 10009], lr: 0.010000, loss: 1.6191
2022-03-06 16:57:19 - train: epoch 0051, iter [04000, 10009], lr: 0.010000, loss: 1.3937
2022-03-06 16:57:39 - train: epoch 0051, iter [04100, 10009], lr: 0.010000, loss: 1.6735
2022-03-06 16:57:59 - train: epoch 0051, iter [04200, 10009], lr: 0.010000, loss: 1.5987
2022-03-06 16:58:18 - train: epoch 0051, iter [04300, 10009], lr: 0.010000, loss: 1.4069
2022-03-06 16:58:38 - train: epoch 0051, iter [04400, 10009], lr: 0.010000, loss: 1.6531
2022-03-06 16:58:58 - train: epoch 0051, iter [04500, 10009], lr: 0.010000, loss: 1.5153
2022-03-06 16:59:18 - train: epoch 0051, iter [04600, 10009], lr: 0.010000, loss: 1.8083
2022-03-06 16:59:38 - train: epoch 0051, iter [04700, 10009], lr: 0.010000, loss: 1.8506
2022-03-06 16:59:58 - train: epoch 0051, iter [04800, 10009], lr: 0.010000, loss: 1.8394
2022-03-06 17:00:18 - train: epoch 0051, iter [04900, 10009], lr: 0.010000, loss: 1.3740
2022-03-06 17:00:38 - train: epoch 0051, iter [05000, 10009], lr: 0.010000, loss: 1.4472
2022-03-06 17:00:58 - train: epoch 0051, iter [05100, 10009], lr: 0.010000, loss: 1.5823
2022-03-06 17:01:18 - train: epoch 0051, iter [05200, 10009], lr: 0.010000, loss: 1.7777
2022-03-06 17:01:38 - train: epoch 0051, iter [05300, 10009], lr: 0.010000, loss: 1.4120
2022-03-06 17:01:58 - train: epoch 0051, iter [05400, 10009], lr: 0.010000, loss: 1.8013
2022-03-06 17:02:17 - train: epoch 0051, iter [05500, 10009], lr: 0.010000, loss: 1.7785
2022-03-06 17:02:37 - train: epoch 0051, iter [05600, 10009], lr: 0.010000, loss: 1.4963
2022-03-06 17:02:57 - train: epoch 0051, iter [05700, 10009], lr: 0.010000, loss: 1.6970
2022-03-06 17:03:17 - train: epoch 0051, iter [05800, 10009], lr: 0.010000, loss: 1.4382
2022-03-06 17:03:37 - train: epoch 0051, iter [05900, 10009], lr: 0.010000, loss: 1.7319
2022-03-06 17:03:57 - train: epoch 0051, iter [06000, 10009], lr: 0.010000, loss: 1.4846
2022-03-06 17:04:17 - train: epoch 0051, iter [06100, 10009], lr: 0.010000, loss: 1.5577
2022-03-06 17:04:37 - train: epoch 0051, iter [06200, 10009], lr: 0.010000, loss: 1.7051
2022-03-06 17:04:56 - train: epoch 0051, iter [06300, 10009], lr: 0.010000, loss: 2.0405
2022-03-06 17:05:16 - train: epoch 0051, iter [06400, 10009], lr: 0.010000, loss: 1.3952
2022-03-06 17:05:36 - train: epoch 0051, iter [06500, 10009], lr: 0.010000, loss: 1.6534
2022-03-06 17:05:56 - train: epoch 0051, iter [06600, 10009], lr: 0.010000, loss: 1.8402
2022-03-06 17:06:16 - train: epoch 0051, iter [06700, 10009], lr: 0.010000, loss: 1.6518
2022-03-06 17:06:36 - train: epoch 0051, iter [06800, 10009], lr: 0.010000, loss: 1.5495
2022-03-06 17:06:56 - train: epoch 0051, iter [06900, 10009], lr: 0.010000, loss: 2.1471
2022-03-06 17:07:16 - train: epoch 0051, iter [07000, 10009], lr: 0.010000, loss: 1.7248
2022-03-06 17:07:36 - train: epoch 0051, iter [07100, 10009], lr: 0.010000, loss: 1.5247
2022-03-06 17:07:56 - train: epoch 0051, iter [07200, 10009], lr: 0.010000, loss: 1.7285
2022-03-06 17:08:16 - train: epoch 0051, iter [07300, 10009], lr: 0.010000, loss: 1.9937
2022-03-06 17:08:36 - train: epoch 0051, iter [07400, 10009], lr: 0.010000, loss: 1.8305
2022-03-06 17:08:56 - train: epoch 0051, iter [07500, 10009], lr: 0.010000, loss: 1.5479
2022-03-06 17:09:15 - train: epoch 0051, iter [07600, 10009], lr: 0.010000, loss: 1.9602
2022-03-06 17:09:35 - train: epoch 0051, iter [07700, 10009], lr: 0.010000, loss: 1.5332
2022-03-06 17:09:55 - train: epoch 0051, iter [07800, 10009], lr: 0.010000, loss: 2.0081
2022-03-06 17:10:15 - train: epoch 0051, iter [07900, 10009], lr: 0.010000, loss: 1.7501
2022-03-06 17:10:35 - train: epoch 0051, iter [08000, 10009], lr: 0.010000, loss: 1.9583
2022-03-06 17:10:54 - train: epoch 0051, iter [08100, 10009], lr: 0.010000, loss: 1.7498
2022-03-06 17:11:14 - train: epoch 0051, iter [08200, 10009], lr: 0.010000, loss: 1.8198
2022-03-06 17:11:34 - train: epoch 0051, iter [08300, 10009], lr: 0.010000, loss: 1.6763
2022-03-06 17:11:54 - train: epoch 0051, iter [08400, 10009], lr: 0.010000, loss: 1.9789
2022-03-06 17:12:14 - train: epoch 0051, iter [08500, 10009], lr: 0.010000, loss: 1.9418
2022-03-06 17:12:34 - train: epoch 0051, iter [08600, 10009], lr: 0.010000, loss: 1.3772
2022-03-06 17:12:54 - train: epoch 0051, iter [08700, 10009], lr: 0.010000, loss: 1.8087
2022-03-06 17:13:14 - train: epoch 0051, iter [08800, 10009], lr: 0.010000, loss: 1.6279
2022-03-06 17:13:34 - train: epoch 0051, iter [08900, 10009], lr: 0.010000, loss: 1.5308
2022-03-06 17:13:53 - train: epoch 0051, iter [09000, 10009], lr: 0.010000, loss: 1.4730
2022-03-06 17:14:13 - train: epoch 0051, iter [09100, 10009], lr: 0.010000, loss: 1.7550
2022-03-06 17:14:33 - train: epoch 0051, iter [09200, 10009], lr: 0.010000, loss: 1.9971
2022-03-06 17:14:53 - train: epoch 0051, iter [09300, 10009], lr: 0.010000, loss: 1.8076
2022-03-06 17:15:14 - train: epoch 0051, iter [09400, 10009], lr: 0.010000, loss: 1.7182
2022-03-06 17:15:33 - train: epoch 0051, iter [09500, 10009], lr: 0.010000, loss: 1.6072
2022-03-06 17:15:53 - train: epoch 0051, iter [09600, 10009], lr: 0.010000, loss: 1.9348
2022-03-06 17:16:13 - train: epoch 0051, iter [09700, 10009], lr: 0.010000, loss: 1.6705
2022-03-06 17:16:33 - train: epoch 0051, iter [09800, 10009], lr: 0.010000, loss: 1.6890
2022-03-06 17:16:53 - train: epoch 0051, iter [09900, 10009], lr: 0.010000, loss: 1.7782
2022-03-06 17:17:13 - train: epoch 0051, iter [10000, 10009], lr: 0.010000, loss: 1.7393
2022-03-06 17:17:15 - train: epoch 051, train_loss: 1.7068
2022-03-06 17:18:28 - eval: epoch: 051, acc1: 65.902%, acc5: 86.970%, test_loss: 1.3985, per_image_load_time: 1.232ms, per_image_inference_time: 0.805ms
2022-03-06 17:18:29 - until epoch: 051, best_acc1: 65.902%
2022-03-06 17:18:29 - epoch 052 lr: 0.010000000000000002
2022-03-06 17:18:52 - train: epoch 0052, iter [00100, 10009], lr: 0.010000, loss: 1.2600
2022-03-06 17:19:11 - train: epoch 0052, iter [00200, 10009], lr: 0.010000, loss: 1.9360
2022-03-06 17:19:31 - train: epoch 0052, iter [00300, 10009], lr: 0.010000, loss: 1.9171
2022-03-06 17:19:50 - train: epoch 0052, iter [00400, 10009], lr: 0.010000, loss: 1.4950
2022-03-06 17:20:10 - train: epoch 0052, iter [00500, 10009], lr: 0.010000, loss: 1.4892
2022-03-06 17:20:30 - train: epoch 0052, iter [00600, 10009], lr: 0.010000, loss: 1.5430
2022-03-06 17:20:49 - train: epoch 0052, iter [00700, 10009], lr: 0.010000, loss: 1.8640
2022-03-06 17:21:09 - train: epoch 0052, iter [00800, 10009], lr: 0.010000, loss: 1.8303
2022-03-06 17:21:28 - train: epoch 0052, iter [00900, 10009], lr: 0.010000, loss: 1.4787
2022-03-06 17:21:48 - train: epoch 0052, iter [01000, 10009], lr: 0.010000, loss: 1.8525
2022-03-06 17:22:08 - train: epoch 0052, iter [01100, 10009], lr: 0.010000, loss: 1.9980
2022-03-06 17:22:27 - train: epoch 0052, iter [01200, 10009], lr: 0.010000, loss: 1.3722
2022-03-06 17:22:47 - train: epoch 0052, iter [01300, 10009], lr: 0.010000, loss: 1.3486
2022-03-06 17:23:07 - train: epoch 0052, iter [01400, 10009], lr: 0.010000, loss: 1.8282
2022-03-06 17:23:26 - train: epoch 0052, iter [01500, 10009], lr: 0.010000, loss: 1.4109
2022-03-06 17:23:46 - train: epoch 0052, iter [01600, 10009], lr: 0.010000, loss: 1.6586
2022-03-06 17:24:06 - train: epoch 0052, iter [01700, 10009], lr: 0.010000, loss: 1.8193
2022-03-06 17:24:26 - train: epoch 0052, iter [01800, 10009], lr: 0.010000, loss: 1.7512
2022-03-06 17:24:45 - train: epoch 0052, iter [01900, 10009], lr: 0.010000, loss: 1.5494
2022-03-06 17:25:05 - train: epoch 0052, iter [02000, 10009], lr: 0.010000, loss: 1.7035
2022-03-06 17:25:25 - train: epoch 0052, iter [02100, 10009], lr: 0.010000, loss: 1.5116
2022-03-06 17:25:44 - train: epoch 0052, iter [02200, 10009], lr: 0.010000, loss: 1.8677
2022-03-06 17:26:04 - train: epoch 0052, iter [02300, 10009], lr: 0.010000, loss: 1.4417
2022-03-06 17:26:24 - train: epoch 0052, iter [02400, 10009], lr: 0.010000, loss: 1.4337
2022-03-06 17:26:43 - train: epoch 0052, iter [02500, 10009], lr: 0.010000, loss: 1.5057
2022-03-06 17:27:03 - train: epoch 0052, iter [02600, 10009], lr: 0.010000, loss: 1.7621
2022-03-06 17:27:23 - train: epoch 0052, iter [02700, 10009], lr: 0.010000, loss: 1.9258
2022-03-06 17:27:43 - train: epoch 0052, iter [02800, 10009], lr: 0.010000, loss: 2.3213
2022-03-06 17:28:02 - train: epoch 0052, iter [02900, 10009], lr: 0.010000, loss: 1.6820
2022-03-06 17:28:22 - train: epoch 0052, iter [03000, 10009], lr: 0.010000, loss: 1.4176
2022-03-06 17:28:42 - train: epoch 0052, iter [03100, 10009], lr: 0.010000, loss: 1.4262
2022-03-06 17:29:01 - train: epoch 0052, iter [03200, 10009], lr: 0.010000, loss: 1.6497
2022-03-06 17:29:21 - train: epoch 0052, iter [03300, 10009], lr: 0.010000, loss: 1.3975
2022-03-06 17:29:41 - train: epoch 0052, iter [03400, 10009], lr: 0.010000, loss: 1.4268
2022-03-06 17:30:00 - train: epoch 0052, iter [03500, 10009], lr: 0.010000, loss: 1.6728
2022-03-06 17:30:20 - train: epoch 0052, iter [03600, 10009], lr: 0.010000, loss: 1.6975
2022-03-06 17:30:40 - train: epoch 0052, iter [03700, 10009], lr: 0.010000, loss: 1.7077
2022-03-06 17:30:59 - train: epoch 0052, iter [03800, 10009], lr: 0.010000, loss: 1.4325
2022-03-06 17:31:19 - train: epoch 0052, iter [03900, 10009], lr: 0.010000, loss: 1.5241
2022-03-06 17:31:38 - train: epoch 0052, iter [04000, 10009], lr: 0.010000, loss: 1.8007
2022-03-06 17:31:58 - train: epoch 0052, iter [04100, 10009], lr: 0.010000, loss: 1.6104
2022-03-06 17:32:18 - train: epoch 0052, iter [04200, 10009], lr: 0.010000, loss: 1.6752
2022-03-06 17:32:38 - train: epoch 0052, iter [04300, 10009], lr: 0.010000, loss: 1.6325
2022-03-06 17:32:58 - train: epoch 0052, iter [04400, 10009], lr: 0.010000, loss: 1.8253
2022-03-06 17:33:18 - train: epoch 0052, iter [04500, 10009], lr: 0.010000, loss: 1.3753
2022-03-06 17:33:37 - train: epoch 0052, iter [04600, 10009], lr: 0.010000, loss: 1.5276
2022-03-06 17:33:57 - train: epoch 0052, iter [04700, 10009], lr: 0.010000, loss: 1.9784
2022-03-06 17:34:17 - train: epoch 0052, iter [04800, 10009], lr: 0.010000, loss: 1.5328
2022-03-06 17:34:37 - train: epoch 0052, iter [04900, 10009], lr: 0.010000, loss: 1.5466
2022-03-06 17:34:57 - train: epoch 0052, iter [05000, 10009], lr: 0.010000, loss: 1.7254
2022-03-06 17:35:17 - train: epoch 0052, iter [05100, 10009], lr: 0.010000, loss: 2.1422
2022-03-06 17:35:37 - train: epoch 0052, iter [05200, 10009], lr: 0.010000, loss: 1.4124
2022-03-06 17:35:56 - train: epoch 0052, iter [05300, 10009], lr: 0.010000, loss: 1.8573
2022-03-06 17:36:16 - train: epoch 0052, iter [05400, 10009], lr: 0.010000, loss: 1.7647
2022-03-06 17:36:36 - train: epoch 0052, iter [05500, 10009], lr: 0.010000, loss: 1.6028
2022-03-06 17:36:56 - train: epoch 0052, iter [05600, 10009], lr: 0.010000, loss: 1.7777
2022-03-06 17:37:16 - train: epoch 0052, iter [05700, 10009], lr: 0.010000, loss: 1.7157
2022-03-06 17:37:36 - train: epoch 0052, iter [05800, 10009], lr: 0.010000, loss: 1.5335
2022-03-06 17:37:56 - train: epoch 0052, iter [05900, 10009], lr: 0.010000, loss: 1.9371
2022-03-06 17:38:15 - train: epoch 0052, iter [06000, 10009], lr: 0.010000, loss: 1.6627
2022-03-06 17:38:35 - train: epoch 0052, iter [06100, 10009], lr: 0.010000, loss: 1.7315
2022-03-06 17:38:55 - train: epoch 0052, iter [06200, 10009], lr: 0.010000, loss: 1.9019
2022-03-06 17:39:15 - train: epoch 0052, iter [06300, 10009], lr: 0.010000, loss: 1.9397
2022-03-06 17:39:35 - train: epoch 0052, iter [06400, 10009], lr: 0.010000, loss: 1.4986
2022-03-06 17:39:55 - train: epoch 0052, iter [06500, 10009], lr: 0.010000, loss: 1.9735
2022-03-06 17:40:15 - train: epoch 0052, iter [06600, 10009], lr: 0.010000, loss: 1.9685
2022-03-06 17:40:35 - train: epoch 0052, iter [06700, 10009], lr: 0.010000, loss: 1.6780
2022-03-06 17:40:55 - train: epoch 0052, iter [06800, 10009], lr: 0.010000, loss: 1.7616
2022-03-06 17:41:15 - train: epoch 0052, iter [06900, 10009], lr: 0.010000, loss: 1.6747
2022-03-06 17:41:35 - train: epoch 0052, iter [07000, 10009], lr: 0.010000, loss: 1.8682
2022-03-06 17:41:55 - train: epoch 0052, iter [07100, 10009], lr: 0.010000, loss: 2.2086
2022-03-06 17:42:15 - train: epoch 0052, iter [07200, 10009], lr: 0.010000, loss: 1.8974
2022-03-06 17:42:35 - train: epoch 0052, iter [07300, 10009], lr: 0.010000, loss: 1.7426
2022-03-06 17:42:55 - train: epoch 0052, iter [07400, 10009], lr: 0.010000, loss: 1.5620
2022-03-06 17:43:15 - train: epoch 0052, iter [07500, 10009], lr: 0.010000, loss: 1.8273
2022-03-06 17:43:35 - train: epoch 0052, iter [07600, 10009], lr: 0.010000, loss: 1.4305
2022-03-06 17:43:55 - train: epoch 0052, iter [07700, 10009], lr: 0.010000, loss: 1.6680
2022-03-06 17:44:16 - train: epoch 0052, iter [07800, 10009], lr: 0.010000, loss: 1.6111
2022-03-06 17:44:36 - train: epoch 0052, iter [07900, 10009], lr: 0.010000, loss: 1.6108
2022-03-06 17:44:56 - train: epoch 0052, iter [08000, 10009], lr: 0.010000, loss: 1.7309
2022-03-06 17:45:16 - train: epoch 0052, iter [08100, 10009], lr: 0.010000, loss: 1.9367
2022-03-06 17:45:36 - train: epoch 0052, iter [08200, 10009], lr: 0.010000, loss: 1.5889
2022-03-06 17:45:56 - train: epoch 0052, iter [08300, 10009], lr: 0.010000, loss: 1.8978
2022-03-06 17:46:16 - train: epoch 0052, iter [08400, 10009], lr: 0.010000, loss: 1.9414
2022-03-06 17:46:36 - train: epoch 0052, iter [08500, 10009], lr: 0.010000, loss: 1.9040
2022-03-06 17:46:56 - train: epoch 0052, iter [08600, 10009], lr: 0.010000, loss: 1.8011
2022-03-06 17:47:16 - train: epoch 0052, iter [08700, 10009], lr: 0.010000, loss: 1.6531
2022-03-06 17:47:36 - train: epoch 0052, iter [08800, 10009], lr: 0.010000, loss: 1.7344
2022-03-06 17:47:56 - train: epoch 0052, iter [08900, 10009], lr: 0.010000, loss: 1.7167
2022-03-06 17:48:16 - train: epoch 0052, iter [09000, 10009], lr: 0.010000, loss: 1.7216
2022-03-06 17:48:36 - train: epoch 0052, iter [09100, 10009], lr: 0.010000, loss: 1.4554
2022-03-06 17:48:57 - train: epoch 0052, iter [09200, 10009], lr: 0.010000, loss: 1.5300
2022-03-06 17:49:17 - train: epoch 0052, iter [09300, 10009], lr: 0.010000, loss: 1.7460
2022-03-06 17:49:37 - train: epoch 0052, iter [09400, 10009], lr: 0.010000, loss: 1.7114
2022-03-06 17:49:57 - train: epoch 0052, iter [09500, 10009], lr: 0.010000, loss: 1.7488
2022-03-06 17:50:17 - train: epoch 0052, iter [09600, 10009], lr: 0.010000, loss: 1.9422
2022-03-06 17:50:37 - train: epoch 0052, iter [09700, 10009], lr: 0.010000, loss: 1.8310
2022-03-06 17:50:57 - train: epoch 0052, iter [09800, 10009], lr: 0.010000, loss: 1.7885
2022-03-06 17:51:17 - train: epoch 0052, iter [09900, 10009], lr: 0.010000, loss: 1.6854
2022-03-06 17:51:37 - train: epoch 0052, iter [10000, 10009], lr: 0.010000, loss: 1.6319
2022-03-06 17:51:40 - train: epoch 052, train_loss: 1.6995
2022-03-06 17:52:53 - eval: epoch: 052, acc1: 65.854%, acc5: 87.014%, test_loss: 1.3995, per_image_load_time: 2.049ms, per_image_inference_time: 0.786ms
2022-03-06 17:52:54 - until epoch: 052, best_acc1: 65.902%
2022-03-06 17:52:54 - epoch 053 lr: 0.010000000000000002
2022-03-06 17:53:17 - train: epoch 0053, iter [00100, 10009], lr: 0.010000, loss: 1.9256
2022-03-06 17:53:37 - train: epoch 0053, iter [00200, 10009], lr: 0.010000, loss: 1.9667
2022-03-06 17:53:57 - train: epoch 0053, iter [00300, 10009], lr: 0.010000, loss: 2.1361
2022-03-06 17:54:17 - train: epoch 0053, iter [00400, 10009], lr: 0.010000, loss: 1.6738
2022-03-06 17:54:36 - train: epoch 0053, iter [00500, 10009], lr: 0.010000, loss: 1.5692
2022-03-06 17:54:56 - train: epoch 0053, iter [00600, 10009], lr: 0.010000, loss: 1.7751
2022-03-06 17:55:16 - train: epoch 0053, iter [00700, 10009], lr: 0.010000, loss: 1.4501
2022-03-06 17:55:36 - train: epoch 0053, iter [00800, 10009], lr: 0.010000, loss: 2.0578
2022-03-06 17:55:55 - train: epoch 0053, iter [00900, 10009], lr: 0.010000, loss: 1.8013
2022-03-06 17:56:15 - train: epoch 0053, iter [01000, 10009], lr: 0.010000, loss: 1.5960
2022-03-06 17:56:35 - train: epoch 0053, iter [01100, 10009], lr: 0.010000, loss: 1.7700
2022-03-06 17:56:55 - train: epoch 0053, iter [01200, 10009], lr: 0.010000, loss: 1.6435
2022-03-06 17:57:15 - train: epoch 0053, iter [01300, 10009], lr: 0.010000, loss: 1.5394
2022-03-06 17:57:35 - train: epoch 0053, iter [01400, 10009], lr: 0.010000, loss: 1.4328
2022-03-06 17:57:54 - train: epoch 0053, iter [01500, 10009], lr: 0.010000, loss: 1.2901
2022-03-06 17:58:14 - train: epoch 0053, iter [01600, 10009], lr: 0.010000, loss: 1.8571
2022-03-06 17:58:34 - train: epoch 0053, iter [01700, 10009], lr: 0.010000, loss: 1.7129
2022-03-06 17:58:54 - train: epoch 0053, iter [01800, 10009], lr: 0.010000, loss: 1.7871
2022-03-06 17:59:14 - train: epoch 0053, iter [01900, 10009], lr: 0.010000, loss: 1.6384
2022-03-06 17:59:33 - train: epoch 0053, iter [02000, 10009], lr: 0.010000, loss: 1.9188
2022-03-06 17:59:53 - train: epoch 0053, iter [02100, 10009], lr: 0.010000, loss: 1.5642
2022-03-06 18:00:14 - train: epoch 0053, iter [02200, 10009], lr: 0.010000, loss: 1.3102
2022-03-06 18:00:34 - train: epoch 0053, iter [02300, 10009], lr: 0.010000, loss: 1.5318
2022-03-06 18:00:54 - train: epoch 0053, iter [02400, 10009], lr: 0.010000, loss: 1.1349
2022-03-06 18:01:14 - train: epoch 0053, iter [02500, 10009], lr: 0.010000, loss: 1.4994
2022-03-06 18:01:33 - train: epoch 0053, iter [02600, 10009], lr: 0.010000, loss: 1.6757
2022-03-06 18:01:53 - train: epoch 0053, iter [02700, 10009], lr: 0.010000, loss: 1.9386
2022-03-06 18:02:13 - train: epoch 0053, iter [02800, 10009], lr: 0.010000, loss: 1.9028
2022-03-06 18:02:33 - train: epoch 0053, iter [02900, 10009], lr: 0.010000, loss: 1.8249
2022-03-06 18:02:53 - train: epoch 0053, iter [03000, 10009], lr: 0.010000, loss: 1.8468
2022-03-06 18:03:13 - train: epoch 0053, iter [03100, 10009], lr: 0.010000, loss: 1.6269
2022-03-06 18:03:33 - train: epoch 0053, iter [03200, 10009], lr: 0.010000, loss: 1.9521
2022-03-06 18:03:53 - train: epoch 0053, iter [03300, 10009], lr: 0.010000, loss: 1.7229
2022-03-06 18:04:13 - train: epoch 0053, iter [03400, 10009], lr: 0.010000, loss: 1.8192
2022-03-06 18:04:32 - train: epoch 0053, iter [03500, 10009], lr: 0.010000, loss: 1.6188
2022-03-06 18:04:52 - train: epoch 0053, iter [03600, 10009], lr: 0.010000, loss: 1.6389
2022-03-06 18:05:12 - train: epoch 0053, iter [03700, 10009], lr: 0.010000, loss: 1.5868
2022-03-06 18:05:32 - train: epoch 0053, iter [03800, 10009], lr: 0.010000, loss: 1.7360
2022-03-06 18:05:52 - train: epoch 0053, iter [03900, 10009], lr: 0.010000, loss: 1.4662
2022-03-06 18:06:12 - train: epoch 0053, iter [04000, 10009], lr: 0.010000, loss: 1.5457
2022-03-06 18:06:32 - train: epoch 0053, iter [04100, 10009], lr: 0.010000, loss: 1.8593
2022-03-06 18:06:52 - train: epoch 0053, iter [04200, 10009], lr: 0.010000, loss: 2.3358
2022-03-06 18:07:12 - train: epoch 0053, iter [04300, 10009], lr: 0.010000, loss: 1.9661
2022-03-06 18:07:32 - train: epoch 0053, iter [04400, 10009], lr: 0.010000, loss: 1.5795
2022-03-06 18:07:52 - train: epoch 0053, iter [04500, 10009], lr: 0.010000, loss: 1.8093
2022-03-06 18:08:11 - train: epoch 0053, iter [04600, 10009], lr: 0.010000, loss: 1.6260
2022-03-06 18:08:31 - train: epoch 0053, iter [04700, 10009], lr: 0.010000, loss: 1.9679
2022-03-06 18:08:51 - train: epoch 0053, iter [04800, 10009], lr: 0.010000, loss: 1.6842
2022-03-06 18:09:11 - train: epoch 0053, iter [04900, 10009], lr: 0.010000, loss: 1.9570
2022-03-06 18:09:31 - train: epoch 0053, iter [05000, 10009], lr: 0.010000, loss: 1.8544
2022-03-06 18:09:51 - train: epoch 0053, iter [05100, 10009], lr: 0.010000, loss: 1.4801
2022-03-06 18:10:11 - train: epoch 0053, iter [05200, 10009], lr: 0.010000, loss: 1.9888
2022-03-06 18:10:31 - train: epoch 0053, iter [05300, 10009], lr: 0.010000, loss: 1.8305
2022-03-06 18:10:50 - train: epoch 0053, iter [05400, 10009], lr: 0.010000, loss: 1.7279
2022-03-06 18:11:10 - train: epoch 0053, iter [05500, 10009], lr: 0.010000, loss: 1.8002
2022-03-06 18:11:30 - train: epoch 0053, iter [05600, 10009], lr: 0.010000, loss: 1.8255
2022-03-06 18:11:50 - train: epoch 0053, iter [05700, 10009], lr: 0.010000, loss: 1.4776
2022-03-06 18:12:10 - train: epoch 0053, iter [05800, 10009], lr: 0.010000, loss: 1.5520
2022-03-06 18:12:30 - train: epoch 0053, iter [05900, 10009], lr: 0.010000, loss: 1.7857
2022-03-06 18:12:53 - train: epoch 0053, iter [06000, 10009], lr: 0.010000, loss: 1.6173
2022-03-06 18:13:20 - train: epoch 0053, iter [06100, 10009], lr: 0.010000, loss: 1.6128
2022-03-06 18:13:55 - train: epoch 0053, iter [06200, 10009], lr: 0.010000, loss: 2.0796
2022-03-06 18:14:15 - train: epoch 0053, iter [06300, 10009], lr: 0.010000, loss: 2.0383
2022-03-06 18:14:34 - train: epoch 0053, iter [06400, 10009], lr: 0.010000, loss: 2.0086
2022-03-06 18:15:01 - train: epoch 0053, iter [06500, 10009], lr: 0.010000, loss: 1.6102
2022-03-06 18:15:41 - train: epoch 0053, iter [06600, 10009], lr: 0.010000, loss: 1.5617
2022-03-06 18:16:01 - train: epoch 0053, iter [06700, 10009], lr: 0.010000, loss: 1.9720
2022-03-06 18:16:39 - train: epoch 0053, iter [06800, 10009], lr: 0.010000, loss: 1.7625
2022-03-06 18:17:10 - train: epoch 0053, iter [06900, 10009], lr: 0.010000, loss: 1.6113
2022-03-06 18:17:30 - train: epoch 0053, iter [07000, 10009], lr: 0.010000, loss: 1.5435
2022-03-06 18:17:50 - train: epoch 0053, iter [07100, 10009], lr: 0.010000, loss: 1.6172
2022-03-06 18:18:16 - train: epoch 0053, iter [07200, 10009], lr: 0.010000, loss: 2.0556
2022-03-06 18:18:57 - train: epoch 0053, iter [07300, 10009], lr: 0.010000, loss: 1.4104
2022-03-06 18:19:36 - train: epoch 0053, iter [07400, 10009], lr: 0.010000, loss: 1.7674
2022-03-06 18:19:56 - train: epoch 0053, iter [07500, 10009], lr: 0.010000, loss: 1.9791
2022-03-06 18:20:16 - train: epoch 0053, iter [07600, 10009], lr: 0.010000, loss: 1.5467
2022-03-06 18:20:36 - train: epoch 0053, iter [07700, 10009], lr: 0.010000, loss: 1.9503
2022-03-06 18:20:56 - train: epoch 0053, iter [07800, 10009], lr: 0.010000, loss: 1.8122
2022-03-06 18:21:16 - train: epoch 0053, iter [07900, 10009], lr: 0.010000, loss: 1.7211
2022-03-06 18:21:36 - train: epoch 0053, iter [08000, 10009], lr: 0.010000, loss: 1.7332
2022-03-06 18:21:56 - train: epoch 0053, iter [08100, 10009], lr: 0.010000, loss: 1.6265
2022-03-06 18:22:17 - train: epoch 0053, iter [08200, 10009], lr: 0.010000, loss: 1.8205
2022-03-06 18:22:37 - train: epoch 0053, iter [08300, 10009], lr: 0.010000, loss: 1.7469
2022-03-06 18:22:57 - train: epoch 0053, iter [08400, 10009], lr: 0.010000, loss: 1.8991
2022-03-06 18:23:17 - train: epoch 0053, iter [08500, 10009], lr: 0.010000, loss: 1.9836
2022-03-06 18:23:37 - train: epoch 0053, iter [08600, 10009], lr: 0.010000, loss: 2.1486
2022-03-06 18:23:57 - train: epoch 0053, iter [08700, 10009], lr: 0.010000, loss: 1.6135
2022-03-06 18:24:17 - train: epoch 0053, iter [08800, 10009], lr: 0.010000, loss: 1.4934
2022-03-06 18:24:37 - train: epoch 0053, iter [08900, 10009], lr: 0.010000, loss: 1.5657
2022-03-06 18:24:57 - train: epoch 0053, iter [09000, 10009], lr: 0.010000, loss: 1.6522
2022-03-06 18:25:17 - train: epoch 0053, iter [09100, 10009], lr: 0.010000, loss: 1.5361
2022-03-06 18:25:37 - train: epoch 0053, iter [09200, 10009], lr: 0.010000, loss: 1.9919
2022-03-06 18:25:57 - train: epoch 0053, iter [09300, 10009], lr: 0.010000, loss: 1.7798
2022-03-06 18:26:17 - train: epoch 0053, iter [09400, 10009], lr: 0.010000, loss: 1.5345
2022-03-06 18:26:37 - train: epoch 0053, iter [09500, 10009], lr: 0.010000, loss: 1.3765
2022-03-06 18:26:56 - train: epoch 0053, iter [09600, 10009], lr: 0.010000, loss: 2.0259
2022-03-06 18:27:16 - train: epoch 0053, iter [09700, 10009], lr: 0.010000, loss: 1.6817
2022-03-06 18:27:36 - train: epoch 0053, iter [09800, 10009], lr: 0.010000, loss: 1.8264
2022-03-06 18:27:56 - train: epoch 0053, iter [09900, 10009], lr: 0.010000, loss: 1.7260
2022-03-06 18:28:16 - train: epoch 0053, iter [10000, 10009], lr: 0.010000, loss: 1.7711
2022-03-06 18:28:19 - train: epoch 053, train_loss: 1.6944
2022-03-06 18:29:33 - eval: epoch: 053, acc1: 65.698%, acc5: 86.968%, test_loss: 1.4019, per_image_load_time: 1.721ms, per_image_inference_time: 0.826ms
2022-03-06 18:29:33 - until epoch: 053, best_acc1: 65.902%
2022-03-06 18:29:33 - epoch 054 lr: 0.010000000000000002
2022-03-06 18:29:57 - train: epoch 0054, iter [00100, 10009], lr: 0.010000, loss: 1.4393
2022-03-06 18:30:17 - train: epoch 0054, iter [00200, 10009], lr: 0.010000, loss: 1.3595
2022-03-06 18:30:36 - train: epoch 0054, iter [00300, 10009], lr: 0.010000, loss: 1.5376
2022-03-06 18:30:56 - train: epoch 0054, iter [00400, 10009], lr: 0.010000, loss: 1.8315
2022-03-06 18:31:15 - train: epoch 0054, iter [00500, 10009], lr: 0.010000, loss: 1.2799
2022-03-06 18:31:35 - train: epoch 0054, iter [00600, 10009], lr: 0.010000, loss: 2.0769
2022-03-06 18:31:54 - train: epoch 0054, iter [00700, 10009], lr: 0.010000, loss: 1.7538
2022-03-06 18:32:14 - train: epoch 0054, iter [00800, 10009], lr: 0.010000, loss: 1.5876
2022-03-06 18:32:33 - train: epoch 0054, iter [00900, 10009], lr: 0.010000, loss: 1.5478
2022-03-06 18:32:53 - train: epoch 0054, iter [01000, 10009], lr: 0.010000, loss: 1.3943
2022-03-06 18:33:13 - train: epoch 0054, iter [01100, 10009], lr: 0.010000, loss: 1.8085
2022-03-06 18:33:33 - train: epoch 0054, iter [01200, 10009], lr: 0.010000, loss: 1.7035
2022-03-06 18:33:52 - train: epoch 0054, iter [01300, 10009], lr: 0.010000, loss: 1.8275
2022-03-06 18:34:12 - train: epoch 0054, iter [01400, 10009], lr: 0.010000, loss: 1.8744
2022-03-06 18:34:32 - train: epoch 0054, iter [01500, 10009], lr: 0.010000, loss: 1.6923
2022-03-06 18:34:51 - train: epoch 0054, iter [01600, 10009], lr: 0.010000, loss: 1.7124
2022-03-06 18:35:11 - train: epoch 0054, iter [01700, 10009], lr: 0.010000, loss: 1.6460
2022-03-06 18:35:30 - train: epoch 0054, iter [01800, 10009], lr: 0.010000, loss: 1.7169
2022-03-06 18:35:50 - train: epoch 0054, iter [01900, 10009], lr: 0.010000, loss: 1.9216
2022-03-06 18:36:10 - train: epoch 0054, iter [02000, 10009], lr: 0.010000, loss: 1.7138
2022-03-06 18:36:29 - train: epoch 0054, iter [02100, 10009], lr: 0.010000, loss: 1.6150
2022-03-06 18:36:49 - train: epoch 0054, iter [02200, 10009], lr: 0.010000, loss: 1.4536
2022-03-06 18:37:09 - train: epoch 0054, iter [02300, 10009], lr: 0.010000, loss: 1.7852
2022-03-06 18:37:28 - train: epoch 0054, iter [02400, 10009], lr: 0.010000, loss: 1.6386
2022-03-06 18:37:48 - train: epoch 0054, iter [02500, 10009], lr: 0.010000, loss: 1.1870
2022-03-06 18:38:08 - train: epoch 0054, iter [02600, 10009], lr: 0.010000, loss: 1.5281
2022-03-06 18:38:28 - train: epoch 0054, iter [02700, 10009], lr: 0.010000, loss: 1.8414
2022-03-06 18:38:47 - train: epoch 0054, iter [02800, 10009], lr: 0.010000, loss: 1.6701
2022-03-06 18:39:07 - train: epoch 0054, iter [02900, 10009], lr: 0.010000, loss: 1.7802
2022-03-06 18:39:28 - train: epoch 0054, iter [03000, 10009], lr: 0.010000, loss: 1.7117
2022-03-06 18:39:54 - train: epoch 0054, iter [03100, 10009], lr: 0.010000, loss: 1.9495
2022-03-06 18:40:18 - train: epoch 0054, iter [03200, 10009], lr: 0.010000, loss: 1.4645
2022-03-06 18:40:40 - train: epoch 0054, iter [03300, 10009], lr: 0.010000, loss: 1.5757
2022-03-06 18:40:59 - train: epoch 0054, iter [03400, 10009], lr: 0.010000, loss: 1.4354
2022-03-06 18:41:24 - train: epoch 0054, iter [03500, 10009], lr: 0.010000, loss: 1.8609
2022-03-06 18:41:48 - train: epoch 0054, iter [03600, 10009], lr: 0.010000, loss: 1.6039
2022-03-06 18:42:08 - train: epoch 0054, iter [03700, 10009], lr: 0.010000, loss: 1.8245
2022-03-06 18:42:27 - train: epoch 0054, iter [03800, 10009], lr: 0.010000, loss: 1.6768
2022-03-06 18:42:47 - train: epoch 0054, iter [03900, 10009], lr: 0.010000, loss: 1.5828
2022-03-06 18:43:07 - train: epoch 0054, iter [04000, 10009], lr: 0.010000, loss: 1.9550
2022-03-06 18:43:27 - train: epoch 0054, iter [04100, 10009], lr: 0.010000, loss: 1.9262
2022-03-06 18:43:57 - train: epoch 0054, iter [04200, 10009], lr: 0.010000, loss: 1.3332
2022-03-06 18:44:17 - train: epoch 0054, iter [04300, 10009], lr: 0.010000, loss: 1.6458
2022-03-06 18:44:37 - train: epoch 0054, iter [04400, 10009], lr: 0.010000, loss: 1.3200
2022-03-06 18:44:57 - train: epoch 0054, iter [04500, 10009], lr: 0.010000, loss: 1.9149
2022-03-06 18:45:16 - train: epoch 0054, iter [04600, 10009], lr: 0.010000, loss: 1.7156
2022-03-06 18:45:36 - train: epoch 0054, iter [04700, 10009], lr: 0.010000, loss: 1.5444
2022-03-06 18:45:56 - train: epoch 0054, iter [04800, 10009], lr: 0.010000, loss: 1.7832
2022-03-06 18:46:16 - train: epoch 0054, iter [04900, 10009], lr: 0.010000, loss: 1.7553
2022-03-06 18:46:35 - train: epoch 0054, iter [05000, 10009], lr: 0.010000, loss: 1.8875
2022-03-06 18:46:55 - train: epoch 0054, iter [05100, 10009], lr: 0.010000, loss: 1.8049
2022-03-06 18:47:15 - train: epoch 0054, iter [05200, 10009], lr: 0.010000, loss: 1.5883
2022-03-06 18:47:35 - train: epoch 0054, iter [05300, 10009], lr: 0.010000, loss: 1.8043
2022-03-06 18:47:55 - train: epoch 0054, iter [05400, 10009], lr: 0.010000, loss: 2.0102
2022-03-06 18:48:14 - train: epoch 0054, iter [05500, 10009], lr: 0.010000, loss: 1.5915
2022-03-06 18:48:34 - train: epoch 0054, iter [05600, 10009], lr: 0.010000, loss: 1.7263
2022-03-06 18:48:54 - train: epoch 0054, iter [05700, 10009], lr: 0.010000, loss: 1.5891
2022-03-06 18:49:14 - train: epoch 0054, iter [05800, 10009], lr: 0.010000, loss: 1.4997
2022-03-06 18:49:34 - train: epoch 0054, iter [05900, 10009], lr: 0.010000, loss: 2.1218
2022-03-06 18:49:53 - train: epoch 0054, iter [06000, 10009], lr: 0.010000, loss: 1.8027
2022-03-06 18:50:13 - train: epoch 0054, iter [06100, 10009], lr: 0.010000, loss: 1.5523
2022-03-06 18:50:33 - train: epoch 0054, iter [06200, 10009], lr: 0.010000, loss: 1.7308
2022-03-06 18:50:52 - train: epoch 0054, iter [06300, 10009], lr: 0.010000, loss: 1.5487
2022-03-06 18:51:12 - train: epoch 0054, iter [06400, 10009], lr: 0.010000, loss: 1.7302
2022-03-06 18:51:32 - train: epoch 0054, iter [06500, 10009], lr: 0.010000, loss: 1.9192
2022-03-06 18:51:52 - train: epoch 0054, iter [06600, 10009], lr: 0.010000, loss: 1.6621
2022-03-06 18:52:12 - train: epoch 0054, iter [06700, 10009], lr: 0.010000, loss: 1.7425
2022-03-06 18:52:31 - train: epoch 0054, iter [06800, 10009], lr: 0.010000, loss: 1.5142
2022-03-06 18:52:51 - train: epoch 0054, iter [06900, 10009], lr: 0.010000, loss: 1.6498
2022-03-06 18:53:11 - train: epoch 0054, iter [07000, 10009], lr: 0.010000, loss: 1.7190
2022-03-06 18:53:31 - train: epoch 0054, iter [07100, 10009], lr: 0.010000, loss: 1.3844
2022-03-06 18:53:51 - train: epoch 0054, iter [07200, 10009], lr: 0.010000, loss: 1.5276
2022-03-06 18:54:10 - train: epoch 0054, iter [07300, 10009], lr: 0.010000, loss: 1.4154
2022-03-06 18:54:30 - train: epoch 0054, iter [07400, 10009], lr: 0.010000, loss: 1.5682
2022-03-06 18:54:50 - train: epoch 0054, iter [07500, 10009], lr: 0.010000, loss: 2.1683
2022-03-06 18:55:10 - train: epoch 0054, iter [07600, 10009], lr: 0.010000, loss: 1.7523
2022-03-06 18:55:30 - train: epoch 0054, iter [07700, 10009], lr: 0.010000, loss: 1.7784
2022-03-06 18:55:50 - train: epoch 0054, iter [07800, 10009], lr: 0.010000, loss: 1.7818
2022-03-06 18:56:09 - train: epoch 0054, iter [07900, 10009], lr: 0.010000, loss: 1.9589
2022-03-06 18:56:29 - train: epoch 0054, iter [08000, 10009], lr: 0.010000, loss: 1.8959
2022-03-06 18:56:49 - train: epoch 0054, iter [08100, 10009], lr: 0.010000, loss: 1.8727
2022-03-06 18:57:09 - train: epoch 0054, iter [08200, 10009], lr: 0.010000, loss: 1.5704
2022-03-06 18:57:29 - train: epoch 0054, iter [08300, 10009], lr: 0.010000, loss: 1.6706
2022-03-06 18:57:49 - train: epoch 0054, iter [08400, 10009], lr: 0.010000, loss: 1.6241
2022-03-06 18:58:09 - train: epoch 0054, iter [08500, 10009], lr: 0.010000, loss: 1.8016
2022-03-06 18:58:29 - train: epoch 0054, iter [08600, 10009], lr: 0.010000, loss: 1.5030
2022-03-06 18:58:49 - train: epoch 0054, iter [08700, 10009], lr: 0.010000, loss: 1.5530
2022-03-06 18:59:09 - train: epoch 0054, iter [08800, 10009], lr: 0.010000, loss: 1.6118
2022-03-06 18:59:29 - train: epoch 0054, iter [08900, 10009], lr: 0.010000, loss: 1.9029
2022-03-06 18:59:49 - train: epoch 0054, iter [09000, 10009], lr: 0.010000, loss: 1.5608
2022-03-06 19:00:09 - train: epoch 0054, iter [09100, 10009], lr: 0.010000, loss: 1.8312
2022-03-06 19:00:29 - train: epoch 0054, iter [09200, 10009], lr: 0.010000, loss: 1.8486
2022-03-06 19:00:49 - train: epoch 0054, iter [09300, 10009], lr: 0.010000, loss: 1.8557
2022-03-06 19:01:09 - train: epoch 0054, iter [09400, 10009], lr: 0.010000, loss: 1.9451
2022-03-06 19:01:29 - train: epoch 0054, iter [09500, 10009], lr: 0.010000, loss: 1.4211
2022-03-06 19:01:49 - train: epoch 0054, iter [09600, 10009], lr: 0.010000, loss: 1.6583
2022-03-06 19:02:09 - train: epoch 0054, iter [09700, 10009], lr: 0.010000, loss: 1.7811
2022-03-06 19:02:29 - train: epoch 0054, iter [09800, 10009], lr: 0.010000, loss: 1.8474
2022-03-06 19:02:49 - train: epoch 0054, iter [09900, 10009], lr: 0.010000, loss: 1.7578
2022-03-06 19:03:09 - train: epoch 0054, iter [10000, 10009], lr: 0.010000, loss: 1.6313
2022-03-06 19:03:12 - train: epoch 054, train_loss: 1.6907
2022-03-06 19:04:26 - eval: epoch: 054, acc1: 65.854%, acc5: 87.134%, test_loss: 1.3991, per_image_load_time: 2.038ms, per_image_inference_time: 0.765ms
2022-03-06 19:04:27 - until epoch: 054, best_acc1: 65.902%
2022-03-06 19:04:27 - epoch 055 lr: 0.010000000000000002
2022-03-06 19:04:50 - train: epoch 0055, iter [00100, 10009], lr: 0.010000, loss: 1.8640
2022-03-06 19:05:10 - train: epoch 0055, iter [00200, 10009], lr: 0.010000, loss: 1.7548
2022-03-06 19:05:29 - train: epoch 0055, iter [00300, 10009], lr: 0.010000, loss: 1.5575
2022-03-06 19:05:49 - train: epoch 0055, iter [00400, 10009], lr: 0.010000, loss: 1.5573
2022-03-06 19:06:09 - train: epoch 0055, iter [00500, 10009], lr: 0.010000, loss: 1.6474
2022-03-06 19:06:28 - train: epoch 0055, iter [00600, 10009], lr: 0.010000, loss: 1.5172
2022-03-06 19:06:48 - train: epoch 0055, iter [00700, 10009], lr: 0.010000, loss: 1.6692
2022-03-06 19:07:08 - train: epoch 0055, iter [00800, 10009], lr: 0.010000, loss: 1.5228
2022-03-06 19:07:28 - train: epoch 0055, iter [00900, 10009], lr: 0.010000, loss: 1.8024
2022-03-06 19:07:47 - train: epoch 0055, iter [01000, 10009], lr: 0.010000, loss: 1.4521
2022-03-06 19:08:07 - train: epoch 0055, iter [01100, 10009], lr: 0.010000, loss: 1.5239
2022-03-06 19:08:27 - train: epoch 0055, iter [01200, 10009], lr: 0.010000, loss: 1.9211
2022-03-06 19:08:47 - train: epoch 0055, iter [01300, 10009], lr: 0.010000, loss: 1.4687
2022-03-06 19:09:07 - train: epoch 0055, iter [01400, 10009], lr: 0.010000, loss: 1.8910
2022-03-06 19:09:26 - train: epoch 0055, iter [01500, 10009], lr: 0.010000, loss: 1.6353
2022-03-06 19:09:46 - train: epoch 0055, iter [01600, 10009], lr: 0.010000, loss: 1.6271
2022-03-06 19:10:06 - train: epoch 0055, iter [01700, 10009], lr: 0.010000, loss: 1.5784
2022-03-06 19:10:26 - train: epoch 0055, iter [01800, 10009], lr: 0.010000, loss: 1.5803
2022-03-06 19:10:46 - train: epoch 0055, iter [01900, 10009], lr: 0.010000, loss: 1.5192
2022-03-06 19:11:06 - train: epoch 0055, iter [02000, 10009], lr: 0.010000, loss: 1.7976
2022-03-06 19:11:26 - train: epoch 0055, iter [02100, 10009], lr: 0.010000, loss: 1.2718
2022-03-06 19:11:46 - train: epoch 0055, iter [02200, 10009], lr: 0.010000, loss: 1.6572
2022-03-06 19:12:06 - train: epoch 0055, iter [02300, 10009], lr: 0.010000, loss: 2.0578
2022-03-06 19:12:25 - train: epoch 0055, iter [02400, 10009], lr: 0.010000, loss: 1.7568
2022-03-06 19:12:45 - train: epoch 0055, iter [02500, 10009], lr: 0.010000, loss: 1.6162
2022-03-06 19:13:05 - train: epoch 0055, iter [02600, 10009], lr: 0.010000, loss: 1.8359
2022-03-06 19:13:25 - train: epoch 0055, iter [02700, 10009], lr: 0.010000, loss: 1.7278
2022-03-06 19:13:45 - train: epoch 0055, iter [02800, 10009], lr: 0.010000, loss: 1.3514
2022-03-06 19:14:05 - train: epoch 0055, iter [02900, 10009], lr: 0.010000, loss: 1.5814
2022-03-06 19:14:25 - train: epoch 0055, iter [03000, 10009], lr: 0.010000, loss: 1.6531
2022-03-06 19:14:44 - train: epoch 0055, iter [03100, 10009], lr: 0.010000, loss: 1.5806
2022-03-06 19:15:04 - train: epoch 0055, iter [03200, 10009], lr: 0.010000, loss: 1.5721
2022-03-06 19:15:24 - train: epoch 0055, iter [03300, 10009], lr: 0.010000, loss: 1.6702
2022-03-06 19:15:44 - train: epoch 0055, iter [03400, 10009], lr: 0.010000, loss: 2.1676
2022-03-06 19:16:04 - train: epoch 0055, iter [03500, 10009], lr: 0.010000, loss: 2.0374
2022-03-06 19:16:24 - train: epoch 0055, iter [03600, 10009], lr: 0.010000, loss: 1.6068
2022-03-06 19:16:44 - train: epoch 0055, iter [03700, 10009], lr: 0.010000, loss: 1.9489
2022-03-06 19:17:03 - train: epoch 0055, iter [03800, 10009], lr: 0.010000, loss: 1.6383
2022-03-06 19:17:23 - train: epoch 0055, iter [03900, 10009], lr: 0.010000, loss: 1.7883
2022-03-06 19:17:43 - train: epoch 0055, iter [04000, 10009], lr: 0.010000, loss: 1.4514
2022-03-06 19:18:03 - train: epoch 0055, iter [04100, 10009], lr: 0.010000, loss: 2.1269
2022-03-06 19:18:23 - train: epoch 0055, iter [04200, 10009], lr: 0.010000, loss: 1.3574
2022-03-06 19:18:43 - train: epoch 0055, iter [04300, 10009], lr: 0.010000, loss: 1.4444
2022-03-06 19:19:02 - train: epoch 0055, iter [04400, 10009], lr: 0.010000, loss: 1.8845
2022-03-06 19:19:22 - train: epoch 0055, iter [04500, 10009], lr: 0.010000, loss: 1.6055
2022-03-06 19:19:42 - train: epoch 0055, iter [04600, 10009], lr: 0.010000, loss: 1.4775
2022-03-06 19:20:02 - train: epoch 0055, iter [04700, 10009], lr: 0.010000, loss: 1.9058
2022-03-06 19:20:21 - train: epoch 0055, iter [04800, 10009], lr: 0.010000, loss: 1.6646
2022-03-06 19:20:41 - train: epoch 0055, iter [04900, 10009], lr: 0.010000, loss: 1.5506
2022-03-06 19:21:01 - train: epoch 0055, iter [05000, 10009], lr: 0.010000, loss: 1.5203
2022-03-06 19:21:21 - train: epoch 0055, iter [05100, 10009], lr: 0.010000, loss: 1.9392
2022-03-06 19:21:41 - train: epoch 0055, iter [05200, 10009], lr: 0.010000, loss: 1.5577
2022-03-06 19:22:01 - train: epoch 0055, iter [05300, 10009], lr: 0.010000, loss: 1.3174
2022-03-06 19:22:21 - train: epoch 0055, iter [05400, 10009], lr: 0.010000, loss: 1.7610
2022-03-06 19:22:40 - train: epoch 0055, iter [05500, 10009], lr: 0.010000, loss: 1.6751
2022-03-06 19:23:00 - train: epoch 0055, iter [05600, 10009], lr: 0.010000, loss: 1.5855
2022-03-06 19:23:20 - train: epoch 0055, iter [05700, 10009], lr: 0.010000, loss: 1.8614
2022-03-06 19:23:40 - train: epoch 0055, iter [05800, 10009], lr: 0.010000, loss: 1.5252
2022-03-06 19:24:00 - train: epoch 0055, iter [05900, 10009], lr: 0.010000, loss: 1.7885
2022-03-06 19:24:19 - train: epoch 0055, iter [06000, 10009], lr: 0.010000, loss: 1.7803
2022-03-06 19:24:39 - train: epoch 0055, iter [06100, 10009], lr: 0.010000, loss: 1.8332
2022-03-06 19:24:59 - train: epoch 0055, iter [06200, 10009], lr: 0.010000, loss: 1.6661
2022-03-06 19:25:19 - train: epoch 0055, iter [06300, 10009], lr: 0.010000, loss: 1.7089
2022-03-06 19:25:39 - train: epoch 0055, iter [06400, 10009], lr: 0.010000, loss: 1.8049
2022-03-06 19:25:59 - train: epoch 0055, iter [06500, 10009], lr: 0.010000, loss: 1.4017
2022-03-06 19:26:18 - train: epoch 0055, iter [06600, 10009], lr: 0.010000, loss: 1.3860
2022-03-06 19:26:38 - train: epoch 0055, iter [06700, 10009], lr: 0.010000, loss: 1.5678
2022-03-06 19:26:58 - train: epoch 0055, iter [06800, 10009], lr: 0.010000, loss: 1.2934
2022-03-06 19:27:18 - train: epoch 0055, iter [06900, 10009], lr: 0.010000, loss: 2.1013
2022-03-06 19:27:37 - train: epoch 0055, iter [07000, 10009], lr: 0.010000, loss: 1.5689
2022-03-06 19:27:57 - train: epoch 0055, iter [07100, 10009], lr: 0.010000, loss: 1.4277
2022-03-06 19:28:17 - train: epoch 0055, iter [07200, 10009], lr: 0.010000, loss: 1.7987
2022-03-06 19:28:37 - train: epoch 0055, iter [07300, 10009], lr: 0.010000, loss: 1.7799
2022-03-06 19:28:57 - train: epoch 0055, iter [07400, 10009], lr: 0.010000, loss: 1.6624
2022-03-06 19:29:16 - train: epoch 0055, iter [07500, 10009], lr: 0.010000, loss: 1.7166
2022-03-06 19:29:36 - train: epoch 0055, iter [07600, 10009], lr: 0.010000, loss: 1.7521
2022-03-06 19:29:56 - train: epoch 0055, iter [07700, 10009], lr: 0.010000, loss: 1.7037
2022-03-06 19:30:16 - train: epoch 0055, iter [07800, 10009], lr: 0.010000, loss: 1.8411
2022-03-06 19:30:36 - train: epoch 0055, iter [07900, 10009], lr: 0.010000, loss: 1.9894
2022-03-06 19:30:56 - train: epoch 0055, iter [08000, 10009], lr: 0.010000, loss: 1.6108
2022-03-06 19:31:16 - train: epoch 0055, iter [08100, 10009], lr: 0.010000, loss: 1.4170
2022-03-06 19:31:35 - train: epoch 0055, iter [08200, 10009], lr: 0.010000, loss: 1.8186
2022-03-06 19:31:55 - train: epoch 0055, iter [08300, 10009], lr: 0.010000, loss: 1.6657
2022-03-06 19:32:15 - train: epoch 0055, iter [08400, 10009], lr: 0.010000, loss: 1.9200
2022-03-06 19:32:35 - train: epoch 0055, iter [08500, 10009], lr: 0.010000, loss: 1.6169
2022-03-06 19:32:54 - train: epoch 0055, iter [08600, 10009], lr: 0.010000, loss: 1.7003
2022-03-06 19:33:14 - train: epoch 0055, iter [08700, 10009], lr: 0.010000, loss: 1.9743
2022-03-06 19:33:34 - train: epoch 0055, iter [08800, 10009], lr: 0.010000, loss: 1.8339
2022-03-06 19:33:54 - train: epoch 0055, iter [08900, 10009], lr: 0.010000, loss: 1.4445
2022-03-06 19:34:14 - train: epoch 0055, iter [09000, 10009], lr: 0.010000, loss: 1.5647
2022-03-06 19:34:33 - train: epoch 0055, iter [09100, 10009], lr: 0.010000, loss: 1.7004
2022-03-06 19:34:53 - train: epoch 0055, iter [09200, 10009], lr: 0.010000, loss: 1.5685
2022-03-06 19:35:13 - train: epoch 0055, iter [09300, 10009], lr: 0.010000, loss: 1.7473
2022-03-06 19:35:33 - train: epoch 0055, iter [09400, 10009], lr: 0.010000, loss: 1.6042
2022-03-06 19:35:52 - train: epoch 0055, iter [09500, 10009], lr: 0.010000, loss: 1.3658
2022-03-06 19:36:12 - train: epoch 0055, iter [09600, 10009], lr: 0.010000, loss: 1.7564
2022-03-06 19:36:32 - train: epoch 0055, iter [09700, 10009], lr: 0.010000, loss: 1.5415
2022-03-06 19:36:52 - train: epoch 0055, iter [09800, 10009], lr: 0.010000, loss: 1.8331
2022-03-06 19:37:12 - train: epoch 0055, iter [09900, 10009], lr: 0.010000, loss: 1.4757
2022-03-06 19:37:32 - train: epoch 0055, iter [10000, 10009], lr: 0.010000, loss: 1.7821
2022-03-06 19:37:34 - train: epoch 055, train_loss: 1.6888
2022-03-06 19:38:49 - eval: epoch: 055, acc1: 65.990%, acc5: 87.226%, test_loss: 1.3795, per_image_load_time: 2.009ms, per_image_inference_time: 0.808ms
2022-03-06 19:38:49 - until epoch: 055, best_acc1: 65.990%
2022-03-06 19:38:49 - epoch 056 lr: 0.010000000000000002
2022-03-06 19:39:13 - train: epoch 0056, iter [00100, 10009], lr: 0.010000, loss: 1.2852
2022-03-06 19:39:32 - train: epoch 0056, iter [00200, 10009], lr: 0.010000, loss: 1.5723
2022-03-06 19:39:52 - train: epoch 0056, iter [00300, 10009], lr: 0.010000, loss: 1.7937
2022-03-06 19:40:12 - train: epoch 0056, iter [00400, 10009], lr: 0.010000, loss: 1.7379
2022-03-06 19:40:31 - train: epoch 0056, iter [00500, 10009], lr: 0.010000, loss: 1.9585
2022-03-06 19:40:51 - train: epoch 0056, iter [00600, 10009], lr: 0.010000, loss: 1.6154
2022-03-06 19:41:11 - train: epoch 0056, iter [00700, 10009], lr: 0.010000, loss: 1.1999
2022-03-06 19:41:31 - train: epoch 0056, iter [00800, 10009], lr: 0.010000, loss: 2.0204
2022-03-06 19:41:50 - train: epoch 0056, iter [00900, 10009], lr: 0.010000, loss: 1.5860
2022-03-06 19:42:10 - train: epoch 0056, iter [01000, 10009], lr: 0.010000, loss: 1.9233
2022-03-06 19:42:30 - train: epoch 0056, iter [01100, 10009], lr: 0.010000, loss: 1.3620
2022-03-06 19:42:50 - train: epoch 0056, iter [01200, 10009], lr: 0.010000, loss: 1.6720
2022-03-06 19:43:10 - train: epoch 0056, iter [01300, 10009], lr: 0.010000, loss: 1.8797
2022-03-06 19:43:29 - train: epoch 0056, iter [01400, 10009], lr: 0.010000, loss: 1.7321
2022-03-06 19:43:49 - train: epoch 0056, iter [01500, 10009], lr: 0.010000, loss: 1.8654
2022-03-06 19:44:09 - train: epoch 0056, iter [01600, 10009], lr: 0.010000, loss: 1.8900
2022-03-06 19:44:29 - train: epoch 0056, iter [01700, 10009], lr: 0.010000, loss: 1.5996
2022-03-06 19:44:49 - train: epoch 0056, iter [01800, 10009], lr: 0.010000, loss: 1.9281
2022-03-06 19:45:09 - train: epoch 0056, iter [01900, 10009], lr: 0.010000, loss: 1.7574
2022-03-06 19:45:28 - train: epoch 0056, iter [02000, 10009], lr: 0.010000, loss: 1.6153
2022-03-06 19:45:48 - train: epoch 0056, iter [02100, 10009], lr: 0.010000, loss: 1.8407
2022-03-06 19:46:08 - train: epoch 0056, iter [02200, 10009], lr: 0.010000, loss: 1.2951
2022-03-06 19:46:28 - train: epoch 0056, iter [02300, 10009], lr: 0.010000, loss: 1.6783
2022-03-06 19:46:48 - train: epoch 0056, iter [02400, 10009], lr: 0.010000, loss: 1.7397
2022-03-06 19:47:08 - train: epoch 0056, iter [02500, 10009], lr: 0.010000, loss: 1.6389
2022-03-06 19:47:28 - train: epoch 0056, iter [02600, 10009], lr: 0.010000, loss: 1.9156
2022-03-06 19:47:48 - train: epoch 0056, iter [02700, 10009], lr: 0.010000, loss: 1.8356
2022-03-06 19:48:08 - train: epoch 0056, iter [02800, 10009], lr: 0.010000, loss: 1.4525
2022-03-06 19:48:28 - train: epoch 0056, iter [02900, 10009], lr: 0.010000, loss: 1.7212
2022-03-06 19:48:48 - train: epoch 0056, iter [03000, 10009], lr: 0.010000, loss: 1.7987
2022-03-06 19:49:08 - train: epoch 0056, iter [03100, 10009], lr: 0.010000, loss: 1.4616
2022-03-06 19:49:28 - train: epoch 0056, iter [03200, 10009], lr: 0.010000, loss: 1.7100
2022-03-06 19:49:48 - train: epoch 0056, iter [03300, 10009], lr: 0.010000, loss: 1.8577
2022-03-06 19:50:08 - train: epoch 0056, iter [03400, 10009], lr: 0.010000, loss: 1.9172
2022-03-06 19:50:28 - train: epoch 0056, iter [03500, 10009], lr: 0.010000, loss: 2.0946
2022-03-06 19:50:48 - train: epoch 0056, iter [03600, 10009], lr: 0.010000, loss: 1.8155
2022-03-06 19:51:07 - train: epoch 0056, iter [03700, 10009], lr: 0.010000, loss: 2.2111
2022-03-06 19:51:27 - train: epoch 0056, iter [03800, 10009], lr: 0.010000, loss: 2.0550
2022-03-06 19:51:47 - train: epoch 0056, iter [03900, 10009], lr: 0.010000, loss: 1.8387
2022-03-06 19:52:07 - train: epoch 0056, iter [04000, 10009], lr: 0.010000, loss: 1.5891
2022-03-06 19:52:27 - train: epoch 0056, iter [04100, 10009], lr: 0.010000, loss: 1.7980
2022-03-06 19:52:47 - train: epoch 0056, iter [04200, 10009], lr: 0.010000, loss: 1.8623
2022-03-06 19:53:07 - train: epoch 0056, iter [04300, 10009], lr: 0.010000, loss: 1.3613
2022-03-06 19:53:27 - train: epoch 0056, iter [04400, 10009], lr: 0.010000, loss: 1.8258
2022-03-06 19:53:47 - train: epoch 0056, iter [04500, 10009], lr: 0.010000, loss: 1.7516
2022-03-06 19:54:07 - train: epoch 0056, iter [04600, 10009], lr: 0.010000, loss: 1.6839
2022-03-06 19:54:27 - train: epoch 0056, iter [04700, 10009], lr: 0.010000, loss: 1.5059
2022-03-06 19:54:47 - train: epoch 0056, iter [04800, 10009], lr: 0.010000, loss: 1.6869
2022-03-06 19:55:07 - train: epoch 0056, iter [04900, 10009], lr: 0.010000, loss: 1.4187
2022-03-06 19:55:27 - train: epoch 0056, iter [05000, 10009], lr: 0.010000, loss: 1.7771
2022-03-06 19:55:47 - train: epoch 0056, iter [05100, 10009], lr: 0.010000, loss: 1.6579
2022-03-06 19:56:07 - train: epoch 0056, iter [05200, 10009], lr: 0.010000, loss: 1.5266
2022-03-06 19:56:27 - train: epoch 0056, iter [05300, 10009], lr: 0.010000, loss: 1.7970
2022-03-06 19:56:47 - train: epoch 0056, iter [05400, 10009], lr: 0.010000, loss: 1.5485
2022-03-06 19:57:07 - train: epoch 0056, iter [05500, 10009], lr: 0.010000, loss: 1.4437
2022-03-06 19:57:26 - train: epoch 0056, iter [05600, 10009], lr: 0.010000, loss: 1.5008
2022-03-06 19:57:46 - train: epoch 0056, iter [05700, 10009], lr: 0.010000, loss: 1.8680
2022-03-06 19:58:06 - train: epoch 0056, iter [05800, 10009], lr: 0.010000, loss: 1.7851
2022-03-06 19:58:26 - train: epoch 0056, iter [05900, 10009], lr: 0.010000, loss: 1.6187
2022-03-06 19:58:46 - train: epoch 0056, iter [06000, 10009], lr: 0.010000, loss: 1.8561
2022-03-06 19:59:06 - train: epoch 0056, iter [06100, 10009], lr: 0.010000, loss: 1.5664
2022-03-06 19:59:26 - train: epoch 0056, iter [06200, 10009], lr: 0.010000, loss: 1.6126
2022-03-06 19:59:46 - train: epoch 0056, iter [06300, 10009], lr: 0.010000, loss: 1.5240
2022-03-06 20:00:06 - train: epoch 0056, iter [06400, 10009], lr: 0.010000, loss: 1.5854
2022-03-06 20:00:26 - train: epoch 0056, iter [06500, 10009], lr: 0.010000, loss: 1.5040
2022-03-06 20:00:46 - train: epoch 0056, iter [06600, 10009], lr: 0.010000, loss: 1.6307
2022-03-06 20:01:06 - train: epoch 0056, iter [06700, 10009], lr: 0.010000, loss: 1.8748
2022-03-06 20:01:25 - train: epoch 0056, iter [06800, 10009], lr: 0.010000, loss: 1.7685
2022-03-06 20:01:45 - train: epoch 0056, iter [06900, 10009], lr: 0.010000, loss: 1.7349
2022-03-06 20:02:05 - train: epoch 0056, iter [07000, 10009], lr: 0.010000, loss: 1.3705
2022-03-06 20:02:25 - train: epoch 0056, iter [07100, 10009], lr: 0.010000, loss: 1.7165
2022-03-06 20:02:45 - train: epoch 0056, iter [07200, 10009], lr: 0.010000, loss: 1.4696
2022-03-06 20:03:05 - train: epoch 0056, iter [07300, 10009], lr: 0.010000, loss: 1.6092
2022-03-06 20:03:25 - train: epoch 0056, iter [07400, 10009], lr: 0.010000, loss: 1.8363
2022-03-06 20:03:45 - train: epoch 0056, iter [07500, 10009], lr: 0.010000, loss: 2.1131
2022-03-06 20:04:05 - train: epoch 0056, iter [07600, 10009], lr: 0.010000, loss: 1.8932
2022-03-06 20:04:25 - train: epoch 0056, iter [07700, 10009], lr: 0.010000, loss: 1.6706
2022-03-06 20:04:45 - train: epoch 0056, iter [07800, 10009], lr: 0.010000, loss: 1.7933
2022-03-06 20:05:05 - train: epoch 0056, iter [07900, 10009], lr: 0.010000, loss: 1.6411
2022-03-06 20:05:25 - train: epoch 0056, iter [08000, 10009], lr: 0.010000, loss: 1.3390
2022-03-06 20:05:45 - train: epoch 0056, iter [08100, 10009], lr: 0.010000, loss: 1.6243
2022-03-06 20:06:05 - train: epoch 0056, iter [08200, 10009], lr: 0.010000, loss: 1.8024
2022-03-06 20:06:25 - train: epoch 0056, iter [08300, 10009], lr: 0.010000, loss: 2.0060
2022-03-06 20:06:45 - train: epoch 0056, iter [08400, 10009], lr: 0.010000, loss: 1.8635
2022-03-06 20:07:05 - train: epoch 0056, iter [08500, 10009], lr: 0.010000, loss: 1.4829
2022-03-06 20:07:25 - train: epoch 0056, iter [08600, 10009], lr: 0.010000, loss: 1.6233
2022-03-06 20:07:45 - train: epoch 0056, iter [08700, 10009], lr: 0.010000, loss: 1.4612
2022-03-06 20:08:05 - train: epoch 0056, iter [08800, 10009], lr: 0.010000, loss: 2.0017
2022-03-06 20:08:25 - train: epoch 0056, iter [08900, 10009], lr: 0.010000, loss: 1.7456
2022-03-06 20:08:45 - train: epoch 0056, iter [09000, 10009], lr: 0.010000, loss: 1.6209
2022-03-06 20:09:05 - train: epoch 0056, iter [09100, 10009], lr: 0.010000, loss: 1.7156
2022-03-06 20:09:25 - train: epoch 0056, iter [09200, 10009], lr: 0.010000, loss: 1.6249
2022-03-06 20:09:45 - train: epoch 0056, iter [09300, 10009], lr: 0.010000, loss: 1.6411
2022-03-06 20:10:05 - train: epoch 0056, iter [09400, 10009], lr: 0.010000, loss: 1.5860
2022-03-06 20:10:25 - train: epoch 0056, iter [09500, 10009], lr: 0.010000, loss: 1.9485
2022-03-06 20:10:45 - train: epoch 0056, iter [09600, 10009], lr: 0.010000, loss: 1.9345
2022-03-06 20:11:05 - train: epoch 0056, iter [09700, 10009], lr: 0.010000, loss: 1.8534
2022-03-06 20:11:25 - train: epoch 0056, iter [09800, 10009], lr: 0.010000, loss: 1.5093
2022-03-06 20:11:45 - train: epoch 0056, iter [09900, 10009], lr: 0.010000, loss: 1.6302
2022-03-06 20:12:05 - train: epoch 0056, iter [10000, 10009], lr: 0.010000, loss: 1.6095
2022-03-06 20:12:07 - train: epoch 056, train_loss: 1.6850
2022-03-06 20:13:23 - eval: epoch: 056, acc1: 66.336%, acc5: 87.206%, test_loss: 1.3804, per_image_load_time: 0.841ms, per_image_inference_time: 0.815ms
2022-03-06 20:13:23 - until epoch: 056, best_acc1: 66.336%
2022-03-06 20:13:23 - epoch 057 lr: 0.010000000000000002
2022-03-06 20:13:47 - train: epoch 0057, iter [00100, 10009], lr: 0.010000, loss: 1.4512
2022-03-06 20:14:15 - train: epoch 0057, iter [00200, 10009], lr: 0.010000, loss: 1.5202
2022-03-06 20:14:35 - train: epoch 0057, iter [00300, 10009], lr: 0.010000, loss: 1.4701
2022-03-06 20:14:54 - train: epoch 0057, iter [00400, 10009], lr: 0.010000, loss: 1.8308
2022-03-06 20:15:14 - train: epoch 0057, iter [00500, 10009], lr: 0.010000, loss: 1.6031
2022-03-06 20:15:34 - train: epoch 0057, iter [00600, 10009], lr: 0.010000, loss: 2.1188
2022-03-06 20:15:53 - train: epoch 0057, iter [00700, 10009], lr: 0.010000, loss: 1.5704
2022-03-06 20:16:13 - train: epoch 0057, iter [00800, 10009], lr: 0.010000, loss: 1.8399
2022-03-06 20:16:33 - train: epoch 0057, iter [00900, 10009], lr: 0.010000, loss: 1.7126
2022-03-06 20:16:55 - train: epoch 0057, iter [01000, 10009], lr: 0.010000, loss: 1.6280
2022-03-06 20:17:15 - train: epoch 0057, iter [01100, 10009], lr: 0.010000, loss: 1.7988
2022-03-06 20:17:34 - train: epoch 0057, iter [01200, 10009], lr: 0.010000, loss: 1.7233
2022-03-06 20:17:54 - train: epoch 0057, iter [01300, 10009], lr: 0.010000, loss: 1.4997
2022-03-06 20:18:16 - train: epoch 0057, iter [01400, 10009], lr: 0.010000, loss: 1.4546
2022-03-06 20:18:36 - train: epoch 0057, iter [01500, 10009], lr: 0.010000, loss: 1.7207
2022-03-06 20:18:55 - train: epoch 0057, iter [01600, 10009], lr: 0.010000, loss: 1.6109
2022-03-06 20:19:15 - train: epoch 0057, iter [01700, 10009], lr: 0.010000, loss: 1.6897
2022-03-06 20:19:34 - train: epoch 0057, iter [01800, 10009], lr: 0.010000, loss: 1.6207
2022-03-06 20:19:54 - train: epoch 0057, iter [01900, 10009], lr: 0.010000, loss: 2.0155
2022-03-06 20:20:13 - train: epoch 0057, iter [02000, 10009], lr: 0.010000, loss: 1.4907
2022-03-06 20:20:33 - train: epoch 0057, iter [02100, 10009], lr: 0.010000, loss: 1.7275
2022-03-06 20:20:52 - train: epoch 0057, iter [02200, 10009], lr: 0.010000, loss: 1.7445
2022-03-06 20:21:12 - train: epoch 0057, iter [02300, 10009], lr: 0.010000, loss: 1.8621
2022-03-06 20:21:32 - train: epoch 0057, iter [02400, 10009], lr: 0.010000, loss: 1.6279
2022-03-06 20:21:51 - train: epoch 0057, iter [02500, 10009], lr: 0.010000, loss: 1.2468
2022-03-06 20:22:11 - train: epoch 0057, iter [02600, 10009], lr: 0.010000, loss: 1.6530
2022-03-06 20:22:31 - train: epoch 0057, iter [02700, 10009], lr: 0.010000, loss: 1.6751
2022-03-06 20:22:50 - train: epoch 0057, iter [02800, 10009], lr: 0.010000, loss: 1.7539
2022-03-06 20:23:10 - train: epoch 0057, iter [02900, 10009], lr: 0.010000, loss: 1.7748
2022-03-06 20:23:30 - train: epoch 0057, iter [03000, 10009], lr: 0.010000, loss: 1.5854
2022-03-06 20:23:50 - train: epoch 0057, iter [03100, 10009], lr: 0.010000, loss: 2.0140
2022-03-06 20:24:09 - train: epoch 0057, iter [03200, 10009], lr: 0.010000, loss: 1.8141
2022-03-06 20:24:29 - train: epoch 0057, iter [03300, 10009], lr: 0.010000, loss: 1.7208
2022-03-06 20:24:49 - train: epoch 0057, iter [03400, 10009], lr: 0.010000, loss: 1.9042
2022-03-06 20:25:08 - train: epoch 0057, iter [03500, 10009], lr: 0.010000, loss: 1.8254
2022-03-06 20:25:28 - train: epoch 0057, iter [03600, 10009], lr: 0.010000, loss: 1.6040
2022-03-06 20:25:47 - train: epoch 0057, iter [03700, 10009], lr: 0.010000, loss: 1.6438
2022-03-06 20:26:07 - train: epoch 0057, iter [03800, 10009], lr: 0.010000, loss: 1.8556
2022-03-06 20:26:27 - train: epoch 0057, iter [03900, 10009], lr: 0.010000, loss: 1.4206
2022-03-06 20:26:47 - train: epoch 0057, iter [04000, 10009], lr: 0.010000, loss: 1.9319
2022-03-06 20:27:06 - train: epoch 0057, iter [04100, 10009], lr: 0.010000, loss: 1.6352
2022-03-06 20:27:26 - train: epoch 0057, iter [04200, 10009], lr: 0.010000, loss: 1.6131
2022-03-06 20:27:46 - train: epoch 0057, iter [04300, 10009], lr: 0.010000, loss: 1.8557
2022-03-06 20:28:06 - train: epoch 0057, iter [04400, 10009], lr: 0.010000, loss: 1.8423
2022-03-06 20:28:25 - train: epoch 0057, iter [04500, 10009], lr: 0.010000, loss: 1.4487
2022-03-06 20:28:45 - train: epoch 0057, iter [04600, 10009], lr: 0.010000, loss: 1.5901
2022-03-06 20:29:05 - train: epoch 0057, iter [04700, 10009], lr: 0.010000, loss: 1.8176
2022-03-06 20:29:25 - train: epoch 0057, iter [04800, 10009], lr: 0.010000, loss: 1.5644
2022-03-06 20:29:45 - train: epoch 0057, iter [04900, 10009], lr: 0.010000, loss: 1.7923
2022-03-06 20:30:05 - train: epoch 0057, iter [05000, 10009], lr: 0.010000, loss: 1.8590
2022-03-06 20:30:27 - train: epoch 0057, iter [05100, 10009], lr: 0.010000, loss: 1.6803
2022-03-06 20:30:55 - train: epoch 0057, iter [05200, 10009], lr: 0.010000, loss: 1.4936
2022-03-06 20:31:15 - train: epoch 0057, iter [05300, 10009], lr: 0.010000, loss: 2.0150
2022-03-06 20:31:35 - train: epoch 0057, iter [05400, 10009], lr: 0.010000, loss: 1.7146
2022-03-06 20:31:55 - train: epoch 0057, iter [05500, 10009], lr: 0.010000, loss: 1.7042
2022-03-06 20:32:15 - train: epoch 0057, iter [05600, 10009], lr: 0.010000, loss: 1.3817
2022-03-06 20:32:35 - train: epoch 0057, iter [05700, 10009], lr: 0.010000, loss: 1.3305
2022-03-06 20:32:54 - train: epoch 0057, iter [05800, 10009], lr: 0.010000, loss: 1.5548
2022-03-06 20:33:14 - train: epoch 0057, iter [05900, 10009], lr: 0.010000, loss: 1.5511
2022-03-06 20:33:34 - train: epoch 0057, iter [06000, 10009], lr: 0.010000, loss: 1.8876
2022-03-06 20:34:06 - train: epoch 0057, iter [06100, 10009], lr: 0.010000, loss: 1.9416
2022-03-06 20:34:26 - train: epoch 0057, iter [06200, 10009], lr: 0.010000, loss: 2.1205
2022-03-06 20:34:46 - train: epoch 0057, iter [06300, 10009], lr: 0.010000, loss: 1.8374
2022-03-06 20:35:06 - train: epoch 0057, iter [06400, 10009], lr: 0.010000, loss: 1.8259
2022-03-06 20:35:25 - train: epoch 0057, iter [06500, 10009], lr: 0.010000, loss: 1.7058
2022-03-06 20:35:45 - train: epoch 0057, iter [06600, 10009], lr: 0.010000, loss: 1.8935
2022-03-06 20:36:17 - train: epoch 0057, iter [06700, 10009], lr: 0.010000, loss: 1.8516
2022-03-06 20:36:48 - train: epoch 0057, iter [06800, 10009], lr: 0.010000, loss: 1.7523
2022-03-06 20:37:08 - train: epoch 0057, iter [06900, 10009], lr: 0.010000, loss: 1.4310
2022-03-06 20:37:28 - train: epoch 0057, iter [07000, 10009], lr: 0.010000, loss: 1.8239
2022-03-06 20:37:48 - train: epoch 0057, iter [07100, 10009], lr: 0.010000, loss: 1.8666
2022-03-06 20:38:08 - train: epoch 0057, iter [07200, 10009], lr: 0.010000, loss: 1.5788
2022-03-06 20:38:46 - train: epoch 0057, iter [07300, 10009], lr: 0.010000, loss: 1.5765
2022-03-06 20:39:28 - train: epoch 0057, iter [07400, 10009], lr: 0.010000, loss: 1.7937
2022-03-06 20:39:53 - train: epoch 0057, iter [07500, 10009], lr: 0.010000, loss: 1.8985
2022-03-06 20:40:13 - train: epoch 0057, iter [07600, 10009], lr: 0.010000, loss: 1.3992
2022-03-06 20:40:33 - train: epoch 0057, iter [07700, 10009], lr: 0.010000, loss: 1.9365
2022-03-06 20:40:53 - train: epoch 0057, iter [07800, 10009], lr: 0.010000, loss: 1.5811
2022-03-06 20:41:13 - train: epoch 0057, iter [07900, 10009], lr: 0.010000, loss: 1.5617
2022-03-06 20:41:33 - train: epoch 0057, iter [08000, 10009], lr: 0.010000, loss: 1.9369
2022-03-06 20:42:07 - train: epoch 0057, iter [08100, 10009], lr: 0.010000, loss: 1.7584
2022-03-06 20:42:51 - train: epoch 0057, iter [08200, 10009], lr: 0.010000, loss: 1.8372
2022-03-06 20:43:14 - train: epoch 0057, iter [08300, 10009], lr: 0.010000, loss: 1.6173
2022-03-06 20:43:34 - train: epoch 0057, iter [08400, 10009], lr: 0.010000, loss: 1.7144
2022-03-06 20:43:54 - train: epoch 0057, iter [08500, 10009], lr: 0.010000, loss: 1.7018
2022-03-06 20:44:13 - train: epoch 0057, iter [08600, 10009], lr: 0.010000, loss: 1.5624
2022-03-06 20:44:34 - train: epoch 0057, iter [08700, 10009], lr: 0.010000, loss: 1.8171
2022-03-06 20:44:54 - train: epoch 0057, iter [08800, 10009], lr: 0.010000, loss: 1.8261
2022-03-06 20:45:14 - train: epoch 0057, iter [08900, 10009], lr: 0.010000, loss: 1.6285
2022-03-06 20:45:34 - train: epoch 0057, iter [09000, 10009], lr: 0.010000, loss: 1.5832
2022-03-06 20:45:54 - train: epoch 0057, iter [09100, 10009], lr: 0.010000, loss: 1.6723
2022-03-06 20:46:14 - train: epoch 0057, iter [09200, 10009], lr: 0.010000, loss: 1.6878
2022-03-06 20:46:34 - train: epoch 0057, iter [09300, 10009], lr: 0.010000, loss: 1.3669
2022-03-06 20:46:54 - train: epoch 0057, iter [09400, 10009], lr: 0.010000, loss: 1.7540
2022-03-06 20:47:13 - train: epoch 0057, iter [09500, 10009], lr: 0.010000, loss: 2.0061
2022-03-06 20:47:33 - train: epoch 0057, iter [09600, 10009], lr: 0.010000, loss: 1.9735
2022-03-06 20:47:53 - train: epoch 0057, iter [09700, 10009], lr: 0.010000, loss: 1.7138
2022-03-06 20:48:13 - train: epoch 0057, iter [09800, 10009], lr: 0.010000, loss: 1.9119
2022-03-06 20:48:33 - train: epoch 0057, iter [09900, 10009], lr: 0.010000, loss: 1.5080
2022-03-06 20:48:53 - train: epoch 0057, iter [10000, 10009], lr: 0.010000, loss: 1.7187
2022-03-06 20:48:55 - train: epoch 057, train_loss: 1.6791
2022-03-06 20:50:10 - eval: epoch: 057, acc1: 66.350%, acc5: 87.132%, test_loss: 1.3820, per_image_load_time: 2.028ms, per_image_inference_time: 0.798ms
2022-03-06 20:50:11 - until epoch: 057, best_acc1: 66.350%
2022-03-06 20:50:11 - epoch 058 lr: 0.010000000000000002
2022-03-06 20:50:34 - train: epoch 0058, iter [00100, 10009], lr: 0.010000, loss: 1.4052
2022-03-06 20:50:54 - train: epoch 0058, iter [00200, 10009], lr: 0.010000, loss: 1.6650
2022-03-06 20:51:14 - train: epoch 0058, iter [00300, 10009], lr: 0.010000, loss: 1.9630
2022-03-06 20:51:33 - train: epoch 0058, iter [00400, 10009], lr: 0.010000, loss: 1.6611
2022-03-06 20:51:53 - train: epoch 0058, iter [00500, 10009], lr: 0.010000, loss: 1.5535
2022-03-06 20:52:13 - train: epoch 0058, iter [00600, 10009], lr: 0.010000, loss: 1.7333
2022-03-06 20:52:33 - train: epoch 0058, iter [00700, 10009], lr: 0.010000, loss: 1.6292
2022-03-06 20:52:52 - train: epoch 0058, iter [00800, 10009], lr: 0.010000, loss: 1.7367
2022-03-06 20:53:12 - train: epoch 0058, iter [00900, 10009], lr: 0.010000, loss: 1.6372
2022-03-06 20:53:32 - train: epoch 0058, iter [01000, 10009], lr: 0.010000, loss: 1.6305
2022-03-06 20:53:51 - train: epoch 0058, iter [01100, 10009], lr: 0.010000, loss: 1.5329
2022-03-06 20:54:11 - train: epoch 0058, iter [01200, 10009], lr: 0.010000, loss: 2.1112
2022-03-06 20:54:31 - train: epoch 0058, iter [01300, 10009], lr: 0.010000, loss: 1.8144
2022-03-06 20:54:50 - train: epoch 0058, iter [01400, 10009], lr: 0.010000, loss: 1.5121
2022-03-06 20:55:10 - train: epoch 0058, iter [01500, 10009], lr: 0.010000, loss: 1.5019
2022-03-06 20:55:30 - train: epoch 0058, iter [01600, 10009], lr: 0.010000, loss: 1.5900
2022-03-06 20:55:49 - train: epoch 0058, iter [01700, 10009], lr: 0.010000, loss: 1.3612
2022-03-06 20:56:09 - train: epoch 0058, iter [01800, 10009], lr: 0.010000, loss: 1.7177
2022-03-06 20:56:29 - train: epoch 0058, iter [01900, 10009], lr: 0.010000, loss: 1.9421
2022-03-06 20:56:48 - train: epoch 0058, iter [02000, 10009], lr: 0.010000, loss: 1.4381
2022-03-06 20:57:08 - train: epoch 0058, iter [02100, 10009], lr: 0.010000, loss: 1.4240
2022-03-06 20:57:28 - train: epoch 0058, iter [02200, 10009], lr: 0.010000, loss: 1.4110
2022-03-06 20:57:47 - train: epoch 0058, iter [02300, 10009], lr: 0.010000, loss: 1.6464
2022-03-06 20:58:07 - train: epoch 0058, iter [02400, 10009], lr: 0.010000, loss: 1.5676
2022-03-06 20:58:27 - train: epoch 0058, iter [02500, 10009], lr: 0.010000, loss: 1.8148
2022-03-06 20:58:46 - train: epoch 0058, iter [02600, 10009], lr: 0.010000, loss: 1.9371
2022-03-06 20:59:06 - train: epoch 0058, iter [02700, 10009], lr: 0.010000, loss: 1.5365
2022-03-06 20:59:26 - train: epoch 0058, iter [02800, 10009], lr: 0.010000, loss: 1.5397
2022-03-06 20:59:46 - train: epoch 0058, iter [02900, 10009], lr: 0.010000, loss: 1.8867
2022-03-06 21:00:05 - train: epoch 0058, iter [03000, 10009], lr: 0.010000, loss: 1.4297
2022-03-06 21:00:25 - train: epoch 0058, iter [03100, 10009], lr: 0.010000, loss: 1.8232
2022-03-06 21:00:45 - train: epoch 0058, iter [03200, 10009], lr: 0.010000, loss: 1.4671
2022-03-06 21:01:04 - train: epoch 0058, iter [03300, 10009], lr: 0.010000, loss: 2.1703
2022-03-06 21:01:24 - train: epoch 0058, iter [03400, 10009], lr: 0.010000, loss: 1.9280
2022-03-06 21:01:44 - train: epoch 0058, iter [03500, 10009], lr: 0.010000, loss: 1.4030
2022-03-06 21:02:04 - train: epoch 0058, iter [03600, 10009], lr: 0.010000, loss: 1.7745
2022-03-06 21:02:23 - train: epoch 0058, iter [03700, 10009], lr: 0.010000, loss: 1.3870
2022-03-06 21:02:43 - train: epoch 0058, iter [03800, 10009], lr: 0.010000, loss: 1.8987
2022-03-06 21:03:03 - train: epoch 0058, iter [03900, 10009], lr: 0.010000, loss: 1.6072
2022-03-06 21:03:22 - train: epoch 0058, iter [04000, 10009], lr: 0.010000, loss: 1.9797
2022-03-06 21:03:42 - train: epoch 0058, iter [04100, 10009], lr: 0.010000, loss: 1.6679
2022-03-06 21:04:02 - train: epoch 0058, iter [04200, 10009], lr: 0.010000, loss: 1.4845
2022-03-06 21:04:21 - train: epoch 0058, iter [04300, 10009], lr: 0.010000, loss: 2.0092
2022-03-06 21:04:41 - train: epoch 0058, iter [04400, 10009], lr: 0.010000, loss: 1.7600
2022-03-06 21:05:01 - train: epoch 0058, iter [04500, 10009], lr: 0.010000, loss: 1.9058
2022-03-06 21:05:21 - train: epoch 0058, iter [04600, 10009], lr: 0.010000, loss: 1.7771
2022-03-06 21:05:40 - train: epoch 0058, iter [04700, 10009], lr: 0.010000, loss: 1.5410
2022-03-06 21:06:00 - train: epoch 0058, iter [04800, 10009], lr: 0.010000, loss: 1.6582
2022-03-06 21:06:20 - train: epoch 0058, iter [04900, 10009], lr: 0.010000, loss: 1.6217
2022-03-06 21:06:40 - train: epoch 0058, iter [05000, 10009], lr: 0.010000, loss: 1.7931
2022-03-06 21:06:59 - train: epoch 0058, iter [05100, 10009], lr: 0.010000, loss: 1.8333
2022-03-06 21:07:19 - train: epoch 0058, iter [05200, 10009], lr: 0.010000, loss: 1.5705
2022-03-06 21:07:39 - train: epoch 0058, iter [05300, 10009], lr: 0.010000, loss: 1.5889
2022-03-06 21:07:59 - train: epoch 0058, iter [05400, 10009], lr: 0.010000, loss: 1.9922
2022-03-06 21:08:18 - train: epoch 0058, iter [05500, 10009], lr: 0.010000, loss: 1.9534
2022-03-06 21:08:38 - train: epoch 0058, iter [05600, 10009], lr: 0.010000, loss: 1.7796
2022-03-06 21:08:58 - train: epoch 0058, iter [05700, 10009], lr: 0.010000, loss: 1.6062
2022-03-06 21:09:18 - train: epoch 0058, iter [05800, 10009], lr: 0.010000, loss: 1.4106
2022-03-06 21:09:37 - train: epoch 0058, iter [05900, 10009], lr: 0.010000, loss: 1.5893
2022-03-06 21:09:57 - train: epoch 0058, iter [06000, 10009], lr: 0.010000, loss: 1.4116
2022-03-06 21:10:17 - train: epoch 0058, iter [06100, 10009], lr: 0.010000, loss: 1.6098
2022-03-06 21:10:37 - train: epoch 0058, iter [06200, 10009], lr: 0.010000, loss: 1.6826
2022-03-06 21:10:56 - train: epoch 0058, iter [06300, 10009], lr: 0.010000, loss: 1.3936
2022-03-06 21:11:16 - train: epoch 0058, iter [06400, 10009], lr: 0.010000, loss: 1.5574
2022-03-06 21:11:36 - train: epoch 0058, iter [06500, 10009], lr: 0.010000, loss: 1.6382
2022-03-06 21:11:56 - train: epoch 0058, iter [06600, 10009], lr: 0.010000, loss: 1.9540
2022-03-06 21:12:16 - train: epoch 0058, iter [06700, 10009], lr: 0.010000, loss: 1.3149
2022-03-06 21:12:35 - train: epoch 0058, iter [06800, 10009], lr: 0.010000, loss: 1.7639
2022-03-06 21:12:55 - train: epoch 0058, iter [06900, 10009], lr: 0.010000, loss: 1.3380
2022-03-06 21:13:15 - train: epoch 0058, iter [07000, 10009], lr: 0.010000, loss: 1.4234
2022-03-06 21:13:35 - train: epoch 0058, iter [07100, 10009], lr: 0.010000, loss: 1.7816
2022-03-06 21:13:55 - train: epoch 0058, iter [07200, 10009], lr: 0.010000, loss: 1.6507
2022-03-06 21:14:15 - train: epoch 0058, iter [07300, 10009], lr: 0.010000, loss: 1.6134
2022-03-06 21:14:35 - train: epoch 0058, iter [07400, 10009], lr: 0.010000, loss: 1.9150
2022-03-06 21:14:55 - train: epoch 0058, iter [07500, 10009], lr: 0.010000, loss: 1.6006
2022-03-06 21:15:14 - train: epoch 0058, iter [07600, 10009], lr: 0.010000, loss: 1.4964
2022-03-06 21:15:34 - train: epoch 0058, iter [07700, 10009], lr: 0.010000, loss: 1.5899
2022-03-06 21:15:54 - train: epoch 0058, iter [07800, 10009], lr: 0.010000, loss: 1.9073
2022-03-06 21:16:14 - train: epoch 0058, iter [07900, 10009], lr: 0.010000, loss: 1.8093
2022-03-06 21:16:34 - train: epoch 0058, iter [08000, 10009], lr: 0.010000, loss: 1.8084
2022-03-06 21:16:54 - train: epoch 0058, iter [08100, 10009], lr: 0.010000, loss: 1.5233
2022-03-06 21:17:14 - train: epoch 0058, iter [08200, 10009], lr: 0.010000, loss: 1.6262
2022-03-06 21:17:34 - train: epoch 0058, iter [08300, 10009], lr: 0.010000, loss: 1.8740
2022-03-06 21:17:54 - train: epoch 0058, iter [08400, 10009], lr: 0.010000, loss: 1.8237
2022-03-06 21:18:14 - train: epoch 0058, iter [08500, 10009], lr: 0.010000, loss: 1.9029
2022-03-06 21:18:34 - train: epoch 0058, iter [08600, 10009], lr: 0.010000, loss: 1.7118
2022-03-06 21:18:54 - train: epoch 0058, iter [08700, 10009], lr: 0.010000, loss: 1.8394
2022-03-06 21:19:14 - train: epoch 0058, iter [08800, 10009], lr: 0.010000, loss: 1.7410
2022-03-06 21:19:34 - train: epoch 0058, iter [08900, 10009], lr: 0.010000, loss: 1.5234
2022-03-06 21:19:54 - train: epoch 0058, iter [09000, 10009], lr: 0.010000, loss: 1.6147
2022-03-06 21:20:14 - train: epoch 0058, iter [09100, 10009], lr: 0.010000, loss: 1.7970
2022-03-06 21:20:33 - train: epoch 0058, iter [09200, 10009], lr: 0.010000, loss: 1.4368
2022-03-06 21:20:53 - train: epoch 0058, iter [09300, 10009], lr: 0.010000, loss: 2.0529
2022-03-06 21:21:13 - train: epoch 0058, iter [09400, 10009], lr: 0.010000, loss: 1.8870
2022-03-06 21:21:33 - train: epoch 0058, iter [09500, 10009], lr: 0.010000, loss: 1.4004
2022-03-06 21:21:53 - train: epoch 0058, iter [09600, 10009], lr: 0.010000, loss: 1.5574
2022-03-06 21:22:13 - train: epoch 0058, iter [09700, 10009], lr: 0.010000, loss: 1.7252
2022-03-06 21:22:33 - train: epoch 0058, iter [09800, 10009], lr: 0.010000, loss: 1.7473
2022-03-06 21:22:53 - train: epoch 0058, iter [09900, 10009], lr: 0.010000, loss: 1.6834
2022-03-06 21:23:13 - train: epoch 0058, iter [10000, 10009], lr: 0.010000, loss: 1.4930
2022-03-06 21:23:15 - train: epoch 058, train_loss: 1.6768
2022-03-06 21:24:30 - eval: epoch: 058, acc1: 66.626%, acc5: 87.248%, test_loss: 1.3736, per_image_load_time: 1.390ms, per_image_inference_time: 0.830ms
2022-03-06 21:24:31 - until epoch: 058, best_acc1: 66.626%
2022-03-06 21:24:31 - epoch 059 lr: 0.010000000000000002
2022-03-06 21:24:54 - train: epoch 0059, iter [00100, 10009], lr: 0.010000, loss: 1.5516
2022-03-06 21:25:14 - train: epoch 0059, iter [00200, 10009], lr: 0.010000, loss: 1.7396
2022-03-06 21:25:33 - train: epoch 0059, iter [00300, 10009], lr: 0.010000, loss: 2.0069
2022-03-06 21:25:53 - train: epoch 0059, iter [00400, 10009], lr: 0.010000, loss: 1.4193
2022-03-06 21:26:12 - train: epoch 0059, iter [00500, 10009], lr: 0.010000, loss: 1.5768
2022-03-06 21:26:32 - train: epoch 0059, iter [00600, 10009], lr: 0.010000, loss: 1.4641
2022-03-06 21:26:51 - train: epoch 0059, iter [00700, 10009], lr: 0.010000, loss: 1.6589
2022-03-06 21:27:11 - train: epoch 0059, iter [00800, 10009], lr: 0.010000, loss: 1.4255
2022-03-06 21:27:31 - train: epoch 0059, iter [00900, 10009], lr: 0.010000, loss: 1.7915
2022-03-06 21:27:50 - train: epoch 0059, iter [01000, 10009], lr: 0.010000, loss: 1.7840
2022-03-06 21:28:10 - train: epoch 0059, iter [01100, 10009], lr: 0.010000, loss: 2.0047
2022-03-06 21:28:29 - train: epoch 0059, iter [01200, 10009], lr: 0.010000, loss: 1.5418
2022-03-06 21:28:49 - train: epoch 0059, iter [01300, 10009], lr: 0.010000, loss: 1.8693
2022-03-06 21:29:09 - train: epoch 0059, iter [01400, 10009], lr: 0.010000, loss: 1.6611
2022-03-06 21:29:28 - train: epoch 0059, iter [01500, 10009], lr: 0.010000, loss: 2.0563
2022-03-06 21:29:48 - train: epoch 0059, iter [01600, 10009], lr: 0.010000, loss: 1.6622
2022-03-06 21:30:08 - train: epoch 0059, iter [01700, 10009], lr: 0.010000, loss: 1.4840
2022-03-06 21:30:27 - train: epoch 0059, iter [01800, 10009], lr: 0.010000, loss: 1.6944
2022-03-06 21:30:47 - train: epoch 0059, iter [01900, 10009], lr: 0.010000, loss: 1.6648
2022-03-06 21:31:07 - train: epoch 0059, iter [02000, 10009], lr: 0.010000, loss: 1.6823
2022-03-06 21:31:26 - train: epoch 0059, iter [02100, 10009], lr: 0.010000, loss: 1.7004
2022-03-06 21:31:46 - train: epoch 0059, iter [02200, 10009], lr: 0.010000, loss: 2.0005
2022-03-06 21:32:06 - train: epoch 0059, iter [02300, 10009], lr: 0.010000, loss: 1.6113
2022-03-06 21:32:26 - train: epoch 0059, iter [02400, 10009], lr: 0.010000, loss: 1.3015
2022-03-06 21:32:46 - train: epoch 0059, iter [02500, 10009], lr: 0.010000, loss: 1.5450
2022-03-06 21:33:05 - train: epoch 0059, iter [02600, 10009], lr: 0.010000, loss: 1.7276
2022-03-06 21:33:25 - train: epoch 0059, iter [02700, 10009], lr: 0.010000, loss: 1.5437
2022-03-06 21:33:45 - train: epoch 0059, iter [02800, 10009], lr: 0.010000, loss: 1.7427
2022-03-06 21:34:05 - train: epoch 0059, iter [02900, 10009], lr: 0.010000, loss: 1.5766
2022-03-06 21:34:25 - train: epoch 0059, iter [03000, 10009], lr: 0.010000, loss: 1.7569
2022-03-06 21:34:44 - train: epoch 0059, iter [03100, 10009], lr: 0.010000, loss: 1.8252
2022-03-06 21:35:04 - train: epoch 0059, iter [03200, 10009], lr: 0.010000, loss: 1.4828
2022-03-06 21:35:24 - train: epoch 0059, iter [03300, 10009], lr: 0.010000, loss: 1.9082
2022-03-06 21:35:44 - train: epoch 0059, iter [03400, 10009], lr: 0.010000, loss: 1.8666
2022-03-06 21:36:04 - train: epoch 0059, iter [03500, 10009], lr: 0.010000, loss: 1.7647
2022-03-06 21:36:23 - train: epoch 0059, iter [03600, 10009], lr: 0.010000, loss: 1.5045
2022-03-06 21:36:43 - train: epoch 0059, iter [03700, 10009], lr: 0.010000, loss: 1.8641
2022-03-06 21:37:03 - train: epoch 0059, iter [03800, 10009], lr: 0.010000, loss: 1.5309
2022-03-06 21:37:23 - train: epoch 0059, iter [03900, 10009], lr: 0.010000, loss: 1.3946
2022-03-06 21:37:43 - train: epoch 0059, iter [04000, 10009], lr: 0.010000, loss: 1.3393
2022-03-06 21:38:02 - train: epoch 0059, iter [04100, 10009], lr: 0.010000, loss: 1.8576
2022-03-06 21:38:22 - train: epoch 0059, iter [04200, 10009], lr: 0.010000, loss: 1.7205
2022-03-06 21:38:42 - train: epoch 0059, iter [04300, 10009], lr: 0.010000, loss: 1.7103
2022-03-06 21:39:02 - train: epoch 0059, iter [04400, 10009], lr: 0.010000, loss: 1.8118
2022-03-06 21:39:21 - train: epoch 0059, iter [04500, 10009], lr: 0.010000, loss: 1.3182
2022-03-06 21:39:41 - train: epoch 0059, iter [04600, 10009], lr: 0.010000, loss: 1.5405
2022-03-06 21:40:01 - train: epoch 0059, iter [04700, 10009], lr: 0.010000, loss: 1.6374
2022-03-06 21:40:21 - train: epoch 0059, iter [04800, 10009], lr: 0.010000, loss: 1.8344
2022-03-06 21:40:41 - train: epoch 0059, iter [04900, 10009], lr: 0.010000, loss: 1.5173
2022-03-06 21:41:01 - train: epoch 0059, iter [05000, 10009], lr: 0.010000, loss: 1.8521
2022-03-06 21:41:20 - train: epoch 0059, iter [05100, 10009], lr: 0.010000, loss: 1.5254
2022-03-06 21:41:40 - train: epoch 0059, iter [05200, 10009], lr: 0.010000, loss: 1.5098
2022-03-06 21:42:00 - train: epoch 0059, iter [05300, 10009], lr: 0.010000, loss: 1.5195
2022-03-06 21:42:20 - train: epoch 0059, iter [05400, 10009], lr: 0.010000, loss: 1.5841
2022-03-06 21:42:40 - train: epoch 0059, iter [05500, 10009], lr: 0.010000, loss: 1.9109
2022-03-06 21:43:00 - train: epoch 0059, iter [05600, 10009], lr: 0.010000, loss: 2.1052
2022-03-06 21:43:19 - train: epoch 0059, iter [05700, 10009], lr: 0.010000, loss: 1.7631
2022-03-06 21:43:39 - train: epoch 0059, iter [05800, 10009], lr: 0.010000, loss: 1.5897
2022-03-06 21:43:59 - train: epoch 0059, iter [05900, 10009], lr: 0.010000, loss: 1.8426
2022-03-06 21:44:19 - train: epoch 0059, iter [06000, 10009], lr: 0.010000, loss: 1.9620
2022-03-06 21:44:39 - train: epoch 0059, iter [06100, 10009], lr: 0.010000, loss: 1.5562
2022-03-06 21:44:59 - train: epoch 0059, iter [06200, 10009], lr: 0.010000, loss: 1.6401
2022-03-06 21:45:19 - train: epoch 0059, iter [06300, 10009], lr: 0.010000, loss: 1.4061
2022-03-06 21:45:39 - train: epoch 0059, iter [06400, 10009], lr: 0.010000, loss: 2.0461
2022-03-06 21:45:58 - train: epoch 0059, iter [06500, 10009], lr: 0.010000, loss: 2.0247
2022-03-06 21:46:18 - train: epoch 0059, iter [06600, 10009], lr: 0.010000, loss: 1.7059
2022-03-06 21:46:38 - train: epoch 0059, iter [06700, 10009], lr: 0.010000, loss: 2.0525
2022-03-06 21:46:58 - train: epoch 0059, iter [06800, 10009], lr: 0.010000, loss: 1.9686
2022-03-06 21:47:18 - train: epoch 0059, iter [06900, 10009], lr: 0.010000, loss: 1.8590
2022-03-06 21:47:38 - train: epoch 0059, iter [07000, 10009], lr: 0.010000, loss: 1.7450
2022-03-06 21:47:58 - train: epoch 0059, iter [07100, 10009], lr: 0.010000, loss: 1.7070
2022-03-06 21:48:18 - train: epoch 0059, iter [07200, 10009], lr: 0.010000, loss: 1.5128
2022-03-06 21:48:38 - train: epoch 0059, iter [07300, 10009], lr: 0.010000, loss: 1.3870
2022-03-06 21:48:58 - train: epoch 0059, iter [07400, 10009], lr: 0.010000, loss: 1.6663
2022-03-06 21:49:17 - train: epoch 0059, iter [07500, 10009], lr: 0.010000, loss: 1.9086
2022-03-06 21:49:37 - train: epoch 0059, iter [07600, 10009], lr: 0.010000, loss: 1.9032
2022-03-06 21:49:57 - train: epoch 0059, iter [07700, 10009], lr: 0.010000, loss: 1.8637
2022-03-06 21:50:17 - train: epoch 0059, iter [07800, 10009], lr: 0.010000, loss: 1.4900
2022-03-06 21:50:37 - train: epoch 0059, iter [07900, 10009], lr: 0.010000, loss: 1.7831
2022-03-06 21:50:57 - train: epoch 0059, iter [08000, 10009], lr: 0.010000, loss: 1.5392
2022-03-06 21:51:16 - train: epoch 0059, iter [08100, 10009], lr: 0.010000, loss: 1.7287
2022-03-06 21:51:36 - train: epoch 0059, iter [08200, 10009], lr: 0.010000, loss: 1.4342
2022-03-06 21:51:56 - train: epoch 0059, iter [08300, 10009], lr: 0.010000, loss: 1.8957
2022-03-06 21:52:16 - train: epoch 0059, iter [08400, 10009], lr: 0.010000, loss: 1.8415
2022-03-06 21:52:36 - train: epoch 0059, iter [08500, 10009], lr: 0.010000, loss: 1.6180
2022-03-06 21:52:56 - train: epoch 0059, iter [08600, 10009], lr: 0.010000, loss: 2.0005
2022-03-06 21:53:15 - train: epoch 0059, iter [08700, 10009], lr: 0.010000, loss: 1.9333
2022-03-06 21:53:35 - train: epoch 0059, iter [08800, 10009], lr: 0.010000, loss: 1.8699
2022-03-06 21:53:55 - train: epoch 0059, iter [08900, 10009], lr: 0.010000, loss: 1.7546
2022-03-06 21:54:15 - train: epoch 0059, iter [09000, 10009], lr: 0.010000, loss: 1.7800
2022-03-06 21:54:35 - train: epoch 0059, iter [09100, 10009], lr: 0.010000, loss: 1.7723
2022-03-06 21:54:54 - train: epoch 0059, iter [09200, 10009], lr: 0.010000, loss: 2.0748
2022-03-06 21:55:14 - train: epoch 0059, iter [09300, 10009], lr: 0.010000, loss: 1.7336
2022-03-06 21:55:34 - train: epoch 0059, iter [09400, 10009], lr: 0.010000, loss: 1.6867
2022-03-06 21:55:54 - train: epoch 0059, iter [09500, 10009], lr: 0.010000, loss: 1.8693
2022-03-06 21:56:14 - train: epoch 0059, iter [09600, 10009], lr: 0.010000, loss: 1.6620
2022-03-06 21:56:33 - train: epoch 0059, iter [09700, 10009], lr: 0.010000, loss: 1.6768
2022-03-06 21:56:53 - train: epoch 0059, iter [09800, 10009], lr: 0.010000, loss: 1.6898
2022-03-06 21:57:13 - train: epoch 0059, iter [09900, 10009], lr: 0.010000, loss: 1.8718
2022-03-06 21:57:33 - train: epoch 0059, iter [10000, 10009], lr: 0.010000, loss: 1.7818
2022-03-06 21:57:35 - train: epoch 059, train_loss: 1.6736
2022-03-06 21:58:49 - eval: epoch: 059, acc1: 66.260%, acc5: 87.258%, test_loss: 1.3809, per_image_load_time: 1.921ms, per_image_inference_time: 0.861ms
2022-03-06 21:58:50 - until epoch: 059, best_acc1: 66.626%
2022-03-06 21:58:50 - epoch 060 lr: 0.010000000000000002
2022-03-06 21:59:13 - train: epoch 0060, iter [00100, 10009], lr: 0.010000, loss: 1.7795
2022-03-06 21:59:33 - train: epoch 0060, iter [00200, 10009], lr: 0.010000, loss: 1.8625
2022-03-06 21:59:52 - train: epoch 0060, iter [00300, 10009], lr: 0.010000, loss: 1.4676
2022-03-06 22:00:12 - train: epoch 0060, iter [00400, 10009], lr: 0.010000, loss: 1.6038
2022-03-06 22:00:31 - train: epoch 0060, iter [00500, 10009], lr: 0.010000, loss: 1.6721
2022-03-06 22:00:51 - train: epoch 0060, iter [00600, 10009], lr: 0.010000, loss: 1.6088
2022-03-06 22:01:11 - train: epoch 0060, iter [00700, 10009], lr: 0.010000, loss: 1.6916
2022-03-06 22:01:30 - train: epoch 0060, iter [00800, 10009], lr: 0.010000, loss: 1.9238
2022-03-06 22:01:50 - train: epoch 0060, iter [00900, 10009], lr: 0.010000, loss: 1.5468
2022-03-06 22:02:10 - train: epoch 0060, iter [01000, 10009], lr: 0.010000, loss: 1.8796
2022-03-06 22:02:29 - train: epoch 0060, iter [01100, 10009], lr: 0.010000, loss: 1.5190
2022-03-06 22:02:49 - train: epoch 0060, iter [01200, 10009], lr: 0.010000, loss: 1.8008
2022-03-06 22:03:09 - train: epoch 0060, iter [01300, 10009], lr: 0.010000, loss: 1.6998
2022-03-06 22:03:28 - train: epoch 0060, iter [01400, 10009], lr: 0.010000, loss: 1.3980
2022-03-06 22:03:48 - train: epoch 0060, iter [01500, 10009], lr: 0.010000, loss: 1.6586
2022-03-06 22:04:08 - train: epoch 0060, iter [01600, 10009], lr: 0.010000, loss: 1.7128
2022-03-06 22:04:27 - train: epoch 0060, iter [01700, 10009], lr: 0.010000, loss: 1.5969
2022-03-06 22:04:47 - train: epoch 0060, iter [01800, 10009], lr: 0.010000, loss: 1.4956
2022-03-06 22:05:07 - train: epoch 0060, iter [01900, 10009], lr: 0.010000, loss: 1.7934
2022-03-06 22:05:26 - train: epoch 0060, iter [02000, 10009], lr: 0.010000, loss: 1.3355
2022-03-06 22:05:46 - train: epoch 0060, iter [02100, 10009], lr: 0.010000, loss: 1.6465
2022-03-06 22:06:06 - train: epoch 0060, iter [02200, 10009], lr: 0.010000, loss: 1.8053
2022-03-06 22:06:25 - train: epoch 0060, iter [02300, 10009], lr: 0.010000, loss: 1.8258
2022-03-06 22:06:45 - train: epoch 0060, iter [02400, 10009], lr: 0.010000, loss: 1.7352
2022-03-06 22:07:05 - train: epoch 0060, iter [02500, 10009], lr: 0.010000, loss: 1.7035
2022-03-06 22:07:25 - train: epoch 0060, iter [02600, 10009], lr: 0.010000, loss: 1.8616
2022-03-06 22:07:44 - train: epoch 0060, iter [02700, 10009], lr: 0.010000, loss: 1.5927
2022-03-06 22:08:04 - train: epoch 0060, iter [02800, 10009], lr: 0.010000, loss: 1.6560
2022-03-06 22:08:24 - train: epoch 0060, iter [02900, 10009], lr: 0.010000, loss: 1.7271
2022-03-06 22:08:44 - train: epoch 0060, iter [03000, 10009], lr: 0.010000, loss: 1.8269
2022-03-06 22:09:03 - train: epoch 0060, iter [03100, 10009], lr: 0.010000, loss: 1.8639
2022-03-06 22:09:23 - train: epoch 0060, iter [03200, 10009], lr: 0.010000, loss: 1.9063
2022-03-06 22:09:43 - train: epoch 0060, iter [03300, 10009], lr: 0.010000, loss: 1.5447
2022-03-06 22:10:03 - train: epoch 0060, iter [03400, 10009], lr: 0.010000, loss: 1.6717
2022-03-06 22:10:23 - train: epoch 0060, iter [03500, 10009], lr: 0.010000, loss: 1.7222
2022-03-06 22:10:42 - train: epoch 0060, iter [03600, 10009], lr: 0.010000, loss: 1.7492
2022-03-06 22:11:02 - train: epoch 0060, iter [03700, 10009], lr: 0.010000, loss: 1.9022
2022-03-06 22:11:22 - train: epoch 0060, iter [03800, 10009], lr: 0.010000, loss: 1.9477
2022-03-06 22:11:42 - train: epoch 0060, iter [03900, 10009], lr: 0.010000, loss: 1.9221
2022-03-06 22:12:02 - train: epoch 0060, iter [04000, 10009], lr: 0.010000, loss: 1.8842
2022-03-06 22:12:22 - train: epoch 0060, iter [04100, 10009], lr: 0.010000, loss: 1.8917
2022-03-06 22:12:41 - train: epoch 0060, iter [04200, 10009], lr: 0.010000, loss: 1.6982
2022-03-06 22:13:01 - train: epoch 0060, iter [04300, 10009], lr: 0.010000, loss: 1.7303
2022-03-06 22:13:21 - train: epoch 0060, iter [04400, 10009], lr: 0.010000, loss: 1.8289
2022-03-06 22:13:41 - train: epoch 0060, iter [04500, 10009], lr: 0.010000, loss: 1.5718
2022-03-06 22:14:01 - train: epoch 0060, iter [04600, 10009], lr: 0.010000, loss: 1.5681
2022-03-06 22:14:21 - train: epoch 0060, iter [04700, 10009], lr: 0.010000, loss: 1.6505
2022-03-06 22:14:41 - train: epoch 0060, iter [04800, 10009], lr: 0.010000, loss: 1.6875
2022-03-06 22:15:01 - train: epoch 0060, iter [04900, 10009], lr: 0.010000, loss: 1.8362
2022-03-06 22:15:21 - train: epoch 0060, iter [05000, 10009], lr: 0.010000, loss: 1.4498
2022-03-06 22:15:40 - train: epoch 0060, iter [05100, 10009], lr: 0.010000, loss: 1.6670
2022-03-06 22:16:00 - train: epoch 0060, iter [05200, 10009], lr: 0.010000, loss: 1.7832
2022-03-06 22:16:20 - train: epoch 0060, iter [05300, 10009], lr: 0.010000, loss: 1.7254
2022-03-06 22:16:40 - train: epoch 0060, iter [05400, 10009], lr: 0.010000, loss: 1.4271
2022-03-06 22:17:00 - train: epoch 0060, iter [05500, 10009], lr: 0.010000, loss: 1.9641
2022-03-06 22:17:20 - train: epoch 0060, iter [05600, 10009], lr: 0.010000, loss: 1.5853
2022-03-06 22:17:40 - train: epoch 0060, iter [05700, 10009], lr: 0.010000, loss: 1.8152
2022-03-06 22:18:00 - train: epoch 0060, iter [05800, 10009], lr: 0.010000, loss: 1.8140
2022-03-06 22:18:20 - train: epoch 0060, iter [05900, 10009], lr: 0.010000, loss: 1.7883
2022-03-06 22:18:40 - train: epoch 0060, iter [06000, 10009], lr: 0.010000, loss: 2.0812
2022-03-06 22:19:00 - train: epoch 0060, iter [06100, 10009], lr: 0.010000, loss: 1.3430
2022-03-06 22:19:20 - train: epoch 0060, iter [06200, 10009], lr: 0.010000, loss: 1.9167
2022-03-06 22:19:40 - train: epoch 0060, iter [06300, 10009], lr: 0.010000, loss: 1.6284
2022-03-06 22:20:00 - train: epoch 0060, iter [06400, 10009], lr: 0.010000, loss: 1.5004
2022-03-06 22:20:20 - train: epoch 0060, iter [06500, 10009], lr: 0.010000, loss: 1.6240
2022-03-06 22:20:40 - train: epoch 0060, iter [06600, 10009], lr: 0.010000, loss: 1.5088
2022-03-06 22:21:00 - train: epoch 0060, iter [06700, 10009], lr: 0.010000, loss: 2.1566
2022-03-06 22:21:20 - train: epoch 0060, iter [06800, 10009], lr: 0.010000, loss: 1.8414
2022-03-06 22:21:39 - train: epoch 0060, iter [06900, 10009], lr: 0.010000, loss: 1.8280
2022-03-06 22:21:59 - train: epoch 0060, iter [07000, 10009], lr: 0.010000, loss: 1.7754
2022-03-06 22:22:19 - train: epoch 0060, iter [07100, 10009], lr: 0.010000, loss: 1.4856
2022-03-06 22:22:39 - train: epoch 0060, iter [07200, 10009], lr: 0.010000, loss: 1.5466
2022-03-06 22:22:59 - train: epoch 0060, iter [07300, 10009], lr: 0.010000, loss: 1.3348
2022-03-06 22:23:19 - train: epoch 0060, iter [07400, 10009], lr: 0.010000, loss: 1.6947
2022-03-06 22:23:39 - train: epoch 0060, iter [07500, 10009], lr: 0.010000, loss: 1.6199
2022-03-06 22:23:59 - train: epoch 0060, iter [07600, 10009], lr: 0.010000, loss: 1.5381
2022-03-06 22:24:19 - train: epoch 0060, iter [07700, 10009], lr: 0.010000, loss: 1.7560
2022-03-06 22:24:39 - train: epoch 0060, iter [07800, 10009], lr: 0.010000, loss: 1.9145
2022-03-06 22:24:59 - train: epoch 0060, iter [07900, 10009], lr: 0.010000, loss: 1.3489
2022-03-06 22:25:19 - train: epoch 0060, iter [08000, 10009], lr: 0.010000, loss: 1.6781
2022-03-06 22:25:39 - train: epoch 0060, iter [08100, 10009], lr: 0.010000, loss: 1.5257
2022-03-06 22:25:59 - train: epoch 0060, iter [08200, 10009], lr: 0.010000, loss: 1.9286
2022-03-06 22:26:18 - train: epoch 0060, iter [08300, 10009], lr: 0.010000, loss: 1.7700
2022-03-06 22:26:38 - train: epoch 0060, iter [08400, 10009], lr: 0.010000, loss: 1.7777
2022-03-06 22:26:58 - train: epoch 0060, iter [08500, 10009], lr: 0.010000, loss: 1.4471
2022-03-06 22:27:18 - train: epoch 0060, iter [08600, 10009], lr: 0.010000, loss: 1.2774
2022-03-06 22:27:38 - train: epoch 0060, iter [08700, 10009], lr: 0.010000, loss: 1.5796
2022-03-06 22:27:58 - train: epoch 0060, iter [08800, 10009], lr: 0.010000, loss: 2.1009
2022-03-06 22:28:18 - train: epoch 0060, iter [08900, 10009], lr: 0.010000, loss: 2.0242
2022-03-06 22:28:38 - train: epoch 0060, iter [09000, 10009], lr: 0.010000, loss: 1.9870
2022-03-06 22:28:58 - train: epoch 0060, iter [09100, 10009], lr: 0.010000, loss: 2.0139
2022-03-06 22:29:18 - train: epoch 0060, iter [09200, 10009], lr: 0.010000, loss: 1.5596
2022-03-06 22:29:38 - train: epoch 0060, iter [09300, 10009], lr: 0.010000, loss: 2.0507
2022-03-06 22:29:58 - train: epoch 0060, iter [09400, 10009], lr: 0.010000, loss: 1.4068
2022-03-06 22:30:18 - train: epoch 0060, iter [09500, 10009], lr: 0.010000, loss: 1.9608
2022-03-06 22:30:38 - train: epoch 0060, iter [09600, 10009], lr: 0.010000, loss: 1.7283
2022-03-06 22:30:58 - train: epoch 0060, iter [09700, 10009], lr: 0.010000, loss: 1.6520
2022-03-06 22:31:18 - train: epoch 0060, iter [09800, 10009], lr: 0.010000, loss: 1.8287
2022-03-06 22:31:37 - train: epoch 0060, iter [09900, 10009], lr: 0.010000, loss: 1.6251
2022-03-06 22:31:57 - train: epoch 0060, iter [10000, 10009], lr: 0.010000, loss: 1.5871
2022-03-06 22:32:00 - train: epoch 060, train_loss: 1.6703
2022-03-06 22:33:15 - eval: epoch: 060, acc1: 66.362%, acc5: 87.424%, test_loss: 1.3734, per_image_load_time: 1.432ms, per_image_inference_time: 0.821ms
2022-03-06 22:33:16 - until epoch: 060, best_acc1: 66.626%
2022-03-06 22:33:16 - epoch 061 lr: 0.0010000000000000002
2022-03-06 22:33:39 - train: epoch 0061, iter [00100, 10009], lr: 0.001000, loss: 1.7262
2022-03-06 22:33:59 - train: epoch 0061, iter [00200, 10009], lr: 0.001000, loss: 1.3297
2022-03-06 22:34:18 - train: epoch 0061, iter [00300, 10009], lr: 0.001000, loss: 1.8634
2022-03-06 22:34:38 - train: epoch 0061, iter [00400, 10009], lr: 0.001000, loss: 1.6746
2022-03-06 22:34:58 - train: epoch 0061, iter [00500, 10009], lr: 0.001000, loss: 1.2816
2022-03-06 22:35:17 - train: epoch 0061, iter [00600, 10009], lr: 0.001000, loss: 1.3247
2022-03-06 22:35:37 - train: epoch 0061, iter [00700, 10009], lr: 0.001000, loss: 1.4060
2022-03-06 22:35:56 - train: epoch 0061, iter [00800, 10009], lr: 0.001000, loss: 1.6011
2022-03-06 22:36:16 - train: epoch 0061, iter [00900, 10009], lr: 0.001000, loss: 1.7576
2022-03-06 22:36:36 - train: epoch 0061, iter [01000, 10009], lr: 0.001000, loss: 1.4169
2022-03-06 22:36:55 - train: epoch 0061, iter [01100, 10009], lr: 0.001000, loss: 1.6185
2022-03-06 22:37:15 - train: epoch 0061, iter [01200, 10009], lr: 0.001000, loss: 1.5617
2022-03-06 22:37:35 - train: epoch 0061, iter [01300, 10009], lr: 0.001000, loss: 1.1112
2022-03-06 22:37:54 - train: epoch 0061, iter [01400, 10009], lr: 0.001000, loss: 1.4723
2022-03-06 22:38:14 - train: epoch 0061, iter [01500, 10009], lr: 0.001000, loss: 1.8007
2022-03-06 22:38:33 - train: epoch 0061, iter [01600, 10009], lr: 0.001000, loss: 1.6125
2022-03-06 22:38:53 - train: epoch 0061, iter [01700, 10009], lr: 0.001000, loss: 1.2448
2022-03-06 22:39:13 - train: epoch 0061, iter [01800, 10009], lr: 0.001000, loss: 1.4591
2022-03-06 22:39:32 - train: epoch 0061, iter [01900, 10009], lr: 0.001000, loss: 1.5085
2022-03-06 22:39:52 - train: epoch 0061, iter [02000, 10009], lr: 0.001000, loss: 1.3141
2022-03-06 22:40:12 - train: epoch 0061, iter [02100, 10009], lr: 0.001000, loss: 1.3852
2022-03-06 22:40:31 - train: epoch 0061, iter [02200, 10009], lr: 0.001000, loss: 1.1988
2022-03-06 22:40:51 - train: epoch 0061, iter [02300, 10009], lr: 0.001000, loss: 1.5749
2022-03-06 22:41:11 - train: epoch 0061, iter [02400, 10009], lr: 0.001000, loss: 1.4314
2022-03-06 22:41:30 - train: epoch 0061, iter [02500, 10009], lr: 0.001000, loss: 1.2260
2022-03-06 22:41:50 - train: epoch 0061, iter [02600, 10009], lr: 0.001000, loss: 1.7035
2022-03-06 22:42:10 - train: epoch 0061, iter [02700, 10009], lr: 0.001000, loss: 1.5762
2022-03-06 22:42:29 - train: epoch 0061, iter [02800, 10009], lr: 0.001000, loss: 1.2628
2022-03-06 22:42:49 - train: epoch 0061, iter [02900, 10009], lr: 0.001000, loss: 1.9133
2022-03-06 22:43:09 - train: epoch 0061, iter [03000, 10009], lr: 0.001000, loss: 1.4280
2022-03-06 22:43:28 - train: epoch 0061, iter [03100, 10009], lr: 0.001000, loss: 1.2826
2022-03-06 22:43:48 - train: epoch 0061, iter [03200, 10009], lr: 0.001000, loss: 1.3703
2022-03-06 22:44:08 - train: epoch 0061, iter [03300, 10009], lr: 0.001000, loss: 1.4558
2022-03-06 22:44:28 - train: epoch 0061, iter [03400, 10009], lr: 0.001000, loss: 1.7285
2022-03-06 22:44:48 - train: epoch 0061, iter [03500, 10009], lr: 0.001000, loss: 1.6440
2022-03-06 22:45:07 - train: epoch 0061, iter [03600, 10009], lr: 0.001000, loss: 1.4534
2022-03-06 22:45:27 - train: epoch 0061, iter [03700, 10009], lr: 0.001000, loss: 1.4687
2022-03-06 22:45:47 - train: epoch 0061, iter [03800, 10009], lr: 0.001000, loss: 1.4417
2022-03-06 22:46:06 - train: epoch 0061, iter [03900, 10009], lr: 0.001000, loss: 1.4845
2022-03-06 22:46:26 - train: epoch 0061, iter [04000, 10009], lr: 0.001000, loss: 1.4715
2022-03-06 22:46:46 - train: epoch 0061, iter [04100, 10009], lr: 0.001000, loss: 1.5339
2022-03-06 22:47:05 - train: epoch 0061, iter [04200, 10009], lr: 0.001000, loss: 1.3714
2022-03-06 22:47:25 - train: epoch 0061, iter [04300, 10009], lr: 0.001000, loss: 1.3847
2022-03-06 22:47:45 - train: epoch 0061, iter [04400, 10009], lr: 0.001000, loss: 1.2764
2022-03-06 22:48:05 - train: epoch 0061, iter [04500, 10009], lr: 0.001000, loss: 1.4656
2022-03-06 22:48:24 - train: epoch 0061, iter [04600, 10009], lr: 0.001000, loss: 1.1979
2022-03-06 22:48:44 - train: epoch 0061, iter [04700, 10009], lr: 0.001000, loss: 1.4497
2022-03-06 22:49:04 - train: epoch 0061, iter [04800, 10009], lr: 0.001000, loss: 1.2147
2022-03-06 22:49:24 - train: epoch 0061, iter [04900, 10009], lr: 0.001000, loss: 1.4323
2022-03-06 22:49:44 - train: epoch 0061, iter [05000, 10009], lr: 0.001000, loss: 1.6343
2022-03-06 22:50:03 - train: epoch 0061, iter [05100, 10009], lr: 0.001000, loss: 1.3330
2022-03-06 22:50:23 - train: epoch 0061, iter [05200, 10009], lr: 0.001000, loss: 1.4464
2022-03-06 22:50:42 - train: epoch 0061, iter [05300, 10009], lr: 0.001000, loss: 1.4255
2022-03-06 22:51:02 - train: epoch 0061, iter [05400, 10009], lr: 0.001000, loss: 1.4358
2022-03-06 22:51:22 - train: epoch 0061, iter [05500, 10009], lr: 0.001000, loss: 1.3531
2022-03-06 22:51:41 - train: epoch 0061, iter [05600, 10009], lr: 0.001000, loss: 1.4213
2022-03-06 22:52:01 - train: epoch 0061, iter [05700, 10009], lr: 0.001000, loss: 1.8899
2022-03-06 22:52:21 - train: epoch 0061, iter [05800, 10009], lr: 0.001000, loss: 1.4812
2022-03-06 22:52:41 - train: epoch 0061, iter [05900, 10009], lr: 0.001000, loss: 1.1715
2022-03-06 22:53:00 - train: epoch 0061, iter [06000, 10009], lr: 0.001000, loss: 1.4603
2022-03-06 22:53:20 - train: epoch 0061, iter [06100, 10009], lr: 0.001000, loss: 1.4550
2022-03-06 22:53:40 - train: epoch 0061, iter [06200, 10009], lr: 0.001000, loss: 1.2775
2022-03-06 22:54:00 - train: epoch 0061, iter [06300, 10009], lr: 0.001000, loss: 1.5108
2022-03-06 22:54:19 - train: epoch 0061, iter [06400, 10009], lr: 0.001000, loss: 1.4235
2022-03-06 22:54:39 - train: epoch 0061, iter [06500, 10009], lr: 0.001000, loss: 1.5593
2022-03-06 22:54:59 - train: epoch 0061, iter [06600, 10009], lr: 0.001000, loss: 1.3665
2022-03-06 22:55:19 - train: epoch 0061, iter [06700, 10009], lr: 0.001000, loss: 1.1792
2022-03-06 22:55:38 - train: epoch 0061, iter [06800, 10009], lr: 0.001000, loss: 1.3752
2022-03-06 22:55:58 - train: epoch 0061, iter [06900, 10009], lr: 0.001000, loss: 1.1860
2022-03-06 22:56:18 - train: epoch 0061, iter [07000, 10009], lr: 0.001000, loss: 1.4363
2022-03-06 22:56:38 - train: epoch 0061, iter [07100, 10009], lr: 0.001000, loss: 1.4773
2022-03-06 22:56:58 - train: epoch 0061, iter [07200, 10009], lr: 0.001000, loss: 1.6362
2022-03-06 22:57:18 - train: epoch 0061, iter [07300, 10009], lr: 0.001000, loss: 1.4716
2022-03-06 22:57:37 - train: epoch 0061, iter [07400, 10009], lr: 0.001000, loss: 1.1450
2022-03-06 22:57:57 - train: epoch 0061, iter [07500, 10009], lr: 0.001000, loss: 1.2938
2022-03-06 22:58:17 - train: epoch 0061, iter [07600, 10009], lr: 0.001000, loss: 1.3998
2022-03-06 22:58:36 - train: epoch 0061, iter [07700, 10009], lr: 0.001000, loss: 1.2439
2022-03-06 22:58:56 - train: epoch 0061, iter [07800, 10009], lr: 0.001000, loss: 1.4035
2022-03-06 22:59:16 - train: epoch 0061, iter [07900, 10009], lr: 0.001000, loss: 1.6119
2022-03-06 22:59:36 - train: epoch 0061, iter [08000, 10009], lr: 0.001000, loss: 1.4017
2022-03-06 22:59:56 - train: epoch 0061, iter [08100, 10009], lr: 0.001000, loss: 1.3588
2022-03-06 23:00:15 - train: epoch 0061, iter [08200, 10009], lr: 0.001000, loss: 1.3875
2022-03-06 23:00:35 - train: epoch 0061, iter [08300, 10009], lr: 0.001000, loss: 1.3628
2022-03-06 23:00:55 - train: epoch 0061, iter [08400, 10009], lr: 0.001000, loss: 1.4711
2022-03-06 23:01:15 - train: epoch 0061, iter [08500, 10009], lr: 0.001000, loss: 1.2590
2022-03-06 23:01:34 - train: epoch 0061, iter [08600, 10009], lr: 0.001000, loss: 1.5045
2022-03-06 23:01:54 - train: epoch 0061, iter [08700, 10009], lr: 0.001000, loss: 1.0862
2022-03-06 23:02:14 - train: epoch 0061, iter [08800, 10009], lr: 0.001000, loss: 1.2806
2022-03-06 23:02:33 - train: epoch 0061, iter [08900, 10009], lr: 0.001000, loss: 1.5138
2022-03-06 23:02:53 - train: epoch 0061, iter [09000, 10009], lr: 0.001000, loss: 1.4130
2022-03-06 23:03:12 - train: epoch 0061, iter [09100, 10009], lr: 0.001000, loss: 1.4575
2022-03-06 23:03:32 - train: epoch 0061, iter [09200, 10009], lr: 0.001000, loss: 1.6054
2022-03-06 23:03:52 - train: epoch 0061, iter [09300, 10009], lr: 0.001000, loss: 1.3877
2022-03-06 23:04:11 - train: epoch 0061, iter [09400, 10009], lr: 0.001000, loss: 1.5009
2022-03-06 23:04:31 - train: epoch 0061, iter [09500, 10009], lr: 0.001000, loss: 1.4367
2022-03-06 23:04:51 - train: epoch 0061, iter [09600, 10009], lr: 0.001000, loss: 1.6694
2022-03-06 23:05:10 - train: epoch 0061, iter [09700, 10009], lr: 0.001000, loss: 1.4538
2022-03-06 23:05:30 - train: epoch 0061, iter [09800, 10009], lr: 0.001000, loss: 1.6043
2022-03-06 23:05:50 - train: epoch 0061, iter [09900, 10009], lr: 0.001000, loss: 1.5457
2022-03-06 23:06:09 - train: epoch 0061, iter [10000, 10009], lr: 0.001000, loss: 1.1616
2022-03-06 23:06:12 - train: epoch 061, train_loss: 1.4364
2022-03-06 23:07:26 - eval: epoch: 061, acc1: 70.896%, acc5: 89.782%, test_loss: 1.1827, per_image_load_time: 1.348ms, per_image_inference_time: 0.812ms
2022-03-06 23:07:27 - until epoch: 061, best_acc1: 70.896%
2022-03-06 23:07:27 - epoch 062 lr: 0.0010000000000000002
2022-03-06 23:07:50 - train: epoch 0062, iter [00100, 10009], lr: 0.001000, loss: 1.0322
2022-03-06 23:08:10 - train: epoch 0062, iter [00200, 10009], lr: 0.001000, loss: 1.4068
2022-03-06 23:08:29 - train: epoch 0062, iter [00300, 10009], lr: 0.001000, loss: 1.0511
2022-03-06 23:08:49 - train: epoch 0062, iter [00400, 10009], lr: 0.001000, loss: 1.3055
2022-03-06 23:09:08 - train: epoch 0062, iter [00500, 10009], lr: 0.001000, loss: 1.7869
2022-03-06 23:09:28 - train: epoch 0062, iter [00600, 10009], lr: 0.001000, loss: 1.2490
2022-03-06 23:09:47 - train: epoch 0062, iter [00700, 10009], lr: 0.001000, loss: 1.5691
2022-03-06 23:10:07 - train: epoch 0062, iter [00800, 10009], lr: 0.001000, loss: 1.2661
2022-03-06 23:10:27 - train: epoch 0062, iter [00900, 10009], lr: 0.001000, loss: 1.3208
2022-03-06 23:10:46 - train: epoch 0062, iter [01000, 10009], lr: 0.001000, loss: 1.5913
2022-03-06 23:11:06 - train: epoch 0062, iter [01100, 10009], lr: 0.001000, loss: 1.2453
2022-03-06 23:11:26 - train: epoch 0062, iter [01200, 10009], lr: 0.001000, loss: 1.2867
2022-03-06 23:11:45 - train: epoch 0062, iter [01300, 10009], lr: 0.001000, loss: 1.3679
2022-03-06 23:12:05 - train: epoch 0062, iter [01400, 10009], lr: 0.001000, loss: 1.2554
2022-03-06 23:12:25 - train: epoch 0062, iter [01500, 10009], lr: 0.001000, loss: 1.5019
2022-03-06 23:12:44 - train: epoch 0062, iter [01600, 10009], lr: 0.001000, loss: 1.1893
2022-03-06 23:13:04 - train: epoch 0062, iter [01700, 10009], lr: 0.001000, loss: 1.4116
2022-03-06 23:13:23 - train: epoch 0062, iter [01800, 10009], lr: 0.001000, loss: 1.5418
2022-03-06 23:13:43 - train: epoch 0062, iter [01900, 10009], lr: 0.001000, loss: 1.6060
2022-03-06 23:14:03 - train: epoch 0062, iter [02000, 10009], lr: 0.001000, loss: 1.4351
2022-03-06 23:14:23 - train: epoch 0062, iter [02100, 10009], lr: 0.001000, loss: 1.3588
2022-03-06 23:14:42 - train: epoch 0062, iter [02200, 10009], lr: 0.001000, loss: 1.1192
2022-03-06 23:15:02 - train: epoch 0062, iter [02300, 10009], lr: 0.001000, loss: 1.8005
2022-03-06 23:15:22 - train: epoch 0062, iter [02400, 10009], lr: 0.001000, loss: 1.5675
2022-03-06 23:15:41 - train: epoch 0062, iter [02500, 10009], lr: 0.001000, loss: 1.3710
2022-03-06 23:16:01 - train: epoch 0062, iter [02600, 10009], lr: 0.001000, loss: 1.2935
2022-03-06 23:16:21 - train: epoch 0062, iter [02700, 10009], lr: 0.001000, loss: 1.3108
2022-03-06 23:16:40 - train: epoch 0062, iter [02800, 10009], lr: 0.001000, loss: 1.3286
2022-03-06 23:17:00 - train: epoch 0062, iter [02900, 10009], lr: 0.001000, loss: 1.1830
2022-03-06 23:17:20 - train: epoch 0062, iter [03000, 10009], lr: 0.001000, loss: 1.3349
2022-03-06 23:17:40 - train: epoch 0062, iter [03100, 10009], lr: 0.001000, loss: 1.5580
2022-03-06 23:18:00 - train: epoch 0062, iter [03200, 10009], lr: 0.001000, loss: 1.6770
2022-03-06 23:18:20 - train: epoch 0062, iter [03300, 10009], lr: 0.001000, loss: 1.3519
2022-03-06 23:18:39 - train: epoch 0062, iter [03400, 10009], lr: 0.001000, loss: 1.5464
2022-03-06 23:18:59 - train: epoch 0062, iter [03500, 10009], lr: 0.001000, loss: 1.2806
2022-03-06 23:19:19 - train: epoch 0062, iter [03600, 10009], lr: 0.001000, loss: 1.4260
2022-03-06 23:19:40 - train: epoch 0062, iter [03700, 10009], lr: 0.001000, loss: 1.4796
2022-03-06 23:19:59 - train: epoch 0062, iter [03800, 10009], lr: 0.001000, loss: 1.2697
2022-03-06 23:20:19 - train: epoch 0062, iter [03900, 10009], lr: 0.001000, loss: 1.6557
2022-03-06 23:20:39 - train: epoch 0062, iter [04000, 10009], lr: 0.001000, loss: 1.7609
2022-03-06 23:20:59 - train: epoch 0062, iter [04100, 10009], lr: 0.001000, loss: 1.2880
2022-03-06 23:21:19 - train: epoch 0062, iter [04200, 10009], lr: 0.001000, loss: 1.3132
2022-03-06 23:21:39 - train: epoch 0062, iter [04300, 10009], lr: 0.001000, loss: 1.4220
2022-03-06 23:21:59 - train: epoch 0062, iter [04400, 10009], lr: 0.001000, loss: 1.4813
2022-03-06 23:22:19 - train: epoch 0062, iter [04500, 10009], lr: 0.001000, loss: 1.1609
2022-03-06 23:22:39 - train: epoch 0062, iter [04600, 10009], lr: 0.001000, loss: 1.2338
2022-03-06 23:22:59 - train: epoch 0062, iter [04700, 10009], lr: 0.001000, loss: 1.2312
2022-03-06 23:23:18 - train: epoch 0062, iter [04800, 10009], lr: 0.001000, loss: 1.5572
2022-03-06 23:23:38 - train: epoch 0062, iter [04900, 10009], lr: 0.001000, loss: 1.2118
2022-03-06 23:23:58 - train: epoch 0062, iter [05000, 10009], lr: 0.001000, loss: 1.7444
2022-03-06 23:24:18 - train: epoch 0062, iter [05100, 10009], lr: 0.001000, loss: 0.9895
2022-03-06 23:24:38 - train: epoch 0062, iter [05200, 10009], lr: 0.001000, loss: 1.1720
2022-03-06 23:24:57 - train: epoch 0062, iter [05300, 10009], lr: 0.001000, loss: 1.1173
2022-03-06 23:25:17 - train: epoch 0062, iter [05400, 10009], lr: 0.001000, loss: 1.1923
2022-03-06 23:25:37 - train: epoch 0062, iter [05500, 10009], lr: 0.001000, loss: 1.5568
2022-03-06 23:25:57 - train: epoch 0062, iter [05600, 10009], lr: 0.001000, loss: 1.3337
2022-03-06 23:26:17 - train: epoch 0062, iter [05700, 10009], lr: 0.001000, loss: 1.3506
2022-03-06 23:26:36 - train: epoch 0062, iter [05800, 10009], lr: 0.001000, loss: 1.4001
2022-03-06 23:26:56 - train: epoch 0062, iter [05900, 10009], lr: 0.001000, loss: 1.3048
2022-03-06 23:27:16 - train: epoch 0062, iter [06000, 10009], lr: 0.001000, loss: 1.2764
2022-03-06 23:27:36 - train: epoch 0062, iter [06100, 10009], lr: 0.001000, loss: 1.0081
2022-03-06 23:27:56 - train: epoch 0062, iter [06200, 10009], lr: 0.001000, loss: 1.6165
2022-03-06 23:28:15 - train: epoch 0062, iter [06300, 10009], lr: 0.001000, loss: 1.2153
2022-03-06 23:28:35 - train: epoch 0062, iter [06400, 10009], lr: 0.001000, loss: 1.2199
2022-03-06 23:28:55 - train: epoch 0062, iter [06500, 10009], lr: 0.001000, loss: 1.4842
2022-03-06 23:29:15 - train: epoch 0062, iter [06600, 10009], lr: 0.001000, loss: 1.5829
2022-03-06 23:29:34 - train: epoch 0062, iter [06700, 10009], lr: 0.001000, loss: 1.2045
2022-03-06 23:29:54 - train: epoch 0062, iter [06800, 10009], lr: 0.001000, loss: 1.2548
2022-03-06 23:30:14 - train: epoch 0062, iter [06900, 10009], lr: 0.001000, loss: 1.7409
2022-03-06 23:30:34 - train: epoch 0062, iter [07000, 10009], lr: 0.001000, loss: 1.4829
2022-03-06 23:30:53 - train: epoch 0062, iter [07100, 10009], lr: 0.001000, loss: 1.5711
2022-03-06 23:31:13 - train: epoch 0062, iter [07200, 10009], lr: 0.001000, loss: 1.7837
2022-03-06 23:31:33 - train: epoch 0062, iter [07300, 10009], lr: 0.001000, loss: 1.4857
2022-03-06 23:31:53 - train: epoch 0062, iter [07400, 10009], lr: 0.001000, loss: 1.5263
2022-03-06 23:32:13 - train: epoch 0062, iter [07500, 10009], lr: 0.001000, loss: 1.2719
2022-03-06 23:32:32 - train: epoch 0062, iter [07600, 10009], lr: 0.001000, loss: 1.2865
2022-03-06 23:32:52 - train: epoch 0062, iter [07700, 10009], lr: 0.001000, loss: 1.4342
2022-03-06 23:33:12 - train: epoch 0062, iter [07800, 10009], lr: 0.001000, loss: 1.4945
2022-03-06 23:33:32 - train: epoch 0062, iter [07900, 10009], lr: 0.001000, loss: 1.4205
2022-03-06 23:33:52 - train: epoch 0062, iter [08000, 10009], lr: 0.001000, loss: 1.2272
2022-03-06 23:34:12 - train: epoch 0062, iter [08100, 10009], lr: 0.001000, loss: 1.2768
2022-03-06 23:34:31 - train: epoch 0062, iter [08200, 10009], lr: 0.001000, loss: 1.6744
2022-03-06 23:34:51 - train: epoch 0062, iter [08300, 10009], lr: 0.001000, loss: 1.6113
2022-03-06 23:35:11 - train: epoch 0062, iter [08400, 10009], lr: 0.001000, loss: 1.2001
2022-03-06 23:35:31 - train: epoch 0062, iter [08500, 10009], lr: 0.001000, loss: 1.5400
2022-03-06 23:35:50 - train: epoch 0062, iter [08600, 10009], lr: 0.001000, loss: 1.6024
2022-03-06 23:36:10 - train: epoch 0062, iter [08700, 10009], lr: 0.001000, loss: 1.4826
2022-03-06 23:36:30 - train: epoch 0062, iter [08800, 10009], lr: 0.001000, loss: 1.2145
2022-03-06 23:36:49 - train: epoch 0062, iter [08900, 10009], lr: 0.001000, loss: 1.8144
2022-03-06 23:37:09 - train: epoch 0062, iter [09000, 10009], lr: 0.001000, loss: 1.4325
2022-03-06 23:37:29 - train: epoch 0062, iter [09100, 10009], lr: 0.001000, loss: 1.3029
2022-03-06 23:37:49 - train: epoch 0062, iter [09200, 10009], lr: 0.001000, loss: 1.3296
2022-03-06 23:38:09 - train: epoch 0062, iter [09300, 10009], lr: 0.001000, loss: 1.5594
2022-03-06 23:38:28 - train: epoch 0062, iter [09400, 10009], lr: 0.001000, loss: 1.4617
2022-03-06 23:38:48 - train: epoch 0062, iter [09500, 10009], lr: 0.001000, loss: 1.5525
2022-03-06 23:39:08 - train: epoch 0062, iter [09600, 10009], lr: 0.001000, loss: 1.5072
2022-03-06 23:39:28 - train: epoch 0062, iter [09700, 10009], lr: 0.001000, loss: 1.5891
2022-03-06 23:39:47 - train: epoch 0062, iter [09800, 10009], lr: 0.001000, loss: 1.0483
2022-03-06 23:40:08 - train: epoch 0062, iter [09900, 10009], lr: 0.001000, loss: 1.2909
2022-03-06 23:40:28 - train: epoch 0062, iter [10000, 10009], lr: 0.001000, loss: 1.5816
2022-03-06 23:40:32 - train: epoch 062, train_loss: 1.3760
2022-03-06 23:41:48 - eval: epoch: 062, acc1: 71.330%, acc5: 90.106%, test_loss: 1.1596, per_image_load_time: 2.035ms, per_image_inference_time: 0.837ms
2022-03-06 23:41:49 - until epoch: 062, best_acc1: 71.330%
2022-03-06 23:41:49 - epoch 063 lr: 0.0010000000000000002
2022-03-06 23:42:12 - train: epoch 0063, iter [00100, 10009], lr: 0.001000, loss: 0.9704
2022-03-06 23:42:32 - train: epoch 0063, iter [00200, 10009], lr: 0.001000, loss: 1.3718
2022-03-06 23:42:52 - train: epoch 0063, iter [00300, 10009], lr: 0.001000, loss: 1.3563
2022-03-06 23:43:12 - train: epoch 0063, iter [00400, 10009], lr: 0.001000, loss: 1.1745
2022-03-06 23:43:32 - train: epoch 0063, iter [00500, 10009], lr: 0.001000, loss: 1.0964
2022-03-06 23:43:51 - train: epoch 0063, iter [00600, 10009], lr: 0.001000, loss: 1.4552
2022-03-06 23:44:16 - train: epoch 0063, iter [00700, 10009], lr: 0.001000, loss: 1.4326
2022-03-06 23:45:50 - train: epoch 0063, iter [00800, 10009], lr: 0.001000, loss: 1.3750
2022-03-06 23:46:09 - train: epoch 0063, iter [00900, 10009], lr: 0.001000, loss: 1.2409
2022-03-06 23:46:29 - train: epoch 0063, iter [01000, 10009], lr: 0.001000, loss: 1.3601
2022-03-06 23:46:49 - train: epoch 0063, iter [01100, 10009], lr: 0.001000, loss: 1.4818
2022-03-06 23:47:09 - train: epoch 0063, iter [01200, 10009], lr: 0.001000, loss: 1.2505
2022-03-06 23:47:29 - train: epoch 0063, iter [01300, 10009], lr: 0.001000, loss: 1.4815
2022-03-06 23:47:48 - train: epoch 0063, iter [01400, 10009], lr: 0.001000, loss: 1.4790
2022-03-06 23:48:08 - train: epoch 0063, iter [01500, 10009], lr: 0.001000, loss: 1.3453
2022-03-06 23:50:27 - train: epoch 0063, iter [01600, 10009], lr: 0.001000, loss: 1.3199
2022-03-06 23:50:47 - train: epoch 0063, iter [01700, 10009], lr: 0.001000, loss: 1.1934
2022-03-06 23:51:07 - train: epoch 0063, iter [01800, 10009], lr: 0.001000, loss: 1.4224
2022-03-06 23:51:27 - train: epoch 0063, iter [01900, 10009], lr: 0.001000, loss: 1.4787
2022-03-06 23:51:47 - train: epoch 0063, iter [02000, 10009], lr: 0.001000, loss: 1.2219
2022-03-06 23:52:06 - train: epoch 0063, iter [02100, 10009], lr: 0.001000, loss: 1.3808
2022-03-06 23:52:26 - train: epoch 0063, iter [02200, 10009], lr: 0.001000, loss: 1.3874
2022-03-06 23:52:46 - train: epoch 0063, iter [02300, 10009], lr: 0.001000, loss: 1.3957
2022-03-06 23:53:06 - train: epoch 0063, iter [02400, 10009], lr: 0.001000, loss: 1.5609
2022-03-06 23:53:25 - train: epoch 0063, iter [02500, 10009], lr: 0.001000, loss: 1.6377
2022-03-06 23:53:45 - train: epoch 0063, iter [02600, 10009], lr: 0.001000, loss: 1.0847
2022-03-06 23:54:05 - train: epoch 0063, iter [02700, 10009], lr: 0.001000, loss: 1.5695
2022-03-06 23:54:25 - train: epoch 0063, iter [02800, 10009], lr: 0.001000, loss: 1.4720
2022-03-06 23:54:45 - train: epoch 0063, iter [02900, 10009], lr: 0.001000, loss: 1.6310
2022-03-06 23:55:05 - train: epoch 0063, iter [03000, 10009], lr: 0.001000, loss: 1.2952
2022-03-06 23:55:25 - train: epoch 0063, iter [03100, 10009], lr: 0.001000, loss: 1.0523
2022-03-06 23:55:44 - train: epoch 0063, iter [03200, 10009], lr: 0.001000, loss: 1.4917
2022-03-06 23:56:04 - train: epoch 0063, iter [03300, 10009], lr: 0.001000, loss: 1.3583
2022-03-06 23:56:24 - train: epoch 0063, iter [03400, 10009], lr: 0.001000, loss: 1.4415
2022-03-06 23:56:44 - train: epoch 0063, iter [03500, 10009], lr: 0.001000, loss: 1.3236
2022-03-06 23:57:04 - train: epoch 0063, iter [03600, 10009], lr: 0.001000, loss: 1.5089
2022-03-06 23:57:24 - train: epoch 0063, iter [03700, 10009], lr: 0.001000, loss: 1.1975
2022-03-06 23:57:43 - train: epoch 0063, iter [03800, 10009], lr: 0.001000, loss: 1.4651
2022-03-06 23:58:03 - train: epoch 0063, iter [03900, 10009], lr: 0.001000, loss: 1.3208
2022-03-06 23:58:23 - train: epoch 0063, iter [04000, 10009], lr: 0.001000, loss: 1.1561
2022-03-06 23:58:43 - train: epoch 0063, iter [04100, 10009], lr: 0.001000, loss: 1.2664
2022-03-06 23:59:03 - train: epoch 0063, iter [04200, 10009], lr: 0.001000, loss: 1.3883
2022-03-06 23:59:23 - train: epoch 0063, iter [04300, 10009], lr: 0.001000, loss: 1.5445
2022-03-06 23:59:43 - train: epoch 0063, iter [04400, 10009], lr: 0.001000, loss: 1.7130
2022-03-07 00:00:02 - train: epoch 0063, iter [04500, 10009], lr: 0.001000, loss: 1.2063
2022-03-07 00:00:22 - train: epoch 0063, iter [04600, 10009], lr: 0.001000, loss: 1.4341
2022-03-07 00:00:42 - train: epoch 0063, iter [04700, 10009], lr: 0.001000, loss: 1.5491
2022-03-07 00:01:02 - train: epoch 0063, iter [04800, 10009], lr: 0.001000, loss: 1.3192
2022-03-07 00:01:22 - train: epoch 0063, iter [04900, 10009], lr: 0.001000, loss: 1.6092
2022-03-07 00:01:42 - train: epoch 0063, iter [05000, 10009], lr: 0.001000, loss: 1.3319
2022-03-07 00:02:01 - train: epoch 0063, iter [05100, 10009], lr: 0.001000, loss: 1.2843
2022-03-07 00:02:21 - train: epoch 0063, iter [05200, 10009], lr: 0.001000, loss: 1.5263
2022-03-07 00:02:41 - train: epoch 0063, iter [05300, 10009], lr: 0.001000, loss: 1.3444
2022-03-07 00:03:01 - train: epoch 0063, iter [05400, 10009], lr: 0.001000, loss: 1.7112
2022-03-07 00:03:20 - train: epoch 0063, iter [05500, 10009], lr: 0.001000, loss: 1.2866
2022-03-07 00:03:40 - train: epoch 0063, iter [05600, 10009], lr: 0.001000, loss: 1.5356
2022-03-07 00:04:00 - train: epoch 0063, iter [05700, 10009], lr: 0.001000, loss: 1.3395
2022-03-07 00:04:20 - train: epoch 0063, iter [05800, 10009], lr: 0.001000, loss: 1.5082
2022-03-07 00:04:40 - train: epoch 0063, iter [05900, 10009], lr: 0.001000, loss: 1.2913
2022-03-07 00:04:59 - train: epoch 0063, iter [06000, 10009], lr: 0.001000, loss: 1.6103
2022-03-07 00:05:19 - train: epoch 0063, iter [06100, 10009], lr: 0.001000, loss: 1.4568
2022-03-07 00:05:39 - train: epoch 0063, iter [06200, 10009], lr: 0.001000, loss: 1.3916
2022-03-07 00:05:59 - train: epoch 0063, iter [06300, 10009], lr: 0.001000, loss: 1.7930
2022-03-07 00:06:19 - train: epoch 0063, iter [06400, 10009], lr: 0.001000, loss: 1.2059
2022-03-07 00:06:38 - train: epoch 0063, iter [06500, 10009], lr: 0.001000, loss: 1.3080
2022-03-07 00:06:58 - train: epoch 0063, iter [06600, 10009], lr: 0.001000, loss: 1.2189
2022-03-07 00:07:18 - train: epoch 0063, iter [06700, 10009], lr: 0.001000, loss: 1.4399
2022-03-07 00:07:38 - train: epoch 0063, iter [06800, 10009], lr: 0.001000, loss: 1.2798
2022-03-07 00:07:58 - train: epoch 0063, iter [06900, 10009], lr: 0.001000, loss: 1.4104
2022-03-07 00:08:18 - train: epoch 0063, iter [07000, 10009], lr: 0.001000, loss: 1.5360
2022-03-07 00:08:37 - train: epoch 0063, iter [07100, 10009], lr: 0.001000, loss: 1.4810
2022-03-07 00:08:57 - train: epoch 0063, iter [07200, 10009], lr: 0.001000, loss: 1.2403
2022-03-07 00:09:17 - train: epoch 0063, iter [07300, 10009], lr: 0.001000, loss: 1.3613
2022-03-07 00:09:37 - train: epoch 0063, iter [07400, 10009], lr: 0.001000, loss: 1.4001
2022-03-07 00:09:57 - train: epoch 0063, iter [07500, 10009], lr: 0.001000, loss: 0.9996
2022-03-07 00:10:17 - train: epoch 0063, iter [07600, 10009], lr: 0.001000, loss: 1.5576
2022-03-07 00:10:37 - train: epoch 0063, iter [07700, 10009], lr: 0.001000, loss: 1.0657
2022-03-07 00:10:57 - train: epoch 0063, iter [07800, 10009], lr: 0.001000, loss: 1.3412
2022-03-07 00:11:17 - train: epoch 0063, iter [07900, 10009], lr: 0.001000, loss: 1.5272
2022-03-07 00:11:36 - train: epoch 0063, iter [08000, 10009], lr: 0.001000, loss: 0.9658
2022-03-07 00:11:56 - train: epoch 0063, iter [08100, 10009], lr: 0.001000, loss: 1.5260
2022-03-07 00:12:16 - train: epoch 0063, iter [08200, 10009], lr: 0.001000, loss: 1.2796
2022-03-07 00:12:36 - train: epoch 0063, iter [08300, 10009], lr: 0.001000, loss: 1.2555
2022-03-07 00:12:56 - train: epoch 0063, iter [08400, 10009], lr: 0.001000, loss: 1.3717
2022-03-07 00:13:16 - train: epoch 0063, iter [08500, 10009], lr: 0.001000, loss: 1.2337
2022-03-07 00:13:36 - train: epoch 0063, iter [08600, 10009], lr: 0.001000, loss: 1.3155
2022-03-07 00:13:56 - train: epoch 0063, iter [08700, 10009], lr: 0.001000, loss: 1.2931
2022-03-07 00:14:15 - train: epoch 0063, iter [08800, 10009], lr: 0.001000, loss: 1.2595
2022-03-07 00:14:35 - train: epoch 0063, iter [08900, 10009], lr: 0.001000, loss: 1.3383
2022-03-07 00:14:55 - train: epoch 0063, iter [09000, 10009], lr: 0.001000, loss: 1.1376
2022-03-07 00:15:15 - train: epoch 0063, iter [09100, 10009], lr: 0.001000, loss: 1.5447
2022-03-07 00:15:35 - train: epoch 0063, iter [09200, 10009], lr: 0.001000, loss: 1.3887
2022-03-07 00:15:55 - train: epoch 0063, iter [09300, 10009], lr: 0.001000, loss: 1.4176
2022-03-07 00:16:15 - train: epoch 0063, iter [09400, 10009], lr: 0.001000, loss: 1.4614
2022-03-07 00:16:35 - train: epoch 0063, iter [09500, 10009], lr: 0.001000, loss: 1.4805
2022-03-07 00:16:55 - train: epoch 0063, iter [09600, 10009], lr: 0.001000, loss: 1.1609
2022-03-07 00:17:14 - train: epoch 0063, iter [09700, 10009], lr: 0.001000, loss: 1.3084
2022-03-07 00:17:34 - train: epoch 0063, iter [09800, 10009], lr: 0.001000, loss: 1.5622
2022-03-07 00:17:54 - train: epoch 0063, iter [09900, 10009], lr: 0.001000, loss: 1.3165
2022-03-07 00:18:14 - train: epoch 0063, iter [10000, 10009], lr: 0.001000, loss: 1.5106
2022-03-07 00:18:17 - train: epoch 063, train_loss: 1.3522
2022-03-07 00:19:31 - eval: epoch: 063, acc1: 71.540%, acc5: 90.256%, test_loss: 1.1509, per_image_load_time: 1.950ms, per_image_inference_time: 0.867ms
2022-03-07 00:19:32 - until epoch: 063, best_acc1: 71.540%
2022-03-07 00:19:32 - epoch 064 lr: 0.0010000000000000002
2022-03-07 00:19:56 - train: epoch 0064, iter [00100, 10009], lr: 0.001000, loss: 1.3796
2022-03-07 00:20:15 - train: epoch 0064, iter [00200, 10009], lr: 0.001000, loss: 1.2602
2022-03-07 00:20:35 - train: epoch 0064, iter [00300, 10009], lr: 0.001000, loss: 1.6283
2022-03-07 00:20:54 - train: epoch 0064, iter [00400, 10009], lr: 0.001000, loss: 1.7232
2022-03-07 00:21:14 - train: epoch 0064, iter [00500, 10009], lr: 0.001000, loss: 1.1292
2022-03-07 00:21:33 - train: epoch 0064, iter [00600, 10009], lr: 0.001000, loss: 1.2912
2022-03-07 00:21:53 - train: epoch 0064, iter [00700, 10009], lr: 0.001000, loss: 1.4541
2022-03-07 00:22:13 - train: epoch 0064, iter [00800, 10009], lr: 0.001000, loss: 1.3028
2022-03-07 00:22:33 - train: epoch 0064, iter [00900, 10009], lr: 0.001000, loss: 1.0725
2022-03-07 00:22:52 - train: epoch 0064, iter [01000, 10009], lr: 0.001000, loss: 1.0799
2022-03-07 00:23:12 - train: epoch 0064, iter [01100, 10009], lr: 0.001000, loss: 1.4885
2022-03-07 00:23:32 - train: epoch 0064, iter [01200, 10009], lr: 0.001000, loss: 1.3367
2022-03-07 00:23:52 - train: epoch 0064, iter [01300, 10009], lr: 0.001000, loss: 1.3707
2022-03-07 00:24:11 - train: epoch 0064, iter [01400, 10009], lr: 0.001000, loss: 1.4693
2022-03-07 00:24:31 - train: epoch 0064, iter [01500, 10009], lr: 0.001000, loss: 1.6556
2022-03-07 00:24:51 - train: epoch 0064, iter [01600, 10009], lr: 0.001000, loss: 1.0768
2022-03-07 00:25:11 - train: epoch 0064, iter [01700, 10009], lr: 0.001000, loss: 1.2646
2022-03-07 00:25:31 - train: epoch 0064, iter [01800, 10009], lr: 0.001000, loss: 1.3558
2022-03-07 00:25:50 - train: epoch 0064, iter [01900, 10009], lr: 0.001000, loss: 1.7877
2022-03-07 00:26:10 - train: epoch 0064, iter [02000, 10009], lr: 0.001000, loss: 1.2468
2022-03-07 00:26:30 - train: epoch 0064, iter [02100, 10009], lr: 0.001000, loss: 1.3388
2022-03-07 00:26:50 - train: epoch 0064, iter [02200, 10009], lr: 0.001000, loss: 1.3452
2022-03-07 00:27:09 - train: epoch 0064, iter [02300, 10009], lr: 0.001000, loss: 1.2308
2022-03-07 00:27:29 - train: epoch 0064, iter [02400, 10009], lr: 0.001000, loss: 1.2113
2022-03-07 00:27:49 - train: epoch 0064, iter [02500, 10009], lr: 0.001000, loss: 1.4748
2022-03-07 00:28:09 - train: epoch 0064, iter [02600, 10009], lr: 0.001000, loss: 1.1363
2022-03-07 00:28:28 - train: epoch 0064, iter [02700, 10009], lr: 0.001000, loss: 1.4339
2022-03-07 00:28:48 - train: epoch 0064, iter [02800, 10009], lr: 0.001000, loss: 1.7220
2022-03-07 00:29:08 - train: epoch 0064, iter [02900, 10009], lr: 0.001000, loss: 1.2344
2022-03-07 00:29:28 - train: epoch 0064, iter [03000, 10009], lr: 0.001000, loss: 1.4103
2022-03-07 00:29:48 - train: epoch 0064, iter [03100, 10009], lr: 0.001000, loss: 1.6028
2022-03-07 00:30:07 - train: epoch 0064, iter [03200, 10009], lr: 0.001000, loss: 1.2621
2022-03-07 00:30:27 - train: epoch 0064, iter [03300, 10009], lr: 0.001000, loss: 1.6246
2022-03-07 00:30:47 - train: epoch 0064, iter [03400, 10009], lr: 0.001000, loss: 1.2198
2022-03-07 00:31:07 - train: epoch 0064, iter [03500, 10009], lr: 0.001000, loss: 1.1672
2022-03-07 00:31:26 - train: epoch 0064, iter [03600, 10009], lr: 0.001000, loss: 1.1024
2022-03-07 00:31:46 - train: epoch 0064, iter [03700, 10009], lr: 0.001000, loss: 1.2948
2022-03-07 00:32:06 - train: epoch 0064, iter [03800, 10009], lr: 0.001000, loss: 1.3449
2022-03-07 00:32:26 - train: epoch 0064, iter [03900, 10009], lr: 0.001000, loss: 1.2943
2022-03-07 00:32:45 - train: epoch 0064, iter [04000, 10009], lr: 0.001000, loss: 1.3710
2022-03-07 00:33:05 - train: epoch 0064, iter [04100, 10009], lr: 0.001000, loss: 1.3764
2022-03-07 00:33:25 - train: epoch 0064, iter [04200, 10009], lr: 0.001000, loss: 1.3277
2022-03-07 00:33:45 - train: epoch 0064, iter [04300, 10009], lr: 0.001000, loss: 1.3035
2022-03-07 00:34:05 - train: epoch 0064, iter [04400, 10009], lr: 0.001000, loss: 1.5327
2022-03-07 00:34:24 - train: epoch 0064, iter [04500, 10009], lr: 0.001000, loss: 1.5484
2022-03-07 00:34:44 - train: epoch 0064, iter [04600, 10009], lr: 0.001000, loss: 1.3084
2022-03-07 00:35:04 - train: epoch 0064, iter [04700, 10009], lr: 0.001000, loss: 1.3042
2022-03-07 00:35:24 - train: epoch 0064, iter [04800, 10009], lr: 0.001000, loss: 1.3689
2022-03-07 00:35:44 - train: epoch 0064, iter [04900, 10009], lr: 0.001000, loss: 1.3778
2022-03-07 00:36:03 - train: epoch 0064, iter [05000, 10009], lr: 0.001000, loss: 1.0550
2022-03-07 00:36:23 - train: epoch 0064, iter [05100, 10009], lr: 0.001000, loss: 1.3196
2022-03-07 00:36:43 - train: epoch 0064, iter [05200, 10009], lr: 0.001000, loss: 1.2950
2022-03-07 00:37:03 - train: epoch 0064, iter [05300, 10009], lr: 0.001000, loss: 1.2019
2022-03-07 00:37:22 - train: epoch 0064, iter [05400, 10009], lr: 0.001000, loss: 1.4241
2022-03-07 00:37:42 - train: epoch 0064, iter [05500, 10009], lr: 0.001000, loss: 1.3463
2022-03-07 00:38:02 - train: epoch 0064, iter [05600, 10009], lr: 0.001000, loss: 1.3127
2022-03-07 00:38:21 - train: epoch 0064, iter [05700, 10009], lr: 0.001000, loss: 1.2850
2022-03-07 00:38:41 - train: epoch 0064, iter [05800, 10009], lr: 0.001000, loss: 1.5988
2022-03-07 00:39:01 - train: epoch 0064, iter [05900, 10009], lr: 0.001000, loss: 1.3849
2022-03-07 00:39:21 - train: epoch 0064, iter [06000, 10009], lr: 0.001000, loss: 1.2939
2022-03-07 00:39:41 - train: epoch 0064, iter [06100, 10009], lr: 0.001000, loss: 1.1640
2022-03-07 00:40:00 - train: epoch 0064, iter [06200, 10009], lr: 0.001000, loss: 1.3185
2022-03-07 00:40:20 - train: epoch 0064, iter [06300, 10009], lr: 0.001000, loss: 1.6517
2022-03-07 00:40:40 - train: epoch 0064, iter [06400, 10009], lr: 0.001000, loss: 1.4054
2022-03-07 00:41:00 - train: epoch 0064, iter [06500, 10009], lr: 0.001000, loss: 1.4083
2022-03-07 00:41:20 - train: epoch 0064, iter [06600, 10009], lr: 0.001000, loss: 1.1893
2022-03-07 00:41:39 - train: epoch 0064, iter [06700, 10009], lr: 0.001000, loss: 1.4299
2022-03-07 00:41:59 - train: epoch 0064, iter [06800, 10009], lr: 0.001000, loss: 1.3225
2022-03-07 00:42:19 - train: epoch 0064, iter [06900, 10009], lr: 0.001000, loss: 1.2993
2022-03-07 00:42:39 - train: epoch 0064, iter [07000, 10009], lr: 0.001000, loss: 1.1384
2022-03-07 00:42:58 - train: epoch 0064, iter [07100, 10009], lr: 0.001000, loss: 1.3560
2022-03-07 00:43:19 - train: epoch 0064, iter [07200, 10009], lr: 0.001000, loss: 1.3246
2022-03-07 00:43:39 - train: epoch 0064, iter [07300, 10009], lr: 0.001000, loss: 1.2922
2022-03-07 00:43:59 - train: epoch 0064, iter [07400, 10009], lr: 0.001000, loss: 1.0801
2022-03-07 00:44:18 - train: epoch 0064, iter [07500, 10009], lr: 0.001000, loss: 1.5646
2022-03-07 00:44:38 - train: epoch 0064, iter [07600, 10009], lr: 0.001000, loss: 1.5172
2022-03-07 00:44:58 - train: epoch 0064, iter [07700, 10009], lr: 0.001000, loss: 1.3010
2022-03-07 00:45:17 - train: epoch 0064, iter [07800, 10009], lr: 0.001000, loss: 1.3430
2022-03-07 00:45:37 - train: epoch 0064, iter [07900, 10009], lr: 0.001000, loss: 1.1504
2022-03-07 00:45:57 - train: epoch 0064, iter [08000, 10009], lr: 0.001000, loss: 1.4160
2022-03-07 00:46:17 - train: epoch 0064, iter [08100, 10009], lr: 0.001000, loss: 1.4022
2022-03-07 00:46:37 - train: epoch 0064, iter [08200, 10009], lr: 0.001000, loss: 1.4328
2022-03-07 00:46:57 - train: epoch 0064, iter [08300, 10009], lr: 0.001000, loss: 1.1385
2022-03-07 00:47:17 - train: epoch 0064, iter [08400, 10009], lr: 0.001000, loss: 0.9453
2022-03-07 00:47:36 - train: epoch 0064, iter [08500, 10009], lr: 0.001000, loss: 1.3046
2022-03-07 00:47:56 - train: epoch 0064, iter [08600, 10009], lr: 0.001000, loss: 1.4597
2022-03-07 00:48:16 - train: epoch 0064, iter [08700, 10009], lr: 0.001000, loss: 1.3516
2022-03-07 00:48:36 - train: epoch 0064, iter [08800, 10009], lr: 0.001000, loss: 1.3319
2022-03-07 00:48:56 - train: epoch 0064, iter [08900, 10009], lr: 0.001000, loss: 1.6372
2022-03-07 00:49:16 - train: epoch 0064, iter [09000, 10009], lr: 0.001000, loss: 1.3506
2022-03-07 00:49:35 - train: epoch 0064, iter [09100, 10009], lr: 0.001000, loss: 1.4095
2022-03-07 00:49:55 - train: epoch 0064, iter [09200, 10009], lr: 0.001000, loss: 1.5120
2022-03-07 00:50:15 - train: epoch 0064, iter [09300, 10009], lr: 0.001000, loss: 1.2109
2022-03-07 00:50:35 - train: epoch 0064, iter [09400, 10009], lr: 0.001000, loss: 1.4135
2022-03-07 00:50:55 - train: epoch 0064, iter [09500, 10009], lr: 0.001000, loss: 0.9853
2022-03-07 00:51:15 - train: epoch 0064, iter [09600, 10009], lr: 0.001000, loss: 1.6909
2022-03-07 00:51:35 - train: epoch 0064, iter [09700, 10009], lr: 0.001000, loss: 1.5801
2022-03-07 00:51:55 - train: epoch 0064, iter [09800, 10009], lr: 0.001000, loss: 1.4609
2022-03-07 00:52:14 - train: epoch 0064, iter [09900, 10009], lr: 0.001000, loss: 1.4447
2022-03-07 00:52:34 - train: epoch 0064, iter [10000, 10009], lr: 0.001000, loss: 1.1431
2022-03-07 00:52:37 - train: epoch 064, train_loss: 1.3373
2022-03-07 00:53:51 - eval: epoch: 064, acc1: 71.790%, acc5: 90.322%, test_loss: 1.1404, per_image_load_time: 2.039ms, per_image_inference_time: 0.795ms
2022-03-07 00:53:52 - until epoch: 064, best_acc1: 71.790%
2022-03-07 00:53:52 - epoch 065 lr: 0.0010000000000000002
2022-03-07 00:54:16 - train: epoch 0065, iter [00100, 10009], lr: 0.001000, loss: 1.0794
2022-03-07 00:54:35 - train: epoch 0065, iter [00200, 10009], lr: 0.001000, loss: 1.3728
2022-03-07 00:54:55 - train: epoch 0065, iter [00300, 10009], lr: 0.001000, loss: 1.3185
2022-03-07 00:55:15 - train: epoch 0065, iter [00400, 10009], lr: 0.001000, loss: 1.2544
2022-03-07 00:55:35 - train: epoch 0065, iter [00500, 10009], lr: 0.001000, loss: 1.2941
2022-03-07 00:55:55 - train: epoch 0065, iter [00600, 10009], lr: 0.001000, loss: 0.8440
2022-03-07 00:56:15 - train: epoch 0065, iter [00700, 10009], lr: 0.001000, loss: 1.3102
2022-03-07 00:56:34 - train: epoch 0065, iter [00800, 10009], lr: 0.001000, loss: 1.1873
2022-03-07 00:56:54 - train: epoch 0065, iter [00900, 10009], lr: 0.001000, loss: 1.1698
2022-03-07 00:57:14 - train: epoch 0065, iter [01000, 10009], lr: 0.001000, loss: 1.2928
2022-03-07 00:57:34 - train: epoch 0065, iter [01100, 10009], lr: 0.001000, loss: 1.2542
2022-03-07 00:57:54 - train: epoch 0065, iter [01200, 10009], lr: 0.001000, loss: 1.4211
2022-03-07 00:58:14 - train: epoch 0065, iter [01300, 10009], lr: 0.001000, loss: 1.3022
2022-03-07 00:58:34 - train: epoch 0065, iter [01400, 10009], lr: 0.001000, loss: 1.3831
2022-03-07 00:58:54 - train: epoch 0065, iter [01500, 10009], lr: 0.001000, loss: 1.2051
2022-03-07 00:59:14 - train: epoch 0065, iter [01600, 10009], lr: 0.001000, loss: 1.0933
2022-03-07 00:59:34 - train: epoch 0065, iter [01700, 10009], lr: 0.001000, loss: 1.0677
2022-03-07 00:59:54 - train: epoch 0065, iter [01800, 10009], lr: 0.001000, loss: 1.3886
2022-03-07 01:00:14 - train: epoch 0065, iter [01900, 10009], lr: 0.001000, loss: 1.5859
2022-03-07 01:00:34 - train: epoch 0065, iter [02000, 10009], lr: 0.001000, loss: 1.3013
2022-03-07 01:00:54 - train: epoch 0065, iter [02100, 10009], lr: 0.001000, loss: 1.4316
2022-03-07 01:01:14 - train: epoch 0065, iter [02200, 10009], lr: 0.001000, loss: 1.3573
2022-03-07 01:01:34 - train: epoch 0065, iter [02300, 10009], lr: 0.001000, loss: 1.3843
2022-03-07 01:01:54 - train: epoch 0065, iter [02400, 10009], lr: 0.001000, loss: 1.6904
2022-03-07 01:02:14 - train: epoch 0065, iter [02500, 10009], lr: 0.001000, loss: 1.2951
2022-03-07 01:02:34 - train: epoch 0065, iter [02600, 10009], lr: 0.001000, loss: 1.2048
2022-03-07 01:02:54 - train: epoch 0065, iter [02700, 10009], lr: 0.001000, loss: 1.1543
2022-03-07 01:03:14 - train: epoch 0065, iter [02800, 10009], lr: 0.001000, loss: 1.4571
2022-03-07 01:03:34 - train: epoch 0065, iter [02900, 10009], lr: 0.001000, loss: 1.1954
2022-03-07 01:03:54 - train: epoch 0065, iter [03000, 10009], lr: 0.001000, loss: 0.9783
2022-03-07 01:04:14 - train: epoch 0065, iter [03100, 10009], lr: 0.001000, loss: 1.6923
2022-03-07 01:04:34 - train: epoch 0065, iter [03200, 10009], lr: 0.001000, loss: 1.4846
2022-03-07 01:04:54 - train: epoch 0065, iter [03300, 10009], lr: 0.001000, loss: 1.3670
2022-03-07 01:05:14 - train: epoch 0065, iter [03400, 10009], lr: 0.001000, loss: 1.2540
2022-03-07 01:05:34 - train: epoch 0065, iter [03500, 10009], lr: 0.001000, loss: 1.0090
2022-03-07 01:05:54 - train: epoch 0065, iter [03600, 10009], lr: 0.001000, loss: 1.3235
2022-03-07 01:06:14 - train: epoch 0065, iter [03700, 10009], lr: 0.001000, loss: 1.4523
2022-03-07 01:06:34 - train: epoch 0065, iter [03800, 10009], lr: 0.001000, loss: 1.1509
2022-03-07 01:06:54 - train: epoch 0065, iter [03900, 10009], lr: 0.001000, loss: 1.1551
2022-03-07 01:07:14 - train: epoch 0065, iter [04000, 10009], lr: 0.001000, loss: 1.4862
2022-03-07 01:07:34 - train: epoch 0065, iter [04100, 10009], lr: 0.001000, loss: 1.0979
2022-03-07 01:07:54 - train: epoch 0065, iter [04200, 10009], lr: 0.001000, loss: 1.3411
2022-03-07 01:08:14 - train: epoch 0065, iter [04300, 10009], lr: 0.001000, loss: 0.9815
2022-03-07 01:08:34 - train: epoch 0065, iter [04400, 10009], lr: 0.001000, loss: 1.1934
2022-03-07 01:08:53 - train: epoch 0065, iter [04500, 10009], lr: 0.001000, loss: 1.3470
2022-03-07 01:09:13 - train: epoch 0065, iter [04600, 10009], lr: 0.001000, loss: 1.3419
2022-03-07 01:09:33 - train: epoch 0065, iter [04700, 10009], lr: 0.001000, loss: 1.4620
2022-03-07 01:09:53 - train: epoch 0065, iter [04800, 10009], lr: 0.001000, loss: 1.1395
2022-03-07 01:10:13 - train: epoch 0065, iter [04900, 10009], lr: 0.001000, loss: 1.7122
2022-03-07 01:10:33 - train: epoch 0065, iter [05000, 10009], lr: 0.001000, loss: 1.3529
2022-03-07 01:10:53 - train: epoch 0065, iter [05100, 10009], lr: 0.001000, loss: 1.3008
2022-03-07 01:11:12 - train: epoch 0065, iter [05200, 10009], lr: 0.001000, loss: 1.6162
2022-03-07 01:11:33 - train: epoch 0065, iter [05300, 10009], lr: 0.001000, loss: 1.1276
2022-03-07 01:11:53 - train: epoch 0065, iter [05400, 10009], lr: 0.001000, loss: 1.4236
2022-03-07 01:12:12 - train: epoch 0065, iter [05500, 10009], lr: 0.001000, loss: 1.2349
2022-03-07 01:12:32 - train: epoch 0065, iter [05600, 10009], lr: 0.001000, loss: 1.2863
2022-03-07 01:12:52 - train: epoch 0065, iter [05700, 10009], lr: 0.001000, loss: 1.0316
2022-03-07 01:13:12 - train: epoch 0065, iter [05800, 10009], lr: 0.001000, loss: 1.4160
2022-03-07 01:13:32 - train: epoch 0065, iter [05900, 10009], lr: 0.001000, loss: 1.4664
2022-03-07 01:13:52 - train: epoch 0065, iter [06000, 10009], lr: 0.001000, loss: 1.1777
2022-03-07 01:14:11 - train: epoch 0065, iter [06100, 10009], lr: 0.001000, loss: 1.3755
2022-03-07 01:14:31 - train: epoch 0065, iter [06200, 10009], lr: 0.001000, loss: 1.4721
2022-03-07 01:14:51 - train: epoch 0065, iter [06300, 10009], lr: 0.001000, loss: 1.2734
2022-03-07 01:15:11 - train: epoch 0065, iter [06400, 10009], lr: 0.001000, loss: 1.3322
2022-03-07 01:15:31 - train: epoch 0065, iter [06500, 10009], lr: 0.001000, loss: 1.6203
2022-03-07 01:15:51 - train: epoch 0065, iter [06600, 10009], lr: 0.001000, loss: 1.1860
2022-03-07 01:16:11 - train: epoch 0065, iter [06700, 10009], lr: 0.001000, loss: 1.4458
2022-03-07 01:16:30 - train: epoch 0065, iter [06800, 10009], lr: 0.001000, loss: 1.2343
2022-03-07 01:16:50 - train: epoch 0065, iter [06900, 10009], lr: 0.001000, loss: 1.3008
2022-03-07 01:17:10 - train: epoch 0065, iter [07000, 10009], lr: 0.001000, loss: 1.5366
2022-03-07 01:17:30 - train: epoch 0065, iter [07100, 10009], lr: 0.001000, loss: 1.3431
2022-03-07 01:17:50 - train: epoch 0065, iter [07200, 10009], lr: 0.001000, loss: 1.3949
2022-03-07 01:18:10 - train: epoch 0065, iter [07300, 10009], lr: 0.001000, loss: 1.3168
2022-03-07 01:18:29 - train: epoch 0065, iter [07400, 10009], lr: 0.001000, loss: 1.2212
2022-03-07 01:18:49 - train: epoch 0065, iter [07500, 10009], lr: 0.001000, loss: 1.4728
2022-03-07 01:19:09 - train: epoch 0065, iter [07600, 10009], lr: 0.001000, loss: 1.5364
2022-03-07 01:19:29 - train: epoch 0065, iter [07700, 10009], lr: 0.001000, loss: 1.4443
2022-03-07 01:19:49 - train: epoch 0065, iter [07800, 10009], lr: 0.001000, loss: 1.6133
2022-03-07 01:20:09 - train: epoch 0065, iter [07900, 10009], lr: 0.001000, loss: 1.4449
2022-03-07 01:20:29 - train: epoch 0065, iter [08000, 10009], lr: 0.001000, loss: 1.2382
2022-03-07 01:20:49 - train: epoch 0065, iter [08100, 10009], lr: 0.001000, loss: 1.4753
2022-03-07 01:21:09 - train: epoch 0065, iter [08200, 10009], lr: 0.001000, loss: 1.4440
2022-03-07 01:21:28 - train: epoch 0065, iter [08300, 10009], lr: 0.001000, loss: 1.6558
2022-03-07 01:21:48 - train: epoch 0065, iter [08400, 10009], lr: 0.001000, loss: 1.4441
2022-03-07 01:22:08 - train: epoch 0065, iter [08500, 10009], lr: 0.001000, loss: 1.5794
2022-03-07 01:22:28 - train: epoch 0065, iter [08600, 10009], lr: 0.001000, loss: 1.2630
2022-03-07 01:22:48 - train: epoch 0065, iter [08700, 10009], lr: 0.001000, loss: 1.3286
2022-03-07 01:23:08 - train: epoch 0065, iter [08800, 10009], lr: 0.001000, loss: 1.4022
2022-03-07 01:23:28 - train: epoch 0065, iter [08900, 10009], lr: 0.001000, loss: 1.3915
2022-03-07 01:23:48 - train: epoch 0065, iter [09000, 10009], lr: 0.001000, loss: 1.1117
2022-03-07 01:24:08 - train: epoch 0065, iter [09100, 10009], lr: 0.001000, loss: 1.6182
2022-03-07 01:24:27 - train: epoch 0065, iter [09200, 10009], lr: 0.001000, loss: 1.4561
2022-03-07 01:24:47 - train: epoch 0065, iter [09300, 10009], lr: 0.001000, loss: 1.3437
2022-03-07 01:25:07 - train: epoch 0065, iter [09400, 10009], lr: 0.001000, loss: 1.6255
2022-03-07 01:25:27 - train: epoch 0065, iter [09500, 10009], lr: 0.001000, loss: 1.3064
2022-03-07 01:25:47 - train: epoch 0065, iter [09600, 10009], lr: 0.001000, loss: 1.3331
2022-03-07 01:26:07 - train: epoch 0065, iter [09700, 10009], lr: 0.001000, loss: 0.9234
2022-03-07 01:26:27 - train: epoch 0065, iter [09800, 10009], lr: 0.001000, loss: 1.2348
2022-03-07 01:26:47 - train: epoch 0065, iter [09900, 10009], lr: 0.001000, loss: 1.2460
2022-03-07 01:27:07 - train: epoch 0065, iter [10000, 10009], lr: 0.001000, loss: 1.1571
2022-03-07 01:27:10 - train: epoch 065, train_loss: 1.3234
2022-03-07 01:28:25 - eval: epoch: 065, acc1: 71.944%, acc5: 90.324%, test_loss: 1.1359, per_image_load_time: 1.537ms, per_image_inference_time: 0.897ms
2022-03-07 01:28:25 - until epoch: 065, best_acc1: 71.944%
2022-03-07 01:28:25 - epoch 066 lr: 0.0010000000000000002
2022-03-07 01:28:49 - train: epoch 0066, iter [00100, 10009], lr: 0.001000, loss: 1.2983
2022-03-07 01:29:09 - train: epoch 0066, iter [00200, 10009], lr: 0.001000, loss: 1.6572
2022-03-07 01:29:29 - train: epoch 0066, iter [00300, 10009], lr: 0.001000, loss: 1.3652
2022-03-07 01:29:49 - train: epoch 0066, iter [00400, 10009], lr: 0.001000, loss: 1.2654
2022-03-07 01:30:09 - train: epoch 0066, iter [00500, 10009], lr: 0.001000, loss: 1.3580
2022-03-07 01:30:28 - train: epoch 0066, iter [00600, 10009], lr: 0.001000, loss: 1.1347
2022-03-07 01:30:48 - train: epoch 0066, iter [00700, 10009], lr: 0.001000, loss: 1.3732
2022-03-07 01:31:08 - train: epoch 0066, iter [00800, 10009], lr: 0.001000, loss: 1.2028
2022-03-07 01:31:27 - train: epoch 0066, iter [00900, 10009], lr: 0.001000, loss: 1.2591
2022-03-07 01:31:47 - train: epoch 0066, iter [01000, 10009], lr: 0.001000, loss: 1.6808
2022-03-07 01:32:07 - train: epoch 0066, iter [01100, 10009], lr: 0.001000, loss: 1.5012
2022-03-07 01:32:26 - train: epoch 0066, iter [01200, 10009], lr: 0.001000, loss: 1.3134
2022-03-07 01:32:46 - train: epoch 0066, iter [01300, 10009], lr: 0.001000, loss: 1.2706
2022-03-07 01:33:06 - train: epoch 0066, iter [01400, 10009], lr: 0.001000, loss: 1.1892
2022-03-07 01:33:26 - train: epoch 0066, iter [01500, 10009], lr: 0.001000, loss: 1.3112
2022-03-07 01:33:45 - train: epoch 0066, iter [01600, 10009], lr: 0.001000, loss: 1.3649
2022-03-07 01:34:05 - train: epoch 0066, iter [01700, 10009], lr: 0.001000, loss: 1.0834
2022-03-07 01:34:25 - train: epoch 0066, iter [01800, 10009], lr: 0.001000, loss: 1.3363
2022-03-07 01:34:45 - train: epoch 0066, iter [01900, 10009], lr: 0.001000, loss: 1.2724
2022-03-07 01:35:04 - train: epoch 0066, iter [02000, 10009], lr: 0.001000, loss: 1.3426
2022-03-07 01:35:24 - train: epoch 0066, iter [02100, 10009], lr: 0.001000, loss: 1.8051
2022-03-07 01:35:44 - train: epoch 0066, iter [02200, 10009], lr: 0.001000, loss: 1.7859
2022-03-07 01:36:04 - train: epoch 0066, iter [02300, 10009], lr: 0.001000, loss: 1.0928
2022-03-07 01:36:24 - train: epoch 0066, iter [02400, 10009], lr: 0.001000, loss: 1.3002
2022-03-07 01:36:43 - train: epoch 0066, iter [02500, 10009], lr: 0.001000, loss: 1.0936
2022-03-07 01:37:03 - train: epoch 0066, iter [02600, 10009], lr: 0.001000, loss: 1.4598
2022-03-07 01:37:23 - train: epoch 0066, iter [02700, 10009], lr: 0.001000, loss: 1.3963
2022-03-07 01:37:42 - train: epoch 0066, iter [02800, 10009], lr: 0.001000, loss: 1.2291
2022-03-07 01:38:02 - train: epoch 0066, iter [02900, 10009], lr: 0.001000, loss: 1.3500
2022-03-07 01:38:22 - train: epoch 0066, iter [03000, 10009], lr: 0.001000, loss: 1.1048
2022-03-07 01:38:42 - train: epoch 0066, iter [03100, 10009], lr: 0.001000, loss: 1.0830
2022-03-07 01:39:01 - train: epoch 0066, iter [03200, 10009], lr: 0.001000, loss: 1.2281
2022-03-07 01:39:21 - train: epoch 0066, iter [03300, 10009], lr: 0.001000, loss: 1.4677
2022-03-07 01:39:41 - train: epoch 0066, iter [03400, 10009], lr: 0.001000, loss: 1.3038
2022-03-07 01:40:01 - train: epoch 0066, iter [03500, 10009], lr: 0.001000, loss: 1.1293
2022-03-07 01:40:20 - train: epoch 0066, iter [03600, 10009], lr: 0.001000, loss: 1.2762
2022-03-07 01:40:40 - train: epoch 0066, iter [03700, 10009], lr: 0.001000, loss: 1.3465
2022-03-07 01:41:00 - train: epoch 0066, iter [03800, 10009], lr: 0.001000, loss: 1.3659
2022-03-07 01:41:20 - train: epoch 0066, iter [03900, 10009], lr: 0.001000, loss: 1.0323
2022-03-07 01:41:40 - train: epoch 0066, iter [04000, 10009], lr: 0.001000, loss: 1.1757
2022-03-07 01:42:00 - train: epoch 0066, iter [04100, 10009], lr: 0.001000, loss: 1.3720
2022-03-07 01:42:20 - train: epoch 0066, iter [04200, 10009], lr: 0.001000, loss: 1.3678
2022-03-07 01:42:39 - train: epoch 0066, iter [04300, 10009], lr: 0.001000, loss: 1.4100
2022-03-07 01:42:59 - train: epoch 0066, iter [04400, 10009], lr: 0.001000, loss: 1.4146
2022-03-07 01:43:19 - train: epoch 0066, iter [04500, 10009], lr: 0.001000, loss: 1.2051
2022-03-07 01:43:39 - train: epoch 0066, iter [04600, 10009], lr: 0.001000, loss: 1.4091
2022-03-07 01:43:59 - train: epoch 0066, iter [04700, 10009], lr: 0.001000, loss: 1.5220
2022-03-07 01:44:19 - train: epoch 0066, iter [04800, 10009], lr: 0.001000, loss: 1.3512
2022-03-07 01:44:39 - train: epoch 0066, iter [04900, 10009], lr: 0.001000, loss: 1.3453
2022-03-07 01:44:59 - train: epoch 0066, iter [05000, 10009], lr: 0.001000, loss: 1.6224
2022-03-07 01:45:19 - train: epoch 0066, iter [05100, 10009], lr: 0.001000, loss: 1.2977
2022-03-07 01:45:39 - train: epoch 0066, iter [05200, 10009], lr: 0.001000, loss: 1.5003
2022-03-07 01:45:59 - train: epoch 0066, iter [05300, 10009], lr: 0.001000, loss: 1.2733
2022-03-07 01:46:19 - train: epoch 0066, iter [05400, 10009], lr: 0.001000, loss: 1.4777
2022-03-07 01:46:39 - train: epoch 0066, iter [05500, 10009], lr: 0.001000, loss: 1.8031
2022-03-07 01:46:59 - train: epoch 0066, iter [05600, 10009], lr: 0.001000, loss: 1.2087
2022-03-07 01:47:19 - train: epoch 0066, iter [05700, 10009], lr: 0.001000, loss: 1.1141
2022-03-07 01:47:39 - train: epoch 0066, iter [05800, 10009], lr: 0.001000, loss: 1.0376
2022-03-07 01:47:59 - train: epoch 0066, iter [05900, 10009], lr: 0.001000, loss: 1.3934
2022-03-07 01:48:19 - train: epoch 0066, iter [06000, 10009], lr: 0.001000, loss: 1.1374
2022-03-07 01:48:39 - train: epoch 0066, iter [06100, 10009], lr: 0.001000, loss: 1.1070
2022-03-07 01:48:59 - train: epoch 0066, iter [06200, 10009], lr: 0.001000, loss: 1.6190
2022-03-07 01:49:19 - train: epoch 0066, iter [06300, 10009], lr: 0.001000, loss: 1.1001
2022-03-07 01:49:40 - train: epoch 0066, iter [06400, 10009], lr: 0.001000, loss: 1.1944
2022-03-07 01:50:00 - train: epoch 0066, iter [06500, 10009], lr: 0.001000, loss: 1.2760
2022-03-07 01:50:20 - train: epoch 0066, iter [06600, 10009], lr: 0.001000, loss: 1.2288
2022-03-07 01:50:40 - train: epoch 0066, iter [06700, 10009], lr: 0.001000, loss: 1.3231
2022-03-07 01:51:00 - train: epoch 0066, iter [06800, 10009], lr: 0.001000, loss: 1.3442
2022-03-07 01:51:20 - train: epoch 0066, iter [06900, 10009], lr: 0.001000, loss: 1.3614
2022-03-07 01:51:40 - train: epoch 0066, iter [07000, 10009], lr: 0.001000, loss: 1.4443
2022-03-07 01:52:00 - train: epoch 0066, iter [07100, 10009], lr: 0.001000, loss: 1.2404
2022-03-07 01:52:20 - train: epoch 0066, iter [07200, 10009], lr: 0.001000, loss: 1.2280
2022-03-07 01:52:40 - train: epoch 0066, iter [07300, 10009], lr: 0.001000, loss: 1.4707
2022-03-07 01:53:00 - train: epoch 0066, iter [07400, 10009], lr: 0.001000, loss: 1.4060
2022-03-07 01:53:20 - train: epoch 0066, iter [07500, 10009], lr: 0.001000, loss: 1.3137
2022-03-07 01:53:40 - train: epoch 0066, iter [07600, 10009], lr: 0.001000, loss: 1.2775
2022-03-07 01:54:00 - train: epoch 0066, iter [07700, 10009], lr: 0.001000, loss: 1.1027
2022-03-07 01:54:20 - train: epoch 0066, iter [07800, 10009], lr: 0.001000, loss: 1.1304
2022-03-07 01:54:40 - train: epoch 0066, iter [07900, 10009], lr: 0.001000, loss: 1.3293
2022-03-07 01:55:00 - train: epoch 0066, iter [08000, 10009], lr: 0.001000, loss: 1.4626
2022-03-07 01:55:20 - train: epoch 0066, iter [08100, 10009], lr: 0.001000, loss: 1.7516
2022-03-07 01:55:40 - train: epoch 0066, iter [08200, 10009], lr: 0.001000, loss: 1.1352
2022-03-07 01:56:00 - train: epoch 0066, iter [08300, 10009], lr: 0.001000, loss: 1.0818
2022-03-07 01:56:20 - train: epoch 0066, iter [08400, 10009], lr: 0.001000, loss: 1.0931
2022-03-07 01:56:40 - train: epoch 0066, iter [08500, 10009], lr: 0.001000, loss: 1.3693
2022-03-07 01:57:00 - train: epoch 0066, iter [08600, 10009], lr: 0.001000, loss: 1.3496
2022-03-07 01:57:20 - train: epoch 0066, iter [08700, 10009], lr: 0.001000, loss: 1.3255
2022-03-07 01:57:40 - train: epoch 0066, iter [08800, 10009], lr: 0.001000, loss: 1.1164
2022-03-07 01:58:00 - train: epoch 0066, iter [08900, 10009], lr: 0.001000, loss: 1.7577
2022-03-07 01:58:20 - train: epoch 0066, iter [09000, 10009], lr: 0.001000, loss: 1.4271
2022-03-07 01:58:40 - train: epoch 0066, iter [09100, 10009], lr: 0.001000, loss: 1.3054
2022-03-07 01:59:00 - train: epoch 0066, iter [09200, 10009], lr: 0.001000, loss: 1.6040
2022-03-07 01:59:20 - train: epoch 0066, iter [09300, 10009], lr: 0.001000, loss: 1.4515
2022-03-07 01:59:40 - train: epoch 0066, iter [09400, 10009], lr: 0.001000, loss: 0.9582
2022-03-07 02:00:00 - train: epoch 0066, iter [09500, 10009], lr: 0.001000, loss: 1.6205
2022-03-07 02:00:20 - train: epoch 0066, iter [09600, 10009], lr: 0.001000, loss: 1.3147
2022-03-07 02:00:40 - train: epoch 0066, iter [09700, 10009], lr: 0.001000, loss: 1.2502
2022-03-07 02:01:00 - train: epoch 0066, iter [09800, 10009], lr: 0.001000, loss: 1.2303
2022-03-07 02:01:20 - train: epoch 0066, iter [09900, 10009], lr: 0.001000, loss: 1.3804
2022-03-07 02:01:40 - train: epoch 0066, iter [10000, 10009], lr: 0.001000, loss: 1.0539
2022-03-07 02:01:43 - train: epoch 066, train_loss: 1.3166
2022-03-07 02:02:57 - eval: epoch: 066, acc1: 71.952%, acc5: 90.368%, test_loss: 1.1300, per_image_load_time: 1.191ms, per_image_inference_time: 0.822ms
2022-03-07 02:02:58 - until epoch: 066, best_acc1: 71.952%
2022-03-07 02:02:58 - epoch 067 lr: 0.0010000000000000002
2022-03-07 02:03:22 - train: epoch 0067, iter [00100, 10009], lr: 0.001000, loss: 1.0549
2022-03-07 02:03:42 - train: epoch 0067, iter [00200, 10009], lr: 0.001000, loss: 1.0981
2022-03-07 02:04:02 - train: epoch 0067, iter [00300, 10009], lr: 0.001000, loss: 0.9773
2022-03-07 02:04:22 - train: epoch 0067, iter [00400, 10009], lr: 0.001000, loss: 1.5213
2022-03-07 02:04:42 - train: epoch 0067, iter [00500, 10009], lr: 0.001000, loss: 1.1272
2022-03-07 02:05:03 - train: epoch 0067, iter [00600, 10009], lr: 0.001000, loss: 1.4684
2022-03-07 02:05:22 - train: epoch 0067, iter [00700, 10009], lr: 0.001000, loss: 1.2307
2022-03-07 02:05:42 - train: epoch 0067, iter [00800, 10009], lr: 0.001000, loss: 1.1244
2022-03-07 02:06:02 - train: epoch 0067, iter [00900, 10009], lr: 0.001000, loss: 1.0752
2022-03-07 02:06:22 - train: epoch 0067, iter [01000, 10009], lr: 0.001000, loss: 1.2492
2022-03-07 02:06:42 - train: epoch 0067, iter [01100, 10009], lr: 0.001000, loss: 1.4181
2022-03-07 02:07:02 - train: epoch 0067, iter [01200, 10009], lr: 0.001000, loss: 1.1266
2022-03-07 02:07:22 - train: epoch 0067, iter [01300, 10009], lr: 0.001000, loss: 1.4328
2022-03-07 02:07:42 - train: epoch 0067, iter [01400, 10009], lr: 0.001000, loss: 1.6212
2022-03-07 02:08:01 - train: epoch 0067, iter [01500, 10009], lr: 0.001000, loss: 1.6853
2022-03-07 02:08:21 - train: epoch 0067, iter [01600, 10009], lr: 0.001000, loss: 1.2525
2022-03-07 02:08:41 - train: epoch 0067, iter [01700, 10009], lr: 0.001000, loss: 1.2132
2022-03-07 02:09:01 - train: epoch 0067, iter [01800, 10009], lr: 0.001000, loss: 1.4464
2022-03-07 02:09:21 - train: epoch 0067, iter [01900, 10009], lr: 0.001000, loss: 1.3004
2022-03-07 02:09:41 - train: epoch 0067, iter [02000, 10009], lr: 0.001000, loss: 1.2500
2022-03-07 02:10:01 - train: epoch 0067, iter [02100, 10009], lr: 0.001000, loss: 1.8716
2022-03-07 02:10:21 - train: epoch 0067, iter [02200, 10009], lr: 0.001000, loss: 1.3392
2022-03-07 02:10:40 - train: epoch 0067, iter [02300, 10009], lr: 0.001000, loss: 1.1891
2022-03-07 02:11:00 - train: epoch 0067, iter [02400, 10009], lr: 0.001000, loss: 1.1670
2022-03-07 02:11:20 - train: epoch 0067, iter [02500, 10009], lr: 0.001000, loss: 1.2165
2022-03-07 02:11:40 - train: epoch 0067, iter [02600, 10009], lr: 0.001000, loss: 1.5112
2022-03-07 02:12:00 - train: epoch 0067, iter [02700, 10009], lr: 0.001000, loss: 1.6721
2022-03-07 02:12:20 - train: epoch 0067, iter [02800, 10009], lr: 0.001000, loss: 1.4559
2022-03-07 02:12:40 - train: epoch 0067, iter [02900, 10009], lr: 0.001000, loss: 1.2016
2022-03-07 02:13:00 - train: epoch 0067, iter [03000, 10009], lr: 0.001000, loss: 1.1444
2022-03-07 02:13:19 - train: epoch 0067, iter [03100, 10009], lr: 0.001000, loss: 1.0244
2022-03-07 02:13:39 - train: epoch 0067, iter [03200, 10009], lr: 0.001000, loss: 1.4324
2022-03-07 02:13:59 - train: epoch 0067, iter [03300, 10009], lr: 0.001000, loss: 1.4476
2022-03-07 02:14:19 - train: epoch 0067, iter [03400, 10009], lr: 0.001000, loss: 1.0403
2022-03-07 02:14:39 - train: epoch 0067, iter [03500, 10009], lr: 0.001000, loss: 1.2531
2022-03-07 02:14:58 - train: epoch 0067, iter [03600, 10009], lr: 0.001000, loss: 1.8014
2022-03-07 02:15:18 - train: epoch 0067, iter [03700, 10009], lr: 0.001000, loss: 1.2946
2022-03-07 02:15:38 - train: epoch 0067, iter [03800, 10009], lr: 0.001000, loss: 1.1459
2022-03-07 02:15:58 - train: epoch 0067, iter [03900, 10009], lr: 0.001000, loss: 1.2403
2022-03-07 02:16:18 - train: epoch 0067, iter [04000, 10009], lr: 0.001000, loss: 1.4130
2022-03-07 02:16:38 - train: epoch 0067, iter [04100, 10009], lr: 0.001000, loss: 1.3873
2022-03-07 02:16:58 - train: epoch 0067, iter [04200, 10009], lr: 0.001000, loss: 1.1390
2022-03-07 02:17:18 - train: epoch 0067, iter [04300, 10009], lr: 0.001000, loss: 1.2333
2022-03-07 02:17:38 - train: epoch 0067, iter [04400, 10009], lr: 0.001000, loss: 1.5000
2022-03-07 02:17:57 - train: epoch 0067, iter [04500, 10009], lr: 0.001000, loss: 1.3349
2022-03-07 02:18:17 - train: epoch 0067, iter [04600, 10009], lr: 0.001000, loss: 1.5746
2022-03-07 02:18:37 - train: epoch 0067, iter [04700, 10009], lr: 0.001000, loss: 1.1804
2022-03-07 02:18:57 - train: epoch 0067, iter [04800, 10009], lr: 0.001000, loss: 1.3003
2022-03-07 02:19:17 - train: epoch 0067, iter [04900, 10009], lr: 0.001000, loss: 1.5869
2022-03-07 02:19:37 - train: epoch 0067, iter [05000, 10009], lr: 0.001000, loss: 1.1297
2022-03-07 02:19:57 - train: epoch 0067, iter [05100, 10009], lr: 0.001000, loss: 1.4244
2022-03-07 02:20:17 - train: epoch 0067, iter [05200, 10009], lr: 0.001000, loss: 1.3467
2022-03-07 02:20:37 - train: epoch 0067, iter [05300, 10009], lr: 0.001000, loss: 1.1622
2022-03-07 02:20:57 - train: epoch 0067, iter [05400, 10009], lr: 0.001000, loss: 1.1086
2022-03-07 02:21:16 - train: epoch 0067, iter [05500, 10009], lr: 0.001000, loss: 1.8547
2022-03-07 02:21:36 - train: epoch 0067, iter [05600, 10009], lr: 0.001000, loss: 1.4199
2022-03-07 02:21:56 - train: epoch 0067, iter [05700, 10009], lr: 0.001000, loss: 1.1219
2022-03-07 02:22:16 - train: epoch 0067, iter [05800, 10009], lr: 0.001000, loss: 1.4717
2022-03-07 02:22:36 - train: epoch 0067, iter [05900, 10009], lr: 0.001000, loss: 1.4688
2022-03-07 02:22:56 - train: epoch 0067, iter [06000, 10009], lr: 0.001000, loss: 1.3009
2022-03-07 02:23:16 - train: epoch 0067, iter [06100, 10009], lr: 0.001000, loss: 1.4504
2022-03-07 02:23:36 - train: epoch 0067, iter [06200, 10009], lr: 0.001000, loss: 1.3561
2022-03-07 02:23:56 - train: epoch 0067, iter [06300, 10009], lr: 0.001000, loss: 1.1548
2022-03-07 02:24:16 - train: epoch 0067, iter [06400, 10009], lr: 0.001000, loss: 1.1738
2022-03-07 02:24:36 - train: epoch 0067, iter [06500, 10009], lr: 0.001000, loss: 1.3182
2022-03-07 02:24:56 - train: epoch 0067, iter [06600, 10009], lr: 0.001000, loss: 1.2251
2022-03-07 02:25:16 - train: epoch 0067, iter [06700, 10009], lr: 0.001000, loss: 1.2888
2022-03-07 02:25:36 - train: epoch 0067, iter [06800, 10009], lr: 0.001000, loss: 1.2727
2022-03-07 02:25:56 - train: epoch 0067, iter [06900, 10009], lr: 0.001000, loss: 1.1124
2022-03-07 02:26:15 - train: epoch 0067, iter [07000, 10009], lr: 0.001000, loss: 1.4654
2022-03-07 02:26:35 - train: epoch 0067, iter [07100, 10009], lr: 0.001000, loss: 1.2442
2022-03-07 02:26:55 - train: epoch 0067, iter [07200, 10009], lr: 0.001000, loss: 1.2655
2022-03-07 02:27:15 - train: epoch 0067, iter [07300, 10009], lr: 0.001000, loss: 1.4542
2022-03-07 02:27:35 - train: epoch 0067, iter [07400, 10009], lr: 0.001000, loss: 1.2611
2022-03-07 02:27:55 - train: epoch 0067, iter [07500, 10009], lr: 0.001000, loss: 1.2148
2022-03-07 02:28:15 - train: epoch 0067, iter [07600, 10009], lr: 0.001000, loss: 1.2197
2022-03-07 02:28:35 - train: epoch 0067, iter [07700, 10009], lr: 0.001000, loss: 1.1895
2022-03-07 02:28:55 - train: epoch 0067, iter [07800, 10009], lr: 0.001000, loss: 1.3405
2022-03-07 02:29:15 - train: epoch 0067, iter [07900, 10009], lr: 0.001000, loss: 1.4917
2022-03-07 02:29:35 - train: epoch 0067, iter [08000, 10009], lr: 0.001000, loss: 1.3829
2022-03-07 02:29:55 - train: epoch 0067, iter [08100, 10009], lr: 0.001000, loss: 1.4557
2022-03-07 02:30:15 - train: epoch 0067, iter [08200, 10009], lr: 0.001000, loss: 1.2038
2022-03-07 02:30:35 - train: epoch 0067, iter [08300, 10009], lr: 0.001000, loss: 1.1396
2022-03-07 02:30:55 - train: epoch 0067, iter [08400, 10009], lr: 0.001000, loss: 1.3889
2022-03-07 02:31:14 - train: epoch 0067, iter [08500, 10009], lr: 0.001000, loss: 1.2260
2022-03-07 02:31:34 - train: epoch 0067, iter [08600, 10009], lr: 0.001000, loss: 1.0615
2022-03-07 02:31:54 - train: epoch 0067, iter [08700, 10009], lr: 0.001000, loss: 1.5773
2022-03-07 02:32:14 - train: epoch 0067, iter [08800, 10009], lr: 0.001000, loss: 1.1214
2022-03-07 02:32:34 - train: epoch 0067, iter [08900, 10009], lr: 0.001000, loss: 1.1558
2022-03-07 02:32:54 - train: epoch 0067, iter [09000, 10009], lr: 0.001000, loss: 1.2744
2022-03-07 02:33:14 - train: epoch 0067, iter [09100, 10009], lr: 0.001000, loss: 1.2684
2022-03-07 02:33:34 - train: epoch 0067, iter [09200, 10009], lr: 0.001000, loss: 1.0549
2022-03-07 02:33:55 - train: epoch 0067, iter [09300, 10009], lr: 0.001000, loss: 1.4711
2022-03-07 02:34:15 - train: epoch 0067, iter [09400, 10009], lr: 0.001000, loss: 1.2748
2022-03-07 02:34:35 - train: epoch 0067, iter [09500, 10009], lr: 0.001000, loss: 1.5196
2022-03-07 02:34:55 - train: epoch 0067, iter [09600, 10009], lr: 0.001000, loss: 1.4665
2022-03-07 02:35:15 - train: epoch 0067, iter [09700, 10009], lr: 0.001000, loss: 1.1970
2022-03-07 02:35:34 - train: epoch 0067, iter [09800, 10009], lr: 0.001000, loss: 1.3671
2022-03-07 02:35:54 - train: epoch 0067, iter [09900, 10009], lr: 0.001000, loss: 1.2086
2022-03-07 02:36:14 - train: epoch 0067, iter [10000, 10009], lr: 0.001000, loss: 1.3331
2022-03-07 02:36:17 - train: epoch 067, train_loss: 1.3089
2022-03-07 02:37:30 - eval: epoch: 067, acc1: 72.014%, acc5: 90.480%, test_loss: 1.1274, per_image_load_time: 1.304ms, per_image_inference_time: 0.820ms
2022-03-07 02:37:31 - until epoch: 067, best_acc1: 72.014%
2022-03-07 02:37:31 - epoch 068 lr: 0.0010000000000000002
2022-03-07 02:37:55 - train: epoch 0068, iter [00100, 10009], lr: 0.001000, loss: 1.3970
2022-03-07 02:38:15 - train: epoch 0068, iter [00200, 10009], lr: 0.001000, loss: 1.3241
2022-03-07 02:38:35 - train: epoch 0068, iter [00300, 10009], lr: 0.001000, loss: 1.2302
2022-03-07 02:38:55 - train: epoch 0068, iter [00400, 10009], lr: 0.001000, loss: 1.4341
2022-03-07 02:39:15 - train: epoch 0068, iter [00500, 10009], lr: 0.001000, loss: 0.9710
2022-03-07 02:39:34 - train: epoch 0068, iter [00600, 10009], lr: 0.001000, loss: 1.5763
2022-03-07 02:39:54 - train: epoch 0068, iter [00700, 10009], lr: 0.001000, loss: 1.7032
2022-03-07 02:40:14 - train: epoch 0068, iter [00800, 10009], lr: 0.001000, loss: 1.3255
2022-03-07 02:40:34 - train: epoch 0068, iter [00900, 10009], lr: 0.001000, loss: 1.2520
2022-03-07 02:40:54 - train: epoch 0068, iter [01000, 10009], lr: 0.001000, loss: 1.1180
2022-03-07 02:41:14 - train: epoch 0068, iter [01100, 10009], lr: 0.001000, loss: 1.5295
2022-03-07 02:41:33 - train: epoch 0068, iter [01200, 10009], lr: 0.001000, loss: 1.3176
2022-03-07 02:41:53 - train: epoch 0068, iter [01300, 10009], lr: 0.001000, loss: 1.4138
2022-03-07 02:42:13 - train: epoch 0068, iter [01400, 10009], lr: 0.001000, loss: 1.3046
2022-03-07 02:42:33 - train: epoch 0068, iter [01500, 10009], lr: 0.001000, loss: 1.3154
2022-03-07 02:42:53 - train: epoch 0068, iter [01600, 10009], lr: 0.001000, loss: 1.0556
2022-03-07 02:43:12 - train: epoch 0068, iter [01700, 10009], lr: 0.001000, loss: 1.4087
2022-03-07 02:43:32 - train: epoch 0068, iter [01800, 10009], lr: 0.001000, loss: 1.3880
2022-03-07 02:43:52 - train: epoch 0068, iter [01900, 10009], lr: 0.001000, loss: 1.1022
2022-03-07 02:44:12 - train: epoch 0068, iter [02000, 10009], lr: 0.001000, loss: 1.2212
2022-03-07 02:44:32 - train: epoch 0068, iter [02100, 10009], lr: 0.001000, loss: 0.9695
2022-03-07 02:44:52 - train: epoch 0068, iter [02200, 10009], lr: 0.001000, loss: 1.5441
2022-03-07 02:45:12 - train: epoch 0068, iter [02300, 10009], lr: 0.001000, loss: 0.9565
2022-03-07 02:45:32 - train: epoch 0068, iter [02400, 10009], lr: 0.001000, loss: 1.1315
2022-03-07 02:45:52 - train: epoch 0068, iter [02500, 10009], lr: 0.001000, loss: 1.2471
2022-03-07 02:46:12 - train: epoch 0068, iter [02600, 10009], lr: 0.001000, loss: 1.2093
2022-03-07 02:46:32 - train: epoch 0068, iter [02700, 10009], lr: 0.001000, loss: 1.5010
2022-03-07 02:46:51 - train: epoch 0068, iter [02800, 10009], lr: 0.001000, loss: 1.1054
2022-03-07 02:47:11 - train: epoch 0068, iter [02900, 10009], lr: 0.001000, loss: 1.2647
2022-03-07 02:47:31 - train: epoch 0068, iter [03000, 10009], lr: 0.001000, loss: 1.7818
2022-03-07 02:47:51 - train: epoch 0068, iter [03100, 10009], lr: 0.001000, loss: 1.2098
2022-03-07 02:48:11 - train: epoch 0068, iter [03200, 10009], lr: 0.001000, loss: 1.3419
2022-03-07 02:48:31 - train: epoch 0068, iter [03300, 10009], lr: 0.001000, loss: 1.2948
2022-03-07 02:48:51 - train: epoch 0068, iter [03400, 10009], lr: 0.001000, loss: 1.0596
2022-03-07 02:49:11 - train: epoch 0068, iter [03500, 10009], lr: 0.001000, loss: 1.3355
2022-03-07 02:49:31 - train: epoch 0068, iter [03600, 10009], lr: 0.001000, loss: 1.2762
2022-03-07 02:49:51 - train: epoch 0068, iter [03700, 10009], lr: 0.001000, loss: 1.2150
2022-03-07 02:50:11 - train: epoch 0068, iter [03800, 10009], lr: 0.001000, loss: 1.3329
2022-03-07 02:50:31 - train: epoch 0068, iter [03900, 10009], lr: 0.001000, loss: 1.2053
2022-03-07 02:50:50 - train: epoch 0068, iter [04000, 10009], lr: 0.001000, loss: 1.5295
2022-03-07 02:51:11 - train: epoch 0068, iter [04100, 10009], lr: 0.001000, loss: 0.9769
2022-03-07 02:51:31 - train: epoch 0068, iter [04200, 10009], lr: 0.001000, loss: 1.3899
2022-03-07 02:51:50 - train: epoch 0068, iter [04300, 10009], lr: 0.001000, loss: 1.3751
2022-03-07 02:52:10 - train: epoch 0068, iter [04400, 10009], lr: 0.001000, loss: 1.4543
2022-03-07 02:52:30 - train: epoch 0068, iter [04500, 10009], lr: 0.001000, loss: 1.1934
2022-03-07 02:52:50 - train: epoch 0068, iter [04600, 10009], lr: 0.001000, loss: 1.3257
2022-03-07 02:53:10 - train: epoch 0068, iter [04700, 10009], lr: 0.001000, loss: 1.3541
2022-03-07 02:53:30 - train: epoch 0068, iter [04800, 10009], lr: 0.001000, loss: 1.3132
2022-03-07 02:53:50 - train: epoch 0068, iter [04900, 10009], lr: 0.001000, loss: 1.3505
2022-03-07 02:54:10 - train: epoch 0068, iter [05000, 10009], lr: 0.001000, loss: 1.1488
2022-03-07 02:54:30 - train: epoch 0068, iter [05100, 10009], lr: 0.001000, loss: 1.3919
2022-03-07 02:54:50 - train: epoch 0068, iter [05200, 10009], lr: 0.001000, loss: 1.2418
2022-03-07 02:55:11 - train: epoch 0068, iter [05300, 10009], lr: 0.001000, loss: 1.2428
2022-03-07 02:55:31 - train: epoch 0068, iter [05400, 10009], lr: 0.001000, loss: 1.0872
2022-03-07 02:55:51 - train: epoch 0068, iter [05500, 10009], lr: 0.001000, loss: 1.3529
2022-03-07 02:56:11 - train: epoch 0068, iter [05600, 10009], lr: 0.001000, loss: 1.2923
2022-03-07 02:56:31 - train: epoch 0068, iter [05700, 10009], lr: 0.001000, loss: 1.1866
2022-03-07 02:56:51 - train: epoch 0068, iter [05800, 10009], lr: 0.001000, loss: 1.4713
2022-03-07 02:57:11 - train: epoch 0068, iter [05900, 10009], lr: 0.001000, loss: 1.3193
2022-03-07 02:57:30 - train: epoch 0068, iter [06000, 10009], lr: 0.001000, loss: 1.4108
2022-03-07 02:57:50 - train: epoch 0068, iter [06100, 10009], lr: 0.001000, loss: 1.5847
2022-03-07 02:58:10 - train: epoch 0068, iter [06200, 10009], lr: 0.001000, loss: 1.1711
2022-03-07 02:58:30 - train: epoch 0068, iter [06300, 10009], lr: 0.001000, loss: 1.4701
2022-03-07 02:58:50 - train: epoch 0068, iter [06400, 10009], lr: 0.001000, loss: 1.3196
2022-03-07 02:59:10 - train: epoch 0068, iter [06500, 10009], lr: 0.001000, loss: 1.3096
2022-03-07 02:59:30 - train: epoch 0068, iter [06600, 10009], lr: 0.001000, loss: 1.2903
2022-03-07 02:59:50 - train: epoch 0068, iter [06700, 10009], lr: 0.001000, loss: 1.2208
2022-03-07 03:00:10 - train: epoch 0068, iter [06800, 10009], lr: 0.001000, loss: 1.3129
2022-03-07 03:00:30 - train: epoch 0068, iter [06900, 10009], lr: 0.001000, loss: 1.3136
2022-03-07 03:00:50 - train: epoch 0068, iter [07000, 10009], lr: 0.001000, loss: 1.2225
2022-03-07 03:01:10 - train: epoch 0068, iter [07100, 10009], lr: 0.001000, loss: 1.4248
2022-03-07 03:01:30 - train: epoch 0068, iter [07200, 10009], lr: 0.001000, loss: 1.3750
2022-03-07 03:01:50 - train: epoch 0068, iter [07300, 10009], lr: 0.001000, loss: 1.0077
2022-03-07 03:02:10 - train: epoch 0068, iter [07400, 10009], lr: 0.001000, loss: 1.1415
2022-03-07 03:02:30 - train: epoch 0068, iter [07500, 10009], lr: 0.001000, loss: 1.5583
2022-03-07 03:02:50 - train: epoch 0068, iter [07600, 10009], lr: 0.001000, loss: 1.4920
2022-03-07 03:03:10 - train: epoch 0068, iter [07700, 10009], lr: 0.001000, loss: 1.1882
2022-03-07 03:03:29 - train: epoch 0068, iter [07800, 10009], lr: 0.001000, loss: 1.3100
2022-03-07 03:03:49 - train: epoch 0068, iter [07900, 10009], lr: 0.001000, loss: 1.2340
2022-03-07 03:04:09 - train: epoch 0068, iter [08000, 10009], lr: 0.001000, loss: 1.4028
2022-03-07 03:04:29 - train: epoch 0068, iter [08100, 10009], lr: 0.001000, loss: 1.1362
2022-03-07 03:04:49 - train: epoch 0068, iter [08200, 10009], lr: 0.001000, loss: 1.2449
2022-03-07 03:05:09 - train: epoch 0068, iter [08300, 10009], lr: 0.001000, loss: 1.3005
2022-03-07 03:05:29 - train: epoch 0068, iter [08400, 10009], lr: 0.001000, loss: 1.4378
2022-03-07 03:05:49 - train: epoch 0068, iter [08500, 10009], lr: 0.001000, loss: 1.0214
2022-03-07 03:06:09 - train: epoch 0068, iter [08600, 10009], lr: 0.001000, loss: 1.2291
2022-03-07 03:06:29 - train: epoch 0068, iter [08700, 10009], lr: 0.001000, loss: 1.1816
2022-03-07 03:06:49 - train: epoch 0068, iter [08800, 10009], lr: 0.001000, loss: 1.3241
2022-03-07 03:07:09 - train: epoch 0068, iter [08900, 10009], lr: 0.001000, loss: 0.9883
2022-03-07 03:07:28 - train: epoch 0068, iter [09000, 10009], lr: 0.001000, loss: 1.2186
2022-03-07 03:07:48 - train: epoch 0068, iter [09100, 10009], lr: 0.001000, loss: 1.3624
2022-03-07 03:08:08 - train: epoch 0068, iter [09200, 10009], lr: 0.001000, loss: 1.3353
2022-03-07 03:08:28 - train: epoch 0068, iter [09300, 10009], lr: 0.001000, loss: 1.5450
2022-03-07 03:08:48 - train: epoch 0068, iter [09400, 10009], lr: 0.001000, loss: 1.4239
2022-03-07 03:09:08 - train: epoch 0068, iter [09500, 10009], lr: 0.001000, loss: 1.3636
2022-03-07 03:09:28 - train: epoch 0068, iter [09600, 10009], lr: 0.001000, loss: 1.2201
2022-03-07 03:09:48 - train: epoch 0068, iter [09700, 10009], lr: 0.001000, loss: 1.2912
2022-03-07 03:10:08 - train: epoch 0068, iter [09800, 10009], lr: 0.001000, loss: 1.2371
2022-03-07 03:10:28 - train: epoch 0068, iter [09900, 10009], lr: 0.001000, loss: 1.3830
2022-03-07 03:10:48 - train: epoch 0068, iter [10000, 10009], lr: 0.001000, loss: 1.4672
2022-03-07 03:10:50 - train: epoch 068, train_loss: 1.3016
2022-03-07 03:12:04 - eval: epoch: 068, acc1: 72.116%, acc5: 90.534%, test_loss: 1.1235, per_image_load_time: 0.864ms, per_image_inference_time: 0.807ms
2022-03-07 03:12:05 - until epoch: 068, best_acc1: 72.116%
2022-03-07 03:12:05 - epoch 069 lr: 0.0010000000000000002
2022-03-07 03:12:29 - train: epoch 0069, iter [00100, 10009], lr: 0.001000, loss: 1.4977
2022-03-07 03:12:49 - train: epoch 0069, iter [00200, 10009], lr: 0.001000, loss: 1.3828
2022-03-07 03:13:09 - train: epoch 0069, iter [00300, 10009], lr: 0.001000, loss: 1.1037
2022-03-07 03:13:29 - train: epoch 0069, iter [00400, 10009], lr: 0.001000, loss: 1.2758
2022-03-07 03:13:49 - train: epoch 0069, iter [00500, 10009], lr: 0.001000, loss: 1.2469
2022-03-07 03:14:09 - train: epoch 0069, iter [00600, 10009], lr: 0.001000, loss: 1.5809
2022-03-07 03:14:29 - train: epoch 0069, iter [00700, 10009], lr: 0.001000, loss: 1.2533
2022-03-07 03:14:49 - train: epoch 0069, iter [00800, 10009], lr: 0.001000, loss: 1.5042
2022-03-07 03:15:09 - train: epoch 0069, iter [00900, 10009], lr: 0.001000, loss: 1.2629
2022-03-07 03:15:29 - train: epoch 0069, iter [01000, 10009], lr: 0.001000, loss: 1.6668
2022-03-07 03:15:49 - train: epoch 0069, iter [01100, 10009], lr: 0.001000, loss: 1.2609
2022-03-07 03:16:09 - train: epoch 0069, iter [01200, 10009], lr: 0.001000, loss: 1.1231
2022-03-07 03:16:29 - train: epoch 0069, iter [01300, 10009], lr: 0.001000, loss: 1.3471
2022-03-07 03:16:49 - train: epoch 0069, iter [01400, 10009], lr: 0.001000, loss: 1.5743
2022-03-07 03:17:09 - train: epoch 0069, iter [01500, 10009], lr: 0.001000, loss: 1.3179
2022-03-07 03:17:29 - train: epoch 0069, iter [01600, 10009], lr: 0.001000, loss: 1.1522
2022-03-07 03:17:49 - train: epoch 0069, iter [01700, 10009], lr: 0.001000, loss: 1.3397
2022-03-07 03:18:10 - train: epoch 0069, iter [01800, 10009], lr: 0.001000, loss: 0.9879
2022-03-07 03:18:30 - train: epoch 0069, iter [01900, 10009], lr: 0.001000, loss: 1.4418
2022-03-07 03:18:50 - train: epoch 0069, iter [02000, 10009], lr: 0.001000, loss: 1.3107
2022-03-07 03:19:10 - train: epoch 0069, iter [02100, 10009], lr: 0.001000, loss: 1.6369
2022-03-07 03:19:30 - train: epoch 0069, iter [02200, 10009], lr: 0.001000, loss: 1.3615
2022-03-07 03:19:50 - train: epoch 0069, iter [02300, 10009], lr: 0.001000, loss: 1.2968
2022-03-07 03:20:11 - train: epoch 0069, iter [02400, 10009], lr: 0.001000, loss: 1.2201
2022-03-07 03:20:31 - train: epoch 0069, iter [02500, 10009], lr: 0.001000, loss: 1.2171
2022-03-07 03:20:51 - train: epoch 0069, iter [02600, 10009], lr: 0.001000, loss: 1.4469
2022-03-07 03:21:11 - train: epoch 0069, iter [02700, 10009], lr: 0.001000, loss: 1.7428
2022-03-07 03:21:31 - train: epoch 0069, iter [02800, 10009], lr: 0.001000, loss: 1.1109
2022-03-07 03:21:51 - train: epoch 0069, iter [02900, 10009], lr: 0.001000, loss: 1.1127
2022-03-07 03:22:11 - train: epoch 0069, iter [03000, 10009], lr: 0.001000, loss: 1.2615
2022-03-07 03:22:32 - train: epoch 0069, iter [03100, 10009], lr: 0.001000, loss: 1.1730
2022-03-07 03:22:52 - train: epoch 0069, iter [03200, 10009], lr: 0.001000, loss: 1.3131
2022-03-07 03:23:12 - train: epoch 0069, iter [03300, 10009], lr: 0.001000, loss: 1.1495
2022-03-07 03:23:32 - train: epoch 0069, iter [03400, 10009], lr: 0.001000, loss: 1.3106
2022-03-07 03:23:52 - train: epoch 0069, iter [03500, 10009], lr: 0.001000, loss: 1.3331
2022-03-07 03:24:13 - train: epoch 0069, iter [03600, 10009], lr: 0.001000, loss: 1.1226
2022-03-07 03:24:33 - train: epoch 0069, iter [03700, 10009], lr: 0.001000, loss: 1.1998
2022-03-07 03:24:53 - train: epoch 0069, iter [03800, 10009], lr: 0.001000, loss: 1.2925
2022-03-07 03:25:14 - train: epoch 0069, iter [03900, 10009], lr: 0.001000, loss: 1.0648
2022-03-07 03:25:34 - train: epoch 0069, iter [04000, 10009], lr: 0.001000, loss: 1.4356
2022-03-07 03:25:54 - train: epoch 0069, iter [04100, 10009], lr: 0.001000, loss: 1.3620
2022-03-07 03:26:14 - train: epoch 0069, iter [04200, 10009], lr: 0.001000, loss: 1.3214
2022-03-07 03:26:34 - train: epoch 0069, iter [04300, 10009], lr: 0.001000, loss: 1.4960
2022-03-07 03:26:54 - train: epoch 0069, iter [04400, 10009], lr: 0.001000, loss: 1.2648
2022-03-07 03:27:15 - train: epoch 0069, iter [04500, 10009], lr: 0.001000, loss: 1.2917
2022-03-07 03:27:35 - train: epoch 0069, iter [04600, 10009], lr: 0.001000, loss: 1.1954
2022-03-07 03:27:55 - train: epoch 0069, iter [04700, 10009], lr: 0.001000, loss: 1.3314
2022-03-07 03:28:15 - train: epoch 0069, iter [04800, 10009], lr: 0.001000, loss: 1.2876
2022-03-07 03:28:36 - train: epoch 0069, iter [04900, 10009], lr: 0.001000, loss: 1.1665
2022-03-07 03:28:56 - train: epoch 0069, iter [05000, 10009], lr: 0.001000, loss: 1.3730
2022-03-07 03:29:16 - train: epoch 0069, iter [05100, 10009], lr: 0.001000, loss: 1.2930
2022-03-07 03:29:36 - train: epoch 0069, iter [05200, 10009], lr: 0.001000, loss: 1.0619
2022-03-07 03:29:57 - train: epoch 0069, iter [05300, 10009], lr: 0.001000, loss: 1.0982
2022-03-07 03:30:17 - train: epoch 0069, iter [05400, 10009], lr: 0.001000, loss: 1.4578
2022-03-07 03:30:37 - train: epoch 0069, iter [05500, 10009], lr: 0.001000, loss: 1.1692
2022-03-07 03:30:57 - train: epoch 0069, iter [05600, 10009], lr: 0.001000, loss: 1.6001
2022-03-07 03:31:17 - train: epoch 0069, iter [05700, 10009], lr: 0.001000, loss: 1.2938
2022-03-07 03:31:38 - train: epoch 0069, iter [05800, 10009], lr: 0.001000, loss: 1.4346
2022-03-07 03:31:58 - train: epoch 0069, iter [05900, 10009], lr: 0.001000, loss: 1.3277
2022-03-07 03:32:18 - train: epoch 0069, iter [06000, 10009], lr: 0.001000, loss: 1.4642
2022-03-07 03:32:39 - train: epoch 0069, iter [06100, 10009], lr: 0.001000, loss: 1.1854
2022-03-07 03:32:59 - train: epoch 0069, iter [06200, 10009], lr: 0.001000, loss: 1.5185
2022-03-07 03:33:19 - train: epoch 0069, iter [06300, 10009], lr: 0.001000, loss: 1.3239
2022-03-07 03:33:39 - train: epoch 0069, iter [06400, 10009], lr: 0.001000, loss: 1.2713
2022-03-07 03:33:59 - train: epoch 0069, iter [06500, 10009], lr: 0.001000, loss: 1.2429
2022-03-07 03:34:19 - train: epoch 0069, iter [06600, 10009], lr: 0.001000, loss: 1.0724
2022-03-07 03:34:40 - train: epoch 0069, iter [06700, 10009], lr: 0.001000, loss: 1.1771
2022-03-07 03:35:00 - train: epoch 0069, iter [06800, 10009], lr: 0.001000, loss: 1.3312
2022-03-07 03:35:20 - train: epoch 0069, iter [06900, 10009], lr: 0.001000, loss: 1.2771
2022-03-07 03:35:40 - train: epoch 0069, iter [07000, 10009], lr: 0.001000, loss: 1.1108
2022-03-07 03:36:01 - train: epoch 0069, iter [07100, 10009], lr: 0.001000, loss: 1.2644
2022-03-07 03:36:21 - train: epoch 0069, iter [07200, 10009], lr: 0.001000, loss: 1.1471
2022-03-07 03:36:41 - train: epoch 0069, iter [07300, 10009], lr: 0.001000, loss: 1.7780
2022-03-07 03:37:01 - train: epoch 0069, iter [07400, 10009], lr: 0.001000, loss: 1.3834
2022-03-07 03:37:22 - train: epoch 0069, iter [07500, 10009], lr: 0.001000, loss: 1.5971
2022-03-07 03:37:42 - train: epoch 0069, iter [07600, 10009], lr: 0.001000, loss: 1.1343
2022-03-07 03:38:03 - train: epoch 0069, iter [07700, 10009], lr: 0.001000, loss: 1.2782
2022-03-07 03:38:23 - train: epoch 0069, iter [07800, 10009], lr: 0.001000, loss: 1.2529
2022-03-07 03:38:43 - train: epoch 0069, iter [07900, 10009], lr: 0.001000, loss: 1.1871
2022-03-07 03:39:04 - train: epoch 0069, iter [08000, 10009], lr: 0.001000, loss: 1.2665
2022-03-07 03:39:24 - train: epoch 0069, iter [08100, 10009], lr: 0.001000, loss: 1.3612
2022-03-07 03:39:44 - train: epoch 0069, iter [08200, 10009], lr: 0.001000, loss: 1.3318
2022-03-07 03:40:04 - train: epoch 0069, iter [08300, 10009], lr: 0.001000, loss: 1.1902
2022-03-07 03:40:25 - train: epoch 0069, iter [08400, 10009], lr: 0.001000, loss: 1.5832
2022-03-07 03:40:45 - train: epoch 0069, iter [08500, 10009], lr: 0.001000, loss: 0.9719
2022-03-07 03:41:05 - train: epoch 0069, iter [08600, 10009], lr: 0.001000, loss: 1.1228
2022-03-07 03:41:26 - train: epoch 0069, iter [08700, 10009], lr: 0.001000, loss: 1.2953
2022-03-07 03:41:46 - train: epoch 0069, iter [08800, 10009], lr: 0.001000, loss: 1.6963
2022-03-07 03:42:06 - train: epoch 0069, iter [08900, 10009], lr: 0.001000, loss: 1.3051
2022-03-07 03:42:26 - train: epoch 0069, iter [09000, 10009], lr: 0.001000, loss: 1.2795
2022-03-07 03:42:46 - train: epoch 0069, iter [09100, 10009], lr: 0.001000, loss: 1.2778
2022-03-07 03:43:06 - train: epoch 0069, iter [09200, 10009], lr: 0.001000, loss: 1.1644
2022-03-07 03:43:26 - train: epoch 0069, iter [09300, 10009], lr: 0.001000, loss: 1.1824
2022-03-07 03:43:46 - train: epoch 0069, iter [09400, 10009], lr: 0.001000, loss: 1.3288
2022-03-07 03:44:05 - train: epoch 0069, iter [09500, 10009], lr: 0.001000, loss: 1.0835
2022-03-07 03:44:25 - train: epoch 0069, iter [09600, 10009], lr: 0.001000, loss: 1.2995
2022-03-07 03:44:46 - train: epoch 0069, iter [09700, 10009], lr: 0.001000, loss: 1.3577
2022-03-07 03:45:06 - train: epoch 0069, iter [09800, 10009], lr: 0.001000, loss: 1.1307
2022-03-07 03:45:26 - train: epoch 0069, iter [09900, 10009], lr: 0.001000, loss: 1.3233
2022-03-07 03:45:46 - train: epoch 0069, iter [10000, 10009], lr: 0.001000, loss: 1.4045
2022-03-07 03:45:48 - train: epoch 069, train_loss: 1.2951
2022-03-07 03:47:02 - eval: epoch: 069, acc1: 72.156%, acc5: 90.466%, test_loss: 1.1225, per_image_load_time: 1.287ms, per_image_inference_time: 0.847ms
2022-03-07 03:47:03 - until epoch: 069, best_acc1: 72.156%
2022-03-07 03:47:03 - epoch 070 lr: 0.0010000000000000002
2022-03-07 03:47:27 - train: epoch 0070, iter [00100, 10009], lr: 0.001000, loss: 0.9113
2022-03-07 03:47:47 - train: epoch 0070, iter [00200, 10009], lr: 0.001000, loss: 1.4958
2022-03-07 03:48:07 - train: epoch 0070, iter [00300, 10009], lr: 0.001000, loss: 1.4173
2022-03-07 03:48:26 - train: epoch 0070, iter [00400, 10009], lr: 0.001000, loss: 1.2880
2022-03-07 03:48:46 - train: epoch 0070, iter [00500, 10009], lr: 0.001000, loss: 1.7485
2022-03-07 03:49:06 - train: epoch 0070, iter [00600, 10009], lr: 0.001000, loss: 1.2852
2022-03-07 03:49:26 - train: epoch 0070, iter [00700, 10009], lr: 0.001000, loss: 1.1647
2022-03-07 03:49:46 - train: epoch 0070, iter [00800, 10009], lr: 0.001000, loss: 1.1662
2022-03-07 03:50:06 - train: epoch 0070, iter [00900, 10009], lr: 0.001000, loss: 1.3268
2022-03-07 03:50:26 - train: epoch 0070, iter [01000, 10009], lr: 0.001000, loss: 1.3025
2022-03-07 03:50:46 - train: epoch 0070, iter [01100, 10009], lr: 0.001000, loss: 1.3374
2022-03-07 03:51:06 - train: epoch 0070, iter [01200, 10009], lr: 0.001000, loss: 1.3686
2022-03-07 03:51:26 - train: epoch 0070, iter [01300, 10009], lr: 0.001000, loss: 1.3478
2022-03-07 03:51:46 - train: epoch 0070, iter [01400, 10009], lr: 0.001000, loss: 1.2438
2022-03-07 03:52:06 - train: epoch 0070, iter [01500, 10009], lr: 0.001000, loss: 1.2597
2022-03-07 03:52:26 - train: epoch 0070, iter [01600, 10009], lr: 0.001000, loss: 1.1545
2022-03-07 03:52:46 - train: epoch 0070, iter [01700, 10009], lr: 0.001000, loss: 1.1897
2022-03-07 03:53:06 - train: epoch 0070, iter [01800, 10009], lr: 0.001000, loss: 1.5668
2022-03-07 03:53:25 - train: epoch 0070, iter [01900, 10009], lr: 0.001000, loss: 1.1426
2022-03-07 03:53:45 - train: epoch 0070, iter [02000, 10009], lr: 0.001000, loss: 1.2341
2022-03-07 03:54:05 - train: epoch 0070, iter [02100, 10009], lr: 0.001000, loss: 1.1453
2022-03-07 03:54:25 - train: epoch 0070, iter [02200, 10009], lr: 0.001000, loss: 1.4947
2022-03-07 03:54:45 - train: epoch 0070, iter [02300, 10009], lr: 0.001000, loss: 1.4113
2022-03-07 03:55:05 - train: epoch 0070, iter [02400, 10009], lr: 0.001000, loss: 1.3528
2022-03-07 03:55:25 - train: epoch 0070, iter [02500, 10009], lr: 0.001000, loss: 1.2845
2022-03-07 03:55:45 - train: epoch 0070, iter [02600, 10009], lr: 0.001000, loss: 1.0231
2022-03-07 03:56:05 - train: epoch 0070, iter [02700, 10009], lr: 0.001000, loss: 1.1459
2022-03-07 03:56:25 - train: epoch 0070, iter [02800, 10009], lr: 0.001000, loss: 1.0990
2022-03-07 03:56:45 - train: epoch 0070, iter [02900, 10009], lr: 0.001000, loss: 1.2262
2022-03-07 03:57:05 - train: epoch 0070, iter [03000, 10009], lr: 0.001000, loss: 1.1995
2022-03-07 03:57:25 - train: epoch 0070, iter [03100, 10009], lr: 0.001000, loss: 1.3489
2022-03-07 03:57:45 - train: epoch 0070, iter [03200, 10009], lr: 0.001000, loss: 1.3482
2022-03-07 03:58:05 - train: epoch 0070, iter [03300, 10009], lr: 0.001000, loss: 1.1944
2022-03-07 03:58:25 - train: epoch 0070, iter [03400, 10009], lr: 0.001000, loss: 1.0563
2022-03-07 03:58:45 - train: epoch 0070, iter [03500, 10009], lr: 0.001000, loss: 1.2647
2022-03-07 03:59:04 - train: epoch 0070, iter [03600, 10009], lr: 0.001000, loss: 1.0026
2022-03-07 03:59:24 - train: epoch 0070, iter [03700, 10009], lr: 0.001000, loss: 1.2618
2022-03-07 03:59:44 - train: epoch 0070, iter [03800, 10009], lr: 0.001000, loss: 1.2414
2022-03-07 04:00:04 - train: epoch 0070, iter [03900, 10009], lr: 0.001000, loss: 1.4284
2022-03-07 04:00:24 - train: epoch 0070, iter [04000, 10009], lr: 0.001000, loss: 1.2040
2022-03-07 04:00:44 - train: epoch 0070, iter [04100, 10009], lr: 0.001000, loss: 1.1784
2022-03-07 04:01:04 - train: epoch 0070, iter [04200, 10009], lr: 0.001000, loss: 1.1519
2022-03-07 04:01:24 - train: epoch 0070, iter [04300, 10009], lr: 0.001000, loss: 1.5607
2022-03-07 04:01:44 - train: epoch 0070, iter [04400, 10009], lr: 0.001000, loss: 1.5360
2022-03-07 04:02:03 - train: epoch 0070, iter [04500, 10009], lr: 0.001000, loss: 1.2210
2022-03-07 04:02:23 - train: epoch 0070, iter [04600, 10009], lr: 0.001000, loss: 1.4883
2022-03-07 04:02:43 - train: epoch 0070, iter [04700, 10009], lr: 0.001000, loss: 1.3462
2022-03-07 04:03:03 - train: epoch 0070, iter [04800, 10009], lr: 0.001000, loss: 1.5612
2022-03-07 04:03:23 - train: epoch 0070, iter [04900, 10009], lr: 0.001000, loss: 1.3229
2022-03-07 04:03:43 - train: epoch 0070, iter [05000, 10009], lr: 0.001000, loss: 1.3869
2022-03-07 04:04:03 - train: epoch 0070, iter [05100, 10009], lr: 0.001000, loss: 1.1241
2022-03-07 04:04:23 - train: epoch 0070, iter [05200, 10009], lr: 0.001000, loss: 1.3740
2022-03-07 04:04:43 - train: epoch 0070, iter [05300, 10009], lr: 0.001000, loss: 1.2942
2022-03-07 04:05:03 - train: epoch 0070, iter [05400, 10009], lr: 0.001000, loss: 1.1959
2022-03-07 04:05:23 - train: epoch 0070, iter [05500, 10009], lr: 0.001000, loss: 1.3956
2022-03-07 04:05:43 - train: epoch 0070, iter [05600, 10009], lr: 0.001000, loss: 1.5969
2022-03-07 04:06:02 - train: epoch 0070, iter [05700, 10009], lr: 0.001000, loss: 1.4611
2022-03-07 04:06:22 - train: epoch 0070, iter [05800, 10009], lr: 0.001000, loss: 1.1638
2022-03-07 04:06:42 - train: epoch 0070, iter [05900, 10009], lr: 0.001000, loss: 1.3574
2022-03-07 04:07:02 - train: epoch 0070, iter [06000, 10009], lr: 0.001000, loss: 1.2672
2022-03-07 04:07:22 - train: epoch 0070, iter [06100, 10009], lr: 0.001000, loss: 1.3505
2022-03-07 04:07:42 - train: epoch 0070, iter [06200, 10009], lr: 0.001000, loss: 1.3429
2022-03-07 04:08:02 - train: epoch 0070, iter [06300, 10009], lr: 0.001000, loss: 1.1311
2022-03-07 04:08:22 - train: epoch 0070, iter [06400, 10009], lr: 0.001000, loss: 1.5364
2022-03-07 04:08:41 - train: epoch 0070, iter [06500, 10009], lr: 0.001000, loss: 1.4177
2022-03-07 04:09:01 - train: epoch 0070, iter [06600, 10009], lr: 0.001000, loss: 1.2516
2022-03-07 04:09:21 - train: epoch 0070, iter [06700, 10009], lr: 0.001000, loss: 1.0965
2022-03-07 04:09:41 - train: epoch 0070, iter [06800, 10009], lr: 0.001000, loss: 1.3668
2022-03-07 04:10:01 - train: epoch 0070, iter [06900, 10009], lr: 0.001000, loss: 1.6524
2022-03-07 04:10:21 - train: epoch 0070, iter [07000, 10009], lr: 0.001000, loss: 1.1894
2022-03-07 04:10:41 - train: epoch 0070, iter [07100, 10009], lr: 0.001000, loss: 1.3820
2022-03-07 04:11:01 - train: epoch 0070, iter [07200, 10009], lr: 0.001000, loss: 1.3574
2022-03-07 04:11:21 - train: epoch 0070, iter [07300, 10009], lr: 0.001000, loss: 1.0164
2022-03-07 04:11:41 - train: epoch 0070, iter [07400, 10009], lr: 0.001000, loss: 0.9368
2022-03-07 04:12:01 - train: epoch 0070, iter [07500, 10009], lr: 0.001000, loss: 1.5529
2022-03-07 04:12:21 - train: epoch 0070, iter [07600, 10009], lr: 0.001000, loss: 1.4908
2022-03-07 04:12:41 - train: epoch 0070, iter [07700, 10009], lr: 0.001000, loss: 1.1535
2022-03-07 04:13:01 - train: epoch 0070, iter [07800, 10009], lr: 0.001000, loss: 1.0986
2022-03-07 04:13:21 - train: epoch 0070, iter [07900, 10009], lr: 0.001000, loss: 1.3702
2022-03-07 04:13:40 - train: epoch 0070, iter [08000, 10009], lr: 0.001000, loss: 1.4246
2022-03-07 04:14:00 - train: epoch 0070, iter [08100, 10009], lr: 0.001000, loss: 1.3423
2022-03-07 04:14:20 - train: epoch 0070, iter [08200, 10009], lr: 0.001000, loss: 1.3901
2022-03-07 04:14:40 - train: epoch 0070, iter [08300, 10009], lr: 0.001000, loss: 1.5185
2022-03-07 04:15:00 - train: epoch 0070, iter [08400, 10009], lr: 0.001000, loss: 1.1333
2022-03-07 04:15:20 - train: epoch 0070, iter [08500, 10009], lr: 0.001000, loss: 1.3512
2022-03-07 04:15:40 - train: epoch 0070, iter [08600, 10009], lr: 0.001000, loss: 1.2183
2022-03-07 04:16:00 - train: epoch 0070, iter [08700, 10009], lr: 0.001000, loss: 1.0763
2022-03-07 04:16:20 - train: epoch 0070, iter [08800, 10009], lr: 0.001000, loss: 1.3061
2022-03-07 04:16:39 - train: epoch 0070, iter [08900, 10009], lr: 0.001000, loss: 1.6374
2022-03-07 04:17:00 - train: epoch 0070, iter [09000, 10009], lr: 0.001000, loss: 1.3865
2022-03-07 04:17:20 - train: epoch 0070, iter [09100, 10009], lr: 0.001000, loss: 1.3444
2022-03-07 04:17:40 - train: epoch 0070, iter [09200, 10009], lr: 0.001000, loss: 1.5195
2022-03-07 04:18:00 - train: epoch 0070, iter [09300, 10009], lr: 0.001000, loss: 1.2802
2022-03-07 04:18:20 - train: epoch 0070, iter [09400, 10009], lr: 0.001000, loss: 1.3605
2022-03-07 04:18:40 - train: epoch 0070, iter [09500, 10009], lr: 0.001000, loss: 1.3840
2022-03-07 04:19:00 - train: epoch 0070, iter [09600, 10009], lr: 0.001000, loss: 1.4118
2022-03-07 04:19:20 - train: epoch 0070, iter [09700, 10009], lr: 0.001000, loss: 1.1765
2022-03-07 04:19:40 - train: epoch 0070, iter [09800, 10009], lr: 0.001000, loss: 1.4503
2022-03-07 04:20:00 - train: epoch 0070, iter [09900, 10009], lr: 0.001000, loss: 1.2943
2022-03-07 04:20:20 - train: epoch 0070, iter [10000, 10009], lr: 0.001000, loss: 1.2308
2022-03-07 04:20:23 - train: epoch 070, train_loss: 1.2888
2022-03-07 04:21:36 - eval: epoch: 070, acc1: 72.366%, acc5: 90.570%, test_loss: 1.1162, per_image_load_time: 0.758ms, per_image_inference_time: 0.871ms
2022-03-07 04:21:37 - until epoch: 070, best_acc1: 72.366%
2022-03-07 04:21:37 - epoch 071 lr: 0.0010000000000000002
2022-03-07 04:22:00 - train: epoch 0071, iter [00100, 10009], lr: 0.001000, loss: 1.0000
2022-03-07 04:22:20 - train: epoch 0071, iter [00200, 10009], lr: 0.001000, loss: 1.0815
2022-03-07 04:22:40 - train: epoch 0071, iter [00300, 10009], lr: 0.001000, loss: 1.2202
2022-03-07 04:23:00 - train: epoch 0071, iter [00400, 10009], lr: 0.001000, loss: 1.2177
2022-03-07 04:23:19 - train: epoch 0071, iter [00500, 10009], lr: 0.001000, loss: 1.2950
2022-03-07 04:23:39 - train: epoch 0071, iter [00600, 10009], lr: 0.001000, loss: 1.3138
2022-03-07 04:23:59 - train: epoch 0071, iter [00700, 10009], lr: 0.001000, loss: 1.4096
2022-03-07 04:24:19 - train: epoch 0071, iter [00800, 10009], lr: 0.001000, loss: 1.1505
2022-03-07 04:24:39 - train: epoch 0071, iter [00900, 10009], lr: 0.001000, loss: 1.3390
2022-03-07 04:24:58 - train: epoch 0071, iter [01000, 10009], lr: 0.001000, loss: 1.2810
2022-03-07 04:25:18 - train: epoch 0071, iter [01100, 10009], lr: 0.001000, loss: 1.3423
2022-03-07 04:25:38 - train: epoch 0071, iter [01200, 10009], lr: 0.001000, loss: 1.3835
2022-03-07 04:25:58 - train: epoch 0071, iter [01300, 10009], lr: 0.001000, loss: 1.3479
2022-03-07 04:26:18 - train: epoch 0071, iter [01400, 10009], lr: 0.001000, loss: 1.2193
2022-03-07 04:26:38 - train: epoch 0071, iter [01500, 10009], lr: 0.001000, loss: 1.3297
2022-03-07 04:26:58 - train: epoch 0071, iter [01600, 10009], lr: 0.001000, loss: 1.4951
2022-03-07 04:27:17 - train: epoch 0071, iter [01700, 10009], lr: 0.001000, loss: 1.1348
2022-03-07 04:27:37 - train: epoch 0071, iter [01800, 10009], lr: 0.001000, loss: 1.5119
2022-03-07 04:27:57 - train: epoch 0071, iter [01900, 10009], lr: 0.001000, loss: 1.0730
2022-03-07 04:28:17 - train: epoch 0071, iter [02000, 10009], lr: 0.001000, loss: 1.0696
2022-03-07 04:28:37 - train: epoch 0071, iter [02100, 10009], lr: 0.001000, loss: 1.3363
2022-03-07 04:28:57 - train: epoch 0071, iter [02200, 10009], lr: 0.001000, loss: 1.2693
2022-03-07 04:29:16 - train: epoch 0071, iter [02300, 10009], lr: 0.001000, loss: 1.4071
2022-03-07 04:29:36 - train: epoch 0071, iter [02400, 10009], lr: 0.001000, loss: 1.1912
2022-03-07 04:29:56 - train: epoch 0071, iter [02500, 10009], lr: 0.001000, loss: 1.1186
2022-03-07 04:30:16 - train: epoch 0071, iter [02600, 10009], lr: 0.001000, loss: 1.0292
2022-03-07 04:30:36 - train: epoch 0071, iter [02700, 10009], lr: 0.001000, loss: 1.3831
2022-03-07 04:30:56 - train: epoch 0071, iter [02800, 10009], lr: 0.001000, loss: 1.2571
2022-03-07 04:31:16 - train: epoch 0071, iter [02900, 10009], lr: 0.001000, loss: 1.4307
2022-03-07 04:31:36 - train: epoch 0071, iter [03000, 10009], lr: 0.001000, loss: 1.1468
2022-03-07 04:31:56 - train: epoch 0071, iter [03100, 10009], lr: 0.001000, loss: 1.2124
2022-03-07 04:32:16 - train: epoch 0071, iter [03200, 10009], lr: 0.001000, loss: 1.5961
2022-03-07 04:32:36 - train: epoch 0071, iter [03300, 10009], lr: 0.001000, loss: 1.1244
2022-03-07 04:32:56 - train: epoch 0071, iter [03400, 10009], lr: 0.001000, loss: 1.1697
2022-03-07 04:33:16 - train: epoch 0071, iter [03500, 10009], lr: 0.001000, loss: 1.2464
2022-03-07 04:33:36 - train: epoch 0071, iter [03600, 10009], lr: 0.001000, loss: 1.4021
2022-03-07 04:33:55 - train: epoch 0071, iter [03700, 10009], lr: 0.001000, loss: 1.8392
2022-03-07 04:34:15 - train: epoch 0071, iter [03800, 10009], lr: 0.001000, loss: 1.0082
2022-03-07 04:34:35 - train: epoch 0071, iter [03900, 10009], lr: 0.001000, loss: 1.4540
2022-03-07 04:34:55 - train: epoch 0071, iter [04000, 10009], lr: 0.001000, loss: 1.3207
2022-03-07 04:35:15 - train: epoch 0071, iter [04100, 10009], lr: 0.001000, loss: 1.1894
2022-03-07 04:35:35 - train: epoch 0071, iter [04200, 10009], lr: 0.001000, loss: 1.3925
2022-03-07 04:35:55 - train: epoch 0071, iter [04300, 10009], lr: 0.001000, loss: 1.1084
2022-03-07 04:36:15 - train: epoch 0071, iter [04400, 10009], lr: 0.001000, loss: 1.1720
2022-03-07 04:36:35 - train: epoch 0071, iter [04500, 10009], lr: 0.001000, loss: 1.1603
2022-03-07 04:36:54 - train: epoch 0071, iter [04600, 10009], lr: 0.001000, loss: 0.9184
2022-03-07 04:37:14 - train: epoch 0071, iter [04700, 10009], lr: 0.001000, loss: 1.3459
2022-03-07 04:37:34 - train: epoch 0071, iter [04800, 10009], lr: 0.001000, loss: 1.3059
2022-03-07 04:37:54 - train: epoch 0071, iter [04900, 10009], lr: 0.001000, loss: 1.3155
2022-03-07 04:38:14 - train: epoch 0071, iter [05000, 10009], lr: 0.001000, loss: 1.3783
2022-03-07 04:38:34 - train: epoch 0071, iter [05100, 10009], lr: 0.001000, loss: 1.3926
2022-03-07 04:38:54 - train: epoch 0071, iter [05200, 10009], lr: 0.001000, loss: 1.3403
2022-03-07 04:39:14 - train: epoch 0071, iter [05300, 10009], lr: 0.001000, loss: 1.1964
2022-03-07 04:39:34 - train: epoch 0071, iter [05400, 10009], lr: 0.001000, loss: 1.3974
2022-03-07 04:39:53 - train: epoch 0071, iter [05500, 10009], lr: 0.001000, loss: 1.0078
2022-03-07 04:40:13 - train: epoch 0071, iter [05600, 10009], lr: 0.001000, loss: 1.3700
2022-03-07 04:40:33 - train: epoch 0071, iter [05700, 10009], lr: 0.001000, loss: 1.2572
2022-03-07 04:40:53 - train: epoch 0071, iter [05800, 10009], lr: 0.001000, loss: 1.2660
2022-03-07 04:41:13 - train: epoch 0071, iter [05900, 10009], lr: 0.001000, loss: 1.4630
2022-03-07 04:41:33 - train: epoch 0071, iter [06000, 10009], lr: 0.001000, loss: 1.3637
2022-03-07 04:41:53 - train: epoch 0071, iter [06100, 10009], lr: 0.001000, loss: 1.3623
2022-03-07 04:42:13 - train: epoch 0071, iter [06200, 10009], lr: 0.001000, loss: 1.3851
2022-03-07 04:42:33 - train: epoch 0071, iter [06300, 10009], lr: 0.001000, loss: 1.2738
2022-03-07 04:42:53 - train: epoch 0071, iter [06400, 10009], lr: 0.001000, loss: 1.3548
2022-03-07 04:43:13 - train: epoch 0071, iter [06500, 10009], lr: 0.001000, loss: 1.5068
2022-03-07 04:43:33 - train: epoch 0071, iter [06600, 10009], lr: 0.001000, loss: 1.3465
2022-03-07 04:43:53 - train: epoch 0071, iter [06700, 10009], lr: 0.001000, loss: 1.3222
2022-03-07 04:44:13 - train: epoch 0071, iter [06800, 10009], lr: 0.001000, loss: 1.2534
2022-03-07 04:44:33 - train: epoch 0071, iter [06900, 10009], lr: 0.001000, loss: 1.5267
2022-03-07 04:44:52 - train: epoch 0071, iter [07000, 10009], lr: 0.001000, loss: 1.3379
2022-03-07 04:45:12 - train: epoch 0071, iter [07100, 10009], lr: 0.001000, loss: 1.2967
2022-03-07 04:45:32 - train: epoch 0071, iter [07200, 10009], lr: 0.001000, loss: 1.5393
2022-03-07 04:45:52 - train: epoch 0071, iter [07300, 10009], lr: 0.001000, loss: 1.6446
2022-03-07 04:46:12 - train: epoch 0071, iter [07400, 10009], lr: 0.001000, loss: 1.1916
2022-03-07 04:46:32 - train: epoch 0071, iter [07500, 10009], lr: 0.001000, loss: 1.0627
2022-03-07 04:46:52 - train: epoch 0071, iter [07600, 10009], lr: 0.001000, loss: 1.4015
2022-03-07 04:47:12 - train: epoch 0071, iter [07700, 10009], lr: 0.001000, loss: 1.2303
2022-03-07 04:47:32 - train: epoch 0071, iter [07800, 10009], lr: 0.001000, loss: 1.1661
2022-03-07 04:47:52 - train: epoch 0071, iter [07900, 10009], lr: 0.001000, loss: 1.1626
2022-03-07 04:48:12 - train: epoch 0071, iter [08000, 10009], lr: 0.001000, loss: 1.2424
2022-03-07 04:48:32 - train: epoch 0071, iter [08100, 10009], lr: 0.001000, loss: 1.2215
2022-03-07 04:48:52 - train: epoch 0071, iter [08200, 10009], lr: 0.001000, loss: 0.9833
2022-03-07 04:49:12 - train: epoch 0071, iter [08300, 10009], lr: 0.001000, loss: 0.9121
2022-03-07 04:49:32 - train: epoch 0071, iter [08400, 10009], lr: 0.001000, loss: 1.5989
2022-03-07 04:49:52 - train: epoch 0071, iter [08500, 10009], lr: 0.001000, loss: 1.0550
2022-03-07 04:50:12 - train: epoch 0071, iter [08600, 10009], lr: 0.001000, loss: 1.3243
2022-03-07 04:50:32 - train: epoch 0071, iter [08700, 10009], lr: 0.001000, loss: 1.1776
2022-03-07 04:50:52 - train: epoch 0071, iter [08800, 10009], lr: 0.001000, loss: 1.3600
2022-03-07 04:51:12 - train: epoch 0071, iter [08900, 10009], lr: 0.001000, loss: 1.2768
2022-03-07 04:51:32 - train: epoch 0071, iter [09000, 10009], lr: 0.001000, loss: 1.1896
2022-03-07 04:51:52 - train: epoch 0071, iter [09100, 10009], lr: 0.001000, loss: 1.3640
2022-03-07 04:52:12 - train: epoch 0071, iter [09200, 10009], lr: 0.001000, loss: 1.3630
2022-03-07 04:52:32 - train: epoch 0071, iter [09300, 10009], lr: 0.001000, loss: 1.3341
2022-03-07 04:52:52 - train: epoch 0071, iter [09400, 10009], lr: 0.001000, loss: 1.3167
2022-03-07 04:53:12 - train: epoch 0071, iter [09500, 10009], lr: 0.001000, loss: 1.4420
2022-03-07 04:53:32 - train: epoch 0071, iter [09600, 10009], lr: 0.001000, loss: 1.2544
2022-03-07 04:53:52 - train: epoch 0071, iter [09700, 10009], lr: 0.001000, loss: 1.4821
2022-03-07 04:54:12 - train: epoch 0071, iter [09800, 10009], lr: 0.001000, loss: 1.1068
2022-03-07 04:54:32 - train: epoch 0071, iter [09900, 10009], lr: 0.001000, loss: 1.1621
2022-03-07 04:54:52 - train: epoch 0071, iter [10000, 10009], lr: 0.001000, loss: 1.2820
2022-03-07 04:54:55 - train: epoch 071, train_loss: 1.2830
2022-03-07 04:56:09 - eval: epoch: 071, acc1: 72.320%, acc5: 90.546%, test_loss: 1.1158, per_image_load_time: 1.401ms, per_image_inference_time: 0.873ms
2022-03-07 04:56:10 - until epoch: 071, best_acc1: 72.366%
2022-03-07 04:56:10 - epoch 072 lr: 0.0010000000000000002
2022-03-07 04:56:33 - train: epoch 0072, iter [00100, 10009], lr: 0.001000, loss: 1.0172
2022-03-07 04:56:53 - train: epoch 0072, iter [00200, 10009], lr: 0.001000, loss: 1.4162
2022-03-07 04:57:13 - train: epoch 0072, iter [00300, 10009], lr: 0.001000, loss: 1.1098
2022-03-07 04:57:33 - train: epoch 0072, iter [00400, 10009], lr: 0.001000, loss: 1.1253
2022-03-07 04:57:53 - train: epoch 0072, iter [00500, 10009], lr: 0.001000, loss: 1.2564
2022-03-07 04:58:13 - train: epoch 0072, iter [00600, 10009], lr: 0.001000, loss: 1.2139
2022-03-07 04:58:33 - train: epoch 0072, iter [00700, 10009], lr: 0.001000, loss: 0.9912
2022-03-07 04:58:53 - train: epoch 0072, iter [00800, 10009], lr: 0.001000, loss: 1.2471
2022-03-07 04:59:13 - train: epoch 0072, iter [00900, 10009], lr: 0.001000, loss: 1.1684
2022-03-07 04:59:32 - train: epoch 0072, iter [01000, 10009], lr: 0.001000, loss: 1.0756
2022-03-07 04:59:52 - train: epoch 0072, iter [01100, 10009], lr: 0.001000, loss: 1.3481
2022-03-07 05:00:12 - train: epoch 0072, iter [01200, 10009], lr: 0.001000, loss: 1.2691
2022-03-07 05:00:32 - train: epoch 0072, iter [01300, 10009], lr: 0.001000, loss: 1.2992
2022-03-07 05:00:52 - train: epoch 0072, iter [01400, 10009], lr: 0.001000, loss: 1.2453
2022-03-07 05:01:12 - train: epoch 0072, iter [01500, 10009], lr: 0.001000, loss: 1.1594
2022-03-07 05:01:31 - train: epoch 0072, iter [01600, 10009], lr: 0.001000, loss: 1.7684
2022-03-07 05:01:51 - train: epoch 0072, iter [01700, 10009], lr: 0.001000, loss: 1.3895
2022-03-07 05:02:11 - train: epoch 0072, iter [01800, 10009], lr: 0.001000, loss: 1.1365
2022-03-07 05:02:31 - train: epoch 0072, iter [01900, 10009], lr: 0.001000, loss: 0.9758
2022-03-07 05:02:51 - train: epoch 0072, iter [02000, 10009], lr: 0.001000, loss: 1.1876
2022-03-07 05:03:10 - train: epoch 0072, iter [02100, 10009], lr: 0.001000, loss: 1.0878
2022-03-07 05:03:30 - train: epoch 0072, iter [02200, 10009], lr: 0.001000, loss: 1.5912
2022-03-07 05:03:50 - train: epoch 0072, iter [02300, 10009], lr: 0.001000, loss: 1.0771
2022-03-07 05:04:10 - train: epoch 0072, iter [02400, 10009], lr: 0.001000, loss: 1.3943
2022-03-07 05:04:30 - train: epoch 0072, iter [02500, 10009], lr: 0.001000, loss: 0.9426
2022-03-07 05:04:50 - train: epoch 0072, iter [02600, 10009], lr: 0.001000, loss: 1.3977
2022-03-07 05:05:10 - train: epoch 0072, iter [02700, 10009], lr: 0.001000, loss: 1.1377
2022-03-07 05:05:29 - train: epoch 0072, iter [02800, 10009], lr: 0.001000, loss: 1.2734
2022-03-07 05:05:49 - train: epoch 0072, iter [02900, 10009], lr: 0.001000, loss: 1.3423
2022-03-07 05:06:09 - train: epoch 0072, iter [03000, 10009], lr: 0.001000, loss: 1.2149
2022-03-07 05:06:29 - train: epoch 0072, iter [03100, 10009], lr: 0.001000, loss: 1.2732
2022-03-07 05:06:49 - train: epoch 0072, iter [03200, 10009], lr: 0.001000, loss: 1.4125
2022-03-07 05:07:09 - train: epoch 0072, iter [03300, 10009], lr: 0.001000, loss: 1.5142
2022-03-07 05:07:29 - train: epoch 0072, iter [03400, 10009], lr: 0.001000, loss: 1.1921
2022-03-07 05:07:49 - train: epoch 0072, iter [03500, 10009], lr: 0.001000, loss: 1.2674
2022-03-07 05:08:09 - train: epoch 0072, iter [03600, 10009], lr: 0.001000, loss: 1.2625
2022-03-07 05:08:29 - train: epoch 0072, iter [03700, 10009], lr: 0.001000, loss: 1.3877
2022-03-07 05:08:49 - train: epoch 0072, iter [03800, 10009], lr: 0.001000, loss: 1.2220
2022-03-07 05:09:09 - train: epoch 0072, iter [03900, 10009], lr: 0.001000, loss: 1.1577
2022-03-07 05:09:28 - train: epoch 0072, iter [04000, 10009], lr: 0.001000, loss: 1.2604
2022-03-07 05:09:48 - train: epoch 0072, iter [04100, 10009], lr: 0.001000, loss: 1.1277
2022-03-07 05:10:08 - train: epoch 0072, iter [04200, 10009], lr: 0.001000, loss: 1.3623
2022-03-07 05:10:28 - train: epoch 0072, iter [04300, 10009], lr: 0.001000, loss: 1.3715
2022-03-07 05:10:48 - train: epoch 0072, iter [04400, 10009], lr: 0.001000, loss: 1.2798
2022-03-07 05:11:08 - train: epoch 0072, iter [04500, 10009], lr: 0.001000, loss: 1.1512
2022-03-07 05:11:28 - train: epoch 0072, iter [04600, 10009], lr: 0.001000, loss: 1.5439
2022-03-07 05:11:48 - train: epoch 0072, iter [04700, 10009], lr: 0.001000, loss: 1.2648
2022-03-07 05:12:08 - train: epoch 0072, iter [04800, 10009], lr: 0.001000, loss: 1.2426
2022-03-07 05:12:28 - train: epoch 0072, iter [04900, 10009], lr: 0.001000, loss: 0.9909
2022-03-07 05:12:48 - train: epoch 0072, iter [05000, 10009], lr: 0.001000, loss: 1.2846
2022-03-07 05:13:08 - train: epoch 0072, iter [05100, 10009], lr: 0.001000, loss: 1.4318
2022-03-07 05:13:28 - train: epoch 0072, iter [05200, 10009], lr: 0.001000, loss: 1.4022
2022-03-07 05:13:48 - train: epoch 0072, iter [05300, 10009], lr: 0.001000, loss: 1.2052
2022-03-07 05:14:07 - train: epoch 0072, iter [05400, 10009], lr: 0.001000, loss: 1.1372
2022-03-07 05:14:27 - train: epoch 0072, iter [05500, 10009], lr: 0.001000, loss: 1.1577
2022-03-07 05:14:47 - train: epoch 0072, iter [05600, 10009], lr: 0.001000, loss: 1.3053
2022-03-07 05:15:07 - train: epoch 0072, iter [05700, 10009], lr: 0.001000, loss: 1.4978
2022-03-07 05:15:27 - train: epoch 0072, iter [05800, 10009], lr: 0.001000, loss: 1.2495
2022-03-07 05:15:48 - train: epoch 0072, iter [05900, 10009], lr: 0.001000, loss: 1.6307
2022-03-07 05:16:07 - train: epoch 0072, iter [06000, 10009], lr: 0.001000, loss: 1.2831
2022-03-07 05:16:27 - train: epoch 0072, iter [06100, 10009], lr: 0.001000, loss: 1.1411
2022-03-07 05:16:48 - train: epoch 0072, iter [06200, 10009], lr: 0.001000, loss: 1.1991
2022-03-07 05:17:08 - train: epoch 0072, iter [06300, 10009], lr: 0.001000, loss: 1.4137
2022-03-07 05:17:27 - train: epoch 0072, iter [06400, 10009], lr: 0.001000, loss: 1.4474
2022-03-07 05:17:48 - train: epoch 0072, iter [06500, 10009], lr: 0.001000, loss: 1.1478
2022-03-07 05:18:08 - train: epoch 0072, iter [06600, 10009], lr: 0.001000, loss: 1.1432
2022-03-07 05:18:28 - train: epoch 0072, iter [06700, 10009], lr: 0.001000, loss: 1.3988
2022-03-07 05:18:48 - train: epoch 0072, iter [06800, 10009], lr: 0.001000, loss: 1.5156
2022-03-07 05:19:08 - train: epoch 0072, iter [06900, 10009], lr: 0.001000, loss: 1.3165
2022-03-07 05:19:28 - train: epoch 0072, iter [07000, 10009], lr: 0.001000, loss: 1.3597
2022-03-07 05:19:48 - train: epoch 0072, iter [07100, 10009], lr: 0.001000, loss: 1.2478
2022-03-07 05:20:08 - train: epoch 0072, iter [07200, 10009], lr: 0.001000, loss: 1.1808
2022-03-07 05:20:28 - train: epoch 0072, iter [07300, 10009], lr: 0.001000, loss: 1.3104
2022-03-07 05:20:48 - train: epoch 0072, iter [07400, 10009], lr: 0.001000, loss: 1.2513
2022-03-07 05:21:08 - train: epoch 0072, iter [07500, 10009], lr: 0.001000, loss: 1.2894
2022-03-07 05:21:28 - train: epoch 0072, iter [07600, 10009], lr: 0.001000, loss: 1.2704
2022-03-07 05:21:48 - train: epoch 0072, iter [07700, 10009], lr: 0.001000, loss: 1.2846
2022-03-07 05:22:08 - train: epoch 0072, iter [07800, 10009], lr: 0.001000, loss: 1.4506
2022-03-07 05:22:28 - train: epoch 0072, iter [07900, 10009], lr: 0.001000, loss: 0.9105
2022-03-07 05:22:48 - train: epoch 0072, iter [08000, 10009], lr: 0.001000, loss: 1.3075
2022-03-07 05:23:08 - train: epoch 0072, iter [08100, 10009], lr: 0.001000, loss: 1.4148
2022-03-07 05:23:28 - train: epoch 0072, iter [08200, 10009], lr: 0.001000, loss: 1.6714
2022-03-07 05:23:48 - train: epoch 0072, iter [08300, 10009], lr: 0.001000, loss: 1.4968
2022-03-07 05:24:08 - train: epoch 0072, iter [08400, 10009], lr: 0.001000, loss: 1.1022
2022-03-07 05:24:29 - train: epoch 0072, iter [08500, 10009], lr: 0.001000, loss: 1.3056
2022-03-07 05:24:49 - train: epoch 0072, iter [08600, 10009], lr: 0.001000, loss: 1.1458
2022-03-07 05:25:09 - train: epoch 0072, iter [08700, 10009], lr: 0.001000, loss: 1.0712
2022-03-07 05:25:29 - train: epoch 0072, iter [08800, 10009], lr: 0.001000, loss: 1.2385
2022-03-07 05:25:49 - train: epoch 0072, iter [08900, 10009], lr: 0.001000, loss: 1.3292
2022-03-07 05:26:09 - train: epoch 0072, iter [09000, 10009], lr: 0.001000, loss: 1.2481
2022-03-07 05:26:29 - train: epoch 0072, iter [09100, 10009], lr: 0.001000, loss: 1.3387
2022-03-07 05:26:49 - train: epoch 0072, iter [09200, 10009], lr: 0.001000, loss: 1.1053
2022-03-07 05:27:09 - train: epoch 0072, iter [09300, 10009], lr: 0.001000, loss: 1.3146
2022-03-07 05:27:29 - train: epoch 0072, iter [09400, 10009], lr: 0.001000, loss: 1.2367
2022-03-07 05:27:49 - train: epoch 0072, iter [09500, 10009], lr: 0.001000, loss: 1.2104
2022-03-07 05:28:09 - train: epoch 0072, iter [09600, 10009], lr: 0.001000, loss: 1.4458
2022-03-07 05:28:29 - train: epoch 0072, iter [09700, 10009], lr: 0.001000, loss: 1.2057
2022-03-07 05:28:49 - train: epoch 0072, iter [09800, 10009], lr: 0.001000, loss: 1.1499
2022-03-07 05:29:10 - train: epoch 0072, iter [09900, 10009], lr: 0.001000, loss: 1.3088
2022-03-07 05:29:30 - train: epoch 0072, iter [10000, 10009], lr: 0.001000, loss: 1.2350
2022-03-07 05:29:32 - train: epoch 072, train_loss: 1.2817
2022-03-07 05:30:46 - eval: epoch: 072, acc1: 72.264%, acc5: 90.580%, test_loss: 1.1126, per_image_load_time: 1.910ms, per_image_inference_time: 0.832ms
2022-03-07 05:30:47 - until epoch: 072, best_acc1: 72.366%
2022-03-07 05:30:47 - epoch 073 lr: 0.0010000000000000002
2022-03-07 05:31:11 - train: epoch 0073, iter [00100, 10009], lr: 0.001000, loss: 1.1574
2022-03-07 05:31:31 - train: epoch 0073, iter [00200, 10009], lr: 0.001000, loss: 1.4314
2022-03-07 05:31:50 - train: epoch 0073, iter [00300, 10009], lr: 0.001000, loss: 1.1210
2022-03-07 05:32:10 - train: epoch 0073, iter [00400, 10009], lr: 0.001000, loss: 1.2762
2022-03-07 05:32:30 - train: epoch 0073, iter [00500, 10009], lr: 0.001000, loss: 1.1192
2022-03-07 05:32:50 - train: epoch 0073, iter [00600, 10009], lr: 0.001000, loss: 1.3264
2022-03-07 05:33:10 - train: epoch 0073, iter [00700, 10009], lr: 0.001000, loss: 1.1509
2022-03-07 05:33:30 - train: epoch 0073, iter [00800, 10009], lr: 0.001000, loss: 1.0249
2022-03-07 05:33:50 - train: epoch 0073, iter [00900, 10009], lr: 0.001000, loss: 1.3317
2022-03-07 05:34:09 - train: epoch 0073, iter [01000, 10009], lr: 0.001000, loss: 1.1941
2022-03-07 05:34:29 - train: epoch 0073, iter [01100, 10009], lr: 0.001000, loss: 1.5154
2022-03-07 05:34:49 - train: epoch 0073, iter [01200, 10009], lr: 0.001000, loss: 1.1533
2022-03-07 05:35:09 - train: epoch 0073, iter [01300, 10009], lr: 0.001000, loss: 1.4665
2022-03-07 05:35:29 - train: epoch 0073, iter [01400, 10009], lr: 0.001000, loss: 1.3697
2022-03-07 05:35:49 - train: epoch 0073, iter [01500, 10009], lr: 0.001000, loss: 0.8941
2022-03-07 05:36:09 - train: epoch 0073, iter [01600, 10009], lr: 0.001000, loss: 1.1876
2022-03-07 05:36:28 - train: epoch 0073, iter [01700, 10009], lr: 0.001000, loss: 1.0958
2022-03-07 05:36:48 - train: epoch 0073, iter [01800, 10009], lr: 0.001000, loss: 1.3207
2022-03-07 05:37:08 - train: epoch 0073, iter [01900, 10009], lr: 0.001000, loss: 1.4371
2022-03-07 05:37:28 - train: epoch 0073, iter [02000, 10009], lr: 0.001000, loss: 1.1826
2022-03-07 05:37:48 - train: epoch 0073, iter [02100, 10009], lr: 0.001000, loss: 1.4718
2022-03-07 05:38:08 - train: epoch 0073, iter [02200, 10009], lr: 0.001000, loss: 1.4647
2022-03-07 05:38:28 - train: epoch 0073, iter [02300, 10009], lr: 0.001000, loss: 1.0124
2022-03-07 05:38:48 - train: epoch 0073, iter [02400, 10009], lr: 0.001000, loss: 1.2537
2022-03-07 05:39:08 - train: epoch 0073, iter [02500, 10009], lr: 0.001000, loss: 1.0232
2022-03-07 05:39:27 - train: epoch 0073, iter [02600, 10009], lr: 0.001000, loss: 1.4782
2022-03-07 05:39:47 - train: epoch 0073, iter [02700, 10009], lr: 0.001000, loss: 1.6835
2022-03-07 05:40:07 - train: epoch 0073, iter [02800, 10009], lr: 0.001000, loss: 1.3032
2022-03-07 05:40:27 - train: epoch 0073, iter [02900, 10009], lr: 0.001000, loss: 1.0553
2022-03-07 05:40:47 - train: epoch 0073, iter [03000, 10009], lr: 0.001000, loss: 1.2657
2022-03-07 05:41:07 - train: epoch 0073, iter [03100, 10009], lr: 0.001000, loss: 1.3587
2022-03-07 05:41:27 - train: epoch 0073, iter [03200, 10009], lr: 0.001000, loss: 1.2870
2022-03-07 05:41:46 - train: epoch 0073, iter [03300, 10009], lr: 0.001000, loss: 1.1612
2022-03-07 05:42:06 - train: epoch 0073, iter [03400, 10009], lr: 0.001000, loss: 1.4309
2022-03-07 05:42:26 - train: epoch 0073, iter [03500, 10009], lr: 0.001000, loss: 1.0823
2022-03-07 05:42:46 - train: epoch 0073, iter [03600, 10009], lr: 0.001000, loss: 1.1038
2022-03-07 05:43:06 - train: epoch 0073, iter [03700, 10009], lr: 0.001000, loss: 1.2490
2022-03-07 05:43:26 - train: epoch 0073, iter [03800, 10009], lr: 0.001000, loss: 1.2152
2022-03-07 05:43:46 - train: epoch 0073, iter [03900, 10009], lr: 0.001000, loss: 1.3934
2022-03-07 05:44:06 - train: epoch 0073, iter [04000, 10009], lr: 0.001000, loss: 1.1801
2022-03-07 05:44:26 - train: epoch 0073, iter [04100, 10009], lr: 0.001000, loss: 1.1342
2022-03-07 05:44:46 - train: epoch 0073, iter [04200, 10009], lr: 0.001000, loss: 1.1122
2022-03-07 05:45:06 - train: epoch 0073, iter [04300, 10009], lr: 0.001000, loss: 1.3525
2022-03-07 05:45:26 - train: epoch 0073, iter [04400, 10009], lr: 0.001000, loss: 1.1882
2022-03-07 05:45:46 - train: epoch 0073, iter [04500, 10009], lr: 0.001000, loss: 1.7225
2022-03-07 05:46:06 - train: epoch 0073, iter [04600, 10009], lr: 0.001000, loss: 1.2417
2022-03-07 05:46:26 - train: epoch 0073, iter [04700, 10009], lr: 0.001000, loss: 1.4947
2022-03-07 05:46:46 - train: epoch 0073, iter [04800, 10009], lr: 0.001000, loss: 1.4399
2022-03-07 05:47:06 - train: epoch 0073, iter [04900, 10009], lr: 0.001000, loss: 1.0647
2022-03-07 05:47:26 - train: epoch 0073, iter [05000, 10009], lr: 0.001000, loss: 1.3571
2022-03-07 05:47:46 - train: epoch 0073, iter [05100, 10009], lr: 0.001000, loss: 1.1996
2022-03-07 05:48:06 - train: epoch 0073, iter [05200, 10009], lr: 0.001000, loss: 1.1268
2022-03-07 05:48:26 - train: epoch 0073, iter [05300, 10009], lr: 0.001000, loss: 1.2739
2022-03-07 05:48:47 - train: epoch 0073, iter [05400, 10009], lr: 0.001000, loss: 1.2366
2022-03-07 05:49:07 - train: epoch 0073, iter [05500, 10009], lr: 0.001000, loss: 1.3586
2022-03-07 05:49:27 - train: epoch 0073, iter [05600, 10009], lr: 0.001000, loss: 1.1470
2022-03-07 05:49:47 - train: epoch 0073, iter [05700, 10009], lr: 0.001000, loss: 1.5432
2022-03-07 05:50:08 - train: epoch 0073, iter [05800, 10009], lr: 0.001000, loss: 1.2273
2022-03-07 05:50:28 - train: epoch 0073, iter [05900, 10009], lr: 0.001000, loss: 1.2220
2022-03-07 05:50:49 - train: epoch 0073, iter [06000, 10009], lr: 0.001000, loss: 1.1493
2022-03-07 05:51:09 - train: epoch 0073, iter [06100, 10009], lr: 0.001000, loss: 1.1790
2022-03-07 05:51:29 - train: epoch 0073, iter [06200, 10009], lr: 0.001000, loss: 1.3497
2022-03-07 05:51:50 - train: epoch 0073, iter [06300, 10009], lr: 0.001000, loss: 1.3590
2022-03-07 05:52:10 - train: epoch 0073, iter [06400, 10009], lr: 0.001000, loss: 1.0809
2022-03-07 05:52:30 - train: epoch 0073, iter [06500, 10009], lr: 0.001000, loss: 1.2717
2022-03-07 05:52:51 - train: epoch 0073, iter [06600, 10009], lr: 0.001000, loss: 1.1909
2022-03-07 05:53:11 - train: epoch 0073, iter [06700, 10009], lr: 0.001000, loss: 1.2867
2022-03-07 05:53:31 - train: epoch 0073, iter [06800, 10009], lr: 0.001000, loss: 1.0784
2022-03-07 05:53:51 - train: epoch 0073, iter [06900, 10009], lr: 0.001000, loss: 1.4082
2022-03-07 05:54:12 - train: epoch 0073, iter [07000, 10009], lr: 0.001000, loss: 1.4233
2022-03-07 05:54:32 - train: epoch 0073, iter [07100, 10009], lr: 0.001000, loss: 1.3617
2022-03-07 05:54:52 - train: epoch 0073, iter [07200, 10009], lr: 0.001000, loss: 1.1506
2022-03-07 05:55:12 - train: epoch 0073, iter [07300, 10009], lr: 0.001000, loss: 1.4329
2022-03-07 05:55:33 - train: epoch 0073, iter [07400, 10009], lr: 0.001000, loss: 1.1660
2022-03-07 05:55:53 - train: epoch 0073, iter [07500, 10009], lr: 0.001000, loss: 1.1486
2022-03-07 05:56:14 - train: epoch 0073, iter [07600, 10009], lr: 0.001000, loss: 1.3307
2022-03-07 05:56:34 - train: epoch 0073, iter [07700, 10009], lr: 0.001000, loss: 1.1681
2022-03-07 05:56:54 - train: epoch 0073, iter [07800, 10009], lr: 0.001000, loss: 1.4323
2022-03-07 05:57:14 - train: epoch 0073, iter [07900, 10009], lr: 0.001000, loss: 1.1009
2022-03-07 05:57:35 - train: epoch 0073, iter [08000, 10009], lr: 0.001000, loss: 1.4217
2022-03-07 05:57:55 - train: epoch 0073, iter [08100, 10009], lr: 0.001000, loss: 1.1759
2022-03-07 05:58:16 - train: epoch 0073, iter [08200, 10009], lr: 0.001000, loss: 1.3923
2022-03-07 05:58:36 - train: epoch 0073, iter [08300, 10009], lr: 0.001000, loss: 1.0669
2022-03-07 05:58:56 - train: epoch 0073, iter [08400, 10009], lr: 0.001000, loss: 1.1529
2022-03-07 05:59:17 - train: epoch 0073, iter [08500, 10009], lr: 0.001000, loss: 1.4667
2022-03-07 05:59:37 - train: epoch 0073, iter [08600, 10009], lr: 0.001000, loss: 1.4481
2022-03-07 05:59:58 - train: epoch 0073, iter [08700, 10009], lr: 0.001000, loss: 1.3026
2022-03-07 06:00:18 - train: epoch 0073, iter [08800, 10009], lr: 0.001000, loss: 1.4694
2022-03-07 06:00:38 - train: epoch 0073, iter [08900, 10009], lr: 0.001000, loss: 1.6593
2022-03-07 06:00:59 - train: epoch 0073, iter [09000, 10009], lr: 0.001000, loss: 1.0926
2022-03-07 06:01:19 - train: epoch 0073, iter [09100, 10009], lr: 0.001000, loss: 1.4326
2022-03-07 06:01:39 - train: epoch 0073, iter [09200, 10009], lr: 0.001000, loss: 1.2171
2022-03-07 06:02:00 - train: epoch 0073, iter [09300, 10009], lr: 0.001000, loss: 1.2635
2022-03-07 06:02:20 - train: epoch 0073, iter [09400, 10009], lr: 0.001000, loss: 1.0737
2022-03-07 06:02:40 - train: epoch 0073, iter [09500, 10009], lr: 0.001000, loss: 1.2837
2022-03-07 06:03:00 - train: epoch 0073, iter [09600, 10009], lr: 0.001000, loss: 1.2259
2022-03-07 06:03:21 - train: epoch 0073, iter [09700, 10009], lr: 0.001000, loss: 1.3049
2022-03-07 06:03:41 - train: epoch 0073, iter [09800, 10009], lr: 0.001000, loss: 1.0824
2022-03-07 06:04:01 - train: epoch 0073, iter [09900, 10009], lr: 0.001000, loss: 1.4031
2022-03-07 06:04:22 - train: epoch 0073, iter [10000, 10009], lr: 0.001000, loss: 1.4623
2022-03-07 06:04:25 - train: epoch 073, train_loss: 1.2744
2022-03-07 06:05:39 - eval: epoch: 073, acc1: 72.288%, acc5: 90.566%, test_loss: 1.1141, per_image_load_time: 1.935ms, per_image_inference_time: 0.845ms
2022-03-07 06:05:39 - until epoch: 073, best_acc1: 72.366%
2022-03-07 06:05:39 - epoch 074 lr: 0.0010000000000000002
2022-03-07 06:06:03 - train: epoch 0074, iter [00100, 10009], lr: 0.001000, loss: 1.4423
2022-03-07 06:06:23 - train: epoch 0074, iter [00200, 10009], lr: 0.001000, loss: 1.0575
2022-03-07 06:06:43 - train: epoch 0074, iter [00300, 10009], lr: 0.001000, loss: 1.1455
2022-03-07 06:07:03 - train: epoch 0074, iter [00400, 10009], lr: 0.001000, loss: 1.3746
2022-03-07 06:07:23 - train: epoch 0074, iter [00500, 10009], lr: 0.001000, loss: 1.2424
2022-03-07 06:07:43 - train: epoch 0074, iter [00600, 10009], lr: 0.001000, loss: 1.1376
2022-03-07 06:08:03 - train: epoch 0074, iter [00700, 10009], lr: 0.001000, loss: 1.0863
2022-03-07 06:08:23 - train: epoch 0074, iter [00800, 10009], lr: 0.001000, loss: 1.4791
2022-03-07 06:08:42 - train: epoch 0074, iter [00900, 10009], lr: 0.001000, loss: 1.0614
2022-03-07 06:09:02 - train: epoch 0074, iter [01000, 10009], lr: 0.001000, loss: 1.2623
2022-03-07 06:09:22 - train: epoch 0074, iter [01100, 10009], lr: 0.001000, loss: 1.3152
2022-03-07 06:09:42 - train: epoch 0074, iter [01200, 10009], lr: 0.001000, loss: 1.1689
2022-03-07 06:10:02 - train: epoch 0074, iter [01300, 10009], lr: 0.001000, loss: 1.1475
2022-03-07 06:10:22 - train: epoch 0074, iter [01400, 10009], lr: 0.001000, loss: 1.2783
2022-03-07 06:10:42 - train: epoch 0074, iter [01500, 10009], lr: 0.001000, loss: 1.3772
2022-03-07 06:11:02 - train: epoch 0074, iter [01600, 10009], lr: 0.001000, loss: 1.1737
2022-03-07 06:11:22 - train: epoch 0074, iter [01700, 10009], lr: 0.001000, loss: 0.9938
2022-03-07 06:11:42 - train: epoch 0074, iter [01800, 10009], lr: 0.001000, loss: 1.1648
2022-03-07 06:12:03 - train: epoch 0074, iter [01900, 10009], lr: 0.001000, loss: 1.4075
2022-03-07 06:12:22 - train: epoch 0074, iter [02000, 10009], lr: 0.001000, loss: 1.2393
2022-03-07 06:12:42 - train: epoch 0074, iter [02100, 10009], lr: 0.001000, loss: 1.4365
2022-03-07 06:13:03 - train: epoch 0074, iter [02200, 10009], lr: 0.001000, loss: 1.2432
2022-03-07 06:13:23 - train: epoch 0074, iter [02300, 10009], lr: 0.001000, loss: 1.4028
2022-03-07 06:13:42 - train: epoch 0074, iter [02400, 10009], lr: 0.001000, loss: 1.4434
2022-03-07 06:14:02 - train: epoch 0074, iter [02500, 10009], lr: 0.001000, loss: 1.1870
2022-03-07 06:14:22 - train: epoch 0074, iter [02600, 10009], lr: 0.001000, loss: 1.1458
2022-03-07 06:14:42 - train: epoch 0074, iter [02700, 10009], lr: 0.001000, loss: 0.9991
2022-03-07 06:15:03 - train: epoch 0074, iter [02800, 10009], lr: 0.001000, loss: 1.0561
2022-03-07 06:15:23 - train: epoch 0074, iter [02900, 10009], lr: 0.001000, loss: 1.1113
2022-03-07 06:15:42 - train: epoch 0074, iter [03000, 10009], lr: 0.001000, loss: 1.4680
2022-03-07 06:16:02 - train: epoch 0074, iter [03100, 10009], lr: 0.001000, loss: 1.5049
2022-03-07 06:16:22 - train: epoch 0074, iter [03200, 10009], lr: 0.001000, loss: 1.3814
2022-03-07 06:16:42 - train: epoch 0074, iter [03300, 10009], lr: 0.001000, loss: 1.2024
2022-03-07 06:17:03 - train: epoch 0074, iter [03400, 10009], lr: 0.001000, loss: 1.3824
2022-03-07 06:17:22 - train: epoch 0074, iter [03500, 10009], lr: 0.001000, loss: 1.2885
2022-03-07 06:17:43 - train: epoch 0074, iter [03600, 10009], lr: 0.001000, loss: 1.4541
2022-03-07 06:18:03 - train: epoch 0074, iter [03700, 10009], lr: 0.001000, loss: 1.5550
2022-03-07 06:18:23 - train: epoch 0074, iter [03800, 10009], lr: 0.001000, loss: 1.4592
2022-03-07 06:18:43 - train: epoch 0074, iter [03900, 10009], lr: 0.001000, loss: 1.0510
2022-03-07 06:19:03 - train: epoch 0074, iter [04000, 10009], lr: 0.001000, loss: 0.8472
2022-03-07 06:19:23 - train: epoch 0074, iter [04100, 10009], lr: 0.001000, loss: 1.1758
2022-03-07 06:19:43 - train: epoch 0074, iter [04200, 10009], lr: 0.001000, loss: 0.9071
2022-03-07 06:20:03 - train: epoch 0074, iter [04300, 10009], lr: 0.001000, loss: 1.1786
2022-03-07 06:20:23 - train: epoch 0074, iter [04400, 10009], lr: 0.001000, loss: 1.5863
2022-03-07 06:20:43 - train: epoch 0074, iter [04500, 10009], lr: 0.001000, loss: 1.3226
2022-03-07 06:21:03 - train: epoch 0074, iter [04600, 10009], lr: 0.001000, loss: 1.1561
2022-03-07 06:21:23 - train: epoch 0074, iter [04700, 10009], lr: 0.001000, loss: 1.2779
2022-03-07 06:21:43 - train: epoch 0074, iter [04800, 10009], lr: 0.001000, loss: 1.0917
2022-03-07 06:22:03 - train: epoch 0074, iter [04900, 10009], lr: 0.001000, loss: 1.3138
2022-03-07 06:22:23 - train: epoch 0074, iter [05000, 10009], lr: 0.001000, loss: 1.1262
2022-03-07 06:22:43 - train: epoch 0074, iter [05100, 10009], lr: 0.001000, loss: 1.3573
2022-03-07 06:23:03 - train: epoch 0074, iter [05200, 10009], lr: 0.001000, loss: 0.9873
2022-03-07 06:23:24 - train: epoch 0074, iter [05300, 10009], lr: 0.001000, loss: 1.1431
2022-03-07 06:23:44 - train: epoch 0074, iter [05400, 10009], lr: 0.001000, loss: 1.2886
2022-03-07 06:24:04 - train: epoch 0074, iter [05500, 10009], lr: 0.001000, loss: 1.3362
2022-03-07 06:24:24 - train: epoch 0074, iter [05600, 10009], lr: 0.001000, loss: 1.5685
2022-03-07 06:24:44 - train: epoch 0074, iter [05700, 10009], lr: 0.001000, loss: 1.3610
2022-03-07 06:25:04 - train: epoch 0074, iter [05800, 10009], lr: 0.001000, loss: 1.1225
2022-03-07 06:25:24 - train: epoch 0074, iter [05900, 10009], lr: 0.001000, loss: 1.3622
2022-03-07 06:25:44 - train: epoch 0074, iter [06000, 10009], lr: 0.001000, loss: 1.2294
2022-03-07 06:26:04 - train: epoch 0074, iter [06100, 10009], lr: 0.001000, loss: 0.9275
2022-03-07 06:26:24 - train: epoch 0074, iter [06200, 10009], lr: 0.001000, loss: 1.3403
2022-03-07 06:26:44 - train: epoch 0074, iter [06300, 10009], lr: 0.001000, loss: 1.2061
2022-03-07 06:27:04 - train: epoch 0074, iter [06400, 10009], lr: 0.001000, loss: 1.3034
2022-03-07 06:27:24 - train: epoch 0074, iter [06500, 10009], lr: 0.001000, loss: 1.2292
2022-03-07 06:27:44 - train: epoch 0074, iter [06600, 10009], lr: 0.001000, loss: 1.3653
2022-03-07 06:28:04 - train: epoch 0074, iter [06700, 10009], lr: 0.001000, loss: 1.2755
2022-03-07 06:28:24 - train: epoch 0074, iter [06800, 10009], lr: 0.001000, loss: 1.2021
2022-03-07 06:28:44 - train: epoch 0074, iter [06900, 10009], lr: 0.001000, loss: 1.3131
2022-03-07 06:29:04 - train: epoch 0074, iter [07000, 10009], lr: 0.001000, loss: 1.2190
2022-03-07 06:29:24 - train: epoch 0074, iter [07100, 10009], lr: 0.001000, loss: 1.3673
2022-03-07 06:29:44 - train: epoch 0074, iter [07200, 10009], lr: 0.001000, loss: 1.5542
2022-03-07 06:30:04 - train: epoch 0074, iter [07300, 10009], lr: 0.001000, loss: 1.1498
2022-03-07 06:30:24 - train: epoch 0074, iter [07400, 10009], lr: 0.001000, loss: 1.0450
2022-03-07 06:30:44 - train: epoch 0074, iter [07500, 10009], lr: 0.001000, loss: 1.3453
2022-03-07 06:31:04 - train: epoch 0074, iter [07600, 10009], lr: 0.001000, loss: 1.2448
2022-03-07 06:31:24 - train: epoch 0074, iter [07700, 10009], lr: 0.001000, loss: 1.3979
2022-03-07 06:31:44 - train: epoch 0074, iter [07800, 10009], lr: 0.001000, loss: 0.9890
2022-03-07 06:32:04 - train: epoch 0074, iter [07900, 10009], lr: 0.001000, loss: 1.2707
2022-03-07 06:32:24 - train: epoch 0074, iter [08000, 10009], lr: 0.001000, loss: 1.4676
2022-03-07 06:32:44 - train: epoch 0074, iter [08100, 10009], lr: 0.001000, loss: 1.0597
2022-03-07 06:33:04 - train: epoch 0074, iter [08200, 10009], lr: 0.001000, loss: 1.2302
2022-03-07 06:33:24 - train: epoch 0074, iter [08300, 10009], lr: 0.001000, loss: 1.2708
2022-03-07 06:33:44 - train: epoch 0074, iter [08400, 10009], lr: 0.001000, loss: 1.3361
2022-03-07 06:34:04 - train: epoch 0074, iter [08500, 10009], lr: 0.001000, loss: 1.4736
2022-03-07 06:34:24 - train: epoch 0074, iter [08600, 10009], lr: 0.001000, loss: 1.1237
2022-03-07 06:34:44 - train: epoch 0074, iter [08700, 10009], lr: 0.001000, loss: 1.2942
2022-03-07 06:35:04 - train: epoch 0074, iter [08800, 10009], lr: 0.001000, loss: 0.8169
2022-03-07 06:35:24 - train: epoch 0074, iter [08900, 10009], lr: 0.001000, loss: 1.2850
2022-03-07 06:35:44 - train: epoch 0074, iter [09000, 10009], lr: 0.001000, loss: 1.0709
2022-03-07 06:36:04 - train: epoch 0074, iter [09100, 10009], lr: 0.001000, loss: 0.9497
2022-03-07 06:36:24 - train: epoch 0074, iter [09200, 10009], lr: 0.001000, loss: 1.1785
2022-03-07 06:36:44 - train: epoch 0074, iter [09300, 10009], lr: 0.001000, loss: 1.3062
2022-03-07 06:37:04 - train: epoch 0074, iter [09400, 10009], lr: 0.001000, loss: 1.0827
2022-03-07 06:37:24 - train: epoch 0074, iter [09500, 10009], lr: 0.001000, loss: 1.0786
2022-03-07 06:37:44 - train: epoch 0074, iter [09600, 10009], lr: 0.001000, loss: 1.2312
2022-03-07 06:38:03 - train: epoch 0074, iter [09700, 10009], lr: 0.001000, loss: 1.2786
2022-03-07 06:38:23 - train: epoch 0074, iter [09800, 10009], lr: 0.001000, loss: 1.5974
2022-03-07 06:38:43 - train: epoch 0074, iter [09900, 10009], lr: 0.001000, loss: 1.3189
2022-03-07 06:39:03 - train: epoch 0074, iter [10000, 10009], lr: 0.001000, loss: 1.2454
2022-03-07 06:39:06 - train: epoch 074, train_loss: 1.2708
2022-03-07 06:40:20 - eval: epoch: 074, acc1: 72.400%, acc5: 90.672%, test_loss: 1.1100, per_image_load_time: 1.274ms, per_image_inference_time: 0.849ms
2022-03-07 06:40:21 - until epoch: 074, best_acc1: 72.400%
2022-03-07 06:40:21 - epoch 075 lr: 0.0010000000000000002
2022-03-07 06:40:45 - train: epoch 0075, iter [00100, 10009], lr: 0.001000, loss: 1.4780
2022-03-07 06:41:05 - train: epoch 0075, iter [00200, 10009], lr: 0.001000, loss: 1.3351
2022-03-07 06:41:25 - train: epoch 0075, iter [00300, 10009], lr: 0.001000, loss: 1.2688
2022-03-07 06:41:44 - train: epoch 0075, iter [00400, 10009], lr: 0.001000, loss: 1.1837
2022-03-07 06:42:04 - train: epoch 0075, iter [00500, 10009], lr: 0.001000, loss: 1.2031
2022-03-07 06:42:24 - train: epoch 0075, iter [00600, 10009], lr: 0.001000, loss: 1.3976
2022-03-07 06:42:44 - train: epoch 0075, iter [00700, 10009], lr: 0.001000, loss: 1.3254
2022-03-07 06:43:04 - train: epoch 0075, iter [00800, 10009], lr: 0.001000, loss: 1.0870
2022-03-07 06:43:24 - train: epoch 0075, iter [00900, 10009], lr: 0.001000, loss: 1.2561
2022-03-07 06:43:44 - train: epoch 0075, iter [01000, 10009], lr: 0.001000, loss: 1.3293
2022-03-07 06:44:04 - train: epoch 0075, iter [01100, 10009], lr: 0.001000, loss: 1.1914
2022-03-07 06:44:24 - train: epoch 0075, iter [01200, 10009], lr: 0.001000, loss: 1.2016
2022-03-07 06:44:44 - train: epoch 0075, iter [01300, 10009], lr: 0.001000, loss: 1.0716
2022-03-07 06:45:04 - train: epoch 0075, iter [01400, 10009], lr: 0.001000, loss: 1.2441
2022-03-07 06:45:23 - train: epoch 0075, iter [01500, 10009], lr: 0.001000, loss: 1.4198
2022-03-07 06:45:43 - train: epoch 0075, iter [01600, 10009], lr: 0.001000, loss: 1.5350
2022-03-07 06:46:03 - train: epoch 0075, iter [01700, 10009], lr: 0.001000, loss: 1.1554
2022-03-07 06:46:23 - train: epoch 0075, iter [01800, 10009], lr: 0.001000, loss: 1.5119
2022-03-07 06:46:43 - train: epoch 0075, iter [01900, 10009], lr: 0.001000, loss: 1.3708
2022-03-07 06:47:03 - train: epoch 0075, iter [02000, 10009], lr: 0.001000, loss: 1.2704
2022-03-07 06:47:23 - train: epoch 0075, iter [02100, 10009], lr: 0.001000, loss: 1.3178
2022-03-07 06:47:43 - train: epoch 0075, iter [02200, 10009], lr: 0.001000, loss: 1.4582
2022-03-07 06:48:03 - train: epoch 0075, iter [02300, 10009], lr: 0.001000, loss: 1.5242
2022-03-07 06:48:23 - train: epoch 0075, iter [02400, 10009], lr: 0.001000, loss: 1.3908
2022-03-07 06:48:43 - train: epoch 0075, iter [02500, 10009], lr: 0.001000, loss: 1.4797
2022-03-07 06:49:03 - train: epoch 0075, iter [02600, 10009], lr: 0.001000, loss: 1.2731
2022-03-07 06:49:23 - train: epoch 0075, iter [02700, 10009], lr: 0.001000, loss: 1.3632
2022-03-07 06:49:43 - train: epoch 0075, iter [02800, 10009], lr: 0.001000, loss: 1.1567
2022-03-07 06:50:03 - train: epoch 0075, iter [02900, 10009], lr: 0.001000, loss: 1.4301
2022-03-07 06:50:23 - train: epoch 0075, iter [03000, 10009], lr: 0.001000, loss: 1.1740
2022-03-07 06:50:43 - train: epoch 0075, iter [03100, 10009], lr: 0.001000, loss: 1.2820
2022-03-07 06:51:03 - train: epoch 0075, iter [03200, 10009], lr: 0.001000, loss: 1.3138
2022-03-07 06:51:22 - train: epoch 0075, iter [03300, 10009], lr: 0.001000, loss: 1.2306
2022-03-07 06:51:42 - train: epoch 0075, iter [03400, 10009], lr: 0.001000, loss: 1.2738
2022-03-07 06:52:02 - train: epoch 0075, iter [03500, 10009], lr: 0.001000, loss: 1.3832
2022-03-07 06:52:22 - train: epoch 0075, iter [03600, 10009], lr: 0.001000, loss: 0.9590
2022-03-07 06:52:42 - train: epoch 0075, iter [03700, 10009], lr: 0.001000, loss: 1.0608
2022-03-07 06:53:02 - train: epoch 0075, iter [03800, 10009], lr: 0.001000, loss: 1.0131
2022-03-07 06:53:22 - train: epoch 0075, iter [03900, 10009], lr: 0.001000, loss: 1.4345
2022-03-07 06:53:42 - train: epoch 0075, iter [04000, 10009], lr: 0.001000, loss: 1.0690
2022-03-07 06:54:02 - train: epoch 0075, iter [04100, 10009], lr: 0.001000, loss: 1.4434
2022-03-07 06:54:22 - train: epoch 0075, iter [04200, 10009], lr: 0.001000, loss: 1.0460
2022-03-07 06:54:42 - train: epoch 0075, iter [04300, 10009], lr: 0.001000, loss: 1.3315
2022-03-07 06:55:02 - train: epoch 0075, iter [04400, 10009], lr: 0.001000, loss: 1.3245
2022-03-07 06:55:22 - train: epoch 0075, iter [04500, 10009], lr: 0.001000, loss: 1.1263
2022-03-07 06:55:42 - train: epoch 0075, iter [04600, 10009], lr: 0.001000, loss: 0.7701
2022-03-07 06:56:02 - train: epoch 0075, iter [04700, 10009], lr: 0.001000, loss: 1.2017
2022-03-07 06:56:21 - train: epoch 0075, iter [04800, 10009], lr: 0.001000, loss: 1.2463
2022-03-07 06:56:41 - train: epoch 0075, iter [04900, 10009], lr: 0.001000, loss: 1.3937
2022-03-07 06:57:01 - train: epoch 0075, iter [05000, 10009], lr: 0.001000, loss: 1.0708
2022-03-07 06:57:21 - train: epoch 0075, iter [05100, 10009], lr: 0.001000, loss: 1.5504
2022-03-07 06:57:41 - train: epoch 0075, iter [05200, 10009], lr: 0.001000, loss: 1.4164
2022-03-07 06:58:01 - train: epoch 0075, iter [05300, 10009], lr: 0.001000, loss: 1.2382
2022-03-07 06:58:22 - train: epoch 0075, iter [05400, 10009], lr: 0.001000, loss: 1.2454
2022-03-07 06:58:42 - train: epoch 0075, iter [05500, 10009], lr: 0.001000, loss: 1.5656
2022-03-07 06:59:02 - train: epoch 0075, iter [05600, 10009], lr: 0.001000, loss: 1.3303
2022-03-07 06:59:22 - train: epoch 0075, iter [05700, 10009], lr: 0.001000, loss: 1.3004
2022-03-07 06:59:42 - train: epoch 0075, iter [05800, 10009], lr: 0.001000, loss: 1.0983
2022-03-07 07:00:02 - train: epoch 0075, iter [05900, 10009], lr: 0.001000, loss: 1.5778
2022-03-07 07:00:22 - train: epoch 0075, iter [06000, 10009], lr: 0.001000, loss: 1.2685
2022-03-07 07:00:42 - train: epoch 0075, iter [06100, 10009], lr: 0.001000, loss: 1.5500
2022-03-07 07:01:02 - train: epoch 0075, iter [06200, 10009], lr: 0.001000, loss: 1.3749
2022-03-07 07:01:22 - train: epoch 0075, iter [06300, 10009], lr: 0.001000, loss: 1.2623
2022-03-07 07:01:42 - train: epoch 0075, iter [06400, 10009], lr: 0.001000, loss: 1.4594
2022-03-07 07:02:02 - train: epoch 0075, iter [06500, 10009], lr: 0.001000, loss: 1.2133
2022-03-07 07:02:21 - train: epoch 0075, iter [06600, 10009], lr: 0.001000, loss: 1.5180
2022-03-07 07:02:41 - train: epoch 0075, iter [06700, 10009], lr: 0.001000, loss: 1.3042
2022-03-07 07:03:02 - train: epoch 0075, iter [06800, 10009], lr: 0.001000, loss: 1.1600
2022-03-07 07:03:22 - train: epoch 0075, iter [06900, 10009], lr: 0.001000, loss: 1.0956
2022-03-07 07:03:42 - train: epoch 0075, iter [07000, 10009], lr: 0.001000, loss: 1.3037
2022-03-07 07:04:02 - train: epoch 0075, iter [07100, 10009], lr: 0.001000, loss: 1.4440
2022-03-07 07:04:22 - train: epoch 0075, iter [07200, 10009], lr: 0.001000, loss: 1.4763
2022-03-07 07:04:42 - train: epoch 0075, iter [07300, 10009], lr: 0.001000, loss: 1.1997
2022-03-07 07:05:02 - train: epoch 0075, iter [07400, 10009], lr: 0.001000, loss: 1.5136
2022-03-07 07:05:22 - train: epoch 0075, iter [07500, 10009], lr: 0.001000, loss: 1.1005
2022-03-07 07:05:42 - train: epoch 0075, iter [07600, 10009], lr: 0.001000, loss: 1.1817
2022-03-07 07:06:02 - train: epoch 0075, iter [07700, 10009], lr: 0.001000, loss: 1.3853
2022-03-07 07:06:22 - train: epoch 0075, iter [07800, 10009], lr: 0.001000, loss: 1.4720
2022-03-07 07:06:42 - train: epoch 0075, iter [07900, 10009], lr: 0.001000, loss: 0.9985
2022-03-07 07:07:02 - train: epoch 0075, iter [08000, 10009], lr: 0.001000, loss: 1.2222
2022-03-07 07:07:22 - train: epoch 0075, iter [08100, 10009], lr: 0.001000, loss: 1.0055
2022-03-07 07:07:42 - train: epoch 0075, iter [08200, 10009], lr: 0.001000, loss: 1.1080
2022-03-07 07:08:02 - train: epoch 0075, iter [08300, 10009], lr: 0.001000, loss: 1.1077
2022-03-07 07:08:22 - train: epoch 0075, iter [08400, 10009], lr: 0.001000, loss: 1.1909
2022-03-07 07:08:42 - train: epoch 0075, iter [08500, 10009], lr: 0.001000, loss: 1.4016
2022-03-07 07:09:02 - train: epoch 0075, iter [08600, 10009], lr: 0.001000, loss: 1.2758
2022-03-07 07:09:22 - train: epoch 0075, iter [08700, 10009], lr: 0.001000, loss: 1.4097
2022-03-07 07:09:42 - train: epoch 0075, iter [08800, 10009], lr: 0.001000, loss: 1.0467
2022-03-07 07:10:02 - train: epoch 0075, iter [08900, 10009], lr: 0.001000, loss: 1.1836
2022-03-07 07:10:22 - train: epoch 0075, iter [09000, 10009], lr: 0.001000, loss: 1.1242
2022-03-07 07:10:42 - train: epoch 0075, iter [09100, 10009], lr: 0.001000, loss: 1.3454
2022-03-07 07:11:03 - train: epoch 0075, iter [09200, 10009], lr: 0.001000, loss: 0.9892
2022-03-07 07:11:23 - train: epoch 0075, iter [09300, 10009], lr: 0.001000, loss: 1.1822
2022-03-07 07:11:43 - train: epoch 0075, iter [09400, 10009], lr: 0.001000, loss: 1.5439
2022-03-07 07:12:03 - train: epoch 0075, iter [09500, 10009], lr: 0.001000, loss: 1.1794
2022-03-07 07:12:23 - train: epoch 0075, iter [09600, 10009], lr: 0.001000, loss: 0.9062
2022-03-07 07:12:43 - train: epoch 0075, iter [09700, 10009], lr: 0.001000, loss: 1.1297
2022-03-07 07:13:03 - train: epoch 0075, iter [09800, 10009], lr: 0.001000, loss: 1.2289
2022-03-07 07:13:23 - train: epoch 0075, iter [09900, 10009], lr: 0.001000, loss: 1.0724
2022-03-07 07:13:43 - train: epoch 0075, iter [10000, 10009], lr: 0.001000, loss: 1.0005
2022-03-07 07:13:46 - train: epoch 075, train_loss: 1.2688
2022-03-07 07:14:59 - eval: epoch: 075, acc1: 72.452%, acc5: 90.738%, test_loss: 1.1082, per_image_load_time: 0.947ms, per_image_inference_time: 0.913ms
2022-03-07 07:14:59 - until epoch: 075, best_acc1: 72.452%
2022-03-07 07:14:59 - epoch 076 lr: 0.0010000000000000002
2022-03-07 07:15:24 - train: epoch 0076, iter [00100, 10009], lr: 0.001000, loss: 1.2839
2022-03-07 07:15:43 - train: epoch 0076, iter [00200, 10009], lr: 0.001000, loss: 1.3765
2022-03-07 07:16:03 - train: epoch 0076, iter [00300, 10009], lr: 0.001000, loss: 1.1068
2022-03-07 07:16:23 - train: epoch 0076, iter [00400, 10009], lr: 0.001000, loss: 1.3425
2022-03-07 07:16:43 - train: epoch 0076, iter [00500, 10009], lr: 0.001000, loss: 1.4141
2022-03-07 07:17:03 - train: epoch 0076, iter [00600, 10009], lr: 0.001000, loss: 1.3431
2022-03-07 07:17:23 - train: epoch 0076, iter [00700, 10009], lr: 0.001000, loss: 1.1474
2022-03-07 07:17:42 - train: epoch 0076, iter [00800, 10009], lr: 0.001000, loss: 1.2260
2022-03-07 07:18:02 - train: epoch 0076, iter [00900, 10009], lr: 0.001000, loss: 1.0227
2022-03-07 07:18:22 - train: epoch 0076, iter [01000, 10009], lr: 0.001000, loss: 1.2950
2022-03-07 07:18:42 - train: epoch 0076, iter [01100, 10009], lr: 0.001000, loss: 1.6054
2022-03-07 07:19:02 - train: epoch 0076, iter [01200, 10009], lr: 0.001000, loss: 1.3092
2022-03-07 07:19:22 - train: epoch 0076, iter [01300, 10009], lr: 0.001000, loss: 1.4860
2022-03-07 07:19:42 - train: epoch 0076, iter [01400, 10009], lr: 0.001000, loss: 1.2988
2022-03-07 07:20:02 - train: epoch 0076, iter [01500, 10009], lr: 0.001000, loss: 1.3721
2022-03-07 07:20:22 - train: epoch 0076, iter [01600, 10009], lr: 0.001000, loss: 1.4858
2022-03-07 07:20:42 - train: epoch 0076, iter [01700, 10009], lr: 0.001000, loss: 1.3886
2022-03-07 07:21:01 - train: epoch 0076, iter [01800, 10009], lr: 0.001000, loss: 1.1766
2022-03-07 07:21:21 - train: epoch 0076, iter [01900, 10009], lr: 0.001000, loss: 1.4446
2022-03-07 07:21:41 - train: epoch 0076, iter [02000, 10009], lr: 0.001000, loss: 1.4311
2022-03-07 07:22:01 - train: epoch 0076, iter [02100, 10009], lr: 0.001000, loss: 1.2111
2022-03-07 07:22:21 - train: epoch 0076, iter [02200, 10009], lr: 0.001000, loss: 1.0628
2022-03-07 07:22:41 - train: epoch 0076, iter [02300, 10009], lr: 0.001000, loss: 1.2121
2022-03-07 07:23:00 - train: epoch 0076, iter [02400, 10009], lr: 0.001000, loss: 1.1475
2022-03-07 07:23:20 - train: epoch 0076, iter [02500, 10009], lr: 0.001000, loss: 1.1702
2022-03-07 07:23:40 - train: epoch 0076, iter [02600, 10009], lr: 0.001000, loss: 1.3419
2022-03-07 07:24:00 - train: epoch 0076, iter [02700, 10009], lr: 0.001000, loss: 1.3212
2022-03-07 07:24:20 - train: epoch 0076, iter [02800, 10009], lr: 0.001000, loss: 1.1726
2022-03-07 07:24:40 - train: epoch 0076, iter [02900, 10009], lr: 0.001000, loss: 1.2092
2022-03-07 07:25:00 - train: epoch 0076, iter [03000, 10009], lr: 0.001000, loss: 1.2496
2022-03-07 07:25:20 - train: epoch 0076, iter [03100, 10009], lr: 0.001000, loss: 1.1651
2022-03-07 07:25:39 - train: epoch 0076, iter [03200, 10009], lr: 0.001000, loss: 1.3651
2022-03-07 07:25:59 - train: epoch 0076, iter [03300, 10009], lr: 0.001000, loss: 1.2546
2022-03-07 07:26:19 - train: epoch 0076, iter [03400, 10009], lr: 0.001000, loss: 1.3841
2022-03-07 07:26:39 - train: epoch 0076, iter [03500, 10009], lr: 0.001000, loss: 1.3537
2022-03-07 07:26:59 - train: epoch 0076, iter [03600, 10009], lr: 0.001000, loss: 1.1634
2022-03-07 07:27:19 - train: epoch 0076, iter [03700, 10009], lr: 0.001000, loss: 1.4563
2022-03-07 07:27:39 - train: epoch 0076, iter [03800, 10009], lr: 0.001000, loss: 1.3984
2022-03-07 07:27:59 - train: epoch 0076, iter [03900, 10009], lr: 0.001000, loss: 1.1152
2022-03-07 07:28:19 - train: epoch 0076, iter [04000, 10009], lr: 0.001000, loss: 1.0477
2022-03-07 07:28:38 - train: epoch 0076, iter [04100, 10009], lr: 0.001000, loss: 1.4690
2022-03-07 07:28:58 - train: epoch 0076, iter [04200, 10009], lr: 0.001000, loss: 1.2616
2022-03-07 07:29:18 - train: epoch 0076, iter [04300, 10009], lr: 0.001000, loss: 1.2771
2022-03-07 07:29:38 - train: epoch 0076, iter [04400, 10009], lr: 0.001000, loss: 1.3182
2022-03-07 07:29:58 - train: epoch 0076, iter [04500, 10009], lr: 0.001000, loss: 1.3673
2022-03-07 07:30:18 - train: epoch 0076, iter [04600, 10009], lr: 0.001000, loss: 1.3945
2022-03-07 07:30:38 - train: epoch 0076, iter [04700, 10009], lr: 0.001000, loss: 1.0973
2022-03-07 07:30:57 - train: epoch 0076, iter [04800, 10009], lr: 0.001000, loss: 1.2245
2022-03-07 07:31:17 - train: epoch 0076, iter [04900, 10009], lr: 0.001000, loss: 1.1037
2022-03-07 07:31:37 - train: epoch 0076, iter [05000, 10009], lr: 0.001000, loss: 1.4644
2022-03-07 07:31:57 - train: epoch 0076, iter [05100, 10009], lr: 0.001000, loss: 1.4127
2022-03-07 07:32:17 - train: epoch 0076, iter [05200, 10009], lr: 0.001000, loss: 1.5655
2022-03-07 07:32:37 - train: epoch 0076, iter [05300, 10009], lr: 0.001000, loss: 1.4079
2022-03-07 07:32:57 - train: epoch 0076, iter [05400, 10009], lr: 0.001000, loss: 1.1478
2022-03-07 07:33:17 - train: epoch 0076, iter [05500, 10009], lr: 0.001000, loss: 1.5370
2022-03-07 07:33:37 - train: epoch 0076, iter [05600, 10009], lr: 0.001000, loss: 1.1725
2022-03-07 07:33:56 - train: epoch 0076, iter [05700, 10009], lr: 0.001000, loss: 1.1482
2022-03-07 07:34:16 - train: epoch 0076, iter [05800, 10009], lr: 0.001000, loss: 1.4383
2022-03-07 07:34:36 - train: epoch 0076, iter [05900, 10009], lr: 0.001000, loss: 1.2377
2022-03-07 07:34:56 - train: epoch 0076, iter [06000, 10009], lr: 0.001000, loss: 1.1003
2022-03-07 07:35:16 - train: epoch 0076, iter [06100, 10009], lr: 0.001000, loss: 1.4134
2022-03-07 07:35:36 - train: epoch 0076, iter [06200, 10009], lr: 0.001000, loss: 1.4952
2022-03-07 07:35:56 - train: epoch 0076, iter [06300, 10009], lr: 0.001000, loss: 1.2465
2022-03-07 07:36:16 - train: epoch 0076, iter [06400, 10009], lr: 0.001000, loss: 1.0669
2022-03-07 07:36:36 - train: epoch 0076, iter [06500, 10009], lr: 0.001000, loss: 1.2356
2022-03-07 07:36:56 - train: epoch 0076, iter [06600, 10009], lr: 0.001000, loss: 1.2340
2022-03-07 07:37:16 - train: epoch 0076, iter [06700, 10009], lr: 0.001000, loss: 1.2762
2022-03-07 07:37:36 - train: epoch 0076, iter [06800, 10009], lr: 0.001000, loss: 1.1832
2022-03-07 07:37:55 - train: epoch 0076, iter [06900, 10009], lr: 0.001000, loss: 1.2857
2022-03-07 07:38:15 - train: epoch 0076, iter [07000, 10009], lr: 0.001000, loss: 1.0690
2022-03-07 07:38:35 - train: epoch 0076, iter [07100, 10009], lr: 0.001000, loss: 1.1796
2022-03-07 07:38:55 - train: epoch 0076, iter [07200, 10009], lr: 0.001000, loss: 1.3429
2022-03-07 07:39:15 - train: epoch 0076, iter [07300, 10009], lr: 0.001000, loss: 1.2532
2022-03-07 07:39:35 - train: epoch 0076, iter [07400, 10009], lr: 0.001000, loss: 1.1009
2022-03-07 07:39:55 - train: epoch 0076, iter [07500, 10009], lr: 0.001000, loss: 1.0796
2022-03-07 07:40:15 - train: epoch 0076, iter [07600, 10009], lr: 0.001000, loss: 1.3241
2022-03-07 07:40:35 - train: epoch 0076, iter [07700, 10009], lr: 0.001000, loss: 1.1858
2022-03-07 07:40:55 - train: epoch 0076, iter [07800, 10009], lr: 0.001000, loss: 1.2462
2022-03-07 07:41:15 - train: epoch 0076, iter [07900, 10009], lr: 0.001000, loss: 1.2627
2022-03-07 07:41:34 - train: epoch 0076, iter [08000, 10009], lr: 0.001000, loss: 1.3766
2022-03-07 07:41:54 - train: epoch 0076, iter [08100, 10009], lr: 0.001000, loss: 1.1888
2022-03-07 07:42:14 - train: epoch 0076, iter [08200, 10009], lr: 0.001000, loss: 1.3351
2022-03-07 07:42:34 - train: epoch 0076, iter [08300, 10009], lr: 0.001000, loss: 1.1555
2022-03-07 07:42:54 - train: epoch 0076, iter [08400, 10009], lr: 0.001000, loss: 1.5410
2022-03-07 07:43:14 - train: epoch 0076, iter [08500, 10009], lr: 0.001000, loss: 1.2141
2022-03-07 07:43:34 - train: epoch 0076, iter [08600, 10009], lr: 0.001000, loss: 1.2975
2022-03-07 07:43:54 - train: epoch 0076, iter [08700, 10009], lr: 0.001000, loss: 1.2841
2022-03-07 07:44:14 - train: epoch 0076, iter [08800, 10009], lr: 0.001000, loss: 1.3962
2022-03-07 07:44:34 - train: epoch 0076, iter [08900, 10009], lr: 0.001000, loss: 1.0867
2022-03-07 07:44:54 - train: epoch 0076, iter [09000, 10009], lr: 0.001000, loss: 1.1141
2022-03-07 07:45:14 - train: epoch 0076, iter [09100, 10009], lr: 0.001000, loss: 1.4036
2022-03-07 07:45:34 - train: epoch 0076, iter [09200, 10009], lr: 0.001000, loss: 1.1760
2022-03-07 07:45:53 - train: epoch 0076, iter [09300, 10009], lr: 0.001000, loss: 1.2417
2022-03-07 07:46:13 - train: epoch 0076, iter [09400, 10009], lr: 0.001000, loss: 1.3576
2022-03-07 07:46:33 - train: epoch 0076, iter [09500, 10009], lr: 0.001000, loss: 1.2081
2022-03-07 07:46:53 - train: epoch 0076, iter [09600, 10009], lr: 0.001000, loss: 1.3964
2022-03-07 07:47:13 - train: epoch 0076, iter [09700, 10009], lr: 0.001000, loss: 1.3128
2022-03-07 07:47:33 - train: epoch 0076, iter [09800, 10009], lr: 0.001000, loss: 1.2939
2022-03-07 07:47:53 - train: epoch 0076, iter [09900, 10009], lr: 0.001000, loss: 1.3603
2022-03-07 07:48:13 - train: epoch 0076, iter [10000, 10009], lr: 0.001000, loss: 1.5584
2022-03-07 07:48:16 - train: epoch 076, train_loss: 1.2647
2022-03-07 07:49:30 - eval: epoch: 076, acc1: 72.428%, acc5: 90.670%, test_loss: 1.1080, per_image_load_time: 1.340ms, per_image_inference_time: 0.877ms
2022-03-07 07:49:31 - until epoch: 076, best_acc1: 72.452%
2022-03-07 07:49:31 - epoch 077 lr: 0.0010000000000000002
2022-03-07 07:49:54 - train: epoch 0077, iter [00100, 10009], lr: 0.001000, loss: 1.4288
2022-03-07 07:50:14 - train: epoch 0077, iter [00200, 10009], lr: 0.001000, loss: 1.4893
2022-03-07 07:50:34 - train: epoch 0077, iter [00300, 10009], lr: 0.001000, loss: 0.9071
2022-03-07 07:50:54 - train: epoch 0077, iter [00400, 10009], lr: 0.001000, loss: 1.2191
2022-03-07 07:51:14 - train: epoch 0077, iter [00500, 10009], lr: 0.001000, loss: 1.3869
2022-03-07 07:51:33 - train: epoch 0077, iter [00600, 10009], lr: 0.001000, loss: 1.3781
2022-03-07 07:51:53 - train: epoch 0077, iter [00700, 10009], lr: 0.001000, loss: 1.1194
2022-03-07 07:52:13 - train: epoch 0077, iter [00800, 10009], lr: 0.001000, loss: 1.3028
2022-03-07 07:52:33 - train: epoch 0077, iter [00900, 10009], lr: 0.001000, loss: 1.5124
2022-03-07 07:52:53 - train: epoch 0077, iter [01000, 10009], lr: 0.001000, loss: 1.1822
2022-03-07 07:53:13 - train: epoch 0077, iter [01100, 10009], lr: 0.001000, loss: 1.4155
2022-03-07 07:53:33 - train: epoch 0077, iter [01200, 10009], lr: 0.001000, loss: 1.3751
2022-03-07 07:53:52 - train: epoch 0077, iter [01300, 10009], lr: 0.001000, loss: 1.2014
2022-03-07 07:54:12 - train: epoch 0077, iter [01400, 10009], lr: 0.001000, loss: 1.3848
2022-03-07 07:54:32 - train: epoch 0077, iter [01500, 10009], lr: 0.001000, loss: 1.4233
2022-03-07 07:54:52 - train: epoch 0077, iter [01600, 10009], lr: 0.001000, loss: 1.2948
2022-03-07 07:55:12 - train: epoch 0077, iter [01700, 10009], lr: 0.001000, loss: 1.0690
2022-03-07 07:55:32 - train: epoch 0077, iter [01800, 10009], lr: 0.001000, loss: 1.6220
2022-03-07 07:55:52 - train: epoch 0077, iter [01900, 10009], lr: 0.001000, loss: 1.0841
2022-03-07 07:56:12 - train: epoch 0077, iter [02000, 10009], lr: 0.001000, loss: 1.0871
2022-03-07 07:56:32 - train: epoch 0077, iter [02100, 10009], lr: 0.001000, loss: 1.3056
2022-03-07 07:56:51 - train: epoch 0077, iter [02200, 10009], lr: 0.001000, loss: 1.3041
2022-03-07 07:57:11 - train: epoch 0077, iter [02300, 10009], lr: 0.001000, loss: 1.1727
2022-03-07 07:57:31 - train: epoch 0077, iter [02400, 10009], lr: 0.001000, loss: 1.5872
2022-03-07 07:57:51 - train: epoch 0077, iter [02500, 10009], lr: 0.001000, loss: 1.3227
2022-03-07 07:58:11 - train: epoch 0077, iter [02600, 10009], lr: 0.001000, loss: 1.2646
2022-03-07 07:58:31 - train: epoch 0077, iter [02700, 10009], lr: 0.001000, loss: 1.1420
2022-03-07 07:58:51 - train: epoch 0077, iter [02800, 10009], lr: 0.001000, loss: 1.0138
2022-03-07 07:59:11 - train: epoch 0077, iter [02900, 10009], lr: 0.001000, loss: 0.9753
2022-03-07 07:59:31 - train: epoch 0077, iter [03000, 10009], lr: 0.001000, loss: 1.0308
2022-03-07 07:59:51 - train: epoch 0077, iter [03100, 10009], lr: 0.001000, loss: 1.3881
2022-03-07 08:00:11 - train: epoch 0077, iter [03200, 10009], lr: 0.001000, loss: 1.3096
2022-03-07 08:00:31 - train: epoch 0077, iter [03300, 10009], lr: 0.001000, loss: 1.1191
2022-03-07 08:00:51 - train: epoch 0077, iter [03400, 10009], lr: 0.001000, loss: 1.3370
2022-03-07 08:01:11 - train: epoch 0077, iter [03500, 10009], lr: 0.001000, loss: 1.3184
2022-03-07 08:01:31 - train: epoch 0077, iter [03600, 10009], lr: 0.001000, loss: 1.2412
2022-03-07 08:01:51 - train: epoch 0077, iter [03700, 10009], lr: 0.001000, loss: 1.2813
2022-03-07 08:02:11 - train: epoch 0077, iter [03800, 10009], lr: 0.001000, loss: 1.2755
2022-03-07 08:02:31 - train: epoch 0077, iter [03900, 10009], lr: 0.001000, loss: 1.5043
2022-03-07 08:02:50 - train: epoch 0077, iter [04000, 10009], lr: 0.001000, loss: 1.1206
2022-03-07 08:03:10 - train: epoch 0077, iter [04100, 10009], lr: 0.001000, loss: 1.0888
2022-03-07 08:03:30 - train: epoch 0077, iter [04200, 10009], lr: 0.001000, loss: 1.1486
2022-03-07 08:03:50 - train: epoch 0077, iter [04300, 10009], lr: 0.001000, loss: 1.0787
2022-03-07 08:04:11 - train: epoch 0077, iter [04400, 10009], lr: 0.001000, loss: 1.2172
2022-03-07 08:04:30 - train: epoch 0077, iter [04500, 10009], lr: 0.001000, loss: 1.0881
2022-03-07 08:04:50 - train: epoch 0077, iter [04600, 10009], lr: 0.001000, loss: 1.3311
2022-03-07 08:05:10 - train: epoch 0077, iter [04700, 10009], lr: 0.001000, loss: 1.2723
2022-03-07 08:05:30 - train: epoch 0077, iter [04800, 10009], lr: 0.001000, loss: 1.2682
2022-03-07 08:05:50 - train: epoch 0077, iter [04900, 10009], lr: 0.001000, loss: 1.1767
2022-03-07 08:06:10 - train: epoch 0077, iter [05000, 10009], lr: 0.001000, loss: 1.3639
2022-03-07 08:06:30 - train: epoch 0077, iter [05100, 10009], lr: 0.001000, loss: 1.3051
2022-03-07 08:06:50 - train: epoch 0077, iter [05200, 10009], lr: 0.001000, loss: 1.1025
2022-03-07 08:07:10 - train: epoch 0077, iter [05300, 10009], lr: 0.001000, loss: 1.1948
2022-03-07 08:07:30 - train: epoch 0077, iter [05400, 10009], lr: 0.001000, loss: 1.2473
2022-03-07 08:07:50 - train: epoch 0077, iter [05500, 10009], lr: 0.001000, loss: 1.3442
2022-03-07 08:08:10 - train: epoch 0077, iter [05600, 10009], lr: 0.001000, loss: 1.2899
2022-03-07 08:08:30 - train: epoch 0077, iter [05700, 10009], lr: 0.001000, loss: 1.2292
2022-03-07 08:08:50 - train: epoch 0077, iter [05800, 10009], lr: 0.001000, loss: 1.6001
2022-03-07 08:09:09 - train: epoch 0077, iter [05900, 10009], lr: 0.001000, loss: 1.3819
2022-03-07 08:09:29 - train: epoch 0077, iter [06000, 10009], lr: 0.001000, loss: 1.3292
2022-03-07 08:09:49 - train: epoch 0077, iter [06100, 10009], lr: 0.001000, loss: 1.2618
2022-03-07 08:10:09 - train: epoch 0077, iter [06200, 10009], lr: 0.001000, loss: 0.9723
2022-03-07 08:10:29 - train: epoch 0077, iter [06300, 10009], lr: 0.001000, loss: 1.4408
2022-03-07 08:10:49 - train: epoch 0077, iter [06400, 10009], lr: 0.001000, loss: 1.4661
2022-03-07 08:11:09 - train: epoch 0077, iter [06500, 10009], lr: 0.001000, loss: 1.0383
2022-03-07 08:11:29 - train: epoch 0077, iter [06600, 10009], lr: 0.001000, loss: 1.1275
2022-03-07 08:11:49 - train: epoch 0077, iter [06700, 10009], lr: 0.001000, loss: 1.1728
2022-03-07 08:12:09 - train: epoch 0077, iter [06800, 10009], lr: 0.001000, loss: 1.3263
2022-03-07 08:12:29 - train: epoch 0077, iter [06900, 10009], lr: 0.001000, loss: 1.5611
2022-03-07 08:12:49 - train: epoch 0077, iter [07000, 10009], lr: 0.001000, loss: 1.2449
2022-03-07 08:13:09 - train: epoch 0077, iter [07100, 10009], lr: 0.001000, loss: 1.3963
2022-03-07 08:13:29 - train: epoch 0077, iter [07200, 10009], lr: 0.001000, loss: 1.2319
2022-03-07 08:13:49 - train: epoch 0077, iter [07300, 10009], lr: 0.001000, loss: 1.1772
2022-03-07 08:14:09 - train: epoch 0077, iter [07400, 10009], lr: 0.001000, loss: 1.0032
2022-03-07 08:14:29 - train: epoch 0077, iter [07500, 10009], lr: 0.001000, loss: 1.4373
2022-03-07 08:14:49 - train: epoch 0077, iter [07600, 10009], lr: 0.001000, loss: 0.9633
2022-03-07 08:15:09 - train: epoch 0077, iter [07700, 10009], lr: 0.001000, loss: 1.2434
2022-03-07 08:15:29 - train: epoch 0077, iter [07800, 10009], lr: 0.001000, loss: 1.3959
2022-03-07 08:15:49 - train: epoch 0077, iter [07900, 10009], lr: 0.001000, loss: 1.2830
2022-03-07 08:16:09 - train: epoch 0077, iter [08000, 10009], lr: 0.001000, loss: 1.1364
2022-03-07 08:16:29 - train: epoch 0077, iter [08100, 10009], lr: 0.001000, loss: 1.7196
2022-03-07 08:16:49 - train: epoch 0077, iter [08200, 10009], lr: 0.001000, loss: 1.4671
2022-03-07 08:17:09 - train: epoch 0077, iter [08300, 10009], lr: 0.001000, loss: 1.1235
2022-03-07 08:17:29 - train: epoch 0077, iter [08400, 10009], lr: 0.001000, loss: 1.2659
2022-03-07 08:17:49 - train: epoch 0077, iter [08500, 10009], lr: 0.001000, loss: 1.4029
2022-03-07 08:18:09 - train: epoch 0077, iter [08600, 10009], lr: 0.001000, loss: 1.1512
2022-03-07 08:18:29 - train: epoch 0077, iter [08700, 10009], lr: 0.001000, loss: 0.9903
2022-03-07 08:18:49 - train: epoch 0077, iter [08800, 10009], lr: 0.001000, loss: 1.1930
2022-03-07 08:19:09 - train: epoch 0077, iter [08900, 10009], lr: 0.001000, loss: 0.9477
2022-03-07 08:19:29 - train: epoch 0077, iter [09000, 10009], lr: 0.001000, loss: 1.3355
2022-03-07 08:19:49 - train: epoch 0077, iter [09100, 10009], lr: 0.001000, loss: 1.3399
2022-03-07 08:20:09 - train: epoch 0077, iter [09200, 10009], lr: 0.001000, loss: 1.0589
2022-03-07 08:20:29 - train: epoch 0077, iter [09300, 10009], lr: 0.001000, loss: 0.7525
2022-03-07 08:20:49 - train: epoch 0077, iter [09400, 10009], lr: 0.001000, loss: 1.0837
2022-03-07 08:21:09 - train: epoch 0077, iter [09500, 10009], lr: 0.001000, loss: 1.1701
2022-03-07 08:21:29 - train: epoch 0077, iter [09600, 10009], lr: 0.001000, loss: 1.1565
2022-03-07 08:21:49 - train: epoch 0077, iter [09700, 10009], lr: 0.001000, loss: 1.4057
2022-03-07 08:22:09 - train: epoch 0077, iter [09800, 10009], lr: 0.001000, loss: 1.3338
2022-03-07 08:22:29 - train: epoch 0077, iter [09900, 10009], lr: 0.001000, loss: 1.2137
2022-03-07 08:22:49 - train: epoch 0077, iter [10000, 10009], lr: 0.001000, loss: 1.4079
2022-03-07 08:22:52 - train: epoch 077, train_loss: 1.2619
2022-03-07 08:24:07 - eval: epoch: 077, acc1: 72.252%, acc5: 90.618%, test_loss: 1.1104, per_image_load_time: 1.235ms, per_image_inference_time: 0.870ms
2022-03-07 08:24:07 - until epoch: 077, best_acc1: 72.452%
2022-03-07 08:24:07 - epoch 078 lr: 0.0010000000000000002
2022-03-07 08:24:32 - train: epoch 0078, iter [00100, 10009], lr: 0.001000, loss: 0.9495
2022-03-07 08:24:52 - train: epoch 0078, iter [00200, 10009], lr: 0.001000, loss: 1.2344
2022-03-07 08:25:12 - train: epoch 0078, iter [00300, 10009], lr: 0.001000, loss: 1.2260
2022-03-07 08:25:31 - train: epoch 0078, iter [00400, 10009], lr: 0.001000, loss: 1.2705
2022-03-07 08:25:51 - train: epoch 0078, iter [00500, 10009], lr: 0.001000, loss: 1.5325
2022-03-07 08:26:11 - train: epoch 0078, iter [00600, 10009], lr: 0.001000, loss: 1.3849
2022-03-07 08:26:31 - train: epoch 0078, iter [00700, 10009], lr: 0.001000, loss: 1.1363
2022-03-07 08:26:51 - train: epoch 0078, iter [00800, 10009], lr: 0.001000, loss: 1.1911
2022-03-07 08:27:10 - train: epoch 0078, iter [00900, 10009], lr: 0.001000, loss: 1.0746
2022-03-07 08:27:30 - train: epoch 0078, iter [01000, 10009], lr: 0.001000, loss: 1.3510
2022-03-07 08:27:50 - train: epoch 0078, iter [01100, 10009], lr: 0.001000, loss: 1.3412
2022-03-07 08:28:10 - train: epoch 0078, iter [01200, 10009], lr: 0.001000, loss: 1.0753
2022-03-07 08:28:30 - train: epoch 0078, iter [01300, 10009], lr: 0.001000, loss: 1.1581
2022-03-07 08:28:49 - train: epoch 0078, iter [01400, 10009], lr: 0.001000, loss: 1.4474
2022-03-07 08:29:09 - train: epoch 0078, iter [01500, 10009], lr: 0.001000, loss: 1.4182
2022-03-07 08:29:29 - train: epoch 0078, iter [01600, 10009], lr: 0.001000, loss: 1.5107
2022-03-07 08:29:49 - train: epoch 0078, iter [01700, 10009], lr: 0.001000, loss: 1.3325
2022-03-07 08:30:09 - train: epoch 0078, iter [01800, 10009], lr: 0.001000, loss: 1.3982
2022-03-07 08:30:28 - train: epoch 0078, iter [01900, 10009], lr: 0.001000, loss: 1.1859
2022-03-07 08:30:48 - train: epoch 0078, iter [02000, 10009], lr: 0.001000, loss: 1.1485
2022-03-07 08:31:08 - train: epoch 0078, iter [02100, 10009], lr: 0.001000, loss: 1.1950
2022-03-07 08:31:28 - train: epoch 0078, iter [02200, 10009], lr: 0.001000, loss: 0.9797
2022-03-07 08:31:48 - train: epoch 0078, iter [02300, 10009], lr: 0.001000, loss: 1.0054
2022-03-07 08:32:08 - train: epoch 0078, iter [02400, 10009], lr: 0.001000, loss: 1.2601
2022-03-07 08:32:28 - train: epoch 0078, iter [02500, 10009], lr: 0.001000, loss: 1.4507
2022-03-07 08:32:48 - train: epoch 0078, iter [02600, 10009], lr: 0.001000, loss: 1.0891
2022-03-07 08:33:07 - train: epoch 0078, iter [02700, 10009], lr: 0.001000, loss: 1.2068
2022-03-07 08:33:27 - train: epoch 0078, iter [02800, 10009], lr: 0.001000, loss: 1.0160
2022-03-07 08:33:47 - train: epoch 0078, iter [02900, 10009], lr: 0.001000, loss: 1.4441
2022-03-07 08:34:07 - train: epoch 0078, iter [03000, 10009], lr: 0.001000, loss: 1.1907
2022-03-07 08:34:27 - train: epoch 0078, iter [03100, 10009], lr: 0.001000, loss: 1.4300
2022-03-07 08:34:47 - train: epoch 0078, iter [03200, 10009], lr: 0.001000, loss: 1.3793
2022-03-07 08:35:06 - train: epoch 0078, iter [03300, 10009], lr: 0.001000, loss: 1.1265
2022-03-07 08:35:26 - train: epoch 0078, iter [03400, 10009], lr: 0.001000, loss: 1.5016
2022-03-07 08:35:46 - train: epoch 0078, iter [03500, 10009], lr: 0.001000, loss: 1.8100
2022-03-07 08:36:06 - train: epoch 0078, iter [03600, 10009], lr: 0.001000, loss: 1.2301
2022-03-07 08:36:26 - train: epoch 0078, iter [03700, 10009], lr: 0.001000, loss: 1.0222
2022-03-07 08:36:46 - train: epoch 0078, iter [03800, 10009], lr: 0.001000, loss: 1.4445
2022-03-07 08:37:06 - train: epoch 0078, iter [03900, 10009], lr: 0.001000, loss: 1.3099
2022-03-07 08:37:25 - train: epoch 0078, iter [04000, 10009], lr: 0.001000, loss: 1.1924
2022-03-07 08:37:45 - train: epoch 0078, iter [04100, 10009], lr: 0.001000, loss: 1.3095
2022-03-07 08:38:05 - train: epoch 0078, iter [04200, 10009], lr: 0.001000, loss: 1.4109
2022-03-07 08:38:25 - train: epoch 0078, iter [04300, 10009], lr: 0.001000, loss: 1.4274
2022-03-07 08:38:45 - train: epoch 0078, iter [04400, 10009], lr: 0.001000, loss: 1.3306
2022-03-07 08:39:05 - train: epoch 0078, iter [04500, 10009], lr: 0.001000, loss: 1.1952
2022-03-07 08:39:25 - train: epoch 0078, iter [04600, 10009], lr: 0.001000, loss: 1.2858
2022-03-07 08:39:45 - train: epoch 0078, iter [04700, 10009], lr: 0.001000, loss: 1.2177
2022-03-07 08:40:05 - train: epoch 0078, iter [04800, 10009], lr: 0.001000, loss: 1.3053
2022-03-07 08:40:24 - train: epoch 0078, iter [04900, 10009], lr: 0.001000, loss: 1.3035
2022-03-07 08:40:44 - train: epoch 0078, iter [05000, 10009], lr: 0.001000, loss: 1.4632
2022-03-07 08:41:04 - train: epoch 0078, iter [05100, 10009], lr: 0.001000, loss: 1.0579
2022-03-07 08:41:24 - train: epoch 0078, iter [05200, 10009], lr: 0.001000, loss: 1.5526
2022-03-07 08:41:44 - train: epoch 0078, iter [05300, 10009], lr: 0.001000, loss: 0.9882
2022-03-07 08:42:04 - train: epoch 0078, iter [05400, 10009], lr: 0.001000, loss: 1.0561
2022-03-07 08:42:24 - train: epoch 0078, iter [05500, 10009], lr: 0.001000, loss: 1.1974
2022-03-07 08:42:44 - train: epoch 0078, iter [05600, 10009], lr: 0.001000, loss: 1.1866
2022-03-07 08:43:04 - train: epoch 0078, iter [05700, 10009], lr: 0.001000, loss: 0.8925
2022-03-07 08:43:24 - train: epoch 0078, iter [05800, 10009], lr: 0.001000, loss: 1.0087
2022-03-07 08:43:44 - train: epoch 0078, iter [05900, 10009], lr: 0.001000, loss: 1.1984
2022-03-07 08:44:04 - train: epoch 0078, iter [06000, 10009], lr: 0.001000, loss: 1.0483
2022-03-07 08:44:24 - train: epoch 0078, iter [06100, 10009], lr: 0.001000, loss: 1.1090
2022-03-07 08:44:44 - train: epoch 0078, iter [06200, 10009], lr: 0.001000, loss: 1.3339
2022-03-07 08:45:04 - train: epoch 0078, iter [06300, 10009], lr: 0.001000, loss: 1.3216
2022-03-07 08:45:24 - train: epoch 0078, iter [06400, 10009], lr: 0.001000, loss: 1.0880
2022-03-07 08:45:44 - train: epoch 0078, iter [06500, 10009], lr: 0.001000, loss: 1.5279
2022-03-07 08:46:04 - train: epoch 0078, iter [06600, 10009], lr: 0.001000, loss: 1.3961
2022-03-07 08:46:24 - train: epoch 0078, iter [06700, 10009], lr: 0.001000, loss: 1.1925
2022-03-07 08:46:44 - train: epoch 0078, iter [06800, 10009], lr: 0.001000, loss: 1.2154
2022-03-07 08:47:04 - train: epoch 0078, iter [06900, 10009], lr: 0.001000, loss: 1.1909
2022-03-07 08:47:24 - train: epoch 0078, iter [07000, 10009], lr: 0.001000, loss: 1.2179
2022-03-07 08:47:43 - train: epoch 0078, iter [07100, 10009], lr: 0.001000, loss: 1.2626
2022-03-07 08:48:03 - train: epoch 0078, iter [07200, 10009], lr: 0.001000, loss: 1.4322
2022-03-07 08:48:23 - train: epoch 0078, iter [07300, 10009], lr: 0.001000, loss: 1.3733
2022-03-07 08:48:43 - train: epoch 0078, iter [07400, 10009], lr: 0.001000, loss: 1.4928
2022-03-07 08:49:03 - train: epoch 0078, iter [07500, 10009], lr: 0.001000, loss: 1.1792
2022-03-07 08:49:23 - train: epoch 0078, iter [07600, 10009], lr: 0.001000, loss: 1.1985
2022-03-07 08:49:42 - train: epoch 0078, iter [07700, 10009], lr: 0.001000, loss: 1.1163
2022-03-07 08:50:02 - train: epoch 0078, iter [07800, 10009], lr: 0.001000, loss: 1.1195
2022-03-07 08:50:22 - train: epoch 0078, iter [07900, 10009], lr: 0.001000, loss: 1.2740
2022-03-07 08:50:42 - train: epoch 0078, iter [08000, 10009], lr: 0.001000, loss: 1.4566
2022-03-07 08:51:02 - train: epoch 0078, iter [08100, 10009], lr: 0.001000, loss: 1.3205
2022-03-07 08:51:22 - train: epoch 0078, iter [08200, 10009], lr: 0.001000, loss: 1.2123
2022-03-07 08:51:42 - train: epoch 0078, iter [08300, 10009], lr: 0.001000, loss: 1.2762
2022-03-07 08:52:02 - train: epoch 0078, iter [08400, 10009], lr: 0.001000, loss: 1.4107
2022-03-07 08:52:21 - train: epoch 0078, iter [08500, 10009], lr: 0.001000, loss: 1.0352
2022-03-07 08:52:41 - train: epoch 0078, iter [08600, 10009], lr: 0.001000, loss: 1.3089
2022-03-07 08:53:01 - train: epoch 0078, iter [08700, 10009], lr: 0.001000, loss: 0.8440
2022-03-07 08:53:21 - train: epoch 0078, iter [08800, 10009], lr: 0.001000, loss: 1.4041
2022-03-07 08:53:41 - train: epoch 0078, iter [08900, 10009], lr: 0.001000, loss: 1.1399
2022-03-07 08:54:01 - train: epoch 0078, iter [09000, 10009], lr: 0.001000, loss: 1.4182
2022-03-07 08:54:21 - train: epoch 0078, iter [09100, 10009], lr: 0.001000, loss: 1.4489
2022-03-07 08:54:41 - train: epoch 0078, iter [09200, 10009], lr: 0.001000, loss: 1.5011
2022-03-07 08:55:01 - train: epoch 0078, iter [09300, 10009], lr: 0.001000, loss: 1.2803
2022-03-07 08:55:21 - train: epoch 0078, iter [09400, 10009], lr: 0.001000, loss: 1.6968
2022-03-07 08:55:41 - train: epoch 0078, iter [09500, 10009], lr: 0.001000, loss: 1.3314
2022-03-07 08:56:01 - train: epoch 0078, iter [09600, 10009], lr: 0.001000, loss: 1.5386
2022-03-07 08:56:21 - train: epoch 0078, iter [09700, 10009], lr: 0.001000, loss: 1.0190
2022-03-07 08:56:41 - train: epoch 0078, iter [09800, 10009], lr: 0.001000, loss: 1.3400
2022-03-07 08:57:01 - train: epoch 0078, iter [09900, 10009], lr: 0.001000, loss: 1.0355
2022-03-07 08:57:21 - train: epoch 0078, iter [10000, 10009], lr: 0.001000, loss: 1.1714
2022-03-07 08:57:24 - train: epoch 078, train_loss: 1.2570
2022-03-07 08:58:40 - eval: epoch: 078, acc1: 72.266%, acc5: 90.626%, test_loss: 1.1075, per_image_load_time: 1.262ms, per_image_inference_time: 0.857ms
2022-03-07 08:58:41 - until epoch: 078, best_acc1: 72.452%
2022-03-07 08:58:41 - epoch 079 lr: 0.0010000000000000002
2022-03-07 08:59:05 - train: epoch 0079, iter [00100, 10009], lr: 0.001000, loss: 1.2766
2022-03-07 08:59:24 - train: epoch 0079, iter [00200, 10009], lr: 0.001000, loss: 1.0410
2022-03-07 08:59:44 - train: epoch 0079, iter [00300, 10009], lr: 0.001000, loss: 1.2836
2022-03-07 09:00:04 - train: epoch 0079, iter [00400, 10009], lr: 0.001000, loss: 1.0577
2022-03-07 09:00:24 - train: epoch 0079, iter [00500, 10009], lr: 0.001000, loss: 1.3642
2022-03-07 09:00:43 - train: epoch 0079, iter [00600, 10009], lr: 0.001000, loss: 1.2599
2022-03-07 09:01:03 - train: epoch 0079, iter [00700, 10009], lr: 0.001000, loss: 1.2999
2022-03-07 09:01:22 - train: epoch 0079, iter [00800, 10009], lr: 0.001000, loss: 1.2787
2022-03-07 09:01:42 - train: epoch 0079, iter [00900, 10009], lr: 0.001000, loss: 1.0974
2022-03-07 09:02:02 - train: epoch 0079, iter [01000, 10009], lr: 0.001000, loss: 1.2572
2022-03-07 09:02:21 - train: epoch 0079, iter [01100, 10009], lr: 0.001000, loss: 1.0564
2022-03-07 09:02:41 - train: epoch 0079, iter [01200, 10009], lr: 0.001000, loss: 1.1465
2022-03-07 09:03:01 - train: epoch 0079, iter [01300, 10009], lr: 0.001000, loss: 1.2482
2022-03-07 09:03:20 - train: epoch 0079, iter [01400, 10009], lr: 0.001000, loss: 1.2481
2022-03-07 09:03:40 - train: epoch 0079, iter [01500, 10009], lr: 0.001000, loss: 1.4846
2022-03-07 09:03:59 - train: epoch 0079, iter [01600, 10009], lr: 0.001000, loss: 1.4718
2022-03-07 09:04:19 - train: epoch 0079, iter [01700, 10009], lr: 0.001000, loss: 1.0768
2022-03-07 09:04:39 - train: epoch 0079, iter [01800, 10009], lr: 0.001000, loss: 1.2568
2022-03-07 09:04:58 - train: epoch 0079, iter [01900, 10009], lr: 0.001000, loss: 1.4142
2022-03-07 09:05:18 - train: epoch 0079, iter [02000, 10009], lr: 0.001000, loss: 1.2836
2022-03-07 09:05:37 - train: epoch 0079, iter [02100, 10009], lr: 0.001000, loss: 1.3625
2022-03-07 09:05:57 - train: epoch 0079, iter [02200, 10009], lr: 0.001000, loss: 1.3832
2022-03-07 09:06:17 - train: epoch 0079, iter [02300, 10009], lr: 0.001000, loss: 1.3851
2022-03-07 09:06:36 - train: epoch 0079, iter [02400, 10009], lr: 0.001000, loss: 1.3396
2022-03-07 09:06:56 - train: epoch 0079, iter [02500, 10009], lr: 0.001000, loss: 1.1741
2022-03-07 09:07:15 - train: epoch 0079, iter [02600, 10009], lr: 0.001000, loss: 0.9710
2022-03-07 09:07:35 - train: epoch 0079, iter [02700, 10009], lr: 0.001000, loss: 1.3878
2022-03-07 09:07:54 - train: epoch 0079, iter [02800, 10009], lr: 0.001000, loss: 1.4428
2022-03-07 09:08:14 - train: epoch 0079, iter [02900, 10009], lr: 0.001000, loss: 1.0364
2022-03-07 09:08:34 - train: epoch 0079, iter [03000, 10009], lr: 0.001000, loss: 1.2256
2022-03-07 09:08:53 - train: epoch 0079, iter [03100, 10009], lr: 0.001000, loss: 1.0770
2022-03-07 09:09:13 - train: epoch 0079, iter [03200, 10009], lr: 0.001000, loss: 1.4325
2022-03-07 09:09:32 - train: epoch 0079, iter [03300, 10009], lr: 0.001000, loss: 1.1987
2022-03-07 09:09:52 - train: epoch 0079, iter [03400, 10009], lr: 0.001000, loss: 1.3275
2022-03-07 09:10:12 - train: epoch 0079, iter [03500, 10009], lr: 0.001000, loss: 1.0783
2022-03-07 09:10:31 - train: epoch 0079, iter [03600, 10009], lr: 0.001000, loss: 1.5745
2022-03-07 09:10:51 - train: epoch 0079, iter [03700, 10009], lr: 0.001000, loss: 1.2933
2022-03-07 09:11:10 - train: epoch 0079, iter [03800, 10009], lr: 0.001000, loss: 1.1947
2022-03-07 09:11:30 - train: epoch 0079, iter [03900, 10009], lr: 0.001000, loss: 1.4552
2022-03-07 09:11:50 - train: epoch 0079, iter [04000, 10009], lr: 0.001000, loss: 1.2338
2022-03-07 09:12:09 - train: epoch 0079, iter [04100, 10009], lr: 0.001000, loss: 0.9877
2022-03-07 09:12:29 - train: epoch 0079, iter [04200, 10009], lr: 0.001000, loss: 1.1728
2022-03-07 09:12:49 - train: epoch 0079, iter [04300, 10009], lr: 0.001000, loss: 1.2792
2022-03-07 09:13:08 - train: epoch 0079, iter [04400, 10009], lr: 0.001000, loss: 1.1425
2022-03-07 09:13:28 - train: epoch 0079, iter [04500, 10009], lr: 0.001000, loss: 1.0026
2022-03-07 09:13:47 - train: epoch 0079, iter [04600, 10009], lr: 0.001000, loss: 1.3793
2022-03-07 09:14:07 - train: epoch 0079, iter [04700, 10009], lr: 0.001000, loss: 1.1945
2022-03-07 09:14:26 - train: epoch 0079, iter [04800, 10009], lr: 0.001000, loss: 1.1613
2022-03-07 09:14:46 - train: epoch 0079, iter [04900, 10009], lr: 0.001000, loss: 1.1624
2022-03-07 09:15:06 - train: epoch 0079, iter [05000, 10009], lr: 0.001000, loss: 1.0865
2022-03-07 09:15:25 - train: epoch 0079, iter [05100, 10009], lr: 0.001000, loss: 1.2892
2022-03-07 09:15:45 - train: epoch 0079, iter [05200, 10009], lr: 0.001000, loss: 1.1058
2022-03-07 09:16:05 - train: epoch 0079, iter [05300, 10009], lr: 0.001000, loss: 1.1517
2022-03-07 09:16:24 - train: epoch 0079, iter [05400, 10009], lr: 0.001000, loss: 0.9889
2022-03-07 09:16:44 - train: epoch 0079, iter [05500, 10009], lr: 0.001000, loss: 1.3048
2022-03-07 09:17:04 - train: epoch 0079, iter [05600, 10009], lr: 0.001000, loss: 1.0460
2022-03-07 09:17:24 - train: epoch 0079, iter [05700, 10009], lr: 0.001000, loss: 1.3036
2022-03-07 09:17:43 - train: epoch 0079, iter [05800, 10009], lr: 0.001000, loss: 1.0572
2022-03-07 09:18:03 - train: epoch 0079, iter [05900, 10009], lr: 0.001000, loss: 0.9416
2022-03-07 09:18:23 - train: epoch 0079, iter [06000, 10009], lr: 0.001000, loss: 1.1942
2022-03-07 09:18:42 - train: epoch 0079, iter [06100, 10009], lr: 0.001000, loss: 1.4396
2022-03-07 09:19:02 - train: epoch 0079, iter [06200, 10009], lr: 0.001000, loss: 1.4533
2022-03-07 09:19:22 - train: epoch 0079, iter [06300, 10009], lr: 0.001000, loss: 1.0323
2022-03-07 09:19:41 - train: epoch 0079, iter [06400, 10009], lr: 0.001000, loss: 1.3342
2022-03-07 09:20:01 - train: epoch 0079, iter [06500, 10009], lr: 0.001000, loss: 1.0929
2022-03-07 09:20:20 - train: epoch 0079, iter [06600, 10009], lr: 0.001000, loss: 1.0752
2022-03-07 09:20:40 - train: epoch 0079, iter [06700, 10009], lr: 0.001000, loss: 1.2154
2022-03-07 09:21:00 - train: epoch 0079, iter [06800, 10009], lr: 0.001000, loss: 1.3725
2022-03-07 09:21:19 - train: epoch 0079, iter [06900, 10009], lr: 0.001000, loss: 1.1897
2022-03-07 09:21:39 - train: epoch 0079, iter [07000, 10009], lr: 0.001000, loss: 1.2515
2022-03-07 09:21:59 - train: epoch 0079, iter [07100, 10009], lr: 0.001000, loss: 1.2200
2022-03-07 09:22:18 - train: epoch 0079, iter [07200, 10009], lr: 0.001000, loss: 1.4096
2022-03-07 09:22:38 - train: epoch 0079, iter [07300, 10009], lr: 0.001000, loss: 1.3398
2022-03-07 09:22:58 - train: epoch 0079, iter [07400, 10009], lr: 0.001000, loss: 1.2659
2022-03-07 09:23:17 - train: epoch 0079, iter [07500, 10009], lr: 0.001000, loss: 0.9579
2022-03-07 09:23:37 - train: epoch 0079, iter [07600, 10009], lr: 0.001000, loss: 1.4467
2022-03-07 09:23:57 - train: epoch 0079, iter [07700, 10009], lr: 0.001000, loss: 1.2527
2022-03-07 09:24:17 - train: epoch 0079, iter [07800, 10009], lr: 0.001000, loss: 1.2349
2022-03-07 09:24:36 - train: epoch 0079, iter [07900, 10009], lr: 0.001000, loss: 1.4041
2022-03-07 09:24:56 - train: epoch 0079, iter [08000, 10009], lr: 0.001000, loss: 1.0526
2022-03-07 09:25:16 - train: epoch 0079, iter [08100, 10009], lr: 0.001000, loss: 1.4841
2022-03-07 09:25:36 - train: epoch 0079, iter [08200, 10009], lr: 0.001000, loss: 1.4479
2022-03-07 09:25:55 - train: epoch 0079, iter [08300, 10009], lr: 0.001000, loss: 1.1542
2022-03-07 09:26:15 - train: epoch 0079, iter [08400, 10009], lr: 0.001000, loss: 1.0955
2022-03-07 09:26:35 - train: epoch 0079, iter [08500, 10009], lr: 0.001000, loss: 1.3425
2022-03-07 09:26:55 - train: epoch 0079, iter [08600, 10009], lr: 0.001000, loss: 1.0406
2022-03-07 09:27:15 - train: epoch 0079, iter [08700, 10009], lr: 0.001000, loss: 1.3336
2022-03-07 09:27:35 - train: epoch 0079, iter [08800, 10009], lr: 0.001000, loss: 1.5520
2022-03-07 09:27:54 - train: epoch 0079, iter [08900, 10009], lr: 0.001000, loss: 1.0568
2022-03-07 09:28:14 - train: epoch 0079, iter [09000, 10009], lr: 0.001000, loss: 1.2876
2022-03-07 09:28:34 - train: epoch 0079, iter [09100, 10009], lr: 0.001000, loss: 1.4095
2022-03-07 09:28:54 - train: epoch 0079, iter [09200, 10009], lr: 0.001000, loss: 1.3619
2022-03-07 09:29:14 - train: epoch 0079, iter [09300, 10009], lr: 0.001000, loss: 1.0072
2022-03-07 09:29:33 - train: epoch 0079, iter [09400, 10009], lr: 0.001000, loss: 0.8316
2022-03-07 09:29:53 - train: epoch 0079, iter [09500, 10009], lr: 0.001000, loss: 1.5082
2022-03-07 09:30:13 - train: epoch 0079, iter [09600, 10009], lr: 0.001000, loss: 1.4948
2022-03-07 09:30:33 - train: epoch 0079, iter [09700, 10009], lr: 0.001000, loss: 1.5247
2022-03-07 09:30:53 - train: epoch 0079, iter [09800, 10009], lr: 0.001000, loss: 1.5993
2022-03-07 09:31:13 - train: epoch 0079, iter [09900, 10009], lr: 0.001000, loss: 1.0883
2022-03-07 09:31:32 - train: epoch 0079, iter [10000, 10009], lr: 0.001000, loss: 1.0623
2022-03-07 09:31:35 - train: epoch 079, train_loss: 1.2550
2022-03-07 09:32:51 - eval: epoch: 079, acc1: 72.394%, acc5: 90.640%, test_loss: 1.1063, per_image_load_time: 1.284ms, per_image_inference_time: 0.844ms
2022-03-07 09:32:51 - until epoch: 079, best_acc1: 72.452%
2022-03-07 09:32:51 - epoch 080 lr: 0.0010000000000000002
2022-03-07 09:33:14 - train: epoch 0080, iter [00100, 10009], lr: 0.001000, loss: 0.9998
2022-03-07 09:33:34 - train: epoch 0080, iter [00200, 10009], lr: 0.001000, loss: 1.3284
2022-03-07 09:33:54 - train: epoch 0080, iter [00300, 10009], lr: 0.001000, loss: 1.0148
2022-03-07 09:34:13 - train: epoch 0080, iter [00400, 10009], lr: 0.001000, loss: 1.7216
2022-03-07 09:34:33 - train: epoch 0080, iter [00500, 10009], lr: 0.001000, loss: 1.3236
2022-03-07 09:34:52 - train: epoch 0080, iter [00600, 10009], lr: 0.001000, loss: 1.4436
2022-03-07 09:35:12 - train: epoch 0080, iter [00700, 10009], lr: 0.001000, loss: 1.3993
2022-03-07 09:35:32 - train: epoch 0080, iter [00800, 10009], lr: 0.001000, loss: 1.1890
2022-03-07 09:35:51 - train: epoch 0080, iter [00900, 10009], lr: 0.001000, loss: 1.2271
2022-03-07 09:36:11 - train: epoch 0080, iter [01000, 10009], lr: 0.001000, loss: 0.9480
2022-03-07 09:36:31 - train: epoch 0080, iter [01100, 10009], lr: 0.001000, loss: 1.1185
2022-03-07 09:36:50 - train: epoch 0080, iter [01200, 10009], lr: 0.001000, loss: 1.1944
2022-03-07 09:37:10 - train: epoch 0080, iter [01300, 10009], lr: 0.001000, loss: 1.3948
2022-03-07 09:37:29 - train: epoch 0080, iter [01400, 10009], lr: 0.001000, loss: 0.8840
2022-03-07 09:37:49 - train: epoch 0080, iter [01500, 10009], lr: 0.001000, loss: 1.2900
2022-03-07 09:38:09 - train: epoch 0080, iter [01600, 10009], lr: 0.001000, loss: 0.8809
2022-03-07 09:38:28 - train: epoch 0080, iter [01700, 10009], lr: 0.001000, loss: 1.3607
2022-03-07 09:38:48 - train: epoch 0080, iter [01800, 10009], lr: 0.001000, loss: 1.1627
2022-03-07 09:39:08 - train: epoch 0080, iter [01900, 10009], lr: 0.001000, loss: 1.3587
2022-03-07 09:39:27 - train: epoch 0080, iter [02000, 10009], lr: 0.001000, loss: 1.1003
2022-03-07 09:39:47 - train: epoch 0080, iter [02100, 10009], lr: 0.001000, loss: 1.2723
2022-03-07 09:40:07 - train: epoch 0080, iter [02200, 10009], lr: 0.001000, loss: 1.4016
2022-03-07 09:40:27 - train: epoch 0080, iter [02300, 10009], lr: 0.001000, loss: 1.2610
2022-03-07 09:40:46 - train: epoch 0080, iter [02400, 10009], lr: 0.001000, loss: 1.3246
2022-03-07 09:41:06 - train: epoch 0080, iter [02500, 10009], lr: 0.001000, loss: 1.3403
2022-03-07 09:41:26 - train: epoch 0080, iter [02600, 10009], lr: 0.001000, loss: 1.1753
2022-03-07 09:41:45 - train: epoch 0080, iter [02700, 10009], lr: 0.001000, loss: 1.0816
2022-03-07 09:42:05 - train: epoch 0080, iter [02800, 10009], lr: 0.001000, loss: 1.0011
2022-03-07 09:42:25 - train: epoch 0080, iter [02900, 10009], lr: 0.001000, loss: 1.3104
2022-03-07 09:42:44 - train: epoch 0080, iter [03000, 10009], lr: 0.001000, loss: 1.1076
2022-03-07 09:43:04 - train: epoch 0080, iter [03100, 10009], lr: 0.001000, loss: 1.2356
2022-03-07 09:43:24 - train: epoch 0080, iter [03200, 10009], lr: 0.001000, loss: 1.2111
2022-03-07 09:43:43 - train: epoch 0080, iter [03300, 10009], lr: 0.001000, loss: 1.2949
2022-03-07 09:44:03 - train: epoch 0080, iter [03400, 10009], lr: 0.001000, loss: 1.3570
2022-03-07 09:44:23 - train: epoch 0080, iter [03500, 10009], lr: 0.001000, loss: 1.2926
2022-03-07 09:44:42 - train: epoch 0080, iter [03600, 10009], lr: 0.001000, loss: 1.1910
2022-03-07 09:45:02 - train: epoch 0080, iter [03700, 10009], lr: 0.001000, loss: 1.0504
2022-03-07 09:45:22 - train: epoch 0080, iter [03800, 10009], lr: 0.001000, loss: 1.3438
2022-03-07 09:45:42 - train: epoch 0080, iter [03900, 10009], lr: 0.001000, loss: 1.0502
2022-03-07 09:46:02 - train: epoch 0080, iter [04000, 10009], lr: 0.001000, loss: 1.1294
2022-03-07 09:46:21 - train: epoch 0080, iter [04100, 10009], lr: 0.001000, loss: 0.9619
2022-03-07 09:46:41 - train: epoch 0080, iter [04200, 10009], lr: 0.001000, loss: 1.5451
2022-03-07 09:47:01 - train: epoch 0080, iter [04300, 10009], lr: 0.001000, loss: 1.0264
2022-03-07 09:47:21 - train: epoch 0080, iter [04400, 10009], lr: 0.001000, loss: 1.2024
2022-03-07 09:47:41 - train: epoch 0080, iter [04500, 10009], lr: 0.001000, loss: 1.0855
2022-03-07 09:48:00 - train: epoch 0080, iter [04600, 10009], lr: 0.001000, loss: 1.1819
2022-03-07 09:48:20 - train: epoch 0080, iter [04700, 10009], lr: 0.001000, loss: 1.3568
2022-03-07 09:48:40 - train: epoch 0080, iter [04800, 10009], lr: 0.001000, loss: 1.2302
2022-03-07 09:49:00 - train: epoch 0080, iter [04900, 10009], lr: 0.001000, loss: 1.1596
2022-03-07 09:49:20 - train: epoch 0080, iter [05000, 10009], lr: 0.001000, loss: 0.9973
2022-03-07 09:49:39 - train: epoch 0080, iter [05100, 10009], lr: 0.001000, loss: 1.3791
2022-03-07 09:49:59 - train: epoch 0080, iter [05200, 10009], lr: 0.001000, loss: 1.5602
2022-03-07 09:50:19 - train: epoch 0080, iter [05300, 10009], lr: 0.001000, loss: 1.0737
2022-03-07 09:50:38 - train: epoch 0080, iter [05400, 10009], lr: 0.001000, loss: 1.0854
2022-03-07 09:50:58 - train: epoch 0080, iter [05500, 10009], lr: 0.001000, loss: 1.1131
2022-03-07 09:51:18 - train: epoch 0080, iter [05600, 10009], lr: 0.001000, loss: 1.3262
2022-03-07 09:51:37 - train: epoch 0080, iter [05700, 10009], lr: 0.001000, loss: 1.1661
2022-03-07 09:51:57 - train: epoch 0080, iter [05800, 10009], lr: 0.001000, loss: 1.2790
2022-03-07 09:52:17 - train: epoch 0080, iter [05900, 10009], lr: 0.001000, loss: 1.2843
2022-03-07 09:52:37 - train: epoch 0080, iter [06000, 10009], lr: 0.001000, loss: 1.2240
2022-03-07 09:52:56 - train: epoch 0080, iter [06100, 10009], lr: 0.001000, loss: 1.3532
2022-03-07 09:53:16 - train: epoch 0080, iter [06200, 10009], lr: 0.001000, loss: 1.3623
2022-03-07 09:53:36 - train: epoch 0080, iter [06300, 10009], lr: 0.001000, loss: 1.1987
2022-03-07 09:53:56 - train: epoch 0080, iter [06400, 10009], lr: 0.001000, loss: 1.0846
2022-03-07 09:54:15 - train: epoch 0080, iter [06500, 10009], lr: 0.001000, loss: 1.1786
2022-03-07 09:54:35 - train: epoch 0080, iter [06600, 10009], lr: 0.001000, loss: 1.3907
2022-03-07 09:54:55 - train: epoch 0080, iter [06700, 10009], lr: 0.001000, loss: 1.2255
2022-03-07 09:55:15 - train: epoch 0080, iter [06800, 10009], lr: 0.001000, loss: 1.2517
2022-03-07 09:55:35 - train: epoch 0080, iter [06900, 10009], lr: 0.001000, loss: 1.0216
2022-03-07 09:55:54 - train: epoch 0080, iter [07000, 10009], lr: 0.001000, loss: 1.2871
2022-03-07 09:56:14 - train: epoch 0080, iter [07100, 10009], lr: 0.001000, loss: 0.8823
2022-03-07 09:56:34 - train: epoch 0080, iter [07200, 10009], lr: 0.001000, loss: 1.1398
2022-03-07 09:56:54 - train: epoch 0080, iter [07300, 10009], lr: 0.001000, loss: 1.2667
2022-03-07 09:57:13 - train: epoch 0080, iter [07400, 10009], lr: 0.001000, loss: 1.2068
2022-03-07 09:57:33 - train: epoch 0080, iter [07500, 10009], lr: 0.001000, loss: 1.3156
2022-03-07 09:57:53 - train: epoch 0080, iter [07600, 10009], lr: 0.001000, loss: 1.1357
2022-03-07 09:58:13 - train: epoch 0080, iter [07700, 10009], lr: 0.001000, loss: 0.8969
2022-03-07 09:58:32 - train: epoch 0080, iter [07800, 10009], lr: 0.001000, loss: 1.0256
2022-03-07 09:58:52 - train: epoch 0080, iter [07900, 10009], lr: 0.001000, loss: 1.4444
2022-03-07 09:59:12 - train: epoch 0080, iter [08000, 10009], lr: 0.001000, loss: 1.1384
2022-03-07 09:59:32 - train: epoch 0080, iter [08100, 10009], lr: 0.001000, loss: 1.4806
2022-03-07 09:59:51 - train: epoch 0080, iter [08200, 10009], lr: 0.001000, loss: 1.5712
2022-03-07 10:00:11 - train: epoch 0080, iter [08300, 10009], lr: 0.001000, loss: 1.3468
2022-03-07 10:00:31 - train: epoch 0080, iter [08400, 10009], lr: 0.001000, loss: 1.5561
2022-03-07 10:00:51 - train: epoch 0080, iter [08500, 10009], lr: 0.001000, loss: 1.6295
2022-03-07 10:01:10 - train: epoch 0080, iter [08600, 10009], lr: 0.001000, loss: 1.5045
2022-03-07 10:01:30 - train: epoch 0080, iter [08700, 10009], lr: 0.001000, loss: 1.1426
2022-03-07 10:01:50 - train: epoch 0080, iter [08800, 10009], lr: 0.001000, loss: 1.5568
2022-03-07 10:02:10 - train: epoch 0080, iter [08900, 10009], lr: 0.001000, loss: 1.3174
2022-03-07 10:02:29 - train: epoch 0080, iter [09000, 10009], lr: 0.001000, loss: 1.6066
2022-03-07 10:02:49 - train: epoch 0080, iter [09100, 10009], lr: 0.001000, loss: 1.2940
2022-03-07 10:03:09 - train: epoch 0080, iter [09200, 10009], lr: 0.001000, loss: 1.1041
2022-03-07 10:03:29 - train: epoch 0080, iter [09300, 10009], lr: 0.001000, loss: 1.1566
2022-03-07 10:03:48 - train: epoch 0080, iter [09400, 10009], lr: 0.001000, loss: 1.4382
2022-03-07 10:04:08 - train: epoch 0080, iter [09500, 10009], lr: 0.001000, loss: 1.1567
2022-03-07 10:04:28 - train: epoch 0080, iter [09600, 10009], lr: 0.001000, loss: 1.3366
2022-03-07 10:04:48 - train: epoch 0080, iter [09700, 10009], lr: 0.001000, loss: 1.0937
2022-03-07 10:05:08 - train: epoch 0080, iter [09800, 10009], lr: 0.001000, loss: 1.4212
2022-03-07 10:05:27 - train: epoch 0080, iter [09900, 10009], lr: 0.001000, loss: 1.3218
2022-03-07 10:05:47 - train: epoch 0080, iter [10000, 10009], lr: 0.001000, loss: 1.2280
2022-03-07 10:05:50 - train: epoch 080, train_loss: 1.2524
2022-03-07 10:07:06 - eval: epoch: 080, acc1: 72.392%, acc5: 90.668%, test_loss: 1.1075, per_image_load_time: 1.067ms, per_image_inference_time: 0.877ms
2022-03-07 10:07:06 - until epoch: 080, best_acc1: 72.452%
2022-03-07 10:07:06 - epoch 081 lr: 0.0010000000000000002
2022-03-07 10:07:30 - train: epoch 0081, iter [00100, 10009], lr: 0.001000, loss: 1.0234
2022-03-07 10:07:49 - train: epoch 0081, iter [00200, 10009], lr: 0.001000, loss: 1.4210
2022-03-07 10:08:09 - train: epoch 0081, iter [00300, 10009], lr: 0.001000, loss: 1.2120
2022-03-07 10:08:28 - train: epoch 0081, iter [00400, 10009], lr: 0.001000, loss: 1.1272
2022-03-07 10:08:48 - train: epoch 0081, iter [00500, 10009], lr: 0.001000, loss: 1.5631
2022-03-07 10:09:07 - train: epoch 0081, iter [00600, 10009], lr: 0.001000, loss: 1.2467
2022-03-07 10:09:27 - train: epoch 0081, iter [00700, 10009], lr: 0.001000, loss: 1.1801
2022-03-07 10:09:46 - train: epoch 0081, iter [00800, 10009], lr: 0.001000, loss: 0.9562
2022-03-07 10:10:06 - train: epoch 0081, iter [00900, 10009], lr: 0.001000, loss: 1.4523
2022-03-07 10:10:26 - train: epoch 0081, iter [01000, 10009], lr: 0.001000, loss: 1.3214
2022-03-07 10:10:45 - train: epoch 0081, iter [01100, 10009], lr: 0.001000, loss: 1.3838
2022-03-07 10:11:05 - train: epoch 0081, iter [01200, 10009], lr: 0.001000, loss: 1.2135
2022-03-07 10:11:24 - train: epoch 0081, iter [01300, 10009], lr: 0.001000, loss: 1.0995
2022-03-07 10:11:44 - train: epoch 0081, iter [01400, 10009], lr: 0.001000, loss: 1.3600
2022-03-07 10:12:04 - train: epoch 0081, iter [01500, 10009], lr: 0.001000, loss: 1.3889
2022-03-07 10:12:23 - train: epoch 0081, iter [01600, 10009], lr: 0.001000, loss: 1.3520
2022-03-07 10:12:43 - train: epoch 0081, iter [01700, 10009], lr: 0.001000, loss: 1.2424
2022-03-07 10:13:03 - train: epoch 0081, iter [01800, 10009], lr: 0.001000, loss: 1.2607
2022-03-07 10:13:22 - train: epoch 0081, iter [01900, 10009], lr: 0.001000, loss: 1.0886
2022-03-07 10:13:42 - train: epoch 0081, iter [02000, 10009], lr: 0.001000, loss: 1.4599
2022-03-07 10:14:02 - train: epoch 0081, iter [02100, 10009], lr: 0.001000, loss: 1.2234
2022-03-07 10:14:21 - train: epoch 0081, iter [02200, 10009], lr: 0.001000, loss: 1.0654
2022-03-07 10:14:41 - train: epoch 0081, iter [02300, 10009], lr: 0.001000, loss: 1.3974
2022-03-07 10:15:01 - train: epoch 0081, iter [02400, 10009], lr: 0.001000, loss: 1.2738
2022-03-07 10:15:20 - train: epoch 0081, iter [02500, 10009], lr: 0.001000, loss: 1.3040
2022-03-07 10:15:40 - train: epoch 0081, iter [02600, 10009], lr: 0.001000, loss: 1.0571
2022-03-07 10:16:00 - train: epoch 0081, iter [02700, 10009], lr: 0.001000, loss: 1.4289
2022-03-07 10:16:19 - train: epoch 0081, iter [02800, 10009], lr: 0.001000, loss: 1.0192
2022-03-07 10:16:39 - train: epoch 0081, iter [02900, 10009], lr: 0.001000, loss: 1.3345
2022-03-07 10:16:59 - train: epoch 0081, iter [03000, 10009], lr: 0.001000, loss: 1.1737
2022-03-07 10:17:18 - train: epoch 0081, iter [03100, 10009], lr: 0.001000, loss: 1.4198
2022-03-07 10:17:38 - train: epoch 0081, iter [03200, 10009], lr: 0.001000, loss: 1.2605
2022-03-07 10:17:58 - train: epoch 0081, iter [03300, 10009], lr: 0.001000, loss: 1.5945
2022-03-07 10:18:17 - train: epoch 0081, iter [03400, 10009], lr: 0.001000, loss: 1.0802
2022-03-07 10:18:37 - train: epoch 0081, iter [03500, 10009], lr: 0.001000, loss: 0.9381
2022-03-07 10:18:56 - train: epoch 0081, iter [03600, 10009], lr: 0.001000, loss: 1.4300
2022-03-07 10:19:16 - train: epoch 0081, iter [03700, 10009], lr: 0.001000, loss: 1.1240
2022-03-07 10:19:36 - train: epoch 0081, iter [03800, 10009], lr: 0.001000, loss: 1.2089
2022-03-07 10:19:55 - train: epoch 0081, iter [03900, 10009], lr: 0.001000, loss: 1.2526
2022-03-07 10:20:15 - train: epoch 0081, iter [04000, 10009], lr: 0.001000, loss: 1.3407
2022-03-07 10:20:35 - train: epoch 0081, iter [04100, 10009], lr: 0.001000, loss: 1.1364
2022-03-07 10:20:55 - train: epoch 0081, iter [04200, 10009], lr: 0.001000, loss: 1.3580
2022-03-07 10:21:14 - train: epoch 0081, iter [04300, 10009], lr: 0.001000, loss: 0.9323
2022-03-07 10:21:34 - train: epoch 0081, iter [04400, 10009], lr: 0.001000, loss: 1.1866
2022-03-07 10:21:54 - train: epoch 0081, iter [04500, 10009], lr: 0.001000, loss: 1.4965
2022-03-07 10:22:13 - train: epoch 0081, iter [04600, 10009], lr: 0.001000, loss: 0.9151
2022-03-07 10:22:33 - train: epoch 0081, iter [04700, 10009], lr: 0.001000, loss: 1.4489
2022-03-07 10:22:53 - train: epoch 0081, iter [04800, 10009], lr: 0.001000, loss: 1.4813
2022-03-07 10:23:12 - train: epoch 0081, iter [04900, 10009], lr: 0.001000, loss: 1.1584
2022-03-07 10:23:32 - train: epoch 0081, iter [05000, 10009], lr: 0.001000, loss: 1.4540
2022-03-07 10:23:51 - train: epoch 0081, iter [05100, 10009], lr: 0.001000, loss: 1.2040
2022-03-07 10:24:11 - train: epoch 0081, iter [05200, 10009], lr: 0.001000, loss: 1.3804
2022-03-07 10:24:31 - train: epoch 0081, iter [05300, 10009], lr: 0.001000, loss: 1.2702
2022-03-07 10:24:50 - train: epoch 0081, iter [05400, 10009], lr: 0.001000, loss: 1.1713
2022-03-07 10:25:10 - train: epoch 0081, iter [05500, 10009], lr: 0.001000, loss: 1.1756
2022-03-07 10:25:29 - train: epoch 0081, iter [05600, 10009], lr: 0.001000, loss: 1.1697
2022-03-07 10:25:49 - train: epoch 0081, iter [05700, 10009], lr: 0.001000, loss: 1.0920
2022-03-07 10:26:09 - train: epoch 0081, iter [05800, 10009], lr: 0.001000, loss: 1.0604
2022-03-07 10:26:28 - train: epoch 0081, iter [05900, 10009], lr: 0.001000, loss: 1.2562
2022-03-07 10:26:48 - train: epoch 0081, iter [06000, 10009], lr: 0.001000, loss: 1.1317
2022-03-07 10:27:08 - train: epoch 0081, iter [06100, 10009], lr: 0.001000, loss: 1.0591
2022-03-07 10:27:28 - train: epoch 0081, iter [06200, 10009], lr: 0.001000, loss: 1.5421
2022-03-07 10:27:48 - train: epoch 0081, iter [06300, 10009], lr: 0.001000, loss: 1.2440
2022-03-07 10:28:08 - train: epoch 0081, iter [06400, 10009], lr: 0.001000, loss: 0.9772
2022-03-07 10:28:28 - train: epoch 0081, iter [06500, 10009], lr: 0.001000, loss: 1.2420
2022-03-07 10:28:48 - train: epoch 0081, iter [06600, 10009], lr: 0.001000, loss: 1.3931
2022-03-07 10:29:08 - train: epoch 0081, iter [06700, 10009], lr: 0.001000, loss: 1.1840
2022-03-07 10:29:28 - train: epoch 0081, iter [06800, 10009], lr: 0.001000, loss: 1.4112
2022-03-07 10:29:47 - train: epoch 0081, iter [06900, 10009], lr: 0.001000, loss: 1.2790
2022-03-07 10:30:07 - train: epoch 0081, iter [07000, 10009], lr: 0.001000, loss: 1.5511
2022-03-07 10:30:27 - train: epoch 0081, iter [07100, 10009], lr: 0.001000, loss: 1.2832
2022-03-07 10:30:47 - train: epoch 0081, iter [07200, 10009], lr: 0.001000, loss: 1.1345
2022-03-07 10:31:07 - train: epoch 0081, iter [07300, 10009], lr: 0.001000, loss: 1.3839
2022-03-07 10:31:27 - train: epoch 0081, iter [07400, 10009], lr: 0.001000, loss: 1.4495
2022-03-07 10:31:46 - train: epoch 0081, iter [07500, 10009], lr: 0.001000, loss: 1.1723
2022-03-07 10:32:06 - train: epoch 0081, iter [07600, 10009], lr: 0.001000, loss: 1.3403
2022-03-07 10:32:26 - train: epoch 0081, iter [07700, 10009], lr: 0.001000, loss: 1.2669
2022-03-07 10:32:46 - train: epoch 0081, iter [07800, 10009], lr: 0.001000, loss: 1.7696
2022-03-07 10:33:06 - train: epoch 0081, iter [07900, 10009], lr: 0.001000, loss: 1.1908
2022-03-07 10:33:26 - train: epoch 0081, iter [08000, 10009], lr: 0.001000, loss: 1.0799
2022-03-07 10:33:45 - train: epoch 0081, iter [08100, 10009], lr: 0.001000, loss: 1.1958
2022-03-07 10:34:05 - train: epoch 0081, iter [08200, 10009], lr: 0.001000, loss: 1.2772
2022-03-07 10:34:25 - train: epoch 0081, iter [08300, 10009], lr: 0.001000, loss: 1.3775
2022-03-07 10:34:45 - train: epoch 0081, iter [08400, 10009], lr: 0.001000, loss: 1.2261
2022-03-07 10:35:05 - train: epoch 0081, iter [08500, 10009], lr: 0.001000, loss: 1.1953
2022-03-07 10:35:25 - train: epoch 0081, iter [08600, 10009], lr: 0.001000, loss: 1.2212
2022-03-07 10:35:45 - train: epoch 0081, iter [08700, 10009], lr: 0.001000, loss: 1.1226
2022-03-07 10:36:04 - train: epoch 0081, iter [08800, 10009], lr: 0.001000, loss: 1.4684
2022-03-07 10:36:24 - train: epoch 0081, iter [08900, 10009], lr: 0.001000, loss: 1.3722
2022-03-07 10:36:44 - train: epoch 0081, iter [09000, 10009], lr: 0.001000, loss: 1.2057
2022-03-07 10:37:04 - train: epoch 0081, iter [09100, 10009], lr: 0.001000, loss: 0.8936
2022-03-07 10:37:24 - train: epoch 0081, iter [09200, 10009], lr: 0.001000, loss: 1.0113
2022-03-07 10:37:44 - train: epoch 0081, iter [09300, 10009], lr: 0.001000, loss: 1.1269
2022-03-07 10:38:04 - train: epoch 0081, iter [09400, 10009], lr: 0.001000, loss: 1.2595
2022-03-07 10:38:24 - train: epoch 0081, iter [09500, 10009], lr: 0.001000, loss: 1.1960
2022-03-07 10:38:44 - train: epoch 0081, iter [09600, 10009], lr: 0.001000, loss: 1.1439
2022-03-07 10:39:03 - train: epoch 0081, iter [09700, 10009], lr: 0.001000, loss: 1.3444
2022-03-07 10:39:23 - train: epoch 0081, iter [09800, 10009], lr: 0.001000, loss: 1.2308
2022-03-07 10:39:43 - train: epoch 0081, iter [09900, 10009], lr: 0.001000, loss: 1.5475
2022-03-07 10:40:03 - train: epoch 0081, iter [10000, 10009], lr: 0.001000, loss: 0.9748
2022-03-07 10:40:06 - train: epoch 081, train_loss: 1.2502
2022-03-07 10:41:22 - eval: epoch: 081, acc1: 72.506%, acc5: 90.744%, test_loss: 1.1039, per_image_load_time: 1.898ms, per_image_inference_time: 0.880ms
2022-03-07 10:41:22 - until epoch: 081, best_acc1: 72.506%
2022-03-07 10:41:22 - epoch 082 lr: 0.0010000000000000002
2022-03-07 10:41:46 - train: epoch 0082, iter [00100, 10009], lr: 0.001000, loss: 1.0910
2022-03-07 10:42:06 - train: epoch 0082, iter [00200, 10009], lr: 0.001000, loss: 1.1870
2022-03-07 10:42:25 - train: epoch 0082, iter [00300, 10009], lr: 0.001000, loss: 1.0328
2022-03-07 10:42:45 - train: epoch 0082, iter [00400, 10009], lr: 0.001000, loss: 1.4601
2022-03-07 10:43:05 - train: epoch 0082, iter [00500, 10009], lr: 0.001000, loss: 1.2744
2022-03-07 10:43:24 - train: epoch 0082, iter [00600, 10009], lr: 0.001000, loss: 1.0904
2022-03-07 10:43:44 - train: epoch 0082, iter [00700, 10009], lr: 0.001000, loss: 1.3940
2022-03-07 10:44:03 - train: epoch 0082, iter [00800, 10009], lr: 0.001000, loss: 1.3650
2022-03-07 10:44:23 - train: epoch 0082, iter [00900, 10009], lr: 0.001000, loss: 1.5470
2022-03-07 10:44:42 - train: epoch 0082, iter [01000, 10009], lr: 0.001000, loss: 1.2174
2022-03-07 10:45:01 - train: epoch 0082, iter [01100, 10009], lr: 0.001000, loss: 1.2262
2022-03-07 10:45:21 - train: epoch 0082, iter [01200, 10009], lr: 0.001000, loss: 1.3659
2022-03-07 10:45:40 - train: epoch 0082, iter [01300, 10009], lr: 0.001000, loss: 1.2970
2022-03-07 10:46:00 - train: epoch 0082, iter [01400, 10009], lr: 0.001000, loss: 1.2570
2022-03-07 10:46:20 - train: epoch 0082, iter [01500, 10009], lr: 0.001000, loss: 1.3167
2022-03-07 10:46:39 - train: epoch 0082, iter [01600, 10009], lr: 0.001000, loss: 1.1356
2022-03-07 10:46:59 - train: epoch 0082, iter [01700, 10009], lr: 0.001000, loss: 1.3492
2022-03-07 10:47:19 - train: epoch 0082, iter [01800, 10009], lr: 0.001000, loss: 1.5472
2022-03-07 10:47:38 - train: epoch 0082, iter [01900, 10009], lr: 0.001000, loss: 1.2021
2022-03-07 10:47:58 - train: epoch 0082, iter [02000, 10009], lr: 0.001000, loss: 1.1032
2022-03-07 10:48:17 - train: epoch 0082, iter [02100, 10009], lr: 0.001000, loss: 1.2411
2022-03-07 10:48:37 - train: epoch 0082, iter [02200, 10009], lr: 0.001000, loss: 1.4833
2022-03-07 10:48:57 - train: epoch 0082, iter [02300, 10009], lr: 0.001000, loss: 1.3808
2022-03-07 10:49:16 - train: epoch 0082, iter [02400, 10009], lr: 0.001000, loss: 1.3052
2022-03-07 10:49:36 - train: epoch 0082, iter [02500, 10009], lr: 0.001000, loss: 1.2382
2022-03-07 10:49:56 - train: epoch 0082, iter [02600, 10009], lr: 0.001000, loss: 1.6077
2022-03-07 10:50:15 - train: epoch 0082, iter [02700, 10009], lr: 0.001000, loss: 1.3201
2022-03-07 10:50:35 - train: epoch 0082, iter [02800, 10009], lr: 0.001000, loss: 1.1289
2022-03-07 10:50:55 - train: epoch 0082, iter [02900, 10009], lr: 0.001000, loss: 1.3096
2022-03-07 10:51:14 - train: epoch 0082, iter [03000, 10009], lr: 0.001000, loss: 1.2390
2022-03-07 10:51:34 - train: epoch 0082, iter [03100, 10009], lr: 0.001000, loss: 1.2225
2022-03-07 10:51:54 - train: epoch 0082, iter [03200, 10009], lr: 0.001000, loss: 1.2132
2022-03-07 10:52:14 - train: epoch 0082, iter [03300, 10009], lr: 0.001000, loss: 1.1892
2022-03-07 10:52:33 - train: epoch 0082, iter [03400, 10009], lr: 0.001000, loss: 1.1720
2022-03-07 10:52:53 - train: epoch 0082, iter [03500, 10009], lr: 0.001000, loss: 1.0075
2022-03-07 10:53:13 - train: epoch 0082, iter [03600, 10009], lr: 0.001000, loss: 0.9267
2022-03-07 10:53:33 - train: epoch 0082, iter [03700, 10009], lr: 0.001000, loss: 1.3534
2022-03-07 10:53:53 - train: epoch 0082, iter [03800, 10009], lr: 0.001000, loss: 1.3439
2022-03-07 10:54:12 - train: epoch 0082, iter [03900, 10009], lr: 0.001000, loss: 1.3022
2022-03-07 10:54:32 - train: epoch 0082, iter [04000, 10009], lr: 0.001000, loss: 1.0907
2022-03-07 10:54:52 - train: epoch 0082, iter [04100, 10009], lr: 0.001000, loss: 0.9213
2022-03-07 10:55:12 - train: epoch 0082, iter [04200, 10009], lr: 0.001000, loss: 1.5185
2022-03-07 10:55:32 - train: epoch 0082, iter [04300, 10009], lr: 0.001000, loss: 1.0938
