2022-07-10 15:32:52 - network: RegNetX_1_6GF
2022-07-10 15:32:52 - num_classes: 1000
2022-07-10 15:32:52 - input_image_size: 224
2022-07-10 15:32:52 - scale: 1.1428571428571428
2022-07-10 15:32:52 - trained_model_path: 
2022-07-10 15:32:52 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-10 15:32:52 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-10 15:32:52 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f742d1f4550>
2022-07-10 15:32:52 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f742d1f4700>
2022-07-10 15:32:52 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f742d1f4730>
2022-07-10 15:32:52 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f742d1f4790>
2022-07-10 15:32:52 - seed: 0
2022-07-10 15:32:52 - batch_size: 256
2022-07-10 15:32:52 - num_workers: 16
2022-07-10 15:32:52 - optimizer: ('SGD', {'lr': 0.2, 'momentum': 0.9, 'global_weight_decay': False, 'nesterov': True, 'weight_decay': 5e-05, 'no_weight_decay_layer_name_list': []})
2022-07-10 15:32:52 - scheduler: ('CosineLR', {'warm_up_epochs': 5})
2022-07-10 15:32:52 - epochs: 100
2022-07-10 15:32:52 - print_interval: 100
2022-07-10 15:32:52 - accumulation_steps: 1
2022-07-10 15:32:52 - sync_bn: False
2022-07-10 15:32:52 - apex: True
2022-07-10 15:32:52 - use_ema_model: False
2022-07-10 15:32:52 - ema_model_decay: 0.9999
2022-07-10 15:32:52 - gpus_type: NVIDIA RTX A5000
2022-07-10 15:32:52 - gpus_num: 2
2022-07-10 15:32:52 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f740ef281f0>
2022-07-10 15:32:52 - --------------------parameters--------------------
2022-07-10 15:32:52 - name: conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer1.0.downsample_layer.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer1.0.downsample_layer.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer1.0.downsample_layer.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.0.downsample_layer.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.0.downsample_layer.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.0.downsample_layer.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.0.downsample_layer.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.0.downsample_layer.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.0.downsample_layer.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.6.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.6.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.6.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.6.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.6.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.6.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.6.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.6.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.6.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.7.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.7.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.7.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.7.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.7.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.7.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.7.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.7.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.7.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.8.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.8.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.8.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.8.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.8.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.8.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.8.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.8.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.8.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.9.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.9.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.9.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.9.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.9.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.9.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer3.9.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer3.9.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer3.9.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer4.0.downsample_layer.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer4.0.downsample_layer.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer4.0.downsample_layer.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-07-10 15:32:52 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-07-10 15:32:52 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-07-10 15:32:52 - name: fc.weight, grad: True
2022-07-10 15:32:52 - name: fc.bias, grad: True
2022-07-10 15:32:52 - --------------------buffers--------------------
2022-07-10 15:32:52 - name: conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer1.0.downsample_layer.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer1.0.downsample_layer.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer1.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.0.downsample_layer.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.0.downsample_layer.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.0.downsample_layer.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.0.downsample_layer.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.6.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.6.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.6.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.6.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.6.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.6.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.6.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.6.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.6.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.7.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.7.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.7.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.7.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.7.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.7.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.7.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.7.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.7.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.8.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.8.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.8.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.8.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.8.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.8.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.8.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.8.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.8.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.9.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.9.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.9.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.9.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.9.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.9.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer3.9.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer3.9.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer3.9.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer4.0.downsample_layer.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer4.0.downsample_layer.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer4.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-07-10 15:32:52 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-07-10 15:32:52 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-10 15:32:52 - -----------no weight decay layers--------------
2022-07-10 15:32:52 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.7.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.7.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.7.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.7.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.7.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.7.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.8.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.8.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.8.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.8.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.8.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.8.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.9.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.9.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.9.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.9.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.9.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.9.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-10 15:32:52 - -------------weight decay layers---------------
2022-07-10 15:32:52 - name: conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer1.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.2.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.2.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.2.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.3.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.3.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer2.3.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.2.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.2.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.2.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.3.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.3.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.3.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.4.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.4.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.4.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.5.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.5.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.5.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.6.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.6.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.6.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.7.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.7.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.7.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.8.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.8.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.8.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.9.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.9.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer3.9.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: layer4.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - name: fc.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-10 15:32:52 - epoch 001 lr: 0.200000
2022-07-10 15:34:06 - train: epoch 0001, iter [00100, 05004], lr: 0.040799, loss: 6.9324
2022-07-10 15:35:08 - train: epoch 0001, iter [00200, 05004], lr: 0.041599, loss: 6.7101
2022-07-10 15:36:10 - train: epoch 0001, iter [00300, 05004], lr: 0.042398, loss: 6.5882
2022-07-10 15:37:14 - train: epoch 0001, iter [00400, 05004], lr: 0.043197, loss: 6.4732
2022-07-10 15:38:17 - train: epoch 0001, iter [00500, 05004], lr: 0.043997, loss: 6.3785
2022-07-10 15:39:20 - train: epoch 0001, iter [00600, 05004], lr: 0.044796, loss: 6.0893
2022-07-10 15:40:22 - train: epoch 0001, iter [00700, 05004], lr: 0.045596, loss: 6.1601
2022-07-10 15:41:25 - train: epoch 0001, iter [00800, 05004], lr: 0.046395, loss: 6.0426
2022-07-10 15:42:29 - train: epoch 0001, iter [00900, 05004], lr: 0.047194, loss: 5.9141
2022-07-10 15:43:33 - train: epoch 0001, iter [01000, 05004], lr: 0.047994, loss: 5.7929
2022-07-10 15:44:36 - train: epoch 0001, iter [01100, 05004], lr: 0.048793, loss: 5.9309
2022-07-10 15:45:40 - train: epoch 0001, iter [01200, 05004], lr: 0.049592, loss: 5.6737
2022-07-10 15:46:44 - train: epoch 0001, iter [01300, 05004], lr: 0.050392, loss: 5.5055
2022-07-10 15:47:48 - train: epoch 0001, iter [01400, 05004], lr: 0.051191, loss: 5.5045
2022-07-10 15:48:51 - train: epoch 0001, iter [01500, 05004], lr: 0.051990, loss: 5.3886
2022-07-10 15:49:55 - train: epoch 0001, iter [01600, 05004], lr: 0.052790, loss: 5.5366
2022-07-10 15:50:59 - train: epoch 0001, iter [01700, 05004], lr: 0.053589, loss: 5.1719
2022-07-10 15:52:03 - train: epoch 0001, iter [01800, 05004], lr: 0.054388, loss: 5.2771
2022-07-10 15:53:06 - train: epoch 0001, iter [01900, 05004], lr: 0.055188, loss: 5.1709
2022-07-10 15:54:08 - train: epoch 0001, iter [02000, 05004], lr: 0.055987, loss: 5.1133
2022-07-10 15:55:11 - train: epoch 0001, iter [02100, 05004], lr: 0.056787, loss: 5.0584
2022-07-10 15:56:13 - train: epoch 0001, iter [02200, 05004], lr: 0.057586, loss: 5.0609
2022-07-10 15:57:17 - train: epoch 0001, iter [02300, 05004], lr: 0.058385, loss: 4.9131
2022-07-10 15:58:20 - train: epoch 0001, iter [02400, 05004], lr: 0.059185, loss: 4.9443
2022-07-10 15:59:23 - train: epoch 0001, iter [02500, 05004], lr: 0.059984, loss: 4.9432
2022-07-10 16:00:27 - train: epoch 0001, iter [02600, 05004], lr: 0.060783, loss: 4.9491
2022-07-10 16:01:32 - train: epoch 0001, iter [02700, 05004], lr: 0.061583, loss: 4.7962
2022-07-10 16:02:34 - train: epoch 0001, iter [02800, 05004], lr: 0.062382, loss: 4.6265
2022-07-10 16:03:39 - train: epoch 0001, iter [02900, 05004], lr: 0.063181, loss: 4.5514
2022-07-10 16:04:43 - train: epoch 0001, iter [03000, 05004], lr: 0.063981, loss: 4.8982
2022-07-10 16:05:46 - train: epoch 0001, iter [03100, 05004], lr: 0.064780, loss: 4.7235
2022-07-10 16:06:50 - train: epoch 0001, iter [03200, 05004], lr: 0.065580, loss: 4.7187
2022-07-10 16:07:52 - train: epoch 0001, iter [03300, 05004], lr: 0.066379, loss: 4.3487
2022-07-10 16:08:54 - train: epoch 0001, iter [03400, 05004], lr: 0.067178, loss: 4.4064
2022-07-10 16:09:55 - train: epoch 0001, iter [03500, 05004], lr: 0.067978, loss: 4.6022
2022-07-10 16:10:58 - train: epoch 0001, iter [03600, 05004], lr: 0.068777, loss: 4.3692
2022-07-10 16:12:01 - train: epoch 0001, iter [03700, 05004], lr: 0.069576, loss: 4.5321
2022-07-10 16:13:04 - train: epoch 0001, iter [03800, 05004], lr: 0.070376, loss: 4.2817
2022-07-10 16:14:08 - train: epoch 0001, iter [03900, 05004], lr: 0.071175, loss: 4.3598
2022-07-10 16:15:09 - train: epoch 0001, iter [04000, 05004], lr: 0.071974, loss: 4.3133
2022-07-10 16:16:12 - train: epoch 0001, iter [04100, 05004], lr: 0.072774, loss: 4.3112
2022-07-10 16:17:17 - train: epoch 0001, iter [04200, 05004], lr: 0.073573, loss: 4.1756
2022-07-10 16:18:20 - train: epoch 0001, iter [04300, 05004], lr: 0.074373, loss: 4.2662
2022-07-10 16:19:23 - train: epoch 0001, iter [04400, 05004], lr: 0.075172, loss: 3.9884
2022-07-10 16:20:25 - train: epoch 0001, iter [04500, 05004], lr: 0.075971, loss: 4.2852
2022-07-10 16:21:28 - train: epoch 0001, iter [04600, 05004], lr: 0.076771, loss: 4.5179
2022-07-10 16:22:32 - train: epoch 0001, iter [04700, 05004], lr: 0.077570, loss: 4.0405
2022-07-10 16:23:33 - train: epoch 0001, iter [04800, 05004], lr: 0.078369, loss: 4.1901
2022-07-10 16:24:33 - train: epoch 0001, iter [04900, 05004], lr: 0.079169, loss: 4.1932
2022-07-10 16:25:33 - train: epoch 0001, iter [05000, 05004], lr: 0.079968, loss: 3.8481
2022-07-10 16:25:35 - train: epoch 001, train_loss: 5.0109
2022-07-10 16:27:48 - eval: epoch: 001, acc1: 22.516%, acc5: 46.388%, test_loss: 3.7887, per_image_load_time: 1.379ms, per_image_inference_time: 0.811ms
2022-07-10 16:27:49 - until epoch: 001, best_acc1: 22.516%
2022-07-10 16:27:49 - epoch 002 lr: 0.080008
2022-07-10 16:29:01 - train: epoch 0002, iter [00100, 05004], lr: 0.080799, loss: 3.9112
2022-07-10 16:30:04 - train: epoch 0002, iter [00200, 05004], lr: 0.081599, loss: 3.7023
2022-07-10 16:31:03 - train: epoch 0002, iter [00300, 05004], lr: 0.082398, loss: 3.9750
2022-07-10 16:32:04 - train: epoch 0002, iter [00400, 05004], lr: 0.083197, loss: 3.9785
2022-07-10 16:33:03 - train: epoch 0002, iter [00500, 05004], lr: 0.083997, loss: 3.8672
2022-07-10 16:34:04 - train: epoch 0002, iter [00600, 05004], lr: 0.084796, loss: 3.6722
2022-07-10 16:35:04 - train: epoch 0002, iter [00700, 05004], lr: 0.085596, loss: 3.9730
2022-07-10 16:36:04 - train: epoch 0002, iter [00800, 05004], lr: 0.086395, loss: 3.6739
2022-07-10 16:37:03 - train: epoch 0002, iter [00900, 05004], lr: 0.087194, loss: 3.6268
2022-07-10 16:38:05 - train: epoch 0002, iter [01000, 05004], lr: 0.087994, loss: 3.8479
2022-07-10 16:39:07 - train: epoch 0002, iter [01100, 05004], lr: 0.088793, loss: 3.6536
2022-07-10 16:40:09 - train: epoch 0002, iter [01200, 05004], lr: 0.089592, loss: 3.7037
2022-07-10 16:41:11 - train: epoch 0002, iter [01300, 05004], lr: 0.090392, loss: 3.7745
2022-07-10 16:42:14 - train: epoch 0002, iter [01400, 05004], lr: 0.091191, loss: 3.9393
2022-07-10 16:43:16 - train: epoch 0002, iter [01500, 05004], lr: 0.091990, loss: 3.7938
2022-07-10 16:44:19 - train: epoch 0002, iter [01600, 05004], lr: 0.092790, loss: 3.5540
2022-07-10 16:45:20 - train: epoch 0002, iter [01700, 05004], lr: 0.093589, loss: 3.7274
2022-07-10 16:46:23 - train: epoch 0002, iter [01800, 05004], lr: 0.094388, loss: 3.6376
2022-07-10 16:47:25 - train: epoch 0002, iter [01900, 05004], lr: 0.095188, loss: 3.3800
2022-07-10 16:48:30 - train: epoch 0002, iter [02000, 05004], lr: 0.095987, loss: 3.3541
2022-07-10 16:49:32 - train: epoch 0002, iter [02100, 05004], lr: 0.096787, loss: 3.6937
2022-07-10 16:50:34 - train: epoch 0002, iter [02200, 05004], lr: 0.097586, loss: 3.5300
2022-07-10 16:51:38 - train: epoch 0002, iter [02300, 05004], lr: 0.098385, loss: 3.5373
2022-07-10 16:52:41 - train: epoch 0002, iter [02400, 05004], lr: 0.099185, loss: 3.4754
2022-07-10 16:53:43 - train: epoch 0002, iter [02500, 05004], lr: 0.099984, loss: 3.4305
2022-07-10 16:54:45 - train: epoch 0002, iter [02600, 05004], lr: 0.100783, loss: 3.4733
2022-07-10 16:55:49 - train: epoch 0002, iter [02700, 05004], lr: 0.101583, loss: 3.6235
2022-07-10 16:56:52 - train: epoch 0002, iter [02800, 05004], lr: 0.102382, loss: 3.5932
2022-07-10 16:57:58 - train: epoch 0002, iter [02900, 05004], lr: 0.103181, loss: 3.3515
2022-07-10 16:59:01 - train: epoch 0002, iter [03000, 05004], lr: 0.103981, loss: 3.3993
2022-07-10 17:00:07 - train: epoch 0002, iter [03100, 05004], lr: 0.104780, loss: 3.3290
2022-07-10 17:01:11 - train: epoch 0002, iter [03200, 05004], lr: 0.105580, loss: 3.3344
2022-07-10 17:02:15 - train: epoch 0002, iter [03300, 05004], lr: 0.106379, loss: 3.3612
2022-07-10 17:03:16 - train: epoch 0002, iter [03400, 05004], lr: 0.107178, loss: 3.3756
2022-07-10 17:04:15 - train: epoch 0002, iter [03500, 05004], lr: 0.107978, loss: 3.4364
2022-07-10 17:05:18 - train: epoch 0002, iter [03600, 05004], lr: 0.108777, loss: 3.5522
2022-07-10 17:06:23 - train: epoch 0002, iter [03700, 05004], lr: 0.109576, loss: 3.5385
2022-07-10 17:07:26 - train: epoch 0002, iter [03800, 05004], lr: 0.110376, loss: 3.0983
2022-07-10 17:08:31 - train: epoch 0002, iter [03900, 05004], lr: 0.111175, loss: 3.2447
2022-07-10 17:09:35 - train: epoch 0002, iter [04000, 05004], lr: 0.111974, loss: 3.3010
2022-07-10 17:10:39 - train: epoch 0002, iter [04100, 05004], lr: 0.112774, loss: 3.3858
2022-07-10 17:11:41 - train: epoch 0002, iter [04200, 05004], lr: 0.113573, loss: 3.2228
2022-07-10 17:12:44 - train: epoch 0002, iter [04300, 05004], lr: 0.114373, loss: 3.1573
2022-07-10 17:13:47 - train: epoch 0002, iter [04400, 05004], lr: 0.115172, loss: 2.9708
2022-07-10 17:14:52 - train: epoch 0002, iter [04500, 05004], lr: 0.115971, loss: 3.1625
2022-07-10 17:15:57 - train: epoch 0002, iter [04600, 05004], lr: 0.116771, loss: 3.1131
2022-07-10 17:16:58 - train: epoch 0002, iter [04700, 05004], lr: 0.117570, loss: 3.2327
2022-07-10 17:18:01 - train: epoch 0002, iter [04800, 05004], lr: 0.118369, loss: 3.2745
2022-07-10 17:19:05 - train: epoch 0002, iter [04900, 05004], lr: 0.119169, loss: 3.1718
2022-07-10 17:20:06 - train: epoch 0002, iter [05000, 05004], lr: 0.119968, loss: 3.3172
2022-07-10 17:20:09 - train: epoch 002, train_loss: 3.4931
2022-07-10 17:22:35 - eval: epoch: 002, acc1: 35.206%, acc5: 61.856%, test_loss: 2.9897, per_image_load_time: 3.003ms, per_image_inference_time: 0.827ms
2022-07-10 17:22:35 - until epoch: 002, best_acc1: 35.206%
2022-07-10 17:22:35 - epoch 003 lr: 0.120008
2022-07-10 17:23:53 - train: epoch 0003, iter [00100, 05004], lr: 0.120799, loss: 3.0317
2022-07-10 17:24:59 - train: epoch 0003, iter [00200, 05004], lr: 0.121599, loss: 3.1470
2022-07-10 17:26:02 - train: epoch 0003, iter [00300, 05004], lr: 0.122398, loss: 3.1315
2022-07-10 17:27:08 - train: epoch 0003, iter [00400, 05004], lr: 0.123197, loss: 3.0336
2022-07-10 17:28:13 - train: epoch 0003, iter [00500, 05004], lr: 0.123997, loss: 3.1264
2022-07-10 17:29:17 - train: epoch 0003, iter [00600, 05004], lr: 0.124796, loss: 2.9354
2022-07-10 17:30:21 - train: epoch 0003, iter [00700, 05004], lr: 0.125596, loss: 3.1704
2022-07-10 17:31:23 - train: epoch 0003, iter [00800, 05004], lr: 0.126395, loss: 3.2016
2022-07-10 17:32:28 - train: epoch 0003, iter [00900, 05004], lr: 0.127194, loss: 3.1435
2022-07-10 17:33:32 - train: epoch 0003, iter [01000, 05004], lr: 0.127994, loss: 3.0850
2022-07-10 17:34:36 - train: epoch 0003, iter [01100, 05004], lr: 0.128793, loss: 2.9710
2022-07-10 17:35:37 - train: epoch 0003, iter [01200, 05004], lr: 0.129592, loss: 2.9381
2022-07-10 17:36:38 - train: epoch 0003, iter [01300, 05004], lr: 0.130392, loss: 3.0002
2022-07-10 17:37:40 - train: epoch 0003, iter [01400, 05004], lr: 0.131191, loss: 2.8907
2022-07-10 17:38:40 - train: epoch 0003, iter [01500, 05004], lr: 0.131990, loss: 3.2970
2022-07-10 17:39:40 - train: epoch 0003, iter [01600, 05004], lr: 0.132790, loss: 3.0124
2022-07-10 17:40:42 - train: epoch 0003, iter [01700, 05004], lr: 0.133589, loss: 2.9707
2022-07-10 17:41:44 - train: epoch 0003, iter [01800, 05004], lr: 0.134388, loss: 3.0576
2022-07-10 17:42:45 - train: epoch 0003, iter [01900, 05004], lr: 0.135188, loss: 3.2918
2022-07-10 17:43:45 - train: epoch 0003, iter [02000, 05004], lr: 0.135987, loss: 2.9934
2022-07-10 17:44:43 - train: epoch 0003, iter [02100, 05004], lr: 0.136787, loss: 3.0010
2022-07-10 17:45:42 - train: epoch 0003, iter [02200, 05004], lr: 0.137586, loss: 3.3269
2022-07-10 17:46:42 - train: epoch 0003, iter [02300, 05004], lr: 0.138385, loss: 2.7605
2022-07-10 17:47:44 - train: epoch 0003, iter [02400, 05004], lr: 0.139185, loss: 2.8715
2022-07-10 17:48:44 - train: epoch 0003, iter [02500, 05004], lr: 0.139984, loss: 3.0627
2022-07-10 17:49:44 - train: epoch 0003, iter [02600, 05004], lr: 0.140783, loss: 3.1076
2022-07-10 17:50:44 - train: epoch 0003, iter [02700, 05004], lr: 0.141583, loss: 3.2261
2022-07-10 17:51:45 - train: epoch 0003, iter [02800, 05004], lr: 0.142382, loss: 2.8668
2022-07-10 17:52:46 - train: epoch 0003, iter [02900, 05004], lr: 0.143181, loss: 2.9056
2022-07-10 17:53:47 - train: epoch 0003, iter [03000, 05004], lr: 0.143981, loss: 2.9312
2022-07-10 17:54:48 - train: epoch 0003, iter [03100, 05004], lr: 0.144780, loss: 3.0758
2022-07-10 17:55:50 - train: epoch 0003, iter [03200, 05004], lr: 0.145580, loss: 2.9300
2022-07-10 17:56:53 - train: epoch 0003, iter [03300, 05004], lr: 0.146379, loss: 2.8476
2022-07-10 17:57:53 - train: epoch 0003, iter [03400, 05004], lr: 0.147178, loss: 3.1211
2022-07-10 17:58:54 - train: epoch 0003, iter [03500, 05004], lr: 0.147978, loss: 2.8056
2022-07-10 17:59:55 - train: epoch 0003, iter [03600, 05004], lr: 0.148777, loss: 2.9303
2022-07-10 18:00:57 - train: epoch 0003, iter [03700, 05004], lr: 0.149576, loss: 2.9375
2022-07-10 18:01:59 - train: epoch 0003, iter [03800, 05004], lr: 0.150376, loss: 3.0512
2022-07-10 18:03:02 - train: epoch 0003, iter [03900, 05004], lr: 0.151175, loss: 3.2062
2022-07-10 18:04:02 - train: epoch 0003, iter [04000, 05004], lr: 0.151974, loss: 2.7611
2022-07-10 18:05:05 - train: epoch 0003, iter [04100, 05004], lr: 0.152774, loss: 2.8771
2022-07-10 18:06:07 - train: epoch 0003, iter [04200, 05004], lr: 0.153573, loss: 2.9278
2022-07-10 18:07:12 - train: epoch 0003, iter [04300, 05004], lr: 0.154373, loss: 2.7740
2022-07-10 18:08:14 - train: epoch 0003, iter [04400, 05004], lr: 0.155172, loss: 2.7656
2022-07-10 18:09:17 - train: epoch 0003, iter [04500, 05004], lr: 0.155971, loss: 2.9617
2022-07-10 18:10:20 - train: epoch 0003, iter [04600, 05004], lr: 0.156771, loss: 2.7791
2022-07-10 18:11:19 - train: epoch 0003, iter [04700, 05004], lr: 0.157570, loss: 2.7552
2022-07-10 18:12:20 - train: epoch 0003, iter [04800, 05004], lr: 0.158369, loss: 2.9270
2022-07-10 18:13:22 - train: epoch 0003, iter [04900, 05004], lr: 0.159169, loss: 3.0391
2022-07-10 18:14:22 - train: epoch 0003, iter [05000, 05004], lr: 0.159968, loss: 2.8595
2022-07-10 18:14:24 - train: epoch 003, train_loss: 2.9846
2022-07-10 18:16:43 - eval: epoch: 003, acc1: 41.450%, acc5: 67.916%, test_loss: 2.6269, per_image_load_time: 4.325ms, per_image_inference_time: 0.857ms
2022-07-10 18:16:44 - until epoch: 003, best_acc1: 41.450%
2022-07-10 18:16:44 - epoch 004 lr: 0.160008
2022-07-10 18:18:00 - train: epoch 0004, iter [00100, 05004], lr: 0.160799, loss: 2.9254
2022-07-10 18:19:01 - train: epoch 0004, iter [00200, 05004], lr: 0.161599, loss: 2.7184
2022-07-10 18:20:06 - train: epoch 0004, iter [00300, 05004], lr: 0.162398, loss: 2.8826
2022-07-10 18:21:09 - train: epoch 0004, iter [00400, 05004], lr: 0.163197, loss: 2.7255
2022-07-10 18:22:12 - train: epoch 0004, iter [00500, 05004], lr: 0.163997, loss: 2.7260
2022-07-10 18:23:15 - train: epoch 0004, iter [00600, 05004], lr: 0.164796, loss: 3.0772
2022-07-10 18:24:14 - train: epoch 0004, iter [00700, 05004], lr: 0.165596, loss: 2.8077
2022-07-10 18:25:09 - train: epoch 0004, iter [00800, 05004], lr: 0.166395, loss: 2.7413
2022-07-10 18:26:07 - train: epoch 0004, iter [00900, 05004], lr: 0.167194, loss: 2.6342
2022-07-10 18:27:10 - train: epoch 0004, iter [01000, 05004], lr: 0.167994, loss: 2.7426
2022-07-10 18:28:11 - train: epoch 0004, iter [01100, 05004], lr: 0.168793, loss: 3.0148
2022-07-10 18:29:14 - train: epoch 0004, iter [01200, 05004], lr: 0.169592, loss: 2.7773
2022-07-10 18:30:16 - train: epoch 0004, iter [01300, 05004], lr: 0.170392, loss: 2.5494
2022-07-10 18:31:16 - train: epoch 0004, iter [01400, 05004], lr: 0.171191, loss: 2.8607
2022-07-10 18:32:20 - train: epoch 0004, iter [01500, 05004], lr: 0.171990, loss: 2.8111
2022-07-10 18:33:22 - train: epoch 0004, iter [01600, 05004], lr: 0.172790, loss: 2.6795
2022-07-10 18:34:24 - train: epoch 0004, iter [01700, 05004], lr: 0.173589, loss: 2.7013
2022-07-10 18:35:27 - train: epoch 0004, iter [01800, 05004], lr: 0.174388, loss: 2.8851
2022-07-10 18:36:31 - train: epoch 0004, iter [01900, 05004], lr: 0.175188, loss: 2.8865
2022-07-10 18:37:30 - train: epoch 0004, iter [02000, 05004], lr: 0.175987, loss: 2.8841
2022-07-10 18:38:30 - train: epoch 0004, iter [02100, 05004], lr: 0.176787, loss: 2.8476
2022-07-10 18:39:32 - train: epoch 0004, iter [02200, 05004], lr: 0.177586, loss: 2.7081
2022-07-10 18:40:35 - train: epoch 0004, iter [02300, 05004], lr: 0.178385, loss: 2.4543
2022-07-10 18:41:38 - train: epoch 0004, iter [02400, 05004], lr: 0.179185, loss: 2.7278
2022-07-10 18:42:41 - train: epoch 0004, iter [02500, 05004], lr: 0.179984, loss: 2.7330
2022-07-10 18:43:42 - train: epoch 0004, iter [02600, 05004], lr: 0.180783, loss: 2.8029
2022-07-10 18:44:43 - train: epoch 0004, iter [02700, 05004], lr: 0.181583, loss: 2.6898
2022-07-10 18:45:46 - train: epoch 0004, iter [02800, 05004], lr: 0.182382, loss: 2.7823
2022-07-10 18:46:49 - train: epoch 0004, iter [02900, 05004], lr: 0.183181, loss: 2.6247
2022-07-10 18:47:52 - train: epoch 0004, iter [03000, 05004], lr: 0.183981, loss: 2.7303
2022-07-10 18:48:55 - train: epoch 0004, iter [03100, 05004], lr: 0.184780, loss: 2.7255
2022-07-10 18:49:57 - train: epoch 0004, iter [03200, 05004], lr: 0.185580, loss: 2.9380
2022-07-10 18:50:57 - train: epoch 0004, iter [03300, 05004], lr: 0.186379, loss: 2.6703
2022-07-10 18:51:58 - train: epoch 0004, iter [03400, 05004], lr: 0.187178, loss: 2.6796
2022-07-10 18:53:01 - train: epoch 0004, iter [03500, 05004], lr: 0.187978, loss: 2.7311
2022-07-10 18:54:04 - train: epoch 0004, iter [03600, 05004], lr: 0.188777, loss: 2.5677
2022-07-10 18:55:09 - train: epoch 0004, iter [03700, 05004], lr: 0.189576, loss: 2.6407
2022-07-10 18:56:12 - train: epoch 0004, iter [03800, 05004], lr: 0.190376, loss: 2.5553
2022-07-10 18:57:15 - train: epoch 0004, iter [03900, 05004], lr: 0.191175, loss: 2.5694
2022-07-10 18:58:17 - train: epoch 0004, iter [04000, 05004], lr: 0.191974, loss: 2.5411
2022-07-10 18:59:20 - train: epoch 0004, iter [04100, 05004], lr: 0.192774, loss: 2.5839
2022-07-10 19:00:22 - train: epoch 0004, iter [04200, 05004], lr: 0.193573, loss: 2.7364
2022-07-10 19:01:23 - train: epoch 0004, iter [04300, 05004], lr: 0.194373, loss: 2.6364
2022-07-10 19:02:24 - train: epoch 0004, iter [04400, 05004], lr: 0.195172, loss: 2.6361
2022-07-10 19:03:27 - train: epoch 0004, iter [04500, 05004], lr: 0.195971, loss: 2.2766
2022-07-10 19:04:26 - train: epoch 0004, iter [04600, 05004], lr: 0.196771, loss: 2.5565
2022-07-10 19:05:28 - train: epoch 0004, iter [04700, 05004], lr: 0.197570, loss: 2.5410
2022-07-10 19:06:31 - train: epoch 0004, iter [04800, 05004], lr: 0.198369, loss: 2.6028
2022-07-10 19:07:34 - train: epoch 0004, iter [04900, 05004], lr: 0.199169, loss: 2.6479
2022-07-10 19:08:34 - train: epoch 0004, iter [05000, 05004], lr: 0.199968, loss: 2.6727
2022-07-10 19:08:36 - train: epoch 004, train_loss: 2.7474
2022-07-10 19:10:55 - eval: epoch: 004, acc1: 45.284%, acc5: 72.164%, test_loss: 2.3991, per_image_load_time: 4.518ms, per_image_inference_time: 0.833ms
2022-07-10 19:10:56 - until epoch: 004, best_acc1: 45.284%
2022-07-10 19:10:56 - epoch 005 lr: 0.200008
2022-07-10 19:12:13 - train: epoch 0005, iter [00100, 05004], lr: 0.200799, loss: 2.5608
2022-07-10 19:13:17 - train: epoch 0005, iter [00200, 05004], lr: 0.201599, loss: 2.6820
2022-07-10 19:14:19 - train: epoch 0005, iter [00300, 05004], lr: 0.202398, loss: 2.8423
2022-07-10 19:15:22 - train: epoch 0005, iter [00400, 05004], lr: 0.203197, loss: 2.6198
2022-07-10 19:16:23 - train: epoch 0005, iter [00500, 05004], lr: 0.203997, loss: 2.4842
2022-07-10 19:17:23 - train: epoch 0005, iter [00600, 05004], lr: 0.204796, loss: 2.6625
2022-07-10 19:18:25 - train: epoch 0005, iter [00700, 05004], lr: 0.205596, loss: 2.4539
2022-07-10 19:19:28 - train: epoch 0005, iter [00800, 05004], lr: 0.206395, loss: 2.8404
2022-07-10 19:20:30 - train: epoch 0005, iter [00900, 05004], lr: 0.207194, loss: 2.6900
2022-07-10 19:21:32 - train: epoch 0005, iter [01000, 05004], lr: 0.207994, loss: 2.4942
2022-07-10 19:22:34 - train: epoch 0005, iter [01100, 05004], lr: 0.208793, loss: 2.7347
2022-07-10 19:23:37 - train: epoch 0005, iter [01200, 05004], lr: 0.209592, loss: 2.8913
2022-07-10 19:24:39 - train: epoch 0005, iter [01300, 05004], lr: 0.210392, loss: 2.5794
2022-07-10 19:25:41 - train: epoch 0005, iter [01400, 05004], lr: 0.211191, loss: 2.6941
2022-07-10 19:26:44 - train: epoch 0005, iter [01500, 05004], lr: 0.211990, loss: 2.4195
2022-07-10 19:27:47 - train: epoch 0005, iter [01600, 05004], lr: 0.212790, loss: 2.4185
2022-07-10 19:28:50 - train: epoch 0005, iter [01700, 05004], lr: 0.213589, loss: 2.5044
2022-07-10 19:29:52 - train: epoch 0005, iter [01800, 05004], lr: 0.214388, loss: 2.6279
2022-07-10 19:30:54 - train: epoch 0005, iter [01900, 05004], lr: 0.215188, loss: 2.5319
2022-07-10 19:31:55 - train: epoch 0005, iter [02000, 05004], lr: 0.215987, loss: 2.5504
2022-07-10 19:32:57 - train: epoch 0005, iter [02100, 05004], lr: 0.216787, loss: 2.4806
2022-07-10 19:34:00 - train: epoch 0005, iter [02200, 05004], lr: 0.217586, loss: 2.7845
2022-07-10 19:35:03 - train: epoch 0005, iter [02300, 05004], lr: 0.218385, loss: 2.5139
2022-07-10 19:36:06 - train: epoch 0005, iter [02400, 05004], lr: 0.219185, loss: 2.7133
2022-07-10 19:37:11 - train: epoch 0005, iter [02500, 05004], lr: 0.219984, loss: 2.6997
2022-07-10 19:38:17 - train: epoch 0005, iter [02600, 05004], lr: 0.220783, loss: 2.6464
2022-07-10 19:39:21 - train: epoch 0005, iter [02700, 05004], lr: 0.221583, loss: 2.7168
2022-07-10 19:40:25 - train: epoch 0005, iter [02800, 05004], lr: 0.222382, loss: 2.6089
2022-07-10 19:41:28 - train: epoch 0005, iter [02900, 05004], lr: 0.223181, loss: 2.4059
2022-07-10 19:42:33 - train: epoch 0005, iter [03000, 05004], lr: 0.223981, loss: 2.5989
2022-07-10 19:43:37 - train: epoch 0005, iter [03100, 05004], lr: 0.224780, loss: 2.6823
2022-07-10 19:44:40 - train: epoch 0005, iter [03200, 05004], lr: 0.225580, loss: 2.5132
2022-07-10 19:45:42 - train: epoch 0005, iter [03300, 05004], lr: 0.226379, loss: 2.6705
2022-07-10 19:46:44 - train: epoch 0005, iter [03400, 05004], lr: 0.227178, loss: 2.6108
2022-07-10 19:47:45 - train: epoch 0005, iter [03500, 05004], lr: 0.227978, loss: 2.5960
2022-07-10 19:48:47 - train: epoch 0005, iter [03600, 05004], lr: 0.228777, loss: 2.5633
2022-07-10 19:49:50 - train: epoch 0005, iter [03700, 05004], lr: 0.229576, loss: 2.5351
2022-07-10 19:50:52 - train: epoch 0005, iter [03800, 05004], lr: 0.230376, loss: 2.6237
2022-07-10 19:51:54 - train: epoch 0005, iter [03900, 05004], lr: 0.231175, loss: 2.6635
2022-07-10 19:52:56 - train: epoch 0005, iter [04000, 05004], lr: 0.231974, loss: 2.5752
2022-07-10 19:53:57 - train: epoch 0005, iter [04100, 05004], lr: 0.232774, loss: 2.5474
2022-07-10 19:54:59 - train: epoch 0005, iter [04200, 05004], lr: 0.233573, loss: 2.7990
2022-07-10 19:56:02 - train: epoch 0005, iter [04300, 05004], lr: 0.234373, loss: 2.5509
2022-07-10 19:57:04 - train: epoch 0005, iter [04400, 05004], lr: 0.235172, loss: 2.6000
2022-07-10 19:58:06 - train: epoch 0005, iter [04500, 05004], lr: 0.235971, loss: 2.6910
2022-07-10 19:59:07 - train: epoch 0005, iter [04600, 05004], lr: 0.236771, loss: 2.6730
2022-07-10 20:00:08 - train: epoch 0005, iter [04700, 05004], lr: 0.237570, loss: 2.5376
2022-07-10 20:01:11 - train: epoch 0005, iter [04800, 05004], lr: 0.238369, loss: 2.5589
2022-07-10 20:02:13 - train: epoch 0005, iter [04900, 05004], lr: 0.239169, loss: 2.6344
2022-07-10 20:03:12 - train: epoch 0005, iter [05000, 05004], lr: 0.239968, loss: 2.4784
2022-07-10 20:03:14 - train: epoch 005, train_loss: 2.6192
2022-07-10 20:05:33 - eval: epoch: 005, acc1: 46.128%, acc5: 72.712%, test_loss: 2.3611, per_image_load_time: 3.550ms, per_image_inference_time: 0.844ms
2022-07-10 20:05:33 - until epoch: 005, best_acc1: 46.128%
2022-07-10 20:45:14 - epoch 006 lr: 0.200000
2022-07-10 20:46:31 - train: epoch 0006, iter [00100, 05004], lr: 0.200000, loss: 2.3711
2022-07-10 20:47:38 - train: epoch 0006, iter [00200, 05004], lr: 0.200000, loss: 2.4184
2022-07-10 20:48:43 - train: epoch 0006, iter [00300, 05004], lr: 0.200000, loss: 2.4954
2022-07-10 20:49:47 - train: epoch 0006, iter [00400, 05004], lr: 0.200000, loss: 2.4132
2022-07-10 20:50:50 - train: epoch 0006, iter [00500, 05004], lr: 0.199999, loss: 2.4025
2022-07-10 20:51:53 - train: epoch 0006, iter [00600, 05004], lr: 0.199999, loss: 2.4589
2022-07-10 20:52:57 - train: epoch 0006, iter [00700, 05004], lr: 0.199999, loss: 2.7345
2022-07-10 20:54:00 - train: epoch 0006, iter [00800, 05004], lr: 0.199999, loss: 2.3742
2022-07-10 20:55:02 - train: epoch 0006, iter [00900, 05004], lr: 0.199998, loss: 2.2095
2022-07-10 20:56:03 - train: epoch 0006, iter [01000, 05004], lr: 0.199998, loss: 2.4358
2022-07-10 20:57:05 - train: epoch 0006, iter [01100, 05004], lr: 0.199997, loss: 2.5710
2022-07-10 20:58:07 - train: epoch 0006, iter [01200, 05004], lr: 0.199997, loss: 2.4458
2022-07-10 20:59:09 - train: epoch 0006, iter [01300, 05004], lr: 0.199996, loss: 2.4578
2022-07-10 21:00:11 - train: epoch 0006, iter [01400, 05004], lr: 0.199996, loss: 2.4992
2022-07-10 21:01:13 - train: epoch 0006, iter [01500, 05004], lr: 0.199995, loss: 2.5720
2022-07-10 21:02:16 - train: epoch 0006, iter [01600, 05004], lr: 0.199994, loss: 2.2508
2022-07-10 21:03:18 - train: epoch 0006, iter [01700, 05004], lr: 0.199994, loss: 2.6870
2022-07-10 21:04:20 - train: epoch 0006, iter [01800, 05004], lr: 0.199993, loss: 2.3684
2022-07-10 21:05:23 - train: epoch 0006, iter [01900, 05004], lr: 0.199992, loss: 2.2106
2022-07-10 21:06:24 - train: epoch 0006, iter [02000, 05004], lr: 0.199991, loss: 2.6553
2022-07-10 21:07:26 - train: epoch 0006, iter [02100, 05004], lr: 0.199990, loss: 2.4891
2022-07-10 21:08:29 - train: epoch 0006, iter [02200, 05004], lr: 0.199989, loss: 2.0336
2022-07-10 21:09:31 - train: epoch 0006, iter [02300, 05004], lr: 0.199988, loss: 2.4526
2022-07-10 21:10:33 - train: epoch 0006, iter [02400, 05004], lr: 0.199987, loss: 2.3118
2022-07-10 21:11:37 - train: epoch 0006, iter [02500, 05004], lr: 0.199986, loss: 2.4307
2022-07-10 21:12:38 - train: epoch 0006, iter [02600, 05004], lr: 0.199985, loss: 2.3750
2022-07-10 21:13:41 - train: epoch 0006, iter [02700, 05004], lr: 0.199984, loss: 2.5360
2022-07-10 21:14:45 - train: epoch 0006, iter [02800, 05004], lr: 0.199983, loss: 2.0975
2022-07-10 21:15:46 - train: epoch 0006, iter [02900, 05004], lr: 0.199982, loss: 2.4999
2022-07-10 21:16:51 - train: epoch 0006, iter [03000, 05004], lr: 0.199980, loss: 2.2534
2022-07-10 21:17:53 - train: epoch 0006, iter [03100, 05004], lr: 0.199979, loss: 2.4051
2022-07-10 21:18:55 - train: epoch 0006, iter [03200, 05004], lr: 0.199978, loss: 2.4780
2022-07-10 21:19:57 - train: epoch 0006, iter [03300, 05004], lr: 0.199976, loss: 2.5634
2022-07-10 21:20:58 - train: epoch 0006, iter [03400, 05004], lr: 0.199975, loss: 2.5515
2022-07-10 21:21:59 - train: epoch 0006, iter [03500, 05004], lr: 0.199973, loss: 2.4610
2022-07-10 21:23:00 - train: epoch 0006, iter [03600, 05004], lr: 0.199972, loss: 2.4876
2022-07-10 21:24:01 - train: epoch 0006, iter [03700, 05004], lr: 0.199970, loss: 2.5125
2022-07-10 21:25:04 - train: epoch 0006, iter [03800, 05004], lr: 0.199968, loss: 2.4981
2022-07-10 21:26:06 - train: epoch 0006, iter [03900, 05004], lr: 0.199967, loss: 2.3954
2022-07-10 21:27:08 - train: epoch 0006, iter [04000, 05004], lr: 0.199965, loss: 2.5227
2022-07-10 21:28:09 - train: epoch 0006, iter [04100, 05004], lr: 0.199963, loss: 2.4428
2022-07-10 21:29:11 - train: epoch 0006, iter [04200, 05004], lr: 0.199961, loss: 2.1926
2022-07-10 21:30:13 - train: epoch 0006, iter [04300, 05004], lr: 0.199960, loss: 2.3046
2022-07-10 21:31:15 - train: epoch 0006, iter [04400, 05004], lr: 0.199958, loss: 2.5626
2022-07-10 21:32:18 - train: epoch 0006, iter [04500, 05004], lr: 0.199956, loss: 2.4677
2022-07-10 21:33:20 - train: epoch 0006, iter [04600, 05004], lr: 0.199954, loss: 2.7846
2022-07-10 21:34:20 - train: epoch 0006, iter [04700, 05004], lr: 0.199952, loss: 2.3820
2022-07-10 21:35:21 - train: epoch 0006, iter [04800, 05004], lr: 0.199950, loss: 2.5106
2022-07-10 21:36:23 - train: epoch 0006, iter [04900, 05004], lr: 0.199948, loss: 2.6743
2022-07-10 21:37:24 - train: epoch 0006, iter [05000, 05004], lr: 0.199945, loss: 2.3207
2022-07-10 21:37:27 - train: epoch 006, train_loss: 2.4360
2022-07-10 21:39:47 - eval: epoch: 006, acc1: 50.710%, acc5: 76.292%, test_loss: 2.1306, per_image_load_time: 4.477ms, per_image_inference_time: 0.810ms
2022-07-10 21:39:47 - until epoch: 006, best_acc1: 50.710%
2022-07-10 21:39:47 - epoch 007 lr: 0.199945
2022-07-10 21:41:08 - train: epoch 0007, iter [00100, 05004], lr: 0.199943, loss: 2.1361
2022-07-10 21:42:13 - train: epoch 0007, iter [00200, 05004], lr: 0.199941, loss: 2.6238
2022-07-10 21:43:17 - train: epoch 0007, iter [00300, 05004], lr: 0.199939, loss: 2.6409
2022-07-10 21:44:19 - train: epoch 0007, iter [00400, 05004], lr: 0.199936, loss: 2.4046
2022-07-10 21:45:23 - train: epoch 0007, iter [00500, 05004], lr: 0.199934, loss: 2.3069
2022-07-10 21:46:25 - train: epoch 0007, iter [00600, 05004], lr: 0.199931, loss: 2.5788
2022-07-10 21:47:28 - train: epoch 0007, iter [00700, 05004], lr: 0.199929, loss: 2.2429
2022-07-10 21:48:28 - train: epoch 0007, iter [00800, 05004], lr: 0.199926, loss: 2.5614
2022-07-10 21:49:30 - train: epoch 0007, iter [00900, 05004], lr: 0.199924, loss: 2.4735
2022-07-10 21:50:32 - train: epoch 0007, iter [01000, 05004], lr: 0.199921, loss: 2.2771
2022-07-10 21:51:35 - train: epoch 0007, iter [01100, 05004], lr: 0.199919, loss: 2.3239
2022-07-10 21:52:38 - train: epoch 0007, iter [01200, 05004], lr: 0.199916, loss: 2.3326
2022-07-10 21:53:41 - train: epoch 0007, iter [01300, 05004], lr: 0.199913, loss: 2.0910
2022-07-10 21:54:43 - train: epoch 0007, iter [01400, 05004], lr: 0.199910, loss: 2.4739
2022-07-10 21:55:46 - train: epoch 0007, iter [01500, 05004], lr: 0.199908, loss: 2.4722
2022-07-10 21:56:48 - train: epoch 0007, iter [01600, 05004], lr: 0.199905, loss: 2.2346
2022-07-10 21:57:51 - train: epoch 0007, iter [01700, 05004], lr: 0.199902, loss: 2.3427
2022-07-10 21:58:55 - train: epoch 0007, iter [01800, 05004], lr: 0.199899, loss: 2.4389
2022-07-10 21:59:58 - train: epoch 0007, iter [01900, 05004], lr: 0.199896, loss: 2.5989
2022-07-10 22:01:02 - train: epoch 0007, iter [02000, 05004], lr: 0.199893, loss: 2.2319
2022-07-10 22:02:04 - train: epoch 0007, iter [02100, 05004], lr: 0.199890, loss: 2.4716
2022-07-10 22:03:05 - train: epoch 0007, iter [02200, 05004], lr: 0.199887, loss: 2.3429
2022-07-10 22:04:07 - train: epoch 0007, iter [02300, 05004], lr: 0.199884, loss: 2.3809
2022-07-10 22:05:08 - train: epoch 0007, iter [02400, 05004], lr: 0.199880, loss: 2.6627
2022-07-10 22:06:13 - train: epoch 0007, iter [02500, 05004], lr: 0.199877, loss: 2.4058
2022-07-10 22:07:16 - train: epoch 0007, iter [02600, 05004], lr: 0.199874, loss: 2.3015
2022-07-10 22:08:18 - train: epoch 0007, iter [02700, 05004], lr: 0.199870, loss: 2.3426
2022-07-10 22:09:22 - train: epoch 0007, iter [02800, 05004], lr: 0.199867, loss: 2.1349
2022-07-10 22:10:23 - train: epoch 0007, iter [02900, 05004], lr: 0.199864, loss: 2.2332
2022-07-10 22:11:26 - train: epoch 0007, iter [03000, 05004], lr: 0.199860, loss: 2.5718
2022-07-10 22:12:30 - train: epoch 0007, iter [03100, 05004], lr: 0.199857, loss: 2.2613
2022-07-10 22:13:34 - train: epoch 0007, iter [03200, 05004], lr: 0.199853, loss: 2.0932
2022-07-10 22:14:35 - train: epoch 0007, iter [03300, 05004], lr: 0.199849, loss: 2.5754
2022-07-10 22:15:39 - train: epoch 0007, iter [03400, 05004], lr: 0.199846, loss: 2.4332
2022-07-10 22:16:41 - train: epoch 0007, iter [03500, 05004], lr: 0.199842, loss: 2.2880
2022-07-10 22:17:41 - train: epoch 0007, iter [03600, 05004], lr: 0.199838, loss: 2.2133
2022-07-10 22:18:43 - train: epoch 0007, iter [03700, 05004], lr: 0.199835, loss: 2.3251
2022-07-10 22:19:42 - train: epoch 0007, iter [03800, 05004], lr: 0.199831, loss: 2.4684
2022-07-10 22:20:43 - train: epoch 0007, iter [03900, 05004], lr: 0.199827, loss: 2.1922
2022-07-10 22:21:45 - train: epoch 0007, iter [04000, 05004], lr: 0.199823, loss: 2.5877
2022-07-10 22:22:44 - train: epoch 0007, iter [04100, 05004], lr: 0.199819, loss: 2.1969
2022-07-10 22:23:45 - train: epoch 0007, iter [04200, 05004], lr: 0.199815, loss: 2.1559
2022-07-10 22:24:44 - train: epoch 0007, iter [04300, 05004], lr: 0.199811, loss: 2.6397
2022-07-10 22:25:44 - train: epoch 0007, iter [04400, 05004], lr: 0.199807, loss: 2.2621
2022-07-10 22:26:47 - train: epoch 0007, iter [04500, 05004], lr: 0.199803, loss: 2.4284
2022-07-10 22:27:48 - train: epoch 0007, iter [04600, 05004], lr: 0.199799, loss: 2.4429
2022-07-10 22:28:48 - train: epoch 0007, iter [04700, 05004], lr: 0.199794, loss: 2.2046
2022-07-10 22:29:50 - train: epoch 0007, iter [04800, 05004], lr: 0.199790, loss: 2.5605
2022-07-10 22:30:50 - train: epoch 0007, iter [04900, 05004], lr: 0.199786, loss: 2.3004
2022-07-10 22:31:49 - train: epoch 0007, iter [05000, 05004], lr: 0.199782, loss: 2.2185
2022-07-10 22:31:52 - train: epoch 007, train_loss: 2.3590
2022-07-10 22:34:13 - eval: epoch: 007, acc1: 52.418%, acc5: 77.904%, test_loss: 2.0367, per_image_load_time: 4.140ms, per_image_inference_time: 0.834ms
2022-07-10 22:34:13 - until epoch: 007, best_acc1: 52.418%
2022-07-10 22:34:13 - epoch 008 lr: 0.199781
2022-07-10 22:35:29 - train: epoch 0008, iter [00100, 05004], lr: 0.199777, loss: 2.4428
2022-07-10 22:36:29 - train: epoch 0008, iter [00200, 05004], lr: 0.199773, loss: 2.5002
2022-07-10 22:37:30 - train: epoch 0008, iter [00300, 05004], lr: 0.199768, loss: 2.3487
2022-07-10 22:38:31 - train: epoch 0008, iter [00400, 05004], lr: 0.199764, loss: 2.3548
2022-07-10 22:39:32 - train: epoch 0008, iter [00500, 05004], lr: 0.199759, loss: 2.3067
2022-07-10 22:40:34 - train: epoch 0008, iter [00600, 05004], lr: 0.199754, loss: 2.2866
2022-07-10 22:41:35 - train: epoch 0008, iter [00700, 05004], lr: 0.199750, loss: 2.2574
2022-07-10 22:42:36 - train: epoch 0008, iter [00800, 05004], lr: 0.199745, loss: 2.2411
2022-07-10 22:43:36 - train: epoch 0008, iter [00900, 05004], lr: 0.199740, loss: 2.2115
2022-07-10 22:44:36 - train: epoch 0008, iter [01000, 05004], lr: 0.199736, loss: 2.3576
2022-07-10 22:45:33 - train: epoch 0008, iter [01100, 05004], lr: 0.199731, loss: 2.2947
2022-07-10 22:46:28 - train: epoch 0008, iter [01200, 05004], lr: 0.199726, loss: 2.3203
2022-07-10 22:47:24 - train: epoch 0008, iter [01300, 05004], lr: 0.199721, loss: 2.3890
2022-07-10 22:48:18 - train: epoch 0008, iter [01400, 05004], lr: 0.199716, loss: 2.0531
2022-07-10 22:49:14 - train: epoch 0008, iter [01500, 05004], lr: 0.199711, loss: 2.5317
2022-07-10 22:50:15 - train: epoch 0008, iter [01600, 05004], lr: 0.199706, loss: 2.2604
2022-07-10 22:51:17 - train: epoch 0008, iter [01700, 05004], lr: 0.199701, loss: 2.2547
2022-07-10 22:52:18 - train: epoch 0008, iter [01800, 05004], lr: 0.199696, loss: 2.4026
2022-07-10 22:53:19 - train: epoch 0008, iter [01900, 05004], lr: 0.199691, loss: 2.2486
2022-07-10 22:54:19 - train: epoch 0008, iter [02000, 05004], lr: 0.199685, loss: 2.2919
2022-07-10 22:55:22 - train: epoch 0008, iter [02100, 05004], lr: 0.199680, loss: 2.3402
2022-07-10 22:56:25 - train: epoch 0008, iter [02200, 05004], lr: 0.199675, loss: 2.1659
2022-07-10 22:57:23 - train: epoch 0008, iter [02300, 05004], lr: 0.199669, loss: 2.1781
2022-07-10 22:58:22 - train: epoch 0008, iter [02400, 05004], lr: 0.199664, loss: 2.2420
2022-07-10 22:59:24 - train: epoch 0008, iter [02500, 05004], lr: 0.199659, loss: 2.3058
2022-07-10 23:00:26 - train: epoch 0008, iter [02600, 05004], lr: 0.199653, loss: 2.2848
2022-07-10 23:01:26 - train: epoch 0008, iter [02700, 05004], lr: 0.199648, loss: 2.7125
2022-07-10 23:02:26 - train: epoch 0008, iter [02800, 05004], lr: 0.199642, loss: 2.3820
2022-07-10 23:03:26 - train: epoch 0008, iter [02900, 05004], lr: 0.199636, loss: 2.2849
2022-07-10 23:04:27 - train: epoch 0008, iter [03000, 05004], lr: 0.199631, loss: 2.3694
2022-07-10 23:05:27 - train: epoch 0008, iter [03100, 05004], lr: 0.199625, loss: 2.3198
2022-07-10 23:06:28 - train: epoch 0008, iter [03200, 05004], lr: 0.199619, loss: 2.4322
2022-07-10 23:07:28 - train: epoch 0008, iter [03300, 05004], lr: 0.199614, loss: 2.3203
2022-07-10 23:08:28 - train: epoch 0008, iter [03400, 05004], lr: 0.199608, loss: 2.1660
2022-07-10 23:09:28 - train: epoch 0008, iter [03500, 05004], lr: 0.199602, loss: 2.2590
2022-07-10 23:10:30 - train: epoch 0008, iter [03600, 05004], lr: 0.199596, loss: 2.6155
2022-07-10 23:11:28 - train: epoch 0008, iter [03700, 05004], lr: 0.199590, loss: 2.2326
2022-07-10 23:12:28 - train: epoch 0008, iter [03800, 05004], lr: 0.199584, loss: 2.1928
2022-07-10 23:13:26 - train: epoch 0008, iter [03900, 05004], lr: 0.199578, loss: 2.5302
2022-07-10 23:14:27 - train: epoch 0008, iter [04000, 05004], lr: 0.199572, loss: 2.5408
2022-07-10 23:15:28 - train: epoch 0008, iter [04100, 05004], lr: 0.199566, loss: 2.1310
2022-07-10 23:16:27 - train: epoch 0008, iter [04200, 05004], lr: 0.199560, loss: 2.1786
2022-07-10 23:17:27 - train: epoch 0008, iter [04300, 05004], lr: 0.199553, loss: 2.0497
2022-07-10 23:18:27 - train: epoch 0008, iter [04400, 05004], lr: 0.199547, loss: 2.2122
2022-07-10 23:19:27 - train: epoch 0008, iter [04500, 05004], lr: 0.199541, loss: 2.1894
2022-07-10 23:20:28 - train: epoch 0008, iter [04600, 05004], lr: 0.199534, loss: 2.3244
2022-07-10 23:21:28 - train: epoch 0008, iter [04700, 05004], lr: 0.199528, loss: 2.3321
2022-07-10 23:22:28 - train: epoch 0008, iter [04800, 05004], lr: 0.199522, loss: 2.2946
2022-07-10 23:23:30 - train: epoch 0008, iter [04900, 05004], lr: 0.199515, loss: 2.3939
2022-07-10 23:24:28 - train: epoch 0008, iter [05000, 05004], lr: 0.199509, loss: 2.1727
2022-07-10 23:24:31 - train: epoch 008, train_loss: 2.3003
2022-07-10 23:26:52 - eval: epoch: 008, acc1: 53.170%, acc5: 78.634%, test_loss: 1.9953, per_image_load_time: 3.666ms, per_image_inference_time: 0.845ms
2022-07-10 23:26:52 - until epoch: 008, best_acc1: 53.170%
2022-07-10 23:26:52 - epoch 009 lr: 0.199508
2022-07-10 23:28:09 - train: epoch 0009, iter [00100, 05004], lr: 0.199502, loss: 2.0166
2022-07-10 23:29:09 - train: epoch 0009, iter [00200, 05004], lr: 0.199495, loss: 2.1590
2022-07-10 23:30:10 - train: epoch 0009, iter [00300, 05004], lr: 0.199488, loss: 2.1502
2022-07-10 23:31:13 - train: epoch 0009, iter [00400, 05004], lr: 0.199482, loss: 2.2817
2022-07-10 23:32:16 - train: epoch 0009, iter [00500, 05004], lr: 0.199475, loss: 2.2678
2022-07-10 23:33:17 - train: epoch 0009, iter [00600, 05004], lr: 0.199468, loss: 2.0829
2022-07-10 23:34:20 - train: epoch 0009, iter [00700, 05004], lr: 0.199461, loss: 2.1241
2022-07-10 23:35:23 - train: epoch 0009, iter [00800, 05004], lr: 0.199455, loss: 2.1901
2022-07-10 23:36:25 - train: epoch 0009, iter [00900, 05004], lr: 0.199448, loss: 2.0534
2022-07-10 23:37:27 - train: epoch 0009, iter [01000, 05004], lr: 0.199441, loss: 2.3125
2022-07-10 23:38:28 - train: epoch 0009, iter [01100, 05004], lr: 0.199434, loss: 2.4253
2022-07-10 23:39:31 - train: epoch 0009, iter [01200, 05004], lr: 0.199427, loss: 2.3091
2022-07-10 23:40:34 - train: epoch 0009, iter [01300, 05004], lr: 0.199420, loss: 2.5525
2022-07-10 23:41:37 - train: epoch 0009, iter [01400, 05004], lr: 0.199412, loss: 2.0772
2022-07-10 23:42:40 - train: epoch 0009, iter [01500, 05004], lr: 0.199405, loss: 2.0666
2022-07-10 23:43:41 - train: epoch 0009, iter [01600, 05004], lr: 0.199398, loss: 2.2876
2022-07-10 23:44:43 - train: epoch 0009, iter [01700, 05004], lr: 0.199391, loss: 2.2289
2022-07-10 23:45:45 - train: epoch 0009, iter [01800, 05004], lr: 0.199383, loss: 2.0292
2022-07-10 23:46:48 - train: epoch 0009, iter [01900, 05004], lr: 0.199376, loss: 2.0368
2022-07-10 23:47:52 - train: epoch 0009, iter [02000, 05004], lr: 0.199369, loss: 2.0537
2022-07-10 23:48:55 - train: epoch 0009, iter [02100, 05004], lr: 0.199361, loss: 2.4082
2022-07-10 23:49:58 - train: epoch 0009, iter [02200, 05004], lr: 0.199354, loss: 2.4204
2022-07-10 23:51:00 - train: epoch 0009, iter [02300, 05004], lr: 0.199346, loss: 2.1305
2022-07-10 23:52:03 - train: epoch 0009, iter [02400, 05004], lr: 0.199339, loss: 2.4332
2022-07-10 23:53:07 - train: epoch 0009, iter [02500, 05004], lr: 0.199331, loss: 2.0788
2022-07-10 23:54:10 - train: epoch 0009, iter [02600, 05004], lr: 0.199323, loss: 2.2386
2022-07-10 23:55:12 - train: epoch 0009, iter [02700, 05004], lr: 0.199316, loss: 2.0531
2022-07-10 23:56:13 - train: epoch 0009, iter [02800, 05004], lr: 0.199308, loss: 2.2877
2022-07-10 23:57:15 - train: epoch 0009, iter [02900, 05004], lr: 0.199300, loss: 1.9197
2022-07-10 23:58:18 - train: epoch 0009, iter [03000, 05004], lr: 0.199292, loss: 2.0564
2022-07-10 23:59:20 - train: epoch 0009, iter [03100, 05004], lr: 0.199285, loss: 2.2897
2022-07-11 00:00:23 - train: epoch 0009, iter [03200, 05004], lr: 0.199277, loss: 2.0312
2022-07-11 00:01:25 - train: epoch 0009, iter [03300, 05004], lr: 0.199269, loss: 2.4576
2022-07-11 00:02:28 - train: epoch 0009, iter [03400, 05004], lr: 0.199261, loss: 2.4854
2022-07-11 00:03:31 - train: epoch 0009, iter [03500, 05004], lr: 0.199253, loss: 2.3973
2022-07-11 00:04:31 - train: epoch 0009, iter [03600, 05004], lr: 0.199245, loss: 2.1504
2022-07-11 00:05:32 - train: epoch 0009, iter [03700, 05004], lr: 0.199236, loss: 2.3913
2022-07-11 00:06:33 - train: epoch 0009, iter [03800, 05004], lr: 0.199228, loss: 2.5927
2022-07-11 00:07:33 - train: epoch 0009, iter [03900, 05004], lr: 0.199220, loss: 2.0588
2022-07-11 00:08:36 - train: epoch 0009, iter [04000, 05004], lr: 0.199212, loss: 2.3687
2022-07-11 00:09:36 - train: epoch 0009, iter [04100, 05004], lr: 0.199203, loss: 2.3374
2022-07-11 00:10:37 - train: epoch 0009, iter [04200, 05004], lr: 0.199195, loss: 2.0384
2022-07-11 00:11:39 - train: epoch 0009, iter [04300, 05004], lr: 0.199187, loss: 2.2559
2022-07-11 00:12:42 - train: epoch 0009, iter [04400, 05004], lr: 0.199178, loss: 2.3065
2022-07-11 00:13:44 - train: epoch 0009, iter [04500, 05004], lr: 0.199170, loss: 2.2381
2022-07-11 00:14:45 - train: epoch 0009, iter [04600, 05004], lr: 0.199161, loss: 2.3927
2022-07-11 00:15:47 - train: epoch 0009, iter [04700, 05004], lr: 0.199153, loss: 2.4280
2022-07-11 00:16:49 - train: epoch 0009, iter [04800, 05004], lr: 0.199144, loss: 2.2906
2022-07-11 00:17:49 - train: epoch 0009, iter [04900, 05004], lr: 0.199135, loss: 2.3904
2022-07-11 00:18:49 - train: epoch 0009, iter [05000, 05004], lr: 0.199127, loss: 2.2306
2022-07-11 00:18:51 - train: epoch 009, train_loss: 2.2561
2022-07-11 00:21:09 - eval: epoch: 009, acc1: 53.902%, acc5: 78.662%, test_loss: 1.9929, per_image_load_time: 3.549ms, per_image_inference_time: 0.821ms
2022-07-11 00:21:09 - until epoch: 009, best_acc1: 53.902%
2022-07-11 00:21:09 - epoch 010 lr: 0.199126
2022-07-11 00:22:25 - train: epoch 0010, iter [00100, 05004], lr: 0.199118, loss: 2.0908
2022-07-11 00:23:26 - train: epoch 0010, iter [00200, 05004], lr: 0.199109, loss: 2.2394
2022-07-11 00:24:28 - train: epoch 0010, iter [00300, 05004], lr: 0.199100, loss: 2.2228
2022-07-11 00:25:30 - train: epoch 0010, iter [00400, 05004], lr: 0.199091, loss: 2.5109
2022-07-11 00:26:33 - train: epoch 0010, iter [00500, 05004], lr: 0.199082, loss: 2.1072
2022-07-11 00:27:36 - train: epoch 0010, iter [00600, 05004], lr: 0.199073, loss: 2.2417
2022-07-11 00:28:38 - train: epoch 0010, iter [00700, 05004], lr: 0.199064, loss: 2.1977
2022-07-11 00:29:40 - train: epoch 0010, iter [00800, 05004], lr: 0.199055, loss: 2.2587
2022-07-11 00:30:44 - train: epoch 0010, iter [00900, 05004], lr: 0.199046, loss: 2.2397
2022-07-11 00:31:45 - train: epoch 0010, iter [01000, 05004], lr: 0.199037, loss: 2.0445
2022-07-11 00:32:48 - train: epoch 0010, iter [01100, 05004], lr: 0.199028, loss: 2.1389
2022-07-11 00:33:51 - train: epoch 0010, iter [01200, 05004], lr: 0.199019, loss: 2.1963
2022-07-11 00:34:55 - train: epoch 0010, iter [01300, 05004], lr: 0.199009, loss: 2.0855
2022-07-11 00:35:59 - train: epoch 0010, iter [01400, 05004], lr: 0.199000, loss: 2.3146
2022-07-11 00:37:01 - train: epoch 0010, iter [01500, 05004], lr: 0.198991, loss: 1.7929
2022-07-11 00:38:03 - train: epoch 0010, iter [01600, 05004], lr: 0.198981, loss: 2.3265
2022-07-11 00:39:06 - train: epoch 0010, iter [01700, 05004], lr: 0.198972, loss: 2.2314
2022-07-11 00:40:08 - train: epoch 0010, iter [01800, 05004], lr: 0.198963, loss: 1.9573
2022-07-11 00:41:12 - train: epoch 0010, iter [01900, 05004], lr: 0.198953, loss: 2.2274
2022-07-11 00:42:14 - train: epoch 0010, iter [02000, 05004], lr: 0.198943, loss: 2.3458
2022-07-11 00:43:17 - train: epoch 0010, iter [02100, 05004], lr: 0.198934, loss: 2.0474
2022-07-11 00:44:20 - train: epoch 0010, iter [02200, 05004], lr: 0.198924, loss: 2.3635
2022-07-11 00:45:22 - train: epoch 0010, iter [02300, 05004], lr: 0.198914, loss: 2.4107
2022-07-11 00:46:25 - train: epoch 0010, iter [02400, 05004], lr: 0.198905, loss: 2.5297
2022-07-11 00:47:28 - train: epoch 0010, iter [02500, 05004], lr: 0.198895, loss: 2.3401
2022-07-11 00:48:30 - train: epoch 0010, iter [02600, 05004], lr: 0.198885, loss: 2.3222
2022-07-11 00:49:33 - train: epoch 0010, iter [02700, 05004], lr: 0.198875, loss: 1.9048
2022-07-11 00:50:37 - train: epoch 0010, iter [02800, 05004], lr: 0.198865, loss: 2.3521
2022-07-11 00:51:38 - train: epoch 0010, iter [02900, 05004], lr: 0.198855, loss: 2.2842
2022-07-11 00:52:41 - train: epoch 0010, iter [03000, 05004], lr: 0.198845, loss: 2.2065
2022-07-11 00:53:44 - train: epoch 0010, iter [03100, 05004], lr: 0.198835, loss: 2.5624
2022-07-11 00:54:49 - train: epoch 0010, iter [03200, 05004], lr: 0.198825, loss: 2.1932
2022-07-11 00:55:52 - train: epoch 0010, iter [03300, 05004], lr: 0.198815, loss: 2.1943
2022-07-11 00:56:56 - train: epoch 0010, iter [03400, 05004], lr: 0.198805, loss: 2.4098
2022-07-11 00:58:00 - train: epoch 0010, iter [03500, 05004], lr: 0.198795, loss: 2.3181
2022-07-11 00:59:01 - train: epoch 0010, iter [03600, 05004], lr: 0.198785, loss: 2.4331
2022-07-11 01:00:03 - train: epoch 0010, iter [03700, 05004], lr: 0.198774, loss: 2.3692
2022-07-11 01:01:06 - train: epoch 0010, iter [03800, 05004], lr: 0.198764, loss: 2.2850
2022-07-11 01:02:08 - train: epoch 0010, iter [03900, 05004], lr: 0.198754, loss: 1.9448
2022-07-11 01:03:10 - train: epoch 0010, iter [04000, 05004], lr: 0.198743, loss: 2.1272
2022-07-11 01:04:12 - train: epoch 0010, iter [04100, 05004], lr: 0.198733, loss: 2.0900
2022-07-11 01:05:12 - train: epoch 0010, iter [04200, 05004], lr: 0.198722, loss: 2.2830
2022-07-11 01:06:13 - train: epoch 0010, iter [04300, 05004], lr: 0.198712, loss: 2.0998
2022-07-11 01:07:16 - train: epoch 0010, iter [04400, 05004], lr: 0.198701, loss: 2.3204
2022-07-11 01:08:17 - train: epoch 0010, iter [04500, 05004], lr: 0.198690, loss: 2.2675
2022-07-11 01:09:19 - train: epoch 0010, iter [04600, 05004], lr: 0.198680, loss: 2.0936
2022-07-11 01:10:21 - train: epoch 0010, iter [04700, 05004], lr: 0.198669, loss: 2.2987
2022-07-11 01:11:23 - train: epoch 0010, iter [04800, 05004], lr: 0.198658, loss: 2.2324
2022-07-11 01:12:25 - train: epoch 0010, iter [04900, 05004], lr: 0.198647, loss: 2.2784
2022-07-11 01:13:23 - train: epoch 0010, iter [05000, 05004], lr: 0.198637, loss: 2.0721
2022-07-11 01:13:25 - train: epoch 010, train_loss: 2.2160
2022-07-11 01:15:42 - eval: epoch: 010, acc1: 53.926%, acc5: 78.600%, test_loss: 1.9875, per_image_load_time: 4.239ms, per_image_inference_time: 0.805ms
2022-07-11 01:15:43 - until epoch: 010, best_acc1: 53.926%
2022-07-11 01:15:43 - epoch 011 lr: 0.198636
2022-07-11 01:16:58 - train: epoch 0011, iter [00100, 05004], lr: 0.198625, loss: 2.0604
2022-07-11 01:18:00 - train: epoch 0011, iter [00200, 05004], lr: 0.198614, loss: 2.2969
2022-07-11 01:19:01 - train: epoch 0011, iter [00300, 05004], lr: 0.198603, loss: 2.3319
2022-07-11 01:20:04 - train: epoch 0011, iter [00400, 05004], lr: 0.198592, loss: 2.3226
2022-07-11 01:21:07 - train: epoch 0011, iter [00500, 05004], lr: 0.198581, loss: 2.0845
2022-07-11 01:22:09 - train: epoch 0011, iter [00600, 05004], lr: 0.198570, loss: 2.2901
2022-07-11 01:23:11 - train: epoch 0011, iter [00700, 05004], lr: 0.198559, loss: 2.0989
2022-07-11 01:24:14 - train: epoch 0011, iter [00800, 05004], lr: 0.198548, loss: 2.2883
2022-07-11 01:25:17 - train: epoch 0011, iter [00900, 05004], lr: 0.198536, loss: 2.2255
2022-07-11 01:26:19 - train: epoch 0011, iter [01000, 05004], lr: 0.198525, loss: 2.0953
2022-07-11 01:27:22 - train: epoch 0011, iter [01100, 05004], lr: 0.198514, loss: 2.2165
2022-07-11 01:28:25 - train: epoch 0011, iter [01200, 05004], lr: 0.198503, loss: 2.3724
2022-07-11 01:29:28 - train: epoch 0011, iter [01300, 05004], lr: 0.198491, loss: 2.2846
2022-07-11 01:30:31 - train: epoch 0011, iter [01400, 05004], lr: 0.198480, loss: 2.2354
2022-07-11 01:31:34 - train: epoch 0011, iter [01500, 05004], lr: 0.198468, loss: 2.0637
2022-07-11 01:32:36 - train: epoch 0011, iter [01600, 05004], lr: 0.198457, loss: 2.1144
2022-07-11 01:33:41 - train: epoch 0011, iter [01700, 05004], lr: 0.198445, loss: 2.3375
2022-07-11 01:34:44 - train: epoch 0011, iter [01800, 05004], lr: 0.198433, loss: 2.1954
2022-07-11 01:35:48 - train: epoch 0011, iter [01900, 05004], lr: 0.198422, loss: 2.1581
2022-07-11 01:36:52 - train: epoch 0011, iter [02000, 05004], lr: 0.198410, loss: 2.1924
2022-07-11 01:37:55 - train: epoch 0011, iter [02100, 05004], lr: 0.198398, loss: 2.1980
2022-07-11 01:38:57 - train: epoch 0011, iter [02200, 05004], lr: 0.198386, loss: 2.1270
2022-07-11 01:39:59 - train: epoch 0011, iter [02300, 05004], lr: 0.198375, loss: 2.3912
2022-07-11 01:41:01 - train: epoch 0011, iter [02400, 05004], lr: 0.198363, loss: 2.2337
2022-07-11 01:42:04 - train: epoch 0011, iter [02500, 05004], lr: 0.198351, loss: 2.1599
2022-07-11 01:43:07 - train: epoch 0011, iter [02600, 05004], lr: 0.198339, loss: 2.1586
2022-07-11 01:44:10 - train: epoch 0011, iter [02700, 05004], lr: 0.198327, loss: 2.3378
2022-07-11 01:45:13 - train: epoch 0011, iter [02800, 05004], lr: 0.198315, loss: 1.9394
2022-07-11 01:46:13 - train: epoch 0011, iter [02900, 05004], lr: 0.198303, loss: 2.1477
2022-07-11 01:47:15 - train: epoch 0011, iter [03000, 05004], lr: 0.198290, loss: 2.4357
2022-07-11 01:48:17 - train: epoch 0011, iter [03100, 05004], lr: 0.198278, loss: 2.1708
2022-07-11 01:49:18 - train: epoch 0011, iter [03200, 05004], lr: 0.198266, loss: 2.2227
2022-07-11 01:50:20 - train: epoch 0011, iter [03300, 05004], lr: 0.198254, loss: 2.2043
2022-07-11 01:51:23 - train: epoch 0011, iter [03400, 05004], lr: 0.198241, loss: 2.1257
2022-07-11 01:52:25 - train: epoch 0011, iter [03500, 05004], lr: 0.198229, loss: 1.9373
2022-07-11 01:53:27 - train: epoch 0011, iter [03600, 05004], lr: 0.198217, loss: 2.0728
2022-07-11 01:54:29 - train: epoch 0011, iter [03700, 05004], lr: 0.198204, loss: 2.3170
2022-07-11 01:55:31 - train: epoch 0011, iter [03800, 05004], lr: 0.198192, loss: 2.0737
2022-07-11 01:56:34 - train: epoch 0011, iter [03900, 05004], lr: 0.198179, loss: 2.2965
2022-07-11 01:57:37 - train: epoch 0011, iter [04000, 05004], lr: 0.198167, loss: 2.1540
2022-07-11 01:58:39 - train: epoch 0011, iter [04100, 05004], lr: 0.198154, loss: 2.0252
2022-07-11 01:59:39 - train: epoch 0011, iter [04200, 05004], lr: 0.198141, loss: 2.0675
2022-07-11 02:00:40 - train: epoch 0011, iter [04300, 05004], lr: 0.198129, loss: 2.1017
2022-07-11 02:01:44 - train: epoch 0011, iter [04400, 05004], lr: 0.198116, loss: 2.2850
2022-07-11 02:02:46 - train: epoch 0011, iter [04500, 05004], lr: 0.198103, loss: 2.1490
2022-07-11 02:03:47 - train: epoch 0011, iter [04600, 05004], lr: 0.198090, loss: 2.0877
2022-07-11 02:04:51 - train: epoch 0011, iter [04700, 05004], lr: 0.198077, loss: 2.1143
2022-07-11 02:05:53 - train: epoch 0011, iter [04800, 05004], lr: 0.198064, loss: 1.9390
2022-07-11 02:06:54 - train: epoch 0011, iter [04900, 05004], lr: 0.198052, loss: 2.1035
2022-07-11 02:07:54 - train: epoch 0011, iter [05000, 05004], lr: 0.198039, loss: 2.1689
2022-07-11 02:07:56 - train: epoch 011, train_loss: 2.1834
2022-07-11 02:10:15 - eval: epoch: 011, acc1: 55.222%, acc5: 80.096%, test_loss: 1.9022, per_image_load_time: 4.366ms, per_image_inference_time: 0.796ms
2022-07-11 02:10:15 - until epoch: 011, best_acc1: 55.222%
2022-07-11 02:10:15 - epoch 012 lr: 0.198038
2022-07-11 02:11:33 - train: epoch 0012, iter [00100, 05004], lr: 0.198025, loss: 2.0752
2022-07-11 02:12:34 - train: epoch 0012, iter [00200, 05004], lr: 0.198012, loss: 1.9955
2022-07-11 02:13:35 - train: epoch 0012, iter [00300, 05004], lr: 0.197999, loss: 2.1444
2022-07-11 02:14:35 - train: epoch 0012, iter [00400, 05004], lr: 0.197986, loss: 2.1951
2022-07-11 02:15:37 - train: epoch 0012, iter [00500, 05004], lr: 0.197972, loss: 2.4381
2022-07-11 02:16:38 - train: epoch 0012, iter [00600, 05004], lr: 0.197959, loss: 1.9396
2022-07-11 02:17:39 - train: epoch 0012, iter [00700, 05004], lr: 0.197946, loss: 2.0602
2022-07-11 02:18:40 - train: epoch 0012, iter [00800, 05004], lr: 0.197932, loss: 2.2490
2022-07-11 02:19:42 - train: epoch 0012, iter [00900, 05004], lr: 0.197919, loss: 2.1898
2022-07-11 02:20:43 - train: epoch 0012, iter [01000, 05004], lr: 0.197906, loss: 2.0023
2022-07-11 02:21:45 - train: epoch 0012, iter [01100, 05004], lr: 0.197892, loss: 2.4177
2022-07-11 02:22:47 - train: epoch 0012, iter [01200, 05004], lr: 0.197879, loss: 1.8474
2022-07-11 02:23:50 - train: epoch 0012, iter [01300, 05004], lr: 0.197865, loss: 2.1261
2022-07-11 02:24:53 - train: epoch 0012, iter [01400, 05004], lr: 0.197851, loss: 2.1302
2022-07-11 02:25:56 - train: epoch 0012, iter [01500, 05004], lr: 0.197838, loss: 2.0639
2022-07-11 02:26:58 - train: epoch 0012, iter [01600, 05004], lr: 0.197824, loss: 2.2180
2022-07-11 02:27:59 - train: epoch 0012, iter [01700, 05004], lr: 0.197810, loss: 2.0803
2022-07-11 02:29:01 - train: epoch 0012, iter [01800, 05004], lr: 0.197797, loss: 2.1484
2022-07-11 02:30:03 - train: epoch 0012, iter [01900, 05004], lr: 0.197783, loss: 2.1830
2022-07-11 02:31:05 - train: epoch 0012, iter [02000, 05004], lr: 0.197769, loss: 2.2957
2022-07-11 02:32:07 - train: epoch 0012, iter [02100, 05004], lr: 0.197755, loss: 2.3286
2022-07-11 02:33:10 - train: epoch 0012, iter [02200, 05004], lr: 0.197741, loss: 2.0595
2022-07-11 02:34:12 - train: epoch 0012, iter [02300, 05004], lr: 0.197727, loss: 2.0807
2022-07-11 02:35:13 - train: epoch 0012, iter [02400, 05004], lr: 0.197713, loss: 2.2490
2022-07-11 02:36:15 - train: epoch 0012, iter [02500, 05004], lr: 0.197699, loss: 2.0800
2022-07-11 02:37:18 - train: epoch 0012, iter [02600, 05004], lr: 0.197685, loss: 2.0313
2022-07-11 02:38:20 - train: epoch 0012, iter [02700, 05004], lr: 0.197671, loss: 2.2826
2022-07-11 02:39:22 - train: epoch 0012, iter [02800, 05004], lr: 0.197656, loss: 2.2003
2022-07-11 02:40:23 - train: epoch 0012, iter [02900, 05004], lr: 0.197642, loss: 2.3365
2022-07-11 02:41:24 - train: epoch 0012, iter [03000, 05004], lr: 0.197628, loss: 1.9606
2022-07-11 02:42:26 - train: epoch 0012, iter [03100, 05004], lr: 0.197614, loss: 2.1762
2022-07-11 02:43:27 - train: epoch 0012, iter [03200, 05004], lr: 0.197599, loss: 1.9049
2022-07-11 02:44:29 - train: epoch 0012, iter [03300, 05004], lr: 0.197585, loss: 2.3244
2022-07-11 02:45:32 - train: epoch 0012, iter [03400, 05004], lr: 0.197570, loss: 2.3215
2022-07-11 02:46:36 - train: epoch 0012, iter [03500, 05004], lr: 0.197556, loss: 2.2694
2022-07-11 02:47:38 - train: epoch 0012, iter [03600, 05004], lr: 0.197541, loss: 2.0358
2022-07-11 02:48:40 - train: epoch 0012, iter [03700, 05004], lr: 0.197527, loss: 2.1848
2022-07-11 02:49:43 - train: epoch 0012, iter [03800, 05004], lr: 0.197512, loss: 2.2844
2022-07-11 02:50:45 - train: epoch 0012, iter [03900, 05004], lr: 0.197497, loss: 1.9030
2022-07-11 02:51:49 - train: epoch 0012, iter [04000, 05004], lr: 0.197483, loss: 2.3432
2022-07-11 02:52:52 - train: epoch 0012, iter [04100, 05004], lr: 0.197468, loss: 2.1230
2022-07-11 02:53:54 - train: epoch 0012, iter [04200, 05004], lr: 0.197453, loss: 2.0015
2022-07-11 02:54:55 - train: epoch 0012, iter [04300, 05004], lr: 0.197438, loss: 2.3578
2022-07-11 02:55:56 - train: epoch 0012, iter [04400, 05004], lr: 0.197423, loss: 2.1084
2022-07-11 02:57:00 - train: epoch 0012, iter [04500, 05004], lr: 0.197409, loss: 1.8983
2022-07-11 02:58:02 - train: epoch 0012, iter [04600, 05004], lr: 0.197394, loss: 2.4535
2022-07-11 02:59:06 - train: epoch 0012, iter [04700, 05004], lr: 0.197379, loss: 2.1916
2022-07-11 03:00:09 - train: epoch 0012, iter [04800, 05004], lr: 0.197364, loss: 2.4646
2022-07-11 03:01:11 - train: epoch 0012, iter [04900, 05004], lr: 0.197348, loss: 2.0674
2022-07-11 03:02:11 - train: epoch 0012, iter [05000, 05004], lr: 0.197333, loss: 1.8580
2022-07-11 03:02:13 - train: epoch 012, train_loss: 2.1579
2022-07-11 03:04:35 - eval: epoch: 012, acc1: 55.620%, acc5: 80.426%, test_loss: 1.8884, per_image_load_time: 4.134ms, per_image_inference_time: 0.819ms
2022-07-11 03:04:35 - until epoch: 012, best_acc1: 55.620%
2022-07-11 03:04:35 - epoch 013 lr: 0.197333
2022-07-11 03:05:52 - train: epoch 0013, iter [00100, 05004], lr: 0.197317, loss: 1.8372
2022-07-11 03:06:54 - train: epoch 0013, iter [00200, 05004], lr: 0.197302, loss: 2.1444
2022-07-11 03:07:57 - train: epoch 0013, iter [00300, 05004], lr: 0.197287, loss: 2.0205
2022-07-11 03:08:58 - train: epoch 0013, iter [00400, 05004], lr: 0.197272, loss: 2.0868
2022-07-11 03:10:01 - train: epoch 0013, iter [00500, 05004], lr: 0.197256, loss: 2.1966
2022-07-11 03:11:04 - train: epoch 0013, iter [00600, 05004], lr: 0.197241, loss: 1.9390
2022-07-11 03:12:07 - train: epoch 0013, iter [00700, 05004], lr: 0.197225, loss: 2.0048
2022-07-11 03:13:08 - train: epoch 0013, iter [00800, 05004], lr: 0.197210, loss: 2.3016
2022-07-11 03:14:10 - train: epoch 0013, iter [00900, 05004], lr: 0.197194, loss: 2.0589
2022-07-11 03:15:12 - train: epoch 0013, iter [01000, 05004], lr: 0.197179, loss: 1.9328
2022-07-11 03:16:14 - train: epoch 0013, iter [01100, 05004], lr: 0.197163, loss: 2.2940
2022-07-11 03:17:18 - train: epoch 0013, iter [01200, 05004], lr: 0.197148, loss: 2.1525
2022-07-11 03:18:20 - train: epoch 0013, iter [01300, 05004], lr: 0.197132, loss: 2.0260
2022-07-11 03:19:23 - train: epoch 0013, iter [01400, 05004], lr: 0.197116, loss: 2.1979
2022-07-11 03:20:26 - train: epoch 0013, iter [01500, 05004], lr: 0.197100, loss: 2.3488
2022-07-11 03:21:29 - train: epoch 0013, iter [01600, 05004], lr: 0.197085, loss: 2.1364
2022-07-11 03:22:31 - train: epoch 0013, iter [01700, 05004], lr: 0.197069, loss: 1.9907
2022-07-11 03:23:33 - train: epoch 0013, iter [01800, 05004], lr: 0.197053, loss: 2.1092
2022-07-11 03:24:37 - train: epoch 0013, iter [01900, 05004], lr: 0.197037, loss: 2.3467
2022-07-11 03:25:40 - train: epoch 0013, iter [02000, 05004], lr: 0.197021, loss: 2.0506
2022-07-11 03:26:43 - train: epoch 0013, iter [02100, 05004], lr: 0.197005, loss: 2.3035
2022-07-11 03:27:45 - train: epoch 0013, iter [02200, 05004], lr: 0.196989, loss: 2.1800
2022-07-11 03:28:47 - train: epoch 0013, iter [02300, 05004], lr: 0.196973, loss: 2.1479
2022-07-11 03:29:49 - train: epoch 0013, iter [02400, 05004], lr: 0.196957, loss: 2.0206
2022-07-11 03:30:52 - train: epoch 0013, iter [02500, 05004], lr: 0.196940, loss: 2.1670
2022-07-11 03:31:54 - train: epoch 0013, iter [02600, 05004], lr: 0.196924, loss: 2.1384
2022-07-11 03:32:56 - train: epoch 0013, iter [02700, 05004], lr: 0.196908, loss: 1.9833
2022-07-11 03:33:59 - train: epoch 0013, iter [02800, 05004], lr: 0.196891, loss: 2.0681
2022-07-11 03:35:01 - train: epoch 0013, iter [02900, 05004], lr: 0.196875, loss: 2.2571
2022-07-11 03:36:02 - train: epoch 0013, iter [03000, 05004], lr: 0.196859, loss: 1.9676
2022-07-11 03:37:03 - train: epoch 0013, iter [03100, 05004], lr: 0.196842, loss: 2.0935
2022-07-11 03:38:05 - train: epoch 0013, iter [03200, 05004], lr: 0.196826, loss: 2.2380
2022-07-11 03:39:07 - train: epoch 0013, iter [03300, 05004], lr: 0.196809, loss: 1.9784
2022-07-11 03:40:09 - train: epoch 0013, iter [03400, 05004], lr: 0.196793, loss: 2.2508
2022-07-11 03:41:11 - train: epoch 0013, iter [03500, 05004], lr: 0.196776, loss: 1.9636
2022-07-11 03:42:11 - train: epoch 0013, iter [03600, 05004], lr: 0.196759, loss: 2.4593
2022-07-11 03:43:13 - train: epoch 0013, iter [03700, 05004], lr: 0.196743, loss: 1.7921
2022-07-11 03:44:16 - train: epoch 0013, iter [03800, 05004], lr: 0.196726, loss: 2.3864
2022-07-11 03:45:18 - train: epoch 0013, iter [03900, 05004], lr: 0.196709, loss: 2.1518
2022-07-11 03:46:20 - train: epoch 0013, iter [04000, 05004], lr: 0.196692, loss: 2.2126
2022-07-11 03:47:21 - train: epoch 0013, iter [04100, 05004], lr: 0.196675, loss: 2.0141
2022-07-11 03:48:24 - train: epoch 0013, iter [04200, 05004], lr: 0.196658, loss: 2.1013
2022-07-11 03:49:25 - train: epoch 0013, iter [04300, 05004], lr: 0.196641, loss: 2.0926
2022-07-11 03:50:26 - train: epoch 0013, iter [04400, 05004], lr: 0.196624, loss: 2.1005
2022-07-11 03:51:28 - train: epoch 0013, iter [04500, 05004], lr: 0.196607, loss: 2.1790
2022-07-11 03:52:32 - train: epoch 0013, iter [04600, 05004], lr: 0.196590, loss: 2.0124
2022-07-11 03:53:33 - train: epoch 0013, iter [04700, 05004], lr: 0.196573, loss: 2.3236
2022-07-11 03:54:36 - train: epoch 0013, iter [04800, 05004], lr: 0.196556, loss: 2.0445
2022-07-11 03:55:37 - train: epoch 0013, iter [04900, 05004], lr: 0.196539, loss: 2.2791
2022-07-11 03:56:37 - train: epoch 0013, iter [05000, 05004], lr: 0.196522, loss: 2.2620
2022-07-11 03:56:40 - train: epoch 013, train_loss: 2.1345
2022-07-11 03:59:01 - eval: epoch: 013, acc1: 56.706%, acc5: 81.340%, test_loss: 1.8362, per_image_load_time: 3.861ms, per_image_inference_time: 0.817ms
2022-07-11 03:59:01 - until epoch: 013, best_acc1: 56.706%
2022-07-11 03:59:01 - epoch 014 lr: 0.196521
2022-07-11 04:00:17 - train: epoch 0014, iter [00100, 05004], lr: 0.196504, loss: 1.8456
2022-07-11 04:01:19 - train: epoch 0014, iter [00200, 05004], lr: 0.196486, loss: 2.2986
2022-07-11 04:02:21 - train: epoch 0014, iter [00300, 05004], lr: 0.196469, loss: 2.0989
2022-07-11 04:03:22 - train: epoch 0014, iter [00400, 05004], lr: 0.196451, loss: 1.9983
2022-07-11 04:04:21 - train: epoch 0014, iter [00500, 05004], lr: 0.196434, loss: 2.0120
2022-07-11 04:05:23 - train: epoch 0014, iter [00600, 05004], lr: 0.196416, loss: 2.1370
2022-07-11 04:06:24 - train: epoch 0014, iter [00700, 05004], lr: 0.196399, loss: 1.9692
2022-07-11 04:07:25 - train: epoch 0014, iter [00800, 05004], lr: 0.196381, loss: 1.9593
2022-07-11 04:08:26 - train: epoch 0014, iter [00900, 05004], lr: 0.196364, loss: 2.1332
2022-07-11 04:09:26 - train: epoch 0014, iter [01000, 05004], lr: 0.196346, loss: 2.4037
2022-07-11 04:10:26 - train: epoch 0014, iter [01100, 05004], lr: 0.196328, loss: 2.0823
2022-07-11 04:11:28 - train: epoch 0014, iter [01200, 05004], lr: 0.196310, loss: 2.2392
2022-07-11 04:12:29 - train: epoch 0014, iter [01300, 05004], lr: 0.196293, loss: 2.1081
2022-07-11 04:13:32 - train: epoch 0014, iter [01400, 05004], lr: 0.196275, loss: 2.3446
2022-07-11 04:14:32 - train: epoch 0014, iter [01500, 05004], lr: 0.196257, loss: 2.1753
2022-07-11 04:15:36 - train: epoch 0014, iter [01600, 05004], lr: 0.196239, loss: 2.0133
2022-07-11 04:16:38 - train: epoch 0014, iter [01700, 05004], lr: 0.196221, loss: 2.2448
2022-07-11 04:17:38 - train: epoch 0014, iter [01800, 05004], lr: 0.196203, loss: 2.1719
2022-07-11 04:18:40 - train: epoch 0014, iter [01900, 05004], lr: 0.196185, loss: 2.0360
2022-07-11 04:19:42 - train: epoch 0014, iter [02000, 05004], lr: 0.196167, loss: 2.0935
2022-07-11 04:20:44 - train: epoch 0014, iter [02100, 05004], lr: 0.196149, loss: 2.1611
2022-07-11 04:21:43 - train: epoch 0014, iter [02200, 05004], lr: 0.196131, loss: 2.0891
2022-07-11 04:22:44 - train: epoch 0014, iter [02300, 05004], lr: 0.196112, loss: 1.9541
2022-07-11 04:23:45 - train: epoch 0014, iter [02400, 05004], lr: 0.196094, loss: 2.1839
2022-07-11 04:24:46 - train: epoch 0014, iter [02500, 05004], lr: 0.196076, loss: 2.1044
2022-07-11 04:25:48 - train: epoch 0014, iter [02600, 05004], lr: 0.196057, loss: 2.1422
2022-07-11 04:26:49 - train: epoch 0014, iter [02700, 05004], lr: 0.196039, loss: 2.0746
2022-07-11 04:27:49 - train: epoch 0014, iter [02800, 05004], lr: 0.196021, loss: 2.2246
2022-07-11 04:28:51 - train: epoch 0014, iter [02900, 05004], lr: 0.196002, loss: 2.1641
2022-07-11 04:29:52 - train: epoch 0014, iter [03000, 05004], lr: 0.195984, loss: 2.1136
2022-07-11 04:30:52 - train: epoch 0014, iter [03100, 05004], lr: 0.195965, loss: 2.0865
2022-07-11 04:31:53 - train: epoch 0014, iter [03200, 05004], lr: 0.195946, loss: 2.0700
2022-07-11 04:32:54 - train: epoch 0014, iter [03300, 05004], lr: 0.195928, loss: 2.0254
2022-07-11 04:33:57 - train: epoch 0014, iter [03400, 05004], lr: 0.195909, loss: 2.0540
2022-07-11 04:34:59 - train: epoch 0014, iter [03500, 05004], lr: 0.195890, loss: 2.1030
2022-07-11 04:36:00 - train: epoch 0014, iter [03600, 05004], lr: 0.195872, loss: 1.9010
2022-07-11 04:37:01 - train: epoch 0014, iter [03700, 05004], lr: 0.195853, loss: 1.9276
2022-07-11 04:38:02 - train: epoch 0014, iter [03800, 05004], lr: 0.195834, loss: 2.0901
2022-07-11 04:39:04 - train: epoch 0014, iter [03900, 05004], lr: 0.195815, loss: 2.0625
2022-07-11 04:40:05 - train: epoch 0014, iter [04000, 05004], lr: 0.195796, loss: 1.9905
2022-07-11 04:41:07 - train: epoch 0014, iter [04100, 05004], lr: 0.195777, loss: 2.1371
2022-07-11 04:42:09 - train: epoch 0014, iter [04200, 05004], lr: 0.195758, loss: 2.0642
2022-07-11 04:43:11 - train: epoch 0014, iter [04300, 05004], lr: 0.195739, loss: 2.0888
2022-07-11 04:44:09 - train: epoch 0014, iter [04400, 05004], lr: 0.195720, loss: 1.9062
2022-07-11 04:45:10 - train: epoch 0014, iter [04500, 05004], lr: 0.195701, loss: 2.0812
2022-07-11 04:46:11 - train: epoch 0014, iter [04600, 05004], lr: 0.195682, loss: 2.0526
2022-07-11 04:47:13 - train: epoch 0014, iter [04700, 05004], lr: 0.195662, loss: 2.1608
2022-07-11 04:48:13 - train: epoch 0014, iter [04800, 05004], lr: 0.195643, loss: 2.1293
2022-07-11 04:49:14 - train: epoch 0014, iter [04900, 05004], lr: 0.195624, loss: 2.1761
2022-07-11 04:50:13 - train: epoch 0014, iter [05000, 05004], lr: 0.195604, loss: 2.2413
2022-07-11 04:50:16 - train: epoch 014, train_loss: 2.1124
2022-07-11 04:52:34 - eval: epoch: 014, acc1: 55.698%, acc5: 80.396%, test_loss: 1.9040, per_image_load_time: 2.641ms, per_image_inference_time: 0.820ms
2022-07-11 04:52:34 - until epoch: 014, best_acc1: 56.706%
2022-07-11 04:52:34 - epoch 015 lr: 0.195603
2022-07-11 04:53:50 - train: epoch 0015, iter [00100, 05004], lr: 0.195584, loss: 1.7917
2022-07-11 04:54:52 - train: epoch 0015, iter [00200, 05004], lr: 0.195565, loss: 2.2335
2022-07-11 04:55:54 - train: epoch 0015, iter [00300, 05004], lr: 0.195545, loss: 2.1865
2022-07-11 04:56:53 - train: epoch 0015, iter [00400, 05004], lr: 0.195526, loss: 2.1051
2022-07-11 04:57:54 - train: epoch 0015, iter [00500, 05004], lr: 0.195506, loss: 2.1590
2022-07-11 04:58:57 - train: epoch 0015, iter [00600, 05004], lr: 0.195487, loss: 2.1079
2022-07-11 04:59:58 - train: epoch 0015, iter [00700, 05004], lr: 0.195467, loss: 2.1258
2022-07-11 05:01:00 - train: epoch 0015, iter [00800, 05004], lr: 0.195447, loss: 2.1942
2022-07-11 05:02:02 - train: epoch 0015, iter [00900, 05004], lr: 0.195427, loss: 2.1115
2022-07-11 05:03:03 - train: epoch 0015, iter [01000, 05004], lr: 0.195408, loss: 1.9417
2022-07-11 05:04:04 - train: epoch 0015, iter [01100, 05004], lr: 0.195388, loss: 1.9923
2022-07-11 05:05:08 - train: epoch 0015, iter [01200, 05004], lr: 0.195368, loss: 2.0047
2022-07-11 05:06:11 - train: epoch 0015, iter [01300, 05004], lr: 0.195348, loss: 2.3970
2022-07-11 05:07:15 - train: epoch 0015, iter [01400, 05004], lr: 0.195328, loss: 2.0399
2022-07-11 05:08:16 - train: epoch 0015, iter [01500, 05004], lr: 0.195308, loss: 2.0232
2022-07-11 05:09:18 - train: epoch 0015, iter [01600, 05004], lr: 0.195288, loss: 2.1675
2022-07-11 05:10:21 - train: epoch 0015, iter [01700, 05004], lr: 0.195268, loss: 2.0332
2022-07-11 05:11:22 - train: epoch 0015, iter [01800, 05004], lr: 0.195248, loss: 1.8310
2022-07-11 05:12:24 - train: epoch 0015, iter [01900, 05004], lr: 0.195228, loss: 1.9814
2022-07-11 05:13:27 - train: epoch 0015, iter [02000, 05004], lr: 0.195208, loss: 1.8658
2022-07-11 05:14:28 - train: epoch 0015, iter [02100, 05004], lr: 0.195187, loss: 1.9732
2022-07-11 05:15:31 - train: epoch 0015, iter [02200, 05004], lr: 0.195167, loss: 2.1926
2022-07-11 05:16:34 - train: epoch 0015, iter [02300, 05004], lr: 0.195147, loss: 2.1407
2022-07-11 05:17:35 - train: epoch 0015, iter [02400, 05004], lr: 0.195126, loss: 2.1405
2022-07-11 05:18:37 - train: epoch 0015, iter [02500, 05004], lr: 0.195106, loss: 2.2210
2022-07-11 05:19:38 - train: epoch 0015, iter [02600, 05004], lr: 0.195086, loss: 1.8021
2022-07-11 05:20:41 - train: epoch 0015, iter [02700, 05004], lr: 0.195065, loss: 2.0732
2022-07-11 05:21:43 - train: epoch 0015, iter [02800, 05004], lr: 0.195045, loss: 2.2099
2022-07-11 05:22:44 - train: epoch 0015, iter [02900, 05004], lr: 0.195024, loss: 2.3435
2022-07-11 05:23:45 - train: epoch 0015, iter [03000, 05004], lr: 0.195003, loss: 2.0128
2022-07-11 05:24:46 - train: epoch 0015, iter [03100, 05004], lr: 0.194983, loss: 2.1208
2022-07-11 05:25:47 - train: epoch 0015, iter [03200, 05004], lr: 0.194962, loss: 2.0778
2022-07-11 05:26:50 - train: epoch 0015, iter [03300, 05004], lr: 0.194941, loss: 1.9398
2022-07-11 05:27:51 - train: epoch 0015, iter [03400, 05004], lr: 0.194921, loss: 2.1745
2022-07-11 05:28:52 - train: epoch 0015, iter [03500, 05004], lr: 0.194900, loss: 2.3206
2022-07-11 05:29:56 - train: epoch 0015, iter [03600, 05004], lr: 0.194879, loss: 2.0734
2022-07-11 05:30:58 - train: epoch 0015, iter [03700, 05004], lr: 0.194858, loss: 1.9429
2022-07-11 05:32:00 - train: epoch 0015, iter [03800, 05004], lr: 0.194837, loss: 2.1662
2022-07-11 05:33:02 - train: epoch 0015, iter [03900, 05004], lr: 0.194816, loss: 1.9614
2022-07-11 05:34:04 - train: epoch 0015, iter [04000, 05004], lr: 0.194795, loss: 2.0469
2022-07-11 05:35:06 - train: epoch 0015, iter [04100, 05004], lr: 0.194774, loss: 2.2785
2022-07-11 05:36:09 - train: epoch 0015, iter [04200, 05004], lr: 0.194753, loss: 1.8694
2022-07-11 05:37:09 - train: epoch 0015, iter [04300, 05004], lr: 0.194732, loss: 2.1712
2022-07-11 05:38:10 - train: epoch 0015, iter [04400, 05004], lr: 0.194711, loss: 1.8167
2022-07-11 05:39:11 - train: epoch 0015, iter [04500, 05004], lr: 0.194689, loss: 2.2397
2022-07-11 05:40:12 - train: epoch 0015, iter [04600, 05004], lr: 0.194668, loss: 2.2040
2022-07-11 05:41:13 - train: epoch 0015, iter [04700, 05004], lr: 0.194647, loss: 2.1633
2022-07-11 05:42:14 - train: epoch 0015, iter [04800, 05004], lr: 0.194625, loss: 2.2775
2022-07-11 05:43:15 - train: epoch 0015, iter [04900, 05004], lr: 0.194604, loss: 2.1800
2022-07-11 05:44:16 - train: epoch 0015, iter [05000, 05004], lr: 0.194583, loss: 2.2876
2022-07-11 05:44:18 - train: epoch 015, train_loss: 2.0948
2022-07-11 05:46:37 - eval: epoch: 015, acc1: 56.782%, acc5: 81.090%, test_loss: 1.8379, per_image_load_time: 4.433ms, per_image_inference_time: 0.851ms
2022-07-11 05:46:37 - until epoch: 015, best_acc1: 56.782%
2022-07-11 05:46:37 - epoch 016 lr: 0.194582
2022-07-11 05:47:54 - train: epoch 0016, iter [00100, 05004], lr: 0.194560, loss: 1.9183
2022-07-11 05:48:54 - train: epoch 0016, iter [00200, 05004], lr: 0.194539, loss: 1.9916
2022-07-11 05:49:54 - train: epoch 0016, iter [00300, 05004], lr: 0.194517, loss: 2.1182
2022-07-11 05:50:51 - train: epoch 0016, iter [00400, 05004], lr: 0.194496, loss: 2.3312
2022-07-11 05:51:49 - train: epoch 0016, iter [00500, 05004], lr: 0.194474, loss: 2.0200
2022-07-11 05:52:49 - train: epoch 0016, iter [00600, 05004], lr: 0.194452, loss: 2.1702
2022-07-11 05:53:48 - train: epoch 0016, iter [00700, 05004], lr: 0.194431, loss: 1.8492
2022-07-11 05:54:48 - train: epoch 0016, iter [00800, 05004], lr: 0.194409, loss: 2.0059
2022-07-11 05:55:48 - train: epoch 0016, iter [00900, 05004], lr: 0.194387, loss: 1.9849
2022-07-11 05:56:50 - train: epoch 0016, iter [01000, 05004], lr: 0.194365, loss: 1.9361
2022-07-11 05:57:52 - train: epoch 0016, iter [01100, 05004], lr: 0.194343, loss: 2.0826
2022-07-11 05:58:57 - train: epoch 0016, iter [01200, 05004], lr: 0.194321, loss: 1.7961
2022-07-11 06:00:00 - train: epoch 0016, iter [01300, 05004], lr: 0.194299, loss: 2.0371
2022-07-11 06:01:03 - train: epoch 0016, iter [01400, 05004], lr: 0.194277, loss: 1.9638
2022-07-11 06:02:06 - train: epoch 0016, iter [01500, 05004], lr: 0.194255, loss: 2.2385
2022-07-11 06:03:06 - train: epoch 0016, iter [01600, 05004], lr: 0.194233, loss: 2.1336
2022-07-11 06:04:07 - train: epoch 0016, iter [01700, 05004], lr: 0.194211, loss: 2.1590
2022-07-11 06:05:08 - train: epoch 0016, iter [01800, 05004], lr: 0.194189, loss: 2.2225
2022-07-11 06:06:09 - train: epoch 0016, iter [01900, 05004], lr: 0.194167, loss: 2.1725
2022-07-11 06:07:11 - train: epoch 0016, iter [02000, 05004], lr: 0.194144, loss: 1.8508
2022-07-11 06:08:14 - train: epoch 0016, iter [02100, 05004], lr: 0.194122, loss: 2.0570
2022-07-11 06:09:16 - train: epoch 0016, iter [02200, 05004], lr: 0.194100, loss: 1.9147
2022-07-11 06:10:17 - train: epoch 0016, iter [02300, 05004], lr: 0.194077, loss: 2.2467
2022-07-11 06:11:20 - train: epoch 0016, iter [02400, 05004], lr: 0.194055, loss: 2.0380
2022-07-11 06:12:22 - train: epoch 0016, iter [02500, 05004], lr: 0.194032, loss: 1.9439
2022-07-11 06:13:24 - train: epoch 0016, iter [02600, 05004], lr: 0.194010, loss: 2.1986
2022-07-11 06:14:24 - train: epoch 0016, iter [02700, 05004], lr: 0.193987, loss: 2.1262
2022-07-11 06:15:26 - train: epoch 0016, iter [02800, 05004], lr: 0.193965, loss: 1.9837
2022-07-11 06:16:27 - train: epoch 0016, iter [02900, 05004], lr: 0.193942, loss: 2.0061
2022-07-11 06:17:30 - train: epoch 0016, iter [03000, 05004], lr: 0.193919, loss: 2.2039
2022-07-11 06:18:31 - train: epoch 0016, iter [03100, 05004], lr: 0.193897, loss: 2.1592
2022-07-11 06:19:33 - train: epoch 0016, iter [03200, 05004], lr: 0.193874, loss: 2.2935
2022-07-11 06:20:36 - train: epoch 0016, iter [03300, 05004], lr: 0.193851, loss: 2.2724
2022-07-11 06:21:39 - train: epoch 0016, iter [03400, 05004], lr: 0.193828, loss: 2.0550
2022-07-11 06:22:42 - train: epoch 0016, iter [03500, 05004], lr: 0.193805, loss: 1.9855
2022-07-11 06:23:43 - train: epoch 0016, iter [03600, 05004], lr: 0.193783, loss: 1.9999
2022-07-11 06:24:45 - train: epoch 0016, iter [03700, 05004], lr: 0.193760, loss: 2.1030
2022-07-11 06:25:45 - train: epoch 0016, iter [03800, 05004], lr: 0.193737, loss: 2.3123
2022-07-11 06:26:48 - train: epoch 0016, iter [03900, 05004], lr: 0.193714, loss: 1.9767
2022-07-11 06:27:50 - train: epoch 0016, iter [04000, 05004], lr: 0.193690, loss: 2.3646
2022-07-11 06:28:52 - train: epoch 0016, iter [04100, 05004], lr: 0.193667, loss: 2.1242
2022-07-11 06:29:55 - train: epoch 0016, iter [04200, 05004], lr: 0.193644, loss: 1.9774
2022-07-11 06:30:55 - train: epoch 0016, iter [04300, 05004], lr: 0.193621, loss: 2.0119
2022-07-11 06:31:55 - train: epoch 0016, iter [04400, 05004], lr: 0.193598, loss: 1.7615
2022-07-11 06:32:56 - train: epoch 0016, iter [04500, 05004], lr: 0.193574, loss: 2.1740
2022-07-11 06:33:58 - train: epoch 0016, iter [04600, 05004], lr: 0.193551, loss: 2.0787
2022-07-11 06:35:00 - train: epoch 0016, iter [04700, 05004], lr: 0.193528, loss: 2.2107
2022-07-11 06:36:03 - train: epoch 0016, iter [04800, 05004], lr: 0.193504, loss: 2.0334
2022-07-11 06:37:06 - train: epoch 0016, iter [04900, 05004], lr: 0.193481, loss: 2.1480
2022-07-11 06:38:05 - train: epoch 0016, iter [05000, 05004], lr: 0.193457, loss: 2.2352
2022-07-11 06:38:08 - train: epoch 016, train_loss: 2.0750
2022-07-11 06:40:25 - eval: epoch: 016, acc1: 56.630%, acc5: 81.106%, test_loss: 1.8362, per_image_load_time: 2.752ms, per_image_inference_time: 0.862ms
2022-07-11 06:40:25 - until epoch: 016, best_acc1: 56.782%
2022-07-11 06:40:25 - epoch 017 lr: 0.193456
2022-07-11 06:41:41 - train: epoch 0017, iter [00100, 05004], lr: 0.193433, loss: 2.2469
2022-07-11 06:42:42 - train: epoch 0017, iter [00200, 05004], lr: 0.193409, loss: 2.0540
2022-07-11 06:43:44 - train: epoch 0017, iter [00300, 05004], lr: 0.193386, loss: 2.2905
2022-07-11 06:44:44 - train: epoch 0017, iter [00400, 05004], lr: 0.193362, loss: 1.9374
2022-07-11 06:45:45 - train: epoch 0017, iter [00500, 05004], lr: 0.193338, loss: 2.2417
2022-07-11 06:46:46 - train: epoch 0017, iter [00600, 05004], lr: 0.193315, loss: 2.2076
2022-07-11 06:47:48 - train: epoch 0017, iter [00700, 05004], lr: 0.193291, loss: 1.8490
2022-07-11 06:48:50 - train: epoch 0017, iter [00800, 05004], lr: 0.193267, loss: 2.0611
2022-07-11 06:49:52 - train: epoch 0017, iter [00900, 05004], lr: 0.193243, loss: 1.9378
2022-07-11 06:50:56 - train: epoch 0017, iter [01000, 05004], lr: 0.193219, loss: 2.1046
2022-07-11 06:51:58 - train: epoch 0017, iter [01100, 05004], lr: 0.193195, loss: 2.3366
2022-07-11 06:53:00 - train: epoch 0017, iter [01200, 05004], lr: 0.193171, loss: 2.1133
2022-07-11 06:54:04 - train: epoch 0017, iter [01300, 05004], lr: 0.193147, loss: 2.1233
2022-07-11 06:55:06 - train: epoch 0017, iter [01400, 05004], lr: 0.193123, loss: 1.9958
2022-07-11 06:56:08 - train: epoch 0017, iter [01500, 05004], lr: 0.193099, loss: 1.8079
2022-07-11 06:57:11 - train: epoch 0017, iter [01600, 05004], lr: 0.193075, loss: 1.8763
2022-07-11 06:58:13 - train: epoch 0017, iter [01700, 05004], lr: 0.193051, loss: 2.0039
2022-07-11 06:59:14 - train: epoch 0017, iter [01800, 05004], lr: 0.193027, loss: 2.0984
2022-07-11 07:00:17 - train: epoch 0017, iter [01900, 05004], lr: 0.193002, loss: 2.0180
2022-07-11 07:01:18 - train: epoch 0017, iter [02000, 05004], lr: 0.192978, loss: 2.1893
2022-07-11 07:02:20 - train: epoch 0017, iter [02100, 05004], lr: 0.192954, loss: 2.0203
2022-07-11 07:03:21 - train: epoch 0017, iter [02200, 05004], lr: 0.192929, loss: 2.0041
2022-07-11 07:04:22 - train: epoch 0017, iter [02300, 05004], lr: 0.192905, loss: 1.9236
2022-07-11 07:05:24 - train: epoch 0017, iter [02400, 05004], lr: 0.192880, loss: 2.0321
2022-07-11 07:06:28 - train: epoch 0017, iter [02500, 05004], lr: 0.192856, loss: 2.2210
2022-07-11 07:07:30 - train: epoch 0017, iter [02600, 05004], lr: 0.192831, loss: 2.0672
2022-07-11 07:08:31 - train: epoch 0017, iter [02700, 05004], lr: 0.192807, loss: 1.9639
2022-07-11 07:09:33 - train: epoch 0017, iter [02800, 05004], lr: 0.192782, loss: 2.0235
2022-07-11 07:10:35 - train: epoch 0017, iter [02900, 05004], lr: 0.192757, loss: 2.3107
2022-07-11 07:11:34 - train: epoch 0017, iter [03000, 05004], lr: 0.192733, loss: 1.9584
2022-07-11 07:12:34 - train: epoch 0017, iter [03100, 05004], lr: 0.192708, loss: 2.0401
2022-07-11 07:13:36 - train: epoch 0017, iter [03200, 05004], lr: 0.192683, loss: 1.8414
2022-07-11 07:14:37 - train: epoch 0017, iter [03300, 05004], lr: 0.192658, loss: 2.3316
2022-07-11 07:15:39 - train: epoch 0017, iter [03400, 05004], lr: 0.192633, loss: 1.8009
2022-07-11 07:16:41 - train: epoch 0017, iter [03500, 05004], lr: 0.192609, loss: 2.1069
2022-07-11 07:17:44 - train: epoch 0017, iter [03600, 05004], lr: 0.192584, loss: 2.2148
2022-07-11 07:18:46 - train: epoch 0017, iter [03700, 05004], lr: 0.192559, loss: 1.8971
2022-07-11 07:19:47 - train: epoch 0017, iter [03800, 05004], lr: 0.192534, loss: 1.9329
2022-07-11 07:20:49 - train: epoch 0017, iter [03900, 05004], lr: 0.192509, loss: 1.9315
2022-07-11 07:21:50 - train: epoch 0017, iter [04000, 05004], lr: 0.192483, loss: 2.0674
2022-07-11 07:22:52 - train: epoch 0017, iter [04100, 05004], lr: 0.192458, loss: 2.1391
2022-07-11 07:23:53 - train: epoch 0017, iter [04200, 05004], lr: 0.192433, loss: 1.9679
2022-07-11 07:24:53 - train: epoch 0017, iter [04300, 05004], lr: 0.192408, loss: 1.9480
2022-07-11 07:25:53 - train: epoch 0017, iter [04400, 05004], lr: 0.192383, loss: 2.1705
2022-07-11 07:26:55 - train: epoch 0017, iter [04500, 05004], lr: 0.192357, loss: 2.2227
2022-07-11 07:27:57 - train: epoch 0017, iter [04600, 05004], lr: 0.192332, loss: 1.8064
2022-07-11 07:28:59 - train: epoch 0017, iter [04700, 05004], lr: 0.192306, loss: 2.1444
2022-07-11 07:30:02 - train: epoch 0017, iter [04800, 05004], lr: 0.192281, loss: 2.0222
2022-07-11 07:31:03 - train: epoch 0017, iter [04900, 05004], lr: 0.192256, loss: 1.9694
2022-07-11 07:32:03 - train: epoch 0017, iter [05000, 05004], lr: 0.192230, loss: 1.7885
2022-07-11 07:32:05 - train: epoch 017, train_loss: 2.0579
2022-07-11 07:34:21 - eval: epoch: 017, acc1: 58.214%, acc5: 82.332%, test_loss: 1.7559, per_image_load_time: 2.695ms, per_image_inference_time: 0.846ms
2022-07-11 07:34:22 - until epoch: 017, best_acc1: 58.214%
2022-07-11 07:34:22 - epoch 018 lr: 0.192229
2022-07-11 07:35:33 - train: epoch 0018, iter [00100, 05004], lr: 0.192203, loss: 1.7910
2022-07-11 07:36:35 - train: epoch 0018, iter [00200, 05004], lr: 0.192178, loss: 2.1917
2022-07-11 07:37:36 - train: epoch 0018, iter [00300, 05004], lr: 0.192152, loss: 2.2348
2022-07-11 07:38:37 - train: epoch 0018, iter [00400, 05004], lr: 0.192126, loss: 2.3976
2022-07-11 07:39:37 - train: epoch 0018, iter [00500, 05004], lr: 0.192101, loss: 2.0662
2022-07-11 07:40:40 - train: epoch 0018, iter [00600, 05004], lr: 0.192075, loss: 2.2670
2022-07-11 07:41:43 - train: epoch 0018, iter [00700, 05004], lr: 0.192049, loss: 2.1170
2022-07-11 07:42:45 - train: epoch 0018, iter [00800, 05004], lr: 0.192023, loss: 2.2251
2022-07-11 07:43:48 - train: epoch 0018, iter [00900, 05004], lr: 0.191997, loss: 2.0695
2022-07-11 07:44:49 - train: epoch 0018, iter [01000, 05004], lr: 0.191972, loss: 2.1098
2022-07-11 07:45:50 - train: epoch 0018, iter [01100, 05004], lr: 0.191946, loss: 2.2471
2022-07-11 07:46:52 - train: epoch 0018, iter [01200, 05004], lr: 0.191920, loss: 2.1229
2022-07-11 07:47:55 - train: epoch 0018, iter [01300, 05004], lr: 0.191894, loss: 2.3515
2022-07-11 07:48:57 - train: epoch 0018, iter [01400, 05004], lr: 0.191867, loss: 2.2521
2022-07-11 07:50:00 - train: epoch 0018, iter [01500, 05004], lr: 0.191841, loss: 2.2804
2022-07-11 07:51:02 - train: epoch 0018, iter [01600, 05004], lr: 0.191815, loss: 1.8884
2022-07-11 07:52:04 - train: epoch 0018, iter [01700, 05004], lr: 0.191789, loss: 2.1416
2022-07-11 07:53:05 - train: epoch 0018, iter [01800, 05004], lr: 0.191763, loss: 1.8326
2022-07-11 07:54:05 - train: epoch 0018, iter [01900, 05004], lr: 0.191736, loss: 2.0999
2022-07-11 07:55:06 - train: epoch 0018, iter [02000, 05004], lr: 0.191710, loss: 2.3286
2022-07-11 07:56:09 - train: epoch 0018, iter [02100, 05004], lr: 0.191684, loss: 2.0965
2022-07-11 07:57:11 - train: epoch 0018, iter [02200, 05004], lr: 0.191657, loss: 2.3263
2022-07-11 07:58:13 - train: epoch 0018, iter [02300, 05004], lr: 0.191631, loss: 2.2867
2022-07-11 07:59:15 - train: epoch 0018, iter [02400, 05004], lr: 0.191604, loss: 2.0345
2022-07-11 08:00:18 - train: epoch 0018, iter [02500, 05004], lr: 0.191578, loss: 1.7862
2022-07-11 08:01:21 - train: epoch 0018, iter [02600, 05004], lr: 0.191551, loss: 2.0415
2022-07-11 08:02:24 - train: epoch 0018, iter [02700, 05004], lr: 0.191525, loss: 2.2227
2022-07-11 08:03:26 - train: epoch 0018, iter [02800, 05004], lr: 0.191498, loss: 1.8982
2022-07-11 08:04:30 - train: epoch 0018, iter [02900, 05004], lr: 0.191471, loss: 1.9312
2022-07-11 08:05:34 - train: epoch 0018, iter [03000, 05004], lr: 0.191445, loss: 2.1080
2022-07-11 08:06:36 - train: epoch 0018, iter [03100, 05004], lr: 0.191418, loss: 2.1771
2022-07-11 08:07:39 - train: epoch 0018, iter [03200, 05004], lr: 0.191391, loss: 2.0562
2022-07-11 08:08:43 - train: epoch 0018, iter [03300, 05004], lr: 0.191364, loss: 1.8556
2022-07-11 08:09:47 - train: epoch 0018, iter [03400, 05004], lr: 0.191337, loss: 1.9870
2022-07-11 08:10:51 - train: epoch 0018, iter [03500, 05004], lr: 0.191310, loss: 2.0289
2022-07-11 08:11:52 - train: epoch 0018, iter [03600, 05004], lr: 0.191283, loss: 1.9305
2022-07-11 08:12:54 - train: epoch 0018, iter [03700, 05004], lr: 0.191256, loss: 2.5907
2022-07-11 08:13:57 - train: epoch 0018, iter [03800, 05004], lr: 0.191229, loss: 2.3213
2022-07-11 08:15:01 - train: epoch 0018, iter [03900, 05004], lr: 0.191202, loss: 2.1950
2022-07-11 08:16:04 - train: epoch 0018, iter [04000, 05004], lr: 0.191175, loss: 2.3451
2022-07-11 08:17:04 - train: epoch 0018, iter [04100, 05004], lr: 0.191148, loss: 2.2320
2022-07-11 08:18:05 - train: epoch 0018, iter [04200, 05004], lr: 0.191121, loss: 1.9742
2022-07-11 08:19:04 - train: epoch 0018, iter [04300, 05004], lr: 0.191094, loss: 1.8601
2022-07-11 08:20:05 - train: epoch 0018, iter [04400, 05004], lr: 0.191066, loss: 2.0070
2022-07-11 08:21:08 - train: epoch 0018, iter [04500, 05004], lr: 0.191039, loss: 2.0793
2022-07-11 08:22:12 - train: epoch 0018, iter [04600, 05004], lr: 0.191012, loss: 1.9291
2022-07-11 08:23:16 - train: epoch 0018, iter [04700, 05004], lr: 0.190984, loss: 2.4346
2022-07-11 08:24:19 - train: epoch 0018, iter [04800, 05004], lr: 0.190957, loss: 2.0220
2022-07-11 08:25:22 - train: epoch 0018, iter [04900, 05004], lr: 0.190929, loss: 2.1268
2022-07-11 08:26:22 - train: epoch 0018, iter [05000, 05004], lr: 0.190902, loss: 2.2300
2022-07-11 08:26:24 - train: epoch 018, train_loss: 2.0453
2022-07-11 08:28:45 - eval: epoch: 018, acc1: 57.914%, acc5: 82.088%, test_loss: 1.7670, per_image_load_time: 3.613ms, per_image_inference_time: 0.877ms
2022-07-11 08:28:45 - until epoch: 018, best_acc1: 58.214%
2022-07-11 08:28:45 - epoch 019 lr: 0.190900
2022-07-11 08:29:59 - train: epoch 0019, iter [00100, 05004], lr: 0.190873, loss: 1.9885
2022-07-11 08:31:01 - train: epoch 0019, iter [00200, 05004], lr: 0.190845, loss: 2.1216
2022-07-11 08:32:03 - train: epoch 0019, iter [00300, 05004], lr: 0.190818, loss: 2.1613
2022-07-11 08:33:04 - train: epoch 0019, iter [00400, 05004], lr: 0.190790, loss: 1.9554
2022-07-11 08:34:04 - train: epoch 0019, iter [00500, 05004], lr: 0.190762, loss: 1.9662
2022-07-11 08:35:06 - train: epoch 0019, iter [00600, 05004], lr: 0.190735, loss: 2.0614
2022-07-11 08:36:08 - train: epoch 0019, iter [00700, 05004], lr: 0.190707, loss: 1.8060
2022-07-11 08:37:12 - train: epoch 0019, iter [00800, 05004], lr: 0.190679, loss: 2.0938
2022-07-11 08:38:15 - train: epoch 0019, iter [00900, 05004], lr: 0.190651, loss: 2.2998
2022-07-11 08:39:18 - train: epoch 0019, iter [01000, 05004], lr: 0.190623, loss: 2.0018
2022-07-11 08:40:21 - train: epoch 0019, iter [01100, 05004], lr: 0.190595, loss: 1.9384
2022-07-11 08:41:23 - train: epoch 0019, iter [01200, 05004], lr: 0.190567, loss: 1.9295
2022-07-11 08:42:24 - train: epoch 0019, iter [01300, 05004], lr: 0.190539, loss: 2.1170
2022-07-11 08:43:25 - train: epoch 0019, iter [01400, 05004], lr: 0.190511, loss: 1.9560
2022-07-11 08:44:26 - train: epoch 0019, iter [01500, 05004], lr: 0.190483, loss: 2.2592
2022-07-11 08:45:27 - train: epoch 0019, iter [01600, 05004], lr: 0.190455, loss: 1.9312
2022-07-11 08:46:28 - train: epoch 0019, iter [01700, 05004], lr: 0.190427, loss: 2.0531
2022-07-11 08:47:29 - train: epoch 0019, iter [01800, 05004], lr: 0.190398, loss: 2.1963
2022-07-11 08:48:33 - train: epoch 0019, iter [01900, 05004], lr: 0.190370, loss: 2.3343
2022-07-11 08:49:37 - train: epoch 0019, iter [02000, 05004], lr: 0.190342, loss: 1.9930
2022-07-11 08:50:39 - train: epoch 0019, iter [02100, 05004], lr: 0.190314, loss: 1.7214
2022-07-11 08:51:42 - train: epoch 0019, iter [02200, 05004], lr: 0.190285, loss: 2.0112
2022-07-11 08:52:42 - train: epoch 0019, iter [02300, 05004], lr: 0.190257, loss: 1.9598
2022-07-11 08:53:43 - train: epoch 0019, iter [02400, 05004], lr: 0.190228, loss: 2.3248
2022-07-11 08:54:44 - train: epoch 0019, iter [02500, 05004], lr: 0.190200, loss: 1.9472
2022-07-11 08:55:45 - train: epoch 0019, iter [02600, 05004], lr: 0.190171, loss: 1.9313
2022-07-11 08:56:46 - train: epoch 0019, iter [02700, 05004], lr: 0.190143, loss: 1.9213
2022-07-11 08:57:48 - train: epoch 0019, iter [02800, 05004], lr: 0.190114, loss: 2.0993
2022-07-11 08:58:50 - train: epoch 0019, iter [02900, 05004], lr: 0.190085, loss: 1.9862
2022-07-11 08:59:50 - train: epoch 0019, iter [03000, 05004], lr: 0.190057, loss: 2.1613
2022-07-11 09:00:52 - train: epoch 0019, iter [03100, 05004], lr: 0.190028, loss: 1.9719
2022-07-11 09:01:54 - train: epoch 0019, iter [03200, 05004], lr: 0.189999, loss: 1.6683
2022-07-11 09:02:57 - train: epoch 0019, iter [03300, 05004], lr: 0.189970, loss: 1.9490
2022-07-11 09:04:00 - train: epoch 0019, iter [03400, 05004], lr: 0.189941, loss: 2.2010
2022-07-11 09:05:04 - train: epoch 0019, iter [03500, 05004], lr: 0.189912, loss: 2.3074
2022-07-11 09:06:07 - train: epoch 0019, iter [03600, 05004], lr: 0.189883, loss: 1.6958
2022-07-11 09:07:08 - train: epoch 0019, iter [03700, 05004], lr: 0.189854, loss: 2.1499
2022-07-11 09:08:09 - train: epoch 0019, iter [03800, 05004], lr: 0.189825, loss: 2.1878
2022-07-11 09:09:11 - train: epoch 0019, iter [03900, 05004], lr: 0.189796, loss: 2.1024
2022-07-11 09:10:11 - train: epoch 0019, iter [04000, 05004], lr: 0.189767, loss: 2.0687
2022-07-11 09:11:11 - train: epoch 0019, iter [04100, 05004], lr: 0.189738, loss: 1.9621
2022-07-11 09:12:13 - train: epoch 0019, iter [04200, 05004], lr: 0.189709, loss: 1.9955
2022-07-11 09:13:13 - train: epoch 0019, iter [04300, 05004], lr: 0.189680, loss: 1.8979
2022-07-11 09:14:13 - train: epoch 0019, iter [04400, 05004], lr: 0.189650, loss: 2.1990
2022-07-11 09:15:14 - train: epoch 0019, iter [04500, 05004], lr: 0.189621, loss: 2.3651
2022-07-11 09:16:15 - train: epoch 0019, iter [04600, 05004], lr: 0.189592, loss: 2.0070
2022-07-11 09:17:16 - train: epoch 0019, iter [04700, 05004], lr: 0.189562, loss: 2.2137
2022-07-11 09:18:17 - train: epoch 0019, iter [04800, 05004], lr: 0.189533, loss: 1.9394
2022-07-11 09:19:18 - train: epoch 0019, iter [04900, 05004], lr: 0.189504, loss: 2.0604
2022-07-11 09:20:18 - train: epoch 0019, iter [05000, 05004], lr: 0.189474, loss: 2.0228
2022-07-11 09:20:20 - train: epoch 019, train_loss: 2.0318
2022-07-11 09:22:34 - eval: epoch: 019, acc1: 58.548%, acc5: 82.710%, test_loss: 1.7454, per_image_load_time: 3.047ms, per_image_inference_time: 0.878ms
2022-07-11 09:22:34 - until epoch: 019, best_acc1: 58.548%
2022-07-11 09:22:34 - epoch 020 lr: 0.189473
2022-07-11 09:23:47 - train: epoch 0020, iter [00100, 05004], lr: 0.189443, loss: 2.1281
2022-07-11 09:24:46 - train: epoch 0020, iter [00200, 05004], lr: 0.189414, loss: 1.7784
2022-07-11 09:25:48 - train: epoch 0020, iter [00300, 05004], lr: 0.189384, loss: 2.0106
2022-07-11 09:26:48 - train: epoch 0020, iter [00400, 05004], lr: 0.189355, loss: 1.8863
2022-07-11 09:27:50 - train: epoch 0020, iter [00500, 05004], lr: 0.189325, loss: 1.8606
2022-07-11 09:28:53 - train: epoch 0020, iter [00600, 05004], lr: 0.189295, loss: 2.1359
2022-07-11 09:29:54 - train: epoch 0020, iter [00700, 05004], lr: 0.189265, loss: 1.8191
2022-07-11 09:30:56 - train: epoch 0020, iter [00800, 05004], lr: 0.189236, loss: 2.0941
2022-07-11 09:31:58 - train: epoch 0020, iter [00900, 05004], lr: 0.189206, loss: 2.0040
2022-07-11 09:33:01 - train: epoch 0020, iter [01000, 05004], lr: 0.189176, loss: 2.1004
2022-07-11 09:34:03 - train: epoch 0020, iter [01100, 05004], lr: 0.189146, loss: 1.7648
2022-07-11 09:35:06 - train: epoch 0020, iter [01200, 05004], lr: 0.189116, loss: 1.9649
2022-07-11 09:36:10 - train: epoch 0020, iter [01300, 05004], lr: 0.189086, loss: 1.8423
2022-07-11 09:37:14 - train: epoch 0020, iter [01400, 05004], lr: 0.189056, loss: 2.1663
2022-07-11 09:38:15 - train: epoch 0020, iter [01500, 05004], lr: 0.189026, loss: 1.9058
2022-07-11 09:39:19 - train: epoch 0020, iter [01600, 05004], lr: 0.188996, loss: 2.0367
2022-07-11 09:40:19 - train: epoch 0020, iter [01700, 05004], lr: 0.188966, loss: 1.9221
2022-07-11 09:41:20 - train: epoch 0020, iter [01800, 05004], lr: 0.188935, loss: 2.0537
2022-07-11 09:42:22 - train: epoch 0020, iter [01900, 05004], lr: 0.188905, loss: 1.8043
2022-07-11 09:43:26 - train: epoch 0020, iter [02000, 05004], lr: 0.188875, loss: 2.0373
2022-07-11 09:44:29 - train: epoch 0020, iter [02100, 05004], lr: 0.188845, loss: 2.1991
2022-07-11 09:45:32 - train: epoch 0020, iter [02200, 05004], lr: 0.188814, loss: 1.9008
2022-07-11 09:46:35 - train: epoch 0020, iter [02300, 05004], lr: 0.188784, loss: 1.8844
2022-07-11 09:47:37 - train: epoch 0020, iter [02400, 05004], lr: 0.188753, loss: 2.4181
2022-07-11 09:48:38 - train: epoch 0020, iter [02500, 05004], lr: 0.188723, loss: 2.1235
2022-07-11 09:49:40 - train: epoch 0020, iter [02600, 05004], lr: 0.188692, loss: 2.0869
2022-07-11 09:50:42 - train: epoch 0020, iter [02700, 05004], lr: 0.188662, loss: 2.1430
2022-07-11 09:51:43 - train: epoch 0020, iter [02800, 05004], lr: 0.188631, loss: 2.2169
2022-07-11 09:52:46 - train: epoch 0020, iter [02900, 05004], lr: 0.188601, loss: 2.2094
2022-07-11 09:53:47 - train: epoch 0020, iter [03000, 05004], lr: 0.188570, loss: 2.2738
2022-07-11 09:54:47 - train: epoch 0020, iter [03100, 05004], lr: 0.188539, loss: 2.0664
2022-07-11 09:55:48 - train: epoch 0020, iter [03200, 05004], lr: 0.188509, loss: 2.1836
2022-07-11 09:56:49 - train: epoch 0020, iter [03300, 05004], lr: 0.188478, loss: 1.7519
2022-07-11 09:57:52 - train: epoch 0020, iter [03400, 05004], lr: 0.188447, loss: 1.9356
2022-07-11 09:58:55 - train: epoch 0020, iter [03500, 05004], lr: 0.188416, loss: 1.9223
2022-07-11 09:59:57 - train: epoch 0020, iter [03600, 05004], lr: 0.188385, loss: 1.9477
2022-07-11 10:00:58 - train: epoch 0020, iter [03700, 05004], lr: 0.188354, loss: 2.0316
2022-07-11 10:01:59 - train: epoch 0020, iter [03800, 05004], lr: 0.188323, loss: 2.0158
2022-07-11 10:03:02 - train: epoch 0020, iter [03900, 05004], lr: 0.188292, loss: 2.0511
2022-07-11 10:04:04 - train: epoch 0020, iter [04000, 05004], lr: 0.188261, loss: 1.7849
2022-07-11 10:05:06 - train: epoch 0020, iter [04100, 05004], lr: 0.188230, loss: 1.9955
2022-07-11 10:06:08 - train: epoch 0020, iter [04200, 05004], lr: 0.188199, loss: 1.8582
2022-07-11 10:07:09 - train: epoch 0020, iter [04300, 05004], lr: 0.188168, loss: 2.2661
2022-07-11 10:08:11 - train: epoch 0020, iter [04400, 05004], lr: 0.188137, loss: 1.8186
2022-07-11 10:09:12 - train: epoch 0020, iter [04500, 05004], lr: 0.188105, loss: 1.9564
2022-07-11 10:10:15 - train: epoch 0020, iter [04600, 05004], lr: 0.188074, loss: 1.8021
2022-07-11 10:11:17 - train: epoch 0020, iter [04700, 05004], lr: 0.188043, loss: 1.7869
2022-07-11 10:12:19 - train: epoch 0020, iter [04800, 05004], lr: 0.188011, loss: 1.7835
2022-07-11 10:13:21 - train: epoch 0020, iter [04900, 05004], lr: 0.187980, loss: 2.0725
2022-07-11 10:14:19 - train: epoch 0020, iter [05000, 05004], lr: 0.187949, loss: 1.9306
2022-07-11 10:14:21 - train: epoch 020, train_loss: 2.0185
2022-07-11 10:16:37 - eval: epoch: 020, acc1: 59.340%, acc5: 83.224%, test_loss: 1.7016, per_image_load_time: 2.951ms, per_image_inference_time: 0.864ms
2022-07-11 10:16:37 - until epoch: 020, best_acc1: 59.340%
2022-07-11 10:16:37 - epoch 021 lr: 0.187947
2022-07-11 10:17:50 - train: epoch 0021, iter [00100, 05004], lr: 0.187916, loss: 2.0938
2022-07-11 10:18:50 - train: epoch 0021, iter [00200, 05004], lr: 0.187884, loss: 2.0046
2022-07-11 10:19:51 - train: epoch 0021, iter [00300, 05004], lr: 0.187853, loss: 1.7905
2022-07-11 10:20:51 - train: epoch 0021, iter [00400, 05004], lr: 0.187821, loss: 2.0330
2022-07-11 10:21:53 - train: epoch 0021, iter [00500, 05004], lr: 0.187790, loss: 1.8061
2022-07-11 10:22:54 - train: epoch 0021, iter [00600, 05004], lr: 0.187758, loss: 1.8645
2022-07-11 10:23:56 - train: epoch 0021, iter [00700, 05004], lr: 0.187726, loss: 1.9363
2022-07-11 10:24:59 - train: epoch 0021, iter [00800, 05004], lr: 0.187695, loss: 2.1446
2022-07-11 10:26:00 - train: epoch 0021, iter [00900, 05004], lr: 0.187663, loss: 2.0667
2022-07-11 10:27:01 - train: epoch 0021, iter [01000, 05004], lr: 0.187631, loss: 1.9530
2022-07-11 10:28:03 - train: epoch 0021, iter [01100, 05004], lr: 0.187599, loss: 1.9322
2022-07-11 10:29:04 - train: epoch 0021, iter [01200, 05004], lr: 0.187567, loss: 1.9792
2022-07-11 10:30:04 - train: epoch 0021, iter [01300, 05004], lr: 0.187535, loss: 1.7800
2022-07-11 10:31:07 - train: epoch 0021, iter [01400, 05004], lr: 0.187503, loss: 1.7197
2022-07-11 10:32:07 - train: epoch 0021, iter [01500, 05004], lr: 0.187471, loss: 1.9247
2022-07-11 10:33:09 - train: epoch 0021, iter [01600, 05004], lr: 0.187439, loss: 1.8858
2022-07-11 10:34:09 - train: epoch 0021, iter [01700, 05004], lr: 0.187407, loss: 2.0788
2022-07-11 10:35:10 - train: epoch 0021, iter [01800, 05004], lr: 0.187375, loss: 1.8522
2022-07-11 10:36:11 - train: epoch 0021, iter [01900, 05004], lr: 0.187343, loss: 2.0660
2022-07-11 10:37:12 - train: epoch 0021, iter [02000, 05004], lr: 0.187311, loss: 2.1215
2022-07-11 10:38:13 - train: epoch 0021, iter [02100, 05004], lr: 0.187278, loss: 1.8454
2022-07-11 10:39:14 - train: epoch 0021, iter [02200, 05004], lr: 0.187246, loss: 2.1163
2022-07-11 10:40:14 - train: epoch 0021, iter [02300, 05004], lr: 0.187214, loss: 2.0194
2022-07-11 10:41:18 - train: epoch 0021, iter [02400, 05004], lr: 0.187181, loss: 1.7589
2022-07-11 10:42:19 - train: epoch 0021, iter [02500, 05004], lr: 0.187149, loss: 1.9995
2022-07-11 10:43:22 - train: epoch 0021, iter [02600, 05004], lr: 0.187117, loss: 2.2671
2022-07-11 10:44:26 - train: epoch 0021, iter [02700, 05004], lr: 0.187084, loss: 1.8626
2022-07-11 10:45:28 - train: epoch 0021, iter [02800, 05004], lr: 0.187052, loss: 1.8889
2022-07-11 10:46:29 - train: epoch 0021, iter [02900, 05004], lr: 0.187019, loss: 1.9553
2022-07-11 10:47:30 - train: epoch 0021, iter [03000, 05004], lr: 0.186987, loss: 2.1008
2022-07-11 10:48:31 - train: epoch 0021, iter [03100, 05004], lr: 0.186954, loss: 2.0735
2022-07-11 10:49:33 - train: epoch 0021, iter [03200, 05004], lr: 0.186921, loss: 1.9955
2022-07-11 10:50:35 - train: epoch 0021, iter [03300, 05004], lr: 0.186889, loss: 2.2357
2022-07-11 10:51:37 - train: epoch 0021, iter [03400, 05004], lr: 0.186856, loss: 2.2641
2022-07-11 10:52:39 - train: epoch 0021, iter [03500, 05004], lr: 0.186823, loss: 2.0621
2022-07-11 10:53:41 - train: epoch 0021, iter [03600, 05004], lr: 0.186790, loss: 1.9623
2022-07-11 10:54:42 - train: epoch 0021, iter [03700, 05004], lr: 0.186757, loss: 2.0719
2022-07-11 10:55:43 - train: epoch 0021, iter [03800, 05004], lr: 0.186725, loss: 2.0750
2022-07-11 10:56:46 - train: epoch 0021, iter [03900, 05004], lr: 0.186692, loss: 2.1224
2022-07-11 10:57:47 - train: epoch 0021, iter [04000, 05004], lr: 0.186659, loss: 2.3074
2022-07-11 10:58:51 - train: epoch 0021, iter [04100, 05004], lr: 0.186626, loss: 2.0434
2022-07-11 10:59:53 - train: epoch 0021, iter [04200, 05004], lr: 0.186593, loss: 2.0332
2022-07-11 11:00:54 - train: epoch 0021, iter [04300, 05004], lr: 0.186560, loss: 1.9804
2022-07-11 11:01:54 - train: epoch 0021, iter [04400, 05004], lr: 0.186526, loss: 2.0167
2022-07-11 11:02:56 - train: epoch 0021, iter [04500, 05004], lr: 0.186493, loss: 1.8808
2022-07-11 11:03:57 - train: epoch 0021, iter [04600, 05004], lr: 0.186460, loss: 2.0017
2022-07-11 11:04:59 - train: epoch 0021, iter [04700, 05004], lr: 0.186427, loss: 2.2115
2022-07-11 11:05:59 - train: epoch 0021, iter [04800, 05004], lr: 0.186394, loss: 2.1003
2022-07-11 11:07:01 - train: epoch 0021, iter [04900, 05004], lr: 0.186360, loss: 1.8195
2022-07-11 11:08:02 - train: epoch 0021, iter [05000, 05004], lr: 0.186327, loss: 1.7990
2022-07-11 11:08:04 - train: epoch 021, train_loss: 2.0069
2022-07-11 11:10:21 - eval: epoch: 021, acc1: 58.648%, acc5: 82.720%, test_loss: 1.7252, per_image_load_time: 2.511ms, per_image_inference_time: 0.854ms
2022-07-11 11:10:21 - until epoch: 021, best_acc1: 59.340%
2022-07-11 11:10:21 - epoch 022 lr: 0.186325
2022-07-11 11:11:35 - train: epoch 0022, iter [00100, 05004], lr: 0.186292, loss: 1.7654
2022-07-11 11:12:37 - train: epoch 0022, iter [00200, 05004], lr: 0.186259, loss: 1.8289
2022-07-11 11:13:40 - train: epoch 0022, iter [00300, 05004], lr: 0.186225, loss: 1.8386
2022-07-11 11:14:39 - train: epoch 0022, iter [00400, 05004], lr: 0.186192, loss: 1.8410
2022-07-11 11:15:39 - train: epoch 0022, iter [00500, 05004], lr: 0.186158, loss: 1.9930
2022-07-11 11:16:41 - train: epoch 0022, iter [00600, 05004], lr: 0.186125, loss: 1.8087
2022-07-11 11:17:42 - train: epoch 0022, iter [00700, 05004], lr: 0.186091, loss: 2.3154
2022-07-11 11:18:41 - train: epoch 0022, iter [00800, 05004], lr: 0.186058, loss: 2.0930
2022-07-11 11:19:42 - train: epoch 0022, iter [00900, 05004], lr: 0.186024, loss: 2.1943
2022-07-11 11:20:43 - train: epoch 0022, iter [01000, 05004], lr: 0.185990, loss: 2.1330
2022-07-11 11:21:44 - train: epoch 0022, iter [01100, 05004], lr: 0.185956, loss: 1.9181
2022-07-11 11:22:46 - train: epoch 0022, iter [01200, 05004], lr: 0.185923, loss: 1.6214
2022-07-11 11:23:48 - train: epoch 0022, iter [01300, 05004], lr: 0.185889, loss: 2.0532
2022-07-11 11:24:50 - train: epoch 0022, iter [01400, 05004], lr: 0.185855, loss: 1.8097
2022-07-11 11:25:53 - train: epoch 0022, iter [01500, 05004], lr: 0.185821, loss: 2.1531
2022-07-11 11:26:56 - train: epoch 0022, iter [01600, 05004], lr: 0.185787, loss: 1.9666
2022-07-11 11:27:57 - train: epoch 0022, iter [01700, 05004], lr: 0.185753, loss: 2.1144
2022-07-11 11:28:59 - train: epoch 0022, iter [01800, 05004], lr: 0.185719, loss: 2.4963
2022-07-11 11:30:00 - train: epoch 0022, iter [01900, 05004], lr: 0.185685, loss: 1.9098
2022-07-11 11:31:02 - train: epoch 0022, iter [02000, 05004], lr: 0.185651, loss: 2.1003
2022-07-11 11:32:03 - train: epoch 0022, iter [02100, 05004], lr: 0.185617, loss: 1.9025
2022-07-11 11:33:03 - train: epoch 0022, iter [02200, 05004], lr: 0.185583, loss: 1.7804
2022-07-11 11:34:04 - train: epoch 0022, iter [02300, 05004], lr: 0.185548, loss: 2.1245
2022-07-11 11:35:06 - train: epoch 0022, iter [02400, 05004], lr: 0.185514, loss: 2.0585
2022-07-11 11:36:08 - train: epoch 0022, iter [02500, 05004], lr: 0.185480, loss: 2.1163
2022-07-11 11:37:10 - train: epoch 0022, iter [02600, 05004], lr: 0.185446, loss: 2.0003
2022-07-11 11:38:12 - train: epoch 0022, iter [02700, 05004], lr: 0.185411, loss: 1.8179
2022-07-11 11:39:17 - train: epoch 0022, iter [02800, 05004], lr: 0.185377, loss: 2.1757
2022-07-11 11:40:22 - train: epoch 0022, iter [02900, 05004], lr: 0.185342, loss: 1.8296
2022-07-11 11:41:25 - train: epoch 0022, iter [03000, 05004], lr: 0.185308, loss: 2.2504
2022-07-11 11:42:25 - train: epoch 0022, iter [03100, 05004], lr: 0.185274, loss: 2.1189
2022-07-11 11:43:27 - train: epoch 0022, iter [03200, 05004], lr: 0.185239, loss: 2.1478
2022-07-11 11:44:28 - train: epoch 0022, iter [03300, 05004], lr: 0.185204, loss: 2.0018
2022-07-11 11:45:29 - train: epoch 0022, iter [03400, 05004], lr: 0.185170, loss: 1.9717
2022-07-11 11:46:31 - train: epoch 0022, iter [03500, 05004], lr: 0.185135, loss: 2.1615
2022-07-11 11:47:32 - train: epoch 0022, iter [03600, 05004], lr: 0.185100, loss: 1.9797
2022-07-11 11:48:34 - train: epoch 0022, iter [03700, 05004], lr: 0.185066, loss: 2.1030
2022-07-11 11:49:36 - train: epoch 0022, iter [03800, 05004], lr: 0.185031, loss: 2.1661
2022-07-11 11:50:38 - train: epoch 0022, iter [03900, 05004], lr: 0.184996, loss: 1.9968
2022-07-11 11:51:40 - train: epoch 0022, iter [04000, 05004], lr: 0.184961, loss: 2.0998
2022-07-11 11:52:41 - train: epoch 0022, iter [04100, 05004], lr: 0.184926, loss: 2.0730
2022-07-11 11:53:43 - train: epoch 0022, iter [04200, 05004], lr: 0.184892, loss: 2.1649
2022-07-11 11:54:45 - train: epoch 0022, iter [04300, 05004], lr: 0.184857, loss: 2.0450
2022-07-11 11:55:43 - train: epoch 0022, iter [04400, 05004], lr: 0.184822, loss: 1.9068
2022-07-11 11:56:45 - train: epoch 0022, iter [04500, 05004], lr: 0.184787, loss: 1.9823
2022-07-11 11:57:47 - train: epoch 0022, iter [04600, 05004], lr: 0.184752, loss: 2.1653
2022-07-11 11:58:49 - train: epoch 0022, iter [04700, 05004], lr: 0.184716, loss: 1.9162
2022-07-11 11:59:51 - train: epoch 0022, iter [04800, 05004], lr: 0.184681, loss: 1.9250
2022-07-11 12:00:52 - train: epoch 0022, iter [04900, 05004], lr: 0.184646, loss: 2.0041
2022-07-11 12:01:54 - train: epoch 0022, iter [05000, 05004], lr: 0.184611, loss: 1.8896
2022-07-11 12:01:56 - train: epoch 022, train_loss: 1.9920
2022-07-11 12:04:15 - eval: epoch: 022, acc1: 58.494%, acc5: 82.616%, test_loss: 1.7575, per_image_load_time: 2.676ms, per_image_inference_time: 0.850ms
2022-07-11 12:04:15 - until epoch: 022, best_acc1: 59.340%
2022-07-11 12:04:15 - epoch 023 lr: 0.184609
2022-07-11 12:05:31 - train: epoch 0023, iter [00100, 05004], lr: 0.184574, loss: 1.8440
2022-07-11 12:06:33 - train: epoch 0023, iter [00200, 05004], lr: 0.184539, loss: 1.7376
2022-07-11 12:07:37 - train: epoch 0023, iter [00300, 05004], lr: 0.184504, loss: 2.0215
2022-07-11 12:08:39 - train: epoch 0023, iter [00400, 05004], lr: 0.184468, loss: 1.9946
2022-07-11 12:09:40 - train: epoch 0023, iter [00500, 05004], lr: 0.184433, loss: 1.8880
2022-07-11 12:10:43 - train: epoch 0023, iter [00600, 05004], lr: 0.184398, loss: 1.9487
2022-07-11 12:11:43 - train: epoch 0023, iter [00700, 05004], lr: 0.184362, loss: 1.8588
2022-07-11 12:12:45 - train: epoch 0023, iter [00800, 05004], lr: 0.184327, loss: 2.0747
2022-07-11 12:13:46 - train: epoch 0023, iter [00900, 05004], lr: 0.184291, loss: 1.9979
2022-07-11 12:14:51 - train: epoch 0023, iter [01000, 05004], lr: 0.184255, loss: 1.8622
2022-07-11 12:15:55 - train: epoch 0023, iter [01100, 05004], lr: 0.184220, loss: 1.7915
2022-07-11 12:16:59 - train: epoch 0023, iter [01200, 05004], lr: 0.184184, loss: 1.8821
2022-07-11 12:18:02 - train: epoch 0023, iter [01300, 05004], lr: 0.184148, loss: 1.7811
2022-07-11 12:19:05 - train: epoch 0023, iter [01400, 05004], lr: 0.184113, loss: 2.0505
2022-07-11 12:20:07 - train: epoch 0023, iter [01500, 05004], lr: 0.184077, loss: 1.8999
2022-07-11 12:21:09 - train: epoch 0023, iter [01600, 05004], lr: 0.184041, loss: 1.8575
2022-07-11 12:22:11 - train: epoch 0023, iter [01700, 05004], lr: 0.184005, loss: 2.1486
2022-07-11 12:23:11 - train: epoch 0023, iter [01800, 05004], lr: 0.183969, loss: 1.8727
2022-07-11 12:24:12 - train: epoch 0023, iter [01900, 05004], lr: 0.183934, loss: 2.1352
2022-07-11 12:25:14 - train: epoch 0023, iter [02000, 05004], lr: 0.183898, loss: 1.8270
2022-07-11 12:26:15 - train: epoch 0023, iter [02100, 05004], lr: 0.183862, loss: 2.1492
2022-07-11 12:27:16 - train: epoch 0023, iter [02200, 05004], lr: 0.183826, loss: 1.8519
2022-07-11 12:28:18 - train: epoch 0023, iter [02300, 05004], lr: 0.183790, loss: 1.8981
2022-07-11 12:29:20 - train: epoch 0023, iter [02400, 05004], lr: 0.183753, loss: 2.0536
2022-07-11 12:30:22 - train: epoch 0023, iter [02500, 05004], lr: 0.183717, loss: 2.2349
2022-07-11 12:31:25 - train: epoch 0023, iter [02600, 05004], lr: 0.183681, loss: 2.0478
2022-07-11 12:32:27 - train: epoch 0023, iter [02700, 05004], lr: 0.183645, loss: 1.9633
2022-07-11 12:33:29 - train: epoch 0023, iter [02800, 05004], lr: 0.183609, loss: 1.9301
2022-07-11 12:34:31 - train: epoch 0023, iter [02900, 05004], lr: 0.183572, loss: 2.0546
2022-07-11 12:35:33 - train: epoch 0023, iter [03000, 05004], lr: 0.183536, loss: 2.0645
2022-07-11 12:36:34 - train: epoch 0023, iter [03100, 05004], lr: 0.183500, loss: 2.0416
2022-07-11 12:37:35 - train: epoch 0023, iter [03200, 05004], lr: 0.183463, loss: 2.1280
2022-07-11 12:38:37 - train: epoch 0023, iter [03300, 05004], lr: 0.183427, loss: 2.2394
2022-07-11 12:39:40 - train: epoch 0023, iter [03400, 05004], lr: 0.183391, loss: 2.1684
2022-07-11 12:40:41 - train: epoch 0023, iter [03500, 05004], lr: 0.183354, loss: 1.7993
2022-07-11 12:41:45 - train: epoch 0023, iter [03600, 05004], lr: 0.183318, loss: 1.9406
2022-07-11 12:42:48 - train: epoch 0023, iter [03700, 05004], lr: 0.183281, loss: 1.8919
2022-07-11 12:43:52 - train: epoch 0023, iter [03800, 05004], lr: 0.183244, loss: 2.0451
2022-07-11 12:44:53 - train: epoch 0023, iter [03900, 05004], lr: 0.183208, loss: 2.1910
2022-07-11 12:45:55 - train: epoch 0023, iter [04000, 05004], lr: 0.183171, loss: 1.9662
2022-07-11 12:46:55 - train: epoch 0023, iter [04100, 05004], lr: 0.183134, loss: 1.9248
2022-07-11 12:47:55 - train: epoch 0023, iter [04200, 05004], lr: 0.183098, loss: 1.7161
2022-07-11 12:48:56 - train: epoch 0023, iter [04300, 05004], lr: 0.183061, loss: 1.9804
2022-07-11 12:49:54 - train: epoch 0023, iter [04400, 05004], lr: 0.183024, loss: 1.9653
2022-07-11 12:50:53 - train: epoch 0023, iter [04500, 05004], lr: 0.182987, loss: 2.0373
2022-07-11 12:51:52 - train: epoch 0023, iter [04600, 05004], lr: 0.182950, loss: 2.0648
2022-07-11 12:52:51 - train: epoch 0023, iter [04700, 05004], lr: 0.182913, loss: 1.8386
2022-07-11 12:53:50 - train: epoch 0023, iter [04800, 05004], lr: 0.182876, loss: 1.8693
2022-07-11 12:54:50 - train: epoch 0023, iter [04900, 05004], lr: 0.182839, loss: 1.9524
2022-07-11 12:55:48 - train: epoch 0023, iter [05000, 05004], lr: 0.182802, loss: 1.9898
2022-07-11 12:55:51 - train: epoch 023, train_loss: 1.9815
2022-07-11 12:58:07 - eval: epoch: 023, acc1: 59.560%, acc5: 83.312%, test_loss: 1.7054, per_image_load_time: 3.854ms, per_image_inference_time: 0.851ms
2022-07-11 12:58:07 - until epoch: 023, best_acc1: 59.560%
2022-07-11 12:58:07 - epoch 024 lr: 0.182801
2022-07-11 12:59:22 - train: epoch 0024, iter [00100, 05004], lr: 0.182764, loss: 2.0016
2022-07-11 13:00:23 - train: epoch 0024, iter [00200, 05004], lr: 0.182727, loss: 1.9069
2022-07-11 13:01:24 - train: epoch 0024, iter [00300, 05004], lr: 0.182690, loss: 2.0927
2022-07-11 13:02:27 - train: epoch 0024, iter [00400, 05004], lr: 0.182652, loss: 1.9375
2022-07-11 13:03:30 - train: epoch 0024, iter [00500, 05004], lr: 0.182615, loss: 1.9283
2022-07-11 13:04:30 - train: epoch 0024, iter [00600, 05004], lr: 0.182578, loss: 2.0801
2022-07-11 13:05:31 - train: epoch 0024, iter [00700, 05004], lr: 0.182541, loss: 1.9447
2022-07-11 13:06:32 - train: epoch 0024, iter [00800, 05004], lr: 0.182503, loss: 1.9298
2022-07-11 13:07:33 - train: epoch 0024, iter [00900, 05004], lr: 0.182466, loss: 2.0933
2022-07-11 13:08:36 - train: epoch 0024, iter [01000, 05004], lr: 0.182429, loss: 2.0082
2022-07-11 13:09:37 - train: epoch 0024, iter [01100, 05004], lr: 0.182391, loss: 1.7172
2022-07-11 13:10:40 - train: epoch 0024, iter [01200, 05004], lr: 0.182354, loss: 1.9192
2022-07-11 13:11:43 - train: epoch 0024, iter [01300, 05004], lr: 0.182316, loss: 2.1428
2022-07-11 13:12:44 - train: epoch 0024, iter [01400, 05004], lr: 0.182279, loss: 1.8071
2022-07-11 13:13:45 - train: epoch 0024, iter [01500, 05004], lr: 0.182241, loss: 2.0987
2022-07-11 13:14:49 - train: epoch 0024, iter [01600, 05004], lr: 0.182203, loss: 2.0323
2022-07-11 13:15:52 - train: epoch 0024, iter [01700, 05004], lr: 0.182166, loss: 1.9110
2022-07-11 13:16:54 - train: epoch 0024, iter [01800, 05004], lr: 0.182128, loss: 2.1260
2022-07-11 13:17:55 - train: epoch 0024, iter [01900, 05004], lr: 0.182090, loss: 1.8747
2022-07-11 13:18:56 - train: epoch 0024, iter [02000, 05004], lr: 0.182053, loss: 1.9433
2022-07-11 13:19:58 - train: epoch 0024, iter [02100, 05004], lr: 0.182015, loss: 2.0666
2022-07-11 13:21:01 - train: epoch 0024, iter [02200, 05004], lr: 0.181977, loss: 1.7971
2022-07-11 13:22:02 - train: epoch 0024, iter [02300, 05004], lr: 0.181939, loss: 2.0604
2022-07-11 13:23:04 - train: epoch 0024, iter [02400, 05004], lr: 0.181901, loss: 1.9934
2022-07-11 13:24:06 - train: epoch 0024, iter [02500, 05004], lr: 0.181863, loss: 1.8342
2022-07-11 13:25:09 - train: epoch 0024, iter [02600, 05004], lr: 0.181825, loss: 2.2703
2022-07-11 13:26:11 - train: epoch 0024, iter [02700, 05004], lr: 0.181787, loss: 2.0653
2022-07-11 13:27:12 - train: epoch 0024, iter [02800, 05004], lr: 0.181749, loss: 2.1992
2022-07-11 13:28:13 - train: epoch 0024, iter [02900, 05004], lr: 0.181711, loss: 1.9258
2022-07-11 13:29:14 - train: epoch 0024, iter [03000, 05004], lr: 0.181673, loss: 1.7720
2022-07-11 13:30:14 - train: epoch 0024, iter [03100, 05004], lr: 0.181635, loss: 1.8954
2022-07-11 13:31:14 - train: epoch 0024, iter [03200, 05004], lr: 0.181597, loss: 2.1634
2022-07-11 13:32:15 - train: epoch 0024, iter [03300, 05004], lr: 0.181558, loss: 1.7787
2022-07-11 13:33:16 - train: epoch 0024, iter [03400, 05004], lr: 0.181520, loss: 1.8037
2022-07-11 13:34:18 - train: epoch 0024, iter [03500, 05004], lr: 0.181482, loss: 2.0495
2022-07-11 13:35:19 - train: epoch 0024, iter [03600, 05004], lr: 0.181444, loss: 1.9646
2022-07-11 13:36:20 - train: epoch 0024, iter [03700, 05004], lr: 0.181405, loss: 1.9522
2022-07-11 13:37:22 - train: epoch 0024, iter [03800, 05004], lr: 0.181367, loss: 2.1662
2022-07-11 13:38:23 - train: epoch 0024, iter [03900, 05004], lr: 0.181328, loss: 2.0204
2022-07-11 13:39:24 - train: epoch 0024, iter [04000, 05004], lr: 0.181290, loss: 1.9874
2022-07-11 13:40:25 - train: epoch 0024, iter [04100, 05004], lr: 0.181251, loss: 1.7998
2022-07-11 13:41:26 - train: epoch 0024, iter [04200, 05004], lr: 0.181213, loss: 1.9335
2022-07-11 13:42:27 - train: epoch 0024, iter [04300, 05004], lr: 0.181174, loss: 1.9228
2022-07-11 13:43:27 - train: epoch 0024, iter [04400, 05004], lr: 0.181136, loss: 1.8461
2022-07-11 13:44:26 - train: epoch 0024, iter [04500, 05004], lr: 0.181097, loss: 1.8110
2022-07-11 13:45:27 - train: epoch 0024, iter [04600, 05004], lr: 0.181058, loss: 2.1208
2022-07-11 13:46:30 - train: epoch 0024, iter [04700, 05004], lr: 0.181020, loss: 1.8910
2022-07-11 13:47:33 - train: epoch 0024, iter [04800, 05004], lr: 0.180981, loss: 1.8516
2022-07-11 13:48:36 - train: epoch 0024, iter [04900, 05004], lr: 0.180942, loss: 1.9484
2022-07-11 13:49:36 - train: epoch 0024, iter [05000, 05004], lr: 0.180903, loss: 1.9399
2022-07-11 13:49:38 - train: epoch 024, train_loss: 1.9703
2022-07-11 13:51:56 - eval: epoch: 024, acc1: 60.116%, acc5: 83.772%, test_loss: 1.6832, per_image_load_time: 2.915ms, per_image_inference_time: 0.916ms
2022-07-11 13:51:56 - until epoch: 024, best_acc1: 60.116%
2022-07-11 13:51:56 - epoch 025 lr: 0.180901
2022-07-11 13:53:10 - train: epoch 0025, iter [00100, 05004], lr: 0.180863, loss: 1.9061
2022-07-11 13:54:12 - train: epoch 0025, iter [00200, 05004], lr: 0.180824, loss: 1.7291
2022-07-11 13:55:13 - train: epoch 0025, iter [00300, 05004], lr: 0.180785, loss: 1.9703
2022-07-11 13:56:13 - train: epoch 0025, iter [00400, 05004], lr: 0.180746, loss: 1.8642
2022-07-11 13:57:15 - train: epoch 0025, iter [00500, 05004], lr: 0.180707, loss: 1.8437
2022-07-11 13:58:14 - train: epoch 0025, iter [00600, 05004], lr: 0.180668, loss: 1.9867
2022-07-11 13:59:14 - train: epoch 0025, iter [00700, 05004], lr: 0.180629, loss: 1.9471
2022-07-11 14:00:16 - train: epoch 0025, iter [00800, 05004], lr: 0.180590, loss: 2.0605
2022-07-11 14:01:16 - train: epoch 0025, iter [00900, 05004], lr: 0.180551, loss: 1.8196
2022-07-11 14:02:19 - train: epoch 0025, iter [01000, 05004], lr: 0.180511, loss: 2.0201
2022-07-11 14:03:21 - train: epoch 0025, iter [01100, 05004], lr: 0.180472, loss: 1.9174
2022-07-11 14:04:23 - train: epoch 0025, iter [01200, 05004], lr: 0.180433, loss: 1.8997
2022-07-11 14:05:25 - train: epoch 0025, iter [01300, 05004], lr: 0.180394, loss: 1.7915
2022-07-11 14:06:27 - train: epoch 0025, iter [01400, 05004], lr: 0.180354, loss: 2.3259
2022-07-11 14:07:28 - train: epoch 0025, iter [01500, 05004], lr: 0.180315, loss: 1.9885
2022-07-11 14:08:30 - train: epoch 0025, iter [01600, 05004], lr: 0.180276, loss: 1.6942
2022-07-11 14:09:31 - train: epoch 0025, iter [01700, 05004], lr: 0.180236, loss: 2.0018
2022-07-11 14:10:32 - train: epoch 0025, iter [01800, 05004], lr: 0.180197, loss: 1.6762
2022-07-11 14:11:30 - train: epoch 0025, iter [01900, 05004], lr: 0.180157, loss: 1.8742
2022-07-11 14:12:31 - train: epoch 0025, iter [02000, 05004], lr: 0.180118, loss: 1.8809
2022-07-11 14:13:34 - train: epoch 0025, iter [02100, 05004], lr: 0.180078, loss: 1.8792
2022-07-11 14:14:36 - train: epoch 0025, iter [02200, 05004], lr: 0.180039, loss: 1.6742
2022-07-11 14:15:36 - train: epoch 0025, iter [02300, 05004], lr: 0.179999, loss: 1.9624
2022-07-11 14:16:39 - train: epoch 0025, iter [02400, 05004], lr: 0.179959, loss: 1.7644
2022-07-11 14:17:41 - train: epoch 0025, iter [02500, 05004], lr: 0.179920, loss: 2.0829
2022-07-11 14:18:43 - train: epoch 0025, iter [02600, 05004], lr: 0.179880, loss: 1.9990
2022-07-11 14:19:44 - train: epoch 0025, iter [02700, 05004], lr: 0.179840, loss: 2.0862
2022-07-11 14:20:47 - train: epoch 0025, iter [02800, 05004], lr: 0.179800, loss: 1.8749
2022-07-11 14:21:50 - train: epoch 0025, iter [02900, 05004], lr: 0.179760, loss: 2.1759
2022-07-11 14:22:52 - train: epoch 0025, iter [03000, 05004], lr: 0.179721, loss: 2.0734
2022-07-11 14:23:52 - train: epoch 0025, iter [03100, 05004], lr: 0.179681, loss: 2.1019
2022-07-11 14:24:49 - train: epoch 0025, iter [03200, 05004], lr: 0.179641, loss: 1.9553
2022-07-11 14:25:44 - train: epoch 0025, iter [03300, 05004], lr: 0.179601, loss: 1.9179
2022-07-11 14:26:39 - train: epoch 0025, iter [03400, 05004], lr: 0.179561, loss: 2.1040
2022-07-11 14:27:33 - train: epoch 0025, iter [03500, 05004], lr: 0.179521, loss: 1.7732
2022-07-11 14:28:27 - train: epoch 0025, iter [03600, 05004], lr: 0.179481, loss: 1.8525
2022-07-11 14:29:21 - train: epoch 0025, iter [03700, 05004], lr: 0.179440, loss: 1.9232
2022-07-11 14:30:16 - train: epoch 0025, iter [03800, 05004], lr: 0.179400, loss: 2.0422
2022-07-11 14:31:11 - train: epoch 0025, iter [03900, 05004], lr: 0.179360, loss: 2.2594
2022-07-11 14:32:07 - train: epoch 0025, iter [04000, 05004], lr: 0.179320, loss: 2.1310
2022-07-11 14:33:01 - train: epoch 0025, iter [04100, 05004], lr: 0.179280, loss: 2.4463
2022-07-11 14:33:56 - train: epoch 0025, iter [04200, 05004], lr: 0.179239, loss: 2.0036
2022-07-11 14:34:50 - train: epoch 0025, iter [04300, 05004], lr: 0.179199, loss: 1.8400
2022-07-11 14:35:45 - train: epoch 0025, iter [04400, 05004], lr: 0.179159, loss: 1.8133
2022-07-11 14:36:39 - train: epoch 0025, iter [04500, 05004], lr: 0.179118, loss: 1.9197
2022-07-11 14:37:33 - train: epoch 0025, iter [04600, 05004], lr: 0.179078, loss: 1.8158
2022-07-11 14:38:27 - train: epoch 0025, iter [04700, 05004], lr: 0.179037, loss: 1.8131
2022-07-11 14:39:21 - train: epoch 0025, iter [04800, 05004], lr: 0.178997, loss: 1.9338
2022-07-11 14:40:16 - train: epoch 0025, iter [04900, 05004], lr: 0.178956, loss: 2.0334
2022-07-11 14:41:10 - train: epoch 0025, iter [05000, 05004], lr: 0.178916, loss: 2.1538
2022-07-11 14:41:12 - train: epoch 025, train_loss: 1.9571
2022-07-11 14:43:20 - eval: epoch: 025, acc1: 59.412%, acc5: 83.048%, test_loss: 1.7041, per_image_load_time: 2.224ms, per_image_inference_time: 0.855ms
2022-07-11 14:43:20 - until epoch: 025, best_acc1: 60.116%
2022-07-11 14:43:20 - epoch 026 lr: 0.178914
2022-07-11 14:44:27 - train: epoch 0026, iter [00100, 05004], lr: 0.178873, loss: 2.0114
2022-07-11 14:45:22 - train: epoch 0026, iter [00200, 05004], lr: 0.178833, loss: 1.7381
2022-07-11 14:46:17 - train: epoch 0026, iter [00300, 05004], lr: 0.178792, loss: 2.1126
2022-07-11 14:47:11 - train: epoch 0026, iter [00400, 05004], lr: 0.178751, loss: 1.8398
2022-07-11 14:48:05 - train: epoch 0026, iter [00500, 05004], lr: 0.178711, loss: 1.9749
2022-07-11 14:48:59 - train: epoch 0026, iter [00600, 05004], lr: 0.178670, loss: 2.1142
2022-07-11 14:49:53 - train: epoch 0026, iter [00700, 05004], lr: 0.178629, loss: 1.8774
2022-07-11 14:50:49 - train: epoch 0026, iter [00800, 05004], lr: 0.178588, loss: 1.7095
2022-07-11 14:51:44 - train: epoch 0026, iter [00900, 05004], lr: 0.178547, loss: 2.0698
2022-07-11 14:52:39 - train: epoch 0026, iter [01000, 05004], lr: 0.178506, loss: 2.0163
2022-07-11 14:53:34 - train: epoch 0026, iter [01100, 05004], lr: 0.178465, loss: 2.1249
2022-07-11 14:54:29 - train: epoch 0026, iter [01200, 05004], lr: 0.178424, loss: 1.9019
2022-07-11 14:55:25 - train: epoch 0026, iter [01300, 05004], lr: 0.178383, loss: 1.9149
2022-07-11 14:56:20 - train: epoch 0026, iter [01400, 05004], lr: 0.178342, loss: 1.9304
2022-07-11 14:57:15 - train: epoch 0026, iter [01500, 05004], lr: 0.178301, loss: 1.9585
2022-07-11 14:58:11 - train: epoch 0026, iter [01600, 05004], lr: 0.178260, loss: 1.9956
2022-07-11 14:59:07 - train: epoch 0026, iter [01700, 05004], lr: 0.178219, loss: 1.9790
2022-07-11 15:00:01 - train: epoch 0026, iter [01800, 05004], lr: 0.178178, loss: 2.1034
2022-07-11 15:00:55 - train: epoch 0026, iter [01900, 05004], lr: 0.178137, loss: 1.9359
2022-07-11 15:01:50 - train: epoch 0026, iter [02000, 05004], lr: 0.178095, loss: 1.9704
2022-07-11 15:02:46 - train: epoch 0026, iter [02100, 05004], lr: 0.178054, loss: 2.1874
2022-07-11 15:03:43 - train: epoch 0026, iter [02200, 05004], lr: 0.178013, loss: 2.1189
2022-07-11 15:04:39 - train: epoch 0026, iter [02300, 05004], lr: 0.177971, loss: 1.9831
2022-07-11 15:05:35 - train: epoch 0026, iter [02400, 05004], lr: 0.177930, loss: 2.1752
2022-07-11 15:06:29 - train: epoch 0026, iter [02500, 05004], lr: 0.177889, loss: 1.9215
2022-07-11 15:07:24 - train: epoch 0026, iter [02600, 05004], lr: 0.177847, loss: 2.0320
2022-07-11 15:08:18 - train: epoch 0026, iter [02700, 05004], lr: 0.177806, loss: 2.1583
2022-07-11 15:09:12 - train: epoch 0026, iter [02800, 05004], lr: 0.177764, loss: 2.2329
2022-07-11 15:10:08 - train: epoch 0026, iter [02900, 05004], lr: 0.177722, loss: 1.8987
2022-07-11 15:11:03 - train: epoch 0026, iter [03000, 05004], lr: 0.177681, loss: 1.9116
2022-07-11 15:11:59 - train: epoch 0026, iter [03100, 05004], lr: 0.177639, loss: 1.8570
2022-07-11 15:12:54 - train: epoch 0026, iter [03200, 05004], lr: 0.177598, loss: 2.0067
2022-07-11 15:13:49 - train: epoch 0026, iter [03300, 05004], lr: 0.177556, loss: 1.8017
2022-07-11 15:14:45 - train: epoch 0026, iter [03400, 05004], lr: 0.177514, loss: 2.0803
2022-07-11 15:15:41 - train: epoch 0026, iter [03500, 05004], lr: 0.177472, loss: 2.0360
2022-07-11 15:16:36 - train: epoch 0026, iter [03600, 05004], lr: 0.177431, loss: 1.8930
2022-07-11 15:17:32 - train: epoch 0026, iter [03700, 05004], lr: 0.177389, loss: 2.0262
2022-07-11 15:18:28 - train: epoch 0026, iter [03800, 05004], lr: 0.177347, loss: 2.0634
2022-07-11 15:19:24 - train: epoch 0026, iter [03900, 05004], lr: 0.177305, loss: 2.0695
2022-07-11 15:20:19 - train: epoch 0026, iter [04000, 05004], lr: 0.177263, loss: 1.9021
2022-07-11 15:21:15 - train: epoch 0026, iter [04100, 05004], lr: 0.177221, loss: 1.8351
2022-07-11 15:22:10 - train: epoch 0026, iter [04200, 05004], lr: 0.177179, loss: 1.9354
2022-07-11 15:23:06 - train: epoch 0026, iter [04300, 05004], lr: 0.177137, loss: 2.2176
2022-07-11 15:24:03 - train: epoch 0026, iter [04400, 05004], lr: 0.177095, loss: 2.2060
2022-07-11 15:24:59 - train: epoch 0026, iter [04500, 05004], lr: 0.177053, loss: 2.0490
2022-07-11 15:25:54 - train: epoch 0026, iter [04600, 05004], lr: 0.177011, loss: 1.9920
2022-07-11 15:26:48 - train: epoch 0026, iter [04700, 05004], lr: 0.176969, loss: 1.9081
2022-07-11 15:27:42 - train: epoch 0026, iter [04800, 05004], lr: 0.176926, loss: 1.9124
2022-07-11 15:28:37 - train: epoch 0026, iter [04900, 05004], lr: 0.176884, loss: 1.9366
2022-07-11 15:29:30 - train: epoch 0026, iter [05000, 05004], lr: 0.176842, loss: 2.0048
2022-07-11 15:29:32 - train: epoch 026, train_loss: 1.9442
2022-07-11 15:31:41 - eval: epoch: 026, acc1: 60.190%, acc5: 83.644%, test_loss: 1.6658, per_image_load_time: 4.061ms, per_image_inference_time: 0.850ms
2022-07-11 15:31:41 - until epoch: 026, best_acc1: 60.190%
2022-07-11 15:31:41 - epoch 027 lr: 0.176840
2022-07-11 15:32:47 - train: epoch 0027, iter [00100, 05004], lr: 0.176798, loss: 2.0511
2022-07-11 15:33:42 - train: epoch 0027, iter [00200, 05004], lr: 0.176755, loss: 1.9322
2022-07-11 15:34:37 - train: epoch 0027, iter [00300, 05004], lr: 0.176713, loss: 2.0288
2022-07-11 15:35:33 - train: epoch 0027, iter [00400, 05004], lr: 0.176671, loss: 1.9658
2022-07-11 15:36:28 - train: epoch 0027, iter [00500, 05004], lr: 0.176628, loss: 2.0310
2022-07-11 15:37:20 - train: epoch 0027, iter [00600, 05004], lr: 0.176586, loss: 2.1021
2022-07-11 15:38:14 - train: epoch 0027, iter [00700, 05004], lr: 0.176543, loss: 1.8649
2022-07-11 15:39:09 - train: epoch 0027, iter [00800, 05004], lr: 0.176501, loss: 1.9649
2022-07-11 15:40:04 - train: epoch 0027, iter [00900, 05004], lr: 0.176458, loss: 1.7802
2022-07-11 15:40:59 - train: epoch 0027, iter [01000, 05004], lr: 0.176416, loss: 1.9800
2022-07-11 15:41:53 - train: epoch 0027, iter [01100, 05004], lr: 0.176373, loss: 1.8395
2022-07-11 15:42:48 - train: epoch 0027, iter [01200, 05004], lr: 0.176330, loss: 2.0293
2022-07-11 15:43:43 - train: epoch 0027, iter [01300, 05004], lr: 0.176287, loss: 2.0115
2022-07-11 15:44:38 - train: epoch 0027, iter [01400, 05004], lr: 0.176245, loss: 1.9863
2022-07-11 15:45:33 - train: epoch 0027, iter [01500, 05004], lr: 0.176202, loss: 2.0065
2022-07-11 15:46:28 - train: epoch 0027, iter [01600, 05004], lr: 0.176159, loss: 1.9759
2022-07-11 15:47:23 - train: epoch 0027, iter [01700, 05004], lr: 0.176116, loss: 1.8932
2022-07-11 15:48:18 - train: epoch 0027, iter [01800, 05004], lr: 0.176073, loss: 1.6972
2022-07-11 15:49:12 - train: epoch 0027, iter [01900, 05004], lr: 0.176031, loss: 1.8702
2022-07-11 15:50:07 - train: epoch 0027, iter [02000, 05004], lr: 0.175988, loss: 1.9154
2022-07-11 15:51:03 - train: epoch 0027, iter [02100, 05004], lr: 0.175945, loss: 2.0540
2022-07-11 15:51:59 - train: epoch 0027, iter [02200, 05004], lr: 0.175902, loss: 2.1482
2022-07-11 15:52:55 - train: epoch 0027, iter [02300, 05004], lr: 0.175859, loss: 2.3377
2022-07-11 15:53:51 - train: epoch 0027, iter [02400, 05004], lr: 0.175815, loss: 1.9588
2022-07-11 15:54:45 - train: epoch 0027, iter [02500, 05004], lr: 0.175772, loss: 1.8053
2022-07-11 15:55:40 - train: epoch 0027, iter [02600, 05004], lr: 0.175729, loss: 2.0185
2022-07-11 15:56:35 - train: epoch 0027, iter [02700, 05004], lr: 0.175686, loss: 1.9837
2022-07-11 15:57:31 - train: epoch 0027, iter [02800, 05004], lr: 0.175643, loss: 1.9836
2022-07-11 15:58:27 - train: epoch 0027, iter [02900, 05004], lr: 0.175600, loss: 2.0206
2022-07-11 15:59:23 - train: epoch 0027, iter [03000, 05004], lr: 0.175556, loss: 1.7979
2022-07-11 16:00:18 - train: epoch 0027, iter [03100, 05004], lr: 0.175513, loss: 1.8931
2022-07-11 16:01:12 - train: epoch 0027, iter [03200, 05004], lr: 0.175470, loss: 1.8165
2022-07-11 16:02:07 - train: epoch 0027, iter [03300, 05004], lr: 0.175426, loss: 2.0795
2022-07-11 16:03:03 - train: epoch 0027, iter [03400, 05004], lr: 0.175383, loss: 1.9846
2022-07-11 16:03:58 - train: epoch 0027, iter [03500, 05004], lr: 0.175339, loss: 2.2130
2022-07-11 16:04:53 - train: epoch 0027, iter [03600, 05004], lr: 0.175296, loss: 2.0802
2022-07-11 16:05:48 - train: epoch 0027, iter [03700, 05004], lr: 0.175252, loss: 1.8958
2022-07-11 16:06:44 - train: epoch 0027, iter [03800, 05004], lr: 0.175209, loss: 1.8628
2022-07-11 16:07:39 - train: epoch 0027, iter [03900, 05004], lr: 0.175165, loss: 1.9951
2022-07-11 16:08:35 - train: epoch 0027, iter [04000, 05004], lr: 0.175122, loss: 2.0099
2022-07-11 16:09:32 - train: epoch 0027, iter [04100, 05004], lr: 0.175078, loss: 1.8275
2022-07-11 16:10:26 - train: epoch 0027, iter [04200, 05004], lr: 0.175034, loss: 1.9883
2022-07-11 16:11:22 - train: epoch 0027, iter [04300, 05004], lr: 0.174991, loss: 1.8139
2022-07-11 16:12:16 - train: epoch 0027, iter [04400, 05004], lr: 0.174947, loss: 1.9388
2022-07-11 16:13:10 - train: epoch 0027, iter [04500, 05004], lr: 0.174903, loss: 1.7580
2022-07-11 16:14:03 - train: epoch 0027, iter [04600, 05004], lr: 0.174859, loss: 1.8053
2022-07-11 16:14:56 - train: epoch 0027, iter [04700, 05004], lr: 0.174816, loss: 2.2078
2022-07-11 16:15:49 - train: epoch 0027, iter [04800, 05004], lr: 0.174772, loss: 2.0815
2022-07-11 16:16:42 - train: epoch 0027, iter [04900, 05004], lr: 0.174728, loss: 1.7571
2022-07-11 16:17:34 - train: epoch 0027, iter [05000, 05004], lr: 0.174684, loss: 1.6203
2022-07-11 16:17:37 - train: epoch 027, train_loss: 1.9333
2022-07-11 16:19:44 - eval: epoch: 027, acc1: 60.354%, acc5: 83.938%, test_loss: 1.6623, per_image_load_time: 3.949ms, per_image_inference_time: 0.858ms
2022-07-11 16:19:44 - until epoch: 027, best_acc1: 60.354%
2022-07-11 16:19:44 - epoch 028 lr: 0.174682
2022-07-11 16:20:51 - train: epoch 0028, iter [00100, 05004], lr: 0.174638, loss: 1.6712
2022-07-11 16:21:45 - train: epoch 0028, iter [00200, 05004], lr: 0.174594, loss: 1.8857
2022-07-11 16:22:40 - train: epoch 0028, iter [00300, 05004], lr: 0.174550, loss: 1.8734
2022-07-11 16:23:34 - train: epoch 0028, iter [00400, 05004], lr: 0.174506, loss: 2.0369
2022-07-11 16:24:29 - train: epoch 0028, iter [00500, 05004], lr: 0.174462, loss: 1.7707
2022-07-11 16:25:23 - train: epoch 0028, iter [00600, 05004], lr: 0.174418, loss: 2.1080
2022-07-11 16:26:16 - train: epoch 0028, iter [00700, 05004], lr: 0.174374, loss: 2.1829
2022-07-11 16:27:10 - train: epoch 0028, iter [00800, 05004], lr: 0.174330, loss: 1.7359
2022-07-11 16:28:04 - train: epoch 0028, iter [00900, 05004], lr: 0.174285, loss: 1.7495
2022-07-11 16:28:58 - train: epoch 0028, iter [01000, 05004], lr: 0.174241, loss: 2.0993
2022-07-11 16:29:52 - train: epoch 0028, iter [01100, 05004], lr: 0.174197, loss: 1.8279
2022-07-11 16:30:46 - train: epoch 0028, iter [01200, 05004], lr: 0.174152, loss: 1.7140
2022-07-11 16:31:42 - train: epoch 0028, iter [01300, 05004], lr: 0.174108, loss: 1.7313
2022-07-11 16:32:36 - train: epoch 0028, iter [01400, 05004], lr: 0.174064, loss: 1.9911
2022-07-11 16:33:31 - train: epoch 0028, iter [01500, 05004], lr: 0.174019, loss: 1.9846
2022-07-11 16:34:27 - train: epoch 0028, iter [01600, 05004], lr: 0.173975, loss: 1.9565
2022-07-11 16:35:22 - train: epoch 0028, iter [01700, 05004], lr: 0.173930, loss: 1.8670
2022-07-11 16:36:16 - train: epoch 0028, iter [01800, 05004], lr: 0.173886, loss: 1.9891
2022-07-11 16:37:09 - train: epoch 0028, iter [01900, 05004], lr: 0.173841, loss: 1.9841
2022-07-11 16:38:02 - train: epoch 0028, iter [02000, 05004], lr: 0.173797, loss: 2.0008
2022-07-11 16:38:56 - train: epoch 0028, iter [02100, 05004], lr: 0.173752, loss: 1.9903
2022-07-11 16:39:51 - train: epoch 0028, iter [02200, 05004], lr: 0.173707, loss: 2.0344
2022-07-11 16:40:46 - train: epoch 0028, iter [02300, 05004], lr: 0.173663, loss: 2.0107
2022-07-11 16:41:42 - train: epoch 0028, iter [02400, 05004], lr: 0.173618, loss: 2.0162
2022-07-11 16:42:37 - train: epoch 0028, iter [02500, 05004], lr: 0.173573, loss: 1.8969
2022-07-11 16:43:33 - train: epoch 0028, iter [02600, 05004], lr: 0.173529, loss: 1.9353
2022-07-11 16:44:29 - train: epoch 0028, iter [02700, 05004], lr: 0.173484, loss: 1.8291
2022-07-11 16:45:24 - train: epoch 0028, iter [02800, 05004], lr: 0.173439, loss: 1.9151
2022-07-11 16:46:21 - train: epoch 0028, iter [02900, 05004], lr: 0.173394, loss: 2.0812
2022-07-11 16:47:16 - train: epoch 0028, iter [03000, 05004], lr: 0.173349, loss: 1.8590
2022-07-11 16:48:12 - train: epoch 0028, iter [03100, 05004], lr: 0.173304, loss: 2.0576
2022-07-11 16:49:08 - train: epoch 0028, iter [03200, 05004], lr: 0.173259, loss: 1.7936
2022-07-11 16:50:03 - train: epoch 0028, iter [03300, 05004], lr: 0.173214, loss: 1.9674
2022-07-11 16:50:58 - train: epoch 0028, iter [03400, 05004], lr: 0.173169, loss: 2.0368
2022-07-11 16:51:51 - train: epoch 0028, iter [03500, 05004], lr: 0.173124, loss: 1.7927
2022-07-11 16:52:46 - train: epoch 0028, iter [03600, 05004], lr: 0.173079, loss: 1.8057
2022-07-11 16:53:40 - train: epoch 0028, iter [03700, 05004], lr: 0.173034, loss: 1.8998
2022-07-11 16:54:34 - train: epoch 0028, iter [03800, 05004], lr: 0.172989, loss: 1.7519
2022-07-11 16:55:30 - train: epoch 0028, iter [03900, 05004], lr: 0.172944, loss: 2.0681
2022-07-11 16:56:24 - train: epoch 0028, iter [04000, 05004], lr: 0.172898, loss: 2.0779
2022-07-11 16:57:19 - train: epoch 0028, iter [04100, 05004], lr: 0.172853, loss: 1.8145
2022-07-11 16:58:15 - train: epoch 0028, iter [04200, 05004], lr: 0.172808, loss: 1.9587
2022-07-11 16:59:09 - train: epoch 0028, iter [04300, 05004], lr: 0.172762, loss: 1.8684
2022-07-11 17:00:03 - train: epoch 0028, iter [04400, 05004], lr: 0.172717, loss: 1.9300
2022-07-11 17:00:57 - train: epoch 0028, iter [04500, 05004], lr: 0.172672, loss: 1.9159
2022-07-11 17:01:51 - train: epoch 0028, iter [04600, 05004], lr: 0.172626, loss: 2.0135
2022-07-11 17:02:45 - train: epoch 0028, iter [04700, 05004], lr: 0.172581, loss: 2.0674
2022-07-11 17:03:39 - train: epoch 0028, iter [04800, 05004], lr: 0.172535, loss: 1.9405
2022-07-11 17:04:33 - train: epoch 0028, iter [04900, 05004], lr: 0.172490, loss: 2.0643
2022-07-11 17:05:26 - train: epoch 0028, iter [05000, 05004], lr: 0.172444, loss: 1.6127
2022-07-11 17:05:28 - train: epoch 028, train_loss: 1.9254
2022-07-11 17:07:36 - eval: epoch: 028, acc1: 60.750%, acc5: 83.870%, test_loss: 1.6605, per_image_load_time: 4.108ms, per_image_inference_time: 0.836ms
2022-07-11 17:07:36 - until epoch: 028, best_acc1: 60.750%
2022-07-11 17:07:36 - epoch 029 lr: 0.172442
2022-07-11 17:08:45 - train: epoch 0029, iter [00100, 05004], lr: 0.172397, loss: 2.0980
2022-07-11 17:09:41 - train: epoch 0029, iter [00200, 05004], lr: 0.172351, loss: 1.8093
2022-07-11 17:10:37 - train: epoch 0029, iter [00300, 05004], lr: 0.172306, loss: 2.0757
2022-07-11 17:11:33 - train: epoch 0029, iter [00400, 05004], lr: 0.172260, loss: 1.8922
2022-07-11 17:12:29 - train: epoch 0029, iter [00500, 05004], lr: 0.172214, loss: 1.9448
2022-07-11 17:13:26 - train: epoch 0029, iter [00600, 05004], lr: 0.172169, loss: 1.9918
2022-07-11 17:14:22 - train: epoch 0029, iter [00700, 05004], lr: 0.172123, loss: 1.7445
2022-07-11 17:15:18 - train: epoch 0029, iter [00800, 05004], lr: 0.172077, loss: 1.6654
2022-07-11 17:16:15 - train: epoch 0029, iter [00900, 05004], lr: 0.172031, loss: 1.9534
2022-07-11 17:17:11 - train: epoch 0029, iter [01000, 05004], lr: 0.171985, loss: 1.7558
2022-07-11 17:18:07 - train: epoch 0029, iter [01100, 05004], lr: 0.171939, loss: 1.8154
2022-07-11 17:19:04 - train: epoch 0029, iter [01200, 05004], lr: 0.171894, loss: 1.8576
2022-07-11 17:19:59 - train: epoch 0029, iter [01300, 05004], lr: 0.171848, loss: 1.8564
2022-07-11 17:20:55 - train: epoch 0029, iter [01400, 05004], lr: 0.171802, loss: 1.8641
2022-07-11 17:21:50 - train: epoch 0029, iter [01500, 05004], lr: 0.171756, loss: 2.0073
2022-07-11 17:22:45 - train: epoch 0029, iter [01600, 05004], lr: 0.171710, loss: 1.8904
2022-07-11 17:23:41 - train: epoch 0029, iter [01700, 05004], lr: 0.171664, loss: 1.9606
2022-07-11 17:24:36 - train: epoch 0029, iter [01800, 05004], lr: 0.171617, loss: 1.9381
2022-07-11 17:25:31 - train: epoch 0029, iter [01900, 05004], lr: 0.171571, loss: 1.7279
2022-07-11 17:26:25 - train: epoch 0029, iter [02000, 05004], lr: 0.171525, loss: 1.9514
2022-07-11 17:27:22 - train: epoch 0029, iter [02100, 05004], lr: 0.171479, loss: 1.8324
2022-07-11 17:28:18 - train: epoch 0029, iter [02200, 05004], lr: 0.171433, loss: 1.9309
2022-07-11 17:29:15 - train: epoch 0029, iter [02300, 05004], lr: 0.171386, loss: 1.9610
2022-07-11 17:30:11 - train: epoch 0029, iter [02400, 05004], lr: 0.171340, loss: 1.9086
2022-07-11 17:31:08 - train: epoch 0029, iter [02500, 05004], lr: 0.171294, loss: 1.9407
2022-07-11 17:32:05 - train: epoch 0029, iter [02600, 05004], lr: 0.171247, loss: 2.1009
2022-07-11 17:33:02 - train: epoch 0029, iter [02700, 05004], lr: 0.171201, loss: 1.8737
2022-07-11 17:33:59 - train: epoch 0029, iter [02800, 05004], lr: 0.171155, loss: 1.7551
2022-07-11 17:34:56 - train: epoch 0029, iter [02900, 05004], lr: 0.171108, loss: 1.9895
2022-07-11 17:35:54 - train: epoch 0029, iter [03000, 05004], lr: 0.171062, loss: 1.8624
2022-07-11 17:36:51 - train: epoch 0029, iter [03100, 05004], lr: 0.171015, loss: 1.9640
2022-07-11 17:37:47 - train: epoch 0029, iter [03200, 05004], lr: 0.170969, loss: 2.0067
2022-07-11 17:38:41 - train: epoch 0029, iter [03300, 05004], lr: 0.170922, loss: 1.7771
2022-07-11 17:39:36 - train: epoch 0029, iter [03400, 05004], lr: 0.170875, loss: 1.5833
2022-07-11 17:40:32 - train: epoch 0029, iter [03500, 05004], lr: 0.170829, loss: 2.1328
2022-07-11 17:41:26 - train: epoch 0029, iter [03600, 05004], lr: 0.170782, loss: 1.8036
2022-07-11 17:42:20 - train: epoch 0029, iter [03700, 05004], lr: 0.170735, loss: 1.8877
2022-07-11 17:43:15 - train: epoch 0029, iter [03800, 05004], lr: 0.170689, loss: 1.7830
2022-07-11 17:44:11 - train: epoch 0029, iter [03900, 05004], lr: 0.170642, loss: 1.8265
2022-07-11 17:45:08 - train: epoch 0029, iter [04000, 05004], lr: 0.170595, loss: 1.9535
2022-07-11 17:46:04 - train: epoch 0029, iter [04100, 05004], lr: 0.170548, loss: 1.8309
2022-07-11 17:47:01 - train: epoch 0029, iter [04200, 05004], lr: 0.170501, loss: 1.8006
2022-07-11 17:47:57 - train: epoch 0029, iter [04300, 05004], lr: 0.170455, loss: 1.8654
2022-07-11 17:48:55 - train: epoch 0029, iter [04400, 05004], lr: 0.170408, loss: 1.8570
2022-07-11 17:49:53 - train: epoch 0029, iter [04500, 05004], lr: 0.170361, loss: 2.2226
2022-07-11 17:50:50 - train: epoch 0029, iter [04600, 05004], lr: 0.170314, loss: 1.8509
2022-07-11 17:51:45 - train: epoch 0029, iter [04700, 05004], lr: 0.170267, loss: 1.6371
2022-07-11 17:52:40 - train: epoch 0029, iter [04800, 05004], lr: 0.170220, loss: 1.7579
2022-07-11 17:53:35 - train: epoch 0029, iter [04900, 05004], lr: 0.170173, loss: 2.0119
2022-07-11 17:54:30 - train: epoch 0029, iter [05000, 05004], lr: 0.170126, loss: 1.6684
2022-07-11 17:54:32 - train: epoch 029, train_loss: 1.9135
2022-07-11 17:56:37 - eval: epoch: 029, acc1: 60.540%, acc5: 83.998%, test_loss: 1.6400, per_image_load_time: 1.877ms, per_image_inference_time: 0.811ms
2022-07-11 17:56:37 - until epoch: 029, best_acc1: 60.750%
2022-07-11 17:56:37 - epoch 030 lr: 0.170123
2022-07-11 17:57:47 - train: epoch 0030, iter [00100, 05004], lr: 0.170077, loss: 1.8239
2022-07-11 17:58:44 - train: epoch 0030, iter [00200, 05004], lr: 0.170029, loss: 1.7817
2022-07-11 17:59:39 - train: epoch 0030, iter [00300, 05004], lr: 0.169982, loss: 1.6119
2022-07-11 18:00:34 - train: epoch 0030, iter [00400, 05004], lr: 0.169935, loss: 1.7150
2022-07-11 18:01:29 - train: epoch 0030, iter [00500, 05004], lr: 0.169888, loss: 1.8382
2022-07-11 18:02:25 - train: epoch 0030, iter [00600, 05004], lr: 0.169840, loss: 1.8897
2022-07-11 18:03:21 - train: epoch 0030, iter [00700, 05004], lr: 0.169793, loss: 1.9616
2022-07-11 18:04:17 - train: epoch 0030, iter [00800, 05004], lr: 0.169746, loss: 1.7260
2022-07-11 18:05:11 - train: epoch 0030, iter [00900, 05004], lr: 0.169698, loss: 1.9047
2022-07-11 18:06:06 - train: epoch 0030, iter [01000, 05004], lr: 0.169651, loss: 1.7101
2022-07-11 18:07:02 - train: epoch 0030, iter [01100, 05004], lr: 0.169604, loss: 1.6731
2022-07-11 18:07:59 - train: epoch 0030, iter [01200, 05004], lr: 0.169556, loss: 1.9313
2022-07-11 18:08:56 - train: epoch 0030, iter [01300, 05004], lr: 0.169509, loss: 1.8475
2022-07-11 18:09:55 - train: epoch 0030, iter [01400, 05004], lr: 0.169461, loss: 1.9967
2022-07-11 18:10:54 - train: epoch 0030, iter [01500, 05004], lr: 0.169414, loss: 1.9430
2022-07-11 18:11:53 - train: epoch 0030, iter [01600, 05004], lr: 0.169366, loss: 1.8863
2022-07-11 18:12:51 - train: epoch 0030, iter [01700, 05004], lr: 0.169318, loss: 2.0396
2022-07-11 18:13:49 - train: epoch 0030, iter [01800, 05004], lr: 0.169271, loss: 1.8563
2022-07-11 18:14:47 - train: epoch 0030, iter [01900, 05004], lr: 0.169223, loss: 1.8549
2022-07-11 18:15:44 - train: epoch 0030, iter [02000, 05004], lr: 0.169175, loss: 1.8677
2022-07-11 18:16:43 - train: epoch 0030, iter [02100, 05004], lr: 0.169128, loss: 2.1691
2022-07-11 18:17:40 - train: epoch 0030, iter [02200, 05004], lr: 0.169080, loss: 1.9959
2022-07-11 18:18:39 - train: epoch 0030, iter [02300, 05004], lr: 0.169032, loss: 1.7738
2022-07-11 18:19:37 - train: epoch 0030, iter [02400, 05004], lr: 0.168984, loss: 1.8946
2022-07-11 18:20:36 - train: epoch 0030, iter [02500, 05004], lr: 0.168936, loss: 2.0510
2022-07-11 18:21:33 - train: epoch 0030, iter [02600, 05004], lr: 0.168888, loss: 1.9184
2022-07-11 18:22:31 - train: epoch 0030, iter [02700, 05004], lr: 0.168840, loss: 1.9286
2022-07-11 18:23:28 - train: epoch 0030, iter [02800, 05004], lr: 0.168793, loss: 1.8714
2022-07-11 18:24:27 - train: epoch 0030, iter [02900, 05004], lr: 0.168745, loss: 1.7723
2022-07-11 18:25:26 - train: epoch 0030, iter [03000, 05004], lr: 0.168697, loss: 1.9893
2022-07-11 18:26:25 - train: epoch 0030, iter [03100, 05004], lr: 0.168649, loss: 2.0117
2022-07-11 18:27:23 - train: epoch 0030, iter [03200, 05004], lr: 0.168600, loss: 1.9942
2022-07-11 18:28:18 - train: epoch 0030, iter [03300, 05004], lr: 0.168552, loss: 1.9434
2022-07-11 18:29:11 - train: epoch 0030, iter [03400, 05004], lr: 0.168504, loss: 1.8666
2022-07-11 18:30:04 - train: epoch 0030, iter [03500, 05004], lr: 0.168456, loss: 1.9057
2022-07-11 18:30:57 - train: epoch 0030, iter [03600, 05004], lr: 0.168408, loss: 1.9672
2022-07-11 18:31:51 - train: epoch 0030, iter [03700, 05004], lr: 0.168360, loss: 1.7949
2022-07-11 18:32:44 - train: epoch 0030, iter [03800, 05004], lr: 0.168311, loss: 1.9887
2022-07-11 18:33:36 - train: epoch 0030, iter [03900, 05004], lr: 0.168263, loss: 1.7918
2022-07-11 18:34:28 - train: epoch 0030, iter [04000, 05004], lr: 0.168215, loss: 1.7440
2022-07-11 18:35:21 - train: epoch 0030, iter [04100, 05004], lr: 0.168166, loss: 1.8292
2022-07-11 18:36:16 - train: epoch 0030, iter [04200, 05004], lr: 0.168118, loss: 2.2018
2022-07-11 18:37:09 - train: epoch 0030, iter [04300, 05004], lr: 0.168070, loss: 1.9929
2022-07-11 18:38:03 - train: epoch 0030, iter [04400, 05004], lr: 0.168021, loss: 1.8871
2022-07-11 18:38:55 - train: epoch 0030, iter [04500, 05004], lr: 0.167973, loss: 2.2047
2022-07-11 18:39:47 - train: epoch 0030, iter [04600, 05004], lr: 0.167924, loss: 1.8975
2022-07-11 18:40:40 - train: epoch 0030, iter [04700, 05004], lr: 0.167876, loss: 1.8931
2022-07-11 18:41:34 - train: epoch 0030, iter [04800, 05004], lr: 0.167827, loss: 1.8961
2022-07-11 18:42:27 - train: epoch 0030, iter [04900, 05004], lr: 0.167779, loss: 2.0078
2022-07-11 18:43:19 - train: epoch 0030, iter [05000, 05004], lr: 0.167730, loss: 1.8622
2022-07-11 18:43:22 - train: epoch 030, train_loss: 1.9039
2022-07-11 18:45:28 - eval: epoch: 030, acc1: 61.244%, acc5: 84.146%, test_loss: 1.6361, per_image_load_time: 3.983ms, per_image_inference_time: 0.821ms
2022-07-11 18:45:28 - until epoch: 030, best_acc1: 61.244%
2022-07-11 18:45:28 - epoch 031 lr: 0.167728
2022-07-11 18:46:34 - train: epoch 0031, iter [00100, 05004], lr: 0.167680, loss: 1.7544
2022-07-11 18:47:29 - train: epoch 0031, iter [00200, 05004], lr: 0.167631, loss: 1.9012
2022-07-11 18:48:23 - train: epoch 0031, iter [00300, 05004], lr: 0.167582, loss: 1.6369
2022-07-11 18:49:17 - train: epoch 0031, iter [00400, 05004], lr: 0.167533, loss: 1.8901
2022-07-11 18:50:10 - train: epoch 0031, iter [00500, 05004], lr: 0.167485, loss: 1.7930
2022-07-11 18:51:04 - train: epoch 0031, iter [00600, 05004], lr: 0.167436, loss: 1.9806
2022-07-11 18:51:58 - train: epoch 0031, iter [00700, 05004], lr: 0.167387, loss: 1.9300
2022-07-11 18:52:51 - train: epoch 0031, iter [00800, 05004], lr: 0.167338, loss: 1.8571
2022-07-11 18:53:43 - train: epoch 0031, iter [00900, 05004], lr: 0.167289, loss: 1.7785
2022-07-11 18:54:36 - train: epoch 0031, iter [01000, 05004], lr: 0.167240, loss: 2.2401
2022-07-11 18:55:28 - train: epoch 0031, iter [01100, 05004], lr: 0.167192, loss: 1.8172
2022-07-11 18:56:21 - train: epoch 0031, iter [01200, 05004], lr: 0.167143, loss: 2.0396
2022-07-11 18:57:14 - train: epoch 0031, iter [01300, 05004], lr: 0.167094, loss: 1.5467
2022-07-11 18:58:07 - train: epoch 0031, iter [01400, 05004], lr: 0.167045, loss: 1.7608
2022-07-11 18:59:00 - train: epoch 0031, iter [01500, 05004], lr: 0.166996, loss: 2.0332
2022-07-11 18:59:56 - train: epoch 0031, iter [01600, 05004], lr: 0.166946, loss: 1.9384
2022-07-11 19:00:49 - train: epoch 0031, iter [01700, 05004], lr: 0.166897, loss: 1.8167
2022-07-11 19:01:43 - train: epoch 0031, iter [01800, 05004], lr: 0.166848, loss: 1.6169
2022-07-11 19:02:36 - train: epoch 0031, iter [01900, 05004], lr: 0.166799, loss: 2.0188
2022-07-11 19:03:31 - train: epoch 0031, iter [02000, 05004], lr: 0.166750, loss: 1.8114
2022-07-11 19:04:24 - train: epoch 0031, iter [02100, 05004], lr: 0.166701, loss: 1.8661
2022-07-11 19:05:17 - train: epoch 0031, iter [02200, 05004], lr: 0.166651, loss: 1.7687
2022-07-11 19:06:11 - train: epoch 0031, iter [02300, 05004], lr: 0.166602, loss: 1.7027
2022-07-11 19:07:05 - train: epoch 0031, iter [02400, 05004], lr: 0.166553, loss: 2.0502
2022-07-11 19:07:58 - train: epoch 0031, iter [02500, 05004], lr: 0.166503, loss: 1.7041
2022-07-11 19:08:52 - train: epoch 0031, iter [02600, 05004], lr: 0.166454, loss: 1.9636
2022-07-11 19:09:46 - train: epoch 0031, iter [02700, 05004], lr: 0.166405, loss: 2.2628
2022-07-11 19:10:42 - train: epoch 0031, iter [02800, 05004], lr: 0.166355, loss: 2.2095
2022-07-11 19:11:35 - train: epoch 0031, iter [02900, 05004], lr: 0.166306, loss: 1.9438
2022-07-11 19:12:29 - train: epoch 0031, iter [03000, 05004], lr: 0.166256, loss: 2.0637
2022-07-11 19:13:23 - train: epoch 0031, iter [03100, 05004], lr: 0.166207, loss: 1.8389
2022-07-11 19:14:16 - train: epoch 0031, iter [03200, 05004], lr: 0.166157, loss: 1.9987
2022-07-11 19:15:11 - train: epoch 0031, iter [03300, 05004], lr: 0.166108, loss: 1.8340
2022-07-11 19:16:05 - train: epoch 0031, iter [03400, 05004], lr: 0.166058, loss: 2.0150
2022-07-11 19:16:59 - train: epoch 0031, iter [03500, 05004], lr: 0.166008, loss: 2.0988
2022-07-11 19:17:54 - train: epoch 0031, iter [03600, 05004], lr: 0.165959, loss: 1.9018
2022-07-11 19:18:48 - train: epoch 0031, iter [03700, 05004], lr: 0.165909, loss: 1.8646
2022-07-11 19:19:42 - train: epoch 0031, iter [03800, 05004], lr: 0.165859, loss: 1.9086
2022-07-11 19:20:35 - train: epoch 0031, iter [03900, 05004], lr: 0.165810, loss: 1.9873
2022-07-11 19:21:30 - train: epoch 0031, iter [04000, 05004], lr: 0.165760, loss: 1.9489
2022-07-11 19:22:24 - train: epoch 0031, iter [04100, 05004], lr: 0.165710, loss: 1.9756
2022-07-11 19:23:18 - train: epoch 0031, iter [04200, 05004], lr: 0.165660, loss: 2.0241
2022-07-11 19:24:12 - train: epoch 0031, iter [04300, 05004], lr: 0.165610, loss: 2.0105
2022-07-11 19:25:07 - train: epoch 0031, iter [04400, 05004], lr: 0.165561, loss: 2.1041
2022-07-11 19:26:00 - train: epoch 0031, iter [04500, 05004], lr: 0.165511, loss: 2.0187
2022-07-11 19:26:54 - train: epoch 0031, iter [04600, 05004], lr: 0.165461, loss: 2.1540
2022-07-11 19:27:48 - train: epoch 0031, iter [04700, 05004], lr: 0.165411, loss: 1.6054
2022-07-11 19:28:42 - train: epoch 0031, iter [04800, 05004], lr: 0.165361, loss: 1.7602
2022-07-11 19:29:36 - train: epoch 0031, iter [04900, 05004], lr: 0.165311, loss: 1.8065
2022-07-11 19:30:28 - train: epoch 0031, iter [05000, 05004], lr: 0.165261, loss: 1.8725
2022-07-11 19:30:31 - train: epoch 031, train_loss: 1.8921
2022-07-11 19:32:40 - eval: epoch: 031, acc1: 61.200%, acc5: 84.386%, test_loss: 1.6264, per_image_load_time: 4.048ms, per_image_inference_time: 0.836ms
2022-07-11 19:32:40 - until epoch: 031, best_acc1: 61.244%
2022-07-11 19:32:40 - epoch 032 lr: 0.165258
2022-07-11 19:33:47 - train: epoch 0032, iter [00100, 05004], lr: 0.165208, loss: 1.7476
2022-07-11 19:34:41 - train: epoch 0032, iter [00200, 05004], lr: 0.165158, loss: 1.9258
2022-07-11 19:35:34 - train: epoch 0032, iter [00300, 05004], lr: 0.165108, loss: 1.7213
2022-07-11 19:36:27 - train: epoch 0032, iter [00400, 05004], lr: 0.165058, loss: 1.8158
2022-07-11 19:37:20 - train: epoch 0032, iter [00500, 05004], lr: 0.165008, loss: 1.8093
2022-07-11 19:38:13 - train: epoch 0032, iter [00600, 05004], lr: 0.164958, loss: 1.7069
2022-07-11 19:39:07 - train: epoch 0032, iter [00700, 05004], lr: 0.164907, loss: 1.7231
2022-07-11 19:39:59 - train: epoch 0032, iter [00800, 05004], lr: 0.164857, loss: 1.7257
2022-07-11 19:40:52 - train: epoch 0032, iter [00900, 05004], lr: 0.164807, loss: 1.9420
2022-07-11 19:41:44 - train: epoch 0032, iter [01000, 05004], lr: 0.164756, loss: 1.8368
2022-07-11 19:42:38 - train: epoch 0032, iter [01100, 05004], lr: 0.164706, loss: 1.8518
2022-07-11 19:43:32 - train: epoch 0032, iter [01200, 05004], lr: 0.164656, loss: 1.8908
2022-07-11 19:44:25 - train: epoch 0032, iter [01300, 05004], lr: 0.164605, loss: 1.9653
2022-07-11 19:45:19 - train: epoch 0032, iter [01400, 05004], lr: 0.164555, loss: 2.0803
2022-07-11 19:46:12 - train: epoch 0032, iter [01500, 05004], lr: 0.164504, loss: 1.8541
2022-07-11 19:47:05 - train: epoch 0032, iter [01600, 05004], lr: 0.164454, loss: 2.0243
2022-07-11 19:47:59 - train: epoch 0032, iter [01700, 05004], lr: 0.164403, loss: 1.8863
2022-07-11 19:48:52 - train: epoch 0032, iter [01800, 05004], lr: 0.164353, loss: 2.1795
2022-07-11 19:49:45 - train: epoch 0032, iter [01900, 05004], lr: 0.164302, loss: 1.7851
2022-07-11 19:50:38 - train: epoch 0032, iter [02000, 05004], lr: 0.164251, loss: 1.8940
2022-07-11 19:51:31 - train: epoch 0032, iter [02100, 05004], lr: 0.164201, loss: 1.7164
2022-07-11 19:52:25 - train: epoch 0032, iter [02200, 05004], lr: 0.164150, loss: 1.9897
2022-07-11 19:53:18 - train: epoch 0032, iter [02300, 05004], lr: 0.164099, loss: 1.9756
2022-07-11 19:54:13 - train: epoch 0032, iter [02400, 05004], lr: 0.164049, loss: 1.8902
2022-07-11 19:55:08 - train: epoch 0032, iter [02500, 05004], lr: 0.163998, loss: 1.9542
2022-07-11 19:56:04 - train: epoch 0032, iter [02600, 05004], lr: 0.163947, loss: 1.6256
2022-07-11 19:57:01 - train: epoch 0032, iter [02700, 05004], lr: 0.163896, loss: 1.8667
2022-07-11 19:57:57 - train: epoch 0032, iter [02800, 05004], lr: 0.163845, loss: 1.9242
2022-07-11 19:58:54 - train: epoch 0032, iter [02900, 05004], lr: 0.163795, loss: 1.8457
2022-07-11 19:59:52 - train: epoch 0032, iter [03000, 05004], lr: 0.163744, loss: 1.7762
2022-07-11 20:00:48 - train: epoch 0032, iter [03100, 05004], lr: 0.163693, loss: 1.8763
2022-07-11 20:01:43 - train: epoch 0032, iter [03200, 05004], lr: 0.163642, loss: 1.9708
2022-07-11 20:02:40 - train: epoch 0032, iter [03300, 05004], lr: 0.163591, loss: 1.8773
2022-07-11 20:03:35 - train: epoch 0032, iter [03400, 05004], lr: 0.163540, loss: 1.7424
2022-07-11 20:04:30 - train: epoch 0032, iter [03500, 05004], lr: 0.163489, loss: 1.8673
2022-07-11 20:05:25 - train: epoch 0032, iter [03600, 05004], lr: 0.163438, loss: 1.9626
2022-07-11 20:06:22 - train: epoch 0032, iter [03700, 05004], lr: 0.163387, loss: 1.9823
2022-07-11 20:07:18 - train: epoch 0032, iter [03800, 05004], lr: 0.163335, loss: 1.5529
2022-07-11 20:08:14 - train: epoch 0032, iter [03900, 05004], lr: 0.163284, loss: 1.7974
2022-07-11 20:09:10 - train: epoch 0032, iter [04000, 05004], lr: 0.163233, loss: 1.8109
2022-07-11 20:10:05 - train: epoch 0032, iter [04100, 05004], lr: 0.163182, loss: 1.8465
2022-07-11 20:11:01 - train: epoch 0032, iter [04200, 05004], lr: 0.163131, loss: 1.8055
2022-07-11 20:11:56 - train: epoch 0032, iter [04300, 05004], lr: 0.163079, loss: 1.8714
2022-07-11 20:12:52 - train: epoch 0032, iter [04400, 05004], lr: 0.163028, loss: 1.8732
2022-07-11 20:13:49 - train: epoch 0032, iter [04500, 05004], lr: 0.162977, loss: 1.9824
2022-07-11 20:14:45 - train: epoch 0032, iter [04600, 05004], lr: 0.162925, loss: 1.8687
2022-07-11 20:15:42 - train: epoch 0032, iter [04700, 05004], lr: 0.162874, loss: 2.0117
2022-07-11 20:16:40 - train: epoch 0032, iter [04800, 05004], lr: 0.162823, loss: 1.7489
2022-07-11 20:17:36 - train: epoch 0032, iter [04900, 05004], lr: 0.162771, loss: 1.8292
2022-07-11 20:18:31 - train: epoch 0032, iter [05000, 05004], lr: 0.162720, loss: 1.9546
2022-07-11 20:18:33 - train: epoch 032, train_loss: 1.8806
2022-07-11 20:20:42 - eval: epoch: 032, acc1: 62.212%, acc5: 85.162%, test_loss: 1.5653, per_image_load_time: 1.370ms, per_image_inference_time: 0.854ms
2022-07-11 20:20:42 - until epoch: 032, best_acc1: 62.212%
2022-07-11 20:20:42 - epoch 033 lr: 0.162717
2022-07-11 20:21:51 - train: epoch 0033, iter [00100, 05004], lr: 0.162666, loss: 1.7880
2022-07-11 20:22:48 - train: epoch 0033, iter [00200, 05004], lr: 0.162615, loss: 1.8850
2022-07-11 20:23:42 - train: epoch 0033, iter [00300, 05004], lr: 0.162563, loss: 1.7991
2022-07-11 20:24:36 - train: epoch 0033, iter [00400, 05004], lr: 0.162512, loss: 1.6118
2022-07-11 20:25:30 - train: epoch 0033, iter [00500, 05004], lr: 0.162460, loss: 2.0114
2022-07-11 20:26:24 - train: epoch 0033, iter [00600, 05004], lr: 0.162408, loss: 1.8132
2022-07-11 20:27:18 - train: epoch 0033, iter [00700, 05004], lr: 0.162357, loss: 2.0692
2022-07-11 20:28:12 - train: epoch 0033, iter [00800, 05004], lr: 0.162305, loss: 1.9920
2022-07-11 20:29:04 - train: epoch 0033, iter [00900, 05004], lr: 0.162253, loss: 1.9045
2022-07-11 20:29:56 - train: epoch 0033, iter [01000, 05004], lr: 0.162202, loss: 2.1083
2022-07-11 20:30:51 - train: epoch 0033, iter [01100, 05004], lr: 0.162150, loss: 1.8867
2022-07-11 20:31:44 - train: epoch 0033, iter [01200, 05004], lr: 0.162098, loss: 1.9703
2022-07-11 20:32:36 - train: epoch 0033, iter [01300, 05004], lr: 0.162046, loss: 1.7340
2022-07-11 20:33:30 - train: epoch 0033, iter [01400, 05004], lr: 0.161994, loss: 2.0654
2022-07-11 20:34:24 - train: epoch 0033, iter [01500, 05004], lr: 0.161942, loss: 2.0696
2022-07-11 20:35:17 - train: epoch 0033, iter [01600, 05004], lr: 0.161891, loss: 2.0463
2022-07-11 20:36:10 - train: epoch 0033, iter [01700, 05004], lr: 0.161839, loss: 2.0276
2022-07-11 20:37:03 - train: epoch 0033, iter [01800, 05004], lr: 0.161787, loss: 1.9446
2022-07-11 20:37:56 - train: epoch 0033, iter [01900, 05004], lr: 0.161735, loss: 2.0133
2022-07-11 20:38:48 - train: epoch 0033, iter [02000, 05004], lr: 0.161683, loss: 1.6727
2022-07-11 20:39:41 - train: epoch 0033, iter [02100, 05004], lr: 0.161631, loss: 1.8539
2022-07-11 20:40:32 - train: epoch 0033, iter [02200, 05004], lr: 0.161579, loss: 1.7891
2022-07-11 20:41:24 - train: epoch 0033, iter [02300, 05004], lr: 0.161527, loss: 1.8048
2022-07-11 20:42:17 - train: epoch 0033, iter [02400, 05004], lr: 0.161474, loss: 1.9842
2022-07-11 20:43:10 - train: epoch 0033, iter [02500, 05004], lr: 0.161422, loss: 1.8180
2022-07-11 20:44:03 - train: epoch 0033, iter [02600, 05004], lr: 0.161370, loss: 1.8234
2022-07-11 20:44:57 - train: epoch 0033, iter [02700, 05004], lr: 0.161318, loss: 1.8671
