2022-07-11 20:45:50 - train: epoch 0033, iter [02800, 05004], lr: 0.161266, loss: 1.9827
2022-07-11 20:46:43 - train: epoch 0033, iter [02900, 05004], lr: 0.161213, loss: 1.8180
2022-07-11 20:47:35 - train: epoch 0033, iter [03000, 05004], lr: 0.161161, loss: 1.8913
2022-07-11 20:48:29 - train: epoch 0033, iter [03100, 05004], lr: 0.161109, loss: 1.9403
2022-07-11 20:49:21 - train: epoch 0033, iter [03200, 05004], lr: 0.161057, loss: 1.8002
2022-07-11 20:50:14 - train: epoch 0033, iter [03300, 05004], lr: 0.161004, loss: 1.9066
2022-07-11 20:51:08 - train: epoch 0033, iter [03400, 05004], lr: 0.160952, loss: 1.8871
2022-07-11 20:52:00 - train: epoch 0033, iter [03500, 05004], lr: 0.160899, loss: 1.8209
2022-07-11 20:52:53 - train: epoch 0033, iter [03600, 05004], lr: 0.160847, loss: 2.0379
2022-07-11 20:53:46 - train: epoch 0033, iter [03700, 05004], lr: 0.160795, loss: 1.7370
2022-07-11 20:54:39 - train: epoch 0033, iter [03800, 05004], lr: 0.160742, loss: 1.8901
2022-07-11 20:55:31 - train: epoch 0033, iter [03900, 05004], lr: 0.160690, loss: 2.0016
2022-07-11 20:56:25 - train: epoch 0033, iter [04000, 05004], lr: 0.160637, loss: 2.0339
2022-07-11 20:57:17 - train: epoch 0033, iter [04100, 05004], lr: 0.160584, loss: 1.8141
2022-07-11 20:58:10 - train: epoch 0033, iter [04200, 05004], lr: 0.160532, loss: 1.9857
2022-07-11 20:59:02 - train: epoch 0033, iter [04300, 05004], lr: 0.160479, loss: 1.8944
2022-07-11 20:59:55 - train: epoch 0033, iter [04400, 05004], lr: 0.160427, loss: 1.8500
2022-07-11 21:00:48 - train: epoch 0033, iter [04500, 05004], lr: 0.160374, loss: 2.0487
2022-07-11 21:01:41 - train: epoch 0033, iter [04600, 05004], lr: 0.160321, loss: 1.8662
2022-07-11 21:02:34 - train: epoch 0033, iter [04700, 05004], lr: 0.160269, loss: 1.7516
2022-07-11 21:03:27 - train: epoch 0033, iter [04800, 05004], lr: 0.160216, loss: 2.1585
2022-07-11 21:04:20 - train: epoch 0033, iter [04900, 05004], lr: 0.160163, loss: 1.7641
2022-07-11 21:05:12 - train: epoch 0033, iter [05000, 05004], lr: 0.160110, loss: 1.8763
2022-07-11 21:05:15 - train: epoch 033, train_loss: 1.8710
2022-07-11 21:07:26 - eval: epoch: 033, acc1: 62.032%, acc5: 84.748%, test_loss: 1.5842, per_image_load_time: 1.881ms, per_image_inference_time: 0.857ms
2022-07-11 21:07:26 - until epoch: 033, best_acc1: 62.212%
2022-07-11 21:07:26 - epoch 034 lr: 0.160108
2022-07-11 21:08:33 - train: epoch 0034, iter [00100, 05004], lr: 0.160055, loss: 1.7023
2022-07-11 21:09:28 - train: epoch 0034, iter [00200, 05004], lr: 0.160002, loss: 1.5661
2022-07-11 21:10:24 - train: epoch 0034, iter [00300, 05004], lr: 0.159950, loss: 1.6803
2022-07-11 21:11:18 - train: epoch 0034, iter [00400, 05004], lr: 0.159897, loss: 1.6869
2022-07-11 21:12:14 - train: epoch 0034, iter [00500, 05004], lr: 0.159844, loss: 1.7338
2022-07-11 21:13:07 - train: epoch 0034, iter [00600, 05004], lr: 0.159791, loss: 1.9310
2022-07-11 21:14:00 - train: epoch 0034, iter [00700, 05004], lr: 0.159738, loss: 1.8380
2022-07-11 21:14:54 - train: epoch 0034, iter [00800, 05004], lr: 0.159685, loss: 1.9274
2022-07-11 21:15:47 - train: epoch 0034, iter [00900, 05004], lr: 0.159632, loss: 1.8271
2022-07-11 21:16:41 - train: epoch 0034, iter [01000, 05004], lr: 0.159579, loss: 1.6166
2022-07-11 21:17:35 - train: epoch 0034, iter [01100, 05004], lr: 0.159526, loss: 1.9026
2022-07-11 21:18:29 - train: epoch 0034, iter [01200, 05004], lr: 0.159472, loss: 1.9459
2022-07-11 21:19:23 - train: epoch 0034, iter [01300, 05004], lr: 0.159419, loss: 1.8206
2022-07-11 21:20:17 - train: epoch 0034, iter [01400, 05004], lr: 0.159366, loss: 1.7622
2022-07-11 21:21:10 - train: epoch 0034, iter [01500, 05004], lr: 0.159313, loss: 1.9433
2022-07-11 21:22:04 - train: epoch 0034, iter [01600, 05004], lr: 0.159260, loss: 1.8510
2022-07-11 21:22:58 - train: epoch 0034, iter [01700, 05004], lr: 0.159206, loss: 1.9817
2022-07-11 21:23:52 - train: epoch 0034, iter [01800, 05004], lr: 0.159153, loss: 2.2344
2022-07-11 21:24:46 - train: epoch 0034, iter [01900, 05004], lr: 0.159100, loss: 1.9770
2022-07-11 21:25:39 - train: epoch 0034, iter [02000, 05004], lr: 0.159047, loss: 1.7507
2022-07-11 21:26:32 - train: epoch 0034, iter [02100, 05004], lr: 0.158993, loss: 1.9614
2022-07-11 21:27:24 - train: epoch 0034, iter [02200, 05004], lr: 0.158940, loss: 1.5899
2022-07-11 21:28:17 - train: epoch 0034, iter [02300, 05004], lr: 0.158886, loss: 1.6823
2022-07-11 21:29:10 - train: epoch 0034, iter [02400, 05004], lr: 0.158833, loss: 1.7004
2022-07-11 21:30:03 - train: epoch 0034, iter [02500, 05004], lr: 0.158780, loss: 1.9031
2022-07-11 21:30:55 - train: epoch 0034, iter [02600, 05004], lr: 0.158726, loss: 1.9600
2022-07-11 21:31:48 - train: epoch 0034, iter [02700, 05004], lr: 0.158673, loss: 1.9025
2022-07-11 21:32:42 - train: epoch 0034, iter [02800, 05004], lr: 0.158619, loss: 1.5742
2022-07-11 21:33:34 - train: epoch 0034, iter [02900, 05004], lr: 0.158566, loss: 1.7001
2022-07-11 21:34:27 - train: epoch 0034, iter [03000, 05004], lr: 0.158512, loss: 1.6364
2022-07-11 21:35:19 - train: epoch 0034, iter [03100, 05004], lr: 0.158458, loss: 1.7520
2022-07-11 21:36:12 - train: epoch 0034, iter [03200, 05004], lr: 0.158405, loss: 1.9556
2022-07-11 21:37:06 - train: epoch 0034, iter [03300, 05004], lr: 0.158351, loss: 1.9534
2022-07-11 21:37:59 - train: epoch 0034, iter [03400, 05004], lr: 0.158297, loss: 1.8680
2022-07-11 21:38:51 - train: epoch 0034, iter [03500, 05004], lr: 0.158244, loss: 1.7943
2022-07-11 21:39:43 - train: epoch 0034, iter [03600, 05004], lr: 0.158190, loss: 1.4224
2022-07-11 21:40:36 - train: epoch 0034, iter [03700, 05004], lr: 0.158136, loss: 1.7946
2022-07-11 21:41:29 - train: epoch 0034, iter [03800, 05004], lr: 0.158082, loss: 1.8972
2022-07-11 21:42:24 - train: epoch 0034, iter [03900, 05004], lr: 0.158029, loss: 1.9757
2022-07-11 21:43:17 - train: epoch 0034, iter [04000, 05004], lr: 0.157975, loss: 1.7818
2022-07-11 21:44:09 - train: epoch 0034, iter [04100, 05004], lr: 0.157921, loss: 1.8869
2022-07-11 21:45:02 - train: epoch 0034, iter [04200, 05004], lr: 0.157867, loss: 1.8914
2022-07-11 21:45:54 - train: epoch 0034, iter [04300, 05004], lr: 0.157813, loss: 1.8922
2022-07-11 21:46:46 - train: epoch 0034, iter [04400, 05004], lr: 0.157759, loss: 1.8800
2022-07-11 21:47:39 - train: epoch 0034, iter [04500, 05004], lr: 0.157705, loss: 1.9211
2022-07-11 21:48:32 - train: epoch 0034, iter [04600, 05004], lr: 0.157651, loss: 2.1944
2022-07-11 21:49:24 - train: epoch 0034, iter [04700, 05004], lr: 0.157597, loss: 1.8848
2022-07-11 21:50:16 - train: epoch 0034, iter [04800, 05004], lr: 0.157543, loss: 1.9477
2022-07-11 21:51:08 - train: epoch 0034, iter [04900, 05004], lr: 0.157489, loss: 1.8577
2022-07-11 21:52:00 - train: epoch 0034, iter [05000, 05004], lr: 0.157435, loss: 1.8168
2022-07-11 21:52:02 - train: epoch 034, train_loss: 1.8595
2022-07-11 21:54:10 - eval: epoch: 034, acc1: 62.252%, acc5: 85.020%, test_loss: 1.5738, per_image_load_time: 4.079ms, per_image_inference_time: 0.837ms
2022-07-11 21:54:10 - until epoch: 034, best_acc1: 62.252%
2022-07-11 21:54:10 - epoch 035 lr: 0.157432
2022-07-11 21:55:14 - train: epoch 0035, iter [00100, 05004], lr: 0.157379, loss: 1.6664
2022-07-11 21:56:07 - train: epoch 0035, iter [00200, 05004], lr: 0.157325, loss: 1.7418
2022-07-11 21:56:59 - train: epoch 0035, iter [00300, 05004], lr: 0.157270, loss: 1.8983
2022-07-11 21:57:51 - train: epoch 0035, iter [00400, 05004], lr: 0.157216, loss: 1.7235
2022-07-11 21:58:43 - train: epoch 0035, iter [00500, 05004], lr: 0.157162, loss: 1.7607
2022-07-11 21:59:36 - train: epoch 0035, iter [00600, 05004], lr: 0.157108, loss: 1.8900
2022-07-11 22:00:29 - train: epoch 0035, iter [00700, 05004], lr: 0.157054, loss: 1.7373
2022-07-11 22:01:23 - train: epoch 0035, iter [00800, 05004], lr: 0.156999, loss: 1.7935
2022-07-11 22:02:16 - train: epoch 0035, iter [00900, 05004], lr: 0.156945, loss: 1.8979
2022-07-11 22:03:09 - train: epoch 0035, iter [01000, 05004], lr: 0.156891, loss: 1.7577
2022-07-11 22:04:04 - train: epoch 0035, iter [01100, 05004], lr: 0.156836, loss: 1.9686
2022-07-11 22:04:57 - train: epoch 0035, iter [01200, 05004], lr: 0.156782, loss: 1.6161
2022-07-11 22:05:50 - train: epoch 0035, iter [01300, 05004], lr: 0.156727, loss: 1.8579
2022-07-11 22:06:43 - train: epoch 0035, iter [01400, 05004], lr: 0.156673, loss: 1.9200
2022-07-11 22:07:37 - train: epoch 0035, iter [01500, 05004], lr: 0.156619, loss: 1.8678
2022-07-11 22:08:31 - train: epoch 0035, iter [01600, 05004], lr: 0.156564, loss: 1.7054
2022-07-11 22:09:25 - train: epoch 0035, iter [01700, 05004], lr: 0.156510, loss: 1.8269
2022-07-11 22:10:18 - train: epoch 0035, iter [01800, 05004], lr: 0.156455, loss: 1.8491
2022-07-11 22:11:11 - train: epoch 0035, iter [01900, 05004], lr: 0.156400, loss: 1.7991
2022-07-11 22:12:06 - train: epoch 0035, iter [02000, 05004], lr: 0.156346, loss: 1.9760
2022-07-11 22:12:59 - train: epoch 0035, iter [02100, 05004], lr: 0.156291, loss: 1.7558
2022-07-11 22:13:53 - train: epoch 0035, iter [02200, 05004], lr: 0.156237, loss: 2.1084
2022-07-11 22:14:47 - train: epoch 0035, iter [02300, 05004], lr: 0.156182, loss: 1.8410
2022-07-11 22:15:41 - train: epoch 0035, iter [02400, 05004], lr: 0.156127, loss: 2.1089
2022-07-11 22:16:36 - train: epoch 0035, iter [02500, 05004], lr: 0.156073, loss: 1.6596
2022-07-11 22:17:30 - train: epoch 0035, iter [02600, 05004], lr: 0.156018, loss: 1.9780
2022-07-11 22:18:24 - train: epoch 0035, iter [02700, 05004], lr: 0.155963, loss: 1.7026
2022-07-11 22:19:18 - train: epoch 0035, iter [02800, 05004], lr: 0.155908, loss: 1.7620
2022-07-11 22:20:12 - train: epoch 0035, iter [02900, 05004], lr: 0.155854, loss: 1.8514
2022-07-11 22:21:06 - train: epoch 0035, iter [03000, 05004], lr: 0.155799, loss: 1.6684
2022-07-11 22:22:00 - train: epoch 0035, iter [03100, 05004], lr: 0.155744, loss: 1.8403
2022-07-11 22:22:54 - train: epoch 0035, iter [03200, 05004], lr: 0.155689, loss: 1.7279
2022-07-11 22:23:47 - train: epoch 0035, iter [03300, 05004], lr: 0.155634, loss: 1.7443
2022-07-11 22:24:42 - train: epoch 0035, iter [03400, 05004], lr: 0.155579, loss: 1.8247
2022-07-11 22:25:36 - train: epoch 0035, iter [03500, 05004], lr: 0.155524, loss: 1.5126
2022-07-11 22:26:31 - train: epoch 0035, iter [03600, 05004], lr: 0.155469, loss: 2.0462
2022-07-11 22:27:26 - train: epoch 0035, iter [03700, 05004], lr: 0.155414, loss: 1.8199
2022-07-11 22:28:21 - train: epoch 0035, iter [03800, 05004], lr: 0.155359, loss: 1.8862
2022-07-11 22:29:15 - train: epoch 0035, iter [03900, 05004], lr: 0.155304, loss: 1.9545
2022-07-11 22:30:10 - train: epoch 0035, iter [04000, 05004], lr: 0.155249, loss: 1.6145
2022-07-11 22:31:05 - train: epoch 0035, iter [04100, 05004], lr: 0.155194, loss: 2.1687
2022-07-11 22:31:59 - train: epoch 0035, iter [04200, 05004], lr: 0.155139, loss: 1.5513
2022-07-11 22:32:54 - train: epoch 0035, iter [04300, 05004], lr: 0.155084, loss: 1.8368
2022-07-11 22:33:49 - train: epoch 0035, iter [04400, 05004], lr: 0.155029, loss: 1.9600
2022-07-11 22:34:43 - train: epoch 0035, iter [04500, 05004], lr: 0.154973, loss: 1.8211
2022-07-11 22:35:37 - train: epoch 0035, iter [04600, 05004], lr: 0.154918, loss: 1.8212
2022-07-11 22:36:32 - train: epoch 0035, iter [04700, 05004], lr: 0.154863, loss: 2.0291
2022-07-11 22:37:25 - train: epoch 0035, iter [04800, 05004], lr: 0.154808, loss: 2.0015
2022-07-11 22:38:19 - train: epoch 0035, iter [04900, 05004], lr: 0.154752, loss: 2.0453
2022-07-11 22:39:13 - train: epoch 0035, iter [05000, 05004], lr: 0.154697, loss: 2.0745
2022-07-11 22:39:15 - train: epoch 035, train_loss: 1.8479
2022-07-11 22:41:24 - eval: epoch: 035, acc1: 62.156%, acc5: 84.864%, test_loss: 1.5769, per_image_load_time: 2.774ms, per_image_inference_time: 0.873ms
2022-07-11 22:41:24 - until epoch: 035, best_acc1: 62.252%
2022-07-11 22:41:24 - epoch 036 lr: 0.154694
2022-07-11 22:42:30 - train: epoch 0036, iter [00100, 05004], lr: 0.154639, loss: 1.8235
2022-07-11 22:43:24 - train: epoch 0036, iter [00200, 05004], lr: 0.154584, loss: 1.6556
2022-07-11 22:44:19 - train: epoch 0036, iter [00300, 05004], lr: 0.154529, loss: 1.7166
2022-07-11 22:45:13 - train: epoch 0036, iter [00400, 05004], lr: 0.154473, loss: 1.8316
2022-07-11 22:46:06 - train: epoch 0036, iter [00500, 05004], lr: 0.154418, loss: 1.6691
2022-07-11 22:46:59 - train: epoch 0036, iter [00600, 05004], lr: 0.154362, loss: 1.7252
2022-07-11 22:47:52 - train: epoch 0036, iter [00700, 05004], lr: 0.154307, loss: 1.7817
2022-07-11 22:48:46 - train: epoch 0036, iter [00800, 05004], lr: 0.154251, loss: 1.6701
2022-07-11 22:49:40 - train: epoch 0036, iter [00900, 05004], lr: 0.154196, loss: 1.8238
2022-07-11 22:50:34 - train: epoch 0036, iter [01000, 05004], lr: 0.154140, loss: 1.8195
2022-07-11 22:51:28 - train: epoch 0036, iter [01100, 05004], lr: 0.154085, loss: 1.8887
2022-07-11 22:52:23 - train: epoch 0036, iter [01200, 05004], lr: 0.154029, loss: 1.8286
2022-07-11 22:53:17 - train: epoch 0036, iter [01300, 05004], lr: 0.153974, loss: 2.0150
2022-07-11 22:54:11 - train: epoch 0036, iter [01400, 05004], lr: 0.153918, loss: 1.8026
2022-07-11 22:55:06 - train: epoch 0036, iter [01500, 05004], lr: 0.153862, loss: 1.9885
2022-07-11 22:56:01 - train: epoch 0036, iter [01600, 05004], lr: 0.153807, loss: 1.9007
2022-07-11 22:56:57 - train: epoch 0036, iter [01700, 05004], lr: 0.153751, loss: 1.7820
2022-07-11 22:57:51 - train: epoch 0036, iter [01800, 05004], lr: 0.153695, loss: 1.7208
2022-07-11 22:58:46 - train: epoch 0036, iter [01900, 05004], lr: 0.153639, loss: 2.0677
2022-07-11 22:59:40 - train: epoch 0036, iter [02000, 05004], lr: 0.153584, loss: 1.7466
2022-07-11 23:00:35 - train: epoch 0036, iter [02100, 05004], lr: 0.153528, loss: 1.8557
2022-07-11 23:01:30 - train: epoch 0036, iter [02200, 05004], lr: 0.153472, loss: 1.8740
2022-07-11 23:02:25 - train: epoch 0036, iter [02300, 05004], lr: 0.153416, loss: 1.7255
2022-07-11 23:03:20 - train: epoch 0036, iter [02400, 05004], lr: 0.153360, loss: 2.1100
2022-07-11 23:04:14 - train: epoch 0036, iter [02500, 05004], lr: 0.153304, loss: 1.6549
2022-07-11 23:05:09 - train: epoch 0036, iter [02600, 05004], lr: 0.153248, loss: 1.9452
2022-07-11 23:06:02 - train: epoch 0036, iter [02700, 05004], lr: 0.153192, loss: 1.5324
2022-07-11 23:06:57 - train: epoch 0036, iter [02800, 05004], lr: 0.153136, loss: 1.7110
2022-07-11 23:07:52 - train: epoch 0036, iter [02900, 05004], lr: 0.153080, loss: 1.5124
2022-07-11 23:08:46 - train: epoch 0036, iter [03000, 05004], lr: 0.153024, loss: 1.7549
2022-07-11 23:09:42 - train: epoch 0036, iter [03100, 05004], lr: 0.152968, loss: 1.9139
2022-07-11 23:10:37 - train: epoch 0036, iter [03200, 05004], lr: 0.152912, loss: 1.9102
2022-07-11 23:11:33 - train: epoch 0036, iter [03300, 05004], lr: 0.152856, loss: 1.7731
2022-07-11 23:12:28 - train: epoch 0036, iter [03400, 05004], lr: 0.152800, loss: 1.5679
2022-07-11 23:13:23 - train: epoch 0036, iter [03500, 05004], lr: 0.152744, loss: 1.9133
2022-07-11 23:14:18 - train: epoch 0036, iter [03600, 05004], lr: 0.152688, loss: 1.8711
2022-07-11 23:15:12 - train: epoch 0036, iter [03700, 05004], lr: 0.152632, loss: 1.8896
2022-07-11 23:16:07 - train: epoch 0036, iter [03800, 05004], lr: 0.152575, loss: 1.7558
2022-07-11 23:17:02 - train: epoch 0036, iter [03900, 05004], lr: 0.152519, loss: 1.8087
2022-07-11 23:17:57 - train: epoch 0036, iter [04000, 05004], lr: 0.152463, loss: 1.8063
2022-07-11 23:18:52 - train: epoch 0036, iter [04100, 05004], lr: 0.152407, loss: 1.6781
2022-07-11 23:19:47 - train: epoch 0036, iter [04200, 05004], lr: 0.152350, loss: 1.8389
2022-07-11 23:20:42 - train: epoch 0036, iter [04300, 05004], lr: 0.152294, loss: 1.7664
2022-07-11 23:21:37 - train: epoch 0036, iter [04400, 05004], lr: 0.152238, loss: 1.8574
2022-07-11 23:22:33 - train: epoch 0036, iter [04500, 05004], lr: 0.152181, loss: 1.7070
2022-07-11 23:23:28 - train: epoch 0036, iter [04600, 05004], lr: 0.152125, loss: 1.7488
2022-07-11 23:24:23 - train: epoch 0036, iter [04700, 05004], lr: 0.152069, loss: 1.7042
2022-07-11 23:25:17 - train: epoch 0036, iter [04800, 05004], lr: 0.152012, loss: 1.7596
2022-07-11 23:26:11 - train: epoch 0036, iter [04900, 05004], lr: 0.151956, loss: 1.9880
2022-07-11 23:27:04 - train: epoch 0036, iter [05000, 05004], lr: 0.151899, loss: 1.8682
2022-07-11 23:27:07 - train: epoch 036, train_loss: 1.8393
2022-07-11 23:29:14 - eval: epoch: 036, acc1: 62.144%, acc5: 85.010%, test_loss: 1.5738, per_image_load_time: 3.322ms, per_image_inference_time: 0.839ms
2022-07-11 23:29:15 - until epoch: 036, best_acc1: 62.252%
2022-07-11 23:29:15 - epoch 037 lr: 0.151896
2022-07-11 23:30:20 - train: epoch 0037, iter [00100, 05004], lr: 0.151840, loss: 1.5709
2022-07-11 23:31:16 - train: epoch 0037, iter [00200, 05004], lr: 0.151784, loss: 1.6351
2022-07-11 23:32:11 - train: epoch 0037, iter [00300, 05004], lr: 0.151727, loss: 1.9375
2022-07-11 23:33:06 - train: epoch 0037, iter [00400, 05004], lr: 0.151671, loss: 1.6214
2022-07-11 23:34:00 - train: epoch 0037, iter [00500, 05004], lr: 0.151614, loss: 1.8053
2022-07-11 23:34:55 - train: epoch 0037, iter [00600, 05004], lr: 0.151558, loss: 2.0023
2022-07-11 23:35:50 - train: epoch 0037, iter [00700, 05004], lr: 0.151501, loss: 1.7166
2022-07-11 23:36:44 - train: epoch 0037, iter [00800, 05004], lr: 0.151444, loss: 1.8358
2022-07-11 23:37:39 - train: epoch 0037, iter [00900, 05004], lr: 0.151388, loss: 2.1120
2022-07-11 23:38:34 - train: epoch 0037, iter [01000, 05004], lr: 0.151331, loss: 1.7383
2022-07-11 23:39:29 - train: epoch 0037, iter [01100, 05004], lr: 0.151274, loss: 1.7596
2022-07-11 23:40:24 - train: epoch 0037, iter [01200, 05004], lr: 0.151217, loss: 1.8432
2022-07-11 23:41:19 - train: epoch 0037, iter [01300, 05004], lr: 0.151161, loss: 1.8201
2022-07-11 23:42:15 - train: epoch 0037, iter [01400, 05004], lr: 0.151104, loss: 2.0743
2022-07-11 23:43:09 - train: epoch 0037, iter [01500, 05004], lr: 0.151047, loss: 1.7943
2022-07-11 23:44:05 - train: epoch 0037, iter [01600, 05004], lr: 0.150990, loss: 1.5743
2022-07-11 23:45:00 - train: epoch 0037, iter [01700, 05004], lr: 0.150933, loss: 1.7373
2022-07-11 23:45:55 - train: epoch 0037, iter [01800, 05004], lr: 0.150876, loss: 1.8714
2022-07-11 23:46:50 - train: epoch 0037, iter [01900, 05004], lr: 0.150820, loss: 1.8710
2022-07-11 23:47:45 - train: epoch 0037, iter [02000, 05004], lr: 0.150763, loss: 1.8626
2022-07-11 23:48:40 - train: epoch 0037, iter [02100, 05004], lr: 0.150706, loss: 1.6493
2022-07-11 23:49:35 - train: epoch 0037, iter [02200, 05004], lr: 0.150649, loss: 1.7096
2022-07-11 23:50:29 - train: epoch 0037, iter [02300, 05004], lr: 0.150592, loss: 1.8366
2022-07-11 23:51:24 - train: epoch 0037, iter [02400, 05004], lr: 0.150535, loss: 1.7383
2022-07-11 23:52:19 - train: epoch 0037, iter [02500, 05004], lr: 0.150478, loss: 1.9017
2022-07-11 23:53:14 - train: epoch 0037, iter [02600, 05004], lr: 0.150421, loss: 2.0824
2022-07-11 23:54:08 - train: epoch 0037, iter [02700, 05004], lr: 0.150364, loss: 1.9087
2022-07-11 23:55:02 - train: epoch 0037, iter [02800, 05004], lr: 0.150306, loss: 1.9684
2022-07-11 23:55:57 - train: epoch 0037, iter [02900, 05004], lr: 0.150249, loss: 1.6680
2022-07-11 23:56:52 - train: epoch 0037, iter [03000, 05004], lr: 0.150192, loss: 1.9206
2022-07-11 23:57:46 - train: epoch 0037, iter [03100, 05004], lr: 0.150135, loss: 1.7928
2022-07-11 23:58:41 - train: epoch 0037, iter [03200, 05004], lr: 0.150078, loss: 1.8800
2022-07-11 23:59:34 - train: epoch 0037, iter [03300, 05004], lr: 0.150021, loss: 1.7615
2022-07-12 00:00:30 - train: epoch 0037, iter [03400, 05004], lr: 0.149963, loss: 1.6548
2022-07-12 00:01:24 - train: epoch 0037, iter [03500, 05004], lr: 0.149906, loss: 1.7350
2022-07-12 00:02:18 - train: epoch 0037, iter [03600, 05004], lr: 0.149849, loss: 1.9003
2022-07-12 00:03:14 - train: epoch 0037, iter [03700, 05004], lr: 0.149792, loss: 1.8598
2022-07-12 00:04:08 - train: epoch 0037, iter [03800, 05004], lr: 0.149734, loss: 1.9372
2022-07-12 00:05:02 - train: epoch 0037, iter [03900, 05004], lr: 0.149677, loss: 1.8242
2022-07-12 00:05:56 - train: epoch 0037, iter [04000, 05004], lr: 0.149619, loss: 1.7991
2022-07-12 00:06:50 - train: epoch 0037, iter [04100, 05004], lr: 0.149562, loss: 1.6985
2022-07-12 00:07:45 - train: epoch 0037, iter [04200, 05004], lr: 0.149505, loss: 1.9808
2022-07-12 00:08:38 - train: epoch 0037, iter [04300, 05004], lr: 0.149447, loss: 1.7729
2022-07-12 00:09:33 - train: epoch 0037, iter [04400, 05004], lr: 0.149390, loss: 1.6766
2022-07-12 00:10:27 - train: epoch 0037, iter [04500, 05004], lr: 0.149332, loss: 1.7822
2022-07-12 00:11:21 - train: epoch 0037, iter [04600, 05004], lr: 0.149275, loss: 1.7130
2022-07-12 00:12:15 - train: epoch 0037, iter [04700, 05004], lr: 0.149217, loss: 1.7717
2022-07-12 00:13:09 - train: epoch 0037, iter [04800, 05004], lr: 0.149160, loss: 1.9097
2022-07-12 00:14:04 - train: epoch 0037, iter [04900, 05004], lr: 0.149102, loss: 1.9643
2022-07-12 00:14:57 - train: epoch 0037, iter [05000, 05004], lr: 0.149045, loss: 1.7625
2022-07-12 00:14:59 - train: epoch 037, train_loss: 1.8267
2022-07-12 00:17:07 - eval: epoch: 037, acc1: 62.688%, acc5: 85.450%, test_loss: 1.5477, per_image_load_time: 4.099ms, per_image_inference_time: 0.839ms
2022-07-12 00:17:08 - until epoch: 037, best_acc1: 62.688%
2022-07-12 00:17:08 - epoch 038 lr: 0.149042
2022-07-12 00:18:15 - train: epoch 0038, iter [00100, 05004], lr: 0.148985, loss: 1.5110
2022-07-12 00:19:10 - train: epoch 0038, iter [00200, 05004], lr: 0.148927, loss: 1.5902
2022-07-12 00:20:04 - train: epoch 0038, iter [00300, 05004], lr: 0.148869, loss: 1.5181
2022-07-12 00:20:59 - train: epoch 0038, iter [00400, 05004], lr: 0.148812, loss: 1.6731
2022-07-12 00:21:54 - train: epoch 0038, iter [00500, 05004], lr: 0.148754, loss: 1.7160
2022-07-12 00:22:48 - train: epoch 0038, iter [00600, 05004], lr: 0.148696, loss: 1.7303
2022-07-12 00:23:41 - train: epoch 0038, iter [00700, 05004], lr: 0.148639, loss: 1.6373
2022-07-12 00:24:35 - train: epoch 0038, iter [00800, 05004], lr: 0.148581, loss: 1.7442
2022-07-12 00:25:29 - train: epoch 0038, iter [00900, 05004], lr: 0.148523, loss: 1.8398
2022-07-12 00:26:22 - train: epoch 0038, iter [01000, 05004], lr: 0.148465, loss: 1.8732
2022-07-12 00:27:17 - train: epoch 0038, iter [01100, 05004], lr: 0.148408, loss: 1.9170
2022-07-12 00:28:11 - train: epoch 0038, iter [01200, 05004], lr: 0.148350, loss: 1.9249
2022-07-12 00:29:05 - train: epoch 0038, iter [01300, 05004], lr: 0.148292, loss: 1.8743
2022-07-12 00:29:59 - train: epoch 0038, iter [01400, 05004], lr: 0.148234, loss: 1.7634
2022-07-12 00:30:53 - train: epoch 0038, iter [01500, 05004], lr: 0.148176, loss: 1.7672
2022-07-12 00:31:46 - train: epoch 0038, iter [01600, 05004], lr: 0.148118, loss: 1.9096
2022-07-12 00:32:40 - train: epoch 0038, iter [01700, 05004], lr: 0.148060, loss: 2.0103
2022-07-12 00:33:34 - train: epoch 0038, iter [01800, 05004], lr: 0.148002, loss: 1.8648
2022-07-12 00:34:28 - train: epoch 0038, iter [01900, 05004], lr: 0.147944, loss: 1.8771
2022-07-12 00:35:22 - train: epoch 0038, iter [02000, 05004], lr: 0.147886, loss: 1.9920
2022-07-12 00:36:16 - train: epoch 0038, iter [02100, 05004], lr: 0.147828, loss: 1.9082
2022-07-12 00:37:10 - train: epoch 0038, iter [02200, 05004], lr: 0.147770, loss: 1.8201
2022-07-12 00:38:05 - train: epoch 0038, iter [02300, 05004], lr: 0.147712, loss: 1.8638
2022-07-12 00:38:59 - train: epoch 0038, iter [02400, 05004], lr: 0.147654, loss: 2.0778
2022-07-12 00:39:54 - train: epoch 0038, iter [02500, 05004], lr: 0.147596, loss: 1.6960
2022-07-12 00:40:49 - train: epoch 0038, iter [02600, 05004], lr: 0.147538, loss: 1.8650
2022-07-12 00:41:44 - train: epoch 0038, iter [02700, 05004], lr: 0.147480, loss: 1.7335
2022-07-12 00:42:39 - train: epoch 0038, iter [02800, 05004], lr: 0.147421, loss: 1.9208
2022-07-12 00:43:34 - train: epoch 0038, iter [02900, 05004], lr: 0.147363, loss: 1.9951
2022-07-12 00:44:29 - train: epoch 0038, iter [03000, 05004], lr: 0.147305, loss: 1.7180
2022-07-12 00:45:23 - train: epoch 0038, iter [03100, 05004], lr: 0.147247, loss: 1.9082
2022-07-12 00:46:17 - train: epoch 0038, iter [03200, 05004], lr: 0.147189, loss: 1.5868
2022-07-12 00:47:12 - train: epoch 0038, iter [03300, 05004], lr: 0.147130, loss: 1.6127
2022-07-12 00:48:07 - train: epoch 0038, iter [03400, 05004], lr: 0.147072, loss: 1.7373
2022-07-12 00:49:03 - train: epoch 0038, iter [03500, 05004], lr: 0.147014, loss: 1.7931
2022-07-12 00:49:57 - train: epoch 0038, iter [03600, 05004], lr: 0.146955, loss: 1.7919
2022-07-12 00:50:51 - train: epoch 0038, iter [03700, 05004], lr: 0.146897, loss: 1.5982
2022-07-12 00:51:47 - train: epoch 0038, iter [03800, 05004], lr: 0.146839, loss: 2.0146
2022-07-12 00:52:41 - train: epoch 0038, iter [03900, 05004], lr: 0.146780, loss: 1.7600
2022-07-12 00:53:36 - train: epoch 0038, iter [04000, 05004], lr: 0.146722, loss: 1.9602
2022-07-12 00:54:31 - train: epoch 0038, iter [04100, 05004], lr: 0.146663, loss: 1.8153
2022-07-12 00:55:25 - train: epoch 0038, iter [04200, 05004], lr: 0.146605, loss: 1.5644
2022-07-12 00:56:19 - train: epoch 0038, iter [04300, 05004], lr: 0.146546, loss: 1.9869
2022-07-12 00:57:14 - train: epoch 0038, iter [04400, 05004], lr: 0.146488, loss: 1.9645
2022-07-12 00:58:08 - train: epoch 0038, iter [04500, 05004], lr: 0.146429, loss: 1.8493
2022-07-12 00:59:03 - train: epoch 0038, iter [04600, 05004], lr: 0.146371, loss: 1.9613
2022-07-12 00:59:57 - train: epoch 0038, iter [04700, 05004], lr: 0.146312, loss: 1.6615
2022-07-12 01:00:51 - train: epoch 0038, iter [04800, 05004], lr: 0.146254, loss: 1.8944
2022-07-12 01:01:45 - train: epoch 0038, iter [04900, 05004], lr: 0.146195, loss: 1.8282
2022-07-12 01:02:39 - train: epoch 0038, iter [05000, 05004], lr: 0.146136, loss: 1.6728
2022-07-12 01:02:41 - train: epoch 038, train_loss: 1.8162
2022-07-12 01:04:49 - eval: epoch: 038, acc1: 62.410%, acc5: 85.396%, test_loss: 1.5526, per_image_load_time: 2.817ms, per_image_inference_time: 0.849ms
2022-07-12 01:04:49 - until epoch: 038, best_acc1: 62.688%
2022-07-12 01:04:49 - epoch 039 lr: 0.146134
2022-07-12 01:05:57 - train: epoch 0039, iter [00100, 05004], lr: 0.146075, loss: 1.9107
2022-07-12 01:06:52 - train: epoch 0039, iter [00200, 05004], lr: 0.146017, loss: 1.7682
2022-07-12 01:07:46 - train: epoch 0039, iter [00300, 05004], lr: 0.145958, loss: 1.8542
2022-07-12 01:08:39 - train: epoch 0039, iter [00400, 05004], lr: 0.145899, loss: 1.7235
2022-07-12 01:09:33 - train: epoch 0039, iter [00500, 05004], lr: 0.145841, loss: 1.8796
2022-07-12 01:10:26 - train: epoch 0039, iter [00600, 05004], lr: 0.145782, loss: 1.6529
2022-07-12 01:11:20 - train: epoch 0039, iter [00700, 05004], lr: 0.145723, loss: 2.0519
2022-07-12 01:12:14 - train: epoch 0039, iter [00800, 05004], lr: 0.145664, loss: 1.6026
2022-07-12 01:13:07 - train: epoch 0039, iter [00900, 05004], lr: 0.145606, loss: 1.9045
2022-07-12 01:14:01 - train: epoch 0039, iter [01000, 05004], lr: 0.145547, loss: 1.5854
2022-07-12 01:14:56 - train: epoch 0039, iter [01100, 05004], lr: 0.145488, loss: 1.7963
2022-07-12 01:15:51 - train: epoch 0039, iter [01200, 05004], lr: 0.145429, loss: 1.7736
2022-07-12 01:16:45 - train: epoch 0039, iter [01300, 05004], lr: 0.145370, loss: 1.8100
2022-07-12 01:17:39 - train: epoch 0039, iter [01400, 05004], lr: 0.145311, loss: 1.8150
2022-07-12 01:18:33 - train: epoch 0039, iter [01500, 05004], lr: 0.145252, loss: 1.6395
2022-07-12 01:19:27 - train: epoch 0039, iter [01600, 05004], lr: 0.145193, loss: 1.7944
2022-07-12 01:20:21 - train: epoch 0039, iter [01700, 05004], lr: 0.145134, loss: 1.8824
2022-07-12 01:21:15 - train: epoch 0039, iter [01800, 05004], lr: 0.145075, loss: 1.8124
2022-07-12 01:22:10 - train: epoch 0039, iter [01900, 05004], lr: 0.145016, loss: 1.5177
2022-07-12 01:23:03 - train: epoch 0039, iter [02000, 05004], lr: 0.144957, loss: 1.8351
2022-07-12 01:23:56 - train: epoch 0039, iter [02100, 05004], lr: 0.144898, loss: 1.8856
2022-07-12 01:24:50 - train: epoch 0039, iter [02200, 05004], lr: 0.144839, loss: 1.7090
2022-07-12 01:25:43 - train: epoch 0039, iter [02300, 05004], lr: 0.144780, loss: 2.0397
2022-07-12 01:26:37 - train: epoch 0039, iter [02400, 05004], lr: 0.144721, loss: 1.9060
2022-07-12 01:27:31 - train: epoch 0039, iter [02500, 05004], lr: 0.144662, loss: 1.8557
2022-07-12 01:28:25 - train: epoch 0039, iter [02600, 05004], lr: 0.144603, loss: 1.7454
2022-07-12 01:29:19 - train: epoch 0039, iter [02700, 05004], lr: 0.144544, loss: 1.8771
2022-07-12 01:30:14 - train: epoch 0039, iter [02800, 05004], lr: 0.144485, loss: 1.7665
2022-07-12 01:31:07 - train: epoch 0039, iter [02900, 05004], lr: 0.144425, loss: 1.9016
2022-07-12 01:32:02 - train: epoch 0039, iter [03000, 05004], lr: 0.144366, loss: 2.0094
2022-07-12 01:32:56 - train: epoch 0039, iter [03100, 05004], lr: 0.144307, loss: 1.7557
2022-07-12 01:33:50 - train: epoch 0039, iter [03200, 05004], lr: 0.144248, loss: 2.0198
2022-07-12 01:34:44 - train: epoch 0039, iter [03300, 05004], lr: 0.144188, loss: 2.0139
2022-07-12 01:35:37 - train: epoch 0039, iter [03400, 05004], lr: 0.144129, loss: 1.9756
2022-07-12 01:36:31 - train: epoch 0039, iter [03500, 05004], lr: 0.144070, loss: 1.9841
2022-07-12 01:37:24 - train: epoch 0039, iter [03600, 05004], lr: 0.144010, loss: 1.9073
2022-07-12 01:38:17 - train: epoch 0039, iter [03700, 05004], lr: 0.143951, loss: 1.7322
2022-07-12 01:39:09 - train: epoch 0039, iter [03800, 05004], lr: 0.143892, loss: 1.6737
2022-07-12 01:40:02 - train: epoch 0039, iter [03900, 05004], lr: 0.143832, loss: 1.6542
2022-07-12 01:40:56 - train: epoch 0039, iter [04000, 05004], lr: 0.143773, loss: 1.8143
2022-07-12 01:41:49 - train: epoch 0039, iter [04100, 05004], lr: 0.143714, loss: 1.8172
2022-07-12 01:42:43 - train: epoch 0039, iter [04200, 05004], lr: 0.143654, loss: 1.9495
2022-07-12 01:43:37 - train: epoch 0039, iter [04300, 05004], lr: 0.143595, loss: 1.7796
2022-07-12 01:44:30 - train: epoch 0039, iter [04400, 05004], lr: 0.143535, loss: 1.7515
2022-07-12 01:45:25 - train: epoch 0039, iter [04500, 05004], lr: 0.143476, loss: 1.8164
2022-07-12 01:46:19 - train: epoch 0039, iter [04600, 05004], lr: 0.143416, loss: 1.8592
2022-07-12 01:47:14 - train: epoch 0039, iter [04700, 05004], lr: 0.143357, loss: 1.9349
2022-07-12 01:48:08 - train: epoch 0039, iter [04800, 05004], lr: 0.143297, loss: 1.8168
2022-07-12 01:49:02 - train: epoch 0039, iter [04900, 05004], lr: 0.143237, loss: 1.7294
2022-07-12 01:49:56 - train: epoch 0039, iter [05000, 05004], lr: 0.143178, loss: 1.6838
2022-07-12 01:49:58 - train: epoch 039, train_loss: 1.8062
2022-07-12 01:52:05 - eval: epoch: 039, acc1: 61.836%, acc5: 84.896%, test_loss: 1.5758, per_image_load_time: 4.055ms, per_image_inference_time: 0.848ms
2022-07-12 01:52:05 - until epoch: 039, best_acc1: 62.688%
2022-07-12 01:52:05 - epoch 040 lr: 0.143175
2022-07-12 01:53:10 - train: epoch 0040, iter [00100, 05004], lr: 0.143116, loss: 2.0993
2022-07-12 01:54:05 - train: epoch 0040, iter [00200, 05004], lr: 0.143056, loss: 1.7783
2022-07-12 01:54:59 - train: epoch 0040, iter [00300, 05004], lr: 0.142997, loss: 1.8844
2022-07-12 01:55:56 - train: epoch 0040, iter [00400, 05004], lr: 0.142937, loss: 1.7766
2022-07-12 01:56:51 - train: epoch 0040, iter [00500, 05004], lr: 0.142877, loss: 1.6913
2022-07-12 01:57:48 - train: epoch 0040, iter [00600, 05004], lr: 0.142817, loss: 1.6876
2022-07-12 01:58:43 - train: epoch 0040, iter [00700, 05004], lr: 0.142758, loss: 1.8328
2022-07-12 01:59:39 - train: epoch 0040, iter [00800, 05004], lr: 0.142698, loss: 2.0663
2022-07-12 02:00:34 - train: epoch 0040, iter [00900, 05004], lr: 0.142638, loss: 1.7602
2022-07-12 02:01:27 - train: epoch 0040, iter [01000, 05004], lr: 0.142578, loss: 1.6828
2022-07-12 02:02:21 - train: epoch 0040, iter [01100, 05004], lr: 0.142519, loss: 1.7461
2022-07-12 02:03:15 - train: epoch 0040, iter [01200, 05004], lr: 0.142459, loss: 1.6369
2022-07-12 02:04:09 - train: epoch 0040, iter [01300, 05004], lr: 0.142399, loss: 1.7784
2022-07-12 02:05:03 - train: epoch 0040, iter [01400, 05004], lr: 0.142339, loss: 1.8412
2022-07-12 02:05:57 - train: epoch 0040, iter [01500, 05004], lr: 0.142279, loss: 1.8390
2022-07-12 02:06:51 - train: epoch 0040, iter [01600, 05004], lr: 0.142219, loss: 1.8130
2022-07-12 02:07:46 - train: epoch 0040, iter [01700, 05004], lr: 0.142159, loss: 1.6818
2022-07-12 02:08:40 - train: epoch 0040, iter [01800, 05004], lr: 0.142099, loss: 1.5696
2022-07-12 02:09:35 - train: epoch 0040, iter [01900, 05004], lr: 0.142039, loss: 1.6662
2022-07-12 02:10:30 - train: epoch 0040, iter [02000, 05004], lr: 0.141980, loss: 1.7843
2022-07-12 02:11:25 - train: epoch 0040, iter [02100, 05004], lr: 0.141920, loss: 1.8092
2022-07-12 02:12:21 - train: epoch 0040, iter [02200, 05004], lr: 0.141860, loss: 1.8674
2022-07-12 02:13:15 - train: epoch 0040, iter [02300, 05004], lr: 0.141799, loss: 1.7716
2022-07-12 02:14:10 - train: epoch 0040, iter [02400, 05004], lr: 0.141739, loss: 1.9902
2022-07-12 02:15:04 - train: epoch 0040, iter [02500, 05004], lr: 0.141679, loss: 1.8130
2022-07-12 02:15:59 - train: epoch 0040, iter [02600, 05004], lr: 0.141619, loss: 1.5506
2022-07-12 02:16:54 - train: epoch 0040, iter [02700, 05004], lr: 0.141559, loss: 1.9543
2022-07-12 02:17:49 - train: epoch 0040, iter [02800, 05004], lr: 0.141499, loss: 1.7791
2022-07-12 02:18:43 - train: epoch 0040, iter [02900, 05004], lr: 0.141439, loss: 2.0566
2022-07-12 02:19:37 - train: epoch 0040, iter [03000, 05004], lr: 0.141379, loss: 1.8114
2022-07-12 02:20:32 - train: epoch 0040, iter [03100, 05004], lr: 0.141319, loss: 1.8261
2022-07-12 02:21:26 - train: epoch 0040, iter [03200, 05004], lr: 0.141258, loss: 2.0678
2022-07-12 02:22:21 - train: epoch 0040, iter [03300, 05004], lr: 0.141198, loss: 1.7384
2022-07-12 02:23:16 - train: epoch 0040, iter [03400, 05004], lr: 0.141138, loss: 1.6907
2022-07-12 02:24:13 - train: epoch 0040, iter [03500, 05004], lr: 0.141078, loss: 1.9692
2022-07-12 02:25:09 - train: epoch 0040, iter [03600, 05004], lr: 0.141017, loss: 1.6135
2022-07-12 02:26:06 - train: epoch 0040, iter [03700, 05004], lr: 0.140957, loss: 1.8526
2022-07-12 02:27:02 - train: epoch 0040, iter [03800, 05004], lr: 0.140897, loss: 1.5931
2022-07-12 02:28:00 - train: epoch 0040, iter [03900, 05004], lr: 0.140837, loss: 1.8051
2022-07-12 02:28:58 - train: epoch 0040, iter [04000, 05004], lr: 0.140776, loss: 1.7807
2022-07-12 02:29:55 - train: epoch 0040, iter [04100, 05004], lr: 0.140716, loss: 1.7414
2022-07-12 02:30:52 - train: epoch 0040, iter [04200, 05004], lr: 0.140656, loss: 1.6278
2022-07-12 02:31:48 - train: epoch 0040, iter [04300, 05004], lr: 0.140595, loss: 1.7864
2022-07-12 02:32:43 - train: epoch 0040, iter [04400, 05004], lr: 0.140535, loss: 1.8046
2022-07-12 02:33:39 - train: epoch 0040, iter [04500, 05004], lr: 0.140474, loss: 1.6059
2022-07-12 02:34:34 - train: epoch 0040, iter [04600, 05004], lr: 0.140414, loss: 1.6618
2022-07-12 02:35:30 - train: epoch 0040, iter [04700, 05004], lr: 0.140353, loss: 1.8246
2022-07-12 02:36:25 - train: epoch 0040, iter [04800, 05004], lr: 0.140293, loss: 1.7189
2022-07-12 02:37:20 - train: epoch 0040, iter [04900, 05004], lr: 0.140232, loss: 1.8622
2022-07-12 02:38:15 - train: epoch 0040, iter [05000, 05004], lr: 0.140172, loss: 1.7820
2022-07-12 02:38:17 - train: epoch 040, train_loss: 1.7942
2022-07-12 02:40:31 - eval: epoch: 040, acc1: 63.494%, acc5: 85.942%, test_loss: 1.5077, per_image_load_time: 4.280ms, per_image_inference_time: 0.841ms
2022-07-12 02:40:32 - until epoch: 040, best_acc1: 63.494%
2022-07-12 02:40:32 - epoch 041 lr: 0.140169
2022-07-12 02:41:40 - train: epoch 0041, iter [00100, 05004], lr: 0.140109, loss: 2.0377
2022-07-12 02:42:37 - train: epoch 0041, iter [00200, 05004], lr: 0.140048, loss: 2.0506
2022-07-12 02:43:33 - train: epoch 0041, iter [00300, 05004], lr: 0.139988, loss: 1.7381
2022-07-12 02:44:29 - train: epoch 0041, iter [00400, 05004], lr: 0.139927, loss: 1.8192
2022-07-12 02:45:25 - train: epoch 0041, iter [00500, 05004], lr: 0.139867, loss: 1.7461
2022-07-12 02:46:21 - train: epoch 0041, iter [00600, 05004], lr: 0.139806, loss: 1.8830
2022-07-12 02:47:17 - train: epoch 0041, iter [00700, 05004], lr: 0.139745, loss: 1.6527
2022-07-12 02:48:13 - train: epoch 0041, iter [00800, 05004], lr: 0.139685, loss: 1.7265
2022-07-12 02:49:08 - train: epoch 0041, iter [00900, 05004], lr: 0.139624, loss: 1.7007
2022-07-12 02:50:03 - train: epoch 0041, iter [01000, 05004], lr: 0.139563, loss: 2.0385
2022-07-12 02:50:58 - train: epoch 0041, iter [01100, 05004], lr: 0.139503, loss: 1.7813
2022-07-12 02:51:53 - train: epoch 0041, iter [01200, 05004], lr: 0.139442, loss: 1.5988
2022-07-12 02:52:48 - train: epoch 0041, iter [01300, 05004], lr: 0.139381, loss: 1.7837
2022-07-12 02:53:43 - train: epoch 0041, iter [01400, 05004], lr: 0.139321, loss: 1.8318
2022-07-12 02:54:37 - train: epoch 0041, iter [01500, 05004], lr: 0.139260, loss: 2.0666
2022-07-12 02:55:33 - train: epoch 0041, iter [01600, 05004], lr: 0.139199, loss: 1.7816
2022-07-12 02:56:29 - train: epoch 0041, iter [01700, 05004], lr: 0.139138, loss: 1.7206
2022-07-12 02:57:24 - train: epoch 0041, iter [01800, 05004], lr: 0.139077, loss: 1.7141
2022-07-12 02:58:19 - train: epoch 0041, iter [01900, 05004], lr: 0.139017, loss: 1.6857
2022-07-12 02:59:13 - train: epoch 0041, iter [02000, 05004], lr: 0.138956, loss: 1.7500
2022-07-12 03:00:07 - train: epoch 0041, iter [02100, 05004], lr: 0.138895, loss: 1.5526
2022-07-12 03:01:02 - train: epoch 0041, iter [02200, 05004], lr: 0.138834, loss: 1.5823
2022-07-12 03:01:56 - train: epoch 0041, iter [02300, 05004], lr: 0.138773, loss: 1.7388
2022-07-12 03:02:50 - train: epoch 0041, iter [02400, 05004], lr: 0.138712, loss: 1.8917
2022-07-12 03:03:45 - train: epoch 0041, iter [02500, 05004], lr: 0.138651, loss: 1.7440
2022-07-12 03:04:39 - train: epoch 0041, iter [02600, 05004], lr: 0.138590, loss: 1.9424
2022-07-12 03:05:33 - train: epoch 0041, iter [02700, 05004], lr: 0.138529, loss: 1.7393
2022-07-12 03:06:27 - train: epoch 0041, iter [02800, 05004], lr: 0.138468, loss: 1.7698
2022-07-12 03:07:22 - train: epoch 0041, iter [02900, 05004], lr: 0.138407, loss: 1.6747
2022-07-12 03:08:17 - train: epoch 0041, iter [03000, 05004], lr: 0.138346, loss: 1.8594
2022-07-12 03:09:11 - train: epoch 0041, iter [03100, 05004], lr: 0.138285, loss: 1.9517
2022-07-12 03:10:06 - train: epoch 0041, iter [03200, 05004], lr: 0.138224, loss: 1.8609
2022-07-12 03:11:01 - train: epoch 0041, iter [03300, 05004], lr: 0.138163, loss: 1.7640
2022-07-12 03:11:55 - train: epoch 0041, iter [03400, 05004], lr: 0.138102, loss: 1.5225
2022-07-12 03:12:50 - train: epoch 0041, iter [03500, 05004], lr: 0.138041, loss: 1.8730
2022-07-12 03:13:44 - train: epoch 0041, iter [03600, 05004], lr: 0.137980, loss: 1.7563
2022-07-12 03:14:38 - train: epoch 0041, iter [03700, 05004], lr: 0.137919, loss: 1.7900
2022-07-12 03:15:32 - train: epoch 0041, iter [03800, 05004], lr: 0.137857, loss: 1.7402
2022-07-12 03:16:26 - train: epoch 0041, iter [03900, 05004], lr: 0.137796, loss: 1.7973
2022-07-12 03:17:21 - train: epoch 0041, iter [04000, 05004], lr: 0.137735, loss: 1.5260
2022-07-12 03:18:16 - train: epoch 0041, iter [04100, 05004], lr: 0.137674, loss: 1.7924
2022-07-12 03:19:11 - train: epoch 0041, iter [04200, 05004], lr: 0.137613, loss: 1.9648
2022-07-12 03:20:05 - train: epoch 0041, iter [04300, 05004], lr: 0.137551, loss: 1.8369
2022-07-12 03:20:59 - train: epoch 0041, iter [04400, 05004], lr: 0.137490, loss: 1.5615
2022-07-12 03:21:53 - train: epoch 0041, iter [04500, 05004], lr: 0.137429, loss: 1.9853
2022-07-12 03:22:48 - train: epoch 0041, iter [04600, 05004], lr: 0.137368, loss: 2.0059
2022-07-12 03:23:42 - train: epoch 0041, iter [04700, 05004], lr: 0.137306, loss: 1.8164
2022-07-12 03:24:36 - train: epoch 0041, iter [04800, 05004], lr: 0.137245, loss: 1.6383
2022-07-12 03:25:30 - train: epoch 0041, iter [04900, 05004], lr: 0.137184, loss: 1.7293
2022-07-12 03:26:22 - train: epoch 0041, iter [05000, 05004], lr: 0.137122, loss: 1.9016
2022-07-12 03:26:25 - train: epoch 041, train_loss: 1.7843
2022-07-12 03:28:33 - eval: epoch: 041, acc1: 63.670%, acc5: 85.942%, test_loss: 1.5051, per_image_load_time: 2.404ms, per_image_inference_time: 0.855ms
2022-07-12 03:28:33 - until epoch: 041, best_acc1: 63.670%
2022-07-12 03:28:33 - epoch 042 lr: 0.137119
2022-07-12 03:29:41 - train: epoch 0042, iter [00100, 05004], lr: 0.137058, loss: 1.4577
2022-07-12 03:30:37 - train: epoch 0042, iter [00200, 05004], lr: 0.136997, loss: 1.7868
2022-07-12 03:31:32 - train: epoch 0042, iter [00300, 05004], lr: 0.136936, loss: 1.7597
2022-07-12 03:32:27 - train: epoch 0042, iter [00400, 05004], lr: 0.136874, loss: 1.5575
2022-07-12 03:33:21 - train: epoch 0042, iter [00500, 05004], lr: 0.136813, loss: 1.6457
2022-07-12 03:34:16 - train: epoch 0042, iter [00600, 05004], lr: 0.136751, loss: 1.6893
2022-07-12 03:35:10 - train: epoch 0042, iter [00700, 05004], lr: 0.136690, loss: 1.8413
2022-07-12 03:36:05 - train: epoch 0042, iter [00800, 05004], lr: 0.136628, loss: 1.7943
2022-07-12 03:36:59 - train: epoch 0042, iter [00900, 05004], lr: 0.136567, loss: 1.8191
2022-07-12 03:37:54 - train: epoch 0042, iter [01000, 05004], lr: 0.136505, loss: 1.9229
2022-07-12 03:38:48 - train: epoch 0042, iter [01100, 05004], lr: 0.136444, loss: 1.5991
2022-07-12 03:39:42 - train: epoch 0042, iter [01200, 05004], lr: 0.136382, loss: 1.6281
2022-07-12 03:40:38 - train: epoch 0042, iter [01300, 05004], lr: 0.136321, loss: 1.7338
2022-07-12 03:41:33 - train: epoch 0042, iter [01400, 05004], lr: 0.136259, loss: 1.8635
2022-07-12 03:42:27 - train: epoch 0042, iter [01500, 05004], lr: 0.136197, loss: 1.6024
2022-07-12 03:43:22 - train: epoch 0042, iter [01600, 05004], lr: 0.136136, loss: 1.9447
2022-07-12 03:44:17 - train: epoch 0042, iter [01700, 05004], lr: 0.136074, loss: 1.7470
2022-07-12 03:45:11 - train: epoch 0042, iter [01800, 05004], lr: 0.136013, loss: 1.5564
2022-07-12 03:46:06 - train: epoch 0042, iter [01900, 05004], lr: 0.135951, loss: 1.8410
2022-07-12 03:47:02 - train: epoch 0042, iter [02000, 05004], lr: 0.135889, loss: 1.8484
2022-07-12 03:47:56 - train: epoch 0042, iter [02100, 05004], lr: 0.135828, loss: 1.5003
2022-07-12 03:48:52 - train: epoch 0042, iter [02200, 05004], lr: 0.135766, loss: 1.6200
2022-07-12 03:49:47 - train: epoch 0042, iter [02300, 05004], lr: 0.135704, loss: 1.7177
2022-07-12 03:50:42 - train: epoch 0042, iter [02400, 05004], lr: 0.135642, loss: 2.1221
2022-07-12 03:51:37 - train: epoch 0042, iter [02500, 05004], lr: 0.135581, loss: 1.8266
2022-07-12 03:52:32 - train: epoch 0042, iter [02600, 05004], lr: 0.135519, loss: 1.8661
2022-07-12 03:53:26 - train: epoch 0042, iter [02700, 05004], lr: 0.135457, loss: 1.6151
2022-07-12 03:54:21 - train: epoch 0042, iter [02800, 05004], lr: 0.135395, loss: 1.5783
2022-07-12 03:55:17 - train: epoch 0042, iter [02900, 05004], lr: 0.135333, loss: 1.6875
2022-07-12 03:56:12 - train: epoch 0042, iter [03000, 05004], lr: 0.135272, loss: 1.8074
2022-07-12 03:57:07 - train: epoch 0042, iter [03100, 05004], lr: 0.135210, loss: 1.7443
2022-07-12 03:58:03 - train: epoch 0042, iter [03200, 05004], lr: 0.135148, loss: 1.7799
2022-07-12 03:58:58 - train: epoch 0042, iter [03300, 05004], lr: 0.135086, loss: 1.8771
2022-07-12 03:59:53 - train: epoch 0042, iter [03400, 05004], lr: 0.135024, loss: 1.7251
2022-07-12 04:00:49 - train: epoch 0042, iter [03500, 05004], lr: 0.134962, loss: 1.7768
2022-07-12 04:01:43 - train: epoch 0042, iter [03600, 05004], lr: 0.134900, loss: 2.0264
2022-07-12 04:02:38 - train: epoch 0042, iter [03700, 05004], lr: 0.134838, loss: 1.7226
2022-07-12 04:03:33 - train: epoch 0042, iter [03800, 05004], lr: 0.134776, loss: 1.5691
2022-07-12 04:04:28 - train: epoch 0042, iter [03900, 05004], lr: 0.134714, loss: 1.8475
2022-07-12 04:05:23 - train: epoch 0042, iter [04000, 05004], lr: 0.134652, loss: 1.7333
2022-07-12 04:06:18 - train: epoch 0042, iter [04100, 05004], lr: 0.134590, loss: 1.7668
2022-07-12 04:07:13 - train: epoch 0042, iter [04200, 05004], lr: 0.134528, loss: 1.8411
2022-07-12 04:08:09 - train: epoch 0042, iter [04300, 05004], lr: 0.134466, loss: 1.7327
2022-07-12 04:09:04 - train: epoch 0042, iter [04400, 05004], lr: 0.134404, loss: 1.7006
2022-07-12 04:09:59 - train: epoch 0042, iter [04500, 05004], lr: 0.134342, loss: 1.7431
2022-07-12 04:10:54 - train: epoch 0042, iter [04600, 05004], lr: 0.134280, loss: 1.8673
2022-07-12 04:11:48 - train: epoch 0042, iter [04700, 05004], lr: 0.134218, loss: 1.6319
2022-07-12 04:12:43 - train: epoch 0042, iter [04800, 05004], lr: 0.134156, loss: 1.7950
2022-07-12 04:13:39 - train: epoch 0042, iter [04900, 05004], lr: 0.134094, loss: 1.8094
2022-07-12 04:14:32 - train: epoch 0042, iter [05000, 05004], lr: 0.134032, loss: 1.7777
2022-07-12 04:14:34 - train: epoch 042, train_loss: 1.7726
2022-07-12 04:16:42 - eval: epoch: 042, acc1: 63.358%, acc5: 85.978%, test_loss: 1.5181, per_image_load_time: 3.770ms, per_image_inference_time: 0.859ms
2022-07-12 04:16:43 - until epoch: 042, best_acc1: 63.670%
2022-07-12 04:16:43 - epoch 043 lr: 0.134029
2022-07-12 04:17:50 - train: epoch 0043, iter [00100, 05004], lr: 0.133967, loss: 1.8801
2022-07-12 04:18:45 - train: epoch 0043, iter [00200, 05004], lr: 0.133905, loss: 1.7969
2022-07-12 04:19:39 - train: epoch 0043, iter [00300, 05004], lr: 0.133843, loss: 1.5644
2022-07-12 04:20:33 - train: epoch 0043, iter [00400, 05004], lr: 0.133781, loss: 1.4556
2022-07-12 04:21:28 - train: epoch 0043, iter [00500, 05004], lr: 0.133718, loss: 1.5921
2022-07-12 04:22:22 - train: epoch 0043, iter [00600, 05004], lr: 0.133656, loss: 1.8574
2022-07-12 04:23:17 - train: epoch 0043, iter [00700, 05004], lr: 0.133594, loss: 1.9359
2022-07-12 04:24:11 - train: epoch 0043, iter [00800, 05004], lr: 0.133532, loss: 1.7502
2022-07-12 04:25:05 - train: epoch 0043, iter [00900, 05004], lr: 0.133469, loss: 1.6573
2022-07-12 04:26:00 - train: epoch 0043, iter [01000, 05004], lr: 0.133407, loss: 1.8461
2022-07-12 04:26:54 - train: epoch 0043, iter [01100, 05004], lr: 0.133345, loss: 2.0169
2022-07-12 04:27:48 - train: epoch 0043, iter [01200, 05004], lr: 0.133283, loss: 1.6419
2022-07-12 04:28:42 - train: epoch 0043, iter [01300, 05004], lr: 0.133220, loss: 1.8085
2022-07-12 04:29:36 - train: epoch 0043, iter [01400, 05004], lr: 0.133158, loss: 1.5649
2022-07-12 04:30:30 - train: epoch 0043, iter [01500, 05004], lr: 0.133096, loss: 1.6245
2022-07-12 04:31:25 - train: epoch 0043, iter [01600, 05004], lr: 0.133033, loss: 1.5878
2022-07-12 04:32:20 - train: epoch 0043, iter [01700, 05004], lr: 0.132971, loss: 1.6937
2022-07-12 04:33:15 - train: epoch 0043, iter [01800, 05004], lr: 0.132908, loss: 1.9262
2022-07-12 04:34:09 - train: epoch 0043, iter [01900, 05004], lr: 0.132846, loss: 1.9228
2022-07-12 04:35:03 - train: epoch 0043, iter [02000, 05004], lr: 0.132784, loss: 1.6669
2022-07-12 04:35:57 - train: epoch 0043, iter [02100, 05004], lr: 0.132721, loss: 1.5550
2022-07-12 04:36:52 - train: epoch 0043, iter [02200, 05004], lr: 0.132659, loss: 1.8046
2022-07-12 04:37:46 - train: epoch 0043, iter [02300, 05004], lr: 0.132596, loss: 2.0813
2022-07-12 04:38:40 - train: epoch 0043, iter [02400, 05004], lr: 0.132534, loss: 1.4459
2022-07-12 04:39:35 - train: epoch 0043, iter [02500, 05004], lr: 0.132471, loss: 1.8438
2022-07-12 04:40:28 - train: epoch 0043, iter [02600, 05004], lr: 0.132409, loss: 1.4677
2022-07-12 04:41:23 - train: epoch 0043, iter [02700, 05004], lr: 0.132346, loss: 2.0247
2022-07-12 04:42:18 - train: epoch 0043, iter [02800, 05004], lr: 0.132284, loss: 1.6316
2022-07-12 04:43:13 - train: epoch 0043, iter [02900, 05004], lr: 0.132221, loss: 1.8758
2022-07-12 04:44:08 - train: epoch 0043, iter [03000, 05004], lr: 0.132158, loss: 1.8245
2022-07-12 04:45:03 - train: epoch 0043, iter [03100, 05004], lr: 0.132096, loss: 1.8997
2022-07-12 04:45:58 - train: epoch 0043, iter [03200, 05004], lr: 0.132033, loss: 1.8408
2022-07-12 04:46:52 - train: epoch 0043, iter [03300, 05004], lr: 0.131971, loss: 1.7846
2022-07-12 04:47:46 - train: epoch 0043, iter [03400, 05004], lr: 0.131908, loss: 1.7106
2022-07-12 04:48:39 - train: epoch 0043, iter [03500, 05004], lr: 0.131845, loss: 1.8032
2022-07-12 04:49:34 - train: epoch 0043, iter [03600, 05004], lr: 0.131783, loss: 1.8739
2022-07-12 04:50:29 - train: epoch 0043, iter [03700, 05004], lr: 0.131720, loss: 1.7481
2022-07-12 04:51:23 - train: epoch 0043, iter [03800, 05004], lr: 0.131657, loss: 1.8803
2022-07-12 04:52:16 - train: epoch 0043, iter [03900, 05004], lr: 0.131595, loss: 1.7375
2022-07-12 04:53:10 - train: epoch 0043, iter [04000, 05004], lr: 0.131532, loss: 1.8364
2022-07-12 04:54:03 - train: epoch 0043, iter [04100, 05004], lr: 0.131469, loss: 1.9050
2022-07-12 04:54:56 - train: epoch 0043, iter [04200, 05004], lr: 0.131407, loss: 1.6271
2022-07-12 04:55:50 - train: epoch 0043, iter [04300, 05004], lr: 0.131344, loss: 1.8454
2022-07-12 04:56:44 - train: epoch 0043, iter [04400, 05004], lr: 0.131281, loss: 1.7981
2022-07-12 04:57:38 - train: epoch 0043, iter [04500, 05004], lr: 0.131218, loss: 1.6947
2022-07-12 04:58:32 - train: epoch 0043, iter [04600, 05004], lr: 0.131156, loss: 1.7718
2022-07-12 04:59:26 - train: epoch 0043, iter [04700, 05004], lr: 0.131093, loss: 1.8595
2022-07-12 05:00:20 - train: epoch 0043, iter [04800, 05004], lr: 0.131030, loss: 1.6916
2022-07-12 05:01:13 - train: epoch 0043, iter [04900, 05004], lr: 0.130967, loss: 1.7262
2022-07-12 05:02:06 - train: epoch 0043, iter [05000, 05004], lr: 0.130904, loss: 1.7638
2022-07-12 05:02:08 - train: epoch 043, train_loss: 1.7620
2022-07-12 05:04:17 - eval: epoch: 043, acc1: 63.388%, acc5: 86.118%, test_loss: 1.5196, per_image_load_time: 2.582ms, per_image_inference_time: 0.824ms
2022-07-12 05:04:17 - until epoch: 043, best_acc1: 63.670%
2022-07-12 05:04:17 - epoch 044 lr: 0.130901
2022-07-12 05:05:26 - train: epoch 0044, iter [00100, 05004], lr: 0.130839, loss: 1.9576
2022-07-12 05:06:23 - train: epoch 0044, iter [00200, 05004], lr: 0.130776, loss: 1.7357
2022-07-12 05:07:18 - train: epoch 0044, iter [00300, 05004], lr: 0.130713, loss: 1.7318
2022-07-12 05:08:13 - train: epoch 0044, iter [00400, 05004], lr: 0.130650, loss: 1.4989
2022-07-12 05:09:07 - train: epoch 0044, iter [00500, 05004], lr: 0.130587, loss: 1.5567
2022-07-12 05:10:01 - train: epoch 0044, iter [00600, 05004], lr: 0.130524, loss: 1.5102
2022-07-12 05:10:54 - train: epoch 0044, iter [00700, 05004], lr: 0.130461, loss: 1.6738
2022-07-12 05:11:48 - train: epoch 0044, iter [00800, 05004], lr: 0.130398, loss: 1.7535
2022-07-12 05:12:43 - train: epoch 0044, iter [00900, 05004], lr: 0.130335, loss: 1.7026
2022-07-12 05:13:38 - train: epoch 0044, iter [01000, 05004], lr: 0.130273, loss: 1.6861
2022-07-12 05:14:33 - train: epoch 0044, iter [01100, 05004], lr: 0.130210, loss: 1.6832
2022-07-12 05:15:28 - train: epoch 0044, iter [01200, 05004], lr: 0.130147, loss: 1.6233
2022-07-12 05:16:22 - train: epoch 0044, iter [01300, 05004], lr: 0.130084, loss: 1.6765
2022-07-12 05:17:18 - train: epoch 0044, iter [01400, 05004], lr: 0.130020, loss: 1.6093
2022-07-12 05:18:13 - train: epoch 0044, iter [01500, 05004], lr: 0.129957, loss: 1.8094
2022-07-12 05:19:08 - train: epoch 0044, iter [01600, 05004], lr: 0.129894, loss: 1.5349
2022-07-12 05:20:03 - train: epoch 0044, iter [01700, 05004], lr: 0.129831, loss: 1.7441
2022-07-12 05:20:58 - train: epoch 0044, iter [01800, 05004], lr: 0.129768, loss: 1.5744
2022-07-12 05:21:53 - train: epoch 0044, iter [01900, 05004], lr: 0.129705, loss: 1.8629
2022-07-12 05:22:49 - train: epoch 0044, iter [02000, 05004], lr: 0.129642, loss: 1.7146
2022-07-12 05:23:44 - train: epoch 0044, iter [02100, 05004], lr: 0.129579, loss: 1.8479
2022-07-12 05:24:39 - train: epoch 0044, iter [02200, 05004], lr: 0.129516, loss: 1.8500
2022-07-12 05:25:35 - train: epoch 0044, iter [02300, 05004], lr: 0.129453, loss: 1.8143
2022-07-12 05:26:31 - train: epoch 0044, iter [02400, 05004], lr: 0.129389, loss: 1.6606
2022-07-12 05:27:27 - train: epoch 0044, iter [02500, 05004], lr: 0.129326, loss: 1.7375
2022-07-12 05:28:22 - train: epoch 0044, iter [02600, 05004], lr: 0.129263, loss: 1.7658
2022-07-12 05:29:17 - train: epoch 0044, iter [02700, 05004], lr: 0.129200, loss: 1.7405
2022-07-12 05:30:12 - train: epoch 0044, iter [02800, 05004], lr: 0.129137, loss: 1.6635
2022-07-12 05:31:06 - train: epoch 0044, iter [02900, 05004], lr: 0.129073, loss: 1.7394
2022-07-12 05:32:01 - train: epoch 0044, iter [03000, 05004], lr: 0.129010, loss: 1.7566
2022-07-12 05:32:55 - train: epoch 0044, iter [03100, 05004], lr: 0.128947, loss: 1.8147
2022-07-12 05:33:50 - train: epoch 0044, iter [03200, 05004], lr: 0.128884, loss: 1.5632
2022-07-12 05:34:44 - train: epoch 0044, iter [03300, 05004], lr: 0.128820, loss: 1.8168
2022-07-12 05:35:39 - train: epoch 0044, iter [03400, 05004], lr: 0.128757, loss: 1.8167
2022-07-12 05:36:34 - train: epoch 0044, iter [03500, 05004], lr: 0.128694, loss: 1.6111
2022-07-12 05:37:28 - train: epoch 0044, iter [03600, 05004], lr: 0.128631, loss: 1.9017
2022-07-12 05:38:23 - train: epoch 0044, iter [03700, 05004], lr: 0.128567, loss: 1.7294
2022-07-12 05:39:17 - train: epoch 0044, iter [03800, 05004], lr: 0.128504, loss: 1.4644
2022-07-12 05:40:12 - train: epoch 0044, iter [03900, 05004], lr: 0.128441, loss: 1.6971
2022-07-12 05:41:07 - train: epoch 0044, iter [04000, 05004], lr: 0.128377, loss: 1.8990
2022-07-12 05:42:00 - train: epoch 0044, iter [04100, 05004], lr: 0.128314, loss: 1.7585
2022-07-12 05:42:55 - train: epoch 0044, iter [04200, 05004], lr: 0.128250, loss: 1.7029
2022-07-12 05:43:50 - train: epoch 0044, iter [04300, 05004], lr: 0.128187, loss: 1.7337
2022-07-12 05:44:45 - train: epoch 0044, iter [04400, 05004], lr: 0.128124, loss: 1.6671
2022-07-12 05:45:39 - train: epoch 0044, iter [04500, 05004], lr: 0.128060, loss: 1.8954
2022-07-12 05:46:34 - train: epoch 0044, iter [04600, 05004], lr: 0.127997, loss: 1.6469
2022-07-12 05:47:28 - train: epoch 0044, iter [04700, 05004], lr: 0.127933, loss: 1.9255
2022-07-12 05:48:22 - train: epoch 0044, iter [04800, 05004], lr: 0.127870, loss: 1.7601
2022-07-12 05:49:17 - train: epoch 0044, iter [04900, 05004], lr: 0.127806, loss: 1.7670
2022-07-12 05:50:11 - train: epoch 0044, iter [05000, 05004], lr: 0.127743, loss: 1.8099
2022-07-12 05:50:13 - train: epoch 044, train_loss: 1.7508
2022-07-12 05:52:23 - eval: epoch: 044, acc1: 63.832%, acc5: 86.110%, test_loss: 1.4982, per_image_load_time: 2.799ms, per_image_inference_time: 0.865ms
2022-07-12 05:52:23 - until epoch: 044, best_acc1: 63.832%
2022-07-12 05:52:23 - epoch 045 lr: 0.127740
2022-07-12 05:53:30 - train: epoch 0045, iter [00100, 05004], lr: 0.127677, loss: 1.6699
2022-07-12 05:54:23 - train: epoch 0045, iter [00200, 05004], lr: 0.127613, loss: 1.6973
2022-07-12 05:55:17 - train: epoch 0045, iter [00300, 05004], lr: 0.127550, loss: 1.6777
2022-07-12 05:56:11 - train: epoch 0045, iter [00400, 05004], lr: 0.127486, loss: 1.8352
2022-07-12 05:57:05 - train: epoch 0045, iter [00500, 05004], lr: 0.127423, loss: 1.8710
2022-07-12 05:57:57 - train: epoch 0045, iter [00600, 05004], lr: 0.127359, loss: 1.9358
2022-07-12 05:58:51 - train: epoch 0045, iter [00700, 05004], lr: 0.127296, loss: 1.5074
2022-07-12 05:59:44 - train: epoch 0045, iter [00800, 05004], lr: 0.127232, loss: 1.7107
2022-07-12 06:00:38 - train: epoch 0045, iter [00900, 05004], lr: 0.127168, loss: 1.5299
2022-07-12 06:01:32 - train: epoch 0045, iter [01000, 05004], lr: 0.127105, loss: 1.7555
2022-07-12 06:02:26 - train: epoch 0045, iter [01100, 05004], lr: 0.127041, loss: 1.8510
2022-07-12 06:03:21 - train: epoch 0045, iter [01200, 05004], lr: 0.126978, loss: 1.5545
2022-07-12 06:04:15 - train: epoch 0045, iter [01300, 05004], lr: 0.126914, loss: 1.7640
2022-07-12 06:05:09 - train: epoch 0045, iter [01400, 05004], lr: 0.126850, loss: 1.7647
2022-07-12 06:06:03 - train: epoch 0045, iter [01500, 05004], lr: 0.126787, loss: 1.6206
2022-07-12 06:06:57 - train: epoch 0045, iter [01600, 05004], lr: 0.126723, loss: 1.6136
2022-07-12 06:07:50 - train: epoch 0045, iter [01700, 05004], lr: 0.126659, loss: 1.5588
2022-07-12 06:08:44 - train: epoch 0045, iter [01800, 05004], lr: 0.126595, loss: 1.5753
2022-07-12 06:09:38 - train: epoch 0045, iter [01900, 05004], lr: 0.126532, loss: 1.7186
2022-07-12 06:10:32 - train: epoch 0045, iter [02000, 05004], lr: 0.126468, loss: 1.8095
2022-07-12 06:11:25 - train: epoch 0045, iter [02100, 05004], lr: 0.126404, loss: 1.8936
2022-07-12 06:12:19 - train: epoch 0045, iter [02200, 05004], lr: 0.126341, loss: 1.7604
2022-07-12 06:13:13 - train: epoch 0045, iter [02300, 05004], lr: 0.126277, loss: 1.6636
2022-07-12 06:14:07 - train: epoch 0045, iter [02400, 05004], lr: 0.126213, loss: 1.8089
2022-07-12 06:15:01 - train: epoch 0045, iter [02500, 05004], lr: 0.126149, loss: 1.7204
2022-07-12 06:15:55 - train: epoch 0045, iter [02600, 05004], lr: 0.126085, loss: 1.7522
2022-07-12 06:16:49 - train: epoch 0045, iter [02700, 05004], lr: 0.126022, loss: 1.7020
2022-07-12 06:17:42 - train: epoch 0045, iter [02800, 05004], lr: 0.125958, loss: 1.6652
2022-07-12 06:18:36 - train: epoch 0045, iter [02900, 05004], lr: 0.125894, loss: 1.9491
2022-07-12 06:19:31 - train: epoch 0045, iter [03000, 05004], lr: 0.125830, loss: 1.7017
2022-07-12 06:20:26 - train: epoch 0045, iter [03100, 05004], lr: 0.125766, loss: 1.7523
2022-07-12 06:21:20 - train: epoch 0045, iter [03200, 05004], lr: 0.125702, loss: 1.9324
2022-07-12 06:22:16 - train: epoch 0045, iter [03300, 05004], lr: 0.125639, loss: 1.6310
2022-07-12 06:23:10 - train: epoch 0045, iter [03400, 05004], lr: 0.125575, loss: 1.5785
2022-07-12 06:24:05 - train: epoch 0045, iter [03500, 05004], lr: 0.125511, loss: 1.8523
2022-07-12 06:24:59 - train: epoch 0045, iter [03600, 05004], lr: 0.125447, loss: 1.8421
2022-07-12 06:25:52 - train: epoch 0045, iter [03700, 05004], lr: 0.125383, loss: 1.6538
2022-07-12 06:26:46 - train: epoch 0045, iter [03800, 05004], lr: 0.125319, loss: 1.6904
2022-07-12 06:27:39 - train: epoch 0045, iter [03900, 05004], lr: 0.125255, loss: 1.8306
2022-07-12 06:28:33 - train: epoch 0045, iter [04000, 05004], lr: 0.125191, loss: 1.9322
2022-07-12 06:29:27 - train: epoch 0045, iter [04100, 05004], lr: 0.125127, loss: 1.8195
2022-07-12 06:30:20 - train: epoch 0045, iter [04200, 05004], lr: 0.125063, loss: 1.9887
2022-07-12 06:31:14 - train: epoch 0045, iter [04300, 05004], lr: 0.124999, loss: 1.8928
2022-07-12 06:32:08 - train: epoch 0045, iter [04400, 05004], lr: 0.124935, loss: 1.8841
2022-07-12 06:33:02 - train: epoch 0045, iter [04500, 05004], lr: 0.124871, loss: 1.7236
2022-07-12 06:33:56 - train: epoch 0045, iter [04600, 05004], lr: 0.124807, loss: 1.9851
2022-07-12 06:34:49 - train: epoch 0045, iter [04700, 05004], lr: 0.124743, loss: 1.7257
2022-07-12 06:35:44 - train: epoch 0045, iter [04800, 05004], lr: 0.124679, loss: 1.6064
2022-07-12 06:36:37 - train: epoch 0045, iter [04900, 05004], lr: 0.124615, loss: 1.9385
2022-07-12 06:37:30 - train: epoch 0045, iter [05000, 05004], lr: 0.124551, loss: 1.6772
2022-07-12 06:37:32 - train: epoch 045, train_loss: 1.7374
2022-07-12 06:39:41 - eval: epoch: 045, acc1: 64.018%, acc5: 86.510%, test_loss: 1.4753, per_image_load_time: 2.701ms, per_image_inference_time: 0.848ms
2022-07-12 06:39:41 - until epoch: 045, best_acc1: 64.018%
2022-07-12 06:39:41 - epoch 046 lr: 0.124548
2022-07-12 06:40:49 - train: epoch 0046, iter [00100, 05004], lr: 0.124484, loss: 1.4003
2022-07-12 06:41:44 - train: epoch 0046, iter [00200, 05004], lr: 0.124420, loss: 1.6302
2022-07-12 06:42:39 - train: epoch 0046, iter [00300, 05004], lr: 0.124356, loss: 1.6726
2022-07-12 06:43:34 - train: epoch 0046, iter [00400, 05004], lr: 0.124292, loss: 1.7432
2022-07-12 06:44:27 - train: epoch 0046, iter [00500, 05004], lr: 0.124228, loss: 1.7211
2022-07-12 06:45:21 - train: epoch 0046, iter [00600, 05004], lr: 0.124164, loss: 1.7502
2022-07-12 06:46:16 - train: epoch 0046, iter [00700, 05004], lr: 0.124100, loss: 1.3667
2022-07-12 06:47:11 - train: epoch 0046, iter [00800, 05004], lr: 0.124036, loss: 1.8791
2022-07-12 06:48:04 - train: epoch 0046, iter [00900, 05004], lr: 0.123972, loss: 1.7146
2022-07-12 06:48:58 - train: epoch 0046, iter [01000, 05004], lr: 0.123907, loss: 1.5393
2022-07-12 06:49:52 - train: epoch 0046, iter [01100, 05004], lr: 0.123843, loss: 1.6398
2022-07-12 06:50:46 - train: epoch 0046, iter [01200, 05004], lr: 0.123779, loss: 1.7341
2022-07-12 06:51:40 - train: epoch 0046, iter [01300, 05004], lr: 0.123715, loss: 1.7391
2022-07-12 06:52:33 - train: epoch 0046, iter [01400, 05004], lr: 0.123651, loss: 1.7365
2022-07-12 06:53:28 - train: epoch 0046, iter [01500, 05004], lr: 0.123586, loss: 1.7679
2022-07-12 06:54:23 - train: epoch 0046, iter [01600, 05004], lr: 0.123522, loss: 1.8125
2022-07-12 06:55:18 - train: epoch 0046, iter [01700, 05004], lr: 0.123458, loss: 1.6697
2022-07-12 06:56:12 - train: epoch 0046, iter [01800, 05004], lr: 0.123394, loss: 1.7216
2022-07-12 06:57:06 - train: epoch 0046, iter [01900, 05004], lr: 0.123329, loss: 1.6148
2022-07-12 06:58:00 - train: epoch 0046, iter [02000, 05004], lr: 0.123265, loss: 1.8507
2022-07-12 06:58:54 - train: epoch 0046, iter [02100, 05004], lr: 0.123201, loss: 1.9302
2022-07-12 06:59:49 - train: epoch 0046, iter [02200, 05004], lr: 0.123137, loss: 1.6918
2022-07-12 07:00:43 - train: epoch 0046, iter [02300, 05004], lr: 0.123072, loss: 1.8507
2022-07-12 07:01:37 - train: epoch 0046, iter [02400, 05004], lr: 0.123008, loss: 1.7217
2022-07-12 07:02:32 - train: epoch 0046, iter [02500, 05004], lr: 0.122944, loss: 1.6307
2022-07-12 07:03:26 - train: epoch 0046, iter [02600, 05004], lr: 0.122879, loss: 1.8950
2022-07-12 07:04:21 - train: epoch 0046, iter [02700, 05004], lr: 0.122815, loss: 1.5655
2022-07-12 07:05:16 - train: epoch 0046, iter [02800, 05004], lr: 0.122751, loss: 1.6865
2022-07-12 07:06:11 - train: epoch 0046, iter [02900, 05004], lr: 0.122686, loss: 1.9837
2022-07-12 07:07:07 - train: epoch 0046, iter [03000, 05004], lr: 0.122622, loss: 1.6033
2022-07-12 07:08:01 - train: epoch 0046, iter [03100, 05004], lr: 0.122558, loss: 1.5826
2022-07-12 07:08:55 - train: epoch 0046, iter [03200, 05004], lr: 0.122493, loss: 1.8265
2022-07-12 07:09:49 - train: epoch 0046, iter [03300, 05004], lr: 0.122429, loss: 1.6702
2022-07-12 07:10:43 - train: epoch 0046, iter [03400, 05004], lr: 0.122364, loss: 1.7936
2022-07-12 07:11:38 - train: epoch 0046, iter [03500, 05004], lr: 0.122300, loss: 2.0369
2022-07-12 07:12:32 - train: epoch 0046, iter [03600, 05004], lr: 0.122236, loss: 1.9528
2022-07-12 07:13:25 - train: epoch 0046, iter [03700, 05004], lr: 0.122171, loss: 1.7008
2022-07-12 07:14:18 - train: epoch 0046, iter [03800, 05004], lr: 0.122107, loss: 1.7909
2022-07-12 07:15:12 - train: epoch 0046, iter [03900, 05004], lr: 0.122042, loss: 1.6933
2022-07-12 07:16:05 - train: epoch 0046, iter [04000, 05004], lr: 0.121978, loss: 1.5398
2022-07-12 07:16:59 - train: epoch 0046, iter [04100, 05004], lr: 0.121913, loss: 1.8330
2022-07-12 07:17:51 - train: epoch 0046, iter [04200, 05004], lr: 0.121849, loss: 1.4894
2022-07-12 07:18:45 - train: epoch 0046, iter [04300, 05004], lr: 0.121784, loss: 1.7915
2022-07-12 07:19:37 - train: epoch 0046, iter [04400, 05004], lr: 0.121720, loss: 1.8895
2022-07-12 07:20:31 - train: epoch 0046, iter [04500, 05004], lr: 0.121655, loss: 1.7990
2022-07-12 07:21:25 - train: epoch 0046, iter [04600, 05004], lr: 0.121591, loss: 1.8336
2022-07-12 07:22:18 - train: epoch 0046, iter [04700, 05004], lr: 0.121526, loss: 1.7626
2022-07-12 07:23:11 - train: epoch 0046, iter [04800, 05004], lr: 0.121462, loss: 1.6208
2022-07-12 07:24:04 - train: epoch 0046, iter [04900, 05004], lr: 0.121397, loss: 1.8496
2022-07-12 07:24:56 - train: epoch 0046, iter [05000, 05004], lr: 0.121333, loss: 1.7192
2022-07-12 07:24:58 - train: epoch 046, train_loss: 1.7283
2022-07-12 07:27:06 - eval: epoch: 046, acc1: 64.540%, acc5: 86.440%, test_loss: 1.4684, per_image_load_time: 3.318ms, per_image_inference_time: 0.866ms
2022-07-12 07:27:07 - until epoch: 046, best_acc1: 64.540%
2022-07-12 07:27:07 - epoch 047 lr: 0.121329
2022-07-12 07:28:13 - train: epoch 0047, iter [00100, 05004], lr: 0.121265, loss: 1.7191
2022-07-12 07:29:06 - train: epoch 0047, iter [00200, 05004], lr: 0.121201, loss: 1.7474
2022-07-12 07:30:00 - train: epoch 0047, iter [00300, 05004], lr: 0.121136, loss: 1.5567
2022-07-12 07:30:53 - train: epoch 0047, iter [00400, 05004], lr: 0.121072, loss: 1.6470
2022-07-12 07:31:46 - train: epoch 0047, iter [00500, 05004], lr: 0.121007, loss: 1.8174
2022-07-12 07:32:39 - train: epoch 0047, iter [00600, 05004], lr: 0.120942, loss: 1.6118
2022-07-12 07:33:31 - train: epoch 0047, iter [00700, 05004], lr: 0.120878, loss: 1.7777
2022-07-12 07:34:24 - train: epoch 0047, iter [00800, 05004], lr: 0.120813, loss: 1.6937
2022-07-12 07:35:17 - train: epoch 0047, iter [00900, 05004], lr: 0.120749, loss: 1.8232
2022-07-12 07:36:09 - train: epoch 0047, iter [01000, 05004], lr: 0.120684, loss: 1.7393
2022-07-12 07:37:02 - train: epoch 0047, iter [01100, 05004], lr: 0.120619, loss: 1.6815
2022-07-12 07:37:55 - train: epoch 0047, iter [01200, 05004], lr: 0.120555, loss: 1.7568
2022-07-12 07:38:48 - train: epoch 0047, iter [01300, 05004], lr: 0.120490, loss: 1.5926
2022-07-12 07:39:41 - train: epoch 0047, iter [01400, 05004], lr: 0.120425, loss: 1.7510
2022-07-12 07:40:34 - train: epoch 0047, iter [01500, 05004], lr: 0.120360, loss: 1.8292
2022-07-12 07:41:27 - train: epoch 0047, iter [01600, 05004], lr: 0.120296, loss: 1.7088
2022-07-12 07:42:20 - train: epoch 0047, iter [01700, 05004], lr: 0.120231, loss: 1.5641
2022-07-12 07:43:13 - train: epoch 0047, iter [01800, 05004], lr: 0.120166, loss: 1.8407
2022-07-12 07:44:08 - train: epoch 0047, iter [01900, 05004], lr: 0.120102, loss: 1.5552
2022-07-12 07:45:02 - train: epoch 0047, iter [02000, 05004], lr: 0.120037, loss: 1.7741
2022-07-12 07:45:57 - train: epoch 0047, iter [02100, 05004], lr: 0.119972, loss: 1.9049
2022-07-12 07:46:52 - train: epoch 0047, iter [02200, 05004], lr: 0.119907, loss: 1.9053
2022-07-12 07:47:46 - train: epoch 0047, iter [02300, 05004], lr: 0.119843, loss: 1.7880
2022-07-12 07:48:40 - train: epoch 0047, iter [02400, 05004], lr: 0.119778, loss: 1.4953
2022-07-12 07:49:33 - train: epoch 0047, iter [02500, 05004], lr: 0.119713, loss: 1.8052
2022-07-12 07:50:27 - train: epoch 0047, iter [02600, 05004], lr: 0.119648, loss: 1.9015
2022-07-12 07:51:22 - train: epoch 0047, iter [02700, 05004], lr: 0.119583, loss: 1.5842
2022-07-12 07:52:16 - train: epoch 0047, iter [02800, 05004], lr: 0.119519, loss: 1.8890
2022-07-12 07:53:10 - train: epoch 0047, iter [02900, 05004], lr: 0.119454, loss: 1.6747
2022-07-12 07:54:05 - train: epoch 0047, iter [03000, 05004], lr: 0.119389, loss: 1.7809
2022-07-12 07:55:00 - train: epoch 0047, iter [03100, 05004], lr: 0.119324, loss: 1.7646
2022-07-12 07:55:54 - train: epoch 0047, iter [03200, 05004], lr: 0.119259, loss: 1.6484
2022-07-12 07:56:47 - train: epoch 0047, iter [03300, 05004], lr: 0.119194, loss: 1.7879
2022-07-12 07:57:41 - train: epoch 0047, iter [03400, 05004], lr: 0.119130, loss: 1.5340
2022-07-12 07:58:34 - train: epoch 0047, iter [03500, 05004], lr: 0.119065, loss: 1.9844
2022-07-12 07:59:29 - train: epoch 0047, iter [03600, 05004], lr: 0.119000, loss: 1.9066
2022-07-12 08:00:23 - train: epoch 0047, iter [03700, 05004], lr: 0.118935, loss: 1.5889
2022-07-12 08:01:18 - train: epoch 0047, iter [03800, 05004], lr: 0.118870, loss: 1.7916
2022-07-12 08:02:12 - train: epoch 0047, iter [03900, 05004], lr: 0.118805, loss: 1.7722
2022-07-12 08:03:07 - train: epoch 0047, iter [04000, 05004], lr: 0.118740, loss: 1.6804
2022-07-12 08:04:01 - train: epoch 0047, iter [04100, 05004], lr: 0.118675, loss: 1.7269
2022-07-12 08:04:55 - train: epoch 0047, iter [04200, 05004], lr: 0.118610, loss: 1.7920
2022-07-12 08:05:48 - train: epoch 0047, iter [04300, 05004], lr: 0.118545, loss: 1.6847
2022-07-12 08:06:42 - train: epoch 0047, iter [04400, 05004], lr: 0.118480, loss: 1.5974
2022-07-12 08:07:35 - train: epoch 0047, iter [04500, 05004], lr: 0.118416, loss: 1.7991
2022-07-12 08:08:28 - train: epoch 0047, iter [04600, 05004], lr: 0.118351, loss: 1.7776
2022-07-12 08:09:22 - train: epoch 0047, iter [04700, 05004], lr: 0.118286, loss: 1.8242
2022-07-12 08:10:16 - train: epoch 0047, iter [04800, 05004], lr: 0.118221, loss: 1.6193
2022-07-12 08:11:09 - train: epoch 0047, iter [04900, 05004], lr: 0.118156, loss: 1.7799
2022-07-12 08:12:02 - train: epoch 0047, iter [05000, 05004], lr: 0.118091, loss: 1.7588
2022-07-12 08:12:04 - train: epoch 047, train_loss: 1.7146
2022-07-12 08:14:14 - eval: epoch: 047, acc1: 64.454%, acc5: 86.614%, test_loss: 1.4608, per_image_load_time: 2.950ms, per_image_inference_time: 0.877ms
2022-07-12 08:14:14 - until epoch: 047, best_acc1: 64.540%
2022-07-12 08:14:14 - epoch 048 lr: 0.118087
2022-07-12 08:15:20 - train: epoch 0048, iter [00100, 05004], lr: 0.118023, loss: 1.7175
2022-07-12 08:16:14 - train: epoch 0048, iter [00200, 05004], lr: 0.117958, loss: 2.0297
2022-07-12 08:17:07 - train: epoch 0048, iter [00300, 05004], lr: 0.117893, loss: 1.5926
2022-07-12 08:18:00 - train: epoch 0048, iter [00400, 05004], lr: 0.117828, loss: 1.6479
2022-07-12 08:18:53 - train: epoch 0048, iter [00500, 05004], lr: 0.117763, loss: 1.6037
2022-07-12 08:19:45 - train: epoch 0048, iter [00600, 05004], lr: 0.117698, loss: 1.8434
2022-07-12 08:20:38 - train: epoch 0048, iter [00700, 05004], lr: 0.117633, loss: 1.6901
2022-07-12 08:21:31 - train: epoch 0048, iter [00800, 05004], lr: 0.117568, loss: 1.5985
2022-07-12 08:22:23 - train: epoch 0048, iter [00900, 05004], lr: 0.117503, loss: 1.8336
2022-07-12 08:23:16 - train: epoch 0048, iter [01000, 05004], lr: 0.117438, loss: 1.7543
2022-07-12 08:24:09 - train: epoch 0048, iter [01100, 05004], lr: 0.117373, loss: 1.7132
2022-07-12 08:25:03 - train: epoch 0048, iter [01200, 05004], lr: 0.117308, loss: 1.7621
2022-07-12 08:25:57 - train: epoch 0048, iter [01300, 05004], lr: 0.117242, loss: 1.5164
2022-07-12 08:26:50 - train: epoch 0048, iter [01400, 05004], lr: 0.117177, loss: 1.8346
2022-07-12 08:27:44 - train: epoch 0048, iter [01500, 05004], lr: 0.117112, loss: 1.9140
2022-07-12 08:28:38 - train: epoch 0048, iter [01600, 05004], lr: 0.117047, loss: 1.7505
2022-07-12 08:29:32 - train: epoch 0048, iter [01700, 05004], lr: 0.116982, loss: 1.5344
2022-07-12 08:30:26 - train: epoch 0048, iter [01800, 05004], lr: 0.116917, loss: 1.6989
2022-07-12 08:31:21 - train: epoch 0048, iter [01900, 05004], lr: 0.116852, loss: 1.9273
2022-07-12 08:32:15 - train: epoch 0048, iter [02000, 05004], lr: 0.116787, loss: 1.7905
2022-07-12 08:33:08 - train: epoch 0048, iter [02100, 05004], lr: 0.116721, loss: 1.8187
2022-07-12 08:34:02 - train: epoch 0048, iter [02200, 05004], lr: 0.116656, loss: 1.9559
2022-07-12 08:34:56 - train: epoch 0048, iter [02300, 05004], lr: 0.116591, loss: 1.6189
2022-07-12 08:35:49 - train: epoch 0048, iter [02400, 05004], lr: 0.116526, loss: 1.7756
2022-07-12 08:36:43 - train: epoch 0048, iter [02500, 05004], lr: 0.116461, loss: 1.6286
2022-07-12 08:37:38 - train: epoch 0048, iter [02600, 05004], lr: 0.116396, loss: 1.8616
2022-07-12 08:38:32 - train: epoch 0048, iter [02700, 05004], lr: 0.116330, loss: 1.8370
2022-07-12 08:39:25 - train: epoch 0048, iter [02800, 05004], lr: 0.116265, loss: 1.5537
2022-07-12 08:40:19 - train: epoch 0048, iter [02900, 05004], lr: 0.116200, loss: 1.7860
2022-07-12 08:41:13 - train: epoch 0048, iter [03000, 05004], lr: 0.116135, loss: 1.6478
2022-07-12 08:42:06 - train: epoch 0048, iter [03100, 05004], lr: 0.116070, loss: 1.6778
2022-07-12 08:43:00 - train: epoch 0048, iter [03200, 05004], lr: 0.116004, loss: 1.5963
2022-07-12 08:43:54 - train: epoch 0048, iter [03300, 05004], lr: 0.115939, loss: 1.8173
2022-07-12 08:44:48 - train: epoch 0048, iter [03400, 05004], lr: 0.115874, loss: 1.8313
2022-07-12 08:45:42 - train: epoch 0048, iter [03500, 05004], lr: 0.115809, loss: 1.8960
2022-07-12 08:46:36 - train: epoch 0048, iter [03600, 05004], lr: 0.115743, loss: 1.7490
2022-07-12 08:47:30 - train: epoch 0048, iter [03700, 05004], lr: 0.115678, loss: 1.7948
2022-07-12 08:48:23 - train: epoch 0048, iter [03800, 05004], lr: 0.115613, loss: 1.6606
2022-07-12 08:49:17 - train: epoch 0048, iter [03900, 05004], lr: 0.115547, loss: 1.7539
2022-07-12 08:50:11 - train: epoch 0048, iter [04000, 05004], lr: 0.115482, loss: 1.7956
2022-07-12 08:51:04 - train: epoch 0048, iter [04100, 05004], lr: 0.115417, loss: 1.9383
2022-07-12 08:51:58 - train: epoch 0048, iter [04200, 05004], lr: 0.115352, loss: 1.6728
2022-07-12 08:52:51 - train: epoch 0048, iter [04300, 05004], lr: 0.115286, loss: 1.9225
2022-07-12 08:53:45 - train: epoch 0048, iter [04400, 05004], lr: 0.115221, loss: 1.8690
2022-07-12 08:54:39 - train: epoch 0048, iter [04500, 05004], lr: 0.115156, loss: 1.9391
2022-07-12 08:55:32 - train: epoch 0048, iter [04600, 05004], lr: 0.115090, loss: 1.5698
2022-07-12 08:56:26 - train: epoch 0048, iter [04700, 05004], lr: 0.115025, loss: 1.7088
2022-07-12 08:57:20 - train: epoch 0048, iter [04800, 05004], lr: 0.114960, loss: 2.0650
2022-07-12 08:58:12 - train: epoch 0048, iter [04900, 05004], lr: 0.114894, loss: 1.6911
2022-07-12 08:59:05 - train: epoch 0048, iter [05000, 05004], lr: 0.114829, loss: 1.6436
2022-07-12 08:59:07 - train: epoch 048, train_loss: 1.7024
2022-07-12 09:01:16 - eval: epoch: 048, acc1: 64.576%, acc5: 86.748%, test_loss: 1.4502, per_image_load_time: 3.522ms, per_image_inference_time: 0.887ms
2022-07-12 09:01:16 - until epoch: 048, best_acc1: 64.576%
2022-07-12 09:01:16 - epoch 049 lr: 0.114826
2022-07-12 09:02:22 - train: epoch 0049, iter [00100, 05004], lr: 0.114761, loss: 1.8404
2022-07-12 09:03:16 - train: epoch 0049, iter [00200, 05004], lr: 0.114696, loss: 1.4153
2022-07-12 09:04:09 - train: epoch 0049, iter [00300, 05004], lr: 0.114630, loss: 1.7153
2022-07-12 09:05:03 - train: epoch 0049, iter [00400, 05004], lr: 0.114565, loss: 1.6804
2022-07-12 09:05:55 - train: epoch 0049, iter [00500, 05004], lr: 0.114500, loss: 1.7926
2022-07-12 09:06:48 - train: epoch 0049, iter [00600, 05004], lr: 0.114434, loss: 1.4886
2022-07-12 09:07:41 - train: epoch 0049, iter [00700, 05004], lr: 0.114369, loss: 1.5915
2022-07-12 09:08:34 - train: epoch 0049, iter [00800, 05004], lr: 0.114303, loss: 1.8314
2022-07-12 09:09:27 - train: epoch 0049, iter [00900, 05004], lr: 0.114238, loss: 1.6205
2022-07-12 09:10:21 - train: epoch 0049, iter [01000, 05004], lr: 0.114172, loss: 1.6371
2022-07-12 09:11:14 - train: epoch 0049, iter [01100, 05004], lr: 0.114107, loss: 1.4663
2022-07-12 09:12:07 - train: epoch 0049, iter [01200, 05004], lr: 0.114042, loss: 1.7431
2022-07-12 09:13:00 - train: epoch 0049, iter [01300, 05004], lr: 0.113976, loss: 1.8136
2022-07-12 09:13:54 - train: epoch 0049, iter [01400, 05004], lr: 0.113911, loss: 1.9192
2022-07-12 09:14:48 - train: epoch 0049, iter [01500, 05004], lr: 0.113845, loss: 1.5676
2022-07-12 09:15:43 - train: epoch 0049, iter [01600, 05004], lr: 0.113780, loss: 1.7425
2022-07-12 09:16:38 - train: epoch 0049, iter [01700, 05004], lr: 0.113714, loss: 1.7261
2022-07-12 09:17:32 - train: epoch 0049, iter [01800, 05004], lr: 0.113649, loss: 1.6900
2022-07-12 09:18:27 - train: epoch 0049, iter [01900, 05004], lr: 0.113583, loss: 1.5880
2022-07-12 09:19:22 - train: epoch 0049, iter [02000, 05004], lr: 0.113518, loss: 1.8281
2022-07-12 09:20:17 - train: epoch 0049, iter [02100, 05004], lr: 0.113453, loss: 1.6187
2022-07-12 09:21:09 - train: epoch 0049, iter [02200, 05004], lr: 0.113387, loss: 1.6784
2022-07-12 09:22:03 - train: epoch 0049, iter [02300, 05004], lr: 0.113322, loss: 1.5346
2022-07-12 09:22:57 - train: epoch 0049, iter [02400, 05004], lr: 0.113256, loss: 1.7125
2022-07-12 09:23:51 - train: epoch 0049, iter [02500, 05004], lr: 0.113191, loss: 1.8926
2022-07-12 09:24:45 - train: epoch 0049, iter [02600, 05004], lr: 0.113125, loss: 1.8128
2022-07-12 09:25:39 - train: epoch 0049, iter [02700, 05004], lr: 0.113059, loss: 1.7683
2022-07-12 09:26:32 - train: epoch 0049, iter [02800, 05004], lr: 0.112994, loss: 1.7298
2022-07-12 09:27:26 - train: epoch 0049, iter [02900, 05004], lr: 0.112928, loss: 1.8818
2022-07-12 09:28:20 - train: epoch 0049, iter [03000, 05004], lr: 0.112863, loss: 1.7027
2022-07-12 09:29:14 - train: epoch 0049, iter [03100, 05004], lr: 0.112797, loss: 1.7450
2022-07-12 09:30:06 - train: epoch 0049, iter [03200, 05004], lr: 0.112732, loss: 1.7802
2022-07-12 09:31:00 - train: epoch 0049, iter [03300, 05004], lr: 0.112666, loss: 1.5351
2022-07-12 09:31:53 - train: epoch 0049, iter [03400, 05004], lr: 0.112601, loss: 1.6484
2022-07-12 09:32:47 - train: epoch 0049, iter [03500, 05004], lr: 0.112535, loss: 1.7853
2022-07-12 09:33:40 - train: epoch 0049, iter [03600, 05004], lr: 0.112470, loss: 1.6190
2022-07-12 09:34:33 - train: epoch 0049, iter [03700, 05004], lr: 0.112404, loss: 1.5086
2022-07-12 09:35:27 - train: epoch 0049, iter [03800, 05004], lr: 0.112338, loss: 1.8396
2022-07-12 09:36:20 - train: epoch 0049, iter [03900, 05004], lr: 0.112273, loss: 1.7588
2022-07-12 09:37:12 - train: epoch 0049, iter [04000, 05004], lr: 0.112207, loss: 1.4000
2022-07-12 09:38:05 - train: epoch 0049, iter [04100, 05004], lr: 0.112142, loss: 1.4359
2022-07-12 09:38:58 - train: epoch 0049, iter [04200, 05004], lr: 0.112076, loss: 1.6979
2022-07-12 09:39:50 - train: epoch 0049, iter [04300, 05004], lr: 0.112010, loss: 1.7812
2022-07-12 09:40:43 - train: epoch 0049, iter [04400, 05004], lr: 0.111945, loss: 1.8098
2022-07-12 09:41:36 - train: epoch 0049, iter [04500, 05004], lr: 0.111879, loss: 1.5286
2022-07-12 09:42:28 - train: epoch 0049, iter [04600, 05004], lr: 0.111814, loss: 1.8081
2022-07-12 09:43:21 - train: epoch 0049, iter [04700, 05004], lr: 0.111748, loss: 1.8406
2022-07-12 09:44:14 - train: epoch 0049, iter [04800, 05004], lr: 0.111682, loss: 1.6380
2022-07-12 09:45:08 - train: epoch 0049, iter [04900, 05004], lr: 0.111617, loss: 1.6230
2022-07-12 09:46:00 - train: epoch 0049, iter [05000, 05004], lr: 0.111551, loss: 1.5874
2022-07-12 09:46:02 - train: epoch 049, train_loss: 1.6891
2022-07-12 09:48:10 - eval: epoch: 049, acc1: 65.156%, acc5: 86.820%, test_loss: 1.4450, per_image_load_time: 2.960ms, per_image_inference_time: 0.840ms
2022-07-12 09:48:10 - until epoch: 049, best_acc1: 65.156%
2022-07-12 09:48:10 - epoch 050 lr: 0.111548
2022-07-12 09:49:18 - train: epoch 0050, iter [00100, 05004], lr: 0.111483, loss: 1.5972
2022-07-12 09:50:11 - train: epoch 0050, iter [00200, 05004], lr: 0.111417, loss: 1.5917
2022-07-12 09:51:03 - train: epoch 0050, iter [00300, 05004], lr: 0.111352, loss: 1.5659
2022-07-12 09:51:55 - train: epoch 0050, iter [00400, 05004], lr: 0.111286, loss: 1.5439
2022-07-12 09:52:48 - train: epoch 0050, iter [00500, 05004], lr: 0.111220, loss: 1.6592
2022-07-12 09:53:42 - train: epoch 0050, iter [00600, 05004], lr: 0.111155, loss: 1.7671
2022-07-12 09:54:35 - train: epoch 0050, iter [00700, 05004], lr: 0.111089, loss: 1.6304
2022-07-12 09:55:28 - train: epoch 0050, iter [00800, 05004], lr: 0.111023, loss: 1.4694
2022-07-12 09:56:21 - train: epoch 0050, iter [00900, 05004], lr: 0.110957, loss: 1.7056
2022-07-12 09:57:14 - train: epoch 0050, iter [01000, 05004], lr: 0.110892, loss: 1.7109
2022-07-12 09:58:08 - train: epoch 0050, iter [01100, 05004], lr: 0.110826, loss: 1.8587
2022-07-12 09:59:02 - train: epoch 0050, iter [01200, 05004], lr: 0.110760, loss: 1.6778
2022-07-12 09:59:56 - train: epoch 0050, iter [01300, 05004], lr: 0.110695, loss: 1.4945
2022-07-12 10:00:51 - train: epoch 0050, iter [01400, 05004], lr: 0.110629, loss: 1.6080
2022-07-12 10:01:43 - train: epoch 0050, iter [01500, 05004], lr: 0.110563, loss: 1.5939
2022-07-12 10:02:38 - train: epoch 0050, iter [01600, 05004], lr: 0.110498, loss: 1.6613
2022-07-12 10:03:32 - train: epoch 0050, iter [01700, 05004], lr: 0.110432, loss: 1.6447
2022-07-12 10:04:26 - train: epoch 0050, iter [01800, 05004], lr: 0.110366, loss: 1.9182
2022-07-12 10:05:20 - train: epoch 0050, iter [01900, 05004], lr: 0.110300, loss: 1.8270
2022-07-12 10:06:12 - train: epoch 0050, iter [02000, 05004], lr: 0.110235, loss: 1.6982
2022-07-12 10:07:06 - train: epoch 0050, iter [02100, 05004], lr: 0.110169, loss: 1.6680
2022-07-12 10:07:58 - train: epoch 0050, iter [02200, 05004], lr: 0.110103, loss: 1.7125
2022-07-12 10:08:52 - train: epoch 0050, iter [02300, 05004], lr: 0.110037, loss: 1.4226
2022-07-12 10:09:45 - train: epoch 0050, iter [02400, 05004], lr: 0.109972, loss: 1.7071
2022-07-12 10:10:37 - train: epoch 0050, iter [02500, 05004], lr: 0.109906, loss: 1.7724
2022-07-12 10:11:31 - train: epoch 0050, iter [02600, 05004], lr: 0.109840, loss: 1.5068
2022-07-12 10:12:24 - train: epoch 0050, iter [02700, 05004], lr: 0.109774, loss: 1.8370
2022-07-12 10:13:16 - train: epoch 0050, iter [02800, 05004], lr: 0.109709, loss: 1.8126
2022-07-12 10:14:09 - train: epoch 0050, iter [02900, 05004], lr: 0.109643, loss: 2.2000
2022-07-12 10:15:02 - train: epoch 0050, iter [03000, 05004], lr: 0.109577, loss: 1.7196
2022-07-12 10:15:56 - train: epoch 0050, iter [03100, 05004], lr: 0.109511, loss: 1.6378
2022-07-12 10:16:48 - train: epoch 0050, iter [03200, 05004], lr: 0.109445, loss: 1.6102
2022-07-12 10:17:41 - train: epoch 0050, iter [03300, 05004], lr: 0.109380, loss: 1.4537
2022-07-12 10:18:33 - train: epoch 0050, iter [03400, 05004], lr: 0.109314, loss: 1.8092
2022-07-12 10:19:25 - train: epoch 0050, iter [03500, 05004], lr: 0.109248, loss: 1.6335
2022-07-12 10:20:18 - train: epoch 0050, iter [03600, 05004], lr: 0.109182, loss: 1.7135
2022-07-12 10:21:11 - train: epoch 0050, iter [03700, 05004], lr: 0.109116, loss: 1.9172
2022-07-12 10:22:04 - train: epoch 0050, iter [03800, 05004], lr: 0.109051, loss: 1.7073
2022-07-12 10:22:57 - train: epoch 0050, iter [03900, 05004], lr: 0.108985, loss: 1.4486
2022-07-12 10:23:50 - train: epoch 0050, iter [04000, 05004], lr: 0.108919, loss: 1.7266
2022-07-12 10:24:43 - train: epoch 0050, iter [04100, 05004], lr: 0.108853, loss: 1.5232
2022-07-12 10:25:35 - train: epoch 0050, iter [04200, 05004], lr: 0.108787, loss: 1.7506
2022-07-12 10:26:28 - train: epoch 0050, iter [04300, 05004], lr: 0.108721, loss: 1.8458
2022-07-12 10:27:20 - train: epoch 0050, iter [04400, 05004], lr: 0.108656, loss: 1.6079
2022-07-12 10:28:13 - train: epoch 0050, iter [04500, 05004], lr: 0.108590, loss: 1.5501
2022-07-12 10:29:07 - train: epoch 0050, iter [04600, 05004], lr: 0.108524, loss: 1.7743
2022-07-12 10:30:00 - train: epoch 0050, iter [04700, 05004], lr: 0.108458, loss: 1.7465
2022-07-12 10:30:52 - train: epoch 0050, iter [04800, 05004], lr: 0.108392, loss: 1.7352
2022-07-12 10:31:45 - train: epoch 0050, iter [04900, 05004], lr: 0.108326, loss: 1.6358
2022-07-12 10:32:37 - train: epoch 0050, iter [05000, 05004], lr: 0.108261, loss: 1.5308
2022-07-12 10:32:40 - train: epoch 050, train_loss: 1.6779
2022-07-12 10:34:48 - eval: epoch: 050, acc1: 64.710%, acc5: 86.726%, test_loss: 1.4596, per_image_load_time: 3.637ms, per_image_inference_time: 0.867ms
2022-07-12 10:34:48 - until epoch: 050, best_acc1: 65.156%
2022-07-12 10:34:48 - epoch 051 lr: 0.108257
2022-07-12 10:35:55 - train: epoch 0051, iter [00100, 05004], lr: 0.108192, loss: 1.8046
2022-07-12 10:36:52 - train: epoch 0051, iter [00200, 05004], lr: 0.108126, loss: 1.9534
2022-07-12 10:37:45 - train: epoch 0051, iter [00300, 05004], lr: 0.108060, loss: 1.5043
2022-07-12 10:38:39 - train: epoch 0051, iter [00400, 05004], lr: 0.107994, loss: 1.5647
2022-07-12 10:39:33 - train: epoch 0051, iter [00500, 05004], lr: 0.107929, loss: 1.6366
2022-07-12 10:40:26 - train: epoch 0051, iter [00600, 05004], lr: 0.107863, loss: 1.5827
2022-07-12 10:41:20 - train: epoch 0051, iter [00700, 05004], lr: 0.107797, loss: 1.6974
2022-07-12 10:42:13 - train: epoch 0051, iter [00800, 05004], lr: 0.107731, loss: 1.7808
2022-07-12 10:43:07 - train: epoch 0051, iter [00900, 05004], lr: 0.107665, loss: 1.6393
2022-07-12 10:44:01 - train: epoch 0051, iter [01000, 05004], lr: 0.107599, loss: 1.9930
2022-07-12 10:44:54 - train: epoch 0051, iter [01100, 05004], lr: 0.107533, loss: 1.7881
2022-07-12 10:45:47 - train: epoch 0051, iter [01200, 05004], lr: 0.107467, loss: 1.5735
2022-07-12 10:46:40 - train: epoch 0051, iter [01300, 05004], lr: 0.107401, loss: 1.5423
2022-07-12 10:47:35 - train: epoch 0051, iter [01400, 05004], lr: 0.107336, loss: 1.6653
2022-07-12 10:48:28 - train: epoch 0051, iter [01500, 05004], lr: 0.107270, loss: 1.5426
2022-07-12 10:49:22 - train: epoch 0051, iter [01600, 05004], lr: 0.107204, loss: 1.3228
2022-07-12 10:50:15 - train: epoch 0051, iter [01700, 05004], lr: 0.107138, loss: 1.8006
2022-07-12 10:51:08 - train: epoch 0051, iter [01800, 05004], lr: 0.107072, loss: 1.8045
2022-07-12 10:52:01 - train: epoch 0051, iter [01900, 05004], lr: 0.107006, loss: 1.4992
2022-07-12 10:52:55 - train: epoch 0051, iter [02000, 05004], lr: 0.106940, loss: 1.8409
2022-07-12 10:53:49 - train: epoch 0051, iter [02100, 05004], lr: 0.106874, loss: 1.6658
2022-07-12 10:54:44 - train: epoch 0051, iter [02200, 05004], lr: 0.106808, loss: 1.4286
2022-07-12 10:55:38 - train: epoch 0051, iter [02300, 05004], lr: 0.106742, loss: 1.7559
2022-07-12 10:56:31 - train: epoch 0051, iter [02400, 05004], lr: 0.106676, loss: 1.7331
2022-07-12 10:57:26 - train: epoch 0051, iter [02500, 05004], lr: 0.106610, loss: 1.4423
2022-07-12 10:58:21 - train: epoch 0051, iter [02600, 05004], lr: 0.106544, loss: 1.6675
2022-07-12 10:59:15 - train: epoch 0051, iter [02700, 05004], lr: 0.106478, loss: 1.6252
2022-07-12 11:00:09 - train: epoch 0051, iter [02800, 05004], lr: 0.106413, loss: 1.5560
2022-07-12 11:01:03 - train: epoch 0051, iter [02900, 05004], lr: 0.106347, loss: 1.6263
2022-07-12 11:01:55 - train: epoch 0051, iter [03000, 05004], lr: 0.106281, loss: 1.6181
2022-07-12 11:02:49 - train: epoch 0051, iter [03100, 05004], lr: 0.106215, loss: 1.5078
2022-07-12 11:03:43 - train: epoch 0051, iter [03200, 05004], lr: 0.106149, loss: 1.5717
2022-07-12 11:04:37 - train: epoch 0051, iter [03300, 05004], lr: 0.106083, loss: 1.7247
2022-07-12 11:05:31 - train: epoch 0051, iter [03400, 05004], lr: 0.106017, loss: 1.5438
2022-07-12 11:06:24 - train: epoch 0051, iter [03500, 05004], lr: 0.105951, loss: 1.6712
2022-07-12 11:07:18 - train: epoch 0051, iter [03600, 05004], lr: 0.105885, loss: 1.8035
2022-07-12 11:08:11 - train: epoch 0051, iter [03700, 05004], lr: 0.105819, loss: 1.8482
2022-07-12 11:09:05 - train: epoch 0051, iter [03800, 05004], lr: 0.105753, loss: 1.6269
2022-07-12 11:09:59 - train: epoch 0051, iter [03900, 05004], lr: 0.105687, loss: 1.6990
2022-07-12 11:10:52 - train: epoch 0051, iter [04000, 05004], lr: 0.105621, loss: 1.5464
2022-07-12 11:11:45 - train: epoch 0051, iter [04100, 05004], lr: 0.105555, loss: 1.5951
2022-07-12 11:12:38 - train: epoch 0051, iter [04200, 05004], lr: 0.105489, loss: 1.8731
2022-07-12 11:13:31 - train: epoch 0051, iter [04300, 05004], lr: 0.105423, loss: 1.5258
2022-07-12 11:14:24 - train: epoch 0051, iter [04400, 05004], lr: 0.105357, loss: 1.6340
2022-07-12 11:15:16 - train: epoch 0051, iter [04500, 05004], lr: 0.105291, loss: 1.4609
2022-07-12 11:16:10 - train: epoch 0051, iter [04600, 05004], lr: 0.105225, loss: 2.0082
2022-07-12 11:17:04 - train: epoch 0051, iter [04700, 05004], lr: 0.105159, loss: 1.6146
2022-07-12 11:17:57 - train: epoch 0051, iter [04800, 05004], lr: 0.105093, loss: 1.8032
2022-07-12 11:18:51 - train: epoch 0051, iter [04900, 05004], lr: 0.105027, loss: 1.7216
2022-07-12 11:19:43 - train: epoch 0051, iter [05000, 05004], lr: 0.104961, loss: 1.5655
2022-07-12 11:19:45 - train: epoch 051, train_loss: 1.6628
2022-07-12 11:21:57 - eval: epoch: 051, acc1: 65.578%, acc5: 87.186%, test_loss: 1.4152, per_image_load_time: 1.373ms, per_image_inference_time: 0.805ms
2022-07-12 11:21:57 - until epoch: 051, best_acc1: 65.578%
2022-07-12 11:21:57 - epoch 052 lr: 0.104958
2022-07-12 11:23:03 - train: epoch 0052, iter [00100, 05004], lr: 0.104892, loss: 1.5661
2022-07-12 11:23:58 - train: epoch 0052, iter [00200, 05004], lr: 0.104826, loss: 1.4994
2022-07-12 11:24:52 - train: epoch 0052, iter [00300, 05004], lr: 0.104760, loss: 1.6174
2022-07-12 11:25:47 - train: epoch 0052, iter [00400, 05004], lr: 0.104694, loss: 1.7652
2022-07-12 11:26:41 - train: epoch 0052, iter [00500, 05004], lr: 0.104628, loss: 1.6972
2022-07-12 11:27:36 - train: epoch 0052, iter [00600, 05004], lr: 0.104562, loss: 1.5022
2022-07-12 11:28:30 - train: epoch 0052, iter [00700, 05004], lr: 0.104496, loss: 1.5733
2022-07-12 11:29:24 - train: epoch 0052, iter [00800, 05004], lr: 0.104430, loss: 1.5664
2022-07-12 11:30:19 - train: epoch 0052, iter [00900, 05004], lr: 0.104364, loss: 1.5046
2022-07-12 11:31:12 - train: epoch 0052, iter [01000, 05004], lr: 0.104298, loss: 1.8343
2022-07-12 11:32:07 - train: epoch 0052, iter [01100, 05004], lr: 0.104232, loss: 1.5689
2022-07-12 11:33:01 - train: epoch 0052, iter [01200, 05004], lr: 0.104166, loss: 1.5109
2022-07-12 11:33:56 - train: epoch 0052, iter [01300, 05004], lr: 0.104100, loss: 1.6664
2022-07-12 11:34:50 - train: epoch 0052, iter [01400, 05004], lr: 0.104034, loss: 1.9687
2022-07-12 11:35:44 - train: epoch 0052, iter [01500, 05004], lr: 0.103968, loss: 1.5406
2022-07-12 11:36:38 - train: epoch 0052, iter [01600, 05004], lr: 0.103902, loss: 1.5040
2022-07-12 11:37:31 - train: epoch 0052, iter [01700, 05004], lr: 0.103836, loss: 1.5541
2022-07-12 11:38:24 - train: epoch 0052, iter [01800, 05004], lr: 0.103770, loss: 1.4365
2022-07-12 11:39:17 - train: epoch 0052, iter [01900, 05004], lr: 0.103704, loss: 1.4033
2022-07-12 11:40:10 - train: epoch 0052, iter [02000, 05004], lr: 0.103638, loss: 1.7949
2022-07-12 11:41:03 - train: epoch 0052, iter [02100, 05004], lr: 0.103572, loss: 1.4971
2022-07-12 11:41:55 - train: epoch 0052, iter [02200, 05004], lr: 0.103506, loss: 1.8705
2022-07-12 11:42:48 - train: epoch 0052, iter [02300, 05004], lr: 0.103440, loss: 1.5599
2022-07-12 11:43:42 - train: epoch 0052, iter [02400, 05004], lr: 0.103374, loss: 1.4490
2022-07-12 11:44:33 - train: epoch 0052, iter [02500, 05004], lr: 0.103308, loss: 1.6712
2022-07-12 11:45:25 - train: epoch 0052, iter [02600, 05004], lr: 0.103242, loss: 1.4770
2022-07-12 11:46:18 - train: epoch 0052, iter [02700, 05004], lr: 0.103176, loss: 1.5992
2022-07-12 11:47:10 - train: epoch 0052, iter [02800, 05004], lr: 0.103110, loss: 1.6863
2022-07-12 11:48:03 - train: epoch 0052, iter [02900, 05004], lr: 0.103043, loss: 1.4509
2022-07-12 11:48:56 - train: epoch 0052, iter [03000, 05004], lr: 0.102977, loss: 1.6234
2022-07-12 11:49:48 - train: epoch 0052, iter [03100, 05004], lr: 0.102911, loss: 1.7655
2022-07-12 11:50:41 - train: epoch 0052, iter [03200, 05004], lr: 0.102845, loss: 1.7179
2022-07-12 11:51:34 - train: epoch 0052, iter [03300, 05004], lr: 0.102779, loss: 1.7250
2022-07-12 11:52:27 - train: epoch 0052, iter [03400, 05004], lr: 0.102713, loss: 1.6471
2022-07-12 11:53:20 - train: epoch 0052, iter [03500, 05004], lr: 0.102647, loss: 1.7043
2022-07-12 11:54:13 - train: epoch 0052, iter [03600, 05004], lr: 0.102581, loss: 1.6688
2022-07-12 11:55:08 - train: epoch 0052, iter [03700, 05004], lr: 0.102515, loss: 1.7820
2022-07-12 11:56:00 - train: epoch 0052, iter [03800, 05004], lr: 0.102449, loss: 1.6993
2022-07-12 11:56:52 - train: epoch 0052, iter [03900, 05004], lr: 0.102383, loss: 1.4031
2022-07-12 11:57:45 - train: epoch 0052, iter [04000, 05004], lr: 0.102317, loss: 1.8215
2022-07-12 11:58:38 - train: epoch 0052, iter [04100, 05004], lr: 0.102251, loss: 1.3676
2022-07-12 11:59:31 - train: epoch 0052, iter [04200, 05004], lr: 0.102185, loss: 1.7294
2022-07-12 12:00:24 - train: epoch 0052, iter [04300, 05004], lr: 0.102119, loss: 1.7209
2022-07-12 12:01:17 - train: epoch 0052, iter [04400, 05004], lr: 0.102052, loss: 1.8650
2022-07-12 12:02:10 - train: epoch 0052, iter [04500, 05004], lr: 0.101986, loss: 1.8014
2022-07-12 12:03:03 - train: epoch 0052, iter [04600, 05004], lr: 0.101920, loss: 1.6627
2022-07-12 12:03:56 - train: epoch 0052, iter [04700, 05004], lr: 0.101854, loss: 1.5784
2022-07-12 12:04:50 - train: epoch 0052, iter [04800, 05004], lr: 0.101788, loss: 1.3657
2022-07-12 12:05:43 - train: epoch 0052, iter [04900, 05004], lr: 0.101722, loss: 1.5958
2022-07-12 12:06:35 - train: epoch 0052, iter [05000, 05004], lr: 0.101656, loss: 1.6982
2022-07-12 12:06:38 - train: epoch 052, train_loss: 1.6489
2022-07-12 12:08:46 - eval: epoch: 052, acc1: 65.744%, acc5: 87.408%, test_loss: 1.4059, per_image_load_time: 2.087ms, per_image_inference_time: 0.850ms
2022-07-12 12:08:46 - until epoch: 052, best_acc1: 65.744%
2022-07-12 12:08:46 - epoch 053 lr: 0.101653
2022-07-12 12:09:51 - train: epoch 0053, iter [00100, 05004], lr: 0.101587, loss: 1.8001
2022-07-12 12:10:46 - train: epoch 0053, iter [00200, 05004], lr: 0.101521, loss: 1.6982
2022-07-12 12:11:40 - train: epoch 0053, iter [00300, 05004], lr: 0.101455, loss: 1.6339
2022-07-12 12:12:35 - train: epoch 0053, iter [00400, 05004], lr: 0.101389, loss: 1.6783
2022-07-12 12:13:30 - train: epoch 0053, iter [00500, 05004], lr: 0.101323, loss: 1.8501
2022-07-12 12:14:25 - train: epoch 0053, iter [00600, 05004], lr: 0.101257, loss: 1.5705
2022-07-12 12:15:20 - train: epoch 0053, iter [00700, 05004], lr: 0.101191, loss: 1.5845
2022-07-12 12:16:14 - train: epoch 0053, iter [00800, 05004], lr: 0.101125, loss: 1.6383
2022-07-12 12:17:10 - train: epoch 0053, iter [00900, 05004], lr: 0.101059, loss: 1.6703
2022-07-12 12:18:05 - train: epoch 0053, iter [01000, 05004], lr: 0.100993, loss: 1.6102
2022-07-12 12:18:59 - train: epoch 0053, iter [01100, 05004], lr: 0.100927, loss: 1.4676
2022-07-12 12:19:54 - train: epoch 0053, iter [01200, 05004], lr: 0.100860, loss: 1.3887
2022-07-12 12:20:49 - train: epoch 0053, iter [01300, 05004], lr: 0.100794, loss: 1.6333
2022-07-12 12:21:44 - train: epoch 0053, iter [01400, 05004], lr: 0.100728, loss: 1.7144
2022-07-12 12:22:38 - train: epoch 0053, iter [01500, 05004], lr: 0.100662, loss: 1.6552
2022-07-12 12:23:34 - train: epoch 0053, iter [01600, 05004], lr: 0.100596, loss: 1.8425
2022-07-12 12:24:29 - train: epoch 0053, iter [01700, 05004], lr: 0.100530, loss: 1.7936
2022-07-12 12:25:23 - train: epoch 0053, iter [01800, 05004], lr: 0.100464, loss: 1.7842
2022-07-12 12:26:18 - train: epoch 0053, iter [01900, 05004], lr: 0.100398, loss: 1.5801
2022-07-12 12:27:12 - train: epoch 0053, iter [02000, 05004], lr: 0.100332, loss: 1.6268
2022-07-12 12:28:04 - train: epoch 0053, iter [02100, 05004], lr: 0.100266, loss: 1.5859
2022-07-12 12:28:58 - train: epoch 0053, iter [02200, 05004], lr: 0.100200, loss: 1.4940
2022-07-12 12:29:52 - train: epoch 0053, iter [02300, 05004], lr: 0.100133, loss: 1.6463
2022-07-12 12:30:48 - train: epoch 0053, iter [02400, 05004], lr: 0.100067, loss: 1.7372
2022-07-12 12:31:42 - train: epoch 0053, iter [02500, 05004], lr: 0.100001, loss: 1.7362
2022-07-12 12:32:35 - train: epoch 0053, iter [02600, 05004], lr: 0.099935, loss: 1.7870
2022-07-12 12:33:29 - train: epoch 0053, iter [02700, 05004], lr: 0.099869, loss: 1.6357
2022-07-12 12:34:24 - train: epoch 0053, iter [02800, 05004], lr: 0.099803, loss: 1.8473
2022-07-12 12:35:17 - train: epoch 0053, iter [02900, 05004], lr: 0.099737, loss: 1.4842
2022-07-12 12:36:11 - train: epoch 0053, iter [03000, 05004], lr: 0.099671, loss: 1.5556
2022-07-12 12:37:04 - train: epoch 0053, iter [03100, 05004], lr: 0.099605, loss: 1.7920
2022-07-12 12:37:58 - train: epoch 0053, iter [03200, 05004], lr: 0.099539, loss: 1.7819
2022-07-12 12:38:51 - train: epoch 0053, iter [03300, 05004], lr: 0.099473, loss: 1.6059
2022-07-12 12:39:45 - train: epoch 0053, iter [03400, 05004], lr: 0.099407, loss: 1.5884
2022-07-12 12:40:39 - train: epoch 0053, iter [03500, 05004], lr: 0.099340, loss: 1.6525
2022-07-12 12:41:33 - train: epoch 0053, iter [03600, 05004], lr: 0.099274, loss: 1.6364
2022-07-12 12:42:28 - train: epoch 0053, iter [03700, 05004], lr: 0.099208, loss: 1.6810
2022-07-12 12:43:21 - train: epoch 0053, iter [03800, 05004], lr: 0.099142, loss: 1.6681
2022-07-12 12:44:15 - train: epoch 0053, iter [03900, 05004], lr: 0.099076, loss: 2.0863
2022-07-12 12:45:09 - train: epoch 0053, iter [04000, 05004], lr: 0.099010, loss: 1.7769
2022-07-12 12:46:02 - train: epoch 0053, iter [04100, 05004], lr: 0.098944, loss: 1.7654
2022-07-12 12:46:56 - train: epoch 0053, iter [04200, 05004], lr: 0.098878, loss: 1.7762
2022-07-12 12:47:49 - train: epoch 0053, iter [04300, 05004], lr: 0.098812, loss: 1.8583
2022-07-12 12:48:42 - train: epoch 0053, iter [04400, 05004], lr: 0.098746, loss: 1.6172
2022-07-12 12:49:35 - train: epoch 0053, iter [04500, 05004], lr: 0.098680, loss: 1.8549
2022-07-12 12:50:29 - train: epoch 0053, iter [04600, 05004], lr: 0.098614, loss: 1.5222
2022-07-12 12:51:21 - train: epoch 0053, iter [04700, 05004], lr: 0.098547, loss: 1.5696
2022-07-12 12:52:15 - train: epoch 0053, iter [04800, 05004], lr: 0.098481, loss: 1.8077
2022-07-12 12:53:08 - train: epoch 0053, iter [04900, 05004], lr: 0.098415, loss: 1.6571
2022-07-12 12:54:00 - train: epoch 0053, iter [05000, 05004], lr: 0.098349, loss: 1.5911
2022-07-12 12:54:03 - train: epoch 053, train_loss: 1.6380
2022-07-12 12:56:10 - eval: epoch: 053, acc1: 65.920%, acc5: 87.490%, test_loss: 1.3938, per_image_load_time: 1.977ms, per_image_inference_time: 0.812ms
2022-07-12 12:56:10 - until epoch: 053, best_acc1: 65.920%
2022-07-12 12:56:10 - epoch 054 lr: 0.098346
2022-07-12 12:57:16 - train: epoch 0054, iter [00100, 05004], lr: 0.098281, loss: 1.6499
2022-07-12 12:58:10 - train: epoch 0054, iter [00200, 05004], lr: 0.098214, loss: 1.6095
2022-07-12 12:59:04 - train: epoch 0054, iter [00300, 05004], lr: 0.098148, loss: 1.5574
2022-07-12 12:59:57 - train: epoch 0054, iter [00400, 05004], lr: 0.098082, loss: 1.4821
2022-07-12 13:00:50 - train: epoch 0054, iter [00500, 05004], lr: 0.098016, loss: 1.4423
2022-07-12 13:01:44 - train: epoch 0054, iter [00600, 05004], lr: 0.097950, loss: 1.6369
2022-07-12 13:02:38 - train: epoch 0054, iter [00700, 05004], lr: 0.097884, loss: 1.6287
2022-07-12 13:03:32 - train: epoch 0054, iter [00800, 05004], lr: 0.097818, loss: 1.7393
2022-07-12 13:04:25 - train: epoch 0054, iter [00900, 05004], lr: 0.097752, loss: 1.6064
2022-07-12 13:05:19 - train: epoch 0054, iter [01000, 05004], lr: 0.097686, loss: 1.4289
2022-07-12 13:06:12 - train: epoch 0054, iter [01100, 05004], lr: 0.097620, loss: 1.4772
2022-07-12 13:07:06 - train: epoch 0054, iter [01200, 05004], lr: 0.097554, loss: 1.7350
2022-07-12 13:07:59 - train: epoch 0054, iter [01300, 05004], lr: 0.097488, loss: 1.6904
2022-07-12 13:08:53 - train: epoch 0054, iter [01400, 05004], lr: 0.097422, loss: 1.7673
2022-07-12 13:09:46 - train: epoch 0054, iter [01500, 05004], lr: 0.097356, loss: 1.4029
2022-07-12 13:10:40 - train: epoch 0054, iter [01600, 05004], lr: 0.097289, loss: 1.4814
2022-07-12 13:11:33 - train: epoch 0054, iter [01700, 05004], lr: 0.097223, loss: 1.5234
2022-07-12 13:12:27 - train: epoch 0054, iter [01800, 05004], lr: 0.097157, loss: 1.3931
2022-07-12 13:13:20 - train: epoch 0054, iter [01900, 05004], lr: 0.097091, loss: 1.8790
2022-07-12 13:14:14 - train: epoch 0054, iter [02000, 05004], lr: 0.097025, loss: 1.6352
2022-07-12 13:15:07 - train: epoch 0054, iter [02100, 05004], lr: 0.096959, loss: 1.5260
2022-07-12 13:16:01 - train: epoch 0054, iter [02200, 05004], lr: 0.096893, loss: 1.6441
2022-07-12 13:16:55 - train: epoch 0054, iter [02300, 05004], lr: 0.096827, loss: 1.5168
2022-07-12 13:17:50 - train: epoch 0054, iter [02400, 05004], lr: 0.096761, loss: 1.7400
2022-07-12 13:18:44 - train: epoch 0054, iter [02500, 05004], lr: 0.096695, loss: 1.7354
2022-07-12 13:19:38 - train: epoch 0054, iter [02600, 05004], lr: 0.096629, loss: 1.4349
2022-07-12 13:20:32 - train: epoch 0054, iter [02700, 05004], lr: 0.096563, loss: 1.6036
2022-07-12 13:21:27 - train: epoch 0054, iter [02800, 05004], lr: 0.096497, loss: 1.8938
2022-07-12 13:22:21 - train: epoch 0054, iter [02900, 05004], lr: 0.096431, loss: 1.3679
2022-07-12 13:23:14 - train: epoch 0054, iter [03000, 05004], lr: 0.096365, loss: 1.8119
2022-07-12 13:24:09 - train: epoch 0054, iter [03100, 05004], lr: 0.096299, loss: 1.6901
2022-07-12 13:25:03 - train: epoch 0054, iter [03200, 05004], lr: 0.096233, loss: 1.7798
2022-07-12 13:25:58 - train: epoch 0054, iter [03300, 05004], lr: 0.096167, loss: 1.4730
2022-07-12 13:26:52 - train: epoch 0054, iter [03400, 05004], lr: 0.096101, loss: 1.5539
2022-07-12 13:27:46 - train: epoch 0054, iter [03500, 05004], lr: 0.096035, loss: 1.6733
2022-07-12 13:28:40 - train: epoch 0054, iter [03600, 05004], lr: 0.095969, loss: 1.6575
2022-07-12 13:29:35 - train: epoch 0054, iter [03700, 05004], lr: 0.095902, loss: 1.5545
2022-07-12 13:30:30 - train: epoch 0054, iter [03800, 05004], lr: 0.095836, loss: 1.5243
2022-07-12 13:31:25 - train: epoch 0054, iter [03900, 05004], lr: 0.095770, loss: 1.6420
2022-07-12 13:32:19 - train: epoch 0054, iter [04000, 05004], lr: 0.095704, loss: 1.5895
2022-07-12 13:33:13 - train: epoch 0054, iter [04100, 05004], lr: 0.095638, loss: 1.6069
2022-07-12 13:34:07 - train: epoch 0054, iter [04200, 05004], lr: 0.095572, loss: 1.7213
2022-07-12 13:35:01 - train: epoch 0054, iter [04300, 05004], lr: 0.095506, loss: 1.7029
2022-07-12 13:35:55 - train: epoch 0054, iter [04400, 05004], lr: 0.095440, loss: 1.6199
2022-07-12 13:36:51 - train: epoch 0054, iter [04500, 05004], lr: 0.095374, loss: 1.5009
2022-07-12 13:37:45 - train: epoch 0054, iter [04600, 05004], lr: 0.095308, loss: 1.8454
2022-07-12 13:38:40 - train: epoch 0054, iter [04700, 05004], lr: 0.095242, loss: 1.8970
2022-07-12 13:39:35 - train: epoch 0054, iter [04800, 05004], lr: 0.095176, loss: 1.5513
2022-07-12 13:40:30 - train: epoch 0054, iter [04900, 05004], lr: 0.095110, loss: 1.4644
2022-07-12 13:41:23 - train: epoch 0054, iter [05000, 05004], lr: 0.095044, loss: 1.5513
2022-07-12 13:41:25 - train: epoch 054, train_loss: 1.6233
2022-07-12 13:43:35 - eval: epoch: 054, acc1: 65.970%, acc5: 87.688%, test_loss: 1.3911, per_image_load_time: 2.296ms, per_image_inference_time: 0.856ms
2022-07-12 13:43:35 - until epoch: 054, best_acc1: 65.970%
2022-07-12 13:43:35 - epoch 055 lr: 0.095041
2022-07-12 13:44:42 - train: epoch 0055, iter [00100, 05004], lr: 0.094976, loss: 1.4478
2022-07-12 13:45:36 - train: epoch 0055, iter [00200, 05004], lr: 0.094910, loss: 1.5259
2022-07-12 13:46:29 - train: epoch 0055, iter [00300, 05004], lr: 0.094844, loss: 1.5037
2022-07-12 13:47:24 - train: epoch 0055, iter [00400, 05004], lr: 0.094778, loss: 1.4525
2022-07-12 13:48:17 - train: epoch 0055, iter [00500, 05004], lr: 0.094712, loss: 1.4647
2022-07-12 13:49:11 - train: epoch 0055, iter [00600, 05004], lr: 0.094646, loss: 1.4485
2022-07-12 13:50:04 - train: epoch 0055, iter [00700, 05004], lr: 0.094580, loss: 1.5701
2022-07-12 13:50:57 - train: epoch 0055, iter [00800, 05004], lr: 0.094514, loss: 1.5312
2022-07-12 13:51:50 - train: epoch 0055, iter [00900, 05004], lr: 0.094448, loss: 1.5848
2022-07-12 13:52:44 - train: epoch 0055, iter [01000, 05004], lr: 0.094382, loss: 1.6763
2022-07-12 13:53:37 - train: epoch 0055, iter [01100, 05004], lr: 0.094316, loss: 1.4347
2022-07-12 13:54:31 - train: epoch 0055, iter [01200, 05004], lr: 0.094250, loss: 1.5941
2022-07-12 13:55:25 - train: epoch 0055, iter [01300, 05004], lr: 0.094184, loss: 1.6669
2022-07-12 13:56:19 - train: epoch 0055, iter [01400, 05004], lr: 0.094118, loss: 1.6014
2022-07-12 13:57:13 - train: epoch 0055, iter [01500, 05004], lr: 0.094052, loss: 1.5101
2022-07-12 13:58:07 - train: epoch 0055, iter [01600, 05004], lr: 0.093986, loss: 1.7557
2022-07-12 13:59:01 - train: epoch 0055, iter [01700, 05004], lr: 0.093920, loss: 1.7062
2022-07-12 13:59:55 - train: epoch 0055, iter [01800, 05004], lr: 0.093854, loss: 1.5278
2022-07-12 14:00:49 - train: epoch 0055, iter [01900, 05004], lr: 0.093788, loss: 1.6001
2022-07-12 14:01:43 - train: epoch 0055, iter [02000, 05004], lr: 0.093722, loss: 1.6152
2022-07-12 14:02:37 - train: epoch 0055, iter [02100, 05004], lr: 0.093656, loss: 1.4166
2022-07-12 14:03:31 - train: epoch 0055, iter [02200, 05004], lr: 0.093590, loss: 1.7266
2022-07-12 14:04:25 - train: epoch 0055, iter [02300, 05004], lr: 0.093524, loss: 1.4772
2022-07-12 14:05:19 - train: epoch 0055, iter [02400, 05004], lr: 0.093458, loss: 1.5179
2022-07-12 14:06:14 - train: epoch 0055, iter [02500, 05004], lr: 0.093392, loss: 1.6405
2022-07-12 14:07:08 - train: epoch 0055, iter [02600, 05004], lr: 0.093326, loss: 1.6912
2022-07-12 14:08:02 - train: epoch 0055, iter [02700, 05004], lr: 0.093260, loss: 1.5194
2022-07-12 14:08:56 - train: epoch 0055, iter [02800, 05004], lr: 0.093194, loss: 1.8305
2022-07-12 14:09:50 - train: epoch 0055, iter [02900, 05004], lr: 0.093129, loss: 1.5584
2022-07-12 14:10:43 - train: epoch 0055, iter [03000, 05004], lr: 0.093063, loss: 1.6335
2022-07-12 14:11:37 - train: epoch 0055, iter [03100, 05004], lr: 0.092997, loss: 1.6216
2022-07-12 14:12:31 - train: epoch 0055, iter [03200, 05004], lr: 0.092931, loss: 1.6821
2022-07-12 14:13:25 - train: epoch 0055, iter [03300, 05004], lr: 0.092865, loss: 1.3970
2022-07-12 14:14:19 - train: epoch 0055, iter [03400, 05004], lr: 0.092799, loss: 1.5945
2022-07-12 14:15:12 - train: epoch 0055, iter [03500, 05004], lr: 0.092733, loss: 1.5389
2022-07-12 14:16:07 - train: epoch 0055, iter [03600, 05004], lr: 0.092667, loss: 1.7563
2022-07-12 14:17:00 - train: epoch 0055, iter [03700, 05004], lr: 0.092601, loss: 1.5235
2022-07-12 14:17:55 - train: epoch 0055, iter [03800, 05004], lr: 0.092535, loss: 1.6919
2022-07-12 14:18:48 - train: epoch 0055, iter [03900, 05004], lr: 0.092469, loss: 2.0142
2022-07-12 14:19:41 - train: epoch 0055, iter [04000, 05004], lr: 0.092403, loss: 1.5495
2022-07-12 14:20:35 - train: epoch 0055, iter [04100, 05004], lr: 0.092338, loss: 1.6791
2022-07-12 14:21:28 - train: epoch 0055, iter [04200, 05004], lr: 0.092272, loss: 1.6854
2022-07-12 14:22:22 - train: epoch 0055, iter [04300, 05004], lr: 0.092206, loss: 1.7358
2022-07-12 14:23:17 - train: epoch 0055, iter [04400, 05004], lr: 0.092140, loss: 1.7314
2022-07-12 14:24:11 - train: epoch 0055, iter [04500, 05004], lr: 0.092074, loss: 1.6623
2022-07-12 14:25:04 - train: epoch 0055, iter [04600, 05004], lr: 0.092008, loss: 1.6640
2022-07-12 14:25:58 - train: epoch 0055, iter [04700, 05004], lr: 0.091942, loss: 1.6302
2022-07-12 14:26:51 - train: epoch 0055, iter [04800, 05004], lr: 0.091876, loss: 1.6696
2022-07-12 14:27:46 - train: epoch 0055, iter [04900, 05004], lr: 0.091811, loss: 1.5970
2022-07-12 14:28:39 - train: epoch 0055, iter [05000, 05004], lr: 0.091745, loss: 1.6803
2022-07-12 14:28:41 - train: epoch 055, train_loss: 1.6114
2022-07-12 14:30:52 - eval: epoch: 055, acc1: 66.728%, acc5: 88.072%, test_loss: 1.3616, per_image_load_time: 2.597ms, per_image_inference_time: 0.844ms
2022-07-12 14:30:52 - until epoch: 055, best_acc1: 66.728%
2022-07-12 14:30:52 - epoch 056 lr: 0.091741
2022-07-12 14:31:57 - train: epoch 0056, iter [00100, 05004], lr: 0.091676, loss: 1.4749
2022-07-12 14:32:51 - train: epoch 0056, iter [00200, 05004], lr: 0.091610, loss: 1.5476
2022-07-12 14:33:44 - train: epoch 0056, iter [00300, 05004], lr: 0.091545, loss: 1.3618
2022-07-12 14:34:38 - train: epoch 0056, iter [00400, 05004], lr: 0.091479, loss: 1.6033
2022-07-12 14:35:31 - train: epoch 0056, iter [00500, 05004], lr: 0.091413, loss: 1.6251
2022-07-12 14:36:23 - train: epoch 0056, iter [00600, 05004], lr: 0.091347, loss: 1.5395
2022-07-12 14:37:16 - train: epoch 0056, iter [00700, 05004], lr: 0.091281, loss: 1.7927
2022-07-12 14:38:08 - train: epoch 0056, iter [00800, 05004], lr: 0.091215, loss: 1.6921
2022-07-12 14:39:00 - train: epoch 0056, iter [00900, 05004], lr: 0.091149, loss: 1.6255
2022-07-12 14:39:53 - train: epoch 0056, iter [01000, 05004], lr: 0.091084, loss: 1.6184
2022-07-12 14:40:46 - train: epoch 0056, iter [01100, 05004], lr: 0.091018, loss: 1.4242
2022-07-12 14:41:39 - train: epoch 0056, iter [01200, 05004], lr: 0.090952, loss: 1.5863
2022-07-12 14:42:33 - train: epoch 0056, iter [01300, 05004], lr: 0.090886, loss: 1.7036
2022-07-12 14:43:28 - train: epoch 0056, iter [01400, 05004], lr: 0.090820, loss: 1.4568
2022-07-12 14:44:24 - train: epoch 0056, iter [01500, 05004], lr: 0.090755, loss: 1.8277
2022-07-12 14:45:17 - train: epoch 0056, iter [01600, 05004], lr: 0.090689, loss: 1.4351
2022-07-12 14:46:10 - train: epoch 0056, iter [01700, 05004], lr: 0.090623, loss: 1.6705
2022-07-12 14:47:03 - train: epoch 0056, iter [01800, 05004], lr: 0.090557, loss: 1.6627
2022-07-12 14:47:56 - train: epoch 0056, iter [01900, 05004], lr: 0.090491, loss: 1.7187
2022-07-12 14:48:49 - train: epoch 0056, iter [02000, 05004], lr: 0.090426, loss: 1.6313
2022-07-12 14:49:43 - train: epoch 0056, iter [02100, 05004], lr: 0.090360, loss: 1.6170
2022-07-12 14:50:36 - train: epoch 0056, iter [02200, 05004], lr: 0.090294, loss: 1.7203
2022-07-12 14:51:30 - train: epoch 0056, iter [02300, 05004], lr: 0.090228, loss: 1.6812
2022-07-12 14:52:24 - train: epoch 0056, iter [02400, 05004], lr: 0.090163, loss: 1.5866
2022-07-12 14:53:18 - train: epoch 0056, iter [02500, 05004], lr: 0.090097, loss: 1.9316
2022-07-12 14:54:11 - train: epoch 0056, iter [02600, 05004], lr: 0.090031, loss: 1.5096
2022-07-12 14:55:05 - train: epoch 0056, iter [02700, 05004], lr: 0.089965, loss: 1.5259
2022-07-12 14:55:59 - train: epoch 0056, iter [02800, 05004], lr: 0.089899, loss: 1.6121
2022-07-12 14:56:53 - train: epoch 0056, iter [02900, 05004], lr: 0.089834, loss: 1.8126
2022-07-12 14:57:48 - train: epoch 0056, iter [03000, 05004], lr: 0.089768, loss: 1.6968
2022-07-12 14:58:42 - train: epoch 0056, iter [03100, 05004], lr: 0.089702, loss: 1.3328
2022-07-12 14:59:36 - train: epoch 0056, iter [03200, 05004], lr: 0.089637, loss: 1.6098
2022-07-12 15:00:30 - train: epoch 0056, iter [03300, 05004], lr: 0.089571, loss: 1.6629
2022-07-12 15:01:23 - train: epoch 0056, iter [03400, 05004], lr: 0.089505, loss: 1.5609
2022-07-12 15:02:17 - train: epoch 0056, iter [03500, 05004], lr: 0.089439, loss: 1.6007
2022-07-12 15:03:10 - train: epoch 0056, iter [03600, 05004], lr: 0.089374, loss: 1.4746
2022-07-12 15:04:05 - train: epoch 0056, iter [03700, 05004], lr: 0.089308, loss: 1.6232
2022-07-12 15:04:58 - train: epoch 0056, iter [03800, 05004], lr: 0.089242, loss: 1.5803
2022-07-12 15:05:52 - train: epoch 0056, iter [03900, 05004], lr: 0.089177, loss: 1.8751
2022-07-12 15:06:46 - train: epoch 0056, iter [04000, 05004], lr: 0.089111, loss: 1.8177
2022-07-12 15:07:39 - train: epoch 0056, iter [04100, 05004], lr: 0.089045, loss: 1.9221
2022-07-12 15:08:33 - train: epoch 0056, iter [04200, 05004], lr: 0.088979, loss: 1.6797
2022-07-12 15:09:27 - train: epoch 0056, iter [04300, 05004], lr: 0.088914, loss: 1.5399
2022-07-12 15:10:20 - train: epoch 0056, iter [04400, 05004], lr: 0.088848, loss: 1.7309
2022-07-12 15:11:14 - train: epoch 0056, iter [04500, 05004], lr: 0.088782, loss: 1.6587
2022-07-12 15:12:07 - train: epoch 0056, iter [04600, 05004], lr: 0.088717, loss: 1.5732
2022-07-12 15:13:00 - train: epoch 0056, iter [04700, 05004], lr: 0.088651, loss: 1.6828
2022-07-12 15:13:53 - train: epoch 0056, iter [04800, 05004], lr: 0.088585, loss: 1.5389
2022-07-12 15:14:46 - train: epoch 0056, iter [04900, 05004], lr: 0.088520, loss: 1.6566
2022-07-12 15:15:39 - train: epoch 0056, iter [05000, 05004], lr: 0.088454, loss: 1.7472
2022-07-12 15:15:42 - train: epoch 056, train_loss: 1.5966
2022-07-12 15:17:52 - eval: epoch: 056, acc1: 66.822%, acc5: 87.924%, test_loss: 1.3652, per_image_load_time: 2.630ms, per_image_inference_time: 0.871ms
2022-07-12 15:17:52 - until epoch: 056, best_acc1: 66.822%
2022-07-12 15:17:52 - epoch 057 lr: 0.088451
2022-07-12 15:18:59 - train: epoch 0057, iter [00100, 05004], lr: 0.088386, loss: 1.4544
2022-07-12 15:19:53 - train: epoch 0057, iter [00200, 05004], lr: 0.088320, loss: 1.5023
2022-07-12 15:20:48 - train: epoch 0057, iter [00300, 05004], lr: 0.088255, loss: 1.5708
2022-07-12 15:21:41 - train: epoch 0057, iter [00400, 05004], lr: 0.088189, loss: 1.4874
2022-07-12 15:22:35 - train: epoch 0057, iter [00500, 05004], lr: 0.088123, loss: 1.3480
2022-07-12 15:23:28 - train: epoch 0057, iter [00600, 05004], lr: 0.088058, loss: 1.7345
2022-07-12 15:24:21 - train: epoch 0057, iter [00700, 05004], lr: 0.087992, loss: 1.4224
2022-07-12 15:25:14 - train: epoch 0057, iter [00800, 05004], lr: 0.087927, loss: 1.6063
2022-07-12 15:26:07 - train: epoch 0057, iter [00900, 05004], lr: 0.087861, loss: 1.7400
2022-07-12 15:27:01 - train: epoch 0057, iter [01000, 05004], lr: 0.087795, loss: 1.5233
2022-07-12 15:27:56 - train: epoch 0057, iter [01100, 05004], lr: 0.087730, loss: 1.6009
2022-07-12 15:28:50 - train: epoch 0057, iter [01200, 05004], lr: 0.087664, loss: 1.4940
2022-07-12 15:29:44 - train: epoch 0057, iter [01300, 05004], lr: 0.087599, loss: 1.6575
2022-07-12 15:30:38 - train: epoch 0057, iter [01400, 05004], lr: 0.087533, loss: 1.7024
2022-07-12 15:31:32 - train: epoch 0057, iter [01500, 05004], lr: 0.087467, loss: 1.6065
2022-07-12 15:32:26 - train: epoch 0057, iter [01600, 05004], lr: 0.087402, loss: 1.7503
2022-07-12 15:33:21 - train: epoch 0057, iter [01700, 05004], lr: 0.087336, loss: 1.6694
2022-07-12 15:34:15 - train: epoch 0057, iter [01800, 05004], lr: 0.087271, loss: 1.6264
2022-07-12 15:35:10 - train: epoch 0057, iter [01900, 05004], lr: 0.087205, loss: 1.5913
2022-07-12 15:36:04 - train: epoch 0057, iter [02000, 05004], lr: 0.087140, loss: 1.7170
2022-07-12 15:36:58 - train: epoch 0057, iter [02100, 05004], lr: 0.087074, loss: 1.5052
2022-07-12 15:37:52 - train: epoch 0057, iter [02200, 05004], lr: 0.087009, loss: 1.6146
2022-07-12 15:38:45 - train: epoch 0057, iter [02300, 05004], lr: 0.086943, loss: 1.7315
2022-07-12 15:39:39 - train: epoch 0057, iter [02400, 05004], lr: 0.086878, loss: 1.6543
2022-07-12 15:40:32 - train: epoch 0057, iter [02500, 05004], lr: 0.086812, loss: 1.6880
2022-07-12 15:41:26 - train: epoch 0057, iter [02600, 05004], lr: 0.086747, loss: 1.4265
2022-07-12 15:42:19 - train: epoch 0057, iter [02700, 05004], lr: 0.086681, loss: 1.4490
2022-07-12 15:43:13 - train: epoch 0057, iter [02800, 05004], lr: 0.086616, loss: 1.2597
2022-07-12 15:44:07 - train: epoch 0057, iter [02900, 05004], lr: 0.086550, loss: 1.5603
2022-07-12 15:44:59 - train: epoch 0057, iter [03000, 05004], lr: 0.086485, loss: 1.6113
2022-07-12 15:45:52 - train: epoch 0057, iter [03100, 05004], lr: 0.086419, loss: 1.6294
2022-07-12 15:46:45 - train: epoch 0057, iter [03200, 05004], lr: 0.086354, loss: 1.7044
2022-07-12 15:47:38 - train: epoch 0057, iter [03300, 05004], lr: 0.086288, loss: 1.5353
2022-07-12 15:48:32 - train: epoch 0057, iter [03400, 05004], lr: 0.086223, loss: 1.7522
2022-07-12 15:49:25 - train: epoch 0057, iter [03500, 05004], lr: 0.086157, loss: 1.5623
2022-07-12 15:50:18 - train: epoch 0057, iter [03600, 05004], lr: 0.086092, loss: 1.4372
2022-07-12 15:51:11 - train: epoch 0057, iter [03700, 05004], lr: 0.086026, loss: 1.5652
2022-07-12 15:52:05 - train: epoch 0057, iter [03800, 05004], lr: 0.085961, loss: 1.5308
2022-07-12 15:52:57 - train: epoch 0057, iter [03900, 05004], lr: 0.085896, loss: 1.8706
2022-07-12 15:53:51 - train: epoch 0057, iter [04000, 05004], lr: 0.085830, loss: 1.5171
2022-07-12 15:54:45 - train: epoch 0057, iter [04100, 05004], lr: 0.085765, loss: 1.5943
2022-07-12 15:55:38 - train: epoch 0057, iter [04200, 05004], lr: 0.085699, loss: 1.6210
2022-07-12 15:56:31 - train: epoch 0057, iter [04300, 05004], lr: 0.085634, loss: 1.4419
2022-07-12 15:57:24 - train: epoch 0057, iter [04400, 05004], lr: 0.085568, loss: 1.5451
2022-07-12 15:58:17 - train: epoch 0057, iter [04500, 05004], lr: 0.085503, loss: 1.8247
2022-07-12 15:59:10 - train: epoch 0057, iter [04600, 05004], lr: 0.085438, loss: 1.7370
2022-07-12 16:00:03 - train: epoch 0057, iter [04700, 05004], lr: 0.085372, loss: 1.6137
2022-07-12 16:00:57 - train: epoch 0057, iter [04800, 05004], lr: 0.085307, loss: 1.8034
2022-07-12 16:01:51 - train: epoch 0057, iter [04900, 05004], lr: 0.085242, loss: 1.7333
2022-07-12 16:02:43 - train: epoch 0057, iter [05000, 05004], lr: 0.085176, loss: 1.6020
2022-07-12 16:02:45 - train: epoch 057, train_loss: 1.5837
2022-07-12 16:04:55 - eval: epoch: 057, acc1: 66.378%, acc5: 87.716%, test_loss: 1.3742, per_image_load_time: 2.179ms, per_image_inference_time: 0.834ms
2022-07-12 16:04:55 - until epoch: 057, best_acc1: 66.822%
2022-07-12 16:04:55 - epoch 058 lr: 0.085173
2022-07-12 16:06:02 - train: epoch 0058, iter [00100, 05004], lr: 0.085108, loss: 1.5634
2022-07-12 16:06:58 - train: epoch 0058, iter [00200, 05004], lr: 0.085043, loss: 1.4582
2022-07-12 16:07:53 - train: epoch 0058, iter [00300, 05004], lr: 0.084978, loss: 1.4540
2022-07-12 16:08:46 - train: epoch 0058, iter [00400, 05004], lr: 0.084912, loss: 1.5137
2022-07-12 16:09:41 - train: epoch 0058, iter [00500, 05004], lr: 0.084847, loss: 1.3990
2022-07-12 16:10:36 - train: epoch 0058, iter [00600, 05004], lr: 0.084782, loss: 1.7199
2022-07-12 16:11:32 - train: epoch 0058, iter [00700, 05004], lr: 0.084716, loss: 1.6219
2022-07-12 16:12:26 - train: epoch 0058, iter [00800, 05004], lr: 0.084651, loss: 1.6427
2022-07-12 16:13:21 - train: epoch 0058, iter [00900, 05004], lr: 0.084586, loss: 1.2293
2022-07-12 16:14:17 - train: epoch 0058, iter [01000, 05004], lr: 0.084520, loss: 1.5534
2022-07-12 16:15:11 - train: epoch 0058, iter [01100, 05004], lr: 0.084455, loss: 1.4781
2022-07-12 16:16:05 - train: epoch 0058, iter [01200, 05004], lr: 0.084390, loss: 1.5110
2022-07-12 16:17:00 - train: epoch 0058, iter [01300, 05004], lr: 0.084325, loss: 1.6816
2022-07-12 16:17:53 - train: epoch 0058, iter [01400, 05004], lr: 0.084259, loss: 1.7400
2022-07-12 16:18:47 - train: epoch 0058, iter [01500, 05004], lr: 0.084194, loss: 1.6100
2022-07-12 16:19:39 - train: epoch 0058, iter [01600, 05004], lr: 0.084129, loss: 1.5477
2022-07-12 16:20:32 - train: epoch 0058, iter [01700, 05004], lr: 0.084064, loss: 1.5721
2022-07-12 16:21:29 - train: epoch 0058, iter [01800, 05004], lr: 0.083998, loss: 1.6420
2022-07-12 16:22:25 - train: epoch 0058, iter [01900, 05004], lr: 0.083933, loss: 1.6445
2022-07-12 16:23:19 - train: epoch 0058, iter [02000, 05004], lr: 0.083868, loss: 1.9482
2022-07-12 16:24:11 - train: epoch 0058, iter [02100, 05004], lr: 0.083803, loss: 1.4693
2022-07-12 16:25:03 - train: epoch 0058, iter [02200, 05004], lr: 0.083737, loss: 1.4657
2022-07-12 16:25:57 - train: epoch 0058, iter [02300, 05004], lr: 0.083672, loss: 1.6297
2022-07-12 16:26:57 - train: epoch 0058, iter [02400, 05004], lr: 0.083607, loss: 1.7011
2022-07-12 16:27:51 - train: epoch 0058, iter [02500, 05004], lr: 0.083542, loss: 1.5849
2022-07-12 16:28:45 - train: epoch 0058, iter [02600, 05004], lr: 0.083477, loss: 1.4733
2022-07-12 16:29:39 - train: epoch 0058, iter [02700, 05004], lr: 0.083411, loss: 1.8336
2022-07-12 16:30:32 - train: epoch 0058, iter [02800, 05004], lr: 0.083346, loss: 1.3721
2022-07-12 16:31:25 - train: epoch 0058, iter [02900, 05004], lr: 0.083281, loss: 1.7178
2022-07-12 16:32:17 - train: epoch 0058, iter [03000, 05004], lr: 0.083216, loss: 1.6623
2022-07-12 16:33:09 - train: epoch 0058, iter [03100, 05004], lr: 0.083151, loss: 1.7048
2022-07-12 16:34:01 - train: epoch 0058, iter [03200, 05004], lr: 0.083086, loss: 1.4817
2022-07-12 16:34:54 - train: epoch 0058, iter [03300, 05004], lr: 0.083021, loss: 1.5127
2022-07-12 16:35:46 - train: epoch 0058, iter [03400, 05004], lr: 0.082955, loss: 1.7965
2022-07-12 16:36:40 - train: epoch 0058, iter [03500, 05004], lr: 0.082890, loss: 1.5115
2022-07-12 16:37:32 - train: epoch 0058, iter [03600, 05004], lr: 0.082825, loss: 1.7024
2022-07-12 16:38:25 - train: epoch 0058, iter [03700, 05004], lr: 0.082760, loss: 1.6318
2022-07-12 16:39:18 - train: epoch 0058, iter [03800, 05004], lr: 0.082695, loss: 1.5342
2022-07-12 16:40:11 - train: epoch 0058, iter [03900, 05004], lr: 0.082630, loss: 1.5372
2022-07-12 16:41:03 - train: epoch 0058, iter [04000, 05004], lr: 0.082565, loss: 1.5226
2022-07-12 16:41:56 - train: epoch 0058, iter [04100, 05004], lr: 0.082500, loss: 1.5209
2022-07-12 16:42:52 - train: epoch 0058, iter [04200, 05004], lr: 0.082435, loss: 1.4508
2022-07-12 16:43:46 - train: epoch 0058, iter [04300, 05004], lr: 0.082370, loss: 1.7134
2022-07-12 16:44:39 - train: epoch 0058, iter [04400, 05004], lr: 0.082305, loss: 1.4066
2022-07-12 16:45:32 - train: epoch 0058, iter [04500, 05004], lr: 0.082240, loss: 1.5062
2022-07-12 16:46:27 - train: epoch 0058, iter [04600, 05004], lr: 0.082175, loss: 1.5205
2022-07-12 16:47:29 - train: epoch 0058, iter [04700, 05004], lr: 0.082110, loss: 1.6970
2022-07-12 16:48:25 - train: epoch 0058, iter [04800, 05004], lr: 0.082045, loss: 1.8697
2022-07-12 16:49:18 - train: epoch 0058, iter [04900, 05004], lr: 0.081980, loss: 1.4513
2022-07-12 16:50:09 - train: epoch 0058, iter [05000, 05004], lr: 0.081915, loss: 1.4086
2022-07-12 16:50:12 - train: epoch 058, train_loss: 1.5680
2022-07-12 16:52:17 - eval: epoch: 058, acc1: 67.562%, acc5: 88.534%, test_loss: 1.3272, per_image_load_time: 2.155ms, per_image_inference_time: 0.807ms
2022-07-12 16:52:17 - until epoch: 058, best_acc1: 67.562%
2022-07-12 16:52:17 - epoch 059 lr: 0.081911
2022-07-12 16:53:26 - train: epoch 0059, iter [00100, 05004], lr: 0.081847, loss: 1.5059
2022-07-12 16:54:22 - train: epoch 0059, iter [00200, 05004], lr: 0.081782, loss: 1.5805
2022-07-12 16:55:17 - train: epoch 0059, iter [00300, 05004], lr: 0.081717, loss: 1.4779
2022-07-12 16:56:15 - train: epoch 0059, iter [00400, 05004], lr: 0.081652, loss: 1.6422
2022-07-12 16:57:13 - train: epoch 0059, iter [00500, 05004], lr: 0.081587, loss: 1.7061
2022-07-12 16:58:10 - train: epoch 0059, iter [00600, 05004], lr: 0.081522, loss: 1.4870
2022-07-12 16:59:05 - train: epoch 0059, iter [00700, 05004], lr: 0.081457, loss: 1.4637
2022-07-12 17:00:00 - train: epoch 0059, iter [00800, 05004], lr: 0.081392, loss: 1.6384
2022-07-12 17:01:00 - train: epoch 0059, iter [00900, 05004], lr: 0.081327, loss: 1.5386
2022-07-12 17:01:59 - train: epoch 0059, iter [01000, 05004], lr: 0.081262, loss: 1.5018
2022-07-12 17:02:56 - train: epoch 0059, iter [01100, 05004], lr: 0.081197, loss: 1.7991
2022-07-12 17:03:52 - train: epoch 0059, iter [01200, 05004], lr: 0.081133, loss: 1.5041
2022-07-12 17:04:50 - train: epoch 0059, iter [01300, 05004], lr: 0.081068, loss: 1.7294
2022-07-12 17:05:48 - train: epoch 0059, iter [01400, 05004], lr: 0.081003, loss: 1.8996
2022-07-12 17:06:47 - train: epoch 0059, iter [01500, 05004], lr: 0.080938, loss: 1.6660
2022-07-12 17:07:41 - train: epoch 0059, iter [01600, 05004], lr: 0.080873, loss: 1.3180
2022-07-12 17:08:36 - train: epoch 0059, iter [01700, 05004], lr: 0.080808, loss: 1.5915
2022-07-12 17:09:34 - train: epoch 0059, iter [01800, 05004], lr: 0.080743, loss: 1.4916
2022-07-12 17:10:31 - train: epoch 0059, iter [01900, 05004], lr: 0.080678, loss: 1.4709
2022-07-12 17:11:27 - train: epoch 0059, iter [02000, 05004], lr: 0.080614, loss: 1.4006
2022-07-12 17:12:23 - train: epoch 0059, iter [02100, 05004], lr: 0.080549, loss: 1.7967
2022-07-12 17:13:20 - train: epoch 0059, iter [02200, 05004], lr: 0.080484, loss: 1.4892
2022-07-12 17:14:19 - train: epoch 0059, iter [02300, 05004], lr: 0.080419, loss: 1.5077
2022-07-12 17:15:14 - train: epoch 0059, iter [02400, 05004], lr: 0.080354, loss: 1.5715
2022-07-12 17:16:08 - train: epoch 0059, iter [02500, 05004], lr: 0.080290, loss: 1.5706
2022-07-12 17:17:03 - train: epoch 0059, iter [02600, 05004], lr: 0.080225, loss: 1.5066
2022-07-12 17:18:05 - train: epoch 0059, iter [02700, 05004], lr: 0.080160, loss: 1.5059
2022-07-12 17:19:03 - train: epoch 0059, iter [02800, 05004], lr: 0.080095, loss: 1.7246
2022-07-12 17:19:59 - train: epoch 0059, iter [02900, 05004], lr: 0.080031, loss: 1.5840
2022-07-12 17:20:56 - train: epoch 0059, iter [03000, 05004], lr: 0.079966, loss: 1.6577
2022-07-12 17:21:55 - train: epoch 0059, iter [03100, 05004], lr: 0.079901, loss: 1.5454
2022-07-12 17:22:55 - train: epoch 0059, iter [03200, 05004], lr: 0.079836, loss: 1.6916
2022-07-12 17:23:52 - train: epoch 0059, iter [03300, 05004], lr: 0.079772, loss: 1.6326
2022-07-12 17:24:49 - train: epoch 0059, iter [03400, 05004], lr: 0.079707, loss: 1.8942
2022-07-12 17:25:46 - train: epoch 0059, iter [03500, 05004], lr: 0.079642, loss: 1.5034
2022-07-12 17:26:46 - train: epoch 0059, iter [03600, 05004], lr: 0.079577, loss: 1.6140
2022-07-12 17:27:46 - train: epoch 0059, iter [03700, 05004], lr: 0.079513, loss: 1.5690
2022-07-12 17:28:43 - train: epoch 0059, iter [03800, 05004], lr: 0.079448, loss: 1.5764
2022-07-12 17:29:38 - train: epoch 0059, iter [03900, 05004], lr: 0.079383, loss: 1.2440
2022-07-12 17:30:35 - train: epoch 0059, iter [04000, 05004], lr: 0.079319, loss: 1.7887
2022-07-12 17:31:34 - train: epoch 0059, iter [04100, 05004], lr: 0.079254, loss: 1.4190
2022-07-12 17:32:30 - train: epoch 0059, iter [04200, 05004], lr: 0.079189, loss: 1.6780
2022-07-12 17:33:25 - train: epoch 0059, iter [04300, 05004], lr: 0.079125, loss: 1.7732
2022-07-12 17:34:19 - train: epoch 0059, iter [04400, 05004], lr: 0.079060, loss: 1.5996
2022-07-12 17:35:14 - train: epoch 0059, iter [04500, 05004], lr: 0.078996, loss: 1.7547
2022-07-12 17:36:11 - train: epoch 0059, iter [04600, 05004], lr: 0.078931, loss: 1.5454
2022-07-12 17:37:08 - train: epoch 0059, iter [04700, 05004], lr: 0.078866, loss: 1.7238
2022-07-12 17:38:03 - train: epoch 0059, iter [04800, 05004], lr: 0.078802, loss: 1.6370
2022-07-12 17:38:57 - train: epoch 0059, iter [04900, 05004], lr: 0.078737, loss: 1.6069
2022-07-12 17:39:52 - train: epoch 0059, iter [05000, 05004], lr: 0.078673, loss: 1.6530
2022-07-12 17:39:55 - train: epoch 059, train_loss: 1.5539
2022-07-12 17:41:59 - eval: epoch: 059, acc1: 67.302%, acc5: 88.402%, test_loss: 1.3331, per_image_load_time: 2.607ms, per_image_inference_time: 0.792ms
2022-07-12 17:41:59 - until epoch: 059, best_acc1: 67.562%
2022-07-12 17:41:59 - epoch 060 lr: 0.078669
2022-07-12 17:43:04 - train: epoch 0060, iter [00100, 05004], lr: 0.078605, loss: 1.5400
2022-07-12 17:44:02 - train: epoch 0060, iter [00200, 05004], lr: 0.078541, loss: 1.3556
2022-07-12 17:45:00 - train: epoch 0060, iter [00300, 05004], lr: 0.078476, loss: 1.3317
2022-07-12 17:45:57 - train: epoch 0060, iter [00400, 05004], lr: 0.078412, loss: 1.7955
2022-07-12 17:46:53 - train: epoch 0060, iter [00500, 05004], lr: 0.078347, loss: 1.6973
2022-07-12 17:47:47 - train: epoch 0060, iter [00600, 05004], lr: 0.078283, loss: 1.5461
2022-07-12 17:48:47 - train: epoch 0060, iter [00700, 05004], lr: 0.078218, loss: 1.4578
2022-07-12 17:49:45 - train: epoch 0060, iter [00800, 05004], lr: 0.078154, loss: 1.6733
2022-07-12 17:50:43 - train: epoch 0060, iter [00900, 05004], lr: 0.078089, loss: 1.4956
2022-07-12 17:51:41 - train: epoch 0060, iter [01000, 05004], lr: 0.078025, loss: 1.1862
2022-07-12 17:52:40 - train: epoch 0060, iter [01100, 05004], lr: 0.077960, loss: 1.4545
2022-07-12 17:53:39 - train: epoch 0060, iter [01200, 05004], lr: 0.077896, loss: 1.4233
2022-07-12 17:54:37 - train: epoch 0060, iter [01300, 05004], lr: 0.077831, loss: 1.3601
2022-07-12 17:55:33 - train: epoch 0060, iter [01400, 05004], lr: 0.077767, loss: 1.6680
2022-07-12 17:56:29 - train: epoch 0060, iter [01500, 05004], lr: 0.077703, loss: 1.5932
2022-07-12 17:57:28 - train: epoch 0060, iter [01600, 05004], lr: 0.077638, loss: 1.4006
2022-07-12 17:58:26 - train: epoch 0060, iter [01700, 05004], lr: 0.077574, loss: 1.5951
2022-07-12 17:59:26 - train: epoch 0060, iter [01800, 05004], lr: 0.077509, loss: 1.6221
2022-07-12 18:00:24 - train: epoch 0060, iter [01900, 05004], lr: 0.077445, loss: 1.8967
2022-07-12 18:01:26 - train: epoch 0060, iter [02000, 05004], lr: 0.077381, loss: 1.6216
2022-07-12 18:02:24 - train: epoch 0060, iter [02100, 05004], lr: 0.077316, loss: 1.6651
2022-07-12 18:03:21 - train: epoch 0060, iter [02200, 05004], lr: 0.077252, loss: 1.8523
2022-07-12 18:04:18 - train: epoch 0060, iter [02300, 05004], lr: 0.077188, loss: 1.5526
2022-07-12 18:05:16 - train: epoch 0060, iter [02400, 05004], lr: 0.077123, loss: 1.5481
2022-07-12 18:06:17 - train: epoch 0060, iter [02500, 05004], lr: 0.077059, loss: 1.5138
2022-07-12 18:07:12 - train: epoch 0060, iter [02600, 05004], lr: 0.076995, loss: 1.6613
2022-07-12 18:08:08 - train: epoch 0060, iter [02700, 05004], lr: 0.076930, loss: 1.4530
2022-07-12 18:09:04 - train: epoch 0060, iter [02800, 05004], lr: 0.076866, loss: 1.7513
2022-07-12 18:10:03 - train: epoch 0060, iter [02900, 05004], lr: 0.076802, loss: 1.5722
2022-07-12 18:11:01 - train: epoch 0060, iter [03000, 05004], lr: 0.076737, loss: 1.6733
2022-07-12 18:11:56 - train: epoch 0060, iter [03100, 05004], lr: 0.076673, loss: 1.7064
2022-07-12 18:12:53 - train: epoch 0060, iter [03200, 05004], lr: 0.076609, loss: 1.5277
2022-07-12 18:13:49 - train: epoch 0060, iter [03300, 05004], lr: 0.076545, loss: 1.3274
2022-07-12 18:14:48 - train: epoch 0060, iter [03400, 05004], lr: 0.076480, loss: 1.5339
2022-07-12 18:15:44 - train: epoch 0060, iter [03500, 05004], lr: 0.076416, loss: 1.6505
2022-07-12 18:16:42 - train: epoch 0060, iter [03600, 05004], lr: 0.076352, loss: 1.6886
2022-07-12 18:17:40 - train: epoch 0060, iter [03700, 05004], lr: 0.076288, loss: 1.6890
2022-07-12 18:18:40 - train: epoch 0060, iter [03800, 05004], lr: 0.076224, loss: 1.6500
2022-07-12 18:19:38 - train: epoch 0060, iter [03900, 05004], lr: 0.076159, loss: 1.5329
2022-07-12 18:20:37 - train: epoch 0060, iter [04000, 05004], lr: 0.076095, loss: 1.5852
2022-07-12 18:21:35 - train: epoch 0060, iter [04100, 05004], lr: 0.076031, loss: 1.8933
2022-07-12 18:22:33 - train: epoch 0060, iter [04200, 05004], lr: 0.075967, loss: 1.4664
2022-07-12 18:23:34 - train: epoch 0060, iter [04300, 05004], lr: 0.075903, loss: 1.2884
2022-07-12 18:24:30 - train: epoch 0060, iter [04400, 05004], lr: 0.075839, loss: 1.7607
2022-07-12 18:25:31 - train: epoch 0060, iter [04500, 05004], lr: 0.075774, loss: 1.6968
2022-07-12 18:26:27 - train: epoch 0060, iter [04600, 05004], lr: 0.075710, loss: 1.5398
2022-07-12 18:27:28 - train: epoch 0060, iter [04700, 05004], lr: 0.075646, loss: 1.3965
2022-07-12 18:28:25 - train: epoch 0060, iter [04800, 05004], lr: 0.075582, loss: 1.3972
2022-07-12 18:29:26 - train: epoch 0060, iter [04900, 05004], lr: 0.075518, loss: 1.5330
2022-07-12 18:30:24 - train: epoch 0060, iter [05000, 05004], lr: 0.075454, loss: 1.7103
2022-07-12 18:30:26 - train: epoch 060, train_loss: 1.5404
2022-07-12 18:32:33 - eval: epoch: 060, acc1: 67.588%, acc5: 88.484%, test_loss: 1.3141, per_image_load_time: 2.382ms, per_image_inference_time: 0.778ms
2022-07-12 18:32:33 - until epoch: 060, best_acc1: 67.588%
2022-07-12 18:32:33 - epoch 061 lr: 0.075451
2022-07-12 18:33:44 - train: epoch 0061, iter [00100, 05004], lr: 0.075387, loss: 1.2938
2022-07-12 18:34:40 - train: epoch 0061, iter [00200, 05004], lr: 0.075323, loss: 1.5002
2022-07-12 18:35:38 - train: epoch 0061, iter [00300, 05004], lr: 0.075259, loss: 1.3169
2022-07-12 18:36:36 - train: epoch 0061, iter [00400, 05004], lr: 0.075195, loss: 1.6796
2022-07-12 18:37:37 - train: epoch 0061, iter [00500, 05004], lr: 0.075131, loss: 1.6012
2022-07-12 18:38:40 - train: epoch 0061, iter [00600, 05004], lr: 0.075067, loss: 1.4788
2022-07-12 18:39:36 - train: epoch 0061, iter [00700, 05004], lr: 0.075003, loss: 1.3225
2022-07-12 18:40:35 - train: epoch 0061, iter [00800, 05004], lr: 0.074939, loss: 1.4715
2022-07-12 18:41:37 - train: epoch 0061, iter [00900, 05004], lr: 0.074875, loss: 1.5093
2022-07-12 18:42:36 - train: epoch 0061, iter [01000, 05004], lr: 0.074811, loss: 1.4546
2022-07-12 18:43:33 - train: epoch 0061, iter [01100, 05004], lr: 0.074747, loss: 1.4111
2022-07-12 18:44:30 - train: epoch 0061, iter [01200, 05004], lr: 0.074683, loss: 1.5648
2022-07-12 18:45:29 - train: epoch 0061, iter [01300, 05004], lr: 0.074620, loss: 1.8245
2022-07-12 18:46:31 - train: epoch 0061, iter [01400, 05004], lr: 0.074556, loss: 1.4079
2022-07-12 18:47:30 - train: epoch 0061, iter [01500, 05004], lr: 0.074492, loss: 1.5543
2022-07-12 18:48:26 - train: epoch 0061, iter [01600, 05004], lr: 0.074428, loss: 1.2298
2022-07-12 18:49:23 - train: epoch 0061, iter [01700, 05004], lr: 0.074364, loss: 1.4911
2022-07-12 18:50:26 - train: epoch 0061, iter [01800, 05004], lr: 0.074300, loss: 1.5750
2022-07-12 18:51:23 - train: epoch 0061, iter [01900, 05004], lr: 0.074236, loss: 1.5913
2022-07-12 18:52:22 - train: epoch 0061, iter [02000, 05004], lr: 0.074172, loss: 1.7797
2022-07-12 18:53:17 - train: epoch 0061, iter [02100, 05004], lr: 0.074109, loss: 1.6302
2022-07-12 18:54:19 - train: epoch 0061, iter [02200, 05004], lr: 0.074045, loss: 1.6847
2022-07-12 18:55:16 - train: epoch 0061, iter [02300, 05004], lr: 0.073981, loss: 1.3272
2022-07-12 18:56:15 - train: epoch 0061, iter [02400, 05004], lr: 0.073917, loss: 1.3159
2022-07-12 18:57:11 - train: epoch 0061, iter [02500, 05004], lr: 0.073853, loss: 1.4626
2022-07-12 18:58:10 - train: epoch 0061, iter [02600, 05004], lr: 0.073790, loss: 1.5071
2022-07-12 18:59:10 - train: epoch 0061, iter [02700, 05004], lr: 0.073726, loss: 1.7254
2022-07-12 19:00:07 - train: epoch 0061, iter [02800, 05004], lr: 0.073662, loss: 1.6004
2022-07-12 19:01:05 - train: epoch 0061, iter [02900, 05004], lr: 0.073598, loss: 1.5601
2022-07-12 19:02:04 - train: epoch 0061, iter [03000, 05004], lr: 0.073534, loss: 1.3726
2022-07-12 19:03:05 - train: epoch 0061, iter [03100, 05004], lr: 0.073471, loss: 1.5621
2022-07-12 19:04:02 - train: epoch 0061, iter [03200, 05004], lr: 0.073407, loss: 1.5089
2022-07-12 19:04:59 - train: epoch 0061, iter [03300, 05004], lr: 0.073343, loss: 1.3875
2022-07-12 19:05:57 - train: epoch 0061, iter [03400, 05004], lr: 0.073280, loss: 1.3672
2022-07-12 19:06:56 - train: epoch 0061, iter [03500, 05004], lr: 0.073216, loss: 1.4712
2022-07-12 19:07:55 - train: epoch 0061, iter [03600, 05004], lr: 0.073152, loss: 1.3912
2022-07-12 19:08:51 - train: epoch 0061, iter [03700, 05004], lr: 0.073089, loss: 1.5098
2022-07-12 19:09:49 - train: epoch 0061, iter [03800, 05004], lr: 0.073025, loss: 1.6054
2022-07-12 19:10:45 - train: epoch 0061, iter [03900, 05004], lr: 0.072961, loss: 1.6601
2022-07-12 19:11:42 - train: epoch 0061, iter [04000, 05004], lr: 0.072898, loss: 1.5340
2022-07-12 19:12:41 - train: epoch 0061, iter [04100, 05004], lr: 0.072834, loss: 1.7070
2022-07-12 19:13:37 - train: epoch 0061, iter [04200, 05004], lr: 0.072771, loss: 1.6379
2022-07-12 19:14:34 - train: epoch 0061, iter [04300, 05004], lr: 0.072707, loss: 1.5012
2022-07-12 19:15:30 - train: epoch 0061, iter [04400, 05004], lr: 0.072643, loss: 1.4883
2022-07-12 19:16:30 - train: epoch 0061, iter [04500, 05004], lr: 0.072580, loss: 1.5441
2022-07-12 19:17:27 - train: epoch 0061, iter [04600, 05004], lr: 0.072516, loss: 1.6503
2022-07-12 19:18:22 - train: epoch 0061, iter [04700, 05004], lr: 0.072453, loss: 1.4464
2022-07-12 19:19:19 - train: epoch 0061, iter [04800, 05004], lr: 0.072389, loss: 1.5782
2022-07-12 19:20:16 - train: epoch 0061, iter [04900, 05004], lr: 0.072326, loss: 1.5337
2022-07-12 19:21:14 - train: epoch 0061, iter [05000, 05004], lr: 0.072262, loss: 1.6712
2022-07-12 19:21:17 - train: epoch 061, train_loss: 1.5194
2022-07-12 19:23:19 - eval: epoch: 061, acc1: 67.658%, acc5: 88.502%, test_loss: 1.3177, per_image_load_time: 1.097ms, per_image_inference_time: 0.765ms
2022-07-12 19:23:19 - until epoch: 061, best_acc1: 67.658%
2022-07-12 19:23:19 - epoch 062 lr: 0.072259
2022-07-12 19:24:26 - train: epoch 0062, iter [00100, 05004], lr: 0.072196, loss: 1.5051
2022-07-12 19:25:26 - train: epoch 0062, iter [00200, 05004], lr: 0.072133, loss: 1.4947
2022-07-12 19:26:23 - train: epoch 0062, iter [00300, 05004], lr: 0.072069, loss: 1.3738
2022-07-12 19:27:21 - train: epoch 0062, iter [00400, 05004], lr: 0.072006, loss: 1.4008
2022-07-12 19:28:18 - train: epoch 0062, iter [00500, 05004], lr: 0.071942, loss: 1.4932
2022-07-12 19:29:16 - train: epoch 0062, iter [00600, 05004], lr: 0.071879, loss: 1.3943
2022-07-12 19:30:13 - train: epoch 0062, iter [00700, 05004], lr: 0.071816, loss: 1.5422
2022-07-12 19:31:09 - train: epoch 0062, iter [00800, 05004], lr: 0.071752, loss: 1.7499
2022-07-12 19:32:05 - train: epoch 0062, iter [00900, 05004], lr: 0.071689, loss: 1.6110
2022-07-12 19:33:01 - train: epoch 0062, iter [01000, 05004], lr: 0.071625, loss: 1.6508
2022-07-12 19:34:00 - train: epoch 0062, iter [01100, 05004], lr: 0.071562, loss: 1.5767
2022-07-12 19:34:57 - train: epoch 0062, iter [01200, 05004], lr: 0.071499, loss: 1.6881
2022-07-12 19:35:53 - train: epoch 0062, iter [01300, 05004], lr: 0.071435, loss: 1.5428
2022-07-12 19:36:49 - train: epoch 0062, iter [01400, 05004], lr: 0.071372, loss: 1.2835
2022-07-12 19:37:45 - train: epoch 0062, iter [01500, 05004], lr: 0.071309, loss: 1.5825
2022-07-12 19:38:44 - train: epoch 0062, iter [01600, 05004], lr: 0.071245, loss: 1.6748
2022-07-12 19:39:39 - train: epoch 0062, iter [01700, 05004], lr: 0.071182, loss: 1.6805
2022-07-12 19:40:36 - train: epoch 0062, iter [01800, 05004], lr: 0.071119, loss: 1.5417
2022-07-12 19:41:32 - train: epoch 0062, iter [01900, 05004], lr: 0.071056, loss: 1.3157
2022-07-12 19:42:31 - train: epoch 0062, iter [02000, 05004], lr: 0.070992, loss: 1.4823
2022-07-12 19:43:26 - train: epoch 0062, iter [02100, 05004], lr: 0.070929, loss: 1.6812
2022-07-12 19:44:22 - train: epoch 0062, iter [02200, 05004], lr: 0.070866, loss: 1.3636
2022-07-12 19:45:19 - train: epoch 0062, iter [02300, 05004], lr: 0.070803, loss: 1.4684
2022-07-12 19:46:15 - train: epoch 0062, iter [02400, 05004], lr: 0.070739, loss: 1.5330
2022-07-12 19:47:13 - train: epoch 0062, iter [02500, 05004], lr: 0.070676, loss: 1.5644
2022-07-12 19:48:07 - train: epoch 0062, iter [02600, 05004], lr: 0.070613, loss: 1.2922
2022-07-12 19:49:04 - train: epoch 0062, iter [02700, 05004], lr: 0.070550, loss: 1.3218
2022-07-12 19:50:01 - train: epoch 0062, iter [02800, 05004], lr: 0.070487, loss: 1.6299
2022-07-12 19:50:57 - train: epoch 0062, iter [02900, 05004], lr: 0.070424, loss: 1.6043
2022-07-12 19:51:54 - train: epoch 0062, iter [03000, 05004], lr: 0.070361, loss: 1.6201
2022-07-12 19:52:47 - train: epoch 0062, iter [03100, 05004], lr: 0.070297, loss: 1.5604
2022-07-12 19:53:42 - train: epoch 0062, iter [03200, 05004], lr: 0.070234, loss: 1.6604
2022-07-12 19:54:36 - train: epoch 0062, iter [03300, 05004], lr: 0.070171, loss: 1.5088
2022-07-12 19:55:31 - train: epoch 0062, iter [03400, 05004], lr: 0.070108, loss: 1.6205
2022-07-12 19:56:26 - train: epoch 0062, iter [03500, 05004], lr: 0.070045, loss: 1.7284
2022-07-12 19:57:20 - train: epoch 0062, iter [03600, 05004], lr: 0.069982, loss: 1.6222
2022-07-12 19:58:17 - train: epoch 0062, iter [03700, 05004], lr: 0.069919, loss: 1.6045
2022-07-12 19:59:13 - train: epoch 0062, iter [03800, 05004], lr: 0.069856, loss: 1.5447
2022-07-12 20:00:10 - train: epoch 0062, iter [03900, 05004], lr: 0.069793, loss: 1.5702
2022-07-12 20:01:04 - train: epoch 0062, iter [04000, 05004], lr: 0.069730, loss: 1.4588
2022-07-12 20:01:59 - train: epoch 0062, iter [04100, 05004], lr: 0.069667, loss: 1.6050
2022-07-12 20:02:54 - train: epoch 0062, iter [04200, 05004], lr: 0.069604, loss: 1.4307
2022-07-12 20:03:49 - train: epoch 0062, iter [04300, 05004], lr: 0.069541, loss: 1.3250
2022-07-12 20:04:47 - train: epoch 0062, iter [04400, 05004], lr: 0.069478, loss: 1.4877
2022-07-12 20:05:43 - train: epoch 0062, iter [04500, 05004], lr: 0.069415, loss: 1.4307
2022-07-12 20:06:42 - train: epoch 0062, iter [04600, 05004], lr: 0.069352, loss: 1.4084
2022-07-12 20:07:40 - train: epoch 0062, iter [04700, 05004], lr: 0.069289, loss: 1.4057
2022-07-12 20:08:40 - train: epoch 0062, iter [04800, 05004], lr: 0.069227, loss: 1.4163
2022-07-12 20:09:38 - train: epoch 0062, iter [04900, 05004], lr: 0.069164, loss: 1.5890
2022-07-12 20:10:36 - train: epoch 0062, iter [05000, 05004], lr: 0.069101, loss: 1.4549
2022-07-12 20:10:38 - train: epoch 062, train_loss: 1.5051
2022-07-12 20:12:45 - eval: epoch: 062, acc1: 68.308%, acc5: 88.768%, test_loss: 1.2957, per_image_load_time: 2.075ms, per_image_inference_time: 0.773ms
2022-07-12 20:12:45 - until epoch: 062, best_acc1: 68.308%
2022-07-12 20:12:45 - epoch 063 lr: 0.069098
2022-07-12 20:13:52 - train: epoch 0063, iter [00100, 05004], lr: 0.069035, loss: 1.6241
2022-07-12 20:14:52 - train: epoch 0063, iter [00200, 05004], lr: 0.068973, loss: 1.3554
2022-07-12 20:15:50 - train: epoch 0063, iter [00300, 05004], lr: 0.068910, loss: 1.5360
2022-07-12 20:16:51 - train: epoch 0063, iter [00400, 05004], lr: 0.068847, loss: 1.3579
2022-07-12 20:17:48 - train: epoch 0063, iter [00500, 05004], lr: 0.068784, loss: 1.2504
2022-07-12 20:18:44 - train: epoch 0063, iter [00600, 05004], lr: 0.068721, loss: 1.6517
2022-07-12 20:19:39 - train: epoch 0063, iter [00700, 05004], lr: 0.068659, loss: 1.5466
2022-07-12 20:20:32 - train: epoch 0063, iter [00800, 05004], lr: 0.068596, loss: 1.4024
2022-07-12 20:21:27 - train: epoch 0063, iter [00900, 05004], lr: 0.068533, loss: 1.6250
2022-07-12 20:22:21 - train: epoch 0063, iter [01000, 05004], lr: 0.068470, loss: 1.5167
2022-07-12 20:23:16 - train: epoch 0063, iter [01100, 05004], lr: 0.068408, loss: 1.4890
2022-07-12 20:24:15 - train: epoch 0063, iter [01200, 05004], lr: 0.068345, loss: 1.5651
2022-07-12 20:25:13 - train: epoch 0063, iter [01300, 05004], lr: 0.068282, loss: 1.2878
2022-07-12 20:26:14 - train: epoch 0063, iter [01400, 05004], lr: 0.068220, loss: 1.8375
2022-07-12 20:27:12 - train: epoch 0063, iter [01500, 05004], lr: 0.068157, loss: 1.5226
2022-07-12 20:28:12 - train: epoch 0063, iter [01600, 05004], lr: 0.068094, loss: 1.5225
2022-07-12 20:29:11 - train: epoch 0063, iter [01700, 05004], lr: 0.068032, loss: 1.5168
2022-07-12 20:30:11 - train: epoch 0063, iter [01800, 05004], lr: 0.067969, loss: 1.4452
2022-07-12 20:31:08 - train: epoch 0063, iter [01900, 05004], lr: 0.067907, loss: 1.7061
2022-07-12 20:32:08 - train: epoch 0063, iter [02000, 05004], lr: 0.067844, loss: 1.3237
2022-07-12 20:33:07 - train: epoch 0063, iter [02100, 05004], lr: 0.067781, loss: 1.3218
2022-07-12 20:34:04 - train: epoch 0063, iter [02200, 05004], lr: 0.067719, loss: 1.8429
2022-07-12 20:35:00 - train: epoch 0063, iter [02300, 05004], lr: 0.067656, loss: 1.5510
2022-07-12 20:35:59 - train: epoch 0063, iter [02400, 05004], lr: 0.067594, loss: 1.3877
2022-07-12 20:36:55 - train: epoch 0063, iter [02500, 05004], lr: 0.067531, loss: 1.5328
2022-07-12 20:37:49 - train: epoch 0063, iter [02600, 05004], lr: 0.067469, loss: 1.4357
2022-07-12 20:38:47 - train: epoch 0063, iter [02700, 05004], lr: 0.067406, loss: 1.8017
2022-07-12 20:39:47 - train: epoch 0063, iter [02800, 05004], lr: 0.067344, loss: 1.3188
2022-07-12 20:40:48 - train: epoch 0063, iter [02900, 05004], lr: 0.067281, loss: 1.6343
2022-07-12 20:41:45 - train: epoch 0063, iter [03000, 05004], lr: 0.067219, loss: 1.6125
2022-07-12 20:42:44 - train: epoch 0063, iter [03100, 05004], lr: 0.067157, loss: 1.7296
2022-07-12 20:43:42 - train: epoch 0063, iter [03200, 05004], lr: 0.067094, loss: 1.5215
2022-07-12 20:44:43 - train: epoch 0063, iter [03300, 05004], lr: 0.067032, loss: 1.3794
2022-07-12 20:45:40 - train: epoch 0063, iter [03400, 05004], lr: 0.066969, loss: 1.3949
