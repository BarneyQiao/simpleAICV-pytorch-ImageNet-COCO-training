2022-02-23 06:09:20 - network: yolov4cspdarknet53
2022-02-23 06:09:20 - num_classes: 1000
2022-02-23 06:09:20 - input_image_size: 256
2022-02-23 06:09:20 - scale: 1.1428571428571428
2022-02-23 06:09:20 - trained_model_path: 
2022-02-23 06:09:20 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-23 06:09:20 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f3dac173df0>
2022-02-23 06:09:20 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f3d8b388100>
2022-02-23 06:09:20 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f3d8b388130>
2022-02-23 06:09:20 - seed: 0
2022-02-23 06:09:20 - batch_size: 256
2022-02-23 06:09:20 - num_workers: 16
2022-02-23 06:09:20 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-23 06:09:20 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-23 06:09:20 - epochs: 100
2022-02-23 06:09:20 - print_interval: 100
2022-02-23 06:09:20 - distributed: True
2022-02-23 06:09:20 - sync_bn: False
2022-02-23 06:09:20 - apex: True
2022-02-23 06:09:20 - gpus_type: NVIDIA RTX A5000
2022-02-23 06:09:20 - gpus_num: 2
2022-02-23 06:09:20 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f3d89f3fb70>
2022-02-23 06:09:25 - --------------------parameters--------------------
2022-02-23 06:09:25 - name: conv1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: conv1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: conv1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block1.front_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block1.front_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block1.front_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block1.left_conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block1.left_conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block1.left_conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block1.left_conv.1.0.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block1.left_conv.1.0.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block1.left_conv.1.0.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block1.left_conv.1.0.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block1.left_conv.1.0.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block1.left_conv.1.0.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block1.left_conv.2.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block1.left_conv.2.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block1.left_conv.2.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block1.right_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block1.right_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block1.right_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block1.out_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block1.out_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block1.out_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block2.front_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block2.front_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block2.front_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.1.0.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.1.0.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.1.0.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.1.0.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.1.0.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.1.0.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.1.1.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.1.1.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.1.1.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.1.1.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.1.1.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.1.1.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.2.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.2.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block2.left_conv.2.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block2.right_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block2.right_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block2.right_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block2.out_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block2.out_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block2.out_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.front_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.front_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.front_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.0.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.0.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.0.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.0.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.0.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.0.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.1.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.1.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.1.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.1.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.1.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.1.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.2.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.2.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.2.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.2.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.2.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.2.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.3.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.3.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.3.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.3.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.3.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.3.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.4.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.4.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.4.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.4.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.4.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.4.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.5.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.5.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.5.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.5.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.5.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.5.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.6.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.6.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.6.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.6.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.6.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.6.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.7.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.7.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.7.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.7.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.7.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.1.7.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.2.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.2.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.left_conv.2.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.right_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.right_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.right_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block3.out_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block3.out_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block3.out_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.front_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.front_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.front_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.0.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.0.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.0.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.0.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.0.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.0.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.1.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.1.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.1.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.1.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.1.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.1.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.2.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.2.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.2.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.2.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.2.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.2.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.3.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.3.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.3.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.3.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.3.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.3.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.4.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.4.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.4.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.4.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.4.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.4.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.5.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.5.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.5.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.5.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.5.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.5.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.6.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.6.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.6.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.6.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.6.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.6.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.7.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.7.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.7.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.7.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.7.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.1.7.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.2.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.2.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.left_conv.2.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.right_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.right_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.right_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block4.out_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block4.out_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block4.out_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.front_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.front_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.front_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.0.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.0.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.0.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.0.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.0.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.0.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.1.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.1.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.1.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.1.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.1.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.1.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.2.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.2.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.2.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.2.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.2.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.2.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.3.conv.0.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.3.conv.0.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.3.conv.0.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.3.conv.1.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.3.conv.1.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.1.3.conv.1.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.2.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.2.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.left_conv.2.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.right_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.right_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.right_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: block5.out_conv.layer.0.weight, grad: True
2022-02-23 06:09:25 - name: block5.out_conv.layer.1.weight, grad: True
2022-02-23 06:09:25 - name: block5.out_conv.layer.1.bias, grad: True
2022-02-23 06:09:25 - name: fc.weight, grad: True
2022-02-23 06:09:25 - name: fc.bias, grad: True
2022-02-23 06:09:25 - --------------------buffers--------------------
2022-02-23 06:09:25 - name: conv1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: conv1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block1.front_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block1.front_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block1.front_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block1.left_conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block1.left_conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block1.left_conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block1.left_conv.1.0.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block1.left_conv.1.0.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block1.left_conv.1.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block1.left_conv.1.0.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block1.left_conv.1.0.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block1.left_conv.1.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block1.left_conv.2.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block1.left_conv.2.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block1.left_conv.2.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block1.right_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block1.right_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block1.right_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block1.out_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block1.out_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block1.out_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block2.front_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block2.front_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block2.front_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.1.0.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.1.0.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.1.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.1.0.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.1.0.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.1.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.1.1.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.1.1.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.1.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.1.1.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.1.1.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.1.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.2.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.2.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block2.left_conv.2.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block2.right_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block2.right_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block2.right_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block2.out_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block2.out_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block2.out_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.front_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.front_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.front_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.0.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.0.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.0.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.0.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.1.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.1.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.1.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.1.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.2.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.2.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.2.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.2.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.3.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.3.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.3.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.3.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.4.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.4.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.4.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.4.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.5.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.5.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.5.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.5.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.6.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.6.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.6.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.6.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.7.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.7.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.7.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.7.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.1.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.2.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.2.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.left_conv.2.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.right_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.right_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.right_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block3.out_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block3.out_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block3.out_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.front_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.front_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.front_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.0.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.0.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.0.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.0.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.1.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.1.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.1.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.1.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.2.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.2.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.2.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.2.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.3.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.3.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.3.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.3.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.4.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.4.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.4.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.4.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.5.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.5.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.5.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.5.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.6.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.6.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.6.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.6.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.7.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.7.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.7.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.7.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.1.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.2.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.2.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.left_conv.2.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.right_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.right_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.right_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block4.out_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block4.out_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block4.out_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.front_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.front_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.front_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.0.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.0.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.0.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.0.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.1.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.1.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.1.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.1.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.2.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.2.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.2.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.2.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.3.conv.0.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.3.conv.0.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.3.conv.1.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.3.conv.1.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.1.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.2.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.2.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.left_conv.2.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.right_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.right_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.right_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - name: block5.out_conv.layer.1.running_mean, grad: False
2022-02-23 06:09:25 - name: block5.out_conv.layer.1.running_var, grad: False
2022-02-23 06:09:25 - name: block5.out_conv.layer.1.num_batches_tracked, grad: False
2022-02-23 06:09:25 - epoch 001 lr: 0.1
2022-02-23 06:10:05 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.7903
2022-02-23 06:10:39 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.5997
2022-02-23 06:11:12 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.4271
2022-02-23 06:11:45 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.3327
2022-02-23 06:12:19 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.2218
2022-02-23 06:12:52 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 5.8537
2022-02-23 06:13:25 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 5.8891
2022-02-23 06:13:59 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 5.7511
2022-02-23 06:14:32 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.6523
2022-02-23 06:15:06 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.5118
2022-02-23 06:15:40 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.5449
2022-02-23 06:16:14 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.2928
2022-02-23 06:16:48 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.1530
2022-02-23 06:17:22 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.2152
2022-02-23 06:17:55 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.0287
2022-02-23 06:18:30 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.1291
2022-02-23 06:19:04 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 4.8071
2022-02-23 06:19:38 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 4.9341
2022-02-23 06:20:12 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 4.6938
2022-02-23 06:20:46 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 4.7027
2022-02-23 06:21:20 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 4.5694
2022-02-23 06:21:54 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 4.5536
2022-02-23 06:22:28 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.3572
2022-02-23 06:23:02 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.4587
2022-02-23 06:23:36 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.5780
2022-02-23 06:24:11 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 4.4406
2022-02-23 06:24:45 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 4.4988
2022-02-23 06:25:19 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.2716
2022-02-23 06:25:55 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.1240
2022-02-23 06:26:29 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.3012
2022-02-23 06:27:03 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.3784
2022-02-23 06:27:38 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.2418
2022-02-23 06:28:13 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.0326
2022-02-23 06:28:48 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.0188
2022-02-23 06:29:23 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.0651
2022-02-23 06:29:57 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.0915
2022-02-23 06:30:32 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.1322
2022-02-23 06:31:08 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 3.8705
2022-02-23 06:31:43 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.0834
2022-02-23 06:32:20 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 3.8480
2022-02-23 06:32:57 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.0217
2022-02-23 06:33:34 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 3.8346
2022-02-23 06:34:10 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 3.7936
2022-02-23 06:34:49 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 3.6804
2022-02-23 06:35:29 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 3.8646
2022-02-23 06:36:11 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 3.9710
2022-02-23 06:36:53 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 3.7548
2022-02-23 06:37:31 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.0057
2022-02-23 06:38:18 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 3.6387
2022-02-23 06:39:05 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 3.7241
2022-02-23 06:39:07 - train: epoch 001, train_loss: 4.6935
2022-02-23 06:40:45 - eval: epoch: 001, acc1: 24.490%, acc5: 49.530%, test_loss: 3.6278, per_image_load_time: 1.963ms, per_image_inference_time: 0.533ms
2022-02-23 06:40:45 - until epoch: 001, best_acc1: 24.490%
2022-02-23 06:40:45 - epoch 002 lr: 0.1
2022-02-23 06:41:25 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 3.7459
2022-02-23 06:41:59 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 3.5427
2022-02-23 06:42:32 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 3.7786
2022-02-23 06:43:06 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 3.7874
2022-02-23 06:43:39 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.4698
2022-02-23 06:44:13 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 3.4571
2022-02-23 06:44:47 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 3.7384
2022-02-23 06:45:20 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.4032
2022-02-23 06:45:54 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.3059
2022-02-23 06:46:27 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 3.6697
2022-02-23 06:47:01 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 3.6742
2022-02-23 06:47:35 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 3.4643
2022-02-23 06:48:09 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.4692
2022-02-23 06:48:42 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 3.5627
2022-02-23 06:49:16 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 3.5864
2022-02-23 06:49:50 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.3717
2022-02-23 06:50:24 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.4960
2022-02-23 06:50:58 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.4964
2022-02-23 06:51:32 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.2763
2022-02-23 06:52:06 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.0892
2022-02-23 06:52:40 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.3838
2022-02-23 06:53:15 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.1466
2022-02-23 06:53:49 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.4328
2022-02-23 06:54:23 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.1677
2022-02-23 06:54:58 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.2149
2022-02-23 06:55:32 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.2443
2022-02-23 06:56:06 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.4134
2022-02-23 06:56:41 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.2631
2022-02-23 06:57:15 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.2831
2022-02-23 06:57:49 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.1118
2022-02-23 06:58:24 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.1726
2022-02-23 06:58:58 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.1926
2022-02-23 06:59:33 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.0719
2022-02-23 07:00:08 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.2762
2022-02-23 07:00:43 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.1397
2022-02-23 07:01:17 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.1293
2022-02-23 07:01:53 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.1839
2022-02-23 07:02:28 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 2.9993
2022-02-23 07:03:03 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.2010
2022-02-23 07:03:39 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.0227
2022-02-23 07:04:15 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.2195
2022-02-23 07:04:51 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.0417
2022-02-23 07:05:30 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.0598
2022-02-23 07:06:10 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 2.9374
2022-02-23 07:06:52 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 2.9787
2022-02-23 07:07:28 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.0754
2022-02-23 07:08:10 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.1368
2022-02-23 07:08:55 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.0792
2022-02-23 07:09:41 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 2.9246
2022-02-23 07:10:29 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 2.9538
2022-02-23 07:10:31 - train: epoch 002, train_loss: 3.3028
2022-02-23 07:12:14 - eval: epoch: 002, acc1: 35.576%, acc5: 63.068%, test_loss: 2.9048, per_image_load_time: 3.396ms, per_image_inference_time: 0.535ms
2022-02-23 07:12:15 - until epoch: 002, best_acc1: 35.576%
2022-02-23 07:12:15 - epoch 003 lr: 0.1
2022-02-23 07:12:54 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.0852
2022-02-23 07:13:27 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.0202
2022-02-23 07:14:01 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 2.9422
2022-02-23 07:14:34 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.0669
2022-02-23 07:15:07 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.2698
2022-02-23 07:15:41 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 2.9040
2022-02-23 07:16:14 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.1770
2022-02-23 07:16:47 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.0353
2022-02-23 07:17:21 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 2.8412
2022-02-23 07:17:54 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 2.9381
2022-02-23 07:18:28 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 2.9356
2022-02-23 07:19:02 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 2.8198
2022-02-23 07:19:36 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 2.8151
2022-02-23 07:20:09 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 2.8718
2022-02-23 07:20:43 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.1650
2022-02-23 07:21:17 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 2.8164
2022-02-23 07:21:51 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 2.8771
2022-02-23 07:22:25 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 2.7670
2022-02-23 07:22:59 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 2.9959
2022-02-23 07:23:33 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.2203
2022-02-23 07:24:07 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.1646
2022-02-23 07:24:42 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.3281
2022-02-23 07:25:16 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 2.9423
2022-02-23 07:25:50 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 2.7825
2022-02-23 07:26:24 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 2.8576
2022-02-23 07:26:58 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 2.9826
2022-02-23 07:27:32 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.1495
2022-02-23 07:28:06 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 2.7969
2022-02-23 07:28:40 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 2.9130
2022-02-23 07:29:14 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.1440
2022-02-23 07:29:49 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.0121
2022-02-23 07:30:23 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 2.9717
2022-02-23 07:30:58 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 2.9269
2022-02-23 07:31:33 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.0715
2022-02-23 07:32:07 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.6510
2022-02-23 07:32:43 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 2.7687
2022-02-23 07:33:18 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 2.7230
2022-02-23 07:33:53 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.0140
2022-02-23 07:34:28 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 2.9992
2022-02-23 07:35:04 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 2.8577
2022-02-23 07:35:40 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 2.8291
2022-02-23 07:36:17 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.8205
2022-02-23 07:36:55 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.4258
2022-02-23 07:37:35 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.7717
2022-02-23 07:38:14 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 2.8198
2022-02-23 07:38:51 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 2.9086
2022-02-23 07:39:33 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.5854
2022-02-23 07:40:19 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 2.9108
2022-02-23 07:41:05 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 2.8605
2022-02-23 07:41:52 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 2.8238
2022-02-23 07:41:54 - train: epoch 003, train_loss: 2.8891
2022-02-23 07:43:25 - eval: epoch: 003, acc1: 42.136%, acc5: 69.270%, test_loss: 2.5517, per_image_load_time: 1.529ms, per_image_inference_time: 0.565ms
2022-02-23 07:43:26 - until epoch: 003, best_acc1: 42.136%
2022-02-23 07:43:26 - epoch 004 lr: 0.1
2022-02-23 07:44:05 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 2.8479
2022-02-23 07:44:39 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.5801
2022-02-23 07:45:13 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.8383
2022-02-23 07:45:48 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.7533
2022-02-23 07:46:22 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.6469
2022-02-23 07:46:56 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 2.8528
2022-02-23 07:47:31 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 2.8593
2022-02-23 07:48:05 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.6156
2022-02-23 07:48:40 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.3879
2022-02-23 07:49:14 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.6600
2022-02-23 07:49:49 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 2.8638
2022-02-23 07:50:23 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.5525
2022-02-23 07:50:57 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.5056
2022-02-23 07:51:31 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 2.8762
2022-02-23 07:52:06 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 2.7904
2022-02-23 07:52:40 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.6058
2022-02-23 07:53:14 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.7621
2022-02-23 07:53:50 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 2.9234
2022-02-23 07:54:25 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 2.7406
2022-02-23 07:55:00 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.6912
2022-02-23 07:55:34 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.7205
2022-02-23 07:56:08 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.6573
2022-02-23 07:56:43 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.5643
2022-02-23 07:57:18 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.6082
2022-02-23 07:57:52 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.6210
2022-02-23 07:58:26 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.7884
2022-02-23 07:59:01 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.5646
2022-02-23 07:59:35 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.6928
2022-02-23 08:00:10 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.5712
2022-02-23 08:00:45 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.6014
2022-02-23 08:01:19 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.6926
2022-02-23 08:01:53 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.6068
2022-02-23 08:02:28 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.8203
2022-02-23 08:03:02 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.7600
2022-02-23 08:03:37 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.7324
2022-02-23 08:04:12 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.4996
2022-02-23 08:04:47 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.6377
2022-02-23 08:05:22 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.5198
2022-02-23 08:05:58 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.6629
2022-02-23 08:06:33 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.3679
2022-02-23 08:07:08 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.6263
2022-02-23 08:07:45 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.4564
2022-02-23 08:08:20 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.4341
2022-02-23 08:08:58 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.4438
2022-02-23 08:09:39 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.1515
2022-02-23 08:10:21 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.6109
2022-02-23 08:11:04 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.5871
2022-02-23 08:11:42 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.4955
2022-02-23 08:12:24 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.7095
2022-02-23 08:13:11 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.6877
2022-02-23 08:13:13 - train: epoch 004, train_loss: 2.6868
2022-02-23 08:14:48 - eval: epoch: 004, acc1: 45.478%, acc5: 72.162%, test_loss: 2.3785, per_image_load_time: 1.121ms, per_image_inference_time: 0.550ms
2022-02-23 08:14:49 - until epoch: 004, best_acc1: 45.478%
2022-02-23 08:14:49 - epoch 005 lr: 0.1
2022-02-23 08:15:29 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.7255
2022-02-23 08:16:03 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.7167
2022-02-23 08:16:37 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.6459
2022-02-23 08:17:11 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.5513
2022-02-23 08:17:45 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.4697
2022-02-23 08:18:19 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.5263
2022-02-23 08:18:52 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.6680
2022-02-23 08:19:26 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 2.8457
2022-02-23 08:20:00 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.5722
2022-02-23 08:20:34 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.6545
2022-02-23 08:21:08 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.6700
2022-02-23 08:21:42 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.6495
2022-02-23 08:22:16 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.5467
2022-02-23 08:22:50 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.6775
2022-02-23 08:23:24 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.4910
2022-02-23 08:23:58 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.3765
2022-02-23 08:24:32 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.4430
2022-02-23 08:25:06 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.6815
2022-02-23 08:25:40 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.5859
2022-02-23 08:26:14 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.5200
2022-02-23 08:26:48 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.2988
2022-02-23 08:27:22 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.4767
2022-02-23 08:27:56 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.4026
2022-02-23 08:28:31 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.4872
2022-02-23 08:29:05 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.6360
2022-02-23 08:29:40 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.6219
2022-02-23 08:30:14 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.8045
2022-02-23 08:30:49 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.5441
2022-02-23 08:31:23 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.4507
2022-02-23 08:31:57 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.5619
2022-02-23 08:32:32 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.6141
2022-02-23 08:33:06 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.6631
2022-02-23 08:33:41 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.3669
2022-02-23 08:34:15 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.4450
2022-02-23 08:34:50 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.5716
2022-02-23 08:35:25 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.6330
2022-02-23 08:36:00 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.5403
2022-02-23 08:36:35 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.4521
2022-02-23 08:37:11 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.7335
2022-02-23 08:37:46 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.5038
2022-02-23 08:38:22 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.4401
2022-02-23 08:38:58 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.6708
2022-02-23 08:39:35 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.4671
2022-02-23 08:40:13 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.6078
2022-02-23 08:40:54 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.5781
2022-02-23 08:41:35 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.4401
2022-02-23 08:42:15 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.3458
2022-02-23 08:42:59 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.4660
2022-02-23 08:43:44 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.6809
2022-02-23 08:44:29 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.3916
2022-02-23 08:44:31 - train: epoch 005, train_loss: 2.5584
2022-02-23 08:46:08 - eval: epoch: 005, acc1: 48.222%, acc5: 74.668%, test_loss: 2.2303, per_image_load_time: 3.242ms, per_image_inference_time: 0.522ms
2022-02-23 08:46:09 - until epoch: 005, best_acc1: 48.222%
2022-02-23 08:46:09 - epoch 006 lr: 0.1
2022-02-23 08:46:49 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.3724
2022-02-23 08:47:22 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.5249
2022-02-23 08:47:55 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.3181
2022-02-23 08:48:29 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.5673
2022-02-23 08:49:02 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.5280
2022-02-23 08:49:35 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.6172
2022-02-23 08:50:08 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.6248
2022-02-23 08:50:41 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.4786
2022-02-23 08:51:15 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.3078
2022-02-23 08:51:48 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.4032
2022-02-23 08:52:21 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.3630
2022-02-23 08:52:55 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.5656
2022-02-23 08:53:28 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.5707
2022-02-23 08:54:02 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.6687
2022-02-23 08:54:36 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.7356
2022-02-23 08:55:09 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.3733
2022-02-23 08:55:43 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.5987
2022-02-23 08:56:17 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.6746
2022-02-23 08:56:51 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.4661
2022-02-23 08:57:24 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.6141
2022-02-23 08:57:58 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.5627
2022-02-23 08:58:32 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.4771
2022-02-23 08:59:06 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.3337
2022-02-23 08:59:40 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.5017
2022-02-23 09:00:13 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.7152
2022-02-23 09:00:48 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.3949
2022-02-23 09:01:22 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.6643
2022-02-23 09:01:56 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.3839
2022-02-23 09:02:30 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.6248
2022-02-23 09:03:04 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.4530
2022-02-23 09:03:38 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.2592
2022-02-23 09:04:13 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.4365
2022-02-23 09:04:47 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.2954
2022-02-23 09:05:21 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.5806
2022-02-23 09:05:56 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.4923
2022-02-23 09:06:31 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.4968
2022-02-23 09:07:06 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.4054
2022-02-23 09:07:41 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.2615
2022-02-23 09:08:16 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.4310
2022-02-23 09:08:51 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.7654
2022-02-23 09:09:27 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.4218
2022-02-23 09:10:02 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.3033
2022-02-23 09:10:38 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.4583
2022-02-23 09:11:17 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.5047
2022-02-23 09:11:57 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.4489
2022-02-23 09:12:33 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.3897
2022-02-23 09:13:13 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.4759
2022-02-23 09:13:55 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.4047
2022-02-23 09:14:37 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.5375
2022-02-23 09:15:19 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.2377
2022-02-23 09:15:21 - train: epoch 006, train_loss: 2.4764
2022-02-23 09:16:48 - eval: epoch: 006, acc1: 48.888%, acc5: 75.150%, test_loss: 2.2204, per_image_load_time: 2.828ms, per_image_inference_time: 0.570ms
2022-02-23 09:16:49 - until epoch: 006, best_acc1: 48.888%
2022-02-23 09:16:49 - epoch 007 lr: 0.1
2022-02-23 09:17:29 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.3628
2022-02-23 09:18:02 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.5840
2022-02-23 09:18:35 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.7170
2022-02-23 09:19:09 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.4657
2022-02-23 09:19:42 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.3184
2022-02-23 09:20:15 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.6776
2022-02-23 09:20:48 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.4382
2022-02-23 09:21:22 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.4952
2022-02-23 09:21:55 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.4179
2022-02-23 09:22:28 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.4985
2022-02-23 09:23:02 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.4431
2022-02-23 09:23:35 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.4789
2022-02-23 09:24:08 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.3964
2022-02-23 09:24:42 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.4224
2022-02-23 09:25:15 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.5883
2022-02-23 09:25:49 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.4143
2022-02-23 09:26:22 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.5411
2022-02-23 09:26:56 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.2761
2022-02-23 09:27:29 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.4049
2022-02-23 09:28:03 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.2728
2022-02-23 09:28:36 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.5543
2022-02-23 09:29:10 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.2566
2022-02-23 09:29:44 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.3802
2022-02-23 09:30:18 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.3795
2022-02-23 09:30:51 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.4933
2022-02-23 09:31:26 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.4043
2022-02-23 09:31:59 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.2281
2022-02-23 09:32:33 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.3490
2022-02-23 09:33:07 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.2552
2022-02-23 09:33:41 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.4727
2022-02-23 09:34:15 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.2673
2022-02-23 09:34:50 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.2825
2022-02-23 09:35:24 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.6693
2022-02-23 09:35:58 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.2418
2022-02-23 09:36:33 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.4664
2022-02-23 09:37:07 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.1659
2022-02-23 09:37:42 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.3825
2022-02-23 09:38:17 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.5930
2022-02-23 09:38:51 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.2416
2022-02-23 09:39:27 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.4923
2022-02-23 09:40:02 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.3481
2022-02-23 09:40:36 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.2289
2022-02-23 09:41:13 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.4625
2022-02-23 09:41:48 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.2741
2022-02-23 09:42:25 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.5821
2022-02-23 09:43:05 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.4637
2022-02-23 09:43:45 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.5129
2022-02-23 09:44:23 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.5891
2022-02-23 09:45:03 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.4091
2022-02-23 09:45:47 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.3417
2022-02-23 09:45:48 - train: epoch 007, train_loss: 2.4124
2022-02-23 09:47:21 - eval: epoch: 007, acc1: 48.842%, acc5: 75.518%, test_loss: 2.1877, per_image_load_time: 2.259ms, per_image_inference_time: 0.536ms
2022-02-23 09:47:22 - until epoch: 007, best_acc1: 48.888%
2022-02-23 09:47:22 - epoch 008 lr: 0.1
2022-02-23 09:48:02 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.3113
2022-02-23 09:48:34 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.5118
2022-02-23 09:49:07 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.1308
2022-02-23 09:49:41 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.2116
2022-02-23 09:50:14 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.2748
2022-02-23 09:50:47 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.4230
2022-02-23 09:51:20 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.5465
2022-02-23 09:51:53 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.2210
2022-02-23 09:52:26 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.4192
2022-02-23 09:52:59 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.3672
2022-02-23 09:53:33 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.1923
2022-02-23 09:54:06 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.3228
2022-02-23 09:54:39 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.4448
2022-02-23 09:55:12 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.3692
2022-02-23 09:55:45 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.3448
2022-02-23 09:56:18 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.4788
2022-02-23 09:56:52 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.1892
2022-02-23 09:57:25 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.5412
2022-02-23 09:57:58 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.1662
2022-02-23 09:58:31 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.3276
2022-02-23 09:59:05 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.4445
2022-02-23 09:59:38 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.3197
2022-02-23 10:00:11 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.4328
2022-02-23 10:00:45 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.2531
2022-02-23 10:01:18 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.2948
2022-02-23 10:01:52 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.5805
2022-02-23 10:02:26 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.4515
2022-02-23 10:02:59 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.4521
2022-02-23 10:03:34 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.4613
2022-02-23 10:04:08 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.4930
2022-02-23 10:04:41 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.2375
2022-02-23 10:05:15 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.5719
2022-02-23 10:05:49 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.5856
2022-02-23 10:06:23 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.6084
2022-02-23 10:06:57 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.2433
2022-02-23 10:07:32 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.4858
2022-02-23 10:08:06 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.3217
2022-02-23 10:08:41 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.2307
2022-02-23 10:09:15 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.4604
2022-02-23 10:09:49 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.6974
2022-02-23 10:10:25 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.3724
2022-02-23 10:11:00 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.2246
2022-02-23 10:11:34 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.1682
2022-02-23 10:12:12 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.3405
2022-02-23 10:12:46 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.4603
2022-02-23 10:13:24 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.5377
2022-02-23 10:14:04 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.2189
2022-02-23 10:14:46 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.4802
2022-02-23 10:15:27 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.3831
2022-02-23 10:16:09 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.2961
2022-02-23 10:16:11 - train: epoch 008, train_loss: 2.3652
2022-02-23 10:17:40 - eval: epoch: 008, acc1: 50.718%, acc5: 76.950%, test_loss: 2.1051, per_image_load_time: 2.887ms, per_image_inference_time: 0.550ms
2022-02-23 10:17:41 - until epoch: 008, best_acc1: 50.718%
2022-02-23 10:17:41 - epoch 009 lr: 0.1
2022-02-23 10:18:22 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.0562
2022-02-23 10:18:55 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.2329
2022-02-23 10:19:28 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.1292
2022-02-23 10:20:01 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.4481
2022-02-23 10:20:34 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.2458
2022-02-23 10:21:07 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.2940
2022-02-23 10:21:40 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.3608
2022-02-23 10:22:13 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.2144
2022-02-23 10:22:47 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.2018
2022-02-23 10:23:20 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.2036
2022-02-23 10:23:53 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.6391
2022-02-23 10:24:26 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.5404
2022-02-23 10:25:00 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.5502
2022-02-23 10:25:33 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.0034
2022-02-23 10:26:06 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.2201
2022-02-23 10:26:40 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.4235
2022-02-23 10:27:13 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.4798
2022-02-23 10:27:46 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.3190
2022-02-23 10:28:20 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.0938
2022-02-23 10:28:53 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 1.9781
2022-02-23 10:29:26 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.3677
2022-02-23 10:30:00 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.4542
2022-02-23 10:30:33 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.1623
2022-02-23 10:31:06 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.2265
2022-02-23 10:31:40 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.1880
2022-02-23 10:32:14 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.3745
2022-02-23 10:32:47 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.2278
2022-02-23 10:33:21 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.3254
2022-02-23 10:33:54 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.1029
2022-02-23 10:34:28 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.2394
2022-02-23 10:35:02 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.4478
2022-02-23 10:35:36 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.3708
2022-02-23 10:36:10 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.3092
2022-02-23 10:36:44 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.4947
2022-02-23 10:37:17 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.3623
2022-02-23 10:37:52 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.2292
2022-02-23 10:38:26 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.6062
2022-02-23 10:39:00 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.4815
2022-02-23 10:39:35 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.1240
2022-02-23 10:40:10 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.4992
2022-02-23 10:40:44 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.3051
2022-02-23 10:41:21 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.1575
2022-02-23 10:41:56 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.4251
2022-02-23 10:42:33 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.2763
2022-02-23 10:43:12 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.2542
2022-02-23 10:43:52 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.4365
2022-02-23 10:44:32 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.4752
2022-02-23 10:45:12 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.4710
2022-02-23 10:45:58 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.3370
2022-02-23 10:46:44 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.3415
2022-02-23 10:46:46 - train: epoch 009, train_loss: 2.3307
2022-02-23 10:48:19 - eval: epoch: 009, acc1: 51.572%, acc5: 77.166%, test_loss: 2.0723, per_image_load_time: 3.079ms, per_image_inference_time: 0.538ms
2022-02-23 10:48:20 - until epoch: 009, best_acc1: 51.572%
2022-02-23 10:48:20 - epoch 010 lr: 0.1
2022-02-23 10:49:00 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.1542
2022-02-23 10:49:33 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.4090
2022-02-23 10:50:06 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.3740
2022-02-23 10:50:40 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.3041
2022-02-23 10:51:13 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.3245
2022-02-23 10:51:46 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.3275
2022-02-23 10:52:19 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.3253
2022-02-23 10:52:52 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.1419
2022-02-23 10:53:25 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.1247
2022-02-23 10:53:59 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.0585
2022-02-23 10:54:32 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.2764
2022-02-23 10:55:05 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.1266
2022-02-23 10:55:39 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.1635
2022-02-23 10:56:12 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.3524
2022-02-23 10:56:45 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.0638
2022-02-23 10:57:18 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.3732
2022-02-23 10:57:52 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.3562
2022-02-23 10:58:25 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.3187
2022-02-23 10:58:58 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.2833
2022-02-23 10:59:32 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.3843
2022-02-23 11:00:05 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.2569
2022-02-23 11:00:39 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.4574
2022-02-23 11:01:12 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.5645
2022-02-23 11:01:46 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.4513
2022-02-23 11:02:20 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.3583
2022-02-23 11:02:54 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.3794
2022-02-23 11:03:28 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.1294
2022-02-23 11:04:01 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.3010
2022-02-23 11:04:35 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.4876
2022-02-23 11:05:09 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.2161
2022-02-23 11:05:42 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.4686
2022-02-23 11:06:16 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.3330
2022-02-23 11:06:51 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.4631
2022-02-23 11:07:24 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.4668
2022-02-23 11:07:59 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.5803
2022-02-23 11:08:33 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.5400
2022-02-23 11:09:07 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.1323
2022-02-23 11:09:42 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.3587
2022-02-23 11:10:17 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.0122
2022-02-23 11:10:51 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.2744
2022-02-23 11:11:26 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.1724
2022-02-23 11:12:02 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.3745
2022-02-23 11:12:38 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.3766
2022-02-23 11:13:16 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.3106
2022-02-23 11:13:55 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.1979
2022-02-23 11:14:32 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.4312
2022-02-23 11:15:10 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.2960
2022-02-23 11:15:53 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.2605
2022-02-23 11:16:38 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.1920
2022-02-23 11:17:27 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.0798
2022-02-23 11:17:29 - train: epoch 010, train_loss: 2.3007
2022-02-23 11:19:04 - eval: epoch: 010, acc1: 49.842%, acc5: 75.976%, test_loss: 2.1579, per_image_load_time: 2.067ms, per_image_inference_time: 0.530ms
2022-02-23 11:19:05 - until epoch: 010, best_acc1: 51.572%
2022-02-23 11:19:05 - epoch 011 lr: 0.1
2022-02-23 11:19:45 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.1250
2022-02-23 11:20:18 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.4399
2022-02-23 11:20:51 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.1183
2022-02-23 11:21:24 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.2946
2022-02-23 11:21:57 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.2089
2022-02-23 11:22:30 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.3443
2022-02-23 11:23:03 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.2500
2022-02-23 11:23:36 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.2918
2022-02-23 11:24:10 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.3931
2022-02-23 11:24:43 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.1744
2022-02-23 11:25:16 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.3286
2022-02-23 11:25:50 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.5989
2022-02-23 11:26:23 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.3870
2022-02-23 11:26:56 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.2506
2022-02-23 11:27:29 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.2620
2022-02-23 11:28:03 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.3763
2022-02-23 11:28:37 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.3154
2022-02-23 11:29:10 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.1322
2022-02-23 11:29:43 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.1376
2022-02-23 11:30:16 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.4887
2022-02-23 11:30:49 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.3699
2022-02-23 11:31:23 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.2185
2022-02-23 11:31:57 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.4768
2022-02-23 11:32:30 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.1155
2022-02-23 11:33:04 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.5127
2022-02-23 11:33:37 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.3276
2022-02-23 11:34:11 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.1902
2022-02-23 11:34:45 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 1.9417
2022-02-23 11:35:18 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.3980
2022-02-23 11:35:52 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.4503
2022-02-23 11:36:26 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.2720
2022-02-23 11:37:00 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.2108
2022-02-23 11:37:34 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.3794
2022-02-23 11:38:08 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.1801
2022-02-23 11:38:42 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.2674
2022-02-23 11:39:17 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.2511
2022-02-23 11:39:51 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.4427
2022-02-23 11:40:25 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.0082
2022-02-23 11:41:00 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.3460
2022-02-23 11:41:34 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.2487
2022-02-23 11:42:09 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 1.9776
2022-02-23 11:42:45 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.0790
2022-02-23 11:43:20 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.3277
2022-02-23 11:43:56 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.1233
2022-02-23 11:44:34 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.1294
2022-02-23 11:45:09 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.0681
2022-02-23 11:45:48 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 1.9616
2022-02-23 11:46:29 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.1000
2022-02-23 11:47:11 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 1.9911
2022-02-23 11:47:50 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.2267
2022-02-23 11:47:52 - train: epoch 011, train_loss: 2.2767
2022-02-23 11:49:15 - eval: epoch: 011, acc1: 52.940%, acc5: 78.786%, test_loss: 1.9802, per_image_load_time: 2.706ms, per_image_inference_time: 0.553ms
2022-02-23 11:49:16 - until epoch: 011, best_acc1: 52.940%
2022-02-23 11:49:16 - epoch 012 lr: 0.1
2022-02-23 11:49:55 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.1151
2022-02-23 11:50:28 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.1219
2022-02-23 11:51:01 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.2216
2022-02-23 11:51:34 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.3521
2022-02-23 11:52:07 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.3804
2022-02-23 11:52:39 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.0037
2022-02-23 11:53:13 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.0758
2022-02-23 11:53:46 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.2500
2022-02-23 11:54:19 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.3951
2022-02-23 11:54:52 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.1206
2022-02-23 11:55:25 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.6774
2022-02-23 11:55:59 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.0777
2022-02-23 11:56:32 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.1947
2022-02-23 11:57:05 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.4320
2022-02-23 11:57:39 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.1534
2022-02-23 11:58:12 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.2526
2022-02-23 11:58:46 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.1221
2022-02-23 11:59:19 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.2697
2022-02-23 11:59:53 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.2421
2022-02-23 12:00:26 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.3597
2022-02-23 12:01:00 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.2693
2022-02-23 12:01:33 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.3769
2022-02-23 12:02:07 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.2507
2022-02-23 12:02:40 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.2375
2022-02-23 12:03:14 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.1221
2022-02-23 12:03:48 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.1236
2022-02-23 12:04:21 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.1366
2022-02-23 12:04:55 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.3550
2022-02-23 12:05:29 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.0915
2022-02-23 12:06:02 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.1182
2022-02-23 12:06:37 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.3089
2022-02-23 12:07:11 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.0435
2022-02-23 12:07:45 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.2071
2022-02-23 12:08:19 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.2077
2022-02-23 12:08:53 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.4419
2022-02-23 12:09:27 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.2385
2022-02-23 12:10:02 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.1228
2022-02-23 12:10:35 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.2839
2022-02-23 12:11:10 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.1352
2022-02-23 12:11:45 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.1295
2022-02-23 12:12:19 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.1851
2022-02-23 12:12:54 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.1805
2022-02-23 12:13:31 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.3140
2022-02-23 12:14:05 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.1237
2022-02-23 12:14:43 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.2193
2022-02-23 12:15:25 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.5268
2022-02-23 12:16:04 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.1834
2022-02-23 12:16:41 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.4193
2022-02-23 12:17:22 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.2228
2022-02-23 12:18:05 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.0689
2022-02-23 12:18:07 - train: epoch 012, train_loss: 2.2560
2022-02-23 12:19:35 - eval: epoch: 012, acc1: 52.888%, acc5: 78.376%, test_loss: 1.9973, per_image_load_time: 1.868ms, per_image_inference_time: 0.554ms
2022-02-23 12:19:36 - until epoch: 012, best_acc1: 52.940%
2022-02-23 12:19:36 - epoch 013 lr: 0.1
2022-02-23 12:20:15 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 1.9808
2022-02-23 12:20:48 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.2448
2022-02-23 12:21:21 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.2029
2022-02-23 12:21:54 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.2413
2022-02-23 12:22:27 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.1543
2022-02-23 12:23:00 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.3465
2022-02-23 12:23:33 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.0549
2022-02-23 12:24:06 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.4159
2022-02-23 12:24:39 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.0894
2022-02-23 12:25:12 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.2748
2022-02-23 12:25:45 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.2377
2022-02-23 12:26:18 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.3901
2022-02-23 12:26:51 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.2486
2022-02-23 12:27:24 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.1482
2022-02-23 12:27:57 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.3354
2022-02-23 12:28:30 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 1.9587
2022-02-23 12:29:03 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.2783
2022-02-23 12:29:36 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.1813
2022-02-23 12:30:10 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.2615
2022-02-23 12:30:43 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.4275
2022-02-23 12:31:16 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.5959
2022-02-23 12:31:50 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.2249
2022-02-23 12:32:23 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.2951
2022-02-23 12:32:56 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.3247
2022-02-23 12:33:30 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.2509
2022-02-23 12:34:03 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.1425
2022-02-23 12:34:37 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.0897
2022-02-23 12:35:10 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.1565
2022-02-23 12:35:44 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.3426
2022-02-23 12:36:18 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.0756
2022-02-23 12:36:52 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.0744
2022-02-23 12:37:25 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.2555
2022-02-23 12:37:59 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.0179
2022-02-23 12:38:32 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.1688
2022-02-23 12:39:07 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.1185
2022-02-23 12:39:40 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.5756
2022-02-23 12:40:14 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 1.9784
2022-02-23 12:40:49 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.3566
2022-02-23 12:41:23 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.2574
2022-02-23 12:41:57 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.3329
2022-02-23 12:42:32 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.2059
2022-02-23 12:43:07 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.1550
2022-02-23 12:43:42 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.1452
2022-02-23 12:44:20 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.1613
2022-02-23 12:44:57 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.1844
2022-02-23 12:45:35 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.2835
2022-02-23 12:46:14 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.3297
2022-02-23 12:46:57 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.2081
2022-02-23 12:47:43 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.2126
2022-02-23 12:48:28 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.2696
2022-02-23 12:48:30 - train: epoch 013, train_loss: 2.2385
2022-02-23 12:50:04 - eval: epoch: 013, acc1: 52.306%, acc5: 78.162%, test_loss: 2.0289, per_image_load_time: 1.262ms, per_image_inference_time: 0.554ms
2022-02-23 12:50:05 - until epoch: 013, best_acc1: 52.940%
2022-02-23 12:50:05 - epoch 014 lr: 0.1
2022-02-23 12:50:45 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.2565
2022-02-23 12:51:17 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.3194
2022-02-23 12:51:51 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 1.9371
2022-02-23 12:52:24 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.2206
2022-02-23 12:52:57 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.0908
2022-02-23 12:53:30 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.2176
2022-02-23 12:54:03 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.1879
2022-02-23 12:54:36 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.2787
2022-02-23 12:55:09 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.2178
2022-02-23 12:55:42 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.3799
2022-02-23 12:56:16 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.1221
2022-02-23 12:56:49 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.2067
2022-02-23 12:57:22 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.3199
2022-02-23 12:57:56 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.3010
2022-02-23 12:58:29 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.3736
2022-02-23 12:59:03 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.1737
2022-02-23 12:59:37 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.3399
2022-02-23 13:00:10 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.3346
2022-02-23 13:00:44 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.0956
2022-02-23 13:01:17 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.1029
2022-02-23 13:01:50 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.3902
2022-02-23 13:02:24 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.1888
2022-02-23 13:02:57 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.3448
2022-02-23 13:03:31 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.2820
2022-02-23 13:04:05 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.1748
2022-02-23 13:04:39 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.2121
2022-02-23 13:05:13 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.2754
2022-02-23 13:05:47 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.4722
2022-02-23 13:06:20 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.1461
2022-02-23 13:06:55 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.2730
2022-02-23 13:07:29 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.0230
2022-02-23 13:08:02 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.2410
2022-02-23 13:08:37 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.1155
2022-02-23 13:09:12 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.2615
2022-02-23 13:09:45 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.0127
2022-02-23 13:10:20 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.0734
2022-02-23 13:10:54 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.1477
2022-02-23 13:11:28 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.3616
2022-02-23 13:12:03 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.0251
2022-02-23 13:12:38 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.3677
2022-02-23 13:13:12 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.1032
2022-02-23 13:13:48 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.0975
2022-02-23 13:14:22 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.1214
2022-02-23 13:14:59 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.1084
2022-02-23 13:15:38 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.2354
2022-02-23 13:16:20 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.1410
2022-02-23 13:16:58 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.2006
2022-02-23 13:17:36 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.1283
2022-02-23 13:18:21 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.0594
2022-02-23 13:19:08 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.1137
2022-02-23 13:19:10 - train: epoch 014, train_loss: 2.2242
2022-02-23 13:20:47 - eval: epoch: 014, acc1: 53.312%, acc5: 79.098%, test_loss: 1.9749, per_image_load_time: 3.261ms, per_image_inference_time: 0.530ms
2022-02-23 13:20:48 - until epoch: 014, best_acc1: 53.312%
2022-02-23 13:20:48 - epoch 015 lr: 0.1
2022-02-23 13:21:28 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 1.9845
2022-02-23 13:22:02 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.5224
2022-02-23 13:22:35 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.4952
2022-02-23 13:23:08 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.2255
2022-02-23 13:23:41 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.2162
2022-02-23 13:24:14 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.4006
2022-02-23 13:24:47 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.1488
2022-02-23 13:25:20 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.0320
2022-02-23 13:25:54 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.0711
2022-02-23 13:26:27 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.3431
2022-02-23 13:27:00 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.0182
2022-02-23 13:27:34 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.2276
2022-02-23 13:28:07 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.4477
2022-02-23 13:28:41 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.0073
2022-02-23 13:29:14 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 1.8628
2022-02-23 13:29:47 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.2761
2022-02-23 13:30:21 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.4819
2022-02-23 13:30:54 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.0834
2022-02-23 13:31:27 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.0035
2022-02-23 13:32:00 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.0701
2022-02-23 13:32:34 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.0417
2022-02-23 13:33:07 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.4367
2022-02-23 13:33:40 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.0772
2022-02-23 13:34:14 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.2723
2022-02-23 13:34:47 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.2573
2022-02-23 13:35:21 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.0640
2022-02-23 13:35:55 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.2343
2022-02-23 13:36:29 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.2901
2022-02-23 13:37:03 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.0903
2022-02-23 13:37:37 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 1.9679
2022-02-23 13:38:12 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.1643
2022-02-23 13:38:46 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.1367
2022-02-23 13:39:20 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 1.9459
2022-02-23 13:39:54 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.3290
2022-02-23 13:40:29 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.3705
2022-02-23 13:41:04 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.1685
2022-02-23 13:41:38 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.1159
2022-02-23 13:42:14 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.1764
2022-02-23 13:42:49 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.3647
2022-02-23 13:43:25 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.2807
2022-02-23 13:44:00 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.5127
2022-02-23 13:44:36 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 1.9275
2022-02-23 13:45:13 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.2280
2022-02-23 13:45:48 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.2960
2022-02-23 13:46:27 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.2512
2022-02-23 13:47:08 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.1146
2022-02-23 13:47:47 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.3340
2022-02-23 13:48:28 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.2647
2022-02-23 13:49:13 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.1704
2022-02-23 13:50:00 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.3601
2022-02-23 13:50:02 - train: epoch 015, train_loss: 2.2133
2022-02-23 13:51:26 - eval: epoch: 015, acc1: 51.836%, acc5: 77.706%, test_loss: 2.0623, per_image_load_time: 2.704ms, per_image_inference_time: 0.566ms
2022-02-23 13:51:27 - until epoch: 015, best_acc1: 53.312%
2022-02-23 13:51:27 - epoch 016 lr: 0.1
2022-02-23 13:52:07 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.1343
2022-02-23 13:52:39 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 1.9462
2022-02-23 13:53:13 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.2422
2022-02-23 13:53:46 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.4672
2022-02-23 13:54:19 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 1.9947
2022-02-23 13:54:52 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.2963
2022-02-23 13:55:25 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 1.9240
2022-02-23 13:55:58 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.1072
2022-02-23 13:56:31 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.2458
2022-02-23 13:57:04 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.1440
2022-02-23 13:57:37 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.1228
2022-02-23 13:58:10 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.1284
2022-02-23 13:58:43 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.2686
2022-02-23 13:59:16 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.1680
2022-02-23 13:59:50 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.3334
2022-02-23 14:00:23 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.4000
2022-02-23 14:00:56 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.0863
2022-02-23 14:01:29 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.2096
2022-02-23 14:02:03 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.2455
2022-02-23 14:02:37 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 1.8871
2022-02-23 14:03:11 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.2920
2022-02-23 14:03:45 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.2365
2022-02-23 14:04:18 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.3460
2022-02-23 14:04:52 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.2916
2022-02-23 14:05:26 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.1140
2022-02-23 14:06:00 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.3322
2022-02-23 14:06:34 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.1677
2022-02-23 14:07:08 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.0027
2022-02-23 14:07:42 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.2182
2022-02-23 14:08:16 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.4519
2022-02-23 14:08:50 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.3463
2022-02-23 14:09:25 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.2854
2022-02-23 14:09:59 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.2958
2022-02-23 14:10:33 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.0425
2022-02-23 14:11:08 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.2058
2022-02-23 14:11:42 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.0674
2022-02-23 14:12:17 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.2850
2022-02-23 14:12:52 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.5627
2022-02-23 14:13:26 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.2798
2022-02-23 14:14:02 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.1952
2022-02-23 14:14:36 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.1203
2022-02-23 14:15:13 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.1737
2022-02-23 14:15:48 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.0183
2022-02-23 14:16:28 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.0543
2022-02-23 14:17:05 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.2877
2022-02-23 14:17:48 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.0985
2022-02-23 14:18:37 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.4597
2022-02-23 14:19:18 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.1311
2022-02-23 14:20:10 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.2630
2022-02-23 14:20:57 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.2825
2022-02-23 14:20:59 - train: epoch 016, train_loss: 2.2018
2022-02-23 14:22:26 - eval: epoch: 016, acc1: 53.358%, acc5: 78.786%, test_loss: 1.9908, per_image_load_time: 0.560ms, per_image_inference_time: 0.553ms
2022-02-23 14:22:27 - until epoch: 016, best_acc1: 53.358%
2022-02-23 14:22:27 - epoch 017 lr: 0.1
2022-02-23 14:23:07 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.0993
2022-02-23 14:23:41 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.3378
2022-02-23 14:24:15 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.4063
2022-02-23 14:24:49 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 1.9631
2022-02-23 14:25:23 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.2169
2022-02-23 14:25:57 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.5254
2022-02-23 14:26:31 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.1965
2022-02-23 14:27:04 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.1391
2022-02-23 14:27:38 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.1580
2022-02-23 14:28:12 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.1579
2022-02-23 14:28:47 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.4468
2022-02-23 14:29:21 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.4575
2022-02-23 14:29:55 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.1821
2022-02-23 14:30:30 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.3003
2022-02-23 14:31:05 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 1.8268
2022-02-23 14:31:39 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 1.9610
2022-02-23 14:32:13 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.1618
2022-02-23 14:32:48 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.1576
2022-02-23 14:33:23 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.1375
2022-02-23 14:33:57 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.2688
2022-02-23 14:34:32 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.1841
2022-02-23 14:35:07 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.0571
2022-02-23 14:35:41 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.1095
2022-02-23 14:36:16 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.1348
2022-02-23 14:36:50 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.4605
2022-02-23 14:37:25 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.1303
2022-02-23 14:37:59 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.1515
2022-02-23 14:38:33 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.5115
2022-02-23 14:39:08 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.4489
2022-02-23 14:39:42 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.1115
2022-02-23 14:40:17 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.2422
2022-02-23 14:40:50 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.1126
2022-02-23 14:41:25 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.2967
2022-02-23 14:41:59 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.0370
2022-02-23 14:42:34 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.2021
2022-02-23 14:43:08 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.2912
2022-02-23 14:43:42 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.2226
2022-02-23 14:44:17 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.3197
2022-02-23 14:44:50 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.1546
2022-02-23 14:45:26 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.0912
2022-02-23 14:46:01 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.2307
2022-02-23 14:46:36 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.0784
2022-02-23 14:47:10 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.2618
2022-02-23 14:47:48 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.2487
2022-02-23 14:48:29 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.3659
2022-02-23 14:49:07 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.1631
2022-02-23 14:49:49 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.2190
2022-02-23 14:50:36 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.3645
2022-02-23 14:51:22 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.0451
2022-02-23 14:52:04 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 1.9637
2022-02-23 14:52:06 - train: epoch 017, train_loss: 2.1908
2022-02-23 14:53:45 - eval: epoch: 017, acc1: 53.668%, acc5: 79.220%, test_loss: 1.9642, per_image_load_time: 0.551ms, per_image_inference_time: 0.531ms
2022-02-23 14:53:46 - until epoch: 017, best_acc1: 53.668%
2022-02-23 14:53:46 - epoch 018 lr: 0.1
2022-02-23 14:54:25 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.2891
2022-02-23 14:54:59 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.2359
2022-02-23 14:55:32 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.2310
2022-02-23 14:56:05 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.3117
2022-02-23 14:56:39 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.1775
2022-02-23 14:57:12 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.2610
2022-02-23 14:57:45 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 1.9473
2022-02-23 14:58:19 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.1617
2022-02-23 14:58:52 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.1947
2022-02-23 14:59:26 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.0078
2022-02-23 14:59:59 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.4471
2022-02-23 15:00:33 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.1901
2022-02-23 15:01:07 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.5762
2022-02-23 15:01:40 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.2040
2022-02-23 15:02:13 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.2912
2022-02-23 15:02:47 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.2093
2022-02-23 15:03:21 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.0337
2022-02-23 15:03:55 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.0564
2022-02-23 15:04:28 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.0214
2022-02-23 15:05:02 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.4781
2022-02-23 15:05:38 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.3993
2022-02-23 15:06:11 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.0876
2022-02-23 15:06:46 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.1747
2022-02-23 15:07:19 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 1.9986
2022-02-23 15:07:53 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 1.8862
2022-02-23 15:08:27 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.0335
2022-02-23 15:09:00 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.3990
2022-02-23 15:09:35 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 1.9071
2022-02-23 15:10:08 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.2147
2022-02-23 15:10:42 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.0531
2022-02-23 15:11:16 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.6067
2022-02-23 15:11:50 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.1014
2022-02-23 15:12:25 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.1091
2022-02-23 15:13:03 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.0999
2022-02-23 15:13:41 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.3649
2022-02-23 15:14:18 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.1755
2022-02-23 15:14:59 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.5458
2022-02-23 15:15:33 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.2821
2022-02-23 15:16:08 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.2590
2022-02-23 15:16:53 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.1685
2022-02-23 15:17:27 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.1237
2022-02-23 15:18:02 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.1789
2022-02-23 15:18:37 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 1.9670
2022-02-23 15:19:14 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.2929
2022-02-23 15:19:56 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.1233
2022-02-23 15:20:35 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.0275
2022-02-23 15:21:18 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.4656
2022-02-23 15:22:07 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.2418
2022-02-23 15:22:57 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.1306
2022-02-23 15:23:46 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.3288
2022-02-23 15:23:48 - train: epoch 018, train_loss: 2.1810
2022-02-23 15:25:19 - eval: epoch: 018, acc1: 53.486%, acc5: 79.040%, test_loss: 1.9722, per_image_load_time: 1.498ms, per_image_inference_time: 0.559ms
2022-02-23 15:25:20 - until epoch: 018, best_acc1: 53.668%
2022-02-23 15:25:20 - epoch 019 lr: 0.1
2022-02-23 15:26:00 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.0010
2022-02-23 15:26:34 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.3305
2022-02-23 15:27:07 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.4496
2022-02-23 15:27:40 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.0800
2022-02-23 15:28:13 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.1645
2022-02-23 15:28:47 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.1540
2022-02-23 15:29:20 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.0234
2022-02-23 15:29:54 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.3101
2022-02-23 15:30:27 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.2261
2022-02-23 15:31:01 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.3686
2022-02-23 15:31:34 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.0678
2022-02-23 15:32:08 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.2685
2022-02-23 15:32:41 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.2833
2022-02-23 15:33:15 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.0172
2022-02-23 15:33:48 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.5314
2022-02-23 15:34:21 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.1303
2022-02-23 15:34:55 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.3264
2022-02-23 15:35:28 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.1097
2022-02-23 15:36:02 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.3288
2022-02-23 15:36:36 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.0989
2022-02-23 15:37:10 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.0358
2022-02-23 15:37:44 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.1896
2022-02-23 15:38:18 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.1930
2022-02-23 15:38:52 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.2277
2022-02-23 15:39:25 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.1317
2022-02-23 15:39:59 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.2231
2022-02-23 15:40:33 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.2893
2022-02-23 15:41:08 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.2677
2022-02-23 15:41:41 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.2113
2022-02-23 15:42:15 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.5187
2022-02-23 15:42:50 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.2956
2022-02-23 15:43:23 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.8896
2022-02-23 15:43:58 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.0682
2022-02-23 15:44:32 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.2938
2022-02-23 15:45:06 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.2851
2022-02-23 15:45:41 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 1.9883
2022-02-23 15:46:15 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.2303
2022-02-23 15:46:49 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.2332
2022-02-23 15:47:24 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.1380
2022-02-23 15:48:00 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 1.9885
2022-02-23 15:48:33 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.2582
2022-02-23 15:49:09 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.2020
2022-02-23 15:49:43 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.0209
2022-02-23 15:50:21 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.3027
2022-02-23 15:51:00 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.3907
2022-02-23 15:51:37 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.0859
2022-02-23 15:52:21 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.0714
2022-02-23 15:53:07 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.1946
2022-02-23 15:53:48 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.1932
2022-02-23 15:54:35 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.1733
2022-02-23 15:54:37 - train: epoch 019, train_loss: 2.1771
2022-02-23 15:56:17 - eval: epoch: 019, acc1: 54.278%, acc5: 79.738%, test_loss: 1.9230, per_image_load_time: 0.737ms, per_image_inference_time: 0.526ms
2022-02-23 15:56:18 - until epoch: 019, best_acc1: 54.278%
2022-02-23 15:56:18 - epoch 020 lr: 0.1
2022-02-23 15:56:58 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.3533
2022-02-23 15:57:31 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 1.9589
2022-02-23 15:58:04 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.1637
2022-02-23 15:58:37 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 1.9309
2022-02-23 15:59:10 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.1515
2022-02-23 15:59:43 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.3197
2022-02-23 16:00:17 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 1.9854
2022-02-23 16:00:50 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.1238
2022-02-23 16:01:23 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.4942
2022-02-23 16:01:57 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.1826
2022-02-23 16:02:30 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.0680
2022-02-23 16:03:04 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 1.9183
2022-02-23 16:03:37 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 1.9695
2022-02-23 16:04:11 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.3427
2022-02-23 16:04:44 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.2750
2022-02-23 16:05:18 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.0451
2022-02-23 16:05:51 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 1.8966
2022-02-23 16:06:25 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.1995
2022-02-23 16:06:58 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.0035
2022-02-23 16:07:32 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.1077
2022-02-23 16:08:05 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.2623
2022-02-23 16:08:39 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 1.9685
2022-02-23 16:09:12 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 1.9375
2022-02-23 16:09:46 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.3658
2022-02-23 16:10:20 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.0319
2022-02-23 16:10:53 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 1.9991
2022-02-23 16:11:27 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.2683
2022-02-23 16:12:01 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.2576
2022-02-23 16:12:34 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.2629
2022-02-23 16:13:08 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.0999
2022-02-23 16:13:42 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.1731
2022-02-23 16:14:15 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.2686
2022-02-23 16:14:49 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.0554
2022-02-23 16:15:23 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.3376
2022-02-23 16:15:58 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.1099
2022-02-23 16:16:33 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.1433
2022-02-23 16:17:07 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.0889
2022-02-23 16:17:42 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.2490
2022-02-23 16:18:16 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.3042
2022-02-23 16:18:50 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.1313
2022-02-23 16:19:25 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.0282
2022-02-23 16:20:01 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.0670
2022-02-23 16:20:38 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.1577
2022-02-23 16:21:13 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.2229
2022-02-23 16:21:52 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.2284
2022-02-23 16:22:29 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.3052
2022-02-23 16:23:09 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 1.9974
2022-02-23 16:23:53 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.1745
2022-02-23 16:24:31 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.2962
2022-02-23 16:25:16 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.0528
2022-02-23 16:25:18 - train: epoch 020, train_loss: 2.1651
2022-02-23 16:26:49 - eval: epoch: 020, acc1: 54.846%, acc5: 80.160%, test_loss: 1.9101, per_image_load_time: 0.614ms, per_image_inference_time: 0.537ms
2022-02-23 16:26:50 - until epoch: 020, best_acc1: 54.846%
2022-02-23 16:26:50 - epoch 021 lr: 0.1
2022-02-23 16:27:29 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.0288
2022-02-23 16:28:03 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.1519
2022-02-23 16:28:36 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.8345
2022-02-23 16:29:09 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.2685
2022-02-23 16:29:43 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.0082
2022-02-23 16:30:16 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.0144
2022-02-23 16:30:50 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 1.9884
2022-02-23 16:31:23 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.2787
2022-02-23 16:31:57 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.1922
2022-02-23 16:32:30 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.0687
2022-02-23 16:33:04 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.0049
2022-02-23 16:33:37 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.1240
2022-02-23 16:34:11 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.0685
2022-02-23 16:34:44 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.0916
2022-02-23 16:35:17 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 1.8901
2022-02-23 16:35:51 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.2491
2022-02-23 16:36:26 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.1546
2022-02-23 16:37:00 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.0495
2022-02-23 16:37:34 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.2695
2022-02-23 16:38:07 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.3948
2022-02-23 16:38:41 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 1.9925
2022-02-23 16:39:15 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.2586
2022-02-23 16:39:50 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.0244
2022-02-23 16:40:23 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.0266
2022-02-23 16:40:58 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.1396
2022-02-23 16:41:31 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.3579
2022-02-23 16:42:05 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.0764
2022-02-23 16:42:40 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.1316
2022-02-23 16:43:14 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.0195
2022-02-23 16:43:48 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.2096
2022-02-23 16:44:22 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.1871
2022-02-23 16:44:56 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.0395
2022-02-23 16:45:30 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.3992
2022-02-23 16:46:05 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.2947
2022-02-23 16:46:39 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 1.9890
2022-02-23 16:47:14 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.0939
2022-02-23 16:47:48 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.1382
2022-02-23 16:48:23 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.0752
2022-02-23 16:48:58 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.1242
2022-02-23 16:49:32 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.3420
2022-02-23 16:50:08 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.1160
2022-02-23 16:50:43 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.1626
2022-02-23 16:51:19 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.1285
2022-02-23 16:51:54 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.1535
2022-02-23 16:52:33 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.2923
2022-02-23 16:53:10 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.0879
2022-02-23 16:53:50 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.2712
2022-02-23 16:54:33 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.3390
2022-02-23 16:55:11 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 1.9126
2022-02-23 16:55:54 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.0234
2022-02-23 16:55:57 - train: epoch 021, train_loss: 2.1602
2022-02-23 16:57:39 - eval: epoch: 021, acc1: 53.650%, acc5: 79.240%, test_loss: 1.9764, per_image_load_time: 3.418ms, per_image_inference_time: 0.537ms
2022-02-23 16:57:40 - until epoch: 021, best_acc1: 54.846%
2022-02-23 16:57:40 - epoch 022 lr: 0.1
2022-02-23 16:58:20 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 1.9622
2022-02-23 16:58:53 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.0704
2022-02-23 16:59:26 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 1.9123
2022-02-23 17:00:00 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.0046
2022-02-23 17:00:33 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.1467
2022-02-23 17:01:07 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.2594
2022-02-23 17:01:41 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.1788
2022-02-23 17:02:14 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.2980
2022-02-23 17:02:49 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.3575
2022-02-23 17:03:22 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.1803
2022-02-23 17:03:56 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.0818
2022-02-23 17:04:30 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.8556
2022-02-23 17:05:04 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.1235
2022-02-23 17:05:38 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.1429
2022-02-23 17:06:12 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.1696
2022-02-23 17:06:46 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.0000
2022-02-23 17:07:20 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 1.9764
2022-02-23 17:07:53 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.3236
2022-02-23 17:08:27 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 1.9887
2022-02-23 17:09:01 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.1199
2022-02-23 17:09:34 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.2345
2022-02-23 17:10:08 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 1.9355
2022-02-23 17:10:42 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.2750
2022-02-23 17:11:16 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.2804
2022-02-23 17:11:49 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.0707
2022-02-23 17:12:23 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 1.8933
2022-02-23 17:12:56 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 1.9827
2022-02-23 17:13:31 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.4606
2022-02-23 17:14:04 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.0815
2022-02-23 17:14:38 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.1692
2022-02-23 17:15:12 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.3594
2022-02-23 17:15:47 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.2416
2022-02-23 17:16:21 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.1441
2022-02-23 17:16:55 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 1.9543
2022-02-23 17:17:29 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.2125
2022-02-23 17:18:04 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.1789
2022-02-23 17:18:39 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.2877
2022-02-23 17:19:13 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.2221
2022-02-23 17:19:48 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 1.9892
2022-02-23 17:20:23 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.2017
2022-02-23 17:20:58 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.0347
2022-02-23 17:21:33 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.0395
2022-02-23 17:22:09 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.3847
2022-02-23 17:22:45 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.1983
2022-02-23 17:23:21 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.1034
2022-02-23 17:24:02 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.3731
2022-02-23 17:24:40 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.1781
2022-02-23 17:25:21 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 1.9250
2022-02-23 17:26:08 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.0176
2022-02-23 17:26:53 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.0477
2022-02-23 17:26:55 - train: epoch 022, train_loss: 2.1555
2022-02-23 17:28:19 - eval: epoch: 022, acc1: 52.558%, acc5: 78.316%, test_loss: 2.0197, per_image_load_time: 2.031ms, per_image_inference_time: 0.552ms
2022-02-23 17:28:20 - until epoch: 022, best_acc1: 54.846%
2022-02-23 17:28:20 - epoch 023 lr: 0.1
2022-02-23 17:29:00 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 1.9674
2022-02-23 17:29:33 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 1.8298
2022-02-23 17:30:07 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 1.9380
2022-02-23 17:30:40 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.1633
2022-02-23 17:31:14 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.1913
2022-02-23 17:31:47 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.1041
2022-02-23 17:32:20 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 1.9366
2022-02-23 17:32:54 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.0221
2022-02-23 17:33:27 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.1453
2022-02-23 17:34:00 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.0029
2022-02-23 17:34:34 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.2236
2022-02-23 17:35:07 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.0579
2022-02-23 17:35:41 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.2239
2022-02-23 17:36:14 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.1288
2022-02-23 17:36:48 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.0665
2022-02-23 17:37:22 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.1498
2022-02-23 17:37:55 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.2180
2022-02-23 17:38:29 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.0428
2022-02-23 17:39:02 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.1941
2022-02-23 17:39:36 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.8109
2022-02-23 17:40:10 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.2693
2022-02-23 17:40:44 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.7974
2022-02-23 17:41:18 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.0663
2022-02-23 17:41:51 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.1287
2022-02-23 17:42:25 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.2089
2022-02-23 17:42:59 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.1416
2022-02-23 17:43:33 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.0991
2022-02-23 17:44:07 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.1816
2022-02-23 17:44:42 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.2469
2022-02-23 17:45:15 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.3257
2022-02-23 17:45:50 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.1822
2022-02-23 17:46:23 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.3286
2022-02-23 17:46:58 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.2075
2022-02-23 17:47:32 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.1957
2022-02-23 17:48:06 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.0681
2022-02-23 17:48:41 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 1.9485
2022-02-23 17:49:15 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.1899
2022-02-23 17:49:51 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.1022
2022-02-23 17:50:25 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.1682
2022-02-23 17:51:01 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.1294
2022-02-23 17:51:35 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 1.9746
2022-02-23 17:52:12 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.0603
2022-02-23 17:52:47 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.0005
2022-02-23 17:53:25 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 1.8842
2022-02-23 17:54:06 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.0420
2022-02-23 17:54:40 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.2332
2022-02-23 17:55:23 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.0738
2022-02-23 17:56:06 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.0299
2022-02-23 17:56:44 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.1397
2022-02-23 17:57:31 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.2345
2022-02-23 17:57:34 - train: epoch 023, train_loss: 2.1477
2022-02-23 17:58:58 - eval: epoch: 023, acc1: 55.756%, acc5: 80.704%, test_loss: 1.8583, per_image_load_time: 2.362ms, per_image_inference_time: 0.555ms
2022-02-23 17:58:59 - until epoch: 023, best_acc1: 55.756%
2022-02-23 17:58:59 - epoch 024 lr: 0.1
2022-02-23 17:59:39 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.1806
2022-02-23 18:00:12 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.1784
2022-02-23 18:00:45 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.0451
2022-02-23 18:01:19 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.1909
2022-02-23 18:01:52 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.0869
2022-02-23 18:02:26 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.0320
2022-02-23 18:02:59 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.0017
2022-02-23 18:03:33 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.0307
2022-02-23 18:04:07 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.1576
2022-02-23 18:04:40 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.1961
2022-02-23 18:05:14 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.8484
2022-02-23 18:05:47 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.0269
2022-02-23 18:06:21 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.4748
2022-02-23 18:06:55 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.0056
2022-02-23 18:07:28 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.3394
2022-02-23 18:08:02 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.1236
2022-02-23 18:08:36 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.1117
2022-02-23 18:09:09 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.3514
2022-02-23 18:09:44 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 1.9671
2022-02-23 18:10:17 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.1617
2022-02-23 18:10:51 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.1484
2022-02-23 18:11:25 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.0227
2022-02-23 18:11:59 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.1919
2022-02-23 18:12:32 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.1084
2022-02-23 18:13:06 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.1116
2022-02-23 18:13:40 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.1097
2022-02-23 18:14:14 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.2723
2022-02-23 18:14:48 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.2845
2022-02-23 18:15:22 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.2551
2022-02-23 18:15:56 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.1103
2022-02-23 18:16:30 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.0119
2022-02-23 18:17:05 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.1852
2022-02-23 18:17:39 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.8913
2022-02-23 18:18:14 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.1391
2022-02-23 18:18:48 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 1.9832
2022-02-23 18:19:23 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.2165
2022-02-23 18:19:57 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.0948
2022-02-23 18:20:32 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.2811
2022-02-23 18:21:07 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.0916
2022-02-23 18:21:41 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.1378
2022-02-23 18:22:17 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.1747
2022-02-23 18:22:51 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.0895
2022-02-23 18:23:29 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.1156
2022-02-23 18:24:03 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.1251
2022-02-23 18:24:42 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.0072
2022-02-23 18:25:21 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.2204
2022-02-23 18:25:57 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.1486
2022-02-23 18:26:41 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.0648
2022-02-23 18:27:22 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.1609
2022-02-23 18:28:02 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.2482
2022-02-23 18:28:04 - train: epoch 024, train_loss: 2.1443
2022-02-23 18:29:43 - eval: epoch: 024, acc1: 53.410%, acc5: 79.110%, test_loss: 1.9769, per_image_load_time: 3.281ms, per_image_inference_time: 0.525ms
2022-02-23 18:29:44 - until epoch: 024, best_acc1: 55.756%
2022-02-23 18:29:44 - epoch 025 lr: 0.1
2022-02-23 18:30:24 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.0138
2022-02-23 18:30:58 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 1.9577
2022-02-23 18:31:31 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 1.9189
2022-02-23 18:32:04 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.0937
2022-02-23 18:32:38 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.1279
2022-02-23 18:33:11 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.1202
2022-02-23 18:33:44 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.1530
2022-02-23 18:34:17 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.1128
2022-02-23 18:34:51 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 1.9540
2022-02-23 18:35:24 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.0559
2022-02-23 18:35:58 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 1.9282
2022-02-23 18:36:31 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.2106
2022-02-23 18:37:05 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.0469
2022-02-23 18:37:38 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.2798
2022-02-23 18:38:11 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.1522
2022-02-23 18:38:45 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 1.8730
2022-02-23 18:39:19 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.1351
2022-02-23 18:39:52 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.0528
2022-02-23 18:40:26 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.0523
2022-02-23 18:41:00 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.0999
2022-02-23 18:41:33 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 1.9922
2022-02-23 18:42:07 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.0145
2022-02-23 18:42:40 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.2197
2022-02-23 18:43:15 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 1.9152
2022-02-23 18:43:48 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.1108
2022-02-23 18:44:22 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.2634
2022-02-23 18:44:56 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.1283
2022-02-23 18:45:30 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.0131
2022-02-23 18:46:04 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.3088
2022-02-23 18:46:38 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.3991
2022-02-23 18:47:13 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.0350
2022-02-23 18:47:48 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.2998
2022-02-23 18:48:22 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.0788
2022-02-23 18:48:56 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.3259
2022-02-23 18:49:30 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 1.9134
2022-02-23 18:50:04 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.2583
2022-02-23 18:50:38 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.1159
2022-02-23 18:51:13 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.1620
2022-02-23 18:51:47 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.3005
2022-02-23 18:52:21 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.2360
2022-02-23 18:52:56 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.3434
2022-02-23 18:53:31 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.2169
2022-02-23 18:54:08 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.0511
2022-02-23 18:54:42 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.0435
2022-02-23 18:55:21 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.0776
2022-02-23 18:55:57 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.0805
2022-02-23 18:56:37 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.1608
2022-02-23 18:57:22 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 1.9913
2022-02-23 18:57:57 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.1070
2022-02-23 18:58:43 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.2655
2022-02-23 18:58:45 - train: epoch 025, train_loss: 2.1382
2022-02-23 19:00:18 - eval: epoch: 025, acc1: 54.342%, acc5: 79.718%, test_loss: 1.9280, per_image_load_time: 1.492ms, per_image_inference_time: 0.552ms
2022-02-23 19:00:19 - until epoch: 025, best_acc1: 55.756%
2022-02-23 19:00:19 - epoch 026 lr: 0.1
2022-02-23 19:00:59 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 1.9133
2022-02-23 19:01:32 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.9270
2022-02-23 19:02:05 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.0683
2022-02-23 19:02:38 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.0678
2022-02-23 19:03:11 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.1736
2022-02-23 19:03:44 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.2800
2022-02-23 19:04:17 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.0074
2022-02-23 19:04:51 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 1.9274
2022-02-23 19:05:24 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.3445
2022-02-23 19:05:57 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 1.9749
2022-02-23 19:06:31 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.1089
2022-02-23 19:07:04 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.1599
2022-02-23 19:07:38 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.0012
2022-02-23 19:08:11 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.1945
2022-02-23 19:08:45 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.0494
2022-02-23 19:09:18 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.2402
2022-02-23 19:09:52 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.0718
2022-02-23 19:10:26 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.2220
2022-02-23 19:10:59 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.3872
2022-02-23 19:11:33 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.2208
2022-02-23 19:12:06 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.2047
2022-02-23 19:12:40 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.0645
2022-02-23 19:13:14 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.1665
2022-02-23 19:13:48 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.4482
2022-02-23 19:14:21 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.2061
2022-02-23 19:14:55 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.1426
2022-02-23 19:15:29 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.0130
2022-02-23 19:16:02 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.0101
2022-02-23 19:16:36 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.2389
2022-02-23 19:17:10 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.0151
2022-02-23 19:17:44 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.1226
2022-02-23 19:18:18 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.2266
2022-02-23 19:18:52 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.0616
2022-02-23 19:19:25 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.2969
2022-02-23 19:20:00 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.2154
2022-02-23 19:20:34 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.0226
2022-02-23 19:21:07 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.2091
2022-02-23 19:21:42 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.3396
2022-02-23 19:22:16 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.2373
2022-02-23 19:22:51 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.3104
2022-02-23 19:23:25 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.1630
2022-02-23 19:24:00 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.3040
2022-02-23 19:24:35 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.2899
2022-02-23 19:25:11 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.3770
2022-02-23 19:25:49 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.3487
2022-02-23 19:26:25 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.0607
2022-02-23 19:27:09 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.1491
2022-02-23 19:27:50 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.0665
2022-02-23 19:28:30 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.1052
2022-02-23 19:29:20 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.3154
2022-02-23 19:29:22 - train: epoch 026, train_loss: 2.1353
2022-02-23 19:30:59 - eval: epoch: 026, acc1: 54.962%, acc5: 80.588%, test_loss: 1.8870, per_image_load_time: 0.618ms, per_image_inference_time: 0.540ms
2022-02-23 19:31:00 - until epoch: 026, best_acc1: 55.756%
2022-02-23 19:31:00 - epoch 027 lr: 0.1
2022-02-23 19:31:40 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.2699
2022-02-23 19:32:14 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.0098
2022-02-23 19:32:47 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.1408
2022-02-23 19:33:20 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.2547
2022-02-23 19:33:53 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.2819
2022-02-23 19:34:27 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.4383
2022-02-23 19:35:00 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.3753
2022-02-23 19:35:33 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.1489
2022-02-23 19:36:07 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.0732
2022-02-23 19:36:40 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.1303
2022-02-23 19:37:14 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.0883
2022-02-23 19:37:47 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.4259
2022-02-23 19:38:20 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.1532
2022-02-23 19:38:54 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.3414
2022-02-23 19:39:27 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.2150
2022-02-23 19:40:01 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.1377
2022-02-23 19:40:35 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.1580
2022-02-23 19:41:08 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.1866
2022-02-23 19:41:42 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.3227
2022-02-23 19:42:16 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.1113
2022-02-23 19:42:49 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.3610
2022-02-23 19:43:23 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.3509
2022-02-23 19:43:57 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.3575
2022-02-23 19:44:31 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.1385
2022-02-23 19:45:04 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 1.9693
2022-02-23 19:45:38 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.3303
2022-02-23 19:46:12 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.1219
2022-02-23 19:46:46 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.2815
2022-02-23 19:47:20 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.1949
2022-02-23 19:47:53 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 1.9910
2022-02-23 19:48:28 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 1.9858
2022-02-23 19:49:02 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 1.9971
2022-02-23 19:49:36 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.1926
2022-02-23 19:50:10 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.0733
2022-02-23 19:50:44 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.4368
2022-02-23 19:51:19 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 1.8968
2022-02-23 19:51:53 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.0610
2022-02-23 19:52:28 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 1.9118
2022-02-23 19:53:02 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 1.9601
2022-02-23 19:53:37 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.3009
2022-02-23 19:54:11 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.0503
2022-02-23 19:54:47 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.0779
2022-02-23 19:55:20 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.0759
2022-02-23 19:55:58 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.1305
2022-02-23 19:56:35 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 1.9883
2022-02-23 19:57:14 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.0780
2022-02-23 19:57:58 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.1737
2022-02-23 19:58:41 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.2872
2022-02-23 19:59:22 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.1496
2022-02-23 20:00:13 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.7819
2022-02-23 20:00:16 - train: epoch 027, train_loss: 2.1316
2022-02-23 20:01:57 - eval: epoch: 027, acc1: 56.064%, acc5: 81.040%, test_loss: 1.8505, per_image_load_time: 3.386ms, per_image_inference_time: 0.525ms
2022-02-23 20:01:58 - until epoch: 027, best_acc1: 56.064%
2022-02-23 20:01:58 - epoch 028 lr: 0.1
2022-02-23 20:02:38 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 1.8912
2022-02-23 20:03:11 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.0736
2022-02-23 20:03:45 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.1217
2022-02-23 20:04:18 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.0105
2022-02-23 20:04:51 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 1.9896
2022-02-23 20:05:25 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.2910
2022-02-23 20:05:58 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.2773
2022-02-23 20:06:31 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.7634
2022-02-23 20:07:05 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 1.9645
2022-02-23 20:07:38 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.2230
2022-02-23 20:08:12 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.2098
2022-02-23 20:08:46 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.0309
2022-02-23 20:09:19 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.1190
2022-02-23 20:09:53 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.4472
2022-02-23 20:10:27 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.0129
2022-02-23 20:11:00 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.0674
2022-02-23 20:11:34 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.0198
2022-02-23 20:12:07 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 1.9567
2022-02-23 20:12:41 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.0552
2022-02-23 20:13:14 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.3829
2022-02-23 20:13:48 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.1006
2022-02-23 20:14:22 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.2435
2022-02-23 20:14:56 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.2114
2022-02-23 20:15:30 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.2728
2022-02-23 20:16:03 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.0341
2022-02-23 20:16:37 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.0651
2022-02-23 20:17:11 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.0619
2022-02-23 20:17:44 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.1073
2022-02-23 20:18:19 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.1400
2022-02-23 20:18:53 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.2223
2022-02-23 20:19:27 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.3498
2022-02-23 20:20:01 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.1657
2022-02-23 20:20:35 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.2486
2022-02-23 20:21:10 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.0403
2022-02-23 20:21:44 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.0563
2022-02-23 20:22:19 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.0046
2022-02-23 20:22:53 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 1.9632
2022-02-23 20:23:28 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.8381
2022-02-23 20:24:03 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.1081
2022-02-23 20:24:38 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.2664
2022-02-23 20:25:14 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.2663
2022-02-23 20:25:48 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.2370
2022-02-23 20:26:26 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.1003
2022-02-23 20:27:01 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 1.9773
2022-02-23 20:27:41 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.1044
2022-02-23 20:28:25 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.2200
2022-02-23 20:29:06 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.0766
2022-02-23 20:29:49 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.0790
2022-02-23 20:30:37 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 1.9920
2022-02-23 20:31:27 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.0852
2022-02-23 20:31:30 - train: epoch 028, train_loss: 2.1243
2022-02-23 20:32:54 - eval: epoch: 028, acc1: 56.294%, acc5: 81.302%, test_loss: 1.8324, per_image_load_time: 2.690ms, per_image_inference_time: 0.577ms
2022-02-23 20:32:55 - until epoch: 028, best_acc1: 56.294%
2022-02-23 20:32:55 - epoch 029 lr: 0.1
2022-02-23 20:33:36 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.1819
2022-02-23 20:34:09 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.1299
2022-02-23 20:34:43 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.3134
2022-02-23 20:35:16 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.0751
2022-02-23 20:35:50 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.0752
2022-02-23 20:36:23 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.3889
2022-02-23 20:36:56 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.6102
2022-02-23 20:37:30 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.3569
2022-02-23 20:38:04 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.0001
2022-02-23 20:38:38 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 1.9337
2022-02-23 20:39:11 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.1061
2022-02-23 20:39:45 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.2410
2022-02-23 20:40:18 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.1282
2022-02-23 20:40:52 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.3971
2022-02-23 20:41:26 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.2608
2022-02-23 20:41:59 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.1082
2022-02-23 20:42:33 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.2329
2022-02-23 20:43:06 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.2683
2022-02-23 20:43:40 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 1.9368
2022-02-23 20:44:14 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.3789
2022-02-23 20:44:47 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.1553
2022-02-23 20:45:21 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.2693
2022-02-23 20:45:55 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.1370
2022-02-23 20:46:29 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.0701
2022-02-23 20:47:02 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 1.9880
2022-02-23 20:47:36 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.1871
2022-02-23 20:48:10 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.0010
2022-02-23 20:48:44 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.0574
2022-02-23 20:49:17 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.0619
2022-02-23 20:49:51 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.0394
2022-02-23 20:50:25 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.1082
2022-02-23 20:50:59 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.1602
2022-02-23 20:51:34 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.1612
2022-02-23 20:52:07 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 1.9898
2022-02-23 20:52:42 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.3199
2022-02-23 20:53:16 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.1315
2022-02-23 20:53:50 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.1214
2022-02-23 20:54:25 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.0349
2022-02-23 20:55:00 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 1.8378
2022-02-23 20:55:36 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.0616
2022-02-23 20:56:10 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.0219
2022-02-23 20:56:45 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.0365
2022-02-23 20:57:20 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.2329
2022-02-23 20:57:57 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.1263
2022-02-23 20:58:38 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.2500
2022-02-23 20:59:18 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.3428
2022-02-23 20:59:56 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 1.9445
2022-02-23 21:00:43 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.1834
2022-02-23 21:01:33 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.4204
2022-02-23 21:02:25 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.7802
2022-02-23 21:02:27 - train: epoch 029, train_loss: 2.1222
2022-02-23 21:03:50 - eval: epoch: 029, acc1: 55.772%, acc5: 81.002%, test_loss: 1.8482, per_image_load_time: 0.932ms, per_image_inference_time: 0.613ms
2022-02-23 21:03:51 - until epoch: 029, best_acc1: 56.294%
2022-02-23 21:03:51 - epoch 030 lr: 0.1
2022-02-23 21:04:32 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.2103
2022-02-23 21:05:06 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.1270
2022-02-23 21:05:39 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.0152
2022-02-23 21:06:12 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 1.8648
2022-02-23 21:06:46 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.4076
2022-02-23 21:07:19 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 1.9604
2022-02-23 21:07:53 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.0844
2022-02-23 21:08:26 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.2562
2022-02-23 21:09:00 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.1606
2022-02-23 21:09:33 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 1.7905
2022-02-23 21:10:06 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 1.9404
2022-02-23 21:10:40 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.1451
2022-02-23 21:11:13 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 1.9115
2022-02-23 21:11:47 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.0129
2022-02-23 21:12:21 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 1.9563
2022-02-23 21:12:54 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.1274
2022-02-23 21:13:27 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.2911
2022-02-23 21:14:02 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.1115
2022-02-23 21:14:35 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.1623
2022-02-23 21:15:09 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.1168
2022-02-23 21:15:43 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.2963
2022-02-23 21:16:16 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.0841
2022-02-23 21:16:50 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.1500
2022-02-23 21:17:24 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.3398
2022-02-23 21:17:57 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.2097
2022-02-23 21:18:31 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.1360
2022-02-23 21:19:05 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.0253
2022-02-23 21:19:39 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.0979
2022-02-23 21:20:14 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.1253
2022-02-23 21:20:48 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.2194
2022-02-23 21:21:22 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.0188
2022-02-23 21:21:56 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.0721
2022-02-23 21:22:30 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.3165
2022-02-23 21:23:04 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.1559
2022-02-23 21:23:39 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.2517
2022-02-23 21:24:13 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 1.9615
2022-02-23 21:24:48 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.0345
2022-02-23 21:25:22 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.2826
2022-02-23 21:25:56 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.1249
2022-02-23 21:26:31 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 1.8748
2022-02-23 21:27:06 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.0510
2022-02-23 21:27:41 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.2785
2022-02-23 21:28:17 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.0654
2022-02-23 21:28:52 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.1428
2022-02-23 21:29:31 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.2408
2022-02-23 21:30:13 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 1.9289
2022-02-23 21:30:54 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.1161
2022-02-23 21:31:34 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.1287
2022-02-23 21:32:24 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.2315
2022-02-23 21:33:15 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.1609
2022-02-23 21:33:17 - train: epoch 030, train_loss: 2.1220
2022-02-23 21:34:48 - eval: epoch: 030, acc1: 55.402%, acc5: 80.786%, test_loss: 1.8744, per_image_load_time: 2.703ms, per_image_inference_time: 0.578ms
2022-02-23 21:34:48 - until epoch: 030, best_acc1: 56.294%
2022-02-23 21:34:48 - epoch 031 lr: 0.010000000000000002
2022-02-23 21:35:29 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 1.9322
2022-02-23 21:36:03 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.5950
2022-02-23 21:36:36 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.7211
2022-02-23 21:37:09 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.8544
2022-02-23 21:37:43 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.6032
2022-02-23 21:38:17 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.5984
2022-02-23 21:38:50 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.5779
2022-02-23 21:39:24 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.5156
2022-02-23 21:39:57 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.6323
2022-02-23 21:40:31 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 1.8461
2022-02-23 21:41:05 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 1.8569
2022-02-23 21:41:39 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.5979
2022-02-23 21:42:12 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.4132
2022-02-23 21:42:46 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.5311
2022-02-23 21:43:19 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.6616
2022-02-23 21:43:53 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.4486
2022-02-23 21:44:27 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.5076
2022-02-23 21:45:01 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.5826
2022-02-23 21:45:34 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.7120
2022-02-23 21:46:08 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.6096
2022-02-23 21:46:42 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.4512
2022-02-23 21:47:15 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.3709
2022-02-23 21:47:48 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.4834
2022-02-23 21:48:22 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.5213
2022-02-23 21:48:56 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.4803
2022-02-23 21:49:30 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.6295
2022-02-23 21:50:03 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.5776
2022-02-23 21:50:37 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.7250
2022-02-23 21:51:10 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.5902
2022-02-23 21:51:44 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.6982
2022-02-23 21:52:19 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.3932
2022-02-23 21:52:52 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.7095
2022-02-23 21:53:26 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.5789
2022-02-23 21:54:00 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.6610
2022-02-23 21:54:34 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.6841
2022-02-23 21:55:09 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.6552
2022-02-23 21:55:43 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.4923
2022-02-23 21:56:18 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.3857
2022-02-23 21:56:52 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.3812
2022-02-23 21:57:27 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.5250
2022-02-23 21:58:03 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.4822
2022-02-23 21:58:37 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.5190
2022-02-23 21:59:12 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.4185
2022-02-23 21:59:49 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.6557
2022-02-23 22:00:24 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.6094
2022-02-23 22:01:06 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.5435
2022-02-23 22:01:50 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.5341
2022-02-23 22:02:32 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.4318
2022-02-23 22:03:13 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.4554
2022-02-23 22:04:02 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.4843
2022-02-23 22:04:05 - train: epoch 031, train_loss: 1.5746
2022-02-23 22:05:42 - eval: epoch: 031, acc1: 70.046%, acc5: 89.884%, test_loss: 1.1946, per_image_load_time: 1.899ms, per_image_inference_time: 0.591ms
2022-02-23 22:05:43 - until epoch: 031, best_acc1: 70.046%
2022-02-23 22:05:43 - epoch 032 lr: 0.010000000000000002
2022-02-23 22:06:24 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.4338
2022-02-23 22:06:57 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.5233
2022-02-23 22:07:31 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.5227
2022-02-23 22:08:04 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.6472
2022-02-23 22:08:38 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.5817
2022-02-23 22:09:11 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.5752
2022-02-23 22:09:45 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.5322
2022-02-23 22:10:18 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.4076
2022-02-23 22:10:52 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.4850
2022-02-23 22:11:25 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.5798
2022-02-23 22:11:59 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.5776
2022-02-23 22:12:32 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.5308
2022-02-23 22:13:06 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.4597
2022-02-23 22:13:39 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.5792
2022-02-23 22:14:13 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.5248
2022-02-23 22:14:46 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.6057
2022-02-23 22:15:20 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.6571
2022-02-23 22:15:53 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.7517
2022-02-23 22:16:27 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.2886
2022-02-23 22:17:01 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.5024
2022-02-23 22:17:34 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.4112
2022-02-23 22:18:08 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.3207
2022-02-23 22:18:42 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.5326
2022-02-23 22:19:15 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.4232
2022-02-23 22:19:49 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.6013
2022-02-23 22:20:23 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.2447
2022-02-23 22:20:56 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.4609
2022-02-23 22:21:31 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.5847
2022-02-23 22:22:04 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.2713
2022-02-23 22:22:38 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.2493
2022-02-23 22:23:13 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.4133
2022-02-23 22:23:46 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.4448
2022-02-23 22:24:21 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.3647
2022-02-23 22:24:55 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.2127
2022-02-23 22:25:28 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.4558
2022-02-23 22:26:04 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.4519
2022-02-23 22:26:38 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.5176
2022-02-23 22:27:12 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.3365
2022-02-23 22:27:47 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.5155
2022-02-23 22:28:21 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.4276
2022-02-23 22:28:57 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.4145
2022-02-23 22:29:32 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.3225
2022-02-23 22:30:07 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.6398
2022-02-23 22:30:45 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.4367
2022-02-23 22:31:22 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.6382
2022-02-23 22:32:00 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.4186
2022-02-23 22:32:47 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.5433
2022-02-23 22:33:35 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.3130
2022-02-23 22:34:28 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.6227
2022-02-23 22:35:22 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.4540
2022-02-23 22:35:25 - train: epoch 032, train_loss: 1.4623
2022-02-23 22:36:58 - eval: epoch: 032, acc1: 70.956%, acc5: 90.352%, test_loss: 1.1475, per_image_load_time: 1.093ms, per_image_inference_time: 0.575ms
2022-02-23 22:36:59 - until epoch: 032, best_acc1: 70.956%
2022-02-23 22:36:59 - epoch 033 lr: 0.010000000000000002
2022-02-23 22:37:39 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.2845
2022-02-23 22:38:12 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.5119
2022-02-23 22:38:46 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.3124
2022-02-23 22:39:19 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.3522
2022-02-23 22:39:53 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.4854
2022-02-23 22:40:26 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.2758
2022-02-23 22:41:00 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.5022
2022-02-23 22:41:33 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.5284
2022-02-23 22:42:07 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.4565
2022-02-23 22:42:40 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.4634
2022-02-23 22:43:14 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.3816
2022-02-23 22:43:47 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.4026
2022-02-23 22:44:21 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.3004
2022-02-23 22:44:55 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.5372
2022-02-23 22:45:28 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.6029
2022-02-23 22:46:02 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.5751
2022-02-23 22:46:35 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.2991
2022-02-23 22:47:09 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.6313
2022-02-23 22:47:42 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.4239
2022-02-23 22:48:16 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.3502
2022-02-23 22:48:49 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.3957
2022-02-23 22:49:23 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.5237
2022-02-23 22:49:57 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.3450
2022-02-23 22:50:31 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.6615
2022-02-23 22:51:05 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.4147
2022-02-23 22:51:39 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.2414
2022-02-23 22:52:14 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.5572
2022-02-23 22:52:48 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.4654
2022-02-23 22:53:22 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.5204
2022-02-23 22:53:56 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.4859
2022-02-23 22:54:31 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.4578
2022-02-23 22:55:05 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.4310
2022-02-23 22:55:40 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.5130
2022-02-23 22:56:14 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.3126
2022-02-23 22:56:48 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.4479
2022-02-23 22:57:23 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.6172
2022-02-23 22:57:56 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.3146
2022-02-23 22:58:31 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.3205
2022-02-23 22:59:05 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.4610
2022-02-23 22:59:40 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.4980
2022-02-23 23:00:15 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.4728
2022-02-23 23:00:50 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.3213
2022-02-23 23:01:25 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.5498
2022-02-23 23:02:02 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.5326
2022-02-23 23:02:41 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.5452
2022-02-23 23:03:18 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.4120
2022-02-23 23:04:00 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.2728
2022-02-23 23:04:46 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.7353
2022-02-23 23:05:32 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.2294
2022-02-23 23:06:13 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.3797
2022-02-23 23:06:16 - train: epoch 033, train_loss: 1.4196
2022-02-23 23:07:44 - eval: epoch: 033, acc1: 71.422%, acc5: 90.580%, test_loss: 1.1297, per_image_load_time: 1.835ms, per_image_inference_time: 0.570ms
2022-02-23 23:07:45 - until epoch: 033, best_acc1: 71.422%
2022-02-23 23:07:45 - epoch 034 lr: 0.010000000000000002
2022-02-23 23:08:26 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.3718
2022-02-23 23:08:59 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.4029
2022-02-23 23:09:32 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.4077
2022-02-23 23:10:05 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.1702
2022-02-23 23:10:39 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.3759
2022-02-23 23:11:12 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.6269
2022-02-23 23:11:45 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.4006
2022-02-23 23:12:19 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.3869
2022-02-23 23:12:52 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.4108
2022-02-23 23:13:26 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.3355
2022-02-23 23:13:59 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.3310
2022-02-23 23:14:33 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.3770
2022-02-23 23:15:06 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.2792
2022-02-23 23:15:40 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.3739
2022-02-23 23:16:13 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.2744
2022-02-23 23:16:47 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.2758
2022-02-23 23:17:21 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.3367
2022-02-23 23:17:54 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.4971
2022-02-23 23:18:28 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.4725
2022-02-23 23:19:02 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.4634
2022-02-23 23:19:36 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.5618
2022-02-23 23:20:09 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.3299
2022-02-23 23:20:43 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.5176
2022-02-23 23:21:17 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.2573
2022-02-23 23:21:51 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.3010
2022-02-23 23:22:25 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.5255
2022-02-23 23:22:59 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.4335
2022-02-23 23:23:33 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.2456
2022-02-23 23:24:07 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.1729
2022-02-23 23:24:40 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.2284
2022-02-23 23:25:15 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.3733
2022-02-23 23:25:48 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.4241
2022-02-23 23:26:23 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.4284
2022-02-23 23:26:57 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.4608
2022-02-23 23:27:31 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.3212
2022-02-23 23:28:06 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.2551
2022-02-23 23:28:40 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.2548
2022-02-23 23:29:15 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.3174
2022-02-23 23:29:48 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.4629
2022-02-23 23:30:24 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.2544
2022-02-23 23:30:58 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.4122
2022-02-23 23:31:33 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.3349
2022-02-23 23:32:07 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.3868
2022-02-23 23:32:44 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.4503
2022-02-23 23:33:24 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.5773
2022-02-23 23:34:00 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.5203
2022-02-23 23:34:45 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.3580
2022-02-23 23:35:32 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.4321
2022-02-23 23:36:09 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.3858
2022-02-23 23:36:58 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.2479
2022-02-23 23:37:01 - train: epoch 034, train_loss: 1.3935
2022-02-23 23:38:41 - eval: epoch: 034, acc1: 71.702%, acc5: 90.836%, test_loss: 1.1174, per_image_load_time: 1.741ms, per_image_inference_time: 0.547ms
2022-02-23 23:38:42 - until epoch: 034, best_acc1: 71.702%
2022-02-23 23:38:42 - epoch 035 lr: 0.010000000000000002
2022-02-23 23:39:22 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.2706
2022-02-23 23:39:55 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.1578
2022-02-23 23:40:28 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.5352
2022-02-23 23:41:02 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.2533
2022-02-23 23:41:35 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.3120
2022-02-23 23:42:08 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.2845
2022-02-23 23:42:42 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.3403
2022-02-23 23:43:15 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.3510
2022-02-23 23:43:49 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.4972
2022-02-23 23:44:22 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.4776
2022-02-23 23:44:56 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.5555
2022-02-23 23:45:29 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.2896
2022-02-23 23:46:03 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.4484
2022-02-23 23:46:36 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.3835
2022-02-23 23:47:09 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.4298
2022-02-23 23:47:43 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.3699
2022-02-23 23:48:16 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.2706
2022-02-23 23:48:50 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.4027
2022-02-23 23:49:23 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.4728
2022-02-23 23:49:57 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.3850
2022-02-23 23:50:31 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.3728
2022-02-23 23:51:05 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.4336
2022-02-23 23:51:38 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.3854
2022-02-23 23:52:12 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.5042
2022-02-23 23:52:46 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.3374
2022-02-23 23:53:20 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.4797
2022-02-23 23:53:54 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.5250
2022-02-23 23:54:27 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.2718
2022-02-23 23:55:01 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.4234
2022-02-23 23:55:35 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.5008
2022-02-23 23:56:09 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.4391
2022-02-23 23:56:43 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.3329
2022-02-23 23:57:17 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.3133
2022-02-23 23:57:51 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.3944
2022-02-23 23:58:25 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.2251
2022-02-23 23:59:01 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.3207
2022-02-23 23:59:34 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.1381
2022-02-24 00:00:09 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.4228
2022-02-24 00:00:43 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.4580
2022-02-24 00:01:19 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.2147
2022-02-24 00:01:53 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.5667
2022-02-24 00:02:28 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.3285
2022-02-24 00:03:03 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.4290
2022-02-24 00:03:40 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.3604
2022-02-24 00:04:21 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.4968
2022-02-24 00:04:56 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.3386
2022-02-24 00:05:42 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.4773
2022-02-24 00:06:29 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.5269
2022-02-24 00:07:13 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.4325
2022-02-24 00:07:56 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.2019
2022-02-24 00:07:59 - train: epoch 035, train_loss: 1.3767
2022-02-24 00:09:40 - eval: epoch: 035, acc1: 71.592%, acc5: 90.770%, test_loss: 1.1186, per_image_load_time: 2.013ms, per_image_inference_time: 0.533ms
2022-02-24 00:09:40 - until epoch: 035, best_acc1: 71.702%
2022-02-24 00:09:40 - epoch 036 lr: 0.010000000000000002
2022-02-24 00:10:21 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.4357
2022-02-24 00:10:54 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.1714
2022-02-24 00:11:27 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.2379
2022-02-24 00:12:00 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.3825
2022-02-24 00:12:33 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.3276
2022-02-24 00:13:07 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.2961
2022-02-24 00:13:40 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.2807
2022-02-24 00:14:14 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.2298
2022-02-24 00:14:47 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.3160
2022-02-24 00:15:21 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.2690
2022-02-24 00:15:54 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.5498
2022-02-24 00:16:27 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.2911
2022-02-24 00:17:00 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.3205
2022-02-24 00:17:34 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.4995
2022-02-24 00:18:08 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.5834
2022-02-24 00:18:41 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.3138
2022-02-24 00:19:15 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.3726
2022-02-24 00:19:48 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.2928
2022-02-24 00:20:22 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.3419
2022-02-24 00:20:55 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.3472
2022-02-24 00:21:29 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.3859
2022-02-24 00:22:02 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.3746
2022-02-24 00:22:36 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.4322
2022-02-24 00:23:09 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.4349
2022-02-24 00:23:43 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.2384
2022-02-24 00:24:16 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.3845
2022-02-24 00:24:50 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.3357
2022-02-24 00:25:23 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.2741
2022-02-24 00:25:57 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.2265
2022-02-24 00:26:31 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.4701
2022-02-24 00:27:04 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.3475
2022-02-24 00:27:39 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.3801
2022-02-24 00:28:12 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.2627
2022-02-24 00:28:46 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.2922
2022-02-24 00:29:20 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.2706
2022-02-24 00:29:54 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.4401
2022-02-24 00:30:29 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.4886
2022-02-24 00:31:03 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.1820
2022-02-24 00:31:37 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.3161
2022-02-24 00:32:11 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.3957
2022-02-24 00:32:46 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.3427
2022-02-24 00:33:20 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.3452
2022-02-24 00:33:56 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.2626
2022-02-24 00:34:31 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.4110
2022-02-24 00:35:09 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.1468
2022-02-24 00:35:52 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.1839
2022-02-24 00:36:29 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.3618
2022-02-24 00:37:13 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.3748
2022-02-24 00:38:01 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.3779
2022-02-24 00:38:49 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.3248
2022-02-24 00:38:51 - train: epoch 036, train_loss: 1.3634
2022-02-24 00:40:17 - eval: epoch: 036, acc1: 71.968%, acc5: 90.820%, test_loss: 1.1103, per_image_load_time: 2.135ms, per_image_inference_time: 0.569ms
2022-02-24 00:40:18 - until epoch: 036, best_acc1: 71.968%
2022-02-24 00:40:18 - epoch 037 lr: 0.010000000000000002
2022-02-24 00:40:58 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.1400
2022-02-24 00:41:31 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.0574
2022-02-24 00:42:04 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.3005
2022-02-24 00:42:38 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.5323
2022-02-24 00:43:11 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.2828
2022-02-24 00:43:44 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.4007
2022-02-24 00:44:18 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.3931
2022-02-24 00:44:51 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.2978
2022-02-24 00:45:25 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.5724
2022-02-24 00:45:59 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.2624
2022-02-24 00:46:32 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.3357
2022-02-24 00:47:06 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.4090
2022-02-24 00:47:39 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.4309
2022-02-24 00:48:13 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.3557
2022-02-24 00:48:46 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.1782
2022-02-24 00:49:20 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.1496
2022-02-24 00:49:53 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.5186
2022-02-24 00:50:27 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.4940
2022-02-24 00:51:00 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.4825
2022-02-24 00:51:34 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.3444
2022-02-24 00:52:08 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.3639
2022-02-24 00:52:41 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.3646
2022-02-24 00:53:15 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.2656
2022-02-24 00:53:48 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.3249
2022-02-24 00:54:21 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.3163
2022-02-24 00:54:55 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.5420
2022-02-24 00:55:29 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.4079
2022-02-24 00:56:03 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.4568
2022-02-24 00:56:36 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.4036
2022-02-24 00:57:11 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.5670
2022-02-24 00:57:44 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.2950
2022-02-24 00:58:18 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.4504
2022-02-24 00:58:52 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.2605
2022-02-24 00:59:26 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.2208
2022-02-24 01:00:00 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.3415
2022-02-24 01:00:33 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.4477
2022-02-24 01:01:08 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.4226
2022-02-24 01:01:42 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.3373
2022-02-24 01:02:16 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.5322
2022-02-24 01:02:51 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.3337
2022-02-24 01:03:25 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.3702
2022-02-24 01:04:01 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.5118
2022-02-24 01:04:35 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.4316
2022-02-24 01:05:12 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.3299
2022-02-24 01:05:51 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.2851
2022-02-24 01:06:27 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.4287
2022-02-24 01:07:12 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.3649
2022-02-24 01:07:57 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.3918
2022-02-24 01:08:38 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.4551
2022-02-24 01:09:27 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.3902
2022-02-24 01:09:30 - train: epoch 037, train_loss: 1.3587
2022-02-24 01:11:07 - eval: epoch: 037, acc1: 72.062%, acc5: 90.980%, test_loss: 1.1031, per_image_load_time: 2.931ms, per_image_inference_time: 0.582ms
2022-02-24 01:11:08 - until epoch: 037, best_acc1: 72.062%
2022-02-24 01:11:08 - epoch 038 lr: 0.010000000000000002
2022-02-24 01:11:47 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.3575
2022-02-24 01:12:20 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.1335
2022-02-24 01:12:54 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.1096
2022-02-24 01:13:27 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.2125
2022-02-24 01:14:01 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.2209
2022-02-24 01:14:34 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.4402
2022-02-24 01:15:07 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.2110
2022-02-24 01:15:40 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.2422
2022-02-24 01:16:14 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.2917
2022-02-24 01:16:48 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.4312
2022-02-24 01:17:21 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.2828
2022-02-24 01:17:54 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.4162
2022-02-24 01:18:28 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.4894
2022-02-24 01:19:01 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.4269
2022-02-24 01:19:35 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.3695
2022-02-24 01:20:09 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.4137
2022-02-24 01:20:42 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.5354
2022-02-24 01:21:15 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.4853
2022-02-24 01:21:49 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.3673
2022-02-24 01:22:23 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.3324
2022-02-24 01:22:56 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.3888
2022-02-24 01:23:30 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.3224
2022-02-24 01:24:04 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.4970
2022-02-24 01:24:37 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.5395
2022-02-24 01:25:11 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.2935
2022-02-24 01:25:44 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.3852
2022-02-24 01:26:18 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.4726
2022-02-24 01:26:52 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.5585
2022-02-24 01:27:25 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.4456
2022-02-24 01:27:59 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.3993
2022-02-24 01:28:33 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.3417
2022-02-24 01:29:07 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.0749
2022-02-24 01:29:41 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.1859
2022-02-24 01:30:16 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.2807
2022-02-24 01:30:49 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.4455
2022-02-24 01:31:24 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.4788
2022-02-24 01:31:57 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.1446
2022-02-24 01:32:33 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.4498
2022-02-24 01:33:06 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.4378
2022-02-24 01:33:42 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.3551
2022-02-24 01:34:16 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.2922
2022-02-24 01:34:51 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.2570
2022-02-24 01:35:26 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.4362
2022-02-24 01:36:02 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.4486
2022-02-24 01:36:41 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.3983
2022-02-24 01:37:18 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.5386
2022-02-24 01:38:02 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.2452
2022-02-24 01:38:49 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.4191
2022-02-24 01:39:31 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.3644
2022-02-24 01:40:14 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.1260
2022-02-24 01:40:17 - train: epoch 038, train_loss: 1.3535
2022-02-24 01:42:03 - eval: epoch: 038, acc1: 71.676%, acc5: 90.796%, test_loss: 1.1187, per_image_load_time: 3.092ms, per_image_inference_time: 0.550ms
2022-02-24 01:42:04 - until epoch: 038, best_acc1: 72.062%
2022-02-24 01:42:04 - epoch 039 lr: 0.010000000000000002
2022-02-24 01:42:45 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.3941
2022-02-24 01:43:18 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.5356
2022-02-24 01:43:52 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.2080
2022-02-24 01:44:25 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.3902
2022-02-24 01:44:59 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.3292
2022-02-24 01:45:32 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.2832
2022-02-24 01:46:06 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.5033
2022-02-24 01:46:39 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.3332
2022-02-24 01:47:12 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.3740
2022-02-24 01:47:46 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.2753
2022-02-24 01:48:19 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.4087
2022-02-24 01:48:53 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.4064
2022-02-24 01:49:26 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.5304
2022-02-24 01:50:00 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.4222
2022-02-24 01:50:33 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.3415
2022-02-24 01:51:07 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.5242
2022-02-24 01:51:41 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.1331
2022-02-24 01:52:14 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.4202
2022-02-24 01:52:48 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.1047
2022-02-24 01:53:21 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.2651
2022-02-24 01:53:55 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.3661
2022-02-24 01:54:28 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.2555
2022-02-24 01:55:02 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.5651
2022-02-24 01:55:36 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.4375
2022-02-24 01:56:10 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.2532
2022-02-24 01:56:44 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.3157
2022-02-24 01:57:17 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.5030
2022-02-24 01:57:52 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.1573
2022-02-24 01:58:25 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.2597
2022-02-24 01:58:59 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.4212
2022-02-24 01:59:33 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.1816
2022-02-24 02:00:07 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.4110
2022-02-24 02:00:41 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.3612
2022-02-24 02:01:15 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.4273
2022-02-24 02:01:50 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.5182
2022-02-24 02:02:24 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.4880
2022-02-24 02:02:59 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.3571
2022-02-24 02:03:32 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.2121
2022-02-24 02:04:07 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.3843
2022-02-24 02:04:41 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.3596
2022-02-24 02:05:16 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.4194
2022-02-24 02:05:50 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.3625
2022-02-24 02:06:26 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.3334
2022-02-24 02:07:01 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.1151
2022-02-24 02:07:39 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.1871
2022-02-24 02:08:22 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.4047
2022-02-24 02:09:03 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.3794
2022-02-24 02:09:42 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.3597
2022-02-24 02:10:30 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.2584
2022-02-24 02:11:17 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.2341
2022-02-24 02:11:19 - train: epoch 039, train_loss: 1.3505
2022-02-24 02:12:44 - eval: epoch: 039, acc1: 71.616%, acc5: 90.832%, test_loss: 1.1167, per_image_load_time: 2.364ms, per_image_inference_time: 0.566ms
2022-02-24 02:12:45 - until epoch: 039, best_acc1: 72.062%
2022-02-24 02:12:45 - epoch 040 lr: 0.010000000000000002
2022-02-24 02:13:25 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.5282
2022-02-24 02:13:58 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.5492
2022-02-24 02:14:31 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.4447
2022-02-24 02:15:05 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.2607
2022-02-24 02:15:38 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.2860
2022-02-24 02:16:11 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.5007
2022-02-24 02:16:45 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.4050
2022-02-24 02:17:18 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.4858
2022-02-24 02:17:51 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.2558
2022-02-24 02:18:25 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.0892
2022-02-24 02:18:59 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.3375
2022-02-24 02:19:32 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.2702
2022-02-24 02:20:06 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.2331
2022-02-24 02:20:39 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.2789
2022-02-24 02:21:13 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.5385
2022-02-24 02:21:46 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.3971
2022-02-24 02:22:20 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.3447
2022-02-24 02:22:53 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.1599
2022-02-24 02:23:27 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.3299
2022-02-24 02:24:00 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.3117
2022-02-24 02:24:34 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.2727
2022-02-24 02:25:07 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.2050
2022-02-24 02:25:41 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.2408
2022-02-24 02:26:15 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.3663
2022-02-24 02:26:49 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.4042
2022-02-24 02:27:22 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.1936
2022-02-24 02:27:56 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.4534
2022-02-24 02:28:30 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.3054
2022-02-24 02:29:03 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.5680
2022-02-24 02:29:37 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.5918
2022-02-24 02:30:11 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.2901
2022-02-24 02:30:45 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.4354
2022-02-24 02:31:19 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.4070
2022-02-24 02:31:53 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.4067
2022-02-24 02:32:27 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.4595
2022-02-24 02:33:01 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.4075
2022-02-24 02:33:36 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.2896
2022-02-24 02:34:10 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.1479
2022-02-24 02:34:44 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.4518
2022-02-24 02:35:19 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.3884
2022-02-24 02:35:54 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.3770
2022-02-24 02:36:30 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.2312
2022-02-24 02:37:06 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.3461
2022-02-24 02:37:41 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.3836
2022-02-24 02:38:20 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.2341
2022-02-24 02:39:03 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.3936
2022-02-24 02:39:48 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.3312
2022-02-24 02:40:37 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.2989
2022-02-24 02:41:28 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.4444
2022-02-24 02:42:21 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.4300
2022-02-24 02:42:24 - train: epoch 040, train_loss: 1.3497
2022-02-24 02:43:48 - eval: epoch: 040, acc1: 71.412%, acc5: 90.778%, test_loss: 1.1234, per_image_load_time: 1.317ms, per_image_inference_time: 0.617ms
2022-02-24 02:43:49 - until epoch: 040, best_acc1: 72.062%
2022-02-24 02:43:49 - epoch 041 lr: 0.010000000000000002
2022-02-24 02:44:29 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.4923
2022-02-24 02:45:03 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.4454
2022-02-24 02:45:36 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.3499
2022-02-24 02:46:09 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.3402
2022-02-24 02:46:43 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.1831
2022-02-24 02:47:16 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.3930
2022-02-24 02:47:49 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.2757
2022-02-24 02:48:23 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.1467
2022-02-24 02:48:56 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.1764
2022-02-24 02:49:29 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.4989
2022-02-24 02:50:03 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.2305
2022-02-24 02:50:36 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.1035
2022-02-24 02:51:10 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.3770
2022-02-24 02:51:43 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.3756
2022-02-24 02:52:17 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.6579
2022-02-24 02:52:51 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.2060
2022-02-24 02:53:24 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.3604
2022-02-24 02:53:58 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.2964
2022-02-24 02:54:31 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.3542
2022-02-24 02:55:05 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.2862
2022-02-24 02:55:39 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.1871
2022-02-24 02:56:12 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.2860
2022-02-24 02:56:46 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.1896
2022-02-24 02:57:20 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.5017
2022-02-24 02:57:54 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.2512
2022-02-24 02:58:27 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.5272
2022-02-24 02:59:01 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.4832
2022-02-24 02:59:35 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.3461
2022-02-24 03:00:10 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.4467
2022-02-24 03:00:44 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.4373
2022-02-24 03:01:17 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.5007
2022-02-24 03:01:52 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.2817
2022-02-24 03:02:26 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.2504
2022-02-24 03:03:00 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.2580
2022-02-24 03:03:35 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.4455
2022-02-24 03:04:09 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.3350
2022-02-24 03:04:43 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.3650
2022-02-24 03:05:17 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.3146
2022-02-24 03:05:51 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.3816
2022-02-24 03:06:26 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.3048
2022-02-24 03:07:01 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.3295
2022-02-24 03:07:36 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.3190
2022-02-24 03:08:12 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.5138
2022-02-24 03:08:46 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.3076
2022-02-24 03:09:25 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.3907
2022-02-24 03:10:07 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.4265
2022-02-24 03:10:50 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.5359
2022-02-24 03:11:32 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.2818
2022-02-24 03:12:18 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.3556
2022-02-24 03:13:09 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.4740
2022-02-24 03:13:12 - train: epoch 041, train_loss: 1.3516
2022-02-24 03:14:40 - eval: epoch: 041, acc1: 71.382%, acc5: 90.766%, test_loss: 1.1284, per_image_load_time: 0.710ms, per_image_inference_time: 0.597ms
2022-02-24 03:14:41 - until epoch: 041, best_acc1: 72.062%
2022-02-24 03:14:41 - epoch 042 lr: 0.010000000000000002
2022-02-24 03:15:22 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.1316
2022-02-24 03:15:55 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.3001
2022-02-24 03:16:29 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.4683
2022-02-24 03:17:03 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.0780
2022-02-24 03:17:37 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.4837
2022-02-24 03:18:10 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.2479
2022-02-24 03:18:44 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.3717
2022-02-24 03:19:18 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.2638
2022-02-24 03:19:51 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.5161
2022-02-24 03:20:25 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.3988
2022-02-24 03:20:58 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.1680
2022-02-24 03:21:32 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.3261
2022-02-24 03:22:06 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.3815
2022-02-24 03:22:40 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.5926
2022-02-24 03:23:13 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.3284
2022-02-24 03:23:47 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.4956
2022-02-24 03:24:21 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.3151
2022-02-24 03:24:55 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.3005
2022-02-24 03:25:29 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.5021
2022-02-24 03:26:03 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.2937
2022-02-24 03:26:36 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.0639
2022-02-24 03:27:11 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.2366
2022-02-24 03:27:44 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.2616
2022-02-24 03:28:18 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.6501
2022-02-24 03:28:52 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.3973
2022-02-24 03:29:26 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.4677
2022-02-24 03:30:00 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.2255
2022-02-24 03:30:34 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.2904
2022-02-24 03:31:08 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.3495
2022-02-24 03:31:42 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.3713
2022-02-24 03:32:16 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.3034
2022-02-24 03:32:50 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.3910
2022-02-24 03:33:24 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.4854
2022-02-24 03:33:58 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.2566
2022-02-24 03:34:33 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.3383
2022-02-24 03:35:08 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.5250
2022-02-24 03:35:42 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.3569
2022-02-24 03:36:17 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.1996
2022-02-24 03:36:52 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.2341
2022-02-24 03:37:27 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.4388
2022-02-24 03:38:02 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.5308
2022-02-24 03:38:38 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.3949
2022-02-24 03:39:14 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.2519
2022-02-24 03:39:49 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.2305
2022-02-24 03:40:29 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.3193
2022-02-24 03:41:07 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.5534
2022-02-24 03:41:47 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.1955
2022-02-24 03:42:33 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.4508
2022-02-24 03:43:20 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.3904
2022-02-24 03:43:59 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.3068
2022-02-24 03:44:04 - train: epoch 042, train_loss: 1.3509
2022-02-24 03:45:34 - eval: epoch: 042, acc1: 71.212%, acc5: 90.648%, test_loss: 1.1360, per_image_load_time: 2.834ms, per_image_inference_time: 0.587ms
2022-02-24 03:45:34 - until epoch: 042, best_acc1: 72.062%
2022-02-24 03:45:34 - epoch 043 lr: 0.010000000000000002
2022-02-24 03:46:15 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.2775
2022-02-24 03:46:48 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.3859
2022-02-24 03:47:22 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.0607
2022-02-24 03:47:55 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.2850
2022-02-24 03:48:28 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.4116
2022-02-24 03:49:02 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.2431
2022-02-24 03:49:35 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.4820
2022-02-24 03:50:09 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.4725
2022-02-24 03:50:42 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.2623
2022-02-24 03:51:15 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.5807
2022-02-24 03:51:49 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.4371
2022-02-24 03:52:22 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.4500
2022-02-24 03:52:55 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.4550
2022-02-24 03:53:29 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.3587
2022-02-24 03:54:03 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.1982
2022-02-24 03:54:36 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.3606
2022-02-24 03:55:10 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.4938
2022-02-24 03:55:43 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.3272
2022-02-24 03:56:17 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.3699
2022-02-24 03:56:50 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.2075
2022-02-24 03:57:24 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.2277
2022-02-24 03:57:57 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.4016
2022-02-24 03:58:31 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.5532
2022-02-24 03:59:05 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.2126
2022-02-24 03:59:39 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.5246
2022-02-24 04:00:13 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.1724
2022-02-24 04:00:47 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.4411
2022-02-24 04:01:21 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.2539
2022-02-24 04:01:54 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.3456
2022-02-24 04:02:29 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.6082
2022-02-24 04:03:02 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.5152
2022-02-24 04:03:37 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.4340
2022-02-24 04:04:10 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.4775
2022-02-24 04:04:45 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.3850
2022-02-24 04:05:19 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.4081
2022-02-24 04:05:53 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.2970
2022-02-24 04:06:27 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.3990
2022-02-24 04:07:01 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.4907
2022-02-24 04:07:36 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.4869
2022-02-24 04:08:10 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.4980
2022-02-24 04:08:44 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.5866
2022-02-24 04:09:18 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.3880
2022-02-24 04:09:55 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.3924
2022-02-24 04:10:29 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.3254
2022-02-24 04:11:06 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.3845
2022-02-24 04:11:50 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.3236
2022-02-24 04:12:35 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.2154
2022-02-24 04:13:14 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.2485
2022-02-24 04:13:58 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.3336
2022-02-24 04:14:46 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.2713
2022-02-24 04:14:49 - train: epoch 043, train_loss: 1.3481
2022-02-24 04:16:11 - eval: epoch: 043, acc1: 71.390%, acc5: 90.708%, test_loss: 1.1321, per_image_load_time: 0.857ms, per_image_inference_time: 0.625ms
2022-02-24 04:16:12 - until epoch: 043, best_acc1: 72.062%
2022-02-24 04:16:12 - epoch 044 lr: 0.010000000000000002
2022-02-24 04:16:52 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.3690
2022-02-24 04:17:25 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.3938
2022-02-24 04:17:58 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.3611
2022-02-24 04:18:32 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.0737
2022-02-24 04:19:05 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.2818
2022-02-24 04:19:38 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.1103
2022-02-24 04:20:11 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.0674
2022-02-24 04:20:45 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.3084
2022-02-24 04:21:18 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.3485
2022-02-24 04:21:52 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.3390
2022-02-24 04:22:25 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.2910
2022-02-24 04:22:59 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.2161
2022-02-24 04:23:32 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.4752
2022-02-24 04:24:06 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.2820
2022-02-24 04:24:40 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.3332
2022-02-24 04:25:13 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.1955
2022-02-24 04:25:47 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.2738
2022-02-24 04:26:20 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.2405
2022-02-24 04:26:54 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.5180
2022-02-24 04:27:28 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.3356
2022-02-24 04:28:01 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.4360
2022-02-24 04:28:35 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.3147
2022-02-24 04:29:09 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.4096
2022-02-24 04:29:43 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.4824
2022-02-24 04:30:17 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.4425
2022-02-24 04:30:51 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.4164
2022-02-24 04:31:24 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.5300
2022-02-24 04:31:58 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.1037
2022-02-24 04:32:32 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.1239
2022-02-24 04:33:06 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.2849
2022-02-24 04:33:40 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.4274
2022-02-24 04:34:14 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.0597
2022-02-24 04:34:48 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.2937
2022-02-24 04:35:22 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.5177
2022-02-24 04:35:57 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.2238
2022-02-24 04:36:31 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.4309
2022-02-24 04:37:06 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.4313
2022-02-24 04:37:40 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.0965
2022-02-24 04:38:15 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.4049
2022-02-24 04:38:49 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.5413
2022-02-24 04:39:24 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.2821
2022-02-24 04:40:00 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.3204
2022-02-24 04:40:34 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.5513
2022-02-24 04:41:11 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.2339
2022-02-24 04:41:46 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.4338
2022-02-24 04:42:27 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.2678
2022-02-24 04:43:09 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.3977
2022-02-24 04:43:46 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.5651
2022-02-24 04:44:35 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.1458
2022-02-24 04:45:27 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.3068
2022-02-24 04:45:30 - train: epoch 044, train_loss: 1.3500
2022-02-24 04:46:51 - eval: epoch: 044, acc1: 71.498%, acc5: 90.772%, test_loss: 1.1280, per_image_load_time: 0.813ms, per_image_inference_time: 0.604ms
2022-02-24 04:46:52 - until epoch: 044, best_acc1: 72.062%
2022-02-24 04:46:52 - epoch 045 lr: 0.010000000000000002
2022-02-24 04:47:33 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.1814
2022-02-24 04:48:06 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.4469
2022-02-24 04:48:39 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.4457
2022-02-24 04:49:13 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.1699
2022-02-24 04:49:46 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.4548
2022-02-24 04:50:20 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.3523
2022-02-24 04:50:54 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.1204
2022-02-24 04:51:27 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.3696
2022-02-24 04:52:01 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.3218
2022-02-24 04:52:34 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.3728
2022-02-24 04:53:08 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.4516
2022-02-24 04:53:41 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.3437
2022-02-24 04:54:15 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.5180
2022-02-24 04:54:49 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.3126
2022-02-24 04:55:22 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.4654
2022-02-24 04:55:55 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.3728
2022-02-24 04:56:29 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.2983
2022-02-24 04:57:02 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.1704
2022-02-24 04:57:36 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.3680
2022-02-24 04:58:09 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.4533
2022-02-24 04:58:43 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.4574
2022-02-24 04:59:17 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.3609
2022-02-24 04:59:51 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.2988
2022-02-24 05:00:25 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.5209
2022-02-24 05:00:58 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.3390
2022-02-24 05:01:33 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.3317
2022-02-24 05:02:07 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.2064
2022-02-24 05:02:40 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.3286
2022-02-24 05:03:14 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.4725
2022-02-24 05:03:48 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.4900
2022-02-24 05:04:21 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.3369
2022-02-24 05:04:55 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.5077
2022-02-24 05:05:29 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.2953
2022-02-24 05:06:03 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.1962
2022-02-24 05:06:38 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.5583
2022-02-24 05:07:12 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.3860
2022-02-24 05:07:46 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.3342
2022-02-24 05:08:20 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.2435
2022-02-24 05:08:54 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.4062
2022-02-24 05:09:29 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.4597
2022-02-24 05:10:03 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.3317
2022-02-24 05:10:38 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.4071
2022-02-24 05:11:13 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.5251
2022-02-24 05:11:49 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.4906
2022-02-24 05:12:28 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.2122
2022-02-24 05:13:10 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.5228
2022-02-24 05:13:48 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.4016
2022-02-24 05:14:31 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.1991
2022-02-24 05:15:20 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.2973
2022-02-24 05:16:12 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.4169
2022-02-24 05:16:15 - train: epoch 045, train_loss: 1.3472
2022-02-24 05:17:44 - eval: epoch: 045, acc1: 71.186%, acc5: 90.624%, test_loss: 1.1347, per_image_load_time: 0.872ms, per_image_inference_time: 0.603ms
2022-02-24 05:17:45 - until epoch: 045, best_acc1: 72.062%
2022-02-24 05:17:45 - epoch 046 lr: 0.010000000000000002
2022-02-24 05:18:25 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.0887
2022-02-24 05:18:58 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.2647
2022-02-24 05:19:32 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.3658
2022-02-24 05:20:05 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.3868
2022-02-24 05:20:38 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.0482
2022-02-24 05:21:11 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.4206
2022-02-24 05:21:44 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.2738
2022-02-24 05:22:18 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.4127
2022-02-24 05:22:51 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.3482
2022-02-24 05:23:25 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.1563
2022-02-24 05:23:58 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.2530
2022-02-24 05:24:31 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.3129
2022-02-24 05:25:05 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.3451
2022-02-24 05:25:38 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.5368
2022-02-24 05:26:12 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.2778
2022-02-24 05:26:45 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.3808
2022-02-24 05:27:19 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.1720
2022-02-24 05:27:52 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.3840
2022-02-24 05:28:25 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.1726
2022-02-24 05:28:59 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.3460
2022-02-24 05:29:32 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.4153
2022-02-24 05:30:06 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.1711
2022-02-24 05:30:40 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.4692
2022-02-24 05:31:13 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.3160
2022-02-24 05:31:46 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.4168
2022-02-24 05:32:20 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.4753
2022-02-24 05:32:54 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.2423
2022-02-24 05:33:28 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.1337
2022-02-24 05:34:01 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.4438
2022-02-24 05:34:35 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.1719
2022-02-24 05:35:09 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.2317
2022-02-24 05:35:42 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.5794
2022-02-24 05:36:17 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.3523
2022-02-24 05:36:50 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.4097
2022-02-24 05:37:24 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.2943
2022-02-24 05:37:59 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.2153
2022-02-24 05:38:33 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.2293
2022-02-24 05:39:07 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.4151
2022-02-24 05:39:41 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.3620
2022-02-24 05:40:15 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.2891
2022-02-24 05:40:51 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.4931
2022-02-24 05:41:25 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.2134
2022-02-24 05:42:01 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.5041
2022-02-24 05:42:35 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.3657
2022-02-24 05:43:13 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.2290
2022-02-24 05:43:53 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.3722
2022-02-24 05:44:31 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.2742
2022-02-24 05:45:18 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.2904
2022-02-24 05:46:07 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.3422
2022-02-24 05:46:56 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.3385
2022-02-24 05:46:59 - train: epoch 046, train_loss: 1.3471
2022-02-24 05:48:21 - eval: epoch: 046, acc1: 71.260%, acc5: 90.724%, test_loss: 1.1357, per_image_load_time: 2.519ms, per_image_inference_time: 0.578ms
2022-02-24 05:48:22 - until epoch: 046, best_acc1: 72.062%
2022-02-24 05:48:22 - epoch 047 lr: 0.010000000000000002
2022-02-24 05:49:03 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.2692
2022-02-24 05:49:36 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.4271
2022-02-24 05:50:09 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.2279
2022-02-24 05:50:43 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.2535
2022-02-24 05:51:16 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.3838
2022-02-24 05:51:50 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.3354
2022-02-24 05:52:23 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.5211
2022-02-24 05:52:57 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.2221
2022-02-24 05:53:30 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.4850
2022-02-24 05:54:04 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.3784
2022-02-24 05:54:37 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.3784
2022-02-24 05:55:11 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.2638
2022-02-24 05:55:45 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.3859
2022-02-24 05:56:18 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.3580
2022-02-24 05:56:52 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.2710
2022-02-24 05:57:26 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.2302
2022-02-24 05:58:00 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.2119
2022-02-24 05:58:33 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.4021
2022-02-24 05:59:07 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.2324
2022-02-24 05:59:40 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.3538
2022-02-24 06:00:14 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.5413
2022-02-24 06:00:48 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.5534
2022-02-24 06:01:21 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.3590
2022-02-24 06:01:56 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.1633
2022-02-24 06:02:29 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.2842
2022-02-24 06:03:03 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.5551
2022-02-24 06:03:37 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.3780
2022-02-24 06:04:11 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.5143
2022-02-24 06:04:45 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.3851
2022-02-24 06:05:18 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.3635
2022-02-24 06:05:53 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.2706
2022-02-24 06:06:27 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.5844
2022-02-24 06:07:01 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.3097
2022-02-24 06:07:35 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.3466
2022-02-24 06:08:09 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.4659
2022-02-24 06:08:44 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.4537
2022-02-24 06:09:18 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.5543
