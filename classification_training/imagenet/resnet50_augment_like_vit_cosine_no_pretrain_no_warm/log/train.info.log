2022-10-11 00:00:06 - train: epoch 0029, iter [04200, 05004], lr: 0.080846, loss: 3.7783
2022-10-11 00:00:43 - train: epoch 0029, iter [04300, 05004], lr: 0.080822, loss: 4.2342
2022-10-11 00:01:19 - train: epoch 0029, iter [04400, 05004], lr: 0.080797, loss: 4.5275
2022-10-11 00:01:57 - train: epoch 0029, iter [04500, 05004], lr: 0.080772, loss: 4.3119
2022-10-11 00:02:34 - train: epoch 0029, iter [04600, 05004], lr: 0.080747, loss: 4.4592
2022-10-11 00:03:10 - train: epoch 0029, iter [04700, 05004], lr: 0.080723, loss: 4.1184
2022-10-11 00:03:47 - train: epoch 0029, iter [04800, 05004], lr: 0.080698, loss: 5.1674
2022-10-11 00:04:24 - train: epoch 0029, iter [04900, 05004], lr: 0.080673, loss: 4.5793
2022-10-11 00:04:59 - train: epoch 0029, iter [05000, 05004], lr: 0.080648, loss: 3.8456
2022-10-11 00:05:01 - train: epoch 029, train_loss: 4.5168
2022-10-11 00:06:20 - eval: epoch: 029, acc1: 52.522%, acc5: 78.244%, test_loss: 2.2292, per_image_load_time: 2.287ms, per_image_inference_time: 0.537ms
2022-10-11 00:06:20 - until epoch: 029, best_acc1: 55.340%
2022-10-11 00:06:20 - epoch 030 lr: 0.080647
2022-10-11 00:07:03 - train: epoch 0030, iter [00100, 05004], lr: 0.080622, loss: 4.4120
2022-10-11 00:07:39 - train: epoch 0030, iter [00200, 05004], lr: 0.080598, loss: 4.8835
2022-10-11 00:08:16 - train: epoch 0030, iter [00300, 05004], lr: 0.080573, loss: 4.1113
2022-10-11 00:08:52 - train: epoch 0030, iter [00400, 05004], lr: 0.080548, loss: 5.1960
2022-10-11 00:09:29 - train: epoch 0030, iter [00500, 05004], lr: 0.080523, loss: 4.5199
2022-10-11 00:10:05 - train: epoch 0030, iter [00600, 05004], lr: 0.080498, loss: 5.2931
2022-10-11 00:10:41 - train: epoch 0030, iter [00700, 05004], lr: 0.080473, loss: 4.7925
2022-10-11 00:11:18 - train: epoch 0030, iter [00800, 05004], lr: 0.080448, loss: 4.8125
2022-10-11 00:11:54 - train: epoch 0030, iter [00900, 05004], lr: 0.080424, loss: 4.9738
2022-10-11 00:12:31 - train: epoch 0030, iter [01000, 05004], lr: 0.080399, loss: 4.5288
2022-10-11 00:13:07 - train: epoch 0030, iter [01100, 05004], lr: 0.080374, loss: 4.7235
2022-10-11 00:13:44 - train: epoch 0030, iter [01200, 05004], lr: 0.080349, loss: 4.7739
2022-10-11 00:14:22 - train: epoch 0030, iter [01300, 05004], lr: 0.080324, loss: 4.9684
2022-10-11 00:14:58 - train: epoch 0030, iter [01400, 05004], lr: 0.080299, loss: 4.4761
2022-10-11 00:15:35 - train: epoch 0030, iter [01500, 05004], lr: 0.080274, loss: 4.2811
2022-10-11 00:16:12 - train: epoch 0030, iter [01600, 05004], lr: 0.080249, loss: 4.1236
2022-10-11 00:16:49 - train: epoch 0030, iter [01700, 05004], lr: 0.080224, loss: 4.8341
2022-10-11 00:17:25 - train: epoch 0030, iter [01800, 05004], lr: 0.080199, loss: 5.0602
2022-10-11 00:18:02 - train: epoch 0030, iter [01900, 05004], lr: 0.080174, loss: 4.4450
2022-10-11 00:18:38 - train: epoch 0030, iter [02000, 05004], lr: 0.080149, loss: 4.7696
2022-10-11 00:19:15 - train: epoch 0030, iter [02100, 05004], lr: 0.080124, loss: 4.5149
2022-10-11 00:19:51 - train: epoch 0030, iter [02200, 05004], lr: 0.080099, loss: 4.8304
2022-10-11 00:20:28 - train: epoch 0030, iter [02300, 05004], lr: 0.080074, loss: 4.3738
2022-10-11 00:21:06 - train: epoch 0030, iter [02400, 05004], lr: 0.080049, loss: 4.8696
2022-10-11 00:21:42 - train: epoch 0030, iter [02500, 05004], lr: 0.080024, loss: 4.8821
2022-10-11 00:22:20 - train: epoch 0030, iter [02600, 05004], lr: 0.079998, loss: 4.3617
2022-10-11 00:22:56 - train: epoch 0030, iter [02700, 05004], lr: 0.079973, loss: 4.5000
2022-10-11 00:23:34 - train: epoch 0030, iter [02800, 05004], lr: 0.079948, loss: 4.7348
2022-10-11 00:24:11 - train: epoch 0030, iter [02900, 05004], lr: 0.079923, loss: 4.8749
2022-10-11 00:24:48 - train: epoch 0030, iter [03000, 05004], lr: 0.079898, loss: 4.8696
2022-10-11 00:25:26 - train: epoch 0030, iter [03100, 05004], lr: 0.079873, loss: 4.8738
2022-10-11 00:26:02 - train: epoch 0030, iter [03200, 05004], lr: 0.079848, loss: 4.7186
2022-10-11 00:26:40 - train: epoch 0030, iter [03300, 05004], lr: 0.079822, loss: 5.0482
2022-10-11 00:27:16 - train: epoch 0030, iter [03400, 05004], lr: 0.079797, loss: 5.0258
2022-10-11 00:27:54 - train: epoch 0030, iter [03500, 05004], lr: 0.079772, loss: 4.4138
2022-10-11 00:28:31 - train: epoch 0030, iter [03600, 05004], lr: 0.079747, loss: 4.5639
2022-10-11 00:29:08 - train: epoch 0030, iter [03700, 05004], lr: 0.079721, loss: 4.7277
2022-10-11 00:29:45 - train: epoch 0030, iter [03800, 05004], lr: 0.079696, loss: 4.7237
2022-10-11 00:30:23 - train: epoch 0030, iter [03900, 05004], lr: 0.079671, loss: 4.6126
2022-10-11 00:30:59 - train: epoch 0030, iter [04000, 05004], lr: 0.079646, loss: 4.9602
2022-10-11 00:31:36 - train: epoch 0030, iter [04100, 05004], lr: 0.079620, loss: 4.0236
2022-10-11 00:32:13 - train: epoch 0030, iter [04200, 05004], lr: 0.079595, loss: 4.6382
2022-10-11 00:32:51 - train: epoch 0030, iter [04300, 05004], lr: 0.079570, loss: 5.2117
2022-10-11 00:33:27 - train: epoch 0030, iter [04400, 05004], lr: 0.079544, loss: 4.5231
2022-10-11 00:34:05 - train: epoch 0030, iter [04500, 05004], lr: 0.079519, loss: 4.2842
2022-10-11 00:34:42 - train: epoch 0030, iter [04600, 05004], lr: 0.079494, loss: 4.1446
2022-10-11 00:35:19 - train: epoch 0030, iter [04700, 05004], lr: 0.079468, loss: 5.1245
2022-10-11 00:35:56 - train: epoch 0030, iter [04800, 05004], lr: 0.079443, loss: 4.7987
2022-10-11 00:36:33 - train: epoch 0030, iter [04900, 05004], lr: 0.079418, loss: 4.5928
2022-10-11 00:37:08 - train: epoch 0030, iter [05000, 05004], lr: 0.079392, loss: 4.2358
2022-10-11 00:37:10 - train: epoch 030, train_loss: 4.5126
2022-10-11 00:38:30 - eval: epoch: 030, acc1: 55.260%, acc5: 80.460%, test_loss: 2.1418, per_image_load_time: 2.331ms, per_image_inference_time: 0.518ms
2022-10-11 00:38:30 - until epoch: 030, best_acc1: 55.340%
2022-10-11 00:38:30 - epoch 031 lr: 0.079391
2022-10-11 00:39:15 - train: epoch 0031, iter [00100, 05004], lr: 0.079366, loss: 4.8018
2022-10-11 00:39:51 - train: epoch 0031, iter [00200, 05004], lr: 0.079341, loss: 4.2469
2022-10-11 00:40:27 - train: epoch 0031, iter [00300, 05004], lr: 0.079315, loss: 4.3359
2022-10-11 00:41:02 - train: epoch 0031, iter [00400, 05004], lr: 0.079290, loss: 4.6545
2022-10-11 00:41:39 - train: epoch 0031, iter [00500, 05004], lr: 0.079264, loss: 4.8162
2022-10-11 00:42:15 - train: epoch 0031, iter [00600, 05004], lr: 0.079239, loss: 4.0912
2022-10-11 00:42:52 - train: epoch 0031, iter [00700, 05004], lr: 0.079213, loss: 3.4544
2022-10-11 00:43:29 - train: epoch 0031, iter [00800, 05004], lr: 0.079188, loss: 4.0455
2022-10-11 00:44:05 - train: epoch 0031, iter [00900, 05004], lr: 0.079162, loss: 3.9913
2022-10-11 00:44:42 - train: epoch 0031, iter [01000, 05004], lr: 0.079137, loss: 4.4237
2022-10-11 00:45:19 - train: epoch 0031, iter [01100, 05004], lr: 0.079111, loss: 4.9464
2022-10-11 00:45:55 - train: epoch 0031, iter [01200, 05004], lr: 0.079086, loss: 4.7190
2022-10-11 00:46:33 - train: epoch 0031, iter [01300, 05004], lr: 0.079060, loss: 4.0094
2022-10-11 00:47:09 - train: epoch 0031, iter [01400, 05004], lr: 0.079035, loss: 4.5550
2022-10-11 00:47:46 - train: epoch 0031, iter [01500, 05004], lr: 0.079009, loss: 5.3380
2022-10-11 00:48:23 - train: epoch 0031, iter [01600, 05004], lr: 0.078984, loss: 4.4519
2022-10-11 00:48:59 - train: epoch 0031, iter [01700, 05004], lr: 0.078958, loss: 3.8895
2022-10-11 00:49:36 - train: epoch 0031, iter [01800, 05004], lr: 0.078932, loss: 5.3441
2022-10-11 00:50:13 - train: epoch 0031, iter [01900, 05004], lr: 0.078907, loss: 4.5688
2022-10-11 00:50:49 - train: epoch 0031, iter [02000, 05004], lr: 0.078881, loss: 4.4575
2022-10-11 00:51:26 - train: epoch 0031, iter [02100, 05004], lr: 0.078856, loss: 4.7944
2022-10-11 00:52:03 - train: epoch 0031, iter [02200, 05004], lr: 0.078830, loss: 4.7749
2022-10-11 00:52:40 - train: epoch 0031, iter [02300, 05004], lr: 0.078804, loss: 4.6490
2022-10-11 00:53:17 - train: epoch 0031, iter [02400, 05004], lr: 0.078779, loss: 4.9658
2022-10-11 00:53:54 - train: epoch 0031, iter [02500, 05004], lr: 0.078753, loss: 3.8822
2022-10-11 00:54:30 - train: epoch 0031, iter [02600, 05004], lr: 0.078727, loss: 4.6272
2022-10-11 00:55:07 - train: epoch 0031, iter [02700, 05004], lr: 0.078702, loss: 4.7787
2022-10-11 00:55:44 - train: epoch 0031, iter [02800, 05004], lr: 0.078676, loss: 4.3917
2022-10-11 00:56:21 - train: epoch 0031, iter [02900, 05004], lr: 0.078650, loss: 4.1831
2022-10-11 00:56:58 - train: epoch 0031, iter [03000, 05004], lr: 0.078624, loss: 4.3029
2022-10-11 00:57:34 - train: epoch 0031, iter [03100, 05004], lr: 0.078599, loss: 4.2891
2022-10-11 00:58:11 - train: epoch 0031, iter [03200, 05004], lr: 0.078573, loss: 4.4819
2022-10-11 00:58:48 - train: epoch 0031, iter [03300, 05004], lr: 0.078547, loss: 4.0672
2022-10-11 00:59:25 - train: epoch 0031, iter [03400, 05004], lr: 0.078521, loss: 4.4025
2022-10-11 01:00:02 - train: epoch 0031, iter [03500, 05004], lr: 0.078496, loss: 4.6014
2022-10-11 01:00:38 - train: epoch 0031, iter [03600, 05004], lr: 0.078470, loss: 4.6614
2022-10-11 01:01:16 - train: epoch 0031, iter [03700, 05004], lr: 0.078444, loss: 4.5969
2022-10-11 01:01:53 - train: epoch 0031, iter [03800, 05004], lr: 0.078418, loss: 4.6740
2022-10-11 01:02:30 - train: epoch 0031, iter [03900, 05004], lr: 0.078392, loss: 4.9587
2022-10-11 01:03:06 - train: epoch 0031, iter [04000, 05004], lr: 0.078366, loss: 4.5124
2022-10-11 01:03:44 - train: epoch 0031, iter [04100, 05004], lr: 0.078341, loss: 4.4521
2022-10-11 01:04:20 - train: epoch 0031, iter [04200, 05004], lr: 0.078315, loss: 4.5187
2022-10-11 01:04:58 - train: epoch 0031, iter [04300, 05004], lr: 0.078289, loss: 5.1668
2022-10-11 01:05:35 - train: epoch 0031, iter [04400, 05004], lr: 0.078263, loss: 4.5540
2022-10-11 01:06:12 - train: epoch 0031, iter [04500, 05004], lr: 0.078237, loss: 4.9537
2022-10-11 01:06:48 - train: epoch 0031, iter [04600, 05004], lr: 0.078211, loss: 3.8805
2022-10-11 01:07:25 - train: epoch 0031, iter [04700, 05004], lr: 0.078185, loss: 5.3465
2022-10-11 01:08:02 - train: epoch 0031, iter [04800, 05004], lr: 0.078159, loss: 4.7974
2022-10-11 01:08:39 - train: epoch 0031, iter [04900, 05004], lr: 0.078133, loss: 4.3003
2022-10-11 01:09:14 - train: epoch 0031, iter [05000, 05004], lr: 0.078107, loss: 4.6580
2022-10-11 01:09:16 - train: epoch 031, train_loss: 4.5029
2022-10-11 01:10:34 - eval: epoch: 031, acc1: 56.140%, acc5: 80.958%, test_loss: 2.1232, per_image_load_time: 1.843ms, per_image_inference_time: 0.558ms
2022-10-11 01:10:34 - until epoch: 031, best_acc1: 56.140%
2022-10-11 01:10:34 - epoch 032 lr: 0.078106
2022-10-11 01:11:17 - train: epoch 0032, iter [00100, 05004], lr: 0.078080, loss: 4.1398
2022-10-11 01:11:53 - train: epoch 0032, iter [00200, 05004], lr: 0.078054, loss: 4.7628
2022-10-11 01:12:30 - train: epoch 0032, iter [00300, 05004], lr: 0.078028, loss: 4.6344
2022-10-11 01:13:06 - train: epoch 0032, iter [00400, 05004], lr: 0.078002, loss: 4.2538
2022-10-11 01:13:43 - train: epoch 0032, iter [00500, 05004], lr: 0.077976, loss: 4.3300
2022-10-11 01:14:19 - train: epoch 0032, iter [00600, 05004], lr: 0.077950, loss: 4.9091
2022-10-11 01:14:56 - train: epoch 0032, iter [00700, 05004], lr: 0.077924, loss: 4.7332
2022-10-11 01:15:32 - train: epoch 0032, iter [00800, 05004], lr: 0.077898, loss: 4.8665
2022-10-11 01:16:10 - train: epoch 0032, iter [00900, 05004], lr: 0.077872, loss: 4.3479
2022-10-11 01:16:46 - train: epoch 0032, iter [01000, 05004], lr: 0.077846, loss: 4.7695
2022-10-11 01:17:22 - train: epoch 0032, iter [01100, 05004], lr: 0.077820, loss: 4.7054
2022-10-11 01:17:59 - train: epoch 0032, iter [01200, 05004], lr: 0.077794, loss: 4.2517
2022-10-11 01:18:36 - train: epoch 0032, iter [01300, 05004], lr: 0.077768, loss: 4.0464
2022-10-11 01:19:13 - train: epoch 0032, iter [01400, 05004], lr: 0.077742, loss: 4.2731
2022-10-11 01:19:49 - train: epoch 0032, iter [01500, 05004], lr: 0.077716, loss: 4.8096
2022-10-11 01:20:26 - train: epoch 0032, iter [01600, 05004], lr: 0.077690, loss: 4.4414
2022-10-11 01:21:03 - train: epoch 0032, iter [01700, 05004], lr: 0.077663, loss: 4.4314
2022-10-11 01:21:41 - train: epoch 0032, iter [01800, 05004], lr: 0.077637, loss: 4.3789
2022-10-11 01:22:18 - train: epoch 0032, iter [01900, 05004], lr: 0.077611, loss: 4.6731
2022-10-11 01:22:54 - train: epoch 0032, iter [02000, 05004], lr: 0.077585, loss: 3.8588
2022-10-11 01:23:30 - train: epoch 0032, iter [02100, 05004], lr: 0.077559, loss: 3.9864
2022-10-11 01:24:06 - train: epoch 0032, iter [02200, 05004], lr: 0.077533, loss: 4.9731
2022-10-11 01:24:44 - train: epoch 0032, iter [02300, 05004], lr: 0.077506, loss: 4.5655
2022-10-11 01:25:21 - train: epoch 0032, iter [02400, 05004], lr: 0.077480, loss: 4.1520
2022-10-11 01:25:58 - train: epoch 0032, iter [02500, 05004], lr: 0.077454, loss: 4.0609
2022-10-11 01:26:34 - train: epoch 0032, iter [02600, 05004], lr: 0.077428, loss: 4.9061
2022-10-11 01:27:11 - train: epoch 0032, iter [02700, 05004], lr: 0.077401, loss: 4.1468
2022-10-11 01:27:48 - train: epoch 0032, iter [02800, 05004], lr: 0.077375, loss: 4.2666
2022-10-11 01:28:25 - train: epoch 0032, iter [02900, 05004], lr: 0.077349, loss: 4.9201
2022-10-11 01:29:02 - train: epoch 0032, iter [03000, 05004], lr: 0.077323, loss: 4.3913
2022-10-11 01:29:39 - train: epoch 0032, iter [03100, 05004], lr: 0.077296, loss: 3.6001
2022-10-11 01:30:15 - train: epoch 0032, iter [03200, 05004], lr: 0.077270, loss: 4.8995
2022-10-11 01:30:53 - train: epoch 0032, iter [03300, 05004], lr: 0.077244, loss: 4.8951
2022-10-11 01:31:29 - train: epoch 0032, iter [03400, 05004], lr: 0.077217, loss: 3.9875
2022-10-11 01:32:06 - train: epoch 0032, iter [03500, 05004], lr: 0.077191, loss: 4.4983
2022-10-11 01:32:43 - train: epoch 0032, iter [03600, 05004], lr: 0.077165, loss: 5.0599
2022-10-11 01:33:20 - train: epoch 0032, iter [03700, 05004], lr: 0.077138, loss: 3.7717
2022-10-11 01:33:57 - train: epoch 0032, iter [03800, 05004], lr: 0.077112, loss: 4.5703
2022-10-11 01:34:33 - train: epoch 0032, iter [03900, 05004], lr: 0.077086, loss: 4.5523
2022-10-11 01:35:11 - train: epoch 0032, iter [04000, 05004], lr: 0.077059, loss: 3.4428
2022-10-11 01:35:48 - train: epoch 0032, iter [04100, 05004], lr: 0.077033, loss: 4.7185
2022-10-11 01:36:25 - train: epoch 0032, iter [04200, 05004], lr: 0.077006, loss: 4.7874
2022-10-11 01:37:01 - train: epoch 0032, iter [04300, 05004], lr: 0.076980, loss: 4.9728
2022-10-11 01:37:38 - train: epoch 0032, iter [04400, 05004], lr: 0.076954, loss: 4.5453
2022-10-11 01:38:16 - train: epoch 0032, iter [04500, 05004], lr: 0.076927, loss: 4.8057
2022-10-11 01:38:53 - train: epoch 0032, iter [04600, 05004], lr: 0.076901, loss: 4.3482
2022-10-11 01:39:29 - train: epoch 0032, iter [04700, 05004], lr: 0.076874, loss: 3.7149
2022-10-11 01:40:06 - train: epoch 0032, iter [04800, 05004], lr: 0.076848, loss: 4.5686
2022-10-11 01:40:43 - train: epoch 0032, iter [04900, 05004], lr: 0.076821, loss: 4.6475
2022-10-11 01:41:17 - train: epoch 0032, iter [05000, 05004], lr: 0.076795, loss: 4.3405
2022-10-11 01:41:20 - train: epoch 032, train_loss: 4.4998
2022-10-11 01:42:38 - eval: epoch: 032, acc1: 56.322%, acc5: 81.212%, test_loss: 2.1683, per_image_load_time: 2.153ms, per_image_inference_time: 0.536ms
2022-10-11 01:42:39 - until epoch: 032, best_acc1: 56.322%
2022-10-11 01:42:39 - epoch 033 lr: 0.076794
2022-10-11 01:43:22 - train: epoch 0033, iter [00100, 05004], lr: 0.076767, loss: 4.4544
2022-10-11 01:43:58 - train: epoch 0033, iter [00200, 05004], lr: 0.076741, loss: 4.3959
2022-10-11 01:44:34 - train: epoch 0033, iter [00300, 05004], lr: 0.076714, loss: 4.3934
2022-10-11 01:45:11 - train: epoch 0033, iter [00400, 05004], lr: 0.076688, loss: 4.0181
2022-10-11 01:45:47 - train: epoch 0033, iter [00500, 05004], lr: 0.076661, loss: 4.1786
2022-10-11 01:46:23 - train: epoch 0033, iter [00600, 05004], lr: 0.076634, loss: 4.4980
2022-10-11 01:47:00 - train: epoch 0033, iter [00700, 05004], lr: 0.076608, loss: 3.9815
2022-10-11 01:47:37 - train: epoch 0033, iter [00800, 05004], lr: 0.076581, loss: 4.9785
2022-10-11 01:48:14 - train: epoch 0033, iter [00900, 05004], lr: 0.076555, loss: 4.6156
2022-10-11 01:48:51 - train: epoch 0033, iter [01000, 05004], lr: 0.076528, loss: 4.7273
2022-10-11 01:49:29 - train: epoch 0033, iter [01100, 05004], lr: 0.076502, loss: 4.2314
2022-10-11 01:50:06 - train: epoch 0033, iter [01200, 05004], lr: 0.076475, loss: 4.2854
2022-10-11 01:50:43 - train: epoch 0033, iter [01300, 05004], lr: 0.076448, loss: 4.2262
2022-10-11 01:51:20 - train: epoch 0033, iter [01400, 05004], lr: 0.076422, loss: 4.6787
2022-10-11 01:51:58 - train: epoch 0033, iter [01500, 05004], lr: 0.076395, loss: 5.4556
2022-10-11 01:52:35 - train: epoch 0033, iter [01600, 05004], lr: 0.076368, loss: 4.6466
2022-10-11 01:53:11 - train: epoch 0033, iter [01700, 05004], lr: 0.076342, loss: 5.0867
2022-10-11 01:53:48 - train: epoch 0033, iter [01800, 05004], lr: 0.076315, loss: 3.5944
2022-10-11 01:54:25 - train: epoch 0033, iter [01900, 05004], lr: 0.076288, loss: 4.7018
2022-10-11 01:55:03 - train: epoch 0033, iter [02000, 05004], lr: 0.076262, loss: 5.0034
2022-10-11 01:55:39 - train: epoch 0033, iter [02100, 05004], lr: 0.076235, loss: 4.6251
2022-10-11 01:56:17 - train: epoch 0033, iter [02200, 05004], lr: 0.076208, loss: 4.4156
2022-10-11 01:56:53 - train: epoch 0033, iter [02300, 05004], lr: 0.076181, loss: 4.1039
2022-10-11 01:57:31 - train: epoch 0033, iter [02400, 05004], lr: 0.076155, loss: 5.2044
2022-10-11 01:58:07 - train: epoch 0033, iter [02500, 05004], lr: 0.076128, loss: 4.7796
2022-10-11 01:58:44 - train: epoch 0033, iter [02600, 05004], lr: 0.076101, loss: 4.3004
2022-10-11 01:59:21 - train: epoch 0033, iter [02700, 05004], lr: 0.076074, loss: 4.5112
2022-10-11 01:59:59 - train: epoch 0033, iter [02800, 05004], lr: 0.076048, loss: 4.7852
2022-10-11 02:00:36 - train: epoch 0033, iter [02900, 05004], lr: 0.076021, loss: 4.8116
2022-10-11 02:01:13 - train: epoch 0033, iter [03000, 05004], lr: 0.075994, loss: 3.9721
2022-10-11 02:01:49 - train: epoch 0033, iter [03100, 05004], lr: 0.075967, loss: 4.5918
2022-10-11 02:02:27 - train: epoch 0033, iter [03200, 05004], lr: 0.075940, loss: 4.4172
2022-10-11 02:03:04 - train: epoch 0033, iter [03300, 05004], lr: 0.075913, loss: 4.5405
2022-10-11 02:03:41 - train: epoch 0033, iter [03400, 05004], lr: 0.075887, loss: 4.3151
2022-10-11 02:04:17 - train: epoch 0033, iter [03500, 05004], lr: 0.075860, loss: 5.1802
2022-10-11 02:04:54 - train: epoch 0033, iter [03600, 05004], lr: 0.075833, loss: 4.9849
2022-10-11 02:05:30 - train: epoch 0033, iter [03700, 05004], lr: 0.075806, loss: 4.8082
2022-10-11 02:06:07 - train: epoch 0033, iter [03800, 05004], lr: 0.075779, loss: 4.9457
2022-10-11 02:06:45 - train: epoch 0033, iter [03900, 05004], lr: 0.075752, loss: 4.0911
2022-10-11 02:07:21 - train: epoch 0033, iter [04000, 05004], lr: 0.075725, loss: 5.1057
2022-10-11 02:07:59 - train: epoch 0033, iter [04100, 05004], lr: 0.075698, loss: 3.8155
2022-10-11 02:08:35 - train: epoch 0033, iter [04200, 05004], lr: 0.075671, loss: 4.4782
2022-10-11 02:09:13 - train: epoch 0033, iter [04300, 05004], lr: 0.075644, loss: 4.6316
2022-10-11 02:09:50 - train: epoch 0033, iter [04400, 05004], lr: 0.075618, loss: 4.1749
2022-10-11 02:10:27 - train: epoch 0033, iter [04500, 05004], lr: 0.075591, loss: 4.7585
2022-10-11 02:11:04 - train: epoch 0033, iter [04600, 05004], lr: 0.075564, loss: 5.0140
2022-10-11 02:11:41 - train: epoch 0033, iter [04700, 05004], lr: 0.075537, loss: 4.4812
2022-10-11 02:12:18 - train: epoch 0033, iter [04800, 05004], lr: 0.075510, loss: 4.4698
2022-10-11 02:12:55 - train: epoch 0033, iter [04900, 05004], lr: 0.075483, loss: 4.2361
2022-10-11 02:13:30 - train: epoch 0033, iter [05000, 05004], lr: 0.075456, loss: 4.7731
2022-10-11 02:13:32 - train: epoch 033, train_loss: 4.4746
2022-10-11 02:14:50 - eval: epoch: 033, acc1: 55.872%, acc5: 80.666%, test_loss: 2.1359, per_image_load_time: 1.672ms, per_image_inference_time: 0.560ms
2022-10-11 02:14:50 - until epoch: 033, best_acc1: 56.322%
2022-10-11 02:14:50 - epoch 034 lr: 0.075455
2022-10-11 02:15:34 - train: epoch 0034, iter [00100, 05004], lr: 0.075428, loss: 4.9557
2022-10-11 02:16:09 - train: epoch 0034, iter [00200, 05004], lr: 0.075400, loss: 3.9118
2022-10-11 02:16:46 - train: epoch 0034, iter [00300, 05004], lr: 0.075373, loss: 4.3023
2022-10-11 02:17:23 - train: epoch 0034, iter [00400, 05004], lr: 0.075346, loss: 4.4751
2022-10-11 02:18:00 - train: epoch 0034, iter [00500, 05004], lr: 0.075319, loss: 4.6632
2022-10-11 02:18:38 - train: epoch 0034, iter [00600, 05004], lr: 0.075292, loss: 3.8997
2022-10-11 02:19:14 - train: epoch 0034, iter [00700, 05004], lr: 0.075265, loss: 5.2273
2022-10-11 02:19:51 - train: epoch 0034, iter [00800, 05004], lr: 0.075238, loss: 4.5305
2022-10-11 02:20:29 - train: epoch 0034, iter [00900, 05004], lr: 0.075211, loss: 4.0102
2022-10-11 02:21:06 - train: epoch 0034, iter [01000, 05004], lr: 0.075184, loss: 4.5375
2022-10-11 02:21:43 - train: epoch 0034, iter [01100, 05004], lr: 0.075157, loss: 4.1081
2022-10-11 02:22:19 - train: epoch 0034, iter [01200, 05004], lr: 0.075130, loss: 4.3825
2022-10-11 02:22:56 - train: epoch 0034, iter [01300, 05004], lr: 0.075102, loss: 4.2587
2022-10-11 02:23:33 - train: epoch 0034, iter [01400, 05004], lr: 0.075075, loss: 4.6056
2022-10-11 02:24:10 - train: epoch 0034, iter [01500, 05004], lr: 0.075048, loss: 4.9973
2022-10-11 02:24:47 - train: epoch 0034, iter [01600, 05004], lr: 0.075021, loss: 4.0808
2022-10-11 02:25:23 - train: epoch 0034, iter [01700, 05004], lr: 0.074994, loss: 4.5456
2022-10-11 02:26:01 - train: epoch 0034, iter [01800, 05004], lr: 0.074967, loss: 4.6698
2022-10-11 02:26:38 - train: epoch 0034, iter [01900, 05004], lr: 0.074939, loss: 4.2149
2022-10-11 02:27:15 - train: epoch 0034, iter [02000, 05004], lr: 0.074912, loss: 4.4804
2022-10-11 02:27:52 - train: epoch 0034, iter [02100, 05004], lr: 0.074885, loss: 4.2935
2022-10-11 02:28:30 - train: epoch 0034, iter [02200, 05004], lr: 0.074858, loss: 4.9296
2022-10-11 02:29:08 - train: epoch 0034, iter [02300, 05004], lr: 0.074831, loss: 4.6691
2022-10-11 02:29:43 - train: epoch 0034, iter [02400, 05004], lr: 0.074803, loss: 3.8446
2022-10-11 02:30:20 - train: epoch 0034, iter [02500, 05004], lr: 0.074776, loss: 4.8882
2022-10-11 02:30:57 - train: epoch 0034, iter [02600, 05004], lr: 0.074749, loss: 4.3310
2022-10-11 02:31:34 - train: epoch 0034, iter [02700, 05004], lr: 0.074721, loss: 4.2819
2022-10-11 02:32:11 - train: epoch 0034, iter [02800, 05004], lr: 0.074694, loss: 4.2759
2022-10-11 02:32:49 - train: epoch 0034, iter [02900, 05004], lr: 0.074667, loss: 4.9215
2022-10-11 02:33:26 - train: epoch 0034, iter [03000, 05004], lr: 0.074640, loss: 4.3624
2022-10-11 02:34:03 - train: epoch 0034, iter [03100, 05004], lr: 0.074612, loss: 4.0608
2022-10-11 02:34:40 - train: epoch 0034, iter [03200, 05004], lr: 0.074585, loss: 4.5969
2022-10-11 02:35:17 - train: epoch 0034, iter [03300, 05004], lr: 0.074558, loss: 4.0945
2022-10-11 02:35:54 - train: epoch 0034, iter [03400, 05004], lr: 0.074530, loss: 5.0804
2022-10-11 02:36:30 - train: epoch 0034, iter [03500, 05004], lr: 0.074503, loss: 4.3008
2022-10-11 02:37:07 - train: epoch 0034, iter [03600, 05004], lr: 0.074476, loss: 4.5746
2022-10-11 02:37:44 - train: epoch 0034, iter [03700, 05004], lr: 0.074448, loss: 4.8181
2022-10-11 02:38:21 - train: epoch 0034, iter [03800, 05004], lr: 0.074421, loss: 4.3607
2022-10-11 02:38:57 - train: epoch 0034, iter [03900, 05004], lr: 0.074393, loss: 4.5830
2022-10-11 02:39:35 - train: epoch 0034, iter [04000, 05004], lr: 0.074366, loss: 4.2352
2022-10-11 02:40:12 - train: epoch 0034, iter [04100, 05004], lr: 0.074339, loss: 5.0041
2022-10-11 02:40:49 - train: epoch 0034, iter [04200, 05004], lr: 0.074311, loss: 4.4397
2022-10-11 02:41:26 - train: epoch 0034, iter [04300, 05004], lr: 0.074284, loss: 3.6160
2022-10-11 02:42:04 - train: epoch 0034, iter [04400, 05004], lr: 0.074256, loss: 4.8816
2022-10-11 02:42:41 - train: epoch 0034, iter [04500, 05004], lr: 0.074229, loss: 4.3732
2022-10-11 02:43:18 - train: epoch 0034, iter [04600, 05004], lr: 0.074201, loss: 4.5378
2022-10-11 02:43:54 - train: epoch 0034, iter [04700, 05004], lr: 0.074174, loss: 3.8242
2022-10-11 02:44:31 - train: epoch 0034, iter [04800, 05004], lr: 0.074146, loss: 4.1317
2022-10-11 02:45:09 - train: epoch 0034, iter [04900, 05004], lr: 0.074119, loss: 4.1540
2022-10-11 02:45:43 - train: epoch 0034, iter [05000, 05004], lr: 0.074091, loss: 3.9891
2022-10-11 02:45:46 - train: epoch 034, train_loss: 4.4629
2022-10-11 02:47:05 - eval: epoch: 034, acc1: 54.314%, acc5: 79.596%, test_loss: 2.1431, per_image_load_time: 2.447ms, per_image_inference_time: 0.545ms
2022-10-11 02:47:05 - until epoch: 034, best_acc1: 56.322%
2022-10-11 02:47:05 - epoch 035 lr: 0.074090
2022-10-11 02:47:49 - train: epoch 0035, iter [00100, 05004], lr: 0.074063, loss: 4.3103
2022-10-11 02:48:24 - train: epoch 0035, iter [00200, 05004], lr: 0.074035, loss: 4.2571
2022-10-11 02:49:01 - train: epoch 0035, iter [00300, 05004], lr: 0.074008, loss: 4.7092
2022-10-11 02:49:38 - train: epoch 0035, iter [00400, 05004], lr: 0.073980, loss: 4.6378
2022-10-11 02:50:14 - train: epoch 0035, iter [00500, 05004], lr: 0.073953, loss: 4.6499
2022-10-11 02:50:51 - train: epoch 0035, iter [00600, 05004], lr: 0.073925, loss: 4.1045
2022-10-11 02:51:28 - train: epoch 0035, iter [00700, 05004], lr: 0.073898, loss: 4.9271
2022-10-11 02:52:05 - train: epoch 0035, iter [00800, 05004], lr: 0.073870, loss: 4.1402
2022-10-11 02:52:42 - train: epoch 0035, iter [00900, 05004], lr: 0.073842, loss: 4.5028
2022-10-11 02:53:19 - train: epoch 0035, iter [01000, 05004], lr: 0.073815, loss: 5.1486
2022-10-11 02:53:55 - train: epoch 0035, iter [01100, 05004], lr: 0.073787, loss: 4.6780
2022-10-11 02:54:33 - train: epoch 0035, iter [01200, 05004], lr: 0.073760, loss: 4.1582
2022-10-11 02:55:10 - train: epoch 0035, iter [01300, 05004], lr: 0.073732, loss: 4.2682
2022-10-11 02:55:47 - train: epoch 0035, iter [01400, 05004], lr: 0.073704, loss: 4.5914
2022-10-11 02:56:24 - train: epoch 0035, iter [01500, 05004], lr: 0.073677, loss: 3.4908
2022-10-11 02:57:01 - train: epoch 0035, iter [01600, 05004], lr: 0.073649, loss: 3.9704
2022-10-11 02:57:38 - train: epoch 0035, iter [01700, 05004], lr: 0.073621, loss: 4.8367
2022-10-11 02:58:14 - train: epoch 0035, iter [01800, 05004], lr: 0.073594, loss: 4.2657
2022-10-11 02:58:51 - train: epoch 0035, iter [01900, 05004], lr: 0.073566, loss: 4.3117
2022-10-11 02:59:27 - train: epoch 0035, iter [02000, 05004], lr: 0.073538, loss: 4.3714
2022-10-11 03:00:04 - train: epoch 0035, iter [02100, 05004], lr: 0.073511, loss: 3.7866
2022-10-11 03:00:40 - train: epoch 0035, iter [02200, 05004], lr: 0.073483, loss: 4.5655
2022-10-11 03:01:17 - train: epoch 0035, iter [02300, 05004], lr: 0.073455, loss: 4.2774
2022-10-11 03:01:53 - train: epoch 0035, iter [02400, 05004], lr: 0.073427, loss: 4.2493
2022-10-11 03:02:30 - train: epoch 0035, iter [02500, 05004], lr: 0.073400, loss: 4.6718
2022-10-11 03:03:07 - train: epoch 0035, iter [02600, 05004], lr: 0.073372, loss: 4.4618
2022-10-11 03:03:44 - train: epoch 0035, iter [02700, 05004], lr: 0.073344, loss: 4.4066
2022-10-11 03:04:21 - train: epoch 0035, iter [02800, 05004], lr: 0.073316, loss: 4.6790
2022-10-11 03:04:58 - train: epoch 0035, iter [02900, 05004], lr: 0.073289, loss: 4.2163
2022-10-11 03:05:35 - train: epoch 0035, iter [03000, 05004], lr: 0.073261, loss: 4.5975
2022-10-11 03:06:13 - train: epoch 0035, iter [03100, 05004], lr: 0.073233, loss: 3.9733
2022-10-11 03:06:50 - train: epoch 0035, iter [03200, 05004], lr: 0.073205, loss: 4.8135
2022-10-11 03:07:26 - train: epoch 0035, iter [03300, 05004], lr: 0.073177, loss: 3.9740
2022-10-11 03:08:03 - train: epoch 0035, iter [03400, 05004], lr: 0.073150, loss: 5.0796
2022-10-11 03:08:40 - train: epoch 0035, iter [03500, 05004], lr: 0.073122, loss: 3.9760
2022-10-11 03:09:16 - train: epoch 0035, iter [03600, 05004], lr: 0.073094, loss: 4.3098
2022-10-11 03:09:53 - train: epoch 0035, iter [03700, 05004], lr: 0.073066, loss: 4.7027
2022-10-11 03:10:30 - train: epoch 0035, iter [03800, 05004], lr: 0.073038, loss: 4.8190
2022-10-11 03:11:06 - train: epoch 0035, iter [03900, 05004], lr: 0.073010, loss: 3.8315
2022-10-11 03:11:44 - train: epoch 0035, iter [04000, 05004], lr: 0.072983, loss: 4.3289
2022-10-11 03:12:22 - train: epoch 0035, iter [04100, 05004], lr: 0.072955, loss: 4.7955
2022-10-11 03:12:59 - train: epoch 0035, iter [04200, 05004], lr: 0.072927, loss: 4.4176
2022-10-11 03:13:35 - train: epoch 0035, iter [04300, 05004], lr: 0.072899, loss: 4.9924
2022-10-11 03:14:12 - train: epoch 0035, iter [04400, 05004], lr: 0.072871, loss: 4.3416
2022-10-11 03:14:48 - train: epoch 0035, iter [04500, 05004], lr: 0.072843, loss: 4.6324
2022-10-11 03:15:25 - train: epoch 0035, iter [04600, 05004], lr: 0.072815, loss: 4.1635
2022-10-11 03:16:01 - train: epoch 0035, iter [04700, 05004], lr: 0.072787, loss: 4.7425
2022-10-11 03:16:38 - train: epoch 0035, iter [04800, 05004], lr: 0.072759, loss: 5.0698
2022-10-11 03:17:16 - train: epoch 0035, iter [04900, 05004], lr: 0.072731, loss: 4.9706
2022-10-11 03:17:51 - train: epoch 0035, iter [05000, 05004], lr: 0.072703, loss: 4.9015
2022-10-11 03:17:53 - train: epoch 035, train_loss: 4.4581
2022-10-11 03:19:13 - eval: epoch: 035, acc1: 54.056%, acc5: 79.186%, test_loss: 2.1998, per_image_load_time: 2.432ms, per_image_inference_time: 0.563ms
2022-10-11 03:19:13 - until epoch: 035, best_acc1: 56.322%
2022-10-11 03:19:13 - epoch 036 lr: 0.072702
2022-10-11 03:19:56 - train: epoch 0036, iter [00100, 05004], lr: 0.072674, loss: 5.0033
2022-10-11 03:20:33 - train: epoch 0036, iter [00200, 05004], lr: 0.072646, loss: 4.9518
2022-10-11 03:21:09 - train: epoch 0036, iter [00300, 05004], lr: 0.072618, loss: 4.9030
2022-10-11 03:21:47 - train: epoch 0036, iter [00400, 05004], lr: 0.072590, loss: 4.8429
2022-10-11 03:22:24 - train: epoch 0036, iter [00500, 05004], lr: 0.072562, loss: 4.1920
2022-10-11 03:23:00 - train: epoch 0036, iter [00600, 05004], lr: 0.072534, loss: 4.1977
2022-10-11 03:23:38 - train: epoch 0036, iter [00700, 05004], lr: 0.072506, loss: 5.2443
2022-10-11 03:24:15 - train: epoch 0036, iter [00800, 05004], lr: 0.072478, loss: 4.5018
2022-10-11 03:24:52 - train: epoch 0036, iter [00900, 05004], lr: 0.072450, loss: 4.0625
2022-10-11 03:25:29 - train: epoch 0036, iter [01000, 05004], lr: 0.072422, loss: 4.5995
2022-10-11 03:26:06 - train: epoch 0036, iter [01100, 05004], lr: 0.072394, loss: 3.8093
2022-10-11 03:26:42 - train: epoch 0036, iter [01200, 05004], lr: 0.072366, loss: 4.1402
2022-10-11 03:27:19 - train: epoch 0036, iter [01300, 05004], lr: 0.072338, loss: 3.7646
2022-10-11 03:27:55 - train: epoch 0036, iter [01400, 05004], lr: 0.072310, loss: 4.9365
2022-10-11 03:28:33 - train: epoch 0036, iter [01500, 05004], lr: 0.072282, loss: 4.8706
2022-10-11 03:29:09 - train: epoch 0036, iter [01600, 05004], lr: 0.072254, loss: 4.4018
2022-10-11 03:29:46 - train: epoch 0036, iter [01700, 05004], lr: 0.072226, loss: 4.6205
2022-10-11 03:30:23 - train: epoch 0036, iter [01800, 05004], lr: 0.072197, loss: 4.3852
2022-10-11 03:30:59 - train: epoch 0036, iter [01900, 05004], lr: 0.072169, loss: 4.8091
2022-10-11 03:31:36 - train: epoch 0036, iter [02000, 05004], lr: 0.072141, loss: 4.6201
2022-10-11 03:32:12 - train: epoch 0036, iter [02100, 05004], lr: 0.072113, loss: 4.1108
2022-10-11 03:32:49 - train: epoch 0036, iter [02200, 05004], lr: 0.072085, loss: 4.2836
2022-10-11 03:33:26 - train: epoch 0036, iter [02300, 05004], lr: 0.072057, loss: 4.6203
2022-10-11 03:34:04 - train: epoch 0036, iter [02400, 05004], lr: 0.072029, loss: 4.4799
2022-10-11 03:34:41 - train: epoch 0036, iter [02500, 05004], lr: 0.072000, loss: 4.6713
2022-10-11 03:35:18 - train: epoch 0036, iter [02600, 05004], lr: 0.071972, loss: 4.7232
2022-10-11 03:35:54 - train: epoch 0036, iter [02700, 05004], lr: 0.071944, loss: 3.4138
2022-10-11 03:36:31 - train: epoch 0036, iter [02800, 05004], lr: 0.071916, loss: 4.2856
2022-10-11 03:37:07 - train: epoch 0036, iter [02900, 05004], lr: 0.071888, loss: 5.0354
2022-10-11 03:37:44 - train: epoch 0036, iter [03000, 05004], lr: 0.071859, loss: 4.3475
2022-10-11 03:38:20 - train: epoch 0036, iter [03100, 05004], lr: 0.071831, loss: 5.0538
2022-10-11 03:38:56 - train: epoch 0036, iter [03200, 05004], lr: 0.071803, loss: 4.5491
2022-10-11 03:39:34 - train: epoch 0036, iter [03300, 05004], lr: 0.071775, loss: 4.5908
2022-10-11 03:40:10 - train: epoch 0036, iter [03400, 05004], lr: 0.071746, loss: 4.4487
2022-10-11 03:40:48 - train: epoch 0036, iter [03500, 05004], lr: 0.071718, loss: 4.1837
2022-10-11 03:41:25 - train: epoch 0036, iter [03600, 05004], lr: 0.071690, loss: 4.9037
2022-10-11 03:42:01 - train: epoch 0036, iter [03700, 05004], lr: 0.071661, loss: 3.6578
2022-10-11 03:42:38 - train: epoch 0036, iter [03800, 05004], lr: 0.071633, loss: 4.9419
2022-10-11 03:43:15 - train: epoch 0036, iter [03900, 05004], lr: 0.071605, loss: 4.7288
2022-10-11 03:43:51 - train: epoch 0036, iter [04000, 05004], lr: 0.071577, loss: 5.1141
2022-10-11 03:44:29 - train: epoch 0036, iter [04100, 05004], lr: 0.071548, loss: 3.8475
2022-10-11 03:45:05 - train: epoch 0036, iter [04200, 05004], lr: 0.071520, loss: 4.2672
2022-10-11 03:45:42 - train: epoch 0036, iter [04300, 05004], lr: 0.071492, loss: 4.3243
2022-10-11 03:46:18 - train: epoch 0036, iter [04400, 05004], lr: 0.071463, loss: 4.1410
2022-10-11 03:46:56 - train: epoch 0036, iter [04500, 05004], lr: 0.071435, loss: 3.9003
2022-10-11 03:47:33 - train: epoch 0036, iter [04600, 05004], lr: 0.071407, loss: 5.0740
2022-10-11 03:48:10 - train: epoch 0036, iter [04700, 05004], lr: 0.071378, loss: 4.9547
2022-10-11 03:48:47 - train: epoch 0036, iter [04800, 05004], lr: 0.071350, loss: 3.7418
2022-10-11 03:49:23 - train: epoch 0036, iter [04900, 05004], lr: 0.071321, loss: 3.9446
2022-10-11 03:49:58 - train: epoch 0036, iter [05000, 05004], lr: 0.071293, loss: 4.3672
2022-10-11 03:50:01 - train: epoch 036, train_loss: 4.4427
2022-10-11 03:51:20 - eval: epoch: 036, acc1: 55.402%, acc5: 80.322%, test_loss: 2.1671, per_image_load_time: 2.418ms, per_image_inference_time: 0.582ms
2022-10-11 03:51:21 - until epoch: 036, best_acc1: 56.322%
2022-10-11 03:51:21 - epoch 037 lr: 0.071292
2022-10-11 03:52:03 - train: epoch 0037, iter [00100, 05004], lr: 0.071263, loss: 3.9315
2022-10-11 03:52:39 - train: epoch 0037, iter [00200, 05004], lr: 0.071235, loss: 4.0505
2022-10-11 03:53:16 - train: epoch 0037, iter [00300, 05004], lr: 0.071207, loss: 5.0052
2022-10-11 03:53:52 - train: epoch 0037, iter [00400, 05004], lr: 0.071178, loss: 3.4300
2022-10-11 03:54:29 - train: epoch 0037, iter [00500, 05004], lr: 0.071150, loss: 4.8286
2022-10-11 03:55:06 - train: epoch 0037, iter [00600, 05004], lr: 0.071121, loss: 5.0866
2022-10-11 03:55:42 - train: epoch 0037, iter [00700, 05004], lr: 0.071093, loss: 4.0580
2022-10-11 03:56:19 - train: epoch 0037, iter [00800, 05004], lr: 0.071064, loss: 4.0240
2022-10-11 03:56:56 - train: epoch 0037, iter [00900, 05004], lr: 0.071036, loss: 4.5858
2022-10-11 03:57:33 - train: epoch 0037, iter [01000, 05004], lr: 0.071007, loss: 4.7996
2022-10-11 03:58:08 - train: epoch 0037, iter [01100, 05004], lr: 0.070979, loss: 4.5220
2022-10-11 03:58:46 - train: epoch 0037, iter [01200, 05004], lr: 0.070950, loss: 4.4053
2022-10-11 03:59:22 - train: epoch 0037, iter [01300, 05004], lr: 0.070922, loss: 3.7870
2022-10-11 03:59:59 - train: epoch 0037, iter [01400, 05004], lr: 0.070893, loss: 4.7325
2022-10-11 04:00:35 - train: epoch 0037, iter [01500, 05004], lr: 0.070865, loss: 3.9175
2022-10-11 04:01:12 - train: epoch 0037, iter [01600, 05004], lr: 0.070836, loss: 3.9991
2022-10-11 04:01:48 - train: epoch 0037, iter [01700, 05004], lr: 0.070808, loss: 4.0136
2022-10-11 04:02:25 - train: epoch 0037, iter [01800, 05004], lr: 0.070779, loss: 4.7000
2022-10-11 04:03:02 - train: epoch 0037, iter [01900, 05004], lr: 0.070751, loss: 4.8671
2022-10-11 04:03:38 - train: epoch 0037, iter [02000, 05004], lr: 0.070722, loss: 4.3429
2022-10-11 04:04:14 - train: epoch 0037, iter [02100, 05004], lr: 0.070694, loss: 4.9239
2022-10-11 04:04:51 - train: epoch 0037, iter [02200, 05004], lr: 0.070665, loss: 3.7711
2022-10-11 04:05:28 - train: epoch 0037, iter [02300, 05004], lr: 0.070636, loss: 3.9978
2022-10-11 04:06:05 - train: epoch 0037, iter [02400, 05004], lr: 0.070608, loss: 4.5136
2022-10-11 04:06:41 - train: epoch 0037, iter [02500, 05004], lr: 0.070579, loss: 4.4891
2022-10-11 04:07:18 - train: epoch 0037, iter [02600, 05004], lr: 0.070551, loss: 4.9780
2022-10-11 04:07:55 - train: epoch 0037, iter [02700, 05004], lr: 0.070522, loss: 4.2900
2022-10-11 04:08:32 - train: epoch 0037, iter [02800, 05004], lr: 0.070493, loss: 4.3715
2022-10-11 04:09:09 - train: epoch 0037, iter [02900, 05004], lr: 0.070465, loss: 4.9540
2022-10-11 04:09:45 - train: epoch 0037, iter [03000, 05004], lr: 0.070436, loss: 5.0124
2022-10-11 04:10:23 - train: epoch 0037, iter [03100, 05004], lr: 0.070407, loss: 3.5456
2022-10-11 04:11:00 - train: epoch 0037, iter [03200, 05004], lr: 0.070379, loss: 5.2464
2022-10-11 04:11:36 - train: epoch 0037, iter [03300, 05004], lr: 0.070350, loss: 4.1296
2022-10-11 04:12:13 - train: epoch 0037, iter [03400, 05004], lr: 0.070321, loss: 5.3099
2022-10-11 04:12:50 - train: epoch 0037, iter [03500, 05004], lr: 0.070293, loss: 4.3496
2022-10-11 04:13:27 - train: epoch 0037, iter [03600, 05004], lr: 0.070264, loss: 4.3978
2022-10-11 04:14:05 - train: epoch 0037, iter [03700, 05004], lr: 0.070235, loss: 4.2121
2022-10-11 04:14:41 - train: epoch 0037, iter [03800, 05004], lr: 0.070207, loss: 5.1038
2022-10-11 04:15:18 - train: epoch 0037, iter [03900, 05004], lr: 0.070178, loss: 4.7186
2022-10-11 04:15:56 - train: epoch 0037, iter [04000, 05004], lr: 0.070149, loss: 3.8543
2022-10-11 04:16:32 - train: epoch 0037, iter [04100, 05004], lr: 0.070120, loss: 4.4647
2022-10-11 04:17:09 - train: epoch 0037, iter [04200, 05004], lr: 0.070092, loss: 3.7360
2022-10-11 04:17:45 - train: epoch 0037, iter [04300, 05004], lr: 0.070063, loss: 4.1682
2022-10-11 04:18:22 - train: epoch 0037, iter [04400, 05004], lr: 0.070034, loss: 4.5167
2022-10-11 04:18:59 - train: epoch 0037, iter [04500, 05004], lr: 0.070005, loss: 4.6409
2022-10-11 04:19:36 - train: epoch 0037, iter [04600, 05004], lr: 0.069977, loss: 5.2287
2022-10-11 04:20:13 - train: epoch 0037, iter [04700, 05004], lr: 0.069948, loss: 4.1676
2022-10-11 04:20:50 - train: epoch 0037, iter [04800, 05004], lr: 0.069919, loss: 4.9951
2022-10-11 04:21:27 - train: epoch 0037, iter [04900, 05004], lr: 0.069890, loss: 3.9498
2022-10-11 04:22:02 - train: epoch 0037, iter [05000, 05004], lr: 0.069862, loss: 4.3074
2022-10-11 04:22:04 - train: epoch 037, train_loss: 4.4301
2022-10-11 04:23:24 - eval: epoch: 037, acc1: 56.310%, acc5: 80.946%, test_loss: 2.1719, per_image_load_time: 2.078ms, per_image_inference_time: 0.597ms
2022-10-11 04:23:24 - until epoch: 037, best_acc1: 56.322%
2022-10-11 04:23:24 - epoch 038 lr: 0.069860
2022-10-11 04:24:06 - train: epoch 0038, iter [00100, 05004], lr: 0.069832, loss: 4.6846
2022-10-11 04:24:43 - train: epoch 0038, iter [00200, 05004], lr: 0.069803, loss: 4.7942
2022-10-11 04:25:18 - train: epoch 0038, iter [00300, 05004], lr: 0.069774, loss: 4.3563
2022-10-11 04:25:54 - train: epoch 0038, iter [00400, 05004], lr: 0.069745, loss: 3.6798
2022-10-11 04:26:30 - train: epoch 0038, iter [00500, 05004], lr: 0.069716, loss: 3.9230
2022-10-11 04:27:07 - train: epoch 0038, iter [00600, 05004], lr: 0.069687, loss: 5.3394
2022-10-11 04:27:44 - train: epoch 0038, iter [00700, 05004], lr: 0.069659, loss: 3.5396
2022-10-11 04:28:20 - train: epoch 0038, iter [00800, 05004], lr: 0.069630, loss: 3.8753
2022-10-11 04:28:57 - train: epoch 0038, iter [00900, 05004], lr: 0.069601, loss: 4.5465
2022-10-11 04:29:33 - train: epoch 0038, iter [01000, 05004], lr: 0.069572, loss: 4.6203
2022-10-11 04:30:10 - train: epoch 0038, iter [01100, 05004], lr: 0.069543, loss: 4.8175
2022-10-11 04:30:47 - train: epoch 0038, iter [01200, 05004], lr: 0.069514, loss: 4.7628
2022-10-11 04:31:24 - train: epoch 0038, iter [01300, 05004], lr: 0.069485, loss: 4.7138
2022-10-11 04:32:00 - train: epoch 0038, iter [01400, 05004], lr: 0.069456, loss: 4.5706
2022-10-11 04:32:37 - train: epoch 0038, iter [01500, 05004], lr: 0.069427, loss: 4.6063
2022-10-11 04:33:13 - train: epoch 0038, iter [01600, 05004], lr: 0.069399, loss: 4.1321
2022-10-11 04:33:50 - train: epoch 0038, iter [01700, 05004], lr: 0.069370, loss: 4.2592
2022-10-11 04:34:27 - train: epoch 0038, iter [01800, 05004], lr: 0.069341, loss: 4.8871
2022-10-11 04:35:02 - train: epoch 0038, iter [01900, 05004], lr: 0.069312, loss: 3.9406
2022-10-11 04:35:38 - train: epoch 0038, iter [02000, 05004], lr: 0.069283, loss: 4.5460
2022-10-11 04:36:17 - train: epoch 0038, iter [02100, 05004], lr: 0.069254, loss: 4.6871
2022-10-11 04:36:54 - train: epoch 0038, iter [02200, 05004], lr: 0.069225, loss: 4.7691
2022-10-11 04:37:31 - train: epoch 0038, iter [02300, 05004], lr: 0.069196, loss: 4.4595
2022-10-11 04:38:08 - train: epoch 0038, iter [02400, 05004], lr: 0.069167, loss: 4.6640
2022-10-11 04:38:44 - train: epoch 0038, iter [02500, 05004], lr: 0.069138, loss: 4.7129
2022-10-11 04:39:20 - train: epoch 0038, iter [02600, 05004], lr: 0.069109, loss: 4.3227
2022-10-11 04:39:58 - train: epoch 0038, iter [02700, 05004], lr: 0.069080, loss: 4.5922
2022-10-11 04:40:36 - train: epoch 0038, iter [02800, 05004], lr: 0.069051, loss: 4.8925
2022-10-11 04:41:11 - train: epoch 0038, iter [02900, 05004], lr: 0.069022, loss: 4.5086
2022-10-11 04:41:49 - train: epoch 0038, iter [03000, 05004], lr: 0.068993, loss: 4.2647
2022-10-11 04:42:26 - train: epoch 0038, iter [03100, 05004], lr: 0.068964, loss: 4.8504
2022-10-11 04:43:01 - train: epoch 0038, iter [03200, 05004], lr: 0.068935, loss: 3.9652
2022-10-11 04:43:39 - train: epoch 0038, iter [03300, 05004], lr: 0.068906, loss: 4.3581
2022-10-11 04:44:16 - train: epoch 0038, iter [03400, 05004], lr: 0.068877, loss: 4.1263
2022-10-11 04:44:52 - train: epoch 0038, iter [03500, 05004], lr: 0.068847, loss: 4.8219
2022-10-11 04:45:28 - train: epoch 0038, iter [03600, 05004], lr: 0.068818, loss: 4.4533
2022-10-11 04:46:06 - train: epoch 0038, iter [03700, 05004], lr: 0.068789, loss: 3.6625
2022-10-11 04:46:43 - train: epoch 0038, iter [03800, 05004], lr: 0.068760, loss: 4.9964
2022-10-11 04:47:20 - train: epoch 0038, iter [03900, 05004], lr: 0.068731, loss: 3.6297
2022-10-11 04:47:58 - train: epoch 0038, iter [04000, 05004], lr: 0.068702, loss: 4.5102
2022-10-11 04:48:34 - train: epoch 0038, iter [04100, 05004], lr: 0.068673, loss: 4.4087
2022-10-11 04:49:11 - train: epoch 0038, iter [04200, 05004], lr: 0.068644, loss: 3.6999
2022-10-11 04:49:48 - train: epoch 0038, iter [04300, 05004], lr: 0.068615, loss: 4.4575
2022-10-11 04:50:25 - train: epoch 0038, iter [04400, 05004], lr: 0.068586, loss: 4.3606
2022-10-11 04:51:02 - train: epoch 0038, iter [04500, 05004], lr: 0.068556, loss: 3.9967
2022-10-11 04:51:38 - train: epoch 0038, iter [04600, 05004], lr: 0.068527, loss: 4.0940
2022-10-11 04:52:15 - train: epoch 0038, iter [04700, 05004], lr: 0.068498, loss: 3.5065
2022-10-11 04:52:52 - train: epoch 0038, iter [04800, 05004], lr: 0.068469, loss: 4.9451
2022-10-11 04:53:29 - train: epoch 0038, iter [04900, 05004], lr: 0.068440, loss: 4.0517
2022-10-11 04:54:04 - train: epoch 0038, iter [05000, 05004], lr: 0.068411, loss: 4.8059
2022-10-11 04:54:06 - train: epoch 038, train_loss: 4.4259
2022-10-11 04:55:24 - eval: epoch: 038, acc1: 58.430%, acc5: 82.548%, test_loss: 2.0031, per_image_load_time: 2.221ms, per_image_inference_time: 0.600ms
2022-10-11 04:55:25 - until epoch: 038, best_acc1: 58.430%
2022-10-11 04:55:25 - epoch 039 lr: 0.068409
2022-10-11 04:56:06 - train: epoch 0039, iter [00100, 05004], lr: 0.068380, loss: 4.7124
2022-10-11 04:56:42 - train: epoch 0039, iter [00200, 05004], lr: 0.068351, loss: 3.9670
2022-10-11 04:57:20 - train: epoch 0039, iter [00300, 05004], lr: 0.068322, loss: 4.6623
2022-10-11 04:57:56 - train: epoch 0039, iter [00400, 05004], lr: 0.068293, loss: 4.6706
2022-10-11 04:58:32 - train: epoch 0039, iter [00500, 05004], lr: 0.068263, loss: 4.7068
2022-10-11 04:59:09 - train: epoch 0039, iter [00600, 05004], lr: 0.068234, loss: 4.6564
2022-10-11 04:59:45 - train: epoch 0039, iter [00700, 05004], lr: 0.068205, loss: 4.4118
2022-10-11 05:00:22 - train: epoch 0039, iter [00800, 05004], lr: 0.068176, loss: 4.3634
2022-10-11 05:00:59 - train: epoch 0039, iter [00900, 05004], lr: 0.068146, loss: 4.7443
2022-10-11 05:01:36 - train: epoch 0039, iter [01000, 05004], lr: 0.068117, loss: 4.0334
2022-10-11 05:02:13 - train: epoch 0039, iter [01100, 05004], lr: 0.068088, loss: 4.9591
2022-10-11 05:02:50 - train: epoch 0039, iter [01200, 05004], lr: 0.068059, loss: 4.4849
2022-10-11 05:03:27 - train: epoch 0039, iter [01300, 05004], lr: 0.068029, loss: 4.8897
2022-10-11 05:04:03 - train: epoch 0039, iter [01400, 05004], lr: 0.068000, loss: 4.5794
2022-10-11 05:04:40 - train: epoch 0039, iter [01500, 05004], lr: 0.067971, loss: 4.0936
2022-10-11 05:05:16 - train: epoch 0039, iter [01600, 05004], lr: 0.067942, loss: 5.0382
2022-10-11 05:05:53 - train: epoch 0039, iter [01700, 05004], lr: 0.067912, loss: 4.8845
2022-10-11 05:06:30 - train: epoch 0039, iter [01800, 05004], lr: 0.067883, loss: 4.4829
2022-10-11 05:07:07 - train: epoch 0039, iter [01900, 05004], lr: 0.067854, loss: 3.6912
2022-10-11 05:07:43 - train: epoch 0039, iter [02000, 05004], lr: 0.067824, loss: 4.3241
2022-10-11 05:08:21 - train: epoch 0039, iter [02100, 05004], lr: 0.067795, loss: 4.9897
2022-10-11 05:08:57 - train: epoch 0039, iter [02200, 05004], lr: 0.067766, loss: 3.8034
2022-10-11 05:09:33 - train: epoch 0039, iter [02300, 05004], lr: 0.067736, loss: 4.9428
2022-10-11 05:10:11 - train: epoch 0039, iter [02400, 05004], lr: 0.067707, loss: 3.9705
2022-10-11 05:10:48 - train: epoch 0039, iter [02500, 05004], lr: 0.067678, loss: 3.9569
2022-10-11 05:11:24 - train: epoch 0039, iter [02600, 05004], lr: 0.067648, loss: 4.2299
2022-10-11 05:12:02 - train: epoch 0039, iter [02700, 05004], lr: 0.067619, loss: 4.4957
2022-10-11 05:12:40 - train: epoch 0039, iter [02800, 05004], lr: 0.067589, loss: 4.5650
2022-10-11 05:13:15 - train: epoch 0039, iter [02900, 05004], lr: 0.067560, loss: 4.7583
2022-10-11 05:13:52 - train: epoch 0039, iter [03000, 05004], lr: 0.067531, loss: 4.9565
2022-10-11 05:14:29 - train: epoch 0039, iter [03100, 05004], lr: 0.067501, loss: 4.7895
2022-10-11 05:15:05 - train: epoch 0039, iter [03200, 05004], lr: 0.067472, loss: 4.4103
2022-10-11 05:15:42 - train: epoch 0039, iter [03300, 05004], lr: 0.067442, loss: 4.5884
2022-10-11 05:16:19 - train: epoch 0039, iter [03400, 05004], lr: 0.067413, loss: 4.6965
2022-10-11 05:16:56 - train: epoch 0039, iter [03500, 05004], lr: 0.067384, loss: 4.8337
2022-10-11 05:17:33 - train: epoch 0039, iter [03600, 05004], lr: 0.067354, loss: 4.9207
2022-10-11 05:18:10 - train: epoch 0039, iter [03700, 05004], lr: 0.067325, loss: 3.6424
2022-10-11 05:18:46 - train: epoch 0039, iter [03800, 05004], lr: 0.067295, loss: 4.4001
2022-10-11 05:19:22 - train: epoch 0039, iter [03900, 05004], lr: 0.067266, loss: 4.6873
2022-10-11 05:20:00 - train: epoch 0039, iter [04000, 05004], lr: 0.067236, loss: 4.0722
2022-10-11 05:20:37 - train: epoch 0039, iter [04100, 05004], lr: 0.067207, loss: 3.4369
2022-10-11 05:21:13 - train: epoch 0039, iter [04200, 05004], lr: 0.067177, loss: 4.7936
2022-10-11 05:21:50 - train: epoch 0039, iter [04300, 05004], lr: 0.067148, loss: 4.4636
2022-10-11 05:22:27 - train: epoch 0039, iter [04400, 05004], lr: 0.067118, loss: 4.6752
2022-10-11 05:23:04 - train: epoch 0039, iter [04500, 05004], lr: 0.067089, loss: 3.5873
2022-10-11 05:23:41 - train: epoch 0039, iter [04600, 05004], lr: 0.067059, loss: 4.3302
2022-10-11 05:24:18 - train: epoch 0039, iter [04700, 05004], lr: 0.067030, loss: 4.6470
2022-10-11 05:24:54 - train: epoch 0039, iter [04800, 05004], lr: 0.067000, loss: 4.5136
2022-10-11 05:25:31 - train: epoch 0039, iter [04900, 05004], lr: 0.066971, loss: 4.1385
2022-10-11 05:26:06 - train: epoch 0039, iter [05000, 05004], lr: 0.066941, loss: 3.7202
2022-10-11 05:26:08 - train: epoch 039, train_loss: 4.4063
2022-10-11 05:27:26 - eval: epoch: 039, acc1: 56.802%, acc5: 81.090%, test_loss: 2.1198, per_image_load_time: 2.309ms, per_image_inference_time: 0.609ms
2022-10-11 05:27:26 - until epoch: 039, best_acc1: 58.430%
2022-10-11 05:27:26 - epoch 040 lr: 0.066940
2022-10-11 05:28:08 - train: epoch 0040, iter [00100, 05004], lr: 0.066911, loss: 4.9195
2022-10-11 05:28:45 - train: epoch 0040, iter [00200, 05004], lr: 0.066881, loss: 4.4817
2022-10-11 05:29:22 - train: epoch 0040, iter [00300, 05004], lr: 0.066852, loss: 4.1356
2022-10-11 05:29:58 - train: epoch 0040, iter [00400, 05004], lr: 0.066822, loss: 4.5582
2022-10-11 05:30:35 - train: epoch 0040, iter [00500, 05004], lr: 0.066792, loss: 4.5012
2022-10-11 05:31:11 - train: epoch 0040, iter [00600, 05004], lr: 0.066763, loss: 4.0329
2022-10-11 05:31:48 - train: epoch 0040, iter [00700, 05004], lr: 0.066733, loss: 3.8468
2022-10-11 05:32:25 - train: epoch 0040, iter [00800, 05004], lr: 0.066704, loss: 4.0509
2022-10-11 05:33:02 - train: epoch 0040, iter [00900, 05004], lr: 0.066674, loss: 4.3125
2022-10-11 05:33:39 - train: epoch 0040, iter [01000, 05004], lr: 0.066645, loss: 4.0339
2022-10-11 05:34:15 - train: epoch 0040, iter [01100, 05004], lr: 0.066615, loss: 4.8532
2022-10-11 05:34:51 - train: epoch 0040, iter [01200, 05004], lr: 0.066585, loss: 4.1479
2022-10-11 05:35:28 - train: epoch 0040, iter [01300, 05004], lr: 0.066556, loss: 3.5857
2022-10-11 05:36:05 - train: epoch 0040, iter [01400, 05004], lr: 0.066526, loss: 4.2674
2022-10-11 05:36:41 - train: epoch 0040, iter [01500, 05004], lr: 0.066496, loss: 4.1041
2022-10-11 05:37:18 - train: epoch 0040, iter [01600, 05004], lr: 0.066467, loss: 4.0194
2022-10-11 05:37:55 - train: epoch 0040, iter [01700, 05004], lr: 0.066437, loss: 5.1823
2022-10-11 05:38:31 - train: epoch 0040, iter [01800, 05004], lr: 0.066408, loss: 3.8494
2022-10-11 05:39:08 - train: epoch 0040, iter [01900, 05004], lr: 0.066378, loss: 3.9954
2022-10-11 05:39:45 - train: epoch 0040, iter [02000, 05004], lr: 0.066348, loss: 4.2413
2022-10-11 05:40:22 - train: epoch 0040, iter [02100, 05004], lr: 0.066319, loss: 4.6448
2022-10-11 05:40:59 - train: epoch 0040, iter [02200, 05004], lr: 0.066289, loss: 3.7784
2022-10-11 05:41:35 - train: epoch 0040, iter [02300, 05004], lr: 0.066259, loss: 4.3498
2022-10-11 05:42:12 - train: epoch 0040, iter [02400, 05004], lr: 0.066230, loss: 4.3088
2022-10-11 05:42:50 - train: epoch 0040, iter [02500, 05004], lr: 0.066200, loss: 3.4954
2022-10-11 05:43:26 - train: epoch 0040, iter [02600, 05004], lr: 0.066170, loss: 4.4485
2022-10-11 05:44:03 - train: epoch 0040, iter [02700, 05004], lr: 0.066140, loss: 4.9108
2022-10-11 05:44:39 - train: epoch 0040, iter [02800, 05004], lr: 0.066111, loss: 4.2489
2022-10-11 05:45:15 - train: epoch 0040, iter [02900, 05004], lr: 0.066081, loss: 4.2045
2022-10-11 05:45:53 - train: epoch 0040, iter [03000, 05004], lr: 0.066051, loss: 4.5117
2022-10-11 05:46:30 - train: epoch 0040, iter [03100, 05004], lr: 0.066022, loss: 4.6230
2022-10-11 05:47:06 - train: epoch 0040, iter [03200, 05004], lr: 0.065992, loss: 4.1663
2022-10-11 05:47:43 - train: epoch 0040, iter [03300, 05004], lr: 0.065962, loss: 4.4102
2022-10-11 05:48:20 - train: epoch 0040, iter [03400, 05004], lr: 0.065932, loss: 4.8298
2022-10-11 05:48:56 - train: epoch 0040, iter [03500, 05004], lr: 0.065903, loss: 4.5559
2022-10-11 05:49:32 - train: epoch 0040, iter [03600, 05004], lr: 0.065873, loss: 5.0121
2022-10-11 05:50:09 - train: epoch 0040, iter [03700, 05004], lr: 0.065843, loss: 4.0276
2022-10-11 05:50:46 - train: epoch 0040, iter [03800, 05004], lr: 0.065813, loss: 4.0213
2022-10-11 05:51:22 - train: epoch 0040, iter [03900, 05004], lr: 0.065783, loss: 4.9012
2022-10-11 05:52:00 - train: epoch 0040, iter [04000, 05004], lr: 0.065754, loss: 5.1136
2022-10-11 05:52:37 - train: epoch 0040, iter [04100, 05004], lr: 0.065724, loss: 4.9338
2022-10-11 05:53:14 - train: epoch 0040, iter [04200, 05004], lr: 0.065694, loss: 4.4765
2022-10-11 05:53:50 - train: epoch 0040, iter [04300, 05004], lr: 0.065664, loss: 5.0142
2022-10-11 05:54:27 - train: epoch 0040, iter [04400, 05004], lr: 0.065634, loss: 4.8551
2022-10-11 05:55:04 - train: epoch 0040, iter [04500, 05004], lr: 0.065605, loss: 4.3215
2022-10-11 05:55:41 - train: epoch 0040, iter [04600, 05004], lr: 0.065575, loss: 4.2674
2022-10-11 05:56:18 - train: epoch 0040, iter [04700, 05004], lr: 0.065545, loss: 4.1951
2022-10-11 05:56:55 - train: epoch 0040, iter [04800, 05004], lr: 0.065515, loss: 4.4518
2022-10-11 05:57:31 - train: epoch 0040, iter [04900, 05004], lr: 0.065485, loss: 4.6202
2022-10-11 05:58:06 - train: epoch 0040, iter [05000, 05004], lr: 0.065455, loss: 4.8352
2022-10-11 05:58:08 - train: epoch 040, train_loss: 4.4040
2022-10-11 05:59:28 - eval: epoch: 040, acc1: 57.096%, acc5: 81.456%, test_loss: 2.0568, per_image_load_time: 2.183ms, per_image_inference_time: 0.600ms
2022-10-11 05:59:28 - until epoch: 040, best_acc1: 58.430%
2022-10-11 05:59:28 - epoch 041 lr: 0.065454
2022-10-11 06:00:10 - train: epoch 0041, iter [00100, 05004], lr: 0.065424, loss: 4.6372
2022-10-11 06:00:46 - train: epoch 0041, iter [00200, 05004], lr: 0.065395, loss: 4.1909
2022-10-11 06:01:22 - train: epoch 0041, iter [00300, 05004], lr: 0.065365, loss: 4.3545
2022-10-11 06:01:59 - train: epoch 0041, iter [00400, 05004], lr: 0.065335, loss: 4.0739
2022-10-11 06:02:36 - train: epoch 0041, iter [00500, 05004], lr: 0.065305, loss: 4.8384
2022-10-11 06:03:13 - train: epoch 0041, iter [00600, 05004], lr: 0.065275, loss: 4.1524
2022-10-11 06:03:49 - train: epoch 0041, iter [00700, 05004], lr: 0.065245, loss: 4.5405
2022-10-11 06:04:25 - train: epoch 0041, iter [00800, 05004], lr: 0.065215, loss: 4.2250
2022-10-11 06:05:02 - train: epoch 0041, iter [00900, 05004], lr: 0.065185, loss: 3.2017
2022-10-11 06:05:39 - train: epoch 0041, iter [01000, 05004], lr: 0.065155, loss: 5.0397
2022-10-11 06:06:16 - train: epoch 0041, iter [01100, 05004], lr: 0.065126, loss: 4.1727
2022-10-11 06:06:52 - train: epoch 0041, iter [01200, 05004], lr: 0.065096, loss: 4.7929
2022-10-11 06:07:28 - train: epoch 0041, iter [01300, 05004], lr: 0.065066, loss: 4.3024
2022-10-11 06:08:05 - train: epoch 0041, iter [01400, 05004], lr: 0.065036, loss: 4.3803
2022-10-11 06:08:42 - train: epoch 0041, iter [01500, 05004], lr: 0.065006, loss: 5.1146
2022-10-11 06:09:19 - train: epoch 0041, iter [01600, 05004], lr: 0.064976, loss: 4.1911
2022-10-11 06:09:56 - train: epoch 0041, iter [01700, 05004], lr: 0.064946, loss: 4.5984
2022-10-11 06:10:33 - train: epoch 0041, iter [01800, 05004], lr: 0.064916, loss: 4.7909
2022-10-11 06:11:10 - train: epoch 0041, iter [01900, 05004], lr: 0.064886, loss: 4.7664
2022-10-11 06:11:46 - train: epoch 0041, iter [02000, 05004], lr: 0.064856, loss: 5.0682
2022-10-11 06:12:23 - train: epoch 0041, iter [02100, 05004], lr: 0.064826, loss: 3.9433
2022-10-11 06:13:00 - train: epoch 0041, iter [02200, 05004], lr: 0.064796, loss: 3.8058
2022-10-11 06:13:36 - train: epoch 0041, iter [02300, 05004], lr: 0.064766, loss: 4.4343
2022-10-11 06:14:12 - train: epoch 0041, iter [02400, 05004], lr: 0.064736, loss: 3.8883
2022-10-11 06:14:49 - train: epoch 0041, iter [02500, 05004], lr: 0.064706, loss: 4.5015
2022-10-11 06:15:27 - train: epoch 0041, iter [02600, 05004], lr: 0.064676, loss: 4.0981
2022-10-11 06:16:04 - train: epoch 0041, iter [02700, 05004], lr: 0.064646, loss: 4.5954
2022-10-11 06:16:40 - train: epoch 0041, iter [02800, 05004], lr: 0.064616, loss: 5.0826
2022-10-11 06:17:18 - train: epoch 0041, iter [02900, 05004], lr: 0.064586, loss: 4.2620
2022-10-11 06:17:54 - train: epoch 0041, iter [03000, 05004], lr: 0.064556, loss: 4.5394
2022-10-11 06:18:31 - train: epoch 0041, iter [03100, 05004], lr: 0.064526, loss: 5.0404
2022-10-11 06:19:09 - train: epoch 0041, iter [03200, 05004], lr: 0.064496, loss: 4.0654
2022-10-11 06:19:45 - train: epoch 0041, iter [03300, 05004], lr: 0.064466, loss: 3.5213
2022-10-11 06:20:22 - train: epoch 0041, iter [03400, 05004], lr: 0.064436, loss: 3.6563
2022-10-11 06:21:00 - train: epoch 0041, iter [03500, 05004], lr: 0.064406, loss: 4.0411
2022-10-11 06:21:36 - train: epoch 0041, iter [03600, 05004], lr: 0.064376, loss: 5.0063
2022-10-11 06:22:13 - train: epoch 0041, iter [03700, 05004], lr: 0.064346, loss: 4.6327
2022-10-11 06:22:51 - train: epoch 0041, iter [03800, 05004], lr: 0.064316, loss: 4.3078
2022-10-11 06:23:26 - train: epoch 0041, iter [03900, 05004], lr: 0.064286, loss: 4.6683
2022-10-11 06:24:04 - train: epoch 0041, iter [04000, 05004], lr: 0.064256, loss: 4.1923
2022-10-11 06:24:40 - train: epoch 0041, iter [04100, 05004], lr: 0.064225, loss: 4.4586
2022-10-11 06:25:18 - train: epoch 0041, iter [04200, 05004], lr: 0.064195, loss: 4.9593
2022-10-11 06:25:55 - train: epoch 0041, iter [04300, 05004], lr: 0.064165, loss: 4.2051
2022-10-11 06:26:32 - train: epoch 0041, iter [04400, 05004], lr: 0.064135, loss: 5.0348
2022-10-11 06:27:09 - train: epoch 0041, iter [04500, 05004], lr: 0.064105, loss: 4.2977
2022-10-11 06:27:45 - train: epoch 0041, iter [04600, 05004], lr: 0.064075, loss: 4.8229
2022-10-11 06:28:22 - train: epoch 0041, iter [04700, 05004], lr: 0.064045, loss: 4.9478
2022-10-11 06:28:59 - train: epoch 0041, iter [04800, 05004], lr: 0.064015, loss: 3.9553
2022-10-11 06:29:37 - train: epoch 0041, iter [04900, 05004], lr: 0.063985, loss: 4.1676
2022-10-11 06:30:11 - train: epoch 0041, iter [05000, 05004], lr: 0.063954, loss: 4.7724
2022-10-11 06:30:13 - train: epoch 041, train_loss: 4.3915
2022-10-11 06:31:32 - eval: epoch: 041, acc1: 58.088%, acc5: 82.060%, test_loss: 2.0585, per_image_load_time: 2.235ms, per_image_inference_time: 0.605ms
2022-10-11 06:31:32 - until epoch: 041, best_acc1: 58.430%
2022-10-11 06:31:32 - epoch 042 lr: 0.063953
2022-10-11 06:32:15 - train: epoch 0042, iter [00100, 05004], lr: 0.063923, loss: 4.7671
2022-10-11 06:32:51 - train: epoch 0042, iter [00200, 05004], lr: 0.063893, loss: 4.3309
2022-10-11 06:33:27 - train: epoch 0042, iter [00300, 05004], lr: 0.063863, loss: 4.7226
2022-10-11 06:34:04 - train: epoch 0042, iter [00400, 05004], lr: 0.063833, loss: 3.7466
2022-10-11 06:34:39 - train: epoch 0042, iter [00500, 05004], lr: 0.063802, loss: 4.0250
2022-10-11 06:35:17 - train: epoch 0042, iter [00600, 05004], lr: 0.063772, loss: 4.2994
2022-10-11 06:35:53 - train: epoch 0042, iter [00700, 05004], lr: 0.063742, loss: 4.5241
2022-10-11 06:36:30 - train: epoch 0042, iter [00800, 05004], lr: 0.063712, loss: 4.3829
2022-10-11 06:37:08 - train: epoch 0042, iter [00900, 05004], lr: 0.063682, loss: 3.4899
2022-10-11 06:37:45 - train: epoch 0042, iter [01000, 05004], lr: 0.063651, loss: 4.9366
2022-10-11 06:38:20 - train: epoch 0042, iter [01100, 05004], lr: 0.063621, loss: 4.2942
2022-10-11 06:38:57 - train: epoch 0042, iter [01200, 05004], lr: 0.063591, loss: 3.8271
2022-10-11 06:39:34 - train: epoch 0042, iter [01300, 05004], lr: 0.063561, loss: 3.8087
2022-10-11 06:40:11 - train: epoch 0042, iter [01400, 05004], lr: 0.063531, loss: 4.9800
2022-10-11 06:40:48 - train: epoch 0042, iter [01500, 05004], lr: 0.063500, loss: 4.3960
2022-10-11 06:41:25 - train: epoch 0042, iter [01600, 05004], lr: 0.063470, loss: 4.5642
2022-10-11 06:42:01 - train: epoch 0042, iter [01700, 05004], lr: 0.063440, loss: 4.3857
2022-10-11 06:42:38 - train: epoch 0042, iter [01800, 05004], lr: 0.063410, loss: 4.7111
2022-10-11 06:43:15 - train: epoch 0042, iter [01900, 05004], lr: 0.063379, loss: 4.8814
2022-10-11 06:43:53 - train: epoch 0042, iter [02000, 05004], lr: 0.063349, loss: 4.8238
2022-10-11 06:44:29 - train: epoch 0042, iter [02100, 05004], lr: 0.063319, loss: 4.8144
2022-10-11 06:45:07 - train: epoch 0042, iter [02200, 05004], lr: 0.063289, loss: 4.7982
2022-10-11 06:45:42 - train: epoch 0042, iter [02300, 05004], lr: 0.063258, loss: 3.8573
2022-10-11 06:46:20 - train: epoch 0042, iter [02400, 05004], lr: 0.063228, loss: 4.7697
2022-10-11 06:46:56 - train: epoch 0042, iter [02500, 05004], lr: 0.063198, loss: 4.5939
2022-10-11 06:47:33 - train: epoch 0042, iter [02600, 05004], lr: 0.063168, loss: 4.1122
2022-10-11 06:48:10 - train: epoch 0042, iter [02700, 05004], lr: 0.063137, loss: 4.3727
2022-10-11 06:48:46 - train: epoch 0042, iter [02800, 05004], lr: 0.063107, loss: 4.6441
2022-10-11 06:49:24 - train: epoch 0042, iter [02900, 05004], lr: 0.063077, loss: 3.8547
2022-10-11 06:50:00 - train: epoch 0042, iter [03000, 05004], lr: 0.063046, loss: 3.9532
2022-10-11 06:50:36 - train: epoch 0042, iter [03100, 05004], lr: 0.063016, loss: 4.6315
2022-10-11 06:51:14 - train: epoch 0042, iter [03200, 05004], lr: 0.062986, loss: 4.6237
2022-10-11 06:51:49 - train: epoch 0042, iter [03300, 05004], lr: 0.062956, loss: 4.1261
2022-10-11 06:52:27 - train: epoch 0042, iter [03400, 05004], lr: 0.062925, loss: 4.7242
2022-10-11 06:53:04 - train: epoch 0042, iter [03500, 05004], lr: 0.062895, loss: 4.2706
2022-10-11 06:53:41 - train: epoch 0042, iter [03600, 05004], lr: 0.062865, loss: 3.9057
2022-10-11 06:54:17 - train: epoch 0042, iter [03700, 05004], lr: 0.062834, loss: 4.3250
2022-10-11 06:54:55 - train: epoch 0042, iter [03800, 05004], lr: 0.062804, loss: 4.5148
2022-10-11 06:55:32 - train: epoch 0042, iter [03900, 05004], lr: 0.062774, loss: 5.0124
2022-10-11 06:56:09 - train: epoch 0042, iter [04000, 05004], lr: 0.062743, loss: 4.3092
2022-10-11 06:56:45 - train: epoch 0042, iter [04100, 05004], lr: 0.062713, loss: 4.8545
2022-10-11 06:57:22 - train: epoch 0042, iter [04200, 05004], lr: 0.062683, loss: 4.6995
2022-10-11 06:57:58 - train: epoch 0042, iter [04300, 05004], lr: 0.062652, loss: 4.8389
2022-10-11 06:58:36 - train: epoch 0042, iter [04400, 05004], lr: 0.062622, loss: 3.4570
2022-10-11 06:59:12 - train: epoch 0042, iter [04500, 05004], lr: 0.062591, loss: 4.0310
2022-10-11 06:59:49 - train: epoch 0042, iter [04600, 05004], lr: 0.062561, loss: 4.0309
2022-10-11 07:00:26 - train: epoch 0042, iter [04700, 05004], lr: 0.062531, loss: 4.3026
2022-10-11 07:01:02 - train: epoch 0042, iter [04800, 05004], lr: 0.062500, loss: 4.1643
2022-10-11 07:01:39 - train: epoch 0042, iter [04900, 05004], lr: 0.062470, loss: 5.0616
2022-10-11 07:02:15 - train: epoch 0042, iter [05000, 05004], lr: 0.062439, loss: 4.2041
2022-10-11 07:02:17 - train: epoch 042, train_loss: 4.3787
2022-10-11 07:03:37 - eval: epoch: 042, acc1: 58.588%, acc5: 82.752%, test_loss: 2.0454, per_image_load_time: 2.388ms, per_image_inference_time: 0.566ms
2022-10-11 07:03:37 - until epoch: 042, best_acc1: 58.588%
2022-10-11 07:03:37 - epoch 043 lr: 0.062438
2022-10-11 07:04:20 - train: epoch 0043, iter [00100, 05004], lr: 0.062408, loss: 4.5896
2022-10-11 07:04:56 - train: epoch 0043, iter [00200, 05004], lr: 0.062377, loss: 4.6124
2022-10-11 07:05:32 - train: epoch 0043, iter [00300, 05004], lr: 0.062347, loss: 4.2050
2022-10-11 07:06:08 - train: epoch 0043, iter [00400, 05004], lr: 0.062317, loss: 4.2152
2022-10-11 07:06:45 - train: epoch 0043, iter [00500, 05004], lr: 0.062286, loss: 4.5754
2022-10-11 07:07:21 - train: epoch 0043, iter [00600, 05004], lr: 0.062256, loss: 4.4729
2022-10-11 07:07:57 - train: epoch 0043, iter [00700, 05004], lr: 0.062225, loss: 4.8343
2022-10-11 07:08:33 - train: epoch 0043, iter [00800, 05004], lr: 0.062195, loss: 4.8009
2022-10-11 07:09:10 - train: epoch 0043, iter [00900, 05004], lr: 0.062164, loss: 4.0242
2022-10-11 07:09:48 - train: epoch 0043, iter [01000, 05004], lr: 0.062134, loss: 4.7051
2022-10-11 07:10:26 - train: epoch 0043, iter [01100, 05004], lr: 0.062104, loss: 3.9907
2022-10-11 07:11:02 - train: epoch 0043, iter [01200, 05004], lr: 0.062073, loss: 4.0099
2022-10-11 07:11:39 - train: epoch 0043, iter [01300, 05004], lr: 0.062043, loss: 4.4999
2022-10-11 07:12:16 - train: epoch 0043, iter [01400, 05004], lr: 0.062012, loss: 4.8174
2022-10-11 07:12:52 - train: epoch 0043, iter [01500, 05004], lr: 0.061982, loss: 4.5223
2022-10-11 07:13:29 - train: epoch 0043, iter [01600, 05004], lr: 0.061951, loss: 4.7436
2022-10-11 07:14:06 - train: epoch 0043, iter [01700, 05004], lr: 0.061921, loss: 4.2260
2022-10-11 07:14:44 - train: epoch 0043, iter [01800, 05004], lr: 0.061890, loss: 4.5667
2022-10-11 07:15:20 - train: epoch 0043, iter [01900, 05004], lr: 0.061860, loss: 4.3282
2022-10-11 07:15:57 - train: epoch 0043, iter [02000, 05004], lr: 0.061829, loss: 4.0305
2022-10-11 07:16:33 - train: epoch 0043, iter [02100, 05004], lr: 0.061799, loss: 4.5331
2022-10-11 07:17:10 - train: epoch 0043, iter [02200, 05004], lr: 0.061768, loss: 5.0627
2022-10-11 07:17:46 - train: epoch 0043, iter [02300, 05004], lr: 0.061738, loss: 4.6610
2022-10-11 07:18:23 - train: epoch 0043, iter [02400, 05004], lr: 0.061707, loss: 3.9712
2022-10-11 07:19:00 - train: epoch 0043, iter [02500, 05004], lr: 0.061677, loss: 4.0012
2022-10-11 07:19:37 - train: epoch 0043, iter [02600, 05004], lr: 0.061646, loss: 4.6402
2022-10-11 07:20:14 - train: epoch 0043, iter [02700, 05004], lr: 0.061616, loss: 4.1149
2022-10-11 07:20:52 - train: epoch 0043, iter [02800, 05004], lr: 0.061585, loss: 4.4087
2022-10-11 07:21:29 - train: epoch 0043, iter [02900, 05004], lr: 0.061555, loss: 4.1471
2022-10-11 07:22:05 - train: epoch 0043, iter [03000, 05004], lr: 0.061524, loss: 4.4485
2022-10-11 07:22:42 - train: epoch 0043, iter [03100, 05004], lr: 0.061494, loss: 4.0395
2022-10-11 07:23:19 - train: epoch 0043, iter [03200, 05004], lr: 0.061463, loss: 3.7275
2022-10-11 07:23:56 - train: epoch 0043, iter [03300, 05004], lr: 0.061432, loss: 4.3835
2022-10-11 07:24:33 - train: epoch 0043, iter [03400, 05004], lr: 0.061402, loss: 4.1543
2022-10-11 07:25:10 - train: epoch 0043, iter [03500, 05004], lr: 0.061371, loss: 4.8605
2022-10-11 07:25:47 - train: epoch 0043, iter [03600, 05004], lr: 0.061341, loss: 4.1785
2022-10-11 07:26:23 - train: epoch 0043, iter [03700, 05004], lr: 0.061310, loss: 4.3832
2022-10-11 07:27:00 - train: epoch 0043, iter [03800, 05004], lr: 0.061280, loss: 4.6235
2022-10-11 07:27:37 - train: epoch 0043, iter [03900, 05004], lr: 0.061249, loss: 3.9410
2022-10-11 07:28:15 - train: epoch 0043, iter [04000, 05004], lr: 0.061218, loss: 4.1593
2022-10-11 07:28:52 - train: epoch 0043, iter [04100, 05004], lr: 0.061188, loss: 4.5736
2022-10-11 07:29:28 - train: epoch 0043, iter [04200, 05004], lr: 0.061157, loss: 4.6916
2022-10-11 07:30:04 - train: epoch 0043, iter [04300, 05004], lr: 0.061127, loss: 4.1928
2022-10-11 07:30:43 - train: epoch 0043, iter [04400, 05004], lr: 0.061096, loss: 4.6083
2022-10-11 07:31:21 - train: epoch 0043, iter [04500, 05004], lr: 0.061065, loss: 4.5803
2022-10-11 07:31:58 - train: epoch 0043, iter [04600, 05004], lr: 0.061035, loss: 4.1388
2022-10-11 07:32:35 - train: epoch 0043, iter [04700, 05004], lr: 0.061004, loss: 4.0409
2022-10-11 07:33:12 - train: epoch 0043, iter [04800, 05004], lr: 0.060974, loss: 3.2766
2022-10-11 07:33:49 - train: epoch 0043, iter [04900, 05004], lr: 0.060943, loss: 4.3220
2022-10-11 07:34:24 - train: epoch 0043, iter [05000, 05004], lr: 0.060912, loss: 4.4527
2022-10-11 07:34:26 - train: epoch 043, train_loss: 4.3756
2022-10-11 07:35:44 - eval: epoch: 043, acc1: 56.760%, acc5: 81.108%, test_loss: 2.1292, per_image_load_time: 2.232ms, per_image_inference_time: 0.619ms
2022-10-11 07:35:45 - until epoch: 043, best_acc1: 58.588%
2022-10-11 07:35:45 - epoch 044 lr: 0.060911
2022-10-11 07:36:28 - train: epoch 0044, iter [00100, 05004], lr: 0.060880, loss: 4.7821
2022-10-11 07:37:04 - train: epoch 0044, iter [00200, 05004], lr: 0.060850, loss: 4.7437
2022-10-11 07:37:41 - train: epoch 0044, iter [00300, 05004], lr: 0.060819, loss: 4.9167
2022-10-11 07:38:17 - train: epoch 0044, iter [00400, 05004], lr: 0.060789, loss: 4.7070
2022-10-11 07:38:54 - train: epoch 0044, iter [00500, 05004], lr: 0.060758, loss: 4.1479
2022-10-11 07:39:30 - train: epoch 0044, iter [00600, 05004], lr: 0.060727, loss: 4.8103
2022-10-11 07:40:07 - train: epoch 0044, iter [00700, 05004], lr: 0.060697, loss: 3.6782
2022-10-11 07:40:44 - train: epoch 0044, iter [00800, 05004], lr: 0.060666, loss: 4.4476
2022-10-11 07:41:21 - train: epoch 0044, iter [00900, 05004], lr: 0.060635, loss: 4.4802
2022-10-11 07:41:56 - train: epoch 0044, iter [01000, 05004], lr: 0.060605, loss: 4.4389
2022-10-11 07:42:33 - train: epoch 0044, iter [01100, 05004], lr: 0.060574, loss: 3.3259
2022-10-11 07:43:11 - train: epoch 0044, iter [01200, 05004], lr: 0.060543, loss: 4.3136
2022-10-11 07:43:46 - train: epoch 0044, iter [01300, 05004], lr: 0.060512, loss: 4.6191
2022-10-11 07:44:23 - train: epoch 0044, iter [01400, 05004], lr: 0.060482, loss: 4.2977
2022-10-11 07:45:01 - train: epoch 0044, iter [01500, 05004], lr: 0.060451, loss: 4.1019
2022-10-11 07:45:38 - train: epoch 0044, iter [01600, 05004], lr: 0.060420, loss: 4.6822
2022-10-11 07:46:15 - train: epoch 0044, iter [01700, 05004], lr: 0.060390, loss: 5.0168
2022-10-11 07:46:51 - train: epoch 0044, iter [01800, 05004], lr: 0.060359, loss: 3.5737
2022-10-11 07:47:28 - train: epoch 0044, iter [01900, 05004], lr: 0.060328, loss: 4.6326
2022-10-11 07:48:06 - train: epoch 0044, iter [02000, 05004], lr: 0.060298, loss: 4.0233
2022-10-11 07:48:42 - train: epoch 0044, iter [02100, 05004], lr: 0.060267, loss: 4.5520
2022-10-11 07:49:19 - train: epoch 0044, iter [02200, 05004], lr: 0.060236, loss: 4.8470
2022-10-11 07:49:55 - train: epoch 0044, iter [02300, 05004], lr: 0.060205, loss: 3.9560
2022-10-11 07:50:32 - train: epoch 0044, iter [02400, 05004], lr: 0.060175, loss: 4.6057
2022-10-11 07:51:10 - train: epoch 0044, iter [02500, 05004], lr: 0.060144, loss: 4.4063
2022-10-11 07:51:47 - train: epoch 0044, iter [02600, 05004], lr: 0.060113, loss: 3.9007
2022-10-11 07:52:25 - train: epoch 0044, iter [02700, 05004], lr: 0.060082, loss: 4.6168
2022-10-11 07:53:01 - train: epoch 0044, iter [02800, 05004], lr: 0.060052, loss: 4.7266
2022-10-11 07:53:38 - train: epoch 0044, iter [02900, 05004], lr: 0.060021, loss: 4.5006
2022-10-11 07:54:15 - train: epoch 0044, iter [03000, 05004], lr: 0.059990, loss: 4.4276
2022-10-11 07:54:53 - train: epoch 0044, iter [03100, 05004], lr: 0.059959, loss: 4.8370
2022-10-11 07:55:29 - train: epoch 0044, iter [03200, 05004], lr: 0.059929, loss: 4.9160
2022-10-11 07:56:05 - train: epoch 0044, iter [03300, 05004], lr: 0.059898, loss: 4.5144
2022-10-11 07:56:43 - train: epoch 0044, iter [03400, 05004], lr: 0.059867, loss: 3.4342
2022-10-11 07:57:20 - train: epoch 0044, iter [03500, 05004], lr: 0.059836, loss: 3.9094
2022-10-11 07:57:57 - train: epoch 0044, iter [03600, 05004], lr: 0.059806, loss: 4.2072
2022-10-11 07:58:33 - train: epoch 0044, iter [03700, 05004], lr: 0.059775, loss: 3.6712
2022-10-11 07:59:11 - train: epoch 0044, iter [03800, 05004], lr: 0.059744, loss: 3.9178
2022-10-11 07:59:47 - train: epoch 0044, iter [03900, 05004], lr: 0.059713, loss: 4.1569
2022-10-11 08:00:23 - train: epoch 0044, iter [04000, 05004], lr: 0.059682, loss: 3.9004
2022-10-11 08:01:01 - train: epoch 0044, iter [04100, 05004], lr: 0.059652, loss: 4.7527
2022-10-11 08:01:38 - train: epoch 0044, iter [04200, 05004], lr: 0.059621, loss: 4.7831
2022-10-11 08:02:14 - train: epoch 0044, iter [04300, 05004], lr: 0.059590, loss: 4.3069
2022-10-11 08:02:52 - train: epoch 0044, iter [04400, 05004], lr: 0.059559, loss: 3.8920
2022-10-11 08:03:29 - train: epoch 0044, iter [04500, 05004], lr: 0.059528, loss: 4.8618
2022-10-11 08:04:06 - train: epoch 0044, iter [04600, 05004], lr: 0.059498, loss: 4.8191
2022-10-11 08:04:43 - train: epoch 0044, iter [04700, 05004], lr: 0.059467, loss: 4.7364
2022-10-11 08:05:20 - train: epoch 0044, iter [04800, 05004], lr: 0.059436, loss: 4.7308
2022-10-11 08:05:57 - train: epoch 0044, iter [04900, 05004], lr: 0.059405, loss: 4.2839
2022-10-11 08:06:32 - train: epoch 0044, iter [05000, 05004], lr: 0.059374, loss: 4.1813
2022-10-11 08:06:34 - train: epoch 044, train_loss: 4.3596
2022-10-11 08:07:53 - eval: epoch: 044, acc1: 58.980%, acc5: 82.830%, test_loss: 1.9900, per_image_load_time: 0.658ms, per_image_inference_time: 0.637ms
2022-10-11 08:07:54 - until epoch: 044, best_acc1: 58.980%
2022-10-11 08:07:54 - epoch 045 lr: 0.059373
2022-10-11 08:08:36 - train: epoch 0045, iter [00100, 05004], lr: 0.059342, loss: 4.3404
2022-10-11 08:09:14 - train: epoch 0045, iter [00200, 05004], lr: 0.059311, loss: 4.5814
2022-10-11 08:09:51 - train: epoch 0045, iter [00300, 05004], lr: 0.059281, loss: 4.7716
2022-10-11 08:10:28 - train: epoch 0045, iter [00400, 05004], lr: 0.059250, loss: 3.8189
2022-10-11 08:11:04 - train: epoch 0045, iter [00500, 05004], lr: 0.059219, loss: 4.7268
2022-10-11 08:11:40 - train: epoch 0045, iter [00600, 05004], lr: 0.059188, loss: 5.3722
2022-10-11 08:12:17 - train: epoch 0045, iter [00700, 05004], lr: 0.059157, loss: 4.4823
2022-10-11 08:12:54 - train: epoch 0045, iter [00800, 05004], lr: 0.059126, loss: 4.9208
2022-10-11 08:13:32 - train: epoch 0045, iter [00900, 05004], lr: 0.059095, loss: 3.7478
2022-10-11 08:14:06 - train: epoch 0045, iter [01000, 05004], lr: 0.059065, loss: 4.0641
2022-10-11 08:14:43 - train: epoch 0045, iter [01100, 05004], lr: 0.059034, loss: 4.4085
2022-10-11 08:15:20 - train: epoch 0045, iter [01200, 05004], lr: 0.059003, loss: 4.5677
2022-10-11 08:15:57 - train: epoch 0045, iter [01300, 05004], lr: 0.058972, loss: 4.4904
2022-10-11 08:16:35 - train: epoch 0045, iter [01400, 05004], lr: 0.058941, loss: 4.1091
2022-10-11 08:17:11 - train: epoch 0045, iter [01500, 05004], lr: 0.058910, loss: 4.4939
2022-10-11 08:17:49 - train: epoch 0045, iter [01600, 05004], lr: 0.058879, loss: 4.3467
2022-10-11 08:18:25 - train: epoch 0045, iter [01700, 05004], lr: 0.058848, loss: 4.5795
2022-10-11 08:19:03 - train: epoch 0045, iter [01800, 05004], lr: 0.058818, loss: 4.4358
2022-10-11 08:19:39 - train: epoch 0045, iter [01900, 05004], lr: 0.058787, loss: 3.7723
2022-10-11 08:20:17 - train: epoch 0045, iter [02000, 05004], lr: 0.058756, loss: 4.7851
2022-10-11 08:20:54 - train: epoch 0045, iter [02100, 05004], lr: 0.058725, loss: 4.8149
2022-10-11 08:21:31 - train: epoch 0045, iter [02200, 05004], lr: 0.058694, loss: 3.7360
2022-10-11 08:22:08 - train: epoch 0045, iter [02300, 05004], lr: 0.058663, loss: 3.8057
2022-10-11 08:22:45 - train: epoch 0045, iter [02400, 05004], lr: 0.058632, loss: 5.2053
2022-10-11 08:23:21 - train: epoch 0045, iter [02500, 05004], lr: 0.058601, loss: 4.2102
2022-10-11 08:23:59 - train: epoch 0045, iter [02600, 05004], lr: 0.058570, loss: 4.7076
2022-10-11 08:24:36 - train: epoch 0045, iter [02700, 05004], lr: 0.058539, loss: 4.3359
2022-10-11 08:25:14 - train: epoch 0045, iter [02800, 05004], lr: 0.058508, loss: 3.8691
2022-10-11 08:25:50 - train: epoch 0045, iter [02900, 05004], lr: 0.058478, loss: 4.7366
2022-10-11 08:26:27 - train: epoch 0045, iter [03000, 05004], lr: 0.058447, loss: 3.5823
2022-10-11 08:27:05 - train: epoch 0045, iter [03100, 05004], lr: 0.058416, loss: 4.6943
2022-10-11 08:27:41 - train: epoch 0045, iter [03200, 05004], lr: 0.058385, loss: 4.8603
2022-10-11 08:28:19 - train: epoch 0045, iter [03300, 05004], lr: 0.058354, loss: 4.7841
2022-10-11 08:28:55 - train: epoch 0045, iter [03400, 05004], lr: 0.058323, loss: 4.5851
2022-10-11 08:29:32 - train: epoch 0045, iter [03500, 05004], lr: 0.058292, loss: 4.3709
2022-10-11 08:30:09 - train: epoch 0045, iter [03600, 05004], lr: 0.058261, loss: 4.4831
2022-10-11 08:30:45 - train: epoch 0045, iter [03700, 05004], lr: 0.058230, loss: 4.7373
2022-10-11 08:31:21 - train: epoch 0045, iter [03800, 05004], lr: 0.058199, loss: 4.4866
2022-10-11 08:31:59 - train: epoch 0045, iter [03900, 05004], lr: 0.058168, loss: 4.6722
2022-10-11 08:32:36 - train: epoch 0045, iter [04000, 05004], lr: 0.058137, loss: 3.4530
2022-10-11 08:33:13 - train: epoch 0045, iter [04100, 05004], lr: 0.058106, loss: 4.4996
2022-10-11 08:33:50 - train: epoch 0045, iter [04200, 05004], lr: 0.058075, loss: 3.9693
2022-10-11 08:34:27 - train: epoch 0045, iter [04300, 05004], lr: 0.058044, loss: 4.9400
2022-10-11 08:35:03 - train: epoch 0045, iter [04400, 05004], lr: 0.058013, loss: 4.0529
2022-10-11 08:35:40 - train: epoch 0045, iter [04500, 05004], lr: 0.057982, loss: 4.6732
2022-10-11 08:36:17 - train: epoch 0045, iter [04600, 05004], lr: 0.057951, loss: 5.0218
2022-10-11 08:36:53 - train: epoch 0045, iter [04700, 05004], lr: 0.057920, loss: 4.4509
2022-10-11 08:37:31 - train: epoch 0045, iter [04800, 05004], lr: 0.057889, loss: 4.1294
2022-10-11 08:38:08 - train: epoch 0045, iter [04900, 05004], lr: 0.057858, loss: 4.6140
2022-10-11 08:38:43 - train: epoch 0045, iter [05000, 05004], lr: 0.057827, loss: 5.1825
2022-10-11 08:38:45 - train: epoch 045, train_loss: 4.3414
2022-10-11 08:40:05 - eval: epoch: 045, acc1: 58.154%, acc5: 82.366%, test_loss: 2.1341, per_image_load_time: 0.791ms, per_image_inference_time: 0.619ms
2022-10-11 08:40:05 - until epoch: 045, best_acc1: 58.980%
2022-10-11 08:40:05 - epoch 046 lr: 0.057826
2022-10-11 08:40:48 - train: epoch 0046, iter [00100, 05004], lr: 0.057795, loss: 4.7716
2022-10-11 08:41:25 - train: epoch 0046, iter [00200, 05004], lr: 0.057764, loss: 3.5434
2022-10-11 08:42:02 - train: epoch 0046, iter [00300, 05004], lr: 0.057733, loss: 4.3171
2022-10-11 08:42:38 - train: epoch 0046, iter [00400, 05004], lr: 0.057702, loss: 3.8251
2022-10-11 08:43:14 - train: epoch 0046, iter [00500, 05004], lr: 0.057671, loss: 4.1978
2022-10-11 08:43:51 - train: epoch 0046, iter [00600, 05004], lr: 0.057640, loss: 3.7314
2022-10-11 08:44:28 - train: epoch 0046, iter [00700, 05004], lr: 0.057609, loss: 4.7031
2022-10-11 08:45:05 - train: epoch 0046, iter [00800, 05004], lr: 0.057578, loss: 4.4943
2022-10-11 08:45:40 - train: epoch 0046, iter [00900, 05004], lr: 0.057547, loss: 4.1150
2022-10-11 08:46:18 - train: epoch 0046, iter [01000, 05004], lr: 0.057516, loss: 4.2140
2022-10-11 08:46:54 - train: epoch 0046, iter [01100, 05004], lr: 0.057485, loss: 4.5976
2022-10-11 08:47:30 - train: epoch 0046, iter [01200, 05004], lr: 0.057454, loss: 3.8060
2022-10-11 08:48:07 - train: epoch 0046, iter [01300, 05004], lr: 0.057423, loss: 4.5575
2022-10-11 08:48:44 - train: epoch 0046, iter [01400, 05004], lr: 0.057392, loss: 5.0829
2022-10-11 08:49:21 - train: epoch 0046, iter [01500, 05004], lr: 0.057361, loss: 4.3069
2022-10-11 08:49:57 - train: epoch 0046, iter [01600, 05004], lr: 0.057330, loss: 4.5363
2022-10-11 08:50:34 - train: epoch 0046, iter [01700, 05004], lr: 0.057298, loss: 3.9925
2022-10-11 08:51:11 - train: epoch 0046, iter [01800, 05004], lr: 0.057267, loss: 4.7646
2022-10-11 08:51:48 - train: epoch 0046, iter [01900, 05004], lr: 0.057236, loss: 4.5009
2022-10-11 08:52:25 - train: epoch 0046, iter [02000, 05004], lr: 0.057205, loss: 4.5118
2022-10-11 08:53:01 - train: epoch 0046, iter [02100, 05004], lr: 0.057174, loss: 4.8951
2022-10-11 08:53:38 - train: epoch 0046, iter [02200, 05004], lr: 0.057143, loss: 4.3776
2022-10-11 08:54:14 - train: epoch 0046, iter [02300, 05004], lr: 0.057112, loss: 4.6520
2022-10-11 08:54:51 - train: epoch 0046, iter [02400, 05004], lr: 0.057081, loss: 4.3649
2022-10-11 08:55:28 - train: epoch 0046, iter [02500, 05004], lr: 0.057050, loss: 4.9475
2022-10-11 08:56:06 - train: epoch 0046, iter [02600, 05004], lr: 0.057019, loss: 4.3907
2022-10-11 08:56:43 - train: epoch 0046, iter [02700, 05004], lr: 0.056988, loss: 4.7841
2022-10-11 08:57:18 - train: epoch 0046, iter [02800, 05004], lr: 0.056957, loss: 4.3692
2022-10-11 08:57:56 - train: epoch 0046, iter [02900, 05004], lr: 0.056926, loss: 4.7927
2022-10-11 08:58:33 - train: epoch 0046, iter [03000, 05004], lr: 0.056895, loss: 3.3405
2022-10-11 08:59:10 - train: epoch 0046, iter [03100, 05004], lr: 0.056863, loss: 5.1367
2022-10-11 08:59:46 - train: epoch 0046, iter [03200, 05004], lr: 0.056832, loss: 3.9955
2022-10-11 09:00:23 - train: epoch 0046, iter [03300, 05004], lr: 0.056801, loss: 4.9738
2022-10-11 09:01:00 - train: epoch 0046, iter [03400, 05004], lr: 0.056770, loss: 4.4258
2022-10-11 09:01:36 - train: epoch 0046, iter [03500, 05004], lr: 0.056739, loss: 3.7842
2022-10-11 09:02:13 - train: epoch 0046, iter [03600, 05004], lr: 0.056708, loss: 4.7400
2022-10-11 09:02:50 - train: epoch 0046, iter [03700, 05004], lr: 0.056677, loss: 4.2423
2022-10-11 09:03:27 - train: epoch 0046, iter [03800, 05004], lr: 0.056646, loss: 4.4168
2022-10-11 09:04:04 - train: epoch 0046, iter [03900, 05004], lr: 0.056615, loss: 3.3485
2022-10-11 09:04:41 - train: epoch 0046, iter [04000, 05004], lr: 0.056584, loss: 3.6534
2022-10-11 09:05:18 - train: epoch 0046, iter [04100, 05004], lr: 0.056552, loss: 3.9559
2022-10-11 09:05:54 - train: epoch 0046, iter [04200, 05004], lr: 0.056521, loss: 4.4968
2022-10-11 09:06:31 - train: epoch 0046, iter [04300, 05004], lr: 0.056490, loss: 4.8081
2022-10-11 09:07:08 - train: epoch 0046, iter [04400, 05004], lr: 0.056459, loss: 4.9560
2022-10-11 09:07:46 - train: epoch 0046, iter [04500, 05004], lr: 0.056428, loss: 3.9797
2022-10-11 09:08:22 - train: epoch 0046, iter [04600, 05004], lr: 0.056397, loss: 4.2540
2022-10-11 09:08:59 - train: epoch 0046, iter [04700, 05004], lr: 0.056366, loss: 4.5508
2022-10-11 09:09:36 - train: epoch 0046, iter [04800, 05004], lr: 0.056335, loss: 4.5585
2022-10-11 09:10:13 - train: epoch 0046, iter [04900, 05004], lr: 0.056303, loss: 4.1295
2022-10-11 09:10:48 - train: epoch 0046, iter [05000, 05004], lr: 0.056272, loss: 4.7785
2022-10-11 09:10:50 - train: epoch 046, train_loss: 4.3345
2022-10-11 09:12:09 - eval: epoch: 046, acc1: 59.534%, acc5: 83.330%, test_loss: 1.9847, per_image_load_time: 0.292ms, per_image_inference_time: 0.616ms
2022-10-11 09:12:10 - until epoch: 046, best_acc1: 59.534%
2022-10-11 09:12:10 - epoch 047 lr: 0.056271
2022-10-11 09:12:53 - train: epoch 0047, iter [00100, 05004], lr: 0.056240, loss: 4.9324
2022-10-11 09:13:30 - train: epoch 0047, iter [00200, 05004], lr: 0.056209, loss: 4.5044
2022-10-11 09:14:06 - train: epoch 0047, iter [00300, 05004], lr: 0.056178, loss: 4.2681
2022-10-11 09:14:42 - train: epoch 0047, iter [00400, 05004], lr: 0.056146, loss: 4.0233
2022-10-11 09:15:18 - train: epoch 0047, iter [00500, 05004], lr: 0.056115, loss: 4.0165
2022-10-11 09:15:56 - train: epoch 0047, iter [00600, 05004], lr: 0.056084, loss: 4.9665
2022-10-11 09:16:34 - train: epoch 0047, iter [00700, 05004], lr: 0.056053, loss: 5.1350
2022-10-11 09:17:09 - train: epoch 0047, iter [00800, 05004], lr: 0.056022, loss: 4.5222
2022-10-11 09:17:46 - train: epoch 0047, iter [00900, 05004], lr: 0.055991, loss: 3.7073
2022-10-11 09:18:22 - train: epoch 0047, iter [01000, 05004], lr: 0.055960, loss: 5.0575
2022-10-11 09:18:58 - train: epoch 0047, iter [01100, 05004], lr: 0.055928, loss: 4.7180
2022-10-11 09:19:35 - train: epoch 0047, iter [01200, 05004], lr: 0.055897, loss: 4.5095
2022-10-11 09:20:12 - train: epoch 0047, iter [01300, 05004], lr: 0.055866, loss: 4.1835
2022-10-11 09:20:49 - train: epoch 0047, iter [01400, 05004], lr: 0.055835, loss: 4.3973
2022-10-11 09:21:25 - train: epoch 0047, iter [01500, 05004], lr: 0.055804, loss: 5.1072
2022-10-11 09:22:01 - train: epoch 0047, iter [01600, 05004], lr: 0.055772, loss: 4.5308
2022-10-11 09:22:39 - train: epoch 0047, iter [01700, 05004], lr: 0.055741, loss: 4.4805
2022-10-11 09:23:14 - train: epoch 0047, iter [01800, 05004], lr: 0.055710, loss: 3.9040
2022-10-11 09:23:52 - train: epoch 0047, iter [01900, 05004], lr: 0.055679, loss: 3.7120
2022-10-11 09:24:29 - train: epoch 0047, iter [02000, 05004], lr: 0.055648, loss: 4.2848
2022-10-11 09:25:06 - train: epoch 0047, iter [02100, 05004], lr: 0.055617, loss: 4.9293
2022-10-11 09:25:42 - train: epoch 0047, iter [02200, 05004], lr: 0.055585, loss: 4.5989
2022-10-11 09:26:19 - train: epoch 0047, iter [02300, 05004], lr: 0.055554, loss: 4.3205
2022-10-11 09:26:56 - train: epoch 0047, iter [02400, 05004], lr: 0.055523, loss: 4.0231
2022-10-11 09:27:33 - train: epoch 0047, iter [02500, 05004], lr: 0.055492, loss: 4.8297
2022-10-11 09:28:10 - train: epoch 0047, iter [02600, 05004], lr: 0.055461, loss: 3.6158
2022-10-11 09:28:47 - train: epoch 0047, iter [02700, 05004], lr: 0.055429, loss: 4.0753
2022-10-11 09:29:23 - train: epoch 0047, iter [02800, 05004], lr: 0.055398, loss: 4.4705
2022-10-11 09:30:00 - train: epoch 0047, iter [02900, 05004], lr: 0.055367, loss: 4.2508
2022-10-11 09:30:37 - train: epoch 0047, iter [03000, 05004], lr: 0.055336, loss: 4.0893
2022-10-11 09:31:14 - train: epoch 0047, iter [03100, 05004], lr: 0.055305, loss: 3.8698
2022-10-11 09:31:50 - train: epoch 0047, iter [03200, 05004], lr: 0.055273, loss: 3.7450
2022-10-11 09:32:27 - train: epoch 0047, iter [03300, 05004], lr: 0.055242, loss: 3.2241
2022-10-11 09:33:05 - train: epoch 0047, iter [03400, 05004], lr: 0.055211, loss: 4.3498
2022-10-11 09:33:42 - train: epoch 0047, iter [03500, 05004], lr: 0.055180, loss: 4.2989
2022-10-11 09:34:18 - train: epoch 0047, iter [03600, 05004], lr: 0.055148, loss: 4.1744
2022-10-11 09:34:55 - train: epoch 0047, iter [03700, 05004], lr: 0.055117, loss: 4.9262
2022-10-11 09:35:33 - train: epoch 0047, iter [03800, 05004], lr: 0.055086, loss: 3.9501
2022-10-11 09:36:09 - train: epoch 0047, iter [03900, 05004], lr: 0.055055, loss: 4.2302
2022-10-11 09:36:45 - train: epoch 0047, iter [04000, 05004], lr: 0.055024, loss: 5.2115
2022-10-11 09:37:23 - train: epoch 0047, iter [04100, 05004], lr: 0.054992, loss: 3.8754
2022-10-11 09:37:59 - train: epoch 0047, iter [04200, 05004], lr: 0.054961, loss: 4.2676
2022-10-11 09:38:37 - train: epoch 0047, iter [04300, 05004], lr: 0.054930, loss: 3.0565
2022-10-11 09:39:13 - train: epoch 0047, iter [04400, 05004], lr: 0.054899, loss: 5.0696
2022-10-11 09:39:51 - train: epoch 0047, iter [04500, 05004], lr: 0.054867, loss: 4.6871
2022-10-11 09:40:27 - train: epoch 0047, iter [04600, 05004], lr: 0.054836, loss: 4.7523
2022-10-11 09:41:05 - train: epoch 0047, iter [04700, 05004], lr: 0.054805, loss: 3.5398
2022-10-11 09:41:41 - train: epoch 0047, iter [04800, 05004], lr: 0.054774, loss: 3.8766
2022-10-11 09:42:19 - train: epoch 0047, iter [04900, 05004], lr: 0.054742, loss: 4.6209
2022-10-11 09:42:54 - train: epoch 0047, iter [05000, 05004], lr: 0.054711, loss: 4.2237
2022-10-11 09:42:56 - train: epoch 047, train_loss: 4.3188
2022-10-11 09:44:15 - eval: epoch: 047, acc1: 59.618%, acc5: 83.524%, test_loss: 1.9896, per_image_load_time: 0.299ms, per_image_inference_time: 0.584ms
2022-10-11 09:44:15 - until epoch: 047, best_acc1: 59.618%
2022-10-11 09:44:15 - epoch 048 lr: 0.054710
2022-10-11 09:44:58 - train: epoch 0048, iter [00100, 05004], lr: 0.054679, loss: 4.7734
2022-10-11 09:45:35 - train: epoch 0048, iter [00200, 05004], lr: 0.054647, loss: 4.3778
2022-10-11 09:46:11 - train: epoch 0048, iter [00300, 05004], lr: 0.054616, loss: 4.7412
2022-10-11 09:46:47 - train: epoch 0048, iter [00400, 05004], lr: 0.054585, loss: 4.4752
2022-10-11 09:47:23 - train: epoch 0048, iter [00500, 05004], lr: 0.054554, loss: 4.9249
2022-10-11 09:48:00 - train: epoch 0048, iter [00600, 05004], lr: 0.054522, loss: 3.6867
2022-10-11 09:48:36 - train: epoch 0048, iter [00700, 05004], lr: 0.054491, loss: 3.6266
2022-10-11 09:49:14 - train: epoch 0048, iter [00800, 05004], lr: 0.054460, loss: 4.2802
2022-10-11 09:49:51 - train: epoch 0048, iter [00900, 05004], lr: 0.054429, loss: 4.5659
2022-10-11 09:50:27 - train: epoch 0048, iter [01000, 05004], lr: 0.054397, loss: 4.8833
2022-10-11 09:51:05 - train: epoch 0048, iter [01100, 05004], lr: 0.054366, loss: 4.8172
2022-10-11 09:51:41 - train: epoch 0048, iter [01200, 05004], lr: 0.054335, loss: 4.0389
2022-10-11 09:52:17 - train: epoch 0048, iter [01300, 05004], lr: 0.054304, loss: 4.2705
2022-10-11 09:52:55 - train: epoch 0048, iter [01400, 05004], lr: 0.054272, loss: 4.5808
2022-10-11 09:53:31 - train: epoch 0048, iter [01500, 05004], lr: 0.054241, loss: 3.3194
2022-10-11 09:54:07 - train: epoch 0048, iter [01600, 05004], lr: 0.054210, loss: 4.8570
2022-10-11 09:54:45 - train: epoch 0048, iter [01700, 05004], lr: 0.054178, loss: 4.5964
2022-10-11 09:55:21 - train: epoch 0048, iter [01800, 05004], lr: 0.054147, loss: 4.3072
2022-10-11 09:55:58 - train: epoch 0048, iter [01900, 05004], lr: 0.054116, loss: 4.3033
2022-10-11 09:56:34 - train: epoch 0048, iter [02000, 05004], lr: 0.054085, loss: 4.2112
2022-10-11 09:57:12 - train: epoch 0048, iter [02100, 05004], lr: 0.054053, loss: 4.7576
2022-10-11 09:57:49 - train: epoch 0048, iter [02200, 05004], lr: 0.054022, loss: 5.0572
2022-10-11 09:58:25 - train: epoch 0048, iter [02300, 05004], lr: 0.053991, loss: 3.6672
2022-10-11 09:59:02 - train: epoch 0048, iter [02400, 05004], lr: 0.053959, loss: 4.5954
2022-10-11 09:59:39 - train: epoch 0048, iter [02500, 05004], lr: 0.053928, loss: 5.0407
2022-10-11 10:00:15 - train: epoch 0048, iter [02600, 05004], lr: 0.053897, loss: 4.7388
2022-10-11 10:00:52 - train: epoch 0048, iter [02700, 05004], lr: 0.053866, loss: 4.4796
2022-10-11 10:01:30 - train: epoch 0048, iter [02800, 05004], lr: 0.053834, loss: 4.5897
2022-10-11 10:02:07 - train: epoch 0048, iter [02900, 05004], lr: 0.053803, loss: 4.7992
2022-10-11 10:02:43 - train: epoch 0048, iter [03000, 05004], lr: 0.053772, loss: 4.9196
2022-10-11 10:03:21 - train: epoch 0048, iter [03100, 05004], lr: 0.053740, loss: 4.2002
2022-10-11 10:03:58 - train: epoch 0048, iter [03200, 05004], lr: 0.053709, loss: 4.0644
2022-10-11 10:04:35 - train: epoch 0048, iter [03300, 05004], lr: 0.053678, loss: 4.3972
2022-10-11 10:05:11 - train: epoch 0048, iter [03400, 05004], lr: 0.053647, loss: 4.3502
2022-10-11 10:05:48 - train: epoch 0048, iter [03500, 05004], lr: 0.053615, loss: 4.0598
2022-10-11 10:06:24 - train: epoch 0048, iter [03600, 05004], lr: 0.053584, loss: 4.5183
2022-10-11 10:07:01 - train: epoch 0048, iter [03700, 05004], lr: 0.053553, loss: 4.4352
2022-10-11 10:07:39 - train: epoch 0048, iter [03800, 05004], lr: 0.053521, loss: 3.8829
2022-10-11 10:08:15 - train: epoch 0048, iter [03900, 05004], lr: 0.053490, loss: 4.2388
2022-10-11 10:08:52 - train: epoch 0048, iter [04000, 05004], lr: 0.053459, loss: 4.8012
2022-10-11 10:09:30 - train: epoch 0048, iter [04100, 05004], lr: 0.053427, loss: 5.0106
2022-10-11 10:10:07 - train: epoch 0048, iter [04200, 05004], lr: 0.053396, loss: 4.2190
2022-10-11 10:10:44 - train: epoch 0048, iter [04300, 05004], lr: 0.053365, loss: 4.1836
2022-10-11 10:11:20 - train: epoch 0048, iter [04400, 05004], lr: 0.053333, loss: 4.9312
2022-10-11 10:11:58 - train: epoch 0048, iter [04500, 05004], lr: 0.053302, loss: 4.2224
2022-10-11 10:12:35 - train: epoch 0048, iter [04600, 05004], lr: 0.053271, loss: 4.4641
2022-10-11 10:13:12 - train: epoch 0048, iter [04700, 05004], lr: 0.053239, loss: 4.4055
2022-10-11 10:13:48 - train: epoch 0048, iter [04800, 05004], lr: 0.053208, loss: 5.0085
2022-10-11 10:14:27 - train: epoch 0048, iter [04900, 05004], lr: 0.053177, loss: 4.3660
2022-10-11 10:15:01 - train: epoch 0048, iter [05000, 05004], lr: 0.053145, loss: 4.2214
2022-10-11 10:15:03 - train: epoch 048, train_loss: 4.3106
2022-10-11 10:16:22 - eval: epoch: 048, acc1: 60.176%, acc5: 83.922%, test_loss: 1.8726, per_image_load_time: 0.310ms, per_image_inference_time: 0.603ms
2022-10-11 10:16:23 - until epoch: 048, best_acc1: 60.176%
2022-10-11 10:16:23 - epoch 049 lr: 0.053144
2022-10-11 10:17:05 - train: epoch 0049, iter [00100, 05004], lr: 0.053113, loss: 4.3216
2022-10-11 10:17:42 - train: epoch 0049, iter [00200, 05004], lr: 0.053082, loss: 4.3253
2022-10-11 10:18:18 - train: epoch 0049, iter [00300, 05004], lr: 0.053050, loss: 4.4825
2022-10-11 10:18:54 - train: epoch 0049, iter [00400, 05004], lr: 0.053019, loss: 4.0458
2022-10-11 10:19:32 - train: epoch 0049, iter [00500, 05004], lr: 0.052988, loss: 4.0564
2022-10-11 10:20:08 - train: epoch 0049, iter [00600, 05004], lr: 0.052956, loss: 3.8136
2022-10-11 10:20:44 - train: epoch 0049, iter [00700, 05004], lr: 0.052925, loss: 3.8237
2022-10-11 10:21:22 - train: epoch 0049, iter [00800, 05004], lr: 0.052894, loss: 4.7174
2022-10-11 10:21:57 - train: epoch 0049, iter [00900, 05004], lr: 0.052862, loss: 4.5460
2022-10-11 10:22:35 - train: epoch 0049, iter [01000, 05004], lr: 0.052831, loss: 4.6490
2022-10-11 10:23:12 - train: epoch 0049, iter [01100, 05004], lr: 0.052800, loss: 3.4431
2022-10-11 10:23:49 - train: epoch 0049, iter [01200, 05004], lr: 0.052768, loss: 4.4970
2022-10-11 10:24:26 - train: epoch 0049, iter [01300, 05004], lr: 0.052737, loss: 4.2798
2022-10-11 10:25:03 - train: epoch 0049, iter [01400, 05004], lr: 0.052706, loss: 4.0066
2022-10-11 10:25:39 - train: epoch 0049, iter [01500, 05004], lr: 0.052674, loss: 3.8445
2022-10-11 10:26:17 - train: epoch 0049, iter [01600, 05004], lr: 0.052643, loss: 3.8713
2022-10-11 10:26:52 - train: epoch 0049, iter [01700, 05004], lr: 0.052612, loss: 4.0874
2022-10-11 10:27:30 - train: epoch 0049, iter [01800, 05004], lr: 0.052580, loss: 4.4686
2022-10-11 10:28:07 - train: epoch 0049, iter [01900, 05004], lr: 0.052549, loss: 4.7362
2022-10-11 10:28:44 - train: epoch 0049, iter [02000, 05004], lr: 0.052517, loss: 4.1972
2022-10-11 10:29:19 - train: epoch 0049, iter [02100, 05004], lr: 0.052486, loss: 4.3963
2022-10-11 10:29:56 - train: epoch 0049, iter [02200, 05004], lr: 0.052455, loss: 4.2986
2022-10-11 10:30:32 - train: epoch 0049, iter [02300, 05004], lr: 0.052423, loss: 3.6807
2022-10-11 10:31:10 - train: epoch 0049, iter [02400, 05004], lr: 0.052392, loss: 3.7338
2022-10-11 10:31:47 - train: epoch 0049, iter [02500, 05004], lr: 0.052361, loss: 4.1534
2022-10-11 10:32:24 - train: epoch 0049, iter [02600, 05004], lr: 0.052329, loss: 4.6569
2022-10-11 10:33:00 - train: epoch 0049, iter [02700, 05004], lr: 0.052298, loss: 4.5483
2022-10-11 10:33:38 - train: epoch 0049, iter [02800, 05004], lr: 0.052267, loss: 4.3780
2022-10-11 10:34:15 - train: epoch 0049, iter [02900, 05004], lr: 0.052235, loss: 3.4953
2022-10-11 10:34:51 - train: epoch 0049, iter [03000, 05004], lr: 0.052204, loss: 4.0925
2022-10-11 10:35:28 - train: epoch 0049, iter [03100, 05004], lr: 0.052173, loss: 4.2245
2022-10-11 10:36:05 - train: epoch 0049, iter [03200, 05004], lr: 0.052141, loss: 4.5976
2022-10-11 10:36:41 - train: epoch 0049, iter [03300, 05004], lr: 0.052110, loss: 4.6731
2022-10-11 10:37:18 - train: epoch 0049, iter [03400, 05004], lr: 0.052079, loss: 3.6277
2022-10-11 10:37:55 - train: epoch 0049, iter [03500, 05004], lr: 0.052047, loss: 4.7916
2022-10-11 10:38:31 - train: epoch 0049, iter [03600, 05004], lr: 0.052016, loss: 3.7272
2022-10-11 10:39:08 - train: epoch 0049, iter [03700, 05004], lr: 0.051984, loss: 3.6337
2022-10-11 10:39:46 - train: epoch 0049, iter [03800, 05004], lr: 0.051953, loss: 4.8451
2022-10-11 10:40:23 - train: epoch 0049, iter [03900, 05004], lr: 0.051922, loss: 4.5185
2022-10-11 10:40:59 - train: epoch 0049, iter [04000, 05004], lr: 0.051890, loss: 3.8098
2022-10-11 10:41:37 - train: epoch 0049, iter [04100, 05004], lr: 0.051859, loss: 4.5481
2022-10-11 10:42:13 - train: epoch 0049, iter [04200, 05004], lr: 0.051828, loss: 4.4353
2022-10-11 10:42:49 - train: epoch 0049, iter [04300, 05004], lr: 0.051796, loss: 4.9550
2022-10-11 10:43:27 - train: epoch 0049, iter [04400, 05004], lr: 0.051765, loss: 4.7442
2022-10-11 10:44:03 - train: epoch 0049, iter [04500, 05004], lr: 0.051733, loss: 3.8199
2022-10-11 10:44:40 - train: epoch 0049, iter [04600, 05004], lr: 0.051702, loss: 4.3813
2022-10-11 10:45:18 - train: epoch 0049, iter [04700, 05004], lr: 0.051671, loss: 3.3494
2022-10-11 10:45:56 - train: epoch 0049, iter [04800, 05004], lr: 0.051639, loss: 4.4908
2022-10-11 10:46:32 - train: epoch 0049, iter [04900, 05004], lr: 0.051608, loss: 4.4327
2022-10-11 10:47:07 - train: epoch 0049, iter [05000, 05004], lr: 0.051577, loss: 4.5752
2022-10-11 10:47:09 - train: epoch 049, train_loss: 4.2895
2022-10-11 10:48:28 - eval: epoch: 049, acc1: 60.108%, acc5: 83.328%, test_loss: 2.0031, per_image_load_time: 0.343ms, per_image_inference_time: 0.601ms
2022-10-11 10:48:28 - until epoch: 049, best_acc1: 60.176%
2022-10-11 10:48:28 - epoch 050 lr: 0.051575
2022-10-11 10:49:10 - train: epoch 0050, iter [00100, 05004], lr: 0.051544, loss: 4.1161
2022-10-11 10:49:47 - train: epoch 0050, iter [00200, 05004], lr: 0.051513, loss: 4.6653
2022-10-11 10:50:23 - train: epoch 0050, iter [00300, 05004], lr: 0.051481, loss: 4.3497
2022-10-11 10:50:59 - train: epoch 0050, iter [00400, 05004], lr: 0.051450, loss: 4.1515
2022-10-11 10:51:36 - train: epoch 0050, iter [00500, 05004], lr: 0.051419, loss: 4.1468
2022-10-11 10:52:13 - train: epoch 0050, iter [00600, 05004], lr: 0.051387, loss: 4.7521
2022-10-11 10:52:50 - train: epoch 0050, iter [00700, 05004], lr: 0.051356, loss: 3.9055
2022-10-11 10:53:27 - train: epoch 0050, iter [00800, 05004], lr: 0.051324, loss: 3.4518
2022-10-11 10:54:04 - train: epoch 0050, iter [00900, 05004], lr: 0.051293, loss: 4.2191
2022-10-11 10:54:42 - train: epoch 0050, iter [01000, 05004], lr: 0.051262, loss: 4.1837
2022-10-11 10:55:19 - train: epoch 0050, iter [01100, 05004], lr: 0.051230, loss: 4.2315
2022-10-11 10:55:54 - train: epoch 0050, iter [01200, 05004], lr: 0.051199, loss: 3.8925
2022-10-11 10:56:30 - train: epoch 0050, iter [01300, 05004], lr: 0.051167, loss: 4.5193
2022-10-11 10:57:07 - train: epoch 0050, iter [01400, 05004], lr: 0.051136, loss: 5.1339
2022-10-11 10:57:43 - train: epoch 0050, iter [01500, 05004], lr: 0.051105, loss: 3.8860
2022-10-11 10:58:20 - train: epoch 0050, iter [01600, 05004], lr: 0.051073, loss: 4.7685
2022-10-11 10:58:57 - train: epoch 0050, iter [01700, 05004], lr: 0.051042, loss: 4.5222
2022-10-11 10:59:34 - train: epoch 0050, iter [01800, 05004], lr: 0.051011, loss: 4.7085
2022-10-11 11:00:11 - train: epoch 0050, iter [01900, 05004], lr: 0.050979, loss: 3.9667
2022-10-11 11:00:48 - train: epoch 0050, iter [02000, 05004], lr: 0.050948, loss: 3.6760
2022-10-11 11:01:25 - train: epoch 0050, iter [02100, 05004], lr: 0.050916, loss: 4.2935
2022-10-11 11:02:03 - train: epoch 0050, iter [02200, 05004], lr: 0.050885, loss: 4.7502
2022-10-11 11:02:40 - train: epoch 0050, iter [02300, 05004], lr: 0.050854, loss: 3.7828
2022-10-11 11:03:18 - train: epoch 0050, iter [02400, 05004], lr: 0.050822, loss: 4.6270
2022-10-11 11:03:55 - train: epoch 0050, iter [02500, 05004], lr: 0.050791, loss: 4.2321
2022-10-11 11:04:31 - train: epoch 0050, iter [02600, 05004], lr: 0.050760, loss: 4.0123
2022-10-11 11:05:09 - train: epoch 0050, iter [02700, 05004], lr: 0.050728, loss: 4.6020
2022-10-11 11:05:45 - train: epoch 0050, iter [02800, 05004], lr: 0.050697, loss: 4.7429
2022-10-11 11:06:23 - train: epoch 0050, iter [02900, 05004], lr: 0.050665, loss: 4.7872
2022-10-11 11:06:59 - train: epoch 0050, iter [03000, 05004], lr: 0.050634, loss: 4.3134
2022-10-11 11:07:36 - train: epoch 0050, iter [03100, 05004], lr: 0.050603, loss: 3.8315
2022-10-11 11:08:12 - train: epoch 0050, iter [03200, 05004], lr: 0.050571, loss: 3.6287
2022-10-11 11:08:49 - train: epoch 0050, iter [03300, 05004], lr: 0.050540, loss: 4.6914
2022-10-11 11:09:26 - train: epoch 0050, iter [03400, 05004], lr: 0.050508, loss: 4.3371
2022-10-11 11:10:03 - train: epoch 0050, iter [03500, 05004], lr: 0.050477, loss: 4.4669
2022-10-11 11:10:39 - train: epoch 0050, iter [03600, 05004], lr: 0.050446, loss: 4.2909
2022-10-11 11:11:16 - train: epoch 0050, iter [03700, 05004], lr: 0.050414, loss: 4.7024
2022-10-11 11:11:53 - train: epoch 0050, iter [03800, 05004], lr: 0.050383, loss: 4.7137
2022-10-11 11:12:30 - train: epoch 0050, iter [03900, 05004], lr: 0.050352, loss: 4.7718
2022-10-11 11:13:07 - train: epoch 0050, iter [04000, 05004], lr: 0.050320, loss: 3.5317
2022-10-11 11:13:44 - train: epoch 0050, iter [04100, 05004], lr: 0.050289, loss: 5.1356
2022-10-11 11:14:21 - train: epoch 0050, iter [04200, 05004], lr: 0.050257, loss: 4.6415
2022-10-11 11:14:58 - train: epoch 0050, iter [04300, 05004], lr: 0.050226, loss: 4.2827
2022-10-11 11:15:35 - train: epoch 0050, iter [04400, 05004], lr: 0.050195, loss: 3.7042
2022-10-11 11:16:11 - train: epoch 0050, iter [04500, 05004], lr: 0.050163, loss: 4.3421
2022-10-11 11:16:48 - train: epoch 0050, iter [04600, 05004], lr: 0.050132, loss: 4.9297
2022-10-11 11:17:27 - train: epoch 0050, iter [04700, 05004], lr: 0.050100, loss: 4.3268
2022-10-11 11:18:03 - train: epoch 0050, iter [04800, 05004], lr: 0.050069, loss: 4.0670
2022-10-11 11:18:40 - train: epoch 0050, iter [04900, 05004], lr: 0.050038, loss: 4.3843
2022-10-11 11:19:15 - train: epoch 0050, iter [05000, 05004], lr: 0.050006, loss: 4.4238
2022-10-11 11:19:17 - train: epoch 050, train_loss: 4.2865
2022-10-11 11:20:38 - eval: epoch: 050, acc1: 61.062%, acc5: 84.316%, test_loss: 1.8786, per_image_load_time: 0.935ms, per_image_inference_time: 0.546ms
2022-10-11 11:20:38 - until epoch: 050, best_acc1: 61.062%
2022-10-11 11:20:38 - epoch 051 lr: 0.050005
2022-10-11 11:21:23 - train: epoch 0051, iter [00100, 05004], lr: 0.049974, loss: 4.0670
2022-10-11 11:22:02 - train: epoch 0051, iter [00200, 05004], lr: 0.049942, loss: 4.1078
2022-10-11 11:22:38 - train: epoch 0051, iter [00300, 05004], lr: 0.049911, loss: 3.6211
2022-10-11 11:23:15 - train: epoch 0051, iter [00400, 05004], lr: 0.049879, loss: 3.6405
2022-10-11 11:23:52 - train: epoch 0051, iter [00500, 05004], lr: 0.049848, loss: 4.5366
2022-10-11 11:24:28 - train: epoch 0051, iter [00600, 05004], lr: 0.049817, loss: 5.0681
2022-10-11 11:25:05 - train: epoch 0051, iter [00700, 05004], lr: 0.049785, loss: 4.1791
2022-10-11 11:25:42 - train: epoch 0051, iter [00800, 05004], lr: 0.049754, loss: 4.7435
2022-10-11 11:26:19 - train: epoch 0051, iter [00900, 05004], lr: 0.049723, loss: 4.7770
2022-10-11 11:26:56 - train: epoch 0051, iter [01000, 05004], lr: 0.049691, loss: 4.2280
2022-10-11 11:27:34 - train: epoch 0051, iter [01100, 05004], lr: 0.049660, loss: 4.1570
2022-10-11 11:28:12 - train: epoch 0051, iter [01200, 05004], lr: 0.049628, loss: 4.3720
2022-10-11 11:28:48 - train: epoch 0051, iter [01300, 05004], lr: 0.049597, loss: 3.7482
2022-10-11 11:29:25 - train: epoch 0051, iter [01400, 05004], lr: 0.049566, loss: 4.3154
2022-10-11 11:30:02 - train: epoch 0051, iter [01500, 05004], lr: 0.049534, loss: 3.6613
2022-10-11 11:30:38 - train: epoch 0051, iter [01600, 05004], lr: 0.049503, loss: 3.7490
2022-10-11 11:31:16 - train: epoch 0051, iter [01700, 05004], lr: 0.049471, loss: 3.3804
2022-10-11 11:31:53 - train: epoch 0051, iter [01800, 05004], lr: 0.049440, loss: 4.3609
2022-10-11 11:32:30 - train: epoch 0051, iter [01900, 05004], lr: 0.049409, loss: 3.8118
2022-10-11 11:33:08 - train: epoch 0051, iter [02000, 05004], lr: 0.049377, loss: 4.7309
2022-10-11 11:33:45 - train: epoch 0051, iter [02100, 05004], lr: 0.049346, loss: 4.5004
2022-10-11 11:34:23 - train: epoch 0051, iter [02200, 05004], lr: 0.049314, loss: 4.0942
2022-10-11 11:35:00 - train: epoch 0051, iter [02300, 05004], lr: 0.049283, loss: 4.2699
2022-10-11 11:35:36 - train: epoch 0051, iter [02400, 05004], lr: 0.049252, loss: 4.3902
2022-10-11 11:36:14 - train: epoch 0051, iter [02500, 05004], lr: 0.049220, loss: 3.5370
2022-10-11 11:36:51 - train: epoch 0051, iter [02600, 05004], lr: 0.049189, loss: 4.4056
2022-10-11 11:37:28 - train: epoch 0051, iter [02700, 05004], lr: 0.049158, loss: 4.6159
2022-10-11 11:38:05 - train: epoch 0051, iter [02800, 05004], lr: 0.049126, loss: 4.2628
2022-10-11 11:38:42 - train: epoch 0051, iter [02900, 05004], lr: 0.049095, loss: 4.3372
2022-10-11 11:39:18 - train: epoch 0051, iter [03000, 05004], lr: 0.049063, loss: 3.4480
2022-10-11 11:39:55 - train: epoch 0051, iter [03100, 05004], lr: 0.049032, loss: 3.9260
2022-10-11 11:40:32 - train: epoch 0051, iter [03200, 05004], lr: 0.049001, loss: 4.4750
2022-10-11 11:41:09 - train: epoch 0051, iter [03300, 05004], lr: 0.048969, loss: 4.5337
2022-10-11 11:41:46 - train: epoch 0051, iter [03400, 05004], lr: 0.048938, loss: 3.1348
2022-10-11 11:42:23 - train: epoch 0051, iter [03500, 05004], lr: 0.048907, loss: 4.1772
2022-10-11 11:43:01 - train: epoch 0051, iter [03600, 05004], lr: 0.048875, loss: 4.4632
2022-10-11 11:43:37 - train: epoch 0051, iter [03700, 05004], lr: 0.048844, loss: 4.4324
2022-10-11 11:44:14 - train: epoch 0051, iter [03800, 05004], lr: 0.048812, loss: 3.7057
2022-10-11 11:44:51 - train: epoch 0051, iter [03900, 05004], lr: 0.048781, loss: 4.8833
2022-10-11 11:45:27 - train: epoch 0051, iter [04000, 05004], lr: 0.048750, loss: 4.2459
2022-10-11 11:46:05 - train: epoch 0051, iter [04100, 05004], lr: 0.048718, loss: 3.7639
2022-10-11 11:46:42 - train: epoch 0051, iter [04200, 05004], lr: 0.048687, loss: 4.0369
2022-10-11 11:47:18 - train: epoch 0051, iter [04300, 05004], lr: 0.048655, loss: 4.4523
2022-10-11 11:47:55 - train: epoch 0051, iter [04400, 05004], lr: 0.048624, loss: 4.0240
2022-10-11 11:48:32 - train: epoch 0051, iter [04500, 05004], lr: 0.048593, loss: 3.4176
2022-10-11 11:49:09 - train: epoch 0051, iter [04600, 05004], lr: 0.048561, loss: 4.1564
2022-10-11 11:49:46 - train: epoch 0051, iter [04700, 05004], lr: 0.048530, loss: 3.8953
2022-10-11 11:50:22 - train: epoch 0051, iter [04800, 05004], lr: 0.048499, loss: 4.5846
2022-10-11 11:50:59 - train: epoch 0051, iter [04900, 05004], lr: 0.048467, loss: 5.0935
2022-10-11 11:51:33 - train: epoch 0051, iter [05000, 05004], lr: 0.048436, loss: 4.1516
2022-10-11 11:51:36 - train: epoch 051, train_loss: 4.2713
2022-10-11 11:52:57 - eval: epoch: 051, acc1: 61.038%, acc5: 83.898%, test_loss: 1.9500, per_image_load_time: 1.136ms, per_image_inference_time: 0.592ms
2022-10-11 11:52:58 - until epoch: 051, best_acc1: 61.062%
2022-10-11 11:52:58 - epoch 052 lr: 0.048435
2022-10-11 11:53:42 - train: epoch 0052, iter [00100, 05004], lr: 0.048403, loss: 4.2986
2022-10-11 11:54:20 - train: epoch 0052, iter [00200, 05004], lr: 0.048372, loss: 4.4365
2022-10-11 11:54:57 - train: epoch 0052, iter [00300, 05004], lr: 0.048341, loss: 3.5059
2022-10-11 11:55:34 - train: epoch 0052, iter [00400, 05004], lr: 0.048309, loss: 4.2915
2022-10-11 11:56:11 - train: epoch 0052, iter [00500, 05004], lr: 0.048278, loss: 4.1121
2022-10-11 11:56:48 - train: epoch 0052, iter [00600, 05004], lr: 0.048246, loss: 4.2588
2022-10-11 11:57:25 - train: epoch 0052, iter [00700, 05004], lr: 0.048215, loss: 3.6700
2022-10-11 11:58:02 - train: epoch 0052, iter [00800, 05004], lr: 0.048184, loss: 3.8369
2022-10-11 11:58:40 - train: epoch 0052, iter [00900, 05004], lr: 0.048152, loss: 3.9740
2022-10-11 11:59:16 - train: epoch 0052, iter [01000, 05004], lr: 0.048121, loss: 4.6033
2022-10-11 11:59:52 - train: epoch 0052, iter [01100, 05004], lr: 0.048090, loss: 4.5609
2022-10-11 12:00:29 - train: epoch 0052, iter [01200, 05004], lr: 0.048058, loss: 4.1612
2022-10-11 12:01:06 - train: epoch 0052, iter [01300, 05004], lr: 0.048027, loss: 4.6947
2022-10-11 12:01:43 - train: epoch 0052, iter [01400, 05004], lr: 0.047995, loss: 4.1063
2022-10-11 12:02:20 - train: epoch 0052, iter [01500, 05004], lr: 0.047964, loss: 3.9504
2022-10-11 12:02:57 - train: epoch 0052, iter [01600, 05004], lr: 0.047933, loss: 4.2254
2022-10-11 12:03:34 - train: epoch 0052, iter [01700, 05004], lr: 0.047901, loss: 4.2930
2022-10-11 12:04:11 - train: epoch 0052, iter [01800, 05004], lr: 0.047870, loss: 4.4904
2022-10-11 12:04:48 - train: epoch 0052, iter [01900, 05004], lr: 0.047839, loss: 3.5373
2022-10-11 12:05:25 - train: epoch 0052, iter [02000, 05004], lr: 0.047807, loss: 4.4261
2022-10-11 12:06:01 - train: epoch 0052, iter [02100, 05004], lr: 0.047776, loss: 4.3095
2022-10-11 12:06:39 - train: epoch 0052, iter [02200, 05004], lr: 0.047745, loss: 4.1479
2022-10-11 12:07:16 - train: epoch 0052, iter [02300, 05004], lr: 0.047713, loss: 4.1983
2022-10-11 12:07:54 - train: epoch 0052, iter [02400, 05004], lr: 0.047682, loss: 3.6228
2022-10-11 12:08:31 - train: epoch 0052, iter [02500, 05004], lr: 0.047651, loss: 3.6029
2022-10-11 12:09:07 - train: epoch 0052, iter [02600, 05004], lr: 0.047619, loss: 3.7287
2022-10-11 12:09:45 - train: epoch 0052, iter [02700, 05004], lr: 0.047588, loss: 4.0187
2022-10-11 12:10:22 - train: epoch 0052, iter [02800, 05004], lr: 0.047556, loss: 4.4640
2022-10-11 12:10:59 - train: epoch 0052, iter [02900, 05004], lr: 0.047525, loss: 4.7103
2022-10-11 12:11:35 - train: epoch 0052, iter [03000, 05004], lr: 0.047494, loss: 4.3753
2022-10-11 12:12:13 - train: epoch 0052, iter [03100, 05004], lr: 0.047462, loss: 4.0296
2022-10-11 12:12:50 - train: epoch 0052, iter [03200, 05004], lr: 0.047431, loss: 4.3987
2022-10-11 12:13:27 - train: epoch 0052, iter [03300, 05004], lr: 0.047400, loss: 3.8359
2022-10-11 12:14:03 - train: epoch 0052, iter [03400, 05004], lr: 0.047368, loss: 4.8673
2022-10-11 12:14:42 - train: epoch 0052, iter [03500, 05004], lr: 0.047337, loss: 4.6018
2022-10-11 12:15:19 - train: epoch 0052, iter [03600, 05004], lr: 0.047306, loss: 4.5329
2022-10-11 12:15:56 - train: epoch 0052, iter [03700, 05004], lr: 0.047274, loss: 4.7016
2022-10-11 12:16:33 - train: epoch 0052, iter [03800, 05004], lr: 0.047243, loss: 4.3585
2022-10-11 12:17:10 - train: epoch 0052, iter [03900, 05004], lr: 0.047212, loss: 4.6954
2022-10-11 12:17:47 - train: epoch 0052, iter [04000, 05004], lr: 0.047180, loss: 4.0558
2022-10-11 12:18:25 - train: epoch 0052, iter [04100, 05004], lr: 0.047149, loss: 3.7999
2022-10-11 12:19:02 - train: epoch 0052, iter [04200, 05004], lr: 0.047118, loss: 4.3967
2022-10-11 12:19:38 - train: epoch 0052, iter [04300, 05004], lr: 0.047086, loss: 3.4943
2022-10-11 12:20:16 - train: epoch 0052, iter [04400, 05004], lr: 0.047055, loss: 4.4019
2022-10-11 12:20:53 - train: epoch 0052, iter [04500, 05004], lr: 0.047024, loss: 4.0471
2022-10-11 12:21:30 - train: epoch 0052, iter [04600, 05004], lr: 0.046992, loss: 3.9673
2022-10-11 12:22:07 - train: epoch 0052, iter [04700, 05004], lr: 0.046961, loss: 4.6934
2022-10-11 12:22:44 - train: epoch 0052, iter [04800, 05004], lr: 0.046930, loss: 4.4921
2022-10-11 12:23:22 - train: epoch 0052, iter [04900, 05004], lr: 0.046898, loss: 4.2049
2022-10-11 12:23:56 - train: epoch 0052, iter [05000, 05004], lr: 0.046867, loss: 4.8858
2022-10-11 12:23:59 - train: epoch 052, train_loss: 4.2620
2022-10-11 12:25:20 - eval: epoch: 052, acc1: 61.266%, acc5: 84.484%, test_loss: 1.8624, per_image_load_time: 0.753ms, per_image_inference_time: 0.565ms
2022-10-11 12:25:20 - until epoch: 052, best_acc1: 61.266%
2022-10-11 12:25:20 - epoch 053 lr: 0.046866
2022-10-11 12:26:05 - train: epoch 0053, iter [00100, 05004], lr: 0.046834, loss: 3.5344
2022-10-11 12:26:43 - train: epoch 0053, iter [00200, 05004], lr: 0.046803, loss: 4.0763
2022-10-11 12:27:19 - train: epoch 0053, iter [00300, 05004], lr: 0.046772, loss: 3.7058
2022-10-11 12:27:55 - train: epoch 0053, iter [00400, 05004], lr: 0.046740, loss: 5.0535
2022-10-11 12:28:32 - train: epoch 0053, iter [00500, 05004], lr: 0.046709, loss: 3.6584
2022-10-11 12:29:10 - train: epoch 0053, iter [00600, 05004], lr: 0.046678, loss: 4.4186
2022-10-11 12:29:47 - train: epoch 0053, iter [00700, 05004], lr: 0.046647, loss: 4.1748
2022-10-11 12:30:24 - train: epoch 0053, iter [00800, 05004], lr: 0.046615, loss: 4.4339
2022-10-11 12:31:01 - train: epoch 0053, iter [00900, 05004], lr: 0.046584, loss: 4.2650
2022-10-11 12:31:38 - train: epoch 0053, iter [01000, 05004], lr: 0.046553, loss: 5.0573
2022-10-11 12:32:15 - train: epoch 0053, iter [01100, 05004], lr: 0.046521, loss: 4.6644
2022-10-11 12:32:51 - train: epoch 0053, iter [01200, 05004], lr: 0.046490, loss: 4.2728
2022-10-11 12:33:28 - train: epoch 0053, iter [01300, 05004], lr: 0.046459, loss: 3.8746
2022-10-11 12:34:05 - train: epoch 0053, iter [01400, 05004], lr: 0.046427, loss: 5.1034
2022-10-11 12:34:42 - train: epoch 0053, iter [01500, 05004], lr: 0.046396, loss: 4.3501
2022-10-11 12:35:19 - train: epoch 0053, iter [01600, 05004], lr: 0.046365, loss: 4.4240
2022-10-11 12:35:56 - train: epoch 0053, iter [01700, 05004], lr: 0.046333, loss: 4.4027
2022-10-11 12:36:32 - train: epoch 0053, iter [01800, 05004], lr: 0.046302, loss: 4.7002
2022-10-11 12:37:10 - train: epoch 0053, iter [01900, 05004], lr: 0.046271, loss: 3.8424
2022-10-11 12:37:47 - train: epoch 0053, iter [02000, 05004], lr: 0.046240, loss: 3.7440
2022-10-11 12:38:23 - train: epoch 0053, iter [02100, 05004], lr: 0.046208, loss: 3.9375
2022-10-11 12:39:00 - train: epoch 0053, iter [02200, 05004], lr: 0.046177, loss: 4.3515
2022-10-11 12:39:38 - train: epoch 0053, iter [02300, 05004], lr: 0.046146, loss: 4.0630
2022-10-11 12:40:15 - train: epoch 0053, iter [02400, 05004], lr: 0.046114, loss: 3.6650
2022-10-11 12:40:53 - train: epoch 0053, iter [02500, 05004], lr: 0.046083, loss: 4.2679
2022-10-11 12:41:30 - train: epoch 0053, iter [02600, 05004], lr: 0.046052, loss: 4.1104
2022-10-11 12:42:07 - train: epoch 0053, iter [02700, 05004], lr: 0.046020, loss: 4.9252
2022-10-11 12:42:45 - train: epoch 0053, iter [02800, 05004], lr: 0.045989, loss: 3.9323
2022-10-11 12:43:22 - train: epoch 0053, iter [02900, 05004], lr: 0.045958, loss: 4.4792
2022-10-11 12:43:59 - train: epoch 0053, iter [03000, 05004], lr: 0.045927, loss: 4.7050
2022-10-11 12:44:36 - train: epoch 0053, iter [03100, 05004], lr: 0.045895, loss: 5.0219
2022-10-11 12:45:13 - train: epoch 0053, iter [03200, 05004], lr: 0.045864, loss: 3.9813
2022-10-11 12:45:50 - train: epoch 0053, iter [03300, 05004], lr: 0.045833, loss: 3.4995
2022-10-11 12:46:27 - train: epoch 0053, iter [03400, 05004], lr: 0.045802, loss: 4.1046
2022-10-11 12:47:04 - train: epoch 0053, iter [03500, 05004], lr: 0.045770, loss: 4.3701
2022-10-11 12:47:41 - train: epoch 0053, iter [03600, 05004], lr: 0.045739, loss: 4.2143
2022-10-11 12:48:18 - train: epoch 0053, iter [03700, 05004], lr: 0.045708, loss: 4.1989
2022-10-11 12:48:56 - train: epoch 0053, iter [03800, 05004], lr: 0.045676, loss: 3.9560
2022-10-11 12:49:33 - train: epoch 0053, iter [03900, 05004], lr: 0.045645, loss: 4.9459
2022-10-11 12:50:10 - train: epoch 0053, iter [04000, 05004], lr: 0.045614, loss: 3.6162
2022-10-11 12:50:47 - train: epoch 0053, iter [04100, 05004], lr: 0.045583, loss: 3.9954
2022-10-11 12:51:24 - train: epoch 0053, iter [04200, 05004], lr: 0.045551, loss: 4.7304
2022-10-11 12:52:01 - train: epoch 0053, iter [04300, 05004], lr: 0.045520, loss: 4.5747
2022-10-11 12:52:38 - train: epoch 0053, iter [04400, 05004], lr: 0.045489, loss: 4.1970
2022-10-11 12:53:14 - train: epoch 0053, iter [04500, 05004], lr: 0.045458, loss: 3.9381
2022-10-11 12:53:52 - train: epoch 0053, iter [04600, 05004], lr: 0.045426, loss: 4.1058
2022-10-11 12:54:29 - train: epoch 0053, iter [04700, 05004], lr: 0.045395, loss: 3.8723
2022-10-11 12:55:06 - train: epoch 0053, iter [04800, 05004], lr: 0.045364, loss: 4.4518
2022-10-11 12:55:43 - train: epoch 0053, iter [04900, 05004], lr: 0.045333, loss: 4.2886
2022-10-11 12:56:18 - train: epoch 0053, iter [05000, 05004], lr: 0.045301, loss: 4.5844
2022-10-11 12:56:20 - train: epoch 053, train_loss: 4.2509
2022-10-11 12:57:41 - eval: epoch: 053, acc1: 61.716%, acc5: 84.664%, test_loss: 1.8539, per_image_load_time: 1.289ms, per_image_inference_time: 0.555ms
2022-10-11 12:57:42 - until epoch: 053, best_acc1: 61.716%
2022-10-11 12:57:42 - epoch 054 lr: 0.045300
2022-10-11 12:58:28 - train: epoch 0054, iter [00100, 05004], lr: 0.045269, loss: 4.9990
2022-10-11 12:59:05 - train: epoch 0054, iter [00200, 05004], lr: 0.045238, loss: 3.6913
2022-10-11 12:59:41 - train: epoch 0054, iter [00300, 05004], lr: 0.045206, loss: 4.3756
2022-10-11 13:00:18 - train: epoch 0054, iter [00400, 05004], lr: 0.045175, loss: 3.3921
2022-10-11 13:00:54 - train: epoch 0054, iter [00500, 05004], lr: 0.045144, loss: 4.6745
2022-10-11 13:01:31 - train: epoch 0054, iter [00600, 05004], lr: 0.045113, loss: 4.3178
2022-10-11 13:02:08 - train: epoch 0054, iter [00700, 05004], lr: 0.045081, loss: 3.7889
2022-10-11 13:02:45 - train: epoch 0054, iter [00800, 05004], lr: 0.045050, loss: 3.7598
2022-10-11 13:03:21 - train: epoch 0054, iter [00900, 05004], lr: 0.045019, loss: 3.8560
2022-10-11 13:03:58 - train: epoch 0054, iter [01000, 05004], lr: 0.044988, loss: 4.0726
2022-10-11 13:04:35 - train: epoch 0054, iter [01100, 05004], lr: 0.044956, loss: 4.7869
2022-10-11 13:05:11 - train: epoch 0054, iter [01200, 05004], lr: 0.044925, loss: 4.7943
2022-10-11 13:05:48 - train: epoch 0054, iter [01300, 05004], lr: 0.044894, loss: 3.7090
2022-10-11 13:06:25 - train: epoch 0054, iter [01400, 05004], lr: 0.044863, loss: 3.3788
2022-10-11 13:07:02 - train: epoch 0054, iter [01500, 05004], lr: 0.044832, loss: 3.3826
2022-10-11 13:07:39 - train: epoch 0054, iter [01600, 05004], lr: 0.044800, loss: 4.7692
2022-10-11 13:08:16 - train: epoch 0054, iter [01700, 05004], lr: 0.044769, loss: 4.0386
2022-10-11 13:08:53 - train: epoch 0054, iter [01800, 05004], lr: 0.044738, loss: 4.3692
2022-10-11 13:09:30 - train: epoch 0054, iter [01900, 05004], lr: 0.044707, loss: 4.3397
2022-10-11 13:10:07 - train: epoch 0054, iter [02000, 05004], lr: 0.044675, loss: 4.7765
2022-10-11 13:10:44 - train: epoch 0054, iter [02100, 05004], lr: 0.044644, loss: 3.9817
2022-10-11 13:11:22 - train: epoch 0054, iter [02200, 05004], lr: 0.044613, loss: 4.8434
2022-10-11 13:11:59 - train: epoch 0054, iter [02300, 05004], lr: 0.044582, loss: 3.8546
2022-10-11 13:12:35 - train: epoch 0054, iter [02400, 05004], lr: 0.044551, loss: 4.9540
2022-10-11 13:13:12 - train: epoch 0054, iter [02500, 05004], lr: 0.044519, loss: 4.1989
2022-10-11 13:13:48 - train: epoch 0054, iter [02600, 05004], lr: 0.044488, loss: 4.1264
2022-10-11 13:14:26 - train: epoch 0054, iter [02700, 05004], lr: 0.044457, loss: 4.4957
2022-10-11 13:15:02 - train: epoch 0054, iter [02800, 05004], lr: 0.044426, loss: 4.3862
2022-10-11 13:15:39 - train: epoch 0054, iter [02900, 05004], lr: 0.044395, loss: 4.5220
2022-10-11 13:16:16 - train: epoch 0054, iter [03000, 05004], lr: 0.044363, loss: 4.4400
2022-10-11 13:16:53 - train: epoch 0054, iter [03100, 05004], lr: 0.044332, loss: 4.6676
2022-10-11 13:17:30 - train: epoch 0054, iter [03200, 05004], lr: 0.044301, loss: 4.9129
2022-10-11 13:18:07 - train: epoch 0054, iter [03300, 05004], lr: 0.044270, loss: 4.6572
2022-10-11 13:18:43 - train: epoch 0054, iter [03400, 05004], lr: 0.044239, loss: 3.9685
2022-10-11 13:19:19 - train: epoch 0054, iter [03500, 05004], lr: 0.044208, loss: 4.0519
2022-10-11 13:19:57 - train: epoch 0054, iter [03600, 05004], lr: 0.044176, loss: 4.2659
2022-10-11 13:20:34 - train: epoch 0054, iter [03700, 05004], lr: 0.044145, loss: 4.2819
2022-10-11 13:21:10 - train: epoch 0054, iter [03800, 05004], lr: 0.044114, loss: 4.0586
2022-10-11 13:21:46 - train: epoch 0054, iter [03900, 05004], lr: 0.044083, loss: 3.8946
2022-10-11 13:22:23 - train: epoch 0054, iter [04000, 05004], lr: 0.044052, loss: 4.1260
2022-10-11 13:23:00 - train: epoch 0054, iter [04100, 05004], lr: 0.044021, loss: 3.8018
2022-10-11 13:23:36 - train: epoch 0054, iter [04200, 05004], lr: 0.043989, loss: 4.3900
2022-10-11 13:24:13 - train: epoch 0054, iter [04300, 05004], lr: 0.043958, loss: 3.7127
2022-10-11 13:24:50 - train: epoch 0054, iter [04400, 05004], lr: 0.043927, loss: 4.1098
2022-10-11 13:25:28 - train: epoch 0054, iter [04500, 05004], lr: 0.043896, loss: 4.4437
2022-10-11 13:26:05 - train: epoch 0054, iter [04600, 05004], lr: 0.043865, loss: 4.0727
2022-10-11 13:26:42 - train: epoch 0054, iter [04700, 05004], lr: 0.043834, loss: 4.6211
2022-10-11 13:27:19 - train: epoch 0054, iter [04800, 05004], lr: 0.043802, loss: 3.6729
2022-10-11 13:27:56 - train: epoch 0054, iter [04900, 05004], lr: 0.043771, loss: 4.5948
2022-10-11 13:28:32 - train: epoch 0054, iter [05000, 05004], lr: 0.043740, loss: 3.9383
2022-10-11 13:28:34 - train: epoch 054, train_loss: 4.2246
2022-10-11 13:29:56 - eval: epoch: 054, acc1: 61.334%, acc5: 84.758%, test_loss: 1.8647, per_image_load_time: 1.264ms, per_image_inference_time: 0.561ms
2022-10-11 13:29:56 - until epoch: 054, best_acc1: 61.716%
2022-10-11 13:29:56 - epoch 055 lr: 0.043739
2022-10-11 13:30:40 - train: epoch 0055, iter [00100, 05004], lr: 0.043708, loss: 4.4809
2022-10-11 13:31:18 - train: epoch 0055, iter [00200, 05004], lr: 0.043677, loss: 4.0038
2022-10-11 13:31:55 - train: epoch 0055, iter [00300, 05004], lr: 0.043646, loss: 4.0014
2022-10-11 13:32:33 - train: epoch 0055, iter [00400, 05004], lr: 0.043614, loss: 4.3837
2022-10-11 13:33:10 - train: epoch 0055, iter [00500, 05004], lr: 0.043583, loss: 3.4262
2022-10-11 13:33:47 - train: epoch 0055, iter [00600, 05004], lr: 0.043552, loss: 4.7823
2022-10-11 13:34:24 - train: epoch 0055, iter [00700, 05004], lr: 0.043521, loss: 4.4514
2022-10-11 13:35:01 - train: epoch 0055, iter [00800, 05004], lr: 0.043490, loss: 3.5855
2022-10-11 13:35:37 - train: epoch 0055, iter [00900, 05004], lr: 0.043459, loss: 4.7341
2022-10-11 13:36:15 - train: epoch 0055, iter [01000, 05004], lr: 0.043428, loss: 4.0685
2022-10-11 13:36:52 - train: epoch 0055, iter [01100, 05004], lr: 0.043397, loss: 3.7427
2022-10-11 13:37:29 - train: epoch 0055, iter [01200, 05004], lr: 0.043365, loss: 4.1221
2022-10-11 13:38:06 - train: epoch 0055, iter [01300, 05004], lr: 0.043334, loss: 4.0277
2022-10-11 13:38:44 - train: epoch 0055, iter [01400, 05004], lr: 0.043303, loss: 4.7911
2022-10-11 13:39:21 - train: epoch 0055, iter [01500, 05004], lr: 0.043272, loss: 4.3917
2022-10-11 13:39:58 - train: epoch 0055, iter [01600, 05004], lr: 0.043241, loss: 4.8809
2022-10-11 13:40:35 - train: epoch 0055, iter [01700, 05004], lr: 0.043210, loss: 4.8426
2022-10-11 13:41:12 - train: epoch 0055, iter [01800, 05004], lr: 0.043179, loss: 4.2976
2022-10-11 13:41:49 - train: epoch 0055, iter [01900, 05004], lr: 0.043148, loss: 4.3965
2022-10-11 13:42:26 - train: epoch 0055, iter [02000, 05004], lr: 0.043117, loss: 3.5535
2022-10-11 13:43:04 - train: epoch 0055, iter [02100, 05004], lr: 0.043086, loss: 3.9147
2022-10-11 13:43:41 - train: epoch 0055, iter [02200, 05004], lr: 0.043055, loss: 4.4763
2022-10-11 13:44:18 - train: epoch 0055, iter [02300, 05004], lr: 0.043023, loss: 4.7600
2022-10-11 13:44:55 - train: epoch 0055, iter [02400, 05004], lr: 0.042992, loss: 3.5227
2022-10-11 13:45:32 - train: epoch 0055, iter [02500, 05004], lr: 0.042961, loss: 4.5234
2022-10-11 13:46:10 - train: epoch 0055, iter [02600, 05004], lr: 0.042930, loss: 4.6016
2022-10-11 13:46:47 - train: epoch 0055, iter [02700, 05004], lr: 0.042899, loss: 4.2760
2022-10-11 13:47:24 - train: epoch 0055, iter [02800, 05004], lr: 0.042868, loss: 4.4818
2022-10-11 13:48:00 - train: epoch 0055, iter [02900, 05004], lr: 0.042837, loss: 4.0485
2022-10-11 13:48:37 - train: epoch 0055, iter [03000, 05004], lr: 0.042806, loss: 4.5669
2022-10-11 13:49:13 - train: epoch 0055, iter [03100, 05004], lr: 0.042775, loss: 3.1380
2022-10-11 13:49:50 - train: epoch 0055, iter [03200, 05004], lr: 0.042744, loss: 4.4364
2022-10-11 13:50:27 - train: epoch 0055, iter [03300, 05004], lr: 0.042713, loss: 4.5269
2022-10-11 13:51:04 - train: epoch 0055, iter [03400, 05004], lr: 0.042682, loss: 4.4862
2022-10-11 13:51:40 - train: epoch 0055, iter [03500, 05004], lr: 0.042651, loss: 4.1258
2022-10-11 13:52:18 - train: epoch 0055, iter [03600, 05004], lr: 0.042620, loss: 4.4711
2022-10-11 13:52:55 - train: epoch 0055, iter [03700, 05004], lr: 0.042589, loss: 4.4011
2022-10-11 13:53:31 - train: epoch 0055, iter [03800, 05004], lr: 0.042558, loss: 4.1560
2022-10-11 13:54:08 - train: epoch 0055, iter [03900, 05004], lr: 0.042526, loss: 4.1898
2022-10-11 13:54:45 - train: epoch 0055, iter [04000, 05004], lr: 0.042495, loss: 3.4398
2022-10-11 13:55:22 - train: epoch 0055, iter [04100, 05004], lr: 0.042464, loss: 3.2222
2022-10-11 13:56:00 - train: epoch 0055, iter [04200, 05004], lr: 0.042433, loss: 4.1551
2022-10-11 13:56:37 - train: epoch 0055, iter [04300, 05004], lr: 0.042402, loss: 4.8469
2022-10-11 13:57:13 - train: epoch 0055, iter [04400, 05004], lr: 0.042371, loss: 3.8827
2022-10-11 13:57:51 - train: epoch 0055, iter [04500, 05004], lr: 0.042340, loss: 4.3663
2022-10-11 13:58:28 - train: epoch 0055, iter [04600, 05004], lr: 0.042309, loss: 3.8642
2022-10-11 13:59:05 - train: epoch 0055, iter [04700, 05004], lr: 0.042278, loss: 3.7455
2022-10-11 13:59:42 - train: epoch 0055, iter [04800, 05004], lr: 0.042247, loss: 3.8790
2022-10-11 14:00:20 - train: epoch 0055, iter [04900, 05004], lr: 0.042216, loss: 4.2661
2022-10-11 14:00:54 - train: epoch 0055, iter [05000, 05004], lr: 0.042185, loss: 4.6095
2022-10-11 14:00:56 - train: epoch 055, train_loss: 4.2196
2022-10-11 14:02:17 - eval: epoch: 055, acc1: 61.590%, acc5: 84.828%, test_loss: 1.9804, per_image_load_time: 1.567ms, per_image_inference_time: 0.542ms
2022-10-11 14:02:18 - until epoch: 055, best_acc1: 61.716%
2022-10-11 14:02:18 - epoch 056 lr: 0.042184
2022-10-11 14:03:02 - train: epoch 0056, iter [00100, 05004], lr: 0.042153, loss: 4.0464
2022-10-11 14:03:39 - train: epoch 0056, iter [00200, 05004], lr: 0.042122, loss: 4.5558
2022-10-11 14:04:16 - train: epoch 0056, iter [00300, 05004], lr: 0.042091, loss: 3.4017
2022-10-11 14:04:53 - train: epoch 0056, iter [00400, 05004], lr: 0.042060, loss: 4.9706
2022-10-11 14:05:30 - train: epoch 0056, iter [00500, 05004], lr: 0.042029, loss: 4.6356
2022-10-11 14:06:07 - train: epoch 0056, iter [00600, 05004], lr: 0.041998, loss: 4.7275
2022-10-11 14:06:44 - train: epoch 0056, iter [00700, 05004], lr: 0.041967, loss: 3.4840
2022-10-11 14:07:21 - train: epoch 0056, iter [00800, 05004], lr: 0.041936, loss: 4.6830
2022-10-11 14:07:58 - train: epoch 0056, iter [00900, 05004], lr: 0.041905, loss: 4.3457
2022-10-11 14:08:35 - train: epoch 0056, iter [01000, 05004], lr: 0.041874, loss: 4.6571
2022-10-11 14:09:12 - train: epoch 0056, iter [01100, 05004], lr: 0.041843, loss: 4.6205
2022-10-11 14:09:49 - train: epoch 0056, iter [01200, 05004], lr: 0.041812, loss: 4.1336
2022-10-11 14:10:25 - train: epoch 0056, iter [01300, 05004], lr: 0.041781, loss: 4.4734
2022-10-11 14:11:02 - train: epoch 0056, iter [01400, 05004], lr: 0.041750, loss: 3.8380
2022-10-11 14:11:40 - train: epoch 0056, iter [01500, 05004], lr: 0.041719, loss: 4.7829
2022-10-11 14:12:16 - train: epoch 0056, iter [01600, 05004], lr: 0.041688, loss: 3.7865
2022-10-11 14:12:56 - train: epoch 0056, iter [01700, 05004], lr: 0.041657, loss: 3.5889
2022-10-11 14:13:31 - train: epoch 0056, iter [01800, 05004], lr: 0.041627, loss: 4.0359
2022-10-11 14:14:09 - train: epoch 0056, iter [01900, 05004], lr: 0.041596, loss: 3.1623
2022-10-11 14:14:46 - train: epoch 0056, iter [02000, 05004], lr: 0.041565, loss: 4.1120
2022-10-11 14:15:24 - train: epoch 0056, iter [02100, 05004], lr: 0.041534, loss: 4.0363
2022-10-11 14:16:01 - train: epoch 0056, iter [02200, 05004], lr: 0.041503, loss: 4.1872
2022-10-11 14:16:37 - train: epoch 0056, iter [02300, 05004], lr: 0.041472, loss: 4.5226
2022-10-11 14:17:14 - train: epoch 0056, iter [02400, 05004], lr: 0.041441, loss: 4.3031
2022-10-11 14:17:51 - train: epoch 0056, iter [02500, 05004], lr: 0.041410, loss: 4.6153
2022-10-11 14:18:29 - train: epoch 0056, iter [02600, 05004], lr: 0.041379, loss: 4.6288
2022-10-11 14:19:06 - train: epoch 0056, iter [02700, 05004], lr: 0.041348, loss: 3.6737
2022-10-11 14:19:44 - train: epoch 0056, iter [02800, 05004], lr: 0.041317, loss: 3.7216
2022-10-11 14:20:21 - train: epoch 0056, iter [02900, 05004], lr: 0.041286, loss: 3.8886
2022-10-11 14:20:58 - train: epoch 0056, iter [03000, 05004], lr: 0.041255, loss: 4.3610
2022-10-11 14:21:34 - train: epoch 0056, iter [03100, 05004], lr: 0.041225, loss: 3.6962
2022-10-11 14:22:11 - train: epoch 0056, iter [03200, 05004], lr: 0.041194, loss: 4.5604
2022-10-11 14:22:48 - train: epoch 0056, iter [03300, 05004], lr: 0.041163, loss: 4.5388
2022-10-11 14:23:24 - train: epoch 0056, iter [03400, 05004], lr: 0.041132, loss: 3.7840
2022-10-11 14:24:00 - train: epoch 0056, iter [03500, 05004], lr: 0.041101, loss: 3.1469
2022-10-11 14:24:38 - train: epoch 0056, iter [03600, 05004], lr: 0.041070, loss: 4.6173
2022-10-11 14:25:15 - train: epoch 0056, iter [03700, 05004], lr: 0.041039, loss: 4.4397
2022-10-11 14:25:52 - train: epoch 0056, iter [03800, 05004], lr: 0.041008, loss: 4.2680
2022-10-11 14:26:30 - train: epoch 0056, iter [03900, 05004], lr: 0.040977, loss: 4.6414
2022-10-11 14:27:06 - train: epoch 0056, iter [04000, 05004], lr: 0.040947, loss: 4.2091
2022-10-11 14:27:43 - train: epoch 0056, iter [04100, 05004], lr: 0.040916, loss: 3.8118
2022-10-11 14:28:20 - train: epoch 0056, iter [04200, 05004], lr: 0.040885, loss: 4.3275
2022-10-11 14:28:57 - train: epoch 0056, iter [04300, 05004], lr: 0.040854, loss: 4.7416
2022-10-11 14:29:34 - train: epoch 0056, iter [04400, 05004], lr: 0.040823, loss: 4.1817
2022-10-11 14:30:12 - train: epoch 0056, iter [04500, 05004], lr: 0.040792, loss: 4.0933
2022-10-11 14:30:49 - train: epoch 0056, iter [04600, 05004], lr: 0.040761, loss: 4.6042
2022-10-11 14:31:26 - train: epoch 0056, iter [04700, 05004], lr: 0.040731, loss: 4.7419
2022-10-11 14:32:03 - train: epoch 0056, iter [04800, 05004], lr: 0.040700, loss: 4.5180
2022-10-11 14:32:38 - train: epoch 0056, iter [04900, 05004], lr: 0.040669, loss: 3.9844
2022-10-11 14:33:14 - train: epoch 0056, iter [05000, 05004], lr: 0.040638, loss: 4.4308
2022-10-11 14:33:16 - train: epoch 056, train_loss: 4.2032
2022-10-11 14:34:36 - eval: epoch: 056, acc1: 63.400%, acc5: 85.790%, test_loss: 1.8403, per_image_load_time: 1.544ms, per_image_inference_time: 0.582ms
2022-10-11 14:34:37 - until epoch: 056, best_acc1: 63.400%
2022-10-11 14:34:37 - epoch 057 lr: 0.040637
2022-10-11 14:35:21 - train: epoch 0057, iter [00100, 05004], lr: 0.040606, loss: 4.6173
2022-10-11 14:36:00 - train: epoch 0057, iter [00200, 05004], lr: 0.040575, loss: 4.4699
2022-10-11 14:36:38 - train: epoch 0057, iter [00300, 05004], lr: 0.040544, loss: 4.9143
2022-10-11 14:37:17 - train: epoch 0057, iter [00400, 05004], lr: 0.040514, loss: 4.6577
2022-10-11 14:37:53 - train: epoch 0057, iter [00500, 05004], lr: 0.040483, loss: 3.5246
2022-10-11 14:38:30 - train: epoch 0057, iter [00600, 05004], lr: 0.040452, loss: 3.9141
2022-10-11 14:39:07 - train: epoch 0057, iter [00700, 05004], lr: 0.040421, loss: 4.0703
2022-10-11 14:39:45 - train: epoch 0057, iter [00800, 05004], lr: 0.040390, loss: 3.9035
2022-10-11 14:40:21 - train: epoch 0057, iter [00900, 05004], lr: 0.040360, loss: 3.4891
2022-10-11 14:40:57 - train: epoch 0057, iter [01000, 05004], lr: 0.040329, loss: 4.5654
2022-10-11 14:41:34 - train: epoch 0057, iter [01100, 05004], lr: 0.040298, loss: 4.1316
2022-10-11 14:42:11 - train: epoch 0057, iter [01200, 05004], lr: 0.040267, loss: 4.6287
2022-10-11 14:42:49 - train: epoch 0057, iter [01300, 05004], lr: 0.040236, loss: 4.6065
2022-10-11 14:43:26 - train: epoch 0057, iter [01400, 05004], lr: 0.040206, loss: 4.7467
2022-10-11 14:44:02 - train: epoch 0057, iter [01500, 05004], lr: 0.040175, loss: 3.1595
2022-10-11 14:44:40 - train: epoch 0057, iter [01600, 05004], lr: 0.040144, loss: 3.9709
2022-10-11 14:45:17 - train: epoch 0057, iter [01700, 05004], lr: 0.040113, loss: 4.4450
2022-10-11 14:45:54 - train: epoch 0057, iter [01800, 05004], lr: 0.040083, loss: 3.9905
2022-10-11 14:46:31 - train: epoch 0057, iter [01900, 05004], lr: 0.040052, loss: 4.7255
2022-10-11 14:47:08 - train: epoch 0057, iter [02000, 05004], lr: 0.040021, loss: 4.3226
2022-10-11 14:47:45 - train: epoch 0057, iter [02100, 05004], lr: 0.039990, loss: 3.1641
2022-10-11 14:48:22 - train: epoch 0057, iter [02200, 05004], lr: 0.039959, loss: 4.4033
2022-10-11 14:49:00 - train: epoch 0057, iter [02300, 05004], lr: 0.039929, loss: 4.0724
2022-10-11 14:49:37 - train: epoch 0057, iter [02400, 05004], lr: 0.039898, loss: 4.3783
2022-10-11 14:50:15 - train: epoch 0057, iter [02500, 05004], lr: 0.039867, loss: 4.1095
2022-10-11 14:50:52 - train: epoch 0057, iter [02600, 05004], lr: 0.039837, loss: 4.1052
2022-10-11 14:51:29 - train: epoch 0057, iter [02700, 05004], lr: 0.039806, loss: 3.1405
2022-10-11 14:52:06 - train: epoch 0057, iter [02800, 05004], lr: 0.039775, loss: 3.7938
2022-10-11 14:52:42 - train: epoch 0057, iter [02900, 05004], lr: 0.039744, loss: 4.2390
2022-10-11 14:53:19 - train: epoch 0057, iter [03000, 05004], lr: 0.039714, loss: 4.8157
2022-10-11 14:53:58 - train: epoch 0057, iter [03100, 05004], lr: 0.039683, loss: 4.2523
2022-10-11 14:54:35 - train: epoch 0057, iter [03200, 05004], lr: 0.039652, loss: 4.6028
2022-10-11 14:55:12 - train: epoch 0057, iter [03300, 05004], lr: 0.039622, loss: 4.7395
2022-10-11 14:55:49 - train: epoch 0057, iter [03400, 05004], lr: 0.039591, loss: 3.7014
2022-10-11 14:56:26 - train: epoch 0057, iter [03500, 05004], lr: 0.039560, loss: 4.5990
2022-10-11 14:57:03 - train: epoch 0057, iter [03600, 05004], lr: 0.039529, loss: 4.1165
2022-10-11 14:57:40 - train: epoch 0057, iter [03700, 05004], lr: 0.039499, loss: 3.7002
2022-10-11 14:58:18 - train: epoch 0057, iter [03800, 05004], lr: 0.039468, loss: 3.0493
2022-10-11 14:58:56 - train: epoch 0057, iter [03900, 05004], lr: 0.039437, loss: 3.9892
2022-10-11 14:59:32 - train: epoch 0057, iter [04000, 05004], lr: 0.039407, loss: 4.6006
2022-10-11 15:00:11 - train: epoch 0057, iter [04100, 05004], lr: 0.039376, loss: 4.4222
2022-10-11 15:00:48 - train: epoch 0057, iter [04200, 05004], lr: 0.039345, loss: 4.6849
2022-10-11 15:01:25 - train: epoch 0057, iter [04300, 05004], lr: 0.039315, loss: 3.5592
2022-10-11 15:02:03 - train: epoch 0057, iter [04400, 05004], lr: 0.039284, loss: 4.1985
2022-10-11 15:02:41 - train: epoch 0057, iter [04500, 05004], lr: 0.039253, loss: 4.4242
2022-10-11 15:03:18 - train: epoch 0057, iter [04600, 05004], lr: 0.039223, loss: 3.2253
2022-10-11 15:03:55 - train: epoch 0057, iter [04700, 05004], lr: 0.039192, loss: 4.5283
2022-10-11 15:04:33 - train: epoch 0057, iter [04800, 05004], lr: 0.039161, loss: 4.8840
2022-10-11 15:05:09 - train: epoch 0057, iter [04900, 05004], lr: 0.039131, loss: 3.9261
2022-10-11 15:05:44 - train: epoch 0057, iter [05000, 05004], lr: 0.039100, loss: 4.1363
2022-10-11 15:05:47 - train: epoch 057, train_loss: 4.1918
2022-10-11 15:07:09 - eval: epoch: 057, acc1: 63.162%, acc5: 85.644%, test_loss: 1.8067, per_image_load_time: 1.339ms, per_image_inference_time: 0.568ms
2022-10-11 15:07:09 - until epoch: 057, best_acc1: 63.400%
2022-10-11 15:07:09 - epoch 058 lr: 0.039099
2022-10-11 15:07:53 - train: epoch 0058, iter [00100, 05004], lr: 0.039068, loss: 4.5698
2022-10-11 15:08:31 - train: epoch 0058, iter [00200, 05004], lr: 0.039038, loss: 4.4502
2022-10-11 15:09:08 - train: epoch 0058, iter [00300, 05004], lr: 0.039007, loss: 4.1265
2022-10-11 15:09:45 - train: epoch 0058, iter [00400, 05004], lr: 0.038976, loss: 4.0797
2022-10-11 15:10:22 - train: epoch 0058, iter [00500, 05004], lr: 0.038946, loss: 3.9871
2022-10-11 15:10:59 - train: epoch 0058, iter [00600, 05004], lr: 0.038915, loss: 4.7338
2022-10-11 15:11:36 - train: epoch 0058, iter [00700, 05004], lr: 0.038885, loss: 4.1971
2022-10-11 15:12:13 - train: epoch 0058, iter [00800, 05004], lr: 0.038854, loss: 4.6497
2022-10-11 15:12:50 - train: epoch 0058, iter [00900, 05004], lr: 0.038823, loss: 3.9683
2022-10-11 15:13:27 - train: epoch 0058, iter [01000, 05004], lr: 0.038793, loss: 4.2801
2022-10-11 15:14:05 - train: epoch 0058, iter [01100, 05004], lr: 0.038762, loss: 4.4920
2022-10-11 15:14:42 - train: epoch 0058, iter [01200, 05004], lr: 0.038732, loss: 4.3476
2022-10-11 15:15:19 - train: epoch 0058, iter [01300, 05004], lr: 0.038701, loss: 3.9282
2022-10-11 15:15:56 - train: epoch 0058, iter [01400, 05004], lr: 0.038671, loss: 4.2909
2022-10-11 15:16:33 - train: epoch 0058, iter [01500, 05004], lr: 0.038640, loss: 4.3694
2022-10-11 15:17:09 - train: epoch 0058, iter [01600, 05004], lr: 0.038609, loss: 4.1771
2022-10-11 15:17:46 - train: epoch 0058, iter [01700, 05004], lr: 0.038579, loss: 4.4997
2022-10-11 15:18:23 - train: epoch 0058, iter [01800, 05004], lr: 0.038548, loss: 4.5962
2022-10-11 15:19:00 - train: epoch 0058, iter [01900, 05004], lr: 0.038518, loss: 3.9380
2022-10-11 15:19:37 - train: epoch 0058, iter [02000, 05004], lr: 0.038487, loss: 4.5886
2022-10-11 15:20:14 - train: epoch 0058, iter [02100, 05004], lr: 0.038457, loss: 4.3234
2022-10-11 15:20:51 - train: epoch 0058, iter [02200, 05004], lr: 0.038426, loss: 3.8182
2022-10-11 15:21:28 - train: epoch 0058, iter [02300, 05004], lr: 0.038396, loss: 4.5342
2022-10-11 15:22:05 - train: epoch 0058, iter [02400, 05004], lr: 0.038365, loss: 4.4691
2022-10-11 15:22:42 - train: epoch 0058, iter [02500, 05004], lr: 0.038335, loss: 4.0159
2022-10-11 15:23:20 - train: epoch 0058, iter [02600, 05004], lr: 0.038304, loss: 3.7796
2022-10-11 15:23:57 - train: epoch 0058, iter [02700, 05004], lr: 0.038273, loss: 4.1141
2022-10-11 15:24:34 - train: epoch 0058, iter [02800, 05004], lr: 0.038243, loss: 3.9094
2022-10-11 15:25:11 - train: epoch 0058, iter [02900, 05004], lr: 0.038212, loss: 4.2389
2022-10-11 15:25:48 - train: epoch 0058, iter [03000, 05004], lr: 0.038182, loss: 4.0548
2022-10-11 15:26:26 - train: epoch 0058, iter [03100, 05004], lr: 0.038151, loss: 5.0490
2022-10-11 15:27:02 - train: epoch 0058, iter [03200, 05004], lr: 0.038121, loss: 3.5772
2022-10-11 15:27:39 - train: epoch 0058, iter [03300, 05004], lr: 0.038090, loss: 4.1432
2022-10-11 15:28:16 - train: epoch 0058, iter [03400, 05004], lr: 0.038060, loss: 4.2643
2022-10-11 15:28:53 - train: epoch 0058, iter [03500, 05004], lr: 0.038030, loss: 4.0053
2022-10-11 15:29:29 - train: epoch 0058, iter [03600, 05004], lr: 0.037999, loss: 4.2829
2022-10-11 15:30:06 - train: epoch 0058, iter [03700, 05004], lr: 0.037969, loss: 4.0757
2022-10-11 15:30:42 - train: epoch 0058, iter [03800, 05004], lr: 0.037938, loss: 3.8018
2022-10-11 15:31:20 - train: epoch 0058, iter [03900, 05004], lr: 0.037908, loss: 4.5685
2022-10-11 15:31:56 - train: epoch 0058, iter [04000, 05004], lr: 0.037877, loss: 4.2373
2022-10-11 15:32:33 - train: epoch 0058, iter [04100, 05004], lr: 0.037847, loss: 4.0402
2022-10-11 15:33:09 - train: epoch 0058, iter [04200, 05004], lr: 0.037816, loss: 3.7671
2022-10-11 15:33:46 - train: epoch 0058, iter [04300, 05004], lr: 0.037786, loss: 4.5638
2022-10-11 15:34:22 - train: epoch 0058, iter [04400, 05004], lr: 0.037755, loss: 4.2177
2022-10-11 15:34:59 - train: epoch 0058, iter [04500, 05004], lr: 0.037725, loss: 4.2424
2022-10-11 15:35:35 - train: epoch 0058, iter [04600, 05004], lr: 0.037695, loss: 4.3234
2022-10-11 15:36:13 - train: epoch 0058, iter [04700, 05004], lr: 0.037664, loss: 4.4874
2022-10-11 15:36:49 - train: epoch 0058, iter [04800, 05004], lr: 0.037634, loss: 3.6939
2022-10-11 15:37:27 - train: epoch 0058, iter [04900, 05004], lr: 0.037603, loss: 3.8375
2022-10-11 15:38:01 - train: epoch 0058, iter [05000, 05004], lr: 0.037573, loss: 4.0457
2022-10-11 15:38:03 - train: epoch 058, train_loss: 4.1832
2022-10-11 15:39:22 - eval: epoch: 058, acc1: 64.570%, acc5: 86.536%, test_loss: 1.7761, per_image_load_time: 0.282ms, per_image_inference_time: 0.531ms
2022-10-11 15:39:23 - until epoch: 058, best_acc1: 64.570%
2022-10-11 15:39:23 - epoch 059 lr: 0.037572
2022-10-11 15:40:05 - train: epoch 0059, iter [00100, 05004], lr: 0.037541, loss: 4.5341
2022-10-11 15:40:41 - train: epoch 0059, iter [00200, 05004], lr: 0.037511, loss: 4.2892
2022-10-11 15:41:17 - train: epoch 0059, iter [00300, 05004], lr: 0.037481, loss: 4.3803
2022-10-11 15:41:55 - train: epoch 0059, iter [00400, 05004], lr: 0.037450, loss: 2.9840
2022-10-11 15:42:34 - train: epoch 0059, iter [00500, 05004], lr: 0.037420, loss: 4.0519
2022-10-11 15:43:10 - train: epoch 0059, iter [00600, 05004], lr: 0.037389, loss: 4.5548
2022-10-11 15:43:46 - train: epoch 0059, iter [00700, 05004], lr: 0.037359, loss: 3.7120
2022-10-11 15:44:23 - train: epoch 0059, iter [00800, 05004], lr: 0.037329, loss: 4.0871
2022-10-11 15:45:00 - train: epoch 0059, iter [00900, 05004], lr: 0.037298, loss: 4.7219
2022-10-11 15:45:36 - train: epoch 0059, iter [01000, 05004], lr: 0.037268, loss: 4.3525
2022-10-11 15:46:13 - train: epoch 0059, iter [01100, 05004], lr: 0.037238, loss: 4.5693
2022-10-11 15:46:50 - train: epoch 0059, iter [01200, 05004], lr: 0.037207, loss: 4.0550
2022-10-11 15:47:26 - train: epoch 0059, iter [01300, 05004], lr: 0.037177, loss: 3.6685
2022-10-11 15:48:02 - train: epoch 0059, iter [01400, 05004], lr: 0.037147, loss: 3.6919
2022-10-11 15:48:39 - train: epoch 0059, iter [01500, 05004], lr: 0.037116, loss: 3.8287
2022-10-11 15:49:15 - train: epoch 0059, iter [01600, 05004], lr: 0.037086, loss: 4.0970
2022-10-11 15:49:52 - train: epoch 0059, iter [01700, 05004], lr: 0.037056, loss: 4.5186
2022-10-11 15:50:29 - train: epoch 0059, iter [01800, 05004], lr: 0.037025, loss: 4.0681
2022-10-11 15:51:05 - train: epoch 0059, iter [01900, 05004], lr: 0.036995, loss: 4.2658
2022-10-11 15:51:41 - train: epoch 0059, iter [02000, 05004], lr: 0.036965, loss: 4.4614
2022-10-11 15:52:18 - train: epoch 0059, iter [02100, 05004], lr: 0.036934, loss: 4.6443
2022-10-11 15:52:55 - train: epoch 0059, iter [02200, 05004], lr: 0.036904, loss: 4.7099
2022-10-11 15:53:31 - train: epoch 0059, iter [02300, 05004], lr: 0.036874, loss: 4.5102
2022-10-11 15:54:07 - train: epoch 0059, iter [02400, 05004], lr: 0.036844, loss: 3.0584
2022-10-11 15:54:44 - train: epoch 0059, iter [02500, 05004], lr: 0.036813, loss: 4.5150
2022-10-11 15:55:21 - train: epoch 0059, iter [02600, 05004], lr: 0.036783, loss: 4.6693
2022-10-11 15:55:58 - train: epoch 0059, iter [02700, 05004], lr: 0.036753, loss: 4.1812
2022-10-11 15:56:33 - train: epoch 0059, iter [02800, 05004], lr: 0.036722, loss: 3.6966
2022-10-11 15:57:10 - train: epoch 0059, iter [02900, 05004], lr: 0.036692, loss: 3.8537
2022-10-11 15:57:46 - train: epoch 0059, iter [03000, 05004], lr: 0.036662, loss: 4.1804
2022-10-11 15:58:24 - train: epoch 0059, iter [03100, 05004], lr: 0.036632, loss: 4.1750
2022-10-11 15:59:00 - train: epoch 0059, iter [03200, 05004], lr: 0.036601, loss: 4.8151
2022-10-11 15:59:38 - train: epoch 0059, iter [03300, 05004], lr: 0.036571, loss: 4.6215
2022-10-11 16:00:15 - train: epoch 0059, iter [03400, 05004], lr: 0.036541, loss: 4.4691
2022-10-11 16:00:51 - train: epoch 0059, iter [03500, 05004], lr: 0.036511, loss: 4.1803
2022-10-11 16:01:29 - train: epoch 0059, iter [03600, 05004], lr: 0.036481, loss: 4.6471
2022-10-11 16:02:06 - train: epoch 0059, iter [03700, 05004], lr: 0.036450, loss: 3.4112
2022-10-11 16:02:42 - train: epoch 0059, iter [03800, 05004], lr: 0.036420, loss: 4.3784
2022-10-11 16:03:19 - train: epoch 0059, iter [03900, 05004], lr: 0.036390, loss: 3.8912
2022-10-11 16:03:56 - train: epoch 0059, iter [04000, 05004], lr: 0.036360, loss: 4.0363
2022-10-11 16:04:33 - train: epoch 0059, iter [04100, 05004], lr: 0.036330, loss: 4.5987
2022-10-11 16:05:09 - train: epoch 0059, iter [04200, 05004], lr: 0.036299, loss: 3.1866
2022-10-11 16:05:46 - train: epoch 0059, iter [04300, 05004], lr: 0.036269, loss: 3.7280
2022-10-11 16:06:22 - train: epoch 0059, iter [04400, 05004], lr: 0.036239, loss: 4.7788
2022-10-11 16:06:59 - train: epoch 0059, iter [04500, 05004], lr: 0.036209, loss: 4.3828
2022-10-11 16:07:36 - train: epoch 0059, iter [04600, 05004], lr: 0.036179, loss: 4.6576
2022-10-11 16:08:13 - train: epoch 0059, iter [04700, 05004], lr: 0.036148, loss: 3.9569
2022-10-11 16:08:49 - train: epoch 0059, iter [04800, 05004], lr: 0.036118, loss: 4.0856
2022-10-11 16:09:26 - train: epoch 0059, iter [04900, 05004], lr: 0.036088, loss: 4.5634
2022-10-11 16:10:01 - train: epoch 0059, iter [05000, 05004], lr: 0.036058, loss: 4.7254
2022-10-11 16:10:03 - train: epoch 059, train_loss: 4.1629
2022-10-11 16:11:22 - eval: epoch: 059, acc1: 63.032%, acc5: 85.600%, test_loss: 1.8805, per_image_load_time: 0.234ms, per_image_inference_time: 0.526ms
2022-10-11 16:11:22 - until epoch: 059, best_acc1: 64.570%
2022-10-11 16:11:22 - epoch 060 lr: 0.036057
2022-10-11 16:12:05 - train: epoch 0060, iter [00100, 05004], lr: 0.036027, loss: 4.3199
2022-10-11 16:12:42 - train: epoch 0060, iter [00200, 05004], lr: 0.035997, loss: 4.0790
2022-10-11 16:13:18 - train: epoch 0060, iter [00300, 05004], lr: 0.035966, loss: 4.3285
2022-10-11 16:13:55 - train: epoch 0060, iter [00400, 05004], lr: 0.035936, loss: 4.8770
2022-10-11 16:14:32 - train: epoch 0060, iter [00500, 05004], lr: 0.035906, loss: 3.5700
2022-10-11 16:15:08 - train: epoch 0060, iter [00600, 05004], lr: 0.035876, loss: 3.5243
2022-10-11 16:15:45 - train: epoch 0060, iter [00700, 05004], lr: 0.035846, loss: 3.9158
2022-10-11 16:16:21 - train: epoch 0060, iter [00800, 05004], lr: 0.035816, loss: 4.0677
2022-10-11 16:16:59 - train: epoch 0060, iter [00900, 05004], lr: 0.035786, loss: 3.9914
2022-10-11 16:17:35 - train: epoch 0060, iter [01000, 05004], lr: 0.035756, loss: 4.4241
2022-10-11 16:18:12 - train: epoch 0060, iter [01100, 05004], lr: 0.035726, loss: 3.9478
2022-10-11 16:18:48 - train: epoch 0060, iter [01200, 05004], lr: 0.035696, loss: 4.0843
2022-10-11 16:19:25 - train: epoch 0060, iter [01300, 05004], lr: 0.035665, loss: 4.7085
2022-10-11 16:20:02 - train: epoch 0060, iter [01400, 05004], lr: 0.035635, loss: 3.6400
2022-10-11 16:20:39 - train: epoch 0060, iter [01500, 05004], lr: 0.035605, loss: 4.2589
2022-10-11 16:21:17 - train: epoch 0060, iter [01600, 05004], lr: 0.035575, loss: 4.6566
2022-10-11 16:21:53 - train: epoch 0060, iter [01700, 05004], lr: 0.035545, loss: 3.5415
2022-10-11 16:22:30 - train: epoch 0060, iter [01800, 05004], lr: 0.035515, loss: 4.0434
2022-10-11 16:23:07 - train: epoch 0060, iter [01900, 05004], lr: 0.035485, loss: 3.5776
2022-10-11 16:23:44 - train: epoch 0060, iter [02000, 05004], lr: 0.035455, loss: 3.9836
2022-10-11 16:24:20 - train: epoch 0060, iter [02100, 05004], lr: 0.035425, loss: 5.1174
2022-10-11 16:24:57 - train: epoch 0060, iter [02200, 05004], lr: 0.035395, loss: 4.8624
2022-10-11 16:25:33 - train: epoch 0060, iter [02300, 05004], lr: 0.035365, loss: 4.3602
2022-10-11 16:26:10 - train: epoch 0060, iter [02400, 05004], lr: 0.035335, loss: 3.7902
2022-10-11 16:26:47 - train: epoch 0060, iter [02500, 05004], lr: 0.035305, loss: 4.4635
2022-10-11 16:27:23 - train: epoch 0060, iter [02600, 05004], lr: 0.035275, loss: 4.6882
2022-10-11 16:28:00 - train: epoch 0060, iter [02700, 05004], lr: 0.035245, loss: 4.3784
2022-10-11 16:28:38 - train: epoch 0060, iter [02800, 05004], lr: 0.035215, loss: 4.3958
2022-10-11 16:29:14 - train: epoch 0060, iter [02900, 05004], lr: 0.035185, loss: 4.8994
2022-10-11 16:29:50 - train: epoch 0060, iter [03000, 05004], lr: 0.035155, loss: 4.5224
2022-10-11 16:30:27 - train: epoch 0060, iter [03100, 05004], lr: 0.035125, loss: 3.9979
2022-10-11 16:31:04 - train: epoch 0060, iter [03200, 05004], lr: 0.035095, loss: 3.5156
2022-10-11 16:31:40 - train: epoch 0060, iter [03300, 05004], lr: 0.035065, loss: 4.3421
2022-10-11 16:32:17 - train: epoch 0060, iter [03400, 05004], lr: 0.035035, loss: 4.5414
2022-10-11 16:32:54 - train: epoch 0060, iter [03500, 05004], lr: 0.035005, loss: 4.2677
2022-10-11 16:33:30 - train: epoch 0060, iter [03600, 05004], lr: 0.034975, loss: 3.5723
2022-10-11 16:34:06 - train: epoch 0060, iter [03700, 05004], lr: 0.034945, loss: 4.4485
2022-10-11 16:34:43 - train: epoch 0060, iter [03800, 05004], lr: 0.034916, loss: 4.2646
2022-10-11 16:35:20 - train: epoch 0060, iter [03900, 05004], lr: 0.034886, loss: 3.9488
2022-10-11 16:35:57 - train: epoch 0060, iter [04000, 05004], lr: 0.034856, loss: 3.4498
2022-10-11 16:36:34 - train: epoch 0060, iter [04100, 05004], lr: 0.034826, loss: 3.7275
2022-10-11 16:37:10 - train: epoch 0060, iter [04200, 05004], lr: 0.034796, loss: 3.7477
2022-10-11 16:37:47 - train: epoch 0060, iter [04300, 05004], lr: 0.034766, loss: 4.4719
2022-10-11 16:38:24 - train: epoch 0060, iter [04400, 05004], lr: 0.034736, loss: 4.2033
2022-10-11 16:39:00 - train: epoch 0060, iter [04500, 05004], lr: 0.034706, loss: 3.4834
2022-10-11 16:39:37 - train: epoch 0060, iter [04600, 05004], lr: 0.034676, loss: 4.3927
2022-10-11 16:40:13 - train: epoch 0060, iter [04700, 05004], lr: 0.034646, loss: 4.0638
2022-10-11 16:40:50 - train: epoch 0060, iter [04800, 05004], lr: 0.034617, loss: 4.1335
2022-10-11 16:41:27 - train: epoch 0060, iter [04900, 05004], lr: 0.034587, loss: 4.2413
2022-10-11 16:42:01 - train: epoch 0060, iter [05000, 05004], lr: 0.034557, loss: 4.0151
2022-10-11 16:42:03 - train: epoch 060, train_loss: 4.1438
2022-10-11 16:43:21 - eval: epoch: 060, acc1: 64.542%, acc5: 86.770%, test_loss: 1.7771, per_image_load_time: 0.256ms, per_image_inference_time: 0.519ms
2022-10-11 16:43:21 - until epoch: 060, best_acc1: 64.570%
2022-10-11 16:43:21 - epoch 061 lr: 0.034556
2022-10-11 16:44:04 - train: epoch 0061, iter [00100, 05004], lr: 0.034526, loss: 4.1125
2022-10-11 16:44:40 - train: epoch 0061, iter [00200, 05004], lr: 0.034496, loss: 3.8353
2022-10-11 16:45:16 - train: epoch 0061, iter [00300, 05004], lr: 0.034466, loss: 4.0523
2022-10-11 16:45:52 - train: epoch 0061, iter [00400, 05004], lr: 0.034436, loss: 4.3262
2022-10-11 16:46:30 - train: epoch 0061, iter [00500, 05004], lr: 0.034407, loss: 4.2854
2022-10-11 16:47:05 - train: epoch 0061, iter [00600, 05004], lr: 0.034377, loss: 4.4355
2022-10-11 16:47:42 - train: epoch 0061, iter [00700, 05004], lr: 0.034347, loss: 3.8542
2022-10-11 16:48:18 - train: epoch 0061, iter [00800, 05004], lr: 0.034317, loss: 3.6473
2022-10-11 16:48:56 - train: epoch 0061, iter [00900, 05004], lr: 0.034287, loss: 4.6982
2022-10-11 16:49:33 - train: epoch 0061, iter [01000, 05004], lr: 0.034257, loss: 4.1451
2022-10-11 16:50:09 - train: epoch 0061, iter [01100, 05004], lr: 0.034228, loss: 4.2373
2022-10-11 16:50:46 - train: epoch 0061, iter [01200, 05004], lr: 0.034198, loss: 3.7986
2022-10-11 16:51:23 - train: epoch 0061, iter [01300, 05004], lr: 0.034168, loss: 4.2973
2022-10-11 16:51:59 - train: epoch 0061, iter [01400, 05004], lr: 0.034138, loss: 4.0565
2022-10-11 16:52:35 - train: epoch 0061, iter [01500, 05004], lr: 0.034109, loss: 3.8885
2022-10-11 16:53:12 - train: epoch 0061, iter [01600, 05004], lr: 0.034079, loss: 3.8161
2022-10-11 16:53:48 - train: epoch 0061, iter [01700, 05004], lr: 0.034049, loss: 3.7442
2022-10-11 16:54:24 - train: epoch 0061, iter [01800, 05004], lr: 0.034019, loss: 4.2580
2022-10-11 16:55:01 - train: epoch 0061, iter [01900, 05004], lr: 0.033990, loss: 4.1641
2022-10-11 16:55:38 - train: epoch 0061, iter [02000, 05004], lr: 0.033960, loss: 4.3967
2022-10-11 16:56:15 - train: epoch 0061, iter [02100, 05004], lr: 0.033930, loss: 4.0796
2022-10-11 16:56:51 - train: epoch 0061, iter [02200, 05004], lr: 0.033900, loss: 3.7626
2022-10-11 16:57:28 - train: epoch 0061, iter [02300, 05004], lr: 0.033871, loss: 3.5738
2022-10-11 16:58:03 - train: epoch 0061, iter [02400, 05004], lr: 0.033841, loss: 2.9938
2022-10-11 16:58:41 - train: epoch 0061, iter [02500, 05004], lr: 0.033811, loss: 4.4960
2022-10-11 16:59:17 - train: epoch 0061, iter [02600, 05004], lr: 0.033782, loss: 3.2491
2022-10-11 16:59:54 - train: epoch 0061, iter [02700, 05004], lr: 0.033752, loss: 4.2798
2022-10-11 17:00:31 - train: epoch 0061, iter [02800, 05004], lr: 0.033722, loss: 4.0015
2022-10-11 17:01:07 - train: epoch 0061, iter [02900, 05004], lr: 0.033693, loss: 4.3669
2022-10-11 17:01:44 - train: epoch 0061, iter [03000, 05004], lr: 0.033663, loss: 4.0750
2022-10-11 17:02:22 - train: epoch 0061, iter [03100, 05004], lr: 0.033633, loss: 4.0837
2022-10-11 17:02:59 - train: epoch 0061, iter [03200, 05004], lr: 0.033604, loss: 3.8467
2022-10-11 17:03:35 - train: epoch 0061, iter [03300, 05004], lr: 0.033574, loss: 4.2123
2022-10-11 17:04:12 - train: epoch 0061, iter [03400, 05004], lr: 0.033544, loss: 4.7110
2022-10-11 17:04:49 - train: epoch 0061, iter [03500, 05004], lr: 0.033515, loss: 4.9423
2022-10-11 17:05:25 - train: epoch 0061, iter [03600, 05004], lr: 0.033485, loss: 4.6715
2022-10-11 17:06:03 - train: epoch 0061, iter [03700, 05004], lr: 0.033455, loss: 4.3992
2022-10-11 17:06:40 - train: epoch 0061, iter [03800, 05004], lr: 0.033426, loss: 4.7325
2022-10-11 17:07:16 - train: epoch 0061, iter [03900, 05004], lr: 0.033396, loss: 4.3081
2022-10-11 17:07:53 - train: epoch 0061, iter [04000, 05004], lr: 0.033367, loss: 3.7945
2022-10-11 17:08:29 - train: epoch 0061, iter [04100, 05004], lr: 0.033337, loss: 3.7930
2022-10-11 17:09:06 - train: epoch 0061, iter [04200, 05004], lr: 0.033307, loss: 4.2475
2022-10-11 17:09:43 - train: epoch 0061, iter [04300, 05004], lr: 0.033278, loss: 3.4212
2022-10-11 17:10:20 - train: epoch 0061, iter [04400, 05004], lr: 0.033248, loss: 4.2647
2022-10-11 17:10:57 - train: epoch 0061, iter [04500, 05004], lr: 0.033219, loss: 4.1206
2022-10-11 17:11:35 - train: epoch 0061, iter [04600, 05004], lr: 0.033189, loss: 3.8098
2022-10-11 17:12:12 - train: epoch 0061, iter [04700, 05004], lr: 0.033160, loss: 4.0723
2022-10-11 17:12:49 - train: epoch 0061, iter [04800, 05004], lr: 0.033130, loss: 4.2376
2022-10-11 17:13:25 - train: epoch 0061, iter [04900, 05004], lr: 0.033101, loss: 4.3434
2022-10-11 17:14:00 - train: epoch 0061, iter [05000, 05004], lr: 0.033071, loss: 4.5776
2022-10-11 17:14:02 - train: epoch 061, train_loss: 4.1343
2022-10-11 17:15:21 - eval: epoch: 061, acc1: 64.024%, acc5: 86.434%, test_loss: 1.7594, per_image_load_time: 0.250ms, per_image_inference_time: 0.516ms
2022-10-11 17:15:21 - until epoch: 061, best_acc1: 64.570%
2022-10-11 17:15:21 - epoch 062 lr: 0.033070
2022-10-11 17:16:04 - train: epoch 0062, iter [00100, 05004], lr: 0.033040, loss: 5.0457
2022-10-11 17:16:40 - train: epoch 0062, iter [00200, 05004], lr: 0.033011, loss: 4.0901
2022-10-11 17:17:17 - train: epoch 0062, iter [00300, 05004], lr: 0.032981, loss: 3.8434
2022-10-11 17:17:54 - train: epoch 0062, iter [00400, 05004], lr: 0.032952, loss: 4.8670
2022-10-11 17:18:30 - train: epoch 0062, iter [00500, 05004], lr: 0.032922, loss: 3.7869
2022-10-11 17:19:06 - train: epoch 0062, iter [00600, 05004], lr: 0.032893, loss: 4.3213
2022-10-11 17:19:42 - train: epoch 0062, iter [00700, 05004], lr: 0.032863, loss: 3.9152
2022-10-11 17:20:19 - train: epoch 0062, iter [00800, 05004], lr: 0.032834, loss: 4.1005
2022-10-11 17:20:55 - train: epoch 0062, iter [00900, 05004], lr: 0.032804, loss: 4.4334
2022-10-11 17:21:31 - train: epoch 0062, iter [01000, 05004], lr: 0.032775, loss: 4.0501
2022-10-11 17:22:07 - train: epoch 0062, iter [01100, 05004], lr: 0.032745, loss: 3.9169
2022-10-11 17:22:45 - train: epoch 0062, iter [01200, 05004], lr: 0.032716, loss: 3.7765
2022-10-11 17:23:21 - train: epoch 0062, iter [01300, 05004], lr: 0.032686, loss: 3.9320
2022-10-11 17:23:58 - train: epoch 0062, iter [01400, 05004], lr: 0.032657, loss: 4.1908
2022-10-11 17:24:34 - train: epoch 0062, iter [01500, 05004], lr: 0.032628, loss: 3.7718
2022-10-11 17:25:10 - train: epoch 0062, iter [01600, 05004], lr: 0.032598, loss: 3.6751
2022-10-11 17:25:47 - train: epoch 0062, iter [01700, 05004], lr: 0.032569, loss: 4.5316
2022-10-11 17:26:23 - train: epoch 0062, iter [01800, 05004], lr: 0.032539, loss: 4.4752
2022-10-11 17:27:00 - train: epoch 0062, iter [01900, 05004], lr: 0.032510, loss: 4.0398
2022-10-11 17:27:37 - train: epoch 0062, iter [02000, 05004], lr: 0.032481, loss: 4.6197
2022-10-11 17:28:13 - train: epoch 0062, iter [02100, 05004], lr: 0.032451, loss: 3.1884
2022-10-11 17:28:49 - train: epoch 0062, iter [02200, 05004], lr: 0.032422, loss: 4.5301
2022-10-11 17:29:25 - train: epoch 0062, iter [02300, 05004], lr: 0.032392, loss: 3.9135
2022-10-11 17:30:02 - train: epoch 0062, iter [02400, 05004], lr: 0.032363, loss: 4.1012
2022-10-11 17:30:38 - train: epoch 0062, iter [02500, 05004], lr: 0.032334, loss: 3.9269
2022-10-11 17:31:15 - train: epoch 0062, iter [02600, 05004], lr: 0.032304, loss: 3.8653
2022-10-11 17:31:51 - train: epoch 0062, iter [02700, 05004], lr: 0.032275, loss: 4.5674
2022-10-11 17:32:28 - train: epoch 0062, iter [02800, 05004], lr: 0.032246, loss: 4.7459
2022-10-11 17:33:05 - train: epoch 0062, iter [02900, 05004], lr: 0.032216, loss: 4.5804
2022-10-11 17:33:41 - train: epoch 0062, iter [03000, 05004], lr: 0.032187, loss: 4.8265
2022-10-11 17:34:17 - train: epoch 0062, iter [03100, 05004], lr: 0.032158, loss: 4.0186
2022-10-11 17:34:54 - train: epoch 0062, iter [03200, 05004], lr: 0.032128, loss: 5.0013
2022-10-11 17:35:31 - train: epoch 0062, iter [03300, 05004], lr: 0.032099, loss: 3.5106
2022-10-11 17:36:08 - train: epoch 0062, iter [03400, 05004], lr: 0.032070, loss: 4.5393
2022-10-11 17:36:43 - train: epoch 0062, iter [03500, 05004], lr: 0.032040, loss: 4.3724
2022-10-11 17:37:20 - train: epoch 0062, iter [03600, 05004], lr: 0.032011, loss: 4.3951
2022-10-11 17:37:58 - train: epoch 0062, iter [03700, 05004], lr: 0.031982, loss: 3.7679
2022-10-11 17:38:34 - train: epoch 0062, iter [03800, 05004], lr: 0.031953, loss: 4.1697
2022-10-11 17:39:10 - train: epoch 0062, iter [03900, 05004], lr: 0.031923, loss: 4.6770
2022-10-11 17:39:47 - train: epoch 0062, iter [04000, 05004], lr: 0.031894, loss: 3.9450
2022-10-11 17:40:24 - train: epoch 0062, iter [04100, 05004], lr: 0.031865, loss: 4.4048
2022-10-11 17:41:00 - train: epoch 0062, iter [04200, 05004], lr: 0.031835, loss: 3.7063
2022-10-11 17:41:36 - train: epoch 0062, iter [04300, 05004], lr: 0.031806, loss: 4.5417
2022-10-11 17:42:13 - train: epoch 0062, iter [04400, 05004], lr: 0.031777, loss: 4.1167
2022-10-11 17:42:50 - train: epoch 0062, iter [04500, 05004], lr: 0.031748, loss: 4.2512
2022-10-11 17:43:27 - train: epoch 0062, iter [04600, 05004], lr: 0.031719, loss: 3.6109
2022-10-11 17:44:04 - train: epoch 0062, iter [04700, 05004], lr: 0.031689, loss: 4.2368
2022-10-11 17:44:41 - train: epoch 0062, iter [04800, 05004], lr: 0.031660, loss: 3.9780
2022-10-11 17:45:18 - train: epoch 0062, iter [04900, 05004], lr: 0.031631, loss: 3.2867
2022-10-11 17:45:53 - train: epoch 0062, iter [05000, 05004], lr: 0.031602, loss: 4.3015
2022-10-11 17:45:55 - train: epoch 062, train_loss: 4.1133
2022-10-11 17:47:13 - eval: epoch: 062, acc1: 64.334%, acc5: 86.446%, test_loss: 1.7361, per_image_load_time: 0.267ms, per_image_inference_time: 0.510ms
2022-10-11 17:47:13 - until epoch: 062, best_acc1: 64.570%
2022-10-11 17:47:13 - epoch 063 lr: 0.031601
2022-10-11 17:47:55 - train: epoch 0063, iter [00100, 05004], lr: 0.031571, loss: 3.8309
2022-10-11 17:48:32 - train: epoch 0063, iter [00200, 05004], lr: 0.031542, loss: 4.7736
2022-10-11 17:49:08 - train: epoch 0063, iter [00300, 05004], lr: 0.031513, loss: 4.2959
2022-10-11 17:49:45 - train: epoch 0063, iter [00400, 05004], lr: 0.031484, loss: 3.9989
2022-10-11 17:50:21 - train: epoch 0063, iter [00500, 05004], lr: 0.031455, loss: 3.4024
2022-10-11 17:50:57 - train: epoch 0063, iter [00600, 05004], lr: 0.031426, loss: 4.5754
2022-10-11 17:51:35 - train: epoch 0063, iter [00700, 05004], lr: 0.031397, loss: 4.1809
2022-10-11 17:52:11 - train: epoch 0063, iter [00800, 05004], lr: 0.031367, loss: 3.5195
2022-10-11 17:52:48 - train: epoch 0063, iter [00900, 05004], lr: 0.031338, loss: 3.8880
2022-10-11 17:53:25 - train: epoch 0063, iter [01000, 05004], lr: 0.031309, loss: 4.1420
2022-10-11 17:54:02 - train: epoch 0063, iter [01100, 05004], lr: 0.031280, loss: 4.1297
2022-10-11 17:54:38 - train: epoch 0063, iter [01200, 05004], lr: 0.031251, loss: 3.2327
2022-10-11 17:55:15 - train: epoch 0063, iter [01300, 05004], lr: 0.031222, loss: 4.4621
2022-10-11 17:55:52 - train: epoch 0063, iter [01400, 05004], lr: 0.031193, loss: 4.0582
2022-10-11 17:56:29 - train: epoch 0063, iter [01500, 05004], lr: 0.031164, loss: 3.8907
2022-10-11 17:57:05 - train: epoch 0063, iter [01600, 05004], lr: 0.031135, loss: 4.4796
2022-10-11 17:57:42 - train: epoch 0063, iter [01700, 05004], lr: 0.031106, loss: 3.8114
2022-10-11 17:58:19 - train: epoch 0063, iter [01800, 05004], lr: 0.031076, loss: 4.3257
2022-10-11 17:58:56 - train: epoch 0063, iter [01900, 05004], lr: 0.031047, loss: 3.7923
2022-10-11 17:59:33 - train: epoch 0063, iter [02000, 05004], lr: 0.031018, loss: 4.1776
2022-10-11 18:00:09 - train: epoch 0063, iter [02100, 05004], lr: 0.030989, loss: 4.2624
2022-10-11 18:00:46 - train: epoch 0063, iter [02200, 05004], lr: 0.030960, loss: 4.0889
2022-10-11 18:01:25 - train: epoch 0063, iter [02300, 05004], lr: 0.030931, loss: 4.9376
2022-10-11 18:02:01 - train: epoch 0063, iter [02400, 05004], lr: 0.030902, loss: 4.6481
2022-10-11 18:02:38 - train: epoch 0063, iter [02500, 05004], lr: 0.030873, loss: 4.0605
2022-10-11 18:03:14 - train: epoch 0063, iter [02600, 05004], lr: 0.030844, loss: 4.2025
2022-10-11 18:03:51 - train: epoch 0063, iter [02700, 05004], lr: 0.030815, loss: 4.1087
2022-10-11 18:04:28 - train: epoch 0063, iter [02800, 05004], lr: 0.030786, loss: 4.4476
2022-10-11 18:05:05 - train: epoch 0063, iter [02900, 05004], lr: 0.030757, loss: 3.2158
2022-10-11 18:05:42 - train: epoch 0063, iter [03000, 05004], lr: 0.030728, loss: 4.8261
2022-10-11 18:06:19 - train: epoch 0063, iter [03100, 05004], lr: 0.030699, loss: 4.8822
2022-10-11 18:06:55 - train: epoch 0063, iter [03200, 05004], lr: 0.030671, loss: 4.2825
2022-10-11 18:07:32 - train: epoch 0063, iter [03300, 05004], lr: 0.030642, loss: 3.9899
2022-10-11 18:08:08 - train: epoch 0063, iter [03400, 05004], lr: 0.030613, loss: 4.2767
2022-10-11 18:08:46 - train: epoch 0063, iter [03500, 05004], lr: 0.030584, loss: 4.4348
2022-10-11 18:09:22 - train: epoch 0063, iter [03600, 05004], lr: 0.030555, loss: 4.0888
2022-10-11 18:09:59 - train: epoch 0063, iter [03700, 05004], lr: 0.030526, loss: 4.4833
2022-10-11 18:10:36 - train: epoch 0063, iter [03800, 05004], lr: 0.030497, loss: 4.3287
2022-10-11 18:11:13 - train: epoch 0063, iter [03900, 05004], lr: 0.030468, loss: 3.8051
2022-10-11 18:11:50 - train: epoch 0063, iter [04000, 05004], lr: 0.030439, loss: 4.5139
2022-10-11 18:12:27 - train: epoch 0063, iter [04100, 05004], lr: 0.030410, loss: 4.7827
2022-10-11 18:13:04 - train: epoch 0063, iter [04200, 05004], lr: 0.030381, loss: 3.7199
2022-10-11 18:13:41 - train: epoch 0063, iter [04300, 05004], lr: 0.030353, loss: 4.0411
2022-10-11 18:14:18 - train: epoch 0063, iter [04400, 05004], lr: 0.030324, loss: 3.9350
2022-10-11 18:14:54 - train: epoch 0063, iter [04500, 05004], lr: 0.030295, loss: 4.0405
2022-10-11 18:15:31 - train: epoch 0063, iter [04600, 05004], lr: 0.030266, loss: 3.9904
2022-10-11 18:16:08 - train: epoch 0063, iter [04700, 05004], lr: 0.030237, loss: 3.9550
2022-10-11 18:16:44 - train: epoch 0063, iter [04800, 05004], lr: 0.030208, loss: 4.0901
2022-10-11 18:17:22 - train: epoch 0063, iter [04900, 05004], lr: 0.030180, loss: 4.0822
2022-10-11 18:17:57 - train: epoch 0063, iter [05000, 05004], lr: 0.030151, loss: 4.6096
2022-10-11 18:18:00 - train: epoch 063, train_loss: 4.1022
2022-10-11 18:19:18 - eval: epoch: 063, acc1: 64.934%, acc5: 86.914%, test_loss: 1.7743, per_image_load_time: 0.249ms, per_image_inference_time: 0.506ms
2022-10-11 18:19:19 - until epoch: 063, best_acc1: 64.934%
2022-10-11 18:19:19 - epoch 064 lr: 0.030150
2022-10-11 18:20:02 - train: epoch 0064, iter [00100, 05004], lr: 0.030121, loss: 3.8233
2022-10-11 18:20:38 - train: epoch 0064, iter [00200, 05004], lr: 0.030092, loss: 3.5382
2022-10-11 18:21:15 - train: epoch 0064, iter [00300, 05004], lr: 0.030063, loss: 3.9992
2022-10-11 18:21:51 - train: epoch 0064, iter [00400, 05004], lr: 0.030034, loss: 4.3800
2022-10-11 18:22:28 - train: epoch 0064, iter [00500, 05004], lr: 0.030006, loss: 4.1439
2022-10-11 18:23:05 - train: epoch 0064, iter [00600, 05004], lr: 0.029977, loss: 3.8669
2022-10-11 18:23:41 - train: epoch 0064, iter [00700, 05004], lr: 0.029948, loss: 3.6632
2022-10-11 18:24:18 - train: epoch 0064, iter [00800, 05004], lr: 0.029919, loss: 4.8663
2022-10-11 18:24:55 - train: epoch 0064, iter [00900, 05004], lr: 0.029891, loss: 3.9534
2022-10-11 18:25:31 - train: epoch 0064, iter [01000, 05004], lr: 0.029862, loss: 4.3463
2022-10-11 18:26:08 - train: epoch 0064, iter [01100, 05004], lr: 0.029833, loss: 4.7798
2022-10-11 18:26:45 - train: epoch 0064, iter [01200, 05004], lr: 0.029804, loss: 3.9966
2022-10-11 18:27:21 - train: epoch 0064, iter [01300, 05004], lr: 0.029776, loss: 4.0446
2022-10-11 18:27:58 - train: epoch 0064, iter [01400, 05004], lr: 0.029747, loss: 4.1466
2022-10-11 18:28:35 - train: epoch 0064, iter [01500, 05004], lr: 0.029718, loss: 4.0741
2022-10-11 18:29:11 - train: epoch 0064, iter [01600, 05004], lr: 0.029690, loss: 3.6388
2022-10-11 18:29:49 - train: epoch 0064, iter [01700, 05004], lr: 0.029661, loss: 4.2581
2022-10-11 18:30:25 - train: epoch 0064, iter [01800, 05004], lr: 0.029632, loss: 3.9798
2022-10-11 18:31:02 - train: epoch 0064, iter [01900, 05004], lr: 0.029604, loss: 3.8341
2022-10-11 18:31:39 - train: epoch 0064, iter [02000, 05004], lr: 0.029575, loss: 3.9083
2022-10-11 18:32:16 - train: epoch 0064, iter [02100, 05004], lr: 0.029546, loss: 3.6865
2022-10-11 18:32:53 - train: epoch 0064, iter [02200, 05004], lr: 0.029518, loss: 3.9912
2022-10-11 18:33:30 - train: epoch 0064, iter [02300, 05004], lr: 0.029489, loss: 4.4485
2022-10-11 18:34:05 - train: epoch 0064, iter [02400, 05004], lr: 0.029461, loss: 3.3322
2022-10-11 18:34:42 - train: epoch 0064, iter [02500, 05004], lr: 0.029432, loss: 4.3185
2022-10-11 18:35:19 - train: epoch 0064, iter [02600, 05004], lr: 0.029403, loss: 3.8435
2022-10-11 18:35:56 - train: epoch 0064, iter [02700, 05004], lr: 0.029375, loss: 3.9545
2022-10-11 18:36:33 - train: epoch 0064, iter [02800, 05004], lr: 0.029346, loss: 3.6784
2022-10-11 18:37:10 - train: epoch 0064, iter [02900, 05004], lr: 0.029318, loss: 4.3723
2022-10-11 18:37:48 - train: epoch 0064, iter [03000, 05004], lr: 0.029289, loss: 3.9857
2022-10-11 18:38:24 - train: epoch 0064, iter [03100, 05004], lr: 0.029260, loss: 4.6272
2022-10-11 18:39:00 - train: epoch 0064, iter [03200, 05004], lr: 0.029232, loss: 3.9841
2022-10-11 18:39:37 - train: epoch 0064, iter [03300, 05004], lr: 0.029203, loss: 4.3597
2022-10-11 18:40:14 - train: epoch 0064, iter [03400, 05004], lr: 0.029175, loss: 3.8859
2022-10-11 18:40:52 - train: epoch 0064, iter [03500, 05004], lr: 0.029146, loss: 4.0782
2022-10-11 18:41:29 - train: epoch 0064, iter [03600, 05004], lr: 0.029118, loss: 4.4558
2022-10-11 18:42:06 - train: epoch 0064, iter [03700, 05004], lr: 0.029089, loss: 4.1426
2022-10-11 18:42:43 - train: epoch 0064, iter [03800, 05004], lr: 0.029061, loss: 4.6390
2022-10-11 18:43:20 - train: epoch 0064, iter [03900, 05004], lr: 0.029032, loss: 4.6679
2022-10-11 18:43:57 - train: epoch 0064, iter [04000, 05004], lr: 0.029004, loss: 4.3923
2022-10-11 18:44:33 - train: epoch 0064, iter [04100, 05004], lr: 0.028975, loss: 4.1338
2022-10-11 18:45:11 - train: epoch 0064, iter [04200, 05004], lr: 0.028947, loss: 3.2358
2022-10-11 18:45:48 - train: epoch 0064, iter [04300, 05004], lr: 0.028918, loss: 3.4233
2022-10-11 18:46:25 - train: epoch 0064, iter [04400, 05004], lr: 0.028890, loss: 3.9659
2022-10-11 18:47:02 - train: epoch 0064, iter [04500, 05004], lr: 0.028861, loss: 3.5096
2022-10-11 18:47:39 - train: epoch 0064, iter [04600, 05004], lr: 0.028833, loss: 3.6148
2022-10-11 18:48:16 - train: epoch 0064, iter [04700, 05004], lr: 0.028805, loss: 4.6294
2022-10-11 18:48:53 - train: epoch 0064, iter [04800, 05004], lr: 0.028776, loss: 4.1445
2022-10-11 18:49:29 - train: epoch 0064, iter [04900, 05004], lr: 0.028748, loss: 3.7603
2022-10-11 18:50:04 - train: epoch 0064, iter [05000, 05004], lr: 0.028719, loss: 3.9817
2022-10-11 18:50:06 - train: epoch 064, train_loss: 4.0811
2022-10-11 18:51:25 - eval: epoch: 064, acc1: 65.336%, acc5: 87.280%, test_loss: 1.6810, per_image_load_time: 0.259ms, per_image_inference_time: 0.510ms
2022-10-11 18:51:25 - until epoch: 064, best_acc1: 65.336%
2022-10-11 18:51:25 - epoch 065 lr: 0.028718
2022-10-11 18:52:08 - train: epoch 0065, iter [00100, 05004], lr: 0.028690, loss: 4.6588
2022-10-11 18:52:44 - train: epoch 0065, iter [00200, 05004], lr: 0.028661, loss: 4.3237
2022-10-11 18:53:20 - train: epoch 0065, iter [00300, 05004], lr: 0.028633, loss: 4.1280
2022-10-11 18:53:58 - train: epoch 0065, iter [00400, 05004], lr: 0.028605, loss: 4.5032
2022-10-11 18:54:34 - train: epoch 0065, iter [00500, 05004], lr: 0.028576, loss: 4.2507
2022-10-11 18:55:11 - train: epoch 0065, iter [00600, 05004], lr: 0.028548, loss: 3.6491
2022-10-11 18:55:47 - train: epoch 0065, iter [00700, 05004], lr: 0.028520, loss: 4.2242
2022-10-11 18:56:23 - train: epoch 0065, iter [00800, 05004], lr: 0.028491, loss: 4.3529
2022-10-11 18:57:00 - train: epoch 0065, iter [00900, 05004], lr: 0.028463, loss: 3.6403
2022-10-11 18:57:36 - train: epoch 0065, iter [01000, 05004], lr: 0.028435, loss: 4.1228
2022-10-11 18:58:12 - train: epoch 0065, iter [01100, 05004], lr: 0.028406, loss: 3.9268
2022-10-11 18:58:49 - train: epoch 0065, iter [01200, 05004], lr: 0.028378, loss: 3.7576
2022-10-11 18:59:26 - train: epoch 0065, iter [01300, 05004], lr: 0.028350, loss: 3.9122
2022-10-11 19:00:02 - train: epoch 0065, iter [01400, 05004], lr: 0.028321, loss: 4.1179
2022-10-11 19:00:39 - train: epoch 0065, iter [01500, 05004], lr: 0.028293, loss: 4.3515
2022-10-11 19:01:15 - train: epoch 0065, iter [01600, 05004], lr: 0.028265, loss: 4.3044
2022-10-11 19:01:52 - train: epoch 0065, iter [01700, 05004], lr: 0.028237, loss: 4.5478
2022-10-11 19:02:29 - train: epoch 0065, iter [01800, 05004], lr: 0.028208, loss: 3.8128
2022-10-11 19:03:05 - train: epoch 0065, iter [01900, 05004], lr: 0.028180, loss: 4.4982
2022-10-11 19:03:42 - train: epoch 0065, iter [02000, 05004], lr: 0.028152, loss: 3.8985
2022-10-11 19:04:18 - train: epoch 0065, iter [02100, 05004], lr: 0.028124, loss: 3.9738
2022-10-11 19:04:55 - train: epoch 0065, iter [02200, 05004], lr: 0.028095, loss: 3.8029
2022-10-11 19:05:32 - train: epoch 0065, iter [02300, 05004], lr: 0.028067, loss: 4.4387
2022-10-11 19:06:09 - train: epoch 0065, iter [02400, 05004], lr: 0.028039, loss: 3.6783
2022-10-11 19:06:46 - train: epoch 0065, iter [02500, 05004], lr: 0.028011, loss: 4.3886
2022-10-11 19:07:23 - train: epoch 0065, iter [02600, 05004], lr: 0.027983, loss: 3.6245
2022-10-11 19:08:00 - train: epoch 0065, iter [02700, 05004], lr: 0.027954, loss: 4.3421
2022-10-11 19:08:37 - train: epoch 0065, iter [02800, 05004], lr: 0.027926, loss: 4.2615
2022-10-11 19:09:13 - train: epoch 0065, iter [02900, 05004], lr: 0.027898, loss: 4.9779
2022-10-11 19:09:49 - train: epoch 0065, iter [03000, 05004], lr: 0.027870, loss: 4.0131
2022-10-11 19:10:25 - train: epoch 0065, iter [03100, 05004], lr: 0.027842, loss: 4.7342
2022-10-11 19:11:01 - train: epoch 0065, iter [03200, 05004], lr: 0.027814, loss: 4.3030
2022-10-11 19:11:38 - train: epoch 0065, iter [03300, 05004], lr: 0.027786, loss: 3.8090
2022-10-11 19:12:16 - train: epoch 0065, iter [03400, 05004], lr: 0.027757, loss: 3.8512
2022-10-11 19:12:52 - train: epoch 0065, iter [03500, 05004], lr: 0.027729, loss: 4.1002
2022-10-11 19:13:29 - train: epoch 0065, iter [03600, 05004], lr: 0.027701, loss: 3.9591
2022-10-11 19:14:06 - train: epoch 0065, iter [03700, 05004], lr: 0.027673, loss: 4.3842
2022-10-11 19:14:43 - train: epoch 0065, iter [03800, 05004], lr: 0.027645, loss: 4.0526
2022-10-11 19:15:19 - train: epoch 0065, iter [03900, 05004], lr: 0.027617, loss: 4.6101
2022-10-11 19:15:56 - train: epoch 0065, iter [04000, 05004], lr: 0.027589, loss: 3.5626
2022-10-11 19:16:33 - train: epoch 0065, iter [04100, 05004], lr: 0.027561, loss: 4.2269
2022-10-11 19:17:09 - train: epoch 0065, iter [04200, 05004], lr: 0.027533, loss: 4.4113
2022-10-11 19:17:46 - train: epoch 0065, iter [04300, 05004], lr: 0.027505, loss: 4.3988
2022-10-11 19:18:22 - train: epoch 0065, iter [04400, 05004], lr: 0.027477, loss: 4.1650
2022-10-11 19:18:59 - train: epoch 0065, iter [04500, 05004], lr: 0.027449, loss: 4.4545
2022-10-11 19:19:36 - train: epoch 0065, iter [04600, 05004], lr: 0.027421, loss: 3.5218
2022-10-11 19:20:13 - train: epoch 0065, iter [04700, 05004], lr: 0.027393, loss: 3.8406
2022-10-11 19:20:49 - train: epoch 0065, iter [04800, 05004], lr: 0.027365, loss: 3.6230
2022-10-11 19:21:27 - train: epoch 0065, iter [04900, 05004], lr: 0.027337, loss: 3.5159
2022-10-11 19:22:00 - train: epoch 0065, iter [05000, 05004], lr: 0.027309, loss: 3.5848
2022-10-11 19:22:02 - train: epoch 065, train_loss: 4.0754
2022-10-11 19:23:21 - eval: epoch: 065, acc1: 65.096%, acc5: 87.168%, test_loss: 1.6782, per_image_load_time: 0.241ms, per_image_inference_time: 0.524ms
2022-10-11 19:23:21 - until epoch: 065, best_acc1: 65.336%
2022-10-11 19:23:21 - epoch 066 lr: 0.027308
2022-10-11 19:24:04 - train: epoch 0066, iter [00100, 05004], lr: 0.027280, loss: 4.0969
2022-10-11 19:24:41 - train: epoch 0066, iter [00200, 05004], lr: 0.027252, loss: 4.4655
2022-10-11 19:25:18 - train: epoch 0066, iter [00300, 05004], lr: 0.027224, loss: 4.5429
2022-10-11 19:25:54 - train: epoch 0066, iter [00400, 05004], lr: 0.027196, loss: 3.9837
2022-10-11 19:26:32 - train: epoch 0066, iter [00500, 05004], lr: 0.027168, loss: 4.3371
2022-10-11 19:27:08 - train: epoch 0066, iter [00600, 05004], lr: 0.027140, loss: 4.1226
2022-10-11 19:27:44 - train: epoch 0066, iter [00700, 05004], lr: 0.027112, loss: 3.4398
2022-10-11 19:28:21 - train: epoch 0066, iter [00800, 05004], lr: 0.027084, loss: 3.7615
2022-10-11 19:28:58 - train: epoch 0066, iter [00900, 05004], lr: 0.027056, loss: 3.9624
2022-10-11 19:29:36 - train: epoch 0066, iter [01000, 05004], lr: 0.027029, loss: 4.1016
2022-10-11 19:30:12 - train: epoch 0066, iter [01100, 05004], lr: 0.027001, loss: 3.7873
2022-10-11 19:30:49 - train: epoch 0066, iter [01200, 05004], lr: 0.026973, loss: 3.9568
2022-10-11 19:31:25 - train: epoch 0066, iter [01300, 05004], lr: 0.026945, loss: 3.2017
2022-10-11 19:32:01 - train: epoch 0066, iter [01400, 05004], lr: 0.026917, loss: 3.9282
2022-10-11 19:32:37 - train: epoch 0066, iter [01500, 05004], lr: 0.026889, loss: 4.6480
2022-10-11 19:33:14 - train: epoch 0066, iter [01600, 05004], lr: 0.026861, loss: 3.0448
2022-10-11 19:33:52 - train: epoch 0066, iter [01700, 05004], lr: 0.026834, loss: 4.0220
2022-10-11 19:34:27 - train: epoch 0066, iter [01800, 05004], lr: 0.026806, loss: 4.3789
2022-10-11 19:35:04 - train: epoch 0066, iter [01900, 05004], lr: 0.026778, loss: 3.8703
2022-10-11 19:35:41 - train: epoch 0066, iter [02000, 05004], lr: 0.026750, loss: 4.2548
2022-10-11 19:36:17 - train: epoch 0066, iter [02100, 05004], lr: 0.026722, loss: 4.1778
2022-10-11 19:36:54 - train: epoch 0066, iter [02200, 05004], lr: 0.026695, loss: 4.3389
2022-10-11 19:37:31 - train: epoch 0066, iter [02300, 05004], lr: 0.026667, loss: 3.7184
2022-10-11 19:38:08 - train: epoch 0066, iter [02400, 05004], lr: 0.026639, loss: 3.8319
2022-10-11 19:38:44 - train: epoch 0066, iter [02500, 05004], lr: 0.026611, loss: 4.8215
2022-10-11 19:39:21 - train: epoch 0066, iter [02600, 05004], lr: 0.026584, loss: 4.1099
2022-10-11 19:39:57 - train: epoch 0066, iter [02700, 05004], lr: 0.026556, loss: 4.0972
2022-10-11 19:40:35 - train: epoch 0066, iter [02800, 05004], lr: 0.026528, loss: 4.3310
2022-10-11 19:41:10 - train: epoch 0066, iter [02900, 05004], lr: 0.026501, loss: 4.5609
2022-10-11 19:41:48 - train: epoch 0066, iter [03000, 05004], lr: 0.026473, loss: 3.9494
2022-10-11 19:42:25 - train: epoch 0066, iter [03100, 05004], lr: 0.026445, loss: 4.1084
2022-10-11 19:43:01 - train: epoch 0066, iter [03200, 05004], lr: 0.026417, loss: 3.8072
2022-10-11 19:43:38 - train: epoch 0066, iter [03300, 05004], lr: 0.026390, loss: 3.8546
2022-10-11 19:44:14 - train: epoch 0066, iter [03400, 05004], lr: 0.026362, loss: 4.3686
2022-10-11 19:44:51 - train: epoch 0066, iter [03500, 05004], lr: 0.026334, loss: 3.9236
2022-10-11 19:45:28 - train: epoch 0066, iter [03600, 05004], lr: 0.026307, loss: 4.3311
2022-10-11 19:46:05 - train: epoch 0066, iter [03700, 05004], lr: 0.026279, loss: 3.5638
2022-10-11 19:46:42 - train: epoch 0066, iter [03800, 05004], lr: 0.026252, loss: 3.8895
2022-10-11 19:47:19 - train: epoch 0066, iter [03900, 05004], lr: 0.026224, loss: 3.4476
2022-10-11 19:47:56 - train: epoch 0066, iter [04000, 05004], lr: 0.026196, loss: 3.6191
2022-10-11 19:48:32 - train: epoch 0066, iter [04100, 05004], lr: 0.026169, loss: 4.3414
2022-10-11 19:49:09 - train: epoch 0066, iter [04200, 05004], lr: 0.026141, loss: 4.8986
2022-10-11 19:49:46 - train: epoch 0066, iter [04300, 05004], lr: 0.026114, loss: 4.3854
2022-10-11 19:50:23 - train: epoch 0066, iter [04400, 05004], lr: 0.026086, loss: 4.3470
2022-10-11 19:50:59 - train: epoch 0066, iter [04500, 05004], lr: 0.026058, loss: 4.5606
2022-10-11 19:51:36 - train: epoch 0066, iter [04600, 05004], lr: 0.026031, loss: 4.1834
2022-10-11 19:52:12 - train: epoch 0066, iter [04700, 05004], lr: 0.026003, loss: 4.5296
2022-10-11 19:52:50 - train: epoch 0066, iter [04800, 05004], lr: 0.025976, loss: 4.0810
2022-10-11 19:53:26 - train: epoch 0066, iter [04900, 05004], lr: 0.025948, loss: 3.4384
2022-10-11 19:54:01 - train: epoch 0066, iter [05000, 05004], lr: 0.025921, loss: 4.3328
2022-10-11 19:54:03 - train: epoch 066, train_loss: 4.0714
2022-10-11 19:55:21 - eval: epoch: 066, acc1: 65.256%, acc5: 86.922%, test_loss: 1.7230, per_image_load_time: 0.254ms, per_image_inference_time: 0.499ms
2022-10-11 19:55:22 - until epoch: 066, best_acc1: 65.336%
2022-10-11 19:55:22 - epoch 067 lr: 0.025920
2022-10-11 19:56:05 - train: epoch 0067, iter [00100, 05004], lr: 0.025892, loss: 3.2968
2022-10-11 19:56:40 - train: epoch 0067, iter [00200, 05004], lr: 0.025865, loss: 4.5027
2022-10-11 19:57:17 - train: epoch 0067, iter [00300, 05004], lr: 0.025837, loss: 3.7195
2022-10-11 19:57:54 - train: epoch 0067, iter [00400, 05004], lr: 0.025810, loss: 4.8238
2022-10-11 19:58:31 - train: epoch 0067, iter [00500, 05004], lr: 0.025782, loss: 3.5131
2022-10-11 19:59:07 - train: epoch 0067, iter [00600, 05004], lr: 0.025755, loss: 3.7592
2022-10-11 19:59:44 - train: epoch 0067, iter [00700, 05004], lr: 0.025727, loss: 4.0780
2022-10-11 20:00:20 - train: epoch 0067, iter [00800, 05004], lr: 0.025700, loss: 4.4434
2022-10-11 20:00:57 - train: epoch 0067, iter [00900, 05004], lr: 0.025673, loss: 4.3938
2022-10-11 20:01:34 - train: epoch 0067, iter [01000, 05004], lr: 0.025645, loss: 3.7778
2022-10-11 20:02:11 - train: epoch 0067, iter [01100, 05004], lr: 0.025618, loss: 4.0514
2022-10-11 20:02:47 - train: epoch 0067, iter [01200, 05004], lr: 0.025590, loss: 3.8503
2022-10-11 20:03:24 - train: epoch 0067, iter [01300, 05004], lr: 0.025563, loss: 4.2377
2022-10-11 20:04:00 - train: epoch 0067, iter [01400, 05004], lr: 0.025536, loss: 4.1092
2022-10-11 20:04:37 - train: epoch 0067, iter [01500, 05004], lr: 0.025508, loss: 3.7201
2022-10-11 20:05:14 - train: epoch 0067, iter [01600, 05004], lr: 0.025481, loss: 3.5381
2022-10-11 20:05:50 - train: epoch 0067, iter [01700, 05004], lr: 0.025454, loss: 4.3960
2022-10-11 20:06:27 - train: epoch 0067, iter [01800, 05004], lr: 0.025426, loss: 4.4668
2022-10-11 20:07:04 - train: epoch 0067, iter [01900, 05004], lr: 0.025399, loss: 4.5236
2022-10-11 20:07:40 - train: epoch 0067, iter [02000, 05004], lr: 0.025372, loss: 4.5522
2022-10-11 20:08:16 - train: epoch 0067, iter [02100, 05004], lr: 0.025344, loss: 3.3883
2022-10-11 20:08:52 - train: epoch 0067, iter [02200, 05004], lr: 0.025317, loss: 3.9843
2022-10-11 20:09:29 - train: epoch 0067, iter [02300, 05004], lr: 0.025290, loss: 3.0207
2022-10-11 20:10:05 - train: epoch 0067, iter [02400, 05004], lr: 0.025262, loss: 4.2849
2022-10-11 20:10:42 - train: epoch 0067, iter [02500, 05004], lr: 0.025235, loss: 4.4346
2022-10-11 20:11:18 - train: epoch 0067, iter [02600, 05004], lr: 0.025208, loss: 4.4008
2022-10-11 20:11:54 - train: epoch 0067, iter [02700, 05004], lr: 0.025181, loss: 3.9335
2022-10-11 20:12:31 - train: epoch 0067, iter [02800, 05004], lr: 0.025153, loss: 3.8619
2022-10-11 20:13:08 - train: epoch 0067, iter [02900, 05004], lr: 0.025126, loss: 3.6523
2022-10-11 20:13:45 - train: epoch 0067, iter [03000, 05004], lr: 0.025099, loss: 3.7751
2022-10-11 20:14:21 - train: epoch 0067, iter [03100, 05004], lr: 0.025072, loss: 4.4007
2022-10-11 20:14:57 - train: epoch 0067, iter [03200, 05004], lr: 0.025044, loss: 4.5116
2022-10-11 20:15:34 - train: epoch 0067, iter [03300, 05004], lr: 0.025017, loss: 3.6446
2022-10-11 20:16:10 - train: epoch 0067, iter [03400, 05004], lr: 0.024990, loss: 2.8756
2022-10-11 20:16:48 - train: epoch 0067, iter [03500, 05004], lr: 0.024963, loss: 3.7653
2022-10-11 20:17:24 - train: epoch 0067, iter [03600, 05004], lr: 0.024936, loss: 4.5470
2022-10-11 20:18:00 - train: epoch 0067, iter [03700, 05004], lr: 0.024909, loss: 4.3765
2022-10-11 20:18:37 - train: epoch 0067, iter [03800, 05004], lr: 0.024881, loss: 3.5561
2022-10-11 20:19:14 - train: epoch 0067, iter [03900, 05004], lr: 0.024854, loss: 4.3377
2022-10-11 20:19:51 - train: epoch 0067, iter [04000, 05004], lr: 0.024827, loss: 3.6296
2022-10-11 20:20:29 - train: epoch 0067, iter [04100, 05004], lr: 0.024800, loss: 4.5021
2022-10-11 20:21:05 - train: epoch 0067, iter [04200, 05004], lr: 0.024773, loss: 4.1297
2022-10-11 20:21:42 - train: epoch 0067, iter [04300, 05004], lr: 0.024746, loss: 4.1110
2022-10-11 20:22:19 - train: epoch 0067, iter [04400, 05004], lr: 0.024719, loss: 4.5338
2022-10-11 20:22:56 - train: epoch 0067, iter [04500, 05004], lr: 0.024692, loss: 3.7469
2022-10-11 20:23:33 - train: epoch 0067, iter [04600, 05004], lr: 0.024665, loss: 3.6098
2022-10-11 20:24:09 - train: epoch 0067, iter [04700, 05004], lr: 0.024638, loss: 4.8457
2022-10-11 20:24:45 - train: epoch 0067, iter [04800, 05004], lr: 0.024611, loss: 4.3807
2022-10-11 20:25:23 - train: epoch 0067, iter [04900, 05004], lr: 0.024584, loss: 3.9783
2022-10-11 20:25:57 - train: epoch 0067, iter [05000, 05004], lr: 0.024557, loss: 4.1746
2022-10-11 20:26:00 - train: epoch 067, train_loss: 4.0309
2022-10-11 20:27:18 - eval: epoch: 067, acc1: 65.510%, acc5: 87.526%, test_loss: 1.7426, per_image_load_time: 0.260ms, per_image_inference_time: 0.529ms
2022-10-11 20:27:19 - until epoch: 067, best_acc1: 65.510%
2022-10-11 20:27:19 - epoch 068 lr: 0.024555
2022-10-11 20:28:01 - train: epoch 0068, iter [00100, 05004], lr: 0.024528, loss: 3.6222
2022-10-11 20:28:37 - train: epoch 0068, iter [00200, 05004], lr: 0.024501, loss: 4.3344
2022-10-11 20:29:13 - train: epoch 0068, iter [00300, 05004], lr: 0.024474, loss: 3.8193
2022-10-11 20:29:50 - train: epoch 0068, iter [00400, 05004], lr: 0.024447, loss: 3.9417
2022-10-11 20:30:26 - train: epoch 0068, iter [00500, 05004], lr: 0.024421, loss: 4.1341
2022-10-11 20:31:02 - train: epoch 0068, iter [00600, 05004], lr: 0.024394, loss: 4.7188
2022-10-11 20:31:40 - train: epoch 0068, iter [00700, 05004], lr: 0.024367, loss: 4.7042
2022-10-11 20:32:16 - train: epoch 0068, iter [00800, 05004], lr: 0.024340, loss: 3.8940
2022-10-11 20:32:53 - train: epoch 0068, iter [00900, 05004], lr: 0.024313, loss: 4.4595
2022-10-11 20:33:29 - train: epoch 0068, iter [01000, 05004], lr: 0.024286, loss: 3.8987
2022-10-11 20:34:05 - train: epoch 0068, iter [01100, 05004], lr: 0.024259, loss: 4.8150
2022-10-11 20:34:41 - train: epoch 0068, iter [01200, 05004], lr: 0.024232, loss: 3.0782
2022-10-11 20:35:19 - train: epoch 0068, iter [01300, 05004], lr: 0.024205, loss: 4.3453
2022-10-11 20:35:55 - train: epoch 0068, iter [01400, 05004], lr: 0.024178, loss: 4.4673
2022-10-11 20:36:32 - train: epoch 0068, iter [01500, 05004], lr: 0.024151, loss: 3.8367
2022-10-11 20:37:08 - train: epoch 0068, iter [01600, 05004], lr: 0.024124, loss: 4.5593
2022-10-11 20:37:46 - train: epoch 0068, iter [01700, 05004], lr: 0.024098, loss: 3.9063
2022-10-11 20:38:23 - train: epoch 0068, iter [01800, 05004], lr: 0.024071, loss: 4.2181
2022-10-11 20:38:59 - train: epoch 0068, iter [01900, 05004], lr: 0.024044, loss: 3.4104
2022-10-11 20:39:36 - train: epoch 0068, iter [02000, 05004], lr: 0.024017, loss: 3.8546
2022-10-11 20:40:13 - train: epoch 0068, iter [02100, 05004], lr: 0.023990, loss: 4.2921
2022-10-11 20:40:49 - train: epoch 0068, iter [02200, 05004], lr: 0.023964, loss: 4.2119
2022-10-11 20:41:26 - train: epoch 0068, iter [02300, 05004], lr: 0.023937, loss: 3.2455
2022-10-11 20:42:02 - train: epoch 0068, iter [02400, 05004], lr: 0.023910, loss: 3.9057
2022-10-11 20:42:38 - train: epoch 0068, iter [02500, 05004], lr: 0.023883, loss: 3.5311
2022-10-11 20:43:15 - train: epoch 0068, iter [02600, 05004], lr: 0.023856, loss: 3.4714
2022-10-11 20:43:51 - train: epoch 0068, iter [02700, 05004], lr: 0.023830, loss: 3.7213
2022-10-11 20:44:29 - train: epoch 0068, iter [02800, 05004], lr: 0.023803, loss: 3.6412
2022-10-11 20:45:05 - train: epoch 0068, iter [02900, 05004], lr: 0.023776, loss: 4.6417
2022-10-11 20:45:41 - train: epoch 0068, iter [03000, 05004], lr: 0.023750, loss: 3.8526
2022-10-11 20:46:19 - train: epoch 0068, iter [03100, 05004], lr: 0.023723, loss: 4.0024
2022-10-11 20:46:55 - train: epoch 0068, iter [03200, 05004], lr: 0.023696, loss: 4.2054
2022-10-11 20:47:32 - train: epoch 0068, iter [03300, 05004], lr: 0.023669, loss: 3.5769
2022-10-11 20:48:10 - train: epoch 0068, iter [03400, 05004], lr: 0.023643, loss: 4.6882
2022-10-11 20:48:47 - train: epoch 0068, iter [03500, 05004], lr: 0.023616, loss: 4.3753
2022-10-11 20:49:23 - train: epoch 0068, iter [03600, 05004], lr: 0.023589, loss: 3.6045
2022-10-11 20:49:59 - train: epoch 0068, iter [03700, 05004], lr: 0.023563, loss: 2.9206
2022-10-11 20:50:37 - train: epoch 0068, iter [03800, 05004], lr: 0.023536, loss: 4.0359
2022-10-11 20:51:13 - train: epoch 0068, iter [03900, 05004], lr: 0.023510, loss: 4.2229
2022-10-11 20:51:50 - train: epoch 0068, iter [04000, 05004], lr: 0.023483, loss: 4.2349
2022-10-11 20:52:27 - train: epoch 0068, iter [04100, 05004], lr: 0.023456, loss: 4.0349
2022-10-11 20:53:03 - train: epoch 0068, iter [04200, 05004], lr: 0.023430, loss: 4.0824
2022-10-11 20:53:40 - train: epoch 0068, iter [04300, 05004], lr: 0.023403, loss: 4.4013
2022-10-11 20:54:17 - train: epoch 0068, iter [04400, 05004], lr: 0.023377, loss: 3.5497
2022-10-11 20:54:54 - train: epoch 0068, iter [04500, 05004], lr: 0.023350, loss: 3.6878
2022-10-11 20:55:31 - train: epoch 0068, iter [04600, 05004], lr: 0.023323, loss: 3.9267
2022-10-11 20:56:08 - train: epoch 0068, iter [04700, 05004], lr: 0.023297, loss: 4.3593
2022-10-11 20:56:44 - train: epoch 0068, iter [04800, 05004], lr: 0.023270, loss: 4.2521
2022-10-11 20:57:21 - train: epoch 0068, iter [04900, 05004], lr: 0.023244, loss: 4.6507
2022-10-11 20:57:56 - train: epoch 0068, iter [05000, 05004], lr: 0.023217, loss: 4.3185
2022-10-11 20:57:59 - train: epoch 068, train_loss: 4.0124
2022-10-11 20:59:17 - eval: epoch: 068, acc1: 66.094%, acc5: 87.892%, test_loss: 1.6056, per_image_load_time: 0.259ms, per_image_inference_time: 0.525ms
2022-10-11 20:59:18 - until epoch: 068, best_acc1: 66.094%
2022-10-11 20:59:18 - epoch 069 lr: 0.023216
2022-10-11 21:00:01 - train: epoch 0069, iter [00100, 05004], lr: 0.023190, loss: 3.2383
2022-10-11 21:00:38 - train: epoch 0069, iter [00200, 05004], lr: 0.023163, loss: 4.4806
2022-10-11 21:01:14 - train: epoch 0069, iter [00300, 05004], lr: 0.023137, loss: 4.4043
2022-10-11 21:01:51 - train: epoch 0069, iter [00400, 05004], lr: 0.023110, loss: 4.2543
2022-10-11 21:02:27 - train: epoch 0069, iter [00500, 05004], lr: 0.023084, loss: 3.8719
2022-10-11 21:03:03 - train: epoch 0069, iter [00600, 05004], lr: 0.023058, loss: 3.9211
2022-10-11 21:03:40 - train: epoch 0069, iter [00700, 05004], lr: 0.023031, loss: 3.4206
2022-10-11 21:04:16 - train: epoch 0069, iter [00800, 05004], lr: 0.023005, loss: 4.2151
2022-10-11 21:04:53 - train: epoch 0069, iter [00900, 05004], lr: 0.022978, loss: 4.0069
2022-10-11 21:05:30 - train: epoch 0069, iter [01000, 05004], lr: 0.022952, loss: 3.5766
2022-10-11 21:06:06 - train: epoch 0069, iter [01100, 05004], lr: 0.022925, loss: 3.5836
2022-10-11 21:06:43 - train: epoch 0069, iter [01200, 05004], lr: 0.022899, loss: 3.6658
2022-10-11 21:07:20 - train: epoch 0069, iter [01300, 05004], lr: 0.022873, loss: 4.4874
2022-10-11 21:07:57 - train: epoch 0069, iter [01400, 05004], lr: 0.022846, loss: 3.8417
2022-10-11 21:08:33 - train: epoch 0069, iter [01500, 05004], lr: 0.022820, loss: 3.6481
2022-10-11 21:09:10 - train: epoch 0069, iter [01600, 05004], lr: 0.022794, loss: 3.6371
2022-10-11 21:09:46 - train: epoch 0069, iter [01700, 05004], lr: 0.022767, loss: 4.4097
2022-10-11 21:10:23 - train: epoch 0069, iter [01800, 05004], lr: 0.022741, loss: 4.1683
2022-10-11 21:11:00 - train: epoch 0069, iter [01900, 05004], lr: 0.022715, loss: 4.5180
2022-10-11 21:11:37 - train: epoch 0069, iter [02000, 05004], lr: 0.022688, loss: 3.9228
2022-10-11 21:12:13 - train: epoch 0069, iter [02100, 05004], lr: 0.022662, loss: 3.3150
2022-10-11 21:12:50 - train: epoch 0069, iter [02200, 05004], lr: 0.022636, loss: 3.8379
2022-10-11 21:13:26 - train: epoch 0069, iter [02300, 05004], lr: 0.022610, loss: 3.4808
2022-10-11 21:14:02 - train: epoch 0069, iter [02400, 05004], lr: 0.022583, loss: 3.8553
2022-10-11 21:14:39 - train: epoch 0069, iter [02500, 05004], lr: 0.022557, loss: 4.4459
2022-10-11 21:15:16 - train: epoch 0069, iter [02600, 05004], lr: 0.022531, loss: 4.3506
2022-10-11 21:15:53 - train: epoch 0069, iter [02700, 05004], lr: 0.022505, loss: 3.6170
2022-10-11 21:16:30 - train: epoch 0069, iter [02800, 05004], lr: 0.022478, loss: 4.1409
2022-10-11 21:17:07 - train: epoch 0069, iter [02900, 05004], lr: 0.022452, loss: 4.3425
2022-10-11 21:17:44 - train: epoch 0069, iter [03000, 05004], lr: 0.022426, loss: 3.4317
2022-10-11 21:18:21 - train: epoch 0069, iter [03100, 05004], lr: 0.022400, loss: 4.3533
2022-10-11 21:18:58 - train: epoch 0069, iter [03200, 05004], lr: 0.022374, loss: 4.4561
2022-10-11 21:19:33 - train: epoch 0069, iter [03300, 05004], lr: 0.022348, loss: 3.5308
2022-10-11 21:20:10 - train: epoch 0069, iter [03400, 05004], lr: 0.022321, loss: 4.6489
2022-10-11 21:20:46 - train: epoch 0069, iter [03500, 05004], lr: 0.022295, loss: 4.0773
2022-10-11 21:21:22 - train: epoch 0069, iter [03600, 05004], lr: 0.022269, loss: 3.9155
2022-10-11 21:21:59 - train: epoch 0069, iter [03700, 05004], lr: 0.022243, loss: 4.0109
2022-10-11 21:22:36 - train: epoch 0069, iter [03800, 05004], lr: 0.022217, loss: 4.2248
2022-10-11 21:23:13 - train: epoch 0069, iter [03900, 05004], lr: 0.022191, loss: 3.7446
2022-10-11 21:23:50 - train: epoch 0069, iter [04000, 05004], lr: 0.022165, loss: 3.8728
2022-10-11 21:24:25 - train: epoch 0069, iter [04100, 05004], lr: 0.022139, loss: 3.6222
2022-10-11 21:25:02 - train: epoch 0069, iter [04200, 05004], lr: 0.022113, loss: 4.5193
2022-10-11 21:25:39 - train: epoch 0069, iter [04300, 05004], lr: 0.022087, loss: 3.5253
2022-10-11 21:26:16 - train: epoch 0069, iter [04400, 05004], lr: 0.022061, loss: 3.4820
2022-10-11 21:26:52 - train: epoch 0069, iter [04500, 05004], lr: 0.022035, loss: 4.8586
2022-10-11 21:27:29 - train: epoch 0069, iter [04600, 05004], lr: 0.022009, loss: 4.3354
2022-10-11 21:28:06 - train: epoch 0069, iter [04700, 05004], lr: 0.021983, loss: 2.8353
2022-10-11 21:28:42 - train: epoch 0069, iter [04800, 05004], lr: 0.021957, loss: 4.5761
2022-10-11 21:29:20 - train: epoch 0069, iter [04900, 05004], lr: 0.021931, loss: 4.2414
2022-10-11 21:29:54 - train: epoch 0069, iter [05000, 05004], lr: 0.021905, loss: 4.2212
2022-10-11 21:29:56 - train: epoch 069, train_loss: 4.0055
2022-10-11 21:31:14 - eval: epoch: 069, acc1: 66.572%, acc5: 87.956%, test_loss: 1.7000, per_image_load_time: 0.270ms, per_image_inference_time: 0.557ms
2022-10-11 21:31:15 - until epoch: 069, best_acc1: 66.572%
2022-10-11 21:31:15 - epoch 070 lr: 0.021904
2022-10-11 21:31:57 - train: epoch 0070, iter [00100, 05004], lr: 0.021878, loss: 3.8709
2022-10-11 21:32:33 - train: epoch 0070, iter [00200, 05004], lr: 0.021852, loss: 3.6322
2022-10-11 21:33:10 - train: epoch 0070, iter [00300, 05004], lr: 0.021826, loss: 4.1449
2022-10-11 21:33:46 - train: epoch 0070, iter [00400, 05004], lr: 0.021800, loss: 4.3028
2022-10-11 21:34:23 - train: epoch 0070, iter [00500, 05004], lr: 0.021774, loss: 4.4866
2022-10-11 21:34:59 - train: epoch 0070, iter [00600, 05004], lr: 0.021748, loss: 4.1801
2022-10-11 21:35:36 - train: epoch 0070, iter [00700, 05004], lr: 0.021722, loss: 4.0804
2022-10-11 21:36:12 - train: epoch 0070, iter [00800, 05004], lr: 0.021696, loss: 4.4065
2022-10-11 21:36:48 - train: epoch 0070, iter [00900, 05004], lr: 0.021670, loss: 3.3543
2022-10-11 21:37:25 - train: epoch 0070, iter [01000, 05004], lr: 0.021645, loss: 3.8936
2022-10-11 21:38:02 - train: epoch 0070, iter [01100, 05004], lr: 0.021619, loss: 3.7616
2022-10-11 21:38:38 - train: epoch 0070, iter [01200, 05004], lr: 0.021593, loss: 4.1498
2022-10-11 21:39:15 - train: epoch 0070, iter [01300, 05004], lr: 0.021567, loss: 4.3938
2022-10-11 21:39:52 - train: epoch 0070, iter [01400, 05004], lr: 0.021541, loss: 4.0187
2022-10-11 21:40:29 - train: epoch 0070, iter [01500, 05004], lr: 0.021515, loss: 3.6273
2022-10-11 21:41:07 - train: epoch 0070, iter [01600, 05004], lr: 0.021490, loss: 4.0024
2022-10-11 21:41:42 - train: epoch 0070, iter [01700, 05004], lr: 0.021464, loss: 4.0118
2022-10-11 21:42:19 - train: epoch 0070, iter [01800, 05004], lr: 0.021438, loss: 3.7344
2022-10-11 21:42:56 - train: epoch 0070, iter [01900, 05004], lr: 0.021412, loss: 3.7206
2022-10-11 21:43:33 - train: epoch 0070, iter [02000, 05004], lr: 0.021387, loss: 3.9952
2022-10-11 21:44:10 - train: epoch 0070, iter [02100, 05004], lr: 0.021361, loss: 3.8312
2022-10-11 21:44:47 - train: epoch 0070, iter [02200, 05004], lr: 0.021335, loss: 4.9196
2022-10-11 21:45:24 - train: epoch 0070, iter [02300, 05004], lr: 0.021310, loss: 4.1076
2022-10-11 21:46:01 - train: epoch 0070, iter [02400, 05004], lr: 0.021284, loss: 4.5402
2022-10-11 21:46:38 - train: epoch 0070, iter [02500, 05004], lr: 0.021258, loss: 4.0799
2022-10-11 21:47:15 - train: epoch 0070, iter [02600, 05004], lr: 0.021232, loss: 4.4040
2022-10-11 21:47:51 - train: epoch 0070, iter [02700, 05004], lr: 0.021207, loss: 4.4083
2022-10-11 21:48:29 - train: epoch 0070, iter [02800, 05004], lr: 0.021181, loss: 4.2862
2022-10-11 21:49:06 - train: epoch 0070, iter [02900, 05004], lr: 0.021155, loss: 4.2422
2022-10-11 21:49:43 - train: epoch 0070, iter [03000, 05004], lr: 0.021130, loss: 3.2153
2022-10-11 21:50:19 - train: epoch 0070, iter [03100, 05004], lr: 0.021104, loss: 4.1029
2022-10-11 21:50:57 - train: epoch 0070, iter [03200, 05004], lr: 0.021079, loss: 3.8891
2022-10-11 21:51:34 - train: epoch 0070, iter [03300, 05004], lr: 0.021053, loss: 4.0539
2022-10-11 21:52:10 - train: epoch 0070, iter [03400, 05004], lr: 0.021027, loss: 3.5404
2022-10-11 21:52:47 - train: epoch 0070, iter [03500, 05004], lr: 0.021002, loss: 4.0491
2022-10-11 21:53:24 - train: epoch 0070, iter [03600, 05004], lr: 0.020976, loss: 3.9073
2022-10-11 21:54:01 - train: epoch 0070, iter [03700, 05004], lr: 0.020951, loss: 4.5931
2022-10-11 21:54:39 - train: epoch 0070, iter [03800, 05004], lr: 0.020925, loss: 4.2946
2022-10-11 21:55:15 - train: epoch 0070, iter [03900, 05004], lr: 0.020900, loss: 3.0888
2022-10-11 21:55:51 - train: epoch 0070, iter [04000, 05004], lr: 0.020874, loss: 4.5728
2022-10-11 21:56:28 - train: epoch 0070, iter [04100, 05004], lr: 0.020849, loss: 4.1859
2022-10-11 21:57:04 - train: epoch 0070, iter [04200, 05004], lr: 0.020823, loss: 3.7456
2022-10-11 21:57:41 - train: epoch 0070, iter [04300, 05004], lr: 0.020798, loss: 3.1929
2022-10-11 21:58:18 - train: epoch 0070, iter [04400, 05004], lr: 0.020772, loss: 4.6057
2022-10-11 21:58:55 - train: epoch 0070, iter [04500, 05004], lr: 0.020747, loss: 4.2332
2022-10-11 21:59:31 - train: epoch 0070, iter [04600, 05004], lr: 0.020721, loss: 4.0784
2022-10-11 22:00:07 - train: epoch 0070, iter [04700, 05004], lr: 0.020696, loss: 4.3449
2022-10-11 22:00:45 - train: epoch 0070, iter [04800, 05004], lr: 0.020671, loss: 4.3289
2022-10-11 22:01:22 - train: epoch 0070, iter [04900, 05004], lr: 0.020645, loss: 4.4885
2022-10-11 22:01:56 - train: epoch 0070, iter [05000, 05004], lr: 0.020620, loss: 3.4295
2022-10-11 22:01:58 - train: epoch 070, train_loss: 3.9926
2022-10-11 22:03:17 - eval: epoch: 070, acc1: 66.562%, acc5: 87.920%, test_loss: 1.6929, per_image_load_time: 0.253ms, per_image_inference_time: 0.505ms
2022-10-11 22:03:17 - until epoch: 070, best_acc1: 66.572%
2022-10-11 22:03:17 - epoch 071 lr: 0.020619
2022-10-11 22:03:59 - train: epoch 0071, iter [00100, 05004], lr: 0.020593, loss: 3.9960
2022-10-11 22:04:36 - train: epoch 0071, iter [00200, 05004], lr: 0.020568, loss: 3.1250
2022-10-11 22:05:13 - train: epoch 0071, iter [00300, 05004], lr: 0.020543, loss: 3.7927
2022-10-11 22:05:49 - train: epoch 0071, iter [00400, 05004], lr: 0.020517, loss: 2.8328
2022-10-11 22:06:26 - train: epoch 0071, iter [00500, 05004], lr: 0.020492, loss: 4.1367
2022-10-11 22:07:02 - train: epoch 0071, iter [00600, 05004], lr: 0.020467, loss: 3.6328
2022-10-11 22:07:38 - train: epoch 0071, iter [00700, 05004], lr: 0.020441, loss: 3.7764
2022-10-11 22:08:14 - train: epoch 0071, iter [00800, 05004], lr: 0.020416, loss: 4.2702
2022-10-11 22:08:51 - train: epoch 0071, iter [00900, 05004], lr: 0.020391, loss: 3.4928
2022-10-11 22:09:27 - train: epoch 0071, iter [01000, 05004], lr: 0.020365, loss: 3.5688
2022-10-11 22:10:04 - train: epoch 0071, iter [01100, 05004], lr: 0.020340, loss: 3.7026
2022-10-11 22:10:40 - train: epoch 0071, iter [01200, 05004], lr: 0.020315, loss: 4.3374
2022-10-11 22:11:17 - train: epoch 0071, iter [01300, 05004], lr: 0.020290, loss: 4.3779
2022-10-11 22:11:54 - train: epoch 0071, iter [01400, 05004], lr: 0.020264, loss: 3.6767
2022-10-11 22:12:31 - train: epoch 0071, iter [01500, 05004], lr: 0.020239, loss: 3.6619
2022-10-11 22:13:07 - train: epoch 0071, iter [01600, 05004], lr: 0.020214, loss: 3.7087
2022-10-11 22:13:45 - train: epoch 0071, iter [01700, 05004], lr: 0.020189, loss: 3.9578
2022-10-11 22:14:21 - train: epoch 0071, iter [01800, 05004], lr: 0.020163, loss: 3.7909
2022-10-11 22:14:58 - train: epoch 0071, iter [01900, 05004], lr: 0.020138, loss: 3.2234
2022-10-11 22:15:34 - train: epoch 0071, iter [02000, 05004], lr: 0.020113, loss: 3.2558
2022-10-11 22:16:11 - train: epoch 0071, iter [02100, 05004], lr: 0.020088, loss: 4.3445
2022-10-11 22:16:50 - train: epoch 0071, iter [02200, 05004], lr: 0.020063, loss: 4.1855
2022-10-11 22:17:25 - train: epoch 0071, iter [02300, 05004], lr: 0.020038, loss: 3.5622
2022-10-11 22:18:02 - train: epoch 0071, iter [02400, 05004], lr: 0.020013, loss: 3.8909
2022-10-11 22:18:39 - train: epoch 0071, iter [02500, 05004], lr: 0.019987, loss: 4.6050
2022-10-11 22:19:15 - train: epoch 0071, iter [02600, 05004], lr: 0.019962, loss: 4.1736
2022-10-11 22:19:51 - train: epoch 0071, iter [02700, 05004], lr: 0.019937, loss: 3.3160
2022-10-11 22:20:27 - train: epoch 0071, iter [02800, 05004], lr: 0.019912, loss: 4.5824
2022-10-11 22:21:04 - train: epoch 0071, iter [02900, 05004], lr: 0.019887, loss: 3.7077
2022-10-11 22:21:41 - train: epoch 0071, iter [03000, 05004], lr: 0.019862, loss: 4.3912
2022-10-11 22:22:18 - train: epoch 0071, iter [03100, 05004], lr: 0.019837, loss: 3.7320
2022-10-11 22:22:55 - train: epoch 0071, iter [03200, 05004], lr: 0.019812, loss: 3.4874
2022-10-11 22:23:32 - train: epoch 0071, iter [03300, 05004], lr: 0.019787, loss: 4.3003
2022-10-11 22:24:08 - train: epoch 0071, iter [03400, 05004], lr: 0.019762, loss: 3.0887
2022-10-11 22:24:45 - train: epoch 0071, iter [03500, 05004], lr: 0.019737, loss: 3.7579
2022-10-11 22:25:23 - train: epoch 0071, iter [03600, 05004], lr: 0.019712, loss: 3.9784
2022-10-11 22:26:00 - train: epoch 0071, iter [03700, 05004], lr: 0.019687, loss: 4.3601
2022-10-11 22:26:36 - train: epoch 0071, iter [03800, 05004], lr: 0.019662, loss: 4.5105
2022-10-11 22:27:13 - train: epoch 0071, iter [03900, 05004], lr: 0.019637, loss: 4.0464
2022-10-11 22:27:50 - train: epoch 0071, iter [04000, 05004], lr: 0.019612, loss: 3.6997
2022-10-11 22:28:26 - train: epoch 0071, iter [04100, 05004], lr: 0.019587, loss: 3.8590
2022-10-11 22:29:02 - train: epoch 0071, iter [04200, 05004], lr: 0.019563, loss: 4.0988
2022-10-11 22:29:39 - train: epoch 0071, iter [04300, 05004], lr: 0.019538, loss: 4.0490
2022-10-11 22:30:15 - train: epoch 0071, iter [04400, 05004], lr: 0.019513, loss: 4.1104
2022-10-11 22:30:52 - train: epoch 0071, iter [04500, 05004], lr: 0.019488, loss: 3.2265
2022-10-11 22:31:29 - train: epoch 0071, iter [04600, 05004], lr: 0.019463, loss: 3.5530
2022-10-11 22:32:05 - train: epoch 0071, iter [04700, 05004], lr: 0.019438, loss: 4.1698
2022-10-11 22:32:42 - train: epoch 0071, iter [04800, 05004], lr: 0.019413, loss: 3.4160
2022-10-11 22:33:18 - train: epoch 0071, iter [04900, 05004], lr: 0.019389, loss: 4.3004
2022-10-11 22:33:53 - train: epoch 0071, iter [05000, 05004], lr: 0.019364, loss: 4.3215
2022-10-11 22:33:55 - train: epoch 071, train_loss: 3.9679
2022-10-11 22:35:14 - eval: epoch: 071, acc1: 68.600%, acc5: 89.130%, test_loss: 1.5671, per_image_load_time: 0.248ms, per_image_inference_time: 0.527ms
2022-10-11 22:35:14 - until epoch: 071, best_acc1: 68.600%
2022-10-11 22:35:14 - epoch 072 lr: 0.019363
2022-10-11 22:35:56 - train: epoch 0072, iter [00100, 05004], lr: 0.019338, loss: 4.1237
2022-10-11 22:36:33 - train: epoch 0072, iter [00200, 05004], lr: 0.019313, loss: 4.0806
2022-10-11 22:37:10 - train: epoch 0072, iter [00300, 05004], lr: 0.019288, loss: 3.7938
2022-10-11 22:37:47 - train: epoch 0072, iter [00400, 05004], lr: 0.019264, loss: 4.3099
2022-10-11 22:38:24 - train: epoch 0072, iter [00500, 05004], lr: 0.019239, loss: 4.1387
2022-10-11 22:39:00 - train: epoch 0072, iter [00600, 05004], lr: 0.019214, loss: 3.4638
2022-10-11 22:39:36 - train: epoch 0072, iter [00700, 05004], lr: 0.019189, loss: 3.5987
2022-10-11 22:40:14 - train: epoch 0072, iter [00800, 05004], lr: 0.019165, loss: 4.0931
2022-10-11 22:40:50 - train: epoch 0072, iter [00900, 05004], lr: 0.019140, loss: 3.2790
2022-10-11 22:41:27 - train: epoch 0072, iter [01000, 05004], lr: 0.019115, loss: 3.9982
2022-10-11 22:42:05 - train: epoch 0072, iter [01100, 05004], lr: 0.019091, loss: 3.6391
2022-10-11 22:42:41 - train: epoch 0072, iter [01200, 05004], lr: 0.019066, loss: 3.9960
2022-10-11 22:43:17 - train: epoch 0072, iter [01300, 05004], lr: 0.019041, loss: 3.7933
2022-10-11 22:43:54 - train: epoch 0072, iter [01400, 05004], lr: 0.019017, loss: 4.0240
2022-10-11 22:44:30 - train: epoch 0072, iter [01500, 05004], lr: 0.018992, loss: 4.0509
2022-10-11 22:45:07 - train: epoch 0072, iter [01600, 05004], lr: 0.018967, loss: 4.1123
2022-10-11 22:45:42 - train: epoch 0072, iter [01700, 05004], lr: 0.018943, loss: 4.1881
2022-10-11 22:46:19 - train: epoch 0072, iter [01800, 05004], lr: 0.018918, loss: 4.7905
2022-10-11 22:46:56 - train: epoch 0072, iter [01900, 05004], lr: 0.018894, loss: 3.8032
2022-10-11 22:47:32 - train: epoch 0072, iter [02000, 05004], lr: 0.018869, loss: 4.5301
2022-10-11 22:48:09 - train: epoch 0072, iter [02100, 05004], lr: 0.018845, loss: 3.6931
2022-10-11 22:48:47 - train: epoch 0072, iter [02200, 05004], lr: 0.018820, loss: 3.7482
2022-10-11 22:49:23 - train: epoch 0072, iter [02300, 05004], lr: 0.018796, loss: 4.1278
2022-10-11 22:50:00 - train: epoch 0072, iter [02400, 05004], lr: 0.018771, loss: 4.0108
2022-10-11 22:50:37 - train: epoch 0072, iter [02500, 05004], lr: 0.018746, loss: 4.4384
2022-10-11 22:51:14 - train: epoch 0072, iter [02600, 05004], lr: 0.018722, loss: 4.2891
2022-10-11 22:51:50 - train: epoch 0072, iter [02700, 05004], lr: 0.018698, loss: 4.7552
2022-10-11 22:52:28 - train: epoch 0072, iter [02800, 05004], lr: 0.018673, loss: 4.4675
2022-10-11 22:53:04 - train: epoch 0072, iter [02900, 05004], lr: 0.018649, loss: 3.3238
2022-10-11 22:53:42 - train: epoch 0072, iter [03000, 05004], lr: 0.018624, loss: 3.4449
2022-10-11 22:54:18 - train: epoch 0072, iter [03100, 05004], lr: 0.018600, loss: 4.5099
2022-10-11 22:54:54 - train: epoch 0072, iter [03200, 05004], lr: 0.018575, loss: 3.9184
2022-10-11 22:55:32 - train: epoch 0072, iter [03300, 05004], lr: 0.018551, loss: 3.6403
2022-10-11 22:56:08 - train: epoch 0072, iter [03400, 05004], lr: 0.018527, loss: 3.6153
2022-10-11 22:56:45 - train: epoch 0072, iter [03500, 05004], lr: 0.018502, loss: 3.6140
2022-10-11 22:57:23 - train: epoch 0072, iter [03600, 05004], lr: 0.018478, loss: 3.9444
2022-10-11 22:57:59 - train: epoch 0072, iter [03700, 05004], lr: 0.018453, loss: 3.9268
2022-10-11 22:58:36 - train: epoch 0072, iter [03800, 05004], lr: 0.018429, loss: 3.8474
2022-10-11 22:59:12 - train: epoch 0072, iter [03900, 05004], lr: 0.018405, loss: 4.4580
2022-10-11 22:59:50 - train: epoch 0072, iter [04000, 05004], lr: 0.018380, loss: 3.5749
2022-10-11 23:00:27 - train: epoch 0072, iter [04100, 05004], lr: 0.018356, loss: 4.3191
2022-10-11 23:01:04 - train: epoch 0072, iter [04200, 05004], lr: 0.018332, loss: 4.6027
2022-10-11 23:01:40 - train: epoch 0072, iter [04300, 05004], lr: 0.018308, loss: 3.7455
2022-10-11 23:02:17 - train: epoch 0072, iter [04400, 05004], lr: 0.018283, loss: 4.4974
2022-10-11 23:02:54 - train: epoch 0072, iter [04500, 05004], lr: 0.018259, loss: 3.7959
2022-10-11 23:03:31 - train: epoch 0072, iter [04600, 05004], lr: 0.018235, loss: 3.3319
2022-10-11 23:04:08 - train: epoch 0072, iter [04700, 05004], lr: 0.018211, loss: 4.2919
2022-10-11 23:04:45 - train: epoch 0072, iter [04800, 05004], lr: 0.018186, loss: 4.0792
2022-10-11 23:05:21 - train: epoch 0072, iter [04900, 05004], lr: 0.018162, loss: 4.0687
2022-10-11 23:05:57 - train: epoch 0072, iter [05000, 05004], lr: 0.018138, loss: 3.2838
2022-10-11 23:05:59 - train: epoch 072, train_loss: 3.9558
2022-10-11 23:07:17 - eval: epoch: 072, acc1: 67.544%, acc5: 88.308%, test_loss: 1.6008, per_image_load_time: 0.254ms, per_image_inference_time: 0.518ms
2022-10-11 23:07:18 - until epoch: 072, best_acc1: 68.600%
2022-10-11 23:07:18 - epoch 073 lr: 0.018137
2022-10-11 23:08:00 - train: epoch 0073, iter [00100, 05004], lr: 0.018113, loss: 4.1198
2022-10-11 23:08:37 - train: epoch 0073, iter [00200, 05004], lr: 0.018089, loss: 3.3667
2022-10-11 23:09:14 - train: epoch 0073, iter [00300, 05004], lr: 0.018064, loss: 4.4927
2022-10-11 23:09:50 - train: epoch 0073, iter [00400, 05004], lr: 0.018040, loss: 4.1127
2022-10-11 23:10:27 - train: epoch 0073, iter [00500, 05004], lr: 0.018016, loss: 3.4166
2022-10-11 23:11:04 - train: epoch 0073, iter [00600, 05004], lr: 0.017992, loss: 4.4555
2022-10-11 23:11:40 - train: epoch 0073, iter [00700, 05004], lr: 0.017968, loss: 3.5837
2022-10-11 23:12:16 - train: epoch 0073, iter [00800, 05004], lr: 0.017944, loss: 3.7620
2022-10-11 23:12:52 - train: epoch 0073, iter [00900, 05004], lr: 0.017920, loss: 3.4936
2022-10-11 23:13:29 - train: epoch 0073, iter [01000, 05004], lr: 0.017896, loss: 3.7436
2022-10-11 23:14:05 - train: epoch 0073, iter [01100, 05004], lr: 0.017872, loss: 4.4536
2022-10-11 23:14:42 - train: epoch 0073, iter [01200, 05004], lr: 0.017848, loss: 4.4931
2022-10-11 23:15:19 - train: epoch 0073, iter [01300, 05004], lr: 0.017824, loss: 3.9026
2022-10-11 23:15:55 - train: epoch 0073, iter [01400, 05004], lr: 0.017800, loss: 3.6872
2022-10-11 23:16:32 - train: epoch 0073, iter [01500, 05004], lr: 0.017776, loss: 3.8533
2022-10-11 23:17:09 - train: epoch 0073, iter [01600, 05004], lr: 0.017752, loss: 4.3039
2022-10-11 23:17:47 - train: epoch 0073, iter [01700, 05004], lr: 0.017728, loss: 4.1521
2022-10-11 23:18:23 - train: epoch 0073, iter [01800, 05004], lr: 0.017704, loss: 3.5813
2022-10-11 23:19:00 - train: epoch 0073, iter [01900, 05004], lr: 0.017680, loss: 4.0926
2022-10-11 23:19:36 - train: epoch 0073, iter [02000, 05004], lr: 0.017656, loss: 4.1674
2022-10-11 23:20:13 - train: epoch 0073, iter [02100, 05004], lr: 0.017632, loss: 4.1347
2022-10-11 23:20:50 - train: epoch 0073, iter [02200, 05004], lr: 0.017608, loss: 4.4767
2022-10-11 23:21:27 - train: epoch 0073, iter [02300, 05004], lr: 0.017584, loss: 3.5268
2022-10-11 23:22:03 - train: epoch 0073, iter [02400, 05004], lr: 0.017560, loss: 3.5171
2022-10-11 23:22:39 - train: epoch 0073, iter [02500, 05004], lr: 0.017536, loss: 4.7279
2022-10-11 23:23:16 - train: epoch 0073, iter [02600, 05004], lr: 0.017512, loss: 3.6407
2022-10-11 23:23:53 - train: epoch 0073, iter [02700, 05004], lr: 0.017489, loss: 3.9458
2022-10-11 23:24:30 - train: epoch 0073, iter [02800, 05004], lr: 0.017465, loss: 3.9068
2022-10-11 23:25:06 - train: epoch 0073, iter [02900, 05004], lr: 0.017441, loss: 4.1431
2022-10-11 23:25:43 - train: epoch 0073, iter [03000, 05004], lr: 0.017417, loss: 4.0377
2022-10-11 23:26:20 - train: epoch 0073, iter [03100, 05004], lr: 0.017393, loss: 4.3831
2022-10-11 23:26:56 - train: epoch 0073, iter [03200, 05004], lr: 0.017370, loss: 4.6035
2022-10-11 23:27:32 - train: epoch 0073, iter [03300, 05004], lr: 0.017346, loss: 3.6184
2022-10-11 23:28:09 - train: epoch 0073, iter [03400, 05004], lr: 0.017322, loss: 4.7952
2022-10-11 23:28:45 - train: epoch 0073, iter [03500, 05004], lr: 0.017298, loss: 4.4943
2022-10-11 23:29:22 - train: epoch 0073, iter [03600, 05004], lr: 0.017275, loss: 4.2325
2022-10-11 23:29:59 - train: epoch 0073, iter [03700, 05004], lr: 0.017251, loss: 4.5691
2022-10-11 23:30:36 - train: epoch 0073, iter [03800, 05004], lr: 0.017227, loss: 3.9099
2022-10-11 23:31:12 - train: epoch 0073, iter [03900, 05004], lr: 0.017203, loss: 3.9922
2022-10-11 23:31:49 - train: epoch 0073, iter [04000, 05004], lr: 0.017180, loss: 3.7041
2022-10-11 23:32:26 - train: epoch 0073, iter [04100, 05004], lr: 0.017156, loss: 3.4613
2022-10-11 23:33:03 - train: epoch 0073, iter [04200, 05004], lr: 0.017132, loss: 4.4559
2022-10-11 23:33:40 - train: epoch 0073, iter [04300, 05004], lr: 0.017109, loss: 4.0774
2022-10-11 23:34:16 - train: epoch 0073, iter [04400, 05004], lr: 0.017085, loss: 3.2786
2022-10-11 23:34:53 - train: epoch 0073, iter [04500, 05004], lr: 0.017062, loss: 4.3016
2022-10-11 23:35:30 - train: epoch 0073, iter [04600, 05004], lr: 0.017038, loss: 3.8774
2022-10-11 23:36:05 - train: epoch 0073, iter [04700, 05004], lr: 0.017014, loss: 3.1701
2022-10-11 23:36:42 - train: epoch 0073, iter [04800, 05004], lr: 0.016991, loss: 4.1384
2022-10-11 23:37:20 - train: epoch 0073, iter [04900, 05004], lr: 0.016967, loss: 4.3733
2022-10-11 23:37:53 - train: epoch 0073, iter [05000, 05004], lr: 0.016944, loss: 4.1994
2022-10-11 23:37:55 - train: epoch 073, train_loss: 3.9369
2022-10-11 23:39:14 - eval: epoch: 073, acc1: 68.332%, acc5: 88.972%, test_loss: 1.5626, per_image_load_time: 0.272ms, per_image_inference_time: 0.518ms
2022-10-11 23:39:15 - until epoch: 073, best_acc1: 68.600%
2022-10-11 23:39:15 - epoch 074 lr: 0.016943
2022-10-11 23:39:57 - train: epoch 0074, iter [00100, 05004], lr: 0.016919, loss: 4.1364
2022-10-11 23:40:34 - train: epoch 0074, iter [00200, 05004], lr: 0.016896, loss: 3.3565
2022-10-11 23:41:11 - train: epoch 0074, iter [00300, 05004], lr: 0.016872, loss: 4.0596
2022-10-11 23:41:47 - train: epoch 0074, iter [00400, 05004], lr: 0.016849, loss: 3.9090
2022-10-11 23:42:24 - train: epoch 0074, iter [00500, 05004], lr: 0.016825, loss: 3.8336
2022-10-11 23:43:01 - train: epoch 0074, iter [00600, 05004], lr: 0.016802, loss: 3.4827
2022-10-11 23:43:37 - train: epoch 0074, iter [00700, 05004], lr: 0.016778, loss: 4.3523
2022-10-11 23:44:14 - train: epoch 0074, iter [00800, 05004], lr: 0.016755, loss: 3.9265
2022-10-11 23:44:51 - train: epoch 0074, iter [00900, 05004], lr: 0.016731, loss: 3.5379
2022-10-11 23:45:27 - train: epoch 0074, iter [01000, 05004], lr: 0.016708, loss: 4.5136
2022-10-11 23:46:03 - train: epoch 0074, iter [01100, 05004], lr: 0.016685, loss: 3.3009
2022-10-11 23:46:40 - train: epoch 0074, iter [01200, 05004], lr: 0.016661, loss: 4.1847
2022-10-11 23:47:17 - train: epoch 0074, iter [01300, 05004], lr: 0.016638, loss: 3.7092
2022-10-11 23:47:54 - train: epoch 0074, iter [01400, 05004], lr: 0.016614, loss: 3.2278
2022-10-11 23:48:31 - train: epoch 0074, iter [01500, 05004], lr: 0.016591, loss: 4.6031
2022-10-11 23:49:07 - train: epoch 0074, iter [01600, 05004], lr: 0.016568, loss: 4.2354
2022-10-11 23:49:44 - train: epoch 0074, iter [01700, 05004], lr: 0.016544, loss: 4.4241
2022-10-11 23:50:21 - train: epoch 0074, iter [01800, 05004], lr: 0.016521, loss: 3.5343
2022-10-11 23:50:57 - train: epoch 0074, iter [01900, 05004], lr: 0.016498, loss: 4.1935
2022-10-11 23:51:35 - train: epoch 0074, iter [02000, 05004], lr: 0.016474, loss: 4.0250
2022-10-11 23:52:11 - train: epoch 0074, iter [02100, 05004], lr: 0.016451, loss: 3.9183
2022-10-11 23:52:48 - train: epoch 0074, iter [02200, 05004], lr: 0.016428, loss: 3.9394
2022-10-11 23:53:25 - train: epoch 0074, iter [02300, 05004], lr: 0.016405, loss: 3.9923
2022-10-11 23:54:02 - train: epoch 0074, iter [02400, 05004], lr: 0.016381, loss: 4.5511
2022-10-11 23:54:38 - train: epoch 0074, iter [02500, 05004], lr: 0.016358, loss: 3.9578
2022-10-11 23:55:15 - train: epoch 0074, iter [02600, 05004], lr: 0.016335, loss: 4.2015
2022-10-11 23:55:52 - train: epoch 0074, iter [02700, 05004], lr: 0.016312, loss: 3.8943
2022-10-11 23:56:28 - train: epoch 0074, iter [02800, 05004], lr: 0.016289, loss: 3.7664
2022-10-11 23:57:05 - train: epoch 0074, iter [02900, 05004], lr: 0.016265, loss: 4.3833
2022-10-11 23:57:42 - train: epoch 0074, iter [03000, 05004], lr: 0.016242, loss: 3.8486
2022-10-11 23:58:19 - train: epoch 0074, iter [03100, 05004], lr: 0.016219, loss: 3.3397
2022-10-11 23:58:56 - train: epoch 0074, iter [03200, 05004], lr: 0.016196, loss: 3.2188
2022-10-11 23:59:34 - train: epoch 0074, iter [03300, 05004], lr: 0.016173, loss: 3.2982
2022-10-12 00:00:10 - train: epoch 0074, iter [03400, 05004], lr: 0.016150, loss: 3.4652
2022-10-12 00:00:49 - train: epoch 0074, iter [03500, 05004], lr: 0.016127, loss: 3.3233
2022-10-12 00:01:25 - train: epoch 0074, iter [03600, 05004], lr: 0.016104, loss: 3.6564
2022-10-12 00:02:02 - train: epoch 0074, iter [03700, 05004], lr: 0.016081, loss: 3.3211
2022-10-12 00:02:40 - train: epoch 0074, iter [03800, 05004], lr: 0.016058, loss: 4.8161
2022-10-12 00:03:16 - train: epoch 0074, iter [03900, 05004], lr: 0.016034, loss: 4.4976
2022-10-12 00:03:54 - train: epoch 0074, iter [04000, 05004], lr: 0.016011, loss: 3.6474
2022-10-12 00:04:31 - train: epoch 0074, iter [04100, 05004], lr: 0.015988, loss: 4.0665
2022-10-12 00:05:08 - train: epoch 0074, iter [04200, 05004], lr: 0.015965, loss: 3.4032
2022-10-12 00:05:45 - train: epoch 0074, iter [04300, 05004], lr: 0.015942, loss: 4.3953
2022-10-12 00:06:23 - train: epoch 0074, iter [04400, 05004], lr: 0.015920, loss: 3.9958
2022-10-12 00:07:00 - train: epoch 0074, iter [04500, 05004], lr: 0.015897, loss: 4.3338
2022-10-12 00:07:37 - train: epoch 0074, iter [04600, 05004], lr: 0.015874, loss: 3.4009
2022-10-12 00:08:14 - train: epoch 0074, iter [04700, 05004], lr: 0.015851, loss: 4.6954
2022-10-12 00:08:51 - train: epoch 0074, iter [04800, 05004], lr: 0.015828, loss: 4.5569
2022-10-12 00:09:28 - train: epoch 0074, iter [04900, 05004], lr: 0.015805, loss: 4.3613
2022-10-12 00:10:03 - train: epoch 0074, iter [05000, 05004], lr: 0.015782, loss: 3.8150
2022-10-12 00:10:05 - train: epoch 074, train_loss: 3.9235
2022-10-12 00:11:23 - eval: epoch: 074, acc1: 69.114%, acc5: 89.436%, test_loss: 1.5046, per_image_load_time: 0.271ms, per_image_inference_time: 0.526ms
2022-10-12 00:11:23 - until epoch: 074, best_acc1: 69.114%
2022-10-12 00:11:23 - epoch 075 lr: 0.015781
2022-10-12 00:12:06 - train: epoch 0075, iter [00100, 05004], lr: 0.015758, loss: 4.1208
2022-10-12 00:12:43 - train: epoch 0075, iter [00200, 05004], lr: 0.015735, loss: 3.2950
2022-10-12 00:13:20 - train: epoch 0075, iter [00300, 05004], lr: 0.015712, loss: 4.8764
2022-10-12 00:13:56 - train: epoch 0075, iter [00400, 05004], lr: 0.015690, loss: 4.3400
2022-10-12 00:14:32 - train: epoch 0075, iter [00500, 05004], lr: 0.015667, loss: 4.1680
2022-10-12 00:15:10 - train: epoch 0075, iter [00600, 05004], lr: 0.015644, loss: 3.5565
2022-10-12 00:15:47 - train: epoch 0075, iter [00700, 05004], lr: 0.015621, loss: 3.5634
2022-10-12 00:16:23 - train: epoch 0075, iter [00800, 05004], lr: 0.015598, loss: 4.4421
2022-10-12 00:16:59 - train: epoch 0075, iter [00900, 05004], lr: 0.015576, loss: 4.8935
2022-10-12 00:17:35 - train: epoch 0075, iter [01000, 05004], lr: 0.015553, loss: 4.1307
2022-10-12 00:18:11 - train: epoch 0075, iter [01100, 05004], lr: 0.015530, loss: 3.0351
2022-10-12 00:18:49 - train: epoch 0075, iter [01200, 05004], lr: 0.015507, loss: 3.5062
2022-10-12 00:19:25 - train: epoch 0075, iter [01300, 05004], lr: 0.015485, loss: 4.0429
2022-10-12 00:20:03 - train: epoch 0075, iter [01400, 05004], lr: 0.015462, loss: 4.2713
2022-10-12 00:20:39 - train: epoch 0075, iter [01500, 05004], lr: 0.015439, loss: 4.2434
2022-10-12 00:21:15 - train: epoch 0075, iter [01600, 05004], lr: 0.015417, loss: 3.4486
2022-10-12 00:21:52 - train: epoch 0075, iter [01700, 05004], lr: 0.015394, loss: 3.9673
2022-10-12 00:22:29 - train: epoch 0075, iter [01800, 05004], lr: 0.015371, loss: 3.5269
2022-10-12 00:23:05 - train: epoch 0075, iter [01900, 05004], lr: 0.015349, loss: 4.2424
2022-10-12 00:23:43 - train: epoch 0075, iter [02000, 05004], lr: 0.015326, loss: 4.4988
2022-10-12 00:24:19 - train: epoch 0075, iter [02100, 05004], lr: 0.015304, loss: 3.4812
2022-10-12 00:24:56 - train: epoch 0075, iter [02200, 05004], lr: 0.015281, loss: 4.2978
2022-10-12 00:25:33 - train: epoch 0075, iter [02300, 05004], lr: 0.015258, loss: 4.2108
2022-10-12 00:26:09 - train: epoch 0075, iter [02400, 05004], lr: 0.015236, loss: 4.5284
2022-10-12 00:26:45 - train: epoch 0075, iter [02500, 05004], lr: 0.015213, loss: 3.1975
2022-10-12 00:27:21 - train: epoch 0075, iter [02600, 05004], lr: 0.015191, loss: 4.2421
2022-10-12 00:27:59 - train: epoch 0075, iter [02700, 05004], lr: 0.015168, loss: 3.9480
2022-10-12 00:28:36 - train: epoch 0075, iter [02800, 05004], lr: 0.015146, loss: 3.1473
2022-10-12 00:29:12 - train: epoch 0075, iter [02900, 05004], lr: 0.015123, loss: 4.3276
2022-10-12 00:29:48 - train: epoch 0075, iter [03000, 05004], lr: 0.015101, loss: 3.7980
2022-10-12 00:30:25 - train: epoch 0075, iter [03100, 05004], lr: 0.015078, loss: 3.8041
2022-10-12 00:31:02 - train: epoch 0075, iter [03200, 05004], lr: 0.015056, loss: 3.8428
2022-10-12 00:31:39 - train: epoch 0075, iter [03300, 05004], lr: 0.015033, loss: 3.3224
2022-10-12 00:32:15 - train: epoch 0075, iter [03400, 05004], lr: 0.015011, loss: 4.2998
2022-10-12 00:32:52 - train: epoch 0075, iter [03500, 05004], lr: 0.014989, loss: 3.4671
2022-10-12 00:33:29 - train: epoch 0075, iter [03600, 05004], lr: 0.014966, loss: 3.7839
2022-10-12 00:34:06 - train: epoch 0075, iter [03700, 05004], lr: 0.014944, loss: 3.8665
2022-10-12 00:34:42 - train: epoch 0075, iter [03800, 05004], lr: 0.014921, loss: 4.5203
2022-10-12 00:35:19 - train: epoch 0075, iter [03900, 05004], lr: 0.014899, loss: 4.8097
2022-10-12 00:35:55 - train: epoch 0075, iter [04000, 05004], lr: 0.014877, loss: 3.6709
2022-10-12 00:36:33 - train: epoch 0075, iter [04100, 05004], lr: 0.014854, loss: 3.8147
2022-10-12 00:37:09 - train: epoch 0075, iter [04200, 05004], lr: 0.014832, loss: 3.6304
2022-10-12 00:37:46 - train: epoch 0075, iter [04300, 05004], lr: 0.014810, loss: 4.5618
2022-10-12 00:38:23 - train: epoch 0075, iter [04400, 05004], lr: 0.014788, loss: 4.0086
2022-10-12 00:39:00 - train: epoch 0075, iter [04500, 05004], lr: 0.014765, loss: 4.6316
2022-10-12 00:39:37 - train: epoch 0075, iter [04600, 05004], lr: 0.014743, loss: 3.4450
2022-10-12 00:40:14 - train: epoch 0075, iter [04700, 05004], lr: 0.014721, loss: 3.8331
2022-10-12 00:40:51 - train: epoch 0075, iter [04800, 05004], lr: 0.014699, loss: 3.6254
2022-10-12 00:41:28 - train: epoch 0075, iter [04900, 05004], lr: 0.014676, loss: 3.7721
2022-10-12 00:42:02 - train: epoch 0075, iter [05000, 05004], lr: 0.014654, loss: 3.8874
2022-10-12 00:42:04 - train: epoch 075, train_loss: 3.9003
2022-10-12 00:43:23 - eval: epoch: 075, acc1: 69.400%, acc5: 89.494%, test_loss: 1.4835, per_image_load_time: 0.268ms, per_image_inference_time: 0.539ms
2022-10-12 00:43:24 - until epoch: 075, best_acc1: 69.400%
2022-10-12 00:43:24 - epoch 076 lr: 0.014653
2022-10-12 00:44:06 - train: epoch 0076, iter [00100, 05004], lr: 0.014631, loss: 3.8803
2022-10-12 00:44:42 - train: epoch 0076, iter [00200, 05004], lr: 0.014609, loss: 2.5532
2022-10-12 00:45:18 - train: epoch 0076, iter [00300, 05004], lr: 0.014587, loss: 3.9195
2022-10-12 00:45:55 - train: epoch 0076, iter [00400, 05004], lr: 0.014565, loss: 4.0867
2022-10-12 00:46:32 - train: epoch 0076, iter [00500, 05004], lr: 0.014542, loss: 4.0787
2022-10-12 00:47:09 - train: epoch 0076, iter [00600, 05004], lr: 0.014520, loss: 4.3841
2022-10-12 00:47:45 - train: epoch 0076, iter [00700, 05004], lr: 0.014498, loss: 3.8487
2022-10-12 00:48:21 - train: epoch 0076, iter [00800, 05004], lr: 0.014476, loss: 3.6388
2022-10-12 00:48:58 - train: epoch 0076, iter [00900, 05004], lr: 0.014454, loss: 3.9753
2022-10-12 00:49:34 - train: epoch 0076, iter [01000, 05004], lr: 0.014432, loss: 4.4366
2022-10-12 00:50:11 - train: epoch 0076, iter [01100, 05004], lr: 0.014410, loss: 3.4308
2022-10-12 00:50:48 - train: epoch 0076, iter [01200, 05004], lr: 0.014388, loss: 3.1584
2022-10-12 00:51:24 - train: epoch 0076, iter [01300, 05004], lr: 0.014366, loss: 3.7195
2022-10-12 00:52:01 - train: epoch 0076, iter [01400, 05004], lr: 0.014344, loss: 3.2786
2022-10-12 00:52:38 - train: epoch 0076, iter [01500, 05004], lr: 0.014322, loss: 3.0553
2022-10-12 00:53:15 - train: epoch 0076, iter [01600, 05004], lr: 0.014300, loss: 3.6602
2022-10-12 00:53:51 - train: epoch 0076, iter [01700, 05004], lr: 0.014278, loss: 3.5983
2022-10-12 00:54:28 - train: epoch 0076, iter [01800, 05004], lr: 0.014256, loss: 4.2733
2022-10-12 00:55:04 - train: epoch 0076, iter [01900, 05004], lr: 0.014234, loss: 4.2247
2022-10-12 00:55:41 - train: epoch 0076, iter [02000, 05004], lr: 0.014212, loss: 4.4095
2022-10-12 00:56:18 - train: epoch 0076, iter [02100, 05004], lr: 0.014190, loss: 3.3618
2022-10-12 00:56:54 - train: epoch 0076, iter [02200, 05004], lr: 0.014168, loss: 3.8256
2022-10-12 00:57:31 - train: epoch 0076, iter [02300, 05004], lr: 0.014146, loss: 4.0452
2022-10-12 00:58:09 - train: epoch 0076, iter [02400, 05004], lr: 0.014125, loss: 4.0560
2022-10-12 00:58:46 - train: epoch 0076, iter [02500, 05004], lr: 0.014103, loss: 3.6498
2022-10-12 00:59:22 - train: epoch 0076, iter [02600, 05004], lr: 0.014081, loss: 3.8100
2022-10-12 00:59:59 - train: epoch 0076, iter [02700, 05004], lr: 0.014059, loss: 4.0387
2022-10-12 01:00:35 - train: epoch 0076, iter [02800, 05004], lr: 0.014037, loss: 3.2110
2022-10-12 01:01:12 - train: epoch 0076, iter [02900, 05004], lr: 0.014015, loss: 4.2037
2022-10-12 01:01:48 - train: epoch 0076, iter [03000, 05004], lr: 0.013994, loss: 3.4575
2022-10-12 01:02:25 - train: epoch 0076, iter [03100, 05004], lr: 0.013972, loss: 3.9005
2022-10-12 01:03:02 - train: epoch 0076, iter [03200, 05004], lr: 0.013950, loss: 3.7443
2022-10-12 01:03:39 - train: epoch 0076, iter [03300, 05004], lr: 0.013928, loss: 4.2979
2022-10-12 01:04:15 - train: epoch 0076, iter [03400, 05004], lr: 0.013907, loss: 4.1890
2022-10-12 01:04:52 - train: epoch 0076, iter [03500, 05004], lr: 0.013885, loss: 3.2772
2022-10-12 01:05:30 - train: epoch 0076, iter [03600, 05004], lr: 0.013863, loss: 3.8087
2022-10-12 01:06:05 - train: epoch 0076, iter [03700, 05004], lr: 0.013842, loss: 3.5164
2022-10-12 01:06:43 - train: epoch 0076, iter [03800, 05004], lr: 0.013820, loss: 3.7944
2022-10-12 01:07:18 - train: epoch 0076, iter [03900, 05004], lr: 0.013798, loss: 4.4621
2022-10-12 01:07:56 - train: epoch 0076, iter [04000, 05004], lr: 0.013777, loss: 3.5654
2022-10-12 01:08:32 - train: epoch 0076, iter [04100, 05004], lr: 0.013755, loss: 3.5237
2022-10-12 01:09:09 - train: epoch 0076, iter [04200, 05004], lr: 0.013733, loss: 3.4992
2022-10-12 01:09:46 - train: epoch 0076, iter [04300, 05004], lr: 0.013712, loss: 3.8223
2022-10-12 01:10:24 - train: epoch 0076, iter [04400, 05004], lr: 0.013690, loss: 3.7532
2022-10-12 01:11:00 - train: epoch 0076, iter [04500, 05004], lr: 0.013669, loss: 2.9837
2022-10-12 01:11:37 - train: epoch 0076, iter [04600, 05004], lr: 0.013647, loss: 4.0457
2022-10-12 01:12:14 - train: epoch 0076, iter [04700, 05004], lr: 0.013626, loss: 3.6894
2022-10-12 01:12:51 - train: epoch 0076, iter [04800, 05004], lr: 0.013604, loss: 3.8550
2022-10-12 01:13:27 - train: epoch 0076, iter [04900, 05004], lr: 0.013583, loss: 3.1497
2022-10-12 01:14:02 - train: epoch 0076, iter [05000, 05004], lr: 0.013561, loss: 3.5755
2022-10-12 01:14:05 - train: epoch 076, train_loss: 3.8704
2022-10-12 01:15:23 - eval: epoch: 076, acc1: 69.554%, acc5: 89.602%, test_loss: 1.5104, per_image_load_time: 0.295ms, per_image_inference_time: 0.523ms
2022-10-12 01:15:23 - until epoch: 076, best_acc1: 69.554%
2022-10-12 01:15:23 - epoch 077 lr: 0.013560
2022-10-12 01:16:06 - train: epoch 0077, iter [00100, 05004], lr: 0.013539, loss: 4.0862
2022-10-12 01:16:42 - train: epoch 0077, iter [00200, 05004], lr: 0.013517, loss: 4.2512
2022-10-12 01:17:18 - train: epoch 0077, iter [00300, 05004], lr: 0.013496, loss: 3.3361
2022-10-12 01:17:54 - train: epoch 0077, iter [00400, 05004], lr: 0.013474, loss: 3.6257
2022-10-12 01:18:32 - train: epoch 0077, iter [00500, 05004], lr: 0.013453, loss: 4.1625
2022-10-12 01:19:08 - train: epoch 0077, iter [00600, 05004], lr: 0.013432, loss: 3.4454
2022-10-12 01:19:43 - train: epoch 0077, iter [00700, 05004], lr: 0.013410, loss: 4.5019
2022-10-12 01:20:21 - train: epoch 0077, iter [00800, 05004], lr: 0.013389, loss: 3.8707
2022-10-12 01:20:57 - train: epoch 0077, iter [00900, 05004], lr: 0.013367, loss: 4.7996
2022-10-12 01:21:34 - train: epoch 0077, iter [01000, 05004], lr: 0.013346, loss: 4.1220
2022-10-12 01:22:10 - train: epoch 0077, iter [01100, 05004], lr: 0.013325, loss: 3.6926
2022-10-12 01:22:46 - train: epoch 0077, iter [01200, 05004], lr: 0.013303, loss: 3.7311
2022-10-12 01:23:24 - train: epoch 0077, iter [01300, 05004], lr: 0.013282, loss: 3.3306
2022-10-12 01:24:00 - train: epoch 0077, iter [01400, 05004], lr: 0.013261, loss: 3.4794
2022-10-12 01:24:37 - train: epoch 0077, iter [01500, 05004], lr: 0.013240, loss: 2.4657
2022-10-12 01:25:13 - train: epoch 0077, iter [01600, 05004], lr: 0.013218, loss: 3.5605
2022-10-12 01:25:50 - train: epoch 0077, iter [01700, 05004], lr: 0.013197, loss: 4.3008
2022-10-12 01:26:25 - train: epoch 0077, iter [01800, 05004], lr: 0.013176, loss: 3.0901
2022-10-12 01:27:01 - train: epoch 0077, iter [01900, 05004], lr: 0.013155, loss: 3.6809
2022-10-12 01:27:38 - train: epoch 0077, iter [02000, 05004], lr: 0.013133, loss: 3.6212
2022-10-12 01:28:15 - train: epoch 0077, iter [02100, 05004], lr: 0.013112, loss: 4.1254
2022-10-12 01:28:51 - train: epoch 0077, iter [02200, 05004], lr: 0.013091, loss: 3.5359
2022-10-12 01:29:29 - train: epoch 0077, iter [02300, 05004], lr: 0.013070, loss: 3.4999
2022-10-12 01:30:05 - train: epoch 0077, iter [02400, 05004], lr: 0.013049, loss: 3.7820
2022-10-12 01:30:41 - train: epoch 0077, iter [02500, 05004], lr: 0.013028, loss: 3.7874
2022-10-12 01:31:18 - train: epoch 0077, iter [02600, 05004], lr: 0.013006, loss: 3.6786
2022-10-12 01:31:55 - train: epoch 0077, iter [02700, 05004], lr: 0.012985, loss: 4.2983
2022-10-12 01:32:32 - train: epoch 0077, iter [02800, 05004], lr: 0.012964, loss: 4.1479
2022-10-12 01:33:08 - train: epoch 0077, iter [02900, 05004], lr: 0.012943, loss: 3.5417
2022-10-12 01:33:45 - train: epoch 0077, iter [03000, 05004], lr: 0.012922, loss: 3.0097
2022-10-12 01:34:22 - train: epoch 0077, iter [03100, 05004], lr: 0.012901, loss: 4.1340
2022-10-12 01:34:59 - train: epoch 0077, iter [03200, 05004], lr: 0.012880, loss: 3.3225
2022-10-12 01:35:35 - train: epoch 0077, iter [03300, 05004], lr: 0.012859, loss: 3.5288
2022-10-12 01:36:12 - train: epoch 0077, iter [03400, 05004], lr: 0.012838, loss: 3.9140
2022-10-12 01:36:48 - train: epoch 0077, iter [03500, 05004], lr: 0.012817, loss: 3.3821
2022-10-12 01:37:25 - train: epoch 0077, iter [03600, 05004], lr: 0.012796, loss: 3.6303
2022-10-12 01:38:02 - train: epoch 0077, iter [03700, 05004], lr: 0.012775, loss: 3.5439
2022-10-12 01:38:39 - train: epoch 0077, iter [03800, 05004], lr: 0.012754, loss: 4.1617
2022-10-12 01:39:15 - train: epoch 0077, iter [03900, 05004], lr: 0.012733, loss: 3.4420
2022-10-12 01:39:52 - train: epoch 0077, iter [04000, 05004], lr: 0.012712, loss: 4.0600
2022-10-12 01:40:29 - train: epoch 0077, iter [04100, 05004], lr: 0.012691, loss: 3.9635
2022-10-12 01:41:06 - train: epoch 0077, iter [04200, 05004], lr: 0.012671, loss: 3.5997
2022-10-12 01:41:42 - train: epoch 0077, iter [04300, 05004], lr: 0.012650, loss: 3.8359
2022-10-12 01:42:19 - train: epoch 0077, iter [04400, 05004], lr: 0.012629, loss: 2.9151
2022-10-12 01:42:57 - train: epoch 0077, iter [04500, 05004], lr: 0.012608, loss: 4.1023
2022-10-12 01:43:33 - train: epoch 0077, iter [04600, 05004], lr: 0.012587, loss: 3.5230
2022-10-12 01:44:10 - train: epoch 0077, iter [04700, 05004], lr: 0.012566, loss: 4.0342
2022-10-12 01:44:46 - train: epoch 0077, iter [04800, 05004], lr: 0.012546, loss: 3.2219
2022-10-12 01:45:23 - train: epoch 0077, iter [04900, 05004], lr: 0.012525, loss: 3.9272
2022-10-12 01:45:57 - train: epoch 0077, iter [05000, 05004], lr: 0.012504, loss: 4.4537
2022-10-12 01:45:59 - train: epoch 077, train_loss: 3.8579
2022-10-12 01:47:18 - eval: epoch: 077, acc1: 67.756%, acc5: 88.454%, test_loss: 1.6665, per_image_load_time: 0.314ms, per_image_inference_time: 0.533ms
2022-10-12 01:47:18 - until epoch: 077, best_acc1: 69.554%
2022-10-12 01:47:18 - epoch 078 lr: 0.012503
2022-10-12 01:48:01 - train: epoch 0078, iter [00100, 05004], lr: 0.012482, loss: 4.0594
2022-10-12 01:48:37 - train: epoch 0078, iter [00200, 05004], lr: 0.012462, loss: 3.6295
2022-10-12 01:49:14 - train: epoch 0078, iter [00300, 05004], lr: 0.012441, loss: 4.0805
2022-10-12 01:49:50 - train: epoch 0078, iter [00400, 05004], lr: 0.012420, loss: 4.4425
2022-10-12 01:50:27 - train: epoch 0078, iter [00500, 05004], lr: 0.012400, loss: 3.2361
2022-10-12 01:51:04 - train: epoch 0078, iter [00600, 05004], lr: 0.012379, loss: 4.0826
2022-10-12 01:51:39 - train: epoch 0078, iter [00700, 05004], lr: 0.012358, loss: 3.5123
2022-10-12 01:52:15 - train: epoch 0078, iter [00800, 05004], lr: 0.012338, loss: 3.4020
2022-10-12 01:52:51 - train: epoch 0078, iter [00900, 05004], lr: 0.012317, loss: 4.3771
2022-10-12 01:53:27 - train: epoch 0078, iter [01000, 05004], lr: 0.012296, loss: 3.7613
2022-10-12 01:54:04 - train: epoch 0078, iter [01100, 05004], lr: 0.012276, loss: 2.6320
2022-10-12 01:54:40 - train: epoch 0078, iter [01200, 05004], lr: 0.012255, loss: 3.5529
2022-10-12 01:55:17 - train: epoch 0078, iter [01300, 05004], lr: 0.012235, loss: 3.2530
2022-10-12 01:55:54 - train: epoch 0078, iter [01400, 05004], lr: 0.012214, loss: 3.2564
2022-10-12 01:56:31 - train: epoch 0078, iter [01500, 05004], lr: 0.012194, loss: 3.4360
2022-10-12 01:57:07 - train: epoch 0078, iter [01600, 05004], lr: 0.012173, loss: 4.4442
2022-10-12 01:57:44 - train: epoch 0078, iter [01700, 05004], lr: 0.012152, loss: 3.8659
2022-10-12 01:58:21 - train: epoch 0078, iter [01800, 05004], lr: 0.012132, loss: 3.6687
2022-10-12 01:58:58 - train: epoch 0078, iter [01900, 05004], lr: 0.012111, loss: 3.8133
2022-10-12 01:59:34 - train: epoch 0078, iter [02000, 05004], lr: 0.012091, loss: 3.6030
2022-10-12 02:00:11 - train: epoch 0078, iter [02100, 05004], lr: 0.012071, loss: 4.0011
2022-10-12 02:00:48 - train: epoch 0078, iter [02200, 05004], lr: 0.012050, loss: 4.4379
2022-10-12 02:01:24 - train: epoch 0078, iter [02300, 05004], lr: 0.012030, loss: 4.2218
2022-10-12 02:02:01 - train: epoch 0078, iter [02400, 05004], lr: 0.012009, loss: 3.2890
2022-10-12 02:02:38 - train: epoch 0078, iter [02500, 05004], lr: 0.011989, loss: 3.9096
2022-10-12 02:03:15 - train: epoch 0078, iter [02600, 05004], lr: 0.011969, loss: 4.0125
2022-10-12 02:03:52 - train: epoch 0078, iter [02700, 05004], lr: 0.011948, loss: 3.9480
2022-10-12 02:04:28 - train: epoch 0078, iter [02800, 05004], lr: 0.011928, loss: 4.3786
2022-10-12 02:05:05 - train: epoch 0078, iter [02900, 05004], lr: 0.011907, loss: 4.4677
2022-10-12 02:05:42 - train: epoch 0078, iter [03000, 05004], lr: 0.011887, loss: 3.1857
2022-10-12 02:06:18 - train: epoch 0078, iter [03100, 05004], lr: 0.011867, loss: 3.1800
2022-10-12 02:06:56 - train: epoch 0078, iter [03200, 05004], lr: 0.011847, loss: 3.5319
2022-10-12 02:07:32 - train: epoch 0078, iter [03300, 05004], lr: 0.011826, loss: 4.0004
2022-10-12 02:08:09 - train: epoch 0078, iter [03400, 05004], lr: 0.011806, loss: 3.9920
2022-10-12 02:08:46 - train: epoch 0078, iter [03500, 05004], lr: 0.011786, loss: 3.7106
2022-10-12 02:09:23 - train: epoch 0078, iter [03600, 05004], lr: 0.011766, loss: 3.7849
2022-10-12 02:09:59 - train: epoch 0078, iter [03700, 05004], lr: 0.011745, loss: 4.2922
2022-10-12 02:10:36 - train: epoch 0078, iter [03800, 05004], lr: 0.011725, loss: 3.5093
2022-10-12 02:11:13 - train: epoch 0078, iter [03900, 05004], lr: 0.011705, loss: 4.0289
2022-10-12 02:11:50 - train: epoch 0078, iter [04000, 05004], lr: 0.011685, loss: 4.4572
2022-10-12 02:12:27 - train: epoch 0078, iter [04100, 05004], lr: 0.011665, loss: 4.2769
2022-10-12 02:13:03 - train: epoch 0078, iter [04200, 05004], lr: 0.011645, loss: 4.3456
2022-10-12 02:13:40 - train: epoch 0078, iter [04300, 05004], lr: 0.011624, loss: 3.2494
2022-10-12 02:14:16 - train: epoch 0078, iter [04400, 05004], lr: 0.011604, loss: 3.6158
2022-10-12 02:14:53 - train: epoch 0078, iter [04500, 05004], lr: 0.011584, loss: 3.7721
2022-10-12 02:15:30 - train: epoch 0078, iter [04600, 05004], lr: 0.011564, loss: 3.8169
2022-10-12 02:16:07 - train: epoch 0078, iter [04700, 05004], lr: 0.011544, loss: 3.6480
2022-10-12 02:16:44 - train: epoch 0078, iter [04800, 05004], lr: 0.011524, loss: 4.4301
2022-10-12 02:17:20 - train: epoch 0078, iter [04900, 05004], lr: 0.011504, loss: 4.4359
2022-10-12 02:17:56 - train: epoch 0078, iter [05000, 05004], lr: 0.011484, loss: 3.9828
2022-10-12 02:17:58 - train: epoch 078, train_loss: 3.8349
2022-10-12 02:19:16 - eval: epoch: 078, acc1: 70.300%, acc5: 90.074%, test_loss: 1.4689, per_image_load_time: 0.266ms, per_image_inference_time: 0.501ms
2022-10-12 02:19:17 - until epoch: 078, best_acc1: 70.300%
2022-10-12 02:19:17 - epoch 079 lr: 0.011483
2022-10-12 02:19:59 - train: epoch 0079, iter [00100, 05004], lr: 0.011463, loss: 2.7489
2022-10-12 02:20:35 - train: epoch 0079, iter [00200, 05004], lr: 0.011443, loss: 4.5576
2022-10-12 02:21:12 - train: epoch 0079, iter [00300, 05004], lr: 0.011423, loss: 3.6239
2022-10-12 02:21:50 - train: epoch 0079, iter [00400, 05004], lr: 0.011403, loss: 4.4567
2022-10-12 02:22:27 - train: epoch 0079, iter [00500, 05004], lr: 0.011383, loss: 3.8787
2022-10-12 02:23:03 - train: epoch 0079, iter [00600, 05004], lr: 0.011363, loss: 3.5345
2022-10-12 02:23:41 - train: epoch 0079, iter [00700, 05004], lr: 0.011344, loss: 4.3829
2022-10-12 02:24:17 - train: epoch 0079, iter [00800, 05004], lr: 0.011324, loss: 3.5454
2022-10-12 02:24:54 - train: epoch 0079, iter [00900, 05004], lr: 0.011304, loss: 4.0606
2022-10-12 02:25:31 - train: epoch 0079, iter [01000, 05004], lr: 0.011284, loss: 4.6501
2022-10-12 02:26:08 - train: epoch 0079, iter [01100, 05004], lr: 0.011264, loss: 3.4284
2022-10-12 02:26:45 - train: epoch 0079, iter [01200, 05004], lr: 0.011244, loss: 4.4294
2022-10-12 02:27:22 - train: epoch 0079, iter [01300, 05004], lr: 0.011224, loss: 4.4914
2022-10-12 02:27:59 - train: epoch 0079, iter [01400, 05004], lr: 0.011205, loss: 3.3556
2022-10-12 02:28:36 - train: epoch 0079, iter [01500, 05004], lr: 0.011185, loss: 4.4797
2022-10-12 02:29:13 - train: epoch 0079, iter [01600, 05004], lr: 0.011165, loss: 4.1119
2022-10-12 02:29:49 - train: epoch 0079, iter [01700, 05004], lr: 0.011145, loss: 4.4339
2022-10-12 02:30:25 - train: epoch 0079, iter [01800, 05004], lr: 0.011126, loss: 3.4837
2022-10-12 02:31:03 - train: epoch 0079, iter [01900, 05004], lr: 0.011106, loss: 3.9318
2022-10-12 02:31:39 - train: epoch 0079, iter [02000, 05004], lr: 0.011086, loss: 3.9988
2022-10-12 02:32:16 - train: epoch 0079, iter [02100, 05004], lr: 0.011066, loss: 3.1615
2022-10-12 02:32:53 - train: epoch 0079, iter [02200, 05004], lr: 0.011047, loss: 4.4087
2022-10-12 02:33:30 - train: epoch 0079, iter [02300, 05004], lr: 0.011027, loss: 3.7620
2022-10-12 02:34:05 - train: epoch 0079, iter [02400, 05004], lr: 0.011007, loss: 3.8868
2022-10-12 02:34:43 - train: epoch 0079, iter [02500, 05004], lr: 0.010988, loss: 4.6672
2022-10-12 02:35:20 - train: epoch 0079, iter [02600, 05004], lr: 0.010968, loss: 3.0935
2022-10-12 02:35:56 - train: epoch 0079, iter [02700, 05004], lr: 0.010949, loss: 3.3547
2022-10-12 02:36:33 - train: epoch 0079, iter [02800, 05004], lr: 0.010929, loss: 2.7472
2022-10-12 02:37:09 - train: epoch 0079, iter [02900, 05004], lr: 0.010909, loss: 4.0102
2022-10-12 02:37:46 - train: epoch 0079, iter [03000, 05004], lr: 0.010890, loss: 3.3331
2022-10-12 02:38:22 - train: epoch 0079, iter [03100, 05004], lr: 0.010870, loss: 3.1571
2022-10-12 02:38:59 - train: epoch 0079, iter [03200, 05004], lr: 0.010851, loss: 3.9234
2022-10-12 02:39:36 - train: epoch 0079, iter [03300, 05004], lr: 0.010831, loss: 3.9941
2022-10-12 02:40:14 - train: epoch 0079, iter [03400, 05004], lr: 0.010812, loss: 3.5555
2022-10-12 02:40:50 - train: epoch 0079, iter [03500, 05004], lr: 0.010792, loss: 3.5115
2022-10-12 02:41:27 - train: epoch 0079, iter [03600, 05004], lr: 0.010773, loss: 3.3854
2022-10-12 02:42:04 - train: epoch 0079, iter [03700, 05004], lr: 0.010753, loss: 3.1031
2022-10-12 02:42:41 - train: epoch 0079, iter [03800, 05004], lr: 0.010734, loss: 4.2319
2022-10-12 02:43:18 - train: epoch 0079, iter [03900, 05004], lr: 0.010715, loss: 3.6743
2022-10-12 02:43:55 - train: epoch 0079, iter [04000, 05004], lr: 0.010695, loss: 3.5205
2022-10-12 02:44:32 - train: epoch 0079, iter [04100, 05004], lr: 0.010676, loss: 3.8250
2022-10-12 02:45:08 - train: epoch 0079, iter [04200, 05004], lr: 0.010656, loss: 3.9713
2022-10-12 02:45:45 - train: epoch 0079, iter [04300, 05004], lr: 0.010637, loss: 3.5639
2022-10-12 02:46:21 - train: epoch 0079, iter [04400, 05004], lr: 0.010618, loss: 3.9744
2022-10-12 02:46:57 - train: epoch 0079, iter [04500, 05004], lr: 0.010598, loss: 4.5013
2022-10-12 02:47:34 - train: epoch 0079, iter [04600, 05004], lr: 0.010579, loss: 3.9810
2022-10-12 02:48:11 - train: epoch 0079, iter [04700, 05004], lr: 0.010560, loss: 3.9720
2022-10-12 02:48:48 - train: epoch 0079, iter [04800, 05004], lr: 0.010540, loss: 4.1046
2022-10-12 02:49:24 - train: epoch 0079, iter [04900, 05004], lr: 0.010521, loss: 3.7371
2022-10-12 02:49:59 - train: epoch 0079, iter [05000, 05004], lr: 0.010502, loss: 3.8487
2022-10-12 02:50:01 - train: epoch 079, train_loss: 3.8124
2022-10-12 02:51:19 - eval: epoch: 079, acc1: 69.560%, acc5: 89.646%, test_loss: 1.6419, per_image_load_time: 0.255ms, per_image_inference_time: 0.533ms
2022-10-12 02:51:20 - until epoch: 079, best_acc1: 70.300%
2022-10-12 02:51:20 - epoch 080 lr: 0.010501
2022-10-12 02:52:02 - train: epoch 0080, iter [00100, 05004], lr: 0.010482, loss: 4.0232
2022-10-12 02:52:38 - train: epoch 0080, iter [00200, 05004], lr: 0.010463, loss: 3.7714
2022-10-12 02:53:14 - train: epoch 0080, iter [00300, 05004], lr: 0.010444, loss: 4.5486
2022-10-12 02:53:50 - train: epoch 0080, iter [00400, 05004], lr: 0.010424, loss: 3.6525
2022-10-12 02:54:28 - train: epoch 0080, iter [00500, 05004], lr: 0.010405, loss: 3.3456
2022-10-12 02:55:04 - train: epoch 0080, iter [00600, 05004], lr: 0.010386, loss: 3.8219
2022-10-12 02:55:40 - train: epoch 0080, iter [00700, 05004], lr: 0.010367, loss: 3.6523
2022-10-12 02:56:17 - train: epoch 0080, iter [00800, 05004], lr: 0.010348, loss: 4.0789
2022-10-12 02:56:53 - train: epoch 0080, iter [00900, 05004], lr: 0.010329, loss: 3.9923
2022-10-12 02:57:30 - train: epoch 0080, iter [01000, 05004], lr: 0.010310, loss: 3.9456
2022-10-12 02:58:07 - train: epoch 0080, iter [01100, 05004], lr: 0.010291, loss: 4.1083
2022-10-12 02:58:43 - train: epoch 0080, iter [01200, 05004], lr: 0.010271, loss: 4.3489
2022-10-12 02:59:18 - train: epoch 0080, iter [01300, 05004], lr: 0.010252, loss: 3.8788
2022-10-12 02:59:56 - train: epoch 0080, iter [01400, 05004], lr: 0.010233, loss: 3.3023
2022-10-12 03:00:31 - train: epoch 0080, iter [01500, 05004], lr: 0.010214, loss: 4.0547
2022-10-12 03:01:08 - train: epoch 0080, iter [01600, 05004], lr: 0.010195, loss: 3.9744
2022-10-12 03:01:44 - train: epoch 0080, iter [01700, 05004], lr: 0.010176, loss: 3.3724
2022-10-12 03:02:20 - train: epoch 0080, iter [01800, 05004], lr: 0.010157, loss: 3.3928
2022-10-12 03:02:58 - train: epoch 0080, iter [01900, 05004], lr: 0.010139, loss: 4.0939
2022-10-12 03:03:34 - train: epoch 0080, iter [02000, 05004], lr: 0.010120, loss: 4.2307
2022-10-12 03:04:10 - train: epoch 0080, iter [02100, 05004], lr: 0.010101, loss: 4.3499
2022-10-12 03:04:47 - train: epoch 0080, iter [02200, 05004], lr: 0.010082, loss: 4.2708
2022-10-12 03:05:24 - train: epoch 0080, iter [02300, 05004], lr: 0.010063, loss: 3.6001
2022-10-12 03:06:00 - train: epoch 0080, iter [02400, 05004], lr: 0.010044, loss: 4.2249
2022-10-12 03:06:37 - train: epoch 0080, iter [02500, 05004], lr: 0.010025, loss: 4.2481
2022-10-12 03:07:14 - train: epoch 0080, iter [02600, 05004], lr: 0.010006, loss: 4.2692
2022-10-12 03:07:51 - train: epoch 0080, iter [02700, 05004], lr: 0.009987, loss: 3.6070
2022-10-12 03:08:28 - train: epoch 0080, iter [02800, 05004], lr: 0.009969, loss: 3.3359
2022-10-12 03:09:05 - train: epoch 0080, iter [02900, 05004], lr: 0.009950, loss: 3.7160
2022-10-12 03:09:41 - train: epoch 0080, iter [03000, 05004], lr: 0.009931, loss: 3.6412
2022-10-12 03:10:18 - train: epoch 0080, iter [03100, 05004], lr: 0.009912, loss: 4.2660
2022-10-12 03:10:54 - train: epoch 0080, iter [03200, 05004], lr: 0.009894, loss: 3.3364
2022-10-12 03:11:31 - train: epoch 0080, iter [03300, 05004], lr: 0.009875, loss: 4.1551
2022-10-12 03:12:08 - train: epoch 0080, iter [03400, 05004], lr: 0.009856, loss: 4.2731
2022-10-12 03:12:45 - train: epoch 0080, iter [03500, 05004], lr: 0.009837, loss: 3.8155
2022-10-12 03:13:21 - train: epoch 0080, iter [03600, 05004], lr: 0.009819, loss: 4.2115
2022-10-12 03:13:57 - train: epoch 0080, iter [03700, 05004], lr: 0.009800, loss: 3.8762
2022-10-12 03:14:34 - train: epoch 0080, iter [03800, 05004], lr: 0.009781, loss: 3.9726
2022-10-12 03:15:11 - train: epoch 0080, iter [03900, 05004], lr: 0.009763, loss: 3.2199
2022-10-12 03:15:48 - train: epoch 0080, iter [04000, 05004], lr: 0.009744, loss: 3.8257
2022-10-12 03:16:24 - train: epoch 0080, iter [04100, 05004], lr: 0.009726, loss: 4.0281
2022-10-12 03:17:00 - train: epoch 0080, iter [04200, 05004], lr: 0.009707, loss: 3.8325
2022-10-12 03:17:37 - train: epoch 0080, iter [04300, 05004], lr: 0.009688, loss: 3.7411
2022-10-12 03:18:13 - train: epoch 0080, iter [04400, 05004], lr: 0.009670, loss: 3.9861
2022-10-12 03:18:49 - train: epoch 0080, iter [04500, 05004], lr: 0.009651, loss: 3.5551
2022-10-12 03:19:26 - train: epoch 0080, iter [04600, 05004], lr: 0.009633, loss: 3.5767
2022-10-12 03:20:03 - train: epoch 0080, iter [04700, 05004], lr: 0.009614, loss: 3.3655
2022-10-12 03:20:39 - train: epoch 0080, iter [04800, 05004], lr: 0.009596, loss: 3.4854
2022-10-12 03:21:16 - train: epoch 0080, iter [04900, 05004], lr: 0.009577, loss: 4.5024
2022-10-12 03:21:50 - train: epoch 0080, iter [05000, 05004], lr: 0.009559, loss: 4.4309
2022-10-12 03:21:52 - train: epoch 080, train_loss: 3.7979
2022-10-12 03:23:11 - eval: epoch: 080, acc1: 71.534%, acc5: 90.784%, test_loss: 1.4240, per_image_load_time: 0.246ms, per_image_inference_time: 0.511ms
2022-10-12 03:23:12 - until epoch: 080, best_acc1: 71.534%
2022-10-12 03:23:12 - epoch 081 lr: 0.009558
2022-10-12 03:23:53 - train: epoch 0081, iter [00100, 05004], lr: 0.009540, loss: 3.5134
2022-10-12 03:24:30 - train: epoch 0081, iter [00200, 05004], lr: 0.009521, loss: 3.3150
2022-10-12 03:25:07 - train: epoch 0081, iter [00300, 05004], lr: 0.009503, loss: 3.5312
2022-10-12 03:25:43 - train: epoch 0081, iter [00400, 05004], lr: 0.009485, loss: 4.2920
2022-10-12 03:26:20 - train: epoch 0081, iter [00500, 05004], lr: 0.009466, loss: 4.3569
2022-10-12 03:26:56 - train: epoch 0081, iter [00600, 05004], lr: 0.009448, loss: 3.5403
2022-10-12 03:27:33 - train: epoch 0081, iter [00700, 05004], lr: 0.009429, loss: 3.6403
2022-10-12 03:28:10 - train: epoch 0081, iter [00800, 05004], lr: 0.009411, loss: 4.5189
2022-10-12 03:28:47 - train: epoch 0081, iter [00900, 05004], lr: 0.009393, loss: 3.2764
2022-10-12 03:29:23 - train: epoch 0081, iter [01000, 05004], lr: 0.009375, loss: 4.2316
2022-10-12 03:29:59 - train: epoch 0081, iter [01100, 05004], lr: 0.009356, loss: 4.4009
2022-10-12 03:30:37 - train: epoch 0081, iter [01200, 05004], lr: 0.009338, loss: 2.8860
2022-10-12 03:31:14 - train: epoch 0081, iter [01300, 05004], lr: 0.009320, loss: 3.7356
2022-10-12 03:31:50 - train: epoch 0081, iter [01400, 05004], lr: 0.009301, loss: 4.1986
2022-10-12 03:32:27 - train: epoch 0081, iter [01500, 05004], lr: 0.009283, loss: 3.5099
2022-10-12 03:33:03 - train: epoch 0081, iter [01600, 05004], lr: 0.009265, loss: 3.4050
2022-10-12 03:33:41 - train: epoch 0081, iter [01700, 05004], lr: 0.009247, loss: 3.2548
2022-10-12 03:34:18 - train: epoch 0081, iter [01800, 05004], lr: 0.009229, loss: 3.4752
2022-10-12 03:34:54 - train: epoch 0081, iter [01900, 05004], lr: 0.009211, loss: 4.0054
2022-10-12 03:35:31 - train: epoch 0081, iter [02000, 05004], lr: 0.009192, loss: 3.5638
2022-10-12 03:36:09 - train: epoch 0081, iter [02100, 05004], lr: 0.009174, loss: 3.7803
2022-10-12 03:36:46 - train: epoch 0081, iter [02200, 05004], lr: 0.009156, loss: 4.3538
2022-10-12 03:37:22 - train: epoch 0081, iter [02300, 05004], lr: 0.009138, loss: 2.9184
2022-10-12 03:37:59 - train: epoch 0081, iter [02400, 05004], lr: 0.009120, loss: 3.8056
2022-10-12 03:38:36 - train: epoch 0081, iter [02500, 05004], lr: 0.009102, loss: 3.0685
2022-10-12 03:39:13 - train: epoch 0081, iter [02600, 05004], lr: 0.009084, loss: 4.7366
2022-10-12 03:39:50 - train: epoch 0081, iter [02700, 05004], lr: 0.009066, loss: 4.6063
2022-10-12 03:40:27 - train: epoch 0081, iter [02800, 05004], lr: 0.009048, loss: 3.7454
2022-10-12 03:41:04 - train: epoch 0081, iter [02900, 05004], lr: 0.009030, loss: 4.0659
2022-10-12 03:41:42 - train: epoch 0081, iter [03000, 05004], lr: 0.009012, loss: 3.6709
2022-10-12 03:42:18 - train: epoch 0081, iter [03100, 05004], lr: 0.008994, loss: 3.7674
2022-10-12 03:42:54 - train: epoch 0081, iter [03200, 05004], lr: 0.008976, loss: 3.6490
2022-10-12 03:43:32 - train: epoch 0081, iter [03300, 05004], lr: 0.008958, loss: 3.4689
2022-10-12 03:44:08 - train: epoch 0081, iter [03400, 05004], lr: 0.008940, loss: 4.2806
2022-10-12 03:44:45 - train: epoch 0081, iter [03500, 05004], lr: 0.008922, loss: 3.7079
2022-10-12 03:45:23 - train: epoch 0081, iter [03600, 05004], lr: 0.008904, loss: 4.0745
2022-10-12 03:45:58 - train: epoch 0081, iter [03700, 05004], lr: 0.008887, loss: 4.0164
2022-10-12 03:46:36 - train: epoch 0081, iter [03800, 05004], lr: 0.008869, loss: 3.7373
2022-10-12 03:47:13 - train: epoch 0081, iter [03900, 05004], lr: 0.008851, loss: 3.8225
2022-10-12 03:47:49 - train: epoch 0081, iter [04000, 05004], lr: 0.008833, loss: 3.8291
2022-10-12 03:48:27 - train: epoch 0081, iter [04100, 05004], lr: 0.008815, loss: 3.9558
2022-10-12 03:49:04 - train: epoch 0081, iter [04200, 05004], lr: 0.008797, loss: 3.4137
2022-10-12 03:49:40 - train: epoch 0081, iter [04300, 05004], lr: 0.008780, loss: 4.4829
2022-10-12 03:50:17 - train: epoch 0081, iter [04400, 05004], lr: 0.008762, loss: 4.7236
2022-10-12 03:50:54 - train: epoch 0081, iter [04500, 05004], lr: 0.008744, loss: 3.3807
2022-10-12 03:51:30 - train: epoch 0081, iter [04600, 05004], lr: 0.008727, loss: 3.4948
2022-10-12 03:52:07 - train: epoch 0081, iter [04700, 05004], lr: 0.008709, loss: 3.5421
2022-10-12 03:52:44 - train: epoch 0081, iter [04800, 05004], lr: 0.008691, loss: 3.3695
2022-10-12 03:53:20 - train: epoch 0081, iter [04900, 05004], lr: 0.008673, loss: 3.5591
2022-10-12 03:53:55 - train: epoch 0081, iter [05000, 05004], lr: 0.008656, loss: 3.6004
2022-10-12 03:53:57 - train: epoch 081, train_loss: 3.7778
2022-10-12 03:55:15 - eval: epoch: 081, acc1: 71.396%, acc5: 90.656%, test_loss: 1.4143, per_image_load_time: 0.264ms, per_image_inference_time: 0.518ms
2022-10-12 03:55:15 - until epoch: 081, best_acc1: 71.534%
2022-10-12 03:55:15 - epoch 082 lr: 0.008655
2022-10-12 03:55:58 - train: epoch 0082, iter [00100, 05004], lr: 0.008637, loss: 4.1724
2022-10-12 03:56:35 - train: epoch 0082, iter [00200, 05004], lr: 0.008620, loss: 2.8040
2022-10-12 03:57:11 - train: epoch 0082, iter [00300, 05004], lr: 0.008602, loss: 3.2375
2022-10-12 03:57:48 - train: epoch 0082, iter [00400, 05004], lr: 0.008585, loss: 4.1009
2022-10-12 03:58:24 - train: epoch 0082, iter [00500, 05004], lr: 0.008567, loss: 3.6981
2022-10-12 03:59:00 - train: epoch 0082, iter [00600, 05004], lr: 0.008550, loss: 3.3622
2022-10-12 03:59:37 - train: epoch 0082, iter [00700, 05004], lr: 0.008532, loss: 4.3931
2022-10-12 04:00:13 - train: epoch 0082, iter [00800, 05004], lr: 0.008514, loss: 4.5885
2022-10-12 04:00:49 - train: epoch 0082, iter [00900, 05004], lr: 0.008497, loss: 3.0281
2022-10-12 04:01:26 - train: epoch 0082, iter [01000, 05004], lr: 0.008479, loss: 3.8739
2022-10-12 04:02:03 - train: epoch 0082, iter [01100, 05004], lr: 0.008462, loss: 3.9575
2022-10-12 04:02:40 - train: epoch 0082, iter [01200, 05004], lr: 0.008445, loss: 3.5966
2022-10-12 04:03:16 - train: epoch 0082, iter [01300, 05004], lr: 0.008427, loss: 3.8571
2022-10-12 04:03:53 - train: epoch 0082, iter [01400, 05004], lr: 0.008410, loss: 3.9603
2022-10-12 04:04:30 - train: epoch 0082, iter [01500, 05004], lr: 0.008392, loss: 2.9487
2022-10-12 04:05:06 - train: epoch 0082, iter [01600, 05004], lr: 0.008375, loss: 3.8589
2022-10-12 04:05:44 - train: epoch 0082, iter [01700, 05004], lr: 0.008358, loss: 3.0791
2022-10-12 04:06:20 - train: epoch 0082, iter [01800, 05004], lr: 0.008340, loss: 3.9785
2022-10-12 04:06:57 - train: epoch 0082, iter [01900, 05004], lr: 0.008323, loss: 4.5163
2022-10-12 04:07:34 - train: epoch 0082, iter [02000, 05004], lr: 0.008306, loss: 3.7748
2022-10-12 04:08:11 - train: epoch 0082, iter [02100, 05004], lr: 0.008288, loss: 3.6012
2022-10-12 04:08:47 - train: epoch 0082, iter [02200, 05004], lr: 0.008271, loss: 3.0005
2022-10-12 04:09:24 - train: epoch 0082, iter [02300, 05004], lr: 0.008254, loss: 3.0036
2022-10-12 04:10:01 - train: epoch 0082, iter [02400, 05004], lr: 0.008236, loss: 3.5163
2022-10-12 04:10:37 - train: epoch 0082, iter [02500, 05004], lr: 0.008219, loss: 3.2972
2022-10-12 04:11:14 - train: epoch 0082, iter [02600, 05004], lr: 0.008202, loss: 3.7008
2022-10-12 04:11:51 - train: epoch 0082, iter [02700, 05004], lr: 0.008185, loss: 3.4769
2022-10-12 04:12:27 - train: epoch 0082, iter [02800, 05004], lr: 0.008168, loss: 3.8005
2022-10-12 04:13:04 - train: epoch 0082, iter [02900, 05004], lr: 0.008150, loss: 3.2208
2022-10-12 04:13:41 - train: epoch 0082, iter [03000, 05004], lr: 0.008133, loss: 3.6555
2022-10-12 04:14:18 - train: epoch 0082, iter [03100, 05004], lr: 0.008116, loss: 3.8379
2022-10-12 04:14:54 - train: epoch 0082, iter [03200, 05004], lr: 0.008099, loss: 3.4641
2022-10-12 04:15:31 - train: epoch 0082, iter [03300, 05004], lr: 0.008082, loss: 3.8403
2022-10-12 04:16:08 - train: epoch 0082, iter [03400, 05004], lr: 0.008065, loss: 3.2653
2022-10-12 04:16:45 - train: epoch 0082, iter [03500, 05004], lr: 0.008048, loss: 4.0666
2022-10-12 04:17:21 - train: epoch 0082, iter [03600, 05004], lr: 0.008031, loss: 3.5713
2022-10-12 04:17:58 - train: epoch 0082, iter [03700, 05004], lr: 0.008014, loss: 3.2427
2022-10-12 04:18:34 - train: epoch 0082, iter [03800, 05004], lr: 0.007997, loss: 4.2944
2022-10-12 04:19:11 - train: epoch 0082, iter [03900, 05004], lr: 0.007980, loss: 3.5246
2022-10-12 04:19:48 - train: epoch 0082, iter [04000, 05004], lr: 0.007963, loss: 3.3705
2022-10-12 04:20:24 - train: epoch 0082, iter [04100, 05004], lr: 0.007946, loss: 3.4471
2022-10-12 04:21:01 - train: epoch 0082, iter [04200, 05004], lr: 0.007929, loss: 3.8186
2022-10-12 04:21:38 - train: epoch 0082, iter [04300, 05004], lr: 0.007912, loss: 3.5799
2022-10-12 04:22:14 - train: epoch 0082, iter [04400, 05004], lr: 0.007895, loss: 3.9367
2022-10-12 04:22:52 - train: epoch 0082, iter [04500, 05004], lr: 0.007878, loss: 3.0483
2022-10-12 04:23:28 - train: epoch 0082, iter [04600, 05004], lr: 0.007861, loss: 4.1847
2022-10-12 04:24:05 - train: epoch 0082, iter [04700, 05004], lr: 0.007844, loss: 3.2936
2022-10-12 04:24:41 - train: epoch 0082, iter [04800, 05004], lr: 0.007827, loss: 4.0135
2022-10-12 04:25:19 - train: epoch 0082, iter [04900, 05004], lr: 0.007810, loss: 3.6312
2022-10-12 04:25:54 - train: epoch 0082, iter [05000, 05004], lr: 0.007793, loss: 4.0693
2022-10-12 04:25:56 - train: epoch 082, train_loss: 3.7535
2022-10-12 04:27:15 - eval: epoch: 082, acc1: 71.964%, acc5: 90.776%, test_loss: 1.3959, per_image_load_time: 0.263ms, per_image_inference_time: 0.526ms
2022-10-12 04:27:15 - until epoch: 082, best_acc1: 71.964%
2022-10-12 04:27:15 - epoch 083 lr: 0.007793
2022-10-12 04:27:57 - train: epoch 0083, iter [00100, 05004], lr: 0.007776, loss: 3.8058
2022-10-12 04:28:34 - train: epoch 0083, iter [00200, 05004], lr: 0.007759, loss: 4.2272
2022-10-12 04:29:11 - train: epoch 0083, iter [00300, 05004], lr: 0.007742, loss: 3.8420
2022-10-12 04:29:48 - train: epoch 0083, iter [00400, 05004], lr: 0.007726, loss: 3.7941
2022-10-12 04:30:24 - train: epoch 0083, iter [00500, 05004], lr: 0.007709, loss: 4.0459
2022-10-12 04:31:01 - train: epoch 0083, iter [00600, 05004], lr: 0.007692, loss: 4.0087
2022-10-12 04:31:37 - train: epoch 0083, iter [00700, 05004], lr: 0.007676, loss: 4.3435
2022-10-12 04:32:13 - train: epoch 0083, iter [00800, 05004], lr: 0.007659, loss: 3.6895
2022-10-12 04:32:49 - train: epoch 0083, iter [00900, 05004], lr: 0.007642, loss: 2.6219
2022-10-12 04:33:25 - train: epoch 0083, iter [01000, 05004], lr: 0.007625, loss: 3.1582
2022-10-12 04:34:03 - train: epoch 0083, iter [01100, 05004], lr: 0.007609, loss: 3.7472
2022-10-12 04:34:40 - train: epoch 0083, iter [01200, 05004], lr: 0.007592, loss: 4.1539
2022-10-12 04:35:16 - train: epoch 0083, iter [01300, 05004], lr: 0.007576, loss: 3.9801
2022-10-12 04:35:53 - train: epoch 0083, iter [01400, 05004], lr: 0.007559, loss: 3.5522
2022-10-12 04:36:31 - train: epoch 0083, iter [01500, 05004], lr: 0.007542, loss: 3.7714
2022-10-12 04:37:08 - train: epoch 0083, iter [01600, 05004], lr: 0.007526, loss: 3.5144
2022-10-12 04:37:44 - train: epoch 0083, iter [01700, 05004], lr: 0.007509, loss: 3.8948
2022-10-12 04:38:20 - train: epoch 0083, iter [01800, 05004], lr: 0.007493, loss: 3.4434
2022-10-12 04:38:57 - train: epoch 0083, iter [01900, 05004], lr: 0.007476, loss: 4.2455
2022-10-12 04:39:33 - train: epoch 0083, iter [02000, 05004], lr: 0.007460, loss: 3.8863
2022-10-12 04:40:09 - train: epoch 0083, iter [02100, 05004], lr: 0.007443, loss: 3.3682
2022-10-12 04:40:47 - train: epoch 0083, iter [02200, 05004], lr: 0.007427, loss: 3.5148
2022-10-12 04:41:25 - train: epoch 0083, iter [02300, 05004], lr: 0.007410, loss: 3.5217
2022-10-12 04:42:03 - train: epoch 0083, iter [02400, 05004], lr: 0.007394, loss: 4.3610
2022-10-12 04:42:40 - train: epoch 0083, iter [02500, 05004], lr: 0.007378, loss: 3.0514
2022-10-12 04:43:17 - train: epoch 0083, iter [02600, 05004], lr: 0.007361, loss: 4.2686
2022-10-12 04:43:54 - train: epoch 0083, iter [02700, 05004], lr: 0.007345, loss: 2.8800
2022-10-12 04:44:31 - train: epoch 0083, iter [02800, 05004], lr: 0.007328, loss: 2.8524
2022-10-12 04:45:08 - train: epoch 0083, iter [02900, 05004], lr: 0.007312, loss: 4.1305
2022-10-12 04:45:45 - train: epoch 0083, iter [03000, 05004], lr: 0.007296, loss: 3.7883
2022-10-12 04:46:21 - train: epoch 0083, iter [03100, 05004], lr: 0.007279, loss: 3.6900
2022-10-12 04:46:58 - train: epoch 0083, iter [03200, 05004], lr: 0.007263, loss: 4.0720
2022-10-12 04:47:35 - train: epoch 0083, iter [03300, 05004], lr: 0.007247, loss: 3.5065
2022-10-12 04:48:12 - train: epoch 0083, iter [03400, 05004], lr: 0.007231, loss: 3.5977
2022-10-12 04:48:48 - train: epoch 0083, iter [03500, 05004], lr: 0.007214, loss: 3.8726
2022-10-12 04:49:26 - train: epoch 0083, iter [03600, 05004], lr: 0.007198, loss: 4.3803
2022-10-12 04:50:02 - train: epoch 0083, iter [03700, 05004], lr: 0.007182, loss: 3.7110
2022-10-12 04:50:40 - train: epoch 0083, iter [03800, 05004], lr: 0.007166, loss: 3.8703
2022-10-12 04:51:17 - train: epoch 0083, iter [03900, 05004], lr: 0.007150, loss: 3.6537
2022-10-12 04:51:54 - train: epoch 0083, iter [04000, 05004], lr: 0.007133, loss: 3.8636
2022-10-12 04:52:32 - train: epoch 0083, iter [04100, 05004], lr: 0.007117, loss: 3.6117
2022-10-12 04:53:08 - train: epoch 0083, iter [04200, 05004], lr: 0.007101, loss: 3.6220
2022-10-12 04:53:45 - train: epoch 0083, iter [04300, 05004], lr: 0.007085, loss: 3.5558
2022-10-12 04:54:22 - train: epoch 0083, iter [04400, 05004], lr: 0.007069, loss: 3.2786
2022-10-12 04:54:58 - train: epoch 0083, iter [04500, 05004], lr: 0.007053, loss: 3.8680
2022-10-12 04:55:36 - train: epoch 0083, iter [04600, 05004], lr: 0.007037, loss: 3.6247
2022-10-12 04:56:12 - train: epoch 0083, iter [04700, 05004], lr: 0.007021, loss: 4.3408
2022-10-12 04:56:50 - train: epoch 0083, iter [04800, 05004], lr: 0.007005, loss: 4.0744
2022-10-12 04:57:27 - train: epoch 0083, iter [04900, 05004], lr: 0.006989, loss: 4.2601
2022-10-12 04:58:02 - train: epoch 0083, iter [05000, 05004], lr: 0.006973, loss: 3.9871
2022-10-12 04:58:04 - train: epoch 083, train_loss: 3.7336
2022-10-12 04:59:27 - eval: epoch: 083, acc1: 71.670%, acc5: 90.912%, test_loss: 1.4434, per_image_load_time: 1.307ms, per_image_inference_time: 0.548ms
2022-10-12 04:59:27 - until epoch: 083, best_acc1: 71.964%
2022-10-12 04:59:27 - epoch 084 lr: 0.006972
2022-10-12 05:00:11 - train: epoch 0084, iter [00100, 05004], lr: 0.006956, loss: 3.9333
2022-10-12 05:00:51 - train: epoch 0084, iter [00200, 05004], lr: 0.006940, loss: 3.7254
2022-10-12 05:01:27 - train: epoch 0084, iter [00300, 05004], lr: 0.006924, loss: 3.1778
2022-10-12 05:02:03 - train: epoch 0084, iter [00400, 05004], lr: 0.006908, loss: 3.2699
2022-10-12 05:02:40 - train: epoch 0084, iter [00500, 05004], lr: 0.006893, loss: 3.7297
2022-10-12 05:03:17 - train: epoch 0084, iter [00600, 05004], lr: 0.006877, loss: 3.9343
2022-10-12 05:03:53 - train: epoch 0084, iter [00700, 05004], lr: 0.006861, loss: 3.6813
2022-10-12 05:04:31 - train: epoch 0084, iter [00800, 05004], lr: 0.006845, loss: 4.1828
2022-10-12 05:05:09 - train: epoch 0084, iter [00900, 05004], lr: 0.006829, loss: 4.2638
2022-10-12 05:05:45 - train: epoch 0084, iter [01000, 05004], lr: 0.006813, loss: 3.5615
2022-10-12 05:06:21 - train: epoch 0084, iter [01100, 05004], lr: 0.006797, loss: 4.1412
2022-10-12 05:06:58 - train: epoch 0084, iter [01200, 05004], lr: 0.006782, loss: 3.4878
2022-10-12 05:07:35 - train: epoch 0084, iter [01300, 05004], lr: 0.006766, loss: 3.7824
2022-10-12 05:08:12 - train: epoch 0084, iter [01400, 05004], lr: 0.006750, loss: 4.0265
2022-10-12 05:08:49 - train: epoch 0084, iter [01500, 05004], lr: 0.006734, loss: 3.5777
2022-10-12 05:09:26 - train: epoch 0084, iter [01600, 05004], lr: 0.006719, loss: 4.3321
2022-10-12 05:10:04 - train: epoch 0084, iter [01700, 05004], lr: 0.006703, loss: 3.8920
2022-10-12 05:10:41 - train: epoch 0084, iter [01800, 05004], lr: 0.006687, loss: 3.8990
2022-10-12 05:11:18 - train: epoch 0084, iter [01900, 05004], lr: 0.006672, loss: 3.5061
2022-10-12 05:11:56 - train: epoch 0084, iter [02000, 05004], lr: 0.006656, loss: 3.1010
2022-10-12 05:12:32 - train: epoch 0084, iter [02100, 05004], lr: 0.006640, loss: 3.6742
2022-10-12 05:13:10 - train: epoch 0084, iter [02200, 05004], lr: 0.006625, loss: 3.0148
2022-10-12 05:13:47 - train: epoch 0084, iter [02300, 05004], lr: 0.006609, loss: 3.4251
2022-10-12 05:14:25 - train: epoch 0084, iter [02400, 05004], lr: 0.006594, loss: 3.7533
2022-10-12 05:15:02 - train: epoch 0084, iter [02500, 05004], lr: 0.006578, loss: 4.3014
2022-10-12 05:15:39 - train: epoch 0084, iter [02600, 05004], lr: 0.006563, loss: 3.5005
2022-10-12 05:16:16 - train: epoch 0084, iter [02700, 05004], lr: 0.006547, loss: 4.2137
2022-10-12 05:16:54 - train: epoch 0084, iter [02800, 05004], lr: 0.006532, loss: 4.6150
2022-10-12 05:17:31 - train: epoch 0084, iter [02900, 05004], lr: 0.006516, loss: 3.2189
2022-10-12 05:18:09 - train: epoch 0084, iter [03000, 05004], lr: 0.006501, loss: 4.0982
2022-10-12 05:18:47 - train: epoch 0084, iter [03100, 05004], lr: 0.006485, loss: 3.3150
2022-10-12 05:19:24 - train: epoch 0084, iter [03200, 05004], lr: 0.006470, loss: 3.6722
2022-10-12 05:20:02 - train: epoch 0084, iter [03300, 05004], lr: 0.006454, loss: 3.8905
2022-10-12 05:20:39 - train: epoch 0084, iter [03400, 05004], lr: 0.006439, loss: 3.2928
2022-10-12 05:21:16 - train: epoch 0084, iter [03500, 05004], lr: 0.006423, loss: 3.5710
2022-10-12 05:21:53 - train: epoch 0084, iter [03600, 05004], lr: 0.006408, loss: 4.2890
2022-10-12 05:22:31 - train: epoch 0084, iter [03700, 05004], lr: 0.006393, loss: 3.1524
2022-10-12 05:23:08 - train: epoch 0084, iter [03800, 05004], lr: 0.006377, loss: 3.3406
2022-10-12 05:23:46 - train: epoch 0084, iter [03900, 05004], lr: 0.006362, loss: 3.9959
2022-10-12 05:24:23 - train: epoch 0084, iter [04000, 05004], lr: 0.006347, loss: 2.5554
2022-10-12 05:24:59 - train: epoch 0084, iter [04100, 05004], lr: 0.006331, loss: 4.1358
2022-10-12 05:25:37 - train: epoch 0084, iter [04200, 05004], lr: 0.006316, loss: 3.1545
2022-10-12 05:26:14 - train: epoch 0084, iter [04300, 05004], lr: 0.006301, loss: 3.6273
2022-10-12 05:26:51 - train: epoch 0084, iter [04400, 05004], lr: 0.006286, loss: 3.8650
2022-10-12 05:27:30 - train: epoch 0084, iter [04500, 05004], lr: 0.006270, loss: 3.6779
2022-10-12 05:28:07 - train: epoch 0084, iter [04600, 05004], lr: 0.006255, loss: 3.7684
2022-10-12 05:28:44 - train: epoch 0084, iter [04700, 05004], lr: 0.006240, loss: 2.9307
2022-10-12 05:29:20 - train: epoch 0084, iter [04800, 05004], lr: 0.006225, loss: 4.0774
2022-10-12 05:29:58 - train: epoch 0084, iter [04900, 05004], lr: 0.006210, loss: 2.5119
2022-10-12 05:30:32 - train: epoch 0084, iter [05000, 05004], lr: 0.006195, loss: 3.3694
2022-10-12 05:30:35 - train: epoch 084, train_loss: 3.7108
2022-10-12 05:31:55 - eval: epoch: 084, acc1: 72.490%, acc5: 91.360%, test_loss: 1.3622, per_image_load_time: 1.214ms, per_image_inference_time: 0.557ms
2022-10-12 05:31:56 - until epoch: 084, best_acc1: 72.490%
2022-10-12 05:31:56 - epoch 085 lr: 0.006194
2022-10-12 05:32:40 - train: epoch 0085, iter [00100, 05004], lr: 0.006179, loss: 3.0111
2022-10-12 05:33:19 - train: epoch 0085, iter [00200, 05004], lr: 0.006164, loss: 3.8787
2022-10-12 05:33:55 - train: epoch 0085, iter [00300, 05004], lr: 0.006149, loss: 3.5058
2022-10-12 05:34:32 - train: epoch 0085, iter [00400, 05004], lr: 0.006134, loss: 3.1345
2022-10-12 05:35:09 - train: epoch 0085, iter [00500, 05004], lr: 0.006119, loss: 3.3253
2022-10-12 05:35:45 - train: epoch 0085, iter [00600, 05004], lr: 0.006104, loss: 3.6820
2022-10-12 05:36:23 - train: epoch 0085, iter [00700, 05004], lr: 0.006089, loss: 4.1099
2022-10-12 05:36:59 - train: epoch 0085, iter [00800, 05004], lr: 0.006074, loss: 3.1134
2022-10-12 05:37:36 - train: epoch 0085, iter [00900, 05004], lr: 0.006059, loss: 3.4807
2022-10-12 05:38:13 - train: epoch 0085, iter [01000, 05004], lr: 0.006044, loss: 3.8850
2022-10-12 05:38:50 - train: epoch 0085, iter [01100, 05004], lr: 0.006029, loss: 4.3417
2022-10-12 05:39:27 - train: epoch 0085, iter [01200, 05004], lr: 0.006014, loss: 3.4633
2022-10-12 05:40:04 - train: epoch 0085, iter [01300, 05004], lr: 0.005999, loss: 4.1234
2022-10-12 05:40:40 - train: epoch 0085, iter [01400, 05004], lr: 0.005984, loss: 4.5873
2022-10-12 05:41:17 - train: epoch 0085, iter [01500, 05004], lr: 0.005969, loss: 3.2068
2022-10-12 05:41:54 - train: epoch 0085, iter [01600, 05004], lr: 0.005954, loss: 3.6115
2022-10-12 05:42:31 - train: epoch 0085, iter [01700, 05004], lr: 0.005939, loss: 3.8069
2022-10-12 05:43:08 - train: epoch 0085, iter [01800, 05004], lr: 0.005925, loss: 3.2765
2022-10-12 05:43:45 - train: epoch 0085, iter [01900, 05004], lr: 0.005910, loss: 3.7448
2022-10-12 05:44:22 - train: epoch 0085, iter [02000, 05004], lr: 0.005895, loss: 4.0232
2022-10-12 05:44:59 - train: epoch 0085, iter [02100, 05004], lr: 0.005880, loss: 3.4096
2022-10-12 05:45:36 - train: epoch 0085, iter [02200, 05004], lr: 0.005866, loss: 3.8896
2022-10-12 05:46:13 - train: epoch 0085, iter [02300, 05004], lr: 0.005851, loss: 3.7269
2022-10-12 05:46:51 - train: epoch 0085, iter [02400, 05004], lr: 0.005836, loss: 3.3568
2022-10-12 05:47:27 - train: epoch 0085, iter [02500, 05004], lr: 0.005821, loss: 3.6565
2022-10-12 05:48:04 - train: epoch 0085, iter [02600, 05004], lr: 0.005807, loss: 3.6894
2022-10-12 05:48:41 - train: epoch 0085, iter [02700, 05004], lr: 0.005792, loss: 3.6099
2022-10-12 05:49:18 - train: epoch 0085, iter [02800, 05004], lr: 0.005777, loss: 4.0244
2022-10-12 05:49:55 - train: epoch 0085, iter [02900, 05004], lr: 0.005763, loss: 3.6746
2022-10-12 05:50:33 - train: epoch 0085, iter [03000, 05004], lr: 0.005748, loss: 3.8845
2022-10-12 05:51:10 - train: epoch 0085, iter [03100, 05004], lr: 0.005734, loss: 3.3120
2022-10-12 05:51:47 - train: epoch 0085, iter [03200, 05004], lr: 0.005719, loss: 3.5925
2022-10-12 05:52:24 - train: epoch 0085, iter [03300, 05004], lr: 0.005704, loss: 3.7943
2022-10-12 05:53:01 - train: epoch 0085, iter [03400, 05004], lr: 0.005690, loss: 3.8104
2022-10-12 05:53:38 - train: epoch 0085, iter [03500, 05004], lr: 0.005675, loss: 3.6792
2022-10-12 05:54:15 - train: epoch 0085, iter [03600, 05004], lr: 0.005661, loss: 3.1426
2022-10-12 05:54:52 - train: epoch 0085, iter [03700, 05004], lr: 0.005646, loss: 3.6682
2022-10-12 05:55:29 - train: epoch 0085, iter [03800, 05004], lr: 0.005632, loss: 3.4584
2022-10-12 05:56:06 - train: epoch 0085, iter [03900, 05004], lr: 0.005618, loss: 3.7240
2022-10-12 05:56:44 - train: epoch 0085, iter [04000, 05004], lr: 0.005603, loss: 3.8831
2022-10-12 05:57:20 - train: epoch 0085, iter [04100, 05004], lr: 0.005589, loss: 3.9437
2022-10-12 05:57:57 - train: epoch 0085, iter [04200, 05004], lr: 0.005574, loss: 3.2222
2022-10-12 05:58:34 - train: epoch 0085, iter [04300, 05004], lr: 0.005560, loss: 4.0425
2022-10-12 05:59:11 - train: epoch 0085, iter [04400, 05004], lr: 0.005546, loss: 3.5993
2022-10-12 05:59:49 - train: epoch 0085, iter [04500, 05004], lr: 0.005531, loss: 4.1817
2022-10-12 06:00:26 - train: epoch 0085, iter [04600, 05004], lr: 0.005517, loss: 3.9297
2022-10-12 06:01:04 - train: epoch 0085, iter [04700, 05004], lr: 0.005503, loss: 3.3625
2022-10-12 06:01:40 - train: epoch 0085, iter [04800, 05004], lr: 0.005488, loss: 3.4524
2022-10-12 06:02:18 - train: epoch 0085, iter [04900, 05004], lr: 0.005474, loss: 3.6384
2022-10-12 06:02:53 - train: epoch 0085, iter [05000, 05004], lr: 0.005460, loss: 3.6478
2022-10-12 06:02:55 - train: epoch 085, train_loss: 3.6840
2022-10-12 06:04:17 - eval: epoch: 085, acc1: 73.410%, acc5: 91.740%, test_loss: 1.3784, per_image_load_time: 1.054ms, per_image_inference_time: 0.532ms
2022-10-12 06:04:17 - until epoch: 085, best_acc1: 73.410%
2022-10-12 06:04:17 - epoch 086 lr: 0.005459
2022-10-12 06:05:01 - train: epoch 0086, iter [00100, 05004], lr: 0.005445, loss: 3.5495
2022-10-12 06:05:39 - train: epoch 0086, iter [00200, 05004], lr: 0.005431, loss: 3.5077
2022-10-12 06:06:16 - train: epoch 0086, iter [00300, 05004], lr: 0.005416, loss: 4.4843
2022-10-12 06:06:53 - train: epoch 0086, iter [00400, 05004], lr: 0.005402, loss: 3.6195
2022-10-12 06:07:29 - train: epoch 0086, iter [00500, 05004], lr: 0.005388, loss: 3.7569
2022-10-12 06:08:06 - train: epoch 0086, iter [00600, 05004], lr: 0.005374, loss: 4.2166
2022-10-12 06:08:43 - train: epoch 0086, iter [00700, 05004], lr: 0.005360, loss: 3.9259
2022-10-12 06:09:19 - train: epoch 0086, iter [00800, 05004], lr: 0.005346, loss: 3.4742
2022-10-12 06:09:56 - train: epoch 0086, iter [00900, 05004], lr: 0.005332, loss: 3.5881
2022-10-12 06:10:34 - train: epoch 0086, iter [01000, 05004], lr: 0.005318, loss: 4.1962
2022-10-12 06:11:11 - train: epoch 0086, iter [01100, 05004], lr: 0.005303, loss: 3.6352
2022-10-12 06:11:48 - train: epoch 0086, iter [01200, 05004], lr: 0.005289, loss: 3.8524
2022-10-12 06:12:25 - train: epoch 0086, iter [01300, 05004], lr: 0.005275, loss: 3.2765
2022-10-12 06:13:02 - train: epoch 0086, iter [01400, 05004], lr: 0.005261, loss: 3.4463
2022-10-12 06:13:40 - train: epoch 0086, iter [01500, 05004], lr: 0.005247, loss: 3.1887
2022-10-12 06:14:17 - train: epoch 0086, iter [01600, 05004], lr: 0.005233, loss: 3.1839
2022-10-12 06:14:53 - train: epoch 0086, iter [01700, 05004], lr: 0.005219, loss: 4.3319
2022-10-12 06:15:31 - train: epoch 0086, iter [01800, 05004], lr: 0.005205, loss: 4.2637
2022-10-12 06:16:09 - train: epoch 0086, iter [01900, 05004], lr: 0.005192, loss: 3.9237
2022-10-12 06:16:46 - train: epoch 0086, iter [02000, 05004], lr: 0.005178, loss: 4.0501
2022-10-12 06:17:23 - train: epoch 0086, iter [02100, 05004], lr: 0.005164, loss: 3.7901
2022-10-12 06:18:00 - train: epoch 0086, iter [02200, 05004], lr: 0.005150, loss: 3.9263
2022-10-12 06:18:37 - train: epoch 0086, iter [02300, 05004], lr: 0.005136, loss: 3.3517
2022-10-12 06:19:13 - train: epoch 0086, iter [02400, 05004], lr: 0.005122, loss: 3.9652
2022-10-12 06:19:51 - train: epoch 0086, iter [02500, 05004], lr: 0.005108, loss: 2.8698
2022-10-12 06:20:27 - train: epoch 0086, iter [02600, 05004], lr: 0.005095, loss: 3.0486
2022-10-12 06:21:05 - train: epoch 0086, iter [02700, 05004], lr: 0.005081, loss: 2.5995
2022-10-12 06:21:43 - train: epoch 0086, iter [02800, 05004], lr: 0.005067, loss: 2.8359
2022-10-12 06:22:19 - train: epoch 0086, iter [02900, 05004], lr: 0.005053, loss: 3.7666
2022-10-12 06:22:56 - train: epoch 0086, iter [03000, 05004], lr: 0.005040, loss: 2.5574
2022-10-12 06:23:33 - train: epoch 0086, iter [03100, 05004], lr: 0.005026, loss: 3.4395
2022-10-12 06:24:11 - train: epoch 0086, iter [03200, 05004], lr: 0.005012, loss: 3.7195
2022-10-12 06:24:48 - train: epoch 0086, iter [03300, 05004], lr: 0.004998, loss: 3.9578
2022-10-12 06:25:25 - train: epoch 0086, iter [03400, 05004], lr: 0.004985, loss: 3.2064
2022-10-12 06:26:02 - train: epoch 0086, iter [03500, 05004], lr: 0.004971, loss: 4.2142
2022-10-12 06:26:39 - train: epoch 0086, iter [03600, 05004], lr: 0.004958, loss: 3.3318
2022-10-12 06:27:17 - train: epoch 0086, iter [03700, 05004], lr: 0.004944, loss: 3.7489
2022-10-12 06:27:53 - train: epoch 0086, iter [03800, 05004], lr: 0.004930, loss: 4.1908
2022-10-12 06:28:30 - train: epoch 0086, iter [03900, 05004], lr: 0.004917, loss: 2.7454
2022-10-12 06:29:06 - train: epoch 0086, iter [04000, 05004], lr: 0.004903, loss: 3.7612
2022-10-12 06:29:44 - train: epoch 0086, iter [04100, 05004], lr: 0.004890, loss: 3.5317
2022-10-12 06:30:22 - train: epoch 0086, iter [04200, 05004], lr: 0.004876, loss: 4.3194
2022-10-12 06:30:58 - train: epoch 0086, iter [04300, 05004], lr: 0.004863, loss: 3.2914
2022-10-12 06:31:36 - train: epoch 0086, iter [04400, 05004], lr: 0.004849, loss: 3.1137
2022-10-12 06:32:14 - train: epoch 0086, iter [04500, 05004], lr: 0.004836, loss: 3.9280
2022-10-12 06:32:52 - train: epoch 0086, iter [04600, 05004], lr: 0.004822, loss: 3.9515
2022-10-12 06:33:29 - train: epoch 0086, iter [04700, 05004], lr: 0.004809, loss: 4.1232
2022-10-12 06:34:06 - train: epoch 0086, iter [04800, 05004], lr: 0.004795, loss: 3.7706
2022-10-12 06:34:43 - train: epoch 0086, iter [04900, 05004], lr: 0.004782, loss: 4.1872
2022-10-12 06:35:18 - train: epoch 0086, iter [05000, 05004], lr: 0.004769, loss: 3.6591
2022-10-12 06:35:20 - train: epoch 086, train_loss: 3.6591
2022-10-12 06:36:42 - eval: epoch: 086, acc1: 73.360%, acc5: 91.792%, test_loss: 1.3651, per_image_load_time: 1.015ms, per_image_inference_time: 0.574ms
2022-10-12 06:36:42 - until epoch: 086, best_acc1: 73.410%
2022-10-12 06:36:42 - epoch 087 lr: 0.004768
2022-10-12 06:37:26 - train: epoch 0087, iter [00100, 05004], lr: 0.004755, loss: 3.3638
2022-10-12 06:38:05 - train: epoch 0087, iter [00200, 05004], lr: 0.004741, loss: 3.2026
2022-10-12 06:38:42 - train: epoch 0087, iter [00300, 05004], lr: 0.004728, loss: 3.0550
2022-10-12 06:39:18 - train: epoch 0087, iter [00400, 05004], lr: 0.004715, loss: 3.4782
2022-10-12 06:39:54 - train: epoch 0087, iter [00500, 05004], lr: 0.004702, loss: 4.2413
2022-10-12 06:40:32 - train: epoch 0087, iter [00600, 05004], lr: 0.004688, loss: 3.4006
2022-10-12 06:41:09 - train: epoch 0087, iter [00700, 05004], lr: 0.004675, loss: 3.8348
2022-10-12 06:41:45 - train: epoch 0087, iter [00800, 05004], lr: 0.004662, loss: 3.8389
2022-10-12 06:42:23 - train: epoch 0087, iter [00900, 05004], lr: 0.004649, loss: 3.9131
2022-10-12 06:43:00 - train: epoch 0087, iter [01000, 05004], lr: 0.004635, loss: 3.3716
2022-10-12 06:43:36 - train: epoch 0087, iter [01100, 05004], lr: 0.004622, loss: 3.2298
2022-10-12 06:44:13 - train: epoch 0087, iter [01200, 05004], lr: 0.004609, loss: 3.7438
2022-10-12 06:44:50 - train: epoch 0087, iter [01300, 05004], lr: 0.004596, loss: 3.7006
2022-10-12 06:45:27 - train: epoch 0087, iter [01400, 05004], lr: 0.004583, loss: 4.4396
2022-10-12 06:46:03 - train: epoch 0087, iter [01500, 05004], lr: 0.004570, loss: 2.9580
2022-10-12 06:46:41 - train: epoch 0087, iter [01600, 05004], lr: 0.004557, loss: 4.5475
2022-10-12 06:47:18 - train: epoch 0087, iter [01700, 05004], lr: 0.004544, loss: 4.3400
2022-10-12 06:47:54 - train: epoch 0087, iter [01800, 05004], lr: 0.004531, loss: 3.8626
2022-10-12 06:48:32 - train: epoch 0087, iter [01900, 05004], lr: 0.004517, loss: 4.1877
2022-10-12 06:49:09 - train: epoch 0087, iter [02000, 05004], lr: 0.004504, loss: 3.9359
2022-10-12 06:49:45 - train: epoch 0087, iter [02100, 05004], lr: 0.004491, loss: 3.4151
2022-10-12 06:50:22 - train: epoch 0087, iter [02200, 05004], lr: 0.004478, loss: 3.5609
2022-10-12 06:50:59 - train: epoch 0087, iter [02300, 05004], lr: 0.004466, loss: 4.2831
2022-10-12 06:51:37 - train: epoch 0087, iter [02400, 05004], lr: 0.004453, loss: 3.8967
2022-10-12 06:52:14 - train: epoch 0087, iter [02500, 05004], lr: 0.004440, loss: 3.4147
2022-10-12 06:52:51 - train: epoch 0087, iter [02600, 05004], lr: 0.004427, loss: 4.1242
2022-10-12 06:53:28 - train: epoch 0087, iter [02700, 05004], lr: 0.004414, loss: 4.0192
2022-10-12 06:54:05 - train: epoch 0087, iter [02800, 05004], lr: 0.004401, loss: 4.0836
2022-10-12 06:54:42 - train: epoch 0087, iter [02900, 05004], lr: 0.004388, loss: 3.2913
2022-10-12 06:55:18 - train: epoch 0087, iter [03000, 05004], lr: 0.004375, loss: 3.7674
2022-10-12 06:55:55 - train: epoch 0087, iter [03100, 05004], lr: 0.004362, loss: 4.3115
2022-10-12 06:56:31 - train: epoch 0087, iter [03200, 05004], lr: 0.004350, loss: 2.7084
2022-10-12 06:57:09 - train: epoch 0087, iter [03300, 05004], lr: 0.004337, loss: 3.9511
2022-10-12 06:57:46 - train: epoch 0087, iter [03400, 05004], lr: 0.004324, loss: 4.1712
2022-10-12 06:58:22 - train: epoch 0087, iter [03500, 05004], lr: 0.004311, loss: 3.7840
2022-10-12 06:58:58 - train: epoch 0087, iter [03600, 05004], lr: 0.004299, loss: 3.2308
2022-10-12 06:59:36 - train: epoch 0087, iter [03700, 05004], lr: 0.004286, loss: 3.2038
2022-10-12 07:00:13 - train: epoch 0087, iter [03800, 05004], lr: 0.004273, loss: 3.1451
2022-10-12 07:00:50 - train: epoch 0087, iter [03900, 05004], lr: 0.004261, loss: 2.7701
2022-10-12 07:01:27 - train: epoch 0087, iter [04000, 05004], lr: 0.004248, loss: 3.0067
2022-10-12 07:02:04 - train: epoch 0087, iter [04100, 05004], lr: 0.004235, loss: 3.8312
2022-10-12 07:02:40 - train: epoch 0087, iter [04200, 05004], lr: 0.004223, loss: 3.8998
2022-10-12 07:03:17 - train: epoch 0087, iter [04300, 05004], lr: 0.004210, loss: 3.1326
2022-10-12 07:03:54 - train: epoch 0087, iter [04400, 05004], lr: 0.004197, loss: 3.5648
2022-10-12 07:04:31 - train: epoch 0087, iter [04500, 05004], lr: 0.004185, loss: 3.5352
2022-10-12 07:05:10 - train: epoch 0087, iter [04600, 05004], lr: 0.004172, loss: 2.5793
2022-10-12 07:05:46 - train: epoch 0087, iter [04700, 05004], lr: 0.004160, loss: 3.8909
2022-10-12 07:06:22 - train: epoch 0087, iter [04800, 05004], lr: 0.004147, loss: 3.0853
2022-10-12 07:06:59 - train: epoch 0087, iter [04900, 05004], lr: 0.004135, loss: 3.8093
2022-10-12 07:07:34 - train: epoch 0087, iter [05000, 05004], lr: 0.004122, loss: 3.7667
2022-10-12 07:07:36 - train: epoch 087, train_loss: 3.6435
2022-10-12 07:08:58 - eval: epoch: 087, acc1: 74.024%, acc5: 92.136%, test_loss: 1.3468, per_image_load_time: 1.430ms, per_image_inference_time: 0.576ms
2022-10-12 07:08:59 - until epoch: 087, best_acc1: 74.024%
2022-10-12 07:08:59 - epoch 088 lr: 0.004122
2022-10-12 07:09:43 - train: epoch 0088, iter [00100, 05004], lr: 0.004109, loss: 4.0574
2022-10-12 07:10:22 - train: epoch 0088, iter [00200, 05004], lr: 0.004097, loss: 3.4715
2022-10-12 07:10:58 - train: epoch 0088, iter [00300, 05004], lr: 0.004085, loss: 3.8160
2022-10-12 07:11:35 - train: epoch 0088, iter [00400, 05004], lr: 0.004072, loss: 3.5446
2022-10-12 07:12:11 - train: epoch 0088, iter [00500, 05004], lr: 0.004060, loss: 3.3943
2022-10-12 07:12:47 - train: epoch 0088, iter [00600, 05004], lr: 0.004047, loss: 3.3495
2022-10-12 07:13:24 - train: epoch 0088, iter [00700, 05004], lr: 0.004035, loss: 3.4005
2022-10-12 07:14:02 - train: epoch 0088, iter [00800, 05004], lr: 0.004023, loss: 3.9662
2022-10-12 07:14:40 - train: epoch 0088, iter [00900, 05004], lr: 0.004010, loss: 3.7561
2022-10-12 07:15:18 - train: epoch 0088, iter [01000, 05004], lr: 0.003998, loss: 3.7938
2022-10-12 07:15:55 - train: epoch 0088, iter [01100, 05004], lr: 0.003986, loss: 3.1665
2022-10-12 07:16:32 - train: epoch 0088, iter [01200, 05004], lr: 0.003974, loss: 3.5265
2022-10-12 07:17:09 - train: epoch 0088, iter [01300, 05004], lr: 0.003961, loss: 3.0349
2022-10-12 07:17:46 - train: epoch 0088, iter [01400, 05004], lr: 0.003949, loss: 3.8789
2022-10-12 07:18:23 - train: epoch 0088, iter [01500, 05004], lr: 0.003937, loss: 3.5617
2022-10-12 07:19:01 - train: epoch 0088, iter [01600, 05004], lr: 0.003925, loss: 3.6135
2022-10-12 07:19:37 - train: epoch 0088, iter [01700, 05004], lr: 0.003913, loss: 3.7939
2022-10-12 07:20:15 - train: epoch 0088, iter [01800, 05004], lr: 0.003900, loss: 3.6445
2022-10-12 07:20:52 - train: epoch 0088, iter [01900, 05004], lr: 0.003888, loss: 3.6584
2022-10-12 07:21:29 - train: epoch 0088, iter [02000, 05004], lr: 0.003876, loss: 3.5742
2022-10-12 07:22:06 - train: epoch 0088, iter [02100, 05004], lr: 0.003864, loss: 3.3209
2022-10-12 07:22:44 - train: epoch 0088, iter [02200, 05004], lr: 0.003852, loss: 2.6725
2022-10-12 07:23:21 - train: epoch 0088, iter [02300, 05004], lr: 0.003840, loss: 3.8316
2022-10-12 07:23:58 - train: epoch 0088, iter [02400, 05004], lr: 0.003828, loss: 3.1978
2022-10-12 07:24:35 - train: epoch 0088, iter [02500, 05004], lr: 0.003816, loss: 3.5621
2022-10-12 07:25:12 - train: epoch 0088, iter [02600, 05004], lr: 0.003804, loss: 3.6360
2022-10-12 07:25:49 - train: epoch 0088, iter [02700, 05004], lr: 0.003792, loss: 3.3199
2022-10-12 07:26:26 - train: epoch 0088, iter [02800, 05004], lr: 0.003780, loss: 3.6184
2022-10-12 07:27:02 - train: epoch 0088, iter [02900, 05004], lr: 0.003768, loss: 3.8452
2022-10-12 07:27:40 - train: epoch 0088, iter [03000, 05004], lr: 0.003756, loss: 3.3737
2022-10-12 07:28:17 - train: epoch 0088, iter [03100, 05004], lr: 0.003744, loss: 3.7846
2022-10-12 07:28:54 - train: epoch 0088, iter [03200, 05004], lr: 0.003732, loss: 3.3802
2022-10-12 07:29:32 - train: epoch 0088, iter [03300, 05004], lr: 0.003720, loss: 3.7246
2022-10-12 07:30:10 - train: epoch 0088, iter [03400, 05004], lr: 0.003709, loss: 2.7959
2022-10-12 07:30:46 - train: epoch 0088, iter [03500, 05004], lr: 0.003697, loss: 2.9877
2022-10-12 07:31:24 - train: epoch 0088, iter [03600, 05004], lr: 0.003685, loss: 3.3417
2022-10-12 07:32:01 - train: epoch 0088, iter [03700, 05004], lr: 0.003673, loss: 3.8813
2022-10-12 07:32:39 - train: epoch 0088, iter [03800, 05004], lr: 0.003661, loss: 4.3034
2022-10-12 07:33:15 - train: epoch 0088, iter [03900, 05004], lr: 0.003650, loss: 3.5303
2022-10-12 07:33:54 - train: epoch 0088, iter [04000, 05004], lr: 0.003638, loss: 4.3764
2022-10-12 07:34:31 - train: epoch 0088, iter [04100, 05004], lr: 0.003626, loss: 4.5239
2022-10-12 07:35:08 - train: epoch 0088, iter [04200, 05004], lr: 0.003614, loss: 3.4593
2022-10-12 07:35:45 - train: epoch 0088, iter [04300, 05004], lr: 0.003603, loss: 3.5103
2022-10-12 07:36:22 - train: epoch 0088, iter [04400, 05004], lr: 0.003591, loss: 3.8166
2022-10-12 07:36:59 - train: epoch 0088, iter [04500, 05004], lr: 0.003579, loss: 4.2257
2022-10-12 07:37:37 - train: epoch 0088, iter [04600, 05004], lr: 0.003568, loss: 3.5759
2022-10-12 07:38:13 - train: epoch 0088, iter [04700, 05004], lr: 0.003556, loss: 2.5855
2022-10-12 07:38:51 - train: epoch 0088, iter [04800, 05004], lr: 0.003544, loss: 4.3605
2022-10-12 07:39:28 - train: epoch 0088, iter [04900, 05004], lr: 0.003533, loss: 3.7990
2022-10-12 07:40:02 - train: epoch 0088, iter [05000, 05004], lr: 0.003521, loss: 3.8120
2022-10-12 07:40:04 - train: epoch 088, train_loss: 3.6176
2022-10-12 07:41:26 - eval: epoch: 088, acc1: 74.314%, acc5: 92.132%, test_loss: 1.3654, per_image_load_time: 1.630ms, per_image_inference_time: 0.561ms
2022-10-12 07:41:26 - until epoch: 088, best_acc1: 74.314%
2022-10-12 07:41:26 - epoch 089 lr: 0.003521
2022-10-12 07:42:10 - train: epoch 0089, iter [00100, 05004], lr: 0.003509, loss: 3.6098
2022-10-12 07:42:49 - train: epoch 0089, iter [00200, 05004], lr: 0.003498, loss: 3.2398
2022-10-12 07:43:25 - train: epoch 0089, iter [00300, 05004], lr: 0.003486, loss: 3.9668
2022-10-12 07:44:01 - train: epoch 0089, iter [00400, 05004], lr: 0.003475, loss: 2.7562
2022-10-12 07:44:38 - train: epoch 0089, iter [00500, 05004], lr: 0.003463, loss: 2.6301
2022-10-12 07:45:15 - train: epoch 0089, iter [00600, 05004], lr: 0.003452, loss: 3.3776
2022-10-12 07:45:51 - train: epoch 0089, iter [00700, 05004], lr: 0.003440, loss: 4.0508
2022-10-12 07:46:27 - train: epoch 0089, iter [00800, 05004], lr: 0.003429, loss: 4.1779
2022-10-12 07:47:05 - train: epoch 0089, iter [00900, 05004], lr: 0.003418, loss: 3.5096
2022-10-12 07:47:42 - train: epoch 0089, iter [01000, 05004], lr: 0.003406, loss: 2.5728
2022-10-12 07:48:19 - train: epoch 0089, iter [01100, 05004], lr: 0.003395, loss: 3.8729
2022-10-12 07:48:56 - train: epoch 0089, iter [01200, 05004], lr: 0.003383, loss: 3.1836
2022-10-12 07:49:33 - train: epoch 0089, iter [01300, 05004], lr: 0.003372, loss: 4.1569
2022-10-12 07:50:10 - train: epoch 0089, iter [01400, 05004], lr: 0.003361, loss: 3.5169
2022-10-12 07:50:46 - train: epoch 0089, iter [01500, 05004], lr: 0.003350, loss: 3.5491
2022-10-12 07:51:24 - train: epoch 0089, iter [01600, 05004], lr: 0.003338, loss: 3.9396
2022-10-12 07:52:02 - train: epoch 0089, iter [01700, 05004], lr: 0.003327, loss: 2.8643
2022-10-12 07:52:39 - train: epoch 0089, iter [01800, 05004], lr: 0.003316, loss: 4.0672
2022-10-12 07:53:17 - train: epoch 0089, iter [01900, 05004], lr: 0.003305, loss: 3.8836
2022-10-12 07:53:53 - train: epoch 0089, iter [02000, 05004], lr: 0.003293, loss: 3.3396
2022-10-12 07:54:30 - train: epoch 0089, iter [02100, 05004], lr: 0.003282, loss: 3.6363
2022-10-12 07:55:07 - train: epoch 0089, iter [02200, 05004], lr: 0.003271, loss: 3.6025
2022-10-12 07:55:44 - train: epoch 0089, iter [02300, 05004], lr: 0.003260, loss: 3.8393
2022-10-12 07:56:21 - train: epoch 0089, iter [02400, 05004], lr: 0.003249, loss: 4.0439
2022-10-12 07:56:58 - train: epoch 0089, iter [02500, 05004], lr: 0.003238, loss: 3.7947
2022-10-12 07:57:36 - train: epoch 0089, iter [02600, 05004], lr: 0.003227, loss: 3.2319
2022-10-12 07:58:13 - train: epoch 0089, iter [02700, 05004], lr: 0.003216, loss: 3.6780
2022-10-12 07:58:50 - train: epoch 0089, iter [02800, 05004], lr: 0.003204, loss: 3.4855
2022-10-12 07:59:26 - train: epoch 0089, iter [02900, 05004], lr: 0.003193, loss: 3.8956
2022-10-12 08:00:03 - train: epoch 0089, iter [03000, 05004], lr: 0.003182, loss: 3.5489
2022-10-12 08:00:40 - train: epoch 0089, iter [03100, 05004], lr: 0.003171, loss: 2.9184
2022-10-12 08:01:17 - train: epoch 0089, iter [03200, 05004], lr: 0.003160, loss: 3.8098
2022-10-12 08:01:54 - train: epoch 0089, iter [03300, 05004], lr: 0.003150, loss: 3.5983
2022-10-12 08:02:32 - train: epoch 0089, iter [03400, 05004], lr: 0.003139, loss: 3.3540
2022-10-12 08:03:10 - train: epoch 0089, iter [03500, 05004], lr: 0.003128, loss: 3.5254
2022-10-12 08:03:47 - train: epoch 0089, iter [03600, 05004], lr: 0.003117, loss: 3.8354
2022-10-12 08:04:24 - train: epoch 0089, iter [03700, 05004], lr: 0.003106, loss: 3.6493
2022-10-12 08:05:01 - train: epoch 0089, iter [03800, 05004], lr: 0.003095, loss: 3.6852
2022-10-12 08:05:39 - train: epoch 0089, iter [03900, 05004], lr: 0.003084, loss: 3.4711
2022-10-12 08:06:16 - train: epoch 0089, iter [04000, 05004], lr: 0.003073, loss: 4.0147
2022-10-12 08:06:53 - train: epoch 0089, iter [04100, 05004], lr: 0.003063, loss: 4.0947
2022-10-12 08:07:30 - train: epoch 0089, iter [04200, 05004], lr: 0.003052, loss: 3.8543
2022-10-12 08:08:07 - train: epoch 0089, iter [04300, 05004], lr: 0.003041, loss: 3.5282
2022-10-12 08:08:45 - train: epoch 0089, iter [04400, 05004], lr: 0.003030, loss: 3.8329
2022-10-12 08:09:22 - train: epoch 0089, iter [04500, 05004], lr: 0.003019, loss: 3.6636
2022-10-12 08:09:59 - train: epoch 0089, iter [04600, 05004], lr: 0.003009, loss: 3.5946
2022-10-12 08:10:37 - train: epoch 0089, iter [04700, 05004], lr: 0.002998, loss: 3.1566
2022-10-12 08:11:14 - train: epoch 0089, iter [04800, 05004], lr: 0.002987, loss: 3.1758
2022-10-12 08:11:51 - train: epoch 0089, iter [04900, 05004], lr: 0.002977, loss: 3.1119
2022-10-12 08:12:26 - train: epoch 0089, iter [05000, 05004], lr: 0.002966, loss: 3.6059
2022-10-12 08:12:29 - train: epoch 089, train_loss: 3.6023
2022-10-12 08:13:50 - eval: epoch: 089, acc1: 74.440%, acc5: 92.400%, test_loss: 1.3185, per_image_load_time: 1.282ms, per_image_inference_time: 0.545ms
2022-10-12 08:13:50 - until epoch: 089, best_acc1: 74.440%
2022-10-12 08:13:50 - epoch 090 lr: 0.002966
2022-10-12 08:14:34 - train: epoch 0090, iter [00100, 05004], lr: 0.002955, loss: 3.0587
2022-10-12 08:15:12 - train: epoch 0090, iter [00200, 05004], lr: 0.002944, loss: 4.2001
2022-10-12 08:15:49 - train: epoch 0090, iter [00300, 05004], lr: 0.002934, loss: 3.3669
2022-10-12 08:16:25 - train: epoch 0090, iter [00400, 05004], lr: 0.002923, loss: 3.6601
2022-10-12 08:17:02 - train: epoch 0090, iter [00500, 05004], lr: 0.002913, loss: 3.3429
2022-10-12 08:17:38 - train: epoch 0090, iter [00600, 05004], lr: 0.002902, loss: 3.8968
2022-10-12 08:18:15 - train: epoch 0090, iter [00700, 05004], lr: 0.002892, loss: 4.2160
2022-10-12 08:18:51 - train: epoch 0090, iter [00800, 05004], lr: 0.002881, loss: 3.1623
2022-10-12 08:19:28 - train: epoch 0090, iter [00900, 05004], lr: 0.002871, loss: 3.2507
2022-10-12 08:20:05 - train: epoch 0090, iter [01000, 05004], lr: 0.002860, loss: 3.5847
2022-10-12 08:20:42 - train: epoch 0090, iter [01100, 05004], lr: 0.002850, loss: 3.6463
2022-10-12 08:21:19 - train: epoch 0090, iter [01200, 05004], lr: 0.002839, loss: 2.6462
2022-10-12 08:21:55 - train: epoch 0090, iter [01300, 05004], lr: 0.002829, loss: 4.0024
2022-10-12 08:22:30 - train: epoch 0090, iter [01400, 05004], lr: 0.002819, loss: 3.4358
2022-10-12 08:23:07 - train: epoch 0090, iter [01500, 05004], lr: 0.002808, loss: 3.4936
2022-10-12 08:23:43 - train: epoch 0090, iter [01600, 05004], lr: 0.002798, loss: 3.7208
2022-10-12 08:24:19 - train: epoch 0090, iter [01700, 05004], lr: 0.002788, loss: 3.6263
2022-10-12 08:24:56 - train: epoch 0090, iter [01800, 05004], lr: 0.002777, loss: 2.8487
2022-10-12 08:25:33 - train: epoch 0090, iter [01900, 05004], lr: 0.002767, loss: 3.4266
2022-10-12 08:26:09 - train: epoch 0090, iter [02000, 05004], lr: 0.002757, loss: 3.4778
2022-10-12 08:26:45 - train: epoch 0090, iter [02100, 05004], lr: 0.002746, loss: 4.3516
2022-10-12 08:27:22 - train: epoch 0090, iter [02200, 05004], lr: 0.002736, loss: 2.8520
2022-10-12 08:27:59 - train: epoch 0090, iter [02300, 05004], lr: 0.002726, loss: 3.7114
2022-10-12 08:28:35 - train: epoch 0090, iter [02400, 05004], lr: 0.002716, loss: 3.8924
2022-10-12 08:29:13 - train: epoch 0090, iter [02500, 05004], lr: 0.002706, loss: 3.8142
2022-10-12 08:29:49 - train: epoch 0090, iter [02600, 05004], lr: 0.002696, loss: 4.0019
2022-10-12 08:30:26 - train: epoch 0090, iter [02700, 05004], lr: 0.002685, loss: 3.0165
2022-10-12 08:31:03 - train: epoch 0090, iter [02800, 05004], lr: 0.002675, loss: 4.0478
2022-10-12 08:31:39 - train: epoch 0090, iter [02900, 05004], lr: 0.002665, loss: 3.2880
2022-10-12 08:32:16 - train: epoch 0090, iter [03000, 05004], lr: 0.002655, loss: 3.3617
2022-10-12 08:32:53 - train: epoch 0090, iter [03100, 05004], lr: 0.002645, loss: 3.3818
2022-10-12 08:33:30 - train: epoch 0090, iter [03200, 05004], lr: 0.002635, loss: 4.4190
2022-10-12 08:34:07 - train: epoch 0090, iter [03300, 05004], lr: 0.002625, loss: 3.8934
2022-10-12 08:34:44 - train: epoch 0090, iter [03400, 05004], lr: 0.002615, loss: 4.2090
2022-10-12 08:35:21 - train: epoch 0090, iter [03500, 05004], lr: 0.002605, loss: 3.8446
2022-10-12 08:35:59 - train: epoch 0090, iter [03600, 05004], lr: 0.002595, loss: 3.8709
2022-10-12 08:36:35 - train: epoch 0090, iter [03700, 05004], lr: 0.002585, loss: 3.3274
2022-10-12 08:37:12 - train: epoch 0090, iter [03800, 05004], lr: 0.002575, loss: 3.6061
2022-10-12 08:37:49 - train: epoch 0090, iter [03900, 05004], lr: 0.002565, loss: 3.2924
2022-10-12 08:38:26 - train: epoch 0090, iter [04000, 05004], lr: 0.002555, loss: 3.7969
2022-10-12 08:39:03 - train: epoch 0090, iter [04100, 05004], lr: 0.002545, loss: 3.4890
2022-10-12 08:39:40 - train: epoch 0090, iter [04200, 05004], lr: 0.002536, loss: 4.2537
2022-10-12 08:40:17 - train: epoch 0090, iter [04300, 05004], lr: 0.002526, loss: 4.0321
2022-10-12 08:40:54 - train: epoch 0090, iter [04400, 05004], lr: 0.002516, loss: 3.6277
2022-10-12 08:41:31 - train: epoch 0090, iter [04500, 05004], lr: 0.002506, loss: 3.6265
2022-10-12 08:42:08 - train: epoch 0090, iter [04600, 05004], lr: 0.002496, loss: 2.9433
2022-10-12 08:42:45 - train: epoch 0090, iter [04700, 05004], lr: 0.002487, loss: 2.8457
2022-10-12 08:43:22 - train: epoch 0090, iter [04800, 05004], lr: 0.002477, loss: 3.1445
2022-10-12 08:43:58 - train: epoch 0090, iter [04900, 05004], lr: 0.002467, loss: 3.8061
2022-10-12 08:44:34 - train: epoch 0090, iter [05000, 05004], lr: 0.002457, loss: 3.7743
2022-10-12 08:44:36 - train: epoch 090, train_loss: 3.5729
2022-10-12 08:45:57 - eval: epoch: 090, acc1: 75.020%, acc5: 92.480%, test_loss: 1.2894, per_image_load_time: 1.478ms, per_image_inference_time: 0.562ms
2022-10-12 08:45:57 - until epoch: 090, best_acc1: 75.020%
2022-10-12 08:45:57 - epoch 091 lr: 0.002457
2022-10-12 08:46:42 - train: epoch 0091, iter [00100, 05004], lr: 0.002447, loss: 3.0361
2022-10-12 08:47:19 - train: epoch 0091, iter [00200, 05004], lr: 0.002438, loss: 2.6405
2022-10-12 08:47:57 - train: epoch 0091, iter [00300, 05004], lr: 0.002428, loss: 3.6674
2022-10-12 08:48:33 - train: epoch 0091, iter [00400, 05004], lr: 0.002418, loss: 3.6448
2022-10-12 08:49:09 - train: epoch 0091, iter [00500, 05004], lr: 0.002409, loss: 3.2366
2022-10-12 08:49:46 - train: epoch 0091, iter [00600, 05004], lr: 0.002399, loss: 3.6369
2022-10-12 08:50:24 - train: epoch 0091, iter [00700, 05004], lr: 0.002389, loss: 3.5840
2022-10-12 08:51:00 - train: epoch 0091, iter [00800, 05004], lr: 0.002380, loss: 3.9357
2022-10-12 08:51:37 - train: epoch 0091, iter [00900, 05004], lr: 0.002370, loss: 3.3503
2022-10-12 08:52:14 - train: epoch 0091, iter [01000, 05004], lr: 0.002361, loss: 3.0526
2022-10-12 08:52:50 - train: epoch 0091, iter [01100, 05004], lr: 0.002351, loss: 3.8722
2022-10-12 08:53:28 - train: epoch 0091, iter [01200, 05004], lr: 0.002342, loss: 3.1267
2022-10-12 08:54:05 - train: epoch 0091, iter [01300, 05004], lr: 0.002332, loss: 2.4706
2022-10-12 08:54:41 - train: epoch 0091, iter [01400, 05004], lr: 0.002323, loss: 3.6152
2022-10-12 08:55:18 - train: epoch 0091, iter [01500, 05004], lr: 0.002314, loss: 4.3650
2022-10-12 08:55:56 - train: epoch 0091, iter [01600, 05004], lr: 0.002304, loss: 3.1999
2022-10-12 08:56:32 - train: epoch 0091, iter [01700, 05004], lr: 0.002295, loss: 4.2986
2022-10-12 08:57:09 - train: epoch 0091, iter [01800, 05004], lr: 0.002285, loss: 3.4636
2022-10-12 08:57:46 - train: epoch 0091, iter [01900, 05004], lr: 0.002276, loss: 4.0638
2022-10-12 08:58:24 - train: epoch 0091, iter [02000, 05004], lr: 0.002267, loss: 3.5438
2022-10-12 08:59:01 - train: epoch 0091, iter [02100, 05004], lr: 0.002257, loss: 3.2428
2022-10-12 08:59:39 - train: epoch 0091, iter [02200, 05004], lr: 0.002248, loss: 3.9202
2022-10-12 09:00:15 - train: epoch 0091, iter [02300, 05004], lr: 0.002239, loss: 3.5725
2022-10-12 09:00:52 - train: epoch 0091, iter [02400, 05004], lr: 0.002230, loss: 3.6931
2022-10-12 09:01:29 - train: epoch 0091, iter [02500, 05004], lr: 0.002220, loss: 3.7920
2022-10-12 09:02:07 - train: epoch 0091, iter [02600, 05004], lr: 0.002211, loss: 3.4698
2022-10-12 09:02:44 - train: epoch 0091, iter [02700, 05004], lr: 0.002202, loss: 4.1734
2022-10-12 09:03:21 - train: epoch 0091, iter [02800, 05004], lr: 0.002193, loss: 3.3058
2022-10-12 09:03:58 - train: epoch 0091, iter [02900, 05004], lr: 0.002184, loss: 3.1344
2022-10-12 09:04:36 - train: epoch 0091, iter [03000, 05004], lr: 0.002174, loss: 3.1759
2022-10-12 09:05:13 - train: epoch 0091, iter [03100, 05004], lr: 0.002165, loss: 3.3519
2022-10-12 09:05:50 - train: epoch 0091, iter [03200, 05004], lr: 0.002156, loss: 3.5468
2022-10-12 09:06:27 - train: epoch 0091, iter [03300, 05004], lr: 0.002147, loss: 3.8789
2022-10-12 09:07:04 - train: epoch 0091, iter [03400, 05004], lr: 0.002138, loss: 3.7658
2022-10-12 09:07:41 - train: epoch 0091, iter [03500, 05004], lr: 0.002129, loss: 4.3699
2022-10-12 09:08:18 - train: epoch 0091, iter [03600, 05004], lr: 0.002120, loss: 3.3314
2022-10-12 09:08:56 - train: epoch 0091, iter [03700, 05004], lr: 0.002111, loss: 3.3217
2022-10-12 09:09:32 - train: epoch 0091, iter [03800, 05004], lr: 0.002102, loss: 4.0435
2022-10-12 09:10:10 - train: epoch 0091, iter [03900, 05004], lr: 0.002093, loss: 3.4501
2022-10-12 09:10:48 - train: epoch 0091, iter [04000, 05004], lr: 0.002084, loss: 3.5359
2022-10-12 09:11:24 - train: epoch 0091, iter [04100, 05004], lr: 0.002075, loss: 4.1088
2022-10-12 09:12:02 - train: epoch 0091, iter [04200, 05004], lr: 0.002066, loss: 4.1735
2022-10-12 09:12:39 - train: epoch 0091, iter [04300, 05004], lr: 0.002057, loss: 3.7060
2022-10-12 09:13:15 - train: epoch 0091, iter [04400, 05004], lr: 0.002048, loss: 3.4809
2022-10-12 09:13:53 - train: epoch 0091, iter [04500, 05004], lr: 0.002039, loss: 4.2446
2022-10-12 09:14:29 - train: epoch 0091, iter [04600, 05004], lr: 0.002031, loss: 3.7227
2022-10-12 09:15:06 - train: epoch 0091, iter [04700, 05004], lr: 0.002022, loss: 2.8738
2022-10-12 09:15:44 - train: epoch 0091, iter [04800, 05004], lr: 0.002013, loss: 3.6730
2022-10-12 09:16:21 - train: epoch 0091, iter [04900, 05004], lr: 0.002004, loss: 3.1701
2022-10-12 09:16:55 - train: epoch 0091, iter [05000, 05004], lr: 0.001995, loss: 3.5232
2022-10-12 09:16:58 - train: epoch 091, train_loss: 3.5732
2022-10-12 09:18:18 - eval: epoch: 091, acc1: 75.108%, acc5: 92.672%, test_loss: 1.3053, per_image_load_time: 0.428ms, per_image_inference_time: 0.568ms
2022-10-12 09:18:19 - until epoch: 091, best_acc1: 75.108%
2022-10-12 09:18:19 - epoch 092 lr: 0.001995
2022-10-12 09:19:03 - train: epoch 0092, iter [00100, 05004], lr: 0.001986, loss: 3.5470
2022-10-12 09:19:43 - train: epoch 0092, iter [00200, 05004], lr: 0.001978, loss: 3.9722
2022-10-12 09:20:19 - train: epoch 0092, iter [00300, 05004], lr: 0.001969, loss: 3.5840
2022-10-12 09:20:56 - train: epoch 0092, iter [00400, 05004], lr: 0.001960, loss: 3.5613
2022-10-12 09:21:32 - train: epoch 0092, iter [00500, 05004], lr: 0.001952, loss: 3.9276
2022-10-12 09:22:09 - train: epoch 0092, iter [00600, 05004], lr: 0.001943, loss: 3.2110
2022-10-12 09:22:45 - train: epoch 0092, iter [00700, 05004], lr: 0.001934, loss: 3.1760
2022-10-12 09:23:22 - train: epoch 0092, iter [00800, 05004], lr: 0.001926, loss: 3.6818
2022-10-12 09:23:59 - train: epoch 0092, iter [00900, 05004], lr: 0.001917, loss: 3.0860
2022-10-12 09:24:36 - train: epoch 0092, iter [01000, 05004], lr: 0.001908, loss: 4.0909
2022-10-12 09:25:11 - train: epoch 0092, iter [01100, 05004], lr: 0.001900, loss: 3.3027
2022-10-12 09:25:49 - train: epoch 0092, iter [01200, 05004], lr: 0.001891, loss: 3.6013
2022-10-12 09:26:26 - train: epoch 0092, iter [01300, 05004], lr: 0.001883, loss: 3.6963
2022-10-12 09:27:04 - train: epoch 0092, iter [01400, 05004], lr: 0.001874, loss: 3.6558
2022-10-12 09:27:40 - train: epoch 0092, iter [01500, 05004], lr: 0.001866, loss: 4.1968
2022-10-12 09:28:17 - train: epoch 0092, iter [01600, 05004], lr: 0.001857, loss: 3.5494
2022-10-12 09:28:54 - train: epoch 0092, iter [01700, 05004], lr: 0.001849, loss: 3.0365
2022-10-12 09:29:31 - train: epoch 0092, iter [01800, 05004], lr: 0.001841, loss: 3.6707
2022-10-12 09:30:08 - train: epoch 0092, iter [01900, 05004], lr: 0.001832, loss: 2.8865
2022-10-12 09:30:45 - train: epoch 0092, iter [02000, 05004], lr: 0.001824, loss: 3.7143
2022-10-12 09:31:22 - train: epoch 0092, iter [02100, 05004], lr: 0.001815, loss: 4.1610
2022-10-12 09:32:00 - train: epoch 0092, iter [02200, 05004], lr: 0.001807, loss: 3.1287
2022-10-12 09:32:37 - train: epoch 0092, iter [02300, 05004], lr: 0.001799, loss: 3.3761
2022-10-12 09:33:14 - train: epoch 0092, iter [02400, 05004], lr: 0.001790, loss: 3.9518
2022-10-12 09:33:51 - train: epoch 0092, iter [02500, 05004], lr: 0.001782, loss: 4.0530
2022-10-12 09:34:28 - train: epoch 0092, iter [02600, 05004], lr: 0.001774, loss: 3.4300
2022-10-12 09:35:06 - train: epoch 0092, iter [02700, 05004], lr: 0.001766, loss: 3.5027
2022-10-12 09:35:42 - train: epoch 0092, iter [02800, 05004], lr: 0.001757, loss: 2.9984
2022-10-12 09:36:20 - train: epoch 0092, iter [02900, 05004], lr: 0.001749, loss: 3.5007
2022-10-12 09:36:56 - train: epoch 0092, iter [03000, 05004], lr: 0.001741, loss: 4.0599
2022-10-12 09:37:34 - train: epoch 0092, iter [03100, 05004], lr: 0.001733, loss: 3.4211
2022-10-12 09:38:11 - train: epoch 0092, iter [03200, 05004], lr: 0.001725, loss: 2.8646
2022-10-12 09:38:48 - train: epoch 0092, iter [03300, 05004], lr: 0.001716, loss: 3.3278
2022-10-12 09:39:26 - train: epoch 0092, iter [03400, 05004], lr: 0.001708, loss: 4.0232
2022-10-12 09:40:03 - train: epoch 0092, iter [03500, 05004], lr: 0.001700, loss: 3.4847
2022-10-12 09:40:40 - train: epoch 0092, iter [03600, 05004], lr: 0.001692, loss: 3.8479
2022-10-12 09:41:17 - train: epoch 0092, iter [03700, 05004], lr: 0.001684, loss: 2.7846
2022-10-12 09:41:54 - train: epoch 0092, iter [03800, 05004], lr: 0.001676, loss: 3.5333
2022-10-12 09:42:32 - train: epoch 0092, iter [03900, 05004], lr: 0.001668, loss: 3.9557
2022-10-12 09:43:08 - train: epoch 0092, iter [04000, 05004], lr: 0.001660, loss: 3.8341
2022-10-12 09:43:45 - train: epoch 0092, iter [04100, 05004], lr: 0.001652, loss: 3.7696
2022-10-12 09:44:22 - train: epoch 0092, iter [04200, 05004], lr: 0.001644, loss: 3.7620
2022-10-12 09:45:00 - train: epoch 0092, iter [04300, 05004], lr: 0.001636, loss: 3.0562
2022-10-12 09:45:37 - train: epoch 0092, iter [04400, 05004], lr: 0.001628, loss: 3.7296
2022-10-12 09:46:15 - train: epoch 0092, iter [04500, 05004], lr: 0.001620, loss: 3.5393
2022-10-12 09:46:52 - train: epoch 0092, iter [04600, 05004], lr: 0.001612, loss: 3.8165
2022-10-12 09:47:29 - train: epoch 0092, iter [04700, 05004], lr: 0.001605, loss: 3.7631
2022-10-12 09:48:05 - train: epoch 0092, iter [04800, 05004], lr: 0.001597, loss: 3.9938
2022-10-12 09:48:43 - train: epoch 0092, iter [04900, 05004], lr: 0.001589, loss: 3.6192
2022-10-12 09:49:18 - train: epoch 0092, iter [05000, 05004], lr: 0.001581, loss: 3.5509
2022-10-12 09:49:20 - train: epoch 092, train_loss: 3.5361
2022-10-12 09:50:41 - eval: epoch: 092, acc1: 75.360%, acc5: 92.850%, test_loss: 1.2679, per_image_load_time: 1.855ms, per_image_inference_time: 0.571ms
2022-10-12 09:50:41 - until epoch: 092, best_acc1: 75.360%
2022-10-12 09:50:41 - epoch 093 lr: 0.001581
2022-10-12 09:51:25 - train: epoch 0093, iter [00100, 05004], lr: 0.001573, loss: 4.1222
2022-10-12 09:52:03 - train: epoch 0093, iter [00200, 05004], lr: 0.001565, loss: 2.7445
2022-10-12 09:52:39 - train: epoch 0093, iter [00300, 05004], lr: 0.001557, loss: 3.4705
2022-10-12 09:53:16 - train: epoch 0093, iter [00400, 05004], lr: 0.001550, loss: 4.2507
2022-10-12 09:53:52 - train: epoch 0093, iter [00500, 05004], lr: 0.001542, loss: 3.7715
2022-10-12 09:54:29 - train: epoch 0093, iter [00600, 05004], lr: 0.001534, loss: 3.8470
2022-10-12 09:55:06 - train: epoch 0093, iter [00700, 05004], lr: 0.001527, loss: 4.3519
2022-10-12 09:55:43 - train: epoch 0093, iter [00800, 05004], lr: 0.001519, loss: 3.9247
2022-10-12 09:56:19 - train: epoch 0093, iter [00900, 05004], lr: 0.001511, loss: 3.1629
2022-10-12 09:56:56 - train: epoch 0093, iter [01000, 05004], lr: 0.001504, loss: 3.1303
2022-10-12 09:57:34 - train: epoch 0093, iter [01100, 05004], lr: 0.001496, loss: 3.2465
2022-10-12 09:58:10 - train: epoch 0093, iter [01200, 05004], lr: 0.001488, loss: 3.2837
2022-10-12 09:58:47 - train: epoch 0093, iter [01300, 05004], lr: 0.001481, loss: 3.8318
2022-10-12 09:59:24 - train: epoch 0093, iter [01400, 05004], lr: 0.001473, loss: 3.6373
2022-10-12 10:00:02 - train: epoch 0093, iter [01500, 05004], lr: 0.001466, loss: 3.0183
2022-10-12 10:00:39 - train: epoch 0093, iter [01600, 05004], lr: 0.001458, loss: 3.2185
2022-10-12 10:01:16 - train: epoch 0093, iter [01700, 05004], lr: 0.001451, loss: 3.4368
2022-10-12 10:01:53 - train: epoch 0093, iter [01800, 05004], lr: 0.001443, loss: 3.3171
2022-10-12 10:02:30 - train: epoch 0093, iter [01900, 05004], lr: 0.001436, loss: 2.7971
2022-10-12 10:03:07 - train: epoch 0093, iter [02000, 05004], lr: 0.001428, loss: 3.8246
2022-10-12 10:03:45 - train: epoch 0093, iter [02100, 05004], lr: 0.001421, loss: 3.1644
2022-10-12 10:04:22 - train: epoch 0093, iter [02200, 05004], lr: 0.001414, loss: 3.2161
2022-10-12 10:04:59 - train: epoch 0093, iter [02300, 05004], lr: 0.001406, loss: 3.8343
2022-10-12 10:05:36 - train: epoch 0093, iter [02400, 05004], lr: 0.001399, loss: 3.2937
2022-10-12 10:06:13 - train: epoch 0093, iter [02500, 05004], lr: 0.001392, loss: 3.6883
2022-10-12 10:06:51 - train: epoch 0093, iter [02600, 05004], lr: 0.001384, loss: 2.8990
2022-10-12 10:07:28 - train: epoch 0093, iter [02700, 05004], lr: 0.001377, loss: 4.1168
2022-10-12 10:08:05 - train: epoch 0093, iter [02800, 05004], lr: 0.001370, loss: 3.8803
2022-10-12 10:08:43 - train: epoch 0093, iter [02900, 05004], lr: 0.001362, loss: 3.0673
2022-10-12 10:09:20 - train: epoch 0093, iter [03000, 05004], lr: 0.001355, loss: 3.6435
2022-10-12 10:09:58 - train: epoch 0093, iter [03100, 05004], lr: 0.001348, loss: 3.4238
2022-10-12 10:10:35 - train: epoch 0093, iter [03200, 05004], lr: 0.001341, loss: 3.2604
2022-10-12 10:11:12 - train: epoch 0093, iter [03300, 05004], lr: 0.001334, loss: 4.1160
2022-10-12 10:11:49 - train: epoch 0093, iter [03400, 05004], lr: 0.001326, loss: 3.8833
2022-10-12 10:12:26 - train: epoch 0093, iter [03500, 05004], lr: 0.001319, loss: 3.6809
2022-10-12 10:13:04 - train: epoch 0093, iter [03600, 05004], lr: 0.001312, loss: 3.1847
2022-10-12 10:13:41 - train: epoch 0093, iter [03700, 05004], lr: 0.001305, loss: 3.1354
2022-10-12 10:14:18 - train: epoch 0093, iter [03800, 05004], lr: 0.001298, loss: 3.2940
2022-10-12 10:14:54 - train: epoch 0093, iter [03900, 05004], lr: 0.001291, loss: 3.2831
2022-10-12 10:15:32 - train: epoch 0093, iter [04000, 05004], lr: 0.001284, loss: 3.5024
2022-10-12 10:16:09 - train: epoch 0093, iter [04100, 05004], lr: 0.001277, loss: 3.3025
2022-10-12 10:16:46 - train: epoch 0093, iter [04200, 05004], lr: 0.001270, loss: 4.0647
2022-10-12 10:17:23 - train: epoch 0093, iter [04300, 05004], lr: 0.001263, loss: 3.5351
2022-10-12 10:18:00 - train: epoch 0093, iter [04400, 05004], lr: 0.001256, loss: 3.7787
2022-10-12 10:18:36 - train: epoch 0093, iter [04500, 05004], lr: 0.001249, loss: 2.4775
2022-10-12 10:19:14 - train: epoch 0093, iter [04600, 05004], lr: 0.001242, loss: 3.6560
2022-10-12 10:19:51 - train: epoch 0093, iter [04700, 05004], lr: 0.001235, loss: 4.4315
2022-10-12 10:20:28 - train: epoch 0093, iter [04800, 05004], lr: 0.001228, loss: 3.1554
2022-10-12 10:21:04 - train: epoch 0093, iter [04900, 05004], lr: 0.001221, loss: 3.5628
2022-10-12 10:21:39 - train: epoch 0093, iter [05000, 05004], lr: 0.001214, loss: 3.6551
2022-10-12 10:21:42 - train: epoch 093, train_loss: 3.5150
2022-10-12 10:23:01 - eval: epoch: 093, acc1: 75.738%, acc5: 92.962%, test_loss: 1.2528, per_image_load_time: 2.080ms, per_image_inference_time: 0.531ms
2022-10-12 10:23:01 - until epoch: 093, best_acc1: 75.738%
2022-10-12 10:23:01 - epoch 094 lr: 0.001214
2022-10-12 10:23:44 - train: epoch 0094, iter [00100, 05004], lr: 0.001207, loss: 3.6536
2022-10-12 10:24:22 - train: epoch 0094, iter [00200, 05004], lr: 0.001200, loss: 4.0319
2022-10-12 10:24:59 - train: epoch 0094, iter [00300, 05004], lr: 0.001194, loss: 2.5870
2022-10-12 10:25:35 - train: epoch 0094, iter [00400, 05004], lr: 0.001187, loss: 3.9322
2022-10-12 10:26:13 - train: epoch 0094, iter [00500, 05004], lr: 0.001180, loss: 3.2318
2022-10-12 10:26:49 - train: epoch 0094, iter [00600, 05004], lr: 0.001173, loss: 3.9061
2022-10-12 10:27:26 - train: epoch 0094, iter [00700, 05004], lr: 0.001167, loss: 3.8540
2022-10-12 10:28:03 - train: epoch 0094, iter [00800, 05004], lr: 0.001160, loss: 3.5731
2022-10-12 10:28:39 - train: epoch 0094, iter [00900, 05004], lr: 0.001153, loss: 3.9892
2022-10-12 10:29:16 - train: epoch 0094, iter [01000, 05004], lr: 0.001147, loss: 3.2673
2022-10-12 10:29:53 - train: epoch 0094, iter [01100, 05004], lr: 0.001140, loss: 4.0375
2022-10-12 10:30:30 - train: epoch 0094, iter [01200, 05004], lr: 0.001133, loss: 3.3083
2022-10-12 10:31:08 - train: epoch 0094, iter [01300, 05004], lr: 0.001127, loss: 3.5280
2022-10-12 10:31:45 - train: epoch 0094, iter [01400, 05004], lr: 0.001120, loss: 3.9144
2022-10-12 10:32:22 - train: epoch 0094, iter [01500, 05004], lr: 0.001114, loss: 3.5149
2022-10-12 10:33:00 - train: epoch 0094, iter [01600, 05004], lr: 0.001107, loss: 3.9900
2022-10-12 10:33:35 - train: epoch 0094, iter [01700, 05004], lr: 0.001100, loss: 2.5882
2022-10-12 10:34:13 - train: epoch 0094, iter [01800, 05004], lr: 0.001094, loss: 3.1672
2022-10-12 10:34:49 - train: epoch 0094, iter [01900, 05004], lr: 0.001087, loss: 3.7256
2022-10-12 10:35:26 - train: epoch 0094, iter [02000, 05004], lr: 0.001081, loss: 3.7188
2022-10-12 10:36:03 - train: epoch 0094, iter [02100, 05004], lr: 0.001074, loss: 3.7627
2022-10-12 10:36:40 - train: epoch 0094, iter [02200, 05004], lr: 0.001068, loss: 3.9642
2022-10-12 10:37:17 - train: epoch 0094, iter [02300, 05004], lr: 0.001062, loss: 3.7994
2022-10-12 10:37:54 - train: epoch 0094, iter [02400, 05004], lr: 0.001055, loss: 3.8826
2022-10-12 10:38:31 - train: epoch 0094, iter [02500, 05004], lr: 0.001049, loss: 3.2166
2022-10-12 10:39:08 - train: epoch 0094, iter [02600, 05004], lr: 0.001043, loss: 2.9776
2022-10-12 10:39:46 - train: epoch 0094, iter [02700, 05004], lr: 0.001036, loss: 3.4164
2022-10-12 10:40:23 - train: epoch 0094, iter [02800, 05004], lr: 0.001030, loss: 3.5239
2022-10-12 10:40:59 - train: epoch 0094, iter [02900, 05004], lr: 0.001024, loss: 3.7061
2022-10-12 10:41:36 - train: epoch 0094, iter [03000, 05004], lr: 0.001017, loss: 3.8506
2022-10-12 10:42:13 - train: epoch 0094, iter [03100, 05004], lr: 0.001011, loss: 3.2780
2022-10-12 10:42:49 - train: epoch 0094, iter [03200, 05004], lr: 0.001005, loss: 2.9061
2022-10-12 10:43:26 - train: epoch 0094, iter [03300, 05004], lr: 0.000999, loss: 3.9551
2022-10-12 10:44:03 - train: epoch 0094, iter [03400, 05004], lr: 0.000992, loss: 3.6431
2022-10-12 10:44:40 - train: epoch 0094, iter [03500, 05004], lr: 0.000986, loss: 3.3775
2022-10-12 10:45:18 - train: epoch 0094, iter [03600, 05004], lr: 0.000980, loss: 3.3105
2022-10-12 10:45:54 - train: epoch 0094, iter [03700, 05004], lr: 0.000974, loss: 3.5532
2022-10-12 10:46:32 - train: epoch 0094, iter [03800, 05004], lr: 0.000968, loss: 3.1115
2022-10-12 10:47:09 - train: epoch 0094, iter [03900, 05004], lr: 0.000962, loss: 3.2242
2022-10-12 10:47:46 - train: epoch 0094, iter [04000, 05004], lr: 0.000956, loss: 3.1298
2022-10-12 10:48:22 - train: epoch 0094, iter [04100, 05004], lr: 0.000950, loss: 2.9318
2022-10-12 10:48:59 - train: epoch 0094, iter [04200, 05004], lr: 0.000943, loss: 3.1280
2022-10-12 10:49:36 - train: epoch 0094, iter [04300, 05004], lr: 0.000937, loss: 2.6522
2022-10-12 10:50:13 - train: epoch 0094, iter [04400, 05004], lr: 0.000931, loss: 2.8333
2022-10-12 10:50:51 - train: epoch 0094, iter [04500, 05004], lr: 0.000925, loss: 3.7057
2022-10-12 10:51:27 - train: epoch 0094, iter [04600, 05004], lr: 0.000919, loss: 3.2421
2022-10-12 10:52:04 - train: epoch 0094, iter [04700, 05004], lr: 0.000914, loss: 4.1116
2022-10-12 10:52:42 - train: epoch 0094, iter [04800, 05004], lr: 0.000908, loss: 3.2912
2022-10-12 10:53:18 - train: epoch 0094, iter [04900, 05004], lr: 0.000902, loss: 3.7468
2022-10-12 10:53:53 - train: epoch 0094, iter [05000, 05004], lr: 0.000896, loss: 3.8799
2022-10-12 10:53:56 - train: epoch 094, train_loss: 3.4986
2022-10-12 10:55:16 - eval: epoch: 094, acc1: 75.942%, acc5: 93.046%, test_loss: 1.2669, per_image_load_time: 1.457ms, per_image_inference_time: 0.572ms
2022-10-12 10:55:16 - until epoch: 094, best_acc1: 75.942%
2022-10-12 10:55:16 - epoch 095 lr: 0.000896
2022-10-12 10:56:01 - train: epoch 0095, iter [00100, 05004], lr: 0.000890, loss: 3.1146
2022-10-12 10:56:39 - train: epoch 0095, iter [00200, 05004], lr: 0.000884, loss: 3.9347
2022-10-12 10:57:16 - train: epoch 0095, iter [00300, 05004], lr: 0.000878, loss: 3.1542
2022-10-12 10:57:53 - train: epoch 0095, iter [00400, 05004], lr: 0.000872, loss: 3.6445
2022-10-12 10:58:29 - train: epoch 0095, iter [00500, 05004], lr: 0.000866, loss: 2.9192
2022-10-12 10:59:06 - train: epoch 0095, iter [00600, 05004], lr: 0.000861, loss: 3.5903
2022-10-12 10:59:42 - train: epoch 0095, iter [00700, 05004], lr: 0.000855, loss: 4.0144
2022-10-12 11:00:19 - train: epoch 0095, iter [00800, 05004], lr: 0.000849, loss: 3.0510
2022-10-12 11:00:56 - train: epoch 0095, iter [00900, 05004], lr: 0.000843, loss: 3.9655
2022-10-12 11:01:33 - train: epoch 0095, iter [01000, 05004], lr: 0.000838, loss: 3.7122
2022-10-12 11:02:10 - train: epoch 0095, iter [01100, 05004], lr: 0.000832, loss: 3.4729
2022-10-12 11:02:47 - train: epoch 0095, iter [01200, 05004], lr: 0.000826, loss: 3.7853
2022-10-12 11:03:25 - train: epoch 0095, iter [01300, 05004], lr: 0.000821, loss: 3.4932
2022-10-12 11:04:02 - train: epoch 0095, iter [01400, 05004], lr: 0.000815, loss: 3.0536
2022-10-12 11:04:38 - train: epoch 0095, iter [01500, 05004], lr: 0.000810, loss: 2.5654
2022-10-12 11:05:15 - train: epoch 0095, iter [01600, 05004], lr: 0.000804, loss: 3.8482
2022-10-12 11:05:52 - train: epoch 0095, iter [01700, 05004], lr: 0.000798, loss: 4.0062
2022-10-12 11:06:29 - train: epoch 0095, iter [01800, 05004], lr: 0.000793, loss: 3.8132
2022-10-12 11:07:06 - train: epoch 0095, iter [01900, 05004], lr: 0.000787, loss: 3.4837
2022-10-12 11:07:43 - train: epoch 0095, iter [02000, 05004], lr: 0.000782, loss: 3.3868
2022-10-12 11:08:19 - train: epoch 0095, iter [02100, 05004], lr: 0.000776, loss: 2.7830
2022-10-12 11:08:57 - train: epoch 0095, iter [02200, 05004], lr: 0.000771, loss: 3.5463
2022-10-12 11:09:33 - train: epoch 0095, iter [02300, 05004], lr: 0.000765, loss: 3.6576
2022-10-12 11:10:11 - train: epoch 0095, iter [02400, 05004], lr: 0.000760, loss: 3.8847
2022-10-12 11:10:48 - train: epoch 0095, iter [02500, 05004], lr: 0.000755, loss: 3.9112
2022-10-12 11:11:25 - train: epoch 0095, iter [02600, 05004], lr: 0.000749, loss: 3.2925
2022-10-12 11:12:02 - train: epoch 0095, iter [02700, 05004], lr: 0.000744, loss: 2.8765
2022-10-12 11:12:39 - train: epoch 0095, iter [02800, 05004], lr: 0.000738, loss: 3.4137
2022-10-12 11:13:16 - train: epoch 0095, iter [02900, 05004], lr: 0.000733, loss: 2.7568
2022-10-12 11:13:53 - train: epoch 0095, iter [03000, 05004], lr: 0.000728, loss: 3.4177
2022-10-12 11:14:31 - train: epoch 0095, iter [03100, 05004], lr: 0.000723, loss: 4.1755
2022-10-12 11:15:07 - train: epoch 0095, iter [03200, 05004], lr: 0.000717, loss: 3.6395
2022-10-12 11:15:45 - train: epoch 0095, iter [03300, 05004], lr: 0.000712, loss: 3.0347
2022-10-12 11:16:22 - train: epoch 0095, iter [03400, 05004], lr: 0.000707, loss: 4.1765
2022-10-12 11:16:58 - train: epoch 0095, iter [03500, 05004], lr: 0.000702, loss: 2.6178
2022-10-12 11:17:36 - train: epoch 0095, iter [03600, 05004], lr: 0.000696, loss: 4.2170
2022-10-12 11:18:14 - train: epoch 0095, iter [03700, 05004], lr: 0.000691, loss: 3.3129
2022-10-12 11:18:51 - train: epoch 0095, iter [03800, 05004], lr: 0.000686, loss: 3.0728
2022-10-12 11:19:28 - train: epoch 0095, iter [03900, 05004], lr: 0.000681, loss: 3.5256
2022-10-12 11:20:05 - train: epoch 0095, iter [04000, 05004], lr: 0.000676, loss: 4.3082
2022-10-12 11:20:42 - train: epoch 0095, iter [04100, 05004], lr: 0.000671, loss: 2.9877
2022-10-12 11:21:18 - train: epoch 0095, iter [04200, 05004], lr: 0.000666, loss: 3.3872
2022-10-12 11:21:55 - train: epoch 0095, iter [04300, 05004], lr: 0.000661, loss: 3.1545
2022-10-12 11:22:32 - train: epoch 0095, iter [04400, 05004], lr: 0.000656, loss: 4.0932
2022-10-12 11:23:08 - train: epoch 0095, iter [04500, 05004], lr: 0.000651, loss: 3.3052
2022-10-12 11:23:45 - train: epoch 0095, iter [04600, 05004], lr: 0.000646, loss: 3.2292
2022-10-12 11:24:22 - train: epoch 0095, iter [04700, 05004], lr: 0.000641, loss: 3.0612
2022-10-12 11:24:59 - train: epoch 0095, iter [04800, 05004], lr: 0.000636, loss: 3.9558
2022-10-12 11:25:35 - train: epoch 0095, iter [04900, 05004], lr: 0.000631, loss: 2.8261
2022-10-12 11:26:10 - train: epoch 0095, iter [05000, 05004], lr: 0.000626, loss: 3.3686
2022-10-12 11:26:13 - train: epoch 095, train_loss: 3.4916
2022-10-12 11:27:36 - eval: epoch: 095, acc1: 76.078%, acc5: 93.134%, test_loss: 1.2511, per_image_load_time: 1.621ms, per_image_inference_time: 0.564ms
2022-10-12 11:27:36 - until epoch: 095, best_acc1: 76.078%
2022-10-12 11:27:36 - epoch 096 lr: 0.000626
2022-10-12 11:28:18 - train: epoch 0096, iter [00100, 05004], lr: 0.000621, loss: 3.1706
2022-10-12 11:28:57 - train: epoch 0096, iter [00200, 05004], lr: 0.000616, loss: 3.2837
2022-10-12 11:29:35 - train: epoch 0096, iter [00300, 05004], lr: 0.000611, loss: 3.4700
2022-10-12 11:30:12 - train: epoch 0096, iter [00400, 05004], lr: 0.000606, loss: 3.3937
2022-10-12 11:30:49 - train: epoch 0096, iter [00500, 05004], lr: 0.000601, loss: 3.1182
2022-10-12 11:31:25 - train: epoch 0096, iter [00600, 05004], lr: 0.000596, loss: 3.3797
2022-10-12 11:32:02 - train: epoch 0096, iter [00700, 05004], lr: 0.000592, loss: 4.3859
2022-10-12 11:32:37 - train: epoch 0096, iter [00800, 05004], lr: 0.000587, loss: 3.6129
2022-10-12 11:33:14 - train: epoch 0096, iter [00900, 05004], lr: 0.000582, loss: 3.4934
2022-10-12 11:33:51 - train: epoch 0096, iter [01000, 05004], lr: 0.000577, loss: 3.3739
2022-10-12 11:34:28 - train: epoch 0096, iter [01100, 05004], lr: 0.000573, loss: 3.5851
2022-10-12 11:35:03 - train: epoch 0096, iter [01200, 05004], lr: 0.000568, loss: 3.5923
2022-10-12 11:35:41 - train: epoch 0096, iter [01300, 05004], lr: 0.000563, loss: 4.3204
2022-10-12 11:36:16 - train: epoch 0096, iter [01400, 05004], lr: 0.000559, loss: 3.2373
2022-10-12 11:36:52 - train: epoch 0096, iter [01500, 05004], lr: 0.000554, loss: 3.0963
2022-10-12 11:37:29 - train: epoch 0096, iter [01600, 05004], lr: 0.000549, loss: 3.6165
2022-10-12 11:38:05 - train: epoch 0096, iter [01700, 05004], lr: 0.000545, loss: 3.0741
2022-10-12 11:38:42 - train: epoch 0096, iter [01800, 05004], lr: 0.000540, loss: 3.3845
2022-10-12 11:39:18 - train: epoch 0096, iter [01900, 05004], lr: 0.000536, loss: 3.5403
2022-10-12 11:39:55 - train: epoch 0096, iter [02000, 05004], lr: 0.000531, loss: 2.3460
2022-10-12 11:40:31 - train: epoch 0096, iter [02100, 05004], lr: 0.000527, loss: 3.3276
2022-10-12 11:41:08 - train: epoch 0096, iter [02200, 05004], lr: 0.000522, loss: 3.3655
2022-10-12 11:41:44 - train: epoch 0096, iter [02300, 05004], lr: 0.000518, loss: 3.5175
2022-10-12 11:42:21 - train: epoch 0096, iter [02400, 05004], lr: 0.000513, loss: 2.8630
2022-10-12 11:42:57 - train: epoch 0096, iter [02500, 05004], lr: 0.000509, loss: 3.9455
2022-10-12 11:43:33 - train: epoch 0096, iter [02600, 05004], lr: 0.000504, loss: 3.0579
2022-10-12 11:44:10 - train: epoch 0096, iter [02700, 05004], lr: 0.000500, loss: 4.0027
2022-10-12 11:44:47 - train: epoch 0096, iter [02800, 05004], lr: 0.000496, loss: 3.7781
2022-10-12 11:45:23 - train: epoch 0096, iter [02900, 05004], lr: 0.000491, loss: 4.0026
2022-10-12 11:46:01 - train: epoch 0096, iter [03000, 05004], lr: 0.000487, loss: 3.6052
2022-10-12 11:46:37 - train: epoch 0096, iter [03100, 05004], lr: 0.000483, loss: 3.1201
2022-10-12 11:47:14 - train: epoch 0096, iter [03200, 05004], lr: 0.000478, loss: 3.3064
2022-10-12 11:47:51 - train: epoch 0096, iter [03300, 05004], lr: 0.000474, loss: 3.2206
2022-10-12 11:48:28 - train: epoch 0096, iter [03400, 05004], lr: 0.000470, loss: 3.8151
2022-10-12 11:49:05 - train: epoch 0096, iter [03500, 05004], lr: 0.000466, loss: 3.9317
2022-10-12 11:49:43 - train: epoch 0096, iter [03600, 05004], lr: 0.000461, loss: 3.5501
2022-10-12 11:50:20 - train: epoch 0096, iter [03700, 05004], lr: 0.000457, loss: 3.0418
2022-10-12 11:50:56 - train: epoch 0096, iter [03800, 05004], lr: 0.000453, loss: 2.9094
2022-10-12 11:51:34 - train: epoch 0096, iter [03900, 05004], lr: 0.000449, loss: 3.3908
2022-10-12 11:52:10 - train: epoch 0096, iter [04000, 05004], lr: 0.000445, loss: 3.0057
2022-10-12 11:52:47 - train: epoch 0096, iter [04100, 05004], lr: 0.000441, loss: 3.1684
2022-10-12 11:53:23 - train: epoch 0096, iter [04200, 05004], lr: 0.000436, loss: 3.1241
2022-10-12 11:53:59 - train: epoch 0096, iter [04300, 05004], lr: 0.000432, loss: 3.4824
2022-10-12 11:54:36 - train: epoch 0096, iter [04400, 05004], lr: 0.000428, loss: 3.4534
2022-10-12 11:55:12 - train: epoch 0096, iter [04500, 05004], lr: 0.000424, loss: 3.6808
2022-10-12 11:55:48 - train: epoch 0096, iter [04600, 05004], lr: 0.000420, loss: 3.0350
2022-10-12 11:56:24 - train: epoch 0096, iter [04700, 05004], lr: 0.000416, loss: 3.5954
2022-10-12 11:57:01 - train: epoch 0096, iter [04800, 05004], lr: 0.000412, loss: 3.2332
2022-10-12 11:57:36 - train: epoch 0096, iter [04900, 05004], lr: 0.000408, loss: 3.5987
2022-10-12 11:58:11 - train: epoch 0096, iter [05000, 05004], lr: 0.000404, loss: 3.3947
2022-10-12 11:58:14 - train: epoch 096, train_loss: 3.4840
2022-10-12 11:59:34 - eval: epoch: 096, acc1: 76.166%, acc5: 93.160%, test_loss: 1.2382, per_image_load_time: 1.705ms, per_image_inference_time: 0.586ms
2022-10-12 11:59:34 - until epoch: 096, best_acc1: 76.166%
2022-10-12 11:59:34 - epoch 097 lr: 0.000404
2022-10-12 12:00:17 - train: epoch 0097, iter [00100, 05004], lr: 0.000400, loss: 2.8445
2022-10-12 12:00:54 - train: epoch 0097, iter [00200, 05004], lr: 0.000396, loss: 3.4968
2022-10-12 12:01:31 - train: epoch 0097, iter [00300, 05004], lr: 0.000393, loss: 3.4640
2022-10-12 12:02:08 - train: epoch 0097, iter [00400, 05004], lr: 0.000389, loss: 2.5849
2022-10-12 12:02:44 - train: epoch 0097, iter [00500, 05004], lr: 0.000385, loss: 3.3083
2022-10-12 12:03:20 - train: epoch 0097, iter [00600, 05004], lr: 0.000381, loss: 3.6972
2022-10-12 12:03:57 - train: epoch 0097, iter [00700, 05004], lr: 0.000377, loss: 3.8051
2022-10-12 12:04:33 - train: epoch 0097, iter [00800, 05004], lr: 0.000373, loss: 3.6620
2022-10-12 12:05:10 - train: epoch 0097, iter [00900, 05004], lr: 0.000370, loss: 3.4578
2022-10-12 12:05:47 - train: epoch 0097, iter [01000, 05004], lr: 0.000366, loss: 3.3572
2022-10-12 12:06:24 - train: epoch 0097, iter [01100, 05004], lr: 0.000362, loss: 3.5956
2022-10-12 12:07:00 - train: epoch 0097, iter [01200, 05004], lr: 0.000358, loss: 4.0548
2022-10-12 12:07:37 - train: epoch 0097, iter [01300, 05004], lr: 0.000355, loss: 3.4697
2022-10-12 12:08:13 - train: epoch 0097, iter [01400, 05004], lr: 0.000351, loss: 3.6602
2022-10-12 12:08:49 - train: epoch 0097, iter [01500, 05004], lr: 0.000347, loss: 2.9562
2022-10-12 12:09:26 - train: epoch 0097, iter [01600, 05004], lr: 0.000344, loss: 3.4424
2022-10-12 12:10:03 - train: epoch 0097, iter [01700, 05004], lr: 0.000340, loss: 3.6025
2022-10-12 12:10:40 - train: epoch 0097, iter [01800, 05004], lr: 0.000337, loss: 3.3575
2022-10-12 12:11:17 - train: epoch 0097, iter [01900, 05004], lr: 0.000333, loss: 3.0624
2022-10-12 12:11:54 - train: epoch 0097, iter [02000, 05004], lr: 0.000329, loss: 4.0525
2022-10-12 12:12:30 - train: epoch 0097, iter [02100, 05004], lr: 0.000326, loss: 3.3414
2022-10-12 12:13:06 - train: epoch 0097, iter [02200, 05004], lr: 0.000322, loss: 3.7003
2022-10-12 12:13:43 - train: epoch 0097, iter [02300, 05004], lr: 0.000319, loss: 3.0957
2022-10-12 12:14:19 - train: epoch 0097, iter [02400, 05004], lr: 0.000315, loss: 3.6939
2022-10-12 12:14:56 - train: epoch 0097, iter [02500, 05004], lr: 0.000312, loss: 3.5272
2022-10-12 12:15:33 - train: epoch 0097, iter [02600, 05004], lr: 0.000309, loss: 3.5207
2022-10-12 12:16:09 - train: epoch 0097, iter [02700, 05004], lr: 0.000305, loss: 3.9692
2022-10-12 12:16:46 - train: epoch 0097, iter [02800, 05004], lr: 0.000302, loss: 2.7600
2022-10-12 12:17:23 - train: epoch 0097, iter [02900, 05004], lr: 0.000298, loss: 3.5324
2022-10-12 12:17:59 - train: epoch 0097, iter [03000, 05004], lr: 0.000295, loss: 3.1540
2022-10-12 12:18:37 - train: epoch 0097, iter [03100, 05004], lr: 0.000292, loss: 3.1016
2022-10-12 12:19:14 - train: epoch 0097, iter [03200, 05004], lr: 0.000288, loss: 3.5608
2022-10-12 12:19:50 - train: epoch 0097, iter [03300, 05004], lr: 0.000285, loss: 3.3820
2022-10-12 12:20:27 - train: epoch 0097, iter [03400, 05004], lr: 0.000282, loss: 3.0250
2022-10-12 12:21:03 - train: epoch 0097, iter [03500, 05004], lr: 0.000279, loss: 3.9464
2022-10-12 12:21:40 - train: epoch 0097, iter [03600, 05004], lr: 0.000275, loss: 3.6423
2022-10-12 12:22:17 - train: epoch 0097, iter [03700, 05004], lr: 0.000272, loss: 3.1526
2022-10-12 12:22:54 - train: epoch 0097, iter [03800, 05004], lr: 0.000269, loss: 3.9479
2022-10-12 12:23:30 - train: epoch 0097, iter [03900, 05004], lr: 0.000266, loss: 4.2464
2022-10-12 12:24:06 - train: epoch 0097, iter [04000, 05004], lr: 0.000263, loss: 3.5421
2022-10-12 12:24:43 - train: epoch 0097, iter [04100, 05004], lr: 0.000259, loss: 4.0421
2022-10-12 12:25:20 - train: epoch 0097, iter [04200, 05004], lr: 0.000256, loss: 4.0997
2022-10-12 12:25:57 - train: epoch 0097, iter [04300, 05004], lr: 0.000253, loss: 3.6371
2022-10-12 12:26:34 - train: epoch 0097, iter [04400, 05004], lr: 0.000250, loss: 4.0241
2022-10-12 12:27:11 - train: epoch 0097, iter [04500, 05004], lr: 0.000247, loss: 4.1750
2022-10-12 12:27:47 - train: epoch 0097, iter [04600, 05004], lr: 0.000244, loss: 3.7232
2022-10-12 12:28:24 - train: epoch 0097, iter [04700, 05004], lr: 0.000241, loss: 2.3215
2022-10-12 12:29:02 - train: epoch 0097, iter [04800, 05004], lr: 0.000238, loss: 3.5551
2022-10-12 12:29:38 - train: epoch 0097, iter [04900, 05004], lr: 0.000235, loss: 3.2478
2022-10-12 12:30:13 - train: epoch 0097, iter [05000, 05004], lr: 0.000232, loss: 3.0433
2022-10-12 12:30:15 - train: epoch 097, train_loss: 3.4638
2022-10-12 12:31:35 - eval: epoch: 097, acc1: 76.270%, acc5: 93.200%, test_loss: 1.2529, per_image_load_time: 1.132ms, per_image_inference_time: 0.590ms
2022-10-12 12:31:36 - until epoch: 097, best_acc1: 76.270%
2022-10-12 12:31:36 - epoch 098 lr: 0.000232
2022-10-12 12:32:19 - train: epoch 0098, iter [00100, 05004], lr: 0.000229, loss: 3.2754
2022-10-12 12:32:56 - train: epoch 0098, iter [00200, 05004], lr: 0.000226, loss: 3.1701
2022-10-12 12:33:34 - train: epoch 0098, iter [00300, 05004], lr: 0.000223, loss: 3.2802
2022-10-12 12:34:09 - train: epoch 0098, iter [00400, 05004], lr: 0.000220, loss: 3.7057
2022-10-12 12:34:47 - train: epoch 0098, iter [00500, 05004], lr: 0.000217, loss: 3.7697
2022-10-12 12:35:24 - train: epoch 0098, iter [00600, 05004], lr: 0.000215, loss: 4.0152
2022-10-12 12:36:02 - train: epoch 0098, iter [00700, 05004], lr: 0.000212, loss: 3.6290
2022-10-12 12:36:38 - train: epoch 0098, iter [00800, 05004], lr: 0.000209, loss: 3.3188
2022-10-12 12:37:15 - train: epoch 0098, iter [00900, 05004], lr: 0.000206, loss: 4.0709
2022-10-12 12:37:52 - train: epoch 0098, iter [01000, 05004], lr: 0.000203, loss: 3.2290
2022-10-12 12:38:29 - train: epoch 0098, iter [01100, 05004], lr: 0.000201, loss: 3.1070
2022-10-12 12:39:05 - train: epoch 0098, iter [01200, 05004], lr: 0.000198, loss: 2.9103
2022-10-12 12:39:41 - train: epoch 0098, iter [01300, 05004], lr: 0.000195, loss: 3.2115
2022-10-12 12:40:18 - train: epoch 0098, iter [01400, 05004], lr: 0.000192, loss: 3.6439
2022-10-12 12:40:54 - train: epoch 0098, iter [01500, 05004], lr: 0.000190, loss: 3.4447
2022-10-12 12:41:30 - train: epoch 0098, iter [01600, 05004], lr: 0.000187, loss: 3.5603
2022-10-12 12:42:07 - train: epoch 0098, iter [01700, 05004], lr: 0.000185, loss: 3.9621
2022-10-12 12:42:44 - train: epoch 0098, iter [01800, 05004], lr: 0.000182, loss: 4.0403
2022-10-12 12:43:20 - train: epoch 0098, iter [01900, 05004], lr: 0.000179, loss: 3.1339
2022-10-12 12:43:56 - train: epoch 0098, iter [02000, 05004], lr: 0.000177, loss: 2.9879
2022-10-12 12:44:33 - train: epoch 0098, iter [02100, 05004], lr: 0.000174, loss: 3.8049
2022-10-12 12:45:10 - train: epoch 0098, iter [02200, 05004], lr: 0.000172, loss: 3.3701
2022-10-12 12:45:48 - train: epoch 0098, iter [02300, 05004], lr: 0.000169, loss: 2.8263
2022-10-12 12:46:25 - train: epoch 0098, iter [02400, 05004], lr: 0.000167, loss: 3.9146
2022-10-12 12:47:01 - train: epoch 0098, iter [02500, 05004], lr: 0.000164, loss: 2.9777
2022-10-12 12:47:38 - train: epoch 0098, iter [02600, 05004], lr: 0.000162, loss: 3.4685
2022-10-12 12:48:15 - train: epoch 0098, iter [02700, 05004], lr: 0.000159, loss: 3.3457
2022-10-12 12:48:52 - train: epoch 0098, iter [02800, 05004], lr: 0.000157, loss: 2.6416
2022-10-12 12:49:28 - train: epoch 0098, iter [02900, 05004], lr: 0.000154, loss: 3.3258
2022-10-12 12:50:05 - train: epoch 0098, iter [03000, 05004], lr: 0.000152, loss: 3.4847
2022-10-12 12:50:43 - train: epoch 0098, iter [03100, 05004], lr: 0.000150, loss: 3.0469
2022-10-12 12:51:19 - train: epoch 0098, iter [03200, 05004], lr: 0.000147, loss: 2.9118
2022-10-12 12:51:55 - train: epoch 0098, iter [03300, 05004], lr: 0.000145, loss: 2.8075
2022-10-12 12:52:32 - train: epoch 0098, iter [03400, 05004], lr: 0.000143, loss: 4.3033
2022-10-12 12:53:09 - train: epoch 0098, iter [03500, 05004], lr: 0.000141, loss: 3.0691
2022-10-12 12:53:46 - train: epoch 0098, iter [03600, 05004], lr: 0.000138, loss: 3.5407
2022-10-12 12:54:23 - train: epoch 0098, iter [03700, 05004], lr: 0.000136, loss: 3.6502
2022-10-12 12:54:58 - train: epoch 0098, iter [03800, 05004], lr: 0.000134, loss: 3.4336
2022-10-12 12:55:35 - train: epoch 0098, iter [03900, 05004], lr: 0.000132, loss: 3.7669
2022-10-12 12:56:12 - train: epoch 0098, iter [04000, 05004], lr: 0.000129, loss: 3.1300
2022-10-12 12:56:49 - train: epoch 0098, iter [04100, 05004], lr: 0.000127, loss: 3.4943
2022-10-12 12:57:26 - train: epoch 0098, iter [04200, 05004], lr: 0.000125, loss: 3.8454
2022-10-12 12:58:03 - train: epoch 0098, iter [04300, 05004], lr: 0.000123, loss: 2.9567
2022-10-12 12:58:40 - train: epoch 0098, iter [04400, 05004], lr: 0.000121, loss: 3.5795
2022-10-12 12:59:17 - train: epoch 0098, iter [04500, 05004], lr: 0.000119, loss: 3.4195
2022-10-12 12:59:54 - train: epoch 0098, iter [04600, 05004], lr: 0.000117, loss: 3.4837
2022-10-12 13:00:30 - train: epoch 0098, iter [04700, 05004], lr: 0.000115, loss: 2.7954
2022-10-12 13:01:07 - train: epoch 0098, iter [04800, 05004], lr: 0.000113, loss: 2.9228
2022-10-12 13:01:44 - train: epoch 0098, iter [04900, 05004], lr: 0.000111, loss: 3.4918
2022-10-12 13:02:18 - train: epoch 0098, iter [05000, 05004], lr: 0.000109, loss: 4.1057
2022-10-12 13:02:20 - train: epoch 098, train_loss: 3.4759
2022-10-12 13:03:41 - eval: epoch: 098, acc1: 76.298%, acc5: 93.190%, test_loss: 1.2635, per_image_load_time: 1.429ms, per_image_inference_time: 0.589ms
2022-10-12 13:03:42 - until epoch: 098, best_acc1: 76.298%
2022-10-12 13:03:42 - epoch 099 lr: 0.000109
2022-10-12 13:04:25 - train: epoch 0099, iter [00100, 05004], lr: 0.000107, loss: 3.0689
2022-10-12 13:05:02 - train: epoch 0099, iter [00200, 05004], lr: 0.000105, loss: 3.1060
2022-10-12 13:05:38 - train: epoch 0099, iter [00300, 05004], lr: 0.000103, loss: 3.6085
2022-10-12 13:06:16 - train: epoch 0099, iter [00400, 05004], lr: 0.000101, loss: 2.7706
2022-10-12 13:06:52 - train: epoch 0099, iter [00500, 05004], lr: 0.000099, loss: 3.5481
2022-10-12 13:07:28 - train: epoch 0099, iter [00600, 05004], lr: 0.000097, loss: 3.1637
2022-10-12 13:08:04 - train: epoch 0099, iter [00700, 05004], lr: 0.000095, loss: 3.8551
2022-10-12 13:08:41 - train: epoch 0099, iter [00800, 05004], lr: 0.000094, loss: 3.7913
2022-10-12 13:09:18 - train: epoch 0099, iter [00900, 05004], lr: 0.000092, loss: 3.9283
2022-10-12 13:09:54 - train: epoch 0099, iter [01000, 05004], lr: 0.000090, loss: 3.1104
2022-10-12 13:10:32 - train: epoch 0099, iter [01100, 05004], lr: 0.000088, loss: 4.0676
2022-10-12 13:11:08 - train: epoch 0099, iter [01200, 05004], lr: 0.000086, loss: 2.9695
2022-10-12 13:11:44 - train: epoch 0099, iter [01300, 05004], lr: 0.000085, loss: 3.7039
2022-10-12 13:12:20 - train: epoch 0099, iter [01400, 05004], lr: 0.000083, loss: 3.8382
2022-10-12 13:12:57 - train: epoch 0099, iter [01500, 05004], lr: 0.000081, loss: 3.5497
2022-10-12 13:13:33 - train: epoch 0099, iter [01600, 05004], lr: 0.000080, loss: 3.5132
2022-10-12 13:14:09 - train: epoch 0099, iter [01700, 05004], lr: 0.000078, loss: 3.6360
2022-10-12 13:14:46 - train: epoch 0099, iter [01800, 05004], lr: 0.000076, loss: 3.7592
2022-10-12 13:15:23 - train: epoch 0099, iter [01900, 05004], lr: 0.000075, loss: 4.3678
2022-10-12 13:16:00 - train: epoch 0099, iter [02000, 05004], lr: 0.000073, loss: 3.2165
2022-10-12 13:16:37 - train: epoch 0099, iter [02100, 05004], lr: 0.000072, loss: 3.7615
2022-10-12 13:17:13 - train: epoch 0099, iter [02200, 05004], lr: 0.000070, loss: 3.8727
2022-10-12 13:17:49 - train: epoch 0099, iter [02300, 05004], lr: 0.000069, loss: 3.5577
2022-10-12 13:18:26 - train: epoch 0099, iter [02400, 05004], lr: 0.000067, loss: 4.5562
2022-10-12 13:19:03 - train: epoch 0099, iter [02500, 05004], lr: 0.000066, loss: 3.2447
2022-10-12 13:19:40 - train: epoch 0099, iter [02600, 05004], lr: 0.000064, loss: 2.5476
2022-10-12 13:20:16 - train: epoch 0099, iter [02700, 05004], lr: 0.000063, loss: 4.0613
2022-10-12 13:20:53 - train: epoch 0099, iter [02800, 05004], lr: 0.000061, loss: 3.6705
2022-10-12 13:21:30 - train: epoch 0099, iter [02900, 05004], lr: 0.000060, loss: 3.8411
2022-10-12 13:22:06 - train: epoch 0099, iter [03000, 05004], lr: 0.000058, loss: 4.3389
2022-10-12 13:22:43 - train: epoch 0099, iter [03100, 05004], lr: 0.000057, loss: 3.3981
2022-10-12 13:23:20 - train: epoch 0099, iter [03200, 05004], lr: 0.000056, loss: 3.7674
2022-10-12 13:23:56 - train: epoch 0099, iter [03300, 05004], lr: 0.000054, loss: 3.1950
2022-10-12 13:24:33 - train: epoch 0099, iter [03400, 05004], lr: 0.000053, loss: 3.6752
2022-10-12 13:25:09 - train: epoch 0099, iter [03500, 05004], lr: 0.000052, loss: 3.0051
2022-10-12 13:25:46 - train: epoch 0099, iter [03600, 05004], lr: 0.000050, loss: 4.0141
2022-10-12 13:26:23 - train: epoch 0099, iter [03700, 05004], lr: 0.000049, loss: 3.0433
2022-10-12 13:26:59 - train: epoch 0099, iter [03800, 05004], lr: 0.000048, loss: 3.0868
2022-10-12 13:27:36 - train: epoch 0099, iter [03900, 05004], lr: 0.000047, loss: 2.8863
2022-10-12 13:28:14 - train: epoch 0099, iter [04000, 05004], lr: 0.000046, loss: 3.9800
2022-10-12 13:28:50 - train: epoch 0099, iter [04100, 05004], lr: 0.000044, loss: 3.1846
2022-10-12 13:29:27 - train: epoch 0099, iter [04200, 05004], lr: 0.000043, loss: 3.5471
2022-10-12 13:30:05 - train: epoch 0099, iter [04300, 05004], lr: 0.000042, loss: 3.6616
2022-10-12 13:30:40 - train: epoch 0099, iter [04400, 05004], lr: 0.000041, loss: 3.8951
2022-10-12 13:31:17 - train: epoch 0099, iter [04500, 05004], lr: 0.000040, loss: 3.5031
2022-10-12 13:31:54 - train: epoch 0099, iter [04600, 05004], lr: 0.000039, loss: 3.4493
2022-10-12 13:32:30 - train: epoch 0099, iter [04700, 05004], lr: 0.000038, loss: 2.9905
2022-10-12 13:33:07 - train: epoch 0099, iter [04800, 05004], lr: 0.000037, loss: 3.6572
2022-10-12 13:33:43 - train: epoch 0099, iter [04900, 05004], lr: 0.000036, loss: 3.5453
2022-10-12 13:34:18 - train: epoch 0099, iter [05000, 05004], lr: 0.000035, loss: 3.1504
2022-10-12 13:34:20 - train: epoch 099, train_loss: 3.4641
2022-10-12 13:35:40 - eval: epoch: 099, acc1: 76.440%, acc5: 93.268%, test_loss: 1.2109, per_image_load_time: 1.320ms, per_image_inference_time: 0.596ms
2022-10-12 13:35:41 - until epoch: 099, best_acc1: 76.440%
2022-10-12 13:35:41 - epoch 100 lr: 0.000035
2022-10-12 13:36:24 - train: epoch 0100, iter [00100, 05004], lr: 0.000034, loss: 3.4916
2022-10-12 13:37:01 - train: epoch 0100, iter [00200, 05004], lr: 0.000033, loss: 3.2521
2022-10-12 13:37:38 - train: epoch 0100, iter [00300, 05004], lr: 0.000032, loss: 2.9015
2022-10-12 13:38:16 - train: epoch 0100, iter [00400, 05004], lr: 0.000031, loss: 2.4393
2022-10-12 13:38:52 - train: epoch 0100, iter [00500, 05004], lr: 0.000030, loss: 4.1325
2022-10-12 13:39:28 - train: epoch 0100, iter [00600, 05004], lr: 0.000029, loss: 2.7777
2022-10-12 13:40:05 - train: epoch 0100, iter [00700, 05004], lr: 0.000028, loss: 3.1637
2022-10-12 13:40:42 - train: epoch 0100, iter [00800, 05004], lr: 0.000027, loss: 2.6290
2022-10-12 13:41:18 - train: epoch 0100, iter [00900, 05004], lr: 0.000027, loss: 3.3440
2022-10-12 13:41:55 - train: epoch 0100, iter [01000, 05004], lr: 0.000026, loss: 3.0777
2022-10-12 13:42:31 - train: epoch 0100, iter [01100, 05004], lr: 0.000025, loss: 3.2571
2022-10-12 13:43:08 - train: epoch 0100, iter [01200, 05004], lr: 0.000024, loss: 3.7639
2022-10-12 13:43:44 - train: epoch 0100, iter [01300, 05004], lr: 0.000024, loss: 3.4693
2022-10-12 13:44:21 - train: epoch 0100, iter [01400, 05004], lr: 0.000023, loss: 3.1734
2022-10-12 13:44:57 - train: epoch 0100, iter [01500, 05004], lr: 0.000022, loss: 3.5868
2022-10-12 13:45:33 - train: epoch 0100, iter [01600, 05004], lr: 0.000021, loss: 3.1010
2022-10-12 13:46:10 - train: epoch 0100, iter [01700, 05004], lr: 0.000021, loss: 2.8566
2022-10-12 13:46:46 - train: epoch 0100, iter [01800, 05004], lr: 0.000020, loss: 2.9677
2022-10-12 13:47:23 - train: epoch 0100, iter [01900, 05004], lr: 0.000019, loss: 3.9859
2022-10-12 13:47:59 - train: epoch 0100, iter [02000, 05004], lr: 0.000019, loss: 3.1233
2022-10-12 13:48:36 - train: epoch 0100, iter [02100, 05004], lr: 0.000018, loss: 3.4514
2022-10-12 13:49:13 - train: epoch 0100, iter [02200, 05004], lr: 0.000018, loss: 3.2432
2022-10-12 13:49:49 - train: epoch 0100, iter [02300, 05004], lr: 0.000017, loss: 3.1545
2022-10-12 13:50:25 - train: epoch 0100, iter [02400, 05004], lr: 0.000017, loss: 3.7630
2022-10-12 13:51:01 - train: epoch 0100, iter [02500, 05004], lr: 0.000016, loss: 4.2421
2022-10-12 13:51:38 - train: epoch 0100, iter [02600, 05004], lr: 0.000016, loss: 3.3643
2022-10-12 13:52:14 - train: epoch 0100, iter [02700, 05004], lr: 0.000015, loss: 3.8246
2022-10-12 13:52:50 - train: epoch 0100, iter [02800, 05004], lr: 0.000015, loss: 3.6521
2022-10-12 13:53:27 - train: epoch 0100, iter [02900, 05004], lr: 0.000014, loss: 2.9343
2022-10-12 13:54:04 - train: epoch 0100, iter [03000, 05004], lr: 0.000014, loss: 3.8008
2022-10-12 13:54:40 - train: epoch 0100, iter [03100, 05004], lr: 0.000014, loss: 3.3592
2022-10-12 13:55:17 - train: epoch 0100, iter [03200, 05004], lr: 0.000013, loss: 4.1728
2022-10-12 13:55:53 - train: epoch 0100, iter [03300, 05004], lr: 0.000013, loss: 4.0510
2022-10-12 13:56:29 - train: epoch 0100, iter [03400, 05004], lr: 0.000013, loss: 2.9503
2022-10-12 13:57:06 - train: epoch 0100, iter [03500, 05004], lr: 0.000012, loss: 3.5225
2022-10-12 13:57:43 - train: epoch 0100, iter [03600, 05004], lr: 0.000012, loss: 3.1029
2022-10-12 13:58:19 - train: epoch 0100, iter [03700, 05004], lr: 0.000012, loss: 3.3487
2022-10-12 13:58:56 - train: epoch 0100, iter [03800, 05004], lr: 0.000011, loss: 3.4332
2022-10-12 13:59:33 - train: epoch 0100, iter [03900, 05004], lr: 0.000011, loss: 3.9939
2022-10-12 14:00:09 - train: epoch 0100, iter [04000, 05004], lr: 0.000011, loss: 3.6316
2022-10-12 14:00:45 - train: epoch 0100, iter [04100, 05004], lr: 0.000011, loss: 3.0739
2022-10-12 14:01:22 - train: epoch 0100, iter [04200, 05004], lr: 0.000011, loss: 3.9118
2022-10-12 14:01:58 - train: epoch 0100, iter [04300, 05004], lr: 0.000010, loss: 3.3831
2022-10-12 14:02:35 - train: epoch 0100, iter [04400, 05004], lr: 0.000010, loss: 3.6048
2022-10-12 14:03:12 - train: epoch 0100, iter [04500, 05004], lr: 0.000010, loss: 3.6831
2022-10-12 14:03:48 - train: epoch 0100, iter [04600, 05004], lr: 0.000010, loss: 3.2399
2022-10-12 14:04:24 - train: epoch 0100, iter [04700, 05004], lr: 0.000010, loss: 3.7535
2022-10-12 14:05:01 - train: epoch 0100, iter [04800, 05004], lr: 0.000010, loss: 2.4436
2022-10-12 14:05:37 - train: epoch 0100, iter [04900, 05004], lr: 0.000010, loss: 3.8808
2022-10-12 14:06:12 - train: epoch 0100, iter [05000, 05004], lr: 0.000010, loss: 3.6207
2022-10-12 14:06:14 - train: epoch 100, train_loss: 3.4586
2022-10-12 14:07:35 - eval: epoch: 100, acc1: 76.360%, acc5: 93.258%, test_loss: 1.2115, per_image_load_time: 1.494ms, per_image_inference_time: 0.574ms
2022-10-12 14:07:35 - until epoch: 100, best_acc1: 76.440%
2022-10-12 14:07:35 - train done. model: resnet50, train time: 53.510 hours, best_acc1: 76.440%
