2022-10-10 08:36:24 - network: resnet50
2022-10-10 08:36:24 - num_classes: 1000
2022-10-10 08:36:24 - input_image_size: 224
2022-10-10 08:36:24 - scale: 1.1428571428571428
2022-10-10 08:36:24 - trained_model_path: 
2022-10-10 08:36:24 - train_criterion: OneHotLabelCELoss()
2022-10-10 08:36:24 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-10-10 08:36:24 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fcde4c7a1c0>
2022-10-10 08:36:24 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fcde4c7a490>
2022-10-10 08:36:24 - train_collater: <simpleAICV.classification.mixupcutmixclassificationcollator.MixupCutmixClassificationCollater object at 0x7fcde4c7a4c0>
2022-10-10 08:36:24 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fcde4c7a520>
2022-10-10 08:36:24 - seed: 0
2022-10-10 08:36:24 - batch_size: 256
2022-10-10 08:36:24 - num_workers: 20
2022-10-10 08:36:24 - accumulation_steps: 1
2022-10-10 08:36:24 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-10-10 08:36:24 - scheduler: ('CosineLR', {'warm_up_epochs': 0, 'min_lr': 1e-05})
2022-10-10 08:36:24 - epochs: 100
2022-10-10 08:36:24 - print_interval: 100
2022-10-10 08:36:24 - sync_bn: False
2022-10-10 08:36:24 - apex: True
2022-10-10 08:36:24 - use_ema_model: False
2022-10-10 08:36:24 - ema_model_decay: 0.9999
2022-10-10 08:36:24 - gpus_type: NVIDIA RTX A5000
2022-10-10 08:36:24 - gpus_num: 2
2022-10-10 08:36:24 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fcdada2c6f0>
2022-10-10 08:36:24 - --------------------parameters--------------------
2022-10-10 08:36:24 - name: conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-10-10 08:36:24 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-10-10 08:36:24 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-10-10 08:36:24 - name: fc.weight, grad: True
2022-10-10 08:36:24 - name: fc.bias, grad: True
2022-10-10 08:36:24 - --------------------buffers--------------------
2022-10-10 08:36:24 - name: conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-10-10 08:36:24 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-10-10 08:36:24 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-10-10 08:36:24 - -----------no weight decay layers--------------
2022-10-10 08:36:24 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-10 08:36:24 - -------------weight decay layers---------------
2022-10-10 08:36:24 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer1.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer2.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer3.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-10 08:36:24 - epoch 001 lr: 0.100000
2022-10-10 08:37:08 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9036
2022-10-10 08:37:44 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.9149
2022-10-10 08:38:21 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.9094
2022-10-10 08:38:57 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.9080
2022-10-10 08:39:35 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.8891
2022-10-10 08:40:12 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.8951
2022-10-10 08:40:48 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.9053
2022-10-10 08:41:24 - train: epoch 0001, iter [00800, 05004], lr: 0.099999, loss: 6.9109
2022-10-10 08:42:00 - train: epoch 0001, iter [00900, 05004], lr: 0.099999, loss: 6.9011
2022-10-10 08:42:37 - train: epoch 0001, iter [01000, 05004], lr: 0.099999, loss: 6.9048
2022-10-10 08:43:13 - train: epoch 0001, iter [01100, 05004], lr: 0.099999, loss: 6.9016
2022-10-10 08:43:50 - train: epoch 0001, iter [01200, 05004], lr: 0.099999, loss: 6.8827
2022-10-10 08:44:26 - train: epoch 0001, iter [01300, 05004], lr: 0.099998, loss: 6.8811
2022-10-10 08:45:03 - train: epoch 0001, iter [01400, 05004], lr: 0.099998, loss: 6.8942
2022-10-10 08:45:39 - train: epoch 0001, iter [01500, 05004], lr: 0.099998, loss: 6.8781
2022-10-10 08:46:16 - train: epoch 0001, iter [01600, 05004], lr: 0.099997, loss: 6.8683
2022-10-10 08:46:52 - train: epoch 0001, iter [01700, 05004], lr: 0.099997, loss: 6.8791
2022-10-10 08:47:29 - train: epoch 0001, iter [01800, 05004], lr: 0.099997, loss: 6.8410
2022-10-10 08:48:05 - train: epoch 0001, iter [01900, 05004], lr: 0.099996, loss: 6.8420
2022-10-10 08:48:41 - train: epoch 0001, iter [02000, 05004], lr: 0.099996, loss: 6.8645
2022-10-10 08:49:19 - train: epoch 0001, iter [02100, 05004], lr: 0.099996, loss: 6.8480
2022-10-10 08:49:55 - train: epoch 0001, iter [02200, 05004], lr: 0.099995, loss: 6.8113
2022-10-10 08:50:31 - train: epoch 0001, iter [02300, 05004], lr: 0.099995, loss: 6.8031
2022-10-10 08:51:07 - train: epoch 0001, iter [02400, 05004], lr: 0.099994, loss: 6.8541
2022-10-10 08:51:45 - train: epoch 0001, iter [02500, 05004], lr: 0.099994, loss: 6.7507
2022-10-10 08:52:21 - train: epoch 0001, iter [02600, 05004], lr: 0.099993, loss: 6.7140
2022-10-10 08:52:58 - train: epoch 0001, iter [02700, 05004], lr: 0.099993, loss: 6.8326
2022-10-10 08:53:35 - train: epoch 0001, iter [02800, 05004], lr: 0.099992, loss: 6.7392
2022-10-10 08:54:11 - train: epoch 0001, iter [02900, 05004], lr: 0.099992, loss: 6.6480
2022-10-10 08:54:47 - train: epoch 0001, iter [03000, 05004], lr: 0.099991, loss: 6.6763
2022-10-10 08:55:24 - train: epoch 0001, iter [03100, 05004], lr: 0.099991, loss: 6.6978
2022-10-10 08:56:01 - train: epoch 0001, iter [03200, 05004], lr: 0.099990, loss: 6.6547
2022-10-10 08:56:37 - train: epoch 0001, iter [03300, 05004], lr: 0.099989, loss: 6.6698
2022-10-10 08:57:14 - train: epoch 0001, iter [03400, 05004], lr: 0.099989, loss: 6.6165
2022-10-10 08:57:50 - train: epoch 0001, iter [03500, 05004], lr: 0.099988, loss: 6.6642
2022-10-10 08:58:27 - train: epoch 0001, iter [03600, 05004], lr: 0.099987, loss: 6.6056
2022-10-10 08:59:04 - train: epoch 0001, iter [03700, 05004], lr: 0.099987, loss: 6.6295
2022-10-10 08:59:40 - train: epoch 0001, iter [03800, 05004], lr: 0.099986, loss: 6.5703
2022-10-10 09:00:16 - train: epoch 0001, iter [03900, 05004], lr: 0.099985, loss: 6.4917
2022-10-10 09:00:54 - train: epoch 0001, iter [04000, 05004], lr: 0.099984, loss: 6.4047
2022-10-10 09:01:30 - train: epoch 0001, iter [04100, 05004], lr: 0.099983, loss: 6.5513
2022-10-10 09:02:07 - train: epoch 0001, iter [04200, 05004], lr: 0.099983, loss: 6.6054
2022-10-10 09:02:44 - train: epoch 0001, iter [04300, 05004], lr: 0.099982, loss: 6.4542
2022-10-10 09:03:21 - train: epoch 0001, iter [04400, 05004], lr: 0.099981, loss: 6.5509
2022-10-10 09:03:57 - train: epoch 0001, iter [04500, 05004], lr: 0.099980, loss: 6.4838
2022-10-10 09:04:33 - train: epoch 0001, iter [04600, 05004], lr: 0.099979, loss: 6.4084
2022-10-10 09:05:10 - train: epoch 0001, iter [04700, 05004], lr: 0.099978, loss: 6.5512
2022-10-10 09:05:46 - train: epoch 0001, iter [04800, 05004], lr: 0.099977, loss: 6.4898
2022-10-10 09:06:23 - train: epoch 0001, iter [04900, 05004], lr: 0.099976, loss: 6.3542
2022-10-10 09:06:57 - train: epoch 0001, iter [05000, 05004], lr: 0.099975, loss: 6.4885
2022-10-10 09:06:59 - train: epoch 001, train_loss: 6.7428
2022-10-10 09:08:18 - eval: epoch: 001, acc1: 3.182%, acc5: 10.516%, test_loss: 5.9808, per_image_load_time: 1.514ms, per_image_inference_time: 0.576ms
2022-10-10 09:08:18 - until epoch: 001, best_acc1: 3.182%
2022-10-10 09:08:18 - epoch 002 lr: 0.099975
2022-10-10 09:09:00 - train: epoch 0002, iter [00100, 05004], lr: 0.099974, loss: 6.4683
2022-10-10 09:09:37 - train: epoch 0002, iter [00200, 05004], lr: 0.099973, loss: 6.5478
2022-10-10 09:10:12 - train: epoch 0002, iter [00300, 05004], lr: 0.099972, loss: 6.5335
2022-10-10 09:10:48 - train: epoch 0002, iter [00400, 05004], lr: 0.099971, loss: 6.3494
2022-10-10 09:11:25 - train: epoch 0002, iter [00500, 05004], lr: 0.099970, loss: 6.3932
2022-10-10 09:12:00 - train: epoch 0002, iter [00600, 05004], lr: 0.099969, loss: 6.5668
2022-10-10 09:12:37 - train: epoch 0002, iter [00700, 05004], lr: 0.099968, loss: 6.4473
2022-10-10 09:13:13 - train: epoch 0002, iter [00800, 05004], lr: 0.099967, loss: 6.3444
2022-10-10 09:13:50 - train: epoch 0002, iter [00900, 05004], lr: 0.099966, loss: 6.2695
2022-10-10 09:14:26 - train: epoch 0002, iter [01000, 05004], lr: 0.099964, loss: 6.4375
2022-10-10 09:15:03 - train: epoch 0002, iter [01100, 05004], lr: 0.099963, loss: 6.2420
2022-10-10 09:15:39 - train: epoch 0002, iter [01200, 05004], lr: 0.099962, loss: 6.2755
2022-10-10 09:16:16 - train: epoch 0002, iter [01300, 05004], lr: 0.099961, loss: 6.3100
2022-10-10 09:16:53 - train: epoch 0002, iter [01400, 05004], lr: 0.099960, loss: 6.3049
2022-10-10 09:17:30 - train: epoch 0002, iter [01500, 05004], lr: 0.099958, loss: 6.2013
2022-10-10 09:18:06 - train: epoch 0002, iter [01600, 05004], lr: 0.099957, loss: 6.2849
2022-10-10 09:18:43 - train: epoch 0002, iter [01700, 05004], lr: 0.099956, loss: 6.4676
2022-10-10 09:19:20 - train: epoch 0002, iter [01800, 05004], lr: 0.099954, loss: 6.3401
2022-10-10 09:19:56 - train: epoch 0002, iter [01900, 05004], lr: 0.099953, loss: 6.2478
2022-10-10 09:20:33 - train: epoch 0002, iter [02000, 05004], lr: 0.099952, loss: 6.2861
2022-10-10 09:21:10 - train: epoch 0002, iter [02100, 05004], lr: 0.099950, loss: 6.3952
2022-10-10 09:21:47 - train: epoch 0002, iter [02200, 05004], lr: 0.099949, loss: 6.3035
2022-10-10 09:22:24 - train: epoch 0002, iter [02300, 05004], lr: 0.099947, loss: 6.4290
2022-10-10 09:23:00 - train: epoch 0002, iter [02400, 05004], lr: 0.099946, loss: 6.2640
2022-10-10 09:23:37 - train: epoch 0002, iter [02500, 05004], lr: 0.099945, loss: 6.0922
2022-10-10 09:24:13 - train: epoch 0002, iter [02600, 05004], lr: 0.099943, loss: 5.9772
2022-10-10 09:24:50 - train: epoch 0002, iter [02700, 05004], lr: 0.099942, loss: 6.1869
2022-10-10 09:25:26 - train: epoch 0002, iter [02800, 05004], lr: 0.099940, loss: 6.0938
2022-10-10 09:26:03 - train: epoch 0002, iter [02900, 05004], lr: 0.099938, loss: 6.0593
2022-10-10 09:26:39 - train: epoch 0002, iter [03000, 05004], lr: 0.099937, loss: 6.3014
2022-10-10 09:27:16 - train: epoch 0002, iter [03100, 05004], lr: 0.099935, loss: 6.2635
2022-10-10 09:27:53 - train: epoch 0002, iter [03200, 05004], lr: 0.099934, loss: 5.8813
2022-10-10 09:28:29 - train: epoch 0002, iter [03300, 05004], lr: 0.099932, loss: 6.1355
2022-10-10 09:29:06 - train: epoch 0002, iter [03400, 05004], lr: 0.099930, loss: 6.1802
2022-10-10 09:29:43 - train: epoch 0002, iter [03500, 05004], lr: 0.099929, loss: 6.4318
2022-10-10 09:30:20 - train: epoch 0002, iter [03600, 05004], lr: 0.099927, loss: 6.0842
2022-10-10 09:30:56 - train: epoch 0002, iter [03700, 05004], lr: 0.099925, loss: 6.1404
2022-10-10 09:31:32 - train: epoch 0002, iter [03800, 05004], lr: 0.099924, loss: 6.2151
2022-10-10 09:32:10 - train: epoch 0002, iter [03900, 05004], lr: 0.099922, loss: 5.9728
2022-10-10 09:32:46 - train: epoch 0002, iter [04000, 05004], lr: 0.099920, loss: 5.8660
2022-10-10 09:33:23 - train: epoch 0002, iter [04100, 05004], lr: 0.099918, loss: 6.2065
2022-10-10 09:34:00 - train: epoch 0002, iter [04200, 05004], lr: 0.099917, loss: 6.2776
2022-10-10 09:34:37 - train: epoch 0002, iter [04300, 05004], lr: 0.099915, loss: 6.2928
2022-10-10 09:35:14 - train: epoch 0002, iter [04400, 05004], lr: 0.099913, loss: 6.3251
2022-10-10 09:35:51 - train: epoch 0002, iter [04500, 05004], lr: 0.099911, loss: 5.8918
2022-10-10 09:36:27 - train: epoch 0002, iter [04600, 05004], lr: 0.099909, loss: 6.2238
2022-10-10 09:37:04 - train: epoch 0002, iter [04700, 05004], lr: 0.099907, loss: 6.3188
2022-10-10 09:37:40 - train: epoch 0002, iter [04800, 05004], lr: 0.099905, loss: 5.8456
2022-10-10 09:38:18 - train: epoch 0002, iter [04900, 05004], lr: 0.099903, loss: 6.0958
2022-10-10 09:38:52 - train: epoch 0002, iter [05000, 05004], lr: 0.099901, loss: 5.9923
2022-10-10 09:38:54 - train: epoch 002, train_loss: 6.2303
2022-10-10 09:40:13 - eval: epoch: 002, acc1: 10.846%, acc5: 27.978%, test_loss: 4.8390, per_image_load_time: 1.862ms, per_image_inference_time: 0.576ms
2022-10-10 09:40:14 - until epoch: 002, best_acc1: 10.846%
2022-10-10 09:40:14 - epoch 003 lr: 0.099901
2022-10-10 09:40:56 - train: epoch 0003, iter [00100, 05004], lr: 0.099899, loss: 6.0727
2022-10-10 09:41:32 - train: epoch 0003, iter [00200, 05004], lr: 0.099897, loss: 6.2334
2022-10-10 09:42:08 - train: epoch 0003, iter [00300, 05004], lr: 0.099895, loss: 6.1018
2022-10-10 09:42:44 - train: epoch 0003, iter [00400, 05004], lr: 0.099893, loss: 5.6971
2022-10-10 09:43:20 - train: epoch 0003, iter [00500, 05004], lr: 0.099891, loss: 5.9050
2022-10-10 09:43:56 - train: epoch 0003, iter [00600, 05004], lr: 0.099889, loss: 6.1613
2022-10-10 09:44:33 - train: epoch 0003, iter [00700, 05004], lr: 0.099887, loss: 5.9111
2022-10-10 09:45:10 - train: epoch 0003, iter [00800, 05004], lr: 0.099885, loss: 5.6800
2022-10-10 09:45:47 - train: epoch 0003, iter [00900, 05004], lr: 0.099883, loss: 5.9071
2022-10-10 09:46:23 - train: epoch 0003, iter [01000, 05004], lr: 0.099881, loss: 5.3540
2022-10-10 09:46:59 - train: epoch 0003, iter [01100, 05004], lr: 0.099878, loss: 5.8234
2022-10-10 09:47:36 - train: epoch 0003, iter [01200, 05004], lr: 0.099876, loss: 6.1264
2022-10-10 09:48:12 - train: epoch 0003, iter [01300, 05004], lr: 0.099874, loss: 5.8088
2022-10-10 09:48:49 - train: epoch 0003, iter [01400, 05004], lr: 0.099872, loss: 5.3488
2022-10-10 09:49:26 - train: epoch 0003, iter [01500, 05004], lr: 0.099870, loss: 6.2234
2022-10-10 09:50:02 - train: epoch 0003, iter [01600, 05004], lr: 0.099867, loss: 5.8348
2022-10-10 09:50:38 - train: epoch 0003, iter [01700, 05004], lr: 0.099865, loss: 5.6712
2022-10-10 09:51:15 - train: epoch 0003, iter [01800, 05004], lr: 0.099863, loss: 5.6259
2022-10-10 09:51:52 - train: epoch 0003, iter [01900, 05004], lr: 0.099860, loss: 5.9478
2022-10-10 09:52:29 - train: epoch 0003, iter [02000, 05004], lr: 0.099858, loss: 6.1102
2022-10-10 09:53:05 - train: epoch 0003, iter [02100, 05004], lr: 0.099856, loss: 6.0089
2022-10-10 09:53:42 - train: epoch 0003, iter [02200, 05004], lr: 0.099853, loss: 6.2752
2022-10-10 09:54:18 - train: epoch 0003, iter [02300, 05004], lr: 0.099851, loss: 5.8881
2022-10-10 09:54:55 - train: epoch 0003, iter [02400, 05004], lr: 0.099848, loss: 6.0299
2022-10-10 09:55:32 - train: epoch 0003, iter [02500, 05004], lr: 0.099846, loss: 5.8393
2022-10-10 09:56:08 - train: epoch 0003, iter [02600, 05004], lr: 0.099843, loss: 5.8517
2022-10-10 09:56:45 - train: epoch 0003, iter [02700, 05004], lr: 0.099841, loss: 6.1039
2022-10-10 09:57:22 - train: epoch 0003, iter [02800, 05004], lr: 0.099838, loss: 5.8938
2022-10-10 09:57:58 - train: epoch 0003, iter [02900, 05004], lr: 0.099836, loss: 6.0940
2022-10-10 09:58:36 - train: epoch 0003, iter [03000, 05004], lr: 0.099833, loss: 5.5541
2022-10-10 09:59:12 - train: epoch 0003, iter [03100, 05004], lr: 0.099831, loss: 5.7695
2022-10-10 09:59:48 - train: epoch 0003, iter [03200, 05004], lr: 0.099828, loss: 6.0028
2022-10-10 10:00:26 - train: epoch 0003, iter [03300, 05004], lr: 0.099826, loss: 5.6400
2022-10-10 10:01:03 - train: epoch 0003, iter [03400, 05004], lr: 0.099823, loss: 5.5827
2022-10-10 10:01:40 - train: epoch 0003, iter [03500, 05004], lr: 0.099820, loss: 5.5907
2022-10-10 10:02:16 - train: epoch 0003, iter [03600, 05004], lr: 0.099818, loss: 5.7818
2022-10-10 10:02:53 - train: epoch 0003, iter [03700, 05004], lr: 0.099815, loss: 5.7058
2022-10-10 10:03:30 - train: epoch 0003, iter [03800, 05004], lr: 0.099812, loss: 5.3919
2022-10-10 10:04:07 - train: epoch 0003, iter [03900, 05004], lr: 0.099810, loss: 6.0013
2022-10-10 10:04:43 - train: epoch 0003, iter [04000, 05004], lr: 0.099807, loss: 5.7496
2022-10-10 10:05:21 - train: epoch 0003, iter [04100, 05004], lr: 0.099804, loss: 5.4404
2022-10-10 10:05:57 - train: epoch 0003, iter [04200, 05004], lr: 0.099801, loss: 5.8838
2022-10-10 10:06:34 - train: epoch 0003, iter [04300, 05004], lr: 0.099798, loss: 5.4961
2022-10-10 10:07:11 - train: epoch 0003, iter [04400, 05004], lr: 0.099796, loss: 5.9000
2022-10-10 10:07:48 - train: epoch 0003, iter [04500, 05004], lr: 0.099793, loss: 5.9590
2022-10-10 10:08:25 - train: epoch 0003, iter [04600, 05004], lr: 0.099790, loss: 5.6174
2022-10-10 10:09:01 - train: epoch 0003, iter [04700, 05004], lr: 0.099787, loss: 5.8950
2022-10-10 10:09:38 - train: epoch 0003, iter [04800, 05004], lr: 0.099784, loss: 5.6755
2022-10-10 10:10:15 - train: epoch 0003, iter [04900, 05004], lr: 0.099781, loss: 5.6771
2022-10-10 10:10:49 - train: epoch 0003, iter [05000, 05004], lr: 0.099778, loss: 6.0471
2022-10-10 10:10:52 - train: epoch 003, train_loss: 5.8492
2022-10-10 10:12:10 - eval: epoch: 003, acc1: 19.486%, acc5: 41.836%, test_loss: 4.1045, per_image_load_time: 2.176ms, per_image_inference_time: 0.553ms
2022-10-10 10:12:11 - until epoch: 003, best_acc1: 19.486%
2022-10-10 10:12:11 - epoch 004 lr: 0.099778
2022-10-10 10:12:53 - train: epoch 0004, iter [00100, 05004], lr: 0.099775, loss: 6.0131
2022-10-10 10:13:30 - train: epoch 0004, iter [00200, 05004], lr: 0.099772, loss: 5.0839
2022-10-10 10:14:05 - train: epoch 0004, iter [00300, 05004], lr: 0.099769, loss: 5.9776
2022-10-10 10:14:42 - train: epoch 0004, iter [00400, 05004], lr: 0.099766, loss: 5.4886
2022-10-10 10:15:17 - train: epoch 0004, iter [00500, 05004], lr: 0.099763, loss: 5.0087
2022-10-10 10:15:54 - train: epoch 0004, iter [00600, 05004], lr: 0.099760, loss: 5.3839
2022-10-10 10:16:30 - train: epoch 0004, iter [00700, 05004], lr: 0.099757, loss: 5.7044
2022-10-10 10:17:07 - train: epoch 0004, iter [00800, 05004], lr: 0.099754, loss: 5.6872
2022-10-10 10:17:43 - train: epoch 0004, iter [00900, 05004], lr: 0.099751, loss: 5.3820
2022-10-10 10:18:19 - train: epoch 0004, iter [01000, 05004], lr: 0.099748, loss: 5.3756
2022-10-10 10:18:57 - train: epoch 0004, iter [01100, 05004], lr: 0.099744, loss: 5.8698
2022-10-10 10:19:33 - train: epoch 0004, iter [01200, 05004], lr: 0.099741, loss: 5.7201
2022-10-10 10:20:10 - train: epoch 0004, iter [01300, 05004], lr: 0.099738, loss: 5.6484
2022-10-10 10:20:47 - train: epoch 0004, iter [01400, 05004], lr: 0.099735, loss: 5.6323
2022-10-10 10:21:23 - train: epoch 0004, iter [01500, 05004], lr: 0.099732, loss: 5.8358
2022-10-10 10:22:00 - train: epoch 0004, iter [01600, 05004], lr: 0.099728, loss: 6.0091
2022-10-10 10:22:37 - train: epoch 0004, iter [01700, 05004], lr: 0.099725, loss: 5.8611
2022-10-10 10:23:14 - train: epoch 0004, iter [01800, 05004], lr: 0.099722, loss: 5.6481
2022-10-10 10:23:50 - train: epoch 0004, iter [01900, 05004], lr: 0.099718, loss: 5.3601
2022-10-10 10:24:27 - train: epoch 0004, iter [02000, 05004], lr: 0.099715, loss: 6.0182
2022-10-10 10:25:03 - train: epoch 0004, iter [02100, 05004], lr: 0.099712, loss: 5.4918
2022-10-10 10:25:39 - train: epoch 0004, iter [02200, 05004], lr: 0.099708, loss: 5.6709
2022-10-10 10:26:17 - train: epoch 0004, iter [02300, 05004], lr: 0.099705, loss: 5.1974
2022-10-10 10:26:53 - train: epoch 0004, iter [02400, 05004], lr: 0.099702, loss: 5.7020
2022-10-10 10:27:30 - train: epoch 0004, iter [02500, 05004], lr: 0.099698, loss: 5.3681
2022-10-10 10:28:07 - train: epoch 0004, iter [02600, 05004], lr: 0.099695, loss: 5.3162
2022-10-10 10:28:44 - train: epoch 0004, iter [02700, 05004], lr: 0.099691, loss: 5.3062
2022-10-10 10:29:20 - train: epoch 0004, iter [02800, 05004], lr: 0.099688, loss: 5.1794
2022-10-10 10:29:58 - train: epoch 0004, iter [02900, 05004], lr: 0.099684, loss: 5.7604
2022-10-10 10:30:34 - train: epoch 0004, iter [03000, 05004], lr: 0.099681, loss: 5.1877
2022-10-10 10:31:11 - train: epoch 0004, iter [03100, 05004], lr: 0.099677, loss: 5.4475
2022-10-10 10:31:47 - train: epoch 0004, iter [03200, 05004], lr: 0.099674, loss: 5.8037
2022-10-10 10:32:24 - train: epoch 0004, iter [03300, 05004], lr: 0.099670, loss: 5.4245
2022-10-10 10:33:01 - train: epoch 0004, iter [03400, 05004], lr: 0.099666, loss: 5.1921
2022-10-10 10:33:37 - train: epoch 0004, iter [03500, 05004], lr: 0.099663, loss: 5.5998
2022-10-10 10:34:14 - train: epoch 0004, iter [03600, 05004], lr: 0.099659, loss: 5.1375
2022-10-10 10:34:51 - train: epoch 0004, iter [03700, 05004], lr: 0.099655, loss: 5.0792
2022-10-10 10:35:27 - train: epoch 0004, iter [03800, 05004], lr: 0.099652, loss: 5.5269
2022-10-10 10:36:04 - train: epoch 0004, iter [03900, 05004], lr: 0.099648, loss: 5.5185
2022-10-10 10:36:41 - train: epoch 0004, iter [04000, 05004], lr: 0.099644, loss: 5.6779
2022-10-10 10:37:17 - train: epoch 0004, iter [04100, 05004], lr: 0.099641, loss: 5.3145
2022-10-10 10:37:55 - train: epoch 0004, iter [04200, 05004], lr: 0.099637, loss: 5.7232
2022-10-10 10:38:31 - train: epoch 0004, iter [04300, 05004], lr: 0.099633, loss: 5.6220
2022-10-10 10:39:08 - train: epoch 0004, iter [04400, 05004], lr: 0.099629, loss: 5.7199
2022-10-10 10:39:44 - train: epoch 0004, iter [04500, 05004], lr: 0.099625, loss: 5.6397
2022-10-10 10:40:21 - train: epoch 0004, iter [04600, 05004], lr: 0.099622, loss: 5.3658
2022-10-10 10:40:58 - train: epoch 0004, iter [04700, 05004], lr: 0.099618, loss: 5.6894
2022-10-10 10:41:34 - train: epoch 0004, iter [04800, 05004], lr: 0.099614, loss: 5.7382
2022-10-10 10:42:11 - train: epoch 0004, iter [04900, 05004], lr: 0.099610, loss: 5.4096
2022-10-10 10:42:46 - train: epoch 0004, iter [05000, 05004], lr: 0.099606, loss: 5.4204
2022-10-10 10:42:48 - train: epoch 004, train_loss: 5.5608
2022-10-10 10:44:06 - eval: epoch: 004, acc1: 27.694%, acc5: 52.986%, test_loss: 3.5272, per_image_load_time: 1.856ms, per_image_inference_time: 0.553ms
2022-10-10 10:44:06 - until epoch: 004, best_acc1: 27.694%
2022-10-10 10:44:06 - epoch 005 lr: 0.099606
2022-10-10 10:44:47 - train: epoch 0005, iter [00100, 05004], lr: 0.099602, loss: 5.5572
2022-10-10 10:45:25 - train: epoch 0005, iter [00200, 05004], lr: 0.099598, loss: 5.2290
2022-10-10 10:46:01 - train: epoch 0005, iter [00300, 05004], lr: 0.099594, loss: 5.7764
2022-10-10 10:46:37 - train: epoch 0005, iter [00400, 05004], lr: 0.099590, loss: 5.7311
2022-10-10 10:47:12 - train: epoch 0005, iter [00500, 05004], lr: 0.099586, loss: 5.1354
2022-10-10 10:47:48 - train: epoch 0005, iter [00600, 05004], lr: 0.099582, loss: 5.4756
2022-10-10 10:48:25 - train: epoch 0005, iter [00700, 05004], lr: 0.099578, loss: 5.1657
2022-10-10 10:49:01 - train: epoch 0005, iter [00800, 05004], lr: 0.099574, loss: 5.4005
2022-10-10 10:49:38 - train: epoch 0005, iter [00900, 05004], lr: 0.099570, loss: 5.2643
2022-10-10 10:50:14 - train: epoch 0005, iter [01000, 05004], lr: 0.099565, loss: 5.0820
2022-10-10 10:50:51 - train: epoch 0005, iter [01100, 05004], lr: 0.099561, loss: 5.7836
2022-10-10 10:51:28 - train: epoch 0005, iter [01200, 05004], lr: 0.099557, loss: 5.1046
2022-10-10 10:52:04 - train: epoch 0005, iter [01300, 05004], lr: 0.099553, loss: 4.6565
2022-10-10 10:52:40 - train: epoch 0005, iter [01400, 05004], lr: 0.099549, loss: 5.4510
2022-10-10 10:53:17 - train: epoch 0005, iter [01500, 05004], lr: 0.099545, loss: 4.4427
2022-10-10 10:53:54 - train: epoch 0005, iter [01600, 05004], lr: 0.099540, loss: 5.5396
2022-10-10 10:54:29 - train: epoch 0005, iter [01700, 05004], lr: 0.099536, loss: 5.3332
2022-10-10 10:55:06 - train: epoch 0005, iter [01800, 05004], lr: 0.099532, loss: 5.8023
2022-10-10 10:55:42 - train: epoch 0005, iter [01900, 05004], lr: 0.099528, loss: 5.6284
2022-10-10 10:56:20 - train: epoch 0005, iter [02000, 05004], lr: 0.099523, loss: 5.8662
2022-10-10 10:56:56 - train: epoch 0005, iter [02100, 05004], lr: 0.099519, loss: 5.5019
2022-10-10 10:57:33 - train: epoch 0005, iter [02200, 05004], lr: 0.099514, loss: 4.5846
2022-10-10 10:58:09 - train: epoch 0005, iter [02300, 05004], lr: 0.099510, loss: 5.0745
2022-10-10 10:58:47 - train: epoch 0005, iter [02400, 05004], lr: 0.099506, loss: 5.0832
2022-10-10 10:59:23 - train: epoch 0005, iter [02500, 05004], lr: 0.099501, loss: 5.3918
2022-10-10 10:59:59 - train: epoch 0005, iter [02600, 05004], lr: 0.099497, loss: 5.5446
2022-10-10 11:00:37 - train: epoch 0005, iter [02700, 05004], lr: 0.099492, loss: 5.5479
2022-10-10 11:01:13 - train: epoch 0005, iter [02800, 05004], lr: 0.099488, loss: 5.6168
2022-10-10 11:01:50 - train: epoch 0005, iter [02900, 05004], lr: 0.099483, loss: 5.5583
2022-10-10 11:02:27 - train: epoch 0005, iter [03000, 05004], lr: 0.099479, loss: 5.4941
2022-10-10 11:03:03 - train: epoch 0005, iter [03100, 05004], lr: 0.099474, loss: 5.5664
2022-10-10 11:03:39 - train: epoch 0005, iter [03200, 05004], lr: 0.099470, loss: 5.1398
2022-10-10 11:04:16 - train: epoch 0005, iter [03300, 05004], lr: 0.099465, loss: 5.0699
2022-10-10 11:04:53 - train: epoch 0005, iter [03400, 05004], lr: 0.099461, loss: 5.0877
2022-10-10 11:05:30 - train: epoch 0005, iter [03500, 05004], lr: 0.099456, loss: 5.3524
2022-10-10 11:06:06 - train: epoch 0005, iter [03600, 05004], lr: 0.099451, loss: 5.7220
2022-10-10 11:06:42 - train: epoch 0005, iter [03700, 05004], lr: 0.099447, loss: 5.4535
2022-10-10 11:07:19 - train: epoch 0005, iter [03800, 05004], lr: 0.099442, loss: 5.6823
2022-10-10 11:07:56 - train: epoch 0005, iter [03900, 05004], lr: 0.099437, loss: 4.6487
2022-10-10 11:08:33 - train: epoch 0005, iter [04000, 05004], lr: 0.099433, loss: 5.6850
2022-10-10 11:09:09 - train: epoch 0005, iter [04100, 05004], lr: 0.099428, loss: 5.3358
2022-10-10 11:09:45 - train: epoch 0005, iter [04200, 05004], lr: 0.099423, loss: 5.0590
2022-10-10 11:10:22 - train: epoch 0005, iter [04300, 05004], lr: 0.099419, loss: 5.5760
2022-10-10 11:10:59 - train: epoch 0005, iter [04400, 05004], lr: 0.099414, loss: 5.7380
2022-10-10 11:11:35 - train: epoch 0005, iter [04500, 05004], lr: 0.099409, loss: 4.6328
2022-10-10 11:12:12 - train: epoch 0005, iter [04600, 05004], lr: 0.099404, loss: 5.1548
2022-10-10 11:12:49 - train: epoch 0005, iter [04700, 05004], lr: 0.099399, loss: 4.8955
2022-10-10 11:13:25 - train: epoch 0005, iter [04800, 05004], lr: 0.099394, loss: 5.5121
2022-10-10 11:14:02 - train: epoch 0005, iter [04900, 05004], lr: 0.099390, loss: 5.5639
2022-10-10 11:14:36 - train: epoch 0005, iter [05000, 05004], lr: 0.099385, loss: 5.2863
2022-10-10 11:14:39 - train: epoch 005, train_loss: 5.3390
2022-10-10 11:15:57 - eval: epoch: 005, acc1: 32.636%, acc5: 58.476%, test_loss: 3.3007, per_image_load_time: 1.894ms, per_image_inference_time: 0.551ms
2022-10-10 11:15:58 - until epoch: 005, best_acc1: 32.636%
2022-10-10 11:15:58 - epoch 006 lr: 0.099384
2022-10-10 11:16:40 - train: epoch 0006, iter [00100, 05004], lr: 0.099380, loss: 5.4641
2022-10-10 11:17:17 - train: epoch 0006, iter [00200, 05004], lr: 0.099375, loss: 5.6448
2022-10-10 11:17:53 - train: epoch 0006, iter [00300, 05004], lr: 0.099370, loss: 5.2888
2022-10-10 11:18:29 - train: epoch 0006, iter [00400, 05004], lr: 0.099365, loss: 4.4958
2022-10-10 11:19:06 - train: epoch 0006, iter [00500, 05004], lr: 0.099360, loss: 5.0882
2022-10-10 11:19:42 - train: epoch 0006, iter [00600, 05004], lr: 0.099355, loss: 5.6759
2022-10-10 11:20:18 - train: epoch 0006, iter [00700, 05004], lr: 0.099350, loss: 5.1695
2022-10-10 11:20:55 - train: epoch 0006, iter [00800, 05004], lr: 0.099345, loss: 5.0891
2022-10-10 11:21:32 - train: epoch 0006, iter [00900, 05004], lr: 0.099339, loss: 5.0639
2022-10-10 11:22:08 - train: epoch 0006, iter [01000, 05004], lr: 0.099334, loss: 5.2710
2022-10-10 11:22:43 - train: epoch 0006, iter [01100, 05004], lr: 0.099329, loss: 5.5077
2022-10-10 11:23:21 - train: epoch 0006, iter [01200, 05004], lr: 0.099324, loss: 5.5122
2022-10-10 11:23:58 - train: epoch 0006, iter [01300, 05004], lr: 0.099319, loss: 5.7358
2022-10-10 11:24:35 - train: epoch 0006, iter [01400, 05004], lr: 0.099314, loss: 5.4058
2022-10-10 11:25:12 - train: epoch 0006, iter [01500, 05004], lr: 0.099309, loss: 5.4050
2022-10-10 11:25:49 - train: epoch 0006, iter [01600, 05004], lr: 0.099303, loss: 5.2413
2022-10-10 11:26:27 - train: epoch 0006, iter [01700, 05004], lr: 0.099298, loss: 5.5261
2022-10-10 11:27:03 - train: epoch 0006, iter [01800, 05004], lr: 0.099293, loss: 5.0930
2022-10-10 11:27:40 - train: epoch 0006, iter [01900, 05004], lr: 0.099288, loss: 5.0522
2022-10-10 11:28:16 - train: epoch 0006, iter [02000, 05004], lr: 0.099282, loss: 5.3658
2022-10-10 11:28:54 - train: epoch 0006, iter [02100, 05004], lr: 0.099277, loss: 5.0874
2022-10-10 11:29:31 - train: epoch 0006, iter [02200, 05004], lr: 0.099272, loss: 5.0980
2022-10-10 11:30:08 - train: epoch 0006, iter [02300, 05004], lr: 0.099266, loss: 4.9813
2022-10-10 11:30:45 - train: epoch 0006, iter [02400, 05004], lr: 0.099261, loss: 5.6218
2022-10-10 11:31:22 - train: epoch 0006, iter [02500, 05004], lr: 0.099256, loss: 4.7452
2022-10-10 11:31:58 - train: epoch 0006, iter [02600, 05004], lr: 0.099250, loss: 5.0078
2022-10-10 11:32:35 - train: epoch 0006, iter [02700, 05004], lr: 0.099245, loss: 5.6134
2022-10-10 11:33:12 - train: epoch 0006, iter [02800, 05004], lr: 0.099239, loss: 5.0390
2022-10-10 11:33:49 - train: epoch 0006, iter [02900, 05004], lr: 0.099234, loss: 4.6138
2022-10-10 11:34:26 - train: epoch 0006, iter [03000, 05004], lr: 0.099228, loss: 4.8334
2022-10-10 11:35:03 - train: epoch 0006, iter [03100, 05004], lr: 0.099223, loss: 5.1781
2022-10-10 11:35:39 - train: epoch 0006, iter [03200, 05004], lr: 0.099217, loss: 5.1825
2022-10-10 11:36:16 - train: epoch 0006, iter [03300, 05004], lr: 0.099212, loss: 5.4836
2022-10-10 11:36:53 - train: epoch 0006, iter [03400, 05004], lr: 0.099206, loss: 4.8408
2022-10-10 11:37:29 - train: epoch 0006, iter [03500, 05004], lr: 0.099201, loss: 5.3357
2022-10-10 11:38:06 - train: epoch 0006, iter [03600, 05004], lr: 0.099195, loss: 5.2462
2022-10-10 11:38:42 - train: epoch 0006, iter [03700, 05004], lr: 0.099189, loss: 5.3181
2022-10-10 11:39:20 - train: epoch 0006, iter [03800, 05004], lr: 0.099184, loss: 5.4190
2022-10-10 11:39:56 - train: epoch 0006, iter [03900, 05004], lr: 0.099178, loss: 5.0768
2022-10-10 11:40:33 - train: epoch 0006, iter [04000, 05004], lr: 0.099173, loss: 5.3514
2022-10-10 11:41:10 - train: epoch 0006, iter [04100, 05004], lr: 0.099167, loss: 5.6984
2022-10-10 11:41:47 - train: epoch 0006, iter [04200, 05004], lr: 0.099161, loss: 5.0463
2022-10-10 11:42:23 - train: epoch 0006, iter [04300, 05004], lr: 0.099155, loss: 5.1391
2022-10-10 11:43:00 - train: epoch 0006, iter [04400, 05004], lr: 0.099150, loss: 5.2177
2022-10-10 11:43:37 - train: epoch 0006, iter [04500, 05004], lr: 0.099144, loss: 4.7988
2022-10-10 11:44:14 - train: epoch 0006, iter [04600, 05004], lr: 0.099138, loss: 4.7913
2022-10-10 11:44:51 - train: epoch 0006, iter [04700, 05004], lr: 0.099132, loss: 5.3012
2022-10-10 11:45:27 - train: epoch 0006, iter [04800, 05004], lr: 0.099126, loss: 5.4741
2022-10-10 11:46:04 - train: epoch 0006, iter [04900, 05004], lr: 0.099121, loss: 5.3178
2022-10-10 11:46:39 - train: epoch 0006, iter [05000, 05004], lr: 0.099115, loss: 5.2490
2022-10-10 11:46:41 - train: epoch 006, train_loss: 5.1903
2022-10-10 11:47:59 - eval: epoch: 006, acc1: 34.614%, acc5: 60.488%, test_loss: 3.1841, per_image_load_time: 0.939ms, per_image_inference_time: 0.595ms
2022-10-10 11:48:00 - until epoch: 006, best_acc1: 34.614%
2022-10-10 11:48:00 - epoch 007 lr: 0.099114
2022-10-10 11:48:41 - train: epoch 0007, iter [00100, 05004], lr: 0.099109, loss: 5.6781
2022-10-10 11:49:18 - train: epoch 0007, iter [00200, 05004], lr: 0.099103, loss: 5.3253
2022-10-10 11:49:55 - train: epoch 0007, iter [00300, 05004], lr: 0.099097, loss: 5.4077
2022-10-10 11:50:32 - train: epoch 0007, iter [00400, 05004], lr: 0.099091, loss: 4.9404
2022-10-10 11:51:08 - train: epoch 0007, iter [00500, 05004], lr: 0.099085, loss: 5.1909
2022-10-10 11:51:45 - train: epoch 0007, iter [00600, 05004], lr: 0.099079, loss: 5.3070
2022-10-10 11:52:22 - train: epoch 0007, iter [00700, 05004], lr: 0.099073, loss: 5.0434
2022-10-10 11:52:58 - train: epoch 0007, iter [00800, 05004], lr: 0.099067, loss: 5.1242
2022-10-10 11:53:36 - train: epoch 0007, iter [00900, 05004], lr: 0.099061, loss: 5.7868
2022-10-10 11:54:12 - train: epoch 0007, iter [01000, 05004], lr: 0.099055, loss: 5.3232
2022-10-10 11:54:49 - train: epoch 0007, iter [01100, 05004], lr: 0.099049, loss: 4.4673
2022-10-10 11:55:26 - train: epoch 0007, iter [01200, 05004], lr: 0.099042, loss: 5.0928
2022-10-10 11:56:02 - train: epoch 0007, iter [01300, 05004], lr: 0.099036, loss: 5.1185
2022-10-10 11:56:40 - train: epoch 0007, iter [01400, 05004], lr: 0.099030, loss: 5.3025
2022-10-10 11:57:17 - train: epoch 0007, iter [01500, 05004], lr: 0.099024, loss: 5.2951
2022-10-10 11:57:53 - train: epoch 0007, iter [01600, 05004], lr: 0.099018, loss: 4.7328
2022-10-10 11:58:30 - train: epoch 0007, iter [01700, 05004], lr: 0.099012, loss: 5.2349
2022-10-10 11:59:07 - train: epoch 0007, iter [01800, 05004], lr: 0.099005, loss: 4.4851
2022-10-10 11:59:44 - train: epoch 0007, iter [01900, 05004], lr: 0.098999, loss: 5.5133
2022-10-10 12:00:21 - train: epoch 0007, iter [02000, 05004], lr: 0.098993, loss: 5.2065
2022-10-10 12:00:58 - train: epoch 0007, iter [02100, 05004], lr: 0.098987, loss: 5.0867
2022-10-10 12:01:34 - train: epoch 0007, iter [02200, 05004], lr: 0.098980, loss: 5.2285
2022-10-10 12:02:11 - train: epoch 0007, iter [02300, 05004], lr: 0.098974, loss: 4.9315
2022-10-10 12:02:48 - train: epoch 0007, iter [02400, 05004], lr: 0.098968, loss: 4.5490
2022-10-10 12:03:25 - train: epoch 0007, iter [02500, 05004], lr: 0.098961, loss: 5.4093
2022-10-10 12:04:02 - train: epoch 0007, iter [02600, 05004], lr: 0.098955, loss: 5.1322
2022-10-10 12:04:39 - train: epoch 0007, iter [02700, 05004], lr: 0.098949, loss: 3.9029
2022-10-10 12:05:15 - train: epoch 0007, iter [02800, 05004], lr: 0.098942, loss: 4.7037
2022-10-10 12:05:52 - train: epoch 0007, iter [02900, 05004], lr: 0.098936, loss: 4.6506
2022-10-10 12:06:29 - train: epoch 0007, iter [03000, 05004], lr: 0.098929, loss: 4.1491
2022-10-10 12:07:06 - train: epoch 0007, iter [03100, 05004], lr: 0.098923, loss: 5.4593
2022-10-10 12:07:43 - train: epoch 0007, iter [03200, 05004], lr: 0.098916, loss: 5.0021
2022-10-10 12:08:19 - train: epoch 0007, iter [03300, 05004], lr: 0.098910, loss: 5.0881
2022-10-10 12:08:56 - train: epoch 0007, iter [03400, 05004], lr: 0.098903, loss: 5.1747
2022-10-10 12:09:32 - train: epoch 0007, iter [03500, 05004], lr: 0.098897, loss: 4.7692
2022-10-10 12:10:08 - train: epoch 0007, iter [03600, 05004], lr: 0.098890, loss: 5.3034
2022-10-10 12:10:44 - train: epoch 0007, iter [03700, 05004], lr: 0.098884, loss: 5.2242
2022-10-10 12:11:21 - train: epoch 0007, iter [03800, 05004], lr: 0.098877, loss: 5.2424
2022-10-10 12:11:58 - train: epoch 0007, iter [03900, 05004], lr: 0.098870, loss: 5.2176
2022-10-10 12:12:34 - train: epoch 0007, iter [04000, 05004], lr: 0.098864, loss: 5.3788
2022-10-10 12:13:11 - train: epoch 0007, iter [04100, 05004], lr: 0.098857, loss: 5.2807
2022-10-10 12:13:48 - train: epoch 0007, iter [04200, 05004], lr: 0.098850, loss: 4.6722
2022-10-10 12:14:24 - train: epoch 0007, iter [04300, 05004], lr: 0.098844, loss: 5.2422
2022-10-10 12:15:01 - train: epoch 0007, iter [04400, 05004], lr: 0.098837, loss: 4.3679
2022-10-10 12:15:38 - train: epoch 0007, iter [04500, 05004], lr: 0.098830, loss: 5.1837
2022-10-10 12:16:14 - train: epoch 0007, iter [04600, 05004], lr: 0.098823, loss: 5.4041
2022-10-10 12:16:51 - train: epoch 0007, iter [04700, 05004], lr: 0.098817, loss: 4.9780
2022-10-10 12:17:28 - train: epoch 0007, iter [04800, 05004], lr: 0.098810, loss: 5.5295
2022-10-10 12:18:05 - train: epoch 0007, iter [04900, 05004], lr: 0.098803, loss: 5.2378
2022-10-10 12:18:40 - train: epoch 0007, iter [05000, 05004], lr: 0.098796, loss: 5.4356
2022-10-10 12:18:42 - train: epoch 007, train_loss: 5.0872
2022-10-10 12:20:00 - eval: epoch: 007, acc1: 41.776%, acc5: 68.344%, test_loss: 2.7735, per_image_load_time: 1.843ms, per_image_inference_time: 0.578ms
2022-10-10 12:20:01 - until epoch: 007, best_acc1: 41.776%
2022-10-10 12:20:01 - epoch 008 lr: 0.098796
2022-10-10 12:20:44 - train: epoch 0008, iter [00100, 05004], lr: 0.098789, loss: 4.7534
2022-10-10 12:21:19 - train: epoch 0008, iter [00200, 05004], lr: 0.098782, loss: 4.6770
2022-10-10 12:21:56 - train: epoch 0008, iter [00300, 05004], lr: 0.098775, loss: 4.8169
2022-10-10 12:22:32 - train: epoch 0008, iter [00400, 05004], lr: 0.098768, loss: 4.9166
2022-10-10 12:23:07 - train: epoch 0008, iter [00500, 05004], lr: 0.098761, loss: 5.4108
2022-10-10 12:23:44 - train: epoch 0008, iter [00600, 05004], lr: 0.098755, loss: 5.3542
2022-10-10 12:24:20 - train: epoch 0008, iter [00700, 05004], lr: 0.098748, loss: 4.7254
2022-10-10 12:24:57 - train: epoch 0008, iter [00800, 05004], lr: 0.098741, loss: 4.3678
2022-10-10 12:25:35 - train: epoch 0008, iter [00900, 05004], lr: 0.098734, loss: 5.2229
2022-10-10 12:26:11 - train: epoch 0008, iter [01000, 05004], lr: 0.098727, loss: 5.0217
2022-10-10 12:26:47 - train: epoch 0008, iter [01100, 05004], lr: 0.098719, loss: 4.0483
2022-10-10 12:27:24 - train: epoch 0008, iter [01200, 05004], lr: 0.098712, loss: 4.6349
2022-10-10 12:28:00 - train: epoch 0008, iter [01300, 05004], lr: 0.098705, loss: 5.3922
2022-10-10 12:28:37 - train: epoch 0008, iter [01400, 05004], lr: 0.098698, loss: 5.3374
2022-10-10 12:29:14 - train: epoch 0008, iter [01500, 05004], lr: 0.098691, loss: 4.9819
2022-10-10 12:29:51 - train: epoch 0008, iter [01600, 05004], lr: 0.098684, loss: 5.0641
2022-10-10 12:30:28 - train: epoch 0008, iter [01700, 05004], lr: 0.098677, loss: 5.0054
2022-10-10 12:31:05 - train: epoch 0008, iter [01800, 05004], lr: 0.098670, loss: 4.8643
2022-10-10 12:31:41 - train: epoch 0008, iter [01900, 05004], lr: 0.098662, loss: 5.5482
2022-10-10 12:32:18 - train: epoch 0008, iter [02000, 05004], lr: 0.098655, loss: 4.5268
2022-10-10 12:32:55 - train: epoch 0008, iter [02100, 05004], lr: 0.098648, loss: 5.1616
2022-10-10 12:33:31 - train: epoch 0008, iter [02200, 05004], lr: 0.098641, loss: 4.9296
2022-10-10 12:34:08 - train: epoch 0008, iter [02300, 05004], lr: 0.098633, loss: 5.1314
2022-10-10 12:34:45 - train: epoch 0008, iter [02400, 05004], lr: 0.098626, loss: 5.6297
2022-10-10 12:35:21 - train: epoch 0008, iter [02500, 05004], lr: 0.098619, loss: 5.1782
2022-10-10 12:35:58 - train: epoch 0008, iter [02600, 05004], lr: 0.098611, loss: 5.1742
2022-10-10 12:36:35 - train: epoch 0008, iter [02700, 05004], lr: 0.098604, loss: 5.5519
2022-10-10 12:37:13 - train: epoch 0008, iter [02800, 05004], lr: 0.098597, loss: 4.7841
2022-10-10 12:37:49 - train: epoch 0008, iter [02900, 05004], lr: 0.098589, loss: 5.1488
2022-10-10 12:38:26 - train: epoch 0008, iter [03000, 05004], lr: 0.098582, loss: 4.7523
2022-10-10 12:39:03 - train: epoch 0008, iter [03100, 05004], lr: 0.098574, loss: 5.4020
2022-10-10 12:39:39 - train: epoch 0008, iter [03200, 05004], lr: 0.098567, loss: 5.3360
2022-10-10 12:40:15 - train: epoch 0008, iter [03300, 05004], lr: 0.098560, loss: 4.6739
2022-10-10 12:40:52 - train: epoch 0008, iter [03400, 05004], lr: 0.098552, loss: 5.3486
2022-10-10 12:41:29 - train: epoch 0008, iter [03500, 05004], lr: 0.098545, loss: 5.0003
2022-10-10 12:42:06 - train: epoch 0008, iter [03600, 05004], lr: 0.098537, loss: 4.7891
2022-10-10 12:42:43 - train: epoch 0008, iter [03700, 05004], lr: 0.098529, loss: 5.0141
2022-10-10 12:43:20 - train: epoch 0008, iter [03800, 05004], lr: 0.098522, loss: 4.3148
2022-10-10 12:43:56 - train: epoch 0008, iter [03900, 05004], lr: 0.098514, loss: 5.2902
2022-10-10 12:44:33 - train: epoch 0008, iter [04000, 05004], lr: 0.098507, loss: 5.4428
2022-10-10 12:45:11 - train: epoch 0008, iter [04100, 05004], lr: 0.098499, loss: 4.9092
2022-10-10 12:45:49 - train: epoch 0008, iter [04200, 05004], lr: 0.098491, loss: 4.7681
2022-10-10 12:46:24 - train: epoch 0008, iter [04300, 05004], lr: 0.098484, loss: 4.5123
2022-10-10 12:47:01 - train: epoch 0008, iter [04400, 05004], lr: 0.098476, loss: 5.2200
2022-10-10 12:47:38 - train: epoch 0008, iter [04500, 05004], lr: 0.098468, loss: 3.9453
2022-10-10 12:48:15 - train: epoch 0008, iter [04600, 05004], lr: 0.098461, loss: 4.4459
2022-10-10 12:48:52 - train: epoch 0008, iter [04700, 05004], lr: 0.098453, loss: 4.5627
2022-10-10 12:49:28 - train: epoch 0008, iter [04800, 05004], lr: 0.098445, loss: 5.0933
2022-10-10 12:50:05 - train: epoch 0008, iter [04900, 05004], lr: 0.098437, loss: 4.7120
2022-10-10 12:50:40 - train: epoch 0008, iter [05000, 05004], lr: 0.098430, loss: 5.3798
2022-10-10 12:50:43 - train: epoch 008, train_loss: 4.9958
2022-10-10 12:52:02 - eval: epoch: 008, acc1: 42.980%, acc5: 69.754%, test_loss: 2.7720, per_image_load_time: 1.445ms, per_image_inference_time: 0.555ms
2022-10-10 12:52:02 - until epoch: 008, best_acc1: 42.980%
2022-10-10 12:52:02 - epoch 009 lr: 0.098429
2022-10-10 12:52:45 - train: epoch 0009, iter [00100, 05004], lr: 0.098421, loss: 4.5198
2022-10-10 12:53:21 - train: epoch 0009, iter [00200, 05004], lr: 0.098414, loss: 5.6189
2022-10-10 12:53:57 - train: epoch 0009, iter [00300, 05004], lr: 0.098406, loss: 4.5419
2022-10-10 12:54:33 - train: epoch 0009, iter [00400, 05004], lr: 0.098398, loss: 4.8986
2022-10-10 12:55:09 - train: epoch 0009, iter [00500, 05004], lr: 0.098390, loss: 5.1251
2022-10-10 12:55:45 - train: epoch 0009, iter [00600, 05004], lr: 0.098382, loss: 4.7933
2022-10-10 12:56:22 - train: epoch 0009, iter [00700, 05004], lr: 0.098374, loss: 4.2885
2022-10-10 12:56:58 - train: epoch 0009, iter [00800, 05004], lr: 0.098366, loss: 4.6982
2022-10-10 12:57:35 - train: epoch 0009, iter [00900, 05004], lr: 0.098358, loss: 4.5115
2022-10-10 12:58:10 - train: epoch 0009, iter [01000, 05004], lr: 0.098350, loss: 4.8912
2022-10-10 12:58:48 - train: epoch 0009, iter [01100, 05004], lr: 0.098342, loss: 5.5054
2022-10-10 12:59:24 - train: epoch 0009, iter [01200, 05004], lr: 0.098334, loss: 5.1309
2022-10-10 13:00:00 - train: epoch 0009, iter [01300, 05004], lr: 0.098326, loss: 5.2793
2022-10-10 13:00:37 - train: epoch 0009, iter [01400, 05004], lr: 0.098318, loss: 4.3117
2022-10-10 13:01:14 - train: epoch 0009, iter [01500, 05004], lr: 0.098310, loss: 4.9291
2022-10-10 13:01:51 - train: epoch 0009, iter [01600, 05004], lr: 0.098302, loss: 5.0039
2022-10-10 13:02:27 - train: epoch 0009, iter [01700, 05004], lr: 0.098294, loss: 5.3997
2022-10-10 13:03:04 - train: epoch 0009, iter [01800, 05004], lr: 0.098286, loss: 4.2343
2022-10-10 13:03:41 - train: epoch 0009, iter [01900, 05004], lr: 0.098278, loss: 4.9997
2022-10-10 13:04:18 - train: epoch 0009, iter [02000, 05004], lr: 0.098269, loss: 4.5458
2022-10-10 13:04:55 - train: epoch 0009, iter [02100, 05004], lr: 0.098261, loss: 4.6517
2022-10-10 13:05:32 - train: epoch 0009, iter [02200, 05004], lr: 0.098253, loss: 5.4641
2022-10-10 13:06:09 - train: epoch 0009, iter [02300, 05004], lr: 0.098245, loss: 4.6019
2022-10-10 13:06:45 - train: epoch 0009, iter [02400, 05004], lr: 0.098236, loss: 5.0539
2022-10-10 13:07:22 - train: epoch 0009, iter [02500, 05004], lr: 0.098228, loss: 4.7293
2022-10-10 13:07:59 - train: epoch 0009, iter [02600, 05004], lr: 0.098220, loss: 5.5604
2022-10-10 13:08:35 - train: epoch 0009, iter [02700, 05004], lr: 0.098212, loss: 5.1820
2022-10-10 13:09:12 - train: epoch 0009, iter [02800, 05004], lr: 0.098203, loss: 4.7810
2022-10-10 13:09:49 - train: epoch 0009, iter [02900, 05004], lr: 0.098195, loss: 5.2028
2022-10-10 13:10:25 - train: epoch 0009, iter [03000, 05004], lr: 0.098187, loss: 4.3659
2022-10-10 13:11:02 - train: epoch 0009, iter [03100, 05004], lr: 0.098178, loss: 5.1994
2022-10-10 13:11:39 - train: epoch 0009, iter [03200, 05004], lr: 0.098170, loss: 5.4999
2022-10-10 13:12:15 - train: epoch 0009, iter [03300, 05004], lr: 0.098161, loss: 4.8729
2022-10-10 13:12:52 - train: epoch 0009, iter [03400, 05004], lr: 0.098153, loss: 4.6023
2022-10-10 13:13:29 - train: epoch 0009, iter [03500, 05004], lr: 0.098144, loss: 4.7957
2022-10-10 13:14:07 - train: epoch 0009, iter [03600, 05004], lr: 0.098136, loss: 4.9317
2022-10-10 13:14:42 - train: epoch 0009, iter [03700, 05004], lr: 0.098127, loss: 5.5153
2022-10-10 13:15:19 - train: epoch 0009, iter [03800, 05004], lr: 0.098119, loss: 4.9762
2022-10-10 13:15:56 - train: epoch 0009, iter [03900, 05004], lr: 0.098110, loss: 4.8639
2022-10-10 13:16:33 - train: epoch 0009, iter [04000, 05004], lr: 0.098102, loss: 4.8527
2022-10-10 13:17:10 - train: epoch 0009, iter [04100, 05004], lr: 0.098093, loss: 4.6615
2022-10-10 13:17:47 - train: epoch 0009, iter [04200, 05004], lr: 0.098085, loss: 4.5049
2022-10-10 13:18:24 - train: epoch 0009, iter [04300, 05004], lr: 0.098076, loss: 5.1006
2022-10-10 13:19:01 - train: epoch 0009, iter [04400, 05004], lr: 0.098067, loss: 4.6053
2022-10-10 13:19:38 - train: epoch 0009, iter [04500, 05004], lr: 0.098059, loss: 4.5175
2022-10-10 13:20:14 - train: epoch 0009, iter [04600, 05004], lr: 0.098050, loss: 5.2166
2022-10-10 13:20:52 - train: epoch 0009, iter [04700, 05004], lr: 0.098041, loss: 5.4393
2022-10-10 13:21:29 - train: epoch 0009, iter [04800, 05004], lr: 0.098033, loss: 4.5850
2022-10-10 13:22:06 - train: epoch 0009, iter [04900, 05004], lr: 0.098024, loss: 4.9636
2022-10-10 13:22:40 - train: epoch 0009, iter [05000, 05004], lr: 0.098015, loss: 3.9843
2022-10-10 13:22:42 - train: epoch 009, train_loss: 4.9327
2022-10-10 13:24:02 - eval: epoch: 009, acc1: 45.314%, acc5: 71.694%, test_loss: 2.6243, per_image_load_time: 2.313ms, per_image_inference_time: 0.570ms
2022-10-10 13:24:02 - until epoch: 009, best_acc1: 45.314%
2022-10-10 13:24:02 - epoch 010 lr: 0.098015
2022-10-10 13:24:44 - train: epoch 0010, iter [00100, 05004], lr: 0.098006, loss: 4.6963
2022-10-10 13:25:20 - train: epoch 0010, iter [00200, 05004], lr: 0.097997, loss: 4.2323
2022-10-10 13:25:57 - train: epoch 0010, iter [00300, 05004], lr: 0.097989, loss: 5.5170
2022-10-10 13:26:34 - train: epoch 0010, iter [00400, 05004], lr: 0.097980, loss: 4.9732
2022-10-10 13:27:11 - train: epoch 0010, iter [00500, 05004], lr: 0.097971, loss: 4.1109
2022-10-10 13:27:48 - train: epoch 0010, iter [00600, 05004], lr: 0.097962, loss: 5.4365
2022-10-10 13:28:24 - train: epoch 0010, iter [00700, 05004], lr: 0.097953, loss: 5.4904
2022-10-10 13:29:01 - train: epoch 0010, iter [00800, 05004], lr: 0.097944, loss: 4.4797
2022-10-10 13:29:37 - train: epoch 0010, iter [00900, 05004], lr: 0.097935, loss: 5.0957
2022-10-10 13:30:14 - train: epoch 0010, iter [01000, 05004], lr: 0.097926, loss: 5.0811
2022-10-10 13:30:51 - train: epoch 0010, iter [01100, 05004], lr: 0.097917, loss: 4.8965
2022-10-10 13:31:28 - train: epoch 0010, iter [01200, 05004], lr: 0.097908, loss: 4.9772
2022-10-10 13:32:05 - train: epoch 0010, iter [01300, 05004], lr: 0.097899, loss: 4.4537
2022-10-10 13:32:42 - train: epoch 0010, iter [01400, 05004], lr: 0.097890, loss: 4.9452
2022-10-10 13:33:19 - train: epoch 0010, iter [01500, 05004], lr: 0.097881, loss: 4.2915
2022-10-10 13:33:56 - train: epoch 0010, iter [01600, 05004], lr: 0.097872, loss: 4.0098
2022-10-10 13:34:33 - train: epoch 0010, iter [01700, 05004], lr: 0.097863, loss: 4.8719
2022-10-10 13:35:10 - train: epoch 0010, iter [01800, 05004], lr: 0.097854, loss: 4.0174
2022-10-10 13:35:45 - train: epoch 0010, iter [01900, 05004], lr: 0.097845, loss: 5.0248
2022-10-10 13:36:22 - train: epoch 0010, iter [02000, 05004], lr: 0.097836, loss: 4.5811
2022-10-10 13:36:58 - train: epoch 0010, iter [02100, 05004], lr: 0.097827, loss: 5.1287
2022-10-10 13:37:35 - train: epoch 0010, iter [02200, 05004], lr: 0.097818, loss: 5.3698
2022-10-10 13:38:11 - train: epoch 0010, iter [02300, 05004], lr: 0.097808, loss: 4.8216
2022-10-10 13:38:48 - train: epoch 0010, iter [02400, 05004], lr: 0.097799, loss: 4.9964
2022-10-10 13:39:25 - train: epoch 0010, iter [02500, 05004], lr: 0.097790, loss: 4.5459
2022-10-10 13:40:02 - train: epoch 0010, iter [02600, 05004], lr: 0.097781, loss: 5.2676
2022-10-10 13:40:40 - train: epoch 0010, iter [02700, 05004], lr: 0.097772, loss: 4.6067
2022-10-10 13:41:17 - train: epoch 0010, iter [02800, 05004], lr: 0.097762, loss: 4.9927
2022-10-10 13:41:53 - train: epoch 0010, iter [02900, 05004], lr: 0.097753, loss: 4.9438
2022-10-10 13:42:30 - train: epoch 0010, iter [03000, 05004], lr: 0.097744, loss: 4.9020
2022-10-10 13:43:07 - train: epoch 0010, iter [03100, 05004], lr: 0.097734, loss: 4.7267
2022-10-10 13:43:44 - train: epoch 0010, iter [03200, 05004], lr: 0.097725, loss: 4.3746
2022-10-10 13:44:21 - train: epoch 0010, iter [03300, 05004], lr: 0.097716, loss: 5.2367
2022-10-10 13:44:57 - train: epoch 0010, iter [03400, 05004], lr: 0.097706, loss: 5.1439
2022-10-10 13:45:34 - train: epoch 0010, iter [03500, 05004], lr: 0.097697, loss: 5.3515
2022-10-10 13:46:11 - train: epoch 0010, iter [03600, 05004], lr: 0.097687, loss: 4.6758
2022-10-10 13:46:48 - train: epoch 0010, iter [03700, 05004], lr: 0.097678, loss: 4.9168
2022-10-10 13:47:25 - train: epoch 0010, iter [03800, 05004], lr: 0.097668, loss: 4.5026
2022-10-10 13:48:02 - train: epoch 0010, iter [03900, 05004], lr: 0.097659, loss: 4.2970
2022-10-10 13:48:39 - train: epoch 0010, iter [04000, 05004], lr: 0.097650, loss: 4.7286
2022-10-10 13:49:16 - train: epoch 0010, iter [04100, 05004], lr: 0.097640, loss: 5.1210
2022-10-10 13:49:52 - train: epoch 0010, iter [04200, 05004], lr: 0.097630, loss: 4.7827
2022-10-10 13:50:29 - train: epoch 0010, iter [04300, 05004], lr: 0.097621, loss: 5.1066
2022-10-10 13:51:06 - train: epoch 0010, iter [04400, 05004], lr: 0.097611, loss: 5.2874
2022-10-10 13:51:44 - train: epoch 0010, iter [04500, 05004], lr: 0.097602, loss: 4.1394
2022-10-10 13:52:20 - train: epoch 0010, iter [04600, 05004], lr: 0.097592, loss: 5.1566
2022-10-10 13:52:58 - train: epoch 0010, iter [04700, 05004], lr: 0.097582, loss: 4.7125
2022-10-10 13:53:35 - train: epoch 0010, iter [04800, 05004], lr: 0.097573, loss: 5.4006
2022-10-10 13:54:12 - train: epoch 0010, iter [04900, 05004], lr: 0.097563, loss: 4.5882
2022-10-10 13:54:47 - train: epoch 0010, iter [05000, 05004], lr: 0.097553, loss: 5.0371
2022-10-10 13:54:49 - train: epoch 010, train_loss: 4.8795
2022-10-10 13:56:08 - eval: epoch: 010, acc1: 45.048%, acc5: 71.772%, test_loss: 2.5956, per_image_load_time: 2.117ms, per_image_inference_time: 0.577ms
2022-10-10 13:56:09 - until epoch: 010, best_acc1: 45.314%
2022-10-10 13:56:09 - epoch 011 lr: 0.097553
2022-10-10 13:56:51 - train: epoch 0011, iter [00100, 05004], lr: 0.097543, loss: 4.7728
2022-10-10 13:57:28 - train: epoch 0011, iter [00200, 05004], lr: 0.097534, loss: 5.0919
2022-10-10 13:58:04 - train: epoch 0011, iter [00300, 05004], lr: 0.097524, loss: 4.6228
2022-10-10 13:58:41 - train: epoch 0011, iter [00400, 05004], lr: 0.097514, loss: 5.1581
2022-10-10 13:59:18 - train: epoch 0011, iter [00500, 05004], lr: 0.097504, loss: 5.2473
2022-10-10 13:59:54 - train: epoch 0011, iter [00600, 05004], lr: 0.097495, loss: 4.7964
2022-10-10 14:00:31 - train: epoch 0011, iter [00700, 05004], lr: 0.097485, loss: 4.3695
2022-10-10 14:01:07 - train: epoch 0011, iter [00800, 05004], lr: 0.097475, loss: 4.9410
2022-10-10 14:01:43 - train: epoch 0011, iter [00900, 05004], lr: 0.097465, loss: 4.4011
2022-10-10 14:02:20 - train: epoch 0011, iter [01000, 05004], lr: 0.097455, loss: 5.0626
2022-10-10 14:02:56 - train: epoch 0011, iter [01100, 05004], lr: 0.097445, loss: 5.3714
2022-10-10 14:03:33 - train: epoch 0011, iter [01200, 05004], lr: 0.097435, loss: 5.0008
2022-10-10 14:04:09 - train: epoch 0011, iter [01300, 05004], lr: 0.097425, loss: 5.6222
2022-10-10 14:04:45 - train: epoch 0011, iter [01400, 05004], lr: 0.097415, loss: 4.9606
2022-10-10 14:05:23 - train: epoch 0011, iter [01500, 05004], lr: 0.097405, loss: 4.9476
2022-10-10 14:05:59 - train: epoch 0011, iter [01600, 05004], lr: 0.097395, loss: 4.5920
2022-10-10 14:06:36 - train: epoch 0011, iter [01700, 05004], lr: 0.097385, loss: 4.8762
2022-10-10 14:07:14 - train: epoch 0011, iter [01800, 05004], lr: 0.097375, loss: 5.0892
2022-10-10 14:07:50 - train: epoch 0011, iter [01900, 05004], lr: 0.097365, loss: 5.3757
2022-10-10 14:08:27 - train: epoch 0011, iter [02000, 05004], lr: 0.097355, loss: 4.3866
2022-10-10 14:09:04 - train: epoch 0011, iter [02100, 05004], lr: 0.097345, loss: 4.5480
2022-10-10 14:09:41 - train: epoch 0011, iter [02200, 05004], lr: 0.097335, loss: 5.1127
2022-10-10 14:10:18 - train: epoch 0011, iter [02300, 05004], lr: 0.097325, loss: 5.3846
2022-10-10 14:10:55 - train: epoch 0011, iter [02400, 05004], lr: 0.097315, loss: 5.2795
2022-10-10 14:11:32 - train: epoch 0011, iter [02500, 05004], lr: 0.097305, loss: 4.4985
2022-10-10 14:12:09 - train: epoch 0011, iter [02600, 05004], lr: 0.097295, loss: 5.0204
2022-10-10 14:12:46 - train: epoch 0011, iter [02700, 05004], lr: 0.097284, loss: 5.2493
2022-10-10 14:13:22 - train: epoch 0011, iter [02800, 05004], lr: 0.097274, loss: 4.9387
2022-10-10 14:13:59 - train: epoch 0011, iter [02900, 05004], lr: 0.097264, loss: 4.4969
2022-10-10 14:14:36 - train: epoch 0011, iter [03000, 05004], lr: 0.097254, loss: 5.1074
2022-10-10 14:15:12 - train: epoch 0011, iter [03100, 05004], lr: 0.097243, loss: 4.7945
2022-10-10 14:15:49 - train: epoch 0011, iter [03200, 05004], lr: 0.097233, loss: 4.6509
2022-10-10 14:16:26 - train: epoch 0011, iter [03300, 05004], lr: 0.097223, loss: 5.3250
2022-10-10 14:17:03 - train: epoch 0011, iter [03400, 05004], lr: 0.097212, loss: 4.4637
2022-10-10 14:17:40 - train: epoch 0011, iter [03500, 05004], lr: 0.097202, loss: 4.1589
2022-10-10 14:18:17 - train: epoch 0011, iter [03600, 05004], lr: 0.097192, loss: 5.0823
2022-10-10 14:18:54 - train: epoch 0011, iter [03700, 05004], lr: 0.097181, loss: 4.7751
2022-10-10 14:19:30 - train: epoch 0011, iter [03800, 05004], lr: 0.097171, loss: 4.2577
2022-10-10 14:20:06 - train: epoch 0011, iter [03900, 05004], lr: 0.097161, loss: 5.3017
2022-10-10 14:20:43 - train: epoch 0011, iter [04000, 05004], lr: 0.097150, loss: 4.5975
2022-10-10 14:21:20 - train: epoch 0011, iter [04100, 05004], lr: 0.097140, loss: 4.9627
2022-10-10 14:21:57 - train: epoch 0011, iter [04200, 05004], lr: 0.097129, loss: 4.4868
2022-10-10 14:22:34 - train: epoch 0011, iter [04300, 05004], lr: 0.097119, loss: 5.1305
2022-10-10 14:23:11 - train: epoch 0011, iter [04400, 05004], lr: 0.097108, loss: 5.2727
2022-10-10 14:23:47 - train: epoch 0011, iter [04500, 05004], lr: 0.097098, loss: 5.1839
2022-10-10 14:24:25 - train: epoch 0011, iter [04600, 05004], lr: 0.097087, loss: 5.0595
2022-10-10 14:25:02 - train: epoch 0011, iter [04700, 05004], lr: 0.097077, loss: 4.1801
2022-10-10 14:25:39 - train: epoch 0011, iter [04800, 05004], lr: 0.097066, loss: 4.6566
2022-10-10 14:26:15 - train: epoch 0011, iter [04900, 05004], lr: 0.097055, loss: 4.9748
2022-10-10 14:26:49 - train: epoch 0011, iter [05000, 05004], lr: 0.097045, loss: 4.9730
2022-10-10 14:26:51 - train: epoch 011, train_loss: 4.8257
2022-10-10 14:28:11 - eval: epoch: 011, acc1: 47.878%, acc5: 74.434%, test_loss: 2.5262, per_image_load_time: 2.370ms, per_image_inference_time: 0.593ms
2022-10-10 14:28:11 - until epoch: 011, best_acc1: 47.878%
2022-10-10 14:28:11 - epoch 012 lr: 0.097044
2022-10-10 14:28:54 - train: epoch 0012, iter [00100, 05004], lr: 0.097034, loss: 4.5717
2022-10-10 14:29:31 - train: epoch 0012, iter [00200, 05004], lr: 0.097023, loss: 5.3359
2022-10-10 14:30:07 - train: epoch 0012, iter [00300, 05004], lr: 0.097012, loss: 5.1367
2022-10-10 14:30:44 - train: epoch 0012, iter [00400, 05004], lr: 0.097002, loss: 5.2200
2022-10-10 14:31:21 - train: epoch 0012, iter [00500, 05004], lr: 0.096991, loss: 5.0300
2022-10-10 14:31:57 - train: epoch 0012, iter [00600, 05004], lr: 0.096980, loss: 4.1902
2022-10-10 14:32:34 - train: epoch 0012, iter [00700, 05004], lr: 0.096969, loss: 4.6564
2022-10-10 14:33:11 - train: epoch 0012, iter [00800, 05004], lr: 0.096959, loss: 4.6972
2022-10-10 14:33:48 - train: epoch 0012, iter [00900, 05004], lr: 0.096948, loss: 5.1250
2022-10-10 14:34:25 - train: epoch 0012, iter [01000, 05004], lr: 0.096937, loss: 4.6389
2022-10-10 14:35:02 - train: epoch 0012, iter [01100, 05004], lr: 0.096926, loss: 4.2690
2022-10-10 14:35:39 - train: epoch 0012, iter [01200, 05004], lr: 0.096915, loss: 4.3248
2022-10-10 14:36:15 - train: epoch 0012, iter [01300, 05004], lr: 0.096905, loss: 4.7404
2022-10-10 14:36:53 - train: epoch 0012, iter [01400, 05004], lr: 0.096894, loss: 5.2405
2022-10-10 14:37:30 - train: epoch 0012, iter [01500, 05004], lr: 0.096883, loss: 5.1881
2022-10-10 14:38:06 - train: epoch 0012, iter [01600, 05004], lr: 0.096872, loss: 4.6662
2022-10-10 14:38:44 - train: epoch 0012, iter [01700, 05004], lr: 0.096861, loss: 4.6273
2022-10-10 14:39:21 - train: epoch 0012, iter [01800, 05004], lr: 0.096850, loss: 4.9410
2022-10-10 14:39:58 - train: epoch 0012, iter [01900, 05004], lr: 0.096839, loss: 4.8175
2022-10-10 14:40:35 - train: epoch 0012, iter [02000, 05004], lr: 0.096828, loss: 4.7542
2022-10-10 14:41:12 - train: epoch 0012, iter [02100, 05004], lr: 0.096817, loss: 5.4302
2022-10-10 14:41:48 - train: epoch 0012, iter [02200, 05004], lr: 0.096806, loss: 4.4053
2022-10-10 14:42:26 - train: epoch 0012, iter [02300, 05004], lr: 0.096795, loss: 4.8430
2022-10-10 14:43:02 - train: epoch 0012, iter [02400, 05004], lr: 0.096784, loss: 4.5535
2022-10-10 14:43:40 - train: epoch 0012, iter [02500, 05004], lr: 0.096773, loss: 5.4608
2022-10-10 14:44:17 - train: epoch 0012, iter [02600, 05004], lr: 0.096762, loss: 4.2733
2022-10-10 14:44:53 - train: epoch 0012, iter [02700, 05004], lr: 0.096751, loss: 4.9372
2022-10-10 14:45:31 - train: epoch 0012, iter [02800, 05004], lr: 0.096739, loss: 5.3618
2022-10-10 14:46:08 - train: epoch 0012, iter [02900, 05004], lr: 0.096728, loss: 4.1809
2022-10-10 14:46:44 - train: epoch 0012, iter [03000, 05004], lr: 0.096717, loss: 4.3628
2022-10-10 14:47:21 - train: epoch 0012, iter [03100, 05004], lr: 0.096706, loss: 5.3745
2022-10-10 14:47:57 - train: epoch 0012, iter [03200, 05004], lr: 0.096695, loss: 5.1873
2022-10-10 14:48:35 - train: epoch 0012, iter [03300, 05004], lr: 0.096683, loss: 4.6187
2022-10-10 14:49:11 - train: epoch 0012, iter [03400, 05004], lr: 0.096672, loss: 4.8600
2022-10-10 14:49:48 - train: epoch 0012, iter [03500, 05004], lr: 0.096661, loss: 4.9453
2022-10-10 14:50:25 - train: epoch 0012, iter [03600, 05004], lr: 0.096650, loss: 4.7100
2022-10-10 14:51:01 - train: epoch 0012, iter [03700, 05004], lr: 0.096638, loss: 4.9588
2022-10-10 14:51:38 - train: epoch 0012, iter [03800, 05004], lr: 0.096627, loss: 4.8087
2022-10-10 14:52:14 - train: epoch 0012, iter [03900, 05004], lr: 0.096616, loss: 4.6006
2022-10-10 14:52:52 - train: epoch 0012, iter [04000, 05004], lr: 0.096604, loss: 5.2006
2022-10-10 14:53:28 - train: epoch 0012, iter [04100, 05004], lr: 0.096593, loss: 4.7768
2022-10-10 14:54:05 - train: epoch 0012, iter [04200, 05004], lr: 0.096581, loss: 5.3393
2022-10-10 14:54:42 - train: epoch 0012, iter [04300, 05004], lr: 0.096570, loss: 4.3468
2022-10-10 14:55:20 - train: epoch 0012, iter [04400, 05004], lr: 0.096559, loss: 4.6926
2022-10-10 14:55:57 - train: epoch 0012, iter [04500, 05004], lr: 0.096547, loss: 4.5658
2022-10-10 14:56:33 - train: epoch 0012, iter [04600, 05004], lr: 0.096536, loss: 4.7519
2022-10-10 14:57:10 - train: epoch 0012, iter [04700, 05004], lr: 0.096524, loss: 4.1310
2022-10-10 14:57:48 - train: epoch 0012, iter [04800, 05004], lr: 0.096513, loss: 5.0610
2022-10-10 14:58:25 - train: epoch 0012, iter [04900, 05004], lr: 0.096501, loss: 4.6153
2022-10-10 14:58:59 - train: epoch 0012, iter [05000, 05004], lr: 0.096490, loss: 5.2721
2022-10-10 14:59:01 - train: epoch 012, train_loss: 4.8035
2022-10-10 15:00:19 - eval: epoch: 012, acc1: 47.308%, acc5: 73.510%, test_loss: 2.6248, per_image_load_time: 2.201ms, per_image_inference_time: 0.610ms
2022-10-10 15:00:19 - until epoch: 012, best_acc1: 47.878%
2022-10-10 15:00:19 - epoch 013 lr: 0.096489
2022-10-10 15:01:02 - train: epoch 0013, iter [00100, 05004], lr: 0.096478, loss: 4.5200
2022-10-10 15:01:38 - train: epoch 0013, iter [00200, 05004], lr: 0.096466, loss: 4.9568
2022-10-10 15:02:14 - train: epoch 0013, iter [00300, 05004], lr: 0.096454, loss: 4.1027
2022-10-10 15:02:51 - train: epoch 0013, iter [00400, 05004], lr: 0.096443, loss: 4.9813
2022-10-10 15:03:27 - train: epoch 0013, iter [00500, 05004], lr: 0.096431, loss: 4.3069
2022-10-10 15:04:03 - train: epoch 0013, iter [00600, 05004], lr: 0.096420, loss: 4.9824
2022-10-10 15:04:40 - train: epoch 0013, iter [00700, 05004], lr: 0.096408, loss: 4.6175
2022-10-10 15:05:17 - train: epoch 0013, iter [00800, 05004], lr: 0.096396, loss: 4.8585
2022-10-10 15:05:54 - train: epoch 0013, iter [00900, 05004], lr: 0.096384, loss: 5.2620
2022-10-10 15:06:31 - train: epoch 0013, iter [01000, 05004], lr: 0.096373, loss: 4.9094
2022-10-10 15:07:08 - train: epoch 0013, iter [01100, 05004], lr: 0.096361, loss: 4.8373
2022-10-10 15:07:44 - train: epoch 0013, iter [01200, 05004], lr: 0.096349, loss: 5.0514
2022-10-10 15:08:22 - train: epoch 0013, iter [01300, 05004], lr: 0.096337, loss: 5.2972
2022-10-10 15:08:58 - train: epoch 0013, iter [01400, 05004], lr: 0.096326, loss: 4.3779
2022-10-10 15:09:36 - train: epoch 0013, iter [01500, 05004], lr: 0.096314, loss: 5.0360
2022-10-10 15:10:12 - train: epoch 0013, iter [01600, 05004], lr: 0.096302, loss: 5.1928
2022-10-10 15:10:49 - train: epoch 0013, iter [01700, 05004], lr: 0.096290, loss: 4.6118
2022-10-10 15:11:26 - train: epoch 0013, iter [01800, 05004], lr: 0.096278, loss: 4.6634
2022-10-10 15:12:03 - train: epoch 0013, iter [01900, 05004], lr: 0.096266, loss: 4.2733
2022-10-10 15:12:40 - train: epoch 0013, iter [02000, 05004], lr: 0.096254, loss: 5.2767
2022-10-10 15:13:16 - train: epoch 0013, iter [02100, 05004], lr: 0.096242, loss: 4.6557
2022-10-10 15:13:54 - train: epoch 0013, iter [02200, 05004], lr: 0.096231, loss: 4.4111
2022-10-10 15:14:31 - train: epoch 0013, iter [02300, 05004], lr: 0.096219, loss: 4.7677
2022-10-10 15:15:08 - train: epoch 0013, iter [02400, 05004], lr: 0.096207, loss: 4.7613
2022-10-10 15:15:44 - train: epoch 0013, iter [02500, 05004], lr: 0.096195, loss: 4.3706
2022-10-10 15:16:21 - train: epoch 0013, iter [02600, 05004], lr: 0.096183, loss: 4.8282
2022-10-10 15:16:58 - train: epoch 0013, iter [02700, 05004], lr: 0.096171, loss: 4.4727
2022-10-10 15:17:35 - train: epoch 0013, iter [02800, 05004], lr: 0.096158, loss: 5.3003
2022-10-10 15:18:11 - train: epoch 0013, iter [02900, 05004], lr: 0.096146, loss: 4.8960
2022-10-10 15:18:48 - train: epoch 0013, iter [03000, 05004], lr: 0.096134, loss: 5.2644
2022-10-10 15:19:25 - train: epoch 0013, iter [03100, 05004], lr: 0.096122, loss: 4.5840
2022-10-10 15:20:01 - train: epoch 0013, iter [03200, 05004], lr: 0.096110, loss: 4.2384
2022-10-10 15:20:39 - train: epoch 0013, iter [03300, 05004], lr: 0.096098, loss: 4.8574
2022-10-10 15:21:16 - train: epoch 0013, iter [03400, 05004], lr: 0.096086, loss: 4.6920
2022-10-10 15:21:52 - train: epoch 0013, iter [03500, 05004], lr: 0.096074, loss: 4.8330
2022-10-10 15:22:29 - train: epoch 0013, iter [03600, 05004], lr: 0.096061, loss: 5.0408
2022-10-10 15:23:07 - train: epoch 0013, iter [03700, 05004], lr: 0.096049, loss: 4.9496
2022-10-10 15:23:43 - train: epoch 0013, iter [03800, 05004], lr: 0.096037, loss: 5.0075
2022-10-10 15:24:19 - train: epoch 0013, iter [03900, 05004], lr: 0.096025, loss: 5.1762
2022-10-10 15:24:56 - train: epoch 0013, iter [04000, 05004], lr: 0.096012, loss: 4.9123
2022-10-10 15:25:33 - train: epoch 0013, iter [04100, 05004], lr: 0.096000, loss: 4.6915
2022-10-10 15:26:10 - train: epoch 0013, iter [04200, 05004], lr: 0.095988, loss: 5.4615
2022-10-10 15:26:47 - train: epoch 0013, iter [04300, 05004], lr: 0.095975, loss: 5.0672
2022-10-10 15:27:24 - train: epoch 0013, iter [04400, 05004], lr: 0.095963, loss: 5.5014
2022-10-10 15:28:01 - train: epoch 0013, iter [04500, 05004], lr: 0.095951, loss: 3.7644
2022-10-10 15:28:38 - train: epoch 0013, iter [04600, 05004], lr: 0.095938, loss: 4.9016
2022-10-10 15:29:15 - train: epoch 0013, iter [04700, 05004], lr: 0.095926, loss: 4.6418
2022-10-10 15:29:51 - train: epoch 0013, iter [04800, 05004], lr: 0.095914, loss: 4.9209
2022-10-10 15:30:28 - train: epoch 0013, iter [04900, 05004], lr: 0.095901, loss: 5.1626
2022-10-10 15:31:03 - train: epoch 0013, iter [05000, 05004], lr: 0.095889, loss: 4.9473
2022-10-10 15:31:06 - train: epoch 013, train_loss: 4.7698
2022-10-10 15:32:25 - eval: epoch: 013, acc1: 48.646%, acc5: 74.452%, test_loss: 2.4739, per_image_load_time: 2.230ms, per_image_inference_time: 0.570ms
2022-10-10 15:32:25 - until epoch: 013, best_acc1: 48.646%
2022-10-10 15:32:25 - epoch 014 lr: 0.095888
2022-10-10 15:33:07 - train: epoch 0014, iter [00100, 05004], lr: 0.095876, loss: 4.8077
2022-10-10 15:33:43 - train: epoch 0014, iter [00200, 05004], lr: 0.095863, loss: 5.5342
2022-10-10 15:34:20 - train: epoch 0014, iter [00300, 05004], lr: 0.095851, loss: 4.9147
2022-10-10 15:34:57 - train: epoch 0014, iter [00400, 05004], lr: 0.095838, loss: 4.5476
2022-10-10 15:35:34 - train: epoch 0014, iter [00500, 05004], lr: 0.095826, loss: 4.4131
2022-10-10 15:36:10 - train: epoch 0014, iter [00600, 05004], lr: 0.095813, loss: 4.5802
2022-10-10 15:36:48 - train: epoch 0014, iter [00700, 05004], lr: 0.095800, loss: 4.2377
2022-10-10 15:37:24 - train: epoch 0014, iter [00800, 05004], lr: 0.095788, loss: 4.7518
2022-10-10 15:38:00 - train: epoch 0014, iter [00900, 05004], lr: 0.095775, loss: 4.6833
2022-10-10 15:38:36 - train: epoch 0014, iter [01000, 05004], lr: 0.095763, loss: 5.2306
2022-10-10 15:39:13 - train: epoch 0014, iter [01100, 05004], lr: 0.095750, loss: 4.6889
2022-10-10 15:39:50 - train: epoch 0014, iter [01200, 05004], lr: 0.095737, loss: 4.9578
2022-10-10 15:40:26 - train: epoch 0014, iter [01300, 05004], lr: 0.095725, loss: 4.4956
2022-10-10 15:41:03 - train: epoch 0014, iter [01400, 05004], lr: 0.095712, loss: 5.4125
2022-10-10 15:41:41 - train: epoch 0014, iter [01500, 05004], lr: 0.095699, loss: 4.9280
2022-10-10 15:42:17 - train: epoch 0014, iter [01600, 05004], lr: 0.095686, loss: 5.0795
2022-10-10 15:42:54 - train: epoch 0014, iter [01700, 05004], lr: 0.095674, loss: 3.9801
2022-10-10 15:43:30 - train: epoch 0014, iter [01800, 05004], lr: 0.095661, loss: 4.7457
2022-10-10 15:44:07 - train: epoch 0014, iter [01900, 05004], lr: 0.095648, loss: 5.4859
2022-10-10 15:44:43 - train: epoch 0014, iter [02000, 05004], lr: 0.095635, loss: 4.5200
2022-10-10 15:45:20 - train: epoch 0014, iter [02100, 05004], lr: 0.095622, loss: 5.4300
2022-10-10 15:45:57 - train: epoch 0014, iter [02200, 05004], lr: 0.095610, loss: 4.5003
2022-10-10 15:46:33 - train: epoch 0014, iter [02300, 05004], lr: 0.095597, loss: 4.5678
2022-10-10 15:47:11 - train: epoch 0014, iter [02400, 05004], lr: 0.095584, loss: 5.1228
2022-10-10 15:47:49 - train: epoch 0014, iter [02500, 05004], lr: 0.095571, loss: 4.5294
2022-10-10 15:48:25 - train: epoch 0014, iter [02600, 05004], lr: 0.095558, loss: 4.9326
2022-10-10 15:49:01 - train: epoch 0014, iter [02700, 05004], lr: 0.095545, loss: 4.5729
2022-10-10 15:49:40 - train: epoch 0014, iter [02800, 05004], lr: 0.095532, loss: 4.7370
2022-10-10 15:50:15 - train: epoch 0014, iter [02900, 05004], lr: 0.095519, loss: 4.7350
2022-10-10 15:50:53 - train: epoch 0014, iter [03000, 05004], lr: 0.095506, loss: 5.1296
2022-10-10 15:51:30 - train: epoch 0014, iter [03100, 05004], lr: 0.095493, loss: 4.9886
2022-10-10 15:52:07 - train: epoch 0014, iter [03200, 05004], lr: 0.095480, loss: 5.1118
2022-10-10 15:52:43 - train: epoch 0014, iter [03300, 05004], lr: 0.095467, loss: 5.3338
2022-10-10 15:53:21 - train: epoch 0014, iter [03400, 05004], lr: 0.095454, loss: 4.2673
2022-10-10 15:53:58 - train: epoch 0014, iter [03500, 05004], lr: 0.095441, loss: 4.4213
2022-10-10 15:54:34 - train: epoch 0014, iter [03600, 05004], lr: 0.095428, loss: 4.3422
2022-10-10 15:55:12 - train: epoch 0014, iter [03700, 05004], lr: 0.095415, loss: 4.3410
2022-10-10 15:55:49 - train: epoch 0014, iter [03800, 05004], lr: 0.095401, loss: 5.2295
2022-10-10 15:56:26 - train: epoch 0014, iter [03900, 05004], lr: 0.095388, loss: 4.9772
2022-10-10 15:57:03 - train: epoch 0014, iter [04000, 05004], lr: 0.095375, loss: 4.5443
2022-10-10 15:57:39 - train: epoch 0014, iter [04100, 05004], lr: 0.095362, loss: 4.8121
2022-10-10 15:58:16 - train: epoch 0014, iter [04200, 05004], lr: 0.095349, loss: 4.1458
2022-10-10 15:58:54 - train: epoch 0014, iter [04300, 05004], lr: 0.095335, loss: 3.8222
2022-10-10 15:59:30 - train: epoch 0014, iter [04400, 05004], lr: 0.095322, loss: 4.5673
2022-10-10 16:00:08 - train: epoch 0014, iter [04500, 05004], lr: 0.095309, loss: 5.2131
2022-10-10 16:00:44 - train: epoch 0014, iter [04600, 05004], lr: 0.095296, loss: 4.8029
2022-10-10 16:01:21 - train: epoch 0014, iter [04700, 05004], lr: 0.095282, loss: 4.4228
2022-10-10 16:01:58 - train: epoch 0014, iter [04800, 05004], lr: 0.095269, loss: 4.3705
2022-10-10 16:02:35 - train: epoch 0014, iter [04900, 05004], lr: 0.095256, loss: 4.0440
2022-10-10 16:03:10 - train: epoch 0014, iter [05000, 05004], lr: 0.095242, loss: 4.4623
2022-10-10 16:03:12 - train: epoch 014, train_loss: 4.7422
2022-10-10 16:04:29 - eval: epoch: 014, acc1: 47.258%, acc5: 73.070%, test_loss: 2.5355, per_image_load_time: 1.883ms, per_image_inference_time: 0.586ms
2022-10-10 16:04:29 - until epoch: 014, best_acc1: 48.646%
2022-10-10 16:04:29 - epoch 015 lr: 0.095242
2022-10-10 16:05:11 - train: epoch 0015, iter [00100, 05004], lr: 0.095228, loss: 4.0523
2022-10-10 16:05:48 - train: epoch 0015, iter [00200, 05004], lr: 0.095215, loss: 5.0311
2022-10-10 16:06:25 - train: epoch 0015, iter [00300, 05004], lr: 0.095202, loss: 4.8662
2022-10-10 16:07:01 - train: epoch 0015, iter [00400, 05004], lr: 0.095188, loss: 5.1750
2022-10-10 16:07:37 - train: epoch 0015, iter [00500, 05004], lr: 0.095175, loss: 4.9146
2022-10-10 16:08:14 - train: epoch 0015, iter [00600, 05004], lr: 0.095161, loss: 4.7955
2022-10-10 16:08:51 - train: epoch 0015, iter [00700, 05004], lr: 0.095148, loss: 5.0171
2022-10-10 16:09:27 - train: epoch 0015, iter [00800, 05004], lr: 0.095134, loss: 3.9552
2022-10-10 16:10:05 - train: epoch 0015, iter [00900, 05004], lr: 0.095121, loss: 4.4458
2022-10-10 16:10:42 - train: epoch 0015, iter [01000, 05004], lr: 0.095107, loss: 4.8461
2022-10-10 16:11:18 - train: epoch 0015, iter [01100, 05004], lr: 0.095094, loss: 4.4839
2022-10-10 16:11:55 - train: epoch 0015, iter [01200, 05004], lr: 0.095080, loss: 4.9518
2022-10-10 16:12:31 - train: epoch 0015, iter [01300, 05004], lr: 0.095067, loss: 4.9223
2022-10-10 16:13:08 - train: epoch 0015, iter [01400, 05004], lr: 0.095053, loss: 4.2976
2022-10-10 16:13:45 - train: epoch 0015, iter [01500, 05004], lr: 0.095039, loss: 4.9497
2022-10-10 16:14:22 - train: epoch 0015, iter [01600, 05004], lr: 0.095026, loss: 5.2831
2022-10-10 16:14:59 - train: epoch 0015, iter [01700, 05004], lr: 0.095012, loss: 4.6172
2022-10-10 16:15:37 - train: epoch 0015, iter [01800, 05004], lr: 0.094998, loss: 4.6897
2022-10-10 16:16:12 - train: epoch 0015, iter [01900, 05004], lr: 0.094985, loss: 5.1374
2022-10-10 16:16:49 - train: epoch 0015, iter [02000, 05004], lr: 0.094971, loss: 4.5914
2022-10-10 16:17:26 - train: epoch 0015, iter [02100, 05004], lr: 0.094957, loss: 4.2566
2022-10-10 16:18:02 - train: epoch 0015, iter [02200, 05004], lr: 0.094944, loss: 4.6473
2022-10-10 16:18:39 - train: epoch 0015, iter [02300, 05004], lr: 0.094930, loss: 4.6122
2022-10-10 16:19:16 - train: epoch 0015, iter [02400, 05004], lr: 0.094916, loss: 4.2608
2022-10-10 16:19:53 - train: epoch 0015, iter [02500, 05004], lr: 0.094902, loss: 4.4074
2022-10-10 16:20:30 - train: epoch 0015, iter [02600, 05004], lr: 0.094888, loss: 4.8503
2022-10-10 16:21:07 - train: epoch 0015, iter [02700, 05004], lr: 0.094875, loss: 4.0328
2022-10-10 16:21:44 - train: epoch 0015, iter [02800, 05004], lr: 0.094861, loss: 4.6906
2022-10-10 16:22:22 - train: epoch 0015, iter [02900, 05004], lr: 0.094847, loss: 4.8980
2022-10-10 16:22:59 - train: epoch 0015, iter [03000, 05004], lr: 0.094833, loss: 4.8921
2022-10-10 16:23:35 - train: epoch 0015, iter [03100, 05004], lr: 0.094819, loss: 4.8260
2022-10-10 16:24:11 - train: epoch 0015, iter [03200, 05004], lr: 0.094805, loss: 3.9354
2022-10-10 16:24:48 - train: epoch 0015, iter [03300, 05004], lr: 0.094791, loss: 4.9860
2022-10-10 16:25:25 - train: epoch 0015, iter [03400, 05004], lr: 0.094777, loss: 5.0106
2022-10-10 16:26:02 - train: epoch 0015, iter [03500, 05004], lr: 0.094763, loss: 5.0537
2022-10-10 16:26:39 - train: epoch 0015, iter [03600, 05004], lr: 0.094749, loss: 4.8581
2022-10-10 16:27:16 - train: epoch 0015, iter [03700, 05004], lr: 0.094735, loss: 4.5993
2022-10-10 16:27:53 - train: epoch 0015, iter [03800, 05004], lr: 0.094721, loss: 4.2722
2022-10-10 16:28:30 - train: epoch 0015, iter [03900, 05004], lr: 0.094707, loss: 5.1914
2022-10-10 16:29:07 - train: epoch 0015, iter [04000, 05004], lr: 0.094693, loss: 5.1192
2022-10-10 16:29:44 - train: epoch 0015, iter [04100, 05004], lr: 0.094679, loss: 4.7598
2022-10-10 16:30:21 - train: epoch 0015, iter [04200, 05004], lr: 0.094665, loss: 4.8396
2022-10-10 16:30:58 - train: epoch 0015, iter [04300, 05004], lr: 0.094651, loss: 4.2855
2022-10-10 16:31:35 - train: epoch 0015, iter [04400, 05004], lr: 0.094637, loss: 5.0999
2022-10-10 16:32:12 - train: epoch 0015, iter [04500, 05004], lr: 0.094622, loss: 4.9164
2022-10-10 16:32:49 - train: epoch 0015, iter [04600, 05004], lr: 0.094608, loss: 4.9810
2022-10-10 16:33:26 - train: epoch 0015, iter [04700, 05004], lr: 0.094594, loss: 5.0310
2022-10-10 16:34:03 - train: epoch 0015, iter [04800, 05004], lr: 0.094580, loss: 5.1156
2022-10-10 16:34:40 - train: epoch 0015, iter [04900, 05004], lr: 0.094566, loss: 5.0598
2022-10-10 16:35:14 - train: epoch 0015, iter [05000, 05004], lr: 0.094551, loss: 4.7515
2022-10-10 16:35:17 - train: epoch 015, train_loss: 4.7211
2022-10-10 16:36:36 - eval: epoch: 015, acc1: 48.446%, acc5: 74.436%, test_loss: 2.5352, per_image_load_time: 2.113ms, per_image_inference_time: 0.590ms
2022-10-10 16:36:36 - until epoch: 015, best_acc1: 48.646%
2022-10-10 16:36:36 - epoch 016 lr: 0.094551
2022-10-10 16:37:19 - train: epoch 0016, iter [00100, 05004], lr: 0.094537, loss: 4.3303
2022-10-10 16:37:55 - train: epoch 0016, iter [00200, 05004], lr: 0.094522, loss: 4.9716
2022-10-10 16:38:32 - train: epoch 0016, iter [00300, 05004], lr: 0.094508, loss: 4.7705
2022-10-10 16:39:08 - train: epoch 0016, iter [00400, 05004], lr: 0.094494, loss: 4.8091
2022-10-10 16:39:44 - train: epoch 0016, iter [00500, 05004], lr: 0.094479, loss: 4.8077
2022-10-10 16:40:21 - train: epoch 0016, iter [00600, 05004], lr: 0.094465, loss: 5.2084
2022-10-10 16:40:58 - train: epoch 0016, iter [00700, 05004], lr: 0.094451, loss: 3.5772
2022-10-10 16:41:36 - train: epoch 0016, iter [00800, 05004], lr: 0.094436, loss: 4.8397
2022-10-10 16:42:12 - train: epoch 0016, iter [00900, 05004], lr: 0.094422, loss: 5.1745
2022-10-10 16:42:49 - train: epoch 0016, iter [01000, 05004], lr: 0.094407, loss: 4.9465
2022-10-10 16:43:25 - train: epoch 0016, iter [01100, 05004], lr: 0.094393, loss: 4.5883
2022-10-10 16:44:01 - train: epoch 0016, iter [01200, 05004], lr: 0.094379, loss: 4.3724
2022-10-10 16:44:38 - train: epoch 0016, iter [01300, 05004], lr: 0.094364, loss: 4.2790
2022-10-10 16:45:14 - train: epoch 0016, iter [01400, 05004], lr: 0.094350, loss: 4.2339
2022-10-10 16:45:51 - train: epoch 0016, iter [01500, 05004], lr: 0.094335, loss: 5.3398
2022-10-10 16:46:27 - train: epoch 0016, iter [01600, 05004], lr: 0.094321, loss: 3.8451
2022-10-10 16:47:04 - train: epoch 0016, iter [01700, 05004], lr: 0.094306, loss: 4.4117
2022-10-10 16:47:41 - train: epoch 0016, iter [01800, 05004], lr: 0.094292, loss: 4.7447
2022-10-10 16:48:18 - train: epoch 0016, iter [01900, 05004], lr: 0.094277, loss: 4.9044
2022-10-10 16:48:55 - train: epoch 0016, iter [02000, 05004], lr: 0.094262, loss: 4.6976
2022-10-10 16:49:32 - train: epoch 0016, iter [02100, 05004], lr: 0.094248, loss: 4.6530
2022-10-10 16:50:09 - train: epoch 0016, iter [02200, 05004], lr: 0.094233, loss: 4.6605
2022-10-10 16:50:45 - train: epoch 0016, iter [02300, 05004], lr: 0.094218, loss: 4.3189
2022-10-10 16:51:22 - train: epoch 0016, iter [02400, 05004], lr: 0.094204, loss: 4.7485
2022-10-10 16:51:59 - train: epoch 0016, iter [02500, 05004], lr: 0.094189, loss: 4.3316
2022-10-10 16:52:38 - train: epoch 0016, iter [02600, 05004], lr: 0.094174, loss: 5.0974
2022-10-10 16:53:14 - train: epoch 0016, iter [02700, 05004], lr: 0.094160, loss: 4.8027
2022-10-10 16:53:51 - train: epoch 0016, iter [02800, 05004], lr: 0.094145, loss: 4.8590
2022-10-10 16:54:28 - train: epoch 0016, iter [02900, 05004], lr: 0.094130, loss: 4.2688
2022-10-10 16:55:04 - train: epoch 0016, iter [03000, 05004], lr: 0.094116, loss: 3.7838
2022-10-10 16:55:42 - train: epoch 0016, iter [03100, 05004], lr: 0.094101, loss: 4.2649
2022-10-10 16:56:19 - train: epoch 0016, iter [03200, 05004], lr: 0.094086, loss: 4.2948
2022-10-10 16:56:55 - train: epoch 0016, iter [03300, 05004], lr: 0.094071, loss: 4.4894
2022-10-10 16:57:33 - train: epoch 0016, iter [03400, 05004], lr: 0.094056, loss: 5.4818
2022-10-10 16:58:10 - train: epoch 0016, iter [03500, 05004], lr: 0.094041, loss: 4.1618
2022-10-10 16:58:47 - train: epoch 0016, iter [03600, 05004], lr: 0.094027, loss: 5.1500
2022-10-10 16:59:24 - train: epoch 0016, iter [03700, 05004], lr: 0.094012, loss: 4.0757
2022-10-10 17:00:01 - train: epoch 0016, iter [03800, 05004], lr: 0.093997, loss: 5.3245
2022-10-10 17:00:38 - train: epoch 0016, iter [03900, 05004], lr: 0.093982, loss: 4.9643
2022-10-10 17:01:15 - train: epoch 0016, iter [04000, 05004], lr: 0.093967, loss: 4.8723
2022-10-10 17:01:53 - train: epoch 0016, iter [04100, 05004], lr: 0.093952, loss: 4.8895
2022-10-10 17:02:29 - train: epoch 0016, iter [04200, 05004], lr: 0.093937, loss: 4.7311
2022-10-10 17:03:07 - train: epoch 0016, iter [04300, 05004], lr: 0.093922, loss: 3.5622
2022-10-10 17:03:43 - train: epoch 0016, iter [04400, 05004], lr: 0.093907, loss: 4.7716
2022-10-10 17:04:20 - train: epoch 0016, iter [04500, 05004], lr: 0.093892, loss: 5.0090
2022-10-10 17:04:57 - train: epoch 0016, iter [04600, 05004], lr: 0.093877, loss: 4.3457
2022-10-10 17:05:35 - train: epoch 0016, iter [04700, 05004], lr: 0.093862, loss: 4.4969
2022-10-10 17:06:12 - train: epoch 0016, iter [04800, 05004], lr: 0.093847, loss: 5.2920
2022-10-10 17:06:49 - train: epoch 0016, iter [04900, 05004], lr: 0.093832, loss: 4.1596
2022-10-10 17:07:23 - train: epoch 0016, iter [05000, 05004], lr: 0.093817, loss: 5.0822
2022-10-10 17:07:26 - train: epoch 016, train_loss: 4.7069
2022-10-10 17:08:46 - eval: epoch: 016, acc1: 50.832%, acc5: 76.298%, test_loss: 2.4378, per_image_load_time: 2.381ms, per_image_inference_time: 0.573ms
2022-10-10 17:08:46 - until epoch: 016, best_acc1: 50.832%
2022-10-10 17:08:46 - epoch 017 lr: 0.093816
2022-10-10 17:09:28 - train: epoch 0017, iter [00100, 05004], lr: 0.093801, loss: 5.1669
2022-10-10 17:10:05 - train: epoch 0017, iter [00200, 05004], lr: 0.093786, loss: 4.3001
2022-10-10 17:10:42 - train: epoch 0017, iter [00300, 05004], lr: 0.093771, loss: 4.8072
2022-10-10 17:11:19 - train: epoch 0017, iter [00400, 05004], lr: 0.093755, loss: 4.5206
2022-10-10 17:11:56 - train: epoch 0017, iter [00500, 05004], lr: 0.093740, loss: 5.3453
2022-10-10 17:12:33 - train: epoch 0017, iter [00600, 05004], lr: 0.093725, loss: 4.5600
2022-10-10 17:13:09 - train: epoch 0017, iter [00700, 05004], lr: 0.093710, loss: 5.0913
2022-10-10 17:13:46 - train: epoch 0017, iter [00800, 05004], lr: 0.093694, loss: 4.3204
2022-10-10 17:14:23 - train: epoch 0017, iter [00900, 05004], lr: 0.093679, loss: 3.8510
2022-10-10 17:15:00 - train: epoch 0017, iter [01000, 05004], lr: 0.093664, loss: 4.3272
2022-10-10 17:15:38 - train: epoch 0017, iter [01100, 05004], lr: 0.093649, loss: 4.8957
2022-10-10 17:16:14 - train: epoch 0017, iter [01200, 05004], lr: 0.093633, loss: 5.3832
2022-10-10 17:16:52 - train: epoch 0017, iter [01300, 05004], lr: 0.093618, loss: 4.4288
2022-10-10 17:17:29 - train: epoch 0017, iter [01400, 05004], lr: 0.093603, loss: 4.8632
2022-10-10 17:18:07 - train: epoch 0017, iter [01500, 05004], lr: 0.093587, loss: 4.4926
2022-10-10 17:18:43 - train: epoch 0017, iter [01600, 05004], lr: 0.093572, loss: 4.7107
2022-10-10 17:19:21 - train: epoch 0017, iter [01700, 05004], lr: 0.093556, loss: 4.5035
2022-10-10 17:19:58 - train: epoch 0017, iter [01800, 05004], lr: 0.093541, loss: 4.7918
2022-10-10 17:20:34 - train: epoch 0017, iter [01900, 05004], lr: 0.093526, loss: 4.4463
2022-10-10 17:21:11 - train: epoch 0017, iter [02000, 05004], lr: 0.093510, loss: 4.9879
2022-10-10 17:21:48 - train: epoch 0017, iter [02100, 05004], lr: 0.093495, loss: 4.9011
2022-10-10 17:22:25 - train: epoch 0017, iter [02200, 05004], lr: 0.093479, loss: 4.6648
2022-10-10 17:23:02 - train: epoch 0017, iter [02300, 05004], lr: 0.093464, loss: 4.3658
2022-10-10 17:23:39 - train: epoch 0017, iter [02400, 05004], lr: 0.093448, loss: 4.5807
2022-10-10 17:24:16 - train: epoch 0017, iter [02500, 05004], lr: 0.093433, loss: 5.1871
2022-10-10 17:24:53 - train: epoch 0017, iter [02600, 05004], lr: 0.093417, loss: 5.2997
2022-10-10 17:25:29 - train: epoch 0017, iter [02700, 05004], lr: 0.093401, loss: 4.3635
2022-10-10 17:26:07 - train: epoch 0017, iter [02800, 05004], lr: 0.093386, loss: 5.2498
2022-10-10 17:26:43 - train: epoch 0017, iter [02900, 05004], lr: 0.093370, loss: 5.0898
2022-10-10 17:27:20 - train: epoch 0017, iter [03000, 05004], lr: 0.093355, loss: 4.2590
2022-10-10 17:27:57 - train: epoch 0017, iter [03100, 05004], lr: 0.093339, loss: 5.1317
2022-10-10 17:28:34 - train: epoch 0017, iter [03200, 05004], lr: 0.093323, loss: 3.8094
2022-10-10 17:29:11 - train: epoch 0017, iter [03300, 05004], lr: 0.093308, loss: 4.9176
2022-10-10 17:29:49 - train: epoch 0017, iter [03400, 05004], lr: 0.093292, loss: 4.1726
2022-10-10 17:30:25 - train: epoch 0017, iter [03500, 05004], lr: 0.093276, loss: 4.8625
2022-10-10 17:31:01 - train: epoch 0017, iter [03600, 05004], lr: 0.093260, loss: 4.4355
2022-10-10 17:31:39 - train: epoch 0017, iter [03700, 05004], lr: 0.093245, loss: 4.6493
2022-10-10 17:32:16 - train: epoch 0017, iter [03800, 05004], lr: 0.093229, loss: 4.8274
2022-10-10 17:32:53 - train: epoch 0017, iter [03900, 05004], lr: 0.093213, loss: 4.1454
2022-10-10 17:33:29 - train: epoch 0017, iter [04000, 05004], lr: 0.093197, loss: 4.9058
2022-10-10 17:34:07 - train: epoch 0017, iter [04100, 05004], lr: 0.093182, loss: 3.9363
2022-10-10 17:34:43 - train: epoch 0017, iter [04200, 05004], lr: 0.093166, loss: 4.6309
2022-10-10 17:35:20 - train: epoch 0017, iter [04300, 05004], lr: 0.093150, loss: 5.2879
2022-10-10 17:35:57 - train: epoch 0017, iter [04400, 05004], lr: 0.093134, loss: 4.9270
2022-10-10 17:36:34 - train: epoch 0017, iter [04500, 05004], lr: 0.093118, loss: 4.3494
2022-10-10 17:37:11 - train: epoch 0017, iter [04600, 05004], lr: 0.093102, loss: 4.5701
2022-10-10 17:37:48 - train: epoch 0017, iter [04700, 05004], lr: 0.093086, loss: 4.9075
2022-10-10 17:38:25 - train: epoch 0017, iter [04800, 05004], lr: 0.093070, loss: 4.7478
2022-10-10 17:39:03 - train: epoch 0017, iter [04900, 05004], lr: 0.093054, loss: 4.9142
2022-10-10 17:39:37 - train: epoch 0017, iter [05000, 05004], lr: 0.093038, loss: 4.1636
2022-10-10 17:39:39 - train: epoch 017, train_loss: 4.6869
2022-10-10 17:40:59 - eval: epoch: 017, acc1: 51.830%, acc5: 77.266%, test_loss: 2.3483, per_image_load_time: 1.806ms, per_image_inference_time: 0.572ms
2022-10-10 17:41:00 - until epoch: 017, best_acc1: 51.830%
2022-10-10 17:41:00 - epoch 018 lr: 0.093038
2022-10-10 17:41:42 - train: epoch 0018, iter [00100, 05004], lr: 0.093022, loss: 4.8752
2022-10-10 17:42:18 - train: epoch 0018, iter [00200, 05004], lr: 0.093006, loss: 4.8514
2022-10-10 17:42:54 - train: epoch 0018, iter [00300, 05004], lr: 0.092990, loss: 3.7791
2022-10-10 17:43:31 - train: epoch 0018, iter [00400, 05004], lr: 0.092974, loss: 5.3533
2022-10-10 17:44:08 - train: epoch 0018, iter [00500, 05004], lr: 0.092958, loss: 4.6410
2022-10-10 17:44:45 - train: epoch 0018, iter [00600, 05004], lr: 0.092942, loss: 3.8262
2022-10-10 17:45:21 - train: epoch 0018, iter [00700, 05004], lr: 0.092926, loss: 4.7927
2022-10-10 17:45:58 - train: epoch 0018, iter [00800, 05004], lr: 0.092909, loss: 4.9049
2022-10-10 17:46:35 - train: epoch 0018, iter [00900, 05004], lr: 0.092893, loss: 4.5680
2022-10-10 17:47:11 - train: epoch 0018, iter [01000, 05004], lr: 0.092877, loss: 4.2264
2022-10-10 17:47:48 - train: epoch 0018, iter [01100, 05004], lr: 0.092861, loss: 5.2360
2022-10-10 17:48:24 - train: epoch 0018, iter [01200, 05004], lr: 0.092845, loss: 5.1384
2022-10-10 17:49:01 - train: epoch 0018, iter [01300, 05004], lr: 0.092829, loss: 4.2548
2022-10-10 17:49:38 - train: epoch 0018, iter [01400, 05004], lr: 0.092812, loss: 4.6093
2022-10-10 17:50:15 - train: epoch 0018, iter [01500, 05004], lr: 0.092796, loss: 5.0679
2022-10-10 17:50:52 - train: epoch 0018, iter [01600, 05004], lr: 0.092780, loss: 3.9714
2022-10-10 17:51:29 - train: epoch 0018, iter [01700, 05004], lr: 0.092764, loss: 4.4097
2022-10-10 17:52:06 - train: epoch 0018, iter [01800, 05004], lr: 0.092747, loss: 4.9192
2022-10-10 17:52:43 - train: epoch 0018, iter [01900, 05004], lr: 0.092731, loss: 4.8329
2022-10-10 17:53:20 - train: epoch 0018, iter [02000, 05004], lr: 0.092715, loss: 5.1991
2022-10-10 17:53:57 - train: epoch 0018, iter [02100, 05004], lr: 0.092699, loss: 4.7121
2022-10-10 17:54:34 - train: epoch 0018, iter [02200, 05004], lr: 0.092682, loss: 4.3061
2022-10-10 17:55:10 - train: epoch 0018, iter [02300, 05004], lr: 0.092666, loss: 4.3891
2022-10-10 17:55:47 - train: epoch 0018, iter [02400, 05004], lr: 0.092649, loss: 4.2750
2022-10-10 17:56:24 - train: epoch 0018, iter [02500, 05004], lr: 0.092633, loss: 4.1319
2022-10-10 17:57:00 - train: epoch 0018, iter [02600, 05004], lr: 0.092617, loss: 4.8459
2022-10-10 17:57:37 - train: epoch 0018, iter [02700, 05004], lr: 0.092600, loss: 4.4534
2022-10-10 17:58:14 - train: epoch 0018, iter [02800, 05004], lr: 0.092584, loss: 4.1842
2022-10-10 17:58:51 - train: epoch 0018, iter [02900, 05004], lr: 0.092567, loss: 5.1582
2022-10-10 17:59:29 - train: epoch 0018, iter [03000, 05004], lr: 0.092551, loss: 4.7692
2022-10-10 18:00:05 - train: epoch 0018, iter [03100, 05004], lr: 0.092534, loss: 5.2368
2022-10-10 18:00:42 - train: epoch 0018, iter [03200, 05004], lr: 0.092518, loss: 5.1247
2022-10-10 18:01:19 - train: epoch 0018, iter [03300, 05004], lr: 0.092501, loss: 4.9105
2022-10-10 18:01:56 - train: epoch 0018, iter [03400, 05004], lr: 0.092485, loss: 4.6636
2022-10-10 18:02:32 - train: epoch 0018, iter [03500, 05004], lr: 0.092468, loss: 4.6098
2022-10-10 18:03:09 - train: epoch 0018, iter [03600, 05004], lr: 0.092452, loss: 4.9034
2022-10-10 18:03:45 - train: epoch 0018, iter [03700, 05004], lr: 0.092435, loss: 4.8775
2022-10-10 18:04:23 - train: epoch 0018, iter [03800, 05004], lr: 0.092418, loss: 4.2484
2022-10-10 18:05:00 - train: epoch 0018, iter [03900, 05004], lr: 0.092402, loss: 4.8074
2022-10-10 18:05:37 - train: epoch 0018, iter [04000, 05004], lr: 0.092385, loss: 5.1489
2022-10-10 18:06:14 - train: epoch 0018, iter [04100, 05004], lr: 0.092369, loss: 4.5129
2022-10-10 18:06:50 - train: epoch 0018, iter [04200, 05004], lr: 0.092352, loss: 4.4158
2022-10-10 18:07:28 - train: epoch 0018, iter [04300, 05004], lr: 0.092335, loss: 4.4062
2022-10-10 18:08:03 - train: epoch 0018, iter [04400, 05004], lr: 0.092318, loss: 4.7767
2022-10-10 18:08:40 - train: epoch 0018, iter [04500, 05004], lr: 0.092302, loss: 4.8551
2022-10-10 18:09:18 - train: epoch 0018, iter [04600, 05004], lr: 0.092285, loss: 5.0028
2022-10-10 18:09:56 - train: epoch 0018, iter [04700, 05004], lr: 0.092268, loss: 4.8774
2022-10-10 18:10:32 - train: epoch 0018, iter [04800, 05004], lr: 0.092251, loss: 4.9749
2022-10-10 18:11:09 - train: epoch 0018, iter [04900, 05004], lr: 0.092235, loss: 4.1999
2022-10-10 18:11:44 - train: epoch 0018, iter [05000, 05004], lr: 0.092218, loss: 4.4509
2022-10-10 18:11:46 - train: epoch 018, train_loss: 4.6678
2022-10-10 18:13:04 - eval: epoch: 018, acc1: 50.864%, acc5: 76.674%, test_loss: 2.4105, per_image_load_time: 1.630ms, per_image_inference_time: 0.599ms
2022-10-10 18:13:04 - until epoch: 018, best_acc1: 51.830%
2022-10-10 18:13:04 - epoch 019 lr: 0.092217
2022-10-10 18:13:47 - train: epoch 0019, iter [00100, 05004], lr: 0.092200, loss: 5.3621
2022-10-10 18:14:24 - train: epoch 0019, iter [00200, 05004], lr: 0.092184, loss: 4.4820
2022-10-10 18:15:00 - train: epoch 0019, iter [00300, 05004], lr: 0.092167, loss: 4.8620
2022-10-10 18:15:37 - train: epoch 0019, iter [00400, 05004], lr: 0.092150, loss: 4.8968
2022-10-10 18:16:14 - train: epoch 0019, iter [00500, 05004], lr: 0.092133, loss: 4.5463
2022-10-10 18:16:49 - train: epoch 0019, iter [00600, 05004], lr: 0.092116, loss: 4.8716
2022-10-10 18:17:26 - train: epoch 0019, iter [00700, 05004], lr: 0.092099, loss: 4.6157
2022-10-10 18:18:04 - train: epoch 0019, iter [00800, 05004], lr: 0.092082, loss: 4.9527
2022-10-10 18:18:40 - train: epoch 0019, iter [00900, 05004], lr: 0.092065, loss: 4.6593
2022-10-10 18:19:17 - train: epoch 0019, iter [01000, 05004], lr: 0.092048, loss: 4.8878
2022-10-10 18:19:53 - train: epoch 0019, iter [01100, 05004], lr: 0.092031, loss: 4.2673
2022-10-10 18:20:30 - train: epoch 0019, iter [01200, 05004], lr: 0.092014, loss: 4.0023
2022-10-10 18:21:06 - train: epoch 0019, iter [01300, 05004], lr: 0.091997, loss: 5.3584
2022-10-10 18:21:43 - train: epoch 0019, iter [01400, 05004], lr: 0.091980, loss: 4.7134
2022-10-10 18:22:21 - train: epoch 0019, iter [01500, 05004], lr: 0.091963, loss: 4.7954
2022-10-10 18:22:57 - train: epoch 0019, iter [01600, 05004], lr: 0.091946, loss: 4.3848
2022-10-10 18:23:33 - train: epoch 0019, iter [01700, 05004], lr: 0.091929, loss: 4.9093
2022-10-10 18:24:10 - train: epoch 0019, iter [01800, 05004], lr: 0.091912, loss: 5.2522
2022-10-10 18:24:46 - train: epoch 0019, iter [01900, 05004], lr: 0.091895, loss: 4.9303
2022-10-10 18:25:24 - train: epoch 0019, iter [02000, 05004], lr: 0.091877, loss: 4.7438
2022-10-10 18:26:00 - train: epoch 0019, iter [02100, 05004], lr: 0.091860, loss: 3.2345
2022-10-10 18:26:37 - train: epoch 0019, iter [02200, 05004], lr: 0.091843, loss: 4.3146
2022-10-10 18:27:14 - train: epoch 0019, iter [02300, 05004], lr: 0.091826, loss: 5.1968
2022-10-10 18:27:51 - train: epoch 0019, iter [02400, 05004], lr: 0.091809, loss: 5.0427
2022-10-10 18:28:28 - train: epoch 0019, iter [02500, 05004], lr: 0.091792, loss: 4.0633
2022-10-10 18:29:06 - train: epoch 0019, iter [02600, 05004], lr: 0.091774, loss: 4.8363
2022-10-10 18:29:42 - train: epoch 0019, iter [02700, 05004], lr: 0.091757, loss: 4.6095
2022-10-10 18:30:19 - train: epoch 0019, iter [02800, 05004], lr: 0.091740, loss: 5.1740
2022-10-10 18:30:55 - train: epoch 0019, iter [02900, 05004], lr: 0.091722, loss: 5.2750
2022-10-10 18:31:33 - train: epoch 0019, iter [03000, 05004], lr: 0.091705, loss: 4.5774
2022-10-10 18:32:09 - train: epoch 0019, iter [03100, 05004], lr: 0.091688, loss: 4.1236
2022-10-10 18:32:47 - train: epoch 0019, iter [03200, 05004], lr: 0.091671, loss: 4.9296
2022-10-10 18:33:24 - train: epoch 0019, iter [03300, 05004], lr: 0.091653, loss: 4.6858
2022-10-10 18:34:01 - train: epoch 0019, iter [03400, 05004], lr: 0.091636, loss: 4.8135
2022-10-10 18:34:38 - train: epoch 0019, iter [03500, 05004], lr: 0.091618, loss: 4.7900
2022-10-10 18:35:14 - train: epoch 0019, iter [03600, 05004], lr: 0.091601, loss: 5.1771
2022-10-10 18:35:51 - train: epoch 0019, iter [03700, 05004], lr: 0.091584, loss: 4.3801
2022-10-10 18:36:29 - train: epoch 0019, iter [03800, 05004], lr: 0.091566, loss: 4.5925
2022-10-10 18:37:05 - train: epoch 0019, iter [03900, 05004], lr: 0.091549, loss: 4.5719
2022-10-10 18:37:43 - train: epoch 0019, iter [04000, 05004], lr: 0.091531, loss: 4.7622
2022-10-10 18:38:20 - train: epoch 0019, iter [04100, 05004], lr: 0.091514, loss: 4.2568
2022-10-10 18:38:56 - train: epoch 0019, iter [04200, 05004], lr: 0.091496, loss: 4.8994
2022-10-10 18:39:33 - train: epoch 0019, iter [04300, 05004], lr: 0.091479, loss: 4.6172
2022-10-10 18:40:10 - train: epoch 0019, iter [04400, 05004], lr: 0.091461, loss: 4.5032
2022-10-10 18:40:47 - train: epoch 0019, iter [04500, 05004], lr: 0.091444, loss: 4.5542
2022-10-10 18:41:24 - train: epoch 0019, iter [04600, 05004], lr: 0.091426, loss: 3.7617
2022-10-10 18:42:00 - train: epoch 0019, iter [04700, 05004], lr: 0.091408, loss: 4.0148
2022-10-10 18:42:37 - train: epoch 0019, iter [04800, 05004], lr: 0.091391, loss: 4.7224
2022-10-10 18:43:15 - train: epoch 0019, iter [04900, 05004], lr: 0.091373, loss: 5.0350
2022-10-10 18:43:49 - train: epoch 0019, iter [05000, 05004], lr: 0.091356, loss: 5.1013
2022-10-10 18:43:51 - train: epoch 019, train_loss: 4.6375
2022-10-10 18:45:10 - eval: epoch: 019, acc1: 51.162%, acc5: 76.516%, test_loss: 2.3736, per_image_load_time: 1.874ms, per_image_inference_time: 0.581ms
2022-10-10 18:45:11 - until epoch: 019, best_acc1: 51.830%
2022-10-10 18:45:11 - epoch 020 lr: 0.091355
2022-10-10 18:45:54 - train: epoch 0020, iter [00100, 05004], lr: 0.091337, loss: 4.3331
2022-10-10 18:46:30 - train: epoch 0020, iter [00200, 05004], lr: 0.091320, loss: 4.2316
2022-10-10 18:47:06 - train: epoch 0020, iter [00300, 05004], lr: 0.091302, loss: 4.2176
2022-10-10 18:47:43 - train: epoch 0020, iter [00400, 05004], lr: 0.091284, loss: 4.6344
2022-10-10 18:48:18 - train: epoch 0020, iter [00500, 05004], lr: 0.091266, loss: 4.7698
2022-10-10 18:48:55 - train: epoch 0020, iter [00600, 05004], lr: 0.091249, loss: 4.9122
2022-10-10 18:49:32 - train: epoch 0020, iter [00700, 05004], lr: 0.091231, loss: 4.7655
2022-10-10 18:50:08 - train: epoch 0020, iter [00800, 05004], lr: 0.091213, loss: 5.0184
2022-10-10 18:50:45 - train: epoch 0020, iter [00900, 05004], lr: 0.091195, loss: 4.6518
2022-10-10 18:51:21 - train: epoch 0020, iter [01000, 05004], lr: 0.091178, loss: 4.3163
2022-10-10 18:51:59 - train: epoch 0020, iter [01100, 05004], lr: 0.091160, loss: 3.7509
2022-10-10 18:52:36 - train: epoch 0020, iter [01200, 05004], lr: 0.091142, loss: 5.0579
2022-10-10 18:53:13 - train: epoch 0020, iter [01300, 05004], lr: 0.091124, loss: 4.7461
2022-10-10 18:53:51 - train: epoch 0020, iter [01400, 05004], lr: 0.091106, loss: 4.4255
2022-10-10 18:54:27 - train: epoch 0020, iter [01500, 05004], lr: 0.091088, loss: 5.3390
2022-10-10 18:55:04 - train: epoch 0020, iter [01600, 05004], lr: 0.091071, loss: 4.4526
2022-10-10 18:55:41 - train: epoch 0020, iter [01700, 05004], lr: 0.091053, loss: 4.3008
2022-10-10 18:56:17 - train: epoch 0020, iter [01800, 05004], lr: 0.091035, loss: 4.3046
2022-10-10 18:56:54 - train: epoch 0020, iter [01900, 05004], lr: 0.091017, loss: 4.2583
2022-10-10 18:57:31 - train: epoch 0020, iter [02000, 05004], lr: 0.090999, loss: 4.3728
2022-10-10 18:58:08 - train: epoch 0020, iter [02100, 05004], lr: 0.090981, loss: 4.3115
2022-10-10 18:58:44 - train: epoch 0020, iter [02200, 05004], lr: 0.090963, loss: 4.5941
2022-10-10 18:59:21 - train: epoch 0020, iter [02300, 05004], lr: 0.090945, loss: 4.6492
2022-10-10 18:59:58 - train: epoch 0020, iter [02400, 05004], lr: 0.090927, loss: 4.4811
2022-10-10 19:00:35 - train: epoch 0020, iter [02500, 05004], lr: 0.090909, loss: 4.8380
2022-10-10 19:01:11 - train: epoch 0020, iter [02600, 05004], lr: 0.090891, loss: 5.2264
2022-10-10 19:01:48 - train: epoch 0020, iter [02700, 05004], lr: 0.090873, loss: 4.7708
2022-10-10 19:02:25 - train: epoch 0020, iter [02800, 05004], lr: 0.090855, loss: 4.6731
2022-10-10 19:03:01 - train: epoch 0020, iter [02900, 05004], lr: 0.090836, loss: 4.6660
2022-10-10 19:03:38 - train: epoch 0020, iter [03000, 05004], lr: 0.090818, loss: 4.8135
2022-10-10 19:04:14 - train: epoch 0020, iter [03100, 05004], lr: 0.090800, loss: 4.6930
2022-10-10 19:04:52 - train: epoch 0020, iter [03200, 05004], lr: 0.090782, loss: 4.7573
2022-10-10 19:05:29 - train: epoch 0020, iter [03300, 05004], lr: 0.090764, loss: 4.6767
2022-10-10 19:06:05 - train: epoch 0020, iter [03400, 05004], lr: 0.090746, loss: 4.3089
2022-10-10 19:06:42 - train: epoch 0020, iter [03500, 05004], lr: 0.090727, loss: 4.6415
2022-10-10 19:07:18 - train: epoch 0020, iter [03600, 05004], lr: 0.090709, loss: 5.4054
2022-10-10 19:07:55 - train: epoch 0020, iter [03700, 05004], lr: 0.090691, loss: 4.4526
2022-10-10 19:08:32 - train: epoch 0020, iter [03800, 05004], lr: 0.090673, loss: 4.7477
2022-10-10 19:09:08 - train: epoch 0020, iter [03900, 05004], lr: 0.090655, loss: 5.3540
2022-10-10 19:09:45 - train: epoch 0020, iter [04000, 05004], lr: 0.090636, loss: 4.4801
2022-10-10 19:10:22 - train: epoch 0020, iter [04100, 05004], lr: 0.090618, loss: 4.0543
2022-10-10 19:10:58 - train: epoch 0020, iter [04200, 05004], lr: 0.090600, loss: 4.3775
2022-10-10 19:11:35 - train: epoch 0020, iter [04300, 05004], lr: 0.090581, loss: 4.5342
2022-10-10 19:12:12 - train: epoch 0020, iter [04400, 05004], lr: 0.090563, loss: 4.7741
2022-10-10 19:12:49 - train: epoch 0020, iter [04500, 05004], lr: 0.090545, loss: 5.3720
2022-10-10 19:13:26 - train: epoch 0020, iter [04600, 05004], lr: 0.090526, loss: 5.0816
2022-10-10 19:14:03 - train: epoch 0020, iter [04700, 05004], lr: 0.090508, loss: 4.5269
2022-10-10 19:14:41 - train: epoch 0020, iter [04800, 05004], lr: 0.090489, loss: 4.9048
2022-10-10 19:15:18 - train: epoch 0020, iter [04900, 05004], lr: 0.090471, loss: 5.3324
2022-10-10 19:15:52 - train: epoch 0020, iter [05000, 05004], lr: 0.090453, loss: 4.0786
2022-10-10 19:15:54 - train: epoch 020, train_loss: 4.6317
2022-10-10 19:17:13 - eval: epoch: 020, acc1: 53.378%, acc5: 78.854%, test_loss: 2.1882, per_image_load_time: 2.168ms, per_image_inference_time: 0.591ms
2022-10-10 19:17:14 - until epoch: 020, best_acc1: 53.378%
2022-10-10 19:17:14 - epoch 021 lr: 0.090452
2022-10-10 19:17:56 - train: epoch 0021, iter [00100, 05004], lr: 0.090433, loss: 4.8355
2022-10-10 19:18:32 - train: epoch 0021, iter [00200, 05004], lr: 0.090415, loss: 4.5598
2022-10-10 19:19:08 - train: epoch 0021, iter [00300, 05004], lr: 0.090396, loss: 4.6146
2022-10-10 19:19:45 - train: epoch 0021, iter [00400, 05004], lr: 0.090378, loss: 4.4741
2022-10-10 19:20:22 - train: epoch 0021, iter [00500, 05004], lr: 0.090359, loss: 4.3071
2022-10-10 19:20:58 - train: epoch 0021, iter [00600, 05004], lr: 0.090341, loss: 4.0296
2022-10-10 19:21:36 - train: epoch 0021, iter [00700, 05004], lr: 0.090322, loss: 4.6664
2022-10-10 19:22:12 - train: epoch 0021, iter [00800, 05004], lr: 0.090304, loss: 4.6758
2022-10-10 19:22:48 - train: epoch 0021, iter [00900, 05004], lr: 0.090285, loss: 4.5821
2022-10-10 19:23:24 - train: epoch 0021, iter [01000, 05004], lr: 0.090267, loss: 4.5808
2022-10-10 19:24:02 - train: epoch 0021, iter [01100, 05004], lr: 0.090248, loss: 5.0474
2022-10-10 19:24:38 - train: epoch 0021, iter [01200, 05004], lr: 0.090229, loss: 4.0598
2022-10-10 19:25:15 - train: epoch 0021, iter [01300, 05004], lr: 0.090211, loss: 4.2239
2022-10-10 19:25:52 - train: epoch 0021, iter [01400, 05004], lr: 0.090192, loss: 4.3381
2022-10-10 19:26:29 - train: epoch 0021, iter [01500, 05004], lr: 0.090173, loss: 4.0865
2022-10-10 19:27:06 - train: epoch 0021, iter [01600, 05004], lr: 0.090155, loss: 4.8281
2022-10-10 19:27:43 - train: epoch 0021, iter [01700, 05004], lr: 0.090136, loss: 4.4096
2022-10-10 19:28:20 - train: epoch 0021, iter [01800, 05004], lr: 0.090117, loss: 4.8952
2022-10-10 19:28:56 - train: epoch 0021, iter [01900, 05004], lr: 0.090098, loss: 4.5329
2022-10-10 19:29:34 - train: epoch 0021, iter [02000, 05004], lr: 0.090080, loss: 4.7110
2022-10-10 19:30:10 - train: epoch 0021, iter [02100, 05004], lr: 0.090061, loss: 4.5380
2022-10-10 19:30:47 - train: epoch 0021, iter [02200, 05004], lr: 0.090042, loss: 5.0072
2022-10-10 19:31:24 - train: epoch 0021, iter [02300, 05004], lr: 0.090023, loss: 3.9256
2022-10-10 19:32:02 - train: epoch 0021, iter [02400, 05004], lr: 0.090004, loss: 4.5736
2022-10-10 19:32:39 - train: epoch 0021, iter [02500, 05004], lr: 0.089986, loss: 5.1508
2022-10-10 19:33:16 - train: epoch 0021, iter [02600, 05004], lr: 0.089967, loss: 4.9598
2022-10-10 19:33:53 - train: epoch 0021, iter [02700, 05004], lr: 0.089948, loss: 4.5270
2022-10-10 19:34:29 - train: epoch 0021, iter [02800, 05004], lr: 0.089929, loss: 4.4141
2022-10-10 19:35:06 - train: epoch 0021, iter [02900, 05004], lr: 0.089910, loss: 4.3392
2022-10-10 19:35:43 - train: epoch 0021, iter [03000, 05004], lr: 0.089891, loss: 4.1928
2022-10-10 19:36:20 - train: epoch 0021, iter [03100, 05004], lr: 0.089872, loss: 5.2515
2022-10-10 19:36:57 - train: epoch 0021, iter [03200, 05004], lr: 0.089853, loss: 4.6893
2022-10-10 19:37:34 - train: epoch 0021, iter [03300, 05004], lr: 0.089834, loss: 4.9159
2022-10-10 19:38:11 - train: epoch 0021, iter [03400, 05004], lr: 0.089815, loss: 4.0999
2022-10-10 19:38:48 - train: epoch 0021, iter [03500, 05004], lr: 0.089796, loss: 4.4101
2022-10-10 19:39:25 - train: epoch 0021, iter [03600, 05004], lr: 0.089777, loss: 4.5080
2022-10-10 19:40:03 - train: epoch 0021, iter [03700, 05004], lr: 0.089758, loss: 4.8798
2022-10-10 19:40:39 - train: epoch 0021, iter [03800, 05004], lr: 0.089739, loss: 4.6668
2022-10-10 19:41:16 - train: epoch 0021, iter [03900, 05004], lr: 0.089720, loss: 4.9435
2022-10-10 19:41:53 - train: epoch 0021, iter [04000, 05004], lr: 0.089701, loss: 5.1968
2022-10-10 19:42:30 - train: epoch 0021, iter [04100, 05004], lr: 0.089682, loss: 4.0852
2022-10-10 19:43:07 - train: epoch 0021, iter [04200, 05004], lr: 0.089663, loss: 4.2566
2022-10-10 19:43:44 - train: epoch 0021, iter [04300, 05004], lr: 0.089644, loss: 4.4633
2022-10-10 19:44:21 - train: epoch 0021, iter [04400, 05004], lr: 0.089625, loss: 4.6110
2022-10-10 19:44:57 - train: epoch 0021, iter [04500, 05004], lr: 0.089606, loss: 4.3572
2022-10-10 19:45:34 - train: epoch 0021, iter [04600, 05004], lr: 0.089586, loss: 3.9525
2022-10-10 19:46:11 - train: epoch 0021, iter [04700, 05004], lr: 0.089567, loss: 4.8531
2022-10-10 19:46:49 - train: epoch 0021, iter [04800, 05004], lr: 0.089548, loss: 4.6975
2022-10-10 19:47:26 - train: epoch 0021, iter [04900, 05004], lr: 0.089529, loss: 4.8125
2022-10-10 19:48:01 - train: epoch 0021, iter [05000, 05004], lr: 0.089510, loss: 3.8714
2022-10-10 19:48:03 - train: epoch 021, train_loss: 4.6173
2022-10-10 19:49:22 - eval: epoch: 021, acc1: 50.524%, acc5: 76.360%, test_loss: 2.3934, per_image_load_time: 1.738ms, per_image_inference_time: 0.549ms
2022-10-10 19:49:22 - until epoch: 021, best_acc1: 53.378%
2022-10-10 19:49:22 - epoch 022 lr: 0.089509
2022-10-10 19:50:05 - train: epoch 0022, iter [00100, 05004], lr: 0.089490, loss: 4.7014
2022-10-10 19:50:41 - train: epoch 0022, iter [00200, 05004], lr: 0.089470, loss: 4.8903
2022-10-10 19:51:18 - train: epoch 0022, iter [00300, 05004], lr: 0.089451, loss: 4.2618
2022-10-10 19:51:54 - train: epoch 0022, iter [00400, 05004], lr: 0.089432, loss: 4.7228
2022-10-10 19:52:32 - train: epoch 0022, iter [00500, 05004], lr: 0.089412, loss: 4.0300
2022-10-10 19:53:09 - train: epoch 0022, iter [00600, 05004], lr: 0.089393, loss: 4.5748
2022-10-10 19:53:45 - train: epoch 0022, iter [00700, 05004], lr: 0.089374, loss: 4.9929
2022-10-10 19:54:22 - train: epoch 0022, iter [00800, 05004], lr: 0.089354, loss: 4.0293
2022-10-10 19:54:59 - train: epoch 0022, iter [00900, 05004], lr: 0.089335, loss: 4.5723
2022-10-10 19:55:37 - train: epoch 0022, iter [01000, 05004], lr: 0.089316, loss: 3.7094
2022-10-10 19:56:13 - train: epoch 0022, iter [01100, 05004], lr: 0.089296, loss: 4.3409
2022-10-10 19:56:51 - train: epoch 0022, iter [01200, 05004], lr: 0.089277, loss: 4.5563
2022-10-10 19:57:27 - train: epoch 0022, iter [01300, 05004], lr: 0.089257, loss: 4.8961
2022-10-10 19:58:03 - train: epoch 0022, iter [01400, 05004], lr: 0.089238, loss: 4.2756
2022-10-10 19:58:40 - train: epoch 0022, iter [01500, 05004], lr: 0.089218, loss: 5.1293
2022-10-10 19:59:18 - train: epoch 0022, iter [01600, 05004], lr: 0.089199, loss: 4.4497
2022-10-10 19:59:56 - train: epoch 0022, iter [01700, 05004], lr: 0.089180, loss: 4.2790
2022-10-10 20:00:31 - train: epoch 0022, iter [01800, 05004], lr: 0.089160, loss: 4.6273
2022-10-10 20:01:09 - train: epoch 0022, iter [01900, 05004], lr: 0.089140, loss: 4.5735
2022-10-10 20:01:45 - train: epoch 0022, iter [02000, 05004], lr: 0.089121, loss: 3.7301
2022-10-10 20:02:22 - train: epoch 0022, iter [02100, 05004], lr: 0.089101, loss: 4.6884
2022-10-10 20:02:59 - train: epoch 0022, iter [02200, 05004], lr: 0.089082, loss: 4.9391
2022-10-10 20:03:35 - train: epoch 0022, iter [02300, 05004], lr: 0.089062, loss: 4.8501
2022-10-10 20:04:12 - train: epoch 0022, iter [02400, 05004], lr: 0.089043, loss: 4.4343
2022-10-10 20:04:49 - train: epoch 0022, iter [02500, 05004], lr: 0.089023, loss: 4.4553
2022-10-10 20:05:26 - train: epoch 0022, iter [02600, 05004], lr: 0.089003, loss: 4.9024
2022-10-10 20:06:04 - train: epoch 0022, iter [02700, 05004], lr: 0.088984, loss: 4.9378
2022-10-10 20:06:40 - train: epoch 0022, iter [02800, 05004], lr: 0.088964, loss: 4.5154
2022-10-10 20:07:17 - train: epoch 0022, iter [02900, 05004], lr: 0.088944, loss: 5.1481
2022-10-10 20:07:54 - train: epoch 0022, iter [03000, 05004], lr: 0.088925, loss: 4.1135
2022-10-10 20:08:31 - train: epoch 0022, iter [03100, 05004], lr: 0.088905, loss: 4.7963
2022-10-10 20:09:07 - train: epoch 0022, iter [03200, 05004], lr: 0.088885, loss: 4.8242
2022-10-10 20:09:44 - train: epoch 0022, iter [03300, 05004], lr: 0.088866, loss: 4.9182
2022-10-10 20:10:22 - train: epoch 0022, iter [03400, 05004], lr: 0.088846, loss: 4.5521
2022-10-10 20:10:58 - train: epoch 0022, iter [03500, 05004], lr: 0.088826, loss: 5.1772
2022-10-10 20:11:36 - train: epoch 0022, iter [03600, 05004], lr: 0.088806, loss: 3.8097
2022-10-10 20:12:13 - train: epoch 0022, iter [03700, 05004], lr: 0.088786, loss: 4.3544
2022-10-10 20:12:51 - train: epoch 0022, iter [03800, 05004], lr: 0.088767, loss: 5.4243
2022-10-10 20:13:28 - train: epoch 0022, iter [03900, 05004], lr: 0.088747, loss: 5.2290
2022-10-10 20:14:05 - train: epoch 0022, iter [04000, 05004], lr: 0.088727, loss: 4.4517
2022-10-10 20:14:41 - train: epoch 0022, iter [04100, 05004], lr: 0.088707, loss: 4.5181
2022-10-10 20:15:18 - train: epoch 0022, iter [04200, 05004], lr: 0.088687, loss: 4.9231
2022-10-10 20:15:55 - train: epoch 0022, iter [04300, 05004], lr: 0.088667, loss: 4.4470
2022-10-10 20:16:31 - train: epoch 0022, iter [04400, 05004], lr: 0.088647, loss: 4.2223
2022-10-10 20:17:08 - train: epoch 0022, iter [04500, 05004], lr: 0.088627, loss: 3.9810
2022-10-10 20:17:45 - train: epoch 0022, iter [04600, 05004], lr: 0.088608, loss: 5.2858
2022-10-10 20:18:22 - train: epoch 0022, iter [04700, 05004], lr: 0.088588, loss: 4.5145
2022-10-10 20:18:59 - train: epoch 0022, iter [04800, 05004], lr: 0.088568, loss: 4.0953
2022-10-10 20:19:35 - train: epoch 0022, iter [04900, 05004], lr: 0.088548, loss: 4.3606
2022-10-10 20:20:10 - train: epoch 0022, iter [05000, 05004], lr: 0.088528, loss: 4.4708
2022-10-10 20:20:12 - train: epoch 022, train_loss: 4.6034
2022-10-10 20:21:32 - eval: epoch: 022, acc1: 49.776%, acc5: 75.624%, test_loss: 2.4017, per_image_load_time: 2.301ms, per_image_inference_time: 0.528ms
2022-10-10 20:21:32 - until epoch: 022, best_acc1: 53.378%
2022-10-10 20:21:32 - epoch 023 lr: 0.088527
2022-10-10 20:22:16 - train: epoch 0023, iter [00100, 05004], lr: 0.088507, loss: 5.1964
2022-10-10 20:22:52 - train: epoch 0023, iter [00200, 05004], lr: 0.088487, loss: 4.6061
2022-10-10 20:23:28 - train: epoch 0023, iter [00300, 05004], lr: 0.088467, loss: 4.2978
2022-10-10 20:24:04 - train: epoch 0023, iter [00400, 05004], lr: 0.088447, loss: 4.2854
2022-10-10 20:24:41 - train: epoch 0023, iter [00500, 05004], lr: 0.088427, loss: 4.8485
2022-10-10 20:25:18 - train: epoch 0023, iter [00600, 05004], lr: 0.088406, loss: 4.8748
2022-10-10 20:25:53 - train: epoch 0023, iter [00700, 05004], lr: 0.088386, loss: 4.7622
2022-10-10 20:26:30 - train: epoch 0023, iter [00800, 05004], lr: 0.088366, loss: 5.1950
2022-10-10 20:27:06 - train: epoch 0023, iter [00900, 05004], lr: 0.088346, loss: 4.9776
2022-10-10 20:27:43 - train: epoch 0023, iter [01000, 05004], lr: 0.088326, loss: 4.6440
2022-10-10 20:28:21 - train: epoch 0023, iter [01100, 05004], lr: 0.088306, loss: 4.8566
2022-10-10 20:28:58 - train: epoch 0023, iter [01200, 05004], lr: 0.088286, loss: 4.8474
2022-10-10 20:29:36 - train: epoch 0023, iter [01300, 05004], lr: 0.088265, loss: 4.2999
2022-10-10 20:30:13 - train: epoch 0023, iter [01400, 05004], lr: 0.088245, loss: 5.0737
2022-10-10 20:30:50 - train: epoch 0023, iter [01500, 05004], lr: 0.088225, loss: 4.3022
2022-10-10 20:31:27 - train: epoch 0023, iter [01600, 05004], lr: 0.088205, loss: 4.8097
2022-10-10 20:32:04 - train: epoch 0023, iter [01700, 05004], lr: 0.088184, loss: 5.1866
2022-10-10 20:32:41 - train: epoch 0023, iter [01800, 05004], lr: 0.088164, loss: 4.2249
2022-10-10 20:33:18 - train: epoch 0023, iter [01900, 05004], lr: 0.088144, loss: 5.2473
2022-10-10 20:33:55 - train: epoch 0023, iter [02000, 05004], lr: 0.088124, loss: 4.7357
2022-10-10 20:34:33 - train: epoch 0023, iter [02100, 05004], lr: 0.088103, loss: 3.9222
2022-10-10 20:35:10 - train: epoch 0023, iter [02200, 05004], lr: 0.088083, loss: 4.6721
2022-10-10 20:35:47 - train: epoch 0023, iter [02300, 05004], lr: 0.088063, loss: 5.2389
2022-10-10 20:36:25 - train: epoch 0023, iter [02400, 05004], lr: 0.088042, loss: 4.3082
2022-10-10 20:37:02 - train: epoch 0023, iter [02500, 05004], lr: 0.088022, loss: 4.4467
2022-10-10 20:37:39 - train: epoch 0023, iter [02600, 05004], lr: 0.088002, loss: 4.9547
2022-10-10 20:38:16 - train: epoch 0023, iter [02700, 05004], lr: 0.087981, loss: 4.9398
2022-10-10 20:38:53 - train: epoch 0023, iter [02800, 05004], lr: 0.087961, loss: 5.0460
2022-10-10 20:39:29 - train: epoch 0023, iter [02900, 05004], lr: 0.087940, loss: 4.9176
2022-10-10 20:40:07 - train: epoch 0023, iter [03000, 05004], lr: 0.087920, loss: 3.8701
2022-10-10 20:40:43 - train: epoch 0023, iter [03100, 05004], lr: 0.087899, loss: 4.8858
2022-10-10 20:41:20 - train: epoch 0023, iter [03200, 05004], lr: 0.087879, loss: 5.0071
2022-10-10 20:41:56 - train: epoch 0023, iter [03300, 05004], lr: 0.087858, loss: 5.0984
2022-10-10 20:42:33 - train: epoch 0023, iter [03400, 05004], lr: 0.087838, loss: 4.3496
2022-10-10 20:43:10 - train: epoch 0023, iter [03500, 05004], lr: 0.087817, loss: 3.9441
2022-10-10 20:43:47 - train: epoch 0023, iter [03600, 05004], lr: 0.087797, loss: 4.5651
2022-10-10 20:44:24 - train: epoch 0023, iter [03700, 05004], lr: 0.087776, loss: 4.7731
2022-10-10 20:45:01 - train: epoch 0023, iter [03800, 05004], lr: 0.087756, loss: 4.4365
2022-10-10 20:45:37 - train: epoch 0023, iter [03900, 05004], lr: 0.087735, loss: 4.8407
2022-10-10 20:46:14 - train: epoch 0023, iter [04000, 05004], lr: 0.087714, loss: 4.4379
2022-10-10 20:46:51 - train: epoch 0023, iter [04100, 05004], lr: 0.087694, loss: 4.9570
2022-10-10 20:47:27 - train: epoch 0023, iter [04200, 05004], lr: 0.087673, loss: 5.0324
2022-10-10 20:48:04 - train: epoch 0023, iter [04300, 05004], lr: 0.087653, loss: 4.1581
2022-10-10 20:48:41 - train: epoch 0023, iter [04400, 05004], lr: 0.087632, loss: 4.6226
2022-10-10 20:49:17 - train: epoch 0023, iter [04500, 05004], lr: 0.087611, loss: 4.1941
2022-10-10 20:49:54 - train: epoch 0023, iter [04600, 05004], lr: 0.087591, loss: 3.8030
2022-10-10 20:50:31 - train: epoch 0023, iter [04700, 05004], lr: 0.087570, loss: 4.7613
2022-10-10 20:51:08 - train: epoch 0023, iter [04800, 05004], lr: 0.087549, loss: 4.8219
2022-10-10 20:51:44 - train: epoch 0023, iter [04900, 05004], lr: 0.087528, loss: 5.1450
2022-10-10 20:52:19 - train: epoch 0023, iter [05000, 05004], lr: 0.087508, loss: 4.3779
2022-10-10 20:52:22 - train: epoch 023, train_loss: 4.5855
2022-10-10 20:53:41 - eval: epoch: 023, acc1: 52.768%, acc5: 78.208%, test_loss: 2.2587, per_image_load_time: 2.017ms, per_image_inference_time: 0.530ms
2022-10-10 20:53:41 - until epoch: 023, best_acc1: 53.378%
2022-10-10 20:53:41 - epoch 024 lr: 0.087507
2022-10-10 20:54:25 - train: epoch 0024, iter [00100, 05004], lr: 0.087486, loss: 4.7556
2022-10-10 20:55:02 - train: epoch 0024, iter [00200, 05004], lr: 0.087465, loss: 4.3018
2022-10-10 20:55:39 - train: epoch 0024, iter [00300, 05004], lr: 0.087444, loss: 4.4014
2022-10-10 20:56:14 - train: epoch 0024, iter [00400, 05004], lr: 0.087424, loss: 4.2426
2022-10-10 20:56:51 - train: epoch 0024, iter [00500, 05004], lr: 0.087403, loss: 3.4425
2022-10-10 20:57:27 - train: epoch 0024, iter [00600, 05004], lr: 0.087382, loss: 4.2228
2022-10-10 20:58:04 - train: epoch 0024, iter [00700, 05004], lr: 0.087361, loss: 4.7507
2022-10-10 20:58:41 - train: epoch 0024, iter [00800, 05004], lr: 0.087340, loss: 4.5473
2022-10-10 20:59:18 - train: epoch 0024, iter [00900, 05004], lr: 0.087319, loss: 4.8472
2022-10-10 20:59:54 - train: epoch 0024, iter [01000, 05004], lr: 0.087298, loss: 4.7638
2022-10-10 21:00:32 - train: epoch 0024, iter [01100, 05004], lr: 0.087278, loss: 3.9239
2022-10-10 21:01:09 - train: epoch 0024, iter [01200, 05004], lr: 0.087257, loss: 4.2183
2022-10-10 21:01:46 - train: epoch 0024, iter [01300, 05004], lr: 0.087236, loss: 4.7288
2022-10-10 21:02:23 - train: epoch 0024, iter [01400, 05004], lr: 0.087215, loss: 4.1566
2022-10-10 21:03:00 - train: epoch 0024, iter [01500, 05004], lr: 0.087194, loss: 4.4959
2022-10-10 21:03:37 - train: epoch 0024, iter [01600, 05004], lr: 0.087173, loss: 3.4483
2022-10-10 21:04:15 - train: epoch 0024, iter [01700, 05004], lr: 0.087152, loss: 4.6558
2022-10-10 21:04:52 - train: epoch 0024, iter [01800, 05004], lr: 0.087131, loss: 4.6008
2022-10-10 21:05:28 - train: epoch 0024, iter [01900, 05004], lr: 0.087110, loss: 4.1290
2022-10-10 21:06:05 - train: epoch 0024, iter [02000, 05004], lr: 0.087089, loss: 4.5576
2022-10-10 21:06:42 - train: epoch 0024, iter [02100, 05004], lr: 0.087068, loss: 4.7131
2022-10-10 21:07:19 - train: epoch 0024, iter [02200, 05004], lr: 0.087047, loss: 4.3956
2022-10-10 21:07:56 - train: epoch 0024, iter [02300, 05004], lr: 0.087025, loss: 4.7271
2022-10-10 21:08:32 - train: epoch 0024, iter [02400, 05004], lr: 0.087004, loss: 5.0013
2022-10-10 21:09:09 - train: epoch 0024, iter [02500, 05004], lr: 0.086983, loss: 4.9524
2022-10-10 21:09:46 - train: epoch 0024, iter [02600, 05004], lr: 0.086962, loss: 4.9120
2022-10-10 21:10:23 - train: epoch 0024, iter [02700, 05004], lr: 0.086941, loss: 3.9799
2022-10-10 21:11:00 - train: epoch 0024, iter [02800, 05004], lr: 0.086920, loss: 4.4486
2022-10-10 21:11:37 - train: epoch 0024, iter [02900, 05004], lr: 0.086899, loss: 5.1365
2022-10-10 21:12:14 - train: epoch 0024, iter [03000, 05004], lr: 0.086877, loss: 4.8594
2022-10-10 21:12:50 - train: epoch 0024, iter [03100, 05004], lr: 0.086856, loss: 4.9758
2022-10-10 21:13:27 - train: epoch 0024, iter [03200, 05004], lr: 0.086835, loss: 5.1052
2022-10-10 21:14:03 - train: epoch 0024, iter [03300, 05004], lr: 0.086814, loss: 4.6671
2022-10-10 21:14:40 - train: epoch 0024, iter [03400, 05004], lr: 0.086793, loss: 5.0351
2022-10-10 21:15:17 - train: epoch 0024, iter [03500, 05004], lr: 0.086771, loss: 5.0741
2022-10-10 21:15:53 - train: epoch 0024, iter [03600, 05004], lr: 0.086750, loss: 4.6139
2022-10-10 21:16:29 - train: epoch 0024, iter [03700, 05004], lr: 0.086729, loss: 4.4243
2022-10-10 21:17:06 - train: epoch 0024, iter [03800, 05004], lr: 0.086707, loss: 5.3862
2022-10-10 21:17:43 - train: epoch 0024, iter [03900, 05004], lr: 0.086686, loss: 4.4253
2022-10-10 21:18:20 - train: epoch 0024, iter [04000, 05004], lr: 0.086665, loss: 5.0513
2022-10-10 21:18:57 - train: epoch 0024, iter [04100, 05004], lr: 0.086643, loss: 4.8082
2022-10-10 21:19:34 - train: epoch 0024, iter [04200, 05004], lr: 0.086622, loss: 3.8255
2022-10-10 21:20:11 - train: epoch 0024, iter [04300, 05004], lr: 0.086601, loss: 4.8491
2022-10-10 21:20:48 - train: epoch 0024, iter [04400, 05004], lr: 0.086579, loss: 4.9037
2022-10-10 21:21:25 - train: epoch 0024, iter [04500, 05004], lr: 0.086558, loss: 4.7391
2022-10-10 21:22:02 - train: epoch 0024, iter [04600, 05004], lr: 0.086536, loss: 5.3393
2022-10-10 21:22:38 - train: epoch 0024, iter [04700, 05004], lr: 0.086515, loss: 4.4130
2022-10-10 21:23:15 - train: epoch 0024, iter [04800, 05004], lr: 0.086494, loss: 4.1631
2022-10-10 21:23:52 - train: epoch 0024, iter [04900, 05004], lr: 0.086472, loss: 4.9290
2022-10-10 21:24:27 - train: epoch 0024, iter [05000, 05004], lr: 0.086451, loss: 4.6185
2022-10-10 21:24:29 - train: epoch 024, train_loss: 4.5822
2022-10-10 21:25:47 - eval: epoch: 024, acc1: 52.362%, acc5: 77.854%, test_loss: 2.3391, per_image_load_time: 2.157ms, per_image_inference_time: 0.546ms
2022-10-10 21:25:48 - until epoch: 024, best_acc1: 53.378%
2022-10-10 21:25:48 - epoch 025 lr: 0.086450
2022-10-10 21:26:31 - train: epoch 0025, iter [00100, 05004], lr: 0.086428, loss: 4.5193
2022-10-10 21:27:08 - train: epoch 0025, iter [00200, 05004], lr: 0.086407, loss: 4.0519
2022-10-10 21:27:44 - train: epoch 0025, iter [00300, 05004], lr: 0.086385, loss: 5.3183
2022-10-10 21:28:20 - train: epoch 0025, iter [00400, 05004], lr: 0.086364, loss: 5.2386
2022-10-10 21:28:57 - train: epoch 0025, iter [00500, 05004], lr: 0.086342, loss: 4.1715
2022-10-10 21:29:33 - train: epoch 0025, iter [00600, 05004], lr: 0.086321, loss: 5.2314
2022-10-10 21:30:09 - train: epoch 0025, iter [00700, 05004], lr: 0.086299, loss: 4.7913
2022-10-10 21:30:45 - train: epoch 0025, iter [00800, 05004], lr: 0.086277, loss: 3.9932
2022-10-10 21:31:22 - train: epoch 0025, iter [00900, 05004], lr: 0.086256, loss: 4.0964
2022-10-10 21:31:59 - train: epoch 0025, iter [01000, 05004], lr: 0.086234, loss: 4.5332
2022-10-10 21:32:35 - train: epoch 0025, iter [01100, 05004], lr: 0.086213, loss: 4.6290
2022-10-10 21:33:12 - train: epoch 0025, iter [01200, 05004], lr: 0.086191, loss: 4.4038
2022-10-10 21:33:50 - train: epoch 0025, iter [01300, 05004], lr: 0.086169, loss: 4.8939
2022-10-10 21:34:26 - train: epoch 0025, iter [01400, 05004], lr: 0.086148, loss: 3.8537
2022-10-10 21:35:03 - train: epoch 0025, iter [01500, 05004], lr: 0.086126, loss: 4.4379
2022-10-10 21:35:40 - train: epoch 0025, iter [01600, 05004], lr: 0.086104, loss: 4.1663
2022-10-10 21:36:17 - train: epoch 0025, iter [01700, 05004], lr: 0.086082, loss: 5.0337
2022-10-10 21:36:54 - train: epoch 0025, iter [01800, 05004], lr: 0.086061, loss: 4.9399
2022-10-10 21:37:31 - train: epoch 0025, iter [01900, 05004], lr: 0.086039, loss: 4.9257
2022-10-10 21:38:08 - train: epoch 0025, iter [02000, 05004], lr: 0.086017, loss: 5.2128
2022-10-10 21:38:46 - train: epoch 0025, iter [02100, 05004], lr: 0.085995, loss: 4.1411
2022-10-10 21:39:22 - train: epoch 0025, iter [02200, 05004], lr: 0.085974, loss: 4.5473
2022-10-10 21:40:00 - train: epoch 0025, iter [02300, 05004], lr: 0.085952, loss: 3.9751
2022-10-10 21:40:37 - train: epoch 0025, iter [02400, 05004], lr: 0.085930, loss: 4.5143
2022-10-10 21:41:13 - train: epoch 0025, iter [02500, 05004], lr: 0.085908, loss: 5.0003
2022-10-10 21:41:50 - train: epoch 0025, iter [02600, 05004], lr: 0.085886, loss: 4.5292
2022-10-10 21:42:26 - train: epoch 0025, iter [02700, 05004], lr: 0.085864, loss: 4.7249
2022-10-10 21:43:03 - train: epoch 0025, iter [02800, 05004], lr: 0.085843, loss: 4.3586
2022-10-10 21:43:40 - train: epoch 0025, iter [02900, 05004], lr: 0.085821, loss: 4.8412
2022-10-10 21:44:17 - train: epoch 0025, iter [03000, 05004], lr: 0.085799, loss: 4.1586
2022-10-10 21:44:55 - train: epoch 0025, iter [03100, 05004], lr: 0.085777, loss: 5.1856
2022-10-10 21:45:31 - train: epoch 0025, iter [03200, 05004], lr: 0.085755, loss: 4.1680
2022-10-10 21:46:08 - train: epoch 0025, iter [03300, 05004], lr: 0.085733, loss: 4.4016
2022-10-10 21:46:45 - train: epoch 0025, iter [03400, 05004], lr: 0.085711, loss: 4.5504
2022-10-10 21:47:21 - train: epoch 0025, iter [03500, 05004], lr: 0.085689, loss: 4.7274
2022-10-10 21:47:58 - train: epoch 0025, iter [03600, 05004], lr: 0.085667, loss: 4.4030
2022-10-10 21:48:35 - train: epoch 0025, iter [03700, 05004], lr: 0.085645, loss: 4.9778
2022-10-10 21:49:11 - train: epoch 0025, iter [03800, 05004], lr: 0.085623, loss: 4.3821
2022-10-10 21:49:48 - train: epoch 0025, iter [03900, 05004], lr: 0.085601, loss: 5.1397
2022-10-10 21:50:24 - train: epoch 0025, iter [04000, 05004], lr: 0.085579, loss: 4.3294
2022-10-10 21:51:02 - train: epoch 0025, iter [04100, 05004], lr: 0.085557, loss: 4.7172
2022-10-10 21:51:39 - train: epoch 0025, iter [04200, 05004], lr: 0.085535, loss: 4.9105
2022-10-10 21:52:16 - train: epoch 0025, iter [04300, 05004], lr: 0.085513, loss: 4.3419
2022-10-10 21:52:53 - train: epoch 0025, iter [04400, 05004], lr: 0.085491, loss: 5.0880
2022-10-10 21:53:29 - train: epoch 0025, iter [04500, 05004], lr: 0.085468, loss: 4.4380
2022-10-10 21:54:06 - train: epoch 0025, iter [04600, 05004], lr: 0.085446, loss: 4.7652
2022-10-10 21:54:42 - train: epoch 0025, iter [04700, 05004], lr: 0.085424, loss: 4.9274
2022-10-10 21:55:18 - train: epoch 0025, iter [04800, 05004], lr: 0.085402, loss: 3.9392
2022-10-10 21:55:56 - train: epoch 0025, iter [04900, 05004], lr: 0.085380, loss: 4.3464
2022-10-10 21:56:30 - train: epoch 0025, iter [05000, 05004], lr: 0.085358, loss: 5.1281
2022-10-10 21:56:33 - train: epoch 025, train_loss: 4.5739
2022-10-10 21:57:53 - eval: epoch: 025, acc1: 52.706%, acc5: 78.340%, test_loss: 2.2876, per_image_load_time: 2.238ms, per_image_inference_time: 0.539ms
2022-10-10 21:57:53 - until epoch: 025, best_acc1: 53.378%
2022-10-10 21:57:53 - epoch 026 lr: 0.085357
2022-10-10 21:58:36 - train: epoch 0026, iter [00100, 05004], lr: 0.085335, loss: 5.1922
2022-10-10 21:59:13 - train: epoch 0026, iter [00200, 05004], lr: 0.085312, loss: 5.1214
2022-10-10 21:59:49 - train: epoch 0026, iter [00300, 05004], lr: 0.085290, loss: 5.2423
2022-10-10 22:00:25 - train: epoch 0026, iter [00400, 05004], lr: 0.085268, loss: 3.5535
2022-10-10 22:01:00 - train: epoch 0026, iter [00500, 05004], lr: 0.085246, loss: 4.7041
2022-10-10 22:01:37 - train: epoch 0026, iter [00600, 05004], lr: 0.085223, loss: 5.1980
2022-10-10 22:02:14 - train: epoch 0026, iter [00700, 05004], lr: 0.085201, loss: 3.8696
2022-10-10 22:02:51 - train: epoch 0026, iter [00800, 05004], lr: 0.085179, loss: 4.7182
2022-10-10 22:03:27 - train: epoch 0026, iter [00900, 05004], lr: 0.085156, loss: 4.6404
2022-10-10 22:04:05 - train: epoch 0026, iter [01000, 05004], lr: 0.085134, loss: 4.1696
2022-10-10 22:04:42 - train: epoch 0026, iter [01100, 05004], lr: 0.085112, loss: 5.3015
2022-10-10 22:05:19 - train: epoch 0026, iter [01200, 05004], lr: 0.085089, loss: 4.7651
2022-10-10 22:05:56 - train: epoch 0026, iter [01300, 05004], lr: 0.085067, loss: 4.5023
2022-10-10 22:06:32 - train: epoch 0026, iter [01400, 05004], lr: 0.085045, loss: 4.5378
2022-10-10 22:07:08 - train: epoch 0026, iter [01500, 05004], lr: 0.085022, loss: 4.2753
2022-10-10 22:07:45 - train: epoch 0026, iter [01600, 05004], lr: 0.085000, loss: 4.8017
2022-10-10 22:08:21 - train: epoch 0026, iter [01700, 05004], lr: 0.084977, loss: 4.3684
2022-10-10 22:08:58 - train: epoch 0026, iter [01800, 05004], lr: 0.084955, loss: 5.2485
2022-10-10 22:09:34 - train: epoch 0026, iter [01900, 05004], lr: 0.084933, loss: 4.1856
2022-10-10 22:10:11 - train: epoch 0026, iter [02000, 05004], lr: 0.084910, loss: 4.0957
2022-10-10 22:10:49 - train: epoch 0026, iter [02100, 05004], lr: 0.084888, loss: 4.2863
2022-10-10 22:11:26 - train: epoch 0026, iter [02200, 05004], lr: 0.084865, loss: 4.9449
2022-10-10 22:12:03 - train: epoch 0026, iter [02300, 05004], lr: 0.084843, loss: 4.6823
2022-10-10 22:12:40 - train: epoch 0026, iter [02400, 05004], lr: 0.084820, loss: 4.2748
2022-10-10 22:13:17 - train: epoch 0026, iter [02500, 05004], lr: 0.084798, loss: 4.2772
2022-10-10 22:13:54 - train: epoch 0026, iter [02600, 05004], lr: 0.084775, loss: 3.9317
2022-10-10 22:14:31 - train: epoch 0026, iter [02700, 05004], lr: 0.084753, loss: 4.6209
2022-10-10 22:15:08 - train: epoch 0026, iter [02800, 05004], lr: 0.084730, loss: 5.0014
2022-10-10 22:15:44 - train: epoch 0026, iter [02900, 05004], lr: 0.084707, loss: 4.1920
2022-10-10 22:16:21 - train: epoch 0026, iter [03000, 05004], lr: 0.084685, loss: 3.6754
2022-10-10 22:16:59 - train: epoch 0026, iter [03100, 05004], lr: 0.084662, loss: 5.2302
2022-10-10 22:17:35 - train: epoch 0026, iter [03200, 05004], lr: 0.084639, loss: 4.8170
2022-10-10 22:18:12 - train: epoch 0026, iter [03300, 05004], lr: 0.084617, loss: 4.6701
2022-10-10 22:18:49 - train: epoch 0026, iter [03400, 05004], lr: 0.084594, loss: 4.9620
2022-10-10 22:19:26 - train: epoch 0026, iter [03500, 05004], lr: 0.084572, loss: 4.4641
2022-10-10 22:20:04 - train: epoch 0026, iter [03600, 05004], lr: 0.084549, loss: 4.8114
2022-10-10 22:20:41 - train: epoch 0026, iter [03700, 05004], lr: 0.084526, loss: 4.0089
2022-10-10 22:21:18 - train: epoch 0026, iter [03800, 05004], lr: 0.084503, loss: 4.7576
2022-10-10 22:21:55 - train: epoch 0026, iter [03900, 05004], lr: 0.084481, loss: 4.7198
2022-10-10 22:22:33 - train: epoch 0026, iter [04000, 05004], lr: 0.084458, loss: 4.7956
2022-10-10 22:23:10 - train: epoch 0026, iter [04100, 05004], lr: 0.084435, loss: 4.9902
2022-10-10 22:23:46 - train: epoch 0026, iter [04200, 05004], lr: 0.084412, loss: 5.4600
2022-10-10 22:24:23 - train: epoch 0026, iter [04300, 05004], lr: 0.084390, loss: 4.3820
2022-10-10 22:25:00 - train: epoch 0026, iter [04400, 05004], lr: 0.084367, loss: 4.4802
2022-10-10 22:25:38 - train: epoch 0026, iter [04500, 05004], lr: 0.084344, loss: 4.7887
2022-10-10 22:26:14 - train: epoch 0026, iter [04600, 05004], lr: 0.084321, loss: 4.2197
2022-10-10 22:26:52 - train: epoch 0026, iter [04700, 05004], lr: 0.084298, loss: 4.0089
2022-10-10 22:27:29 - train: epoch 0026, iter [04800, 05004], lr: 0.084276, loss: 4.5345
2022-10-10 22:28:06 - train: epoch 0026, iter [04900, 05004], lr: 0.084253, loss: 4.7188
2022-10-10 22:28:40 - train: epoch 0026, iter [05000, 05004], lr: 0.084230, loss: 4.6987
2022-10-10 22:28:42 - train: epoch 026, train_loss: 4.5579
2022-10-10 22:30:03 - eval: epoch: 026, acc1: 53.448%, acc5: 78.754%, test_loss: 2.2973, per_image_load_time: 2.309ms, per_image_inference_time: 0.517ms
2022-10-10 22:30:03 - until epoch: 026, best_acc1: 53.448%
2022-10-10 22:30:03 - epoch 027 lr: 0.084229
2022-10-10 22:30:47 - train: epoch 0027, iter [00100, 05004], lr: 0.084206, loss: 4.5059
2022-10-10 22:31:22 - train: epoch 0027, iter [00200, 05004], lr: 0.084183, loss: 4.5664
2022-10-10 22:31:59 - train: epoch 0027, iter [00300, 05004], lr: 0.084160, loss: 3.8260
2022-10-10 22:32:36 - train: epoch 0027, iter [00400, 05004], lr: 0.084137, loss: 4.2235
2022-10-10 22:33:12 - train: epoch 0027, iter [00500, 05004], lr: 0.084114, loss: 5.0123
2022-10-10 22:33:49 - train: epoch 0027, iter [00600, 05004], lr: 0.084091, loss: 5.1756
2022-10-10 22:34:26 - train: epoch 0027, iter [00700, 05004], lr: 0.084068, loss: 4.5041
2022-10-10 22:35:04 - train: epoch 0027, iter [00800, 05004], lr: 0.084045, loss: 5.1512
2022-10-10 22:35:40 - train: epoch 0027, iter [00900, 05004], lr: 0.084022, loss: 4.7274
2022-10-10 22:36:16 - train: epoch 0027, iter [01000, 05004], lr: 0.083999, loss: 4.5559
2022-10-10 22:36:53 - train: epoch 0027, iter [01100, 05004], lr: 0.083976, loss: 4.7371
2022-10-10 22:37:29 - train: epoch 0027, iter [01200, 05004], lr: 0.083953, loss: 4.7492
2022-10-10 22:38:05 - train: epoch 0027, iter [01300, 05004], lr: 0.083930, loss: 4.7313
2022-10-10 22:38:43 - train: epoch 0027, iter [01400, 05004], lr: 0.083907, loss: 4.1781
2022-10-10 22:39:19 - train: epoch 0027, iter [01500, 05004], lr: 0.083884, loss: 3.9130
2022-10-10 22:39:56 - train: epoch 0027, iter [01600, 05004], lr: 0.083861, loss: 4.4748
2022-10-10 22:40:32 - train: epoch 0027, iter [01700, 05004], lr: 0.083838, loss: 5.1943
2022-10-10 22:41:09 - train: epoch 0027, iter [01800, 05004], lr: 0.083815, loss: 4.5784
2022-10-10 22:41:46 - train: epoch 0027, iter [01900, 05004], lr: 0.083792, loss: 3.9558
2022-10-10 22:42:23 - train: epoch 0027, iter [02000, 05004], lr: 0.083769, loss: 3.9226
2022-10-10 22:43:00 - train: epoch 0027, iter [02100, 05004], lr: 0.083745, loss: 4.9440
2022-10-10 22:43:37 - train: epoch 0027, iter [02200, 05004], lr: 0.083722, loss: 4.5037
2022-10-10 22:44:14 - train: epoch 0027, iter [02300, 05004], lr: 0.083699, loss: 4.8101
2022-10-10 22:44:51 - train: epoch 0027, iter [02400, 05004], lr: 0.083676, loss: 4.5997
2022-10-10 22:45:27 - train: epoch 0027, iter [02500, 05004], lr: 0.083653, loss: 4.7631
2022-10-10 22:46:04 - train: epoch 0027, iter [02600, 05004], lr: 0.083630, loss: 5.2794
2022-10-10 22:46:41 - train: epoch 0027, iter [02700, 05004], lr: 0.083606, loss: 4.3919
2022-10-10 22:47:18 - train: epoch 0027, iter [02800, 05004], lr: 0.083583, loss: 3.7579
2022-10-10 22:47:55 - train: epoch 0027, iter [02900, 05004], lr: 0.083560, loss: 4.3181
2022-10-10 22:48:32 - train: epoch 0027, iter [03000, 05004], lr: 0.083536, loss: 3.7565
2022-10-10 22:49:08 - train: epoch 0027, iter [03100, 05004], lr: 0.083513, loss: 4.4565
2022-10-10 22:49:46 - train: epoch 0027, iter [03200, 05004], lr: 0.083490, loss: 4.3588
2022-10-10 22:50:22 - train: epoch 0027, iter [03300, 05004], lr: 0.083467, loss: 4.0437
2022-10-10 22:50:59 - train: epoch 0027, iter [03400, 05004], lr: 0.083443, loss: 5.2978
2022-10-10 22:51:36 - train: epoch 0027, iter [03500, 05004], lr: 0.083420, loss: 4.9777
2022-10-10 22:52:13 - train: epoch 0027, iter [03600, 05004], lr: 0.083397, loss: 5.1084
2022-10-10 22:52:50 - train: epoch 0027, iter [03700, 05004], lr: 0.083373, loss: 4.7263
2022-10-10 22:53:28 - train: epoch 0027, iter [03800, 05004], lr: 0.083350, loss: 4.5225
2022-10-10 22:54:05 - train: epoch 0027, iter [03900, 05004], lr: 0.083326, loss: 4.4197
2022-10-10 22:54:42 - train: epoch 0027, iter [04000, 05004], lr: 0.083303, loss: 4.9960
2022-10-10 22:55:19 - train: epoch 0027, iter [04100, 05004], lr: 0.083280, loss: 4.3152
2022-10-10 22:55:56 - train: epoch 0027, iter [04200, 05004], lr: 0.083256, loss: 4.6536
2022-10-10 22:56:33 - train: epoch 0027, iter [04300, 05004], lr: 0.083233, loss: 4.7055
2022-10-10 22:57:10 - train: epoch 0027, iter [04400, 05004], lr: 0.083209, loss: 4.3677
2022-10-10 22:57:47 - train: epoch 0027, iter [04500, 05004], lr: 0.083186, loss: 4.1993
2022-10-10 22:58:24 - train: epoch 0027, iter [04600, 05004], lr: 0.083162, loss: 4.2139
2022-10-10 22:59:01 - train: epoch 0027, iter [04700, 05004], lr: 0.083139, loss: 5.0124
2022-10-10 22:59:38 - train: epoch 0027, iter [04800, 05004], lr: 0.083115, loss: 4.1650
2022-10-10 23:00:15 - train: epoch 0027, iter [04900, 05004], lr: 0.083092, loss: 5.0246
2022-10-10 23:00:49 - train: epoch 0027, iter [05000, 05004], lr: 0.083068, loss: 3.9553
2022-10-10 23:00:51 - train: epoch 027, train_loss: 4.5406
2022-10-10 23:02:10 - eval: epoch: 027, acc1: 53.676%, acc5: 78.774%, test_loss: 2.2887, per_image_load_time: 2.464ms, per_image_inference_time: 0.557ms
2022-10-10 23:02:11 - until epoch: 027, best_acc1: 53.676%
2022-10-10 23:02:11 - epoch 028 lr: 0.083067
2022-10-10 23:02:53 - train: epoch 0028, iter [00100, 05004], lr: 0.083044, loss: 4.6800
2022-10-10 23:03:30 - train: epoch 0028, iter [00200, 05004], lr: 0.083020, loss: 4.8025
2022-10-10 23:04:05 - train: epoch 0028, iter [00300, 05004], lr: 0.082997, loss: 4.9400
2022-10-10 23:04:41 - train: epoch 0028, iter [00400, 05004], lr: 0.082973, loss: 4.9950
2022-10-10 23:05:18 - train: epoch 0028, iter [00500, 05004], lr: 0.082949, loss: 4.5261
2022-10-10 23:05:55 - train: epoch 0028, iter [00600, 05004], lr: 0.082926, loss: 4.4476
2022-10-10 23:06:32 - train: epoch 0028, iter [00700, 05004], lr: 0.082902, loss: 4.0318
2022-10-10 23:07:09 - train: epoch 0028, iter [00800, 05004], lr: 0.082879, loss: 4.7531
2022-10-10 23:07:46 - train: epoch 0028, iter [00900, 05004], lr: 0.082855, loss: 4.1886
2022-10-10 23:08:23 - train: epoch 0028, iter [01000, 05004], lr: 0.082831, loss: 3.8330
2022-10-10 23:09:00 - train: epoch 0028, iter [01100, 05004], lr: 0.082808, loss: 4.6028
2022-10-10 23:09:38 - train: epoch 0028, iter [01200, 05004], lr: 0.082784, loss: 4.7485
2022-10-10 23:10:15 - train: epoch 0028, iter [01300, 05004], lr: 0.082760, loss: 4.8743
2022-10-10 23:10:53 - train: epoch 0028, iter [01400, 05004], lr: 0.082736, loss: 4.5295
2022-10-10 23:11:29 - train: epoch 0028, iter [01500, 05004], lr: 0.082713, loss: 3.8028
2022-10-10 23:12:06 - train: epoch 0028, iter [01600, 05004], lr: 0.082689, loss: 3.8381
2022-10-10 23:12:43 - train: epoch 0028, iter [01700, 05004], lr: 0.082665, loss: 4.7098
2022-10-10 23:13:19 - train: epoch 0028, iter [01800, 05004], lr: 0.082641, loss: 4.5131
2022-10-10 23:13:58 - train: epoch 0028, iter [01900, 05004], lr: 0.082618, loss: 4.2660
2022-10-10 23:14:34 - train: epoch 0028, iter [02000, 05004], lr: 0.082594, loss: 4.5252
2022-10-10 23:15:11 - train: epoch 0028, iter [02100, 05004], lr: 0.082570, loss: 5.4726
2022-10-10 23:15:48 - train: epoch 0028, iter [02200, 05004], lr: 0.082546, loss: 4.7946
2022-10-10 23:16:24 - train: epoch 0028, iter [02300, 05004], lr: 0.082522, loss: 4.2843
2022-10-10 23:17:01 - train: epoch 0028, iter [02400, 05004], lr: 0.082498, loss: 4.4055
2022-10-10 23:17:38 - train: epoch 0028, iter [02500, 05004], lr: 0.082475, loss: 4.4634
2022-10-10 23:18:14 - train: epoch 0028, iter [02600, 05004], lr: 0.082451, loss: 4.2606
2022-10-10 23:18:51 - train: epoch 0028, iter [02700, 05004], lr: 0.082427, loss: 4.6648
2022-10-10 23:19:28 - train: epoch 0028, iter [02800, 05004], lr: 0.082403, loss: 5.0555
2022-10-10 23:20:04 - train: epoch 0028, iter [02900, 05004], lr: 0.082379, loss: 4.1251
2022-10-10 23:20:42 - train: epoch 0028, iter [03000, 05004], lr: 0.082355, loss: 4.9669
2022-10-10 23:21:18 - train: epoch 0028, iter [03100, 05004], lr: 0.082331, loss: 4.3608
2022-10-10 23:21:54 - train: epoch 0028, iter [03200, 05004], lr: 0.082307, loss: 4.9425
2022-10-10 23:22:31 - train: epoch 0028, iter [03300, 05004], lr: 0.082283, loss: 3.8294
2022-10-10 23:23:06 - train: epoch 0028, iter [03400, 05004], lr: 0.082259, loss: 4.5081
2022-10-10 23:23:43 - train: epoch 0028, iter [03500, 05004], lr: 0.082235, loss: 3.5125
2022-10-10 23:24:21 - train: epoch 0028, iter [03600, 05004], lr: 0.082211, loss: 5.0110
2022-10-10 23:24:57 - train: epoch 0028, iter [03700, 05004], lr: 0.082187, loss: 3.9354
2022-10-10 23:25:34 - train: epoch 0028, iter [03800, 05004], lr: 0.082163, loss: 4.9919
2022-10-10 23:26:11 - train: epoch 0028, iter [03900, 05004], lr: 0.082139, loss: 3.6452
2022-10-10 23:26:48 - train: epoch 0028, iter [04000, 05004], lr: 0.082115, loss: 4.8866
2022-10-10 23:27:25 - train: epoch 0028, iter [04100, 05004], lr: 0.082091, loss: 4.6793
2022-10-10 23:28:02 - train: epoch 0028, iter [04200, 05004], lr: 0.082067, loss: 4.7942
2022-10-10 23:28:38 - train: epoch 0028, iter [04300, 05004], lr: 0.082043, loss: 4.4471
2022-10-10 23:29:15 - train: epoch 0028, iter [04400, 05004], lr: 0.082019, loss: 4.6895
2022-10-10 23:29:51 - train: epoch 0028, iter [04500, 05004], lr: 0.081995, loss: 4.3195
2022-10-10 23:30:29 - train: epoch 0028, iter [04600, 05004], lr: 0.081971, loss: 4.6857
2022-10-10 23:31:06 - train: epoch 0028, iter [04700, 05004], lr: 0.081946, loss: 4.1772
2022-10-10 23:31:42 - train: epoch 0028, iter [04800, 05004], lr: 0.081922, loss: 4.9281
2022-10-10 23:32:20 - train: epoch 0028, iter [04900, 05004], lr: 0.081898, loss: 3.9578
2022-10-10 23:32:54 - train: epoch 0028, iter [05000, 05004], lr: 0.081874, loss: 4.7329
2022-10-10 23:32:56 - train: epoch 028, train_loss: 4.5335
2022-10-10 23:34:15 - eval: epoch: 028, acc1: 55.340%, acc5: 80.260%, test_loss: 2.1410, per_image_load_time: 2.326ms, per_image_inference_time: 0.530ms
2022-10-10 23:34:16 - until epoch: 028, best_acc1: 55.340%
2022-10-10 23:34:16 - epoch 029 lr: 0.081873
2022-10-10 23:34:58 - train: epoch 0029, iter [00100, 05004], lr: 0.081849, loss: 4.5648
2022-10-10 23:35:34 - train: epoch 0029, iter [00200, 05004], lr: 0.081825, loss: 4.7535
2022-10-10 23:36:11 - train: epoch 0029, iter [00300, 05004], lr: 0.081800, loss: 4.3738
2022-10-10 23:36:47 - train: epoch 0029, iter [00400, 05004], lr: 0.081776, loss: 3.8109
2022-10-10 23:37:23 - train: epoch 0029, iter [00500, 05004], lr: 0.081752, loss: 4.1100
2022-10-10 23:38:00 - train: epoch 0029, iter [00600, 05004], lr: 0.081728, loss: 4.0269
2022-10-10 23:38:37 - train: epoch 0029, iter [00700, 05004], lr: 0.081703, loss: 3.5179
2022-10-10 23:39:14 - train: epoch 0029, iter [00800, 05004], lr: 0.081679, loss: 4.2102
2022-10-10 23:39:50 - train: epoch 0029, iter [00900, 05004], lr: 0.081655, loss: 4.6446
2022-10-10 23:40:28 - train: epoch 0029, iter [01000, 05004], lr: 0.081631, loss: 4.8049
2022-10-10 23:41:04 - train: epoch 0029, iter [01100, 05004], lr: 0.081606, loss: 4.6067
2022-10-10 23:41:41 - train: epoch 0029, iter [01200, 05004], lr: 0.081582, loss: 4.7667
2022-10-10 23:42:17 - train: epoch 0029, iter [01300, 05004], lr: 0.081558, loss: 4.3043
2022-10-10 23:42:54 - train: epoch 0029, iter [01400, 05004], lr: 0.081533, loss: 4.0346
2022-10-10 23:43:31 - train: epoch 0029, iter [01500, 05004], lr: 0.081509, loss: 4.4604
2022-10-10 23:44:08 - train: epoch 0029, iter [01600, 05004], lr: 0.081484, loss: 3.8806
2022-10-10 23:44:44 - train: epoch 0029, iter [01700, 05004], lr: 0.081460, loss: 4.3888
2022-10-10 23:45:20 - train: epoch 0029, iter [01800, 05004], lr: 0.081436, loss: 4.7495
2022-10-10 23:45:57 - train: epoch 0029, iter [01900, 05004], lr: 0.081411, loss: 4.3056
2022-10-10 23:46:34 - train: epoch 0029, iter [02000, 05004], lr: 0.081387, loss: 3.7626
2022-10-10 23:47:11 - train: epoch 0029, iter [02100, 05004], lr: 0.081362, loss: 4.4964
2022-10-10 23:47:48 - train: epoch 0029, iter [02200, 05004], lr: 0.081338, loss: 5.1676
2022-10-10 23:48:25 - train: epoch 0029, iter [02300, 05004], lr: 0.081313, loss: 4.9335
2022-10-10 23:49:03 - train: epoch 0029, iter [02400, 05004], lr: 0.081289, loss: 4.9044
2022-10-10 23:49:39 - train: epoch 0029, iter [02500, 05004], lr: 0.081264, loss: 4.7400
2022-10-10 23:50:17 - train: epoch 0029, iter [02600, 05004], lr: 0.081240, loss: 4.5402
2022-10-10 23:50:53 - train: epoch 0029, iter [02700, 05004], lr: 0.081215, loss: 4.1162
2022-10-10 23:51:30 - train: epoch 0029, iter [02800, 05004], lr: 0.081191, loss: 4.5445
2022-10-10 23:52:07 - train: epoch 0029, iter [02900, 05004], lr: 0.081166, loss: 5.3223
2022-10-10 23:52:44 - train: epoch 0029, iter [03000, 05004], lr: 0.081142, loss: 3.8164
2022-10-10 23:53:21 - train: epoch 0029, iter [03100, 05004], lr: 0.081117, loss: 5.0460
2022-10-10 23:53:58 - train: epoch 0029, iter [03200, 05004], lr: 0.081093, loss: 5.2802
2022-10-10 23:54:35 - train: epoch 0029, iter [03300, 05004], lr: 0.081068, loss: 4.5799
2022-10-10 23:55:11 - train: epoch 0029, iter [03400, 05004], lr: 0.081044, loss: 3.7540
2022-10-10 23:55:48 - train: epoch 0029, iter [03500, 05004], lr: 0.081019, loss: 5.0474
2022-10-10 23:56:26 - train: epoch 0029, iter [03600, 05004], lr: 0.080994, loss: 4.7786
2022-10-10 23:57:02 - train: epoch 0029, iter [03700, 05004], lr: 0.080970, loss: 4.6245
2022-10-10 23:57:38 - train: epoch 0029, iter [03800, 05004], lr: 0.080945, loss: 4.4886
2022-10-10 23:58:16 - train: epoch 0029, iter [03900, 05004], lr: 0.080920, loss: 4.5542
2022-10-10 23:58:53 - train: epoch 0029, iter [04000, 05004], lr: 0.080896, loss: 4.7888
2022-10-10 23:59:29 - train: epoch 0029, iter [04100, 05004], lr: 0.080871, loss: 4.3849
