2022-03-14 23:24:27 - network: yoloxmbackbone
2022-03-14 23:24:27 - num_classes: 1000
2022-03-14 23:24:27 - input_image_size: 256
2022-03-14 23:24:27 - scale: 1.1428571428571428
2022-03-14 23:24:27 - trained_model_path: 
2022-03-14 23:24:27 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-14 23:24:27 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4ff288d4f0>
2022-03-14 23:24:27 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4ff288d7c0>
2022-03-14 23:24:27 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4ff288d7f0>
2022-03-14 23:24:27 - seed: 0
2022-03-14 23:24:27 - batch_size: 256
2022-03-14 23:24:27 - num_workers: 16
2022-03-14 23:24:27 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-14 23:24:27 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-03-14 23:24:27 - epochs: 100
2022-03-14 23:24:27 - print_interval: 100
2022-03-14 23:24:27 - distributed: True
2022-03-14 23:24:27 - sync_bn: False
2022-03-14 23:24:27 - apex: True
2022-03-14 23:24:27 - gpus_type: NVIDIA RTX A5000
2022-03-14 23:24:27 - gpus_num: 2
2022-03-14 23:24:27 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f4fd8973db0>
2022-03-14 23:24:27 - --------------------parameters--------------------
2022-03-14 23:24:27 - name: conv.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: conv.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: conv.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer1.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer1.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer1.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.4.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.4.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.4.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.4.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.4.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.4.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.5.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.5.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.5.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.5.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.5.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.5.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.4.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.4.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.4.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.4.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.4.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.4.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.5.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.5.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.5.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.5.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.5.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.5.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer4.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer4.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer4.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-14 23:24:27 - name: fc.weight, grad: True
2022-03-14 23:24:27 - name: fc.bias, grad: True
2022-03-14 23:24:27 - --------------------buffers--------------------
2022-03-14 23:24:27 - name: conv.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: conv.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: conv.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer1.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer1.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer1.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer1.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.4.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.4.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.4.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.4.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.5.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.5.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.5.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.5.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer2.1.bottlenecks.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.4.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.4.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.4.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.4.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.5.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.5.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.5.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.5.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer3.1.bottlenecks.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer4.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer4.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer4.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-14 23:24:27 - name: layer4.2.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-14 23:24:27 - epoch 001 lr: 0.1
2022-03-14 23:25:05 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9004
2022-03-14 23:25:38 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.8791
2022-03-14 23:26:11 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.7878
2022-03-14 23:26:44 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.7924
2022-03-14 23:27:18 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.6962
2022-03-14 23:27:50 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.5476
2022-03-14 23:28:23 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.6500
2022-03-14 23:28:56 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.4330
2022-03-14 23:29:29 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.2291
2022-03-14 23:30:01 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.1869
2022-03-14 23:30:34 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.0456
2022-03-14 23:31:07 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.7927
2022-03-14 23:31:40 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.7914
2022-03-14 23:32:12 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.6914
2022-03-14 23:32:45 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.5706
2022-03-14 23:33:18 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.5914
2022-03-14 23:33:52 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.3567
2022-03-14 23:34:24 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.4055
2022-03-14 23:34:57 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.3387
2022-03-14 23:35:30 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.2392
2022-03-14 23:36:03 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.2345
2022-03-14 23:36:36 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.1067
2022-03-14 23:37:09 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.9117
2022-03-14 23:37:42 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.9095
2022-03-14 23:38:15 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.0718
2022-03-14 23:38:47 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.0617
2022-03-14 23:39:20 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.0929
2022-03-14 23:39:53 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.8400
2022-03-14 23:40:26 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.6708
2022-03-14 23:40:59 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.7809
2022-03-14 23:41:32 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.8275
2022-03-14 23:42:05 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.7708
2022-03-14 23:42:38 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.6116
2022-03-14 23:43:11 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.5154
2022-03-14 23:43:45 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.6328
2022-03-14 23:44:18 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.6842
2022-03-14 23:44:51 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.6561
2022-03-14 23:45:24 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.5888
2022-03-14 23:45:57 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.5755
2022-03-14 23:46:30 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.4800
2022-03-14 23:47:04 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.5808
2022-03-14 23:47:36 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.3704
2022-03-14 23:48:10 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.3266
2022-03-14 23:48:43 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.0823
2022-03-14 23:49:16 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.4714
2022-03-14 23:49:49 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.5326
2022-03-14 23:50:22 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.2739
2022-03-14 23:50:55 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.4295
2022-03-14 23:51:29 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.2452
2022-03-14 23:51:59 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 3.9742
2022-03-14 23:52:00 - train: epoch 001, train_loss: 5.2232
2022-03-14 23:53:15 - eval: epoch: 001, acc1: 18.966%, acc5: 41.210%, test_loss: 4.0282, per_image_load_time: 1.034ms, per_image_inference_time: 0.354ms
2022-03-14 23:53:15 - until epoch: 001, best_acc1: 18.966%
2022-03-14 23:53:15 - epoch 002 lr: 0.1
2022-03-14 23:53:53 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.2957
2022-03-14 23:54:26 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 3.9585
2022-03-14 23:54:59 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.3031
2022-03-14 23:55:32 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.1507
2022-03-14 23:56:05 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.0160
2022-03-14 23:56:37 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 3.9690
2022-03-14 23:57:11 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.2486
2022-03-14 23:57:43 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.8473
2022-03-14 23:58:18 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.6558
2022-03-14 23:58:50 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.3351
2022-03-14 23:59:23 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.1635
2022-03-14 23:59:56 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.0704
2022-03-15 00:00:30 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.8821
2022-03-15 00:01:03 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.0592
2022-03-15 00:01:36 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.1090
2022-03-15 00:02:08 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.8300
2022-03-15 00:02:42 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.9560
2022-03-15 00:03:14 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.9578
2022-03-15 00:03:48 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.8368
2022-03-15 00:04:21 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.6644
2022-03-15 00:04:54 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.8286
2022-03-15 00:05:27 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.5667
2022-03-15 00:06:00 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.9546
2022-03-15 00:06:33 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.6371
2022-03-15 00:07:06 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.7393
2022-03-15 00:07:39 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.6820
2022-03-15 00:08:12 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 4.0133
2022-03-15 00:08:45 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.8755
2022-03-15 00:09:18 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.6499
2022-03-15 00:09:50 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.6569
2022-03-15 00:10:24 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.6115
2022-03-15 00:10:57 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.7505
2022-03-15 00:11:29 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.6960
2022-03-15 00:12:03 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.8638
2022-03-15 00:12:36 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.5968
2022-03-15 00:13:09 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.6328
2022-03-15 00:13:42 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.8280
2022-03-15 00:14:15 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.5584
2022-03-15 00:14:48 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.7605
2022-03-15 00:15:21 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.6839
2022-03-15 00:15:54 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.8049
2022-03-15 00:16:28 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.5274
2022-03-15 00:17:00 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.6132
2022-03-15 00:17:34 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.4667
2022-03-15 00:18:06 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.5570
2022-03-15 00:18:40 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.4600
2022-03-15 00:19:12 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.5902
2022-03-15 00:19:44 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.6082
2022-03-15 00:20:18 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.5341
2022-03-15 00:20:49 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.5214
2022-03-15 00:20:50 - train: epoch 002, train_loss: 3.8221
2022-03-15 00:22:04 - eval: epoch: 002, acc1: 28.594%, acc5: 54.858%, test_loss: 3.3421, per_image_load_time: 0.892ms, per_image_inference_time: 0.326ms
2022-03-15 00:22:05 - until epoch: 002, best_acc1: 28.594%
2022-03-15 00:22:05 - epoch 003 lr: 0.1
2022-03-15 00:22:43 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.5215
2022-03-15 00:23:17 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.5140
2022-03-15 00:23:49 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.4765
2022-03-15 00:24:22 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.5291
2022-03-15 00:24:55 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.7331
2022-03-15 00:25:28 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.4295
2022-03-15 00:26:01 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.7854
2022-03-15 00:26:34 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.5942
2022-03-15 00:27:07 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.4991
2022-03-15 00:27:41 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.4645
2022-03-15 00:28:13 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.3737
2022-03-15 00:28:46 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.4505
2022-03-15 00:29:20 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.4492
2022-03-15 00:29:52 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.4297
2022-03-15 00:30:25 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.5507
2022-03-15 00:30:59 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.3672
2022-03-15 00:31:31 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.2989
2022-03-15 00:32:04 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.2496
2022-03-15 00:32:38 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.4946
2022-03-15 00:33:10 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.6804
2022-03-15 00:33:43 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.5619
2022-03-15 00:34:16 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.8032
2022-03-15 00:34:50 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.4178
2022-03-15 00:35:22 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.3420
2022-03-15 00:35:56 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.3353
2022-03-15 00:36:28 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.2988
2022-03-15 00:37:02 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.5707
2022-03-15 00:37:35 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.3150
2022-03-15 00:38:08 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.3154
2022-03-15 00:38:41 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.5513
2022-03-15 00:39:15 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.5091
2022-03-15 00:39:47 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.4450
2022-03-15 00:40:21 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.3465
2022-03-15 00:40:53 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.6610
2022-03-15 00:41:27 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.1718
2022-03-15 00:41:59 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.2763
2022-03-15 00:42:33 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.2790
2022-03-15 00:43:06 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.3704
2022-03-15 00:43:39 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.6805
2022-03-15 00:44:12 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.2934
2022-03-15 00:44:45 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.2736
2022-03-15 00:45:17 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.2556
2022-03-15 00:45:51 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.9757
2022-03-15 00:46:24 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.0173
2022-03-15 00:46:57 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.3157
2022-03-15 00:47:30 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.1680
2022-03-15 00:48:03 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.2094
2022-03-15 00:48:36 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.3049
2022-03-15 00:49:09 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.4165
2022-03-15 00:49:41 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.2921
2022-03-15 00:49:42 - train: epoch 003, train_loss: 3.3918
2022-03-15 00:50:56 - eval: epoch: 003, acc1: 34.190%, acc5: 60.864%, test_loss: 3.0138, per_image_load_time: 1.471ms, per_image_inference_time: 0.361ms
2022-03-15 00:50:57 - until epoch: 003, best_acc1: 34.190%
2022-03-15 00:50:57 - epoch 004 lr: 0.1
2022-03-15 00:51:36 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.2952
2022-03-15 00:52:09 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.1853
2022-03-15 00:52:42 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.2748
2022-03-15 00:53:15 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 3.2157
2022-03-15 00:53:48 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.1801
2022-03-15 00:54:22 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.4590
2022-03-15 00:54:55 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.2325
2022-03-15 00:55:29 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 3.0483
2022-03-15 00:56:02 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.9325
2022-03-15 00:56:36 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.3001
2022-03-15 00:57:09 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.3280
2022-03-15 00:57:43 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 3.0794
2022-03-15 00:58:16 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 3.1124
2022-03-15 00:58:49 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.1568
2022-03-15 00:59:23 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.2817
2022-03-15 00:59:57 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.9999
2022-03-15 01:00:30 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.2389
2022-03-15 01:01:04 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.3056
2022-03-15 01:01:36 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.3474
2022-03-15 01:02:11 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.1083
2022-03-15 01:02:43 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.2619
2022-03-15 01:03:17 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.1486
2022-03-15 01:03:50 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.9633
2022-03-15 01:04:24 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.9817
2022-03-15 01:04:57 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.9769
2022-03-15 01:05:31 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.1859
2022-03-15 01:06:04 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 3.0215
2022-03-15 01:06:38 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.1432
2022-03-15 01:07:11 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 3.1063
2022-03-15 01:07:45 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 3.1078
2022-03-15 01:08:19 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.0632
2022-03-15 01:08:53 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 3.1497
2022-03-15 01:09:25 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.1592
2022-03-15 01:09:58 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 3.1238
2022-03-15 01:10:32 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 3.0224
2022-03-15 01:11:06 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.9294
2022-03-15 01:11:40 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.0316
2022-03-15 01:12:14 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.9878
2022-03-15 01:12:47 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 3.0078
2022-03-15 01:13:20 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.8252
2022-03-15 01:13:53 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 3.0632
2022-03-15 01:14:26 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.9340
2022-03-15 01:15:00 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.9713
2022-03-15 01:15:33 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.8259
2022-03-15 01:16:08 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.6053
2022-03-15 01:16:40 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 3.0026
2022-03-15 01:17:14 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.9011
2022-03-15 01:17:47 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.9278
2022-03-15 01:18:21 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.9991
2022-03-15 01:18:53 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 3.0891
2022-03-15 01:18:54 - train: epoch 004, train_loss: 3.1377
2022-03-15 01:20:08 - eval: epoch: 004, acc1: 38.410%, acc5: 65.118%, test_loss: 2.7785, per_image_load_time: 2.055ms, per_image_inference_time: 0.367ms
2022-03-15 01:20:08 - until epoch: 004, best_acc1: 38.410%
2022-03-15 01:20:08 - epoch 005 lr: 0.1
2022-03-15 01:20:46 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.1382
2022-03-15 01:21:20 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.0248
2022-03-15 01:21:53 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.1739
2022-03-15 01:22:26 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.0382
2022-03-15 01:22:59 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.8755
2022-03-15 01:23:32 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 3.0017
2022-03-15 01:24:05 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 3.0962
2022-03-15 01:24:40 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.3599
2022-03-15 01:25:12 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.9947
2022-03-15 01:25:45 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.0873
2022-03-15 01:26:19 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.0468
2022-03-15 01:26:52 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.9470
2022-03-15 01:27:25 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 3.0430
2022-03-15 01:27:59 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.1342
2022-03-15 01:28:32 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.8199
2022-03-15 01:29:05 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.8608
2022-03-15 01:29:39 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.8614
2022-03-15 01:30:11 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 3.0353
2022-03-15 01:30:44 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.7729
2022-03-15 01:31:18 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.9526
2022-03-15 01:31:52 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.8689
2022-03-15 01:32:25 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.8838
2022-03-15 01:32:59 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.7625
2022-03-15 01:33:33 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.8797
2022-03-15 01:34:05 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 3.0338
2022-03-15 01:34:39 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.2358
2022-03-15 01:35:12 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.0398
2022-03-15 01:35:46 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.9535
2022-03-15 01:36:20 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.8283
2022-03-15 01:36:53 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.9918
2022-03-15 01:37:26 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.0612
2022-03-15 01:38:00 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.9990
2022-03-15 01:38:34 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.8504
2022-03-15 01:39:08 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.8019
2022-03-15 01:39:41 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.9321
2022-03-15 01:40:14 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.9377
2022-03-15 01:40:47 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.9014
2022-03-15 01:41:22 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.7738
2022-03-15 01:41:55 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.1869
2022-03-15 01:42:30 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.8933
2022-03-15 01:43:03 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.8447
2022-03-15 01:43:36 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.9451
2022-03-15 01:44:10 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.8285
2022-03-15 01:44:43 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 3.0269
2022-03-15 01:45:17 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 3.0646
2022-03-15 01:45:50 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.8477
2022-03-15 01:46:25 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.7087
2022-03-15 01:46:57 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.8604
2022-03-15 01:47:31 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.9033
2022-03-15 01:48:03 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.8225
2022-03-15 01:48:04 - train: epoch 005, train_loss: 2.9614
2022-03-15 01:49:18 - eval: epoch: 005, acc1: 41.678%, acc5: 68.224%, test_loss: 2.5823, per_image_load_time: 1.506ms, per_image_inference_time: 0.364ms
2022-03-15 01:49:18 - until epoch: 005, best_acc1: 41.678%
2022-03-15 01:49:18 - epoch 006 lr: 0.1
2022-03-15 01:49:57 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.7632
2022-03-15 01:50:29 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.9206
2022-03-15 01:51:03 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.7184
2022-03-15 01:51:37 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.9891
2022-03-15 01:52:10 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.9182
2022-03-15 01:52:43 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.8681
2022-03-15 01:53:16 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.9106
2022-03-15 01:53:50 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.8954
2022-03-15 01:54:23 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.7826
2022-03-15 01:54:56 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.8027
2022-03-15 01:55:30 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.7735
2022-03-15 01:56:03 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.9997
2022-03-15 01:56:37 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.9882
2022-03-15 01:57:09 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.9751
2022-03-15 01:57:43 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.8906
2022-03-15 01:58:16 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.7157
2022-03-15 01:58:50 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.9668
2022-03-15 01:59:24 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 3.0584
2022-03-15 01:59:58 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.8084
2022-03-15 02:00:30 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 3.0066
2022-03-15 02:01:04 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.8903
2022-03-15 02:01:38 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.7702
2022-03-15 02:02:11 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.6544
2022-03-15 02:02:44 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.8505
2022-03-15 02:03:18 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 3.0424
2022-03-15 02:03:52 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.7448
2022-03-15 02:04:26 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.8882
2022-03-15 02:04:59 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.7272
2022-03-15 02:05:32 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.9003
2022-03-15 02:06:06 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.7607
2022-03-15 02:06:39 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.6649
2022-03-15 02:07:12 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.7458
2022-03-15 02:07:45 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.6725
2022-03-15 02:08:19 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.9466
2022-03-15 02:08:53 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.8382
2022-03-15 02:09:26 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.8727
2022-03-15 02:10:00 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.8544
2022-03-15 02:10:33 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.6731
2022-03-15 02:11:07 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.7126
2022-03-15 02:11:39 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.0365
2022-03-15 02:12:13 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.7066
2022-03-15 02:12:46 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.6691
2022-03-15 02:13:20 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.7110
2022-03-15 02:13:53 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.8406
2022-03-15 02:14:26 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.8207
2022-03-15 02:14:59 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.7757
2022-03-15 02:15:33 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.9075
2022-03-15 02:16:07 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.8276
2022-03-15 02:16:40 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.9756
2022-03-15 02:17:12 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.6036
2022-03-15 02:17:13 - train: epoch 006, train_loss: 2.8452
2022-03-15 02:18:26 - eval: epoch: 006, acc1: 42.406%, acc5: 69.238%, test_loss: 2.5533, per_image_load_time: 1.829ms, per_image_inference_time: 0.367ms
2022-03-15 02:18:27 - until epoch: 006, best_acc1: 42.406%
2022-03-15 02:18:27 - epoch 007 lr: 0.1
2022-03-15 02:19:05 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.6271
2022-03-15 02:19:38 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 3.0150
2022-03-15 02:20:12 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 3.0727
2022-03-15 02:20:45 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.7882
2022-03-15 02:21:18 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.7182
2022-03-15 02:21:51 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 3.0329
2022-03-15 02:22:25 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.8190
2022-03-15 02:22:58 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.7954
2022-03-15 02:23:31 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.8997
2022-03-15 02:24:05 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.7356
2022-03-15 02:24:37 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.7037
2022-03-15 02:25:10 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.7125
2022-03-15 02:25:43 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.7729
2022-03-15 02:26:16 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.8727
2022-03-15 02:26:50 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.8011
2022-03-15 02:27:24 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.7069
2022-03-15 02:27:57 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.7531
2022-03-15 02:28:30 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.7301
2022-03-15 02:29:02 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.7559
2022-03-15 02:29:37 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.5658
2022-03-15 02:30:10 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.8888
2022-03-15 02:30:42 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.5537
2022-03-15 02:31:16 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.7675
2022-03-15 02:31:49 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.8047
2022-03-15 02:32:23 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.6431
2022-03-15 02:32:55 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.6885
2022-03-15 02:33:29 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.6693
2022-03-15 02:34:02 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.8367
2022-03-15 02:34:36 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.5871
2022-03-15 02:35:09 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.9330
2022-03-15 02:35:43 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.5390
2022-03-15 02:36:16 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.6982
2022-03-15 02:36:50 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 3.1284
2022-03-15 02:37:23 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.5884
2022-03-15 02:37:56 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.8341
2022-03-15 02:38:29 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.5125
2022-03-15 02:39:03 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.8472
2022-03-15 02:39:36 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.8808
2022-03-15 02:40:10 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.7002
2022-03-15 02:40:43 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.8673
2022-03-15 02:41:17 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.6301
2022-03-15 02:41:51 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.5601
2022-03-15 02:42:23 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.8335
2022-03-15 02:42:57 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.6380
2022-03-15 02:43:30 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.8482
2022-03-15 02:44:03 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.6723
2022-03-15 02:44:37 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.8578
2022-03-15 02:45:10 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.8979
2022-03-15 02:45:45 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.7442
2022-03-15 02:46:16 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.7996
2022-03-15 02:46:17 - train: epoch 007, train_loss: 2.7611
2022-03-15 02:47:31 - eval: epoch: 007, acc1: 44.504%, acc5: 71.490%, test_loss: 2.4147, per_image_load_time: 1.677ms, per_image_inference_time: 0.359ms
2022-03-15 02:47:32 - until epoch: 007, best_acc1: 44.504%
2022-03-15 02:47:32 - epoch 008 lr: 0.1
2022-03-15 02:48:10 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.7022
2022-03-15 02:48:44 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.8291
2022-03-15 02:49:16 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.5048
2022-03-15 02:49:50 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.5548
2022-03-15 02:50:24 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.5136
2022-03-15 02:50:58 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.6507
2022-03-15 02:51:31 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.9392
2022-03-15 02:52:04 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.5753
2022-03-15 02:52:38 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.6694
2022-03-15 02:53:11 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.7380
2022-03-15 02:53:45 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.5528
2022-03-15 02:54:17 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.5195
2022-03-15 02:54:51 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.8003
2022-03-15 02:55:24 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.5529
2022-03-15 02:55:58 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.7765
2022-03-15 02:56:32 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.7869
2022-03-15 02:57:05 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.4994
2022-03-15 02:57:39 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.8485
2022-03-15 02:58:12 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.4202
2022-03-15 02:58:46 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.8283
2022-03-15 02:59:19 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.7615
2022-03-15 02:59:53 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.4973
2022-03-15 03:00:26 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.7501
2022-03-15 03:01:00 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.6214
2022-03-15 03:01:33 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.6075
2022-03-15 03:02:06 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.8575
2022-03-15 03:02:39 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.8363
2022-03-15 03:03:13 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.8709
2022-03-15 03:03:46 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.7564
2022-03-15 03:04:19 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.7821
2022-03-15 03:04:52 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.6883
2022-03-15 03:05:26 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.9019
2022-03-15 03:06:00 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.8855
2022-03-15 03:06:33 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.8273
2022-03-15 03:07:06 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.7903
2022-03-15 03:07:40 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.9029
2022-03-15 03:08:13 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.6501
2022-03-15 03:08:47 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.5889
2022-03-15 03:09:20 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.7191
2022-03-15 03:09:54 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 3.0356
2022-03-15 03:10:28 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.6167
2022-03-15 03:11:01 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.5883
2022-03-15 03:11:34 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.4446
2022-03-15 03:12:07 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.6305
2022-03-15 03:12:41 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.8605
2022-03-15 03:13:14 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.8510
2022-03-15 03:13:48 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.5619
2022-03-15 03:14:21 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.6852
2022-03-15 03:14:54 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.7319
2022-03-15 03:15:26 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.6262
2022-03-15 03:15:27 - train: epoch 008, train_loss: 2.6964
2022-03-15 03:16:42 - eval: epoch: 008, acc1: 45.392%, acc5: 72.010%, test_loss: 2.3774, per_image_load_time: 2.269ms, per_image_inference_time: 0.363ms
2022-03-15 03:16:42 - until epoch: 008, best_acc1: 45.392%
2022-03-15 03:16:42 - epoch 009 lr: 0.1
2022-03-15 03:17:21 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.3723
2022-03-15 03:17:55 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.5212
2022-03-15 03:18:28 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.3462
2022-03-15 03:19:01 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.8066
2022-03-15 03:19:35 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.7019
2022-03-15 03:20:08 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.6944
2022-03-15 03:20:42 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.6657
2022-03-15 03:21:15 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.4096
2022-03-15 03:21:48 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.5871
2022-03-15 03:22:21 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.5556
2022-03-15 03:22:55 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 3.0187
2022-03-15 03:23:29 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.8459
2022-03-15 03:24:02 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.8369
2022-03-15 03:24:35 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.4141
2022-03-15 03:25:08 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.6371
2022-03-15 03:25:42 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.7964
2022-03-15 03:26:17 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.7546
2022-03-15 03:26:49 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.5345
2022-03-15 03:27:23 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.4114
2022-03-15 03:27:56 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.3826
2022-03-15 03:28:30 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.7179
2022-03-15 03:29:03 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.7838
2022-03-15 03:29:37 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.4097
2022-03-15 03:30:10 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.6346
2022-03-15 03:30:43 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.3505
2022-03-15 03:31:17 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.7081
2022-03-15 03:31:50 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.5588
2022-03-15 03:32:24 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.6930
2022-03-15 03:32:58 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.3905
2022-03-15 03:33:31 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.5913
2022-03-15 03:34:05 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.6015
2022-03-15 03:34:38 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.6553
2022-03-15 03:35:11 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.6619
2022-03-15 03:35:44 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.8152
2022-03-15 03:36:18 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.7885
2022-03-15 03:36:52 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.5710
2022-03-15 03:37:26 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.8512
2022-03-15 03:37:59 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.7751
2022-03-15 03:38:32 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.4212
2022-03-15 03:39:06 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.8484
2022-03-15 03:39:39 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.6713
2022-03-15 03:40:14 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.4427
2022-03-15 03:40:47 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.7441
2022-03-15 03:41:20 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.6005
2022-03-15 03:41:54 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.6513
2022-03-15 03:42:28 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.6728
2022-03-15 03:43:01 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.7269
2022-03-15 03:43:36 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.8599
2022-03-15 03:44:09 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.6404
2022-03-15 03:44:41 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.4940
2022-03-15 03:44:42 - train: epoch 009, train_loss: 2.6489
2022-03-15 03:45:57 - eval: epoch: 009, acc1: 45.870%, acc5: 72.758%, test_loss: 2.3545, per_image_load_time: 1.981ms, per_image_inference_time: 0.371ms
2022-03-15 03:45:57 - until epoch: 009, best_acc1: 45.870%
2022-03-15 03:45:57 - epoch 010 lr: 0.1
2022-03-15 03:46:36 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.5656
2022-03-15 03:47:09 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.6757
2022-03-15 03:47:43 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.7306
2022-03-15 03:48:17 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.5606
2022-03-15 03:48:49 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.5534
2022-03-15 03:49:23 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.7064
2022-03-15 03:49:56 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.6922
2022-03-15 03:50:29 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.5258
2022-03-15 03:51:03 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.3883
2022-03-15 03:51:35 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.4104
2022-03-15 03:52:09 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.7678
2022-03-15 03:52:42 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.4694
2022-03-15 03:53:16 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.4393
2022-03-15 03:53:49 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.7322
2022-03-15 03:54:22 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.2770
2022-03-15 03:54:56 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.6278
2022-03-15 03:55:29 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.7403
2022-03-15 03:56:02 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.6627
2022-03-15 03:56:35 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.6324
2022-03-15 03:57:08 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.5613
2022-03-15 03:57:42 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.5101
2022-03-15 03:58:15 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.8377
2022-03-15 03:58:48 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.8526
2022-03-15 03:59:22 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.7716
2022-03-15 03:59:56 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.7459
2022-03-15 04:00:30 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.7013
2022-03-15 04:01:03 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.4052
2022-03-15 04:01:37 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.6745
2022-03-15 04:02:10 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.7448
2022-03-15 04:02:44 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.5698
2022-03-15 04:03:17 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.7727
2022-03-15 04:03:51 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.5465
2022-03-15 04:04:24 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.6542
2022-03-15 04:04:57 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.8009
2022-03-15 04:05:31 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.8349
2022-03-15 04:06:04 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.9018
2022-03-15 04:06:38 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.5447
2022-03-15 04:07:11 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.7397
2022-03-15 04:07:44 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.2785
2022-03-15 04:08:17 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.5405
2022-03-15 04:08:51 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.4938
2022-03-15 04:09:24 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.7096
2022-03-15 04:09:56 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.6331
2022-03-15 04:10:30 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.6409
2022-03-15 04:11:04 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.4149
2022-03-15 04:11:38 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.6523
2022-03-15 04:12:11 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.5661
2022-03-15 04:12:44 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.5341
2022-03-15 04:13:17 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.3113
2022-03-15 04:13:49 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.4068
2022-03-15 04:13:50 - train: epoch 010, train_loss: 2.6093
2022-03-15 04:15:05 - eval: epoch: 010, acc1: 46.856%, acc5: 73.288%, test_loss: 2.3007, per_image_load_time: 2.201ms, per_image_inference_time: 0.370ms
2022-03-15 04:15:05 - until epoch: 010, best_acc1: 46.856%
2022-03-15 04:15:05 - epoch 011 lr: 0.1
2022-03-15 04:15:43 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.3960
2022-03-15 04:16:16 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.6819
2022-03-15 04:16:49 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.3739
2022-03-15 04:17:23 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.5448
2022-03-15 04:17:57 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.4756
2022-03-15 04:18:30 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.4970
2022-03-15 04:19:03 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.6280
2022-03-15 04:19:36 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.5762
2022-03-15 04:20:10 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.6889
2022-03-15 04:20:43 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.4771
2022-03-15 04:21:17 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.6189
2022-03-15 04:21:51 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.9298
2022-03-15 04:22:23 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.6522
2022-03-15 04:22:56 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.6658
2022-03-15 04:23:30 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.3375
2022-03-15 04:24:03 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.5942
2022-03-15 04:24:37 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.5679
2022-03-15 04:25:11 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.4401
2022-03-15 04:25:45 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.4383
2022-03-15 04:26:18 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.7577
2022-03-15 04:26:51 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.6598
2022-03-15 04:27:25 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.4972
2022-03-15 04:27:58 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.8590
2022-03-15 04:28:32 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.3912
2022-03-15 04:29:06 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.8717
2022-03-15 04:29:38 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.5858
2022-03-15 04:30:13 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.5725
2022-03-15 04:30:46 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.2286
2022-03-15 04:31:19 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.6504
2022-03-15 04:31:53 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.8485
2022-03-15 04:32:26 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.6883
2022-03-15 04:33:01 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.4119
2022-03-15 04:33:34 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.6751
2022-03-15 04:34:06 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.5162
2022-03-15 04:34:41 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.6206
2022-03-15 04:35:14 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.5951
2022-03-15 04:35:47 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.7000
2022-03-15 04:36:20 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.3008
2022-03-15 04:36:54 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.6399
2022-03-15 04:37:27 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.5960
2022-03-15 04:38:01 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.4683
2022-03-15 04:38:34 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.4804
2022-03-15 04:39:07 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.6442
2022-03-15 04:39:40 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.5639
2022-03-15 04:40:14 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.4703
2022-03-15 04:40:48 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.4713
2022-03-15 04:41:21 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.3051
2022-03-15 04:41:55 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.4152
2022-03-15 04:42:28 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.4132
2022-03-15 04:43:00 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.4734
2022-03-15 04:43:01 - train: epoch 011, train_loss: 2.5792
2022-03-15 04:44:15 - eval: epoch: 011, acc1: 47.702%, acc5: 73.952%, test_loss: 2.2629, per_image_load_time: 1.216ms, per_image_inference_time: 0.389ms
2022-03-15 04:44:15 - until epoch: 011, best_acc1: 47.702%
2022-03-15 04:44:15 - epoch 012 lr: 0.1
2022-03-15 04:44:54 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.3689
2022-03-15 04:45:27 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.4341
2022-03-15 04:46:01 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.5375
2022-03-15 04:46:34 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.6071
2022-03-15 04:47:08 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.8471
2022-03-15 04:47:41 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.2234
2022-03-15 04:48:14 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.4459
2022-03-15 04:48:48 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.6047
2022-03-15 04:49:21 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.6615
2022-03-15 04:49:54 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.4203
2022-03-15 04:50:27 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.8061
2022-03-15 04:51:01 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.5048
2022-03-15 04:51:34 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.5525
2022-03-15 04:52:08 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.6660
2022-03-15 04:52:41 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.4078
2022-03-15 04:53:15 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.5189
2022-03-15 04:53:48 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.4886
2022-03-15 04:54:22 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.5370
2022-03-15 04:54:55 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.6063
2022-03-15 04:55:28 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.6760
2022-03-15 04:56:02 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.5246
2022-03-15 04:56:35 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.7188
2022-03-15 04:57:08 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.5925
2022-03-15 04:57:41 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.5578
2022-03-15 04:58:15 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.3924
2022-03-15 04:58:48 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.4647
2022-03-15 04:59:21 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.5155
2022-03-15 04:59:54 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.5209
2022-03-15 05:00:28 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.3480
2022-03-15 05:01:01 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.4278
2022-03-15 05:01:35 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.6894
2022-03-15 05:02:08 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.2925
2022-03-15 05:02:41 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.3904
2022-03-15 05:03:14 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.6233
2022-03-15 05:03:48 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.6981
2022-03-15 05:04:21 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.6181
2022-03-15 05:04:54 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.6105
2022-03-15 05:05:28 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.5409
2022-03-15 05:06:01 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.5322
2022-03-15 05:06:34 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.4701
2022-03-15 05:07:08 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.5094
2022-03-15 05:07:42 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.4012
2022-03-15 05:08:15 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.6448
2022-03-15 05:08:49 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.3315
2022-03-15 05:09:21 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.5559
2022-03-15 05:09:55 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.9191
2022-03-15 05:10:29 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.3783
2022-03-15 05:11:03 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.7067
2022-03-15 05:11:36 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.5217
2022-03-15 05:12:09 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.2566
2022-03-15 05:12:10 - train: epoch 012, train_loss: 2.5530
2022-03-15 05:13:24 - eval: epoch: 012, acc1: 48.484%, acc5: 74.782%, test_loss: 2.2114, per_image_load_time: 1.132ms, per_image_inference_time: 0.352ms
2022-03-15 05:13:24 - until epoch: 012, best_acc1: 48.484%
2022-03-15 05:13:24 - epoch 013 lr: 0.1
2022-03-15 05:14:01 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.3373
2022-03-15 05:14:36 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.5682
2022-03-15 05:15:10 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.4957
2022-03-15 05:15:43 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.4567
2022-03-15 05:16:16 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.4227
2022-03-15 05:16:49 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.6085
2022-03-15 05:17:23 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.2838
2022-03-15 05:17:56 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.5916
2022-03-15 05:18:30 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.5092
2022-03-15 05:19:03 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.6490
2022-03-15 05:19:36 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.5284
2022-03-15 05:20:10 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.7242
2022-03-15 05:20:43 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.6551
2022-03-15 05:21:17 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.4650
2022-03-15 05:21:50 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.6607
2022-03-15 05:22:23 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.1882
2022-03-15 05:22:56 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.5867
2022-03-15 05:23:30 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.4654
2022-03-15 05:24:04 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.6458
2022-03-15 05:24:36 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.7962
2022-03-15 05:25:10 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.7882
2022-03-15 05:25:43 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.4786
2022-03-15 05:26:16 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.5168
2022-03-15 05:26:50 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.5214
2022-03-15 05:27:24 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.4682
2022-03-15 05:27:57 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.5272
2022-03-15 05:28:30 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.4506
2022-03-15 05:29:04 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.6398
2022-03-15 05:29:37 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.4977
2022-03-15 05:30:09 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.3177
2022-03-15 05:30:43 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.4293
2022-03-15 05:31:18 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.5411
2022-03-15 05:31:51 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.3278
2022-03-15 05:32:24 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.4455
2022-03-15 05:32:58 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.4950
2022-03-15 05:33:31 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.7147
2022-03-15 05:34:05 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.3357
2022-03-15 05:34:38 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.6134
2022-03-15 05:35:11 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.5440
2022-03-15 05:35:45 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.5854
2022-03-15 05:36:18 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.4850
2022-03-15 05:36:52 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.6112
2022-03-15 05:37:26 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.5341
2022-03-15 05:37:59 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.4235
2022-03-15 05:38:33 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.5879
2022-03-15 05:39:06 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.4036
2022-03-15 05:39:39 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.6340
2022-03-15 05:40:12 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.4770
2022-03-15 05:40:46 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.5457
2022-03-15 05:41:18 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.7775
2022-03-15 05:41:19 - train: epoch 013, train_loss: 2.5318
2022-03-15 05:42:33 - eval: epoch: 013, acc1: 49.364%, acc5: 75.402%, test_loss: 2.1779, per_image_load_time: 1.465ms, per_image_inference_time: 0.370ms
2022-03-15 05:42:33 - until epoch: 013, best_acc1: 49.364%
2022-03-15 05:42:33 - epoch 014 lr: 0.1
2022-03-15 05:43:11 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.5399
2022-03-15 05:43:45 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.6244
2022-03-15 05:44:19 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.1686
2022-03-15 05:44:51 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.4487
2022-03-15 05:45:25 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.3932
2022-03-15 05:45:58 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.5098
2022-03-15 05:46:31 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.3767
2022-03-15 05:47:04 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.4078
2022-03-15 05:47:37 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.5341
2022-03-15 05:48:11 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.5667
2022-03-15 05:48:46 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.4635
2022-03-15 05:49:18 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.5384
2022-03-15 05:49:51 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.4324
2022-03-15 05:50:24 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.6572
2022-03-15 05:50:58 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.6128
2022-03-15 05:51:32 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.4460
2022-03-15 05:52:05 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.7109
2022-03-15 05:52:40 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.6687
2022-03-15 05:53:12 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.4198
2022-03-15 05:53:46 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.4912
2022-03-15 05:54:19 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.7407
2022-03-15 05:54:52 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.5008
2022-03-15 05:55:26 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.5689
2022-03-15 05:55:59 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.6505
2022-03-15 05:56:33 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.5720
2022-03-15 05:57:06 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.5771
2022-03-15 05:57:39 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.5138
2022-03-15 05:58:13 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.6683
2022-03-15 05:58:46 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.4316
2022-03-15 05:59:19 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.6853
2022-03-15 05:59:52 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.3546
2022-03-15 06:00:26 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.4066
2022-03-15 06:01:00 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.2830
2022-03-15 06:01:34 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.3764
2022-03-15 06:02:07 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.4346
2022-03-15 06:02:41 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.3881
2022-03-15 06:03:14 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.3397
2022-03-15 06:03:47 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.6699
2022-03-15 06:04:20 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.3783
2022-03-15 06:04:54 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.7039
2022-03-15 06:05:27 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.5479
2022-03-15 06:06:02 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.2985
2022-03-15 06:06:35 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.3791
2022-03-15 06:07:08 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.3103
2022-03-15 06:07:42 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.5201
2022-03-15 06:08:15 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.4226
2022-03-15 06:08:48 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.4226
2022-03-15 06:09:22 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.4165
2022-03-15 06:09:55 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.3244
2022-03-15 06:10:27 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.5290
2022-03-15 06:10:27 - train: epoch 014, train_loss: 2.5112
2022-03-15 06:11:41 - eval: epoch: 014, acc1: 47.598%, acc5: 74.204%, test_loss: 2.2694, per_image_load_time: 2.056ms, per_image_inference_time: 0.358ms
2022-03-15 06:11:42 - until epoch: 014, best_acc1: 49.364%
2022-03-15 06:11:42 - epoch 015 lr: 0.1
2022-03-15 06:12:20 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.2963
2022-03-15 06:12:54 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.6776
2022-03-15 06:13:27 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.7360
2022-03-15 06:14:01 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.5028
2022-03-15 06:14:34 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.5173
2022-03-15 06:15:07 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.7657
2022-03-15 06:15:40 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.3625
2022-03-15 06:16:13 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.3837
2022-03-15 06:16:46 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.3859
2022-03-15 06:17:20 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.5415
2022-03-15 06:17:53 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.3548
2022-03-15 06:18:26 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.5404
2022-03-15 06:18:59 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.6730
2022-03-15 06:19:32 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.4512
2022-03-15 06:20:07 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.2230
2022-03-15 06:20:39 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.4617
2022-03-15 06:21:13 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.6892
2022-03-15 06:21:46 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.3606
2022-03-15 06:22:19 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.3841
2022-03-15 06:22:53 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.3982
2022-03-15 06:23:26 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.3352
2022-03-15 06:23:59 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.6262
2022-03-15 06:24:33 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.4094
2022-03-15 06:25:06 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.4938
2022-03-15 06:25:39 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.6282
2022-03-15 06:26:12 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.2694
2022-03-15 06:26:46 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.5922
2022-03-15 06:27:19 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.5748
2022-03-15 06:27:52 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.5095
2022-03-15 06:28:26 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.3092
2022-03-15 06:28:58 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.3412
2022-03-15 06:29:31 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.4231
2022-03-15 06:30:05 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.4272
2022-03-15 06:30:39 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.5146
2022-03-15 06:31:12 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.6868
2022-03-15 06:31:45 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.5636
2022-03-15 06:32:18 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.3222
2022-03-15 06:32:51 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.3522
2022-03-15 06:33:24 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.6055
2022-03-15 06:33:58 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.4833
2022-03-15 06:34:32 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.7347
2022-03-15 06:35:05 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.1858
2022-03-15 06:35:38 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.5137
2022-03-15 06:36:11 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.4486
2022-03-15 06:36:45 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.4711
2022-03-15 06:37:19 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.4507
2022-03-15 06:37:52 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.6833
2022-03-15 06:38:26 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.5574
2022-03-15 06:39:00 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.5569
2022-03-15 06:39:32 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.6707
2022-03-15 06:39:33 - train: epoch 015, train_loss: 2.4969
2022-03-15 06:40:47 - eval: epoch: 015, acc1: 49.632%, acc5: 75.582%, test_loss: 2.1695, per_image_load_time: 2.159ms, per_image_inference_time: 0.355ms
2022-03-15 06:40:47 - until epoch: 015, best_acc1: 49.632%
2022-03-15 06:40:47 - epoch 016 lr: 0.1
2022-03-15 06:41:25 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.5227
2022-03-15 06:42:00 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.3030
2022-03-15 06:42:32 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.4136
2022-03-15 06:43:06 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.7600
2022-03-15 06:43:39 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.2965
2022-03-15 06:44:13 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.5853
2022-03-15 06:44:46 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.2850
2022-03-15 06:45:20 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.5039
2022-03-15 06:45:52 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.5570
2022-03-15 06:46:26 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.3466
2022-03-15 06:46:59 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.3118
2022-03-15 06:47:32 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.3153
2022-03-15 06:48:06 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.6943
2022-03-15 06:48:39 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.3758
2022-03-15 06:49:13 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.5951
2022-03-15 06:49:46 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.5212
2022-03-15 06:50:20 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.4499
2022-03-15 06:50:54 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.4281
2022-03-15 06:51:27 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.5091
2022-03-15 06:52:00 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.1526
2022-03-15 06:52:34 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.5978
2022-03-15 06:53:07 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.4614
2022-03-15 06:53:41 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.5977
2022-03-15 06:54:15 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.6818
2022-03-15 06:54:48 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.3569
2022-03-15 06:55:22 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.5355
2022-03-15 06:55:55 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.3534
2022-03-15 06:56:28 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.3003
2022-03-15 06:57:02 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.6035
2022-03-15 06:57:35 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.7192
2022-03-15 06:58:08 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.6001
2022-03-15 06:58:42 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.6505
2022-03-15 06:59:16 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.6439
2022-03-15 06:59:50 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.5545
2022-03-15 07:00:23 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.3402
2022-03-15 07:00:57 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.4302
2022-03-15 07:01:31 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.6003
2022-03-15 07:02:04 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.8804
2022-03-15 07:02:38 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.5925
2022-03-15 07:03:11 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.4836
2022-03-15 07:03:44 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.4566
2022-03-15 07:04:18 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.4969
2022-03-15 07:04:52 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.2970
2022-03-15 07:05:25 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.3119
2022-03-15 07:05:59 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.5404
2022-03-15 07:06:32 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.3236
2022-03-15 07:07:06 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.6536
2022-03-15 07:07:39 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.4250
2022-03-15 07:08:13 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.5420
2022-03-15 07:08:44 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.5712
2022-03-15 07:08:45 - train: epoch 016, train_loss: 2.4817
2022-03-15 07:10:00 - eval: epoch: 016, acc1: 48.882%, acc5: 75.284%, test_loss: 2.2019, per_image_load_time: 0.811ms, per_image_inference_time: 0.345ms
2022-03-15 07:10:00 - until epoch: 016, best_acc1: 49.632%
2022-03-15 07:10:00 - epoch 017 lr: 0.1
2022-03-15 07:10:38 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.3154
2022-03-15 07:11:12 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.6032
2022-03-15 07:11:45 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.6556
2022-03-15 07:12:19 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.2373
2022-03-15 07:12:52 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.4582
2022-03-15 07:13:25 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.8368
2022-03-15 07:13:57 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.4571
2022-03-15 07:14:31 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.4452
2022-03-15 07:15:04 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.3943
2022-03-15 07:15:37 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.3680
2022-03-15 07:16:11 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.6695
2022-03-15 07:16:45 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.6727
2022-03-15 07:17:18 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.5586
2022-03-15 07:17:51 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.4920
2022-03-15 07:18:24 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.1291
2022-03-15 07:18:58 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.2801
2022-03-15 07:19:31 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.4358
2022-03-15 07:20:05 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.4247
2022-03-15 07:20:39 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.3089
2022-03-15 07:21:13 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.6497
2022-03-15 07:21:46 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.5233
2022-03-15 07:22:20 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.4161
2022-03-15 07:22:53 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.4884
2022-03-15 07:23:27 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.4115
2022-03-15 07:24:00 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.6763
2022-03-15 07:24:35 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.3626
2022-03-15 07:25:08 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.3526
2022-03-15 07:25:41 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.5834
2022-03-15 07:26:15 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.7304
2022-03-15 07:26:48 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.3302
2022-03-15 07:27:21 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.5921
2022-03-15 07:27:55 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.3950
2022-03-15 07:28:28 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.5394
2022-03-15 07:29:02 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.3131
2022-03-15 07:29:35 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.3669
2022-03-15 07:30:09 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.6816
2022-03-15 07:30:42 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.3981
2022-03-15 07:31:16 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.7055
2022-03-15 07:31:50 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.4366
2022-03-15 07:32:23 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.3321
2022-03-15 07:32:57 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.5334
2022-03-15 07:33:30 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.2967
2022-03-15 07:34:04 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.4831
2022-03-15 07:34:38 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.5187
2022-03-15 07:35:11 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.5049
2022-03-15 07:35:45 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.4813
2022-03-15 07:36:18 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.4597
2022-03-15 07:36:52 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.6233
2022-03-15 07:37:25 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.3201
2022-03-15 07:37:57 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.3885
2022-03-15 07:37:58 - train: epoch 017, train_loss: 2.4700
2022-03-15 07:39:13 - eval: epoch: 017, acc1: 49.198%, acc5: 75.422%, test_loss: 2.1779, per_image_load_time: 2.008ms, per_image_inference_time: 0.357ms
2022-03-15 07:39:13 - until epoch: 017, best_acc1: 49.632%
2022-03-15 07:39:13 - epoch 018 lr: 0.1
2022-03-15 07:39:51 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.3948
2022-03-15 07:40:25 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.4315
2022-03-15 07:40:59 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.5384
2022-03-15 07:41:31 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.6077
2022-03-15 07:42:05 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.4184
2022-03-15 07:42:38 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.6044
2022-03-15 07:43:11 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.3097
2022-03-15 07:43:44 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.3532
2022-03-15 07:44:18 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.4617
2022-03-15 07:44:51 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.3175
2022-03-15 07:45:25 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.7777
2022-03-15 07:45:58 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.4330
2022-03-15 07:46:31 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.6832
2022-03-15 07:47:04 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.5769
2022-03-15 07:47:38 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.6644
2022-03-15 07:48:11 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.3945
2022-03-15 07:48:45 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.3393
2022-03-15 07:49:18 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.3548
2022-03-15 07:49:51 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.4655
2022-03-15 07:50:25 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.7599
2022-03-15 07:50:59 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.6453
2022-03-15 07:51:32 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.3939
2022-03-15 07:52:06 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.5867
2022-03-15 07:52:39 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.3519
2022-03-15 07:53:12 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.1246
2022-03-15 07:53:46 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.2707
2022-03-15 07:54:19 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.6256
2022-03-15 07:54:53 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.2943
2022-03-15 07:55:26 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.4813
2022-03-15 07:56:00 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.2904
2022-03-15 07:56:33 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.8716
2022-03-15 07:57:06 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.3355
2022-03-15 07:57:39 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.3876
2022-03-15 07:58:13 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.3923
2022-03-15 07:58:45 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.5857
2022-03-15 07:59:19 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.4726
2022-03-15 07:59:52 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.7406
2022-03-15 08:00:25 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.4270
2022-03-15 08:00:59 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.4882
2022-03-15 08:01:32 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.3396
2022-03-15 08:02:06 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.2314
2022-03-15 08:02:39 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.5003
2022-03-15 08:03:11 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.3268
2022-03-15 08:03:45 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.5266
2022-03-15 08:04:19 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.4982
2022-03-15 08:04:53 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.3110
2022-03-15 08:05:26 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.6061
2022-03-15 08:06:00 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.5665
2022-03-15 08:06:33 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.4613
2022-03-15 08:07:05 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.5593
2022-03-15 08:07:06 - train: epoch 018, train_loss: 2.4556
2022-03-15 08:08:20 - eval: epoch: 018, acc1: 49.684%, acc5: 76.016%, test_loss: 2.1482, per_image_load_time: 1.265ms, per_image_inference_time: 0.354ms
2022-03-15 08:08:21 - until epoch: 018, best_acc1: 49.684%
2022-03-15 08:08:21 - epoch 019 lr: 0.1
2022-03-15 08:09:00 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.2028
2022-03-15 08:09:32 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.6207
2022-03-15 08:10:05 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.7000
2022-03-15 08:10:39 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.3823
2022-03-15 08:11:12 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.4093
2022-03-15 08:11:45 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.4035
2022-03-15 08:12:18 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.1958
2022-03-15 08:12:52 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.6375
2022-03-15 08:13:25 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.4135
2022-03-15 08:13:57 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.5998
2022-03-15 08:14:30 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.2719
2022-03-15 08:15:04 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.5134
2022-03-15 08:15:37 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.5132
2022-03-15 08:16:11 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.3697
2022-03-15 08:16:45 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.8305
2022-03-15 08:17:18 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.3520
2022-03-15 08:17:51 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.5008
2022-03-15 08:18:24 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.3625
2022-03-15 08:18:58 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.6474
2022-03-15 08:19:32 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.4439
2022-03-15 08:20:04 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.5084
2022-03-15 08:20:38 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.4617
2022-03-15 08:21:12 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.5569
2022-03-15 08:21:46 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.5662
2022-03-15 08:22:18 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.3489
2022-03-15 08:22:52 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.4804
2022-03-15 08:23:26 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.6018
2022-03-15 08:23:59 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.4792
2022-03-15 08:24:33 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.5552
2022-03-15 08:25:07 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.7175
2022-03-15 08:25:40 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.4810
2022-03-15 08:26:13 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.1697
2022-03-15 08:26:47 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.4100
2022-03-15 08:27:20 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.5693
2022-03-15 08:27:54 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.4262
2022-03-15 08:28:28 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.3084
2022-03-15 08:29:02 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.5855
2022-03-15 08:29:35 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.5991
2022-03-15 08:30:09 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.3128
2022-03-15 08:30:42 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.2400
2022-03-15 08:31:15 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.5447
2022-03-15 08:31:49 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.4081
2022-03-15 08:32:22 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.5030
2022-03-15 08:32:56 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.6123
2022-03-15 08:33:29 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.6951
2022-03-15 08:34:02 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.3878
2022-03-15 08:34:36 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.3411
2022-03-15 08:35:10 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.4715
2022-03-15 08:35:42 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.4062
2022-03-15 08:36:15 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.3810
2022-03-15 08:36:16 - train: epoch 019, train_loss: 2.4503
2022-03-15 08:37:30 - eval: epoch: 019, acc1: 49.802%, acc5: 76.118%, test_loss: 2.1426, per_image_load_time: 1.127ms, per_image_inference_time: 0.354ms
2022-03-15 08:37:30 - until epoch: 019, best_acc1: 49.802%
2022-03-15 08:37:30 - epoch 020 lr: 0.1
2022-03-15 08:38:08 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.5830
2022-03-15 08:38:42 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.1936
2022-03-15 08:39:15 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.5609
2022-03-15 08:39:48 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.2159
2022-03-15 08:40:22 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.3000
2022-03-15 08:40:55 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.5692
2022-03-15 08:41:29 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.2443
2022-03-15 08:42:02 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.5645
2022-03-15 08:42:36 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.7492
2022-03-15 08:43:08 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.5135
2022-03-15 08:43:41 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.2915
2022-03-15 08:44:14 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.2566
2022-03-15 08:44:48 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.3913
2022-03-15 08:45:22 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.5603
2022-03-15 08:45:56 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.4257
2022-03-15 08:46:29 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.2646
2022-03-15 08:47:03 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.1484
2022-03-15 08:47:35 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.4633
2022-03-15 08:48:09 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.2921
2022-03-15 08:48:42 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.4063
2022-03-15 08:49:16 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.6553
2022-03-15 08:49:49 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.2521
2022-03-15 08:50:23 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.4592
2022-03-15 08:50:56 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.5579
2022-03-15 08:51:30 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.2294
2022-03-15 08:52:02 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.2799
2022-03-15 08:52:36 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.4240
2022-03-15 08:53:09 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.6287
2022-03-15 08:53:43 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.5969
2022-03-15 08:54:17 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.3203
2022-03-15 08:54:50 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.5968
2022-03-15 08:55:22 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.6622
2022-03-15 08:55:57 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.4135
2022-03-15 08:56:30 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.4805
2022-03-15 08:57:05 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.2700
2022-03-15 08:57:38 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.4101
2022-03-15 08:58:11 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.3233
2022-03-15 08:58:44 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.4115
2022-03-15 08:59:18 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.5964
2022-03-15 08:59:52 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.3902
2022-03-15 09:00:25 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.2521
2022-03-15 09:00:59 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.3209
2022-03-15 09:01:31 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.5307
2022-03-15 09:02:06 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.3560
2022-03-15 09:02:40 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.3911
2022-03-15 09:03:12 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.6177
2022-03-15 09:03:46 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.3056
2022-03-15 09:04:20 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.4560
2022-03-15 09:04:54 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.4947
2022-03-15 09:05:26 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.3471
2022-03-15 09:05:27 - train: epoch 020, train_loss: 2.4364
2022-03-15 09:06:41 - eval: epoch: 020, acc1: 50.938%, acc5: 76.830%, test_loss: 2.0897, per_image_load_time: 0.947ms, per_image_inference_time: 0.346ms
2022-03-15 09:06:42 - until epoch: 020, best_acc1: 50.938%
2022-03-15 09:06:42 - epoch 021 lr: 0.1
2022-03-15 09:07:19 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.2611
2022-03-15 09:07:53 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.5374
2022-03-15 09:08:26 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.9894
2022-03-15 09:09:00 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.5265
2022-03-15 09:09:33 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.2132
2022-03-15 09:10:06 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.3187
2022-03-15 09:10:40 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.1993
2022-03-15 09:11:13 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.6241
2022-03-15 09:11:47 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.4728
2022-03-15 09:12:20 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.2306
2022-03-15 09:12:53 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.2527
2022-03-15 09:13:26 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.3100
2022-03-15 09:14:00 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.3381
2022-03-15 09:14:32 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.3675
2022-03-15 09:15:06 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.2577
2022-03-15 09:15:39 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.4363
2022-03-15 09:16:13 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.4621
2022-03-15 09:16:46 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.3659
2022-03-15 09:17:20 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.4640
2022-03-15 09:17:53 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.6533
2022-03-15 09:18:27 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.2330
2022-03-15 09:18:59 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.4285
2022-03-15 09:19:33 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.1686
2022-03-15 09:20:06 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.2786
2022-03-15 09:20:40 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.3595
2022-03-15 09:21:13 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.7581
2022-03-15 09:21:46 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.4310
2022-03-15 09:22:20 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.5287
2022-03-15 09:22:54 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.4200
2022-03-15 09:23:27 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.5915
2022-03-15 09:24:00 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.4377
2022-03-15 09:24:34 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.4494
2022-03-15 09:25:06 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.8560
2022-03-15 09:25:40 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.5315
2022-03-15 09:26:13 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.3039
2022-03-15 09:26:47 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.4623
2022-03-15 09:27:20 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.3605
2022-03-15 09:27:54 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.3425
2022-03-15 09:28:28 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.4257
2022-03-15 09:29:01 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.6693
2022-03-15 09:29:35 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.2700
2022-03-15 09:30:08 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.4410
2022-03-15 09:30:41 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.4352
2022-03-15 09:31:15 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.5096
2022-03-15 09:31:48 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.6256
2022-03-15 09:32:21 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.4567
2022-03-15 09:32:55 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.5780
2022-03-15 09:33:28 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.5202
2022-03-15 09:34:02 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.2431
2022-03-15 09:34:33 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.3031
2022-03-15 09:34:34 - train: epoch 021, train_loss: 2.4297
2022-03-15 09:35:48 - eval: epoch: 021, acc1: 49.872%, acc5: 76.046%, test_loss: 2.1464, per_image_load_time: 1.967ms, per_image_inference_time: 0.390ms
2022-03-15 09:35:49 - until epoch: 021, best_acc1: 50.938%
2022-03-15 09:35:49 - epoch 022 lr: 0.1
2022-03-15 09:36:28 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.1577
2022-03-15 09:37:01 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.2616
2022-03-15 09:37:34 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.1427
2022-03-15 09:38:08 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.2481
2022-03-15 09:38:41 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.3966
2022-03-15 09:39:13 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.4349
2022-03-15 09:39:47 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.5290
2022-03-15 09:40:20 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.5670
2022-03-15 09:40:54 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.5382
2022-03-15 09:41:27 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.4691
2022-03-15 09:42:00 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.4472
2022-03-15 09:42:34 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.2533
2022-03-15 09:43:08 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.3808
2022-03-15 09:43:41 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.5823
2022-03-15 09:44:14 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.3640
2022-03-15 09:44:48 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.1798
2022-03-15 09:45:20 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.1105
2022-03-15 09:45:54 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.6535
2022-03-15 09:46:27 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.2909
2022-03-15 09:47:01 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.4406
2022-03-15 09:47:34 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.5452
2022-03-15 09:48:08 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.2622
2022-03-15 09:48:40 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.5073
2022-03-15 09:49:14 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.3850
2022-03-15 09:49:47 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.3530
2022-03-15 09:50:21 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.2291
2022-03-15 09:50:54 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.2321
2022-03-15 09:51:28 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.7838
2022-03-15 09:52:01 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.2336
2022-03-15 09:52:35 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.4637
2022-03-15 09:53:08 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.5202
2022-03-15 09:53:41 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.4440
2022-03-15 09:54:15 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.3870
2022-03-15 09:54:48 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.2919
2022-03-15 09:55:21 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.4931
2022-03-15 09:55:56 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.5003
2022-03-15 09:56:28 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.4835
2022-03-15 09:57:02 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.5415
2022-03-15 09:57:35 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.3311
2022-03-15 09:58:09 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.4114
2022-03-15 09:58:41 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.3663
2022-03-15 09:59:15 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.3959
2022-03-15 09:59:49 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.5799
2022-03-15 10:00:22 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.4695
2022-03-15 10:00:56 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.4122
2022-03-15 10:01:28 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.6049
2022-03-15 10:02:01 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.4503
2022-03-15 10:02:34 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.2215
2022-03-15 10:03:08 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.2983
2022-03-15 10:03:39 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.3345
2022-03-15 10:03:40 - train: epoch 022, train_loss: 2.4251
2022-03-15 10:04:54 - eval: epoch: 022, acc1: 50.494%, acc5: 76.742%, test_loss: 2.1010, per_image_load_time: 2.229ms, per_image_inference_time: 0.388ms
2022-03-15 10:04:55 - until epoch: 022, best_acc1: 50.938%
2022-03-15 10:04:55 - epoch 023 lr: 0.1
2022-03-15 10:05:33 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.2163
2022-03-15 10:06:06 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.1206
2022-03-15 10:06:39 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.2579
2022-03-15 10:07:13 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.4540
2022-03-15 10:07:46 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.3334
2022-03-15 10:08:20 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.3606
2022-03-15 10:08:54 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.1843
2022-03-15 10:09:27 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.3550
2022-03-15 10:10:00 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.4233
2022-03-15 10:10:33 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.2482
2022-03-15 10:11:07 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.5374
2022-03-15 10:11:39 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.2299
2022-03-15 10:12:13 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.4191
2022-03-15 10:12:46 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.3723
2022-03-15 10:13:19 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.3207
2022-03-15 10:13:53 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.3400
2022-03-15 10:14:25 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.6534
2022-03-15 10:14:59 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.3664
2022-03-15 10:15:32 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.3674
2022-03-15 10:16:05 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.0714
2022-03-15 10:16:39 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.5359
2022-03-15 10:17:12 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.0326
2022-03-15 10:17:46 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.3110
2022-03-15 10:18:19 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.3995
2022-03-15 10:18:53 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.6552
2022-03-15 10:19:26 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.4599
2022-03-15 10:19:59 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.3825
2022-03-15 10:20:33 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.4578
2022-03-15 10:21:06 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.4189
2022-03-15 10:21:39 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.6069
2022-03-15 10:22:13 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.4454
2022-03-15 10:22:47 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.6228
2022-03-15 10:23:19 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.6018
2022-03-15 10:23:53 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.6153
2022-03-15 10:24:27 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.3221
2022-03-15 10:25:04 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.2713
2022-03-15 10:25:41 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.4017
2022-03-15 10:26:16 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.4279
2022-03-15 10:26:51 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.4236
2022-03-15 10:27:28 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.3707
2022-03-15 10:28:01 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.3046
2022-03-15 10:28:38 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.3756
2022-03-15 10:29:15 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.3016
2022-03-15 10:29:51 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.1724
2022-03-15 10:30:29 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.2808
2022-03-15 10:31:04 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.5138
2022-03-15 10:31:41 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.2315
2022-03-15 10:32:16 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.3375
2022-03-15 10:32:53 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.4332
2022-03-15 10:33:28 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.5726
2022-03-15 10:33:29 - train: epoch 023, train_loss: 2.4160
2022-03-15 10:34:50 - eval: epoch: 023, acc1: 50.526%, acc5: 76.340%, test_loss: 2.1226, per_image_load_time: 1.755ms, per_image_inference_time: 0.366ms
2022-03-15 10:34:51 - until epoch: 023, best_acc1: 50.938%
2022-03-15 10:34:51 - epoch 024 lr: 0.1
2022-03-15 10:35:33 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.3877
2022-03-15 10:36:11 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.4586
2022-03-15 10:36:47 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.3213
2022-03-15 10:37:23 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.5640
2022-03-15 10:38:00 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.3515
2022-03-15 10:38:36 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.1960
2022-03-15 10:39:12 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.3301
2022-03-15 10:39:48 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.2832
2022-03-15 10:40:24 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.5073
2022-03-15 10:40:56 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.3851
2022-03-15 10:41:34 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.1250
2022-03-15 10:42:10 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.2281
2022-03-15 10:42:46 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.8584
2022-03-15 10:43:22 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.2108
2022-03-15 10:43:57 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.6099
2022-03-15 10:44:33 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.4044
2022-03-15 10:45:10 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.3419
2022-03-15 10:45:45 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.5649
2022-03-15 10:46:22 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.1858
2022-03-15 10:46:59 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.4044
2022-03-15 10:47:36 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.4528
2022-03-15 10:48:13 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.1824
2022-03-15 10:48:50 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.3324
2022-03-15 10:49:27 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.4514
2022-03-15 10:50:04 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.3800
2022-03-15 10:50:40 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.3244
2022-03-15 10:51:16 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.4577
2022-03-15 10:51:53 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.5719
2022-03-15 10:52:30 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.5479
2022-03-15 10:53:03 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.2966
2022-03-15 10:53:37 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.3797
2022-03-15 10:54:12 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.5081
2022-03-15 10:54:47 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.2253
2022-03-15 10:55:22 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.4802
2022-03-15 10:55:57 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.2839
2022-03-15 10:56:31 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.3008
2022-03-15 10:57:05 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.3409
2022-03-15 10:57:41 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.6201
2022-03-15 10:58:15 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.5529
2022-03-15 10:58:48 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.4807
2022-03-15 10:59:26 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.3679
2022-03-15 11:00:01 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.4069
2022-03-15 11:00:35 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.3954
2022-03-15 11:01:08 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.4009
2022-03-15 11:01:42 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.3047
2022-03-15 11:02:20 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.4901
2022-03-15 11:02:56 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.4402
2022-03-15 11:03:34 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.3197
2022-03-15 11:04:10 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.4254
2022-03-15 11:04:45 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.5729
2022-03-15 11:04:46 - train: epoch 024, train_loss: 2.4095
2022-03-15 11:06:08 - eval: epoch: 024, acc1: 50.506%, acc5: 76.454%, test_loss: 2.1141, per_image_load_time: 1.689ms, per_image_inference_time: 0.361ms
2022-03-15 11:06:08 - until epoch: 024, best_acc1: 50.938%
2022-03-15 11:06:08 - epoch 025 lr: 0.1
2022-03-15 11:06:51 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.2809
2022-03-15 11:07:27 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.1527
2022-03-15 11:08:03 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.2860
2022-03-15 11:08:39 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.4322
2022-03-15 11:09:15 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.3107
2022-03-15 11:09:51 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.3721
2022-03-15 11:10:28 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.4165
2022-03-15 11:11:04 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.4703
2022-03-15 11:11:41 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.2071
2022-03-15 11:12:19 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.2881
2022-03-15 11:12:55 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.4609
2022-03-15 11:13:31 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.4262
2022-03-15 11:14:04 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.2989
2022-03-15 11:14:39 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.4243
2022-03-15 11:15:17 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.3244
2022-03-15 11:15:53 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.2865
2022-03-15 11:16:29 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.3660
2022-03-15 11:17:06 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.2780
2022-03-15 11:17:42 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.2719
2022-03-15 11:18:19 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.5164
2022-03-15 11:18:56 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.1848
2022-03-15 11:19:33 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.3441
2022-03-15 11:20:10 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.3853
2022-03-15 11:20:45 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.1866
2022-03-15 11:21:23 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.4654
2022-03-15 11:21:59 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.4229
2022-03-15 11:22:36 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.4453
2022-03-15 11:23:12 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.2866
2022-03-15 11:23:49 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.4993
2022-03-15 11:24:25 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.6175
2022-03-15 11:25:03 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.5027
2022-03-15 11:25:39 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.5166
2022-03-15 11:26:13 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.4716
2022-03-15 11:26:46 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.6216
2022-03-15 11:27:24 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.2218
2022-03-15 11:28:00 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.5966
2022-03-15 11:28:36 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.3033
2022-03-15 11:29:12 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.5517
2022-03-15 11:29:48 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.6977
2022-03-15 11:30:24 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.5170
2022-03-15 11:31:01 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.5855
2022-03-15 11:31:36 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.5159
2022-03-15 11:32:13 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.2598
2022-03-15 11:32:49 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.2701
2022-03-15 11:33:25 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.3658
2022-03-15 11:34:01 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.3324
2022-03-15 11:34:38 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.4190
2022-03-15 11:35:13 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.1446
2022-03-15 11:35:51 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.3407
2022-03-15 11:36:26 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.4689
2022-03-15 11:36:27 - train: epoch 025, train_loss: 2.4028
2022-03-15 11:37:48 - eval: epoch: 025, acc1: 50.778%, acc5: 76.564%, test_loss: 2.1131, per_image_load_time: 2.499ms, per_image_inference_time: 0.358ms
2022-03-15 11:37:48 - until epoch: 025, best_acc1: 50.938%
2022-03-15 11:37:48 - epoch 026 lr: 0.1
2022-03-15 11:38:29 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.1813
2022-03-15 11:39:02 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.1851
2022-03-15 11:39:39 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.3660
2022-03-15 11:40:15 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.3371
2022-03-15 11:40:52 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.5135
2022-03-15 11:41:29 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.4092
2022-03-15 11:42:05 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.3074
2022-03-15 11:42:41 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.2108
2022-03-15 11:43:19 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.7635
2022-03-15 11:43:54 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.2664
2022-03-15 11:44:31 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.3083
2022-03-15 11:45:09 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.4209
2022-03-15 11:45:44 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.3484
2022-03-15 11:46:22 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.4797
2022-03-15 11:46:58 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.3572
2022-03-15 11:47:35 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.5239
2022-03-15 11:48:11 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.3574
2022-03-15 11:48:49 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.4647
2022-03-15 11:49:25 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.7883
2022-03-15 11:50:02 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.4697
2022-03-15 11:50:39 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.5163
2022-03-15 11:51:12 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.4938
2022-03-15 11:51:48 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.3282
2022-03-15 11:52:26 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.6182
2022-03-15 11:53:01 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.5860
2022-03-15 11:53:38 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.3417
2022-03-15 11:54:14 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.3326
2022-03-15 11:54:50 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.2051
2022-03-15 11:55:27 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.6134
2022-03-15 11:56:03 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.1544
2022-03-15 11:56:40 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.3359
2022-03-15 11:57:17 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.4204
2022-03-15 11:57:52 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.2010
2022-03-15 11:58:29 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.5002
2022-03-15 11:59:05 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.4946
2022-03-15 11:59:41 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.2619
2022-03-15 12:00:18 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.4520
2022-03-15 12:00:55 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.5094
2022-03-15 12:01:32 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.5425
2022-03-15 12:02:08 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.5280
2022-03-15 12:02:44 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.5332
2022-03-15 12:03:19 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.5109
2022-03-15 12:03:53 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.5204
2022-03-15 12:04:29 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.5746
2022-03-15 12:05:05 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.7298
2022-03-15 12:05:41 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.4015
2022-03-15 12:06:17 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.1797
2022-03-15 12:06:54 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.2947
2022-03-15 12:07:30 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.4517
2022-03-15 12:08:06 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.6262
2022-03-15 12:08:07 - train: epoch 026, train_loss: 2.3973
2022-03-15 12:09:29 - eval: epoch: 026, acc1: 51.184%, acc5: 77.246%, test_loss: 2.0804, per_image_load_time: 2.818ms, per_image_inference_time: 0.350ms
2022-03-15 12:09:29 - until epoch: 026, best_acc1: 51.184%
2022-03-15 12:09:29 - epoch 027 lr: 0.1
2022-03-15 12:10:12 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.5981
2022-03-15 12:10:49 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.3217
2022-03-15 12:11:25 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.5348
2022-03-15 12:12:01 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.6179
2022-03-15 12:12:38 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.4042
2022-03-15 12:13:14 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.6056
2022-03-15 12:13:52 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.5850
2022-03-15 12:14:28 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.3829
2022-03-15 12:15:05 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.2812
2022-03-15 12:15:42 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.4824
2022-03-15 12:16:15 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.2957
2022-03-15 12:16:50 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.6220
2022-03-15 12:17:26 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.4857
2022-03-15 12:18:03 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.6909
2022-03-15 12:18:40 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.4023
2022-03-15 12:19:15 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.3998
2022-03-15 12:19:52 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.3601
2022-03-15 12:20:30 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.4472
2022-03-15 12:21:06 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.5396
2022-03-15 12:21:43 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.3394
2022-03-15 12:22:19 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.6613
2022-03-15 12:22:56 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.5709
2022-03-15 12:23:32 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.7058
2022-03-15 12:24:09 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.3394
2022-03-15 12:24:46 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.3317
2022-03-15 12:25:23 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.6108
2022-03-15 12:25:59 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.4251
2022-03-15 12:26:36 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.3918
2022-03-15 12:27:12 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.4899
2022-03-15 12:27:49 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.1270
2022-03-15 12:28:24 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.2734
2022-03-15 12:28:57 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.2477
2022-03-15 12:29:33 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.3373
2022-03-15 12:30:10 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.2284
2022-03-15 12:30:45 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.6291
2022-03-15 12:31:21 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.2037
2022-03-15 12:31:56 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.3410
2022-03-15 12:32:33 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.2277
2022-03-15 12:33:09 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.1868
2022-03-15 12:33:45 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.5543
2022-03-15 12:34:22 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.3465
2022-03-15 12:34:58 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.5415
2022-03-15 12:35:34 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.2447
2022-03-15 12:36:11 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.3889
2022-03-15 12:36:46 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.2618
2022-03-15 12:37:24 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.2889
2022-03-15 12:37:59 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.4364
2022-03-15 12:38:36 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.5791
2022-03-15 12:39:12 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.4008
2022-03-15 12:39:46 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.9666
2022-03-15 12:39:47 - train: epoch 027, train_loss: 2.3930
2022-03-15 12:41:04 - eval: epoch: 027, acc1: 51.534%, acc5: 77.134%, test_loss: 2.0716, per_image_load_time: 2.469ms, per_image_inference_time: 0.336ms
2022-03-15 12:41:04 - until epoch: 027, best_acc1: 51.534%
2022-03-15 12:41:04 - epoch 028 lr: 0.1
2022-03-15 12:41:46 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.1189
2022-03-15 12:42:22 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.2553
2022-03-15 12:42:59 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.2672
2022-03-15 12:43:36 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.2377
2022-03-15 12:44:13 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.2205
2022-03-15 12:44:49 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.4142
2022-03-15 12:45:24 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.7349
2022-03-15 12:46:01 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.0912
2022-03-15 12:46:37 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.2415
2022-03-15 12:47:14 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.4397
2022-03-15 12:47:51 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.4529
2022-03-15 12:48:26 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.1898
2022-03-15 12:49:04 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.3554
2022-03-15 12:49:41 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.6228
2022-03-15 12:50:16 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.3056
2022-03-15 12:50:53 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.2587
2022-03-15 12:51:29 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.3124
2022-03-15 12:52:07 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.2184
2022-03-15 12:52:43 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.3591
2022-03-15 12:53:18 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.6185
2022-03-15 12:53:50 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.5266
2022-03-15 12:54:29 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.4675
2022-03-15 12:55:04 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.4631
2022-03-15 12:55:41 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.5854
2022-03-15 12:56:17 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.3691
2022-03-15 12:56:53 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.2608
2022-03-15 12:57:30 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.3695
2022-03-15 12:58:07 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.3020
2022-03-15 12:58:43 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.4451
2022-03-15 12:59:20 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.4094
2022-03-15 12:59:56 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.5476
2022-03-15 13:00:32 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.3363
2022-03-15 13:01:08 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.4032
2022-03-15 13:01:44 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.3350
2022-03-15 13:02:20 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.2879
2022-03-15 13:02:57 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.2656
2022-03-15 13:03:32 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.3375
2022-03-15 13:04:08 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.1313
2022-03-15 13:04:46 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.3942
2022-03-15 13:05:20 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.4227
2022-03-15 13:05:53 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.4552
2022-03-15 13:06:29 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.4977
2022-03-15 13:07:06 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.4037
2022-03-15 13:07:43 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.3719
2022-03-15 13:08:18 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.4411
2022-03-15 13:08:55 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.4584
2022-03-15 13:09:30 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.3855
2022-03-15 13:10:07 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.3559
2022-03-15 13:10:42 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.2355
2022-03-15 13:11:15 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.3112
2022-03-15 13:11:16 - train: epoch 028, train_loss: 2.3845
2022-03-15 13:12:36 - eval: epoch: 028, acc1: 50.930%, acc5: 76.472%, test_loss: 2.1005, per_image_load_time: 0.805ms, per_image_inference_time: 0.307ms
2022-03-15 13:12:37 - until epoch: 028, best_acc1: 51.534%
2022-03-15 13:12:37 - epoch 029 lr: 0.1
2022-03-15 13:13:19 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.3171
2022-03-15 13:13:54 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.4944
2022-03-15 13:14:30 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.5140
2022-03-15 13:15:04 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.3913
2022-03-15 13:15:40 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.3933
2022-03-15 13:16:15 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.7572
2022-03-15 13:16:51 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.0539
2022-03-15 13:17:25 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.4477
2022-03-15 13:17:57 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.2846
2022-03-15 13:18:32 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.2351
2022-03-15 13:19:07 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.2746
2022-03-15 13:19:43 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.4816
2022-03-15 13:20:18 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.4296
2022-03-15 13:20:53 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.5878
2022-03-15 13:21:29 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.4973
2022-03-15 13:22:04 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.2978
2022-03-15 13:22:39 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.3568
2022-03-15 13:23:14 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.4334
2022-03-15 13:23:51 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.2163
2022-03-15 13:24:26 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.6455
2022-03-15 13:25:01 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.3418
2022-03-15 13:25:36 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.5158
2022-03-15 13:26:13 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.3759
2022-03-15 13:26:48 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.3729
2022-03-15 13:27:23 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.2759
2022-03-15 13:27:59 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.3918
2022-03-15 13:28:34 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.3166
2022-03-15 13:29:09 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.3189
2022-03-15 13:29:42 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.3005
2022-03-15 13:30:15 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.3755
2022-03-15 13:30:51 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.3458
2022-03-15 13:31:26 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.3669
2022-03-15 13:32:02 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.2418
2022-03-15 13:32:37 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.1959
2022-03-15 13:33:13 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.5336
2022-03-15 13:33:48 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.3487
2022-03-15 13:34:24 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.3081
2022-03-15 13:34:58 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.3441
2022-03-15 13:35:35 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.1134
2022-03-15 13:36:09 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.3329
2022-03-15 13:36:45 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.3367
2022-03-15 13:37:20 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.2588
2022-03-15 13:37:55 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.4835
2022-03-15 13:38:31 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.3311
2022-03-15 13:39:06 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.5253
2022-03-15 13:39:42 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.5440
2022-03-15 13:40:18 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.2237
2022-03-15 13:40:53 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.5206
2022-03-15 13:41:28 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.8427
2022-03-15 13:42:00 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.0494
2022-03-15 13:42:01 - train: epoch 029, train_loss: 2.3817
2022-03-15 13:43:19 - eval: epoch: 029, acc1: 51.628%, acc5: 77.458%, test_loss: 2.0586, per_image_load_time: 0.856ms, per_image_inference_time: 0.324ms
2022-03-15 13:43:19 - until epoch: 029, best_acc1: 51.628%
2022-03-15 13:43:19 - epoch 030 lr: 0.1
2022-03-15 13:44:01 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.4406
2022-03-15 13:44:35 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.4390
2022-03-15 13:45:11 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.3420
2022-03-15 13:45:46 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.1208
2022-03-15 13:46:21 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.7205
2022-03-15 13:46:58 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.2137
2022-03-15 13:47:32 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.3941
2022-03-15 13:48:07 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.5284
2022-03-15 13:48:41 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.4454
2022-03-15 13:49:16 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.1363
2022-03-15 13:49:49 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.1977
2022-03-15 13:50:24 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.3006
2022-03-15 13:50:58 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.0960
2022-03-15 13:51:33 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.2666
2022-03-15 13:52:08 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.3019
2022-03-15 13:52:43 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.4650
2022-03-15 13:53:18 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.6025
2022-03-15 13:53:51 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.4178
2022-03-15 13:54:25 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.5450
2022-03-15 13:55:00 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.4138
2022-03-15 13:55:35 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.4590
2022-03-15 13:56:09 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.3523
2022-03-15 13:56:44 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.3364
2022-03-15 13:57:20 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.4981
2022-03-15 13:57:54 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.4491
2022-03-15 13:58:30 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.4739
2022-03-15 13:59:05 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.2972
2022-03-15 13:59:40 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.3871
2022-03-15 14:00:15 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.2576
2022-03-15 14:00:50 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.5357
2022-03-15 14:01:25 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.4070
2022-03-15 14:02:01 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.1902
2022-03-15 14:02:36 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.5502
2022-03-15 14:03:12 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.2967
2022-03-15 14:03:48 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.4321
2022-03-15 14:04:24 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.1663
2022-03-15 14:04:58 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.2870
2022-03-15 14:05:33 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.5573
2022-03-15 14:06:06 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.4140
2022-03-15 14:06:39 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.2344
2022-03-15 14:07:14 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.2891
2022-03-15 14:07:50 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.5564
2022-03-15 14:08:24 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.3150
2022-03-15 14:08:59 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.3314
2022-03-15 14:09:34 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.5960
2022-03-15 14:10:09 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.1754
2022-03-15 14:10:43 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.4581
2022-03-15 14:11:18 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.4146
2022-03-15 14:11:53 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.6477
2022-03-15 14:12:26 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.5126
2022-03-15 14:12:27 - train: epoch 030, train_loss: 2.3805
2022-03-15 14:13:46 - eval: epoch: 030, acc1: 51.902%, acc5: 77.542%, test_loss: 2.0564, per_image_load_time: 1.096ms, per_image_inference_time: 0.318ms
2022-03-15 14:13:47 - until epoch: 030, best_acc1: 51.902%
2022-03-15 14:13:47 - epoch 031 lr: 0.010000000000000002
2022-03-15 14:14:28 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.2459
2022-03-15 14:15:03 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.9255
2022-03-15 14:15:39 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.8998
2022-03-15 14:16:14 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.0472
2022-03-15 14:16:48 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.8504
2022-03-15 14:17:24 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.9378
2022-03-15 14:17:57 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.8612
2022-03-15 14:18:29 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.9110
2022-03-15 14:19:05 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.8498
2022-03-15 14:19:41 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.2181
2022-03-15 14:20:15 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.1122
2022-03-15 14:20:51 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.9217
2022-03-15 14:21:25 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.6692
2022-03-15 14:22:00 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.9559
2022-03-15 14:22:36 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.0101
2022-03-15 14:23:11 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.7409
2022-03-15 14:23:46 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.7702
2022-03-15 14:24:21 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.8795
2022-03-15 14:24:57 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.9729
2022-03-15 14:25:31 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.8484
2022-03-15 14:26:07 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.7455
2022-03-15 14:26:42 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.6980
2022-03-15 14:27:18 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.7719
2022-03-15 14:27:52 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.7660
2022-03-15 14:28:27 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.7125
2022-03-15 14:29:03 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.8199
2022-03-15 14:29:38 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.8764
2022-03-15 14:30:09 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.0713
2022-03-15 14:30:45 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.9099
2022-03-15 14:31:19 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.0268
2022-03-15 14:31:54 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.7404
2022-03-15 14:32:30 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.9990
2022-03-15 14:33:05 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.9293
2022-03-15 14:33:41 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.9740
2022-03-15 14:34:15 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.9525
2022-03-15 14:34:51 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.9090
2022-03-15 14:35:27 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.7303
2022-03-15 14:36:02 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.6391
2022-03-15 14:36:38 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.8512
2022-03-15 14:37:13 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.7046
2022-03-15 14:37:49 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.7719
2022-03-15 14:38:24 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.9185
2022-03-15 14:38:59 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.7644
2022-03-15 14:39:35 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.8866
2022-03-15 14:40:11 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.8814
2022-03-15 14:40:46 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.9040
2022-03-15 14:41:22 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.7994
2022-03-15 14:41:54 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.6792
2022-03-15 14:42:27 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.6489
2022-03-15 14:43:01 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.7492
2022-03-15 14:43:02 - train: epoch 031, train_loss: 1.8651
2022-03-15 14:44:22 - eval: epoch: 031, acc1: 64.256%, acc5: 85.822%, test_loss: 1.4664, per_image_load_time: 0.857ms, per_image_inference_time: 0.286ms
2022-03-15 14:44:22 - until epoch: 031, best_acc1: 64.256%
2022-03-15 14:44:22 - epoch 032 lr: 0.010000000000000002
2022-03-15 14:45:04 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.6983
2022-03-15 14:45:39 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.8044
2022-03-15 14:46:14 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.7491
2022-03-15 14:46:50 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.9586
2022-03-15 14:47:25 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.8007
2022-03-15 14:48:00 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.9714
2022-03-15 14:48:35 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.7612
2022-03-15 14:49:10 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.7566
2022-03-15 14:49:45 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.8116
2022-03-15 14:50:21 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.8679
2022-03-15 14:50:56 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.8312
2022-03-15 14:51:32 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.7081
2022-03-15 14:52:06 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.6909
2022-03-15 14:52:42 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.8171
2022-03-15 14:53:17 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.8215
2022-03-15 14:53:50 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.9323
2022-03-15 14:54:24 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.8382
2022-03-15 14:54:59 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.0193
2022-03-15 14:55:34 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.6128
2022-03-15 14:56:10 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.6964
2022-03-15 14:56:44 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.6591
2022-03-15 14:57:21 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.6393
2022-03-15 14:57:56 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.8320
2022-03-15 14:58:31 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.7100
2022-03-15 14:59:06 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.8992
2022-03-15 14:59:41 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.5103
2022-03-15 15:00:17 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.7544
2022-03-15 15:00:52 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.8886
2022-03-15 15:01:27 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.5046
2022-03-15 15:02:03 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.4873
2022-03-15 15:02:38 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.7656
2022-03-15 15:03:13 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.7154
2022-03-15 15:03:49 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.5582
2022-03-15 15:04:25 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.5574
2022-03-15 15:05:00 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.6804
2022-03-15 15:05:35 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.8595
2022-03-15 15:06:07 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.8123
2022-03-15 15:06:42 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.5989
2022-03-15 15:07:16 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.7195
2022-03-15 15:07:52 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.6125
2022-03-15 15:08:26 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.7339
2022-03-15 15:09:01 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.6352
2022-03-15 15:09:36 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.8813
2022-03-15 15:10:12 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.6736
2022-03-15 15:10:47 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.8412
2022-03-15 15:11:22 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.6799
2022-03-15 15:11:57 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.8561
2022-03-15 15:12:33 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.6078
2022-03-15 15:13:07 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.9617
2022-03-15 15:13:41 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.7427
2022-03-15 15:13:42 - train: epoch 032, train_loss: 1.7441
2022-03-15 15:15:02 - eval: epoch: 032, acc1: 65.216%, acc5: 86.446%, test_loss: 1.4251, per_image_load_time: 0.808ms, per_image_inference_time: 0.314ms
2022-03-15 15:15:03 - until epoch: 032, best_acc1: 65.216%
2022-03-15 15:15:03 - epoch 033 lr: 0.010000000000000002
2022-03-15 15:15:44 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.4771
2022-03-15 15:16:20 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.8150
2022-03-15 15:16:55 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.6021
2022-03-15 15:17:29 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.6026
2022-03-15 15:18:01 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.7325
2022-03-15 15:18:36 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.5703
2022-03-15 15:19:11 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.7979
2022-03-15 15:19:47 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.8484
2022-03-15 15:20:21 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.6688
2022-03-15 15:20:57 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.7670
2022-03-15 15:21:32 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.6958
2022-03-15 15:22:08 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.6544
2022-03-15 15:22:43 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.5634
2022-03-15 15:23:18 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.8157
2022-03-15 15:23:55 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.9636
2022-03-15 15:24:30 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.8298
2022-03-15 15:25:06 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.5942
2022-03-15 15:25:41 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.9349
2022-03-15 15:26:18 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.7837
2022-03-15 15:26:53 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.5964
2022-03-15 15:27:30 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.7054
2022-03-15 15:28:05 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.7819
2022-03-15 15:28:41 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.5048
2022-03-15 15:29:17 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.9175
2022-03-15 15:29:50 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.6801
2022-03-15 15:30:23 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.5403
2022-03-15 15:30:59 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.8524
2022-03-15 15:31:35 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.7154
2022-03-15 15:32:11 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.7924
2022-03-15 15:32:46 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.7861
2022-03-15 15:33:21 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.6675
2022-03-15 15:33:57 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.7137
2022-03-15 15:34:32 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.7356
2022-03-15 15:35:08 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.6628
2022-03-15 15:35:43 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.7738
2022-03-15 15:36:19 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.8864
2022-03-15 15:36:53 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.7038
2022-03-15 15:37:30 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.5966
2022-03-15 15:38:04 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.8544
2022-03-15 15:38:41 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.7501
2022-03-15 15:39:17 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.5847
2022-03-15 15:39:52 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.5930
2022-03-15 15:40:28 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.8245
2022-03-15 15:41:04 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.7389
2022-03-15 15:41:38 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.9347
2022-03-15 15:42:10 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.5631
2022-03-15 15:42:46 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.6184
2022-03-15 15:43:23 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.0664
2022-03-15 15:43:58 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.5403
2022-03-15 15:44:32 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.6353
2022-03-15 15:44:33 - train: epoch 033, train_loss: 1.6982
2022-03-15 15:45:52 - eval: epoch: 033, acc1: 65.524%, acc5: 86.774%, test_loss: 1.4015, per_image_load_time: 2.659ms, per_image_inference_time: 0.347ms
2022-03-15 15:45:53 - until epoch: 033, best_acc1: 65.524%
2022-03-15 15:45:53 - epoch 034 lr: 0.010000000000000002
2022-03-15 15:46:35 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.6195
2022-03-15 15:47:10 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.6107
2022-03-15 15:47:46 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.6866
2022-03-15 15:48:20 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.3860
2022-03-15 15:48:56 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.6388
2022-03-15 15:49:31 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.9547
2022-03-15 15:50:06 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.6776
2022-03-15 15:50:41 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.6022
2022-03-15 15:51:17 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.6392
2022-03-15 15:51:52 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.5414
2022-03-15 15:52:27 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.6644
2022-03-15 15:53:02 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.6346
2022-03-15 15:53:36 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.6477
2022-03-15 15:54:07 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.7117
2022-03-15 15:54:43 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.6544
2022-03-15 15:55:18 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.5884
2022-03-15 15:55:54 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.6779
2022-03-15 15:56:30 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.8785
2022-03-15 15:57:05 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.7983
2022-03-15 15:57:41 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.8649
2022-03-15 15:58:16 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.9565
2022-03-15 15:58:52 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.5696
2022-03-15 15:59:27 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.8777
2022-03-15 16:00:02 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.4713
2022-03-15 16:00:37 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.7843
2022-03-15 16:01:12 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.6748
2022-03-15 16:01:47 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.7168
2022-03-15 16:02:23 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.5761
2022-03-15 16:02:58 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.3872
2022-03-15 16:03:33 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.4795
2022-03-15 16:04:08 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.6364
2022-03-15 16:04:44 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.7420
2022-03-15 16:05:19 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.7522
2022-03-15 16:05:53 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.7061
2022-03-15 16:06:25 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.4667
2022-03-15 16:07:01 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.5542
2022-03-15 16:07:35 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.4788
2022-03-15 16:08:11 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.6013
2022-03-15 16:08:45 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.7424
2022-03-15 16:09:21 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.5010
2022-03-15 16:09:55 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.6268
2022-03-15 16:10:30 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.6032
2022-03-15 16:11:06 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.6904
2022-03-15 16:11:41 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.6677
2022-03-15 16:12:16 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.8050
2022-03-15 16:12:52 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.7297
2022-03-15 16:13:27 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.7103
2022-03-15 16:14:02 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.5851
2022-03-15 16:14:38 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.7108
2022-03-15 16:15:12 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.4708
2022-03-15 16:15:13 - train: epoch 034, train_loss: 1.6737
2022-03-15 16:16:34 - eval: epoch: 034, acc1: 66.212%, acc5: 87.114%, test_loss: 1.3806, per_image_load_time: 0.978ms, per_image_inference_time: 0.314ms
2022-03-15 16:16:34 - until epoch: 034, best_acc1: 66.212%
2022-03-15 16:16:34 - epoch 035 lr: 0.010000000000000002
2022-03-15 16:17:16 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.5720
2022-03-15 16:17:50 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.4237
2022-03-15 16:18:22 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.7202
2022-03-15 16:18:57 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.5630
2022-03-15 16:19:32 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.7292
2022-03-15 16:20:07 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.6851
2022-03-15 16:20:43 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.5817
2022-03-15 16:21:19 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.6304
2022-03-15 16:21:54 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.7868
2022-03-15 16:22:29 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.8339
2022-03-15 16:23:05 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.7027
2022-03-15 16:23:40 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.5668
2022-03-15 16:24:16 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.7556
2022-03-15 16:24:51 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.6303
2022-03-15 16:25:26 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.7433
2022-03-15 16:26:02 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.6349
2022-03-15 16:26:38 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.4281
2022-03-15 16:27:12 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.7326
2022-03-15 16:27:49 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.7085
2022-03-15 16:28:24 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.6961
2022-03-15 16:29:00 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.6166
2022-03-15 16:29:35 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.7679
2022-03-15 16:30:07 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.7205
2022-03-15 16:30:41 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.8058
2022-03-15 16:31:17 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.5928
2022-03-15 16:31:51 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.7975
2022-03-15 16:32:28 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.7670
2022-03-15 16:33:04 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.6480
2022-03-15 16:33:40 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.7523
2022-03-15 16:34:15 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.7685
2022-03-15 16:34:51 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.6767
2022-03-15 16:35:27 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.4527
2022-03-15 16:36:03 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.6083
2022-03-15 16:36:40 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.6740
2022-03-15 16:37:15 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.3640
2022-03-15 16:37:51 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.6865
2022-03-15 16:38:27 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.4185
2022-03-15 16:39:02 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.5712
2022-03-15 16:39:39 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.8269
2022-03-15 16:40:15 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.4008
2022-03-15 16:40:51 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.8875
2022-03-15 16:41:27 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.4893
2022-03-15 16:42:02 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.7293
2022-03-15 16:42:35 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.6620
2022-03-15 16:43:10 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.7888
2022-03-15 16:43:45 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.5790
2022-03-15 16:44:20 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.9534
2022-03-15 16:44:56 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.8699
2022-03-15 16:45:31 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.7158
2022-03-15 16:46:06 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.5114
2022-03-15 16:46:07 - train: epoch 035, train_loss: 1.6568
2022-03-15 16:47:28 - eval: epoch: 035, acc1: 65.954%, acc5: 87.134%, test_loss: 1.3798, per_image_load_time: 0.973ms, per_image_inference_time: 0.321ms
2022-03-15 16:47:28 - until epoch: 035, best_acc1: 66.212%
2022-03-15 16:47:28 - epoch 036 lr: 0.010000000000000002
2022-03-15 16:48:11 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.7343
2022-03-15 16:48:47 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.3010
2022-03-15 16:49:22 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.5478
2022-03-15 16:49:58 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.6413
2022-03-15 16:50:34 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.6598
2022-03-15 16:51:10 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.6053
2022-03-15 16:51:46 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.4730
2022-03-15 16:52:21 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.4612
2022-03-15 16:52:58 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.6310
2022-03-15 16:53:33 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.6599
2022-03-15 16:54:07 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.7831
2022-03-15 16:54:41 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.7112
2022-03-15 16:55:15 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.6416
2022-03-15 16:55:52 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.8741
2022-03-15 16:56:27 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.8904
2022-03-15 16:57:03 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.6960
2022-03-15 16:57:39 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.6766
2022-03-15 16:58:14 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.5258
2022-03-15 16:58:50 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.6288
2022-03-15 16:59:26 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.5409
2022-03-15 17:00:02 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.7557
2022-03-15 17:00:38 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.5451
2022-03-15 17:01:14 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.6611
2022-03-15 17:01:49 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.8258
2022-03-15 17:02:25 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.4801
2022-03-15 17:03:01 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.6881
2022-03-15 17:03:36 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.6212
2022-03-15 17:04:13 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.5867
2022-03-15 17:04:48 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.5251
2022-03-15 17:05:25 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.6816
2022-03-15 17:06:00 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.7075
2022-03-15 17:06:33 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.6432
2022-03-15 17:07:06 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.5388
2022-03-15 17:07:42 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.6043
2022-03-15 17:08:17 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.6186
2022-03-15 17:08:52 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.7848
2022-03-15 17:09:29 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.6262
2022-03-15 17:10:05 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.6002
2022-03-15 17:10:41 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.6293
2022-03-15 17:11:15 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.6122
2022-03-15 17:11:51 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.6460
2022-03-15 17:12:26 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.5754
2022-03-15 17:13:02 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.5811
2022-03-15 17:13:37 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.6735
2022-03-15 17:14:14 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.3943
2022-03-15 17:14:50 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.4961
2022-03-15 17:15:25 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.6604
2022-03-15 17:16:01 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.7405
2022-03-15 17:16:36 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.6493
2022-03-15 17:17:10 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.6123
2022-03-15 17:17:11 - train: epoch 036, train_loss: 1.6455
2022-03-15 17:18:31 - eval: epoch: 036, acc1: 66.086%, acc5: 87.194%, test_loss: 1.3762, per_image_load_time: 1.074ms, per_image_inference_time: 0.315ms
2022-03-15 17:18:31 - until epoch: 036, best_acc1: 66.212%
2022-03-15 17:18:31 - epoch 037 lr: 0.010000000000000002
2022-03-15 17:19:10 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.4085
2022-03-15 17:19:46 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.3685
2022-03-15 17:20:22 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.5343
2022-03-15 17:20:58 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.6911
2022-03-15 17:21:33 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.5504
2022-03-15 17:22:09 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.7693
2022-03-15 17:22:44 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.7270
2022-03-15 17:23:20 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.5709
2022-03-15 17:23:56 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.9421
2022-03-15 17:24:32 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.5109
2022-03-15 17:25:08 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.7673
2022-03-15 17:25:44 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.6693
2022-03-15 17:26:20 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.6254
2022-03-15 17:26:56 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.7897
2022-03-15 17:27:32 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.5378
2022-03-15 17:28:07 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.4902
2022-03-15 17:28:43 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.8637
2022-03-15 17:29:18 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.7142
2022-03-15 17:29:54 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.6900
2022-03-15 17:30:30 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.6580
2022-03-15 17:31:03 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.6693
2022-03-15 17:31:37 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.5746
2022-03-15 17:32:12 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.5720
2022-03-15 17:32:47 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.6887
2022-03-15 17:33:24 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.5671
2022-03-15 17:33:59 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.8690
2022-03-15 17:34:35 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.7532
2022-03-15 17:35:11 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.5878
2022-03-15 17:35:46 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.6472
2022-03-15 17:36:22 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.7644
2022-03-15 17:36:58 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.6008
2022-03-15 17:37:34 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.7372
2022-03-15 17:38:10 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.5249
2022-03-15 17:38:46 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.4575
2022-03-15 17:39:22 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.6748
2022-03-15 17:39:58 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.7173
2022-03-15 17:40:33 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.7294
2022-03-15 17:41:09 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.5671
2022-03-15 17:41:45 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.7832
2022-03-15 17:42:21 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.5963
2022-03-15 17:42:55 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.6255
2022-03-15 17:43:28 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.8710
2022-03-15 17:44:02 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.6780
2022-03-15 17:44:39 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.5760
2022-03-15 17:45:13 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.5808
2022-03-15 17:45:49 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.6589
2022-03-15 17:46:24 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.6499
2022-03-15 17:47:00 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.6218
2022-03-15 17:47:35 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.7094
2022-03-15 17:48:09 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.6111
2022-03-15 17:48:10 - train: epoch 037, train_loss: 1.6405
2022-03-15 17:49:31 - eval: epoch: 037, acc1: 66.210%, acc5: 87.250%, test_loss: 1.3725, per_image_load_time: 0.979ms, per_image_inference_time: 0.323ms
2022-03-15 17:49:31 - until epoch: 037, best_acc1: 66.212%
2022-03-15 17:49:31 - epoch 038 lr: 0.010000000000000002
2022-03-15 17:50:12 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.6587
2022-03-15 17:50:48 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.4007
2022-03-15 17:51:23 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.3449
2022-03-15 17:51:59 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.5466
2022-03-15 17:52:34 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.5385
2022-03-15 17:53:10 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.7712
2022-03-15 17:53:45 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.4512
2022-03-15 17:54:20 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.4692
2022-03-15 17:54:54 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.6060
2022-03-15 17:55:27 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.8413
2022-03-15 17:56:02 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.6663
2022-03-15 17:56:38 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.6996
2022-03-15 17:57:14 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.8185
2022-03-15 17:57:49 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.7727
2022-03-15 17:58:25 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.6377
2022-03-15 17:59:00 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.6837
2022-03-15 17:59:36 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.8012
2022-03-15 18:00:11 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.7135
2022-03-15 18:00:47 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.6580
2022-03-15 18:01:22 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.5371
2022-03-15 18:01:58 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.6704
2022-03-15 18:02:33 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.5225
2022-03-15 18:03:09 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.6622
2022-03-15 18:03:45 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.9235
2022-03-15 18:04:23 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.5072
2022-03-15 18:04:59 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.6814
2022-03-15 18:05:36 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.7364
2022-03-15 18:06:12 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.8997
2022-03-15 18:06:50 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.7063
2022-03-15 18:07:24 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.6108
2022-03-15 18:07:57 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.6025
2022-03-15 18:08:35 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.3669
2022-03-15 18:09:11 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.4203
2022-03-15 18:09:47 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.6137
2022-03-15 18:10:24 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.7078
2022-03-15 18:11:01 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.6927
2022-03-15 18:11:38 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.3597
2022-03-15 18:12:15 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.7937
2022-03-15 18:12:51 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.5835
2022-03-15 18:13:27 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.6915
2022-03-15 18:14:04 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.6171
2022-03-15 18:14:41 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.5496
2022-03-15 18:15:18 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.7441
2022-03-15 18:15:54 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.7037
2022-03-15 18:16:30 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.7648
2022-03-15 18:17:06 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.7989
2022-03-15 18:17:41 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.5935
2022-03-15 18:18:17 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.6725
2022-03-15 18:18:52 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.5493
2022-03-15 18:19:26 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.4560
2022-03-15 18:19:27 - train: epoch 038, train_loss: 1.6374
2022-03-15 18:20:45 - eval: epoch: 038, acc1: 65.986%, acc5: 87.276%, test_loss: 1.3713, per_image_load_time: 2.364ms, per_image_inference_time: 0.350ms
2022-03-15 18:20:45 - until epoch: 038, best_acc1: 66.212%
2022-03-15 18:20:45 - epoch 039 lr: 0.010000000000000002
2022-03-15 18:21:27 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.6321
2022-03-15 18:22:04 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.8840
2022-03-15 18:22:38 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.4252
2022-03-15 18:23:15 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.6844
2022-03-15 18:23:51 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.5468
2022-03-15 18:24:27 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.4591
2022-03-15 18:25:03 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.7323
2022-03-15 18:25:39 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.5798
2022-03-15 18:26:14 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.6762
2022-03-15 18:26:51 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.4691
2022-03-15 18:27:26 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.7021
2022-03-15 18:28:02 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.7231
2022-03-15 18:28:37 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.8438
2022-03-15 18:29:12 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.6473
2022-03-15 18:29:47 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.4861
2022-03-15 18:30:23 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.7492
2022-03-15 18:30:59 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.4078
2022-03-15 18:31:33 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.7285
2022-03-15 18:32:07 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.4311
2022-03-15 18:32:43 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.5240
2022-03-15 18:33:19 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.6944
2022-03-15 18:33:53 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.5450
2022-03-15 18:34:28 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.8618
2022-03-15 18:35:03 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.7758
2022-03-15 18:35:38 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.4261
2022-03-15 18:36:15 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.6364
2022-03-15 18:36:49 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.7527
2022-03-15 18:37:25 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.5624
2022-03-15 18:37:59 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.4487
2022-03-15 18:38:35 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.6860
2022-03-15 18:39:10 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.5381
2022-03-15 18:39:46 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.6990
2022-03-15 18:40:22 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.6921
2022-03-15 18:40:57 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.6838
2022-03-15 18:41:32 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.8808
2022-03-15 18:42:08 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.7881
2022-03-15 18:42:43 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.5612
2022-03-15 18:43:17 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.4625
2022-03-15 18:43:50 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.7483
2022-03-15 18:44:26 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.6555
2022-03-15 18:45:01 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.6942
2022-03-15 18:45:37 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.6553
2022-03-15 18:46:14 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.6012
2022-03-15 18:46:50 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.4564
2022-03-15 18:47:27 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.4934
2022-03-15 18:48:04 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.8241
2022-03-15 18:48:40 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.6118
2022-03-15 18:49:17 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.6979
2022-03-15 18:49:53 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.5651
2022-03-15 18:50:28 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.5168
2022-03-15 18:50:29 - train: epoch 039, train_loss: 1.6340
2022-03-15 18:51:52 - eval: epoch: 039, acc1: 66.218%, acc5: 87.336%, test_loss: 1.3682, per_image_load_time: 2.786ms, per_image_inference_time: 0.338ms
2022-03-15 18:51:52 - until epoch: 039, best_acc1: 66.218%
2022-03-15 18:51:52 - epoch 040 lr: 0.010000000000000002
2022-03-15 18:52:35 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.8373
2022-03-15 18:53:13 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.9016
2022-03-15 18:53:49 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.7533
2022-03-15 18:54:25 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.6325
2022-03-15 18:55:02 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.5039
2022-03-15 18:55:39 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.7071
2022-03-15 18:56:13 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.5671
2022-03-15 18:56:48 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.8856
2022-03-15 18:57:25 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.5806
2022-03-15 18:58:02 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.3160
2022-03-15 18:58:38 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.5240
2022-03-15 18:59:16 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.5356
2022-03-15 18:59:52 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.5981
2022-03-15 19:00:29 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.5365
2022-03-15 19:01:05 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.8716
2022-03-15 19:01:42 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.6603
2022-03-15 19:02:19 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.7229
2022-03-15 19:02:56 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.4791
2022-03-15 19:03:33 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.6415
2022-03-15 19:04:10 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.6666
2022-03-15 19:04:46 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.5277
2022-03-15 19:05:24 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.4120
2022-03-15 19:06:00 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.5548
2022-03-15 19:06:36 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.7008
2022-03-15 19:07:13 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.7083
2022-03-15 19:07:50 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.5304
2022-03-15 19:08:25 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.7706
2022-03-15 19:08:59 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.6816
2022-03-15 19:09:37 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.9002
2022-03-15 19:10:13 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.7259
2022-03-15 19:10:50 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.5732
2022-03-15 19:11:27 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.7837
2022-03-15 19:12:03 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.6898
2022-03-15 19:12:39 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.5958
2022-03-15 19:13:16 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.6649
2022-03-15 19:13:52 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.7513
2022-03-15 19:14:29 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.6675
2022-03-15 19:15:06 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.4990
2022-03-15 19:15:42 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.7251
2022-03-15 19:16:19 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.7699
2022-03-15 19:16:56 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.7059
2022-03-15 19:17:33 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.5377
2022-03-15 19:18:09 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.6046
2022-03-15 19:18:46 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.6144
2022-03-15 19:19:22 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.4296
2022-03-15 19:19:59 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.6506
2022-03-15 19:20:36 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.6006
2022-03-15 19:21:10 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.5803
2022-03-15 19:21:45 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.7333
2022-03-15 19:22:20 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.7637
2022-03-15 19:22:21 - train: epoch 040, train_loss: 1.6314
2022-03-15 19:23:44 - eval: epoch: 040, acc1: 66.080%, acc5: 87.076%, test_loss: 1.3762, per_image_load_time: 2.876ms, per_image_inference_time: 0.324ms
2022-03-15 19:23:44 - until epoch: 040, best_acc1: 66.218%
2022-03-15 19:23:44 - epoch 041 lr: 0.010000000000000002
2022-03-15 19:24:27 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.7581
2022-03-15 19:25:05 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.8023
2022-03-15 19:25:41 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.5268
2022-03-15 19:26:18 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.6727
2022-03-15 19:26:54 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.4320
2022-03-15 19:27:32 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.7648
2022-03-15 19:28:08 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.5007
2022-03-15 19:28:45 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.3477
2022-03-15 19:29:21 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.3509
2022-03-15 19:29:57 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.8270
2022-03-15 19:30:35 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.5418
2022-03-15 19:31:10 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.3979
2022-03-15 19:31:48 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.6378
2022-03-15 19:32:24 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.7436
2022-03-15 19:33:01 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.9733
2022-03-15 19:33:36 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.4017
2022-03-15 19:34:10 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.6215
2022-03-15 19:34:48 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.5136
2022-03-15 19:35:25 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.7024
2022-03-15 19:36:01 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.6017
2022-03-15 19:36:38 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.4642
2022-03-15 19:37:15 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.4683
2022-03-15 19:37:51 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.3532
2022-03-15 19:38:27 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.8330
2022-03-15 19:39:04 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.5456
2022-03-15 19:39:40 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.8028
2022-03-15 19:40:17 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.8684
2022-03-15 19:40:53 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.6444
2022-03-15 19:41:30 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.7279
2022-03-15 19:42:07 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.7945
2022-03-15 19:42:44 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.7552
2022-03-15 19:43:20 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.5575
2022-03-15 19:43:57 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.4218
2022-03-15 19:44:33 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.5819
2022-03-15 19:45:10 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.6601
2022-03-15 19:45:45 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.6253
2022-03-15 19:46:19 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.6350
2022-03-15 19:46:55 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.4637
2022-03-15 19:47:33 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.7091
2022-03-15 19:48:10 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.4621
2022-03-15 19:48:46 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.6401
2022-03-15 19:49:22 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.5575
2022-03-15 19:50:01 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.6959
2022-03-15 19:50:37 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.5488
2022-03-15 19:51:14 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.5988
2022-03-15 19:51:51 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.6764
2022-03-15 19:52:27 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.7941
2022-03-15 19:53:04 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.6170
2022-03-15 19:53:41 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.6031
2022-03-15 19:54:15 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.7372
2022-03-15 19:54:16 - train: epoch 041, train_loss: 1.6341
2022-03-15 19:55:39 - eval: epoch: 041, acc1: 66.190%, acc5: 87.110%, test_loss: 1.3733, per_image_load_time: 2.844ms, per_image_inference_time: 0.352ms
2022-03-15 19:55:39 - until epoch: 041, best_acc1: 66.218%
2022-03-15 19:55:39 - epoch 042 lr: 0.010000000000000002
2022-03-15 19:56:23 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.4535
2022-03-15 19:56:59 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.5918
2022-03-15 19:57:34 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.6457
2022-03-15 19:58:10 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.3519
2022-03-15 19:58:43 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.7056
2022-03-15 19:59:20 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.4778
2022-03-15 19:59:56 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.7616
2022-03-15 20:00:34 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.4668
2022-03-15 20:01:10 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.7979
2022-03-15 20:01:46 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.7939
2022-03-15 20:02:23 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.4021
2022-03-15 20:02:59 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.5340
2022-03-15 20:03:37 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.6253
2022-03-15 20:04:13 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.9302
2022-03-15 20:04:51 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.5544
2022-03-15 20:05:28 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.6268
2022-03-15 20:06:03 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.6558
2022-03-15 20:06:40 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.6362
2022-03-15 20:07:16 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.7811
2022-03-15 20:07:53 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.5096
2022-03-15 20:08:30 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.3283
2022-03-15 20:09:06 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.4501
2022-03-15 20:09:43 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.5391
2022-03-15 20:10:20 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.8398
2022-03-15 20:10:54 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.7737
2022-03-15 20:11:28 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.7386
2022-03-15 20:12:07 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.4852
2022-03-15 20:12:43 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.5158
2022-03-15 20:13:19 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.6881
2022-03-15 20:13:57 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.6429
2022-03-15 20:14:34 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.5900
2022-03-15 20:15:10 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.7198
2022-03-15 20:15:47 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.8284
2022-03-15 20:16:24 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.4565
2022-03-15 20:17:00 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.5663
2022-03-15 20:17:37 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.7867
2022-03-15 20:18:13 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.6254
2022-03-15 20:18:50 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.4670
2022-03-15 20:19:26 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.4893
2022-03-15 20:20:03 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.8007
2022-03-15 20:20:39 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.8147
2022-03-15 20:21:15 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.6735
2022-03-15 20:21:52 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.4794
2022-03-15 20:22:28 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.5059
2022-03-15 20:23:04 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.6241
2022-03-15 20:23:38 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.8778
2022-03-15 20:24:11 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.5113
2022-03-15 20:24:48 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.6965
2022-03-15 20:25:24 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.6905
2022-03-15 20:25:58 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.5676
2022-03-15 20:25:59 - train: epoch 042, train_loss: 1.6320
2022-03-15 20:27:21 - eval: epoch: 042, acc1: 65.788%, acc5: 87.144%, test_loss: 1.3840, per_image_load_time: 1.063ms, per_image_inference_time: 0.351ms
2022-03-15 20:27:21 - until epoch: 042, best_acc1: 66.218%
2022-03-15 20:27:21 - epoch 043 lr: 0.010000000000000002
2022-03-15 20:28:04 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.5881
2022-03-15 20:28:41 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.7295
2022-03-15 20:29:18 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.4219
2022-03-15 20:29:55 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.5996
2022-03-15 20:30:31 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.6072
2022-03-15 20:31:08 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.6384
2022-03-15 20:31:45 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.6564
2022-03-15 20:32:21 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.8352
2022-03-15 20:32:58 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.6820
2022-03-15 20:33:36 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.7411
2022-03-15 20:34:12 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.7158
2022-03-15 20:34:49 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.7654
2022-03-15 20:35:26 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.6837
2022-03-15 20:36:02 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.6138
2022-03-15 20:36:35 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.4898
2022-03-15 20:37:10 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.6489
2022-03-15 20:37:46 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.6491
2022-03-15 20:38:21 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.7555
2022-03-15 20:38:57 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.6595
2022-03-15 20:39:32 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.3967
2022-03-15 20:40:07 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.6023
2022-03-15 20:40:43 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.7317
2022-03-15 20:41:18 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.7650
2022-03-15 20:41:54 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.4908
2022-03-15 20:42:29 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.7716
2022-03-15 20:43:06 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.3529
2022-03-15 20:43:41 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.6203
2022-03-15 20:44:18 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.4418
2022-03-15 20:44:53 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.7149
2022-03-15 20:45:29 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.8750
2022-03-15 20:46:06 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.8251
2022-03-15 20:46:41 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.7552
2022-03-15 20:47:18 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.6752
2022-03-15 20:47:54 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.6775
2022-03-15 20:48:29 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.7144
2022-03-15 20:49:02 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.6813
2022-03-15 20:49:38 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.6253
2022-03-15 20:50:14 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.7570
2022-03-15 20:50:50 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.7770
2022-03-15 20:51:26 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.7603
2022-03-15 20:52:02 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.8291
2022-03-15 20:52:38 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.6774
2022-03-15 20:53:14 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.6456
2022-03-15 20:53:50 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.6367
2022-03-15 20:54:26 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.6407
2022-03-15 20:55:02 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.6152
2022-03-15 20:55:40 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.6362
2022-03-15 20:56:15 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.6232
2022-03-15 20:56:52 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.5563
2022-03-15 20:57:26 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.5552
2022-03-15 20:57:27 - train: epoch 043, train_loss: 1.6299
2022-03-15 20:58:48 - eval: epoch: 043, acc1: 66.156%, acc5: 87.316%, test_loss: 1.3667, per_image_load_time: 1.080ms, per_image_inference_time: 0.311ms
2022-03-15 20:58:48 - until epoch: 043, best_acc1: 66.218%
2022-03-15 20:58:48 - epoch 044 lr: 0.010000000000000002
2022-03-15 20:59:31 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.6012
2022-03-15 21:00:08 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.7075
2022-03-15 21:00:43 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.6657
2022-03-15 21:01:18 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.3326
2022-03-15 21:01:51 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.5249
2022-03-15 21:02:28 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.3068
2022-03-15 21:03:05 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.4443
2022-03-15 21:03:40 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.5512
2022-03-15 21:04:16 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.7025
2022-03-15 21:04:53 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.6686
2022-03-15 21:05:30 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.5044
2022-03-15 21:06:07 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.5655
2022-03-15 21:06:44 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.7286
2022-03-15 21:07:20 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.5436
2022-03-15 21:07:57 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.5834
2022-03-15 21:08:33 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.4926
2022-03-15 21:09:10 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.5381
2022-03-15 21:09:46 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.5803
2022-03-15 21:10:23 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.7571
2022-03-15 21:11:00 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.6931
2022-03-15 21:11:37 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.8037
2022-03-15 21:12:13 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.6497
2022-03-15 21:12:50 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.7735
2022-03-15 21:13:26 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.7634
2022-03-15 21:14:01 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.7507
2022-03-15 21:14:34 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.7217
2022-03-15 21:15:11 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.7843
2022-03-15 21:15:46 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.3579
2022-03-15 21:16:24 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.4839
2022-03-15 21:16:59 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.5093
2022-03-15 21:17:35 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.6364
2022-03-15 21:18:11 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.3809
2022-03-15 21:18:48 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.6349
2022-03-15 21:19:24 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.7682
2022-03-15 21:20:01 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.5613
2022-03-15 21:20:37 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.7236
2022-03-15 21:21:13 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.7276
2022-03-15 21:21:50 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.3177
2022-03-15 21:22:26 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.7563
2022-03-15 21:23:03 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.8145
2022-03-15 21:23:40 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.5386
2022-03-15 21:24:15 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.7040
2022-03-15 21:24:52 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.7491
2022-03-15 21:25:28 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.4343
2022-03-15 21:26:05 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.7842
2022-03-15 21:26:39 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.5724
2022-03-15 21:27:13 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.6874
2022-03-15 21:27:49 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.7289
2022-03-15 21:28:26 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.4660
2022-03-15 21:29:00 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.6304
2022-03-15 21:29:01 - train: epoch 044, train_loss: 1.6306
2022-03-15 21:30:23 - eval: epoch: 044, acc1: 65.852%, acc5: 87.064%, test_loss: 1.3837, per_image_load_time: 1.548ms, per_image_inference_time: 0.356ms
2022-03-15 21:30:24 - until epoch: 044, best_acc1: 66.218%
2022-03-15 21:30:24 - epoch 045 lr: 0.010000000000000002
2022-03-15 21:31:07 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.4172
2022-03-15 21:31:44 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.5860
2022-03-15 21:32:21 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.7116
2022-03-15 21:32:56 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.3884
2022-03-15 21:33:32 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.7785
2022-03-15 21:34:08 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.6600
2022-03-15 21:34:45 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.2986
2022-03-15 21:35:21 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.5749
2022-03-15 21:35:58 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.5550
2022-03-15 21:36:34 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.5961
2022-03-15 21:37:11 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.7817
2022-03-15 21:37:47 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.6673
2022-03-15 21:38:23 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.6545
2022-03-15 21:39:00 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.6477
2022-03-15 21:39:33 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.6902
2022-03-15 21:40:08 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.5904
2022-03-15 21:40:44 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.5249
2022-03-15 21:41:20 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.4726
2022-03-15 21:41:55 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.7394
2022-03-15 21:42:31 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.8428
2022-03-15 21:43:08 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.7017
2022-03-15 21:43:44 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.7042
2022-03-15 21:44:20 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.5203
2022-03-15 21:44:57 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.7822
2022-03-15 21:45:33 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.6335
2022-03-15 21:46:09 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.6109
2022-03-15 21:46:45 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.5331
2022-03-15 21:47:22 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.5867
2022-03-15 21:47:58 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.7742
2022-03-15 21:48:35 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.8503
2022-03-15 21:49:11 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.6135
2022-03-15 21:49:48 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.7222
2022-03-15 21:50:24 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.6278
2022-03-15 21:51:01 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.4362
2022-03-15 21:51:36 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.8406
2022-03-15 21:52:10 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.6940
2022-03-15 21:52:46 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.5795
2022-03-15 21:53:22 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.4299
2022-03-15 21:53:59 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.6757
2022-03-15 21:54:35 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.7450
2022-03-15 21:55:12 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.5104
2022-03-15 21:55:48 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.7695
2022-03-15 21:56:24 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.7358
2022-03-15 21:57:01 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.8009
2022-03-15 21:57:38 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.5112
2022-03-15 21:58:14 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.7893
2022-03-15 21:58:51 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.6965
2022-03-15 21:59:26 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.5042
2022-03-15 22:00:04 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.5942
2022-03-15 22:00:38 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.5260
2022-03-15 22:00:39 - train: epoch 045, train_loss: 1.6260
2022-03-15 22:02:01 - eval: epoch: 045, acc1: 65.792%, acc5: 87.046%, test_loss: 1.3770, per_image_load_time: 0.842ms, per_image_inference_time: 0.354ms
2022-03-15 22:02:01 - until epoch: 045, best_acc1: 66.218%
2022-03-15 22:02:01 - epoch 046 lr: 0.010000000000000002
2022-03-15 22:02:45 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.3231
2022-03-15 22:03:22 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.4371
2022-03-15 22:03:58 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.6534
2022-03-15 22:04:32 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.7603
2022-03-15 22:05:07 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.3769
2022-03-15 22:05:44 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.6310
2022-03-15 22:06:20 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.5384
2022-03-15 22:06:56 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.7897
2022-03-15 22:07:32 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.6290
2022-03-15 22:08:08 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.4678
2022-03-15 22:08:44 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.6451
2022-03-15 22:09:21 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.6362
2022-03-15 22:09:58 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.6347
2022-03-15 22:10:34 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.7737
2022-03-15 22:11:11 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.6265
2022-03-15 22:11:47 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.6161
2022-03-15 22:12:25 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.5489
2022-03-15 22:13:01 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.6124
2022-03-15 22:13:38 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.4421
2022-03-15 22:14:14 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.6785
2022-03-15 22:14:51 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.7092
2022-03-15 22:15:27 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.4775
2022-03-15 22:16:04 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.7727
2022-03-15 22:16:40 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.6191
2022-03-15 22:17:14 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.6375
2022-03-15 22:17:50 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.7986
2022-03-15 22:18:27 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.5381
2022-03-15 22:19:03 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.5893
2022-03-15 22:19:39 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.6542
2022-03-15 22:20:16 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.4636
2022-03-15 22:20:53 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.4304
2022-03-15 22:21:29 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.8684
2022-03-15 22:22:05 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.6541
2022-03-15 22:22:43 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.5671
2022-03-15 22:23:19 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.7058
2022-03-15 22:23:56 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.4861
2022-03-15 22:24:32 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.4629
2022-03-15 22:25:10 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.7997
2022-03-15 22:25:46 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.6789
2022-03-15 22:26:23 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.4563
2022-03-15 22:27:00 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.8282
2022-03-15 22:27:37 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.4222
2022-03-15 22:28:13 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.7573
2022-03-15 22:28:50 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.6570
2022-03-15 22:29:25 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.4512
2022-03-15 22:29:59 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.7662
2022-03-15 22:30:36 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.5403
2022-03-15 22:31:12 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.6398
2022-03-15 22:31:48 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.6945
2022-03-15 22:32:22 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.5446
2022-03-15 22:32:23 - train: epoch 046, train_loss: 1.6259
2022-03-15 22:33:45 - eval: epoch: 046, acc1: 65.936%, acc5: 87.106%, test_loss: 1.3767, per_image_load_time: 1.067ms, per_image_inference_time: 0.352ms
2022-03-15 22:33:45 - until epoch: 046, best_acc1: 66.218%
2022-03-15 22:33:45 - epoch 047 lr: 0.010000000000000002
2022-03-15 22:34:28 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.5142
2022-03-15 22:35:05 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.7049
2022-03-15 22:35:41 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.4790
2022-03-15 22:36:16 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.4687
2022-03-15 22:36:52 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.6924
2022-03-15 22:37:29 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.6564
2022-03-15 22:38:05 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.7131
2022-03-15 22:38:43 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.5509
2022-03-15 22:39:18 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.7580
2022-03-15 22:39:55 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.7009
2022-03-15 22:40:32 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.7177
2022-03-15 22:41:09 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.6359
2022-03-15 22:41:43 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.6412
2022-03-15 22:42:18 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.6620
2022-03-15 22:42:55 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.5244
2022-03-15 22:43:30 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.6115
2022-03-15 22:44:05 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.4383
2022-03-15 22:44:42 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.6384
2022-03-15 22:45:19 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.5478
2022-03-15 22:45:55 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.6000
2022-03-15 22:46:32 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.7941
2022-03-15 22:47:09 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.6836
2022-03-15 22:47:46 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.5343
2022-03-15 22:48:22 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.4821
2022-03-15 22:48:59 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.6724
2022-03-15 22:49:37 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.8575
2022-03-15 22:50:12 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.6026
2022-03-15 22:50:50 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.7788
2022-03-15 22:51:27 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.6177
2022-03-15 22:52:04 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.6774
2022-03-15 22:52:42 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.5742
2022-03-15 22:53:19 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.9638
2022-03-15 22:53:57 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.5088
2022-03-15 22:54:32 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.6790
2022-03-15 22:55:06 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.6483
2022-03-15 22:55:44 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.7862
2022-03-15 22:56:21 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.7593
2022-03-15 22:56:57 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.6158
2022-03-15 22:57:34 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.5009
2022-03-15 22:58:10 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.7292
2022-03-15 22:58:47 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.6724
2022-03-15 22:59:24 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.5817
2022-03-15 23:00:01 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.4241
2022-03-15 23:00:38 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.4878
2022-03-15 23:01:15 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.7079
2022-03-15 23:01:52 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.6035
2022-03-15 23:02:29 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.7433
2022-03-15 23:03:07 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.4424
2022-03-15 23:03:44 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.7312
2022-03-15 23:04:20 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.5050
2022-03-15 23:04:21 - train: epoch 047, train_loss: 1.6252
2022-03-15 23:05:45 - eval: epoch: 047, acc1: 66.020%, acc5: 87.006%, test_loss: 1.3771, per_image_load_time: 2.775ms, per_image_inference_time: 0.368ms
2022-03-15 23:05:45 - until epoch: 047, best_acc1: 66.218%
2022-03-15 23:05:45 - epoch 048 lr: 0.010000000000000002
2022-03-15 23:06:29 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.7435
2022-03-15 23:07:05 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.0267
2022-03-15 23:07:41 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.6152
2022-03-15 23:08:14 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.5947
2022-03-15 23:08:53 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.4891
2022-03-15 23:09:28 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.6568
2022-03-15 23:10:06 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.6598
2022-03-15 23:10:42 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.6752
2022-03-15 23:11:20 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.9145
2022-03-15 23:11:55 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.7048
2022-03-15 23:12:33 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.6743
2022-03-15 23:13:10 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.7192
2022-03-15 23:13:46 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.4139
2022-03-15 23:14:24 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.5505
2022-03-15 23:15:01 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.6868
2022-03-15 23:15:38 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.6387
2022-03-15 23:16:16 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.8017
2022-03-15 23:16:54 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.7042
2022-03-15 23:17:30 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.7625
2022-03-15 23:18:08 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.9010
2022-03-15 23:18:45 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.5757
2022-03-15 23:19:22 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.7622
2022-03-15 23:19:59 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.4439
2022-03-15 23:20:34 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.7351
2022-03-15 23:21:08 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.5999
2022-03-15 23:21:43 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.8376
2022-03-15 23:22:19 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.8483
2022-03-15 23:22:54 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.5460
2022-03-15 23:23:30 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.7881
2022-03-15 23:24:06 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.6446
