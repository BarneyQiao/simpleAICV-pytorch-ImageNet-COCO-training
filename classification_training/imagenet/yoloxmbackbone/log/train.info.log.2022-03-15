2022-03-15 23:24:42 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.5710
2022-03-15 23:25:18 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.4786
2022-03-15 23:25:54 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.6830
2022-03-15 23:26:30 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.5350
2022-03-15 23:27:06 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.8317
2022-03-15 23:27:42 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.6321
2022-03-15 23:28:19 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.7503
2022-03-15 23:28:55 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.5782
2022-03-15 23:29:31 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.6897
2022-03-15 23:30:07 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.4052
2022-03-15 23:30:44 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.7925
2022-03-15 23:31:21 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.6051
2022-03-15 23:31:56 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.6310
2022-03-15 23:32:33 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.4621
2022-03-15 23:33:10 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.6445
2022-03-15 23:33:43 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.6270
2022-03-15 23:34:17 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.6933
2022-03-15 23:34:53 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.9049
2022-03-15 23:35:30 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.7739
2022-03-15 23:36:03 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.6519
2022-03-15 23:36:04 - train: epoch 048, train_loss: 1.6258
2022-03-15 23:37:26 - eval: epoch: 048, acc1: 66.248%, acc5: 87.244%, test_loss: 1.3664, per_image_load_time: 2.780ms, per_image_inference_time: 0.338ms
2022-03-15 23:37:26 - until epoch: 048, best_acc1: 66.248%
2022-03-15 23:37:26 - epoch 049 lr: 0.010000000000000002
2022-03-15 23:38:09 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.8513
2022-03-15 23:38:45 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.5242
2022-03-15 23:39:21 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.6793
2022-03-15 23:39:57 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.7065
2022-03-15 23:40:34 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.5202
2022-03-15 23:41:09 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.5276
2022-03-15 23:41:45 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.6637
2022-03-15 23:42:22 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 2.0012
2022-03-15 23:42:58 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.4980
2022-03-15 23:43:34 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.6595
2022-03-15 23:44:12 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.5642
2022-03-15 23:44:47 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.4082
2022-03-15 23:45:24 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.8058
2022-03-15 23:45:59 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.7246
2022-03-15 23:46:33 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.5686
2022-03-15 23:47:09 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.7796
2022-03-15 23:47:47 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.7213
2022-03-15 23:48:24 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.5409
2022-03-15 23:49:00 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.5668
2022-03-15 23:49:37 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.5410
2022-03-15 23:50:13 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.5162
2022-03-15 23:50:51 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.5641
2022-03-15 23:51:28 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.6166
2022-03-15 23:52:04 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.7341
2022-03-15 23:52:41 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.6215
2022-03-15 23:53:19 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.6774
2022-03-15 23:53:55 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.4035
2022-03-15 23:54:32 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.5617
2022-03-15 23:55:08 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.7620
2022-03-15 23:55:44 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.6861
2022-03-15 23:56:21 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.6812
2022-03-15 23:56:58 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.7936
2022-03-15 23:57:35 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.6237
2022-03-15 23:58:11 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.7296
2022-03-15 23:58:47 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.7381
2022-03-15 23:59:20 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.7751
2022-03-15 23:59:58 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.6329
2022-03-16 00:00:35 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.7307
2022-03-16 00:01:11 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.8523
2022-03-16 00:01:49 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.5488
2022-03-16 00:02:25 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.6370
2022-03-16 00:03:02 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.6692
2022-03-16 00:03:39 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.7884
2022-03-16 00:04:16 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.6950
2022-03-16 00:04:52 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.6935
2022-03-16 00:05:29 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.7812
2022-03-16 00:06:06 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.8205
2022-03-16 00:06:43 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.4464
2022-03-16 00:07:20 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.4726
2022-03-16 00:07:55 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.5720
2022-03-16 00:07:56 - train: epoch 049, train_loss: 1.6201
2022-03-16 00:09:19 - eval: epoch: 049, acc1: 66.122%, acc5: 86.932%, test_loss: 1.3774, per_image_load_time: 2.892ms, per_image_inference_time: 0.348ms
2022-03-16 00:09:19 - until epoch: 049, best_acc1: 66.248%
2022-03-16 00:09:19 - epoch 050 lr: 0.010000000000000002
2022-03-16 00:10:02 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.8749
2022-03-16 00:10:39 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.5527
2022-03-16 00:11:14 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.5384
2022-03-16 00:11:47 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.5016
2022-03-16 00:12:24 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.5977
2022-03-16 00:13:00 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.6989
2022-03-16 00:13:37 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.4309
2022-03-16 00:14:13 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.3940
2022-03-16 00:14:50 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.4400
2022-03-16 00:15:27 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.7577
2022-03-16 00:16:03 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.7419
2022-03-16 00:16:41 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.5088
2022-03-16 00:17:17 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.5072
2022-03-16 00:17:55 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.6792
2022-03-16 00:18:32 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.6521
2022-03-16 00:19:08 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.6012
2022-03-16 00:19:45 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.6735
2022-03-16 00:20:22 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.5820
2022-03-16 00:20:59 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.6095
2022-03-16 00:21:37 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.5856
2022-03-16 00:22:13 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.3228
2022-03-16 00:22:50 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.7145
2022-03-16 00:23:27 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.5682
2022-03-16 00:24:02 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.6193
2022-03-16 00:24:36 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.7095
2022-03-16 00:25:14 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.5374
2022-03-16 00:25:50 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.7126
2022-03-16 00:26:26 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.8552
2022-03-16 00:27:03 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.8377
2022-03-16 00:27:40 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.7810
2022-03-16 00:28:17 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.5801
2022-03-16 00:28:54 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.6410
2022-03-16 00:29:31 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.5245
2022-03-16 00:30:07 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.4957
2022-03-16 00:30:45 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.5878
2022-03-16 00:31:21 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.6161
2022-03-16 00:31:59 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.7266
2022-03-16 00:32:34 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.4622
2022-03-16 00:33:11 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.4250
2022-03-16 00:33:48 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.6799
2022-03-16 00:34:25 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.5697
2022-03-16 00:35:02 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.6205
2022-03-16 00:35:39 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.6496
2022-03-16 00:36:15 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.5591
2022-03-16 00:36:50 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.5241
2022-03-16 00:37:26 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.5713
2022-03-16 00:38:02 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.6457
2022-03-16 00:38:40 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.3545
2022-03-16 00:39:16 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.4404
2022-03-16 00:39:51 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.5483
2022-03-16 00:39:52 - train: epoch 050, train_loss: 1.6174
2022-03-16 00:41:16 - eval: epoch: 050, acc1: 66.116%, acc5: 87.358%, test_loss: 1.3665, per_image_load_time: 2.869ms, per_image_inference_time: 0.360ms
2022-03-16 00:41:16 - until epoch: 050, best_acc1: 66.248%
2022-03-16 00:41:16 - epoch 051 lr: 0.010000000000000002
2022-03-16 00:42:00 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.8110
2022-03-16 00:42:36 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.9772
2022-03-16 00:43:13 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.6297
2022-03-16 00:43:49 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.3442
2022-03-16 00:44:27 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.6854
2022-03-16 00:45:04 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.5242
2022-03-16 00:45:40 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.5612
2022-03-16 00:46:16 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.8996
2022-03-16 00:46:54 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.5100
2022-03-16 00:47:30 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.8988
2022-03-16 00:48:07 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.7629
2022-03-16 00:48:42 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.5169
2022-03-16 00:49:17 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.2961
2022-03-16 00:49:53 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.4523
2022-03-16 00:50:30 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.5300
2022-03-16 00:51:06 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.5377
2022-03-16 00:51:44 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.6194
2022-03-16 00:52:20 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.6135
2022-03-16 00:52:57 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.4152
2022-03-16 00:53:33 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.5724
2022-03-16 00:54:11 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.5377
2022-03-16 00:54:47 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.5634
2022-03-16 00:55:24 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.6790
2022-03-16 00:56:01 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.7160
2022-03-16 00:56:38 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.5273
2022-03-16 00:57:15 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.4433
2022-03-16 00:57:52 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.6997
2022-03-16 00:58:29 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.5455
2022-03-16 00:59:06 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.5903
2022-03-16 00:59:42 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.5196
2022-03-16 01:00:19 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.4909
2022-03-16 01:00:55 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.6057
2022-03-16 01:01:31 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.6653
2022-03-16 01:02:05 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.6564
2022-03-16 01:02:42 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.5852
2022-03-16 01:03:18 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.5491
2022-03-16 01:03:54 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.7916
2022-03-16 01:04:31 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.6253
2022-03-16 01:05:08 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.6119
2022-03-16 01:05:46 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.5654
2022-03-16 01:06:22 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.7614
2022-03-16 01:06:59 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.9000
2022-03-16 01:07:35 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.5664
2022-03-16 01:08:13 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.6456
2022-03-16 01:08:50 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.4811
2022-03-16 01:09:27 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.7406
2022-03-16 01:10:03 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.6941
2022-03-16 01:10:40 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.6143
2022-03-16 01:11:17 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.7295
2022-03-16 01:11:52 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.5908
2022-03-16 01:11:53 - train: epoch 051, train_loss: 1.6158
2022-03-16 01:13:16 - eval: epoch: 051, acc1: 65.810%, acc5: 87.226%, test_loss: 1.3756, per_image_load_time: 2.912ms, per_image_inference_time: 0.348ms
2022-03-16 01:13:17 - until epoch: 051, best_acc1: 66.248%
2022-03-16 01:13:17 - epoch 052 lr: 0.010000000000000002
2022-03-16 01:14:00 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.4881
2022-03-16 01:14:33 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.6780
2022-03-16 01:15:09 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.7276
2022-03-16 01:15:46 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.6778
2022-03-16 01:16:22 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.6825
2022-03-16 01:16:59 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.4257
2022-03-16 01:17:36 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.7173
2022-03-16 01:18:13 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.6050
2022-03-16 01:18:50 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.6975
2022-03-16 01:19:27 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.7491
2022-03-16 01:20:03 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.5380
2022-03-16 01:20:41 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.4280
2022-03-16 01:21:18 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.5674
2022-03-16 01:21:56 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.9297
2022-03-16 01:22:32 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.4400
2022-03-16 01:23:09 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.3749
2022-03-16 01:23:46 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.4943
2022-03-16 01:24:23 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.3508
2022-03-16 01:25:00 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.6279
2022-03-16 01:25:37 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.6929
2022-03-16 01:26:14 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.4568
2022-03-16 01:26:49 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.6633
2022-03-16 01:27:23 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.4646
2022-03-16 01:28:00 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.4587
2022-03-16 01:28:36 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.4069
2022-03-16 01:29:12 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.4572
2022-03-16 01:29:47 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.5588
2022-03-16 01:30:24 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.6713
2022-03-16 01:31:02 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.4856
2022-03-16 01:31:38 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.5814
2022-03-16 01:32:15 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.6718
2022-03-16 01:32:52 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.7112
2022-03-16 01:33:29 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.5760
2022-03-16 01:34:05 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.6735
2022-03-16 01:34:42 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.7074
2022-03-16 01:35:18 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.6473
2022-03-16 01:35:56 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.8301
2022-03-16 01:36:32 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.5180
2022-03-16 01:37:09 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.5327
2022-03-16 01:37:45 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.8069
2022-03-16 01:38:22 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.3723
2022-03-16 01:38:59 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.5500
2022-03-16 01:39:34 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.6219
2022-03-16 01:40:08 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.7581
2022-03-16 01:40:45 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.5194
2022-03-16 01:41:23 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.7210
2022-03-16 01:41:59 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.5587
2022-03-16 01:42:36 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.4134
2022-03-16 01:43:13 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.5115
2022-03-16 01:43:47 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.6429
2022-03-16 01:43:48 - train: epoch 052, train_loss: 1.6119
2022-03-16 01:45:11 - eval: epoch: 052, acc1: 66.050%, acc5: 87.240%, test_loss: 1.3730, per_image_load_time: 2.877ms, per_image_inference_time: 0.355ms
2022-03-16 01:45:12 - until epoch: 052, best_acc1: 66.248%
2022-03-16 01:45:12 - epoch 053 lr: 0.010000000000000002
2022-03-16 01:45:54 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.6342
2022-03-16 01:46:32 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.8944
2022-03-16 01:47:09 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.6767
2022-03-16 01:47:46 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.6366
2022-03-16 01:48:22 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.8615
2022-03-16 01:48:59 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.6649
2022-03-16 01:49:35 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.5114
2022-03-16 01:50:12 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.7038
2022-03-16 01:50:49 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.5712
2022-03-16 01:51:26 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.5271
2022-03-16 01:51:59 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.6400
2022-03-16 01:52:35 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.5318
2022-03-16 01:53:13 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.6540
2022-03-16 01:53:49 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.8372
2022-03-16 01:54:25 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.6437
2022-03-16 01:55:02 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.8710
2022-03-16 01:55:39 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.8451
2022-03-16 01:56:16 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.6975
2022-03-16 01:56:52 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.3860
2022-03-16 01:57:29 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.6281
2022-03-16 01:58:06 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.6514
2022-03-16 01:58:44 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.4890
2022-03-16 01:59:20 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.6742
2022-03-16 01:59:57 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.5940
2022-03-16 02:00:34 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.8345
2022-03-16 02:01:11 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.7073
2022-03-16 02:01:47 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.8212
2022-03-16 02:02:24 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.7080
2022-03-16 02:03:01 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.3939
2022-03-16 02:03:37 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.4049
2022-03-16 02:04:14 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.7736
2022-03-16 02:04:47 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.9496
2022-03-16 02:05:25 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.5181
2022-03-16 02:06:01 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.7203
2022-03-16 02:06:38 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.6585
2022-03-16 02:07:15 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.7006
2022-03-16 02:07:52 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.8119
2022-03-16 02:08:28 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.5656
2022-03-16 02:09:06 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.7764
2022-03-16 02:09:42 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.5928
2022-03-16 02:10:20 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.7201
2022-03-16 02:10:55 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.6190
2022-03-16 02:11:32 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.8814
2022-03-16 02:12:09 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.4962
2022-03-16 02:12:46 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.6929
2022-03-16 02:13:23 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.5693
2022-03-16 02:13:59 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.7696
2022-03-16 02:14:36 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.7827
2022-03-16 02:15:12 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.5196
2022-03-16 02:15:47 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.6446
2022-03-16 02:15:48 - train: epoch 053, train_loss: 1.6091
2022-03-16 02:17:09 - eval: epoch: 053, acc1: 66.134%, acc5: 87.446%, test_loss: 1.3572, per_image_load_time: 2.713ms, per_image_inference_time: 0.357ms
2022-03-16 02:17:09 - until epoch: 053, best_acc1: 66.248%
2022-03-16 02:17:09 - epoch 054 lr: 0.010000000000000002
2022-03-16 02:17:50 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.4049
2022-03-16 02:18:27 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.8484
2022-03-16 02:19:04 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.6834
2022-03-16 02:19:39 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.4967
2022-03-16 02:20:16 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.6878
2022-03-16 02:20:53 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.4420
2022-03-16 02:21:30 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.6521
2022-03-16 02:22:08 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.7448
2022-03-16 02:22:44 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.3690
2022-03-16 02:23:22 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.3590
2022-03-16 02:23:58 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.4406
2022-03-16 02:24:36 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.7658
2022-03-16 02:25:12 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.6127
2022-03-16 02:25:49 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.6793
2022-03-16 02:26:26 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.5739
2022-03-16 02:27:03 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.1888
2022-03-16 02:27:40 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.5923
2022-03-16 02:28:17 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.5253
2022-03-16 02:28:54 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.9679
2022-03-16 02:29:30 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.6884
2022-03-16 02:30:04 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.3564
2022-03-16 02:30:41 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.5623
2022-03-16 02:31:17 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.5677
2022-03-16 02:31:55 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.4442
2022-03-16 02:32:32 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.6139
2022-03-16 02:33:08 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.6057
2022-03-16 02:33:46 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.6055
2022-03-16 02:34:22 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.9210
2022-03-16 02:34:59 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.4524
2022-03-16 02:35:36 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.6924
2022-03-16 02:36:13 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.6220
2022-03-16 02:36:49 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.8626
2022-03-16 02:37:27 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.5496
2022-03-16 02:38:03 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.6378
2022-03-16 02:38:41 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.6586
2022-03-16 02:39:18 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.5174
2022-03-16 02:39:55 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.4016
2022-03-16 02:40:32 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.6460
2022-03-16 02:41:10 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.6286
2022-03-16 02:41:45 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.4873
2022-03-16 02:42:20 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.6484
2022-03-16 02:42:55 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.7030
2022-03-16 02:43:32 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.6977
2022-03-16 02:44:09 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.3844
2022-03-16 02:44:45 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.4755
2022-03-16 02:45:21 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.5431
2022-03-16 02:45:58 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.7815
2022-03-16 02:46:35 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.7171
2022-03-16 02:47:12 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.5232
2022-03-16 02:47:47 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.6935
2022-03-16 02:47:48 - train: epoch 054, train_loss: 1.6086
2022-03-16 02:49:11 - eval: epoch: 054, acc1: 65.986%, acc5: 87.460%, test_loss: 1.3646, per_image_load_time: 2.834ms, per_image_inference_time: 0.346ms
2022-03-16 02:49:12 - until epoch: 054, best_acc1: 66.248%
2022-03-16 02:49:12 - epoch 055 lr: 0.010000000000000002
2022-03-16 02:49:55 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.5745
2022-03-16 02:50:32 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.4911
2022-03-16 02:51:09 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.3828
2022-03-16 02:51:46 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.5757
2022-03-16 02:52:22 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.4292
2022-03-16 02:52:59 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.5113
2022-03-16 02:53:36 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.7268
2022-03-16 02:54:13 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.3954
2022-03-16 02:54:47 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.5639
2022-03-16 02:55:21 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.5102
2022-03-16 02:55:59 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.6015
2022-03-16 02:56:35 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.5634
2022-03-16 02:57:12 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.7479
2022-03-16 02:57:48 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.5211
2022-03-16 02:58:26 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.5979
2022-03-16 02:59:03 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.7731
2022-03-16 02:59:39 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.7671
2022-03-16 03:00:17 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.7538
2022-03-16 03:00:54 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.7131
2022-03-16 03:01:31 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.5823
2022-03-16 03:02:07 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.3133
2022-03-16 03:02:45 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.7394
2022-03-16 03:03:22 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.5152
2022-03-16 03:03:59 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.4516
2022-03-16 03:04:35 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.6831
2022-03-16 03:05:12 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.5394
2022-03-16 03:05:49 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.4795
2022-03-16 03:06:26 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.6118
2022-03-16 03:07:03 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.7088
2022-03-16 03:07:38 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.4530
2022-03-16 03:08:12 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.6359
2022-03-16 03:08:51 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.6490
2022-03-16 03:09:27 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.3649
2022-03-16 03:10:04 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.4911
2022-03-16 03:10:41 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.3531
2022-03-16 03:11:18 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.6997
2022-03-16 03:11:55 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.5668
2022-03-16 03:12:32 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.6127
2022-03-16 03:13:09 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.8967
2022-03-16 03:13:46 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.6732
2022-03-16 03:14:22 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.5833
2022-03-16 03:15:00 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.5565
2022-03-16 03:15:37 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.6766
2022-03-16 03:16:14 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.8167
2022-03-16 03:16:51 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.6026
2022-03-16 03:17:28 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.6554
2022-03-16 03:18:05 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.4100
2022-03-16 03:18:42 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.7232
2022-03-16 03:19:19 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.5238
2022-03-16 03:19:54 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.7302
2022-03-16 03:19:55 - train: epoch 055, train_loss: 1.6033
2022-03-16 03:21:13 - eval: epoch: 055, acc1: 66.528%, acc5: 87.280%, test_loss: 1.3587, per_image_load_time: 2.235ms, per_image_inference_time: 0.338ms
2022-03-16 03:21:14 - until epoch: 055, best_acc1: 66.528%
2022-03-16 03:21:14 - epoch 056 lr: 0.010000000000000002
2022-03-16 03:21:57 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.6730
2022-03-16 03:22:35 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.8180
2022-03-16 03:23:10 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.5426
2022-03-16 03:23:47 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.4978
2022-03-16 03:24:24 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.5164
2022-03-16 03:25:01 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.6586
2022-03-16 03:25:38 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.6429
2022-03-16 03:26:15 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.7871
2022-03-16 03:26:51 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.6422
2022-03-16 03:27:27 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.5290
2022-03-16 03:28:05 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.4506
2022-03-16 03:28:41 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.5798
2022-03-16 03:29:18 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.7385
2022-03-16 03:29:55 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.5700
2022-03-16 03:30:32 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.9164
2022-03-16 03:31:08 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.4380
2022-03-16 03:31:46 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.7367
2022-03-16 03:32:22 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.8455
2022-03-16 03:32:56 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.6551
2022-03-16 03:33:31 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.5406
2022-03-16 03:34:09 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.6035
2022-03-16 03:34:46 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.6791
2022-03-16 03:35:22 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.7671
2022-03-16 03:35:59 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.5798
2022-03-16 03:36:36 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.7330
2022-03-16 03:37:13 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.5620
2022-03-16 03:37:49 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.6219
2022-03-16 03:38:26 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.6230
2022-03-16 03:39:02 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.7714
2022-03-16 03:39:40 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.8251
2022-03-16 03:40:16 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.4478
2022-03-16 03:40:54 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.4188
2022-03-16 03:41:30 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.8490
2022-03-16 03:42:07 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.4952
2022-03-16 03:42:44 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.6347
2022-03-16 03:43:21 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.3030
2022-03-16 03:43:57 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.5784
2022-03-16 03:44:35 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.3873
2022-03-16 03:45:11 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.7923
2022-03-16 03:45:45 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.5584
2022-03-16 03:46:22 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.7582
2022-03-16 03:46:59 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.6497
2022-03-16 03:47:36 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.5130
2022-03-16 03:48:13 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.7526
2022-03-16 03:48:50 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.5424
2022-03-16 03:49:26 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.6191
2022-03-16 03:50:03 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.6722
2022-03-16 03:50:40 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.7165
2022-03-16 03:51:17 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.5928
2022-03-16 03:51:53 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.7627
2022-03-16 03:51:54 - train: epoch 056, train_loss: 1.5992
2022-03-16 03:53:16 - eval: epoch: 056, acc1: 66.454%, acc5: 87.490%, test_loss: 1.3559, per_image_load_time: 2.839ms, per_image_inference_time: 0.364ms
2022-03-16 03:53:17 - until epoch: 056, best_acc1: 66.528%
2022-03-16 03:53:17 - epoch 057 lr: 0.010000000000000002
2022-03-16 03:54:01 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.6185
2022-03-16 03:54:38 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.5767
2022-03-16 03:55:14 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.4909
2022-03-16 03:55:52 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.5894
2022-03-16 03:56:29 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.3755
2022-03-16 03:57:05 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.8414
2022-03-16 03:57:41 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.3091
2022-03-16 03:58:14 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.5419
2022-03-16 03:58:51 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.6324
2022-03-16 03:59:28 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.4838
2022-03-16 04:00:04 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.5533
2022-03-16 04:00:40 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.5470
2022-03-16 04:01:17 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.5974
2022-03-16 04:01:53 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.6748
2022-03-16 04:02:30 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.6872
2022-03-16 04:03:06 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.7357
2022-03-16 04:03:43 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.7486
2022-03-16 04:04:19 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.7242
2022-03-16 04:04:57 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.4727
2022-03-16 04:05:34 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.5817
2022-03-16 04:06:10 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.6773
2022-03-16 04:06:47 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.7065
2022-03-16 04:07:23 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.5329
2022-03-16 04:08:00 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.4583
2022-03-16 04:08:37 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.5078
2022-03-16 04:09:15 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.4193
2022-03-16 04:09:51 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.2692
2022-03-16 04:10:26 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.1787
2022-03-16 04:11:01 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.6623
2022-03-16 04:11:38 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.6470
2022-03-16 04:12:15 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.8366
2022-03-16 04:12:52 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.7446
2022-03-16 04:13:28 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.6358
2022-03-16 04:14:05 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.6197
2022-03-16 04:14:42 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.6809
2022-03-16 04:15:20 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.6142
2022-03-16 04:15:56 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.4693
2022-03-16 04:16:32 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.5363
2022-03-16 04:17:09 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.6625
2022-03-16 04:17:46 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.5342
2022-03-16 04:18:23 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.7110
2022-03-16 04:18:59 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.7771
2022-03-16 04:19:36 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.5266
2022-03-16 04:20:13 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.6341
2022-03-16 04:20:49 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.8079
2022-03-16 04:21:27 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.8315
2022-03-16 04:22:03 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.5145
2022-03-16 04:22:41 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.8737
2022-03-16 04:23:13 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.7231
2022-03-16 04:23:47 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.8532
2022-03-16 04:23:48 - train: epoch 057, train_loss: 1.5977
2022-03-16 04:25:12 - eval: epoch: 057, acc1: 66.470%, acc5: 87.390%, test_loss: 1.3612, per_image_load_time: 2.836ms, per_image_inference_time: 0.351ms
2022-03-16 04:25:12 - until epoch: 057, best_acc1: 66.528%
2022-03-16 04:25:12 - epoch 058 lr: 0.010000000000000002
2022-03-16 04:25:55 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.5913
2022-03-16 04:26:33 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.4129
2022-03-16 04:27:09 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.7187
2022-03-16 04:27:46 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.5755
2022-03-16 04:28:21 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.4249
2022-03-16 04:28:59 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.6786
2022-03-16 04:29:35 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.5146
2022-03-16 04:30:11 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.4931
2022-03-16 04:30:48 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.4743
2022-03-16 04:31:25 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.6732
2022-03-16 04:32:01 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.3007
2022-03-16 04:32:38 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.4935
2022-03-16 04:33:14 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.6960
2022-03-16 04:33:51 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.5588
2022-03-16 04:34:27 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.5798
2022-03-16 04:35:05 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.5697
2022-03-16 04:35:38 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.7713
2022-03-16 04:36:14 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.5617
2022-03-16 04:36:50 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.6531
2022-03-16 04:37:26 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.7532
2022-03-16 04:38:03 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.5019
2022-03-16 04:38:39 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.3951
2022-03-16 04:39:16 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.5698
2022-03-16 04:39:52 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.6069
2022-03-16 04:40:29 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.7614
2022-03-16 04:41:05 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.4774
2022-03-16 04:41:42 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.7226
2022-03-16 04:42:18 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.2866
2022-03-16 04:42:56 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.5139
2022-03-16 04:43:31 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.7514
2022-03-16 04:44:08 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.4013
2022-03-16 04:44:43 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.4217
2022-03-16 04:45:20 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.6360
2022-03-16 04:45:57 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.4391
2022-03-16 04:46:34 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.5889
2022-03-16 04:47:10 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.4763
2022-03-16 04:47:45 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.5179
2022-03-16 04:48:18 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.5698
2022-03-16 04:48:57 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.6925
2022-03-16 04:49:32 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.7244
2022-03-16 04:50:09 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.7181
2022-03-16 04:50:45 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.3424
2022-03-16 04:51:20 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.9020
2022-03-16 04:51:57 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.3316
2022-03-16 04:52:33 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.6658
2022-03-16 04:53:09 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.5434
2022-03-16 04:53:47 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.7106
2022-03-16 04:54:23 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.7572
2022-03-16 04:55:00 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.4810
2022-03-16 04:55:33 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.5330
2022-03-16 04:55:34 - train: epoch 058, train_loss: 1.5924
2022-03-16 04:56:56 - eval: epoch: 058, acc1: 66.364%, acc5: 87.482%, test_loss: 1.3532, per_image_load_time: 2.806ms, per_image_inference_time: 0.344ms
2022-03-16 04:56:56 - until epoch: 058, best_acc1: 66.528%
2022-03-16 04:56:56 - epoch 059 lr: 0.010000000000000002
2022-03-16 04:57:39 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.7611
2022-03-16 04:58:16 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.4844
2022-03-16 04:58:52 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.5627
2022-03-16 04:59:28 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.6927
2022-03-16 05:00:05 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.7767
2022-03-16 05:00:37 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.5700
2022-03-16 05:01:15 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.4847
2022-03-16 05:01:52 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.5855
2022-03-16 05:02:29 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.5614
2022-03-16 05:03:06 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.5761
2022-03-16 05:03:43 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 1.7728
2022-03-16 05:04:20 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.3784
2022-03-16 05:04:57 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.8165
2022-03-16 05:05:33 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.7237
2022-03-16 05:06:10 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.6860
2022-03-16 05:06:47 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.4735
2022-03-16 05:07:24 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.6665
2022-03-16 05:08:00 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.6038
2022-03-16 05:08:38 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.6259
2022-03-16 05:09:13 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.5171
2022-03-16 05:09:51 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.8921
2022-03-16 05:10:28 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.7557
2022-03-16 05:11:05 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.5321
2022-03-16 05:11:42 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.6824
2022-03-16 05:12:18 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.5396
2022-03-16 05:12:51 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.6337
2022-03-16 05:13:27 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.5702
2022-03-16 05:14:04 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.7732
2022-03-16 05:14:41 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.4796
2022-03-16 05:15:19 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 2.0672
2022-03-16 05:15:55 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.4645
2022-03-16 05:16:32 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.5858
2022-03-16 05:17:08 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.5791
2022-03-16 05:17:45 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 2.0142
2022-03-16 05:18:23 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.4820
2022-03-16 05:19:00 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.5812
2022-03-16 05:19:37 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.3959
2022-03-16 05:20:13 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.6042
2022-03-16 05:20:51 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.4394
2022-03-16 05:21:27 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 1.8459
2022-03-16 05:22:04 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.5933
2022-03-16 05:22:40 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.6209
2022-03-16 05:23:17 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.7663
2022-03-16 05:23:53 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.7808
2022-03-16 05:24:30 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.7087
2022-03-16 05:25:05 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.6213
2022-03-16 05:25:38 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.5730
2022-03-16 05:26:16 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.5905
2022-03-16 05:26:53 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.7240
2022-03-16 05:27:28 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.6782
2022-03-16 05:27:29 - train: epoch 059, train_loss: 1.5922
2022-03-16 05:28:52 - eval: epoch: 059, acc1: 66.378%, acc5: 87.454%, test_loss: 1.3559, per_image_load_time: 2.851ms, per_image_inference_time: 0.349ms
2022-03-16 05:28:52 - until epoch: 059, best_acc1: 66.528%
2022-03-16 05:28:52 - epoch 060 lr: 0.010000000000000002
2022-03-16 05:29:35 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.5749
2022-03-16 05:30:13 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.6996
2022-03-16 05:30:48 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.5393
2022-03-16 05:31:24 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.6661
2022-03-16 05:32:01 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.7134
2022-03-16 05:32:38 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.6822
2022-03-16 05:33:15 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.6995
2022-03-16 05:33:52 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.7312
2022-03-16 05:34:29 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.3295
2022-03-16 05:35:06 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.2375
2022-03-16 05:35:42 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.3196
2022-03-16 05:36:20 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.5011
2022-03-16 05:36:57 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.4722
2022-03-16 05:37:32 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.5695
2022-03-16 05:38:05 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.5854
2022-03-16 05:38:43 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.5528
2022-03-16 05:39:19 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.6013
2022-03-16 05:39:56 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.6059
2022-03-16 05:40:32 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.6001
2022-03-16 05:41:10 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.4463
2022-03-16 05:41:46 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.6311
2022-03-16 05:42:23 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.6441
2022-03-16 05:42:59 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.4384
2022-03-16 05:43:36 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.5576
2022-03-16 05:44:12 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.5566
2022-03-16 05:44:49 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.6742
2022-03-16 05:45:25 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.5177
2022-03-16 05:46:01 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.5552
2022-03-16 05:46:38 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.6399
2022-03-16 05:47:15 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.8455
2022-03-16 05:47:53 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.6531
2022-03-16 05:48:29 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.5739
2022-03-16 05:49:06 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.4851
2022-03-16 05:49:43 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.6210
2022-03-16 05:50:17 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.6249
2022-03-16 05:50:51 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.5660
2022-03-16 05:51:29 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.6552
2022-03-16 05:52:05 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.5815
2022-03-16 05:52:42 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.8674
2022-03-16 05:53:17 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.5728
2022-03-16 05:53:53 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.6965
2022-03-16 05:54:30 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.6535
2022-03-16 05:55:06 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.5727
2022-03-16 05:55:43 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.6927
2022-03-16 05:56:20 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.5417
2022-03-16 05:56:56 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.5716
2022-03-16 05:57:33 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.7119
2022-03-16 05:58:10 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.4228
2022-03-16 05:58:46 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.5506
2022-03-16 05:59:21 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.4977
2022-03-16 05:59:22 - train: epoch 060, train_loss: 1.5871
2022-03-16 06:00:45 - eval: epoch: 060, acc1: 66.434%, acc5: 87.446%, test_loss: 1.3541, per_image_load_time: 2.880ms, per_image_inference_time: 0.344ms
2022-03-16 06:00:45 - until epoch: 060, best_acc1: 66.528%
2022-03-16 06:00:45 - epoch 061 lr: 0.0010000000000000002
2022-03-16 06:01:28 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.3614
2022-03-16 06:02:06 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.5107
2022-03-16 06:02:40 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.2982
2022-03-16 06:03:14 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.6898
2022-03-16 06:03:52 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.4750
2022-03-16 06:04:28 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.4667
2022-03-16 06:05:04 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.1654
2022-03-16 06:05:41 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.5598
2022-03-16 06:06:18 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.4029
2022-03-16 06:06:56 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.1499
2022-03-16 06:07:32 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.2097
2022-03-16 06:08:09 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.3893
2022-03-16 06:08:46 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.3596
2022-03-16 06:09:23 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.3248
2022-03-16 06:10:00 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.4881
2022-03-16 06:10:37 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.2048
2022-03-16 06:11:14 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.3957
2022-03-16 06:11:51 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.4524
2022-03-16 06:12:28 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.5485
2022-03-16 06:13:05 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.3270
2022-03-16 06:13:42 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.7838
2022-03-16 06:14:19 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.2238
2022-03-16 06:14:56 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.4208
2022-03-16 06:15:30 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.1102
2022-03-16 06:16:05 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.3160
2022-03-16 06:16:42 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.4925
2022-03-16 06:17:19 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.6016
2022-03-16 06:17:57 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.3855
2022-03-16 06:18:32 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.4979
2022-03-16 06:19:09 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.2887
2022-03-16 06:19:45 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.5797
2022-03-16 06:20:22 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.4143
2022-03-16 06:20:58 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.3847
2022-03-16 06:21:36 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.2649
2022-03-16 06:22:12 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.4329
2022-03-16 06:22:49 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.4519
2022-03-16 06:23:25 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.5205
2022-03-16 06:24:02 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.3844
2022-03-16 06:24:39 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.3216
2022-03-16 06:25:16 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.3817
2022-03-16 06:25:52 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.5905
2022-03-16 06:26:29 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.3268
2022-03-16 06:27:05 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.4640
2022-03-16 06:27:41 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.3877
2022-03-16 06:28:15 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.2935
2022-03-16 06:28:52 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.4959
2022-03-16 06:29:28 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.3256
2022-03-16 06:30:05 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.3870
2022-03-16 06:30:43 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.4762
2022-03-16 06:31:17 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.4658
2022-03-16 06:31:18 - train: epoch 061, train_loss: 1.3988
2022-03-16 06:32:41 - eval: epoch: 061, acc1: 70.264%, acc5: 89.618%, test_loss: 1.1856, per_image_load_time: 2.887ms, per_image_inference_time: 0.335ms
2022-03-16 06:32:41 - until epoch: 061, best_acc1: 70.264%
2022-03-16 06:32:41 - epoch 062 lr: 0.0010000000000000002
2022-03-16 06:33:25 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.3713
2022-03-16 06:34:02 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.4426
2022-03-16 06:34:38 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.3346
2022-03-16 06:35:16 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.1888
2022-03-16 06:35:52 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.2644
2022-03-16 06:36:29 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.3158
2022-03-16 06:37:05 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.6215
2022-03-16 06:37:42 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.2870
2022-03-16 06:38:19 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.2848
2022-03-16 06:38:55 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.4500
2022-03-16 06:39:33 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.2751
2022-03-16 06:40:09 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.5885
2022-03-16 06:40:43 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.4854
2022-03-16 06:41:19 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.3234
2022-03-16 06:41:56 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.4370
2022-03-16 06:42:32 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.5127
2022-03-16 06:43:09 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.4756
2022-03-16 06:43:46 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.2696
2022-03-16 06:44:22 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.4133
2022-03-16 06:44:59 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.3334
2022-03-16 06:45:36 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.4892
2022-03-16 06:46:13 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.2591
2022-03-16 06:46:49 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.2631
2022-03-16 06:47:27 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.3272
2022-03-16 06:48:03 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.3485
2022-03-16 06:48:40 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.2156
2022-03-16 06:49:17 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.2709
2022-03-16 06:49:54 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.4344
2022-03-16 06:50:31 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.4035
2022-03-16 06:51:08 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.5386
2022-03-16 06:51:45 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.4681
2022-03-16 06:52:21 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.1629
2022-03-16 06:52:57 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.4801
2022-03-16 06:53:30 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.4025
2022-03-16 06:54:08 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.4121
2022-03-16 06:54:44 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.4189
2022-03-16 06:55:21 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.3099
2022-03-16 06:55:58 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.3426
2022-03-16 06:56:35 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.3373
2022-03-16 06:57:11 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.1105
2022-03-16 06:57:48 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.5100
2022-03-16 06:58:25 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 1.2643
2022-03-16 06:59:02 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.4080
2022-03-16 06:59:39 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.2543
2022-03-16 07:00:16 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.1625
2022-03-16 07:00:52 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 1.1172
2022-03-16 07:01:28 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.2489
2022-03-16 07:02:05 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.3247
2022-03-16 07:02:42 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.2820
2022-03-16 07:03:17 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.4229
2022-03-16 07:03:18 - train: epoch 062, train_loss: 1.3507
2022-03-16 07:04:41 - eval: epoch: 062, acc1: 70.710%, acc5: 89.828%, test_loss: 1.1697, per_image_load_time: 2.833ms, per_image_inference_time: 0.354ms
2022-03-16 07:04:41 - until epoch: 062, best_acc1: 70.710%
2022-03-16 07:04:41 - epoch 063 lr: 0.0010000000000000002
2022-03-16 07:05:23 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 1.3614
2022-03-16 07:05:56 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.3172
2022-03-16 07:06:34 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.4148
2022-03-16 07:07:12 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.3453
2022-03-16 07:07:48 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.1987
2022-03-16 07:08:25 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.4731
2022-03-16 07:09:02 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.3741
2022-03-16 07:09:39 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.4130
2022-03-16 07:10:15 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.4199
2022-03-16 07:10:52 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.3544
2022-03-16 07:11:28 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.1665
2022-03-16 07:12:05 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.3363
2022-03-16 07:12:41 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.3069
2022-03-16 07:13:19 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.6149
2022-03-16 07:13:55 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.3521
2022-03-16 07:14:33 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.2630
2022-03-16 07:15:09 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.2093
2022-03-16 07:15:47 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.4078
2022-03-16 07:16:23 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.4717
2022-03-16 07:17:01 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 1.2515
2022-03-16 07:17:36 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.2677
2022-03-16 07:18:11 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.6001
2022-03-16 07:18:47 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.4397
2022-03-16 07:19:24 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.5568
2022-03-16 07:20:00 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.3448
2022-03-16 07:20:38 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.2744
2022-03-16 07:21:14 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.5398
2022-03-16 07:21:52 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.3863
2022-03-16 07:22:29 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.4283
2022-03-16 07:23:06 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.3699
2022-03-16 07:23:43 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.6279
2022-03-16 07:24:20 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.4448
2022-03-16 07:24:57 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.2941
2022-03-16 07:25:32 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.1141
2022-03-16 07:26:10 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.4759
2022-03-16 07:26:46 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.2099
2022-03-16 07:27:24 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.4743
2022-03-16 07:28:01 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.5370
2022-03-16 07:28:38 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 1.0461
2022-03-16 07:29:14 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 1.0602
2022-03-16 07:29:51 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.4786
2022-03-16 07:30:25 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.3250
2022-03-16 07:30:59 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.4424
2022-03-16 07:31:38 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.3920
2022-03-16 07:32:14 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.2469
2022-03-16 07:32:52 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.2703
2022-03-16 07:33:27 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.1925
2022-03-16 07:34:05 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.3568
2022-03-16 07:34:41 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.3044
2022-03-16 07:35:16 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.3294
2022-03-16 07:35:17 - train: epoch 063, train_loss: 1.3332
2022-03-16 07:36:40 - eval: epoch: 063, acc1: 70.910%, acc5: 89.976%, test_loss: 1.1628, per_image_load_time: 2.829ms, per_image_inference_time: 0.335ms
2022-03-16 07:36:40 - until epoch: 063, best_acc1: 70.910%
2022-03-16 07:36:40 - epoch 064 lr: 0.0010000000000000002
2022-03-16 07:37:23 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.2692
2022-03-16 07:38:01 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.3179
2022-03-16 07:38:37 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.1886
2022-03-16 07:39:14 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.4341
2022-03-16 07:39:51 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.2188
2022-03-16 07:40:27 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.5429
2022-03-16 07:41:04 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.3899
2022-03-16 07:41:42 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 1.2262
2022-03-16 07:42:18 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.3315
2022-03-16 07:42:53 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.3506
2022-03-16 07:43:27 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.2363
2022-03-16 07:44:01 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.1959
2022-03-16 07:44:33 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.2860
2022-03-16 07:45:07 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.5841
2022-03-16 07:45:41 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.4680
2022-03-16 07:46:15 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.2271
2022-03-16 07:46:48 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.1882
2022-03-16 07:47:22 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.1797
2022-03-16 07:47:55 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.2899
2022-03-16 07:48:28 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.2456
2022-03-16 07:49:02 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 1.3611
2022-03-16 07:49:35 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.3753
2022-03-16 07:50:09 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.4711
2022-03-16 07:50:44 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.3217
2022-03-16 07:51:17 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 1.2147
2022-03-16 07:51:52 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.1996
2022-03-16 07:52:26 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.3343
2022-03-16 07:53:00 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.1242
2022-03-16 07:53:33 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.5476
2022-03-16 07:54:07 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.3428
2022-03-16 07:54:41 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.3032
2022-03-16 07:55:15 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.3434
2022-03-16 07:55:49 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.2976
2022-03-16 07:56:22 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.4612
2022-03-16 07:56:56 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.2433
2022-03-16 07:57:30 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.2796
2022-03-16 07:58:04 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 1.0143
2022-03-16 07:58:39 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.4834
2022-03-16 07:59:12 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.1717
2022-03-16 07:59:46 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.1501
2022-03-16 08:00:19 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.3689
2022-03-16 08:00:53 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 1.2088
2022-03-16 08:01:27 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.5143
2022-03-16 08:02:00 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.2895
2022-03-16 08:02:34 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 1.2532
2022-03-16 08:03:09 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.4873
2022-03-16 08:03:42 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.6016
2022-03-16 08:04:15 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.3280
2022-03-16 08:04:49 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.5500
2022-03-16 08:05:21 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.2324
2022-03-16 08:05:22 - train: epoch 064, train_loss: 1.3226
2022-03-16 08:06:37 - eval: epoch: 064, acc1: 70.972%, acc5: 90.018%, test_loss: 1.1564, per_image_load_time: 2.027ms, per_image_inference_time: 0.358ms
2022-03-16 08:06:38 - until epoch: 064, best_acc1: 70.972%
2022-03-16 08:06:38 - epoch 065 lr: 0.0010000000000000002
2022-03-16 08:07:17 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.4464
2022-03-16 08:07:51 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.3246
2022-03-16 08:08:23 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.3669
2022-03-16 08:08:57 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.4627
2022-03-16 08:09:30 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.3311
2022-03-16 08:10:03 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.4091
2022-03-16 08:10:37 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.2912
2022-03-16 08:11:11 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.2882
2022-03-16 08:11:45 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.1709
2022-03-16 08:12:19 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.2381
2022-03-16 08:12:53 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.2749
2022-03-16 08:13:26 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.4640
2022-03-16 08:13:59 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.2576
2022-03-16 08:14:34 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 1.0730
2022-03-16 08:15:07 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.2623
2022-03-16 08:15:41 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.2919
2022-03-16 08:16:14 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.2847
2022-03-16 08:16:48 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.1970
2022-03-16 08:17:22 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 1.1306
2022-03-16 08:17:55 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.2953
2022-03-16 08:18:29 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.2414
2022-03-16 08:19:03 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.4237
2022-03-16 08:19:37 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.1842
2022-03-16 08:20:10 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.2793
2022-03-16 08:20:44 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.3423
2022-03-16 08:21:18 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.5514
2022-03-16 08:21:52 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.4024
2022-03-16 08:22:25 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.1647
2022-03-16 08:22:59 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.3192
2022-03-16 08:23:33 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.1493
2022-03-16 08:24:06 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.3264
2022-03-16 08:24:40 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.4156
2022-03-16 08:25:13 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.2503
2022-03-16 08:25:47 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.1520
2022-03-16 08:26:21 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.5370
2022-03-16 08:26:55 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.4048
2022-03-16 08:27:28 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.1590
2022-03-16 08:28:02 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.1341
2022-03-16 08:28:36 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.3527
2022-03-16 08:29:10 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.3514
2022-03-16 08:29:44 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.2651
2022-03-16 08:30:17 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.3810
2022-03-16 08:30:51 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.2046
2022-03-16 08:31:25 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.2599
2022-03-16 08:31:59 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.2681
2022-03-16 08:32:33 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.3125
2022-03-16 08:33:07 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.3637
2022-03-16 08:33:40 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 1.1571
2022-03-16 08:34:14 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.2431
2022-03-16 08:34:47 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.4085
2022-03-16 08:34:48 - train: epoch 065, train_loss: 1.3126
2022-03-16 08:36:03 - eval: epoch: 065, acc1: 71.208%, acc5: 90.076%, test_loss: 1.1517, per_image_load_time: 2.517ms, per_image_inference_time: 0.356ms
2022-03-16 08:36:04 - until epoch: 065, best_acc1: 71.208%
2022-03-16 08:36:04 - epoch 066 lr: 0.0010000000000000002
2022-03-16 08:36:43 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.2546
2022-03-16 08:37:16 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.4800
2022-03-16 08:37:50 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 1.0859
2022-03-16 08:38:24 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 1.0414
2022-03-16 08:38:58 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.3914
2022-03-16 08:39:31 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.2450
2022-03-16 08:40:05 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.2661
2022-03-16 08:40:39 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.6551
2022-03-16 08:41:13 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.4019
2022-03-16 08:41:47 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.3998
2022-03-16 08:42:22 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.3384
2022-03-16 08:42:56 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.4417
2022-03-16 08:43:30 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.4914
2022-03-16 08:44:04 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 1.1017
2022-03-16 08:44:38 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.2846
2022-03-16 08:45:12 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.3424
2022-03-16 08:45:46 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.2005
2022-03-16 08:46:20 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.0976
2022-03-16 08:46:54 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.4466
2022-03-16 08:47:27 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.3984
2022-03-16 08:48:01 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.3369
2022-03-16 08:48:34 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 1.2899
2022-03-16 08:49:08 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.4306
2022-03-16 08:49:42 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.2238
2022-03-16 08:50:16 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.2968
2022-03-16 08:50:50 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.2245
2022-03-16 08:51:23 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.5240
2022-03-16 08:51:56 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.4371
2022-03-16 08:52:31 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.2754
2022-03-16 08:53:04 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.2177
2022-03-16 08:53:38 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.4192
2022-03-16 08:54:12 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.2317
2022-03-16 08:54:46 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.1772
2022-03-16 08:55:20 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.4684
2022-03-16 08:55:54 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.4148
2022-03-16 08:56:28 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.3670
2022-03-16 08:57:02 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.3388
2022-03-16 08:57:35 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.1668
2022-03-16 08:58:10 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.1513
2022-03-16 08:58:43 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.3903
2022-03-16 08:59:17 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 1.1362
2022-03-16 08:59:50 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 1.1358
2022-03-16 09:00:24 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.1391
2022-03-16 09:00:58 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.3102
2022-03-16 09:01:31 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.3593
2022-03-16 09:02:05 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.5075
2022-03-16 09:02:39 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.2309
2022-03-16 09:03:13 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.2911
2022-03-16 09:03:46 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.3045
2022-03-16 09:04:18 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 1.2062
2022-03-16 09:04:19 - train: epoch 066, train_loss: 1.3053
2022-03-16 09:05:35 - eval: epoch: 066, acc1: 71.262%, acc5: 90.156%, test_loss: 1.1479, per_image_load_time: 2.459ms, per_image_inference_time: 0.371ms
2022-03-16 09:05:35 - until epoch: 066, best_acc1: 71.262%
2022-03-16 09:05:35 - epoch 067 lr: 0.0010000000000000002
2022-03-16 09:06:14 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.2366
2022-03-16 09:06:48 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.2675
2022-03-16 09:07:21 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.5133
2022-03-16 09:07:54 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.3571
2022-03-16 09:08:28 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.1867
2022-03-16 09:09:02 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 1.2742
2022-03-16 09:09:35 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.3420
2022-03-16 09:10:09 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.3667
2022-03-16 09:10:43 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.4169
2022-03-16 09:11:16 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 1.1689
2022-03-16 09:11:50 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 1.4414
2022-03-16 09:12:23 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.2756
2022-03-16 09:12:57 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.5528
2022-03-16 09:13:30 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.3128
2022-03-16 09:14:05 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.3950
2022-03-16 09:14:37 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.3628
2022-03-16 09:15:11 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.1239
2022-03-16 09:15:45 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.4172
2022-03-16 09:16:19 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.2008
2022-03-16 09:16:52 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.5817
2022-03-16 09:17:26 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 1.1495
2022-03-16 09:18:00 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.3626
2022-03-16 09:18:34 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.2913
2022-03-16 09:19:08 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 1.3014
2022-03-16 09:19:42 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 1.1905
2022-03-16 09:20:15 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.2843
2022-03-16 09:20:49 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.1391
2022-03-16 09:21:23 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.3966
2022-03-16 09:21:57 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.2868
2022-03-16 09:22:30 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.2725
2022-03-16 09:23:04 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 1.1052
2022-03-16 09:23:37 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.4918
2022-03-16 09:24:11 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.1407
2022-03-16 09:24:45 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.3708
2022-03-16 09:25:20 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.2693
2022-03-16 09:25:53 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.3173
2022-03-16 09:26:27 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.5176
2022-03-16 09:27:01 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.3462
2022-03-16 09:27:35 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 1.3376
2022-03-16 09:28:08 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.4174
2022-03-16 09:28:42 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.4302
2022-03-16 09:29:15 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.5247
2022-03-16 09:29:49 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.1596
2022-03-16 09:30:24 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.3560
2022-03-16 09:30:58 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 1.1667
2022-03-16 09:31:32 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.0562
2022-03-16 09:32:06 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.2084
2022-03-16 09:32:39 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.0579
2022-03-16 09:33:14 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.3567
2022-03-16 09:33:46 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.3692
2022-03-16 09:33:47 - train: epoch 067, train_loss: 1.2985
2022-03-16 09:35:02 - eval: epoch: 067, acc1: 71.330%, acc5: 90.166%, test_loss: 1.1469, per_image_load_time: 2.408ms, per_image_inference_time: 0.348ms
2022-03-16 09:35:03 - until epoch: 067, best_acc1: 71.330%
2022-03-16 09:35:03 - epoch 068 lr: 0.0010000000000000002
2022-03-16 09:35:42 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.2945
2022-03-16 09:36:16 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.2917
2022-03-16 09:36:50 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.4297
2022-03-16 09:37:23 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.2415
2022-03-16 09:37:57 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.2473
2022-03-16 09:38:30 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.2578
2022-03-16 09:39:03 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.5577
2022-03-16 09:39:37 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.2994
2022-03-16 09:40:10 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.2325
2022-03-16 09:40:44 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.2929
2022-03-16 09:41:17 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.3865
2022-03-16 09:41:51 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 1.1571
2022-03-16 09:42:24 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.2089
2022-03-16 09:42:58 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.3349
2022-03-16 09:43:31 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.4439
2022-03-16 09:44:05 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.3623
2022-03-16 09:44:39 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.4076
2022-03-16 09:45:13 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.2935
2022-03-16 09:45:46 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.4126
2022-03-16 09:46:20 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.3648
2022-03-16 09:46:54 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.3760
2022-03-16 09:47:27 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.3545
2022-03-16 09:48:01 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 1.1531
2022-03-16 09:48:34 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.3949
2022-03-16 09:49:08 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.3588
2022-03-16 09:49:41 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.3394
2022-03-16 09:50:15 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.3525
2022-03-16 09:50:49 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.4447
2022-03-16 09:51:23 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.4347
2022-03-16 09:51:56 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.4619
2022-03-16 09:52:30 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.1273
2022-03-16 09:53:04 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.3547
2022-03-16 09:53:37 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.2668
2022-03-16 09:54:12 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 1.1262
2022-03-16 09:54:45 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.4061
2022-03-16 09:55:18 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.2543
2022-03-16 09:55:52 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.3587
2022-03-16 09:56:27 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.4142
2022-03-16 09:57:00 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.3548
2022-03-16 09:57:34 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.2803
2022-03-16 09:58:07 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.1405
2022-03-16 09:58:42 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.4445
2022-03-16 09:59:15 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 1.3761
2022-03-16 09:59:49 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.2913
2022-03-16 10:00:23 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.3263
2022-03-16 10:00:56 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.4336
2022-03-16 10:01:30 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.5295
2022-03-16 10:02:03 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.4115
2022-03-16 10:02:38 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.3879
2022-03-16 10:03:09 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.2686
2022-03-16 10:03:10 - train: epoch 068, train_loss: 1.2923
2022-03-16 10:04:25 - eval: epoch: 068, acc1: 71.296%, acc5: 90.162%, test_loss: 1.1421, per_image_load_time: 2.542ms, per_image_inference_time: 0.347ms
2022-03-16 10:04:26 - until epoch: 068, best_acc1: 71.330%
2022-03-16 10:04:26 - epoch 069 lr: 0.0010000000000000002
2022-03-16 10:05:05 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.4756
2022-03-16 10:05:38 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.4859
2022-03-16 10:06:11 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.2418
2022-03-16 10:06:45 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.1966
2022-03-16 10:07:18 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.1948
2022-03-16 10:07:52 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 1.1607
2022-03-16 10:08:25 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.2322
2022-03-16 10:08:58 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.3043
2022-03-16 10:09:32 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.2950
2022-03-16 10:10:05 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.3489
2022-03-16 10:10:38 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.1869
2022-03-16 10:11:12 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.2749
2022-03-16 10:11:46 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.5957
2022-03-16 10:12:19 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.3660
2022-03-16 10:12:53 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.2991
2022-03-16 10:13:26 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.3851
2022-03-16 10:13:59 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.2335
2022-03-16 10:14:32 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.0700
2022-03-16 10:15:06 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.3001
2022-03-16 10:15:39 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 1.1417
2022-03-16 10:16:12 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.2708
2022-03-16 10:16:46 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.2625
2022-03-16 10:17:20 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.3221
2022-03-16 10:17:53 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.3732
2022-03-16 10:18:26 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.2482
2022-03-16 10:19:00 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.3389
2022-03-16 10:19:33 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.5654
2022-03-16 10:20:08 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.4720
2022-03-16 10:20:41 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 1.1997
2022-03-16 10:21:14 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.1595
2022-03-16 10:21:48 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 1.1748
2022-03-16 10:22:22 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 1.1849
2022-03-16 10:22:55 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.2799
2022-03-16 10:23:29 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 1.1366
2022-03-16 10:24:02 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 1.1588
2022-03-16 10:24:36 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.1191
2022-03-16 10:25:10 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.3760
2022-03-16 10:25:43 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.3541
2022-03-16 10:26:18 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.1781
2022-03-16 10:26:51 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.4246
2022-03-16 10:27:25 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.3538
2022-03-16 10:27:58 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.1472
2022-03-16 10:28:32 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.2833
2022-03-16 10:29:05 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.3061
2022-03-16 10:29:40 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.3411
2022-03-16 10:30:13 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.4779
2022-03-16 10:30:47 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.2750
2022-03-16 10:31:21 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.3789
2022-03-16 10:31:55 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 1.1960
2022-03-16 10:32:27 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.2974
2022-03-16 10:32:28 - train: epoch 069, train_loss: 1.2870
2022-03-16 10:33:43 - eval: epoch: 069, acc1: 71.328%, acc5: 90.152%, test_loss: 1.1423, per_image_load_time: 1.851ms, per_image_inference_time: 0.345ms
2022-03-16 10:33:43 - until epoch: 069, best_acc1: 71.330%
2022-03-16 10:33:43 - epoch 070 lr: 0.0010000000000000002
2022-03-16 10:34:22 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.3541
2022-03-16 10:34:56 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.3018
2022-03-16 10:35:29 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.3787
2022-03-16 10:36:03 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.2390
2022-03-16 10:36:37 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.3663
2022-03-16 10:37:11 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.2534
2022-03-16 10:37:44 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.2313
2022-03-16 10:38:17 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.1689
2022-03-16 10:38:51 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.3494
2022-03-16 10:39:25 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.2583
2022-03-16 10:39:59 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.6423
2022-03-16 10:40:32 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 1.1176
2022-03-16 10:41:06 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.2099
2022-03-16 10:41:40 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.2945
2022-03-16 10:42:13 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 1.2066
2022-03-16 10:42:46 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.3398
2022-03-16 10:43:20 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.3952
2022-03-16 10:43:54 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.2151
2022-03-16 10:44:28 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.1970
2022-03-16 10:45:01 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.2924
2022-03-16 10:45:35 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 1.3554
2022-03-16 10:46:09 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.3146
2022-03-16 10:46:42 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.3072
2022-03-16 10:47:15 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.3380
2022-03-16 10:47:49 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.2723
2022-03-16 10:48:23 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.2900
2022-03-16 10:48:56 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.3073
2022-03-16 10:49:31 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.4461
2022-03-16 10:50:04 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.4520
2022-03-16 10:50:38 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.2901
2022-03-16 10:51:11 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.3258
2022-03-16 10:51:45 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.4061
2022-03-16 10:52:18 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 1.2252
2022-03-16 10:52:54 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.3472
2022-03-16 10:53:27 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.3520
2022-03-16 10:54:01 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.2860
2022-03-16 10:54:35 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.3498
2022-03-16 10:55:08 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.2316
2022-03-16 10:55:42 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 1.1516
2022-03-16 10:56:16 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.1563
2022-03-16 10:56:50 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 1.2007
2022-03-16 10:57:24 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.3181
2022-03-16 10:57:58 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.3123
2022-03-16 10:58:32 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.2835
2022-03-16 10:59:06 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.2782
2022-03-16 10:59:40 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.4092
2022-03-16 11:00:14 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.3863
2022-03-16 11:00:48 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.3800
2022-03-16 11:01:20 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.4139
2022-03-16 11:01:52 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.2503
2022-03-16 11:01:53 - train: epoch 070, train_loss: 1.2841
2022-03-16 11:03:08 - eval: epoch: 070, acc1: 71.384%, acc5: 90.248%, test_loss: 1.1376, per_image_load_time: 2.356ms, per_image_inference_time: 0.354ms
2022-03-16 11:03:08 - until epoch: 070, best_acc1: 71.384%
2022-03-16 11:03:08 - epoch 071 lr: 0.0010000000000000002
2022-03-16 11:03:47 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.1728
2022-03-16 11:04:21 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.3099
2022-03-16 11:04:54 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.2357
2022-03-16 11:05:27 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.3271
2022-03-16 11:06:00 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.2347
2022-03-16 11:06:34 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.5037
2022-03-16 11:07:06 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.3828
2022-03-16 11:07:40 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.1960
2022-03-16 11:08:14 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.2908
2022-03-16 11:08:46 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.3477
2022-03-16 11:09:20 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.4712
2022-03-16 11:09:54 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.1959
2022-03-16 11:10:27 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.2755
2022-03-16 11:11:00 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.2867
2022-03-16 11:11:33 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 1.1504
2022-03-16 11:12:07 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 1.1256
2022-03-16 11:12:40 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.2698
2022-03-16 11:13:14 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.1768
2022-03-16 11:13:48 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 1.1574
2022-03-16 11:14:22 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.3477
2022-03-16 11:14:55 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.1331
2022-03-16 11:15:29 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 1.1229
2022-03-16 11:16:03 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 1.2304
2022-03-16 11:16:37 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.2436
2022-03-16 11:17:11 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.3804
2022-03-16 11:17:45 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.2326
2022-03-16 11:18:19 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.3135
2022-03-16 11:18:52 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.3489
2022-03-16 11:19:27 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.2255
2022-03-16 11:20:01 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.3436
2022-03-16 11:20:35 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.2013
2022-03-16 11:21:10 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.0994
2022-03-16 11:21:43 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.2430
2022-03-16 11:22:18 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.2037
2022-03-16 11:22:52 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.3562
2022-03-16 11:23:26 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.4907
2022-03-16 11:24:00 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.1875
2022-03-16 11:24:34 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.2074
2022-03-16 11:25:08 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.3336
2022-03-16 11:25:42 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.4924
2022-03-16 11:26:16 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.2530
2022-03-16 11:26:50 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.4608
2022-03-16 11:27:24 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.2652
2022-03-16 11:27:59 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.3299
2022-03-16 11:28:32 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.2341
2022-03-16 11:29:07 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.3766
2022-03-16 11:29:40 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.1420
2022-03-16 11:30:15 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 1.2404
2022-03-16 11:30:48 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 0.9796
2022-03-16 11:31:21 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.2516
2022-03-16 11:31:22 - train: epoch 071, train_loss: 1.2788
2022-03-16 11:32:38 - eval: epoch: 071, acc1: 71.434%, acc5: 90.302%, test_loss: 1.1361, per_image_load_time: 2.515ms, per_image_inference_time: 0.376ms
2022-03-16 11:32:38 - until epoch: 071, best_acc1: 71.434%
2022-03-16 11:32:38 - epoch 072 lr: 0.0010000000000000002
2022-03-16 11:33:17 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.4360
2022-03-16 11:33:50 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.1639
2022-03-16 11:34:23 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.2224
2022-03-16 11:34:56 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.3468
2022-03-16 11:35:28 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.2111
2022-03-16 11:36:00 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.1839
2022-03-16 11:36:34 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.1822
2022-03-16 11:37:08 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.4440
2022-03-16 11:37:41 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.0904
2022-03-16 11:38:16 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.2231
2022-03-16 11:38:49 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 1.3164
2022-03-16 11:39:23 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 1.1792
2022-03-16 11:39:57 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.3135
2022-03-16 11:40:31 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.3221
2022-03-16 11:41:04 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.2793
2022-03-16 11:41:37 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.2722
2022-03-16 11:42:11 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 1.1798
2022-03-16 11:42:45 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.1741
2022-03-16 11:43:18 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.2080
2022-03-16 11:43:52 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.3942
2022-03-16 11:44:26 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.2748
2022-03-16 11:45:00 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.4329
2022-03-16 11:45:33 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.3720
2022-03-16 11:46:07 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 1.2351
2022-03-16 11:46:41 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.1125
2022-03-16 11:47:15 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 1.1146
2022-03-16 11:47:48 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 1.2749
2022-03-16 11:48:23 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.3620
2022-03-16 11:48:56 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.1916
2022-03-16 11:49:30 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.2439
2022-03-16 11:50:04 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.3537
2022-03-16 11:50:39 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.2217
2022-03-16 11:51:12 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.2561
2022-03-16 11:51:47 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 1.4251
2022-03-16 11:52:20 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 1.2527
2022-03-16 11:52:54 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.2480
2022-03-16 11:53:28 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.3697
2022-03-16 11:54:02 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.3225
2022-03-16 11:54:35 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.3585
2022-03-16 11:55:09 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.2580
2022-03-16 11:55:43 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.4466
2022-03-16 11:56:17 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.2986
2022-03-16 11:56:51 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.3206
2022-03-16 11:57:25 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 1.1789
2022-03-16 11:57:59 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.4111
2022-03-16 11:58:33 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.1565
2022-03-16 11:59:06 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.2978
2022-03-16 11:59:40 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.3996
2022-03-16 12:00:14 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.2634
2022-03-16 12:00:46 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.2729
2022-03-16 12:00:47 - train: epoch 072, train_loss: 1.2776
2022-03-16 12:02:03 - eval: epoch: 072, acc1: 71.498%, acc5: 90.126%, test_loss: 1.1367, per_image_load_time: 2.568ms, per_image_inference_time: 0.352ms
2022-03-16 12:02:03 - until epoch: 072, best_acc1: 71.498%
2022-03-16 12:02:03 - epoch 073 lr: 0.0010000000000000002
2022-03-16 12:02:42 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.5122
2022-03-16 12:03:16 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.3189
2022-03-16 12:03:50 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.4424
2022-03-16 12:04:24 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 0.9965
2022-03-16 12:04:57 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.1885
2022-03-16 12:05:31 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.1617
2022-03-16 12:06:06 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.2951
2022-03-16 12:06:39 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.1896
2022-03-16 12:07:14 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.0702
2022-03-16 12:07:47 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 1.2203
2022-03-16 12:08:21 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.3003
2022-03-16 12:08:55 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.2363
2022-03-16 12:09:28 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.2646
2022-03-16 12:10:02 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.2303
2022-03-16 12:10:36 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.2332
2022-03-16 12:11:10 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.2870
2022-03-16 12:11:44 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.5592
2022-03-16 12:12:17 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 1.0830
2022-03-16 12:12:52 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.3356
2022-03-16 12:13:25 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.0645
2022-03-16 12:13:59 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.2203
2022-03-16 12:14:33 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.3883
2022-03-16 12:15:07 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.2902
2022-03-16 12:15:41 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.2209
2022-03-16 12:16:15 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.4991
2022-03-16 12:16:49 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 1.3891
2022-03-16 12:17:23 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.3606
2022-03-16 12:17:57 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.3828
2022-03-16 12:18:31 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.4677
2022-03-16 12:19:05 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 1.1124
2022-03-16 12:19:39 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.1860
2022-03-16 12:20:13 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 1.1327
2022-03-16 12:20:47 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 1.3151
2022-03-16 12:21:21 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 1.2285
2022-03-16 12:21:55 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.3347
2022-03-16 12:22:29 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 1.1410
2022-03-16 12:23:03 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.3220
2022-03-16 12:23:36 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.4648
2022-03-16 12:24:11 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.2856
2022-03-16 12:24:45 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.3761
2022-03-16 12:25:19 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 1.2211
2022-03-16 12:25:53 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.4620
2022-03-16 12:26:27 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.2624
2022-03-16 12:27:00 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.3181
2022-03-16 12:27:35 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.2294
2022-03-16 12:28:09 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.5367
2022-03-16 12:28:42 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.0816
2022-03-16 12:29:16 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.3970
2022-03-16 12:29:50 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.1423
2022-03-16 12:30:23 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.3664
2022-03-16 12:30:24 - train: epoch 073, train_loss: 1.2736
2022-03-16 12:31:39 - eval: epoch: 073, acc1: 71.390%, acc5: 90.296%, test_loss: 1.1350, per_image_load_time: 2.197ms, per_image_inference_time: 0.363ms
2022-03-16 12:31:39 - until epoch: 073, best_acc1: 71.498%
2022-03-16 12:31:39 - epoch 074 lr: 0.0010000000000000002
2022-03-16 12:32:18 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.1659
2022-03-16 12:32:52 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 1.3472
2022-03-16 12:33:25 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.2963
2022-03-16 12:33:58 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.4776
2022-03-16 12:34:31 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.2951
2022-03-16 12:35:06 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.3521
2022-03-16 12:35:39 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 1.3146
2022-03-16 12:36:12 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.4917
2022-03-16 12:36:45 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.2220
2022-03-16 12:37:19 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.3835
2022-03-16 12:37:52 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.4308
2022-03-16 12:38:25 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 1.1491
2022-03-16 12:38:58 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.3914
2022-03-16 12:39:32 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 1.1793
2022-03-16 12:40:05 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.2615
2022-03-16 12:40:39 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 1.0721
2022-03-16 12:41:11 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.3618
2022-03-16 12:41:45 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.6003
2022-03-16 12:42:18 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.3725
2022-03-16 12:42:51 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 1.0808
2022-03-16 12:43:25 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.2308
2022-03-16 12:43:58 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.7213
2022-03-16 12:44:31 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.4423
2022-03-16 12:45:04 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.2574
2022-03-16 12:45:37 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.1169
2022-03-16 12:46:11 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.0946
2022-03-16 12:46:43 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 1.2701
2022-03-16 12:47:17 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.4796
2022-03-16 12:47:51 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.2278
2022-03-16 12:48:25 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.3525
2022-03-16 12:48:58 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.3172
2022-03-16 12:49:32 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 1.2144
2022-03-16 12:50:05 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.3741
2022-03-16 12:50:39 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.2940
2022-03-16 12:51:13 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.0510
2022-03-16 12:51:47 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 1.2514
2022-03-16 12:52:21 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.4009
2022-03-16 12:52:55 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.2214
2022-03-16 12:53:28 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 1.0225
2022-03-16 12:54:02 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 1.2568
2022-03-16 12:54:35 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.5311
2022-03-16 12:55:10 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.2959
2022-03-16 12:55:44 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.1867
2022-03-16 12:56:18 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 1.1428
2022-03-16 12:56:51 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 1.0914
2022-03-16 12:57:25 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.3710
2022-03-16 12:57:58 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.3165
2022-03-16 12:58:32 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.2999
2022-03-16 12:59:06 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.5125
2022-03-16 12:59:39 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.2259
2022-03-16 12:59:39 - train: epoch 074, train_loss: 1.2695
2022-03-16 13:00:55 - eval: epoch: 074, acc1: 71.582%, acc5: 90.262%, test_loss: 1.1302, per_image_load_time: 2.472ms, per_image_inference_time: 0.365ms
2022-03-16 13:00:55 - until epoch: 074, best_acc1: 71.582%
2022-03-16 13:00:55 - epoch 075 lr: 0.0010000000000000002
2022-03-16 13:01:34 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 1.3883
2022-03-16 13:02:07 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 1.2990
2022-03-16 13:02:42 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.3281
2022-03-16 13:03:15 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.3822
2022-03-16 13:03:48 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.1909
2022-03-16 13:04:22 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.1125
2022-03-16 13:04:55 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.3974
2022-03-16 13:05:29 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.4186
2022-03-16 13:06:03 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.4617
2022-03-16 13:06:35 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 1.0826
2022-03-16 13:07:09 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.4076
2022-03-16 13:07:42 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.1793
2022-03-16 13:08:16 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.1897
2022-03-16 13:08:50 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.1972
2022-03-16 13:09:23 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.3894
2022-03-16 13:09:57 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 0.9577
2022-03-16 13:10:30 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 1.2426
2022-03-16 13:11:04 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.2204
2022-03-16 13:11:37 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 1.1610
2022-03-16 13:12:12 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.2975
2022-03-16 13:12:45 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.2031
2022-03-16 13:13:19 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.3103
2022-03-16 13:13:51 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 1.2376
2022-03-16 13:14:25 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.2435
2022-03-16 13:14:59 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.3152
2022-03-16 13:15:32 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.2809
2022-03-16 13:16:06 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 1.0456
2022-03-16 13:16:40 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.2840
2022-03-16 13:17:14 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.3655
2022-03-16 13:17:47 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.4275
2022-03-16 13:18:21 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.3191
2022-03-16 13:18:54 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.2802
2022-03-16 13:19:29 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.3113
2022-03-16 13:20:02 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 1.1031
2022-03-16 13:20:37 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.1999
2022-03-16 13:21:10 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.3204
2022-03-16 13:21:43 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.2971
2022-03-16 13:22:17 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 1.1708
2022-03-16 13:22:51 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.4193
2022-03-16 13:23:24 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.1758
2022-03-16 13:23:58 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 1.0569
2022-03-16 13:24:32 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.2942
2022-03-16 13:25:06 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.3801
2022-03-16 13:25:40 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 1.0060
2022-03-16 13:26:13 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.3252
2022-03-16 13:26:47 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 1.0833
2022-03-16 13:27:21 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.3931
2022-03-16 13:27:55 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.3130
2022-03-16 13:28:28 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.1270
2022-03-16 13:29:01 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.2704
2022-03-16 13:29:02 - train: epoch 075, train_loss: 1.2673
2022-03-16 13:30:17 - eval: epoch: 075, acc1: 71.596%, acc5: 90.422%, test_loss: 1.1298, per_image_load_time: 2.497ms, per_image_inference_time: 0.356ms
2022-03-16 13:30:18 - until epoch: 075, best_acc1: 71.596%
2022-03-16 13:30:18 - epoch 076 lr: 0.0010000000000000002
2022-03-16 13:30:57 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 1.1132
2022-03-16 13:31:31 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.2828
2022-03-16 13:32:03 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.3473
2022-03-16 13:32:37 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.2988
2022-03-16 13:33:11 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.3320
2022-03-16 13:33:45 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.1536
2022-03-16 13:34:18 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.3268
2022-03-16 13:34:51 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.1743
2022-03-16 13:35:24 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 1.0737
2022-03-16 13:35:58 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.0609
2022-03-16 13:36:31 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.1572
2022-03-16 13:37:05 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.2687
2022-03-16 13:37:38 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.3258
2022-03-16 13:38:12 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 1.1586
2022-03-16 13:38:46 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.1597
2022-03-16 13:39:19 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.2528
2022-03-16 13:39:54 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.3089
2022-03-16 13:40:26 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 1.0402
2022-03-16 13:41:00 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.3549
2022-03-16 13:41:34 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 1.2656
2022-03-16 13:42:08 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.3342
2022-03-16 13:42:41 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.4506
2022-03-16 13:43:14 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 1.2099
2022-03-16 13:43:48 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.3793
2022-03-16 13:44:22 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 1.2780
2022-03-16 13:44:55 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.4883
2022-03-16 13:45:28 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.3733
2022-03-16 13:46:02 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 1.1523
2022-03-16 13:46:36 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 1.3125
2022-03-16 13:47:10 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 1.1200
2022-03-16 13:47:43 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 1.4009
2022-03-16 13:48:18 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.3349
2022-03-16 13:48:51 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.2433
2022-03-16 13:49:25 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.2961
2022-03-16 13:49:57 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 1.1955
2022-03-16 13:50:32 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 1.1826
2022-03-16 13:51:06 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 1.0779
2022-03-16 13:51:40 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 1.1293
2022-03-16 13:52:13 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 1.2040
2022-03-16 13:52:47 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 1.2585
2022-03-16 13:53:21 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 1.1894
2022-03-16 13:53:55 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.4179
2022-03-16 13:54:29 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 1.3491
2022-03-16 13:55:03 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 1.1865
2022-03-16 13:55:36 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 1.2820
2022-03-16 13:56:10 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 1.1425
2022-03-16 13:56:44 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.3035
2022-03-16 13:57:17 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 0.9897
2022-03-16 13:57:51 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 1.2880
2022-03-16 13:58:23 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.2128
2022-03-16 13:58:24 - train: epoch 076, train_loss: 1.2645
2022-03-16 13:59:39 - eval: epoch: 076, acc1: 71.566%, acc5: 90.288%, test_loss: 1.1332, per_image_load_time: 2.248ms, per_image_inference_time: 0.362ms
2022-03-16 13:59:40 - until epoch: 076, best_acc1: 71.596%
2022-03-16 13:59:40 - epoch 077 lr: 0.0010000000000000002
2022-03-16 14:00:19 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 1.3271
2022-03-16 14:00:52 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 1.3946
2022-03-16 14:01:26 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 1.0919
2022-03-16 14:01:59 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.3611
2022-03-16 14:02:33 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 1.1667
2022-03-16 14:03:06 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 1.0753
2022-03-16 14:03:38 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 1.1814
2022-03-16 14:04:12 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.3287
2022-03-16 14:04:46 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.4547
2022-03-16 14:05:19 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 1.0460
2022-03-16 14:05:53 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 1.2984
2022-03-16 14:06:26 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 1.3842
2022-03-16 14:07:00 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 1.0656
2022-03-16 14:07:33 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.3268
2022-03-16 14:08:07 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 1.2273
2022-03-16 14:08:41 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 1.2339
2022-03-16 14:09:14 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.4858
2022-03-16 14:09:48 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 1.1435
2022-03-16 14:10:21 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 1.3484
2022-03-16 14:10:55 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 1.2275
2022-03-16 14:11:29 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 1.2655
2022-03-16 14:12:02 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 1.2457
2022-03-16 14:12:36 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.4358
2022-03-16 14:13:09 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 1.2558
2022-03-16 14:13:42 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.4127
2022-03-16 14:14:16 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 1.0447
2022-03-16 14:14:50 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 1.3563
2022-03-16 14:15:24 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.3991
2022-03-16 14:15:57 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.5662
2022-03-16 14:16:31 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 1.1219
2022-03-16 14:17:05 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 1.2553
2022-03-16 14:17:39 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.3993
2022-03-16 14:18:12 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 1.0634
2022-03-16 14:18:46 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.4260
2022-03-16 14:19:20 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.1651
2022-03-16 14:19:53 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 1.2539
2022-03-16 14:20:26 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 1.2831
2022-03-16 14:21:01 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 1.2322
2022-03-16 14:21:34 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 1.3272
2022-03-16 14:22:08 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 1.2736
2022-03-16 14:22:42 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 1.4089
2022-03-16 14:23:16 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.4090
2022-03-16 14:23:50 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 1.1892
2022-03-16 14:24:23 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 1.2259
2022-03-16 14:24:56 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.3798
2022-03-16 14:25:29 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 1.2124
2022-03-16 14:26:03 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 1.0484
2022-03-16 14:26:37 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 1.2912
2022-03-16 14:27:10 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.4221
2022-03-16 14:27:43 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.4776
2022-03-16 14:27:44 - train: epoch 077, train_loss: 1.2611
2022-03-16 14:28:59 - eval: epoch: 077, acc1: 71.596%, acc5: 90.386%, test_loss: 1.1289, per_image_load_time: 1.965ms, per_image_inference_time: 0.349ms
2022-03-16 14:29:00 - until epoch: 077, best_acc1: 71.596%
2022-03-16 14:29:00 - epoch 078 lr: 0.0010000000000000002
2022-03-16 14:29:38 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 1.2375
2022-03-16 14:30:12 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.3437
2022-03-16 14:30:46 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.3944
2022-03-16 14:31:18 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 1.2582
2022-03-16 14:31:52 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.2131
2022-03-16 14:32:26 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 1.1816
2022-03-16 14:32:59 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.3788
2022-03-16 14:33:32 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.5032
2022-03-16 14:34:05 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 1.3458
2022-03-16 14:34:39 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 1.3660
2022-03-16 14:35:13 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 1.0060
2022-03-16 14:35:46 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 1.0619
2022-03-16 14:36:19 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 1.4176
2022-03-16 14:36:53 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 1.0284
2022-03-16 14:37:26 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 1.2006
2022-03-16 14:38:00 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 1.3343
2022-03-16 14:38:33 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 1.2327
2022-03-16 14:39:07 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 1.1370
2022-03-16 14:39:40 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 1.1342
2022-03-16 14:40:15 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 1.1492
2022-03-16 14:40:48 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.4543
2022-03-16 14:41:23 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 1.2091
2022-03-16 14:41:56 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 1.3327
2022-03-16 14:42:30 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 1.2098
2022-03-16 14:43:03 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.0812
2022-03-16 14:43:37 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 1.1891
2022-03-16 14:44:11 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 1.1701
2022-03-16 14:44:45 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 1.2791
2022-03-16 14:45:18 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 1.1827
2022-03-16 14:45:52 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 1.2579
2022-03-16 14:46:26 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.3181
2022-03-16 14:47:00 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 1.2727
2022-03-16 14:47:33 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.3343
2022-03-16 14:48:07 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 1.2691
2022-03-16 14:48:41 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 1.2410
2022-03-16 14:49:15 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.2833
2022-03-16 14:49:48 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 1.1695
2022-03-16 14:50:23 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 1.3270
2022-03-16 14:50:56 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 1.3124
2022-03-16 14:51:30 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 1.1397
2022-03-16 14:52:04 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.3430
2022-03-16 14:52:37 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.4024
2022-03-16 14:53:10 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 1.3554
2022-03-16 14:53:45 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 1.2455
2022-03-16 14:54:18 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.4001
2022-03-16 14:54:52 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 1.0355
2022-03-16 14:55:27 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 1.3364
2022-03-16 14:56:00 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 1.4754
2022-03-16 14:56:34 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 1.4136
2022-03-16 14:57:06 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 1.3273
2022-03-16 14:57:07 - train: epoch 078, train_loss: 1.2556
2022-03-16 14:58:22 - eval: epoch: 078, acc1: 71.534%, acc5: 90.296%, test_loss: 1.1318, per_image_load_time: 1.267ms, per_image_inference_time: 0.366ms
2022-03-16 14:58:23 - until epoch: 078, best_acc1: 71.596%
2022-03-16 14:58:23 - epoch 079 lr: 0.0010000000000000002
2022-03-16 14:59:01 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 1.1793
2022-03-16 14:59:35 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 1.1607
2022-03-16 15:00:09 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 1.3920
2022-03-16 15:00:42 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 1.3723
2022-03-16 15:01:16 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 1.2663
2022-03-16 15:01:49 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 1.2778
2022-03-16 15:02:21 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 1.1802
2022-03-16 15:02:55 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 1.2528
2022-03-16 15:03:28 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 1.1698
2022-03-16 15:04:02 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 1.1256
2022-03-16 15:04:35 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 1.3913
2022-03-16 15:05:09 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.4715
2022-03-16 15:05:42 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 1.0042
2022-03-16 15:06:15 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 1.2085
2022-03-16 15:06:49 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 1.1177
2022-03-16 15:07:23 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 1.1613
2022-03-16 15:07:57 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 1.1777
2022-03-16 15:08:30 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 1.2341
2022-03-16 15:09:02 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.2919
2022-03-16 15:09:36 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.4037
2022-03-16 15:10:09 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 1.0532
2022-03-16 15:10:44 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 1.2870
2022-03-16 15:11:17 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 1.1420
2022-03-16 15:11:50 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.3136
2022-03-16 15:12:23 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 1.2691
2022-03-16 15:12:57 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 1.1759
2022-03-16 15:13:31 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 1.0168
2022-03-16 15:14:04 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 1.1472
2022-03-16 15:14:37 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 1.1294
2022-03-16 15:15:11 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.2382
2022-03-16 15:15:45 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 1.3168
2022-03-16 15:16:18 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.4308
2022-03-16 15:16:52 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 1.2608
2022-03-16 15:17:25 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 1.0613
2022-03-16 15:18:00 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.3792
2022-03-16 15:18:33 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 1.2220
2022-03-16 15:19:06 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 1.0767
2022-03-16 15:19:40 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 1.2995
2022-03-16 15:20:12 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 1.1193
2022-03-16 15:20:46 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 1.1389
2022-03-16 15:21:19 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.3439
2022-03-16 15:21:53 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.2029
2022-03-16 15:22:27 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 1.0594
2022-03-16 15:23:00 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.4171
2022-03-16 15:23:34 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.5304
2022-03-16 15:24:07 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.4450
2022-03-16 15:24:41 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 1.2573
2022-03-16 15:25:15 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.3617
2022-03-16 15:25:48 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.4348
2022-03-16 15:26:21 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 1.1035
2022-03-16 15:26:22 - train: epoch 079, train_loss: 1.2550
2022-03-16 15:27:36 - eval: epoch: 079, acc1: 71.586%, acc5: 90.356%, test_loss: 1.1276, per_image_load_time: 1.654ms, per_image_inference_time: 0.355ms
2022-03-16 15:27:37 - until epoch: 079, best_acc1: 71.596%
2022-03-16 15:27:37 - epoch 080 lr: 0.0010000000000000002
2022-03-16 15:28:15 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 1.0311
2022-03-16 15:28:48 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 1.2057
2022-03-16 15:29:22 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.3816
2022-03-16 15:29:55 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 1.0833
2022-03-16 15:30:28 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 1.3134
2022-03-16 15:31:01 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 1.2013
2022-03-16 15:31:34 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 1.2001
2022-03-16 15:32:08 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 1.1310
2022-03-16 15:32:41 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 1.2597
2022-03-16 15:33:15 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 1.2135
2022-03-16 15:33:48 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 1.2311
2022-03-16 15:34:21 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 1.2486
2022-03-16 15:34:54 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 1.1579
2022-03-16 15:35:27 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 1.2587
2022-03-16 15:36:00 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 1.1608
2022-03-16 15:36:34 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 1.2163
2022-03-16 15:37:08 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 1.1964
2022-03-16 15:37:42 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 1.3841
2022-03-16 15:38:14 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 1.2005
2022-03-16 15:38:48 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 1.2846
2022-03-16 15:39:22 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 1.3140
2022-03-16 15:39:55 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 1.3490
2022-03-16 15:40:29 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 1.0923
2022-03-16 15:41:02 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 1.2823
2022-03-16 15:41:35 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 1.2178
2022-03-16 15:42:10 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 1.2233
2022-03-16 15:42:43 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 1.3641
2022-03-16 15:43:17 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 1.3585
2022-03-16 15:43:50 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 1.1947
2022-03-16 15:44:24 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 1.2691
2022-03-16 15:44:58 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 1.3972
2022-03-16 15:45:31 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 0.9438
2022-03-16 15:46:04 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 1.2253
2022-03-16 15:46:38 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 1.1206
2022-03-16 15:47:12 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 1.0872
2022-03-16 15:47:45 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 1.2466
2022-03-16 15:48:19 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 1.1943
2022-03-16 15:48:52 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 1.2804
2022-03-16 15:49:26 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 1.1954
2022-03-16 15:50:00 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 1.3470
2022-03-16 15:50:32 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 1.3279
2022-03-16 15:51:06 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 1.2264
2022-03-16 15:51:40 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 1.3845
2022-03-16 15:52:13 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 1.1829
2022-03-16 15:52:47 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 1.1325
2022-03-16 15:53:20 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 1.2533
2022-03-16 15:53:54 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 1.3924
2022-03-16 15:54:27 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 1.2794
2022-03-16 15:55:01 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.4048
2022-03-16 15:55:33 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 1.0701
2022-03-16 15:55:34 - train: epoch 080, train_loss: 1.2534
2022-03-16 15:56:48 - eval: epoch: 080, acc1: 71.606%, acc5: 90.466%, test_loss: 1.1254, per_image_load_time: 1.605ms, per_image_inference_time: 0.391ms
2022-03-16 15:56:48 - until epoch: 080, best_acc1: 71.606%
2022-03-16 15:56:48 - epoch 081 lr: 0.0010000000000000002
2022-03-16 15:57:27 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 1.0997
2022-03-16 15:58:01 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 1.3493
2022-03-16 15:58:34 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 1.2575
2022-03-16 15:59:06 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 1.4265
2022-03-16 15:59:39 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 1.4078
2022-03-16 16:00:13 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 1.3193
2022-03-16 16:00:47 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 1.1836
2022-03-16 16:01:20 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 1.4169
2022-03-16 16:01:53 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 1.1531
2022-03-16 16:02:26 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 1.2428
2022-03-16 16:03:00 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 1.2296
2022-03-16 16:03:33 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 1.3528
2022-03-16 16:04:07 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 1.1819
2022-03-16 16:04:41 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 0.9805
2022-03-16 16:05:14 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 1.3096
2022-03-16 16:05:47 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 1.2004
2022-03-16 16:06:21 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 1.2552
2022-03-16 16:06:54 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 1.1304
2022-03-16 16:07:28 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 1.1973
2022-03-16 16:08:01 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 1.4118
2022-03-16 16:08:35 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 1.2938
2022-03-16 16:09:09 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 1.2952
2022-03-16 16:09:41 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 1.2072
2022-03-16 16:10:15 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 1.2531
2022-03-16 16:10:48 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 1.4135
2022-03-16 16:11:22 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.4114
2022-03-16 16:11:56 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 1.2191
2022-03-16 16:12:29 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 1.1761
2022-03-16 16:13:02 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 1.0921
2022-03-16 16:13:36 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 1.2421
2022-03-16 16:14:09 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 1.0945
2022-03-16 16:14:42 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 1.1934
2022-03-16 16:15:16 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 1.2685
2022-03-16 16:15:49 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 1.4220
2022-03-16 16:16:22 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 1.3186
2022-03-16 16:16:57 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 1.0917
2022-03-16 16:17:30 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 1.3425
2022-03-16 16:18:03 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 1.0661
2022-03-16 16:18:37 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 1.3511
2022-03-16 16:19:10 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 1.0611
2022-03-16 16:19:43 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 1.3158
2022-03-16 16:20:17 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 1.2526
2022-03-16 16:20:50 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 1.1959
2022-03-16 16:21:24 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 1.2978
2022-03-16 16:21:58 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 1.3539
2022-03-16 16:22:31 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 1.2799
2022-03-16 16:23:04 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 1.3393
2022-03-16 16:23:38 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 1.2834
2022-03-16 16:24:11 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 1.1011
2022-03-16 16:24:43 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 0.9939
2022-03-16 16:24:44 - train: epoch 081, train_loss: 1.2519
2022-03-16 16:25:58 - eval: epoch: 081, acc1: 71.498%, acc5: 90.402%, test_loss: 1.1297, per_image_load_time: 1.795ms, per_image_inference_time: 0.361ms
2022-03-16 16:25:58 - until epoch: 081, best_acc1: 71.606%
2022-03-16 16:25:58 - epoch 082 lr: 0.0010000000000000002
2022-03-16 16:26:36 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 1.0158
2022-03-16 16:27:10 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 1.1887
2022-03-16 16:27:43 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 1.3111
2022-03-16 16:28:17 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 1.3953
2022-03-16 16:28:49 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 1.2724
2022-03-16 16:29:23 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 1.0897
2022-03-16 16:29:56 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 1.3287
2022-03-16 16:30:30 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 1.2426
2022-03-16 16:31:04 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 1.4033
2022-03-16 16:31:36 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 1.3230
2022-03-16 16:32:09 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 1.4110
2022-03-16 16:32:44 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 1.2865
2022-03-16 16:33:17 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 1.3608
2022-03-16 16:33:51 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 1.1408
2022-03-16 16:34:24 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 1.1507
2022-03-16 16:34:58 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 1.2095
2022-03-16 16:35:31 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 1.2315
2022-03-16 16:36:04 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 1.1378
2022-03-16 16:36:36 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 1.1021
2022-03-16 16:37:10 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 1.1440
2022-03-16 16:37:44 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 1.3677
2022-03-16 16:38:17 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 1.2927
2022-03-16 16:38:50 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 1.2614
2022-03-16 16:39:22 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 1.3808
2022-03-16 16:39:57 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 1.1799
2022-03-16 16:40:30 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 1.1544
2022-03-16 16:41:03 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 1.1909
2022-03-16 16:41:37 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 1.1484
2022-03-16 16:42:10 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 1.1142
2022-03-16 16:42:43 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 1.2502
2022-03-16 16:43:16 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 1.1615
2022-03-16 16:43:49 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 1.4134
2022-03-16 16:44:22 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 1.2126
2022-03-16 16:44:55 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 1.2971
2022-03-16 16:45:29 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 1.1759
2022-03-16 16:46:02 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 1.2714
2022-03-16 16:46:36 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 1.1619
2022-03-16 16:47:09 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 1.4957
2022-03-16 16:47:42 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 1.2613
2022-03-16 16:48:14 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 1.2985
2022-03-16 16:48:48 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 1.2779
2022-03-16 16:49:21 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 1.3895
2022-03-16 16:49:55 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 1.1083
2022-03-16 16:50:27 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 1.1754
2022-03-16 16:51:01 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 1.2099
2022-03-16 16:51:34 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 1.3910
2022-03-16 16:52:07 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 1.1276
2022-03-16 16:52:40 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 1.2314
2022-03-16 16:53:14 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 1.2208
2022-03-16 16:53:45 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 1.1885
2022-03-16 16:53:46 - train: epoch 082, train_loss: 1.2471
2022-03-16 16:55:00 - eval: epoch: 082, acc1: 71.648%, acc5: 90.422%, test_loss: 1.1266, per_image_load_time: 2.438ms, per_image_inference_time: 0.329ms
2022-03-16 16:55:01 - until epoch: 082, best_acc1: 71.648%
2022-03-16 16:55:01 - epoch 083 lr: 0.0010000000000000002
2022-03-16 16:55:39 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 1.1357
2022-03-16 16:56:12 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 1.0992
2022-03-16 16:56:45 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 1.3102
2022-03-16 16:57:19 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 1.3966
2022-03-16 16:57:52 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 1.2058
2022-03-16 16:58:25 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 1.2167
2022-03-16 16:58:58 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 1.1720
2022-03-16 16:59:31 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 1.1352
2022-03-16 17:00:05 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 1.3143
2022-03-16 17:00:37 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 1.3829
2022-03-16 17:01:11 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 1.2253
2022-03-16 17:01:45 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 1.2207
2022-03-16 17:02:18 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 1.0560
2022-03-16 17:02:52 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 1.3273
2022-03-16 17:03:25 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 1.2795
2022-03-16 17:03:59 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 1.2826
2022-03-16 17:04:32 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 1.4221
2022-03-16 17:05:06 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 1.5177
2022-03-16 17:05:38 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 1.1006
2022-03-16 17:06:12 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 0.9675
2022-03-16 17:06:45 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 1.1707
2022-03-16 17:07:19 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 1.2366
2022-03-16 17:07:51 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 1.3715
2022-03-16 17:08:25 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 1.1688
2022-03-16 17:08:59 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 1.2641
2022-03-16 17:09:32 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 1.0942
2022-03-16 17:10:05 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 1.2450
2022-03-16 17:10:39 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 1.1229
2022-03-16 17:11:12 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 1.2051
2022-03-16 17:11:45 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 1.3793
2022-03-16 17:12:18 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 1.1487
2022-03-16 17:12:52 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 1.1766
2022-03-16 17:13:25 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 1.4894
2022-03-16 17:13:59 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 1.3469
2022-03-16 17:14:32 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 1.3051
2022-03-16 17:15:06 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 1.2777
2022-03-16 17:15:39 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 1.2884
2022-03-16 17:16:13 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 1.1367
2022-03-16 17:16:46 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 1.3529
2022-03-16 17:17:20 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 1.5031
2022-03-16 17:17:52 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 1.2041
2022-03-16 17:18:26 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 1.5572
2022-03-16 17:18:59 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 1.2733
2022-03-16 17:19:32 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 1.0964
2022-03-16 17:20:06 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 1.3291
2022-03-16 17:20:39 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 1.4287
2022-03-16 17:21:11 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.4092
2022-03-16 17:21:45 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 1.3776
2022-03-16 17:22:18 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 1.5008
2022-03-16 17:22:50 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 1.3005
2022-03-16 17:22:51 - train: epoch 083, train_loss: 1.2471
2022-03-16 17:24:06 - eval: epoch: 083, acc1: 71.604%, acc5: 90.390%, test_loss: 1.1257, per_image_load_time: 2.526ms, per_image_inference_time: 0.332ms
2022-03-16 17:24:06 - until epoch: 083, best_acc1: 71.648%
2022-03-16 17:24:06 - epoch 084 lr: 0.0010000000000000002
2022-03-16 17:24:45 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 1.2572
2022-03-16 17:25:18 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 1.3824
2022-03-16 17:25:50 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 1.0355
2022-03-16 17:26:24 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 1.3060
2022-03-16 17:26:58 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 1.3480
2022-03-16 17:27:31 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 1.4261
2022-03-16 17:28:04 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 1.2569
2022-03-16 17:28:37 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 1.3880
2022-03-16 17:29:11 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 1.1976
2022-03-16 17:29:44 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 1.3692
2022-03-16 17:30:18 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 1.1860
2022-03-16 17:30:51 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 1.4563
2022-03-16 17:31:25 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 1.2086
2022-03-16 17:31:58 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 1.2477
2022-03-16 17:32:30 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 1.2989
2022-03-16 17:33:04 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 1.1639
2022-03-16 17:33:37 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 1.2739
2022-03-16 17:34:11 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 1.2057
2022-03-16 17:34:44 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 1.2789
2022-03-16 17:35:17 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 1.2711
2022-03-16 17:35:52 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 1.1429
2022-03-16 17:36:25 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 1.0102
2022-03-16 17:36:59 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 1.2748
2022-03-16 17:37:32 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 1.1696
2022-03-16 17:38:06 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 1.2971
2022-03-16 17:38:40 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 1.3533
2022-03-16 17:39:13 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 1.2068
2022-03-16 17:39:47 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 1.3435
2022-03-16 17:40:20 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 1.2487
2022-03-16 17:40:55 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 1.1855
2022-03-16 17:41:28 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 1.1683
2022-03-16 17:42:02 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 1.1795
2022-03-16 17:42:35 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 1.2702
2022-03-16 17:43:09 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 1.1965
2022-03-16 17:43:42 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 1.0528
2022-03-16 17:44:16 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 1.2173
2022-03-16 17:44:49 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 1.3644
2022-03-16 17:45:23 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 1.5153
2022-03-16 17:45:56 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 1.3576
2022-03-16 17:46:30 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 1.3925
2022-03-16 17:47:03 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 1.3430
2022-03-16 17:47:37 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 1.3990
2022-03-16 17:48:10 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 1.2641
2022-03-16 17:48:44 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.4790
2022-03-16 17:49:18 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 1.2045
2022-03-16 17:49:51 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 1.1970
2022-03-16 17:50:24 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 1.4664
2022-03-16 17:50:59 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 1.1784
2022-03-16 17:51:32 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 1.0246
2022-03-16 17:52:04 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 1.2377
2022-03-16 17:52:04 - train: epoch 084, train_loss: 1.2435
2022-03-16 17:53:18 - eval: epoch: 084, acc1: 71.616%, acc5: 90.494%, test_loss: 1.1237, per_image_load_time: 1.669ms, per_image_inference_time: 0.360ms
2022-03-16 17:53:19 - until epoch: 084, best_acc1: 71.648%
2022-03-16 17:53:19 - epoch 085 lr: 0.0010000000000000002
2022-03-16 17:53:58 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 1.1464
2022-03-16 17:54:31 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 1.1197
2022-03-16 17:55:05 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 1.3389
2022-03-16 17:55:38 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 1.2056
2022-03-16 17:56:11 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 1.5111
2022-03-16 17:56:44 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 1.0493
2022-03-16 17:57:18 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 1.1165
2022-03-16 17:57:51 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 1.2129
2022-03-16 17:58:25 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 1.1371
2022-03-16 17:58:59 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 1.3308
2022-03-16 17:59:31 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 1.2721
2022-03-16 18:00:05 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 1.1622
2022-03-16 18:00:39 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 1.2298
2022-03-16 18:01:12 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 1.2904
2022-03-16 18:01:45 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 1.1081
2022-03-16 18:02:19 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 1.2837
2022-03-16 18:02:52 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 1.4177
2022-03-16 18:03:25 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 1.0662
2022-03-16 18:03:58 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 1.1205
2022-03-16 18:04:32 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 1.2334
2022-03-16 18:05:05 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 0.9147
2022-03-16 18:05:39 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 1.2948
2022-03-16 18:06:11 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 1.0431
2022-03-16 18:06:46 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 1.3411
2022-03-16 18:07:19 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 1.4464
2022-03-16 18:07:53 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 1.2541
2022-03-16 18:08:27 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 1.1512
2022-03-16 18:09:00 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 1.3420
2022-03-16 18:09:33 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 1.3409
2022-03-16 18:10:06 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 1.3718
2022-03-16 18:10:41 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 1.2264
2022-03-16 18:11:14 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 1.1640
2022-03-16 18:11:47 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 1.2929
2022-03-16 18:12:20 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 1.3100
2022-03-16 18:12:55 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 1.3317
2022-03-16 18:13:28 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 1.3185
2022-03-16 18:14:02 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 1.0623
2022-03-16 18:14:36 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 1.2599
2022-03-16 18:15:09 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 1.2370
2022-03-16 18:15:43 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 1.4594
2022-03-16 18:16:16 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 1.2135
2022-03-16 18:16:49 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 1.3224
2022-03-16 18:17:23 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 1.1002
2022-03-16 18:17:57 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 1.2613
2022-03-16 18:18:30 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 1.3257
2022-03-16 18:19:03 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 1.2469
2022-03-16 18:19:36 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.4368
2022-03-16 18:20:10 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 1.1307
2022-03-16 18:20:45 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 1.2661
2022-03-16 18:21:16 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 1.1349
2022-03-16 18:21:17 - train: epoch 085, train_loss: 1.2453
2022-03-16 18:22:32 - eval: epoch: 085, acc1: 71.672%, acc5: 90.390%, test_loss: 1.1267, per_image_load_time: 0.812ms, per_image_inference_time: 0.361ms
2022-03-16 18:22:32 - until epoch: 085, best_acc1: 71.672%
2022-03-16 18:22:32 - epoch 086 lr: 0.0010000000000000002
2022-03-16 18:23:11 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 1.1012
2022-03-16 18:23:44 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 1.1268
2022-03-16 18:24:17 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 1.2858
2022-03-16 18:24:51 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 1.2981
2022-03-16 18:25:24 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 1.2043
2022-03-16 18:25:57 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 1.2018
2022-03-16 18:26:31 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 1.1452
2022-03-16 18:27:03 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 1.3910
2022-03-16 18:27:37 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 1.3708
2022-03-16 18:28:11 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 1.2780
2022-03-16 18:28:44 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 1.2858
2022-03-16 18:29:17 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 1.1029
2022-03-16 18:29:52 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 1.1739
2022-03-16 18:30:25 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 1.1069
2022-03-16 18:30:58 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 1.1519
2022-03-16 18:31:31 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 1.3219
2022-03-16 18:32:05 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 1.1464
2022-03-16 18:32:38 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 1.2540
2022-03-16 18:33:12 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 1.3868
2022-03-16 18:33:46 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 1.3196
2022-03-16 18:34:19 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 1.1003
2022-03-16 18:34:54 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 1.2614
2022-03-16 18:35:27 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 1.1380
2022-03-16 18:36:01 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 0.9975
2022-03-16 18:36:34 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 1.1515
2022-03-16 18:37:07 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 1.2578
2022-03-16 18:37:41 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 1.0629
2022-03-16 18:38:14 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 1.3211
2022-03-16 18:38:48 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 1.0360
2022-03-16 18:39:21 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 1.3517
2022-03-16 18:39:54 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 1.1737
2022-03-16 18:40:28 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 1.3573
2022-03-16 18:41:01 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 1.3074
2022-03-16 18:41:35 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 1.3961
2022-03-16 18:42:09 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 1.2611
2022-03-16 18:42:42 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 1.2943
2022-03-16 18:43:15 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 1.3404
2022-03-16 18:43:50 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 1.3499
2022-03-16 18:44:23 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 1.3212
2022-03-16 18:44:57 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 1.2812
2022-03-16 18:45:30 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 1.2362
2022-03-16 18:46:05 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 1.3461
2022-03-16 18:46:38 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 1.2473
2022-03-16 18:47:12 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 1.1425
2022-03-16 18:47:45 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 1.1072
2022-03-16 18:48:19 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 1.1889
2022-03-16 18:48:53 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 1.2108
2022-03-16 18:49:26 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 1.2395
2022-03-16 18:49:59 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 1.2567
2022-03-16 18:50:31 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 1.1945
2022-03-16 18:50:32 - train: epoch 086, train_loss: 1.2415
2022-03-16 18:51:48 - eval: epoch: 086, acc1: 71.606%, acc5: 90.354%, test_loss: 1.1292, per_image_load_time: 1.567ms, per_image_inference_time: 0.349ms
2022-03-16 18:51:48 - until epoch: 086, best_acc1: 71.672%
2022-03-16 18:51:48 - epoch 087 lr: 0.0010000000000000002
2022-03-16 18:52:27 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 1.3001
2022-03-16 18:53:00 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 1.1558
2022-03-16 18:53:33 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 1.3386
2022-03-16 18:54:07 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 1.3169
2022-03-16 18:54:41 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 0.9946
2022-03-16 18:55:14 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 1.2069
2022-03-16 18:55:48 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 1.0207
2022-03-16 18:56:21 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 1.3010
2022-03-16 18:56:55 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 1.2866
2022-03-16 18:57:28 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 1.1932
2022-03-16 18:58:02 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 1.2584
2022-03-16 18:58:36 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 1.2941
2022-03-16 18:59:09 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 1.3569
2022-03-16 18:59:42 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 1.2098
2022-03-16 19:00:16 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 1.1823
2022-03-16 19:00:49 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 1.1397
2022-03-16 19:01:22 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 1.4428
2022-03-16 19:01:56 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 1.2707
2022-03-16 19:02:29 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 1.2692
2022-03-16 19:03:02 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 1.2581
2022-03-16 19:03:35 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 1.2311
2022-03-16 19:04:08 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 1.1983
2022-03-16 19:04:42 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 1.3354
2022-03-16 19:05:15 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 1.1712
2022-03-16 19:05:49 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 1.1641
2022-03-16 19:06:22 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 1.2866
2022-03-16 19:06:55 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 1.1304
2022-03-16 19:07:29 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 1.0270
2022-03-16 19:08:01 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 1.1391
2022-03-16 19:08:35 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 1.3145
2022-03-16 19:09:08 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 1.2035
2022-03-16 19:09:41 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 1.2015
2022-03-16 19:10:14 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 1.2250
2022-03-16 19:10:48 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 1.2269
2022-03-16 19:11:21 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 1.1076
2022-03-16 19:11:54 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 1.0430
2022-03-16 19:12:27 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 1.0609
2022-03-16 19:13:00 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 1.2951
2022-03-16 19:13:32 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 1.2432
2022-03-16 19:14:06 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 1.4121
2022-03-16 19:14:39 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 1.3952
2022-03-16 19:15:12 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 1.2731
2022-03-16 19:15:46 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 1.2091
2022-03-16 19:16:19 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 1.3311
2022-03-16 19:16:52 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 1.3463
2022-03-16 19:17:25 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 1.2020
2022-03-16 19:17:58 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 1.2343
2022-03-16 19:18:31 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 1.2137
2022-03-16 19:19:04 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 1.1023
2022-03-16 19:19:36 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 1.2448
2022-03-16 19:19:37 - train: epoch 087, train_loss: 1.2386
2022-03-16 19:20:51 - eval: epoch: 087, acc1: 71.610%, acc5: 90.430%, test_loss: 1.1256, per_image_load_time: 2.383ms, per_image_inference_time: 0.347ms
2022-03-16 19:20:51 - until epoch: 087, best_acc1: 71.672%
2022-03-16 19:20:51 - epoch 088 lr: 0.0010000000000000002
2022-03-16 19:21:30 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 1.2758
2022-03-16 19:22:02 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 1.2245
2022-03-16 19:22:36 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 1.2454
2022-03-16 19:23:10 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 1.2829
2022-03-16 19:23:42 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 1.2745
2022-03-16 19:24:16 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 1.2145
2022-03-16 19:24:48 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 1.3039
2022-03-16 19:25:21 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 1.2625
2022-03-16 19:25:54 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 1.2935
2022-03-16 19:26:27 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 1.1679
2022-03-16 19:27:00 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 1.2324
2022-03-16 19:27:34 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 1.2051
2022-03-16 19:28:06 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 1.3647
2022-03-16 19:28:39 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 1.1046
2022-03-16 19:29:13 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 1.3373
2022-03-16 19:29:46 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 1.1125
2022-03-16 19:30:19 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 1.3223
2022-03-16 19:30:52 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 1.1072
2022-03-16 19:31:25 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 1.2999
2022-03-16 19:31:58 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 1.1176
2022-03-16 19:32:32 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 1.1496
2022-03-16 19:33:05 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 1.1871
2022-03-16 19:33:38 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 1.1253
2022-03-16 19:34:10 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 1.2775
2022-03-16 19:34:43 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 1.3591
2022-03-16 19:35:17 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 1.4036
2022-03-16 19:35:50 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 1.2484
2022-03-16 19:36:23 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 1.2459
2022-03-16 19:36:56 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 1.2232
2022-03-16 19:37:29 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 1.3886
2022-03-16 19:38:03 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 1.0663
2022-03-16 19:38:36 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 1.1484
2022-03-16 19:39:09 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 1.1969
2022-03-16 19:39:42 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 1.0387
2022-03-16 19:40:16 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 1.1561
2022-03-16 19:40:48 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 1.2277
2022-03-16 19:41:22 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 1.3262
2022-03-16 19:41:56 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 1.2685
2022-03-16 19:42:28 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 1.4606
2022-03-16 19:43:02 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 1.1776
2022-03-16 19:43:35 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 1.2430
2022-03-16 19:44:09 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 1.3185
2022-03-16 19:44:42 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 1.1804
2022-03-16 19:45:15 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 1.1878
2022-03-16 19:45:48 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 1.2932
2022-03-16 19:46:22 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 1.3780
2022-03-16 19:46:54 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 1.2984
2022-03-16 19:47:27 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 1.0941
2022-03-16 19:48:01 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 1.0826
2022-03-16 19:48:33 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 1.2430
2022-03-16 19:48:34 - train: epoch 088, train_loss: 1.2385
2022-03-16 19:49:47 - eval: epoch: 088, acc1: 71.638%, acc5: 90.480%, test_loss: 1.1221, per_image_load_time: 2.476ms, per_image_inference_time: 0.360ms
2022-03-16 19:49:48 - until epoch: 088, best_acc1: 71.672%
2022-03-16 19:49:48 - epoch 089 lr: 0.0010000000000000002
2022-03-16 19:50:26 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 1.4777
2022-03-16 19:50:59 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 1.0530
2022-03-16 19:51:32 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 1.3394
2022-03-16 19:52:06 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 1.1647
2022-03-16 19:52:39 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 1.2414
2022-03-16 19:53:11 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 1.1449
2022-03-16 19:53:45 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 1.2651
2022-03-16 19:54:17 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 1.3839
2022-03-16 19:54:50 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 1.2935
2022-03-16 19:55:23 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 1.4122
2022-03-16 19:55:56 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 1.1738
2022-03-16 19:56:30 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 1.3418
2022-03-16 19:57:03 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 1.2806
2022-03-16 19:57:36 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 1.3617
2022-03-16 19:58:09 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 1.2517
2022-03-16 19:58:43 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 1.0768
2022-03-16 19:59:16 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 1.2165
2022-03-16 19:59:49 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 1.2042
2022-03-16 20:00:22 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 1.0384
2022-03-16 20:00:56 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 1.1390
2022-03-16 20:01:28 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 1.2439
2022-03-16 20:02:01 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 1.2414
2022-03-16 20:02:34 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 1.1261
2022-03-16 20:03:07 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 1.2851
2022-03-16 20:03:40 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 1.1862
2022-03-16 20:04:13 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 1.2810
2022-03-16 20:04:46 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 1.2203
2022-03-16 20:05:19 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 1.2906
2022-03-16 20:05:52 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 1.2451
2022-03-16 20:06:25 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 1.2751
2022-03-16 20:06:58 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 1.3484
2022-03-16 20:07:31 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 1.3140
2022-03-16 20:08:04 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 1.3370
2022-03-16 20:08:36 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 1.2074
2022-03-16 20:09:10 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 1.0305
2022-03-16 20:09:43 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 1.1552
2022-03-16 20:10:16 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 1.4395
2022-03-16 20:10:49 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 1.1889
2022-03-16 20:11:22 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 1.4082
2022-03-16 20:11:56 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 1.1916
2022-03-16 20:12:29 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 1.5058
2022-03-16 20:13:01 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 1.2274
2022-03-16 20:13:35 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 1.3029
2022-03-16 20:14:09 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 1.1748
2022-03-16 20:14:41 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 1.0876
2022-03-16 20:15:14 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 1.2414
2022-03-16 20:15:47 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 1.1195
2022-03-16 20:16:20 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 1.1788
2022-03-16 20:16:53 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 1.1990
2022-03-16 20:17:25 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 1.0465
2022-03-16 20:17:26 - train: epoch 089, train_loss: 1.2371
2022-03-16 20:18:40 - eval: epoch: 089, acc1: 71.710%, acc5: 90.512%, test_loss: 1.1220, per_image_load_time: 2.457ms, per_image_inference_time: 0.344ms
2022-03-16 20:18:40 - until epoch: 089, best_acc1: 71.710%
2022-03-16 20:18:40 - epoch 090 lr: 0.0010000000000000002
2022-03-16 20:19:18 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 1.2652
2022-03-16 20:19:52 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 1.2165
2022-03-16 20:20:25 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 1.2323
2022-03-16 20:20:58 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 1.1484
2022-03-16 20:21:31 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 1.2402
2022-03-16 20:22:04 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 1.4194
2022-03-16 20:22:37 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 1.2036
2022-03-16 20:23:11 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 1.1882
2022-03-16 20:23:44 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 1.2171
2022-03-16 20:24:17 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 1.0615
2022-03-16 20:24:51 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 1.2996
2022-03-16 20:25:23 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 1.2016
2022-03-16 20:25:57 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 1.3987
2022-03-16 20:26:29 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 1.1295
2022-03-16 20:27:02 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 1.4648
2022-03-16 20:27:36 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 1.1632
2022-03-16 20:28:09 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 1.0563
2022-03-16 20:28:42 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 1.1150
2022-03-16 20:29:15 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 1.1575
2022-03-16 20:29:48 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 1.3646
2022-03-16 20:30:21 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 1.3447
2022-03-16 20:30:54 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 1.2918
2022-03-16 20:31:28 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 1.3885
2022-03-16 20:32:01 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 1.3333
2022-03-16 20:32:35 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 1.3153
2022-03-16 20:33:08 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 1.2479
2022-03-16 20:33:41 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 1.1624
2022-03-16 20:34:15 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 1.2400
2022-03-16 20:34:48 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 1.1147
2022-03-16 20:35:21 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 1.2168
2022-03-16 20:35:54 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 1.0858
2022-03-16 20:36:27 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 1.2243
2022-03-16 20:37:01 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 1.4594
2022-03-16 20:37:35 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 1.0788
2022-03-16 20:38:07 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 1.2466
2022-03-16 20:38:41 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 1.1368
2022-03-16 20:39:14 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 1.2409
2022-03-16 20:39:47 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 1.3109
2022-03-16 20:40:21 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 0.9765
2022-03-16 20:40:54 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 1.1996
2022-03-16 20:41:27 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 1.5526
2022-03-16 20:42:00 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 1.3686
2022-03-16 20:42:33 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 1.2839
2022-03-16 20:43:07 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 1.2505
2022-03-16 20:43:39 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 1.1466
2022-03-16 20:44:14 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 1.1991
2022-03-16 20:44:46 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 1.1582
2022-03-16 20:45:20 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 1.2631
2022-03-16 20:45:52 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 1.1755
2022-03-16 20:46:24 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 1.2102
2022-03-16 20:46:25 - train: epoch 090, train_loss: 1.2346
2022-03-16 20:47:39 - eval: epoch: 090, acc1: 71.772%, acc5: 90.558%, test_loss: 1.1195, per_image_load_time: 2.410ms, per_image_inference_time: 0.348ms
2022-03-16 20:47:39 - until epoch: 090, best_acc1: 71.772%
2022-03-16 20:47:39 - epoch 091 lr: 0.00010000000000000003
2022-03-16 20:48:18 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 1.1474
2022-03-16 20:48:51 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 1.1539
2022-03-16 20:49:24 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 1.2179
2022-03-16 20:49:58 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 1.1769
2022-03-16 20:50:30 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 1.1570
2022-03-16 20:51:03 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 1.2918
2022-03-16 20:51:37 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 1.1673
2022-03-16 20:52:10 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 0.9831
2022-03-16 20:52:43 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 1.2062
2022-03-16 20:53:15 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 1.1536
2022-03-16 20:53:49 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 1.0558
2022-03-16 20:54:22 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 1.2407
2022-03-16 20:54:56 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 1.1236
2022-03-16 20:55:29 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 1.2759
2022-03-16 20:56:02 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 1.1739
2022-03-16 20:56:35 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 1.2136
2022-03-16 20:57:09 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 1.3334
2022-03-16 20:57:42 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 1.2569
2022-03-16 20:58:15 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 1.1534
2022-03-16 20:58:48 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 1.1440
2022-03-16 20:59:21 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 0.9687
2022-03-16 20:59:54 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 1.2900
2022-03-16 21:00:27 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 1.2196
2022-03-16 21:01:00 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 1.1344
2022-03-16 21:01:34 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 1.1961
2022-03-16 21:02:06 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 1.1621
2022-03-16 21:02:40 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 1.0432
2022-03-16 21:03:13 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 1.1712
2022-03-16 21:03:45 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 1.2134
2022-03-16 21:04:18 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 1.2424
2022-03-16 21:04:52 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 1.0868
2022-03-16 21:05:25 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 1.2794
2022-03-16 21:05:58 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 1.0917
2022-03-16 21:06:31 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 1.0739
2022-03-16 21:07:04 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 1.2076
2022-03-16 21:07:38 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 1.1827
2022-03-16 21:08:11 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 1.2457
2022-03-16 21:08:44 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 1.1869
2022-03-16 21:09:17 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 1.1082
2022-03-16 21:09:51 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 1.2071
2022-03-16 21:10:23 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 1.2305
2022-03-16 21:10:56 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 1.2415
2022-03-16 21:11:30 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 1.2006
2022-03-16 21:12:03 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 1.1049
2022-03-16 21:12:37 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 1.0706
2022-03-16 21:13:12 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 1.2190
2022-03-16 21:13:47 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 1.2437
2022-03-16 21:14:22 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 1.2301
2022-03-16 21:14:58 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 1.1439
2022-03-16 21:15:32 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 1.3473
2022-03-16 21:15:33 - train: epoch 091, train_loss: 1.2033
2022-03-16 21:16:54 - eval: epoch: 091, acc1: 72.252%, acc5: 90.754%, test_loss: 1.1019, per_image_load_time: 2.845ms, per_image_inference_time: 0.301ms
2022-03-16 21:16:55 - until epoch: 091, best_acc1: 72.252%
2022-03-16 21:16:55 - epoch 092 lr: 0.00010000000000000003
2022-03-16 21:17:37 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 1.1702
2022-03-16 21:18:13 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 1.2773
2022-03-16 21:18:48 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 1.1970
2022-03-16 21:19:24 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 1.0191
2022-03-16 21:19:59 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 1.2796
2022-03-16 21:20:35 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 1.2279
2022-03-16 21:21:11 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 1.2103
2022-03-16 21:21:47 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 1.3132
2022-03-16 21:22:22 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 1.1897
2022-03-16 21:22:58 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 1.3552
2022-03-16 21:23:34 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 1.1031
2022-03-16 21:24:06 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 1.0958
2022-03-16 21:24:40 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 1.1861
2022-03-16 21:25:12 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 1.1486
2022-03-16 21:25:45 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 1.1375
2022-03-16 21:26:17 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 1.2286
2022-03-16 21:26:49 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 1.2356
2022-03-16 21:27:22 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 1.0688
2022-03-16 21:27:54 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 0.9959
2022-03-16 21:28:27 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 1.0354
2022-03-16 21:29:00 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 1.1676
2022-03-16 21:29:32 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 1.1627
2022-03-16 21:30:06 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 1.1866
2022-03-16 21:30:38 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 1.1215
2022-03-16 21:31:11 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 1.1009
2022-03-16 21:31:44 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 1.3668
2022-03-16 21:32:17 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 1.1704
2022-03-16 21:32:50 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 1.1053
2022-03-16 21:33:22 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 1.2065
2022-03-16 21:33:56 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 1.1441
2022-03-16 21:34:28 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 1.2099
2022-03-16 21:35:02 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 1.0504
2022-03-16 21:35:34 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 1.1894
2022-03-16 21:36:08 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 1.3078
2022-03-16 21:36:40 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 1.1936
2022-03-16 21:37:14 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 1.1288
2022-03-16 21:37:46 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 1.1264
2022-03-16 21:38:20 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 1.4324
2022-03-16 21:38:54 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 1.2620
2022-03-16 21:39:27 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 1.3357
2022-03-16 21:40:02 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 1.0681
2022-03-16 21:40:38 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 1.2101
2022-03-16 21:41:14 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 1.2314
2022-03-16 21:41:50 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 1.0513
2022-03-16 21:42:26 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 1.1021
2022-03-16 21:43:02 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 1.0694
2022-03-16 21:43:38 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 0.9640
2022-03-16 21:44:13 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 1.0921
2022-03-16 21:44:49 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 1.1441
2022-03-16 21:45:23 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 1.3357
2022-03-16 21:45:24 - train: epoch 092, train_loss: 1.1936
2022-03-16 21:46:45 - eval: epoch: 092, acc1: 72.178%, acc5: 90.764%, test_loss: 1.1002, per_image_load_time: 1.315ms, per_image_inference_time: 0.341ms
2022-03-16 21:46:45 - until epoch: 092, best_acc1: 72.252%
2022-03-16 21:46:45 - epoch 093 lr: 0.00010000000000000003
2022-03-16 21:47:27 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 1.1221
2022-03-16 21:48:03 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 1.0648
2022-03-16 21:48:41 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 1.1445
2022-03-16 21:49:16 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 1.2235
2022-03-16 21:49:52 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 1.3268
2022-03-16 21:50:28 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 1.0922
2022-03-16 21:51:03 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 1.2861
2022-03-16 21:51:41 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 1.0740
2022-03-16 21:52:16 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 1.1099
2022-03-16 21:52:52 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 1.0364
2022-03-16 21:53:27 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 0.9982
2022-03-16 21:54:04 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 1.1049
2022-03-16 21:54:39 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 1.1631
2022-03-16 21:55:17 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 1.1524
2022-03-16 21:55:51 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 1.4193
2022-03-16 21:56:25 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 1.2063
2022-03-16 21:56:59 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 1.0776
2022-03-16 21:57:33 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 1.2034
2022-03-16 21:58:10 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 1.1319
2022-03-16 21:58:45 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 1.0521
2022-03-16 21:59:21 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 1.0519
2022-03-16 21:59:57 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 1.3682
2022-03-16 22:00:35 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 1.3348
2022-03-16 22:01:10 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 1.1101
2022-03-16 22:01:46 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 1.2175
2022-03-16 22:02:22 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 1.4534
2022-03-16 22:02:58 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 1.1148
2022-03-16 22:03:35 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 1.1453
2022-03-16 22:04:12 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 1.1845
2022-03-16 22:04:48 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 1.1187
2022-03-16 22:05:24 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 1.3483
2022-03-16 22:06:00 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 1.0572
2022-03-16 22:06:37 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 1.1919
2022-03-16 22:07:12 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 1.4045
2022-03-16 22:07:49 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 0.9195
2022-03-16 22:08:25 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 1.3756
2022-03-16 22:09:02 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 1.0456
2022-03-16 22:09:34 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 1.0906
2022-03-16 22:10:11 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 1.2401
2022-03-16 22:10:46 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 1.3828
2022-03-16 22:11:21 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 1.2691
2022-03-16 22:11:57 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 1.3117
2022-03-16 22:12:33 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 1.3370
2022-03-16 22:13:09 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 1.1086
2022-03-16 22:13:45 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 1.2013
2022-03-16 22:14:21 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 0.9540
2022-03-16 22:14:57 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 1.4275
2022-03-16 22:15:32 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 1.1275
2022-03-16 22:16:09 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 1.3494
2022-03-16 22:16:43 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 1.1649
2022-03-16 22:16:44 - train: epoch 093, train_loss: 1.1909
2022-03-16 22:18:05 - eval: epoch: 093, acc1: 72.294%, acc5: 90.798%, test_loss: 1.0983, per_image_load_time: 2.514ms, per_image_inference_time: 0.356ms
2022-03-16 22:18:06 - until epoch: 093, best_acc1: 72.294%
2022-03-16 22:18:06 - epoch 094 lr: 0.00010000000000000003
2022-03-16 22:18:48 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 1.1916
2022-03-16 22:19:25 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 1.0910
2022-03-16 22:20:01 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 1.3032
2022-03-16 22:20:36 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 1.3451
2022-03-16 22:21:13 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 1.0886
2022-03-16 22:21:45 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 1.0899
2022-03-16 22:22:21 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 1.3284
2022-03-16 22:22:57 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 1.1339
2022-03-16 22:23:32 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 1.1528
2022-03-16 22:24:08 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 1.2307
2022-03-16 22:24:44 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 1.3652
2022-03-16 22:25:20 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 1.2102
2022-03-16 22:25:56 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 1.1455
2022-03-16 22:26:32 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 1.1141
2022-03-16 22:27:08 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 1.2844
2022-03-16 22:27:45 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.5123
2022-03-16 22:28:21 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 1.0996
2022-03-16 22:28:57 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 1.0932
2022-03-16 22:29:32 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 1.1866
2022-03-16 22:30:08 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 1.0552
2022-03-16 22:30:44 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 1.3120
2022-03-16 22:31:20 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 1.0762
2022-03-16 22:31:57 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 0.9869
2022-03-16 22:32:33 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 1.2150
2022-03-16 22:33:10 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 1.1633
2022-03-16 22:33:43 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 1.0348
2022-03-16 22:34:20 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 1.1437
2022-03-16 22:34:54 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 1.1763
2022-03-16 22:35:28 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 1.1942
2022-03-16 22:36:00 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 1.1004
2022-03-16 22:36:34 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 1.3587
2022-03-16 22:37:08 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 1.2064
2022-03-16 22:37:41 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 1.1246
2022-03-16 22:38:15 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 1.2072
2022-03-16 22:38:48 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 1.4302
2022-03-16 22:39:22 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 1.2677
2022-03-16 22:39:55 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 1.4395
2022-03-16 22:40:28 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 1.0853
2022-03-16 22:41:01 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 1.1978
2022-03-16 22:41:34 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 1.3450
2022-03-16 22:42:07 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 1.5060
2022-03-16 22:42:40 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 1.0076
2022-03-16 22:43:14 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 1.2960
2022-03-16 22:43:48 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 1.2452
2022-03-16 22:44:21 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 1.2225
2022-03-16 22:44:55 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 1.2667
2022-03-16 22:45:29 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 1.1632
2022-03-16 22:46:03 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 1.2001
2022-03-16 22:46:36 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 1.1021
2022-03-16 22:47:08 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 1.0978
2022-03-16 22:47:08 - train: epoch 094, train_loss: 1.1883
2022-03-16 22:48:23 - eval: epoch: 094, acc1: 72.316%, acc5: 90.780%, test_loss: 1.0977, per_image_load_time: 2.448ms, per_image_inference_time: 0.386ms
2022-03-16 22:48:23 - until epoch: 094, best_acc1: 72.316%
2022-03-16 22:48:23 - epoch 095 lr: 0.00010000000000000003
2022-03-16 22:49:02 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 1.0642
2022-03-16 22:49:35 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 1.3289
2022-03-16 22:50:09 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 1.1278
2022-03-16 22:50:42 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 1.2933
2022-03-16 22:51:15 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 1.2908
2022-03-16 22:51:49 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 1.1772
2022-03-16 22:52:22 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 1.3070
2022-03-16 22:52:56 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 1.2960
2022-03-16 22:53:29 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 1.3877
2022-03-16 22:54:02 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 1.2596
2022-03-16 22:54:35 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 1.0664
2022-03-16 22:55:10 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 1.1208
2022-03-16 22:55:43 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 1.1356
2022-03-16 22:56:17 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 1.1527
2022-03-16 22:56:50 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 1.1633
2022-03-16 22:57:24 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 0.8105
2022-03-16 22:57:57 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 1.1498
2022-03-16 22:58:31 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 1.2171
2022-03-16 22:59:04 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 1.1805
2022-03-16 22:59:38 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 1.2364
2022-03-16 23:00:11 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 1.2303
2022-03-16 23:00:45 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 0.9644
2022-03-16 23:01:19 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 1.2324
2022-03-16 23:01:52 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 1.2468
2022-03-16 23:02:25 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 1.1025
2022-03-16 23:03:00 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 1.2032
2022-03-16 23:03:33 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 1.2610
2022-03-16 23:04:06 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 1.1289
2022-03-16 23:04:40 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 1.1007
2022-03-16 23:05:13 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 1.2884
2022-03-16 23:05:47 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 1.3642
2022-03-16 23:06:20 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 1.2145
2022-03-16 23:06:53 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 1.2823
2022-03-16 23:07:28 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 1.1035
2022-03-16 23:08:01 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 1.1591
2022-03-16 23:08:34 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 1.0386
2022-03-16 23:09:07 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 1.0753
2022-03-16 23:09:41 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 0.9561
2022-03-16 23:10:15 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 1.3097
2022-03-16 23:10:48 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 1.0503
2022-03-16 23:11:20 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 1.2058
2022-03-16 23:11:54 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 1.1251
2022-03-16 23:12:27 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 1.1849
2022-03-16 23:13:01 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 1.2983
2022-03-16 23:13:34 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 1.0621
2022-03-16 23:14:09 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 1.2009
2022-03-16 23:14:42 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 1.1729
2022-03-16 23:15:15 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 1.3323
2022-03-16 23:15:49 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 1.1112
2022-03-16 23:16:21 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 1.0513
2022-03-16 23:16:22 - train: epoch 095, train_loss: 1.1886
2022-03-16 23:17:36 - eval: epoch: 095, acc1: 72.262%, acc5: 90.774%, test_loss: 1.0976, per_image_load_time: 1.919ms, per_image_inference_time: 0.359ms
2022-03-16 23:17:36 - until epoch: 095, best_acc1: 72.316%
2022-03-16 23:17:36 - epoch 096 lr: 0.00010000000000000003
2022-03-16 23:18:15 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 1.2014
2022-03-16 23:18:48 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 1.1621
2022-03-16 23:19:21 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 1.1849
2022-03-16 23:19:55 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 1.0239
2022-03-16 23:20:29 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 1.2161
2022-03-16 23:21:02 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 1.3427
2022-03-16 23:21:35 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 1.0798
2022-03-16 23:22:09 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 1.1173
2022-03-16 23:22:42 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 1.1255
2022-03-16 23:23:15 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 1.1336
2022-03-16 23:23:48 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 1.2427
2022-03-16 23:24:23 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 1.2708
