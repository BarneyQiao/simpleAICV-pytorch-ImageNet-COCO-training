2022-03-08 13:19:03 - train: epoch 0148, iter [00500, 05004], lr: 0.017146, loss: 2.1122
2022-03-08 13:19:36 - train: epoch 0148, iter [00600, 05004], lr: 0.017146, loss: 2.2621
2022-03-08 13:20:11 - train: epoch 0148, iter [00700, 05004], lr: 0.017146, loss: 2.1097
2022-03-08 13:20:44 - train: epoch 0148, iter [00800, 05004], lr: 0.017146, loss: 2.0181
2022-03-08 13:21:19 - train: epoch 0148, iter [00900, 05004], lr: 0.017146, loss: 2.2497
2022-03-08 13:21:52 - train: epoch 0148, iter [01000, 05004], lr: 0.017146, loss: 2.3243
2022-03-08 13:22:25 - train: epoch 0148, iter [01100, 05004], lr: 0.017146, loss: 2.3350
2022-03-08 13:22:59 - train: epoch 0148, iter [01200, 05004], lr: 0.017146, loss: 2.2442
2022-03-08 13:23:34 - train: epoch 0148, iter [01300, 05004], lr: 0.017146, loss: 2.4320
2022-03-08 13:24:07 - train: epoch 0148, iter [01400, 05004], lr: 0.017146, loss: 2.3390
2022-03-08 13:24:42 - train: epoch 0148, iter [01500, 05004], lr: 0.017146, loss: 2.2574
2022-03-08 13:25:16 - train: epoch 0148, iter [01600, 05004], lr: 0.017146, loss: 2.5477
2022-03-08 13:25:50 - train: epoch 0148, iter [01700, 05004], lr: 0.017146, loss: 2.6773
2022-03-08 13:26:24 - train: epoch 0148, iter [01800, 05004], lr: 0.017146, loss: 2.3592
2022-03-08 13:26:58 - train: epoch 0148, iter [01900, 05004], lr: 0.017146, loss: 2.3085
2022-03-08 13:27:31 - train: epoch 0148, iter [02000, 05004], lr: 0.017146, loss: 2.3623
2022-03-08 13:28:05 - train: epoch 0148, iter [02100, 05004], lr: 0.017146, loss: 2.2204
2022-03-08 13:28:39 - train: epoch 0148, iter [02200, 05004], lr: 0.017146, loss: 2.2456
2022-03-08 13:29:12 - train: epoch 0148, iter [02300, 05004], lr: 0.017146, loss: 2.4961
2022-03-08 13:29:46 - train: epoch 0148, iter [02400, 05004], lr: 0.017146, loss: 2.1750
2022-03-08 13:30:21 - train: epoch 0148, iter [02500, 05004], lr: 0.017146, loss: 2.1769
2022-03-08 13:30:55 - train: epoch 0148, iter [02600, 05004], lr: 0.017146, loss: 2.2443
2022-03-08 13:31:29 - train: epoch 0148, iter [02700, 05004], lr: 0.017146, loss: 2.2003
2022-03-08 13:32:03 - train: epoch 0148, iter [02800, 05004], lr: 0.017146, loss: 2.4475
2022-03-08 13:32:37 - train: epoch 0148, iter [02900, 05004], lr: 0.017146, loss: 2.5029
2022-03-08 13:33:10 - train: epoch 0148, iter [03000, 05004], lr: 0.017146, loss: 2.4446
2022-03-08 13:33:44 - train: epoch 0148, iter [03100, 05004], lr: 0.017146, loss: 2.2788
2022-03-08 13:34:18 - train: epoch 0148, iter [03200, 05004], lr: 0.017146, loss: 2.2389
2022-03-08 13:34:52 - train: epoch 0148, iter [03300, 05004], lr: 0.017146, loss: 2.2626
2022-03-08 13:35:27 - train: epoch 0148, iter [03400, 05004], lr: 0.017146, loss: 2.5395
2022-03-08 13:36:00 - train: epoch 0148, iter [03500, 05004], lr: 0.017146, loss: 2.5090
2022-03-08 13:36:34 - train: epoch 0148, iter [03600, 05004], lr: 0.017146, loss: 2.2666
2022-03-08 13:37:09 - train: epoch 0148, iter [03700, 05004], lr: 0.017146, loss: 2.2374
2022-03-08 13:37:42 - train: epoch 0148, iter [03800, 05004], lr: 0.017146, loss: 2.4215
2022-03-08 13:38:16 - train: epoch 0148, iter [03900, 05004], lr: 0.017146, loss: 2.2138
2022-03-08 13:38:49 - train: epoch 0148, iter [04000, 05004], lr: 0.017146, loss: 2.2680
2022-03-08 13:39:24 - train: epoch 0148, iter [04100, 05004], lr: 0.017146, loss: 2.2448
2022-03-08 13:39:57 - train: epoch 0148, iter [04200, 05004], lr: 0.017146, loss: 2.2466
2022-03-08 13:40:32 - train: epoch 0148, iter [04300, 05004], lr: 0.017146, loss: 2.3971
2022-03-08 13:41:06 - train: epoch 0148, iter [04400, 05004], lr: 0.017146, loss: 2.2052
2022-03-08 13:41:40 - train: epoch 0148, iter [04500, 05004], lr: 0.017146, loss: 2.3576
2022-03-08 13:42:14 - train: epoch 0148, iter [04600, 05004], lr: 0.017146, loss: 2.3619
2022-03-08 13:42:49 - train: epoch 0148, iter [04700, 05004], lr: 0.017146, loss: 2.0928
2022-03-08 13:43:22 - train: epoch 0148, iter [04800, 05004], lr: 0.017146, loss: 2.0945
2022-03-08 13:43:56 - train: epoch 0148, iter [04900, 05004], lr: 0.017146, loss: 2.7430
2022-03-08 13:44:28 - train: epoch 0148, iter [05000, 05004], lr: 0.017146, loss: 2.7417
2022-03-08 13:44:30 - train: epoch 148, train_loss: 2.3406
2022-03-08 13:45:43 - eval: epoch: 148, acc1: 67.630%, acc5: 88.506%, test_loss: 1.3130, per_image_load_time: 0.891ms, per_image_inference_time: 0.500ms
2022-03-08 13:45:44 - until epoch: 148, best_acc1: 68.366%
2022-03-08 13:45:44 - epoch 149 lr: 0.016543469682057107
2022-03-08 13:46:23 - train: epoch 0149, iter [00100, 05004], lr: 0.016543, loss: 2.3803
2022-03-08 13:46:57 - train: epoch 0149, iter [00200, 05004], lr: 0.016543, loss: 2.2133
2022-03-08 13:47:31 - train: epoch 0149, iter [00300, 05004], lr: 0.016543, loss: 2.5145
2022-03-08 13:48:05 - train: epoch 0149, iter [00400, 05004], lr: 0.016543, loss: 2.0442
2022-03-08 13:48:38 - train: epoch 0149, iter [00500, 05004], lr: 0.016543, loss: 2.2528
2022-03-08 13:49:12 - train: epoch 0149, iter [00600, 05004], lr: 0.016543, loss: 2.3409
2022-03-08 13:49:46 - train: epoch 0149, iter [00700, 05004], lr: 0.016543, loss: 2.5722
2022-03-08 13:50:20 - train: epoch 0149, iter [00800, 05004], lr: 0.016543, loss: 2.4865
2022-03-08 13:50:54 - train: epoch 0149, iter [00900, 05004], lr: 0.016543, loss: 2.4558
2022-03-08 13:51:29 - train: epoch 0149, iter [01000, 05004], lr: 0.016543, loss: 2.4859
2022-03-08 13:52:03 - train: epoch 0149, iter [01100, 05004], lr: 0.016543, loss: 2.2960
2022-03-08 13:52:36 - train: epoch 0149, iter [01200, 05004], lr: 0.016543, loss: 2.3871
2022-03-08 13:53:10 - train: epoch 0149, iter [01300, 05004], lr: 0.016543, loss: 2.4994
2022-03-08 13:53:45 - train: epoch 0149, iter [01400, 05004], lr: 0.016543, loss: 2.0830
2022-03-08 13:54:19 - train: epoch 0149, iter [01500, 05004], lr: 0.016543, loss: 2.1808
2022-03-08 13:54:53 - train: epoch 0149, iter [01600, 05004], lr: 0.016543, loss: 2.2954
2022-03-08 13:55:27 - train: epoch 0149, iter [01700, 05004], lr: 0.016543, loss: 2.2593
2022-03-08 13:56:02 - train: epoch 0149, iter [01800, 05004], lr: 0.016543, loss: 2.0189
2022-03-08 13:56:37 - train: epoch 0149, iter [01900, 05004], lr: 0.016543, loss: 2.3643
2022-03-08 13:57:11 - train: epoch 0149, iter [02000, 05004], lr: 0.016543, loss: 2.5393
2022-03-08 13:57:45 - train: epoch 0149, iter [02100, 05004], lr: 0.016543, loss: 2.5331
2022-03-08 13:58:19 - train: epoch 0149, iter [02200, 05004], lr: 0.016543, loss: 2.2141
2022-03-08 13:58:53 - train: epoch 0149, iter [02300, 05004], lr: 0.016543, loss: 2.3811
2022-03-08 13:59:28 - train: epoch 0149, iter [02400, 05004], lr: 0.016543, loss: 2.2873
2022-03-08 14:00:01 - train: epoch 0149, iter [02500, 05004], lr: 0.016543, loss: 2.2997
2022-03-08 14:00:35 - train: epoch 0149, iter [02600, 05004], lr: 0.016543, loss: 2.4362
2022-03-08 14:01:08 - train: epoch 0149, iter [02700, 05004], lr: 0.016543, loss: 2.5872
2022-03-08 14:01:43 - train: epoch 0149, iter [02800, 05004], lr: 0.016543, loss: 2.0638
2022-03-08 14:02:16 - train: epoch 0149, iter [02900, 05004], lr: 0.016543, loss: 2.2595
2022-03-08 14:02:50 - train: epoch 0149, iter [03000, 05004], lr: 0.016543, loss: 2.3328
2022-03-08 14:03:23 - train: epoch 0149, iter [03100, 05004], lr: 0.016543, loss: 2.2354
2022-03-08 14:03:57 - train: epoch 0149, iter [03200, 05004], lr: 0.016543, loss: 2.1936
2022-03-08 14:04:31 - train: epoch 0149, iter [03300, 05004], lr: 0.016543, loss: 2.2927
2022-03-08 14:05:05 - train: epoch 0149, iter [03400, 05004], lr: 0.016543, loss: 2.3096
2022-03-08 14:05:38 - train: epoch 0149, iter [03500, 05004], lr: 0.016543, loss: 2.4339
2022-03-08 14:06:12 - train: epoch 0149, iter [03600, 05004], lr: 0.016543, loss: 2.3735
2022-03-08 14:06:45 - train: epoch 0149, iter [03700, 05004], lr: 0.016543, loss: 1.8748
2022-03-08 14:07:19 - train: epoch 0149, iter [03800, 05004], lr: 0.016543, loss: 2.2680
2022-03-08 14:07:53 - train: epoch 0149, iter [03900, 05004], lr: 0.016543, loss: 2.4088
2022-03-08 14:08:28 - train: epoch 0149, iter [04000, 05004], lr: 0.016543, loss: 2.4181
2022-03-08 14:09:01 - train: epoch 0149, iter [04100, 05004], lr: 0.016543, loss: 2.3093
2022-03-08 14:09:36 - train: epoch 0149, iter [04200, 05004], lr: 0.016543, loss: 1.8061
2022-03-08 14:10:08 - train: epoch 0149, iter [04300, 05004], lr: 0.016543, loss: 2.4083
2022-03-08 14:10:43 - train: epoch 0149, iter [04400, 05004], lr: 0.016543, loss: 2.5934
2022-03-08 14:11:16 - train: epoch 0149, iter [04500, 05004], lr: 0.016543, loss: 2.2898
2022-03-08 14:11:51 - train: epoch 0149, iter [04600, 05004], lr: 0.016543, loss: 2.2159
2022-03-08 14:12:25 - train: epoch 0149, iter [04700, 05004], lr: 0.016543, loss: 2.3865
2022-03-08 14:13:01 - train: epoch 0149, iter [04800, 05004], lr: 0.016543, loss: 2.2801
2022-03-08 14:13:34 - train: epoch 0149, iter [04900, 05004], lr: 0.016543, loss: 2.2280
2022-03-08 14:14:07 - train: epoch 0149, iter [05000, 05004], lr: 0.016543, loss: 2.1686
2022-03-08 14:14:08 - train: epoch 149, train_loss: 2.3269
2022-03-08 14:15:22 - eval: epoch: 149, acc1: 68.352%, acc5: 88.824%, test_loss: 1.2862, per_image_load_time: 0.810ms, per_image_inference_time: 0.513ms
2022-03-08 14:15:23 - until epoch: 149, best_acc1: 68.366%
2022-03-08 14:15:23 - epoch 150 lr: 0.015949207060660137
2022-03-08 14:16:02 - train: epoch 0150, iter [00100, 05004], lr: 0.015949, loss: 2.1920
2022-03-08 14:16:36 - train: epoch 0150, iter [00200, 05004], lr: 0.015949, loss: 2.2113
2022-03-08 14:17:10 - train: epoch 0150, iter [00300, 05004], lr: 0.015949, loss: 2.4499
2022-03-08 14:17:44 - train: epoch 0150, iter [00400, 05004], lr: 0.015949, loss: 2.2980
2022-03-08 14:18:18 - train: epoch 0150, iter [00500, 05004], lr: 0.015949, loss: 2.1541
2022-03-08 14:18:52 - train: epoch 0150, iter [00600, 05004], lr: 0.015949, loss: 2.1271
2022-03-08 14:19:26 - train: epoch 0150, iter [00700, 05004], lr: 0.015949, loss: 2.1472
2022-03-08 14:20:00 - train: epoch 0150, iter [00800, 05004], lr: 0.015949, loss: 2.4785
2022-03-08 14:20:35 - train: epoch 0150, iter [00900, 05004], lr: 0.015949, loss: 2.2315
2022-03-08 14:21:09 - train: epoch 0150, iter [01000, 05004], lr: 0.015949, loss: 2.4322
2022-03-08 14:21:43 - train: epoch 0150, iter [01100, 05004], lr: 0.015949, loss: 2.6633
2022-03-08 14:22:16 - train: epoch 0150, iter [01200, 05004], lr: 0.015949, loss: 2.1210
2022-03-08 14:22:51 - train: epoch 0150, iter [01300, 05004], lr: 0.015949, loss: 2.4297
2022-03-08 14:23:26 - train: epoch 0150, iter [01400, 05004], lr: 0.015949, loss: 2.0926
2022-03-08 14:23:59 - train: epoch 0150, iter [01500, 05004], lr: 0.015949, loss: 2.0559
2022-03-08 14:24:34 - train: epoch 0150, iter [01600, 05004], lr: 0.015949, loss: 2.3659
2022-03-08 14:25:09 - train: epoch 0150, iter [01700, 05004], lr: 0.015949, loss: 2.4211
2022-03-08 14:25:43 - train: epoch 0150, iter [01800, 05004], lr: 0.015949, loss: 2.6642
2022-03-08 14:26:17 - train: epoch 0150, iter [01900, 05004], lr: 0.015949, loss: 2.1058
2022-03-08 14:26:51 - train: epoch 0150, iter [02000, 05004], lr: 0.015949, loss: 2.3161
2022-03-08 14:27:25 - train: epoch 0150, iter [02100, 05004], lr: 0.015949, loss: 2.1174
2022-03-08 14:28:00 - train: epoch 0150, iter [02200, 05004], lr: 0.015949, loss: 2.2869
2022-03-08 14:28:34 - train: epoch 0150, iter [02300, 05004], lr: 0.015949, loss: 2.5586
2022-03-08 14:29:08 - train: epoch 0150, iter [02400, 05004], lr: 0.015949, loss: 2.6337
2022-03-08 14:29:42 - train: epoch 0150, iter [02500, 05004], lr: 0.015949, loss: 2.1493
2022-03-08 14:30:17 - train: epoch 0150, iter [02600, 05004], lr: 0.015949, loss: 2.1851
2022-03-08 14:30:51 - train: epoch 0150, iter [02700, 05004], lr: 0.015949, loss: 2.1578
2022-03-08 14:31:25 - train: epoch 0150, iter [02800, 05004], lr: 0.015949, loss: 2.3679
2022-03-08 14:31:59 - train: epoch 0150, iter [02900, 05004], lr: 0.015949, loss: 2.0762
2022-03-08 14:32:34 - train: epoch 0150, iter [03000, 05004], lr: 0.015949, loss: 2.2033
2022-03-08 14:33:09 - train: epoch 0150, iter [03100, 05004], lr: 0.015949, loss: 2.3711
2022-03-08 14:33:42 - train: epoch 0150, iter [03200, 05004], lr: 0.015949, loss: 2.3346
2022-03-08 14:34:16 - train: epoch 0150, iter [03300, 05004], lr: 0.015949, loss: 2.4429
2022-03-08 14:34:50 - train: epoch 0150, iter [03400, 05004], lr: 0.015949, loss: 2.4078
2022-03-08 14:35:25 - train: epoch 0150, iter [03500, 05004], lr: 0.015949, loss: 2.3146
2022-03-08 14:35:59 - train: epoch 0150, iter [03600, 05004], lr: 0.015949, loss: 2.2796
2022-03-08 14:36:34 - train: epoch 0150, iter [03700, 05004], lr: 0.015949, loss: 2.5407
2022-03-08 14:37:08 - train: epoch 0150, iter [03800, 05004], lr: 0.015949, loss: 2.1718
2022-03-08 14:37:43 - train: epoch 0150, iter [03900, 05004], lr: 0.015949, loss: 2.5655
2022-03-08 14:38:17 - train: epoch 0150, iter [04000, 05004], lr: 0.015949, loss: 2.1313
2022-03-08 14:38:51 - train: epoch 0150, iter [04100, 05004], lr: 0.015949, loss: 2.2521
2022-03-08 14:39:26 - train: epoch 0150, iter [04200, 05004], lr: 0.015949, loss: 2.6462
2022-03-08 14:40:00 - train: epoch 0150, iter [04300, 05004], lr: 0.015949, loss: 2.1438
2022-03-08 14:40:34 - train: epoch 0150, iter [04400, 05004], lr: 0.015949, loss: 2.4125
2022-03-08 14:41:09 - train: epoch 0150, iter [04500, 05004], lr: 0.015949, loss: 2.2068
2022-03-08 14:41:42 - train: epoch 0150, iter [04600, 05004], lr: 0.015949, loss: 2.4214
2022-03-08 14:42:17 - train: epoch 0150, iter [04700, 05004], lr: 0.015949, loss: 2.2705
2022-03-08 14:42:51 - train: epoch 0150, iter [04800, 05004], lr: 0.015949, loss: 2.4277
2022-03-08 14:43:25 - train: epoch 0150, iter [04900, 05004], lr: 0.015949, loss: 2.2672
2022-03-08 14:43:58 - train: epoch 0150, iter [05000, 05004], lr: 0.015949, loss: 2.1146
2022-03-08 14:43:59 - train: epoch 150, train_loss: 2.3160
2022-03-08 14:45:13 - eval: epoch: 150, acc1: 68.626%, acc5: 89.016%, test_loss: 1.2679, per_image_load_time: 2.005ms, per_image_inference_time: 0.527ms
2022-03-08 14:45:14 - until epoch: 150, best_acc1: 68.626%
2022-03-08 14:45:14 - epoch 151 lr: 0.015363782324520032
2022-03-08 14:45:54 - train: epoch 0151, iter [00100, 05004], lr: 0.015364, loss: 2.3068
2022-03-08 14:46:27 - train: epoch 0151, iter [00200, 05004], lr: 0.015364, loss: 2.3585
2022-03-08 14:47:02 - train: epoch 0151, iter [00300, 05004], lr: 0.015364, loss: 2.1403
2022-03-08 14:47:37 - train: epoch 0151, iter [00400, 05004], lr: 0.015364, loss: 2.3226
2022-03-08 14:48:11 - train: epoch 0151, iter [00500, 05004], lr: 0.015364, loss: 2.2613
2022-03-08 14:48:45 - train: epoch 0151, iter [00600, 05004], lr: 0.015364, loss: 2.3037
2022-03-08 14:49:20 - train: epoch 0151, iter [00700, 05004], lr: 0.015364, loss: 2.2131
2022-03-08 14:49:54 - train: epoch 0151, iter [00800, 05004], lr: 0.015364, loss: 2.2923
2022-03-08 14:50:29 - train: epoch 0151, iter [00900, 05004], lr: 0.015364, loss: 2.4493
2022-03-08 14:51:03 - train: epoch 0151, iter [01000, 05004], lr: 0.015364, loss: 2.4461
2022-03-08 14:51:37 - train: epoch 0151, iter [01100, 05004], lr: 0.015364, loss: 2.1607
2022-03-08 14:52:11 - train: epoch 0151, iter [01200, 05004], lr: 0.015364, loss: 2.1577
2022-03-08 14:52:45 - train: epoch 0151, iter [01300, 05004], lr: 0.015364, loss: 2.2584
2022-03-08 14:53:20 - train: epoch 0151, iter [01400, 05004], lr: 0.015364, loss: 2.2857
2022-03-08 14:53:54 - train: epoch 0151, iter [01500, 05004], lr: 0.015364, loss: 2.3331
2022-03-08 14:54:29 - train: epoch 0151, iter [01600, 05004], lr: 0.015364, loss: 2.1461
2022-03-08 14:55:03 - train: epoch 0151, iter [01700, 05004], lr: 0.015364, loss: 2.1173
2022-03-08 14:55:37 - train: epoch 0151, iter [01800, 05004], lr: 0.015364, loss: 2.3521
2022-03-08 14:56:12 - train: epoch 0151, iter [01900, 05004], lr: 0.015364, loss: 2.6347
2022-03-08 14:56:45 - train: epoch 0151, iter [02000, 05004], lr: 0.015364, loss: 2.4689
2022-03-08 14:57:21 - train: epoch 0151, iter [02100, 05004], lr: 0.015364, loss: 2.0165
2022-03-08 14:57:55 - train: epoch 0151, iter [02200, 05004], lr: 0.015364, loss: 2.2695
2022-03-08 14:58:29 - train: epoch 0151, iter [02300, 05004], lr: 0.015364, loss: 2.1934
2022-03-08 14:59:04 - train: epoch 0151, iter [02400, 05004], lr: 0.015364, loss: 2.2972
2022-03-08 14:59:38 - train: epoch 0151, iter [02500, 05004], lr: 0.015364, loss: 2.3806
2022-03-08 15:00:13 - train: epoch 0151, iter [02600, 05004], lr: 0.015364, loss: 2.1396
2022-03-08 15:00:47 - train: epoch 0151, iter [02700, 05004], lr: 0.015364, loss: 2.1170
2022-03-08 15:01:21 - train: epoch 0151, iter [02800, 05004], lr: 0.015364, loss: 2.2332
2022-03-08 15:01:55 - train: epoch 0151, iter [02900, 05004], lr: 0.015364, loss: 2.3668
2022-03-08 15:02:30 - train: epoch 0151, iter [03000, 05004], lr: 0.015364, loss: 2.3758
2022-03-08 15:03:04 - train: epoch 0151, iter [03100, 05004], lr: 0.015364, loss: 2.2347
2022-03-08 15:03:38 - train: epoch 0151, iter [03200, 05004], lr: 0.015364, loss: 2.2666
2022-03-08 15:04:12 - train: epoch 0151, iter [03300, 05004], lr: 0.015364, loss: 2.1012
2022-03-08 15:04:46 - train: epoch 0151, iter [03400, 05004], lr: 0.015364, loss: 2.0798
2022-03-08 15:05:21 - train: epoch 0151, iter [03500, 05004], lr: 0.015364, loss: 2.5845
2022-03-08 15:05:55 - train: epoch 0151, iter [03600, 05004], lr: 0.015364, loss: 2.3937
2022-03-08 15:06:30 - train: epoch 0151, iter [03700, 05004], lr: 0.015364, loss: 2.1779
2022-03-08 15:07:04 - train: epoch 0151, iter [03800, 05004], lr: 0.015364, loss: 2.1918
2022-03-08 15:07:38 - train: epoch 0151, iter [03900, 05004], lr: 0.015364, loss: 2.3760
2022-03-08 15:08:13 - train: epoch 0151, iter [04000, 05004], lr: 0.015364, loss: 2.3131
2022-03-08 15:08:47 - train: epoch 0151, iter [04100, 05004], lr: 0.015364, loss: 2.5678
2022-03-08 15:09:21 - train: epoch 0151, iter [04200, 05004], lr: 0.015364, loss: 2.3209
2022-03-08 15:09:55 - train: epoch 0151, iter [04300, 05004], lr: 0.015364, loss: 2.4125
2022-03-08 15:10:30 - train: epoch 0151, iter [04400, 05004], lr: 0.015364, loss: 2.3239
2022-03-08 15:11:04 - train: epoch 0151, iter [04500, 05004], lr: 0.015364, loss: 2.0634
2022-03-08 15:11:39 - train: epoch 0151, iter [04600, 05004], lr: 0.015364, loss: 2.3797
2022-03-08 15:12:13 - train: epoch 0151, iter [04700, 05004], lr: 0.015364, loss: 1.9356
2022-03-08 15:12:48 - train: epoch 0151, iter [04800, 05004], lr: 0.015364, loss: 2.4874
2022-03-08 15:13:22 - train: epoch 0151, iter [04900, 05004], lr: 0.015364, loss: 2.3437
2022-03-08 15:13:55 - train: epoch 0151, iter [05000, 05004], lr: 0.015364, loss: 2.3767
2022-03-08 15:13:56 - train: epoch 151, train_loss: 2.3054
2022-03-08 15:15:11 - eval: epoch: 151, acc1: 69.246%, acc5: 89.164%, test_loss: 1.2557, per_image_load_time: 1.227ms, per_image_inference_time: 0.560ms
2022-03-08 15:15:11 - until epoch: 151, best_acc1: 69.246%
2022-03-08 15:15:11 - epoch 152 lr: 0.01478734742066054
2022-03-08 15:15:52 - train: epoch 0152, iter [00100, 05004], lr: 0.014787, loss: 2.3719
2022-03-08 15:16:26 - train: epoch 0152, iter [00200, 05004], lr: 0.014787, loss: 2.3887
2022-03-08 15:17:00 - train: epoch 0152, iter [00300, 05004], lr: 0.014787, loss: 2.2918
2022-03-08 15:17:34 - train: epoch 0152, iter [00400, 05004], lr: 0.014787, loss: 2.2538
2022-03-08 15:18:08 - train: epoch 0152, iter [00500, 05004], lr: 0.014787, loss: 2.3813
2022-03-08 15:18:42 - train: epoch 0152, iter [00600, 05004], lr: 0.014787, loss: 2.1094
2022-03-08 15:19:16 - train: epoch 0152, iter [00700, 05004], lr: 0.014787, loss: 2.4071
2022-03-08 15:19:49 - train: epoch 0152, iter [00800, 05004], lr: 0.014787, loss: 2.3205
2022-03-08 15:20:24 - train: epoch 0152, iter [00900, 05004], lr: 0.014787, loss: 2.3995
2022-03-08 15:20:58 - train: epoch 0152, iter [01000, 05004], lr: 0.014787, loss: 2.3429
2022-03-08 15:21:33 - train: epoch 0152, iter [01100, 05004], lr: 0.014787, loss: 2.3412
2022-03-08 15:22:07 - train: epoch 0152, iter [01200, 05004], lr: 0.014787, loss: 1.9899
2022-03-08 15:22:41 - train: epoch 0152, iter [01300, 05004], lr: 0.014787, loss: 2.2549
2022-03-08 15:23:14 - train: epoch 0152, iter [01400, 05004], lr: 0.014787, loss: 2.5084
2022-03-08 15:23:48 - train: epoch 0152, iter [01500, 05004], lr: 0.014787, loss: 2.3488
2022-03-08 15:24:23 - train: epoch 0152, iter [01600, 05004], lr: 0.014787, loss: 2.1716
2022-03-08 15:24:57 - train: epoch 0152, iter [01700, 05004], lr: 0.014787, loss: 2.4233
2022-03-08 15:25:31 - train: epoch 0152, iter [01800, 05004], lr: 0.014787, loss: 2.1189
2022-03-08 15:26:05 - train: epoch 0152, iter [01900, 05004], lr: 0.014787, loss: 2.2653
2022-03-08 15:26:39 - train: epoch 0152, iter [02000, 05004], lr: 0.014787, loss: 2.3097
2022-03-08 15:27:14 - train: epoch 0152, iter [02100, 05004], lr: 0.014787, loss: 1.8950
2022-03-08 15:27:48 - train: epoch 0152, iter [02200, 05004], lr: 0.014787, loss: 2.4399
2022-03-08 15:28:22 - train: epoch 0152, iter [02300, 05004], lr: 0.014787, loss: 2.2027
2022-03-08 15:28:56 - train: epoch 0152, iter [02400, 05004], lr: 0.014787, loss: 2.3331
2022-03-08 15:29:30 - train: epoch 0152, iter [02500, 05004], lr: 0.014787, loss: 2.4925
2022-03-08 15:30:05 - train: epoch 0152, iter [02600, 05004], lr: 0.014787, loss: 2.4167
2022-03-08 15:30:39 - train: epoch 0152, iter [02700, 05004], lr: 0.014787, loss: 2.2955
2022-03-08 15:31:12 - train: epoch 0152, iter [02800, 05004], lr: 0.014787, loss: 2.1022
2022-03-08 15:31:47 - train: epoch 0152, iter [02900, 05004], lr: 0.014787, loss: 2.3520
2022-03-08 15:32:21 - train: epoch 0152, iter [03000, 05004], lr: 0.014787, loss: 2.2824
2022-03-08 15:32:55 - train: epoch 0152, iter [03100, 05004], lr: 0.014787, loss: 2.3870
2022-03-08 15:33:28 - train: epoch 0152, iter [03200, 05004], lr: 0.014787, loss: 2.2685
2022-03-08 15:34:02 - train: epoch 0152, iter [03300, 05004], lr: 0.014787, loss: 2.6223
2022-03-08 15:34:36 - train: epoch 0152, iter [03400, 05004], lr: 0.014787, loss: 2.1797
2022-03-08 15:35:10 - train: epoch 0152, iter [03500, 05004], lr: 0.014787, loss: 2.0259
2022-03-08 15:35:45 - train: epoch 0152, iter [03600, 05004], lr: 0.014787, loss: 2.2870
2022-03-08 15:36:18 - train: epoch 0152, iter [03700, 05004], lr: 0.014787, loss: 2.2011
2022-03-08 15:36:53 - train: epoch 0152, iter [03800, 05004], lr: 0.014787, loss: 2.1776
2022-03-08 15:37:26 - train: epoch 0152, iter [03900, 05004], lr: 0.014787, loss: 2.2115
2022-03-08 15:38:00 - train: epoch 0152, iter [04000, 05004], lr: 0.014787, loss: 2.0265
2022-03-08 15:38:35 - train: epoch 0152, iter [04100, 05004], lr: 0.014787, loss: 2.3198
2022-03-08 15:39:08 - train: epoch 0152, iter [04200, 05004], lr: 0.014787, loss: 2.3351
2022-03-08 15:39:43 - train: epoch 0152, iter [04300, 05004], lr: 0.014787, loss: 2.2821
2022-03-08 15:40:16 - train: epoch 0152, iter [04400, 05004], lr: 0.014787, loss: 2.4691
2022-03-08 15:40:51 - train: epoch 0152, iter [04500, 05004], lr: 0.014787, loss: 2.4373
2022-03-08 15:41:25 - train: epoch 0152, iter [04600, 05004], lr: 0.014787, loss: 2.2081
2022-03-08 15:41:59 - train: epoch 0152, iter [04700, 05004], lr: 0.014787, loss: 2.1814
2022-03-08 15:42:33 - train: epoch 0152, iter [04800, 05004], lr: 0.014787, loss: 2.3608
2022-03-08 15:43:07 - train: epoch 0152, iter [04900, 05004], lr: 0.014787, loss: 2.4526
2022-03-08 15:43:40 - train: epoch 0152, iter [05000, 05004], lr: 0.014787, loss: 2.1186
2022-03-08 15:43:41 - train: epoch 152, train_loss: 2.2959
2022-03-08 15:44:56 - eval: epoch: 152, acc1: 69.354%, acc5: 89.434%, test_loss: 1.2362, per_image_load_time: 1.539ms, per_image_inference_time: 0.533ms
2022-03-08 15:44:57 - until epoch: 152, best_acc1: 69.354%
2022-03-08 15:44:57 - epoch 153 lr: 0.014220051962793951
2022-03-08 15:45:36 - train: epoch 0153, iter [00100, 05004], lr: 0.014220, loss: 2.5528
2022-03-08 15:46:11 - train: epoch 0153, iter [00200, 05004], lr: 0.014220, loss: 2.0653
2022-03-08 15:46:44 - train: epoch 0153, iter [00300, 05004], lr: 0.014220, loss: 2.2841
2022-03-08 15:47:19 - train: epoch 0153, iter [00400, 05004], lr: 0.014220, loss: 2.1783
2022-03-08 15:47:53 - train: epoch 0153, iter [00500, 05004], lr: 0.014220, loss: 2.3816
2022-03-08 15:48:27 - train: epoch 0153, iter [00600, 05004], lr: 0.014220, loss: 2.3491
2022-03-08 15:49:01 - train: epoch 0153, iter [00700, 05004], lr: 0.014220, loss: 2.1332
2022-03-08 15:49:36 - train: epoch 0153, iter [00800, 05004], lr: 0.014220, loss: 2.1242
2022-03-08 15:50:10 - train: epoch 0153, iter [00900, 05004], lr: 0.014220, loss: 2.5284
2022-03-08 15:50:44 - train: epoch 0153, iter [01000, 05004], lr: 0.014220, loss: 2.1669
2022-03-08 15:51:18 - train: epoch 0153, iter [01100, 05004], lr: 0.014220, loss: 2.0601
2022-03-08 15:51:53 - train: epoch 0153, iter [01200, 05004], lr: 0.014220, loss: 2.1724
2022-03-08 15:52:27 - train: epoch 0153, iter [01300, 05004], lr: 0.014220, loss: 2.4011
2022-03-08 15:53:02 - train: epoch 0153, iter [01400, 05004], lr: 0.014220, loss: 2.3534
2022-03-08 15:53:35 - train: epoch 0153, iter [01500, 05004], lr: 0.014220, loss: 2.4093
2022-03-08 15:54:11 - train: epoch 0153, iter [01600, 05004], lr: 0.014220, loss: 2.3174
2022-03-08 15:54:44 - train: epoch 0153, iter [01700, 05004], lr: 0.014220, loss: 2.2865
2022-03-08 15:55:18 - train: epoch 0153, iter [01800, 05004], lr: 0.014220, loss: 2.2213
2022-03-08 15:55:54 - train: epoch 0153, iter [01900, 05004], lr: 0.014220, loss: 2.0348
2022-03-08 15:56:28 - train: epoch 0153, iter [02000, 05004], lr: 0.014220, loss: 2.1823
2022-03-08 15:57:02 - train: epoch 0153, iter [02100, 05004], lr: 0.014220, loss: 2.2914
2022-03-08 15:57:37 - train: epoch 0153, iter [02200, 05004], lr: 0.014220, loss: 2.1569
2022-03-08 15:58:11 - train: epoch 0153, iter [02300, 05004], lr: 0.014220, loss: 2.2211
2022-03-08 15:58:45 - train: epoch 0153, iter [02400, 05004], lr: 0.014220, loss: 2.5751
2022-03-08 15:59:20 - train: epoch 0153, iter [02500, 05004], lr: 0.014220, loss: 2.2284
2022-03-08 15:59:54 - train: epoch 0153, iter [02600, 05004], lr: 0.014220, loss: 2.3018
2022-03-08 16:00:28 - train: epoch 0153, iter [02700, 05004], lr: 0.014220, loss: 2.2843
2022-03-08 16:01:02 - train: epoch 0153, iter [02800, 05004], lr: 0.014220, loss: 2.4764
2022-03-08 16:01:36 - train: epoch 0153, iter [02900, 05004], lr: 0.014220, loss: 2.3778
2022-03-08 16:02:11 - train: epoch 0153, iter [03000, 05004], lr: 0.014220, loss: 2.3979
2022-03-08 16:02:45 - train: epoch 0153, iter [03100, 05004], lr: 0.014220, loss: 2.3063
2022-03-08 16:03:18 - train: epoch 0153, iter [03200, 05004], lr: 0.014220, loss: 2.4193
2022-03-08 16:03:53 - train: epoch 0153, iter [03300, 05004], lr: 0.014220, loss: 2.2691
2022-03-08 16:04:28 - train: epoch 0153, iter [03400, 05004], lr: 0.014220, loss: 2.2679
2022-03-08 16:05:01 - train: epoch 0153, iter [03500, 05004], lr: 0.014220, loss: 2.3147
2022-03-08 16:05:36 - train: epoch 0153, iter [03600, 05004], lr: 0.014220, loss: 2.2536
2022-03-08 16:06:10 - train: epoch 0153, iter [03700, 05004], lr: 0.014220, loss: 2.1568
2022-03-08 16:06:45 - train: epoch 0153, iter [03800, 05004], lr: 0.014220, loss: 2.2429
2022-03-08 16:07:18 - train: epoch 0153, iter [03900, 05004], lr: 0.014220, loss: 2.1713
2022-03-08 16:07:53 - train: epoch 0153, iter [04000, 05004], lr: 0.014220, loss: 2.4441
2022-03-08 16:08:27 - train: epoch 0153, iter [04100, 05004], lr: 0.014220, loss: 2.1132
2022-03-08 16:09:01 - train: epoch 0153, iter [04200, 05004], lr: 0.014220, loss: 2.5037
2022-03-08 16:09:35 - train: epoch 0153, iter [04300, 05004], lr: 0.014220, loss: 2.4589
2022-03-08 16:10:10 - train: epoch 0153, iter [04400, 05004], lr: 0.014220, loss: 2.2955
2022-03-08 16:10:44 - train: epoch 0153, iter [04500, 05004], lr: 0.014220, loss: 2.2614
2022-03-08 16:11:18 - train: epoch 0153, iter [04600, 05004], lr: 0.014220, loss: 2.1413
2022-03-08 16:11:53 - train: epoch 0153, iter [04700, 05004], lr: 0.014220, loss: 2.1949
2022-03-08 16:12:27 - train: epoch 0153, iter [04800, 05004], lr: 0.014220, loss: 2.2721
2022-03-08 16:13:01 - train: epoch 0153, iter [04900, 05004], lr: 0.014220, loss: 2.2293
2022-03-08 16:13:33 - train: epoch 0153, iter [05000, 05004], lr: 0.014220, loss: 2.2301
2022-03-08 16:13:34 - train: epoch 153, train_loss: 2.2828
2022-03-08 16:14:48 - eval: epoch: 153, acc1: 69.616%, acc5: 89.522%, test_loss: 1.2249, per_image_load_time: 1.180ms, per_image_inference_time: 0.542ms
2022-03-08 16:14:49 - until epoch: 153, best_acc1: 69.616%
2022-03-08 16:14:49 - epoch 154 lr: 0.01366204319248885
2022-03-08 16:15:29 - train: epoch 0154, iter [00100, 05004], lr: 0.013662, loss: 2.0306
2022-03-08 16:16:04 - train: epoch 0154, iter [00200, 05004], lr: 0.013662, loss: 2.3820
2022-03-08 16:16:36 - train: epoch 0154, iter [00300, 05004], lr: 0.013662, loss: 2.2760
2022-03-08 16:17:11 - train: epoch 0154, iter [00400, 05004], lr: 0.013662, loss: 2.1143
2022-03-08 16:17:44 - train: epoch 0154, iter [00500, 05004], lr: 0.013662, loss: 2.3032
2022-03-08 16:18:19 - train: epoch 0154, iter [00600, 05004], lr: 0.013662, loss: 2.2063
2022-03-08 16:18:53 - train: epoch 0154, iter [00700, 05004], lr: 0.013662, loss: 2.4251
2022-03-08 16:19:28 - train: epoch 0154, iter [00800, 05004], lr: 0.013662, loss: 2.0882
2022-03-08 16:20:02 - train: epoch 0154, iter [00900, 05004], lr: 0.013662, loss: 2.2813
2022-03-08 16:20:36 - train: epoch 0154, iter [01000, 05004], lr: 0.013662, loss: 2.1329
2022-03-08 16:21:11 - train: epoch 0154, iter [01100, 05004], lr: 0.013662, loss: 2.3574
2022-03-08 16:21:45 - train: epoch 0154, iter [01200, 05004], lr: 0.013662, loss: 2.3696
2022-03-08 16:22:20 - train: epoch 0154, iter [01300, 05004], lr: 0.013662, loss: 2.2492
2022-03-08 16:22:54 - train: epoch 0154, iter [01400, 05004], lr: 0.013662, loss: 2.3315
2022-03-08 16:23:28 - train: epoch 0154, iter [01500, 05004], lr: 0.013662, loss: 2.4301
2022-03-08 16:24:02 - train: epoch 0154, iter [01600, 05004], lr: 0.013662, loss: 2.1589
2022-03-08 16:24:37 - train: epoch 0154, iter [01700, 05004], lr: 0.013662, loss: 2.3831
2022-03-08 16:25:11 - train: epoch 0154, iter [01800, 05004], lr: 0.013662, loss: 2.0329
2022-03-08 16:25:46 - train: epoch 0154, iter [01900, 05004], lr: 0.013662, loss: 2.0903
2022-03-08 16:26:21 - train: epoch 0154, iter [02000, 05004], lr: 0.013662, loss: 2.3368
2022-03-08 16:26:54 - train: epoch 0154, iter [02100, 05004], lr: 0.013662, loss: 2.1570
2022-03-08 16:27:29 - train: epoch 0154, iter [02200, 05004], lr: 0.013662, loss: 2.1781
2022-03-08 16:28:04 - train: epoch 0154, iter [02300, 05004], lr: 0.013662, loss: 2.1100
2022-03-08 16:28:38 - train: epoch 0154, iter [02400, 05004], lr: 0.013662, loss: 2.2375
2022-03-08 16:29:13 - train: epoch 0154, iter [02500, 05004], lr: 0.013662, loss: 2.3346
2022-03-08 16:29:46 - train: epoch 0154, iter [02600, 05004], lr: 0.013662, loss: 2.2964
2022-03-08 16:30:21 - train: epoch 0154, iter [02700, 05004], lr: 0.013662, loss: 2.2440
2022-03-08 16:30:55 - train: epoch 0154, iter [02800, 05004], lr: 0.013662, loss: 2.3478
2022-03-08 16:31:28 - train: epoch 0154, iter [02900, 05004], lr: 0.013662, loss: 2.3527
2022-03-08 16:32:04 - train: epoch 0154, iter [03000, 05004], lr: 0.013662, loss: 2.4760
2022-03-08 16:32:38 - train: epoch 0154, iter [03100, 05004], lr: 0.013662, loss: 2.0740
2022-03-08 16:33:12 - train: epoch 0154, iter [03200, 05004], lr: 0.013662, loss: 2.3558
2022-03-08 16:33:47 - train: epoch 0154, iter [03300, 05004], lr: 0.013662, loss: 2.2465
2022-03-08 16:34:21 - train: epoch 0154, iter [03400, 05004], lr: 0.013662, loss: 2.1477
2022-03-08 16:34:56 - train: epoch 0154, iter [03500, 05004], lr: 0.013662, loss: 2.4247
2022-03-08 16:35:29 - train: epoch 0154, iter [03600, 05004], lr: 0.013662, loss: 2.1676
2022-03-08 16:36:04 - train: epoch 0154, iter [03700, 05004], lr: 0.013662, loss: 2.1347
2022-03-08 16:36:39 - train: epoch 0154, iter [03800, 05004], lr: 0.013662, loss: 2.4839
2022-03-08 16:37:14 - train: epoch 0154, iter [03900, 05004], lr: 0.013662, loss: 2.3073
2022-03-08 16:37:47 - train: epoch 0154, iter [04000, 05004], lr: 0.013662, loss: 2.3233
2022-03-08 16:38:21 - train: epoch 0154, iter [04100, 05004], lr: 0.013662, loss: 2.3465
2022-03-08 16:38:56 - train: epoch 0154, iter [04200, 05004], lr: 0.013662, loss: 2.3691
2022-03-08 16:39:29 - train: epoch 0154, iter [04300, 05004], lr: 0.013662, loss: 2.5447
2022-03-08 16:40:04 - train: epoch 0154, iter [04400, 05004], lr: 0.013662, loss: 2.2245
2022-03-08 16:40:38 - train: epoch 0154, iter [04500, 05004], lr: 0.013662, loss: 2.3249
2022-03-08 16:41:11 - train: epoch 0154, iter [04600, 05004], lr: 0.013662, loss: 2.1465
2022-03-08 16:41:47 - train: epoch 0154, iter [04700, 05004], lr: 0.013662, loss: 2.2942
2022-03-08 16:42:21 - train: epoch 0154, iter [04800, 05004], lr: 0.013662, loss: 2.7233
2022-03-08 16:42:56 - train: epoch 0154, iter [04900, 05004], lr: 0.013662, loss: 2.6213
2022-03-08 16:43:28 - train: epoch 0154, iter [05000, 05004], lr: 0.013662, loss: 2.0915
2022-03-08 16:43:29 - train: epoch 154, train_loss: 2.2693
2022-03-08 16:44:44 - eval: epoch: 154, acc1: 69.550%, acc5: 89.284%, test_loss: 1.2334, per_image_load_time: 1.136ms, per_image_inference_time: 0.541ms
2022-03-08 16:44:44 - until epoch: 154, best_acc1: 69.616%
2022-03-08 16:44:44 - epoch 155 lr: 0.013113465940953495
2022-03-08 16:45:24 - train: epoch 0155, iter [00100, 05004], lr: 0.013113, loss: 2.3614
2022-03-08 16:45:58 - train: epoch 0155, iter [00200, 05004], lr: 0.013113, loss: 2.2791
2022-03-08 16:46:32 - train: epoch 0155, iter [00300, 05004], lr: 0.013113, loss: 2.4152
2022-03-08 16:47:06 - train: epoch 0155, iter [00400, 05004], lr: 0.013113, loss: 2.5621
2022-03-08 16:47:41 - train: epoch 0155, iter [00500, 05004], lr: 0.013113, loss: 2.6321
2022-03-08 16:48:15 - train: epoch 0155, iter [00600, 05004], lr: 0.013113, loss: 1.9746
2022-03-08 16:48:49 - train: epoch 0155, iter [00700, 05004], lr: 0.013113, loss: 2.3297
2022-03-08 16:49:23 - train: epoch 0155, iter [00800, 05004], lr: 0.013113, loss: 2.2638
2022-03-08 16:49:58 - train: epoch 0155, iter [00900, 05004], lr: 0.013113, loss: 2.1637
2022-03-08 16:50:31 - train: epoch 0155, iter [01000, 05004], lr: 0.013113, loss: 2.3801
2022-03-08 16:51:06 - train: epoch 0155, iter [01100, 05004], lr: 0.013113, loss: 2.2842
2022-03-08 16:51:40 - train: epoch 0155, iter [01200, 05004], lr: 0.013113, loss: 2.1676
2022-03-08 16:52:15 - train: epoch 0155, iter [01300, 05004], lr: 0.013113, loss: 2.1637
2022-03-08 16:52:50 - train: epoch 0155, iter [01400, 05004], lr: 0.013113, loss: 2.4688
2022-03-08 16:53:24 - train: epoch 0155, iter [01500, 05004], lr: 0.013113, loss: 2.1211
2022-03-08 16:53:58 - train: epoch 0155, iter [01600, 05004], lr: 0.013113, loss: 2.3818
2022-03-08 16:54:32 - train: epoch 0155, iter [01700, 05004], lr: 0.013113, loss: 2.1584
2022-03-08 16:55:07 - train: epoch 0155, iter [01800, 05004], lr: 0.013113, loss: 2.2050
2022-03-08 16:55:40 - train: epoch 0155, iter [01900, 05004], lr: 0.013113, loss: 2.1510
2022-03-08 16:56:15 - train: epoch 0155, iter [02000, 05004], lr: 0.013113, loss: 2.3405
2022-03-08 16:56:49 - train: epoch 0155, iter [02100, 05004], lr: 0.013113, loss: 1.9683
2022-03-08 16:57:24 - train: epoch 0155, iter [02200, 05004], lr: 0.013113, loss: 2.2716
2022-03-08 16:57:58 - train: epoch 0155, iter [02300, 05004], lr: 0.013113, loss: 2.0082
2022-03-08 16:58:32 - train: epoch 0155, iter [02400, 05004], lr: 0.013113, loss: 2.4092
2022-03-08 16:59:07 - train: epoch 0155, iter [02500, 05004], lr: 0.013113, loss: 2.3571
2022-03-08 16:59:41 - train: epoch 0155, iter [02600, 05004], lr: 0.013113, loss: 2.4939
2022-03-08 17:00:15 - train: epoch 0155, iter [02700, 05004], lr: 0.013113, loss: 2.4813
2022-03-08 17:00:49 - train: epoch 0155, iter [02800, 05004], lr: 0.013113, loss: 2.4162
2022-03-08 17:01:24 - train: epoch 0155, iter [02900, 05004], lr: 0.013113, loss: 2.1522
2022-03-08 17:01:58 - train: epoch 0155, iter [03000, 05004], lr: 0.013113, loss: 2.1126
2022-03-08 17:02:33 - train: epoch 0155, iter [03100, 05004], lr: 0.013113, loss: 2.2377
2022-03-08 17:03:07 - train: epoch 0155, iter [03200, 05004], lr: 0.013113, loss: 2.2340
2022-03-08 17:03:41 - train: epoch 0155, iter [03300, 05004], lr: 0.013113, loss: 2.3176
2022-03-08 17:04:14 - train: epoch 0155, iter [03400, 05004], lr: 0.013113, loss: 2.2499
2022-03-08 17:04:49 - train: epoch 0155, iter [03500, 05004], lr: 0.013113, loss: 2.3620
2022-03-08 17:05:23 - train: epoch 0155, iter [03600, 05004], lr: 0.013113, loss: 2.1513
2022-03-08 17:05:57 - train: epoch 0155, iter [03700, 05004], lr: 0.013113, loss: 2.1105
2022-03-08 17:06:31 - train: epoch 0155, iter [03800, 05004], lr: 0.013113, loss: 2.1576
2022-03-08 17:07:05 - train: epoch 0155, iter [03900, 05004], lr: 0.013113, loss: 2.1246
2022-03-08 17:07:40 - train: epoch 0155, iter [04000, 05004], lr: 0.013113, loss: 2.1001
2022-03-08 17:08:13 - train: epoch 0155, iter [04100, 05004], lr: 0.013113, loss: 2.3729
2022-03-08 17:08:47 - train: epoch 0155, iter [04200, 05004], lr: 0.013113, loss: 2.2332
2022-03-08 17:09:22 - train: epoch 0155, iter [04300, 05004], lr: 0.013113, loss: 2.4057
2022-03-08 17:09:56 - train: epoch 0155, iter [04400, 05004], lr: 0.013113, loss: 2.2263
2022-03-08 17:10:31 - train: epoch 0155, iter [04500, 05004], lr: 0.013113, loss: 2.0316
2022-03-08 17:11:05 - train: epoch 0155, iter [04600, 05004], lr: 0.013113, loss: 2.4705
2022-03-08 17:11:38 - train: epoch 0155, iter [04700, 05004], lr: 0.013113, loss: 2.3323
2022-03-08 17:12:14 - train: epoch 0155, iter [04800, 05004], lr: 0.013113, loss: 2.2982
2022-03-08 17:12:48 - train: epoch 0155, iter [04900, 05004], lr: 0.013113, loss: 2.4845
2022-03-08 17:13:21 - train: epoch 0155, iter [05000, 05004], lr: 0.013113, loss: 2.1522
2022-03-08 17:13:22 - train: epoch 155, train_loss: 2.2575
2022-03-08 17:14:37 - eval: epoch: 155, acc1: 69.984%, acc5: 89.692%, test_loss: 1.2118, per_image_load_time: 1.483ms, per_image_inference_time: 0.492ms
2022-03-08 17:14:38 - until epoch: 155, best_acc1: 69.984%
2022-03-08 17:14:38 - epoch 156 lr: 0.012574462591444941
2022-03-08 17:15:17 - train: epoch 0156, iter [00100, 05004], lr: 0.012574, loss: 2.2149
2022-03-08 17:15:51 - train: epoch 0156, iter [00200, 05004], lr: 0.012574, loss: 2.3759
2022-03-08 17:16:24 - train: epoch 0156, iter [00300, 05004], lr: 0.012574, loss: 2.3716
2022-03-08 17:16:59 - train: epoch 0156, iter [00400, 05004], lr: 0.012574, loss: 2.2687
2022-03-08 17:17:31 - train: epoch 0156, iter [00500, 05004], lr: 0.012574, loss: 2.2744
2022-03-08 17:18:06 - train: epoch 0156, iter [00600, 05004], lr: 0.012574, loss: 2.1042
2022-03-08 17:18:40 - train: epoch 0156, iter [00700, 05004], lr: 0.012574, loss: 2.2057
2022-03-08 17:19:14 - train: epoch 0156, iter [00800, 05004], lr: 0.012574, loss: 2.3860
2022-03-08 17:19:47 - train: epoch 0156, iter [00900, 05004], lr: 0.012574, loss: 2.3835
2022-03-08 17:20:22 - train: epoch 0156, iter [01000, 05004], lr: 0.012574, loss: 2.3017
2022-03-08 17:20:56 - train: epoch 0156, iter [01100, 05004], lr: 0.012574, loss: 2.4181
2022-03-08 17:21:31 - train: epoch 0156, iter [01200, 05004], lr: 0.012574, loss: 2.1650
2022-03-08 17:22:05 - train: epoch 0156, iter [01300, 05004], lr: 0.012574, loss: 2.1166
2022-03-08 17:22:39 - train: epoch 0156, iter [01400, 05004], lr: 0.012574, loss: 2.5290
2022-03-08 17:23:13 - train: epoch 0156, iter [01500, 05004], lr: 0.012574, loss: 2.3443
2022-03-08 17:23:48 - train: epoch 0156, iter [01600, 05004], lr: 0.012574, loss: 1.8752
2022-03-08 17:24:21 - train: epoch 0156, iter [01700, 05004], lr: 0.012574, loss: 2.4707
2022-03-08 17:24:56 - train: epoch 0156, iter [01800, 05004], lr: 0.012574, loss: 2.2688
2022-03-08 17:25:29 - train: epoch 0156, iter [01900, 05004], lr: 0.012574, loss: 2.0252
2022-03-08 17:26:04 - train: epoch 0156, iter [02000, 05004], lr: 0.012574, loss: 2.0953
2022-03-08 17:26:37 - train: epoch 0156, iter [02100, 05004], lr: 0.012574, loss: 2.1846
2022-03-08 17:27:10 - train: epoch 0156, iter [02200, 05004], lr: 0.012574, loss: 2.1637
2022-03-08 17:27:45 - train: epoch 0156, iter [02300, 05004], lr: 0.012574, loss: 2.2352
2022-03-08 17:28:18 - train: epoch 0156, iter [02400, 05004], lr: 0.012574, loss: 2.1170
2022-03-08 17:28:52 - train: epoch 0156, iter [02500, 05004], lr: 0.012574, loss: 2.2544
2022-03-08 17:29:26 - train: epoch 0156, iter [02600, 05004], lr: 0.012574, loss: 2.4483
2022-03-08 17:30:00 - train: epoch 0156, iter [02700, 05004], lr: 0.012574, loss: 1.9331
2022-03-08 17:30:34 - train: epoch 0156, iter [02800, 05004], lr: 0.012574, loss: 2.1690
2022-03-08 17:31:07 - train: epoch 0156, iter [02900, 05004], lr: 0.012574, loss: 2.1662
2022-03-08 17:31:40 - train: epoch 0156, iter [03000, 05004], lr: 0.012574, loss: 2.3322
2022-03-08 17:32:15 - train: epoch 0156, iter [03100, 05004], lr: 0.012574, loss: 2.2470
2022-03-08 17:32:48 - train: epoch 0156, iter [03200, 05004], lr: 0.012574, loss: 2.1246
2022-03-08 17:33:22 - train: epoch 0156, iter [03300, 05004], lr: 0.012574, loss: 2.2844
2022-03-08 17:33:55 - train: epoch 0156, iter [03400, 05004], lr: 0.012574, loss: 2.0239
2022-03-08 17:34:29 - train: epoch 0156, iter [03500, 05004], lr: 0.012574, loss: 2.2162
2022-03-08 17:35:02 - train: epoch 0156, iter [03600, 05004], lr: 0.012574, loss: 2.4523
2022-03-08 17:35:35 - train: epoch 0156, iter [03700, 05004], lr: 0.012574, loss: 2.5951
2022-03-08 17:36:10 - train: epoch 0156, iter [03800, 05004], lr: 0.012574, loss: 2.3994
2022-03-08 17:36:43 - train: epoch 0156, iter [03900, 05004], lr: 0.012574, loss: 2.5095
2022-03-08 17:37:17 - train: epoch 0156, iter [04000, 05004], lr: 0.012574, loss: 2.2197
2022-03-08 17:37:50 - train: epoch 0156, iter [04100, 05004], lr: 0.012574, loss: 2.3633
2022-03-08 17:38:25 - train: epoch 0156, iter [04200, 05004], lr: 0.012574, loss: 2.3482
2022-03-08 17:38:58 - train: epoch 0156, iter [04300, 05004], lr: 0.012574, loss: 2.2263
2022-03-08 17:39:31 - train: epoch 0156, iter [04400, 05004], lr: 0.012574, loss: 2.1815
2022-03-08 17:40:06 - train: epoch 0156, iter [04500, 05004], lr: 0.012574, loss: 2.3625
2022-03-08 17:40:40 - train: epoch 0156, iter [04600, 05004], lr: 0.012574, loss: 2.1747
2022-03-08 17:41:13 - train: epoch 0156, iter [04700, 05004], lr: 0.012574, loss: 2.2452
2022-03-08 17:41:47 - train: epoch 0156, iter [04800, 05004], lr: 0.012574, loss: 2.2178
2022-03-08 17:42:21 - train: epoch 0156, iter [04900, 05004], lr: 0.012574, loss: 2.0446
2022-03-08 17:42:53 - train: epoch 0156, iter [05000, 05004], lr: 0.012574, loss: 2.3107
2022-03-08 17:42:54 - train: epoch 156, train_loss: 2.2432
2022-03-08 17:44:08 - eval: epoch: 156, acc1: 70.186%, acc5: 89.886%, test_loss: 1.2048, per_image_load_time: 1.793ms, per_image_inference_time: 0.508ms
2022-03-08 17:44:09 - until epoch: 156, best_acc1: 70.186%
2022-03-08 17:44:09 - epoch 157 lr: 0.01204517304231343
2022-03-08 17:44:48 - train: epoch 0157, iter [00100, 05004], lr: 0.012045, loss: 2.1436
2022-03-08 17:45:22 - train: epoch 0157, iter [00200, 05004], lr: 0.012045, loss: 2.4878
2022-03-08 17:45:56 - train: epoch 0157, iter [00300, 05004], lr: 0.012045, loss: 2.2614
2022-03-08 17:46:30 - train: epoch 0157, iter [00400, 05004], lr: 0.012045, loss: 2.2245
2022-03-08 17:47:03 - train: epoch 0157, iter [00500, 05004], lr: 0.012045, loss: 2.1722
2022-03-08 17:47:37 - train: epoch 0157, iter [00600, 05004], lr: 0.012045, loss: 2.4158
2022-03-08 17:48:10 - train: epoch 0157, iter [00700, 05004], lr: 0.012045, loss: 2.2606
2022-03-08 17:48:44 - train: epoch 0157, iter [00800, 05004], lr: 0.012045, loss: 2.2657
2022-03-08 17:49:18 - train: epoch 0157, iter [00900, 05004], lr: 0.012045, loss: 2.5392
2022-03-08 17:49:52 - train: epoch 0157, iter [01000, 05004], lr: 0.012045, loss: 2.1691
2022-03-08 17:50:26 - train: epoch 0157, iter [01100, 05004], lr: 0.012045, loss: 1.9085
2022-03-08 17:51:00 - train: epoch 0157, iter [01200, 05004], lr: 0.012045, loss: 2.3751
2022-03-08 17:51:34 - train: epoch 0157, iter [01300, 05004], lr: 0.012045, loss: 2.2450
2022-03-08 17:52:07 - train: epoch 0157, iter [01400, 05004], lr: 0.012045, loss: 2.3328
2022-03-08 17:52:41 - train: epoch 0157, iter [01500, 05004], lr: 0.012045, loss: 2.0716
2022-03-08 17:53:15 - train: epoch 0157, iter [01600, 05004], lr: 0.012045, loss: 1.9290
2022-03-08 17:53:48 - train: epoch 0157, iter [01700, 05004], lr: 0.012045, loss: 2.1853
2022-03-08 17:54:21 - train: epoch 0157, iter [01800, 05004], lr: 0.012045, loss: 2.2288
2022-03-08 17:54:55 - train: epoch 0157, iter [01900, 05004], lr: 0.012045, loss: 2.5938
2022-03-08 17:55:28 - train: epoch 0157, iter [02000, 05004], lr: 0.012045, loss: 2.3807
2022-03-08 17:56:02 - train: epoch 0157, iter [02100, 05004], lr: 0.012045, loss: 2.3490
2022-03-08 17:56:36 - train: epoch 0157, iter [02200, 05004], lr: 0.012045, loss: 2.3620
2022-03-08 17:57:09 - train: epoch 0157, iter [02300, 05004], lr: 0.012045, loss: 2.3597
2022-03-08 17:57:43 - train: epoch 0157, iter [02400, 05004], lr: 0.012045, loss: 2.1612
2022-03-08 17:58:16 - train: epoch 0157, iter [02500, 05004], lr: 0.012045, loss: 2.2167
2022-03-08 17:58:50 - train: epoch 0157, iter [02600, 05004], lr: 0.012045, loss: 1.9669
2022-03-08 17:59:24 - train: epoch 0157, iter [02700, 05004], lr: 0.012045, loss: 2.0721
2022-03-08 17:59:58 - train: epoch 0157, iter [02800, 05004], lr: 0.012045, loss: 2.2453
2022-03-08 18:00:31 - train: epoch 0157, iter [02900, 05004], lr: 0.012045, loss: 2.0231
2022-03-08 18:01:05 - train: epoch 0157, iter [03000, 05004], lr: 0.012045, loss: 2.3909
2022-03-08 18:01:39 - train: epoch 0157, iter [03100, 05004], lr: 0.012045, loss: 2.2014
2022-03-08 18:02:13 - train: epoch 0157, iter [03200, 05004], lr: 0.012045, loss: 2.1901
2022-03-08 18:02:47 - train: epoch 0157, iter [03300, 05004], lr: 0.012045, loss: 2.1964
2022-03-08 18:03:20 - train: epoch 0157, iter [03400, 05004], lr: 0.012045, loss: 1.8637
2022-03-08 18:03:54 - train: epoch 0157, iter [03500, 05004], lr: 0.012045, loss: 2.3595
2022-03-08 18:04:28 - train: epoch 0157, iter [03600, 05004], lr: 0.012045, loss: 2.1551
2022-03-08 18:05:02 - train: epoch 0157, iter [03700, 05004], lr: 0.012045, loss: 2.1149
2022-03-08 18:05:37 - train: epoch 0157, iter [03800, 05004], lr: 0.012045, loss: 2.2601
2022-03-08 18:06:09 - train: epoch 0157, iter [03900, 05004], lr: 0.012045, loss: 2.1648
2022-03-08 18:06:42 - train: epoch 0157, iter [04000, 05004], lr: 0.012045, loss: 2.1297
2022-03-08 18:07:16 - train: epoch 0157, iter [04100, 05004], lr: 0.012045, loss: 2.2591
2022-03-08 18:07:50 - train: epoch 0157, iter [04200, 05004], lr: 0.012045, loss: 2.1309
2022-03-08 18:08:24 - train: epoch 0157, iter [04300, 05004], lr: 0.012045, loss: 2.2343
2022-03-08 18:08:58 - train: epoch 0157, iter [04400, 05004], lr: 0.012045, loss: 2.2779
2022-03-08 18:09:32 - train: epoch 0157, iter [04500, 05004], lr: 0.012045, loss: 2.2939
2022-03-08 18:10:06 - train: epoch 0157, iter [04600, 05004], lr: 0.012045, loss: 2.3978
2022-03-08 18:10:39 - train: epoch 0157, iter [04700, 05004], lr: 0.012045, loss: 2.2464
2022-03-08 18:11:13 - train: epoch 0157, iter [04800, 05004], lr: 0.012045, loss: 2.0467
2022-03-08 18:11:47 - train: epoch 0157, iter [04900, 05004], lr: 0.012045, loss: 2.1833
2022-03-08 18:12:19 - train: epoch 0157, iter [05000, 05004], lr: 0.012045, loss: 2.2602
2022-03-08 18:12:20 - train: epoch 157, train_loss: 2.2356
2022-03-08 18:13:34 - eval: epoch: 157, acc1: 69.930%, acc5: 89.928%, test_loss: 1.2039, per_image_load_time: 1.519ms, per_image_inference_time: 0.508ms
2022-03-08 18:13:34 - until epoch: 157, best_acc1: 70.186%
2022-03-08 18:13:34 - epoch 158 lr: 0.011525734670691701
2022-03-08 18:14:13 - train: epoch 0158, iter [00100, 05004], lr: 0.011526, loss: 2.4405
2022-03-08 18:14:48 - train: epoch 0158, iter [00200, 05004], lr: 0.011526, loss: 2.0862
2022-03-08 18:15:22 - train: epoch 0158, iter [00300, 05004], lr: 0.011526, loss: 1.9940
2022-03-08 18:15:55 - train: epoch 0158, iter [00400, 05004], lr: 0.011526, loss: 2.0815
2022-03-08 18:16:28 - train: epoch 0158, iter [00500, 05004], lr: 0.011526, loss: 1.9242
2022-03-08 18:17:02 - train: epoch 0158, iter [00600, 05004], lr: 0.011526, loss: 2.0868
2022-03-08 18:17:35 - train: epoch 0158, iter [00700, 05004], lr: 0.011526, loss: 2.0766
2022-03-08 18:18:09 - train: epoch 0158, iter [00800, 05004], lr: 0.011526, loss: 2.3623
2022-03-08 18:18:42 - train: epoch 0158, iter [00900, 05004], lr: 0.011526, loss: 2.2898
2022-03-08 18:19:15 - train: epoch 0158, iter [01000, 05004], lr: 0.011526, loss: 2.1079
2022-03-08 18:19:49 - train: epoch 0158, iter [01100, 05004], lr: 0.011526, loss: 1.9353
2022-03-08 18:20:22 - train: epoch 0158, iter [01200, 05004], lr: 0.011526, loss: 2.0055
2022-03-08 18:20:56 - train: epoch 0158, iter [01300, 05004], lr: 0.011526, loss: 2.1629
2022-03-08 18:21:30 - train: epoch 0158, iter [01400, 05004], lr: 0.011526, loss: 2.4212
2022-03-08 18:22:03 - train: epoch 0158, iter [01500, 05004], lr: 0.011526, loss: 2.1550
2022-03-08 18:22:36 - train: epoch 0158, iter [01600, 05004], lr: 0.011526, loss: 2.1684
2022-03-08 18:23:11 - train: epoch 0158, iter [01700, 05004], lr: 0.011526, loss: 2.2222
2022-03-08 18:23:43 - train: epoch 0158, iter [01800, 05004], lr: 0.011526, loss: 2.2784
2022-03-08 18:24:17 - train: epoch 0158, iter [01900, 05004], lr: 0.011526, loss: 2.3137
2022-03-08 18:24:52 - train: epoch 0158, iter [02000, 05004], lr: 0.011526, loss: 2.2412
2022-03-08 18:25:25 - train: epoch 0158, iter [02100, 05004], lr: 0.011526, loss: 2.3670
2022-03-08 18:25:59 - train: epoch 0158, iter [02200, 05004], lr: 0.011526, loss: 2.1318
2022-03-08 18:26:33 - train: epoch 0158, iter [02300, 05004], lr: 0.011526, loss: 2.2831
2022-03-08 18:27:06 - train: epoch 0158, iter [02400, 05004], lr: 0.011526, loss: 2.2303
2022-03-08 18:27:40 - train: epoch 0158, iter [02500, 05004], lr: 0.011526, loss: 2.1964
2022-03-08 18:28:13 - train: epoch 0158, iter [02600, 05004], lr: 0.011526, loss: 1.9806
2022-03-08 18:28:47 - train: epoch 0158, iter [02700, 05004], lr: 0.011526, loss: 2.3303
2022-03-08 18:29:20 - train: epoch 0158, iter [02800, 05004], lr: 0.011526, loss: 2.5010
2022-03-08 18:29:54 - train: epoch 0158, iter [02900, 05004], lr: 0.011526, loss: 2.3469
2022-03-08 18:30:27 - train: epoch 0158, iter [03000, 05004], lr: 0.011526, loss: 2.4282
2022-03-08 18:31:02 - train: epoch 0158, iter [03100, 05004], lr: 0.011526, loss: 1.9639
2022-03-08 18:31:36 - train: epoch 0158, iter [03200, 05004], lr: 0.011526, loss: 2.2044
2022-03-08 18:32:11 - train: epoch 0158, iter [03300, 05004], lr: 0.011526, loss: 2.0830
2022-03-08 18:32:45 - train: epoch 0158, iter [03400, 05004], lr: 0.011526, loss: 2.0958
2022-03-08 18:33:19 - train: epoch 0158, iter [03500, 05004], lr: 0.011526, loss: 2.0562
2022-03-08 18:33:54 - train: epoch 0158, iter [03600, 05004], lr: 0.011526, loss: 2.0977
2022-03-08 18:34:28 - train: epoch 0158, iter [03700, 05004], lr: 0.011526, loss: 2.4216
2022-03-08 18:35:03 - train: epoch 0158, iter [03800, 05004], lr: 0.011526, loss: 2.4678
2022-03-08 18:35:36 - train: epoch 0158, iter [03900, 05004], lr: 0.011526, loss: 2.3500
2022-03-08 18:36:11 - train: epoch 0158, iter [04000, 05004], lr: 0.011526, loss: 2.1493
2022-03-08 18:36:46 - train: epoch 0158, iter [04100, 05004], lr: 0.011526, loss: 2.2614
2022-03-08 18:37:20 - train: epoch 0158, iter [04200, 05004], lr: 0.011526, loss: 2.1978
2022-03-08 18:37:54 - train: epoch 0158, iter [04300, 05004], lr: 0.011526, loss: 2.2608
2022-03-08 18:38:28 - train: epoch 0158, iter [04400, 05004], lr: 0.011526, loss: 2.1445
2022-03-08 18:39:02 - train: epoch 0158, iter [04500, 05004], lr: 0.011526, loss: 2.4626
2022-03-08 18:39:37 - train: epoch 0158, iter [04600, 05004], lr: 0.011526, loss: 2.1214
2022-03-08 18:40:11 - train: epoch 0158, iter [04700, 05004], lr: 0.011526, loss: 2.0081
2022-03-08 18:40:45 - train: epoch 0158, iter [04800, 05004], lr: 0.011526, loss: 2.5189
2022-03-08 18:41:20 - train: epoch 0158, iter [04900, 05004], lr: 0.011526, loss: 2.1339
2022-03-08 18:41:53 - train: epoch 0158, iter [05000, 05004], lr: 0.011526, loss: 2.2445
2022-03-08 18:41:54 - train: epoch 158, train_loss: 2.2191
2022-03-08 18:43:09 - eval: epoch: 158, acc1: 70.610%, acc5: 90.076%, test_loss: 1.1813, per_image_load_time: 1.678ms, per_image_inference_time: 0.513ms
2022-03-08 18:43:09 - until epoch: 158, best_acc1: 70.610%
2022-03-08 18:43:09 - epoch 159 lr: 0.011016282296838887
2022-03-08 18:43:49 - train: epoch 0159, iter [00100, 05004], lr: 0.011016, loss: 2.1107
2022-03-08 18:44:24 - train: epoch 0159, iter [00200, 05004], lr: 0.011016, loss: 2.2630
2022-03-08 18:44:59 - train: epoch 0159, iter [00300, 05004], lr: 0.011016, loss: 2.4193
2022-03-08 18:45:34 - train: epoch 0159, iter [00400, 05004], lr: 0.011016, loss: 2.1096
2022-03-08 18:46:07 - train: epoch 0159, iter [00500, 05004], lr: 0.011016, loss: 2.4343
2022-03-08 18:46:41 - train: epoch 0159, iter [00600, 05004], lr: 0.011016, loss: 1.9851
2022-03-08 18:47:16 - train: epoch 0159, iter [00700, 05004], lr: 0.011016, loss: 2.4565
2022-03-08 18:47:50 - train: epoch 0159, iter [00800, 05004], lr: 0.011016, loss: 2.2952
2022-03-08 18:48:25 - train: epoch 0159, iter [00900, 05004], lr: 0.011016, loss: 1.8314
2022-03-08 18:49:00 - train: epoch 0159, iter [01000, 05004], lr: 0.011016, loss: 2.2432
2022-03-08 18:49:33 - train: epoch 0159, iter [01100, 05004], lr: 0.011016, loss: 2.2368
2022-03-08 18:50:07 - train: epoch 0159, iter [01200, 05004], lr: 0.011016, loss: 1.8916
2022-03-08 18:50:41 - train: epoch 0159, iter [01300, 05004], lr: 0.011016, loss: 2.3995
2022-03-08 18:51:16 - train: epoch 0159, iter [01400, 05004], lr: 0.011016, loss: 2.3346
2022-03-08 18:51:49 - train: epoch 0159, iter [01500, 05004], lr: 0.011016, loss: 2.3735
2022-03-08 18:52:23 - train: epoch 0159, iter [01600, 05004], lr: 0.011016, loss: 2.4160
2022-03-08 18:52:58 - train: epoch 0159, iter [01700, 05004], lr: 0.011016, loss: 2.1015
2022-03-08 18:53:32 - train: epoch 0159, iter [01800, 05004], lr: 0.011016, loss: 2.0833
2022-03-08 18:54:06 - train: epoch 0159, iter [01900, 05004], lr: 0.011016, loss: 2.2622
2022-03-08 18:54:39 - train: epoch 0159, iter [02000, 05004], lr: 0.011016, loss: 2.3156
2022-03-08 18:55:12 - train: epoch 0159, iter [02100, 05004], lr: 0.011016, loss: 2.1657
2022-03-08 18:55:47 - train: epoch 0159, iter [02200, 05004], lr: 0.011016, loss: 2.4281
2022-03-08 18:56:20 - train: epoch 0159, iter [02300, 05004], lr: 0.011016, loss: 2.0763
2022-03-08 18:56:54 - train: epoch 0159, iter [02400, 05004], lr: 0.011016, loss: 1.9675
2022-03-08 18:57:27 - train: epoch 0159, iter [02500, 05004], lr: 0.011016, loss: 2.1404
2022-03-08 18:58:01 - train: epoch 0159, iter [02600, 05004], lr: 0.011016, loss: 2.4130
2022-03-08 18:58:35 - train: epoch 0159, iter [02700, 05004], lr: 0.011016, loss: 2.2628
2022-03-08 18:59:08 - train: epoch 0159, iter [02800, 05004], lr: 0.011016, loss: 2.4828
2022-03-08 18:59:42 - train: epoch 0159, iter [02900, 05004], lr: 0.011016, loss: 2.4994
2022-03-08 19:00:15 - train: epoch 0159, iter [03000, 05004], lr: 0.011016, loss: 2.2382
2022-03-08 19:00:49 - train: epoch 0159, iter [03100, 05004], lr: 0.011016, loss: 1.8763
2022-03-08 19:01:22 - train: epoch 0159, iter [03200, 05004], lr: 0.011016, loss: 2.2226
2022-03-08 19:01:56 - train: epoch 0159, iter [03300, 05004], lr: 0.011016, loss: 2.2070
2022-03-08 19:02:30 - train: epoch 0159, iter [03400, 05004], lr: 0.011016, loss: 2.5200
2022-03-08 19:03:04 - train: epoch 0159, iter [03500, 05004], lr: 0.011016, loss: 2.2131
2022-03-08 19:03:37 - train: epoch 0159, iter [03600, 05004], lr: 0.011016, loss: 2.2678
2022-03-08 19:04:11 - train: epoch 0159, iter [03700, 05004], lr: 0.011016, loss: 2.1293
2022-03-08 19:04:44 - train: epoch 0159, iter [03800, 05004], lr: 0.011016, loss: 2.1705
2022-03-08 19:05:18 - train: epoch 0159, iter [03900, 05004], lr: 0.011016, loss: 2.2341
2022-03-08 19:05:51 - train: epoch 0159, iter [04000, 05004], lr: 0.011016, loss: 2.3727
2022-03-08 19:06:25 - train: epoch 0159, iter [04100, 05004], lr: 0.011016, loss: 2.1415
2022-03-08 19:06:58 - train: epoch 0159, iter [04200, 05004], lr: 0.011016, loss: 2.2444
2022-03-08 19:07:32 - train: epoch 0159, iter [04300, 05004], lr: 0.011016, loss: 2.2589
2022-03-08 19:08:07 - train: epoch 0159, iter [04400, 05004], lr: 0.011016, loss: 2.2557
2022-03-08 19:08:40 - train: epoch 0159, iter [04500, 05004], lr: 0.011016, loss: 2.4940
2022-03-08 19:09:13 - train: epoch 0159, iter [04600, 05004], lr: 0.011016, loss: 2.1705
2022-03-08 19:09:47 - train: epoch 0159, iter [04700, 05004], lr: 0.011016, loss: 2.0457
2022-03-08 19:10:20 - train: epoch 0159, iter [04800, 05004], lr: 0.011016, loss: 2.3446
2022-03-08 19:10:54 - train: epoch 0159, iter [04900, 05004], lr: 0.011016, loss: 2.3615
2022-03-08 19:11:27 - train: epoch 0159, iter [05000, 05004], lr: 0.011016, loss: 2.3495
2022-03-08 19:11:28 - train: epoch 159, train_loss: 2.2097
2022-03-08 19:12:42 - eval: epoch: 159, acc1: 71.064%, acc5: 90.196%, test_loss: 1.1664, per_image_load_time: 2.107ms, per_image_inference_time: 0.490ms
2022-03-08 19:12:42 - until epoch: 159, best_acc1: 71.064%
2022-03-08 19:12:42 - epoch 160 lr: 0.010516948149147755
2022-03-08 19:13:21 - train: epoch 0160, iter [00100, 05004], lr: 0.010517, loss: 2.2509
2022-03-08 19:13:56 - train: epoch 0160, iter [00200, 05004], lr: 0.010517, loss: 2.1426
2022-03-08 19:14:29 - train: epoch 0160, iter [00300, 05004], lr: 0.010517, loss: 2.2650
2022-03-08 19:15:03 - train: epoch 0160, iter [00400, 05004], lr: 0.010517, loss: 2.1921
2022-03-08 19:15:36 - train: epoch 0160, iter [00500, 05004], lr: 0.010517, loss: 2.4250
2022-03-08 19:16:11 - train: epoch 0160, iter [00600, 05004], lr: 0.010517, loss: 2.1374
2022-03-08 19:16:44 - train: epoch 0160, iter [00700, 05004], lr: 0.010517, loss: 2.1486
2022-03-08 19:17:18 - train: epoch 0160, iter [00800, 05004], lr: 0.010517, loss: 2.3364
2022-03-08 19:17:52 - train: epoch 0160, iter [00900, 05004], lr: 0.010517, loss: 2.3372
2022-03-08 19:18:26 - train: epoch 0160, iter [01000, 05004], lr: 0.010517, loss: 2.0731
2022-03-08 19:19:00 - train: epoch 0160, iter [01100, 05004], lr: 0.010517, loss: 2.3061
2022-03-08 19:19:33 - train: epoch 0160, iter [01200, 05004], lr: 0.010517, loss: 2.0221
2022-03-08 19:20:07 - train: epoch 0160, iter [01300, 05004], lr: 0.010517, loss: 2.2602
2022-03-08 19:20:41 - train: epoch 0160, iter [01400, 05004], lr: 0.010517, loss: 2.1653
2022-03-08 19:21:14 - train: epoch 0160, iter [01500, 05004], lr: 0.010517, loss: 2.3032
2022-03-08 19:21:48 - train: epoch 0160, iter [01600, 05004], lr: 0.010517, loss: 2.1178
2022-03-08 19:22:22 - train: epoch 0160, iter [01700, 05004], lr: 0.010517, loss: 2.0999
2022-03-08 19:22:57 - train: epoch 0160, iter [01800, 05004], lr: 0.010517, loss: 2.1364
2022-03-08 19:23:31 - train: epoch 0160, iter [01900, 05004], lr: 0.010517, loss: 2.3026
2022-03-08 19:24:04 - train: epoch 0160, iter [02000, 05004], lr: 0.010517, loss: 2.1994
2022-03-08 19:24:37 - train: epoch 0160, iter [02100, 05004], lr: 0.010517, loss: 2.3641
2022-03-08 19:25:12 - train: epoch 0160, iter [02200, 05004], lr: 0.010517, loss: 2.1206
2022-03-08 19:25:45 - train: epoch 0160, iter [02300, 05004], lr: 0.010517, loss: 2.2175
2022-03-08 19:26:20 - train: epoch 0160, iter [02400, 05004], lr: 0.010517, loss: 2.2334
2022-03-08 19:26:53 - train: epoch 0160, iter [02500, 05004], lr: 0.010517, loss: 2.0027
2022-03-08 19:27:27 - train: epoch 0160, iter [02600, 05004], lr: 0.010517, loss: 1.7502
2022-03-08 19:28:01 - train: epoch 0160, iter [02700, 05004], lr: 0.010517, loss: 1.9679
2022-03-08 19:28:35 - train: epoch 0160, iter [02800, 05004], lr: 0.010517, loss: 2.3734
2022-03-08 19:29:08 - train: epoch 0160, iter [02900, 05004], lr: 0.010517, loss: 2.1065
2022-03-08 19:29:41 - train: epoch 0160, iter [03000, 05004], lr: 0.010517, loss: 2.1451
2022-03-08 19:30:14 - train: epoch 0160, iter [03100, 05004], lr: 0.010517, loss: 2.4483
2022-03-08 19:30:48 - train: epoch 0160, iter [03200, 05004], lr: 0.010517, loss: 2.2392
2022-03-08 19:31:21 - train: epoch 0160, iter [03300, 05004], lr: 0.010517, loss: 2.3090
2022-03-08 19:31:54 - train: epoch 0160, iter [03400, 05004], lr: 0.010517, loss: 2.4679
2022-03-08 19:32:27 - train: epoch 0160, iter [03500, 05004], lr: 0.010517, loss: 2.1771
2022-03-08 19:33:01 - train: epoch 0160, iter [03600, 05004], lr: 0.010517, loss: 1.9382
2022-03-08 19:33:33 - train: epoch 0160, iter [03700, 05004], lr: 0.010517, loss: 2.3209
2022-03-08 19:34:06 - train: epoch 0160, iter [03800, 05004], lr: 0.010517, loss: 2.1235
2022-03-08 19:34:40 - train: epoch 0160, iter [03900, 05004], lr: 0.010517, loss: 2.2699
2022-03-08 19:35:14 - train: epoch 0160, iter [04000, 05004], lr: 0.010517, loss: 2.1773
2022-03-08 19:35:47 - train: epoch 0160, iter [04100, 05004], lr: 0.010517, loss: 2.1154
2022-03-08 19:36:21 - train: epoch 0160, iter [04200, 05004], lr: 0.010517, loss: 2.3225
2022-03-08 19:36:54 - train: epoch 0160, iter [04300, 05004], lr: 0.010517, loss: 1.7950
2022-03-08 19:37:27 - train: epoch 0160, iter [04400, 05004], lr: 0.010517, loss: 2.2047
2022-03-08 19:38:00 - train: epoch 0160, iter [04500, 05004], lr: 0.010517, loss: 2.2306
2022-03-08 19:38:33 - train: epoch 0160, iter [04600, 05004], lr: 0.010517, loss: 2.2319
2022-03-08 19:39:06 - train: epoch 0160, iter [04700, 05004], lr: 0.010517, loss: 2.3244
2022-03-08 19:39:39 - train: epoch 0160, iter [04800, 05004], lr: 0.010517, loss: 2.2161
2022-03-08 19:40:13 - train: epoch 0160, iter [04900, 05004], lr: 0.010517, loss: 2.2208
2022-03-08 19:40:44 - train: epoch 0160, iter [05000, 05004], lr: 0.010517, loss: 1.9357
2022-03-08 19:40:45 - train: epoch 160, train_loss: 2.1956
2022-03-08 19:41:57 - eval: epoch: 160, acc1: 70.764%, acc5: 90.194%, test_loss: 1.1740, per_image_load_time: 1.852ms, per_image_inference_time: 0.449ms
2022-03-08 19:41:57 - until epoch: 160, best_acc1: 71.064%
2022-03-08 19:41:57 - epoch 161 lr: 0.010027861829824953
2022-03-08 19:42:35 - train: epoch 0161, iter [00100, 05004], lr: 0.010028, loss: 2.0597
2022-03-08 19:43:08 - train: epoch 0161, iter [00200, 05004], lr: 0.010028, loss: 2.2198
2022-03-08 19:43:41 - train: epoch 0161, iter [00300, 05004], lr: 0.010028, loss: 1.7763
2022-03-08 19:44:15 - train: epoch 0161, iter [00400, 05004], lr: 0.010028, loss: 1.9189
2022-03-08 19:44:49 - train: epoch 0161, iter [00500, 05004], lr: 0.010028, loss: 2.1073
2022-03-08 19:45:22 - train: epoch 0161, iter [00600, 05004], lr: 0.010028, loss: 2.0022
2022-03-08 19:45:55 - train: epoch 0161, iter [00700, 05004], lr: 0.010028, loss: 2.2502
2022-03-08 19:46:29 - train: epoch 0161, iter [00800, 05004], lr: 0.010028, loss: 2.1140
2022-03-08 19:47:02 - train: epoch 0161, iter [00900, 05004], lr: 0.010028, loss: 1.7265
2022-03-08 19:47:36 - train: epoch 0161, iter [01000, 05004], lr: 0.010028, loss: 2.4368
2022-03-08 19:48:10 - train: epoch 0161, iter [01100, 05004], lr: 0.010028, loss: 2.0909
2022-03-08 19:48:43 - train: epoch 0161, iter [01200, 05004], lr: 0.010028, loss: 2.2857
2022-03-08 19:49:17 - train: epoch 0161, iter [01300, 05004], lr: 0.010028, loss: 2.2618
2022-03-08 19:49:49 - train: epoch 0161, iter [01400, 05004], lr: 0.010028, loss: 2.2509
2022-03-08 19:50:23 - train: epoch 0161, iter [01500, 05004], lr: 0.010028, loss: 2.1525
2022-03-08 19:50:56 - train: epoch 0161, iter [01600, 05004], lr: 0.010028, loss: 2.0092
2022-03-08 19:51:29 - train: epoch 0161, iter [01700, 05004], lr: 0.010028, loss: 1.9712
2022-03-08 19:52:02 - train: epoch 0161, iter [01800, 05004], lr: 0.010028, loss: 2.0302
2022-03-08 19:52:35 - train: epoch 0161, iter [01900, 05004], lr: 0.010028, loss: 2.2742
2022-03-08 19:53:08 - train: epoch 0161, iter [02000, 05004], lr: 0.010028, loss: 2.1703
2022-03-08 19:53:42 - train: epoch 0161, iter [02100, 05004], lr: 0.010028, loss: 2.2635
2022-03-08 19:54:16 - train: epoch 0161, iter [02200, 05004], lr: 0.010028, loss: 2.1003
2022-03-08 19:54:49 - train: epoch 0161, iter [02300, 05004], lr: 0.010028, loss: 2.0050
2022-03-08 19:55:22 - train: epoch 0161, iter [02400, 05004], lr: 0.010028, loss: 2.1802
2022-03-08 19:55:56 - train: epoch 0161, iter [02500, 05004], lr: 0.010028, loss: 2.1864
2022-03-08 19:56:30 - train: epoch 0161, iter [02600, 05004], lr: 0.010028, loss: 2.1138
2022-03-08 19:57:03 - train: epoch 0161, iter [02700, 05004], lr: 0.010028, loss: 2.1320
2022-03-08 19:57:37 - train: epoch 0161, iter [02800, 05004], lr: 0.010028, loss: 2.0256
2022-03-08 19:58:10 - train: epoch 0161, iter [02900, 05004], lr: 0.010028, loss: 2.1886
2022-03-08 19:58:42 - train: epoch 0161, iter [03000, 05004], lr: 0.010028, loss: 1.9025
2022-03-08 19:59:16 - train: epoch 0161, iter [03100, 05004], lr: 0.010028, loss: 2.0683
2022-03-08 19:59:49 - train: epoch 0161, iter [03200, 05004], lr: 0.010028, loss: 2.1564
2022-03-08 20:00:23 - train: epoch 0161, iter [03300, 05004], lr: 0.010028, loss: 1.9486
2022-03-08 20:00:56 - train: epoch 0161, iter [03400, 05004], lr: 0.010028, loss: 1.9148
2022-03-08 20:01:29 - train: epoch 0161, iter [03500, 05004], lr: 0.010028, loss: 2.2716
2022-03-08 20:02:03 - train: epoch 0161, iter [03600, 05004], lr: 0.010028, loss: 2.2919
2022-03-08 20:02:37 - train: epoch 0161, iter [03700, 05004], lr: 0.010028, loss: 2.0071
2022-03-08 20:03:11 - train: epoch 0161, iter [03800, 05004], lr: 0.010028, loss: 2.1492
2022-03-08 20:03:44 - train: epoch 0161, iter [03900, 05004], lr: 0.010028, loss: 1.7790
2022-03-08 20:04:18 - train: epoch 0161, iter [04000, 05004], lr: 0.010028, loss: 2.2369
2022-03-08 20:04:51 - train: epoch 0161, iter [04100, 05004], lr: 0.010028, loss: 1.8960
2022-03-08 20:05:26 - train: epoch 0161, iter [04200, 05004], lr: 0.010028, loss: 2.0501
2022-03-08 20:05:59 - train: epoch 0161, iter [04300, 05004], lr: 0.010028, loss: 2.1004
2022-03-08 20:06:34 - train: epoch 0161, iter [04400, 05004], lr: 0.010028, loss: 2.2308
2022-03-08 20:07:07 - train: epoch 0161, iter [04500, 05004], lr: 0.010028, loss: 2.2128
2022-03-08 20:07:41 - train: epoch 0161, iter [04600, 05004], lr: 0.010028, loss: 2.2580
2022-03-08 20:08:14 - train: epoch 0161, iter [04700, 05004], lr: 0.010028, loss: 2.1511
2022-03-08 20:08:48 - train: epoch 0161, iter [04800, 05004], lr: 0.010028, loss: 2.5180
2022-03-08 20:09:21 - train: epoch 0161, iter [04900, 05004], lr: 0.010028, loss: 1.9046
2022-03-08 20:09:53 - train: epoch 0161, iter [05000, 05004], lr: 0.010028, loss: 2.1515
2022-03-08 20:09:54 - train: epoch 161, train_loss: 2.1827
2022-03-08 20:11:07 - eval: epoch: 161, acc1: 71.010%, acc5: 90.460%, test_loss: 1.1588, per_image_load_time: 1.774ms, per_image_inference_time: 0.486ms
2022-03-08 20:11:08 - until epoch: 161, best_acc1: 71.064%
2022-03-08 20:11:08 - epoch 162 lr: 0.009549150281252633
2022-03-08 20:11:47 - train: epoch 0162, iter [00100, 05004], lr: 0.009549, loss: 2.0425
2022-03-08 20:12:20 - train: epoch 0162, iter [00200, 05004], lr: 0.009549, loss: 2.1913
2022-03-08 20:12:54 - train: epoch 0162, iter [00300, 05004], lr: 0.009549, loss: 2.5501
2022-03-08 20:13:27 - train: epoch 0162, iter [00400, 05004], lr: 0.009549, loss: 2.3570
2022-03-08 20:14:01 - train: epoch 0162, iter [00500, 05004], lr: 0.009549, loss: 2.1686
2022-03-08 20:14:35 - train: epoch 0162, iter [00600, 05004], lr: 0.009549, loss: 1.9208
2022-03-08 20:15:08 - train: epoch 0162, iter [00700, 05004], lr: 0.009549, loss: 2.2847
2022-03-08 20:15:42 - train: epoch 0162, iter [00800, 05004], lr: 0.009549, loss: 1.9531
2022-03-08 20:16:15 - train: epoch 0162, iter [00900, 05004], lr: 0.009549, loss: 2.6970
2022-03-08 20:16:49 - train: epoch 0162, iter [01000, 05004], lr: 0.009549, loss: 2.0432
2022-03-08 20:17:23 - train: epoch 0162, iter [01100, 05004], lr: 0.009549, loss: 2.2687
2022-03-08 20:17:57 - train: epoch 0162, iter [01200, 05004], lr: 0.009549, loss: 2.2197
2022-03-08 20:18:29 - train: epoch 0162, iter [01300, 05004], lr: 0.009549, loss: 2.1409
2022-03-08 20:19:03 - train: epoch 0162, iter [01400, 05004], lr: 0.009549, loss: 1.8697
2022-03-08 20:19:37 - train: epoch 0162, iter [01500, 05004], lr: 0.009549, loss: 2.1543
2022-03-08 20:20:10 - train: epoch 0162, iter [01600, 05004], lr: 0.009549, loss: 2.0612
2022-03-08 20:20:43 - train: epoch 0162, iter [01700, 05004], lr: 0.009549, loss: 2.1265
2022-03-08 20:21:16 - train: epoch 0162, iter [01800, 05004], lr: 0.009549, loss: 2.3001
2022-03-08 20:21:50 - train: epoch 0162, iter [01900, 05004], lr: 0.009549, loss: 2.1272
2022-03-08 20:22:23 - train: epoch 0162, iter [02000, 05004], lr: 0.009549, loss: 1.9671
2022-03-08 20:22:56 - train: epoch 0162, iter [02100, 05004], lr: 0.009549, loss: 2.1791
2022-03-08 20:23:30 - train: epoch 0162, iter [02200, 05004], lr: 0.009549, loss: 1.8664
2022-03-08 20:24:03 - train: epoch 0162, iter [02300, 05004], lr: 0.009549, loss: 2.2514
2022-03-08 20:24:37 - train: epoch 0162, iter [02400, 05004], lr: 0.009549, loss: 2.1959
2022-03-08 20:25:10 - train: epoch 0162, iter [02500, 05004], lr: 0.009549, loss: 2.3548
2022-03-08 20:25:43 - train: epoch 0162, iter [02600, 05004], lr: 0.009549, loss: 1.9884
2022-03-08 20:26:16 - train: epoch 0162, iter [02700, 05004], lr: 0.009549, loss: 2.0761
2022-03-08 20:26:50 - train: epoch 0162, iter [02800, 05004], lr: 0.009549, loss: 1.8475
2022-03-08 20:27:23 - train: epoch 0162, iter [02900, 05004], lr: 0.009549, loss: 2.1581
2022-03-08 20:27:56 - train: epoch 0162, iter [03000, 05004], lr: 0.009549, loss: 1.8702
2022-03-08 20:28:30 - train: epoch 0162, iter [03100, 05004], lr: 0.009549, loss: 2.2564
2022-03-08 20:29:04 - train: epoch 0162, iter [03200, 05004], lr: 0.009549, loss: 1.7628
2022-03-08 20:29:36 - train: epoch 0162, iter [03300, 05004], lr: 0.009549, loss: 2.2398
2022-03-08 20:30:10 - train: epoch 0162, iter [03400, 05004], lr: 0.009549, loss: 1.9509
2022-03-08 20:30:43 - train: epoch 0162, iter [03500, 05004], lr: 0.009549, loss: 2.3175
2022-03-08 20:31:16 - train: epoch 0162, iter [03600, 05004], lr: 0.009549, loss: 2.3745
2022-03-08 20:31:49 - train: epoch 0162, iter [03700, 05004], lr: 0.009549, loss: 2.0248
2022-03-08 20:32:22 - train: epoch 0162, iter [03800, 05004], lr: 0.009549, loss: 2.3077
2022-03-08 20:32:56 - train: epoch 0162, iter [03900, 05004], lr: 0.009549, loss: 2.3331
2022-03-08 20:33:29 - train: epoch 0162, iter [04000, 05004], lr: 0.009549, loss: 2.0720
2022-03-08 20:34:01 - train: epoch 0162, iter [04100, 05004], lr: 0.009549, loss: 1.9357
2022-03-08 20:34:34 - train: epoch 0162, iter [04200, 05004], lr: 0.009549, loss: 2.2356
2022-03-08 20:35:07 - train: epoch 0162, iter [04300, 05004], lr: 0.009549, loss: 2.1879
2022-03-08 20:35:40 - train: epoch 0162, iter [04400, 05004], lr: 0.009549, loss: 2.0192
2022-03-08 20:36:13 - train: epoch 0162, iter [04500, 05004], lr: 0.009549, loss: 2.2315
2022-03-08 20:36:47 - train: epoch 0162, iter [04600, 05004], lr: 0.009549, loss: 2.4182
2022-03-08 20:37:20 - train: epoch 0162, iter [04700, 05004], lr: 0.009549, loss: 2.0143
2022-03-08 20:37:54 - train: epoch 0162, iter [04800, 05004], lr: 0.009549, loss: 2.2846
2022-03-08 20:38:27 - train: epoch 0162, iter [04900, 05004], lr: 0.009549, loss: 2.3516
2022-03-08 20:38:59 - train: epoch 0162, iter [05000, 05004], lr: 0.009549, loss: 2.3138
2022-03-08 20:39:01 - train: epoch 162, train_loss: 2.1716
2022-03-08 20:40:13 - eval: epoch: 162, acc1: 70.784%, acc5: 90.306%, test_loss: 1.1770, per_image_load_time: 1.719ms, per_image_inference_time: 0.472ms
2022-03-08 20:40:13 - until epoch: 162, best_acc1: 71.064%
2022-03-08 20:40:13 - epoch 163 lr: 0.009080937753040646
2022-03-08 20:40:51 - train: epoch 0163, iter [00100, 05004], lr: 0.009081, loss: 2.3730
2022-03-08 20:41:25 - train: epoch 0163, iter [00200, 05004], lr: 0.009081, loss: 2.2051
2022-03-08 20:41:59 - train: epoch 0163, iter [00300, 05004], lr: 0.009081, loss: 2.1261
2022-03-08 20:42:32 - train: epoch 0163, iter [00400, 05004], lr: 0.009081, loss: 2.0768
2022-03-08 20:43:05 - train: epoch 0163, iter [00500, 05004], lr: 0.009081, loss: 2.2671
2022-03-08 20:43:38 - train: epoch 0163, iter [00600, 05004], lr: 0.009081, loss: 2.3310
2022-03-08 20:44:12 - train: epoch 0163, iter [00700, 05004], lr: 0.009081, loss: 2.3163
2022-03-08 20:44:45 - train: epoch 0163, iter [00800, 05004], lr: 0.009081, loss: 2.1269
2022-03-08 20:45:17 - train: epoch 0163, iter [00900, 05004], lr: 0.009081, loss: 2.1884
2022-03-08 20:45:50 - train: epoch 0163, iter [01000, 05004], lr: 0.009081, loss: 2.1614
2022-03-08 20:46:24 - train: epoch 0163, iter [01100, 05004], lr: 0.009081, loss: 2.2521
2022-03-08 20:46:57 - train: epoch 0163, iter [01200, 05004], lr: 0.009081, loss: 2.1768
2022-03-08 20:47:30 - train: epoch 0163, iter [01300, 05004], lr: 0.009081, loss: 2.0582
2022-03-08 20:48:04 - train: epoch 0163, iter [01400, 05004], lr: 0.009081, loss: 1.8504
2022-03-08 20:48:38 - train: epoch 0163, iter [01500, 05004], lr: 0.009081, loss: 1.9958
2022-03-08 20:49:11 - train: epoch 0163, iter [01600, 05004], lr: 0.009081, loss: 2.5462
2022-03-08 20:49:44 - train: epoch 0163, iter [01700, 05004], lr: 0.009081, loss: 2.0388
2022-03-08 20:50:17 - train: epoch 0163, iter [01800, 05004], lr: 0.009081, loss: 2.1598
2022-03-08 20:50:51 - train: epoch 0163, iter [01900, 05004], lr: 0.009081, loss: 2.1267
2022-03-08 20:51:23 - train: epoch 0163, iter [02000, 05004], lr: 0.009081, loss: 1.9021
2022-03-08 20:51:56 - train: epoch 0163, iter [02100, 05004], lr: 0.009081, loss: 2.1718
2022-03-08 20:52:28 - train: epoch 0163, iter [02200, 05004], lr: 0.009081, loss: 2.2316
2022-03-08 20:53:02 - train: epoch 0163, iter [02300, 05004], lr: 0.009081, loss: 2.1302
2022-03-08 20:53:36 - train: epoch 0163, iter [02400, 05004], lr: 0.009081, loss: 2.1687
2022-03-08 20:54:09 - train: epoch 0163, iter [02500, 05004], lr: 0.009081, loss: 2.0705
2022-03-08 20:54:42 - train: epoch 0163, iter [02600, 05004], lr: 0.009081, loss: 1.9224
2022-03-08 20:55:15 - train: epoch 0163, iter [02700, 05004], lr: 0.009081, loss: 2.3471
2022-03-08 20:55:49 - train: epoch 0163, iter [02800, 05004], lr: 0.009081, loss: 2.1115
2022-03-08 20:56:22 - train: epoch 0163, iter [02900, 05004], lr: 0.009081, loss: 1.9169
2022-03-08 20:56:55 - train: epoch 0163, iter [03000, 05004], lr: 0.009081, loss: 2.2736
2022-03-08 20:57:28 - train: epoch 0163, iter [03100, 05004], lr: 0.009081, loss: 2.0544
2022-03-08 20:58:02 - train: epoch 0163, iter [03200, 05004], lr: 0.009081, loss: 2.0755
2022-03-08 20:58:35 - train: epoch 0163, iter [03300, 05004], lr: 0.009081, loss: 1.9597
2022-03-08 20:59:08 - train: epoch 0163, iter [03400, 05004], lr: 0.009081, loss: 2.2889
2022-03-08 20:59:42 - train: epoch 0163, iter [03500, 05004], lr: 0.009081, loss: 2.1721
2022-03-08 21:00:15 - train: epoch 0163, iter [03600, 05004], lr: 0.009081, loss: 1.8838
2022-03-08 21:00:49 - train: epoch 0163, iter [03700, 05004], lr: 0.009081, loss: 2.0916
2022-03-08 21:01:21 - train: epoch 0163, iter [03800, 05004], lr: 0.009081, loss: 2.2174
2022-03-08 21:01:56 - train: epoch 0163, iter [03900, 05004], lr: 0.009081, loss: 2.1040
2022-03-08 21:02:29 - train: epoch 0163, iter [04000, 05004], lr: 0.009081, loss: 2.2345
2022-03-08 21:03:03 - train: epoch 0163, iter [04100, 05004], lr: 0.009081, loss: 2.0563
2022-03-08 21:03:35 - train: epoch 0163, iter [04200, 05004], lr: 0.009081, loss: 2.0298
2022-03-08 21:04:08 - train: epoch 0163, iter [04300, 05004], lr: 0.009081, loss: 1.9185
2022-03-08 21:04:42 - train: epoch 0163, iter [04400, 05004], lr: 0.009081, loss: 2.1478
2022-03-08 21:05:15 - train: epoch 0163, iter [04500, 05004], lr: 0.009081, loss: 1.9367
2022-03-08 21:05:48 - train: epoch 0163, iter [04600, 05004], lr: 0.009081, loss: 2.3339
2022-03-08 21:06:22 - train: epoch 0163, iter [04700, 05004], lr: 0.009081, loss: 2.3251
2022-03-08 21:06:56 - train: epoch 0163, iter [04800, 05004], lr: 0.009081, loss: 2.2049
2022-03-08 21:07:28 - train: epoch 0163, iter [04900, 05004], lr: 0.009081, loss: 2.3286
2022-03-08 21:08:00 - train: epoch 0163, iter [05000, 05004], lr: 0.009081, loss: 2.0944
2022-03-08 21:08:01 - train: epoch 163, train_loss: 2.1581
2022-03-08 21:09:14 - eval: epoch: 163, acc1: 71.506%, acc5: 90.512%, test_loss: 1.1465, per_image_load_time: 2.010ms, per_image_inference_time: 0.424ms
2022-03-08 21:09:15 - until epoch: 163, best_acc1: 71.506%
2022-03-08 21:09:15 - epoch 164 lr: 0.008623345769777514
2022-03-08 21:09:54 - train: epoch 0164, iter [00100, 05004], lr: 0.008623, loss: 1.9703
2022-03-08 21:10:26 - train: epoch 0164, iter [00200, 05004], lr: 0.008623, loss: 2.1316
2022-03-08 21:10:59 - train: epoch 0164, iter [00300, 05004], lr: 0.008623, loss: 2.1073
2022-03-08 21:11:33 - train: epoch 0164, iter [00400, 05004], lr: 0.008623, loss: 2.0648
2022-03-08 21:12:06 - train: epoch 0164, iter [00500, 05004], lr: 0.008623, loss: 2.1466
2022-03-08 21:12:39 - train: epoch 0164, iter [00600, 05004], lr: 0.008623, loss: 2.0848
2022-03-08 21:13:12 - train: epoch 0164, iter [00700, 05004], lr: 0.008623, loss: 2.0682
2022-03-08 21:13:45 - train: epoch 0164, iter [00800, 05004], lr: 0.008623, loss: 1.9493
2022-03-08 21:14:19 - train: epoch 0164, iter [00900, 05004], lr: 0.008623, loss: 2.1334
2022-03-08 21:14:51 - train: epoch 0164, iter [01000, 05004], lr: 0.008623, loss: 2.0412
2022-03-08 21:15:25 - train: epoch 0164, iter [01100, 05004], lr: 0.008623, loss: 2.1671
2022-03-08 21:15:58 - train: epoch 0164, iter [01200, 05004], lr: 0.008623, loss: 1.9125
2022-03-08 21:16:32 - train: epoch 0164, iter [01300, 05004], lr: 0.008623, loss: 2.1247
2022-03-08 21:17:05 - train: epoch 0164, iter [01400, 05004], lr: 0.008623, loss: 2.2762
2022-03-08 21:17:37 - train: epoch 0164, iter [01500, 05004], lr: 0.008623, loss: 2.3380
2022-03-08 21:18:10 - train: epoch 0164, iter [01600, 05004], lr: 0.008623, loss: 2.1618
2022-03-08 21:18:44 - train: epoch 0164, iter [01700, 05004], lr: 0.008623, loss: 2.1656
2022-03-08 21:19:16 - train: epoch 0164, iter [01800, 05004], lr: 0.008623, loss: 2.2697
2022-03-08 21:19:50 - train: epoch 0164, iter [01900, 05004], lr: 0.008623, loss: 2.0914
2022-03-08 21:20:23 - train: epoch 0164, iter [02000, 05004], lr: 0.008623, loss: 2.3098
2022-03-08 21:20:55 - train: epoch 0164, iter [02100, 05004], lr: 0.008623, loss: 2.2814
2022-03-08 21:21:28 - train: epoch 0164, iter [02200, 05004], lr: 0.008623, loss: 2.3049
2022-03-08 21:22:01 - train: epoch 0164, iter [02300, 05004], lr: 0.008623, loss: 2.2257
2022-03-08 21:22:35 - train: epoch 0164, iter [02400, 05004], lr: 0.008623, loss: 2.1735
2022-03-08 21:23:07 - train: epoch 0164, iter [02500, 05004], lr: 0.008623, loss: 2.0886
2022-03-08 21:23:41 - train: epoch 0164, iter [02600, 05004], lr: 0.008623, loss: 2.1210
2022-03-08 21:24:13 - train: epoch 0164, iter [02700, 05004], lr: 0.008623, loss: 2.2546
2022-03-08 21:24:46 - train: epoch 0164, iter [02800, 05004], lr: 0.008623, loss: 2.1613
2022-03-08 21:25:19 - train: epoch 0164, iter [02900, 05004], lr: 0.008623, loss: 2.0201
2022-03-08 21:25:53 - train: epoch 0164, iter [03000, 05004], lr: 0.008623, loss: 2.0219
2022-03-08 21:26:26 - train: epoch 0164, iter [03100, 05004], lr: 0.008623, loss: 2.0378
2022-03-08 21:26:59 - train: epoch 0164, iter [03200, 05004], lr: 0.008623, loss: 2.2185
2022-03-08 21:27:32 - train: epoch 0164, iter [03300, 05004], lr: 0.008623, loss: 2.1241
2022-03-08 21:28:05 - train: epoch 0164, iter [03400, 05004], lr: 0.008623, loss: 2.1099
2022-03-08 21:28:38 - train: epoch 0164, iter [03500, 05004], lr: 0.008623, loss: 2.0754
2022-03-08 21:29:12 - train: epoch 0164, iter [03600, 05004], lr: 0.008623, loss: 2.1218
2022-03-08 21:29:45 - train: epoch 0164, iter [03700, 05004], lr: 0.008623, loss: 2.3093
2022-03-08 21:30:18 - train: epoch 0164, iter [03800, 05004], lr: 0.008623, loss: 2.2304
2022-03-08 21:30:51 - train: epoch 0164, iter [03900, 05004], lr: 0.008623, loss: 2.1583
2022-03-08 21:31:24 - train: epoch 0164, iter [04000, 05004], lr: 0.008623, loss: 2.1076
2022-03-08 21:31:56 - train: epoch 0164, iter [04100, 05004], lr: 0.008623, loss: 1.7568
2022-03-08 21:32:30 - train: epoch 0164, iter [04200, 05004], lr: 0.008623, loss: 2.3275
2022-03-08 21:33:03 - train: epoch 0164, iter [04300, 05004], lr: 0.008623, loss: 1.9727
2022-03-08 21:33:36 - train: epoch 0164, iter [04400, 05004], lr: 0.008623, loss: 1.8402
2022-03-08 21:34:09 - train: epoch 0164, iter [04500, 05004], lr: 0.008623, loss: 2.1517
2022-03-08 21:34:42 - train: epoch 0164, iter [04600, 05004], lr: 0.008623, loss: 2.1463
2022-03-08 21:35:16 - train: epoch 0164, iter [04700, 05004], lr: 0.008623, loss: 1.9314
2022-03-08 21:35:49 - train: epoch 0164, iter [04800, 05004], lr: 0.008623, loss: 2.1826
2022-03-08 21:36:22 - train: epoch 0164, iter [04900, 05004], lr: 0.008623, loss: 2.1101
2022-03-08 21:36:55 - train: epoch 0164, iter [05000, 05004], lr: 0.008623, loss: 1.9225
2022-03-08 21:36:56 - train: epoch 164, train_loss: 2.1455
2022-03-08 21:38:08 - eval: epoch: 164, acc1: 71.442%, acc5: 90.706%, test_loss: 1.1354, per_image_load_time: 1.061ms, per_image_inference_time: 0.461ms
2022-03-08 21:38:09 - until epoch: 164, best_acc1: 71.506%
2022-03-08 21:38:09 - epoch 165 lr: 0.008176493099488664
2022-03-08 21:38:47 - train: epoch 0165, iter [00100, 05004], lr: 0.008176, loss: 2.0021
2022-03-08 21:39:20 - train: epoch 0165, iter [00200, 05004], lr: 0.008176, loss: 2.3214
2022-03-08 21:39:54 - train: epoch 0165, iter [00300, 05004], lr: 0.008176, loss: 2.3900
2022-03-08 21:40:28 - train: epoch 0165, iter [00400, 05004], lr: 0.008176, loss: 2.0659
2022-03-08 21:41:01 - train: epoch 0165, iter [00500, 05004], lr: 0.008176, loss: 2.1250
2022-03-08 21:41:34 - train: epoch 0165, iter [00600, 05004], lr: 0.008176, loss: 2.1040
2022-03-08 21:42:08 - train: epoch 0165, iter [00700, 05004], lr: 0.008176, loss: 2.0833
2022-03-08 21:42:41 - train: epoch 0165, iter [00800, 05004], lr: 0.008176, loss: 2.1334
2022-03-08 21:43:14 - train: epoch 0165, iter [00900, 05004], lr: 0.008176, loss: 2.1165
2022-03-08 21:43:48 - train: epoch 0165, iter [01000, 05004], lr: 0.008176, loss: 2.0112
2022-03-08 21:44:21 - train: epoch 0165, iter [01100, 05004], lr: 0.008176, loss: 2.0791
2022-03-08 21:44:53 - train: epoch 0165, iter [01200, 05004], lr: 0.008176, loss: 2.2064
2022-03-08 21:45:28 - train: epoch 0165, iter [01300, 05004], lr: 0.008176, loss: 2.0341
2022-03-08 21:46:00 - train: epoch 0165, iter [01400, 05004], lr: 0.008176, loss: 2.1702
2022-03-08 21:46:34 - train: epoch 0165, iter [01500, 05004], lr: 0.008176, loss: 2.0076
2022-03-08 21:47:07 - train: epoch 0165, iter [01600, 05004], lr: 0.008176, loss: 2.3199
2022-03-08 21:47:41 - train: epoch 0165, iter [01700, 05004], lr: 0.008176, loss: 2.1109
2022-03-08 21:48:14 - train: epoch 0165, iter [01800, 05004], lr: 0.008176, loss: 2.1303
2022-03-08 21:48:47 - train: epoch 0165, iter [01900, 05004], lr: 0.008176, loss: 2.1391
2022-03-08 21:49:20 - train: epoch 0165, iter [02000, 05004], lr: 0.008176, loss: 1.8245
2022-03-08 21:49:54 - train: epoch 0165, iter [02100, 05004], lr: 0.008176, loss: 2.1572
2022-03-08 21:50:27 - train: epoch 0165, iter [02200, 05004], lr: 0.008176, loss: 2.3083
2022-03-08 21:51:01 - train: epoch 0165, iter [02300, 05004], lr: 0.008176, loss: 2.2362
2022-03-08 21:51:34 - train: epoch 0165, iter [02400, 05004], lr: 0.008176, loss: 2.1641
2022-03-08 21:52:08 - train: epoch 0165, iter [02500, 05004], lr: 0.008176, loss: 2.0713
2022-03-08 21:52:40 - train: epoch 0165, iter [02600, 05004], lr: 0.008176, loss: 2.0229
2022-03-08 21:53:14 - train: epoch 0165, iter [02700, 05004], lr: 0.008176, loss: 2.2055
2022-03-08 21:53:47 - train: epoch 0165, iter [02800, 05004], lr: 0.008176, loss: 1.6964
2022-03-08 21:54:21 - train: epoch 0165, iter [02900, 05004], lr: 0.008176, loss: 2.3216
2022-03-08 21:54:53 - train: epoch 0165, iter [03000, 05004], lr: 0.008176, loss: 2.4787
2022-03-08 21:55:27 - train: epoch 0165, iter [03100, 05004], lr: 0.008176, loss: 1.9459
2022-03-08 21:56:00 - train: epoch 0165, iter [03200, 05004], lr: 0.008176, loss: 2.1121
2022-03-08 21:56:34 - train: epoch 0165, iter [03300, 05004], lr: 0.008176, loss: 1.9486
2022-03-08 21:57:06 - train: epoch 0165, iter [03400, 05004], lr: 0.008176, loss: 2.1063
2022-03-08 21:57:40 - train: epoch 0165, iter [03500, 05004], lr: 0.008176, loss: 2.2518
2022-03-08 21:58:13 - train: epoch 0165, iter [03600, 05004], lr: 0.008176, loss: 2.3491
2022-03-08 21:58:46 - train: epoch 0165, iter [03700, 05004], lr: 0.008176, loss: 1.9834
2022-03-08 21:59:19 - train: epoch 0165, iter [03800, 05004], lr: 0.008176, loss: 2.3438
2022-03-08 21:59:52 - train: epoch 0165, iter [03900, 05004], lr: 0.008176, loss: 2.0102
2022-03-08 22:00:26 - train: epoch 0165, iter [04000, 05004], lr: 0.008176, loss: 2.1932
2022-03-08 22:00:59 - train: epoch 0165, iter [04100, 05004], lr: 0.008176, loss: 1.9002
2022-03-08 22:01:33 - train: epoch 0165, iter [04200, 05004], lr: 0.008176, loss: 2.0738
2022-03-08 22:02:06 - train: epoch 0165, iter [04300, 05004], lr: 0.008176, loss: 2.1606
2022-03-08 22:02:38 - train: epoch 0165, iter [04400, 05004], lr: 0.008176, loss: 2.1382
2022-03-08 22:03:12 - train: epoch 0165, iter [04500, 05004], lr: 0.008176, loss: 2.1009
2022-03-08 22:03:45 - train: epoch 0165, iter [04600, 05004], lr: 0.008176, loss: 2.3086
2022-03-08 22:04:17 - train: epoch 0165, iter [04700, 05004], lr: 0.008176, loss: 2.2142
2022-03-08 22:04:51 - train: epoch 0165, iter [04800, 05004], lr: 0.008176, loss: 2.2131
2022-03-08 22:05:24 - train: epoch 0165, iter [04900, 05004], lr: 0.008176, loss: 2.0634
2022-03-08 22:05:56 - train: epoch 0165, iter [05000, 05004], lr: 0.008176, loss: 1.9529
2022-03-08 22:05:58 - train: epoch 165, train_loss: 2.1333
2022-03-08 22:07:10 - eval: epoch: 165, acc1: 71.800%, acc5: 90.896%, test_loss: 1.1233, per_image_load_time: 0.760ms, per_image_inference_time: 0.456ms
2022-03-08 22:07:11 - until epoch: 165, best_acc1: 71.800%
2022-03-08 22:07:11 - epoch 166 lr: 0.00774049572281027
2022-03-08 22:07:49 - train: epoch 0166, iter [00100, 05004], lr: 0.007740, loss: 1.9132
2022-03-08 22:08:22 - train: epoch 0166, iter [00200, 05004], lr: 0.007740, loss: 2.0665
2022-03-08 22:08:56 - train: epoch 0166, iter [00300, 05004], lr: 0.007740, loss: 2.4342
2022-03-08 22:09:29 - train: epoch 0166, iter [00400, 05004], lr: 0.007740, loss: 2.2096
2022-03-08 22:10:03 - train: epoch 0166, iter [00500, 05004], lr: 0.007740, loss: 2.0732
2022-03-08 22:10:36 - train: epoch 0166, iter [00600, 05004], lr: 0.007740, loss: 1.9386
2022-03-08 22:11:09 - train: epoch 0166, iter [00700, 05004], lr: 0.007740, loss: 2.1150
2022-03-08 22:11:42 - train: epoch 0166, iter [00800, 05004], lr: 0.007740, loss: 2.3533
2022-03-08 22:12:16 - train: epoch 0166, iter [00900, 05004], lr: 0.007740, loss: 2.1652
2022-03-08 22:12:49 - train: epoch 0166, iter [01000, 05004], lr: 0.007740, loss: 2.2404
2022-03-08 22:13:22 - train: epoch 0166, iter [01100, 05004], lr: 0.007740, loss: 2.1484
2022-03-08 22:13:55 - train: epoch 0166, iter [01200, 05004], lr: 0.007740, loss: 2.1484
2022-03-08 22:14:29 - train: epoch 0166, iter [01300, 05004], lr: 0.007740, loss: 2.0081
2022-03-08 22:15:03 - train: epoch 0166, iter [01400, 05004], lr: 0.007740, loss: 2.0728
2022-03-08 22:15:36 - train: epoch 0166, iter [01500, 05004], lr: 0.007740, loss: 2.0853
2022-03-08 22:16:09 - train: epoch 0166, iter [01600, 05004], lr: 0.007740, loss: 2.1531
2022-03-08 22:16:43 - train: epoch 0166, iter [01700, 05004], lr: 0.007740, loss: 2.1780
2022-03-08 22:17:16 - train: epoch 0166, iter [01800, 05004], lr: 0.007740, loss: 1.9939
2022-03-08 22:17:49 - train: epoch 0166, iter [01900, 05004], lr: 0.007740, loss: 2.2361
2022-03-08 22:18:22 - train: epoch 0166, iter [02000, 05004], lr: 0.007740, loss: 1.9981
2022-03-08 22:18:56 - train: epoch 0166, iter [02100, 05004], lr: 0.007740, loss: 1.9055
2022-03-08 22:19:29 - train: epoch 0166, iter [02200, 05004], lr: 0.007740, loss: 2.1233
2022-03-08 22:20:02 - train: epoch 0166, iter [02300, 05004], lr: 0.007740, loss: 1.9901
2022-03-08 22:20:35 - train: epoch 0166, iter [02400, 05004], lr: 0.007740, loss: 2.0259
2022-03-08 22:21:08 - train: epoch 0166, iter [02500, 05004], lr: 0.007740, loss: 2.2665
2022-03-08 22:21:41 - train: epoch 0166, iter [02600, 05004], lr: 0.007740, loss: 1.8297
2022-03-08 22:22:15 - train: epoch 0166, iter [02700, 05004], lr: 0.007740, loss: 1.9707
2022-03-08 22:22:48 - train: epoch 0166, iter [02800, 05004], lr: 0.007740, loss: 2.1839
2022-03-08 22:23:21 - train: epoch 0166, iter [02900, 05004], lr: 0.007740, loss: 2.0618
2022-03-08 22:23:54 - train: epoch 0166, iter [03000, 05004], lr: 0.007740, loss: 2.0484
2022-03-08 22:24:27 - train: epoch 0166, iter [03100, 05004], lr: 0.007740, loss: 2.3685
2022-03-08 22:25:01 - train: epoch 0166, iter [03200, 05004], lr: 0.007740, loss: 1.8142
2022-03-08 22:25:34 - train: epoch 0166, iter [03300, 05004], lr: 0.007740, loss: 1.9463
2022-03-08 22:26:08 - train: epoch 0166, iter [03400, 05004], lr: 0.007740, loss: 1.9726
2022-03-08 22:26:41 - train: epoch 0166, iter [03500, 05004], lr: 0.007740, loss: 2.2752
2022-03-08 22:27:15 - train: epoch 0166, iter [03600, 05004], lr: 0.007740, loss: 2.2233
2022-03-08 22:27:48 - train: epoch 0166, iter [03700, 05004], lr: 0.007740, loss: 2.1010
2022-03-08 22:28:21 - train: epoch 0166, iter [03800, 05004], lr: 0.007740, loss: 2.0574
2022-03-08 22:28:54 - train: epoch 0166, iter [03900, 05004], lr: 0.007740, loss: 2.1937
2022-03-08 22:29:28 - train: epoch 0166, iter [04000, 05004], lr: 0.007740, loss: 2.2645
2022-03-08 22:30:01 - train: epoch 0166, iter [04100, 05004], lr: 0.007740, loss: 1.7702
2022-03-08 22:30:35 - train: epoch 0166, iter [04200, 05004], lr: 0.007740, loss: 2.1923
2022-03-08 22:31:07 - train: epoch 0166, iter [04300, 05004], lr: 0.007740, loss: 2.2144
2022-03-08 22:31:40 - train: epoch 0166, iter [04400, 05004], lr: 0.007740, loss: 1.8611
2022-03-08 22:32:13 - train: epoch 0166, iter [04500, 05004], lr: 0.007740, loss: 2.1908
2022-03-08 22:32:47 - train: epoch 0166, iter [04600, 05004], lr: 0.007740, loss: 2.3695
2022-03-08 22:33:20 - train: epoch 0166, iter [04700, 05004], lr: 0.007740, loss: 2.2084
2022-03-08 22:33:53 - train: epoch 0166, iter [04800, 05004], lr: 0.007740, loss: 2.0019
2022-03-08 22:34:26 - train: epoch 0166, iter [04900, 05004], lr: 0.007740, loss: 1.9713
2022-03-08 22:34:58 - train: epoch 0166, iter [05000, 05004], lr: 0.007740, loss: 2.4462
2022-03-08 22:34:59 - train: epoch 166, train_loss: 2.1146
2022-03-08 22:36:12 - eval: epoch: 166, acc1: 72.254%, acc5: 91.262%, test_loss: 1.0963, per_image_load_time: 1.030ms, per_image_inference_time: 0.486ms
2022-03-08 22:36:13 - until epoch: 166, best_acc1: 72.254%
2022-03-08 22:36:13 - epoch 167 lr: 0.0073154668028864
2022-03-08 22:36:51 - train: epoch 0167, iter [00100, 05004], lr: 0.007315, loss: 2.0981
2022-03-08 22:37:24 - train: epoch 0167, iter [00200, 05004], lr: 0.007315, loss: 1.9500
2022-03-08 22:37:58 - train: epoch 0167, iter [00300, 05004], lr: 0.007315, loss: 2.1855
2022-03-08 22:38:32 - train: epoch 0167, iter [00400, 05004], lr: 0.007315, loss: 1.8505
2022-03-08 22:39:04 - train: epoch 0167, iter [00500, 05004], lr: 0.007315, loss: 2.0033
2022-03-08 22:39:37 - train: epoch 0167, iter [00600, 05004], lr: 0.007315, loss: 2.0812
2022-03-08 22:40:10 - train: epoch 0167, iter [00700, 05004], lr: 0.007315, loss: 1.7955
2022-03-08 22:40:43 - train: epoch 0167, iter [00800, 05004], lr: 0.007315, loss: 2.3400
2022-03-08 22:41:17 - train: epoch 0167, iter [00900, 05004], lr: 0.007315, loss: 2.2481
2022-03-08 22:41:50 - train: epoch 0167, iter [01000, 05004], lr: 0.007315, loss: 2.0809
2022-03-08 22:42:23 - train: epoch 0167, iter [01100, 05004], lr: 0.007315, loss: 2.1124
2022-03-08 22:42:57 - train: epoch 0167, iter [01200, 05004], lr: 0.007315, loss: 1.9877
2022-03-08 22:43:31 - train: epoch 0167, iter [01300, 05004], lr: 0.007315, loss: 2.2482
2022-03-08 22:44:05 - train: epoch 0167, iter [01400, 05004], lr: 0.007315, loss: 2.0946
2022-03-08 22:44:37 - train: epoch 0167, iter [01500, 05004], lr: 0.007315, loss: 1.8109
2022-03-08 22:45:11 - train: epoch 0167, iter [01600, 05004], lr: 0.007315, loss: 2.1655
2022-03-08 22:45:45 - train: epoch 0167, iter [01700, 05004], lr: 0.007315, loss: 2.1620
2022-03-08 22:46:17 - train: epoch 0167, iter [01800, 05004], lr: 0.007315, loss: 2.1616
2022-03-08 22:46:50 - train: epoch 0167, iter [01900, 05004], lr: 0.007315, loss: 2.2653
2022-03-08 22:47:24 - train: epoch 0167, iter [02000, 05004], lr: 0.007315, loss: 2.3112
2022-03-08 22:47:58 - train: epoch 0167, iter [02100, 05004], lr: 0.007315, loss: 2.0561
2022-03-08 22:48:31 - train: epoch 0167, iter [02200, 05004], lr: 0.007315, loss: 2.2949
2022-03-08 22:49:05 - train: epoch 0167, iter [02300, 05004], lr: 0.007315, loss: 2.1350
2022-03-08 22:49:37 - train: epoch 0167, iter [02400, 05004], lr: 0.007315, loss: 1.9003
2022-03-08 22:50:11 - train: epoch 0167, iter [02500, 05004], lr: 0.007315, loss: 2.0715
2022-03-08 22:50:44 - train: epoch 0167, iter [02600, 05004], lr: 0.007315, loss: 2.0960
2022-03-08 22:51:17 - train: epoch 0167, iter [02700, 05004], lr: 0.007315, loss: 2.2971
2022-03-08 22:51:51 - train: epoch 0167, iter [02800, 05004], lr: 0.007315, loss: 2.2199
2022-03-08 22:52:24 - train: epoch 0167, iter [02900, 05004], lr: 0.007315, loss: 1.9444
2022-03-08 22:52:57 - train: epoch 0167, iter [03000, 05004], lr: 0.007315, loss: 2.0158
2022-03-08 22:53:30 - train: epoch 0167, iter [03100, 05004], lr: 0.007315, loss: 2.0370
2022-03-08 22:54:04 - train: epoch 0167, iter [03200, 05004], lr: 0.007315, loss: 2.0007
2022-03-08 22:54:36 - train: epoch 0167, iter [03300, 05004], lr: 0.007315, loss: 2.2824
2022-03-08 22:55:10 - train: epoch 0167, iter [03400, 05004], lr: 0.007315, loss: 1.9859
2022-03-08 22:55:43 - train: epoch 0167, iter [03500, 05004], lr: 0.007315, loss: 2.2869
2022-03-08 22:56:16 - train: epoch 0167, iter [03600, 05004], lr: 0.007315, loss: 1.9244
2022-03-08 22:56:49 - train: epoch 0167, iter [03700, 05004], lr: 0.007315, loss: 2.0215
2022-03-08 22:57:23 - train: epoch 0167, iter [03800, 05004], lr: 0.007315, loss: 2.2064
2022-03-08 22:57:56 - train: epoch 0167, iter [03900, 05004], lr: 0.007315, loss: 2.0107
2022-03-08 22:58:29 - train: epoch 0167, iter [04000, 05004], lr: 0.007315, loss: 2.3793
2022-03-08 22:59:02 - train: epoch 0167, iter [04100, 05004], lr: 0.007315, loss: 1.9691
2022-03-08 22:59:36 - train: epoch 0167, iter [04200, 05004], lr: 0.007315, loss: 2.3356
2022-03-08 23:00:09 - train: epoch 0167, iter [04300, 05004], lr: 0.007315, loss: 1.8632
2022-03-08 23:00:42 - train: epoch 0167, iter [04400, 05004], lr: 0.007315, loss: 1.9508
2022-03-08 23:01:15 - train: epoch 0167, iter [04500, 05004], lr: 0.007315, loss: 1.9074
2022-03-08 23:01:48 - train: epoch 0167, iter [04600, 05004], lr: 0.007315, loss: 2.1546
2022-03-08 23:02:22 - train: epoch 0167, iter [04700, 05004], lr: 0.007315, loss: 2.2460
2022-03-08 23:02:56 - train: epoch 0167, iter [04800, 05004], lr: 0.007315, loss: 1.9236
2022-03-08 23:03:30 - train: epoch 0167, iter [04900, 05004], lr: 0.007315, loss: 2.1622
2022-03-08 23:04:03 - train: epoch 0167, iter [05000, 05004], lr: 0.007315, loss: 2.2281
2022-03-08 23:04:04 - train: epoch 167, train_loss: 2.1000
2022-03-08 23:05:18 - eval: epoch: 167, acc1: 72.564%, acc5: 91.388%, test_loss: 1.0889, per_image_load_time: 1.880ms, per_image_inference_time: 0.523ms
2022-03-08 23:05:19 - until epoch: 167, best_acc1: 72.564%
2022-03-08 23:05:19 - epoch 168 lr: 0.006901516655997537
2022-03-08 23:05:59 - train: epoch 0168, iter [00100, 05004], lr: 0.006902, loss: 1.8823
2022-03-08 23:06:32 - train: epoch 0168, iter [00200, 05004], lr: 0.006902, loss: 1.9922
2022-03-08 23:07:06 - train: epoch 0168, iter [00300, 05004], lr: 0.006902, loss: 2.1684
2022-03-08 23:07:40 - train: epoch 0168, iter [00400, 05004], lr: 0.006902, loss: 1.8657
2022-03-08 23:08:14 - train: epoch 0168, iter [00500, 05004], lr: 0.006902, loss: 2.1707
2022-03-08 23:08:49 - train: epoch 0168, iter [00600, 05004], lr: 0.006902, loss: 2.1107
2022-03-08 23:09:23 - train: epoch 0168, iter [00700, 05004], lr: 0.006902, loss: 2.1230
2022-03-08 23:09:58 - train: epoch 0168, iter [00800, 05004], lr: 0.006902, loss: 2.2834
2022-03-08 23:10:31 - train: epoch 0168, iter [00900, 05004], lr: 0.006902, loss: 2.0515
2022-03-08 23:11:06 - train: epoch 0168, iter [01000, 05004], lr: 0.006902, loss: 2.0836
2022-03-08 23:11:40 - train: epoch 0168, iter [01100, 05004], lr: 0.006902, loss: 1.8962
2022-03-08 23:12:14 - train: epoch 0168, iter [01200, 05004], lr: 0.006902, loss: 2.0084
2022-03-08 23:12:47 - train: epoch 0168, iter [01300, 05004], lr: 0.006902, loss: 1.9797
2022-03-08 23:13:21 - train: epoch 0168, iter [01400, 05004], lr: 0.006902, loss: 2.1128
2022-03-08 23:13:55 - train: epoch 0168, iter [01500, 05004], lr: 0.006902, loss: 2.0502
2022-03-08 23:14:29 - train: epoch 0168, iter [01600, 05004], lr: 0.006902, loss: 2.1241
2022-03-08 23:15:02 - train: epoch 0168, iter [01700, 05004], lr: 0.006902, loss: 2.1989
2022-03-08 23:15:36 - train: epoch 0168, iter [01800, 05004], lr: 0.006902, loss: 2.4203
2022-03-08 23:16:10 - train: epoch 0168, iter [01900, 05004], lr: 0.006902, loss: 1.9363
2022-03-08 23:16:44 - train: epoch 0168, iter [02000, 05004], lr: 0.006902, loss: 1.9242
2022-03-08 23:17:17 - train: epoch 0168, iter [02100, 05004], lr: 0.006902, loss: 2.2926
2022-03-08 23:17:51 - train: epoch 0168, iter [02200, 05004], lr: 0.006902, loss: 2.3044
2022-03-08 23:18:25 - train: epoch 0168, iter [02300, 05004], lr: 0.006902, loss: 1.9464
2022-03-08 23:18:59 - train: epoch 0168, iter [02400, 05004], lr: 0.006902, loss: 2.1305
2022-03-08 23:19:32 - train: epoch 0168, iter [02500, 05004], lr: 0.006902, loss: 2.0738
2022-03-08 23:20:06 - train: epoch 0168, iter [02600, 05004], lr: 0.006902, loss: 2.1435
2022-03-08 23:20:40 - train: epoch 0168, iter [02700, 05004], lr: 0.006902, loss: 1.8731
2022-03-08 23:21:14 - train: epoch 0168, iter [02800, 05004], lr: 0.006902, loss: 2.0884
2022-03-08 23:21:48 - train: epoch 0168, iter [02900, 05004], lr: 0.006902, loss: 2.5318
2022-03-08 23:22:22 - train: epoch 0168, iter [03000, 05004], lr: 0.006902, loss: 1.8433
2022-03-08 23:22:54 - train: epoch 0168, iter [03100, 05004], lr: 0.006902, loss: 2.0270
2022-03-08 23:23:29 - train: epoch 0168, iter [03200, 05004], lr: 0.006902, loss: 1.9082
2022-03-08 23:24:03 - train: epoch 0168, iter [03300, 05004], lr: 0.006902, loss: 2.1463
2022-03-08 23:24:37 - train: epoch 0168, iter [03400, 05004], lr: 0.006902, loss: 2.2688
2022-03-08 23:25:10 - train: epoch 0168, iter [03500, 05004], lr: 0.006902, loss: 1.8855
2022-03-08 23:25:44 - train: epoch 0168, iter [03600, 05004], lr: 0.006902, loss: 2.2866
2022-03-08 23:26:18 - train: epoch 0168, iter [03700, 05004], lr: 0.006902, loss: 2.0000
2022-03-08 23:26:51 - train: epoch 0168, iter [03800, 05004], lr: 0.006902, loss: 2.1373
2022-03-08 23:27:25 - train: epoch 0168, iter [03900, 05004], lr: 0.006902, loss: 2.3072
2022-03-08 23:27:58 - train: epoch 0168, iter [04000, 05004], lr: 0.006902, loss: 2.1511
2022-03-08 23:28:33 - train: epoch 0168, iter [04100, 05004], lr: 0.006902, loss: 2.2628
2022-03-08 23:29:06 - train: epoch 0168, iter [04200, 05004], lr: 0.006902, loss: 1.7549
2022-03-08 23:29:41 - train: epoch 0168, iter [04300, 05004], lr: 0.006902, loss: 2.2149
2022-03-08 23:30:14 - train: epoch 0168, iter [04400, 05004], lr: 0.006902, loss: 2.1447
2022-03-08 23:30:48 - train: epoch 0168, iter [04500, 05004], lr: 0.006902, loss: 2.3546
2022-03-08 23:31:23 - train: epoch 0168, iter [04600, 05004], lr: 0.006902, loss: 2.1677
2022-03-08 23:31:56 - train: epoch 0168, iter [04700, 05004], lr: 0.006902, loss: 2.1394
2022-03-08 23:32:29 - train: epoch 0168, iter [04800, 05004], lr: 0.006902, loss: 1.9847
2022-03-08 23:33:04 - train: epoch 0168, iter [04900, 05004], lr: 0.006902, loss: 2.2614
2022-03-08 23:33:37 - train: epoch 0168, iter [05000, 05004], lr: 0.006902, loss: 2.0991
2022-03-08 23:33:38 - train: epoch 168, train_loss: 2.0877
2022-03-08 23:34:51 - eval: epoch: 168, acc1: 72.560%, acc5: 91.104%, test_loss: 1.1047, per_image_load_time: 0.741ms, per_image_inference_time: 0.486ms
2022-03-08 23:34:52 - until epoch: 168, best_acc1: 72.564%
2022-03-08 23:34:52 - epoch 169 lr: 0.006498752722928042
2022-03-08 23:35:31 - train: epoch 0169, iter [00100, 05004], lr: 0.006499, loss: 1.9026
2022-03-08 23:36:05 - train: epoch 0169, iter [00200, 05004], lr: 0.006499, loss: 2.0838
2022-03-08 23:36:38 - train: epoch 0169, iter [00300, 05004], lr: 0.006499, loss: 2.2614
2022-03-08 23:37:12 - train: epoch 0169, iter [00400, 05004], lr: 0.006499, loss: 2.1259
2022-03-08 23:37:46 - train: epoch 0169, iter [00500, 05004], lr: 0.006499, loss: 2.0905
2022-03-08 23:38:20 - train: epoch 0169, iter [00600, 05004], lr: 0.006499, loss: 2.0075
2022-03-08 23:38:53 - train: epoch 0169, iter [00700, 05004], lr: 0.006499, loss: 2.1587
2022-03-08 23:39:27 - train: epoch 0169, iter [00800, 05004], lr: 0.006499, loss: 2.1252
2022-03-08 23:40:00 - train: epoch 0169, iter [00900, 05004], lr: 0.006499, loss: 2.0733
2022-03-08 23:40:35 - train: epoch 0169, iter [01000, 05004], lr: 0.006499, loss: 1.9619
2022-03-08 23:41:08 - train: epoch 0169, iter [01100, 05004], lr: 0.006499, loss: 2.0991
2022-03-08 23:41:42 - train: epoch 0169, iter [01200, 05004], lr: 0.006499, loss: 2.1018
2022-03-08 23:42:15 - train: epoch 0169, iter [01300, 05004], lr: 0.006499, loss: 2.1463
2022-03-08 23:42:49 - train: epoch 0169, iter [01400, 05004], lr: 0.006499, loss: 2.0176
2022-03-08 23:43:23 - train: epoch 0169, iter [01500, 05004], lr: 0.006499, loss: 1.9936
2022-03-08 23:43:56 - train: epoch 0169, iter [01600, 05004], lr: 0.006499, loss: 2.1171
2022-03-08 23:44:30 - train: epoch 0169, iter [01700, 05004], lr: 0.006499, loss: 2.1365
2022-03-08 23:45:04 - train: epoch 0169, iter [01800, 05004], lr: 0.006499, loss: 2.1933
2022-03-08 23:45:37 - train: epoch 0169, iter [01900, 05004], lr: 0.006499, loss: 2.1099
2022-03-08 23:46:11 - train: epoch 0169, iter [02000, 05004], lr: 0.006499, loss: 2.0130
2022-03-08 23:46:45 - train: epoch 0169, iter [02100, 05004], lr: 0.006499, loss: 2.0600
2022-03-08 23:47:18 - train: epoch 0169, iter [02200, 05004], lr: 0.006499, loss: 2.1038
2022-03-08 23:47:52 - train: epoch 0169, iter [02300, 05004], lr: 0.006499, loss: 2.2246
2022-03-08 23:48:26 - train: epoch 0169, iter [02400, 05004], lr: 0.006499, loss: 1.9270
2022-03-08 23:49:00 - train: epoch 0169, iter [02500, 05004], lr: 0.006499, loss: 2.0027
2022-03-08 23:49:33 - train: epoch 0169, iter [02600, 05004], lr: 0.006499, loss: 2.0195
2022-03-08 23:50:08 - train: epoch 0169, iter [02700, 05004], lr: 0.006499, loss: 2.0746
2022-03-08 23:50:41 - train: epoch 0169, iter [02800, 05004], lr: 0.006499, loss: 2.4252
2022-03-08 23:51:15 - train: epoch 0169, iter [02900, 05004], lr: 0.006499, loss: 2.3686
2022-03-08 23:51:48 - train: epoch 0169, iter [03000, 05004], lr: 0.006499, loss: 1.9672
2022-03-08 23:52:23 - train: epoch 0169, iter [03100, 05004], lr: 0.006499, loss: 1.9915
2022-03-08 23:52:57 - train: epoch 0169, iter [03200, 05004], lr: 0.006499, loss: 2.1408
2022-03-08 23:53:31 - train: epoch 0169, iter [03300, 05004], lr: 0.006499, loss: 1.9626
2022-03-08 23:54:04 - train: epoch 0169, iter [03400, 05004], lr: 0.006499, loss: 1.9106
2022-03-08 23:54:39 - train: epoch 0169, iter [03500, 05004], lr: 0.006499, loss: 2.1598
2022-03-08 23:55:12 - train: epoch 0169, iter [03600, 05004], lr: 0.006499, loss: 2.0916
2022-03-08 23:55:46 - train: epoch 0169, iter [03700, 05004], lr: 0.006499, loss: 2.1461
2022-03-08 23:56:19 - train: epoch 0169, iter [03800, 05004], lr: 0.006499, loss: 2.1590
2022-03-08 23:56:54 - train: epoch 0169, iter [03900, 05004], lr: 0.006499, loss: 1.9421
2022-03-08 23:57:27 - train: epoch 0169, iter [04000, 05004], lr: 0.006499, loss: 1.9347
2022-03-08 23:58:01 - train: epoch 0169, iter [04100, 05004], lr: 0.006499, loss: 1.9580
2022-03-08 23:58:35 - train: epoch 0169, iter [04200, 05004], lr: 0.006499, loss: 2.2507
2022-03-08 23:59:08 - train: epoch 0169, iter [04300, 05004], lr: 0.006499, loss: 2.1727
2022-03-08 23:59:42 - train: epoch 0169, iter [04400, 05004], lr: 0.006499, loss: 2.1970
2022-03-09 00:00:16 - train: epoch 0169, iter [04500, 05004], lr: 0.006499, loss: 2.3589
2022-03-09 00:00:49 - train: epoch 0169, iter [04600, 05004], lr: 0.006499, loss: 2.3785
2022-03-09 00:01:23 - train: epoch 0169, iter [04700, 05004], lr: 0.006499, loss: 2.1459
2022-03-09 00:01:57 - train: epoch 0169, iter [04800, 05004], lr: 0.006499, loss: 2.0359
2022-03-09 00:02:31 - train: epoch 0169, iter [04900, 05004], lr: 0.006499, loss: 1.9174
2022-03-09 00:03:04 - train: epoch 0169, iter [05000, 05004], lr: 0.006499, loss: 2.0453
2022-03-09 00:03:05 - train: epoch 169, train_loss: 2.0729
2022-03-09 00:04:19 - eval: epoch: 169, acc1: 73.004%, acc5: 91.310%, test_loss: 1.0854, per_image_load_time: 1.119ms, per_image_inference_time: 0.526ms
2022-03-09 00:04:20 - until epoch: 169, best_acc1: 73.004%
2022-03-09 00:04:20 - epoch 170 lr: 0.006107279541079769
2022-03-09 00:04:58 - train: epoch 0170, iter [00100, 05004], lr: 0.006107, loss: 2.2709
2022-03-09 00:05:32 - train: epoch 0170, iter [00200, 05004], lr: 0.006107, loss: 2.1045
2022-03-09 00:06:05 - train: epoch 0170, iter [00300, 05004], lr: 0.006107, loss: 2.0965
2022-03-09 00:06:38 - train: epoch 0170, iter [00400, 05004], lr: 0.006107, loss: 2.0981
2022-03-09 00:07:12 - train: epoch 0170, iter [00500, 05004], lr: 0.006107, loss: 2.4756
2022-03-09 00:07:44 - train: epoch 0170, iter [00600, 05004], lr: 0.006107, loss: 2.0921
2022-03-09 00:08:18 - train: epoch 0170, iter [00700, 05004], lr: 0.006107, loss: 2.1331
2022-03-09 00:08:52 - train: epoch 0170, iter [00800, 05004], lr: 0.006107, loss: 1.9482
2022-03-09 00:09:25 - train: epoch 0170, iter [00900, 05004], lr: 0.006107, loss: 1.9074
2022-03-09 00:09:59 - train: epoch 0170, iter [01000, 05004], lr: 0.006107, loss: 1.8910
2022-03-09 00:10:31 - train: epoch 0170, iter [01100, 05004], lr: 0.006107, loss: 1.9847
2022-03-09 00:11:05 - train: epoch 0170, iter [01200, 05004], lr: 0.006107, loss: 2.1923
2022-03-09 00:11:37 - train: epoch 0170, iter [01300, 05004], lr: 0.006107, loss: 2.1179
2022-03-09 00:12:12 - train: epoch 0170, iter [01400, 05004], lr: 0.006107, loss: 2.2671
2022-03-09 00:12:45 - train: epoch 0170, iter [01500, 05004], lr: 0.006107, loss: 2.1752
2022-03-09 00:13:17 - train: epoch 0170, iter [01600, 05004], lr: 0.006107, loss: 2.2676
2022-03-09 00:13:52 - train: epoch 0170, iter [01700, 05004], lr: 0.006107, loss: 2.2801
2022-03-09 00:14:24 - train: epoch 0170, iter [01800, 05004], lr: 0.006107, loss: 1.7520
2022-03-09 00:14:58 - train: epoch 0170, iter [01900, 05004], lr: 0.006107, loss: 2.3725
2022-03-09 00:15:31 - train: epoch 0170, iter [02000, 05004], lr: 0.006107, loss: 1.9238
2022-03-09 00:16:05 - train: epoch 0170, iter [02100, 05004], lr: 0.006107, loss: 2.0257
2022-03-09 00:16:37 - train: epoch 0170, iter [02200, 05004], lr: 0.006107, loss: 2.1955
2022-03-09 00:17:11 - train: epoch 0170, iter [02300, 05004], lr: 0.006107, loss: 2.0022
2022-03-09 00:17:44 - train: epoch 0170, iter [02400, 05004], lr: 0.006107, loss: 2.1835
2022-03-09 00:18:19 - train: epoch 0170, iter [02500, 05004], lr: 0.006107, loss: 1.9806
2022-03-09 00:18:52 - train: epoch 0170, iter [02600, 05004], lr: 0.006107, loss: 2.0450
2022-03-09 00:19:25 - train: epoch 0170, iter [02700, 05004], lr: 0.006107, loss: 1.9018
2022-03-09 00:19:58 - train: epoch 0170, iter [02800, 05004], lr: 0.006107, loss: 1.9516
2022-03-09 00:20:32 - train: epoch 0170, iter [02900, 05004], lr: 0.006107, loss: 2.1628
2022-03-09 00:21:05 - train: epoch 0170, iter [03000, 05004], lr: 0.006107, loss: 1.8434
2022-03-09 00:21:38 - train: epoch 0170, iter [03100, 05004], lr: 0.006107, loss: 2.1145
2022-03-09 00:22:12 - train: epoch 0170, iter [03200, 05004], lr: 0.006107, loss: 2.0397
2022-03-09 00:22:45 - train: epoch 0170, iter [03300, 05004], lr: 0.006107, loss: 1.9167
2022-03-09 00:23:19 - train: epoch 0170, iter [03400, 05004], lr: 0.006107, loss: 2.1021
2022-03-09 00:23:52 - train: epoch 0170, iter [03500, 05004], lr: 0.006107, loss: 2.0771
2022-03-09 00:24:25 - train: epoch 0170, iter [03600, 05004], lr: 0.006107, loss: 1.9323
2022-03-09 00:24:58 - train: epoch 0170, iter [03700, 05004], lr: 0.006107, loss: 2.1980
2022-03-09 00:25:32 - train: epoch 0170, iter [03800, 05004], lr: 0.006107, loss: 2.1029
2022-03-09 00:26:05 - train: epoch 0170, iter [03900, 05004], lr: 0.006107, loss: 2.0726
2022-03-09 00:26:39 - train: epoch 0170, iter [04000, 05004], lr: 0.006107, loss: 1.9419
2022-03-09 00:27:12 - train: epoch 0170, iter [04100, 05004], lr: 0.006107, loss: 1.7453
2022-03-09 00:27:46 - train: epoch 0170, iter [04200, 05004], lr: 0.006107, loss: 2.0949
2022-03-09 00:28:18 - train: epoch 0170, iter [04300, 05004], lr: 0.006107, loss: 2.0819
2022-03-09 00:28:52 - train: epoch 0170, iter [04400, 05004], lr: 0.006107, loss: 1.8474
2022-03-09 00:29:25 - train: epoch 0170, iter [04500, 05004], lr: 0.006107, loss: 2.0541
2022-03-09 00:29:58 - train: epoch 0170, iter [04600, 05004], lr: 0.006107, loss: 2.0488
2022-03-09 00:30:33 - train: epoch 0170, iter [04700, 05004], lr: 0.006107, loss: 2.0176
2022-03-09 00:31:06 - train: epoch 0170, iter [04800, 05004], lr: 0.006107, loss: 2.1190
2022-03-09 00:31:39 - train: epoch 0170, iter [04900, 05004], lr: 0.006107, loss: 2.1470
2022-03-09 00:32:11 - train: epoch 0170, iter [05000, 05004], lr: 0.006107, loss: 2.1598
2022-03-09 00:32:12 - train: epoch 170, train_loss: 2.0604
2022-03-09 00:33:26 - eval: epoch: 170, acc1: 72.940%, acc5: 91.254%, test_loss: 1.0806, per_image_load_time: 1.828ms, per_image_inference_time: 0.498ms
2022-03-09 00:33:26 - until epoch: 170, best_acc1: 73.004%
2022-03-09 00:33:26 - epoch 171 lr: 0.00572719871733951
2022-03-09 00:34:06 - train: epoch 0171, iter [00100, 05004], lr: 0.005727, loss: 2.1695
2022-03-09 00:34:38 - train: epoch 0171, iter [00200, 05004], lr: 0.005727, loss: 2.0105
2022-03-09 00:35:12 - train: epoch 0171, iter [00300, 05004], lr: 0.005727, loss: 2.1835
2022-03-09 00:35:45 - train: epoch 0171, iter [00400, 05004], lr: 0.005727, loss: 2.2541
2022-03-09 00:36:20 - train: epoch 0171, iter [00500, 05004], lr: 0.005727, loss: 1.9815
2022-03-09 00:36:53 - train: epoch 0171, iter [00600, 05004], lr: 0.005727, loss: 2.3163
2022-03-09 00:37:26 - train: epoch 0171, iter [00700, 05004], lr: 0.005727, loss: 2.1407
2022-03-09 00:37:59 - train: epoch 0171, iter [00800, 05004], lr: 0.005727, loss: 2.2038
2022-03-09 00:38:32 - train: epoch 0171, iter [00900, 05004], lr: 0.005727, loss: 1.7426
2022-03-09 00:39:06 - train: epoch 0171, iter [01000, 05004], lr: 0.005727, loss: 2.1973
2022-03-09 00:39:39 - train: epoch 0171, iter [01100, 05004], lr: 0.005727, loss: 1.8982
2022-03-09 00:40:12 - train: epoch 0171, iter [01200, 05004], lr: 0.005727, loss: 2.0403
2022-03-09 00:40:46 - train: epoch 0171, iter [01300, 05004], lr: 0.005727, loss: 2.3597
2022-03-09 00:41:19 - train: epoch 0171, iter [01400, 05004], lr: 0.005727, loss: 2.1204
2022-03-09 00:41:53 - train: epoch 0171, iter [01500, 05004], lr: 0.005727, loss: 1.8115
2022-03-09 00:42:27 - train: epoch 0171, iter [01600, 05004], lr: 0.005727, loss: 2.1741
2022-03-09 00:43:01 - train: epoch 0171, iter [01700, 05004], lr: 0.005727, loss: 2.0884
2022-03-09 00:43:35 - train: epoch 0171, iter [01800, 05004], lr: 0.005727, loss: 2.0485
2022-03-09 00:44:08 - train: epoch 0171, iter [01900, 05004], lr: 0.005727, loss: 1.9076
2022-03-09 00:44:42 - train: epoch 0171, iter [02000, 05004], lr: 0.005727, loss: 1.9186
2022-03-09 00:45:15 - train: epoch 0171, iter [02100, 05004], lr: 0.005727, loss: 1.9502
2022-03-09 00:45:49 - train: epoch 0171, iter [02200, 05004], lr: 0.005727, loss: 1.6612
2022-03-09 00:46:23 - train: epoch 0171, iter [02300, 05004], lr: 0.005727, loss: 2.2138
2022-03-09 00:46:56 - train: epoch 0171, iter [02400, 05004], lr: 0.005727, loss: 2.0973
2022-03-09 00:47:30 - train: epoch 0171, iter [02500, 05004], lr: 0.005727, loss: 2.0777
2022-03-09 00:48:03 - train: epoch 0171, iter [02600, 05004], lr: 0.005727, loss: 1.9116
2022-03-09 00:48:37 - train: epoch 0171, iter [02700, 05004], lr: 0.005727, loss: 2.1748
2022-03-09 00:49:09 - train: epoch 0171, iter [02800, 05004], lr: 0.005727, loss: 2.1681
2022-03-09 00:49:43 - train: epoch 0171, iter [02900, 05004], lr: 0.005727, loss: 2.1650
2022-03-09 00:50:16 - train: epoch 0171, iter [03000, 05004], lr: 0.005727, loss: 2.4599
2022-03-09 00:50:51 - train: epoch 0171, iter [03100, 05004], lr: 0.005727, loss: 2.3110
2022-03-09 00:51:24 - train: epoch 0171, iter [03200, 05004], lr: 0.005727, loss: 2.0392
2022-03-09 00:51:57 - train: epoch 0171, iter [03300, 05004], lr: 0.005727, loss: 1.8874
2022-03-09 00:52:31 - train: epoch 0171, iter [03400, 05004], lr: 0.005727, loss: 1.8897
2022-03-09 00:53:04 - train: epoch 0171, iter [03500, 05004], lr: 0.005727, loss: 2.1578
2022-03-09 00:53:38 - train: epoch 0171, iter [03600, 05004], lr: 0.005727, loss: 2.2729
2022-03-09 00:54:12 - train: epoch 0171, iter [03700, 05004], lr: 0.005727, loss: 2.2581
2022-03-09 00:54:46 - train: epoch 0171, iter [03800, 05004], lr: 0.005727, loss: 2.1378
2022-03-09 00:55:18 - train: epoch 0171, iter [03900, 05004], lr: 0.005727, loss: 2.0843
2022-03-09 00:55:51 - train: epoch 0171, iter [04000, 05004], lr: 0.005727, loss: 2.0138
2022-03-09 00:56:24 - train: epoch 0171, iter [04100, 05004], lr: 0.005727, loss: 2.3451
2022-03-09 00:56:58 - train: epoch 0171, iter [04200, 05004], lr: 0.005727, loss: 1.9841
2022-03-09 00:57:31 - train: epoch 0171, iter [04300, 05004], lr: 0.005727, loss: 2.1503
2022-03-09 00:58:04 - train: epoch 0171, iter [04400, 05004], lr: 0.005727, loss: 1.9786
2022-03-09 00:58:37 - train: epoch 0171, iter [04500, 05004], lr: 0.005727, loss: 2.1315
2022-03-09 00:59:11 - train: epoch 0171, iter [04600, 05004], lr: 0.005727, loss: 2.2210
2022-03-09 00:59:45 - train: epoch 0171, iter [04700, 05004], lr: 0.005727, loss: 2.2959
2022-03-09 01:00:18 - train: epoch 0171, iter [04800, 05004], lr: 0.005727, loss: 2.1185
2022-03-09 01:00:51 - train: epoch 0171, iter [04900, 05004], lr: 0.005727, loss: 2.2057
2022-03-09 01:01:23 - train: epoch 0171, iter [05000, 05004], lr: 0.005727, loss: 2.2543
2022-03-09 01:01:24 - train: epoch 171, train_loss: 2.0454
2022-03-09 01:02:35 - eval: epoch: 171, acc1: 72.812%, acc5: 91.256%, test_loss: 1.0792, per_image_load_time: 1.496ms, per_image_inference_time: 0.437ms
2022-03-09 01:02:36 - until epoch: 171, best_acc1: 73.004%
2022-03-09 01:02:36 - epoch 172 lr: 0.005358608901706802
2022-03-09 01:03:14 - train: epoch 0172, iter [00100, 05004], lr: 0.005359, loss: 2.0328
2022-03-09 01:03:47 - train: epoch 0172, iter [00200, 05004], lr: 0.005359, loss: 1.7728
2022-03-09 01:04:20 - train: epoch 0172, iter [00300, 05004], lr: 0.005359, loss: 2.0036
2022-03-09 01:04:52 - train: epoch 0172, iter [00400, 05004], lr: 0.005359, loss: 1.9940
2022-03-09 01:05:25 - train: epoch 0172, iter [00500, 05004], lr: 0.005359, loss: 2.2485
2022-03-09 01:05:57 - train: epoch 0172, iter [00600, 05004], lr: 0.005359, loss: 2.0100
2022-03-09 01:06:31 - train: epoch 0172, iter [00700, 05004], lr: 0.005359, loss: 1.5529
2022-03-09 01:07:04 - train: epoch 0172, iter [00800, 05004], lr: 0.005359, loss: 1.9466
2022-03-09 01:07:36 - train: epoch 0172, iter [00900, 05004], lr: 0.005359, loss: 1.8970
2022-03-09 01:08:08 - train: epoch 0172, iter [01000, 05004], lr: 0.005359, loss: 2.0332
2022-03-09 01:08:43 - train: epoch 0172, iter [01100, 05004], lr: 0.005359, loss: 2.0164
2022-03-09 01:09:17 - train: epoch 0172, iter [01200, 05004], lr: 0.005359, loss: 2.0548
2022-03-09 01:09:50 - train: epoch 0172, iter [01300, 05004], lr: 0.005359, loss: 1.9933
2022-03-09 01:10:23 - train: epoch 0172, iter [01400, 05004], lr: 0.005359, loss: 2.1471
2022-03-09 01:10:56 - train: epoch 0172, iter [01500, 05004], lr: 0.005359, loss: 1.8429
2022-03-09 01:11:30 - train: epoch 0172, iter [01600, 05004], lr: 0.005359, loss: 2.2398
2022-03-09 01:12:04 - train: epoch 0172, iter [01700, 05004], lr: 0.005359, loss: 2.2960
2022-03-09 01:12:37 - train: epoch 0172, iter [01800, 05004], lr: 0.005359, loss: 2.4004
2022-03-09 01:13:12 - train: epoch 0172, iter [01900, 05004], lr: 0.005359, loss: 1.6159
2022-03-09 01:13:44 - train: epoch 0172, iter [02000, 05004], lr: 0.005359, loss: 1.8438
2022-03-09 01:14:18 - train: epoch 0172, iter [02100, 05004], lr: 0.005359, loss: 2.0765
2022-03-09 01:14:51 - train: epoch 0172, iter [02200, 05004], lr: 0.005359, loss: 2.1149
2022-03-09 01:15:25 - train: epoch 0172, iter [02300, 05004], lr: 0.005359, loss: 2.1691
2022-03-09 01:15:58 - train: epoch 0172, iter [02400, 05004], lr: 0.005359, loss: 2.3852
2022-03-09 01:16:32 - train: epoch 0172, iter [02500, 05004], lr: 0.005359, loss: 1.9307
2022-03-09 01:17:05 - train: epoch 0172, iter [02600, 05004], lr: 0.005359, loss: 2.0601
2022-03-09 01:17:38 - train: epoch 0172, iter [02700, 05004], lr: 0.005359, loss: 1.6315
2022-03-09 01:18:13 - train: epoch 0172, iter [02800, 05004], lr: 0.005359, loss: 1.9612
2022-03-09 01:18:45 - train: epoch 0172, iter [02900, 05004], lr: 0.005359, loss: 2.0895
2022-03-09 01:19:19 - train: epoch 0172, iter [03000, 05004], lr: 0.005359, loss: 2.0326
2022-03-09 01:19:53 - train: epoch 0172, iter [03100, 05004], lr: 0.005359, loss: 2.0335
2022-03-09 01:20:26 - train: epoch 0172, iter [03200, 05004], lr: 0.005359, loss: 2.1013
2022-03-09 01:20:59 - train: epoch 0172, iter [03300, 05004], lr: 0.005359, loss: 1.9592
2022-03-09 01:21:33 - train: epoch 0172, iter [03400, 05004], lr: 0.005359, loss: 2.1211
2022-03-09 01:22:06 - train: epoch 0172, iter [03500, 05004], lr: 0.005359, loss: 1.8508
2022-03-09 01:22:39 - train: epoch 0172, iter [03600, 05004], lr: 0.005359, loss: 1.8723
2022-03-09 01:23:13 - train: epoch 0172, iter [03700, 05004], lr: 0.005359, loss: 2.0837
2022-03-09 01:23:47 - train: epoch 0172, iter [03800, 05004], lr: 0.005359, loss: 2.1240
2022-03-09 01:24:20 - train: epoch 0172, iter [03900, 05004], lr: 0.005359, loss: 1.7316
2022-03-09 01:24:54 - train: epoch 0172, iter [04000, 05004], lr: 0.005359, loss: 2.0531
2022-03-09 01:25:27 - train: epoch 0172, iter [04100, 05004], lr: 0.005359, loss: 2.1438
2022-03-09 01:26:01 - train: epoch 0172, iter [04200, 05004], lr: 0.005359, loss: 2.1222
2022-03-09 01:26:35 - train: epoch 0172, iter [04300, 05004], lr: 0.005359, loss: 2.0500
2022-03-09 01:27:08 - train: epoch 0172, iter [04400, 05004], lr: 0.005359, loss: 2.1464
2022-03-09 01:27:42 - train: epoch 0172, iter [04500, 05004], lr: 0.005359, loss: 2.1112
2022-03-09 01:28:16 - train: epoch 0172, iter [04600, 05004], lr: 0.005359, loss: 2.1271
2022-03-09 01:28:50 - train: epoch 0172, iter [04700, 05004], lr: 0.005359, loss: 1.8584
2022-03-09 01:29:23 - train: epoch 0172, iter [04800, 05004], lr: 0.005359, loss: 1.8343
2022-03-09 01:29:57 - train: epoch 0172, iter [04900, 05004], lr: 0.005359, loss: 1.8957
2022-03-09 01:30:29 - train: epoch 0172, iter [05000, 05004], lr: 0.005359, loss: 1.9469
2022-03-09 01:30:30 - train: epoch 172, train_loss: 2.0300
2022-03-09 01:31:43 - eval: epoch: 172, acc1: 73.848%, acc5: 91.738%, test_loss: 1.0444, per_image_load_time: 1.763ms, per_image_inference_time: 0.458ms
2022-03-09 01:31:44 - until epoch: 172, best_acc1: 73.848%
2022-03-09 01:31:44 - epoch 173 lr: 0.005001605761689398
2022-03-09 01:32:22 - train: epoch 0173, iter [00100, 05004], lr: 0.005002, loss: 2.1066
2022-03-09 01:32:56 - train: epoch 0173, iter [00200, 05004], lr: 0.005002, loss: 2.0978
2022-03-09 01:33:30 - train: epoch 0173, iter [00300, 05004], lr: 0.005002, loss: 1.9770
2022-03-09 01:34:03 - train: epoch 0173, iter [00400, 05004], lr: 0.005002, loss: 1.9883
2022-03-09 01:34:35 - train: epoch 0173, iter [00500, 05004], lr: 0.005002, loss: 2.0282
2022-03-09 01:35:09 - train: epoch 0173, iter [00600, 05004], lr: 0.005002, loss: 1.9481
2022-03-09 01:35:42 - train: epoch 0173, iter [00700, 05004], lr: 0.005002, loss: 1.8346
2022-03-09 01:36:15 - train: epoch 0173, iter [00800, 05004], lr: 0.005002, loss: 2.0841
2022-03-09 01:36:49 - train: epoch 0173, iter [00900, 05004], lr: 0.005002, loss: 1.9725
2022-03-09 01:37:22 - train: epoch 0173, iter [01000, 05004], lr: 0.005002, loss: 2.5215
2022-03-09 01:37:55 - train: epoch 0173, iter [01100, 05004], lr: 0.005002, loss: 2.1166
2022-03-09 01:38:28 - train: epoch 0173, iter [01200, 05004], lr: 0.005002, loss: 2.1155
2022-03-09 01:39:01 - train: epoch 0173, iter [01300, 05004], lr: 0.005002, loss: 2.1756
2022-03-09 01:39:35 - train: epoch 0173, iter [01400, 05004], lr: 0.005002, loss: 2.1465
2022-03-09 01:40:09 - train: epoch 0173, iter [01500, 05004], lr: 0.005002, loss: 1.7958
2022-03-09 01:40:42 - train: epoch 0173, iter [01600, 05004], lr: 0.005002, loss: 2.0360
2022-03-09 01:41:16 - train: epoch 0173, iter [01700, 05004], lr: 0.005002, loss: 1.9887
2022-03-09 01:41:49 - train: epoch 0173, iter [01800, 05004], lr: 0.005002, loss: 1.8998
2022-03-09 01:42:23 - train: epoch 0173, iter [01900, 05004], lr: 0.005002, loss: 2.0035
2022-03-09 01:42:56 - train: epoch 0173, iter [02000, 05004], lr: 0.005002, loss: 2.4131
2022-03-09 01:43:29 - train: epoch 0173, iter [02100, 05004], lr: 0.005002, loss: 1.9764
2022-03-09 01:44:03 - train: epoch 0173, iter [02200, 05004], lr: 0.005002, loss: 2.0518
2022-03-09 01:44:36 - train: epoch 0173, iter [02300, 05004], lr: 0.005002, loss: 2.2596
2022-03-09 01:45:10 - train: epoch 0173, iter [02400, 05004], lr: 0.005002, loss: 2.1840
2022-03-09 01:45:43 - train: epoch 0173, iter [02500, 05004], lr: 0.005002, loss: 1.8625
2022-03-09 01:46:16 - train: epoch 0173, iter [02600, 05004], lr: 0.005002, loss: 1.9892
2022-03-09 01:46:49 - train: epoch 0173, iter [02700, 05004], lr: 0.005002, loss: 2.1386
2022-03-09 01:47:23 - train: epoch 0173, iter [02800, 05004], lr: 0.005002, loss: 2.2487
2022-03-09 01:47:56 - train: epoch 0173, iter [02900, 05004], lr: 0.005002, loss: 1.8001
2022-03-09 01:48:30 - train: epoch 0173, iter [03000, 05004], lr: 0.005002, loss: 1.8820
2022-03-09 01:49:03 - train: epoch 0173, iter [03100, 05004], lr: 0.005002, loss: 1.8630
2022-03-09 01:49:36 - train: epoch 0173, iter [03200, 05004], lr: 0.005002, loss: 2.1139
2022-03-09 01:50:09 - train: epoch 0173, iter [03300, 05004], lr: 0.005002, loss: 1.9250
2022-03-09 01:50:42 - train: epoch 0173, iter [03400, 05004], lr: 0.005002, loss: 2.0629
2022-03-09 01:51:15 - train: epoch 0173, iter [03500, 05004], lr: 0.005002, loss: 1.9985
2022-03-09 01:51:48 - train: epoch 0173, iter [03600, 05004], lr: 0.005002, loss: 1.8530
2022-03-09 01:52:20 - train: epoch 0173, iter [03700, 05004], lr: 0.005002, loss: 1.9015
2022-03-09 01:52:54 - train: epoch 0173, iter [03800, 05004], lr: 0.005002, loss: 2.0397
2022-03-09 01:53:27 - train: epoch 0173, iter [03900, 05004], lr: 0.005002, loss: 1.9515
2022-03-09 01:54:00 - train: epoch 0173, iter [04000, 05004], lr: 0.005002, loss: 1.8057
2022-03-09 01:54:31 - train: epoch 0173, iter [04100, 05004], lr: 0.005002, loss: 2.0732
2022-03-09 01:55:05 - train: epoch 0173, iter [04200, 05004], lr: 0.005002, loss: 2.2962
2022-03-09 01:55:38 - train: epoch 0173, iter [04300, 05004], lr: 0.005002, loss: 2.1666
2022-03-09 01:56:10 - train: epoch 0173, iter [04400, 05004], lr: 0.005002, loss: 1.8398
2022-03-09 01:56:44 - train: epoch 0173, iter [04500, 05004], lr: 0.005002, loss: 2.1704
2022-03-09 01:57:17 - train: epoch 0173, iter [04600, 05004], lr: 0.005002, loss: 2.0474
2022-03-09 01:57:49 - train: epoch 0173, iter [04700, 05004], lr: 0.005002, loss: 2.1704
2022-03-09 01:58:23 - train: epoch 0173, iter [04800, 05004], lr: 0.005002, loss: 1.9867
2022-03-09 01:58:55 - train: epoch 0173, iter [04900, 05004], lr: 0.005002, loss: 1.9754
2022-03-09 01:59:27 - train: epoch 0173, iter [05000, 05004], lr: 0.005002, loss: 1.9201
2022-03-09 01:59:28 - train: epoch 173, train_loss: 2.0186
2022-03-09 02:00:39 - eval: epoch: 173, acc1: 73.786%, acc5: 91.796%, test_loss: 1.0430, per_image_load_time: 1.990ms, per_image_inference_time: 0.432ms
2022-03-09 02:00:40 - until epoch: 173, best_acc1: 73.848%
2022-03-09 02:00:40 - epoch 174 lr: 0.00465628195747273
2022-03-09 02:01:18 - train: epoch 0174, iter [00100, 05004], lr: 0.004656, loss: 1.7914
2022-03-09 02:01:51 - train: epoch 0174, iter [00200, 05004], lr: 0.004656, loss: 2.0706
2022-03-09 02:02:23 - train: epoch 0174, iter [00300, 05004], lr: 0.004656, loss: 2.1093
2022-03-09 02:02:56 - train: epoch 0174, iter [00400, 05004], lr: 0.004656, loss: 1.9560
2022-03-09 02:03:29 - train: epoch 0174, iter [00500, 05004], lr: 0.004656, loss: 2.0746
2022-03-09 02:04:02 - train: epoch 0174, iter [00600, 05004], lr: 0.004656, loss: 1.9006
2022-03-09 02:04:35 - train: epoch 0174, iter [00700, 05004], lr: 0.004656, loss: 1.7470
2022-03-09 02:05:08 - train: epoch 0174, iter [00800, 05004], lr: 0.004656, loss: 2.0466
2022-03-09 02:05:41 - train: epoch 0174, iter [00900, 05004], lr: 0.004656, loss: 1.9337
2022-03-09 02:06:14 - train: epoch 0174, iter [01000, 05004], lr: 0.004656, loss: 1.9092
2022-03-09 02:06:47 - train: epoch 0174, iter [01100, 05004], lr: 0.004656, loss: 1.9336
2022-03-09 02:07:19 - train: epoch 0174, iter [01200, 05004], lr: 0.004656, loss: 1.8919
2022-03-09 02:07:52 - train: epoch 0174, iter [01300, 05004], lr: 0.004656, loss: 2.0748
2022-03-09 02:08:25 - train: epoch 0174, iter [01400, 05004], lr: 0.004656, loss: 2.0731
2022-03-09 02:08:57 - train: epoch 0174, iter [01500, 05004], lr: 0.004656, loss: 2.0851
2022-03-09 02:09:30 - train: epoch 0174, iter [01600, 05004], lr: 0.004656, loss: 2.1243
2022-03-09 02:10:03 - train: epoch 0174, iter [01700, 05004], lr: 0.004656, loss: 2.0351
2022-03-09 02:10:36 - train: epoch 0174, iter [01800, 05004], lr: 0.004656, loss: 2.1161
2022-03-09 02:11:08 - train: epoch 0174, iter [01900, 05004], lr: 0.004656, loss: 1.9444
2022-03-09 02:11:40 - train: epoch 0174, iter [02000, 05004], lr: 0.004656, loss: 1.9504
2022-03-09 02:12:14 - train: epoch 0174, iter [02100, 05004], lr: 0.004656, loss: 1.9454
2022-03-09 02:12:47 - train: epoch 0174, iter [02200, 05004], lr: 0.004656, loss: 2.0043
2022-03-09 02:13:20 - train: epoch 0174, iter [02300, 05004], lr: 0.004656, loss: 2.2670
2022-03-09 02:13:52 - train: epoch 0174, iter [02400, 05004], lr: 0.004656, loss: 1.8157
2022-03-09 02:14:26 - train: epoch 0174, iter [02500, 05004], lr: 0.004656, loss: 1.8686
2022-03-09 02:14:59 - train: epoch 0174, iter [02600, 05004], lr: 0.004656, loss: 1.7598
2022-03-09 02:15:32 - train: epoch 0174, iter [02700, 05004], lr: 0.004656, loss: 1.9428
2022-03-09 02:16:05 - train: epoch 0174, iter [02800, 05004], lr: 0.004656, loss: 1.9382
2022-03-09 02:16:38 - train: epoch 0174, iter [02900, 05004], lr: 0.004656, loss: 1.9780
2022-03-09 02:17:11 - train: epoch 0174, iter [03000, 05004], lr: 0.004656, loss: 1.9536
2022-03-09 02:17:44 - train: epoch 0174, iter [03100, 05004], lr: 0.004656, loss: 1.8394
2022-03-09 02:18:16 - train: epoch 0174, iter [03200, 05004], lr: 0.004656, loss: 2.0880
2022-03-09 02:18:49 - train: epoch 0174, iter [03300, 05004], lr: 0.004656, loss: 2.0837
2022-03-09 02:19:22 - train: epoch 0174, iter [03400, 05004], lr: 0.004656, loss: 1.7635
2022-03-09 02:19:56 - train: epoch 0174, iter [03500, 05004], lr: 0.004656, loss: 2.2313
2022-03-09 02:20:28 - train: epoch 0174, iter [03600, 05004], lr: 0.004656, loss: 2.1190
2022-03-09 02:21:03 - train: epoch 0174, iter [03700, 05004], lr: 0.004656, loss: 2.1056
2022-03-09 02:21:36 - train: epoch 0174, iter [03800, 05004], lr: 0.004656, loss: 1.9076
2022-03-09 02:22:10 - train: epoch 0174, iter [03900, 05004], lr: 0.004656, loss: 2.1455
2022-03-09 02:22:44 - train: epoch 0174, iter [04000, 05004], lr: 0.004656, loss: 1.9432
2022-03-09 02:23:17 - train: epoch 0174, iter [04100, 05004], lr: 0.004656, loss: 1.8823
2022-03-09 02:23:51 - train: epoch 0174, iter [04200, 05004], lr: 0.004656, loss: 2.0838
2022-03-09 02:24:25 - train: epoch 0174, iter [04300, 05004], lr: 0.004656, loss: 1.8409
2022-03-09 02:24:58 - train: epoch 0174, iter [04400, 05004], lr: 0.004656, loss: 2.0817
2022-03-09 02:25:31 - train: epoch 0174, iter [04500, 05004], lr: 0.004656, loss: 2.1201
2022-03-09 02:26:03 - train: epoch 0174, iter [04600, 05004], lr: 0.004656, loss: 1.6944
2022-03-09 02:26:38 - train: epoch 0174, iter [04700, 05004], lr: 0.004656, loss: 2.0936
2022-03-09 02:27:10 - train: epoch 0174, iter [04800, 05004], lr: 0.004656, loss: 1.9775
2022-03-09 02:27:43 - train: epoch 0174, iter [04900, 05004], lr: 0.004656, loss: 1.6963
2022-03-09 02:28:16 - train: epoch 0174, iter [05000, 05004], lr: 0.004656, loss: 2.1921
2022-03-09 02:28:17 - train: epoch 174, train_loss: 2.0042
2022-03-09 02:29:30 - eval: epoch: 174, acc1: 73.084%, acc5: 91.222%, test_loss: 1.0795, per_image_load_time: 1.460ms, per_image_inference_time: 0.495ms
2022-03-09 02:29:31 - until epoch: 174, best_acc1: 73.848%
2022-03-09 02:29:31 - epoch 175 lr: 0.004322727117869951
2022-03-09 02:30:10 - train: epoch 0175, iter [00100, 05004], lr: 0.004323, loss: 1.8228
2022-03-09 02:30:43 - train: epoch 0175, iter [00200, 05004], lr: 0.004323, loss: 1.9440
2022-03-09 02:31:17 - train: epoch 0175, iter [00300, 05004], lr: 0.004323, loss: 1.9166
2022-03-09 02:31:50 - train: epoch 0175, iter [00400, 05004], lr: 0.004323, loss: 1.8704
2022-03-09 02:32:24 - train: epoch 0175, iter [00500, 05004], lr: 0.004323, loss: 1.8237
2022-03-09 02:32:57 - train: epoch 0175, iter [00600, 05004], lr: 0.004323, loss: 1.8053
2022-03-09 02:33:31 - train: epoch 0175, iter [00700, 05004], lr: 0.004323, loss: 2.1079
2022-03-09 02:34:04 - train: epoch 0175, iter [00800, 05004], lr: 0.004323, loss: 2.4232
2022-03-09 02:34:38 - train: epoch 0175, iter [00900, 05004], lr: 0.004323, loss: 1.9566
2022-03-09 02:35:12 - train: epoch 0175, iter [01000, 05004], lr: 0.004323, loss: 1.8957
2022-03-09 02:35:45 - train: epoch 0175, iter [01100, 05004], lr: 0.004323, loss: 1.6736
2022-03-09 02:36:18 - train: epoch 0175, iter [01200, 05004], lr: 0.004323, loss: 2.0600
2022-03-09 02:36:52 - train: epoch 0175, iter [01300, 05004], lr: 0.004323, loss: 2.0332
2022-03-09 02:37:26 - train: epoch 0175, iter [01400, 05004], lr: 0.004323, loss: 2.0568
2022-03-09 02:38:00 - train: epoch 0175, iter [01500, 05004], lr: 0.004323, loss: 2.1894
2022-03-09 02:38:33 - train: epoch 0175, iter [01600, 05004], lr: 0.004323, loss: 2.1706
2022-03-09 02:39:07 - train: epoch 0175, iter [01700, 05004], lr: 0.004323, loss: 2.0364
2022-03-09 02:39:40 - train: epoch 0175, iter [01800, 05004], lr: 0.004323, loss: 1.8840
2022-03-09 02:40:14 - train: epoch 0175, iter [01900, 05004], lr: 0.004323, loss: 1.8533
2022-03-09 02:40:48 - train: epoch 0175, iter [02000, 05004], lr: 0.004323, loss: 1.9941
2022-03-09 02:41:21 - train: epoch 0175, iter [02100, 05004], lr: 0.004323, loss: 1.6340
2022-03-09 02:41:56 - train: epoch 0175, iter [02200, 05004], lr: 0.004323, loss: 1.9140
2022-03-09 02:42:29 - train: epoch 0175, iter [02300, 05004], lr: 0.004323, loss: 2.0627
2022-03-09 02:43:03 - train: epoch 0175, iter [02400, 05004], lr: 0.004323, loss: 1.8234
2022-03-09 02:43:36 - train: epoch 0175, iter [02500, 05004], lr: 0.004323, loss: 1.8937
2022-03-09 02:44:11 - train: epoch 0175, iter [02600, 05004], lr: 0.004323, loss: 2.0566
2022-03-09 02:44:44 - train: epoch 0175, iter [02700, 05004], lr: 0.004323, loss: 1.9263
2022-03-09 02:45:18 - train: epoch 0175, iter [02800, 05004], lr: 0.004323, loss: 2.1296
2022-03-09 02:45:51 - train: epoch 0175, iter [02900, 05004], lr: 0.004323, loss: 1.9813
2022-03-09 02:46:25 - train: epoch 0175, iter [03000, 05004], lr: 0.004323, loss: 1.8325
2022-03-09 02:46:59 - train: epoch 0175, iter [03100, 05004], lr: 0.004323, loss: 1.9878
2022-03-09 02:47:33 - train: epoch 0175, iter [03200, 05004], lr: 0.004323, loss: 2.1531
2022-03-09 02:48:06 - train: epoch 0175, iter [03300, 05004], lr: 0.004323, loss: 2.0789
2022-03-09 02:48:40 - train: epoch 0175, iter [03400, 05004], lr: 0.004323, loss: 2.1243
2022-03-09 02:49:13 - train: epoch 0175, iter [03500, 05004], lr: 0.004323, loss: 1.7854
2022-03-09 02:49:46 - train: epoch 0175, iter [03600, 05004], lr: 0.004323, loss: 2.1025
2022-03-09 02:50:20 - train: epoch 0175, iter [03700, 05004], lr: 0.004323, loss: 1.9769
2022-03-09 02:50:52 - train: epoch 0175, iter [03800, 05004], lr: 0.004323, loss: 1.9224
2022-03-09 02:51:26 - train: epoch 0175, iter [03900, 05004], lr: 0.004323, loss: 1.9635
2022-03-09 02:52:00 - train: epoch 0175, iter [04000, 05004], lr: 0.004323, loss: 1.8803
2022-03-09 02:52:33 - train: epoch 0175, iter [04100, 05004], lr: 0.004323, loss: 2.0274
2022-03-09 02:53:07 - train: epoch 0175, iter [04200, 05004], lr: 0.004323, loss: 1.9979
2022-03-09 02:53:41 - train: epoch 0175, iter [04300, 05004], lr: 0.004323, loss: 1.7626
2022-03-09 02:54:15 - train: epoch 0175, iter [04400, 05004], lr: 0.004323, loss: 1.8548
2022-03-09 02:54:48 - train: epoch 0175, iter [04500, 05004], lr: 0.004323, loss: 1.7529
2022-03-09 02:55:22 - train: epoch 0175, iter [04600, 05004], lr: 0.004323, loss: 1.8176
2022-03-09 02:55:55 - train: epoch 0175, iter [04700, 05004], lr: 0.004323, loss: 2.0955
2022-03-09 02:56:29 - train: epoch 0175, iter [04800, 05004], lr: 0.004323, loss: 2.1460
2022-03-09 02:57:02 - train: epoch 0175, iter [04900, 05004], lr: 0.004323, loss: 2.2266
2022-03-09 02:57:34 - train: epoch 0175, iter [05000, 05004], lr: 0.004323, loss: 2.0107
2022-03-09 02:57:35 - train: epoch 175, train_loss: 1.9897
2022-03-09 02:58:48 - eval: epoch: 175, acc1: 74.136%, acc5: 91.854%, test_loss: 1.0328, per_image_load_time: 1.885ms, per_image_inference_time: 0.473ms
2022-03-09 02:58:49 - until epoch: 175, best_acc1: 74.136%
2022-03-09 02:58:49 - epoch 176 lr: 0.0040010278170587886
2022-03-09 02:59:28 - train: epoch 0176, iter [00100, 05004], lr: 0.004001, loss: 1.6798
2022-03-09 03:00:01 - train: epoch 0176, iter [00200, 05004], lr: 0.004001, loss: 1.6975
2022-03-09 03:00:34 - train: epoch 0176, iter [00300, 05004], lr: 0.004001, loss: 2.0205
2022-03-09 03:01:07 - train: epoch 0176, iter [00400, 05004], lr: 0.004001, loss: 1.9464
2022-03-09 03:01:41 - train: epoch 0176, iter [00500, 05004], lr: 0.004001, loss: 2.2758
2022-03-09 03:02:14 - train: epoch 0176, iter [00600, 05004], lr: 0.004001, loss: 1.8696
2022-03-09 03:02:47 - train: epoch 0176, iter [00700, 05004], lr: 0.004001, loss: 2.1761
2022-03-09 03:03:21 - train: epoch 0176, iter [00800, 05004], lr: 0.004001, loss: 2.1365
2022-03-09 03:03:54 - train: epoch 0176, iter [00900, 05004], lr: 0.004001, loss: 1.6710
2022-03-09 03:04:27 - train: epoch 0176, iter [01000, 05004], lr: 0.004001, loss: 1.8421
2022-03-09 03:05:00 - train: epoch 0176, iter [01100, 05004], lr: 0.004001, loss: 1.5542
2022-03-09 03:05:33 - train: epoch 0176, iter [01200, 05004], lr: 0.004001, loss: 2.0247
2022-03-09 03:06:07 - train: epoch 0176, iter [01300, 05004], lr: 0.004001, loss: 1.9364
2022-03-09 03:06:41 - train: epoch 0176, iter [01400, 05004], lr: 0.004001, loss: 2.0827
2022-03-09 03:07:14 - train: epoch 0176, iter [01500, 05004], lr: 0.004001, loss: 1.7686
2022-03-09 03:07:47 - train: epoch 0176, iter [01600, 05004], lr: 0.004001, loss: 1.8063
2022-03-09 03:08:21 - train: epoch 0176, iter [01700, 05004], lr: 0.004001, loss: 1.8854
2022-03-09 03:08:55 - train: epoch 0176, iter [01800, 05004], lr: 0.004001, loss: 2.1649
2022-03-09 03:09:28 - train: epoch 0176, iter [01900, 05004], lr: 0.004001, loss: 1.9856
2022-03-09 03:10:02 - train: epoch 0176, iter [02000, 05004], lr: 0.004001, loss: 1.8290
2022-03-09 03:10:35 - train: epoch 0176, iter [02100, 05004], lr: 0.004001, loss: 1.9547
2022-03-09 03:11:08 - train: epoch 0176, iter [02200, 05004], lr: 0.004001, loss: 1.9721
2022-03-09 03:11:42 - train: epoch 0176, iter [02300, 05004], lr: 0.004001, loss: 2.0044
2022-03-09 03:12:16 - train: epoch 0176, iter [02400, 05004], lr: 0.004001, loss: 1.9894
2022-03-09 03:12:49 - train: epoch 0176, iter [02500, 05004], lr: 0.004001, loss: 2.0901
2022-03-09 03:13:23 - train: epoch 0176, iter [02600, 05004], lr: 0.004001, loss: 1.9032
2022-03-09 03:13:56 - train: epoch 0176, iter [02700, 05004], lr: 0.004001, loss: 1.9356
2022-03-09 03:14:29 - train: epoch 0176, iter [02800, 05004], lr: 0.004001, loss: 1.9719
2022-03-09 03:15:03 - train: epoch 0176, iter [02900, 05004], lr: 0.004001, loss: 2.2121
2022-03-09 03:15:36 - train: epoch 0176, iter [03000, 05004], lr: 0.004001, loss: 2.2721
2022-03-09 03:16:10 - train: epoch 0176, iter [03100, 05004], lr: 0.004001, loss: 1.9413
2022-03-09 03:16:44 - train: epoch 0176, iter [03200, 05004], lr: 0.004001, loss: 1.9692
2022-03-09 03:17:17 - train: epoch 0176, iter [03300, 05004], lr: 0.004001, loss: 1.8754
2022-03-09 03:17:50 - train: epoch 0176, iter [03400, 05004], lr: 0.004001, loss: 1.9795
2022-03-09 03:18:24 - train: epoch 0176, iter [03500, 05004], lr: 0.004001, loss: 1.8242
2022-03-09 03:18:58 - train: epoch 0176, iter [03600, 05004], lr: 0.004001, loss: 2.0442
2022-03-09 03:19:30 - train: epoch 0176, iter [03700, 05004], lr: 0.004001, loss: 1.8127
2022-03-09 03:20:03 - train: epoch 0176, iter [03800, 05004], lr: 0.004001, loss: 1.8140
2022-03-09 03:20:36 - train: epoch 0176, iter [03900, 05004], lr: 0.004001, loss: 2.0058
2022-03-09 03:21:10 - train: epoch 0176, iter [04000, 05004], lr: 0.004001, loss: 1.9939
2022-03-09 03:21:44 - train: epoch 0176, iter [04100, 05004], lr: 0.004001, loss: 1.8624
2022-03-09 03:22:18 - train: epoch 0176, iter [04200, 05004], lr: 0.004001, loss: 2.1568
2022-03-09 03:22:50 - train: epoch 0176, iter [04300, 05004], lr: 0.004001, loss: 1.9934
2022-03-09 03:23:24 - train: epoch 0176, iter [04400, 05004], lr: 0.004001, loss: 1.8807
2022-03-09 03:23:58 - train: epoch 0176, iter [04500, 05004], lr: 0.004001, loss: 2.1123
2022-03-09 03:24:31 - train: epoch 0176, iter [04600, 05004], lr: 0.004001, loss: 1.8516
2022-03-09 03:25:04 - train: epoch 0176, iter [04700, 05004], lr: 0.004001, loss: 2.0206
2022-03-09 03:25:38 - train: epoch 0176, iter [04800, 05004], lr: 0.004001, loss: 1.6693
2022-03-09 03:26:11 - train: epoch 0176, iter [04900, 05004], lr: 0.004001, loss: 1.8093
2022-03-09 03:26:42 - train: epoch 0176, iter [05000, 05004], lr: 0.004001, loss: 2.0489
2022-03-09 03:26:43 - train: epoch 176, train_loss: 1.9758
2022-03-09 03:27:56 - eval: epoch: 176, acc1: 74.640%, acc5: 92.238%, test_loss: 1.0040, per_image_load_time: 1.867ms, per_image_inference_time: 0.516ms
2022-03-09 03:27:57 - until epoch: 176, best_acc1: 74.640%
2022-03-09 03:27:57 - epoch 177 lr: 0.003691267552111183
2022-03-09 03:28:36 - train: epoch 0177, iter [00100, 05004], lr: 0.003691, loss: 1.9187
2022-03-09 03:29:10 - train: epoch 0177, iter [00200, 05004], lr: 0.003691, loss: 2.1168
2022-03-09 03:29:43 - train: epoch 0177, iter [00300, 05004], lr: 0.003691, loss: 1.8785
2022-03-09 03:30:17 - train: epoch 0177, iter [00400, 05004], lr: 0.003691, loss: 2.2663
2022-03-09 03:30:50 - train: epoch 0177, iter [00500, 05004], lr: 0.003691, loss: 2.1299
2022-03-09 03:31:24 - train: epoch 0177, iter [00600, 05004], lr: 0.003691, loss: 2.1378
2022-03-09 03:31:57 - train: epoch 0177, iter [00700, 05004], lr: 0.003691, loss: 1.9612
2022-03-09 03:32:30 - train: epoch 0177, iter [00800, 05004], lr: 0.003691, loss: 2.3033
2022-03-09 03:33:03 - train: epoch 0177, iter [00900, 05004], lr: 0.003691, loss: 1.9767
2022-03-09 03:33:37 - train: epoch 0177, iter [01000, 05004], lr: 0.003691, loss: 1.8132
2022-03-09 03:34:10 - train: epoch 0177, iter [01100, 05004], lr: 0.003691, loss: 1.9091
2022-03-09 03:34:43 - train: epoch 0177, iter [01200, 05004], lr: 0.003691, loss: 1.8608
2022-03-09 03:35:16 - train: epoch 0177, iter [01300, 05004], lr: 0.003691, loss: 2.0197
2022-03-09 03:35:49 - train: epoch 0177, iter [01400, 05004], lr: 0.003691, loss: 2.1049
2022-03-09 03:36:22 - train: epoch 0177, iter [01500, 05004], lr: 0.003691, loss: 2.1217
2022-03-09 03:36:56 - train: epoch 0177, iter [01600, 05004], lr: 0.003691, loss: 1.9639
2022-03-09 03:37:28 - train: epoch 0177, iter [01700, 05004], lr: 0.003691, loss: 2.1579
2022-03-09 03:38:01 - train: epoch 0177, iter [01800, 05004], lr: 0.003691, loss: 1.9297
2022-03-09 03:38:35 - train: epoch 0177, iter [01900, 05004], lr: 0.003691, loss: 1.7869
2022-03-09 03:39:09 - train: epoch 0177, iter [02000, 05004], lr: 0.003691, loss: 1.9131
2022-03-09 03:39:42 - train: epoch 0177, iter [02100, 05004], lr: 0.003691, loss: 2.0017
2022-03-09 03:40:15 - train: epoch 0177, iter [02200, 05004], lr: 0.003691, loss: 1.9856
2022-03-09 03:40:49 - train: epoch 0177, iter [02300, 05004], lr: 0.003691, loss: 1.7733
2022-03-09 03:41:22 - train: epoch 0177, iter [02400, 05004], lr: 0.003691, loss: 2.0016
2022-03-09 03:41:56 - train: epoch 0177, iter [02500, 05004], lr: 0.003691, loss: 1.9977
2022-03-09 03:42:29 - train: epoch 0177, iter [02600, 05004], lr: 0.003691, loss: 1.8046
2022-03-09 03:43:04 - train: epoch 0177, iter [02700, 05004], lr: 0.003691, loss: 1.8587
2022-03-09 03:43:36 - train: epoch 0177, iter [02800, 05004], lr: 0.003691, loss: 1.7433
2022-03-09 03:44:10 - train: epoch 0177, iter [02900, 05004], lr: 0.003691, loss: 2.1365
2022-03-09 03:44:42 - train: epoch 0177, iter [03000, 05004], lr: 0.003691, loss: 1.8630
2022-03-09 03:45:17 - train: epoch 0177, iter [03100, 05004], lr: 0.003691, loss: 1.7965
2022-03-09 03:45:50 - train: epoch 0177, iter [03200, 05004], lr: 0.003691, loss: 1.8299
2022-03-09 03:46:23 - train: epoch 0177, iter [03300, 05004], lr: 0.003691, loss: 2.0013
2022-03-09 03:46:57 - train: epoch 0177, iter [03400, 05004], lr: 0.003691, loss: 1.9934
2022-03-09 03:47:30 - train: epoch 0177, iter [03500, 05004], lr: 0.003691, loss: 1.8242
2022-03-09 03:48:03 - train: epoch 0177, iter [03600, 05004], lr: 0.003691, loss: 1.8457
2022-03-09 03:48:36 - train: epoch 0177, iter [03700, 05004], lr: 0.003691, loss: 1.8367
2022-03-09 03:49:10 - train: epoch 0177, iter [03800, 05004], lr: 0.003691, loss: 1.8637
2022-03-09 03:49:44 - train: epoch 0177, iter [03900, 05004], lr: 0.003691, loss: 1.5883
2022-03-09 03:50:18 - train: epoch 0177, iter [04000, 05004], lr: 0.003691, loss: 1.7607
2022-03-09 03:50:50 - train: epoch 0177, iter [04100, 05004], lr: 0.003691, loss: 2.0030
2022-03-09 03:51:23 - train: epoch 0177, iter [04200, 05004], lr: 0.003691, loss: 2.2902
2022-03-09 03:51:56 - train: epoch 0177, iter [04300, 05004], lr: 0.003691, loss: 2.2227
2022-03-09 03:52:29 - train: epoch 0177, iter [04400, 05004], lr: 0.003691, loss: 1.8238
2022-03-09 03:53:03 - train: epoch 0177, iter [04500, 05004], lr: 0.003691, loss: 2.0099
2022-03-09 03:53:37 - train: epoch 0177, iter [04600, 05004], lr: 0.003691, loss: 1.9177
2022-03-09 03:54:10 - train: epoch 0177, iter [04700, 05004], lr: 0.003691, loss: 1.7689
2022-03-09 03:54:43 - train: epoch 0177, iter [04800, 05004], lr: 0.003691, loss: 2.1568
2022-03-09 03:55:17 - train: epoch 0177, iter [04900, 05004], lr: 0.003691, loss: 2.2090
2022-03-09 03:55:49 - train: epoch 0177, iter [05000, 05004], lr: 0.003691, loss: 2.1749
2022-03-09 03:55:50 - train: epoch 177, train_loss: 1.9588
2022-03-09 03:57:04 - eval: epoch: 177, acc1: 74.676%, acc5: 92.174%, test_loss: 1.0055, per_image_load_time: 2.096ms, per_image_inference_time: 0.499ms
2022-03-09 03:57:05 - until epoch: 177, best_acc1: 74.676%
2022-03-09 03:57:05 - epoch 178 lr: 0.003393526721321616
2022-03-09 03:57:43 - train: epoch 0178, iter [00100, 05004], lr: 0.003394, loss: 1.8097
2022-03-09 03:58:16 - train: epoch 0178, iter [00200, 05004], lr: 0.003394, loss: 1.8950
2022-03-09 03:58:50 - train: epoch 0178, iter [00300, 05004], lr: 0.003394, loss: 2.1030
2022-03-09 03:59:23 - train: epoch 0178, iter [00400, 05004], lr: 0.003394, loss: 1.9173
2022-03-09 03:59:57 - train: epoch 0178, iter [00500, 05004], lr: 0.003394, loss: 1.8988
2022-03-09 04:00:30 - train: epoch 0178, iter [00600, 05004], lr: 0.003394, loss: 1.7881
2022-03-09 04:01:04 - train: epoch 0178, iter [00700, 05004], lr: 0.003394, loss: 1.9297
2022-03-09 04:01:37 - train: epoch 0178, iter [00800, 05004], lr: 0.003394, loss: 1.8908
2022-03-09 04:02:10 - train: epoch 0178, iter [00900, 05004], lr: 0.003394, loss: 2.2414
2022-03-09 04:02:44 - train: epoch 0178, iter [01000, 05004], lr: 0.003394, loss: 2.1279
2022-03-09 04:03:17 - train: epoch 0178, iter [01100, 05004], lr: 0.003394, loss: 1.9002
2022-03-09 04:03:51 - train: epoch 0178, iter [01200, 05004], lr: 0.003394, loss: 1.7961
2022-03-09 04:04:24 - train: epoch 0178, iter [01300, 05004], lr: 0.003394, loss: 1.7795
2022-03-09 04:04:58 - train: epoch 0178, iter [01400, 05004], lr: 0.003394, loss: 1.9128
2022-03-09 04:05:32 - train: epoch 0178, iter [01500, 05004], lr: 0.003394, loss: 2.0757
2022-03-09 04:06:05 - train: epoch 0178, iter [01600, 05004], lr: 0.003394, loss: 1.6887
2022-03-09 04:06:38 - train: epoch 0178, iter [01700, 05004], lr: 0.003394, loss: 2.0007
2022-03-09 04:07:11 - train: epoch 0178, iter [01800, 05004], lr: 0.003394, loss: 1.8700
2022-03-09 04:07:44 - train: epoch 0178, iter [01900, 05004], lr: 0.003394, loss: 2.0852
2022-03-09 04:08:18 - train: epoch 0178, iter [02000, 05004], lr: 0.003394, loss: 1.9541
2022-03-09 04:08:52 - train: epoch 0178, iter [02100, 05004], lr: 0.003394, loss: 1.7262
2022-03-09 04:09:25 - train: epoch 0178, iter [02200, 05004], lr: 0.003394, loss: 1.9107
2022-03-09 04:09:58 - train: epoch 0178, iter [02300, 05004], lr: 0.003394, loss: 1.9567
2022-03-09 04:10:31 - train: epoch 0178, iter [02400, 05004], lr: 0.003394, loss: 1.9432
2022-03-09 04:11:04 - train: epoch 0178, iter [02500, 05004], lr: 0.003394, loss: 1.8289
2022-03-09 04:11:38 - train: epoch 0178, iter [02600, 05004], lr: 0.003394, loss: 1.7764
2022-03-09 04:12:12 - train: epoch 0178, iter [02700, 05004], lr: 0.003394, loss: 2.0053
2022-03-09 04:12:46 - train: epoch 0178, iter [02800, 05004], lr: 0.003394, loss: 1.7620
2022-03-09 04:13:18 - train: epoch 0178, iter [02900, 05004], lr: 0.003394, loss: 2.0490
2022-03-09 04:13:52 - train: epoch 0178, iter [03000, 05004], lr: 0.003394, loss: 2.1085
2022-03-09 04:14:25 - train: epoch 0178, iter [03100, 05004], lr: 0.003394, loss: 1.8001
2022-03-09 04:14:59 - train: epoch 0178, iter [03200, 05004], lr: 0.003394, loss: 1.8822
2022-03-09 04:15:32 - train: epoch 0178, iter [03300, 05004], lr: 0.003394, loss: 1.7693
2022-03-09 04:16:06 - train: epoch 0178, iter [03400, 05004], lr: 0.003394, loss: 1.7687
2022-03-09 04:16:40 - train: epoch 0178, iter [03500, 05004], lr: 0.003394, loss: 1.9871
2022-03-09 04:17:13 - train: epoch 0178, iter [03600, 05004], lr: 0.003394, loss: 1.9351
2022-03-09 04:17:46 - train: epoch 0178, iter [03700, 05004], lr: 0.003394, loss: 1.8978
2022-03-09 04:18:19 - train: epoch 0178, iter [03800, 05004], lr: 0.003394, loss: 1.8624
2022-03-09 04:18:53 - train: epoch 0178, iter [03900, 05004], lr: 0.003394, loss: 2.0730
2022-03-09 04:19:26 - train: epoch 0178, iter [04000, 05004], lr: 0.003394, loss: 1.8422
2022-03-09 04:20:00 - train: epoch 0178, iter [04100, 05004], lr: 0.003394, loss: 2.0240
2022-03-09 04:20:34 - train: epoch 0178, iter [04200, 05004], lr: 0.003394, loss: 1.8099
2022-03-09 04:21:06 - train: epoch 0178, iter [04300, 05004], lr: 0.003394, loss: 1.8478
2022-03-09 04:21:39 - train: epoch 0178, iter [04400, 05004], lr: 0.003394, loss: 1.9576
2022-03-09 04:22:12 - train: epoch 0178, iter [04500, 05004], lr: 0.003394, loss: 2.0170
2022-03-09 04:22:46 - train: epoch 0178, iter [04600, 05004], lr: 0.003394, loss: 2.0618
2022-03-09 04:23:18 - train: epoch 0178, iter [04700, 05004], lr: 0.003394, loss: 1.9677
2022-03-09 04:23:51 - train: epoch 0178, iter [04800, 05004], lr: 0.003394, loss: 1.8875
2022-03-09 04:24:26 - train: epoch 0178, iter [04900, 05004], lr: 0.003394, loss: 2.0070
2022-03-09 04:24:57 - train: epoch 0178, iter [05000, 05004], lr: 0.003394, loss: 2.0596
2022-03-09 04:24:58 - train: epoch 178, train_loss: 1.9426
2022-03-09 04:26:11 - eval: epoch: 178, acc1: 74.802%, acc5: 92.388%, test_loss: 0.9975, per_image_load_time: 1.883ms, per_image_inference_time: 0.533ms
2022-03-09 04:26:12 - until epoch: 178, best_acc1: 74.802%
2022-03-09 04:26:12 - epoch 179 lr: 0.0031078826033397846
2022-03-09 04:26:50 - train: epoch 0179, iter [00100, 05004], lr: 0.003108, loss: 2.0978
2022-03-09 04:27:23 - train: epoch 0179, iter [00200, 05004], lr: 0.003108, loss: 1.9923
2022-03-09 04:27:57 - train: epoch 0179, iter [00300, 05004], lr: 0.003108, loss: 2.3054
2022-03-09 04:28:30 - train: epoch 0179, iter [00400, 05004], lr: 0.003108, loss: 1.7228
2022-03-09 04:29:03 - train: epoch 0179, iter [00500, 05004], lr: 0.003108, loss: 1.7084
2022-03-09 04:29:37 - train: epoch 0179, iter [00600, 05004], lr: 0.003108, loss: 1.9801
2022-03-09 04:30:11 - train: epoch 0179, iter [00700, 05004], lr: 0.003108, loss: 1.7781
2022-03-09 04:30:44 - train: epoch 0179, iter [00800, 05004], lr: 0.003108, loss: 1.6976
2022-03-09 04:31:18 - train: epoch 0179, iter [00900, 05004], lr: 0.003108, loss: 1.8726
2022-03-09 04:31:50 - train: epoch 0179, iter [01000, 05004], lr: 0.003108, loss: 1.6715
2022-03-09 04:32:24 - train: epoch 0179, iter [01100, 05004], lr: 0.003108, loss: 1.7352
2022-03-09 04:32:58 - train: epoch 0179, iter [01200, 05004], lr: 0.003108, loss: 2.1220
2022-03-09 04:33:31 - train: epoch 0179, iter [01300, 05004], lr: 0.003108, loss: 2.0853
2022-03-09 04:34:05 - train: epoch 0179, iter [01400, 05004], lr: 0.003108, loss: 1.9049
2022-03-09 04:34:38 - train: epoch 0179, iter [01500, 05004], lr: 0.003108, loss: 1.9861
2022-03-09 04:35:11 - train: epoch 0179, iter [01600, 05004], lr: 0.003108, loss: 1.8188
2022-03-09 04:35:44 - train: epoch 0179, iter [01700, 05004], lr: 0.003108, loss: 1.9942
2022-03-09 04:36:18 - train: epoch 0179, iter [01800, 05004], lr: 0.003108, loss: 1.7637
2022-03-09 04:36:51 - train: epoch 0179, iter [01900, 05004], lr: 0.003108, loss: 1.9117
2022-03-09 04:37:25 - train: epoch 0179, iter [02000, 05004], lr: 0.003108, loss: 2.1531
2022-03-09 04:37:57 - train: epoch 0179, iter [02100, 05004], lr: 0.003108, loss: 1.9811
2022-03-09 04:38:31 - train: epoch 0179, iter [02200, 05004], lr: 0.003108, loss: 1.9816
2022-03-09 04:39:03 - train: epoch 0179, iter [02300, 05004], lr: 0.003108, loss: 1.8217
2022-03-09 04:39:37 - train: epoch 0179, iter [02400, 05004], lr: 0.003108, loss: 2.0734
2022-03-09 04:40:11 - train: epoch 0179, iter [02500, 05004], lr: 0.003108, loss: 2.2099
2022-03-09 04:40:44 - train: epoch 0179, iter [02600, 05004], lr: 0.003108, loss: 1.9395
2022-03-09 04:41:17 - train: epoch 0179, iter [02700, 05004], lr: 0.003108, loss: 2.1606
2022-03-09 04:41:51 - train: epoch 0179, iter [02800, 05004], lr: 0.003108, loss: 2.0613
2022-03-09 04:42:24 - train: epoch 0179, iter [02900, 05004], lr: 0.003108, loss: 2.0962
2022-03-09 04:42:57 - train: epoch 0179, iter [03000, 05004], lr: 0.003108, loss: 1.8908
2022-03-09 04:43:30 - train: epoch 0179, iter [03100, 05004], lr: 0.003108, loss: 2.0047
2022-03-09 04:44:03 - train: epoch 0179, iter [03200, 05004], lr: 0.003108, loss: 2.2083
2022-03-09 04:44:36 - train: epoch 0179, iter [03300, 05004], lr: 0.003108, loss: 2.0278
2022-03-09 04:45:10 - train: epoch 0179, iter [03400, 05004], lr: 0.003108, loss: 1.8404
2022-03-09 04:45:44 - train: epoch 0179, iter [03500, 05004], lr: 0.003108, loss: 1.7488
2022-03-09 04:46:17 - train: epoch 0179, iter [03600, 05004], lr: 0.003108, loss: 1.9548
2022-03-09 04:46:51 - train: epoch 0179, iter [03700, 05004], lr: 0.003108, loss: 1.7784
2022-03-09 04:47:24 - train: epoch 0179, iter [03800, 05004], lr: 0.003108, loss: 1.7024
2022-03-09 04:47:57 - train: epoch 0179, iter [03900, 05004], lr: 0.003108, loss: 2.0798
2022-03-09 04:48:31 - train: epoch 0179, iter [04000, 05004], lr: 0.003108, loss: 1.7980
2022-03-09 04:49:04 - train: epoch 0179, iter [04100, 05004], lr: 0.003108, loss: 2.0722
2022-03-09 04:49:37 - train: epoch 0179, iter [04200, 05004], lr: 0.003108, loss: 1.7978
2022-03-09 04:50:12 - train: epoch 0179, iter [04300, 05004], lr: 0.003108, loss: 1.8822
2022-03-09 04:50:46 - train: epoch 0179, iter [04400, 05004], lr: 0.003108, loss: 2.0516
2022-03-09 04:51:18 - train: epoch 0179, iter [04500, 05004], lr: 0.003108, loss: 1.9179
2022-03-09 04:51:52 - train: epoch 0179, iter [04600, 05004], lr: 0.003108, loss: 1.7417
2022-03-09 04:52:25 - train: epoch 0179, iter [04700, 05004], lr: 0.003108, loss: 1.9829
2022-03-09 04:52:59 - train: epoch 0179, iter [04800, 05004], lr: 0.003108, loss: 1.8831
2022-03-09 04:53:32 - train: epoch 0179, iter [04900, 05004], lr: 0.003108, loss: 1.9339
2022-03-09 04:54:04 - train: epoch 0179, iter [05000, 05004], lr: 0.003108, loss: 1.8584
2022-03-09 04:54:05 - train: epoch 179, train_loss: 1.9269
2022-03-09 04:55:17 - eval: epoch: 179, acc1: 75.148%, acc5: 92.456%, test_loss: 0.9872, per_image_load_time: 2.225ms, per_image_inference_time: 0.484ms
2022-03-09 04:55:18 - until epoch: 179, best_acc1: 75.148%
2022-03-09 04:55:18 - epoch 180 lr: 0.0028344093371128424
2022-03-09 04:55:56 - train: epoch 0180, iter [00100, 05004], lr: 0.002834, loss: 1.8321
2022-03-09 04:56:30 - train: epoch 0180, iter [00200, 05004], lr: 0.002834, loss: 1.8559
2022-03-09 04:57:03 - train: epoch 0180, iter [00300, 05004], lr: 0.002834, loss: 1.9486
2022-03-09 04:57:36 - train: epoch 0180, iter [00400, 05004], lr: 0.002834, loss: 1.7281
2022-03-09 04:58:09 - train: epoch 0180, iter [00500, 05004], lr: 0.002834, loss: 1.9870
2022-03-09 04:58:43 - train: epoch 0180, iter [00600, 05004], lr: 0.002834, loss: 1.8267
2022-03-09 04:59:16 - train: epoch 0180, iter [00700, 05004], lr: 0.002834, loss: 2.1775
2022-03-09 04:59:50 - train: epoch 0180, iter [00800, 05004], lr: 0.002834, loss: 1.6840
2022-03-09 05:00:23 - train: epoch 0180, iter [00900, 05004], lr: 0.002834, loss: 2.0074
2022-03-09 05:00:56 - train: epoch 0180, iter [01000, 05004], lr: 0.002834, loss: 1.6052
2022-03-09 05:01:30 - train: epoch 0180, iter [01100, 05004], lr: 0.002834, loss: 2.1839
2022-03-09 05:02:03 - train: epoch 0180, iter [01200, 05004], lr: 0.002834, loss: 1.7201
2022-03-09 05:02:38 - train: epoch 0180, iter [01300, 05004], lr: 0.002834, loss: 1.8836
2022-03-09 05:03:11 - train: epoch 0180, iter [01400, 05004], lr: 0.002834, loss: 1.9345
2022-03-09 05:03:45 - train: epoch 0180, iter [01500, 05004], lr: 0.002834, loss: 1.9894
2022-03-09 05:04:18 - train: epoch 0180, iter [01600, 05004], lr: 0.002834, loss: 1.8986
2022-03-09 05:04:51 - train: epoch 0180, iter [01700, 05004], lr: 0.002834, loss: 2.0256
2022-03-09 05:05:25 - train: epoch 0180, iter [01800, 05004], lr: 0.002834, loss: 2.1016
2022-03-09 05:05:59 - train: epoch 0180, iter [01900, 05004], lr: 0.002834, loss: 1.8429
2022-03-09 05:06:32 - train: epoch 0180, iter [02000, 05004], lr: 0.002834, loss: 2.0144
2022-03-09 05:07:05 - train: epoch 0180, iter [02100, 05004], lr: 0.002834, loss: 1.6349
2022-03-09 05:07:39 - train: epoch 0180, iter [02200, 05004], lr: 0.002834, loss: 1.9282
2022-03-09 05:08:13 - train: epoch 0180, iter [02300, 05004], lr: 0.002834, loss: 2.1908
2022-03-09 05:08:45 - train: epoch 0180, iter [02400, 05004], lr: 0.002834, loss: 2.2055
2022-03-09 05:09:19 - train: epoch 0180, iter [02500, 05004], lr: 0.002834, loss: 2.0334
2022-03-09 05:09:52 - train: epoch 0180, iter [02600, 05004], lr: 0.002834, loss: 2.0038
2022-03-09 05:10:25 - train: epoch 0180, iter [02700, 05004], lr: 0.002834, loss: 1.7818
2022-03-09 05:10:59 - train: epoch 0180, iter [02800, 05004], lr: 0.002834, loss: 2.0035
2022-03-09 05:11:32 - train: epoch 0180, iter [02900, 05004], lr: 0.002834, loss: 1.9392
2022-03-09 05:12:05 - train: epoch 0180, iter [03000, 05004], lr: 0.002834, loss: 1.7433
2022-03-09 05:12:38 - train: epoch 0180, iter [03100, 05004], lr: 0.002834, loss: 2.0231
2022-03-09 05:13:13 - train: epoch 0180, iter [03200, 05004], lr: 0.002834, loss: 1.8118
2022-03-09 05:13:46 - train: epoch 0180, iter [03300, 05004], lr: 0.002834, loss: 1.8651
2022-03-09 05:14:20 - train: epoch 0180, iter [03400, 05004], lr: 0.002834, loss: 2.0256
2022-03-09 05:14:53 - train: epoch 0180, iter [03500, 05004], lr: 0.002834, loss: 1.9054
2022-03-09 05:15:26 - train: epoch 0180, iter [03600, 05004], lr: 0.002834, loss: 1.9852
2022-03-09 05:16:00 - train: epoch 0180, iter [03700, 05004], lr: 0.002834, loss: 2.0382
2022-03-09 05:16:33 - train: epoch 0180, iter [03800, 05004], lr: 0.002834, loss: 2.1064
2022-03-09 05:17:07 - train: epoch 0180, iter [03900, 05004], lr: 0.002834, loss: 2.3462
2022-03-09 05:17:40 - train: epoch 0180, iter [04000, 05004], lr: 0.002834, loss: 1.8317
2022-03-09 05:18:13 - train: epoch 0180, iter [04100, 05004], lr: 0.002834, loss: 1.9616
2022-03-09 05:18:46 - train: epoch 0180, iter [04200, 05004], lr: 0.002834, loss: 1.7218
2022-03-09 05:19:20 - train: epoch 0180, iter [04300, 05004], lr: 0.002834, loss: 1.9616
2022-03-09 05:19:53 - train: epoch 0180, iter [04400, 05004], lr: 0.002834, loss: 1.8509
2022-03-09 05:20:26 - train: epoch 0180, iter [04500, 05004], lr: 0.002834, loss: 1.9427
2022-03-09 05:20:59 - train: epoch 0180, iter [04600, 05004], lr: 0.002834, loss: 1.9392
2022-03-09 05:21:33 - train: epoch 0180, iter [04700, 05004], lr: 0.002834, loss: 2.0291
2022-03-09 05:22:06 - train: epoch 0180, iter [04800, 05004], lr: 0.002834, loss: 1.7199
2022-03-09 05:22:40 - train: epoch 0180, iter [04900, 05004], lr: 0.002834, loss: 1.8539
2022-03-09 05:23:13 - train: epoch 0180, iter [05000, 05004], lr: 0.002834, loss: 1.8255
2022-03-09 05:23:14 - train: epoch 180, train_loss: 1.9096
2022-03-09 05:24:26 - eval: epoch: 180, acc1: 75.476%, acc5: 92.596%, test_loss: 0.9749, per_image_load_time: 1.459ms, per_image_inference_time: 0.483ms
2022-03-09 05:24:27 - until epoch: 180, best_acc1: 75.476%
2022-03-09 05:24:27 - epoch 181 lr: 0.002573177902642726
2022-03-09 05:25:06 - train: epoch 0181, iter [00100, 05004], lr: 0.002573, loss: 2.0077
2022-03-09 05:25:39 - train: epoch 0181, iter [00200, 05004], lr: 0.002573, loss: 1.6091
2022-03-09 05:26:12 - train: epoch 0181, iter [00300, 05004], lr: 0.002573, loss: 2.0837
2022-03-09 05:26:46 - train: epoch 0181, iter [00400, 05004], lr: 0.002573, loss: 2.0373
2022-03-09 05:27:19 - train: epoch 0181, iter [00500, 05004], lr: 0.002573, loss: 2.1547
2022-03-09 05:27:52 - train: epoch 0181, iter [00600, 05004], lr: 0.002573, loss: 1.7677
2022-03-09 05:28:26 - train: epoch 0181, iter [00700, 05004], lr: 0.002573, loss: 1.9251
2022-03-09 05:28:58 - train: epoch 0181, iter [00800, 05004], lr: 0.002573, loss: 1.7994
2022-03-09 05:29:32 - train: epoch 0181, iter [00900, 05004], lr: 0.002573, loss: 1.6220
2022-03-09 05:30:05 - train: epoch 0181, iter [01000, 05004], lr: 0.002573, loss: 1.9286
2022-03-09 05:30:38 - train: epoch 0181, iter [01100, 05004], lr: 0.002573, loss: 1.8749
2022-03-09 05:31:12 - train: epoch 0181, iter [01200, 05004], lr: 0.002573, loss: 1.7553
2022-03-09 05:31:45 - train: epoch 0181, iter [01300, 05004], lr: 0.002573, loss: 1.5188
2022-03-09 05:32:19 - train: epoch 0181, iter [01400, 05004], lr: 0.002573, loss: 1.8423
2022-03-09 05:32:53 - train: epoch 0181, iter [01500, 05004], lr: 0.002573, loss: 1.9181
2022-03-09 05:33:25 - train: epoch 0181, iter [01600, 05004], lr: 0.002573, loss: 1.8421
2022-03-09 05:33:59 - train: epoch 0181, iter [01700, 05004], lr: 0.002573, loss: 1.9177
2022-03-09 05:34:32 - train: epoch 0181, iter [01800, 05004], lr: 0.002573, loss: 1.7585
2022-03-09 05:35:06 - train: epoch 0181, iter [01900, 05004], lr: 0.002573, loss: 1.8029
2022-03-09 05:35:39 - train: epoch 0181, iter [02000, 05004], lr: 0.002573, loss: 2.1751
2022-03-09 05:36:13 - train: epoch 0181, iter [02100, 05004], lr: 0.002573, loss: 2.0434
2022-03-09 05:36:46 - train: epoch 0181, iter [02200, 05004], lr: 0.002573, loss: 1.9985
2022-03-09 05:37:19 - train: epoch 0181, iter [02300, 05004], lr: 0.002573, loss: 2.1004
2022-03-09 05:37:53 - train: epoch 0181, iter [02400, 05004], lr: 0.002573, loss: 1.7390
2022-03-09 05:38:26 - train: epoch 0181, iter [02500, 05004], lr: 0.002573, loss: 1.7198
2022-03-09 05:39:00 - train: epoch 0181, iter [02600, 05004], lr: 0.002573, loss: 1.9118
2022-03-09 05:39:33 - train: epoch 0181, iter [02700, 05004], lr: 0.002573, loss: 2.0222
2022-03-09 05:40:07 - train: epoch 0181, iter [02800, 05004], lr: 0.002573, loss: 2.0419
2022-03-09 05:40:39 - train: epoch 0181, iter [02900, 05004], lr: 0.002573, loss: 1.7846
2022-03-09 05:41:12 - train: epoch 0181, iter [03000, 05004], lr: 0.002573, loss: 2.0128
2022-03-09 05:41:45 - train: epoch 0181, iter [03100, 05004], lr: 0.002573, loss: 2.1299
2022-03-09 05:42:18 - train: epoch 0181, iter [03200, 05004], lr: 0.002573, loss: 1.5583
2022-03-09 05:42:51 - train: epoch 0181, iter [03300, 05004], lr: 0.002573, loss: 1.7224
2022-03-09 05:43:25 - train: epoch 0181, iter [03400, 05004], lr: 0.002573, loss: 1.6958
2022-03-09 05:43:58 - train: epoch 0181, iter [03500, 05004], lr: 0.002573, loss: 1.7320
2022-03-09 05:44:32 - train: epoch 0181, iter [03600, 05004], lr: 0.002573, loss: 1.9726
2022-03-09 05:45:06 - train: epoch 0181, iter [03700, 05004], lr: 0.002573, loss: 1.9254
2022-03-09 05:45:39 - train: epoch 0181, iter [03800, 05004], lr: 0.002573, loss: 1.8951
2022-03-09 05:46:12 - train: epoch 0181, iter [03900, 05004], lr: 0.002573, loss: 1.8699
2022-03-09 05:46:46 - train: epoch 0181, iter [04000, 05004], lr: 0.002573, loss: 1.7777
2022-03-09 05:47:19 - train: epoch 0181, iter [04100, 05004], lr: 0.002573, loss: 1.7722
2022-03-09 05:47:52 - train: epoch 0181, iter [04200, 05004], lr: 0.002573, loss: 1.9177
2022-03-09 05:48:25 - train: epoch 0181, iter [04300, 05004], lr: 0.002573, loss: 2.2095
2022-03-09 05:48:59 - train: epoch 0181, iter [04400, 05004], lr: 0.002573, loss: 1.9729
2022-03-09 05:49:33 - train: epoch 0181, iter [04500, 05004], lr: 0.002573, loss: 2.1952
2022-03-09 05:50:06 - train: epoch 0181, iter [04600, 05004], lr: 0.002573, loss: 1.7276
2022-03-09 05:50:39 - train: epoch 0181, iter [04700, 05004], lr: 0.002573, loss: 1.9233
2022-03-09 05:51:12 - train: epoch 0181, iter [04800, 05004], lr: 0.002573, loss: 1.7172
2022-03-09 05:51:45 - train: epoch 0181, iter [04900, 05004], lr: 0.002573, loss: 1.8240
2022-03-09 05:52:17 - train: epoch 0181, iter [05000, 05004], lr: 0.002573, loss: 1.7463
2022-03-09 05:52:18 - train: epoch 181, train_loss: 1.8951
2022-03-09 05:53:31 - eval: epoch: 181, acc1: 75.882%, acc5: 92.730%, test_loss: 0.9602, per_image_load_time: 2.041ms, per_image_inference_time: 0.497ms
2022-03-09 05:53:32 - until epoch: 181, best_acc1: 75.882%
2022-03-09 05:53:32 - epoch 182 lr: 0.002324256102563188
2022-03-09 05:54:10 - train: epoch 0182, iter [00100, 05004], lr: 0.002324, loss: 1.7177
2022-03-09 05:54:44 - train: epoch 0182, iter [00200, 05004], lr: 0.002324, loss: 1.9024
2022-03-09 05:55:16 - train: epoch 0182, iter [00300, 05004], lr: 0.002324, loss: 1.9866
2022-03-09 05:55:49 - train: epoch 0182, iter [00400, 05004], lr: 0.002324, loss: 1.6547
2022-03-09 05:56:23 - train: epoch 0182, iter [00500, 05004], lr: 0.002324, loss: 1.9735
2022-03-09 05:56:56 - train: epoch 0182, iter [00600, 05004], lr: 0.002324, loss: 1.8120
2022-03-09 05:57:30 - train: epoch 0182, iter [00700, 05004], lr: 0.002324, loss: 1.9079
2022-03-09 05:58:02 - train: epoch 0182, iter [00800, 05004], lr: 0.002324, loss: 1.9015
2022-03-09 05:58:36 - train: epoch 0182, iter [00900, 05004], lr: 0.002324, loss: 1.8882
2022-03-09 05:59:09 - train: epoch 0182, iter [01000, 05004], lr: 0.002324, loss: 1.8410
2022-03-09 05:59:43 - train: epoch 0182, iter [01100, 05004], lr: 0.002324, loss: 1.8390
2022-03-09 06:00:16 - train: epoch 0182, iter [01200, 05004], lr: 0.002324, loss: 1.9703
2022-03-09 06:00:50 - train: epoch 0182, iter [01300, 05004], lr: 0.002324, loss: 1.7523
2022-03-09 06:01:23 - train: epoch 0182, iter [01400, 05004], lr: 0.002324, loss: 2.2232
2022-03-09 06:01:56 - train: epoch 0182, iter [01500, 05004], lr: 0.002324, loss: 2.0075
2022-03-09 06:02:30 - train: epoch 0182, iter [01600, 05004], lr: 0.002324, loss: 1.9857
2022-03-09 06:03:03 - train: epoch 0182, iter [01700, 05004], lr: 0.002324, loss: 1.8925
2022-03-09 06:03:37 - train: epoch 0182, iter [01800, 05004], lr: 0.002324, loss: 1.8436
2022-03-09 06:04:10 - train: epoch 0182, iter [01900, 05004], lr: 0.002324, loss: 1.9300
2022-03-09 06:04:43 - train: epoch 0182, iter [02000, 05004], lr: 0.002324, loss: 2.0450
2022-03-09 06:05:17 - train: epoch 0182, iter [02100, 05004], lr: 0.002324, loss: 1.7163
2022-03-09 06:05:51 - train: epoch 0182, iter [02200, 05004], lr: 0.002324, loss: 1.9243
2022-03-09 06:06:24 - train: epoch 0182, iter [02300, 05004], lr: 0.002324, loss: 1.8451
2022-03-09 06:06:57 - train: epoch 0182, iter [02400, 05004], lr: 0.002324, loss: 1.8202
2022-03-09 06:07:31 - train: epoch 0182, iter [02500, 05004], lr: 0.002324, loss: 1.8748
2022-03-09 06:08:04 - train: epoch 0182, iter [02600, 05004], lr: 0.002324, loss: 1.7117
2022-03-09 06:08:38 - train: epoch 0182, iter [02700, 05004], lr: 0.002324, loss: 1.6139
2022-03-09 06:09:12 - train: epoch 0182, iter [02800, 05004], lr: 0.002324, loss: 2.0129
2022-03-09 06:09:44 - train: epoch 0182, iter [02900, 05004], lr: 0.002324, loss: 1.7286
2022-03-09 06:10:18 - train: epoch 0182, iter [03000, 05004], lr: 0.002324, loss: 1.8113
2022-03-09 06:10:51 - train: epoch 0182, iter [03100, 05004], lr: 0.002324, loss: 2.0536
2022-03-09 06:11:24 - train: epoch 0182, iter [03200, 05004], lr: 0.002324, loss: 1.6143
2022-03-09 06:11:56 - train: epoch 0182, iter [03300, 05004], lr: 0.002324, loss: 1.6338
2022-03-09 06:12:30 - train: epoch 0182, iter [03400, 05004], lr: 0.002324, loss: 2.0887
2022-03-09 06:13:03 - train: epoch 0182, iter [03500, 05004], lr: 0.002324, loss: 1.9431
2022-03-09 06:13:37 - train: epoch 0182, iter [03600, 05004], lr: 0.002324, loss: 1.8414
2022-03-09 06:14:10 - train: epoch 0182, iter [03700, 05004], lr: 0.002324, loss: 1.9693
2022-03-09 06:14:43 - train: epoch 0182, iter [03800, 05004], lr: 0.002324, loss: 1.8633
2022-03-09 06:15:16 - train: epoch 0182, iter [03900, 05004], lr: 0.002324, loss: 1.9828
2022-03-09 06:15:49 - train: epoch 0182, iter [04000, 05004], lr: 0.002324, loss: 1.7231
2022-03-09 06:16:22 - train: epoch 0182, iter [04100, 05004], lr: 0.002324, loss: 2.0341
2022-03-09 06:16:56 - train: epoch 0182, iter [04200, 05004], lr: 0.002324, loss: 1.6956
2022-03-09 06:17:30 - train: epoch 0182, iter [04300, 05004], lr: 0.002324, loss: 2.1052
2022-03-09 06:18:04 - train: epoch 0182, iter [04400, 05004], lr: 0.002324, loss: 1.5972
2022-03-09 06:18:37 - train: epoch 0182, iter [04500, 05004], lr: 0.002324, loss: 1.6868
2022-03-09 06:19:09 - train: epoch 0182, iter [04600, 05004], lr: 0.002324, loss: 1.8883
2022-03-09 06:19:43 - train: epoch 0182, iter [04700, 05004], lr: 0.002324, loss: 1.6521
2022-03-09 06:20:16 - train: epoch 0182, iter [04800, 05004], lr: 0.002324, loss: 1.8395
2022-03-09 06:20:49 - train: epoch 0182, iter [04900, 05004], lr: 0.002324, loss: 1.7877
2022-03-09 06:21:22 - train: epoch 0182, iter [05000, 05004], lr: 0.002324, loss: 1.6939
2022-03-09 06:21:23 - train: epoch 182, train_loss: 1.8822
2022-03-09 06:22:36 - eval: epoch: 182, acc1: 75.934%, acc5: 92.872%, test_loss: 0.9513, per_image_load_time: 1.387ms, per_image_inference_time: 0.477ms
2022-03-09 06:22:37 - until epoch: 182, best_acc1: 75.934%
2022-03-09 06:22:37 - epoch 183 lr: 0.002087708544541689
2022-03-09 06:23:16 - train: epoch 0183, iter [00100, 05004], lr: 0.002088, loss: 2.1352
2022-03-09 06:23:49 - train: epoch 0183, iter [00200, 05004], lr: 0.002088, loss: 1.6594
2022-03-09 06:24:23 - train: epoch 0183, iter [00300, 05004], lr: 0.002088, loss: 1.7505
2022-03-09 06:24:56 - train: epoch 0183, iter [00400, 05004], lr: 0.002088, loss: 1.7373
2022-03-09 06:25:29 - train: epoch 0183, iter [00500, 05004], lr: 0.002088, loss: 1.9873
2022-03-09 06:26:02 - train: epoch 0183, iter [00600, 05004], lr: 0.002088, loss: 1.9130
2022-03-09 06:26:35 - train: epoch 0183, iter [00700, 05004], lr: 0.002088, loss: 1.9142
2022-03-09 06:27:08 - train: epoch 0183, iter [00800, 05004], lr: 0.002088, loss: 1.9162
2022-03-09 06:27:41 - train: epoch 0183, iter [00900, 05004], lr: 0.002088, loss: 1.8015
2022-03-09 06:28:15 - train: epoch 0183, iter [01000, 05004], lr: 0.002088, loss: 1.7441
2022-03-09 06:28:49 - train: epoch 0183, iter [01100, 05004], lr: 0.002088, loss: 1.9554
2022-03-09 06:29:22 - train: epoch 0183, iter [01200, 05004], lr: 0.002088, loss: 1.9661
2022-03-09 06:29:54 - train: epoch 0183, iter [01300, 05004], lr: 0.002088, loss: 1.9485
2022-03-09 06:30:28 - train: epoch 0183, iter [01400, 05004], lr: 0.002088, loss: 2.0810
2022-03-09 06:31:02 - train: epoch 0183, iter [01500, 05004], lr: 0.002088, loss: 2.2219
2022-03-09 06:31:35 - train: epoch 0183, iter [01600, 05004], lr: 0.002088, loss: 1.8557
2022-03-09 06:32:09 - train: epoch 0183, iter [01700, 05004], lr: 0.002088, loss: 1.6986
2022-03-09 06:32:42 - train: epoch 0183, iter [01800, 05004], lr: 0.002088, loss: 1.9846
2022-03-09 06:33:15 - train: epoch 0183, iter [01900, 05004], lr: 0.002088, loss: 1.7425
2022-03-09 06:33:49 - train: epoch 0183, iter [02000, 05004], lr: 0.002088, loss: 1.8136
2022-03-09 06:34:22 - train: epoch 0183, iter [02100, 05004], lr: 0.002088, loss: 1.8838
2022-03-09 06:34:55 - train: epoch 0183, iter [02200, 05004], lr: 0.002088, loss: 2.1623
2022-03-09 06:35:28 - train: epoch 0183, iter [02300, 05004], lr: 0.002088, loss: 1.7631
2022-03-09 06:36:02 - train: epoch 0183, iter [02400, 05004], lr: 0.002088, loss: 1.8773
2022-03-09 06:36:35 - train: epoch 0183, iter [02500, 05004], lr: 0.002088, loss: 1.7668
2022-03-09 06:37:09 - train: epoch 0183, iter [02600, 05004], lr: 0.002088, loss: 1.9470
2022-03-09 06:37:42 - train: epoch 0183, iter [02700, 05004], lr: 0.002088, loss: 1.7956
2022-03-09 06:38:16 - train: epoch 0183, iter [02800, 05004], lr: 0.002088, loss: 1.9559
2022-03-09 06:38:49 - train: epoch 0183, iter [02900, 05004], lr: 0.002088, loss: 2.1354
2022-03-09 06:39:22 - train: epoch 0183, iter [03000, 05004], lr: 0.002088, loss: 2.1656
2022-03-09 06:39:56 - train: epoch 0183, iter [03100, 05004], lr: 0.002088, loss: 1.4985
2022-03-09 06:40:30 - train: epoch 0183, iter [03200, 05004], lr: 0.002088, loss: 1.6180
2022-03-09 06:41:03 - train: epoch 0183, iter [03300, 05004], lr: 0.002088, loss: 1.9284
2022-03-09 06:41:36 - train: epoch 0183, iter [03400, 05004], lr: 0.002088, loss: 1.6225
2022-03-09 06:42:09 - train: epoch 0183, iter [03500, 05004], lr: 0.002088, loss: 1.9554
2022-03-09 06:42:43 - train: epoch 0183, iter [03600, 05004], lr: 0.002088, loss: 1.8971
2022-03-09 06:43:16 - train: epoch 0183, iter [03700, 05004], lr: 0.002088, loss: 2.1496
2022-03-09 06:43:49 - train: epoch 0183, iter [03800, 05004], lr: 0.002088, loss: 1.4108
2022-03-09 06:44:23 - train: epoch 0183, iter [03900, 05004], lr: 0.002088, loss: 1.9690
2022-03-09 06:44:57 - train: epoch 0183, iter [04000, 05004], lr: 0.002088, loss: 1.7931
2022-03-09 06:45:29 - train: epoch 0183, iter [04100, 05004], lr: 0.002088, loss: 2.0020
2022-03-09 06:46:03 - train: epoch 0183, iter [04200, 05004], lr: 0.002088, loss: 1.8592
2022-03-09 06:46:36 - train: epoch 0183, iter [04300, 05004], lr: 0.002088, loss: 1.6254
2022-03-09 06:47:10 - train: epoch 0183, iter [04400, 05004], lr: 0.002088, loss: 1.7611
2022-03-09 06:47:43 - train: epoch 0183, iter [04500, 05004], lr: 0.002088, loss: 1.9241
2022-03-09 06:48:16 - train: epoch 0183, iter [04600, 05004], lr: 0.002088, loss: 2.1403
2022-03-09 06:48:50 - train: epoch 0183, iter [04700, 05004], lr: 0.002088, loss: 1.9664
2022-03-09 06:49:23 - train: epoch 0183, iter [04800, 05004], lr: 0.002088, loss: 2.0118
2022-03-09 06:49:56 - train: epoch 0183, iter [04900, 05004], lr: 0.002088, loss: 1.9771
2022-03-09 06:50:28 - train: epoch 0183, iter [05000, 05004], lr: 0.002088, loss: 1.9902
2022-03-09 06:50:29 - train: epoch 183, train_loss: 1.8717
2022-03-09 06:51:42 - eval: epoch: 183, acc1: 75.900%, acc5: 92.816%, test_loss: 0.9565, per_image_load_time: 1.298ms, per_image_inference_time: 0.487ms
2022-03-09 06:51:42 - until epoch: 183, best_acc1: 75.934%
2022-03-09 06:51:42 - epoch 184 lr: 0.0018635966245104663
2022-03-09 06:52:20 - train: epoch 0184, iter [00100, 05004], lr: 0.001864, loss: 1.8604
2022-03-09 06:52:54 - train: epoch 0184, iter [00200, 05004], lr: 0.001864, loss: 1.7861
2022-03-09 06:53:28 - train: epoch 0184, iter [00300, 05004], lr: 0.001864, loss: 2.0576
2022-03-09 06:54:01 - train: epoch 0184, iter [00400, 05004], lr: 0.001864, loss: 1.8664
2022-03-09 06:54:35 - train: epoch 0184, iter [00500, 05004], lr: 0.001864, loss: 2.2090
2022-03-09 06:55:08 - train: epoch 0184, iter [00600, 05004], lr: 0.001864, loss: 1.8616
2022-03-09 06:55:41 - train: epoch 0184, iter [00700, 05004], lr: 0.001864, loss: 2.0502
2022-03-09 06:56:14 - train: epoch 0184, iter [00800, 05004], lr: 0.001864, loss: 1.7570
2022-03-09 06:56:48 - train: epoch 0184, iter [00900, 05004], lr: 0.001864, loss: 2.1980
2022-03-09 06:57:21 - train: epoch 0184, iter [01000, 05004], lr: 0.001864, loss: 1.4761
2022-03-09 06:57:54 - train: epoch 0184, iter [01100, 05004], lr: 0.001864, loss: 2.1708
2022-03-09 06:58:26 - train: epoch 0184, iter [01200, 05004], lr: 0.001864, loss: 1.5996
2022-03-09 06:58:59 - train: epoch 0184, iter [01300, 05004], lr: 0.001864, loss: 1.9938
2022-03-09 06:59:33 - train: epoch 0184, iter [01400, 05004], lr: 0.001864, loss: 1.8432
2022-03-09 07:00:07 - train: epoch 0184, iter [01500, 05004], lr: 0.001864, loss: 1.8695
2022-03-09 07:00:40 - train: epoch 0184, iter [01600, 05004], lr: 0.001864, loss: 1.9772
2022-03-09 07:01:14 - train: epoch 0184, iter [01700, 05004], lr: 0.001864, loss: 1.4019
2022-03-09 07:01:47 - train: epoch 0184, iter [01800, 05004], lr: 0.001864, loss: 2.0232
2022-03-09 07:02:21 - train: epoch 0184, iter [01900, 05004], lr: 0.001864, loss: 1.7785
2022-03-09 07:02:54 - train: epoch 0184, iter [02000, 05004], lr: 0.001864, loss: 1.9602
2022-03-09 07:03:27 - train: epoch 0184, iter [02100, 05004], lr: 0.001864, loss: 1.8215
2022-03-09 07:04:01 - train: epoch 0184, iter [02200, 05004], lr: 0.001864, loss: 2.3647
2022-03-09 07:04:35 - train: epoch 0184, iter [02300, 05004], lr: 0.001864, loss: 1.8900
2022-03-09 07:05:09 - train: epoch 0184, iter [02400, 05004], lr: 0.001864, loss: 1.7506
2022-03-09 07:05:41 - train: epoch 0184, iter [02500, 05004], lr: 0.001864, loss: 1.6733
2022-03-09 07:06:16 - train: epoch 0184, iter [02600, 05004], lr: 0.001864, loss: 1.9120
2022-03-09 07:06:49 - train: epoch 0184, iter [02700, 05004], lr: 0.001864, loss: 1.7524
2022-03-09 07:07:22 - train: epoch 0184, iter [02800, 05004], lr: 0.001864, loss: 1.8192
2022-03-09 07:07:55 - train: epoch 0184, iter [02900, 05004], lr: 0.001864, loss: 1.7684
2022-03-09 07:08:28 - train: epoch 0184, iter [03000, 05004], lr: 0.001864, loss: 1.6079
2022-03-09 07:09:01 - train: epoch 0184, iter [03100, 05004], lr: 0.001864, loss: 1.9525
2022-03-09 07:09:35 - train: epoch 0184, iter [03200, 05004], lr: 0.001864, loss: 2.0422
2022-03-09 07:10:08 - train: epoch 0184, iter [03300, 05004], lr: 0.001864, loss: 1.7886
2022-03-09 07:10:41 - train: epoch 0184, iter [03400, 05004], lr: 0.001864, loss: 1.6701
2022-03-09 07:11:15 - train: epoch 0184, iter [03500, 05004], lr: 0.001864, loss: 1.8281
2022-03-09 07:11:47 - train: epoch 0184, iter [03600, 05004], lr: 0.001864, loss: 1.7414
2022-03-09 07:12:21 - train: epoch 0184, iter [03700, 05004], lr: 0.001864, loss: 1.5987
2022-03-09 07:12:54 - train: epoch 0184, iter [03800, 05004], lr: 0.001864, loss: 1.9161
2022-03-09 07:13:27 - train: epoch 0184, iter [03900, 05004], lr: 0.001864, loss: 1.9309
2022-03-09 07:14:00 - train: epoch 0184, iter [04000, 05004], lr: 0.001864, loss: 2.0478
2022-03-09 07:14:34 - train: epoch 0184, iter [04100, 05004], lr: 0.001864, loss: 2.0247
2022-03-09 07:15:07 - train: epoch 0184, iter [04200, 05004], lr: 0.001864, loss: 1.9234
2022-03-09 07:15:40 - train: epoch 0184, iter [04300, 05004], lr: 0.001864, loss: 2.0478
2022-03-09 07:16:13 - train: epoch 0184, iter [04400, 05004], lr: 0.001864, loss: 1.7209
2022-03-09 07:16:46 - train: epoch 0184, iter [04500, 05004], lr: 0.001864, loss: 2.0860
2022-03-09 07:17:20 - train: epoch 0184, iter [04600, 05004], lr: 0.001864, loss: 1.6909
2022-03-09 07:17:54 - train: epoch 0184, iter [04700, 05004], lr: 0.001864, loss: 1.8678
2022-03-09 07:18:27 - train: epoch 0184, iter [04800, 05004], lr: 0.001864, loss: 1.7262
2022-03-09 07:19:00 - train: epoch 0184, iter [04900, 05004], lr: 0.001864, loss: 1.6881
2022-03-09 07:19:32 - train: epoch 0184, iter [05000, 05004], lr: 0.001864, loss: 1.7670
2022-03-09 07:19:33 - train: epoch 184, train_loss: 1.8547
2022-03-09 07:20:45 - eval: epoch: 184, acc1: 76.176%, acc5: 92.868%, test_loss: 0.9471, per_image_load_time: 2.208ms, per_image_inference_time: 0.505ms
2022-03-09 07:20:46 - until epoch: 184, best_acc1: 76.176%
2022-03-09 07:20:46 - epoch 185 lr: 0.0016519785107311892
2022-03-09 07:21:25 - train: epoch 0185, iter [00100, 05004], lr: 0.001652, loss: 1.7717
2022-03-09 07:21:58 - train: epoch 0185, iter [00200, 05004], lr: 0.001652, loss: 1.8272
2022-03-09 07:22:31 - train: epoch 0185, iter [00300, 05004], lr: 0.001652, loss: 1.8212
2022-03-09 07:23:05 - train: epoch 0185, iter [00400, 05004], lr: 0.001652, loss: 1.8498
2022-03-09 07:23:39 - train: epoch 0185, iter [00500, 05004], lr: 0.001652, loss: 1.6247
2022-03-09 07:24:12 - train: epoch 0185, iter [00600, 05004], lr: 0.001652, loss: 1.9878
2022-03-09 07:24:45 - train: epoch 0185, iter [00700, 05004], lr: 0.001652, loss: 1.7843
2022-03-09 07:25:19 - train: epoch 0185, iter [00800, 05004], lr: 0.001652, loss: 1.7997
2022-03-09 07:25:52 - train: epoch 0185, iter [00900, 05004], lr: 0.001652, loss: 1.7760
2022-03-09 07:26:26 - train: epoch 0185, iter [01000, 05004], lr: 0.001652, loss: 1.5201
2022-03-09 07:26:59 - train: epoch 0185, iter [01100, 05004], lr: 0.001652, loss: 1.8128
2022-03-09 07:27:33 - train: epoch 0185, iter [01200, 05004], lr: 0.001652, loss: 1.7664
2022-03-09 07:28:07 - train: epoch 0185, iter [01300, 05004], lr: 0.001652, loss: 1.6154
2022-03-09 07:28:39 - train: epoch 0185, iter [01400, 05004], lr: 0.001652, loss: 1.6972
2022-03-09 07:29:13 - train: epoch 0185, iter [01500, 05004], lr: 0.001652, loss: 2.1315
2022-03-09 07:29:45 - train: epoch 0185, iter [01600, 05004], lr: 0.001652, loss: 1.7298
2022-03-09 07:30:20 - train: epoch 0185, iter [01700, 05004], lr: 0.001652, loss: 1.6537
2022-03-09 07:30:53 - train: epoch 0185, iter [01800, 05004], lr: 0.001652, loss: 1.7423
2022-03-09 07:31:26 - train: epoch 0185, iter [01900, 05004], lr: 0.001652, loss: 1.6832
2022-03-09 07:32:00 - train: epoch 0185, iter [02000, 05004], lr: 0.001652, loss: 1.6302
2022-03-09 07:32:33 - train: epoch 0185, iter [02100, 05004], lr: 0.001652, loss: 1.8267
2022-03-09 07:33:06 - train: epoch 0185, iter [02200, 05004], lr: 0.001652, loss: 1.7994
2022-03-09 07:33:40 - train: epoch 0185, iter [02300, 05004], lr: 0.001652, loss: 1.4793
2022-03-09 07:34:14 - train: epoch 0185, iter [02400, 05004], lr: 0.001652, loss: 1.7680
2022-03-09 07:34:47 - train: epoch 0185, iter [02500, 05004], lr: 0.001652, loss: 1.6650
2022-03-09 07:35:20 - train: epoch 0185, iter [02600, 05004], lr: 0.001652, loss: 1.2951
2022-03-09 07:35:55 - train: epoch 0185, iter [02700, 05004], lr: 0.001652, loss: 1.9334
2022-03-09 07:36:28 - train: epoch 0185, iter [02800, 05004], lr: 0.001652, loss: 1.9380
2022-03-09 07:37:02 - train: epoch 0185, iter [02900, 05004], lr: 0.001652, loss: 1.7158
2022-03-09 07:37:35 - train: epoch 0185, iter [03000, 05004], lr: 0.001652, loss: 1.6772
2022-03-09 07:38:09 - train: epoch 0185, iter [03100, 05004], lr: 0.001652, loss: 1.7548
2022-03-09 07:38:43 - train: epoch 0185, iter [03200, 05004], lr: 0.001652, loss: 1.8451
2022-03-09 07:39:16 - train: epoch 0185, iter [03300, 05004], lr: 0.001652, loss: 1.4888
2022-03-09 07:39:49 - train: epoch 0185, iter [03400, 05004], lr: 0.001652, loss: 1.8751
2022-03-09 07:40:23 - train: epoch 0185, iter [03500, 05004], lr: 0.001652, loss: 1.9927
2022-03-09 07:40:57 - train: epoch 0185, iter [03600, 05004], lr: 0.001652, loss: 2.1077
2022-03-09 07:41:31 - train: epoch 0185, iter [03700, 05004], lr: 0.001652, loss: 1.8752
2022-03-09 07:42:04 - train: epoch 0185, iter [03800, 05004], lr: 0.001652, loss: 1.6595
2022-03-09 07:42:37 - train: epoch 0185, iter [03900, 05004], lr: 0.001652, loss: 1.7732
2022-03-09 07:43:10 - train: epoch 0185, iter [04000, 05004], lr: 0.001652, loss: 1.9244
2022-03-09 07:43:44 - train: epoch 0185, iter [04100, 05004], lr: 0.001652, loss: 1.7884
2022-03-09 07:44:17 - train: epoch 0185, iter [04200, 05004], lr: 0.001652, loss: 1.8253
2022-03-09 07:44:50 - train: epoch 0185, iter [04300, 05004], lr: 0.001652, loss: 1.7299
2022-03-09 07:45:23 - train: epoch 0185, iter [04400, 05004], lr: 0.001652, loss: 1.7872
2022-03-09 07:45:57 - train: epoch 0185, iter [04500, 05004], lr: 0.001652, loss: 1.9837
2022-03-09 07:46:30 - train: epoch 0185, iter [04600, 05004], lr: 0.001652, loss: 1.6360
2022-03-09 07:47:05 - train: epoch 0185, iter [04700, 05004], lr: 0.001652, loss: 1.9194
2022-03-09 07:47:37 - train: epoch 0185, iter [04800, 05004], lr: 0.001652, loss: 1.8477
2022-03-09 07:48:10 - train: epoch 0185, iter [04900, 05004], lr: 0.001652, loss: 1.6817
2022-03-09 07:48:43 - train: epoch 0185, iter [05000, 05004], lr: 0.001652, loss: 1.8062
2022-03-09 07:48:44 - train: epoch 185, train_loss: 1.8364
2022-03-09 07:49:57 - eval: epoch: 185, acc1: 76.206%, acc5: 92.936%, test_loss: 0.9356, per_image_load_time: 2.123ms, per_image_inference_time: 0.495ms
2022-03-09 07:49:58 - until epoch: 185, best_acc1: 76.206%
2022-03-09 22:25:59 - epoch 186 lr: 0.0014529091286973996
2022-03-09 22:26:38 - train: epoch 0186, iter [00100, 05004], lr: 0.001453, loss: 1.8080
2022-03-09 22:27:10 - train: epoch 0186, iter [00200, 05004], lr: 0.001453, loss: 1.6804
2022-03-09 22:27:43 - train: epoch 0186, iter [00300, 05004], lr: 0.001453, loss: 1.6588
2022-03-09 22:28:15 - train: epoch 0186, iter [00400, 05004], lr: 0.001453, loss: 1.6423
2022-03-09 22:28:49 - train: epoch 0186, iter [00500, 05004], lr: 0.001453, loss: 1.8225
2022-03-09 22:29:21 - train: epoch 0186, iter [00600, 05004], lr: 0.001453, loss: 1.6717
2022-03-09 22:29:55 - train: epoch 0186, iter [00700, 05004], lr: 0.001453, loss: 1.5513
2022-03-09 22:30:27 - train: epoch 0186, iter [00800, 05004], lr: 0.001453, loss: 1.6841
2022-03-09 22:30:59 - train: epoch 0186, iter [00900, 05004], lr: 0.001453, loss: 1.8977
2022-03-09 22:31:32 - train: epoch 0186, iter [01000, 05004], lr: 0.001453, loss: 1.7202
2022-03-09 22:32:05 - train: epoch 0186, iter [01100, 05004], lr: 0.001453, loss: 1.9296
2022-03-09 22:32:38 - train: epoch 0186, iter [01200, 05004], lr: 0.001453, loss: 1.8936
2022-03-09 22:33:11 - train: epoch 0186, iter [01300, 05004], lr: 0.001453, loss: 1.7466
2022-03-09 22:33:44 - train: epoch 0186, iter [01400, 05004], lr: 0.001453, loss: 1.9378
2022-03-09 22:34:16 - train: epoch 0186, iter [01500, 05004], lr: 0.001453, loss: 1.8242
2022-03-09 22:34:49 - train: epoch 0186, iter [01600, 05004], lr: 0.001453, loss: 1.5402
2022-03-09 22:35:21 - train: epoch 0186, iter [01700, 05004], lr: 0.001453, loss: 1.7006
2022-03-09 22:35:54 - train: epoch 0186, iter [01800, 05004], lr: 0.001453, loss: 1.7852
2022-03-09 22:36:27 - train: epoch 0186, iter [01900, 05004], lr: 0.001453, loss: 1.7834
2022-03-09 22:37:01 - train: epoch 0186, iter [02000, 05004], lr: 0.001453, loss: 1.7131
2022-03-09 22:37:32 - train: epoch 0186, iter [02100, 05004], lr: 0.001453, loss: 1.9363
2022-03-09 22:38:05 - train: epoch 0186, iter [02200, 05004], lr: 0.001453, loss: 1.6754
2022-03-09 22:38:38 - train: epoch 0186, iter [02300, 05004], lr: 0.001453, loss: 1.9807
2022-03-09 22:39:10 - train: epoch 0186, iter [02400, 05004], lr: 0.001453, loss: 1.8147
2022-03-09 22:39:43 - train: epoch 0186, iter [02500, 05004], lr: 0.001453, loss: 1.8699
2022-03-09 22:40:16 - train: epoch 0186, iter [02600, 05004], lr: 0.001453, loss: 1.8337
2022-03-09 22:40:49 - train: epoch 0186, iter [02700, 05004], lr: 0.001453, loss: 1.9636
2022-03-09 22:41:21 - train: epoch 0186, iter [02800, 05004], lr: 0.001453, loss: 1.6673
2022-03-09 22:41:54 - train: epoch 0186, iter [02900, 05004], lr: 0.001453, loss: 1.9074
2022-03-09 22:42:27 - train: epoch 0186, iter [03000, 05004], lr: 0.001453, loss: 1.7044
2022-03-09 22:43:00 - train: epoch 0186, iter [03100, 05004], lr: 0.001453, loss: 1.7799
2022-03-09 22:43:32 - train: epoch 0186, iter [03200, 05004], lr: 0.001453, loss: 1.8119
2022-03-09 22:44:05 - train: epoch 0186, iter [03300, 05004], lr: 0.001453, loss: 1.8112
2022-03-09 22:44:37 - train: epoch 0186, iter [03400, 05004], lr: 0.001453, loss: 1.7859
2022-03-09 22:45:09 - train: epoch 0186, iter [03500, 05004], lr: 0.001453, loss: 1.6670
2022-03-09 22:45:43 - train: epoch 0186, iter [03600, 05004], lr: 0.001453, loss: 1.8473
2022-03-09 22:46:15 - train: epoch 0186, iter [03700, 05004], lr: 0.001453, loss: 1.9155
2022-03-09 22:46:47 - train: epoch 0186, iter [03800, 05004], lr: 0.001453, loss: 1.5748
2022-03-09 22:47:20 - train: epoch 0186, iter [03900, 05004], lr: 0.001453, loss: 1.8631
2022-03-09 22:47:53 - train: epoch 0186, iter [04000, 05004], lr: 0.001453, loss: 1.5261
2022-03-09 22:48:27 - train: epoch 0186, iter [04100, 05004], lr: 0.001453, loss: 1.8272
2022-03-09 22:48:58 - train: epoch 0186, iter [04200, 05004], lr: 0.001453, loss: 2.0129
2022-03-09 22:49:31 - train: epoch 0186, iter [04300, 05004], lr: 0.001453, loss: 1.8147
2022-03-09 22:50:04 - train: epoch 0186, iter [04400, 05004], lr: 0.001453, loss: 1.8092
2022-03-09 22:50:37 - train: epoch 0186, iter [04500, 05004], lr: 0.001453, loss: 1.6936
2022-03-09 22:51:09 - train: epoch 0186, iter [04600, 05004], lr: 0.001453, loss: 1.8386
2022-03-09 22:51:42 - train: epoch 0186, iter [04700, 05004], lr: 0.001453, loss: 1.8141
2022-03-09 22:52:15 - train: epoch 0186, iter [04800, 05004], lr: 0.001453, loss: 1.9359
2022-03-09 22:52:47 - train: epoch 0186, iter [04900, 05004], lr: 0.001453, loss: 1.6724
2022-03-09 22:53:18 - train: epoch 0186, iter [05000, 05004], lr: 0.001453, loss: 2.1177
2022-03-09 22:53:20 - train: epoch 186, train_loss: 1.8258
2022-03-09 22:54:31 - eval: epoch: 186, acc1: 76.432%, acc5: 93.172%, test_loss: 0.9267, per_image_load_time: 2.295ms, per_image_inference_time: 0.412ms
2022-03-09 22:54:32 - until epoch: 186, best_acc1: 76.432%
2022-03-09 22:54:32 - epoch 187 lr: 0.0012664401468786115
2022-03-09 22:55:09 - train: epoch 0187, iter [00100, 05004], lr: 0.001266, loss: 1.7840
2022-03-09 22:55:41 - train: epoch 0187, iter [00200, 05004], lr: 0.001266, loss: 1.8055
2022-03-09 22:56:15 - train: epoch 0187, iter [00300, 05004], lr: 0.001266, loss: 1.9196
2022-03-09 22:56:47 - train: epoch 0187, iter [00400, 05004], lr: 0.001266, loss: 1.9054
2022-03-09 22:57:20 - train: epoch 0187, iter [00500, 05004], lr: 0.001266, loss: 1.7714
2022-03-09 22:57:52 - train: epoch 0187, iter [00600, 05004], lr: 0.001266, loss: 2.0123
2022-03-09 22:58:26 - train: epoch 0187, iter [00700, 05004], lr: 0.001266, loss: 1.7835
2022-03-09 22:58:58 - train: epoch 0187, iter [00800, 05004], lr: 0.001266, loss: 1.5698
2022-03-09 22:59:31 - train: epoch 0187, iter [00900, 05004], lr: 0.001266, loss: 1.6827
2022-03-09 23:00:04 - train: epoch 0187, iter [01000, 05004], lr: 0.001266, loss: 1.9489
2022-03-09 23:00:37 - train: epoch 0187, iter [01100, 05004], lr: 0.001266, loss: 2.0368
2022-03-09 23:01:10 - train: epoch 0187, iter [01200, 05004], lr: 0.001266, loss: 1.5705
2022-03-09 23:01:43 - train: epoch 0187, iter [01300, 05004], lr: 0.001266, loss: 1.7348
2022-03-09 23:02:15 - train: epoch 0187, iter [01400, 05004], lr: 0.001266, loss: 1.8268
2022-03-09 23:02:47 - train: epoch 0187, iter [01500, 05004], lr: 0.001266, loss: 2.0587
2022-03-09 23:03:20 - train: epoch 0187, iter [01600, 05004], lr: 0.001266, loss: 1.9325
2022-03-09 23:03:53 - train: epoch 0187, iter [01700, 05004], lr: 0.001266, loss: 2.2213
2022-03-09 23:04:26 - train: epoch 0187, iter [01800, 05004], lr: 0.001266, loss: 1.9092
2022-03-09 23:04:58 - train: epoch 0187, iter [01900, 05004], lr: 0.001266, loss: 1.7104
2022-03-09 23:05:32 - train: epoch 0187, iter [02000, 05004], lr: 0.001266, loss: 1.7787
2022-03-09 23:06:04 - train: epoch 0187, iter [02100, 05004], lr: 0.001266, loss: 1.8951
2022-03-09 23:06:37 - train: epoch 0187, iter [02200, 05004], lr: 0.001266, loss: 1.3911
2022-03-09 23:07:10 - train: epoch 0187, iter [02300, 05004], lr: 0.001266, loss: 1.7091
2022-03-09 23:07:43 - train: epoch 0187, iter [02400, 05004], lr: 0.001266, loss: 1.9114
2022-03-09 23:08:15 - train: epoch 0187, iter [02500, 05004], lr: 0.001266, loss: 1.4256
2022-03-09 23:08:47 - train: epoch 0187, iter [02600, 05004], lr: 0.001266, loss: 1.8101
2022-03-09 23:09:20 - train: epoch 0187, iter [02700, 05004], lr: 0.001266, loss: 1.8880
2022-03-09 23:09:53 - train: epoch 0187, iter [02800, 05004], lr: 0.001266, loss: 1.8272
2022-03-09 23:10:25 - train: epoch 0187, iter [02900, 05004], lr: 0.001266, loss: 1.8141
2022-03-09 23:10:58 - train: epoch 0187, iter [03000, 05004], lr: 0.001266, loss: 1.6945
2022-03-09 23:11:31 - train: epoch 0187, iter [03100, 05004], lr: 0.001266, loss: 1.9149
2022-03-09 23:12:03 - train: epoch 0187, iter [03200, 05004], lr: 0.001266, loss: 2.0211
2022-03-09 23:12:36 - train: epoch 0187, iter [03300, 05004], lr: 0.001266, loss: 1.9587
2022-03-09 23:13:09 - train: epoch 0187, iter [03400, 05004], lr: 0.001266, loss: 1.5860
2022-03-09 23:13:42 - train: epoch 0187, iter [03500, 05004], lr: 0.001266, loss: 1.9213
2022-03-09 23:14:14 - train: epoch 0187, iter [03600, 05004], lr: 0.001266, loss: 2.0427
2022-03-09 23:14:47 - train: epoch 0187, iter [03700, 05004], lr: 0.001266, loss: 1.8934
2022-03-09 23:15:20 - train: epoch 0187, iter [03800, 05004], lr: 0.001266, loss: 1.7392
2022-03-09 23:15:53 - train: epoch 0187, iter [03900, 05004], lr: 0.001266, loss: 1.7424
2022-03-09 23:16:25 - train: epoch 0187, iter [04000, 05004], lr: 0.001266, loss: 1.8110
2022-03-09 23:16:58 - train: epoch 0187, iter [04100, 05004], lr: 0.001266, loss: 1.8363
2022-03-09 23:17:31 - train: epoch 0187, iter [04200, 05004], lr: 0.001266, loss: 1.6865
2022-03-09 23:18:04 - train: epoch 0187, iter [04300, 05004], lr: 0.001266, loss: 1.8568
2022-03-09 23:18:36 - train: epoch 0187, iter [04400, 05004], lr: 0.001266, loss: 1.8567
2022-03-09 23:19:09 - train: epoch 0187, iter [04500, 05004], lr: 0.001266, loss: 1.8540
2022-03-09 23:19:42 - train: epoch 0187, iter [04600, 05004], lr: 0.001266, loss: 1.4592
2022-03-09 23:20:14 - train: epoch 0187, iter [04700, 05004], lr: 0.001266, loss: 1.8555
2022-03-09 23:20:47 - train: epoch 0187, iter [04800, 05004], lr: 0.001266, loss: 1.6447
2022-03-09 23:21:20 - train: epoch 0187, iter [04900, 05004], lr: 0.001266, loss: 1.8369
2022-03-09 23:21:52 - train: epoch 0187, iter [05000, 05004], lr: 0.001266, loss: 1.5411
2022-03-09 23:21:53 - train: epoch 187, train_loss: 1.8172
2022-03-09 23:23:05 - eval: epoch: 187, acc1: 76.770%, acc5: 93.282%, test_loss: 0.9226, per_image_load_time: 1.845ms, per_image_inference_time: 0.464ms
2022-03-09 23:23:06 - until epoch: 187, best_acc1: 76.770%
2022-03-09 23:23:06 - epoch 188 lr: 0.0010926199633097156
2022-03-09 23:23:44 - train: epoch 0188, iter [00100, 05004], lr: 0.001093, loss: 1.6543
2022-03-09 23:24:16 - train: epoch 0188, iter [00200, 05004], lr: 0.001093, loss: 1.7985
2022-03-09 23:24:49 - train: epoch 0188, iter [00300, 05004], lr: 0.001093, loss: 1.4202
2022-03-09 23:25:22 - train: epoch 0188, iter [00400, 05004], lr: 0.001093, loss: 1.6105
2022-03-09 23:25:55 - train: epoch 0188, iter [00500, 05004], lr: 0.001093, loss: 1.8413
2022-03-09 23:26:28 - train: epoch 0188, iter [00600, 05004], lr: 0.001093, loss: 1.7744
2022-03-09 23:27:00 - train: epoch 0188, iter [00700, 05004], lr: 0.001093, loss: 1.7988
2022-03-09 23:27:33 - train: epoch 0188, iter [00800, 05004], lr: 0.001093, loss: 1.9265
2022-03-09 23:28:06 - train: epoch 0188, iter [00900, 05004], lr: 0.001093, loss: 1.9800
2022-03-09 23:28:38 - train: epoch 0188, iter [01000, 05004], lr: 0.001093, loss: 1.9489
2022-03-09 23:29:12 - train: epoch 0188, iter [01100, 05004], lr: 0.001093, loss: 1.6515
2022-03-09 23:29:45 - train: epoch 0188, iter [01200, 05004], lr: 0.001093, loss: 1.7125
2022-03-09 23:30:18 - train: epoch 0188, iter [01300, 05004], lr: 0.001093, loss: 1.6796
2022-03-09 23:30:51 - train: epoch 0188, iter [01400, 05004], lr: 0.001093, loss: 1.8045
2022-03-09 23:31:24 - train: epoch 0188, iter [01500, 05004], lr: 0.001093, loss: 2.0403
2022-03-09 23:31:56 - train: epoch 0188, iter [01600, 05004], lr: 0.001093, loss: 1.6834
2022-03-09 23:32:30 - train: epoch 0188, iter [01700, 05004], lr: 0.001093, loss: 2.0011
2022-03-09 23:33:03 - train: epoch 0188, iter [01800, 05004], lr: 0.001093, loss: 1.7460
2022-03-09 23:33:36 - train: epoch 0188, iter [01900, 05004], lr: 0.001093, loss: 1.9211
2022-03-09 23:34:09 - train: epoch 0188, iter [02000, 05004], lr: 0.001093, loss: 2.0479
2022-03-09 23:34:42 - train: epoch 0188, iter [02100, 05004], lr: 0.001093, loss: 1.5499
2022-03-09 23:35:16 - train: epoch 0188, iter [02200, 05004], lr: 0.001093, loss: 1.8124
2022-03-09 23:35:49 - train: epoch 0188, iter [02300, 05004], lr: 0.001093, loss: 1.5342
2022-03-09 23:36:22 - train: epoch 0188, iter [02400, 05004], lr: 0.001093, loss: 1.7421
2022-03-09 23:36:56 - train: epoch 0188, iter [02500, 05004], lr: 0.001093, loss: 1.7641
2022-03-09 23:37:29 - train: epoch 0188, iter [02600, 05004], lr: 0.001093, loss: 2.0969
2022-03-09 23:38:03 - train: epoch 0188, iter [02700, 05004], lr: 0.001093, loss: 2.0448
2022-03-09 23:38:38 - train: epoch 0188, iter [02800, 05004], lr: 0.001093, loss: 1.6834
2022-03-09 23:39:11 - train: epoch 0188, iter [02900, 05004], lr: 0.001093, loss: 1.7330
2022-03-09 23:39:45 - train: epoch 0188, iter [03000, 05004], lr: 0.001093, loss: 2.1647
2022-03-09 23:40:19 - train: epoch 0188, iter [03100, 05004], lr: 0.001093, loss: 2.0371
2022-03-09 23:40:52 - train: epoch 0188, iter [03200, 05004], lr: 0.001093, loss: 1.7497
2022-03-09 23:41:26 - train: epoch 0188, iter [03300, 05004], lr: 0.001093, loss: 1.8823
2022-03-09 23:42:01 - train: epoch 0188, iter [03400, 05004], lr: 0.001093, loss: 1.8861
2022-03-09 23:42:34 - train: epoch 0188, iter [03500, 05004], lr: 0.001093, loss: 1.5678
2022-03-09 23:43:08 - train: epoch 0188, iter [03600, 05004], lr: 0.001093, loss: 1.6559
2022-03-09 23:43:43 - train: epoch 0188, iter [03700, 05004], lr: 0.001093, loss: 1.8955
2022-03-09 23:44:16 - train: epoch 0188, iter [03800, 05004], lr: 0.001093, loss: 1.9985
2022-03-09 23:44:51 - train: epoch 0188, iter [03900, 05004], lr: 0.001093, loss: 1.6790
2022-03-09 23:45:25 - train: epoch 0188, iter [04000, 05004], lr: 0.001093, loss: 1.5279
2022-03-09 23:45:59 - train: epoch 0188, iter [04100, 05004], lr: 0.001093, loss: 1.6773
2022-03-09 23:46:33 - train: epoch 0188, iter [04200, 05004], lr: 0.001093, loss: 1.9513
2022-03-09 23:47:07 - train: epoch 0188, iter [04300, 05004], lr: 0.001093, loss: 2.1287
2022-03-09 23:47:40 - train: epoch 0188, iter [04400, 05004], lr: 0.001093, loss: 2.1582
2022-03-09 23:48:15 - train: epoch 0188, iter [04500, 05004], lr: 0.001093, loss: 1.5396
2022-03-09 23:48:49 - train: epoch 0188, iter [04600, 05004], lr: 0.001093, loss: 1.8043
2022-03-09 23:49:23 - train: epoch 0188, iter [04700, 05004], lr: 0.001093, loss: 1.7934
2022-03-09 23:49:56 - train: epoch 0188, iter [04800, 05004], lr: 0.001093, loss: 1.8779
2022-03-09 23:50:31 - train: epoch 0188, iter [04900, 05004], lr: 0.001093, loss: 1.9151
2022-03-09 23:51:03 - train: epoch 0188, iter [05000, 05004], lr: 0.001093, loss: 1.9891
2022-03-09 23:51:04 - train: epoch 188, train_loss: 1.8025
2022-03-09 23:52:18 - eval: epoch: 188, acc1: 76.870%, acc5: 93.338%, test_loss: 0.9166, per_image_load_time: 1.804ms, per_image_inference_time: 0.495ms
2022-03-09 23:52:19 - until epoch: 188, best_acc1: 76.870%
2022-03-09 23:52:19 - epoch 189 lr: 0.0009314936930293283
2022-03-09 23:52:57 - train: epoch 0189, iter [00100, 05004], lr: 0.000931, loss: 1.7340
2022-03-09 23:53:31 - train: epoch 0189, iter [00200, 05004], lr: 0.000931, loss: 1.6526
2022-03-09 23:54:05 - train: epoch 0189, iter [00300, 05004], lr: 0.000931, loss: 1.9522
2022-03-09 23:54:40 - train: epoch 0189, iter [00400, 05004], lr: 0.000931, loss: 1.7025
2022-03-09 23:55:13 - train: epoch 0189, iter [00500, 05004], lr: 0.000931, loss: 1.8389
2022-03-09 23:55:47 - train: epoch 0189, iter [00600, 05004], lr: 0.000931, loss: 1.5953
2022-03-09 23:56:21 - train: epoch 0189, iter [00700, 05004], lr: 0.000931, loss: 1.9364
2022-03-09 23:56:55 - train: epoch 0189, iter [00800, 05004], lr: 0.000931, loss: 1.8840
2022-03-09 23:57:29 - train: epoch 0189, iter [00900, 05004], lr: 0.000931, loss: 1.7621
2022-03-09 23:58:03 - train: epoch 0189, iter [01000, 05004], lr: 0.000931, loss: 1.7471
2022-03-09 23:58:37 - train: epoch 0189, iter [01100, 05004], lr: 0.000931, loss: 1.6528
2022-03-09 23:59:11 - train: epoch 0189, iter [01200, 05004], lr: 0.000931, loss: 1.6689
2022-03-09 23:59:45 - train: epoch 0189, iter [01300, 05004], lr: 0.000931, loss: 1.7796
2022-03-10 00:00:20 - train: epoch 0189, iter [01400, 05004], lr: 0.000931, loss: 1.9523
2022-03-10 00:00:53 - train: epoch 0189, iter [01500, 05004], lr: 0.000931, loss: 1.7255
2022-03-10 00:01:28 - train: epoch 0189, iter [01600, 05004], lr: 0.000931, loss: 1.6709
2022-03-10 00:02:02 - train: epoch 0189, iter [01700, 05004], lr: 0.000931, loss: 1.8701
2022-03-10 00:02:36 - train: epoch 0189, iter [01800, 05004], lr: 0.000931, loss: 2.0620
2022-03-10 00:03:11 - train: epoch 0189, iter [01900, 05004], lr: 0.000931, loss: 1.7069
2022-03-10 00:03:46 - train: epoch 0189, iter [02000, 05004], lr: 0.000931, loss: 1.8545
2022-03-10 00:04:19 - train: epoch 0189, iter [02100, 05004], lr: 0.000931, loss: 2.0271
2022-03-10 00:04:53 - train: epoch 0189, iter [02200, 05004], lr: 0.000931, loss: 1.9750
2022-03-10 00:05:28 - train: epoch 0189, iter [02300, 05004], lr: 0.000931, loss: 1.8413
2022-03-10 00:06:01 - train: epoch 0189, iter [02400, 05004], lr: 0.000931, loss: 1.7616
2022-03-10 00:06:36 - train: epoch 0189, iter [02500, 05004], lr: 0.000931, loss: 1.8972
2022-03-10 00:07:10 - train: epoch 0189, iter [02600, 05004], lr: 0.000931, loss: 1.9427
2022-03-10 00:07:45 - train: epoch 0189, iter [02700, 05004], lr: 0.000931, loss: 1.8359
2022-03-10 00:08:19 - train: epoch 0189, iter [02800, 05004], lr: 0.000931, loss: 1.4777
2022-03-10 00:08:52 - train: epoch 0189, iter [02900, 05004], lr: 0.000931, loss: 1.8822
2022-03-10 00:09:27 - train: epoch 0189, iter [03000, 05004], lr: 0.000931, loss: 1.8082
2022-03-10 00:10:02 - train: epoch 0189, iter [03100, 05004], lr: 0.000931, loss: 1.6511
2022-03-10 00:10:37 - train: epoch 0189, iter [03200, 05004], lr: 0.000931, loss: 1.7812
2022-03-10 00:11:11 - train: epoch 0189, iter [03300, 05004], lr: 0.000931, loss: 1.8575
2022-03-10 00:11:45 - train: epoch 0189, iter [03400, 05004], lr: 0.000931, loss: 1.9507
2022-03-10 00:12:19 - train: epoch 0189, iter [03500, 05004], lr: 0.000931, loss: 1.9449
2022-03-10 00:12:53 - train: epoch 0189, iter [03600, 05004], lr: 0.000931, loss: 1.5949
2022-03-10 00:13:28 - train: epoch 0189, iter [03700, 05004], lr: 0.000931, loss: 1.8056
2022-03-10 00:14:03 - train: epoch 0189, iter [03800, 05004], lr: 0.000931, loss: 1.5234
2022-03-10 00:14:37 - train: epoch 0189, iter [03900, 05004], lr: 0.000931, loss: 1.8890
2022-03-10 00:15:11 - train: epoch 0189, iter [04000, 05004], lr: 0.000931, loss: 1.7762
2022-03-10 00:15:46 - train: epoch 0189, iter [04100, 05004], lr: 0.000931, loss: 2.0734
2022-03-10 00:16:21 - train: epoch 0189, iter [04200, 05004], lr: 0.000931, loss: 1.7209
2022-03-10 00:16:55 - train: epoch 0189, iter [04300, 05004], lr: 0.000931, loss: 1.6974
2022-03-10 00:17:29 - train: epoch 0189, iter [04400, 05004], lr: 0.000931, loss: 1.9139
2022-03-10 00:18:04 - train: epoch 0189, iter [04500, 05004], lr: 0.000931, loss: 1.6605
2022-03-10 00:18:38 - train: epoch 0189, iter [04600, 05004], lr: 0.000931, loss: 1.7152
2022-03-10 00:19:13 - train: epoch 0189, iter [04700, 05004], lr: 0.000931, loss: 1.8412
2022-03-10 00:19:47 - train: epoch 0189, iter [04800, 05004], lr: 0.000931, loss: 1.7949
2022-03-10 00:20:22 - train: epoch 0189, iter [04900, 05004], lr: 0.000931, loss: 2.1077
2022-03-10 00:20:55 - train: epoch 0189, iter [05000, 05004], lr: 0.000931, loss: 1.9173
2022-03-10 00:20:56 - train: epoch 189, train_loss: 1.7904
2022-03-10 00:22:10 - eval: epoch: 189, acc1: 76.998%, acc5: 93.288%, test_loss: 0.9114, per_image_load_time: 1.914ms, per_image_inference_time: 0.563ms
2022-03-10 00:22:11 - until epoch: 189, best_acc1: 76.998%
2022-03-10 00:22:11 - epoch 190 lr: 0.000783103156370113
2022-03-10 00:22:50 - train: epoch 0190, iter [00100, 05004], lr: 0.000783, loss: 1.6520
2022-03-10 00:23:25 - train: epoch 0190, iter [00200, 05004], lr: 0.000783, loss: 1.8062
2022-03-10 00:23:58 - train: epoch 0190, iter [00300, 05004], lr: 0.000783, loss: 1.6448
2022-03-10 00:24:32 - train: epoch 0190, iter [00400, 05004], lr: 0.000783, loss: 2.0620
2022-03-10 00:25:07 - train: epoch 0190, iter [00500, 05004], lr: 0.000783, loss: 1.9446
2022-03-10 00:25:41 - train: epoch 0190, iter [00600, 05004], lr: 0.000783, loss: 1.8192
2022-03-10 00:26:15 - train: epoch 0190, iter [00700, 05004], lr: 0.000783, loss: 1.6324
2022-03-10 00:26:50 - train: epoch 0190, iter [00800, 05004], lr: 0.000783, loss: 2.0377
2022-03-10 00:27:24 - train: epoch 0190, iter [00900, 05004], lr: 0.000783, loss: 1.5490
2022-03-10 00:27:58 - train: epoch 0190, iter [01000, 05004], lr: 0.000783, loss: 1.7869
2022-03-10 00:28:32 - train: epoch 0190, iter [01100, 05004], lr: 0.000783, loss: 1.6598
2022-03-10 00:29:06 - train: epoch 0190, iter [01200, 05004], lr: 0.000783, loss: 1.6486
2022-03-10 00:29:40 - train: epoch 0190, iter [01300, 05004], lr: 0.000783, loss: 1.8766
2022-03-10 00:30:14 - train: epoch 0190, iter [01400, 05004], lr: 0.000783, loss: 1.7897
2022-03-10 00:30:48 - train: epoch 0190, iter [01500, 05004], lr: 0.000783, loss: 1.7090
2022-03-10 00:31:23 - train: epoch 0190, iter [01600, 05004], lr: 0.000783, loss: 1.8297
2022-03-10 00:31:56 - train: epoch 0190, iter [01700, 05004], lr: 0.000783, loss: 1.5128
2022-03-10 00:32:30 - train: epoch 0190, iter [01800, 05004], lr: 0.000783, loss: 1.8576
2022-03-10 00:33:04 - train: epoch 0190, iter [01900, 05004], lr: 0.000783, loss: 2.0469
2022-03-10 00:33:39 - train: epoch 0190, iter [02000, 05004], lr: 0.000783, loss: 1.7141
2022-03-10 00:34:12 - train: epoch 0190, iter [02100, 05004], lr: 0.000783, loss: 1.8778
2022-03-10 00:34:47 - train: epoch 0190, iter [02200, 05004], lr: 0.000783, loss: 1.9186
2022-03-10 00:35:21 - train: epoch 0190, iter [02300, 05004], lr: 0.000783, loss: 1.7553
2022-03-10 00:35:55 - train: epoch 0190, iter [02400, 05004], lr: 0.000783, loss: 1.6718
2022-03-10 00:36:29 - train: epoch 0190, iter [02500, 05004], lr: 0.000783, loss: 1.4871
2022-03-10 00:37:03 - train: epoch 0190, iter [02600, 05004], lr: 0.000783, loss: 1.7430
2022-03-10 00:37:38 - train: epoch 0190, iter [02700, 05004], lr: 0.000783, loss: 1.7533
2022-03-10 00:38:11 - train: epoch 0190, iter [02800, 05004], lr: 0.000783, loss: 1.6829
2022-03-10 00:38:45 - train: epoch 0190, iter [02900, 05004], lr: 0.000783, loss: 1.9099
2022-03-10 00:39:20 - train: epoch 0190, iter [03000, 05004], lr: 0.000783, loss: 1.7397
2022-03-10 00:39:54 - train: epoch 0190, iter [03100, 05004], lr: 0.000783, loss: 1.7189
2022-03-10 00:40:28 - train: epoch 0190, iter [03200, 05004], lr: 0.000783, loss: 2.1278
2022-03-10 00:41:02 - train: epoch 0190, iter [03300, 05004], lr: 0.000783, loss: 1.7963
2022-03-10 00:41:36 - train: epoch 0190, iter [03400, 05004], lr: 0.000783, loss: 2.0444
2022-03-10 00:42:10 - train: epoch 0190, iter [03500, 05004], lr: 0.000783, loss: 1.7822
2022-03-10 00:42:44 - train: epoch 0190, iter [03600, 05004], lr: 0.000783, loss: 1.7555
2022-03-10 00:43:18 - train: epoch 0190, iter [03700, 05004], lr: 0.000783, loss: 1.7031
2022-03-10 00:43:53 - train: epoch 0190, iter [03800, 05004], lr: 0.000783, loss: 1.8170
2022-03-10 00:44:27 - train: epoch 0190, iter [03900, 05004], lr: 0.000783, loss: 1.9969
2022-03-10 00:45:01 - train: epoch 0190, iter [04000, 05004], lr: 0.000783, loss: 1.9276
2022-03-10 00:45:36 - train: epoch 0190, iter [04100, 05004], lr: 0.000783, loss: 1.7318
2022-03-10 00:46:09 - train: epoch 0190, iter [04200, 05004], lr: 0.000783, loss: 1.7445
2022-03-10 00:46:43 - train: epoch 0190, iter [04300, 05004], lr: 0.000783, loss: 1.7750
2022-03-10 00:47:17 - train: epoch 0190, iter [04400, 05004], lr: 0.000783, loss: 1.5598
2022-03-10 00:47:52 - train: epoch 0190, iter [04500, 05004], lr: 0.000783, loss: 1.7018
2022-03-10 00:48:27 - train: epoch 0190, iter [04600, 05004], lr: 0.000783, loss: 1.7342
2022-03-10 00:49:01 - train: epoch 0190, iter [04700, 05004], lr: 0.000783, loss: 1.8692
2022-03-10 00:49:35 - train: epoch 0190, iter [04800, 05004], lr: 0.000783, loss: 1.8736
2022-03-10 00:50:10 - train: epoch 0190, iter [04900, 05004], lr: 0.000783, loss: 1.7057
2022-03-10 00:50:43 - train: epoch 0190, iter [05000, 05004], lr: 0.000783, loss: 1.8265
2022-03-10 00:50:44 - train: epoch 190, train_loss: 1.7755
2022-03-10 00:51:59 - eval: epoch: 190, acc1: 77.024%, acc5: 93.358%, test_loss: 0.9078, per_image_load_time: 2.179ms, per_image_inference_time: 0.548ms
2022-03-10 00:52:00 - until epoch: 190, best_acc1: 77.024%
2022-03-10 00:52:00 - epoch 191 lr: 0.0006474868681043578
2022-03-10 00:52:39 - train: epoch 0191, iter [00100, 05004], lr: 0.000647, loss: 1.8326
2022-03-10 00:53:13 - train: epoch 0191, iter [00200, 05004], lr: 0.000647, loss: 1.6572
2022-03-10 00:53:47 - train: epoch 0191, iter [00300, 05004], lr: 0.000647, loss: 1.6428
2022-03-10 00:54:22 - train: epoch 0191, iter [00400, 05004], lr: 0.000647, loss: 1.5479
2022-03-10 00:54:56 - train: epoch 0191, iter [00500, 05004], lr: 0.000647, loss: 1.8518
2022-03-10 00:55:30 - train: epoch 0191, iter [00600, 05004], lr: 0.000647, loss: 1.6496
2022-03-10 00:56:04 - train: epoch 0191, iter [00700, 05004], lr: 0.000647, loss: 1.9020
2022-03-10 00:56:39 - train: epoch 0191, iter [00800, 05004], lr: 0.000647, loss: 1.8008
2022-03-10 00:57:13 - train: epoch 0191, iter [00900, 05004], lr: 0.000647, loss: 1.7249
2022-03-10 00:57:47 - train: epoch 0191, iter [01000, 05004], lr: 0.000647, loss: 1.5034
2022-03-10 00:58:21 - train: epoch 0191, iter [01100, 05004], lr: 0.000647, loss: 1.5058
2022-03-10 00:58:56 - train: epoch 0191, iter [01200, 05004], lr: 0.000647, loss: 1.6856
2022-03-10 00:59:29 - train: epoch 0191, iter [01300, 05004], lr: 0.000647, loss: 2.0068
2022-03-10 01:00:04 - train: epoch 0191, iter [01400, 05004], lr: 0.000647, loss: 1.5926
2022-03-10 01:00:38 - train: epoch 0191, iter [01500, 05004], lr: 0.000647, loss: 1.5596
2022-03-10 01:01:13 - train: epoch 0191, iter [01600, 05004], lr: 0.000647, loss: 1.7409
2022-03-10 01:01:46 - train: epoch 0191, iter [01700, 05004], lr: 0.000647, loss: 1.9227
2022-03-10 01:02:21 - train: epoch 0191, iter [01800, 05004], lr: 0.000647, loss: 1.6869
2022-03-10 01:02:55 - train: epoch 0191, iter [01900, 05004], lr: 0.000647, loss: 1.4226
2022-03-10 01:03:29 - train: epoch 0191, iter [02000, 05004], lr: 0.000647, loss: 1.8780
2022-03-10 01:04:04 - train: epoch 0191, iter [02100, 05004], lr: 0.000647, loss: 1.9197
2022-03-10 01:04:38 - train: epoch 0191, iter [02200, 05004], lr: 0.000647, loss: 1.7743
2022-03-10 01:05:13 - train: epoch 0191, iter [02300, 05004], lr: 0.000647, loss: 1.9261
2022-03-10 01:05:47 - train: epoch 0191, iter [02400, 05004], lr: 0.000647, loss: 1.7575
2022-03-10 01:06:21 - train: epoch 0191, iter [02500, 05004], lr: 0.000647, loss: 1.8190
2022-03-10 01:06:56 - train: epoch 0191, iter [02600, 05004], lr: 0.000647, loss: 1.8189
2022-03-10 01:07:30 - train: epoch 0191, iter [02700, 05004], lr: 0.000647, loss: 1.7497
2022-03-10 01:08:04 - train: epoch 0191, iter [02800, 05004], lr: 0.000647, loss: 1.9367
2022-03-10 01:08:38 - train: epoch 0191, iter [02900, 05004], lr: 0.000647, loss: 1.8529
2022-03-10 01:09:12 - train: epoch 0191, iter [03000, 05004], lr: 0.000647, loss: 1.8033
2022-03-10 01:09:47 - train: epoch 0191, iter [03100, 05004], lr: 0.000647, loss: 2.0860
2022-03-10 01:10:21 - train: epoch 0191, iter [03200, 05004], lr: 0.000647, loss: 1.9871
2022-03-10 01:10:55 - train: epoch 0191, iter [03300, 05004], lr: 0.000647, loss: 1.8779
2022-03-10 01:11:29 - train: epoch 0191, iter [03400, 05004], lr: 0.000647, loss: 1.8149
2022-03-10 01:12:03 - train: epoch 0191, iter [03500, 05004], lr: 0.000647, loss: 1.8043
2022-03-10 01:12:38 - train: epoch 0191, iter [03600, 05004], lr: 0.000647, loss: 1.6489
2022-03-10 01:13:11 - train: epoch 0191, iter [03700, 05004], lr: 0.000647, loss: 1.6906
2022-03-10 01:13:45 - train: epoch 0191, iter [03800, 05004], lr: 0.000647, loss: 1.9956
2022-03-10 01:14:20 - train: epoch 0191, iter [03900, 05004], lr: 0.000647, loss: 1.6891
2022-03-10 01:14:53 - train: epoch 0191, iter [04000, 05004], lr: 0.000647, loss: 2.1353
2022-03-10 01:15:28 - train: epoch 0191, iter [04100, 05004], lr: 0.000647, loss: 1.7912
2022-03-10 01:16:02 - train: epoch 0191, iter [04200, 05004], lr: 0.000647, loss: 1.7406
2022-03-10 01:16:36 - train: epoch 0191, iter [04300, 05004], lr: 0.000647, loss: 1.8329
2022-03-10 01:17:11 - train: epoch 0191, iter [04400, 05004], lr: 0.000647, loss: 1.5449
2022-03-10 01:17:45 - train: epoch 0191, iter [04500, 05004], lr: 0.000647, loss: 1.8468
2022-03-10 01:18:20 - train: epoch 0191, iter [04600, 05004], lr: 0.000647, loss: 1.3159
2022-03-10 01:18:54 - train: epoch 0191, iter [04700, 05004], lr: 0.000647, loss: 1.6334
2022-03-10 01:19:28 - train: epoch 0191, iter [04800, 05004], lr: 0.000647, loss: 1.7859
2022-03-10 01:20:03 - train: epoch 0191, iter [04900, 05004], lr: 0.000647, loss: 1.6912
2022-03-10 01:20:35 - train: epoch 0191, iter [05000, 05004], lr: 0.000647, loss: 1.6978
2022-03-10 01:20:36 - train: epoch 191, train_loss: 1.7671
2022-03-10 01:21:52 - eval: epoch: 191, acc1: 77.114%, acc5: 93.392%, test_loss: 0.9038, per_image_load_time: 2.414ms, per_image_inference_time: 0.513ms
2022-03-10 01:21:53 - until epoch: 191, best_acc1: 77.114%
2022-03-10 01:21:53 - epoch 192 lr: 0.000524680027447444
2022-03-10 01:22:31 - train: epoch 0192, iter [00100, 05004], lr: 0.000525, loss: 1.7250
2022-03-10 01:23:05 - train: epoch 0192, iter [00200, 05004], lr: 0.000525, loss: 1.8519
2022-03-10 01:23:40 - train: epoch 0192, iter [00300, 05004], lr: 0.000525, loss: 1.9946
2022-03-10 01:24:13 - train: epoch 0192, iter [00400, 05004], lr: 0.000525, loss: 1.9203
2022-03-10 01:24:47 - train: epoch 0192, iter [00500, 05004], lr: 0.000525, loss: 2.0258
2022-03-10 01:25:21 - train: epoch 0192, iter [00600, 05004], lr: 0.000525, loss: 1.8191
2022-03-10 01:25:55 - train: epoch 0192, iter [00700, 05004], lr: 0.000525, loss: 2.0672
2022-03-10 01:26:30 - train: epoch 0192, iter [00800, 05004], lr: 0.000525, loss: 1.7126
2022-03-10 01:27:03 - train: epoch 0192, iter [00900, 05004], lr: 0.000525, loss: 1.7990
2022-03-10 01:27:39 - train: epoch 0192, iter [01000, 05004], lr: 0.000525, loss: 2.0017
2022-03-10 01:28:12 - train: epoch 0192, iter [01100, 05004], lr: 0.000525, loss: 2.0016
2022-03-10 01:28:47 - train: epoch 0192, iter [01200, 05004], lr: 0.000525, loss: 2.2240
2022-03-10 01:29:21 - train: epoch 0192, iter [01300, 05004], lr: 0.000525, loss: 1.7128
2022-03-10 01:29:54 - train: epoch 0192, iter [01400, 05004], lr: 0.000525, loss: 1.6261
2022-03-10 01:30:28 - train: epoch 0192, iter [01500, 05004], lr: 0.000525, loss: 1.7512
2022-03-10 01:31:03 - train: epoch 0192, iter [01600, 05004], lr: 0.000525, loss: 1.6444
2022-03-10 01:31:37 - train: epoch 0192, iter [01700, 05004], lr: 0.000525, loss: 1.6996
2022-03-10 01:32:11 - train: epoch 0192, iter [01800, 05004], lr: 0.000525, loss: 1.6357
2022-03-10 01:32:45 - train: epoch 0192, iter [01900, 05004], lr: 0.000525, loss: 1.6605
2022-03-10 01:33:19 - train: epoch 0192, iter [02000, 05004], lr: 0.000525, loss: 1.8835
2022-03-10 01:33:54 - train: epoch 0192, iter [02100, 05004], lr: 0.000525, loss: 1.6096
2022-03-10 01:34:27 - train: epoch 0192, iter [02200, 05004], lr: 0.000525, loss: 1.5336
2022-03-10 01:35:02 - train: epoch 0192, iter [02300, 05004], lr: 0.000525, loss: 1.7363
2022-03-10 01:35:36 - train: epoch 0192, iter [02400, 05004], lr: 0.000525, loss: 1.6191
2022-03-10 01:36:10 - train: epoch 0192, iter [02500, 05004], lr: 0.000525, loss: 1.8062
2022-03-10 01:36:45 - train: epoch 0192, iter [02600, 05004], lr: 0.000525, loss: 1.6871
2022-03-10 01:37:18 - train: epoch 0192, iter [02700, 05004], lr: 0.000525, loss: 1.9402
2022-03-10 01:37:53 - train: epoch 0192, iter [02800, 05004], lr: 0.000525, loss: 2.0293
2022-03-10 01:38:27 - train: epoch 0192, iter [02900, 05004], lr: 0.000525, loss: 1.7719
2022-03-10 01:39:01 - train: epoch 0192, iter [03000, 05004], lr: 0.000525, loss: 2.0867
2022-03-10 01:39:34 - train: epoch 0192, iter [03100, 05004], lr: 0.000525, loss: 1.8967
2022-03-10 01:40:09 - train: epoch 0192, iter [03200, 05004], lr: 0.000525, loss: 1.9034
2022-03-10 01:40:42 - train: epoch 0192, iter [03300, 05004], lr: 0.000525, loss: 1.7466
2022-03-10 01:41:18 - train: epoch 0192, iter [03400, 05004], lr: 0.000525, loss: 2.0089
2022-03-10 01:41:51 - train: epoch 0192, iter [03500, 05004], lr: 0.000525, loss: 1.9294
2022-03-10 01:42:25 - train: epoch 0192, iter [03600, 05004], lr: 0.000525, loss: 1.7826
2022-03-10 01:42:59 - train: epoch 0192, iter [03700, 05004], lr: 0.000525, loss: 1.7616
2022-03-10 01:43:34 - train: epoch 0192, iter [03800, 05004], lr: 0.000525, loss: 1.4004
2022-03-10 01:44:08 - train: epoch 0192, iter [03900, 05004], lr: 0.000525, loss: 1.6455
2022-03-10 01:44:42 - train: epoch 0192, iter [04000, 05004], lr: 0.000525, loss: 1.7420
2022-03-10 01:45:17 - train: epoch 0192, iter [04100, 05004], lr: 0.000525, loss: 1.7810
2022-03-10 01:45:51 - train: epoch 0192, iter [04200, 05004], lr: 0.000525, loss: 2.1621
2022-03-10 01:46:26 - train: epoch 0192, iter [04300, 05004], lr: 0.000525, loss: 1.7677
2022-03-10 01:46:59 - train: epoch 0192, iter [04400, 05004], lr: 0.000525, loss: 1.8302
2022-03-10 01:47:34 - train: epoch 0192, iter [04500, 05004], lr: 0.000525, loss: 1.8492
2022-03-10 01:48:08 - train: epoch 0192, iter [04600, 05004], lr: 0.000525, loss: 1.6866
2022-03-10 01:48:42 - train: epoch 0192, iter [04700, 05004], lr: 0.000525, loss: 1.8100
2022-03-10 01:49:17 - train: epoch 0192, iter [04800, 05004], lr: 0.000525, loss: 1.7818
2022-03-10 01:49:51 - train: epoch 0192, iter [04900, 05004], lr: 0.000525, loss: 1.8873
2022-03-10 01:50:24 - train: epoch 0192, iter [05000, 05004], lr: 0.000525, loss: 1.8405
2022-03-10 01:50:25 - train: epoch 192, train_loss: 1.7576
2022-03-10 01:51:40 - eval: epoch: 192, acc1: 77.214%, acc5: 93.448%, test_loss: 0.9005, per_image_load_time: 2.340ms, per_image_inference_time: 0.566ms
2022-03-10 01:51:41 - until epoch: 192, best_acc1: 77.214%
2022-03-10 01:51:41 - epoch 193 lr: 0.00041471450892189844
2022-03-10 01:52:20 - train: epoch 0193, iter [00100, 05004], lr: 0.000415, loss: 1.8565
2022-03-10 01:52:54 - train: epoch 0193, iter [00200, 05004], lr: 0.000415, loss: 1.6278
2022-03-10 01:53:29 - train: epoch 0193, iter [00300, 05004], lr: 0.000415, loss: 1.6104
2022-03-10 01:54:03 - train: epoch 0193, iter [00400, 05004], lr: 0.000415, loss: 1.6753
2022-03-10 01:54:36 - train: epoch 0193, iter [00500, 05004], lr: 0.000415, loss: 1.8205
2022-03-10 01:55:11 - train: epoch 0193, iter [00600, 05004], lr: 0.000415, loss: 1.6472
2022-03-10 01:55:46 - train: epoch 0193, iter [00700, 05004], lr: 0.000415, loss: 2.0191
2022-03-10 01:56:19 - train: epoch 0193, iter [00800, 05004], lr: 0.000415, loss: 1.6442
2022-03-10 01:56:53 - train: epoch 0193, iter [00900, 05004], lr: 0.000415, loss: 1.6425
2022-03-10 01:57:28 - train: epoch 0193, iter [01000, 05004], lr: 0.000415, loss: 1.9745
2022-03-10 01:58:02 - train: epoch 0193, iter [01100, 05004], lr: 0.000415, loss: 1.7838
2022-03-10 01:58:37 - train: epoch 0193, iter [01200, 05004], lr: 0.000415, loss: 1.7141
2022-03-10 01:59:10 - train: epoch 0193, iter [01300, 05004], lr: 0.000415, loss: 1.9279
2022-03-10 01:59:45 - train: epoch 0193, iter [01400, 05004], lr: 0.000415, loss: 1.7127
2022-03-10 02:00:18 - train: epoch 0193, iter [01500, 05004], lr: 0.000415, loss: 1.7777
2022-03-10 02:00:53 - train: epoch 0193, iter [01600, 05004], lr: 0.000415, loss: 1.4734
2022-03-10 02:01:27 - train: epoch 0193, iter [01700, 05004], lr: 0.000415, loss: 1.7642
2022-03-10 02:02:01 - train: epoch 0193, iter [01800, 05004], lr: 0.000415, loss: 1.9355
2022-03-10 02:02:35 - train: epoch 0193, iter [01900, 05004], lr: 0.000415, loss: 1.5734
2022-03-10 02:03:09 - train: epoch 0193, iter [02000, 05004], lr: 0.000415, loss: 1.7934
2022-03-10 02:03:43 - train: epoch 0193, iter [02100, 05004], lr: 0.000415, loss: 1.7080
2022-03-10 02:04:18 - train: epoch 0193, iter [02200, 05004], lr: 0.000415, loss: 1.7283
2022-03-10 02:04:52 - train: epoch 0193, iter [02300, 05004], lr: 0.000415, loss: 1.8473
2022-03-10 02:05:26 - train: epoch 0193, iter [02400, 05004], lr: 0.000415, loss: 1.6282
2022-03-10 02:06:00 - train: epoch 0193, iter [02500, 05004], lr: 0.000415, loss: 1.8894
2022-03-10 02:06:35 - train: epoch 0193, iter [02600, 05004], lr: 0.000415, loss: 1.6533
2022-03-10 02:07:09 - train: epoch 0193, iter [02700, 05004], lr: 0.000415, loss: 1.9478
2022-03-10 02:07:42 - train: epoch 0193, iter [02800, 05004], lr: 0.000415, loss: 1.8747
2022-03-10 02:08:16 - train: epoch 0193, iter [02900, 05004], lr: 0.000415, loss: 1.8022
2022-03-10 02:08:51 - train: epoch 0193, iter [03000, 05004], lr: 0.000415, loss: 1.5715
2022-03-10 02:09:25 - train: epoch 0193, iter [03100, 05004], lr: 0.000415, loss: 1.7176
2022-03-10 02:09:59 - train: epoch 0193, iter [03200, 05004], lr: 0.000415, loss: 1.8410
2022-03-10 02:10:33 - train: epoch 0193, iter [03300, 05004], lr: 0.000415, loss: 1.8908
2022-03-10 02:11:08 - train: epoch 0193, iter [03400, 05004], lr: 0.000415, loss: 1.9546
2022-03-10 02:11:42 - train: epoch 0193, iter [03500, 05004], lr: 0.000415, loss: 1.5897
2022-03-10 02:12:16 - train: epoch 0193, iter [03600, 05004], lr: 0.000415, loss: 1.6812
2022-03-10 02:12:50 - train: epoch 0193, iter [03700, 05004], lr: 0.000415, loss: 1.4172
2022-03-10 02:13:24 - train: epoch 0193, iter [03800, 05004], lr: 0.000415, loss: 1.7816
2022-03-10 02:13:59 - train: epoch 0193, iter [03900, 05004], lr: 0.000415, loss: 1.6300
2022-03-10 02:14:34 - train: epoch 0193, iter [04000, 05004], lr: 0.000415, loss: 1.6384
2022-03-10 02:15:07 - train: epoch 0193, iter [04100, 05004], lr: 0.000415, loss: 1.9138
2022-03-10 02:15:42 - train: epoch 0193, iter [04200, 05004], lr: 0.000415, loss: 1.7882
2022-03-10 02:16:16 - train: epoch 0193, iter [04300, 05004], lr: 0.000415, loss: 1.9391
2022-03-10 02:16:50 - train: epoch 0193, iter [04400, 05004], lr: 0.000415, loss: 1.5203
2022-03-10 02:17:24 - train: epoch 0193, iter [04500, 05004], lr: 0.000415, loss: 1.5561
2022-03-10 02:17:58 - train: epoch 0193, iter [04600, 05004], lr: 0.000415, loss: 1.9040
2022-03-10 02:18:33 - train: epoch 0193, iter [04700, 05004], lr: 0.000415, loss: 1.9929
2022-03-10 02:19:08 - train: epoch 0193, iter [04800, 05004], lr: 0.000415, loss: 1.6679
2022-03-10 02:19:41 - train: epoch 0193, iter [04900, 05004], lr: 0.000415, loss: 1.6939
2022-03-10 02:20:14 - train: epoch 0193, iter [05000, 05004], lr: 0.000415, loss: 1.8655
2022-03-10 02:20:15 - train: epoch 193, train_loss: 1.7480
2022-03-10 02:21:30 - eval: epoch: 193, acc1: 77.292%, acc5: 93.450%, test_loss: 0.8962, per_image_load_time: 2.400ms, per_image_inference_time: 0.531ms
2022-03-10 02:21:31 - until epoch: 193, best_acc1: 77.292%
2022-03-10 02:21:31 - epoch 194 lr: 0.00031761885408435053
2022-03-10 02:22:10 - train: epoch 0194, iter [00100, 05004], lr: 0.000318, loss: 1.3920
2022-03-10 02:22:45 - train: epoch 0194, iter [00200, 05004], lr: 0.000318, loss: 1.5850
2022-03-10 02:23:19 - train: epoch 0194, iter [00300, 05004], lr: 0.000318, loss: 1.6326
2022-03-10 02:23:53 - train: epoch 0194, iter [00400, 05004], lr: 0.000318, loss: 1.8419
2022-03-10 02:24:28 - train: epoch 0194, iter [00500, 05004], lr: 0.000318, loss: 2.1028
2022-03-10 02:25:02 - train: epoch 0194, iter [00600, 05004], lr: 0.000318, loss: 1.8047
2022-03-10 02:25:36 - train: epoch 0194, iter [00700, 05004], lr: 0.000318, loss: 1.9427
2022-03-10 02:26:10 - train: epoch 0194, iter [00800, 05004], lr: 0.000318, loss: 1.7474
2022-03-10 02:26:44 - train: epoch 0194, iter [00900, 05004], lr: 0.000318, loss: 1.6929
2022-03-10 02:27:19 - train: epoch 0194, iter [01000, 05004], lr: 0.000318, loss: 1.6130
2022-03-10 02:27:53 - train: epoch 0194, iter [01100, 05004], lr: 0.000318, loss: 1.9219
2022-03-10 02:28:28 - train: epoch 0194, iter [01200, 05004], lr: 0.000318, loss: 1.8125
2022-03-10 02:29:02 - train: epoch 0194, iter [01300, 05004], lr: 0.000318, loss: 1.6041
2022-03-10 02:29:36 - train: epoch 0194, iter [01400, 05004], lr: 0.000318, loss: 1.5386
2022-03-10 02:30:11 - train: epoch 0194, iter [01500, 05004], lr: 0.000318, loss: 2.1889
2022-03-10 02:30:45 - train: epoch 0194, iter [01600, 05004], lr: 0.000318, loss: 1.9723
2022-03-10 02:31:19 - train: epoch 0194, iter [01700, 05004], lr: 0.000318, loss: 1.6233
2022-03-10 02:31:53 - train: epoch 0194, iter [01800, 05004], lr: 0.000318, loss: 1.7040
2022-03-10 02:32:28 - train: epoch 0194, iter [01900, 05004], lr: 0.000318, loss: 1.5942
2022-03-10 02:33:02 - train: epoch 0194, iter [02000, 05004], lr: 0.000318, loss: 1.7704
2022-03-10 02:33:36 - train: epoch 0194, iter [02100, 05004], lr: 0.000318, loss: 1.7165
2022-03-10 02:34:10 - train: epoch 0194, iter [02200, 05004], lr: 0.000318, loss: 1.8189
2022-03-10 02:34:45 - train: epoch 0194, iter [02300, 05004], lr: 0.000318, loss: 1.6761
2022-03-10 02:35:20 - train: epoch 0194, iter [02400, 05004], lr: 0.000318, loss: 1.6390
2022-03-10 02:35:53 - train: epoch 0194, iter [02500, 05004], lr: 0.000318, loss: 1.8665
2022-03-10 02:36:28 - train: epoch 0194, iter [02600, 05004], lr: 0.000318, loss: 2.0824
2022-03-10 02:37:02 - train: epoch 0194, iter [02700, 05004], lr: 0.000318, loss: 1.4572
2022-03-10 02:37:36 - train: epoch 0194, iter [02800, 05004], lr: 0.000318, loss: 1.8351
2022-03-10 02:38:10 - train: epoch 0194, iter [02900, 05004], lr: 0.000318, loss: 1.6510
2022-03-10 02:38:44 - train: epoch 0194, iter [03000, 05004], lr: 0.000318, loss: 1.9889
2022-03-10 02:39:18 - train: epoch 0194, iter [03100, 05004], lr: 0.000318, loss: 1.5348
2022-03-10 02:39:52 - train: epoch 0194, iter [03200, 05004], lr: 0.000318, loss: 1.8983
2022-03-10 02:40:28 - train: epoch 0194, iter [03300, 05004], lr: 0.000318, loss: 1.7706
2022-03-10 02:41:01 - train: epoch 0194, iter [03400, 05004], lr: 0.000318, loss: 1.7927
2022-03-10 02:41:35 - train: epoch 0194, iter [03500, 05004], lr: 0.000318, loss: 1.5655
2022-03-10 02:42:10 - train: epoch 0194, iter [03600, 05004], lr: 0.000318, loss: 1.8242
2022-03-10 02:42:43 - train: epoch 0194, iter [03700, 05004], lr: 0.000318, loss: 1.9593
2022-03-10 02:43:18 - train: epoch 0194, iter [03800, 05004], lr: 0.000318, loss: 2.0347
2022-03-10 02:43:52 - train: epoch 0194, iter [03900, 05004], lr: 0.000318, loss: 1.7771
2022-03-10 02:44:26 - train: epoch 0194, iter [04000, 05004], lr: 0.000318, loss: 1.7044
2022-03-10 02:45:01 - train: epoch 0194, iter [04100, 05004], lr: 0.000318, loss: 1.5472
2022-03-10 02:45:35 - train: epoch 0194, iter [04200, 05004], lr: 0.000318, loss: 1.6545
2022-03-10 02:46:09 - train: epoch 0194, iter [04300, 05004], lr: 0.000318, loss: 1.7002
2022-03-10 02:46:43 - train: epoch 0194, iter [04400, 05004], lr: 0.000318, loss: 1.8085
2022-03-10 02:47:18 - train: epoch 0194, iter [04500, 05004], lr: 0.000318, loss: 1.6378
2022-03-10 02:47:53 - train: epoch 0194, iter [04600, 05004], lr: 0.000318, loss: 1.4387
2022-03-10 02:48:27 - train: epoch 0194, iter [04700, 05004], lr: 0.000318, loss: 1.7523
2022-03-10 02:49:01 - train: epoch 0194, iter [04800, 05004], lr: 0.000318, loss: 1.4224
2022-03-10 02:49:35 - train: epoch 0194, iter [04900, 05004], lr: 0.000318, loss: 1.9567
2022-03-10 02:50:08 - train: epoch 0194, iter [05000, 05004], lr: 0.000318, loss: 1.8031
2022-03-10 02:50:09 - train: epoch 194, train_loss: 1.7424
2022-03-10 02:51:24 - eval: epoch: 194, acc1: 77.366%, acc5: 93.534%, test_loss: 0.8920, per_image_load_time: 2.305ms, per_image_inference_time: 0.568ms
2022-03-10 02:51:25 - until epoch: 194, best_acc1: 77.366%
2022-03-10 02:51:25 - epoch 195 lr: 0.00023341826411756863
2022-03-10 02:52:03 - train: epoch 0195, iter [00100, 05004], lr: 0.000233, loss: 1.7001
2022-03-10 02:52:37 - train: epoch 0195, iter [00200, 05004], lr: 0.000233, loss: 1.7105
2022-03-10 02:53:12 - train: epoch 0195, iter [00300, 05004], lr: 0.000233, loss: 1.5715
2022-03-10 02:53:46 - train: epoch 0195, iter [00400, 05004], lr: 0.000233, loss: 1.8352
2022-03-10 02:54:20 - train: epoch 0195, iter [00500, 05004], lr: 0.000233, loss: 1.7046
2022-03-10 02:54:55 - train: epoch 0195, iter [00600, 05004], lr: 0.000233, loss: 1.6699
2022-03-10 02:55:29 - train: epoch 0195, iter [00700, 05004], lr: 0.000233, loss: 1.7808
2022-03-10 02:56:03 - train: epoch 0195, iter [00800, 05004], lr: 0.000233, loss: 1.7562
2022-03-10 02:56:37 - train: epoch 0195, iter [00900, 05004], lr: 0.000233, loss: 1.6354
2022-03-10 02:57:11 - train: epoch 0195, iter [01000, 05004], lr: 0.000233, loss: 1.9210
2022-03-10 02:57:45 - train: epoch 0195, iter [01100, 05004], lr: 0.000233, loss: 1.9286
2022-03-10 02:58:20 - train: epoch 0195, iter [01200, 05004], lr: 0.000233, loss: 1.7502
2022-03-10 02:58:55 - train: epoch 0195, iter [01300, 05004], lr: 0.000233, loss: 1.7814
2022-03-10 02:59:30 - train: epoch 0195, iter [01400, 05004], lr: 0.000233, loss: 1.8869
2022-03-10 03:00:04 - train: epoch 0195, iter [01500, 05004], lr: 0.000233, loss: 1.7092
2022-03-10 03:00:38 - train: epoch 0195, iter [01600, 05004], lr: 0.000233, loss: 1.7159
2022-03-10 03:01:11 - train: epoch 0195, iter [01700, 05004], lr: 0.000233, loss: 1.7177
2022-03-10 03:01:46 - train: epoch 0195, iter [01800, 05004], lr: 0.000233, loss: 2.0054
2022-03-10 03:02:19 - train: epoch 0195, iter [01900, 05004], lr: 0.000233, loss: 1.9852
2022-03-10 03:02:55 - train: epoch 0195, iter [02000, 05004], lr: 0.000233, loss: 1.6471
2022-03-10 03:03:29 - train: epoch 0195, iter [02100, 05004], lr: 0.000233, loss: 1.9789
2022-03-10 03:04:04 - train: epoch 0195, iter [02200, 05004], lr: 0.000233, loss: 1.8442
2022-03-10 03:04:37 - train: epoch 0195, iter [02300, 05004], lr: 0.000233, loss: 1.7638
2022-03-10 03:05:12 - train: epoch 0195, iter [02400, 05004], lr: 0.000233, loss: 1.6693
2022-03-10 03:05:47 - train: epoch 0195, iter [02500, 05004], lr: 0.000233, loss: 1.5880
2022-03-10 03:06:21 - train: epoch 0195, iter [02600, 05004], lr: 0.000233, loss: 1.5586
2022-03-10 03:06:56 - train: epoch 0195, iter [02700, 05004], lr: 0.000233, loss: 1.8712
2022-03-10 03:07:30 - train: epoch 0195, iter [02800, 05004], lr: 0.000233, loss: 1.5539
2022-03-10 03:08:05 - train: epoch 0195, iter [02900, 05004], lr: 0.000233, loss: 1.6796
2022-03-10 03:08:39 - train: epoch 0195, iter [03000, 05004], lr: 0.000233, loss: 1.6151
2022-03-10 03:09:13 - train: epoch 0195, iter [03100, 05004], lr: 0.000233, loss: 1.6947
2022-03-10 03:09:48 - train: epoch 0195, iter [03200, 05004], lr: 0.000233, loss: 1.6293
2022-03-10 03:10:23 - train: epoch 0195, iter [03300, 05004], lr: 0.000233, loss: 1.6607
2022-03-10 03:10:57 - train: epoch 0195, iter [03400, 05004], lr: 0.000233, loss: 1.5560
2022-03-10 03:11:31 - train: epoch 0195, iter [03500, 05004], lr: 0.000233, loss: 1.6525
2022-03-10 03:12:05 - train: epoch 0195, iter [03600, 05004], lr: 0.000233, loss: 1.7703
2022-03-10 03:12:39 - train: epoch 0195, iter [03700, 05004], lr: 0.000233, loss: 1.7519
2022-03-10 03:13:13 - train: epoch 0195, iter [03800, 05004], lr: 0.000233, loss: 1.5878
2022-03-10 03:13:48 - train: epoch 0195, iter [03900, 05004], lr: 0.000233, loss: 1.6613
2022-03-10 03:14:23 - train: epoch 0195, iter [04000, 05004], lr: 0.000233, loss: 1.8504
2022-03-10 03:14:57 - train: epoch 0195, iter [04100, 05004], lr: 0.000233, loss: 1.5240
2022-03-10 03:15:31 - train: epoch 0195, iter [04200, 05004], lr: 0.000233, loss: 1.7445
2022-03-10 03:16:06 - train: epoch 0195, iter [04300, 05004], lr: 0.000233, loss: 1.6206
2022-03-10 03:16:40 - train: epoch 0195, iter [04400, 05004], lr: 0.000233, loss: 1.5814
2022-03-10 03:17:15 - train: epoch 0195, iter [04500, 05004], lr: 0.000233, loss: 1.8694
2022-03-10 03:17:49 - train: epoch 0195, iter [04600, 05004], lr: 0.000233, loss: 1.7879
2022-03-10 03:18:23 - train: epoch 0195, iter [04700, 05004], lr: 0.000233, loss: 1.7511
2022-03-10 03:18:57 - train: epoch 0195, iter [04800, 05004], lr: 0.000233, loss: 1.5433
2022-03-10 03:19:32 - train: epoch 0195, iter [04900, 05004], lr: 0.000233, loss: 1.6524
2022-03-10 03:20:05 - train: epoch 0195, iter [05000, 05004], lr: 0.000233, loss: 1.7729
2022-03-10 03:20:06 - train: epoch 195, train_loss: 1.7354
2022-03-10 03:21:20 - eval: epoch: 195, acc1: 77.476%, acc5: 93.574%, test_loss: 0.8906, per_image_load_time: 2.331ms, per_image_inference_time: 0.549ms
2022-03-10 03:21:21 - until epoch: 195, best_acc1: 77.476%
2022-03-10 03:21:21 - epoch 196 lr: 0.00016213459328950355
2022-03-10 03:22:00 - train: epoch 0196, iter [00100, 05004], lr: 0.000162, loss: 1.8488
2022-03-10 03:22:34 - train: epoch 0196, iter [00200, 05004], lr: 0.000162, loss: 1.6748
2022-03-10 03:23:08 - train: epoch 0196, iter [00300, 05004], lr: 0.000162, loss: 1.8865
2022-03-10 03:23:43 - train: epoch 0196, iter [00400, 05004], lr: 0.000162, loss: 1.8029
2022-03-10 03:24:17 - train: epoch 0196, iter [00500, 05004], lr: 0.000162, loss: 1.6463
2022-03-10 03:24:52 - train: epoch 0196, iter [00600, 05004], lr: 0.000162, loss: 1.8102
2022-03-10 03:25:26 - train: epoch 0196, iter [00700, 05004], lr: 0.000162, loss: 1.7443
2022-03-10 03:25:59 - train: epoch 0196, iter [00800, 05004], lr: 0.000162, loss: 1.5278
2022-03-10 03:26:34 - train: epoch 0196, iter [00900, 05004], lr: 0.000162, loss: 1.4387
2022-03-10 03:27:08 - train: epoch 0196, iter [01000, 05004], lr: 0.000162, loss: 1.6234
2022-03-10 03:27:42 - train: epoch 0196, iter [01100, 05004], lr: 0.000162, loss: 1.8182
2022-03-10 03:28:16 - train: epoch 0196, iter [01200, 05004], lr: 0.000162, loss: 1.8157
2022-03-10 03:28:50 - train: epoch 0196, iter [01300, 05004], lr: 0.000162, loss: 1.9575
2022-03-10 03:29:25 - train: epoch 0196, iter [01400, 05004], lr: 0.000162, loss: 1.8917
2022-03-10 03:29:59 - train: epoch 0196, iter [01500, 05004], lr: 0.000162, loss: 1.8704
2022-03-10 03:30:33 - train: epoch 0196, iter [01600, 05004], lr: 0.000162, loss: 1.5734
2022-03-10 03:31:07 - train: epoch 0196, iter [01700, 05004], lr: 0.000162, loss: 1.4596
2022-03-10 03:31:41 - train: epoch 0196, iter [01800, 05004], lr: 0.000162, loss: 1.7076
2022-03-10 03:32:14 - train: epoch 0196, iter [01900, 05004], lr: 0.000162, loss: 1.6429
2022-03-10 03:32:48 - train: epoch 0196, iter [02000, 05004], lr: 0.000162, loss: 1.6059
2022-03-10 03:33:23 - train: epoch 0196, iter [02100, 05004], lr: 0.000162, loss: 1.8328
2022-03-10 03:33:58 - train: epoch 0196, iter [02200, 05004], lr: 0.000162, loss: 1.8273
2022-03-10 03:34:31 - train: epoch 0196, iter [02300, 05004], lr: 0.000162, loss: 1.6300
2022-03-10 03:35:05 - train: epoch 0196, iter [02400, 05004], lr: 0.000162, loss: 1.7585
2022-03-10 03:35:38 - train: epoch 0196, iter [02500, 05004], lr: 0.000162, loss: 1.7547
2022-03-10 03:36:13 - train: epoch 0196, iter [02600, 05004], lr: 0.000162, loss: 1.7521
2022-03-10 03:36:46 - train: epoch 0196, iter [02700, 05004], lr: 0.000162, loss: 1.8372
2022-03-10 03:37:21 - train: epoch 0196, iter [02800, 05004], lr: 0.000162, loss: 1.6607
2022-03-10 03:37:55 - train: epoch 0196, iter [02900, 05004], lr: 0.000162, loss: 1.8533
2022-03-10 03:38:30 - train: epoch 0196, iter [03000, 05004], lr: 0.000162, loss: 1.8235
2022-03-10 03:39:03 - train: epoch 0196, iter [03100, 05004], lr: 0.000162, loss: 1.9368
2022-03-10 03:39:38 - train: epoch 0196, iter [03200, 05004], lr: 0.000162, loss: 1.7215
2022-03-10 03:40:11 - train: epoch 0196, iter [03300, 05004], lr: 0.000162, loss: 1.9386
2022-03-10 03:40:45 - train: epoch 0196, iter [03400, 05004], lr: 0.000162, loss: 1.7849
2022-03-10 03:41:20 - train: epoch 0196, iter [03500, 05004], lr: 0.000162, loss: 1.6546
2022-03-10 03:41:53 - train: epoch 0196, iter [03600, 05004], lr: 0.000162, loss: 1.8307
2022-03-10 03:42:28 - train: epoch 0196, iter [03700, 05004], lr: 0.000162, loss: 1.9050
2022-03-10 03:43:02 - train: epoch 0196, iter [03800, 05004], lr: 0.000162, loss: 1.4315
2022-03-10 03:43:36 - train: epoch 0196, iter [03900, 05004], lr: 0.000162, loss: 1.5031
2022-03-10 03:44:11 - train: epoch 0196, iter [04000, 05004], lr: 0.000162, loss: 1.4966
2022-03-10 03:44:45 - train: epoch 0196, iter [04100, 05004], lr: 0.000162, loss: 1.8833
2022-03-10 03:45:19 - train: epoch 0196, iter [04200, 05004], lr: 0.000162, loss: 1.7762
2022-03-10 03:45:53 - train: epoch 0196, iter [04300, 05004], lr: 0.000162, loss: 1.7639
2022-03-10 03:46:27 - train: epoch 0196, iter [04400, 05004], lr: 0.000162, loss: 1.6467
2022-03-10 03:47:01 - train: epoch 0196, iter [04500, 05004], lr: 0.000162, loss: 1.2712
2022-03-10 03:47:35 - train: epoch 0196, iter [04600, 05004], lr: 0.000162, loss: 1.7736
2022-03-10 03:48:10 - train: epoch 0196, iter [04700, 05004], lr: 0.000162, loss: 1.9833
2022-03-10 03:48:45 - train: epoch 0196, iter [04800, 05004], lr: 0.000162, loss: 1.7848
2022-03-10 03:49:19 - train: epoch 0196, iter [04900, 05004], lr: 0.000162, loss: 1.6327
2022-03-10 03:49:52 - train: epoch 0196, iter [05000, 05004], lr: 0.000162, loss: 1.5763
2022-03-10 03:49:53 - train: epoch 196, train_loss: 1.7301
2022-03-10 03:51:08 - eval: epoch: 196, acc1: 77.540%, acc5: 93.532%, test_loss: 0.8905, per_image_load_time: 2.117ms, per_image_inference_time: 0.536ms
2022-03-10 03:51:09 - until epoch: 196, best_acc1: 77.540%
2022-03-10 03:51:09 - epoch 197 lr: 0.00010378634328099269
2022-03-10 03:51:47 - train: epoch 0197, iter [00100, 05004], lr: 0.000104, loss: 1.4498
2022-03-10 03:52:21 - train: epoch 0197, iter [00200, 05004], lr: 0.000104, loss: 1.7342
2022-03-10 03:52:57 - train: epoch 0197, iter [00300, 05004], lr: 0.000104, loss: 1.5360
2022-03-10 03:53:30 - train: epoch 0197, iter [00400, 05004], lr: 0.000104, loss: 1.6106
2022-03-10 03:54:04 - train: epoch 0197, iter [00500, 05004], lr: 0.000104, loss: 1.9746
2022-03-10 03:54:38 - train: epoch 0197, iter [00600, 05004], lr: 0.000104, loss: 2.0018
2022-03-10 03:55:12 - train: epoch 0197, iter [00700, 05004], lr: 0.000104, loss: 1.6021
2022-03-10 03:55:47 - train: epoch 0197, iter [00800, 05004], lr: 0.000104, loss: 1.8806
2022-03-10 03:56:20 - train: epoch 0197, iter [00900, 05004], lr: 0.000104, loss: 1.6282
2022-03-10 03:56:55 - train: epoch 0197, iter [01000, 05004], lr: 0.000104, loss: 1.7306
2022-03-10 03:57:29 - train: epoch 0197, iter [01100, 05004], lr: 0.000104, loss: 1.9494
2022-03-10 03:58:03 - train: epoch 0197, iter [01200, 05004], lr: 0.000104, loss: 1.9374
2022-03-10 03:58:37 - train: epoch 0197, iter [01300, 05004], lr: 0.000104, loss: 1.7722
2022-03-10 03:59:11 - train: epoch 0197, iter [01400, 05004], lr: 0.000104, loss: 1.9481
2022-03-10 03:59:45 - train: epoch 0197, iter [01500, 05004], lr: 0.000104, loss: 1.6930
2022-03-10 04:00:20 - train: epoch 0197, iter [01600, 05004], lr: 0.000104, loss: 1.4182
2022-03-10 04:00:55 - train: epoch 0197, iter [01700, 05004], lr: 0.000104, loss: 2.0177
2022-03-10 04:01:29 - train: epoch 0197, iter [01800, 05004], lr: 0.000104, loss: 1.8568
2022-03-10 04:02:03 - train: epoch 0197, iter [01900, 05004], lr: 0.000104, loss: 1.8540
2022-03-10 04:02:37 - train: epoch 0197, iter [02000, 05004], lr: 0.000104, loss: 1.8973
2022-03-10 04:03:11 - train: epoch 0197, iter [02100, 05004], lr: 0.000104, loss: 1.7077
2022-03-10 04:03:44 - train: epoch 0197, iter [02200, 05004], lr: 0.000104, loss: 1.6875
2022-03-10 04:04:19 - train: epoch 0197, iter [02300, 05004], lr: 0.000104, loss: 1.8307
2022-03-10 04:04:53 - train: epoch 0197, iter [02400, 05004], lr: 0.000104, loss: 1.6899
2022-03-10 04:05:28 - train: epoch 0197, iter [02500, 05004], lr: 0.000104, loss: 1.7897
2022-03-10 04:06:02 - train: epoch 0197, iter [02600, 05004], lr: 0.000104, loss: 1.8170
2022-03-10 04:06:36 - train: epoch 0197, iter [02700, 05004], lr: 0.000104, loss: 1.8659
2022-03-10 04:07:10 - train: epoch 0197, iter [02800, 05004], lr: 0.000104, loss: 1.5953
2022-03-10 04:07:44 - train: epoch 0197, iter [02900, 05004], lr: 0.000104, loss: 1.8191
2022-03-10 04:08:19 - train: epoch 0197, iter [03000, 05004], lr: 0.000104, loss: 1.7614
2022-03-10 04:08:53 - train: epoch 0197, iter [03100, 05004], lr: 0.000104, loss: 1.9898
2022-03-10 04:09:26 - train: epoch 0197, iter [03200, 05004], lr: 0.000104, loss: 1.4004
2022-03-10 04:10:01 - train: epoch 0197, iter [03300, 05004], lr: 0.000104, loss: 1.8947
2022-03-10 04:10:36 - train: epoch 0197, iter [03400, 05004], lr: 0.000104, loss: 1.4753
2022-03-10 04:11:09 - train: epoch 0197, iter [03500, 05004], lr: 0.000104, loss: 2.0941
2022-03-10 04:11:43 - train: epoch 0197, iter [03600, 05004], lr: 0.000104, loss: 1.6757
2022-03-10 04:12:18 - train: epoch 0197, iter [03700, 05004], lr: 0.000104, loss: 1.7790
2022-03-10 04:12:53 - train: epoch 0197, iter [03800, 05004], lr: 0.000104, loss: 1.7342
2022-03-10 04:13:27 - train: epoch 0197, iter [03900, 05004], lr: 0.000104, loss: 1.4875
2022-03-10 04:14:02 - train: epoch 0197, iter [04000, 05004], lr: 0.000104, loss: 1.6255
2022-03-10 04:14:35 - train: epoch 0197, iter [04100, 05004], lr: 0.000104, loss: 1.8650
2022-03-10 04:15:10 - train: epoch 0197, iter [04200, 05004], lr: 0.000104, loss: 1.5200
2022-03-10 04:15:44 - train: epoch 0197, iter [04300, 05004], lr: 0.000104, loss: 1.6706
2022-03-10 04:16:18 - train: epoch 0197, iter [04400, 05004], lr: 0.000104, loss: 1.7666
2022-03-10 04:16:53 - train: epoch 0197, iter [04500, 05004], lr: 0.000104, loss: 1.6504
2022-03-10 04:17:27 - train: epoch 0197, iter [04600, 05004], lr: 0.000104, loss: 1.8661
2022-03-10 04:18:02 - train: epoch 0197, iter [04700, 05004], lr: 0.000104, loss: 1.6739
2022-03-10 04:18:35 - train: epoch 0197, iter [04800, 05004], lr: 0.000104, loss: 1.6704
2022-03-10 04:19:10 - train: epoch 0197, iter [04900, 05004], lr: 0.000104, loss: 1.7493
2022-03-10 04:19:43 - train: epoch 0197, iter [05000, 05004], lr: 0.000104, loss: 1.8639
2022-03-10 04:19:44 - train: epoch 197, train_loss: 1.7228
2022-03-10 04:20:59 - eval: epoch: 197, acc1: 77.618%, acc5: 93.534%, test_loss: 0.8868, per_image_load_time: 2.115ms, per_image_inference_time: 0.569ms
2022-03-10 04:21:00 - until epoch: 197, best_acc1: 77.618%
2022-03-10 04:21:00 - epoch 198 lr: 5.8388658383667914e-05
2022-03-10 04:21:39 - train: epoch 0198, iter [00100, 05004], lr: 0.000058, loss: 1.6111
2022-03-10 04:22:13 - train: epoch 0198, iter [00200, 05004], lr: 0.000058, loss: 1.6186
2022-03-10 04:22:46 - train: epoch 0198, iter [00300, 05004], lr: 0.000058, loss: 1.8249
2022-03-10 04:23:21 - train: epoch 0198, iter [00400, 05004], lr: 0.000058, loss: 1.9320
2022-03-10 04:23:54 - train: epoch 0198, iter [00500, 05004], lr: 0.000058, loss: 1.7752
2022-03-10 04:24:29 - train: epoch 0198, iter [00600, 05004], lr: 0.000058, loss: 1.8536
2022-03-10 04:25:02 - train: epoch 0198, iter [00700, 05004], lr: 0.000058, loss: 1.6104
2022-03-10 04:25:37 - train: epoch 0198, iter [00800, 05004], lr: 0.000058, loss: 1.4899
2022-03-10 04:26:11 - train: epoch 0198, iter [00900, 05004], lr: 0.000058, loss: 1.7717
2022-03-10 04:26:45 - train: epoch 0198, iter [01000, 05004], lr: 0.000058, loss: 1.6819
2022-03-10 04:27:19 - train: epoch 0198, iter [01100, 05004], lr: 0.000058, loss: 1.5347
2022-03-10 04:27:53 - train: epoch 0198, iter [01200, 05004], lr: 0.000058, loss: 1.5852
2022-03-10 04:28:27 - train: epoch 0198, iter [01300, 05004], lr: 0.000058, loss: 1.7755
2022-03-10 04:29:01 - train: epoch 0198, iter [01400, 05004], lr: 0.000058, loss: 1.6021
2022-03-10 04:29:36 - train: epoch 0198, iter [01500, 05004], lr: 0.000058, loss: 1.7017
2022-03-10 04:30:11 - train: epoch 0198, iter [01600, 05004], lr: 0.000058, loss: 2.0453
2022-03-10 04:30:45 - train: epoch 0198, iter [01700, 05004], lr: 0.000058, loss: 1.4451
2022-03-10 04:31:19 - train: epoch 0198, iter [01800, 05004], lr: 0.000058, loss: 1.7625
2022-03-10 04:31:54 - train: epoch 0198, iter [01900, 05004], lr: 0.000058, loss: 1.7687
2022-03-10 04:32:28 - train: epoch 0198, iter [02000, 05004], lr: 0.000058, loss: 1.8061
2022-03-10 04:33:03 - train: epoch 0198, iter [02100, 05004], lr: 0.000058, loss: 1.7333
2022-03-10 04:33:37 - train: epoch 0198, iter [02200, 05004], lr: 0.000058, loss: 1.7803
2022-03-10 04:34:11 - train: epoch 0198, iter [02300, 05004], lr: 0.000058, loss: 1.6713
2022-03-10 04:34:46 - train: epoch 0198, iter [02400, 05004], lr: 0.000058, loss: 1.4148
2022-03-10 04:35:19 - train: epoch 0198, iter [02500, 05004], lr: 0.000058, loss: 1.4223
2022-03-10 04:35:54 - train: epoch 0198, iter [02600, 05004], lr: 0.000058, loss: 1.6013
2022-03-10 04:36:29 - train: epoch 0198, iter [02700, 05004], lr: 0.000058, loss: 1.9963
2022-03-10 04:37:03 - train: epoch 0198, iter [02800, 05004], lr: 0.000058, loss: 1.7569
2022-03-10 04:37:38 - train: epoch 0198, iter [02900, 05004], lr: 0.000058, loss: 1.8303
2022-03-10 04:38:13 - train: epoch 0198, iter [03000, 05004], lr: 0.000058, loss: 2.0157
2022-03-10 04:38:47 - train: epoch 0198, iter [03100, 05004], lr: 0.000058, loss: 1.7243
2022-03-10 04:39:21 - train: epoch 0198, iter [03200, 05004], lr: 0.000058, loss: 1.8540
2022-03-10 04:39:56 - train: epoch 0198, iter [03300, 05004], lr: 0.000058, loss: 1.7608
2022-03-10 04:40:30 - train: epoch 0198, iter [03400, 05004], lr: 0.000058, loss: 1.7369
2022-03-10 04:41:04 - train: epoch 0198, iter [03500, 05004], lr: 0.000058, loss: 1.7013
2022-03-10 04:41:38 - train: epoch 0198, iter [03600, 05004], lr: 0.000058, loss: 2.0475
2022-03-10 04:42:12 - train: epoch 0198, iter [03700, 05004], lr: 0.000058, loss: 1.6821
2022-03-10 04:42:47 - train: epoch 0198, iter [03800, 05004], lr: 0.000058, loss: 1.9125
2022-03-10 04:43:22 - train: epoch 0198, iter [03900, 05004], lr: 0.000058, loss: 1.7026
2022-03-10 04:43:56 - train: epoch 0198, iter [04000, 05004], lr: 0.000058, loss: 1.5661
2022-03-10 04:44:30 - train: epoch 0198, iter [04100, 05004], lr: 0.000058, loss: 1.4848
2022-03-10 04:45:05 - train: epoch 0198, iter [04200, 05004], lr: 0.000058, loss: 1.7170
2022-03-10 04:45:40 - train: epoch 0198, iter [04300, 05004], lr: 0.000058, loss: 1.4024
2022-03-10 04:46:12 - train: epoch 0198, iter [04400, 05004], lr: 0.000058, loss: 1.6020
2022-03-10 04:46:47 - train: epoch 0198, iter [04500, 05004], lr: 0.000058, loss: 1.6430
2022-03-10 04:47:21 - train: epoch 0198, iter [04600, 05004], lr: 0.000058, loss: 1.8675
2022-03-10 04:47:57 - train: epoch 0198, iter [04700, 05004], lr: 0.000058, loss: 1.5460
2022-03-10 04:48:31 - train: epoch 0198, iter [04800, 05004], lr: 0.000058, loss: 1.8711
2022-03-10 04:49:05 - train: epoch 0198, iter [04900, 05004], lr: 0.000058, loss: 1.6229
2022-03-10 04:49:39 - train: epoch 0198, iter [05000, 05004], lr: 0.000058, loss: 1.6764
2022-03-10 04:49:40 - train: epoch 198, train_loss: 1.7232
2022-03-10 04:50:55 - eval: epoch: 198, acc1: 77.520%, acc5: 93.584%, test_loss: 0.8862, per_image_load_time: 1.982ms, per_image_inference_time: 0.567ms
2022-03-10 04:50:55 - until epoch: 198, best_acc1: 77.618%
2022-03-10 04:50:55 - epoch 199 lr: 2.595332156925534e-05
2022-03-10 04:51:34 - train: epoch 0199, iter [00100, 05004], lr: 0.000026, loss: 1.7873
2022-03-10 04:52:08 - train: epoch 0199, iter [00200, 05004], lr: 0.000026, loss: 1.6360
2022-03-10 04:52:43 - train: epoch 0199, iter [00300, 05004], lr: 0.000026, loss: 1.6936
2022-03-10 04:53:16 - train: epoch 0199, iter [00400, 05004], lr: 0.000026, loss: 1.7634
2022-03-10 04:53:50 - train: epoch 0199, iter [00500, 05004], lr: 0.000026, loss: 1.5833
2022-03-10 04:54:24 - train: epoch 0199, iter [00600, 05004], lr: 0.000026, loss: 1.7078
2022-03-10 04:54:57 - train: epoch 0199, iter [00700, 05004], lr: 0.000026, loss: 1.6492
2022-03-10 04:55:31 - train: epoch 0199, iter [00800, 05004], lr: 0.000026, loss: 1.9202
2022-03-10 04:56:06 - train: epoch 0199, iter [00900, 05004], lr: 0.000026, loss: 2.0613
2022-03-10 04:56:40 - train: epoch 0199, iter [01000, 05004], lr: 0.000026, loss: 1.6526
2022-03-10 04:57:14 - train: epoch 0199, iter [01100, 05004], lr: 0.000026, loss: 1.6076
2022-03-10 04:57:48 - train: epoch 0199, iter [01200, 05004], lr: 0.000026, loss: 1.4562
2022-03-10 04:58:22 - train: epoch 0199, iter [01300, 05004], lr: 0.000026, loss: 1.9410
2022-03-10 04:58:56 - train: epoch 0199, iter [01400, 05004], lr: 0.000026, loss: 1.5100
2022-03-10 04:59:29 - train: epoch 0199, iter [01500, 05004], lr: 0.000026, loss: 1.8874
2022-03-10 05:00:04 - train: epoch 0199, iter [01600, 05004], lr: 0.000026, loss: 1.6608
2022-03-10 05:00:39 - train: epoch 0199, iter [01700, 05004], lr: 0.000026, loss: 1.7197
2022-03-10 05:01:13 - train: epoch 0199, iter [01800, 05004], lr: 0.000026, loss: 2.0820
2022-03-10 05:01:48 - train: epoch 0199, iter [01900, 05004], lr: 0.000026, loss: 1.6523
2022-03-10 05:02:22 - train: epoch 0199, iter [02000, 05004], lr: 0.000026, loss: 1.8486
2022-03-10 05:02:57 - train: epoch 0199, iter [02100, 05004], lr: 0.000026, loss: 1.7193
2022-03-10 05:03:31 - train: epoch 0199, iter [02200, 05004], lr: 0.000026, loss: 1.7747
2022-03-10 05:04:06 - train: epoch 0199, iter [02300, 05004], lr: 0.000026, loss: 1.5835
2022-03-10 05:04:40 - train: epoch 0199, iter [02400, 05004], lr: 0.000026, loss: 1.5820
2022-03-10 05:05:14 - train: epoch 0199, iter [02500, 05004], lr: 0.000026, loss: 1.7488
2022-03-10 05:05:48 - train: epoch 0199, iter [02600, 05004], lr: 0.000026, loss: 1.8193
2022-03-10 05:06:22 - train: epoch 0199, iter [02700, 05004], lr: 0.000026, loss: 2.1648
2022-03-10 05:06:57 - train: epoch 0199, iter [02800, 05004], lr: 0.000026, loss: 1.6102
2022-03-10 05:07:31 - train: epoch 0199, iter [02900, 05004], lr: 0.000026, loss: 1.8329
2022-03-10 05:08:04 - train: epoch 0199, iter [03000, 05004], lr: 0.000026, loss: 1.7394
2022-03-10 05:08:38 - train: epoch 0199, iter [03100, 05004], lr: 0.000026, loss: 1.7729
2022-03-10 05:09:12 - train: epoch 0199, iter [03200, 05004], lr: 0.000026, loss: 1.5076
2022-03-10 05:09:47 - train: epoch 0199, iter [03300, 05004], lr: 0.000026, loss: 1.8572
2022-03-10 05:10:21 - train: epoch 0199, iter [03400, 05004], lr: 0.000026, loss: 1.7218
2022-03-10 05:10:55 - train: epoch 0199, iter [03500, 05004], lr: 0.000026, loss: 1.9200
2022-03-10 05:11:29 - train: epoch 0199, iter [03600, 05004], lr: 0.000026, loss: 1.4592
2022-03-10 05:12:03 - train: epoch 0199, iter [03700, 05004], lr: 0.000026, loss: 1.3216
2022-03-10 05:12:38 - train: epoch 0199, iter [03800, 05004], lr: 0.000026, loss: 1.7429
2022-03-10 05:13:12 - train: epoch 0199, iter [03900, 05004], lr: 0.000026, loss: 1.7891
2022-03-10 05:13:46 - train: epoch 0199, iter [04000, 05004], lr: 0.000026, loss: 1.8572
2022-03-10 05:14:20 - train: epoch 0199, iter [04100, 05004], lr: 0.000026, loss: 1.9753
2022-03-10 05:14:54 - train: epoch 0199, iter [04200, 05004], lr: 0.000026, loss: 1.6478
2022-03-10 05:15:29 - train: epoch 0199, iter [04300, 05004], lr: 0.000026, loss: 1.6005
2022-03-10 05:16:03 - train: epoch 0199, iter [04400, 05004], lr: 0.000026, loss: 1.8970
2022-03-10 05:16:36 - train: epoch 0199, iter [04500, 05004], lr: 0.000026, loss: 1.8006
2022-03-10 05:17:11 - train: epoch 0199, iter [04600, 05004], lr: 0.000026, loss: 1.6294
2022-03-10 05:17:45 - train: epoch 0199, iter [04700, 05004], lr: 0.000026, loss: 1.7262
2022-03-10 05:18:19 - train: epoch 0199, iter [04800, 05004], lr: 0.000026, loss: 1.7884
2022-03-10 05:18:54 - train: epoch 0199, iter [04900, 05004], lr: 0.000026, loss: 1.6064
2022-03-10 05:19:27 - train: epoch 0199, iter [05000, 05004], lr: 0.000026, loss: 1.6470
2022-03-10 05:19:28 - train: epoch 199, train_loss: 1.7227
2022-03-10 05:20:43 - eval: epoch: 199, acc1: 77.582%, acc5: 93.578%, test_loss: 0.8858, per_image_load_time: 2.352ms, per_image_inference_time: 0.524ms
2022-03-10 05:20:43 - until epoch: 199, best_acc1: 77.618%
2022-03-10 05:20:43 - epoch 200 lr: 6.488751431266149e-06
2022-03-10 05:21:23 - train: epoch 0200, iter [00100, 05004], lr: 0.000006, loss: 1.7415
2022-03-10 05:21:57 - train: epoch 0200, iter [00200, 05004], lr: 0.000006, loss: 1.8013
2022-03-10 05:22:32 - train: epoch 0200, iter [00300, 05004], lr: 0.000006, loss: 1.6172
2022-03-10 05:23:06 - train: epoch 0200, iter [00400, 05004], lr: 0.000006, loss: 1.7622
2022-03-10 05:23:41 - train: epoch 0200, iter [00500, 05004], lr: 0.000006, loss: 1.5024
2022-03-10 05:24:14 - train: epoch 0200, iter [00600, 05004], lr: 0.000006, loss: 1.4527
2022-03-10 05:24:49 - train: epoch 0200, iter [00700, 05004], lr: 0.000006, loss: 1.6707
2022-03-10 05:25:23 - train: epoch 0200, iter [00800, 05004], lr: 0.000006, loss: 1.9006
2022-03-10 05:25:58 - train: epoch 0200, iter [00900, 05004], lr: 0.000006, loss: 1.5795
2022-03-10 05:26:32 - train: epoch 0200, iter [01000, 05004], lr: 0.000006, loss: 1.6073
2022-03-10 05:27:05 - train: epoch 0200, iter [01100, 05004], lr: 0.000006, loss: 1.5032
2022-03-10 05:27:40 - train: epoch 0200, iter [01200, 05004], lr: 0.000006, loss: 1.5611
2022-03-10 05:28:14 - train: epoch 0200, iter [01300, 05004], lr: 0.000006, loss: 1.8222
2022-03-10 05:28:48 - train: epoch 0200, iter [01400, 05004], lr: 0.000006, loss: 1.4644
2022-03-10 05:29:22 - train: epoch 0200, iter [01500, 05004], lr: 0.000006, loss: 1.6756
2022-03-10 05:29:56 - train: epoch 0200, iter [01600, 05004], lr: 0.000006, loss: 1.6688
2022-03-10 05:30:30 - train: epoch 0200, iter [01700, 05004], lr: 0.000006, loss: 2.0048
2022-03-10 05:31:05 - train: epoch 0200, iter [01800, 05004], lr: 0.000006, loss: 1.8015
2022-03-10 05:31:39 - train: epoch 0200, iter [01900, 05004], lr: 0.000006, loss: 1.5342
2022-03-10 05:32:14 - train: epoch 0200, iter [02000, 05004], lr: 0.000006, loss: 1.7773
2022-03-10 05:32:48 - train: epoch 0200, iter [02100, 05004], lr: 0.000006, loss: 1.9748
2022-03-10 05:33:22 - train: epoch 0200, iter [02200, 05004], lr: 0.000006, loss: 1.9278
2022-03-10 05:33:58 - train: epoch 0200, iter [02300, 05004], lr: 0.000006, loss: 1.8847
2022-03-10 05:34:31 - train: epoch 0200, iter [02400, 05004], lr: 0.000006, loss: 1.8126
2022-03-10 05:35:06 - train: epoch 0200, iter [02500, 05004], lr: 0.000006, loss: 1.5542
2022-03-10 05:35:40 - train: epoch 0200, iter [02600, 05004], lr: 0.000006, loss: 1.7460
2022-03-10 05:36:14 - train: epoch 0200, iter [02700, 05004], lr: 0.000006, loss: 1.7203
2022-03-10 05:36:49 - train: epoch 0200, iter [02800, 05004], lr: 0.000006, loss: 1.8104
2022-03-10 05:37:23 - train: epoch 0200, iter [02900, 05004], lr: 0.000006, loss: 1.6797
2022-03-10 05:37:57 - train: epoch 0200, iter [03000, 05004], lr: 0.000006, loss: 1.6851
2022-03-10 05:38:32 - train: epoch 0200, iter [03100, 05004], lr: 0.000006, loss: 1.5318
2022-03-10 05:39:06 - train: epoch 0200, iter [03200, 05004], lr: 0.000006, loss: 1.6667
2022-03-10 05:39:41 - train: epoch 0200, iter [03300, 05004], lr: 0.000006, loss: 2.0390
2022-03-10 05:40:14 - train: epoch 0200, iter [03400, 05004], lr: 0.000006, loss: 1.5036
2022-03-10 05:40:49 - train: epoch 0200, iter [03500, 05004], lr: 0.000006, loss: 1.7855
2022-03-10 05:41:22 - train: epoch 0200, iter [03600, 05004], lr: 0.000006, loss: 1.7117
2022-03-10 05:41:57 - train: epoch 0200, iter [03700, 05004], lr: 0.000006, loss: 1.8824
2022-03-10 05:42:31 - train: epoch 0200, iter [03800, 05004], lr: 0.000006, loss: 1.8488
2022-03-10 05:43:05 - train: epoch 0200, iter [03900, 05004], lr: 0.000006, loss: 1.5519
2022-03-10 05:43:39 - train: epoch 0200, iter [04000, 05004], lr: 0.000006, loss: 1.6444
2022-03-10 05:44:14 - train: epoch 0200, iter [04100, 05004], lr: 0.000006, loss: 1.8512
2022-03-10 05:44:47 - train: epoch 0200, iter [04200, 05004], lr: 0.000006, loss: 1.3448
2022-03-10 05:45:22 - train: epoch 0200, iter [04300, 05004], lr: 0.000006, loss: 1.6303
2022-03-10 05:45:56 - train: epoch 0200, iter [04400, 05004], lr: 0.000006, loss: 1.6273
2022-03-10 05:46:30 - train: epoch 0200, iter [04500, 05004], lr: 0.000006, loss: 1.9152
2022-03-10 05:47:04 - train: epoch 0200, iter [04600, 05004], lr: 0.000006, loss: 1.6630
2022-03-10 05:47:38 - train: epoch 0200, iter [04700, 05004], lr: 0.000006, loss: 1.4134
2022-03-10 05:48:13 - train: epoch 0200, iter [04800, 05004], lr: 0.000006, loss: 1.8368
2022-03-10 05:48:47 - train: epoch 0200, iter [04900, 05004], lr: 0.000006, loss: 1.7779
2022-03-10 05:49:19 - train: epoch 0200, iter [05000, 05004], lr: 0.000006, loss: 1.8326
2022-03-10 05:49:21 - train: epoch 200, train_loss: 1.7247
2022-03-10 05:50:36 - eval: epoch: 200, acc1: 77.510%, acc5: 93.588%, test_loss: 0.8868, per_image_load_time: 2.350ms, per_image_inference_time: 0.548ms
2022-03-10 05:50:36 - until epoch: 200, best_acc1: 77.618%
2022-03-10 05:50:36 - train done. model: resnet50, train time: 97.890 hours, best_acc1: 77.618%
