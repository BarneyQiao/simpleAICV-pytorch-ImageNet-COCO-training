2022-03-06 13:18:57 - train: epoch 0050, iter [04500, 05004], lr: 0.087955, loss: 3.0859
2022-03-06 13:19:30 - train: epoch 0050, iter [04600, 05004], lr: 0.087955, loss: 2.8967
2022-03-06 13:20:01 - train: epoch 0050, iter [04700, 05004], lr: 0.087955, loss: 3.0404
2022-03-06 13:20:35 - train: epoch 0050, iter [04800, 05004], lr: 0.087955, loss: 2.8293
2022-03-06 13:21:06 - train: epoch 0050, iter [04900, 05004], lr: 0.087955, loss: 2.9809
2022-03-06 13:21:38 - train: epoch 0050, iter [05000, 05004], lr: 0.087955, loss: 2.6631
2022-03-06 13:21:39 - train: epoch 050, train_loss: 3.0038
2022-03-06 13:22:51 - eval: epoch: 050, acc1: 52.162%, acc5: 77.908%, test_loss: 2.0410, per_image_load_time: 2.206ms, per_image_inference_time: 0.518ms
2022-03-06 13:22:52 - until epoch: 050, best_acc1: 53.588%
2022-03-06 13:22:52 - epoch 051 lr: 0.08742553740855506
2022-03-06 13:23:31 - train: epoch 0051, iter [00100, 05004], lr: 0.087426, loss: 3.4584
2022-03-06 13:24:05 - train: epoch 0051, iter [00200, 05004], lr: 0.087426, loss: 3.3655
2022-03-06 13:24:38 - train: epoch 0051, iter [00300, 05004], lr: 0.087426, loss: 3.2058
2022-03-06 13:25:12 - train: epoch 0051, iter [00400, 05004], lr: 0.087426, loss: 2.6219
2022-03-06 13:25:46 - train: epoch 0051, iter [00500, 05004], lr: 0.087426, loss: 3.0081
2022-03-06 13:26:18 - train: epoch 0051, iter [00600, 05004], lr: 0.087426, loss: 2.9005
2022-03-06 13:26:51 - train: epoch 0051, iter [00700, 05004], lr: 0.087426, loss: 3.0720
2022-03-06 13:27:23 - train: epoch 0051, iter [00800, 05004], lr: 0.087426, loss: 3.2062
2022-03-06 13:27:56 - train: epoch 0051, iter [00900, 05004], lr: 0.087426, loss: 2.8347
2022-03-06 13:28:28 - train: epoch 0051, iter [01000, 05004], lr: 0.087426, loss: 3.1395
2022-03-06 13:29:02 - train: epoch 0051, iter [01100, 05004], lr: 0.087426, loss: 3.0268
2022-03-06 13:29:34 - train: epoch 0051, iter [01200, 05004], lr: 0.087426, loss: 2.8786
2022-03-06 13:30:07 - train: epoch 0051, iter [01300, 05004], lr: 0.087426, loss: 2.7812
2022-03-06 13:30:40 - train: epoch 0051, iter [01400, 05004], lr: 0.087426, loss: 3.0802
2022-03-06 13:31:14 - train: epoch 0051, iter [01500, 05004], lr: 0.087426, loss: 2.9527
2022-03-06 13:31:48 - train: epoch 0051, iter [01600, 05004], lr: 0.087426, loss: 3.0066
2022-03-06 13:32:22 - train: epoch 0051, iter [01700, 05004], lr: 0.087426, loss: 3.1285
2022-03-06 13:32:56 - train: epoch 0051, iter [01800, 05004], lr: 0.087426, loss: 2.8815
2022-03-06 13:33:29 - train: epoch 0051, iter [01900, 05004], lr: 0.087426, loss: 3.0932
2022-03-06 13:34:02 - train: epoch 0051, iter [02000, 05004], lr: 0.087426, loss: 2.9790
2022-03-06 13:34:35 - train: epoch 0051, iter [02100, 05004], lr: 0.087426, loss: 3.0441
2022-03-06 13:35:07 - train: epoch 0051, iter [02200, 05004], lr: 0.087426, loss: 3.0365
2022-03-06 13:35:40 - train: epoch 0051, iter [02300, 05004], lr: 0.087426, loss: 3.4635
2022-03-06 13:36:12 - train: epoch 0051, iter [02400, 05004], lr: 0.087426, loss: 3.0769
2022-03-06 13:36:45 - train: epoch 0051, iter [02500, 05004], lr: 0.087426, loss: 2.9513
2022-03-06 13:37:18 - train: epoch 0051, iter [02600, 05004], lr: 0.087426, loss: 2.9350
2022-03-06 13:37:50 - train: epoch 0051, iter [02700, 05004], lr: 0.087426, loss: 3.0251
2022-03-06 13:38:23 - train: epoch 0051, iter [02800, 05004], lr: 0.087426, loss: 2.9390
2022-03-06 13:38:58 - train: epoch 0051, iter [02900, 05004], lr: 0.087426, loss: 3.1623
2022-03-06 13:39:32 - train: epoch 0051, iter [03000, 05004], lr: 0.087426, loss: 2.8755
2022-03-06 13:40:06 - train: epoch 0051, iter [03100, 05004], lr: 0.087426, loss: 2.9114
2022-03-06 13:40:40 - train: epoch 0051, iter [03200, 05004], lr: 0.087426, loss: 3.1295
2022-03-06 13:41:14 - train: epoch 0051, iter [03300, 05004], lr: 0.087426, loss: 3.1567
2022-03-06 13:41:46 - train: epoch 0051, iter [03400, 05004], lr: 0.087426, loss: 2.7254
2022-03-06 13:42:19 - train: epoch 0051, iter [03500, 05004], lr: 0.087426, loss: 2.9200
2022-03-06 13:42:51 - train: epoch 0051, iter [03600, 05004], lr: 0.087426, loss: 3.0196
2022-03-06 13:43:24 - train: epoch 0051, iter [03700, 05004], lr: 0.087426, loss: 2.9392
2022-03-06 13:43:57 - train: epoch 0051, iter [03800, 05004], lr: 0.087426, loss: 3.0325
2022-03-06 13:44:29 - train: epoch 0051, iter [03900, 05004], lr: 0.087426, loss: 3.0896
2022-03-06 13:45:02 - train: epoch 0051, iter [04000, 05004], lr: 0.087426, loss: 3.0706
2022-03-06 13:45:35 - train: epoch 0051, iter [04100, 05004], lr: 0.087426, loss: 3.4062
2022-03-06 13:46:07 - train: epoch 0051, iter [04200, 05004], lr: 0.087426, loss: 3.2331
2022-03-06 13:46:41 - train: epoch 0051, iter [04300, 05004], lr: 0.087426, loss: 3.0789
2022-03-06 13:47:15 - train: epoch 0051, iter [04400, 05004], lr: 0.087426, loss: 2.9906
2022-03-06 13:47:49 - train: epoch 0051, iter [04500, 05004], lr: 0.087426, loss: 2.8631
2022-03-06 13:48:23 - train: epoch 0051, iter [04600, 05004], lr: 0.087426, loss: 3.1255
2022-03-06 13:48:57 - train: epoch 0051, iter [04700, 05004], lr: 0.087426, loss: 2.9938
2022-03-06 13:49:30 - train: epoch 0051, iter [04800, 05004], lr: 0.087426, loss: 3.0718
2022-03-06 13:50:03 - train: epoch 0051, iter [04900, 05004], lr: 0.087426, loss: 3.0721
2022-03-06 13:50:33 - train: epoch 0051, iter [05000, 05004], lr: 0.087426, loss: 2.8747
2022-03-06 13:50:35 - train: epoch 051, train_loss: 3.0050
2022-03-06 13:51:46 - eval: epoch: 051, acc1: 54.264%, acc5: 79.668%, test_loss: 1.9376, per_image_load_time: 2.286ms, per_image_inference_time: 0.477ms
2022-03-06 13:51:47 - until epoch: 051, best_acc1: 54.264%
2022-03-06 13:51:47 - epoch 052 lr: 0.08688653405904652
2022-03-06 13:52:25 - train: epoch 0052, iter [00100, 05004], lr: 0.086887, loss: 2.6254
2022-03-06 13:52:57 - train: epoch 0052, iter [00200, 05004], lr: 0.086887, loss: 2.9121
2022-03-06 13:53:30 - train: epoch 0052, iter [00300, 05004], lr: 0.086887, loss: 2.8800
2022-03-06 13:54:01 - train: epoch 0052, iter [00400, 05004], lr: 0.086887, loss: 3.0391
2022-03-06 13:54:35 - train: epoch 0052, iter [00500, 05004], lr: 0.086887, loss: 3.1145
2022-03-06 13:55:10 - train: epoch 0052, iter [00600, 05004], lr: 0.086887, loss: 3.0248
2022-03-06 13:55:44 - train: epoch 0052, iter [00700, 05004], lr: 0.086887, loss: 3.0378
2022-03-06 13:56:17 - train: epoch 0052, iter [00800, 05004], lr: 0.086887, loss: 2.9464
2022-03-06 13:56:51 - train: epoch 0052, iter [00900, 05004], lr: 0.086887, loss: 3.2473
2022-03-06 13:57:24 - train: epoch 0052, iter [01000, 05004], lr: 0.086887, loss: 3.3184
2022-03-06 13:57:57 - train: epoch 0052, iter [01100, 05004], lr: 0.086887, loss: 2.9047
2022-03-06 13:58:29 - train: epoch 0052, iter [01200, 05004], lr: 0.086887, loss: 2.8357
2022-03-06 13:59:02 - train: epoch 0052, iter [01300, 05004], lr: 0.086887, loss: 2.9097
2022-03-06 13:59:35 - train: epoch 0052, iter [01400, 05004], lr: 0.086887, loss: 3.1408
2022-03-06 14:00:08 - train: epoch 0052, iter [01500, 05004], lr: 0.086887, loss: 2.9075
2022-03-06 14:00:40 - train: epoch 0052, iter [01600, 05004], lr: 0.086887, loss: 2.7761
2022-03-06 14:01:13 - train: epoch 0052, iter [01700, 05004], lr: 0.086887, loss: 2.6855
2022-03-06 14:01:45 - train: epoch 0052, iter [01800, 05004], lr: 0.086887, loss: 2.8507
2022-03-06 14:02:19 - train: epoch 0052, iter [01900, 05004], lr: 0.086887, loss: 2.9591
2022-03-06 14:02:53 - train: epoch 0052, iter [02000, 05004], lr: 0.086887, loss: 3.2087
2022-03-06 14:03:26 - train: epoch 0052, iter [02100, 05004], lr: 0.086887, loss: 2.9902
2022-03-06 14:04:01 - train: epoch 0052, iter [02200, 05004], lr: 0.086887, loss: 2.9098
2022-03-06 14:04:34 - train: epoch 0052, iter [02300, 05004], lr: 0.086887, loss: 2.8360
2022-03-06 14:05:07 - train: epoch 0052, iter [02400, 05004], lr: 0.086887, loss: 3.0644
2022-03-06 14:05:39 - train: epoch 0052, iter [02500, 05004], lr: 0.086887, loss: 2.6373
2022-03-06 14:06:13 - train: epoch 0052, iter [02600, 05004], lr: 0.086887, loss: 2.6466
2022-03-06 14:06:45 - train: epoch 0052, iter [02700, 05004], lr: 0.086887, loss: 2.8785
2022-03-06 14:07:18 - train: epoch 0052, iter [02800, 05004], lr: 0.086887, loss: 3.1301
2022-03-06 14:07:50 - train: epoch 0052, iter [02900, 05004], lr: 0.086887, loss: 2.8217
2022-03-06 14:08:23 - train: epoch 0052, iter [03000, 05004], lr: 0.086887, loss: 2.9056
2022-03-06 14:08:56 - train: epoch 0052, iter [03100, 05004], lr: 0.086887, loss: 3.0119
2022-03-06 14:09:28 - train: epoch 0052, iter [03200, 05004], lr: 0.086887, loss: 2.9599
2022-03-06 14:10:01 - train: epoch 0052, iter [03300, 05004], lr: 0.086887, loss: 3.0208
2022-03-06 14:10:36 - train: epoch 0052, iter [03400, 05004], lr: 0.086887, loss: 3.0726
2022-03-06 14:11:09 - train: epoch 0052, iter [03500, 05004], lr: 0.086887, loss: 2.9951
2022-03-06 14:11:43 - train: epoch 0052, iter [03600, 05004], lr: 0.086887, loss: 3.0597
2022-03-06 14:12:17 - train: epoch 0052, iter [03700, 05004], lr: 0.086887, loss: 3.0610
2022-03-06 14:12:50 - train: epoch 0052, iter [03800, 05004], lr: 0.086887, loss: 3.2816
2022-03-06 14:13:23 - train: epoch 0052, iter [03900, 05004], lr: 0.086887, loss: 3.0334
2022-03-06 14:13:55 - train: epoch 0052, iter [04000, 05004], lr: 0.086887, loss: 3.0158
2022-03-06 14:14:28 - train: epoch 0052, iter [04100, 05004], lr: 0.086887, loss: 3.1446
2022-03-06 14:15:00 - train: epoch 0052, iter [04200, 05004], lr: 0.086887, loss: 2.7458
2022-03-06 14:15:33 - train: epoch 0052, iter [04300, 05004], lr: 0.086887, loss: 3.1007
2022-03-06 14:16:06 - train: epoch 0052, iter [04400, 05004], lr: 0.086887, loss: 3.2555
2022-03-06 14:16:38 - train: epoch 0052, iter [04500, 05004], lr: 0.086887, loss: 3.0544
2022-03-06 14:17:10 - train: epoch 0052, iter [04600, 05004], lr: 0.086887, loss: 2.9922
2022-03-06 14:17:43 - train: epoch 0052, iter [04700, 05004], lr: 0.086887, loss: 3.0227
2022-03-06 14:18:17 - train: epoch 0052, iter [04800, 05004], lr: 0.086887, loss: 2.7818
2022-03-06 14:18:50 - train: epoch 0052, iter [04900, 05004], lr: 0.086887, loss: 2.9286
2022-03-06 14:19:23 - train: epoch 0052, iter [05000, 05004], lr: 0.086887, loss: 3.0829
2022-03-06 14:19:24 - train: epoch 052, train_loss: 2.9949
2022-03-06 14:20:38 - eval: epoch: 052, acc1: 52.030%, acc5: 77.632%, test_loss: 2.0534, per_image_load_time: 2.395ms, per_image_inference_time: 0.477ms
2022-03-06 14:20:39 - until epoch: 052, best_acc1: 54.264%
2022-03-06 14:20:39 - epoch 053 lr: 0.08633795680751116
2022-03-06 14:21:16 - train: epoch 0053, iter [00100, 05004], lr: 0.086338, loss: 2.9440
2022-03-06 14:21:49 - train: epoch 0053, iter [00200, 05004], lr: 0.086338, loss: 3.2721
2022-03-06 14:22:22 - train: epoch 0053, iter [00300, 05004], lr: 0.086338, loss: 2.9600
2022-03-06 14:22:54 - train: epoch 0053, iter [00400, 05004], lr: 0.086338, loss: 3.2983
2022-03-06 14:23:26 - train: epoch 0053, iter [00500, 05004], lr: 0.086338, loss: 3.1993
2022-03-06 14:23:59 - train: epoch 0053, iter [00600, 05004], lr: 0.086338, loss: 3.1656
2022-03-06 14:24:32 - train: epoch 0053, iter [00700, 05004], lr: 0.086338, loss: 2.8435
2022-03-06 14:25:04 - train: epoch 0053, iter [00800, 05004], lr: 0.086338, loss: 3.0953
2022-03-06 14:25:38 - train: epoch 0053, iter [00900, 05004], lr: 0.086338, loss: 2.8550
2022-03-06 14:26:11 - train: epoch 0053, iter [01000, 05004], lr: 0.086338, loss: 3.1373
2022-03-06 14:26:46 - train: epoch 0053, iter [01100, 05004], lr: 0.086338, loss: 2.9033
2022-03-06 14:27:20 - train: epoch 0053, iter [01200, 05004], lr: 0.086338, loss: 2.9165
2022-03-06 14:27:53 - train: epoch 0053, iter [01300, 05004], lr: 0.086338, loss: 3.0089
2022-03-06 14:28:27 - train: epoch 0053, iter [01400, 05004], lr: 0.086338, loss: 3.1277
2022-03-06 14:29:00 - train: epoch 0053, iter [01500, 05004], lr: 0.086338, loss: 2.9494
2022-03-06 14:29:33 - train: epoch 0053, iter [01600, 05004], lr: 0.086338, loss: 3.2475
2022-03-06 14:30:05 - train: epoch 0053, iter [01700, 05004], lr: 0.086338, loss: 3.1988
2022-03-06 14:30:38 - train: epoch 0053, iter [01800, 05004], lr: 0.086338, loss: 2.9934
2022-03-06 14:31:11 - train: epoch 0053, iter [01900, 05004], lr: 0.086338, loss: 2.8701
2022-03-06 14:31:43 - train: epoch 0053, iter [02000, 05004], lr: 0.086338, loss: 2.9813
2022-03-06 14:32:16 - train: epoch 0053, iter [02100, 05004], lr: 0.086338, loss: 3.0328
2022-03-06 14:32:49 - train: epoch 0053, iter [02200, 05004], lr: 0.086338, loss: 2.8484
2022-03-06 14:33:22 - train: epoch 0053, iter [02300, 05004], lr: 0.086338, loss: 2.8252
2022-03-06 14:33:56 - train: epoch 0053, iter [02400, 05004], lr: 0.086338, loss: 3.1200
2022-03-06 14:34:30 - train: epoch 0053, iter [02500, 05004], lr: 0.086338, loss: 3.1298
2022-03-06 14:35:04 - train: epoch 0053, iter [02600, 05004], lr: 0.086338, loss: 3.0626
2022-03-06 14:35:38 - train: epoch 0053, iter [02700, 05004], lr: 0.086338, loss: 3.1593
2022-03-06 14:36:10 - train: epoch 0053, iter [02800, 05004], lr: 0.086338, loss: 3.1107
2022-03-06 14:36:44 - train: epoch 0053, iter [02900, 05004], lr: 0.086338, loss: 2.5683
2022-03-06 14:37:16 - train: epoch 0053, iter [03000, 05004], lr: 0.086338, loss: 2.9003
2022-03-06 14:37:49 - train: epoch 0053, iter [03100, 05004], lr: 0.086338, loss: 3.1575
2022-03-06 14:38:21 - train: epoch 0053, iter [03200, 05004], lr: 0.086338, loss: 3.3127
2022-03-06 14:38:55 - train: epoch 0053, iter [03300, 05004], lr: 0.086338, loss: 3.0195
2022-03-06 14:39:27 - train: epoch 0053, iter [03400, 05004], lr: 0.086338, loss: 3.1114
2022-03-06 14:40:00 - train: epoch 0053, iter [03500, 05004], lr: 0.086338, loss: 2.7695
2022-03-06 14:40:32 - train: epoch 0053, iter [03600, 05004], lr: 0.086338, loss: 2.7826
2022-03-06 14:41:06 - train: epoch 0053, iter [03700, 05004], lr: 0.086338, loss: 3.0592
2022-03-06 14:41:40 - train: epoch 0053, iter [03800, 05004], lr: 0.086338, loss: 2.8873
2022-03-06 14:42:14 - train: epoch 0053, iter [03900, 05004], lr: 0.086338, loss: 3.0077
2022-03-06 14:42:48 - train: epoch 0053, iter [04000, 05004], lr: 0.086338, loss: 2.8059
2022-03-06 14:43:21 - train: epoch 0053, iter [04100, 05004], lr: 0.086338, loss: 3.2220
2022-03-06 14:43:55 - train: epoch 0053, iter [04200, 05004], lr: 0.086338, loss: 2.7986
2022-03-06 14:44:27 - train: epoch 0053, iter [04300, 05004], lr: 0.086338, loss: 3.2814
2022-03-06 14:45:00 - train: epoch 0053, iter [04400, 05004], lr: 0.086338, loss: 2.9801
2022-03-06 14:45:33 - train: epoch 0053, iter [04500, 05004], lr: 0.086338, loss: 3.2326
2022-03-06 14:46:06 - train: epoch 0053, iter [04600, 05004], lr: 0.086338, loss: 2.8558
2022-03-06 14:46:39 - train: epoch 0053, iter [04700, 05004], lr: 0.086338, loss: 3.1855
2022-03-06 14:47:11 - train: epoch 0053, iter [04800, 05004], lr: 0.086338, loss: 2.9602
2022-03-06 14:47:44 - train: epoch 0053, iter [04900, 05004], lr: 0.086338, loss: 2.9861
2022-03-06 14:48:15 - train: epoch 0053, iter [05000, 05004], lr: 0.086338, loss: 2.8963
2022-03-06 14:48:16 - train: epoch 053, train_loss: 2.9889
2022-03-06 14:49:29 - eval: epoch: 053, acc1: 52.316%, acc5: 78.344%, test_loss: 2.0349, per_image_load_time: 2.343ms, per_image_inference_time: 0.469ms
2022-03-06 14:49:30 - until epoch: 053, best_acc1: 54.264%
2022-03-06 14:49:30 - epoch 054 lr: 0.08577994803720607
2022-03-06 14:50:09 - train: epoch 0054, iter [00100, 05004], lr: 0.085780, loss: 2.6148
2022-03-06 14:50:43 - train: epoch 0054, iter [00200, 05004], lr: 0.085780, loss: 3.1152
2022-03-06 14:51:16 - train: epoch 0054, iter [00300, 05004], lr: 0.085780, loss: 3.0389
2022-03-06 14:51:49 - train: epoch 0054, iter [00400, 05004], lr: 0.085780, loss: 2.8510
2022-03-06 14:52:23 - train: epoch 0054, iter [00500, 05004], lr: 0.085780, loss: 3.1519
2022-03-06 14:52:55 - train: epoch 0054, iter [00600, 05004], lr: 0.085780, loss: 2.6067
2022-03-06 14:53:28 - train: epoch 0054, iter [00700, 05004], lr: 0.085780, loss: 3.0549
2022-03-06 14:54:01 - train: epoch 0054, iter [00800, 05004], lr: 0.085780, loss: 3.1421
2022-03-06 14:54:34 - train: epoch 0054, iter [00900, 05004], lr: 0.085780, loss: 2.8817
2022-03-06 14:55:06 - train: epoch 0054, iter [01000, 05004], lr: 0.085780, loss: 2.7892
2022-03-06 14:55:39 - train: epoch 0054, iter [01100, 05004], lr: 0.085780, loss: 2.9510
2022-03-06 14:56:12 - train: epoch 0054, iter [01200, 05004], lr: 0.085780, loss: 3.0181
2022-03-06 14:56:45 - train: epoch 0054, iter [01300, 05004], lr: 0.085780, loss: 2.7804
2022-03-06 14:57:18 - train: epoch 0054, iter [01400, 05004], lr: 0.085780, loss: 3.2137
2022-03-06 14:57:51 - train: epoch 0054, iter [01500, 05004], lr: 0.085780, loss: 3.0180
2022-03-06 14:58:25 - train: epoch 0054, iter [01600, 05004], lr: 0.085780, loss: 2.8829
2022-03-06 14:59:00 - train: epoch 0054, iter [01700, 05004], lr: 0.085780, loss: 2.9977
2022-03-06 14:59:33 - train: epoch 0054, iter [01800, 05004], lr: 0.085780, loss: 3.1361
2022-03-06 15:00:07 - train: epoch 0054, iter [01900, 05004], lr: 0.085780, loss: 3.2495
2022-03-06 15:00:40 - train: epoch 0054, iter [02000, 05004], lr: 0.085780, loss: 2.9782
2022-03-06 15:01:13 - train: epoch 0054, iter [02100, 05004], lr: 0.085780, loss: 2.7577
2022-03-06 15:01:45 - train: epoch 0054, iter [02200, 05004], lr: 0.085780, loss: 3.1641
2022-03-06 15:02:18 - train: epoch 0054, iter [02300, 05004], lr: 0.085780, loss: 2.8937
2022-03-06 15:02:51 - train: epoch 0054, iter [02400, 05004], lr: 0.085780, loss: 2.8730
2022-03-06 15:03:24 - train: epoch 0054, iter [02500, 05004], lr: 0.085780, loss: 3.0694
2022-03-06 15:03:56 - train: epoch 0054, iter [02600, 05004], lr: 0.085780, loss: 3.1224
2022-03-06 15:04:30 - train: epoch 0054, iter [02700, 05004], lr: 0.085780, loss: 3.0386
2022-03-06 15:05:02 - train: epoch 0054, iter [02800, 05004], lr: 0.085780, loss: 3.3550
2022-03-06 15:05:36 - train: epoch 0054, iter [02900, 05004], lr: 0.085780, loss: 2.8495
2022-03-06 15:06:10 - train: epoch 0054, iter [03000, 05004], lr: 0.085780, loss: 2.8441
2022-03-06 15:06:43 - train: epoch 0054, iter [03100, 05004], lr: 0.085780, loss: 2.9284
2022-03-06 15:07:17 - train: epoch 0054, iter [03200, 05004], lr: 0.085780, loss: 3.2582
2022-03-06 15:07:51 - train: epoch 0054, iter [03300, 05004], lr: 0.085780, loss: 2.9619
2022-03-06 15:08:24 - train: epoch 0054, iter [03400, 05004], lr: 0.085780, loss: 3.0215
2022-03-06 15:08:56 - train: epoch 0054, iter [03500, 05004], lr: 0.085780, loss: 3.2766
2022-03-06 15:09:29 - train: epoch 0054, iter [03600, 05004], lr: 0.085780, loss: 2.9686
2022-03-06 15:10:01 - train: epoch 0054, iter [03700, 05004], lr: 0.085780, loss: 2.8669
2022-03-06 15:10:34 - train: epoch 0054, iter [03800, 05004], lr: 0.085780, loss: 3.1212
2022-03-06 15:11:07 - train: epoch 0054, iter [03900, 05004], lr: 0.085780, loss: 3.0096
2022-03-06 15:11:39 - train: epoch 0054, iter [04000, 05004], lr: 0.085780, loss: 3.0165
2022-03-06 15:12:12 - train: epoch 0054, iter [04100, 05004], lr: 0.085780, loss: 2.9655
2022-03-06 15:12:45 - train: epoch 0054, iter [04200, 05004], lr: 0.085780, loss: 3.0422
2022-03-06 15:13:18 - train: epoch 0054, iter [04300, 05004], lr: 0.085780, loss: 3.0983
2022-03-06 15:13:52 - train: epoch 0054, iter [04400, 05004], lr: 0.085780, loss: 2.8274
2022-03-06 15:14:25 - train: epoch 0054, iter [04500, 05004], lr: 0.085780, loss: 2.7519
2022-03-06 15:14:59 - train: epoch 0054, iter [04600, 05004], lr: 0.085780, loss: 2.9215
2022-03-06 15:15:33 - train: epoch 0054, iter [04700, 05004], lr: 0.085780, loss: 3.4538
2022-03-06 15:16:05 - train: epoch 0054, iter [04800, 05004], lr: 0.085780, loss: 3.1749
2022-03-06 15:16:38 - train: epoch 0054, iter [04900, 05004], lr: 0.085780, loss: 2.9391
2022-03-06 15:17:10 - train: epoch 0054, iter [05000, 05004], lr: 0.085780, loss: 3.1823
2022-03-06 15:17:11 - train: epoch 054, train_loss: 2.9932
2022-03-06 15:18:22 - eval: epoch: 054, acc1: 52.142%, acc5: 77.984%, test_loss: 2.0551, per_image_load_time: 2.218ms, per_image_inference_time: 0.463ms
2022-03-06 15:18:23 - until epoch: 054, best_acc1: 54.264%
2022-03-06 15:18:23 - epoch 055 lr: 0.08521265257933948
2022-03-06 15:19:01 - train: epoch 0055, iter [00100, 05004], lr: 0.085213, loss: 2.9381
2022-03-06 15:19:34 - train: epoch 0055, iter [00200, 05004], lr: 0.085213, loss: 3.0970
2022-03-06 15:20:07 - train: epoch 0055, iter [00300, 05004], lr: 0.085213, loss: 2.9463
2022-03-06 15:20:40 - train: epoch 0055, iter [00400, 05004], lr: 0.085213, loss: 2.8537
2022-03-06 15:21:13 - train: epoch 0055, iter [00500, 05004], lr: 0.085213, loss: 2.9190
2022-03-06 15:21:47 - train: epoch 0055, iter [00600, 05004], lr: 0.085213, loss: 2.8384
2022-03-06 15:22:20 - train: epoch 0055, iter [00700, 05004], lr: 0.085213, loss: 3.0101
2022-03-06 15:22:54 - train: epoch 0055, iter [00800, 05004], lr: 0.085213, loss: 3.0148
2022-03-06 15:23:27 - train: epoch 0055, iter [00900, 05004], lr: 0.085213, loss: 3.1658
2022-03-06 15:24:01 - train: epoch 0055, iter [01000, 05004], lr: 0.085213, loss: 2.9717
2022-03-06 15:24:33 - train: epoch 0055, iter [01100, 05004], lr: 0.085213, loss: 3.0932
2022-03-06 15:25:05 - train: epoch 0055, iter [01200, 05004], lr: 0.085213, loss: 3.1550
2022-03-06 15:25:38 - train: epoch 0055, iter [01300, 05004], lr: 0.085213, loss: 3.0739
2022-03-06 15:26:10 - train: epoch 0055, iter [01400, 05004], lr: 0.085213, loss: 2.9150
2022-03-06 15:26:43 - train: epoch 0055, iter [01500, 05004], lr: 0.085213, loss: 2.9686
2022-03-06 15:27:16 - train: epoch 0055, iter [01600, 05004], lr: 0.085213, loss: 3.2658
2022-03-06 15:27:48 - train: epoch 0055, iter [01700, 05004], lr: 0.085213, loss: 3.2327
2022-03-06 15:28:21 - train: epoch 0055, iter [01800, 05004], lr: 0.085213, loss: 3.1409
2022-03-06 15:28:54 - train: epoch 0055, iter [01900, 05004], lr: 0.085213, loss: 2.9463
2022-03-06 15:29:28 - train: epoch 0055, iter [02000, 05004], lr: 0.085213, loss: 2.8281
2022-03-06 15:30:01 - train: epoch 0055, iter [02100, 05004], lr: 0.085213, loss: 2.5905
2022-03-06 15:30:36 - train: epoch 0055, iter [02200, 05004], lr: 0.085213, loss: 3.2599
2022-03-06 15:31:09 - train: epoch 0055, iter [02300, 05004], lr: 0.085213, loss: 2.7874
2022-03-06 15:31:42 - train: epoch 0055, iter [02400, 05004], lr: 0.085213, loss: 2.8133
2022-03-06 15:32:15 - train: epoch 0055, iter [02500, 05004], lr: 0.085213, loss: 3.0968
2022-03-06 15:32:48 - train: epoch 0055, iter [02600, 05004], lr: 0.085213, loss: 2.8230
2022-03-06 15:33:20 - train: epoch 0055, iter [02700, 05004], lr: 0.085213, loss: 2.6932
2022-03-06 15:33:53 - train: epoch 0055, iter [02800, 05004], lr: 0.085213, loss: 3.1404
2022-03-06 15:34:25 - train: epoch 0055, iter [02900, 05004], lr: 0.085213, loss: 3.0597
2022-03-06 15:34:57 - train: epoch 0055, iter [03000, 05004], lr: 0.085213, loss: 2.9679
2022-03-06 15:35:30 - train: epoch 0055, iter [03100, 05004], lr: 0.085213, loss: 3.0403
2022-03-06 15:36:02 - train: epoch 0055, iter [03200, 05004], lr: 0.085213, loss: 3.0780
2022-03-06 15:36:36 - train: epoch 0055, iter [03300, 05004], lr: 0.085213, loss: 2.8065
2022-03-06 15:37:10 - train: epoch 0055, iter [03400, 05004], lr: 0.085213, loss: 2.9186
2022-03-06 15:37:44 - train: epoch 0055, iter [03500, 05004], lr: 0.085213, loss: 2.9765
2022-03-06 15:38:17 - train: epoch 0055, iter [03600, 05004], lr: 0.085213, loss: 3.1103
2022-03-06 15:38:51 - train: epoch 0055, iter [03700, 05004], lr: 0.085213, loss: 3.1138
2022-03-06 15:39:25 - train: epoch 0055, iter [03800, 05004], lr: 0.085213, loss: 3.0825
2022-03-06 15:39:57 - train: epoch 0055, iter [03900, 05004], lr: 0.085213, loss: 3.2834
2022-03-06 15:40:30 - train: epoch 0055, iter [04000, 05004], lr: 0.085213, loss: 3.0585
2022-03-06 15:41:02 - train: epoch 0055, iter [04100, 05004], lr: 0.085213, loss: 3.0112
2022-03-06 15:41:34 - train: epoch 0055, iter [04200, 05004], lr: 0.085213, loss: 3.1988
2022-03-06 15:42:07 - train: epoch 0055, iter [04300, 05004], lr: 0.085213, loss: 3.1902
2022-03-06 15:42:40 - train: epoch 0055, iter [04400, 05004], lr: 0.085213, loss: 3.1561
2022-03-06 15:43:13 - train: epoch 0055, iter [04500, 05004], lr: 0.085213, loss: 3.0601
2022-03-06 15:43:45 - train: epoch 0055, iter [04600, 05004], lr: 0.085213, loss: 3.0829
2022-03-06 15:44:19 - train: epoch 0055, iter [04700, 05004], lr: 0.085213, loss: 2.9194
2022-03-06 15:44:52 - train: epoch 0055, iter [04800, 05004], lr: 0.085213, loss: 2.8519
2022-03-06 15:45:26 - train: epoch 0055, iter [04900, 05004], lr: 0.085213, loss: 2.8837
2022-03-06 15:45:59 - train: epoch 0055, iter [05000, 05004], lr: 0.085213, loss: 3.1390
2022-03-06 15:46:00 - train: epoch 055, train_loss: 2.9872
2022-03-06 15:47:13 - eval: epoch: 055, acc1: 53.534%, acc5: 78.794%, test_loss: 1.9922, per_image_load_time: 2.008ms, per_image_inference_time: 0.527ms
2022-03-06 15:47:14 - until epoch: 055, best_acc1: 54.264%
2022-03-06 15:47:14 - epoch 056 lr: 0.08463621767547998
2022-03-06 15:47:51 - train: epoch 0056, iter [00100, 05004], lr: 0.084636, loss: 2.9846
2022-03-06 15:48:23 - train: epoch 0056, iter [00200, 05004], lr: 0.084636, loss: 2.9999
2022-03-06 15:48:56 - train: epoch 0056, iter [00300, 05004], lr: 0.084636, loss: 2.9474
2022-03-06 15:49:28 - train: epoch 0056, iter [00400, 05004], lr: 0.084636, loss: 2.7655
2022-03-06 15:50:01 - train: epoch 0056, iter [00500, 05004], lr: 0.084636, loss: 2.8651
2022-03-06 15:50:34 - train: epoch 0056, iter [00600, 05004], lr: 0.084636, loss: 3.0025
2022-03-06 15:51:06 - train: epoch 0056, iter [00700, 05004], lr: 0.084636, loss: 2.9098
2022-03-06 15:51:38 - train: epoch 0056, iter [00800, 05004], lr: 0.084636, loss: 3.1827
2022-03-06 15:52:12 - train: epoch 0056, iter [00900, 05004], lr: 0.084636, loss: 3.0224
2022-03-06 15:52:46 - train: epoch 0056, iter [01000, 05004], lr: 0.084636, loss: 2.8648
2022-03-06 15:53:20 - train: epoch 0056, iter [01100, 05004], lr: 0.084636, loss: 2.8422
2022-03-06 15:53:55 - train: epoch 0056, iter [01200, 05004], lr: 0.084636, loss: 2.9959
2022-03-06 15:54:28 - train: epoch 0056, iter [01300, 05004], lr: 0.084636, loss: 2.8286
2022-03-06 15:55:01 - train: epoch 0056, iter [01400, 05004], lr: 0.084636, loss: 2.9658
2022-03-06 15:55:34 - train: epoch 0056, iter [01500, 05004], lr: 0.084636, loss: 3.2816
2022-03-06 15:56:06 - train: epoch 0056, iter [01600, 05004], lr: 0.084636, loss: 2.8497
2022-03-06 15:56:38 - train: epoch 0056, iter [01700, 05004], lr: 0.084636, loss: 3.0960
2022-03-06 15:57:11 - train: epoch 0056, iter [01800, 05004], lr: 0.084636, loss: 3.0351
2022-03-06 15:57:44 - train: epoch 0056, iter [01900, 05004], lr: 0.084636, loss: 3.0591
2022-03-06 15:58:17 - train: epoch 0056, iter [02000, 05004], lr: 0.084636, loss: 2.8346
2022-03-06 15:58:49 - train: epoch 0056, iter [02100, 05004], lr: 0.084636, loss: 3.0027
2022-03-06 15:59:23 - train: epoch 0056, iter [02200, 05004], lr: 0.084636, loss: 3.1014
2022-03-06 15:59:55 - train: epoch 0056, iter [02300, 05004], lr: 0.084636, loss: 3.1127
2022-03-06 16:00:29 - train: epoch 0056, iter [02400, 05004], lr: 0.084636, loss: 2.6722
2022-03-06 16:01:03 - train: epoch 0056, iter [02500, 05004], lr: 0.084636, loss: 2.9037
2022-03-06 16:01:36 - train: epoch 0056, iter [02600, 05004], lr: 0.084636, loss: 2.9385
2022-03-06 16:02:10 - train: epoch 0056, iter [02700, 05004], lr: 0.084636, loss: 3.1541
2022-03-06 16:02:45 - train: epoch 0056, iter [02800, 05004], lr: 0.084636, loss: 2.8518
2022-03-06 16:03:19 - train: epoch 0056, iter [02900, 05004], lr: 0.084636, loss: 3.1045
2022-03-06 16:03:53 - train: epoch 0056, iter [03000, 05004], lr: 0.084636, loss: 3.3724
2022-03-06 16:04:27 - train: epoch 0056, iter [03100, 05004], lr: 0.084636, loss: 2.9734
2022-03-06 16:05:01 - train: epoch 0056, iter [03200, 05004], lr: 0.084636, loss: 2.8562
2022-03-06 16:05:36 - train: epoch 0056, iter [03300, 05004], lr: 0.084636, loss: 3.1941
2022-03-06 16:06:09 - train: epoch 0056, iter [03400, 05004], lr: 0.084636, loss: 2.9752
2022-03-06 16:06:44 - train: epoch 0056, iter [03500, 05004], lr: 0.084636, loss: 3.0756
2022-03-06 16:07:17 - train: epoch 0056, iter [03600, 05004], lr: 0.084636, loss: 2.6581
2022-03-06 16:07:52 - train: epoch 0056, iter [03700, 05004], lr: 0.084636, loss: 2.7980
2022-03-06 16:08:26 - train: epoch 0056, iter [03800, 05004], lr: 0.084636, loss: 2.9918
2022-03-06 16:09:01 - train: epoch 0056, iter [03900, 05004], lr: 0.084636, loss: 3.0855
2022-03-06 16:09:35 - train: epoch 0056, iter [04000, 05004], lr: 0.084636, loss: 3.0441
2022-03-06 16:10:09 - train: epoch 0056, iter [04100, 05004], lr: 0.084636, loss: 3.1015
2022-03-06 16:10:43 - train: epoch 0056, iter [04200, 05004], lr: 0.084636, loss: 2.8105
2022-03-06 16:11:18 - train: epoch 0056, iter [04300, 05004], lr: 0.084636, loss: 2.8979
2022-03-06 16:11:51 - train: epoch 0056, iter [04400, 05004], lr: 0.084636, loss: 2.9449
2022-03-06 16:12:25 - train: epoch 0056, iter [04500, 05004], lr: 0.084636, loss: 2.7493
2022-03-06 16:12:59 - train: epoch 0056, iter [04600, 05004], lr: 0.084636, loss: 2.9502
2022-03-06 16:13:33 - train: epoch 0056, iter [04700, 05004], lr: 0.084636, loss: 2.7016
2022-03-06 16:14:07 - train: epoch 0056, iter [04800, 05004], lr: 0.084636, loss: 3.3679
2022-03-06 16:14:41 - train: epoch 0056, iter [04900, 05004], lr: 0.084636, loss: 2.9626
2022-03-06 16:15:15 - train: epoch 0056, iter [05000, 05004], lr: 0.084636, loss: 3.1680
2022-03-06 16:15:16 - train: epoch 056, train_loss: 2.9780
2022-03-06 16:16:30 - eval: epoch: 056, acc1: 52.444%, acc5: 78.144%, test_loss: 2.0475, per_image_load_time: 1.160ms, per_image_inference_time: 0.540ms
2022-03-06 16:16:31 - until epoch: 056, best_acc1: 54.264%
2022-03-06 16:16:31 - epoch 057 lr: 0.08405079293933987
2022-03-06 16:17:10 - train: epoch 0057, iter [00100, 05004], lr: 0.084051, loss: 3.0183
2022-03-06 16:17:44 - train: epoch 0057, iter [00200, 05004], lr: 0.084051, loss: 3.0449
2022-03-06 16:18:18 - train: epoch 0057, iter [00300, 05004], lr: 0.084051, loss: 2.8375
2022-03-06 16:18:53 - train: epoch 0057, iter [00400, 05004], lr: 0.084051, loss: 3.0781
2022-03-06 16:19:26 - train: epoch 0057, iter [00500, 05004], lr: 0.084051, loss: 2.6946
2022-03-06 16:20:00 - train: epoch 0057, iter [00600, 05004], lr: 0.084051, loss: 3.0804
2022-03-06 16:20:35 - train: epoch 0057, iter [00700, 05004], lr: 0.084051, loss: 2.8065
2022-03-06 16:21:09 - train: epoch 0057, iter [00800, 05004], lr: 0.084051, loss: 2.6699
2022-03-06 16:21:43 - train: epoch 0057, iter [00900, 05004], lr: 0.084051, loss: 2.8391
2022-03-06 16:22:17 - train: epoch 0057, iter [01000, 05004], lr: 0.084051, loss: 2.7651
2022-03-06 16:22:52 - train: epoch 0057, iter [01100, 05004], lr: 0.084051, loss: 3.1246
2022-03-06 16:23:26 - train: epoch 0057, iter [01200, 05004], lr: 0.084051, loss: 3.1183
2022-03-06 16:24:00 - train: epoch 0057, iter [01300, 05004], lr: 0.084051, loss: 2.9940
2022-03-06 16:24:34 - train: epoch 0057, iter [01400, 05004], lr: 0.084051, loss: 2.8478
2022-03-06 16:25:08 - train: epoch 0057, iter [01500, 05004], lr: 0.084051, loss: 3.1148
2022-03-06 16:25:42 - train: epoch 0057, iter [01600, 05004], lr: 0.084051, loss: 3.0826
2022-03-06 16:26:16 - train: epoch 0057, iter [01700, 05004], lr: 0.084051, loss: 3.0395
2022-03-06 16:26:50 - train: epoch 0057, iter [01800, 05004], lr: 0.084051, loss: 3.2428
2022-03-06 16:27:25 - train: epoch 0057, iter [01900, 05004], lr: 0.084051, loss: 2.8270
2022-03-06 16:27:58 - train: epoch 0057, iter [02000, 05004], lr: 0.084051, loss: 3.0171
2022-03-06 16:28:34 - train: epoch 0057, iter [02100, 05004], lr: 0.084051, loss: 3.2805
2022-03-06 16:29:07 - train: epoch 0057, iter [02200, 05004], lr: 0.084051, loss: 3.0325
2022-03-06 16:29:43 - train: epoch 0057, iter [02300, 05004], lr: 0.084051, loss: 3.1681
2022-03-06 16:30:17 - train: epoch 0057, iter [02400, 05004], lr: 0.084051, loss: 2.7333
2022-03-06 16:30:50 - train: epoch 0057, iter [02500, 05004], lr: 0.084051, loss: 3.0209
2022-03-06 16:31:25 - train: epoch 0057, iter [02600, 05004], lr: 0.084051, loss: 2.6648
2022-03-06 16:32:00 - train: epoch 0057, iter [02700, 05004], lr: 0.084051, loss: 2.7449
2022-03-06 16:32:34 - train: epoch 0057, iter [02800, 05004], lr: 0.084051, loss: 2.5835
2022-03-06 16:33:07 - train: epoch 0057, iter [02900, 05004], lr: 0.084051, loss: 3.0253
2022-03-06 16:33:41 - train: epoch 0057, iter [03000, 05004], lr: 0.084051, loss: 3.2532
2022-03-06 16:34:15 - train: epoch 0057, iter [03100, 05004], lr: 0.084051, loss: 3.2298
2022-03-06 16:34:49 - train: epoch 0057, iter [03200, 05004], lr: 0.084051, loss: 3.1976
2022-03-06 16:35:22 - train: epoch 0057, iter [03300, 05004], lr: 0.084051, loss: 2.8930
2022-03-06 16:35:55 - train: epoch 0057, iter [03400, 05004], lr: 0.084051, loss: 3.1677
2022-03-06 16:36:30 - train: epoch 0057, iter [03500, 05004], lr: 0.084051, loss: 3.1608
2022-03-06 16:37:04 - train: epoch 0057, iter [03600, 05004], lr: 0.084051, loss: 2.7870
2022-03-06 16:37:38 - train: epoch 0057, iter [03700, 05004], lr: 0.084051, loss: 2.9532
2022-03-06 16:38:12 - train: epoch 0057, iter [03800, 05004], lr: 0.084051, loss: 3.0680
2022-03-06 16:38:47 - train: epoch 0057, iter [03900, 05004], lr: 0.084051, loss: 3.0867
2022-03-06 16:39:19 - train: epoch 0057, iter [04000, 05004], lr: 0.084051, loss: 2.8335
2022-03-06 16:39:53 - train: epoch 0057, iter [04100, 05004], lr: 0.084051, loss: 2.9551
2022-03-06 16:40:27 - train: epoch 0057, iter [04200, 05004], lr: 0.084051, loss: 2.9173
2022-03-06 16:41:01 - train: epoch 0057, iter [04300, 05004], lr: 0.084051, loss: 2.7537
2022-03-06 16:41:36 - train: epoch 0057, iter [04400, 05004], lr: 0.084051, loss: 2.7842
2022-03-06 16:42:09 - train: epoch 0057, iter [04500, 05004], lr: 0.084051, loss: 3.0709
2022-03-06 16:42:42 - train: epoch 0057, iter [04600, 05004], lr: 0.084051, loss: 3.2435
2022-03-06 16:43:16 - train: epoch 0057, iter [04700, 05004], lr: 0.084051, loss: 3.0478
2022-03-06 16:43:50 - train: epoch 0057, iter [04800, 05004], lr: 0.084051, loss: 3.2461
2022-03-06 16:44:24 - train: epoch 0057, iter [04900, 05004], lr: 0.084051, loss: 3.2775
2022-03-06 16:44:57 - train: epoch 0057, iter [05000, 05004], lr: 0.084051, loss: 3.0829
2022-03-06 16:44:58 - train: epoch 057, train_loss: 2.9799
2022-03-06 16:46:12 - eval: epoch: 057, acc1: 51.630%, acc5: 77.366%, test_loss: 2.0884, per_image_load_time: 2.380ms, per_image_inference_time: 0.495ms
2022-03-06 16:46:13 - until epoch: 057, best_acc1: 54.264%
2022-03-06 16:46:13 - epoch 058 lr: 0.08345653031794292
2022-03-06 16:46:51 - train: epoch 0058, iter [00100, 05004], lr: 0.083457, loss: 2.9568
2022-03-06 16:47:26 - train: epoch 0058, iter [00200, 05004], lr: 0.083457, loss: 2.7618
2022-03-06 16:47:59 - train: epoch 0058, iter [00300, 05004], lr: 0.083457, loss: 3.1063
2022-03-06 16:48:33 - train: epoch 0058, iter [00400, 05004], lr: 0.083457, loss: 2.7779
2022-03-06 16:49:06 - train: epoch 0058, iter [00500, 05004], lr: 0.083457, loss: 2.8486
2022-03-06 16:49:40 - train: epoch 0058, iter [00600, 05004], lr: 0.083457, loss: 3.3184
2022-03-06 16:50:13 - train: epoch 0058, iter [00700, 05004], lr: 0.083457, loss: 2.9745
2022-03-06 16:50:47 - train: epoch 0058, iter [00800, 05004], lr: 0.083457, loss: 2.9977
2022-03-06 16:51:21 - train: epoch 0058, iter [00900, 05004], lr: 0.083457, loss: 2.6405
2022-03-06 16:51:55 - train: epoch 0058, iter [01000, 05004], lr: 0.083457, loss: 2.9721
2022-03-06 16:52:29 - train: epoch 0058, iter [01100, 05004], lr: 0.083457, loss: 2.5552
2022-03-06 16:53:03 - train: epoch 0058, iter [01200, 05004], lr: 0.083457, loss: 2.9614
2022-03-06 16:53:37 - train: epoch 0058, iter [01300, 05004], lr: 0.083457, loss: 3.1893
2022-03-06 16:54:11 - train: epoch 0058, iter [01400, 05004], lr: 0.083457, loss: 3.0134
2022-03-06 16:54:45 - train: epoch 0058, iter [01500, 05004], lr: 0.083457, loss: 2.8496
2022-03-06 16:55:19 - train: epoch 0058, iter [01600, 05004], lr: 0.083457, loss: 2.5668
2022-03-06 16:55:52 - train: epoch 0058, iter [01700, 05004], lr: 0.083457, loss: 2.9977
2022-03-06 16:56:26 - train: epoch 0058, iter [01800, 05004], lr: 0.083457, loss: 3.0719
2022-03-06 16:57:00 - train: epoch 0058, iter [01900, 05004], lr: 0.083457, loss: 3.2033
2022-03-06 16:57:34 - train: epoch 0058, iter [02000, 05004], lr: 0.083457, loss: 3.1874
2022-03-06 16:58:07 - train: epoch 0058, iter [02100, 05004], lr: 0.083457, loss: 2.8044
2022-03-06 16:58:41 - train: epoch 0058, iter [02200, 05004], lr: 0.083457, loss: 2.7387
2022-03-06 16:59:16 - train: epoch 0058, iter [02300, 05004], lr: 0.083457, loss: 2.8248
2022-03-06 16:59:49 - train: epoch 0058, iter [02400, 05004], lr: 0.083457, loss: 3.0157
2022-03-06 17:00:22 - train: epoch 0058, iter [02500, 05004], lr: 0.083457, loss: 3.0905
2022-03-06 17:00:56 - train: epoch 0058, iter [02600, 05004], lr: 0.083457, loss: 2.8906
2022-03-06 17:01:29 - train: epoch 0058, iter [02700, 05004], lr: 0.083457, loss: 2.8579
2022-03-06 17:02:03 - train: epoch 0058, iter [02800, 05004], lr: 0.083457, loss: 2.6915
2022-03-06 17:02:38 - train: epoch 0058, iter [02900, 05004], lr: 0.083457, loss: 2.7755
2022-03-06 17:03:11 - train: epoch 0058, iter [03000, 05004], lr: 0.083457, loss: 3.0658
2022-03-06 17:03:44 - train: epoch 0058, iter [03100, 05004], lr: 0.083457, loss: 3.0423
2022-03-06 17:04:18 - train: epoch 0058, iter [03200, 05004], lr: 0.083457, loss: 2.9127
2022-03-06 17:04:51 - train: epoch 0058, iter [03300, 05004], lr: 0.083457, loss: 3.2149
2022-03-06 17:05:26 - train: epoch 0058, iter [03400, 05004], lr: 0.083457, loss: 2.9303
2022-03-06 17:06:00 - train: epoch 0058, iter [03500, 05004], lr: 0.083457, loss: 2.9288
2022-03-06 17:06:33 - train: epoch 0058, iter [03600, 05004], lr: 0.083457, loss: 2.6543
2022-03-06 17:07:07 - train: epoch 0058, iter [03700, 05004], lr: 0.083457, loss: 3.0451
2022-03-06 17:07:40 - train: epoch 0058, iter [03800, 05004], lr: 0.083457, loss: 3.1264
2022-03-06 17:08:14 - train: epoch 0058, iter [03900, 05004], lr: 0.083457, loss: 3.0947
2022-03-06 17:08:48 - train: epoch 0058, iter [04000, 05004], lr: 0.083457, loss: 3.0928
2022-03-06 17:09:22 - train: epoch 0058, iter [04100, 05004], lr: 0.083457, loss: 3.2551
2022-03-06 17:09:55 - train: epoch 0058, iter [04200, 05004], lr: 0.083457, loss: 2.7414
2022-03-06 17:10:29 - train: epoch 0058, iter [04300, 05004], lr: 0.083457, loss: 3.1162
2022-03-06 17:11:02 - train: epoch 0058, iter [04400, 05004], lr: 0.083457, loss: 2.7683
2022-03-06 17:11:37 - train: epoch 0058, iter [04500, 05004], lr: 0.083457, loss: 3.0035
2022-03-06 17:12:10 - train: epoch 0058, iter [04600, 05004], lr: 0.083457, loss: 2.7703
2022-03-06 17:12:44 - train: epoch 0058, iter [04700, 05004], lr: 0.083457, loss: 2.8549
2022-03-06 17:13:17 - train: epoch 0058, iter [04800, 05004], lr: 0.083457, loss: 2.9403
2022-03-06 17:13:51 - train: epoch 0058, iter [04900, 05004], lr: 0.083457, loss: 2.6927
2022-03-06 17:14:24 - train: epoch 0058, iter [05000, 05004], lr: 0.083457, loss: 2.9833
2022-03-06 17:14:25 - train: epoch 058, train_loss: 2.9727
2022-03-06 17:15:38 - eval: epoch: 058, acc1: 54.952%, acc5: 80.294%, test_loss: 1.9100, per_image_load_time: 2.252ms, per_image_inference_time: 0.528ms
2022-03-06 17:15:39 - until epoch: 058, best_acc1: 54.952%
2022-03-06 17:15:39 - epoch 059 lr: 0.08285358405218655
2022-03-06 17:16:18 - train: epoch 0059, iter [00100, 05004], lr: 0.082854, loss: 3.2035
2022-03-06 17:16:51 - train: epoch 0059, iter [00200, 05004], lr: 0.082854, loss: 2.8094
2022-03-06 17:17:25 - train: epoch 0059, iter [00300, 05004], lr: 0.082854, loss: 3.0728
2022-03-06 17:17:58 - train: epoch 0059, iter [00400, 05004], lr: 0.082854, loss: 2.9426
2022-03-06 17:18:32 - train: epoch 0059, iter [00500, 05004], lr: 0.082854, loss: 3.0806
2022-03-06 17:19:05 - train: epoch 0059, iter [00600, 05004], lr: 0.082854, loss: 2.8221
2022-03-06 17:19:39 - train: epoch 0059, iter [00700, 05004], lr: 0.082854, loss: 3.0855
2022-03-06 17:20:13 - train: epoch 0059, iter [00800, 05004], lr: 0.082854, loss: 2.9520
2022-03-06 17:20:47 - train: epoch 0059, iter [00900, 05004], lr: 0.082854, loss: 2.8888
2022-03-06 17:21:21 - train: epoch 0059, iter [01000, 05004], lr: 0.082854, loss: 3.0843
2022-03-06 17:21:55 - train: epoch 0059, iter [01100, 05004], lr: 0.082854, loss: 3.0183
2022-03-06 17:22:29 - train: epoch 0059, iter [01200, 05004], lr: 0.082854, loss: 2.5436
2022-03-06 17:23:03 - train: epoch 0059, iter [01300, 05004], lr: 0.082854, loss: 3.1437
2022-03-06 17:23:37 - train: epoch 0059, iter [01400, 05004], lr: 0.082854, loss: 3.1294
2022-03-06 17:24:10 - train: epoch 0059, iter [01500, 05004], lr: 0.082854, loss: 3.1954
2022-03-06 17:24:44 - train: epoch 0059, iter [01600, 05004], lr: 0.082854, loss: 2.9774
2022-03-06 17:25:19 - train: epoch 0059, iter [01700, 05004], lr: 0.082854, loss: 2.9269
2022-03-06 17:25:53 - train: epoch 0059, iter [01800, 05004], lr: 0.082854, loss: 3.0157
2022-03-06 17:26:26 - train: epoch 0059, iter [01900, 05004], lr: 0.082854, loss: 3.0585
2022-03-06 17:27:00 - train: epoch 0059, iter [02000, 05004], lr: 0.082854, loss: 2.9391
2022-03-06 17:27:34 - train: epoch 0059, iter [02100, 05004], lr: 0.082854, loss: 2.9187
2022-03-06 17:28:07 - train: epoch 0059, iter [02200, 05004], lr: 0.082854, loss: 2.9901
2022-03-06 17:28:41 - train: epoch 0059, iter [02300, 05004], lr: 0.082854, loss: 2.9194
2022-03-06 17:29:15 - train: epoch 0059, iter [02400, 05004], lr: 0.082854, loss: 2.8971
2022-03-06 17:29:49 - train: epoch 0059, iter [02500, 05004], lr: 0.082854, loss: 2.6972
2022-03-06 17:30:23 - train: epoch 0059, iter [02600, 05004], lr: 0.082854, loss: 3.0536
2022-03-06 17:30:56 - train: epoch 0059, iter [02700, 05004], lr: 0.082854, loss: 2.7656
2022-03-06 17:31:30 - train: epoch 0059, iter [02800, 05004], lr: 0.082854, loss: 3.2118
2022-03-06 17:32:04 - train: epoch 0059, iter [02900, 05004], lr: 0.082854, loss: 2.9996
2022-03-06 17:32:38 - train: epoch 0059, iter [03000, 05004], lr: 0.082854, loss: 3.3535
2022-03-06 17:33:12 - train: epoch 0059, iter [03100, 05004], lr: 0.082854, loss: 2.7735
2022-03-06 17:33:45 - train: epoch 0059, iter [03200, 05004], lr: 0.082854, loss: 2.9079
2022-03-06 17:34:19 - train: epoch 0059, iter [03300, 05004], lr: 0.082854, loss: 3.1663
2022-03-06 17:34:53 - train: epoch 0059, iter [03400, 05004], lr: 0.082854, loss: 3.2953
2022-03-06 17:35:27 - train: epoch 0059, iter [03500, 05004], lr: 0.082854, loss: 3.0017
2022-03-06 17:36:01 - train: epoch 0059, iter [03600, 05004], lr: 0.082854, loss: 2.9771
2022-03-06 17:36:36 - train: epoch 0059, iter [03700, 05004], lr: 0.082854, loss: 2.8971
2022-03-06 17:37:08 - train: epoch 0059, iter [03800, 05004], lr: 0.082854, loss: 2.6880
2022-03-06 17:37:43 - train: epoch 0059, iter [03900, 05004], lr: 0.082854, loss: 3.0201
2022-03-06 17:38:16 - train: epoch 0059, iter [04000, 05004], lr: 0.082854, loss: 3.2782
2022-03-06 17:38:50 - train: epoch 0059, iter [04100, 05004], lr: 0.082854, loss: 3.0130
2022-03-06 17:39:25 - train: epoch 0059, iter [04200, 05004], lr: 0.082854, loss: 2.9554
2022-03-06 17:39:59 - train: epoch 0059, iter [04300, 05004], lr: 0.082854, loss: 3.2866
2022-03-06 17:40:32 - train: epoch 0059, iter [04400, 05004], lr: 0.082854, loss: 3.1514
2022-03-06 17:41:06 - train: epoch 0059, iter [04500, 05004], lr: 0.082854, loss: 2.8573
2022-03-06 17:41:40 - train: epoch 0059, iter [04600, 05004], lr: 0.082854, loss: 2.7918
2022-03-06 17:42:13 - train: epoch 0059, iter [04700, 05004], lr: 0.082854, loss: 2.8597
2022-03-06 17:42:48 - train: epoch 0059, iter [04800, 05004], lr: 0.082854, loss: 2.7231
2022-03-06 17:43:22 - train: epoch 0059, iter [04900, 05004], lr: 0.082854, loss: 2.9431
2022-03-06 17:43:54 - train: epoch 0059, iter [05000, 05004], lr: 0.082854, loss: 3.0558
2022-03-06 17:43:55 - train: epoch 059, train_loss: 2.9684
2022-03-06 17:45:09 - eval: epoch: 059, acc1: 54.578%, acc5: 80.132%, test_loss: 1.9127, per_image_load_time: 2.380ms, per_image_inference_time: 0.490ms
2022-03-06 17:45:09 - until epoch: 059, best_acc1: 54.952%
2022-03-06 17:45:09 - epoch 060 lr: 0.08224211063680853
2022-03-06 17:45:48 - train: epoch 0060, iter [00100, 05004], lr: 0.082242, loss: 2.9075
2022-03-06 17:46:22 - train: epoch 0060, iter [00200, 05004], lr: 0.082242, loss: 3.0854
2022-03-06 17:46:55 - train: epoch 0060, iter [00300, 05004], lr: 0.082242, loss: 3.0080
2022-03-06 17:47:28 - train: epoch 0060, iter [00400, 05004], lr: 0.082242, loss: 2.9881
2022-03-06 17:48:03 - train: epoch 0060, iter [00500, 05004], lr: 0.082242, loss: 3.1659
2022-03-06 17:48:36 - train: epoch 0060, iter [00600, 05004], lr: 0.082242, loss: 3.2580
2022-03-06 17:49:10 - train: epoch 0060, iter [00700, 05004], lr: 0.082242, loss: 3.0651
2022-03-06 17:49:43 - train: epoch 0060, iter [00800, 05004], lr: 0.082242, loss: 2.9906
2022-03-06 17:50:17 - train: epoch 0060, iter [00900, 05004], lr: 0.082242, loss: 2.8469
2022-03-06 17:50:51 - train: epoch 0060, iter [01000, 05004], lr: 0.082242, loss: 2.7128
2022-03-06 17:51:25 - train: epoch 0060, iter [01100, 05004], lr: 0.082242, loss: 2.7355
2022-03-06 17:51:59 - train: epoch 0060, iter [01200, 05004], lr: 0.082242, loss: 2.7896
2022-03-06 17:52:33 - train: epoch 0060, iter [01300, 05004], lr: 0.082242, loss: 2.8205
2022-03-06 17:53:07 - train: epoch 0060, iter [01400, 05004], lr: 0.082242, loss: 2.9668
2022-03-06 17:53:40 - train: epoch 0060, iter [01500, 05004], lr: 0.082242, loss: 3.1071
2022-03-06 17:54:14 - train: epoch 0060, iter [01600, 05004], lr: 0.082242, loss: 2.8601
2022-03-06 17:54:48 - train: epoch 0060, iter [01700, 05004], lr: 0.082242, loss: 2.8899
2022-03-06 17:55:22 - train: epoch 0060, iter [01800, 05004], lr: 0.082242, loss: 2.8188
2022-03-06 17:55:56 - train: epoch 0060, iter [01900, 05004], lr: 0.082242, loss: 3.0670
2022-03-06 17:56:30 - train: epoch 0060, iter [02000, 05004], lr: 0.082242, loss: 2.9047
2022-03-06 17:57:03 - train: epoch 0060, iter [02100, 05004], lr: 0.082242, loss: 2.9864
2022-03-06 17:57:37 - train: epoch 0060, iter [02200, 05004], lr: 0.082242, loss: 3.1358
2022-03-06 17:58:10 - train: epoch 0060, iter [02300, 05004], lr: 0.082242, loss: 2.7960
2022-03-06 17:58:44 - train: epoch 0060, iter [02400, 05004], lr: 0.082242, loss: 2.9988
2022-03-06 17:59:18 - train: epoch 0060, iter [02500, 05004], lr: 0.082242, loss: 3.0454
2022-03-06 17:59:52 - train: epoch 0060, iter [02600, 05004], lr: 0.082242, loss: 2.9799
2022-03-06 18:00:27 - train: epoch 0060, iter [02700, 05004], lr: 0.082242, loss: 2.9584
2022-03-06 18:01:00 - train: epoch 0060, iter [02800, 05004], lr: 0.082242, loss: 2.8688
2022-03-06 18:01:34 - train: epoch 0060, iter [02900, 05004], lr: 0.082242, loss: 3.1759
2022-03-06 18:02:08 - train: epoch 0060, iter [03000, 05004], lr: 0.082242, loss: 3.3663
2022-03-06 18:02:42 - train: epoch 0060, iter [03100, 05004], lr: 0.082242, loss: 2.9506
2022-03-06 18:03:16 - train: epoch 0060, iter [03200, 05004], lr: 0.082242, loss: 2.7780
2022-03-06 18:03:50 - train: epoch 0060, iter [03300, 05004], lr: 0.082242, loss: 2.8016
2022-03-06 18:04:22 - train: epoch 0060, iter [03400, 05004], lr: 0.082242, loss: 3.2734
2022-03-06 18:04:56 - train: epoch 0060, iter [03500, 05004], lr: 0.082242, loss: 3.0449
2022-03-06 18:05:29 - train: epoch 0060, iter [03600, 05004], lr: 0.082242, loss: 3.3472
2022-03-06 18:06:03 - train: epoch 0060, iter [03700, 05004], lr: 0.082242, loss: 3.0428
2022-03-06 18:06:37 - train: epoch 0060, iter [03800, 05004], lr: 0.082242, loss: 3.0026
2022-03-06 18:07:11 - train: epoch 0060, iter [03900, 05004], lr: 0.082242, loss: 3.3503
2022-03-06 18:07:44 - train: epoch 0060, iter [04000, 05004], lr: 0.082242, loss: 2.8682
2022-03-06 18:08:17 - train: epoch 0060, iter [04100, 05004], lr: 0.082242, loss: 3.0073
2022-03-06 18:08:50 - train: epoch 0060, iter [04200, 05004], lr: 0.082242, loss: 2.9442
2022-03-06 18:09:25 - train: epoch 0060, iter [04300, 05004], lr: 0.082242, loss: 2.8505
2022-03-06 18:09:58 - train: epoch 0060, iter [04400, 05004], lr: 0.082242, loss: 3.2869
2022-03-06 18:10:32 - train: epoch 0060, iter [04500, 05004], lr: 0.082242, loss: 2.8455
2022-03-06 18:11:05 - train: epoch 0060, iter [04600, 05004], lr: 0.082242, loss: 3.0148
2022-03-06 18:11:39 - train: epoch 0060, iter [04700, 05004], lr: 0.082242, loss: 3.0777
2022-03-06 18:12:13 - train: epoch 0060, iter [04800, 05004], lr: 0.082242, loss: 2.9719
2022-03-06 18:12:47 - train: epoch 0060, iter [04900, 05004], lr: 0.082242, loss: 3.0519
2022-03-06 18:13:19 - train: epoch 0060, iter [05000, 05004], lr: 0.082242, loss: 3.1269
2022-03-06 18:13:21 - train: epoch 060, train_loss: 2.9655
2022-03-06 18:14:34 - eval: epoch: 060, acc1: 52.348%, acc5: 78.168%, test_loss: 2.0435, per_image_load_time: 2.272ms, per_image_inference_time: 0.520ms
2022-03-06 18:14:34 - until epoch: 060, best_acc1: 54.952%
2022-03-06 18:14:34 - epoch 061 lr: 0.08162226877976886
2022-03-06 18:15:13 - train: epoch 0061, iter [00100, 05004], lr: 0.081622, loss: 2.7224
2022-03-06 18:15:47 - train: epoch 0061, iter [00200, 05004], lr: 0.081622, loss: 2.9421
2022-03-06 18:16:21 - train: epoch 0061, iter [00300, 05004], lr: 0.081622, loss: 2.9440
2022-03-06 18:16:54 - train: epoch 0061, iter [00400, 05004], lr: 0.081622, loss: 3.5554
2022-03-06 18:17:28 - train: epoch 0061, iter [00500, 05004], lr: 0.081622, loss: 2.8686
2022-03-06 18:18:01 - train: epoch 0061, iter [00600, 05004], lr: 0.081622, loss: 2.9865
2022-03-06 18:18:35 - train: epoch 0061, iter [00700, 05004], lr: 0.081622, loss: 2.7859
2022-03-06 18:19:09 - train: epoch 0061, iter [00800, 05004], lr: 0.081622, loss: 2.9579
2022-03-06 18:19:42 - train: epoch 0061, iter [00900, 05004], lr: 0.081622, loss: 2.9332
2022-03-06 18:20:16 - train: epoch 0061, iter [01000, 05004], lr: 0.081622, loss: 2.9350
2022-03-06 18:20:51 - train: epoch 0061, iter [01100, 05004], lr: 0.081622, loss: 2.8336
2022-03-06 18:21:24 - train: epoch 0061, iter [01200, 05004], lr: 0.081622, loss: 2.7754
2022-03-06 18:21:59 - train: epoch 0061, iter [01300, 05004], lr: 0.081622, loss: 2.7893
2022-03-06 18:22:32 - train: epoch 0061, iter [01400, 05004], lr: 0.081622, loss: 2.9167
2022-03-06 18:23:06 - train: epoch 0061, iter [01500, 05004], lr: 0.081622, loss: 3.0256
2022-03-06 18:23:40 - train: epoch 0061, iter [01600, 05004], lr: 0.081622, loss: 3.0533
2022-03-06 18:24:14 - train: epoch 0061, iter [01700, 05004], lr: 0.081622, loss: 3.1698
2022-03-06 18:24:47 - train: epoch 0061, iter [01800, 05004], lr: 0.081622, loss: 2.9257
2022-03-06 18:25:21 - train: epoch 0061, iter [01900, 05004], lr: 0.081622, loss: 2.9179
2022-03-06 18:25:54 - train: epoch 0061, iter [02000, 05004], lr: 0.081622, loss: 2.9527
2022-03-06 18:26:27 - train: epoch 0061, iter [02100, 05004], lr: 0.081622, loss: 3.2799
2022-03-06 18:27:02 - train: epoch 0061, iter [02200, 05004], lr: 0.081622, loss: 2.8059
2022-03-06 18:27:35 - train: epoch 0061, iter [02300, 05004], lr: 0.081622, loss: 3.2344
2022-03-06 18:28:07 - train: epoch 0061, iter [02400, 05004], lr: 0.081622, loss: 2.8337
2022-03-06 18:28:43 - train: epoch 0061, iter [02500, 05004], lr: 0.081622, loss: 2.8775
2022-03-06 18:29:16 - train: epoch 0061, iter [02600, 05004], lr: 0.081622, loss: 3.0383
2022-03-06 18:29:50 - train: epoch 0061, iter [02700, 05004], lr: 0.081622, loss: 2.9034
2022-03-06 18:30:23 - train: epoch 0061, iter [02800, 05004], lr: 0.081622, loss: 3.0807
2022-03-06 18:30:57 - train: epoch 0061, iter [02900, 05004], lr: 0.081622, loss: 2.9597
2022-03-06 18:31:31 - train: epoch 0061, iter [03000, 05004], lr: 0.081622, loss: 2.9388
2022-03-06 18:32:04 - train: epoch 0061, iter [03100, 05004], lr: 0.081622, loss: 3.1400
2022-03-06 18:32:38 - train: epoch 0061, iter [03200, 05004], lr: 0.081622, loss: 3.0410
2022-03-06 18:33:12 - train: epoch 0061, iter [03300, 05004], lr: 0.081622, loss: 2.6938
2022-03-06 18:33:46 - train: epoch 0061, iter [03400, 05004], lr: 0.081622, loss: 2.8210
2022-03-06 18:34:19 - train: epoch 0061, iter [03500, 05004], lr: 0.081622, loss: 2.8605
2022-03-06 18:34:53 - train: epoch 0061, iter [03600, 05004], lr: 0.081622, loss: 3.0132
2022-03-06 18:35:27 - train: epoch 0061, iter [03700, 05004], lr: 0.081622, loss: 2.9421
2022-03-06 18:36:01 - train: epoch 0061, iter [03800, 05004], lr: 0.081622, loss: 2.9159
2022-03-06 18:36:34 - train: epoch 0061, iter [03900, 05004], lr: 0.081622, loss: 2.9421
2022-03-06 18:37:08 - train: epoch 0061, iter [04000, 05004], lr: 0.081622, loss: 2.7603
2022-03-06 18:37:42 - train: epoch 0061, iter [04100, 05004], lr: 0.081622, loss: 2.9792
2022-03-06 18:38:15 - train: epoch 0061, iter [04200, 05004], lr: 0.081622, loss: 2.8838
2022-03-06 18:38:48 - train: epoch 0061, iter [04300, 05004], lr: 0.081622, loss: 2.9940
2022-03-06 18:39:22 - train: epoch 0061, iter [04400, 05004], lr: 0.081622, loss: 3.0447
2022-03-06 18:39:55 - train: epoch 0061, iter [04500, 05004], lr: 0.081622, loss: 3.0795
2022-03-06 18:40:29 - train: epoch 0061, iter [04600, 05004], lr: 0.081622, loss: 3.0498
2022-03-06 18:41:03 - train: epoch 0061, iter [04700, 05004], lr: 0.081622, loss: 2.8809
2022-03-06 18:41:36 - train: epoch 0061, iter [04800, 05004], lr: 0.081622, loss: 3.2227
2022-03-06 18:42:10 - train: epoch 0061, iter [04900, 05004], lr: 0.081622, loss: 2.7500
2022-03-06 18:42:42 - train: epoch 0061, iter [05000, 05004], lr: 0.081622, loss: 3.2639
2022-03-06 18:42:43 - train: epoch 061, train_loss: 2.9585
2022-03-06 18:43:57 - eval: epoch: 061, acc1: 54.704%, acc5: 79.968%, test_loss: 1.9343, per_image_load_time: 2.373ms, per_image_inference_time: 0.533ms
2022-03-06 18:43:57 - until epoch: 061, best_acc1: 54.952%
2022-03-06 18:43:57 - epoch 062 lr: 0.08099421936105702
2022-03-06 18:44:36 - train: epoch 0062, iter [00100, 05004], lr: 0.080994, loss: 3.0223
2022-03-06 18:45:09 - train: epoch 0062, iter [00200, 05004], lr: 0.080994, loss: 3.1337
2022-03-06 18:45:44 - train: epoch 0062, iter [00300, 05004], lr: 0.080994, loss: 3.1309
2022-03-06 18:46:17 - train: epoch 0062, iter [00400, 05004], lr: 0.080994, loss: 2.6219
2022-03-06 18:46:51 - train: epoch 0062, iter [00500, 05004], lr: 0.080994, loss: 2.8401
2022-03-06 18:47:25 - train: epoch 0062, iter [00600, 05004], lr: 0.080994, loss: 2.9557
2022-03-06 18:47:59 - train: epoch 0062, iter [00700, 05004], lr: 0.080994, loss: 3.0145
2022-03-06 18:48:32 - train: epoch 0062, iter [00800, 05004], lr: 0.080994, loss: 3.0345
2022-03-06 18:49:06 - train: epoch 0062, iter [00900, 05004], lr: 0.080994, loss: 2.8546
2022-03-06 18:49:40 - train: epoch 0062, iter [01000, 05004], lr: 0.080994, loss: 3.0756
2022-03-06 18:50:14 - train: epoch 0062, iter [01100, 05004], lr: 0.080994, loss: 2.8010
2022-03-06 18:50:48 - train: epoch 0062, iter [01200, 05004], lr: 0.080994, loss: 3.1761
2022-03-06 18:51:22 - train: epoch 0062, iter [01300, 05004], lr: 0.080994, loss: 2.9776
2022-03-06 18:51:55 - train: epoch 0062, iter [01400, 05004], lr: 0.080994, loss: 2.7458
2022-03-06 18:52:29 - train: epoch 0062, iter [01500, 05004], lr: 0.080994, loss: 3.0319
2022-03-06 18:53:03 - train: epoch 0062, iter [01600, 05004], lr: 0.080994, loss: 3.0000
2022-03-06 18:53:37 - train: epoch 0062, iter [01700, 05004], lr: 0.080994, loss: 2.8033
2022-03-06 18:54:11 - train: epoch 0062, iter [01800, 05004], lr: 0.080994, loss: 2.9277
2022-03-06 18:54:45 - train: epoch 0062, iter [01900, 05004], lr: 0.080994, loss: 2.8583
2022-03-06 18:55:18 - train: epoch 0062, iter [02000, 05004], lr: 0.080994, loss: 2.8920
2022-03-06 18:55:52 - train: epoch 0062, iter [02100, 05004], lr: 0.080994, loss: 3.0715
2022-03-06 18:56:26 - train: epoch 0062, iter [02200, 05004], lr: 0.080994, loss: 2.9311
2022-03-06 18:56:59 - train: epoch 0062, iter [02300, 05004], lr: 0.080994, loss: 2.9609
2022-03-06 18:57:33 - train: epoch 0062, iter [02400, 05004], lr: 0.080994, loss: 3.1056
2022-03-06 18:58:07 - train: epoch 0062, iter [02500, 05004], lr: 0.080994, loss: 3.0691
2022-03-06 18:58:40 - train: epoch 0062, iter [02600, 05004], lr: 0.080994, loss: 2.6562
2022-03-06 18:59:14 - train: epoch 0062, iter [02700, 05004], lr: 0.080994, loss: 3.1152
2022-03-06 18:59:48 - train: epoch 0062, iter [02800, 05004], lr: 0.080994, loss: 3.0553
2022-03-06 19:00:22 - train: epoch 0062, iter [02900, 05004], lr: 0.080994, loss: 2.9397
2022-03-06 19:00:56 - train: epoch 0062, iter [03000, 05004], lr: 0.080994, loss: 3.0833
2022-03-06 19:01:29 - train: epoch 0062, iter [03100, 05004], lr: 0.080994, loss: 3.0372
2022-03-06 19:02:03 - train: epoch 0062, iter [03200, 05004], lr: 0.080994, loss: 2.8154
2022-03-06 19:02:36 - train: epoch 0062, iter [03300, 05004], lr: 0.080994, loss: 3.0289
2022-03-06 19:03:11 - train: epoch 0062, iter [03400, 05004], lr: 0.080994, loss: 2.8465
2022-03-06 19:03:44 - train: epoch 0062, iter [03500, 05004], lr: 0.080994, loss: 2.9352
2022-03-06 19:04:18 - train: epoch 0062, iter [03600, 05004], lr: 0.080994, loss: 3.0041
2022-03-06 19:04:52 - train: epoch 0062, iter [03700, 05004], lr: 0.080994, loss: 2.9335
2022-03-06 19:05:25 - train: epoch 0062, iter [03800, 05004], lr: 0.080994, loss: 2.8090
2022-03-06 19:05:58 - train: epoch 0062, iter [03900, 05004], lr: 0.080994, loss: 3.0127
2022-03-06 19:06:32 - train: epoch 0062, iter [04000, 05004], lr: 0.080994, loss: 2.4835
2022-03-06 19:07:06 - train: epoch 0062, iter [04100, 05004], lr: 0.080994, loss: 2.9842
2022-03-06 19:07:41 - train: epoch 0062, iter [04200, 05004], lr: 0.080994, loss: 2.9933
2022-03-06 19:08:14 - train: epoch 0062, iter [04300, 05004], lr: 0.080994, loss: 3.0613
2022-03-06 19:08:48 - train: epoch 0062, iter [04400, 05004], lr: 0.080994, loss: 3.0563
2022-03-06 19:09:22 - train: epoch 0062, iter [04500, 05004], lr: 0.080994, loss: 2.8001
2022-03-06 19:09:56 - train: epoch 0062, iter [04600, 05004], lr: 0.080994, loss: 2.5899
2022-03-06 19:10:30 - train: epoch 0062, iter [04700, 05004], lr: 0.080994, loss: 3.0281
2022-03-06 19:11:04 - train: epoch 0062, iter [04800, 05004], lr: 0.080994, loss: 2.7820
2022-03-06 19:11:38 - train: epoch 0062, iter [04900, 05004], lr: 0.080994, loss: 2.9134
2022-03-06 19:12:10 - train: epoch 0062, iter [05000, 05004], lr: 0.080994, loss: 2.9468
2022-03-06 19:12:11 - train: epoch 062, train_loss: 2.9523
2022-03-06 19:13:25 - eval: epoch: 062, acc1: 53.662%, acc5: 79.230%, test_loss: 1.9656, per_image_load_time: 2.363ms, per_image_inference_time: 0.507ms
2022-03-06 19:13:26 - until epoch: 062, best_acc1: 54.952%
2022-03-06 19:13:26 - epoch 063 lr: 0.08035812539093556
2022-03-06 19:14:04 - train: epoch 0063, iter [00100, 05004], lr: 0.080358, loss: 2.9774
2022-03-06 19:14:39 - train: epoch 0063, iter [00200, 05004], lr: 0.080358, loss: 2.6588
2022-03-06 19:15:12 - train: epoch 0063, iter [00300, 05004], lr: 0.080358, loss: 3.0918
2022-03-06 19:15:46 - train: epoch 0063, iter [00400, 05004], lr: 0.080358, loss: 2.8461
2022-03-06 19:16:19 - train: epoch 0063, iter [00500, 05004], lr: 0.080358, loss: 2.6513
2022-03-06 19:16:53 - train: epoch 0063, iter [00600, 05004], lr: 0.080358, loss: 3.1519
2022-03-06 19:17:27 - train: epoch 0063, iter [00700, 05004], lr: 0.080358, loss: 2.7601
2022-03-06 19:18:00 - train: epoch 0063, iter [00800, 05004], lr: 0.080358, loss: 2.9569
2022-03-06 19:18:34 - train: epoch 0063, iter [00900, 05004], lr: 0.080358, loss: 3.1513
2022-03-06 19:19:07 - train: epoch 0063, iter [01000, 05004], lr: 0.080358, loss: 3.1248
2022-03-06 19:19:41 - train: epoch 0063, iter [01100, 05004], lr: 0.080358, loss: 3.1905
2022-03-06 19:20:15 - train: epoch 0063, iter [01200, 05004], lr: 0.080358, loss: 2.8960
2022-03-06 19:20:49 - train: epoch 0063, iter [01300, 05004], lr: 0.080358, loss: 3.0581
2022-03-06 19:21:22 - train: epoch 0063, iter [01400, 05004], lr: 0.080358, loss: 3.5078
2022-03-06 19:21:56 - train: epoch 0063, iter [01500, 05004], lr: 0.080358, loss: 2.7392
2022-03-06 19:22:29 - train: epoch 0063, iter [01600, 05004], lr: 0.080358, loss: 2.8622
2022-03-06 19:23:02 - train: epoch 0063, iter [01700, 05004], lr: 0.080358, loss: 3.0071
2022-03-06 19:23:37 - train: epoch 0063, iter [01800, 05004], lr: 0.080358, loss: 3.0910
2022-03-06 19:24:10 - train: epoch 0063, iter [01900, 05004], lr: 0.080358, loss: 3.1057
2022-03-06 19:24:45 - train: epoch 0063, iter [02000, 05004], lr: 0.080358, loss: 2.9001
2022-03-06 19:25:18 - train: epoch 0063, iter [02100, 05004], lr: 0.080358, loss: 2.7629
2022-03-06 19:25:51 - train: epoch 0063, iter [02200, 05004], lr: 0.080358, loss: 3.2058
2022-03-06 19:26:25 - train: epoch 0063, iter [02300, 05004], lr: 0.080358, loss: 3.2915
2022-03-06 19:26:59 - train: epoch 0063, iter [02400, 05004], lr: 0.080358, loss: 3.1529
2022-03-06 19:27:33 - train: epoch 0063, iter [02500, 05004], lr: 0.080358, loss: 2.9882
2022-03-06 19:28:07 - train: epoch 0063, iter [02600, 05004], lr: 0.080358, loss: 2.6681
2022-03-06 19:28:41 - train: epoch 0063, iter [02700, 05004], lr: 0.080358, loss: 3.1138
2022-03-06 19:29:14 - train: epoch 0063, iter [02800, 05004], lr: 0.080358, loss: 2.9517
2022-03-06 19:29:48 - train: epoch 0063, iter [02900, 05004], lr: 0.080358, loss: 2.6064
2022-03-06 19:30:22 - train: epoch 0063, iter [03000, 05004], lr: 0.080358, loss: 3.0553
2022-03-06 19:30:55 - train: epoch 0063, iter [03100, 05004], lr: 0.080358, loss: 3.1425
2022-03-06 19:31:29 - train: epoch 0063, iter [03200, 05004], lr: 0.080358, loss: 3.1650
2022-03-06 19:32:03 - train: epoch 0063, iter [03300, 05004], lr: 0.080358, loss: 2.7698
2022-03-06 19:32:36 - train: epoch 0063, iter [03400, 05004], lr: 0.080358, loss: 2.9032
2022-03-06 19:33:10 - train: epoch 0063, iter [03500, 05004], lr: 0.080358, loss: 2.9434
2022-03-06 19:33:44 - train: epoch 0063, iter [03600, 05004], lr: 0.080358, loss: 2.9838
2022-03-06 19:34:17 - train: epoch 0063, iter [03700, 05004], lr: 0.080358, loss: 2.9944
2022-03-06 19:34:51 - train: epoch 0063, iter [03800, 05004], lr: 0.080358, loss: 2.8384
2022-03-06 19:35:25 - train: epoch 0063, iter [03900, 05004], lr: 0.080358, loss: 2.6946
2022-03-06 19:35:59 - train: epoch 0063, iter [04000, 05004], lr: 0.080358, loss: 2.6858
2022-03-06 19:36:32 - train: epoch 0063, iter [04100, 05004], lr: 0.080358, loss: 2.9462
2022-03-06 19:37:06 - train: epoch 0063, iter [04200, 05004], lr: 0.080358, loss: 2.8471
2022-03-06 19:37:39 - train: epoch 0063, iter [04300, 05004], lr: 0.080358, loss: 2.9498
2022-03-06 19:38:13 - train: epoch 0063, iter [04400, 05004], lr: 0.080358, loss: 3.0572
2022-03-06 19:38:47 - train: epoch 0063, iter [04500, 05004], lr: 0.080358, loss: 2.9010
2022-03-06 19:39:20 - train: epoch 0063, iter [04600, 05004], lr: 0.080358, loss: 3.0104
2022-03-06 19:39:54 - train: epoch 0063, iter [04700, 05004], lr: 0.080358, loss: 3.1207
2022-03-06 19:40:28 - train: epoch 0063, iter [04800, 05004], lr: 0.080358, loss: 2.9916
2022-03-06 19:41:01 - train: epoch 0063, iter [04900, 05004], lr: 0.080358, loss: 2.9267
2022-03-06 19:41:34 - train: epoch 0063, iter [05000, 05004], lr: 0.080358, loss: 2.9331
2022-03-06 19:41:35 - train: epoch 063, train_loss: 2.9515
2022-03-06 19:42:49 - eval: epoch: 063, acc1: 54.366%, acc5: 79.854%, test_loss: 1.9303, per_image_load_time: 2.253ms, per_image_inference_time: 0.520ms
2022-03-06 19:42:49 - until epoch: 063, best_acc1: 54.952%
2022-03-06 19:42:49 - epoch 064 lr: 0.07971415196763088
2022-03-06 19:43:28 - train: epoch 0064, iter [00100, 05004], lr: 0.079714, loss: 2.7950
2022-03-06 19:44:02 - train: epoch 0064, iter [00200, 05004], lr: 0.079714, loss: 2.9291
2022-03-06 19:44:36 - train: epoch 0064, iter [00300, 05004], lr: 0.079714, loss: 2.8465
2022-03-06 19:45:10 - train: epoch 0064, iter [00400, 05004], lr: 0.079714, loss: 3.0151
2022-03-06 19:45:43 - train: epoch 0064, iter [00500, 05004], lr: 0.079714, loss: 2.8958
2022-03-06 19:46:16 - train: epoch 0064, iter [00600, 05004], lr: 0.079714, loss: 3.1128
2022-03-06 19:46:50 - train: epoch 0064, iter [00700, 05004], lr: 0.079714, loss: 2.9249
2022-03-06 19:47:23 - train: epoch 0064, iter [00800, 05004], lr: 0.079714, loss: 2.7087
2022-03-06 19:47:58 - train: epoch 0064, iter [00900, 05004], lr: 0.079714, loss: 2.7606
2022-03-06 19:48:32 - train: epoch 0064, iter [01000, 05004], lr: 0.079714, loss: 3.0552
2022-03-06 19:49:06 - train: epoch 0064, iter [01100, 05004], lr: 0.079714, loss: 2.8275
2022-03-06 19:49:39 - train: epoch 0064, iter [01200, 05004], lr: 0.079714, loss: 2.6697
2022-03-06 19:50:13 - train: epoch 0064, iter [01300, 05004], lr: 0.079714, loss: 3.0873
2022-03-06 19:50:46 - train: epoch 0064, iter [01400, 05004], lr: 0.079714, loss: 3.1058
2022-03-06 19:51:19 - train: epoch 0064, iter [01500, 05004], lr: 0.079714, loss: 3.2859
2022-03-06 19:51:53 - train: epoch 0064, iter [01600, 05004], lr: 0.079714, loss: 2.7008
2022-03-06 19:52:27 - train: epoch 0064, iter [01700, 05004], lr: 0.079714, loss: 2.6786
2022-03-06 19:53:00 - train: epoch 0064, iter [01800, 05004], lr: 0.079714, loss: 2.7359
2022-03-06 19:53:34 - train: epoch 0064, iter [01900, 05004], lr: 0.079714, loss: 3.0044
2022-03-06 19:54:07 - train: epoch 0064, iter [02000, 05004], lr: 0.079714, loss: 2.9029
2022-03-06 19:54:41 - train: epoch 0064, iter [02100, 05004], lr: 0.079714, loss: 3.0020
2022-03-06 19:55:14 - train: epoch 0064, iter [02200, 05004], lr: 0.079714, loss: 3.0227
2022-03-06 19:55:49 - train: epoch 0064, iter [02300, 05004], lr: 0.079714, loss: 3.1058
2022-03-06 19:56:21 - train: epoch 0064, iter [02400, 05004], lr: 0.079714, loss: 2.9891
2022-03-06 19:56:55 - train: epoch 0064, iter [02500, 05004], lr: 0.079714, loss: 3.0050
2022-03-06 19:57:29 - train: epoch 0064, iter [02600, 05004], lr: 0.079714, loss: 2.6906
2022-03-06 19:58:03 - train: epoch 0064, iter [02700, 05004], lr: 0.079714, loss: 2.8875
2022-03-06 19:58:36 - train: epoch 0064, iter [02800, 05004], lr: 0.079714, loss: 2.7132
2022-03-06 19:59:10 - train: epoch 0064, iter [02900, 05004], lr: 0.079714, loss: 3.2651
2022-03-06 19:59:45 - train: epoch 0064, iter [03000, 05004], lr: 0.079714, loss: 3.0472
2022-03-06 20:00:17 - train: epoch 0064, iter [03100, 05004], lr: 0.079714, loss: 2.9324
2022-03-06 20:00:51 - train: epoch 0064, iter [03200, 05004], lr: 0.079714, loss: 2.9391
2022-03-06 20:01:25 - train: epoch 0064, iter [03300, 05004], lr: 0.079714, loss: 2.9487
2022-03-06 20:01:59 - train: epoch 0064, iter [03400, 05004], lr: 0.079714, loss: 3.1965
2022-03-06 20:02:33 - train: epoch 0064, iter [03500, 05004], lr: 0.079714, loss: 2.8636
2022-03-06 20:03:07 - train: epoch 0064, iter [03600, 05004], lr: 0.079714, loss: 2.9101
2022-03-06 20:03:41 - train: epoch 0064, iter [03700, 05004], lr: 0.079714, loss: 2.6066
2022-03-06 20:04:15 - train: epoch 0064, iter [03800, 05004], lr: 0.079714, loss: 3.0786
2022-03-06 20:04:49 - train: epoch 0064, iter [03900, 05004], lr: 0.079714, loss: 3.0529
2022-03-06 20:05:23 - train: epoch 0064, iter [04000, 05004], lr: 0.079714, loss: 2.5901
2022-03-06 20:05:56 - train: epoch 0064, iter [04100, 05004], lr: 0.079714, loss: 3.2133
2022-03-06 20:06:30 - train: epoch 0064, iter [04200, 05004], lr: 0.079714, loss: 2.9748
2022-03-06 20:07:03 - train: epoch 0064, iter [04300, 05004], lr: 0.079714, loss: 3.1208
2022-03-06 20:07:37 - train: epoch 0064, iter [04400, 05004], lr: 0.079714, loss: 2.8506
2022-03-06 20:08:10 - train: epoch 0064, iter [04500, 05004], lr: 0.079714, loss: 3.1167
2022-03-06 20:08:44 - train: epoch 0064, iter [04600, 05004], lr: 0.079714, loss: 3.2354
2022-03-06 20:09:18 - train: epoch 0064, iter [04700, 05004], lr: 0.079714, loss: 3.2578
2022-03-06 20:09:51 - train: epoch 0064, iter [04800, 05004], lr: 0.079714, loss: 2.9451
2022-03-06 20:10:25 - train: epoch 0064, iter [04900, 05004], lr: 0.079714, loss: 2.9863
2022-03-06 20:10:57 - train: epoch 0064, iter [05000, 05004], lr: 0.079714, loss: 2.8361
2022-03-06 20:10:58 - train: epoch 064, train_loss: 2.9469
2022-03-06 20:12:12 - eval: epoch: 064, acc1: 55.120%, acc5: 80.434%, test_loss: 1.8977, per_image_load_time: 1.756ms, per_image_inference_time: 0.524ms
2022-03-06 20:12:13 - until epoch: 064, best_acc1: 55.120%
2022-03-06 20:12:13 - epoch 065 lr: 0.07906246623448183
2022-03-06 20:12:52 - train: epoch 0065, iter [00100, 05004], lr: 0.079062, loss: 3.1254
2022-03-06 20:13:25 - train: epoch 0065, iter [00200, 05004], lr: 0.079062, loss: 2.7930
2022-03-06 20:13:59 - train: epoch 0065, iter [00300, 05004], lr: 0.079062, loss: 2.7675
2022-03-06 20:14:33 - train: epoch 0065, iter [00400, 05004], lr: 0.079062, loss: 3.1500
2022-03-06 20:15:07 - train: epoch 0065, iter [00500, 05004], lr: 0.079062, loss: 2.9790
2022-03-06 20:15:41 - train: epoch 0065, iter [00600, 05004], lr: 0.079062, loss: 2.9886
2022-03-06 20:16:15 - train: epoch 0065, iter [00700, 05004], lr: 0.079062, loss: 2.9505
2022-03-06 20:16:49 - train: epoch 0065, iter [00800, 05004], lr: 0.079062, loss: 2.6386
2022-03-06 20:17:23 - train: epoch 0065, iter [00900, 05004], lr: 0.079062, loss: 2.6040
2022-03-06 20:17:56 - train: epoch 0065, iter [01000, 05004], lr: 0.079062, loss: 2.9934
2022-03-06 20:18:31 - train: epoch 0065, iter [01100, 05004], lr: 0.079062, loss: 2.8660
2022-03-06 20:19:05 - train: epoch 0065, iter [01200, 05004], lr: 0.079062, loss: 3.0280
2022-03-06 20:19:39 - train: epoch 0065, iter [01300, 05004], lr: 0.079062, loss: 2.8829
2022-03-06 20:20:12 - train: epoch 0065, iter [01400, 05004], lr: 0.079062, loss: 3.0857
2022-03-06 20:20:46 - train: epoch 0065, iter [01500, 05004], lr: 0.079062, loss: 2.9781
2022-03-06 20:21:20 - train: epoch 0065, iter [01600, 05004], lr: 0.079062, loss: 3.2035
2022-03-06 20:21:54 - train: epoch 0065, iter [01700, 05004], lr: 0.079062, loss: 2.9414
2022-03-06 20:22:29 - train: epoch 0065, iter [01800, 05004], lr: 0.079062, loss: 2.6079
2022-03-06 20:23:03 - train: epoch 0065, iter [01900, 05004], lr: 0.079062, loss: 2.4880
2022-03-06 20:23:36 - train: epoch 0065, iter [02000, 05004], lr: 0.079062, loss: 3.0042
2022-03-06 20:24:10 - train: epoch 0065, iter [02100, 05004], lr: 0.079062, loss: 2.7680
2022-03-06 20:24:44 - train: epoch 0065, iter [02200, 05004], lr: 0.079062, loss: 2.6452
2022-03-06 20:25:18 - train: epoch 0065, iter [02300, 05004], lr: 0.079062, loss: 2.7433
2022-03-06 20:25:52 - train: epoch 0065, iter [02400, 05004], lr: 0.079062, loss: 3.1500
2022-03-06 20:26:26 - train: epoch 0065, iter [02500, 05004], lr: 0.079062, loss: 2.8614
2022-03-06 20:27:00 - train: epoch 0065, iter [02600, 05004], lr: 0.079062, loss: 2.9199
2022-03-06 20:27:34 - train: epoch 0065, iter [02700, 05004], lr: 0.079062, loss: 2.9563
2022-03-06 20:28:07 - train: epoch 0065, iter [02800, 05004], lr: 0.079062, loss: 2.9750
2022-03-06 20:28:41 - train: epoch 0065, iter [02900, 05004], lr: 0.079062, loss: 2.9495
2022-03-06 20:29:16 - train: epoch 0065, iter [03000, 05004], lr: 0.079062, loss: 2.8266
2022-03-06 20:29:50 - train: epoch 0065, iter [03100, 05004], lr: 0.079062, loss: 2.8846
2022-03-06 20:30:25 - train: epoch 0065, iter [03200, 05004], lr: 0.079062, loss: 2.9622
2022-03-06 20:30:58 - train: epoch 0065, iter [03300, 05004], lr: 0.079062, loss: 3.0667
2022-03-06 20:31:32 - train: epoch 0065, iter [03400, 05004], lr: 0.079062, loss: 2.9460
2022-03-06 20:32:05 - train: epoch 0065, iter [03500, 05004], lr: 0.079062, loss: 3.2185
2022-03-06 20:32:40 - train: epoch 0065, iter [03600, 05004], lr: 0.079062, loss: 3.0216
2022-03-06 20:33:13 - train: epoch 0065, iter [03700, 05004], lr: 0.079062, loss: 2.8729
2022-03-06 20:33:47 - train: epoch 0065, iter [03800, 05004], lr: 0.079062, loss: 2.9100
2022-03-06 20:34:20 - train: epoch 0065, iter [03900, 05004], lr: 0.079062, loss: 3.0632
2022-03-06 20:34:54 - train: epoch 0065, iter [04000, 05004], lr: 0.079062, loss: 3.1504
2022-03-06 20:35:27 - train: epoch 0065, iter [04100, 05004], lr: 0.079062, loss: 2.7649
2022-03-06 20:36:01 - train: epoch 0065, iter [04200, 05004], lr: 0.079062, loss: 2.8075
2022-03-06 20:36:35 - train: epoch 0065, iter [04300, 05004], lr: 0.079062, loss: 2.9486
2022-03-06 20:37:09 - train: epoch 0065, iter [04400, 05004], lr: 0.079062, loss: 3.2981
2022-03-06 20:37:41 - train: epoch 0065, iter [04500, 05004], lr: 0.079062, loss: 3.0018
2022-03-06 20:38:15 - train: epoch 0065, iter [04600, 05004], lr: 0.079062, loss: 3.1831
2022-03-06 20:38:50 - train: epoch 0065, iter [04700, 05004], lr: 0.079062, loss: 2.9247
2022-03-06 20:39:23 - train: epoch 0065, iter [04800, 05004], lr: 0.079062, loss: 2.6321
2022-03-06 20:39:57 - train: epoch 0065, iter [04900, 05004], lr: 0.079062, loss: 2.9489
2022-03-06 20:40:29 - train: epoch 0065, iter [05000, 05004], lr: 0.079062, loss: 2.9438
2022-03-06 20:40:30 - train: epoch 065, train_loss: 2.9426
2022-03-06 20:41:43 - eval: epoch: 065, acc1: 54.048%, acc5: 79.424%, test_loss: 1.9558, per_image_load_time: 2.196ms, per_image_inference_time: 0.501ms
2022-03-06 20:41:44 - until epoch: 065, best_acc1: 55.120%
2022-03-06 20:41:44 - epoch 066 lr: 0.0784032373365578
2022-03-06 20:42:23 - train: epoch 0066, iter [00100, 05004], lr: 0.078403, loss: 2.8196
2022-03-06 20:42:56 - train: epoch 0066, iter [00200, 05004], lr: 0.078403, loss: 3.0615
2022-03-06 20:43:30 - train: epoch 0066, iter [00300, 05004], lr: 0.078403, loss: 3.0639
2022-03-06 20:44:04 - train: epoch 0066, iter [00400, 05004], lr: 0.078403, loss: 2.6723
2022-03-06 20:44:37 - train: epoch 0066, iter [00500, 05004], lr: 0.078403, loss: 2.9314
2022-03-06 20:45:11 - train: epoch 0066, iter [00600, 05004], lr: 0.078403, loss: 3.1523
2022-03-06 20:45:45 - train: epoch 0066, iter [00700, 05004], lr: 0.078403, loss: 2.9335
2022-03-06 20:46:18 - train: epoch 0066, iter [00800, 05004], lr: 0.078403, loss: 3.3073
2022-03-06 20:46:51 - train: epoch 0066, iter [00900, 05004], lr: 0.078403, loss: 2.8548
2022-03-06 20:47:25 - train: epoch 0066, iter [01000, 05004], lr: 0.078403, loss: 3.1289
2022-03-06 20:47:58 - train: epoch 0066, iter [01100, 05004], lr: 0.078403, loss: 3.1823
2022-03-06 20:48:32 - train: epoch 0066, iter [01200, 05004], lr: 0.078403, loss: 3.0762
2022-03-06 20:49:06 - train: epoch 0066, iter [01300, 05004], lr: 0.078403, loss: 3.3753
2022-03-06 20:49:39 - train: epoch 0066, iter [01400, 05004], lr: 0.078403, loss: 2.8077
2022-03-06 20:50:14 - train: epoch 0066, iter [01500, 05004], lr: 0.078403, loss: 3.0406
2022-03-06 20:50:47 - train: epoch 0066, iter [01600, 05004], lr: 0.078403, loss: 3.0260
2022-03-06 20:51:20 - train: epoch 0066, iter [01700, 05004], lr: 0.078403, loss: 2.9485
2022-03-06 20:51:54 - train: epoch 0066, iter [01800, 05004], lr: 0.078403, loss: 2.6944
2022-03-06 20:52:28 - train: epoch 0066, iter [01900, 05004], lr: 0.078403, loss: 3.0407
2022-03-06 20:53:01 - train: epoch 0066, iter [02000, 05004], lr: 0.078403, loss: 2.8670
2022-03-06 20:53:35 - train: epoch 0066, iter [02100, 05004], lr: 0.078403, loss: 3.0027
2022-03-06 20:54:08 - train: epoch 0066, iter [02200, 05004], lr: 0.078403, loss: 2.8898
2022-03-06 20:54:42 - train: epoch 0066, iter [02300, 05004], lr: 0.078403, loss: 3.1937
2022-03-06 20:55:15 - train: epoch 0066, iter [02400, 05004], lr: 0.078403, loss: 2.7655
2022-03-06 20:55:49 - train: epoch 0066, iter [02500, 05004], lr: 0.078403, loss: 2.8597
2022-03-06 20:56:23 - train: epoch 0066, iter [02600, 05004], lr: 0.078403, loss: 3.0635
2022-03-06 20:56:57 - train: epoch 0066, iter [02700, 05004], lr: 0.078403, loss: 2.9463
2022-03-06 20:57:30 - train: epoch 0066, iter [02800, 05004], lr: 0.078403, loss: 3.1314
2022-03-06 20:58:04 - train: epoch 0066, iter [02900, 05004], lr: 0.078403, loss: 3.0380
2022-03-06 20:58:37 - train: epoch 0066, iter [03000, 05004], lr: 0.078403, loss: 2.7207
2022-03-06 20:59:11 - train: epoch 0066, iter [03100, 05004], lr: 0.078403, loss: 2.9321
2022-03-06 20:59:45 - train: epoch 0066, iter [03200, 05004], lr: 0.078403, loss: 2.9316
2022-03-06 21:00:19 - train: epoch 0066, iter [03300, 05004], lr: 0.078403, loss: 2.9035
2022-03-06 21:00:52 - train: epoch 0066, iter [03400, 05004], lr: 0.078403, loss: 2.9176
2022-03-06 21:01:26 - train: epoch 0066, iter [03500, 05004], lr: 0.078403, loss: 3.0648
2022-03-06 21:01:59 - train: epoch 0066, iter [03600, 05004], lr: 0.078403, loss: 3.0088
2022-03-06 21:02:33 - train: epoch 0066, iter [03700, 05004], lr: 0.078403, loss: 2.8822
2022-03-06 21:03:06 - train: epoch 0066, iter [03800, 05004], lr: 0.078403, loss: 2.7496
2022-03-06 21:03:39 - train: epoch 0066, iter [03900, 05004], lr: 0.078403, loss: 2.4752
2022-03-06 21:04:13 - train: epoch 0066, iter [04000, 05004], lr: 0.078403, loss: 3.1070
2022-03-06 21:04:46 - train: epoch 0066, iter [04100, 05004], lr: 0.078403, loss: 2.7024
2022-03-06 21:05:19 - train: epoch 0066, iter [04200, 05004], lr: 0.078403, loss: 2.9250
2022-03-06 21:05:53 - train: epoch 0066, iter [04300, 05004], lr: 0.078403, loss: 2.6795
2022-03-06 21:06:26 - train: epoch 0066, iter [04400, 05004], lr: 0.078403, loss: 2.8974
2022-03-06 21:07:01 - train: epoch 0066, iter [04500, 05004], lr: 0.078403, loss: 2.9046
2022-03-06 21:07:34 - train: epoch 0066, iter [04600, 05004], lr: 0.078403, loss: 3.0476
2022-03-06 21:08:08 - train: epoch 0066, iter [04700, 05004], lr: 0.078403, loss: 3.1072
2022-03-06 21:08:41 - train: epoch 0066, iter [04800, 05004], lr: 0.078403, loss: 2.8516
2022-03-06 21:09:15 - train: epoch 0066, iter [04900, 05004], lr: 0.078403, loss: 3.1559
2022-03-06 21:09:47 - train: epoch 0066, iter [05000, 05004], lr: 0.078403, loss: 3.1412
2022-03-06 21:09:48 - train: epoch 066, train_loss: 2.9372
2022-03-06 21:11:01 - eval: epoch: 066, acc1: 52.940%, acc5: 78.578%, test_loss: 2.0214, per_image_load_time: 2.311ms, per_image_inference_time: 0.506ms
2022-03-06 21:11:02 - until epoch: 066, best_acc1: 55.120%
2022-03-06 21:11:02 - epoch 067 lr: 0.07773663637675694
2022-03-06 21:11:41 - train: epoch 0067, iter [00100, 05004], lr: 0.077737, loss: 2.8532
2022-03-06 21:12:14 - train: epoch 0067, iter [00200, 05004], lr: 0.077737, loss: 2.9368
2022-03-06 21:12:48 - train: epoch 0067, iter [00300, 05004], lr: 0.077737, loss: 3.0133
2022-03-06 21:13:21 - train: epoch 0067, iter [00400, 05004], lr: 0.077737, loss: 2.9613
2022-03-06 21:13:55 - train: epoch 0067, iter [00500, 05004], lr: 0.077737, loss: 2.9403
2022-03-06 21:14:28 - train: epoch 0067, iter [00600, 05004], lr: 0.077737, loss: 2.9200
2022-03-06 21:15:02 - train: epoch 0067, iter [00700, 05004], lr: 0.077737, loss: 2.9014
2022-03-06 21:15:36 - train: epoch 0067, iter [00800, 05004], lr: 0.077737, loss: 2.8702
2022-03-06 21:16:10 - train: epoch 0067, iter [00900, 05004], lr: 0.077737, loss: 3.2402
2022-03-06 21:16:43 - train: epoch 0067, iter [01000, 05004], lr: 0.077737, loss: 2.7553
2022-03-06 21:17:17 - train: epoch 0067, iter [01100, 05004], lr: 0.077737, loss: 2.9663
2022-03-06 21:17:51 - train: epoch 0067, iter [01200, 05004], lr: 0.077737, loss: 2.7254
2022-03-06 21:18:24 - train: epoch 0067, iter [01300, 05004], lr: 0.077737, loss: 3.1512
2022-03-06 21:18:57 - train: epoch 0067, iter [01400, 05004], lr: 0.077737, loss: 3.0371
2022-03-06 21:19:32 - train: epoch 0067, iter [01500, 05004], lr: 0.077737, loss: 2.7811
2022-03-06 21:20:05 - train: epoch 0067, iter [01600, 05004], lr: 0.077737, loss: 2.7989
2022-03-06 21:20:39 - train: epoch 0067, iter [01700, 05004], lr: 0.077737, loss: 3.0079
2022-03-06 21:21:13 - train: epoch 0067, iter [01800, 05004], lr: 0.077737, loss: 3.0385
2022-03-06 21:21:46 - train: epoch 0067, iter [01900, 05004], lr: 0.077737, loss: 3.1255
2022-03-06 21:22:19 - train: epoch 0067, iter [02000, 05004], lr: 0.077737, loss: 3.0677
2022-03-06 21:22:54 - train: epoch 0067, iter [02100, 05004], lr: 0.077737, loss: 2.7043
2022-03-06 21:23:27 - train: epoch 0067, iter [02200, 05004], lr: 0.077737, loss: 2.8840
2022-03-06 21:24:01 - train: epoch 0067, iter [02300, 05004], lr: 0.077737, loss: 3.0667
2022-03-06 21:24:35 - train: epoch 0067, iter [02400, 05004], lr: 0.077737, loss: 2.7943
2022-03-06 21:25:08 - train: epoch 0067, iter [02500, 05004], lr: 0.077737, loss: 2.5651
2022-03-06 21:25:41 - train: epoch 0067, iter [02600, 05004], lr: 0.077737, loss: 2.8535
2022-03-06 21:26:15 - train: epoch 0067, iter [02700, 05004], lr: 0.077737, loss: 2.8127
2022-03-06 21:26:49 - train: epoch 0067, iter [02800, 05004], lr: 0.077737, loss: 3.2015
2022-03-06 21:27:23 - train: epoch 0067, iter [02900, 05004], lr: 0.077737, loss: 2.8000
2022-03-06 21:27:57 - train: epoch 0067, iter [03000, 05004], lr: 0.077737, loss: 2.8149
2022-03-06 21:28:30 - train: epoch 0067, iter [03100, 05004], lr: 0.077737, loss: 2.6846
2022-03-06 21:29:03 - train: epoch 0067, iter [03200, 05004], lr: 0.077737, loss: 3.1449
2022-03-06 21:29:36 - train: epoch 0067, iter [03300, 05004], lr: 0.077737, loss: 2.9570
2022-03-06 21:30:10 - train: epoch 0067, iter [03400, 05004], lr: 0.077737, loss: 3.0724
2022-03-06 21:30:43 - train: epoch 0067, iter [03500, 05004], lr: 0.077737, loss: 2.8387
2022-03-06 21:31:18 - train: epoch 0067, iter [03600, 05004], lr: 0.077737, loss: 3.0694
2022-03-06 21:31:51 - train: epoch 0067, iter [03700, 05004], lr: 0.077737, loss: 3.1581
2022-03-06 21:32:24 - train: epoch 0067, iter [03800, 05004], lr: 0.077737, loss: 2.8475
2022-03-06 21:32:58 - train: epoch 0067, iter [03900, 05004], lr: 0.077737, loss: 3.1799
2022-03-06 21:33:31 - train: epoch 0067, iter [04000, 05004], lr: 0.077737, loss: 2.8908
2022-03-06 21:34:04 - train: epoch 0067, iter [04100, 05004], lr: 0.077737, loss: 2.9576
2022-03-06 21:34:38 - train: epoch 0067, iter [04200, 05004], lr: 0.077737, loss: 3.3538
2022-03-06 21:35:13 - train: epoch 0067, iter [04300, 05004], lr: 0.077737, loss: 2.7590
2022-03-06 21:35:46 - train: epoch 0067, iter [04400, 05004], lr: 0.077737, loss: 2.8671
2022-03-06 21:36:19 - train: epoch 0067, iter [04500, 05004], lr: 0.077737, loss: 3.0661
2022-03-06 21:36:53 - train: epoch 0067, iter [04600, 05004], lr: 0.077737, loss: 2.5568
2022-03-06 21:37:26 - train: epoch 0067, iter [04700, 05004], lr: 0.077737, loss: 2.7905
2022-03-06 21:38:00 - train: epoch 0067, iter [04800, 05004], lr: 0.077737, loss: 2.7582
2022-03-06 21:38:34 - train: epoch 0067, iter [04900, 05004], lr: 0.077737, loss: 2.8316
2022-03-06 21:39:05 - train: epoch 0067, iter [05000, 05004], lr: 0.077737, loss: 2.9547
2022-03-06 21:39:07 - train: epoch 067, train_loss: 2.9331
2022-03-06 21:40:21 - eval: epoch: 067, acc1: 54.496%, acc5: 79.544%, test_loss: 1.9418, per_image_load_time: 2.080ms, per_image_inference_time: 0.515ms
2022-03-06 21:40:21 - until epoch: 067, best_acc1: 55.120%
2022-03-06 21:40:21 - epoch 068 lr: 0.07706283637139658
2022-03-06 21:41:00 - train: epoch 0068, iter [00100, 05004], lr: 0.077063, loss: 2.8188
2022-03-06 21:41:34 - train: epoch 0068, iter [00200, 05004], lr: 0.077063, loss: 2.8660
2022-03-06 21:42:08 - train: epoch 0068, iter [00300, 05004], lr: 0.077063, loss: 2.9027
2022-03-06 21:42:40 - train: epoch 0068, iter [00400, 05004], lr: 0.077063, loss: 3.0784
2022-03-06 21:43:14 - train: epoch 0068, iter [00500, 05004], lr: 0.077063, loss: 2.7864
2022-03-06 21:43:48 - train: epoch 0068, iter [00600, 05004], lr: 0.077063, loss: 3.1515
2022-03-06 21:44:22 - train: epoch 0068, iter [00700, 05004], lr: 0.077063, loss: 2.9833
2022-03-06 21:44:56 - train: epoch 0068, iter [00800, 05004], lr: 0.077063, loss: 2.8086
2022-03-06 21:45:29 - train: epoch 0068, iter [00900, 05004], lr: 0.077063, loss: 2.8751
2022-03-06 21:46:02 - train: epoch 0068, iter [01000, 05004], lr: 0.077063, loss: 2.9787
2022-03-06 21:46:35 - train: epoch 0068, iter [01100, 05004], lr: 0.077063, loss: 3.2334
2022-03-06 21:47:09 - train: epoch 0068, iter [01200, 05004], lr: 0.077063, loss: 2.9747
2022-03-06 21:47:43 - train: epoch 0068, iter [01300, 05004], lr: 0.077063, loss: 2.6362
2022-03-06 21:48:17 - train: epoch 0068, iter [01400, 05004], lr: 0.077063, loss: 2.8711
2022-03-06 21:48:52 - train: epoch 0068, iter [01500, 05004], lr: 0.077063, loss: 3.1402
2022-03-06 21:49:25 - train: epoch 0068, iter [01600, 05004], lr: 0.077063, loss: 3.0766
2022-03-06 21:49:59 - train: epoch 0068, iter [01700, 05004], lr: 0.077063, loss: 3.2010
2022-03-06 21:50:33 - train: epoch 0068, iter [01800, 05004], lr: 0.077063, loss: 2.9818
2022-03-06 21:51:07 - train: epoch 0068, iter [01900, 05004], lr: 0.077063, loss: 3.1028
2022-03-06 21:51:40 - train: epoch 0068, iter [02000, 05004], lr: 0.077063, loss: 3.1710
2022-03-06 21:52:13 - train: epoch 0068, iter [02100, 05004], lr: 0.077063, loss: 2.9078
2022-03-06 21:52:46 - train: epoch 0068, iter [02200, 05004], lr: 0.077063, loss: 2.9122
2022-03-06 21:53:20 - train: epoch 0068, iter [02300, 05004], lr: 0.077063, loss: 3.0671
2022-03-06 21:53:54 - train: epoch 0068, iter [02400, 05004], lr: 0.077063, loss: 2.9802
2022-03-06 21:54:27 - train: epoch 0068, iter [02500, 05004], lr: 0.077063, loss: 3.0243
2022-03-06 21:55:01 - train: epoch 0068, iter [02600, 05004], lr: 0.077063, loss: 2.6611
2022-03-06 21:55:35 - train: epoch 0068, iter [02700, 05004], lr: 0.077063, loss: 2.7814
2022-03-06 21:56:08 - train: epoch 0068, iter [02800, 05004], lr: 0.077063, loss: 2.9816
2022-03-06 21:56:42 - train: epoch 0068, iter [02900, 05004], lr: 0.077063, loss: 2.8296
2022-03-06 21:57:16 - train: epoch 0068, iter [03000, 05004], lr: 0.077063, loss: 2.9925
2022-03-06 21:57:50 - train: epoch 0068, iter [03100, 05004], lr: 0.077063, loss: 2.7889
2022-03-06 21:58:24 - train: epoch 0068, iter [03200, 05004], lr: 0.077063, loss: 3.0508
2022-03-06 21:58:57 - train: epoch 0068, iter [03300, 05004], lr: 0.077063, loss: 2.8031
2022-03-06 21:59:31 - train: epoch 0068, iter [03400, 05004], lr: 0.077063, loss: 2.8590
2022-03-06 22:00:04 - train: epoch 0068, iter [03500, 05004], lr: 0.077063, loss: 2.9484
2022-03-06 22:00:38 - train: epoch 0068, iter [03600, 05004], lr: 0.077063, loss: 2.6851
2022-03-06 22:01:12 - train: epoch 0068, iter [03700, 05004], lr: 0.077063, loss: 2.9523
2022-03-06 22:01:46 - train: epoch 0068, iter [03800, 05004], lr: 0.077063, loss: 3.2108
2022-03-06 22:02:20 - train: epoch 0068, iter [03900, 05004], lr: 0.077063, loss: 3.0841
2022-03-06 22:02:53 - train: epoch 0068, iter [04000, 05004], lr: 0.077063, loss: 2.9560
2022-03-06 22:03:27 - train: epoch 0068, iter [04100, 05004], lr: 0.077063, loss: 2.6087
2022-03-06 22:04:01 - train: epoch 0068, iter [04200, 05004], lr: 0.077063, loss: 3.0148
2022-03-06 22:04:35 - train: epoch 0068, iter [04300, 05004], lr: 0.077063, loss: 3.0610
2022-03-06 22:05:08 - train: epoch 0068, iter [04400, 05004], lr: 0.077063, loss: 2.8663
2022-03-06 22:05:43 - train: epoch 0068, iter [04500, 05004], lr: 0.077063, loss: 2.8892
2022-03-06 22:06:16 - train: epoch 0068, iter [04600, 05004], lr: 0.077063, loss: 2.9346
2022-03-06 22:06:50 - train: epoch 0068, iter [04700, 05004], lr: 0.077063, loss: 2.9969
2022-03-06 22:07:23 - train: epoch 0068, iter [04800, 05004], lr: 0.077063, loss: 3.2733
2022-03-06 22:07:58 - train: epoch 0068, iter [04900, 05004], lr: 0.077063, loss: 3.0498
2022-03-06 22:08:30 - train: epoch 0068, iter [05000, 05004], lr: 0.077063, loss: 2.9214
2022-03-06 22:08:31 - train: epoch 068, train_loss: 2.9286
2022-03-06 22:09:44 - eval: epoch: 068, acc1: 54.842%, acc5: 79.834%, test_loss: 1.9140, per_image_load_time: 1.416ms, per_image_inference_time: 0.527ms
2022-03-06 22:09:45 - until epoch: 068, best_acc1: 55.120%
2022-03-06 22:09:45 - epoch 069 lr: 0.07638201220530665
2022-03-06 22:10:24 - train: epoch 0069, iter [00100, 05004], lr: 0.076382, loss: 2.9731
2022-03-06 22:10:57 - train: epoch 0069, iter [00200, 05004], lr: 0.076382, loss: 3.1451
2022-03-06 22:11:31 - train: epoch 0069, iter [00300, 05004], lr: 0.076382, loss: 2.9410
2022-03-06 22:12:05 - train: epoch 0069, iter [00400, 05004], lr: 0.076382, loss: 2.9255
2022-03-06 22:12:39 - train: epoch 0069, iter [00500, 05004], lr: 0.076382, loss: 2.6496
2022-03-06 22:13:12 - train: epoch 0069, iter [00600, 05004], lr: 0.076382, loss: 2.8258
2022-03-06 22:13:44 - train: epoch 0069, iter [00700, 05004], lr: 0.076382, loss: 2.9050
2022-03-06 22:14:19 - train: epoch 0069, iter [00800, 05004], lr: 0.076382, loss: 2.9600
2022-03-06 22:14:52 - train: epoch 0069, iter [00900, 05004], lr: 0.076382, loss: 2.7181
2022-03-06 22:15:27 - train: epoch 0069, iter [01000, 05004], lr: 0.076382, loss: 2.9019
2022-03-06 22:16:00 - train: epoch 0069, iter [01100, 05004], lr: 0.076382, loss: 2.8972
2022-03-06 22:16:33 - train: epoch 0069, iter [01200, 05004], lr: 0.076382, loss: 2.7983
2022-03-06 22:17:06 - train: epoch 0069, iter [01300, 05004], lr: 0.076382, loss: 3.0077
2022-03-06 22:17:40 - train: epoch 0069, iter [01400, 05004], lr: 0.076382, loss: 3.0283
2022-03-06 22:18:13 - train: epoch 0069, iter [01500, 05004], lr: 0.076382, loss: 2.7935
2022-03-06 22:18:47 - train: epoch 0069, iter [01600, 05004], lr: 0.076382, loss: 3.2590
2022-03-06 22:19:21 - train: epoch 0069, iter [01700, 05004], lr: 0.076382, loss: 2.9372
2022-03-06 22:19:56 - train: epoch 0069, iter [01800, 05004], lr: 0.076382, loss: 2.7520
2022-03-06 22:20:28 - train: epoch 0069, iter [01900, 05004], lr: 0.076382, loss: 2.9161
2022-03-06 22:21:03 - train: epoch 0069, iter [02000, 05004], lr: 0.076382, loss: 2.9335
2022-03-06 22:21:36 - train: epoch 0069, iter [02100, 05004], lr: 0.076382, loss: 3.1487
2022-03-06 22:22:09 - train: epoch 0069, iter [02200, 05004], lr: 0.076382, loss: 2.7552
2022-03-06 22:22:43 - train: epoch 0069, iter [02300, 05004], lr: 0.076382, loss: 2.8072
2022-03-06 22:23:17 - train: epoch 0069, iter [02400, 05004], lr: 0.076382, loss: 3.0350
2022-03-06 22:23:50 - train: epoch 0069, iter [02500, 05004], lr: 0.076382, loss: 3.0961
2022-03-06 22:24:24 - train: epoch 0069, iter [02600, 05004], lr: 0.076382, loss: 3.0148
2022-03-06 22:24:58 - train: epoch 0069, iter [02700, 05004], lr: 0.076382, loss: 3.2061
2022-03-06 22:25:32 - train: epoch 0069, iter [02800, 05004], lr: 0.076382, loss: 2.9387
2022-03-06 22:26:05 - train: epoch 0069, iter [02900, 05004], lr: 0.076382, loss: 2.7987
2022-03-06 22:26:39 - train: epoch 0069, iter [03000, 05004], lr: 0.076382, loss: 3.1248
2022-03-06 22:27:13 - train: epoch 0069, iter [03100, 05004], lr: 0.076382, loss: 2.8104
2022-03-06 22:27:47 - train: epoch 0069, iter [03200, 05004], lr: 0.076382, loss: 2.8297
2022-03-06 22:28:20 - train: epoch 0069, iter [03300, 05004], lr: 0.076382, loss: 2.4838
2022-03-06 22:28:54 - train: epoch 0069, iter [03400, 05004], lr: 0.076382, loss: 2.6990
2022-03-06 22:29:28 - train: epoch 0069, iter [03500, 05004], lr: 0.076382, loss: 2.8952
2022-03-06 22:30:01 - train: epoch 0069, iter [03600, 05004], lr: 0.076382, loss: 2.7566
2022-03-06 22:30:35 - train: epoch 0069, iter [03700, 05004], lr: 0.076382, loss: 3.0984
2022-03-06 22:31:08 - train: epoch 0069, iter [03800, 05004], lr: 0.076382, loss: 2.9564
2022-03-06 22:31:42 - train: epoch 0069, iter [03900, 05004], lr: 0.076382, loss: 2.8697
2022-03-06 22:32:15 - train: epoch 0069, iter [04000, 05004], lr: 0.076382, loss: 2.9901
2022-03-06 22:32:49 - train: epoch 0069, iter [04100, 05004], lr: 0.076382, loss: 3.1788
2022-03-06 22:33:23 - train: epoch 0069, iter [04200, 05004], lr: 0.076382, loss: 2.7885
2022-03-06 22:33:56 - train: epoch 0069, iter [04300, 05004], lr: 0.076382, loss: 3.1235
2022-03-06 22:34:30 - train: epoch 0069, iter [04400, 05004], lr: 0.076382, loss: 2.9535
2022-03-06 22:35:03 - train: epoch 0069, iter [04500, 05004], lr: 0.076382, loss: 2.8091
2022-03-06 22:35:37 - train: epoch 0069, iter [04600, 05004], lr: 0.076382, loss: 3.2212
2022-03-06 22:36:10 - train: epoch 0069, iter [04700, 05004], lr: 0.076382, loss: 2.7323
2022-03-06 22:36:44 - train: epoch 0069, iter [04800, 05004], lr: 0.076382, loss: 2.7075
2022-03-06 22:37:18 - train: epoch 0069, iter [04900, 05004], lr: 0.076382, loss: 2.7953
2022-03-06 22:37:50 - train: epoch 0069, iter [05000, 05004], lr: 0.076382, loss: 3.0955
2022-03-06 22:37:51 - train: epoch 069, train_loss: 2.9258
2022-03-06 22:39:05 - eval: epoch: 069, acc1: 54.908%, acc5: 80.202%, test_loss: 1.9119, per_image_load_time: 2.329ms, per_image_inference_time: 0.530ms
2022-03-06 22:39:05 - until epoch: 069, best_acc1: 55.120%
2022-03-06 22:39:05 - epoch 070 lr: 0.07569434058643844
2022-03-06 22:39:45 - train: epoch 0070, iter [00100, 05004], lr: 0.075694, loss: 2.8685
2022-03-06 22:40:18 - train: epoch 0070, iter [00200, 05004], lr: 0.075694, loss: 2.9959
2022-03-06 22:40:52 - train: epoch 0070, iter [00300, 05004], lr: 0.075694, loss: 3.0213
2022-03-06 22:41:25 - train: epoch 0070, iter [00400, 05004], lr: 0.075694, loss: 2.7620
2022-03-06 22:41:59 - train: epoch 0070, iter [00500, 05004], lr: 0.075694, loss: 2.8881
2022-03-06 22:42:33 - train: epoch 0070, iter [00600, 05004], lr: 0.075694, loss: 2.9478
2022-03-06 22:43:06 - train: epoch 0070, iter [00700, 05004], lr: 0.075694, loss: 2.6489
2022-03-06 22:43:40 - train: epoch 0070, iter [00800, 05004], lr: 0.075694, loss: 2.8899
2022-03-06 22:44:13 - train: epoch 0070, iter [00900, 05004], lr: 0.075694, loss: 2.9710
2022-03-06 22:44:47 - train: epoch 0070, iter [01000, 05004], lr: 0.075694, loss: 2.9222
2022-03-06 22:45:21 - train: epoch 0070, iter [01100, 05004], lr: 0.075694, loss: 3.1937
2022-03-06 22:45:55 - train: epoch 0070, iter [01200, 05004], lr: 0.075694, loss: 2.5709
2022-03-06 22:46:29 - train: epoch 0070, iter [01300, 05004], lr: 0.075694, loss: 3.1137
2022-03-06 22:47:02 - train: epoch 0070, iter [01400, 05004], lr: 0.075694, loss: 2.9974
2022-03-06 22:47:35 - train: epoch 0070, iter [01500, 05004], lr: 0.075694, loss: 2.8032
2022-03-06 22:48:09 - train: epoch 0070, iter [01600, 05004], lr: 0.075694, loss: 2.8075
2022-03-06 22:48:42 - train: epoch 0070, iter [01700, 05004], lr: 0.075694, loss: 2.8348
2022-03-06 22:49:16 - train: epoch 0070, iter [01800, 05004], lr: 0.075694, loss: 2.7932
2022-03-06 22:49:50 - train: epoch 0070, iter [01900, 05004], lr: 0.075694, loss: 2.9342
2022-03-06 22:50:23 - train: epoch 0070, iter [02000, 05004], lr: 0.075694, loss: 2.9367
2022-03-06 22:50:57 - train: epoch 0070, iter [02100, 05004], lr: 0.075694, loss: 3.0389
2022-03-06 22:51:31 - train: epoch 0070, iter [02200, 05004], lr: 0.075694, loss: 2.9065
2022-03-06 22:52:04 - train: epoch 0070, iter [02300, 05004], lr: 0.075694, loss: 2.9796
2022-03-06 22:52:38 - train: epoch 0070, iter [02400, 05004], lr: 0.075694, loss: 2.8462
2022-03-06 22:53:12 - train: epoch 0070, iter [02500, 05004], lr: 0.075694, loss: 3.1118
2022-03-06 22:53:45 - train: epoch 0070, iter [02600, 05004], lr: 0.075694, loss: 2.7924
2022-03-06 22:54:19 - train: epoch 0070, iter [02700, 05004], lr: 0.075694, loss: 2.9111
2022-03-06 22:54:52 - train: epoch 0070, iter [02800, 05004], lr: 0.075694, loss: 3.1964
2022-03-06 22:55:26 - train: epoch 0070, iter [02900, 05004], lr: 0.075694, loss: 3.0334
2022-03-06 22:56:00 - train: epoch 0070, iter [03000, 05004], lr: 0.075694, loss: 2.8451
2022-03-06 22:56:33 - train: epoch 0070, iter [03100, 05004], lr: 0.075694, loss: 2.9369
2022-03-06 22:57:06 - train: epoch 0070, iter [03200, 05004], lr: 0.075694, loss: 3.0834
2022-03-06 22:57:39 - train: epoch 0070, iter [03300, 05004], lr: 0.075694, loss: 2.8791
2022-03-06 22:58:14 - train: epoch 0070, iter [03400, 05004], lr: 0.075694, loss: 2.8490
2022-03-06 22:58:47 - train: epoch 0070, iter [03500, 05004], lr: 0.075694, loss: 2.9277
2022-03-06 22:59:22 - train: epoch 0070, iter [03600, 05004], lr: 0.075694, loss: 3.1207
2022-03-06 22:59:55 - train: epoch 0070, iter [03700, 05004], lr: 0.075694, loss: 3.0362
2022-03-06 23:00:28 - train: epoch 0070, iter [03800, 05004], lr: 0.075694, loss: 3.0733
2022-03-06 23:01:01 - train: epoch 0070, iter [03900, 05004], lr: 0.075694, loss: 2.6340
2022-03-06 23:01:34 - train: epoch 0070, iter [04000, 05004], lr: 0.075694, loss: 2.8200
2022-03-06 23:02:08 - train: epoch 0070, iter [04100, 05004], lr: 0.075694, loss: 2.8246
2022-03-06 23:02:42 - train: epoch 0070, iter [04200, 05004], lr: 0.075694, loss: 3.1306
2022-03-06 23:03:15 - train: epoch 0070, iter [04300, 05004], lr: 0.075694, loss: 3.0978
2022-03-06 23:03:49 - train: epoch 0070, iter [04400, 05004], lr: 0.075694, loss: 2.8579
2022-03-06 23:04:22 - train: epoch 0070, iter [04500, 05004], lr: 0.075694, loss: 2.9966
2022-03-06 23:04:55 - train: epoch 0070, iter [04600, 05004], lr: 0.075694, loss: 3.1643
2022-03-06 23:05:28 - train: epoch 0070, iter [04700, 05004], lr: 0.075694, loss: 2.7729
2022-03-06 23:06:02 - train: epoch 0070, iter [04800, 05004], lr: 0.075694, loss: 3.0106
2022-03-06 23:06:36 - train: epoch 0070, iter [04900, 05004], lr: 0.075694, loss: 2.8590
2022-03-06 23:07:08 - train: epoch 0070, iter [05000, 05004], lr: 0.075694, loss: 2.9861
2022-03-06 23:07:09 - train: epoch 070, train_loss: 2.9216
2022-03-06 23:08:23 - eval: epoch: 070, acc1: 54.524%, acc5: 79.832%, test_loss: 1.9287, per_image_load_time: 1.502ms, per_image_inference_time: 0.515ms
2022-03-06 23:08:23 - until epoch: 070, best_acc1: 55.120%
2022-03-06 23:08:23 - epoch 071 lr: 0.07500000000000001
2022-03-06 23:09:03 - train: epoch 0071, iter [00100, 05004], lr: 0.075000, loss: 2.8153
2022-03-06 23:09:35 - train: epoch 0071, iter [00200, 05004], lr: 0.075000, loss: 2.9237
2022-03-06 23:10:09 - train: epoch 0071, iter [00300, 05004], lr: 0.075000, loss: 2.8069
2022-03-06 23:10:42 - train: epoch 0071, iter [00400, 05004], lr: 0.075000, loss: 2.9894
2022-03-06 23:11:16 - train: epoch 0071, iter [00500, 05004], lr: 0.075000, loss: 2.8743
2022-03-06 23:11:50 - train: epoch 0071, iter [00600, 05004], lr: 0.075000, loss: 3.4021
2022-03-06 23:12:23 - train: epoch 0071, iter [00700, 05004], lr: 0.075000, loss: 3.0849
2022-03-06 23:12:57 - train: epoch 0071, iter [00800, 05004], lr: 0.075000, loss: 2.8048
2022-03-06 23:13:30 - train: epoch 0071, iter [00900, 05004], lr: 0.075000, loss: 2.8694
2022-03-06 23:14:04 - train: epoch 0071, iter [01000, 05004], lr: 0.075000, loss: 2.9049
2022-03-06 23:14:38 - train: epoch 0071, iter [01100, 05004], lr: 0.075000, loss: 3.0042
2022-03-06 23:15:11 - train: epoch 0071, iter [01200, 05004], lr: 0.075000, loss: 3.0803
2022-03-06 23:15:45 - train: epoch 0071, iter [01300, 05004], lr: 0.075000, loss: 2.9757
2022-03-06 23:16:19 - train: epoch 0071, iter [01400, 05004], lr: 0.075000, loss: 3.0022
2022-03-06 23:16:51 - train: epoch 0071, iter [01500, 05004], lr: 0.075000, loss: 2.7351
2022-03-06 23:17:25 - train: epoch 0071, iter [01600, 05004], lr: 0.075000, loss: 2.7109
2022-03-06 23:17:59 - train: epoch 0071, iter [01700, 05004], lr: 0.075000, loss: 3.0511
2022-03-06 23:18:32 - train: epoch 0071, iter [01800, 05004], lr: 0.075000, loss: 3.1879
2022-03-06 23:19:05 - train: epoch 0071, iter [01900, 05004], lr: 0.075000, loss: 2.8344
2022-03-06 23:19:39 - train: epoch 0071, iter [02000, 05004], lr: 0.075000, loss: 2.8746
2022-03-06 23:20:13 - train: epoch 0071, iter [02100, 05004], lr: 0.075000, loss: 2.9220
2022-03-06 23:20:46 - train: epoch 0071, iter [02200, 05004], lr: 0.075000, loss: 2.7633
2022-03-06 23:21:20 - train: epoch 0071, iter [02300, 05004], lr: 0.075000, loss: 3.0996
2022-03-06 23:21:54 - train: epoch 0071, iter [02400, 05004], lr: 0.075000, loss: 2.7249
2022-03-06 23:22:27 - train: epoch 0071, iter [02500, 05004], lr: 0.075000, loss: 3.1120
2022-03-06 23:23:01 - train: epoch 0071, iter [02600, 05004], lr: 0.075000, loss: 2.8299
2022-03-06 23:23:35 - train: epoch 0071, iter [02700, 05004], lr: 0.075000, loss: 2.8744
2022-03-06 23:24:08 - train: epoch 0071, iter [02800, 05004], lr: 0.075000, loss: 3.0927
2022-03-06 23:24:42 - train: epoch 0071, iter [02900, 05004], lr: 0.075000, loss: 2.8110
2022-03-06 23:25:16 - train: epoch 0071, iter [03000, 05004], lr: 0.075000, loss: 2.8841
2022-03-06 23:25:50 - train: epoch 0071, iter [03100, 05004], lr: 0.075000, loss: 2.5920
2022-03-06 23:26:23 - train: epoch 0071, iter [03200, 05004], lr: 0.075000, loss: 2.6153
2022-03-06 23:26:55 - train: epoch 0071, iter [03300, 05004], lr: 0.075000, loss: 3.0731
2022-03-06 23:27:29 - train: epoch 0071, iter [03400, 05004], lr: 0.075000, loss: 2.9747
2022-03-06 23:28:03 - train: epoch 0071, iter [03500, 05004], lr: 0.075000, loss: 2.9721
2022-03-06 23:28:36 - train: epoch 0071, iter [03600, 05004], lr: 0.075000, loss: 3.0939
2022-03-06 23:29:10 - train: epoch 0071, iter [03700, 05004], lr: 0.075000, loss: 2.8457
2022-03-06 23:29:45 - train: epoch 0071, iter [03800, 05004], lr: 0.075000, loss: 2.8235
2022-03-06 23:30:17 - train: epoch 0071, iter [03900, 05004], lr: 0.075000, loss: 3.0340
2022-03-06 23:30:51 - train: epoch 0071, iter [04000, 05004], lr: 0.075000, loss: 3.0004
2022-03-06 23:31:25 - train: epoch 0071, iter [04100, 05004], lr: 0.075000, loss: 3.0862
2022-03-06 23:31:58 - train: epoch 0071, iter [04200, 05004], lr: 0.075000, loss: 3.1273
2022-03-06 23:32:31 - train: epoch 0071, iter [04300, 05004], lr: 0.075000, loss: 2.9685
2022-03-06 23:33:05 - train: epoch 0071, iter [04400, 05004], lr: 0.075000, loss: 3.0718
2022-03-06 23:33:39 - train: epoch 0071, iter [04500, 05004], lr: 0.075000, loss: 2.9570
2022-03-06 23:34:12 - train: epoch 0071, iter [04600, 05004], lr: 0.075000, loss: 2.8559
2022-03-06 23:34:46 - train: epoch 0071, iter [04700, 05004], lr: 0.075000, loss: 2.6463
2022-03-06 23:35:19 - train: epoch 0071, iter [04800, 05004], lr: 0.075000, loss: 2.8030
2022-03-06 23:35:54 - train: epoch 0071, iter [04900, 05004], lr: 0.075000, loss: 2.6279
2022-03-06 23:36:26 - train: epoch 0071, iter [05000, 05004], lr: 0.075000, loss: 2.7078
2022-03-06 23:36:27 - train: epoch 071, train_loss: 2.9172
2022-03-06 23:37:40 - eval: epoch: 071, acc1: 55.984%, acc5: 80.800%, test_loss: 1.8590, per_image_load_time: 1.840ms, per_image_inference_time: 0.528ms
2022-03-06 23:37:41 - until epoch: 071, best_acc1: 55.984%
2022-03-06 23:37:41 - epoch 072 lr: 0.0742991706621303
2022-03-06 23:38:20 - train: epoch 0072, iter [00100, 05004], lr: 0.074299, loss: 3.0361
2022-03-06 23:38:54 - train: epoch 0072, iter [00200, 05004], lr: 0.074299, loss: 2.6440
2022-03-06 23:39:27 - train: epoch 0072, iter [00300, 05004], lr: 0.074299, loss: 2.6426
2022-03-06 23:40:00 - train: epoch 0072, iter [00400, 05004], lr: 0.074299, loss: 2.8430
2022-03-06 23:40:34 - train: epoch 0072, iter [00500, 05004], lr: 0.074299, loss: 2.9206
2022-03-06 23:41:07 - train: epoch 0072, iter [00600, 05004], lr: 0.074299, loss: 2.8267
2022-03-06 23:41:41 - train: epoch 0072, iter [00700, 05004], lr: 0.074299, loss: 2.7590
2022-03-06 23:42:14 - train: epoch 0072, iter [00800, 05004], lr: 0.074299, loss: 3.0024
2022-03-06 23:42:48 - train: epoch 0072, iter [00900, 05004], lr: 0.074299, loss: 2.8573
2022-03-06 23:43:22 - train: epoch 0072, iter [01000, 05004], lr: 0.074299, loss: 2.7704
2022-03-06 23:43:55 - train: epoch 0072, iter [01100, 05004], lr: 0.074299, loss: 3.1234
2022-03-06 23:44:29 - train: epoch 0072, iter [01200, 05004], lr: 0.074299, loss: 3.0130
2022-03-06 23:45:03 - train: epoch 0072, iter [01300, 05004], lr: 0.074299, loss: 2.8200
2022-03-06 23:45:37 - train: epoch 0072, iter [01400, 05004], lr: 0.074299, loss: 3.1109
2022-03-06 23:46:10 - train: epoch 0072, iter [01500, 05004], lr: 0.074299, loss: 2.8435
2022-03-06 23:46:44 - train: epoch 0072, iter [01600, 05004], lr: 0.074299, loss: 3.0473
2022-03-06 23:47:17 - train: epoch 0072, iter [01700, 05004], lr: 0.074299, loss: 2.7245
2022-03-06 23:47:51 - train: epoch 0072, iter [01800, 05004], lr: 0.074299, loss: 2.6984
2022-03-06 23:48:24 - train: epoch 0072, iter [01900, 05004], lr: 0.074299, loss: 2.7299
2022-03-06 23:48:58 - train: epoch 0072, iter [02000, 05004], lr: 0.074299, loss: 3.1386
2022-03-06 23:49:31 - train: epoch 0072, iter [02100, 05004], lr: 0.074299, loss: 2.8633
2022-03-06 23:50:05 - train: epoch 0072, iter [02200, 05004], lr: 0.074299, loss: 3.1215
2022-03-06 23:50:38 - train: epoch 0072, iter [02300, 05004], lr: 0.074299, loss: 2.9994
2022-03-06 23:51:12 - train: epoch 0072, iter [02400, 05004], lr: 0.074299, loss: 2.7493
2022-03-06 23:51:46 - train: epoch 0072, iter [02500, 05004], lr: 0.074299, loss: 2.7411
2022-03-06 23:52:20 - train: epoch 0072, iter [02600, 05004], lr: 0.074299, loss: 2.8095
2022-03-06 23:52:53 - train: epoch 0072, iter [02700, 05004], lr: 0.074299, loss: 3.0165
2022-03-06 23:53:27 - train: epoch 0072, iter [02800, 05004], lr: 0.074299, loss: 3.0334
2022-03-06 23:54:00 - train: epoch 0072, iter [02900, 05004], lr: 0.074299, loss: 3.0112
2022-03-06 23:54:34 - train: epoch 0072, iter [03000, 05004], lr: 0.074299, loss: 2.9257
2022-03-06 23:55:08 - train: epoch 0072, iter [03100, 05004], lr: 0.074299, loss: 3.1921
2022-03-06 23:55:41 - train: epoch 0072, iter [03200, 05004], lr: 0.074299, loss: 2.7616
2022-03-06 23:56:15 - train: epoch 0072, iter [03300, 05004], lr: 0.074299, loss: 2.6701
2022-03-06 23:56:48 - train: epoch 0072, iter [03400, 05004], lr: 0.074299, loss: 3.0148
2022-03-06 23:57:21 - train: epoch 0072, iter [03500, 05004], lr: 0.074299, loss: 2.9199
2022-03-06 23:57:55 - train: epoch 0072, iter [03600, 05004], lr: 0.074299, loss: 2.7996
2022-03-06 23:58:29 - train: epoch 0072, iter [03700, 05004], lr: 0.074299, loss: 2.9669
2022-03-06 23:59:03 - train: epoch 0072, iter [03800, 05004], lr: 0.074299, loss: 2.8473
2022-03-06 23:59:36 - train: epoch 0072, iter [03900, 05004], lr: 0.074299, loss: 3.0434
2022-03-07 00:00:09 - train: epoch 0072, iter [04000, 05004], lr: 0.074299, loss: 2.9915
2022-03-07 00:00:43 - train: epoch 0072, iter [04100, 05004], lr: 0.074299, loss: 3.0707
2022-03-07 00:01:16 - train: epoch 0072, iter [04200, 05004], lr: 0.074299, loss: 2.7778
2022-03-07 00:01:50 - train: epoch 0072, iter [04300, 05004], lr: 0.074299, loss: 2.6225
2022-03-07 00:02:24 - train: epoch 0072, iter [04400, 05004], lr: 0.074299, loss: 2.7897
2022-03-07 00:02:57 - train: epoch 0072, iter [04500, 05004], lr: 0.074299, loss: 3.1679
2022-03-07 00:03:30 - train: epoch 0072, iter [04600, 05004], lr: 0.074299, loss: 3.0820
2022-03-07 00:04:04 - train: epoch 0072, iter [04700, 05004], lr: 0.074299, loss: 2.8081
2022-03-07 00:04:38 - train: epoch 0072, iter [04800, 05004], lr: 0.074299, loss: 2.9576
2022-03-07 00:05:10 - train: epoch 0072, iter [04900, 05004], lr: 0.074299, loss: 2.9182
2022-03-07 00:05:43 - train: epoch 0072, iter [05000, 05004], lr: 0.074299, loss: 2.9437
2022-03-07 00:05:44 - train: epoch 072, train_loss: 2.9128
2022-03-07 00:06:57 - eval: epoch: 072, acc1: 54.680%, acc5: 79.846%, test_loss: 1.9217, per_image_load_time: 1.838ms, per_image_inference_time: 0.525ms
2022-03-07 00:06:57 - until epoch: 072, best_acc1: 55.984%
2022-03-07 00:06:57 - epoch 073 lr: 0.07359203447312411
2022-03-07 00:07:35 - train: epoch 0073, iter [00100, 05004], lr: 0.073592, loss: 3.0188
2022-03-07 00:08:10 - train: epoch 0073, iter [00200, 05004], lr: 0.073592, loss: 2.9359
2022-03-07 00:08:44 - train: epoch 0073, iter [00300, 05004], lr: 0.073592, loss: 2.9736
2022-03-07 00:09:17 - train: epoch 0073, iter [00400, 05004], lr: 0.073592, loss: 2.6786
2022-03-07 00:09:51 - train: epoch 0073, iter [00500, 05004], lr: 0.073592, loss: 2.7744
2022-03-07 00:10:25 - train: epoch 0073, iter [00600, 05004], lr: 0.073592, loss: 2.8294
2022-03-07 00:10:59 - train: epoch 0073, iter [00700, 05004], lr: 0.073592, loss: 2.9248
2022-03-07 00:11:33 - train: epoch 0073, iter [00800, 05004], lr: 0.073592, loss: 2.9011
2022-03-07 00:12:06 - train: epoch 0073, iter [00900, 05004], lr: 0.073592, loss: 2.5708
2022-03-07 00:12:40 - train: epoch 0073, iter [01000, 05004], lr: 0.073592, loss: 2.9000
2022-03-07 00:13:13 - train: epoch 0073, iter [01100, 05004], lr: 0.073592, loss: 2.9808
2022-03-07 00:13:47 - train: epoch 0073, iter [01200, 05004], lr: 0.073592, loss: 2.8734
2022-03-07 00:14:21 - train: epoch 0073, iter [01300, 05004], lr: 0.073592, loss: 2.9726
2022-03-07 00:14:54 - train: epoch 0073, iter [01400, 05004], lr: 0.073592, loss: 2.8413
2022-03-07 00:15:29 - train: epoch 0073, iter [01500, 05004], lr: 0.073592, loss: 2.7871
2022-03-07 00:16:02 - train: epoch 0073, iter [01600, 05004], lr: 0.073592, loss: 2.9607
2022-03-07 00:16:36 - train: epoch 0073, iter [01700, 05004], lr: 0.073592, loss: 3.3243
2022-03-07 00:17:09 - train: epoch 0073, iter [01800, 05004], lr: 0.073592, loss: 2.7380
2022-03-07 00:17:43 - train: epoch 0073, iter [01900, 05004], lr: 0.073592, loss: 2.9609
2022-03-07 00:18:17 - train: epoch 0073, iter [02000, 05004], lr: 0.073592, loss: 2.5366
2022-03-07 00:18:51 - train: epoch 0073, iter [02100, 05004], lr: 0.073592, loss: 2.8538
2022-03-07 00:19:24 - train: epoch 0073, iter [02200, 05004], lr: 0.073592, loss: 3.0727
2022-03-07 00:19:58 - train: epoch 0073, iter [02300, 05004], lr: 0.073592, loss: 3.1541
2022-03-07 00:20:32 - train: epoch 0073, iter [02400, 05004], lr: 0.073592, loss: 3.0794
2022-03-07 00:21:06 - train: epoch 0073, iter [02500, 05004], lr: 0.073592, loss: 3.1996
2022-03-07 00:21:39 - train: epoch 0073, iter [02600, 05004], lr: 0.073592, loss: 2.9066
2022-03-07 00:22:13 - train: epoch 0073, iter [02700, 05004], lr: 0.073592, loss: 2.9376
2022-03-07 00:22:47 - train: epoch 0073, iter [02800, 05004], lr: 0.073592, loss: 3.0893
2022-03-07 00:23:20 - train: epoch 0073, iter [02900, 05004], lr: 0.073592, loss: 2.8038
2022-03-07 00:23:54 - train: epoch 0073, iter [03000, 05004], lr: 0.073592, loss: 2.7704
2022-03-07 00:24:28 - train: epoch 0073, iter [03100, 05004], lr: 0.073592, loss: 2.8820
2022-03-07 00:25:02 - train: epoch 0073, iter [03200, 05004], lr: 0.073592, loss: 2.7537
2022-03-07 00:25:35 - train: epoch 0073, iter [03300, 05004], lr: 0.073592, loss: 2.9466
2022-03-07 00:26:09 - train: epoch 0073, iter [03400, 05004], lr: 0.073592, loss: 2.8179
2022-03-07 00:26:42 - train: epoch 0073, iter [03500, 05004], lr: 0.073592, loss: 2.9059
2022-03-07 00:27:16 - train: epoch 0073, iter [03600, 05004], lr: 0.073592, loss: 2.7069
2022-03-07 00:27:50 - train: epoch 0073, iter [03700, 05004], lr: 0.073592, loss: 2.9010
2022-03-07 00:28:24 - train: epoch 0073, iter [03800, 05004], lr: 0.073592, loss: 3.0247
2022-03-07 00:28:58 - train: epoch 0073, iter [03900, 05004], lr: 0.073592, loss: 2.9962
2022-03-07 00:29:30 - train: epoch 0073, iter [04000, 05004], lr: 0.073592, loss: 3.0964
2022-03-07 00:30:04 - train: epoch 0073, iter [04100, 05004], lr: 0.073592, loss: 2.8650
2022-03-07 00:30:38 - train: epoch 0073, iter [04200, 05004], lr: 0.073592, loss: 3.1107
2022-03-07 00:31:11 - train: epoch 0073, iter [04300, 05004], lr: 0.073592, loss: 2.9910
2022-03-07 00:31:45 - train: epoch 0073, iter [04400, 05004], lr: 0.073592, loss: 3.0777
2022-03-07 00:32:19 - train: epoch 0073, iter [04500, 05004], lr: 0.073592, loss: 2.7406
2022-03-07 00:32:52 - train: epoch 0073, iter [04600, 05004], lr: 0.073592, loss: 3.1121
2022-03-07 00:33:26 - train: epoch 0073, iter [04700, 05004], lr: 0.073592, loss: 2.7473
2022-03-07 00:34:00 - train: epoch 0073, iter [04800, 05004], lr: 0.073592, loss: 3.0200
2022-03-07 00:34:33 - train: epoch 0073, iter [04900, 05004], lr: 0.073592, loss: 2.6356
2022-03-07 00:35:05 - train: epoch 0073, iter [05000, 05004], lr: 0.073592, loss: 2.9172
2022-03-07 00:35:06 - train: epoch 073, train_loss: 2.9087
2022-03-07 00:36:20 - eval: epoch: 073, acc1: 55.178%, acc5: 79.926%, test_loss: 1.9079, per_image_load_time: 0.841ms, per_image_inference_time: 0.499ms
2022-03-07 00:36:21 - until epoch: 073, best_acc1: 55.984%
2022-03-07 00:36:21 - epoch 074 lr: 0.07287877497021977
2022-03-07 00:37:00 - train: epoch 0074, iter [00100, 05004], lr: 0.072879, loss: 2.8383
2022-03-07 00:37:33 - train: epoch 0074, iter [00200, 05004], lr: 0.072879, loss: 2.8230
2022-03-07 00:38:07 - train: epoch 0074, iter [00300, 05004], lr: 0.072879, loss: 2.8673
2022-03-07 00:38:41 - train: epoch 0074, iter [00400, 05004], lr: 0.072879, loss: 3.2261
2022-03-07 00:39:14 - train: epoch 0074, iter [00500, 05004], lr: 0.072879, loss: 2.7792
2022-03-07 00:39:48 - train: epoch 0074, iter [00600, 05004], lr: 0.072879, loss: 2.8762
2022-03-07 00:40:22 - train: epoch 0074, iter [00700, 05004], lr: 0.072879, loss: 2.9110
2022-03-07 00:40:55 - train: epoch 0074, iter [00800, 05004], lr: 0.072879, loss: 2.7803
2022-03-07 00:41:30 - train: epoch 0074, iter [00900, 05004], lr: 0.072879, loss: 2.7287
2022-03-07 00:42:03 - train: epoch 0074, iter [01000, 05004], lr: 0.072879, loss: 3.0454
2022-03-07 00:42:37 - train: epoch 0074, iter [01100, 05004], lr: 0.072879, loss: 3.0783
2022-03-07 00:43:11 - train: epoch 0074, iter [01200, 05004], lr: 0.072879, loss: 2.7556
2022-03-07 00:43:44 - train: epoch 0074, iter [01300, 05004], lr: 0.072879, loss: 3.0871
2022-03-07 00:44:18 - train: epoch 0074, iter [01400, 05004], lr: 0.072879, loss: 2.8630
2022-03-07 00:44:53 - train: epoch 0074, iter [01500, 05004], lr: 0.072879, loss: 3.1613
2022-03-07 00:45:27 - train: epoch 0074, iter [01600, 05004], lr: 0.072879, loss: 2.7161
2022-03-07 00:46:01 - train: epoch 0074, iter [01700, 05004], lr: 0.072879, loss: 2.9508
2022-03-07 00:46:34 - train: epoch 0074, iter [01800, 05004], lr: 0.072879, loss: 3.1228
2022-03-07 00:47:08 - train: epoch 0074, iter [01900, 05004], lr: 0.072879, loss: 3.0982
2022-03-07 00:47:42 - train: epoch 0074, iter [02000, 05004], lr: 0.072879, loss: 2.5306
2022-03-07 00:48:16 - train: epoch 0074, iter [02100, 05004], lr: 0.072879, loss: 2.7797
2022-03-07 00:48:50 - train: epoch 0074, iter [02200, 05004], lr: 0.072879, loss: 3.0611
2022-03-07 00:49:23 - train: epoch 0074, iter [02300, 05004], lr: 0.072879, loss: 3.0614
2022-03-07 00:49:57 - train: epoch 0074, iter [02400, 05004], lr: 0.072879, loss: 3.0724
2022-03-07 00:50:30 - train: epoch 0074, iter [02500, 05004], lr: 0.072879, loss: 2.6105
2022-03-07 00:51:04 - train: epoch 0074, iter [02600, 05004], lr: 0.072879, loss: 2.6962
2022-03-07 00:51:37 - train: epoch 0074, iter [02700, 05004], lr: 0.072879, loss: 2.8214
2022-03-07 00:52:11 - train: epoch 0074, iter [02800, 05004], lr: 0.072879, loss: 3.1904
2022-03-07 00:52:45 - train: epoch 0074, iter [02900, 05004], lr: 0.072879, loss: 2.9576
2022-03-07 00:53:18 - train: epoch 0074, iter [03000, 05004], lr: 0.072879, loss: 2.9030
2022-03-07 00:53:52 - train: epoch 0074, iter [03100, 05004], lr: 0.072879, loss: 2.8088
2022-03-07 00:54:25 - train: epoch 0074, iter [03200, 05004], lr: 0.072879, loss: 2.8219
2022-03-07 00:54:59 - train: epoch 0074, iter [03300, 05004], lr: 0.072879, loss: 2.9751
2022-03-07 00:55:33 - train: epoch 0074, iter [03400, 05004], lr: 0.072879, loss: 3.1153
2022-03-07 00:56:06 - train: epoch 0074, iter [03500, 05004], lr: 0.072879, loss: 2.9167
2022-03-07 00:56:40 - train: epoch 0074, iter [03600, 05004], lr: 0.072879, loss: 2.9963
2022-03-07 00:57:14 - train: epoch 0074, iter [03700, 05004], lr: 0.072879, loss: 2.9643
2022-03-07 00:57:48 - train: epoch 0074, iter [03800, 05004], lr: 0.072879, loss: 2.8850
2022-03-07 00:58:21 - train: epoch 0074, iter [03900, 05004], lr: 0.072879, loss: 2.6780
2022-03-07 00:58:56 - train: epoch 0074, iter [04000, 05004], lr: 0.072879, loss: 2.9786
2022-03-07 00:59:29 - train: epoch 0074, iter [04100, 05004], lr: 0.072879, loss: 3.1242
2022-03-07 01:00:03 - train: epoch 0074, iter [04200, 05004], lr: 0.072879, loss: 3.0510
2022-03-07 01:00:36 - train: epoch 0074, iter [04300, 05004], lr: 0.072879, loss: 2.9233
2022-03-07 01:01:10 - train: epoch 0074, iter [04400, 05004], lr: 0.072879, loss: 2.8343
2022-03-07 01:01:44 - train: epoch 0074, iter [04500, 05004], lr: 0.072879, loss: 2.6188
2022-03-07 01:02:18 - train: epoch 0074, iter [04600, 05004], lr: 0.072879, loss: 3.1921
2022-03-07 01:02:52 - train: epoch 0074, iter [04700, 05004], lr: 0.072879, loss: 2.8676
2022-03-07 01:03:25 - train: epoch 0074, iter [04800, 05004], lr: 0.072879, loss: 2.8399
2022-03-07 01:03:58 - train: epoch 0074, iter [04900, 05004], lr: 0.072879, loss: 3.0235
2022-03-07 01:04:31 - train: epoch 0074, iter [05000, 05004], lr: 0.072879, loss: 2.9700
2022-03-07 01:04:32 - train: epoch 074, train_loss: 2.9022
2022-03-07 01:05:46 - eval: epoch: 074, acc1: 56.048%, acc5: 80.796%, test_loss: 1.8571, per_image_load_time: 2.364ms, per_image_inference_time: 0.498ms
2022-03-07 01:05:47 - until epoch: 074, best_acc1: 56.048%
2022-03-07 01:05:47 - epoch 075 lr: 0.07215957727996207
2022-03-07 01:06:24 - train: epoch 0075, iter [00100, 05004], lr: 0.072160, loss: 2.9961
2022-03-07 01:06:59 - train: epoch 0075, iter [00200, 05004], lr: 0.072160, loss: 2.9386
2022-03-07 01:07:32 - train: epoch 0075, iter [00300, 05004], lr: 0.072160, loss: 2.9760
2022-03-07 01:08:06 - train: epoch 0075, iter [00400, 05004], lr: 0.072160, loss: 2.9855
2022-03-07 01:08:39 - train: epoch 0075, iter [00500, 05004], lr: 0.072160, loss: 2.8352
2022-03-07 01:09:13 - train: epoch 0075, iter [00600, 05004], lr: 0.072160, loss: 2.7232
2022-03-07 01:09:46 - train: epoch 0075, iter [00700, 05004], lr: 0.072160, loss: 3.1889
2022-03-07 01:10:20 - train: epoch 0075, iter [00800, 05004], lr: 0.072160, loss: 3.0283
2022-03-07 01:10:54 - train: epoch 0075, iter [00900, 05004], lr: 0.072160, loss: 2.9747
2022-03-07 01:11:27 - train: epoch 0075, iter [01000, 05004], lr: 0.072160, loss: 2.6265
2022-03-07 01:12:02 - train: epoch 0075, iter [01100, 05004], lr: 0.072160, loss: 2.9499
2022-03-07 01:12:35 - train: epoch 0075, iter [01200, 05004], lr: 0.072160, loss: 2.8559
2022-03-07 01:13:08 - train: epoch 0075, iter [01300, 05004], lr: 0.072160, loss: 2.8273
2022-03-07 01:13:41 - train: epoch 0075, iter [01400, 05004], lr: 0.072160, loss: 2.8445
2022-03-07 01:14:15 - train: epoch 0075, iter [01500, 05004], lr: 0.072160, loss: 3.0740
2022-03-07 01:14:49 - train: epoch 0075, iter [01600, 05004], lr: 0.072160, loss: 2.6860
2022-03-07 01:15:23 - train: epoch 0075, iter [01700, 05004], lr: 0.072160, loss: 2.8676
2022-03-07 01:15:56 - train: epoch 0075, iter [01800, 05004], lr: 0.072160, loss: 3.0584
2022-03-07 01:16:29 - train: epoch 0075, iter [01900, 05004], lr: 0.072160, loss: 2.7304
2022-03-07 01:17:03 - train: epoch 0075, iter [02000, 05004], lr: 0.072160, loss: 2.7723
2022-03-07 01:17:37 - train: epoch 0075, iter [02100, 05004], lr: 0.072160, loss: 2.8134
2022-03-07 01:18:10 - train: epoch 0075, iter [02200, 05004], lr: 0.072160, loss: 2.9847
2022-03-07 01:18:44 - train: epoch 0075, iter [02300, 05004], lr: 0.072160, loss: 3.0095
2022-03-07 01:19:18 - train: epoch 0075, iter [02400, 05004], lr: 0.072160, loss: 2.9974
2022-03-07 01:19:51 - train: epoch 0075, iter [02500, 05004], lr: 0.072160, loss: 2.8331
2022-03-07 01:20:25 - train: epoch 0075, iter [02600, 05004], lr: 0.072160, loss: 2.8573
2022-03-07 01:20:58 - train: epoch 0075, iter [02700, 05004], lr: 0.072160, loss: 2.6439
2022-03-07 01:21:32 - train: epoch 0075, iter [02800, 05004], lr: 0.072160, loss: 3.0011
2022-03-07 01:22:05 - train: epoch 0075, iter [02900, 05004], lr: 0.072160, loss: 3.0047
2022-03-07 01:22:39 - train: epoch 0075, iter [03000, 05004], lr: 0.072160, loss: 3.2583
2022-03-07 01:23:12 - train: epoch 0075, iter [03100, 05004], lr: 0.072160, loss: 2.9927
2022-03-07 01:23:46 - train: epoch 0075, iter [03200, 05004], lr: 0.072160, loss: 2.7994
2022-03-07 01:24:19 - train: epoch 0075, iter [03300, 05004], lr: 0.072160, loss: 3.1003
2022-03-07 01:24:53 - train: epoch 0075, iter [03400, 05004], lr: 0.072160, loss: 2.7524
2022-03-07 01:25:26 - train: epoch 0075, iter [03500, 05004], lr: 0.072160, loss: 2.7520
2022-03-07 01:26:00 - train: epoch 0075, iter [03600, 05004], lr: 0.072160, loss: 3.0179
2022-03-07 01:26:33 - train: epoch 0075, iter [03700, 05004], lr: 0.072160, loss: 2.8681
2022-03-07 01:27:06 - train: epoch 0075, iter [03800, 05004], lr: 0.072160, loss: 2.8697
2022-03-07 01:27:40 - train: epoch 0075, iter [03900, 05004], lr: 0.072160, loss: 3.1687
2022-03-07 01:28:14 - train: epoch 0075, iter [04000, 05004], lr: 0.072160, loss: 2.5951
2022-03-07 01:28:47 - train: epoch 0075, iter [04100, 05004], lr: 0.072160, loss: 2.7449
2022-03-07 01:29:20 - train: epoch 0075, iter [04200, 05004], lr: 0.072160, loss: 2.8358
2022-03-07 01:29:54 - train: epoch 0075, iter [04300, 05004], lr: 0.072160, loss: 3.2330
2022-03-07 01:30:27 - train: epoch 0075, iter [04400, 05004], lr: 0.072160, loss: 2.3685
2022-03-07 01:31:01 - train: epoch 0075, iter [04500, 05004], lr: 0.072160, loss: 2.7941
2022-03-07 01:31:34 - train: epoch 0075, iter [04600, 05004], lr: 0.072160, loss: 2.8232
2022-03-07 01:32:09 - train: epoch 0075, iter [04700, 05004], lr: 0.072160, loss: 3.1043
2022-03-07 01:32:42 - train: epoch 0075, iter [04800, 05004], lr: 0.072160, loss: 3.2570
2022-03-07 01:33:15 - train: epoch 0075, iter [04900, 05004], lr: 0.072160, loss: 2.7611
2022-03-07 01:33:47 - train: epoch 0075, iter [05000, 05004], lr: 0.072160, loss: 2.8002
2022-03-07 01:33:48 - train: epoch 075, train_loss: 2.8984
2022-03-07 01:35:02 - eval: epoch: 075, acc1: 56.556%, acc5: 81.270%, test_loss: 1.8247, per_image_load_time: 1.858ms, per_image_inference_time: 0.534ms
2022-03-07 01:35:03 - until epoch: 075, best_acc1: 56.556%
2022-03-07 01:35:03 - epoch 076 lr: 0.0714346280701527
2022-03-07 01:35:42 - train: epoch 0076, iter [00100, 05004], lr: 0.071435, loss: 2.6569
2022-03-07 01:36:16 - train: epoch 0076, iter [00200, 05004], lr: 0.071435, loss: 2.8163
2022-03-07 01:36:48 - train: epoch 0076, iter [00300, 05004], lr: 0.071435, loss: 2.7480
2022-03-07 01:37:22 - train: epoch 0076, iter [00400, 05004], lr: 0.071435, loss: 2.9533
2022-03-07 01:37:56 - train: epoch 0076, iter [00500, 05004], lr: 0.071435, loss: 2.8901
2022-03-07 01:38:30 - train: epoch 0076, iter [00600, 05004], lr: 0.071435, loss: 2.8848
2022-03-07 01:39:04 - train: epoch 0076, iter [00700, 05004], lr: 0.071435, loss: 2.9179
2022-03-07 01:39:38 - train: epoch 0076, iter [00800, 05004], lr: 0.071435, loss: 2.8728
2022-03-07 01:40:11 - train: epoch 0076, iter [00900, 05004], lr: 0.071435, loss: 2.6933
2022-03-07 01:40:45 - train: epoch 0076, iter [01000, 05004], lr: 0.071435, loss: 2.6301
2022-03-07 01:41:17 - train: epoch 0076, iter [01100, 05004], lr: 0.071435, loss: 2.6604
2022-03-07 01:41:52 - train: epoch 0076, iter [01200, 05004], lr: 0.071435, loss: 3.0101
2022-03-07 01:42:25 - train: epoch 0076, iter [01300, 05004], lr: 0.071435, loss: 2.8699
2022-03-07 01:42:59 - train: epoch 0076, iter [01400, 05004], lr: 0.071435, loss: 2.8266
2022-03-07 01:43:32 - train: epoch 0076, iter [01500, 05004], lr: 0.071435, loss: 2.9135
2022-03-07 01:44:06 - train: epoch 0076, iter [01600, 05004], lr: 0.071435, loss: 2.7572
2022-03-07 01:44:39 - train: epoch 0076, iter [01700, 05004], lr: 0.071435, loss: 2.7018
2022-03-07 01:45:13 - train: epoch 0076, iter [01800, 05004], lr: 0.071435, loss: 2.7332
2022-03-07 01:45:47 - train: epoch 0076, iter [01900, 05004], lr: 0.071435, loss: 2.9900
2022-03-07 01:46:20 - train: epoch 0076, iter [02000, 05004], lr: 0.071435, loss: 2.8537
2022-03-07 01:46:53 - train: epoch 0076, iter [02100, 05004], lr: 0.071435, loss: 2.7363
2022-03-07 01:47:27 - train: epoch 0076, iter [02200, 05004], lr: 0.071435, loss: 2.8290
2022-03-07 01:48:01 - train: epoch 0076, iter [02300, 05004], lr: 0.071435, loss: 2.7679
2022-03-07 01:48:34 - train: epoch 0076, iter [02400, 05004], lr: 0.071435, loss: 3.1776
2022-03-07 01:49:07 - train: epoch 0076, iter [02500, 05004], lr: 0.071435, loss: 2.6326
2022-03-07 01:49:41 - train: epoch 0076, iter [02600, 05004], lr: 0.071435, loss: 3.0963
2022-03-07 01:50:15 - train: epoch 0076, iter [02700, 05004], lr: 0.071435, loss: 3.1732
2022-03-07 01:50:48 - train: epoch 0076, iter [02800, 05004], lr: 0.071435, loss: 2.6763
2022-03-07 01:51:22 - train: epoch 0076, iter [02900, 05004], lr: 0.071435, loss: 2.9344
2022-03-07 01:51:56 - train: epoch 0076, iter [03000, 05004], lr: 0.071435, loss: 2.6556
2022-03-07 01:52:30 - train: epoch 0076, iter [03100, 05004], lr: 0.071435, loss: 2.8950
2022-03-07 01:53:03 - train: epoch 0076, iter [03200, 05004], lr: 0.071435, loss: 2.8825
2022-03-07 01:53:38 - train: epoch 0076, iter [03300, 05004], lr: 0.071435, loss: 2.6513
2022-03-07 01:54:10 - train: epoch 0076, iter [03400, 05004], lr: 0.071435, loss: 3.1004
2022-03-07 01:54:43 - train: epoch 0076, iter [03500, 05004], lr: 0.071435, loss: 2.8120
2022-03-07 01:55:17 - train: epoch 0076, iter [03600, 05004], lr: 0.071435, loss: 2.7869
2022-03-07 01:55:51 - train: epoch 0076, iter [03700, 05004], lr: 0.071435, loss: 2.8208
2022-03-07 01:56:24 - train: epoch 0076, iter [03800, 05004], lr: 0.071435, loss: 2.6415
2022-03-07 01:56:58 - train: epoch 0076, iter [03900, 05004], lr: 0.071435, loss: 2.9885
2022-03-07 01:57:32 - train: epoch 0076, iter [04000, 05004], lr: 0.071435, loss: 2.9050
2022-03-07 01:58:06 - train: epoch 0076, iter [04100, 05004], lr: 0.071435, loss: 2.8807
2022-03-07 01:58:39 - train: epoch 0076, iter [04200, 05004], lr: 0.071435, loss: 3.0086
2022-03-07 01:59:13 - train: epoch 0076, iter [04300, 05004], lr: 0.071435, loss: 2.9642
2022-03-07 01:59:47 - train: epoch 0076, iter [04400, 05004], lr: 0.071435, loss: 2.5598
2022-03-07 02:00:20 - train: epoch 0076, iter [04500, 05004], lr: 0.071435, loss: 2.9256
2022-03-07 02:00:54 - train: epoch 0076, iter [04600, 05004], lr: 0.071435, loss: 2.8426
2022-03-07 02:01:27 - train: epoch 0076, iter [04700, 05004], lr: 0.071435, loss: 2.8479
2022-03-07 02:02:01 - train: epoch 0076, iter [04800, 05004], lr: 0.071435, loss: 2.6364
2022-03-07 02:02:34 - train: epoch 0076, iter [04900, 05004], lr: 0.071435, loss: 2.8383
2022-03-07 02:03:07 - train: epoch 0076, iter [05000, 05004], lr: 0.071435, loss: 3.1079
2022-03-07 02:03:08 - train: epoch 076, train_loss: 2.8945
2022-03-07 02:04:22 - eval: epoch: 076, acc1: 54.304%, acc5: 79.232%, test_loss: 1.9484, per_image_load_time: 1.936ms, per_image_inference_time: 0.509ms
2022-03-07 02:04:22 - until epoch: 076, best_acc1: 56.556%
2022-03-07 02:04:22 - epoch 077 lr: 0.0707041155014006
2022-03-07 02:05:02 - train: epoch 0077, iter [00100, 05004], lr: 0.070704, loss: 2.9553
2022-03-07 02:05:35 - train: epoch 0077, iter [00200, 05004], lr: 0.070704, loss: 2.8536
2022-03-07 02:06:09 - train: epoch 0077, iter [00300, 05004], lr: 0.070704, loss: 2.7472
2022-03-07 02:06:41 - train: epoch 0077, iter [00400, 05004], lr: 0.070704, loss: 2.8526
2022-03-07 02:07:15 - train: epoch 0077, iter [00500, 05004], lr: 0.070704, loss: 2.5807
2022-03-07 02:07:49 - train: epoch 0077, iter [00600, 05004], lr: 0.070704, loss: 2.6285
2022-03-07 02:08:22 - train: epoch 0077, iter [00700, 05004], lr: 0.070704, loss: 2.9920
2022-03-07 02:08:56 - train: epoch 0077, iter [00800, 05004], lr: 0.070704, loss: 2.9174
2022-03-07 02:09:30 - train: epoch 0077, iter [00900, 05004], lr: 0.070704, loss: 3.0269
2022-03-07 02:10:03 - train: epoch 0077, iter [01000, 05004], lr: 0.070704, loss: 2.6041
2022-03-07 02:10:37 - train: epoch 0077, iter [01100, 05004], lr: 0.070704, loss: 3.0392
2022-03-07 02:11:10 - train: epoch 0077, iter [01200, 05004], lr: 0.070704, loss: 2.7244
2022-03-07 02:11:44 - train: epoch 0077, iter [01300, 05004], lr: 0.070704, loss: 2.6818
2022-03-07 02:12:17 - train: epoch 0077, iter [01400, 05004], lr: 0.070704, loss: 3.0756
2022-03-07 02:12:51 - train: epoch 0077, iter [01500, 05004], lr: 0.070704, loss: 2.5019
2022-03-07 02:13:24 - train: epoch 0077, iter [01600, 05004], lr: 0.070704, loss: 2.9168
2022-03-07 02:13:58 - train: epoch 0077, iter [01700, 05004], lr: 0.070704, loss: 2.9481
2022-03-07 02:14:31 - train: epoch 0077, iter [01800, 05004], lr: 0.070704, loss: 2.8723
2022-03-07 02:15:04 - train: epoch 0077, iter [01900, 05004], lr: 0.070704, loss: 2.7912
2022-03-07 02:15:38 - train: epoch 0077, iter [02000, 05004], lr: 0.070704, loss: 2.7925
2022-03-07 02:16:12 - train: epoch 0077, iter [02100, 05004], lr: 0.070704, loss: 2.7752
2022-03-07 02:16:45 - train: epoch 0077, iter [02200, 05004], lr: 0.070704, loss: 2.9896
2022-03-07 02:17:19 - train: epoch 0077, iter [02300, 05004], lr: 0.070704, loss: 3.2162
2022-03-07 02:17:52 - train: epoch 0077, iter [02400, 05004], lr: 0.070704, loss: 2.8367
2022-03-07 02:18:26 - train: epoch 0077, iter [02500, 05004], lr: 0.070704, loss: 3.2284
2022-03-07 02:19:01 - train: epoch 0077, iter [02600, 05004], lr: 0.070704, loss: 2.9393
2022-03-07 02:19:34 - train: epoch 0077, iter [02700, 05004], lr: 0.070704, loss: 2.9148
2022-03-07 02:20:07 - train: epoch 0077, iter [02800, 05004], lr: 0.070704, loss: 2.9017
2022-03-07 02:20:40 - train: epoch 0077, iter [02900, 05004], lr: 0.070704, loss: 3.2639
2022-03-07 02:21:14 - train: epoch 0077, iter [03000, 05004], lr: 0.070704, loss: 2.7785
2022-03-07 02:21:48 - train: epoch 0077, iter [03100, 05004], lr: 0.070704, loss: 2.8598
2022-03-07 02:22:21 - train: epoch 0077, iter [03200, 05004], lr: 0.070704, loss: 3.0695
2022-03-07 02:22:55 - train: epoch 0077, iter [03300, 05004], lr: 0.070704, loss: 2.6582
2022-03-07 02:23:29 - train: epoch 0077, iter [03400, 05004], lr: 0.070704, loss: 3.1703
2022-03-07 02:24:02 - train: epoch 0077, iter [03500, 05004], lr: 0.070704, loss: 2.5731
2022-03-07 02:24:36 - train: epoch 0077, iter [03600, 05004], lr: 0.070704, loss: 2.9179
2022-03-07 02:25:09 - train: epoch 0077, iter [03700, 05004], lr: 0.070704, loss: 2.8181
2022-03-07 02:25:43 - train: epoch 0077, iter [03800, 05004], lr: 0.070704, loss: 3.0237
2022-03-07 02:26:17 - train: epoch 0077, iter [03900, 05004], lr: 0.070704, loss: 3.0045
2022-03-07 02:26:50 - train: epoch 0077, iter [04000, 05004], lr: 0.070704, loss: 2.9985
2022-03-07 02:27:23 - train: epoch 0077, iter [04100, 05004], lr: 0.070704, loss: 3.1408
2022-03-07 02:27:57 - train: epoch 0077, iter [04200, 05004], lr: 0.070704, loss: 2.8752
2022-03-07 02:28:31 - train: epoch 0077, iter [04300, 05004], lr: 0.070704, loss: 2.6456
2022-03-07 02:29:05 - train: epoch 0077, iter [04400, 05004], lr: 0.070704, loss: 2.9586
2022-03-07 02:29:38 - train: epoch 0077, iter [04500, 05004], lr: 0.070704, loss: 3.0989
2022-03-07 02:30:12 - train: epoch 0077, iter [04600, 05004], lr: 0.070704, loss: 2.7279
2022-03-07 02:30:45 - train: epoch 0077, iter [04700, 05004], lr: 0.070704, loss: 2.7238
2022-03-07 02:31:19 - train: epoch 0077, iter [04800, 05004], lr: 0.070704, loss: 2.9845
2022-03-07 02:31:52 - train: epoch 0077, iter [04900, 05004], lr: 0.070704, loss: 3.1404
2022-03-07 02:32:25 - train: epoch 0077, iter [05000, 05004], lr: 0.070704, loss: 2.9403
2022-03-07 02:32:26 - train: epoch 077, train_loss: 2.8836
2022-03-07 02:33:40 - eval: epoch: 077, acc1: 56.310%, acc5: 81.116%, test_loss: 1.8389, per_image_load_time: 2.334ms, per_image_inference_time: 0.504ms
2022-03-07 02:33:40 - until epoch: 077, best_acc1: 56.556%
2022-03-07 02:33:40 - epoch 078 lr: 0.06996822917828477
2022-03-07 02:34:20 - train: epoch 0078, iter [00100, 05004], lr: 0.069968, loss: 3.1234
2022-03-07 02:34:53 - train: epoch 0078, iter [00200, 05004], lr: 0.069968, loss: 2.8587
2022-03-07 02:35:27 - train: epoch 0078, iter [00300, 05004], lr: 0.069968, loss: 2.7488
2022-03-07 02:36:01 - train: epoch 0078, iter [00400, 05004], lr: 0.069968, loss: 2.6161
2022-03-07 02:36:33 - train: epoch 0078, iter [00500, 05004], lr: 0.069968, loss: 2.8316
2022-03-07 02:37:07 - train: epoch 0078, iter [00600, 05004], lr: 0.069968, loss: 2.7373
2022-03-07 02:37:40 - train: epoch 0078, iter [00700, 05004], lr: 0.069968, loss: 3.1263
2022-03-07 02:38:14 - train: epoch 0078, iter [00800, 05004], lr: 0.069968, loss: 2.8884
2022-03-07 02:38:48 - train: epoch 0078, iter [00900, 05004], lr: 0.069968, loss: 2.8036
2022-03-07 02:39:22 - train: epoch 0078, iter [01000, 05004], lr: 0.069968, loss: 2.9198
2022-03-07 02:39:55 - train: epoch 0078, iter [01100, 05004], lr: 0.069968, loss: 2.6781
2022-03-07 02:40:29 - train: epoch 0078, iter [01200, 05004], lr: 0.069968, loss: 2.4215
2022-03-07 02:41:02 - train: epoch 0078, iter [01300, 05004], lr: 0.069968, loss: 3.0952
2022-03-07 02:41:36 - train: epoch 0078, iter [01400, 05004], lr: 0.069968, loss: 2.9176
2022-03-07 02:42:10 - train: epoch 0078, iter [01500, 05004], lr: 0.069968, loss: 3.0129
2022-03-07 02:42:43 - train: epoch 0078, iter [01600, 05004], lr: 0.069968, loss: 3.1758
2022-03-07 02:43:17 - train: epoch 0078, iter [01700, 05004], lr: 0.069968, loss: 3.0907
2022-03-07 02:43:51 - train: epoch 0078, iter [01800, 05004], lr: 0.069968, loss: 3.0140
2022-03-07 02:44:24 - train: epoch 0078, iter [01900, 05004], lr: 0.069968, loss: 2.4990
2022-03-07 02:44:58 - train: epoch 0078, iter [02000, 05004], lr: 0.069968, loss: 2.9060
2022-03-07 02:45:32 - train: epoch 0078, iter [02100, 05004], lr: 0.069968, loss: 2.9365
2022-03-07 02:46:06 - train: epoch 0078, iter [02200, 05004], lr: 0.069968, loss: 2.9438
2022-03-07 02:46:39 - train: epoch 0078, iter [02300, 05004], lr: 0.069968, loss: 3.1392
2022-03-07 02:47:13 - train: epoch 0078, iter [02400, 05004], lr: 0.069968, loss: 2.7070
2022-03-07 02:47:47 - train: epoch 0078, iter [02500, 05004], lr: 0.069968, loss: 2.6505
2022-03-07 02:48:20 - train: epoch 0078, iter [02600, 05004], lr: 0.069968, loss: 2.7955
2022-03-07 02:48:55 - train: epoch 0078, iter [02700, 05004], lr: 0.069968, loss: 2.8257
2022-03-07 02:49:28 - train: epoch 0078, iter [02800, 05004], lr: 0.069968, loss: 2.6586
2022-03-07 02:50:01 - train: epoch 0078, iter [02900, 05004], lr: 0.069968, loss: 2.8836
2022-03-07 02:50:35 - train: epoch 0078, iter [03000, 05004], lr: 0.069968, loss: 2.7887
2022-03-07 02:51:09 - train: epoch 0078, iter [03100, 05004], lr: 0.069968, loss: 3.0433
2022-03-07 02:51:43 - train: epoch 0078, iter [03200, 05004], lr: 0.069968, loss: 2.8224
2022-03-07 02:52:16 - train: epoch 0078, iter [03300, 05004], lr: 0.069968, loss: 3.1322
2022-03-07 02:52:50 - train: epoch 0078, iter [03400, 05004], lr: 0.069968, loss: 2.7273
2022-03-07 02:53:23 - train: epoch 0078, iter [03500, 05004], lr: 0.069968, loss: 2.8131
2022-03-07 02:53:57 - train: epoch 0078, iter [03600, 05004], lr: 0.069968, loss: 3.0591
2022-03-07 02:54:30 - train: epoch 0078, iter [03700, 05004], lr: 0.069968, loss: 2.8253
2022-03-07 02:55:04 - train: epoch 0078, iter [03800, 05004], lr: 0.069968, loss: 2.6253
2022-03-07 02:55:38 - train: epoch 0078, iter [03900, 05004], lr: 0.069968, loss: 2.9585
2022-03-07 02:56:12 - train: epoch 0078, iter [04000, 05004], lr: 0.069968, loss: 2.8583
2022-03-07 02:56:45 - train: epoch 0078, iter [04100, 05004], lr: 0.069968, loss: 2.8400
2022-03-07 02:57:19 - train: epoch 0078, iter [04200, 05004], lr: 0.069968, loss: 2.9163
2022-03-07 02:57:53 - train: epoch 0078, iter [04300, 05004], lr: 0.069968, loss: 3.0729
2022-03-07 02:58:27 - train: epoch 0078, iter [04400, 05004], lr: 0.069968, loss: 2.9070
2022-03-07 02:59:00 - train: epoch 0078, iter [04500, 05004], lr: 0.069968, loss: 2.8425
2022-03-07 02:59:34 - train: epoch 0078, iter [04600, 05004], lr: 0.069968, loss: 2.5043
2022-03-07 03:00:07 - train: epoch 0078, iter [04700, 05004], lr: 0.069968, loss: 2.9973
2022-03-07 03:00:41 - train: epoch 0078, iter [04800, 05004], lr: 0.069968, loss: 3.0628
2022-03-07 03:01:14 - train: epoch 0078, iter [04900, 05004], lr: 0.069968, loss: 2.7770
2022-03-07 03:01:46 - train: epoch 0078, iter [05000, 05004], lr: 0.069968, loss: 2.7964
2022-03-07 03:01:48 - train: epoch 078, train_loss: 2.8802
2022-03-07 03:03:01 - eval: epoch: 078, acc1: 56.700%, acc5: 81.448%, test_loss: 1.8183, per_image_load_time: 2.342ms, per_image_inference_time: 0.500ms
2022-03-07 03:03:02 - until epoch: 078, best_acc1: 56.700%
2022-03-07 03:03:02 - epoch 079 lr: 0.06922716010014256
2022-03-07 03:03:40 - train: epoch 0079, iter [00100, 05004], lr: 0.069227, loss: 2.9606
2022-03-07 03:04:14 - train: epoch 0079, iter [00200, 05004], lr: 0.069227, loss: 2.6902
2022-03-07 03:04:48 - train: epoch 0079, iter [00300, 05004], lr: 0.069227, loss: 2.7345
2022-03-07 03:05:21 - train: epoch 0079, iter [00400, 05004], lr: 0.069227, loss: 3.0744
2022-03-07 03:05:56 - train: epoch 0079, iter [00500, 05004], lr: 0.069227, loss: 2.8701
2022-03-07 03:06:28 - train: epoch 0079, iter [00600, 05004], lr: 0.069227, loss: 3.0311
2022-03-07 03:07:02 - train: epoch 0079, iter [00700, 05004], lr: 0.069227, loss: 2.9251
2022-03-07 03:07:36 - train: epoch 0079, iter [00800, 05004], lr: 0.069227, loss: 2.9341
2022-03-07 03:08:10 - train: epoch 0079, iter [00900, 05004], lr: 0.069227, loss: 2.8554
2022-03-07 03:08:44 - train: epoch 0079, iter [01000, 05004], lr: 0.069227, loss: 2.7146
2022-03-07 03:09:17 - train: epoch 0079, iter [01100, 05004], lr: 0.069227, loss: 3.2140
2022-03-07 03:09:51 - train: epoch 0079, iter [01200, 05004], lr: 0.069227, loss: 3.1775
2022-03-07 03:10:24 - train: epoch 0079, iter [01300, 05004], lr: 0.069227, loss: 2.8467
2022-03-07 03:10:58 - train: epoch 0079, iter [01400, 05004], lr: 0.069227, loss: 2.8910
2022-03-07 03:11:31 - train: epoch 0079, iter [01500, 05004], lr: 0.069227, loss: 2.7509
2022-03-07 03:12:06 - train: epoch 0079, iter [01600, 05004], lr: 0.069227, loss: 2.8428
2022-03-07 03:12:40 - train: epoch 0079, iter [01700, 05004], lr: 0.069227, loss: 2.8762
2022-03-07 03:13:13 - train: epoch 0079, iter [01800, 05004], lr: 0.069227, loss: 2.8870
2022-03-07 03:13:47 - train: epoch 0079, iter [01900, 05004], lr: 0.069227, loss: 2.8619
2022-03-07 03:14:20 - train: epoch 0079, iter [02000, 05004], lr: 0.069227, loss: 2.9195
2022-03-07 03:14:54 - train: epoch 0079, iter [02100, 05004], lr: 0.069227, loss: 2.7504
2022-03-07 03:15:28 - train: epoch 0079, iter [02200, 05004], lr: 0.069227, loss: 3.0234
2022-03-07 03:16:02 - train: epoch 0079, iter [02300, 05004], lr: 0.069227, loss: 2.7091
2022-03-07 03:16:35 - train: epoch 0079, iter [02400, 05004], lr: 0.069227, loss: 2.9616
2022-03-07 03:17:09 - train: epoch 0079, iter [02500, 05004], lr: 0.069227, loss: 2.7630
2022-03-07 03:17:42 - train: epoch 0079, iter [02600, 05004], lr: 0.069227, loss: 2.8167
2022-03-07 03:18:16 - train: epoch 0079, iter [02700, 05004], lr: 0.069227, loss: 2.7481
2022-03-07 03:18:48 - train: epoch 0079, iter [02800, 05004], lr: 0.069227, loss: 2.5749
2022-03-07 03:19:22 - train: epoch 0079, iter [02900, 05004], lr: 0.069227, loss: 2.8211
2022-03-07 03:19:55 - train: epoch 0079, iter [03000, 05004], lr: 0.069227, loss: 2.7461
2022-03-07 03:20:29 - train: epoch 0079, iter [03100, 05004], lr: 0.069227, loss: 2.9729
2022-03-07 03:21:03 - train: epoch 0079, iter [03200, 05004], lr: 0.069227, loss: 3.2328
2022-03-07 03:21:36 - train: epoch 0079, iter [03300, 05004], lr: 0.069227, loss: 2.6205
2022-03-07 03:22:10 - train: epoch 0079, iter [03400, 05004], lr: 0.069227, loss: 2.7671
2022-03-07 03:22:43 - train: epoch 0079, iter [03500, 05004], lr: 0.069227, loss: 3.1037
2022-03-07 03:23:17 - train: epoch 0079, iter [03600, 05004], lr: 0.069227, loss: 2.8183
2022-03-07 03:23:50 - train: epoch 0079, iter [03700, 05004], lr: 0.069227, loss: 2.7503
2022-03-07 03:24:24 - train: epoch 0079, iter [03800, 05004], lr: 0.069227, loss: 2.8341
2022-03-07 03:24:58 - train: epoch 0079, iter [03900, 05004], lr: 0.069227, loss: 2.5560
2022-03-07 03:25:32 - train: epoch 0079, iter [04000, 05004], lr: 0.069227, loss: 2.6698
2022-03-07 03:26:06 - train: epoch 0079, iter [04100, 05004], lr: 0.069227, loss: 2.9975
2022-03-07 03:26:38 - train: epoch 0079, iter [04200, 05004], lr: 0.069227, loss: 2.5635
2022-03-07 03:27:12 - train: epoch 0079, iter [04300, 05004], lr: 0.069227, loss: 2.5946
2022-03-07 03:27:45 - train: epoch 0079, iter [04400, 05004], lr: 0.069227, loss: 2.7880
2022-03-07 03:28:19 - train: epoch 0079, iter [04500, 05004], lr: 0.069227, loss: 3.1822
2022-03-07 03:28:53 - train: epoch 0079, iter [04600, 05004], lr: 0.069227, loss: 3.0393
2022-03-07 03:29:26 - train: epoch 0079, iter [04700, 05004], lr: 0.069227, loss: 2.7889
2022-03-07 03:29:59 - train: epoch 0079, iter [04800, 05004], lr: 0.069227, loss: 3.0394
2022-03-07 03:30:33 - train: epoch 0079, iter [04900, 05004], lr: 0.069227, loss: 2.9791
2022-03-07 03:31:06 - train: epoch 0079, iter [05000, 05004], lr: 0.069227, loss: 2.7030
2022-03-07 03:31:07 - train: epoch 079, train_loss: 2.8807
2022-03-07 03:32:21 - eval: epoch: 079, acc1: 55.208%, acc5: 80.284%, test_loss: 1.9015, per_image_load_time: 2.321ms, per_image_inference_time: 0.542ms
2022-03-07 03:32:21 - until epoch: 079, best_acc1: 56.700%
2022-03-07 03:32:21 - epoch 080 lr: 0.06848110061149555
2022-03-07 03:33:00 - train: epoch 0080, iter [00100, 05004], lr: 0.068481, loss: 2.7173
2022-03-07 03:33:33 - train: epoch 0080, iter [00200, 05004], lr: 0.068481, loss: 2.8765
2022-03-07 03:34:07 - train: epoch 0080, iter [00300, 05004], lr: 0.068481, loss: 2.9705
2022-03-07 03:34:41 - train: epoch 0080, iter [00400, 05004], lr: 0.068481, loss: 2.7669
2022-03-07 03:35:15 - train: epoch 0080, iter [00500, 05004], lr: 0.068481, loss: 2.7044
2022-03-07 03:35:49 - train: epoch 0080, iter [00600, 05004], lr: 0.068481, loss: 2.6555
2022-03-07 03:36:22 - train: epoch 0080, iter [00700, 05004], lr: 0.068481, loss: 2.7684
2022-03-07 03:36:55 - train: epoch 0080, iter [00800, 05004], lr: 0.068481, loss: 2.9388
2022-03-07 03:37:29 - train: epoch 0080, iter [00900, 05004], lr: 0.068481, loss: 2.8626
2022-03-07 03:38:02 - train: epoch 0080, iter [01000, 05004], lr: 0.068481, loss: 2.7787
2022-03-07 03:38:36 - train: epoch 0080, iter [01100, 05004], lr: 0.068481, loss: 2.8871
2022-03-07 03:39:10 - train: epoch 0080, iter [01200, 05004], lr: 0.068481, loss: 2.9802
2022-03-07 03:39:44 - train: epoch 0080, iter [01300, 05004], lr: 0.068481, loss: 3.2022
2022-03-07 03:40:17 - train: epoch 0080, iter [01400, 05004], lr: 0.068481, loss: 2.8351
2022-03-07 03:40:51 - train: epoch 0080, iter [01500, 05004], lr: 0.068481, loss: 2.9563
2022-03-07 03:41:24 - train: epoch 0080, iter [01600, 05004], lr: 0.068481, loss: 2.5795
2022-03-07 03:41:58 - train: epoch 0080, iter [01700, 05004], lr: 0.068481, loss: 2.8996
2022-03-07 03:42:31 - train: epoch 0080, iter [01800, 05004], lr: 0.068481, loss: 3.3205
2022-03-07 03:43:05 - train: epoch 0080, iter [01900, 05004], lr: 0.068481, loss: 2.9542
2022-03-07 03:43:38 - train: epoch 0080, iter [02000, 05004], lr: 0.068481, loss: 2.9278
2022-03-07 03:44:11 - train: epoch 0080, iter [02100, 05004], lr: 0.068481, loss: 2.8470
2022-03-07 03:44:45 - train: epoch 0080, iter [02200, 05004], lr: 0.068481, loss: 2.8325
2022-03-07 03:45:18 - train: epoch 0080, iter [02300, 05004], lr: 0.068481, loss: 2.7421
2022-03-07 03:45:51 - train: epoch 0080, iter [02400, 05004], lr: 0.068481, loss: 2.7234
2022-03-07 03:46:25 - train: epoch 0080, iter [02500, 05004], lr: 0.068481, loss: 2.6494
2022-03-07 03:46:58 - train: epoch 0080, iter [02600, 05004], lr: 0.068481, loss: 2.7429
2022-03-07 03:47:31 - train: epoch 0080, iter [02700, 05004], lr: 0.068481, loss: 2.9173
2022-03-07 03:48:05 - train: epoch 0080, iter [02800, 05004], lr: 0.068481, loss: 2.8349
2022-03-07 03:48:39 - train: epoch 0080, iter [02900, 05004], lr: 0.068481, loss: 2.6554
2022-03-07 03:49:12 - train: epoch 0080, iter [03000, 05004], lr: 0.068481, loss: 2.8427
2022-03-07 03:49:46 - train: epoch 0080, iter [03100, 05004], lr: 0.068481, loss: 3.2936
2022-03-07 03:50:19 - train: epoch 0080, iter [03200, 05004], lr: 0.068481, loss: 2.5111
2022-03-07 03:50:53 - train: epoch 0080, iter [03300, 05004], lr: 0.068481, loss: 2.5066
2022-03-07 03:51:27 - train: epoch 0080, iter [03400, 05004], lr: 0.068481, loss: 2.7822
2022-03-07 03:52:00 - train: epoch 0080, iter [03500, 05004], lr: 0.068481, loss: 2.6092
2022-03-07 03:52:34 - train: epoch 0080, iter [03600, 05004], lr: 0.068481, loss: 2.8549
2022-03-07 03:53:07 - train: epoch 0080, iter [03700, 05004], lr: 0.068481, loss: 2.7641
2022-03-07 03:53:40 - train: epoch 0080, iter [03800, 05004], lr: 0.068481, loss: 3.0603
2022-03-07 03:54:14 - train: epoch 0080, iter [03900, 05004], lr: 0.068481, loss: 2.7482
2022-03-07 03:54:48 - train: epoch 0080, iter [04000, 05004], lr: 0.068481, loss: 3.1574
2022-03-07 03:55:21 - train: epoch 0080, iter [04100, 05004], lr: 0.068481, loss: 2.9085
2022-03-07 03:55:55 - train: epoch 0080, iter [04200, 05004], lr: 0.068481, loss: 2.8837
2022-03-07 03:56:28 - train: epoch 0080, iter [04300, 05004], lr: 0.068481, loss: 2.9883
2022-03-07 03:57:01 - train: epoch 0080, iter [04400, 05004], lr: 0.068481, loss: 2.7887
2022-03-07 03:57:34 - train: epoch 0080, iter [04500, 05004], lr: 0.068481, loss: 2.7404
2022-03-07 03:58:07 - train: epoch 0080, iter [04600, 05004], lr: 0.068481, loss: 3.2272
2022-03-07 03:58:41 - train: epoch 0080, iter [04700, 05004], lr: 0.068481, loss: 2.7929
2022-03-07 03:59:15 - train: epoch 0080, iter [04800, 05004], lr: 0.068481, loss: 2.6866
2022-03-07 03:59:48 - train: epoch 0080, iter [04900, 05004], lr: 0.068481, loss: 3.0211
2022-03-07 04:00:21 - train: epoch 0080, iter [05000, 05004], lr: 0.068481, loss: 2.5928
2022-03-07 04:00:22 - train: epoch 080, train_loss: 2.8712
2022-03-07 04:01:35 - eval: epoch: 080, acc1: 56.868%, acc5: 81.708%, test_loss: 1.8083, per_image_load_time: 2.377ms, per_image_inference_time: 0.499ms
2022-03-07 04:01:36 - until epoch: 080, best_acc1: 56.868%
2022-03-07 04:01:36 - epoch 081 lr: 0.06773024435212678
2022-03-07 04:02:15 - train: epoch 0081, iter [00100, 05004], lr: 0.067730, loss: 2.5945
2022-03-07 04:02:48 - train: epoch 0081, iter [00200, 05004], lr: 0.067730, loss: 2.6763
2022-03-07 04:03:21 - train: epoch 0081, iter [00300, 05004], lr: 0.067730, loss: 2.9166
2022-03-07 04:03:55 - train: epoch 0081, iter [00400, 05004], lr: 0.067730, loss: 3.1130
2022-03-07 04:04:28 - train: epoch 0081, iter [00500, 05004], lr: 0.067730, loss: 3.2732
2022-03-07 04:05:03 - train: epoch 0081, iter [00600, 05004], lr: 0.067730, loss: 3.1067
2022-03-07 04:05:36 - train: epoch 0081, iter [00700, 05004], lr: 0.067730, loss: 2.8911
2022-03-07 04:06:10 - train: epoch 0081, iter [00800, 05004], lr: 0.067730, loss: 3.0152
2022-03-07 04:06:43 - train: epoch 0081, iter [00900, 05004], lr: 0.067730, loss: 2.8050
2022-03-07 04:07:17 - train: epoch 0081, iter [01000, 05004], lr: 0.067730, loss: 2.9456
2022-03-07 04:07:51 - train: epoch 0081, iter [01100, 05004], lr: 0.067730, loss: 2.5882
2022-03-07 04:08:25 - train: epoch 0081, iter [01200, 05004], lr: 0.067730, loss: 2.5920
2022-03-07 04:08:58 - train: epoch 0081, iter [01300, 05004], lr: 0.067730, loss: 2.5096
2022-03-07 04:09:32 - train: epoch 0081, iter [01400, 05004], lr: 0.067730, loss: 2.4643
2022-03-07 04:10:06 - train: epoch 0081, iter [01500, 05004], lr: 0.067730, loss: 3.0391
2022-03-07 04:10:39 - train: epoch 0081, iter [01600, 05004], lr: 0.067730, loss: 2.6834
2022-03-07 04:11:13 - train: epoch 0081, iter [01700, 05004], lr: 0.067730, loss: 2.9197
2022-03-07 04:11:47 - train: epoch 0081, iter [01800, 05004], lr: 0.067730, loss: 2.7326
2022-03-07 04:12:21 - train: epoch 0081, iter [01900, 05004], lr: 0.067730, loss: 2.5561
2022-03-07 04:12:55 - train: epoch 0081, iter [02000, 05004], lr: 0.067730, loss: 2.8532
2022-03-07 04:13:28 - train: epoch 0081, iter [02100, 05004], lr: 0.067730, loss: 2.8819
2022-03-07 04:14:02 - train: epoch 0081, iter [02200, 05004], lr: 0.067730, loss: 3.1171
2022-03-07 04:14:34 - train: epoch 0081, iter [02300, 05004], lr: 0.067730, loss: 2.7418
2022-03-07 04:15:09 - train: epoch 0081, iter [02400, 05004], lr: 0.067730, loss: 2.8264
2022-03-07 04:15:41 - train: epoch 0081, iter [02500, 05004], lr: 0.067730, loss: 2.9204
2022-03-07 04:16:16 - train: epoch 0081, iter [02600, 05004], lr: 0.067730, loss: 3.0202
2022-03-07 04:16:48 - train: epoch 0081, iter [02700, 05004], lr: 0.067730, loss: 2.7483
2022-03-07 04:17:22 - train: epoch 0081, iter [02800, 05004], lr: 0.067730, loss: 2.5186
2022-03-07 04:17:55 - train: epoch 0081, iter [02900, 05004], lr: 0.067730, loss: 2.5966
2022-03-07 04:18:29 - train: epoch 0081, iter [03000, 05004], lr: 0.067730, loss: 2.7373
2022-03-07 04:19:03 - train: epoch 0081, iter [03100, 05004], lr: 0.067730, loss: 2.7399
2022-03-07 04:19:36 - train: epoch 0081, iter [03200, 05004], lr: 0.067730, loss: 2.9417
2022-03-07 04:20:09 - train: epoch 0081, iter [03300, 05004], lr: 0.067730, loss: 3.0482
2022-03-07 04:20:43 - train: epoch 0081, iter [03400, 05004], lr: 0.067730, loss: 3.3640
2022-03-07 04:21:16 - train: epoch 0081, iter [03500, 05004], lr: 0.067730, loss: 2.9928
2022-03-07 04:21:51 - train: epoch 0081, iter [03600, 05004], lr: 0.067730, loss: 2.6611
2022-03-07 04:22:24 - train: epoch 0081, iter [03700, 05004], lr: 0.067730, loss: 3.0904
2022-03-07 04:22:58 - train: epoch 0081, iter [03800, 05004], lr: 0.067730, loss: 2.9866
2022-03-07 04:23:31 - train: epoch 0081, iter [03900, 05004], lr: 0.067730, loss: 2.8786
2022-03-07 04:24:05 - train: epoch 0081, iter [04000, 05004], lr: 0.067730, loss: 2.7165
2022-03-07 04:24:38 - train: epoch 0081, iter [04100, 05004], lr: 0.067730, loss: 3.0408
2022-03-07 04:25:12 - train: epoch 0081, iter [04200, 05004], lr: 0.067730, loss: 2.8928
2022-03-07 04:25:46 - train: epoch 0081, iter [04300, 05004], lr: 0.067730, loss: 2.7741
2022-03-07 04:26:20 - train: epoch 0081, iter [04400, 05004], lr: 0.067730, loss: 2.9924
2022-03-07 04:26:53 - train: epoch 0081, iter [04500, 05004], lr: 0.067730, loss: 3.2022
2022-03-07 04:27:27 - train: epoch 0081, iter [04600, 05004], lr: 0.067730, loss: 2.9505
2022-03-07 04:28:00 - train: epoch 0081, iter [04700, 05004], lr: 0.067730, loss: 3.1668
2022-03-07 04:28:34 - train: epoch 0081, iter [04800, 05004], lr: 0.067730, loss: 3.0138
2022-03-07 04:29:07 - train: epoch 0081, iter [04900, 05004], lr: 0.067730, loss: 2.7008
2022-03-07 04:29:40 - train: epoch 0081, iter [05000, 05004], lr: 0.067730, loss: 2.6961
2022-03-07 04:29:41 - train: epoch 081, train_loss: 2.8665
2022-03-07 04:30:55 - eval: epoch: 081, acc1: 57.052%, acc5: 81.384%, test_loss: 1.8237, per_image_load_time: 1.500ms, per_image_inference_time: 0.478ms
2022-03-07 04:30:56 - until epoch: 081, best_acc1: 57.052%
2022-03-07 04:30:56 - epoch 082 lr: 0.06697478620682136
2022-03-07 04:31:34 - train: epoch 0082, iter [00100, 05004], lr: 0.066975, loss: 2.6872
2022-03-07 04:32:08 - train: epoch 0082, iter [00200, 05004], lr: 0.066975, loss: 2.8006
2022-03-07 04:32:42 - train: epoch 0082, iter [00300, 05004], lr: 0.066975, loss: 2.7812
2022-03-07 04:33:16 - train: epoch 0082, iter [00400, 05004], lr: 0.066975, loss: 2.7787
2022-03-07 04:33:48 - train: epoch 0082, iter [00500, 05004], lr: 0.066975, loss: 3.0535
2022-03-07 04:34:22 - train: epoch 0082, iter [00600, 05004], lr: 0.066975, loss: 3.0047
2022-03-07 04:34:55 - train: epoch 0082, iter [00700, 05004], lr: 0.066975, loss: 2.8935
2022-03-07 04:35:29 - train: epoch 0082, iter [00800, 05004], lr: 0.066975, loss: 2.5593
2022-03-07 04:36:03 - train: epoch 0082, iter [00900, 05004], lr: 0.066975, loss: 2.8518
2022-03-07 04:36:36 - train: epoch 0082, iter [01000, 05004], lr: 0.066975, loss: 2.9675
2022-03-07 04:37:09 - train: epoch 0082, iter [01100, 05004], lr: 0.066975, loss: 2.8643
2022-03-07 04:37:43 - train: epoch 0082, iter [01200, 05004], lr: 0.066975, loss: 2.8717
2022-03-07 04:38:16 - train: epoch 0082, iter [01300, 05004], lr: 0.066975, loss: 3.1033
2022-03-07 04:38:50 - train: epoch 0082, iter [01400, 05004], lr: 0.066975, loss: 2.6952
2022-03-07 04:39:24 - train: epoch 0082, iter [01500, 05004], lr: 0.066975, loss: 2.6677
2022-03-07 04:39:58 - train: epoch 0082, iter [01600, 05004], lr: 0.066975, loss: 2.9572
2022-03-07 04:40:32 - train: epoch 0082, iter [01700, 05004], lr: 0.066975, loss: 2.8110
2022-03-07 04:41:05 - train: epoch 0082, iter [01800, 05004], lr: 0.066975, loss: 2.6101
2022-03-07 04:41:39 - train: epoch 0082, iter [01900, 05004], lr: 0.066975, loss: 2.8519
2022-03-07 04:42:14 - train: epoch 0082, iter [02000, 05004], lr: 0.066975, loss: 2.6879
2022-03-07 04:42:47 - train: epoch 0082, iter [02100, 05004], lr: 0.066975, loss: 2.7003
2022-03-07 04:43:21 - train: epoch 0082, iter [02200, 05004], lr: 0.066975, loss: 2.8715
2022-03-07 04:43:55 - train: epoch 0082, iter [02300, 05004], lr: 0.066975, loss: 2.8777
2022-03-07 04:44:28 - train: epoch 0082, iter [02400, 05004], lr: 0.066975, loss: 2.9696
2022-03-07 04:45:02 - train: epoch 0082, iter [02500, 05004], lr: 0.066975, loss: 2.7769
2022-03-07 04:45:36 - train: epoch 0082, iter [02600, 05004], lr: 0.066975, loss: 2.7715
2022-03-07 04:46:10 - train: epoch 0082, iter [02700, 05004], lr: 0.066975, loss: 2.7596
2022-03-07 04:46:44 - train: epoch 0082, iter [02800, 05004], lr: 0.066975, loss: 2.5536
2022-03-07 04:47:17 - train: epoch 0082, iter [02900, 05004], lr: 0.066975, loss: 2.7089
2022-03-07 04:47:51 - train: epoch 0082, iter [03000, 05004], lr: 0.066975, loss: 2.8472
2022-03-07 04:48:25 - train: epoch 0082, iter [03100, 05004], lr: 0.066975, loss: 2.9165
2022-03-07 04:48:59 - train: epoch 0082, iter [03200, 05004], lr: 0.066975, loss: 2.8965
2022-03-07 04:49:32 - train: epoch 0082, iter [03300, 05004], lr: 0.066975, loss: 3.0636
2022-03-07 04:50:06 - train: epoch 0082, iter [03400, 05004], lr: 0.066975, loss: 2.9472
2022-03-07 04:50:39 - train: epoch 0082, iter [03500, 05004], lr: 0.066975, loss: 2.9516
2022-03-07 04:51:13 - train: epoch 0082, iter [03600, 05004], lr: 0.066975, loss: 2.8496
2022-03-07 04:51:47 - train: epoch 0082, iter [03700, 05004], lr: 0.066975, loss: 2.7106
2022-03-07 04:52:21 - train: epoch 0082, iter [03800, 05004], lr: 0.066975, loss: 2.8914
2022-03-07 04:52:54 - train: epoch 0082, iter [03900, 05004], lr: 0.066975, loss: 2.8981
2022-03-07 04:53:28 - train: epoch 0082, iter [04000, 05004], lr: 0.066975, loss: 2.9231
2022-03-07 04:54:00 - train: epoch 0082, iter [04100, 05004], lr: 0.066975, loss: 3.0323
2022-03-07 04:54:34 - train: epoch 0082, iter [04200, 05004], lr: 0.066975, loss: 3.1387
2022-03-07 04:55:07 - train: epoch 0082, iter [04300, 05004], lr: 0.066975, loss: 2.8426
2022-03-07 04:55:41 - train: epoch 0082, iter [04400, 05004], lr: 0.066975, loss: 2.8620
2022-03-07 04:56:15 - train: epoch 0082, iter [04500, 05004], lr: 0.066975, loss: 2.9021
2022-03-07 04:56:48 - train: epoch 0082, iter [04600, 05004], lr: 0.066975, loss: 3.0578
2022-03-07 04:57:21 - train: epoch 0082, iter [04700, 05004], lr: 0.066975, loss: 2.7042
2022-03-07 04:57:55 - train: epoch 0082, iter [04800, 05004], lr: 0.066975, loss: 2.8804
2022-03-07 04:58:28 - train: epoch 0082, iter [04900, 05004], lr: 0.066975, loss: 3.0360
2022-03-07 04:59:00 - train: epoch 0082, iter [05000, 05004], lr: 0.066975, loss: 2.6592
2022-03-07 04:59:02 - train: epoch 082, train_loss: 2.8595
2022-03-07 05:00:14 - eval: epoch: 082, acc1: 54.532%, acc5: 79.430%, test_loss: 1.9438, per_image_load_time: 2.286ms, per_image_inference_time: 0.525ms
2022-03-07 05:00:15 - until epoch: 082, best_acc1: 57.052%
2022-03-07 05:00:15 - epoch 083 lr: 0.06621492225478413
2022-03-07 05:00:54 - train: epoch 0083, iter [00100, 05004], lr: 0.066215, loss: 2.7545
2022-03-07 05:01:28 - train: epoch 0083, iter [00200, 05004], lr: 0.066215, loss: 2.9120
2022-03-07 05:02:02 - train: epoch 0083, iter [00300, 05004], lr: 0.066215, loss: 2.8024
2022-03-07 05:02:35 - train: epoch 0083, iter [00400, 05004], lr: 0.066215, loss: 2.9209
2022-03-07 05:03:10 - train: epoch 0083, iter [00500, 05004], lr: 0.066215, loss: 2.9508
2022-03-07 05:03:42 - train: epoch 0083, iter [00600, 05004], lr: 0.066215, loss: 2.8015
2022-03-07 05:04:17 - train: epoch 0083, iter [00700, 05004], lr: 0.066215, loss: 2.9516
2022-03-07 05:04:51 - train: epoch 0083, iter [00800, 05004], lr: 0.066215, loss: 2.8824
2022-03-07 05:05:25 - train: epoch 0083, iter [00900, 05004], lr: 0.066215, loss: 2.9326
2022-03-07 05:05:58 - train: epoch 0083, iter [01000, 05004], lr: 0.066215, loss: 2.8198
2022-03-07 05:06:32 - train: epoch 0083, iter [01100, 05004], lr: 0.066215, loss: 3.0792
2022-03-07 05:07:06 - train: epoch 0083, iter [01200, 05004], lr: 0.066215, loss: 2.9522
2022-03-07 05:07:41 - train: epoch 0083, iter [01300, 05004], lr: 0.066215, loss: 2.8696
2022-03-07 05:08:14 - train: epoch 0083, iter [01400, 05004], lr: 0.066215, loss: 2.8953
2022-03-07 05:08:48 - train: epoch 0083, iter [01500, 05004], lr: 0.066215, loss: 2.5527
2022-03-07 05:09:22 - train: epoch 0083, iter [01600, 05004], lr: 0.066215, loss: 2.6952
2022-03-07 05:09:56 - train: epoch 0083, iter [01700, 05004], lr: 0.066215, loss: 2.9207
2022-03-07 05:10:29 - train: epoch 0083, iter [01800, 05004], lr: 0.066215, loss: 2.9547
2022-03-07 05:11:03 - train: epoch 0083, iter [01900, 05004], lr: 0.066215, loss: 2.7403
2022-03-07 05:11:36 - train: epoch 0083, iter [02000, 05004], lr: 0.066215, loss: 2.8573
2022-03-07 05:12:11 - train: epoch 0083, iter [02100, 05004], lr: 0.066215, loss: 2.4790
2022-03-07 05:12:44 - train: epoch 0083, iter [02200, 05004], lr: 0.066215, loss: 2.7978
2022-03-07 05:13:18 - train: epoch 0083, iter [02300, 05004], lr: 0.066215, loss: 2.9961
2022-03-07 05:13:51 - train: epoch 0083, iter [02400, 05004], lr: 0.066215, loss: 2.8484
2022-03-07 05:14:25 - train: epoch 0083, iter [02500, 05004], lr: 0.066215, loss: 2.7411
2022-03-07 05:14:58 - train: epoch 0083, iter [02600, 05004], lr: 0.066215, loss: 2.8296
2022-03-07 05:15:32 - train: epoch 0083, iter [02700, 05004], lr: 0.066215, loss: 2.7925
2022-03-07 05:16:06 - train: epoch 0083, iter [02800, 05004], lr: 0.066215, loss: 2.6679
2022-03-07 05:16:39 - train: epoch 0083, iter [02900, 05004], lr: 0.066215, loss: 3.1621
2022-03-07 05:17:13 - train: epoch 0083, iter [03000, 05004], lr: 0.066215, loss: 3.0708
2022-03-07 05:17:46 - train: epoch 0083, iter [03100, 05004], lr: 0.066215, loss: 2.5840
2022-03-07 05:18:21 - train: epoch 0083, iter [03200, 05004], lr: 0.066215, loss: 2.6606
2022-03-07 05:18:54 - train: epoch 0083, iter [03300, 05004], lr: 0.066215, loss: 2.8622
2022-03-07 05:19:28 - train: epoch 0083, iter [03400, 05004], lr: 0.066215, loss: 3.0013
2022-03-07 05:20:02 - train: epoch 0083, iter [03500, 05004], lr: 0.066215, loss: 3.0254
2022-03-07 05:20:36 - train: epoch 0083, iter [03600, 05004], lr: 0.066215, loss: 3.2354
2022-03-07 05:21:09 - train: epoch 0083, iter [03700, 05004], lr: 0.066215, loss: 2.6784
2022-03-07 05:21:44 - train: epoch 0083, iter [03800, 05004], lr: 0.066215, loss: 2.6862
2022-03-07 05:22:18 - train: epoch 0083, iter [03900, 05004], lr: 0.066215, loss: 2.8974
2022-03-07 05:22:51 - train: epoch 0083, iter [04000, 05004], lr: 0.066215, loss: 2.9713
2022-03-07 05:23:25 - train: epoch 0083, iter [04100, 05004], lr: 0.066215, loss: 2.7555
2022-03-07 05:23:58 - train: epoch 0083, iter [04200, 05004], lr: 0.066215, loss: 3.3292
2022-03-07 05:24:32 - train: epoch 0083, iter [04300, 05004], lr: 0.066215, loss: 2.9842
2022-03-07 05:25:06 - train: epoch 0083, iter [04400, 05004], lr: 0.066215, loss: 2.8770
2022-03-07 05:25:40 - train: epoch 0083, iter [04500, 05004], lr: 0.066215, loss: 2.8304
2022-03-07 05:26:14 - train: epoch 0083, iter [04600, 05004], lr: 0.066215, loss: 3.0838
2022-03-07 05:26:47 - train: epoch 0083, iter [04700, 05004], lr: 0.066215, loss: 3.0475
2022-03-07 05:27:20 - train: epoch 0083, iter [04800, 05004], lr: 0.066215, loss: 3.0387
2022-03-07 05:27:54 - train: epoch 0083, iter [04900, 05004], lr: 0.066215, loss: 3.1432
2022-03-07 05:28:27 - train: epoch 0083, iter [05000, 05004], lr: 0.066215, loss: 2.7829
2022-03-07 05:28:28 - train: epoch 083, train_loss: 2.8577
2022-03-07 05:29:42 - eval: epoch: 083, acc1: 54.478%, acc5: 79.624%, test_loss: 1.9413, per_image_load_time: 2.329ms, per_image_inference_time: 0.517ms
2022-03-07 05:29:42 - until epoch: 083, best_acc1: 57.052%
2022-03-07 05:29:42 - epoch 084 lr: 0.06545084971874737
2022-03-07 05:30:20 - train: epoch 0084, iter [00100, 05004], lr: 0.065451, loss: 2.8158
2022-03-07 05:30:55 - train: epoch 0084, iter [00200, 05004], lr: 0.065451, loss: 2.8834
2022-03-07 05:31:28 - train: epoch 0084, iter [00300, 05004], lr: 0.065451, loss: 2.6642
2022-03-07 05:32:02 - train: epoch 0084, iter [00400, 05004], lr: 0.065451, loss: 2.8769
2022-03-07 05:32:35 - train: epoch 0084, iter [00500, 05004], lr: 0.065451, loss: 2.7489
2022-03-07 05:33:09 - train: epoch 0084, iter [00600, 05004], lr: 0.065451, loss: 3.2537
2022-03-07 05:33:42 - train: epoch 0084, iter [00700, 05004], lr: 0.065451, loss: 3.0569
2022-03-07 05:34:15 - train: epoch 0084, iter [00800, 05004], lr: 0.065451, loss: 3.0701
2022-03-07 05:34:50 - train: epoch 0084, iter [00900, 05004], lr: 0.065451, loss: 2.8734
2022-03-07 05:35:23 - train: epoch 0084, iter [01000, 05004], lr: 0.065451, loss: 2.8933
2022-03-07 05:35:57 - train: epoch 0084, iter [01100, 05004], lr: 0.065451, loss: 2.9058
2022-03-07 05:36:30 - train: epoch 0084, iter [01200, 05004], lr: 0.065451, loss: 3.1537
2022-03-07 05:37:04 - train: epoch 0084, iter [01300, 05004], lr: 0.065451, loss: 2.7390
2022-03-07 05:37:37 - train: epoch 0084, iter [01400, 05004], lr: 0.065451, loss: 2.9900
2022-03-07 05:38:11 - train: epoch 0084, iter [01500, 05004], lr: 0.065451, loss: 2.8162
2022-03-07 05:38:45 - train: epoch 0084, iter [01600, 05004], lr: 0.065451, loss: 2.9165
2022-03-07 05:39:18 - train: epoch 0084, iter [01700, 05004], lr: 0.065451, loss: 2.7323
2022-03-07 05:39:53 - train: epoch 0084, iter [01800, 05004], lr: 0.065451, loss: 2.9092
2022-03-07 05:40:26 - train: epoch 0084, iter [01900, 05004], lr: 0.065451, loss: 3.0421
2022-03-07 05:40:59 - train: epoch 0084, iter [02000, 05004], lr: 0.065451, loss: 3.0754
2022-03-07 05:41:33 - train: epoch 0084, iter [02100, 05004], lr: 0.065451, loss: 2.7229
2022-03-07 05:42:07 - train: epoch 0084, iter [02200, 05004], lr: 0.065451, loss: 2.6678
2022-03-07 05:42:42 - train: epoch 0084, iter [02300, 05004], lr: 0.065451, loss: 2.9480
2022-03-07 05:43:15 - train: epoch 0084, iter [02400, 05004], lr: 0.065451, loss: 2.8096
2022-03-07 05:43:49 - train: epoch 0084, iter [02500, 05004], lr: 0.065451, loss: 2.7066
2022-03-07 05:44:22 - train: epoch 0084, iter [02600, 05004], lr: 0.065451, loss: 2.7912
2022-03-07 05:44:55 - train: epoch 0084, iter [02700, 05004], lr: 0.065451, loss: 2.9852
2022-03-07 05:45:29 - train: epoch 0084, iter [02800, 05004], lr: 0.065451, loss: 2.8499
2022-03-07 05:46:03 - train: epoch 0084, iter [02900, 05004], lr: 0.065451, loss: 2.7756
2022-03-07 05:46:37 - train: epoch 0084, iter [03000, 05004], lr: 0.065451, loss: 2.8577
2022-03-07 05:47:10 - train: epoch 0084, iter [03100, 05004], lr: 0.065451, loss: 2.8045
2022-03-07 05:47:44 - train: epoch 0084, iter [03200, 05004], lr: 0.065451, loss: 2.8308
2022-03-07 05:48:17 - train: epoch 0084, iter [03300, 05004], lr: 0.065451, loss: 2.9164
2022-03-07 05:48:52 - train: epoch 0084, iter [03400, 05004], lr: 0.065451, loss: 2.6581
2022-03-07 05:49:25 - train: epoch 0084, iter [03500, 05004], lr: 0.065451, loss: 2.7819
2022-03-07 05:49:58 - train: epoch 0084, iter [03600, 05004], lr: 0.065451, loss: 2.9083
2022-03-07 05:50:31 - train: epoch 0084, iter [03700, 05004], lr: 0.065451, loss: 2.8992
2022-03-07 05:51:05 - train: epoch 0084, iter [03800, 05004], lr: 0.065451, loss: 2.8703
2022-03-07 05:51:39 - train: epoch 0084, iter [03900, 05004], lr: 0.065451, loss: 2.7361
2022-03-07 05:52:13 - train: epoch 0084, iter [04000, 05004], lr: 0.065451, loss: 2.8698
2022-03-07 05:52:46 - train: epoch 0084, iter [04100, 05004], lr: 0.065451, loss: 2.9073
2022-03-07 05:53:21 - train: epoch 0084, iter [04200, 05004], lr: 0.065451, loss: 2.7384
2022-03-07 05:53:54 - train: epoch 0084, iter [04300, 05004], lr: 0.065451, loss: 2.8435
2022-03-07 05:54:28 - train: epoch 0084, iter [04400, 05004], lr: 0.065451, loss: 3.1954
2022-03-07 05:55:01 - train: epoch 0084, iter [04500, 05004], lr: 0.065451, loss: 2.9552
2022-03-07 05:55:35 - train: epoch 0084, iter [04600, 05004], lr: 0.065451, loss: 2.9357
2022-03-07 05:56:08 - train: epoch 0084, iter [04700, 05004], lr: 0.065451, loss: 3.1464
2022-03-07 05:56:42 - train: epoch 0084, iter [04800, 05004], lr: 0.065451, loss: 3.0269
2022-03-07 05:57:15 - train: epoch 0084, iter [04900, 05004], lr: 0.065451, loss: 2.8077
2022-03-07 05:57:47 - train: epoch 0084, iter [05000, 05004], lr: 0.065451, loss: 2.8177
2022-03-07 05:57:48 - train: epoch 084, train_loss: 2.8514
2022-03-07 05:59:01 - eval: epoch: 084, acc1: 56.236%, acc5: 81.496%, test_loss: 1.8389, per_image_load_time: 0.934ms, per_image_inference_time: 0.497ms
2022-03-07 05:59:02 - until epoch: 084, best_acc1: 57.052%
2022-03-07 05:59:02 - epoch 085 lr: 0.06468276691378154
2022-03-07 05:59:41 - train: epoch 0085, iter [00100, 05004], lr: 0.064683, loss: 2.7393
2022-03-07 06:00:15 - train: epoch 0085, iter [00200, 05004], lr: 0.064683, loss: 2.5610
2022-03-07 06:00:48 - train: epoch 0085, iter [00300, 05004], lr: 0.064683, loss: 2.9743
2022-03-07 06:01:22 - train: epoch 0085, iter [00400, 05004], lr: 0.064683, loss: 2.7601
2022-03-07 06:01:55 - train: epoch 0085, iter [00500, 05004], lr: 0.064683, loss: 3.1007
2022-03-07 06:02:29 - train: epoch 0085, iter [00600, 05004], lr: 0.064683, loss: 2.6257
2022-03-07 06:03:02 - train: epoch 0085, iter [00700, 05004], lr: 0.064683, loss: 2.6002
2022-03-07 06:03:37 - train: epoch 0085, iter [00800, 05004], lr: 0.064683, loss: 2.7715
2022-03-07 06:04:09 - train: epoch 0085, iter [00900, 05004], lr: 0.064683, loss: 2.8679
2022-03-07 06:04:43 - train: epoch 0085, iter [01000, 05004], lr: 0.064683, loss: 2.9667
2022-03-07 06:05:17 - train: epoch 0085, iter [01100, 05004], lr: 0.064683, loss: 2.8033
2022-03-07 06:05:50 - train: epoch 0085, iter [01200, 05004], lr: 0.064683, loss: 2.8967
2022-03-07 06:06:25 - train: epoch 0085, iter [01300, 05004], lr: 0.064683, loss: 2.7835
2022-03-07 06:06:59 - train: epoch 0085, iter [01400, 05004], lr: 0.064683, loss: 2.9144
2022-03-07 06:07:32 - train: epoch 0085, iter [01500, 05004], lr: 0.064683, loss: 2.9173
2022-03-07 06:08:06 - train: epoch 0085, iter [01600, 05004], lr: 0.064683, loss: 2.6270
2022-03-07 06:08:40 - train: epoch 0085, iter [01700, 05004], lr: 0.064683, loss: 3.2324
2022-03-07 06:09:13 - train: epoch 0085, iter [01800, 05004], lr: 0.064683, loss: 2.6877
2022-03-07 06:09:48 - train: epoch 0085, iter [01900, 05004], lr: 0.064683, loss: 2.7772
2022-03-07 06:10:22 - train: epoch 0085, iter [02000, 05004], lr: 0.064683, loss: 2.8583
2022-03-07 06:10:55 - train: epoch 0085, iter [02100, 05004], lr: 0.064683, loss: 2.6680
2022-03-07 06:11:29 - train: epoch 0085, iter [02200, 05004], lr: 0.064683, loss: 2.6097
2022-03-07 06:12:03 - train: epoch 0085, iter [02300, 05004], lr: 0.064683, loss: 2.6540
2022-03-07 06:12:37 - train: epoch 0085, iter [02400, 05004], lr: 0.064683, loss: 2.9573
2022-03-07 06:13:10 - train: epoch 0085, iter [02500, 05004], lr: 0.064683, loss: 3.0657
2022-03-07 06:13:45 - train: epoch 0085, iter [02600, 05004], lr: 0.064683, loss: 2.8317
2022-03-07 06:14:18 - train: epoch 0085, iter [02700, 05004], lr: 0.064683, loss: 2.5728
2022-03-07 06:14:52 - train: epoch 0085, iter [02800, 05004], lr: 0.064683, loss: 2.9703
2022-03-07 06:15:26 - train: epoch 0085, iter [02900, 05004], lr: 0.064683, loss: 3.0231
2022-03-07 06:16:00 - train: epoch 0085, iter [03000, 05004], lr: 0.064683, loss: 3.0315
2022-03-07 06:16:34 - train: epoch 0085, iter [03100, 05004], lr: 0.064683, loss: 2.7728
2022-03-07 06:17:08 - train: epoch 0085, iter [03200, 05004], lr: 0.064683, loss: 2.8232
2022-03-07 06:17:40 - train: epoch 0085, iter [03300, 05004], lr: 0.064683, loss: 2.8093
2022-03-07 06:18:14 - train: epoch 0085, iter [03400, 05004], lr: 0.064683, loss: 2.8282
2022-03-07 06:18:48 - train: epoch 0085, iter [03500, 05004], lr: 0.064683, loss: 2.8604
2022-03-07 06:19:22 - train: epoch 0085, iter [03600, 05004], lr: 0.064683, loss: 2.7433
2022-03-07 06:19:55 - train: epoch 0085, iter [03700, 05004], lr: 0.064683, loss: 2.7732
2022-03-07 06:20:29 - train: epoch 0085, iter [03800, 05004], lr: 0.064683, loss: 3.0290
2022-03-07 06:21:02 - train: epoch 0085, iter [03900, 05004], lr: 0.064683, loss: 2.7940
2022-03-07 06:21:35 - train: epoch 0085, iter [04000, 05004], lr: 0.064683, loss: 3.0321
2022-03-07 06:22:09 - train: epoch 0085, iter [04100, 05004], lr: 0.064683, loss: 2.6732
2022-03-07 06:22:43 - train: epoch 0085, iter [04200, 05004], lr: 0.064683, loss: 3.0356
2022-03-07 06:23:16 - train: epoch 0085, iter [04300, 05004], lr: 0.064683, loss: 2.5901
2022-03-07 06:23:50 - train: epoch 0085, iter [04400, 05004], lr: 0.064683, loss: 2.8128
2022-03-07 06:24:23 - train: epoch 0085, iter [04500, 05004], lr: 0.064683, loss: 2.8805
2022-03-07 06:24:57 - train: epoch 0085, iter [04600, 05004], lr: 0.064683, loss: 2.9857
2022-03-07 06:25:30 - train: epoch 0085, iter [04700, 05004], lr: 0.064683, loss: 3.1947
2022-03-07 06:26:04 - train: epoch 0085, iter [04800, 05004], lr: 0.064683, loss: 2.5928
2022-03-07 06:26:37 - train: epoch 0085, iter [04900, 05004], lr: 0.064683, loss: 2.8464
2022-03-07 06:27:09 - train: epoch 0085, iter [05000, 05004], lr: 0.064683, loss: 2.9834
2022-03-07 06:27:10 - train: epoch 085, train_loss: 2.8417
2022-03-07 06:28:24 - eval: epoch: 085, acc1: 56.546%, acc5: 80.940%, test_loss: 1.8428, per_image_load_time: 2.315ms, per_image_inference_time: 0.485ms
2022-03-07 06:28:25 - until epoch: 085, best_acc1: 57.052%
2022-03-07 06:28:25 - epoch 086 lr: 0.06391087319582264
2022-03-07 06:29:04 - train: epoch 0086, iter [00100, 05004], lr: 0.063911, loss: 2.8899
2022-03-07 06:29:37 - train: epoch 0086, iter [00200, 05004], lr: 0.063911, loss: 2.6915
2022-03-07 06:30:11 - train: epoch 0086, iter [00300, 05004], lr: 0.063911, loss: 3.2051
2022-03-07 06:30:44 - train: epoch 0086, iter [00400, 05004], lr: 0.063911, loss: 2.8408
2022-03-07 06:31:18 - train: epoch 0086, iter [00500, 05004], lr: 0.063911, loss: 2.9828
2022-03-07 06:31:52 - train: epoch 0086, iter [00600, 05004], lr: 0.063911, loss: 2.6550
2022-03-07 06:32:26 - train: epoch 0086, iter [00700, 05004], lr: 0.063911, loss: 2.6683
2022-03-07 06:33:00 - train: epoch 0086, iter [00800, 05004], lr: 0.063911, loss: 3.2974
2022-03-07 06:33:33 - train: epoch 0086, iter [00900, 05004], lr: 0.063911, loss: 2.8721
2022-03-07 06:34:07 - train: epoch 0086, iter [01000, 05004], lr: 0.063911, loss: 2.7437
2022-03-07 06:34:41 - train: epoch 0086, iter [01100, 05004], lr: 0.063911, loss: 3.0992
2022-03-07 06:35:16 - train: epoch 0086, iter [01200, 05004], lr: 0.063911, loss: 2.6243
2022-03-07 06:35:50 - train: epoch 0086, iter [01300, 05004], lr: 0.063911, loss: 2.8175
2022-03-07 06:36:24 - train: epoch 0086, iter [01400, 05004], lr: 0.063911, loss: 2.7529
2022-03-07 06:36:59 - train: epoch 0086, iter [01500, 05004], lr: 0.063911, loss: 2.6501
2022-03-07 06:37:32 - train: epoch 0086, iter [01600, 05004], lr: 0.063911, loss: 2.8846
2022-03-07 06:38:07 - train: epoch 0086, iter [01700, 05004], lr: 0.063911, loss: 2.8038
2022-03-07 06:38:40 - train: epoch 0086, iter [01800, 05004], lr: 0.063911, loss: 2.9885
2022-03-07 06:39:14 - train: epoch 0086, iter [01900, 05004], lr: 0.063911, loss: 2.9982
2022-03-07 06:39:49 - train: epoch 0086, iter [02000, 05004], lr: 0.063911, loss: 2.7535
2022-03-07 06:40:22 - train: epoch 0086, iter [02100, 05004], lr: 0.063911, loss: 2.4553
2022-03-07 06:40:57 - train: epoch 0086, iter [02200, 05004], lr: 0.063911, loss: 2.9283
2022-03-07 06:41:30 - train: epoch 0086, iter [02300, 05004], lr: 0.063911, loss: 2.6522
2022-03-07 06:42:05 - train: epoch 0086, iter [02400, 05004], lr: 0.063911, loss: 2.5443
2022-03-07 06:42:39 - train: epoch 0086, iter [02500, 05004], lr: 0.063911, loss: 2.6749
2022-03-07 06:43:13 - train: epoch 0086, iter [02600, 05004], lr: 0.063911, loss: 2.8042
2022-03-07 06:43:47 - train: epoch 0086, iter [02700, 05004], lr: 0.063911, loss: 2.8059
2022-03-07 06:44:22 - train: epoch 0086, iter [02800, 05004], lr: 0.063911, loss: 2.6963
2022-03-07 06:44:54 - train: epoch 0086, iter [02900, 05004], lr: 0.063911, loss: 2.5166
2022-03-07 06:45:29 - train: epoch 0086, iter [03000, 05004], lr: 0.063911, loss: 2.9957
2022-03-07 06:46:03 - train: epoch 0086, iter [03100, 05004], lr: 0.063911, loss: 2.7425
2022-03-07 06:46:37 - train: epoch 0086, iter [03200, 05004], lr: 0.063911, loss: 2.9497
2022-03-07 06:47:11 - train: epoch 0086, iter [03300, 05004], lr: 0.063911, loss: 2.6987
2022-03-07 06:47:45 - train: epoch 0086, iter [03400, 05004], lr: 0.063911, loss: 3.0419
2022-03-07 06:48:19 - train: epoch 0086, iter [03500, 05004], lr: 0.063911, loss: 2.6959
2022-03-07 06:48:53 - train: epoch 0086, iter [03600, 05004], lr: 0.063911, loss: 2.9973
2022-03-07 06:49:27 - train: epoch 0086, iter [03700, 05004], lr: 0.063911, loss: 2.9400
2022-03-07 06:50:01 - train: epoch 0086, iter [03800, 05004], lr: 0.063911, loss: 2.8492
2022-03-07 06:50:35 - train: epoch 0086, iter [03900, 05004], lr: 0.063911, loss: 2.8603
2022-03-07 06:51:08 - train: epoch 0086, iter [04000, 05004], lr: 0.063911, loss: 3.0013
2022-03-07 06:51:42 - train: epoch 0086, iter [04100, 05004], lr: 0.063911, loss: 2.7263
2022-03-07 06:52:16 - train: epoch 0086, iter [04200, 05004], lr: 0.063911, loss: 3.0174
2022-03-07 06:52:51 - train: epoch 0086, iter [04300, 05004], lr: 0.063911, loss: 3.1223
2022-03-07 06:53:25 - train: epoch 0086, iter [04400, 05004], lr: 0.063911, loss: 2.8291
2022-03-07 06:53:59 - train: epoch 0086, iter [04500, 05004], lr: 0.063911, loss: 2.5402
2022-03-07 06:54:33 - train: epoch 0086, iter [04600, 05004], lr: 0.063911, loss: 2.7573
2022-03-07 06:55:07 - train: epoch 0086, iter [04700, 05004], lr: 0.063911, loss: 2.9271
2022-03-07 06:55:42 - train: epoch 0086, iter [04800, 05004], lr: 0.063911, loss: 2.9168
2022-03-07 06:56:15 - train: epoch 0086, iter [04900, 05004], lr: 0.063911, loss: 3.1107
2022-03-07 06:56:48 - train: epoch 0086, iter [05000, 05004], lr: 0.063911, loss: 2.7823
2022-03-07 06:56:49 - train: epoch 086, train_loss: 2.8380
2022-03-07 06:58:04 - eval: epoch: 086, acc1: 55.626%, acc5: 80.292%, test_loss: 1.8898, per_image_load_time: 1.586ms, per_image_inference_time: 0.517ms
2022-03-07 06:58:04 - until epoch: 086, best_acc1: 57.052%
2022-03-07 06:58:04 - epoch 087 lr: 0.06313536890992935
2022-03-07 06:58:43 - train: epoch 0087, iter [00100, 05004], lr: 0.063135, loss: 3.0703
2022-03-07 06:59:17 - train: epoch 0087, iter [00200, 05004], lr: 0.063135, loss: 2.5072
2022-03-07 06:59:51 - train: epoch 0087, iter [00300, 05004], lr: 0.063135, loss: 2.9032
2022-03-07 07:00:25 - train: epoch 0087, iter [00400, 05004], lr: 0.063135, loss: 2.9700
2022-03-07 07:01:00 - train: epoch 0087, iter [00500, 05004], lr: 0.063135, loss: 2.5018
2022-03-07 07:01:34 - train: epoch 0087, iter [00600, 05004], lr: 0.063135, loss: 3.2378
2022-03-07 07:02:09 - train: epoch 0087, iter [00700, 05004], lr: 0.063135, loss: 2.7526
2022-03-07 07:02:43 - train: epoch 0087, iter [00800, 05004], lr: 0.063135, loss: 2.6989
2022-03-07 07:03:17 - train: epoch 0087, iter [00900, 05004], lr: 0.063135, loss: 2.8076
2022-03-07 07:03:51 - train: epoch 0087, iter [01000, 05004], lr: 0.063135, loss: 2.7944
2022-03-07 07:04:25 - train: epoch 0087, iter [01100, 05004], lr: 0.063135, loss: 2.9374
2022-03-07 07:05:00 - train: epoch 0087, iter [01200, 05004], lr: 0.063135, loss: 3.0054
2022-03-07 07:05:34 - train: epoch 0087, iter [01300, 05004], lr: 0.063135, loss: 3.1370
2022-03-07 07:06:09 - train: epoch 0087, iter [01400, 05004], lr: 0.063135, loss: 2.8284
2022-03-07 07:06:43 - train: epoch 0087, iter [01500, 05004], lr: 0.063135, loss: 2.5990
2022-03-07 07:07:17 - train: epoch 0087, iter [01600, 05004], lr: 0.063135, loss: 2.5673
2022-03-07 07:07:52 - train: epoch 0087, iter [01700, 05004], lr: 0.063135, loss: 2.9640
2022-03-07 07:08:26 - train: epoch 0087, iter [01800, 05004], lr: 0.063135, loss: 2.8658
2022-03-07 07:09:00 - train: epoch 0087, iter [01900, 05004], lr: 0.063135, loss: 2.8072
2022-03-07 07:09:34 - train: epoch 0087, iter [02000, 05004], lr: 0.063135, loss: 2.7063
2022-03-07 07:10:09 - train: epoch 0087, iter [02100, 05004], lr: 0.063135, loss: 3.1307
2022-03-07 07:10:44 - train: epoch 0087, iter [02200, 05004], lr: 0.063135, loss: 2.7224
2022-03-07 07:11:19 - train: epoch 0087, iter [02300, 05004], lr: 0.063135, loss: 2.8932
2022-03-07 07:11:53 - train: epoch 0087, iter [02400, 05004], lr: 0.063135, loss: 2.7820
2022-03-07 07:12:27 - train: epoch 0087, iter [02500, 05004], lr: 0.063135, loss: 2.8755
2022-03-07 07:13:01 - train: epoch 0087, iter [02600, 05004], lr: 0.063135, loss: 3.0218
2022-03-07 07:13:36 - train: epoch 0087, iter [02700, 05004], lr: 0.063135, loss: 2.8126
2022-03-07 07:14:10 - train: epoch 0087, iter [02800, 05004], lr: 0.063135, loss: 2.6476
2022-03-07 07:14:44 - train: epoch 0087, iter [02900, 05004], lr: 0.063135, loss: 2.7676
2022-03-07 07:15:18 - train: epoch 0087, iter [03000, 05004], lr: 0.063135, loss: 2.8742
2022-03-07 07:15:53 - train: epoch 0087, iter [03100, 05004], lr: 0.063135, loss: 2.9847
2022-03-07 07:16:27 - train: epoch 0087, iter [03200, 05004], lr: 0.063135, loss: 2.9674
2022-03-07 07:17:01 - train: epoch 0087, iter [03300, 05004], lr: 0.063135, loss: 2.6988
2022-03-07 07:17:36 - train: epoch 0087, iter [03400, 05004], lr: 0.063135, loss: 2.8587
2022-03-07 07:18:10 - train: epoch 0087, iter [03500, 05004], lr: 0.063135, loss: 2.8656
2022-03-07 07:18:44 - train: epoch 0087, iter [03600, 05004], lr: 0.063135, loss: 2.7204
2022-03-07 07:19:19 - train: epoch 0087, iter [03700, 05004], lr: 0.063135, loss: 2.8076
2022-03-07 07:19:53 - train: epoch 0087, iter [03800, 05004], lr: 0.063135, loss: 2.8555
2022-03-07 07:20:27 - train: epoch 0087, iter [03900, 05004], lr: 0.063135, loss: 3.1298
2022-03-07 07:21:02 - train: epoch 0087, iter [04000, 05004], lr: 0.063135, loss: 2.7961
2022-03-07 07:21:36 - train: epoch 0087, iter [04100, 05004], lr: 0.063135, loss: 2.7354
2022-03-07 07:22:10 - train: epoch 0087, iter [04200, 05004], lr: 0.063135, loss: 2.6473
2022-03-07 07:22:44 - train: epoch 0087, iter [04300, 05004], lr: 0.063135, loss: 2.8087
2022-03-07 07:23:18 - train: epoch 0087, iter [04400, 05004], lr: 0.063135, loss: 2.8733
2022-03-07 07:23:53 - train: epoch 0087, iter [04500, 05004], lr: 0.063135, loss: 2.7868
2022-03-07 07:24:28 - train: epoch 0087, iter [04600, 05004], lr: 0.063135, loss: 2.9499
2022-03-07 07:25:02 - train: epoch 0087, iter [04700, 05004], lr: 0.063135, loss: 2.8150
2022-03-07 07:25:36 - train: epoch 0087, iter [04800, 05004], lr: 0.063135, loss: 2.8689
2022-03-07 07:26:11 - train: epoch 0087, iter [04900, 05004], lr: 0.063135, loss: 2.8246
2022-03-07 07:26:43 - train: epoch 0087, iter [05000, 05004], lr: 0.063135, loss: 2.8331
2022-03-07 07:26:44 - train: epoch 087, train_loss: 2.8363
2022-03-07 07:27:58 - eval: epoch: 087, acc1: 54.424%, acc5: 79.566%, test_loss: 1.9413, per_image_load_time: 1.410ms, per_image_inference_time: 0.545ms
2022-03-07 07:27:59 - until epoch: 087, best_acc1: 57.052%
2022-03-07 07:27:59 - epoch 088 lr: 0.06235645533828348
2022-03-07 07:28:38 - train: epoch 0088, iter [00100, 05004], lr: 0.062356, loss: 2.6027
2022-03-07 07:29:12 - train: epoch 0088, iter [00200, 05004], lr: 0.062356, loss: 2.6769
2022-03-07 07:29:45 - train: epoch 0088, iter [00300, 05004], lr: 0.062356, loss: 3.0153
2022-03-07 07:30:20 - train: epoch 0088, iter [00400, 05004], lr: 0.062356, loss: 2.9502
2022-03-07 07:30:54 - train: epoch 0088, iter [00500, 05004], lr: 0.062356, loss: 2.7791
2022-03-07 07:31:29 - train: epoch 0088, iter [00600, 05004], lr: 0.062356, loss: 2.9009
2022-03-07 07:32:02 - train: epoch 0088, iter [00700, 05004], lr: 0.062356, loss: 2.6992
2022-03-07 07:32:36 - train: epoch 0088, iter [00800, 05004], lr: 0.062356, loss: 2.8804
2022-03-07 07:33:10 - train: epoch 0088, iter [00900, 05004], lr: 0.062356, loss: 2.9509
2022-03-07 07:33:45 - train: epoch 0088, iter [01000, 05004], lr: 0.062356, loss: 2.6660
2022-03-07 07:34:19 - train: epoch 0088, iter [01100, 05004], lr: 0.062356, loss: 2.8877
2022-03-07 07:34:54 - train: epoch 0088, iter [01200, 05004], lr: 0.062356, loss: 2.8015
2022-03-07 07:35:28 - train: epoch 0088, iter [01300, 05004], lr: 0.062356, loss: 2.8115
2022-03-07 07:36:03 - train: epoch 0088, iter [01400, 05004], lr: 0.062356, loss: 2.5229
2022-03-07 07:36:37 - train: epoch 0088, iter [01500, 05004], lr: 0.062356, loss: 2.8889
2022-03-07 07:37:11 - train: epoch 0088, iter [01600, 05004], lr: 0.062356, loss: 2.7498
2022-03-07 07:37:45 - train: epoch 0088, iter [01700, 05004], lr: 0.062356, loss: 2.7772
2022-03-07 07:38:20 - train: epoch 0088, iter [01800, 05004], lr: 0.062356, loss: 2.5646
2022-03-07 07:38:53 - train: epoch 0088, iter [01900, 05004], lr: 0.062356, loss: 2.7561
2022-03-07 07:39:28 - train: epoch 0088, iter [02000, 05004], lr: 0.062356, loss: 2.6785
2022-03-07 07:40:02 - train: epoch 0088, iter [02100, 05004], lr: 0.062356, loss: 2.9746
2022-03-07 07:40:36 - train: epoch 0088, iter [02200, 05004], lr: 0.062356, loss: 2.6693
2022-03-07 07:41:11 - train: epoch 0088, iter [02300, 05004], lr: 0.062356, loss: 2.7290
2022-03-07 07:41:45 - train: epoch 0088, iter [02400, 05004], lr: 0.062356, loss: 2.8775
2022-03-07 07:42:19 - train: epoch 0088, iter [02500, 05004], lr: 0.062356, loss: 2.9835
2022-03-07 07:42:54 - train: epoch 0088, iter [02600, 05004], lr: 0.062356, loss: 2.7415
2022-03-07 07:43:28 - train: epoch 0088, iter [02700, 05004], lr: 0.062356, loss: 2.8068
2022-03-07 07:44:02 - train: epoch 0088, iter [02800, 05004], lr: 0.062356, loss: 2.7879
2022-03-07 07:44:37 - train: epoch 0088, iter [02900, 05004], lr: 0.062356, loss: 2.8247
2022-03-07 07:45:11 - train: epoch 0088, iter [03000, 05004], lr: 0.062356, loss: 2.8015
2022-03-07 07:45:45 - train: epoch 0088, iter [03100, 05004], lr: 0.062356, loss: 2.5562
2022-03-07 07:46:19 - train: epoch 0088, iter [03200, 05004], lr: 0.062356, loss: 2.6618
2022-03-07 07:46:54 - train: epoch 0088, iter [03300, 05004], lr: 0.062356, loss: 2.5631
2022-03-07 07:47:27 - train: epoch 0088, iter [03400, 05004], lr: 0.062356, loss: 2.5271
2022-03-07 07:48:02 - train: epoch 0088, iter [03500, 05004], lr: 0.062356, loss: 2.4592
2022-03-07 07:48:35 - train: epoch 0088, iter [03600, 05004], lr: 0.062356, loss: 2.7358
2022-03-07 07:49:09 - train: epoch 0088, iter [03700, 05004], lr: 0.062356, loss: 2.8490
2022-03-07 07:49:43 - train: epoch 0088, iter [03800, 05004], lr: 0.062356, loss: 2.7277
2022-03-07 07:50:18 - train: epoch 0088, iter [03900, 05004], lr: 0.062356, loss: 2.9132
2022-03-07 07:50:52 - train: epoch 0088, iter [04000, 05004], lr: 0.062356, loss: 2.6503
2022-03-07 07:51:27 - train: epoch 0088, iter [04100, 05004], lr: 0.062356, loss: 3.0138
2022-03-07 07:52:00 - train: epoch 0088, iter [04200, 05004], lr: 0.062356, loss: 3.0124
2022-03-07 07:52:35 - train: epoch 0088, iter [04300, 05004], lr: 0.062356, loss: 3.0113
2022-03-07 07:53:09 - train: epoch 0088, iter [04400, 05004], lr: 0.062356, loss: 2.8348
2022-03-07 07:53:43 - train: epoch 0088, iter [04500, 05004], lr: 0.062356, loss: 3.0365
2022-03-07 07:54:17 - train: epoch 0088, iter [04600, 05004], lr: 0.062356, loss: 2.8841
2022-03-07 07:54:51 - train: epoch 0088, iter [04700, 05004], lr: 0.062356, loss: 3.0199
2022-03-07 07:55:25 - train: epoch 0088, iter [04800, 05004], lr: 0.062356, loss: 2.9033
2022-03-07 07:56:00 - train: epoch 0088, iter [04900, 05004], lr: 0.062356, loss: 2.8388
2022-03-07 07:56:32 - train: epoch 0088, iter [05000, 05004], lr: 0.062356, loss: 3.0169
2022-03-07 07:56:33 - train: epoch 088, train_loss: 2.8317
2022-03-07 07:57:48 - eval: epoch: 088, acc1: 56.542%, acc5: 81.324%, test_loss: 1.8231, per_image_load_time: 2.111ms, per_image_inference_time: 0.532ms
2022-03-07 07:57:49 - until epoch: 088, best_acc1: 57.052%
2022-03-07 07:57:49 - epoch 089 lr: 0.06157433464794716
2022-03-07 07:58:28 - train: epoch 0089, iter [00100, 05004], lr: 0.061574, loss: 2.8712
2022-03-07 07:59:02 - train: epoch 0089, iter [00200, 05004], lr: 0.061574, loss: 2.7532
2022-03-07 07:59:36 - train: epoch 0089, iter [00300, 05004], lr: 0.061574, loss: 2.9180
2022-03-07 08:00:11 - train: epoch 0089, iter [00400, 05004], lr: 0.061574, loss: 2.7745
2022-03-07 08:00:44 - train: epoch 0089, iter [00500, 05004], lr: 0.061574, loss: 2.8619
2022-03-07 08:01:19 - train: epoch 0089, iter [00600, 05004], lr: 0.061574, loss: 2.9623
2022-03-07 08:01:53 - train: epoch 0089, iter [00700, 05004], lr: 0.061574, loss: 2.8747
2022-03-07 08:02:27 - train: epoch 0089, iter [00800, 05004], lr: 0.061574, loss: 2.9407
2022-03-07 08:03:01 - train: epoch 0089, iter [00900, 05004], lr: 0.061574, loss: 2.9336
2022-03-07 08:03:35 - train: epoch 0089, iter [01000, 05004], lr: 0.061574, loss: 2.8297
2022-03-07 08:04:10 - train: epoch 0089, iter [01100, 05004], lr: 0.061574, loss: 2.7959
2022-03-07 08:04:44 - train: epoch 0089, iter [01200, 05004], lr: 0.061574, loss: 3.1202
2022-03-07 08:05:18 - train: epoch 0089, iter [01300, 05004], lr: 0.061574, loss: 2.7793
2022-03-07 08:05:52 - train: epoch 0089, iter [01400, 05004], lr: 0.061574, loss: 2.7690
2022-03-07 08:06:28 - train: epoch 0089, iter [01500, 05004], lr: 0.061574, loss: 2.8824
2022-03-07 08:07:02 - train: epoch 0089, iter [01600, 05004], lr: 0.061574, loss: 2.9086
2022-03-07 08:07:36 - train: epoch 0089, iter [01700, 05004], lr: 0.061574, loss: 2.7378
2022-03-07 08:08:10 - train: epoch 0089, iter [01800, 05004], lr: 0.061574, loss: 2.9213
2022-03-07 08:08:45 - train: epoch 0089, iter [01900, 05004], lr: 0.061574, loss: 2.7072
2022-03-07 08:09:20 - train: epoch 0089, iter [02000, 05004], lr: 0.061574, loss: 2.6355
2022-03-07 08:09:54 - train: epoch 0089, iter [02100, 05004], lr: 0.061574, loss: 2.8231
2022-03-07 08:10:28 - train: epoch 0089, iter [02200, 05004], lr: 0.061574, loss: 2.7476
2022-03-07 08:11:02 - train: epoch 0089, iter [02300, 05004], lr: 0.061574, loss: 2.8294
2022-03-07 08:11:36 - train: epoch 0089, iter [02400, 05004], lr: 0.061574, loss: 3.0506
2022-03-07 08:12:11 - train: epoch 0089, iter [02500, 05004], lr: 0.061574, loss: 2.9390
2022-03-07 08:12:46 - train: epoch 0089, iter [02600, 05004], lr: 0.061574, loss: 2.8227
2022-03-07 08:13:19 - train: epoch 0089, iter [02700, 05004], lr: 0.061574, loss: 2.9943
2022-03-07 08:13:54 - train: epoch 0089, iter [02800, 05004], lr: 0.061574, loss: 2.9351
2022-03-07 08:14:28 - train: epoch 0089, iter [02900, 05004], lr: 0.061574, loss: 2.7918
2022-03-07 08:15:02 - train: epoch 0089, iter [03000, 05004], lr: 0.061574, loss: 2.9465
2022-03-07 08:15:37 - train: epoch 0089, iter [03100, 05004], lr: 0.061574, loss: 2.9182
2022-03-07 08:16:11 - train: epoch 0089, iter [03200, 05004], lr: 0.061574, loss: 2.9591
2022-03-07 08:16:46 - train: epoch 0089, iter [03300, 05004], lr: 0.061574, loss: 2.9149
2022-03-07 08:17:20 - train: epoch 0089, iter [03400, 05004], lr: 0.061574, loss: 2.6905
2022-03-07 08:17:55 - train: epoch 0089, iter [03500, 05004], lr: 0.061574, loss: 2.5034
2022-03-07 08:18:28 - train: epoch 0089, iter [03600, 05004], lr: 0.061574, loss: 2.8578
2022-03-07 08:19:03 - train: epoch 0089, iter [03700, 05004], lr: 0.061574, loss: 2.9960
2022-03-07 08:19:37 - train: epoch 0089, iter [03800, 05004], lr: 0.061574, loss: 2.7328
2022-03-07 08:20:12 - train: epoch 0089, iter [03900, 05004], lr: 0.061574, loss: 2.8197
2022-03-07 08:20:45 - train: epoch 0089, iter [04000, 05004], lr: 0.061574, loss: 2.7105
2022-03-07 08:21:21 - train: epoch 0089, iter [04100, 05004], lr: 0.061574, loss: 3.1334
2022-03-07 08:21:54 - train: epoch 0089, iter [04200, 05004], lr: 0.061574, loss: 2.9640
2022-03-07 08:22:29 - train: epoch 0089, iter [04300, 05004], lr: 0.061574, loss: 2.8690
2022-03-07 08:23:03 - train: epoch 0089, iter [04400, 05004], lr: 0.061574, loss: 2.7693
2022-03-07 08:23:37 - train: epoch 0089, iter [04500, 05004], lr: 0.061574, loss: 2.8398
2022-03-07 08:24:11 - train: epoch 0089, iter [04600, 05004], lr: 0.061574, loss: 2.5294
2022-03-07 08:24:46 - train: epoch 0089, iter [04700, 05004], lr: 0.061574, loss: 2.7715
2022-03-07 08:25:19 - train: epoch 0089, iter [04800, 05004], lr: 0.061574, loss: 2.9918
2022-03-07 08:25:54 - train: epoch 0089, iter [04900, 05004], lr: 0.061574, loss: 2.6550
2022-03-07 08:26:27 - train: epoch 0089, iter [05000, 05004], lr: 0.061574, loss: 2.7479
2022-03-07 08:26:28 - train: epoch 089, train_loss: 2.8248
2022-03-07 08:27:42 - eval: epoch: 089, acc1: 56.744%, acc5: 81.280%, test_loss: 1.8228, per_image_load_time: 1.128ms, per_image_inference_time: 0.546ms
2022-03-07 08:27:42 - until epoch: 089, best_acc1: 57.052%
2022-03-07 08:27:42 - epoch 090 lr: 0.06078920983839031
2022-03-07 08:28:22 - train: epoch 0090, iter [00100, 05004], lr: 0.060789, loss: 2.7541
2022-03-07 08:28:55 - train: epoch 0090, iter [00200, 05004], lr: 0.060789, loss: 2.7884
2022-03-07 08:29:29 - train: epoch 0090, iter [00300, 05004], lr: 0.060789, loss: 2.9499
2022-03-07 08:30:04 - train: epoch 0090, iter [00400, 05004], lr: 0.060789, loss: 2.6795
2022-03-07 08:30:38 - train: epoch 0090, iter [00500, 05004], lr: 0.060789, loss: 2.7347
2022-03-07 08:31:12 - train: epoch 0090, iter [00600, 05004], lr: 0.060789, loss: 2.7065
2022-03-07 08:31:46 - train: epoch 0090, iter [00700, 05004], lr: 0.060789, loss: 3.0339
2022-03-07 08:32:21 - train: epoch 0090, iter [00800, 05004], lr: 0.060789, loss: 3.1200
2022-03-07 08:32:56 - train: epoch 0090, iter [00900, 05004], lr: 0.060789, loss: 3.0615
2022-03-07 08:33:29 - train: epoch 0090, iter [01000, 05004], lr: 0.060789, loss: 2.8289
2022-03-07 08:34:05 - train: epoch 0090, iter [01100, 05004], lr: 0.060789, loss: 2.8239
2022-03-07 08:34:39 - train: epoch 0090, iter [01200, 05004], lr: 0.060789, loss: 2.8442
2022-03-07 08:35:13 - train: epoch 0090, iter [01300, 05004], lr: 0.060789, loss: 2.8128
2022-03-07 08:35:47 - train: epoch 0090, iter [01400, 05004], lr: 0.060789, loss: 2.6329
2022-03-07 08:36:22 - train: epoch 0090, iter [01500, 05004], lr: 0.060789, loss: 2.9443
2022-03-07 08:36:57 - train: epoch 0090, iter [01600, 05004], lr: 0.060789, loss: 2.5453
2022-03-07 08:37:31 - train: epoch 0090, iter [01700, 05004], lr: 0.060789, loss: 2.7912
2022-03-07 08:38:05 - train: epoch 0090, iter [01800, 05004], lr: 0.060789, loss: 2.3885
2022-03-07 08:38:39 - train: epoch 0090, iter [01900, 05004], lr: 0.060789, loss: 2.6690
2022-03-07 08:39:13 - train: epoch 0090, iter [02000, 05004], lr: 0.060789, loss: 2.8223
2022-03-07 08:39:47 - train: epoch 0090, iter [02100, 05004], lr: 0.060789, loss: 2.9063
2022-03-07 08:40:22 - train: epoch 0090, iter [02200, 05004], lr: 0.060789, loss: 3.0235
2022-03-07 08:40:57 - train: epoch 0090, iter [02300, 05004], lr: 0.060789, loss: 3.1242
2022-03-07 08:41:30 - train: epoch 0090, iter [02400, 05004], lr: 0.060789, loss: 2.8511
2022-03-07 08:42:05 - train: epoch 0090, iter [02500, 05004], lr: 0.060789, loss: 2.8483
2022-03-07 08:42:39 - train: epoch 0090, iter [02600, 05004], lr: 0.060789, loss: 2.8037
2022-03-07 08:43:13 - train: epoch 0090, iter [02700, 05004], lr: 0.060789, loss: 2.9208
2022-03-07 08:43:47 - train: epoch 0090, iter [02800, 05004], lr: 0.060789, loss: 2.8740
2022-03-07 08:44:22 - train: epoch 0090, iter [02900, 05004], lr: 0.060789, loss: 3.0946
2022-03-07 08:44:56 - train: epoch 0090, iter [03000, 05004], lr: 0.060789, loss: 2.7691
2022-03-07 08:45:31 - train: epoch 0090, iter [03100, 05004], lr: 0.060789, loss: 2.8504
2022-03-07 08:46:05 - train: epoch 0090, iter [03200, 05004], lr: 0.060789, loss: 2.5689
2022-03-07 08:46:39 - train: epoch 0090, iter [03300, 05004], lr: 0.060789, loss: 2.9849
2022-03-07 08:47:13 - train: epoch 0090, iter [03400, 05004], lr: 0.060789, loss: 2.8069
2022-03-07 08:47:47 - train: epoch 0090, iter [03500, 05004], lr: 0.060789, loss: 2.6846
2022-03-07 08:48:21 - train: epoch 0090, iter [03600, 05004], lr: 0.060789, loss: 2.5945
2022-03-07 08:48:56 - train: epoch 0090, iter [03700, 05004], lr: 0.060789, loss: 2.9453
2022-03-07 08:49:30 - train: epoch 0090, iter [03800, 05004], lr: 0.060789, loss: 2.9476
2022-03-07 08:50:04 - train: epoch 0090, iter [03900, 05004], lr: 0.060789, loss: 2.5886
2022-03-07 08:50:39 - train: epoch 0090, iter [04000, 05004], lr: 0.060789, loss: 2.6576
2022-03-07 08:51:13 - train: epoch 0090, iter [04100, 05004], lr: 0.060789, loss: 2.9017
2022-03-07 08:51:48 - train: epoch 0090, iter [04200, 05004], lr: 0.060789, loss: 2.8136
2022-03-07 08:52:22 - train: epoch 0090, iter [04300, 05004], lr: 0.060789, loss: 2.6413
2022-03-07 08:52:56 - train: epoch 0090, iter [04400, 05004], lr: 0.060789, loss: 3.0651
2022-03-07 08:53:30 - train: epoch 0090, iter [04500, 05004], lr: 0.060789, loss: 2.6858
2022-03-07 08:54:05 - train: epoch 0090, iter [04600, 05004], lr: 0.060789, loss: 2.7049
2022-03-07 08:54:40 - train: epoch 0090, iter [04700, 05004], lr: 0.060789, loss: 2.5884
2022-03-07 08:55:13 - train: epoch 0090, iter [04800, 05004], lr: 0.060789, loss: 3.0453
2022-03-07 08:55:47 - train: epoch 0090, iter [04900, 05004], lr: 0.060789, loss: 2.9899
2022-03-07 08:56:21 - train: epoch 0090, iter [05000, 05004], lr: 0.060789, loss: 2.7616
2022-03-07 08:56:22 - train: epoch 090, train_loss: 2.8170
2022-03-07 08:57:37 - eval: epoch: 090, acc1: 58.346%, acc5: 82.358%, test_loss: 1.7467, per_image_load_time: 2.191ms, per_image_inference_time: 0.536ms
2022-03-07 08:57:38 - until epoch: 090, best_acc1: 58.346%
2022-03-07 08:57:38 - epoch 091 lr: 0.060001284688802226
2022-03-07 08:58:16 - train: epoch 0091, iter [00100, 05004], lr: 0.060001, loss: 2.7018
2022-03-07 08:58:51 - train: epoch 0091, iter [00200, 05004], lr: 0.060001, loss: 2.7968
2022-03-07 08:59:25 - train: epoch 0091, iter [00300, 05004], lr: 0.060001, loss: 2.8642
2022-03-07 08:59:58 - train: epoch 0091, iter [00400, 05004], lr: 0.060001, loss: 2.8777
2022-03-07 09:00:32 - train: epoch 0091, iter [00500, 05004], lr: 0.060001, loss: 2.7017
2022-03-07 09:01:06 - train: epoch 0091, iter [00600, 05004], lr: 0.060001, loss: 3.0333
2022-03-07 09:01:40 - train: epoch 0091, iter [00700, 05004], lr: 0.060001, loss: 2.8566
2022-03-07 09:02:14 - train: epoch 0091, iter [00800, 05004], lr: 0.060001, loss: 2.4614
2022-03-07 09:02:48 - train: epoch 0091, iter [00900, 05004], lr: 0.060001, loss: 2.7774
2022-03-07 09:03:22 - train: epoch 0091, iter [01000, 05004], lr: 0.060001, loss: 2.6211
2022-03-07 09:03:56 - train: epoch 0091, iter [01100, 05004], lr: 0.060001, loss: 2.6761
2022-03-07 09:04:30 - train: epoch 0091, iter [01200, 05004], lr: 0.060001, loss: 2.5061
2022-03-07 09:05:05 - train: epoch 0091, iter [01300, 05004], lr: 0.060001, loss: 2.5091
2022-03-07 09:05:38 - train: epoch 0091, iter [01400, 05004], lr: 0.060001, loss: 2.7336
2022-03-07 09:06:13 - train: epoch 0091, iter [01500, 05004], lr: 0.060001, loss: 2.8609
2022-03-07 09:06:47 - train: epoch 0091, iter [01600, 05004], lr: 0.060001, loss: 2.8889
2022-03-07 09:07:22 - train: epoch 0091, iter [01700, 05004], lr: 0.060001, loss: 3.1478
2022-03-07 09:07:57 - train: epoch 0091, iter [01800, 05004], lr: 0.060001, loss: 2.6467
2022-03-07 09:08:30 - train: epoch 0091, iter [01900, 05004], lr: 0.060001, loss: 2.7538
2022-03-07 09:09:05 - train: epoch 0091, iter [02000, 05004], lr: 0.060001, loss: 2.8143
2022-03-07 09:09:39 - train: epoch 0091, iter [02100, 05004], lr: 0.060001, loss: 2.6736
2022-03-07 09:10:13 - train: epoch 0091, iter [02200, 05004], lr: 0.060001, loss: 3.0512
2022-03-07 09:10:47 - train: epoch 0091, iter [02300, 05004], lr: 0.060001, loss: 2.8644
2022-03-07 09:11:20 - train: epoch 0091, iter [02400, 05004], lr: 0.060001, loss: 2.6337
2022-03-07 09:11:55 - train: epoch 0091, iter [02500, 05004], lr: 0.060001, loss: 2.6620
2022-03-07 09:12:29 - train: epoch 0091, iter [02600, 05004], lr: 0.060001, loss: 2.7119
2022-03-07 09:13:04 - train: epoch 0091, iter [02700, 05004], lr: 0.060001, loss: 2.8167
2022-03-07 09:13:37 - train: epoch 0091, iter [02800, 05004], lr: 0.060001, loss: 2.9022
2022-03-07 09:14:12 - train: epoch 0091, iter [02900, 05004], lr: 0.060001, loss: 2.8969
2022-03-07 09:14:45 - train: epoch 0091, iter [03000, 05004], lr: 0.060001, loss: 3.0266
2022-03-07 09:15:20 - train: epoch 0091, iter [03100, 05004], lr: 0.060001, loss: 2.7224
2022-03-07 09:15:54 - train: epoch 0091, iter [03200, 05004], lr: 0.060001, loss: 2.7676
2022-03-07 09:16:29 - train: epoch 0091, iter [03300, 05004], lr: 0.060001, loss: 2.6339
2022-03-07 09:17:03 - train: epoch 0091, iter [03400, 05004], lr: 0.060001, loss: 2.7328
2022-03-07 09:17:37 - train: epoch 0091, iter [03500, 05004], lr: 0.060001, loss: 2.8348
2022-03-07 09:18:11 - train: epoch 0091, iter [03600, 05004], lr: 0.060001, loss: 2.6534
2022-03-07 09:18:46 - train: epoch 0091, iter [03700, 05004], lr: 0.060001, loss: 2.9382
2022-03-07 09:19:20 - train: epoch 0091, iter [03800, 05004], lr: 0.060001, loss: 2.8455
2022-03-07 09:19:55 - train: epoch 0091, iter [03900, 05004], lr: 0.060001, loss: 2.7459
2022-03-07 09:20:29 - train: epoch 0091, iter [04000, 05004], lr: 0.060001, loss: 2.9724
2022-03-07 09:21:04 - train: epoch 0091, iter [04100, 05004], lr: 0.060001, loss: 2.8449
2022-03-07 09:21:37 - train: epoch 0091, iter [04200, 05004], lr: 0.060001, loss: 2.8125
2022-03-07 09:22:12 - train: epoch 0091, iter [04300, 05004], lr: 0.060001, loss: 2.9156
2022-03-07 09:22:46 - train: epoch 0091, iter [04400, 05004], lr: 0.060001, loss: 2.7324
2022-03-07 09:23:21 - train: epoch 0091, iter [04500, 05004], lr: 0.060001, loss: 2.7586
2022-03-07 09:23:55 - train: epoch 0091, iter [04600, 05004], lr: 0.060001, loss: 2.7301
2022-03-07 09:24:29 - train: epoch 0091, iter [04700, 05004], lr: 0.060001, loss: 2.9607
2022-03-07 09:25:04 - train: epoch 0091, iter [04800, 05004], lr: 0.060001, loss: 2.7704
2022-03-07 09:25:38 - train: epoch 0091, iter [04900, 05004], lr: 0.060001, loss: 2.7484
2022-03-07 09:26:11 - train: epoch 0091, iter [05000, 05004], lr: 0.060001, loss: 3.0183
2022-03-07 09:26:12 - train: epoch 091, train_loss: 2.8098
2022-03-07 09:27:26 - eval: epoch: 091, acc1: 57.380%, acc5: 81.870%, test_loss: 1.7952, per_image_load_time: 1.967ms, per_image_inference_time: 0.536ms
2022-03-07 09:27:27 - until epoch: 091, best_acc1: 58.346%
2022-03-07 09:27:27 - epoch 092 lr: 0.05921076370520058
2022-03-07 09:28:07 - train: epoch 0092, iter [00100, 05004], lr: 0.059211, loss: 2.9948
2022-03-07 09:28:41 - train: epoch 0092, iter [00200, 05004], lr: 0.059211, loss: 2.8452
2022-03-07 09:29:15 - train: epoch 0092, iter [00300, 05004], lr: 0.059211, loss: 2.8246
2022-03-07 09:29:50 - train: epoch 0092, iter [00400, 05004], lr: 0.059211, loss: 2.6021
2022-03-07 09:30:23 - train: epoch 0092, iter [00500, 05004], lr: 0.059211, loss: 2.7620
2022-03-07 09:30:57 - train: epoch 0092, iter [00600, 05004], lr: 0.059211, loss: 2.8685
2022-03-07 09:31:31 - train: epoch 0092, iter [00700, 05004], lr: 0.059211, loss: 2.7724
2022-03-07 09:32:06 - train: epoch 0092, iter [00800, 05004], lr: 0.059211, loss: 2.9272
2022-03-07 09:32:39 - train: epoch 0092, iter [00900, 05004], lr: 0.059211, loss: 2.9420
2022-03-07 09:33:14 - train: epoch 0092, iter [01000, 05004], lr: 0.059211, loss: 2.8993
2022-03-07 09:33:47 - train: epoch 0092, iter [01100, 05004], lr: 0.059211, loss: 2.6051
2022-03-07 09:34:22 - train: epoch 0092, iter [01200, 05004], lr: 0.059211, loss: 2.5665
2022-03-07 09:34:55 - train: epoch 0092, iter [01300, 05004], lr: 0.059211, loss: 2.8185
2022-03-07 09:35:29 - train: epoch 0092, iter [01400, 05004], lr: 0.059211, loss: 2.6481
2022-03-07 09:36:04 - train: epoch 0092, iter [01500, 05004], lr: 0.059211, loss: 2.8692
2022-03-07 09:36:38 - train: epoch 0092, iter [01600, 05004], lr: 0.059211, loss: 2.6119
2022-03-07 09:37:12 - train: epoch 0092, iter [01700, 05004], lr: 0.059211, loss: 2.8941
2022-03-07 09:37:46 - train: epoch 0092, iter [01800, 05004], lr: 0.059211, loss: 2.8138
2022-03-07 09:38:20 - train: epoch 0092, iter [01900, 05004], lr: 0.059211, loss: 2.5277
2022-03-07 09:38:54 - train: epoch 0092, iter [02000, 05004], lr: 0.059211, loss: 2.7460
2022-03-07 09:39:28 - train: epoch 0092, iter [02100, 05004], lr: 0.059211, loss: 2.8220
2022-03-07 09:40:02 - train: epoch 0092, iter [02200, 05004], lr: 0.059211, loss: 3.1495
2022-03-07 09:40:37 - train: epoch 0092, iter [02300, 05004], lr: 0.059211, loss: 2.7821
2022-03-07 09:41:11 - train: epoch 0092, iter [02400, 05004], lr: 0.059211, loss: 2.7121
2022-03-07 09:41:45 - train: epoch 0092, iter [02500, 05004], lr: 0.059211, loss: 2.7424
2022-03-07 09:42:19 - train: epoch 0092, iter [02600, 05004], lr: 0.059211, loss: 3.0227
2022-03-07 09:42:54 - train: epoch 0092, iter [02700, 05004], lr: 0.059211, loss: 2.7854
2022-03-07 09:43:28 - train: epoch 0092, iter [02800, 05004], lr: 0.059211, loss: 2.5890
2022-03-07 09:44:02 - train: epoch 0092, iter [02900, 05004], lr: 0.059211, loss: 2.8648
2022-03-07 09:44:36 - train: epoch 0092, iter [03000, 05004], lr: 0.059211, loss: 2.6710
2022-03-07 09:45:10 - train: epoch 0092, iter [03100, 05004], lr: 0.059211, loss: 2.9445
2022-03-07 09:45:44 - train: epoch 0092, iter [03200, 05004], lr: 0.059211, loss: 2.8066
2022-03-07 09:46:18 - train: epoch 0092, iter [03300, 05004], lr: 0.059211, loss: 2.6615
2022-03-07 09:46:53 - train: epoch 0092, iter [03400, 05004], lr: 0.059211, loss: 2.8641
2022-03-07 09:47:26 - train: epoch 0092, iter [03500, 05004], lr: 0.059211, loss: 2.9279
2022-03-07 09:48:00 - train: epoch 0092, iter [03600, 05004], lr: 0.059211, loss: 2.8339
2022-03-07 09:48:34 - train: epoch 0092, iter [03700, 05004], lr: 0.059211, loss: 2.9169
2022-03-07 09:49:09 - train: epoch 0092, iter [03800, 05004], lr: 0.059211, loss: 3.1135
2022-03-07 09:49:44 - train: epoch 0092, iter [03900, 05004], lr: 0.059211, loss: 2.8056
2022-03-07 09:50:18 - train: epoch 0092, iter [04000, 05004], lr: 0.059211, loss: 3.0644
2022-03-07 09:50:52 - train: epoch 0092, iter [04100, 05004], lr: 0.059211, loss: 2.5901
2022-03-07 09:51:26 - train: epoch 0092, iter [04200, 05004], lr: 0.059211, loss: 2.6691
2022-03-07 09:52:01 - train: epoch 0092, iter [04300, 05004], lr: 0.059211, loss: 2.7009
2022-03-07 09:52:35 - train: epoch 0092, iter [04400, 05004], lr: 0.059211, loss: 2.9907
2022-03-07 09:53:09 - train: epoch 0092, iter [04500, 05004], lr: 0.059211, loss: 2.9597
2022-03-07 09:53:43 - train: epoch 0092, iter [04600, 05004], lr: 0.059211, loss: 2.6825
2022-03-07 09:54:17 - train: epoch 0092, iter [04700, 05004], lr: 0.059211, loss: 2.6113
2022-03-07 09:54:51 - train: epoch 0092, iter [04800, 05004], lr: 0.059211, loss: 2.5335
2022-03-07 09:55:25 - train: epoch 0092, iter [04900, 05004], lr: 0.059211, loss: 2.6350
2022-03-07 09:55:58 - train: epoch 0092, iter [05000, 05004], lr: 0.059211, loss: 2.8563
2022-03-07 09:55:59 - train: epoch 092, train_loss: 2.8060
2022-03-07 09:57:14 - eval: epoch: 092, acc1: 55.354%, acc5: 80.244%, test_loss: 1.9016, per_image_load_time: 2.225ms, per_image_inference_time: 0.571ms
2022-03-07 09:57:15 - until epoch: 092, best_acc1: 58.346%
2022-03-07 09:57:15 - epoch 093 lr: 0.05841785206735192
2022-03-07 09:57:54 - train: epoch 0093, iter [00100, 05004], lr: 0.058418, loss: 2.8041
2022-03-07 09:58:28 - train: epoch 0093, iter [00200, 05004], lr: 0.058418, loss: 2.7232
2022-03-07 09:59:02 - train: epoch 0093, iter [00300, 05004], lr: 0.058418, loss: 2.8064
2022-03-07 09:59:36 - train: epoch 0093, iter [00400, 05004], lr: 0.058418, loss: 2.7977
2022-03-07 10:00:10 - train: epoch 0093, iter [00500, 05004], lr: 0.058418, loss: 3.0943
2022-03-07 10:00:44 - train: epoch 0093, iter [00600, 05004], lr: 0.058418, loss: 2.5455
2022-03-07 10:01:19 - train: epoch 0093, iter [00700, 05004], lr: 0.058418, loss: 2.8996
2022-03-07 10:01:53 - train: epoch 0093, iter [00800, 05004], lr: 0.058418, loss: 2.8839
2022-03-07 10:02:27 - train: epoch 0093, iter [00900, 05004], lr: 0.058418, loss: 2.8786
2022-03-07 10:03:02 - train: epoch 0093, iter [01000, 05004], lr: 0.058418, loss: 2.4705
2022-03-07 10:03:36 - train: epoch 0093, iter [01100, 05004], lr: 0.058418, loss: 2.7554
2022-03-07 10:04:11 - train: epoch 0093, iter [01200, 05004], lr: 0.058418, loss: 2.5898
2022-03-07 10:04:45 - train: epoch 0093, iter [01300, 05004], lr: 0.058418, loss: 3.0021
2022-03-07 10:05:19 - train: epoch 0093, iter [01400, 05004], lr: 0.058418, loss: 2.5859
2022-03-07 10:05:54 - train: epoch 0093, iter [01500, 05004], lr: 0.058418, loss: 2.7764
2022-03-07 10:06:27 - train: epoch 0093, iter [01600, 05004], lr: 0.058418, loss: 2.8565
2022-03-07 10:07:02 - train: epoch 0093, iter [01700, 05004], lr: 0.058418, loss: 2.5266
2022-03-07 10:07:36 - train: epoch 0093, iter [01800, 05004], lr: 0.058418, loss: 2.6664
2022-03-07 10:08:10 - train: epoch 0093, iter [01900, 05004], lr: 0.058418, loss: 2.7107
2022-03-07 10:08:45 - train: epoch 0093, iter [02000, 05004], lr: 0.058418, loss: 2.6542
2022-03-07 10:09:19 - train: epoch 0093, iter [02100, 05004], lr: 0.058418, loss: 2.5204
2022-03-07 10:09:53 - train: epoch 0093, iter [02200, 05004], lr: 0.058418, loss: 2.7669
2022-03-07 10:10:28 - train: epoch 0093, iter [02300, 05004], lr: 0.058418, loss: 2.8644
2022-03-07 10:11:01 - train: epoch 0093, iter [02400, 05004], lr: 0.058418, loss: 2.5742
2022-03-07 10:11:36 - train: epoch 0093, iter [02500, 05004], lr: 0.058418, loss: 2.8016
2022-03-07 10:12:10 - train: epoch 0093, iter [02600, 05004], lr: 0.058418, loss: 3.1184
2022-03-07 10:12:45 - train: epoch 0093, iter [02700, 05004], lr: 0.058418, loss: 2.6965
2022-03-07 10:13:18 - train: epoch 0093, iter [02800, 05004], lr: 0.058418, loss: 2.5379
2022-03-07 10:13:52 - train: epoch 0093, iter [02900, 05004], lr: 0.058418, loss: 2.6516
2022-03-07 10:14:26 - train: epoch 0093, iter [03000, 05004], lr: 0.058418, loss: 2.7113
2022-03-07 10:15:00 - train: epoch 0093, iter [03100, 05004], lr: 0.058418, loss: 2.9192
2022-03-07 10:15:35 - train: epoch 0093, iter [03200, 05004], lr: 0.058418, loss: 2.5847
2022-03-07 10:16:08 - train: epoch 0093, iter [03300, 05004], lr: 0.058418, loss: 2.9539
2022-03-07 10:16:43 - train: epoch 0093, iter [03400, 05004], lr: 0.058418, loss: 2.8831
2022-03-07 10:17:16 - train: epoch 0093, iter [03500, 05004], lr: 0.058418, loss: 2.4960
2022-03-07 10:17:51 - train: epoch 0093, iter [03600, 05004], lr: 0.058418, loss: 2.9038
2022-03-07 10:18:25 - train: epoch 0093, iter [03700, 05004], lr: 0.058418, loss: 2.7461
2022-03-07 10:18:58 - train: epoch 0093, iter [03800, 05004], lr: 0.058418, loss: 2.7778
2022-03-07 10:19:33 - train: epoch 0093, iter [03900, 05004], lr: 0.058418, loss: 2.7805
2022-03-07 10:20:06 - train: epoch 0093, iter [04000, 05004], lr: 0.058418, loss: 2.9800
2022-03-07 10:20:40 - train: epoch 0093, iter [04100, 05004], lr: 0.058418, loss: 2.9565
2022-03-07 10:21:15 - train: epoch 0093, iter [04200, 05004], lr: 0.058418, loss: 2.7708
2022-03-07 10:21:49 - train: epoch 0093, iter [04300, 05004], lr: 0.058418, loss: 2.9218
2022-03-07 10:22:23 - train: epoch 0093, iter [04400, 05004], lr: 0.058418, loss: 2.5217
2022-03-07 10:22:57 - train: epoch 0093, iter [04500, 05004], lr: 0.058418, loss: 2.5658
2022-03-07 10:23:31 - train: epoch 0093, iter [04600, 05004], lr: 0.058418, loss: 2.6102
2022-03-07 10:24:06 - train: epoch 0093, iter [04700, 05004], lr: 0.058418, loss: 3.2706
2022-03-07 10:24:39 - train: epoch 0093, iter [04800, 05004], lr: 0.058418, loss: 2.6330
2022-03-07 10:25:14 - train: epoch 0093, iter [04900, 05004], lr: 0.058418, loss: 2.9342
2022-03-07 10:25:46 - train: epoch 0093, iter [05000, 05004], lr: 0.058418, loss: 2.7042
2022-03-07 10:25:47 - train: epoch 093, train_loss: 2.7986
2022-03-07 10:27:01 - eval: epoch: 093, acc1: 57.500%, acc5: 81.772%, test_loss: 1.7993, per_image_load_time: 2.109ms, per_image_inference_time: 0.485ms
2022-03-07 10:27:01 - until epoch: 093, best_acc1: 58.346%
2022-03-07 10:27:01 - epoch 094 lr: 0.05762275557551727
2022-03-07 10:27:40 - train: epoch 0094, iter [00100, 05004], lr: 0.057623, loss: 3.0669
2022-03-07 10:28:14 - train: epoch 0094, iter [00200, 05004], lr: 0.057623, loss: 2.7567
2022-03-07 10:28:48 - train: epoch 0094, iter [00300, 05004], lr: 0.057623, loss: 2.8450
2022-03-07 10:29:23 - train: epoch 0094, iter [00400, 05004], lr: 0.057623, loss: 2.7863
2022-03-07 10:29:58 - train: epoch 0094, iter [00500, 05004], lr: 0.057623, loss: 2.8016
2022-03-07 10:30:30 - train: epoch 0094, iter [00600, 05004], lr: 0.057623, loss: 2.5310
2022-03-07 10:31:05 - train: epoch 0094, iter [00700, 05004], lr: 0.057623, loss: 3.0821
2022-03-07 10:31:39 - train: epoch 0094, iter [00800, 05004], lr: 0.057623, loss: 2.7458
2022-03-07 10:32:14 - train: epoch 0094, iter [00900, 05004], lr: 0.057623, loss: 2.5397
2022-03-07 10:32:48 - train: epoch 0094, iter [01000, 05004], lr: 0.057623, loss: 2.7945
2022-03-07 10:33:23 - train: epoch 0094, iter [01100, 05004], lr: 0.057623, loss: 2.9953
2022-03-07 10:33:57 - train: epoch 0094, iter [01200, 05004], lr: 0.057623, loss: 2.5983
2022-03-07 10:34:31 - train: epoch 0094, iter [01300, 05004], lr: 0.057623, loss: 2.5716
2022-03-07 10:35:05 - train: epoch 0094, iter [01400, 05004], lr: 0.057623, loss: 2.7368
2022-03-07 10:35:39 - train: epoch 0094, iter [01500, 05004], lr: 0.057623, loss: 2.8071
2022-03-07 10:36:14 - train: epoch 0094, iter [01600, 05004], lr: 0.057623, loss: 2.9394
2022-03-07 10:36:48 - train: epoch 0094, iter [01700, 05004], lr: 0.057623, loss: 2.7989
2022-03-07 10:37:22 - train: epoch 0094, iter [01800, 05004], lr: 0.057623, loss: 2.7341
2022-03-07 10:37:56 - train: epoch 0094, iter [01900, 05004], lr: 0.057623, loss: 2.5271
2022-03-07 10:38:30 - train: epoch 0094, iter [02000, 05004], lr: 0.057623, loss: 2.5191
2022-03-07 10:39:04 - train: epoch 0094, iter [02100, 05004], lr: 0.057623, loss: 2.8451
2022-03-07 10:39:39 - train: epoch 0094, iter [02200, 05004], lr: 0.057623, loss: 2.8651
2022-03-07 10:40:12 - train: epoch 0094, iter [02300, 05004], lr: 0.057623, loss: 2.8162
2022-03-07 10:40:48 - train: epoch 0094, iter [02400, 05004], lr: 0.057623, loss: 2.9036
2022-03-07 10:41:22 - train: epoch 0094, iter [02500, 05004], lr: 0.057623, loss: 2.6516
2022-03-07 10:41:56 - train: epoch 0094, iter [02600, 05004], lr: 0.057623, loss: 2.6175
2022-03-07 10:42:30 - train: epoch 0094, iter [02700, 05004], lr: 0.057623, loss: 2.7032
2022-03-07 10:43:04 - train: epoch 0094, iter [02800, 05004], lr: 0.057623, loss: 2.8267
2022-03-07 10:43:38 - train: epoch 0094, iter [02900, 05004], lr: 0.057623, loss: 2.7748
2022-03-07 10:44:12 - train: epoch 0094, iter [03000, 05004], lr: 0.057623, loss: 2.9047
2022-03-07 10:44:47 - train: epoch 0094, iter [03100, 05004], lr: 0.057623, loss: 3.0119
2022-03-07 10:45:22 - train: epoch 0094, iter [03200, 05004], lr: 0.057623, loss: 2.8005
2022-03-07 10:45:55 - train: epoch 0094, iter [03300, 05004], lr: 0.057623, loss: 2.7363
2022-03-07 10:46:30 - train: epoch 0094, iter [03400, 05004], lr: 0.057623, loss: 2.7725
2022-03-07 10:47:04 - train: epoch 0094, iter [03500, 05004], lr: 0.057623, loss: 3.1126
2022-03-07 10:47:39 - train: epoch 0094, iter [03600, 05004], lr: 0.057623, loss: 2.9234
2022-03-07 10:48:13 - train: epoch 0094, iter [03700, 05004], lr: 0.057623, loss: 2.9405
2022-03-07 10:48:48 - train: epoch 0094, iter [03800, 05004], lr: 0.057623, loss: 2.5839
2022-03-07 10:49:22 - train: epoch 0094, iter [03900, 05004], lr: 0.057623, loss: 2.7264
2022-03-07 10:49:57 - train: epoch 0094, iter [04000, 05004], lr: 0.057623, loss: 2.8008
2022-03-07 10:50:31 - train: epoch 0094, iter [04100, 05004], lr: 0.057623, loss: 3.1413
2022-03-07 10:51:05 - train: epoch 0094, iter [04200, 05004], lr: 0.057623, loss: 2.5809
2022-03-07 10:51:40 - train: epoch 0094, iter [04300, 05004], lr: 0.057623, loss: 2.9136
2022-03-07 10:52:13 - train: epoch 0094, iter [04400, 05004], lr: 0.057623, loss: 2.7591
2022-03-07 10:52:48 - train: epoch 0094, iter [04500, 05004], lr: 0.057623, loss: 2.8042
2022-03-07 10:53:22 - train: epoch 0094, iter [04600, 05004], lr: 0.057623, loss: 2.8538
2022-03-07 10:53:57 - train: epoch 0094, iter [04700, 05004], lr: 0.057623, loss: 2.5361
2022-03-07 10:54:30 - train: epoch 0094, iter [04800, 05004], lr: 0.057623, loss: 2.7009
2022-03-07 10:55:05 - train: epoch 0094, iter [04900, 05004], lr: 0.057623, loss: 2.6170
2022-03-07 10:55:37 - train: epoch 0094, iter [05000, 05004], lr: 0.057623, loss: 2.4898
2022-03-07 10:55:38 - train: epoch 094, train_loss: 2.7933
2022-03-07 10:56:52 - eval: epoch: 094, acc1: 57.682%, acc5: 81.800%, test_loss: 1.7841, per_image_load_time: 2.355ms, per_image_inference_time: 0.542ms
2022-03-07 10:56:53 - until epoch: 094, best_acc1: 58.346%
2022-03-07 10:56:53 - epoch 095 lr: 0.05682568059703659
2022-03-07 10:57:33 - train: epoch 0095, iter [00100, 05004], lr: 0.056826, loss: 2.7716
2022-03-07 10:58:06 - train: epoch 0095, iter [00200, 05004], lr: 0.056826, loss: 2.7784
2022-03-07 10:58:40 - train: epoch 0095, iter [00300, 05004], lr: 0.056826, loss: 2.6745
2022-03-07 10:59:14 - train: epoch 0095, iter [00400, 05004], lr: 0.056826, loss: 2.7324
2022-03-07 10:59:49 - train: epoch 0095, iter [00500, 05004], lr: 0.056826, loss: 3.0263
2022-03-07 11:00:23 - train: epoch 0095, iter [00600, 05004], lr: 0.056826, loss: 2.7788
2022-03-07 11:00:57 - train: epoch 0095, iter [00700, 05004], lr: 0.056826, loss: 3.0425
2022-03-07 11:01:32 - train: epoch 0095, iter [00800, 05004], lr: 0.056826, loss: 2.9309
2022-03-07 11:02:06 - train: epoch 0095, iter [00900, 05004], lr: 0.056826, loss: 2.9379
2022-03-07 11:02:39 - train: epoch 0095, iter [01000, 05004], lr: 0.056826, loss: 2.9247
2022-03-07 11:03:13 - train: epoch 0095, iter [01100, 05004], lr: 0.056826, loss: 2.7494
2022-03-07 11:03:48 - train: epoch 0095, iter [01200, 05004], lr: 0.056826, loss: 2.8026
2022-03-07 11:04:22 - train: epoch 0095, iter [01300, 05004], lr: 0.056826, loss: 2.8015
2022-03-07 11:04:56 - train: epoch 0095, iter [01400, 05004], lr: 0.056826, loss: 2.6944
2022-03-07 11:05:30 - train: epoch 0095, iter [01500, 05004], lr: 0.056826, loss: 2.7375
2022-03-07 11:06:05 - train: epoch 0095, iter [01600, 05004], lr: 0.056826, loss: 2.3237
2022-03-07 11:06:38 - train: epoch 0095, iter [01700, 05004], lr: 0.056826, loss: 2.9065
2022-03-07 11:07:13 - train: epoch 0095, iter [01800, 05004], lr: 0.056826, loss: 2.9279
2022-03-07 11:07:47 - train: epoch 0095, iter [01900, 05004], lr: 0.056826, loss: 2.7965
2022-03-07 11:08:22 - train: epoch 0095, iter [02000, 05004], lr: 0.056826, loss: 2.7690
2022-03-07 11:08:57 - train: epoch 0095, iter [02100, 05004], lr: 0.056826, loss: 2.8849
2022-03-07 11:09:31 - train: epoch 0095, iter [02200, 05004], lr: 0.056826, loss: 2.3346
2022-03-07 11:10:06 - train: epoch 0095, iter [02300, 05004], lr: 0.056826, loss: 2.6056
2022-03-07 11:10:39 - train: epoch 0095, iter [02400, 05004], lr: 0.056826, loss: 2.7716
2022-03-07 11:11:14 - train: epoch 0095, iter [02500, 05004], lr: 0.056826, loss: 2.6534
2022-03-07 11:11:47 - train: epoch 0095, iter [02600, 05004], lr: 0.056826, loss: 2.5262
2022-03-07 11:12:22 - train: epoch 0095, iter [02700, 05004], lr: 0.056826, loss: 2.6399
2022-03-07 11:12:56 - train: epoch 0095, iter [02800, 05004], lr: 0.056826, loss: 2.7604
2022-03-07 11:13:30 - train: epoch 0095, iter [02900, 05004], lr: 0.056826, loss: 2.9330
2022-03-07 11:14:05 - train: epoch 0095, iter [03000, 05004], lr: 0.056826, loss: 2.8370
2022-03-07 11:14:39 - train: epoch 0095, iter [03100, 05004], lr: 0.056826, loss: 2.8423
2022-03-07 11:15:13 - train: epoch 0095, iter [03200, 05004], lr: 0.056826, loss: 2.8165
2022-03-07 11:15:47 - train: epoch 0095, iter [03300, 05004], lr: 0.056826, loss: 2.6853
2022-03-07 11:16:23 - train: epoch 0095, iter [03400, 05004], lr: 0.056826, loss: 2.3869
2022-03-07 11:16:56 - train: epoch 0095, iter [03500, 05004], lr: 0.056826, loss: 3.0437
2022-03-07 11:17:30 - train: epoch 0095, iter [03600, 05004], lr: 0.056826, loss: 2.7541
2022-03-07 11:18:05 - train: epoch 0095, iter [03700, 05004], lr: 0.056826, loss: 2.6867
2022-03-07 11:18:38 - train: epoch 0095, iter [03800, 05004], lr: 0.056826, loss: 2.5167
2022-03-07 11:19:13 - train: epoch 0095, iter [03900, 05004], lr: 0.056826, loss: 2.7566
2022-03-07 11:19:47 - train: epoch 0095, iter [04000, 05004], lr: 0.056826, loss: 2.6210
2022-03-07 11:20:23 - train: epoch 0095, iter [04100, 05004], lr: 0.056826, loss: 2.7914
2022-03-07 11:20:57 - train: epoch 0095, iter [04200, 05004], lr: 0.056826, loss: 2.6989
2022-03-07 11:21:32 - train: epoch 0095, iter [04300, 05004], lr: 0.056826, loss: 2.9852
2022-03-07 11:22:06 - train: epoch 0095, iter [04400, 05004], lr: 0.056826, loss: 2.7702
2022-03-07 11:22:40 - train: epoch 0095, iter [04500, 05004], lr: 0.056826, loss: 2.8246
2022-03-07 11:23:15 - train: epoch 0095, iter [04600, 05004], lr: 0.056826, loss: 2.7355
2022-03-07 11:23:49 - train: epoch 0095, iter [04700, 05004], lr: 0.056826, loss: 2.6739
2022-03-07 11:24:23 - train: epoch 0095, iter [04800, 05004], lr: 0.056826, loss: 3.1202
2022-03-07 11:24:58 - train: epoch 0095, iter [04900, 05004], lr: 0.056826, loss: 2.7524
2022-03-07 11:25:31 - train: epoch 0095, iter [05000, 05004], lr: 0.056826, loss: 2.5356
2022-03-07 11:25:32 - train: epoch 095, train_loss: 2.7894
2022-03-07 11:26:46 - eval: epoch: 095, acc1: 55.970%, acc5: 80.666%, test_loss: 1.8732, per_image_load_time: 2.256ms, per_image_inference_time: 0.547ms
2022-03-07 11:26:47 - until epoch: 095, best_acc1: 58.346%
2022-03-07 11:26:47 - epoch 096 lr: 0.05602683401276615
2022-03-07 11:27:25 - train: epoch 0096, iter [00100, 05004], lr: 0.056027, loss: 2.8126
2022-03-07 11:27:59 - train: epoch 0096, iter [00200, 05004], lr: 0.056027, loss: 2.5720
2022-03-07 11:28:32 - train: epoch 0096, iter [00300, 05004], lr: 0.056027, loss: 2.5325
2022-03-07 11:29:07 - train: epoch 0096, iter [00400, 05004], lr: 0.056027, loss: 2.6206
2022-03-07 11:29:42 - train: epoch 0096, iter [00500, 05004], lr: 0.056027, loss: 2.8356
2022-03-07 11:30:16 - train: epoch 0096, iter [00600, 05004], lr: 0.056027, loss: 2.7925
2022-03-07 11:30:50 - train: epoch 0096, iter [00700, 05004], lr: 0.056027, loss: 2.7058
2022-03-07 11:31:24 - train: epoch 0096, iter [00800, 05004], lr: 0.056027, loss: 2.4400
2022-03-07 11:31:59 - train: epoch 0096, iter [00900, 05004], lr: 0.056027, loss: 2.7643
2022-03-07 11:32:33 - train: epoch 0096, iter [01000, 05004], lr: 0.056027, loss: 2.4411
2022-03-07 11:33:06 - train: epoch 0096, iter [01100, 05004], lr: 0.056027, loss: 2.9197
2022-03-07 11:33:41 - train: epoch 0096, iter [01200, 05004], lr: 0.056027, loss: 2.9807
2022-03-07 11:34:16 - train: epoch 0096, iter [01300, 05004], lr: 0.056027, loss: 2.5936
2022-03-07 11:34:50 - train: epoch 0096, iter [01400, 05004], lr: 0.056027, loss: 2.9007
2022-03-07 11:35:24 - train: epoch 0096, iter [01500, 05004], lr: 0.056027, loss: 2.8939
2022-03-07 11:35:58 - train: epoch 0096, iter [01600, 05004], lr: 0.056027, loss: 2.4505
2022-03-07 11:36:32 - train: epoch 0096, iter [01700, 05004], lr: 0.056027, loss: 2.8254
2022-03-07 11:37:06 - train: epoch 0096, iter [01800, 05004], lr: 0.056027, loss: 2.9750
2022-03-07 11:37:41 - train: epoch 0096, iter [01900, 05004], lr: 0.056027, loss: 2.9076
2022-03-07 11:38:16 - train: epoch 0096, iter [02000, 05004], lr: 0.056027, loss: 2.4879
2022-03-07 11:38:50 - train: epoch 0096, iter [02100, 05004], lr: 0.056027, loss: 2.6205
2022-03-07 11:39:24 - train: epoch 0096, iter [02200, 05004], lr: 0.056027, loss: 2.7518
2022-03-07 11:39:58 - train: epoch 0096, iter [02300, 05004], lr: 0.056027, loss: 2.7481
2022-03-07 11:40:32 - train: epoch 0096, iter [02400, 05004], lr: 0.056027, loss: 2.6299
2022-03-07 11:41:07 - train: epoch 0096, iter [02500, 05004], lr: 0.056027, loss: 2.7821
2022-03-07 11:41:41 - train: epoch 0096, iter [02600, 05004], lr: 0.056027, loss: 2.6042
2022-03-07 11:42:15 - train: epoch 0096, iter [02700, 05004], lr: 0.056027, loss: 2.8434
2022-03-07 11:42:49 - train: epoch 0096, iter [02800, 05004], lr: 0.056027, loss: 2.8238
2022-03-07 11:43:23 - train: epoch 0096, iter [02900, 05004], lr: 0.056027, loss: 2.7053
2022-03-07 11:43:58 - train: epoch 0096, iter [03000, 05004], lr: 0.056027, loss: 2.5550
2022-03-07 11:44:32 - train: epoch 0096, iter [03100, 05004], lr: 0.056027, loss: 2.9221
2022-03-07 11:45:06 - train: epoch 0096, iter [03200, 05004], lr: 0.056027, loss: 3.0281
2022-03-07 11:45:41 - train: epoch 0096, iter [03300, 05004], lr: 0.056027, loss: 2.7722
2022-03-07 11:46:15 - train: epoch 0096, iter [03400, 05004], lr: 0.056027, loss: 2.5009
2022-03-07 11:46:50 - train: epoch 0096, iter [03500, 05004], lr: 0.056027, loss: 2.4300
2022-03-07 11:47:23 - train: epoch 0096, iter [03600, 05004], lr: 0.056027, loss: 2.6394
2022-03-07 11:47:58 - train: epoch 0096, iter [03700, 05004], lr: 0.056027, loss: 2.7397
2022-03-07 11:48:32 - train: epoch 0096, iter [03800, 05004], lr: 0.056027, loss: 2.7413
2022-03-07 11:49:07 - train: epoch 0096, iter [03900, 05004], lr: 0.056027, loss: 2.5321
2022-03-07 11:49:41 - train: epoch 0096, iter [04000, 05004], lr: 0.056027, loss: 2.5435
2022-03-07 11:50:15 - train: epoch 0096, iter [04100, 05004], lr: 0.056027, loss: 2.6001
2022-03-07 11:50:49 - train: epoch 0096, iter [04200, 05004], lr: 0.056027, loss: 2.6959
2022-03-07 11:51:23 - train: epoch 0096, iter [04300, 05004], lr: 0.056027, loss: 2.6778
2022-03-07 11:51:57 - train: epoch 0096, iter [04400, 05004], lr: 0.056027, loss: 2.6045
2022-03-07 11:52:32 - train: epoch 0096, iter [04500, 05004], lr: 0.056027, loss: 2.7764
2022-03-07 11:53:05 - train: epoch 0096, iter [04600, 05004], lr: 0.056027, loss: 2.6465
2022-03-07 11:53:39 - train: epoch 0096, iter [04700, 05004], lr: 0.056027, loss: 3.0460
2022-03-07 11:54:13 - train: epoch 0096, iter [04800, 05004], lr: 0.056027, loss: 3.0358
2022-03-07 11:54:48 - train: epoch 0096, iter [04900, 05004], lr: 0.056027, loss: 3.1811
2022-03-07 11:55:20 - train: epoch 0096, iter [05000, 05004], lr: 0.056027, loss: 2.5501
2022-03-07 11:55:21 - train: epoch 096, train_loss: 2.7745
2022-03-07 11:56:35 - eval: epoch: 096, acc1: 56.470%, acc5: 81.088%, test_loss: 1.8465, per_image_load_time: 1.778ms, per_image_inference_time: 0.541ms
2022-03-07 11:56:36 - until epoch: 096, best_acc1: 58.346%
2022-03-07 11:56:36 - epoch 097 lr: 0.05522642316338268
2022-03-07 11:57:15 - train: epoch 0097, iter [00100, 05004], lr: 0.055226, loss: 2.8860
2022-03-07 11:57:49 - train: epoch 0097, iter [00200, 05004], lr: 0.055226, loss: 2.6353
2022-03-07 11:58:23 - train: epoch 0097, iter [00300, 05004], lr: 0.055226, loss: 2.8456
2022-03-07 11:58:57 - train: epoch 0097, iter [00400, 05004], lr: 0.055226, loss: 3.2495
2022-03-07 11:59:31 - train: epoch 0097, iter [00500, 05004], lr: 0.055226, loss: 2.7166
2022-03-07 12:00:05 - train: epoch 0097, iter [00600, 05004], lr: 0.055226, loss: 2.8285
2022-03-07 12:00:40 - train: epoch 0097, iter [00700, 05004], lr: 0.055226, loss: 2.6949
2022-03-07 12:01:13 - train: epoch 0097, iter [00800, 05004], lr: 0.055226, loss: 2.9238
2022-03-07 12:01:48 - train: epoch 0097, iter [00900, 05004], lr: 0.055226, loss: 2.9174
2022-03-07 12:02:22 - train: epoch 0097, iter [01000, 05004], lr: 0.055226, loss: 2.7693
2022-03-07 12:02:57 - train: epoch 0097, iter [01100, 05004], lr: 0.055226, loss: 2.5348
2022-03-07 12:03:31 - train: epoch 0097, iter [01200, 05004], lr: 0.055226, loss: 2.8880
2022-03-07 12:04:05 - train: epoch 0097, iter [01300, 05004], lr: 0.055226, loss: 2.5531
2022-03-07 12:04:39 - train: epoch 0097, iter [01400, 05004], lr: 0.055226, loss: 2.7982
2022-03-07 12:05:14 - train: epoch 0097, iter [01500, 05004], lr: 0.055226, loss: 2.7117
2022-03-07 12:05:48 - train: epoch 0097, iter [01600, 05004], lr: 0.055226, loss: 2.6783
2022-03-07 12:06:23 - train: epoch 0097, iter [01700, 05004], lr: 0.055226, loss: 2.6241
2022-03-07 12:06:56 - train: epoch 0097, iter [01800, 05004], lr: 0.055226, loss: 2.5692
2022-03-07 12:07:30 - train: epoch 0097, iter [01900, 05004], lr: 0.055226, loss: 2.6408
2022-03-07 12:08:04 - train: epoch 0097, iter [02000, 05004], lr: 0.055226, loss: 2.7019
2022-03-07 12:08:39 - train: epoch 0097, iter [02100, 05004], lr: 0.055226, loss: 2.7155
2022-03-07 12:09:14 - train: epoch 0097, iter [02200, 05004], lr: 0.055226, loss: 2.7411
2022-03-07 12:09:48 - train: epoch 0097, iter [02300, 05004], lr: 0.055226, loss: 2.7179
2022-03-07 12:10:21 - train: epoch 0097, iter [02400, 05004], lr: 0.055226, loss: 2.8314
2022-03-07 12:10:56 - train: epoch 0097, iter [02500, 05004], lr: 0.055226, loss: 2.8784
2022-03-07 12:11:29 - train: epoch 0097, iter [02600, 05004], lr: 0.055226, loss: 2.8250
2022-03-07 12:12:03 - train: epoch 0097, iter [02700, 05004], lr: 0.055226, loss: 2.6684
2022-03-07 12:12:38 - train: epoch 0097, iter [02800, 05004], lr: 0.055226, loss: 2.6401
2022-03-07 12:13:11 - train: epoch 0097, iter [02900, 05004], lr: 0.055226, loss: 2.8666
2022-03-07 12:13:45 - train: epoch 0097, iter [03000, 05004], lr: 0.055226, loss: 2.7580
2022-03-07 12:14:20 - train: epoch 0097, iter [03100, 05004], lr: 0.055226, loss: 3.0703
2022-03-07 12:14:55 - train: epoch 0097, iter [03200, 05004], lr: 0.055226, loss: 2.6979
2022-03-07 12:15:29 - train: epoch 0097, iter [03300, 05004], lr: 0.055226, loss: 2.8711
2022-03-07 12:16:03 - train: epoch 0097, iter [03400, 05004], lr: 0.055226, loss: 3.1967
2022-03-07 12:16:37 - train: epoch 0097, iter [03500, 05004], lr: 0.055226, loss: 2.7486
2022-03-07 12:17:12 - train: epoch 0097, iter [03600, 05004], lr: 0.055226, loss: 2.7125
2022-03-07 12:17:45 - train: epoch 0097, iter [03700, 05004], lr: 0.055226, loss: 2.6186
2022-03-07 12:18:19 - train: epoch 0097, iter [03800, 05004], lr: 0.055226, loss: 2.4799
2022-03-07 12:18:53 - train: epoch 0097, iter [03900, 05004], lr: 0.055226, loss: 2.5853
2022-03-07 12:19:27 - train: epoch 0097, iter [04000, 05004], lr: 0.055226, loss: 3.0324
2022-03-07 12:20:01 - train: epoch 0097, iter [04100, 05004], lr: 0.055226, loss: 2.8493
2022-03-07 12:20:35 - train: epoch 0097, iter [04200, 05004], lr: 0.055226, loss: 2.7351
2022-03-07 12:21:09 - train: epoch 0097, iter [04300, 05004], lr: 0.055226, loss: 2.8251
2022-03-07 12:21:44 - train: epoch 0097, iter [04400, 05004], lr: 0.055226, loss: 2.9358
2022-03-07 12:22:17 - train: epoch 0097, iter [04500, 05004], lr: 0.055226, loss: 2.6181
2022-03-07 12:22:51 - train: epoch 0097, iter [04600, 05004], lr: 0.055226, loss: 2.9496
2022-03-07 12:23:26 - train: epoch 0097, iter [04700, 05004], lr: 0.055226, loss: 2.9522
2022-03-07 12:24:01 - train: epoch 0097, iter [04800, 05004], lr: 0.055226, loss: 2.4489
2022-03-07 12:24:35 - train: epoch 0097, iter [04900, 05004], lr: 0.055226, loss: 2.9312
2022-03-07 12:25:08 - train: epoch 0097, iter [05000, 05004], lr: 0.055226, loss: 2.9528
2022-03-07 12:25:09 - train: epoch 097, train_loss: 2.7712
2022-03-07 12:26:23 - eval: epoch: 097, acc1: 58.600%, acc5: 82.686%, test_loss: 1.7382, per_image_load_time: 1.619ms, per_image_inference_time: 0.546ms
2022-03-07 12:26:24 - until epoch: 097, best_acc1: 58.600%
2022-03-07 12:26:24 - epoch 098 lr: 0.054424655795567926
2022-03-07 12:27:02 - train: epoch 0098, iter [00100, 05004], lr: 0.054425, loss: 2.8412
2022-03-07 12:27:36 - train: epoch 0098, iter [00200, 05004], lr: 0.054425, loss: 2.9941
2022-03-07 12:28:11 - train: epoch 0098, iter [00300, 05004], lr: 0.054425, loss: 2.7214
2022-03-07 12:28:44 - train: epoch 0098, iter [00400, 05004], lr: 0.054425, loss: 2.8275
2022-03-07 12:29:18 - train: epoch 0098, iter [00500, 05004], lr: 0.054425, loss: 2.7424
2022-03-07 12:29:51 - train: epoch 0098, iter [00600, 05004], lr: 0.054425, loss: 3.1236
2022-03-07 12:30:25 - train: epoch 0098, iter [00700, 05004], lr: 0.054425, loss: 2.5424
2022-03-07 12:30:59 - train: epoch 0098, iter [00800, 05004], lr: 0.054425, loss: 2.8618
2022-03-07 12:31:34 - train: epoch 0098, iter [00900, 05004], lr: 0.054425, loss: 2.7644
2022-03-07 12:32:07 - train: epoch 0098, iter [01000, 05004], lr: 0.054425, loss: 2.5034
2022-03-07 12:32:42 - train: epoch 0098, iter [01100, 05004], lr: 0.054425, loss: 2.3491
2022-03-07 12:33:16 - train: epoch 0098, iter [01200, 05004], lr: 0.054425, loss: 2.6010
2022-03-07 12:33:50 - train: epoch 0098, iter [01300, 05004], lr: 0.054425, loss: 3.1045
2022-03-07 12:34:24 - train: epoch 0098, iter [01400, 05004], lr: 0.054425, loss: 2.9008
2022-03-07 12:34:58 - train: epoch 0098, iter [01500, 05004], lr: 0.054425, loss: 2.5672
2022-03-07 12:35:32 - train: epoch 0098, iter [01600, 05004], lr: 0.054425, loss: 2.5716
2022-03-07 12:36:07 - train: epoch 0098, iter [01700, 05004], lr: 0.054425, loss: 3.0378
2022-03-07 12:36:41 - train: epoch 0098, iter [01800, 05004], lr: 0.054425, loss: 2.7235
2022-03-07 12:37:15 - train: epoch 0098, iter [01900, 05004], lr: 0.054425, loss: 2.5053
2022-03-07 12:37:49 - train: epoch 0098, iter [02000, 05004], lr: 0.054425, loss: 2.7790
2022-03-07 12:38:24 - train: epoch 0098, iter [02100, 05004], lr: 0.054425, loss: 2.7292
2022-03-07 12:38:58 - train: epoch 0098, iter [02200, 05004], lr: 0.054425, loss: 2.9568
2022-03-07 12:39:32 - train: epoch 0098, iter [02300, 05004], lr: 0.054425, loss: 2.6770
2022-03-07 12:40:05 - train: epoch 0098, iter [02400, 05004], lr: 0.054425, loss: 2.6338
2022-03-07 12:40:40 - train: epoch 0098, iter [02500, 05004], lr: 0.054425, loss: 2.8249
2022-03-07 12:41:14 - train: epoch 0098, iter [02600, 05004], lr: 0.054425, loss: 2.7581
2022-03-07 12:41:48 - train: epoch 0098, iter [02700, 05004], lr: 0.054425, loss: 2.9539
2022-03-07 12:42:23 - train: epoch 0098, iter [02800, 05004], lr: 0.054425, loss: 2.7869
2022-03-07 12:42:56 - train: epoch 0098, iter [02900, 05004], lr: 0.054425, loss: 2.8024
2022-03-07 12:43:31 - train: epoch 0098, iter [03000, 05004], lr: 0.054425, loss: 2.8599
2022-03-07 12:44:05 - train: epoch 0098, iter [03100, 05004], lr: 0.054425, loss: 2.7271
2022-03-07 12:44:40 - train: epoch 0098, iter [03200, 05004], lr: 0.054425, loss: 2.5821
2022-03-07 12:45:14 - train: epoch 0098, iter [03300, 05004], lr: 0.054425, loss: 2.5727
2022-03-07 12:45:48 - train: epoch 0098, iter [03400, 05004], lr: 0.054425, loss: 3.0759
2022-03-07 12:46:22 - train: epoch 0098, iter [03500, 05004], lr: 0.054425, loss: 2.5953
2022-03-07 12:46:56 - train: epoch 0098, iter [03600, 05004], lr: 0.054425, loss: 2.8474
2022-03-07 12:47:30 - train: epoch 0098, iter [03700, 05004], lr: 0.054425, loss: 2.8602
2022-03-07 12:48:04 - train: epoch 0098, iter [03800, 05004], lr: 0.054425, loss: 2.6762
2022-03-07 12:48:38 - train: epoch 0098, iter [03900, 05004], lr: 0.054425, loss: 2.7395
2022-03-07 12:49:12 - train: epoch 0098, iter [04000, 05004], lr: 0.054425, loss: 2.8596
2022-03-07 12:49:47 - train: epoch 0098, iter [04100, 05004], lr: 0.054425, loss: 2.8760
2022-03-07 12:50:21 - train: epoch 0098, iter [04200, 05004], lr: 0.054425, loss: 2.6913
2022-03-07 12:50:55 - train: epoch 0098, iter [04300, 05004], lr: 0.054425, loss: 2.7932
2022-03-07 12:51:28 - train: epoch 0098, iter [04400, 05004], lr: 0.054425, loss: 3.0622
2022-03-07 12:52:02 - train: epoch 0098, iter [04500, 05004], lr: 0.054425, loss: 2.9744
2022-03-07 12:52:37 - train: epoch 0098, iter [04600, 05004], lr: 0.054425, loss: 2.8665
2022-03-07 12:53:11 - train: epoch 0098, iter [04700, 05004], lr: 0.054425, loss: 2.4407
2022-03-07 12:53:46 - train: epoch 0098, iter [04800, 05004], lr: 0.054425, loss: 2.4601
2022-03-07 12:54:20 - train: epoch 0098, iter [04900, 05004], lr: 0.054425, loss: 2.9577
2022-03-07 12:54:53 - train: epoch 0098, iter [05000, 05004], lr: 0.054425, loss: 2.7736
2022-03-07 12:54:54 - train: epoch 098, train_loss: 2.7682
2022-03-07 12:56:09 - eval: epoch: 098, acc1: 60.340%, acc5: 83.726%, test_loss: 1.6643, per_image_load_time: 2.268ms, per_image_inference_time: 0.528ms
2022-03-07 12:56:10 - until epoch: 098, best_acc1: 60.340%
2022-03-07 12:56:10 - epoch 099 lr: 0.05362174000808813
2022-03-07 12:56:48 - train: epoch 0099, iter [00100, 05004], lr: 0.053622, loss: 2.5793
2022-03-07 12:57:22 - train: epoch 0099, iter [00200, 05004], lr: 0.053622, loss: 2.6776
2022-03-07 12:57:56 - train: epoch 0099, iter [00300, 05004], lr: 0.053622, loss: 2.2739
2022-03-07 12:58:30 - train: epoch 0099, iter [00400, 05004], lr: 0.053622, loss: 2.8833
2022-03-07 12:59:04 - train: epoch 0099, iter [00500, 05004], lr: 0.053622, loss: 2.8871
2022-03-07 12:59:39 - train: epoch 0099, iter [00600, 05004], lr: 0.053622, loss: 2.8878
2022-03-07 13:00:13 - train: epoch 0099, iter [00700, 05004], lr: 0.053622, loss: 2.9485
2022-03-07 13:00:47 - train: epoch 0099, iter [00800, 05004], lr: 0.053622, loss: 2.4679
2022-03-07 13:01:22 - train: epoch 0099, iter [00900, 05004], lr: 0.053622, loss: 2.5742
2022-03-07 13:01:55 - train: epoch 0099, iter [01000, 05004], lr: 0.053622, loss: 2.7606
2022-03-07 13:02:29 - train: epoch 0099, iter [01100, 05004], lr: 0.053622, loss: 2.9545
2022-03-07 13:03:03 - train: epoch 0099, iter [01200, 05004], lr: 0.053622, loss: 2.7907
2022-03-07 13:03:38 - train: epoch 0099, iter [01300, 05004], lr: 0.053622, loss: 3.0079
2022-03-07 13:04:12 - train: epoch 0099, iter [01400, 05004], lr: 0.053622, loss: 2.8447
2022-03-07 13:04:46 - train: epoch 0099, iter [01500, 05004], lr: 0.053622, loss: 2.5847
2022-03-07 13:05:21 - train: epoch 0099, iter [01600, 05004], lr: 0.053622, loss: 2.7573
2022-03-07 13:05:54 - train: epoch 0099, iter [01700, 05004], lr: 0.053622, loss: 2.8656
2022-03-07 13:06:28 - train: epoch 0099, iter [01800, 05004], lr: 0.053622, loss: 2.8728
2022-03-07 13:07:02 - train: epoch 0099, iter [01900, 05004], lr: 0.053622, loss: 2.7440
2022-03-07 13:07:37 - train: epoch 0099, iter [02000, 05004], lr: 0.053622, loss: 2.6868
2022-03-07 13:08:11 - train: epoch 0099, iter [02100, 05004], lr: 0.053622, loss: 2.5475
2022-03-07 13:08:46 - train: epoch 0099, iter [02200, 05004], lr: 0.053622, loss: 2.7676
2022-03-07 13:09:21 - train: epoch 0099, iter [02300, 05004], lr: 0.053622, loss: 2.6110
2022-03-07 13:09:55 - train: epoch 0099, iter [02400, 05004], lr: 0.053622, loss: 3.0040
2022-03-07 13:10:29 - train: epoch 0099, iter [02500, 05004], lr: 0.053622, loss: 2.8117
2022-03-07 13:11:04 - train: epoch 0099, iter [02600, 05004], lr: 0.053622, loss: 2.6640
2022-03-07 13:11:38 - train: epoch 0099, iter [02700, 05004], lr: 0.053622, loss: 2.9082
2022-03-07 13:12:12 - train: epoch 0099, iter [02800, 05004], lr: 0.053622, loss: 2.8152
2022-03-07 13:12:47 - train: epoch 0099, iter [02900, 05004], lr: 0.053622, loss: 2.6596
2022-03-07 13:13:21 - train: epoch 0099, iter [03000, 05004], lr: 0.053622, loss: 3.3542
2022-03-07 13:13:55 - train: epoch 0099, iter [03100, 05004], lr: 0.053622, loss: 2.5802
2022-03-07 13:14:29 - train: epoch 0099, iter [03200, 05004], lr: 0.053622, loss: 2.8710
2022-03-07 13:15:03 - train: epoch 0099, iter [03300, 05004], lr: 0.053622, loss: 2.6966
2022-03-07 13:15:37 - train: epoch 0099, iter [03400, 05004], lr: 0.053622, loss: 2.8978
2022-03-07 13:16:11 - train: epoch 0099, iter [03500, 05004], lr: 0.053622, loss: 2.9654
2022-03-07 13:16:44 - train: epoch 0099, iter [03600, 05004], lr: 0.053622, loss: 2.5930
2022-03-07 13:17:18 - train: epoch 0099, iter [03700, 05004], lr: 0.053622, loss: 2.8632
2022-03-07 13:17:52 - train: epoch 0099, iter [03800, 05004], lr: 0.053622, loss: 3.0165
2022-03-07 13:18:26 - train: epoch 0099, iter [03900, 05004], lr: 0.053622, loss: 2.7339
