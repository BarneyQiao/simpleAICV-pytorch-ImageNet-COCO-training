2022-03-07 13:19:01 - train: epoch 0099, iter [04000, 05004], lr: 0.053622, loss: 2.9333
2022-03-07 13:19:34 - train: epoch 0099, iter [04100, 05004], lr: 0.053622, loss: 2.5214
2022-03-07 13:20:09 - train: epoch 0099, iter [04200, 05004], lr: 0.053622, loss: 3.2032
2022-03-07 13:20:43 - train: epoch 0099, iter [04300, 05004], lr: 0.053622, loss: 2.7404
2022-03-07 13:21:17 - train: epoch 0099, iter [04400, 05004], lr: 0.053622, loss: 3.0516
2022-03-07 13:21:51 - train: epoch 0099, iter [04500, 05004], lr: 0.053622, loss: 2.8887
2022-03-07 13:22:25 - train: epoch 0099, iter [04600, 05004], lr: 0.053622, loss: 3.1566
2022-03-07 13:22:59 - train: epoch 0099, iter [04700, 05004], lr: 0.053622, loss: 2.7738
2022-03-07 13:23:33 - train: epoch 0099, iter [04800, 05004], lr: 0.053622, loss: 2.8595
2022-03-07 13:24:07 - train: epoch 0099, iter [04900, 05004], lr: 0.053622, loss: 2.6424
2022-03-07 13:24:40 - train: epoch 0099, iter [05000, 05004], lr: 0.053622, loss: 2.5998
2022-03-07 13:24:41 - train: epoch 099, train_loss: 2.7591
2022-03-07 13:25:55 - eval: epoch: 099, acc1: 58.040%, acc5: 82.242%, test_loss: 1.7586, per_image_load_time: 1.165ms, per_image_inference_time: 0.517ms
2022-03-07 13:25:56 - until epoch: 099, best_acc1: 60.340%
2022-03-07 13:25:56 - epoch 100 lr: 0.05281788419778188
2022-03-07 13:26:35 - train: epoch 0100, iter [00100, 05004], lr: 0.052818, loss: 2.6147
2022-03-07 13:27:08 - train: epoch 0100, iter [00200, 05004], lr: 0.052818, loss: 2.5497
2022-03-07 13:27:43 - train: epoch 0100, iter [00300, 05004], lr: 0.052818, loss: 2.5074
2022-03-07 13:28:17 - train: epoch 0100, iter [00400, 05004], lr: 0.052818, loss: 2.6228
2022-03-07 13:28:50 - train: epoch 0100, iter [00500, 05004], lr: 0.052818, loss: 2.8861
2022-03-07 13:29:25 - train: epoch 0100, iter [00600, 05004], lr: 0.052818, loss: 2.8377
2022-03-07 13:29:59 - train: epoch 0100, iter [00700, 05004], lr: 0.052818, loss: 2.6832
2022-03-07 13:30:33 - train: epoch 0100, iter [00800, 05004], lr: 0.052818, loss: 2.6800
2022-03-07 13:31:08 - train: epoch 0100, iter [00900, 05004], lr: 0.052818, loss: 2.3178
2022-03-07 13:31:42 - train: epoch 0100, iter [01000, 05004], lr: 0.052818, loss: 2.6997
2022-03-07 13:32:17 - train: epoch 0100, iter [01100, 05004], lr: 0.052818, loss: 2.6753
2022-03-07 13:32:51 - train: epoch 0100, iter [01200, 05004], lr: 0.052818, loss: 2.8089
2022-03-07 13:33:26 - train: epoch 0100, iter [01300, 05004], lr: 0.052818, loss: 2.9749
2022-03-07 13:34:00 - train: epoch 0100, iter [01400, 05004], lr: 0.052818, loss: 2.7711
2022-03-07 13:34:34 - train: epoch 0100, iter [01500, 05004], lr: 0.052818, loss: 2.7214
2022-03-07 13:35:09 - train: epoch 0100, iter [01600, 05004], lr: 0.052818, loss: 2.7229
2022-03-07 13:35:44 - train: epoch 0100, iter [01700, 05004], lr: 0.052818, loss: 2.4602
2022-03-07 13:36:18 - train: epoch 0100, iter [01800, 05004], lr: 0.052818, loss: 2.8698
2022-03-07 13:36:52 - train: epoch 0100, iter [01900, 05004], lr: 0.052818, loss: 2.5357
2022-03-07 13:37:27 - train: epoch 0100, iter [02000, 05004], lr: 0.052818, loss: 2.8466
2022-03-07 13:38:02 - train: epoch 0100, iter [02100, 05004], lr: 0.052818, loss: 2.7500
2022-03-07 13:38:36 - train: epoch 0100, iter [02200, 05004], lr: 0.052818, loss: 2.6671
2022-03-07 13:39:10 - train: epoch 0100, iter [02300, 05004], lr: 0.052818, loss: 2.7284
2022-03-07 13:39:44 - train: epoch 0100, iter [02400, 05004], lr: 0.052818, loss: 2.8742
2022-03-07 13:40:19 - train: epoch 0100, iter [02500, 05004], lr: 0.052818, loss: 2.9043
2022-03-07 13:40:53 - train: epoch 0100, iter [02600, 05004], lr: 0.052818, loss: 2.7050
2022-03-07 13:41:28 - train: epoch 0100, iter [02700, 05004], lr: 0.052818, loss: 2.9252
2022-03-07 13:42:02 - train: epoch 0100, iter [02800, 05004], lr: 0.052818, loss: 2.7559
2022-03-07 13:42:36 - train: epoch 0100, iter [02900, 05004], lr: 0.052818, loss: 2.8933
2022-03-07 13:43:11 - train: epoch 0100, iter [03000, 05004], lr: 0.052818, loss: 2.6531
2022-03-07 13:43:45 - train: epoch 0100, iter [03100, 05004], lr: 0.052818, loss: 2.7709
2022-03-07 13:44:19 - train: epoch 0100, iter [03200, 05004], lr: 0.052818, loss: 3.1428
2022-03-07 13:44:53 - train: epoch 0100, iter [03300, 05004], lr: 0.052818, loss: 3.1111
2022-03-07 13:45:28 - train: epoch 0100, iter [03400, 05004], lr: 0.052818, loss: 2.8477
2022-03-07 13:46:03 - train: epoch 0100, iter [03500, 05004], lr: 0.052818, loss: 2.5887
2022-03-07 13:46:36 - train: epoch 0100, iter [03600, 05004], lr: 0.052818, loss: 2.8604
2022-03-07 13:47:11 - train: epoch 0100, iter [03700, 05004], lr: 0.052818, loss: 2.5422
2022-03-07 13:47:45 - train: epoch 0100, iter [03800, 05004], lr: 0.052818, loss: 2.6414
2022-03-07 13:48:19 - train: epoch 0100, iter [03900, 05004], lr: 0.052818, loss: 2.7835
2022-03-07 13:48:53 - train: epoch 0100, iter [04000, 05004], lr: 0.052818, loss: 2.8338
2022-03-07 13:49:28 - train: epoch 0100, iter [04100, 05004], lr: 0.052818, loss: 2.9798
2022-03-07 13:50:03 - train: epoch 0100, iter [04200, 05004], lr: 0.052818, loss: 2.8796
2022-03-07 13:50:39 - train: epoch 0100, iter [04300, 05004], lr: 0.052818, loss: 2.9196
2022-03-07 13:51:14 - train: epoch 0100, iter [04400, 05004], lr: 0.052818, loss: 3.0338
2022-03-07 13:51:48 - train: epoch 0100, iter [04500, 05004], lr: 0.052818, loss: 2.6685
2022-03-07 13:52:23 - train: epoch 0100, iter [04600, 05004], lr: 0.052818, loss: 2.5450
2022-03-07 13:52:57 - train: epoch 0100, iter [04700, 05004], lr: 0.052818, loss: 3.0402
2022-03-07 13:53:32 - train: epoch 0100, iter [04800, 05004], lr: 0.052818, loss: 2.6115
2022-03-07 13:54:05 - train: epoch 0100, iter [04900, 05004], lr: 0.052818, loss: 2.6352
2022-03-07 13:54:39 - train: epoch 0100, iter [05000, 05004], lr: 0.052818, loss: 2.8553
2022-03-07 13:54:40 - train: epoch 100, train_loss: 2.7556
2022-03-07 13:55:55 - eval: epoch: 100, acc1: 56.492%, acc5: 81.012%, test_loss: 1.8391, per_image_load_time: 2.335ms, per_image_inference_time: 0.555ms
2022-03-07 13:55:55 - until epoch: 100, best_acc1: 60.340%
2022-03-07 13:55:55 - epoch 101 lr: 0.05201329700547076
2022-03-07 13:56:34 - train: epoch 0101, iter [00100, 05004], lr: 0.052013, loss: 2.7563
2022-03-07 13:57:08 - train: epoch 0101, iter [00200, 05004], lr: 0.052013, loss: 2.7114
2022-03-07 13:57:42 - train: epoch 0101, iter [00300, 05004], lr: 0.052013, loss: 2.4808
2022-03-07 13:58:16 - train: epoch 0101, iter [00400, 05004], lr: 0.052013, loss: 2.5503
2022-03-07 13:58:49 - train: epoch 0101, iter [00500, 05004], lr: 0.052013, loss: 2.7782
2022-03-07 13:59:23 - train: epoch 0101, iter [00600, 05004], lr: 0.052013, loss: 2.7902
2022-03-07 13:59:57 - train: epoch 0101, iter [00700, 05004], lr: 0.052013, loss: 3.0321
2022-03-07 14:00:31 - train: epoch 0101, iter [00800, 05004], lr: 0.052013, loss: 2.7600
2022-03-07 14:01:06 - train: epoch 0101, iter [00900, 05004], lr: 0.052013, loss: 2.8307
2022-03-07 14:01:40 - train: epoch 0101, iter [01000, 05004], lr: 0.052013, loss: 2.8775
2022-03-07 14:02:14 - train: epoch 0101, iter [01100, 05004], lr: 0.052013, loss: 2.6293
2022-03-07 14:02:48 - train: epoch 0101, iter [01200, 05004], lr: 0.052013, loss: 2.6038
2022-03-07 14:03:22 - train: epoch 0101, iter [01300, 05004], lr: 0.052013, loss: 2.9426
2022-03-07 14:03:57 - train: epoch 0101, iter [01400, 05004], lr: 0.052013, loss: 2.6892
2022-03-07 14:04:32 - train: epoch 0101, iter [01500, 05004], lr: 0.052013, loss: 2.7419
2022-03-07 14:05:06 - train: epoch 0101, iter [01600, 05004], lr: 0.052013, loss: 2.8208
2022-03-07 14:05:39 - train: epoch 0101, iter [01700, 05004], lr: 0.052013, loss: 2.6818
2022-03-07 14:06:13 - train: epoch 0101, iter [01800, 05004], lr: 0.052013, loss: 3.0106
2022-03-07 14:06:47 - train: epoch 0101, iter [01900, 05004], lr: 0.052013, loss: 2.3309
2022-03-07 14:07:21 - train: epoch 0101, iter [02000, 05004], lr: 0.052013, loss: 2.7276
2022-03-07 14:07:54 - train: epoch 0101, iter [02100, 05004], lr: 0.052013, loss: 2.8293
2022-03-07 14:08:29 - train: epoch 0101, iter [02200, 05004], lr: 0.052013, loss: 2.8707
2022-03-07 14:09:03 - train: epoch 0101, iter [02300, 05004], lr: 0.052013, loss: 2.5843
2022-03-07 14:09:36 - train: epoch 0101, iter [02400, 05004], lr: 0.052013, loss: 2.7859
2022-03-07 14:10:10 - train: epoch 0101, iter [02500, 05004], lr: 0.052013, loss: 2.8165
2022-03-07 14:10:44 - train: epoch 0101, iter [02600, 05004], lr: 0.052013, loss: 2.7219
2022-03-07 14:11:18 - train: epoch 0101, iter [02700, 05004], lr: 0.052013, loss: 2.9640
2022-03-07 14:11:51 - train: epoch 0101, iter [02800, 05004], lr: 0.052013, loss: 2.7402
2022-03-07 14:12:26 - train: epoch 0101, iter [02900, 05004], lr: 0.052013, loss: 2.6438
2022-03-07 14:12:59 - train: epoch 0101, iter [03000, 05004], lr: 0.052013, loss: 2.6850
2022-03-07 14:13:33 - train: epoch 0101, iter [03100, 05004], lr: 0.052013, loss: 2.7519
2022-03-07 14:14:08 - train: epoch 0101, iter [03200, 05004], lr: 0.052013, loss: 2.8682
2022-03-07 14:14:42 - train: epoch 0101, iter [03300, 05004], lr: 0.052013, loss: 2.8053
2022-03-07 14:15:16 - train: epoch 0101, iter [03400, 05004], lr: 0.052013, loss: 2.9141
2022-03-07 14:15:50 - train: epoch 0101, iter [03500, 05004], lr: 0.052013, loss: 2.5369
2022-03-07 14:16:24 - train: epoch 0101, iter [03600, 05004], lr: 0.052013, loss: 2.6658
2022-03-07 14:16:59 - train: epoch 0101, iter [03700, 05004], lr: 0.052013, loss: 2.8465
2022-03-07 14:17:33 - train: epoch 0101, iter [03800, 05004], lr: 0.052013, loss: 2.7439
2022-03-07 14:18:07 - train: epoch 0101, iter [03900, 05004], lr: 0.052013, loss: 2.8217
2022-03-07 14:18:41 - train: epoch 0101, iter [04000, 05004], lr: 0.052013, loss: 2.9575
2022-03-07 14:19:17 - train: epoch 0101, iter [04100, 05004], lr: 0.052013, loss: 2.6894
2022-03-07 14:19:50 - train: epoch 0101, iter [04200, 05004], lr: 0.052013, loss: 2.9220
2022-03-07 14:20:25 - train: epoch 0101, iter [04300, 05004], lr: 0.052013, loss: 2.5855
2022-03-07 14:20:58 - train: epoch 0101, iter [04400, 05004], lr: 0.052013, loss: 2.4232
2022-03-07 14:21:32 - train: epoch 0101, iter [04500, 05004], lr: 0.052013, loss: 2.8346
2022-03-07 14:22:06 - train: epoch 0101, iter [04600, 05004], lr: 0.052013, loss: 2.7551
2022-03-07 14:22:40 - train: epoch 0101, iter [04700, 05004], lr: 0.052013, loss: 2.6487
2022-03-07 14:23:14 - train: epoch 0101, iter [04800, 05004], lr: 0.052013, loss: 2.5948
2022-03-07 14:23:47 - train: epoch 0101, iter [04900, 05004], lr: 0.052013, loss: 2.6229
2022-03-07 14:24:19 - train: epoch 0101, iter [05000, 05004], lr: 0.052013, loss: 2.6141
2022-03-07 14:24:20 - train: epoch 101, train_loss: 2.7471
2022-03-07 14:25:33 - eval: epoch: 101, acc1: 59.912%, acc5: 83.490%, test_loss: 1.6720, per_image_load_time: 1.061ms, per_image_inference_time: 0.530ms
2022-03-07 14:25:34 - until epoch: 101, best_acc1: 60.340%
2022-03-07 14:25:34 - epoch 102 lr: 0.05120818726180662
2022-03-07 14:26:12 - train: epoch 0102, iter [00100, 05004], lr: 0.051208, loss: 2.8068
2022-03-07 14:26:46 - train: epoch 0102, iter [00200, 05004], lr: 0.051208, loss: 2.9469
2022-03-07 14:27:19 - train: epoch 0102, iter [00300, 05004], lr: 0.051208, loss: 2.6752
2022-03-07 14:27:53 - train: epoch 0102, iter [00400, 05004], lr: 0.051208, loss: 2.9333
2022-03-07 14:28:27 - train: epoch 0102, iter [00500, 05004], lr: 0.051208, loss: 2.5528
2022-03-07 14:29:00 - train: epoch 0102, iter [00600, 05004], lr: 0.051208, loss: 2.6253
2022-03-07 14:29:34 - train: epoch 0102, iter [00700, 05004], lr: 0.051208, loss: 2.7809
2022-03-07 14:30:09 - train: epoch 0102, iter [00800, 05004], lr: 0.051208, loss: 2.8222
2022-03-07 14:30:42 - train: epoch 0102, iter [00900, 05004], lr: 0.051208, loss: 2.4908
2022-03-07 14:31:16 - train: epoch 0102, iter [01000, 05004], lr: 0.051208, loss: 2.9336
2022-03-07 14:31:50 - train: epoch 0102, iter [01100, 05004], lr: 0.051208, loss: 2.5513
2022-03-07 14:32:23 - train: epoch 0102, iter [01200, 05004], lr: 0.051208, loss: 2.8083
2022-03-07 14:32:56 - train: epoch 0102, iter [01300, 05004], lr: 0.051208, loss: 2.7660
2022-03-07 14:33:30 - train: epoch 0102, iter [01400, 05004], lr: 0.051208, loss: 2.6269
2022-03-07 14:34:04 - train: epoch 0102, iter [01500, 05004], lr: 0.051208, loss: 2.5920
2022-03-07 14:34:37 - train: epoch 0102, iter [01600, 05004], lr: 0.051208, loss: 2.8132
2022-03-07 14:35:11 - train: epoch 0102, iter [01700, 05004], lr: 0.051208, loss: 2.7518
2022-03-07 14:35:45 - train: epoch 0102, iter [01800, 05004], lr: 0.051208, loss: 2.8436
2022-03-07 14:36:18 - train: epoch 0102, iter [01900, 05004], lr: 0.051208, loss: 2.6892
2022-03-07 14:36:52 - train: epoch 0102, iter [02000, 05004], lr: 0.051208, loss: 2.7947
2022-03-07 14:37:26 - train: epoch 0102, iter [02100, 05004], lr: 0.051208, loss: 2.7019
2022-03-07 14:37:59 - train: epoch 0102, iter [02200, 05004], lr: 0.051208, loss: 2.9679
2022-03-07 14:38:33 - train: epoch 0102, iter [02300, 05004], lr: 0.051208, loss: 2.7624
2022-03-07 14:39:07 - train: epoch 0102, iter [02400, 05004], lr: 0.051208, loss: 3.0174
2022-03-07 14:39:40 - train: epoch 0102, iter [02500, 05004], lr: 0.051208, loss: 2.8163
2022-03-07 14:40:15 - train: epoch 0102, iter [02600, 05004], lr: 0.051208, loss: 2.6505
2022-03-07 14:40:49 - train: epoch 0102, iter [02700, 05004], lr: 0.051208, loss: 2.9277
2022-03-07 14:41:23 - train: epoch 0102, iter [02800, 05004], lr: 0.051208, loss: 2.7446
2022-03-07 14:41:56 - train: epoch 0102, iter [02900, 05004], lr: 0.051208, loss: 2.5915
2022-03-07 14:42:30 - train: epoch 0102, iter [03000, 05004], lr: 0.051208, loss: 2.8114
2022-03-07 14:43:04 - train: epoch 0102, iter [03100, 05004], lr: 0.051208, loss: 2.4754
2022-03-07 14:43:37 - train: epoch 0102, iter [03200, 05004], lr: 0.051208, loss: 2.3195
2022-03-07 14:44:12 - train: epoch 0102, iter [03300, 05004], lr: 0.051208, loss: 2.5562
2022-03-07 14:44:46 - train: epoch 0102, iter [03400, 05004], lr: 0.051208, loss: 2.9881
2022-03-07 14:45:20 - train: epoch 0102, iter [03500, 05004], lr: 0.051208, loss: 2.9309
2022-03-07 14:45:53 - train: epoch 0102, iter [03600, 05004], lr: 0.051208, loss: 2.9443
2022-03-07 14:46:28 - train: epoch 0102, iter [03700, 05004], lr: 0.051208, loss: 2.7235
2022-03-07 14:47:02 - train: epoch 0102, iter [03800, 05004], lr: 0.051208, loss: 2.7634
2022-03-07 14:47:36 - train: epoch 0102, iter [03900, 05004], lr: 0.051208, loss: 2.7752
2022-03-07 14:48:11 - train: epoch 0102, iter [04000, 05004], lr: 0.051208, loss: 2.5648
2022-03-07 14:48:45 - train: epoch 0102, iter [04100, 05004], lr: 0.051208, loss: 2.6471
2022-03-07 14:49:18 - train: epoch 0102, iter [04200, 05004], lr: 0.051208, loss: 2.8908
2022-03-07 14:49:53 - train: epoch 0102, iter [04300, 05004], lr: 0.051208, loss: 2.7445
2022-03-07 14:50:27 - train: epoch 0102, iter [04400, 05004], lr: 0.051208, loss: 2.5721
2022-03-07 14:51:01 - train: epoch 0102, iter [04500, 05004], lr: 0.051208, loss: 2.9249
2022-03-07 14:51:35 - train: epoch 0102, iter [04600, 05004], lr: 0.051208, loss: 2.8937
2022-03-07 14:52:09 - train: epoch 0102, iter [04700, 05004], lr: 0.051208, loss: 2.6183
2022-03-07 14:52:44 - train: epoch 0102, iter [04800, 05004], lr: 0.051208, loss: 2.7469
2022-03-07 14:53:18 - train: epoch 0102, iter [04900, 05004], lr: 0.051208, loss: 2.8459
2022-03-07 14:53:51 - train: epoch 0102, iter [05000, 05004], lr: 0.051208, loss: 2.6169
2022-03-07 14:53:52 - train: epoch 102, train_loss: 2.7358
2022-03-07 14:55:07 - eval: epoch: 102, acc1: 59.614%, acc5: 83.432%, test_loss: 1.6901, per_image_load_time: 1.367ms, per_image_inference_time: 0.514ms
2022-03-07 14:55:07 - until epoch: 102, best_acc1: 60.340%
2022-03-07 14:55:07 - epoch 103 lr: 0.0504027639330695
2022-03-07 14:55:46 - train: epoch 0103, iter [00100, 05004], lr: 0.050403, loss: 2.4698
2022-03-07 14:56:21 - train: epoch 0103, iter [00200, 05004], lr: 0.050403, loss: 2.8250
2022-03-07 14:56:55 - train: epoch 0103, iter [00300, 05004], lr: 0.050403, loss: 2.6741
2022-03-07 14:57:29 - train: epoch 0103, iter [00400, 05004], lr: 0.050403, loss: 3.0510
2022-03-07 14:58:03 - train: epoch 0103, iter [00500, 05004], lr: 0.050403, loss: 2.7598
2022-03-07 14:58:37 - train: epoch 0103, iter [00600, 05004], lr: 0.050403, loss: 2.5968
2022-03-07 14:59:11 - train: epoch 0103, iter [00700, 05004], lr: 0.050403, loss: 2.9716
2022-03-07 14:59:45 - train: epoch 0103, iter [00800, 05004], lr: 0.050403, loss: 2.8795
2022-03-07 15:00:21 - train: epoch 0103, iter [00900, 05004], lr: 0.050403, loss: 2.5967
2022-03-07 15:00:54 - train: epoch 0103, iter [01000, 05004], lr: 0.050403, loss: 2.4687
2022-03-07 15:01:29 - train: epoch 0103, iter [01100, 05004], lr: 0.050403, loss: 3.0646
2022-03-07 15:02:03 - train: epoch 0103, iter [01200, 05004], lr: 0.050403, loss: 2.9299
2022-03-07 15:02:36 - train: epoch 0103, iter [01300, 05004], lr: 0.050403, loss: 2.9892
2022-03-07 15:03:10 - train: epoch 0103, iter [01400, 05004], lr: 0.050403, loss: 2.8986
2022-03-07 15:03:44 - train: epoch 0103, iter [01500, 05004], lr: 0.050403, loss: 2.7651
2022-03-07 15:04:18 - train: epoch 0103, iter [01600, 05004], lr: 0.050403, loss: 2.5221
2022-03-07 15:04:53 - train: epoch 0103, iter [01700, 05004], lr: 0.050403, loss: 2.7750
2022-03-07 15:05:27 - train: epoch 0103, iter [01800, 05004], lr: 0.050403, loss: 2.5977
2022-03-07 15:06:01 - train: epoch 0103, iter [01900, 05004], lr: 0.050403, loss: 2.7281
2022-03-07 15:06:35 - train: epoch 0103, iter [02000, 05004], lr: 0.050403, loss: 2.2752
2022-03-07 15:07:09 - train: epoch 0103, iter [02100, 05004], lr: 0.050403, loss: 2.8879
2022-03-07 15:07:43 - train: epoch 0103, iter [02200, 05004], lr: 0.050403, loss: 2.7693
2022-03-07 15:08:17 - train: epoch 0103, iter [02300, 05004], lr: 0.050403, loss: 2.7335
2022-03-07 15:08:51 - train: epoch 0103, iter [02400, 05004], lr: 0.050403, loss: 2.7510
2022-03-07 15:09:26 - train: epoch 0103, iter [02500, 05004], lr: 0.050403, loss: 2.8167
2022-03-07 15:10:00 - train: epoch 0103, iter [02600, 05004], lr: 0.050403, loss: 2.9098
2022-03-07 15:10:35 - train: epoch 0103, iter [02700, 05004], lr: 0.050403, loss: 2.7731
2022-03-07 15:11:09 - train: epoch 0103, iter [02800, 05004], lr: 0.050403, loss: 2.2599
2022-03-07 15:11:43 - train: epoch 0103, iter [02900, 05004], lr: 0.050403, loss: 2.6239
2022-03-07 15:12:17 - train: epoch 0103, iter [03000, 05004], lr: 0.050403, loss: 3.0118
2022-03-07 15:12:51 - train: epoch 0103, iter [03100, 05004], lr: 0.050403, loss: 2.7377
2022-03-07 15:13:25 - train: epoch 0103, iter [03200, 05004], lr: 0.050403, loss: 2.5844
2022-03-07 15:14:00 - train: epoch 0103, iter [03300, 05004], lr: 0.050403, loss: 2.6095
2022-03-07 15:14:34 - train: epoch 0103, iter [03400, 05004], lr: 0.050403, loss: 2.8155
2022-03-07 15:15:09 - train: epoch 0103, iter [03500, 05004], lr: 0.050403, loss: 2.4956
2022-03-07 15:15:42 - train: epoch 0103, iter [03600, 05004], lr: 0.050403, loss: 3.0009
2022-03-07 15:16:16 - train: epoch 0103, iter [03700, 05004], lr: 0.050403, loss: 2.8100
2022-03-07 15:16:50 - train: epoch 0103, iter [03800, 05004], lr: 0.050403, loss: 2.6725
2022-03-07 15:17:25 - train: epoch 0103, iter [03900, 05004], lr: 0.050403, loss: 2.7906
2022-03-07 15:17:58 - train: epoch 0103, iter [04000, 05004], lr: 0.050403, loss: 3.0190
2022-03-07 15:18:32 - train: epoch 0103, iter [04100, 05004], lr: 0.050403, loss: 2.6137
2022-03-07 15:19:05 - train: epoch 0103, iter [04200, 05004], lr: 0.050403, loss: 2.9663
2022-03-07 15:19:39 - train: epoch 0103, iter [04300, 05004], lr: 0.050403, loss: 2.6974
2022-03-07 15:20:14 - train: epoch 0103, iter [04400, 05004], lr: 0.050403, loss: 2.6453
2022-03-07 15:20:48 - train: epoch 0103, iter [04500, 05004], lr: 0.050403, loss: 2.6748
2022-03-07 15:21:22 - train: epoch 0103, iter [04600, 05004], lr: 0.050403, loss: 2.8124
2022-03-07 15:21:57 - train: epoch 0103, iter [04700, 05004], lr: 0.050403, loss: 2.4442
2022-03-07 15:22:31 - train: epoch 0103, iter [04800, 05004], lr: 0.050403, loss: 3.0640
2022-03-07 15:23:05 - train: epoch 0103, iter [04900, 05004], lr: 0.050403, loss: 2.7224
2022-03-07 15:23:38 - train: epoch 0103, iter [05000, 05004], lr: 0.050403, loss: 2.6536
2022-03-07 15:23:39 - train: epoch 103, train_loss: 2.7366
2022-03-07 15:24:54 - eval: epoch: 103, acc1: 58.784%, acc5: 82.870%, test_loss: 1.7190, per_image_load_time: 1.088ms, per_image_inference_time: 0.509ms
2022-03-07 15:24:54 - until epoch: 103, best_acc1: 60.340%
2022-03-07 15:24:54 - epoch 104 lr: 0.04959723606693051
2022-03-07 15:25:33 - train: epoch 0104, iter [00100, 05004], lr: 0.049597, loss: 2.5508
2022-03-07 15:26:08 - train: epoch 0104, iter [00200, 05004], lr: 0.049597, loss: 2.7569
2022-03-07 15:26:42 - train: epoch 0104, iter [00300, 05004], lr: 0.049597, loss: 2.9690
2022-03-07 15:27:16 - train: epoch 0104, iter [00400, 05004], lr: 0.049597, loss: 2.7366
2022-03-07 15:27:50 - train: epoch 0104, iter [00500, 05004], lr: 0.049597, loss: 2.8170
2022-03-07 15:28:24 - train: epoch 0104, iter [00600, 05004], lr: 0.049597, loss: 2.8688
2022-03-07 15:28:59 - train: epoch 0104, iter [00700, 05004], lr: 0.049597, loss: 2.9028
2022-03-07 15:29:33 - train: epoch 0104, iter [00800, 05004], lr: 0.049597, loss: 2.5426
2022-03-07 15:30:07 - train: epoch 0104, iter [00900, 05004], lr: 0.049597, loss: 2.5109
2022-03-07 15:30:42 - train: epoch 0104, iter [01000, 05004], lr: 0.049597, loss: 2.5836
2022-03-07 15:31:17 - train: epoch 0104, iter [01100, 05004], lr: 0.049597, loss: 2.6825
2022-03-07 15:31:51 - train: epoch 0104, iter [01200, 05004], lr: 0.049597, loss: 2.6812
2022-03-07 15:32:25 - train: epoch 0104, iter [01300, 05004], lr: 0.049597, loss: 2.5847
2022-03-07 15:32:59 - train: epoch 0104, iter [01400, 05004], lr: 0.049597, loss: 2.6480
2022-03-07 15:33:34 - train: epoch 0104, iter [01500, 05004], lr: 0.049597, loss: 2.7827
2022-03-07 15:34:09 - train: epoch 0104, iter [01600, 05004], lr: 0.049597, loss: 2.6127
2022-03-07 15:34:43 - train: epoch 0104, iter [01700, 05004], lr: 0.049597, loss: 2.5133
2022-03-07 15:35:17 - train: epoch 0104, iter [01800, 05004], lr: 0.049597, loss: 2.7085
2022-03-07 15:35:52 - train: epoch 0104, iter [01900, 05004], lr: 0.049597, loss: 2.5802
2022-03-07 15:36:25 - train: epoch 0104, iter [02000, 05004], lr: 0.049597, loss: 2.8813
2022-03-07 15:37:00 - train: epoch 0104, iter [02100, 05004], lr: 0.049597, loss: 2.4841
2022-03-07 15:37:34 - train: epoch 0104, iter [02200, 05004], lr: 0.049597, loss: 2.6843
2022-03-07 15:38:09 - train: epoch 0104, iter [02300, 05004], lr: 0.049597, loss: 2.8047
2022-03-07 15:38:43 - train: epoch 0104, iter [02400, 05004], lr: 0.049597, loss: 2.8503
2022-03-07 15:39:17 - train: epoch 0104, iter [02500, 05004], lr: 0.049597, loss: 2.7928
2022-03-07 15:39:51 - train: epoch 0104, iter [02600, 05004], lr: 0.049597, loss: 2.5935
2022-03-07 15:40:26 - train: epoch 0104, iter [02700, 05004], lr: 0.049597, loss: 2.5542
2022-03-07 15:41:00 - train: epoch 0104, iter [02800, 05004], lr: 0.049597, loss: 2.7189
2022-03-07 15:41:34 - train: epoch 0104, iter [02900, 05004], lr: 0.049597, loss: 2.7795
2022-03-07 15:42:10 - train: epoch 0104, iter [03000, 05004], lr: 0.049597, loss: 2.9618
2022-03-07 15:42:43 - train: epoch 0104, iter [03100, 05004], lr: 0.049597, loss: 2.5293
2022-03-07 15:43:17 - train: epoch 0104, iter [03200, 05004], lr: 0.049597, loss: 2.9077
2022-03-07 15:43:52 - train: epoch 0104, iter [03300, 05004], lr: 0.049597, loss: 2.8652
2022-03-07 15:44:26 - train: epoch 0104, iter [03400, 05004], lr: 0.049597, loss: 2.8386
2022-03-07 15:45:00 - train: epoch 0104, iter [03500, 05004], lr: 0.049597, loss: 2.7983
2022-03-07 15:45:34 - train: epoch 0104, iter [03600, 05004], lr: 0.049597, loss: 2.7931
2022-03-07 15:46:08 - train: epoch 0104, iter [03700, 05004], lr: 0.049597, loss: 2.9397
2022-03-07 15:46:43 - train: epoch 0104, iter [03800, 05004], lr: 0.049597, loss: 2.3496
2022-03-07 15:47:17 - train: epoch 0104, iter [03900, 05004], lr: 0.049597, loss: 2.6072
2022-03-07 15:47:51 - train: epoch 0104, iter [04000, 05004], lr: 0.049597, loss: 2.5600
2022-03-07 15:48:26 - train: epoch 0104, iter [04100, 05004], lr: 0.049597, loss: 2.6572
2022-03-07 15:48:59 - train: epoch 0104, iter [04200, 05004], lr: 0.049597, loss: 2.6998
2022-03-07 15:49:34 - train: epoch 0104, iter [04300, 05004], lr: 0.049597, loss: 2.5156
2022-03-07 15:50:08 - train: epoch 0104, iter [04400, 05004], lr: 0.049597, loss: 2.5985
2022-03-07 15:50:43 - train: epoch 0104, iter [04500, 05004], lr: 0.049597, loss: 2.6562
2022-03-07 15:51:17 - train: epoch 0104, iter [04600, 05004], lr: 0.049597, loss: 2.7220
2022-03-07 15:51:51 - train: epoch 0104, iter [04700, 05004], lr: 0.049597, loss: 2.5210
2022-03-07 15:52:25 - train: epoch 0104, iter [04800, 05004], lr: 0.049597, loss: 2.4476
2022-03-07 15:52:59 - train: epoch 0104, iter [04900, 05004], lr: 0.049597, loss: 2.6178
2022-03-07 15:53:32 - train: epoch 0104, iter [05000, 05004], lr: 0.049597, loss: 2.7323
2022-03-07 15:53:33 - train: epoch 104, train_loss: 2.7269
2022-03-07 15:54:47 - eval: epoch: 104, acc1: 59.756%, acc5: 83.518%, test_loss: 1.6774, per_image_load_time: 2.329ms, per_image_inference_time: 0.511ms
2022-03-07 15:54:48 - until epoch: 104, best_acc1: 60.340%
2022-03-07 15:54:48 - epoch 105 lr: 0.0487918127381934
2022-03-07 15:55:27 - train: epoch 0105, iter [00100, 05004], lr: 0.048792, loss: 3.0286
2022-03-07 15:56:00 - train: epoch 0105, iter [00200, 05004], lr: 0.048792, loss: 2.8452
2022-03-07 15:56:35 - train: epoch 0105, iter [00300, 05004], lr: 0.048792, loss: 3.1247
2022-03-07 15:57:08 - train: epoch 0105, iter [00400, 05004], lr: 0.048792, loss: 2.8942
2022-03-07 15:57:43 - train: epoch 0105, iter [00500, 05004], lr: 0.048792, loss: 2.8275
2022-03-07 15:58:18 - train: epoch 0105, iter [00600, 05004], lr: 0.048792, loss: 2.9187
2022-03-07 15:58:51 - train: epoch 0105, iter [00700, 05004], lr: 0.048792, loss: 2.5969
2022-03-07 15:59:26 - train: epoch 0105, iter [00800, 05004], lr: 0.048792, loss: 2.5709
2022-03-07 15:59:59 - train: epoch 0105, iter [00900, 05004], lr: 0.048792, loss: 2.6161
2022-03-07 16:00:33 - train: epoch 0105, iter [01000, 05004], lr: 0.048792, loss: 2.7300
2022-03-07 16:01:07 - train: epoch 0105, iter [01100, 05004], lr: 0.048792, loss: 3.0048
2022-03-07 16:01:41 - train: epoch 0105, iter [01200, 05004], lr: 0.048792, loss: 2.7996
2022-03-07 16:02:16 - train: epoch 0105, iter [01300, 05004], lr: 0.048792, loss: 2.4973
2022-03-07 16:02:49 - train: epoch 0105, iter [01400, 05004], lr: 0.048792, loss: 2.7247
2022-03-07 16:03:24 - train: epoch 0105, iter [01500, 05004], lr: 0.048792, loss: 2.7475
2022-03-07 16:03:58 - train: epoch 0105, iter [01600, 05004], lr: 0.048792, loss: 2.3917
2022-03-07 16:04:33 - train: epoch 0105, iter [01700, 05004], lr: 0.048792, loss: 2.7955
2022-03-07 16:05:06 - train: epoch 0105, iter [01800, 05004], lr: 0.048792, loss: 2.8770
2022-03-07 16:05:41 - train: epoch 0105, iter [01900, 05004], lr: 0.048792, loss: 3.0407
2022-03-07 16:06:15 - train: epoch 0105, iter [02000, 05004], lr: 0.048792, loss: 2.6271
2022-03-07 16:06:49 - train: epoch 0105, iter [02100, 05004], lr: 0.048792, loss: 2.6759
2022-03-07 16:07:23 - train: epoch 0105, iter [02200, 05004], lr: 0.048792, loss: 3.1057
2022-03-07 16:07:57 - train: epoch 0105, iter [02300, 05004], lr: 0.048792, loss: 2.6490
2022-03-07 16:08:32 - train: epoch 0105, iter [02400, 05004], lr: 0.048792, loss: 2.5282
2022-03-07 16:09:06 - train: epoch 0105, iter [02500, 05004], lr: 0.048792, loss: 2.2358
2022-03-07 16:09:41 - train: epoch 0105, iter [02600, 05004], lr: 0.048792, loss: 2.5982
2022-03-07 16:10:15 - train: epoch 0105, iter [02700, 05004], lr: 0.048792, loss: 2.8859
2022-03-07 16:10:49 - train: epoch 0105, iter [02800, 05004], lr: 0.048792, loss: 2.9489
2022-03-07 16:11:22 - train: epoch 0105, iter [02900, 05004], lr: 0.048792, loss: 2.9599
2022-03-07 16:11:57 - train: epoch 0105, iter [03000, 05004], lr: 0.048792, loss: 2.6725
2022-03-07 16:12:31 - train: epoch 0105, iter [03100, 05004], lr: 0.048792, loss: 2.6834
2022-03-07 16:13:05 - train: epoch 0105, iter [03200, 05004], lr: 0.048792, loss: 3.0082
2022-03-07 16:13:39 - train: epoch 0105, iter [03300, 05004], lr: 0.048792, loss: 2.6382
2022-03-07 16:14:13 - train: epoch 0105, iter [03400, 05004], lr: 0.048792, loss: 2.7534
2022-03-07 16:14:47 - train: epoch 0105, iter [03500, 05004], lr: 0.048792, loss: 2.9119
2022-03-07 16:15:22 - train: epoch 0105, iter [03600, 05004], lr: 0.048792, loss: 2.7497
2022-03-07 16:15:56 - train: epoch 0105, iter [03700, 05004], lr: 0.048792, loss: 2.3562
2022-03-07 16:16:30 - train: epoch 0105, iter [03800, 05004], lr: 0.048792, loss: 2.9402
2022-03-07 16:17:04 - train: epoch 0105, iter [03900, 05004], lr: 0.048792, loss: 2.8299
2022-03-07 16:17:39 - train: epoch 0105, iter [04000, 05004], lr: 0.048792, loss: 2.7933
2022-03-07 16:18:12 - train: epoch 0105, iter [04100, 05004], lr: 0.048792, loss: 2.5611
2022-03-07 16:18:46 - train: epoch 0105, iter [04200, 05004], lr: 0.048792, loss: 2.7612
2022-03-07 16:19:20 - train: epoch 0105, iter [04300, 05004], lr: 0.048792, loss: 2.9082
2022-03-07 16:19:55 - train: epoch 0105, iter [04400, 05004], lr: 0.048792, loss: 2.4348
2022-03-07 16:20:29 - train: epoch 0105, iter [04500, 05004], lr: 0.048792, loss: 3.1537
2022-03-07 16:21:03 - train: epoch 0105, iter [04600, 05004], lr: 0.048792, loss: 2.8726
2022-03-07 16:21:37 - train: epoch 0105, iter [04700, 05004], lr: 0.048792, loss: 2.9430
2022-03-07 16:22:11 - train: epoch 0105, iter [04800, 05004], lr: 0.048792, loss: 2.8109
2022-03-07 16:22:45 - train: epoch 0105, iter [04900, 05004], lr: 0.048792, loss: 2.7649
2022-03-07 16:23:18 - train: epoch 0105, iter [05000, 05004], lr: 0.048792, loss: 2.5839
2022-03-07 16:23:19 - train: epoch 105, train_loss: 2.7177
2022-03-07 16:24:33 - eval: epoch: 105, acc1: 61.116%, acc5: 84.330%, test_loss: 1.6149, per_image_load_time: 1.072ms, per_image_inference_time: 0.543ms
2022-03-07 16:24:34 - until epoch: 105, best_acc1: 61.116%
2022-03-07 16:24:34 - epoch 106 lr: 0.04798670299452926
2022-03-07 16:25:13 - train: epoch 0106, iter [00100, 05004], lr: 0.047987, loss: 2.5328
2022-03-07 16:25:47 - train: epoch 0106, iter [00200, 05004], lr: 0.047987, loss: 2.6557
2022-03-07 16:26:21 - train: epoch 0106, iter [00300, 05004], lr: 0.047987, loss: 2.2533
2022-03-07 16:26:55 - train: epoch 0106, iter [00400, 05004], lr: 0.047987, loss: 2.6932
2022-03-07 16:27:30 - train: epoch 0106, iter [00500, 05004], lr: 0.047987, loss: 2.9683
2022-03-07 16:28:04 - train: epoch 0106, iter [00600, 05004], lr: 0.047987, loss: 2.6374
2022-03-07 16:28:38 - train: epoch 0106, iter [00700, 05004], lr: 0.047987, loss: 2.8906
2022-03-07 16:29:13 - train: epoch 0106, iter [00800, 05004], lr: 0.047987, loss: 2.4353
2022-03-07 16:29:46 - train: epoch 0106, iter [00900, 05004], lr: 0.047987, loss: 2.9591
2022-03-07 16:30:20 - train: epoch 0106, iter [01000, 05004], lr: 0.047987, loss: 3.0958
2022-03-07 16:30:55 - train: epoch 0106, iter [01100, 05004], lr: 0.047987, loss: 2.4862
2022-03-07 16:31:30 - train: epoch 0106, iter [01200, 05004], lr: 0.047987, loss: 2.4257
2022-03-07 16:32:04 - train: epoch 0106, iter [01300, 05004], lr: 0.047987, loss: 2.3911
2022-03-07 16:32:38 - train: epoch 0106, iter [01400, 05004], lr: 0.047987, loss: 2.8958
2022-03-07 16:33:13 - train: epoch 0106, iter [01500, 05004], lr: 0.047987, loss: 2.9450
2022-03-07 16:33:47 - train: epoch 0106, iter [01600, 05004], lr: 0.047987, loss: 2.5303
2022-03-07 16:34:21 - train: epoch 0106, iter [01700, 05004], lr: 0.047987, loss: 2.7359
2022-03-07 16:34:55 - train: epoch 0106, iter [01800, 05004], lr: 0.047987, loss: 2.5681
2022-03-07 16:35:29 - train: epoch 0106, iter [01900, 05004], lr: 0.047987, loss: 2.6110
2022-03-07 16:36:04 - train: epoch 0106, iter [02000, 05004], lr: 0.047987, loss: 2.6482
2022-03-07 16:36:39 - train: epoch 0106, iter [02100, 05004], lr: 0.047987, loss: 2.4947
2022-03-07 16:37:13 - train: epoch 0106, iter [02200, 05004], lr: 0.047987, loss: 2.6135
2022-03-07 16:37:47 - train: epoch 0106, iter [02300, 05004], lr: 0.047987, loss: 2.6415
2022-03-07 16:38:22 - train: epoch 0106, iter [02400, 05004], lr: 0.047987, loss: 2.8161
2022-03-07 16:38:56 - train: epoch 0106, iter [02500, 05004], lr: 0.047987, loss: 2.9238
2022-03-07 16:39:32 - train: epoch 0106, iter [02600, 05004], lr: 0.047987, loss: 2.6748
2022-03-07 16:40:05 - train: epoch 0106, iter [02700, 05004], lr: 0.047987, loss: 2.9160
2022-03-07 16:40:39 - train: epoch 0106, iter [02800, 05004], lr: 0.047987, loss: 2.7339
2022-03-07 16:41:14 - train: epoch 0106, iter [02900, 05004], lr: 0.047987, loss: 2.4939
2022-03-07 16:41:49 - train: epoch 0106, iter [03000, 05004], lr: 0.047987, loss: 2.9463
2022-03-07 16:42:22 - train: epoch 0106, iter [03100, 05004], lr: 0.047987, loss: 2.5241
2022-03-07 16:42:57 - train: epoch 0106, iter [03200, 05004], lr: 0.047987, loss: 2.7558
2022-03-07 16:43:32 - train: epoch 0106, iter [03300, 05004], lr: 0.047987, loss: 2.6929
2022-03-07 16:44:05 - train: epoch 0106, iter [03400, 05004], lr: 0.047987, loss: 2.5408
2022-03-07 16:44:39 - train: epoch 0106, iter [03500, 05004], lr: 0.047987, loss: 2.5425
2022-03-07 16:45:14 - train: epoch 0106, iter [03600, 05004], lr: 0.047987, loss: 2.5902
2022-03-07 16:45:49 - train: epoch 0106, iter [03700, 05004], lr: 0.047987, loss: 2.6994
2022-03-07 16:46:23 - train: epoch 0106, iter [03800, 05004], lr: 0.047987, loss: 2.9067
2022-03-07 16:46:57 - train: epoch 0106, iter [03900, 05004], lr: 0.047987, loss: 2.9434
2022-03-07 16:47:30 - train: epoch 0106, iter [04000, 05004], lr: 0.047987, loss: 2.7123
2022-03-07 16:48:05 - train: epoch 0106, iter [04100, 05004], lr: 0.047987, loss: 2.8240
2022-03-07 16:48:38 - train: epoch 0106, iter [04200, 05004], lr: 0.047987, loss: 2.5633
2022-03-07 16:49:13 - train: epoch 0106, iter [04300, 05004], lr: 0.047987, loss: 2.3833
2022-03-07 16:49:48 - train: epoch 0106, iter [04400, 05004], lr: 0.047987, loss: 2.7252
2022-03-07 16:50:22 - train: epoch 0106, iter [04500, 05004], lr: 0.047987, loss: 2.9883
2022-03-07 16:50:56 - train: epoch 0106, iter [04600, 05004], lr: 0.047987, loss: 2.8933
2022-03-07 16:51:31 - train: epoch 0106, iter [04700, 05004], lr: 0.047987, loss: 2.8857
2022-03-07 16:52:05 - train: epoch 0106, iter [04800, 05004], lr: 0.047987, loss: 2.8534
2022-03-07 16:52:40 - train: epoch 0106, iter [04900, 05004], lr: 0.047987, loss: 2.6297
2022-03-07 16:53:12 - train: epoch 0106, iter [05000, 05004], lr: 0.047987, loss: 2.8888
2022-03-07 16:53:13 - train: epoch 106, train_loss: 2.7135
2022-03-07 16:54:28 - eval: epoch: 106, acc1: 59.924%, acc5: 83.624%, test_loss: 1.6714, per_image_load_time: 2.397ms, per_image_inference_time: 0.508ms
2022-03-07 16:54:29 - until epoch: 106, best_acc1: 61.116%
2022-03-07 16:54:29 - epoch 107 lr: 0.04718211580221812
2022-03-07 16:55:08 - train: epoch 0107, iter [00100, 05004], lr: 0.047182, loss: 2.6339
2022-03-07 16:55:43 - train: epoch 0107, iter [00200, 05004], lr: 0.047182, loss: 2.8337
2022-03-07 16:56:16 - train: epoch 0107, iter [00300, 05004], lr: 0.047182, loss: 2.4941
2022-03-07 16:56:51 - train: epoch 0107, iter [00400, 05004], lr: 0.047182, loss: 2.7034
2022-03-07 16:57:24 - train: epoch 0107, iter [00500, 05004], lr: 0.047182, loss: 2.4306
2022-03-07 16:57:59 - train: epoch 0107, iter [00600, 05004], lr: 0.047182, loss: 2.4401
2022-03-07 16:58:33 - train: epoch 0107, iter [00700, 05004], lr: 0.047182, loss: 2.7067
2022-03-07 16:59:07 - train: epoch 0107, iter [00800, 05004], lr: 0.047182, loss: 2.5552
2022-03-07 16:59:42 - train: epoch 0107, iter [00900, 05004], lr: 0.047182, loss: 2.8130
2022-03-07 17:00:15 - train: epoch 0107, iter [01000, 05004], lr: 0.047182, loss: 2.5371
2022-03-07 17:00:50 - train: epoch 0107, iter [01100, 05004], lr: 0.047182, loss: 2.8407
2022-03-07 17:01:24 - train: epoch 0107, iter [01200, 05004], lr: 0.047182, loss: 2.6772
2022-03-07 17:01:59 - train: epoch 0107, iter [01300, 05004], lr: 0.047182, loss: 2.5999
2022-03-07 17:02:32 - train: epoch 0107, iter [01400, 05004], lr: 0.047182, loss: 2.6640
2022-03-07 17:03:08 - train: epoch 0107, iter [01500, 05004], lr: 0.047182, loss: 2.7780
2022-03-07 17:03:41 - train: epoch 0107, iter [01600, 05004], lr: 0.047182, loss: 2.5279
2022-03-07 17:04:16 - train: epoch 0107, iter [01700, 05004], lr: 0.047182, loss: 2.6414
2022-03-07 17:04:49 - train: epoch 0107, iter [01800, 05004], lr: 0.047182, loss: 2.8047
2022-03-07 17:05:24 - train: epoch 0107, iter [01900, 05004], lr: 0.047182, loss: 2.6617
2022-03-07 17:05:58 - train: epoch 0107, iter [02000, 05004], lr: 0.047182, loss: 2.6176
2022-03-07 17:06:32 - train: epoch 0107, iter [02100, 05004], lr: 0.047182, loss: 2.5247
2022-03-07 17:07:07 - train: epoch 0107, iter [02200, 05004], lr: 0.047182, loss: 2.6188
2022-03-07 17:07:41 - train: epoch 0107, iter [02300, 05004], lr: 0.047182, loss: 2.6622
2022-03-07 17:08:15 - train: epoch 0107, iter [02400, 05004], lr: 0.047182, loss: 2.7094
2022-03-07 17:08:49 - train: epoch 0107, iter [02500, 05004], lr: 0.047182, loss: 2.9051
2022-03-07 17:09:24 - train: epoch 0107, iter [02600, 05004], lr: 0.047182, loss: 2.7174
2022-03-07 17:09:58 - train: epoch 0107, iter [02700, 05004], lr: 0.047182, loss: 2.5148
2022-03-07 17:10:32 - train: epoch 0107, iter [02800, 05004], lr: 0.047182, loss: 2.8151
2022-03-07 17:11:06 - train: epoch 0107, iter [02900, 05004], lr: 0.047182, loss: 2.6913
2022-03-07 17:11:40 - train: epoch 0107, iter [03000, 05004], lr: 0.047182, loss: 2.6880
2022-03-07 17:12:16 - train: epoch 0107, iter [03100, 05004], lr: 0.047182, loss: 2.9641
2022-03-07 17:12:50 - train: epoch 0107, iter [03200, 05004], lr: 0.047182, loss: 2.7506
2022-03-07 17:13:24 - train: epoch 0107, iter [03300, 05004], lr: 0.047182, loss: 2.7378
2022-03-07 17:13:58 - train: epoch 0107, iter [03400, 05004], lr: 0.047182, loss: 2.6041
2022-03-07 17:14:33 - train: epoch 0107, iter [03500, 05004], lr: 0.047182, loss: 2.6023
2022-03-07 17:15:07 - train: epoch 0107, iter [03600, 05004], lr: 0.047182, loss: 2.9863
2022-03-07 17:15:41 - train: epoch 0107, iter [03700, 05004], lr: 0.047182, loss: 2.8663
2022-03-07 17:16:16 - train: epoch 0107, iter [03800, 05004], lr: 0.047182, loss: 2.6774
2022-03-07 17:16:50 - train: epoch 0107, iter [03900, 05004], lr: 0.047182, loss: 2.8022
2022-03-07 17:17:25 - train: epoch 0107, iter [04000, 05004], lr: 0.047182, loss: 2.8285
2022-03-07 17:17:58 - train: epoch 0107, iter [04100, 05004], lr: 0.047182, loss: 2.3732
2022-03-07 17:18:33 - train: epoch 0107, iter [04200, 05004], lr: 0.047182, loss: 2.8615
2022-03-07 17:19:07 - train: epoch 0107, iter [04300, 05004], lr: 0.047182, loss: 3.0496
2022-03-07 17:19:41 - train: epoch 0107, iter [04400, 05004], lr: 0.047182, loss: 2.6298
2022-03-07 17:20:15 - train: epoch 0107, iter [04500, 05004], lr: 0.047182, loss: 2.7046
2022-03-07 17:20:50 - train: epoch 0107, iter [04600, 05004], lr: 0.047182, loss: 2.4862
2022-03-07 17:21:24 - train: epoch 0107, iter [04700, 05004], lr: 0.047182, loss: 2.7122
2022-03-07 17:21:58 - train: epoch 0107, iter [04800, 05004], lr: 0.047182, loss: 2.8307
2022-03-07 17:22:33 - train: epoch 0107, iter [04900, 05004], lr: 0.047182, loss: 2.5796
2022-03-07 17:23:05 - train: epoch 0107, iter [05000, 05004], lr: 0.047182, loss: 2.3705
2022-03-07 17:23:06 - train: epoch 107, train_loss: 2.7040
2022-03-07 17:24:21 - eval: epoch: 107, acc1: 59.660%, acc5: 83.596%, test_loss: 1.6763, per_image_load_time: 2.180ms, per_image_inference_time: 0.523ms
2022-03-07 17:24:22 - until epoch: 107, best_acc1: 61.116%
2022-03-07 17:24:22 - epoch 108 lr: 0.04637825999191189
2022-03-07 17:25:01 - train: epoch 0108, iter [00100, 05004], lr: 0.046378, loss: 2.6625
2022-03-07 17:25:35 - train: epoch 0108, iter [00200, 05004], lr: 0.046378, loss: 2.4642
2022-03-07 17:26:09 - train: epoch 0108, iter [00300, 05004], lr: 0.046378, loss: 2.7296
2022-03-07 17:26:43 - train: epoch 0108, iter [00400, 05004], lr: 0.046378, loss: 2.8305
2022-03-07 17:27:19 - train: epoch 0108, iter [00500, 05004], lr: 0.046378, loss: 2.7071
2022-03-07 17:27:51 - train: epoch 0108, iter [00600, 05004], lr: 0.046378, loss: 2.7372
2022-03-07 17:28:26 - train: epoch 0108, iter [00700, 05004], lr: 0.046378, loss: 2.6955
2022-03-07 17:29:00 - train: epoch 0108, iter [00800, 05004], lr: 0.046378, loss: 2.6698
2022-03-07 17:29:35 - train: epoch 0108, iter [00900, 05004], lr: 0.046378, loss: 2.6158
2022-03-07 17:30:09 - train: epoch 0108, iter [01000, 05004], lr: 0.046378, loss: 2.4673
2022-03-07 17:30:43 - train: epoch 0108, iter [01100, 05004], lr: 0.046378, loss: 2.7904
2022-03-07 17:31:18 - train: epoch 0108, iter [01200, 05004], lr: 0.046378, loss: 2.7480
2022-03-07 17:31:52 - train: epoch 0108, iter [01300, 05004], lr: 0.046378, loss: 2.6298
2022-03-07 17:32:25 - train: epoch 0108, iter [01400, 05004], lr: 0.046378, loss: 2.4105
2022-03-07 17:33:00 - train: epoch 0108, iter [01500, 05004], lr: 0.046378, loss: 2.6123
2022-03-07 17:33:35 - train: epoch 0108, iter [01600, 05004], lr: 0.046378, loss: 2.9175
2022-03-07 17:34:09 - train: epoch 0108, iter [01700, 05004], lr: 0.046378, loss: 2.4565
2022-03-07 17:34:43 - train: epoch 0108, iter [01800, 05004], lr: 0.046378, loss: 2.5483
2022-03-07 17:35:17 - train: epoch 0108, iter [01900, 05004], lr: 0.046378, loss: 2.7019
2022-03-07 17:35:51 - train: epoch 0108, iter [02000, 05004], lr: 0.046378, loss: 2.9717
2022-03-07 17:36:25 - train: epoch 0108, iter [02100, 05004], lr: 0.046378, loss: 2.4922
2022-03-07 17:37:00 - train: epoch 0108, iter [02200, 05004], lr: 0.046378, loss: 2.8575
2022-03-07 17:37:34 - train: epoch 0108, iter [02300, 05004], lr: 0.046378, loss: 2.7153
2022-03-07 17:38:08 - train: epoch 0108, iter [02400, 05004], lr: 0.046378, loss: 2.8915
2022-03-07 17:38:42 - train: epoch 0108, iter [02500, 05004], lr: 0.046378, loss: 2.9269
2022-03-07 17:39:16 - train: epoch 0108, iter [02600, 05004], lr: 0.046378, loss: 2.8689
2022-03-07 17:39:50 - train: epoch 0108, iter [02700, 05004], lr: 0.046378, loss: 2.7154
2022-03-07 17:40:24 - train: epoch 0108, iter [02800, 05004], lr: 0.046378, loss: 2.5561
2022-03-07 17:40:58 - train: epoch 0108, iter [02900, 05004], lr: 0.046378, loss: 2.7156
2022-03-07 17:41:33 - train: epoch 0108, iter [03000, 05004], lr: 0.046378, loss: 2.5322
2022-03-07 17:42:07 - train: epoch 0108, iter [03100, 05004], lr: 0.046378, loss: 2.5409
2022-03-07 17:42:41 - train: epoch 0108, iter [03200, 05004], lr: 0.046378, loss: 2.6871
2022-03-07 17:43:15 - train: epoch 0108, iter [03300, 05004], lr: 0.046378, loss: 2.4685
2022-03-07 17:43:50 - train: epoch 0108, iter [03400, 05004], lr: 0.046378, loss: 2.6897
2022-03-07 17:44:24 - train: epoch 0108, iter [03500, 05004], lr: 0.046378, loss: 2.7276
2022-03-07 17:44:58 - train: epoch 0108, iter [03600, 05004], lr: 0.046378, loss: 2.8190
2022-03-07 17:45:32 - train: epoch 0108, iter [03700, 05004], lr: 0.046378, loss: 2.9347
2022-03-07 17:46:07 - train: epoch 0108, iter [03800, 05004], lr: 0.046378, loss: 2.7114
2022-03-07 17:46:41 - train: epoch 0108, iter [03900, 05004], lr: 0.046378, loss: 2.7804
2022-03-07 17:47:16 - train: epoch 0108, iter [04000, 05004], lr: 0.046378, loss: 2.4465
2022-03-07 17:47:49 - train: epoch 0108, iter [04100, 05004], lr: 0.046378, loss: 2.5917
2022-03-07 17:48:24 - train: epoch 0108, iter [04200, 05004], lr: 0.046378, loss: 2.6764
2022-03-07 17:48:58 - train: epoch 0108, iter [04300, 05004], lr: 0.046378, loss: 2.5077
2022-03-07 17:49:32 - train: epoch 0108, iter [04400, 05004], lr: 0.046378, loss: 2.8531
2022-03-07 17:50:07 - train: epoch 0108, iter [04500, 05004], lr: 0.046378, loss: 2.8355
2022-03-07 17:50:41 - train: epoch 0108, iter [04600, 05004], lr: 0.046378, loss: 2.7855
2022-03-07 17:51:16 - train: epoch 0108, iter [04700, 05004], lr: 0.046378, loss: 2.6378
2022-03-07 17:51:50 - train: epoch 0108, iter [04800, 05004], lr: 0.046378, loss: 2.3917
2022-03-07 17:52:25 - train: epoch 0108, iter [04900, 05004], lr: 0.046378, loss: 2.9951
2022-03-07 17:52:58 - train: epoch 0108, iter [05000, 05004], lr: 0.046378, loss: 2.7527
2022-03-07 17:52:59 - train: epoch 108, train_loss: 2.7019
2022-03-07 17:54:13 - eval: epoch: 108, acc1: 59.504%, acc5: 83.242%, test_loss: 1.6990, per_image_load_time: 1.363ms, per_image_inference_time: 0.566ms
2022-03-07 17:54:14 - until epoch: 108, best_acc1: 61.116%
2022-03-07 17:54:14 - epoch 109 lr: 0.045575344204432086
2022-03-07 17:54:53 - train: epoch 0109, iter [00100, 05004], lr: 0.045575, loss: 2.5931
2022-03-07 17:55:26 - train: epoch 0109, iter [00200, 05004], lr: 0.045575, loss: 2.6657
2022-03-07 17:56:00 - train: epoch 0109, iter [00300, 05004], lr: 0.045575, loss: 2.7629
2022-03-07 17:56:34 - train: epoch 0109, iter [00400, 05004], lr: 0.045575, loss: 2.4710
2022-03-07 17:57:09 - train: epoch 0109, iter [00500, 05004], lr: 0.045575, loss: 2.7237
2022-03-07 17:57:43 - train: epoch 0109, iter [00600, 05004], lr: 0.045575, loss: 2.5089
2022-03-07 17:58:18 - train: epoch 0109, iter [00700, 05004], lr: 0.045575, loss: 2.6211
2022-03-07 17:58:52 - train: epoch 0109, iter [00800, 05004], lr: 0.045575, loss: 2.6174
2022-03-07 17:59:25 - train: epoch 0109, iter [00900, 05004], lr: 0.045575, loss: 2.4465
2022-03-07 18:00:01 - train: epoch 0109, iter [01000, 05004], lr: 0.045575, loss: 2.3046
2022-03-07 18:00:35 - train: epoch 0109, iter [01100, 05004], lr: 0.045575, loss: 2.7027
2022-03-07 18:01:09 - train: epoch 0109, iter [01200, 05004], lr: 0.045575, loss: 2.7902
2022-03-07 18:01:43 - train: epoch 0109, iter [01300, 05004], lr: 0.045575, loss: 2.6893
2022-03-07 18:02:17 - train: epoch 0109, iter [01400, 05004], lr: 0.045575, loss: 2.7449
2022-03-07 18:02:51 - train: epoch 0109, iter [01500, 05004], lr: 0.045575, loss: 2.8825
2022-03-07 18:03:26 - train: epoch 0109, iter [01600, 05004], lr: 0.045575, loss: 2.7258
2022-03-07 18:04:00 - train: epoch 0109, iter [01700, 05004], lr: 0.045575, loss: 2.8028
2022-03-07 18:04:34 - train: epoch 0109, iter [01800, 05004], lr: 0.045575, loss: 2.9095
2022-03-07 18:05:09 - train: epoch 0109, iter [01900, 05004], lr: 0.045575, loss: 2.6384
2022-03-07 18:05:43 - train: epoch 0109, iter [02000, 05004], lr: 0.045575, loss: 2.6026
2022-03-07 18:06:18 - train: epoch 0109, iter [02100, 05004], lr: 0.045575, loss: 2.9219
2022-03-07 18:06:52 - train: epoch 0109, iter [02200, 05004], lr: 0.045575, loss: 2.5252
2022-03-07 18:07:26 - train: epoch 0109, iter [02300, 05004], lr: 0.045575, loss: 2.5710
2022-03-07 18:08:00 - train: epoch 0109, iter [02400, 05004], lr: 0.045575, loss: 2.8805
2022-03-07 18:08:35 - train: epoch 0109, iter [02500, 05004], lr: 0.045575, loss: 2.5641
2022-03-07 18:09:08 - train: epoch 0109, iter [02600, 05004], lr: 0.045575, loss: 2.8445
2022-03-07 18:09:42 - train: epoch 0109, iter [02700, 05004], lr: 0.045575, loss: 2.7685
2022-03-07 18:10:17 - train: epoch 0109, iter [02800, 05004], lr: 0.045575, loss: 2.7171
2022-03-07 18:10:51 - train: epoch 0109, iter [02900, 05004], lr: 0.045575, loss: 2.4771
2022-03-07 18:11:25 - train: epoch 0109, iter [03000, 05004], lr: 0.045575, loss: 2.7964
2022-03-07 18:12:00 - train: epoch 0109, iter [03100, 05004], lr: 0.045575, loss: 2.6675
2022-03-07 18:12:34 - train: epoch 0109, iter [03200, 05004], lr: 0.045575, loss: 2.8651
2022-03-07 18:13:09 - train: epoch 0109, iter [03300, 05004], lr: 0.045575, loss: 2.7475
2022-03-07 18:13:43 - train: epoch 0109, iter [03400, 05004], lr: 0.045575, loss: 2.6181
2022-03-07 18:14:16 - train: epoch 0109, iter [03500, 05004], lr: 0.045575, loss: 2.6798
2022-03-07 18:14:50 - train: epoch 0109, iter [03600, 05004], lr: 0.045575, loss: 2.4902
2022-03-07 18:15:25 - train: epoch 0109, iter [03700, 05004], lr: 0.045575, loss: 2.3556
2022-03-07 18:15:59 - train: epoch 0109, iter [03800, 05004], lr: 0.045575, loss: 2.7821
2022-03-07 18:16:34 - train: epoch 0109, iter [03900, 05004], lr: 0.045575, loss: 2.5033
2022-03-07 18:17:07 - train: epoch 0109, iter [04000, 05004], lr: 0.045575, loss: 2.8051
2022-03-07 18:17:43 - train: epoch 0109, iter [04100, 05004], lr: 0.045575, loss: 2.7356
2022-03-07 18:18:17 - train: epoch 0109, iter [04200, 05004], lr: 0.045575, loss: 2.9699
2022-03-07 18:18:51 - train: epoch 0109, iter [04300, 05004], lr: 0.045575, loss: 2.7949
2022-03-07 18:19:26 - train: epoch 0109, iter [04400, 05004], lr: 0.045575, loss: 2.7250
2022-03-07 18:19:59 - train: epoch 0109, iter [04500, 05004], lr: 0.045575, loss: 2.5976
2022-03-07 18:20:33 - train: epoch 0109, iter [04600, 05004], lr: 0.045575, loss: 2.6113
2022-03-07 18:21:06 - train: epoch 0109, iter [04700, 05004], lr: 0.045575, loss: 2.3921
2022-03-07 18:21:41 - train: epoch 0109, iter [04800, 05004], lr: 0.045575, loss: 2.6832
2022-03-07 18:22:15 - train: epoch 0109, iter [04900, 05004], lr: 0.045575, loss: 2.5984
2022-03-07 18:22:49 - train: epoch 0109, iter [05000, 05004], lr: 0.045575, loss: 2.4302
2022-03-07 18:22:50 - train: epoch 109, train_loss: 2.6906
2022-03-07 18:24:05 - eval: epoch: 109, acc1: 60.002%, acc5: 83.488%, test_loss: 1.6767, per_image_load_time: 2.345ms, per_image_inference_time: 0.555ms
2022-03-07 18:24:05 - until epoch: 109, best_acc1: 61.116%
2022-03-07 18:24:05 - epoch 110 lr: 0.04477357683661734
2022-03-07 18:24:44 - train: epoch 0110, iter [00100, 05004], lr: 0.044774, loss: 2.7428
2022-03-07 18:25:18 - train: epoch 0110, iter [00200, 05004], lr: 0.044774, loss: 2.3090
2022-03-07 18:25:52 - train: epoch 0110, iter [00300, 05004], lr: 0.044774, loss: 2.8352
2022-03-07 18:26:27 - train: epoch 0110, iter [00400, 05004], lr: 0.044774, loss: 2.7317
2022-03-07 18:27:01 - train: epoch 0110, iter [00500, 05004], lr: 0.044774, loss: 2.4919
2022-03-07 18:27:36 - train: epoch 0110, iter [00600, 05004], lr: 0.044774, loss: 2.5127
2022-03-07 18:28:10 - train: epoch 0110, iter [00700, 05004], lr: 0.044774, loss: 2.6251
2022-03-07 18:28:44 - train: epoch 0110, iter [00800, 05004], lr: 0.044774, loss: 2.7330
2022-03-07 18:29:18 - train: epoch 0110, iter [00900, 05004], lr: 0.044774, loss: 2.4725
2022-03-07 18:29:53 - train: epoch 0110, iter [01000, 05004], lr: 0.044774, loss: 2.7063
2022-03-07 18:30:27 - train: epoch 0110, iter [01100, 05004], lr: 0.044774, loss: 3.1332
2022-03-07 18:31:02 - train: epoch 0110, iter [01200, 05004], lr: 0.044774, loss: 2.4988
2022-03-07 18:31:37 - train: epoch 0110, iter [01300, 05004], lr: 0.044774, loss: 2.4824
2022-03-07 18:32:10 - train: epoch 0110, iter [01400, 05004], lr: 0.044774, loss: 2.5024
2022-03-07 18:32:45 - train: epoch 0110, iter [01500, 05004], lr: 0.044774, loss: 2.8249
2022-03-07 18:33:19 - train: epoch 0110, iter [01600, 05004], lr: 0.044774, loss: 2.8767
2022-03-07 18:33:53 - train: epoch 0110, iter [01700, 05004], lr: 0.044774, loss: 2.6823
2022-03-07 18:34:28 - train: epoch 0110, iter [01800, 05004], lr: 0.044774, loss: 2.8322
2022-03-07 18:35:02 - train: epoch 0110, iter [01900, 05004], lr: 0.044774, loss: 2.8673
2022-03-07 18:35:36 - train: epoch 0110, iter [02000, 05004], lr: 0.044774, loss: 2.6883
2022-03-07 18:36:10 - train: epoch 0110, iter [02100, 05004], lr: 0.044774, loss: 2.7843
2022-03-07 18:36:45 - train: epoch 0110, iter [02200, 05004], lr: 0.044774, loss: 2.5196
2022-03-07 18:37:19 - train: epoch 0110, iter [02300, 05004], lr: 0.044774, loss: 2.5862
2022-03-07 18:37:53 - train: epoch 0110, iter [02400, 05004], lr: 0.044774, loss: 2.6171
2022-03-07 18:38:28 - train: epoch 0110, iter [02500, 05004], lr: 0.044774, loss: 2.6545
2022-03-07 18:39:01 - train: epoch 0110, iter [02600, 05004], lr: 0.044774, loss: 2.8023
2022-03-07 18:39:36 - train: epoch 0110, iter [02700, 05004], lr: 0.044774, loss: 2.7122
2022-03-07 18:40:10 - train: epoch 0110, iter [02800, 05004], lr: 0.044774, loss: 2.6661
2022-03-07 18:40:45 - train: epoch 0110, iter [02900, 05004], lr: 0.044774, loss: 2.7239
2022-03-07 18:41:19 - train: epoch 0110, iter [03000, 05004], lr: 0.044774, loss: 2.4257
2022-03-07 18:41:53 - train: epoch 0110, iter [03100, 05004], lr: 0.044774, loss: 2.7889
2022-03-07 18:42:28 - train: epoch 0110, iter [03200, 05004], lr: 0.044774, loss: 2.6379
2022-03-07 18:43:01 - train: epoch 0110, iter [03300, 05004], lr: 0.044774, loss: 2.6720
2022-03-07 18:43:36 - train: epoch 0110, iter [03400, 05004], lr: 0.044774, loss: 2.6816
2022-03-07 18:44:09 - train: epoch 0110, iter [03500, 05004], lr: 0.044774, loss: 2.7580
2022-03-07 18:44:44 - train: epoch 0110, iter [03600, 05004], lr: 0.044774, loss: 3.0294
2022-03-07 18:45:18 - train: epoch 0110, iter [03700, 05004], lr: 0.044774, loss: 2.8966
2022-03-07 18:45:52 - train: epoch 0110, iter [03800, 05004], lr: 0.044774, loss: 2.6163
2022-03-07 18:46:25 - train: epoch 0110, iter [03900, 05004], lr: 0.044774, loss: 3.0050
2022-03-07 18:47:00 - train: epoch 0110, iter [04000, 05004], lr: 0.044774, loss: 2.7373
2022-03-07 18:47:34 - train: epoch 0110, iter [04100, 05004], lr: 0.044774, loss: 2.7456
2022-03-07 18:48:08 - train: epoch 0110, iter [04200, 05004], lr: 0.044774, loss: 2.6086
2022-03-07 18:48:42 - train: epoch 0110, iter [04300, 05004], lr: 0.044774, loss: 2.4527
2022-03-07 18:49:16 - train: epoch 0110, iter [04400, 05004], lr: 0.044774, loss: 3.0353
2022-03-07 18:49:51 - train: epoch 0110, iter [04500, 05004], lr: 0.044774, loss: 2.6809
2022-03-07 18:50:25 - train: epoch 0110, iter [04600, 05004], lr: 0.044774, loss: 2.6023
2022-03-07 18:50:59 - train: epoch 0110, iter [04700, 05004], lr: 0.044774, loss: 2.8732
2022-03-07 18:51:33 - train: epoch 0110, iter [04800, 05004], lr: 0.044774, loss: 2.9965
2022-03-07 18:52:07 - train: epoch 0110, iter [04900, 05004], lr: 0.044774, loss: 2.6272
2022-03-07 18:52:40 - train: epoch 0110, iter [05000, 05004], lr: 0.044774, loss: 2.8781
2022-03-07 18:52:41 - train: epoch 110, train_loss: 2.6865
2022-03-07 18:53:55 - eval: epoch: 110, acc1: 59.806%, acc5: 83.440%, test_loss: 1.6750, per_image_load_time: 1.908ms, per_image_inference_time: 0.541ms
2022-03-07 18:53:56 - until epoch: 110, best_acc1: 61.116%
2022-03-07 18:53:56 - epoch 111 lr: 0.04397316598723386
2022-03-07 18:54:35 - train: epoch 0111, iter [00100, 05004], lr: 0.043973, loss: 2.4653
2022-03-07 18:55:10 - train: epoch 0111, iter [00200, 05004], lr: 0.043973, loss: 2.7599
2022-03-07 18:55:43 - train: epoch 0111, iter [00300, 05004], lr: 0.043973, loss: 2.7410
2022-03-07 18:56:17 - train: epoch 0111, iter [00400, 05004], lr: 0.043973, loss: 2.6877
2022-03-07 18:56:51 - train: epoch 0111, iter [00500, 05004], lr: 0.043973, loss: 2.3767
2022-03-07 18:57:26 - train: epoch 0111, iter [00600, 05004], lr: 0.043973, loss: 2.7856
2022-03-07 18:57:59 - train: epoch 0111, iter [00700, 05004], lr: 0.043973, loss: 2.6455
2022-03-07 18:58:33 - train: epoch 0111, iter [00800, 05004], lr: 0.043973, loss: 2.6477
2022-03-07 18:59:08 - train: epoch 0111, iter [00900, 05004], lr: 0.043973, loss: 2.3544
2022-03-07 18:59:42 - train: epoch 0111, iter [01000, 05004], lr: 0.043973, loss: 2.5798
2022-03-07 19:00:16 - train: epoch 0111, iter [01100, 05004], lr: 0.043973, loss: 2.3574
2022-03-07 19:00:50 - train: epoch 0111, iter [01200, 05004], lr: 0.043973, loss: 2.5739
2022-03-07 19:01:23 - train: epoch 0111, iter [01300, 05004], lr: 0.043973, loss: 2.5869
2022-03-07 19:01:58 - train: epoch 0111, iter [01400, 05004], lr: 0.043973, loss: 2.5255
2022-03-07 19:02:32 - train: epoch 0111, iter [01500, 05004], lr: 0.043973, loss: 2.6124
2022-03-07 19:03:05 - train: epoch 0111, iter [01600, 05004], lr: 0.043973, loss: 2.5225
2022-03-07 19:03:40 - train: epoch 0111, iter [01700, 05004], lr: 0.043973, loss: 2.5054
2022-03-07 19:04:15 - train: epoch 0111, iter [01800, 05004], lr: 0.043973, loss: 2.4056
2022-03-07 19:04:49 - train: epoch 0111, iter [01900, 05004], lr: 0.043973, loss: 2.7406
2022-03-07 19:05:23 - train: epoch 0111, iter [02000, 05004], lr: 0.043973, loss: 2.8472
2022-03-07 19:05:57 - train: epoch 0111, iter [02100, 05004], lr: 0.043973, loss: 3.1532
2022-03-07 19:06:31 - train: epoch 0111, iter [02200, 05004], lr: 0.043973, loss: 2.8074
2022-03-07 19:07:04 - train: epoch 0111, iter [02300, 05004], lr: 0.043973, loss: 2.6950
2022-03-07 19:07:38 - train: epoch 0111, iter [02400, 05004], lr: 0.043973, loss: 2.4682
2022-03-07 19:08:13 - train: epoch 0111, iter [02500, 05004], lr: 0.043973, loss: 2.7159
2022-03-07 19:08:48 - train: epoch 0111, iter [02600, 05004], lr: 0.043973, loss: 2.4102
2022-03-07 19:09:22 - train: epoch 0111, iter [02700, 05004], lr: 0.043973, loss: 2.7077
2022-03-07 19:09:56 - train: epoch 0111, iter [02800, 05004], lr: 0.043973, loss: 2.5592
2022-03-07 19:10:30 - train: epoch 0111, iter [02900, 05004], lr: 0.043973, loss: 2.7148
2022-03-07 19:11:04 - train: epoch 0111, iter [03000, 05004], lr: 0.043973, loss: 3.0251
2022-03-07 19:11:38 - train: epoch 0111, iter [03100, 05004], lr: 0.043973, loss: 2.5374
2022-03-07 19:12:13 - train: epoch 0111, iter [03200, 05004], lr: 0.043973, loss: 2.7064
2022-03-07 19:12:47 - train: epoch 0111, iter [03300, 05004], lr: 0.043973, loss: 2.7443
2022-03-07 19:13:21 - train: epoch 0111, iter [03400, 05004], lr: 0.043973, loss: 2.9287
2022-03-07 19:13:55 - train: epoch 0111, iter [03500, 05004], lr: 0.043973, loss: 2.8507
2022-03-07 19:14:30 - train: epoch 0111, iter [03600, 05004], lr: 0.043973, loss: 2.5980
2022-03-07 19:15:04 - train: epoch 0111, iter [03700, 05004], lr: 0.043973, loss: 2.7543
2022-03-07 19:15:39 - train: epoch 0111, iter [03800, 05004], lr: 0.043973, loss: 2.6364
2022-03-07 19:16:13 - train: epoch 0111, iter [03900, 05004], lr: 0.043973, loss: 2.6008
2022-03-07 19:16:47 - train: epoch 0111, iter [04000, 05004], lr: 0.043973, loss: 2.8255
2022-03-07 19:17:22 - train: epoch 0111, iter [04100, 05004], lr: 0.043973, loss: 2.8053
2022-03-07 19:17:56 - train: epoch 0111, iter [04200, 05004], lr: 0.043973, loss: 2.3742
2022-03-07 19:18:31 - train: epoch 0111, iter [04300, 05004], lr: 0.043973, loss: 2.6989
2022-03-07 19:19:06 - train: epoch 0111, iter [04400, 05004], lr: 0.043973, loss: 2.8356
2022-03-07 19:19:39 - train: epoch 0111, iter [04500, 05004], lr: 0.043973, loss: 2.6395
2022-03-07 19:20:13 - train: epoch 0111, iter [04600, 05004], lr: 0.043973, loss: 2.5198
2022-03-07 19:20:48 - train: epoch 0111, iter [04700, 05004], lr: 0.043973, loss: 2.6607
2022-03-07 19:21:22 - train: epoch 0111, iter [04800, 05004], lr: 0.043973, loss: 2.8365
2022-03-07 19:21:56 - train: epoch 0111, iter [04900, 05004], lr: 0.043973, loss: 2.7353
2022-03-07 19:22:29 - train: epoch 0111, iter [05000, 05004], lr: 0.043973, loss: 2.8931
2022-03-07 19:22:30 - train: epoch 111, train_loss: 2.6756
2022-03-07 19:23:45 - eval: epoch: 111, acc1: 60.494%, acc5: 83.878%, test_loss: 1.6459, per_image_load_time: 2.307ms, per_image_inference_time: 0.559ms
2022-03-07 19:23:45 - until epoch: 111, best_acc1: 61.116%
2022-03-07 19:23:45 - epoch 112 lr: 0.04317431940296343
2022-03-07 19:24:25 - train: epoch 0112, iter [00100, 05004], lr: 0.043174, loss: 2.5059
2022-03-07 19:24:59 - train: epoch 0112, iter [00200, 05004], lr: 0.043174, loss: 2.6834
2022-03-07 19:25:34 - train: epoch 0112, iter [00300, 05004], lr: 0.043174, loss: 2.5791
2022-03-07 19:26:07 - train: epoch 0112, iter [00400, 05004], lr: 0.043174, loss: 2.7038
2022-03-07 19:26:41 - train: epoch 0112, iter [00500, 05004], lr: 0.043174, loss: 2.6366
2022-03-07 19:27:16 - train: epoch 0112, iter [00600, 05004], lr: 0.043174, loss: 2.5284
2022-03-07 19:27:50 - train: epoch 0112, iter [00700, 05004], lr: 0.043174, loss: 2.7215
2022-03-07 19:28:25 - train: epoch 0112, iter [00800, 05004], lr: 0.043174, loss: 2.6723
2022-03-07 19:28:59 - train: epoch 0112, iter [00900, 05004], lr: 0.043174, loss: 2.6507
2022-03-07 19:29:33 - train: epoch 0112, iter [01000, 05004], lr: 0.043174, loss: 2.5834
2022-03-07 19:30:08 - train: epoch 0112, iter [01100, 05004], lr: 0.043174, loss: 2.6162
2022-03-07 19:30:42 - train: epoch 0112, iter [01200, 05004], lr: 0.043174, loss: 2.6123
2022-03-07 19:31:16 - train: epoch 0112, iter [01300, 05004], lr: 0.043174, loss: 2.6393
2022-03-07 19:31:50 - train: epoch 0112, iter [01400, 05004], lr: 0.043174, loss: 2.7177
2022-03-07 19:32:24 - train: epoch 0112, iter [01500, 05004], lr: 0.043174, loss: 2.7119
2022-03-07 19:32:58 - train: epoch 0112, iter [01600, 05004], lr: 0.043174, loss: 2.9295
2022-03-07 19:33:32 - train: epoch 0112, iter [01700, 05004], lr: 0.043174, loss: 2.4426
2022-03-07 19:34:05 - train: epoch 0112, iter [01800, 05004], lr: 0.043174, loss: 2.5108
2022-03-07 19:34:41 - train: epoch 0112, iter [01900, 05004], lr: 0.043174, loss: 2.2952
2022-03-07 19:35:15 - train: epoch 0112, iter [02000, 05004], lr: 0.043174, loss: 2.7698
2022-03-07 19:35:49 - train: epoch 0112, iter [02100, 05004], lr: 0.043174, loss: 2.7179
2022-03-07 19:36:23 - train: epoch 0112, iter [02200, 05004], lr: 0.043174, loss: 2.4773
2022-03-07 19:36:58 - train: epoch 0112, iter [02300, 05004], lr: 0.043174, loss: 2.5274
2022-03-07 19:37:32 - train: epoch 0112, iter [02400, 05004], lr: 0.043174, loss: 2.4957
2022-03-07 19:38:06 - train: epoch 0112, iter [02500, 05004], lr: 0.043174, loss: 2.5759
2022-03-07 19:38:41 - train: epoch 0112, iter [02600, 05004], lr: 0.043174, loss: 2.4697
2022-03-07 19:39:15 - train: epoch 0112, iter [02700, 05004], lr: 0.043174, loss: 2.6825
2022-03-07 19:39:50 - train: epoch 0112, iter [02800, 05004], lr: 0.043174, loss: 2.5777
2022-03-07 19:40:24 - train: epoch 0112, iter [02900, 05004], lr: 0.043174, loss: 2.6984
2022-03-07 19:40:59 - train: epoch 0112, iter [03000, 05004], lr: 0.043174, loss: 2.9508
2022-03-07 19:41:33 - train: epoch 0112, iter [03100, 05004], lr: 0.043174, loss: 2.7291
2022-03-07 19:42:08 - train: epoch 0112, iter [03200, 05004], lr: 0.043174, loss: 2.5623
2022-03-07 19:42:42 - train: epoch 0112, iter [03300, 05004], lr: 0.043174, loss: 2.6334
2022-03-07 19:43:15 - train: epoch 0112, iter [03400, 05004], lr: 0.043174, loss: 2.6821
2022-03-07 19:43:50 - train: epoch 0112, iter [03500, 05004], lr: 0.043174, loss: 2.5683
2022-03-07 19:44:25 - train: epoch 0112, iter [03600, 05004], lr: 0.043174, loss: 2.5505
2022-03-07 19:44:59 - train: epoch 0112, iter [03700, 05004], lr: 0.043174, loss: 2.9225
2022-03-07 19:45:33 - train: epoch 0112, iter [03800, 05004], lr: 0.043174, loss: 2.7964
2022-03-07 19:46:07 - train: epoch 0112, iter [03900, 05004], lr: 0.043174, loss: 2.6660
2022-03-07 19:46:42 - train: epoch 0112, iter [04000, 05004], lr: 0.043174, loss: 2.5424
2022-03-07 19:47:16 - train: epoch 0112, iter [04100, 05004], lr: 0.043174, loss: 2.6702
2022-03-07 19:47:51 - train: epoch 0112, iter [04200, 05004], lr: 0.043174, loss: 2.5751
2022-03-07 19:48:24 - train: epoch 0112, iter [04300, 05004], lr: 0.043174, loss: 2.8150
2022-03-07 19:48:58 - train: epoch 0112, iter [04400, 05004], lr: 0.043174, loss: 2.8904
2022-03-07 19:49:32 - train: epoch 0112, iter [04500, 05004], lr: 0.043174, loss: 2.5136
2022-03-07 19:50:07 - train: epoch 0112, iter [04600, 05004], lr: 0.043174, loss: 2.6662
2022-03-07 19:50:41 - train: epoch 0112, iter [04700, 05004], lr: 0.043174, loss: 2.8052
2022-03-07 19:51:16 - train: epoch 0112, iter [04800, 05004], lr: 0.043174, loss: 2.5428
2022-03-07 19:51:49 - train: epoch 0112, iter [04900, 05004], lr: 0.043174, loss: 2.6212
2022-03-07 19:52:23 - train: epoch 0112, iter [05000, 05004], lr: 0.043174, loss: 2.4866
2022-03-07 19:52:24 - train: epoch 112, train_loss: 2.6702
2022-03-07 19:53:39 - eval: epoch: 112, acc1: 61.792%, acc5: 85.080%, test_loss: 1.5895, per_image_load_time: 1.872ms, per_image_inference_time: 0.555ms
2022-03-07 19:53:40 - until epoch: 112, best_acc1: 61.792%
2022-03-07 19:53:40 - epoch 113 lr: 0.042377244424482735
2022-03-07 19:54:19 - train: epoch 0113, iter [00100, 05004], lr: 0.042377, loss: 2.6632
2022-03-07 19:54:53 - train: epoch 0113, iter [00200, 05004], lr: 0.042377, loss: 2.4765
2022-03-07 19:55:27 - train: epoch 0113, iter [00300, 05004], lr: 0.042377, loss: 2.7237
2022-03-07 19:56:02 - train: epoch 0113, iter [00400, 05004], lr: 0.042377, loss: 2.6326
2022-03-07 19:56:36 - train: epoch 0113, iter [00500, 05004], lr: 0.042377, loss: 2.9180
2022-03-07 19:57:10 - train: epoch 0113, iter [00600, 05004], lr: 0.042377, loss: 2.6510
2022-03-07 19:57:44 - train: epoch 0113, iter [00700, 05004], lr: 0.042377, loss: 2.7216
2022-03-07 19:58:20 - train: epoch 0113, iter [00800, 05004], lr: 0.042377, loss: 2.7239
2022-03-07 19:58:55 - train: epoch 0113, iter [00900, 05004], lr: 0.042377, loss: 2.4655
2022-03-07 19:59:28 - train: epoch 0113, iter [01000, 05004], lr: 0.042377, loss: 2.8607
2022-03-07 20:00:03 - train: epoch 0113, iter [01100, 05004], lr: 0.042377, loss: 2.5642
2022-03-07 20:00:38 - train: epoch 0113, iter [01200, 05004], lr: 0.042377, loss: 2.5038
2022-03-07 20:01:11 - train: epoch 0113, iter [01300, 05004], lr: 0.042377, loss: 2.4247
2022-03-07 20:01:46 - train: epoch 0113, iter [01400, 05004], lr: 0.042377, loss: 2.9433
2022-03-07 20:02:20 - train: epoch 0113, iter [01500, 05004], lr: 0.042377, loss: 2.5375
2022-03-07 20:02:54 - train: epoch 0113, iter [01600, 05004], lr: 0.042377, loss: 2.7808
2022-03-07 20:03:29 - train: epoch 0113, iter [01700, 05004], lr: 0.042377, loss: 2.8242
2022-03-07 20:04:03 - train: epoch 0113, iter [01800, 05004], lr: 0.042377, loss: 2.4400
2022-03-07 20:04:38 - train: epoch 0113, iter [01900, 05004], lr: 0.042377, loss: 2.6978
2022-03-07 20:05:12 - train: epoch 0113, iter [02000, 05004], lr: 0.042377, loss: 2.4750
2022-03-07 20:05:46 - train: epoch 0113, iter [02100, 05004], lr: 0.042377, loss: 2.9413
2022-03-07 20:06:21 - train: epoch 0113, iter [02200, 05004], lr: 0.042377, loss: 2.7005
2022-03-07 20:06:55 - train: epoch 0113, iter [02300, 05004], lr: 0.042377, loss: 2.6992
2022-03-07 20:07:29 - train: epoch 0113, iter [02400, 05004], lr: 0.042377, loss: 2.5488
2022-03-07 20:08:04 - train: epoch 0113, iter [02500, 05004], lr: 0.042377, loss: 2.6667
2022-03-07 20:08:38 - train: epoch 0113, iter [02600, 05004], lr: 0.042377, loss: 2.5299
2022-03-07 20:09:13 - train: epoch 0113, iter [02700, 05004], lr: 0.042377, loss: 2.5905
2022-03-07 20:09:47 - train: epoch 0113, iter [02800, 05004], lr: 0.042377, loss: 2.6597
2022-03-07 20:10:21 - train: epoch 0113, iter [02900, 05004], lr: 0.042377, loss: 2.6957
2022-03-07 20:10:56 - train: epoch 0113, iter [03000, 05004], lr: 0.042377, loss: 2.4412
2022-03-07 20:11:30 - train: epoch 0113, iter [03100, 05004], lr: 0.042377, loss: 2.4447
2022-03-07 20:12:04 - train: epoch 0113, iter [03200, 05004], lr: 0.042377, loss: 2.3348
2022-03-07 20:12:38 - train: epoch 0113, iter [03300, 05004], lr: 0.042377, loss: 2.6361
2022-03-07 20:13:12 - train: epoch 0113, iter [03400, 05004], lr: 0.042377, loss: 2.5738
2022-03-07 20:13:47 - train: epoch 0113, iter [03500, 05004], lr: 0.042377, loss: 2.6876
2022-03-07 20:14:21 - train: epoch 0113, iter [03600, 05004], lr: 0.042377, loss: 2.5618
2022-03-07 20:14:55 - train: epoch 0113, iter [03700, 05004], lr: 0.042377, loss: 2.4535
2022-03-07 20:15:29 - train: epoch 0113, iter [03800, 05004], lr: 0.042377, loss: 3.2175
2022-03-07 20:16:03 - train: epoch 0113, iter [03900, 05004], lr: 0.042377, loss: 2.5817
2022-03-07 20:16:37 - train: epoch 0113, iter [04000, 05004], lr: 0.042377, loss: 2.6711
2022-03-07 20:17:11 - train: epoch 0113, iter [04100, 05004], lr: 0.042377, loss: 2.7260
2022-03-07 20:17:45 - train: epoch 0113, iter [04200, 05004], lr: 0.042377, loss: 2.4520
2022-03-07 20:18:19 - train: epoch 0113, iter [04300, 05004], lr: 0.042377, loss: 2.4300
2022-03-07 20:18:54 - train: epoch 0113, iter [04400, 05004], lr: 0.042377, loss: 2.7834
2022-03-07 20:19:27 - train: epoch 0113, iter [04500, 05004], lr: 0.042377, loss: 2.6359
2022-03-07 20:20:02 - train: epoch 0113, iter [04600, 05004], lr: 0.042377, loss: 3.0824
2022-03-07 20:20:36 - train: epoch 0113, iter [04700, 05004], lr: 0.042377, loss: 2.7938
2022-03-07 20:21:10 - train: epoch 0113, iter [04800, 05004], lr: 0.042377, loss: 3.0554
2022-03-07 20:21:44 - train: epoch 0113, iter [04900, 05004], lr: 0.042377, loss: 2.6633
2022-03-07 20:22:17 - train: epoch 0113, iter [05000, 05004], lr: 0.042377, loss: 2.8784
2022-03-07 20:22:18 - train: epoch 113, train_loss: 2.6596
2022-03-07 20:23:32 - eval: epoch: 113, acc1: 60.020%, acc5: 83.748%, test_loss: 1.6642, per_image_load_time: 0.879ms, per_image_inference_time: 0.542ms
2022-03-07 20:23:33 - until epoch: 113, best_acc1: 61.792%
2022-03-07 20:23:33 - epoch 114 lr: 0.04158214793264808
2022-03-07 20:24:11 - train: epoch 0114, iter [00100, 05004], lr: 0.041582, loss: 2.6515
2022-03-07 20:24:45 - train: epoch 0114, iter [00200, 05004], lr: 0.041582, loss: 2.4602
2022-03-07 20:25:20 - train: epoch 0114, iter [00300, 05004], lr: 0.041582, loss: 2.5657
2022-03-07 20:25:52 - train: epoch 0114, iter [00400, 05004], lr: 0.041582, loss: 2.7399
2022-03-07 20:26:28 - train: epoch 0114, iter [00500, 05004], lr: 0.041582, loss: 2.7274
2022-03-07 20:27:01 - train: epoch 0114, iter [00600, 05004], lr: 0.041582, loss: 2.8297
2022-03-07 20:27:36 - train: epoch 0114, iter [00700, 05004], lr: 0.041582, loss: 2.4733
2022-03-07 20:28:10 - train: epoch 0114, iter [00800, 05004], lr: 0.041582, loss: 2.5546
2022-03-07 20:28:44 - train: epoch 0114, iter [00900, 05004], lr: 0.041582, loss: 2.5442
2022-03-07 20:29:19 - train: epoch 0114, iter [01000, 05004], lr: 0.041582, loss: 2.7009
2022-03-07 20:29:52 - train: epoch 0114, iter [01100, 05004], lr: 0.041582, loss: 2.3695
2022-03-07 20:30:27 - train: epoch 0114, iter [01200, 05004], lr: 0.041582, loss: 2.7383
2022-03-07 20:31:01 - train: epoch 0114, iter [01300, 05004], lr: 0.041582, loss: 2.5487
2022-03-07 20:31:36 - train: epoch 0114, iter [01400, 05004], lr: 0.041582, loss: 2.7609
2022-03-07 20:32:10 - train: epoch 0114, iter [01500, 05004], lr: 0.041582, loss: 2.5165
2022-03-07 20:32:45 - train: epoch 0114, iter [01600, 05004], lr: 0.041582, loss: 2.4868
2022-03-07 20:33:18 - train: epoch 0114, iter [01700, 05004], lr: 0.041582, loss: 2.3975
2022-03-07 20:33:53 - train: epoch 0114, iter [01800, 05004], lr: 0.041582, loss: 2.6261
2022-03-07 20:34:26 - train: epoch 0114, iter [01900, 05004], lr: 0.041582, loss: 2.4203
2022-03-07 20:35:01 - train: epoch 0114, iter [02000, 05004], lr: 0.041582, loss: 2.8061
2022-03-07 20:35:35 - train: epoch 0114, iter [02100, 05004], lr: 0.041582, loss: 2.4527
2022-03-07 20:36:10 - train: epoch 0114, iter [02200, 05004], lr: 0.041582, loss: 2.6108
2022-03-07 20:36:44 - train: epoch 0114, iter [02300, 05004], lr: 0.041582, loss: 2.9775
2022-03-07 20:37:18 - train: epoch 0114, iter [02400, 05004], lr: 0.041582, loss: 2.4581
2022-03-07 20:37:52 - train: epoch 0114, iter [02500, 05004], lr: 0.041582, loss: 2.3527
2022-03-07 20:38:27 - train: epoch 0114, iter [02600, 05004], lr: 0.041582, loss: 2.6168
2022-03-07 20:39:01 - train: epoch 0114, iter [02700, 05004], lr: 0.041582, loss: 2.7617
2022-03-07 20:39:35 - train: epoch 0114, iter [02800, 05004], lr: 0.041582, loss: 2.8734
2022-03-07 20:40:09 - train: epoch 0114, iter [02900, 05004], lr: 0.041582, loss: 2.8194
2022-03-07 20:40:44 - train: epoch 0114, iter [03000, 05004], lr: 0.041582, loss: 2.6185
2022-03-07 20:41:18 - train: epoch 0114, iter [03100, 05004], lr: 0.041582, loss: 2.8212
2022-03-07 20:41:52 - train: epoch 0114, iter [03200, 05004], lr: 0.041582, loss: 2.6895
2022-03-07 20:42:26 - train: epoch 0114, iter [03300, 05004], lr: 0.041582, loss: 2.7540
2022-03-07 20:43:01 - train: epoch 0114, iter [03400, 05004], lr: 0.041582, loss: 2.6693
2022-03-07 20:43:34 - train: epoch 0114, iter [03500, 05004], lr: 0.041582, loss: 2.4598
2022-03-07 20:44:09 - train: epoch 0114, iter [03600, 05004], lr: 0.041582, loss: 2.7076
2022-03-07 20:44:43 - train: epoch 0114, iter [03700, 05004], lr: 0.041582, loss: 2.5109
2022-03-07 20:45:18 - train: epoch 0114, iter [03800, 05004], lr: 0.041582, loss: 2.6036
2022-03-07 20:45:52 - train: epoch 0114, iter [03900, 05004], lr: 0.041582, loss: 2.4883
2022-03-07 20:46:26 - train: epoch 0114, iter [04000, 05004], lr: 0.041582, loss: 2.4254
2022-03-07 20:47:00 - train: epoch 0114, iter [04100, 05004], lr: 0.041582, loss: 2.5443
2022-03-07 20:47:34 - train: epoch 0114, iter [04200, 05004], lr: 0.041582, loss: 2.6565
2022-03-07 20:48:09 - train: epoch 0114, iter [04300, 05004], lr: 0.041582, loss: 2.7425
2022-03-07 20:48:43 - train: epoch 0114, iter [04400, 05004], lr: 0.041582, loss: 2.6460
2022-03-07 20:49:17 - train: epoch 0114, iter [04500, 05004], lr: 0.041582, loss: 2.8628
2022-03-07 20:49:51 - train: epoch 0114, iter [04600, 05004], lr: 0.041582, loss: 2.5584
2022-03-07 20:50:26 - train: epoch 0114, iter [04700, 05004], lr: 0.041582, loss: 2.6562
2022-03-07 20:50:59 - train: epoch 0114, iter [04800, 05004], lr: 0.041582, loss: 2.8322
2022-03-07 20:51:34 - train: epoch 0114, iter [04900, 05004], lr: 0.041582, loss: 2.4656
2022-03-07 20:52:07 - train: epoch 0114, iter [05000, 05004], lr: 0.041582, loss: 2.9372
2022-03-07 20:52:08 - train: epoch 114, train_loss: 2.6527
2022-03-07 20:53:22 - eval: epoch: 114, acc1: 61.482%, acc5: 84.394%, test_loss: 1.6025, per_image_load_time: 1.102ms, per_image_inference_time: 0.535ms
2022-03-07 20:53:23 - until epoch: 114, best_acc1: 61.792%
2022-03-07 20:53:23 - epoch 115 lr: 0.04078923629479943
2022-03-07 20:54:02 - train: epoch 0115, iter [00100, 05004], lr: 0.040789, loss: 2.5143
2022-03-07 20:54:36 - train: epoch 0115, iter [00200, 05004], lr: 0.040789, loss: 2.6420
2022-03-07 20:55:10 - train: epoch 0115, iter [00300, 05004], lr: 0.040789, loss: 2.9859
2022-03-07 20:55:44 - train: epoch 0115, iter [00400, 05004], lr: 0.040789, loss: 2.6166
2022-03-07 20:56:18 - train: epoch 0115, iter [00500, 05004], lr: 0.040789, loss: 2.7009
2022-03-07 20:56:53 - train: epoch 0115, iter [00600, 05004], lr: 0.040789, loss: 2.7081
2022-03-07 20:57:27 - train: epoch 0115, iter [00700, 05004], lr: 0.040789, loss: 2.5673
2022-03-07 20:58:02 - train: epoch 0115, iter [00800, 05004], lr: 0.040789, loss: 2.6144
2022-03-07 20:58:36 - train: epoch 0115, iter [00900, 05004], lr: 0.040789, loss: 2.6149
2022-03-07 20:59:10 - train: epoch 0115, iter [01000, 05004], lr: 0.040789, loss: 2.3879
2022-03-07 20:59:45 - train: epoch 0115, iter [01100, 05004], lr: 0.040789, loss: 2.4072
2022-03-07 21:00:18 - train: epoch 0115, iter [01200, 05004], lr: 0.040789, loss: 2.7944
2022-03-07 21:00:53 - train: epoch 0115, iter [01300, 05004], lr: 0.040789, loss: 2.6183
2022-03-07 21:01:27 - train: epoch 0115, iter [01400, 05004], lr: 0.040789, loss: 2.4871
2022-03-07 21:02:01 - train: epoch 0115, iter [01500, 05004], lr: 0.040789, loss: 2.7388
2022-03-07 21:02:35 - train: epoch 0115, iter [01600, 05004], lr: 0.040789, loss: 2.5916
2022-03-07 21:03:09 - train: epoch 0115, iter [01700, 05004], lr: 0.040789, loss: 2.6047
2022-03-07 21:03:43 - train: epoch 0115, iter [01800, 05004], lr: 0.040789, loss: 2.7240
2022-03-07 21:04:17 - train: epoch 0115, iter [01900, 05004], lr: 0.040789, loss: 2.5974
2022-03-07 21:04:51 - train: epoch 0115, iter [02000, 05004], lr: 0.040789, loss: 2.7213
2022-03-07 21:05:26 - train: epoch 0115, iter [02100, 05004], lr: 0.040789, loss: 2.6584
2022-03-07 21:06:00 - train: epoch 0115, iter [02200, 05004], lr: 0.040789, loss: 2.8141
2022-03-07 21:06:34 - train: epoch 0115, iter [02300, 05004], lr: 0.040789, loss: 2.8757
2022-03-07 21:07:08 - train: epoch 0115, iter [02400, 05004], lr: 0.040789, loss: 2.8276
2022-03-07 21:07:43 - train: epoch 0115, iter [02500, 05004], lr: 0.040789, loss: 2.5995
2022-03-07 21:08:17 - train: epoch 0115, iter [02600, 05004], lr: 0.040789, loss: 2.5004
2022-03-07 21:08:51 - train: epoch 0115, iter [02700, 05004], lr: 0.040789, loss: 2.3452
2022-03-07 21:09:25 - train: epoch 0115, iter [02800, 05004], lr: 0.040789, loss: 2.7035
2022-03-07 21:09:59 - train: epoch 0115, iter [02900, 05004], lr: 0.040789, loss: 2.7287
2022-03-07 21:10:35 - train: epoch 0115, iter [03000, 05004], lr: 0.040789, loss: 2.3501
2022-03-07 21:11:09 - train: epoch 0115, iter [03100, 05004], lr: 0.040789, loss: 2.7874
2022-03-07 21:11:42 - train: epoch 0115, iter [03200, 05004], lr: 0.040789, loss: 2.5890
2022-03-07 21:12:17 - train: epoch 0115, iter [03300, 05004], lr: 0.040789, loss: 2.8943
2022-03-07 21:12:52 - train: epoch 0115, iter [03400, 05004], lr: 0.040789, loss: 2.7506
2022-03-07 21:13:25 - train: epoch 0115, iter [03500, 05004], lr: 0.040789, loss: 2.6552
2022-03-07 21:14:00 - train: epoch 0115, iter [03600, 05004], lr: 0.040789, loss: 2.5272
2022-03-07 21:14:34 - train: epoch 0115, iter [03700, 05004], lr: 0.040789, loss: 2.7914
2022-03-07 21:15:08 - train: epoch 0115, iter [03800, 05004], lr: 0.040789, loss: 2.4386
2022-03-07 21:15:43 - train: epoch 0115, iter [03900, 05004], lr: 0.040789, loss: 2.6017
2022-03-07 21:16:16 - train: epoch 0115, iter [04000, 05004], lr: 0.040789, loss: 2.8337
2022-03-07 21:16:51 - train: epoch 0115, iter [04100, 05004], lr: 0.040789, loss: 2.6927
2022-03-07 21:17:25 - train: epoch 0115, iter [04200, 05004], lr: 0.040789, loss: 2.3063
2022-03-07 21:18:00 - train: epoch 0115, iter [04300, 05004], lr: 0.040789, loss: 2.5303
2022-03-07 21:18:34 - train: epoch 0115, iter [04400, 05004], lr: 0.040789, loss: 2.5240
2022-03-07 21:19:08 - train: epoch 0115, iter [04500, 05004], lr: 0.040789, loss: 2.6120
2022-03-07 21:19:42 - train: epoch 0115, iter [04600, 05004], lr: 0.040789, loss: 2.8728
2022-03-07 21:20:16 - train: epoch 0115, iter [04700, 05004], lr: 0.040789, loss: 2.6808
2022-03-07 21:20:51 - train: epoch 0115, iter [04800, 05004], lr: 0.040789, loss: 2.3719
2022-03-07 21:21:26 - train: epoch 0115, iter [04900, 05004], lr: 0.040789, loss: 2.6044
2022-03-07 21:21:58 - train: epoch 0115, iter [05000, 05004], lr: 0.040789, loss: 2.5898
2022-03-07 21:21:59 - train: epoch 115, train_loss: 2.6484
2022-03-07 21:23:14 - eval: epoch: 115, acc1: 61.300%, acc5: 84.780%, test_loss: 1.5988, per_image_load_time: 2.253ms, per_image_inference_time: 0.553ms
2022-03-07 21:23:14 - until epoch: 115, best_acc1: 61.792%
2022-03-07 21:23:14 - epoch 116 lr: 0.03999871531119779
2022-03-07 21:23:54 - train: epoch 0116, iter [00100, 05004], lr: 0.039999, loss: 2.5506
2022-03-07 21:24:28 - train: epoch 0116, iter [00200, 05004], lr: 0.039999, loss: 2.8635
2022-03-07 21:25:02 - train: epoch 0116, iter [00300, 05004], lr: 0.039999, loss: 2.5718
2022-03-07 21:25:35 - train: epoch 0116, iter [00400, 05004], lr: 0.039999, loss: 2.6520
2022-03-07 21:26:10 - train: epoch 0116, iter [00500, 05004], lr: 0.039999, loss: 2.7402
2022-03-07 21:26:45 - train: epoch 0116, iter [00600, 05004], lr: 0.039999, loss: 2.5674
2022-03-07 21:27:18 - train: epoch 0116, iter [00700, 05004], lr: 0.039999, loss: 2.8204
2022-03-07 21:27:52 - train: epoch 0116, iter [00800, 05004], lr: 0.039999, loss: 2.5347
2022-03-07 21:28:27 - train: epoch 0116, iter [00900, 05004], lr: 0.039999, loss: 2.6703
2022-03-07 21:29:00 - train: epoch 0116, iter [01000, 05004], lr: 0.039999, loss: 2.9380
2022-03-07 21:29:34 - train: epoch 0116, iter [01100, 05004], lr: 0.039999, loss: 2.3488
2022-03-07 21:30:09 - train: epoch 0116, iter [01200, 05004], lr: 0.039999, loss: 2.5154
2022-03-07 21:30:43 - train: epoch 0116, iter [01300, 05004], lr: 0.039999, loss: 2.8620
2022-03-07 21:31:17 - train: epoch 0116, iter [01400, 05004], lr: 0.039999, loss: 2.8141
2022-03-07 21:31:51 - train: epoch 0116, iter [01500, 05004], lr: 0.039999, loss: 2.5168
2022-03-07 21:32:26 - train: epoch 0116, iter [01600, 05004], lr: 0.039999, loss: 2.6144
2022-03-07 21:33:00 - train: epoch 0116, iter [01700, 05004], lr: 0.039999, loss: 2.8644
2022-03-07 21:33:34 - train: epoch 0116, iter [01800, 05004], lr: 0.039999, loss: 2.9343
2022-03-07 21:34:08 - train: epoch 0116, iter [01900, 05004], lr: 0.039999, loss: 2.6874
2022-03-07 21:34:42 - train: epoch 0116, iter [02000, 05004], lr: 0.039999, loss: 2.5017
2022-03-07 21:35:17 - train: epoch 0116, iter [02100, 05004], lr: 0.039999, loss: 2.8522
2022-03-07 21:35:52 - train: epoch 0116, iter [02200, 05004], lr: 0.039999, loss: 2.8712
2022-03-07 21:36:25 - train: epoch 0116, iter [02300, 05004], lr: 0.039999, loss: 2.7306
2022-03-07 21:37:01 - train: epoch 0116, iter [02400, 05004], lr: 0.039999, loss: 2.5350
2022-03-07 21:37:34 - train: epoch 0116, iter [02500, 05004], lr: 0.039999, loss: 2.5464
2022-03-07 21:38:08 - train: epoch 0116, iter [02600, 05004], lr: 0.039999, loss: 2.6260
2022-03-07 21:38:43 - train: epoch 0116, iter [02700, 05004], lr: 0.039999, loss: 3.0002
2022-03-07 21:39:16 - train: epoch 0116, iter [02800, 05004], lr: 0.039999, loss: 2.8745
2022-03-07 21:39:51 - train: epoch 0116, iter [02900, 05004], lr: 0.039999, loss: 2.9646
2022-03-07 21:40:25 - train: epoch 0116, iter [03000, 05004], lr: 0.039999, loss: 2.8034
2022-03-07 21:40:58 - train: epoch 0116, iter [03100, 05004], lr: 0.039999, loss: 2.9113
2022-03-07 21:41:33 - train: epoch 0116, iter [03200, 05004], lr: 0.039999, loss: 2.8623
2022-03-07 21:42:07 - train: epoch 0116, iter [03300, 05004], lr: 0.039999, loss: 2.8033
2022-03-07 21:42:41 - train: epoch 0116, iter [03400, 05004], lr: 0.039999, loss: 2.8800
2022-03-07 21:43:15 - train: epoch 0116, iter [03500, 05004], lr: 0.039999, loss: 2.9843
2022-03-07 21:43:49 - train: epoch 0116, iter [03600, 05004], lr: 0.039999, loss: 2.8494
2022-03-07 21:44:24 - train: epoch 0116, iter [03700, 05004], lr: 0.039999, loss: 2.5615
2022-03-07 21:44:57 - train: epoch 0116, iter [03800, 05004], lr: 0.039999, loss: 2.4447
2022-03-07 21:45:31 - train: epoch 0116, iter [03900, 05004], lr: 0.039999, loss: 2.7243
2022-03-07 21:46:05 - train: epoch 0116, iter [04000, 05004], lr: 0.039999, loss: 2.6511
2022-03-07 21:46:39 - train: epoch 0116, iter [04100, 05004], lr: 0.039999, loss: 2.7330
2022-03-07 21:47:14 - train: epoch 0116, iter [04200, 05004], lr: 0.039999, loss: 2.4677
2022-03-07 21:47:47 - train: epoch 0116, iter [04300, 05004], lr: 0.039999, loss: 2.7337
2022-03-07 21:48:22 - train: epoch 0116, iter [04400, 05004], lr: 0.039999, loss: 2.5076
2022-03-07 21:48:55 - train: epoch 0116, iter [04500, 05004], lr: 0.039999, loss: 2.6006
2022-03-07 21:49:29 - train: epoch 0116, iter [04600, 05004], lr: 0.039999, loss: 2.7265
2022-03-07 21:50:03 - train: epoch 0116, iter [04700, 05004], lr: 0.039999, loss: 2.4114
2022-03-07 21:50:38 - train: epoch 0116, iter [04800, 05004], lr: 0.039999, loss: 2.6003
2022-03-07 21:51:11 - train: epoch 0116, iter [04900, 05004], lr: 0.039999, loss: 2.3233
2022-03-07 21:51:44 - train: epoch 0116, iter [05000, 05004], lr: 0.039999, loss: 2.6467
2022-03-07 21:51:45 - train: epoch 116, train_loss: 2.6435
2022-03-07 21:53:00 - eval: epoch: 116, acc1: 61.812%, acc5: 84.650%, test_loss: 1.5882, per_image_load_time: 1.446ms, per_image_inference_time: 0.540ms
2022-03-07 21:53:01 - until epoch: 116, best_acc1: 61.812%
2022-03-07 21:53:01 - epoch 117 lr: 0.0392107901616097
2022-03-07 21:53:39 - train: epoch 0117, iter [00100, 05004], lr: 0.039211, loss: 2.7189
2022-03-07 21:54:14 - train: epoch 0117, iter [00200, 05004], lr: 0.039211, loss: 2.4727
2022-03-07 21:54:48 - train: epoch 0117, iter [00300, 05004], lr: 0.039211, loss: 2.8232
2022-03-07 21:55:21 - train: epoch 0117, iter [00400, 05004], lr: 0.039211, loss: 2.4137
2022-03-07 21:55:56 - train: epoch 0117, iter [00500, 05004], lr: 0.039211, loss: 2.4838
2022-03-07 21:56:31 - train: epoch 0117, iter [00600, 05004], lr: 0.039211, loss: 2.9646
2022-03-07 21:57:05 - train: epoch 0117, iter [00700, 05004], lr: 0.039211, loss: 2.6766
2022-03-07 21:57:39 - train: epoch 0117, iter [00800, 05004], lr: 0.039211, loss: 2.3794
2022-03-07 21:58:13 - train: epoch 0117, iter [00900, 05004], lr: 0.039211, loss: 2.2537
2022-03-07 21:58:47 - train: epoch 0117, iter [01000, 05004], lr: 0.039211, loss: 2.3257
2022-03-07 21:59:21 - train: epoch 0117, iter [01100, 05004], lr: 0.039211, loss: 2.7057
2022-03-07 21:59:56 - train: epoch 0117, iter [01200, 05004], lr: 0.039211, loss: 2.5404
2022-03-07 22:00:30 - train: epoch 0117, iter [01300, 05004], lr: 0.039211, loss: 2.5871
2022-03-07 22:01:04 - train: epoch 0117, iter [01400, 05004], lr: 0.039211, loss: 2.8718
2022-03-07 22:01:38 - train: epoch 0117, iter [01500, 05004], lr: 0.039211, loss: 2.6946
2022-03-07 22:02:12 - train: epoch 0117, iter [01600, 05004], lr: 0.039211, loss: 2.5371
2022-03-07 22:02:46 - train: epoch 0117, iter [01700, 05004], lr: 0.039211, loss: 2.7636
2022-03-07 22:03:21 - train: epoch 0117, iter [01800, 05004], lr: 0.039211, loss: 2.8001
2022-03-07 22:03:55 - train: epoch 0117, iter [01900, 05004], lr: 0.039211, loss: 2.5797
2022-03-07 22:04:28 - train: epoch 0117, iter [02000, 05004], lr: 0.039211, loss: 2.6463
2022-03-07 22:05:03 - train: epoch 0117, iter [02100, 05004], lr: 0.039211, loss: 2.5259
2022-03-07 22:05:37 - train: epoch 0117, iter [02200, 05004], lr: 0.039211, loss: 2.2003
2022-03-07 22:06:12 - train: epoch 0117, iter [02300, 05004], lr: 0.039211, loss: 2.5610
2022-03-07 22:06:46 - train: epoch 0117, iter [02400, 05004], lr: 0.039211, loss: 2.4993
2022-03-07 22:07:21 - train: epoch 0117, iter [02500, 05004], lr: 0.039211, loss: 2.7070
2022-03-07 22:07:54 - train: epoch 0117, iter [02600, 05004], lr: 0.039211, loss: 2.5982
2022-03-07 22:08:29 - train: epoch 0117, iter [02700, 05004], lr: 0.039211, loss: 2.5109
2022-03-07 22:09:03 - train: epoch 0117, iter [02800, 05004], lr: 0.039211, loss: 2.3881
2022-03-07 22:09:37 - train: epoch 0117, iter [02900, 05004], lr: 0.039211, loss: 2.5306
2022-03-07 22:10:11 - train: epoch 0117, iter [03000, 05004], lr: 0.039211, loss: 2.3393
2022-03-07 22:10:45 - train: epoch 0117, iter [03100, 05004], lr: 0.039211, loss: 2.6972
2022-03-07 22:11:19 - train: epoch 0117, iter [03200, 05004], lr: 0.039211, loss: 2.5184
2022-03-07 22:11:54 - train: epoch 0117, iter [03300, 05004], lr: 0.039211, loss: 2.4515
2022-03-07 22:12:28 - train: epoch 0117, iter [03400, 05004], lr: 0.039211, loss: 2.5651
2022-03-07 22:13:02 - train: epoch 0117, iter [03500, 05004], lr: 0.039211, loss: 2.7232
2022-03-07 22:13:36 - train: epoch 0117, iter [03600, 05004], lr: 0.039211, loss: 2.6916
2022-03-07 22:14:11 - train: epoch 0117, iter [03700, 05004], lr: 0.039211, loss: 2.2590
2022-03-07 22:14:45 - train: epoch 0117, iter [03800, 05004], lr: 0.039211, loss: 2.5767
2022-03-07 22:15:20 - train: epoch 0117, iter [03900, 05004], lr: 0.039211, loss: 2.5953
2022-03-07 22:15:54 - train: epoch 0117, iter [04000, 05004], lr: 0.039211, loss: 2.5817
2022-03-07 22:16:29 - train: epoch 0117, iter [04100, 05004], lr: 0.039211, loss: 2.6868
2022-03-07 22:17:03 - train: epoch 0117, iter [04200, 05004], lr: 0.039211, loss: 2.6852
2022-03-07 22:17:37 - train: epoch 0117, iter [04300, 05004], lr: 0.039211, loss: 2.4345
2022-03-07 22:18:12 - train: epoch 0117, iter [04400, 05004], lr: 0.039211, loss: 2.5921
2022-03-07 22:18:46 - train: epoch 0117, iter [04500, 05004], lr: 0.039211, loss: 2.4156
2022-03-07 22:19:21 - train: epoch 0117, iter [04600, 05004], lr: 0.039211, loss: 2.5549
2022-03-07 22:19:55 - train: epoch 0117, iter [04700, 05004], lr: 0.039211, loss: 2.8150
2022-03-07 22:20:29 - train: epoch 0117, iter [04800, 05004], lr: 0.039211, loss: 2.7657
2022-03-07 22:21:04 - train: epoch 0117, iter [04900, 05004], lr: 0.039211, loss: 2.9722
2022-03-07 22:21:37 - train: epoch 0117, iter [05000, 05004], lr: 0.039211, loss: 2.7043
2022-03-07 22:21:38 - train: epoch 117, train_loss: 2.6301
2022-03-07 22:22:52 - eval: epoch: 117, acc1: 62.788%, acc5: 85.698%, test_loss: 1.5310, per_image_load_time: 1.025ms, per_image_inference_time: 0.524ms
2022-03-07 22:22:53 - until epoch: 117, best_acc1: 62.788%
2022-03-07 22:22:53 - epoch 118 lr: 0.03842566535205286
2022-03-07 22:23:33 - train: epoch 0118, iter [00100, 05004], lr: 0.038426, loss: 2.3921
2022-03-07 22:24:08 - train: epoch 0118, iter [00200, 05004], lr: 0.038426, loss: 2.4088
2022-03-07 22:24:41 - train: epoch 0118, iter [00300, 05004], lr: 0.038426, loss: 2.6783
2022-03-07 22:25:15 - train: epoch 0118, iter [00400, 05004], lr: 0.038426, loss: 2.5339
2022-03-07 22:25:50 - train: epoch 0118, iter [00500, 05004], lr: 0.038426, loss: 2.4722
2022-03-07 22:26:25 - train: epoch 0118, iter [00600, 05004], lr: 0.038426, loss: 2.3996
2022-03-07 22:26:58 - train: epoch 0118, iter [00700, 05004], lr: 0.038426, loss: 2.5812
2022-03-07 22:27:32 - train: epoch 0118, iter [00800, 05004], lr: 0.038426, loss: 2.7412
2022-03-07 22:28:07 - train: epoch 0118, iter [00900, 05004], lr: 0.038426, loss: 2.5615
2022-03-07 22:28:41 - train: epoch 0118, iter [01000, 05004], lr: 0.038426, loss: 2.9770
2022-03-07 22:29:16 - train: epoch 0118, iter [01100, 05004], lr: 0.038426, loss: 2.7891
2022-03-07 22:29:49 - train: epoch 0118, iter [01200, 05004], lr: 0.038426, loss: 3.1181
2022-03-07 22:30:24 - train: epoch 0118, iter [01300, 05004], lr: 0.038426, loss: 2.5544
2022-03-07 22:30:58 - train: epoch 0118, iter [01400, 05004], lr: 0.038426, loss: 2.4909
2022-03-07 22:31:33 - train: epoch 0118, iter [01500, 05004], lr: 0.038426, loss: 2.5373
2022-03-07 22:32:07 - train: epoch 0118, iter [01600, 05004], lr: 0.038426, loss: 2.6336
2022-03-07 22:32:41 - train: epoch 0118, iter [01700, 05004], lr: 0.038426, loss: 2.7489
2022-03-07 22:33:15 - train: epoch 0118, iter [01800, 05004], lr: 0.038426, loss: 2.4439
2022-03-07 22:33:50 - train: epoch 0118, iter [01900, 05004], lr: 0.038426, loss: 2.7406
2022-03-07 22:34:24 - train: epoch 0118, iter [02000, 05004], lr: 0.038426, loss: 2.5357
2022-03-07 22:34:59 - train: epoch 0118, iter [02100, 05004], lr: 0.038426, loss: 2.5576
2022-03-07 22:35:33 - train: epoch 0118, iter [02200, 05004], lr: 0.038426, loss: 2.3421
2022-03-07 22:36:08 - train: epoch 0118, iter [02300, 05004], lr: 0.038426, loss: 2.4377
2022-03-07 22:36:42 - train: epoch 0118, iter [02400, 05004], lr: 0.038426, loss: 2.9214
2022-03-07 22:37:15 - train: epoch 0118, iter [02500, 05004], lr: 0.038426, loss: 2.9601
2022-03-07 22:37:50 - train: epoch 0118, iter [02600, 05004], lr: 0.038426, loss: 2.3953
2022-03-07 22:38:25 - train: epoch 0118, iter [02700, 05004], lr: 0.038426, loss: 2.6791
2022-03-07 22:38:59 - train: epoch 0118, iter [02800, 05004], lr: 0.038426, loss: 2.5740
2022-03-07 22:39:33 - train: epoch 0118, iter [02900, 05004], lr: 0.038426, loss: 2.7598
2022-03-07 22:40:07 - train: epoch 0118, iter [03000, 05004], lr: 0.038426, loss: 2.6020
2022-03-07 22:40:41 - train: epoch 0118, iter [03100, 05004], lr: 0.038426, loss: 2.6400
2022-03-07 22:41:16 - train: epoch 0118, iter [03200, 05004], lr: 0.038426, loss: 2.6717
2022-03-07 22:41:50 - train: epoch 0118, iter [03300, 05004], lr: 0.038426, loss: 2.7651
2022-03-07 22:42:24 - train: epoch 0118, iter [03400, 05004], lr: 0.038426, loss: 2.4850
2022-03-07 22:42:58 - train: epoch 0118, iter [03500, 05004], lr: 0.038426, loss: 2.5804
2022-03-07 22:43:32 - train: epoch 0118, iter [03600, 05004], lr: 0.038426, loss: 2.6967
2022-03-07 22:44:06 - train: epoch 0118, iter [03700, 05004], lr: 0.038426, loss: 2.6839
2022-03-07 22:44:41 - train: epoch 0118, iter [03800, 05004], lr: 0.038426, loss: 2.7279
2022-03-07 22:45:15 - train: epoch 0118, iter [03900, 05004], lr: 0.038426, loss: 2.9753
2022-03-07 22:45:49 - train: epoch 0118, iter [04000, 05004], lr: 0.038426, loss: 2.7409
2022-03-07 22:46:22 - train: epoch 0118, iter [04100, 05004], lr: 0.038426, loss: 2.7695
2022-03-07 22:46:57 - train: epoch 0118, iter [04200, 05004], lr: 0.038426, loss: 2.5420
2022-03-07 22:47:31 - train: epoch 0118, iter [04300, 05004], lr: 0.038426, loss: 2.6043
2022-03-07 22:48:05 - train: epoch 0118, iter [04400, 05004], lr: 0.038426, loss: 2.8814
2022-03-07 22:48:39 - train: epoch 0118, iter [04500, 05004], lr: 0.038426, loss: 2.6744
2022-03-07 22:49:13 - train: epoch 0118, iter [04600, 05004], lr: 0.038426, loss: 2.8239
2022-03-07 22:49:47 - train: epoch 0118, iter [04700, 05004], lr: 0.038426, loss: 2.3944
2022-03-07 22:50:22 - train: epoch 0118, iter [04800, 05004], lr: 0.038426, loss: 2.5464
2022-03-07 22:50:56 - train: epoch 0118, iter [04900, 05004], lr: 0.038426, loss: 2.6306
2022-03-07 22:51:28 - train: epoch 0118, iter [05000, 05004], lr: 0.038426, loss: 2.5588
2022-03-07 22:51:29 - train: epoch 118, train_loss: 2.6218
2022-03-07 22:52:44 - eval: epoch: 118, acc1: 61.236%, acc5: 84.280%, test_loss: 1.6060, per_image_load_time: 1.635ms, per_image_inference_time: 0.530ms
2022-03-07 22:52:44 - until epoch: 118, best_acc1: 62.788%
2022-03-07 22:52:44 - epoch 119 lr: 0.037643544661716516
2022-03-07 22:53:23 - train: epoch 0119, iter [00100, 05004], lr: 0.037644, loss: 2.8018
2022-03-07 22:53:57 - train: epoch 0119, iter [00200, 05004], lr: 0.037644, loss: 2.5203
2022-03-07 22:54:32 - train: epoch 0119, iter [00300, 05004], lr: 0.037644, loss: 2.8057
2022-03-07 22:55:06 - train: epoch 0119, iter [00400, 05004], lr: 0.037644, loss: 2.4142
2022-03-07 22:55:40 - train: epoch 0119, iter [00500, 05004], lr: 0.037644, loss: 2.4251
2022-03-07 22:56:14 - train: epoch 0119, iter [00600, 05004], lr: 0.037644, loss: 2.6292
2022-03-07 22:56:48 - train: epoch 0119, iter [00700, 05004], lr: 0.037644, loss: 2.7065
2022-03-07 22:57:22 - train: epoch 0119, iter [00800, 05004], lr: 0.037644, loss: 2.7938
2022-03-07 22:57:56 - train: epoch 0119, iter [00900, 05004], lr: 0.037644, loss: 2.5827
2022-03-07 22:58:31 - train: epoch 0119, iter [01000, 05004], lr: 0.037644, loss: 2.5965
2022-03-07 22:59:05 - train: epoch 0119, iter [01100, 05004], lr: 0.037644, loss: 2.6168
2022-03-07 22:59:39 - train: epoch 0119, iter [01200, 05004], lr: 0.037644, loss: 2.6501
2022-03-07 23:00:13 - train: epoch 0119, iter [01300, 05004], lr: 0.037644, loss: 2.5908
2022-03-07 23:00:48 - train: epoch 0119, iter [01400, 05004], lr: 0.037644, loss: 2.7535
2022-03-07 23:01:23 - train: epoch 0119, iter [01500, 05004], lr: 0.037644, loss: 2.8483
2022-03-07 23:01:56 - train: epoch 0119, iter [01600, 05004], lr: 0.037644, loss: 2.6898
2022-03-07 23:02:31 - train: epoch 0119, iter [01700, 05004], lr: 0.037644, loss: 2.6571
2022-03-07 23:03:05 - train: epoch 0119, iter [01800, 05004], lr: 0.037644, loss: 2.6198
2022-03-07 23:03:39 - train: epoch 0119, iter [01900, 05004], lr: 0.037644, loss: 2.5511
2022-03-07 23:04:13 - train: epoch 0119, iter [02000, 05004], lr: 0.037644, loss: 2.4662
2022-03-07 23:04:49 - train: epoch 0119, iter [02100, 05004], lr: 0.037644, loss: 2.7367
2022-03-07 23:05:22 - train: epoch 0119, iter [02200, 05004], lr: 0.037644, loss: 2.6620
2022-03-07 23:05:57 - train: epoch 0119, iter [02300, 05004], lr: 0.037644, loss: 2.5860
2022-03-07 23:06:31 - train: epoch 0119, iter [02400, 05004], lr: 0.037644, loss: 2.5130
2022-03-07 23:07:05 - train: epoch 0119, iter [02500, 05004], lr: 0.037644, loss: 2.8577
2022-03-07 23:07:39 - train: epoch 0119, iter [02600, 05004], lr: 0.037644, loss: 2.3811
2022-03-07 23:08:14 - train: epoch 0119, iter [02700, 05004], lr: 0.037644, loss: 2.8327
2022-03-07 23:08:49 - train: epoch 0119, iter [02800, 05004], lr: 0.037644, loss: 2.8269
2022-03-07 23:09:22 - train: epoch 0119, iter [02900, 05004], lr: 0.037644, loss: 2.7629
2022-03-07 23:09:56 - train: epoch 0119, iter [03000, 05004], lr: 0.037644, loss: 2.3506
2022-03-07 23:10:32 - train: epoch 0119, iter [03100, 05004], lr: 0.037644, loss: 2.4972
2022-03-07 23:11:05 - train: epoch 0119, iter [03200, 05004], lr: 0.037644, loss: 2.4386
2022-03-07 23:11:40 - train: epoch 0119, iter [03300, 05004], lr: 0.037644, loss: 2.2499
2022-03-07 23:12:15 - train: epoch 0119, iter [03400, 05004], lr: 0.037644, loss: 2.8357
2022-03-07 23:12:49 - train: epoch 0119, iter [03500, 05004], lr: 0.037644, loss: 2.6991
2022-03-07 23:13:22 - train: epoch 0119, iter [03600, 05004], lr: 0.037644, loss: 2.9056
2022-03-07 23:13:57 - train: epoch 0119, iter [03700, 05004], lr: 0.037644, loss: 2.5732
2022-03-07 23:14:30 - train: epoch 0119, iter [03800, 05004], lr: 0.037644, loss: 2.7386
2022-03-07 23:15:04 - train: epoch 0119, iter [03900, 05004], lr: 0.037644, loss: 2.8263
2022-03-07 23:15:37 - train: epoch 0119, iter [04000, 05004], lr: 0.037644, loss: 2.4564
2022-03-07 23:16:12 - train: epoch 0119, iter [04100, 05004], lr: 0.037644, loss: 2.8607
2022-03-07 23:16:47 - train: epoch 0119, iter [04200, 05004], lr: 0.037644, loss: 2.6969
2022-03-07 23:17:22 - train: epoch 0119, iter [04300, 05004], lr: 0.037644, loss: 2.5982
2022-03-07 23:17:56 - train: epoch 0119, iter [04400, 05004], lr: 0.037644, loss: 2.7322
2022-03-07 23:18:31 - train: epoch 0119, iter [04500, 05004], lr: 0.037644, loss: 2.5438
2022-03-07 23:19:04 - train: epoch 0119, iter [04600, 05004], lr: 0.037644, loss: 2.5177
2022-03-07 23:19:38 - train: epoch 0119, iter [04700, 05004], lr: 0.037644, loss: 2.7446
2022-03-07 23:20:13 - train: epoch 0119, iter [04800, 05004], lr: 0.037644, loss: 2.6642
2022-03-07 23:20:47 - train: epoch 0119, iter [04900, 05004], lr: 0.037644, loss: 2.9776
2022-03-07 23:21:20 - train: epoch 0119, iter [05000, 05004], lr: 0.037644, loss: 2.7060
2022-03-07 23:21:21 - train: epoch 119, train_loss: 2.6121
2022-03-07 23:22:35 - eval: epoch: 119, acc1: 61.468%, acc5: 84.570%, test_loss: 1.6058, per_image_load_time: 0.642ms, per_image_inference_time: 0.504ms
2022-03-07 23:22:36 - until epoch: 119, best_acc1: 62.788%
2022-03-07 23:22:36 - epoch 120 lr: 0.036864631090070654
2022-03-07 23:23:15 - train: epoch 0120, iter [00100, 05004], lr: 0.036865, loss: 2.7229
2022-03-07 23:23:49 - train: epoch 0120, iter [00200, 05004], lr: 0.036865, loss: 2.6241
2022-03-07 23:24:24 - train: epoch 0120, iter [00300, 05004], lr: 0.036865, loss: 2.6301
2022-03-07 23:24:57 - train: epoch 0120, iter [00400, 05004], lr: 0.036865, loss: 2.4889
2022-03-07 23:25:32 - train: epoch 0120, iter [00500, 05004], lr: 0.036865, loss: 2.4599
2022-03-07 23:26:06 - train: epoch 0120, iter [00600, 05004], lr: 0.036865, loss: 2.4034
2022-03-07 23:26:40 - train: epoch 0120, iter [00700, 05004], lr: 0.036865, loss: 2.8528
2022-03-07 23:27:14 - train: epoch 0120, iter [00800, 05004], lr: 0.036865, loss: 2.8053
2022-03-07 23:27:48 - train: epoch 0120, iter [00900, 05004], lr: 0.036865, loss: 2.6074
2022-03-07 23:28:22 - train: epoch 0120, iter [01000, 05004], lr: 0.036865, loss: 2.7328
2022-03-07 23:28:57 - train: epoch 0120, iter [01100, 05004], lr: 0.036865, loss: 2.6058
2022-03-07 23:29:31 - train: epoch 0120, iter [01200, 05004], lr: 0.036865, loss: 2.7707
2022-03-07 23:30:05 - train: epoch 0120, iter [01300, 05004], lr: 0.036865, loss: 2.3679
2022-03-07 23:30:40 - train: epoch 0120, iter [01400, 05004], lr: 0.036865, loss: 2.7442
2022-03-07 23:31:13 - train: epoch 0120, iter [01500, 05004], lr: 0.036865, loss: 2.5012
2022-03-07 23:31:47 - train: epoch 0120, iter [01600, 05004], lr: 0.036865, loss: 2.5304
2022-03-07 23:32:22 - train: epoch 0120, iter [01700, 05004], lr: 0.036865, loss: 2.4311
2022-03-07 23:32:56 - train: epoch 0120, iter [01800, 05004], lr: 0.036865, loss: 2.2830
2022-03-07 23:33:29 - train: epoch 0120, iter [01900, 05004], lr: 0.036865, loss: 2.7898
2022-03-07 23:34:03 - train: epoch 0120, iter [02000, 05004], lr: 0.036865, loss: 2.5827
2022-03-07 23:34:37 - train: epoch 0120, iter [02100, 05004], lr: 0.036865, loss: 2.7991
2022-03-07 23:35:11 - train: epoch 0120, iter [02200, 05004], lr: 0.036865, loss: 2.5729
2022-03-07 23:35:44 - train: epoch 0120, iter [02300, 05004], lr: 0.036865, loss: 2.5058
2022-03-07 23:36:19 - train: epoch 0120, iter [02400, 05004], lr: 0.036865, loss: 2.7139
2022-03-07 23:36:53 - train: epoch 0120, iter [02500, 05004], lr: 0.036865, loss: 2.5669
2022-03-07 23:37:27 - train: epoch 0120, iter [02600, 05004], lr: 0.036865, loss: 2.6549
2022-03-07 23:38:02 - train: epoch 0120, iter [02700, 05004], lr: 0.036865, loss: 2.5720
2022-03-07 23:38:35 - train: epoch 0120, iter [02800, 05004], lr: 0.036865, loss: 2.6091
2022-03-07 23:39:09 - train: epoch 0120, iter [02900, 05004], lr: 0.036865, loss: 2.3008
2022-03-07 23:39:44 - train: epoch 0120, iter [03000, 05004], lr: 0.036865, loss: 2.6657
2022-03-07 23:40:18 - train: epoch 0120, iter [03100, 05004], lr: 0.036865, loss: 2.6808
2022-03-07 23:40:52 - train: epoch 0120, iter [03200, 05004], lr: 0.036865, loss: 2.8024
2022-03-07 23:41:25 - train: epoch 0120, iter [03300, 05004], lr: 0.036865, loss: 2.4649
2022-03-07 23:42:00 - train: epoch 0120, iter [03400, 05004], lr: 0.036865, loss: 2.5867
2022-03-07 23:42:34 - train: epoch 0120, iter [03500, 05004], lr: 0.036865, loss: 2.7527
2022-03-07 23:43:09 - train: epoch 0120, iter [03600, 05004], lr: 0.036865, loss: 2.6970
2022-03-07 23:43:41 - train: epoch 0120, iter [03700, 05004], lr: 0.036865, loss: 2.7652
2022-03-07 23:44:16 - train: epoch 0120, iter [03800, 05004], lr: 0.036865, loss: 2.5898
2022-03-07 23:44:50 - train: epoch 0120, iter [03900, 05004], lr: 0.036865, loss: 2.6198
2022-03-07 23:45:25 - train: epoch 0120, iter [04000, 05004], lr: 0.036865, loss: 2.5014
2022-03-07 23:45:59 - train: epoch 0120, iter [04100, 05004], lr: 0.036865, loss: 2.6773
2022-03-07 23:46:33 - train: epoch 0120, iter [04200, 05004], lr: 0.036865, loss: 2.5945
2022-03-07 23:47:07 - train: epoch 0120, iter [04300, 05004], lr: 0.036865, loss: 2.9610
2022-03-07 23:47:41 - train: epoch 0120, iter [04400, 05004], lr: 0.036865, loss: 2.6010
2022-03-07 23:48:15 - train: epoch 0120, iter [04500, 05004], lr: 0.036865, loss: 2.7547
2022-03-07 23:48:49 - train: epoch 0120, iter [04600, 05004], lr: 0.036865, loss: 2.3152
2022-03-07 23:49:23 - train: epoch 0120, iter [04700, 05004], lr: 0.036865, loss: 2.6754
2022-03-07 23:49:57 - train: epoch 0120, iter [04800, 05004], lr: 0.036865, loss: 2.8529
2022-03-07 23:50:31 - train: epoch 0120, iter [04900, 05004], lr: 0.036865, loss: 2.9400
2022-03-07 23:51:04 - train: epoch 0120, iter [05000, 05004], lr: 0.036865, loss: 2.4883
2022-03-07 23:51:05 - train: epoch 120, train_loss: 2.6064
2022-03-07 23:52:19 - eval: epoch: 120, acc1: 61.244%, acc5: 84.312%, test_loss: 1.6099, per_image_load_time: 2.183ms, per_image_inference_time: 0.495ms
2022-03-07 23:52:19 - until epoch: 120, best_acc1: 62.788%
2022-03-07 23:52:19 - epoch 121 lr: 0.03608912680417737
2022-03-07 23:52:58 - train: epoch 0121, iter [00100, 05004], lr: 0.036089, loss: 2.6069
2022-03-07 23:53:33 - train: epoch 0121, iter [00200, 05004], lr: 0.036089, loss: 2.7442
2022-03-07 23:54:07 - train: epoch 0121, iter [00300, 05004], lr: 0.036089, loss: 2.8214
2022-03-07 23:54:41 - train: epoch 0121, iter [00400, 05004], lr: 0.036089, loss: 2.3780
2022-03-07 23:55:15 - train: epoch 0121, iter [00500, 05004], lr: 0.036089, loss: 2.6898
2022-03-07 23:55:48 - train: epoch 0121, iter [00600, 05004], lr: 0.036089, loss: 2.6410
2022-03-07 23:56:22 - train: epoch 0121, iter [00700, 05004], lr: 0.036089, loss: 2.5690
2022-03-07 23:56:56 - train: epoch 0121, iter [00800, 05004], lr: 0.036089, loss: 2.8543
2022-03-07 23:57:30 - train: epoch 0121, iter [00900, 05004], lr: 0.036089, loss: 2.5909
2022-03-07 23:58:04 - train: epoch 0121, iter [01000, 05004], lr: 0.036089, loss: 2.7637
2022-03-07 23:58:39 - train: epoch 0121, iter [01100, 05004], lr: 0.036089, loss: 2.4071
2022-03-07 23:59:13 - train: epoch 0121, iter [01200, 05004], lr: 0.036089, loss: 2.7690
2022-03-07 23:59:47 - train: epoch 0121, iter [01300, 05004], lr: 0.036089, loss: 2.5881
2022-03-08 00:00:21 - train: epoch 0121, iter [01400, 05004], lr: 0.036089, loss: 2.4205
2022-03-08 00:00:55 - train: epoch 0121, iter [01500, 05004], lr: 0.036089, loss: 2.5219
2022-03-08 00:01:30 - train: epoch 0121, iter [01600, 05004], lr: 0.036089, loss: 2.6895
2022-03-08 00:02:03 - train: epoch 0121, iter [01700, 05004], lr: 0.036089, loss: 2.6399
2022-03-08 00:02:37 - train: epoch 0121, iter [01800, 05004], lr: 0.036089, loss: 2.7398
2022-03-08 00:03:13 - train: epoch 0121, iter [01900, 05004], lr: 0.036089, loss: 2.5255
2022-03-08 00:03:45 - train: epoch 0121, iter [02000, 05004], lr: 0.036089, loss: 2.5526
2022-03-08 00:04:20 - train: epoch 0121, iter [02100, 05004], lr: 0.036089, loss: 2.6228
2022-03-08 00:04:54 - train: epoch 0121, iter [02200, 05004], lr: 0.036089, loss: 2.2141
2022-03-08 00:05:27 - train: epoch 0121, iter [02300, 05004], lr: 0.036089, loss: 2.6140
2022-03-08 00:06:01 - train: epoch 0121, iter [02400, 05004], lr: 0.036089, loss: 2.5472
2022-03-08 00:06:36 - train: epoch 0121, iter [02500, 05004], lr: 0.036089, loss: 2.5127
2022-03-08 00:07:10 - train: epoch 0121, iter [02600, 05004], lr: 0.036089, loss: 2.6122
2022-03-08 00:07:43 - train: epoch 0121, iter [02700, 05004], lr: 0.036089, loss: 2.6717
2022-03-08 00:08:18 - train: epoch 0121, iter [02800, 05004], lr: 0.036089, loss: 3.1521
2022-03-08 00:08:51 - train: epoch 0121, iter [02900, 05004], lr: 0.036089, loss: 2.7931
2022-03-08 00:09:26 - train: epoch 0121, iter [03000, 05004], lr: 0.036089, loss: 2.7498
2022-03-08 00:09:59 - train: epoch 0121, iter [03100, 05004], lr: 0.036089, loss: 2.4098
2022-03-08 00:10:33 - train: epoch 0121, iter [03200, 05004], lr: 0.036089, loss: 2.5890
2022-03-08 00:11:07 - train: epoch 0121, iter [03300, 05004], lr: 0.036089, loss: 2.5533
2022-03-08 00:11:40 - train: epoch 0121, iter [03400, 05004], lr: 0.036089, loss: 2.6335
2022-03-08 00:12:16 - train: epoch 0121, iter [03500, 05004], lr: 0.036089, loss: 2.3952
2022-03-08 00:12:49 - train: epoch 0121, iter [03600, 05004], lr: 0.036089, loss: 2.7676
2022-03-08 00:13:24 - train: epoch 0121, iter [03700, 05004], lr: 0.036089, loss: 2.6073
2022-03-08 00:13:57 - train: epoch 0121, iter [03800, 05004], lr: 0.036089, loss: 2.4627
2022-03-08 00:14:33 - train: epoch 0121, iter [03900, 05004], lr: 0.036089, loss: 2.2880
2022-03-08 00:15:06 - train: epoch 0121, iter [04000, 05004], lr: 0.036089, loss: 2.5564
2022-03-08 00:15:41 - train: epoch 0121, iter [04100, 05004], lr: 0.036089, loss: 2.5623
2022-03-08 00:16:13 - train: epoch 0121, iter [04200, 05004], lr: 0.036089, loss: 2.5662
2022-03-08 00:16:48 - train: epoch 0121, iter [04300, 05004], lr: 0.036089, loss: 2.4865
2022-03-08 00:17:22 - train: epoch 0121, iter [04400, 05004], lr: 0.036089, loss: 2.6913
2022-03-08 00:17:57 - train: epoch 0121, iter [04500, 05004], lr: 0.036089, loss: 2.6559
2022-03-08 00:18:31 - train: epoch 0121, iter [04600, 05004], lr: 0.036089, loss: 2.5611
2022-03-08 00:19:05 - train: epoch 0121, iter [04700, 05004], lr: 0.036089, loss: 2.9258
2022-03-08 00:19:39 - train: epoch 0121, iter [04800, 05004], lr: 0.036089, loss: 2.6732
2022-03-08 00:20:12 - train: epoch 0121, iter [04900, 05004], lr: 0.036089, loss: 2.3010
2022-03-08 00:20:45 - train: epoch 0121, iter [05000, 05004], lr: 0.036089, loss: 2.7158
2022-03-08 00:20:46 - train: epoch 121, train_loss: 2.5967
2022-03-08 00:22:00 - eval: epoch: 121, acc1: 62.682%, acc5: 85.452%, test_loss: 1.5312, per_image_load_time: 0.661ms, per_image_inference_time: 0.519ms
2022-03-08 00:22:01 - until epoch: 121, best_acc1: 62.788%
2022-03-08 00:22:01 - epoch 122 lr: 0.03531723308621847
2022-03-08 00:22:40 - train: epoch 0122, iter [00100, 05004], lr: 0.035317, loss: 2.4167
2022-03-08 00:23:14 - train: epoch 0122, iter [00200, 05004], lr: 0.035317, loss: 2.3303
2022-03-08 00:23:49 - train: epoch 0122, iter [00300, 05004], lr: 0.035317, loss: 2.6364
2022-03-08 00:24:23 - train: epoch 0122, iter [00400, 05004], lr: 0.035317, loss: 2.2632
2022-03-08 00:24:57 - train: epoch 0122, iter [00500, 05004], lr: 0.035317, loss: 2.5583
2022-03-08 00:25:32 - train: epoch 0122, iter [00600, 05004], lr: 0.035317, loss: 2.5587
2022-03-08 00:26:05 - train: epoch 0122, iter [00700, 05004], lr: 0.035317, loss: 2.2828
2022-03-08 00:26:39 - train: epoch 0122, iter [00800, 05004], lr: 0.035317, loss: 2.7746
2022-03-08 00:27:14 - train: epoch 0122, iter [00900, 05004], lr: 0.035317, loss: 2.5315
2022-03-08 00:27:48 - train: epoch 0122, iter [01000, 05004], lr: 0.035317, loss: 2.6655
2022-03-08 00:28:22 - train: epoch 0122, iter [01100, 05004], lr: 0.035317, loss: 2.3748
2022-03-08 00:28:56 - train: epoch 0122, iter [01200, 05004], lr: 0.035317, loss: 2.7420
2022-03-08 00:29:30 - train: epoch 0122, iter [01300, 05004], lr: 0.035317, loss: 2.2734
2022-03-08 00:30:04 - train: epoch 0122, iter [01400, 05004], lr: 0.035317, loss: 2.7882
2022-03-08 00:30:39 - train: epoch 0122, iter [01500, 05004], lr: 0.035317, loss: 2.9865
2022-03-08 00:31:13 - train: epoch 0122, iter [01600, 05004], lr: 0.035317, loss: 2.3932
2022-03-08 00:31:47 - train: epoch 0122, iter [01700, 05004], lr: 0.035317, loss: 2.4510
2022-03-08 00:32:21 - train: epoch 0122, iter [01800, 05004], lr: 0.035317, loss: 2.6576
2022-03-08 00:32:56 - train: epoch 0122, iter [01900, 05004], lr: 0.035317, loss: 2.4997
2022-03-08 00:33:29 - train: epoch 0122, iter [02000, 05004], lr: 0.035317, loss: 2.6203
2022-03-08 00:34:04 - train: epoch 0122, iter [02100, 05004], lr: 0.035317, loss: 2.6629
2022-03-08 00:34:38 - train: epoch 0122, iter [02200, 05004], lr: 0.035317, loss: 2.7028
2022-03-08 00:35:13 - train: epoch 0122, iter [02300, 05004], lr: 0.035317, loss: 2.7308
2022-03-08 00:35:47 - train: epoch 0122, iter [02400, 05004], lr: 0.035317, loss: 2.9530
2022-03-08 00:36:21 - train: epoch 0122, iter [02500, 05004], lr: 0.035317, loss: 2.5093
2022-03-08 00:36:55 - train: epoch 0122, iter [02600, 05004], lr: 0.035317, loss: 2.4845
2022-03-08 00:37:29 - train: epoch 0122, iter [02700, 05004], lr: 0.035317, loss: 2.4326
2022-03-08 00:38:04 - train: epoch 0122, iter [02800, 05004], lr: 0.035317, loss: 2.7041
2022-03-08 00:38:37 - train: epoch 0122, iter [02900, 05004], lr: 0.035317, loss: 2.5013
2022-03-08 00:39:12 - train: epoch 0122, iter [03000, 05004], lr: 0.035317, loss: 2.7210
2022-03-08 00:39:46 - train: epoch 0122, iter [03100, 05004], lr: 0.035317, loss: 2.7004
2022-03-08 00:40:20 - train: epoch 0122, iter [03200, 05004], lr: 0.035317, loss: 2.7476
2022-03-08 00:40:54 - train: epoch 0122, iter [03300, 05004], lr: 0.035317, loss: 2.4011
2022-03-08 00:41:29 - train: epoch 0122, iter [03400, 05004], lr: 0.035317, loss: 2.5038
2022-03-08 00:42:03 - train: epoch 0122, iter [03500, 05004], lr: 0.035317, loss: 2.6003
2022-03-08 00:42:38 - train: epoch 0122, iter [03600, 05004], lr: 0.035317, loss: 2.5628
2022-03-08 00:43:12 - train: epoch 0122, iter [03700, 05004], lr: 0.035317, loss: 2.2732
2022-03-08 00:43:46 - train: epoch 0122, iter [03800, 05004], lr: 0.035317, loss: 2.5558
2022-03-08 00:44:20 - train: epoch 0122, iter [03900, 05004], lr: 0.035317, loss: 2.6941
2022-03-08 00:44:54 - train: epoch 0122, iter [04000, 05004], lr: 0.035317, loss: 2.5601
2022-03-08 00:45:29 - train: epoch 0122, iter [04100, 05004], lr: 0.035317, loss: 2.4319
2022-03-08 00:46:03 - train: epoch 0122, iter [04200, 05004], lr: 0.035317, loss: 2.4518
2022-03-08 00:46:38 - train: epoch 0122, iter [04300, 05004], lr: 0.035317, loss: 2.4301
2022-03-08 00:47:12 - train: epoch 0122, iter [04400, 05004], lr: 0.035317, loss: 2.6715
2022-03-08 00:47:46 - train: epoch 0122, iter [04500, 05004], lr: 0.035317, loss: 2.5544
2022-03-08 00:48:20 - train: epoch 0122, iter [04600, 05004], lr: 0.035317, loss: 2.6599
2022-03-08 00:48:55 - train: epoch 0122, iter [04700, 05004], lr: 0.035317, loss: 2.3748
2022-03-08 00:49:30 - train: epoch 0122, iter [04800, 05004], lr: 0.035317, loss: 2.4971
2022-03-08 00:50:04 - train: epoch 0122, iter [04900, 05004], lr: 0.035317, loss: 2.5132
2022-03-08 00:50:36 - train: epoch 0122, iter [05000, 05004], lr: 0.035317, loss: 2.7843
2022-03-08 00:50:37 - train: epoch 122, train_loss: 2.5890
2022-03-08 00:51:51 - eval: epoch: 122, acc1: 63.584%, acc5: 86.114%, test_loss: 1.4962, per_image_load_time: 1.179ms, per_image_inference_time: 0.540ms
2022-03-08 00:51:52 - until epoch: 122, best_acc1: 63.584%
2022-03-08 00:51:52 - epoch 123 lr: 0.03454915028125263
2022-03-08 00:52:32 - train: epoch 0123, iter [00100, 05004], lr: 0.034549, loss: 2.7076
2022-03-08 00:53:06 - train: epoch 0123, iter [00200, 05004], lr: 0.034549, loss: 2.5237
2022-03-08 00:53:41 - train: epoch 0123, iter [00300, 05004], lr: 0.034549, loss: 2.4193
2022-03-08 00:54:14 - train: epoch 0123, iter [00400, 05004], lr: 0.034549, loss: 2.1335
2022-03-08 00:54:49 - train: epoch 0123, iter [00500, 05004], lr: 0.034549, loss: 2.4366
2022-03-08 00:55:22 - train: epoch 0123, iter [00600, 05004], lr: 0.034549, loss: 2.4258
2022-03-08 00:55:57 - train: epoch 0123, iter [00700, 05004], lr: 0.034549, loss: 2.6512
2022-03-08 00:56:31 - train: epoch 0123, iter [00800, 05004], lr: 0.034549, loss: 2.3801
2022-03-08 00:57:05 - train: epoch 0123, iter [00900, 05004], lr: 0.034549, loss: 2.8357
2022-03-08 00:57:40 - train: epoch 0123, iter [01000, 05004], lr: 0.034549, loss: 2.8645
2022-03-08 00:58:14 - train: epoch 0123, iter [01100, 05004], lr: 0.034549, loss: 2.4297
2022-03-08 00:58:48 - train: epoch 0123, iter [01200, 05004], lr: 0.034549, loss: 2.5458
2022-03-08 00:59:22 - train: epoch 0123, iter [01300, 05004], lr: 0.034549, loss: 2.2618
2022-03-08 00:59:56 - train: epoch 0123, iter [01400, 05004], lr: 0.034549, loss: 2.5577
2022-03-08 01:00:29 - train: epoch 0123, iter [01500, 05004], lr: 0.034549, loss: 2.5360
2022-03-08 01:01:04 - train: epoch 0123, iter [01600, 05004], lr: 0.034549, loss: 2.8243
2022-03-08 01:01:39 - train: epoch 0123, iter [01700, 05004], lr: 0.034549, loss: 2.7411
2022-03-08 01:02:13 - train: epoch 0123, iter [01800, 05004], lr: 0.034549, loss: 2.6788
2022-03-08 01:02:47 - train: epoch 0123, iter [01900, 05004], lr: 0.034549, loss: 2.6716
2022-03-08 01:03:22 - train: epoch 0123, iter [02000, 05004], lr: 0.034549, loss: 2.8290
2022-03-08 01:03:55 - train: epoch 0123, iter [02100, 05004], lr: 0.034549, loss: 2.8481
2022-03-08 01:04:29 - train: epoch 0123, iter [02200, 05004], lr: 0.034549, loss: 2.6643
2022-03-08 01:05:03 - train: epoch 0123, iter [02300, 05004], lr: 0.034549, loss: 2.5674
2022-03-08 01:05:38 - train: epoch 0123, iter [02400, 05004], lr: 0.034549, loss: 2.6428
2022-03-08 01:06:12 - train: epoch 0123, iter [02500, 05004], lr: 0.034549, loss: 2.5847
2022-03-08 01:06:46 - train: epoch 0123, iter [02600, 05004], lr: 0.034549, loss: 2.5602
2022-03-08 01:07:20 - train: epoch 0123, iter [02700, 05004], lr: 0.034549, loss: 2.8094
2022-03-08 01:07:54 - train: epoch 0123, iter [02800, 05004], lr: 0.034549, loss: 2.4949
2022-03-08 01:08:28 - train: epoch 0123, iter [02900, 05004], lr: 0.034549, loss: 2.6991
2022-03-08 01:09:01 - train: epoch 0123, iter [03000, 05004], lr: 0.034549, loss: 2.3893
2022-03-08 01:09:37 - train: epoch 0123, iter [03100, 05004], lr: 0.034549, loss: 2.5976
2022-03-08 01:10:11 - train: epoch 0123, iter [03200, 05004], lr: 0.034549, loss: 2.4247
2022-03-08 01:10:45 - train: epoch 0123, iter [03300, 05004], lr: 0.034549, loss: 2.4255
2022-03-08 01:11:20 - train: epoch 0123, iter [03400, 05004], lr: 0.034549, loss: 2.5028
2022-03-08 01:11:54 - train: epoch 0123, iter [03500, 05004], lr: 0.034549, loss: 2.7174
2022-03-08 01:12:28 - train: epoch 0123, iter [03600, 05004], lr: 0.034549, loss: 2.4614
2022-03-08 01:13:02 - train: epoch 0123, iter [03700, 05004], lr: 0.034549, loss: 2.3895
2022-03-08 01:13:37 - train: epoch 0123, iter [03800, 05004], lr: 0.034549, loss: 2.8202
2022-03-08 01:14:11 - train: epoch 0123, iter [03900, 05004], lr: 0.034549, loss: 2.4588
2022-03-08 01:14:45 - train: epoch 0123, iter [04000, 05004], lr: 0.034549, loss: 2.5261
2022-03-08 01:15:19 - train: epoch 0123, iter [04100, 05004], lr: 0.034549, loss: 2.6092
2022-03-08 01:15:54 - train: epoch 0123, iter [04200, 05004], lr: 0.034549, loss: 2.8341
2022-03-08 01:16:28 - train: epoch 0123, iter [04300, 05004], lr: 0.034549, loss: 2.7044
2022-03-08 01:17:02 - train: epoch 0123, iter [04400, 05004], lr: 0.034549, loss: 2.8168
2022-03-08 01:17:37 - train: epoch 0123, iter [04500, 05004], lr: 0.034549, loss: 2.6205
2022-03-08 01:18:10 - train: epoch 0123, iter [04600, 05004], lr: 0.034549, loss: 2.6511
2022-03-08 01:18:45 - train: epoch 0123, iter [04700, 05004], lr: 0.034549, loss: 2.6525
2022-03-08 01:19:20 - train: epoch 0123, iter [04800, 05004], lr: 0.034549, loss: 2.4469
2022-03-08 01:19:54 - train: epoch 0123, iter [04900, 05004], lr: 0.034549, loss: 2.4437
2022-03-08 01:20:27 - train: epoch 0123, iter [05000, 05004], lr: 0.034549, loss: 2.7195
2022-03-08 01:20:28 - train: epoch 123, train_loss: 2.5829
2022-03-08 01:21:43 - eval: epoch: 123, acc1: 63.382%, acc5: 85.706%, test_loss: 1.5133, per_image_load_time: 1.314ms, per_image_inference_time: 0.540ms
2022-03-08 01:21:43 - until epoch: 123, best_acc1: 63.584%
2022-03-08 01:21:43 - epoch 124 lr: 0.03378507774521587
2022-03-08 01:22:23 - train: epoch 0124, iter [00100, 05004], lr: 0.033785, loss: 2.4594
2022-03-08 01:22:57 - train: epoch 0124, iter [00200, 05004], lr: 0.033785, loss: 2.4890
2022-03-08 01:23:31 - train: epoch 0124, iter [00300, 05004], lr: 0.033785, loss: 2.4863
2022-03-08 01:24:05 - train: epoch 0124, iter [00400, 05004], lr: 0.033785, loss: 2.4141
2022-03-08 01:24:40 - train: epoch 0124, iter [00500, 05004], lr: 0.033785, loss: 2.4982
2022-03-08 01:25:14 - train: epoch 0124, iter [00600, 05004], lr: 0.033785, loss: 2.6326
2022-03-08 01:25:48 - train: epoch 0124, iter [00700, 05004], lr: 0.033785, loss: 2.5221
2022-03-08 01:26:22 - train: epoch 0124, iter [00800, 05004], lr: 0.033785, loss: 2.7487
2022-03-08 01:26:57 - train: epoch 0124, iter [00900, 05004], lr: 0.033785, loss: 2.7447
2022-03-08 01:27:31 - train: epoch 0124, iter [01000, 05004], lr: 0.033785, loss: 2.5030
2022-03-08 01:28:05 - train: epoch 0124, iter [01100, 05004], lr: 0.033785, loss: 2.6265
2022-03-08 01:28:40 - train: epoch 0124, iter [01200, 05004], lr: 0.033785, loss: 2.3964
2022-03-08 01:29:14 - train: epoch 0124, iter [01300, 05004], lr: 0.033785, loss: 2.2524
2022-03-08 01:29:48 - train: epoch 0124, iter [01400, 05004], lr: 0.033785, loss: 2.7124
2022-03-08 01:30:21 - train: epoch 0124, iter [01500, 05004], lr: 0.033785, loss: 2.7161
2022-03-08 01:30:55 - train: epoch 0124, iter [01600, 05004], lr: 0.033785, loss: 2.6918
2022-03-08 01:31:30 - train: epoch 0124, iter [01700, 05004], lr: 0.033785, loss: 2.3918
2022-03-08 01:32:04 - train: epoch 0124, iter [01800, 05004], lr: 0.033785, loss: 2.5158
2022-03-08 01:32:39 - train: epoch 0124, iter [01900, 05004], lr: 0.033785, loss: 2.8145
2022-03-08 01:33:12 - train: epoch 0124, iter [02000, 05004], lr: 0.033785, loss: 2.6384
2022-03-08 01:33:46 - train: epoch 0124, iter [02100, 05004], lr: 0.033785, loss: 2.7075
2022-03-08 01:34:21 - train: epoch 0124, iter [02200, 05004], lr: 0.033785, loss: 2.4510
2022-03-08 01:34:55 - train: epoch 0124, iter [02300, 05004], lr: 0.033785, loss: 2.6512
2022-03-08 01:35:29 - train: epoch 0124, iter [02400, 05004], lr: 0.033785, loss: 2.5736
2022-03-08 01:36:02 - train: epoch 0124, iter [02500, 05004], lr: 0.033785, loss: 2.3355
2022-03-08 01:36:36 - train: epoch 0124, iter [02600, 05004], lr: 0.033785, loss: 2.6645
2022-03-08 01:37:11 - train: epoch 0124, iter [02700, 05004], lr: 0.033785, loss: 2.7335
2022-03-08 01:37:45 - train: epoch 0124, iter [02800, 05004], lr: 0.033785, loss: 2.5725
2022-03-08 01:38:19 - train: epoch 0124, iter [02900, 05004], lr: 0.033785, loss: 2.5535
2022-03-08 01:38:53 - train: epoch 0124, iter [03000, 05004], lr: 0.033785, loss: 2.6747
2022-03-08 01:39:27 - train: epoch 0124, iter [03100, 05004], lr: 0.033785, loss: 2.7108
2022-03-08 01:40:01 - train: epoch 0124, iter [03200, 05004], lr: 0.033785, loss: 2.3851
2022-03-08 01:40:35 - train: epoch 0124, iter [03300, 05004], lr: 0.033785, loss: 2.1695
2022-03-08 01:41:09 - train: epoch 0124, iter [03400, 05004], lr: 0.033785, loss: 2.7385
2022-03-08 01:41:44 - train: epoch 0124, iter [03500, 05004], lr: 0.033785, loss: 2.5853
2022-03-08 01:42:18 - train: epoch 0124, iter [03600, 05004], lr: 0.033785, loss: 2.2935
2022-03-08 01:42:51 - train: epoch 0124, iter [03700, 05004], lr: 0.033785, loss: 2.6579
2022-03-08 01:43:26 - train: epoch 0124, iter [03800, 05004], lr: 0.033785, loss: 2.7834
2022-03-08 01:43:59 - train: epoch 0124, iter [03900, 05004], lr: 0.033785, loss: 2.1637
2022-03-08 01:44:33 - train: epoch 0124, iter [04000, 05004], lr: 0.033785, loss: 2.5001
2022-03-08 01:45:07 - train: epoch 0124, iter [04100, 05004], lr: 0.033785, loss: 2.6790
2022-03-08 01:45:41 - train: epoch 0124, iter [04200, 05004], lr: 0.033785, loss: 2.4559
2022-03-08 01:46:15 - train: epoch 0124, iter [04300, 05004], lr: 0.033785, loss: 2.6414
2022-03-08 01:46:49 - train: epoch 0124, iter [04400, 05004], lr: 0.033785, loss: 2.5959
2022-03-08 01:47:24 - train: epoch 0124, iter [04500, 05004], lr: 0.033785, loss: 2.5524
2022-03-08 01:47:58 - train: epoch 0124, iter [04600, 05004], lr: 0.033785, loss: 2.6615
2022-03-08 01:48:32 - train: epoch 0124, iter [04700, 05004], lr: 0.033785, loss: 2.4770
2022-03-08 01:49:06 - train: epoch 0124, iter [04800, 05004], lr: 0.033785, loss: 2.6105
2022-03-08 01:49:41 - train: epoch 0124, iter [04900, 05004], lr: 0.033785, loss: 2.4842
2022-03-08 01:50:13 - train: epoch 0124, iter [05000, 05004], lr: 0.033785, loss: 2.7682
2022-03-08 01:50:15 - train: epoch 124, train_loss: 2.5717
2022-03-08 01:51:29 - eval: epoch: 124, acc1: 62.986%, acc5: 85.548%, test_loss: 1.5269, per_image_load_time: 1.543ms, per_image_inference_time: 0.561ms
2022-03-08 01:51:30 - until epoch: 124, best_acc1: 63.584%
2022-03-08 01:51:30 - epoch 125 lr: 0.033025213793178645
2022-03-08 01:52:09 - train: epoch 0125, iter [00100, 05004], lr: 0.033025, loss: 2.1874
2022-03-08 01:52:43 - train: epoch 0125, iter [00200, 05004], lr: 0.033025, loss: 2.6782
2022-03-08 01:53:18 - train: epoch 0125, iter [00300, 05004], lr: 0.033025, loss: 2.4978
2022-03-08 01:53:52 - train: epoch 0125, iter [00400, 05004], lr: 0.033025, loss: 2.6104
2022-03-08 01:54:25 - train: epoch 0125, iter [00500, 05004], lr: 0.033025, loss: 2.6806
2022-03-08 01:54:59 - train: epoch 0125, iter [00600, 05004], lr: 0.033025, loss: 2.6082
2022-03-08 01:55:33 - train: epoch 0125, iter [00700, 05004], lr: 0.033025, loss: 2.5767
2022-03-08 01:56:07 - train: epoch 0125, iter [00800, 05004], lr: 0.033025, loss: 2.5464
2022-03-08 01:56:42 - train: epoch 0125, iter [00900, 05004], lr: 0.033025, loss: 2.4569
2022-03-08 01:57:16 - train: epoch 0125, iter [01000, 05004], lr: 0.033025, loss: 2.7881
2022-03-08 01:57:50 - train: epoch 0125, iter [01100, 05004], lr: 0.033025, loss: 2.5455
2022-03-08 01:58:25 - train: epoch 0125, iter [01200, 05004], lr: 0.033025, loss: 2.5339
2022-03-08 01:58:58 - train: epoch 0125, iter [01300, 05004], lr: 0.033025, loss: 2.4674
2022-03-08 01:59:32 - train: epoch 0125, iter [01400, 05004], lr: 0.033025, loss: 2.4509
2022-03-08 02:00:07 - train: epoch 0125, iter [01500, 05004], lr: 0.033025, loss: 2.6015
2022-03-08 02:00:40 - train: epoch 0125, iter [01600, 05004], lr: 0.033025, loss: 2.5737
2022-03-08 02:01:14 - train: epoch 0125, iter [01700, 05004], lr: 0.033025, loss: 2.5500
2022-03-08 02:01:48 - train: epoch 0125, iter [01800, 05004], lr: 0.033025, loss: 2.3763
2022-03-08 02:02:23 - train: epoch 0125, iter [01900, 05004], lr: 0.033025, loss: 2.5327
2022-03-08 02:02:57 - train: epoch 0125, iter [02000, 05004], lr: 0.033025, loss: 2.5077
2022-03-08 02:03:31 - train: epoch 0125, iter [02100, 05004], lr: 0.033025, loss: 2.3208
2022-03-08 02:04:05 - train: epoch 0125, iter [02200, 05004], lr: 0.033025, loss: 2.8043
2022-03-08 02:04:39 - train: epoch 0125, iter [02300, 05004], lr: 0.033025, loss: 2.3158
2022-03-08 02:05:13 - train: epoch 0125, iter [02400, 05004], lr: 0.033025, loss: 2.6688
2022-03-08 02:05:47 - train: epoch 0125, iter [02500, 05004], lr: 0.033025, loss: 2.4892
2022-03-08 02:06:20 - train: epoch 0125, iter [02600, 05004], lr: 0.033025, loss: 2.4765
2022-03-08 02:06:55 - train: epoch 0125, iter [02700, 05004], lr: 0.033025, loss: 2.5125
2022-03-08 02:07:29 - train: epoch 0125, iter [02800, 05004], lr: 0.033025, loss: 2.6731
2022-03-08 02:08:03 - train: epoch 0125, iter [02900, 05004], lr: 0.033025, loss: 2.7225
2022-03-08 02:08:37 - train: epoch 0125, iter [03000, 05004], lr: 0.033025, loss: 2.4479
2022-03-08 02:09:11 - train: epoch 0125, iter [03100, 05004], lr: 0.033025, loss: 2.8615
2022-03-08 02:09:45 - train: epoch 0125, iter [03200, 05004], lr: 0.033025, loss: 2.5642
2022-03-08 02:10:19 - train: epoch 0125, iter [03300, 05004], lr: 0.033025, loss: 2.5900
2022-03-08 02:10:53 - train: epoch 0125, iter [03400, 05004], lr: 0.033025, loss: 2.5196
2022-03-08 02:11:28 - train: epoch 0125, iter [03500, 05004], lr: 0.033025, loss: 2.6327
2022-03-08 02:12:02 - train: epoch 0125, iter [03600, 05004], lr: 0.033025, loss: 2.4995
2022-03-08 02:12:35 - train: epoch 0125, iter [03700, 05004], lr: 0.033025, loss: 2.8544
2022-03-08 02:13:10 - train: epoch 0125, iter [03800, 05004], lr: 0.033025, loss: 2.9187
2022-03-08 02:13:44 - train: epoch 0125, iter [03900, 05004], lr: 0.033025, loss: 2.5362
2022-03-08 02:14:18 - train: epoch 0125, iter [04000, 05004], lr: 0.033025, loss: 2.8294
2022-03-08 02:14:52 - train: epoch 0125, iter [04100, 05004], lr: 0.033025, loss: 2.4631
2022-03-08 02:15:26 - train: epoch 0125, iter [04200, 05004], lr: 0.033025, loss: 2.6270
2022-03-08 02:16:00 - train: epoch 0125, iter [04300, 05004], lr: 0.033025, loss: 2.5180
2022-03-08 02:16:34 - train: epoch 0125, iter [04400, 05004], lr: 0.033025, loss: 2.6717
2022-03-08 02:17:07 - train: epoch 0125, iter [04500, 05004], lr: 0.033025, loss: 2.7686
2022-03-08 02:17:41 - train: epoch 0125, iter [04600, 05004], lr: 0.033025, loss: 2.2159
2022-03-08 02:18:17 - train: epoch 0125, iter [04700, 05004], lr: 0.033025, loss: 2.5931
2022-03-08 02:18:51 - train: epoch 0125, iter [04800, 05004], lr: 0.033025, loss: 2.5152
2022-03-08 02:19:26 - train: epoch 0125, iter [04900, 05004], lr: 0.033025, loss: 2.4502
2022-03-08 02:19:58 - train: epoch 0125, iter [05000, 05004], lr: 0.033025, loss: 2.5558
2022-03-08 02:19:59 - train: epoch 125, train_loss: 2.5624
2022-03-08 02:21:14 - eval: epoch: 125, acc1: 61.798%, acc5: 84.640%, test_loss: 1.5862, per_image_load_time: 0.711ms, per_image_inference_time: 0.531ms
2022-03-08 02:21:14 - until epoch: 125, best_acc1: 63.584%
2022-03-08 02:21:14 - epoch 126 lr: 0.03226975564787322
2022-03-08 02:21:53 - train: epoch 0126, iter [00100, 05004], lr: 0.032270, loss: 2.6090
2022-03-08 02:22:29 - train: epoch 0126, iter [00200, 05004], lr: 0.032270, loss: 2.5714
2022-03-08 02:23:02 - train: epoch 0126, iter [00300, 05004], lr: 0.032270, loss: 2.4525
2022-03-08 02:23:37 - train: epoch 0126, iter [00400, 05004], lr: 0.032270, loss: 2.6674
2022-03-08 02:24:10 - train: epoch 0126, iter [00500, 05004], lr: 0.032270, loss: 2.2363
2022-03-08 02:24:44 - train: epoch 0126, iter [00600, 05004], lr: 0.032270, loss: 2.5661
2022-03-08 02:25:19 - train: epoch 0126, iter [00700, 05004], lr: 0.032270, loss: 2.5260
2022-03-08 02:25:53 - train: epoch 0126, iter [00800, 05004], lr: 0.032270, loss: 2.4628
2022-03-08 02:26:27 - train: epoch 0126, iter [00900, 05004], lr: 0.032270, loss: 2.4938
2022-03-08 02:27:01 - train: epoch 0126, iter [01000, 05004], lr: 0.032270, loss: 2.4967
2022-03-08 02:27:34 - train: epoch 0126, iter [01100, 05004], lr: 0.032270, loss: 2.6612
2022-03-08 02:28:08 - train: epoch 0126, iter [01200, 05004], lr: 0.032270, loss: 2.4564
2022-03-08 02:28:43 - train: epoch 0126, iter [01300, 05004], lr: 0.032270, loss: 2.8105
2022-03-08 02:29:18 - train: epoch 0126, iter [01400, 05004], lr: 0.032270, loss: 2.7313
2022-03-08 02:29:52 - train: epoch 0126, iter [01500, 05004], lr: 0.032270, loss: 2.4285
2022-03-08 02:30:25 - train: epoch 0126, iter [01600, 05004], lr: 0.032270, loss: 2.2869
2022-03-08 02:30:59 - train: epoch 0126, iter [01700, 05004], lr: 0.032270, loss: 2.3611
2022-03-08 02:31:34 - train: epoch 0126, iter [01800, 05004], lr: 0.032270, loss: 2.6175
2022-03-08 02:32:09 - train: epoch 0126, iter [01900, 05004], lr: 0.032270, loss: 2.3523
2022-03-08 02:32:42 - train: epoch 0126, iter [02000, 05004], lr: 0.032270, loss: 2.4344
2022-03-08 02:33:17 - train: epoch 0126, iter [02100, 05004], lr: 0.032270, loss: 2.4425
2022-03-08 02:33:50 - train: epoch 0126, iter [02200, 05004], lr: 0.032270, loss: 2.5234
2022-03-08 02:34:25 - train: epoch 0126, iter [02300, 05004], lr: 0.032270, loss: 2.6448
2022-03-08 02:34:59 - train: epoch 0126, iter [02400, 05004], lr: 0.032270, loss: 2.6852
2022-03-08 02:35:34 - train: epoch 0126, iter [02500, 05004], lr: 0.032270, loss: 2.8011
2022-03-08 02:36:07 - train: epoch 0126, iter [02600, 05004], lr: 0.032270, loss: 2.6235
2022-03-08 02:36:42 - train: epoch 0126, iter [02700, 05004], lr: 0.032270, loss: 2.4403
2022-03-08 02:37:16 - train: epoch 0126, iter [02800, 05004], lr: 0.032270, loss: 2.6697
2022-03-08 02:37:50 - train: epoch 0126, iter [02900, 05004], lr: 0.032270, loss: 2.7674
2022-03-08 02:38:25 - train: epoch 0126, iter [03000, 05004], lr: 0.032270, loss: 2.2427
2022-03-08 02:38:59 - train: epoch 0126, iter [03100, 05004], lr: 0.032270, loss: 2.3541
2022-03-08 02:39:32 - train: epoch 0126, iter [03200, 05004], lr: 0.032270, loss: 2.6305
2022-03-08 02:40:07 - train: epoch 0126, iter [03300, 05004], lr: 0.032270, loss: 2.5985
2022-03-08 02:40:41 - train: epoch 0126, iter [03400, 05004], lr: 0.032270, loss: 2.6192
2022-03-08 02:41:14 - train: epoch 0126, iter [03500, 05004], lr: 0.032270, loss: 2.5037
2022-03-08 02:41:48 - train: epoch 0126, iter [03600, 05004], lr: 0.032270, loss: 2.6887
2022-03-08 02:42:22 - train: epoch 0126, iter [03700, 05004], lr: 0.032270, loss: 2.4985
2022-03-08 02:42:56 - train: epoch 0126, iter [03800, 05004], lr: 0.032270, loss: 2.7687
2022-03-08 02:43:30 - train: epoch 0126, iter [03900, 05004], lr: 0.032270, loss: 2.5766
2022-03-08 02:44:05 - train: epoch 0126, iter [04000, 05004], lr: 0.032270, loss: 2.5425
2022-03-08 02:44:39 - train: epoch 0126, iter [04100, 05004], lr: 0.032270, loss: 2.5807
2022-03-08 02:45:12 - train: epoch 0126, iter [04200, 05004], lr: 0.032270, loss: 2.4118
2022-03-08 02:45:46 - train: epoch 0126, iter [04300, 05004], lr: 0.032270, loss: 2.5519
2022-03-08 02:46:20 - train: epoch 0126, iter [04400, 05004], lr: 0.032270, loss: 2.5955
2022-03-08 02:46:55 - train: epoch 0126, iter [04500, 05004], lr: 0.032270, loss: 2.3699
2022-03-08 02:47:30 - train: epoch 0126, iter [04600, 05004], lr: 0.032270, loss: 2.4174
2022-03-08 02:48:04 - train: epoch 0126, iter [04700, 05004], lr: 0.032270, loss: 2.5970
2022-03-08 02:48:37 - train: epoch 0126, iter [04800, 05004], lr: 0.032270, loss: 2.6670
2022-03-08 02:49:11 - train: epoch 0126, iter [04900, 05004], lr: 0.032270, loss: 2.5530
2022-03-08 02:49:44 - train: epoch 0126, iter [05000, 05004], lr: 0.032270, loss: 2.5477
2022-03-08 02:49:45 - train: epoch 126, train_loss: 2.5588
2022-03-08 02:50:59 - eval: epoch: 126, acc1: 63.282%, acc5: 85.676%, test_loss: 1.5112, per_image_load_time: 0.993ms, per_image_inference_time: 0.541ms
2022-03-08 02:51:00 - until epoch: 126, best_acc1: 63.584%
2022-03-08 02:51:00 - epoch 127 lr: 0.031518899388504454
2022-03-08 02:51:40 - train: epoch 0127, iter [00100, 05004], lr: 0.031519, loss: 2.7877
2022-03-08 02:52:14 - train: epoch 0127, iter [00200, 05004], lr: 0.031519, loss: 2.3123
2022-03-08 02:52:48 - train: epoch 0127, iter [00300, 05004], lr: 0.031519, loss: 2.3710
2022-03-08 02:53:23 - train: epoch 0127, iter [00400, 05004], lr: 0.031519, loss: 2.7241
2022-03-08 02:53:56 - train: epoch 0127, iter [00500, 05004], lr: 0.031519, loss: 2.3849
2022-03-08 02:54:30 - train: epoch 0127, iter [00600, 05004], lr: 0.031519, loss: 3.1290
2022-03-08 02:55:05 - train: epoch 0127, iter [00700, 05004], lr: 0.031519, loss: 2.6030
2022-03-08 02:55:39 - train: epoch 0127, iter [00800, 05004], lr: 0.031519, loss: 2.3449
2022-03-08 02:56:14 - train: epoch 0127, iter [00900, 05004], lr: 0.031519, loss: 2.4771
2022-03-08 02:56:49 - train: epoch 0127, iter [01000, 05004], lr: 0.031519, loss: 2.6881
2022-03-08 02:57:23 - train: epoch 0127, iter [01100, 05004], lr: 0.031519, loss: 2.4170
2022-03-08 02:57:58 - train: epoch 0127, iter [01200, 05004], lr: 0.031519, loss: 2.3961
2022-03-08 02:58:32 - train: epoch 0127, iter [01300, 05004], lr: 0.031519, loss: 2.7514
2022-03-08 02:59:06 - train: epoch 0127, iter [01400, 05004], lr: 0.031519, loss: 2.2138
2022-03-08 02:59:40 - train: epoch 0127, iter [01500, 05004], lr: 0.031519, loss: 2.4480
2022-03-08 03:00:14 - train: epoch 0127, iter [01600, 05004], lr: 0.031519, loss: 2.3474
2022-03-08 03:00:48 - train: epoch 0127, iter [01700, 05004], lr: 0.031519, loss: 2.5658
2022-03-08 03:01:22 - train: epoch 0127, iter [01800, 05004], lr: 0.031519, loss: 2.3908
2022-03-08 03:01:57 - train: epoch 0127, iter [01900, 05004], lr: 0.031519, loss: 2.4577
2022-03-08 03:02:31 - train: epoch 0127, iter [02000, 05004], lr: 0.031519, loss: 2.5224
2022-03-08 03:03:06 - train: epoch 0127, iter [02100, 05004], lr: 0.031519, loss: 2.6621
2022-03-08 03:03:40 - train: epoch 0127, iter [02200, 05004], lr: 0.031519, loss: 2.7677
2022-03-08 03:04:14 - train: epoch 0127, iter [02300, 05004], lr: 0.031519, loss: 2.5266
2022-03-08 03:04:47 - train: epoch 0127, iter [02400, 05004], lr: 0.031519, loss: 2.7733
2022-03-08 03:05:22 - train: epoch 0127, iter [02500, 05004], lr: 0.031519, loss: 2.4476
2022-03-08 03:05:56 - train: epoch 0127, iter [02600, 05004], lr: 0.031519, loss: 2.3334
2022-03-08 03:06:30 - train: epoch 0127, iter [02700, 05004], lr: 0.031519, loss: 2.4883
2022-03-08 03:07:05 - train: epoch 0127, iter [02800, 05004], lr: 0.031519, loss: 2.6500
2022-03-08 03:07:39 - train: epoch 0127, iter [02900, 05004], lr: 0.031519, loss: 2.4750
2022-03-08 03:08:14 - train: epoch 0127, iter [03000, 05004], lr: 0.031519, loss: 2.6567
2022-03-08 03:08:47 - train: epoch 0127, iter [03100, 05004], lr: 0.031519, loss: 2.6128
2022-03-08 03:09:22 - train: epoch 0127, iter [03200, 05004], lr: 0.031519, loss: 2.5897
2022-03-08 03:09:56 - train: epoch 0127, iter [03300, 05004], lr: 0.031519, loss: 2.6468
2022-03-08 03:10:29 - train: epoch 0127, iter [03400, 05004], lr: 0.031519, loss: 2.7533
2022-03-08 03:11:04 - train: epoch 0127, iter [03500, 05004], lr: 0.031519, loss: 2.6309
2022-03-08 03:11:39 - train: epoch 0127, iter [03600, 05004], lr: 0.031519, loss: 2.7603
2022-03-08 03:12:12 - train: epoch 0127, iter [03700, 05004], lr: 0.031519, loss: 2.7085
2022-03-08 03:12:47 - train: epoch 0127, iter [03800, 05004], lr: 0.031519, loss: 2.5635
2022-03-08 03:13:21 - train: epoch 0127, iter [03900, 05004], lr: 0.031519, loss: 2.4211
2022-03-08 03:13:55 - train: epoch 0127, iter [04000, 05004], lr: 0.031519, loss: 2.2193
2022-03-08 03:14:29 - train: epoch 0127, iter [04100, 05004], lr: 0.031519, loss: 2.2211
2022-03-08 03:15:04 - train: epoch 0127, iter [04200, 05004], lr: 0.031519, loss: 2.3141
2022-03-08 03:15:37 - train: epoch 0127, iter [04300, 05004], lr: 0.031519, loss: 2.3484
2022-03-08 03:16:12 - train: epoch 0127, iter [04400, 05004], lr: 0.031519, loss: 2.3825
2022-03-08 03:16:46 - train: epoch 0127, iter [04500, 05004], lr: 0.031519, loss: 2.4559
2022-03-08 03:17:21 - train: epoch 0127, iter [04600, 05004], lr: 0.031519, loss: 2.3643
2022-03-08 03:17:55 - train: epoch 0127, iter [04700, 05004], lr: 0.031519, loss: 2.4852
2022-03-08 03:18:28 - train: epoch 0127, iter [04800, 05004], lr: 0.031519, loss: 2.5725
2022-03-08 03:19:03 - train: epoch 0127, iter [04900, 05004], lr: 0.031519, loss: 2.6258
2022-03-08 03:19:37 - train: epoch 0127, iter [05000, 05004], lr: 0.031519, loss: 2.7464
2022-03-08 03:19:38 - train: epoch 127, train_loss: 2.5461
2022-03-08 03:20:52 - eval: epoch: 127, acc1: 64.374%, acc5: 86.194%, test_loss: 1.4686, per_image_load_time: 2.001ms, per_image_inference_time: 0.533ms
2022-03-08 03:20:53 - until epoch: 127, best_acc1: 64.374%
2022-03-08 03:20:53 - epoch 128 lr: 0.030772839899857463
2022-03-08 03:21:33 - train: epoch 0128, iter [00100, 05004], lr: 0.030773, loss: 2.5802
2022-03-08 03:22:07 - train: epoch 0128, iter [00200, 05004], lr: 0.030773, loss: 2.5355
2022-03-08 03:22:41 - train: epoch 0128, iter [00300, 05004], lr: 0.030773, loss: 2.7679
2022-03-08 03:23:15 - train: epoch 0128, iter [00400, 05004], lr: 0.030773, loss: 2.3097
2022-03-08 03:23:49 - train: epoch 0128, iter [00500, 05004], lr: 0.030773, loss: 2.5151
2022-03-08 03:24:23 - train: epoch 0128, iter [00600, 05004], lr: 0.030773, loss: 2.6097
2022-03-08 03:24:56 - train: epoch 0128, iter [00700, 05004], lr: 0.030773, loss: 2.5741
2022-03-08 03:25:31 - train: epoch 0128, iter [00800, 05004], lr: 0.030773, loss: 2.7760
2022-03-08 03:26:05 - train: epoch 0128, iter [00900, 05004], lr: 0.030773, loss: 2.5949
2022-03-08 03:26:39 - train: epoch 0128, iter [01000, 05004], lr: 0.030773, loss: 2.2776
2022-03-08 03:27:14 - train: epoch 0128, iter [01100, 05004], lr: 0.030773, loss: 2.5608
2022-03-08 03:27:47 - train: epoch 0128, iter [01200, 05004], lr: 0.030773, loss: 2.4314
2022-03-08 03:28:21 - train: epoch 0128, iter [01300, 05004], lr: 0.030773, loss: 2.4883
2022-03-08 03:28:56 - train: epoch 0128, iter [01400, 05004], lr: 0.030773, loss: 2.7543
2022-03-08 03:29:29 - train: epoch 0128, iter [01500, 05004], lr: 0.030773, loss: 2.5845
2022-03-08 03:30:03 - train: epoch 0128, iter [01600, 05004], lr: 0.030773, loss: 2.5721
2022-03-08 03:30:37 - train: epoch 0128, iter [01700, 05004], lr: 0.030773, loss: 2.3404
2022-03-08 03:31:12 - train: epoch 0128, iter [01800, 05004], lr: 0.030773, loss: 2.4541
2022-03-08 03:31:46 - train: epoch 0128, iter [01900, 05004], lr: 0.030773, loss: 2.1154
2022-03-08 03:32:20 - train: epoch 0128, iter [02000, 05004], lr: 0.030773, loss: 2.6489
2022-03-08 03:32:54 - train: epoch 0128, iter [02100, 05004], lr: 0.030773, loss: 2.4956
2022-03-08 03:33:29 - train: epoch 0128, iter [02200, 05004], lr: 0.030773, loss: 2.7755
2022-03-08 03:34:01 - train: epoch 0128, iter [02300, 05004], lr: 0.030773, loss: 2.4145
2022-03-08 03:34:36 - train: epoch 0128, iter [02400, 05004], lr: 0.030773, loss: 2.4101
2022-03-08 03:35:10 - train: epoch 0128, iter [02500, 05004], lr: 0.030773, loss: 2.7829
2022-03-08 03:35:45 - train: epoch 0128, iter [02600, 05004], lr: 0.030773, loss: 2.4320
2022-03-08 03:36:18 - train: epoch 0128, iter [02700, 05004], lr: 0.030773, loss: 2.4438
2022-03-08 03:36:52 - train: epoch 0128, iter [02800, 05004], lr: 0.030773, loss: 2.4776
2022-03-08 03:37:26 - train: epoch 0128, iter [02900, 05004], lr: 0.030773, loss: 2.6719
2022-03-08 03:38:01 - train: epoch 0128, iter [03000, 05004], lr: 0.030773, loss: 2.6394
2022-03-08 03:38:34 - train: epoch 0128, iter [03100, 05004], lr: 0.030773, loss: 2.5722
2022-03-08 03:39:08 - train: epoch 0128, iter [03200, 05004], lr: 0.030773, loss: 2.6634
2022-03-08 03:39:42 - train: epoch 0128, iter [03300, 05004], lr: 0.030773, loss: 2.4532
2022-03-08 03:40:16 - train: epoch 0128, iter [03400, 05004], lr: 0.030773, loss: 2.3896
2022-03-08 03:40:50 - train: epoch 0128, iter [03500, 05004], lr: 0.030773, loss: 2.7463
2022-03-08 03:41:25 - train: epoch 0128, iter [03600, 05004], lr: 0.030773, loss: 2.7350
2022-03-08 03:41:58 - train: epoch 0128, iter [03700, 05004], lr: 0.030773, loss: 2.3958
2022-03-08 03:42:33 - train: epoch 0128, iter [03800, 05004], lr: 0.030773, loss: 2.4742
2022-03-08 03:43:07 - train: epoch 0128, iter [03900, 05004], lr: 0.030773, loss: 2.4471
2022-03-08 03:43:41 - train: epoch 0128, iter [04000, 05004], lr: 0.030773, loss: 2.6804
2022-03-08 03:44:15 - train: epoch 0128, iter [04100, 05004], lr: 0.030773, loss: 2.4697
2022-03-08 03:44:49 - train: epoch 0128, iter [04200, 05004], lr: 0.030773, loss: 2.6733
2022-03-08 03:45:23 - train: epoch 0128, iter [04300, 05004], lr: 0.030773, loss: 2.6134
2022-03-08 03:45:58 - train: epoch 0128, iter [04400, 05004], lr: 0.030773, loss: 2.4816
2022-03-08 03:46:31 - train: epoch 0128, iter [04500, 05004], lr: 0.030773, loss: 2.5305
2022-03-08 03:47:06 - train: epoch 0128, iter [04600, 05004], lr: 0.030773, loss: 2.3514
2022-03-08 03:47:39 - train: epoch 0128, iter [04700, 05004], lr: 0.030773, loss: 2.7680
2022-03-08 03:48:14 - train: epoch 0128, iter [04800, 05004], lr: 0.030773, loss: 2.4806
2022-03-08 03:48:48 - train: epoch 0128, iter [04900, 05004], lr: 0.030773, loss: 2.5884
2022-03-08 03:49:21 - train: epoch 0128, iter [05000, 05004], lr: 0.030773, loss: 2.5108
2022-03-08 03:49:22 - train: epoch 128, train_loss: 2.5369
2022-03-08 03:50:37 - eval: epoch: 128, acc1: 64.294%, acc5: 86.378%, test_loss: 1.4686, per_image_load_time: 2.294ms, per_image_inference_time: 0.548ms
2022-03-08 03:50:38 - until epoch: 128, best_acc1: 64.374%
2022-03-08 03:50:38 - epoch 129 lr: 0.030031770821715233
2022-03-08 03:51:17 - train: epoch 0129, iter [00100, 05004], lr: 0.030032, loss: 2.3403
2022-03-08 03:51:52 - train: epoch 0129, iter [00200, 05004], lr: 0.030032, loss: 2.6434
2022-03-08 03:52:25 - train: epoch 0129, iter [00300, 05004], lr: 0.030032, loss: 2.6054
2022-03-08 03:52:59 - train: epoch 0129, iter [00400, 05004], lr: 0.030032, loss: 2.6533
2022-03-08 03:53:34 - train: epoch 0129, iter [00500, 05004], lr: 0.030032, loss: 2.3987
2022-03-08 03:54:08 - train: epoch 0129, iter [00600, 05004], lr: 0.030032, loss: 2.5061
2022-03-08 03:54:42 - train: epoch 0129, iter [00700, 05004], lr: 0.030032, loss: 2.7354
2022-03-08 03:55:17 - train: epoch 0129, iter [00800, 05004], lr: 0.030032, loss: 2.8852
2022-03-08 03:55:52 - train: epoch 0129, iter [00900, 05004], lr: 0.030032, loss: 2.4592
2022-03-08 03:56:25 - train: epoch 0129, iter [01000, 05004], lr: 0.030032, loss: 2.8222
2022-03-08 03:57:00 - train: epoch 0129, iter [01100, 05004], lr: 0.030032, loss: 2.6272
2022-03-08 03:57:35 - train: epoch 0129, iter [01200, 05004], lr: 0.030032, loss: 2.5104
2022-03-08 03:58:09 - train: epoch 0129, iter [01300, 05004], lr: 0.030032, loss: 2.5418
2022-03-08 03:58:43 - train: epoch 0129, iter [01400, 05004], lr: 0.030032, loss: 2.5229
2022-03-08 03:59:17 - train: epoch 0129, iter [01500, 05004], lr: 0.030032, loss: 2.5014
2022-03-08 03:59:51 - train: epoch 0129, iter [01600, 05004], lr: 0.030032, loss: 2.6595
2022-03-08 04:00:25 - train: epoch 0129, iter [01700, 05004], lr: 0.030032, loss: 2.6193
2022-03-08 04:00:59 - train: epoch 0129, iter [01800, 05004], lr: 0.030032, loss: 2.7111
2022-03-08 04:01:34 - train: epoch 0129, iter [01900, 05004], lr: 0.030032, loss: 2.6534
2022-03-08 04:02:07 - train: epoch 0129, iter [02000, 05004], lr: 0.030032, loss: 2.4004
2022-03-08 04:02:42 - train: epoch 0129, iter [02100, 05004], lr: 0.030032, loss: 2.2943
2022-03-08 04:03:17 - train: epoch 0129, iter [02200, 05004], lr: 0.030032, loss: 2.2928
2022-03-08 04:03:51 - train: epoch 0129, iter [02300, 05004], lr: 0.030032, loss: 2.6616
2022-03-08 04:04:25 - train: epoch 0129, iter [02400, 05004], lr: 0.030032, loss: 2.8625
2022-03-08 04:05:00 - train: epoch 0129, iter [02500, 05004], lr: 0.030032, loss: 2.9403
2022-03-08 04:05:34 - train: epoch 0129, iter [02600, 05004], lr: 0.030032, loss: 2.6442
2022-03-08 04:06:07 - train: epoch 0129, iter [02700, 05004], lr: 0.030032, loss: 2.8560
2022-03-08 04:06:42 - train: epoch 0129, iter [02800, 05004], lr: 0.030032, loss: 2.4945
2022-03-08 04:07:17 - train: epoch 0129, iter [02900, 05004], lr: 0.030032, loss: 2.3692
2022-03-08 04:07:51 - train: epoch 0129, iter [03000, 05004], lr: 0.030032, loss: 2.4031
2022-03-08 04:08:25 - train: epoch 0129, iter [03100, 05004], lr: 0.030032, loss: 2.7499
2022-03-08 04:08:59 - train: epoch 0129, iter [03200, 05004], lr: 0.030032, loss: 2.5425
2022-03-08 04:09:34 - train: epoch 0129, iter [03300, 05004], lr: 0.030032, loss: 2.4041
2022-03-08 04:10:08 - train: epoch 0129, iter [03400, 05004], lr: 0.030032, loss: 2.4819
2022-03-08 04:10:42 - train: epoch 0129, iter [03500, 05004], lr: 0.030032, loss: 2.5297
2022-03-08 04:11:15 - train: epoch 0129, iter [03600, 05004], lr: 0.030032, loss: 2.5559
2022-03-08 04:11:50 - train: epoch 0129, iter [03700, 05004], lr: 0.030032, loss: 2.4205
2022-03-08 04:12:24 - train: epoch 0129, iter [03800, 05004], lr: 0.030032, loss: 2.3714
2022-03-08 04:12:58 - train: epoch 0129, iter [03900, 05004], lr: 0.030032, loss: 2.3856
2022-03-08 04:13:32 - train: epoch 0129, iter [04000, 05004], lr: 0.030032, loss: 2.4812
2022-03-08 04:14:08 - train: epoch 0129, iter [04100, 05004], lr: 0.030032, loss: 2.5556
2022-03-08 04:14:41 - train: epoch 0129, iter [04200, 05004], lr: 0.030032, loss: 2.7424
2022-03-08 04:15:16 - train: epoch 0129, iter [04300, 05004], lr: 0.030032, loss: 2.8419
2022-03-08 04:15:50 - train: epoch 0129, iter [04400, 05004], lr: 0.030032, loss: 2.4893
2022-03-08 04:16:24 - train: epoch 0129, iter [04500, 05004], lr: 0.030032, loss: 2.3400
2022-03-08 04:16:58 - train: epoch 0129, iter [04600, 05004], lr: 0.030032, loss: 2.4003
2022-03-08 04:17:32 - train: epoch 0129, iter [04700, 05004], lr: 0.030032, loss: 2.7170
2022-03-08 04:18:06 - train: epoch 0129, iter [04800, 05004], lr: 0.030032, loss: 2.6305
2022-03-08 04:18:41 - train: epoch 0129, iter [04900, 05004], lr: 0.030032, loss: 2.5609
2022-03-08 04:19:14 - train: epoch 0129, iter [05000, 05004], lr: 0.030032, loss: 2.5228
2022-03-08 04:19:15 - train: epoch 129, train_loss: 2.5279
2022-03-08 04:20:29 - eval: epoch: 129, acc1: 65.396%, acc5: 86.992%, test_loss: 1.4216, per_image_load_time: 0.618ms, per_image_inference_time: 0.525ms
2022-03-08 04:20:30 - until epoch: 129, best_acc1: 65.396%
2022-03-08 04:20:30 - epoch 130 lr: 0.029295884498599413
2022-03-08 04:21:09 - train: epoch 0130, iter [00100, 05004], lr: 0.029296, loss: 2.3341
2022-03-08 04:21:43 - train: epoch 0130, iter [00200, 05004], lr: 0.029296, loss: 2.3691
2022-03-08 04:22:18 - train: epoch 0130, iter [00300, 05004], lr: 0.029296, loss: 2.7215
2022-03-08 04:22:52 - train: epoch 0130, iter [00400, 05004], lr: 0.029296, loss: 2.8410
2022-03-08 04:23:26 - train: epoch 0130, iter [00500, 05004], lr: 0.029296, loss: 2.3866
2022-03-08 04:24:00 - train: epoch 0130, iter [00600, 05004], lr: 0.029296, loss: 2.7315
2022-03-08 04:24:34 - train: epoch 0130, iter [00700, 05004], lr: 0.029296, loss: 2.5511
2022-03-08 04:25:08 - train: epoch 0130, iter [00800, 05004], lr: 0.029296, loss: 2.5861
2022-03-08 04:25:42 - train: epoch 0130, iter [00900, 05004], lr: 0.029296, loss: 2.6327
2022-03-08 04:26:17 - train: epoch 0130, iter [01000, 05004], lr: 0.029296, loss: 2.4496
2022-03-08 04:26:51 - train: epoch 0130, iter [01100, 05004], lr: 0.029296, loss: 2.5614
2022-03-08 04:27:26 - train: epoch 0130, iter [01200, 05004], lr: 0.029296, loss: 2.5798
2022-03-08 04:28:00 - train: epoch 0130, iter [01300, 05004], lr: 0.029296, loss: 2.5065
2022-03-08 04:28:35 - train: epoch 0130, iter [01400, 05004], lr: 0.029296, loss: 2.6517
2022-03-08 04:29:09 - train: epoch 0130, iter [01500, 05004], lr: 0.029296, loss: 2.4712
2022-03-08 04:29:43 - train: epoch 0130, iter [01600, 05004], lr: 0.029296, loss: 2.4774
2022-03-08 04:30:18 - train: epoch 0130, iter [01700, 05004], lr: 0.029296, loss: 2.4910
2022-03-08 04:30:52 - train: epoch 0130, iter [01800, 05004], lr: 0.029296, loss: 2.5760
2022-03-08 04:31:26 - train: epoch 0130, iter [01900, 05004], lr: 0.029296, loss: 2.4838
2022-03-08 04:32:01 - train: epoch 0130, iter [02000, 05004], lr: 0.029296, loss: 2.7624
2022-03-08 04:32:35 - train: epoch 0130, iter [02100, 05004], lr: 0.029296, loss: 2.6291
2022-03-08 04:33:09 - train: epoch 0130, iter [02200, 05004], lr: 0.029296, loss: 2.7088
2022-03-08 04:33:44 - train: epoch 0130, iter [02300, 05004], lr: 0.029296, loss: 2.5289
2022-03-08 04:34:17 - train: epoch 0130, iter [02400, 05004], lr: 0.029296, loss: 2.5047
2022-03-08 04:34:51 - train: epoch 0130, iter [02500, 05004], lr: 0.029296, loss: 2.5272
2022-03-08 04:35:25 - train: epoch 0130, iter [02600, 05004], lr: 0.029296, loss: 2.4634
2022-03-08 04:35:59 - train: epoch 0130, iter [02700, 05004], lr: 0.029296, loss: 2.8370
2022-03-08 04:36:34 - train: epoch 0130, iter [02800, 05004], lr: 0.029296, loss: 2.4690
2022-03-08 04:37:08 - train: epoch 0130, iter [02900, 05004], lr: 0.029296, loss: 2.8137
2022-03-08 04:37:42 - train: epoch 0130, iter [03000, 05004], lr: 0.029296, loss: 2.1558
2022-03-08 04:38:17 - train: epoch 0130, iter [03100, 05004], lr: 0.029296, loss: 2.7464
2022-03-08 04:38:51 - train: epoch 0130, iter [03200, 05004], lr: 0.029296, loss: 2.3175
2022-03-08 04:39:25 - train: epoch 0130, iter [03300, 05004], lr: 0.029296, loss: 2.3978
2022-03-08 04:39:59 - train: epoch 0130, iter [03400, 05004], lr: 0.029296, loss: 2.1925
2022-03-08 04:40:33 - train: epoch 0130, iter [03500, 05004], lr: 0.029296, loss: 2.6063
2022-03-08 04:41:07 - train: epoch 0130, iter [03600, 05004], lr: 0.029296, loss: 2.4904
2022-03-08 04:41:41 - train: epoch 0130, iter [03700, 05004], lr: 0.029296, loss: 2.6339
2022-03-08 04:42:15 - train: epoch 0130, iter [03800, 05004], lr: 0.029296, loss: 2.5967
2022-03-08 04:42:49 - train: epoch 0130, iter [03900, 05004], lr: 0.029296, loss: 2.4533
2022-03-08 04:43:24 - train: epoch 0130, iter [04000, 05004], lr: 0.029296, loss: 2.2675
2022-03-08 04:43:58 - train: epoch 0130, iter [04100, 05004], lr: 0.029296, loss: 2.5213
2022-03-08 04:44:32 - train: epoch 0130, iter [04200, 05004], lr: 0.029296, loss: 2.6439
2022-03-08 04:45:07 - train: epoch 0130, iter [04300, 05004], lr: 0.029296, loss: 2.5331
2022-03-08 04:45:41 - train: epoch 0130, iter [04400, 05004], lr: 0.029296, loss: 2.5391
2022-03-08 04:46:15 - train: epoch 0130, iter [04500, 05004], lr: 0.029296, loss: 2.7760
2022-03-08 04:46:49 - train: epoch 0130, iter [04600, 05004], lr: 0.029296, loss: 2.4462
2022-03-08 04:47:23 - train: epoch 0130, iter [04700, 05004], lr: 0.029296, loss: 2.4438
2022-03-08 04:47:58 - train: epoch 0130, iter [04800, 05004], lr: 0.029296, loss: 2.4660
2022-03-08 04:48:32 - train: epoch 0130, iter [04900, 05004], lr: 0.029296, loss: 2.0864
2022-03-08 04:49:05 - train: epoch 0130, iter [05000, 05004], lr: 0.029296, loss: 2.5605
2022-03-08 04:49:06 - train: epoch 130, train_loss: 2.5206
2022-03-08 04:50:21 - eval: epoch: 130, acc1: 63.392%, acc5: 85.934%, test_loss: 1.4991, per_image_load_time: 0.609ms, per_image_inference_time: 0.513ms
2022-03-08 04:50:21 - until epoch: 130, best_acc1: 65.396%
2022-03-08 04:50:21 - epoch 131 lr: 0.028565371929847285
2022-03-08 04:51:01 - train: epoch 0131, iter [00100, 05004], lr: 0.028565, loss: 2.3863
2022-03-08 04:51:34 - train: epoch 0131, iter [00200, 05004], lr: 0.028565, loss: 2.4175
2022-03-08 04:52:08 - train: epoch 0131, iter [00300, 05004], lr: 0.028565, loss: 2.4548
2022-03-08 04:52:43 - train: epoch 0131, iter [00400, 05004], lr: 0.028565, loss: 2.5564
2022-03-08 04:53:17 - train: epoch 0131, iter [00500, 05004], lr: 0.028565, loss: 2.4078
2022-03-08 04:53:52 - train: epoch 0131, iter [00600, 05004], lr: 0.028565, loss: 2.5405
2022-03-08 04:54:26 - train: epoch 0131, iter [00700, 05004], lr: 0.028565, loss: 2.5896
2022-03-08 04:55:00 - train: epoch 0131, iter [00800, 05004], lr: 0.028565, loss: 2.2859
2022-03-08 04:55:34 - train: epoch 0131, iter [00900, 05004], lr: 0.028565, loss: 2.3630
2022-03-08 04:56:09 - train: epoch 0131, iter [01000, 05004], lr: 0.028565, loss: 2.3381
2022-03-08 04:56:42 - train: epoch 0131, iter [01100, 05004], lr: 0.028565, loss: 2.5571
2022-03-08 04:57:17 - train: epoch 0131, iter [01200, 05004], lr: 0.028565, loss: 2.6735
2022-03-08 04:57:51 - train: epoch 0131, iter [01300, 05004], lr: 0.028565, loss: 2.6542
2022-03-08 04:58:24 - train: epoch 0131, iter [01400, 05004], lr: 0.028565, loss: 2.3372
2022-03-08 04:58:59 - train: epoch 0131, iter [01500, 05004], lr: 0.028565, loss: 2.6501
2022-03-08 04:59:33 - train: epoch 0131, iter [01600, 05004], lr: 0.028565, loss: 2.8926
2022-03-08 05:00:07 - train: epoch 0131, iter [01700, 05004], lr: 0.028565, loss: 2.6673
2022-03-08 05:00:41 - train: epoch 0131, iter [01800, 05004], lr: 0.028565, loss: 2.8506
2022-03-08 05:01:16 - train: epoch 0131, iter [01900, 05004], lr: 0.028565, loss: 2.6372
2022-03-08 05:01:49 - train: epoch 0131, iter [02000, 05004], lr: 0.028565, loss: 2.4751
2022-03-08 05:02:24 - train: epoch 0131, iter [02100, 05004], lr: 0.028565, loss: 2.7695
2022-03-08 05:02:58 - train: epoch 0131, iter [02200, 05004], lr: 0.028565, loss: 2.6072
2022-03-08 05:03:32 - train: epoch 0131, iter [02300, 05004], lr: 0.028565, loss: 2.7426
2022-03-08 05:04:06 - train: epoch 0131, iter [02400, 05004], lr: 0.028565, loss: 2.3294
2022-03-08 05:04:41 - train: epoch 0131, iter [02500, 05004], lr: 0.028565, loss: 2.5231
2022-03-08 05:05:15 - train: epoch 0131, iter [02600, 05004], lr: 0.028565, loss: 2.2958
2022-03-08 05:05:49 - train: epoch 0131, iter [02700, 05004], lr: 0.028565, loss: 2.7304
2022-03-08 05:06:23 - train: epoch 0131, iter [02800, 05004], lr: 0.028565, loss: 2.0762
2022-03-08 05:06:58 - train: epoch 0131, iter [02900, 05004], lr: 0.028565, loss: 2.4617
2022-03-08 05:07:33 - train: epoch 0131, iter [03000, 05004], lr: 0.028565, loss: 2.2139
2022-03-08 05:08:07 - train: epoch 0131, iter [03100, 05004], lr: 0.028565, loss: 2.5923
2022-03-08 05:08:41 - train: epoch 0131, iter [03200, 05004], lr: 0.028565, loss: 2.7210
2022-03-08 05:09:15 - train: epoch 0131, iter [03300, 05004], lr: 0.028565, loss: 2.6742
2022-03-08 05:09:49 - train: epoch 0131, iter [03400, 05004], lr: 0.028565, loss: 2.3955
2022-03-08 05:10:23 - train: epoch 0131, iter [03500, 05004], lr: 0.028565, loss: 2.6409
2022-03-08 05:10:58 - train: epoch 0131, iter [03600, 05004], lr: 0.028565, loss: 2.6112
2022-03-08 05:11:32 - train: epoch 0131, iter [03700, 05004], lr: 0.028565, loss: 2.5183
2022-03-08 05:12:06 - train: epoch 0131, iter [03800, 05004], lr: 0.028565, loss: 2.6942
2022-03-08 05:12:40 - train: epoch 0131, iter [03900, 05004], lr: 0.028565, loss: 2.4646
2022-03-08 05:13:14 - train: epoch 0131, iter [04000, 05004], lr: 0.028565, loss: 2.6841
2022-03-08 05:13:48 - train: epoch 0131, iter [04100, 05004], lr: 0.028565, loss: 2.7030
2022-03-08 05:14:23 - train: epoch 0131, iter [04200, 05004], lr: 0.028565, loss: 2.4931
2022-03-08 05:14:56 - train: epoch 0131, iter [04300, 05004], lr: 0.028565, loss: 2.7867
2022-03-08 05:15:31 - train: epoch 0131, iter [04400, 05004], lr: 0.028565, loss: 2.7576
2022-03-08 05:16:04 - train: epoch 0131, iter [04500, 05004], lr: 0.028565, loss: 2.2119
2022-03-08 05:16:39 - train: epoch 0131, iter [04600, 05004], lr: 0.028565, loss: 2.5743
2022-03-08 05:17:13 - train: epoch 0131, iter [04700, 05004], lr: 0.028565, loss: 2.3389
2022-03-08 05:17:48 - train: epoch 0131, iter [04800, 05004], lr: 0.028565, loss: 2.5304
2022-03-08 05:18:22 - train: epoch 0131, iter [04900, 05004], lr: 0.028565, loss: 2.3998
2022-03-08 05:18:54 - train: epoch 0131, iter [05000, 05004], lr: 0.028565, loss: 2.4663
2022-03-08 05:18:56 - train: epoch 131, train_loss: 2.5141
2022-03-08 05:20:10 - eval: epoch: 131, acc1: 64.562%, acc5: 86.788%, test_loss: 1.4447, per_image_load_time: 0.840ms, per_image_inference_time: 0.519ms
2022-03-08 05:20:11 - until epoch: 131, best_acc1: 65.396%
2022-03-08 05:20:11 - epoch 132 lr: 0.02784042272003794
2022-03-08 05:20:50 - train: epoch 0132, iter [00100, 05004], lr: 0.027840, loss: 2.6962
2022-03-08 05:21:25 - train: epoch 0132, iter [00200, 05004], lr: 0.027840, loss: 2.2058
2022-03-08 05:21:58 - train: epoch 0132, iter [00300, 05004], lr: 0.027840, loss: 2.5706
2022-03-08 05:22:32 - train: epoch 0132, iter [00400, 05004], lr: 0.027840, loss: 2.2078
2022-03-08 05:23:07 - train: epoch 0132, iter [00500, 05004], lr: 0.027840, loss: 2.4728
2022-03-08 05:23:40 - train: epoch 0132, iter [00600, 05004], lr: 0.027840, loss: 2.6178
2022-03-08 05:24:15 - train: epoch 0132, iter [00700, 05004], lr: 0.027840, loss: 2.2712
2022-03-08 05:24:49 - train: epoch 0132, iter [00800, 05004], lr: 0.027840, loss: 2.3037
2022-03-08 05:25:23 - train: epoch 0132, iter [00900, 05004], lr: 0.027840, loss: 2.4611
2022-03-08 05:25:57 - train: epoch 0132, iter [01000, 05004], lr: 0.027840, loss: 2.5239
2022-03-08 05:26:31 - train: epoch 0132, iter [01100, 05004], lr: 0.027840, loss: 2.6842
2022-03-08 05:27:05 - train: epoch 0132, iter [01200, 05004], lr: 0.027840, loss: 2.4487
2022-03-08 05:27:39 - train: epoch 0132, iter [01300, 05004], lr: 0.027840, loss: 2.1282
2022-03-08 05:28:13 - train: epoch 0132, iter [01400, 05004], lr: 0.027840, loss: 2.1734
2022-03-08 05:28:47 - train: epoch 0132, iter [01500, 05004], lr: 0.027840, loss: 2.6558
2022-03-08 05:29:21 - train: epoch 0132, iter [01600, 05004], lr: 0.027840, loss: 2.6167
2022-03-08 05:29:56 - train: epoch 0132, iter [01700, 05004], lr: 0.027840, loss: 2.5078
2022-03-08 05:30:29 - train: epoch 0132, iter [01800, 05004], lr: 0.027840, loss: 2.6320
2022-03-08 05:31:04 - train: epoch 0132, iter [01900, 05004], lr: 0.027840, loss: 1.9841
2022-03-08 05:31:38 - train: epoch 0132, iter [02000, 05004], lr: 0.027840, loss: 2.2978
2022-03-08 05:32:12 - train: epoch 0132, iter [02100, 05004], lr: 0.027840, loss: 2.5245
2022-03-08 05:32:46 - train: epoch 0132, iter [02200, 05004], lr: 0.027840, loss: 2.4488
2022-03-08 05:33:20 - train: epoch 0132, iter [02300, 05004], lr: 0.027840, loss: 2.7066
2022-03-08 05:33:54 - train: epoch 0132, iter [02400, 05004], lr: 0.027840, loss: 2.5588
2022-03-08 05:34:29 - train: epoch 0132, iter [02500, 05004], lr: 0.027840, loss: 2.4953
2022-03-08 05:35:02 - train: epoch 0132, iter [02600, 05004], lr: 0.027840, loss: 2.5074
2022-03-08 05:35:36 - train: epoch 0132, iter [02700, 05004], lr: 0.027840, loss: 2.5763
2022-03-08 05:36:11 - train: epoch 0132, iter [02800, 05004], lr: 0.027840, loss: 2.3711
2022-03-08 05:36:45 - train: epoch 0132, iter [02900, 05004], lr: 0.027840, loss: 2.4591
2022-03-08 05:37:20 - train: epoch 0132, iter [03000, 05004], lr: 0.027840, loss: 2.5371
2022-03-08 05:37:54 - train: epoch 0132, iter [03100, 05004], lr: 0.027840, loss: 2.5613
2022-03-08 05:38:28 - train: epoch 0132, iter [03200, 05004], lr: 0.027840, loss: 2.6219
2022-03-08 05:39:03 - train: epoch 0132, iter [03300, 05004], lr: 0.027840, loss: 2.6513
2022-03-08 05:39:37 - train: epoch 0132, iter [03400, 05004], lr: 0.027840, loss: 2.4942
2022-03-08 05:40:11 - train: epoch 0132, iter [03500, 05004], lr: 0.027840, loss: 2.5015
2022-03-08 05:40:46 - train: epoch 0132, iter [03600, 05004], lr: 0.027840, loss: 2.5334
2022-03-08 05:41:20 - train: epoch 0132, iter [03700, 05004], lr: 0.027840, loss: 2.4558
2022-03-08 05:41:55 - train: epoch 0132, iter [03800, 05004], lr: 0.027840, loss: 2.5248
2022-03-08 05:42:28 - train: epoch 0132, iter [03900, 05004], lr: 0.027840, loss: 2.3643
2022-03-08 05:43:03 - train: epoch 0132, iter [04000, 05004], lr: 0.027840, loss: 2.4636
2022-03-08 05:43:36 - train: epoch 0132, iter [04100, 05004], lr: 0.027840, loss: 2.7379
2022-03-08 05:44:10 - train: epoch 0132, iter [04200, 05004], lr: 0.027840, loss: 2.4823
2022-03-08 05:44:44 - train: epoch 0132, iter [04300, 05004], lr: 0.027840, loss: 2.5097
2022-03-08 05:45:19 - train: epoch 0132, iter [04400, 05004], lr: 0.027840, loss: 2.4604
2022-03-08 05:45:53 - train: epoch 0132, iter [04500, 05004], lr: 0.027840, loss: 2.6543
2022-03-08 05:46:27 - train: epoch 0132, iter [04600, 05004], lr: 0.027840, loss: 2.6890
2022-03-08 05:47:01 - train: epoch 0132, iter [04700, 05004], lr: 0.027840, loss: 2.5485
2022-03-08 05:47:36 - train: epoch 0132, iter [04800, 05004], lr: 0.027840, loss: 2.4579
2022-03-08 05:48:09 - train: epoch 0132, iter [04900, 05004], lr: 0.027840, loss: 2.8218
2022-03-08 05:48:42 - train: epoch 0132, iter [05000, 05004], lr: 0.027840, loss: 2.5740
2022-03-08 05:48:43 - train: epoch 132, train_loss: 2.5007
2022-03-08 05:49:59 - eval: epoch: 132, acc1: 65.162%, acc5: 86.970%, test_loss: 1.4161, per_image_load_time: 1.906ms, per_image_inference_time: 0.529ms
2022-03-08 05:50:00 - until epoch: 132, best_acc1: 65.396%
2022-03-08 05:50:00 - epoch 133 lr: 0.02712122502978024
2022-03-08 05:50:39 - train: epoch 0133, iter [00100, 05004], lr: 0.027121, loss: 2.3650
2022-03-08 05:51:13 - train: epoch 0133, iter [00200, 05004], lr: 0.027121, loss: 2.3886
2022-03-08 05:51:47 - train: epoch 0133, iter [00300, 05004], lr: 0.027121, loss: 2.1646
2022-03-08 05:52:21 - train: epoch 0133, iter [00400, 05004], lr: 0.027121, loss: 2.4110
2022-03-08 05:52:55 - train: epoch 0133, iter [00500, 05004], lr: 0.027121, loss: 2.6635
2022-03-08 05:53:29 - train: epoch 0133, iter [00600, 05004], lr: 0.027121, loss: 2.5417
2022-03-08 05:54:03 - train: epoch 0133, iter [00700, 05004], lr: 0.027121, loss: 2.3147
2022-03-08 05:54:38 - train: epoch 0133, iter [00800, 05004], lr: 0.027121, loss: 2.7260
2022-03-08 05:55:11 - train: epoch 0133, iter [00900, 05004], lr: 0.027121, loss: 2.3274
2022-03-08 05:55:46 - train: epoch 0133, iter [01000, 05004], lr: 0.027121, loss: 2.5409
2022-03-08 05:56:20 - train: epoch 0133, iter [01100, 05004], lr: 0.027121, loss: 2.4842
2022-03-08 05:56:54 - train: epoch 0133, iter [01200, 05004], lr: 0.027121, loss: 2.3837
2022-03-08 05:57:28 - train: epoch 0133, iter [01300, 05004], lr: 0.027121, loss: 2.3852
2022-03-08 05:58:03 - train: epoch 0133, iter [01400, 05004], lr: 0.027121, loss: 2.4265
2022-03-08 05:58:36 - train: epoch 0133, iter [01500, 05004], lr: 0.027121, loss: 2.5704
2022-03-08 05:59:11 - train: epoch 0133, iter [01600, 05004], lr: 0.027121, loss: 2.6299
2022-03-08 05:59:45 - train: epoch 0133, iter [01700, 05004], lr: 0.027121, loss: 3.0734
2022-03-08 06:00:19 - train: epoch 0133, iter [01800, 05004], lr: 0.027121, loss: 2.3418
2022-03-08 06:00:53 - train: epoch 0133, iter [01900, 05004], lr: 0.027121, loss: 2.2006
2022-03-08 06:01:27 - train: epoch 0133, iter [02000, 05004], lr: 0.027121, loss: 2.3783
2022-03-08 06:02:01 - train: epoch 0133, iter [02100, 05004], lr: 0.027121, loss: 2.6780
2022-03-08 06:02:34 - train: epoch 0133, iter [02200, 05004], lr: 0.027121, loss: 2.4724
2022-03-08 06:03:09 - train: epoch 0133, iter [02300, 05004], lr: 0.027121, loss: 2.2745
2022-03-08 06:03:44 - train: epoch 0133, iter [02400, 05004], lr: 0.027121, loss: 2.4554
2022-03-08 06:04:17 - train: epoch 0133, iter [02500, 05004], lr: 0.027121, loss: 2.3182
2022-03-08 06:04:52 - train: epoch 0133, iter [02600, 05004], lr: 0.027121, loss: 2.6239
2022-03-08 06:05:26 - train: epoch 0133, iter [02700, 05004], lr: 0.027121, loss: 2.6347
2022-03-08 06:06:00 - train: epoch 0133, iter [02800, 05004], lr: 0.027121, loss: 2.6362
2022-03-08 06:06:34 - train: epoch 0133, iter [02900, 05004], lr: 0.027121, loss: 2.0978
2022-03-08 06:07:08 - train: epoch 0133, iter [03000, 05004], lr: 0.027121, loss: 2.6593
2022-03-08 06:07:43 - train: epoch 0133, iter [03100, 05004], lr: 0.027121, loss: 2.3210
2022-03-08 06:08:16 - train: epoch 0133, iter [03200, 05004], lr: 0.027121, loss: 2.2567
2022-03-08 06:08:50 - train: epoch 0133, iter [03300, 05004], lr: 0.027121, loss: 2.3548
2022-03-08 06:09:25 - train: epoch 0133, iter [03400, 05004], lr: 0.027121, loss: 2.5079
2022-03-08 06:09:59 - train: epoch 0133, iter [03500, 05004], lr: 0.027121, loss: 2.6826
2022-03-08 06:10:33 - train: epoch 0133, iter [03600, 05004], lr: 0.027121, loss: 2.7070
2022-03-08 06:11:07 - train: epoch 0133, iter [03700, 05004], lr: 0.027121, loss: 2.5296
2022-03-08 06:11:41 - train: epoch 0133, iter [03800, 05004], lr: 0.027121, loss: 2.5939
2022-03-08 06:12:15 - train: epoch 0133, iter [03900, 05004], lr: 0.027121, loss: 2.4663
2022-03-08 06:12:50 - train: epoch 0133, iter [04000, 05004], lr: 0.027121, loss: 2.6661
2022-03-08 06:13:24 - train: epoch 0133, iter [04100, 05004], lr: 0.027121, loss: 2.3568
2022-03-08 06:13:58 - train: epoch 0133, iter [04200, 05004], lr: 0.027121, loss: 2.3791
2022-03-08 06:14:32 - train: epoch 0133, iter [04300, 05004], lr: 0.027121, loss: 2.7882
2022-03-08 06:15:06 - train: epoch 0133, iter [04400, 05004], lr: 0.027121, loss: 2.5848
2022-03-08 06:15:41 - train: epoch 0133, iter [04500, 05004], lr: 0.027121, loss: 2.6328
2022-03-08 06:16:15 - train: epoch 0133, iter [04600, 05004], lr: 0.027121, loss: 2.3279
2022-03-08 06:16:49 - train: epoch 0133, iter [04700, 05004], lr: 0.027121, loss: 2.4910
2022-03-08 06:17:23 - train: epoch 0133, iter [04800, 05004], lr: 0.027121, loss: 2.4790
2022-03-08 06:17:57 - train: epoch 0133, iter [04900, 05004], lr: 0.027121, loss: 2.5389
2022-03-08 06:18:30 - train: epoch 0133, iter [05000, 05004], lr: 0.027121, loss: 2.2599
2022-03-08 06:18:31 - train: epoch 133, train_loss: 2.4894
2022-03-08 06:19:45 - eval: epoch: 133, acc1: 65.902%, acc5: 87.334%, test_loss: 1.3984, per_image_load_time: 1.048ms, per_image_inference_time: 0.532ms
2022-03-08 06:19:46 - until epoch: 133, best_acc1: 65.902%
2022-03-08 06:19:46 - epoch 134 lr: 0.0264079655268759
2022-03-08 06:20:25 - train: epoch 0134, iter [00100, 05004], lr: 0.026408, loss: 2.1207
2022-03-08 06:20:59 - train: epoch 0134, iter [00200, 05004], lr: 0.026408, loss: 2.3845
2022-03-08 06:21:33 - train: epoch 0134, iter [00300, 05004], lr: 0.026408, loss: 2.7153
2022-03-08 06:22:07 - train: epoch 0134, iter [00400, 05004], lr: 0.026408, loss: 2.4371
2022-03-08 06:22:42 - train: epoch 0134, iter [00500, 05004], lr: 0.026408, loss: 2.3366
2022-03-08 06:23:16 - train: epoch 0134, iter [00600, 05004], lr: 0.026408, loss: 2.8437
2022-03-08 06:23:51 - train: epoch 0134, iter [00700, 05004], lr: 0.026408, loss: 2.5022
2022-03-08 06:24:24 - train: epoch 0134, iter [00800, 05004], lr: 0.026408, loss: 2.3363
2022-03-08 06:24:59 - train: epoch 0134, iter [00900, 05004], lr: 0.026408, loss: 2.6070
2022-03-08 06:25:32 - train: epoch 0134, iter [01000, 05004], lr: 0.026408, loss: 2.4313
2022-03-08 06:26:06 - train: epoch 0134, iter [01100, 05004], lr: 0.026408, loss: 2.4887
2022-03-08 06:26:41 - train: epoch 0134, iter [01200, 05004], lr: 0.026408, loss: 2.3831
2022-03-08 06:27:15 - train: epoch 0134, iter [01300, 05004], lr: 0.026408, loss: 2.6189
2022-03-08 06:27:49 - train: epoch 0134, iter [01400, 05004], lr: 0.026408, loss: 2.1774
2022-03-08 06:28:24 - train: epoch 0134, iter [01500, 05004], lr: 0.026408, loss: 2.4397
2022-03-08 06:28:57 - train: epoch 0134, iter [01600, 05004], lr: 0.026408, loss: 2.4933
2022-03-08 06:29:32 - train: epoch 0134, iter [01700, 05004], lr: 0.026408, loss: 2.4925
2022-03-08 06:30:06 - train: epoch 0134, iter [01800, 05004], lr: 0.026408, loss: 2.7883
2022-03-08 06:30:41 - train: epoch 0134, iter [01900, 05004], lr: 0.026408, loss: 2.4394
2022-03-08 06:31:15 - train: epoch 0134, iter [02000, 05004], lr: 0.026408, loss: 2.3371
2022-03-08 06:31:48 - train: epoch 0134, iter [02100, 05004], lr: 0.026408, loss: 2.5519
2022-03-08 06:32:24 - train: epoch 0134, iter [02200, 05004], lr: 0.026408, loss: 2.3624
2022-03-08 06:32:57 - train: epoch 0134, iter [02300, 05004], lr: 0.026408, loss: 2.6063
2022-03-08 06:33:31 - train: epoch 0134, iter [02400, 05004], lr: 0.026408, loss: 2.3541
2022-03-08 06:34:06 - train: epoch 0134, iter [02500, 05004], lr: 0.026408, loss: 2.5342
2022-03-08 06:34:39 - train: epoch 0134, iter [02600, 05004], lr: 0.026408, loss: 2.3077
2022-03-08 06:35:14 - train: epoch 0134, iter [02700, 05004], lr: 0.026408, loss: 2.5242
2022-03-08 06:35:48 - train: epoch 0134, iter [02800, 05004], lr: 0.026408, loss: 2.6876
2022-03-08 06:36:23 - train: epoch 0134, iter [02900, 05004], lr: 0.026408, loss: 2.5472
2022-03-08 06:36:57 - train: epoch 0134, iter [03000, 05004], lr: 0.026408, loss: 2.5124
2022-03-08 06:37:31 - train: epoch 0134, iter [03100, 05004], lr: 0.026408, loss: 2.6207
2022-03-08 06:38:04 - train: epoch 0134, iter [03200, 05004], lr: 0.026408, loss: 2.4878
2022-03-08 06:38:38 - train: epoch 0134, iter [03300, 05004], lr: 0.026408, loss: 2.4153
2022-03-08 06:39:12 - train: epoch 0134, iter [03400, 05004], lr: 0.026408, loss: 2.4121
2022-03-08 06:39:47 - train: epoch 0134, iter [03500, 05004], lr: 0.026408, loss: 2.4001
2022-03-08 06:40:20 - train: epoch 0134, iter [03600, 05004], lr: 0.026408, loss: 2.4361
2022-03-08 06:40:54 - train: epoch 0134, iter [03700, 05004], lr: 0.026408, loss: 2.6948
2022-03-08 06:41:28 - train: epoch 0134, iter [03800, 05004], lr: 0.026408, loss: 2.4505
2022-03-08 06:42:02 - train: epoch 0134, iter [03900, 05004], lr: 0.026408, loss: 2.5375
2022-03-08 06:42:36 - train: epoch 0134, iter [04000, 05004], lr: 0.026408, loss: 2.7961
2022-03-08 06:43:10 - train: epoch 0134, iter [04100, 05004], lr: 0.026408, loss: 2.8944
2022-03-08 06:43:43 - train: epoch 0134, iter [04200, 05004], lr: 0.026408, loss: 2.3536
2022-03-08 06:44:17 - train: epoch 0134, iter [04300, 05004], lr: 0.026408, loss: 2.5980
2022-03-08 06:44:51 - train: epoch 0134, iter [04400, 05004], lr: 0.026408, loss: 2.5773
2022-03-08 06:45:25 - train: epoch 0134, iter [04500, 05004], lr: 0.026408, loss: 2.7472
2022-03-08 06:45:58 - train: epoch 0134, iter [04600, 05004], lr: 0.026408, loss: 2.3544
2022-03-08 06:46:32 - train: epoch 0134, iter [04700, 05004], lr: 0.026408, loss: 2.4759
2022-03-08 06:47:07 - train: epoch 0134, iter [04800, 05004], lr: 0.026408, loss: 2.5131
2022-03-08 06:47:41 - train: epoch 0134, iter [04900, 05004], lr: 0.026408, loss: 2.2931
2022-03-08 06:48:13 - train: epoch 0134, iter [05000, 05004], lr: 0.026408, loss: 2.8255
2022-03-08 06:48:14 - train: epoch 134, train_loss: 2.4866
2022-03-08 06:49:29 - eval: epoch: 134, acc1: 63.418%, acc5: 85.776%, test_loss: 1.5051, per_image_load_time: 0.784ms, per_image_inference_time: 0.523ms
2022-03-08 06:49:30 - until epoch: 134, best_acc1: 65.902%
2022-03-08 06:49:30 - epoch 135 lr: 0.0257008293378697
2022-03-08 06:50:09 - train: epoch 0135, iter [00100, 05004], lr: 0.025701, loss: 2.2400
2022-03-08 06:50:43 - train: epoch 0135, iter [00200, 05004], lr: 0.025701, loss: 2.5215
2022-03-08 06:51:17 - train: epoch 0135, iter [00300, 05004], lr: 0.025701, loss: 2.5491
2022-03-08 06:51:52 - train: epoch 0135, iter [00400, 05004], lr: 0.025701, loss: 2.2659
2022-03-08 06:52:27 - train: epoch 0135, iter [00500, 05004], lr: 0.025701, loss: 2.5815
2022-03-08 06:53:01 - train: epoch 0135, iter [00600, 05004], lr: 0.025701, loss: 2.2587
2022-03-08 06:53:35 - train: epoch 0135, iter [00700, 05004], lr: 0.025701, loss: 2.7424
2022-03-08 06:54:08 - train: epoch 0135, iter [00800, 05004], lr: 0.025701, loss: 2.7653
2022-03-08 06:54:42 - train: epoch 0135, iter [00900, 05004], lr: 0.025701, loss: 2.4692
2022-03-08 06:55:17 - train: epoch 0135, iter [01000, 05004], lr: 0.025701, loss: 2.4295
2022-03-08 06:55:52 - train: epoch 0135, iter [01100, 05004], lr: 0.025701, loss: 2.3787
2022-03-08 06:56:26 - train: epoch 0135, iter [01200, 05004], lr: 0.025701, loss: 2.5493
2022-03-08 06:56:59 - train: epoch 0135, iter [01300, 05004], lr: 0.025701, loss: 2.4446
2022-03-08 06:57:34 - train: epoch 0135, iter [01400, 05004], lr: 0.025701, loss: 2.6489
2022-03-08 06:58:09 - train: epoch 0135, iter [01500, 05004], lr: 0.025701, loss: 2.3775
2022-03-08 06:58:43 - train: epoch 0135, iter [01600, 05004], lr: 0.025701, loss: 2.4081
2022-03-08 06:59:16 - train: epoch 0135, iter [01700, 05004], lr: 0.025701, loss: 2.8142
2022-03-08 06:59:50 - train: epoch 0135, iter [01800, 05004], lr: 0.025701, loss: 2.8577
2022-03-08 07:00:25 - train: epoch 0135, iter [01900, 05004], lr: 0.025701, loss: 2.4316
2022-03-08 07:00:58 - train: epoch 0135, iter [02000, 05004], lr: 0.025701, loss: 2.4561
2022-03-08 07:01:33 - train: epoch 0135, iter [02100, 05004], lr: 0.025701, loss: 2.7977
2022-03-08 07:02:07 - train: epoch 0135, iter [02200, 05004], lr: 0.025701, loss: 2.4373
2022-03-08 07:02:41 - train: epoch 0135, iter [02300, 05004], lr: 0.025701, loss: 2.5500
2022-03-08 07:03:15 - train: epoch 0135, iter [02400, 05004], lr: 0.025701, loss: 2.2413
2022-03-08 07:03:48 - train: epoch 0135, iter [02500, 05004], lr: 0.025701, loss: 2.4775
2022-03-08 07:04:23 - train: epoch 0135, iter [02600, 05004], lr: 0.025701, loss: 2.5354
2022-03-08 07:04:56 - train: epoch 0135, iter [02700, 05004], lr: 0.025701, loss: 2.4175
2022-03-08 07:05:30 - train: epoch 0135, iter [02800, 05004], lr: 0.025701, loss: 2.7099
2022-03-08 07:06:04 - train: epoch 0135, iter [02900, 05004], lr: 0.025701, loss: 2.5858
2022-03-08 07:06:38 - train: epoch 0135, iter [03000, 05004], lr: 0.025701, loss: 2.4819
2022-03-08 07:07:12 - train: epoch 0135, iter [03100, 05004], lr: 0.025701, loss: 2.5287
2022-03-08 07:07:46 - train: epoch 0135, iter [03200, 05004], lr: 0.025701, loss: 2.6448
2022-03-08 07:08:20 - train: epoch 0135, iter [03300, 05004], lr: 0.025701, loss: 2.7098
2022-03-08 07:08:54 - train: epoch 0135, iter [03400, 05004], lr: 0.025701, loss: 2.3624
2022-03-08 07:09:28 - train: epoch 0135, iter [03500, 05004], lr: 0.025701, loss: 2.6425
2022-03-08 07:10:02 - train: epoch 0135, iter [03600, 05004], lr: 0.025701, loss: 2.3723
2022-03-08 07:10:36 - train: epoch 0135, iter [03700, 05004], lr: 0.025701, loss: 2.4746
2022-03-08 07:11:10 - train: epoch 0135, iter [03800, 05004], lr: 0.025701, loss: 2.2202
2022-03-08 07:11:45 - train: epoch 0135, iter [03900, 05004], lr: 0.025701, loss: 2.5483
2022-03-08 07:12:19 - train: epoch 0135, iter [04000, 05004], lr: 0.025701, loss: 2.4814
2022-03-08 07:12:53 - train: epoch 0135, iter [04100, 05004], lr: 0.025701, loss: 2.6873
2022-03-08 07:13:26 - train: epoch 0135, iter [04200, 05004], lr: 0.025701, loss: 2.5824
2022-03-08 07:14:00 - train: epoch 0135, iter [04300, 05004], lr: 0.025701, loss: 2.6314
2022-03-08 07:14:34 - train: epoch 0135, iter [04400, 05004], lr: 0.025701, loss: 2.5584
2022-03-08 07:15:08 - train: epoch 0135, iter [04500, 05004], lr: 0.025701, loss: 2.2197
2022-03-08 07:15:42 - train: epoch 0135, iter [04600, 05004], lr: 0.025701, loss: 2.7137
2022-03-08 07:16:16 - train: epoch 0135, iter [04700, 05004], lr: 0.025701, loss: 2.2864
2022-03-08 07:16:50 - train: epoch 0135, iter [04800, 05004], lr: 0.025701, loss: 2.4059
2022-03-08 07:17:25 - train: epoch 0135, iter [04900, 05004], lr: 0.025701, loss: 2.3711
2022-03-08 07:17:57 - train: epoch 0135, iter [05000, 05004], lr: 0.025701, loss: 2.1926
2022-03-08 07:17:58 - train: epoch 135, train_loss: 2.4742
2022-03-08 07:19:12 - eval: epoch: 135, acc1: 63.938%, acc5: 86.156%, test_loss: 1.4964, per_image_load_time: 1.103ms, per_image_inference_time: 0.547ms
2022-03-08 07:19:12 - until epoch: 135, best_acc1: 65.902%
2022-03-08 07:19:12 - epoch 136 lr: 0.025000000000000012
2022-03-08 07:19:52 - train: epoch 0136, iter [00100, 05004], lr: 0.025000, loss: 2.3722
2022-03-08 07:20:26 - train: epoch 0136, iter [00200, 05004], lr: 0.025000, loss: 2.5985
2022-03-08 07:21:00 - train: epoch 0136, iter [00300, 05004], lr: 0.025000, loss: 2.5176
2022-03-08 07:21:34 - train: epoch 0136, iter [00400, 05004], lr: 0.025000, loss: 2.4012
2022-03-08 07:22:08 - train: epoch 0136, iter [00500, 05004], lr: 0.025000, loss: 2.3132
2022-03-08 07:22:42 - train: epoch 0136, iter [00600, 05004], lr: 0.025000, loss: 2.1815
2022-03-08 07:23:16 - train: epoch 0136, iter [00700, 05004], lr: 0.025000, loss: 2.4833
2022-03-08 07:23:50 - train: epoch 0136, iter [00800, 05004], lr: 0.025000, loss: 2.7189
2022-03-08 07:24:24 - train: epoch 0136, iter [00900, 05004], lr: 0.025000, loss: 2.4391
2022-03-08 07:24:58 - train: epoch 0136, iter [01000, 05004], lr: 0.025000, loss: 2.2753
2022-03-08 07:25:32 - train: epoch 0136, iter [01100, 05004], lr: 0.025000, loss: 2.4181
2022-03-08 07:26:06 - train: epoch 0136, iter [01200, 05004], lr: 0.025000, loss: 2.5912
2022-03-08 07:26:40 - train: epoch 0136, iter [01300, 05004], lr: 0.025000, loss: 2.6625
2022-03-08 07:27:13 - train: epoch 0136, iter [01400, 05004], lr: 0.025000, loss: 2.5026
2022-03-08 07:27:48 - train: epoch 0136, iter [01500, 05004], lr: 0.025000, loss: 2.5754
2022-03-08 07:28:22 - train: epoch 0136, iter [01600, 05004], lr: 0.025000, loss: 2.2362
2022-03-08 07:28:56 - train: epoch 0136, iter [01700, 05004], lr: 0.025000, loss: 2.4923
2022-03-08 07:29:31 - train: epoch 0136, iter [01800, 05004], lr: 0.025000, loss: 2.2772
2022-03-08 07:30:05 - train: epoch 0136, iter [01900, 05004], lr: 0.025000, loss: 2.3413
2022-03-08 07:30:39 - train: epoch 0136, iter [02000, 05004], lr: 0.025000, loss: 2.4834
2022-03-08 07:31:13 - train: epoch 0136, iter [02100, 05004], lr: 0.025000, loss: 2.3450
2022-03-08 07:31:48 - train: epoch 0136, iter [02200, 05004], lr: 0.025000, loss: 2.5634
2022-03-08 07:32:22 - train: epoch 0136, iter [02300, 05004], lr: 0.025000, loss: 2.4644
2022-03-08 07:32:56 - train: epoch 0136, iter [02400, 05004], lr: 0.025000, loss: 1.9748
2022-03-08 07:33:30 - train: epoch 0136, iter [02500, 05004], lr: 0.025000, loss: 2.4689
2022-03-08 07:34:04 - train: epoch 0136, iter [02600, 05004], lr: 0.025000, loss: 2.3209
2022-03-08 07:34:38 - train: epoch 0136, iter [02700, 05004], lr: 0.025000, loss: 2.6571
2022-03-08 07:35:12 - train: epoch 0136, iter [02800, 05004], lr: 0.025000, loss: 2.4986
2022-03-08 07:35:46 - train: epoch 0136, iter [02900, 05004], lr: 0.025000, loss: 2.4400
2022-03-08 07:36:21 - train: epoch 0136, iter [03000, 05004], lr: 0.025000, loss: 2.6325
2022-03-08 07:36:55 - train: epoch 0136, iter [03100, 05004], lr: 0.025000, loss: 2.4595
2022-03-08 07:37:29 - train: epoch 0136, iter [03200, 05004], lr: 0.025000, loss: 2.5577
2022-03-08 07:38:03 - train: epoch 0136, iter [03300, 05004], lr: 0.025000, loss: 2.3770
2022-03-08 07:38:37 - train: epoch 0136, iter [03400, 05004], lr: 0.025000, loss: 2.5157
2022-03-08 07:39:11 - train: epoch 0136, iter [03500, 05004], lr: 0.025000, loss: 2.4369
2022-03-08 07:39:45 - train: epoch 0136, iter [03600, 05004], lr: 0.025000, loss: 2.5233
2022-03-08 07:40:19 - train: epoch 0136, iter [03700, 05004], lr: 0.025000, loss: 2.2660
2022-03-08 07:40:54 - train: epoch 0136, iter [03800, 05004], lr: 0.025000, loss: 2.5572
2022-03-08 07:41:28 - train: epoch 0136, iter [03900, 05004], lr: 0.025000, loss: 2.6201
2022-03-08 07:42:01 - train: epoch 0136, iter [04000, 05004], lr: 0.025000, loss: 2.6593
2022-03-08 07:42:35 - train: epoch 0136, iter [04100, 05004], lr: 0.025000, loss: 2.5372
2022-03-08 07:43:09 - train: epoch 0136, iter [04200, 05004], lr: 0.025000, loss: 2.3309
2022-03-08 07:43:43 - train: epoch 0136, iter [04300, 05004], lr: 0.025000, loss: 2.2911
2022-03-08 07:44:18 - train: epoch 0136, iter [04400, 05004], lr: 0.025000, loss: 2.2444
2022-03-08 07:44:53 - train: epoch 0136, iter [04500, 05004], lr: 0.025000, loss: 2.7316
2022-03-08 07:45:26 - train: epoch 0136, iter [04600, 05004], lr: 0.025000, loss: 2.7887
2022-03-08 07:46:00 - train: epoch 0136, iter [04700, 05004], lr: 0.025000, loss: 2.3014
2022-03-08 07:46:34 - train: epoch 0136, iter [04800, 05004], lr: 0.025000, loss: 2.5006
2022-03-08 07:47:08 - train: epoch 0136, iter [04900, 05004], lr: 0.025000, loss: 2.7386
2022-03-08 07:47:41 - train: epoch 0136, iter [05000, 05004], lr: 0.025000, loss: 2.6036
2022-03-08 07:47:42 - train: epoch 136, train_loss: 2.4646
2022-03-08 07:48:56 - eval: epoch: 136, acc1: 66.020%, acc5: 87.428%, test_loss: 1.3828, per_image_load_time: 0.907ms, per_image_inference_time: 0.532ms
2022-03-08 07:48:57 - until epoch: 136, best_acc1: 66.020%
2022-03-08 07:48:57 - epoch 137 lr: 0.02430565941356157
2022-03-08 07:49:36 - train: epoch 0137, iter [00100, 05004], lr: 0.024306, loss: 1.8412
2022-03-08 07:50:11 - train: epoch 0137, iter [00200, 05004], lr: 0.024306, loss: 2.3589
2022-03-08 07:50:45 - train: epoch 0137, iter [00300, 05004], lr: 0.024306, loss: 2.2383
2022-03-08 07:51:19 - train: epoch 0137, iter [00400, 05004], lr: 0.024306, loss: 2.3219
2022-03-08 07:51:54 - train: epoch 0137, iter [00500, 05004], lr: 0.024306, loss: 2.6096
2022-03-08 07:52:28 - train: epoch 0137, iter [00600, 05004], lr: 0.024306, loss: 2.5834
2022-03-08 07:53:02 - train: epoch 0137, iter [00700, 05004], lr: 0.024306, loss: 2.4855
2022-03-08 07:53:37 - train: epoch 0137, iter [00800, 05004], lr: 0.024306, loss: 2.2118
2022-03-08 07:54:11 - train: epoch 0137, iter [00900, 05004], lr: 0.024306, loss: 2.4675
2022-03-08 07:54:45 - train: epoch 0137, iter [01000, 05004], lr: 0.024306, loss: 2.3504
2022-03-08 07:55:20 - train: epoch 0137, iter [01100, 05004], lr: 0.024306, loss: 2.6796
2022-03-08 07:55:54 - train: epoch 0137, iter [01200, 05004], lr: 0.024306, loss: 2.3289
2022-03-08 07:56:28 - train: epoch 0137, iter [01300, 05004], lr: 0.024306, loss: 2.0339
2022-03-08 07:57:02 - train: epoch 0137, iter [01400, 05004], lr: 0.024306, loss: 2.5722
2022-03-08 07:57:36 - train: epoch 0137, iter [01500, 05004], lr: 0.024306, loss: 2.7606
2022-03-08 07:58:11 - train: epoch 0137, iter [01600, 05004], lr: 0.024306, loss: 2.4117
2022-03-08 07:58:46 - train: epoch 0137, iter [01700, 05004], lr: 0.024306, loss: 2.4291
2022-03-08 07:59:19 - train: epoch 0137, iter [01800, 05004], lr: 0.024306, loss: 2.4840
2022-03-08 07:59:54 - train: epoch 0137, iter [01900, 05004], lr: 0.024306, loss: 2.2755
2022-03-08 08:00:28 - train: epoch 0137, iter [02000, 05004], lr: 0.024306, loss: 2.2895
2022-03-08 08:01:02 - train: epoch 0137, iter [02100, 05004], lr: 0.024306, loss: 2.3032
2022-03-08 08:01:36 - train: epoch 0137, iter [02200, 05004], lr: 0.024306, loss: 2.2295
2022-03-08 08:02:11 - train: epoch 0137, iter [02300, 05004], lr: 0.024306, loss: 2.2295
2022-03-08 08:02:45 - train: epoch 0137, iter [02400, 05004], lr: 0.024306, loss: 2.3161
2022-03-08 08:03:19 - train: epoch 0137, iter [02500, 05004], lr: 0.024306, loss: 2.3483
2022-03-08 08:03:54 - train: epoch 0137, iter [02600, 05004], lr: 0.024306, loss: 2.3187
2022-03-08 08:04:27 - train: epoch 0137, iter [02700, 05004], lr: 0.024306, loss: 2.3497
2022-03-08 08:05:01 - train: epoch 0137, iter [02800, 05004], lr: 0.024306, loss: 2.3474
2022-03-08 08:05:36 - train: epoch 0137, iter [02900, 05004], lr: 0.024306, loss: 2.4799
2022-03-08 08:06:10 - train: epoch 0137, iter [03000, 05004], lr: 0.024306, loss: 2.4186
2022-03-08 08:06:44 - train: epoch 0137, iter [03100, 05004], lr: 0.024306, loss: 2.6464
2022-03-08 08:07:18 - train: epoch 0137, iter [03200, 05004], lr: 0.024306, loss: 2.4411
2022-03-08 08:07:53 - train: epoch 0137, iter [03300, 05004], lr: 0.024306, loss: 2.3559
2022-03-08 08:08:26 - train: epoch 0137, iter [03400, 05004], lr: 0.024306, loss: 2.7118
2022-03-08 08:09:01 - train: epoch 0137, iter [03500, 05004], lr: 0.024306, loss: 2.2952
2022-03-08 08:09:36 - train: epoch 0137, iter [03600, 05004], lr: 0.024306, loss: 2.3390
2022-03-08 08:10:10 - train: epoch 0137, iter [03700, 05004], lr: 0.024306, loss: 2.5352
2022-03-08 08:10:44 - train: epoch 0137, iter [03800, 05004], lr: 0.024306, loss: 2.5800
2022-03-08 08:11:19 - train: epoch 0137, iter [03900, 05004], lr: 0.024306, loss: 2.4873
2022-03-08 08:11:53 - train: epoch 0137, iter [04000, 05004], lr: 0.024306, loss: 2.4941
2022-03-08 08:12:27 - train: epoch 0137, iter [04100, 05004], lr: 0.024306, loss: 2.3234
2022-03-08 08:13:01 - train: epoch 0137, iter [04200, 05004], lr: 0.024306, loss: 2.8579
2022-03-08 08:13:36 - train: epoch 0137, iter [04300, 05004], lr: 0.024306, loss: 2.4002
2022-03-08 08:14:10 - train: epoch 0137, iter [04400, 05004], lr: 0.024306, loss: 2.6802
2022-03-08 08:14:44 - train: epoch 0137, iter [04500, 05004], lr: 0.024306, loss: 2.6742
2022-03-08 08:15:19 - train: epoch 0137, iter [04600, 05004], lr: 0.024306, loss: 2.2768
2022-03-08 08:15:53 - train: epoch 0137, iter [04700, 05004], lr: 0.024306, loss: 2.3759
2022-03-08 08:16:26 - train: epoch 0137, iter [04800, 05004], lr: 0.024306, loss: 2.6756
2022-03-08 08:17:01 - train: epoch 0137, iter [04900, 05004], lr: 0.024306, loss: 2.4648
2022-03-08 08:17:34 - train: epoch 0137, iter [05000, 05004], lr: 0.024306, loss: 2.4091
2022-03-08 08:17:35 - train: epoch 137, train_loss: 2.4517
2022-03-08 08:18:49 - eval: epoch: 137, acc1: 66.320%, acc5: 87.834%, test_loss: 1.3682, per_image_load_time: 1.583ms, per_image_inference_time: 0.526ms
2022-03-08 08:18:50 - until epoch: 137, best_acc1: 66.320%
2022-03-08 08:18:50 - epoch 138 lr: 0.02361798779469336
2022-03-08 08:19:30 - train: epoch 0138, iter [00100, 05004], lr: 0.023618, loss: 2.2705
2022-03-08 08:20:03 - train: epoch 0138, iter [00200, 05004], lr: 0.023618, loss: 2.4667
2022-03-08 08:20:38 - train: epoch 0138, iter [00300, 05004], lr: 0.023618, loss: 2.4417
2022-03-08 08:21:13 - train: epoch 0138, iter [00400, 05004], lr: 0.023618, loss: 2.4553
2022-03-08 08:21:47 - train: epoch 0138, iter [00500, 05004], lr: 0.023618, loss: 2.3514
2022-03-08 08:22:21 - train: epoch 0138, iter [00600, 05004], lr: 0.023618, loss: 2.6548
2022-03-08 08:22:55 - train: epoch 0138, iter [00700, 05004], lr: 0.023618, loss: 2.2551
2022-03-08 08:23:29 - train: epoch 0138, iter [00800, 05004], lr: 0.023618, loss: 2.2637
2022-03-08 08:24:03 - train: epoch 0138, iter [00900, 05004], lr: 0.023618, loss: 2.6273
2022-03-08 08:24:37 - train: epoch 0138, iter [01000, 05004], lr: 0.023618, loss: 2.2245
2022-03-08 08:25:12 - train: epoch 0138, iter [01100, 05004], lr: 0.023618, loss: 2.6930
2022-03-08 08:25:46 - train: epoch 0138, iter [01200, 05004], lr: 0.023618, loss: 2.5955
2022-03-08 08:26:20 - train: epoch 0138, iter [01300, 05004], lr: 0.023618, loss: 2.4539
2022-03-08 08:26:54 - train: epoch 0138, iter [01400, 05004], lr: 0.023618, loss: 2.4185
2022-03-08 08:27:28 - train: epoch 0138, iter [01500, 05004], lr: 0.023618, loss: 2.2648
2022-03-08 08:28:03 - train: epoch 0138, iter [01600, 05004], lr: 0.023618, loss: 2.5547
2022-03-08 08:28:37 - train: epoch 0138, iter [01700, 05004], lr: 0.023618, loss: 2.1559
2022-03-08 08:29:11 - train: epoch 0138, iter [01800, 05004], lr: 0.023618, loss: 2.3513
2022-03-08 08:29:45 - train: epoch 0138, iter [01900, 05004], lr: 0.023618, loss: 2.5490
2022-03-08 08:30:19 - train: epoch 0138, iter [02000, 05004], lr: 0.023618, loss: 2.4703
2022-03-08 08:30:54 - train: epoch 0138, iter [02100, 05004], lr: 0.023618, loss: 2.5527
2022-03-08 08:31:28 - train: epoch 0138, iter [02200, 05004], lr: 0.023618, loss: 1.9458
2022-03-08 08:32:03 - train: epoch 0138, iter [02300, 05004], lr: 0.023618, loss: 2.5342
2022-03-08 08:32:36 - train: epoch 0138, iter [02400, 05004], lr: 0.023618, loss: 2.0544
2022-03-08 08:33:10 - train: epoch 0138, iter [02500, 05004], lr: 0.023618, loss: 2.2938
2022-03-08 08:33:45 - train: epoch 0138, iter [02600, 05004], lr: 0.023618, loss: 2.7530
2022-03-08 08:34:19 - train: epoch 0138, iter [02700, 05004], lr: 0.023618, loss: 2.5716
2022-03-08 08:34:52 - train: epoch 0138, iter [02800, 05004], lr: 0.023618, loss: 2.4594
2022-03-08 08:35:27 - train: epoch 0138, iter [02900, 05004], lr: 0.023618, loss: 2.5253
2022-03-08 08:36:01 - train: epoch 0138, iter [03000, 05004], lr: 0.023618, loss: 2.5270
2022-03-08 08:36:36 - train: epoch 0138, iter [03100, 05004], lr: 0.023618, loss: 2.4503
2022-03-08 08:37:10 - train: epoch 0138, iter [03200, 05004], lr: 0.023618, loss: 2.5728
2022-03-08 08:37:43 - train: epoch 0138, iter [03300, 05004], lr: 0.023618, loss: 2.7192
2022-03-08 08:38:17 - train: epoch 0138, iter [03400, 05004], lr: 0.023618, loss: 2.2262
2022-03-08 08:38:52 - train: epoch 0138, iter [03500, 05004], lr: 0.023618, loss: 2.2851
2022-03-08 08:39:26 - train: epoch 0138, iter [03600, 05004], lr: 0.023618, loss: 2.5499
2022-03-08 08:40:00 - train: epoch 0138, iter [03700, 05004], lr: 0.023618, loss: 2.6301
2022-03-08 08:40:35 - train: epoch 0138, iter [03800, 05004], lr: 0.023618, loss: 2.5306
2022-03-08 08:41:09 - train: epoch 0138, iter [03900, 05004], lr: 0.023618, loss: 2.6169
2022-03-08 08:41:44 - train: epoch 0138, iter [04000, 05004], lr: 0.023618, loss: 2.4653
2022-03-08 08:42:18 - train: epoch 0138, iter [04100, 05004], lr: 0.023618, loss: 2.4276
2022-03-08 08:42:52 - train: epoch 0138, iter [04200, 05004], lr: 0.023618, loss: 2.3799
2022-03-08 08:43:26 - train: epoch 0138, iter [04300, 05004], lr: 0.023618, loss: 2.2625
2022-03-08 08:44:00 - train: epoch 0138, iter [04400, 05004], lr: 0.023618, loss: 2.3934
2022-03-08 08:44:35 - train: epoch 0138, iter [04500, 05004], lr: 0.023618, loss: 2.4678
2022-03-08 08:45:09 - train: epoch 0138, iter [04600, 05004], lr: 0.023618, loss: 2.3881
2022-03-08 08:45:43 - train: epoch 0138, iter [04700, 05004], lr: 0.023618, loss: 2.1443
2022-03-08 08:46:18 - train: epoch 0138, iter [04800, 05004], lr: 0.023618, loss: 2.5147
2022-03-08 08:46:52 - train: epoch 0138, iter [04900, 05004], lr: 0.023618, loss: 2.5926
2022-03-08 08:47:25 - train: epoch 0138, iter [05000, 05004], lr: 0.023618, loss: 2.6603
2022-03-08 08:47:26 - train: epoch 138, train_loss: 2.4448
2022-03-08 08:48:41 - eval: epoch: 138, acc1: 66.104%, acc5: 87.392%, test_loss: 1.3805, per_image_load_time: 1.152ms, per_image_inference_time: 0.532ms
2022-03-08 08:48:41 - until epoch: 138, best_acc1: 66.320%
2022-03-08 08:48:41 - epoch 139 lr: 0.022937163628603437
2022-03-08 08:49:21 - train: epoch 0139, iter [00100, 05004], lr: 0.022937, loss: 2.3475
2022-03-08 08:49:55 - train: epoch 0139, iter [00200, 05004], lr: 0.022937, loss: 2.2187
2022-03-08 08:50:28 - train: epoch 0139, iter [00300, 05004], lr: 0.022937, loss: 2.1779
2022-03-08 08:51:02 - train: epoch 0139, iter [00400, 05004], lr: 0.022937, loss: 2.5002
2022-03-08 08:51:36 - train: epoch 0139, iter [00500, 05004], lr: 0.022937, loss: 2.6282
2022-03-08 08:52:10 - train: epoch 0139, iter [00600, 05004], lr: 0.022937, loss: 2.5774
2022-03-08 08:52:44 - train: epoch 0139, iter [00700, 05004], lr: 0.022937, loss: 2.3537
2022-03-08 08:53:19 - train: epoch 0139, iter [00800, 05004], lr: 0.022937, loss: 2.3678
2022-03-08 08:53:53 - train: epoch 0139, iter [00900, 05004], lr: 0.022937, loss: 2.5623
2022-03-08 08:54:28 - train: epoch 0139, iter [01000, 05004], lr: 0.022937, loss: 2.5061
2022-03-08 08:55:02 - train: epoch 0139, iter [01100, 05004], lr: 0.022937, loss: 2.4721
2022-03-08 08:55:35 - train: epoch 0139, iter [01200, 05004], lr: 0.022937, loss: 2.6142
2022-03-08 08:56:10 - train: epoch 0139, iter [01300, 05004], lr: 0.022937, loss: 2.4480
2022-03-08 08:56:44 - train: epoch 0139, iter [01400, 05004], lr: 0.022937, loss: 1.9888
2022-03-08 08:57:18 - train: epoch 0139, iter [01500, 05004], lr: 0.022937, loss: 2.5383
2022-03-08 08:57:51 - train: epoch 0139, iter [01600, 05004], lr: 0.022937, loss: 2.4605
2022-03-08 08:58:26 - train: epoch 0139, iter [01700, 05004], lr: 0.022937, loss: 2.2256
2022-03-08 08:59:01 - train: epoch 0139, iter [01800, 05004], lr: 0.022937, loss: 2.2807
2022-03-08 08:59:34 - train: epoch 0139, iter [01900, 05004], lr: 0.022937, loss: 2.7282
2022-03-08 09:00:08 - train: epoch 0139, iter [02000, 05004], lr: 0.022937, loss: 2.6289
2022-03-08 09:00:43 - train: epoch 0139, iter [02100, 05004], lr: 0.022937, loss: 2.2756
2022-03-08 09:01:16 - train: epoch 0139, iter [02200, 05004], lr: 0.022937, loss: 2.4984
2022-03-08 09:01:50 - train: epoch 0139, iter [02300, 05004], lr: 0.022937, loss: 2.3250
2022-03-08 09:02:24 - train: epoch 0139, iter [02400, 05004], lr: 0.022937, loss: 2.5952
2022-03-08 09:02:59 - train: epoch 0139, iter [02500, 05004], lr: 0.022937, loss: 2.1983
2022-03-08 09:03:32 - train: epoch 0139, iter [02600, 05004], lr: 0.022937, loss: 2.6107
2022-03-08 09:04:07 - train: epoch 0139, iter [02700, 05004], lr: 0.022937, loss: 2.6024
2022-03-08 09:04:42 - train: epoch 0139, iter [02800, 05004], lr: 0.022937, loss: 2.3023
2022-03-08 09:05:16 - train: epoch 0139, iter [02900, 05004], lr: 0.022937, loss: 2.5136
2022-03-08 09:05:50 - train: epoch 0139, iter [03000, 05004], lr: 0.022937, loss: 2.0891
2022-03-08 09:06:24 - train: epoch 0139, iter [03100, 05004], lr: 0.022937, loss: 2.6810
2022-03-08 09:06:58 - train: epoch 0139, iter [03200, 05004], lr: 0.022937, loss: 2.2428
2022-03-08 09:07:32 - train: epoch 0139, iter [03300, 05004], lr: 0.022937, loss: 2.2739
2022-03-08 09:08:07 - train: epoch 0139, iter [03400, 05004], lr: 0.022937, loss: 2.4447
2022-03-08 09:08:41 - train: epoch 0139, iter [03500, 05004], lr: 0.022937, loss: 2.6604
2022-03-08 09:09:14 - train: epoch 0139, iter [03600, 05004], lr: 0.022937, loss: 2.2653
2022-03-08 09:09:49 - train: epoch 0139, iter [03700, 05004], lr: 0.022937, loss: 2.4382
2022-03-08 09:10:23 - train: epoch 0139, iter [03800, 05004], lr: 0.022937, loss: 2.5390
2022-03-08 09:10:58 - train: epoch 0139, iter [03900, 05004], lr: 0.022937, loss: 2.5726
2022-03-08 09:11:31 - train: epoch 0139, iter [04000, 05004], lr: 0.022937, loss: 2.5158
2022-03-08 09:12:05 - train: epoch 0139, iter [04100, 05004], lr: 0.022937, loss: 2.6365
2022-03-08 09:12:40 - train: epoch 0139, iter [04200, 05004], lr: 0.022937, loss: 2.6601
2022-03-08 09:13:14 - train: epoch 0139, iter [04300, 05004], lr: 0.022937, loss: 2.3140
2022-03-08 09:13:48 - train: epoch 0139, iter [04400, 05004], lr: 0.022937, loss: 2.7337
2022-03-08 09:14:22 - train: epoch 0139, iter [04500, 05004], lr: 0.022937, loss: 2.6440
2022-03-08 09:14:56 - train: epoch 0139, iter [04600, 05004], lr: 0.022937, loss: 2.5951
2022-03-08 09:15:31 - train: epoch 0139, iter [04700, 05004], lr: 0.022937, loss: 2.4191
2022-03-08 09:16:04 - train: epoch 0139, iter [04800, 05004], lr: 0.022937, loss: 2.2266
2022-03-08 09:16:38 - train: epoch 0139, iter [04900, 05004], lr: 0.022937, loss: 2.5807
2022-03-08 09:17:11 - train: epoch 0139, iter [05000, 05004], lr: 0.022937, loss: 2.6864
2022-03-08 09:17:12 - train: epoch 139, train_loss: 2.4295
2022-03-08 09:18:27 - eval: epoch: 139, acc1: 65.614%, acc5: 87.308%, test_loss: 1.3960, per_image_load_time: 0.753ms, per_image_inference_time: 0.517ms
2022-03-08 09:18:28 - until epoch: 139, best_acc1: 66.320%
2022-03-08 09:18:28 - epoch 140 lr: 0.022263363623243056
2022-03-08 09:19:07 - train: epoch 0140, iter [00100, 05004], lr: 0.022263, loss: 2.3729
2022-03-08 09:19:42 - train: epoch 0140, iter [00200, 05004], lr: 0.022263, loss: 2.5824
2022-03-08 09:20:16 - train: epoch 0140, iter [00300, 05004], lr: 0.022263, loss: 2.2399
2022-03-08 09:20:50 - train: epoch 0140, iter [00400, 05004], lr: 0.022263, loss: 2.2508
2022-03-08 09:21:24 - train: epoch 0140, iter [00500, 05004], lr: 0.022263, loss: 2.3349
2022-03-08 09:21:59 - train: epoch 0140, iter [00600, 05004], lr: 0.022263, loss: 2.5980
2022-03-08 09:22:33 - train: epoch 0140, iter [00700, 05004], lr: 0.022263, loss: 2.1038
2022-03-08 09:23:07 - train: epoch 0140, iter [00800, 05004], lr: 0.022263, loss: 2.5750
2022-03-08 09:23:41 - train: epoch 0140, iter [00900, 05004], lr: 0.022263, loss: 2.4626
2022-03-08 09:24:15 - train: epoch 0140, iter [01000, 05004], lr: 0.022263, loss: 2.3718
2022-03-08 09:24:50 - train: epoch 0140, iter [01100, 05004], lr: 0.022263, loss: 2.3785
2022-03-08 09:25:25 - train: epoch 0140, iter [01200, 05004], lr: 0.022263, loss: 2.6327
2022-03-08 09:25:59 - train: epoch 0140, iter [01300, 05004], lr: 0.022263, loss: 2.5738
2022-03-08 09:26:33 - train: epoch 0140, iter [01400, 05004], lr: 0.022263, loss: 2.4898
2022-03-08 09:27:07 - train: epoch 0140, iter [01500, 05004], lr: 0.022263, loss: 2.5270
2022-03-08 09:27:43 - train: epoch 0140, iter [01600, 05004], lr: 0.022263, loss: 2.4389
2022-03-08 09:28:16 - train: epoch 0140, iter [01700, 05004], lr: 0.022263, loss: 2.5523
2022-03-08 09:28:51 - train: epoch 0140, iter [01800, 05004], lr: 0.022263, loss: 2.2975
2022-03-08 09:29:25 - train: epoch 0140, iter [01900, 05004], lr: 0.022263, loss: 2.5932
2022-03-08 09:29:59 - train: epoch 0140, iter [02000, 05004], lr: 0.022263, loss: 2.4499
2022-03-08 09:30:33 - train: epoch 0140, iter [02100, 05004], lr: 0.022263, loss: 2.4083
2022-03-08 09:31:07 - train: epoch 0140, iter [02200, 05004], lr: 0.022263, loss: 2.3817
2022-03-08 09:31:42 - train: epoch 0140, iter [02300, 05004], lr: 0.022263, loss: 2.3231
2022-03-08 09:32:16 - train: epoch 0140, iter [02400, 05004], lr: 0.022263, loss: 2.5253
2022-03-08 09:32:50 - train: epoch 0140, iter [02500, 05004], lr: 0.022263, loss: 2.4588
2022-03-08 09:33:24 - train: epoch 0140, iter [02600, 05004], lr: 0.022263, loss: 2.1292
2022-03-08 09:33:59 - train: epoch 0140, iter [02700, 05004], lr: 0.022263, loss: 2.3850
2022-03-08 09:34:34 - train: epoch 0140, iter [02800, 05004], lr: 0.022263, loss: 2.3972
2022-03-08 09:35:07 - train: epoch 0140, iter [02900, 05004], lr: 0.022263, loss: 2.5220
2022-03-08 09:35:42 - train: epoch 0140, iter [03000, 05004], lr: 0.022263, loss: 2.5423
2022-03-08 09:36:16 - train: epoch 0140, iter [03100, 05004], lr: 0.022263, loss: 2.4017
2022-03-08 09:36:50 - train: epoch 0140, iter [03200, 05004], lr: 0.022263, loss: 2.3896
2022-03-08 09:37:23 - train: epoch 0140, iter [03300, 05004], lr: 0.022263, loss: 2.7247
2022-03-08 09:37:58 - train: epoch 0140, iter [03400, 05004], lr: 0.022263, loss: 2.5265
2022-03-08 09:38:32 - train: epoch 0140, iter [03500, 05004], lr: 0.022263, loss: 2.3500
2022-03-08 09:39:06 - train: epoch 0140, iter [03600, 05004], lr: 0.022263, loss: 2.2846
2022-03-08 09:39:40 - train: epoch 0140, iter [03700, 05004], lr: 0.022263, loss: 2.1670
2022-03-08 09:40:14 - train: epoch 0140, iter [03800, 05004], lr: 0.022263, loss: 2.3832
2022-03-08 09:40:48 - train: epoch 0140, iter [03900, 05004], lr: 0.022263, loss: 2.2745
2022-03-08 09:41:23 - train: epoch 0140, iter [04000, 05004], lr: 0.022263, loss: 2.3289
2022-03-08 09:41:57 - train: epoch 0140, iter [04100, 05004], lr: 0.022263, loss: 2.4037
2022-03-08 09:42:31 - train: epoch 0140, iter [04200, 05004], lr: 0.022263, loss: 2.4020
2022-03-08 09:43:06 - train: epoch 0140, iter [04300, 05004], lr: 0.022263, loss: 2.3537
2022-03-08 09:43:39 - train: epoch 0140, iter [04400, 05004], lr: 0.022263, loss: 2.6049
2022-03-08 09:44:14 - train: epoch 0140, iter [04500, 05004], lr: 0.022263, loss: 2.4640
2022-03-08 09:44:49 - train: epoch 0140, iter [04600, 05004], lr: 0.022263, loss: 2.3477
2022-03-08 09:45:23 - train: epoch 0140, iter [04700, 05004], lr: 0.022263, loss: 2.3308
2022-03-08 09:45:57 - train: epoch 0140, iter [04800, 05004], lr: 0.022263, loss: 2.5523
2022-03-08 09:46:31 - train: epoch 0140, iter [04900, 05004], lr: 0.022263, loss: 2.5021
2022-03-08 09:47:04 - train: epoch 0140, iter [05000, 05004], lr: 0.022263, loss: 2.3193
2022-03-08 09:47:05 - train: epoch 140, train_loss: 2.4237
2022-03-08 09:48:20 - eval: epoch: 140, acc1: 65.802%, acc5: 87.480%, test_loss: 1.3954, per_image_load_time: 0.737ms, per_image_inference_time: 0.504ms
2022-03-08 09:48:20 - until epoch: 140, best_acc1: 66.320%
2022-03-08 09:48:20 - epoch 141 lr: 0.021596762663442216
2022-03-08 09:48:59 - train: epoch 0141, iter [00100, 05004], lr: 0.021597, loss: 2.3961
2022-03-08 09:49:34 - train: epoch 0141, iter [00200, 05004], lr: 0.021597, loss: 2.2820
2022-03-08 09:50:07 - train: epoch 0141, iter [00300, 05004], lr: 0.021597, loss: 2.4923
2022-03-08 09:50:41 - train: epoch 0141, iter [00400, 05004], lr: 0.021597, loss: 2.3158
2022-03-08 09:51:16 - train: epoch 0141, iter [00500, 05004], lr: 0.021597, loss: 2.3410
2022-03-08 09:51:50 - train: epoch 0141, iter [00600, 05004], lr: 0.021597, loss: 2.7257
2022-03-08 09:52:24 - train: epoch 0141, iter [00700, 05004], lr: 0.021597, loss: 2.5868
2022-03-08 09:52:58 - train: epoch 0141, iter [00800, 05004], lr: 0.021597, loss: 2.2281
2022-03-08 09:53:33 - train: epoch 0141, iter [00900, 05004], lr: 0.021597, loss: 2.4029
2022-03-08 09:54:07 - train: epoch 0141, iter [01000, 05004], lr: 0.021597, loss: 2.3946
2022-03-08 09:54:41 - train: epoch 0141, iter [01100, 05004], lr: 0.021597, loss: 2.4722
2022-03-08 09:55:15 - train: epoch 0141, iter [01200, 05004], lr: 0.021597, loss: 2.5508
2022-03-08 09:55:49 - train: epoch 0141, iter [01300, 05004], lr: 0.021597, loss: 2.4896
2022-03-08 09:56:24 - train: epoch 0141, iter [01400, 05004], lr: 0.021597, loss: 2.8395
2022-03-08 09:56:58 - train: epoch 0141, iter [01500, 05004], lr: 0.021597, loss: 2.6158
2022-03-08 09:57:33 - train: epoch 0141, iter [01600, 05004], lr: 0.021597, loss: 2.4926
2022-03-08 09:58:07 - train: epoch 0141, iter [01700, 05004], lr: 0.021597, loss: 2.4105
2022-03-08 09:58:42 - train: epoch 0141, iter [01800, 05004], lr: 0.021597, loss: 2.3647
2022-03-08 09:59:16 - train: epoch 0141, iter [01900, 05004], lr: 0.021597, loss: 2.1449
2022-03-08 09:59:50 - train: epoch 0141, iter [02000, 05004], lr: 0.021597, loss: 2.3236
2022-03-08 10:00:25 - train: epoch 0141, iter [02100, 05004], lr: 0.021597, loss: 2.2377
2022-03-08 10:00:59 - train: epoch 0141, iter [02200, 05004], lr: 0.021597, loss: 2.5480
2022-03-08 10:01:34 - train: epoch 0141, iter [02300, 05004], lr: 0.021597, loss: 2.3939
2022-03-08 10:02:08 - train: epoch 0141, iter [02400, 05004], lr: 0.021597, loss: 2.5837
2022-03-08 10:02:42 - train: epoch 0141, iter [02500, 05004], lr: 0.021597, loss: 2.7080
2022-03-08 10:03:16 - train: epoch 0141, iter [02600, 05004], lr: 0.021597, loss: 2.6370
2022-03-08 10:03:50 - train: epoch 0141, iter [02700, 05004], lr: 0.021597, loss: 2.5409
2022-03-08 10:04:25 - train: epoch 0141, iter [02800, 05004], lr: 0.021597, loss: 2.2887
2022-03-08 10:05:00 - train: epoch 0141, iter [02900, 05004], lr: 0.021597, loss: 2.2642
2022-03-08 10:05:34 - train: epoch 0141, iter [03000, 05004], lr: 0.021597, loss: 2.2136
2022-03-08 10:06:08 - train: epoch 0141, iter [03100, 05004], lr: 0.021597, loss: 2.4455
2022-03-08 10:06:42 - train: epoch 0141, iter [03200, 05004], lr: 0.021597, loss: 2.5513
2022-03-08 10:07:17 - train: epoch 0141, iter [03300, 05004], lr: 0.021597, loss: 2.4063
2022-03-08 10:07:51 - train: epoch 0141, iter [03400, 05004], lr: 0.021597, loss: 2.5522
2022-03-08 10:08:25 - train: epoch 0141, iter [03500, 05004], lr: 0.021597, loss: 2.6666
2022-03-08 10:08:59 - train: epoch 0141, iter [03600, 05004], lr: 0.021597, loss: 2.3226
2022-03-08 10:09:33 - train: epoch 0141, iter [03700, 05004], lr: 0.021597, loss: 2.4416
2022-03-08 10:10:07 - train: epoch 0141, iter [03800, 05004], lr: 0.021597, loss: 2.5877
2022-03-08 10:10:42 - train: epoch 0141, iter [03900, 05004], lr: 0.021597, loss: 2.4211
2022-03-08 10:11:16 - train: epoch 0141, iter [04000, 05004], lr: 0.021597, loss: 2.3438
2022-03-08 10:11:50 - train: epoch 0141, iter [04100, 05004], lr: 0.021597, loss: 2.4011
2022-03-08 10:12:24 - train: epoch 0141, iter [04200, 05004], lr: 0.021597, loss: 2.7548
2022-03-08 10:12:59 - train: epoch 0141, iter [04300, 05004], lr: 0.021597, loss: 2.5393
2022-03-08 10:13:33 - train: epoch 0141, iter [04400, 05004], lr: 0.021597, loss: 2.5717
2022-03-08 10:14:08 - train: epoch 0141, iter [04500, 05004], lr: 0.021597, loss: 2.6019
2022-03-08 10:14:43 - train: epoch 0141, iter [04600, 05004], lr: 0.021597, loss: 2.3439
2022-03-08 10:15:17 - train: epoch 0141, iter [04700, 05004], lr: 0.021597, loss: 2.4845
2022-03-08 10:15:51 - train: epoch 0141, iter [04800, 05004], lr: 0.021597, loss: 2.4031
2022-03-08 10:16:25 - train: epoch 0141, iter [04900, 05004], lr: 0.021597, loss: 2.5302
2022-03-08 10:16:58 - train: epoch 0141, iter [05000, 05004], lr: 0.021597, loss: 2.6542
2022-03-08 10:16:59 - train: epoch 141, train_loss: 2.4122
2022-03-08 10:18:13 - eval: epoch: 141, acc1: 66.408%, acc5: 87.830%, test_loss: 1.3672, per_image_load_time: 1.253ms, per_image_inference_time: 0.555ms
2022-03-08 10:18:14 - until epoch: 141, best_acc1: 66.408%
2022-03-08 10:18:14 - epoch 142 lr: 0.020937533765518185
2022-03-08 10:18:54 - train: epoch 0142, iter [00100, 05004], lr: 0.020938, loss: 2.3449
2022-03-08 10:19:28 - train: epoch 0142, iter [00200, 05004], lr: 0.020938, loss: 2.5797
2022-03-08 10:20:02 - train: epoch 0142, iter [00300, 05004], lr: 0.020938, loss: 2.3249
2022-03-08 10:20:36 - train: epoch 0142, iter [00400, 05004], lr: 0.020938, loss: 2.3723
2022-03-08 10:21:10 - train: epoch 0142, iter [00500, 05004], lr: 0.020938, loss: 2.2763
2022-03-08 10:21:46 - train: epoch 0142, iter [00600, 05004], lr: 0.020938, loss: 2.6165
2022-03-08 10:22:19 - train: epoch 0142, iter [00700, 05004], lr: 0.020938, loss: 2.3823
2022-03-08 10:22:54 - train: epoch 0142, iter [00800, 05004], lr: 0.020938, loss: 2.4355
2022-03-08 10:23:28 - train: epoch 0142, iter [00900, 05004], lr: 0.020938, loss: 2.2243
2022-03-08 10:24:02 - train: epoch 0142, iter [01000, 05004], lr: 0.020938, loss: 2.6047
2022-03-08 10:24:36 - train: epoch 0142, iter [01100, 05004], lr: 0.020938, loss: 2.0752
2022-03-08 10:25:11 - train: epoch 0142, iter [01200, 05004], lr: 0.020938, loss: 2.5463
2022-03-08 10:25:45 - train: epoch 0142, iter [01300, 05004], lr: 0.020938, loss: 2.3453
2022-03-08 10:26:20 - train: epoch 0142, iter [01400, 05004], lr: 0.020938, loss: 2.4052
2022-03-08 10:26:53 - train: epoch 0142, iter [01500, 05004], lr: 0.020938, loss: 2.4160
2022-03-08 10:27:27 - train: epoch 0142, iter [01600, 05004], lr: 0.020938, loss: 2.4688
2022-03-08 10:28:01 - train: epoch 0142, iter [01700, 05004], lr: 0.020938, loss: 2.3601
2022-03-08 10:28:36 - train: epoch 0142, iter [01800, 05004], lr: 0.020938, loss: 2.6023
2022-03-08 10:29:10 - train: epoch 0142, iter [01900, 05004], lr: 0.020938, loss: 2.3943
2022-03-08 10:29:45 - train: epoch 0142, iter [02000, 05004], lr: 0.020938, loss: 2.4845
2022-03-08 10:30:19 - train: epoch 0142, iter [02100, 05004], lr: 0.020938, loss: 2.2771
2022-03-08 10:30:53 - train: epoch 0142, iter [02200, 05004], lr: 0.020938, loss: 2.4444
2022-03-08 10:31:28 - train: epoch 0142, iter [02300, 05004], lr: 0.020938, loss: 2.3366
2022-03-08 10:32:03 - train: epoch 0142, iter [02400, 05004], lr: 0.020938, loss: 2.1901
2022-03-08 10:32:37 - train: epoch 0142, iter [02500, 05004], lr: 0.020938, loss: 2.3593
2022-03-08 10:33:11 - train: epoch 0142, iter [02600, 05004], lr: 0.020938, loss: 2.5409
2022-03-08 10:33:46 - train: epoch 0142, iter [02700, 05004], lr: 0.020938, loss: 2.5978
2022-03-08 10:34:20 - train: epoch 0142, iter [02800, 05004], lr: 0.020938, loss: 2.1186
2022-03-08 10:34:55 - train: epoch 0142, iter [02900, 05004], lr: 0.020938, loss: 2.2310
2022-03-08 10:35:29 - train: epoch 0142, iter [03000, 05004], lr: 0.020938, loss: 2.4203
2022-03-08 10:36:03 - train: epoch 0142, iter [03100, 05004], lr: 0.020938, loss: 2.2638
2022-03-08 10:36:37 - train: epoch 0142, iter [03200, 05004], lr: 0.020938, loss: 2.4929
2022-03-08 10:37:12 - train: epoch 0142, iter [03300, 05004], lr: 0.020938, loss: 2.3465
2022-03-08 10:37:47 - train: epoch 0142, iter [03400, 05004], lr: 0.020938, loss: 2.1172
2022-03-08 10:38:21 - train: epoch 0142, iter [03500, 05004], lr: 0.020938, loss: 2.3982
2022-03-08 10:38:56 - train: epoch 0142, iter [03600, 05004], lr: 0.020938, loss: 1.8705
2022-03-08 10:39:30 - train: epoch 0142, iter [03700, 05004], lr: 0.020938, loss: 2.3308
2022-03-08 10:40:04 - train: epoch 0142, iter [03800, 05004], lr: 0.020938, loss: 2.3621
2022-03-08 10:40:37 - train: epoch 0142, iter [03900, 05004], lr: 0.020938, loss: 2.3035
2022-03-08 10:41:12 - train: epoch 0142, iter [04000, 05004], lr: 0.020938, loss: 2.3807
2022-03-08 10:41:46 - train: epoch 0142, iter [04100, 05004], lr: 0.020938, loss: 2.5586
2022-03-08 10:42:20 - train: epoch 0142, iter [04200, 05004], lr: 0.020938, loss: 2.4217
2022-03-08 10:42:55 - train: epoch 0142, iter [04300, 05004], lr: 0.020938, loss: 2.3775
2022-03-08 10:43:28 - train: epoch 0142, iter [04400, 05004], lr: 0.020938, loss: 2.3861
2022-03-08 10:44:03 - train: epoch 0142, iter [04500, 05004], lr: 0.020938, loss: 2.1864
2022-03-08 10:44:38 - train: epoch 0142, iter [04600, 05004], lr: 0.020938, loss: 2.6175
2022-03-08 10:45:11 - train: epoch 0142, iter [04700, 05004], lr: 0.020938, loss: 2.5336
2022-03-08 10:45:46 - train: epoch 0142, iter [04800, 05004], lr: 0.020938, loss: 2.2969
2022-03-08 10:46:20 - train: epoch 0142, iter [04900, 05004], lr: 0.020938, loss: 2.5165
2022-03-08 10:46:52 - train: epoch 0142, iter [05000, 05004], lr: 0.020938, loss: 2.1661
2022-03-08 10:46:53 - train: epoch 142, train_loss: 2.4036
2022-03-08 10:48:08 - eval: epoch: 142, acc1: 66.314%, acc5: 87.722%, test_loss: 1.3723, per_image_load_time: 1.783ms, per_image_inference_time: 0.550ms
2022-03-08 10:48:09 - until epoch: 142, best_acc1: 66.408%
2022-03-08 10:48:09 - epoch 143 lr: 0.020285848032369137
2022-03-08 10:48:48 - train: epoch 0143, iter [00100, 05004], lr: 0.020286, loss: 2.8051
2022-03-08 10:49:22 - train: epoch 0143, iter [00200, 05004], lr: 0.020286, loss: 2.1927
2022-03-08 10:49:56 - train: epoch 0143, iter [00300, 05004], lr: 0.020286, loss: 2.2226
2022-03-08 10:50:30 - train: epoch 0143, iter [00400, 05004], lr: 0.020286, loss: 2.3424
2022-03-08 10:51:06 - train: epoch 0143, iter [00500, 05004], lr: 0.020286, loss: 2.4898
2022-03-08 10:51:41 - train: epoch 0143, iter [00600, 05004], lr: 0.020286, loss: 2.2103
2022-03-08 10:52:15 - train: epoch 0143, iter [00700, 05004], lr: 0.020286, loss: 2.2209
2022-03-08 10:52:48 - train: epoch 0143, iter [00800, 05004], lr: 0.020286, loss: 2.5354
2022-03-08 10:53:23 - train: epoch 0143, iter [00900, 05004], lr: 0.020286, loss: 2.5921
2022-03-08 10:53:58 - train: epoch 0143, iter [01000, 05004], lr: 0.020286, loss: 2.4657
2022-03-08 10:54:32 - train: epoch 0143, iter [01100, 05004], lr: 0.020286, loss: 2.4692
2022-03-08 10:55:07 - train: epoch 0143, iter [01200, 05004], lr: 0.020286, loss: 2.4700
2022-03-08 10:55:40 - train: epoch 0143, iter [01300, 05004], lr: 0.020286, loss: 2.3153
2022-03-08 10:56:14 - train: epoch 0143, iter [01400, 05004], lr: 0.020286, loss: 2.4308
2022-03-08 10:56:48 - train: epoch 0143, iter [01500, 05004], lr: 0.020286, loss: 2.4847
2022-03-08 10:57:23 - train: epoch 0143, iter [01600, 05004], lr: 0.020286, loss: 2.4376
2022-03-08 10:57:57 - train: epoch 0143, iter [01700, 05004], lr: 0.020286, loss: 2.3579
2022-03-08 10:58:31 - train: epoch 0143, iter [01800, 05004], lr: 0.020286, loss: 2.5365
2022-03-08 10:59:05 - train: epoch 0143, iter [01900, 05004], lr: 0.020286, loss: 2.1758
2022-03-08 10:59:40 - train: epoch 0143, iter [02000, 05004], lr: 0.020286, loss: 2.7213
2022-03-08 11:00:14 - train: epoch 0143, iter [02100, 05004], lr: 0.020286, loss: 2.4278
2022-03-08 11:00:48 - train: epoch 0143, iter [02200, 05004], lr: 0.020286, loss: 2.5478
2022-03-08 11:01:22 - train: epoch 0143, iter [02300, 05004], lr: 0.020286, loss: 2.4746
2022-03-08 11:01:57 - train: epoch 0143, iter [02400, 05004], lr: 0.020286, loss: 2.5047
2022-03-08 11:02:31 - train: epoch 0143, iter [02500, 05004], lr: 0.020286, loss: 2.4720
2022-03-08 11:03:05 - train: epoch 0143, iter [02600, 05004], lr: 0.020286, loss: 2.4003
2022-03-08 11:03:39 - train: epoch 0143, iter [02700, 05004], lr: 0.020286, loss: 2.3832
2022-03-08 11:04:12 - train: epoch 0143, iter [02800, 05004], lr: 0.020286, loss: 2.7007
2022-03-08 11:04:47 - train: epoch 0143, iter [02900, 05004], lr: 0.020286, loss: 2.3874
2022-03-08 11:05:21 - train: epoch 0143, iter [03000, 05004], lr: 0.020286, loss: 2.3630
2022-03-08 11:05:55 - train: epoch 0143, iter [03100, 05004], lr: 0.020286, loss: 2.2751
2022-03-08 11:06:30 - train: epoch 0143, iter [03200, 05004], lr: 0.020286, loss: 2.6976
2022-03-08 11:07:04 - train: epoch 0143, iter [03300, 05004], lr: 0.020286, loss: 2.4312
2022-03-08 11:07:38 - train: epoch 0143, iter [03400, 05004], lr: 0.020286, loss: 2.3500
2022-03-08 11:08:13 - train: epoch 0143, iter [03500, 05004], lr: 0.020286, loss: 2.8279
2022-03-08 11:08:46 - train: epoch 0143, iter [03600, 05004], lr: 0.020286, loss: 2.5764
2022-03-08 11:09:21 - train: epoch 0143, iter [03700, 05004], lr: 0.020286, loss: 2.8507
2022-03-08 11:09:55 - train: epoch 0143, iter [03800, 05004], lr: 0.020286, loss: 2.4733
2022-03-08 11:10:29 - train: epoch 0143, iter [03900, 05004], lr: 0.020286, loss: 2.2214
2022-03-08 11:11:03 - train: epoch 0143, iter [04000, 05004], lr: 0.020286, loss: 2.4830
2022-03-08 11:11:37 - train: epoch 0143, iter [04100, 05004], lr: 0.020286, loss: 2.3825
2022-03-08 11:12:12 - train: epoch 0143, iter [04200, 05004], lr: 0.020286, loss: 2.1868
2022-03-08 11:12:46 - train: epoch 0143, iter [04300, 05004], lr: 0.020286, loss: 2.6931
2022-03-08 11:13:20 - train: epoch 0143, iter [04400, 05004], lr: 0.020286, loss: 2.5016
2022-03-08 11:13:55 - train: epoch 0143, iter [04500, 05004], lr: 0.020286, loss: 2.3416
2022-03-08 11:14:29 - train: epoch 0143, iter [04600, 05004], lr: 0.020286, loss: 2.8852
2022-03-08 11:15:03 - train: epoch 0143, iter [04700, 05004], lr: 0.020286, loss: 2.4973
2022-03-08 11:15:37 - train: epoch 0143, iter [04800, 05004], lr: 0.020286, loss: 2.5979
2022-03-08 11:16:11 - train: epoch 0143, iter [04900, 05004], lr: 0.020286, loss: 2.1777
2022-03-08 11:16:45 - train: epoch 0143, iter [05000, 05004], lr: 0.020286, loss: 2.5650
2022-03-08 11:16:46 - train: epoch 143, train_loss: 2.3928
2022-03-08 11:18:01 - eval: epoch: 143, acc1: 66.192%, acc5: 87.640%, test_loss: 1.3782, per_image_load_time: 2.079ms, per_image_inference_time: 0.517ms
2022-03-08 11:18:01 - until epoch: 143, best_acc1: 66.408%
2022-03-08 11:18:01 - epoch 144 lr: 0.01964187460906444
2022-03-08 11:18:40 - train: epoch 0144, iter [00100, 05004], lr: 0.019642, loss: 2.1995
2022-03-08 11:19:15 - train: epoch 0144, iter [00200, 05004], lr: 0.019642, loss: 2.5635
2022-03-08 11:19:49 - train: epoch 0144, iter [00300, 05004], lr: 0.019642, loss: 2.5053
2022-03-08 11:20:22 - train: epoch 0144, iter [00400, 05004], lr: 0.019642, loss: 2.4025
2022-03-08 11:20:56 - train: epoch 0144, iter [00500, 05004], lr: 0.019642, loss: 2.4414
2022-03-08 11:21:30 - train: epoch 0144, iter [00600, 05004], lr: 0.019642, loss: 2.2718
2022-03-08 11:22:04 - train: epoch 0144, iter [00700, 05004], lr: 0.019642, loss: 2.1982
2022-03-08 11:22:38 - train: epoch 0144, iter [00800, 05004], lr: 0.019642, loss: 2.7975
2022-03-08 11:23:12 - train: epoch 0144, iter [00900, 05004], lr: 0.019642, loss: 2.6461
2022-03-08 11:23:46 - train: epoch 0144, iter [01000, 05004], lr: 0.019642, loss: 2.4212
2022-03-08 11:24:20 - train: epoch 0144, iter [01100, 05004], lr: 0.019642, loss: 2.4418
2022-03-08 11:24:54 - train: epoch 0144, iter [01200, 05004], lr: 0.019642, loss: 2.3756
2022-03-08 11:25:28 - train: epoch 0144, iter [01300, 05004], lr: 0.019642, loss: 2.5671
2022-03-08 11:26:02 - train: epoch 0144, iter [01400, 05004], lr: 0.019642, loss: 2.4931
2022-03-08 11:26:35 - train: epoch 0144, iter [01500, 05004], lr: 0.019642, loss: 2.3869
2022-03-08 11:27:09 - train: epoch 0144, iter [01600, 05004], lr: 0.019642, loss: 2.2184
2022-03-08 11:27:43 - train: epoch 0144, iter [01700, 05004], lr: 0.019642, loss: 2.3456
2022-03-08 11:28:17 - train: epoch 0144, iter [01800, 05004], lr: 0.019642, loss: 2.2365
2022-03-08 11:28:51 - train: epoch 0144, iter [01900, 05004], lr: 0.019642, loss: 2.2130
2022-03-08 11:29:25 - train: epoch 0144, iter [02000, 05004], lr: 0.019642, loss: 2.4704
2022-03-08 11:29:59 - train: epoch 0144, iter [02100, 05004], lr: 0.019642, loss: 2.6799
2022-03-08 11:30:33 - train: epoch 0144, iter [02200, 05004], lr: 0.019642, loss: 2.3473
2022-03-08 11:31:06 - train: epoch 0144, iter [02300, 05004], lr: 0.019642, loss: 2.0914
2022-03-08 11:31:40 - train: epoch 0144, iter [02400, 05004], lr: 0.019642, loss: 2.3422
2022-03-08 11:32:13 - train: epoch 0144, iter [02500, 05004], lr: 0.019642, loss: 2.3649
2022-03-08 11:32:46 - train: epoch 0144, iter [02600, 05004], lr: 0.019642, loss: 2.3631
2022-03-08 11:33:20 - train: epoch 0144, iter [02700, 05004], lr: 0.019642, loss: 2.2369
2022-03-08 11:33:54 - train: epoch 0144, iter [02800, 05004], lr: 0.019642, loss: 2.3789
2022-03-08 11:34:27 - train: epoch 0144, iter [02900, 05004], lr: 0.019642, loss: 2.5375
2022-03-08 11:35:01 - train: epoch 0144, iter [03000, 05004], lr: 0.019642, loss: 2.6482
2022-03-08 11:35:36 - train: epoch 0144, iter [03100, 05004], lr: 0.019642, loss: 2.2467
2022-03-08 11:36:09 - train: epoch 0144, iter [03200, 05004], lr: 0.019642, loss: 2.5829
2022-03-08 11:36:42 - train: epoch 0144, iter [03300, 05004], lr: 0.019642, loss: 2.5206
2022-03-08 11:37:16 - train: epoch 0144, iter [03400, 05004], lr: 0.019642, loss: 2.4046
2022-03-08 11:37:50 - train: epoch 0144, iter [03500, 05004], lr: 0.019642, loss: 2.3031
2022-03-08 11:38:23 - train: epoch 0144, iter [03600, 05004], lr: 0.019642, loss: 2.2804
2022-03-08 11:38:57 - train: epoch 0144, iter [03700, 05004], lr: 0.019642, loss: 2.2907
2022-03-08 11:39:30 - train: epoch 0144, iter [03800, 05004], lr: 0.019642, loss: 2.0418
2022-03-08 11:40:03 - train: epoch 0144, iter [03900, 05004], lr: 0.019642, loss: 2.3891
2022-03-08 11:40:38 - train: epoch 0144, iter [04000, 05004], lr: 0.019642, loss: 2.3633
2022-03-08 11:41:12 - train: epoch 0144, iter [04100, 05004], lr: 0.019642, loss: 2.6525
2022-03-08 11:41:45 - train: epoch 0144, iter [04200, 05004], lr: 0.019642, loss: 2.1212
2022-03-08 11:42:20 - train: epoch 0144, iter [04300, 05004], lr: 0.019642, loss: 2.4255
2022-03-08 11:42:53 - train: epoch 0144, iter [04400, 05004], lr: 0.019642, loss: 2.3704
2022-03-08 11:43:27 - train: epoch 0144, iter [04500, 05004], lr: 0.019642, loss: 2.3896
2022-03-08 11:44:00 - train: epoch 0144, iter [04600, 05004], lr: 0.019642, loss: 2.2736
2022-03-08 11:44:33 - train: epoch 0144, iter [04700, 05004], lr: 0.019642, loss: 2.4234
2022-03-08 11:45:07 - train: epoch 0144, iter [04800, 05004], lr: 0.019642, loss: 2.3247
2022-03-08 11:45:41 - train: epoch 0144, iter [04900, 05004], lr: 0.019642, loss: 2.8798
2022-03-08 11:46:13 - train: epoch 0144, iter [05000, 05004], lr: 0.019642, loss: 2.5225
2022-03-08 11:46:15 - train: epoch 144, train_loss: 2.3805
2022-03-08 11:47:28 - eval: epoch: 144, acc1: 67.684%, acc5: 88.540%, test_loss: 1.3160, per_image_load_time: 2.124ms, per_image_inference_time: 0.498ms
2022-03-08 11:47:29 - until epoch: 144, best_acc1: 67.684%
2022-03-08 11:47:29 - epoch 145 lr: 0.019005780638942982
2022-03-08 11:48:08 - train: epoch 0145, iter [00100, 05004], lr: 0.019006, loss: 2.1114
2022-03-08 11:48:42 - train: epoch 0145, iter [00200, 05004], lr: 0.019006, loss: 2.5925
2022-03-08 11:49:16 - train: epoch 0145, iter [00300, 05004], lr: 0.019006, loss: 2.5005
2022-03-08 11:49:51 - train: epoch 0145, iter [00400, 05004], lr: 0.019006, loss: 2.2014
2022-03-08 11:50:24 - train: epoch 0145, iter [00500, 05004], lr: 0.019006, loss: 2.3598
2022-03-08 11:50:57 - train: epoch 0145, iter [00600, 05004], lr: 0.019006, loss: 2.4881
2022-03-08 11:51:31 - train: epoch 0145, iter [00700, 05004], lr: 0.019006, loss: 2.4394
2022-03-08 11:52:05 - train: epoch 0145, iter [00800, 05004], lr: 0.019006, loss: 2.3326
2022-03-08 11:52:39 - train: epoch 0145, iter [00900, 05004], lr: 0.019006, loss: 2.3229
2022-03-08 11:53:13 - train: epoch 0145, iter [01000, 05004], lr: 0.019006, loss: 2.3455
2022-03-08 11:53:47 - train: epoch 0145, iter [01100, 05004], lr: 0.019006, loss: 2.4884
2022-03-08 11:54:20 - train: epoch 0145, iter [01200, 05004], lr: 0.019006, loss: 2.7179
2022-03-08 11:54:54 - train: epoch 0145, iter [01300, 05004], lr: 0.019006, loss: 2.6168
2022-03-08 11:55:28 - train: epoch 0145, iter [01400, 05004], lr: 0.019006, loss: 2.3534
2022-03-08 11:56:02 - train: epoch 0145, iter [01500, 05004], lr: 0.019006, loss: 2.5602
2022-03-08 11:56:35 - train: epoch 0145, iter [01600, 05004], lr: 0.019006, loss: 2.2262
2022-03-08 11:57:10 - train: epoch 0145, iter [01700, 05004], lr: 0.019006, loss: 2.7969
2022-03-08 11:57:44 - train: epoch 0145, iter [01800, 05004], lr: 0.019006, loss: 2.1996
2022-03-08 11:58:18 - train: epoch 0145, iter [01900, 05004], lr: 0.019006, loss: 2.5971
2022-03-08 11:58:52 - train: epoch 0145, iter [02000, 05004], lr: 0.019006, loss: 2.2161
2022-03-08 11:59:26 - train: epoch 0145, iter [02100, 05004], lr: 0.019006, loss: 2.5014
2022-03-08 11:59:59 - train: epoch 0145, iter [02200, 05004], lr: 0.019006, loss: 2.2713
2022-03-08 12:00:33 - train: epoch 0145, iter [02300, 05004], lr: 0.019006, loss: 2.2220
2022-03-08 12:01:07 - train: epoch 0145, iter [02400, 05004], lr: 0.019006, loss: 2.4716
2022-03-08 12:01:40 - train: epoch 0145, iter [02500, 05004], lr: 0.019006, loss: 2.1248
2022-03-08 12:02:15 - train: epoch 0145, iter [02600, 05004], lr: 0.019006, loss: 2.2597
2022-03-08 12:02:48 - train: epoch 0145, iter [02700, 05004], lr: 0.019006, loss: 2.3248
2022-03-08 12:03:23 - train: epoch 0145, iter [02800, 05004], lr: 0.019006, loss: 2.5983
2022-03-08 12:03:56 - train: epoch 0145, iter [02900, 05004], lr: 0.019006, loss: 2.5082
2022-03-08 12:04:30 - train: epoch 0145, iter [03000, 05004], lr: 0.019006, loss: 2.1660
2022-03-08 12:05:03 - train: epoch 0145, iter [03100, 05004], lr: 0.019006, loss: 2.6286
2022-03-08 12:05:37 - train: epoch 0145, iter [03200, 05004], lr: 0.019006, loss: 2.5929
2022-03-08 12:06:11 - train: epoch 0145, iter [03300, 05004], lr: 0.019006, loss: 2.4654
2022-03-08 12:06:45 - train: epoch 0145, iter [03400, 05004], lr: 0.019006, loss: 2.2387
2022-03-08 12:07:18 - train: epoch 0145, iter [03500, 05004], lr: 0.019006, loss: 2.0357
2022-03-08 12:07:53 - train: epoch 0145, iter [03600, 05004], lr: 0.019006, loss: 2.6294
2022-03-08 12:08:26 - train: epoch 0145, iter [03700, 05004], lr: 0.019006, loss: 2.1159
2022-03-08 12:09:00 - train: epoch 0145, iter [03800, 05004], lr: 0.019006, loss: 2.4192
2022-03-08 12:09:34 - train: epoch 0145, iter [03900, 05004], lr: 0.019006, loss: 2.3305
2022-03-08 12:10:08 - train: epoch 0145, iter [04000, 05004], lr: 0.019006, loss: 2.2470
2022-03-08 12:10:42 - train: epoch 0145, iter [04100, 05004], lr: 0.019006, loss: 2.4071
2022-03-08 12:11:16 - train: epoch 0145, iter [04200, 05004], lr: 0.019006, loss: 2.6348
2022-03-08 12:11:50 - train: epoch 0145, iter [04300, 05004], lr: 0.019006, loss: 2.3631
2022-03-08 12:12:23 - train: epoch 0145, iter [04400, 05004], lr: 0.019006, loss: 2.1999
2022-03-08 12:12:56 - train: epoch 0145, iter [04500, 05004], lr: 0.019006, loss: 2.0950
2022-03-08 12:13:31 - train: epoch 0145, iter [04600, 05004], lr: 0.019006, loss: 2.3905
2022-03-08 12:14:05 - train: epoch 0145, iter [04700, 05004], lr: 0.019006, loss: 2.3442
2022-03-08 12:14:38 - train: epoch 0145, iter [04800, 05004], lr: 0.019006, loss: 2.3907
2022-03-08 12:15:12 - train: epoch 0145, iter [04900, 05004], lr: 0.019006, loss: 2.5120
2022-03-08 12:15:44 - train: epoch 0145, iter [05000, 05004], lr: 0.019006, loss: 2.4556
2022-03-08 12:15:45 - train: epoch 145, train_loss: 2.3717
2022-03-08 12:16:59 - eval: epoch: 145, acc1: 66.680%, acc5: 87.724%, test_loss: 1.3593, per_image_load_time: 1.992ms, per_image_inference_time: 0.509ms
2022-03-08 12:16:59 - until epoch: 145, best_acc1: 67.684%
2022-03-08 12:16:59 - epoch 146 lr: 0.018377731220231144
2022-03-08 12:17:39 - train: epoch 0146, iter [00100, 05004], lr: 0.018378, loss: 2.6227
2022-03-08 12:18:12 - train: epoch 0146, iter [00200, 05004], lr: 0.018378, loss: 2.3190
2022-03-08 12:18:46 - train: epoch 0146, iter [00300, 05004], lr: 0.018378, loss: 2.3298
2022-03-08 12:19:20 - train: epoch 0146, iter [00400, 05004], lr: 0.018378, loss: 2.4364
2022-03-08 12:19:54 - train: epoch 0146, iter [00500, 05004], lr: 0.018378, loss: 2.2570
2022-03-08 12:20:29 - train: epoch 0146, iter [00600, 05004], lr: 0.018378, loss: 2.0601
2022-03-08 12:21:02 - train: epoch 0146, iter [00700, 05004], lr: 0.018378, loss: 2.4653
2022-03-08 12:21:36 - train: epoch 0146, iter [00800, 05004], lr: 0.018378, loss: 2.5741
2022-03-08 12:22:09 - train: epoch 0146, iter [00900, 05004], lr: 0.018378, loss: 2.2339
2022-03-08 12:22:43 - train: epoch 0146, iter [01000, 05004], lr: 0.018378, loss: 2.3317
2022-03-08 12:23:17 - train: epoch 0146, iter [01100, 05004], lr: 0.018378, loss: 2.2579
2022-03-08 12:23:51 - train: epoch 0146, iter [01200, 05004], lr: 0.018378, loss: 2.1935
2022-03-08 12:24:25 - train: epoch 0146, iter [01300, 05004], lr: 0.018378, loss: 2.2763
2022-03-08 12:24:59 - train: epoch 0146, iter [01400, 05004], lr: 0.018378, loss: 2.6053
2022-03-08 12:25:32 - train: epoch 0146, iter [01500, 05004], lr: 0.018378, loss: 2.5468
2022-03-08 12:26:06 - train: epoch 0146, iter [01600, 05004], lr: 0.018378, loss: 2.4040
2022-03-08 12:26:40 - train: epoch 0146, iter [01700, 05004], lr: 0.018378, loss: 2.4168
2022-03-08 12:27:15 - train: epoch 0146, iter [01800, 05004], lr: 0.018378, loss: 2.4734
2022-03-08 12:27:48 - train: epoch 0146, iter [01900, 05004], lr: 0.018378, loss: 2.3228
2022-03-08 12:28:22 - train: epoch 0146, iter [02000, 05004], lr: 0.018378, loss: 2.0999
2022-03-08 12:28:57 - train: epoch 0146, iter [02100, 05004], lr: 0.018378, loss: 2.4704
2022-03-08 12:29:30 - train: epoch 0146, iter [02200, 05004], lr: 0.018378, loss: 2.1852
2022-03-08 12:30:03 - train: epoch 0146, iter [02300, 05004], lr: 0.018378, loss: 2.4733
2022-03-08 12:30:38 - train: epoch 0146, iter [02400, 05004], lr: 0.018378, loss: 2.5494
2022-03-08 12:31:11 - train: epoch 0146, iter [02500, 05004], lr: 0.018378, loss: 2.5550
2022-03-08 12:31:45 - train: epoch 0146, iter [02600, 05004], lr: 0.018378, loss: 2.3300
2022-03-08 12:32:18 - train: epoch 0146, iter [02700, 05004], lr: 0.018378, loss: 2.3595
2022-03-08 12:32:52 - train: epoch 0146, iter [02800, 05004], lr: 0.018378, loss: 2.3055
2022-03-08 12:33:26 - train: epoch 0146, iter [02900, 05004], lr: 0.018378, loss: 2.4044
2022-03-08 12:34:00 - train: epoch 0146, iter [03000, 05004], lr: 0.018378, loss: 2.4791
2022-03-08 12:34:34 - train: epoch 0146, iter [03100, 05004], lr: 0.018378, loss: 2.3830
2022-03-08 12:35:08 - train: epoch 0146, iter [03200, 05004], lr: 0.018378, loss: 2.1148
2022-03-08 12:35:41 - train: epoch 0146, iter [03300, 05004], lr: 0.018378, loss: 2.3939
2022-03-08 12:36:15 - train: epoch 0146, iter [03400, 05004], lr: 0.018378, loss: 2.4809
2022-03-08 12:36:49 - train: epoch 0146, iter [03500, 05004], lr: 0.018378, loss: 2.2382
2022-03-08 12:37:23 - train: epoch 0146, iter [03600, 05004], lr: 0.018378, loss: 2.1487
2022-03-08 12:37:57 - train: epoch 0146, iter [03700, 05004], lr: 0.018378, loss: 2.4058
2022-03-08 12:38:31 - train: epoch 0146, iter [03800, 05004], lr: 0.018378, loss: 2.3842
2022-03-08 12:39:05 - train: epoch 0146, iter [03900, 05004], lr: 0.018378, loss: 2.3779
2022-03-08 12:39:39 - train: epoch 0146, iter [04000, 05004], lr: 0.018378, loss: 2.5154
2022-03-08 12:40:13 - train: epoch 0146, iter [04100, 05004], lr: 0.018378, loss: 2.3689
2022-03-08 12:40:46 - train: epoch 0146, iter [04200, 05004], lr: 0.018378, loss: 2.3474
2022-03-08 12:41:21 - train: epoch 0146, iter [04300, 05004], lr: 0.018378, loss: 2.5587
2022-03-08 12:41:54 - train: epoch 0146, iter [04400, 05004], lr: 0.018378, loss: 2.5301
2022-03-08 12:42:28 - train: epoch 0146, iter [04500, 05004], lr: 0.018378, loss: 2.2863
2022-03-08 12:43:02 - train: epoch 0146, iter [04600, 05004], lr: 0.018378, loss: 2.5182
2022-03-08 12:43:37 - train: epoch 0146, iter [04700, 05004], lr: 0.018378, loss: 2.2345
2022-03-08 12:44:10 - train: epoch 0146, iter [04800, 05004], lr: 0.018378, loss: 2.4646
2022-03-08 12:44:44 - train: epoch 0146, iter [04900, 05004], lr: 0.018378, loss: 2.5583
2022-03-08 12:45:16 - train: epoch 0146, iter [05000, 05004], lr: 0.018378, loss: 2.5697
2022-03-08 12:45:17 - train: epoch 146, train_loss: 2.3621
2022-03-08 12:46:31 - eval: epoch: 146, acc1: 68.366%, acc5: 88.988%, test_loss: 1.2809, per_image_load_time: 1.735ms, per_image_inference_time: 0.525ms
2022-03-08 12:46:32 - until epoch: 146, best_acc1: 68.366%
2022-03-08 12:46:32 - epoch 147 lr: 0.017757889363191483
2022-03-08 12:47:12 - train: epoch 0147, iter [00100, 05004], lr: 0.017758, loss: 2.2358
2022-03-08 12:47:45 - train: epoch 0147, iter [00200, 05004], lr: 0.017758, loss: 2.5679
2022-03-08 12:48:19 - train: epoch 0147, iter [00300, 05004], lr: 0.017758, loss: 2.3623
2022-03-08 12:48:53 - train: epoch 0147, iter [00400, 05004], lr: 0.017758, loss: 2.1229
2022-03-08 12:49:27 - train: epoch 0147, iter [00500, 05004], lr: 0.017758, loss: 2.4558
2022-03-08 12:50:00 - train: epoch 0147, iter [00600, 05004], lr: 0.017758, loss: 2.4229
2022-03-08 12:50:34 - train: epoch 0147, iter [00700, 05004], lr: 0.017758, loss: 2.2974
2022-03-08 12:51:08 - train: epoch 0147, iter [00800, 05004], lr: 0.017758, loss: 2.2122
2022-03-08 12:51:42 - train: epoch 0147, iter [00900, 05004], lr: 0.017758, loss: 2.3134
2022-03-08 12:52:16 - train: epoch 0147, iter [01000, 05004], lr: 0.017758, loss: 2.5345
2022-03-08 12:52:50 - train: epoch 0147, iter [01100, 05004], lr: 0.017758, loss: 2.3357
2022-03-08 12:53:24 - train: epoch 0147, iter [01200, 05004], lr: 0.017758, loss: 2.2875
2022-03-08 12:53:58 - train: epoch 0147, iter [01300, 05004], lr: 0.017758, loss: 2.5827
2022-03-08 12:54:31 - train: epoch 0147, iter [01400, 05004], lr: 0.017758, loss: 2.3221
2022-03-08 12:55:05 - train: epoch 0147, iter [01500, 05004], lr: 0.017758, loss: 2.3335
2022-03-08 12:55:39 - train: epoch 0147, iter [01600, 05004], lr: 0.017758, loss: 2.5178
2022-03-08 12:56:12 - train: epoch 0147, iter [01700, 05004], lr: 0.017758, loss: 2.3221
2022-03-08 12:56:47 - train: epoch 0147, iter [01800, 05004], lr: 0.017758, loss: 2.1949
2022-03-08 12:57:21 - train: epoch 0147, iter [01900, 05004], lr: 0.017758, loss: 2.5526
2022-03-08 12:57:55 - train: epoch 0147, iter [02000, 05004], lr: 0.017758, loss: 2.1299
2022-03-08 12:58:29 - train: epoch 0147, iter [02100, 05004], lr: 0.017758, loss: 2.2583
2022-03-08 12:59:03 - train: epoch 0147, iter [02200, 05004], lr: 0.017758, loss: 2.8271
2022-03-08 12:59:37 - train: epoch 0147, iter [02300, 05004], lr: 0.017758, loss: 2.4659
2022-03-08 13:00:12 - train: epoch 0147, iter [02400, 05004], lr: 0.017758, loss: 2.1584
2022-03-08 13:00:46 - train: epoch 0147, iter [02500, 05004], lr: 0.017758, loss: 2.3810
2022-03-08 13:01:21 - train: epoch 0147, iter [02600, 05004], lr: 0.017758, loss: 2.3885
2022-03-08 13:01:55 - train: epoch 0147, iter [02700, 05004], lr: 0.017758, loss: 2.3195
2022-03-08 13:02:29 - train: epoch 0147, iter [02800, 05004], lr: 0.017758, loss: 2.2889
2022-03-08 13:03:02 - train: epoch 0147, iter [02900, 05004], lr: 0.017758, loss: 2.2673
2022-03-08 13:03:36 - train: epoch 0147, iter [03000, 05004], lr: 0.017758, loss: 2.5282
2022-03-08 13:04:10 - train: epoch 0147, iter [03100, 05004], lr: 0.017758, loss: 2.1794
2022-03-08 13:04:44 - train: epoch 0147, iter [03200, 05004], lr: 0.017758, loss: 2.4619
2022-03-08 13:05:18 - train: epoch 0147, iter [03300, 05004], lr: 0.017758, loss: 2.4723
2022-03-08 13:05:51 - train: epoch 0147, iter [03400, 05004], lr: 0.017758, loss: 2.3877
2022-03-08 13:06:25 - train: epoch 0147, iter [03500, 05004], lr: 0.017758, loss: 2.4699
2022-03-08 13:07:00 - train: epoch 0147, iter [03600, 05004], lr: 0.017758, loss: 2.3695
2022-03-08 13:07:33 - train: epoch 0147, iter [03700, 05004], lr: 0.017758, loss: 2.2829
2022-03-08 13:08:07 - train: epoch 0147, iter [03800, 05004], lr: 0.017758, loss: 2.5040
2022-03-08 13:08:41 - train: epoch 0147, iter [03900, 05004], lr: 0.017758, loss: 2.3058
2022-03-08 13:09:15 - train: epoch 0147, iter [04000, 05004], lr: 0.017758, loss: 2.0553
2022-03-08 13:09:49 - train: epoch 0147, iter [04100, 05004], lr: 0.017758, loss: 2.1527
2022-03-08 13:10:23 - train: epoch 0147, iter [04200, 05004], lr: 0.017758, loss: 2.6571
2022-03-08 13:10:56 - train: epoch 0147, iter [04300, 05004], lr: 0.017758, loss: 2.0234
2022-03-08 13:11:31 - train: epoch 0147, iter [04400, 05004], lr: 0.017758, loss: 2.3322
2022-03-08 13:12:05 - train: epoch 0147, iter [04500, 05004], lr: 0.017758, loss: 2.5603
2022-03-08 13:12:39 - train: epoch 0147, iter [04600, 05004], lr: 0.017758, loss: 2.5758
2022-03-08 13:13:13 - train: epoch 0147, iter [04700, 05004], lr: 0.017758, loss: 2.4777
2022-03-08 13:13:46 - train: epoch 0147, iter [04800, 05004], lr: 0.017758, loss: 2.5020
2022-03-08 13:14:21 - train: epoch 0147, iter [04900, 05004], lr: 0.017758, loss: 2.0509
2022-03-08 13:14:52 - train: epoch 0147, iter [05000, 05004], lr: 0.017758, loss: 2.3337
2022-03-08 13:14:54 - train: epoch 147, train_loss: 2.3475
2022-03-08 13:16:08 - eval: epoch: 147, acc1: 68.144%, acc5: 88.968%, test_loss: 1.2824, per_image_load_time: 0.706ms, per_image_inference_time: 0.474ms
2022-03-08 13:16:08 - until epoch: 147, best_acc1: 68.366%
2022-03-08 13:16:08 - epoch 148 lr: 0.01714641594781347
2022-03-08 13:16:48 - train: epoch 0148, iter [00100, 05004], lr: 0.017146, loss: 2.4667
2022-03-08 13:17:22 - train: epoch 0148, iter [00200, 05004], lr: 0.017146, loss: 2.1810
2022-03-08 13:17:55 - train: epoch 0148, iter [00300, 05004], lr: 0.017146, loss: 2.3517
2022-03-08 13:18:29 - train: epoch 0148, iter [00400, 05004], lr: 0.017146, loss: 2.2422
