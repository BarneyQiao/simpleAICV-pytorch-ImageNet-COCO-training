2022-03-05 13:18:50 - network: resnet50
2022-03-05 13:18:50 - num_classes: 1000
2022-03-05 13:18:50 - input_image_size: 224
2022-03-05 13:18:50 - scale: 1.1428571428571428
2022-03-05 13:18:50 - trained_model_path: 
2022-03-05 13:18:50 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-05 13:18:50 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f11b8689940>
2022-03-05 13:18:50 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f11b8689c10>
2022-03-05 13:18:50 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f11b8689c40>
2022-03-05 13:18:50 - seed: 0
2022-03-05 13:18:50 - batch_size: 256
2022-03-05 13:18:50 - num_workers: 16
2022-03-05 13:18:50 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-05 13:18:50 - scheduler: ('CosineLR', {'warm_up_epochs': 5})
2022-03-05 13:18:50 - epochs: 200
2022-03-05 13:18:50 - print_interval: 100
2022-03-05 13:18:50 - distributed: True
2022-03-05 13:18:50 - sync_bn: False
2022-03-05 13:18:50 - apex: True
2022-03-05 13:18:50 - gpus_type: NVIDIA GeForce RTX 3090
2022-03-05 13:18:50 - gpus_num: 2
2022-03-05 13:18:50 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f11976b59b0>
2022-03-05 13:18:54 - --------------------parameters--------------------
2022-03-05 13:18:54 - name: conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-03-05 13:18:54 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-03-05 13:18:54 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-03-05 13:18:54 - name: fc.weight, grad: True
2022-03-05 13:18:54 - name: fc.bias, grad: True
2022-03-05 13:18:54 - --------------------buffers--------------------
2022-03-05 13:18:54 - name: conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-05 13:18:54 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-03-05 13:18:54 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-05 13:18:54 - epoch 001 lr: 0.020000000000000004
2022-03-05 13:19:34 - train: epoch 0001, iter [00100, 05004], lr: 0.020000, loss: 7.0095
2022-03-05 13:20:07 - train: epoch 0001, iter [00200, 05004], lr: 0.020000, loss: 6.9343
2022-03-05 13:20:40 - train: epoch 0001, iter [00300, 05004], lr: 0.020000, loss: 6.9195
2022-03-05 13:21:12 - train: epoch 0001, iter [00400, 05004], lr: 0.020000, loss: 6.9115
2022-03-05 13:21:46 - train: epoch 0001, iter [00500, 05004], lr: 0.020000, loss: 6.8722
2022-03-05 13:22:17 - train: epoch 0001, iter [00600, 05004], lr: 0.020000, loss: 6.8161
2022-03-05 13:22:50 - train: epoch 0001, iter [00700, 05004], lr: 0.020000, loss: 6.8073
2022-03-05 13:23:23 - train: epoch 0001, iter [00800, 05004], lr: 0.020000, loss: 6.8138
2022-03-05 13:23:55 - train: epoch 0001, iter [00900, 05004], lr: 0.020000, loss: 6.6569
2022-03-05 13:24:29 - train: epoch 0001, iter [01000, 05004], lr: 0.020000, loss: 6.6963
2022-03-05 13:25:02 - train: epoch 0001, iter [01100, 05004], lr: 0.020000, loss: 6.7060
2022-03-05 13:25:34 - train: epoch 0001, iter [01200, 05004], lr: 0.020000, loss: 6.5711
2022-03-05 13:26:06 - train: epoch 0001, iter [01300, 05004], lr: 0.020000, loss: 6.6082
2022-03-05 13:26:39 - train: epoch 0001, iter [01400, 05004], lr: 0.020000, loss: 6.5009
2022-03-05 13:27:12 - train: epoch 0001, iter [01500, 05004], lr: 0.020000, loss: 6.3532
2022-03-05 13:27:45 - train: epoch 0001, iter [01600, 05004], lr: 0.020000, loss: 6.4807
2022-03-05 13:28:18 - train: epoch 0001, iter [01700, 05004], lr: 0.020000, loss: 6.3740
2022-03-05 13:28:51 - train: epoch 0001, iter [01800, 05004], lr: 0.020000, loss: 6.3174
2022-03-05 13:29:24 - train: epoch 0001, iter [01900, 05004], lr: 0.020000, loss: 6.2558
2022-03-05 13:29:56 - train: epoch 0001, iter [02000, 05004], lr: 0.020000, loss: 6.3135
2022-03-05 13:30:29 - train: epoch 0001, iter [02100, 05004], lr: 0.020000, loss: 6.2115
2022-03-05 13:31:02 - train: epoch 0001, iter [02200, 05004], lr: 0.020000, loss: 6.1103
2022-03-05 13:31:35 - train: epoch 0001, iter [02300, 05004], lr: 0.020000, loss: 6.1363
2022-03-05 13:32:08 - train: epoch 0001, iter [02400, 05004], lr: 0.020000, loss: 6.1386
2022-03-05 13:32:41 - train: epoch 0001, iter [02500, 05004], lr: 0.020000, loss: 6.0536
2022-03-05 13:33:14 - train: epoch 0001, iter [02600, 05004], lr: 0.020000, loss: 6.0738
2022-03-05 13:33:46 - train: epoch 0001, iter [02700, 05004], lr: 0.020000, loss: 6.1654
2022-03-05 13:34:20 - train: epoch 0001, iter [02800, 05004], lr: 0.020000, loss: 5.8967
2022-03-05 13:34:54 - train: epoch 0001, iter [02900, 05004], lr: 0.020000, loss: 5.9558
2022-03-05 13:35:26 - train: epoch 0001, iter [03000, 05004], lr: 0.020000, loss: 6.0685
2022-03-05 13:36:00 - train: epoch 0001, iter [03100, 05004], lr: 0.020000, loss: 5.9871
2022-03-05 13:36:33 - train: epoch 0001, iter [03200, 05004], lr: 0.020000, loss: 5.9517
2022-03-05 13:37:07 - train: epoch 0001, iter [03300, 05004], lr: 0.020000, loss: 5.8524
2022-03-05 13:37:41 - train: epoch 0001, iter [03400, 05004], lr: 0.020000, loss: 5.8794
2022-03-05 13:38:14 - train: epoch 0001, iter [03500, 05004], lr: 0.020000, loss: 5.7959
2022-03-05 13:38:48 - train: epoch 0001, iter [03600, 05004], lr: 0.020000, loss: 5.8992
2022-03-05 13:39:20 - train: epoch 0001, iter [03700, 05004], lr: 0.020000, loss: 5.9503
2022-03-05 13:39:54 - train: epoch 0001, iter [03800, 05004], lr: 0.020000, loss: 5.6645
2022-03-05 13:40:28 - train: epoch 0001, iter [03900, 05004], lr: 0.020000, loss: 5.7745
2022-03-05 13:41:01 - train: epoch 0001, iter [04000, 05004], lr: 0.020000, loss: 5.7183
2022-03-05 13:41:35 - train: epoch 0001, iter [04100, 05004], lr: 0.020000, loss: 5.7728
2022-03-05 13:42:08 - train: epoch 0001, iter [04200, 05004], lr: 0.020000, loss: 5.6117
2022-03-05 13:42:41 - train: epoch 0001, iter [04300, 05004], lr: 0.020000, loss: 5.7499
2022-03-05 13:43:15 - train: epoch 0001, iter [04400, 05004], lr: 0.020000, loss: 5.4613
2022-03-05 13:43:49 - train: epoch 0001, iter [04500, 05004], lr: 0.020000, loss: 5.6375
2022-03-05 13:44:22 - train: epoch 0001, iter [04600, 05004], lr: 0.020000, loss: 5.8477
2022-03-05 13:44:55 - train: epoch 0001, iter [04700, 05004], lr: 0.020000, loss: 5.5232
2022-03-05 13:45:28 - train: epoch 0001, iter [04800, 05004], lr: 0.020000, loss: 5.6589
2022-03-05 13:46:02 - train: epoch 0001, iter [04900, 05004], lr: 0.020000, loss: 5.5734
2022-03-05 13:46:34 - train: epoch 0001, iter [05000, 05004], lr: 0.020000, loss: 5.5070
2022-03-05 13:46:35 - train: epoch 001, train_loss: 6.1858
2022-03-05 13:47:49 - eval: epoch: 001, acc1: 8.446%, acc5: 23.054%, test_loss: 5.0857, per_image_load_time: 2.206ms, per_image_inference_time: 0.516ms
2022-03-05 13:47:49 - until epoch: 001, best_acc1: 8.446%
2022-03-05 13:47:49 - epoch 002 lr: 0.04000000000000001
2022-03-05 13:48:28 - train: epoch 0002, iter [00100, 05004], lr: 0.040000, loss: 5.7828
2022-03-05 13:49:01 - train: epoch 0002, iter [00200, 05004], lr: 0.040000, loss: 5.5331
2022-03-05 13:49:35 - train: epoch 0002, iter [00300, 05004], lr: 0.040000, loss: 5.7248
2022-03-05 13:50:08 - train: epoch 0002, iter [00400, 05004], lr: 0.040000, loss: 5.7006
2022-03-05 13:50:42 - train: epoch 0002, iter [00500, 05004], lr: 0.040000, loss: 5.5564
2022-03-05 13:51:15 - train: epoch 0002, iter [00600, 05004], lr: 0.040000, loss: 5.8802
2022-03-05 13:51:49 - train: epoch 0002, iter [00700, 05004], lr: 0.040000, loss: 5.7434
2022-03-05 13:52:23 - train: epoch 0002, iter [00800, 05004], lr: 0.040000, loss: 5.3717
2022-03-05 13:52:57 - train: epoch 0002, iter [00900, 05004], lr: 0.040000, loss: 5.1486
2022-03-05 13:53:30 - train: epoch 0002, iter [01000, 05004], lr: 0.040000, loss: 5.6154
2022-03-05 13:54:04 - train: epoch 0002, iter [01100, 05004], lr: 0.040000, loss: 5.4348
2022-03-05 13:54:38 - train: epoch 0002, iter [01200, 05004], lr: 0.040000, loss: 5.3433
2022-03-05 13:55:11 - train: epoch 0002, iter [01300, 05004], lr: 0.040000, loss: 5.3376
2022-03-05 13:55:45 - train: epoch 0002, iter [01400, 05004], lr: 0.040000, loss: 5.3143
2022-03-05 13:56:24 - train: epoch 0002, iter [01500, 05004], lr: 0.040000, loss: 5.2659
2022-03-05 13:56:57 - train: epoch 0002, iter [01600, 05004], lr: 0.040000, loss: 5.2057
2022-03-05 13:57:31 - train: epoch 0002, iter [01700, 05004], lr: 0.040000, loss: 5.2964
2022-03-05 13:58:04 - train: epoch 0002, iter [01800, 05004], lr: 0.040000, loss: 5.2317
2022-03-05 13:58:37 - train: epoch 0002, iter [01900, 05004], lr: 0.040000, loss: 5.1233
2022-03-05 13:59:11 - train: epoch 0002, iter [02000, 05004], lr: 0.040000, loss: 5.0030
2022-03-05 13:59:44 - train: epoch 0002, iter [02100, 05004], lr: 0.040000, loss: 5.2429
2022-03-05 14:00:18 - train: epoch 0002, iter [02200, 05004], lr: 0.040000, loss: 5.0311
2022-03-05 14:00:51 - train: epoch 0002, iter [02300, 05004], lr: 0.040000, loss: 5.0114
2022-03-05 14:01:25 - train: epoch 0002, iter [02400, 05004], lr: 0.040000, loss: 5.0645
2022-03-05 14:01:59 - train: epoch 0002, iter [02500, 05004], lr: 0.040000, loss: 4.8882
2022-03-05 14:02:33 - train: epoch 0002, iter [02600, 05004], lr: 0.040000, loss: 5.0323
2022-03-05 14:03:06 - train: epoch 0002, iter [02700, 05004], lr: 0.040000, loss: 5.1267
2022-03-05 14:03:39 - train: epoch 0002, iter [02800, 05004], lr: 0.040000, loss: 4.9868
2022-03-05 14:04:14 - train: epoch 0002, iter [02900, 05004], lr: 0.040000, loss: 4.9075
2022-03-05 14:04:48 - train: epoch 0002, iter [03000, 05004], lr: 0.040000, loss: 4.8826
2022-03-05 14:05:21 - train: epoch 0002, iter [03100, 05004], lr: 0.040000, loss: 4.8715
2022-03-05 14:05:54 - train: epoch 0002, iter [03200, 05004], lr: 0.040000, loss: 4.8887
2022-03-05 14:06:28 - train: epoch 0002, iter [03300, 05004], lr: 0.040000, loss: 4.8162
2022-03-05 14:07:01 - train: epoch 0002, iter [03400, 05004], lr: 0.040000, loss: 5.1236
2022-03-05 14:07:35 - train: epoch 0002, iter [03500, 05004], lr: 0.040000, loss: 4.6944
2022-03-05 14:08:08 - train: epoch 0002, iter [03600, 05004], lr: 0.040000, loss: 4.7704
2022-03-05 14:08:42 - train: epoch 0002, iter [03700, 05004], lr: 0.040000, loss: 4.9006
2022-03-05 14:09:16 - train: epoch 0002, iter [03800, 05004], lr: 0.040000, loss: 4.5212
2022-03-05 14:09:49 - train: epoch 0002, iter [03900, 05004], lr: 0.040000, loss: 4.8484
2022-03-05 14:10:23 - train: epoch 0002, iter [04000, 05004], lr: 0.040000, loss: 4.7723
2022-03-05 14:10:57 - train: epoch 0002, iter [04100, 05004], lr: 0.040000, loss: 4.7350
2022-03-05 14:11:30 - train: epoch 0002, iter [04200, 05004], lr: 0.040000, loss: 4.5752
2022-03-05 14:12:05 - train: epoch 0002, iter [04300, 05004], lr: 0.040000, loss: 4.7720
2022-03-05 14:12:39 - train: epoch 0002, iter [04400, 05004], lr: 0.040000, loss: 4.5597
2022-03-05 14:13:11 - train: epoch 0002, iter [04500, 05004], lr: 0.040000, loss: 4.6192
2022-03-05 14:13:45 - train: epoch 0002, iter [04600, 05004], lr: 0.040000, loss: 4.6865
2022-03-05 14:14:19 - train: epoch 0002, iter [04700, 05004], lr: 0.040000, loss: 4.6903
2022-03-05 14:14:53 - train: epoch 0002, iter [04800, 05004], lr: 0.040000, loss: 4.6036
2022-03-05 14:15:27 - train: epoch 0002, iter [04900, 05004], lr: 0.040000, loss: 4.6754
2022-03-05 14:15:59 - train: epoch 0002, iter [05000, 05004], lr: 0.040000, loss: 4.6365
2022-03-05 14:16:00 - train: epoch 002, train_loss: 5.0734
2022-03-05 14:17:14 - eval: epoch: 002, acc1: 21.142%, acc5: 44.488%, test_loss: 3.8656, per_image_load_time: 2.287ms, per_image_inference_time: 0.480ms
2022-03-05 14:17:14 - until epoch: 002, best_acc1: 21.142%
2022-03-05 14:17:14 - epoch 003 lr: 0.06
2022-03-05 14:17:54 - train: epoch 0003, iter [00100, 05004], lr: 0.060000, loss: 4.8044
2022-03-05 14:18:28 - train: epoch 0003, iter [00200, 05004], lr: 0.060000, loss: 4.7243
2022-03-05 14:19:01 - train: epoch 0003, iter [00300, 05004], lr: 0.060000, loss: 4.7840
2022-03-05 14:19:35 - train: epoch 0003, iter [00400, 05004], lr: 0.060000, loss: 4.7140
2022-03-05 14:20:08 - train: epoch 0003, iter [00500, 05004], lr: 0.060000, loss: 4.6820
2022-03-05 14:20:41 - train: epoch 0003, iter [00600, 05004], lr: 0.060000, loss: 4.5184
2022-03-05 14:21:15 - train: epoch 0003, iter [00700, 05004], lr: 0.060000, loss: 5.0803
2022-03-05 14:21:49 - train: epoch 0003, iter [00800, 05004], lr: 0.060000, loss: 4.6002
2022-03-05 14:22:23 - train: epoch 0003, iter [00900, 05004], lr: 0.060000, loss: 4.6202
2022-03-05 14:22:56 - train: epoch 0003, iter [01000, 05004], lr: 0.060000, loss: 4.5690
2022-03-05 14:23:30 - train: epoch 0003, iter [01100, 05004], lr: 0.060000, loss: 4.4698
2022-03-05 14:24:03 - train: epoch 0003, iter [01200, 05004], lr: 0.060000, loss: 4.5263
2022-03-05 14:24:37 - train: epoch 0003, iter [01300, 05004], lr: 0.060000, loss: 4.4131
2022-03-05 14:25:11 - train: epoch 0003, iter [01400, 05004], lr: 0.060000, loss: 4.4213
2022-03-05 14:25:44 - train: epoch 0003, iter [01500, 05004], lr: 0.060000, loss: 4.7133
2022-03-05 14:26:18 - train: epoch 0003, iter [01600, 05004], lr: 0.060000, loss: 4.5256
2022-03-05 14:26:52 - train: epoch 0003, iter [01700, 05004], lr: 0.060000, loss: 4.6074
2022-03-05 14:27:26 - train: epoch 0003, iter [01800, 05004], lr: 0.060000, loss: 4.3447
2022-03-05 14:28:01 - train: epoch 0003, iter [01900, 05004], lr: 0.060000, loss: 4.4404
2022-03-05 14:28:34 - train: epoch 0003, iter [02000, 05004], lr: 0.060000, loss: 4.6256
2022-03-05 14:29:08 - train: epoch 0003, iter [02100, 05004], lr: 0.060000, loss: 4.5184
2022-03-05 14:29:42 - train: epoch 0003, iter [02200, 05004], lr: 0.060000, loss: 4.7081
2022-03-05 14:30:15 - train: epoch 0003, iter [02300, 05004], lr: 0.060000, loss: 4.3990
2022-03-05 14:30:48 - train: epoch 0003, iter [02400, 05004], lr: 0.060000, loss: 4.2205
2022-03-05 14:31:22 - train: epoch 0003, iter [02500, 05004], lr: 0.060000, loss: 4.4350
2022-03-05 14:31:56 - train: epoch 0003, iter [02600, 05004], lr: 0.060000, loss: 4.3465
2022-03-05 14:32:30 - train: epoch 0003, iter [02700, 05004], lr: 0.060000, loss: 4.5821
2022-03-05 14:33:04 - train: epoch 0003, iter [02800, 05004], lr: 0.060000, loss: 4.2380
2022-03-05 14:33:37 - train: epoch 0003, iter [02900, 05004], lr: 0.060000, loss: 4.2461
2022-03-05 14:34:11 - train: epoch 0003, iter [03000, 05004], lr: 0.060000, loss: 4.3669
2022-03-05 14:34:45 - train: epoch 0003, iter [03100, 05004], lr: 0.060000, loss: 4.4411
2022-03-05 14:35:19 - train: epoch 0003, iter [03200, 05004], lr: 0.060000, loss: 4.3443
2022-03-05 14:35:53 - train: epoch 0003, iter [03300, 05004], lr: 0.060000, loss: 4.1888
2022-03-05 14:36:26 - train: epoch 0003, iter [03400, 05004], lr: 0.060000, loss: 4.3085
2022-03-05 14:37:00 - train: epoch 0003, iter [03500, 05004], lr: 0.060000, loss: 4.1663
2022-03-05 14:37:35 - train: epoch 0003, iter [03600, 05004], lr: 0.060000, loss: 4.2394
2022-03-05 14:38:08 - train: epoch 0003, iter [03700, 05004], lr: 0.060000, loss: 4.3457
2022-03-05 14:38:43 - train: epoch 0003, iter [03800, 05004], lr: 0.060000, loss: 4.3827
2022-03-05 14:39:16 - train: epoch 0003, iter [03900, 05004], lr: 0.060000, loss: 4.2562
2022-03-05 14:39:49 - train: epoch 0003, iter [04000, 05004], lr: 0.060000, loss: 4.1837
2022-03-05 14:40:22 - train: epoch 0003, iter [04100, 05004], lr: 0.060000, loss: 4.1234
2022-03-05 14:40:56 - train: epoch 0003, iter [04200, 05004], lr: 0.060000, loss: 4.1403
2022-03-05 14:41:31 - train: epoch 0003, iter [04300, 05004], lr: 0.060000, loss: 4.0763
2022-03-05 14:42:05 - train: epoch 0003, iter [04400, 05004], lr: 0.060000, loss: 4.1380
2022-03-05 14:42:39 - train: epoch 0003, iter [04500, 05004], lr: 0.060000, loss: 4.1194
2022-03-05 14:43:12 - train: epoch 0003, iter [04600, 05004], lr: 0.060000, loss: 4.2150
2022-03-05 14:43:45 - train: epoch 0003, iter [04700, 05004], lr: 0.060000, loss: 4.1051
2022-03-05 14:44:19 - train: epoch 0003, iter [04800, 05004], lr: 0.060000, loss: 4.2919
2022-03-05 14:44:54 - train: epoch 0003, iter [04900, 05004], lr: 0.060000, loss: 4.1288
2022-03-05 14:45:26 - train: epoch 0003, iter [05000, 05004], lr: 0.060000, loss: 4.0329
2022-03-05 14:45:27 - train: epoch 003, train_loss: 4.3810
2022-03-05 14:46:41 - eval: epoch: 003, acc1: 25.588%, acc5: 49.334%, test_loss: 3.6894, per_image_load_time: 2.344ms, per_image_inference_time: 0.490ms
2022-03-05 14:46:41 - until epoch: 003, best_acc1: 25.588%
2022-03-05 14:46:41 - epoch 004 lr: 0.08000000000000002
2022-03-05 14:47:20 - train: epoch 0004, iter [00100, 05004], lr: 0.080000, loss: 4.2013
2022-03-05 14:47:53 - train: epoch 0004, iter [00200, 05004], lr: 0.080000, loss: 3.9796
2022-03-05 14:48:28 - train: epoch 0004, iter [00300, 05004], lr: 0.080000, loss: 4.2430
2022-03-05 14:49:02 - train: epoch 0004, iter [00400, 05004], lr: 0.080000, loss: 4.0252
2022-03-05 14:49:35 - train: epoch 0004, iter [00500, 05004], lr: 0.080000, loss: 4.0026
2022-03-05 14:50:09 - train: epoch 0004, iter [00600, 05004], lr: 0.080000, loss: 4.3524
2022-03-05 14:50:42 - train: epoch 0004, iter [00700, 05004], lr: 0.080000, loss: 4.1178
2022-03-05 14:51:16 - train: epoch 0004, iter [00800, 05004], lr: 0.080000, loss: 3.9455
2022-03-05 14:51:49 - train: epoch 0004, iter [00900, 05004], lr: 0.080000, loss: 4.0848
2022-03-05 14:52:23 - train: epoch 0004, iter [01000, 05004], lr: 0.080000, loss: 4.1893
2022-03-05 14:52:57 - train: epoch 0004, iter [01100, 05004], lr: 0.080000, loss: 4.1751
2022-03-05 14:53:31 - train: epoch 0004, iter [01200, 05004], lr: 0.080000, loss: 3.9390
2022-03-05 14:54:04 - train: epoch 0004, iter [01300, 05004], lr: 0.080000, loss: 3.8836
2022-03-05 14:54:37 - train: epoch 0004, iter [01400, 05004], lr: 0.080000, loss: 4.1594
2022-03-05 14:55:12 - train: epoch 0004, iter [01500, 05004], lr: 0.080000, loss: 4.0445
2022-03-05 14:55:46 - train: epoch 0004, iter [01600, 05004], lr: 0.080000, loss: 3.9588
2022-03-05 14:56:19 - train: epoch 0004, iter [01700, 05004], lr: 0.080000, loss: 4.0382
2022-03-05 14:56:54 - train: epoch 0004, iter [01800, 05004], lr: 0.080000, loss: 4.1132
2022-03-05 14:57:26 - train: epoch 0004, iter [01900, 05004], lr: 0.080000, loss: 4.1822
2022-03-05 14:58:01 - train: epoch 0004, iter [02000, 05004], lr: 0.080000, loss: 4.0281
2022-03-05 14:58:35 - train: epoch 0004, iter [02100, 05004], lr: 0.080000, loss: 4.0423
2022-03-05 14:59:08 - train: epoch 0004, iter [02200, 05004], lr: 0.080000, loss: 3.9906
2022-03-05 14:59:41 - train: epoch 0004, iter [02300, 05004], lr: 0.080000, loss: 3.8705
2022-03-05 15:00:16 - train: epoch 0004, iter [02400, 05004], lr: 0.080000, loss: 3.7387
2022-03-05 15:00:49 - train: epoch 0004, iter [02500, 05004], lr: 0.080000, loss: 3.8738
2022-03-05 15:01:23 - train: epoch 0004, iter [02600, 05004], lr: 0.080000, loss: 4.0241
2022-03-05 15:01:57 - train: epoch 0004, iter [02700, 05004], lr: 0.080000, loss: 3.9063
2022-03-05 15:02:31 - train: epoch 0004, iter [02800, 05004], lr: 0.080000, loss: 3.8651
2022-03-05 15:03:03 - train: epoch 0004, iter [02900, 05004], lr: 0.080000, loss: 3.8653
2022-03-05 15:03:38 - train: epoch 0004, iter [03000, 05004], lr: 0.080000, loss: 3.8178
2022-03-05 15:04:12 - train: epoch 0004, iter [03100, 05004], lr: 0.080000, loss: 3.7993
2022-03-05 15:04:46 - train: epoch 0004, iter [03200, 05004], lr: 0.080000, loss: 3.9595
2022-03-05 15:05:19 - train: epoch 0004, iter [03300, 05004], lr: 0.080000, loss: 4.0503
2022-03-05 15:05:54 - train: epoch 0004, iter [03400, 05004], lr: 0.080000, loss: 3.9446
2022-03-05 15:06:26 - train: epoch 0004, iter [03500, 05004], lr: 0.080000, loss: 3.9890
2022-03-05 15:07:00 - train: epoch 0004, iter [03600, 05004], lr: 0.080000, loss: 3.8237
2022-03-05 15:07:34 - train: epoch 0004, iter [03700, 05004], lr: 0.080000, loss: 3.9606
2022-03-05 15:08:08 - train: epoch 0004, iter [03800, 05004], lr: 0.080000, loss: 3.8090
2022-03-05 15:08:42 - train: epoch 0004, iter [03900, 05004], lr: 0.080000, loss: 3.7455
2022-03-05 15:09:15 - train: epoch 0004, iter [04000, 05004], lr: 0.080000, loss: 3.4674
2022-03-05 15:09:48 - train: epoch 0004, iter [04100, 05004], lr: 0.080000, loss: 4.0518
2022-03-05 15:10:23 - train: epoch 0004, iter [04200, 05004], lr: 0.080000, loss: 3.8486
2022-03-05 15:10:56 - train: epoch 0004, iter [04300, 05004], lr: 0.080000, loss: 3.7685
2022-03-05 15:11:30 - train: epoch 0004, iter [04400, 05004], lr: 0.080000, loss: 3.8417
2022-03-05 15:12:04 - train: epoch 0004, iter [04500, 05004], lr: 0.080000, loss: 3.5536
2022-03-05 15:12:38 - train: epoch 0004, iter [04600, 05004], lr: 0.080000, loss: 3.7223
2022-03-05 15:13:11 - train: epoch 0004, iter [04700, 05004], lr: 0.080000, loss: 3.8200
2022-03-05 15:13:45 - train: epoch 0004, iter [04800, 05004], lr: 0.080000, loss: 3.8342
2022-03-05 15:14:18 - train: epoch 0004, iter [04900, 05004], lr: 0.080000, loss: 3.9813
2022-03-05 15:14:51 - train: epoch 0004, iter [05000, 05004], lr: 0.080000, loss: 4.0951
2022-03-05 15:14:52 - train: epoch 004, train_loss: 3.9957
2022-03-05 15:16:05 - eval: epoch: 004, acc1: 35.486%, acc5: 62.422%, test_loss: 2.9594, per_image_load_time: 0.852ms, per_image_inference_time: 0.498ms
2022-03-05 15:16:06 - until epoch: 004, best_acc1: 35.486%
2022-03-05 15:16:06 - epoch 005 lr: 0.1
2022-03-05 15:16:44 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 4.0165
2022-03-05 15:17:19 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.8853
2022-03-05 15:17:53 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 4.1888
2022-03-05 15:18:26 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.9940
2022-03-05 15:19:00 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 3.7905
2022-03-05 15:19:33 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 3.9461
2022-03-05 15:20:07 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 3.8126
2022-03-05 15:20:40 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.9552
2022-03-05 15:21:14 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 3.7372
2022-03-05 15:21:48 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.8226
2022-03-05 15:22:22 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 4.0605
2022-03-05 15:22:55 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 3.7457
2022-03-05 15:23:28 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 3.8146
2022-03-05 15:24:02 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.8698
2022-03-05 15:24:36 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 3.8117
2022-03-05 15:25:09 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 3.6434
2022-03-05 15:25:42 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 3.8305
2022-03-05 15:26:16 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 4.0317
2022-03-05 15:26:50 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 3.8125
2022-03-05 15:27:23 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 3.9426
2022-03-05 15:27:58 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 3.4947
2022-03-05 15:28:31 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 3.7510
2022-03-05 15:29:05 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 3.8589
2022-03-05 15:29:38 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 3.8209
2022-03-05 15:30:11 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 3.7486
2022-03-05 15:30:45 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 4.0009
2022-03-05 15:31:20 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.8945
2022-03-05 15:31:53 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 3.8690
2022-03-05 15:32:26 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 3.7661
2022-03-05 15:32:59 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 3.7542
2022-03-05 15:33:33 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.7523
2022-03-05 15:34:07 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 3.7673
2022-03-05 15:34:41 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 3.6275
2022-03-05 15:35:14 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 3.8055
2022-03-05 15:35:48 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 3.7593
2022-03-05 15:36:21 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 3.5902
2022-03-05 15:36:54 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 3.8872
2022-03-05 15:37:29 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 3.6645
2022-03-05 15:38:02 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.9587
2022-03-05 15:38:35 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 3.6433
2022-03-05 15:39:10 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 3.7133
2022-03-05 15:39:43 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 3.5558
2022-03-05 15:40:17 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 3.6662
2022-03-05 15:40:51 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 3.7263
2022-03-05 15:41:25 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 3.6645
2022-03-05 15:41:58 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 3.4246
2022-03-05 15:42:32 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 3.5236
2022-03-05 15:43:05 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 3.5685
2022-03-05 15:43:39 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 3.8555
2022-03-05 15:44:11 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 3.6340
2022-03-05 15:44:12 - train: epoch 005, train_loss: 3.7980
2022-03-05 15:45:26 - eval: epoch: 005, acc1: 38.108%, acc5: 65.198%, test_loss: 2.7825, per_image_load_time: 1.556ms, per_image_inference_time: 0.510ms
2022-03-05 15:45:27 - until epoch: 005, best_acc1: 38.108%
2022-03-05 15:45:27 - epoch 006 lr: 0.1
2022-03-05 15:46:05 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 3.9071
2022-03-05 15:46:39 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 3.5130
2022-03-05 15:47:13 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 3.5849
2022-03-05 15:47:46 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.5613
2022-03-05 15:48:20 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 3.5496
2022-03-05 15:48:52 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 3.8023
2022-03-05 15:49:27 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.6338
2022-03-05 15:50:01 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 3.6634
2022-03-05 15:50:35 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 3.5630
2022-03-05 15:51:08 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 3.4837
2022-03-05 15:51:42 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 3.7232
2022-03-05 15:52:15 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 3.7086
2022-03-05 15:52:48 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.8032
2022-03-05 15:53:23 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 3.7324
2022-03-05 15:53:56 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 3.6595
2022-03-05 15:54:30 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 3.4590
2022-03-05 15:55:03 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 3.7448
2022-03-05 15:55:36 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 3.6477
2022-03-05 15:56:10 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 3.6767
2022-03-05 15:56:43 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 3.9192
2022-03-05 15:57:17 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 3.6320
2022-03-05 15:57:51 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 3.6352
2022-03-05 15:58:24 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 3.4447
2022-03-05 15:58:58 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 3.5010
2022-03-05 15:59:31 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 3.6785
2022-03-05 16:00:05 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 3.6305
2022-03-05 16:00:39 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 3.8266
2022-03-05 16:01:11 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 3.5157
2022-03-05 16:01:44 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 3.6481
2022-03-05 16:02:18 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 3.5089
2022-03-05 16:02:52 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 3.4919
2022-03-05 16:03:26 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 3.6436
2022-03-05 16:03:59 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 3.2687
2022-03-05 16:04:33 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 3.7849
2022-03-05 16:05:07 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 3.7208
2022-03-05 16:05:40 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 3.4137
2022-03-05 16:06:14 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 3.4678
2022-03-05 16:06:47 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 3.3089
2022-03-05 16:07:21 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 3.5349
2022-03-05 16:07:53 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.7137
2022-03-05 16:08:28 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 3.6056
2022-03-05 16:09:02 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 3.2536
2022-03-05 16:09:36 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 3.6295
2022-03-05 16:10:10 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 3.5782
2022-03-05 16:10:43 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 3.4180
2022-03-05 16:11:16 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 3.4431
2022-03-05 16:11:50 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 3.5789
2022-03-05 16:12:24 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 3.4582
2022-03-05 16:12:58 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 3.5301
2022-03-05 16:13:29 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 3.3789
2022-03-05 16:13:30 - train: epoch 006, train_loss: 3.6216
2022-03-05 16:14:44 - eval: epoch: 006, acc1: 39.504%, acc5: 66.344%, test_loss: 2.7538, per_image_load_time: 1.756ms, per_image_inference_time: 0.468ms
2022-03-05 16:14:45 - until epoch: 006, best_acc1: 39.504%
2022-03-05 16:14:45 - epoch 007 lr: 0.09999351124856874
2022-03-05 16:15:23 - train: epoch 0007, iter [00100, 05004], lr: 0.099994, loss: 3.5777
2022-03-05 16:15:58 - train: epoch 0007, iter [00200, 05004], lr: 0.099994, loss: 3.7713
2022-03-05 16:16:31 - train: epoch 0007, iter [00300, 05004], lr: 0.099994, loss: 3.9046
2022-03-05 16:17:06 - train: epoch 0007, iter [00400, 05004], lr: 0.099994, loss: 3.6336
2022-03-05 16:17:38 - train: epoch 0007, iter [00500, 05004], lr: 0.099994, loss: 3.5055
2022-03-05 16:18:12 - train: epoch 0007, iter [00600, 05004], lr: 0.099994, loss: 3.7234
2022-03-05 16:18:46 - train: epoch 0007, iter [00700, 05004], lr: 0.099994, loss: 3.4423
2022-03-05 16:19:20 - train: epoch 0007, iter [00800, 05004], lr: 0.099994, loss: 3.6729
2022-03-05 16:19:53 - train: epoch 0007, iter [00900, 05004], lr: 0.099994, loss: 3.7701
2022-03-05 16:20:27 - train: epoch 0007, iter [01000, 05004], lr: 0.099994, loss: 3.4937
2022-03-05 16:20:59 - train: epoch 0007, iter [01100, 05004], lr: 0.099994, loss: 3.6470
2022-03-05 16:21:34 - train: epoch 0007, iter [01200, 05004], lr: 0.099994, loss: 3.5534
2022-03-05 16:22:08 - train: epoch 0007, iter [01300, 05004], lr: 0.099994, loss: 3.3646
2022-03-05 16:22:42 - train: epoch 0007, iter [01400, 05004], lr: 0.099994, loss: 3.5304
2022-03-05 16:23:16 - train: epoch 0007, iter [01500, 05004], lr: 0.099994, loss: 3.7546
2022-03-05 16:23:50 - train: epoch 0007, iter [01600, 05004], lr: 0.099994, loss: 3.6291
2022-03-05 16:24:23 - train: epoch 0007, iter [01700, 05004], lr: 0.099994, loss: 3.4436
2022-03-05 16:24:57 - train: epoch 0007, iter [01800, 05004], lr: 0.099994, loss: 3.6790
2022-03-05 16:25:31 - train: epoch 0007, iter [01900, 05004], lr: 0.099994, loss: 3.4099
2022-03-05 16:26:05 - train: epoch 0007, iter [02000, 05004], lr: 0.099994, loss: 3.5331
2022-03-05 16:26:39 - train: epoch 0007, iter [02100, 05004], lr: 0.099994, loss: 3.5466
2022-03-05 16:27:12 - train: epoch 0007, iter [02200, 05004], lr: 0.099994, loss: 3.4505
2022-03-05 16:27:46 - train: epoch 0007, iter [02300, 05004], lr: 0.099994, loss: 3.4348
2022-03-05 16:28:19 - train: epoch 0007, iter [02400, 05004], lr: 0.099994, loss: 3.6415
2022-03-05 16:28:53 - train: epoch 0007, iter [02500, 05004], lr: 0.099994, loss: 3.1945
2022-03-05 16:29:27 - train: epoch 0007, iter [02600, 05004], lr: 0.099994, loss: 3.5702
2022-03-05 16:30:00 - train: epoch 0007, iter [02700, 05004], lr: 0.099994, loss: 3.4461
2022-03-05 16:30:35 - train: epoch 0007, iter [02800, 05004], lr: 0.099994, loss: 3.5988
2022-03-05 16:31:08 - train: epoch 0007, iter [02900, 05004], lr: 0.099994, loss: 3.7072
2022-03-05 16:31:43 - train: epoch 0007, iter [03000, 05004], lr: 0.099994, loss: 3.6628
2022-03-05 16:32:16 - train: epoch 0007, iter [03100, 05004], lr: 0.099994, loss: 3.3511
2022-03-05 16:32:50 - train: epoch 0007, iter [03200, 05004], lr: 0.099994, loss: 3.4379
2022-03-05 16:33:24 - train: epoch 0007, iter [03300, 05004], lr: 0.099994, loss: 3.8031
2022-03-05 16:33:58 - train: epoch 0007, iter [03400, 05004], lr: 0.099994, loss: 3.4386
2022-03-05 16:34:31 - train: epoch 0007, iter [03500, 05004], lr: 0.099994, loss: 3.5289
2022-03-05 16:35:05 - train: epoch 0007, iter [03600, 05004], lr: 0.099994, loss: 3.4492
2022-03-05 16:35:39 - train: epoch 0007, iter [03700, 05004], lr: 0.099994, loss: 3.3371
2022-03-05 16:36:13 - train: epoch 0007, iter [03800, 05004], lr: 0.099994, loss: 3.4875
2022-03-05 16:36:47 - train: epoch 0007, iter [03900, 05004], lr: 0.099994, loss: 3.4277
2022-03-05 16:37:21 - train: epoch 0007, iter [04000, 05004], lr: 0.099994, loss: 3.5128
2022-03-05 16:37:54 - train: epoch 0007, iter [04100, 05004], lr: 0.099994, loss: 3.3707
2022-03-05 16:38:29 - train: epoch 0007, iter [04200, 05004], lr: 0.099994, loss: 3.2812
2022-03-05 16:39:03 - train: epoch 0007, iter [04300, 05004], lr: 0.099994, loss: 3.5299
2022-03-05 16:39:37 - train: epoch 0007, iter [04400, 05004], lr: 0.099994, loss: 3.3771
2022-03-05 16:40:11 - train: epoch 0007, iter [04500, 05004], lr: 0.099994, loss: 3.6493
2022-03-05 16:40:44 - train: epoch 0007, iter [04600, 05004], lr: 0.099994, loss: 3.6502
2022-03-05 16:41:19 - train: epoch 0007, iter [04700, 05004], lr: 0.099994, loss: 3.4540
2022-03-05 16:41:53 - train: epoch 0007, iter [04800, 05004], lr: 0.099994, loss: 3.6666
2022-03-05 16:42:27 - train: epoch 0007, iter [04900, 05004], lr: 0.099994, loss: 3.4112
2022-03-05 16:43:00 - train: epoch 0007, iter [05000, 05004], lr: 0.099994, loss: 3.4470
2022-03-05 16:43:01 - train: epoch 007, train_loss: 3.5125
2022-03-05 16:44:14 - eval: epoch: 007, acc1: 43.542%, acc5: 70.822%, test_loss: 2.4701, per_image_load_time: 2.284ms, per_image_inference_time: 0.485ms
2022-03-05 16:44:14 - until epoch: 007, best_acc1: 43.542%
2022-03-05 16:44:14 - epoch 008 lr: 0.09997404667843075
2022-03-05 16:44:54 - train: epoch 0008, iter [00100, 05004], lr: 0.099974, loss: 3.3381
2022-03-05 16:45:28 - train: epoch 0008, iter [00200, 05004], lr: 0.099974, loss: 3.5718
2022-03-05 16:46:01 - train: epoch 0008, iter [00300, 05004], lr: 0.099974, loss: 3.1974
2022-03-05 16:46:34 - train: epoch 0008, iter [00400, 05004], lr: 0.099974, loss: 3.5488
2022-03-05 16:47:08 - train: epoch 0008, iter [00500, 05004], lr: 0.099974, loss: 3.4276
2022-03-05 16:47:41 - train: epoch 0008, iter [00600, 05004], lr: 0.099974, loss: 3.4802
2022-03-05 16:48:15 - train: epoch 0008, iter [00700, 05004], lr: 0.099974, loss: 3.8829
2022-03-05 16:48:49 - train: epoch 0008, iter [00800, 05004], lr: 0.099974, loss: 3.0646
2022-03-05 16:49:22 - train: epoch 0008, iter [00900, 05004], lr: 0.099974, loss: 3.4268
2022-03-05 16:49:56 - train: epoch 0008, iter [01000, 05004], lr: 0.099974, loss: 3.5376
2022-03-05 16:50:29 - train: epoch 0008, iter [01100, 05004], lr: 0.099974, loss: 3.3858
2022-03-05 16:51:03 - train: epoch 0008, iter [01200, 05004], lr: 0.099974, loss: 3.4381
2022-03-05 16:51:37 - train: epoch 0008, iter [01300, 05004], lr: 0.099974, loss: 3.5409
2022-03-05 16:52:11 - train: epoch 0008, iter [01400, 05004], lr: 0.099974, loss: 3.3955
2022-03-05 16:52:45 - train: epoch 0008, iter [01500, 05004], lr: 0.099974, loss: 3.5549
2022-03-05 16:53:18 - train: epoch 0008, iter [01600, 05004], lr: 0.099974, loss: 3.3540
2022-03-05 16:53:52 - train: epoch 0008, iter [01700, 05004], lr: 0.099974, loss: 3.3419
2022-03-05 16:54:26 - train: epoch 0008, iter [01800, 05004], lr: 0.099974, loss: 3.4932
2022-03-05 16:54:59 - train: epoch 0008, iter [01900, 05004], lr: 0.099974, loss: 3.0887
2022-03-05 16:55:32 - train: epoch 0008, iter [02000, 05004], lr: 0.099974, loss: 3.4577
2022-03-05 16:56:06 - train: epoch 0008, iter [02100, 05004], lr: 0.099974, loss: 3.2856
2022-03-05 16:56:40 - train: epoch 0008, iter [02200, 05004], lr: 0.099974, loss: 3.4300
2022-03-05 16:57:14 - train: epoch 0008, iter [02300, 05004], lr: 0.099974, loss: 3.2654
2022-03-05 16:57:47 - train: epoch 0008, iter [02400, 05004], lr: 0.099974, loss: 3.5582
2022-03-05 16:58:21 - train: epoch 0008, iter [02500, 05004], lr: 0.099974, loss: 3.6397
2022-03-05 16:58:55 - train: epoch 0008, iter [02600, 05004], lr: 0.099974, loss: 3.3878
2022-03-05 16:59:29 - train: epoch 0008, iter [02700, 05004], lr: 0.099974, loss: 3.4374
2022-03-05 17:00:03 - train: epoch 0008, iter [02800, 05004], lr: 0.099974, loss: 3.5154
2022-03-05 17:00:36 - train: epoch 0008, iter [02900, 05004], lr: 0.099974, loss: 3.4405
2022-03-05 17:01:10 - train: epoch 0008, iter [03000, 05004], lr: 0.099974, loss: 3.3608
2022-03-05 17:01:43 - train: epoch 0008, iter [03100, 05004], lr: 0.099974, loss: 3.4665
2022-03-05 17:02:17 - train: epoch 0008, iter [03200, 05004], lr: 0.099974, loss: 3.7627
2022-03-05 17:02:50 - train: epoch 0008, iter [03300, 05004], lr: 0.099974, loss: 3.7607
2022-03-05 17:03:24 - train: epoch 0008, iter [03400, 05004], lr: 0.099974, loss: 3.5688
2022-03-05 17:03:57 - train: epoch 0008, iter [03500, 05004], lr: 0.099974, loss: 3.2719
2022-03-05 17:04:31 - train: epoch 0008, iter [03600, 05004], lr: 0.099974, loss: 3.6187
2022-03-05 17:05:04 - train: epoch 0008, iter [03700, 05004], lr: 0.099974, loss: 3.4367
2022-03-05 17:05:37 - train: epoch 0008, iter [03800, 05004], lr: 0.099974, loss: 3.3875
2022-03-05 17:06:10 - train: epoch 0008, iter [03900, 05004], lr: 0.099974, loss: 3.5947
2022-03-05 17:06:43 - train: epoch 0008, iter [04000, 05004], lr: 0.099974, loss: 3.7471
2022-03-05 17:07:16 - train: epoch 0008, iter [04100, 05004], lr: 0.099974, loss: 3.2214
2022-03-05 17:07:49 - train: epoch 0008, iter [04200, 05004], lr: 0.099974, loss: 3.3020
2022-03-05 17:08:22 - train: epoch 0008, iter [04300, 05004], lr: 0.099974, loss: 3.1491
2022-03-05 17:08:55 - train: epoch 0008, iter [04400, 05004], lr: 0.099974, loss: 3.2400
2022-03-05 17:09:27 - train: epoch 0008, iter [04500, 05004], lr: 0.099974, loss: 3.5235
2022-03-05 17:10:01 - train: epoch 0008, iter [04600, 05004], lr: 0.099974, loss: 3.5436
2022-03-05 17:10:34 - train: epoch 0008, iter [04700, 05004], lr: 0.099974, loss: 3.4218
2022-03-05 17:11:08 - train: epoch 0008, iter [04800, 05004], lr: 0.099974, loss: 3.3568
2022-03-05 17:11:41 - train: epoch 0008, iter [04900, 05004], lr: 0.099974, loss: 3.2417
2022-03-05 17:12:13 - train: epoch 0008, iter [05000, 05004], lr: 0.099974, loss: 3.5594
2022-03-05 17:12:14 - train: epoch 008, train_loss: 3.4369
2022-03-05 17:13:28 - eval: epoch: 008, acc1: 45.226%, acc5: 72.300%, test_loss: 2.3869, per_image_load_time: 2.383ms, per_image_inference_time: 0.475ms
2022-03-05 17:13:28 - until epoch: 008, best_acc1: 45.226%
2022-03-05 17:13:28 - epoch 009 lr: 0.09994161134161633
2022-03-05 17:14:07 - train: epoch 0009, iter [00100, 05004], lr: 0.099942, loss: 3.4445
2022-03-05 17:14:40 - train: epoch 0009, iter [00200, 05004], lr: 0.099942, loss: 3.0916
2022-03-05 17:15:14 - train: epoch 0009, iter [00300, 05004], lr: 0.099942, loss: 2.8905
2022-03-05 17:15:46 - train: epoch 0009, iter [00400, 05004], lr: 0.099942, loss: 3.5931
2022-03-05 17:16:19 - train: epoch 0009, iter [00500, 05004], lr: 0.099942, loss: 3.5643
2022-03-05 17:16:51 - train: epoch 0009, iter [00600, 05004], lr: 0.099942, loss: 3.1487
2022-03-05 17:17:25 - train: epoch 0009, iter [00700, 05004], lr: 0.099942, loss: 3.3407
2022-03-05 17:17:58 - train: epoch 0009, iter [00800, 05004], lr: 0.099942, loss: 3.3429
2022-03-05 17:18:32 - train: epoch 0009, iter [00900, 05004], lr: 0.099942, loss: 3.4593
2022-03-05 17:19:05 - train: epoch 0009, iter [01000, 05004], lr: 0.099942, loss: 3.2226
2022-03-05 17:19:38 - train: epoch 0009, iter [01100, 05004], lr: 0.099942, loss: 3.7104
2022-03-05 17:20:12 - train: epoch 0009, iter [01200, 05004], lr: 0.099942, loss: 3.3626
2022-03-05 17:20:45 - train: epoch 0009, iter [01300, 05004], lr: 0.099942, loss: 3.3661
2022-03-05 17:21:19 - train: epoch 0009, iter [01400, 05004], lr: 0.099942, loss: 3.0486
2022-03-05 17:21:53 - train: epoch 0009, iter [01500, 05004], lr: 0.099942, loss: 3.2948
2022-03-05 17:22:26 - train: epoch 0009, iter [01600, 05004], lr: 0.099942, loss: 3.4355
2022-03-05 17:22:59 - train: epoch 0009, iter [01700, 05004], lr: 0.099942, loss: 3.3793
2022-03-05 17:23:32 - train: epoch 0009, iter [01800, 05004], lr: 0.099942, loss: 3.3555
2022-03-05 17:24:04 - train: epoch 0009, iter [01900, 05004], lr: 0.099942, loss: 3.2359
2022-03-05 17:24:38 - train: epoch 0009, iter [02000, 05004], lr: 0.099942, loss: 2.8042
2022-03-05 17:25:11 - train: epoch 0009, iter [02100, 05004], lr: 0.099942, loss: 3.4660
2022-03-05 17:25:44 - train: epoch 0009, iter [02200, 05004], lr: 0.099942, loss: 3.4902
2022-03-05 17:26:18 - train: epoch 0009, iter [02300, 05004], lr: 0.099942, loss: 3.2158
2022-03-05 17:26:51 - train: epoch 0009, iter [02400, 05004], lr: 0.099942, loss: 3.3843
2022-03-05 17:27:25 - train: epoch 0009, iter [02500, 05004], lr: 0.099942, loss: 3.2797
2022-03-05 17:27:59 - train: epoch 0009, iter [02600, 05004], lr: 0.099942, loss: 3.3158
2022-03-05 17:28:31 - train: epoch 0009, iter [02700, 05004], lr: 0.099942, loss: 3.5874
2022-03-05 17:29:05 - train: epoch 0009, iter [02800, 05004], lr: 0.099942, loss: 3.4630
2022-03-05 17:29:38 - train: epoch 0009, iter [02900, 05004], lr: 0.099942, loss: 2.9722
2022-03-05 17:30:11 - train: epoch 0009, iter [03000, 05004], lr: 0.099942, loss: 3.2841
2022-03-05 17:30:44 - train: epoch 0009, iter [03100, 05004], lr: 0.099942, loss: 3.4086
2022-03-05 17:31:16 - train: epoch 0009, iter [03200, 05004], lr: 0.099942, loss: 3.2843
2022-03-05 17:31:50 - train: epoch 0009, iter [03300, 05004], lr: 0.099942, loss: 3.2936
2022-03-05 17:32:23 - train: epoch 0009, iter [03400, 05004], lr: 0.099942, loss: 3.5591
2022-03-05 17:32:56 - train: epoch 0009, iter [03500, 05004], lr: 0.099942, loss: 3.5388
2022-03-05 17:33:29 - train: epoch 0009, iter [03600, 05004], lr: 0.099942, loss: 3.3840
2022-03-05 17:34:03 - train: epoch 0009, iter [03700, 05004], lr: 0.099942, loss: 3.6112
2022-03-05 17:34:36 - train: epoch 0009, iter [03800, 05004], lr: 0.099942, loss: 3.3732
2022-03-05 17:35:10 - train: epoch 0009, iter [03900, 05004], lr: 0.099942, loss: 3.1072
2022-03-05 17:35:43 - train: epoch 0009, iter [04000, 05004], lr: 0.099942, loss: 3.6995
2022-03-05 17:36:16 - train: epoch 0009, iter [04100, 05004], lr: 0.099942, loss: 3.4301
2022-03-05 17:36:50 - train: epoch 0009, iter [04200, 05004], lr: 0.099942, loss: 3.1956
2022-03-05 17:37:23 - train: epoch 0009, iter [04300, 05004], lr: 0.099942, loss: 3.2277
2022-03-05 17:37:57 - train: epoch 0009, iter [04400, 05004], lr: 0.099942, loss: 3.2846
2022-03-05 17:38:30 - train: epoch 0009, iter [04500, 05004], lr: 0.099942, loss: 3.1698
2022-03-05 17:39:03 - train: epoch 0009, iter [04600, 05004], lr: 0.099942, loss: 3.6674
2022-03-05 17:39:35 - train: epoch 0009, iter [04700, 05004], lr: 0.099942, loss: 3.4871
2022-03-05 17:40:09 - train: epoch 0009, iter [04800, 05004], lr: 0.099942, loss: 3.4886
2022-03-05 17:40:42 - train: epoch 0009, iter [04900, 05004], lr: 0.099942, loss: 3.3276
2022-03-05 17:41:14 - train: epoch 0009, iter [05000, 05004], lr: 0.099942, loss: 3.2034
2022-03-05 17:41:15 - train: epoch 009, train_loss: 3.3808
2022-03-05 17:42:28 - eval: epoch: 009, acc1: 42.288%, acc5: 69.454%, test_loss: 2.5740, per_image_load_time: 1.689ms, per_image_inference_time: 0.491ms
2022-03-05 17:42:29 - until epoch: 009, best_acc1: 45.226%
2022-03-05 17:42:29 - epoch 010 lr: 0.09989621365671902
2022-03-05 17:43:07 - train: epoch 0010, iter [00100, 05004], lr: 0.099896, loss: 3.2867
2022-03-05 17:43:40 - train: epoch 0010, iter [00200, 05004], lr: 0.099896, loss: 3.3658
2022-03-05 17:44:14 - train: epoch 0010, iter [00300, 05004], lr: 0.099896, loss: 3.3241
2022-03-05 17:44:48 - train: epoch 0010, iter [00400, 05004], lr: 0.099896, loss: 3.4134
2022-03-05 17:45:21 - train: epoch 0010, iter [00500, 05004], lr: 0.099896, loss: 3.2862
2022-03-05 17:45:54 - train: epoch 0010, iter [00600, 05004], lr: 0.099896, loss: 3.4220
2022-03-05 17:46:27 - train: epoch 0010, iter [00700, 05004], lr: 0.099896, loss: 3.3636
2022-03-05 17:46:59 - train: epoch 0010, iter [00800, 05004], lr: 0.099896, loss: 3.2587
2022-03-05 17:47:32 - train: epoch 0010, iter [00900, 05004], lr: 0.099896, loss: 3.1608
2022-03-05 17:48:06 - train: epoch 0010, iter [01000, 05004], lr: 0.099896, loss: 3.1465
2022-03-05 17:48:40 - train: epoch 0010, iter [01100, 05004], lr: 0.099896, loss: 3.3932
2022-03-05 17:49:13 - train: epoch 0010, iter [01200, 05004], lr: 0.099896, loss: 3.2211
2022-03-05 17:49:47 - train: epoch 0010, iter [01300, 05004], lr: 0.099896, loss: 3.1523
2022-03-05 17:50:20 - train: epoch 0010, iter [01400, 05004], lr: 0.099896, loss: 3.5424
2022-03-05 17:50:54 - train: epoch 0010, iter [01500, 05004], lr: 0.099896, loss: 3.2485
2022-03-05 17:51:27 - train: epoch 0010, iter [01600, 05004], lr: 0.099896, loss: 3.4942
2022-03-05 17:52:01 - train: epoch 0010, iter [01700, 05004], lr: 0.099896, loss: 3.4375
2022-03-05 17:52:34 - train: epoch 0010, iter [01800, 05004], lr: 0.099896, loss: 3.1784
2022-03-05 17:53:07 - train: epoch 0010, iter [01900, 05004], lr: 0.099896, loss: 3.4243
2022-03-05 17:53:41 - train: epoch 0010, iter [02000, 05004], lr: 0.099896, loss: 3.3163
2022-03-05 17:54:13 - train: epoch 0010, iter [02100, 05004], lr: 0.099896, loss: 3.2645
2022-03-05 17:54:46 - train: epoch 0010, iter [02200, 05004], lr: 0.099896, loss: 3.4212
2022-03-05 17:55:20 - train: epoch 0010, iter [02300, 05004], lr: 0.099896, loss: 3.4228
2022-03-05 17:55:52 - train: epoch 0010, iter [02400, 05004], lr: 0.099896, loss: 3.6115
2022-03-05 17:56:26 - train: epoch 0010, iter [02500, 05004], lr: 0.099896, loss: 3.2212
2022-03-05 17:57:00 - train: epoch 0010, iter [02600, 05004], lr: 0.099896, loss: 3.4590
2022-03-05 17:57:33 - train: epoch 0010, iter [02700, 05004], lr: 0.099896, loss: 3.2782
2022-03-05 17:58:07 - train: epoch 0010, iter [02800, 05004], lr: 0.099896, loss: 3.4510
2022-03-05 17:58:40 - train: epoch 0010, iter [02900, 05004], lr: 0.099896, loss: 3.3390
2022-03-05 17:59:13 - train: epoch 0010, iter [03000, 05004], lr: 0.099896, loss: 3.3091
2022-03-05 17:59:46 - train: epoch 0010, iter [03100, 05004], lr: 0.099896, loss: 3.4852
2022-03-05 18:00:19 - train: epoch 0010, iter [03200, 05004], lr: 0.099896, loss: 3.2780
2022-03-05 18:00:52 - train: epoch 0010, iter [03300, 05004], lr: 0.099896, loss: 3.5742
2022-03-05 18:01:25 - train: epoch 0010, iter [03400, 05004], lr: 0.099896, loss: 3.3576
2022-03-05 18:01:58 - train: epoch 0010, iter [03500, 05004], lr: 0.099896, loss: 3.4106
2022-03-05 18:02:31 - train: epoch 0010, iter [03600, 05004], lr: 0.099896, loss: 3.7373
2022-03-05 18:03:05 - train: epoch 0010, iter [03700, 05004], lr: 0.099896, loss: 3.2793
2022-03-05 18:03:39 - train: epoch 0010, iter [03800, 05004], lr: 0.099896, loss: 3.4303
2022-03-05 18:04:11 - train: epoch 0010, iter [03900, 05004], lr: 0.099896, loss: 3.0481
2022-03-05 18:04:46 - train: epoch 0010, iter [04000, 05004], lr: 0.099896, loss: 3.2845
2022-03-05 18:05:19 - train: epoch 0010, iter [04100, 05004], lr: 0.099896, loss: 3.2746
2022-03-05 18:05:52 - train: epoch 0010, iter [04200, 05004], lr: 0.099896, loss: 3.3751
2022-03-05 18:06:24 - train: epoch 0010, iter [04300, 05004], lr: 0.099896, loss: 3.4677
2022-03-05 18:06:58 - train: epoch 0010, iter [04400, 05004], lr: 0.099896, loss: 3.2399
2022-03-05 18:07:32 - train: epoch 0010, iter [04500, 05004], lr: 0.099896, loss: 3.2172
2022-03-05 18:08:05 - train: epoch 0010, iter [04600, 05004], lr: 0.099896, loss: 3.5449
2022-03-05 18:08:39 - train: epoch 0010, iter [04700, 05004], lr: 0.099896, loss: 3.2895
2022-03-05 18:09:11 - train: epoch 0010, iter [04800, 05004], lr: 0.099896, loss: 3.2693
2022-03-05 18:09:44 - train: epoch 0010, iter [04900, 05004], lr: 0.099896, loss: 3.1106
2022-03-05 18:10:15 - train: epoch 0010, iter [05000, 05004], lr: 0.099896, loss: 3.2501
2022-03-05 18:10:16 - train: epoch 010, train_loss: 3.3340
2022-03-05 18:11:30 - eval: epoch: 010, acc1: 47.124%, acc5: 73.816%, test_loss: 2.2996, per_image_load_time: 1.381ms, per_image_inference_time: 0.475ms
2022-03-05 18:11:31 - until epoch: 010, best_acc1: 47.124%
2022-03-05 18:11:31 - epoch 011 lr: 0.09983786540671051
2022-03-05 18:12:09 - train: epoch 0011, iter [00100, 05004], lr: 0.099838, loss: 2.9307
2022-03-05 18:12:42 - train: epoch 0011, iter [00200, 05004], lr: 0.099838, loss: 3.3478
2022-03-05 18:13:15 - train: epoch 0011, iter [00300, 05004], lr: 0.099838, loss: 2.9178
2022-03-05 18:13:48 - train: epoch 0011, iter [00400, 05004], lr: 0.099838, loss: 3.3355
2022-03-05 18:14:22 - train: epoch 0011, iter [00500, 05004], lr: 0.099838, loss: 3.5167
2022-03-05 18:14:55 - train: epoch 0011, iter [00600, 05004], lr: 0.099838, loss: 3.5717
2022-03-05 18:15:29 - train: epoch 0011, iter [00700, 05004], lr: 0.099838, loss: 3.4064
2022-03-05 18:16:01 - train: epoch 0011, iter [00800, 05004], lr: 0.099838, loss: 3.2596
2022-03-05 18:16:35 - train: epoch 0011, iter [00900, 05004], lr: 0.099838, loss: 3.3751
2022-03-05 18:17:07 - train: epoch 0011, iter [01000, 05004], lr: 0.099838, loss: 3.1369
2022-03-05 18:17:40 - train: epoch 0011, iter [01100, 05004], lr: 0.099838, loss: 3.3151
2022-03-05 18:18:14 - train: epoch 0011, iter [01200, 05004], lr: 0.099838, loss: 3.5201
2022-03-05 18:18:47 - train: epoch 0011, iter [01300, 05004], lr: 0.099838, loss: 3.5671
2022-03-05 18:19:20 - train: epoch 0011, iter [01400, 05004], lr: 0.099838, loss: 3.0812
2022-03-05 18:19:53 - train: epoch 0011, iter [01500, 05004], lr: 0.099838, loss: 3.0861
2022-03-05 18:20:26 - train: epoch 0011, iter [01600, 05004], lr: 0.099838, loss: 3.3675
2022-03-05 18:21:01 - train: epoch 0011, iter [01700, 05004], lr: 0.099838, loss: 3.3942
2022-03-05 18:21:34 - train: epoch 0011, iter [01800, 05004], lr: 0.099838, loss: 3.4913
2022-03-05 18:22:07 - train: epoch 0011, iter [01900, 05004], lr: 0.099838, loss: 3.2424
2022-03-05 18:22:40 - train: epoch 0011, iter [02000, 05004], lr: 0.099838, loss: 3.3429
2022-03-05 18:23:14 - train: epoch 0011, iter [02100, 05004], lr: 0.099838, loss: 3.1896
2022-03-05 18:23:47 - train: epoch 0011, iter [02200, 05004], lr: 0.099838, loss: 3.2222
2022-03-05 18:24:21 - train: epoch 0011, iter [02300, 05004], lr: 0.099838, loss: 3.4751
2022-03-05 18:24:53 - train: epoch 0011, iter [02400, 05004], lr: 0.099838, loss: 3.1042
2022-03-05 18:25:27 - train: epoch 0011, iter [02500, 05004], lr: 0.099838, loss: 3.2993
2022-03-05 18:25:59 - train: epoch 0011, iter [02600, 05004], lr: 0.099838, loss: 3.3246
2022-03-05 18:26:33 - train: epoch 0011, iter [02700, 05004], lr: 0.099838, loss: 3.3863
2022-03-05 18:27:06 - train: epoch 0011, iter [02800, 05004], lr: 0.099838, loss: 3.0794
2022-03-05 18:27:40 - train: epoch 0011, iter [02900, 05004], lr: 0.099838, loss: 3.5511
2022-03-05 18:28:13 - train: epoch 0011, iter [03000, 05004], lr: 0.099838, loss: 3.6357
2022-03-05 18:28:47 - train: epoch 0011, iter [03100, 05004], lr: 0.099838, loss: 3.1283
2022-03-05 18:29:19 - train: epoch 0011, iter [03200, 05004], lr: 0.099838, loss: 3.1039
2022-03-05 18:29:52 - train: epoch 0011, iter [03300, 05004], lr: 0.099838, loss: 3.6217
2022-03-05 18:30:27 - train: epoch 0011, iter [03400, 05004], lr: 0.099838, loss: 3.4359
2022-03-05 18:30:59 - train: epoch 0011, iter [03500, 05004], lr: 0.099838, loss: 3.3888
2022-03-05 18:31:33 - train: epoch 0011, iter [03600, 05004], lr: 0.099838, loss: 3.2154
2022-03-05 18:32:05 - train: epoch 0011, iter [03700, 05004], lr: 0.099838, loss: 3.4232
2022-03-05 18:32:37 - train: epoch 0011, iter [03800, 05004], lr: 0.099838, loss: 3.1071
2022-03-05 18:33:10 - train: epoch 0011, iter [03900, 05004], lr: 0.099838, loss: 3.3803
2022-03-05 18:33:44 - train: epoch 0011, iter [04000, 05004], lr: 0.099838, loss: 3.4078
2022-03-05 18:34:17 - train: epoch 0011, iter [04100, 05004], lr: 0.099838, loss: 3.2803
2022-03-05 18:34:50 - train: epoch 0011, iter [04200, 05004], lr: 0.099838, loss: 3.1100
2022-03-05 18:35:24 - train: epoch 0011, iter [04300, 05004], lr: 0.099838, loss: 3.2741
2022-03-05 18:35:57 - train: epoch 0011, iter [04400, 05004], lr: 0.099838, loss: 3.2694
2022-03-05 18:36:30 - train: epoch 0011, iter [04500, 05004], lr: 0.099838, loss: 2.9757
2022-03-05 18:37:03 - train: epoch 0011, iter [04600, 05004], lr: 0.099838, loss: 3.1785
2022-03-05 18:37:36 - train: epoch 0011, iter [04700, 05004], lr: 0.099838, loss: 3.1426
2022-03-05 18:38:10 - train: epoch 0011, iter [04800, 05004], lr: 0.099838, loss: 3.1207
2022-03-05 18:38:42 - train: epoch 0011, iter [04900, 05004], lr: 0.099838, loss: 3.0270
2022-03-05 18:39:14 - train: epoch 0011, iter [05000, 05004], lr: 0.099838, loss: 3.3010
2022-03-05 18:39:15 - train: epoch 011, train_loss: 3.2975
2022-03-05 18:40:29 - eval: epoch: 011, acc1: 42.858%, acc5: 69.712%, test_loss: 2.5353, per_image_load_time: 1.805ms, per_image_inference_time: 0.452ms
2022-03-05 18:40:29 - until epoch: 011, best_acc1: 47.124%
2022-03-05 18:40:29 - epoch 012 lr: 0.09976658173588243
2022-03-05 18:41:08 - train: epoch 0012, iter [00100, 05004], lr: 0.099767, loss: 3.4823
2022-03-05 18:41:42 - train: epoch 0012, iter [00200, 05004], lr: 0.099767, loss: 3.1558
2022-03-05 18:42:15 - train: epoch 0012, iter [00300, 05004], lr: 0.099767, loss: 3.3474
2022-03-05 18:42:47 - train: epoch 0012, iter [00400, 05004], lr: 0.099767, loss: 3.3664
2022-03-05 18:43:20 - train: epoch 0012, iter [00500, 05004], lr: 0.099767, loss: 3.6326
2022-03-05 18:43:52 - train: epoch 0012, iter [00600, 05004], lr: 0.099767, loss: 3.2165
2022-03-05 18:44:25 - train: epoch 0012, iter [00700, 05004], lr: 0.099767, loss: 3.2104
2022-03-05 18:44:57 - train: epoch 0012, iter [00800, 05004], lr: 0.099767, loss: 3.2140
2022-03-05 18:45:30 - train: epoch 0012, iter [00900, 05004], lr: 0.099767, loss: 3.2875
2022-03-05 18:46:03 - train: epoch 0012, iter [01000, 05004], lr: 0.099767, loss: 3.0874
2022-03-05 18:46:36 - train: epoch 0012, iter [01100, 05004], lr: 0.099767, loss: 3.8434
2022-03-05 18:47:09 - train: epoch 0012, iter [01200, 05004], lr: 0.099767, loss: 3.0575
2022-03-05 18:47:42 - train: epoch 0012, iter [01300, 05004], lr: 0.099767, loss: 2.9949
2022-03-05 18:48:17 - train: epoch 0012, iter [01400, 05004], lr: 0.099767, loss: 3.3105
2022-03-05 18:48:50 - train: epoch 0012, iter [01500, 05004], lr: 0.099767, loss: 3.0430
2022-03-05 18:49:23 - train: epoch 0012, iter [01600, 05004], lr: 0.099767, loss: 3.2592
2022-03-05 18:49:56 - train: epoch 0012, iter [01700, 05004], lr: 0.099767, loss: 3.1206
2022-03-05 18:50:29 - train: epoch 0012, iter [01800, 05004], lr: 0.099767, loss: 3.3008
2022-03-05 18:51:03 - train: epoch 0012, iter [01900, 05004], lr: 0.099767, loss: 3.4799
2022-03-05 18:51:36 - train: epoch 0012, iter [02000, 05004], lr: 0.099767, loss: 3.3494
2022-03-05 18:52:10 - train: epoch 0012, iter [02100, 05004], lr: 0.099767, loss: 3.3720
2022-03-05 18:52:42 - train: epoch 0012, iter [02200, 05004], lr: 0.099767, loss: 3.3037
2022-03-05 18:53:15 - train: epoch 0012, iter [02300, 05004], lr: 0.099767, loss: 3.2636
2022-03-05 18:53:48 - train: epoch 0012, iter [02400, 05004], lr: 0.099767, loss: 3.3446
2022-03-05 18:54:21 - train: epoch 0012, iter [02500, 05004], lr: 0.099767, loss: 2.9557
2022-03-05 18:54:54 - train: epoch 0012, iter [02600, 05004], lr: 0.099767, loss: 2.9532
2022-03-05 18:55:27 - train: epoch 0012, iter [02700, 05004], lr: 0.099767, loss: 3.0980
2022-03-05 18:56:01 - train: epoch 0012, iter [02800, 05004], lr: 0.099767, loss: 3.1647
2022-03-05 18:56:34 - train: epoch 0012, iter [02900, 05004], lr: 0.099767, loss: 3.1553
2022-03-05 18:57:06 - train: epoch 0012, iter [03000, 05004], lr: 0.099767, loss: 3.2281
2022-03-05 18:57:40 - train: epoch 0012, iter [03100, 05004], lr: 0.099767, loss: 3.4584
2022-03-05 18:58:13 - train: epoch 0012, iter [03200, 05004], lr: 0.099767, loss: 2.8650
2022-03-05 18:58:46 - train: epoch 0012, iter [03300, 05004], lr: 0.099767, loss: 3.1106
2022-03-05 18:59:19 - train: epoch 0012, iter [03400, 05004], lr: 0.099767, loss: 3.2275
2022-03-05 18:59:52 - train: epoch 0012, iter [03500, 05004], lr: 0.099767, loss: 3.4026
2022-03-05 19:00:25 - train: epoch 0012, iter [03600, 05004], lr: 0.099767, loss: 3.4292
2022-03-05 19:00:58 - train: epoch 0012, iter [03700, 05004], lr: 0.099767, loss: 3.1731
2022-03-05 19:01:31 - train: epoch 0012, iter [03800, 05004], lr: 0.099767, loss: 3.3075
2022-03-05 19:02:04 - train: epoch 0012, iter [03900, 05004], lr: 0.099767, loss: 2.9924
2022-03-05 19:02:38 - train: epoch 0012, iter [04000, 05004], lr: 0.099767, loss: 2.9986
2022-03-05 19:03:10 - train: epoch 0012, iter [04100, 05004], lr: 0.099767, loss: 3.1832
2022-03-05 19:03:43 - train: epoch 0012, iter [04200, 05004], lr: 0.099767, loss: 2.9972
2022-03-05 19:04:16 - train: epoch 0012, iter [04300, 05004], lr: 0.099767, loss: 3.4561
2022-03-05 19:04:49 - train: epoch 0012, iter [04400, 05004], lr: 0.099767, loss: 3.0429
2022-03-05 19:05:21 - train: epoch 0012, iter [04500, 05004], lr: 0.099767, loss: 3.3368
2022-03-05 19:05:55 - train: epoch 0012, iter [04600, 05004], lr: 0.099767, loss: 3.6566
2022-03-05 19:06:28 - train: epoch 0012, iter [04700, 05004], lr: 0.099767, loss: 3.0173
2022-03-05 19:07:00 - train: epoch 0012, iter [04800, 05004], lr: 0.099767, loss: 3.3115
2022-03-05 19:07:33 - train: epoch 0012, iter [04900, 05004], lr: 0.099767, loss: 3.2830
2022-03-05 19:08:04 - train: epoch 0012, iter [05000, 05004], lr: 0.099767, loss: 3.0120
2022-03-05 19:08:05 - train: epoch 012, train_loss: 3.2646
2022-03-05 19:09:19 - eval: epoch: 012, acc1: 48.182%, acc5: 74.638%, test_loss: 2.2416, per_image_load_time: 1.786ms, per_image_inference_time: 0.512ms
2022-03-05 19:09:19 - until epoch: 012, best_acc1: 48.182%
2022-03-05 19:09:19 - epoch 013 lr: 0.09968238114591566
2022-03-05 19:09:58 - train: epoch 0013, iter [00100, 05004], lr: 0.099682, loss: 3.0369
2022-03-05 19:10:30 - train: epoch 0013, iter [00200, 05004], lr: 0.099682, loss: 3.3083
2022-03-05 19:11:03 - train: epoch 0013, iter [00300, 05004], lr: 0.099682, loss: 3.1892
2022-03-05 19:11:38 - train: epoch 0013, iter [00400, 05004], lr: 0.099682, loss: 3.2387
2022-03-05 19:12:10 - train: epoch 0013, iter [00500, 05004], lr: 0.099682, loss: 3.2710
2022-03-05 19:12:44 - train: epoch 0013, iter [00600, 05004], lr: 0.099682, loss: 3.6720
2022-03-05 19:13:16 - train: epoch 0013, iter [00700, 05004], lr: 0.099682, loss: 3.0071
2022-03-05 19:13:50 - train: epoch 0013, iter [00800, 05004], lr: 0.099682, loss: 3.2450
2022-03-05 19:14:22 - train: epoch 0013, iter [00900, 05004], lr: 0.099682, loss: 3.1567
2022-03-05 19:14:55 - train: epoch 0013, iter [01000, 05004], lr: 0.099682, loss: 3.0931
2022-03-05 19:15:29 - train: epoch 0013, iter [01100, 05004], lr: 0.099682, loss: 3.3403
2022-03-05 19:16:02 - train: epoch 0013, iter [01200, 05004], lr: 0.099682, loss: 3.5078
2022-03-05 19:16:35 - train: epoch 0013, iter [01300, 05004], lr: 0.099682, loss: 3.1776
2022-03-05 19:17:08 - train: epoch 0013, iter [01400, 05004], lr: 0.099682, loss: 3.1991
2022-03-05 19:17:42 - train: epoch 0013, iter [01500, 05004], lr: 0.099682, loss: 3.3359
2022-03-05 19:18:15 - train: epoch 0013, iter [01600, 05004], lr: 0.099682, loss: 2.8705
2022-03-05 19:18:48 - train: epoch 0013, iter [01700, 05004], lr: 0.099682, loss: 3.2826
2022-03-05 19:19:22 - train: epoch 0013, iter [01800, 05004], lr: 0.099682, loss: 3.3264
2022-03-05 19:19:55 - train: epoch 0013, iter [01900, 05004], lr: 0.099682, loss: 3.3387
2022-03-05 19:20:28 - train: epoch 0013, iter [02000, 05004], lr: 0.099682, loss: 3.4921
2022-03-05 19:21:00 - train: epoch 0013, iter [02100, 05004], lr: 0.099682, loss: 3.4349
2022-03-05 19:21:34 - train: epoch 0013, iter [02200, 05004], lr: 0.099682, loss: 2.9437
2022-03-05 19:22:07 - train: epoch 0013, iter [02300, 05004], lr: 0.099682, loss: 3.1750
2022-03-05 19:22:40 - train: epoch 0013, iter [02400, 05004], lr: 0.099682, loss: 3.2748
2022-03-05 19:23:13 - train: epoch 0013, iter [02500, 05004], lr: 0.099682, loss: 3.3045
2022-03-05 19:23:46 - train: epoch 0013, iter [02600, 05004], lr: 0.099682, loss: 3.2259
2022-03-05 19:24:20 - train: epoch 0013, iter [02700, 05004], lr: 0.099682, loss: 2.9807
2022-03-05 19:24:53 - train: epoch 0013, iter [02800, 05004], lr: 0.099682, loss: 2.9353
2022-03-05 19:25:27 - train: epoch 0013, iter [02900, 05004], lr: 0.099682, loss: 3.2503
2022-03-05 19:25:59 - train: epoch 0013, iter [03000, 05004], lr: 0.099682, loss: 3.1470
2022-03-05 19:26:33 - train: epoch 0013, iter [03100, 05004], lr: 0.099682, loss: 3.2414
2022-03-05 19:27:06 - train: epoch 0013, iter [03200, 05004], lr: 0.099682, loss: 3.2094
2022-03-05 19:27:40 - train: epoch 0013, iter [03300, 05004], lr: 0.099682, loss: 3.1183
2022-03-05 19:28:13 - train: epoch 0013, iter [03400, 05004], lr: 0.099682, loss: 3.4094
2022-03-05 19:28:47 - train: epoch 0013, iter [03500, 05004], lr: 0.099682, loss: 2.9378
2022-03-05 19:29:19 - train: epoch 0013, iter [03600, 05004], lr: 0.099682, loss: 3.5202
2022-03-05 19:29:53 - train: epoch 0013, iter [03700, 05004], lr: 0.099682, loss: 2.9410
2022-03-05 19:30:25 - train: epoch 0013, iter [03800, 05004], lr: 0.099682, loss: 3.3753
2022-03-05 19:30:58 - train: epoch 0013, iter [03900, 05004], lr: 0.099682, loss: 3.2928
2022-03-05 19:31:32 - train: epoch 0013, iter [04000, 05004], lr: 0.099682, loss: 3.1393
2022-03-05 19:32:05 - train: epoch 0013, iter [04100, 05004], lr: 0.099682, loss: 3.0276
2022-03-05 19:32:38 - train: epoch 0013, iter [04200, 05004], lr: 0.099682, loss: 3.1010
2022-03-05 19:33:12 - train: epoch 0013, iter [04300, 05004], lr: 0.099682, loss: 3.1196
2022-03-05 19:33:45 - train: epoch 0013, iter [04400, 05004], lr: 0.099682, loss: 3.1243
2022-03-05 19:34:18 - train: epoch 0013, iter [04500, 05004], lr: 0.099682, loss: 3.2454
2022-03-05 19:34:53 - train: epoch 0013, iter [04600, 05004], lr: 0.099682, loss: 3.3618
2022-03-05 19:35:26 - train: epoch 0013, iter [04700, 05004], lr: 0.099682, loss: 3.1952
2022-03-05 19:36:00 - train: epoch 0013, iter [04800, 05004], lr: 0.099682, loss: 3.4525
2022-03-05 19:36:33 - train: epoch 0013, iter [04900, 05004], lr: 0.099682, loss: 3.1672
2022-03-05 19:37:04 - train: epoch 0013, iter [05000, 05004], lr: 0.099682, loss: 3.5240
2022-03-05 19:37:05 - train: epoch 013, train_loss: 3.2450
2022-03-05 19:38:19 - eval: epoch: 013, acc1: 47.772%, acc5: 74.340%, test_loss: 2.2669, per_image_load_time: 2.183ms, per_image_inference_time: 0.475ms
2022-03-05 19:38:19 - until epoch: 013, best_acc1: 48.182%
2022-03-05 19:38:19 - epoch 014 lr: 0.0995852854910781
2022-03-05 19:38:59 - train: epoch 0014, iter [00100, 05004], lr: 0.099585, loss: 3.2187
2022-03-05 19:39:32 - train: epoch 0014, iter [00200, 05004], lr: 0.099585, loss: 3.2717
2022-03-05 19:40:08 - train: epoch 0014, iter [00300, 05004], lr: 0.099585, loss: 3.0689
2022-03-05 19:40:40 - train: epoch 0014, iter [00400, 05004], lr: 0.099585, loss: 2.9546
2022-03-05 19:41:14 - train: epoch 0014, iter [00500, 05004], lr: 0.099585, loss: 3.2821
2022-03-05 19:41:47 - train: epoch 0014, iter [00600, 05004], lr: 0.099585, loss: 3.4222
2022-03-05 19:42:21 - train: epoch 0014, iter [00700, 05004], lr: 0.099585, loss: 3.3501
2022-03-05 19:42:54 - train: epoch 0014, iter [00800, 05004], lr: 0.099585, loss: 3.2141
2022-03-05 19:43:28 - train: epoch 0014, iter [00900, 05004], lr: 0.099585, loss: 3.2337
2022-03-05 19:44:01 - train: epoch 0014, iter [01000, 05004], lr: 0.099585, loss: 3.4071
2022-03-05 19:44:35 - train: epoch 0014, iter [01100, 05004], lr: 0.099585, loss: 3.1187
2022-03-05 19:45:09 - train: epoch 0014, iter [01200, 05004], lr: 0.099585, loss: 3.4135
2022-03-05 19:45:43 - train: epoch 0014, iter [01300, 05004], lr: 0.099585, loss: 3.3545
2022-03-05 19:46:16 - train: epoch 0014, iter [01400, 05004], lr: 0.099585, loss: 3.1535
2022-03-05 19:46:49 - train: epoch 0014, iter [01500, 05004], lr: 0.099585, loss: 3.1726
2022-03-05 19:47:21 - train: epoch 0014, iter [01600, 05004], lr: 0.099585, loss: 3.1506
2022-03-05 19:47:56 - train: epoch 0014, iter [01700, 05004], lr: 0.099585, loss: 3.2662
2022-03-05 19:48:30 - train: epoch 0014, iter [01800, 05004], lr: 0.099585, loss: 3.4552
2022-03-05 19:49:04 - train: epoch 0014, iter [01900, 05004], lr: 0.099585, loss: 3.1494
2022-03-05 19:49:37 - train: epoch 0014, iter [02000, 05004], lr: 0.099585, loss: 3.1980
2022-03-05 19:50:11 - train: epoch 0014, iter [02100, 05004], lr: 0.099585, loss: 3.3112
2022-03-05 19:50:44 - train: epoch 0014, iter [02200, 05004], lr: 0.099585, loss: 3.3153
2022-03-05 19:51:17 - train: epoch 0014, iter [02300, 05004], lr: 0.099585, loss: 3.2122
2022-03-05 19:51:51 - train: epoch 0014, iter [02400, 05004], lr: 0.099585, loss: 3.4740
2022-03-05 19:52:24 - train: epoch 0014, iter [02500, 05004], lr: 0.099585, loss: 3.0895
2022-03-05 19:52:58 - train: epoch 0014, iter [02600, 05004], lr: 0.099585, loss: 3.1192
2022-03-05 19:53:31 - train: epoch 0014, iter [02700, 05004], lr: 0.099585, loss: 3.1889
2022-03-05 19:54:04 - train: epoch 0014, iter [02800, 05004], lr: 0.099585, loss: 3.3617
2022-03-05 19:54:38 - train: epoch 0014, iter [02900, 05004], lr: 0.099585, loss: 3.0848
2022-03-05 19:55:12 - train: epoch 0014, iter [03000, 05004], lr: 0.099585, loss: 3.3447
2022-03-05 19:55:45 - train: epoch 0014, iter [03100, 05004], lr: 0.099585, loss: 3.2993
2022-03-05 19:56:19 - train: epoch 0014, iter [03200, 05004], lr: 0.099585, loss: 3.0573
2022-03-05 19:56:52 - train: epoch 0014, iter [03300, 05004], lr: 0.099585, loss: 3.0707
2022-03-05 19:57:26 - train: epoch 0014, iter [03400, 05004], lr: 0.099585, loss: 3.2505
2022-03-05 19:57:59 - train: epoch 0014, iter [03500, 05004], lr: 0.099585, loss: 3.1036
2022-03-05 19:58:33 - train: epoch 0014, iter [03600, 05004], lr: 0.099585, loss: 3.1826
2022-03-05 19:59:07 - train: epoch 0014, iter [03700, 05004], lr: 0.099585, loss: 3.2642
2022-03-05 19:59:41 - train: epoch 0014, iter [03800, 05004], lr: 0.099585, loss: 3.4038
2022-03-05 20:00:13 - train: epoch 0014, iter [03900, 05004], lr: 0.099585, loss: 3.1328
2022-03-05 20:00:47 - train: epoch 0014, iter [04000, 05004], lr: 0.099585, loss: 3.3307
2022-03-05 20:01:20 - train: epoch 0014, iter [04100, 05004], lr: 0.099585, loss: 3.2396
2022-03-05 20:01:55 - train: epoch 0014, iter [04200, 05004], lr: 0.099585, loss: 3.4287
2022-03-05 20:02:28 - train: epoch 0014, iter [04300, 05004], lr: 0.099585, loss: 3.0153
2022-03-05 20:03:02 - train: epoch 0014, iter [04400, 05004], lr: 0.099585, loss: 3.0627
2022-03-05 20:03:35 - train: epoch 0014, iter [04500, 05004], lr: 0.099585, loss: 3.2618
2022-03-05 20:04:09 - train: epoch 0014, iter [04600, 05004], lr: 0.099585, loss: 3.1099
2022-03-05 20:04:42 - train: epoch 0014, iter [04700, 05004], lr: 0.099585, loss: 3.2822
2022-03-05 20:05:16 - train: epoch 0014, iter [04800, 05004], lr: 0.099585, loss: 3.0882
2022-03-05 20:05:50 - train: epoch 0014, iter [04900, 05004], lr: 0.099585, loss: 2.9990
2022-03-05 20:06:22 - train: epoch 0014, iter [05000, 05004], lr: 0.099585, loss: 3.2551
2022-03-05 20:06:23 - train: epoch 014, train_loss: 3.2246
2022-03-05 20:07:37 - eval: epoch: 014, acc1: 45.802%, acc5: 72.018%, test_loss: 2.4025, per_image_load_time: 1.423ms, per_image_inference_time: 0.491ms
2022-03-05 20:07:38 - until epoch: 014, best_acc1: 48.182%
2022-03-05 20:07:38 - epoch 015 lr: 0.09947531997255256
2022-03-05 20:08:18 - train: epoch 0015, iter [00100, 05004], lr: 0.099475, loss: 3.2182
2022-03-05 20:08:52 - train: epoch 0015, iter [00200, 05004], lr: 0.099475, loss: 3.4911
2022-03-05 20:09:24 - train: epoch 0015, iter [00300, 05004], lr: 0.099475, loss: 3.4686
2022-03-05 20:09:57 - train: epoch 0015, iter [00400, 05004], lr: 0.099475, loss: 3.1776
2022-03-05 20:10:30 - train: epoch 0015, iter [00500, 05004], lr: 0.099475, loss: 3.1113
2022-03-05 20:11:04 - train: epoch 0015, iter [00600, 05004], lr: 0.099475, loss: 3.4095
2022-03-05 20:11:37 - train: epoch 0015, iter [00700, 05004], lr: 0.099475, loss: 3.0986
2022-03-05 20:12:11 - train: epoch 0015, iter [00800, 05004], lr: 0.099475, loss: 3.3317
2022-03-05 20:12:45 - train: epoch 0015, iter [00900, 05004], lr: 0.099475, loss: 3.1457
2022-03-05 20:13:17 - train: epoch 0015, iter [01000, 05004], lr: 0.099475, loss: 3.1623
2022-03-05 20:13:50 - train: epoch 0015, iter [01100, 05004], lr: 0.099475, loss: 3.2797
2022-03-05 20:14:22 - train: epoch 0015, iter [01200, 05004], lr: 0.099475, loss: 3.0195
2022-03-05 20:14:55 - train: epoch 0015, iter [01300, 05004], lr: 0.099475, loss: 3.3004
2022-03-05 20:15:28 - train: epoch 0015, iter [01400, 05004], lr: 0.099475, loss: 3.0809
2022-03-05 20:16:01 - train: epoch 0015, iter [01500, 05004], lr: 0.099475, loss: 3.0095
2022-03-05 20:16:35 - train: epoch 0015, iter [01600, 05004], lr: 0.099475, loss: 3.3357
2022-03-05 20:17:06 - train: epoch 0015, iter [01700, 05004], lr: 0.099475, loss: 3.2418
2022-03-05 20:17:39 - train: epoch 0015, iter [01800, 05004], lr: 0.099475, loss: 3.0329
2022-03-05 20:18:12 - train: epoch 0015, iter [01900, 05004], lr: 0.099475, loss: 3.0344
2022-03-05 20:18:46 - train: epoch 0015, iter [02000, 05004], lr: 0.099475, loss: 3.2239
2022-03-05 20:19:18 - train: epoch 0015, iter [02100, 05004], lr: 0.099475, loss: 2.9465
2022-03-05 20:19:51 - train: epoch 0015, iter [02200, 05004], lr: 0.099475, loss: 3.3634
2022-03-05 20:20:24 - train: epoch 0015, iter [02300, 05004], lr: 0.099475, loss: 3.0122
2022-03-05 20:20:57 - train: epoch 0015, iter [02400, 05004], lr: 0.099475, loss: 3.2392
2022-03-05 20:21:30 - train: epoch 0015, iter [02500, 05004], lr: 0.099475, loss: 3.3537
2022-03-05 20:22:03 - train: epoch 0015, iter [02600, 05004], lr: 0.099475, loss: 3.1219
2022-03-05 20:22:37 - train: epoch 0015, iter [02700, 05004], lr: 0.099475, loss: 3.1116
2022-03-05 20:23:10 - train: epoch 0015, iter [02800, 05004], lr: 0.099475, loss: 3.3031
2022-03-05 20:23:44 - train: epoch 0015, iter [02900, 05004], lr: 0.099475, loss: 3.0069
2022-03-05 20:24:16 - train: epoch 0015, iter [03000, 05004], lr: 0.099475, loss: 3.2746
2022-03-05 20:24:49 - train: epoch 0015, iter [03100, 05004], lr: 0.099475, loss: 3.1514
2022-03-05 20:25:23 - train: epoch 0015, iter [03200, 05004], lr: 0.099475, loss: 3.2178
2022-03-05 20:25:57 - train: epoch 0015, iter [03300, 05004], lr: 0.099475, loss: 3.1225
2022-03-05 20:26:30 - train: epoch 0015, iter [03400, 05004], lr: 0.099475, loss: 3.1493
2022-03-05 20:27:03 - train: epoch 0015, iter [03500, 05004], lr: 0.099475, loss: 3.3962
2022-03-05 20:27:35 - train: epoch 0015, iter [03600, 05004], lr: 0.099475, loss: 3.3657
2022-03-05 20:28:09 - train: epoch 0015, iter [03700, 05004], lr: 0.099475, loss: 3.1952
2022-03-05 20:28:43 - train: epoch 0015, iter [03800, 05004], lr: 0.099475, loss: 3.0904
2022-03-05 20:29:16 - train: epoch 0015, iter [03900, 05004], lr: 0.099475, loss: 3.6310
2022-03-05 20:29:50 - train: epoch 0015, iter [04000, 05004], lr: 0.099475, loss: 3.3055
2022-03-05 20:30:22 - train: epoch 0015, iter [04100, 05004], lr: 0.099475, loss: 3.2130
2022-03-05 20:30:55 - train: epoch 0015, iter [04200, 05004], lr: 0.099475, loss: 2.9019
2022-03-05 20:31:27 - train: epoch 0015, iter [04300, 05004], lr: 0.099475, loss: 3.0753
2022-03-05 20:32:00 - train: epoch 0015, iter [04400, 05004], lr: 0.099475, loss: 3.1213
2022-03-05 20:32:33 - train: epoch 0015, iter [04500, 05004], lr: 0.099475, loss: 3.2503
2022-03-05 20:33:06 - train: epoch 0015, iter [04600, 05004], lr: 0.099475, loss: 3.2335
2022-03-05 20:33:40 - train: epoch 0015, iter [04700, 05004], lr: 0.099475, loss: 3.2682
2022-03-05 20:34:13 - train: epoch 0015, iter [04800, 05004], lr: 0.099475, loss: 3.1388
2022-03-05 20:34:46 - train: epoch 0015, iter [04900, 05004], lr: 0.099475, loss: 3.2178
2022-03-05 20:35:18 - train: epoch 0015, iter [05000, 05004], lr: 0.099475, loss: 3.2170
2022-03-05 20:35:19 - train: epoch 015, train_loss: 3.2091
2022-03-05 20:36:32 - eval: epoch: 015, acc1: 49.312%, acc5: 75.398%, test_loss: 2.2030, per_image_load_time: 1.344ms, per_image_inference_time: 0.498ms
2022-03-05 20:36:33 - until epoch: 015, best_acc1: 49.312%
2022-03-05 20:36:33 - epoch 016 lr: 0.09935251313189564
2022-03-05 20:37:13 - train: epoch 0016, iter [00100, 05004], lr: 0.099353, loss: 3.2787
2022-03-05 20:37:46 - train: epoch 0016, iter [00200, 05004], lr: 0.099353, loss: 3.0014
2022-03-05 20:38:18 - train: epoch 0016, iter [00300, 05004], lr: 0.099353, loss: 3.2264
2022-03-05 20:38:51 - train: epoch 0016, iter [00400, 05004], lr: 0.099353, loss: 3.3966
2022-03-05 20:39:24 - train: epoch 0016, iter [00500, 05004], lr: 0.099353, loss: 3.1153
2022-03-05 20:39:58 - train: epoch 0016, iter [00600, 05004], lr: 0.099353, loss: 3.6123
2022-03-05 20:40:29 - train: epoch 0016, iter [00700, 05004], lr: 0.099353, loss: 3.0889
2022-03-05 20:41:02 - train: epoch 0016, iter [00800, 05004], lr: 0.099353, loss: 3.1710
2022-03-05 20:41:34 - train: epoch 0016, iter [00900, 05004], lr: 0.099353, loss: 3.1200
2022-03-05 20:42:08 - train: epoch 0016, iter [01000, 05004], lr: 0.099353, loss: 3.1973
2022-03-05 20:42:40 - train: epoch 0016, iter [01100, 05004], lr: 0.099353, loss: 2.8898
2022-03-05 20:43:13 - train: epoch 0016, iter [01200, 05004], lr: 0.099353, loss: 2.8782
2022-03-05 20:43:46 - train: epoch 0016, iter [01300, 05004], lr: 0.099353, loss: 3.3955
2022-03-05 20:44:18 - train: epoch 0016, iter [01400, 05004], lr: 0.099353, loss: 3.2340
2022-03-05 20:44:51 - train: epoch 0016, iter [01500, 05004], lr: 0.099353, loss: 3.1217
2022-03-05 20:45:25 - train: epoch 0016, iter [01600, 05004], lr: 0.099353, loss: 3.2658
2022-03-05 20:45:57 - train: epoch 0016, iter [01700, 05004], lr: 0.099353, loss: 3.0513
2022-03-05 20:46:30 - train: epoch 0016, iter [01800, 05004], lr: 0.099353, loss: 3.4342
2022-03-05 20:47:04 - train: epoch 0016, iter [01900, 05004], lr: 0.099353, loss: 3.1137
2022-03-05 20:47:37 - train: epoch 0016, iter [02000, 05004], lr: 0.099353, loss: 2.9508
2022-03-05 20:48:08 - train: epoch 0016, iter [02100, 05004], lr: 0.099353, loss: 3.3441
2022-03-05 20:48:41 - train: epoch 0016, iter [02200, 05004], lr: 0.099353, loss: 3.2763
2022-03-05 20:49:14 - train: epoch 0016, iter [02300, 05004], lr: 0.099353, loss: 3.2650
2022-03-05 20:49:47 - train: epoch 0016, iter [02400, 05004], lr: 0.099353, loss: 3.3513
2022-03-05 20:50:20 - train: epoch 0016, iter [02500, 05004], lr: 0.099353, loss: 3.0434
2022-03-05 20:50:53 - train: epoch 0016, iter [02600, 05004], lr: 0.099353, loss: 3.2968
2022-03-05 20:51:25 - train: epoch 0016, iter [02700, 05004], lr: 0.099353, loss: 3.1331
2022-03-05 20:51:58 - train: epoch 0016, iter [02800, 05004], lr: 0.099353, loss: 2.8747
2022-03-05 20:52:30 - train: epoch 0016, iter [02900, 05004], lr: 0.099353, loss: 3.2477
2022-03-05 20:53:03 - train: epoch 0016, iter [03000, 05004], lr: 0.099353, loss: 3.3833
2022-03-05 20:53:36 - train: epoch 0016, iter [03100, 05004], lr: 0.099353, loss: 3.3816
2022-03-05 20:54:08 - train: epoch 0016, iter [03200, 05004], lr: 0.099353, loss: 3.2685
2022-03-05 20:54:39 - train: epoch 0016, iter [03300, 05004], lr: 0.099353, loss: 3.1481
2022-03-05 20:55:13 - train: epoch 0016, iter [03400, 05004], lr: 0.099353, loss: 3.0678
2022-03-05 20:55:45 - train: epoch 0016, iter [03500, 05004], lr: 0.099353, loss: 3.0590
2022-03-05 20:56:18 - train: epoch 0016, iter [03600, 05004], lr: 0.099353, loss: 3.1178
2022-03-05 20:56:51 - train: epoch 0016, iter [03700, 05004], lr: 0.099353, loss: 3.2802
2022-03-05 20:57:24 - train: epoch 0016, iter [03800, 05004], lr: 0.099353, loss: 3.4457
2022-03-05 20:57:56 - train: epoch 0016, iter [03900, 05004], lr: 0.099353, loss: 3.5229
2022-03-05 20:58:29 - train: epoch 0016, iter [04000, 05004], lr: 0.099353, loss: 3.1449
2022-03-05 20:59:02 - train: epoch 0016, iter [04100, 05004], lr: 0.099353, loss: 3.1985
2022-03-05 20:59:35 - train: epoch 0016, iter [04200, 05004], lr: 0.099353, loss: 3.0912
2022-03-05 21:00:08 - train: epoch 0016, iter [04300, 05004], lr: 0.099353, loss: 2.9911
2022-03-05 21:00:42 - train: epoch 0016, iter [04400, 05004], lr: 0.099353, loss: 2.9997
2022-03-05 21:01:15 - train: epoch 0016, iter [04500, 05004], lr: 0.099353, loss: 3.2807
2022-03-05 21:01:48 - train: epoch 0016, iter [04600, 05004], lr: 0.099353, loss: 3.2936
2022-03-05 21:02:21 - train: epoch 0016, iter [04700, 05004], lr: 0.099353, loss: 3.3562
2022-03-05 21:02:53 - train: epoch 0016, iter [04800, 05004], lr: 0.099353, loss: 3.1346
2022-03-05 21:03:25 - train: epoch 0016, iter [04900, 05004], lr: 0.099353, loss: 3.3261
2022-03-05 21:03:57 - train: epoch 0016, iter [05000, 05004], lr: 0.099353, loss: 3.4422
2022-03-05 21:03:58 - train: epoch 016, train_loss: 3.1901
2022-03-05 21:05:10 - eval: epoch: 016, acc1: 46.946%, acc5: 73.830%, test_loss: 2.3067, per_image_load_time: 2.181ms, per_image_inference_time: 0.446ms
2022-03-05 21:05:10 - until epoch: 016, best_acc1: 49.312%
2022-03-05 21:05:10 - epoch 017 lr: 0.09921689684362989
2022-03-05 21:05:48 - train: epoch 0017, iter [00100, 05004], lr: 0.099217, loss: 3.1134
2022-03-05 21:06:21 - train: epoch 0017, iter [00200, 05004], lr: 0.099217, loss: 3.2752
2022-03-05 21:06:53 - train: epoch 0017, iter [00300, 05004], lr: 0.099217, loss: 3.2716
2022-03-05 21:07:26 - train: epoch 0017, iter [00400, 05004], lr: 0.099217, loss: 3.1087
2022-03-05 21:07:59 - train: epoch 0017, iter [00500, 05004], lr: 0.099217, loss: 3.0948
2022-03-05 21:08:32 - train: epoch 0017, iter [00600, 05004], lr: 0.099217, loss: 3.3770
2022-03-05 21:09:05 - train: epoch 0017, iter [00700, 05004], lr: 0.099217, loss: 3.1278
2022-03-05 21:09:38 - train: epoch 0017, iter [00800, 05004], lr: 0.099217, loss: 3.2104
2022-03-05 21:10:11 - train: epoch 0017, iter [00900, 05004], lr: 0.099217, loss: 3.2528
2022-03-05 21:10:45 - train: epoch 0017, iter [01000, 05004], lr: 0.099217, loss: 3.2841
2022-03-05 21:11:18 - train: epoch 0017, iter [01100, 05004], lr: 0.099217, loss: 3.4764
2022-03-05 21:11:51 - train: epoch 0017, iter [01200, 05004], lr: 0.099217, loss: 3.1946
2022-03-05 21:12:25 - train: epoch 0017, iter [01300, 05004], lr: 0.099217, loss: 3.3231
2022-03-05 21:12:58 - train: epoch 0017, iter [01400, 05004], lr: 0.099217, loss: 3.1398
2022-03-05 21:13:32 - train: epoch 0017, iter [01500, 05004], lr: 0.099217, loss: 3.1684
2022-03-05 21:14:05 - train: epoch 0017, iter [01600, 05004], lr: 0.099217, loss: 3.2227
2022-03-05 21:14:38 - train: epoch 0017, iter [01700, 05004], lr: 0.099217, loss: 3.1243
2022-03-05 21:15:11 - train: epoch 0017, iter [01800, 05004], lr: 0.099217, loss: 3.2691
2022-03-05 21:15:45 - train: epoch 0017, iter [01900, 05004], lr: 0.099217, loss: 2.9278
2022-03-05 21:16:19 - train: epoch 0017, iter [02000, 05004], lr: 0.099217, loss: 3.1907
2022-03-05 21:16:52 - train: epoch 0017, iter [02100, 05004], lr: 0.099217, loss: 3.1374
2022-03-05 21:17:24 - train: epoch 0017, iter [02200, 05004], lr: 0.099217, loss: 3.2879
2022-03-05 21:17:57 - train: epoch 0017, iter [02300, 05004], lr: 0.099217, loss: 3.4252
2022-03-05 21:18:30 - train: epoch 0017, iter [02400, 05004], lr: 0.099217, loss: 3.1148
2022-03-05 21:19:04 - train: epoch 0017, iter [02500, 05004], lr: 0.099217, loss: 3.4001
2022-03-05 21:19:37 - train: epoch 0017, iter [02600, 05004], lr: 0.099217, loss: 3.0650
2022-03-05 21:20:11 - train: epoch 0017, iter [02700, 05004], lr: 0.099217, loss: 3.0119
2022-03-05 21:20:45 - train: epoch 0017, iter [02800, 05004], lr: 0.099217, loss: 3.2556
2022-03-05 21:21:18 - train: epoch 0017, iter [02900, 05004], lr: 0.099217, loss: 3.2737
2022-03-05 21:21:51 - train: epoch 0017, iter [03000, 05004], lr: 0.099217, loss: 3.1564
2022-03-05 21:22:23 - train: epoch 0017, iter [03100, 05004], lr: 0.099217, loss: 3.5154
2022-03-05 21:22:56 - train: epoch 0017, iter [03200, 05004], lr: 0.099217, loss: 3.1534
2022-03-05 21:23:29 - train: epoch 0017, iter [03300, 05004], lr: 0.099217, loss: 3.2836
2022-03-05 21:24:02 - train: epoch 0017, iter [03400, 05004], lr: 0.099217, loss: 2.9768
2022-03-05 21:24:35 - train: epoch 0017, iter [03500, 05004], lr: 0.099217, loss: 3.1340
2022-03-05 21:25:07 - train: epoch 0017, iter [03600, 05004], lr: 0.099217, loss: 3.2888
2022-03-05 21:25:40 - train: epoch 0017, iter [03700, 05004], lr: 0.099217, loss: 3.0999
2022-03-05 21:26:13 - train: epoch 0017, iter [03800, 05004], lr: 0.099217, loss: 3.5300
2022-03-05 21:26:46 - train: epoch 0017, iter [03900, 05004], lr: 0.099217, loss: 3.2501
2022-03-05 21:27:19 - train: epoch 0017, iter [04000, 05004], lr: 0.099217, loss: 3.0985
2022-03-05 21:27:53 - train: epoch 0017, iter [04100, 05004], lr: 0.099217, loss: 3.0820
2022-03-05 21:28:26 - train: epoch 0017, iter [04200, 05004], lr: 0.099217, loss: 2.9476
2022-03-05 21:28:59 - train: epoch 0017, iter [04300, 05004], lr: 0.099217, loss: 3.1852
2022-03-05 21:29:32 - train: epoch 0017, iter [04400, 05004], lr: 0.099217, loss: 3.3647
2022-03-05 21:30:05 - train: epoch 0017, iter [04500, 05004], lr: 0.099217, loss: 3.4260
2022-03-05 21:30:39 - train: epoch 0017, iter [04600, 05004], lr: 0.099217, loss: 3.0797
2022-03-05 21:31:12 - train: epoch 0017, iter [04700, 05004], lr: 0.099217, loss: 3.2530
2022-03-05 21:31:45 - train: epoch 0017, iter [04800, 05004], lr: 0.099217, loss: 3.1142
2022-03-05 21:32:18 - train: epoch 0017, iter [04900, 05004], lr: 0.099217, loss: 3.0944
2022-03-05 21:32:48 - train: epoch 0017, iter [05000, 05004], lr: 0.099217, loss: 3.0330
2022-03-05 21:32:49 - train: epoch 017, train_loss: 3.1791
2022-03-05 21:34:03 - eval: epoch: 017, acc1: 48.970%, acc5: 75.300%, test_loss: 2.2145, per_image_load_time: 2.371ms, per_image_inference_time: 0.460ms
2022-03-05 21:34:03 - until epoch: 017, best_acc1: 49.312%
2022-03-05 21:34:03 - epoch 018 lr: 0.09906850630697067
2022-03-05 21:34:42 - train: epoch 0018, iter [00100, 05004], lr: 0.099069, loss: 3.1789
2022-03-05 21:35:15 - train: epoch 0018, iter [00200, 05004], lr: 0.099069, loss: 3.2024
2022-03-05 21:35:48 - train: epoch 0018, iter [00300, 05004], lr: 0.099069, loss: 3.2041
2022-03-05 21:36:21 - train: epoch 0018, iter [00400, 05004], lr: 0.099069, loss: 3.3499
2022-03-05 21:36:55 - train: epoch 0018, iter [00500, 05004], lr: 0.099069, loss: 3.0859
2022-03-05 21:37:28 - train: epoch 0018, iter [00600, 05004], lr: 0.099069, loss: 3.2665
2022-03-05 21:38:01 - train: epoch 0018, iter [00700, 05004], lr: 0.099069, loss: 2.9870
2022-03-05 21:38:34 - train: epoch 0018, iter [00800, 05004], lr: 0.099069, loss: 3.1148
2022-03-05 21:39:07 - train: epoch 0018, iter [00900, 05004], lr: 0.099069, loss: 3.2253
2022-03-05 21:39:41 - train: epoch 0018, iter [01000, 05004], lr: 0.099069, loss: 2.8363
2022-03-05 21:40:13 - train: epoch 0018, iter [01100, 05004], lr: 0.099069, loss: 3.2427
2022-03-05 21:40:46 - train: epoch 0018, iter [01200, 05004], lr: 0.099069, loss: 3.3665
2022-03-05 21:41:19 - train: epoch 0018, iter [01300, 05004], lr: 0.099069, loss: 3.3703
2022-03-05 21:41:53 - train: epoch 0018, iter [01400, 05004], lr: 0.099069, loss: 3.2669
2022-03-05 21:42:26 - train: epoch 0018, iter [01500, 05004], lr: 0.099069, loss: 3.1619
2022-03-05 21:42:59 - train: epoch 0018, iter [01600, 05004], lr: 0.099069, loss: 3.2429
2022-03-05 21:43:33 - train: epoch 0018, iter [01700, 05004], lr: 0.099069, loss: 2.9682
2022-03-05 21:44:07 - train: epoch 0018, iter [01800, 05004], lr: 0.099069, loss: 2.9073
2022-03-05 21:44:39 - train: epoch 0018, iter [01900, 05004], lr: 0.099069, loss: 3.1737
2022-03-05 21:45:13 - train: epoch 0018, iter [02000, 05004], lr: 0.099069, loss: 3.3333
2022-03-05 21:45:46 - train: epoch 0018, iter [02100, 05004], lr: 0.099069, loss: 3.3145
2022-03-05 21:46:20 - train: epoch 0018, iter [02200, 05004], lr: 0.099069, loss: 3.2988
2022-03-05 21:46:53 - train: epoch 0018, iter [02300, 05004], lr: 0.099069, loss: 3.1864
2022-03-05 21:47:26 - train: epoch 0018, iter [02400, 05004], lr: 0.099069, loss: 3.0014
2022-03-05 21:47:58 - train: epoch 0018, iter [02500, 05004], lr: 0.099069, loss: 2.9231
2022-03-05 21:48:31 - train: epoch 0018, iter [02600, 05004], lr: 0.099069, loss: 3.0742
2022-03-05 21:49:04 - train: epoch 0018, iter [02700, 05004], lr: 0.099069, loss: 3.3560
2022-03-05 21:49:38 - train: epoch 0018, iter [02800, 05004], lr: 0.099069, loss: 2.8837
2022-03-05 21:50:11 - train: epoch 0018, iter [02900, 05004], lr: 0.099069, loss: 3.1765
2022-03-05 21:50:45 - train: epoch 0018, iter [03000, 05004], lr: 0.099069, loss: 3.1052
2022-03-05 21:51:17 - train: epoch 0018, iter [03100, 05004], lr: 0.099069, loss: 3.3436
2022-03-05 21:51:50 - train: epoch 0018, iter [03200, 05004], lr: 0.099069, loss: 3.0489
2022-03-05 21:52:23 - train: epoch 0018, iter [03300, 05004], lr: 0.099069, loss: 2.8990
2022-03-05 21:52:57 - train: epoch 0018, iter [03400, 05004], lr: 0.099069, loss: 3.1794
2022-03-05 21:53:30 - train: epoch 0018, iter [03500, 05004], lr: 0.099069, loss: 3.2108
2022-03-05 21:54:03 - train: epoch 0018, iter [03600, 05004], lr: 0.099069, loss: 3.2793
2022-03-05 21:54:36 - train: epoch 0018, iter [03700, 05004], lr: 0.099069, loss: 3.4458
2022-03-05 21:55:10 - train: epoch 0018, iter [03800, 05004], lr: 0.099069, loss: 3.2349
2022-03-05 21:55:41 - train: epoch 0018, iter [03900, 05004], lr: 0.099069, loss: 3.1657
2022-03-05 21:56:14 - train: epoch 0018, iter [04000, 05004], lr: 0.099069, loss: 3.0241
2022-03-05 21:56:47 - train: epoch 0018, iter [04100, 05004], lr: 0.099069, loss: 3.1752
2022-03-05 21:57:20 - train: epoch 0018, iter [04200, 05004], lr: 0.099069, loss: 3.4152
2022-03-05 21:57:53 - train: epoch 0018, iter [04300, 05004], lr: 0.099069, loss: 2.9030
2022-03-05 21:58:27 - train: epoch 0018, iter [04400, 05004], lr: 0.099069, loss: 3.1942
2022-03-05 21:59:00 - train: epoch 0018, iter [04500, 05004], lr: 0.099069, loss: 2.9786
2022-03-05 21:59:33 - train: epoch 0018, iter [04600, 05004], lr: 0.099069, loss: 2.9743
2022-03-05 22:00:06 - train: epoch 0018, iter [04700, 05004], lr: 0.099069, loss: 3.2994
2022-03-05 22:00:39 - train: epoch 0018, iter [04800, 05004], lr: 0.099069, loss: 3.1897
2022-03-05 22:01:12 - train: epoch 0018, iter [04900, 05004], lr: 0.099069, loss: 3.3177
2022-03-05 22:01:45 - train: epoch 0018, iter [05000, 05004], lr: 0.099069, loss: 3.3186
2022-03-05 22:01:46 - train: epoch 018, train_loss: 3.1691
2022-03-05 22:02:58 - eval: epoch: 018, acc1: 49.838%, acc5: 76.050%, test_loss: 2.1581, per_image_load_time: 2.272ms, per_image_inference_time: 0.451ms
2022-03-05 22:02:59 - until epoch: 018, best_acc1: 49.838%
2022-03-05 22:02:59 - epoch 019 lr: 0.09890738003669029
2022-03-05 22:03:37 - train: epoch 0019, iter [00100, 05004], lr: 0.098907, loss: 2.7625
2022-03-05 22:04:10 - train: epoch 0019, iter [00200, 05004], lr: 0.098907, loss: 3.1606
2022-03-05 22:04:43 - train: epoch 0019, iter [00300, 05004], lr: 0.098907, loss: 3.5202
2022-03-05 22:05:17 - train: epoch 0019, iter [00400, 05004], lr: 0.098907, loss: 3.0420
2022-03-05 22:05:49 - train: epoch 0019, iter [00500, 05004], lr: 0.098907, loss: 3.1631
2022-03-05 22:06:22 - train: epoch 0019, iter [00600, 05004], lr: 0.098907, loss: 3.2110
2022-03-05 22:06:56 - train: epoch 0019, iter [00700, 05004], lr: 0.098907, loss: 2.8364
2022-03-05 22:07:29 - train: epoch 0019, iter [00800, 05004], lr: 0.098907, loss: 3.3015
2022-03-05 22:08:02 - train: epoch 0019, iter [00900, 05004], lr: 0.098907, loss: 3.1515
2022-03-05 22:08:36 - train: epoch 0019, iter [01000, 05004], lr: 0.098907, loss: 3.1841
2022-03-05 22:09:09 - train: epoch 0019, iter [01100, 05004], lr: 0.098907, loss: 3.0133
2022-03-05 22:09:43 - train: epoch 0019, iter [01200, 05004], lr: 0.098907, loss: 3.2847
2022-03-05 22:10:15 - train: epoch 0019, iter [01300, 05004], lr: 0.098907, loss: 2.9605
2022-03-05 22:10:48 - train: epoch 0019, iter [01400, 05004], lr: 0.098907, loss: 2.9207
2022-03-05 22:11:21 - train: epoch 0019, iter [01500, 05004], lr: 0.098907, loss: 3.3611
2022-03-05 22:11:54 - train: epoch 0019, iter [01600, 05004], lr: 0.098907, loss: 3.0241
2022-03-05 22:12:28 - train: epoch 0019, iter [01700, 05004], lr: 0.098907, loss: 3.3168
2022-03-05 22:13:01 - train: epoch 0019, iter [01800, 05004], lr: 0.098907, loss: 2.9085
2022-03-05 22:13:34 - train: epoch 0019, iter [01900, 05004], lr: 0.098907, loss: 3.2393
2022-03-05 22:14:08 - train: epoch 0019, iter [02000, 05004], lr: 0.098907, loss: 3.2869
2022-03-05 22:14:41 - train: epoch 0019, iter [02100, 05004], lr: 0.098907, loss: 3.0093
2022-03-05 22:15:15 - train: epoch 0019, iter [02200, 05004], lr: 0.098907, loss: 3.0493
2022-03-05 22:15:48 - train: epoch 0019, iter [02300, 05004], lr: 0.098907, loss: 3.2836
2022-03-05 22:16:22 - train: epoch 0019, iter [02400, 05004], lr: 0.098907, loss: 3.2404
2022-03-05 22:16:55 - train: epoch 0019, iter [02500, 05004], lr: 0.098907, loss: 3.0918
2022-03-05 22:17:28 - train: epoch 0019, iter [02600, 05004], lr: 0.098907, loss: 3.2252
2022-03-05 22:18:01 - train: epoch 0019, iter [02700, 05004], lr: 0.098907, loss: 3.3054
2022-03-05 22:18:33 - train: epoch 0019, iter [02800, 05004], lr: 0.098907, loss: 3.1505
2022-03-05 22:19:06 - train: epoch 0019, iter [02900, 05004], lr: 0.098907, loss: 3.1403
2022-03-05 22:19:40 - train: epoch 0019, iter [03000, 05004], lr: 0.098907, loss: 3.4783
2022-03-05 22:20:13 - train: epoch 0019, iter [03100, 05004], lr: 0.098907, loss: 3.2926
2022-03-05 22:20:46 - train: epoch 0019, iter [03200, 05004], lr: 0.098907, loss: 2.9910
2022-03-05 22:21:20 - train: epoch 0019, iter [03300, 05004], lr: 0.098907, loss: 3.1543
2022-03-05 22:21:54 - train: epoch 0019, iter [03400, 05004], lr: 0.098907, loss: 3.4088
2022-03-05 22:22:27 - train: epoch 0019, iter [03500, 05004], lr: 0.098907, loss: 3.1976
2022-03-05 22:23:00 - train: epoch 0019, iter [03600, 05004], lr: 0.098907, loss: 3.0711
2022-03-05 22:23:33 - train: epoch 0019, iter [03700, 05004], lr: 0.098907, loss: 3.1103
2022-03-05 22:24:07 - train: epoch 0019, iter [03800, 05004], lr: 0.098907, loss: 3.2601
2022-03-05 22:24:40 - train: epoch 0019, iter [03900, 05004], lr: 0.098907, loss: 3.1878
2022-03-05 22:25:14 - train: epoch 0019, iter [04000, 05004], lr: 0.098907, loss: 3.0690
2022-03-05 22:25:46 - train: epoch 0019, iter [04100, 05004], lr: 0.098907, loss: 3.1493
2022-03-05 22:26:19 - train: epoch 0019, iter [04200, 05004], lr: 0.098907, loss: 3.2587
2022-03-05 22:26:52 - train: epoch 0019, iter [04300, 05004], lr: 0.098907, loss: 3.2133
2022-03-05 22:27:25 - train: epoch 0019, iter [04400, 05004], lr: 0.098907, loss: 3.2852
2022-03-05 22:27:59 - train: epoch 0019, iter [04500, 05004], lr: 0.098907, loss: 3.4108
2022-03-05 22:28:32 - train: epoch 0019, iter [04600, 05004], lr: 0.098907, loss: 3.3228
2022-03-05 22:29:06 - train: epoch 0019, iter [04700, 05004], lr: 0.098907, loss: 2.9172
2022-03-05 22:29:40 - train: epoch 0019, iter [04800, 05004], lr: 0.098907, loss: 3.0232
2022-03-05 22:30:13 - train: epoch 0019, iter [04900, 05004], lr: 0.098907, loss: 3.1300
2022-03-05 22:30:46 - train: epoch 0019, iter [05000, 05004], lr: 0.098907, loss: 3.2010
2022-03-05 22:30:47 - train: epoch 019, train_loss: 3.1608
2022-03-05 22:32:01 - eval: epoch: 019, acc1: 50.352%, acc5: 76.480%, test_loss: 2.1429, per_image_load_time: 1.123ms, per_image_inference_time: 0.530ms
2022-03-05 22:32:02 - until epoch: 019, best_acc1: 50.352%
2022-03-05 22:32:02 - epoch 020 lr: 0.0987335598531214
2022-03-05 22:32:41 - train: epoch 0020, iter [00100, 05004], lr: 0.098734, loss: 3.2308
2022-03-05 22:33:14 - train: epoch 0020, iter [00200, 05004], lr: 0.098734, loss: 2.8238
2022-03-05 22:33:48 - train: epoch 0020, iter [00300, 05004], lr: 0.098734, loss: 3.2682
2022-03-05 22:34:21 - train: epoch 0020, iter [00400, 05004], lr: 0.098734, loss: 3.0925
2022-03-05 22:34:55 - train: epoch 0020, iter [00500, 05004], lr: 0.098734, loss: 3.1677
2022-03-05 22:35:28 - train: epoch 0020, iter [00600, 05004], lr: 0.098734, loss: 3.2102
2022-03-05 22:36:02 - train: epoch 0020, iter [00700, 05004], lr: 0.098734, loss: 2.9502
2022-03-05 22:36:36 - train: epoch 0020, iter [00800, 05004], lr: 0.098734, loss: 3.2325
2022-03-05 22:37:09 - train: epoch 0020, iter [00900, 05004], lr: 0.098734, loss: 3.2706
2022-03-05 22:37:44 - train: epoch 0020, iter [01000, 05004], lr: 0.098734, loss: 3.3997
2022-03-05 22:38:17 - train: epoch 0020, iter [01100, 05004], lr: 0.098734, loss: 2.9196
2022-03-05 22:38:51 - train: epoch 0020, iter [01200, 05004], lr: 0.098734, loss: 3.0656
2022-03-05 22:39:25 - train: epoch 0020, iter [01300, 05004], lr: 0.098734, loss: 2.9404
2022-03-05 22:39:59 - train: epoch 0020, iter [01400, 05004], lr: 0.098734, loss: 3.3245
2022-03-05 22:40:33 - train: epoch 0020, iter [01500, 05004], lr: 0.098734, loss: 3.3135
2022-03-05 22:41:07 - train: epoch 0020, iter [01600, 05004], lr: 0.098734, loss: 3.1056
2022-03-05 22:41:41 - train: epoch 0020, iter [01700, 05004], lr: 0.098734, loss: 2.9323
2022-03-05 22:42:15 - train: epoch 0020, iter [01800, 05004], lr: 0.098734, loss: 3.1317
2022-03-05 22:42:48 - train: epoch 0020, iter [01900, 05004], lr: 0.098734, loss: 3.2507
2022-03-05 22:43:22 - train: epoch 0020, iter [02000, 05004], lr: 0.098734, loss: 3.1324
2022-03-05 22:43:56 - train: epoch 0020, iter [02100, 05004], lr: 0.098734, loss: 3.4195
2022-03-05 22:44:30 - train: epoch 0020, iter [02200, 05004], lr: 0.098734, loss: 3.0076
2022-03-05 22:45:04 - train: epoch 0020, iter [02300, 05004], lr: 0.098734, loss: 2.9382
2022-03-05 22:45:38 - train: epoch 0020, iter [02400, 05004], lr: 0.098734, loss: 3.2318
2022-03-05 22:46:11 - train: epoch 0020, iter [02500, 05004], lr: 0.098734, loss: 3.1317
2022-03-05 22:46:45 - train: epoch 0020, iter [02600, 05004], lr: 0.098734, loss: 2.9824
2022-03-05 22:47:18 - train: epoch 0020, iter [02700, 05004], lr: 0.098734, loss: 3.2044
2022-03-05 22:47:52 - train: epoch 0020, iter [02800, 05004], lr: 0.098734, loss: 3.3024
2022-03-05 22:48:25 - train: epoch 0020, iter [02900, 05004], lr: 0.098734, loss: 3.2635
2022-03-05 22:48:59 - train: epoch 0020, iter [03000, 05004], lr: 0.098734, loss: 3.0395
2022-03-05 22:49:32 - train: epoch 0020, iter [03100, 05004], lr: 0.098734, loss: 3.2408
2022-03-05 22:50:05 - train: epoch 0020, iter [03200, 05004], lr: 0.098734, loss: 3.0995
2022-03-05 22:50:38 - train: epoch 0020, iter [03300, 05004], lr: 0.098734, loss: 2.9489
2022-03-05 22:51:12 - train: epoch 0020, iter [03400, 05004], lr: 0.098734, loss: 3.0111
2022-03-05 22:51:45 - train: epoch 0020, iter [03500, 05004], lr: 0.098734, loss: 3.0493
2022-03-05 22:52:19 - train: epoch 0020, iter [03600, 05004], lr: 0.098734, loss: 2.9803
2022-03-05 22:52:52 - train: epoch 0020, iter [03700, 05004], lr: 0.098734, loss: 3.2386
2022-03-05 22:53:25 - train: epoch 0020, iter [03800, 05004], lr: 0.098734, loss: 3.0553
2022-03-05 22:53:59 - train: epoch 0020, iter [03900, 05004], lr: 0.098734, loss: 3.1262
2022-03-05 22:54:33 - train: epoch 0020, iter [04000, 05004], lr: 0.098734, loss: 3.1259
2022-03-05 22:55:06 - train: epoch 0020, iter [04100, 05004], lr: 0.098734, loss: 3.1927
2022-03-05 22:55:39 - train: epoch 0020, iter [04200, 05004], lr: 0.098734, loss: 2.8958
2022-03-05 22:56:12 - train: epoch 0020, iter [04300, 05004], lr: 0.098734, loss: 3.2572
2022-03-05 22:56:44 - train: epoch 0020, iter [04400, 05004], lr: 0.098734, loss: 3.2213
2022-03-05 22:57:18 - train: epoch 0020, iter [04500, 05004], lr: 0.098734, loss: 3.1659
2022-03-05 22:57:51 - train: epoch 0020, iter [04600, 05004], lr: 0.098734, loss: 3.2532
2022-03-05 22:58:25 - train: epoch 0020, iter [04700, 05004], lr: 0.098734, loss: 2.8907
2022-03-05 22:58:58 - train: epoch 0020, iter [04800, 05004], lr: 0.098734, loss: 3.1387
2022-03-05 22:59:32 - train: epoch 0020, iter [04900, 05004], lr: 0.098734, loss: 3.2436
2022-03-05 23:00:04 - train: epoch 0020, iter [05000, 05004], lr: 0.098734, loss: 2.8907
2022-03-05 23:00:05 - train: epoch 020, train_loss: 3.1461
2022-03-05 23:01:20 - eval: epoch: 020, acc1: 49.048%, acc5: 75.142%, test_loss: 2.2183, per_image_load_time: 2.390ms, per_image_inference_time: 0.487ms
2022-03-05 23:01:20 - until epoch: 020, best_acc1: 50.352%
2022-03-05 23:01:20 - epoch 021 lr: 0.0985470908713026
2022-03-05 23:01:58 - train: epoch 0021, iter [00100, 05004], lr: 0.098547, loss: 2.9021
2022-03-05 23:02:31 - train: epoch 0021, iter [00200, 05004], lr: 0.098547, loss: 3.2936
2022-03-05 23:03:05 - train: epoch 0021, iter [00300, 05004], lr: 0.098547, loss: 2.9950
2022-03-05 23:03:39 - train: epoch 0021, iter [00400, 05004], lr: 0.098547, loss: 3.2071
2022-03-05 23:04:12 - train: epoch 0021, iter [00500, 05004], lr: 0.098547, loss: 3.1453
2022-03-05 23:04:44 - train: epoch 0021, iter [00600, 05004], lr: 0.098547, loss: 2.9230
2022-03-05 23:05:17 - train: epoch 0021, iter [00700, 05004], lr: 0.098547, loss: 3.1219
2022-03-05 23:05:50 - train: epoch 0021, iter [00800, 05004], lr: 0.098547, loss: 3.3710
2022-03-05 23:06:24 - train: epoch 0021, iter [00900, 05004], lr: 0.098547, loss: 3.3071
2022-03-05 23:06:58 - train: epoch 0021, iter [01000, 05004], lr: 0.098547, loss: 3.2311
2022-03-05 23:07:32 - train: epoch 0021, iter [01100, 05004], lr: 0.098547, loss: 3.0692
2022-03-05 23:08:05 - train: epoch 0021, iter [01200, 05004], lr: 0.098547, loss: 2.9482
2022-03-05 23:08:38 - train: epoch 0021, iter [01300, 05004], lr: 0.098547, loss: 3.1043
2022-03-05 23:09:12 - train: epoch 0021, iter [01400, 05004], lr: 0.098547, loss: 3.0452
2022-03-05 23:09:45 - train: epoch 0021, iter [01500, 05004], lr: 0.098547, loss: 3.1381
2022-03-05 23:10:19 - train: epoch 0021, iter [01600, 05004], lr: 0.098547, loss: 2.9107
2022-03-05 23:10:53 - train: epoch 0021, iter [01700, 05004], lr: 0.098547, loss: 3.2056
2022-03-05 23:11:26 - train: epoch 0021, iter [01800, 05004], lr: 0.098547, loss: 3.0630
2022-03-05 23:11:59 - train: epoch 0021, iter [01900, 05004], lr: 0.098547, loss: 3.3788
2022-03-05 23:12:31 - train: epoch 0021, iter [02000, 05004], lr: 0.098547, loss: 3.2793
2022-03-05 23:13:05 - train: epoch 0021, iter [02100, 05004], lr: 0.098547, loss: 3.2615
2022-03-05 23:13:38 - train: epoch 0021, iter [02200, 05004], lr: 0.098547, loss: 3.1554
2022-03-05 23:14:12 - train: epoch 0021, iter [02300, 05004], lr: 0.098547, loss: 2.9217
2022-03-05 23:14:46 - train: epoch 0021, iter [02400, 05004], lr: 0.098547, loss: 3.1607
2022-03-05 23:15:19 - train: epoch 0021, iter [02500, 05004], lr: 0.098547, loss: 3.1348
2022-03-05 23:15:53 - train: epoch 0021, iter [02600, 05004], lr: 0.098547, loss: 3.0321
2022-03-05 23:16:26 - train: epoch 0021, iter [02700, 05004], lr: 0.098547, loss: 2.9296
2022-03-05 23:17:00 - train: epoch 0021, iter [02800, 05004], lr: 0.098547, loss: 3.1277
2022-03-05 23:17:33 - train: epoch 0021, iter [02900, 05004], lr: 0.098547, loss: 3.1706
2022-03-05 23:18:06 - train: epoch 0021, iter [03000, 05004], lr: 0.098547, loss: 3.4522
2022-03-05 23:18:39 - train: epoch 0021, iter [03100, 05004], lr: 0.098547, loss: 3.2657
2022-03-05 23:19:13 - train: epoch 0021, iter [03200, 05004], lr: 0.098547, loss: 3.2389
2022-03-05 23:19:44 - train: epoch 0021, iter [03300, 05004], lr: 0.098547, loss: 3.3008
2022-03-05 23:20:17 - train: epoch 0021, iter [03400, 05004], lr: 0.098547, loss: 3.0356
2022-03-05 23:20:51 - train: epoch 0021, iter [03500, 05004], lr: 0.098547, loss: 3.0816
2022-03-05 23:21:24 - train: epoch 0021, iter [03600, 05004], lr: 0.098547, loss: 2.9453
2022-03-05 23:21:57 - train: epoch 0021, iter [03700, 05004], lr: 0.098547, loss: 3.1615
2022-03-05 23:22:31 - train: epoch 0021, iter [03800, 05004], lr: 0.098547, loss: 3.0780
2022-03-05 23:23:05 - train: epoch 0021, iter [03900, 05004], lr: 0.098547, loss: 2.9371
2022-03-05 23:23:38 - train: epoch 0021, iter [04000, 05004], lr: 0.098547, loss: 3.2286
2022-03-05 23:24:12 - train: epoch 0021, iter [04100, 05004], lr: 0.098547, loss: 3.2571
2022-03-05 23:24:46 - train: epoch 0021, iter [04200, 05004], lr: 0.098547, loss: 3.1176
2022-03-05 23:25:19 - train: epoch 0021, iter [04300, 05004], lr: 0.098547, loss: 3.0445
2022-03-05 23:25:53 - train: epoch 0021, iter [04400, 05004], lr: 0.098547, loss: 3.0414
2022-03-05 23:26:26 - train: epoch 0021, iter [04500, 05004], lr: 0.098547, loss: 3.2659
2022-03-05 23:27:00 - train: epoch 0021, iter [04600, 05004], lr: 0.098547, loss: 3.0707
2022-03-05 23:27:31 - train: epoch 0021, iter [04700, 05004], lr: 0.098547, loss: 3.1061
2022-03-05 23:28:05 - train: epoch 0021, iter [04800, 05004], lr: 0.098547, loss: 3.4968
2022-03-05 23:28:38 - train: epoch 0021, iter [04900, 05004], lr: 0.098547, loss: 3.0293
2022-03-05 23:29:10 - train: epoch 0021, iter [05000, 05004], lr: 0.098547, loss: 3.2085
2022-03-05 23:29:11 - train: epoch 021, train_loss: 3.1378
2022-03-05 23:30:25 - eval: epoch: 021, acc1: 50.984%, acc5: 77.118%, test_loss: 2.1047, per_image_load_time: 1.806ms, per_image_inference_time: 0.514ms
2022-03-05 23:30:26 - until epoch: 021, best_acc1: 50.984%
2022-03-05 23:30:26 - epoch 022 lr: 0.09834802148926883
2022-03-05 23:31:05 - train: epoch 0022, iter [00100, 05004], lr: 0.098348, loss: 2.9900
2022-03-05 23:31:39 - train: epoch 0022, iter [00200, 05004], lr: 0.098348, loss: 2.8961
2022-03-05 23:32:12 - train: epoch 0022, iter [00300, 05004], lr: 0.098348, loss: 2.8711
2022-03-05 23:32:45 - train: epoch 0022, iter [00400, 05004], lr: 0.098348, loss: 3.3123
2022-03-05 23:33:19 - train: epoch 0022, iter [00500, 05004], lr: 0.098348, loss: 3.1219
2022-03-05 23:33:53 - train: epoch 0022, iter [00600, 05004], lr: 0.098348, loss: 3.2649
2022-03-05 23:34:26 - train: epoch 0022, iter [00700, 05004], lr: 0.098348, loss: 3.3340
2022-03-05 23:34:58 - train: epoch 0022, iter [00800, 05004], lr: 0.098348, loss: 3.3079
2022-03-05 23:35:30 - train: epoch 0022, iter [00900, 05004], lr: 0.098348, loss: 3.4568
2022-03-05 23:36:03 - train: epoch 0022, iter [01000, 05004], lr: 0.098348, loss: 3.2413
2022-03-05 23:36:37 - train: epoch 0022, iter [01100, 05004], lr: 0.098348, loss: 3.0532
2022-03-05 23:37:10 - train: epoch 0022, iter [01200, 05004], lr: 0.098348, loss: 2.9058
2022-03-05 23:37:44 - train: epoch 0022, iter [01300, 05004], lr: 0.098348, loss: 3.1633
2022-03-05 23:38:17 - train: epoch 0022, iter [01400, 05004], lr: 0.098348, loss: 3.0880
2022-03-05 23:38:51 - train: epoch 0022, iter [01500, 05004], lr: 0.098348, loss: 3.1170
2022-03-05 23:39:24 - train: epoch 0022, iter [01600, 05004], lr: 0.098348, loss: 2.9097
2022-03-05 23:39:58 - train: epoch 0022, iter [01700, 05004], lr: 0.098348, loss: 3.0650
2022-03-05 23:40:32 - train: epoch 0022, iter [01800, 05004], lr: 0.098348, loss: 3.2376
2022-03-05 23:41:05 - train: epoch 0022, iter [01900, 05004], lr: 0.098348, loss: 3.0944
2022-03-05 23:41:39 - train: epoch 0022, iter [02000, 05004], lr: 0.098348, loss: 3.4213
2022-03-05 23:42:11 - train: epoch 0022, iter [02100, 05004], lr: 0.098348, loss: 3.2804
2022-03-05 23:42:44 - train: epoch 0022, iter [02200, 05004], lr: 0.098348, loss: 2.8519
2022-03-05 23:43:16 - train: epoch 0022, iter [02300, 05004], lr: 0.098348, loss: 3.1361
2022-03-05 23:43:50 - train: epoch 0022, iter [02400, 05004], lr: 0.098348, loss: 2.9565
2022-03-05 23:44:24 - train: epoch 0022, iter [02500, 05004], lr: 0.098348, loss: 2.9680
2022-03-05 23:44:58 - train: epoch 0022, iter [02600, 05004], lr: 0.098348, loss: 2.7516
2022-03-05 23:45:31 - train: epoch 0022, iter [02700, 05004], lr: 0.098348, loss: 3.1148
2022-03-05 23:46:04 - train: epoch 0022, iter [02800, 05004], lr: 0.098348, loss: 3.2839
2022-03-05 23:46:38 - train: epoch 0022, iter [02900, 05004], lr: 0.098348, loss: 2.9432
2022-03-05 23:47:12 - train: epoch 0022, iter [03000, 05004], lr: 0.098348, loss: 3.3369
2022-03-05 23:47:46 - train: epoch 0022, iter [03100, 05004], lr: 0.098348, loss: 3.1616
2022-03-05 23:48:20 - train: epoch 0022, iter [03200, 05004], lr: 0.098348, loss: 3.0976
2022-03-05 23:48:53 - train: epoch 0022, iter [03300, 05004], lr: 0.098348, loss: 3.0393
2022-03-05 23:49:27 - train: epoch 0022, iter [03400, 05004], lr: 0.098348, loss: 3.1090
2022-03-05 23:50:00 - train: epoch 0022, iter [03500, 05004], lr: 0.098348, loss: 3.2548
2022-03-05 23:50:33 - train: epoch 0022, iter [03600, 05004], lr: 0.098348, loss: 3.1799
2022-03-05 23:51:05 - train: epoch 0022, iter [03700, 05004], lr: 0.098348, loss: 3.1009
2022-03-05 23:51:40 - train: epoch 0022, iter [03800, 05004], lr: 0.098348, loss: 3.1450
2022-03-05 23:52:12 - train: epoch 0022, iter [03900, 05004], lr: 0.098348, loss: 3.2788
2022-03-05 23:52:47 - train: epoch 0022, iter [04000, 05004], lr: 0.098348, loss: 3.2633
2022-03-05 23:53:21 - train: epoch 0022, iter [04100, 05004], lr: 0.098348, loss: 2.9109
2022-03-05 23:53:55 - train: epoch 0022, iter [04200, 05004], lr: 0.098348, loss: 3.1617
2022-03-05 23:54:28 - train: epoch 0022, iter [04300, 05004], lr: 0.098348, loss: 3.1895
2022-03-05 23:55:02 - train: epoch 0022, iter [04400, 05004], lr: 0.098348, loss: 3.4784
2022-03-05 23:55:36 - train: epoch 0022, iter [04500, 05004], lr: 0.098348, loss: 3.2238
2022-03-05 23:56:10 - train: epoch 0022, iter [04600, 05004], lr: 0.098348, loss: 3.1725
2022-03-05 23:56:43 - train: epoch 0022, iter [04700, 05004], lr: 0.098348, loss: 3.3667
2022-03-05 23:57:17 - train: epoch 0022, iter [04800, 05004], lr: 0.098348, loss: 3.0525
2022-03-05 23:57:50 - train: epoch 0022, iter [04900, 05004], lr: 0.098348, loss: 2.8073
2022-03-05 23:58:22 - train: epoch 0022, iter [05000, 05004], lr: 0.098348, loss: 3.2104
2022-03-05 23:58:23 - train: epoch 022, train_loss: 3.1375
2022-03-05 23:59:37 - eval: epoch: 022, acc1: 48.074%, acc5: 74.584%, test_loss: 2.2589, per_image_load_time: 2.349ms, per_image_inference_time: 0.511ms
2022-03-05 23:59:37 - until epoch: 022, best_acc1: 50.984%
2022-03-05 23:59:37 - epoch 023 lr: 0.09813640337548954
2022-03-06 00:00:16 - train: epoch 0023, iter [00100, 05004], lr: 0.098136, loss: 3.0564
2022-03-06 00:00:50 - train: epoch 0023, iter [00200, 05004], lr: 0.098136, loss: 3.0640
2022-03-06 00:01:23 - train: epoch 0023, iter [00300, 05004], lr: 0.098136, loss: 2.9674
2022-03-06 00:01:57 - train: epoch 0023, iter [00400, 05004], lr: 0.098136, loss: 3.0374
2022-03-06 00:02:31 - train: epoch 0023, iter [00500, 05004], lr: 0.098136, loss: 3.3397
2022-03-06 00:03:05 - train: epoch 0023, iter [00600, 05004], lr: 0.098136, loss: 3.1776
2022-03-06 00:03:39 - train: epoch 0023, iter [00700, 05004], lr: 0.098136, loss: 2.8786
2022-03-06 00:04:13 - train: epoch 0023, iter [00800, 05004], lr: 0.098136, loss: 3.0270
2022-03-06 00:04:47 - train: epoch 0023, iter [00900, 05004], lr: 0.098136, loss: 3.1048
2022-03-06 00:05:21 - train: epoch 0023, iter [01000, 05004], lr: 0.098136, loss: 3.0193
2022-03-06 00:05:53 - train: epoch 0023, iter [01100, 05004], lr: 0.098136, loss: 3.1178
2022-03-06 00:06:26 - train: epoch 0023, iter [01200, 05004], lr: 0.098136, loss: 2.9053
2022-03-06 00:07:01 - train: epoch 0023, iter [01300, 05004], lr: 0.098136, loss: 3.1496
2022-03-06 00:07:34 - train: epoch 0023, iter [01400, 05004], lr: 0.098136, loss: 3.1128
2022-03-06 00:08:08 - train: epoch 0023, iter [01500, 05004], lr: 0.098136, loss: 3.0561
2022-03-06 00:08:43 - train: epoch 0023, iter [01600, 05004], lr: 0.098136, loss: 3.0103
2022-03-06 00:09:16 - train: epoch 0023, iter [01700, 05004], lr: 0.098136, loss: 3.2143
2022-03-06 00:09:50 - train: epoch 0023, iter [01800, 05004], lr: 0.098136, loss: 3.1092
2022-03-06 00:10:24 - train: epoch 0023, iter [01900, 05004], lr: 0.098136, loss: 3.1796
2022-03-06 00:10:58 - train: epoch 0023, iter [02000, 05004], lr: 0.098136, loss: 3.2879
2022-03-06 00:11:31 - train: epoch 0023, iter [02100, 05004], lr: 0.098136, loss: 2.9396
2022-03-06 00:12:06 - train: epoch 0023, iter [02200, 05004], lr: 0.098136, loss: 2.8691
2022-03-06 00:12:40 - train: epoch 0023, iter [02300, 05004], lr: 0.098136, loss: 2.9594
2022-03-06 00:13:14 - train: epoch 0023, iter [02400, 05004], lr: 0.098136, loss: 3.1691
2022-03-06 00:13:47 - train: epoch 0023, iter [02500, 05004], lr: 0.098136, loss: 3.0986
2022-03-06 00:14:21 - train: epoch 0023, iter [02600, 05004], lr: 0.098136, loss: 3.0754
2022-03-06 00:14:55 - train: epoch 0023, iter [02700, 05004], lr: 0.098136, loss: 3.1085
2022-03-06 00:15:28 - train: epoch 0023, iter [02800, 05004], lr: 0.098136, loss: 3.0620
2022-03-06 00:16:02 - train: epoch 0023, iter [02900, 05004], lr: 0.098136, loss: 2.9586
2022-03-06 00:16:36 - train: epoch 0023, iter [03000, 05004], lr: 0.098136, loss: 3.4079
2022-03-06 00:17:08 - train: epoch 0023, iter [03100, 05004], lr: 0.098136, loss: 3.2646
2022-03-06 00:17:41 - train: epoch 0023, iter [03200, 05004], lr: 0.098136, loss: 3.3363
2022-03-06 00:18:13 - train: epoch 0023, iter [03300, 05004], lr: 0.098136, loss: 2.9496
2022-03-06 00:18:46 - train: epoch 0023, iter [03400, 05004], lr: 0.098136, loss: 3.3584
2022-03-06 00:19:18 - train: epoch 0023, iter [03500, 05004], lr: 0.098136, loss: 3.0837
2022-03-06 00:19:50 - train: epoch 0023, iter [03600, 05004], lr: 0.098136, loss: 2.8423
2022-03-06 00:20:23 - train: epoch 0023, iter [03700, 05004], lr: 0.098136, loss: 3.1310
2022-03-06 00:20:55 - train: epoch 0023, iter [03800, 05004], lr: 0.098136, loss: 3.2161
2022-03-06 00:21:27 - train: epoch 0023, iter [03900, 05004], lr: 0.098136, loss: 3.1855
2022-03-06 00:22:01 - train: epoch 0023, iter [04000, 05004], lr: 0.098136, loss: 3.2437
2022-03-06 00:22:34 - train: epoch 0023, iter [04100, 05004], lr: 0.098136, loss: 3.0413
2022-03-06 00:23:07 - train: epoch 0023, iter [04200, 05004], lr: 0.098136, loss: 2.9509
2022-03-06 00:23:40 - train: epoch 0023, iter [04300, 05004], lr: 0.098136, loss: 3.0008
2022-03-06 00:24:13 - train: epoch 0023, iter [04400, 05004], lr: 0.098136, loss: 2.9120
2022-03-06 00:24:46 - train: epoch 0023, iter [04500, 05004], lr: 0.098136, loss: 2.9432
2022-03-06 00:25:18 - train: epoch 0023, iter [04600, 05004], lr: 0.098136, loss: 3.2259
2022-03-06 00:25:51 - train: epoch 0023, iter [04700, 05004], lr: 0.098136, loss: 2.9238
2022-03-06 00:26:23 - train: epoch 0023, iter [04800, 05004], lr: 0.098136, loss: 2.9582
2022-03-06 00:26:55 - train: epoch 0023, iter [04900, 05004], lr: 0.098136, loss: 3.2283
2022-03-06 00:27:26 - train: epoch 0023, iter [05000, 05004], lr: 0.098136, loss: 3.3708
2022-03-06 00:27:27 - train: epoch 023, train_loss: 3.1263
2022-03-06 00:28:37 - eval: epoch: 023, acc1: 45.774%, acc5: 72.298%, test_loss: 2.3951, per_image_load_time: 1.510ms, per_image_inference_time: 0.458ms
2022-03-06 00:28:38 - until epoch: 023, best_acc1: 50.984%
2022-03-06 00:28:38 - epoch 024 lr: 0.09791229145545832
2022-03-06 00:29:15 - train: epoch 0024, iter [00100, 05004], lr: 0.097912, loss: 3.0287
2022-03-06 00:29:49 - train: epoch 0024, iter [00200, 05004], lr: 0.097912, loss: 3.2144
2022-03-06 00:30:21 - train: epoch 0024, iter [00300, 05004], lr: 0.097912, loss: 3.1350
2022-03-06 00:30:54 - train: epoch 0024, iter [00400, 05004], lr: 0.097912, loss: 2.9895
2022-03-06 00:31:28 - train: epoch 0024, iter [00500, 05004], lr: 0.097912, loss: 3.1204
2022-03-06 00:32:01 - train: epoch 0024, iter [00600, 05004], lr: 0.097912, loss: 2.9514
2022-03-06 00:32:35 - train: epoch 0024, iter [00700, 05004], lr: 0.097912, loss: 3.0145
2022-03-06 00:33:08 - train: epoch 0024, iter [00800, 05004], lr: 0.097912, loss: 3.0280
2022-03-06 00:33:41 - train: epoch 0024, iter [00900, 05004], lr: 0.097912, loss: 3.0914
2022-03-06 00:34:14 - train: epoch 0024, iter [01000, 05004], lr: 0.097912, loss: 2.9350
2022-03-06 00:34:46 - train: epoch 0024, iter [01100, 05004], lr: 0.097912, loss: 2.6447
2022-03-06 00:35:18 - train: epoch 0024, iter [01200, 05004], lr: 0.097912, loss: 2.9243
2022-03-06 00:35:50 - train: epoch 0024, iter [01300, 05004], lr: 0.097912, loss: 3.3191
2022-03-06 00:36:22 - train: epoch 0024, iter [01400, 05004], lr: 0.097912, loss: 3.0080
2022-03-06 00:36:54 - train: epoch 0024, iter [01500, 05004], lr: 0.097912, loss: 3.2037
2022-03-06 00:37:27 - train: epoch 0024, iter [01600, 05004], lr: 0.097912, loss: 3.0787
2022-03-06 00:37:59 - train: epoch 0024, iter [01700, 05004], lr: 0.097912, loss: 3.1373
2022-03-06 00:38:31 - train: epoch 0024, iter [01800, 05004], lr: 0.097912, loss: 3.3449
2022-03-06 00:39:04 - train: epoch 0024, iter [01900, 05004], lr: 0.097912, loss: 3.1495
2022-03-06 00:39:38 - train: epoch 0024, iter [02000, 05004], lr: 0.097912, loss: 3.1339
2022-03-06 00:40:11 - train: epoch 0024, iter [02100, 05004], lr: 0.097912, loss: 3.0769
2022-03-06 00:40:45 - train: epoch 0024, iter [02200, 05004], lr: 0.097912, loss: 2.9215
2022-03-06 00:41:19 - train: epoch 0024, iter [02300, 05004], lr: 0.097912, loss: 2.9729
2022-03-06 00:41:51 - train: epoch 0024, iter [02400, 05004], lr: 0.097912, loss: 3.0640
2022-03-06 00:42:24 - train: epoch 0024, iter [02500, 05004], lr: 0.097912, loss: 3.1306
2022-03-06 00:42:57 - train: epoch 0024, iter [02600, 05004], lr: 0.097912, loss: 3.1013
2022-03-06 00:43:28 - train: epoch 0024, iter [02700, 05004], lr: 0.097912, loss: 3.1976
2022-03-06 00:44:00 - train: epoch 0024, iter [02800, 05004], lr: 0.097912, loss: 3.1329
2022-03-06 00:44:32 - train: epoch 0024, iter [02900, 05004], lr: 0.097912, loss: 3.3286
2022-03-06 00:45:05 - train: epoch 0024, iter [03000, 05004], lr: 0.097912, loss: 2.9969
2022-03-06 00:45:37 - train: epoch 0024, iter [03100, 05004], lr: 0.097912, loss: 3.1853
2022-03-06 00:46:10 - train: epoch 0024, iter [03200, 05004], lr: 0.097912, loss: 3.0946
2022-03-06 00:46:43 - train: epoch 0024, iter [03300, 05004], lr: 0.097912, loss: 3.0921
2022-03-06 00:47:16 - train: epoch 0024, iter [03400, 05004], lr: 0.097912, loss: 2.9967
2022-03-06 00:47:48 - train: epoch 0024, iter [03500, 05004], lr: 0.097912, loss: 3.2767
2022-03-06 00:48:22 - train: epoch 0024, iter [03600, 05004], lr: 0.097912, loss: 3.2775
2022-03-06 00:48:55 - train: epoch 0024, iter [03700, 05004], lr: 0.097912, loss: 3.0503
2022-03-06 00:49:29 - train: epoch 0024, iter [03800, 05004], lr: 0.097912, loss: 3.6533
2022-03-06 00:50:02 - train: epoch 0024, iter [03900, 05004], lr: 0.097912, loss: 3.3318
2022-03-06 00:50:35 - train: epoch 0024, iter [04000, 05004], lr: 0.097912, loss: 3.0786
2022-03-06 00:51:07 - train: epoch 0024, iter [04100, 05004], lr: 0.097912, loss: 3.1005
2022-03-06 00:51:40 - train: epoch 0024, iter [04200, 05004], lr: 0.097912, loss: 2.9903
2022-03-06 00:52:11 - train: epoch 0024, iter [04300, 05004], lr: 0.097912, loss: 3.0008
2022-03-06 00:52:44 - train: epoch 0024, iter [04400, 05004], lr: 0.097912, loss: 3.0382
2022-03-06 00:53:18 - train: epoch 0024, iter [04500, 05004], lr: 0.097912, loss: 2.7555
2022-03-06 00:53:50 - train: epoch 0024, iter [04600, 05004], lr: 0.097912, loss: 3.2023
2022-03-06 00:54:24 - train: epoch 0024, iter [04700, 05004], lr: 0.097912, loss: 3.1279
2022-03-06 00:54:56 - train: epoch 0024, iter [04800, 05004], lr: 0.097912, loss: 2.7920
2022-03-06 00:55:29 - train: epoch 0024, iter [04900, 05004], lr: 0.097912, loss: 3.1062
2022-03-06 00:56:00 - train: epoch 0024, iter [05000, 05004], lr: 0.097912, loss: 3.1971
2022-03-06 00:56:01 - train: epoch 024, train_loss: 3.1237
2022-03-06 00:57:14 - eval: epoch: 024, acc1: 49.664%, acc5: 75.642%, test_loss: 2.1995, per_image_load_time: 1.846ms, per_image_inference_time: 0.498ms
2022-03-06 00:57:15 - until epoch: 024, best_acc1: 50.984%
2022-03-06 00:57:15 - epoch 025 lr: 0.09767574389743683
2022-03-06 00:57:54 - train: epoch 0025, iter [00100, 05004], lr: 0.097676, loss: 3.0595
2022-03-06 00:58:27 - train: epoch 0025, iter [00200, 05004], lr: 0.097676, loss: 2.8117
2022-03-06 00:58:59 - train: epoch 0025, iter [00300, 05004], lr: 0.097676, loss: 2.9575
2022-03-06 00:59:31 - train: epoch 0025, iter [00400, 05004], lr: 0.097676, loss: 2.9522
2022-03-06 01:00:04 - train: epoch 0025, iter [00500, 05004], lr: 0.097676, loss: 3.0108
2022-03-06 01:00:37 - train: epoch 0025, iter [00600, 05004], lr: 0.097676, loss: 2.8317
2022-03-06 01:01:09 - train: epoch 0025, iter [00700, 05004], lr: 0.097676, loss: 3.0488
2022-03-06 01:01:42 - train: epoch 0025, iter [00800, 05004], lr: 0.097676, loss: 3.1300
2022-03-06 01:02:14 - train: epoch 0025, iter [00900, 05004], lr: 0.097676, loss: 3.0290
2022-03-06 01:02:48 - train: epoch 0025, iter [01000, 05004], lr: 0.097676, loss: 3.0772
2022-03-06 01:03:20 - train: epoch 0025, iter [01100, 05004], lr: 0.097676, loss: 3.1490
2022-03-06 01:03:53 - train: epoch 0025, iter [01200, 05004], lr: 0.097676, loss: 3.2280
2022-03-06 01:04:26 - train: epoch 0025, iter [01300, 05004], lr: 0.097676, loss: 2.9380
2022-03-06 01:04:59 - train: epoch 0025, iter [01400, 05004], lr: 0.097676, loss: 3.2505
2022-03-06 01:05:32 - train: epoch 0025, iter [01500, 05004], lr: 0.097676, loss: 2.8672
2022-03-06 01:06:04 - train: epoch 0025, iter [01600, 05004], lr: 0.097676, loss: 3.0332
2022-03-06 01:06:36 - train: epoch 0025, iter [01700, 05004], lr: 0.097676, loss: 3.0241
2022-03-06 01:07:10 - train: epoch 0025, iter [01800, 05004], lr: 0.097676, loss: 2.9465
2022-03-06 01:07:43 - train: epoch 0025, iter [01900, 05004], lr: 0.097676, loss: 2.7630
2022-03-06 01:08:16 - train: epoch 0025, iter [02000, 05004], lr: 0.097676, loss: 2.9859
2022-03-06 01:08:49 - train: epoch 0025, iter [02100, 05004], lr: 0.097676, loss: 3.0521
2022-03-06 01:09:22 - train: epoch 0025, iter [02200, 05004], lr: 0.097676, loss: 2.9678
2022-03-06 01:09:55 - train: epoch 0025, iter [02300, 05004], lr: 0.097676, loss: 3.4096
2022-03-06 01:10:28 - train: epoch 0025, iter [02400, 05004], lr: 0.097676, loss: 2.9115
2022-03-06 01:11:01 - train: epoch 0025, iter [02500, 05004], lr: 0.097676, loss: 3.3380
2022-03-06 01:11:33 - train: epoch 0025, iter [02600, 05004], lr: 0.097676, loss: 3.2611
2022-03-06 01:12:07 - train: epoch 0025, iter [02700, 05004], lr: 0.097676, loss: 3.1603
2022-03-06 01:12:39 - train: epoch 0025, iter [02800, 05004], lr: 0.097676, loss: 2.9962
2022-03-06 01:13:13 - train: epoch 0025, iter [02900, 05004], lr: 0.097676, loss: 3.3385
2022-03-06 01:13:45 - train: epoch 0025, iter [03000, 05004], lr: 0.097676, loss: 3.0948
2022-03-06 01:14:17 - train: epoch 0025, iter [03100, 05004], lr: 0.097676, loss: 3.3033
2022-03-06 01:14:50 - train: epoch 0025, iter [03200, 05004], lr: 0.097676, loss: 3.0123
2022-03-06 01:15:24 - train: epoch 0025, iter [03300, 05004], lr: 0.097676, loss: 3.2025
2022-03-06 01:15:57 - train: epoch 0025, iter [03400, 05004], lr: 0.097676, loss: 3.2789
2022-03-06 01:16:31 - train: epoch 0025, iter [03500, 05004], lr: 0.097676, loss: 2.9127
2022-03-06 01:17:03 - train: epoch 0025, iter [03600, 05004], lr: 0.097676, loss: 3.2902
2022-03-06 01:17:37 - train: epoch 0025, iter [03700, 05004], lr: 0.097676, loss: 3.0523
2022-03-06 01:18:09 - train: epoch 0025, iter [03800, 05004], lr: 0.097676, loss: 3.1406
2022-03-06 01:18:42 - train: epoch 0025, iter [03900, 05004], lr: 0.097676, loss: 3.1746
2022-03-06 01:19:14 - train: epoch 0025, iter [04000, 05004], lr: 0.097676, loss: 3.1114
2022-03-06 01:19:47 - train: epoch 0025, iter [04100, 05004], lr: 0.097676, loss: 3.4290
2022-03-06 01:20:20 - train: epoch 0025, iter [04200, 05004], lr: 0.097676, loss: 3.1500
2022-03-06 01:20:53 - train: epoch 0025, iter [04300, 05004], lr: 0.097676, loss: 2.7953
2022-03-06 01:21:25 - train: epoch 0025, iter [04400, 05004], lr: 0.097676, loss: 2.8832
2022-03-06 01:21:57 - train: epoch 0025, iter [04500, 05004], lr: 0.097676, loss: 2.9766
2022-03-06 01:22:30 - train: epoch 0025, iter [04600, 05004], lr: 0.097676, loss: 3.2567
2022-03-06 01:23:04 - train: epoch 0025, iter [04700, 05004], lr: 0.097676, loss: 3.1002
2022-03-06 01:23:37 - train: epoch 0025, iter [04800, 05004], lr: 0.097676, loss: 2.8581
2022-03-06 01:24:11 - train: epoch 0025, iter [04900, 05004], lr: 0.097676, loss: 2.9826
2022-03-06 01:24:43 - train: epoch 0025, iter [05000, 05004], lr: 0.097676, loss: 3.3902
2022-03-06 01:24:44 - train: epoch 025, train_loss: 3.1104
2022-03-06 01:25:56 - eval: epoch: 025, acc1: 51.448%, acc5: 77.746%, test_loss: 2.0791, per_image_load_time: 2.358ms, per_image_inference_time: 0.442ms
2022-03-06 01:25:57 - until epoch: 025, best_acc1: 51.448%
2022-03-06 01:25:57 - epoch 026 lr: 0.09742682209735727
2022-03-06 01:26:35 - train: epoch 0026, iter [00100, 05004], lr: 0.097427, loss: 2.8645
2022-03-06 01:27:08 - train: epoch 0026, iter [00200, 05004], lr: 0.097427, loss: 3.0071
2022-03-06 01:27:39 - train: epoch 0026, iter [00300, 05004], lr: 0.097427, loss: 2.9442
2022-03-06 01:28:13 - train: epoch 0026, iter [00400, 05004], lr: 0.097427, loss: 2.9944
2022-03-06 01:28:45 - train: epoch 0026, iter [00500, 05004], lr: 0.097427, loss: 3.0098
2022-03-06 01:29:17 - train: epoch 0026, iter [00600, 05004], lr: 0.097427, loss: 3.2599
2022-03-06 01:29:49 - train: epoch 0026, iter [00700, 05004], lr: 0.097427, loss: 2.9854
2022-03-06 01:30:22 - train: epoch 0026, iter [00800, 05004], lr: 0.097427, loss: 3.0222
2022-03-06 01:30:54 - train: epoch 0026, iter [00900, 05004], lr: 0.097427, loss: 3.2490
2022-03-06 01:31:29 - train: epoch 0026, iter [01000, 05004], lr: 0.097427, loss: 3.2106
2022-03-06 01:32:02 - train: epoch 0026, iter [01100, 05004], lr: 0.097427, loss: 3.0752
2022-03-06 01:32:35 - train: epoch 0026, iter [01200, 05004], lr: 0.097427, loss: 3.0126
2022-03-06 01:33:08 - train: epoch 0026, iter [01300, 05004], lr: 0.097427, loss: 2.9858
2022-03-06 01:33:42 - train: epoch 0026, iter [01400, 05004], lr: 0.097427, loss: 3.2789
2022-03-06 01:34:14 - train: epoch 0026, iter [01500, 05004], lr: 0.097427, loss: 3.0944
2022-03-06 01:34:47 - train: epoch 0026, iter [01600, 05004], lr: 0.097427, loss: 3.1670
2022-03-06 01:35:20 - train: epoch 0026, iter [01700, 05004], lr: 0.097427, loss: 3.0818
2022-03-06 01:35:53 - train: epoch 0026, iter [01800, 05004], lr: 0.097427, loss: 3.4094
2022-03-06 01:36:26 - train: epoch 0026, iter [01900, 05004], lr: 0.097427, loss: 3.3024
2022-03-06 01:36:58 - train: epoch 0026, iter [02000, 05004], lr: 0.097427, loss: 3.0945
2022-03-06 01:37:29 - train: epoch 0026, iter [02100, 05004], lr: 0.097427, loss: 2.9132
2022-03-06 01:38:03 - train: epoch 0026, iter [02200, 05004], lr: 0.097427, loss: 2.9111
2022-03-06 01:38:35 - train: epoch 0026, iter [02300, 05004], lr: 0.097427, loss: 3.3162
2022-03-06 01:39:08 - train: epoch 0026, iter [02400, 05004], lr: 0.097427, loss: 3.4542
2022-03-06 01:39:42 - train: epoch 0026, iter [02500, 05004], lr: 0.097427, loss: 3.0353
2022-03-06 01:40:15 - train: epoch 0026, iter [02600, 05004], lr: 0.097427, loss: 2.9704
2022-03-06 01:40:47 - train: epoch 0026, iter [02700, 05004], lr: 0.097427, loss: 2.9579
2022-03-06 01:41:21 - train: epoch 0026, iter [02800, 05004], lr: 0.097427, loss: 3.0567
2022-03-06 01:41:55 - train: epoch 0026, iter [02900, 05004], lr: 0.097427, loss: 3.1554
2022-03-06 01:42:28 - train: epoch 0026, iter [03000, 05004], lr: 0.097427, loss: 2.7996
2022-03-06 01:43:01 - train: epoch 0026, iter [03100, 05004], lr: 0.097427, loss: 3.0816
2022-03-06 01:43:35 - train: epoch 0026, iter [03200, 05004], lr: 0.097427, loss: 3.2722
2022-03-06 01:44:07 - train: epoch 0026, iter [03300, 05004], lr: 0.097427, loss: 3.0531
2022-03-06 01:44:39 - train: epoch 0026, iter [03400, 05004], lr: 0.097427, loss: 3.0330
2022-03-06 01:45:11 - train: epoch 0026, iter [03500, 05004], lr: 0.097427, loss: 3.3438
2022-03-06 01:45:44 - train: epoch 0026, iter [03600, 05004], lr: 0.097427, loss: 2.9413
2022-03-06 01:46:17 - train: epoch 0026, iter [03700, 05004], lr: 0.097427, loss: 3.4268
2022-03-06 01:46:50 - train: epoch 0026, iter [03800, 05004], lr: 0.097427, loss: 3.1788
2022-03-06 01:47:22 - train: epoch 0026, iter [03900, 05004], lr: 0.097427, loss: 3.1730
2022-03-06 01:47:55 - train: epoch 0026, iter [04000, 05004], lr: 0.097427, loss: 3.2505
2022-03-06 01:48:28 - train: epoch 0026, iter [04100, 05004], lr: 0.097427, loss: 3.1609
2022-03-06 01:49:01 - train: epoch 0026, iter [04200, 05004], lr: 0.097427, loss: 3.1277
2022-03-06 01:49:35 - train: epoch 0026, iter [04300, 05004], lr: 0.097427, loss: 3.0253
2022-03-06 01:50:08 - train: epoch 0026, iter [04400, 05004], lr: 0.097427, loss: 3.2728
2022-03-06 01:50:42 - train: epoch 0026, iter [04500, 05004], lr: 0.097427, loss: 3.4310
2022-03-06 01:51:15 - train: epoch 0026, iter [04600, 05004], lr: 0.097427, loss: 2.8568
2022-03-06 01:51:48 - train: epoch 0026, iter [04700, 05004], lr: 0.097427, loss: 3.0815
2022-03-06 01:52:20 - train: epoch 0026, iter [04800, 05004], lr: 0.097427, loss: 3.0304
2022-03-06 01:52:52 - train: epoch 0026, iter [04900, 05004], lr: 0.097427, loss: 3.2419
2022-03-06 01:53:23 - train: epoch 0026, iter [05000, 05004], lr: 0.097427, loss: 3.5516
2022-03-06 01:53:24 - train: epoch 026, train_loss: 3.1049
2022-03-06 01:54:36 - eval: epoch: 026, acc1: 51.798%, acc5: 77.926%, test_loss: 2.0520, per_image_load_time: 2.323ms, per_image_inference_time: 0.454ms
2022-03-06 01:54:37 - until epoch: 026, best_acc1: 51.798%
2022-03-06 01:54:37 - epoch 027 lr: 0.09716559066288716
2022-03-06 01:55:15 - train: epoch 0027, iter [00100, 05004], lr: 0.097166, loss: 3.2112
2022-03-06 01:55:48 - train: epoch 0027, iter [00200, 05004], lr: 0.097166, loss: 2.8905
2022-03-06 01:56:20 - train: epoch 0027, iter [00300, 05004], lr: 0.097166, loss: 3.4561
2022-03-06 01:56:54 - train: epoch 0027, iter [00400, 05004], lr: 0.097166, loss: 3.3526
2022-03-06 01:57:26 - train: epoch 0027, iter [00500, 05004], lr: 0.097166, loss: 3.0552
2022-03-06 01:58:00 - train: epoch 0027, iter [00600, 05004], lr: 0.097166, loss: 3.1596
2022-03-06 01:58:33 - train: epoch 0027, iter [00700, 05004], lr: 0.097166, loss: 3.3060
2022-03-06 01:59:06 - train: epoch 0027, iter [00800, 05004], lr: 0.097166, loss: 3.1658
2022-03-06 01:59:40 - train: epoch 0027, iter [00900, 05004], lr: 0.097166, loss: 3.0077
2022-03-06 02:00:12 - train: epoch 0027, iter [01000, 05004], lr: 0.097166, loss: 3.2383
2022-03-06 02:00:43 - train: epoch 0027, iter [01100, 05004], lr: 0.097166, loss: 2.9997
2022-03-06 02:01:16 - train: epoch 0027, iter [01200, 05004], lr: 0.097166, loss: 3.2912
2022-03-06 02:01:49 - train: epoch 0027, iter [01300, 05004], lr: 0.097166, loss: 3.1013
2022-03-06 02:02:22 - train: epoch 0027, iter [01400, 05004], lr: 0.097166, loss: 3.2862
2022-03-06 02:02:55 - train: epoch 0027, iter [01500, 05004], lr: 0.097166, loss: 3.1017
2022-03-06 02:03:28 - train: epoch 0027, iter [01600, 05004], lr: 0.097166, loss: 2.9925
2022-03-06 02:04:01 - train: epoch 0027, iter [01700, 05004], lr: 0.097166, loss: 3.1102
2022-03-06 02:04:34 - train: epoch 0027, iter [01800, 05004], lr: 0.097166, loss: 2.8928
2022-03-06 02:05:06 - train: epoch 0027, iter [01900, 05004], lr: 0.097166, loss: 3.1745
2022-03-06 02:05:40 - train: epoch 0027, iter [02000, 05004], lr: 0.097166, loss: 3.0317
2022-03-06 02:06:13 - train: epoch 0027, iter [02100, 05004], lr: 0.097166, loss: 3.2526
2022-03-06 02:06:46 - train: epoch 0027, iter [02200, 05004], lr: 0.097166, loss: 3.2606
2022-03-06 02:07:20 - train: epoch 0027, iter [02300, 05004], lr: 0.097166, loss: 3.2987
2022-03-06 02:07:53 - train: epoch 0027, iter [02400, 05004], lr: 0.097166, loss: 3.2167
2022-03-06 02:08:26 - train: epoch 0027, iter [02500, 05004], lr: 0.097166, loss: 3.2593
2022-03-06 02:08:59 - train: epoch 0027, iter [02600, 05004], lr: 0.097166, loss: 3.1326
2022-03-06 02:09:31 - train: epoch 0027, iter [02700, 05004], lr: 0.097166, loss: 3.2112
2022-03-06 02:10:04 - train: epoch 0027, iter [02800, 05004], lr: 0.097166, loss: 3.2118
2022-03-06 02:10:37 - train: epoch 0027, iter [02900, 05004], lr: 0.097166, loss: 3.0238
2022-03-06 02:11:10 - train: epoch 0027, iter [03000, 05004], lr: 0.097166, loss: 2.8866
2022-03-06 02:11:42 - train: epoch 0027, iter [03100, 05004], lr: 0.097166, loss: 2.8271
2022-03-06 02:12:15 - train: epoch 0027, iter [03200, 05004], lr: 0.097166, loss: 2.9478
2022-03-06 02:12:48 - train: epoch 0027, iter [03300, 05004], lr: 0.097166, loss: 3.0303
2022-03-06 02:13:20 - train: epoch 0027, iter [03400, 05004], lr: 0.097166, loss: 2.9954
2022-03-06 02:13:53 - train: epoch 0027, iter [03500, 05004], lr: 0.097166, loss: 3.2754
2022-03-06 02:14:26 - train: epoch 0027, iter [03600, 05004], lr: 0.097166, loss: 2.9026
2022-03-06 02:15:00 - train: epoch 0027, iter [03700, 05004], lr: 0.097166, loss: 3.0966
2022-03-06 02:15:32 - train: epoch 0027, iter [03800, 05004], lr: 0.097166, loss: 2.8938
2022-03-06 02:16:05 - train: epoch 0027, iter [03900, 05004], lr: 0.097166, loss: 2.9540
2022-03-06 02:16:38 - train: epoch 0027, iter [04000, 05004], lr: 0.097166, loss: 3.2013
2022-03-06 02:17:11 - train: epoch 0027, iter [04100, 05004], lr: 0.097166, loss: 2.9748
2022-03-06 02:17:43 - train: epoch 0027, iter [04200, 05004], lr: 0.097166, loss: 3.2666
2022-03-06 02:18:17 - train: epoch 0027, iter [04300, 05004], lr: 0.097166, loss: 3.1395
2022-03-06 02:18:48 - train: epoch 0027, iter [04400, 05004], lr: 0.097166, loss: 2.9555
2022-03-06 02:19:22 - train: epoch 0027, iter [04500, 05004], lr: 0.097166, loss: 2.8757
2022-03-06 02:19:54 - train: epoch 0027, iter [04600, 05004], lr: 0.097166, loss: 2.9651
2022-03-06 02:20:28 - train: epoch 0027, iter [04700, 05004], lr: 0.097166, loss: 3.2389
2022-03-06 02:21:00 - train: epoch 0027, iter [04800, 05004], lr: 0.097166, loss: 3.1820
2022-03-06 02:21:33 - train: epoch 0027, iter [04900, 05004], lr: 0.097166, loss: 3.1557
2022-03-06 02:22:05 - train: epoch 0027, iter [05000, 05004], lr: 0.097166, loss: 2.6992
2022-03-06 02:22:06 - train: epoch 027, train_loss: 3.1023
2022-03-06 02:23:18 - eval: epoch: 027, acc1: 49.672%, acc5: 75.792%, test_loss: 2.1800, per_image_load_time: 2.356ms, per_image_inference_time: 0.464ms
2022-03-06 02:23:19 - until epoch: 027, best_acc1: 51.798%
2022-03-06 02:23:19 - epoch 028 lr: 0.09689211739666023
2022-03-06 02:23:57 - train: epoch 0028, iter [00100, 05004], lr: 0.096892, loss: 2.9125
2022-03-06 02:24:30 - train: epoch 0028, iter [00200, 05004], lr: 0.096892, loss: 2.9866
2022-03-06 02:25:04 - train: epoch 0028, iter [00300, 05004], lr: 0.096892, loss: 2.7191
2022-03-06 02:25:37 - train: epoch 0028, iter [00400, 05004], lr: 0.096892, loss: 3.0005
2022-03-06 02:26:10 - train: epoch 0028, iter [00500, 05004], lr: 0.096892, loss: 2.8828
2022-03-06 02:26:43 - train: epoch 0028, iter [00600, 05004], lr: 0.096892, loss: 3.0351
2022-03-06 02:27:16 - train: epoch 0028, iter [00700, 05004], lr: 0.096892, loss: 3.3325
2022-03-06 02:27:49 - train: epoch 0028, iter [00800, 05004], lr: 0.096892, loss: 2.7795
2022-03-06 02:28:22 - train: epoch 0028, iter [00900, 05004], lr: 0.096892, loss: 2.8521
2022-03-06 02:28:54 - train: epoch 0028, iter [01000, 05004], lr: 0.096892, loss: 3.2477
2022-03-06 02:29:27 - train: epoch 0028, iter [01100, 05004], lr: 0.096892, loss: 2.9839
2022-03-06 02:30:00 - train: epoch 0028, iter [01200, 05004], lr: 0.096892, loss: 2.8570
2022-03-06 02:30:33 - train: epoch 0028, iter [01300, 05004], lr: 0.096892, loss: 3.2578
2022-03-06 02:31:04 - train: epoch 0028, iter [01400, 05004], lr: 0.096892, loss: 3.4262
2022-03-06 02:31:37 - train: epoch 0028, iter [01500, 05004], lr: 0.096892, loss: 3.0815
2022-03-06 02:32:10 - train: epoch 0028, iter [01600, 05004], lr: 0.096892, loss: 3.1338
2022-03-06 02:32:44 - train: epoch 0028, iter [01700, 05004], lr: 0.096892, loss: 3.3413
2022-03-06 02:33:17 - train: epoch 0028, iter [01800, 05004], lr: 0.096892, loss: 3.1007
2022-03-06 02:33:51 - train: epoch 0028, iter [01900, 05004], lr: 0.096892, loss: 3.0966
2022-03-06 02:34:23 - train: epoch 0028, iter [02000, 05004], lr: 0.096892, loss: 3.2885
2022-03-06 02:34:56 - train: epoch 0028, iter [02100, 05004], lr: 0.096892, loss: 3.1464
2022-03-06 02:35:29 - train: epoch 0028, iter [02200, 05004], lr: 0.096892, loss: 2.9913
2022-03-06 02:36:01 - train: epoch 0028, iter [02300, 05004], lr: 0.096892, loss: 3.1488
2022-03-06 02:36:34 - train: epoch 0028, iter [02400, 05004], lr: 0.096892, loss: 3.2358
2022-03-06 02:37:07 - train: epoch 0028, iter [02500, 05004], lr: 0.096892, loss: 3.1631
2022-03-06 02:37:39 - train: epoch 0028, iter [02600, 05004], lr: 0.096892, loss: 2.8241
2022-03-06 02:38:12 - train: epoch 0028, iter [02700, 05004], lr: 0.096892, loss: 2.9587
2022-03-06 02:38:45 - train: epoch 0028, iter [02800, 05004], lr: 0.096892, loss: 3.0352
2022-03-06 02:39:16 - train: epoch 0028, iter [02900, 05004], lr: 0.096892, loss: 3.1066
2022-03-06 02:39:50 - train: epoch 0028, iter [03000, 05004], lr: 0.096892, loss: 2.9980
2022-03-06 02:40:23 - train: epoch 0028, iter [03100, 05004], lr: 0.096892, loss: 3.2239
2022-03-06 02:40:57 - train: epoch 0028, iter [03200, 05004], lr: 0.096892, loss: 3.2233
2022-03-06 02:41:29 - train: epoch 0028, iter [03300, 05004], lr: 0.096892, loss: 2.9646
2022-03-06 02:42:03 - train: epoch 0028, iter [03400, 05004], lr: 0.096892, loss: 3.2118
2022-03-06 02:42:36 - train: epoch 0028, iter [03500, 05004], lr: 0.096892, loss: 3.0016
2022-03-06 02:43:10 - train: epoch 0028, iter [03600, 05004], lr: 0.096892, loss: 3.1823
2022-03-06 02:43:42 - train: epoch 0028, iter [03700, 05004], lr: 0.096892, loss: 3.0929
2022-03-06 02:44:15 - train: epoch 0028, iter [03800, 05004], lr: 0.096892, loss: 2.5997
2022-03-06 02:44:47 - train: epoch 0028, iter [03900, 05004], lr: 0.096892, loss: 3.0514
2022-03-06 02:45:21 - train: epoch 0028, iter [04000, 05004], lr: 0.096892, loss: 3.1824
2022-03-06 02:45:52 - train: epoch 0028, iter [04100, 05004], lr: 0.096892, loss: 3.1719
2022-03-06 02:46:24 - train: epoch 0028, iter [04200, 05004], lr: 0.096892, loss: 3.4241
2022-03-06 02:46:56 - train: epoch 0028, iter [04300, 05004], lr: 0.096892, loss: 3.3599
2022-03-06 02:47:29 - train: epoch 0028, iter [04400, 05004], lr: 0.096892, loss: 3.1377
2022-03-06 02:48:02 - train: epoch 0028, iter [04500, 05004], lr: 0.096892, loss: 2.9694
2022-03-06 02:48:35 - train: epoch 0028, iter [04600, 05004], lr: 0.096892, loss: 3.1011
2022-03-06 02:49:08 - train: epoch 0028, iter [04700, 05004], lr: 0.096892, loss: 3.2094
2022-03-06 02:49:41 - train: epoch 0028, iter [04800, 05004], lr: 0.096892, loss: 3.0192
2022-03-06 02:50:15 - train: epoch 0028, iter [04900, 05004], lr: 0.096892, loss: 2.9199
2022-03-06 02:50:46 - train: epoch 0028, iter [05000, 05004], lr: 0.096892, loss: 2.9815
2022-03-06 02:50:47 - train: epoch 028, train_loss: 3.0946
2022-03-06 02:52:00 - eval: epoch: 028, acc1: 45.760%, acc5: 71.704%, test_loss: 2.4380, per_image_load_time: 2.370ms, per_image_inference_time: 0.469ms
2022-03-06 02:52:01 - until epoch: 028, best_acc1: 51.798%
2022-03-06 02:52:01 - epoch 029 lr: 0.0966064732786784
2022-03-06 02:52:39 - train: epoch 0029, iter [00100, 05004], lr: 0.096606, loss: 3.1481
2022-03-06 02:53:12 - train: epoch 0029, iter [00200, 05004], lr: 0.096606, loss: 3.0893
2022-03-06 02:53:45 - train: epoch 0029, iter [00300, 05004], lr: 0.096606, loss: 3.4287
2022-03-06 02:54:16 - train: epoch 0029, iter [00400, 05004], lr: 0.096606, loss: 2.9373
2022-03-06 02:54:49 - train: epoch 0029, iter [00500, 05004], lr: 0.096606, loss: 3.0098
2022-03-06 02:55:21 - train: epoch 0029, iter [00600, 05004], lr: 0.096606, loss: 3.3172
2022-03-06 02:55:54 - train: epoch 0029, iter [00700, 05004], lr: 0.096606, loss: 2.6851
2022-03-06 02:56:27 - train: epoch 0029, iter [00800, 05004], lr: 0.096606, loss: 3.0030
2022-03-06 02:56:59 - train: epoch 0029, iter [00900, 05004], lr: 0.096606, loss: 3.0849
2022-03-06 02:57:33 - train: epoch 0029, iter [01000, 05004], lr: 0.096606, loss: 3.0368
2022-03-06 02:58:06 - train: epoch 0029, iter [01100, 05004], lr: 0.096606, loss: 3.2273
2022-03-06 02:58:39 - train: epoch 0029, iter [01200, 05004], lr: 0.096606, loss: 3.2231
2022-03-06 02:59:12 - train: epoch 0029, iter [01300, 05004], lr: 0.096606, loss: 3.2601
2022-03-06 02:59:45 - train: epoch 0029, iter [01400, 05004], lr: 0.096606, loss: 3.5167
2022-03-06 03:00:18 - train: epoch 0029, iter [01500, 05004], lr: 0.096606, loss: 3.2651
2022-03-06 03:00:51 - train: epoch 0029, iter [01600, 05004], lr: 0.096606, loss: 3.2005
2022-03-06 03:01:24 - train: epoch 0029, iter [01700, 05004], lr: 0.096606, loss: 3.0909
2022-03-06 03:01:55 - train: epoch 0029, iter [01800, 05004], lr: 0.096606, loss: 3.1781
2022-03-06 03:02:28 - train: epoch 0029, iter [01900, 05004], lr: 0.096606, loss: 2.8711
2022-03-06 03:03:00 - train: epoch 0029, iter [02000, 05004], lr: 0.096606, loss: 3.1926
2022-03-06 03:03:33 - train: epoch 0029, iter [02100, 05004], lr: 0.096606, loss: 3.0612
2022-03-06 03:04:05 - train: epoch 0029, iter [02200, 05004], lr: 0.096606, loss: 3.0968
2022-03-06 03:04:38 - train: epoch 0029, iter [02300, 05004], lr: 0.096606, loss: 3.1059
2022-03-06 03:05:11 - train: epoch 0029, iter [02400, 05004], lr: 0.096606, loss: 2.9177
2022-03-06 03:05:44 - train: epoch 0029, iter [02500, 05004], lr: 0.096606, loss: 3.1733
2022-03-06 03:06:18 - train: epoch 0029, iter [02600, 05004], lr: 0.096606, loss: 3.0057
2022-03-06 03:06:51 - train: epoch 0029, iter [02700, 05004], lr: 0.096606, loss: 2.9332
2022-03-06 03:07:24 - train: epoch 0029, iter [02800, 05004], lr: 0.096606, loss: 2.9968
2022-03-06 03:07:57 - train: epoch 0029, iter [02900, 05004], lr: 0.096606, loss: 3.0184
2022-03-06 03:08:30 - train: epoch 0029, iter [03000, 05004], lr: 0.096606, loss: 3.1453
2022-03-06 03:09:03 - train: epoch 0029, iter [03100, 05004], lr: 0.096606, loss: 2.9913
2022-03-06 03:09:35 - train: epoch 0029, iter [03200, 05004], lr: 0.096606, loss: 2.9384
2022-03-06 03:10:07 - train: epoch 0029, iter [03300, 05004], lr: 0.096606, loss: 2.9981
2022-03-06 03:10:40 - train: epoch 0029, iter [03400, 05004], lr: 0.096606, loss: 3.2180
2022-03-06 03:11:12 - train: epoch 0029, iter [03500, 05004], lr: 0.096606, loss: 3.2887
2022-03-06 03:11:45 - train: epoch 0029, iter [03600, 05004], lr: 0.096606, loss: 2.7474
2022-03-06 03:12:18 - train: epoch 0029, iter [03700, 05004], lr: 0.096606, loss: 3.0128
2022-03-06 03:12:50 - train: epoch 0029, iter [03800, 05004], lr: 0.096606, loss: 2.8558
2022-03-06 03:13:23 - train: epoch 0029, iter [03900, 05004], lr: 0.096606, loss: 3.0854
2022-03-06 03:13:57 - train: epoch 0029, iter [04000, 05004], lr: 0.096606, loss: 3.1470
2022-03-06 03:14:29 - train: epoch 0029, iter [04100, 05004], lr: 0.096606, loss: 2.8416
2022-03-06 03:15:03 - train: epoch 0029, iter [04200, 05004], lr: 0.096606, loss: 3.2225
2022-03-06 03:15:36 - train: epoch 0029, iter [04300, 05004], lr: 0.096606, loss: 2.9791
2022-03-06 03:16:10 - train: epoch 0029, iter [04400, 05004], lr: 0.096606, loss: 3.1296
2022-03-06 03:16:43 - train: epoch 0029, iter [04500, 05004], lr: 0.096606, loss: 3.1402
2022-03-06 03:17:15 - train: epoch 0029, iter [04600, 05004], lr: 0.096606, loss: 3.2795
2022-03-06 03:17:47 - train: epoch 0029, iter [04700, 05004], lr: 0.096606, loss: 2.8461
2022-03-06 03:18:20 - train: epoch 0029, iter [04800, 05004], lr: 0.096606, loss: 3.2004
2022-03-06 03:18:53 - train: epoch 0029, iter [04900, 05004], lr: 0.096606, loss: 3.5135
2022-03-06 03:19:25 - train: epoch 0029, iter [05000, 05004], lr: 0.096606, loss: 2.9519
2022-03-06 03:19:26 - train: epoch 029, train_loss: 3.0907
2022-03-06 03:20:37 - eval: epoch: 029, acc1: 52.370%, acc5: 77.832%, test_loss: 2.0466, per_image_load_time: 1.737ms, per_image_inference_time: 0.448ms
2022-03-06 03:20:38 - until epoch: 029, best_acc1: 52.370%
2022-03-06 03:20:38 - epoch 030 lr: 0.09630873244788883
2022-03-06 03:21:16 - train: epoch 0030, iter [00100, 05004], lr: 0.096309, loss: 3.1703
2022-03-06 03:21:49 - train: epoch 0030, iter [00200, 05004], lr: 0.096309, loss: 3.1322
2022-03-06 03:22:21 - train: epoch 0030, iter [00300, 05004], lr: 0.096309, loss: 3.0639
2022-03-06 03:22:53 - train: epoch 0030, iter [00400, 05004], lr: 0.096309, loss: 2.7310
2022-03-06 03:23:27 - train: epoch 0030, iter [00500, 05004], lr: 0.096309, loss: 3.3357
2022-03-06 03:24:00 - train: epoch 0030, iter [00600, 05004], lr: 0.096309, loss: 2.9561
2022-03-06 03:24:33 - train: epoch 0030, iter [00700, 05004], lr: 0.096309, loss: 3.1175
2022-03-06 03:25:05 - train: epoch 0030, iter [00800, 05004], lr: 0.096309, loss: 3.1886
2022-03-06 03:25:38 - train: epoch 0030, iter [00900, 05004], lr: 0.096309, loss: 3.2743
2022-03-06 03:26:12 - train: epoch 0030, iter [01000, 05004], lr: 0.096309, loss: 2.9220
2022-03-06 03:26:44 - train: epoch 0030, iter [01100, 05004], lr: 0.096309, loss: 2.9756
2022-03-06 03:27:17 - train: epoch 0030, iter [01200, 05004], lr: 0.096309, loss: 3.1576
2022-03-06 03:27:51 - train: epoch 0030, iter [01300, 05004], lr: 0.096309, loss: 2.9291
2022-03-06 03:28:23 - train: epoch 0030, iter [01400, 05004], lr: 0.096309, loss: 3.2344
2022-03-06 03:28:56 - train: epoch 0030, iter [01500, 05004], lr: 0.096309, loss: 2.7951
2022-03-06 03:29:29 - train: epoch 0030, iter [01600, 05004], lr: 0.096309, loss: 3.2458
2022-03-06 03:30:02 - train: epoch 0030, iter [01700, 05004], lr: 0.096309, loss: 3.2566
2022-03-06 03:30:34 - train: epoch 0030, iter [01800, 05004], lr: 0.096309, loss: 3.1226
2022-03-06 03:31:07 - train: epoch 0030, iter [01900, 05004], lr: 0.096309, loss: 3.0078
2022-03-06 03:31:39 - train: epoch 0030, iter [02000, 05004], lr: 0.096309, loss: 3.0158
2022-03-06 03:32:12 - train: epoch 0030, iter [02100, 05004], lr: 0.096309, loss: 3.1275
2022-03-06 03:32:44 - train: epoch 0030, iter [02200, 05004], lr: 0.096309, loss: 3.0525
2022-03-06 03:33:17 - train: epoch 0030, iter [02300, 05004], lr: 0.096309, loss: 2.9913
2022-03-06 03:33:51 - train: epoch 0030, iter [02400, 05004], lr: 0.096309, loss: 3.1230
2022-03-06 03:34:24 - train: epoch 0030, iter [02500, 05004], lr: 0.096309, loss: 3.1676
2022-03-06 03:34:58 - train: epoch 0030, iter [02600, 05004], lr: 0.096309, loss: 3.1940
2022-03-06 03:35:30 - train: epoch 0030, iter [02700, 05004], lr: 0.096309, loss: 2.9086
2022-03-06 03:36:03 - train: epoch 0030, iter [02800, 05004], lr: 0.096309, loss: 2.8622
2022-03-06 03:36:35 - train: epoch 0030, iter [02900, 05004], lr: 0.096309, loss: 2.9440
2022-03-06 03:37:09 - train: epoch 0030, iter [03000, 05004], lr: 0.096309, loss: 3.3741
2022-03-06 03:37:41 - train: epoch 0030, iter [03100, 05004], lr: 0.096309, loss: 2.9494
2022-03-06 03:38:14 - train: epoch 0030, iter [03200, 05004], lr: 0.096309, loss: 3.1493
2022-03-06 03:38:46 - train: epoch 0030, iter [03300, 05004], lr: 0.096309, loss: 3.1040
2022-03-06 03:39:19 - train: epoch 0030, iter [03400, 05004], lr: 0.096309, loss: 3.1670
2022-03-06 03:39:52 - train: epoch 0030, iter [03500, 05004], lr: 0.096309, loss: 3.2166
2022-03-06 03:40:23 - train: epoch 0030, iter [03600, 05004], lr: 0.096309, loss: 3.0548
2022-03-06 03:40:57 - train: epoch 0030, iter [03700, 05004], lr: 0.096309, loss: 3.3925
2022-03-06 03:41:30 - train: epoch 0030, iter [03800, 05004], lr: 0.096309, loss: 3.3113
2022-03-06 03:42:03 - train: epoch 0030, iter [03900, 05004], lr: 0.096309, loss: 3.0574
2022-03-06 03:42:36 - train: epoch 0030, iter [04000, 05004], lr: 0.096309, loss: 2.9755
2022-03-06 03:43:10 - train: epoch 0030, iter [04100, 05004], lr: 0.096309, loss: 2.9165
2022-03-06 03:43:43 - train: epoch 0030, iter [04200, 05004], lr: 0.096309, loss: 3.2382
2022-03-06 03:44:16 - train: epoch 0030, iter [04300, 05004], lr: 0.096309, loss: 2.9754
2022-03-06 03:44:49 - train: epoch 0030, iter [04400, 05004], lr: 0.096309, loss: 3.1989
2022-03-06 03:45:22 - train: epoch 0030, iter [04500, 05004], lr: 0.096309, loss: 3.2892
2022-03-06 03:45:54 - train: epoch 0030, iter [04600, 05004], lr: 0.096309, loss: 2.8586
2022-03-06 03:46:27 - train: epoch 0030, iter [04700, 05004], lr: 0.096309, loss: 3.0628
2022-03-06 03:47:00 - train: epoch 0030, iter [04800, 05004], lr: 0.096309, loss: 3.0606
2022-03-06 03:47:32 - train: epoch 0030, iter [04900, 05004], lr: 0.096309, loss: 3.2591
2022-03-06 03:48:03 - train: epoch 0030, iter [05000, 05004], lr: 0.096309, loss: 3.1975
2022-03-06 03:48:04 - train: epoch 030, train_loss: 3.0901
2022-03-06 03:49:15 - eval: epoch: 030, acc1: 50.312%, acc5: 76.480%, test_loss: 2.1418, per_image_load_time: 1.526ms, per_image_inference_time: 0.456ms
2022-03-06 03:49:16 - until epoch: 030, best_acc1: 52.370%
2022-03-06 03:49:16 - epoch 031 lr: 0.09599897218294122
2022-03-06 03:49:54 - train: epoch 0031, iter [00100, 05004], lr: 0.095999, loss: 3.0571
2022-03-06 03:50:28 - train: epoch 0031, iter [00200, 05004], lr: 0.095999, loss: 2.9478
2022-03-06 03:51:00 - train: epoch 0031, iter [00300, 05004], lr: 0.095999, loss: 2.9131
2022-03-06 03:51:35 - train: epoch 0031, iter [00400, 05004], lr: 0.095999, loss: 2.9742
2022-03-06 03:52:08 - train: epoch 0031, iter [00500, 05004], lr: 0.095999, loss: 3.0384
2022-03-06 03:52:41 - train: epoch 0031, iter [00600, 05004], lr: 0.095999, loss: 2.8550
2022-03-06 03:53:14 - train: epoch 0031, iter [00700, 05004], lr: 0.095999, loss: 2.9225
2022-03-06 03:53:46 - train: epoch 0031, iter [00800, 05004], lr: 0.095999, loss: 2.6566
2022-03-06 03:54:19 - train: epoch 0031, iter [00900, 05004], lr: 0.095999, loss: 3.1362
2022-03-06 03:54:52 - train: epoch 0031, iter [01000, 05004], lr: 0.095999, loss: 3.2713
2022-03-06 03:55:23 - train: epoch 0031, iter [01100, 05004], lr: 0.095999, loss: 3.3351
2022-03-06 03:55:56 - train: epoch 0031, iter [01200, 05004], lr: 0.095999, loss: 2.9701
2022-03-06 03:56:28 - train: epoch 0031, iter [01300, 05004], lr: 0.095999, loss: 2.7285
2022-03-06 03:57:01 - train: epoch 0031, iter [01400, 05004], lr: 0.095999, loss: 3.1885
2022-03-06 03:57:33 - train: epoch 0031, iter [01500, 05004], lr: 0.095999, loss: 3.5546
2022-03-06 03:58:06 - train: epoch 0031, iter [01600, 05004], lr: 0.095999, loss: 3.1324
2022-03-06 03:58:39 - train: epoch 0031, iter [01700, 05004], lr: 0.095999, loss: 3.0376
2022-03-06 03:59:13 - train: epoch 0031, iter [01800, 05004], lr: 0.095999, loss: 2.8881
2022-03-06 03:59:47 - train: epoch 0031, iter [01900, 05004], lr: 0.095999, loss: 3.1179
2022-03-06 04:00:19 - train: epoch 0031, iter [02000, 05004], lr: 0.095999, loss: 3.1859
2022-03-06 04:00:52 - train: epoch 0031, iter [02100, 05004], lr: 0.095999, loss: 2.9721
2022-03-06 04:01:26 - train: epoch 0031, iter [02200, 05004], lr: 0.095999, loss: 2.9426
2022-03-06 04:01:59 - train: epoch 0031, iter [02300, 05004], lr: 0.095999, loss: 3.0681
2022-03-06 04:02:31 - train: epoch 0031, iter [02400, 05004], lr: 0.095999, loss: 3.1436
2022-03-06 04:03:03 - train: epoch 0031, iter [02500, 05004], lr: 0.095999, loss: 2.7183
2022-03-06 04:03:35 - train: epoch 0031, iter [02600, 05004], lr: 0.095999, loss: 3.1650
2022-03-06 04:04:07 - train: epoch 0031, iter [02700, 05004], lr: 0.095999, loss: 3.0544
2022-03-06 04:04:39 - train: epoch 0031, iter [02800, 05004], lr: 0.095999, loss: 3.4205
2022-03-06 04:05:12 - train: epoch 0031, iter [02900, 05004], lr: 0.095999, loss: 3.0926
2022-03-06 04:05:43 - train: epoch 0031, iter [03000, 05004], lr: 0.095999, loss: 3.1780
2022-03-06 04:06:16 - train: epoch 0031, iter [03100, 05004], lr: 0.095999, loss: 2.8704
2022-03-06 04:06:48 - train: epoch 0031, iter [03200, 05004], lr: 0.095999, loss: 3.2663
2022-03-06 04:07:21 - train: epoch 0031, iter [03300, 05004], lr: 0.095999, loss: 3.1043
2022-03-06 04:07:55 - train: epoch 0031, iter [03400, 05004], lr: 0.095999, loss: 3.0952
2022-03-06 04:08:28 - train: epoch 0031, iter [03500, 05004], lr: 0.095999, loss: 3.2518
2022-03-06 04:09:01 - train: epoch 0031, iter [03600, 05004], lr: 0.095999, loss: 3.1573
2022-03-06 04:09:34 - train: epoch 0031, iter [03700, 05004], lr: 0.095999, loss: 3.0382
2022-03-06 04:10:08 - train: epoch 0031, iter [03800, 05004], lr: 0.095999, loss: 2.8353
2022-03-06 04:10:40 - train: epoch 0031, iter [03900, 05004], lr: 0.095999, loss: 3.0069
2022-03-06 04:11:12 - train: epoch 0031, iter [04000, 05004], lr: 0.095999, loss: 3.1535
2022-03-06 04:11:44 - train: epoch 0031, iter [04100, 05004], lr: 0.095999, loss: 2.8865
2022-03-06 04:12:16 - train: epoch 0031, iter [04200, 05004], lr: 0.095999, loss: 3.1503
2022-03-06 04:12:48 - train: epoch 0031, iter [04300, 05004], lr: 0.095999, loss: 3.0236
2022-03-06 04:13:21 - train: epoch 0031, iter [04400, 05004], lr: 0.095999, loss: 3.2366
2022-03-06 04:13:53 - train: epoch 0031, iter [04500, 05004], lr: 0.095999, loss: 3.3628
2022-03-06 04:14:26 - train: epoch 0031, iter [04600, 05004], lr: 0.095999, loss: 3.3263
2022-03-06 04:14:58 - train: epoch 0031, iter [04700, 05004], lr: 0.095999, loss: 3.0613
2022-03-06 04:15:30 - train: epoch 0031, iter [04800, 05004], lr: 0.095999, loss: 3.0678
2022-03-06 04:16:04 - train: epoch 0031, iter [04900, 05004], lr: 0.095999, loss: 2.8594
2022-03-06 04:16:36 - train: epoch 0031, iter [05000, 05004], lr: 0.095999, loss: 2.7803
2022-03-06 04:16:37 - train: epoch 031, train_loss: 3.0840
2022-03-06 04:17:50 - eval: epoch: 031, acc1: 51.030%, acc5: 77.352%, test_loss: 2.0891, per_image_load_time: 2.010ms, per_image_inference_time: 0.481ms
2022-03-06 04:17:51 - until epoch: 031, best_acc1: 52.370%
2022-03-06 04:17:51 - epoch 032 lr: 0.09567727288213004
2022-03-06 04:18:29 - train: epoch 0032, iter [00100, 05004], lr: 0.095677, loss: 2.9140
2022-03-06 04:19:02 - train: epoch 0032, iter [00200, 05004], lr: 0.095677, loss: 3.0783
2022-03-06 04:19:34 - train: epoch 0032, iter [00300, 05004], lr: 0.095677, loss: 3.2227
2022-03-06 04:20:07 - train: epoch 0032, iter [00400, 05004], lr: 0.095677, loss: 3.1297
2022-03-06 04:20:39 - train: epoch 0032, iter [00500, 05004], lr: 0.095677, loss: 3.2715
2022-03-06 04:21:11 - train: epoch 0032, iter [00600, 05004], lr: 0.095677, loss: 3.2843
2022-03-06 04:21:43 - train: epoch 0032, iter [00700, 05004], lr: 0.095677, loss: 3.0678
2022-03-06 04:22:16 - train: epoch 0032, iter [00800, 05004], lr: 0.095677, loss: 2.9618
2022-03-06 04:22:48 - train: epoch 0032, iter [00900, 05004], lr: 0.095677, loss: 3.0548
2022-03-06 04:23:21 - train: epoch 0032, iter [01000, 05004], lr: 0.095677, loss: 3.0812
2022-03-06 04:23:54 - train: epoch 0032, iter [01100, 05004], lr: 0.095677, loss: 3.2445
2022-03-06 04:24:27 - train: epoch 0032, iter [01200, 05004], lr: 0.095677, loss: 3.2011
2022-03-06 04:24:59 - train: epoch 0032, iter [01300, 05004], lr: 0.095677, loss: 3.1019
2022-03-06 04:25:34 - train: epoch 0032, iter [01400, 05004], lr: 0.095677, loss: 3.1616
2022-03-06 04:26:07 - train: epoch 0032, iter [01500, 05004], lr: 0.095677, loss: 3.2574
2022-03-06 04:26:39 - train: epoch 0032, iter [01600, 05004], lr: 0.095677, loss: 3.2505
2022-03-06 04:27:13 - train: epoch 0032, iter [01700, 05004], lr: 0.095677, loss: 3.1970
2022-03-06 04:27:46 - train: epoch 0032, iter [01800, 05004], lr: 0.095677, loss: 3.3940
2022-03-06 04:28:18 - train: epoch 0032, iter [01900, 05004], lr: 0.095677, loss: 3.0353
2022-03-06 04:28:50 - train: epoch 0032, iter [02000, 05004], lr: 0.095677, loss: 3.1618
2022-03-06 04:29:23 - train: epoch 0032, iter [02100, 05004], lr: 0.095677, loss: 3.1086
2022-03-06 04:29:55 - train: epoch 0032, iter [02200, 05004], lr: 0.095677, loss: 2.9809
2022-03-06 04:30:27 - train: epoch 0032, iter [02300, 05004], lr: 0.095677, loss: 3.2188
2022-03-06 04:31:00 - train: epoch 0032, iter [02400, 05004], lr: 0.095677, loss: 3.1657
2022-03-06 04:31:32 - train: epoch 0032, iter [02500, 05004], lr: 0.095677, loss: 3.1956
2022-03-06 04:32:04 - train: epoch 0032, iter [02600, 05004], lr: 0.095677, loss: 2.9272
2022-03-06 04:32:36 - train: epoch 0032, iter [02700, 05004], lr: 0.095677, loss: 3.0577
2022-03-06 04:33:09 - train: epoch 0032, iter [02800, 05004], lr: 0.095677, loss: 3.0382
2022-03-06 04:33:42 - train: epoch 0032, iter [02900, 05004], lr: 0.095677, loss: 2.8620
2022-03-06 04:34:15 - train: epoch 0032, iter [03000, 05004], lr: 0.095677, loss: 3.0969
2022-03-06 04:34:47 - train: epoch 0032, iter [03100, 05004], lr: 0.095677, loss: 3.3207
2022-03-06 04:35:20 - train: epoch 0032, iter [03200, 05004], lr: 0.095677, loss: 3.3592
2022-03-06 04:35:53 - train: epoch 0032, iter [03300, 05004], lr: 0.095677, loss: 2.9346
2022-03-06 04:36:27 - train: epoch 0032, iter [03400, 05004], lr: 0.095677, loss: 2.9120
2022-03-06 04:36:59 - train: epoch 0032, iter [03500, 05004], lr: 0.095677, loss: 3.1672
2022-03-06 04:37:31 - train: epoch 0032, iter [03600, 05004], lr: 0.095677, loss: 3.3945
2022-03-06 04:38:03 - train: epoch 0032, iter [03700, 05004], lr: 0.095677, loss: 3.2343
2022-03-06 04:38:36 - train: epoch 0032, iter [03800, 05004], lr: 0.095677, loss: 3.0947
2022-03-06 04:39:08 - train: epoch 0032, iter [03900, 05004], lr: 0.095677, loss: 3.1877
2022-03-06 04:39:40 - train: epoch 0032, iter [04000, 05004], lr: 0.095677, loss: 2.8235
2022-03-06 04:40:13 - train: epoch 0032, iter [04100, 05004], lr: 0.095677, loss: 3.2354
2022-03-06 04:40:45 - train: epoch 0032, iter [04200, 05004], lr: 0.095677, loss: 2.9810
2022-03-06 04:41:17 - train: epoch 0032, iter [04300, 05004], lr: 0.095677, loss: 3.1810
2022-03-06 04:41:50 - train: epoch 0032, iter [04400, 05004], lr: 0.095677, loss: 2.9419
2022-03-06 04:42:22 - train: epoch 0032, iter [04500, 05004], lr: 0.095677, loss: 3.0556
2022-03-06 04:42:55 - train: epoch 0032, iter [04600, 05004], lr: 0.095677, loss: 3.0915
2022-03-06 04:43:29 - train: epoch 0032, iter [04700, 05004], lr: 0.095677, loss: 3.2999
2022-03-06 04:44:03 - train: epoch 0032, iter [04800, 05004], lr: 0.095677, loss: 3.1329
2022-03-06 04:44:36 - train: epoch 0032, iter [04900, 05004], lr: 0.095677, loss: 3.2702
2022-03-06 04:45:09 - train: epoch 0032, iter [05000, 05004], lr: 0.095677, loss: 3.1778
2022-03-06 04:45:10 - train: epoch 032, train_loss: 3.0801
2022-03-06 04:46:21 - eval: epoch: 032, acc1: 52.698%, acc5: 78.440%, test_loss: 2.0258, per_image_load_time: 1.091ms, per_image_inference_time: 0.474ms
2022-03-06 04:46:21 - until epoch: 032, best_acc1: 52.698%
2022-03-06 04:46:21 - epoch 033 lr: 0.09534371804252728
2022-03-06 04:46:59 - train: epoch 0033, iter [00100, 05004], lr: 0.095344, loss: 2.8341
2022-03-06 04:47:32 - train: epoch 0033, iter [00200, 05004], lr: 0.095344, loss: 3.0527
2022-03-06 04:48:04 - train: epoch 0033, iter [00300, 05004], lr: 0.095344, loss: 2.7860
2022-03-06 04:48:36 - train: epoch 0033, iter [00400, 05004], lr: 0.095344, loss: 2.8299
2022-03-06 04:49:08 - train: epoch 0033, iter [00500, 05004], lr: 0.095344, loss: 2.9860
2022-03-06 04:49:41 - train: epoch 0033, iter [00600, 05004], lr: 0.095344, loss: 2.9258
2022-03-06 04:50:12 - train: epoch 0033, iter [00700, 05004], lr: 0.095344, loss: 3.1778
2022-03-06 04:50:45 - train: epoch 0033, iter [00800, 05004], lr: 0.095344, loss: 3.2563
2022-03-06 04:51:19 - train: epoch 0033, iter [00900, 05004], lr: 0.095344, loss: 3.0958
2022-03-06 04:51:53 - train: epoch 0033, iter [01000, 05004], lr: 0.095344, loss: 3.1527
2022-03-06 04:52:25 - train: epoch 0033, iter [01100, 05004], lr: 0.095344, loss: 2.7630
2022-03-06 04:53:00 - train: epoch 0033, iter [01200, 05004], lr: 0.095344, loss: 3.1453
2022-03-06 04:53:33 - train: epoch 0033, iter [01300, 05004], lr: 0.095344, loss: 3.1276
2022-03-06 04:54:06 - train: epoch 0033, iter [01400, 05004], lr: 0.095344, loss: 3.0901
2022-03-06 04:54:39 - train: epoch 0033, iter [01500, 05004], lr: 0.095344, loss: 3.2610
2022-03-06 04:55:11 - train: epoch 0033, iter [01600, 05004], lr: 0.095344, loss: 3.2515
2022-03-06 04:55:43 - train: epoch 0033, iter [01700, 05004], lr: 0.095344, loss: 3.1356
2022-03-06 04:56:17 - train: epoch 0033, iter [01800, 05004], lr: 0.095344, loss: 3.0033
2022-03-06 04:56:48 - train: epoch 0033, iter [01900, 05004], lr: 0.095344, loss: 3.3445
2022-03-06 04:57:20 - train: epoch 0033, iter [02000, 05004], lr: 0.095344, loss: 2.9433
2022-03-06 04:57:52 - train: epoch 0033, iter [02100, 05004], lr: 0.095344, loss: 3.0575
2022-03-06 04:58:25 - train: epoch 0033, iter [02200, 05004], lr: 0.095344, loss: 3.1239
2022-03-06 04:58:58 - train: epoch 0033, iter [02300, 05004], lr: 0.095344, loss: 2.8967
2022-03-06 04:59:29 - train: epoch 0033, iter [02400, 05004], lr: 0.095344, loss: 3.2796
2022-03-06 05:00:03 - train: epoch 0033, iter [02500, 05004], lr: 0.095344, loss: 3.0347
2022-03-06 05:00:36 - train: epoch 0033, iter [02600, 05004], lr: 0.095344, loss: 3.1643
2022-03-06 05:01:10 - train: epoch 0033, iter [02700, 05004], lr: 0.095344, loss: 3.1681
2022-03-06 05:01:43 - train: epoch 0033, iter [02800, 05004], lr: 0.095344, loss: 2.9765
2022-03-06 05:02:17 - train: epoch 0033, iter [02900, 05004], lr: 0.095344, loss: 3.4322
2022-03-06 05:02:50 - train: epoch 0033, iter [03000, 05004], lr: 0.095344, loss: 2.8041
2022-03-06 05:03:22 - train: epoch 0033, iter [03100, 05004], lr: 0.095344, loss: 3.1841
2022-03-06 05:03:55 - train: epoch 0033, iter [03200, 05004], lr: 0.095344, loss: 2.9868
2022-03-06 05:04:27 - train: epoch 0033, iter [03300, 05004], lr: 0.095344, loss: 2.8889
2022-03-06 05:04:59 - train: epoch 0033, iter [03400, 05004], lr: 0.095344, loss: 2.9826
2022-03-06 05:05:31 - train: epoch 0033, iter [03500, 05004], lr: 0.095344, loss: 3.0391
2022-03-06 05:06:03 - train: epoch 0033, iter [03600, 05004], lr: 0.095344, loss: 3.2511
2022-03-06 05:06:35 - train: epoch 0033, iter [03700, 05004], lr: 0.095344, loss: 3.0564
2022-03-06 05:07:08 - train: epoch 0033, iter [03800, 05004], lr: 0.095344, loss: 3.2133
2022-03-06 05:07:40 - train: epoch 0033, iter [03900, 05004], lr: 0.095344, loss: 3.1343
2022-03-06 05:08:13 - train: epoch 0033, iter [04000, 05004], lr: 0.095344, loss: 3.2454
2022-03-06 05:08:46 - train: epoch 0033, iter [04100, 05004], lr: 0.095344, loss: 3.0455
2022-03-06 05:09:20 - train: epoch 0033, iter [04200, 05004], lr: 0.095344, loss: 3.1027
2022-03-06 05:09:53 - train: epoch 0033, iter [04300, 05004], lr: 0.095344, loss: 3.1607
2022-03-06 05:10:26 - train: epoch 0033, iter [04400, 05004], lr: 0.095344, loss: 3.4146
2022-03-06 05:11:01 - train: epoch 0033, iter [04500, 05004], lr: 0.095344, loss: 3.4724
2022-03-06 05:11:34 - train: epoch 0033, iter [04600, 05004], lr: 0.095344, loss: 3.0822
2022-03-06 05:12:07 - train: epoch 0033, iter [04700, 05004], lr: 0.095344, loss: 3.0549
2022-03-06 05:12:38 - train: epoch 0033, iter [04800, 05004], lr: 0.095344, loss: 3.4399
2022-03-06 05:13:11 - train: epoch 0033, iter [04900, 05004], lr: 0.095344, loss: 2.7937
2022-03-06 05:13:42 - train: epoch 0033, iter [05000, 05004], lr: 0.095344, loss: 3.1346
2022-03-06 05:13:43 - train: epoch 033, train_loss: 3.0739
2022-03-06 05:14:54 - eval: epoch: 033, acc1: 49.858%, acc5: 76.094%, test_loss: 2.1619, per_image_load_time: 2.278ms, per_image_inference_time: 0.460ms
2022-03-06 05:14:55 - until epoch: 033, best_acc1: 52.698%
2022-03-06 05:14:55 - epoch 034 lr: 0.09499839423831061
2022-03-06 05:15:33 - train: epoch 0034, iter [00100, 05004], lr: 0.094998, loss: 3.0373
2022-03-06 05:16:05 - train: epoch 0034, iter [00200, 05004], lr: 0.094998, loss: 2.9553
2022-03-06 05:16:37 - train: epoch 0034, iter [00300, 05004], lr: 0.094998, loss: 3.0489
2022-03-06 05:17:10 - train: epoch 0034, iter [00400, 05004], lr: 0.094998, loss: 3.0123
2022-03-06 05:17:43 - train: epoch 0034, iter [00500, 05004], lr: 0.094998, loss: 2.9096
2022-03-06 05:18:16 - train: epoch 0034, iter [00600, 05004], lr: 0.094998, loss: 3.2827
2022-03-06 05:18:50 - train: epoch 0034, iter [00700, 05004], lr: 0.094998, loss: 2.9019
2022-03-06 05:19:24 - train: epoch 0034, iter [00800, 05004], lr: 0.094998, loss: 2.8767
2022-03-06 05:19:57 - train: epoch 0034, iter [00900, 05004], lr: 0.094998, loss: 3.0443
2022-03-06 05:20:29 - train: epoch 0034, iter [01000, 05004], lr: 0.094998, loss: 3.0266
2022-03-06 05:21:01 - train: epoch 0034, iter [01100, 05004], lr: 0.094998, loss: 2.7920
2022-03-06 05:21:33 - train: epoch 0034, iter [01200, 05004], lr: 0.094998, loss: 3.1509
2022-03-06 05:22:06 - train: epoch 0034, iter [01300, 05004], lr: 0.094998, loss: 2.9960
2022-03-06 05:22:38 - train: epoch 0034, iter [01400, 05004], lr: 0.094998, loss: 3.0326
2022-03-06 05:23:11 - train: epoch 0034, iter [01500, 05004], lr: 0.094998, loss: 2.9913
2022-03-06 05:23:43 - train: epoch 0034, iter [01600, 05004], lr: 0.094998, loss: 3.0665
2022-03-06 05:24:16 - train: epoch 0034, iter [01700, 05004], lr: 0.094998, loss: 3.0133
2022-03-06 05:24:48 - train: epoch 0034, iter [01800, 05004], lr: 0.094998, loss: 3.2944
2022-03-06 05:25:21 - train: epoch 0034, iter [01900, 05004], lr: 0.094998, loss: 3.1168
2022-03-06 05:25:54 - train: epoch 0034, iter [02000, 05004], lr: 0.094998, loss: 3.0156
2022-03-06 05:26:26 - train: epoch 0034, iter [02100, 05004], lr: 0.094998, loss: 3.0088
2022-03-06 05:27:00 - train: epoch 0034, iter [02200, 05004], lr: 0.094998, loss: 3.0322
2022-03-06 05:27:34 - train: epoch 0034, iter [02300, 05004], lr: 0.094998, loss: 3.0616
2022-03-06 05:28:06 - train: epoch 0034, iter [02400, 05004], lr: 0.094998, loss: 3.0746
2022-03-06 05:28:39 - train: epoch 0034, iter [02500, 05004], lr: 0.094998, loss: 3.0684
2022-03-06 05:29:12 - train: epoch 0034, iter [02600, 05004], lr: 0.094998, loss: 3.1579
2022-03-06 05:29:45 - train: epoch 0034, iter [02700, 05004], lr: 0.094998, loss: 3.0476
2022-03-06 05:30:18 - train: epoch 0034, iter [02800, 05004], lr: 0.094998, loss: 2.8257
2022-03-06 05:30:50 - train: epoch 0034, iter [02900, 05004], lr: 0.094998, loss: 2.7494
2022-03-06 05:31:22 - train: epoch 0034, iter [03000, 05004], lr: 0.094998, loss: 3.0403
2022-03-06 05:31:54 - train: epoch 0034, iter [03100, 05004], lr: 0.094998, loss: 2.9466
2022-03-06 05:32:27 - train: epoch 0034, iter [03200, 05004], lr: 0.094998, loss: 2.9687
2022-03-06 05:33:00 - train: epoch 0034, iter [03300, 05004], lr: 0.094998, loss: 2.9179
2022-03-06 05:33:33 - train: epoch 0034, iter [03400, 05004], lr: 0.094998, loss: 3.0917
2022-03-06 05:34:05 - train: epoch 0034, iter [03500, 05004], lr: 0.094998, loss: 2.9884
2022-03-06 05:34:38 - train: epoch 0034, iter [03600, 05004], lr: 0.094998, loss: 2.6886
2022-03-06 05:35:11 - train: epoch 0034, iter [03700, 05004], lr: 0.094998, loss: 2.8884
2022-03-06 05:35:43 - train: epoch 0034, iter [03800, 05004], lr: 0.094998, loss: 3.0574
2022-03-06 05:36:15 - train: epoch 0034, iter [03900, 05004], lr: 0.094998, loss: 3.1728
2022-03-06 05:36:49 - train: epoch 0034, iter [04000, 05004], lr: 0.094998, loss: 2.9796
2022-03-06 05:37:22 - train: epoch 0034, iter [04100, 05004], lr: 0.094998, loss: 3.0839
2022-03-06 05:37:56 - train: epoch 0034, iter [04200, 05004], lr: 0.094998, loss: 3.0598
2022-03-06 05:38:30 - train: epoch 0034, iter [04300, 05004], lr: 0.094998, loss: 2.9991
2022-03-06 05:39:02 - train: epoch 0034, iter [04400, 05004], lr: 0.094998, loss: 3.1507
2022-03-06 05:39:35 - train: epoch 0034, iter [04500, 05004], lr: 0.094998, loss: 2.9170
2022-03-06 05:40:07 - train: epoch 0034, iter [04600, 05004], lr: 0.094998, loss: 3.3875
2022-03-06 05:40:39 - train: epoch 0034, iter [04700, 05004], lr: 0.094998, loss: 3.1467
2022-03-06 05:41:12 - train: epoch 0034, iter [04800, 05004], lr: 0.094998, loss: 3.2121
2022-03-06 05:41:44 - train: epoch 0034, iter [04900, 05004], lr: 0.094998, loss: 3.0741
2022-03-06 05:42:15 - train: epoch 0034, iter [05000, 05004], lr: 0.094998, loss: 3.0969
2022-03-06 05:42:16 - train: epoch 034, train_loss: 3.0718
2022-03-06 05:43:27 - eval: epoch: 034, acc1: 49.208%, acc5: 75.492%, test_loss: 2.2072, per_image_load_time: 2.303ms, per_image_inference_time: 0.457ms
2022-03-06 05:43:28 - until epoch: 034, best_acc1: 52.698%
2022-03-06 05:43:28 - epoch 035 lr: 0.09464139109829321
2022-03-06 05:44:05 - train: epoch 0035, iter [00100, 05004], lr: 0.094641, loss: 2.6885
2022-03-06 05:44:39 - train: epoch 0035, iter [00200, 05004], lr: 0.094641, loss: 2.8606
2022-03-06 05:45:11 - train: epoch 0035, iter [00300, 05004], lr: 0.094641, loss: 3.1735
2022-03-06 05:45:45 - train: epoch 0035, iter [00400, 05004], lr: 0.094641, loss: 2.9558
2022-03-06 05:46:18 - train: epoch 0035, iter [00500, 05004], lr: 0.094641, loss: 2.8905
2022-03-06 05:46:51 - train: epoch 0035, iter [00600, 05004], lr: 0.094641, loss: 3.1021
2022-03-06 05:47:24 - train: epoch 0035, iter [00700, 05004], lr: 0.094641, loss: 3.0038
2022-03-06 05:47:56 - train: epoch 0035, iter [00800, 05004], lr: 0.094641, loss: 2.9154
2022-03-06 05:48:29 - train: epoch 0035, iter [00900, 05004], lr: 0.094641, loss: 3.3817
2022-03-06 05:49:01 - train: epoch 0035, iter [01000, 05004], lr: 0.094641, loss: 2.9736
2022-03-06 05:49:34 - train: epoch 0035, iter [01100, 05004], lr: 0.094641, loss: 3.3790
2022-03-06 05:50:07 - train: epoch 0035, iter [01200, 05004], lr: 0.094641, loss: 3.0134
2022-03-06 05:50:39 - train: epoch 0035, iter [01300, 05004], lr: 0.094641, loss: 3.2750
2022-03-06 05:51:12 - train: epoch 0035, iter [01400, 05004], lr: 0.094641, loss: 3.3633
2022-03-06 05:51:43 - train: epoch 0035, iter [01500, 05004], lr: 0.094641, loss: 3.0841
2022-03-06 05:52:16 - train: epoch 0035, iter [01600, 05004], lr: 0.094641, loss: 2.9403
2022-03-06 05:52:48 - train: epoch 0035, iter [01700, 05004], lr: 0.094641, loss: 2.8371
2022-03-06 05:53:22 - train: epoch 0035, iter [01800, 05004], lr: 0.094641, loss: 3.0531
2022-03-06 05:53:55 - train: epoch 0035, iter [01900, 05004], lr: 0.094641, loss: 2.9945
2022-03-06 05:54:28 - train: epoch 0035, iter [02000, 05004], lr: 0.094641, loss: 3.0176
2022-03-06 05:55:03 - train: epoch 0035, iter [02100, 05004], lr: 0.094641, loss: 3.1177
2022-03-06 05:55:36 - train: epoch 0035, iter [02200, 05004], lr: 0.094641, loss: 3.2042
2022-03-06 05:56:09 - train: epoch 0035, iter [02300, 05004], lr: 0.094641, loss: 2.8540
2022-03-06 05:56:41 - train: epoch 0035, iter [02400, 05004], lr: 0.094641, loss: 3.1546
2022-03-06 05:57:14 - train: epoch 0035, iter [02500, 05004], lr: 0.094641, loss: 3.1324
2022-03-06 05:57:47 - train: epoch 0035, iter [02600, 05004], lr: 0.094641, loss: 3.3428
2022-03-06 05:58:19 - train: epoch 0035, iter [02700, 05004], lr: 0.094641, loss: 3.0538
2022-03-06 05:58:52 - train: epoch 0035, iter [02800, 05004], lr: 0.094641, loss: 3.1529
2022-03-06 05:59:23 - train: epoch 0035, iter [02900, 05004], lr: 0.094641, loss: 2.9487
2022-03-06 05:59:56 - train: epoch 0035, iter [03000, 05004], lr: 0.094641, loss: 3.0024
2022-03-06 06:00:28 - train: epoch 0035, iter [03100, 05004], lr: 0.094641, loss: 3.1757
2022-03-06 06:01:00 - train: epoch 0035, iter [03200, 05004], lr: 0.094641, loss: 2.8457
2022-03-06 06:01:32 - train: epoch 0035, iter [03300, 05004], lr: 0.094641, loss: 2.8125
2022-03-06 06:02:05 - train: epoch 0035, iter [03400, 05004], lr: 0.094641, loss: 3.0849
2022-03-06 06:02:39 - train: epoch 0035, iter [03500, 05004], lr: 0.094641, loss: 2.8230
2022-03-06 06:03:13 - train: epoch 0035, iter [03600, 05004], lr: 0.094641, loss: 3.3019
2022-03-06 06:03:45 - train: epoch 0035, iter [03700, 05004], lr: 0.094641, loss: 2.8484
2022-03-06 06:04:19 - train: epoch 0035, iter [03800, 05004], lr: 0.094641, loss: 3.0090
2022-03-06 06:04:52 - train: epoch 0035, iter [03900, 05004], lr: 0.094641, loss: 3.2223
2022-03-06 06:05:25 - train: epoch 0035, iter [04000, 05004], lr: 0.094641, loss: 2.8363
2022-03-06 06:05:58 - train: epoch 0035, iter [04100, 05004], lr: 0.094641, loss: 3.4667
2022-03-06 06:06:30 - train: epoch 0035, iter [04200, 05004], lr: 0.094641, loss: 3.0420
2022-03-06 06:07:01 - train: epoch 0035, iter [04300, 05004], lr: 0.094641, loss: 3.0614
2022-03-06 06:07:33 - train: epoch 0035, iter [04400, 05004], lr: 0.094641, loss: 3.1211
2022-03-06 06:08:05 - train: epoch 0035, iter [04500, 05004], lr: 0.094641, loss: 3.0798
2022-03-06 06:08:38 - train: epoch 0035, iter [04600, 05004], lr: 0.094641, loss: 2.9728
2022-03-06 06:09:10 - train: epoch 0035, iter [04700, 05004], lr: 0.094641, loss: 3.4843
2022-03-06 06:09:43 - train: epoch 0035, iter [04800, 05004], lr: 0.094641, loss: 3.2400
2022-03-06 06:10:14 - train: epoch 0035, iter [04900, 05004], lr: 0.094641, loss: 3.0424
2022-03-06 06:10:45 - train: epoch 0035, iter [05000, 05004], lr: 0.094641, loss: 2.9177
2022-03-06 06:10:47 - train: epoch 035, train_loss: 3.0624
2022-03-06 06:12:00 - eval: epoch: 035, acc1: 51.194%, acc5: 77.026%, test_loss: 2.1101, per_image_load_time: 1.416ms, per_image_inference_time: 0.510ms
2022-03-06 06:12:01 - until epoch: 035, best_acc1: 52.698%
2022-03-06 06:12:01 - epoch 036 lr: 0.0942728012826605
2022-03-06 06:12:39 - train: epoch 0036, iter [00100, 05004], lr: 0.094273, loss: 3.0995
2022-03-06 06:13:12 - train: epoch 0036, iter [00200, 05004], lr: 0.094273, loss: 2.7204
2022-03-06 06:13:47 - train: epoch 0036, iter [00300, 05004], lr: 0.094273, loss: 3.0057
2022-03-06 06:14:19 - train: epoch 0036, iter [00400, 05004], lr: 0.094273, loss: 3.1879
2022-03-06 06:14:50 - train: epoch 0036, iter [00500, 05004], lr: 0.094273, loss: 3.0235
2022-03-06 06:15:23 - train: epoch 0036, iter [00600, 05004], lr: 0.094273, loss: 2.9173
2022-03-06 06:15:55 - train: epoch 0036, iter [00700, 05004], lr: 0.094273, loss: 2.8725
2022-03-06 06:16:28 - train: epoch 0036, iter [00800, 05004], lr: 0.094273, loss: 3.1680
2022-03-06 06:17:00 - train: epoch 0036, iter [00900, 05004], lr: 0.094273, loss: 3.1856
2022-03-06 06:17:33 - train: epoch 0036, iter [01000, 05004], lr: 0.094273, loss: 3.2090
2022-03-06 06:18:05 - train: epoch 0036, iter [01100, 05004], lr: 0.094273, loss: 3.1912
2022-03-06 06:18:38 - train: epoch 0036, iter [01200, 05004], lr: 0.094273, loss: 3.1373
2022-03-06 06:19:10 - train: epoch 0036, iter [01300, 05004], lr: 0.094273, loss: 2.9937
2022-03-06 06:19:42 - train: epoch 0036, iter [01400, 05004], lr: 0.094273, loss: 3.2328
2022-03-06 06:20:16 - train: epoch 0036, iter [01500, 05004], lr: 0.094273, loss: 3.1653
2022-03-06 06:20:50 - train: epoch 0036, iter [01600, 05004], lr: 0.094273, loss: 3.0805
2022-03-06 06:21:23 - train: epoch 0036, iter [01700, 05004], lr: 0.094273, loss: 2.9515
2022-03-06 06:21:57 - train: epoch 0036, iter [01800, 05004], lr: 0.094273, loss: 2.9892
2022-03-06 06:22:28 - train: epoch 0036, iter [01900, 05004], lr: 0.094273, loss: 3.2093
2022-03-06 06:23:01 - train: epoch 0036, iter [02000, 05004], lr: 0.094273, loss: 2.9049
2022-03-06 06:23:34 - train: epoch 0036, iter [02100, 05004], lr: 0.094273, loss: 3.0360
2022-03-06 06:24:06 - train: epoch 0036, iter [02200, 05004], lr: 0.094273, loss: 3.0308
2022-03-06 06:24:38 - train: epoch 0036, iter [02300, 05004], lr: 0.094273, loss: 3.0054
2022-03-06 06:25:10 - train: epoch 0036, iter [02400, 05004], lr: 0.094273, loss: 3.2124
2022-03-06 06:25:44 - train: epoch 0036, iter [02500, 05004], lr: 0.094273, loss: 2.9819
2022-03-06 06:26:16 - train: epoch 0036, iter [02600, 05004], lr: 0.094273, loss: 3.1845
2022-03-06 06:26:47 - train: epoch 0036, iter [02700, 05004], lr: 0.094273, loss: 3.0050
2022-03-06 06:27:20 - train: epoch 0036, iter [02800, 05004], lr: 0.094273, loss: 2.8732
2022-03-06 06:27:52 - train: epoch 0036, iter [02900, 05004], lr: 0.094273, loss: 2.7904
2022-03-06 06:28:24 - train: epoch 0036, iter [03000, 05004], lr: 0.094273, loss: 3.1433
2022-03-06 06:28:57 - train: epoch 0036, iter [03100, 05004], lr: 0.094273, loss: 3.2516
2022-03-06 06:29:30 - train: epoch 0036, iter [03200, 05004], lr: 0.094273, loss: 2.8490
2022-03-06 06:30:03 - train: epoch 0036, iter [03300, 05004], lr: 0.094273, loss: 2.9222
2022-03-06 06:30:36 - train: epoch 0036, iter [03400, 05004], lr: 0.094273, loss: 2.7612
2022-03-06 06:31:10 - train: epoch 0036, iter [03500, 05004], lr: 0.094273, loss: 3.1611
2022-03-06 06:31:43 - train: epoch 0036, iter [03600, 05004], lr: 0.094273, loss: 3.2346
2022-03-06 06:32:15 - train: epoch 0036, iter [03700, 05004], lr: 0.094273, loss: 2.9759
2022-03-06 06:32:47 - train: epoch 0036, iter [03800, 05004], lr: 0.094273, loss: 2.9327
2022-03-06 06:33:20 - train: epoch 0036, iter [03900, 05004], lr: 0.094273, loss: 3.1417
2022-03-06 06:33:53 - train: epoch 0036, iter [04000, 05004], lr: 0.094273, loss: 3.3325
2022-03-06 06:34:24 - train: epoch 0036, iter [04100, 05004], lr: 0.094273, loss: 2.8913
2022-03-06 06:34:58 - train: epoch 0036, iter [04200, 05004], lr: 0.094273, loss: 3.0520
2022-03-06 06:35:30 - train: epoch 0036, iter [04300, 05004], lr: 0.094273, loss: 2.9539
2022-03-06 06:36:02 - train: epoch 0036, iter [04400, 05004], lr: 0.094273, loss: 3.0441
2022-03-06 06:36:35 - train: epoch 0036, iter [04500, 05004], lr: 0.094273, loss: 2.9277
2022-03-06 06:37:07 - train: epoch 0036, iter [04600, 05004], lr: 0.094273, loss: 3.1330
2022-03-06 06:37:39 - train: epoch 0036, iter [04700, 05004], lr: 0.094273, loss: 2.9491
2022-03-06 06:38:12 - train: epoch 0036, iter [04800, 05004], lr: 0.094273, loss: 2.9966
2022-03-06 06:38:46 - train: epoch 0036, iter [04900, 05004], lr: 0.094273, loss: 3.0962
2022-03-06 06:39:18 - train: epoch 0036, iter [05000, 05004], lr: 0.094273, loss: 3.0719
2022-03-06 06:39:19 - train: epoch 036, train_loss: 3.0580
2022-03-06 06:40:32 - eval: epoch: 036, acc1: 51.372%, acc5: 77.460%, test_loss: 2.0824, per_image_load_time: 1.460ms, per_image_inference_time: 0.514ms
2022-03-06 06:40:33 - until epoch: 036, best_acc1: 52.698%
2022-03-06 06:40:33 - epoch 037 lr: 0.09389272045892023
2022-03-06 06:41:11 - train: epoch 0037, iter [00100, 05004], lr: 0.093893, loss: 3.0818
2022-03-06 06:41:43 - train: epoch 0037, iter [00200, 05004], lr: 0.093893, loss: 2.8073
2022-03-06 06:42:16 - train: epoch 0037, iter [00300, 05004], lr: 0.093893, loss: 2.9433
2022-03-06 06:42:49 - train: epoch 0037, iter [00400, 05004], lr: 0.093893, loss: 3.0200
2022-03-06 06:43:21 - train: epoch 0037, iter [00500, 05004], lr: 0.093893, loss: 3.0108
2022-03-06 06:43:54 - train: epoch 0037, iter [00600, 05004], lr: 0.093893, loss: 3.0647
2022-03-06 06:44:26 - train: epoch 0037, iter [00700, 05004], lr: 0.093893, loss: 2.9142
2022-03-06 06:44:58 - train: epoch 0037, iter [00800, 05004], lr: 0.093893, loss: 2.9568
2022-03-06 06:45:30 - train: epoch 0037, iter [00900, 05004], lr: 0.093893, loss: 3.3795
2022-03-06 06:46:02 - train: epoch 0037, iter [01000, 05004], lr: 0.093893, loss: 3.0508
2022-03-06 06:46:36 - train: epoch 0037, iter [01100, 05004], lr: 0.093893, loss: 3.1781
2022-03-06 06:47:09 - train: epoch 0037, iter [01200, 05004], lr: 0.093893, loss: 3.0328
2022-03-06 06:47:42 - train: epoch 0037, iter [01300, 05004], lr: 0.093893, loss: 3.0655
2022-03-06 06:48:16 - train: epoch 0037, iter [01400, 05004], lr: 0.093893, loss: 3.0174
2022-03-06 06:48:50 - train: epoch 0037, iter [01500, 05004], lr: 0.093893, loss: 2.8126
2022-03-06 06:49:23 - train: epoch 0037, iter [01600, 05004], lr: 0.093893, loss: 2.8058
2022-03-06 06:49:56 - train: epoch 0037, iter [01700, 05004], lr: 0.093893, loss: 3.4107
2022-03-06 06:50:28 - train: epoch 0037, iter [01800, 05004], lr: 0.093893, loss: 3.1208
2022-03-06 06:51:01 - train: epoch 0037, iter [01900, 05004], lr: 0.093893, loss: 3.1290
2022-03-06 06:51:33 - train: epoch 0037, iter [02000, 05004], lr: 0.093893, loss: 3.2995
2022-03-06 06:52:06 - train: epoch 0037, iter [02100, 05004], lr: 0.093893, loss: 3.1524
2022-03-06 06:52:38 - train: epoch 0037, iter [02200, 05004], lr: 0.093893, loss: 2.8304
2022-03-06 06:53:10 - train: epoch 0037, iter [02300, 05004], lr: 0.093893, loss: 2.7474
2022-03-06 06:53:42 - train: epoch 0037, iter [02400, 05004], lr: 0.093893, loss: 2.9461
2022-03-06 06:54:14 - train: epoch 0037, iter [02500, 05004], lr: 0.093893, loss: 2.9884
2022-03-06 06:54:47 - train: epoch 0037, iter [02600, 05004], lr: 0.093893, loss: 3.2639
2022-03-06 06:55:20 - train: epoch 0037, iter [02700, 05004], lr: 0.093893, loss: 3.3157
2022-03-06 06:55:54 - train: epoch 0037, iter [02800, 05004], lr: 0.093893, loss: 2.9015
2022-03-06 06:56:27 - train: epoch 0037, iter [02900, 05004], lr: 0.093893, loss: 3.2683
2022-03-06 06:57:00 - train: epoch 0037, iter [03000, 05004], lr: 0.093893, loss: 3.2476
2022-03-06 06:57:34 - train: epoch 0037, iter [03100, 05004], lr: 0.093893, loss: 3.0215
2022-03-06 06:58:08 - train: epoch 0037, iter [03200, 05004], lr: 0.093893, loss: 3.1795
2022-03-06 06:58:40 - train: epoch 0037, iter [03300, 05004], lr: 0.093893, loss: 3.0543
2022-03-06 06:59:13 - train: epoch 0037, iter [03400, 05004], lr: 0.093893, loss: 2.9368
2022-03-06 06:59:45 - train: epoch 0037, iter [03500, 05004], lr: 0.093893, loss: 3.0855
2022-03-06 07:00:18 - train: epoch 0037, iter [03600, 05004], lr: 0.093893, loss: 3.1540
2022-03-06 07:00:49 - train: epoch 0037, iter [03700, 05004], lr: 0.093893, loss: 3.1159
2022-03-06 07:01:22 - train: epoch 0037, iter [03800, 05004], lr: 0.093893, loss: 3.1436
2022-03-06 07:01:54 - train: epoch 0037, iter [03900, 05004], lr: 0.093893, loss: 3.1863
2022-03-06 07:02:27 - train: epoch 0037, iter [04000, 05004], lr: 0.093893, loss: 2.9751
2022-03-06 07:02:59 - train: epoch 0037, iter [04100, 05004], lr: 0.093893, loss: 2.9100
2022-03-06 07:03:32 - train: epoch 0037, iter [04200, 05004], lr: 0.093893, loss: 3.1684
2022-03-06 07:04:04 - train: epoch 0037, iter [04300, 05004], lr: 0.093893, loss: 2.9912
2022-03-06 07:04:38 - train: epoch 0037, iter [04400, 05004], lr: 0.093893, loss: 3.0165
2022-03-06 07:05:11 - train: epoch 0037, iter [04500, 05004], lr: 0.093893, loss: 2.9803
2022-03-06 07:05:45 - train: epoch 0037, iter [04600, 05004], lr: 0.093893, loss: 2.9910
2022-03-06 07:06:19 - train: epoch 0037, iter [04700, 05004], lr: 0.093893, loss: 2.9295
2022-03-06 07:06:51 - train: epoch 0037, iter [04800, 05004], lr: 0.093893, loss: 3.1601
2022-03-06 07:07:25 - train: epoch 0037, iter [04900, 05004], lr: 0.093893, loss: 3.0270
2022-03-06 07:07:56 - train: epoch 0037, iter [05000, 05004], lr: 0.093893, loss: 2.7577
2022-03-06 07:07:57 - train: epoch 037, train_loss: 3.0604
2022-03-06 07:09:07 - eval: epoch: 037, acc1: 49.970%, acc5: 76.490%, test_loss: 2.1454, per_image_load_time: 1.607ms, per_image_inference_time: 0.461ms
2022-03-06 07:09:08 - until epoch: 037, best_acc1: 52.698%
2022-03-06 07:09:08 - epoch 038 lr: 0.09350124727707197
2022-03-06 07:09:45 - train: epoch 0038, iter [00100, 05004], lr: 0.093501, loss: 2.9745
2022-03-06 07:10:18 - train: epoch 0038, iter [00200, 05004], lr: 0.093501, loss: 2.8313
2022-03-06 07:10:49 - train: epoch 0038, iter [00300, 05004], lr: 0.093501, loss: 3.0956
2022-03-06 07:11:22 - train: epoch 0038, iter [00400, 05004], lr: 0.093501, loss: 2.9817
2022-03-06 07:11:55 - train: epoch 0038, iter [00500, 05004], lr: 0.093501, loss: 2.9777
2022-03-06 07:12:27 - train: epoch 0038, iter [00600, 05004], lr: 0.093501, loss: 3.2110
2022-03-06 07:13:00 - train: epoch 0038, iter [00700, 05004], lr: 0.093501, loss: 2.7724
2022-03-06 07:13:34 - train: epoch 0038, iter [00800, 05004], lr: 0.093501, loss: 2.8547
2022-03-06 07:14:07 - train: epoch 0038, iter [00900, 05004], lr: 0.093501, loss: 2.8139
2022-03-06 07:14:41 - train: epoch 0038, iter [01000, 05004], lr: 0.093501, loss: 3.1722
2022-03-06 07:15:15 - train: epoch 0038, iter [01100, 05004], lr: 0.093501, loss: 2.9471
2022-03-06 07:15:47 - train: epoch 0038, iter [01200, 05004], lr: 0.093501, loss: 3.0059
2022-03-06 07:16:19 - train: epoch 0038, iter [01300, 05004], lr: 0.093501, loss: 3.0841
2022-03-06 07:16:52 - train: epoch 0038, iter [01400, 05004], lr: 0.093501, loss: 2.9616
2022-03-06 07:17:24 - train: epoch 0038, iter [01500, 05004], lr: 0.093501, loss: 3.2789
2022-03-06 07:17:56 - train: epoch 0038, iter [01600, 05004], lr: 0.093501, loss: 3.1749
2022-03-06 07:18:28 - train: epoch 0038, iter [01700, 05004], lr: 0.093501, loss: 2.9111
2022-03-06 07:19:01 - train: epoch 0038, iter [01800, 05004], lr: 0.093501, loss: 3.2911
2022-03-06 07:19:34 - train: epoch 0038, iter [01900, 05004], lr: 0.093501, loss: 3.0784
2022-03-06 07:20:06 - train: epoch 0038, iter [02000, 05004], lr: 0.093501, loss: 2.9946
2022-03-06 07:20:39 - train: epoch 0038, iter [02100, 05004], lr: 0.093501, loss: 2.9566
2022-03-06 07:21:11 - train: epoch 0038, iter [02200, 05004], lr: 0.093501, loss: 2.9607
2022-03-06 07:21:44 - train: epoch 0038, iter [02300, 05004], lr: 0.093501, loss: 3.0829
2022-03-06 07:22:17 - train: epoch 0038, iter [02400, 05004], lr: 0.093501, loss: 3.4047
2022-03-06 07:22:50 - train: epoch 0038, iter [02500, 05004], lr: 0.093501, loss: 2.9345
2022-03-06 07:23:23 - train: epoch 0038, iter [02600, 05004], lr: 0.093501, loss: 3.2512
2022-03-06 07:23:55 - train: epoch 0038, iter [02700, 05004], lr: 0.093501, loss: 3.0034
2022-03-06 07:24:29 - train: epoch 0038, iter [02800, 05004], lr: 0.093501, loss: 3.3141
2022-03-06 07:25:01 - train: epoch 0038, iter [02900, 05004], lr: 0.093501, loss: 3.2323
2022-03-06 07:25:34 - train: epoch 0038, iter [03000, 05004], lr: 0.093501, loss: 3.0646
2022-03-06 07:26:06 - train: epoch 0038, iter [03100, 05004], lr: 0.093501, loss: 2.9951
2022-03-06 07:26:38 - train: epoch 0038, iter [03200, 05004], lr: 0.093501, loss: 2.7084
2022-03-06 07:27:11 - train: epoch 0038, iter [03300, 05004], lr: 0.093501, loss: 2.6401
2022-03-06 07:27:44 - train: epoch 0038, iter [03400, 05004], lr: 0.093501, loss: 2.9758
2022-03-06 07:28:15 - train: epoch 0038, iter [03500, 05004], lr: 0.093501, loss: 3.0452
2022-03-06 07:28:48 - train: epoch 0038, iter [03600, 05004], lr: 0.093501, loss: 3.2470
2022-03-06 07:29:20 - train: epoch 0038, iter [03700, 05004], lr: 0.093501, loss: 2.7994
2022-03-06 07:29:53 - train: epoch 0038, iter [03800, 05004], lr: 0.093501, loss: 3.1445
2022-03-06 07:30:26 - train: epoch 0038, iter [03900, 05004], lr: 0.093501, loss: 3.1279
2022-03-06 07:30:58 - train: epoch 0038, iter [04000, 05004], lr: 0.093501, loss: 3.1501
2022-03-06 07:31:30 - train: epoch 0038, iter [04100, 05004], lr: 0.093501, loss: 2.8883
2022-03-06 07:32:04 - train: epoch 0038, iter [04200, 05004], lr: 0.093501, loss: 2.7523
2022-03-06 07:32:38 - train: epoch 0038, iter [04300, 05004], lr: 0.093501, loss: 3.2049
2022-03-06 07:33:10 - train: epoch 0038, iter [04400, 05004], lr: 0.093501, loss: 3.0359
2022-03-06 07:33:44 - train: epoch 0038, iter [04500, 05004], lr: 0.093501, loss: 3.2572
2022-03-06 07:34:16 - train: epoch 0038, iter [04600, 05004], lr: 0.093501, loss: 3.1327
2022-03-06 07:34:49 - train: epoch 0038, iter [04700, 05004], lr: 0.093501, loss: 2.9853
2022-03-06 07:35:22 - train: epoch 0038, iter [04800, 05004], lr: 0.093501, loss: 3.0603
2022-03-06 07:35:54 - train: epoch 0038, iter [04900, 05004], lr: 0.093501, loss: 2.9167
2022-03-06 07:36:25 - train: epoch 0038, iter [05000, 05004], lr: 0.093501, loss: 2.8888
2022-03-06 07:36:26 - train: epoch 038, train_loss: 3.0514
2022-03-06 07:37:38 - eval: epoch: 038, acc1: 49.648%, acc5: 75.790%, test_loss: 2.1766, per_image_load_time: 1.824ms, per_image_inference_time: 0.470ms
2022-03-06 07:37:38 - until epoch: 038, best_acc1: 52.698%
2022-03-06 07:37:38 - epoch 039 lr: 0.09309848334400246
2022-03-06 07:38:16 - train: epoch 0039, iter [00100, 05004], lr: 0.093098, loss: 3.2712
2022-03-06 07:38:48 - train: epoch 0039, iter [00200, 05004], lr: 0.093098, loss: 3.0797
2022-03-06 07:39:20 - train: epoch 0039, iter [00300, 05004], lr: 0.093098, loss: 2.8951
2022-03-06 07:39:54 - train: epoch 0039, iter [00400, 05004], lr: 0.093098, loss: 3.0202
2022-03-06 07:40:27 - train: epoch 0039, iter [00500, 05004], lr: 0.093098, loss: 2.8824
2022-03-06 07:41:00 - train: epoch 0039, iter [00600, 05004], lr: 0.093098, loss: 2.7164
2022-03-06 07:41:34 - train: epoch 0039, iter [00700, 05004], lr: 0.093098, loss: 3.1347
2022-03-06 07:42:07 - train: epoch 0039, iter [00800, 05004], lr: 0.093098, loss: 3.1385
2022-03-06 07:42:41 - train: epoch 0039, iter [00900, 05004], lr: 0.093098, loss: 3.0683
2022-03-06 07:43:13 - train: epoch 0039, iter [01000, 05004], lr: 0.093098, loss: 2.9974
2022-03-06 07:43:45 - train: epoch 0039, iter [01100, 05004], lr: 0.093098, loss: 3.1270
2022-03-06 07:44:18 - train: epoch 0039, iter [01200, 05004], lr: 0.093098, loss: 3.1952
2022-03-06 07:44:50 - train: epoch 0039, iter [01300, 05004], lr: 0.093098, loss: 3.3260
2022-03-06 07:45:22 - train: epoch 0039, iter [01400, 05004], lr: 0.093098, loss: 3.0516
2022-03-06 07:45:55 - train: epoch 0039, iter [01500, 05004], lr: 0.093098, loss: 2.7933
2022-03-06 07:46:27 - train: epoch 0039, iter [01600, 05004], lr: 0.093098, loss: 3.0422
2022-03-06 07:46:59 - train: epoch 0039, iter [01700, 05004], lr: 0.093098, loss: 2.8860
2022-03-06 07:47:31 - train: epoch 0039, iter [01800, 05004], lr: 0.093098, loss: 3.0698
2022-03-06 07:48:03 - train: epoch 0039, iter [01900, 05004], lr: 0.093098, loss: 2.9123
2022-03-06 07:48:36 - train: epoch 0039, iter [02000, 05004], lr: 0.093098, loss: 3.0603
2022-03-06 07:49:10 - train: epoch 0039, iter [02100, 05004], lr: 0.093098, loss: 3.3436
2022-03-06 07:49:43 - train: epoch 0039, iter [02200, 05004], lr: 0.093098, loss: 2.9710
2022-03-06 07:50:16 - train: epoch 0039, iter [02300, 05004], lr: 0.093098, loss: 3.2939
2022-03-06 07:50:50 - train: epoch 0039, iter [02400, 05004], lr: 0.093098, loss: 3.1178
2022-03-06 07:51:24 - train: epoch 0039, iter [02500, 05004], lr: 0.093098, loss: 2.7919
2022-03-06 07:51:55 - train: epoch 0039, iter [02600, 05004], lr: 0.093098, loss: 3.1507
2022-03-06 07:52:28 - train: epoch 0039, iter [02700, 05004], lr: 0.093098, loss: 3.1152
2022-03-06 07:53:00 - train: epoch 0039, iter [02800, 05004], lr: 0.093098, loss: 2.9282
2022-03-06 07:53:33 - train: epoch 0039, iter [02900, 05004], lr: 0.093098, loss: 2.7155
2022-03-06 07:54:05 - train: epoch 0039, iter [03000, 05004], lr: 0.093098, loss: 2.9462
2022-03-06 07:54:37 - train: epoch 0039, iter [03100, 05004], lr: 0.093098, loss: 2.9942
2022-03-06 07:55:10 - train: epoch 0039, iter [03200, 05004], lr: 0.093098, loss: 2.8656
2022-03-06 07:55:42 - train: epoch 0039, iter [03300, 05004], lr: 0.093098, loss: 3.3764
2022-03-06 07:56:15 - train: epoch 0039, iter [03400, 05004], lr: 0.093098, loss: 3.1081
2022-03-06 07:56:47 - train: epoch 0039, iter [03500, 05004], lr: 0.093098, loss: 3.1870
2022-03-06 07:57:20 - train: epoch 0039, iter [03600, 05004], lr: 0.093098, loss: 3.0812
2022-03-06 07:57:53 - train: epoch 0039, iter [03700, 05004], lr: 0.093098, loss: 2.9823
2022-03-06 07:58:26 - train: epoch 0039, iter [03800, 05004], lr: 0.093098, loss: 2.7440
2022-03-06 07:59:00 - train: epoch 0039, iter [03900, 05004], lr: 0.093098, loss: 3.2302
2022-03-06 07:59:33 - train: epoch 0039, iter [04000, 05004], lr: 0.093098, loss: 3.0748
2022-03-06 08:00:07 - train: epoch 0039, iter [04100, 05004], lr: 0.093098, loss: 3.1044
2022-03-06 08:00:39 - train: epoch 0039, iter [04200, 05004], lr: 0.093098, loss: 3.1598
2022-03-06 08:01:11 - train: epoch 0039, iter [04300, 05004], lr: 0.093098, loss: 3.2007
2022-03-06 08:01:43 - train: epoch 0039, iter [04400, 05004], lr: 0.093098, loss: 2.8758
2022-03-06 08:02:16 - train: epoch 0039, iter [04500, 05004], lr: 0.093098, loss: 2.8577
2022-03-06 08:02:48 - train: epoch 0039, iter [04600, 05004], lr: 0.093098, loss: 3.2383
2022-03-06 08:03:21 - train: epoch 0039, iter [04700, 05004], lr: 0.093098, loss: 3.0110
2022-03-06 08:03:53 - train: epoch 0039, iter [04800, 05004], lr: 0.093098, loss: 2.8740
2022-03-06 08:04:27 - train: epoch 0039, iter [04900, 05004], lr: 0.093098, loss: 2.9344
2022-03-06 08:04:58 - train: epoch 0039, iter [05000, 05004], lr: 0.093098, loss: 3.0149
2022-03-06 08:04:59 - train: epoch 039, train_loss: 3.0433
2022-03-06 08:06:11 - eval: epoch: 039, acc1: 50.542%, acc5: 76.626%, test_loss: 2.1262, per_image_load_time: 2.396ms, per_image_inference_time: 0.422ms
2022-03-06 08:06:12 - until epoch: 039, best_acc1: 52.698%
2022-03-06 08:06:12 - epoch 040 lr: 0.09268453319711362
2022-03-06 08:06:50 - train: epoch 0040, iter [00100, 05004], lr: 0.092685, loss: 3.3545
2022-03-06 08:07:24 - train: epoch 0040, iter [00200, 05004], lr: 0.092685, loss: 3.3203
2022-03-06 08:07:57 - train: epoch 0040, iter [00300, 05004], lr: 0.092685, loss: 3.3005
2022-03-06 08:08:30 - train: epoch 0040, iter [00400, 05004], lr: 0.092685, loss: 3.0864
2022-03-06 08:09:04 - train: epoch 0040, iter [00500, 05004], lr: 0.092685, loss: 2.8353
2022-03-06 08:09:35 - train: epoch 0040, iter [00600, 05004], lr: 0.092685, loss: 3.0259
2022-03-06 08:10:07 - train: epoch 0040, iter [00700, 05004], lr: 0.092685, loss: 3.2065
2022-03-06 08:10:39 - train: epoch 0040, iter [00800, 05004], lr: 0.092685, loss: 3.1425
2022-03-06 08:11:13 - train: epoch 0040, iter [00900, 05004], lr: 0.092685, loss: 3.0260
2022-03-06 08:11:45 - train: epoch 0040, iter [01000, 05004], lr: 0.092685, loss: 2.7295
2022-03-06 08:12:18 - train: epoch 0040, iter [01100, 05004], lr: 0.092685, loss: 2.9112
2022-03-06 08:12:50 - train: epoch 0040, iter [01200, 05004], lr: 0.092685, loss: 3.0794
2022-03-06 08:13:23 - train: epoch 0040, iter [01300, 05004], lr: 0.092685, loss: 2.9162
2022-03-06 08:13:56 - train: epoch 0040, iter [01400, 05004], lr: 0.092685, loss: 2.9649
2022-03-06 08:14:29 - train: epoch 0040, iter [01500, 05004], lr: 0.092685, loss: 3.2891
2022-03-06 08:15:02 - train: epoch 0040, iter [01600, 05004], lr: 0.092685, loss: 3.2150
2022-03-06 08:15:36 - train: epoch 0040, iter [01700, 05004], lr: 0.092685, loss: 3.1219
2022-03-06 08:16:09 - train: epoch 0040, iter [01800, 05004], lr: 0.092685, loss: 2.8691
2022-03-06 08:16:42 - train: epoch 0040, iter [01900, 05004], lr: 0.092685, loss: 2.9163
2022-03-06 08:17:15 - train: epoch 0040, iter [02000, 05004], lr: 0.092685, loss: 3.1513
2022-03-06 08:17:48 - train: epoch 0040, iter [02100, 05004], lr: 0.092685, loss: 3.0769
2022-03-06 08:18:21 - train: epoch 0040, iter [02200, 05004], lr: 0.092685, loss: 2.8643
2022-03-06 08:18:52 - train: epoch 0040, iter [02300, 05004], lr: 0.092685, loss: 2.8656
2022-03-06 08:19:26 - train: epoch 0040, iter [02400, 05004], lr: 0.092685, loss: 2.8751
2022-03-06 08:19:59 - train: epoch 0040, iter [02500, 05004], lr: 0.092685, loss: 3.1510
2022-03-06 08:20:32 - train: epoch 0040, iter [02600, 05004], lr: 0.092685, loss: 2.8179
2022-03-06 08:21:04 - train: epoch 0040, iter [02700, 05004], lr: 0.092685, loss: 3.2304
2022-03-06 08:21:37 - train: epoch 0040, iter [02800, 05004], lr: 0.092685, loss: 3.1718
2022-03-06 08:22:10 - train: epoch 0040, iter [02900, 05004], lr: 0.092685, loss: 3.2127
2022-03-06 08:22:43 - train: epoch 0040, iter [03000, 05004], lr: 0.092685, loss: 3.0989
2022-03-06 08:23:16 - train: epoch 0040, iter [03100, 05004], lr: 0.092685, loss: 2.7160
2022-03-06 08:23:49 - train: epoch 0040, iter [03200, 05004], lr: 0.092685, loss: 3.1020
2022-03-06 08:24:23 - train: epoch 0040, iter [03300, 05004], lr: 0.092685, loss: 3.1178
2022-03-06 08:24:55 - train: epoch 0040, iter [03400, 05004], lr: 0.092685, loss: 3.1354
2022-03-06 08:25:28 - train: epoch 0040, iter [03500, 05004], lr: 0.092685, loss: 3.2079
2022-03-06 08:26:01 - train: epoch 0040, iter [03600, 05004], lr: 0.092685, loss: 3.0215
2022-03-06 08:26:34 - train: epoch 0040, iter [03700, 05004], lr: 0.092685, loss: 2.9509
2022-03-06 08:27:07 - train: epoch 0040, iter [03800, 05004], lr: 0.092685, loss: 2.7340
2022-03-06 08:27:39 - train: epoch 0040, iter [03900, 05004], lr: 0.092685, loss: 3.1740
2022-03-06 08:28:12 - train: epoch 0040, iter [04000, 05004], lr: 0.092685, loss: 2.7130
2022-03-06 08:28:45 - train: epoch 0040, iter [04100, 05004], lr: 0.092685, loss: 2.9540
2022-03-06 08:29:18 - train: epoch 0040, iter [04200, 05004], lr: 0.092685, loss: 2.9928
2022-03-06 08:29:50 - train: epoch 0040, iter [04300, 05004], lr: 0.092685, loss: 3.1909
2022-03-06 08:30:23 - train: epoch 0040, iter [04400, 05004], lr: 0.092685, loss: 3.0088
2022-03-06 08:30:55 - train: epoch 0040, iter [04500, 05004], lr: 0.092685, loss: 2.6329
2022-03-06 08:31:28 - train: epoch 0040, iter [04600, 05004], lr: 0.092685, loss: 2.9500
2022-03-06 08:32:02 - train: epoch 0040, iter [04700, 05004], lr: 0.092685, loss: 3.0072
2022-03-06 08:32:35 - train: epoch 0040, iter [04800, 05004], lr: 0.092685, loss: 3.0082
2022-03-06 08:33:07 - train: epoch 0040, iter [04900, 05004], lr: 0.092685, loss: 3.0920
2022-03-06 08:33:39 - train: epoch 0040, iter [05000, 05004], lr: 0.092685, loss: 3.1191
2022-03-06 08:33:40 - train: epoch 040, train_loss: 3.0443
2022-03-06 08:34:54 - eval: epoch: 040, acc1: 52.716%, acc5: 78.646%, test_loss: 2.0050, per_image_load_time: 2.400ms, per_image_inference_time: 0.499ms
2022-03-06 08:34:55 - until epoch: 040, best_acc1: 52.716%
2022-03-06 08:34:55 - epoch 041 lr: 0.09225950427718975
2022-03-06 08:35:32 - train: epoch 0041, iter [00100, 05004], lr: 0.092260, loss: 3.3647
2022-03-06 08:36:05 - train: epoch 0041, iter [00200, 05004], lr: 0.092260, loss: 3.1438
2022-03-06 08:36:37 - train: epoch 0041, iter [00300, 05004], lr: 0.092260, loss: 3.0343
2022-03-06 08:37:10 - train: epoch 0041, iter [00400, 05004], lr: 0.092260, loss: 3.1691
2022-03-06 08:37:43 - train: epoch 0041, iter [00500, 05004], lr: 0.092260, loss: 2.8034
2022-03-06 08:38:15 - train: epoch 0041, iter [00600, 05004], lr: 0.092260, loss: 3.0778
2022-03-06 08:38:48 - train: epoch 0041, iter [00700, 05004], lr: 0.092260, loss: 2.9190
2022-03-06 08:39:20 - train: epoch 0041, iter [00800, 05004], lr: 0.092260, loss: 2.7095
2022-03-06 08:39:52 - train: epoch 0041, iter [00900, 05004], lr: 0.092260, loss: 2.7202
2022-03-06 08:40:24 - train: epoch 0041, iter [01000, 05004], lr: 0.092260, loss: 3.1266
2022-03-06 08:40:57 - train: epoch 0041, iter [01100, 05004], lr: 0.092260, loss: 2.9487
2022-03-06 08:41:30 - train: epoch 0041, iter [01200, 05004], lr: 0.092260, loss: 2.7894
2022-03-06 08:42:03 - train: epoch 0041, iter [01300, 05004], lr: 0.092260, loss: 2.9429
2022-03-06 08:42:37 - train: epoch 0041, iter [01400, 05004], lr: 0.092260, loss: 3.1986
2022-03-06 08:43:11 - train: epoch 0041, iter [01500, 05004], lr: 0.092260, loss: 3.3978
2022-03-06 08:43:44 - train: epoch 0041, iter [01600, 05004], lr: 0.092260, loss: 2.9768
2022-03-06 08:44:17 - train: epoch 0041, iter [01700, 05004], lr: 0.092260, loss: 3.1114
2022-03-06 08:44:50 - train: epoch 0041, iter [01800, 05004], lr: 0.092260, loss: 3.1114
2022-03-06 08:45:23 - train: epoch 0041, iter [01900, 05004], lr: 0.092260, loss: 3.0488
2022-03-06 08:45:54 - train: epoch 0041, iter [02000, 05004], lr: 0.092260, loss: 3.1090
2022-03-06 08:46:27 - train: epoch 0041, iter [02100, 05004], lr: 0.092260, loss: 2.8045
2022-03-06 08:47:00 - train: epoch 0041, iter [02200, 05004], lr: 0.092260, loss: 2.8601
2022-03-06 08:47:33 - train: epoch 0041, iter [02300, 05004], lr: 0.092260, loss: 2.8128
2022-03-06 08:48:04 - train: epoch 0041, iter [02400, 05004], lr: 0.092260, loss: 3.2307
2022-03-06 08:48:36 - train: epoch 0041, iter [02500, 05004], lr: 0.092260, loss: 2.9839
2022-03-06 08:49:09 - train: epoch 0041, iter [02600, 05004], lr: 0.092260, loss: 3.4896
2022-03-06 08:49:42 - train: epoch 0041, iter [02700, 05004], lr: 0.092260, loss: 3.3178
2022-03-06 08:50:15 - train: epoch 0041, iter [02800, 05004], lr: 0.092260, loss: 2.9918
2022-03-06 08:50:49 - train: epoch 0041, iter [02900, 05004], lr: 0.092260, loss: 3.0779
2022-03-06 08:51:23 - train: epoch 0041, iter [03000, 05004], lr: 0.092260, loss: 3.0017
2022-03-06 08:51:56 - train: epoch 0041, iter [03100, 05004], lr: 0.092260, loss: 3.4628
2022-03-06 08:52:30 - train: epoch 0041, iter [03200, 05004], lr: 0.092260, loss: 3.0198
2022-03-06 08:53:03 - train: epoch 0041, iter [03300, 05004], lr: 0.092260, loss: 2.7662
2022-03-06 08:53:36 - train: epoch 0041, iter [03400, 05004], lr: 0.092260, loss: 3.0575
2022-03-06 08:54:08 - train: epoch 0041, iter [03500, 05004], lr: 0.092260, loss: 3.0924
2022-03-06 08:54:40 - train: epoch 0041, iter [03600, 05004], lr: 0.092260, loss: 3.1983
2022-03-06 08:55:12 - train: epoch 0041, iter [03700, 05004], lr: 0.092260, loss: 3.2632
2022-03-06 08:55:44 - train: epoch 0041, iter [03800, 05004], lr: 0.092260, loss: 2.9770
2022-03-06 08:56:16 - train: epoch 0041, iter [03900, 05004], lr: 0.092260, loss: 3.1301
2022-03-06 08:56:48 - train: epoch 0041, iter [04000, 05004], lr: 0.092260, loss: 2.9464
2022-03-06 08:57:21 - train: epoch 0041, iter [04100, 05004], lr: 0.092260, loss: 2.9654
2022-03-06 08:57:54 - train: epoch 0041, iter [04200, 05004], lr: 0.092260, loss: 2.7426
2022-03-06 08:58:27 - train: epoch 0041, iter [04300, 05004], lr: 0.092260, loss: 3.0758
2022-03-06 08:58:59 - train: epoch 0041, iter [04400, 05004], lr: 0.092260, loss: 3.1897
2022-03-06 08:59:32 - train: epoch 0041, iter [04500, 05004], lr: 0.092260, loss: 3.1581
2022-03-06 09:00:06 - train: epoch 0041, iter [04600, 05004], lr: 0.092260, loss: 2.8901
2022-03-06 09:00:40 - train: epoch 0041, iter [04700, 05004], lr: 0.092260, loss: 3.2017
2022-03-06 09:01:13 - train: epoch 0041, iter [04800, 05004], lr: 0.092260, loss: 3.1110
2022-03-06 09:01:46 - train: epoch 0041, iter [04900, 05004], lr: 0.092260, loss: 3.1143
2022-03-06 09:02:17 - train: epoch 0041, iter [05000, 05004], lr: 0.092260, loss: 2.8966
2022-03-06 09:02:18 - train: epoch 041, train_loss: 3.0408
2022-03-06 09:03:29 - eval: epoch: 041, acc1: 51.698%, acc5: 77.500%, test_loss: 2.0761, per_image_load_time: 1.549ms, per_image_inference_time: 0.467ms
2022-03-06 09:03:29 - until epoch: 041, best_acc1: 52.716%
2022-03-06 09:03:29 - epoch 042 lr: 0.09182350690051133
2022-03-06 09:04:06 - train: epoch 0042, iter [00100, 05004], lr: 0.091824, loss: 2.9075
2022-03-06 09:04:39 - train: epoch 0042, iter [00200, 05004], lr: 0.091824, loss: 2.9243
2022-03-06 09:05:11 - train: epoch 0042, iter [00300, 05004], lr: 0.091824, loss: 3.0755
2022-03-06 09:05:44 - train: epoch 0042, iter [00400, 05004], lr: 0.091824, loss: 2.8607
2022-03-06 09:06:16 - train: epoch 0042, iter [00500, 05004], lr: 0.091824, loss: 2.9613
2022-03-06 09:06:49 - train: epoch 0042, iter [00600, 05004], lr: 0.091824, loss: 3.1522
2022-03-06 09:07:21 - train: epoch 0042, iter [00700, 05004], lr: 0.091824, loss: 3.0378
2022-03-06 09:07:55 - train: epoch 0042, iter [00800, 05004], lr: 0.091824, loss: 2.8906
2022-03-06 09:08:28 - train: epoch 0042, iter [00900, 05004], lr: 0.091824, loss: 3.3022
2022-03-06 09:09:01 - train: epoch 0042, iter [01000, 05004], lr: 0.091824, loss: 2.9865
2022-03-06 09:09:35 - train: epoch 0042, iter [01100, 05004], lr: 0.091824, loss: 2.8724
2022-03-06 09:10:08 - train: epoch 0042, iter [01200, 05004], lr: 0.091824, loss: 2.9229
2022-03-06 09:10:41 - train: epoch 0042, iter [01300, 05004], lr: 0.091824, loss: 3.0997
2022-03-06 09:11:12 - train: epoch 0042, iter [01400, 05004], lr: 0.091824, loss: 3.3378
2022-03-06 09:11:45 - train: epoch 0042, iter [01500, 05004], lr: 0.091824, loss: 2.9793
2022-03-06 09:12:17 - train: epoch 0042, iter [01600, 05004], lr: 0.091824, loss: 3.1283
2022-03-06 09:12:49 - train: epoch 0042, iter [01700, 05004], lr: 0.091824, loss: 2.7711
2022-03-06 09:13:22 - train: epoch 0042, iter [01800, 05004], lr: 0.091824, loss: 3.0390
2022-03-06 09:13:53 - train: epoch 0042, iter [01900, 05004], lr: 0.091824, loss: 3.3337
2022-03-06 09:14:26 - train: epoch 0042, iter [02000, 05004], lr: 0.091824, loss: 2.9333
2022-03-06 09:14:58 - train: epoch 0042, iter [02100, 05004], lr: 0.091824, loss: 2.9803
2022-03-06 09:15:31 - train: epoch 0042, iter [02200, 05004], lr: 0.091824, loss: 3.0246
2022-03-06 09:16:04 - train: epoch 0042, iter [02300, 05004], lr: 0.091824, loss: 2.8409
2022-03-06 09:16:37 - train: epoch 0042, iter [02400, 05004], lr: 0.091824, loss: 3.2090
2022-03-06 09:17:10 - train: epoch 0042, iter [02500, 05004], lr: 0.091824, loss: 3.1449
2022-03-06 09:17:44 - train: epoch 0042, iter [02600, 05004], lr: 0.091824, loss: 3.1651
2022-03-06 09:18:17 - train: epoch 0042, iter [02700, 05004], lr: 0.091824, loss: 2.9083
2022-03-06 09:18:49 - train: epoch 0042, iter [02800, 05004], lr: 0.091824, loss: 2.9261
2022-03-06 09:19:21 - train: epoch 0042, iter [02900, 05004], lr: 0.091824, loss: 3.1344
2022-03-06 09:19:54 - train: epoch 0042, iter [03000, 05004], lr: 0.091824, loss: 3.2094
2022-03-06 09:20:26 - train: epoch 0042, iter [03100, 05004], lr: 0.091824, loss: 2.9519
2022-03-06 09:20:59 - train: epoch 0042, iter [03200, 05004], lr: 0.091824, loss: 3.1212
2022-03-06 09:21:31 - train: epoch 0042, iter [03300, 05004], lr: 0.091824, loss: 3.2621
2022-03-06 09:22:03 - train: epoch 0042, iter [03400, 05004], lr: 0.091824, loss: 2.9209
2022-03-06 09:22:36 - train: epoch 0042, iter [03500, 05004], lr: 0.091824, loss: 3.1189
2022-03-06 09:23:08 - train: epoch 0042, iter [03600, 05004], lr: 0.091824, loss: 3.2290
2022-03-06 09:23:41 - train: epoch 0042, iter [03700, 05004], lr: 0.091824, loss: 3.3323
2022-03-06 09:24:14 - train: epoch 0042, iter [03800, 05004], lr: 0.091824, loss: 2.9491
2022-03-06 09:24:46 - train: epoch 0042, iter [03900, 05004], lr: 0.091824, loss: 3.0746
2022-03-06 09:25:19 - train: epoch 0042, iter [04000, 05004], lr: 0.091824, loss: 2.9343
2022-03-06 09:25:52 - train: epoch 0042, iter [04100, 05004], lr: 0.091824, loss: 2.9518
2022-03-06 09:26:26 - train: epoch 0042, iter [04200, 05004], lr: 0.091824, loss: 3.0766
2022-03-06 09:26:58 - train: epoch 0042, iter [04300, 05004], lr: 0.091824, loss: 2.8980
2022-03-06 09:27:31 - train: epoch 0042, iter [04400, 05004], lr: 0.091824, loss: 2.9032
2022-03-06 09:28:05 - train: epoch 0042, iter [04500, 05004], lr: 0.091824, loss: 3.0819
2022-03-06 09:28:37 - train: epoch 0042, iter [04600, 05004], lr: 0.091824, loss: 3.4749
2022-03-06 09:29:10 - train: epoch 0042, iter [04700, 05004], lr: 0.091824, loss: 3.1478
2022-03-06 09:29:42 - train: epoch 0042, iter [04800, 05004], lr: 0.091824, loss: 3.1252
2022-03-06 09:30:14 - train: epoch 0042, iter [04900, 05004], lr: 0.091824, loss: 2.9612
2022-03-06 09:30:45 - train: epoch 0042, iter [05000, 05004], lr: 0.091824, loss: 3.2079
2022-03-06 09:30:46 - train: epoch 042, train_loss: 3.0367
2022-03-06 09:31:58 - eval: epoch: 042, acc1: 50.520%, acc5: 76.618%, test_loss: 2.1273, per_image_load_time: 2.165ms, per_image_inference_time: 0.472ms
2022-03-06 09:31:58 - until epoch: 042, best_acc1: 52.716%
2022-03-06 09:31:58 - epoch 043 lr: 0.09137665423022251
2022-03-06 09:32:36 - train: epoch 0043, iter [00100, 05004], lr: 0.091377, loss: 2.8810
2022-03-06 09:33:09 - train: epoch 0043, iter [00200, 05004], lr: 0.091377, loss: 3.1229
2022-03-06 09:33:42 - train: epoch 0043, iter [00300, 05004], lr: 0.091377, loss: 2.7822
2022-03-06 09:34:14 - train: epoch 0043, iter [00400, 05004], lr: 0.091377, loss: 3.3085
2022-03-06 09:34:46 - train: epoch 0043, iter [00500, 05004], lr: 0.091377, loss: 3.0694
2022-03-06 09:35:19 - train: epoch 0043, iter [00600, 05004], lr: 0.091377, loss: 3.0093
2022-03-06 09:35:52 - train: epoch 0043, iter [00700, 05004], lr: 0.091377, loss: 3.0504
2022-03-06 09:36:27 - train: epoch 0043, iter [00800, 05004], lr: 0.091377, loss: 2.9934
2022-03-06 09:37:00 - train: epoch 0043, iter [00900, 05004], lr: 0.091377, loss: 2.8383
2022-03-06 09:37:34 - train: epoch 0043, iter [01000, 05004], lr: 0.091377, loss: 3.2446
2022-03-06 09:38:06 - train: epoch 0043, iter [01100, 05004], lr: 0.091377, loss: 2.8946
2022-03-06 09:38:38 - train: epoch 0043, iter [01200, 05004], lr: 0.091377, loss: 2.8857
2022-03-06 09:39:11 - train: epoch 0043, iter [01300, 05004], lr: 0.091377, loss: 3.1719
2022-03-06 09:39:44 - train: epoch 0043, iter [01400, 05004], lr: 0.091377, loss: 2.9886
2022-03-06 09:40:17 - train: epoch 0043, iter [01500, 05004], lr: 0.091377, loss: 2.9077
2022-03-06 09:40:50 - train: epoch 0043, iter [01600, 05004], lr: 0.091377, loss: 3.0370
2022-03-06 09:41:23 - train: epoch 0043, iter [01700, 05004], lr: 0.091377, loss: 3.1363
2022-03-06 09:41:56 - train: epoch 0043, iter [01800, 05004], lr: 0.091377, loss: 3.1135
2022-03-06 09:42:30 - train: epoch 0043, iter [01900, 05004], lr: 0.091377, loss: 2.7303
2022-03-06 09:43:04 - train: epoch 0043, iter [02000, 05004], lr: 0.091377, loss: 2.8550
2022-03-06 09:43:37 - train: epoch 0043, iter [02100, 05004], lr: 0.091377, loss: 2.9612
2022-03-06 09:44:11 - train: epoch 0043, iter [02200, 05004], lr: 0.091377, loss: 3.1431
2022-03-06 09:44:44 - train: epoch 0043, iter [02300, 05004], lr: 0.091377, loss: 3.1046
2022-03-06 09:45:17 - train: epoch 0043, iter [02400, 05004], lr: 0.091377, loss: 3.1021
2022-03-06 09:45:50 - train: epoch 0043, iter [02500, 05004], lr: 0.091377, loss: 3.3662
2022-03-06 09:46:24 - train: epoch 0043, iter [02600, 05004], lr: 0.091377, loss: 2.4959
2022-03-06 09:46:57 - train: epoch 0043, iter [02700, 05004], lr: 0.091377, loss: 3.3306
2022-03-06 09:47:30 - train: epoch 0043, iter [02800, 05004], lr: 0.091377, loss: 2.9612
2022-03-06 09:48:04 - train: epoch 0043, iter [02900, 05004], lr: 0.091377, loss: 3.3546
2022-03-06 09:48:36 - train: epoch 0043, iter [03000, 05004], lr: 0.091377, loss: 3.2213
2022-03-06 09:49:08 - train: epoch 0043, iter [03100, 05004], lr: 0.091377, loss: 3.0201
2022-03-06 09:49:41 - train: epoch 0043, iter [03200, 05004], lr: 0.091377, loss: 2.9617
2022-03-06 09:50:15 - train: epoch 0043, iter [03300, 05004], lr: 0.091377, loss: 3.1330
2022-03-06 09:50:49 - train: epoch 0043, iter [03400, 05004], lr: 0.091377, loss: 3.0490
2022-03-06 09:51:23 - train: epoch 0043, iter [03500, 05004], lr: 0.091377, loss: 3.1749
2022-03-06 09:51:56 - train: epoch 0043, iter [03600, 05004], lr: 0.091377, loss: 3.0845
2022-03-06 09:52:31 - train: epoch 0043, iter [03700, 05004], lr: 0.091377, loss: 3.0193
2022-03-06 09:53:03 - train: epoch 0043, iter [03800, 05004], lr: 0.091377, loss: 3.0026
2022-03-06 09:53:37 - train: epoch 0043, iter [03900, 05004], lr: 0.091377, loss: 3.1754
2022-03-06 09:54:09 - train: epoch 0043, iter [04000, 05004], lr: 0.091377, loss: 3.0108
2022-03-06 09:54:42 - train: epoch 0043, iter [04100, 05004], lr: 0.091377, loss: 3.1563
2022-03-06 09:55:15 - train: epoch 0043, iter [04200, 05004], lr: 0.091377, loss: 3.0900
2022-03-06 09:55:47 - train: epoch 0043, iter [04300, 05004], lr: 0.091377, loss: 3.0035
2022-03-06 09:56:20 - train: epoch 0043, iter [04400, 05004], lr: 0.091377, loss: 3.0984
2022-03-06 09:56:53 - train: epoch 0043, iter [04500, 05004], lr: 0.091377, loss: 2.8240
2022-03-06 09:57:26 - train: epoch 0043, iter [04600, 05004], lr: 0.091377, loss: 3.1087
2022-03-06 09:57:59 - train: epoch 0043, iter [04700, 05004], lr: 0.091377, loss: 2.9166
2022-03-06 09:58:33 - train: epoch 0043, iter [04800, 05004], lr: 0.091377, loss: 2.9902
2022-03-06 09:59:06 - train: epoch 0043, iter [04900, 05004], lr: 0.091377, loss: 2.9502
2022-03-06 09:59:39 - train: epoch 0043, iter [05000, 05004], lr: 0.091377, loss: 2.9396
2022-03-06 09:59:40 - train: epoch 043, train_loss: 3.0311
2022-03-06 10:00:54 - eval: epoch: 043, acc1: 52.796%, acc5: 78.364%, test_loss: 2.0266, per_image_load_time: 0.737ms, per_image_inference_time: 0.465ms
2022-03-06 10:00:55 - until epoch: 043, best_acc1: 52.796%
2022-03-06 10:00:55 - epoch 044 lr: 0.09091906224695935
2022-03-06 10:01:31 - train: epoch 0044, iter [00100, 05004], lr: 0.090919, loss: 2.9938
2022-03-06 10:02:05 - train: epoch 0044, iter [00200, 05004], lr: 0.090919, loss: 3.1784
2022-03-06 10:02:38 - train: epoch 0044, iter [00300, 05004], lr: 0.090919, loss: 2.9062
2022-03-06 10:03:10 - train: epoch 0044, iter [00400, 05004], lr: 0.090919, loss: 2.5997
2022-03-06 10:03:43 - train: epoch 0044, iter [00500, 05004], lr: 0.090919, loss: 2.9141
2022-03-06 10:04:17 - train: epoch 0044, iter [00600, 05004], lr: 0.090919, loss: 2.8018
2022-03-06 10:04:50 - train: epoch 0044, iter [00700, 05004], lr: 0.090919, loss: 2.4974
2022-03-06 10:05:22 - train: epoch 0044, iter [00800, 05004], lr: 0.090919, loss: 3.0299
2022-03-06 10:05:56 - train: epoch 0044, iter [00900, 05004], lr: 0.090919, loss: 2.9271
2022-03-06 10:06:30 - train: epoch 0044, iter [01000, 05004], lr: 0.090919, loss: 3.3251
2022-03-06 10:07:04 - train: epoch 0044, iter [01100, 05004], lr: 0.090919, loss: 2.8245
2022-03-06 10:07:37 - train: epoch 0044, iter [01200, 05004], lr: 0.090919, loss: 2.7717
2022-03-06 10:08:11 - train: epoch 0044, iter [01300, 05004], lr: 0.090919, loss: 2.8019
2022-03-06 10:08:45 - train: epoch 0044, iter [01400, 05004], lr: 0.090919, loss: 2.8314
2022-03-06 10:09:17 - train: epoch 0044, iter [01500, 05004], lr: 0.090919, loss: 3.1382
2022-03-06 10:09:50 - train: epoch 0044, iter [01600, 05004], lr: 0.090919, loss: 3.1264
2022-03-06 10:10:23 - train: epoch 0044, iter [01700, 05004], lr: 0.090919, loss: 3.1186
2022-03-06 10:10:56 - train: epoch 0044, iter [01800, 05004], lr: 0.090919, loss: 3.0382
2022-03-06 10:11:28 - train: epoch 0044, iter [01900, 05004], lr: 0.090919, loss: 3.1435
2022-03-06 10:12:01 - train: epoch 0044, iter [02000, 05004], lr: 0.090919, loss: 3.0213
2022-03-06 10:12:34 - train: epoch 0044, iter [02100, 05004], lr: 0.090919, loss: 3.3803
2022-03-06 10:13:07 - train: epoch 0044, iter [02200, 05004], lr: 0.090919, loss: 3.1565
2022-03-06 10:13:41 - train: epoch 0044, iter [02300, 05004], lr: 0.090919, loss: 3.0600
2022-03-06 10:14:14 - train: epoch 0044, iter [02400, 05004], lr: 0.090919, loss: 3.1752
2022-03-06 10:14:49 - train: epoch 0044, iter [02500, 05004], lr: 0.090919, loss: 3.1279
2022-03-06 10:15:23 - train: epoch 0044, iter [02600, 05004], lr: 0.090919, loss: 3.1281
2022-03-06 10:15:57 - train: epoch 0044, iter [02700, 05004], lr: 0.090919, loss: 3.2447
2022-03-06 10:16:30 - train: epoch 0044, iter [02800, 05004], lr: 0.090919, loss: 2.8622
2022-03-06 10:17:04 - train: epoch 0044, iter [02900, 05004], lr: 0.090919, loss: 2.8397
2022-03-06 10:17:36 - train: epoch 0044, iter [03000, 05004], lr: 0.090919, loss: 3.0594
2022-03-06 10:18:09 - train: epoch 0044, iter [03100, 05004], lr: 0.090919, loss: 2.8525
2022-03-06 10:18:41 - train: epoch 0044, iter [03200, 05004], lr: 0.090919, loss: 2.7091
2022-03-06 10:19:14 - train: epoch 0044, iter [03300, 05004], lr: 0.090919, loss: 2.8371
2022-03-06 10:19:46 - train: epoch 0044, iter [03400, 05004], lr: 0.090919, loss: 3.2850
2022-03-06 10:20:19 - train: epoch 0044, iter [03500, 05004], lr: 0.090919, loss: 3.1654
2022-03-06 10:20:52 - train: epoch 0044, iter [03600, 05004], lr: 0.090919, loss: 3.2882
2022-03-06 10:21:25 - train: epoch 0044, iter [03700, 05004], lr: 0.090919, loss: 3.0988
2022-03-06 10:21:58 - train: epoch 0044, iter [03800, 05004], lr: 0.090919, loss: 2.9532
2022-03-06 10:22:32 - train: epoch 0044, iter [03900, 05004], lr: 0.090919, loss: 3.2677
2022-03-06 10:23:05 - train: epoch 0044, iter [04000, 05004], lr: 0.090919, loss: 3.1614
2022-03-06 10:23:40 - train: epoch 0044, iter [04100, 05004], lr: 0.090919, loss: 3.0894
2022-03-06 10:24:13 - train: epoch 0044, iter [04200, 05004], lr: 0.090919, loss: 3.1960
2022-03-06 10:24:47 - train: epoch 0044, iter [04300, 05004], lr: 0.090919, loss: 3.3908
2022-03-06 10:25:19 - train: epoch 0044, iter [04400, 05004], lr: 0.090919, loss: 2.9546
2022-03-06 10:25:52 - train: epoch 0044, iter [04500, 05004], lr: 0.090919, loss: 3.2182
2022-03-06 10:26:24 - train: epoch 0044, iter [04600, 05004], lr: 0.090919, loss: 3.1147
2022-03-06 10:26:57 - train: epoch 0044, iter [04700, 05004], lr: 0.090919, loss: 3.1206
2022-03-06 10:27:29 - train: epoch 0044, iter [04800, 05004], lr: 0.090919, loss: 3.1459
2022-03-06 10:28:02 - train: epoch 0044, iter [04900, 05004], lr: 0.090919, loss: 2.9096
2022-03-06 10:28:33 - train: epoch 0044, iter [05000, 05004], lr: 0.090919, loss: 3.4200
2022-03-06 10:28:34 - train: epoch 044, train_loss: 3.0301
2022-03-06 10:29:47 - eval: epoch: 044, acc1: 53.124%, acc5: 78.652%, test_loss: 2.0073, per_image_load_time: 2.378ms, per_image_inference_time: 0.490ms
2022-03-06 10:29:48 - until epoch: 044, best_acc1: 53.124%
2022-03-06 10:29:48 - epoch 045 lr: 0.09045084971874738
2022-03-06 10:30:27 - train: epoch 0045, iter [00100, 05004], lr: 0.090451, loss: 2.8690
2022-03-06 10:31:00 - train: epoch 0045, iter [00200, 05004], lr: 0.090451, loss: 3.1221
2022-03-06 10:31:34 - train: epoch 0045, iter [00300, 05004], lr: 0.090451, loss: 2.8968
2022-03-06 10:32:08 - train: epoch 0045, iter [00400, 05004], lr: 0.090451, loss: 2.6952
2022-03-06 10:32:41 - train: epoch 0045, iter [00500, 05004], lr: 0.090451, loss: 3.3755
2022-03-06 10:33:13 - train: epoch 0045, iter [00600, 05004], lr: 0.090451, loss: 3.0664
2022-03-06 10:33:45 - train: epoch 0045, iter [00700, 05004], lr: 0.090451, loss: 2.7982
2022-03-06 10:34:17 - train: epoch 0045, iter [00800, 05004], lr: 0.090451, loss: 2.6831
2022-03-06 10:34:50 - train: epoch 0045, iter [00900, 05004], lr: 0.090451, loss: 3.1983
2022-03-06 10:35:23 - train: epoch 0045, iter [01000, 05004], lr: 0.090451, loss: 2.7642
2022-03-06 10:35:56 - train: epoch 0045, iter [01100, 05004], lr: 0.090451, loss: 3.0809
2022-03-06 10:36:28 - train: epoch 0045, iter [01200, 05004], lr: 0.090451, loss: 3.0278
2022-03-06 10:37:01 - train: epoch 0045, iter [01300, 05004], lr: 0.090451, loss: 3.3456
2022-03-06 10:37:35 - train: epoch 0045, iter [01400, 05004], lr: 0.090451, loss: 3.0246
2022-03-06 10:38:09 - train: epoch 0045, iter [01500, 05004], lr: 0.090451, loss: 3.3041
2022-03-06 10:38:42 - train: epoch 0045, iter [01600, 05004], lr: 0.090451, loss: 2.7000
2022-03-06 10:39:17 - train: epoch 0045, iter [01700, 05004], lr: 0.090451, loss: 3.0021
2022-03-06 10:39:50 - train: epoch 0045, iter [01800, 05004], lr: 0.090451, loss: 2.8879
2022-03-06 10:40:23 - train: epoch 0045, iter [01900, 05004], lr: 0.090451, loss: 3.1939
2022-03-06 10:40:56 - train: epoch 0045, iter [02000, 05004], lr: 0.090451, loss: 3.1913
2022-03-06 10:41:29 - train: epoch 0045, iter [02100, 05004], lr: 0.090451, loss: 3.4269
2022-03-06 10:42:01 - train: epoch 0045, iter [02200, 05004], lr: 0.090451, loss: 3.0425
2022-03-06 10:42:34 - train: epoch 0045, iter [02300, 05004], lr: 0.090451, loss: 3.0081
2022-03-06 10:43:06 - train: epoch 0045, iter [02400, 05004], lr: 0.090451, loss: 3.0668
2022-03-06 10:43:38 - train: epoch 0045, iter [02500, 05004], lr: 0.090451, loss: 2.8662
2022-03-06 10:44:12 - train: epoch 0045, iter [02600, 05004], lr: 0.090451, loss: 3.3061
2022-03-06 10:44:44 - train: epoch 0045, iter [02700, 05004], lr: 0.090451, loss: 2.9782
2022-03-06 10:45:18 - train: epoch 0045, iter [02800, 05004], lr: 0.090451, loss: 2.9052
2022-03-06 10:45:51 - train: epoch 0045, iter [02900, 05004], lr: 0.090451, loss: 3.3227
2022-03-06 10:46:26 - train: epoch 0045, iter [03000, 05004], lr: 0.090451, loss: 3.3628
2022-03-06 10:46:59 - train: epoch 0045, iter [03100, 05004], lr: 0.090451, loss: 3.0736
2022-03-06 10:47:33 - train: epoch 0045, iter [03200, 05004], lr: 0.090451, loss: 3.0205
2022-03-06 10:48:06 - train: epoch 0045, iter [03300, 05004], lr: 0.090451, loss: 3.0384
2022-03-06 10:48:39 - train: epoch 0045, iter [03400, 05004], lr: 0.090451, loss: 2.8599
2022-03-06 10:49:12 - train: epoch 0045, iter [03500, 05004], lr: 0.090451, loss: 3.0769
2022-03-06 10:49:44 - train: epoch 0045, iter [03600, 05004], lr: 0.090451, loss: 2.9476
2022-03-06 10:50:17 - train: epoch 0045, iter [03700, 05004], lr: 0.090451, loss: 3.0803
2022-03-06 10:50:49 - train: epoch 0045, iter [03800, 05004], lr: 0.090451, loss: 2.9156
2022-03-06 10:51:21 - train: epoch 0045, iter [03900, 05004], lr: 0.090451, loss: 3.1397
2022-03-06 10:51:55 - train: epoch 0045, iter [04000, 05004], lr: 0.090451, loss: 3.0302
2022-03-06 10:52:28 - train: epoch 0045, iter [04100, 05004], lr: 0.090451, loss: 3.0898
2022-03-06 10:53:01 - train: epoch 0045, iter [04200, 05004], lr: 0.090451, loss: 3.2400
2022-03-06 10:53:34 - train: epoch 0045, iter [04300, 05004], lr: 0.090451, loss: 3.1076
2022-03-06 10:54:08 - train: epoch 0045, iter [04400, 05004], lr: 0.090451, loss: 3.3383
2022-03-06 10:54:42 - train: epoch 0045, iter [04500, 05004], lr: 0.090451, loss: 2.9079
2022-03-06 10:55:16 - train: epoch 0045, iter [04600, 05004], lr: 0.090451, loss: 3.2365
2022-03-06 10:55:50 - train: epoch 0045, iter [04700, 05004], lr: 0.090451, loss: 2.9696
2022-03-06 10:56:22 - train: epoch 0045, iter [04800, 05004], lr: 0.090451, loss: 3.1429
2022-03-06 10:56:55 - train: epoch 0045, iter [04900, 05004], lr: 0.090451, loss: 2.9947
2022-03-06 10:57:26 - train: epoch 0045, iter [05000, 05004], lr: 0.090451, loss: 3.1152
2022-03-06 10:57:27 - train: epoch 045, train_loss: 3.0255
2022-03-06 10:58:39 - eval: epoch: 045, acc1: 52.586%, acc5: 78.454%, test_loss: 2.0257, per_image_load_time: 2.248ms, per_image_inference_time: 0.473ms
2022-03-06 10:58:40 - until epoch: 045, best_acc1: 53.124%
2022-03-06 10:58:40 - epoch 046 lr: 0.08997213817017508
2022-03-06 10:59:18 - train: epoch 0046, iter [00100, 05004], lr: 0.089972, loss: 2.5760
2022-03-06 10:59:51 - train: epoch 0046, iter [00200, 05004], lr: 0.089972, loss: 2.9076
2022-03-06 11:00:23 - train: epoch 0046, iter [00300, 05004], lr: 0.089972, loss: 2.7905
2022-03-06 11:00:57 - train: epoch 0046, iter [00400, 05004], lr: 0.089972, loss: 3.2171
2022-03-06 11:01:30 - train: epoch 0046, iter [00500, 05004], lr: 0.089972, loss: 2.9234
2022-03-06 11:02:03 - train: epoch 0046, iter [00600, 05004], lr: 0.089972, loss: 3.2392
2022-03-06 11:02:37 - train: epoch 0046, iter [00700, 05004], lr: 0.089972, loss: 2.8870
2022-03-06 11:03:11 - train: epoch 0046, iter [00800, 05004], lr: 0.089972, loss: 3.0346
2022-03-06 11:03:45 - train: epoch 0046, iter [00900, 05004], lr: 0.089972, loss: 3.0584
2022-03-06 11:04:17 - train: epoch 0046, iter [01000, 05004], lr: 0.089972, loss: 3.0175
2022-03-06 11:04:49 - train: epoch 0046, iter [01100, 05004], lr: 0.089972, loss: 2.9718
2022-03-06 11:05:22 - train: epoch 0046, iter [01200, 05004], lr: 0.089972, loss: 2.9870
2022-03-06 11:05:55 - train: epoch 0046, iter [01300, 05004], lr: 0.089972, loss: 2.9345
2022-03-06 11:06:28 - train: epoch 0046, iter [01400, 05004], lr: 0.089972, loss: 3.2812
2022-03-06 11:07:01 - train: epoch 0046, iter [01500, 05004], lr: 0.089972, loss: 2.9711
2022-03-06 11:07:34 - train: epoch 0046, iter [01600, 05004], lr: 0.089972, loss: 3.1798
2022-03-06 11:08:06 - train: epoch 0046, iter [01700, 05004], lr: 0.089972, loss: 2.9553
2022-03-06 11:08:39 - train: epoch 0046, iter [01800, 05004], lr: 0.089972, loss: 3.0369
2022-03-06 11:09:13 - train: epoch 0046, iter [01900, 05004], lr: 0.089972, loss: 2.7119
2022-03-06 11:09:47 - train: epoch 0046, iter [02000, 05004], lr: 0.089972, loss: 2.9469
2022-03-06 11:10:21 - train: epoch 0046, iter [02100, 05004], lr: 0.089972, loss: 3.1778
2022-03-06 11:10:54 - train: epoch 0046, iter [02200, 05004], lr: 0.089972, loss: 2.8445
2022-03-06 11:11:28 - train: epoch 0046, iter [02300, 05004], lr: 0.089972, loss: 3.1539
2022-03-06 11:12:01 - train: epoch 0046, iter [02400, 05004], lr: 0.089972, loss: 2.9886
2022-03-06 11:12:33 - train: epoch 0046, iter [02500, 05004], lr: 0.089972, loss: 2.9636
2022-03-06 11:13:06 - train: epoch 0046, iter [02600, 05004], lr: 0.089972, loss: 3.2128
2022-03-06 11:13:39 - train: epoch 0046, iter [02700, 05004], lr: 0.089972, loss: 2.6299
2022-03-06 11:14:11 - train: epoch 0046, iter [02800, 05004], lr: 0.089972, loss: 2.9596
2022-03-06 11:14:44 - train: epoch 0046, iter [02900, 05004], lr: 0.089972, loss: 3.0340
2022-03-06 11:15:17 - train: epoch 0046, iter [03000, 05004], lr: 0.089972, loss: 2.9074
2022-03-06 11:15:50 - train: epoch 0046, iter [03100, 05004], lr: 0.089972, loss: 2.9888
2022-03-06 11:16:22 - train: epoch 0046, iter [03200, 05004], lr: 0.089972, loss: 3.1387
2022-03-06 11:16:56 - train: epoch 0046, iter [03300, 05004], lr: 0.089972, loss: 3.0098
2022-03-06 11:17:30 - train: epoch 0046, iter [03400, 05004], lr: 0.089972, loss: 3.3738
2022-03-06 11:18:04 - train: epoch 0046, iter [03500, 05004], lr: 0.089972, loss: 2.9055
2022-03-06 11:18:38 - train: epoch 0046, iter [03600, 05004], lr: 0.089972, loss: 2.7200
2022-03-06 11:19:11 - train: epoch 0046, iter [03700, 05004], lr: 0.089972, loss: 2.8098
2022-03-06 11:19:44 - train: epoch 0046, iter [03800, 05004], lr: 0.089972, loss: 3.1656
2022-03-06 11:20:17 - train: epoch 0046, iter [03900, 05004], lr: 0.089972, loss: 2.7296
2022-03-06 11:20:49 - train: epoch 0046, iter [04000, 05004], lr: 0.089972, loss: 2.9475
2022-03-06 11:21:22 - train: epoch 0046, iter [04100, 05004], lr: 0.089972, loss: 3.2790
2022-03-06 11:21:55 - train: epoch 0046, iter [04200, 05004], lr: 0.089972, loss: 3.0730
2022-03-06 11:22:28 - train: epoch 0046, iter [04300, 05004], lr: 0.089972, loss: 3.0245
2022-03-06 11:23:01 - train: epoch 0046, iter [04400, 05004], lr: 0.089972, loss: 3.0510
2022-03-06 11:23:32 - train: epoch 0046, iter [04500, 05004], lr: 0.089972, loss: 3.0492
2022-03-06 11:24:05 - train: epoch 0046, iter [04600, 05004], lr: 0.089972, loss: 2.9565
2022-03-06 11:24:39 - train: epoch 0046, iter [04700, 05004], lr: 0.089972, loss: 2.9281
2022-03-06 11:25:13 - train: epoch 0046, iter [04800, 05004], lr: 0.089972, loss: 2.9605
2022-03-06 11:25:46 - train: epoch 0046, iter [04900, 05004], lr: 0.089972, loss: 3.2044
2022-03-06 11:26:19 - train: epoch 0046, iter [05000, 05004], lr: 0.089972, loss: 3.1288
2022-03-06 11:26:20 - train: epoch 046, train_loss: 3.0197
2022-03-06 11:27:33 - eval: epoch: 046, acc1: 52.410%, acc5: 78.428%, test_loss: 2.0298, per_image_load_time: 1.481ms, per_image_inference_time: 0.516ms
2022-03-06 11:27:34 - until epoch: 046, best_acc1: 53.124%
2022-03-06 11:27:34 - epoch 047 lr: 0.08948305185085226
2022-03-06 11:28:12 - train: epoch 0047, iter [00100, 05004], lr: 0.089483, loss: 2.6907
2022-03-06 11:28:45 - train: epoch 0047, iter [00200, 05004], lr: 0.089483, loss: 2.9553
2022-03-06 11:29:18 - train: epoch 0047, iter [00300, 05004], lr: 0.089483, loss: 2.8799
2022-03-06 11:29:50 - train: epoch 0047, iter [00400, 05004], lr: 0.089483, loss: 2.7916
2022-03-06 11:30:22 - train: epoch 0047, iter [00500, 05004], lr: 0.089483, loss: 2.8457
2022-03-06 11:30:55 - train: epoch 0047, iter [00600, 05004], lr: 0.089483, loss: 3.3138
2022-03-06 11:31:27 - train: epoch 0047, iter [00700, 05004], lr: 0.089483, loss: 3.0861
2022-03-06 11:32:00 - train: epoch 0047, iter [00800, 05004], lr: 0.089483, loss: 2.8298
2022-03-06 11:32:31 - train: epoch 0047, iter [00900, 05004], lr: 0.089483, loss: 3.0266
2022-03-06 11:33:06 - train: epoch 0047, iter [01000, 05004], lr: 0.089483, loss: 2.8945
2022-03-06 11:33:40 - train: epoch 0047, iter [01100, 05004], lr: 0.089483, loss: 3.2718
2022-03-06 11:34:14 - train: epoch 0047, iter [01200, 05004], lr: 0.089483, loss: 2.8341
2022-03-06 11:34:48 - train: epoch 0047, iter [01300, 05004], lr: 0.089483, loss: 3.0577
2022-03-06 11:35:21 - train: epoch 0047, iter [01400, 05004], lr: 0.089483, loss: 3.1680
2022-03-06 11:35:54 - train: epoch 0047, iter [01500, 05004], lr: 0.089483, loss: 2.6513
2022-03-06 11:36:26 - train: epoch 0047, iter [01600, 05004], lr: 0.089483, loss: 2.8958
2022-03-06 11:36:59 - train: epoch 0047, iter [01700, 05004], lr: 0.089483, loss: 2.8521
2022-03-06 11:37:31 - train: epoch 0047, iter [01800, 05004], lr: 0.089483, loss: 3.1454
2022-03-06 11:38:05 - train: epoch 0047, iter [01900, 05004], lr: 0.089483, loss: 2.9262
2022-03-06 11:38:37 - train: epoch 0047, iter [02000, 05004], lr: 0.089483, loss: 3.0397
2022-03-06 11:39:10 - train: epoch 0047, iter [02100, 05004], lr: 0.089483, loss: 3.0839
2022-03-06 11:39:43 - train: epoch 0047, iter [02200, 05004], lr: 0.089483, loss: 3.1370
2022-03-06 11:40:15 - train: epoch 0047, iter [02300, 05004], lr: 0.089483, loss: 2.9200
2022-03-06 11:40:50 - train: epoch 0047, iter [02400, 05004], lr: 0.089483, loss: 3.0205
2022-03-06 11:41:23 - train: epoch 0047, iter [02500, 05004], lr: 0.089483, loss: 3.1065
2022-03-06 11:41:57 - train: epoch 0047, iter [02600, 05004], lr: 0.089483, loss: 3.0275
2022-03-06 11:42:31 - train: epoch 0047, iter [02700, 05004], lr: 0.089483, loss: 2.9854
2022-03-06 11:43:04 - train: epoch 0047, iter [02800, 05004], lr: 0.089483, loss: 2.9624
2022-03-06 11:43:38 - train: epoch 0047, iter [02900, 05004], lr: 0.089483, loss: 2.8920
2022-03-06 11:44:10 - train: epoch 0047, iter [03000, 05004], lr: 0.089483, loss: 3.0935
2022-03-06 11:44:42 - train: epoch 0047, iter [03100, 05004], lr: 0.089483, loss: 3.0920
2022-03-06 11:45:16 - train: epoch 0047, iter [03200, 05004], lr: 0.089483, loss: 3.4323
2022-03-06 11:45:48 - train: epoch 0047, iter [03300, 05004], lr: 0.089483, loss: 3.0662
2022-03-06 11:46:21 - train: epoch 0047, iter [03400, 05004], lr: 0.089483, loss: 3.0461
2022-03-06 11:46:53 - train: epoch 0047, iter [03500, 05004], lr: 0.089483, loss: 3.2120
2022-03-06 11:47:26 - train: epoch 0047, iter [03600, 05004], lr: 0.089483, loss: 2.9528
2022-03-06 11:47:58 - train: epoch 0047, iter [03700, 05004], lr: 0.089483, loss: 3.2816
2022-03-06 11:48:32 - train: epoch 0047, iter [03800, 05004], lr: 0.089483, loss: 3.2502
2022-03-06 11:49:06 - train: epoch 0047, iter [03900, 05004], lr: 0.089483, loss: 2.8875
2022-03-06 11:49:40 - train: epoch 0047, iter [04000, 05004], lr: 0.089483, loss: 3.0918
2022-03-06 11:50:14 - train: epoch 0047, iter [04100, 05004], lr: 0.089483, loss: 3.1151
2022-03-06 11:50:48 - train: epoch 0047, iter [04200, 05004], lr: 0.089483, loss: 3.0257
2022-03-06 11:51:21 - train: epoch 0047, iter [04300, 05004], lr: 0.089483, loss: 2.9625
2022-03-06 11:51:54 - train: epoch 0047, iter [04400, 05004], lr: 0.089483, loss: 2.8624
2022-03-06 11:52:26 - train: epoch 0047, iter [04500, 05004], lr: 0.089483, loss: 3.1638
2022-03-06 11:52:58 - train: epoch 0047, iter [04600, 05004], lr: 0.089483, loss: 3.0159
2022-03-06 11:53:31 - train: epoch 0047, iter [04700, 05004], lr: 0.089483, loss: 2.7832
2022-03-06 11:54:03 - train: epoch 0047, iter [04800, 05004], lr: 0.089483, loss: 2.9164
2022-03-06 11:54:36 - train: epoch 0047, iter [04900, 05004], lr: 0.089483, loss: 3.1377
2022-03-06 11:55:07 - train: epoch 0047, iter [05000, 05004], lr: 0.089483, loss: 2.8239
2022-03-06 11:55:09 - train: epoch 047, train_loss: 3.0140
2022-03-06 11:56:21 - eval: epoch: 047, acc1: 52.112%, acc5: 78.098%, test_loss: 2.0462, per_image_load_time: 1.283ms, per_image_inference_time: 0.496ms
2022-03-06 11:56:21 - until epoch: 047, best_acc1: 53.124%
2022-03-06 11:56:21 - epoch 048 lr: 0.08898371770316112
2022-03-06 11:57:01 - train: epoch 0048, iter [00100, 05004], lr: 0.088984, loss: 3.1212
2022-03-06 11:57:35 - train: epoch 0048, iter [00200, 05004], lr: 0.088984, loss: 3.3575
2022-03-06 11:58:09 - train: epoch 0048, iter [00300, 05004], lr: 0.088984, loss: 2.9172
2022-03-06 11:58:40 - train: epoch 0048, iter [00400, 05004], lr: 0.088984, loss: 3.0447
2022-03-06 11:59:14 - train: epoch 0048, iter [00500, 05004], lr: 0.088984, loss: 2.8363
2022-03-06 11:59:47 - train: epoch 0048, iter [00600, 05004], lr: 0.088984, loss: 2.8138
2022-03-06 12:00:19 - train: epoch 0048, iter [00700, 05004], lr: 0.088984, loss: 3.0901
2022-03-06 12:00:52 - train: epoch 0048, iter [00800, 05004], lr: 0.088984, loss: 2.8388
2022-03-06 12:01:25 - train: epoch 0048, iter [00900, 05004], lr: 0.088984, loss: 3.0214
2022-03-06 12:01:58 - train: epoch 0048, iter [01000, 05004], lr: 0.088984, loss: 3.1520
2022-03-06 12:02:30 - train: epoch 0048, iter [01100, 05004], lr: 0.088984, loss: 2.9874
2022-03-06 12:03:03 - train: epoch 0048, iter [01200, 05004], lr: 0.088984, loss: 3.1581
2022-03-06 12:03:35 - train: epoch 0048, iter [01300, 05004], lr: 0.088984, loss: 2.8549
2022-03-06 12:04:10 - train: epoch 0048, iter [01400, 05004], lr: 0.088984, loss: 2.8569
2022-03-06 12:04:43 - train: epoch 0048, iter [01500, 05004], lr: 0.088984, loss: 3.2046
2022-03-06 12:05:17 - train: epoch 0048, iter [01600, 05004], lr: 0.088984, loss: 2.8785
2022-03-06 12:05:51 - train: epoch 0048, iter [01700, 05004], lr: 0.088984, loss: 3.1396
2022-03-06 12:06:25 - train: epoch 0048, iter [01800, 05004], lr: 0.088984, loss: 2.8703
2022-03-06 12:06:58 - train: epoch 0048, iter [01900, 05004], lr: 0.088984, loss: 3.2726
2022-03-06 12:07:31 - train: epoch 0048, iter [02000, 05004], lr: 0.088984, loss: 3.2802
2022-03-06 12:08:04 - train: epoch 0048, iter [02100, 05004], lr: 0.088984, loss: 3.0256
2022-03-06 12:08:36 - train: epoch 0048, iter [02200, 05004], lr: 0.088984, loss: 3.0464
2022-03-06 12:09:08 - train: epoch 0048, iter [02300, 05004], lr: 0.088984, loss: 2.8604
2022-03-06 12:09:41 - train: epoch 0048, iter [02400, 05004], lr: 0.088984, loss: 3.0889
2022-03-06 12:10:14 - train: epoch 0048, iter [02500, 05004], lr: 0.088984, loss: 3.0603
2022-03-06 12:10:47 - train: epoch 0048, iter [02600, 05004], lr: 0.088984, loss: 3.1883
2022-03-06 12:11:19 - train: epoch 0048, iter [02700, 05004], lr: 0.088984, loss: 3.1046
2022-03-06 12:11:52 - train: epoch 0048, iter [02800, 05004], lr: 0.088984, loss: 3.3336
2022-03-06 12:12:26 - train: epoch 0048, iter [02900, 05004], lr: 0.088984, loss: 3.0391
2022-03-06 12:12:59 - train: epoch 0048, iter [03000, 05004], lr: 0.088984, loss: 2.8633
2022-03-06 12:13:33 - train: epoch 0048, iter [03100, 05004], lr: 0.088984, loss: 3.0852
2022-03-06 12:14:07 - train: epoch 0048, iter [03200, 05004], lr: 0.088984, loss: 2.7370
2022-03-06 12:14:40 - train: epoch 0048, iter [03300, 05004], lr: 0.088984, loss: 3.0295
2022-03-06 12:15:13 - train: epoch 0048, iter [03400, 05004], lr: 0.088984, loss: 3.1365
2022-03-06 12:15:45 - train: epoch 0048, iter [03500, 05004], lr: 0.088984, loss: 3.1036
2022-03-06 12:16:18 - train: epoch 0048, iter [03600, 05004], lr: 0.088984, loss: 3.0337
2022-03-06 12:16:50 - train: epoch 0048, iter [03700, 05004], lr: 0.088984, loss: 3.2312
2022-03-06 12:17:23 - train: epoch 0048, iter [03800, 05004], lr: 0.088984, loss: 3.1145
2022-03-06 12:17:56 - train: epoch 0048, iter [03900, 05004], lr: 0.088984, loss: 2.8453
2022-03-06 12:18:29 - train: epoch 0048, iter [04000, 05004], lr: 0.088984, loss: 3.0392
2022-03-06 12:19:02 - train: epoch 0048, iter [04100, 05004], lr: 0.088984, loss: 3.3613
2022-03-06 12:19:34 - train: epoch 0048, iter [04200, 05004], lr: 0.088984, loss: 3.3239
2022-03-06 12:20:07 - train: epoch 0048, iter [04300, 05004], lr: 0.088984, loss: 2.9931
2022-03-06 12:20:41 - train: epoch 0048, iter [04400, 05004], lr: 0.088984, loss: 3.0066
2022-03-06 12:21:15 - train: epoch 0048, iter [04500, 05004], lr: 0.088984, loss: 3.0640
2022-03-06 12:21:49 - train: epoch 0048, iter [04600, 05004], lr: 0.088984, loss: 3.2403
2022-03-06 12:22:22 - train: epoch 0048, iter [04700, 05004], lr: 0.088984, loss: 3.0872
2022-03-06 12:22:55 - train: epoch 0048, iter [04800, 05004], lr: 0.088984, loss: 3.3708
2022-03-06 12:23:28 - train: epoch 0048, iter [04900, 05004], lr: 0.088984, loss: 3.2338
2022-03-06 12:23:59 - train: epoch 0048, iter [05000, 05004], lr: 0.088984, loss: 2.9805
2022-03-06 12:24:00 - train: epoch 048, train_loss: 3.0163
2022-03-06 12:25:11 - eval: epoch: 048, acc1: 53.300%, acc5: 79.150%, test_loss: 1.9752, per_image_load_time: 2.161ms, per_image_inference_time: 0.479ms
2022-03-06 12:25:12 - until epoch: 048, best_acc1: 53.300%
2022-03-06 12:25:12 - epoch 049 lr: 0.0884742653293083
2022-03-06 12:25:51 - train: epoch 0049, iter [00100, 05004], lr: 0.088474, loss: 3.0904
2022-03-06 12:26:24 - train: epoch 0049, iter [00200, 05004], lr: 0.088474, loss: 3.1243
2022-03-06 12:26:56 - train: epoch 0049, iter [00300, 05004], lr: 0.088474, loss: 3.0584
2022-03-06 12:27:28 - train: epoch 0049, iter [00400, 05004], lr: 0.088474, loss: 3.2035
2022-03-06 12:28:01 - train: epoch 0049, iter [00500, 05004], lr: 0.088474, loss: 3.0778
2022-03-06 12:28:35 - train: epoch 0049, iter [00600, 05004], lr: 0.088474, loss: 3.0795
2022-03-06 12:29:09 - train: epoch 0049, iter [00700, 05004], lr: 0.088474, loss: 3.0586
2022-03-06 12:29:43 - train: epoch 0049, iter [00800, 05004], lr: 0.088474, loss: 3.2994
2022-03-06 12:30:17 - train: epoch 0049, iter [00900, 05004], lr: 0.088474, loss: 2.9609
2022-03-06 12:30:49 - train: epoch 0049, iter [01000, 05004], lr: 0.088474, loss: 3.1357
2022-03-06 12:31:22 - train: epoch 0049, iter [01100, 05004], lr: 0.088474, loss: 3.0362
2022-03-06 12:31:55 - train: epoch 0049, iter [01200, 05004], lr: 0.088474, loss: 2.4987
2022-03-06 12:32:27 - train: epoch 0049, iter [01300, 05004], lr: 0.088474, loss: 3.1928
2022-03-06 12:33:00 - train: epoch 0049, iter [01400, 05004], lr: 0.088474, loss: 3.0145
2022-03-06 12:33:33 - train: epoch 0049, iter [01500, 05004], lr: 0.088474, loss: 2.8646
2022-03-06 12:34:05 - train: epoch 0049, iter [01600, 05004], lr: 0.088474, loss: 3.0649
2022-03-06 12:34:39 - train: epoch 0049, iter [01700, 05004], lr: 0.088474, loss: 2.9132
2022-03-06 12:35:11 - train: epoch 0049, iter [01800, 05004], lr: 0.088474, loss: 2.7683
2022-03-06 12:35:45 - train: epoch 0049, iter [01900, 05004], lr: 0.088474, loss: 2.9129
2022-03-06 12:36:19 - train: epoch 0049, iter [02000, 05004], lr: 0.088474, loss: 2.8997
2022-03-06 12:36:52 - train: epoch 0049, iter [02100, 05004], lr: 0.088474, loss: 2.8946
2022-03-06 12:37:27 - train: epoch 0049, iter [02200, 05004], lr: 0.088474, loss: 3.1166
2022-03-06 12:38:00 - train: epoch 0049, iter [02300, 05004], lr: 0.088474, loss: 2.9192
2022-03-06 12:38:34 - train: epoch 0049, iter [02400, 05004], lr: 0.088474, loss: 3.1042
2022-03-06 12:39:06 - train: epoch 0049, iter [02500, 05004], lr: 0.088474, loss: 3.0204
2022-03-06 12:39:39 - train: epoch 0049, iter [02600, 05004], lr: 0.088474, loss: 3.1518
2022-03-06 12:40:12 - train: epoch 0049, iter [02700, 05004], lr: 0.088474, loss: 2.7984
2022-03-06 12:40:45 - train: epoch 0049, iter [02800, 05004], lr: 0.088474, loss: 3.2122
2022-03-06 12:41:17 - train: epoch 0049, iter [02900, 05004], lr: 0.088474, loss: 2.9023
2022-03-06 12:41:50 - train: epoch 0049, iter [03000, 05004], lr: 0.088474, loss: 3.1609
2022-03-06 12:42:23 - train: epoch 0049, iter [03100, 05004], lr: 0.088474, loss: 3.2638
2022-03-06 12:42:56 - train: epoch 0049, iter [03200, 05004], lr: 0.088474, loss: 3.3171
2022-03-06 12:43:29 - train: epoch 0049, iter [03300, 05004], lr: 0.088474, loss: 3.0714
2022-03-06 12:44:03 - train: epoch 0049, iter [03400, 05004], lr: 0.088474, loss: 3.3819
2022-03-06 12:44:37 - train: epoch 0049, iter [03500, 05004], lr: 0.088474, loss: 3.1809
2022-03-06 12:45:11 - train: epoch 0049, iter [03600, 05004], lr: 0.088474, loss: 3.2102
2022-03-06 12:45:44 - train: epoch 0049, iter [03700, 05004], lr: 0.088474, loss: 3.0132
2022-03-06 12:46:18 - train: epoch 0049, iter [03800, 05004], lr: 0.088474, loss: 2.9840
2022-03-06 12:46:51 - train: epoch 0049, iter [03900, 05004], lr: 0.088474, loss: 3.4278
2022-03-06 12:47:23 - train: epoch 0049, iter [04000, 05004], lr: 0.088474, loss: 3.0132
2022-03-06 12:47:56 - train: epoch 0049, iter [04100, 05004], lr: 0.088474, loss: 2.8048
2022-03-06 12:48:28 - train: epoch 0049, iter [04200, 05004], lr: 0.088474, loss: 3.0641
2022-03-06 12:49:01 - train: epoch 0049, iter [04300, 05004], lr: 0.088474, loss: 3.2987
2022-03-06 12:49:34 - train: epoch 0049, iter [04400, 05004], lr: 0.088474, loss: 2.9594
2022-03-06 12:50:06 - train: epoch 0049, iter [04500, 05004], lr: 0.088474, loss: 2.9808
2022-03-06 12:50:38 - train: epoch 0049, iter [04600, 05004], lr: 0.088474, loss: 2.9648
2022-03-06 12:51:11 - train: epoch 0049, iter [04700, 05004], lr: 0.088474, loss: 3.0116
2022-03-06 12:51:46 - train: epoch 0049, iter [04800, 05004], lr: 0.088474, loss: 2.8499
2022-03-06 12:52:19 - train: epoch 0049, iter [04900, 05004], lr: 0.088474, loss: 2.7798
2022-03-06 12:52:51 - train: epoch 0049, iter [05000, 05004], lr: 0.088474, loss: 2.9567
2022-03-06 12:52:52 - train: epoch 049, train_loss: 3.0072
2022-03-06 12:54:06 - eval: epoch: 049, acc1: 53.588%, acc5: 79.114%, test_loss: 1.9735, per_image_load_time: 2.028ms, per_image_inference_time: 0.507ms
2022-03-06 12:54:07 - until epoch: 049, best_acc1: 53.588%
2022-03-06 12:54:07 - epoch 050 lr: 0.08795482695768658
2022-03-06 12:54:44 - train: epoch 0050, iter [00100, 05004], lr: 0.087955, loss: 3.2480
2022-03-06 12:55:17 - train: epoch 0050, iter [00200, 05004], lr: 0.087955, loss: 2.9016
2022-03-06 12:55:50 - train: epoch 0050, iter [00300, 05004], lr: 0.087955, loss: 3.2555
2022-03-06 12:56:21 - train: epoch 0050, iter [00400, 05004], lr: 0.087955, loss: 2.6740
2022-03-06 12:56:54 - train: epoch 0050, iter [00500, 05004], lr: 0.087955, loss: 3.1141
2022-03-06 12:57:27 - train: epoch 0050, iter [00600, 05004], lr: 0.087955, loss: 3.0896
2022-03-06 12:58:00 - train: epoch 0050, iter [00700, 05004], lr: 0.087955, loss: 2.7689
2022-03-06 12:58:32 - train: epoch 0050, iter [00800, 05004], lr: 0.087955, loss: 2.4869
2022-03-06 12:59:06 - train: epoch 0050, iter [00900, 05004], lr: 0.087955, loss: 2.7473
2022-03-06 12:59:40 - train: epoch 0050, iter [01000, 05004], lr: 0.087955, loss: 3.2882
2022-03-06 13:00:13 - train: epoch 0050, iter [01100, 05004], lr: 0.087955, loss: 3.3171
2022-03-06 13:00:47 - train: epoch 0050, iter [01200, 05004], lr: 0.087955, loss: 3.0036
2022-03-06 13:01:21 - train: epoch 0050, iter [01300, 05004], lr: 0.087955, loss: 2.6639
2022-03-06 13:01:55 - train: epoch 0050, iter [01400, 05004], lr: 0.087955, loss: 3.0494
2022-03-06 13:02:28 - train: epoch 0050, iter [01500, 05004], lr: 0.087955, loss: 2.8622
2022-03-06 13:03:00 - train: epoch 0050, iter [01600, 05004], lr: 0.087955, loss: 2.9231
2022-03-06 13:03:33 - train: epoch 0050, iter [01700, 05004], lr: 0.087955, loss: 3.0167
2022-03-06 13:04:05 - train: epoch 0050, iter [01800, 05004], lr: 0.087955, loss: 3.1300
2022-03-06 13:04:38 - train: epoch 0050, iter [01900, 05004], lr: 0.087955, loss: 2.8633
2022-03-06 13:05:10 - train: epoch 0050, iter [02000, 05004], lr: 0.087955, loss: 3.1111
2022-03-06 13:05:43 - train: epoch 0050, iter [02100, 05004], lr: 0.087955, loss: 2.7785
2022-03-06 13:06:15 - train: epoch 0050, iter [02200, 05004], lr: 0.087955, loss: 3.2180
2022-03-06 13:06:48 - train: epoch 0050, iter [02300, 05004], lr: 0.087955, loss: 2.8874
2022-03-06 13:07:21 - train: epoch 0050, iter [02400, 05004], lr: 0.087955, loss: 3.1601
2022-03-06 13:07:56 - train: epoch 0050, iter [02500, 05004], lr: 0.087955, loss: 3.1841
2022-03-06 13:08:28 - train: epoch 0050, iter [02600, 05004], lr: 0.087955, loss: 2.7310
2022-03-06 13:09:03 - train: epoch 0050, iter [02700, 05004], lr: 0.087955, loss: 3.0264
2022-03-06 13:09:36 - train: epoch 0050, iter [02800, 05004], lr: 0.087955, loss: 3.0834
2022-03-06 13:10:09 - train: epoch 0050, iter [02900, 05004], lr: 0.087955, loss: 3.1332
2022-03-06 13:10:42 - train: epoch 0050, iter [03000, 05004], lr: 0.087955, loss: 3.2691
2022-03-06 13:11:15 - train: epoch 0050, iter [03100, 05004], lr: 0.087955, loss: 3.0136
2022-03-06 13:11:47 - train: epoch 0050, iter [03200, 05004], lr: 0.087955, loss: 3.0654
2022-03-06 13:12:20 - train: epoch 0050, iter [03300, 05004], lr: 0.087955, loss: 2.8713
2022-03-06 13:12:52 - train: epoch 0050, iter [03400, 05004], lr: 0.087955, loss: 2.9455
2022-03-06 13:13:25 - train: epoch 0050, iter [03500, 05004], lr: 0.087955, loss: 2.8735
2022-03-06 13:13:58 - train: epoch 0050, iter [03600, 05004], lr: 0.087955, loss: 3.0141
2022-03-06 13:14:30 - train: epoch 0050, iter [03700, 05004], lr: 0.087955, loss: 2.8884
2022-03-06 13:15:03 - train: epoch 0050, iter [03800, 05004], lr: 0.087955, loss: 2.7333
2022-03-06 13:15:37 - train: epoch 0050, iter [03900, 05004], lr: 0.087955, loss: 2.8884
2022-03-06 13:16:11 - train: epoch 0050, iter [04000, 05004], lr: 0.087955, loss: 3.4044
2022-03-06 13:16:45 - train: epoch 0050, iter [04100, 05004], lr: 0.087955, loss: 2.8149
2022-03-06 13:17:19 - train: epoch 0050, iter [04200, 05004], lr: 0.087955, loss: 3.1665
2022-03-06 13:17:52 - train: epoch 0050, iter [04300, 05004], lr: 0.087955, loss: 3.3389
2022-03-06 13:18:25 - train: epoch 0050, iter [04400, 05004], lr: 0.087955, loss: 2.9454
