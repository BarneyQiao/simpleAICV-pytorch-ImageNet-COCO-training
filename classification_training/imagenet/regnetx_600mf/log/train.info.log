2022-08-02 00:00:12 - train: epoch 0043, iter [02500, 05004], lr: 0.132471, loss: 2.0799
2022-08-02 00:01:10 - train: epoch 0043, iter [02600, 05004], lr: 0.132409, loss: 1.7958
2022-08-02 00:02:06 - train: epoch 0043, iter [02700, 05004], lr: 0.132346, loss: 1.9765
2022-08-02 00:03:06 - train: epoch 0043, iter [02800, 05004], lr: 0.132284, loss: 1.7347
2022-08-02 00:04:04 - train: epoch 0043, iter [02900, 05004], lr: 0.132221, loss: 1.9823
2022-08-02 00:05:03 - train: epoch 0043, iter [03000, 05004], lr: 0.132158, loss: 2.0751
2022-08-02 00:06:02 - train: epoch 0043, iter [03100, 05004], lr: 0.132096, loss: 2.1792
2022-08-02 00:07:00 - train: epoch 0043, iter [03200, 05004], lr: 0.132033, loss: 1.8489
2022-08-02 00:07:58 - train: epoch 0043, iter [03300, 05004], lr: 0.131971, loss: 1.9652
2022-08-02 00:08:57 - train: epoch 0043, iter [03400, 05004], lr: 0.131908, loss: 2.0182
2022-08-02 00:09:55 - train: epoch 0043, iter [03500, 05004], lr: 0.131845, loss: 2.0258
2022-08-02 00:10:54 - train: epoch 0043, iter [03600, 05004], lr: 0.131783, loss: 2.0890
2022-08-02 00:11:53 - train: epoch 0043, iter [03700, 05004], lr: 0.131720, loss: 1.8961
2022-08-02 00:12:51 - train: epoch 0043, iter [03800, 05004], lr: 0.131657, loss: 2.0555
2022-08-02 00:13:49 - train: epoch 0043, iter [03900, 05004], lr: 0.131595, loss: 2.2192
2022-08-02 00:14:49 - train: epoch 0043, iter [04000, 05004], lr: 0.131532, loss: 2.0828
2022-08-02 00:15:48 - train: epoch 0043, iter [04100, 05004], lr: 0.131469, loss: 2.1508
2022-08-02 00:16:45 - train: epoch 0043, iter [04200, 05004], lr: 0.131407, loss: 2.1993
2022-08-02 00:17:44 - train: epoch 0043, iter [04300, 05004], lr: 0.131344, loss: 1.9543
2022-08-02 00:18:43 - train: epoch 0043, iter [04400, 05004], lr: 0.131281, loss: 2.0499
2022-08-02 00:19:41 - train: epoch 0043, iter [04500, 05004], lr: 0.131218, loss: 1.9836
2022-08-02 00:20:40 - train: epoch 0043, iter [04600, 05004], lr: 0.131156, loss: 2.1137
2022-08-02 00:21:39 - train: epoch 0043, iter [04700, 05004], lr: 0.131093, loss: 2.0424
2022-08-02 00:22:38 - train: epoch 0043, iter [04800, 05004], lr: 0.131030, loss: 1.9345
2022-08-02 00:23:38 - train: epoch 0043, iter [04900, 05004], lr: 0.130967, loss: 1.7720
2022-08-02 00:24:33 - train: epoch 0043, iter [05000, 05004], lr: 0.130904, loss: 1.8997
2022-08-02 00:24:34 - train: epoch 043, train_loss: 1.9691
2022-08-02 00:26:43 - eval: epoch: 043, acc1: 59.064%, acc5: 82.830%, test_loss: 1.7210, per_image_load_time: 4.077ms, per_image_inference_time: 0.629ms
2022-08-02 00:26:43 - until epoch: 043, best_acc1: 59.674%
2022-08-02 00:26:43 - epoch 044 lr: 0.130901
2022-08-02 00:27:53 - train: epoch 0044, iter [00100, 05004], lr: 0.130839, loss: 1.9684
2022-08-02 00:28:50 - train: epoch 0044, iter [00200, 05004], lr: 0.130776, loss: 2.0395
2022-08-02 00:29:48 - train: epoch 0044, iter [00300, 05004], lr: 0.130713, loss: 1.7660
2022-08-02 00:30:46 - train: epoch 0044, iter [00400, 05004], lr: 0.130650, loss: 1.5718
2022-08-02 00:31:44 - train: epoch 0044, iter [00500, 05004], lr: 0.130587, loss: 1.9928
2022-08-02 00:32:41 - train: epoch 0044, iter [00600, 05004], lr: 0.130524, loss: 1.7214
2022-08-02 00:33:42 - train: epoch 0044, iter [00700, 05004], lr: 0.130461, loss: 1.8680
2022-08-02 00:34:40 - train: epoch 0044, iter [00800, 05004], lr: 0.130398, loss: 1.9767
2022-08-02 00:35:37 - train: epoch 0044, iter [00900, 05004], lr: 0.130335, loss: 2.1226
2022-08-02 00:36:35 - train: epoch 0044, iter [01000, 05004], lr: 0.130273, loss: 2.0007
2022-08-02 00:37:35 - train: epoch 0044, iter [01100, 05004], lr: 0.130210, loss: 1.8569
2022-08-02 00:38:33 - train: epoch 0044, iter [01200, 05004], lr: 0.130147, loss: 1.9605
2022-08-02 00:39:33 - train: epoch 0044, iter [01300, 05004], lr: 0.130084, loss: 2.1069
2022-08-02 00:40:29 - train: epoch 0044, iter [01400, 05004], lr: 0.130020, loss: 1.9034
2022-08-02 00:41:28 - train: epoch 0044, iter [01500, 05004], lr: 0.129957, loss: 2.1088
2022-08-02 00:42:25 - train: epoch 0044, iter [01600, 05004], lr: 0.129894, loss: 1.8020
2022-08-02 00:43:24 - train: epoch 0044, iter [01700, 05004], lr: 0.129831, loss: 2.1352
2022-08-02 00:44:21 - train: epoch 0044, iter [01800, 05004], lr: 0.129768, loss: 2.0864
2022-08-02 00:45:21 - train: epoch 0044, iter [01900, 05004], lr: 0.129705, loss: 1.9782
2022-08-02 00:46:20 - train: epoch 0044, iter [02000, 05004], lr: 0.129642, loss: 1.9342
2022-08-02 00:47:16 - train: epoch 0044, iter [02100, 05004], lr: 0.129579, loss: 2.0535
2022-08-02 00:48:16 - train: epoch 0044, iter [02200, 05004], lr: 0.129516, loss: 1.8563
2022-08-02 00:49:12 - train: epoch 0044, iter [02300, 05004], lr: 0.129453, loss: 2.0267
2022-08-02 00:50:10 - train: epoch 0044, iter [02400, 05004], lr: 0.129389, loss: 1.9533
2022-08-02 00:51:09 - train: epoch 0044, iter [02500, 05004], lr: 0.129326, loss: 1.9411
2022-08-02 00:52:06 - train: epoch 0044, iter [02600, 05004], lr: 0.129263, loss: 1.9289
2022-08-02 00:53:03 - train: epoch 0044, iter [02700, 05004], lr: 0.129200, loss: 1.9714
2022-08-02 00:54:00 - train: epoch 0044, iter [02800, 05004], lr: 0.129137, loss: 1.8609
2022-08-02 00:54:58 - train: epoch 0044, iter [02900, 05004], lr: 0.129073, loss: 1.7128
2022-08-02 00:55:55 - train: epoch 0044, iter [03000, 05004], lr: 0.129010, loss: 1.7047
2022-08-02 00:56:50 - train: epoch 0044, iter [03100, 05004], lr: 0.128947, loss: 1.9806
2022-08-02 00:57:47 - train: epoch 0044, iter [03200, 05004], lr: 0.128884, loss: 1.7841
2022-08-02 00:58:44 - train: epoch 0044, iter [03300, 05004], lr: 0.128820, loss: 1.9275
2022-08-02 00:59:43 - train: epoch 0044, iter [03400, 05004], lr: 0.128757, loss: 2.4542
2022-08-02 01:00:39 - train: epoch 0044, iter [03500, 05004], lr: 0.128694, loss: 1.9005
2022-08-02 01:01:37 - train: epoch 0044, iter [03600, 05004], lr: 0.128631, loss: 2.0170
2022-08-02 01:02:35 - train: epoch 0044, iter [03700, 05004], lr: 0.128567, loss: 2.0356
2022-08-02 01:03:34 - train: epoch 0044, iter [03800, 05004], lr: 0.128504, loss: 1.5919
2022-08-02 01:04:32 - train: epoch 0044, iter [03900, 05004], lr: 0.128441, loss: 2.0529
2022-08-02 01:05:29 - train: epoch 0044, iter [04000, 05004], lr: 0.128377, loss: 1.8353
2022-08-02 01:06:26 - train: epoch 0044, iter [04100, 05004], lr: 0.128314, loss: 1.8299
2022-08-02 01:07:22 - train: epoch 0044, iter [04200, 05004], lr: 0.128250, loss: 2.0156
2022-08-02 01:08:21 - train: epoch 0044, iter [04300, 05004], lr: 0.128187, loss: 1.8333
2022-08-02 01:09:20 - train: epoch 0044, iter [04400, 05004], lr: 0.128124, loss: 1.9294
2022-08-02 01:10:18 - train: epoch 0044, iter [04500, 05004], lr: 0.128060, loss: 2.1812
2022-08-02 01:11:17 - train: epoch 0044, iter [04600, 05004], lr: 0.127997, loss: 1.9668
2022-08-02 01:12:16 - train: epoch 0044, iter [04700, 05004], lr: 0.127933, loss: 1.8644
2022-08-02 01:13:14 - train: epoch 0044, iter [04800, 05004], lr: 0.127870, loss: 2.0930
2022-08-02 01:14:12 - train: epoch 0044, iter [04900, 05004], lr: 0.127806, loss: 2.0004
2022-08-02 01:15:07 - train: epoch 0044, iter [05000, 05004], lr: 0.127743, loss: 2.0532
2022-08-02 01:15:09 - train: epoch 044, train_loss: 1.9590
2022-08-02 01:17:15 - eval: epoch: 044, acc1: 59.966%, acc5: 83.470%, test_loss: 1.6776, per_image_load_time: 3.067ms, per_image_inference_time: 0.583ms
2022-08-02 01:17:15 - until epoch: 044, best_acc1: 59.966%
2022-08-02 01:17:15 - epoch 045 lr: 0.127740
2022-08-02 01:18:24 - train: epoch 0045, iter [00100, 05004], lr: 0.127677, loss: 1.7717
2022-08-02 01:19:22 - train: epoch 0045, iter [00200, 05004], lr: 0.127613, loss: 1.7924
2022-08-02 01:20:20 - train: epoch 0045, iter [00300, 05004], lr: 0.127550, loss: 1.8826
2022-08-02 01:21:16 - train: epoch 0045, iter [00400, 05004], lr: 0.127486, loss: 1.9190
2022-08-02 01:22:12 - train: epoch 0045, iter [00500, 05004], lr: 0.127423, loss: 2.1296
2022-08-02 01:23:09 - train: epoch 0045, iter [00600, 05004], lr: 0.127359, loss: 2.0382
2022-08-02 01:24:06 - train: epoch 0045, iter [00700, 05004], lr: 0.127296, loss: 1.8528
2022-08-02 01:25:04 - train: epoch 0045, iter [00800, 05004], lr: 0.127232, loss: 1.6180
2022-08-02 01:26:00 - train: epoch 0045, iter [00900, 05004], lr: 0.127168, loss: 1.7816
2022-08-02 01:26:58 - train: epoch 0045, iter [01000, 05004], lr: 0.127105, loss: 1.9273
2022-08-02 01:27:56 - train: epoch 0045, iter [01100, 05004], lr: 0.127041, loss: 2.0698
2022-08-02 01:28:53 - train: epoch 0045, iter [01200, 05004], lr: 0.126978, loss: 1.9885
2022-08-02 01:29:49 - train: epoch 0045, iter [01300, 05004], lr: 0.126914, loss: 1.9993
2022-08-02 01:30:48 - train: epoch 0045, iter [01400, 05004], lr: 0.126850, loss: 2.0104
2022-08-02 01:31:44 - train: epoch 0045, iter [01500, 05004], lr: 0.126787, loss: 1.9075
2022-08-02 01:32:43 - train: epoch 0045, iter [01600, 05004], lr: 0.126723, loss: 1.8731
2022-08-02 01:33:40 - train: epoch 0045, iter [01700, 05004], lr: 0.126659, loss: 1.8321
2022-08-02 01:34:37 - train: epoch 0045, iter [01800, 05004], lr: 0.126595, loss: 1.8906
2022-08-02 01:35:36 - train: epoch 0045, iter [01900, 05004], lr: 0.126532, loss: 2.0460
2022-08-02 01:36:31 - train: epoch 0045, iter [02000, 05004], lr: 0.126468, loss: 2.1589
2022-08-02 01:37:30 - train: epoch 0045, iter [02100, 05004], lr: 0.126404, loss: 1.9436
2022-08-02 01:38:27 - train: epoch 0045, iter [02200, 05004], lr: 0.126341, loss: 2.1254
2022-08-02 01:39:24 - train: epoch 0045, iter [02300, 05004], lr: 0.126277, loss: 1.9869
2022-08-02 01:40:23 - train: epoch 0045, iter [02400, 05004], lr: 0.126213, loss: 1.8071
2022-08-02 01:41:22 - train: epoch 0045, iter [02500, 05004], lr: 0.126149, loss: 1.9450
2022-08-02 01:42:19 - train: epoch 0045, iter [02600, 05004], lr: 0.126085, loss: 2.0119
2022-08-02 01:43:16 - train: epoch 0045, iter [02700, 05004], lr: 0.126022, loss: 1.8173
2022-08-02 01:44:13 - train: epoch 0045, iter [02800, 05004], lr: 0.125958, loss: 1.7385
2022-08-02 01:45:11 - train: epoch 0045, iter [02900, 05004], lr: 0.125894, loss: 2.1246
2022-08-02 01:46:08 - train: epoch 0045, iter [03000, 05004], lr: 0.125830, loss: 1.8755
2022-08-02 01:47:06 - train: epoch 0045, iter [03100, 05004], lr: 0.125766, loss: 1.9623
2022-08-02 01:48:03 - train: epoch 0045, iter [03200, 05004], lr: 0.125702, loss: 1.8836
2022-08-02 01:49:01 - train: epoch 0045, iter [03300, 05004], lr: 0.125639, loss: 2.1201
2022-08-02 01:49:59 - train: epoch 0045, iter [03400, 05004], lr: 0.125575, loss: 1.8336
2022-08-02 01:50:56 - train: epoch 0045, iter [03500, 05004], lr: 0.125511, loss: 2.1735
2022-08-02 01:51:51 - train: epoch 0045, iter [03600, 05004], lr: 0.125447, loss: 1.9578
2022-08-02 01:52:48 - train: epoch 0045, iter [03700, 05004], lr: 0.125383, loss: 1.7259
2022-08-02 01:53:45 - train: epoch 0045, iter [03800, 05004], lr: 0.125319, loss: 1.7418
2022-08-02 01:54:40 - train: epoch 0045, iter [03900, 05004], lr: 0.125255, loss: 1.9881
2022-08-02 01:55:37 - train: epoch 0045, iter [04000, 05004], lr: 0.125191, loss: 2.1100
2022-08-02 01:56:34 - train: epoch 0045, iter [04100, 05004], lr: 0.125127, loss: 1.8395
2022-08-02 01:57:30 - train: epoch 0045, iter [04200, 05004], lr: 0.125063, loss: 2.1999
2022-08-02 01:58:26 - train: epoch 0045, iter [04300, 05004], lr: 0.124999, loss: 2.0430
2022-08-02 01:59:22 - train: epoch 0045, iter [04400, 05004], lr: 0.124935, loss: 1.9934
2022-08-02 02:00:19 - train: epoch 0045, iter [04500, 05004], lr: 0.124871, loss: 1.8842
2022-08-02 02:01:15 - train: epoch 0045, iter [04600, 05004], lr: 0.124807, loss: 2.0606
2022-08-02 02:02:12 - train: epoch 0045, iter [04700, 05004], lr: 0.124743, loss: 2.0314
2022-08-02 02:03:08 - train: epoch 0045, iter [04800, 05004], lr: 0.124679, loss: 1.8359
2022-08-02 02:04:05 - train: epoch 0045, iter [04900, 05004], lr: 0.124615, loss: 2.0301
2022-08-02 02:04:59 - train: epoch 0045, iter [05000, 05004], lr: 0.124551, loss: 1.9923
2022-08-02 02:05:00 - train: epoch 045, train_loss: 1.9473
2022-08-02 02:07:06 - eval: epoch: 045, acc1: 59.826%, acc5: 83.228%, test_loss: 1.6736, per_image_load_time: 4.223ms, per_image_inference_time: 0.631ms
2022-08-02 02:07:06 - until epoch: 045, best_acc1: 59.966%
2022-08-02 02:07:06 - epoch 046 lr: 0.124548
2022-08-02 02:08:13 - train: epoch 0046, iter [00100, 05004], lr: 0.124484, loss: 1.6672
2022-08-02 02:09:12 - train: epoch 0046, iter [00200, 05004], lr: 0.124420, loss: 1.9621
2022-08-02 02:10:11 - train: epoch 0046, iter [00300, 05004], lr: 0.124356, loss: 1.9264
2022-08-02 02:11:09 - train: epoch 0046, iter [00400, 05004], lr: 0.124292, loss: 1.9196
2022-08-02 02:12:07 - train: epoch 0046, iter [00500, 05004], lr: 0.124228, loss: 1.8124
2022-08-02 02:13:04 - train: epoch 0046, iter [00600, 05004], lr: 0.124164, loss: 1.8686
2022-08-02 02:14:01 - train: epoch 0046, iter [00700, 05004], lr: 0.124100, loss: 1.7201
2022-08-02 02:14:58 - train: epoch 0046, iter [00800, 05004], lr: 0.124036, loss: 2.0116
2022-08-02 02:15:56 - train: epoch 0046, iter [00900, 05004], lr: 0.123972, loss: 1.8066
2022-08-02 02:16:52 - train: epoch 0046, iter [01000, 05004], lr: 0.123907, loss: 1.7137
2022-08-02 02:17:50 - train: epoch 0046, iter [01100, 05004], lr: 0.123843, loss: 1.8477
2022-08-02 02:18:45 - train: epoch 0046, iter [01200, 05004], lr: 0.123779, loss: 1.8737
2022-08-02 02:19:41 - train: epoch 0046, iter [01300, 05004], lr: 0.123715, loss: 1.9662
2022-08-02 02:20:36 - train: epoch 0046, iter [01400, 05004], lr: 0.123651, loss: 1.9691
2022-08-02 02:21:31 - train: epoch 0046, iter [01500, 05004], lr: 0.123586, loss: 2.0427
2022-08-02 02:22:31 - train: epoch 0046, iter [01600, 05004], lr: 0.123522, loss: 2.1316
2022-08-02 02:23:28 - train: epoch 0046, iter [01700, 05004], lr: 0.123458, loss: 2.0307
2022-08-02 02:24:25 - train: epoch 0046, iter [01800, 05004], lr: 0.123394, loss: 1.8928
2022-08-02 02:25:20 - train: epoch 0046, iter [01900, 05004], lr: 0.123329, loss: 1.8070
2022-08-02 02:26:18 - train: epoch 0046, iter [02000, 05004], lr: 0.123265, loss: 1.8599
2022-08-02 02:27:16 - train: epoch 0046, iter [02100, 05004], lr: 0.123201, loss: 2.1859
2022-08-02 02:28:13 - train: epoch 0046, iter [02200, 05004], lr: 0.123137, loss: 1.7279
2022-08-02 02:29:11 - train: epoch 0046, iter [02300, 05004], lr: 0.123072, loss: 1.9332
2022-08-02 02:30:09 - train: epoch 0046, iter [02400, 05004], lr: 0.123008, loss: 2.0226
2022-08-02 02:31:05 - train: epoch 0046, iter [02500, 05004], lr: 0.122944, loss: 1.8826
2022-08-02 02:32:03 - train: epoch 0046, iter [02600, 05004], lr: 0.122879, loss: 2.1262
2022-08-02 02:33:02 - train: epoch 0046, iter [02700, 05004], lr: 0.122815, loss: 1.7201
2022-08-02 02:33:57 - train: epoch 0046, iter [02800, 05004], lr: 0.122751, loss: 1.9084
2022-08-02 02:34:53 - train: epoch 0046, iter [02900, 05004], lr: 0.122686, loss: 2.0488
2022-08-02 02:35:50 - train: epoch 0046, iter [03000, 05004], lr: 0.122622, loss: 1.6106
2022-08-02 02:36:46 - train: epoch 0046, iter [03100, 05004], lr: 0.122558, loss: 1.7531
2022-08-02 02:37:42 - train: epoch 0046, iter [03200, 05004], lr: 0.122493, loss: 2.0872
2022-08-02 02:38:39 - train: epoch 0046, iter [03300, 05004], lr: 0.122429, loss: 2.0245
2022-08-02 02:39:34 - train: epoch 0046, iter [03400, 05004], lr: 0.122364, loss: 1.9864
2022-08-02 02:40:30 - train: epoch 0046, iter [03500, 05004], lr: 0.122300, loss: 2.1968
2022-08-02 02:41:24 - train: epoch 0046, iter [03600, 05004], lr: 0.122236, loss: 1.9211
2022-08-02 02:42:20 - train: epoch 0046, iter [03700, 05004], lr: 0.122171, loss: 1.6811
2022-08-02 02:43:15 - train: epoch 0046, iter [03800, 05004], lr: 0.122107, loss: 1.8657
2022-08-02 02:44:10 - train: epoch 0046, iter [03900, 05004], lr: 0.122042, loss: 1.8824
2022-08-02 02:45:06 - train: epoch 0046, iter [04000, 05004], lr: 0.121978, loss: 1.8623
2022-08-02 02:46:04 - train: epoch 0046, iter [04100, 05004], lr: 0.121913, loss: 2.0739
2022-08-02 02:46:58 - train: epoch 0046, iter [04200, 05004], lr: 0.121849, loss: 1.7980
2022-08-02 02:47:54 - train: epoch 0046, iter [04300, 05004], lr: 0.121784, loss: 2.1977
2022-08-02 02:48:48 - train: epoch 0046, iter [04400, 05004], lr: 0.121720, loss: 2.1517
2022-08-02 02:49:43 - train: epoch 0046, iter [04500, 05004], lr: 0.121655, loss: 1.8610
2022-08-02 02:50:40 - train: epoch 0046, iter [04600, 05004], lr: 0.121591, loss: 1.9842
2022-08-02 02:51:35 - train: epoch 0046, iter [04700, 05004], lr: 0.121526, loss: 1.9387
2022-08-02 02:52:32 - train: epoch 0046, iter [04800, 05004], lr: 0.121462, loss: 1.6466
2022-08-02 02:53:25 - train: epoch 0046, iter [04900, 05004], lr: 0.121397, loss: 2.0367
2022-08-02 02:54:19 - train: epoch 0046, iter [05000, 05004], lr: 0.121333, loss: 1.7685
2022-08-02 02:54:20 - train: epoch 046, train_loss: 1.9363
2022-08-02 02:56:29 - eval: epoch: 046, acc1: 59.738%, acc5: 83.136%, test_loss: 1.6791, per_image_load_time: 1.300ms, per_image_inference_time: 0.569ms
2022-08-02 02:56:29 - until epoch: 046, best_acc1: 59.966%
2022-08-02 02:56:29 - epoch 047 lr: 0.121329
2022-08-02 02:57:34 - train: epoch 0047, iter [00100, 05004], lr: 0.121265, loss: 1.8623
2022-08-02 02:58:31 - train: epoch 0047, iter [00200, 05004], lr: 0.121201, loss: 1.7685
2022-08-02 02:59:30 - train: epoch 0047, iter [00300, 05004], lr: 0.121136, loss: 1.8469
2022-08-02 03:00:26 - train: epoch 0047, iter [00400, 05004], lr: 0.121072, loss: 1.7172
2022-08-02 03:01:23 - train: epoch 0047, iter [00500, 05004], lr: 0.121007, loss: 1.8736
2022-08-02 03:02:21 - train: epoch 0047, iter [00600, 05004], lr: 0.120942, loss: 2.0141
2022-08-02 03:03:17 - train: epoch 0047, iter [00700, 05004], lr: 0.120878, loss: 1.8638
2022-08-02 03:04:14 - train: epoch 0047, iter [00800, 05004], lr: 0.120813, loss: 1.8952
2022-08-02 03:05:11 - train: epoch 0047, iter [00900, 05004], lr: 0.120749, loss: 2.0292
2022-08-02 03:06:08 - train: epoch 0047, iter [01000, 05004], lr: 0.120684, loss: 1.9198
2022-08-02 03:07:04 - train: epoch 0047, iter [01100, 05004], lr: 0.120619, loss: 2.0617
2022-08-02 03:08:00 - train: epoch 0047, iter [01200, 05004], lr: 0.120555, loss: 1.8319
2022-08-02 03:08:56 - train: epoch 0047, iter [01300, 05004], lr: 0.120490, loss: 2.0806
2022-08-02 03:09:52 - train: epoch 0047, iter [01400, 05004], lr: 0.120425, loss: 2.0245
2022-08-02 03:10:48 - train: epoch 0047, iter [01500, 05004], lr: 0.120360, loss: 2.0154
2022-08-02 03:11:43 - train: epoch 0047, iter [01600, 05004], lr: 0.120296, loss: 1.9203
2022-08-02 03:12:39 - train: epoch 0047, iter [01700, 05004], lr: 0.120231, loss: 1.5592
2022-08-02 03:13:35 - train: epoch 0047, iter [01800, 05004], lr: 0.120166, loss: 1.9259
2022-08-02 03:14:32 - train: epoch 0047, iter [01900, 05004], lr: 0.120102, loss: 1.8302
2022-08-02 03:15:28 - train: epoch 0047, iter [02000, 05004], lr: 0.120037, loss: 1.9931
2022-08-02 03:16:23 - train: epoch 0047, iter [02100, 05004], lr: 0.119972, loss: 2.1897
2022-08-02 03:17:19 - train: epoch 0047, iter [02200, 05004], lr: 0.119907, loss: 2.1488
2022-08-02 03:18:15 - train: epoch 0047, iter [02300, 05004], lr: 0.119843, loss: 1.9046
2022-08-02 03:19:09 - train: epoch 0047, iter [02400, 05004], lr: 0.119778, loss: 1.9867
2022-08-02 03:20:06 - train: epoch 0047, iter [02500, 05004], lr: 0.119713, loss: 2.0431
2022-08-02 03:21:00 - train: epoch 0047, iter [02600, 05004], lr: 0.119648, loss: 2.3098
2022-08-02 03:21:57 - train: epoch 0047, iter [02700, 05004], lr: 0.119583, loss: 1.8227
2022-08-02 03:22:53 - train: epoch 0047, iter [02800, 05004], lr: 0.119519, loss: 1.9126
2022-08-02 03:23:49 - train: epoch 0047, iter [02900, 05004], lr: 0.119454, loss: 1.8819
2022-08-02 03:24:45 - train: epoch 0047, iter [03000, 05004], lr: 0.119389, loss: 2.0899
2022-08-02 03:25:39 - train: epoch 0047, iter [03100, 05004], lr: 0.119324, loss: 1.8295
2022-08-02 03:26:37 - train: epoch 0047, iter [03200, 05004], lr: 0.119259, loss: 2.0252
2022-08-02 03:27:31 - train: epoch 0047, iter [03300, 05004], lr: 0.119194, loss: 1.9238
2022-08-02 03:28:26 - train: epoch 0047, iter [03400, 05004], lr: 0.119130, loss: 1.8364
2022-08-02 03:29:22 - train: epoch 0047, iter [03500, 05004], lr: 0.119065, loss: 2.1028
2022-08-02 03:30:17 - train: epoch 0047, iter [03600, 05004], lr: 0.119000, loss: 2.0760
2022-08-02 03:31:12 - train: epoch 0047, iter [03700, 05004], lr: 0.118935, loss: 1.9965
2022-08-02 03:32:08 - train: epoch 0047, iter [03800, 05004], lr: 0.118870, loss: 2.0517
2022-08-02 03:33:04 - train: epoch 0047, iter [03900, 05004], lr: 0.118805, loss: 1.9187
2022-08-02 03:34:00 - train: epoch 0047, iter [04000, 05004], lr: 0.118740, loss: 2.0139
2022-08-02 03:34:55 - train: epoch 0047, iter [04100, 05004], lr: 0.118675, loss: 2.0211
2022-08-02 03:35:50 - train: epoch 0047, iter [04200, 05004], lr: 0.118610, loss: 1.8301
2022-08-02 03:36:46 - train: epoch 0047, iter [04300, 05004], lr: 0.118545, loss: 1.7317
2022-08-02 03:37:41 - train: epoch 0047, iter [04400, 05004], lr: 0.118480, loss: 1.8868
2022-08-02 03:38:37 - train: epoch 0047, iter [04500, 05004], lr: 0.118416, loss: 2.1051
2022-08-02 03:39:31 - train: epoch 0047, iter [04600, 05004], lr: 0.118351, loss: 1.9177
2022-08-02 03:40:25 - train: epoch 0047, iter [04700, 05004], lr: 0.118286, loss: 2.2504
2022-08-02 03:41:22 - train: epoch 0047, iter [04800, 05004], lr: 0.118221, loss: 1.8852
2022-08-02 03:42:16 - train: epoch 0047, iter [04900, 05004], lr: 0.118156, loss: 2.1525
2022-08-02 03:43:09 - train: epoch 0047, iter [05000, 05004], lr: 0.118091, loss: 1.9980
2022-08-02 03:43:10 - train: epoch 047, train_loss: 1.9267
2022-08-02 03:45:15 - eval: epoch: 047, acc1: 60.472%, acc5: 83.998%, test_loss: 1.6443, per_image_load_time: 3.084ms, per_image_inference_time: 0.573ms
2022-08-02 03:45:16 - until epoch: 047, best_acc1: 60.472%
2022-08-02 03:45:16 - epoch 048 lr: 0.118087
2022-08-02 03:46:24 - train: epoch 0048, iter [00100, 05004], lr: 0.118023, loss: 1.8225
2022-08-02 03:47:22 - train: epoch 0048, iter [00200, 05004], lr: 0.117958, loss: 2.0533
2022-08-02 03:48:21 - train: epoch 0048, iter [00300, 05004], lr: 0.117893, loss: 1.8158
2022-08-02 03:49:19 - train: epoch 0048, iter [00400, 05004], lr: 0.117828, loss: 1.7792
2022-08-02 03:50:18 - train: epoch 0048, iter [00500, 05004], lr: 0.117763, loss: 1.9594
2022-08-02 03:51:15 - train: epoch 0048, iter [00600, 05004], lr: 0.117698, loss: 1.9722
2022-08-02 03:52:17 - train: epoch 0048, iter [00700, 05004], lr: 0.117633, loss: 1.7155
2022-08-02 03:53:14 - train: epoch 0048, iter [00800, 05004], lr: 0.117568, loss: 1.7715
2022-08-02 03:54:12 - train: epoch 0048, iter [00900, 05004], lr: 0.117503, loss: 1.7705
2022-08-02 03:55:07 - train: epoch 0048, iter [01000, 05004], lr: 0.117438, loss: 2.0647
2022-08-02 03:56:04 - train: epoch 0048, iter [01100, 05004], lr: 0.117373, loss: 2.1273
2022-08-02 03:56:58 - train: epoch 0048, iter [01200, 05004], lr: 0.117308, loss: 1.9325
2022-08-02 03:57:55 - train: epoch 0048, iter [01300, 05004], lr: 0.117242, loss: 1.8164
2022-08-02 03:58:51 - train: epoch 0048, iter [01400, 05004], lr: 0.117177, loss: 1.9578
2022-08-02 03:59:45 - train: epoch 0048, iter [01500, 05004], lr: 0.117112, loss: 2.0120
2022-08-02 04:00:41 - train: epoch 0048, iter [01600, 05004], lr: 0.117047, loss: 1.6635
2022-08-02 04:01:37 - train: epoch 0048, iter [01700, 05004], lr: 0.116982, loss: 1.8998
2022-08-02 04:02:32 - train: epoch 0048, iter [01800, 05004], lr: 0.116917, loss: 1.8563
2022-08-02 04:03:28 - train: epoch 0048, iter [01900, 05004], lr: 0.116852, loss: 1.9079
2022-08-02 04:04:25 - train: epoch 0048, iter [02000, 05004], lr: 0.116787, loss: 1.9080
2022-08-02 04:05:20 - train: epoch 0048, iter [02100, 05004], lr: 0.116721, loss: 1.9503
2022-08-02 04:06:16 - train: epoch 0048, iter [02200, 05004], lr: 0.116656, loss: 2.1270
2022-08-02 04:07:11 - train: epoch 0048, iter [02300, 05004], lr: 0.116591, loss: 1.8796
2022-08-02 04:08:06 - train: epoch 0048, iter [02400, 05004], lr: 0.116526, loss: 2.1036
2022-08-02 04:09:02 - train: epoch 0048, iter [02500, 05004], lr: 0.116461, loss: 1.8837
2022-08-02 04:09:56 - train: epoch 0048, iter [02600, 05004], lr: 0.116396, loss: 1.9221
2022-08-02 04:10:52 - train: epoch 0048, iter [02700, 05004], lr: 0.116330, loss: 2.2355
2022-08-02 04:11:50 - train: epoch 0048, iter [02800, 05004], lr: 0.116265, loss: 1.9570
2022-08-02 04:12:44 - train: epoch 0048, iter [02900, 05004], lr: 0.116200, loss: 1.8787
2022-08-02 04:13:40 - train: epoch 0048, iter [03000, 05004], lr: 0.116135, loss: 2.0402
2022-08-02 04:14:36 - train: epoch 0048, iter [03100, 05004], lr: 0.116070, loss: 1.9427
2022-08-02 04:15:32 - train: epoch 0048, iter [03200, 05004], lr: 0.116004, loss: 1.7940
2022-08-02 04:16:27 - train: epoch 0048, iter [03300, 05004], lr: 0.115939, loss: 2.0459
2022-08-02 04:17:23 - train: epoch 0048, iter [03400, 05004], lr: 0.115874, loss: 1.9211
2022-08-02 04:18:18 - train: epoch 0048, iter [03500, 05004], lr: 0.115809, loss: 2.1076
2022-08-02 04:19:14 - train: epoch 0048, iter [03600, 05004], lr: 0.115743, loss: 1.8222
2022-08-02 04:20:07 - train: epoch 0048, iter [03700, 05004], lr: 0.115678, loss: 2.1087
2022-08-02 04:21:04 - train: epoch 0048, iter [03800, 05004], lr: 0.115613, loss: 1.9034
2022-08-02 04:21:59 - train: epoch 0048, iter [03900, 05004], lr: 0.115547, loss: 1.9002
2022-08-02 04:22:56 - train: epoch 0048, iter [04000, 05004], lr: 0.115482, loss: 1.8248
2022-08-02 04:23:51 - train: epoch 0048, iter [04100, 05004], lr: 0.115417, loss: 2.3055
2022-08-02 04:24:47 - train: epoch 0048, iter [04200, 05004], lr: 0.115352, loss: 1.9412
2022-08-02 04:25:42 - train: epoch 0048, iter [04300, 05004], lr: 0.115286, loss: 2.0279
2022-08-02 04:26:38 - train: epoch 0048, iter [04400, 05004], lr: 0.115221, loss: 1.8582
2022-08-02 04:27:33 - train: epoch 0048, iter [04500, 05004], lr: 0.115156, loss: 1.9294
2022-08-02 04:28:30 - train: epoch 0048, iter [04600, 05004], lr: 0.115090, loss: 2.0144
2022-08-02 04:29:25 - train: epoch 0048, iter [04700, 05004], lr: 0.115025, loss: 1.9404
2022-08-02 04:30:21 - train: epoch 0048, iter [04800, 05004], lr: 0.114960, loss: 2.2810
2022-08-02 04:31:15 - train: epoch 0048, iter [04900, 05004], lr: 0.114894, loss: 1.8131
2022-08-02 04:32:08 - train: epoch 0048, iter [05000, 05004], lr: 0.114829, loss: 1.8840
2022-08-02 04:32:09 - train: epoch 048, train_loss: 1.9082
2022-08-02 04:34:19 - eval: epoch: 048, acc1: 60.282%, acc5: 83.846%, test_loss: 1.6484, per_image_load_time: 4.464ms, per_image_inference_time: 0.569ms
2022-08-02 04:34:19 - until epoch: 048, best_acc1: 60.472%
2022-08-02 04:34:19 - epoch 049 lr: 0.114826
2022-08-02 04:35:25 - train: epoch 0049, iter [00100, 05004], lr: 0.114761, loss: 1.8660
2022-08-02 04:36:22 - train: epoch 0049, iter [00200, 05004], lr: 0.114696, loss: 1.6847
2022-08-02 04:37:18 - train: epoch 0049, iter [00300, 05004], lr: 0.114630, loss: 1.7609
2022-08-02 04:38:14 - train: epoch 0049, iter [00400, 05004], lr: 0.114565, loss: 1.9374
2022-08-02 04:39:08 - train: epoch 0049, iter [00500, 05004], lr: 0.114500, loss: 2.0623
2022-08-02 04:40:04 - train: epoch 0049, iter [00600, 05004], lr: 0.114434, loss: 1.8839
2022-08-02 04:41:01 - train: epoch 0049, iter [00700, 05004], lr: 0.114369, loss: 1.9199
2022-08-02 04:41:55 - train: epoch 0049, iter [00800, 05004], lr: 0.114303, loss: 2.0094
2022-08-02 04:42:52 - train: epoch 0049, iter [00900, 05004], lr: 0.114238, loss: 1.9152
2022-08-02 04:43:48 - train: epoch 0049, iter [01000, 05004], lr: 0.114172, loss: 1.8211
2022-08-02 04:44:43 - train: epoch 0049, iter [01100, 05004], lr: 0.114107, loss: 1.9132
2022-08-02 04:45:39 - train: epoch 0049, iter [01200, 05004], lr: 0.114042, loss: 1.9308
2022-08-02 04:46:36 - train: epoch 0049, iter [01300, 05004], lr: 0.113976, loss: 1.8736
2022-08-02 04:47:31 - train: epoch 0049, iter [01400, 05004], lr: 0.113911, loss: 1.9436
2022-08-02 04:48:28 - train: epoch 0049, iter [01500, 05004], lr: 0.113845, loss: 1.6902
2022-08-02 04:49:24 - train: epoch 0049, iter [01600, 05004], lr: 0.113780, loss: 2.0866
2022-08-02 04:50:20 - train: epoch 0049, iter [01700, 05004], lr: 0.113714, loss: 1.8292
2022-08-02 04:51:15 - train: epoch 0049, iter [01800, 05004], lr: 0.113649, loss: 1.9775
2022-08-02 04:52:10 - train: epoch 0049, iter [01900, 05004], lr: 0.113583, loss: 1.7058
2022-08-02 04:53:06 - train: epoch 0049, iter [02000, 05004], lr: 0.113518, loss: 1.9874
2022-08-02 04:54:02 - train: epoch 0049, iter [02100, 05004], lr: 0.113453, loss: 1.8087
2022-08-02 04:54:58 - train: epoch 0049, iter [02200, 05004], lr: 0.113387, loss: 1.9850
2022-08-02 04:55:53 - train: epoch 0049, iter [02300, 05004], lr: 0.113322, loss: 1.7801
2022-08-02 04:56:48 - train: epoch 0049, iter [02400, 05004], lr: 0.113256, loss: 2.0184
2022-08-02 04:57:44 - train: epoch 0049, iter [02500, 05004], lr: 0.113191, loss: 2.0973
2022-08-02 04:58:38 - train: epoch 0049, iter [02600, 05004], lr: 0.113125, loss: 2.0452
2022-08-02 04:59:35 - train: epoch 0049, iter [02700, 05004], lr: 0.113059, loss: 2.0857
2022-08-02 05:00:30 - train: epoch 0049, iter [02800, 05004], lr: 0.112994, loss: 1.9187
2022-08-02 05:01:25 - train: epoch 0049, iter [02900, 05004], lr: 0.112928, loss: 1.9402
2022-08-02 05:02:19 - train: epoch 0049, iter [03000, 05004], lr: 0.112863, loss: 1.9079
2022-08-02 05:03:16 - train: epoch 0049, iter [03100, 05004], lr: 0.112797, loss: 1.8746
2022-08-02 05:04:12 - train: epoch 0049, iter [03200, 05004], lr: 0.112732, loss: 2.0755
2022-08-02 05:05:07 - train: epoch 0049, iter [03300, 05004], lr: 0.112666, loss: 1.7265
2022-08-02 05:06:04 - train: epoch 0049, iter [03400, 05004], lr: 0.112601, loss: 1.9112
2022-08-02 05:07:00 - train: epoch 0049, iter [03500, 05004], lr: 0.112535, loss: 2.1014
2022-08-02 05:07:54 - train: epoch 0049, iter [03600, 05004], lr: 0.112470, loss: 2.0433
2022-08-02 05:08:52 - train: epoch 0049, iter [03700, 05004], lr: 0.112404, loss: 1.7752
2022-08-02 05:09:46 - train: epoch 0049, iter [03800, 05004], lr: 0.112338, loss: 2.1381
2022-08-02 05:10:42 - train: epoch 0049, iter [03900, 05004], lr: 0.112273, loss: 2.0965
2022-08-02 05:11:37 - train: epoch 0049, iter [04000, 05004], lr: 0.112207, loss: 1.8165
2022-08-02 05:12:32 - train: epoch 0049, iter [04100, 05004], lr: 0.112142, loss: 1.9105
2022-08-02 05:13:27 - train: epoch 0049, iter [04200, 05004], lr: 0.112076, loss: 1.9277
2022-08-02 05:14:23 - train: epoch 0049, iter [04300, 05004], lr: 0.112010, loss: 2.0432
2022-08-02 05:15:18 - train: epoch 0049, iter [04400, 05004], lr: 0.111945, loss: 1.9049
2022-08-02 05:16:13 - train: epoch 0049, iter [04500, 05004], lr: 0.111879, loss: 1.8242
2022-08-02 05:17:10 - train: epoch 0049, iter [04600, 05004], lr: 0.111814, loss: 1.9216
2022-08-02 05:18:05 - train: epoch 0049, iter [04700, 05004], lr: 0.111748, loss: 2.0567
2022-08-02 05:19:00 - train: epoch 0049, iter [04800, 05004], lr: 0.111682, loss: 1.7873
2022-08-02 05:19:57 - train: epoch 0049, iter [04900, 05004], lr: 0.111617, loss: 1.8499
2022-08-02 05:20:49 - train: epoch 0049, iter [05000, 05004], lr: 0.111551, loss: 2.1071
2022-08-02 05:20:50 - train: epoch 049, train_loss: 1.8998
2022-08-02 05:22:54 - eval: epoch: 049, acc1: 61.016%, acc5: 84.232%, test_loss: 1.6185, per_image_load_time: 4.241ms, per_image_inference_time: 0.580ms
2022-08-02 05:22:54 - until epoch: 049, best_acc1: 61.016%
2022-08-02 05:22:54 - epoch 050 lr: 0.111548
2022-08-02 05:24:03 - train: epoch 0050, iter [00100, 05004], lr: 0.111483, loss: 1.6432
2022-08-02 05:25:03 - train: epoch 0050, iter [00200, 05004], lr: 0.111417, loss: 1.9080
2022-08-02 05:26:04 - train: epoch 0050, iter [00300, 05004], lr: 0.111352, loss: 1.9389
2022-08-02 05:27:03 - train: epoch 0050, iter [00400, 05004], lr: 0.111286, loss: 1.8406
2022-08-02 05:28:02 - train: epoch 0050, iter [00500, 05004], lr: 0.111220, loss: 1.7288
2022-08-02 05:29:01 - train: epoch 0050, iter [00600, 05004], lr: 0.111155, loss: 1.9709
2022-08-02 05:29:58 - train: epoch 0050, iter [00700, 05004], lr: 0.111089, loss: 1.7262
2022-08-02 05:30:55 - train: epoch 0050, iter [00800, 05004], lr: 0.111023, loss: 1.6715
2022-08-02 05:31:53 - train: epoch 0050, iter [00900, 05004], lr: 0.110957, loss: 1.9422
2022-08-02 05:32:50 - train: epoch 0050, iter [01000, 05004], lr: 0.110892, loss: 1.8472
2022-08-02 05:33:47 - train: epoch 0050, iter [01100, 05004], lr: 0.110826, loss: 1.9474
2022-08-02 05:34:43 - train: epoch 0050, iter [01200, 05004], lr: 0.110760, loss: 1.9020
2022-08-02 05:35:37 - train: epoch 0050, iter [01300, 05004], lr: 0.110695, loss: 1.8349
2022-08-02 05:36:33 - train: epoch 0050, iter [01400, 05004], lr: 0.110629, loss: 1.9810
2022-08-02 05:37:28 - train: epoch 0050, iter [01500, 05004], lr: 0.110563, loss: 1.9119
2022-08-02 05:38:25 - train: epoch 0050, iter [01600, 05004], lr: 0.110498, loss: 1.7778
2022-08-02 05:39:19 - train: epoch 0050, iter [01700, 05004], lr: 0.110432, loss: 2.0959
2022-08-02 05:40:15 - train: epoch 0050, iter [01800, 05004], lr: 0.110366, loss: 2.2324
2022-08-02 05:41:11 - train: epoch 0050, iter [01900, 05004], lr: 0.110300, loss: 1.9748
2022-08-02 05:42:07 - train: epoch 0050, iter [02000, 05004], lr: 0.110235, loss: 1.6962
2022-08-02 05:43:04 - train: epoch 0050, iter [02100, 05004], lr: 0.110169, loss: 1.7578
2022-08-02 05:43:59 - train: epoch 0050, iter [02200, 05004], lr: 0.110103, loss: 1.9099
2022-08-02 05:44:54 - train: epoch 0050, iter [02300, 05004], lr: 0.110037, loss: 1.8981
2022-08-02 05:45:50 - train: epoch 0050, iter [02400, 05004], lr: 0.109972, loss: 2.1515
2022-08-02 05:46:46 - train: epoch 0050, iter [02500, 05004], lr: 0.109906, loss: 1.9499
2022-08-02 05:47:41 - train: epoch 0050, iter [02600, 05004], lr: 0.109840, loss: 1.8902
2022-08-02 05:48:39 - train: epoch 0050, iter [02700, 05004], lr: 0.109774, loss: 1.9910
2022-08-02 05:49:34 - train: epoch 0050, iter [02800, 05004], lr: 0.109709, loss: 1.8876
2022-08-02 05:50:29 - train: epoch 0050, iter [02900, 05004], lr: 0.109643, loss: 2.1351
2022-08-02 05:51:24 - train: epoch 0050, iter [03000, 05004], lr: 0.109577, loss: 1.9278
2022-08-02 05:52:21 - train: epoch 0050, iter [03100, 05004], lr: 0.109511, loss: 2.1296
2022-08-02 05:53:15 - train: epoch 0050, iter [03200, 05004], lr: 0.109445, loss: 2.1359
2022-08-02 05:54:11 - train: epoch 0050, iter [03300, 05004], lr: 0.109380, loss: 1.5820
2022-08-02 05:55:08 - train: epoch 0050, iter [03400, 05004], lr: 0.109314, loss: 1.8398
2022-08-02 05:56:04 - train: epoch 0050, iter [03500, 05004], lr: 0.109248, loss: 1.8477
2022-08-02 05:56:59 - train: epoch 0050, iter [03600, 05004], lr: 0.109182, loss: 1.9658
2022-08-02 05:57:56 - train: epoch 0050, iter [03700, 05004], lr: 0.109116, loss: 2.1828
2022-08-02 05:58:52 - train: epoch 0050, iter [03800, 05004], lr: 0.109051, loss: 1.7120
2022-08-02 05:59:46 - train: epoch 0050, iter [03900, 05004], lr: 0.108985, loss: 1.9141
2022-08-02 06:00:43 - train: epoch 0050, iter [04000, 05004], lr: 0.108919, loss: 2.0611
2022-08-02 06:01:39 - train: epoch 0050, iter [04100, 05004], lr: 0.108853, loss: 2.0714
2022-08-02 06:02:35 - train: epoch 0050, iter [04200, 05004], lr: 0.108787, loss: 1.9123
2022-08-02 06:03:30 - train: epoch 0050, iter [04300, 05004], lr: 0.108721, loss: 1.8240
2022-08-02 06:04:27 - train: epoch 0050, iter [04400, 05004], lr: 0.108656, loss: 2.1202
2022-08-02 06:05:22 - train: epoch 0050, iter [04500, 05004], lr: 0.108590, loss: 1.8029
2022-08-02 06:06:19 - train: epoch 0050, iter [04600, 05004], lr: 0.108524, loss: 1.8397
2022-08-02 06:07:14 - train: epoch 0050, iter [04700, 05004], lr: 0.108458, loss: 1.8499
2022-08-02 06:08:10 - train: epoch 0050, iter [04800, 05004], lr: 0.108392, loss: 1.8476
2022-08-02 06:09:05 - train: epoch 0050, iter [04900, 05004], lr: 0.108326, loss: 1.7179
2022-08-02 06:09:59 - train: epoch 0050, iter [05000, 05004], lr: 0.108261, loss: 1.6163
2022-08-02 06:10:01 - train: epoch 050, train_loss: 1.8857
2022-08-02 06:12:10 - eval: epoch: 050, acc1: 61.106%, acc5: 84.448%, test_loss: 1.6171, per_image_load_time: 4.325ms, per_image_inference_time: 0.612ms
2022-08-02 06:12:10 - until epoch: 050, best_acc1: 61.106%
2022-08-02 06:12:10 - epoch 051 lr: 0.108257
2022-08-02 06:13:17 - train: epoch 0051, iter [00100, 05004], lr: 0.108192, loss: 1.9799
2022-08-02 06:14:15 - train: epoch 0051, iter [00200, 05004], lr: 0.108126, loss: 2.0455
2022-08-02 06:15:16 - train: epoch 0051, iter [00300, 05004], lr: 0.108060, loss: 1.7794
2022-08-02 06:16:14 - train: epoch 0051, iter [00400, 05004], lr: 0.107994, loss: 1.7645
2022-08-02 06:17:14 - train: epoch 0051, iter [00500, 05004], lr: 0.107929, loss: 1.9371
2022-08-02 06:18:11 - train: epoch 0051, iter [00600, 05004], lr: 0.107863, loss: 1.7593
2022-08-02 06:19:09 - train: epoch 0051, iter [00700, 05004], lr: 0.107797, loss: 1.7921
2022-08-02 06:20:07 - train: epoch 0051, iter [00800, 05004], lr: 0.107731, loss: 1.8911
2022-08-02 06:21:05 - train: epoch 0051, iter [00900, 05004], lr: 0.107665, loss: 1.6383
2022-08-02 06:22:00 - train: epoch 0051, iter [01000, 05004], lr: 0.107599, loss: 2.2507
2022-08-02 06:22:58 - train: epoch 0051, iter [01100, 05004], lr: 0.107533, loss: 1.8692
2022-08-02 06:23:53 - train: epoch 0051, iter [01200, 05004], lr: 0.107467, loss: 1.7659
2022-08-02 06:24:49 - train: epoch 0051, iter [01300, 05004], lr: 0.107401, loss: 1.6510
2022-08-02 06:25:45 - train: epoch 0051, iter [01400, 05004], lr: 0.107336, loss: 1.6002
2022-08-02 06:26:41 - train: epoch 0051, iter [01500, 05004], lr: 0.107270, loss: 1.9731
2022-08-02 06:27:37 - train: epoch 0051, iter [01600, 05004], lr: 0.107204, loss: 1.8962
2022-08-02 06:28:33 - train: epoch 0051, iter [01700, 05004], lr: 0.107138, loss: 1.7821
2022-08-02 06:29:29 - train: epoch 0051, iter [01800, 05004], lr: 0.107072, loss: 2.0795
2022-08-02 06:30:23 - train: epoch 0051, iter [01900, 05004], lr: 0.107006, loss: 1.8390
2022-08-02 06:31:19 - train: epoch 0051, iter [02000, 05004], lr: 0.106940, loss: 1.8700
2022-08-02 06:32:15 - train: epoch 0051, iter [02100, 05004], lr: 0.106874, loss: 1.8510
2022-08-02 06:33:11 - train: epoch 0051, iter [02200, 05004], lr: 0.106808, loss: 1.7141
2022-08-02 06:34:06 - train: epoch 0051, iter [02300, 05004], lr: 0.106742, loss: 2.1711
2022-08-02 06:35:01 - train: epoch 0051, iter [02400, 05004], lr: 0.106676, loss: 1.7715
2022-08-02 06:35:57 - train: epoch 0051, iter [02500, 05004], lr: 0.106610, loss: 1.8376
2022-08-02 06:36:54 - train: epoch 0051, iter [02600, 05004], lr: 0.106544, loss: 1.6916
2022-08-02 06:37:49 - train: epoch 0051, iter [02700, 05004], lr: 0.106478, loss: 1.8503
2022-08-02 06:38:44 - train: epoch 0051, iter [02800, 05004], lr: 0.106413, loss: 1.6403
2022-08-02 06:39:39 - train: epoch 0051, iter [02900, 05004], lr: 0.106347, loss: 1.7428
2022-08-02 06:40:36 - train: epoch 0051, iter [03000, 05004], lr: 0.106281, loss: 1.7272
2022-08-02 06:41:31 - train: epoch 0051, iter [03100, 05004], lr: 0.106215, loss: 1.8100
2022-08-02 06:42:26 - train: epoch 0051, iter [03200, 05004], lr: 0.106149, loss: 1.9214
2022-08-02 06:43:21 - train: epoch 0051, iter [03300, 05004], lr: 0.106083, loss: 1.8767
2022-08-02 06:44:16 - train: epoch 0051, iter [03400, 05004], lr: 0.106017, loss: 1.8687
2022-08-02 06:45:12 - train: epoch 0051, iter [03500, 05004], lr: 0.105951, loss: 1.8814
2022-08-02 06:46:08 - train: epoch 0051, iter [03600, 05004], lr: 0.105885, loss: 1.8834
2022-08-02 06:47:05 - train: epoch 0051, iter [03700, 05004], lr: 0.105819, loss: 1.9674
2022-08-02 06:48:02 - train: epoch 0051, iter [03800, 05004], lr: 0.105753, loss: 1.9894
2022-08-02 06:48:58 - train: epoch 0051, iter [03900, 05004], lr: 0.105687, loss: 1.8199
2022-08-02 06:49:56 - train: epoch 0051, iter [04000, 05004], lr: 0.105621, loss: 1.9708
2022-08-02 06:50:54 - train: epoch 0051, iter [04100, 05004], lr: 0.105555, loss: 1.9011
2022-08-02 06:51:50 - train: epoch 0051, iter [04200, 05004], lr: 0.105489, loss: 1.9177
2022-08-02 06:52:46 - train: epoch 0051, iter [04300, 05004], lr: 0.105423, loss: 1.8182
2022-08-02 06:53:43 - train: epoch 0051, iter [04400, 05004], lr: 0.105357, loss: 1.9062
2022-08-02 06:54:41 - train: epoch 0051, iter [04500, 05004], lr: 0.105291, loss: 1.7584
2022-08-02 06:55:37 - train: epoch 0051, iter [04600, 05004], lr: 0.105225, loss: 1.9504
2022-08-02 06:56:34 - train: epoch 0051, iter [04700, 05004], lr: 0.105159, loss: 1.7519
2022-08-02 06:57:31 - train: epoch 0051, iter [04800, 05004], lr: 0.105093, loss: 1.9409
2022-08-02 06:58:29 - train: epoch 0051, iter [04900, 05004], lr: 0.105027, loss: 1.8733
2022-08-02 06:59:22 - train: epoch 0051, iter [05000, 05004], lr: 0.104961, loss: 1.8586
2022-08-02 06:59:24 - train: epoch 051, train_loss: 1.8734
2022-08-02 07:01:33 - eval: epoch: 051, acc1: 61.220%, acc5: 84.192%, test_loss: 1.6050, per_image_load_time: 4.177ms, per_image_inference_time: 0.625ms
2022-08-02 07:01:33 - until epoch: 051, best_acc1: 61.220%
2022-08-02 07:01:33 - epoch 052 lr: 0.104958
2022-08-02 07:02:44 - train: epoch 0052, iter [00100, 05004], lr: 0.104892, loss: 1.6792
2022-08-02 07:03:44 - train: epoch 0052, iter [00200, 05004], lr: 0.104826, loss: 1.8033
2022-08-02 07:04:40 - train: epoch 0052, iter [00300, 05004], lr: 0.104760, loss: 1.6723
2022-08-02 07:05:38 - train: epoch 0052, iter [00400, 05004], lr: 0.104694, loss: 1.8734
2022-08-02 07:06:36 - train: epoch 0052, iter [00500, 05004], lr: 0.104628, loss: 1.7619
2022-08-02 07:07:33 - train: epoch 0052, iter [00600, 05004], lr: 0.104562, loss: 1.7559
2022-08-02 07:08:30 - train: epoch 0052, iter [00700, 05004], lr: 0.104496, loss: 1.9675
2022-08-02 07:09:28 - train: epoch 0052, iter [00800, 05004], lr: 0.104430, loss: 1.5762
2022-08-02 07:10:24 - train: epoch 0052, iter [00900, 05004], lr: 0.104364, loss: 2.0223
2022-08-02 07:11:22 - train: epoch 0052, iter [01000, 05004], lr: 0.104298, loss: 1.8640
2022-08-02 07:12:19 - train: epoch 0052, iter [01100, 05004], lr: 0.104232, loss: 2.0628
2022-08-02 07:13:14 - train: epoch 0052, iter [01200, 05004], lr: 0.104166, loss: 1.7625
2022-08-02 07:14:11 - train: epoch 0052, iter [01300, 05004], lr: 0.104100, loss: 1.8924
2022-08-02 07:15:08 - train: epoch 0052, iter [01400, 05004], lr: 0.104034, loss: 2.0369
2022-08-02 07:16:03 - train: epoch 0052, iter [01500, 05004], lr: 0.103968, loss: 1.8771
2022-08-02 07:16:59 - train: epoch 0052, iter [01600, 05004], lr: 0.103902, loss: 1.6021
2022-08-02 07:17:56 - train: epoch 0052, iter [01700, 05004], lr: 0.103836, loss: 1.7706
2022-08-02 07:18:51 - train: epoch 0052, iter [01800, 05004], lr: 0.103770, loss: 1.8363
2022-08-02 07:19:46 - train: epoch 0052, iter [01900, 05004], lr: 0.103704, loss: 1.6918
2022-08-02 07:20:42 - train: epoch 0052, iter [02000, 05004], lr: 0.103638, loss: 1.9558
2022-08-02 07:21:37 - train: epoch 0052, iter [02100, 05004], lr: 0.103572, loss: 1.7734
2022-08-02 07:22:33 - train: epoch 0052, iter [02200, 05004], lr: 0.103506, loss: 1.8908
2022-08-02 07:23:29 - train: epoch 0052, iter [02300, 05004], lr: 0.103440, loss: 1.9196
2022-08-02 07:24:25 - train: epoch 0052, iter [02400, 05004], lr: 0.103374, loss: 1.5788
2022-08-02 07:25:20 - train: epoch 0052, iter [02500, 05004], lr: 0.103308, loss: 1.7519
2022-08-02 07:26:18 - train: epoch 0052, iter [02600, 05004], lr: 0.103242, loss: 1.7464
2022-08-02 07:27:14 - train: epoch 0052, iter [02700, 05004], lr: 0.103176, loss: 1.8375
2022-08-02 07:28:08 - train: epoch 0052, iter [02800, 05004], lr: 0.103110, loss: 1.9538
2022-08-02 07:29:05 - train: epoch 0052, iter [02900, 05004], lr: 0.103043, loss: 1.7192
2022-08-02 07:30:01 - train: epoch 0052, iter [03000, 05004], lr: 0.102977, loss: 1.7284
2022-08-02 07:30:57 - train: epoch 0052, iter [03100, 05004], lr: 0.102911, loss: 1.9109
2022-08-02 07:31:53 - train: epoch 0052, iter [03200, 05004], lr: 0.102845, loss: 1.6514
2022-08-02 07:32:49 - train: epoch 0052, iter [03300, 05004], lr: 0.102779, loss: 1.8126
2022-08-02 07:33:45 - train: epoch 0052, iter [03400, 05004], lr: 0.102713, loss: 1.9457
2022-08-02 07:34:42 - train: epoch 0052, iter [03500, 05004], lr: 0.102647, loss: 1.7684
2022-08-02 07:35:37 - train: epoch 0052, iter [03600, 05004], lr: 0.102581, loss: 1.9092
2022-08-02 07:36:34 - train: epoch 0052, iter [03700, 05004], lr: 0.102515, loss: 1.9235
2022-08-02 07:37:30 - train: epoch 0052, iter [03800, 05004], lr: 0.102449, loss: 1.7571
2022-08-02 07:38:26 - train: epoch 0052, iter [03900, 05004], lr: 0.102383, loss: 1.7100
2022-08-02 07:39:21 - train: epoch 0052, iter [04000, 05004], lr: 0.102317, loss: 1.9826
2022-08-02 07:40:17 - train: epoch 0052, iter [04100, 05004], lr: 0.102251, loss: 1.5698
2022-08-02 07:41:13 - train: epoch 0052, iter [04200, 05004], lr: 0.102185, loss: 1.8228
2022-08-02 07:42:09 - train: epoch 0052, iter [04300, 05004], lr: 0.102119, loss: 1.9362
2022-08-02 07:43:04 - train: epoch 0052, iter [04400, 05004], lr: 0.102052, loss: 2.0690
2022-08-02 07:44:00 - train: epoch 0052, iter [04500, 05004], lr: 0.101986, loss: 1.8196
2022-08-02 07:44:55 - train: epoch 0052, iter [04600, 05004], lr: 0.101920, loss: 1.8273
2022-08-02 07:45:51 - train: epoch 0052, iter [04700, 05004], lr: 0.101854, loss: 2.0193
2022-08-02 07:46:47 - train: epoch 0052, iter [04800, 05004], lr: 0.101788, loss: 1.5622
2022-08-02 07:47:42 - train: epoch 0052, iter [04900, 05004], lr: 0.101722, loss: 1.7588
2022-08-02 07:48:34 - train: epoch 0052, iter [05000, 05004], lr: 0.101656, loss: 1.8230
2022-08-02 07:48:36 - train: epoch 052, train_loss: 1.8603
2022-08-02 07:50:49 - eval: epoch: 052, acc1: 61.374%, acc5: 84.462%, test_loss: 1.5942, per_image_load_time: 3.117ms, per_image_inference_time: 0.580ms
2022-08-02 07:50:49 - until epoch: 052, best_acc1: 61.374%
2022-08-02 07:50:49 - epoch 053 lr: 0.101653
2022-08-02 07:51:59 - train: epoch 0053, iter [00100, 05004], lr: 0.101587, loss: 1.8706
2022-08-02 07:53:02 - train: epoch 0053, iter [00200, 05004], lr: 0.101521, loss: 1.9522
2022-08-02 07:54:04 - train: epoch 0053, iter [00300, 05004], lr: 0.101455, loss: 1.7760
2022-08-02 07:55:07 - train: epoch 0053, iter [00400, 05004], lr: 0.101389, loss: 1.8432
2022-08-02 07:56:07 - train: epoch 0053, iter [00500, 05004], lr: 0.101323, loss: 1.8176
2022-08-02 07:57:08 - train: epoch 0053, iter [00600, 05004], lr: 0.101257, loss: 1.8025
2022-08-02 07:58:08 - train: epoch 0053, iter [00700, 05004], lr: 0.101191, loss: 1.8321
2022-08-02 07:59:06 - train: epoch 0053, iter [00800, 05004], lr: 0.101125, loss: 1.9804
2022-08-02 08:00:01 - train: epoch 0053, iter [00900, 05004], lr: 0.101059, loss: 1.6540
2022-08-02 08:00:57 - train: epoch 0053, iter [01000, 05004], lr: 0.100993, loss: 1.7898
2022-08-02 08:01:53 - train: epoch 0053, iter [01100, 05004], lr: 0.100927, loss: 1.8913
2022-08-02 08:02:47 - train: epoch 0053, iter [01200, 05004], lr: 0.100860, loss: 1.7712
2022-08-02 08:03:42 - train: epoch 0053, iter [01300, 05004], lr: 0.100794, loss: 1.7671
2022-08-02 08:04:38 - train: epoch 0053, iter [01400, 05004], lr: 0.100728, loss: 2.0751
2022-08-02 08:05:32 - train: epoch 0053, iter [01500, 05004], lr: 0.100662, loss: 1.8133
2022-08-02 08:06:27 - train: epoch 0053, iter [01600, 05004], lr: 0.100596, loss: 1.8755
2022-08-02 08:07:23 - train: epoch 0053, iter [01700, 05004], lr: 0.100530, loss: 2.0382
2022-08-02 08:08:19 - train: epoch 0053, iter [01800, 05004], lr: 0.100464, loss: 1.9867
2022-08-02 08:09:14 - train: epoch 0053, iter [01900, 05004], lr: 0.100398, loss: 1.7947
2022-08-02 08:10:09 - train: epoch 0053, iter [02000, 05004], lr: 0.100332, loss: 1.9227
2022-08-02 08:11:06 - train: epoch 0053, iter [02100, 05004], lr: 0.100266, loss: 1.9284
2022-08-02 08:12:00 - train: epoch 0053, iter [02200, 05004], lr: 0.100200, loss: 1.9995
2022-08-02 08:12:57 - train: epoch 0053, iter [02300, 05004], lr: 0.100133, loss: 1.9029
2022-08-02 08:13:53 - train: epoch 0053, iter [02400, 05004], lr: 0.100067, loss: 1.8348
2022-08-02 08:14:50 - train: epoch 0053, iter [02500, 05004], lr: 0.100001, loss: 2.0358
2022-08-02 08:15:46 - train: epoch 0053, iter [02600, 05004], lr: 0.099935, loss: 1.9340
2022-08-02 08:16:42 - train: epoch 0053, iter [02700, 05004], lr: 0.099869, loss: 2.0065
2022-08-02 08:17:37 - train: epoch 0053, iter [02800, 05004], lr: 0.099803, loss: 1.9363
2022-08-02 08:18:34 - train: epoch 0053, iter [02900, 05004], lr: 0.099737, loss: 1.8072
2022-08-02 08:19:29 - train: epoch 0053, iter [03000, 05004], lr: 0.099671, loss: 1.6141
2022-08-02 08:20:25 - train: epoch 0053, iter [03100, 05004], lr: 0.099605, loss: 2.1989
2022-08-02 08:21:21 - train: epoch 0053, iter [03200, 05004], lr: 0.099539, loss: 2.1048
2022-08-02 08:22:17 - train: epoch 0053, iter [03300, 05004], lr: 0.099473, loss: 1.9673
2022-08-02 08:23:13 - train: epoch 0053, iter [03400, 05004], lr: 0.099407, loss: 1.8358
2022-08-02 08:24:10 - train: epoch 0053, iter [03500, 05004], lr: 0.099340, loss: 1.9524
2022-08-02 08:25:04 - train: epoch 0053, iter [03600, 05004], lr: 0.099274, loss: 1.7935
2022-08-02 08:26:02 - train: epoch 0053, iter [03700, 05004], lr: 0.099208, loss: 2.1583
2022-08-02 08:26:55 - train: epoch 0053, iter [03800, 05004], lr: 0.099142, loss: 1.7294
2022-08-02 08:27:51 - train: epoch 0053, iter [03900, 05004], lr: 0.099076, loss: 2.1141
2022-08-02 08:28:46 - train: epoch 0053, iter [04000, 05004], lr: 0.099010, loss: 1.7254
2022-08-02 08:29:41 - train: epoch 0053, iter [04100, 05004], lr: 0.098944, loss: 1.8862
2022-08-02 08:30:36 - train: epoch 0053, iter [04200, 05004], lr: 0.098878, loss: 2.1039
2022-08-02 08:31:33 - train: epoch 0053, iter [04300, 05004], lr: 0.098812, loss: 1.9319
2022-08-02 08:32:28 - train: epoch 0053, iter [04400, 05004], lr: 0.098746, loss: 1.8831
2022-08-02 08:33:23 - train: epoch 0053, iter [04500, 05004], lr: 0.098680, loss: 1.9844
2022-08-02 08:34:18 - train: epoch 0053, iter [04600, 05004], lr: 0.098614, loss: 1.6562
2022-08-02 08:35:13 - train: epoch 0053, iter [04700, 05004], lr: 0.098547, loss: 1.7544
2022-08-02 08:36:08 - train: epoch 0053, iter [04800, 05004], lr: 0.098481, loss: 2.1422
2022-08-02 08:37:03 - train: epoch 0053, iter [04900, 05004], lr: 0.098415, loss: 1.8337
2022-08-02 08:37:56 - train: epoch 0053, iter [05000, 05004], lr: 0.098349, loss: 1.7168
2022-08-02 08:37:58 - train: epoch 053, train_loss: 1.8464
2022-08-02 08:40:06 - eval: epoch: 053, acc1: 61.334%, acc5: 84.482%, test_loss: 1.6086, per_image_load_time: 4.290ms, per_image_inference_time: 0.553ms
2022-08-02 08:40:06 - until epoch: 053, best_acc1: 61.374%
2022-08-02 08:40:06 - epoch 054 lr: 0.098346
2022-08-02 08:41:14 - train: epoch 0054, iter [00100, 05004], lr: 0.098281, loss: 1.7403
2022-08-02 08:42:16 - train: epoch 0054, iter [00200, 05004], lr: 0.098214, loss: 1.8923
2022-08-02 08:43:15 - train: epoch 0054, iter [00300, 05004], lr: 0.098148, loss: 1.6801
2022-08-02 08:44:15 - train: epoch 0054, iter [00400, 05004], lr: 0.098082, loss: 1.7526
2022-08-02 08:45:15 - train: epoch 0054, iter [00500, 05004], lr: 0.098016, loss: 1.6999
2022-08-02 08:46:14 - train: epoch 0054, iter [00600, 05004], lr: 0.097950, loss: 1.8729
2022-08-02 08:47:12 - train: epoch 0054, iter [00700, 05004], lr: 0.097884, loss: 1.9666
2022-08-02 08:48:11 - train: epoch 0054, iter [00800, 05004], lr: 0.097818, loss: 1.9593
2022-08-02 08:49:08 - train: epoch 0054, iter [00900, 05004], lr: 0.097752, loss: 1.5792
2022-08-02 08:50:02 - train: epoch 0054, iter [01000, 05004], lr: 0.097686, loss: 1.5447
2022-08-02 08:50:58 - train: epoch 0054, iter [01100, 05004], lr: 0.097620, loss: 1.7649
2022-08-02 08:51:55 - train: epoch 0054, iter [01200, 05004], lr: 0.097554, loss: 2.0090
2022-08-02 08:52:49 - train: epoch 0054, iter [01300, 05004], lr: 0.097488, loss: 1.8428
2022-08-02 08:53:45 - train: epoch 0054, iter [01400, 05004], lr: 0.097422, loss: 1.8905
2022-08-02 08:54:40 - train: epoch 0054, iter [01500, 05004], lr: 0.097356, loss: 1.8284
2022-08-02 08:55:36 - train: epoch 0054, iter [01600, 05004], lr: 0.097289, loss: 1.7277
2022-08-02 08:56:31 - train: epoch 0054, iter [01700, 05004], lr: 0.097223, loss: 1.7656
2022-08-02 08:57:27 - train: epoch 0054, iter [01800, 05004], lr: 0.097157, loss: 1.6211
2022-08-02 08:58:22 - train: epoch 0054, iter [01900, 05004], lr: 0.097091, loss: 1.9300
2022-08-02 08:59:17 - train: epoch 0054, iter [02000, 05004], lr: 0.097025, loss: 1.9804
2022-08-02 09:00:13 - train: epoch 0054, iter [02100, 05004], lr: 0.096959, loss: 1.6681
2022-08-02 09:01:08 - train: epoch 0054, iter [02200, 05004], lr: 0.096893, loss: 1.7568
2022-08-02 09:02:03 - train: epoch 0054, iter [02300, 05004], lr: 0.096827, loss: 1.7425
2022-08-02 09:02:58 - train: epoch 0054, iter [02400, 05004], lr: 0.096761, loss: 1.9380
2022-08-02 09:03:54 - train: epoch 0054, iter [02500, 05004], lr: 0.096695, loss: 2.0833
2022-08-02 09:04:49 - train: epoch 0054, iter [02600, 05004], lr: 0.096629, loss: 1.6643
2022-08-02 09:05:44 - train: epoch 0054, iter [02700, 05004], lr: 0.096563, loss: 1.7974
2022-08-02 09:06:41 - train: epoch 0054, iter [02800, 05004], lr: 0.096497, loss: 2.0089
2022-08-02 09:07:35 - train: epoch 0054, iter [02900, 05004], lr: 0.096431, loss: 1.5620
2022-08-02 09:08:29 - train: epoch 0054, iter [03000, 05004], lr: 0.096365, loss: 1.8809
2022-08-02 09:09:25 - train: epoch 0054, iter [03100, 05004], lr: 0.096299, loss: 1.9521
2022-08-02 09:10:21 - train: epoch 0054, iter [03200, 05004], lr: 0.096233, loss: 2.1975
2022-08-02 09:11:15 - train: epoch 0054, iter [03300, 05004], lr: 0.096167, loss: 1.7908
2022-08-02 09:12:11 - train: epoch 0054, iter [03400, 05004], lr: 0.096101, loss: 1.6559
2022-08-02 09:13:06 - train: epoch 0054, iter [03500, 05004], lr: 0.096035, loss: 1.8598
2022-08-02 09:14:02 - train: epoch 0054, iter [03600, 05004], lr: 0.095969, loss: 1.7983
2022-08-02 09:14:58 - train: epoch 0054, iter [03700, 05004], lr: 0.095902, loss: 1.7100
2022-08-02 09:15:53 - train: epoch 0054, iter [03800, 05004], lr: 0.095836, loss: 1.5686
2022-08-02 09:16:53 - train: epoch 0054, iter [03900, 05004], lr: 0.095770, loss: 1.8411
2022-08-02 09:17:50 - train: epoch 0054, iter [04000, 05004], lr: 0.095704, loss: 1.7952
2022-08-02 09:18:48 - train: epoch 0054, iter [04100, 05004], lr: 0.095638, loss: 1.8777
2022-08-02 09:19:46 - train: epoch 0054, iter [04200, 05004], lr: 0.095572, loss: 1.8057
2022-08-02 09:20:43 - train: epoch 0054, iter [04300, 05004], lr: 0.095506, loss: 1.7532
2022-08-02 09:21:41 - train: epoch 0054, iter [04400, 05004], lr: 0.095440, loss: 1.8145
2022-08-02 09:22:40 - train: epoch 0054, iter [04500, 05004], lr: 0.095374, loss: 1.7228
2022-08-02 09:23:40 - train: epoch 0054, iter [04600, 05004], lr: 0.095308, loss: 1.9121
2022-08-02 09:24:37 - train: epoch 0054, iter [04700, 05004], lr: 0.095242, loss: 2.1080
2022-08-02 09:25:37 - train: epoch 0054, iter [04800, 05004], lr: 0.095176, loss: 2.0127
2022-08-02 09:26:35 - train: epoch 0054, iter [04900, 05004], lr: 0.095110, loss: 1.8547
2022-08-02 09:27:30 - train: epoch 0054, iter [05000, 05004], lr: 0.095044, loss: 1.8472
2022-08-02 09:27:31 - train: epoch 054, train_loss: 1.8351
2022-08-02 09:29:39 - eval: epoch: 054, acc1: 61.920%, acc5: 84.634%, test_loss: 1.5697, per_image_load_time: 3.308ms, per_image_inference_time: 0.631ms
2022-08-02 09:29:39 - until epoch: 054, best_acc1: 61.920%
2022-08-02 09:29:39 - epoch 055 lr: 0.095041
2022-08-02 09:30:46 - train: epoch 0055, iter [00100, 05004], lr: 0.094976, loss: 1.8094
2022-08-02 09:31:46 - train: epoch 0055, iter [00200, 05004], lr: 0.094910, loss: 1.6588
2022-08-02 09:32:45 - train: epoch 0055, iter [00300, 05004], lr: 0.094844, loss: 1.7574
2022-08-02 09:33:45 - train: epoch 0055, iter [00400, 05004], lr: 0.094778, loss: 1.6237
2022-08-02 09:34:41 - train: epoch 0055, iter [00500, 05004], lr: 0.094712, loss: 1.5707
2022-08-02 09:35:38 - train: epoch 0055, iter [00600, 05004], lr: 0.094646, loss: 1.8515
2022-08-02 09:36:38 - train: epoch 0055, iter [00700, 05004], lr: 0.094580, loss: 1.7680
2022-08-02 09:37:34 - train: epoch 0055, iter [00800, 05004], lr: 0.094514, loss: 1.5839
2022-08-02 09:38:33 - train: epoch 0055, iter [00900, 05004], lr: 0.094448, loss: 1.7156
2022-08-02 09:39:31 - train: epoch 0055, iter [01000, 05004], lr: 0.094382, loss: 1.8316
2022-08-02 09:40:30 - train: epoch 0055, iter [01100, 05004], lr: 0.094316, loss: 1.8249
2022-08-02 09:41:28 - train: epoch 0055, iter [01200, 05004], lr: 0.094250, loss: 1.7456
2022-08-02 09:42:28 - train: epoch 0055, iter [01300, 05004], lr: 0.094184, loss: 1.9467
2022-08-02 09:43:24 - train: epoch 0055, iter [01400, 05004], lr: 0.094118, loss: 1.6866
2022-08-02 09:44:24 - train: epoch 0055, iter [01500, 05004], lr: 0.094052, loss: 1.6231
2022-08-02 09:45:24 - train: epoch 0055, iter [01600, 05004], lr: 0.093986, loss: 1.9020
2022-08-02 09:46:21 - train: epoch 0055, iter [01700, 05004], lr: 0.093920, loss: 1.8880
2022-08-02 09:47:20 - train: epoch 0055, iter [01800, 05004], lr: 0.093854, loss: 1.8197
2022-08-02 09:48:19 - train: epoch 0055, iter [01900, 05004], lr: 0.093788, loss: 1.8212
2022-08-02 09:49:18 - train: epoch 0055, iter [02000, 05004], lr: 0.093722, loss: 1.9182
2022-08-02 09:50:17 - train: epoch 0055, iter [02100, 05004], lr: 0.093656, loss: 1.5810
2022-08-02 09:51:17 - train: epoch 0055, iter [02200, 05004], lr: 0.093590, loss: 1.8106
2022-08-02 09:52:15 - train: epoch 0055, iter [02300, 05004], lr: 0.093524, loss: 1.9810
2022-08-02 09:53:13 - train: epoch 0055, iter [02400, 05004], lr: 0.093458, loss: 1.7394
2022-08-02 09:54:12 - train: epoch 0055, iter [02500, 05004], lr: 0.093392, loss: 1.8590
2022-08-02 09:55:11 - train: epoch 0055, iter [02600, 05004], lr: 0.093326, loss: 1.7441
2022-08-02 09:56:10 - train: epoch 0055, iter [02700, 05004], lr: 0.093260, loss: 1.8328
2022-08-02 09:57:10 - train: epoch 0055, iter [02800, 05004], lr: 0.093194, loss: 1.9835
2022-08-02 09:58:08 - train: epoch 0055, iter [02900, 05004], lr: 0.093129, loss: 1.8522
2022-08-02 09:59:06 - train: epoch 0055, iter [03000, 05004], lr: 0.093063, loss: 1.9092
2022-08-02 10:00:05 - train: epoch 0055, iter [03100, 05004], lr: 0.092997, loss: 1.6919
2022-08-02 10:01:03 - train: epoch 0055, iter [03200, 05004], lr: 0.092931, loss: 1.6927
2022-08-02 10:02:03 - train: epoch 0055, iter [03300, 05004], lr: 0.092865, loss: 1.6716
2022-08-02 10:03:01 - train: epoch 0055, iter [03400, 05004], lr: 0.092799, loss: 1.8741
2022-08-02 10:04:02 - train: epoch 0055, iter [03500, 05004], lr: 0.092733, loss: 1.6154
2022-08-02 10:05:00 - train: epoch 0055, iter [03600, 05004], lr: 0.092667, loss: 1.8470
2022-08-02 10:05:59 - train: epoch 0055, iter [03700, 05004], lr: 0.092601, loss: 1.7722
2022-08-02 10:06:57 - train: epoch 0055, iter [03800, 05004], lr: 0.092535, loss: 1.9525
2022-08-02 10:07:56 - train: epoch 0055, iter [03900, 05004], lr: 0.092469, loss: 2.1158
2022-08-02 10:08:55 - train: epoch 0055, iter [04000, 05004], lr: 0.092403, loss: 1.6447
2022-08-02 10:09:52 - train: epoch 0055, iter [04100, 05004], lr: 0.092338, loss: 1.7868
2022-08-02 10:10:53 - train: epoch 0055, iter [04200, 05004], lr: 0.092272, loss: 1.9222
2022-08-02 10:11:54 - train: epoch 0055, iter [04300, 05004], lr: 0.092206, loss: 2.0212
2022-08-02 10:12:51 - train: epoch 0055, iter [04400, 05004], lr: 0.092140, loss: 1.8893
2022-08-02 10:13:51 - train: epoch 0055, iter [04500, 05004], lr: 0.092074, loss: 1.8041
2022-08-02 10:14:46 - train: epoch 0055, iter [04600, 05004], lr: 0.092008, loss: 1.7807
2022-08-02 10:15:43 - train: epoch 0055, iter [04700, 05004], lr: 0.091942, loss: 1.8122
2022-08-02 10:16:37 - train: epoch 0055, iter [04800, 05004], lr: 0.091876, loss: 1.6958
2022-08-02 10:17:34 - train: epoch 0055, iter [04900, 05004], lr: 0.091811, loss: 1.7977
2022-08-02 10:18:27 - train: epoch 0055, iter [05000, 05004], lr: 0.091745, loss: 1.8760
2022-08-02 10:18:29 - train: epoch 055, train_loss: 1.8184
2022-08-02 10:20:27 - eval: epoch: 055, acc1: 62.884%, acc5: 85.382%, test_loss: 1.5369, per_image_load_time: 4.066ms, per_image_inference_time: 0.544ms
2022-08-02 10:20:27 - until epoch: 055, best_acc1: 62.884%
2022-08-02 10:20:27 - epoch 056 lr: 0.091741
2022-08-02 10:21:32 - train: epoch 0056, iter [00100, 05004], lr: 0.091676, loss: 1.8661
2022-08-02 10:22:27 - train: epoch 0056, iter [00200, 05004], lr: 0.091610, loss: 1.5267
2022-08-02 10:23:21 - train: epoch 0056, iter [00300, 05004], lr: 0.091545, loss: 1.6434
2022-08-02 10:24:17 - train: epoch 0056, iter [00400, 05004], lr: 0.091479, loss: 1.8411
2022-08-02 10:25:12 - train: epoch 0056, iter [00500, 05004], lr: 0.091413, loss: 1.6420
2022-08-02 10:26:06 - train: epoch 0056, iter [00600, 05004], lr: 0.091347, loss: 1.7221
2022-08-02 10:27:02 - train: epoch 0056, iter [00700, 05004], lr: 0.091281, loss: 1.8295
2022-08-02 10:27:55 - train: epoch 0056, iter [00800, 05004], lr: 0.091215, loss: 2.0037
2022-08-02 10:28:51 - train: epoch 0056, iter [00900, 05004], lr: 0.091149, loss: 1.8747
2022-08-02 10:29:46 - train: epoch 0056, iter [01000, 05004], lr: 0.091084, loss: 1.7519
2022-08-02 10:30:42 - train: epoch 0056, iter [01100, 05004], lr: 0.091018, loss: 1.5512
2022-08-02 10:31:37 - train: epoch 0056, iter [01200, 05004], lr: 0.090952, loss: 1.8434
2022-08-02 10:32:32 - train: epoch 0056, iter [01300, 05004], lr: 0.090886, loss: 2.0474
2022-08-02 10:33:28 - train: epoch 0056, iter [01400, 05004], lr: 0.090820, loss: 1.8113
2022-08-02 10:34:24 - train: epoch 0056, iter [01500, 05004], lr: 0.090755, loss: 2.0497
2022-08-02 10:35:18 - train: epoch 0056, iter [01600, 05004], lr: 0.090689, loss: 1.6313
2022-08-02 10:36:15 - train: epoch 0056, iter [01700, 05004], lr: 0.090623, loss: 1.7880
2022-08-02 10:37:10 - train: epoch 0056, iter [01800, 05004], lr: 0.090557, loss: 1.9152
2022-08-02 10:38:05 - train: epoch 0056, iter [01900, 05004], lr: 0.090491, loss: 1.7889
2022-08-02 10:39:01 - train: epoch 0056, iter [02000, 05004], lr: 0.090426, loss: 1.6762
2022-08-02 10:39:56 - train: epoch 0056, iter [02100, 05004], lr: 0.090360, loss: 1.8480
2022-08-02 10:40:52 - train: epoch 0056, iter [02200, 05004], lr: 0.090294, loss: 2.0720
2022-08-02 10:41:47 - train: epoch 0056, iter [02300, 05004], lr: 0.090228, loss: 1.8898
2022-08-02 10:42:41 - train: epoch 0056, iter [02400, 05004], lr: 0.090163, loss: 1.7433
2022-08-02 10:43:37 - train: epoch 0056, iter [02500, 05004], lr: 0.090097, loss: 1.8754
2022-08-02 10:44:32 - train: epoch 0056, iter [02600, 05004], lr: 0.090031, loss: 1.7747
2022-08-02 10:45:28 - train: epoch 0056, iter [02700, 05004], lr: 0.089965, loss: 1.7727
2022-08-02 10:46:23 - train: epoch 0056, iter [02800, 05004], lr: 0.089899, loss: 1.7775
2022-08-02 10:47:18 - train: epoch 0056, iter [02900, 05004], lr: 0.089834, loss: 1.7456
2022-08-02 10:48:15 - train: epoch 0056, iter [03000, 05004], lr: 0.089768, loss: 2.0161
2022-08-02 10:49:09 - train: epoch 0056, iter [03100, 05004], lr: 0.089702, loss: 1.6985
2022-08-02 10:50:04 - train: epoch 0056, iter [03200, 05004], lr: 0.089637, loss: 1.5200
2022-08-02 10:51:00 - train: epoch 0056, iter [03300, 05004], lr: 0.089571, loss: 2.0034
2022-08-02 10:51:55 - train: epoch 0056, iter [03400, 05004], lr: 0.089505, loss: 1.6901
2022-08-02 10:52:50 - train: epoch 0056, iter [03500, 05004], lr: 0.089439, loss: 1.7757
2022-08-02 10:53:44 - train: epoch 0056, iter [03600, 05004], lr: 0.089374, loss: 1.6564
2022-08-02 10:54:43 - train: epoch 0056, iter [03700, 05004], lr: 0.089308, loss: 1.8608
2022-08-02 10:55:37 - train: epoch 0056, iter [03800, 05004], lr: 0.089242, loss: 1.8020
2022-08-02 10:56:33 - train: epoch 0056, iter [03900, 05004], lr: 0.089177, loss: 2.0607
2022-08-02 10:57:27 - train: epoch 0056, iter [04000, 05004], lr: 0.089111, loss: 1.8979
2022-08-02 10:58:24 - train: epoch 0056, iter [04100, 05004], lr: 0.089045, loss: 1.9565
2022-08-02 10:59:18 - train: epoch 0056, iter [04200, 05004], lr: 0.088979, loss: 1.6884
2022-08-02 11:00:14 - train: epoch 0056, iter [04300, 05004], lr: 0.088914, loss: 1.7769
2022-08-02 11:01:10 - train: epoch 0056, iter [04400, 05004], lr: 0.088848, loss: 1.9538
2022-08-02 11:02:05 - train: epoch 0056, iter [04500, 05004], lr: 0.088782, loss: 1.9321
2022-08-02 11:03:01 - train: epoch 0056, iter [04600, 05004], lr: 0.088717, loss: 1.8840
2022-08-02 11:03:56 - train: epoch 0056, iter [04700, 05004], lr: 0.088651, loss: 1.7776
2022-08-02 11:04:51 - train: epoch 0056, iter [04800, 05004], lr: 0.088585, loss: 1.9390
2022-08-02 11:05:46 - train: epoch 0056, iter [04900, 05004], lr: 0.088520, loss: 1.9527
2022-08-02 11:06:39 - train: epoch 0056, iter [05000, 05004], lr: 0.088454, loss: 1.6884
2022-08-02 11:06:40 - train: epoch 056, train_loss: 1.8061
2022-08-02 11:08:41 - eval: epoch: 056, acc1: 62.534%, acc5: 85.256%, test_loss: 1.5478, per_image_load_time: 3.389ms, per_image_inference_time: 0.586ms
2022-08-02 11:08:41 - until epoch: 056, best_acc1: 62.884%
2022-08-02 11:08:41 - epoch 057 lr: 0.088451
2022-08-02 11:09:45 - train: epoch 0057, iter [00100, 05004], lr: 0.088386, loss: 1.6024
2022-08-02 11:10:41 - train: epoch 0057, iter [00200, 05004], lr: 0.088320, loss: 1.7166
2022-08-02 11:11:34 - train: epoch 0057, iter [00300, 05004], lr: 0.088255, loss: 1.7222
2022-08-02 11:12:30 - train: epoch 0057, iter [00400, 05004], lr: 0.088189, loss: 1.8730
2022-08-02 11:13:26 - train: epoch 0057, iter [00500, 05004], lr: 0.088123, loss: 1.4736
2022-08-02 11:14:21 - train: epoch 0057, iter [00600, 05004], lr: 0.088058, loss: 1.6764
2022-08-02 11:15:16 - train: epoch 0057, iter [00700, 05004], lr: 0.087992, loss: 1.3704
2022-08-02 11:16:12 - train: epoch 0057, iter [00800, 05004], lr: 0.087927, loss: 1.7777
2022-08-02 11:17:05 - train: epoch 0057, iter [00900, 05004], lr: 0.087861, loss: 1.7293
2022-08-02 11:17:59 - train: epoch 0057, iter [01000, 05004], lr: 0.087795, loss: 1.6173
2022-08-02 11:18:55 - train: epoch 0057, iter [01100, 05004], lr: 0.087730, loss: 1.4949
2022-08-02 11:19:51 - train: epoch 0057, iter [01200, 05004], lr: 0.087664, loss: 1.7871
2022-08-02 11:20:44 - train: epoch 0057, iter [01300, 05004], lr: 0.087599, loss: 1.9165
2022-08-02 11:21:42 - train: epoch 0057, iter [01400, 05004], lr: 0.087533, loss: 1.9331
2022-08-02 11:22:36 - train: epoch 0057, iter [01500, 05004], lr: 0.087467, loss: 1.7936
2022-08-02 11:23:31 - train: epoch 0057, iter [01600, 05004], lr: 0.087402, loss: 1.9736
2022-08-02 11:24:26 - train: epoch 0057, iter [01700, 05004], lr: 0.087336, loss: 1.9803
2022-08-02 11:25:21 - train: epoch 0057, iter [01800, 05004], lr: 0.087271, loss: 1.8143
2022-08-02 11:26:16 - train: epoch 0057, iter [01900, 05004], lr: 0.087205, loss: 1.7783
2022-08-02 11:27:12 - train: epoch 0057, iter [02000, 05004], lr: 0.087140, loss: 1.7155
2022-08-02 11:28:08 - train: epoch 0057, iter [02100, 05004], lr: 0.087074, loss: 1.7754
2022-08-02 11:29:04 - train: epoch 0057, iter [02200, 05004], lr: 0.087009, loss: 1.6989
2022-08-02 11:30:00 - train: epoch 0057, iter [02300, 05004], lr: 0.086943, loss: 1.8727
2022-08-02 11:30:57 - train: epoch 0057, iter [02400, 05004], lr: 0.086878, loss: 1.8004
2022-08-02 11:31:52 - train: epoch 0057, iter [02500, 05004], lr: 0.086812, loss: 1.9109
2022-08-02 11:32:48 - train: epoch 0057, iter [02600, 05004], lr: 0.086747, loss: 1.7050
2022-08-02 11:33:45 - train: epoch 0057, iter [02700, 05004], lr: 0.086681, loss: 1.6038
2022-08-02 11:34:40 - train: epoch 0057, iter [02800, 05004], lr: 0.086616, loss: 1.6240
2022-08-02 11:35:35 - train: epoch 0057, iter [02900, 05004], lr: 0.086550, loss: 1.6937
2022-08-02 11:36:31 - train: epoch 0057, iter [03000, 05004], lr: 0.086485, loss: 1.9343
2022-08-02 11:37:27 - train: epoch 0057, iter [03100, 05004], lr: 0.086419, loss: 1.9656
2022-08-02 11:38:26 - train: epoch 0057, iter [03200, 05004], lr: 0.086354, loss: 1.9293
2022-08-02 11:39:20 - train: epoch 0057, iter [03300, 05004], lr: 0.086288, loss: 1.7749
2022-08-02 11:40:16 - train: epoch 0057, iter [03400, 05004], lr: 0.086223, loss: 2.0697
2022-08-02 11:41:12 - train: epoch 0057, iter [03500, 05004], lr: 0.086157, loss: 1.8560
2022-08-02 11:42:08 - train: epoch 0057, iter [03600, 05004], lr: 0.086092, loss: 1.7515
2022-08-02 11:43:04 - train: epoch 0057, iter [03700, 05004], lr: 0.086026, loss: 1.7164
2022-08-02 11:43:59 - train: epoch 0057, iter [03800, 05004], lr: 0.085961, loss: 1.8049
2022-08-02 11:44:56 - train: epoch 0057, iter [03900, 05004], lr: 0.085896, loss: 1.8716
2022-08-02 11:45:52 - train: epoch 0057, iter [04000, 05004], lr: 0.085830, loss: 1.6249
2022-08-02 11:46:48 - train: epoch 0057, iter [04100, 05004], lr: 0.085765, loss: 1.9479
2022-08-02 11:47:45 - train: epoch 0057, iter [04200, 05004], lr: 0.085699, loss: 1.6961
2022-08-02 11:48:41 - train: epoch 0057, iter [04300, 05004], lr: 0.085634, loss: 1.7618
2022-08-02 11:49:35 - train: epoch 0057, iter [04400, 05004], lr: 0.085568, loss: 1.7953
2022-08-02 11:50:31 - train: epoch 0057, iter [04500, 05004], lr: 0.085503, loss: 1.8971
2022-08-02 11:51:27 - train: epoch 0057, iter [04600, 05004], lr: 0.085438, loss: 1.5635
2022-08-02 11:52:23 - train: epoch 0057, iter [04700, 05004], lr: 0.085372, loss: 1.8839
2022-08-02 11:53:21 - train: epoch 0057, iter [04800, 05004], lr: 0.085307, loss: 2.1870
2022-08-02 11:54:15 - train: epoch 0057, iter [04900, 05004], lr: 0.085242, loss: 1.9969
2022-08-02 11:55:08 - train: epoch 0057, iter [05000, 05004], lr: 0.085176, loss: 1.9174
2022-08-02 11:55:09 - train: epoch 057, train_loss: 1.7910
2022-08-02 11:57:11 - eval: epoch: 057, acc1: 62.552%, acc5: 85.258%, test_loss: 1.5421, per_image_load_time: 3.321ms, per_image_inference_time: 0.591ms
2022-08-02 11:57:11 - until epoch: 057, best_acc1: 62.884%
2022-08-02 11:57:11 - epoch 058 lr: 0.085173
2022-08-02 11:58:16 - train: epoch 0058, iter [00100, 05004], lr: 0.085108, loss: 1.8183
2022-08-02 11:59:10 - train: epoch 0058, iter [00200, 05004], lr: 0.085043, loss: 1.7128
2022-08-02 12:00:07 - train: epoch 0058, iter [00300, 05004], lr: 0.084978, loss: 1.8341
2022-08-02 12:01:02 - train: epoch 0058, iter [00400, 05004], lr: 0.084912, loss: 1.7633
2022-08-02 12:01:59 - train: epoch 0058, iter [00500, 05004], lr: 0.084847, loss: 1.5905
2022-08-02 12:02:54 - train: epoch 0058, iter [00600, 05004], lr: 0.084782, loss: 2.0279
2022-08-02 12:03:50 - train: epoch 0058, iter [00700, 05004], lr: 0.084716, loss: 1.8443
2022-08-02 12:04:46 - train: epoch 0058, iter [00800, 05004], lr: 0.084651, loss: 1.6505
2022-08-02 12:05:42 - train: epoch 0058, iter [00900, 05004], lr: 0.084586, loss: 1.4974
2022-08-02 12:06:38 - train: epoch 0058, iter [01000, 05004], lr: 0.084520, loss: 1.6338
2022-08-02 12:07:34 - train: epoch 0058, iter [01100, 05004], lr: 0.084455, loss: 1.6817
2022-08-02 12:08:29 - train: epoch 0058, iter [01200, 05004], lr: 0.084390, loss: 1.7746
2022-08-02 12:09:26 - train: epoch 0058, iter [01300, 05004], lr: 0.084325, loss: 1.9366
2022-08-02 12:10:22 - train: epoch 0058, iter [01400, 05004], lr: 0.084259, loss: 1.7622
2022-08-02 12:11:19 - train: epoch 0058, iter [01500, 05004], lr: 0.084194, loss: 1.6448
2022-08-02 12:12:15 - train: epoch 0058, iter [01600, 05004], lr: 0.084129, loss: 1.7869
2022-08-02 12:13:10 - train: epoch 0058, iter [01700, 05004], lr: 0.084064, loss: 1.6924
2022-08-02 12:14:05 - train: epoch 0058, iter [01800, 05004], lr: 0.083998, loss: 1.8738
2022-08-02 12:15:01 - train: epoch 0058, iter [01900, 05004], lr: 0.083933, loss: 1.9287
2022-08-02 12:15:58 - train: epoch 0058, iter [02000, 05004], lr: 0.083868, loss: 1.8583
2022-08-02 12:16:54 - train: epoch 0058, iter [02100, 05004], lr: 0.083803, loss: 1.7598
2022-08-02 12:17:50 - train: epoch 0058, iter [02200, 05004], lr: 0.083737, loss: 1.8164
2022-08-02 12:18:44 - train: epoch 0058, iter [02300, 05004], lr: 0.083672, loss: 1.7343
2022-08-02 12:19:42 - train: epoch 0058, iter [02400, 05004], lr: 0.083607, loss: 1.8694
2022-08-02 12:20:39 - train: epoch 0058, iter [02500, 05004], lr: 0.083542, loss: 1.7433
2022-08-02 12:21:32 - train: epoch 0058, iter [02600, 05004], lr: 0.083477, loss: 1.6839
2022-08-02 12:22:28 - train: epoch 0058, iter [02700, 05004], lr: 0.083411, loss: 1.8471
2022-08-02 12:23:24 - train: epoch 0058, iter [02800, 05004], lr: 0.083346, loss: 1.7655
2022-08-02 12:24:20 - train: epoch 0058, iter [02900, 05004], lr: 0.083281, loss: 1.6750
2022-08-02 12:25:15 - train: epoch 0058, iter [03000, 05004], lr: 0.083216, loss: 2.0432
2022-08-02 12:26:11 - train: epoch 0058, iter [03100, 05004], lr: 0.083151, loss: 1.7060
2022-08-02 12:27:07 - train: epoch 0058, iter [03200, 05004], lr: 0.083086, loss: 1.4735
2022-08-02 12:28:05 - train: epoch 0058, iter [03300, 05004], lr: 0.083021, loss: 1.7564
2022-08-02 12:29:00 - train: epoch 0058, iter [03400, 05004], lr: 0.082955, loss: 1.7768
2022-08-02 12:29:57 - train: epoch 0058, iter [03500, 05004], lr: 0.082890, loss: 1.7212
2022-08-02 12:30:52 - train: epoch 0058, iter [03600, 05004], lr: 0.082825, loss: 1.6359
2022-08-02 12:31:47 - train: epoch 0058, iter [03700, 05004], lr: 0.082760, loss: 1.6982
2022-08-02 12:32:43 - train: epoch 0058, iter [03800, 05004], lr: 0.082695, loss: 1.6398
2022-08-02 12:33:38 - train: epoch 0058, iter [03900, 05004], lr: 0.082630, loss: 1.6877
2022-08-02 12:34:36 - train: epoch 0058, iter [04000, 05004], lr: 0.082565, loss: 1.6698
2022-08-02 12:35:31 - train: epoch 0058, iter [04100, 05004], lr: 0.082500, loss: 1.8070
2022-08-02 12:36:26 - train: epoch 0058, iter [04200, 05004], lr: 0.082435, loss: 1.5547
2022-08-02 12:37:22 - train: epoch 0058, iter [04300, 05004], lr: 0.082370, loss: 1.9541
2022-08-02 12:38:18 - train: epoch 0058, iter [04400, 05004], lr: 0.082305, loss: 1.7008
2022-08-02 12:39:14 - train: epoch 0058, iter [04500, 05004], lr: 0.082240, loss: 1.8281
2022-08-02 12:40:11 - train: epoch 0058, iter [04600, 05004], lr: 0.082175, loss: 1.6367
2022-08-02 12:41:06 - train: epoch 0058, iter [04700, 05004], lr: 0.082110, loss: 1.7362
2022-08-02 12:42:01 - train: epoch 0058, iter [04800, 05004], lr: 0.082045, loss: 1.9555
2022-08-02 12:42:57 - train: epoch 0058, iter [04900, 05004], lr: 0.081980, loss: 1.8306
2022-08-02 12:43:50 - train: epoch 0058, iter [05000, 05004], lr: 0.081915, loss: 1.5693
2022-08-02 12:43:52 - train: epoch 058, train_loss: 1.7768
2022-08-02 12:45:54 - eval: epoch: 058, acc1: 63.254%, acc5: 85.402%, test_loss: 1.5146, per_image_load_time: 3.245ms, per_image_inference_time: 0.547ms
2022-08-02 12:45:54 - until epoch: 058, best_acc1: 63.254%
2022-08-02 12:45:54 - epoch 059 lr: 0.081911
2022-08-02 12:46:59 - train: epoch 0059, iter [00100, 05004], lr: 0.081847, loss: 1.6794
2022-08-02 12:47:53 - train: epoch 0059, iter [00200, 05004], lr: 0.081782, loss: 1.7731
2022-08-02 12:48:49 - train: epoch 0059, iter [00300, 05004], lr: 0.081717, loss: 1.6095
2022-08-02 12:49:47 - train: epoch 0059, iter [00400, 05004], lr: 0.081652, loss: 1.8966
2022-08-02 12:50:42 - train: epoch 0059, iter [00500, 05004], lr: 0.081587, loss: 1.8422
2022-08-02 12:51:39 - train: epoch 0059, iter [00600, 05004], lr: 0.081522, loss: 1.7324
2022-08-02 12:52:33 - train: epoch 0059, iter [00700, 05004], lr: 0.081457, loss: 1.7634
2022-08-02 12:53:30 - train: epoch 0059, iter [00800, 05004], lr: 0.081392, loss: 1.6655
2022-08-02 12:54:25 - train: epoch 0059, iter [00900, 05004], lr: 0.081327, loss: 1.5353
2022-08-02 12:55:22 - train: epoch 0059, iter [01000, 05004], lr: 0.081262, loss: 2.1407
2022-08-02 12:56:17 - train: epoch 0059, iter [01100, 05004], lr: 0.081197, loss: 1.8839
2022-08-02 12:57:13 - train: epoch 0059, iter [01200, 05004], lr: 0.081133, loss: 1.7486
2022-08-02 12:58:10 - train: epoch 0059, iter [01300, 05004], lr: 0.081068, loss: 2.0842
2022-08-02 12:59:08 - train: epoch 0059, iter [01400, 05004], lr: 0.081003, loss: 1.9092
2022-08-02 13:00:03 - train: epoch 0059, iter [01500, 05004], lr: 0.080938, loss: 1.8271
2022-08-02 13:00:59 - train: epoch 0059, iter [01600, 05004], lr: 0.080873, loss: 1.7592
2022-08-02 13:01:55 - train: epoch 0059, iter [01700, 05004], lr: 0.080808, loss: 1.7986
2022-08-02 13:02:51 - train: epoch 0059, iter [01800, 05004], lr: 0.080743, loss: 1.6784
2022-08-02 13:03:47 - train: epoch 0059, iter [01900, 05004], lr: 0.080678, loss: 1.5729
2022-08-02 13:04:43 - train: epoch 0059, iter [02000, 05004], lr: 0.080614, loss: 1.7123
2022-08-02 13:05:40 - train: epoch 0059, iter [02100, 05004], lr: 0.080549, loss: 1.9337
2022-08-02 13:06:36 - train: epoch 0059, iter [02200, 05004], lr: 0.080484, loss: 1.6950
2022-08-02 13:07:32 - train: epoch 0059, iter [02300, 05004], lr: 0.080419, loss: 1.7808
2022-08-02 13:08:28 - train: epoch 0059, iter [02400, 05004], lr: 0.080354, loss: 1.9267
2022-08-02 13:09:24 - train: epoch 0059, iter [02500, 05004], lr: 0.080290, loss: 1.9490
2022-08-02 13:10:20 - train: epoch 0059, iter [02600, 05004], lr: 0.080225, loss: 1.5713
2022-08-02 13:11:16 - train: epoch 0059, iter [02700, 05004], lr: 0.080160, loss: 1.7580
2022-08-02 13:12:14 - train: epoch 0059, iter [02800, 05004], lr: 0.080095, loss: 1.7931
2022-08-02 13:13:09 - train: epoch 0059, iter [02900, 05004], lr: 0.080031, loss: 1.7398
2022-08-02 13:14:06 - train: epoch 0059, iter [03000, 05004], lr: 0.079966, loss: 1.9031
2022-08-02 13:15:01 - train: epoch 0059, iter [03100, 05004], lr: 0.079901, loss: 1.7811
2022-08-02 13:15:57 - train: epoch 0059, iter [03200, 05004], lr: 0.079836, loss: 1.9498
2022-08-02 13:16:53 - train: epoch 0059, iter [03300, 05004], lr: 0.079772, loss: 1.6699
2022-08-02 13:17:49 - train: epoch 0059, iter [03400, 05004], lr: 0.079707, loss: 2.1607
2022-08-02 13:18:43 - train: epoch 0059, iter [03500, 05004], lr: 0.079642, loss: 1.5981
2022-08-02 13:19:40 - train: epoch 0059, iter [03600, 05004], lr: 0.079577, loss: 1.9853
2022-08-02 13:20:37 - train: epoch 0059, iter [03700, 05004], lr: 0.079513, loss: 1.7663
2022-08-02 13:21:32 - train: epoch 0059, iter [03800, 05004], lr: 0.079448, loss: 1.6662
2022-08-02 13:22:28 - train: epoch 0059, iter [03900, 05004], lr: 0.079383, loss: 1.4832
2022-08-02 13:23:24 - train: epoch 0059, iter [04000, 05004], lr: 0.079319, loss: 1.9123
2022-08-02 13:24:21 - train: epoch 0059, iter [04100, 05004], lr: 0.079254, loss: 1.5442
2022-08-02 13:25:16 - train: epoch 0059, iter [04200, 05004], lr: 0.079189, loss: 1.5851
2022-08-02 13:26:14 - train: epoch 0059, iter [04300, 05004], lr: 0.079125, loss: 1.8847
2022-08-02 13:27:08 - train: epoch 0059, iter [04400, 05004], lr: 0.079060, loss: 1.7695
2022-08-02 13:28:05 - train: epoch 0059, iter [04500, 05004], lr: 0.078996, loss: 1.8228
2022-08-02 13:29:02 - train: epoch 0059, iter [04600, 05004], lr: 0.078931, loss: 2.0186
2022-08-02 13:29:58 - train: epoch 0059, iter [04700, 05004], lr: 0.078866, loss: 1.7484
2022-08-02 13:30:54 - train: epoch 0059, iter [04800, 05004], lr: 0.078802, loss: 1.6533
2022-08-02 13:31:51 - train: epoch 0059, iter [04900, 05004], lr: 0.078737, loss: 1.9489
2022-08-02 13:32:43 - train: epoch 0059, iter [05000, 05004], lr: 0.078673, loss: 1.9546
2022-08-02 13:32:45 - train: epoch 059, train_loss: 1.7624
2022-08-02 13:34:48 - eval: epoch: 059, acc1: 62.718%, acc5: 85.368%, test_loss: 1.5336, per_image_load_time: 3.198ms, per_image_inference_time: 0.582ms
2022-08-02 13:34:48 - until epoch: 059, best_acc1: 63.254%
2022-08-02 13:34:48 - epoch 060 lr: 0.078669
2022-08-02 13:35:55 - train: epoch 0060, iter [00100, 05004], lr: 0.078605, loss: 1.6607
2022-08-02 13:36:50 - train: epoch 0060, iter [00200, 05004], lr: 0.078541, loss: 1.7066
2022-08-02 13:37:46 - train: epoch 0060, iter [00300, 05004], lr: 0.078476, loss: 1.5595
2022-08-02 13:38:42 - train: epoch 0060, iter [00400, 05004], lr: 0.078412, loss: 1.6542
2022-08-02 13:39:38 - train: epoch 0060, iter [00500, 05004], lr: 0.078347, loss: 1.7936
2022-08-02 13:40:34 - train: epoch 0060, iter [00600, 05004], lr: 0.078283, loss: 1.7653
2022-08-02 13:41:31 - train: epoch 0060, iter [00700, 05004], lr: 0.078218, loss: 1.8969
2022-08-02 13:42:27 - train: epoch 0060, iter [00800, 05004], lr: 0.078154, loss: 1.6005
2022-08-02 13:43:23 - train: epoch 0060, iter [00900, 05004], lr: 0.078089, loss: 1.4157
2022-08-02 13:44:19 - train: epoch 0060, iter [01000, 05004], lr: 0.078025, loss: 1.5445
2022-08-02 13:45:15 - train: epoch 0060, iter [01100, 05004], lr: 0.077960, loss: 1.7907
2022-08-02 13:46:12 - train: epoch 0060, iter [01200, 05004], lr: 0.077896, loss: 1.7295
2022-08-02 13:47:08 - train: epoch 0060, iter [01300, 05004], lr: 0.077831, loss: 1.5501
2022-08-02 13:48:03 - train: epoch 0060, iter [01400, 05004], lr: 0.077767, loss: 1.8314
2022-08-02 13:48:58 - train: epoch 0060, iter [01500, 05004], lr: 0.077703, loss: 1.7940
2022-08-02 13:49:55 - train: epoch 0060, iter [01600, 05004], lr: 0.077638, loss: 1.6780
2022-08-02 13:50:53 - train: epoch 0060, iter [01700, 05004], lr: 0.077574, loss: 1.9156
2022-08-02 13:51:49 - train: epoch 0060, iter [01800, 05004], lr: 0.077509, loss: 1.8826
2022-08-02 13:52:45 - train: epoch 0060, iter [01900, 05004], lr: 0.077445, loss: 1.9770
2022-08-02 13:53:41 - train: epoch 0060, iter [02000, 05004], lr: 0.077381, loss: 1.5730
2022-08-02 13:54:39 - train: epoch 0060, iter [02100, 05004], lr: 0.077316, loss: 1.9422
2022-08-02 13:55:34 - train: epoch 0060, iter [02200, 05004], lr: 0.077252, loss: 2.0361
2022-08-02 13:56:30 - train: epoch 0060, iter [02300, 05004], lr: 0.077188, loss: 1.7343
2022-08-02 13:57:26 - train: epoch 0060, iter [02400, 05004], lr: 0.077123, loss: 1.7337
2022-08-02 13:58:22 - train: epoch 0060, iter [02500, 05004], lr: 0.077059, loss: 1.8395
2022-08-02 13:59:19 - train: epoch 0060, iter [02600, 05004], lr: 0.076995, loss: 1.9645
2022-08-02 14:00:16 - train: epoch 0060, iter [02700, 05004], lr: 0.076930, loss: 1.8379
2022-08-02 14:01:12 - train: epoch 0060, iter [02800, 05004], lr: 0.076866, loss: 1.8050
2022-08-02 14:02:09 - train: epoch 0060, iter [02900, 05004], lr: 0.076802, loss: 1.7336
2022-08-02 14:03:04 - train: epoch 0060, iter [03000, 05004], lr: 0.076737, loss: 1.6862
2022-08-02 14:04:00 - train: epoch 0060, iter [03100, 05004], lr: 0.076673, loss: 1.8989
2022-08-02 14:04:58 - train: epoch 0060, iter [03200, 05004], lr: 0.076609, loss: 1.6520
2022-08-02 14:05:54 - train: epoch 0060, iter [03300, 05004], lr: 0.076545, loss: 1.5204
2022-08-02 14:06:49 - train: epoch 0060, iter [03400, 05004], lr: 0.076480, loss: 1.7964
2022-08-02 14:07:45 - train: epoch 0060, iter [03500, 05004], lr: 0.076416, loss: 1.8697
2022-08-02 14:08:40 - train: epoch 0060, iter [03600, 05004], lr: 0.076352, loss: 1.6794
2022-08-02 14:09:38 - train: epoch 0060, iter [03700, 05004], lr: 0.076288, loss: 1.7334
2022-08-02 14:10:34 - train: epoch 0060, iter [03800, 05004], lr: 0.076224, loss: 1.8340
2022-08-02 14:11:30 - train: epoch 0060, iter [03900, 05004], lr: 0.076159, loss: 1.7360
2022-08-02 14:12:28 - train: epoch 0060, iter [04000, 05004], lr: 0.076095, loss: 1.6370
2022-08-02 14:13:22 - train: epoch 0060, iter [04100, 05004], lr: 0.076031, loss: 1.9523
2022-08-02 14:14:19 - train: epoch 0060, iter [04200, 05004], lr: 0.075967, loss: 1.8115
2022-08-02 14:15:14 - train: epoch 0060, iter [04300, 05004], lr: 0.075903, loss: 1.7198
2022-08-02 14:16:12 - train: epoch 0060, iter [04400, 05004], lr: 0.075839, loss: 1.8475
2022-08-02 14:17:08 - train: epoch 0060, iter [04500, 05004], lr: 0.075774, loss: 1.7979
2022-08-02 14:18:03 - train: epoch 0060, iter [04600, 05004], lr: 0.075710, loss: 1.6386
2022-08-02 14:18:59 - train: epoch 0060, iter [04700, 05004], lr: 0.075646, loss: 1.6578
2022-08-02 14:19:56 - train: epoch 0060, iter [04800, 05004], lr: 0.075582, loss: 1.6705
2022-08-02 14:20:51 - train: epoch 0060, iter [04900, 05004], lr: 0.075518, loss: 1.7382
2022-08-02 14:21:44 - train: epoch 0060, iter [05000, 05004], lr: 0.075454, loss: 1.6548
2022-08-02 14:21:46 - train: epoch 060, train_loss: 1.7460
2022-08-02 14:23:49 - eval: epoch: 060, acc1: 63.656%, acc5: 85.980%, test_loss: 1.4905, per_image_load_time: 2.642ms, per_image_inference_time: 0.538ms
2022-08-02 14:23:49 - until epoch: 060, best_acc1: 63.656%
2022-08-02 14:23:49 - epoch 061 lr: 0.075451
2022-08-02 14:24:55 - train: epoch 0061, iter [00100, 05004], lr: 0.075387, loss: 1.5537
2022-08-02 14:25:51 - train: epoch 0061, iter [00200, 05004], lr: 0.075323, loss: 1.7467
2022-08-02 14:26:46 - train: epoch 0061, iter [00300, 05004], lr: 0.075259, loss: 1.6281
2022-08-02 14:27:41 - train: epoch 0061, iter [00400, 05004], lr: 0.075195, loss: 2.0068
2022-08-02 14:28:36 - train: epoch 0061, iter [00500, 05004], lr: 0.075131, loss: 1.7583
2022-08-02 14:29:32 - train: epoch 0061, iter [00600, 05004], lr: 0.075067, loss: 1.7982
2022-08-02 14:30:28 - train: epoch 0061, iter [00700, 05004], lr: 0.075003, loss: 1.4680
2022-08-02 14:31:23 - train: epoch 0061, iter [00800, 05004], lr: 0.074939, loss: 1.7529
2022-08-02 14:32:19 - train: epoch 0061, iter [00900, 05004], lr: 0.074875, loss: 1.6543
2022-08-02 14:33:14 - train: epoch 0061, iter [01000, 05004], lr: 0.074811, loss: 1.6428
2022-08-02 14:34:10 - train: epoch 0061, iter [01100, 05004], lr: 0.074747, loss: 1.4969
2022-08-02 14:35:05 - train: epoch 0061, iter [01200, 05004], lr: 0.074683, loss: 1.5479
2022-08-02 14:36:02 - train: epoch 0061, iter [01300, 05004], lr: 0.074620, loss: 1.6812
2022-08-02 14:36:56 - train: epoch 0061, iter [01400, 05004], lr: 0.074556, loss: 1.7070
2022-08-02 14:37:53 - train: epoch 0061, iter [01500, 05004], lr: 0.074492, loss: 1.5564
2022-08-02 14:38:49 - train: epoch 0061, iter [01600, 05004], lr: 0.074428, loss: 1.6847
2022-08-02 14:39:45 - train: epoch 0061, iter [01700, 05004], lr: 0.074364, loss: 1.6169
2022-08-02 14:40:41 - train: epoch 0061, iter [01800, 05004], lr: 0.074300, loss: 1.7013
2022-08-02 14:41:37 - train: epoch 0061, iter [01900, 05004], lr: 0.074236, loss: 1.8980
2022-08-02 14:42:32 - train: epoch 0061, iter [02000, 05004], lr: 0.074172, loss: 1.6619
2022-08-02 14:43:28 - train: epoch 0061, iter [02100, 05004], lr: 0.074109, loss: 1.9442
2022-08-02 14:44:24 - train: epoch 0061, iter [02200, 05004], lr: 0.074045, loss: 1.7819
2022-08-02 14:45:21 - train: epoch 0061, iter [02300, 05004], lr: 0.073981, loss: 1.7553
2022-08-02 14:46:17 - train: epoch 0061, iter [02400, 05004], lr: 0.073917, loss: 1.6982
2022-08-02 14:47:13 - train: epoch 0061, iter [02500, 05004], lr: 0.073853, loss: 1.6235
2022-08-02 14:48:08 - train: epoch 0061, iter [02600, 05004], lr: 0.073790, loss: 1.8485
2022-08-02 14:49:05 - train: epoch 0061, iter [02700, 05004], lr: 0.073726, loss: 1.7894
2022-08-02 14:50:01 - train: epoch 0061, iter [02800, 05004], lr: 0.073662, loss: 1.7700
2022-08-02 14:50:55 - train: epoch 0061, iter [02900, 05004], lr: 0.073598, loss: 1.7782
2022-08-02 14:51:52 - train: epoch 0061, iter [03000, 05004], lr: 0.073534, loss: 1.7106
2022-08-02 14:52:48 - train: epoch 0061, iter [03100, 05004], lr: 0.073471, loss: 1.9409
2022-08-02 14:53:44 - train: epoch 0061, iter [03200, 05004], lr: 0.073407, loss: 1.8040
2022-08-02 14:54:40 - train: epoch 0061, iter [03300, 05004], lr: 0.073343, loss: 1.6037
2022-08-02 14:55:35 - train: epoch 0061, iter [03400, 05004], lr: 0.073280, loss: 1.6735
2022-08-02 14:56:31 - train: epoch 0061, iter [03500, 05004], lr: 0.073216, loss: 1.8090
2022-08-02 14:57:27 - train: epoch 0061, iter [03600, 05004], lr: 0.073152, loss: 1.4797
2022-08-02 14:58:23 - train: epoch 0061, iter [03700, 05004], lr: 0.073089, loss: 1.6337
2022-08-02 14:59:19 - train: epoch 0061, iter [03800, 05004], lr: 0.073025, loss: 1.6195
2022-08-02 15:00:14 - train: epoch 0061, iter [03900, 05004], lr: 0.072961, loss: 1.7355
2022-08-02 15:01:10 - train: epoch 0061, iter [04000, 05004], lr: 0.072898, loss: 1.7212
2022-08-02 15:02:06 - train: epoch 0061, iter [04100, 05004], lr: 0.072834, loss: 1.9208
2022-08-02 15:03:03 - train: epoch 0061, iter [04200, 05004], lr: 0.072771, loss: 1.7029
2022-08-02 15:03:58 - train: epoch 0061, iter [04300, 05004], lr: 0.072707, loss: 1.6460
2022-08-02 15:04:54 - train: epoch 0061, iter [04400, 05004], lr: 0.072643, loss: 1.8310
2022-08-02 15:05:49 - train: epoch 0061, iter [04500, 05004], lr: 0.072580, loss: 1.8017
2022-08-02 15:06:47 - train: epoch 0061, iter [04600, 05004], lr: 0.072516, loss: 1.5979
2022-08-02 15:07:43 - train: epoch 0061, iter [04700, 05004], lr: 0.072453, loss: 1.7462
2022-08-02 15:08:39 - train: epoch 0061, iter [04800, 05004], lr: 0.072389, loss: 1.6630
2022-08-02 15:09:33 - train: epoch 0061, iter [04900, 05004], lr: 0.072326, loss: 1.7354
2022-08-02 15:10:27 - train: epoch 0061, iter [05000, 05004], lr: 0.072262, loss: 2.0565
2022-08-02 15:10:28 - train: epoch 061, train_loss: 1.7293
2022-08-02 15:12:27 - eval: epoch: 061, acc1: 62.928%, acc5: 85.466%, test_loss: 1.5354, per_image_load_time: 4.046ms, per_image_inference_time: 0.551ms
2022-08-02 15:12:28 - until epoch: 061, best_acc1: 63.656%
2022-08-02 15:12:28 - epoch 062 lr: 0.072259
2022-08-02 15:13:32 - train: epoch 0062, iter [00100, 05004], lr: 0.072196, loss: 1.8584
2022-08-02 15:14:26 - train: epoch 0062, iter [00200, 05004], lr: 0.072133, loss: 1.6818
2022-08-02 15:15:23 - train: epoch 0062, iter [00300, 05004], lr: 0.072069, loss: 1.7974
2022-08-02 15:16:18 - train: epoch 0062, iter [00400, 05004], lr: 0.072006, loss: 1.6706
2022-08-02 15:17:12 - train: epoch 0062, iter [00500, 05004], lr: 0.071942, loss: 1.8167
2022-08-02 15:18:06 - train: epoch 0062, iter [00600, 05004], lr: 0.071879, loss: 1.6067
2022-08-02 15:19:03 - train: epoch 0062, iter [00700, 05004], lr: 0.071816, loss: 1.5985
2022-08-02 15:19:59 - train: epoch 0062, iter [00800, 05004], lr: 0.071752, loss: 1.9615
2022-08-02 15:20:54 - train: epoch 0062, iter [00900, 05004], lr: 0.071689, loss: 1.6400
2022-08-02 15:21:50 - train: epoch 0062, iter [01000, 05004], lr: 0.071625, loss: 1.6944
2022-08-02 15:22:46 - train: epoch 0062, iter [01100, 05004], lr: 0.071562, loss: 1.6727
2022-08-02 15:23:42 - train: epoch 0062, iter [01200, 05004], lr: 0.071499, loss: 1.8374
2022-08-02 15:24:39 - train: epoch 0062, iter [01300, 05004], lr: 0.071435, loss: 1.5653
2022-08-02 15:25:35 - train: epoch 0062, iter [01400, 05004], lr: 0.071372, loss: 1.5889
2022-08-02 15:26:32 - train: epoch 0062, iter [01500, 05004], lr: 0.071309, loss: 1.8369
2022-08-02 15:27:27 - train: epoch 0062, iter [01600, 05004], lr: 0.071245, loss: 1.9729
2022-08-02 15:28:23 - train: epoch 0062, iter [01700, 05004], lr: 0.071182, loss: 1.7050
2022-08-02 15:29:17 - train: epoch 0062, iter [01800, 05004], lr: 0.071119, loss: 1.6001
2022-08-02 15:30:13 - train: epoch 0062, iter [01900, 05004], lr: 0.071056, loss: 1.5890
2022-08-02 15:31:09 - train: epoch 0062, iter [02000, 05004], lr: 0.070992, loss: 1.6916
2022-08-02 15:32:05 - train: epoch 0062, iter [02100, 05004], lr: 0.070929, loss: 1.9801
2022-08-02 15:33:01 - train: epoch 0062, iter [02200, 05004], lr: 0.070866, loss: 1.5504
2022-08-02 15:33:57 - train: epoch 0062, iter [02300, 05004], lr: 0.070803, loss: 1.6439
2022-08-02 15:34:54 - train: epoch 0062, iter [02400, 05004], lr: 0.070739, loss: 1.7457
2022-08-02 15:35:48 - train: epoch 0062, iter [02500, 05004], lr: 0.070676, loss: 1.8490
2022-08-02 15:36:43 - train: epoch 0062, iter [02600, 05004], lr: 0.070613, loss: 1.7854
2022-08-02 15:37:39 - train: epoch 0062, iter [02700, 05004], lr: 0.070550, loss: 1.5086
2022-08-02 15:38:35 - train: epoch 0062, iter [02800, 05004], lr: 0.070487, loss: 1.9411
2022-08-02 15:39:31 - train: epoch 0062, iter [02900, 05004], lr: 0.070424, loss: 1.7178
2022-08-02 15:40:27 - train: epoch 0062, iter [03000, 05004], lr: 0.070361, loss: 1.8752
2022-08-02 15:41:22 - train: epoch 0062, iter [03100, 05004], lr: 0.070297, loss: 1.6089
2022-08-02 15:42:19 - train: epoch 0062, iter [03200, 05004], lr: 0.070234, loss: 1.5974
2022-08-02 15:43:12 - train: epoch 0062, iter [03300, 05004], lr: 0.070171, loss: 1.7687
2022-08-02 15:44:07 - train: epoch 0062, iter [03400, 05004], lr: 0.070108, loss: 1.7390
2022-08-02 15:45:02 - train: epoch 0062, iter [03500, 05004], lr: 0.070045, loss: 1.8533
2022-08-02 15:45:57 - train: epoch 0062, iter [03600, 05004], lr: 0.069982, loss: 1.7658
2022-08-02 15:46:52 - train: epoch 0062, iter [03700, 05004], lr: 0.069919, loss: 1.8727
2022-08-02 15:47:48 - train: epoch 0062, iter [03800, 05004], lr: 0.069856, loss: 1.8604
2022-08-02 15:48:43 - train: epoch 0062, iter [03900, 05004], lr: 0.069793, loss: 2.0600
2022-08-02 15:49:39 - train: epoch 0062, iter [04000, 05004], lr: 0.069730, loss: 1.4422
2022-08-02 15:50:32 - train: epoch 0062, iter [04100, 05004], lr: 0.069667, loss: 1.9351
2022-08-02 15:51:30 - train: epoch 0062, iter [04200, 05004], lr: 0.069604, loss: 1.5497
2022-08-02 15:52:24 - train: epoch 0062, iter [04300, 05004], lr: 0.069541, loss: 1.8519
2022-08-02 15:53:20 - train: epoch 0062, iter [04400, 05004], lr: 0.069478, loss: 1.8501
2022-08-02 15:54:15 - train: epoch 0062, iter [04500, 05004], lr: 0.069415, loss: 1.6457
2022-08-02 15:55:11 - train: epoch 0062, iter [04600, 05004], lr: 0.069352, loss: 1.5764
2022-08-02 15:56:08 - train: epoch 0062, iter [04700, 05004], lr: 0.069289, loss: 1.5741
2022-08-02 15:57:03 - train: epoch 0062, iter [04800, 05004], lr: 0.069227, loss: 1.8729
2022-08-02 15:57:59 - train: epoch 0062, iter [04900, 05004], lr: 0.069164, loss: 1.7058
2022-08-02 15:58:52 - train: epoch 0062, iter [05000, 05004], lr: 0.069101, loss: 1.7143
2022-08-02 15:58:53 - train: epoch 062, train_loss: 1.7165
2022-08-02 16:00:55 - eval: epoch: 062, acc1: 63.848%, acc5: 86.130%, test_loss: 1.4750, per_image_load_time: 4.172ms, per_image_inference_time: 0.512ms
2022-08-02 16:00:55 - until epoch: 062, best_acc1: 63.848%
2022-08-02 16:00:55 - epoch 063 lr: 0.069098
2022-08-02 16:01:58 - train: epoch 0063, iter [00100, 05004], lr: 0.069035, loss: 1.5095
2022-08-02 16:02:54 - train: epoch 0063, iter [00200, 05004], lr: 0.068973, loss: 1.8676
2022-08-02 16:03:50 - train: epoch 0063, iter [00300, 05004], lr: 0.068910, loss: 1.8625
2022-08-02 16:04:44 - train: epoch 0063, iter [00400, 05004], lr: 0.068847, loss: 1.6100
2022-08-02 16:05:40 - train: epoch 0063, iter [00500, 05004], lr: 0.068784, loss: 1.5042
2022-08-02 16:06:35 - train: epoch 0063, iter [00600, 05004], lr: 0.068721, loss: 1.7288
2022-08-02 16:07:29 - train: epoch 0063, iter [00700, 05004], lr: 0.068659, loss: 1.7117
2022-08-02 16:08:24 - train: epoch 0063, iter [00800, 05004], lr: 0.068596, loss: 1.6183
2022-08-02 16:09:19 - train: epoch 0063, iter [00900, 05004], lr: 0.068533, loss: 1.6445
2022-08-02 16:10:13 - train: epoch 0063, iter [01000, 05004], lr: 0.068470, loss: 1.9063
2022-08-02 16:11:09 - train: epoch 0063, iter [01100, 05004], lr: 0.068408, loss: 1.6026
2022-08-02 16:12:04 - train: epoch 0063, iter [01200, 05004], lr: 0.068345, loss: 1.7179
2022-08-02 16:12:59 - train: epoch 0063, iter [01300, 05004], lr: 0.068282, loss: 1.3877
2022-08-02 16:13:54 - train: epoch 0063, iter [01400, 05004], lr: 0.068220, loss: 2.0704
2022-08-02 16:14:48 - train: epoch 0063, iter [01500, 05004], lr: 0.068157, loss: 1.5157
2022-08-02 16:15:41 - train: epoch 0063, iter [01600, 05004], lr: 0.068094, loss: 1.7302
2022-08-02 16:16:34 - train: epoch 0063, iter [01700, 05004], lr: 0.068032, loss: 1.4518
2022-08-02 16:17:29 - train: epoch 0063, iter [01800, 05004], lr: 0.067969, loss: 1.7286
2022-08-02 16:18:20 - train: epoch 0063, iter [01900, 05004], lr: 0.067907, loss: 1.7707
2022-08-02 16:19:14 - train: epoch 0063, iter [02000, 05004], lr: 0.067844, loss: 1.6503
2022-08-02 16:20:06 - train: epoch 0063, iter [02100, 05004], lr: 0.067781, loss: 1.5400
2022-08-02 16:21:01 - train: epoch 0063, iter [02200, 05004], lr: 0.067719, loss: 2.0554
2022-08-02 16:21:55 - train: epoch 0063, iter [02300, 05004], lr: 0.067656, loss: 1.8830
2022-08-02 16:22:49 - train: epoch 0063, iter [02400, 05004], lr: 0.067594, loss: 1.7746
2022-08-02 16:23:41 - train: epoch 0063, iter [02500, 05004], lr: 0.067531, loss: 1.7495
2022-08-02 16:24:34 - train: epoch 0063, iter [02600, 05004], lr: 0.067469, loss: 1.6453
2022-08-02 16:25:28 - train: epoch 0063, iter [02700, 05004], lr: 0.067406, loss: 1.8983
2022-08-02 16:26:23 - train: epoch 0063, iter [02800, 05004], lr: 0.067344, loss: 1.6982
2022-08-02 16:27:15 - train: epoch 0063, iter [02900, 05004], lr: 0.067281, loss: 1.7794
2022-08-02 16:28:08 - train: epoch 0063, iter [03000, 05004], lr: 0.067219, loss: 1.8477
2022-08-02 16:29:01 - train: epoch 0063, iter [03100, 05004], lr: 0.067157, loss: 2.0215
2022-08-02 16:29:56 - train: epoch 0063, iter [03200, 05004], lr: 0.067094, loss: 1.9485
2022-08-02 16:30:51 - train: epoch 0063, iter [03300, 05004], lr: 0.067032, loss: 1.6049
2022-08-02 16:31:48 - train: epoch 0063, iter [03400, 05004], lr: 0.066969, loss: 1.6098
2022-08-02 16:32:44 - train: epoch 0063, iter [03500, 05004], lr: 0.066907, loss: 1.9167
2022-08-02 16:33:40 - train: epoch 0063, iter [03600, 05004], lr: 0.066845, loss: 1.7026
2022-08-02 16:34:36 - train: epoch 0063, iter [03700, 05004], lr: 0.066782, loss: 1.7270
2022-08-02 16:35:31 - train: epoch 0063, iter [03800, 05004], lr: 0.066720, loss: 1.6572
2022-08-02 16:36:27 - train: epoch 0063, iter [03900, 05004], lr: 0.066658, loss: 1.7086
2022-08-02 16:37:21 - train: epoch 0063, iter [04000, 05004], lr: 0.066595, loss: 1.6916
2022-08-02 16:38:16 - train: epoch 0063, iter [04100, 05004], lr: 0.066533, loss: 1.6615
2022-08-02 16:39:12 - train: epoch 0063, iter [04200, 05004], lr: 0.066471, loss: 1.7711
2022-08-02 16:40:08 - train: epoch 0063, iter [04300, 05004], lr: 0.066409, loss: 1.8661
2022-08-02 16:41:05 - train: epoch 0063, iter [04400, 05004], lr: 0.066346, loss: 1.7592
2022-08-02 16:42:00 - train: epoch 0063, iter [04500, 05004], lr: 0.066284, loss: 1.7762
2022-08-02 16:42:57 - train: epoch 0063, iter [04600, 05004], lr: 0.066222, loss: 1.7797
2022-08-02 16:43:52 - train: epoch 0063, iter [04700, 05004], lr: 0.066160, loss: 1.6136
2022-08-02 16:44:48 - train: epoch 0063, iter [04800, 05004], lr: 0.066097, loss: 1.6387
2022-08-02 16:45:44 - train: epoch 0063, iter [04900, 05004], lr: 0.066035, loss: 1.5888
2022-08-02 16:46:36 - train: epoch 0063, iter [05000, 05004], lr: 0.065973, loss: 1.6915
2022-08-02 16:46:37 - train: epoch 063, train_loss: 1.7011
2022-08-02 16:48:45 - eval: epoch: 063, acc1: 64.306%, acc5: 86.376%, test_loss: 1.4653, per_image_load_time: 3.690ms, per_image_inference_time: 0.610ms
2022-08-02 16:48:45 - until epoch: 063, best_acc1: 64.306%
2022-08-02 16:48:45 - epoch 064 lr: 0.065970
2022-08-02 16:49:52 - train: epoch 0064, iter [00100, 05004], lr: 0.065909, loss: 1.4350
2022-08-02 16:50:51 - train: epoch 0064, iter [00200, 05004], lr: 0.065846, loss: 1.7070
2022-08-02 16:51:50 - train: epoch 0064, iter [00300, 05004], lr: 0.065784, loss: 1.4642
2022-08-02 16:52:49 - train: epoch 0064, iter [00400, 05004], lr: 0.065722, loss: 1.3951
2022-08-02 16:53:47 - train: epoch 0064, iter [00500, 05004], lr: 0.065660, loss: 1.4439
2022-08-02 16:54:47 - train: epoch 0064, iter [00600, 05004], lr: 0.065598, loss: 1.6062
2022-08-02 16:55:44 - train: epoch 0064, iter [00700, 05004], lr: 0.065536, loss: 1.7297
2022-08-02 16:56:42 - train: epoch 0064, iter [00800, 05004], lr: 0.065474, loss: 1.4493
2022-08-02 16:57:39 - train: epoch 0064, iter [00900, 05004], lr: 0.065412, loss: 1.5639
2022-08-02 16:58:37 - train: epoch 0064, iter [01000, 05004], lr: 0.065350, loss: 1.5533
2022-08-02 16:59:34 - train: epoch 0064, iter [01100, 05004], lr: 0.065288, loss: 1.7784
2022-08-02 17:00:32 - train: epoch 0064, iter [01200, 05004], lr: 0.065226, loss: 1.5827
2022-08-02 17:01:33 - train: epoch 0064, iter [01300, 05004], lr: 0.065164, loss: 1.6484
2022-08-02 17:02:29 - train: epoch 0064, iter [01400, 05004], lr: 0.065102, loss: 1.8139
2022-08-02 17:03:27 - train: epoch 0064, iter [01500, 05004], lr: 0.065040, loss: 1.5130
2022-08-02 17:04:26 - train: epoch 0064, iter [01600, 05004], lr: 0.064978, loss: 1.6310
2022-08-02 17:05:25 - train: epoch 0064, iter [01700, 05004], lr: 0.064916, loss: 1.5438
2022-08-02 17:06:21 - train: epoch 0064, iter [01800, 05004], lr: 0.064855, loss: 1.5754
2022-08-02 17:07:20 - train: epoch 0064, iter [01900, 05004], lr: 0.064793, loss: 1.5229
2022-08-02 17:08:16 - train: epoch 0064, iter [02000, 05004], lr: 0.064731, loss: 1.5807
2022-08-02 17:09:15 - train: epoch 0064, iter [02100, 05004], lr: 0.064669, loss: 1.7686
2022-08-02 17:10:14 - train: epoch 0064, iter [02200, 05004], lr: 0.064607, loss: 2.0267
2022-08-02 17:11:11 - train: epoch 0064, iter [02300, 05004], lr: 0.064545, loss: 1.8932
2022-08-02 17:12:08 - train: epoch 0064, iter [02400, 05004], lr: 0.064484, loss: 1.6587
2022-08-02 17:13:05 - train: epoch 0064, iter [02500, 05004], lr: 0.064422, loss: 1.6526
2022-08-02 17:14:03 - train: epoch 0064, iter [02600, 05004], lr: 0.064360, loss: 1.6240
2022-08-02 17:15:02 - train: epoch 0064, iter [02700, 05004], lr: 0.064298, loss: 1.7566
2022-08-02 17:16:01 - train: epoch 0064, iter [02800, 05004], lr: 0.064237, loss: 1.6311
2022-08-02 17:16:58 - train: epoch 0064, iter [02900, 05004], lr: 0.064175, loss: 1.9122
2022-08-02 17:18:29 - train: epoch 0064, iter [03000, 05004], lr: 0.064113, loss: 1.6635
2022-08-02 17:19:27 - train: epoch 0064, iter [03100, 05004], lr: 0.064052, loss: 1.6648
2022-08-02 17:20:26 - train: epoch 0064, iter [03200, 05004], lr: 0.063990, loss: 1.7105
2022-08-02 17:21:24 - train: epoch 0064, iter [03300, 05004], lr: 0.063928, loss: 1.8183
2022-08-02 17:22:22 - train: epoch 0064, iter [03400, 05004], lr: 0.063867, loss: 1.8774
2022-08-02 17:23:21 - train: epoch 0064, iter [03500, 05004], lr: 0.063805, loss: 1.5077
2022-08-02 17:24:18 - train: epoch 0064, iter [03600, 05004], lr: 0.063743, loss: 1.7386
2022-08-02 17:25:16 - train: epoch 0064, iter [03700, 05004], lr: 0.063682, loss: 1.6348
2022-08-02 17:26:14 - train: epoch 0064, iter [03800, 05004], lr: 0.063620, loss: 1.8231
2022-08-02 17:27:13 - train: epoch 0064, iter [03900, 05004], lr: 0.063559, loss: 1.6886
2022-08-02 17:28:08 - train: epoch 0064, iter [04000, 05004], lr: 0.063497, loss: 1.7077
2022-08-02 17:29:08 - train: epoch 0064, iter [04100, 05004], lr: 0.063436, loss: 1.7388
2022-08-02 17:30:05 - train: epoch 0064, iter [04200, 05004], lr: 0.063374, loss: 1.5074
2022-08-02 17:31:03 - train: epoch 0064, iter [04300, 05004], lr: 0.063313, loss: 1.7387
2022-08-02 17:31:58 - train: epoch 0064, iter [04400, 05004], lr: 0.063251, loss: 1.4416
2022-08-02 17:32:54 - train: epoch 0064, iter [04500, 05004], lr: 0.063190, loss: 1.6588
2022-08-02 17:33:52 - train: epoch 0064, iter [04600, 05004], lr: 0.063128, loss: 1.9278
2022-08-02 17:34:50 - train: epoch 0064, iter [04700, 05004], lr: 0.063067, loss: 1.9296
2022-08-02 17:35:48 - train: epoch 0064, iter [04800, 05004], lr: 0.063005, loss: 1.8155
2022-08-02 17:36:46 - train: epoch 0064, iter [04900, 05004], lr: 0.062944, loss: 1.9990
2022-08-02 17:37:42 - train: epoch 0064, iter [05000, 05004], lr: 0.062883, loss: 1.6205
2022-08-02 17:37:43 - train: epoch 064, train_loss: 1.6833
2022-08-02 17:39:52 - eval: epoch: 064, acc1: 64.512%, acc5: 86.544%, test_loss: 1.4523, per_image_load_time: 1.660ms, per_image_inference_time: 0.631ms
2022-08-02 17:39:52 - until epoch: 064, best_acc1: 64.512%
2022-08-02 17:39:52 - epoch 065 lr: 0.062880
2022-08-02 17:40:57 - train: epoch 0065, iter [00100, 05004], lr: 0.062819, loss: 1.6919
2022-08-02 17:41:55 - train: epoch 0065, iter [00200, 05004], lr: 0.062758, loss: 1.6809
2022-08-02 17:42:52 - train: epoch 0065, iter [00300, 05004], lr: 0.062696, loss: 1.5814
2022-08-02 17:43:51 - train: epoch 0065, iter [00400, 05004], lr: 0.062635, loss: 1.5567
2022-08-02 17:44:48 - train: epoch 0065, iter [00500, 05004], lr: 0.062574, loss: 1.6963
2022-08-02 17:45:47 - train: epoch 0065, iter [00600, 05004], lr: 0.062512, loss: 1.6854
2022-08-02 17:46:44 - train: epoch 0065, iter [00700, 05004], lr: 0.062451, loss: 1.8213
2022-08-02 17:47:42 - train: epoch 0065, iter [00800, 05004], lr: 0.062390, loss: 1.4666
2022-08-02 17:48:41 - train: epoch 0065, iter [00900, 05004], lr: 0.062329, loss: 1.7373
2022-08-02 17:49:38 - train: epoch 0065, iter [01000, 05004], lr: 0.062267, loss: 1.8366
2022-08-02 17:50:36 - train: epoch 0065, iter [01100, 05004], lr: 0.062206, loss: 1.5475
2022-08-02 17:51:35 - train: epoch 0065, iter [01200, 05004], lr: 0.062145, loss: 1.9326
2022-08-02 17:52:33 - train: epoch 0065, iter [01300, 05004], lr: 0.062084, loss: 1.7281
2022-08-02 17:53:32 - train: epoch 0065, iter [01400, 05004], lr: 0.062023, loss: 1.5984
2022-08-02 17:54:31 - train: epoch 0065, iter [01500, 05004], lr: 0.061962, loss: 1.4911
2022-08-02 17:55:30 - train: epoch 0065, iter [01600, 05004], lr: 0.061901, loss: 1.6967
2022-08-02 17:56:27 - train: epoch 0065, iter [01700, 05004], lr: 0.061839, loss: 1.5549
2022-08-02 17:57:26 - train: epoch 0065, iter [01800, 05004], lr: 0.061778, loss: 1.5694
2022-08-02 17:58:24 - train: epoch 0065, iter [01900, 05004], lr: 0.061717, loss: 1.4194
2022-08-02 17:59:23 - train: epoch 0065, iter [02000, 05004], lr: 0.061656, loss: 1.5803
2022-08-02 18:00:22 - train: epoch 0065, iter [02100, 05004], lr: 0.061595, loss: 1.7451
2022-08-02 18:01:22 - train: epoch 0065, iter [02200, 05004], lr: 0.061534, loss: 1.5988
2022-08-02 18:02:21 - train: epoch 0065, iter [02300, 05004], lr: 0.061473, loss: 1.7202
2022-08-02 18:03:18 - train: epoch 0065, iter [02400, 05004], lr: 0.061412, loss: 1.6144
2022-08-02 18:04:16 - train: epoch 0065, iter [02500, 05004], lr: 0.061351, loss: 1.9581
2022-08-02 18:05:16 - train: epoch 0065, iter [02600, 05004], lr: 0.061290, loss: 1.7155
2022-08-02 18:06:15 - train: epoch 0065, iter [02700, 05004], lr: 0.061229, loss: 1.6442
2022-08-02 18:07:13 - train: epoch 0065, iter [02800, 05004], lr: 0.061169, loss: 1.7217
2022-08-02 18:08:11 - train: epoch 0065, iter [02900, 05004], lr: 0.061108, loss: 1.7129
2022-08-02 18:09:08 - train: epoch 0065, iter [03000, 05004], lr: 0.061047, loss: 1.6815
2022-08-02 18:10:04 - train: epoch 0065, iter [03100, 05004], lr: 0.060986, loss: 1.6805
2022-08-02 18:11:02 - train: epoch 0065, iter [03200, 05004], lr: 0.060925, loss: 1.7485
2022-08-02 18:11:56 - train: epoch 0065, iter [03300, 05004], lr: 0.060864, loss: 1.6223
2022-08-02 18:12:53 - train: epoch 0065, iter [03400, 05004], lr: 0.060803, loss: 1.4251
2022-08-02 18:13:52 - train: epoch 0065, iter [03500, 05004], lr: 0.060743, loss: 1.7309
2022-08-02 18:14:45 - train: epoch 0065, iter [03600, 05004], lr: 0.060682, loss: 1.6597
2022-08-02 18:15:42 - train: epoch 0065, iter [03700, 05004], lr: 0.060621, loss: 1.6215
2022-08-02 18:16:39 - train: epoch 0065, iter [03800, 05004], lr: 0.060560, loss: 1.6886
2022-08-02 18:17:35 - train: epoch 0065, iter [03900, 05004], lr: 0.060500, loss: 1.8184
2022-08-02 18:18:32 - train: epoch 0065, iter [04000, 05004], lr: 0.060439, loss: 1.8116
2022-08-02 18:19:28 - train: epoch 0065, iter [04100, 05004], lr: 0.060378, loss: 1.7796
2022-08-02 18:20:22 - train: epoch 0065, iter [04200, 05004], lr: 0.060318, loss: 1.6355
2022-08-02 18:21:22 - train: epoch 0065, iter [04300, 05004], lr: 0.060257, loss: 1.6213
2022-08-02 18:22:18 - train: epoch 0065, iter [04400, 05004], lr: 0.060196, loss: 1.5834
2022-08-02 18:23:14 - train: epoch 0065, iter [04500, 05004], lr: 0.060136, loss: 1.6897
2022-08-02 18:24:09 - train: epoch 0065, iter [04600, 05004], lr: 0.060075, loss: 1.7602
2022-08-02 18:25:06 - train: epoch 0065, iter [04700, 05004], lr: 0.060015, loss: 1.7538
2022-08-02 18:26:02 - train: epoch 0065, iter [04800, 05004], lr: 0.059954, loss: 1.4530
2022-08-02 18:26:58 - train: epoch 0065, iter [04900, 05004], lr: 0.059893, loss: 1.5985
2022-08-02 18:27:51 - train: epoch 0065, iter [05000, 05004], lr: 0.059833, loss: 1.8377
2022-08-02 18:27:52 - train: epoch 065, train_loss: 1.6659
2022-08-02 18:29:59 - eval: epoch: 065, acc1: 65.186%, acc5: 86.828%, test_loss: 1.4239, per_image_load_time: 2.850ms, per_image_inference_time: 0.656ms
2022-08-02 18:29:59 - until epoch: 065, best_acc1: 65.186%
2022-08-02 18:29:59 - epoch 066 lr: 0.059830
2022-08-02 18:31:05 - train: epoch 0066, iter [00100, 05004], lr: 0.059770, loss: 1.5042
2022-08-02 18:32:01 - train: epoch 0066, iter [00200, 05004], lr: 0.059709, loss: 1.9054
2022-08-02 18:32:56 - train: epoch 0066, iter [00300, 05004], lr: 0.059649, loss: 1.6410
2022-08-02 18:33:54 - train: epoch 0066, iter [00400, 05004], lr: 0.059589, loss: 1.4582
2022-08-02 18:34:50 - train: epoch 0066, iter [00500, 05004], lr: 0.059528, loss: 1.5781
2022-08-02 18:35:46 - train: epoch 0066, iter [00600, 05004], lr: 0.059468, loss: 1.4547
2022-08-02 18:36:41 - train: epoch 0066, iter [00700, 05004], lr: 0.059407, loss: 1.7833
2022-08-02 18:37:37 - train: epoch 0066, iter [00800, 05004], lr: 0.059347, loss: 1.6975
2022-08-02 18:38:32 - train: epoch 0066, iter [00900, 05004], lr: 0.059286, loss: 1.5544
2022-08-02 18:39:30 - train: epoch 0066, iter [01000, 05004], lr: 0.059226, loss: 1.5958
2022-08-02 18:40:24 - train: epoch 0066, iter [01100, 05004], lr: 0.059166, loss: 1.8204
2022-08-02 18:41:21 - train: epoch 0066, iter [01200, 05004], lr: 0.059105, loss: 1.7765
2022-08-02 18:42:18 - train: epoch 0066, iter [01300, 05004], lr: 0.059045, loss: 1.7721
2022-08-02 18:43:14 - train: epoch 0066, iter [01400, 05004], lr: 0.058985, loss: 1.4755
2022-08-02 18:44:09 - train: epoch 0066, iter [01500, 05004], lr: 0.058925, loss: 1.7195
2022-08-02 18:45:04 - train: epoch 0066, iter [01600, 05004], lr: 0.058864, loss: 1.8384
2022-08-02 18:46:00 - train: epoch 0066, iter [01700, 05004], lr: 0.058804, loss: 1.7057
2022-08-02 18:46:55 - train: epoch 0066, iter [01800, 05004], lr: 0.058744, loss: 1.4021
2022-08-02 18:47:52 - train: epoch 0066, iter [01900, 05004], lr: 0.058684, loss: 1.8795
2022-08-02 18:48:48 - train: epoch 0066, iter [02000, 05004], lr: 0.058624, loss: 1.2930
2022-08-02 18:49:44 - train: epoch 0066, iter [02100, 05004], lr: 0.058563, loss: 1.7080
2022-08-02 18:50:40 - train: epoch 0066, iter [02200, 05004], lr: 0.058503, loss: 1.5211
2022-08-02 18:51:38 - train: epoch 0066, iter [02300, 05004], lr: 0.058443, loss: 1.8096
2022-08-02 18:52:33 - train: epoch 0066, iter [02400, 05004], lr: 0.058383, loss: 1.6508
2022-08-02 18:53:31 - train: epoch 0066, iter [02500, 05004], lr: 0.058323, loss: 1.6517
2022-08-02 18:54:26 - train: epoch 0066, iter [02600, 05004], lr: 0.058263, loss: 1.7035
2022-08-02 18:55:23 - train: epoch 0066, iter [02700, 05004], lr: 0.058203, loss: 1.6951
2022-08-02 18:56:19 - train: epoch 0066, iter [02800, 05004], lr: 0.058143, loss: 1.5572
2022-08-02 18:57:15 - train: epoch 0066, iter [02900, 05004], lr: 0.058083, loss: 1.7252
2022-08-02 18:58:12 - train: epoch 0066, iter [03000, 05004], lr: 0.058023, loss: 1.6854
2022-08-02 18:59:10 - train: epoch 0066, iter [03100, 05004], lr: 0.057963, loss: 1.7604
2022-08-02 19:00:03 - train: epoch 0066, iter [03200, 05004], lr: 0.057903, loss: 1.7570
2022-08-02 19:01:01 - train: epoch 0066, iter [03300, 05004], lr: 0.057843, loss: 1.5832
2022-08-02 19:01:56 - train: epoch 0066, iter [03400, 05004], lr: 0.057783, loss: 1.6686
2022-08-02 19:02:54 - train: epoch 0066, iter [03500, 05004], lr: 0.057723, loss: 1.8058
2022-08-02 19:03:48 - train: epoch 0066, iter [03600, 05004], lr: 0.057663, loss: 1.8539
2022-08-02 19:04:45 - train: epoch 0066, iter [03700, 05004], lr: 0.057603, loss: 1.7256
2022-08-02 19:05:44 - train: epoch 0066, iter [03800, 05004], lr: 0.057544, loss: 1.8023
2022-08-02 19:06:38 - train: epoch 0066, iter [03900, 05004], lr: 0.057484, loss: 1.5994
2022-08-02 19:07:33 - train: epoch 0066, iter [04000, 05004], lr: 0.057424, loss: 1.6339
2022-08-02 19:08:32 - train: epoch 0066, iter [04100, 05004], lr: 0.057364, loss: 1.4920
2022-08-02 19:09:26 - train: epoch 0066, iter [04200, 05004], lr: 0.057304, loss: 1.6056
2022-08-02 19:10:22 - train: epoch 0066, iter [04300, 05004], lr: 0.057245, loss: 1.5458
2022-08-02 19:11:18 - train: epoch 0066, iter [04400, 05004], lr: 0.057185, loss: 1.4587
2022-08-02 19:12:13 - train: epoch 0066, iter [04500, 05004], lr: 0.057125, loss: 1.9140
2022-08-02 19:13:10 - train: epoch 0066, iter [04600, 05004], lr: 0.057066, loss: 1.7835
2022-08-02 19:14:05 - train: epoch 0066, iter [04700, 05004], lr: 0.057006, loss: 1.5481
2022-08-02 19:15:02 - train: epoch 0066, iter [04800, 05004], lr: 0.056946, loss: 1.6993
2022-08-02 19:15:58 - train: epoch 0066, iter [04900, 05004], lr: 0.056887, loss: 1.5740
2022-08-02 19:16:51 - train: epoch 0066, iter [05000, 05004], lr: 0.056827, loss: 1.4747
2022-08-02 19:16:52 - train: epoch 066, train_loss: 1.6479
2022-08-02 19:18:54 - eval: epoch: 066, acc1: 65.316%, acc5: 86.950%, test_loss: 1.4273, per_image_load_time: 3.916ms, per_image_inference_time: 0.529ms
2022-08-02 19:18:54 - until epoch: 066, best_acc1: 65.316%
2022-08-02 19:18:54 - epoch 067 lr: 0.056824
2022-08-02 19:20:00 - train: epoch 0067, iter [00100, 05004], lr: 0.056765, loss: 1.5833
2022-08-02 19:20:56 - train: epoch 0067, iter [00200, 05004], lr: 0.056705, loss: 1.7176
2022-08-02 19:21:52 - train: epoch 0067, iter [00300, 05004], lr: 0.056646, loss: 1.8372
2022-08-02 19:22:48 - train: epoch 0067, iter [00400, 05004], lr: 0.056586, loss: 1.5047
2022-08-02 19:23:45 - train: epoch 0067, iter [00500, 05004], lr: 0.056527, loss: 1.7910
2022-08-02 19:24:42 - train: epoch 0067, iter [00600, 05004], lr: 0.056467, loss: 1.4808
2022-08-02 19:25:37 - train: epoch 0067, iter [00700, 05004], lr: 0.056408, loss: 1.5649
2022-08-02 19:26:33 - train: epoch 0067, iter [00800, 05004], lr: 0.056348, loss: 1.6186
2022-08-02 19:27:30 - train: epoch 0067, iter [00900, 05004], lr: 0.056289, loss: 1.8212
2022-08-02 19:28:24 - train: epoch 0067, iter [01000, 05004], lr: 0.056229, loss: 1.6520
2022-08-02 19:29:20 - train: epoch 0067, iter [01100, 05004], lr: 0.056170, loss: 1.5802
2022-08-02 19:30:17 - train: epoch 0067, iter [01200, 05004], lr: 0.056111, loss: 1.5902
2022-08-02 19:31:13 - train: epoch 0067, iter [01300, 05004], lr: 0.056051, loss: 1.7765
2022-08-02 19:32:09 - train: epoch 0067, iter [01400, 05004], lr: 0.055992, loss: 1.8019
2022-08-02 19:33:06 - train: epoch 0067, iter [01500, 05004], lr: 0.055933, loss: 1.4040
2022-08-02 19:34:00 - train: epoch 0067, iter [01600, 05004], lr: 0.055873, loss: 1.6799
2022-08-02 19:34:57 - train: epoch 0067, iter [01700, 05004], lr: 0.055814, loss: 1.5285
2022-08-02 19:35:52 - train: epoch 0067, iter [01800, 05004], lr: 0.055755, loss: 1.6622
2022-08-02 19:36:48 - train: epoch 0067, iter [01900, 05004], lr: 0.055695, loss: 1.6988
2022-08-02 19:37:44 - train: epoch 0067, iter [02000, 05004], lr: 0.055636, loss: 1.6751
2022-08-02 19:38:39 - train: epoch 0067, iter [02100, 05004], lr: 0.055577, loss: 1.4449
2022-08-02 19:39:36 - train: epoch 0067, iter [02200, 05004], lr: 0.055518, loss: 1.6139
2022-08-02 19:40:32 - train: epoch 0067, iter [02300, 05004], lr: 0.055459, loss: 1.7896
2022-08-02 19:41:28 - train: epoch 0067, iter [02400, 05004], lr: 0.055399, loss: 1.6622
2022-08-02 19:42:23 - train: epoch 0067, iter [02500, 05004], lr: 0.055340, loss: 1.7495
2022-08-02 19:43:19 - train: epoch 0067, iter [02600, 05004], lr: 0.055281, loss: 1.6648
2022-08-02 19:44:14 - train: epoch 0067, iter [02700, 05004], lr: 0.055222, loss: 1.3952
2022-08-02 19:45:11 - train: epoch 0067, iter [02800, 05004], lr: 0.055163, loss: 1.5661
2022-08-02 19:46:07 - train: epoch 0067, iter [02900, 05004], lr: 0.055104, loss: 1.7485
2022-08-02 19:47:04 - train: epoch 0067, iter [03000, 05004], lr: 0.055045, loss: 1.6146
2022-08-02 19:48:00 - train: epoch 0067, iter [03100, 05004], lr: 0.054986, loss: 1.7348
2022-08-02 19:48:56 - train: epoch 0067, iter [03200, 05004], lr: 0.054927, loss: 1.6596
2022-08-02 19:49:52 - train: epoch 0067, iter [03300, 05004], lr: 0.054868, loss: 1.5399
2022-08-02 19:50:47 - train: epoch 0067, iter [03400, 05004], lr: 0.054809, loss: 1.4897
2022-08-02 19:51:44 - train: epoch 0067, iter [03500, 05004], lr: 0.054750, loss: 1.5027
2022-08-02 19:52:42 - train: epoch 0067, iter [03600, 05004], lr: 0.054691, loss: 1.7142
2022-08-02 19:53:36 - train: epoch 0067, iter [03700, 05004], lr: 0.054632, loss: 1.8288
2022-08-02 19:54:33 - train: epoch 0067, iter [03800, 05004], lr: 0.054573, loss: 1.7575
2022-08-02 19:55:28 - train: epoch 0067, iter [03900, 05004], lr: 0.054514, loss: 1.6218
2022-08-02 19:56:25 - train: epoch 0067, iter [04000, 05004], lr: 0.054456, loss: 1.7068
2022-08-02 19:57:23 - train: epoch 0067, iter [04100, 05004], lr: 0.054397, loss: 1.5135
2022-08-02 19:58:18 - train: epoch 0067, iter [04200, 05004], lr: 0.054338, loss: 1.6250
2022-08-02 19:59:14 - train: epoch 0067, iter [04300, 05004], lr: 0.054279, loss: 1.3581
2022-08-02 20:00:10 - train: epoch 0067, iter [04400, 05004], lr: 0.054220, loss: 1.6603
2022-08-02 20:01:07 - train: epoch 0067, iter [04500, 05004], lr: 0.054162, loss: 1.7192
2022-08-02 20:02:03 - train: epoch 0067, iter [04600, 05004], lr: 0.054103, loss: 1.4694
2022-08-02 20:02:59 - train: epoch 0067, iter [04700, 05004], lr: 0.054044, loss: 1.7363
2022-08-02 20:03:55 - train: epoch 0067, iter [04800, 05004], lr: 0.053986, loss: 1.3948
2022-08-02 20:04:51 - train: epoch 0067, iter [04900, 05004], lr: 0.053927, loss: 1.7497
2022-08-02 20:05:45 - train: epoch 0067, iter [05000, 05004], lr: 0.053868, loss: 1.7442
2022-08-02 20:05:46 - train: epoch 067, train_loss: 1.6316
2022-08-02 20:07:50 - eval: epoch: 067, acc1: 65.158%, acc5: 86.956%, test_loss: 1.4259, per_image_load_time: 4.270ms, per_image_inference_time: 0.580ms
2022-08-02 20:07:50 - until epoch: 067, best_acc1: 65.316%
2022-08-02 20:07:50 - epoch 068 lr: 0.053865
2022-08-02 20:08:55 - train: epoch 0068, iter [00100, 05004], lr: 0.053807, loss: 1.6235
2022-08-02 20:09:52 - train: epoch 0068, iter [00200, 05004], lr: 0.053749, loss: 1.7190
2022-08-02 20:10:48 - train: epoch 0068, iter [00300, 05004], lr: 0.053690, loss: 1.5202
2022-08-02 20:11:44 - train: epoch 0068, iter [00400, 05004], lr: 0.053632, loss: 1.5467
2022-08-02 20:12:42 - train: epoch 0068, iter [00500, 05004], lr: 0.053573, loss: 1.7536
2022-08-02 20:13:36 - train: epoch 0068, iter [00600, 05004], lr: 0.053514, loss: 1.7040
2022-08-02 20:14:31 - train: epoch 0068, iter [00700, 05004], lr: 0.053456, loss: 1.8087
2022-08-02 20:15:28 - train: epoch 0068, iter [00800, 05004], lr: 0.053397, loss: 1.5163
2022-08-02 20:16:24 - train: epoch 0068, iter [00900, 05004], lr: 0.053339, loss: 1.5594
2022-08-02 20:17:19 - train: epoch 0068, iter [01000, 05004], lr: 0.053281, loss: 1.5055
2022-08-02 20:18:16 - train: epoch 0068, iter [01100, 05004], lr: 0.053222, loss: 1.7224
2022-08-02 20:19:10 - train: epoch 0068, iter [01200, 05004], lr: 0.053164, loss: 1.4180
2022-08-02 20:20:07 - train: epoch 0068, iter [01300, 05004], lr: 0.053105, loss: 1.6054
2022-08-02 20:21:03 - train: epoch 0068, iter [01400, 05004], lr: 0.053047, loss: 1.4866
2022-08-02 20:22:01 - train: epoch 0068, iter [01500, 05004], lr: 0.052989, loss: 1.6505
2022-08-02 20:22:56 - train: epoch 0068, iter [01600, 05004], lr: 0.052930, loss: 1.7453
2022-08-02 20:23:53 - train: epoch 0068, iter [01700, 05004], lr: 0.052872, loss: 1.5465
2022-08-02 20:24:48 - train: epoch 0068, iter [01800, 05004], lr: 0.052814, loss: 1.7584
2022-08-02 20:25:45 - train: epoch 0068, iter [01900, 05004], lr: 0.052756, loss: 1.6995
2022-08-02 20:26:42 - train: epoch 0068, iter [02000, 05004], lr: 0.052697, loss: 1.7916
2022-08-02 20:27:37 - train: epoch 0068, iter [02100, 05004], lr: 0.052639, loss: 1.7370
2022-08-02 20:28:34 - train: epoch 0068, iter [02200, 05004], lr: 0.052581, loss: 1.5884
2022-08-02 20:29:29 - train: epoch 0068, iter [02300, 05004], lr: 0.052523, loss: 1.5619
2022-08-02 20:30:24 - train: epoch 0068, iter [02400, 05004], lr: 0.052465, loss: 1.5030
2022-08-02 20:31:22 - train: epoch 0068, iter [02500, 05004], lr: 0.052406, loss: 1.8291
2022-08-02 20:32:17 - train: epoch 0068, iter [02600, 05004], lr: 0.052348, loss: 1.4073
2022-08-02 20:33:14 - train: epoch 0068, iter [02700, 05004], lr: 0.052290, loss: 1.4744
2022-08-02 20:34:10 - train: epoch 0068, iter [02800, 05004], lr: 0.052232, loss: 1.7168
2022-08-02 20:35:07 - train: epoch 0068, iter [02900, 05004], lr: 0.052174, loss: 1.6977
2022-08-02 20:36:03 - train: epoch 0068, iter [03000, 05004], lr: 0.052116, loss: 1.8246
2022-08-02 20:36:58 - train: epoch 0068, iter [03100, 05004], lr: 0.052058, loss: 1.3206
2022-08-02 20:37:54 - train: epoch 0068, iter [03200, 05004], lr: 0.052000, loss: 1.7394
2022-08-02 20:38:52 - train: epoch 0068, iter [03300, 05004], lr: 0.051942, loss: 1.4412
2022-08-02 20:39:46 - train: epoch 0068, iter [03400, 05004], lr: 0.051884, loss: 1.4999
2022-08-02 20:40:41 - train: epoch 0068, iter [03500, 05004], lr: 0.051826, loss: 1.7142
2022-08-02 20:41:37 - train: epoch 0068, iter [03600, 05004], lr: 0.051768, loss: 1.6582
2022-08-02 20:42:32 - train: epoch 0068, iter [03700, 05004], lr: 0.051710, loss: 1.5795
2022-08-02 20:43:27 - train: epoch 0068, iter [03800, 05004], lr: 0.051653, loss: 1.7795
2022-08-02 20:44:21 - train: epoch 0068, iter [03900, 05004], lr: 0.051595, loss: 1.7481
2022-08-02 20:45:19 - train: epoch 0068, iter [04000, 05004], lr: 0.051537, loss: 1.5812
2022-08-02 20:46:14 - train: epoch 0068, iter [04100, 05004], lr: 0.051479, loss: 1.4650
2022-08-02 20:47:09 - train: epoch 0068, iter [04200, 05004], lr: 0.051421, loss: 1.6311
2022-08-02 20:48:04 - train: epoch 0068, iter [04300, 05004], lr: 0.051364, loss: 1.6438
2022-08-02 20:49:01 - train: epoch 0068, iter [04400, 05004], lr: 0.051306, loss: 1.4262
2022-08-02 20:49:57 - train: epoch 0068, iter [04500, 05004], lr: 0.051248, loss: 1.5013
2022-08-02 20:50:53 - train: epoch 0068, iter [04600, 05004], lr: 0.051190, loss: 1.7334
2022-08-02 20:51:50 - train: epoch 0068, iter [04700, 05004], lr: 0.051133, loss: 1.6277
2022-08-02 20:52:46 - train: epoch 0068, iter [04800, 05004], lr: 0.051075, loss: 1.7223
2022-08-02 20:53:42 - train: epoch 0068, iter [04900, 05004], lr: 0.051018, loss: 1.3674
2022-08-02 20:54:36 - train: epoch 0068, iter [05000, 05004], lr: 0.050960, loss: 1.5250
2022-08-02 20:54:38 - train: epoch 068, train_loss: 1.6113
2022-08-02 20:56:41 - eval: epoch: 068, acc1: 65.922%, acc5: 87.450%, test_loss: 1.3865, per_image_load_time: 3.784ms, per_image_inference_time: 0.586ms
2022-08-02 20:56:41 - until epoch: 068, best_acc1: 65.922%
2022-08-02 20:56:41 - epoch 069 lr: 0.050957
2022-08-02 20:57:48 - train: epoch 0069, iter [00100, 05004], lr: 0.050900, loss: 1.8093
2022-08-02 20:58:44 - train: epoch 0069, iter [00200, 05004], lr: 0.050843, loss: 1.6023
2022-08-02 20:59:40 - train: epoch 0069, iter [00300, 05004], lr: 0.050785, loss: 1.6497
2022-08-02 21:00:37 - train: epoch 0069, iter [00400, 05004], lr: 0.050727, loss: 1.4852
2022-08-02 21:01:33 - train: epoch 0069, iter [00500, 05004], lr: 0.050670, loss: 1.7122
2022-08-02 21:02:30 - train: epoch 0069, iter [00600, 05004], lr: 0.050612, loss: 1.4855
2022-08-02 21:03:25 - train: epoch 0069, iter [00700, 05004], lr: 0.050555, loss: 1.6359
2022-08-02 21:04:21 - train: epoch 0069, iter [00800, 05004], lr: 0.050498, loss: 1.6821
2022-08-02 21:05:19 - train: epoch 0069, iter [00900, 05004], lr: 0.050440, loss: 1.2663
2022-08-02 21:06:14 - train: epoch 0069, iter [01000, 05004], lr: 0.050383, loss: 1.7094
2022-08-02 21:07:11 - train: epoch 0069, iter [01100, 05004], lr: 0.050325, loss: 1.6469
2022-08-02 21:08:06 - train: epoch 0069, iter [01200, 05004], lr: 0.050268, loss: 1.4737
2022-08-02 21:09:04 - train: epoch 0069, iter [01300, 05004], lr: 0.050211, loss: 1.7776
2022-08-02 21:10:00 - train: epoch 0069, iter [01400, 05004], lr: 0.050153, loss: 1.5136
2022-08-02 21:10:56 - train: epoch 0069, iter [01500, 05004], lr: 0.050096, loss: 1.4616
2022-08-02 21:11:53 - train: epoch 0069, iter [01600, 05004], lr: 0.050039, loss: 1.7152
2022-08-02 21:12:48 - train: epoch 0069, iter [01700, 05004], lr: 0.049982, loss: 1.7048
2022-08-02 21:13:45 - train: epoch 0069, iter [01800, 05004], lr: 0.049924, loss: 1.6937
2022-08-02 21:14:42 - train: epoch 0069, iter [01900, 05004], lr: 0.049867, loss: 1.6111
2022-08-02 21:15:40 - train: epoch 0069, iter [02000, 05004], lr: 0.049810, loss: 1.4408
2022-08-02 21:16:35 - train: epoch 0069, iter [02100, 05004], lr: 0.049753, loss: 1.7773
2022-08-02 21:17:31 - train: epoch 0069, iter [02200, 05004], lr: 0.049696, loss: 1.6381
2022-08-02 21:18:30 - train: epoch 0069, iter [02300, 05004], lr: 0.049639, loss: 1.5019
2022-08-02 21:19:25 - train: epoch 0069, iter [02400, 05004], lr: 0.049582, loss: 1.6405
2022-08-02 21:20:22 - train: epoch 0069, iter [02500, 05004], lr: 0.049525, loss: 1.4088
2022-08-02 21:21:19 - train: epoch 0069, iter [02600, 05004], lr: 0.049468, loss: 1.5272
2022-08-02 21:22:18 - train: epoch 0069, iter [02700, 05004], lr: 0.049411, loss: 1.6191
2022-08-02 21:23:16 - train: epoch 0069, iter [02800, 05004], lr: 0.049354, loss: 1.7225
2022-08-02 21:24:14 - train: epoch 0069, iter [02900, 05004], lr: 0.049297, loss: 1.6656
2022-08-02 21:25:13 - train: epoch 0069, iter [03000, 05004], lr: 0.049240, loss: 1.4790
2022-08-02 21:26:10 - train: epoch 0069, iter [03100, 05004], lr: 0.049183, loss: 1.5728
2022-08-02 21:27:08 - train: epoch 0069, iter [03200, 05004], lr: 0.049126, loss: 1.5130
2022-08-02 21:28:05 - train: epoch 0069, iter [03300, 05004], lr: 0.049069, loss: 1.5730
2022-08-02 21:29:06 - train: epoch 0069, iter [03400, 05004], lr: 0.049012, loss: 1.6905
2022-08-02 21:30:03 - train: epoch 0069, iter [03500, 05004], lr: 0.048955, loss: 1.5513
2022-08-02 21:31:03 - train: epoch 0069, iter [03600, 05004], lr: 0.048898, loss: 1.4062
2022-08-02 21:32:01 - train: epoch 0069, iter [03700, 05004], lr: 0.048842, loss: 1.6505
2022-08-02 21:32:59 - train: epoch 0069, iter [03800, 05004], lr: 0.048785, loss: 1.6313
2022-08-02 21:33:57 - train: epoch 0069, iter [03900, 05004], lr: 0.048728, loss: 1.6596
2022-08-02 21:34:55 - train: epoch 0069, iter [04000, 05004], lr: 0.048671, loss: 1.7942
2022-08-02 21:35:52 - train: epoch 0069, iter [04100, 05004], lr: 0.048615, loss: 1.5666
2022-08-02 21:36:50 - train: epoch 0069, iter [04200, 05004], lr: 0.048558, loss: 1.6402
2022-08-02 21:37:47 - train: epoch 0069, iter [04300, 05004], lr: 0.048501, loss: 1.5603
2022-08-02 21:38:43 - train: epoch 0069, iter [04400, 05004], lr: 0.048445, loss: 1.6090
2022-08-02 21:39:40 - train: epoch 0069, iter [04500, 05004], lr: 0.048388, loss: 1.7901
2022-08-02 21:40:37 - train: epoch 0069, iter [04600, 05004], lr: 0.048331, loss: 1.9346
2022-08-02 21:41:35 - train: epoch 0069, iter [04700, 05004], lr: 0.048275, loss: 1.4940
2022-08-02 21:42:33 - train: epoch 0069, iter [04800, 05004], lr: 0.048218, loss: 1.7180
2022-08-02 21:43:30 - train: epoch 0069, iter [04900, 05004], lr: 0.048162, loss: 1.6322
2022-08-02 21:44:26 - train: epoch 0069, iter [05000, 05004], lr: 0.048105, loss: 1.7150
2022-08-02 21:44:27 - train: epoch 069, train_loss: 1.5953
2022-08-02 21:46:39 - eval: epoch: 069, acc1: 66.060%, acc5: 87.436%, test_loss: 1.3844, per_image_load_time: 1.831ms, per_image_inference_time: 0.625ms
2022-08-02 21:46:39 - until epoch: 069, best_acc1: 66.060%
2022-08-02 21:46:39 - epoch 070 lr: 0.048102
2022-08-02 21:47:51 - train: epoch 0070, iter [00100, 05004], lr: 0.048047, loss: 1.4032
2022-08-02 21:48:49 - train: epoch 0070, iter [00200, 05004], lr: 0.047990, loss: 1.3159
2022-08-02 21:49:50 - train: epoch 0070, iter [00300, 05004], lr: 0.047934, loss: 1.6719
2022-08-02 21:50:48 - train: epoch 0070, iter [00400, 05004], lr: 0.047877, loss: 1.3321
2022-08-02 21:51:46 - train: epoch 0070, iter [00500, 05004], lr: 0.047821, loss: 1.6285
2022-08-02 21:52:45 - train: epoch 0070, iter [00600, 05004], lr: 0.047765, loss: 1.6140
2022-08-02 21:53:46 - train: epoch 0070, iter [00700, 05004], lr: 0.047708, loss: 1.4545
2022-08-02 21:54:46 - train: epoch 0070, iter [00800, 05004], lr: 0.047652, loss: 1.5861
2022-08-02 21:55:45 - train: epoch 0070, iter [00900, 05004], lr: 0.047596, loss: 1.6713
2022-08-02 21:56:44 - train: epoch 0070, iter [01000, 05004], lr: 0.047539, loss: 1.4866
2022-08-02 21:57:45 - train: epoch 0070, iter [01100, 05004], lr: 0.047483, loss: 1.9328
2022-08-02 21:58:44 - train: epoch 0070, iter [01200, 05004], lr: 0.047427, loss: 1.4718
2022-08-02 21:59:44 - train: epoch 0070, iter [01300, 05004], lr: 0.047371, loss: 1.5175
2022-08-02 22:00:44 - train: epoch 0070, iter [01400, 05004], lr: 0.047314, loss: 1.4470
2022-08-02 22:01:43 - train: epoch 0070, iter [01500, 05004], lr: 0.047258, loss: 1.6615
2022-08-02 22:02:41 - train: epoch 0070, iter [01600, 05004], lr: 0.047202, loss: 1.5380
2022-08-02 22:03:42 - train: epoch 0070, iter [01700, 05004], lr: 0.047146, loss: 1.5228
2022-08-02 22:04:41 - train: epoch 0070, iter [01800, 05004], lr: 0.047090, loss: 1.4273
2022-08-02 22:05:41 - train: epoch 0070, iter [01900, 05004], lr: 0.047034, loss: 1.5985
2022-08-02 22:06:39 - train: epoch 0070, iter [02000, 05004], lr: 0.046978, loss: 1.5675
2022-08-02 22:07:40 - train: epoch 0070, iter [02100, 05004], lr: 0.046922, loss: 1.6296
2022-08-02 22:08:39 - train: epoch 0070, iter [02200, 05004], lr: 0.046866, loss: 1.6099
2022-08-02 22:09:37 - train: epoch 0070, iter [02300, 05004], lr: 0.046810, loss: 1.6113
2022-08-02 22:10:38 - train: epoch 0070, iter [02400, 05004], lr: 0.046754, loss: 1.7073
2022-08-02 22:11:37 - train: epoch 0070, iter [02500, 05004], lr: 0.046698, loss: 1.5423
2022-08-02 22:12:36 - train: epoch 0070, iter [02600, 05004], lr: 0.046642, loss: 1.6480
2022-08-02 22:13:35 - train: epoch 0070, iter [02700, 05004], lr: 0.046586, loss: 1.5480
2022-08-02 22:14:36 - train: epoch 0070, iter [02800, 05004], lr: 0.046530, loss: 1.6728
2022-08-02 22:15:35 - train: epoch 0070, iter [02900, 05004], lr: 0.046474, loss: 1.5716
2022-08-02 22:16:34 - train: epoch 0070, iter [03000, 05004], lr: 0.046419, loss: 1.6944
2022-08-02 22:17:32 - train: epoch 0070, iter [03100, 05004], lr: 0.046363, loss: 1.5871
2022-08-02 22:18:33 - train: epoch 0070, iter [03200, 05004], lr: 0.046307, loss: 1.6414
2022-08-02 22:19:31 - train: epoch 0070, iter [03300, 05004], lr: 0.046251, loss: 1.5483
2022-08-02 22:20:31 - train: epoch 0070, iter [03400, 05004], lr: 0.046196, loss: 1.8150
2022-08-02 22:21:30 - train: epoch 0070, iter [03500, 05004], lr: 0.046140, loss: 1.6874
2022-08-02 22:22:31 - train: epoch 0070, iter [03600, 05004], lr: 0.046084, loss: 1.4830
2022-08-02 22:23:31 - train: epoch 0070, iter [03700, 05004], lr: 0.046029, loss: 1.6600
2022-08-02 22:24:28 - train: epoch 0070, iter [03800, 05004], lr: 0.045973, loss: 1.4817
2022-08-02 22:25:28 - train: epoch 0070, iter [03900, 05004], lr: 0.045917, loss: 1.4314
2022-08-02 22:26:27 - train: epoch 0070, iter [04000, 05004], lr: 0.045862, loss: 1.6122
2022-08-02 22:27:26 - train: epoch 0070, iter [04100, 05004], lr: 0.045806, loss: 1.6040
2022-08-02 22:28:24 - train: epoch 0070, iter [04200, 05004], lr: 0.045751, loss: 1.7300
2022-08-02 22:29:22 - train: epoch 0070, iter [04300, 05004], lr: 0.045695, loss: 1.4871
2022-08-02 22:30:21 - train: epoch 0070, iter [04400, 05004], lr: 0.045640, loss: 1.3611
2022-08-02 22:31:21 - train: epoch 0070, iter [04500, 05004], lr: 0.045584, loss: 1.5934
2022-08-02 22:32:18 - train: epoch 0070, iter [04600, 05004], lr: 0.045529, loss: 1.7441
2022-08-02 22:33:17 - train: epoch 0070, iter [04700, 05004], lr: 0.045473, loss: 1.4702
2022-08-02 22:34:12 - train: epoch 0070, iter [04800, 05004], lr: 0.045418, loss: 1.5855
2022-08-02 22:35:12 - train: epoch 0070, iter [04900, 05004], lr: 0.045363, loss: 1.7318
2022-08-02 22:36:07 - train: epoch 0070, iter [05000, 05004], lr: 0.045307, loss: 1.5452
2022-08-02 22:36:08 - train: epoch 070, train_loss: 1.5753
2022-08-02 22:38:18 - eval: epoch: 070, acc1: 66.672%, acc5: 87.732%, test_loss: 1.3592, per_image_load_time: 2.397ms, per_image_inference_time: 0.645ms
2022-08-02 22:38:18 - until epoch: 070, best_acc1: 66.672%
2022-08-02 22:38:18 - epoch 071 lr: 0.045305
2022-08-02 22:39:27 - train: epoch 0071, iter [00100, 05004], lr: 0.045250, loss: 1.4302
2022-08-02 22:40:26 - train: epoch 0071, iter [00200, 05004], lr: 0.045195, loss: 1.3907
2022-08-02 22:41:25 - train: epoch 0071, iter [00300, 05004], lr: 0.045139, loss: 1.6622
2022-08-02 22:42:22 - train: epoch 0071, iter [00400, 05004], lr: 0.045084, loss: 1.6787
2022-08-02 22:43:20 - train: epoch 0071, iter [00500, 05004], lr: 0.045029, loss: 1.7381
2022-08-02 22:44:19 - train: epoch 0071, iter [00600, 05004], lr: 0.044974, loss: 1.6120
2022-08-02 22:45:16 - train: epoch 0071, iter [00700, 05004], lr: 0.044918, loss: 1.4093
2022-08-02 22:46:16 - train: epoch 0071, iter [00800, 05004], lr: 0.044863, loss: 1.3797
2022-08-02 22:47:14 - train: epoch 0071, iter [00900, 05004], lr: 0.044808, loss: 1.7127
2022-08-02 22:48:12 - train: epoch 0071, iter [01000, 05004], lr: 0.044753, loss: 1.3858
2022-08-02 22:49:10 - train: epoch 0071, iter [01100, 05004], lr: 0.044698, loss: 1.6597
2022-08-02 22:50:09 - train: epoch 0071, iter [01200, 05004], lr: 0.044643, loss: 1.5923
2022-08-02 22:51:06 - train: epoch 0071, iter [01300, 05004], lr: 0.044588, loss: 1.4565
2022-08-02 22:52:04 - train: epoch 0071, iter [01400, 05004], lr: 0.044533, loss: 1.4736
2022-08-02 22:53:03 - train: epoch 0071, iter [01500, 05004], lr: 0.044478, loss: 1.4689
2022-08-02 22:54:03 - train: epoch 0071, iter [01600, 05004], lr: 0.044423, loss: 1.5005
2022-08-02 22:55:01 - train: epoch 0071, iter [01700, 05004], lr: 0.044368, loss: 1.4033
2022-08-02 22:56:00 - train: epoch 0071, iter [01800, 05004], lr: 0.044313, loss: 1.6438
2022-08-02 22:56:58 - train: epoch 0071, iter [01900, 05004], lr: 0.044258, loss: 1.4661
2022-08-02 22:57:55 - train: epoch 0071, iter [02000, 05004], lr: 0.044203, loss: 1.5420
2022-08-02 22:58:54 - train: epoch 0071, iter [02100, 05004], lr: 0.044149, loss: 1.4388
2022-08-02 22:59:53 - train: epoch 0071, iter [02200, 05004], lr: 0.044094, loss: 1.2519
2022-08-02 23:00:52 - train: epoch 0071, iter [02300, 05004], lr: 0.044039, loss: 1.5549
2022-08-02 23:01:50 - train: epoch 0071, iter [02400, 05004], lr: 0.043984, loss: 1.5936
2022-08-02 23:02:48 - train: epoch 0071, iter [02500, 05004], lr: 0.043930, loss: 1.5983
2022-08-02 23:03:46 - train: epoch 0071, iter [02600, 05004], lr: 0.043875, loss: 1.7336
2022-08-02 23:04:44 - train: epoch 0071, iter [02700, 05004], lr: 0.043820, loss: 1.6167
2022-08-02 23:05:41 - train: epoch 0071, iter [02800, 05004], lr: 0.043766, loss: 1.5918
2022-08-02 23:06:39 - train: epoch 0071, iter [02900, 05004], lr: 0.043711, loss: 1.3656
2022-08-02 23:07:38 - train: epoch 0071, iter [03000, 05004], lr: 0.043656, loss: 1.7168
2022-08-02 23:08:35 - train: epoch 0071, iter [03100, 05004], lr: 0.043602, loss: 1.4108
2022-08-02 23:09:35 - train: epoch 0071, iter [03200, 05004], lr: 0.043547, loss: 1.4927
2022-08-02 23:10:32 - train: epoch 0071, iter [03300, 05004], lr: 0.043493, loss: 1.7510
2022-08-02 23:11:31 - train: epoch 0071, iter [03400, 05004], lr: 0.043438, loss: 1.4477
2022-08-02 23:12:28 - train: epoch 0071, iter [03500, 05004], lr: 0.043384, loss: 1.5779
2022-08-02 23:13:25 - train: epoch 0071, iter [03600, 05004], lr: 0.043329, loss: 1.6605
2022-08-02 23:14:24 - train: epoch 0071, iter [03700, 05004], lr: 0.043275, loss: 1.5871
2022-08-02 23:15:23 - train: epoch 0071, iter [03800, 05004], lr: 0.043220, loss: 1.5922
2022-08-02 23:16:20 - train: epoch 0071, iter [03900, 05004], lr: 0.043166, loss: 1.6557
2022-08-02 23:17:19 - train: epoch 0071, iter [04000, 05004], lr: 0.043112, loss: 1.4949
2022-08-02 23:18:17 - train: epoch 0071, iter [04100, 05004], lr: 0.043057, loss: 1.4753
2022-08-02 23:19:15 - train: epoch 0071, iter [04200, 05004], lr: 0.043003, loss: 1.8144
2022-08-02 23:20:13 - train: epoch 0071, iter [04300, 05004], lr: 0.042949, loss: 1.4833
2022-08-02 23:21:11 - train: epoch 0071, iter [04400, 05004], lr: 0.042894, loss: 1.5421
2022-08-02 23:22:08 - train: epoch 0071, iter [04500, 05004], lr: 0.042840, loss: 1.5939
2022-08-02 23:23:07 - train: epoch 0071, iter [04600, 05004], lr: 0.042786, loss: 1.3786
2022-08-02 23:24:06 - train: epoch 0071, iter [04700, 05004], lr: 0.042732, loss: 1.6802
2022-08-02 23:25:03 - train: epoch 0071, iter [04800, 05004], lr: 0.042678, loss: 1.6018
2022-08-02 23:26:02 - train: epoch 0071, iter [04900, 05004], lr: 0.042623, loss: 1.4522
2022-08-02 23:26:56 - train: epoch 0071, iter [05000, 05004], lr: 0.042569, loss: 1.5920
2022-08-02 23:26:58 - train: epoch 071, train_loss: 1.5563
2022-08-02 23:29:06 - eval: epoch: 071, acc1: 66.792%, acc5: 87.792%, test_loss: 1.3668, per_image_load_time: 4.321ms, per_image_inference_time: 0.621ms
2022-08-02 23:29:06 - until epoch: 071, best_acc1: 66.792%
2022-08-02 23:29:06 - epoch 072 lr: 0.042567
2022-08-02 23:30:16 - train: epoch 0072, iter [00100, 05004], lr: 0.042513, loss: 1.5527
2022-08-02 23:31:15 - train: epoch 0072, iter [00200, 05004], lr: 0.042459, loss: 1.3991
2022-08-02 23:32:13 - train: epoch 0072, iter [00300, 05004], lr: 0.042405, loss: 1.3231
2022-08-02 23:33:10 - train: epoch 0072, iter [00400, 05004], lr: 0.042351, loss: 1.7776
2022-08-02 23:34:08 - train: epoch 0072, iter [00500, 05004], lr: 0.042297, loss: 1.5147
2022-08-02 23:35:06 - train: epoch 0072, iter [00600, 05004], lr: 0.042243, loss: 1.4361
2022-08-02 23:36:04 - train: epoch 0072, iter [00700, 05004], lr: 0.042189, loss: 1.7147
2022-08-02 23:37:01 - train: epoch 0072, iter [00800, 05004], lr: 0.042135, loss: 1.8682
2022-08-02 23:38:00 - train: epoch 0072, iter [00900, 05004], lr: 0.042081, loss: 1.4060
2022-08-02 23:38:57 - train: epoch 0072, iter [01000, 05004], lr: 0.042027, loss: 1.2710
2022-08-02 23:39:54 - train: epoch 0072, iter [01100, 05004], lr: 0.041974, loss: 1.6165
2022-08-02 23:40:53 - train: epoch 0072, iter [01200, 05004], lr: 0.041920, loss: 1.5574
2022-08-02 23:41:51 - train: epoch 0072, iter [01300, 05004], lr: 0.041866, loss: 1.6065
2022-08-02 23:42:48 - train: epoch 0072, iter [01400, 05004], lr: 0.041812, loss: 1.4890
2022-08-02 23:43:46 - train: epoch 0072, iter [01500, 05004], lr: 0.041758, loss: 1.5743
2022-08-02 23:44:43 - train: epoch 0072, iter [01600, 05004], lr: 0.041705, loss: 1.3540
2022-08-02 23:45:42 - train: epoch 0072, iter [01700, 05004], lr: 0.041651, loss: 1.4250
2022-08-02 23:46:40 - train: epoch 0072, iter [01800, 05004], lr: 0.041597, loss: 1.3234
2022-08-02 23:47:37 - train: epoch 0072, iter [01900, 05004], lr: 0.041544, loss: 1.6212
2022-08-02 23:48:37 - train: epoch 0072, iter [02000, 05004], lr: 0.041490, loss: 1.5057
2022-08-02 23:49:36 - train: epoch 0072, iter [02100, 05004], lr: 0.041437, loss: 1.6886
2022-08-02 23:50:31 - train: epoch 0072, iter [02200, 05004], lr: 0.041383, loss: 1.6715
2022-08-02 23:51:32 - train: epoch 0072, iter [02300, 05004], lr: 0.041330, loss: 1.7716
2022-08-02 23:52:29 - train: epoch 0072, iter [02400, 05004], lr: 0.041276, loss: 1.3867
2022-08-02 23:53:27 - train: epoch 0072, iter [02500, 05004], lr: 0.041223, loss: 1.4141
2022-08-02 23:54:25 - train: epoch 0072, iter [02600, 05004], lr: 0.041169, loss: 1.6185
2022-08-02 23:55:23 - train: epoch 0072, iter [02700, 05004], lr: 0.041116, loss: 1.5674
2022-08-02 23:56:21 - train: epoch 0072, iter [02800, 05004], lr: 0.041062, loss: 1.5495
2022-08-02 23:57:19 - train: epoch 0072, iter [02900, 05004], lr: 0.041009, loss: 1.4895
2022-08-02 23:58:16 - train: epoch 0072, iter [03000, 05004], lr: 0.040956, loss: 1.5304
2022-08-02 23:59:15 - train: epoch 0072, iter [03100, 05004], lr: 0.040902, loss: 1.6147
2022-08-03 00:00:13 - train: epoch 0072, iter [03200, 05004], lr: 0.040849, loss: 1.5405
2022-08-03 00:01:10 - train: epoch 0072, iter [03300, 05004], lr: 0.040796, loss: 1.5750
2022-08-03 00:02:09 - train: epoch 0072, iter [03400, 05004], lr: 0.040742, loss: 1.7832
2022-08-03 00:03:07 - train: epoch 0072, iter [03500, 05004], lr: 0.040689, loss: 1.5316
2022-08-03 00:04:04 - train: epoch 0072, iter [03600, 05004], lr: 0.040636, loss: 1.6547
2022-08-03 00:05:04 - train: epoch 0072, iter [03700, 05004], lr: 0.040583, loss: 1.5606
2022-08-03 00:05:59 - train: epoch 0072, iter [03800, 05004], lr: 0.040530, loss: 1.7011
2022-08-03 00:06:58 - train: epoch 0072, iter [03900, 05004], lr: 0.040477, loss: 1.6321
2022-08-03 00:07:56 - train: epoch 0072, iter [04000, 05004], lr: 0.040423, loss: 1.5063
2022-08-03 00:08:55 - train: epoch 0072, iter [04100, 05004], lr: 0.040370, loss: 1.7843
2022-08-03 00:09:52 - train: epoch 0072, iter [04200, 05004], lr: 0.040317, loss: 1.5077
2022-08-03 00:10:50 - train: epoch 0072, iter [04300, 05004], lr: 0.040264, loss: 1.3801
2022-08-03 00:11:48 - train: epoch 0072, iter [04400, 05004], lr: 0.040211, loss: 1.3690
2022-08-03 00:12:46 - train: epoch 0072, iter [04500, 05004], lr: 0.040158, loss: 1.4673
2022-08-03 00:13:43 - train: epoch 0072, iter [04600, 05004], lr: 0.040105, loss: 1.4700
2022-08-03 00:14:41 - train: epoch 0072, iter [04700, 05004], lr: 0.040053, loss: 1.4107
2022-08-03 00:15:40 - train: epoch 0072, iter [04800, 05004], lr: 0.040000, loss: 1.7197
2022-08-03 00:16:38 - train: epoch 0072, iter [04900, 05004], lr: 0.039947, loss: 1.7000
2022-08-03 00:17:33 - train: epoch 0072, iter [05000, 05004], lr: 0.039894, loss: 1.5684
2022-08-03 00:17:34 - train: epoch 072, train_loss: 1.5380
2022-08-03 00:19:48 - eval: epoch: 072, acc1: 66.570%, acc5: 87.806%, test_loss: 1.3598, per_image_load_time: 4.296ms, per_image_inference_time: 0.675ms
2022-08-03 00:19:48 - until epoch: 072, best_acc1: 66.792%
2022-08-03 00:19:48 - epoch 073 lr: 0.039891
2022-08-03 00:20:59 - train: epoch 0073, iter [00100, 05004], lr: 0.039839, loss: 1.6724
2022-08-03 00:21:59 - train: epoch 0073, iter [00200, 05004], lr: 0.039786, loss: 1.5150
2022-08-03 00:22:59 - train: epoch 0073, iter [00300, 05004], lr: 0.039734, loss: 1.5077
2022-08-03 00:23:59 - train: epoch 0073, iter [00400, 05004], lr: 0.039681, loss: 1.3465
2022-08-03 00:24:58 - train: epoch 0073, iter [00500, 05004], lr: 0.039628, loss: 1.4315
2022-08-03 00:25:56 - train: epoch 0073, iter [00600, 05004], lr: 0.039575, loss: 1.4908
2022-08-03 00:26:55 - train: epoch 0073, iter [00700, 05004], lr: 0.039523, loss: 1.5466
2022-08-03 00:27:53 - train: epoch 0073, iter [00800, 05004], lr: 0.039470, loss: 1.3833
2022-08-03 00:28:53 - train: epoch 0073, iter [00900, 05004], lr: 0.039418, loss: 1.3720
2022-08-03 00:29:50 - train: epoch 0073, iter [01000, 05004], lr: 0.039365, loss: 1.4296
2022-08-03 00:30:51 - train: epoch 0073, iter [01100, 05004], lr: 0.039313, loss: 1.5278
2022-08-03 00:31:51 - train: epoch 0073, iter [01200, 05004], lr: 0.039260, loss: 1.6017
2022-08-03 00:32:50 - train: epoch 0073, iter [01300, 05004], lr: 0.039208, loss: 1.7328
2022-08-03 00:33:49 - train: epoch 0073, iter [01400, 05004], lr: 0.039155, loss: 1.4404
2022-08-03 00:34:49 - train: epoch 0073, iter [01500, 05004], lr: 0.039103, loss: 1.6415
2022-08-03 00:35:47 - train: epoch 0073, iter [01600, 05004], lr: 0.039050, loss: 1.4752
2022-08-03 00:36:46 - train: epoch 0073, iter [01700, 05004], lr: 0.038998, loss: 1.4975
2022-08-03 00:37:43 - train: epoch 0073, iter [01800, 05004], lr: 0.038945, loss: 1.3811
2022-08-03 00:38:40 - train: epoch 0073, iter [01900, 05004], lr: 0.038893, loss: 1.7383
2022-08-03 00:39:38 - train: epoch 0073, iter [02000, 05004], lr: 0.038841, loss: 1.2844
2022-08-03 00:40:36 - train: epoch 0073, iter [02100, 05004], lr: 0.038789, loss: 1.5440
2022-08-03 00:41:33 - train: epoch 0073, iter [02200, 05004], lr: 0.038736, loss: 1.6052
2022-08-03 00:42:31 - train: epoch 0073, iter [02300, 05004], lr: 0.038684, loss: 1.5264
2022-08-03 00:43:30 - train: epoch 0073, iter [02400, 05004], lr: 0.038632, loss: 1.7044
2022-08-03 00:44:28 - train: epoch 0073, iter [02500, 05004], lr: 0.038580, loss: 1.6135
2022-08-03 00:45:26 - train: epoch 0073, iter [02600, 05004], lr: 0.038528, loss: 1.6277
2022-08-03 00:46:23 - train: epoch 0073, iter [02700, 05004], lr: 0.038476, loss: 1.5263
2022-08-03 00:47:21 - train: epoch 0073, iter [02800, 05004], lr: 0.038423, loss: 1.4887
2022-08-03 00:48:20 - train: epoch 0073, iter [02900, 05004], lr: 0.038371, loss: 1.8006
2022-08-03 00:49:18 - train: epoch 0073, iter [03000, 05004], lr: 0.038319, loss: 1.4804
2022-08-03 00:50:17 - train: epoch 0073, iter [03100, 05004], lr: 0.038267, loss: 1.3787
2022-08-03 00:51:13 - train: epoch 0073, iter [03200, 05004], lr: 0.038215, loss: 1.4205
2022-08-03 00:52:13 - train: epoch 0073, iter [03300, 05004], lr: 0.038163, loss: 1.5584
2022-08-03 00:53:08 - train: epoch 0073, iter [03400, 05004], lr: 0.038111, loss: 1.5651
2022-08-03 00:54:08 - train: epoch 0073, iter [03500, 05004], lr: 0.038060, loss: 1.5271
2022-08-03 00:55:05 - train: epoch 0073, iter [03600, 05004], lr: 0.038008, loss: 1.3005
2022-08-03 00:56:04 - train: epoch 0073, iter [03700, 05004], lr: 0.037956, loss: 1.3737
2022-08-03 00:57:02 - train: epoch 0073, iter [03800, 05004], lr: 0.037904, loss: 1.7043
2022-08-03 00:58:00 - train: epoch 0073, iter [03900, 05004], lr: 0.037852, loss: 1.4675
2022-08-03 00:58:57 - train: epoch 0073, iter [04000, 05004], lr: 0.037801, loss: 1.5771
2022-08-03 00:59:57 - train: epoch 0073, iter [04100, 05004], lr: 0.037749, loss: 1.5243
2022-08-03 01:00:54 - train: epoch 0073, iter [04200, 05004], lr: 0.037697, loss: 1.5203
2022-08-03 01:01:52 - train: epoch 0073, iter [04300, 05004], lr: 0.037645, loss: 1.4482
2022-08-03 01:02:50 - train: epoch 0073, iter [04400, 05004], lr: 0.037594, loss: 1.8159
2022-08-03 01:03:48 - train: epoch 0073, iter [04500, 05004], lr: 0.037542, loss: 1.4842
2022-08-03 01:04:46 - train: epoch 0073, iter [04600, 05004], lr: 0.037491, loss: 1.6233
2022-08-03 01:05:45 - train: epoch 0073, iter [04700, 05004], lr: 0.037439, loss: 1.3756
2022-08-03 01:06:43 - train: epoch 0073, iter [04800, 05004], lr: 0.037387, loss: 1.4268
2022-08-03 01:07:40 - train: epoch 0073, iter [04900, 05004], lr: 0.037336, loss: 1.5568
2022-08-03 01:08:36 - train: epoch 0073, iter [05000, 05004], lr: 0.037284, loss: 1.6307
2022-08-03 01:08:38 - train: epoch 073, train_loss: 1.5168
2022-08-03 01:10:48 - eval: epoch: 073, acc1: 67.478%, acc5: 88.248%, test_loss: 1.3290, per_image_load_time: 2.840ms, per_image_inference_time: 0.626ms
2022-08-03 01:10:48 - until epoch: 073, best_acc1: 67.478%
2022-08-03 01:10:48 - epoch 074 lr: 0.037282
2022-08-03 01:11:56 - train: epoch 0074, iter [00100, 05004], lr: 0.037231, loss: 1.4346
2022-08-03 01:12:54 - train: epoch 0074, iter [00200, 05004], lr: 0.037179, loss: 1.3947
2022-08-03 01:13:52 - train: epoch 0074, iter [00300, 05004], lr: 0.037128, loss: 1.4881
2022-08-03 01:14:50 - train: epoch 0074, iter [00400, 05004], lr: 0.037077, loss: 1.7935
2022-08-03 01:15:47 - train: epoch 0074, iter [00500, 05004], lr: 0.037025, loss: 1.6437
2022-08-03 01:16:45 - train: epoch 0074, iter [00600, 05004], lr: 0.036974, loss: 1.4810
2022-08-03 01:17:41 - train: epoch 0074, iter [00700, 05004], lr: 0.036923, loss: 1.4145
2022-08-03 01:18:40 - train: epoch 0074, iter [00800, 05004], lr: 0.036871, loss: 1.5299
2022-08-03 01:19:38 - train: epoch 0074, iter [00900, 05004], lr: 0.036820, loss: 1.4414
2022-08-03 01:20:37 - train: epoch 0074, iter [01000, 05004], lr: 0.036769, loss: 1.6291
2022-08-03 01:21:33 - train: epoch 0074, iter [01100, 05004], lr: 0.036718, loss: 1.4811
2022-08-03 01:22:33 - train: epoch 0074, iter [01200, 05004], lr: 0.036667, loss: 1.5124
2022-08-03 01:23:28 - train: epoch 0074, iter [01300, 05004], lr: 0.036616, loss: 1.5027
2022-08-03 01:24:27 - train: epoch 0074, iter [01400, 05004], lr: 0.036564, loss: 1.4749
2022-08-03 01:25:25 - train: epoch 0074, iter [01500, 05004], lr: 0.036513, loss: 1.4630
2022-08-03 01:26:24 - train: epoch 0074, iter [01600, 05004], lr: 0.036462, loss: 1.4962
2022-08-03 01:27:22 - train: epoch 0074, iter [01700, 05004], lr: 0.036411, loss: 1.6201
2022-08-03 01:28:19 - train: epoch 0074, iter [01800, 05004], lr: 0.036360, loss: 1.5979
2022-08-03 01:29:19 - train: epoch 0074, iter [01900, 05004], lr: 0.036309, loss: 1.5466
2022-08-03 01:30:15 - train: epoch 0074, iter [02000, 05004], lr: 0.036258, loss: 1.3017
2022-08-03 01:31:14 - train: epoch 0074, iter [02100, 05004], lr: 0.036208, loss: 1.4928
2022-08-03 01:32:12 - train: epoch 0074, iter [02200, 05004], lr: 0.036157, loss: 1.8033
2022-08-03 01:33:11 - train: epoch 0074, iter [02300, 05004], lr: 0.036106, loss: 1.5616
2022-08-03 01:34:10 - train: epoch 0074, iter [02400, 05004], lr: 0.036055, loss: 1.4592
2022-08-03 01:35:08 - train: epoch 0074, iter [02500, 05004], lr: 0.036004, loss: 1.4526
2022-08-03 01:36:05 - train: epoch 0074, iter [02600, 05004], lr: 0.035953, loss: 1.2949
2022-08-03 01:37:03 - train: epoch 0074, iter [02700, 05004], lr: 0.035903, loss: 1.4600
2022-08-03 01:38:02 - train: epoch 0074, iter [02800, 05004], lr: 0.035852, loss: 1.6393
2022-08-03 01:39:02 - train: epoch 0074, iter [02900, 05004], lr: 0.035801, loss: 1.4583
2022-08-03 01:39:58 - train: epoch 0074, iter [03000, 05004], lr: 0.035751, loss: 1.7654
2022-08-03 01:40:56 - train: epoch 0074, iter [03100, 05004], lr: 0.035700, loss: 1.4877
2022-08-03 01:41:56 - train: epoch 0074, iter [03200, 05004], lr: 0.035649, loss: 1.3585
2022-08-03 01:42:53 - train: epoch 0074, iter [03300, 05004], lr: 0.035599, loss: 1.5395
2022-08-03 01:43:52 - train: epoch 0074, iter [03400, 05004], lr: 0.035548, loss: 1.5329
2022-08-03 01:44:50 - train: epoch 0074, iter [03500, 05004], lr: 0.035498, loss: 1.3232
2022-08-03 01:45:49 - train: epoch 0074, iter [03600, 05004], lr: 0.035447, loss: 1.5321
2022-08-03 01:46:47 - train: epoch 0074, iter [03700, 05004], lr: 0.035397, loss: 1.7569
2022-08-03 01:47:45 - train: epoch 0074, iter [03800, 05004], lr: 0.035346, loss: 1.4018
2022-08-03 01:48:42 - train: epoch 0074, iter [03900, 05004], lr: 0.035296, loss: 1.3167
2022-08-03 01:49:41 - train: epoch 0074, iter [04000, 05004], lr: 0.035246, loss: 1.5660
2022-08-03 01:50:40 - train: epoch 0074, iter [04100, 05004], lr: 0.035195, loss: 1.5405
2022-08-03 01:51:40 - train: epoch 0074, iter [04200, 05004], lr: 0.035145, loss: 1.5444
2022-08-03 01:52:37 - train: epoch 0074, iter [04300, 05004], lr: 0.035095, loss: 1.4179
2022-08-03 01:53:36 - train: epoch 0074, iter [04400, 05004], lr: 0.035044, loss: 1.3112
2022-08-03 01:54:34 - train: epoch 0074, iter [04500, 05004], lr: 0.034994, loss: 1.4647
2022-08-03 01:55:31 - train: epoch 0074, iter [04600, 05004], lr: 0.034944, loss: 1.5499
2022-08-03 01:56:30 - train: epoch 0074, iter [04700, 05004], lr: 0.034894, loss: 1.3623
2022-08-03 01:57:29 - train: epoch 0074, iter [04800, 05004], lr: 0.034844, loss: 1.4103
2022-08-03 01:58:28 - train: epoch 0074, iter [04900, 05004], lr: 0.034794, loss: 1.8023
2022-08-03 01:59:24 - train: epoch 0074, iter [05000, 05004], lr: 0.034743, loss: 1.5143
2022-08-03 01:59:25 - train: epoch 074, train_loss: 1.4977
2022-08-03 02:01:32 - eval: epoch: 074, acc1: 67.696%, acc5: 88.430%, test_loss: 1.3030, per_image_load_time: 2.018ms, per_image_inference_time: 0.609ms
2022-08-03 02:01:32 - until epoch: 074, best_acc1: 67.696%
2022-08-03 02:01:32 - epoch 075 lr: 0.034741
2022-08-03 02:02:38 - train: epoch 0075, iter [00100, 05004], lr: 0.034691, loss: 1.6882
2022-08-03 02:03:36 - train: epoch 0075, iter [00200, 05004], lr: 0.034641, loss: 1.4869
2022-08-03 02:04:33 - train: epoch 0075, iter [00300, 05004], lr: 0.034591, loss: 1.3561
2022-08-03 02:05:30 - train: epoch 0075, iter [00400, 05004], lr: 0.034541, loss: 1.2860
2022-08-03 02:06:30 - train: epoch 0075, iter [00500, 05004], lr: 0.034491, loss: 1.6603
2022-08-03 02:07:25 - train: epoch 0075, iter [00600, 05004], lr: 0.034441, loss: 1.5433
2022-08-03 02:08:22 - train: epoch 0075, iter [00700, 05004], lr: 0.034392, loss: 1.7493
2022-08-03 02:09:20 - train: epoch 0075, iter [00800, 05004], lr: 0.034342, loss: 1.5007
2022-08-03 02:10:19 - train: epoch 0075, iter [00900, 05004], lr: 0.034292, loss: 1.5470
2022-08-03 02:11:18 - train: epoch 0075, iter [01000, 05004], lr: 0.034242, loss: 1.5780
2022-08-03 02:12:17 - train: epoch 0075, iter [01100, 05004], lr: 0.034192, loss: 1.5929
2022-08-03 02:13:11 - train: epoch 0075, iter [01200, 05004], lr: 0.034143, loss: 1.4026
2022-08-03 02:14:11 - train: epoch 0075, iter [01300, 05004], lr: 0.034093, loss: 1.4543
2022-08-03 02:15:07 - train: epoch 0075, iter [01400, 05004], lr: 0.034043, loss: 1.4149
2022-08-03 02:16:08 - train: epoch 0075, iter [01500, 05004], lr: 0.033994, loss: 1.5787
2022-08-03 02:17:05 - train: epoch 0075, iter [01600, 05004], lr: 0.033944, loss: 1.2677
2022-08-03 02:18:03 - train: epoch 0075, iter [01700, 05004], lr: 0.033894, loss: 1.6825
2022-08-03 02:19:02 - train: epoch 0075, iter [01800, 05004], lr: 0.033845, loss: 1.6219
2022-08-03 02:20:00 - train: epoch 0075, iter [01900, 05004], lr: 0.033795, loss: 1.4093
2022-08-03 02:20:58 - train: epoch 0075, iter [02000, 05004], lr: 0.033746, loss: 1.4701
2022-08-03 02:21:56 - train: epoch 0075, iter [02100, 05004], lr: 0.033696, loss: 1.3690
2022-08-03 02:22:54 - train: epoch 0075, iter [02200, 05004], lr: 0.033647, loss: 1.5320
2022-08-03 02:23:53 - train: epoch 0075, iter [02300, 05004], lr: 0.033597, loss: 1.3758
2022-08-03 02:24:53 - train: epoch 0075, iter [02400, 05004], lr: 0.033548, loss: 1.3590
2022-08-03 02:25:50 - train: epoch 0075, iter [02500, 05004], lr: 0.033499, loss: 1.3034
2022-08-03 02:26:47 - train: epoch 0075, iter [02600, 05004], lr: 0.033449, loss: 1.5001
2022-08-03 02:27:48 - train: epoch 0075, iter [02700, 05004], lr: 0.033400, loss: 1.3290
2022-08-03 02:28:45 - train: epoch 0075, iter [02800, 05004], lr: 0.033351, loss: 1.4711
2022-08-03 02:29:42 - train: epoch 0075, iter [02900, 05004], lr: 0.033301, loss: 1.6925
2022-08-03 02:30:41 - train: epoch 0075, iter [03000, 05004], lr: 0.033252, loss: 1.6047
2022-08-03 02:31:38 - train: epoch 0075, iter [03100, 05004], lr: 0.033203, loss: 1.5610
2022-08-03 02:32:37 - train: epoch 0075, iter [03200, 05004], lr: 0.033154, loss: 1.5639
2022-08-03 02:33:36 - train: epoch 0075, iter [03300, 05004], lr: 0.033105, loss: 1.3693
2022-08-03 02:34:34 - train: epoch 0075, iter [03400, 05004], lr: 0.033056, loss: 1.3300
2022-08-03 02:35:31 - train: epoch 0075, iter [03500, 05004], lr: 0.033006, loss: 1.5584
2022-08-03 02:36:32 - train: epoch 0075, iter [03600, 05004], lr: 0.032957, loss: 1.7401
2022-08-03 02:37:28 - train: epoch 0075, iter [03700, 05004], lr: 0.032908, loss: 1.4782
2022-08-03 02:38:27 - train: epoch 0075, iter [03800, 05004], lr: 0.032859, loss: 1.4407
2022-08-03 02:39:24 - train: epoch 0075, iter [03900, 05004], lr: 0.032810, loss: 1.4526
2022-08-03 02:40:22 - train: epoch 0075, iter [04000, 05004], lr: 0.032761, loss: 1.3157
2022-08-03 02:41:20 - train: epoch 0075, iter [04100, 05004], lr: 0.032713, loss: 1.2274
2022-08-03 02:42:19 - train: epoch 0075, iter [04200, 05004], lr: 0.032664, loss: 1.4163
2022-08-03 02:43:16 - train: epoch 0075, iter [04300, 05004], lr: 0.032615, loss: 1.7317
2022-08-03 02:44:16 - train: epoch 0075, iter [04400, 05004], lr: 0.032566, loss: 1.4466
2022-08-03 02:45:13 - train: epoch 0075, iter [04500, 05004], lr: 0.032517, loss: 1.4844
2022-08-03 02:46:10 - train: epoch 0075, iter [04600, 05004], lr: 0.032469, loss: 1.4233
2022-08-03 02:47:09 - train: epoch 0075, iter [04700, 05004], lr: 0.032420, loss: 1.6641
2022-08-03 02:48:07 - train: epoch 0075, iter [04800, 05004], lr: 0.032371, loss: 1.5879
2022-08-03 02:49:04 - train: epoch 0075, iter [04900, 05004], lr: 0.032322, loss: 1.3429
2022-08-03 02:49:59 - train: epoch 0075, iter [05000, 05004], lr: 0.032274, loss: 1.4297
2022-08-03 02:50:00 - train: epoch 075, train_loss: 1.4743
2022-08-03 02:52:11 - eval: epoch: 075, acc1: 67.982%, acc5: 88.440%, test_loss: 1.3071, per_image_load_time: 3.566ms, per_image_inference_time: 0.638ms
2022-08-03 02:52:11 - until epoch: 075, best_acc1: 67.982%
2022-08-03 02:52:11 - epoch 076 lr: 0.032271
2022-08-03 02:53:19 - train: epoch 0076, iter [00100, 05004], lr: 0.032223, loss: 1.4254
2022-08-03 02:54:16 - train: epoch 0076, iter [00200, 05004], lr: 0.032175, loss: 1.4421
2022-08-03 02:55:16 - train: epoch 0076, iter [00300, 05004], lr: 0.032126, loss: 1.6892
2022-08-03 02:56:14 - train: epoch 0076, iter [00400, 05004], lr: 0.032078, loss: 1.4718
2022-08-03 02:57:12 - train: epoch 0076, iter [00500, 05004], lr: 0.032029, loss: 1.4083
2022-08-03 02:58:09 - train: epoch 0076, iter [00600, 05004], lr: 0.031981, loss: 1.4416
2022-08-03 02:59:07 - train: epoch 0076, iter [00700, 05004], lr: 0.031932, loss: 1.4062
2022-08-03 03:00:06 - train: epoch 0076, iter [00800, 05004], lr: 0.031884, loss: 1.4664
2022-08-03 03:01:04 - train: epoch 0076, iter [00900, 05004], lr: 0.031835, loss: 1.4481
2022-08-03 03:02:03 - train: epoch 0076, iter [01000, 05004], lr: 0.031787, loss: 1.3838
2022-08-03 03:03:01 - train: epoch 0076, iter [01100, 05004], lr: 0.031739, loss: 1.3480
2022-08-03 03:03:58 - train: epoch 0076, iter [01200, 05004], lr: 0.031691, loss: 1.4259
2022-08-03 03:04:57 - train: epoch 0076, iter [01300, 05004], lr: 0.031642, loss: 1.5737
2022-08-03 03:05:56 - train: epoch 0076, iter [01400, 05004], lr: 0.031594, loss: 1.4353
2022-08-03 03:06:52 - train: epoch 0076, iter [01500, 05004], lr: 0.031546, loss: 1.4770
2022-08-03 03:07:51 - train: epoch 0076, iter [01600, 05004], lr: 0.031498, loss: 1.4546
2022-08-03 03:08:49 - train: epoch 0076, iter [01700, 05004], lr: 0.031450, loss: 1.5720
2022-08-03 03:09:48 - train: epoch 0076, iter [01800, 05004], lr: 0.031401, loss: 1.2301
2022-08-03 03:10:45 - train: epoch 0076, iter [01900, 05004], lr: 0.031353, loss: 1.5487
2022-08-03 03:11:43 - train: epoch 0076, iter [02000, 05004], lr: 0.031305, loss: 1.3525
2022-08-03 03:12:42 - train: epoch 0076, iter [02100, 05004], lr: 0.031257, loss: 1.4102
2022-08-03 03:13:40 - train: epoch 0076, iter [02200, 05004], lr: 0.031209, loss: 1.3511
2022-08-03 03:14:38 - train: epoch 0076, iter [02300, 05004], lr: 0.031161, loss: 1.3323
2022-08-03 03:15:37 - train: epoch 0076, iter [02400, 05004], lr: 0.031114, loss: 1.6541
2022-08-03 03:16:34 - train: epoch 0076, iter [02500, 05004], lr: 0.031066, loss: 1.5994
2022-08-03 03:17:32 - train: epoch 0076, iter [02600, 05004], lr: 0.031018, loss: 1.6921
2022-08-03 03:18:30 - train: epoch 0076, iter [02700, 05004], lr: 0.030970, loss: 1.5599
2022-08-03 03:19:29 - train: epoch 0076, iter [02800, 05004], lr: 0.030922, loss: 1.4629
2022-08-03 03:20:27 - train: epoch 0076, iter [02900, 05004], lr: 0.030874, loss: 1.4773
2022-08-03 03:21:25 - train: epoch 0076, iter [03000, 05004], lr: 0.030827, loss: 1.1584
2022-08-03 03:22:22 - train: epoch 0076, iter [03100, 05004], lr: 0.030779, loss: 1.5017
2022-08-03 03:23:20 - train: epoch 0076, iter [03200, 05004], lr: 0.030731, loss: 1.3831
2022-08-03 03:24:19 - train: epoch 0076, iter [03300, 05004], lr: 0.030684, loss: 1.3705
2022-08-03 03:25:18 - train: epoch 0076, iter [03400, 05004], lr: 0.030636, loss: 1.2579
2022-08-03 03:26:15 - train: epoch 0076, iter [03500, 05004], lr: 0.030588, loss: 1.3730
2022-08-03 03:27:14 - train: epoch 0076, iter [03600, 05004], lr: 0.030541, loss: 1.3977
2022-08-03 03:28:10 - train: epoch 0076, iter [03700, 05004], lr: 0.030493, loss: 1.4062
2022-08-03 03:29:10 - train: epoch 0076, iter [03800, 05004], lr: 0.030446, loss: 1.1599
2022-08-03 03:30:07 - train: epoch 0076, iter [03900, 05004], lr: 0.030398, loss: 1.4473
2022-08-03 03:31:05 - train: epoch 0076, iter [04000, 05004], lr: 0.030351, loss: 1.3507
2022-08-03 03:32:04 - train: epoch 0076, iter [04100, 05004], lr: 0.030303, loss: 1.4543
2022-08-03 03:33:02 - train: epoch 0076, iter [04200, 05004], lr: 0.030256, loss: 1.4479
2022-08-03 03:33:59 - train: epoch 0076, iter [04300, 05004], lr: 0.030209, loss: 1.6604
2022-08-03 03:34:59 - train: epoch 0076, iter [04400, 05004], lr: 0.030161, loss: 1.3764
2022-08-03 03:35:56 - train: epoch 0076, iter [04500, 05004], lr: 0.030114, loss: 1.3980
2022-08-03 03:36:55 - train: epoch 0076, iter [04600, 05004], lr: 0.030067, loss: 1.5228
2022-08-03 03:37:53 - train: epoch 0076, iter [04700, 05004], lr: 0.030020, loss: 1.4726
2022-08-03 03:38:51 - train: epoch 0076, iter [04800, 05004], lr: 0.029972, loss: 1.3823
2022-08-03 03:39:50 - train: epoch 0076, iter [04900, 05004], lr: 0.029925, loss: 1.4712
2022-08-03 03:40:44 - train: epoch 0076, iter [05000, 05004], lr: 0.029878, loss: 1.6211
2022-08-03 03:40:46 - train: epoch 076, train_loss: 1.4523
2022-08-03 03:42:56 - eval: epoch: 076, acc1: 68.002%, acc5: 88.618%, test_loss: 1.2897, per_image_load_time: 4.370ms, per_image_inference_time: 0.625ms
2022-08-03 03:42:56 - until epoch: 076, best_acc1: 68.002%
2022-08-03 03:42:56 - epoch 077 lr: 0.029876
2022-08-03 03:44:04 - train: epoch 0077, iter [00100, 05004], lr: 0.029829, loss: 1.5068
2022-08-03 03:45:02 - train: epoch 0077, iter [00200, 05004], lr: 0.029782, loss: 1.5564
2022-08-03 03:46:00 - train: epoch 0077, iter [00300, 05004], lr: 0.029735, loss: 1.2190
2022-08-03 03:46:58 - train: epoch 0077, iter [00400, 05004], lr: 0.029688, loss: 1.3952
2022-08-03 03:47:56 - train: epoch 0077, iter [00500, 05004], lr: 0.029641, loss: 1.3276
2022-08-03 03:48:54 - train: epoch 0077, iter [00600, 05004], lr: 0.029594, loss: 1.2505
2022-08-03 03:49:52 - train: epoch 0077, iter [00700, 05004], lr: 0.029547, loss: 1.4735
2022-08-03 03:50:48 - train: epoch 0077, iter [00800, 05004], lr: 0.029500, loss: 1.6447
2022-08-03 03:51:47 - train: epoch 0077, iter [00900, 05004], lr: 0.029454, loss: 1.5619
2022-08-03 03:52:44 - train: epoch 0077, iter [01000, 05004], lr: 0.029407, loss: 1.2069
2022-08-03 03:53:43 - train: epoch 0077, iter [01100, 05004], lr: 0.029360, loss: 1.3943
2022-08-03 03:54:40 - train: epoch 0077, iter [01200, 05004], lr: 0.029313, loss: 1.4676
2022-08-03 03:55:39 - train: epoch 0077, iter [01300, 05004], lr: 0.029266, loss: 1.1856
2022-08-03 03:56:37 - train: epoch 0077, iter [01400, 05004], lr: 0.029220, loss: 1.4008
2022-08-03 03:57:35 - train: epoch 0077, iter [01500, 05004], lr: 0.029173, loss: 1.2917
2022-08-03 03:58:33 - train: epoch 0077, iter [01600, 05004], lr: 0.029126, loss: 1.2229
2022-08-03 03:59:31 - train: epoch 0077, iter [01700, 05004], lr: 0.029080, loss: 1.4692
2022-08-03 04:00:29 - train: epoch 0077, iter [01800, 05004], lr: 0.029033, loss: 1.4340
2022-08-03 04:01:28 - train: epoch 0077, iter [01900, 05004], lr: 0.028987, loss: 1.4961
2022-08-03 04:02:25 - train: epoch 0077, iter [02000, 05004], lr: 0.028940, loss: 1.5425
2022-08-03 04:03:25 - train: epoch 0077, iter [02100, 05004], lr: 0.028894, loss: 1.4839
2022-08-03 04:04:22 - train: epoch 0077, iter [02200, 05004], lr: 0.028847, loss: 1.4443
2022-08-03 04:05:20 - train: epoch 0077, iter [02300, 05004], lr: 0.028801, loss: 1.4870
2022-08-03 04:06:18 - train: epoch 0077, iter [02400, 05004], lr: 0.028754, loss: 1.2923
2022-08-03 04:07:17 - train: epoch 0077, iter [02500, 05004], lr: 0.028708, loss: 1.3586
2022-08-03 04:08:15 - train: epoch 0077, iter [02600, 05004], lr: 0.028662, loss: 1.2438
2022-08-03 04:09:14 - train: epoch 0077, iter [02700, 05004], lr: 0.028615, loss: 1.3290
2022-08-03 04:10:12 - train: epoch 0077, iter [02800, 05004], lr: 0.028569, loss: 1.5425
2022-08-03 04:11:09 - train: epoch 0077, iter [02900, 05004], lr: 0.028523, loss: 1.5992
2022-08-03 04:12:07 - train: epoch 0077, iter [03000, 05004], lr: 0.028477, loss: 1.3468
2022-08-03 04:13:05 - train: epoch 0077, iter [03100, 05004], lr: 0.028431, loss: 1.4240
2022-08-03 04:14:02 - train: epoch 0077, iter [03200, 05004], lr: 0.028384, loss: 1.6660
2022-08-03 04:15:01 - train: epoch 0077, iter [03300, 05004], lr: 0.028338, loss: 1.3839
2022-08-03 04:15:59 - train: epoch 0077, iter [03400, 05004], lr: 0.028292, loss: 1.4807
2022-08-03 04:16:55 - train: epoch 0077, iter [03500, 05004], lr: 0.028246, loss: 1.4523
2022-08-03 04:17:53 - train: epoch 0077, iter [03600, 05004], lr: 0.028200, loss: 1.2584
2022-08-03 04:18:51 - train: epoch 0077, iter [03700, 05004], lr: 0.028154, loss: 1.3090
2022-08-03 04:19:51 - train: epoch 0077, iter [03800, 05004], lr: 0.028108, loss: 1.2748
2022-08-03 04:20:48 - train: epoch 0077, iter [03900, 05004], lr: 0.028062, loss: 1.2912
2022-08-03 04:21:45 - train: epoch 0077, iter [04000, 05004], lr: 0.028016, loss: 1.4648
2022-08-03 04:22:44 - train: epoch 0077, iter [04100, 05004], lr: 0.027971, loss: 1.4389
2022-08-03 04:23:42 - train: epoch 0077, iter [04200, 05004], lr: 0.027925, loss: 1.5147
2022-08-03 04:24:41 - train: epoch 0077, iter [04300, 05004], lr: 0.027879, loss: 1.6030
2022-08-03 04:25:39 - train: epoch 0077, iter [04400, 05004], lr: 0.027833, loss: 1.4025
2022-08-03 04:26:37 - train: epoch 0077, iter [04500, 05004], lr: 0.027787, loss: 1.6284
2022-08-03 04:27:34 - train: epoch 0077, iter [04600, 05004], lr: 0.027742, loss: 1.3199
2022-08-03 04:28:33 - train: epoch 0077, iter [04700, 05004], lr: 0.027696, loss: 1.4105
2022-08-03 04:29:31 - train: epoch 0077, iter [04800, 05004], lr: 0.027650, loss: 1.3414
2022-08-03 04:30:30 - train: epoch 0077, iter [04900, 05004], lr: 0.027605, loss: 1.6027
2022-08-03 04:31:24 - train: epoch 0077, iter [05000, 05004], lr: 0.027559, loss: 1.4043
2022-08-03 04:31:26 - train: epoch 077, train_loss: 1.4306
2022-08-03 04:33:32 - eval: epoch: 077, acc1: 68.506%, acc5: 89.016%, test_loss: 1.2719, per_image_load_time: 4.249ms, per_image_inference_time: 0.623ms
2022-08-03 04:33:32 - until epoch: 077, best_acc1: 68.506%
2022-08-03 04:33:32 - epoch 078 lr: 0.027557
2022-08-03 04:34:38 - train: epoch 0078, iter [00100, 05004], lr: 0.027512, loss: 1.3115
2022-08-03 04:35:35 - train: epoch 0078, iter [00200, 05004], lr: 0.027466, loss: 1.5143
2022-08-03 04:36:34 - train: epoch 0078, iter [00300, 05004], lr: 0.027421, loss: 1.3490
2022-08-03 04:37:32 - train: epoch 0078, iter [00400, 05004], lr: 0.027376, loss: 1.3204
2022-08-03 04:38:31 - train: epoch 0078, iter [00500, 05004], lr: 0.027330, loss: 1.2597
2022-08-03 04:39:29 - train: epoch 0078, iter [00600, 05004], lr: 0.027285, loss: 1.1269
2022-08-03 04:40:26 - train: epoch 0078, iter [00700, 05004], lr: 0.027239, loss: 1.3789
2022-08-03 04:41:24 - train: epoch 0078, iter [00800, 05004], lr: 0.027194, loss: 1.4438
2022-08-03 04:42:22 - train: epoch 0078, iter [00900, 05004], lr: 0.027149, loss: 1.3606
2022-08-03 04:43:20 - train: epoch 0078, iter [01000, 05004], lr: 0.027103, loss: 1.2733
2022-08-03 04:44:19 - train: epoch 0078, iter [01100, 05004], lr: 0.027058, loss: 1.1177
2022-08-03 04:45:16 - train: epoch 0078, iter [01200, 05004], lr: 0.027013, loss: 1.4487
2022-08-03 04:46:14 - train: epoch 0078, iter [01300, 05004], lr: 0.026968, loss: 1.2501
2022-08-03 04:47:12 - train: epoch 0078, iter [01400, 05004], lr: 0.026923, loss: 1.1992
2022-08-03 04:48:11 - train: epoch 0078, iter [01500, 05004], lr: 0.026878, loss: 1.5770
2022-08-03 04:49:08 - train: epoch 0078, iter [01600, 05004], lr: 0.026833, loss: 1.3228
2022-08-03 04:50:06 - train: epoch 0078, iter [01700, 05004], lr: 0.026788, loss: 1.5107
2022-08-03 04:51:04 - train: epoch 0078, iter [01800, 05004], lr: 0.026743, loss: 1.4150
2022-08-03 04:52:03 - train: epoch 0078, iter [01900, 05004], lr: 0.026698, loss: 1.3965
2022-08-03 04:53:00 - train: epoch 0078, iter [02000, 05004], lr: 0.026653, loss: 1.2885
2022-08-03 04:53:57 - train: epoch 0078, iter [02100, 05004], lr: 0.026608, loss: 1.5578
2022-08-03 04:54:55 - train: epoch 0078, iter [02200, 05004], lr: 0.026563, loss: 1.4043
2022-08-03 04:55:54 - train: epoch 0078, iter [02300, 05004], lr: 0.026518, loss: 1.3474
2022-08-03 04:56:50 - train: epoch 0078, iter [02400, 05004], lr: 0.026473, loss: 1.2318
2022-08-03 04:57:48 - train: epoch 0078, iter [02500, 05004], lr: 0.026429, loss: 1.3901
2022-08-03 04:58:47 - train: epoch 0078, iter [02600, 05004], lr: 0.026384, loss: 1.3438
2022-08-03 04:59:45 - train: epoch 0078, iter [02700, 05004], lr: 0.026339, loss: 1.5080
2022-08-03 05:00:43 - train: epoch 0078, iter [02800, 05004], lr: 0.026294, loss: 1.3456
2022-08-03 05:01:39 - train: epoch 0078, iter [02900, 05004], lr: 0.026250, loss: 1.4646
2022-08-03 05:02:38 - train: epoch 0078, iter [03000, 05004], lr: 0.026205, loss: 1.6094
2022-08-03 05:03:37 - train: epoch 0078, iter [03100, 05004], lr: 0.026161, loss: 1.4944
2022-08-03 05:04:35 - train: epoch 0078, iter [03200, 05004], lr: 0.026116, loss: 1.4722
2022-08-03 05:05:33 - train: epoch 0078, iter [03300, 05004], lr: 0.026071, loss: 1.5860
2022-08-03 05:06:31 - train: epoch 0078, iter [03400, 05004], lr: 0.026027, loss: 1.3011
2022-08-03 05:07:29 - train: epoch 0078, iter [03500, 05004], lr: 0.025983, loss: 1.5821
2022-08-03 05:08:26 - train: epoch 0078, iter [03600, 05004], lr: 0.025938, loss: 1.4869
2022-08-03 05:09:23 - train: epoch 0078, iter [03700, 05004], lr: 0.025894, loss: 1.4753
2022-08-03 05:10:22 - train: epoch 0078, iter [03800, 05004], lr: 0.025849, loss: 1.5004
2022-08-03 05:11:18 - train: epoch 0078, iter [03900, 05004], lr: 0.025805, loss: 1.3412
2022-08-03 05:12:17 - train: epoch 0078, iter [04000, 05004], lr: 0.025761, loss: 1.3805
2022-08-03 05:13:15 - train: epoch 0078, iter [04100, 05004], lr: 0.025716, loss: 1.4446
2022-08-03 05:14:13 - train: epoch 0078, iter [04200, 05004], lr: 0.025672, loss: 1.6283
2022-08-03 05:15:09 - train: epoch 0078, iter [04300, 05004], lr: 0.025628, loss: 1.5160
2022-08-03 05:16:09 - train: epoch 0078, iter [04400, 05004], lr: 0.025584, loss: 1.3711
2022-08-03 05:17:07 - train: epoch 0078, iter [04500, 05004], lr: 0.025540, loss: 1.4400
2022-08-03 05:18:04 - train: epoch 0078, iter [04600, 05004], lr: 0.025496, loss: 1.2684
2022-08-03 05:19:03 - train: epoch 0078, iter [04700, 05004], lr: 0.025452, loss: 1.2752
2022-08-03 05:20:00 - train: epoch 0078, iter [04800, 05004], lr: 0.025408, loss: 1.5740
2022-08-03 05:20:58 - train: epoch 0078, iter [04900, 05004], lr: 0.025364, loss: 1.4150
2022-08-03 05:21:53 - train: epoch 0078, iter [05000, 05004], lr: 0.025320, loss: 1.4273
2022-08-03 05:21:54 - train: epoch 078, train_loss: 1.4088
2022-08-03 05:24:05 - eval: epoch: 078, acc1: 68.826%, acc5: 89.120%, test_loss: 1.2539, per_image_load_time: 2.118ms, per_image_inference_time: 0.604ms
2022-08-03 05:24:05 - until epoch: 078, best_acc1: 68.826%
2022-08-03 05:24:05 - epoch 079 lr: 0.025317
2022-08-03 05:25:12 - train: epoch 0079, iter [00100, 05004], lr: 0.025274, loss: 1.3077
2022-08-03 05:26:10 - train: epoch 0079, iter [00200, 05004], lr: 0.025230, loss: 1.4062
2022-08-03 05:27:08 - train: epoch 0079, iter [00300, 05004], lr: 0.025186, loss: 1.5536
2022-08-03 05:28:04 - train: epoch 0079, iter [00400, 05004], lr: 0.025142, loss: 1.2836
2022-08-03 05:29:05 - train: epoch 0079, iter [00500, 05004], lr: 0.025099, loss: 1.2714
2022-08-03 05:30:02 - train: epoch 0079, iter [00600, 05004], lr: 0.025055, loss: 1.4111
2022-08-03 05:30:59 - train: epoch 0079, iter [00700, 05004], lr: 0.025011, loss: 1.4859
2022-08-03 05:31:58 - train: epoch 0079, iter [00800, 05004], lr: 0.024967, loss: 1.5124
2022-08-03 05:32:56 - train: epoch 0079, iter [00900, 05004], lr: 0.024924, loss: 1.3382
2022-08-03 05:33:54 - train: epoch 0079, iter [01000, 05004], lr: 0.024880, loss: 1.4017
2022-08-03 05:34:53 - train: epoch 0079, iter [01100, 05004], lr: 0.024836, loss: 1.4526
2022-08-03 05:35:51 - train: epoch 0079, iter [01200, 05004], lr: 0.024793, loss: 1.4853
2022-08-03 05:36:48 - train: epoch 0079, iter [01300, 05004], lr: 0.024749, loss: 1.2765
2022-08-03 05:37:47 - train: epoch 0079, iter [01400, 05004], lr: 0.024706, loss: 1.4616
2022-08-03 05:38:44 - train: epoch 0079, iter [01500, 05004], lr: 0.024662, loss: 1.4429
2022-08-03 05:39:42 - train: epoch 0079, iter [01600, 05004], lr: 0.024619, loss: 1.2320
2022-08-03 05:40:39 - train: epoch 0079, iter [01700, 05004], lr: 0.024575, loss: 1.2753
2022-08-03 05:41:38 - train: epoch 0079, iter [01800, 05004], lr: 0.024532, loss: 1.4261
2022-08-03 05:42:36 - train: epoch 0079, iter [01900, 05004], lr: 0.024489, loss: 1.3600
2022-08-03 05:43:34 - train: epoch 0079, iter [02000, 05004], lr: 0.024445, loss: 1.5974
2022-08-03 05:44:31 - train: epoch 0079, iter [02100, 05004], lr: 0.024402, loss: 1.4525
2022-08-03 05:45:30 - train: epoch 0079, iter [02200, 05004], lr: 0.024359, loss: 1.4429
2022-08-03 05:46:29 - train: epoch 0079, iter [02300, 05004], lr: 0.024316, loss: 1.3361
2022-08-03 05:47:26 - train: epoch 0079, iter [02400, 05004], lr: 0.024273, loss: 1.3201
2022-08-03 05:48:23 - train: epoch 0079, iter [02500, 05004], lr: 0.024229, loss: 1.5539
2022-08-03 05:49:22 - train: epoch 0079, iter [02600, 05004], lr: 0.024186, loss: 1.5036
2022-08-03 05:50:21 - train: epoch 0079, iter [02700, 05004], lr: 0.024143, loss: 1.2948
2022-08-03 05:51:16 - train: epoch 0079, iter [02800, 05004], lr: 0.024100, loss: 1.3051
2022-08-03 05:52:16 - train: epoch 0079, iter [02900, 05004], lr: 0.024057, loss: 1.3899
2022-08-03 05:53:14 - train: epoch 0079, iter [03000, 05004], lr: 0.024014, loss: 1.4323
2022-08-03 05:54:12 - train: epoch 0079, iter [03100, 05004], lr: 0.023971, loss: 1.4255
2022-08-03 05:55:08 - train: epoch 0079, iter [03200, 05004], lr: 0.023928, loss: 1.4886
2022-08-03 05:56:08 - train: epoch 0079, iter [03300, 05004], lr: 0.023885, loss: 1.3924
2022-08-03 05:57:07 - train: epoch 0079, iter [03400, 05004], lr: 0.023843, loss: 1.3554
2022-08-03 05:58:05 - train: epoch 0079, iter [03500, 05004], lr: 0.023800, loss: 1.5296
2022-08-03 05:59:03 - train: epoch 0079, iter [03600, 05004], lr: 0.023757, loss: 1.2399
2022-08-03 06:00:00 - train: epoch 0079, iter [03700, 05004], lr: 0.023714, loss: 1.3968
2022-08-03 06:01:00 - train: epoch 0079, iter [03800, 05004], lr: 0.023672, loss: 1.3659
2022-08-03 06:01:58 - train: epoch 0079, iter [03900, 05004], lr: 0.023629, loss: 1.3859
2022-08-03 06:02:55 - train: epoch 0079, iter [04000, 05004], lr: 0.023586, loss: 1.2568
2022-08-03 06:03:53 - train: epoch 0079, iter [04100, 05004], lr: 0.023544, loss: 1.4764
2022-08-03 06:04:49 - train: epoch 0079, iter [04200, 05004], lr: 0.023501, loss: 1.2768
2022-08-03 06:05:48 - train: epoch 0079, iter [04300, 05004], lr: 0.023458, loss: 1.3216
2022-08-03 06:06:47 - train: epoch 0079, iter [04400, 05004], lr: 0.023416, loss: 1.3479
2022-08-03 06:07:45 - train: epoch 0079, iter [04500, 05004], lr: 0.023373, loss: 1.3483
2022-08-03 06:08:43 - train: epoch 0079, iter [04600, 05004], lr: 0.023331, loss: 1.4659
2022-08-03 06:09:41 - train: epoch 0079, iter [04700, 05004], lr: 0.023289, loss: 1.2765
2022-08-03 06:10:39 - train: epoch 0079, iter [04800, 05004], lr: 0.023246, loss: 1.5173
2022-08-03 06:11:37 - train: epoch 0079, iter [04900, 05004], lr: 0.023204, loss: 1.2468
2022-08-03 06:12:32 - train: epoch 0079, iter [05000, 05004], lr: 0.023162, loss: 1.2307
2022-08-03 06:12:33 - train: epoch 079, train_loss: 1.3880
2022-08-03 06:14:45 - eval: epoch: 079, acc1: 69.004%, acc5: 89.174%, test_loss: 1.2538, per_image_load_time: 4.355ms, per_image_inference_time: 0.625ms
2022-08-03 06:14:45 - until epoch: 079, best_acc1: 69.004%
2022-08-03 06:14:45 - epoch 080 lr: 0.023159
2022-08-03 06:15:53 - train: epoch 0080, iter [00100, 05004], lr: 0.023118, loss: 1.2192
2022-08-03 06:16:51 - train: epoch 0080, iter [00200, 05004], lr: 0.023075, loss: 1.5451
2022-08-03 06:17:49 - train: epoch 0080, iter [00300, 05004], lr: 0.023033, loss: 1.3970
2022-08-03 06:18:47 - train: epoch 0080, iter [00400, 05004], lr: 0.022991, loss: 1.2598
2022-08-03 06:19:46 - train: epoch 0080, iter [00500, 05004], lr: 0.022949, loss: 1.2997
2022-08-03 06:20:44 - train: epoch 0080, iter [00600, 05004], lr: 0.022907, loss: 1.3992
2022-08-03 06:21:42 - train: epoch 0080, iter [00700, 05004], lr: 0.022865, loss: 1.2218
2022-08-03 06:22:39 - train: epoch 0080, iter [00800, 05004], lr: 0.022823, loss: 1.2333
2022-08-03 06:23:38 - train: epoch 0080, iter [00900, 05004], lr: 0.022781, loss: 1.2173
2022-08-03 06:24:34 - train: epoch 0080, iter [01000, 05004], lr: 0.022739, loss: 1.2971
2022-08-03 06:25:32 - train: epoch 0080, iter [01100, 05004], lr: 0.022697, loss: 1.3228
2022-08-03 06:26:30 - train: epoch 0080, iter [01200, 05004], lr: 0.022655, loss: 1.2559
2022-08-03 06:27:27 - train: epoch 0080, iter [01300, 05004], lr: 0.022613, loss: 1.1535
2022-08-03 06:28:26 - train: epoch 0080, iter [01400, 05004], lr: 0.022571, loss: 1.2775
2022-08-03 06:29:22 - train: epoch 0080, iter [01500, 05004], lr: 0.022529, loss: 1.3886
2022-08-03 06:30:20 - train: epoch 0080, iter [01600, 05004], lr: 0.022488, loss: 1.4841
2022-08-03 06:31:18 - train: epoch 0080, iter [01700, 05004], lr: 0.022446, loss: 1.4711
2022-08-03 06:32:15 - train: epoch 0080, iter [01800, 05004], lr: 0.022404, loss: 1.4918
2022-08-03 06:33:13 - train: epoch 0080, iter [01900, 05004], lr: 0.022362, loss: 1.3035
2022-08-03 06:34:11 - train: epoch 0080, iter [02000, 05004], lr: 0.022321, loss: 1.3760
2022-08-03 06:35:10 - train: epoch 0080, iter [02100, 05004], lr: 0.022279, loss: 1.5013
2022-08-03 06:36:09 - train: epoch 0080, iter [02200, 05004], lr: 0.022238, loss: 1.2922
2022-08-03 06:37:05 - train: epoch 0080, iter [02300, 05004], lr: 0.022196, loss: 1.1740
2022-08-03 06:38:03 - train: epoch 0080, iter [02400, 05004], lr: 0.022155, loss: 1.4904
2022-08-03 06:39:02 - train: epoch 0080, iter [02500, 05004], lr: 0.022113, loss: 1.3566
2022-08-03 06:40:01 - train: epoch 0080, iter [02600, 05004], lr: 0.022072, loss: 1.3019
2022-08-03 06:40:58 - train: epoch 0080, iter [02700, 05004], lr: 0.022030, loss: 1.2551
2022-08-03 06:41:56 - train: epoch 0080, iter [02800, 05004], lr: 0.021989, loss: 1.3066
2022-08-03 06:42:54 - train: epoch 0080, iter [02900, 05004], lr: 0.021948, loss: 1.5731
2022-08-03 06:43:53 - train: epoch 0080, iter [03000, 05004], lr: 0.021906, loss: 1.3016
2022-08-03 06:44:52 - train: epoch 0080, iter [03100, 05004], lr: 0.021865, loss: 1.4280
2022-08-03 06:45:49 - train: epoch 0080, iter [03200, 05004], lr: 0.021824, loss: 1.1986
2022-08-03 06:46:47 - train: epoch 0080, iter [03300, 05004], lr: 0.021783, loss: 1.1975
2022-08-03 06:47:45 - train: epoch 0080, iter [03400, 05004], lr: 0.021741, loss: 1.2237
2022-08-03 06:48:42 - train: epoch 0080, iter [03500, 05004], lr: 0.021700, loss: 1.3311
2022-08-03 06:49:40 - train: epoch 0080, iter [03600, 05004], lr: 0.021659, loss: 1.2028
2022-08-03 06:50:38 - train: epoch 0080, iter [03700, 05004], lr: 0.021618, loss: 1.4735
2022-08-03 06:51:37 - train: epoch 0080, iter [03800, 05004], lr: 0.021577, loss: 1.6048
2022-08-03 06:52:35 - train: epoch 0080, iter [03900, 05004], lr: 0.021536, loss: 1.3395
2022-08-03 06:53:32 - train: epoch 0080, iter [04000, 05004], lr: 0.021495, loss: 1.5024
2022-08-03 06:54:31 - train: epoch 0080, iter [04100, 05004], lr: 0.021454, loss: 1.4901
2022-08-03 06:55:28 - train: epoch 0080, iter [04200, 05004], lr: 0.021413, loss: 1.3040
2022-08-03 06:56:28 - train: epoch 0080, iter [04300, 05004], lr: 0.021373, loss: 1.6283
2022-08-03 06:57:26 - train: epoch 0080, iter [04400, 05004], lr: 0.021332, loss: 1.3887
2022-08-03 06:58:24 - train: epoch 0080, iter [04500, 05004], lr: 0.021291, loss: 1.4920
2022-08-03 06:59:23 - train: epoch 0080, iter [04600, 05004], lr: 0.021250, loss: 1.3845
2022-08-03 07:00:20 - train: epoch 0080, iter [04700, 05004], lr: 0.021210, loss: 1.4778
2022-08-03 07:01:19 - train: epoch 0080, iter [04800, 05004], lr: 0.021169, loss: 1.6449
2022-08-03 07:02:16 - train: epoch 0080, iter [04900, 05004], lr: 0.021128, loss: 1.5586
2022-08-03 07:03:12 - train: epoch 0080, iter [05000, 05004], lr: 0.021088, loss: 1.3403
2022-08-03 07:03:13 - train: epoch 080, train_loss: 1.3633
2022-08-03 07:05:21 - eval: epoch: 080, acc1: 69.638%, acc5: 89.560%, test_loss: 1.2254, per_image_load_time: 2.595ms, per_image_inference_time: 0.632ms
2022-08-03 07:05:21 - until epoch: 080, best_acc1: 69.638%
2022-08-03 07:05:21 - epoch 081 lr: 0.021086
2022-08-03 07:06:28 - train: epoch 0081, iter [00100, 05004], lr: 0.021045, loss: 1.2603
2022-08-03 07:07:25 - train: epoch 0081, iter [00200, 05004], lr: 0.021005, loss: 1.2454
2022-08-03 07:08:22 - train: epoch 0081, iter [00300, 05004], lr: 0.020964, loss: 1.2778
2022-08-03 07:09:20 - train: epoch 0081, iter [00400, 05004], lr: 0.020924, loss: 1.1611
2022-08-03 07:10:17 - train: epoch 0081, iter [00500, 05004], lr: 0.020883, loss: 1.4246
2022-08-03 07:11:15 - train: epoch 0081, iter [00600, 05004], lr: 0.020843, loss: 1.3265
2022-08-03 07:12:12 - train: epoch 0081, iter [00700, 05004], lr: 0.020803, loss: 1.3086
2022-08-03 07:13:10 - train: epoch 0081, iter [00800, 05004], lr: 0.020762, loss: 1.3461
2022-08-03 07:14:09 - train: epoch 0081, iter [00900, 05004], lr: 0.020722, loss: 1.2208
2022-08-03 07:15:07 - train: epoch 0081, iter [01000, 05004], lr: 0.020682, loss: 1.6506
2022-08-03 07:16:04 - train: epoch 0081, iter [01100, 05004], lr: 0.020642, loss: 1.2895
2022-08-03 07:17:05 - train: epoch 0081, iter [01200, 05004], lr: 0.020601, loss: 1.3313
2022-08-03 07:18:01 - train: epoch 0081, iter [01300, 05004], lr: 0.020561, loss: 1.1772
2022-08-03 07:19:00 - train: epoch 0081, iter [01400, 05004], lr: 0.020521, loss: 1.1051
2022-08-03 07:19:58 - train: epoch 0081, iter [01500, 05004], lr: 0.020481, loss: 1.5585
2022-08-03 07:20:57 - train: epoch 0081, iter [01600, 05004], lr: 0.020441, loss: 1.2586
2022-08-03 07:21:57 - train: epoch 0081, iter [01700, 05004], lr: 0.020401, loss: 1.3465
2022-08-03 07:22:56 - train: epoch 0081, iter [01800, 05004], lr: 0.020361, loss: 1.3589
2022-08-03 07:23:54 - train: epoch 0081, iter [01900, 05004], lr: 0.020321, loss: 1.4139
2022-08-03 07:24:49 - train: epoch 0081, iter [02000, 05004], lr: 0.020281, loss: 1.3415
2022-08-03 07:25:49 - train: epoch 0081, iter [02100, 05004], lr: 0.020241, loss: 1.4791
2022-08-03 07:26:48 - train: epoch 0081, iter [02200, 05004], lr: 0.020201, loss: 1.3868
2022-08-03 07:27:45 - train: epoch 0081, iter [02300, 05004], lr: 0.020162, loss: 1.2700
2022-08-03 07:28:42 - train: epoch 0081, iter [02400, 05004], lr: 0.020122, loss: 1.3693
2022-08-03 07:29:42 - train: epoch 0081, iter [02500, 05004], lr: 0.020082, loss: 1.4587
2022-08-03 07:30:41 - train: epoch 0081, iter [02600, 05004], lr: 0.020042, loss: 1.4252
2022-08-03 07:31:38 - train: epoch 0081, iter [02700, 05004], lr: 0.020003, loss: 1.4432
2022-08-03 07:32:38 - train: epoch 0081, iter [02800, 05004], lr: 0.019963, loss: 1.2976
2022-08-03 07:33:37 - train: epoch 0081, iter [02900, 05004], lr: 0.019923, loss: 1.4071
2022-08-03 07:34:34 - train: epoch 0081, iter [03000, 05004], lr: 0.019884, loss: 1.3976
2022-08-03 07:35:32 - train: epoch 0081, iter [03100, 05004], lr: 0.019844, loss: 1.1555
2022-08-03 07:36:31 - train: epoch 0081, iter [03200, 05004], lr: 0.019805, loss: 1.2976
2022-08-03 07:37:30 - train: epoch 0081, iter [03300, 05004], lr: 0.019765, loss: 1.2538
2022-08-03 07:38:28 - train: epoch 0081, iter [03400, 05004], lr: 0.019726, loss: 1.3269
2022-08-03 07:39:26 - train: epoch 0081, iter [03500, 05004], lr: 0.019687, loss: 1.5117
2022-08-03 07:40:23 - train: epoch 0081, iter [03600, 05004], lr: 0.019647, loss: 1.0830
2022-08-03 07:41:20 - train: epoch 0081, iter [03700, 05004], lr: 0.019608, loss: 1.3755
2022-08-03 07:42:17 - train: epoch 0081, iter [03800, 05004], lr: 0.019569, loss: 1.3129
2022-08-03 07:43:16 - train: epoch 0081, iter [03900, 05004], lr: 0.019529, loss: 1.4766
2022-08-03 07:44:12 - train: epoch 0081, iter [04000, 05004], lr: 0.019490, loss: 1.1862
2022-08-03 07:45:11 - train: epoch 0081, iter [04100, 05004], lr: 0.019451, loss: 1.4264
2022-08-03 07:46:12 - train: epoch 0081, iter [04200, 05004], lr: 0.019412, loss: 1.2719
2022-08-03 07:47:08 - train: epoch 0081, iter [04300, 05004], lr: 0.019373, loss: 1.4780
2022-08-03 07:48:06 - train: epoch 0081, iter [04400, 05004], lr: 0.019334, loss: 1.4862
2022-08-03 07:49:05 - train: epoch 0081, iter [04500, 05004], lr: 0.019295, loss: 1.2617
2022-08-03 07:50:04 - train: epoch 0081, iter [04600, 05004], lr: 0.019256, loss: 1.2903
2022-08-03 07:51:02 - train: epoch 0081, iter [04700, 05004], lr: 0.019217, loss: 1.3669
2022-08-03 07:51:59 - train: epoch 0081, iter [04800, 05004], lr: 0.019178, loss: 1.3585
2022-08-03 07:52:57 - train: epoch 0081, iter [04900, 05004], lr: 0.019139, loss: 1.2294
2022-08-03 07:53:52 - train: epoch 0081, iter [05000, 05004], lr: 0.019100, loss: 1.2085
2022-08-03 07:53:53 - train: epoch 081, train_loss: 1.3424
2022-08-03 07:56:04 - eval: epoch: 081, acc1: 69.658%, acc5: 89.552%, test_loss: 1.2317, per_image_load_time: 3.968ms, per_image_inference_time: 0.655ms
2022-08-03 07:56:04 - until epoch: 081, best_acc1: 69.658%
2022-08-03 07:56:04 - epoch 082 lr: 0.019098
2022-08-03 07:57:10 - train: epoch 0082, iter [00100, 05004], lr: 0.019059, loss: 1.2295
2022-08-03 07:58:09 - train: epoch 0082, iter [00200, 05004], lr: 0.019021, loss: 1.2982
2022-08-03 07:59:07 - train: epoch 0082, iter [00300, 05004], lr: 0.018982, loss: 1.4422
2022-08-03 08:00:03 - train: epoch 0082, iter [00400, 05004], lr: 0.018943, loss: 1.2672
2022-08-03 08:01:02 - train: epoch 0082, iter [00500, 05004], lr: 0.018905, loss: 1.4297
2022-08-03 08:01:59 - train: epoch 0082, iter [00600, 05004], lr: 0.018866, loss: 1.2780
2022-08-03 08:02:57 - train: epoch 0082, iter [00700, 05004], lr: 0.018827, loss: 1.5611
2022-08-03 08:03:55 - train: epoch 0082, iter [00800, 05004], lr: 0.018789, loss: 1.2544
2022-08-03 08:04:52 - train: epoch 0082, iter [00900, 05004], lr: 0.018750, loss: 1.3481
2022-08-03 08:05:50 - train: epoch 0082, iter [01000, 05004], lr: 0.018712, loss: 1.3411
2022-08-03 08:06:48 - train: epoch 0082, iter [01100, 05004], lr: 0.018673, loss: 1.4777
2022-08-03 08:07:47 - train: epoch 0082, iter [01200, 05004], lr: 0.018635, loss: 1.3824
2022-08-03 08:08:45 - train: epoch 0082, iter [01300, 05004], lr: 0.018596, loss: 1.4673
2022-08-03 08:09:44 - train: epoch 0082, iter [01400, 05004], lr: 0.018558, loss: 1.2494
2022-08-03 08:10:42 - train: epoch 0082, iter [01500, 05004], lr: 0.018520, loss: 1.3254
2022-08-03 08:11:42 - train: epoch 0082, iter [01600, 05004], lr: 0.018481, loss: 1.2720
2022-08-03 08:12:38 - train: epoch 0082, iter [01700, 05004], lr: 0.018443, loss: 1.3642
2022-08-03 08:13:37 - train: epoch 0082, iter [01800, 05004], lr: 0.018405, loss: 1.2454
2022-08-03 08:14:35 - train: epoch 0082, iter [01900, 05004], lr: 0.018367, loss: 1.2979
2022-08-03 08:15:34 - train: epoch 0082, iter [02000, 05004], lr: 0.018329, loss: 1.1863
2022-08-03 08:16:34 - train: epoch 0082, iter [02100, 05004], lr: 0.018290, loss: 1.2788
2022-08-03 08:17:30 - train: epoch 0082, iter [02200, 05004], lr: 0.018252, loss: 1.3623
2022-08-03 08:18:28 - train: epoch 0082, iter [02300, 05004], lr: 0.018214, loss: 1.3676
2022-08-03 08:19:27 - train: epoch 0082, iter [02400, 05004], lr: 0.018176, loss: 1.4313
2022-08-03 08:20:24 - train: epoch 0082, iter [02500, 05004], lr: 0.018138, loss: 1.1538
2022-08-03 08:21:22 - train: epoch 0082, iter [02600, 05004], lr: 0.018100, loss: 1.1188
2022-08-03 08:22:21 - train: epoch 0082, iter [02700, 05004], lr: 0.018062, loss: 1.2672
2022-08-03 08:23:19 - train: epoch 0082, iter [02800, 05004], lr: 0.018025, loss: 1.1953
2022-08-03 08:24:17 - train: epoch 0082, iter [02900, 05004], lr: 0.017987, loss: 1.2162
2022-08-03 08:25:14 - train: epoch 0082, iter [03000, 05004], lr: 0.017949, loss: 1.3167
2022-08-03 08:26:12 - train: epoch 0082, iter [03100, 05004], lr: 0.017911, loss: 1.4558
2022-08-03 08:27:10 - train: epoch 0082, iter [03200, 05004], lr: 0.017873, loss: 1.3100
2022-08-03 08:28:08 - train: epoch 0082, iter [03300, 05004], lr: 0.017836, loss: 1.2435
2022-08-03 08:29:05 - train: epoch 0082, iter [03400, 05004], lr: 0.017798, loss: 1.2717
2022-08-03 08:30:03 - train: epoch 0082, iter [03500, 05004], lr: 0.017761, loss: 1.3396
2022-08-03 08:31:01 - train: epoch 0082, iter [03600, 05004], lr: 0.017723, loss: 1.2685
2022-08-03 08:31:59 - train: epoch 0082, iter [03700, 05004], lr: 0.017685, loss: 1.3046
2022-08-03 08:32:56 - train: epoch 0082, iter [03800, 05004], lr: 0.017648, loss: 1.4532
2022-08-03 08:33:56 - train: epoch 0082, iter [03900, 05004], lr: 0.017610, loss: 1.2453
2022-08-03 08:34:53 - train: epoch 0082, iter [04000, 05004], lr: 0.017573, loss: 1.4231
2022-08-03 08:35:50 - train: epoch 0082, iter [04100, 05004], lr: 0.017536, loss: 1.4914
2022-08-03 08:36:46 - train: epoch 0082, iter [04200, 05004], lr: 0.017498, loss: 1.3905
2022-08-03 08:37:45 - train: epoch 0082, iter [04300, 05004], lr: 0.017461, loss: 1.3427
2022-08-03 08:38:42 - train: epoch 0082, iter [04400, 05004], lr: 0.017424, loss: 1.2736
2022-08-03 08:39:42 - train: epoch 0082, iter [04500, 05004], lr: 0.017386, loss: 1.2821
2022-08-03 08:40:40 - train: epoch 0082, iter [04600, 05004], lr: 0.017349, loss: 1.2756
2022-08-03 08:41:37 - train: epoch 0082, iter [04700, 05004], lr: 0.017312, loss: 1.2958
2022-08-03 08:42:35 - train: epoch 0082, iter [04800, 05004], lr: 0.017275, loss: 1.2215
2022-08-03 08:43:33 - train: epoch 0082, iter [04900, 05004], lr: 0.017238, loss: 1.4458
2022-08-03 08:44:28 - train: epoch 0082, iter [05000, 05004], lr: 0.017201, loss: 1.2834
2022-08-03 08:44:29 - train: epoch 082, train_loss: 1.3157
2022-08-03 08:46:39 - eval: epoch: 082, acc1: 70.360%, acc5: 89.698%, test_loss: 1.2058, per_image_load_time: 4.365ms, per_image_inference_time: 0.644ms
2022-08-03 08:46:39 - until epoch: 082, best_acc1: 70.360%
2022-08-03 08:46:39 - epoch 083 lr: 0.017199
2022-08-03 08:47:45 - train: epoch 0083, iter [00100, 05004], lr: 0.017162, loss: 1.1661
2022-08-03 08:48:46 - train: epoch 0083, iter [00200, 05004], lr: 0.017125, loss: 1.0154
2022-08-03 08:49:42 - train: epoch 0083, iter [00300, 05004], lr: 0.017088, loss: 1.2240
2022-08-03 08:50:40 - train: epoch 0083, iter [00400, 05004], lr: 0.017051, loss: 1.3601
2022-08-03 08:51:38 - train: epoch 0083, iter [00500, 05004], lr: 0.017014, loss: 1.1297
2022-08-03 08:52:35 - train: epoch 0083, iter [00600, 05004], lr: 0.016977, loss: 1.1648
2022-08-03 08:53:34 - train: epoch 0083, iter [00700, 05004], lr: 0.016941, loss: 1.2724
2022-08-03 08:54:31 - train: epoch 0083, iter [00800, 05004], lr: 0.016904, loss: 1.1693
2022-08-03 08:55:29 - train: epoch 0083, iter [00900, 05004], lr: 0.016867, loss: 1.3117
2022-08-03 08:56:26 - train: epoch 0083, iter [01000, 05004], lr: 0.016830, loss: 1.4716
2022-08-03 08:57:25 - train: epoch 0083, iter [01100, 05004], lr: 0.016794, loss: 1.2908
2022-08-03 08:58:22 - train: epoch 0083, iter [01200, 05004], lr: 0.016757, loss: 1.5103
2022-08-03 08:59:20 - train: epoch 0083, iter [01300, 05004], lr: 0.016720, loss: 1.4017
2022-08-03 09:00:17 - train: epoch 0083, iter [01400, 05004], lr: 0.016684, loss: 1.4503
2022-08-03 09:01:15 - train: epoch 0083, iter [01500, 05004], lr: 0.016647, loss: 1.2511
2022-08-03 09:02:14 - train: epoch 0083, iter [01600, 05004], lr: 0.016611, loss: 1.3563
2022-08-03 09:03:11 - train: epoch 0083, iter [01700, 05004], lr: 0.016574, loss: 1.3683
2022-08-03 09:04:11 - train: epoch 0083, iter [01800, 05004], lr: 0.016538, loss: 1.5621
2022-08-03 09:05:09 - train: epoch 0083, iter [01900, 05004], lr: 0.016502, loss: 1.1061
2022-08-03 09:06:05 - train: epoch 0083, iter [02000, 05004], lr: 0.016465, loss: 0.9952
2022-08-03 09:07:04 - train: epoch 0083, iter [02100, 05004], lr: 0.016429, loss: 1.4587
2022-08-03 09:08:03 - train: epoch 0083, iter [02200, 05004], lr: 0.016393, loss: 1.2869
2022-08-03 09:09:01 - train: epoch 0083, iter [02300, 05004], lr: 0.016356, loss: 1.3635
2022-08-03 09:10:00 - train: epoch 0083, iter [02400, 05004], lr: 0.016320, loss: 1.2283
2022-08-03 09:10:58 - train: epoch 0083, iter [02500, 05004], lr: 0.016284, loss: 1.0617
2022-08-03 09:11:58 - train: epoch 0083, iter [02600, 05004], lr: 0.016248, loss: 1.2852
2022-08-03 09:12:55 - train: epoch 0083, iter [02700, 05004], lr: 0.016212, loss: 1.2422
2022-08-03 09:13:53 - train: epoch 0083, iter [02800, 05004], lr: 0.016176, loss: 1.3568
2022-08-03 09:14:51 - train: epoch 0083, iter [02900, 05004], lr: 0.016140, loss: 1.3658
2022-08-03 09:15:49 - train: epoch 0083, iter [03000, 05004], lr: 0.016104, loss: 1.3485
2022-08-03 09:16:48 - train: epoch 0083, iter [03100, 05004], lr: 0.016068, loss: 1.3209
2022-08-03 09:17:45 - train: epoch 0083, iter [03200, 05004], lr: 0.016032, loss: 1.1626
2022-08-03 09:18:43 - train: epoch 0083, iter [03300, 05004], lr: 0.015996, loss: 1.4334
2022-08-03 09:19:42 - train: epoch 0083, iter [03400, 05004], lr: 0.015960, loss: 1.2612
2022-08-03 09:20:39 - train: epoch 0083, iter [03500, 05004], lr: 0.015924, loss: 1.2329
2022-08-03 09:21:38 - train: epoch 0083, iter [03600, 05004], lr: 0.015889, loss: 1.1769
2022-08-03 09:22:38 - train: epoch 0083, iter [03700, 05004], lr: 0.015853, loss: 1.1814
2022-08-03 09:23:36 - train: epoch 0083, iter [03800, 05004], lr: 0.015817, loss: 1.1265
2022-08-03 09:24:36 - train: epoch 0083, iter [03900, 05004], lr: 0.015782, loss: 1.5222
2022-08-03 09:25:34 - train: epoch 0083, iter [04000, 05004], lr: 0.015746, loss: 1.4612
2022-08-03 09:26:33 - train: epoch 0083, iter [04100, 05004], lr: 0.015710, loss: 1.2036
2022-08-03 09:27:29 - train: epoch 0083, iter [04200, 05004], lr: 0.015675, loss: 1.5812
2022-08-03 09:28:27 - train: epoch 0083, iter [04300, 05004], lr: 0.015639, loss: 1.2162
2022-08-03 09:29:24 - train: epoch 0083, iter [04400, 05004], lr: 0.015604, loss: 1.2600
2022-08-03 09:30:23 - train: epoch 0083, iter [04500, 05004], lr: 0.015568, loss: 1.2403
2022-08-03 09:31:22 - train: epoch 0083, iter [04600, 05004], lr: 0.015533, loss: 1.4724
2022-08-03 09:32:17 - train: epoch 0083, iter [04700, 05004], lr: 0.015498, loss: 1.5581
2022-08-03 09:33:19 - train: epoch 0083, iter [04800, 05004], lr: 0.015462, loss: 1.4051
2022-08-03 09:34:17 - train: epoch 0083, iter [04900, 05004], lr: 0.015427, loss: 1.2984
2022-08-03 09:35:11 - train: epoch 0083, iter [05000, 05004], lr: 0.015392, loss: 1.2631
2022-08-03 09:35:12 - train: epoch 083, train_loss: 1.2906
2022-08-03 09:37:19 - eval: epoch: 083, acc1: 70.538%, acc5: 89.948%, test_loss: 1.1916, per_image_load_time: 2.634ms, per_image_inference_time: 0.698ms
2022-08-03 09:37:19 - until epoch: 083, best_acc1: 70.538%
2022-08-03 09:37:19 - epoch 084 lr: 0.015390
2022-08-03 09:38:29 - train: epoch 0084, iter [00100, 05004], lr: 0.015355, loss: 1.2346
2022-08-03 09:39:27 - train: epoch 0084, iter [00200, 05004], lr: 0.015320, loss: 1.2365
2022-08-03 09:40:23 - train: epoch 0084, iter [00300, 05004], lr: 0.015285, loss: 1.1228
2022-08-03 09:41:23 - train: epoch 0084, iter [00400, 05004], lr: 0.015250, loss: 1.1783
2022-08-03 09:42:20 - train: epoch 0084, iter [00500, 05004], lr: 0.015215, loss: 1.3150
2022-08-03 09:43:18 - train: epoch 0084, iter [00600, 05004], lr: 0.015180, loss: 1.4748
2022-08-03 09:44:16 - train: epoch 0084, iter [00700, 05004], lr: 0.015145, loss: 1.4660
2022-08-03 09:45:14 - train: epoch 0084, iter [00800, 05004], lr: 0.015110, loss: 1.3254
2022-08-03 09:46:12 - train: epoch 0084, iter [00900, 05004], lr: 0.015075, loss: 1.3042
2022-08-03 09:47:08 - train: epoch 0084, iter [01000, 05004], lr: 0.015040, loss: 1.2067
2022-08-03 09:48:06 - train: epoch 0084, iter [01100, 05004], lr: 0.015005, loss: 1.1331
2022-08-03 09:49:04 - train: epoch 0084, iter [01200, 05004], lr: 0.014970, loss: 1.5749
2022-08-03 09:50:01 - train: epoch 0084, iter [01300, 05004], lr: 0.014936, loss: 1.4093
2022-08-03 09:51:00 - train: epoch 0084, iter [01400, 05004], lr: 0.014901, loss: 1.2717
2022-08-03 09:51:59 - train: epoch 0084, iter [01500, 05004], lr: 0.014866, loss: 1.2923
2022-08-03 09:52:57 - train: epoch 0084, iter [01600, 05004], lr: 0.014832, loss: 1.3159
2022-08-03 09:53:54 - train: epoch 0084, iter [01700, 05004], lr: 0.014797, loss: 1.4683
2022-08-03 09:54:53 - train: epoch 0084, iter [01800, 05004], lr: 0.014762, loss: 1.2651
2022-08-03 09:55:49 - train: epoch 0084, iter [01900, 05004], lr: 0.014728, loss: 1.3249
2022-08-03 09:56:48 - train: epoch 0084, iter [02000, 05004], lr: 0.014693, loss: 1.2743
2022-08-03 09:57:46 - train: epoch 0084, iter [02100, 05004], lr: 0.014659, loss: 1.1634
2022-08-03 09:58:43 - train: epoch 0084, iter [02200, 05004], lr: 0.014624, loss: 1.2363
2022-08-03 09:59:42 - train: epoch 0084, iter [02300, 05004], lr: 0.014590, loss: 1.2081
2022-08-03 10:00:40 - train: epoch 0084, iter [02400, 05004], lr: 0.014556, loss: 1.2656
2022-08-03 10:01:40 - train: epoch 0084, iter [02500, 05004], lr: 0.014521, loss: 1.3996
2022-08-03 10:02:37 - train: epoch 0084, iter [02600, 05004], lr: 0.014487, loss: 1.2278
2022-08-03 10:03:36 - train: epoch 0084, iter [02700, 05004], lr: 0.014453, loss: 1.2643
2022-08-03 10:04:35 - train: epoch 0084, iter [02800, 05004], lr: 0.014419, loss: 1.3417
2022-08-03 10:05:32 - train: epoch 0084, iter [02900, 05004], lr: 0.014385, loss: 1.3181
2022-08-03 10:06:30 - train: epoch 0084, iter [03000, 05004], lr: 0.014350, loss: 1.2515
2022-08-03 10:07:28 - train: epoch 0084, iter [03100, 05004], lr: 0.014316, loss: 1.4165
2022-08-03 10:08:25 - train: epoch 0084, iter [03200, 05004], lr: 0.014282, loss: 1.2986
2022-08-03 10:09:24 - train: epoch 0084, iter [03300, 05004], lr: 0.014248, loss: 1.3169
2022-08-03 10:10:22 - train: epoch 0084, iter [03400, 05004], lr: 0.014214, loss: 1.2330
2022-08-03 10:11:20 - train: epoch 0084, iter [03500, 05004], lr: 0.014180, loss: 1.1796
2022-08-03 10:12:17 - train: epoch 0084, iter [03600, 05004], lr: 0.014146, loss: 1.1151
2022-08-03 10:13:17 - train: epoch 0084, iter [03700, 05004], lr: 0.014113, loss: 1.3692
2022-08-03 10:14:15 - train: epoch 0084, iter [03800, 05004], lr: 0.014079, loss: 1.3345
2022-08-03 10:15:14 - train: epoch 0084, iter [03900, 05004], lr: 0.014045, loss: 1.3694
2022-08-03 10:16:11 - train: epoch 0084, iter [04000, 05004], lr: 0.014011, loss: 1.1442
2022-08-03 10:17:12 - train: epoch 0084, iter [04100, 05004], lr: 0.013977, loss: 1.1242
2022-08-03 10:18:07 - train: epoch 0084, iter [04200, 05004], lr: 0.013944, loss: 1.4026
2022-08-03 10:19:05 - train: epoch 0084, iter [04300, 05004], lr: 0.013910, loss: 1.3695
2022-08-03 10:20:04 - train: epoch 0084, iter [04400, 05004], lr: 0.013877, loss: 1.5412
2022-08-03 10:21:03 - train: epoch 0084, iter [04500, 05004], lr: 0.013843, loss: 1.1562
2022-08-03 10:22:00 - train: epoch 0084, iter [04600, 05004], lr: 0.013809, loss: 1.2494
2022-08-03 10:22:58 - train: epoch 0084, iter [04700, 05004], lr: 0.013776, loss: 1.1991
2022-08-03 10:23:55 - train: epoch 0084, iter [04800, 05004], lr: 0.013742, loss: 1.0278
2022-08-03 10:24:55 - train: epoch 0084, iter [04900, 05004], lr: 0.013709, loss: 1.2362
2022-08-03 10:25:49 - train: epoch 0084, iter [05000, 05004], lr: 0.013676, loss: 1.4217
2022-08-03 10:25:50 - train: epoch 084, train_loss: 1.2665
2022-08-03 10:28:01 - eval: epoch: 084, acc1: 70.808%, acc5: 90.302%, test_loss: 1.1736, per_image_load_time: 3.628ms, per_image_inference_time: 0.656ms
2022-08-03 10:28:01 - until epoch: 084, best_acc1: 70.808%
2022-08-03 10:28:01 - epoch 085 lr: 0.013674
2022-08-03 10:29:10 - train: epoch 0085, iter [00100, 05004], lr: 0.013641, loss: 1.1450
2022-08-03 10:30:09 - train: epoch 0085, iter [00200, 05004], lr: 0.013608, loss: 1.1572
2022-08-03 10:31:07 - train: epoch 0085, iter [00300, 05004], lr: 0.013574, loss: 1.2849
2022-08-03 10:32:04 - train: epoch 0085, iter [00400, 05004], lr: 0.013541, loss: 1.3717
2022-08-03 10:33:02 - train: epoch 0085, iter [00500, 05004], lr: 0.013508, loss: 1.1314
2022-08-03 10:33:59 - train: epoch 0085, iter [00600, 05004], lr: 0.013475, loss: 1.1355
2022-08-03 10:34:58 - train: epoch 0085, iter [00700, 05004], lr: 0.013442, loss: 1.2726
2022-08-03 10:35:57 - train: epoch 0085, iter [00800, 05004], lr: 0.013409, loss: 1.0539
2022-08-03 10:36:53 - train: epoch 0085, iter [00900, 05004], lr: 0.013376, loss: 1.1594
2022-08-03 10:37:51 - train: epoch 0085, iter [01000, 05004], lr: 0.013343, loss: 1.1025
2022-08-03 10:38:49 - train: epoch 0085, iter [01100, 05004], lr: 0.013310, loss: 1.2225
2022-08-03 10:39:48 - train: epoch 0085, iter [01200, 05004], lr: 0.013277, loss: 1.3071
2022-08-03 10:40:46 - train: epoch 0085, iter [01300, 05004], lr: 0.013244, loss: 1.2801
2022-08-03 10:41:43 - train: epoch 0085, iter [01400, 05004], lr: 0.013211, loss: 1.2958
2022-08-03 10:42:41 - train: epoch 0085, iter [01500, 05004], lr: 0.013178, loss: 1.4190
2022-08-03 10:43:41 - train: epoch 0085, iter [01600, 05004], lr: 0.013145, loss: 1.2423
2022-08-03 10:44:38 - train: epoch 0085, iter [01700, 05004], lr: 0.013113, loss: 1.4301
2022-08-03 10:45:36 - train: epoch 0085, iter [01800, 05004], lr: 0.013080, loss: 1.0331
2022-08-03 10:46:35 - train: epoch 0085, iter [01900, 05004], lr: 0.013047, loss: 1.1505
2022-08-03 10:47:34 - train: epoch 0085, iter [02000, 05004], lr: 0.013015, loss: 1.1096
2022-08-03 10:48:32 - train: epoch 0085, iter [02100, 05004], lr: 0.012982, loss: 1.3857
2022-08-03 10:49:30 - train: epoch 0085, iter [02200, 05004], lr: 0.012950, loss: 1.4123
2022-08-03 10:50:28 - train: epoch 0085, iter [02300, 05004], lr: 0.012917, loss: 1.2197
2022-08-03 10:51:27 - train: epoch 0085, iter [02400, 05004], lr: 0.012885, loss: 1.2290
2022-08-03 10:52:24 - train: epoch 0085, iter [02500, 05004], lr: 0.012852, loss: 1.3370
2022-08-03 10:53:23 - train: epoch 0085, iter [02600, 05004], lr: 0.012820, loss: 1.3031
2022-08-03 10:54:23 - train: epoch 0085, iter [02700, 05004], lr: 0.012787, loss: 1.0944
2022-08-03 10:55:20 - train: epoch 0085, iter [02800, 05004], lr: 0.012755, loss: 1.4600
2022-08-03 10:56:17 - train: epoch 0085, iter [02900, 05004], lr: 0.012723, loss: 1.5073
2022-08-03 10:57:15 - train: epoch 0085, iter [03000, 05004], lr: 0.012691, loss: 1.3048
2022-08-03 10:58:13 - train: epoch 0085, iter [03100, 05004], lr: 0.012658, loss: 1.1764
2022-08-03 10:59:10 - train: epoch 0085, iter [03200, 05004], lr: 0.012626, loss: 1.3237
2022-08-03 11:00:09 - train: epoch 0085, iter [03300, 05004], lr: 0.012594, loss: 1.2096
2022-08-03 11:01:07 - train: epoch 0085, iter [03400, 05004], lr: 0.012562, loss: 1.2464
2022-08-03 11:02:07 - train: epoch 0085, iter [03500, 05004], lr: 0.012530, loss: 1.3051
2022-08-03 11:03:05 - train: epoch 0085, iter [03600, 05004], lr: 0.012498, loss: 1.4370
2022-08-03 11:04:03 - train: epoch 0085, iter [03700, 05004], lr: 0.012466, loss: 1.1367
2022-08-03 11:05:02 - train: epoch 0085, iter [03800, 05004], lr: 0.012434, loss: 1.3021
2022-08-03 11:05:59 - train: epoch 0085, iter [03900, 05004], lr: 0.012402, loss: 1.2927
2022-08-03 11:06:57 - train: epoch 0085, iter [04000, 05004], lr: 0.012370, loss: 1.4790
2022-08-03 11:07:56 - train: epoch 0085, iter [04100, 05004], lr: 0.012339, loss: 1.3273
2022-08-03 11:08:53 - train: epoch 0085, iter [04200, 05004], lr: 0.012307, loss: 1.2263
2022-08-03 11:09:51 - train: epoch 0085, iter [04300, 05004], lr: 0.012275, loss: 1.1547
2022-08-03 11:10:49 - train: epoch 0085, iter [04400, 05004], lr: 0.012243, loss: 1.2658
2022-08-03 11:11:49 - train: epoch 0085, iter [04500, 05004], lr: 0.012212, loss: 1.1929
2022-08-03 11:12:49 - train: epoch 0085, iter [04600, 05004], lr: 0.012180, loss: 1.2646
2022-08-03 11:13:46 - train: epoch 0085, iter [04700, 05004], lr: 0.012148, loss: 1.6483
2022-08-03 11:14:43 - train: epoch 0085, iter [04800, 05004], lr: 0.012117, loss: 1.1593
2022-08-03 11:15:43 - train: epoch 0085, iter [04900, 05004], lr: 0.012085, loss: 1.3050
2022-08-03 11:16:37 - train: epoch 0085, iter [05000, 05004], lr: 0.012054, loss: 1.0110
2022-08-03 11:16:38 - train: epoch 085, train_loss: 1.2407
2022-08-03 11:18:46 - eval: epoch: 085, acc1: 71.238%, acc5: 90.326%, test_loss: 1.1651, per_image_load_time: 4.260ms, per_image_inference_time: 0.655ms
2022-08-03 11:18:46 - until epoch: 085, best_acc1: 71.238%
2022-08-03 11:18:46 - epoch 086 lr: 0.012052
2022-08-03 11:19:54 - train: epoch 0086, iter [00100, 05004], lr: 0.012021, loss: 1.1058
2022-08-03 11:20:52 - train: epoch 0086, iter [00200, 05004], lr: 0.011990, loss: 0.9844
2022-08-03 11:21:49 - train: epoch 0086, iter [00300, 05004], lr: 0.011958, loss: 1.3659
2022-08-03 11:22:46 - train: epoch 0086, iter [00400, 05004], lr: 0.011927, loss: 1.3005
2022-08-03 11:23:44 - train: epoch 0086, iter [00500, 05004], lr: 0.011896, loss: 1.2597
2022-08-03 11:24:42 - train: epoch 0086, iter [00600, 05004], lr: 0.011865, loss: 1.2758
2022-08-03 11:25:41 - train: epoch 0086, iter [00700, 05004], lr: 0.011833, loss: 1.2158
2022-08-03 11:26:37 - train: epoch 0086, iter [00800, 05004], lr: 0.011802, loss: 1.2539
2022-08-03 11:27:35 - train: epoch 0086, iter [00900, 05004], lr: 0.011771, loss: 1.1367
2022-08-03 11:28:34 - train: epoch 0086, iter [01000, 05004], lr: 0.011740, loss: 1.4245
2022-08-03 11:29:32 - train: epoch 0086, iter [01100, 05004], lr: 0.011709, loss: 1.2866
2022-08-03 11:30:30 - train: epoch 0086, iter [01200, 05004], lr: 0.011678, loss: 1.0888
2022-08-03 11:31:26 - train: epoch 0086, iter [01300, 05004], lr: 0.011647, loss: 1.2963
2022-08-03 11:32:26 - train: epoch 0086, iter [01400, 05004], lr: 0.011616, loss: 1.0918
2022-08-03 11:33:24 - train: epoch 0086, iter [01500, 05004], lr: 0.011585, loss: 0.9733
2022-08-03 11:34:22 - train: epoch 0086, iter [01600, 05004], lr: 0.011554, loss: 1.2731
2022-08-03 11:35:20 - train: epoch 0086, iter [01700, 05004], lr: 0.011523, loss: 1.2472
2022-08-03 11:36:19 - train: epoch 0086, iter [01800, 05004], lr: 0.011493, loss: 1.3738
2022-08-03 11:37:17 - train: epoch 0086, iter [01900, 05004], lr: 0.011462, loss: 1.1708
2022-08-03 11:38:14 - train: epoch 0086, iter [02000, 05004], lr: 0.011431, loss: 1.3698
2022-08-03 11:39:11 - train: epoch 0086, iter [02100, 05004], lr: 0.011401, loss: 1.1413
2022-08-03 11:40:10 - train: epoch 0086, iter [02200, 05004], lr: 0.011370, loss: 1.0997
2022-08-03 11:41:09 - train: epoch 0086, iter [02300, 05004], lr: 0.011339, loss: 1.2102
2022-08-03 11:42:06 - train: epoch 0086, iter [02400, 05004], lr: 0.011309, loss: 1.1354
2022-08-03 11:43:04 - train: epoch 0086, iter [02500, 05004], lr: 0.011278, loss: 1.3466
2022-08-03 11:44:02 - train: epoch 0086, iter [02600, 05004], lr: 0.011248, loss: 1.3528
2022-08-03 11:45:02 - train: epoch 0086, iter [02700, 05004], lr: 0.011217, loss: 1.2163
2022-08-03 11:46:00 - train: epoch 0086, iter [02800, 05004], lr: 0.011187, loss: 1.3686
2022-08-03 11:46:58 - train: epoch 0086, iter [02900, 05004], lr: 0.011157, loss: 1.1896
2022-08-03 11:47:56 - train: epoch 0086, iter [03000, 05004], lr: 0.011126, loss: 1.2394
2022-08-03 11:48:54 - train: epoch 0086, iter [03100, 05004], lr: 0.011096, loss: 1.1840
2022-08-03 11:49:54 - train: epoch 0086, iter [03200, 05004], lr: 0.011066, loss: 1.2871
2022-08-03 11:50:51 - train: epoch 0086, iter [03300, 05004], lr: 0.011036, loss: 1.2858
2022-08-03 11:51:48 - train: epoch 0086, iter [03400, 05004], lr: 0.011005, loss: 1.3670
2022-08-03 11:52:46 - train: epoch 0086, iter [03500, 05004], lr: 0.010975, loss: 1.2579
2022-08-03 11:53:44 - train: epoch 0086, iter [03600, 05004], lr: 0.010945, loss: 1.1521
2022-08-03 11:54:43 - train: epoch 0086, iter [03700, 05004], lr: 0.010915, loss: 1.1432
2022-08-03 11:55:39 - train: epoch 0086, iter [03800, 05004], lr: 0.010885, loss: 1.2405
2022-08-03 11:56:37 - train: epoch 0086, iter [03900, 05004], lr: 0.010855, loss: 1.1178
2022-08-03 11:57:35 - train: epoch 0086, iter [04000, 05004], lr: 0.010825, loss: 1.0527
2022-08-03 11:58:33 - train: epoch 0086, iter [04100, 05004], lr: 0.010795, loss: 1.3302
2022-08-03 11:59:32 - train: epoch 0086, iter [04200, 05004], lr: 0.010766, loss: 1.1431
2022-08-03 12:00:29 - train: epoch 0086, iter [04300, 05004], lr: 0.010736, loss: 1.3275
2022-08-03 12:01:26 - train: epoch 0086, iter [04400, 05004], lr: 0.010706, loss: 1.1904
2022-08-03 12:02:24 - train: epoch 0086, iter [04500, 05004], lr: 0.010676, loss: 1.0839
2022-08-03 12:03:22 - train: epoch 0086, iter [04600, 05004], lr: 0.010647, loss: 1.1546
2022-08-03 12:04:20 - train: epoch 0086, iter [04700, 05004], lr: 0.010617, loss: 1.2006
2022-08-03 12:05:19 - train: epoch 0086, iter [04800, 05004], lr: 0.010587, loss: 1.2117
2022-08-03 12:06:16 - train: epoch 0086, iter [04900, 05004], lr: 0.010558, loss: 1.2986
2022-08-03 12:07:11 - train: epoch 0086, iter [05000, 05004], lr: 0.010528, loss: 1.2546
2022-08-03 12:07:12 - train: epoch 086, train_loss: 1.2132
2022-08-03 12:09:21 - eval: epoch: 086, acc1: 71.526%, acc5: 90.432%, test_loss: 1.1568, per_image_load_time: 4.036ms, per_image_inference_time: 0.633ms
2022-08-03 12:09:22 - until epoch: 086, best_acc1: 71.526%
2022-08-03 12:09:22 - epoch 087 lr: 0.010527
2022-08-03 12:10:29 - train: epoch 0087, iter [00100, 05004], lr: 0.010498, loss: 1.2147
2022-08-03 12:11:27 - train: epoch 0087, iter [00200, 05004], lr: 0.010468, loss: 1.2140
2022-08-03 12:12:25 - train: epoch 0087, iter [00300, 05004], lr: 0.010439, loss: 1.0884
2022-08-03 12:13:22 - train: epoch 0087, iter [00400, 05004], lr: 0.010409, loss: 1.2038
2022-08-03 12:14:21 - train: epoch 0087, iter [00500, 05004], lr: 0.010380, loss: 1.0067
2022-08-03 12:15:18 - train: epoch 0087, iter [00600, 05004], lr: 0.010351, loss: 1.2466
2022-08-03 12:16:15 - train: epoch 0087, iter [00700, 05004], lr: 0.010321, loss: 1.0663
2022-08-03 12:17:13 - train: epoch 0087, iter [00800, 05004], lr: 0.010292, loss: 1.1807
2022-08-03 12:18:11 - train: epoch 0087, iter [00900, 05004], lr: 0.010263, loss: 1.1766
2022-08-03 12:19:09 - train: epoch 0087, iter [01000, 05004], lr: 0.010234, loss: 1.0323
2022-08-03 12:20:07 - train: epoch 0087, iter [01100, 05004], lr: 0.010205, loss: 1.2904
2022-08-03 12:21:05 - train: epoch 0087, iter [01200, 05004], lr: 0.010176, loss: 1.1746
2022-08-03 12:22:02 - train: epoch 0087, iter [01300, 05004], lr: 0.010147, loss: 1.3762
2022-08-03 12:23:01 - train: epoch 0087, iter [01400, 05004], lr: 0.010118, loss: 1.1170
2022-08-03 12:23:59 - train: epoch 0087, iter [01500, 05004], lr: 0.010089, loss: 1.1649
2022-08-03 12:24:58 - train: epoch 0087, iter [01600, 05004], lr: 0.010060, loss: 1.1150
2022-08-03 12:25:56 - train: epoch 0087, iter [01700, 05004], lr: 0.010031, loss: 1.2214
2022-08-03 12:26:53 - train: epoch 0087, iter [01800, 05004], lr: 0.010002, loss: 1.1320
2022-08-03 12:27:52 - train: epoch 0087, iter [01900, 05004], lr: 0.009973, loss: 1.3099
2022-08-03 12:28:49 - train: epoch 0087, iter [02000, 05004], lr: 0.009945, loss: 1.3218
2022-08-03 12:29:47 - train: epoch 0087, iter [02100, 05004], lr: 0.009916, loss: 1.2358
2022-08-03 12:30:46 - train: epoch 0087, iter [02200, 05004], lr: 0.009887, loss: 1.3005
2022-08-03 12:31:44 - train: epoch 0087, iter [02300, 05004], lr: 0.009859, loss: 1.4068
2022-08-03 12:32:42 - train: epoch 0087, iter [02400, 05004], lr: 0.009830, loss: 1.1106
2022-08-03 12:33:41 - train: epoch 0087, iter [02500, 05004], lr: 0.009801, loss: 1.3688
2022-08-03 12:34:37 - train: epoch 0087, iter [02600, 05004], lr: 0.009773, loss: 1.1022
2022-08-03 12:35:37 - train: epoch 0087, iter [02700, 05004], lr: 0.009744, loss: 1.0953
2022-08-03 12:36:36 - train: epoch 0087, iter [02800, 05004], lr: 0.009716, loss: 1.1283
2022-08-03 12:37:32 - train: epoch 0087, iter [02900, 05004], lr: 0.009688, loss: 0.9672
2022-08-03 12:38:30 - train: epoch 0087, iter [03000, 05004], lr: 0.009659, loss: 1.1633
2022-08-03 12:39:29 - train: epoch 0087, iter [03100, 05004], lr: 0.009631, loss: 1.1568
2022-08-03 12:40:27 - train: epoch 0087, iter [03200, 05004], lr: 0.009603, loss: 1.1930
2022-08-03 12:41:24 - train: epoch 0087, iter [03300, 05004], lr: 0.009574, loss: 1.2020
2022-08-03 12:42:22 - train: epoch 0087, iter [03400, 05004], lr: 0.009546, loss: 1.1480
2022-08-03 12:43:20 - train: epoch 0087, iter [03500, 05004], lr: 0.009518, loss: 1.0333
2022-08-03 12:44:19 - train: epoch 0087, iter [03600, 05004], lr: 0.009490, loss: 1.0698
2022-08-03 12:45:17 - train: epoch 0087, iter [03700, 05004], lr: 0.009462, loss: 1.2691
2022-08-03 12:46:15 - train: epoch 0087, iter [03800, 05004], lr: 0.009434, loss: 1.2782
2022-08-03 12:47:14 - train: epoch 0087, iter [03900, 05004], lr: 0.009406, loss: 1.1887
2022-08-03 12:48:12 - train: epoch 0087, iter [04000, 05004], lr: 0.009378, loss: 1.0727
2022-08-03 12:49:09 - train: epoch 0087, iter [04100, 05004], lr: 0.009350, loss: 1.2128
2022-08-03 12:50:08 - train: epoch 0087, iter [04200, 05004], lr: 0.009322, loss: 1.1776
2022-08-03 12:51:05 - train: epoch 0087, iter [04300, 05004], lr: 0.009294, loss: 1.1011
2022-08-03 12:52:03 - train: epoch 0087, iter [04400, 05004], lr: 0.009266, loss: 1.2287
2022-08-03 12:53:02 - train: epoch 0087, iter [04500, 05004], lr: 0.009239, loss: 1.1503
2022-08-03 12:54:01 - train: epoch 0087, iter [04600, 05004], lr: 0.009211, loss: 1.1279
2022-08-03 12:54:59 - train: epoch 0087, iter [04700, 05004], lr: 0.009183, loss: 1.2368
2022-08-03 12:55:57 - train: epoch 0087, iter [04800, 05004], lr: 0.009156, loss: 1.1303
2022-08-03 12:56:54 - train: epoch 0087, iter [04900, 05004], lr: 0.009128, loss: 1.1073
2022-08-03 12:57:50 - train: epoch 0087, iter [05000, 05004], lr: 0.009100, loss: 1.2608
2022-08-03 12:57:51 - train: epoch 087, train_loss: 1.1891
2022-08-03 12:59:56 - eval: epoch: 087, acc1: 71.592%, acc5: 90.600%, test_loss: 1.1424, per_image_load_time: 2.072ms, per_image_inference_time: 0.608ms
2022-08-03 12:59:56 - until epoch: 087, best_acc1: 71.592%
2022-08-03 12:59:56 - epoch 088 lr: 0.009099
2022-08-03 13:01:06 - train: epoch 0088, iter [00100, 05004], lr: 0.009072, loss: 1.0189
2022-08-03 13:02:06 - train: epoch 0088, iter [00200, 05004], lr: 0.009044, loss: 1.0707
2022-08-03 13:03:01 - train: epoch 0088, iter [00300, 05004], lr: 0.009017, loss: 1.2834
2022-08-03 13:04:00 - train: epoch 0088, iter [00400, 05004], lr: 0.008989, loss: 1.2358
2022-08-03 13:04:57 - train: epoch 0088, iter [00500, 05004], lr: 0.008962, loss: 1.1296
2022-08-03 13:05:55 - train: epoch 0088, iter [00600, 05004], lr: 0.008935, loss: 1.1037
2022-08-03 13:06:54 - train: epoch 0088, iter [00700, 05004], lr: 0.008908, loss: 1.1411
2022-08-03 13:07:50 - train: epoch 0088, iter [00800, 05004], lr: 0.008880, loss: 1.1464
2022-08-03 13:08:48 - train: epoch 0088, iter [00900, 05004], lr: 0.008853, loss: 1.0772
2022-08-03 13:09:46 - train: epoch 0088, iter [01000, 05004], lr: 0.008826, loss: 1.2360
2022-08-03 13:10:44 - train: epoch 0088, iter [01100, 05004], lr: 0.008799, loss: 0.9064
2022-08-03 13:11:43 - train: epoch 0088, iter [01200, 05004], lr: 0.008772, loss: 1.2999
2022-08-03 13:12:39 - train: epoch 0088, iter [01300, 05004], lr: 0.008745, loss: 1.0088
2022-08-03 13:13:37 - train: epoch 0088, iter [01400, 05004], lr: 0.008718, loss: 1.0705
2022-08-03 13:14:36 - train: epoch 0088, iter [01500, 05004], lr: 0.008691, loss: 1.2700
2022-08-03 13:15:34 - train: epoch 0088, iter [01600, 05004], lr: 0.008664, loss: 1.0622
2022-08-03 13:16:31 - train: epoch 0088, iter [01700, 05004], lr: 0.008637, loss: 1.3530
2022-08-03 13:17:30 - train: epoch 0088, iter [01800, 05004], lr: 0.008610, loss: 1.1672
2022-08-03 13:18:26 - train: epoch 0088, iter [01900, 05004], lr: 0.008583, loss: 1.2698
2022-08-03 13:19:24 - train: epoch 0088, iter [02000, 05004], lr: 0.008556, loss: 1.0608
2022-08-03 13:20:22 - train: epoch 0088, iter [02100, 05004], lr: 0.008530, loss: 1.2502
2022-08-03 13:21:20 - train: epoch 0088, iter [02200, 05004], lr: 0.008503, loss: 1.2007
2022-08-03 13:22:17 - train: epoch 0088, iter [02300, 05004], lr: 0.008476, loss: 1.1158
2022-08-03 13:23:15 - train: epoch 0088, iter [02400, 05004], lr: 0.008450, loss: 1.3319
2022-08-03 13:24:13 - train: epoch 0088, iter [02500, 05004], lr: 0.008423, loss: 1.2986
2022-08-03 13:25:10 - train: epoch 0088, iter [02600, 05004], lr: 0.008397, loss: 1.0792
2022-08-03 13:26:09 - train: epoch 0088, iter [02700, 05004], lr: 0.008370, loss: 1.1763
2022-08-03 13:27:08 - train: epoch 0088, iter [02800, 05004], lr: 0.008344, loss: 1.2120
2022-08-03 13:28:06 - train: epoch 0088, iter [02900, 05004], lr: 0.008317, loss: 1.0288
2022-08-03 13:29:04 - train: epoch 0088, iter [03000, 05004], lr: 0.008291, loss: 1.3240
2022-08-03 13:30:02 - train: epoch 0088, iter [03100, 05004], lr: 0.008265, loss: 1.0879
2022-08-03 13:31:01 - train: epoch 0088, iter [03200, 05004], lr: 0.008238, loss: 0.9917
2022-08-03 13:31:58 - train: epoch 0088, iter [03300, 05004], lr: 0.008212, loss: 1.2465
2022-08-03 13:32:57 - train: epoch 0088, iter [03400, 05004], lr: 0.008186, loss: 0.9690
2022-08-03 13:33:55 - train: epoch 0088, iter [03500, 05004], lr: 0.008160, loss: 1.1496
2022-08-03 13:34:52 - train: epoch 0088, iter [03600, 05004], lr: 0.008134, loss: 1.0953
2022-08-03 13:35:52 - train: epoch 0088, iter [03700, 05004], lr: 0.008108, loss: 1.3301
2022-08-03 13:36:49 - train: epoch 0088, iter [03800, 05004], lr: 0.008081, loss: 1.2352
2022-08-03 13:37:47 - train: epoch 0088, iter [03900, 05004], lr: 0.008055, loss: 1.2421
2022-08-03 13:38:45 - train: epoch 0088, iter [04000, 05004], lr: 0.008029, loss: 1.0458
2022-08-03 13:39:43 - train: epoch 0088, iter [04100, 05004], lr: 0.008004, loss: 0.9882
2022-08-03 13:40:40 - train: epoch 0088, iter [04200, 05004], lr: 0.007978, loss: 1.1928
2022-08-03 13:41:37 - train: epoch 0088, iter [04300, 05004], lr: 0.007952, loss: 1.1932
2022-08-03 13:42:35 - train: epoch 0088, iter [04400, 05004], lr: 0.007926, loss: 1.1052
2022-08-03 13:43:34 - train: epoch 0088, iter [04500, 05004], lr: 0.007900, loss: 1.1619
2022-08-03 13:44:33 - train: epoch 0088, iter [04600, 05004], lr: 0.007875, loss: 1.2148
2022-08-03 13:45:30 - train: epoch 0088, iter [04700, 05004], lr: 0.007849, loss: 1.2372
2022-08-03 13:46:28 - train: epoch 0088, iter [04800, 05004], lr: 0.007823, loss: 1.1431
2022-08-03 13:47:26 - train: epoch 0088, iter [04900, 05004], lr: 0.007798, loss: 1.0320
2022-08-03 13:48:22 - train: epoch 0088, iter [05000, 05004], lr: 0.007772, loss: 1.1257
2022-08-03 13:48:23 - train: epoch 088, train_loss: 1.1634
2022-08-03 13:50:29 - eval: epoch: 088, acc1: 72.062%, acc5: 90.740%, test_loss: 1.1316, per_image_load_time: 2.631ms, per_image_inference_time: 0.661ms
2022-08-03 13:50:30 - until epoch: 088, best_acc1: 72.062%
2022-08-03 13:50:30 - epoch 089 lr: 0.007771
2022-08-03 13:51:36 - train: epoch 0089, iter [00100, 05004], lr: 0.007746, loss: 1.3268
2022-08-03 13:52:35 - train: epoch 0089, iter [00200, 05004], lr: 0.007720, loss: 0.9343
2022-08-03 13:53:32 - train: epoch 0089, iter [00300, 05004], lr: 0.007695, loss: 1.2122
2022-08-03 13:54:30 - train: epoch 0089, iter [00400, 05004], lr: 0.007669, loss: 1.0547
2022-08-03 13:55:27 - train: epoch 0089, iter [00500, 05004], lr: 0.007644, loss: 1.1479
2022-08-03 13:56:24 - train: epoch 0089, iter [00600, 05004], lr: 0.007618, loss: 1.1150
2022-08-03 13:57:23 - train: epoch 0089, iter [00700, 05004], lr: 0.007593, loss: 1.2238
2022-08-03 13:58:22 - train: epoch 0089, iter [00800, 05004], lr: 0.007568, loss: 1.2636
2022-08-03 13:59:18 - train: epoch 0089, iter [00900, 05004], lr: 0.007543, loss: 1.0987
2022-08-03 14:00:17 - train: epoch 0089, iter [01000, 05004], lr: 0.007518, loss: 1.1862
2022-08-03 14:01:14 - train: epoch 0089, iter [01100, 05004], lr: 0.007493, loss: 0.9939
2022-08-03 14:02:14 - train: epoch 0089, iter [01200, 05004], lr: 0.007467, loss: 1.1955
2022-08-03 14:03:11 - train: epoch 0089, iter [01300, 05004], lr: 0.007442, loss: 0.8848
2022-08-03 14:04:11 - train: epoch 0089, iter [01400, 05004], lr: 0.007417, loss: 1.3016
2022-08-03 14:05:09 - train: epoch 0089, iter [01500, 05004], lr: 0.007392, loss: 1.3718
2022-08-03 14:06:07 - train: epoch 0089, iter [01600, 05004], lr: 0.007368, loss: 0.9956
2022-08-03 14:07:06 - train: epoch 0089, iter [01700, 05004], lr: 0.007343, loss: 1.1435
2022-08-03 14:08:05 - train: epoch 0089, iter [01800, 05004], lr: 0.007318, loss: 1.2182
2022-08-03 14:09:03 - train: epoch 0089, iter [01900, 05004], lr: 0.007293, loss: 1.1091
2022-08-03 14:10:02 - train: epoch 0089, iter [02000, 05004], lr: 0.007268, loss: 1.1042
2022-08-03 14:10:58 - train: epoch 0089, iter [02100, 05004], lr: 0.007244, loss: 1.1660
2022-08-03 14:11:56 - train: epoch 0089, iter [02200, 05004], lr: 0.007219, loss: 1.2317
2022-08-03 14:12:55 - train: epoch 0089, iter [02300, 05004], lr: 0.007194, loss: 1.1692
2022-08-03 14:13:53 - train: epoch 0089, iter [02400, 05004], lr: 0.007170, loss: 1.4867
2022-08-03 14:14:50 - train: epoch 0089, iter [02500, 05004], lr: 0.007145, loss: 1.3457
2022-08-03 14:15:47 - train: epoch 0089, iter [02600, 05004], lr: 0.007121, loss: 0.9646
2022-08-03 14:16:45 - train: epoch 0089, iter [02700, 05004], lr: 0.007096, loss: 1.0420
2022-08-03 14:17:44 - train: epoch 0089, iter [02800, 05004], lr: 0.007072, loss: 1.2566
2022-08-03 14:18:42 - train: epoch 0089, iter [02900, 05004], lr: 0.007047, loss: 1.1474
2022-08-03 14:19:40 - train: epoch 0089, iter [03000, 05004], lr: 0.007023, loss: 1.0449
2022-08-03 14:20:38 - train: epoch 0089, iter [03100, 05004], lr: 0.006999, loss: 1.0723
2022-08-03 14:21:36 - train: epoch 0089, iter [03200, 05004], lr: 0.006974, loss: 1.3187
2022-08-03 14:22:32 - train: epoch 0089, iter [03300, 05004], lr: 0.006950, loss: 1.2335
2022-08-03 14:23:31 - train: epoch 0089, iter [03400, 05004], lr: 0.006926, loss: 1.1280
2022-08-03 14:24:28 - train: epoch 0089, iter [03500, 05004], lr: 0.006902, loss: 0.9209
2022-08-03 14:25:26 - train: epoch 0089, iter [03600, 05004], lr: 0.006878, loss: 1.1042
2022-08-03 14:26:24 - train: epoch 0089, iter [03700, 05004], lr: 0.006854, loss: 1.1692
2022-08-03 14:27:23 - train: epoch 0089, iter [03800, 05004], lr: 0.006830, loss: 0.9971
2022-08-03 14:28:19 - train: epoch 0089, iter [03900, 05004], lr: 0.006806, loss: 1.3956
2022-08-03 14:29:17 - train: epoch 0089, iter [04000, 05004], lr: 0.006782, loss: 1.1152
2022-08-03 14:30:16 - train: epoch 0089, iter [04100, 05004], lr: 0.006758, loss: 1.3033
2022-08-03 14:31:14 - train: epoch 0089, iter [04200, 05004], lr: 0.006734, loss: 1.1441
2022-08-03 14:32:13 - train: epoch 0089, iter [04300, 05004], lr: 0.006710, loss: 1.0446
2022-08-03 14:33:10 - train: epoch 0089, iter [04400, 05004], lr: 0.006686, loss: 1.2170
2022-08-03 14:34:08 - train: epoch 0089, iter [04500, 05004], lr: 0.006663, loss: 0.8356
2022-08-03 14:35:07 - train: epoch 0089, iter [04600, 05004], lr: 0.006639, loss: 1.1723
2022-08-03 14:36:06 - train: epoch 0089, iter [04700, 05004], lr: 0.006615, loss: 1.1041
2022-08-03 14:37:04 - train: epoch 0089, iter [04800, 05004], lr: 0.006592, loss: 1.1114
2022-08-03 14:38:02 - train: epoch 0089, iter [04900, 05004], lr: 0.006568, loss: 1.2273
2022-08-03 14:38:58 - train: epoch 0089, iter [05000, 05004], lr: 0.006544, loss: 0.8898
2022-08-03 14:38:59 - train: epoch 089, train_loss: 1.1397
2022-08-03 14:41:07 - eval: epoch: 089, acc1: 72.142%, acc5: 91.004%, test_loss: 1.1221, per_image_load_time: 3.755ms, per_image_inference_time: 0.641ms
2022-08-03 14:41:07 - until epoch: 089, best_acc1: 72.142%
2022-08-03 14:41:07 - epoch 090 lr: 0.006543
2022-08-03 14:42:15 - train: epoch 0090, iter [00100, 05004], lr: 0.006520, loss: 1.2512
2022-08-03 14:43:14 - train: epoch 0090, iter [00200, 05004], lr: 0.006497, loss: 1.1589
2022-08-03 14:44:11 - train: epoch 0090, iter [00300, 05004], lr: 0.006473, loss: 0.8113
2022-08-03 14:45:10 - train: epoch 0090, iter [00400, 05004], lr: 0.006450, loss: 1.0850
2022-08-03 14:46:06 - train: epoch 0090, iter [00500, 05004], lr: 0.006426, loss: 1.1493
2022-08-03 14:47:03 - train: epoch 0090, iter [00600, 05004], lr: 0.006403, loss: 1.3678
2022-08-03 14:48:00 - train: epoch 0090, iter [00700, 05004], lr: 0.006380, loss: 1.0732
2022-08-03 14:48:58 - train: epoch 0090, iter [00800, 05004], lr: 0.006357, loss: 1.1759
2022-08-03 14:49:55 - train: epoch 0090, iter [00900, 05004], lr: 0.006334, loss: 1.1238
2022-08-03 14:50:53 - train: epoch 0090, iter [01000, 05004], lr: 0.006310, loss: 1.0049
2022-08-03 14:51:52 - train: epoch 0090, iter [01100, 05004], lr: 0.006287, loss: 0.9322
2022-08-03 14:52:49 - train: epoch 0090, iter [01200, 05004], lr: 0.006264, loss: 1.1042
2022-08-03 14:53:46 - train: epoch 0090, iter [01300, 05004], lr: 0.006241, loss: 1.1200
2022-08-03 14:54:44 - train: epoch 0090, iter [01400, 05004], lr: 0.006218, loss: 1.2037
2022-08-03 14:55:43 - train: epoch 0090, iter [01500, 05004], lr: 0.006195, loss: 1.2520
2022-08-03 14:56:42 - train: epoch 0090, iter [01600, 05004], lr: 0.006173, loss: 1.0613
2022-08-03 14:57:39 - train: epoch 0090, iter [01700, 05004], lr: 0.006150, loss: 1.0889
2022-08-03 14:58:37 - train: epoch 0090, iter [01800, 05004], lr: 0.006127, loss: 1.1983
2022-08-03 14:59:35 - train: epoch 0090, iter [01900, 05004], lr: 0.006104, loss: 0.9271
2022-08-03 15:00:35 - train: epoch 0090, iter [02000, 05004], lr: 0.006081, loss: 1.1405
2022-08-03 15:01:32 - train: epoch 0090, iter [02100, 05004], lr: 0.006059, loss: 1.3051
2022-08-03 15:02:30 - train: epoch 0090, iter [02200, 05004], lr: 0.006036, loss: 1.2141
2022-08-03 15:03:28 - train: epoch 0090, iter [02300, 05004], lr: 0.006014, loss: 1.4312
2022-08-03 15:04:26 - train: epoch 0090, iter [02400, 05004], lr: 0.005991, loss: 1.2469
2022-08-03 15:05:25 - train: epoch 0090, iter [02500, 05004], lr: 0.005969, loss: 1.0658
2022-08-03 15:06:22 - train: epoch 0090, iter [02600, 05004], lr: 0.005946, loss: 1.1766
2022-08-03 15:07:20 - train: epoch 0090, iter [02700, 05004], lr: 0.005924, loss: 1.0888
2022-08-03 15:08:18 - train: epoch 0090, iter [02800, 05004], lr: 0.005901, loss: 1.3107
2022-08-03 15:09:17 - train: epoch 0090, iter [02900, 05004], lr: 0.005879, loss: 1.1656
2022-08-03 15:10:14 - train: epoch 0090, iter [03000, 05004], lr: 0.005857, loss: 1.0872
2022-08-03 15:11:14 - train: epoch 0090, iter [03100, 05004], lr: 0.005834, loss: 1.0490
2022-08-03 15:12:11 - train: epoch 0090, iter [03200, 05004], lr: 0.005812, loss: 1.1002
2022-08-03 15:13:09 - train: epoch 0090, iter [03300, 05004], lr: 0.005790, loss: 1.2363
2022-08-03 15:14:08 - train: epoch 0090, iter [03400, 05004], lr: 0.005768, loss: 1.1198
2022-08-03 15:15:05 - train: epoch 0090, iter [03500, 05004], lr: 0.005746, loss: 1.1620
2022-08-03 15:16:05 - train: epoch 0090, iter [03600, 05004], lr: 0.005724, loss: 1.1577
2022-08-03 15:17:02 - train: epoch 0090, iter [03700, 05004], lr: 0.005702, loss: 1.1726
2022-08-03 15:17:59 - train: epoch 0090, iter [03800, 05004], lr: 0.005680, loss: 1.1516
2022-08-03 15:18:58 - train: epoch 0090, iter [03900, 05004], lr: 0.005658, loss: 1.1467
2022-08-03 15:19:57 - train: epoch 0090, iter [04000, 05004], lr: 0.005636, loss: 1.3243
2022-08-03 15:20:55 - train: epoch 0090, iter [04100, 05004], lr: 0.005614, loss: 1.2150
2022-08-03 15:21:52 - train: epoch 0090, iter [04200, 05004], lr: 0.005592, loss: 1.1291
2022-08-03 15:22:50 - train: epoch 0090, iter [04300, 05004], lr: 0.005570, loss: 1.1724
2022-08-03 15:23:50 - train: epoch 0090, iter [04400, 05004], lr: 0.005549, loss: 1.1842
2022-08-03 15:24:46 - train: epoch 0090, iter [04500, 05004], lr: 0.005527, loss: 1.0252
2022-08-03 15:25:45 - train: epoch 0090, iter [04600, 05004], lr: 0.005505, loss: 1.1354
2022-08-03 15:26:44 - train: epoch 0090, iter [04700, 05004], lr: 0.005484, loss: 1.0583
2022-08-03 15:27:42 - train: epoch 0090, iter [04800, 05004], lr: 0.005462, loss: 1.3543
2022-08-03 15:28:40 - train: epoch 0090, iter [04900, 05004], lr: 0.005441, loss: 1.1101
2022-08-03 15:29:35 - train: epoch 0090, iter [05000, 05004], lr: 0.005419, loss: 1.0728
2022-08-03 15:29:36 - train: epoch 090, train_loss: 1.1158
2022-08-03 15:31:45 - eval: epoch: 090, acc1: 72.620%, acc5: 91.114%, test_loss: 1.1096, per_image_load_time: 2.557ms, per_image_inference_time: 0.650ms
2022-08-03 15:31:45 - until epoch: 090, best_acc1: 72.620%
2022-08-03 15:31:45 - epoch 091 lr: 0.005418
2022-08-03 15:32:51 - train: epoch 0091, iter [00100, 05004], lr: 0.005397, loss: 0.9556
2022-08-03 15:33:51 - train: epoch 0091, iter [00200, 05004], lr: 0.005375, loss: 1.2447
2022-08-03 15:34:49 - train: epoch 0091, iter [00300, 05004], lr: 0.005354, loss: 1.0099
2022-08-03 15:35:46 - train: epoch 0091, iter [00400, 05004], lr: 0.005333, loss: 1.1520
2022-08-03 15:36:44 - train: epoch 0091, iter [00500, 05004], lr: 0.005312, loss: 1.1001
2022-08-03 15:37:41 - train: epoch 0091, iter [00600, 05004], lr: 0.005290, loss: 0.9885
2022-08-03 15:38:40 - train: epoch 0091, iter [00700, 05004], lr: 0.005269, loss: 1.3394
2022-08-03 15:39:38 - train: epoch 0091, iter [00800, 05004], lr: 0.005248, loss: 0.9392
2022-08-03 15:40:37 - train: epoch 0091, iter [00900, 05004], lr: 0.005227, loss: 1.1656
2022-08-03 15:41:34 - train: epoch 0091, iter [01000, 05004], lr: 0.005206, loss: 0.8969
2022-08-03 15:42:33 - train: epoch 0091, iter [01100, 05004], lr: 0.005185, loss: 0.8571
2022-08-03 15:43:31 - train: epoch 0091, iter [01200, 05004], lr: 0.005164, loss: 1.1582
2022-08-03 15:44:30 - train: epoch 0091, iter [01300, 05004], lr: 0.005143, loss: 1.1169
2022-08-03 15:45:28 - train: epoch 0091, iter [01400, 05004], lr: 0.005122, loss: 1.0138
2022-08-03 15:46:26 - train: epoch 0091, iter [01500, 05004], lr: 0.005101, loss: 1.0520
2022-08-03 15:47:24 - train: epoch 0091, iter [01600, 05004], lr: 0.005080, loss: 1.1174
2022-08-03 15:48:22 - train: epoch 0091, iter [01700, 05004], lr: 0.005059, loss: 1.2435
2022-08-03 15:49:21 - train: epoch 0091, iter [01800, 05004], lr: 0.005039, loss: 1.2633
2022-08-03 15:50:19 - train: epoch 0091, iter [01900, 05004], lr: 0.005018, loss: 1.2551
2022-08-03 15:51:17 - train: epoch 0091, iter [02000, 05004], lr: 0.004997, loss: 0.9239
2022-08-03 15:52:14 - train: epoch 0091, iter [02100, 05004], lr: 0.004977, loss: 1.0269
2022-08-03 15:53:14 - train: epoch 0091, iter [02200, 05004], lr: 0.004956, loss: 1.1755
2022-08-03 15:54:12 - train: epoch 0091, iter [02300, 05004], lr: 0.004936, loss: 1.1029
2022-08-03 15:55:11 - train: epoch 0091, iter [02400, 05004], lr: 0.004915, loss: 1.0036
2022-08-03 15:56:09 - train: epoch 0091, iter [02500, 05004], lr: 0.004895, loss: 1.0568
2022-08-03 15:57:08 - train: epoch 0091, iter [02600, 05004], lr: 0.004874, loss: 0.9310
2022-08-03 15:58:06 - train: epoch 0091, iter [02700, 05004], lr: 0.004854, loss: 0.8991
2022-08-03 15:59:02 - train: epoch 0091, iter [02800, 05004], lr: 0.004834, loss: 1.0939
2022-08-03 16:00:00 - train: epoch 0091, iter [02900, 05004], lr: 0.004813, loss: 1.1268
2022-08-03 16:00:59 - train: epoch 0091, iter [03000, 05004], lr: 0.004793, loss: 1.1611
2022-08-03 16:01:57 - train: epoch 0091, iter [03100, 05004], lr: 0.004773, loss: 1.0359
2022-08-03 16:02:55 - train: epoch 0091, iter [03200, 05004], lr: 0.004753, loss: 1.1641
2022-08-03 16:03:52 - train: epoch 0091, iter [03300, 05004], lr: 0.004733, loss: 1.2037
2022-08-03 16:04:52 - train: epoch 0091, iter [03400, 05004], lr: 0.004713, loss: 1.0430
2022-08-03 16:05:50 - train: epoch 0091, iter [03500, 05004], lr: 0.004693, loss: 1.0101
2022-08-03 16:06:48 - train: epoch 0091, iter [03600, 05004], lr: 0.004673, loss: 1.2662
2022-08-03 16:07:46 - train: epoch 0091, iter [03700, 05004], lr: 0.004653, loss: 1.1502
2022-08-03 16:08:44 - train: epoch 0091, iter [03800, 05004], lr: 0.004633, loss: 1.1136
2022-08-03 16:09:43 - train: epoch 0091, iter [03900, 05004], lr: 0.004613, loss: 0.9475
2022-08-03 16:10:39 - train: epoch 0091, iter [04000, 05004], lr: 0.004593, loss: 1.0525
2022-08-03 16:11:36 - train: epoch 0091, iter [04100, 05004], lr: 0.004573, loss: 0.9350
2022-08-03 16:12:31 - train: epoch 0091, iter [04200, 05004], lr: 0.004554, loss: 1.0868
2022-08-03 16:13:31 - train: epoch 0091, iter [04300, 05004], lr: 0.004534, loss: 1.0619
2022-08-03 16:14:27 - train: epoch 0091, iter [04400, 05004], lr: 0.004514, loss: 0.9157
2022-08-03 16:15:27 - train: epoch 0091, iter [04500, 05004], lr: 0.004495, loss: 1.0344
2022-08-03 16:16:25 - train: epoch 0091, iter [04600, 05004], lr: 0.004475, loss: 1.0370
2022-08-03 16:17:21 - train: epoch 0091, iter [04700, 05004], lr: 0.004456, loss: 1.0512
2022-08-03 16:18:20 - train: epoch 0091, iter [04800, 05004], lr: 0.004436, loss: 1.1195
2022-08-03 16:19:17 - train: epoch 0091, iter [04900, 05004], lr: 0.004417, loss: 0.9775
2022-08-03 16:20:11 - train: epoch 0091, iter [05000, 05004], lr: 0.004397, loss: 1.2053
2022-08-03 16:20:12 - train: epoch 091, train_loss: 1.0919
2022-08-03 16:22:19 - eval: epoch: 091, acc1: 72.582%, acc5: 91.244%, test_loss: 1.1064, per_image_load_time: 3.655ms, per_image_inference_time: 0.641ms
2022-08-03 16:22:19 - until epoch: 091, best_acc1: 72.620%
2022-08-03 16:22:19 - epoch 092 lr: 0.004396
2022-08-03 16:23:25 - train: epoch 0092, iter [00100, 05004], lr: 0.004377, loss: 0.9651
2022-08-03 16:24:22 - train: epoch 0092, iter [00200, 05004], lr: 0.004358, loss: 1.0281
2022-08-03 16:25:19 - train: epoch 0092, iter [00300, 05004], lr: 0.004338, loss: 1.2380
2022-08-03 16:26:15 - train: epoch 0092, iter [00400, 05004], lr: 0.004319, loss: 0.9624
2022-08-03 16:27:11 - train: epoch 0092, iter [00500, 05004], lr: 0.004300, loss: 1.0771
2022-08-03 16:28:09 - train: epoch 0092, iter [00600, 05004], lr: 0.004281, loss: 1.1721
2022-08-03 16:29:05 - train: epoch 0092, iter [00700, 05004], lr: 0.004262, loss: 0.9682
2022-08-03 16:30:03 - train: epoch 0092, iter [00800, 05004], lr: 0.004243, loss: 1.1984
2022-08-03 16:31:00 - train: epoch 0092, iter [00900, 05004], lr: 0.004224, loss: 1.1787
2022-08-03 16:31:57 - train: epoch 0092, iter [01000, 05004], lr: 0.004205, loss: 0.9585
2022-08-03 16:32:55 - train: epoch 0092, iter [01100, 05004], lr: 0.004186, loss: 1.0280
2022-08-03 16:33:52 - train: epoch 0092, iter [01200, 05004], lr: 0.004167, loss: 1.1060
2022-08-03 16:34:50 - train: epoch 0092, iter [01300, 05004], lr: 0.004148, loss: 0.9126
2022-08-03 16:35:48 - train: epoch 0092, iter [01400, 05004], lr: 0.004129, loss: 0.9366
2022-08-03 16:36:48 - train: epoch 0092, iter [01500, 05004], lr: 0.004110, loss: 0.8694
2022-08-03 16:37:46 - train: epoch 0092, iter [01600, 05004], lr: 0.004092, loss: 1.2374
2022-08-03 16:38:43 - train: epoch 0092, iter [01700, 05004], lr: 0.004073, loss: 1.0932
2022-08-03 16:39:40 - train: epoch 0092, iter [01800, 05004], lr: 0.004054, loss: 0.9623
2022-08-03 16:40:38 - train: epoch 0092, iter [01900, 05004], lr: 0.004036, loss: 1.1326
2022-08-03 16:41:38 - train: epoch 0092, iter [02000, 05004], lr: 0.004017, loss: 1.0046
2022-08-03 16:42:34 - train: epoch 0092, iter [02100, 05004], lr: 0.003999, loss: 1.0403
2022-08-03 16:43:33 - train: epoch 0092, iter [02200, 05004], lr: 0.003980, loss: 1.0699
2022-08-03 16:44:31 - train: epoch 0092, iter [02300, 05004], lr: 0.003962, loss: 1.1151
2022-08-03 16:45:29 - train: epoch 0092, iter [02400, 05004], lr: 0.003943, loss: 0.8939
2022-08-03 16:46:27 - train: epoch 0092, iter [02500, 05004], lr: 0.003925, loss: 1.0359
2022-08-03 16:47:24 - train: epoch 0092, iter [02600, 05004], lr: 0.003907, loss: 1.1373
2022-08-03 16:48:22 - train: epoch 0092, iter [02700, 05004], lr: 0.003888, loss: 1.2241
2022-08-03 16:49:20 - train: epoch 0092, iter [02800, 05004], lr: 0.003870, loss: 1.0440
2022-08-03 16:50:18 - train: epoch 0092, iter [02900, 05004], lr: 0.003852, loss: 1.1690
2022-08-03 16:51:15 - train: epoch 0092, iter [03000, 05004], lr: 0.003834, loss: 1.1276
2022-08-03 16:52:14 - train: epoch 0092, iter [03100, 05004], lr: 0.003816, loss: 1.0809
2022-08-03 16:53:13 - train: epoch 0092, iter [03200, 05004], lr: 0.003798, loss: 1.0566
2022-08-03 16:54:12 - train: epoch 0092, iter [03300, 05004], lr: 0.003780, loss: 1.2350
2022-08-03 16:55:11 - train: epoch 0092, iter [03400, 05004], lr: 0.003762, loss: 1.1512
2022-08-03 16:56:07 - train: epoch 0092, iter [03500, 05004], lr: 0.003744, loss: 0.9419
2022-08-03 16:57:04 - train: epoch 0092, iter [03600, 05004], lr: 0.003726, loss: 1.1507
2022-08-03 16:58:03 - train: epoch 0092, iter [03700, 05004], lr: 0.003708, loss: 0.9147
2022-08-03 16:59:01 - train: epoch 0092, iter [03800, 05004], lr: 0.003690, loss: 1.1693
2022-08-03 17:00:00 - train: epoch 0092, iter [03900, 05004], lr: 0.003672, loss: 1.0998
2022-08-03 17:00:58 - train: epoch 0092, iter [04000, 05004], lr: 0.003655, loss: 1.0233
2022-08-03 17:01:55 - train: epoch 0092, iter [04100, 05004], lr: 0.003637, loss: 0.9125
2022-08-03 17:02:54 - train: epoch 0092, iter [04200, 05004], lr: 0.003619, loss: 1.2130
2022-08-03 17:03:51 - train: epoch 0092, iter [04300, 05004], lr: 0.003602, loss: 1.0464
2022-08-03 17:04:49 - train: epoch 0092, iter [04400, 05004], lr: 0.003584, loss: 1.1279
2022-08-03 17:05:47 - train: epoch 0092, iter [04500, 05004], lr: 0.003567, loss: 1.0467
2022-08-03 17:06:47 - train: epoch 0092, iter [04600, 05004], lr: 0.003549, loss: 1.0270
2022-08-03 17:07:44 - train: epoch 0092, iter [04700, 05004], lr: 0.003532, loss: 0.7542
2022-08-03 17:08:41 - train: epoch 0092, iter [04800, 05004], lr: 0.003514, loss: 1.1753
2022-08-03 17:09:38 - train: epoch 0092, iter [04900, 05004], lr: 0.003497, loss: 1.0187
2022-08-03 17:10:34 - train: epoch 0092, iter [05000, 05004], lr: 0.003480, loss: 1.0813
2022-08-03 17:10:36 - train: epoch 092, train_loss: 1.0709
2022-08-03 17:12:43 - eval: epoch: 092, acc1: 72.756%, acc5: 91.316%, test_loss: 1.0989, per_image_load_time: 2.762ms, per_image_inference_time: 0.614ms
2022-08-03 17:12:44 - until epoch: 092, best_acc1: 72.756%
2022-08-03 17:12:44 - epoch 093 lr: 0.003479
2022-08-03 17:13:50 - train: epoch 0093, iter [00100, 05004], lr: 0.003462, loss: 1.0934
2022-08-03 17:14:49 - train: epoch 0093, iter [00200, 05004], lr: 0.003445, loss: 0.9288
2022-08-03 17:15:47 - train: epoch 0093, iter [00300, 05004], lr: 0.003427, loss: 1.0040
2022-08-03 17:16:45 - train: epoch 0093, iter [00400, 05004], lr: 0.003410, loss: 1.0807
2022-08-03 17:17:41 - train: epoch 0093, iter [00500, 05004], lr: 0.003393, loss: 1.1840
2022-08-03 17:18:40 - train: epoch 0093, iter [00600, 05004], lr: 0.003376, loss: 0.9360
2022-08-03 17:19:39 - train: epoch 0093, iter [00700, 05004], lr: 0.003359, loss: 1.0872
2022-08-03 17:20:35 - train: epoch 0093, iter [00800, 05004], lr: 0.003342, loss: 1.1046
2022-08-03 17:21:34 - train: epoch 0093, iter [00900, 05004], lr: 0.003325, loss: 1.2260
2022-08-03 17:22:33 - train: epoch 0093, iter [01000, 05004], lr: 0.003308, loss: 0.8646
2022-08-03 17:23:30 - train: epoch 0093, iter [01100, 05004], lr: 0.003292, loss: 0.9934
2022-08-03 17:24:29 - train: epoch 0093, iter [01200, 05004], lr: 0.003275, loss: 0.9561
2022-08-03 17:25:26 - train: epoch 0093, iter [01300, 05004], lr: 0.003258, loss: 1.0285
2022-08-03 17:26:27 - train: epoch 0093, iter [01400, 05004], lr: 0.003241, loss: 1.0701
2022-08-03 17:27:24 - train: epoch 0093, iter [01500, 05004], lr: 0.003225, loss: 1.3514
2022-08-03 17:28:23 - train: epoch 0093, iter [01600, 05004], lr: 0.003208, loss: 0.9906
2022-08-03 17:29:21 - train: epoch 0093, iter [01700, 05004], lr: 0.003191, loss: 0.9626
2022-08-03 17:30:19 - train: epoch 0093, iter [01800, 05004], lr: 0.003175, loss: 0.9999
2022-08-03 17:31:15 - train: epoch 0093, iter [01900, 05004], lr: 0.003158, loss: 1.0680
2022-08-03 17:32:15 - train: epoch 0093, iter [02000, 05004], lr: 0.003142, loss: 0.9418
2022-08-03 17:33:13 - train: epoch 0093, iter [02100, 05004], lr: 0.003126, loss: 0.8684
2022-08-03 17:34:11 - train: epoch 0093, iter [02200, 05004], lr: 0.003109, loss: 1.0386
2022-08-03 17:35:09 - train: epoch 0093, iter [02300, 05004], lr: 0.003093, loss: 0.9926
2022-08-03 17:36:07 - train: epoch 0093, iter [02400, 05004], lr: 0.003077, loss: 0.9910
2022-08-03 17:37:04 - train: epoch 0093, iter [02500, 05004], lr: 0.003060, loss: 1.0946
2022-08-03 17:38:02 - train: epoch 0093, iter [02600, 05004], lr: 0.003044, loss: 1.3735
2022-08-03 17:38:59 - train: epoch 0093, iter [02700, 05004], lr: 0.003028, loss: 0.9198
2022-08-03 17:39:58 - train: epoch 0093, iter [02800, 05004], lr: 0.003012, loss: 1.2958
2022-08-03 17:40:53 - train: epoch 0093, iter [02900, 05004], lr: 0.002996, loss: 1.0171
2022-08-03 17:41:52 - train: epoch 0093, iter [03000, 05004], lr: 0.002980, loss: 0.9645
2022-08-03 17:42:49 - train: epoch 0093, iter [03100, 05004], lr: 0.002964, loss: 0.9971
2022-08-03 17:43:48 - train: epoch 0093, iter [03200, 05004], lr: 0.002948, loss: 0.9118
2022-08-03 17:44:44 - train: epoch 0093, iter [03300, 05004], lr: 0.002932, loss: 1.0411
2022-08-03 17:45:44 - train: epoch 0093, iter [03400, 05004], lr: 0.002916, loss: 1.1784
2022-08-03 17:46:39 - train: epoch 0093, iter [03500, 05004], lr: 0.002900, loss: 1.0689
2022-08-03 17:47:38 - train: epoch 0093, iter [03600, 05004], lr: 0.002884, loss: 1.1951
2022-08-03 17:48:34 - train: epoch 0093, iter [03700, 05004], lr: 0.002869, loss: 1.0749
2022-08-03 17:49:31 - train: epoch 0093, iter [03800, 05004], lr: 0.002853, loss: 1.0050
2022-08-03 17:50:30 - train: epoch 0093, iter [03900, 05004], lr: 0.002837, loss: 0.9449
2022-08-03 17:51:26 - train: epoch 0093, iter [04000, 05004], lr: 0.002822, loss: 1.1962
2022-08-03 17:52:23 - train: epoch 0093, iter [04100, 05004], lr: 0.002806, loss: 0.9963
2022-08-03 17:53:21 - train: epoch 0093, iter [04200, 05004], lr: 0.002791, loss: 1.0072
2022-08-03 17:54:18 - train: epoch 0093, iter [04300, 05004], lr: 0.002775, loss: 1.1861
2022-08-03 17:55:16 - train: epoch 0093, iter [04400, 05004], lr: 0.002760, loss: 0.8912
2022-08-03 17:56:15 - train: epoch 0093, iter [04500, 05004], lr: 0.002744, loss: 1.2922
2022-08-03 17:57:13 - train: epoch 0093, iter [04600, 05004], lr: 0.002729, loss: 0.8033
2022-08-03 17:58:12 - train: epoch 0093, iter [04700, 05004], lr: 0.002714, loss: 1.3410
2022-08-03 17:59:10 - train: epoch 0093, iter [04800, 05004], lr: 0.002698, loss: 1.0846
2022-08-03 18:00:07 - train: epoch 0093, iter [04900, 05004], lr: 0.002683, loss: 1.1504
2022-08-03 18:01:01 - train: epoch 0093, iter [05000, 05004], lr: 0.002668, loss: 1.0826
2022-08-03 18:01:02 - train: epoch 093, train_loss: 1.0508
2022-08-03 18:03:11 - eval: epoch: 093, acc1: 72.944%, acc5: 91.340%, test_loss: 1.0908, per_image_load_time: 3.850ms, per_image_inference_time: 0.625ms
2022-08-03 18:03:11 - until epoch: 093, best_acc1: 72.944%
2022-08-03 18:03:11 - epoch 094 lr: 0.002667
2022-08-03 18:04:19 - train: epoch 0094, iter [00100, 05004], lr: 0.002652, loss: 0.9005
2022-08-03 18:05:17 - train: epoch 0094, iter [00200, 05004], lr: 0.002637, loss: 1.0535
2022-08-03 18:06:15 - train: epoch 0094, iter [00300, 05004], lr: 0.002622, loss: 1.0167
2022-08-03 18:07:14 - train: epoch 0094, iter [00400, 05004], lr: 0.002607, loss: 1.0807
2022-08-03 18:08:10 - train: epoch 0094, iter [00500, 05004], lr: 0.002592, loss: 0.9698
2022-08-03 18:09:06 - train: epoch 0094, iter [00600, 05004], lr: 0.002577, loss: 0.8920
2022-08-03 18:10:03 - train: epoch 0094, iter [00700, 05004], lr: 0.002562, loss: 1.0856
2022-08-03 18:11:00 - train: epoch 0094, iter [00800, 05004], lr: 0.002547, loss: 0.9546
2022-08-03 18:11:57 - train: epoch 0094, iter [00900, 05004], lr: 0.002533, loss: 1.1482
2022-08-03 18:12:56 - train: epoch 0094, iter [01000, 05004], lr: 0.002518, loss: 1.0078
2022-08-03 18:13:52 - train: epoch 0094, iter [01100, 05004], lr: 0.002503, loss: 1.1579
2022-08-03 18:14:49 - train: epoch 0094, iter [01200, 05004], lr: 0.002488, loss: 0.9721
2022-08-03 18:15:49 - train: epoch 0094, iter [01300, 05004], lr: 0.002474, loss: 1.1583
2022-08-03 18:16:46 - train: epoch 0094, iter [01400, 05004], lr: 0.002459, loss: 1.0876
2022-08-03 18:17:44 - train: epoch 0094, iter [01500, 05004], lr: 0.002445, loss: 0.9751
2022-08-03 18:18:43 - train: epoch 0094, iter [01600, 05004], lr: 0.002430, loss: 1.3147
2022-08-03 18:19:41 - train: epoch 0094, iter [01700, 05004], lr: 0.002416, loss: 1.0221
2022-08-03 18:20:38 - train: epoch 0094, iter [01800, 05004], lr: 0.002401, loss: 0.9679
2022-08-03 18:21:36 - train: epoch 0094, iter [01900, 05004], lr: 0.002387, loss: 1.1561
2022-08-03 18:22:34 - train: epoch 0094, iter [02000, 05004], lr: 0.002373, loss: 0.8919
2022-08-03 18:23:34 - train: epoch 0094, iter [02100, 05004], lr: 0.002358, loss: 0.7988
2022-08-03 18:24:31 - train: epoch 0094, iter [02200, 05004], lr: 0.002344, loss: 1.0442
2022-08-03 18:25:29 - train: epoch 0094, iter [02300, 05004], lr: 0.002330, loss: 1.0425
2022-08-03 18:26:28 - train: epoch 0094, iter [02400, 05004], lr: 0.002316, loss: 1.0418
2022-08-03 18:27:24 - train: epoch 0094, iter [02500, 05004], lr: 0.002302, loss: 1.0421
2022-08-03 18:28:24 - train: epoch 0094, iter [02600, 05004], lr: 0.002288, loss: 1.0076
2022-08-03 18:29:22 - train: epoch 0094, iter [02700, 05004], lr: 0.002273, loss: 0.9147
2022-08-03 18:30:18 - train: epoch 0094, iter [02800, 05004], lr: 0.002260, loss: 1.0910
2022-08-03 18:31:17 - train: epoch 0094, iter [02900, 05004], lr: 0.002246, loss: 1.0298
2022-08-03 18:32:15 - train: epoch 0094, iter [03000, 05004], lr: 0.002232, loss: 1.1401
2022-08-03 18:33:12 - train: epoch 0094, iter [03100, 05004], lr: 0.002218, loss: 0.9817
2022-08-03 18:34:08 - train: epoch 0094, iter [03200, 05004], lr: 0.002204, loss: 0.9935
2022-08-03 18:35:07 - train: epoch 0094, iter [03300, 05004], lr: 0.002190, loss: 1.0667
2022-08-03 18:36:05 - train: epoch 0094, iter [03400, 05004], lr: 0.002176, loss: 1.0978
2022-08-03 18:37:03 - train: epoch 0094, iter [03500, 05004], lr: 0.002163, loss: 1.0997
2022-08-03 18:38:01 - train: epoch 0094, iter [03600, 05004], lr: 0.002149, loss: 1.1360
2022-08-03 18:38:59 - train: epoch 0094, iter [03700, 05004], lr: 0.002136, loss: 1.1303
2022-08-03 18:39:57 - train: epoch 0094, iter [03800, 05004], lr: 0.002122, loss: 0.9880
2022-08-03 18:40:55 - train: epoch 0094, iter [03900, 05004], lr: 0.002108, loss: 1.0269
2022-08-03 18:41:53 - train: epoch 0094, iter [04000, 05004], lr: 0.002095, loss: 0.8834
2022-08-03 18:42:50 - train: epoch 0094, iter [04100, 05004], lr: 0.002082, loss: 1.1843
2022-08-03 18:43:46 - train: epoch 0094, iter [04200, 05004], lr: 0.002068, loss: 0.9660
2022-08-03 18:44:45 - train: epoch 0094, iter [04300, 05004], lr: 0.002055, loss: 1.2451
2022-08-03 18:45:42 - train: epoch 0094, iter [04400, 05004], lr: 0.002041, loss: 1.1692
2022-08-03 18:46:39 - train: epoch 0094, iter [04500, 05004], lr: 0.002028, loss: 1.0636
2022-08-03 18:47:37 - train: epoch 0094, iter [04600, 05004], lr: 0.002015, loss: 1.1641
2022-08-03 18:48:35 - train: epoch 0094, iter [04700, 05004], lr: 0.002002, loss: 1.1613
2022-08-03 18:49:33 - train: epoch 0094, iter [04800, 05004], lr: 0.001989, loss: 0.8181
2022-08-03 18:50:30 - train: epoch 0094, iter [04900, 05004], lr: 0.001976, loss: 1.2354
2022-08-03 18:51:24 - train: epoch 0094, iter [05000, 05004], lr: 0.001963, loss: 0.9502
2022-08-03 18:51:25 - train: epoch 094, train_loss: 1.0330
2022-08-03 18:53:29 - eval: epoch: 094, acc1: 73.296%, acc5: 91.402%, test_loss: 1.0850, per_image_load_time: 4.177ms, per_image_inference_time: 0.620ms
2022-08-03 18:53:29 - until epoch: 094, best_acc1: 73.296%
2022-08-03 18:53:29 - epoch 095 lr: 0.001962
2022-08-03 18:54:35 - train: epoch 0095, iter [00100, 05004], lr: 0.001949, loss: 1.0634
2022-08-03 18:55:33 - train: epoch 0095, iter [00200, 05004], lr: 0.001936, loss: 0.9797
2022-08-03 18:56:30 - train: epoch 0095, iter [00300, 05004], lr: 0.001923, loss: 1.0866
2022-08-03 18:57:28 - train: epoch 0095, iter [00400, 05004], lr: 0.001910, loss: 1.0831
2022-08-03 18:58:24 - train: epoch 0095, iter [00500, 05004], lr: 0.001897, loss: 0.9967
2022-08-03 18:59:22 - train: epoch 0095, iter [00600, 05004], lr: 0.001885, loss: 0.9853
2022-08-03 19:00:18 - train: epoch 0095, iter [00700, 05004], lr: 0.001872, loss: 1.0916
2022-08-03 19:01:17 - train: epoch 0095, iter [00800, 05004], lr: 0.001859, loss: 1.0461
2022-08-03 19:02:13 - train: epoch 0095, iter [00900, 05004], lr: 0.001846, loss: 1.1873
2022-08-03 19:03:12 - train: epoch 0095, iter [01000, 05004], lr: 0.001834, loss: 1.1801
2022-08-03 19:04:09 - train: epoch 0095, iter [01100, 05004], lr: 0.001821, loss: 0.9502
2022-08-03 19:05:06 - train: epoch 0095, iter [01200, 05004], lr: 0.001809, loss: 0.9624
2022-08-03 19:06:02 - train: epoch 0095, iter [01300, 05004], lr: 0.001796, loss: 0.8468
2022-08-03 19:07:00 - train: epoch 0095, iter [01400, 05004], lr: 0.001784, loss: 0.8974
2022-08-03 19:07:57 - train: epoch 0095, iter [01500, 05004], lr: 0.001771, loss: 0.8891
2022-08-03 19:08:54 - train: epoch 0095, iter [01600, 05004], lr: 0.001759, loss: 0.8379
2022-08-03 19:09:51 - train: epoch 0095, iter [01700, 05004], lr: 0.001747, loss: 0.9298
2022-08-03 19:10:49 - train: epoch 0095, iter [01800, 05004], lr: 0.001734, loss: 1.0973
2022-08-03 19:11:45 - train: epoch 0095, iter [01900, 05004], lr: 0.001722, loss: 1.2121
2022-08-03 19:12:43 - train: epoch 0095, iter [02000, 05004], lr: 0.001710, loss: 1.1231
2022-08-03 19:13:42 - train: epoch 0095, iter [02100, 05004], lr: 0.001698, loss: 1.1191
2022-08-03 19:14:39 - train: epoch 0095, iter [02200, 05004], lr: 0.001686, loss: 0.7172
2022-08-03 19:15:39 - train: epoch 0095, iter [02300, 05004], lr: 0.001674, loss: 0.9525
2022-08-03 19:16:35 - train: epoch 0095, iter [02400, 05004], lr: 0.001662, loss: 1.0042
2022-08-03 19:17:34 - train: epoch 0095, iter [02500, 05004], lr: 0.001650, loss: 1.0290
2022-08-03 19:18:31 - train: epoch 0095, iter [02600, 05004], lr: 0.001638, loss: 0.8767
2022-08-03 19:19:30 - train: epoch 0095, iter [02700, 05004], lr: 0.001626, loss: 1.1043
2022-08-03 19:20:26 - train: epoch 0095, iter [02800, 05004], lr: 0.001614, loss: 0.8659
2022-08-03 19:21:26 - train: epoch 0095, iter [02900, 05004], lr: 0.001602, loss: 1.0490
2022-08-03 19:22:23 - train: epoch 0095, iter [03000, 05004], lr: 0.001590, loss: 1.1476
2022-08-03 19:23:20 - train: epoch 0095, iter [03100, 05004], lr: 0.001579, loss: 1.1258
2022-08-03 19:24:20 - train: epoch 0095, iter [03200, 05004], lr: 0.001567, loss: 0.9895
2022-08-03 19:25:16 - train: epoch 0095, iter [03300, 05004], lr: 0.001555, loss: 1.0338
2022-08-03 19:26:13 - train: epoch 0095, iter [03400, 05004], lr: 0.001544, loss: 0.9790
2022-08-03 19:27:11 - train: epoch 0095, iter [03500, 05004], lr: 0.001532, loss: 0.9633
2022-08-03 19:28:09 - train: epoch 0095, iter [03600, 05004], lr: 0.001521, loss: 0.9331
2022-08-03 19:29:06 - train: epoch 0095, iter [03700, 05004], lr: 0.001509, loss: 1.0168
2022-08-03 19:30:05 - train: epoch 0095, iter [03800, 05004], lr: 0.001498, loss: 0.8880
2022-08-03 19:31:03 - train: epoch 0095, iter [03900, 05004], lr: 0.001487, loss: 1.1289
2022-08-03 19:31:59 - train: epoch 0095, iter [04000, 05004], lr: 0.001475, loss: 0.9768
2022-08-03 19:32:57 - train: epoch 0095, iter [04100, 05004], lr: 0.001464, loss: 0.9620
2022-08-03 19:33:53 - train: epoch 0095, iter [04200, 05004], lr: 0.001453, loss: 0.8397
2022-08-03 19:34:51 - train: epoch 0095, iter [04300, 05004], lr: 0.001442, loss: 0.9173
2022-08-03 19:35:49 - train: epoch 0095, iter [04400, 05004], lr: 0.001430, loss: 1.2173
2022-08-03 19:36:47 - train: epoch 0095, iter [04500, 05004], lr: 0.001419, loss: 1.2208
2022-08-03 19:37:44 - train: epoch 0095, iter [04600, 05004], lr: 0.001408, loss: 1.0411
2022-08-03 19:38:42 - train: epoch 0095, iter [04700, 05004], lr: 0.001397, loss: 1.0338
2022-08-03 19:39:40 - train: epoch 0095, iter [04800, 05004], lr: 0.001386, loss: 1.0225
2022-08-03 19:40:38 - train: epoch 0095, iter [04900, 05004], lr: 0.001375, loss: 0.8443
2022-08-03 19:41:33 - train: epoch 0095, iter [05000, 05004], lr: 0.001364, loss: 1.0283
2022-08-03 19:41:34 - train: epoch 095, train_loss: 1.0196
2022-08-03 19:43:43 - eval: epoch: 095, acc1: 73.302%, acc5: 91.578%, test_loss: 1.0793, per_image_load_time: 4.256ms, per_image_inference_time: 0.611ms
2022-08-03 19:43:43 - until epoch: 095, best_acc1: 73.302%
2022-08-03 19:43:43 - epoch 096 lr: 0.001364
2022-08-03 19:44:49 - train: epoch 0096, iter [00100, 05004], lr: 0.001353, loss: 1.0269
2022-08-03 19:45:45 - train: epoch 0096, iter [00200, 05004], lr: 0.001342, loss: 0.9651
2022-08-03 19:46:44 - train: epoch 0096, iter [00300, 05004], lr: 0.001331, loss: 1.0009
2022-08-03 19:47:41 - train: epoch 0096, iter [00400, 05004], lr: 0.001321, loss: 0.8602
2022-08-03 19:48:40 - train: epoch 0096, iter [00500, 05004], lr: 0.001310, loss: 1.0966
2022-08-03 19:49:37 - train: epoch 0096, iter [00600, 05004], lr: 0.001299, loss: 1.0229
2022-08-03 19:50:35 - train: epoch 0096, iter [00700, 05004], lr: 0.001289, loss: 0.9405
2022-08-03 19:51:32 - train: epoch 0096, iter [00800, 05004], lr: 0.001278, loss: 1.1449
2022-08-03 19:52:30 - train: epoch 0096, iter [00900, 05004], lr: 0.001268, loss: 1.0071
2022-08-03 19:53:28 - train: epoch 0096, iter [01000, 05004], lr: 0.001257, loss: 0.9532
2022-08-03 19:54:25 - train: epoch 0096, iter [01100, 05004], lr: 0.001247, loss: 1.0893
2022-08-03 19:55:24 - train: epoch 0096, iter [01200, 05004], lr: 0.001236, loss: 0.9413
2022-08-03 19:56:21 - train: epoch 0096, iter [01300, 05004], lr: 0.001226, loss: 1.0078
2022-08-03 19:57:20 - train: epoch 0096, iter [01400, 05004], lr: 0.001216, loss: 0.9865
2022-08-03 19:58:17 - train: epoch 0096, iter [01500, 05004], lr: 0.001206, loss: 1.0354
2022-08-03 19:59:16 - train: epoch 0096, iter [01600, 05004], lr: 0.001195, loss: 0.8750
2022-08-03 20:00:15 - train: epoch 0096, iter [01700, 05004], lr: 0.001185, loss: 0.7948
2022-08-03 20:01:12 - train: epoch 0096, iter [01800, 05004], lr: 0.001175, loss: 1.1985
2022-08-03 20:02:10 - train: epoch 0096, iter [01900, 05004], lr: 0.001165, loss: 1.1299
2022-08-03 20:03:09 - train: epoch 0096, iter [02000, 05004], lr: 0.001155, loss: 1.0065
2022-08-03 20:04:07 - train: epoch 0096, iter [02100, 05004], lr: 0.001145, loss: 1.0655
2022-08-03 20:05:05 - train: epoch 0096, iter [02200, 05004], lr: 0.001135, loss: 0.7522
2022-08-03 20:06:04 - train: epoch 0096, iter [02300, 05004], lr: 0.001125, loss: 1.0708
2022-08-03 20:07:01 - train: epoch 0096, iter [02400, 05004], lr: 0.001115, loss: 1.0799
2022-08-03 20:08:00 - train: epoch 0096, iter [02500, 05004], lr: 0.001105, loss: 1.0124
2022-08-03 20:08:57 - train: epoch 0096, iter [02600, 05004], lr: 0.001096, loss: 1.0508
2022-08-03 20:09:55 - train: epoch 0096, iter [02700, 05004], lr: 0.001086, loss: 0.8539
2022-08-03 20:10:53 - train: epoch 0096, iter [02800, 05004], lr: 0.001076, loss: 0.9290
2022-08-03 20:11:51 - train: epoch 0096, iter [02900, 05004], lr: 0.001067, loss: 1.1760
2022-08-03 20:12:48 - train: epoch 0096, iter [03000, 05004], lr: 0.001057, loss: 1.1242
2022-08-03 20:13:46 - train: epoch 0096, iter [03100, 05004], lr: 0.001047, loss: 1.0866
2022-08-03 20:14:45 - train: epoch 0096, iter [03200, 05004], lr: 0.001038, loss: 1.1637
2022-08-03 20:15:42 - train: epoch 0096, iter [03300, 05004], lr: 0.001028, loss: 1.1205
2022-08-03 20:16:41 - train: epoch 0096, iter [03400, 05004], lr: 0.001019, loss: 0.9734
2022-08-03 20:17:39 - train: epoch 0096, iter [03500, 05004], lr: 0.001010, loss: 0.9908
2022-08-03 20:18:38 - train: epoch 0096, iter [03600, 05004], lr: 0.001000, loss: 0.9483
2022-08-03 20:19:38 - train: epoch 0096, iter [03700, 05004], lr: 0.000991, loss: 1.1223
2022-08-03 20:20:35 - train: epoch 0096, iter [03800, 05004], lr: 0.000982, loss: 0.9000
2022-08-03 20:21:33 - train: epoch 0096, iter [03900, 05004], lr: 0.000972, loss: 0.8472
2022-08-03 20:22:32 - train: epoch 0096, iter [04000, 05004], lr: 0.000963, loss: 0.9972
2022-08-03 20:23:29 - train: epoch 0096, iter [04100, 05004], lr: 0.000954, loss: 1.0552
2022-08-03 20:24:28 - train: epoch 0096, iter [04200, 05004], lr: 0.000945, loss: 1.0318
2022-08-03 20:25:24 - train: epoch 0096, iter [04300, 05004], lr: 0.000936, loss: 1.0501
2022-08-03 20:26:23 - train: epoch 0096, iter [04400, 05004], lr: 0.000927, loss: 0.9668
2022-08-03 20:27:21 - train: epoch 0096, iter [04500, 05004], lr: 0.000918, loss: 1.0017
2022-08-03 20:28:19 - train: epoch 0096, iter [04600, 05004], lr: 0.000909, loss: 0.8393
2022-08-03 20:29:18 - train: epoch 0096, iter [04700, 05004], lr: 0.000900, loss: 1.0776
2022-08-03 20:30:16 - train: epoch 0096, iter [04800, 05004], lr: 0.000891, loss: 0.9733
2022-08-03 20:31:13 - train: epoch 0096, iter [04900, 05004], lr: 0.000883, loss: 0.9619
2022-08-03 20:32:09 - train: epoch 0096, iter [05000, 05004], lr: 0.000874, loss: 1.0789
2022-08-03 20:32:10 - train: epoch 096, train_loss: 1.0049
2022-08-03 20:34:25 - eval: epoch: 096, acc1: 73.312%, acc5: 91.568%, test_loss: 1.0763, per_image_load_time: 3.484ms, per_image_inference_time: 0.663ms
2022-08-03 20:34:25 - until epoch: 096, best_acc1: 73.312%
2022-08-03 20:34:25 - epoch 097 lr: 0.000874
2022-08-03 20:35:32 - train: epoch 0097, iter [00100, 05004], lr: 0.000865, loss: 0.9744
2022-08-03 20:36:31 - train: epoch 0097, iter [00200, 05004], lr: 0.000856, loss: 0.8787
2022-08-03 20:37:29 - train: epoch 0097, iter [00300, 05004], lr: 0.000848, loss: 1.1876
2022-08-03 20:38:27 - train: epoch 0097, iter [00400, 05004], lr: 0.000839, loss: 1.2554
2022-08-03 20:39:26 - train: epoch 0097, iter [00500, 05004], lr: 0.000831, loss: 0.9761
2022-08-03 20:40:22 - train: epoch 0097, iter [00600, 05004], lr: 0.000822, loss: 0.9948
2022-08-03 20:41:21 - train: epoch 0097, iter [00700, 05004], lr: 0.000814, loss: 0.9455
2022-08-03 20:42:18 - train: epoch 0097, iter [00800, 05004], lr: 0.000805, loss: 0.8238
2022-08-03 20:43:17 - train: epoch 0097, iter [00900, 05004], lr: 0.000797, loss: 1.0987
2022-08-03 20:44:16 - train: epoch 0097, iter [01000, 05004], lr: 0.000789, loss: 1.0438
2022-08-03 20:45:13 - train: epoch 0097, iter [01100, 05004], lr: 0.000780, loss: 1.0162
2022-08-03 20:46:11 - train: epoch 0097, iter [01200, 05004], lr: 0.000772, loss: 0.9748
2022-08-03 20:47:08 - train: epoch 0097, iter [01300, 05004], lr: 0.000764, loss: 0.8115
2022-08-03 20:48:08 - train: epoch 0097, iter [01400, 05004], lr: 0.000756, loss: 0.9029
2022-08-03 20:49:05 - train: epoch 0097, iter [01500, 05004], lr: 0.000748, loss: 0.8592
2022-08-03 20:50:02 - train: epoch 0097, iter [01600, 05004], lr: 0.000740, loss: 1.1286
2022-08-03 20:51:01 - train: epoch 0097, iter [01700, 05004], lr: 0.000732, loss: 1.0141
2022-08-03 20:51:58 - train: epoch 0097, iter [01800, 05004], lr: 0.000724, loss: 1.1570
2022-08-03 20:52:54 - train: epoch 0097, iter [01900, 05004], lr: 0.000716, loss: 0.9879
2022-08-03 20:53:52 - train: epoch 0097, iter [02000, 05004], lr: 0.000708, loss: 0.9265
2022-08-03 20:54:50 - train: epoch 0097, iter [02100, 05004], lr: 0.000700, loss: 0.9778
2022-08-03 20:55:49 - train: epoch 0097, iter [02200, 05004], lr: 0.000692, loss: 0.9995
2022-08-03 20:56:47 - train: epoch 0097, iter [02300, 05004], lr: 0.000685, loss: 0.9598
2022-08-03 20:57:44 - train: epoch 0097, iter [02400, 05004], lr: 0.000677, loss: 0.9850
2022-08-03 20:58:42 - train: epoch 0097, iter [02500, 05004], lr: 0.000669, loss: 0.9164
2022-08-03 20:59:40 - train: epoch 0097, iter [02600, 05004], lr: 0.000662, loss: 1.1061
2022-08-03 21:00:37 - train: epoch 0097, iter [02700, 05004], lr: 0.000654, loss: 0.8720
2022-08-03 21:01:35 - train: epoch 0097, iter [02800, 05004], lr: 0.000647, loss: 1.0428
2022-08-03 21:02:34 - train: epoch 0097, iter [02900, 05004], lr: 0.000639, loss: 0.9066
2022-08-03 21:03:31 - train: epoch 0097, iter [03000, 05004], lr: 0.000632, loss: 1.0676
2022-08-03 21:04:29 - train: epoch 0097, iter [03100, 05004], lr: 0.000624, loss: 0.9293
2022-08-03 21:05:29 - train: epoch 0097, iter [03200, 05004], lr: 0.000617, loss: 1.1744
2022-08-03 21:06:27 - train: epoch 0097, iter [03300, 05004], lr: 0.000610, loss: 0.9421
2022-08-03 21:07:24 - train: epoch 0097, iter [03400, 05004], lr: 0.000602, loss: 1.1077
2022-08-03 21:08:21 - train: epoch 0097, iter [03500, 05004], lr: 0.000595, loss: 0.8937
2022-08-03 21:09:19 - train: epoch 0097, iter [03600, 05004], lr: 0.000588, loss: 0.8962
2022-08-03 21:10:17 - train: epoch 0097, iter [03700, 05004], lr: 0.000581, loss: 0.7761
2022-08-03 21:11:15 - train: epoch 0097, iter [03800, 05004], lr: 0.000574, loss: 0.9238
2022-08-03 21:12:10 - train: epoch 0097, iter [03900, 05004], lr: 0.000567, loss: 0.8746
2022-08-03 21:13:10 - train: epoch 0097, iter [04000, 05004], lr: 0.000560, loss: 1.1139
2022-08-03 21:14:07 - train: epoch 0097, iter [04100, 05004], lr: 0.000553, loss: 1.1508
2022-08-03 21:15:04 - train: epoch 0097, iter [04200, 05004], lr: 0.000546, loss: 0.9469
2022-08-03 21:16:01 - train: epoch 0097, iter [04300, 05004], lr: 0.000539, loss: 0.7764
2022-08-03 21:17:01 - train: epoch 0097, iter [04400, 05004], lr: 0.000532, loss: 1.0500
2022-08-03 21:17:57 - train: epoch 0097, iter [04500, 05004], lr: 0.000525, loss: 1.1885
2022-08-03 21:18:55 - train: epoch 0097, iter [04600, 05004], lr: 0.000519, loss: 0.8501
2022-08-03 21:19:54 - train: epoch 0097, iter [04700, 05004], lr: 0.000512, loss: 1.0303
2022-08-03 21:20:51 - train: epoch 0097, iter [04800, 05004], lr: 0.000505, loss: 1.0654
2022-08-03 21:21:51 - train: epoch 0097, iter [04900, 05004], lr: 0.000499, loss: 1.1229
2022-08-03 21:22:46 - train: epoch 0097, iter [05000, 05004], lr: 0.000492, loss: 0.8788
2022-08-03 21:22:47 - train: epoch 097, train_loss: 0.9946
2022-08-03 21:24:54 - eval: epoch: 097, acc1: 73.564%, acc5: 91.624%, test_loss: 1.0724, per_image_load_time: 3.620ms, per_image_inference_time: 0.644ms
2022-08-03 21:24:54 - until epoch: 097, best_acc1: 73.564%
2022-08-03 21:24:54 - epoch 098 lr: 0.000492
2022-08-03 21:26:01 - train: epoch 0098, iter [00100, 05004], lr: 0.000485, loss: 1.0291
2022-08-03 21:27:02 - train: epoch 0098, iter [00200, 05004], lr: 0.000479, loss: 1.1855
2022-08-03 21:28:00 - train: epoch 0098, iter [00300, 05004], lr: 0.000472, loss: 1.0372
2022-08-03 21:28:56 - train: epoch 0098, iter [00400, 05004], lr: 0.000466, loss: 0.7912
2022-08-03 21:29:54 - train: epoch 0098, iter [00500, 05004], lr: 0.000460, loss: 1.0046
2022-08-03 21:30:50 - train: epoch 0098, iter [00600, 05004], lr: 0.000453, loss: 1.0387
2022-08-03 21:31:49 - train: epoch 0098, iter [00700, 05004], lr: 0.000447, loss: 1.0871
2022-08-03 21:32:46 - train: epoch 0098, iter [00800, 05004], lr: 0.000441, loss: 1.0611
2022-08-03 21:33:45 - train: epoch 0098, iter [00900, 05004], lr: 0.000435, loss: 1.0368
2022-08-03 21:34:43 - train: epoch 0098, iter [01000, 05004], lr: 0.000428, loss: 0.9628
2022-08-03 21:35:40 - train: epoch 0098, iter [01100, 05004], lr: 0.000422, loss: 0.9532
2022-08-03 21:36:38 - train: epoch 0098, iter [01200, 05004], lr: 0.000416, loss: 0.9657
2022-08-03 21:37:36 - train: epoch 0098, iter [01300, 05004], lr: 0.000410, loss: 0.8037
2022-08-03 21:38:33 - train: epoch 0098, iter [01400, 05004], lr: 0.000404, loss: 1.1312
2022-08-03 21:39:29 - train: epoch 0098, iter [01500, 05004], lr: 0.000398, loss: 0.9881
2022-08-03 21:40:27 - train: epoch 0098, iter [01600, 05004], lr: 0.000393, loss: 1.0469
2022-08-03 21:41:26 - train: epoch 0098, iter [01700, 05004], lr: 0.000387, loss: 1.0863
2022-08-03 21:42:22 - train: epoch 0098, iter [01800, 05004], lr: 0.000381, loss: 1.1275
2022-08-03 21:43:21 - train: epoch 0098, iter [01900, 05004], lr: 0.000375, loss: 0.8531
2022-08-03 21:44:18 - train: epoch 0098, iter [02000, 05004], lr: 0.000369, loss: 1.0627
2022-08-03 21:45:16 - train: epoch 0098, iter [02100, 05004], lr: 0.000364, loss: 1.0377
2022-08-03 21:46:15 - train: epoch 0098, iter [02200, 05004], lr: 0.000358, loss: 0.9320
2022-08-03 21:47:13 - train: epoch 0098, iter [02300, 05004], lr: 0.000353, loss: 0.8278
2022-08-03 21:48:11 - train: epoch 0098, iter [02400, 05004], lr: 0.000347, loss: 1.0272
2022-08-03 21:49:08 - train: epoch 0098, iter [02500, 05004], lr: 0.000342, loss: 1.0132
2022-08-03 21:50:06 - train: epoch 0098, iter [02600, 05004], lr: 0.000336, loss: 0.9898
2022-08-03 21:51:05 - train: epoch 0098, iter [02700, 05004], lr: 0.000331, loss: 0.9701
2022-08-03 21:52:02 - train: epoch 0098, iter [02800, 05004], lr: 0.000325, loss: 1.1057
2022-08-03 21:53:01 - train: epoch 0098, iter [02900, 05004], lr: 0.000320, loss: 1.0321
2022-08-03 21:53:59 - train: epoch 0098, iter [03000, 05004], lr: 0.000315, loss: 1.0478
2022-08-03 21:54:57 - train: epoch 0098, iter [03100, 05004], lr: 0.000310, loss: 1.0659
2022-08-03 21:55:55 - train: epoch 0098, iter [03200, 05004], lr: 0.000305, loss: 0.9256
2022-08-03 21:56:53 - train: epoch 0098, iter [03300, 05004], lr: 0.000299, loss: 0.9275
2022-08-03 21:57:50 - train: epoch 0098, iter [03400, 05004], lr: 0.000294, loss: 0.9554
2022-08-03 21:58:47 - train: epoch 0098, iter [03500, 05004], lr: 0.000289, loss: 1.1245
2022-08-03 21:59:44 - train: epoch 0098, iter [03600, 05004], lr: 0.000284, loss: 1.1173
2022-08-03 22:00:42 - train: epoch 0098, iter [03700, 05004], lr: 0.000279, loss: 1.0044
2022-08-03 22:01:39 - train: epoch 0098, iter [03800, 05004], lr: 0.000274, loss: 0.9414
2022-08-03 22:02:38 - train: epoch 0098, iter [03900, 05004], lr: 0.000270, loss: 1.0868
2022-08-03 22:03:35 - train: epoch 0098, iter [04000, 05004], lr: 0.000265, loss: 1.0060
2022-08-03 22:04:31 - train: epoch 0098, iter [04100, 05004], lr: 0.000260, loss: 0.9771
2022-08-03 22:05:30 - train: epoch 0098, iter [04200, 05004], lr: 0.000255, loss: 0.9215
2022-08-03 22:06:27 - train: epoch 0098, iter [04300, 05004], lr: 0.000250, loss: 0.8420
2022-08-03 22:07:23 - train: epoch 0098, iter [04400, 05004], lr: 0.000246, loss: 1.1195
2022-08-03 22:08:21 - train: epoch 0098, iter [04500, 05004], lr: 0.000241, loss: 0.9888
2022-08-03 22:09:17 - train: epoch 0098, iter [04600, 05004], lr: 0.000237, loss: 1.1516
2022-08-03 22:10:14 - train: epoch 0098, iter [04700, 05004], lr: 0.000232, loss: 1.0238
2022-08-03 22:11:10 - train: epoch 0098, iter [04800, 05004], lr: 0.000228, loss: 0.8353
2022-08-03 22:12:08 - train: epoch 0098, iter [04900, 05004], lr: 0.000223, loss: 0.9870
2022-08-03 22:13:03 - train: epoch 0098, iter [05000, 05004], lr: 0.000219, loss: 0.9354
2022-08-03 22:13:05 - train: epoch 098, train_loss: 0.9884
2022-08-03 22:15:09 - eval: epoch: 098, acc1: 73.546%, acc5: 91.664%, test_loss: 1.0718, per_image_load_time: 4.163ms, per_image_inference_time: 0.597ms
2022-08-03 22:15:09 - until epoch: 098, best_acc1: 73.564%
2022-08-03 22:15:09 - epoch 099 lr: 0.000219
2022-08-03 22:16:14 - train: epoch 0099, iter [00100, 05004], lr: 0.000214, loss: 0.9268
2022-08-03 22:17:12 - train: epoch 0099, iter [00200, 05004], lr: 0.000210, loss: 1.0225
2022-08-03 22:18:10 - train: epoch 0099, iter [00300, 05004], lr: 0.000206, loss: 0.9670
2022-08-03 22:19:09 - train: epoch 0099, iter [00400, 05004], lr: 0.000202, loss: 0.9453
2022-08-03 22:20:03 - train: epoch 0099, iter [00500, 05004], lr: 0.000197, loss: 1.0672
2022-08-03 22:21:01 - train: epoch 0099, iter [00600, 05004], lr: 0.000193, loss: 1.0964
2022-08-03 22:21:59 - train: epoch 0099, iter [00700, 05004], lr: 0.000189, loss: 1.0294
2022-08-03 22:22:55 - train: epoch 0099, iter [00800, 05004], lr: 0.000185, loss: 0.9149
2022-08-03 22:23:54 - train: epoch 0099, iter [00900, 05004], lr: 0.000181, loss: 1.1407
2022-08-03 22:24:52 - train: epoch 0099, iter [01000, 05004], lr: 0.000177, loss: 0.9069
2022-08-03 22:25:49 - train: epoch 0099, iter [01100, 05004], lr: 0.000173, loss: 1.0617
2022-08-03 22:26:47 - train: epoch 0099, iter [01200, 05004], lr: 0.000169, loss: 0.8473
2022-08-03 22:27:45 - train: epoch 0099, iter [01300, 05004], lr: 0.000166, loss: 0.9015
2022-08-03 22:28:43 - train: epoch 0099, iter [01400, 05004], lr: 0.000162, loss: 0.9885
2022-08-03 22:29:40 - train: epoch 0099, iter [01500, 05004], lr: 0.000158, loss: 0.9640
2022-08-03 22:30:38 - train: epoch 0099, iter [01600, 05004], lr: 0.000154, loss: 1.0620
2022-08-03 22:31:36 - train: epoch 0099, iter [01700, 05004], lr: 0.000151, loss: 0.8194
2022-08-03 22:32:33 - train: epoch 0099, iter [01800, 05004], lr: 0.000147, loss: 0.9703
2022-08-03 22:33:31 - train: epoch 0099, iter [01900, 05004], lr: 0.000144, loss: 1.0128
2022-08-03 22:34:29 - train: epoch 0099, iter [02000, 05004], lr: 0.000140, loss: 0.8979
2022-08-03 22:35:28 - train: epoch 0099, iter [02100, 05004], lr: 0.000137, loss: 0.9199
2022-08-03 22:36:26 - train: epoch 0099, iter [02200, 05004], lr: 0.000133, loss: 0.9271
2022-08-03 22:37:25 - train: epoch 0099, iter [02300, 05004], lr: 0.000130, loss: 1.0492
2022-08-03 22:38:23 - train: epoch 0099, iter [02400, 05004], lr: 0.000126, loss: 1.1286
2022-08-03 22:39:21 - train: epoch 0099, iter [02500, 05004], lr: 0.000123, loss: 0.7479
2022-08-03 22:40:19 - train: epoch 0099, iter [02600, 05004], lr: 0.000120, loss: 1.0366
2022-08-03 22:41:17 - train: epoch 0099, iter [02700, 05004], lr: 0.000117, loss: 0.9610
2022-08-03 22:42:16 - train: epoch 0099, iter [02800, 05004], lr: 0.000113, loss: 1.0184
2022-08-03 22:43:13 - train: epoch 0099, iter [02900, 05004], lr: 0.000110, loss: 0.8460
2022-08-03 22:44:12 - train: epoch 0099, iter [03000, 05004], lr: 0.000107, loss: 1.1022
2022-08-03 22:45:10 - train: epoch 0099, iter [03100, 05004], lr: 0.000104, loss: 0.9697
2022-08-03 22:46:08 - train: epoch 0099, iter [03200, 05004], lr: 0.000101, loss: 0.9633
2022-08-03 22:47:06 - train: epoch 0099, iter [03300, 05004], lr: 0.000098, loss: 0.9346
2022-08-03 22:48:05 - train: epoch 0099, iter [03400, 05004], lr: 0.000095, loss: 1.0464
2022-08-03 22:49:03 - train: epoch 0099, iter [03500, 05004], lr: 0.000092, loss: 1.0827
2022-08-03 22:50:00 - train: epoch 0099, iter [03600, 05004], lr: 0.000090, loss: 0.8559
2022-08-03 22:51:01 - train: epoch 0099, iter [03700, 05004], lr: 0.000087, loss: 1.0226
2022-08-03 22:51:57 - train: epoch 0099, iter [03800, 05004], lr: 0.000084, loss: 1.0344
2022-08-03 22:52:56 - train: epoch 0099, iter [03900, 05004], lr: 0.000081, loss: 0.9393
2022-08-03 22:53:52 - train: epoch 0099, iter [04000, 05004], lr: 0.000079, loss: 1.1369
2022-08-03 22:54:51 - train: epoch 0099, iter [04100, 05004], lr: 0.000076, loss: 0.8291
2022-08-03 22:55:49 - train: epoch 0099, iter [04200, 05004], lr: 0.000074, loss: 1.2169
2022-08-03 22:56:45 - train: epoch 0099, iter [04300, 05004], lr: 0.000071, loss: 0.9017
2022-08-03 22:57:44 - train: epoch 0099, iter [04400, 05004], lr: 0.000069, loss: 1.0884
2022-08-03 22:58:39 - train: epoch 0099, iter [04500, 05004], lr: 0.000066, loss: 1.1321
2022-08-03 22:59:37 - train: epoch 0099, iter [04600, 05004], lr: 0.000064, loss: 0.9253
2022-08-03 23:00:34 - train: epoch 0099, iter [04700, 05004], lr: 0.000062, loss: 0.8086
2022-08-03 23:01:33 - train: epoch 0099, iter [04800, 05004], lr: 0.000059, loss: 1.0125
2022-08-03 23:02:31 - train: epoch 0099, iter [04900, 05004], lr: 0.000057, loss: 1.0549
2022-08-03 23:03:25 - train: epoch 0099, iter [05000, 05004], lr: 0.000055, loss: 0.9200
2022-08-03 23:03:27 - train: epoch 099, train_loss: 0.9863
2022-08-03 23:05:30 - eval: epoch: 099, acc1: 73.530%, acc5: 91.642%, test_loss: 1.0723, per_image_load_time: 3.508ms, per_image_inference_time: 0.653ms
2022-08-03 23:05:30 - until epoch: 099, best_acc1: 73.564%
2022-08-03 23:05:30 - epoch 100 lr: 0.000055
2022-08-03 23:06:37 - train: epoch 0100, iter [00100, 05004], lr: 0.000053, loss: 1.0337
2022-08-03 23:07:35 - train: epoch 0100, iter [00200, 05004], lr: 0.000050, loss: 0.9334
2022-08-03 23:08:35 - train: epoch 0100, iter [00300, 05004], lr: 0.000048, loss: 0.9639
2022-08-03 23:09:32 - train: epoch 0100, iter [00400, 05004], lr: 0.000046, loss: 0.8135
2022-08-03 23:10:30 - train: epoch 0100, iter [00500, 05004], lr: 0.000044, loss: 1.0086
2022-08-03 23:11:26 - train: epoch 0100, iter [00600, 05004], lr: 0.000042, loss: 0.9956
2022-08-03 23:12:25 - train: epoch 0100, iter [00700, 05004], lr: 0.000040, loss: 1.0201
2022-08-03 23:13:21 - train: epoch 0100, iter [00800, 05004], lr: 0.000039, loss: 0.9327
2022-08-03 23:14:19 - train: epoch 0100, iter [00900, 05004], lr: 0.000037, loss: 0.9083
2022-08-03 23:15:15 - train: epoch 0100, iter [01000, 05004], lr: 0.000035, loss: 0.9914
2022-08-03 23:16:11 - train: epoch 0100, iter [01100, 05004], lr: 0.000033, loss: 0.8423
2022-08-03 23:17:08 - train: epoch 0100, iter [01200, 05004], lr: 0.000032, loss: 0.8704
2022-08-03 23:18:03 - train: epoch 0100, iter [01300, 05004], lr: 0.000030, loss: 1.0218
2022-08-03 23:19:02 - train: epoch 0100, iter [01400, 05004], lr: 0.000028, loss: 1.1451
2022-08-03 23:19:58 - train: epoch 0100, iter [01500, 05004], lr: 0.000027, loss: 1.1062
2022-08-03 23:20:54 - train: epoch 0100, iter [01600, 05004], lr: 0.000025, loss: 0.8644
2022-08-03 23:21:51 - train: epoch 0100, iter [01700, 05004], lr: 0.000024, loss: 0.8098
2022-08-03 23:22:48 - train: epoch 0100, iter [01800, 05004], lr: 0.000022, loss: 0.9158
2022-08-03 23:23:44 - train: epoch 0100, iter [01900, 05004], lr: 0.000021, loss: 0.9807
2022-08-03 23:24:41 - train: epoch 0100, iter [02000, 05004], lr: 0.000020, loss: 1.1446
2022-08-03 23:25:38 - train: epoch 0100, iter [02100, 05004], lr: 0.000018, loss: 1.0590
2022-08-03 23:26:34 - train: epoch 0100, iter [02200, 05004], lr: 0.000017, loss: 0.9951
2022-08-03 23:27:29 - train: epoch 0100, iter [02300, 05004], lr: 0.000016, loss: 0.8249
2022-08-03 23:28:25 - train: epoch 0100, iter [02400, 05004], lr: 0.000015, loss: 1.0604
2022-08-03 23:29:22 - train: epoch 0100, iter [02500, 05004], lr: 0.000014, loss: 1.0475
2022-08-03 23:30:19 - train: epoch 0100, iter [02600, 05004], lr: 0.000013, loss: 0.8720
2022-08-03 23:31:14 - train: epoch 0100, iter [02700, 05004], lr: 0.000012, loss: 0.8854
2022-08-03 23:32:11 - train: epoch 0100, iter [02800, 05004], lr: 0.000011, loss: 0.9042
2022-08-03 23:33:08 - train: epoch 0100, iter [02900, 05004], lr: 0.000010, loss: 1.0029
2022-08-03 23:34:04 - train: epoch 0100, iter [03000, 05004], lr: 0.000009, loss: 0.8821
2022-08-03 23:35:02 - train: epoch 0100, iter [03100, 05004], lr: 0.000008, loss: 0.8157
2022-08-03 23:35:58 - train: epoch 0100, iter [03200, 05004], lr: 0.000007, loss: 1.1062
2022-08-03 23:36:54 - train: epoch 0100, iter [03300, 05004], lr: 0.000006, loss: 1.0272
2022-08-03 23:37:50 - train: epoch 0100, iter [03400, 05004], lr: 0.000006, loss: 1.0257
2022-08-03 23:38:47 - train: epoch 0100, iter [03500, 05004], lr: 0.000005, loss: 1.0180
2022-08-03 23:39:43 - train: epoch 0100, iter [03600, 05004], lr: 0.000004, loss: 0.9677
2022-08-03 23:40:40 - train: epoch 0100, iter [03700, 05004], lr: 0.000004, loss: 0.8229
2022-08-03 23:41:36 - train: epoch 0100, iter [03800, 05004], lr: 0.000003, loss: 0.9638
2022-08-03 23:42:32 - train: epoch 0100, iter [03900, 05004], lr: 0.000003, loss: 0.9892
2022-08-03 23:43:29 - train: epoch 0100, iter [04000, 05004], lr: 0.000002, loss: 1.0314
2022-08-03 23:44:26 - train: epoch 0100, iter [04100, 05004], lr: 0.000002, loss: 1.0329
2022-08-03 23:45:22 - train: epoch 0100, iter [04200, 05004], lr: 0.000001, loss: 0.9748
2022-08-03 23:46:16 - train: epoch 0100, iter [04300, 05004], lr: 0.000001, loss: 0.9034
2022-08-03 23:47:14 - train: epoch 0100, iter [04400, 05004], lr: 0.000001, loss: 1.3175
2022-08-03 23:48:11 - train: epoch 0100, iter [04500, 05004], lr: 0.000001, loss: 0.9226
2022-08-03 23:49:06 - train: epoch 0100, iter [04600, 05004], lr: 0.000000, loss: 0.9005
2022-08-03 23:50:03 - train: epoch 0100, iter [04700, 05004], lr: 0.000000, loss: 1.1370
2022-08-03 23:51:00 - train: epoch 0100, iter [04800, 05004], lr: 0.000000, loss: 0.8657
2022-08-03 23:51:57 - train: epoch 0100, iter [04900, 05004], lr: 0.000000, loss: 0.8784
2022-08-03 23:52:50 - train: epoch 0100, iter [05000, 05004], lr: 0.000000, loss: 1.2244
2022-08-03 23:52:51 - train: epoch 100, train_loss: 0.9799
2022-08-03 23:54:56 - eval: epoch: 100, acc1: 73.552%, acc5: 91.668%, test_loss: 1.0720, per_image_load_time: 4.192ms, per_image_inference_time: 0.595ms
2022-08-03 23:54:56 - until epoch: 100, best_acc1: 73.564%
2022-08-03 23:54:56 - train done. model: RegNetX_600MF, train time: 83.359 hours, best_acc1: 73.564%
