2022-07-31 12:33:09 - network: RegNetX_600MF
2022-07-31 12:33:09 - num_classes: 1000
2022-07-31 12:33:09 - input_image_size: 224
2022-07-31 12:33:09 - scale: 1.1428571428571428
2022-07-31 12:33:09 - trained_model_path: 
2022-07-31 12:33:09 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-31 12:33:09 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-31 12:33:09 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f91af13beb0>
2022-07-31 12:33:09 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f91a2a280a0>
2022-07-31 12:33:09 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f91a2a280d0>
2022-07-31 12:33:09 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f91a2a28130>
2022-07-31 12:33:09 - seed: 0
2022-07-31 12:33:09 - batch_size: 256
2022-07-31 12:33:09 - num_workers: 16
2022-07-31 12:33:09 - optimizer: ('SGD', {'lr': 0.2, 'momentum': 0.9, 'global_weight_decay': False, 'nesterov': True, 'weight_decay': 5e-05, 'no_weight_decay_layer_name_list': []})
2022-07-31 12:33:09 - scheduler: ('CosineLR', {'warm_up_epochs': 5})
2022-07-31 12:33:09 - epochs: 100
2022-07-31 12:33:09 - print_interval: 100
2022-07-31 12:33:09 - accumulation_steps: 1
2022-07-31 12:33:09 - sync_bn: False
2022-07-31 12:33:09 - apex: True
2022-07-31 12:33:09 - use_ema_model: False
2022-07-31 12:33:09 - ema_model_decay: 0.9999
2022-07-31 12:33:09 - gpus_type: NVIDIA RTX A5000
2022-07-31 12:33:09 - gpus_num: 2
2022-07-31 12:33:09 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f91a1217b70>
2022-07-31 12:33:09 - --------------------parameters--------------------
2022-07-31 12:33:09 - name: conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer1.0.downsample_layer.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer1.0.downsample_layer.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer1.0.downsample_layer.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer2.0.downsample_layer.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer2.0.downsample_layer.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer2.0.downsample_layer.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.0.downsample_layer.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.0.downsample_layer.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.0.downsample_layer.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.0.downsample_layer.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.0.downsample_layer.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.0.downsample_layer.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.6.conv1.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.6.conv1.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.6.conv1.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.6.conv2.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.6.conv2.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.6.conv2.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: layer4.6.conv3.layer.0.weight, grad: True
2022-07-31 12:33:09 - name: layer4.6.conv3.layer.1.weight, grad: True
2022-07-31 12:33:09 - name: layer4.6.conv3.layer.1.bias, grad: True
2022-07-31 12:33:09 - name: fc.weight, grad: True
2022-07-31 12:33:09 - name: fc.bias, grad: True
2022-07-31 12:33:09 - --------------------buffers--------------------
2022-07-31 12:33:09 - name: conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer1.0.downsample_layer.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer1.0.downsample_layer.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer1.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer2.0.downsample_layer.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer2.0.downsample_layer.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer2.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.0.downsample_layer.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.0.downsample_layer.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.0.downsample_layer.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.0.downsample_layer.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.6.conv1.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.6.conv1.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.6.conv1.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.6.conv2.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.6.conv2.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.6.conv2.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - name: layer4.6.conv3.layer.1.running_mean, grad: False
2022-07-31 12:33:09 - name: layer4.6.conv3.layer.1.running_var, grad: False
2022-07-31 12:33:09 - name: layer4.6.conv3.layer.1.num_batches_tracked, grad: False
2022-07-31 12:33:09 - -----------no weight decay layers--------------
2022-07-31 12:33:09 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer1.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer1.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-31 12:33:09 - -------------weight decay layers---------------
2022-07-31 12:33:09 - name: conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer1.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer1.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer1.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer1.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.2.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.2.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer2.2.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.2.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.2.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.2.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.3.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.3.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.3.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.4.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.4.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer3.4.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.2.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.2.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.2.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.3.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.3.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.3.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.4.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.4.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.4.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.5.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.5.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.5.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.6.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.6.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: layer4.6.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - name: fc.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-31 12:33:09 - epoch 001 lr: 0.200000
2022-07-31 12:34:18 - train: epoch 0001, iter [00100, 05004], lr: 0.040799, loss: 6.9275
2022-07-31 12:35:17 - train: epoch 0001, iter [00200, 05004], lr: 0.041599, loss: 6.8200
2022-07-31 12:36:14 - train: epoch 0001, iter [00300, 05004], lr: 0.042398, loss: 6.6062
2022-07-31 12:37:11 - train: epoch 0001, iter [00400, 05004], lr: 0.043197, loss: 6.5122
2022-07-31 12:38:11 - train: epoch 0001, iter [00500, 05004], lr: 0.043997, loss: 6.4672
2022-07-31 12:39:11 - train: epoch 0001, iter [00600, 05004], lr: 0.044796, loss: 6.2013
2022-07-31 12:40:08 - train: epoch 0001, iter [00700, 05004], lr: 0.045596, loss: 6.2291
2022-07-31 12:41:05 - train: epoch 0001, iter [00800, 05004], lr: 0.046395, loss: 6.1755
2022-07-31 12:42:05 - train: epoch 0001, iter [00900, 05004], lr: 0.047194, loss: 6.0630
2022-07-31 12:43:02 - train: epoch 0001, iter [01000, 05004], lr: 0.047994, loss: 5.9960
2022-07-31 12:43:59 - train: epoch 0001, iter [01100, 05004], lr: 0.048793, loss: 5.9580
2022-07-31 12:44:58 - train: epoch 0001, iter [01200, 05004], lr: 0.049592, loss: 5.6992
2022-07-31 12:45:57 - train: epoch 0001, iter [01300, 05004], lr: 0.050392, loss: 5.6094
2022-07-31 12:46:55 - train: epoch 0001, iter [01400, 05004], lr: 0.051191, loss: 5.5765
2022-07-31 12:47:52 - train: epoch 0001, iter [01500, 05004], lr: 0.051990, loss: 5.6777
2022-07-31 12:48:51 - train: epoch 0001, iter [01600, 05004], lr: 0.052790, loss: 5.6864
2022-07-31 12:49:50 - train: epoch 0001, iter [01700, 05004], lr: 0.053589, loss: 5.4643
2022-07-31 12:50:48 - train: epoch 0001, iter [01800, 05004], lr: 0.054388, loss: 5.4999
2022-07-31 12:51:47 - train: epoch 0001, iter [01900, 05004], lr: 0.055188, loss: 5.4012
2022-07-31 12:52:45 - train: epoch 0001, iter [02000, 05004], lr: 0.055987, loss: 5.2984
2022-07-31 12:53:43 - train: epoch 0001, iter [02100, 05004], lr: 0.056787, loss: 5.4301
2022-07-31 12:54:40 - train: epoch 0001, iter [02200, 05004], lr: 0.057586, loss: 5.2262
2022-07-31 12:55:40 - train: epoch 0001, iter [02300, 05004], lr: 0.058385, loss: 5.0539
2022-07-31 12:56:39 - train: epoch 0001, iter [02400, 05004], lr: 0.059185, loss: 5.1819
2022-07-31 12:57:39 - train: epoch 0001, iter [02500, 05004], lr: 0.059984, loss: 5.0681
2022-07-31 12:58:37 - train: epoch 0001, iter [02600, 05004], lr: 0.060783, loss: 5.2393
2022-07-31 12:59:34 - train: epoch 0001, iter [02700, 05004], lr: 0.061583, loss: 5.1431
2022-07-31 13:00:33 - train: epoch 0001, iter [02800, 05004], lr: 0.062382, loss: 4.9660
2022-07-31 13:01:32 - train: epoch 0001, iter [02900, 05004], lr: 0.063181, loss: 4.7842
2022-07-31 13:02:31 - train: epoch 0001, iter [03000, 05004], lr: 0.063981, loss: 4.8725
2022-07-31 13:03:30 - train: epoch 0001, iter [03100, 05004], lr: 0.064780, loss: 4.9864
2022-07-31 13:04:28 - train: epoch 0001, iter [03200, 05004], lr: 0.065580, loss: 4.8959
2022-07-31 13:05:27 - train: epoch 0001, iter [03300, 05004], lr: 0.066379, loss: 4.6035
2022-07-31 13:06:25 - train: epoch 0001, iter [03400, 05004], lr: 0.067178, loss: 4.6947
2022-07-31 13:07:24 - train: epoch 0001, iter [03500, 05004], lr: 0.067978, loss: 4.7093
2022-07-31 13:08:22 - train: epoch 0001, iter [03600, 05004], lr: 0.068777, loss: 4.5895
2022-07-31 13:09:22 - train: epoch 0001, iter [03700, 05004], lr: 0.069576, loss: 4.5598
2022-07-31 13:10:19 - train: epoch 0001, iter [03800, 05004], lr: 0.070376, loss: 4.4794
2022-07-31 13:11:18 - train: epoch 0001, iter [03900, 05004], lr: 0.071175, loss: 4.4323
2022-07-31 13:12:16 - train: epoch 0001, iter [04000, 05004], lr: 0.071974, loss: 4.5691
2022-07-31 13:13:16 - train: epoch 0001, iter [04100, 05004], lr: 0.072774, loss: 4.6243
2022-07-31 13:14:15 - train: epoch 0001, iter [04200, 05004], lr: 0.073573, loss: 4.5320
2022-07-31 13:15:14 - train: epoch 0001, iter [04300, 05004], lr: 0.074373, loss: 4.3575
2022-07-31 13:16:13 - train: epoch 0001, iter [04400, 05004], lr: 0.075172, loss: 4.2659
2022-07-31 13:17:12 - train: epoch 0001, iter [04500, 05004], lr: 0.075971, loss: 4.3680
2022-07-31 13:18:10 - train: epoch 0001, iter [04600, 05004], lr: 0.076771, loss: 4.5302
2022-07-31 13:19:08 - train: epoch 0001, iter [04700, 05004], lr: 0.077570, loss: 4.2290
2022-07-31 13:20:07 - train: epoch 0001, iter [04800, 05004], lr: 0.078369, loss: 4.6180
2022-07-31 13:21:05 - train: epoch 0001, iter [04900, 05004], lr: 0.079169, loss: 4.3148
2022-07-31 13:22:01 - train: epoch 0001, iter [05000, 05004], lr: 0.079968, loss: 4.1972
2022-07-31 13:22:03 - train: epoch 001, train_loss: 5.2109
2022-07-31 13:24:11 - eval: epoch: 001, acc1: 20.060%, acc5: 42.602%, test_loss: 4.0010, per_image_load_time: 2.032ms, per_image_inference_time: 0.609ms
2022-07-31 13:24:11 - until epoch: 001, best_acc1: 20.060%
2022-07-31 13:24:11 - epoch 002 lr: 0.080008
2022-07-31 13:25:21 - train: epoch 0002, iter [00100, 05004], lr: 0.080799, loss: 4.1495
2022-07-31 13:26:20 - train: epoch 0002, iter [00200, 05004], lr: 0.081599, loss: 4.0243
2022-07-31 13:27:20 - train: epoch 0002, iter [00300, 05004], lr: 0.082398, loss: 4.3429
2022-07-31 13:28:18 - train: epoch 0002, iter [00400, 05004], lr: 0.083197, loss: 4.1203
2022-07-31 13:29:17 - train: epoch 0002, iter [00500, 05004], lr: 0.083997, loss: 4.0701
2022-07-31 13:30:14 - train: epoch 0002, iter [00600, 05004], lr: 0.084796, loss: 3.9161
2022-07-31 13:31:13 - train: epoch 0002, iter [00700, 05004], lr: 0.085596, loss: 4.4457
2022-07-31 13:32:12 - train: epoch 0002, iter [00800, 05004], lr: 0.086395, loss: 4.0234
2022-07-31 13:33:09 - train: epoch 0002, iter [00900, 05004], lr: 0.087194, loss: 3.8052
2022-07-31 13:34:08 - train: epoch 0002, iter [01000, 05004], lr: 0.087994, loss: 4.1907
2022-07-31 13:35:08 - train: epoch 0002, iter [01100, 05004], lr: 0.088793, loss: 3.9771
2022-07-31 13:36:09 - train: epoch 0002, iter [01200, 05004], lr: 0.089592, loss: 3.9692
2022-07-31 13:37:06 - train: epoch 0002, iter [01300, 05004], lr: 0.090392, loss: 3.9775
2022-07-31 13:38:05 - train: epoch 0002, iter [01400, 05004], lr: 0.091191, loss: 4.0785
2022-07-31 13:39:05 - train: epoch 0002, iter [01500, 05004], lr: 0.091990, loss: 4.0079
2022-07-31 13:40:05 - train: epoch 0002, iter [01600, 05004], lr: 0.092790, loss: 3.7463
2022-07-31 13:41:03 - train: epoch 0002, iter [01700, 05004], lr: 0.093589, loss: 4.0934
2022-07-31 13:42:01 - train: epoch 0002, iter [01800, 05004], lr: 0.094388, loss: 3.9043
2022-07-31 13:43:00 - train: epoch 0002, iter [01900, 05004], lr: 0.095188, loss: 3.7280
2022-07-31 13:43:58 - train: epoch 0002, iter [02000, 05004], lr: 0.095987, loss: 3.4369
2022-07-31 13:44:56 - train: epoch 0002, iter [02100, 05004], lr: 0.096787, loss: 3.8527
2022-07-31 13:45:54 - train: epoch 0002, iter [02200, 05004], lr: 0.097586, loss: 3.6658
2022-07-31 13:46:52 - train: epoch 0002, iter [02300, 05004], lr: 0.098385, loss: 3.7668
2022-07-31 13:47:49 - train: epoch 0002, iter [02400, 05004], lr: 0.099185, loss: 3.6796
2022-07-31 13:48:49 - train: epoch 0002, iter [02500, 05004], lr: 0.099984, loss: 3.6322
2022-07-31 13:49:47 - train: epoch 0002, iter [02600, 05004], lr: 0.100783, loss: 3.7046
2022-07-31 13:50:44 - train: epoch 0002, iter [02700, 05004], lr: 0.101583, loss: 3.9287
2022-07-31 13:51:43 - train: epoch 0002, iter [02800, 05004], lr: 0.102382, loss: 3.6982
2022-07-31 13:52:41 - train: epoch 0002, iter [02900, 05004], lr: 0.103181, loss: 3.8109
2022-07-31 13:53:38 - train: epoch 0002, iter [03000, 05004], lr: 0.103981, loss: 3.6706
2022-07-31 13:54:35 - train: epoch 0002, iter [03100, 05004], lr: 0.104780, loss: 3.7146
2022-07-31 13:55:33 - train: epoch 0002, iter [03200, 05004], lr: 0.105580, loss: 3.5192
2022-07-31 13:56:29 - train: epoch 0002, iter [03300, 05004], lr: 0.106379, loss: 3.5115
2022-07-31 13:57:26 - train: epoch 0002, iter [03400, 05004], lr: 0.107178, loss: 3.4386
2022-07-31 13:58:24 - train: epoch 0002, iter [03500, 05004], lr: 0.107978, loss: 3.5167
2022-07-31 13:59:21 - train: epoch 0002, iter [03600, 05004], lr: 0.108777, loss: 3.6672
2022-07-31 14:00:17 - train: epoch 0002, iter [03700, 05004], lr: 0.109576, loss: 3.5912
2022-07-31 14:01:14 - train: epoch 0002, iter [03800, 05004], lr: 0.110376, loss: 3.2885
2022-07-31 14:02:12 - train: epoch 0002, iter [03900, 05004], lr: 0.111175, loss: 3.5683
2022-07-31 14:03:12 - train: epoch 0002, iter [04000, 05004], lr: 0.111974, loss: 3.3741
2022-07-31 14:04:08 - train: epoch 0002, iter [04100, 05004], lr: 0.112774, loss: 3.5869
2022-07-31 14:05:05 - train: epoch 0002, iter [04200, 05004], lr: 0.113573, loss: 3.4528
2022-07-31 14:06:03 - train: epoch 0002, iter [04300, 05004], lr: 0.114373, loss: 3.4933
2022-07-31 14:07:01 - train: epoch 0002, iter [04400, 05004], lr: 0.115172, loss: 3.4259
2022-07-31 14:07:57 - train: epoch 0002, iter [04500, 05004], lr: 0.115971, loss: 3.4342
2022-07-31 14:08:55 - train: epoch 0002, iter [04600, 05004], lr: 0.116771, loss: 3.2349
2022-07-31 14:09:52 - train: epoch 0002, iter [04700, 05004], lr: 0.117570, loss: 3.4165
2022-07-31 14:10:48 - train: epoch 0002, iter [04800, 05004], lr: 0.118369, loss: 3.2950
2022-07-31 14:11:45 - train: epoch 0002, iter [04900, 05004], lr: 0.119169, loss: 3.3673
2022-07-31 14:12:40 - train: epoch 0002, iter [05000, 05004], lr: 0.119968, loss: 3.3299
2022-07-31 14:12:41 - train: epoch 002, train_loss: 3.7030
2022-07-31 14:14:49 - eval: epoch: 002, acc1: 33.016%, acc5: 59.388%, test_loss: 3.1165, per_image_load_time: 4.320ms, per_image_inference_time: 0.572ms
2022-07-31 14:14:49 - until epoch: 002, best_acc1: 33.016%
2022-07-31 14:14:49 - epoch 003 lr: 0.120008
2022-07-31 14:15:57 - train: epoch 0003, iter [00100, 05004], lr: 0.120799, loss: 3.2487
2022-07-31 14:16:56 - train: epoch 0003, iter [00200, 05004], lr: 0.121599, loss: 3.1280
2022-07-31 14:17:58 - train: epoch 0003, iter [00300, 05004], lr: 0.122398, loss: 3.4509
2022-07-31 14:18:55 - train: epoch 0003, iter [00400, 05004], lr: 0.123197, loss: 3.3396
2022-07-31 14:19:55 - train: epoch 0003, iter [00500, 05004], lr: 0.123997, loss: 3.2582
2022-07-31 14:20:54 - train: epoch 0003, iter [00600, 05004], lr: 0.124796, loss: 3.2284
2022-07-31 14:21:50 - train: epoch 0003, iter [00700, 05004], lr: 0.125596, loss: 3.3115
2022-07-31 14:22:47 - train: epoch 0003, iter [00800, 05004], lr: 0.126395, loss: 3.3570
2022-07-31 14:23:46 - train: epoch 0003, iter [00900, 05004], lr: 0.127194, loss: 3.1801
2022-07-31 14:24:43 - train: epoch 0003, iter [01000, 05004], lr: 0.127994, loss: 3.3803
2022-07-31 14:25:41 - train: epoch 0003, iter [01100, 05004], lr: 0.128793, loss: 3.1675
2022-07-31 14:26:40 - train: epoch 0003, iter [01200, 05004], lr: 0.129592, loss: 3.1864
2022-07-31 14:27:38 - train: epoch 0003, iter [01300, 05004], lr: 0.130392, loss: 3.0668
2022-07-31 14:28:37 - train: epoch 0003, iter [01400, 05004], lr: 0.131191, loss: 3.0314
2022-07-31 14:29:34 - train: epoch 0003, iter [01500, 05004], lr: 0.131990, loss: 3.5046
2022-07-31 14:30:35 - train: epoch 0003, iter [01600, 05004], lr: 0.132790, loss: 3.1395
2022-07-31 14:31:32 - train: epoch 0003, iter [01700, 05004], lr: 0.133589, loss: 3.3015
2022-07-31 14:32:31 - train: epoch 0003, iter [01800, 05004], lr: 0.134388, loss: 3.0797
2022-07-31 14:33:29 - train: epoch 0003, iter [01900, 05004], lr: 0.135188, loss: 3.1955
2022-07-31 14:34:27 - train: epoch 0003, iter [02000, 05004], lr: 0.135987, loss: 3.1067
2022-07-31 14:35:25 - train: epoch 0003, iter [02100, 05004], lr: 0.136787, loss: 3.2980
2022-07-31 14:36:23 - train: epoch 0003, iter [02200, 05004], lr: 0.137586, loss: 3.5645
2022-07-31 14:37:20 - train: epoch 0003, iter [02300, 05004], lr: 0.138385, loss: 3.0535
2022-07-31 14:38:18 - train: epoch 0003, iter [02400, 05004], lr: 0.139185, loss: 2.9139
2022-07-31 14:39:16 - train: epoch 0003, iter [02500, 05004], lr: 0.139984, loss: 3.2482
2022-07-31 14:40:14 - train: epoch 0003, iter [02600, 05004], lr: 0.140783, loss: 3.2137
2022-07-31 14:41:14 - train: epoch 0003, iter [02700, 05004], lr: 0.141583, loss: 3.3094
2022-07-31 14:42:10 - train: epoch 0003, iter [02800, 05004], lr: 0.142382, loss: 3.1644
2022-07-31 14:43:09 - train: epoch 0003, iter [02900, 05004], lr: 0.143181, loss: 3.1740
2022-07-31 14:44:08 - train: epoch 0003, iter [03000, 05004], lr: 0.143981, loss: 3.0922
2022-07-31 14:45:07 - train: epoch 0003, iter [03100, 05004], lr: 0.144780, loss: 3.3758
2022-07-31 14:46:05 - train: epoch 0003, iter [03200, 05004], lr: 0.145580, loss: 3.2384
2022-07-31 14:47:02 - train: epoch 0003, iter [03300, 05004], lr: 0.146379, loss: 3.0537
2022-07-31 14:48:00 - train: epoch 0003, iter [03400, 05004], lr: 0.147178, loss: 3.2473
2022-07-31 14:48:58 - train: epoch 0003, iter [03500, 05004], lr: 0.147978, loss: 2.9668
2022-07-31 14:49:56 - train: epoch 0003, iter [03600, 05004], lr: 0.148777, loss: 3.0194
2022-07-31 14:50:54 - train: epoch 0003, iter [03700, 05004], lr: 0.149576, loss: 3.0750
2022-07-31 14:51:52 - train: epoch 0003, iter [03800, 05004], lr: 0.150376, loss: 3.1193
2022-07-31 14:52:49 - train: epoch 0003, iter [03900, 05004], lr: 0.151175, loss: 3.3469
2022-07-31 14:53:47 - train: epoch 0003, iter [04000, 05004], lr: 0.151974, loss: 2.6945
2022-07-31 14:54:46 - train: epoch 0003, iter [04100, 05004], lr: 0.152774, loss: 2.8425
2022-07-31 14:55:45 - train: epoch 0003, iter [04200, 05004], lr: 0.153573, loss: 2.9739
2022-07-31 14:56:44 - train: epoch 0003, iter [04300, 05004], lr: 0.154373, loss: 2.8747
2022-07-31 14:57:44 - train: epoch 0003, iter [04400, 05004], lr: 0.155172, loss: 2.7768
2022-07-31 14:58:40 - train: epoch 0003, iter [04500, 05004], lr: 0.155971, loss: 3.0606
2022-07-31 14:59:39 - train: epoch 0003, iter [04600, 05004], lr: 0.156771, loss: 2.8207
2022-07-31 15:00:39 - train: epoch 0003, iter [04700, 05004], lr: 0.157570, loss: 3.0700
2022-07-31 15:01:38 - train: epoch 0003, iter [04800, 05004], lr: 0.158369, loss: 3.1781
2022-07-31 15:02:34 - train: epoch 0003, iter [04900, 05004], lr: 0.159169, loss: 3.2138
2022-07-31 15:03:30 - train: epoch 0003, iter [05000, 05004], lr: 0.159968, loss: 2.9729
2022-07-31 15:03:32 - train: epoch 003, train_loss: 3.1546
2022-07-31 15:05:40 - eval: epoch: 003, acc1: 36.130%, acc5: 62.080%, test_loss: 2.9444, per_image_load_time: 3.436ms, per_image_inference_time: 0.580ms
2022-07-31 15:05:40 - until epoch: 003, best_acc1: 36.130%
2022-07-31 15:05:40 - epoch 004 lr: 0.160008
2022-07-31 15:06:48 - train: epoch 0004, iter [00100, 05004], lr: 0.160799, loss: 3.0782
2022-07-31 15:07:46 - train: epoch 0004, iter [00200, 05004], lr: 0.161599, loss: 2.8181
2022-07-31 15:08:44 - train: epoch 0004, iter [00300, 05004], lr: 0.162398, loss: 2.7229
2022-07-31 15:09:40 - train: epoch 0004, iter [00400, 05004], lr: 0.163197, loss: 3.0283
2022-07-31 15:10:37 - train: epoch 0004, iter [00500, 05004], lr: 0.163997, loss: 2.8204
2022-07-31 15:11:33 - train: epoch 0004, iter [00600, 05004], lr: 0.164796, loss: 3.3137
2022-07-31 15:12:31 - train: epoch 0004, iter [00700, 05004], lr: 0.165596, loss: 2.9371
2022-07-31 15:13:27 - train: epoch 0004, iter [00800, 05004], lr: 0.166395, loss: 2.9937
2022-07-31 15:14:23 - train: epoch 0004, iter [00900, 05004], lr: 0.167194, loss: 2.7201
2022-07-31 15:15:20 - train: epoch 0004, iter [01000, 05004], lr: 0.167994, loss: 2.9823
2022-07-31 15:16:19 - train: epoch 0004, iter [01100, 05004], lr: 0.168793, loss: 3.1873
2022-07-31 15:17:17 - train: epoch 0004, iter [01200, 05004], lr: 0.169592, loss: 2.7042
2022-07-31 15:18:16 - train: epoch 0004, iter [01300, 05004], lr: 0.170392, loss: 2.9347
2022-07-31 15:19:13 - train: epoch 0004, iter [01400, 05004], lr: 0.171191, loss: 3.0169
2022-07-31 15:20:13 - train: epoch 0004, iter [01500, 05004], lr: 0.171990, loss: 3.0669
2022-07-31 15:21:11 - train: epoch 0004, iter [01600, 05004], lr: 0.172790, loss: 2.8855
2022-07-31 15:22:10 - train: epoch 0004, iter [01700, 05004], lr: 0.173589, loss: 2.8290
2022-07-31 15:23:08 - train: epoch 0004, iter [01800, 05004], lr: 0.174388, loss: 3.1138
2022-07-31 15:24:05 - train: epoch 0004, iter [01900, 05004], lr: 0.175188, loss: 2.9510
2022-07-31 15:25:07 - train: epoch 0004, iter [02000, 05004], lr: 0.175987, loss: 2.7771
2022-07-31 15:26:03 - train: epoch 0004, iter [02100, 05004], lr: 0.176787, loss: 3.0881
2022-07-31 15:27:01 - train: epoch 0004, iter [02200, 05004], lr: 0.177586, loss: 2.9723
2022-07-31 15:28:00 - train: epoch 0004, iter [02300, 05004], lr: 0.178385, loss: 2.5304
2022-07-31 15:28:58 - train: epoch 0004, iter [02400, 05004], lr: 0.179185, loss: 2.8038
2022-07-31 15:29:59 - train: epoch 0004, iter [02500, 05004], lr: 0.179984, loss: 2.7978
2022-07-31 15:30:57 - train: epoch 0004, iter [02600, 05004], lr: 0.180783, loss: 3.1349
2022-07-31 15:31:55 - train: epoch 0004, iter [02700, 05004], lr: 0.181583, loss: 2.6232
2022-07-31 15:32:55 - train: epoch 0004, iter [02800, 05004], lr: 0.182382, loss: 2.8451
2022-07-31 15:33:55 - train: epoch 0004, iter [02900, 05004], lr: 0.183181, loss: 2.9080
2022-07-31 15:34:53 - train: epoch 0004, iter [03000, 05004], lr: 0.183981, loss: 2.6712
2022-07-31 15:35:51 - train: epoch 0004, iter [03100, 05004], lr: 0.184780, loss: 3.0660
2022-07-31 15:36:49 - train: epoch 0004, iter [03200, 05004], lr: 0.185580, loss: 2.9996
2022-07-31 15:37:48 - train: epoch 0004, iter [03300, 05004], lr: 0.186379, loss: 2.9160
2022-07-31 15:38:47 - train: epoch 0004, iter [03400, 05004], lr: 0.187178, loss: 3.0718
2022-07-31 15:39:44 - train: epoch 0004, iter [03500, 05004], lr: 0.187978, loss: 2.8653
2022-07-31 15:40:44 - train: epoch 0004, iter [03600, 05004], lr: 0.188777, loss: 2.8646
2022-07-31 15:41:41 - train: epoch 0004, iter [03700, 05004], lr: 0.189576, loss: 2.9405
2022-07-31 15:42:41 - train: epoch 0004, iter [03800, 05004], lr: 0.190376, loss: 2.6409
2022-07-31 15:43:36 - train: epoch 0004, iter [03900, 05004], lr: 0.191175, loss: 2.7442
2022-07-31 15:44:33 - train: epoch 0004, iter [04000, 05004], lr: 0.191974, loss: 2.5058
2022-07-31 15:45:30 - train: epoch 0004, iter [04100, 05004], lr: 0.192774, loss: 2.7846
2022-07-31 15:46:29 - train: epoch 0004, iter [04200, 05004], lr: 0.193573, loss: 2.7114
2022-07-31 15:47:27 - train: epoch 0004, iter [04300, 05004], lr: 0.194373, loss: 2.4855
2022-07-31 15:48:25 - train: epoch 0004, iter [04400, 05004], lr: 0.195172, loss: 2.7774
2022-07-31 15:49:24 - train: epoch 0004, iter [04500, 05004], lr: 0.195971, loss: 2.4463
2022-07-31 15:50:23 - train: epoch 0004, iter [04600, 05004], lr: 0.196771, loss: 2.7432
2022-07-31 15:51:20 - train: epoch 0004, iter [04700, 05004], lr: 0.197570, loss: 2.9336
2022-07-31 15:52:19 - train: epoch 0004, iter [04800, 05004], lr: 0.198369, loss: 2.5944
2022-07-31 15:53:16 - train: epoch 0004, iter [04900, 05004], lr: 0.199169, loss: 2.9228
2022-07-31 15:54:12 - train: epoch 0004, iter [05000, 05004], lr: 0.199968, loss: 3.0169
2022-07-31 15:54:13 - train: epoch 004, train_loss: 2.8921
2022-07-31 15:56:22 - eval: epoch: 004, acc1: 39.956%, acc5: 66.478%, test_loss: 2.7240, per_image_load_time: 4.304ms, per_image_inference_time: 0.617ms
2022-07-31 15:56:22 - until epoch: 004, best_acc1: 39.956%
2022-07-31 15:56:22 - epoch 005 lr: 0.200008
2022-07-31 15:57:29 - train: epoch 0005, iter [00100, 05004], lr: 0.200799, loss: 2.5345
2022-07-31 15:58:28 - train: epoch 0005, iter [00200, 05004], lr: 0.201599, loss: 2.8784
2022-07-31 15:59:25 - train: epoch 0005, iter [00300, 05004], lr: 0.202398, loss: 3.0220
2022-07-31 16:00:22 - train: epoch 0005, iter [00400, 05004], lr: 0.203197, loss: 2.8282
2022-07-31 16:01:22 - train: epoch 0005, iter [00500, 05004], lr: 0.203997, loss: 2.7742
2022-07-31 16:02:22 - train: epoch 0005, iter [00600, 05004], lr: 0.204796, loss: 2.7677
2022-07-31 16:03:21 - train: epoch 0005, iter [00700, 05004], lr: 0.205596, loss: 2.9350
2022-07-31 16:04:21 - train: epoch 0005, iter [00800, 05004], lr: 0.206395, loss: 2.9114
2022-07-31 16:05:18 - train: epoch 0005, iter [00900, 05004], lr: 0.207194, loss: 2.8698
2022-07-31 16:06:17 - train: epoch 0005, iter [01000, 05004], lr: 0.207994, loss: 2.6588
2022-07-31 16:07:15 - train: epoch 0005, iter [01100, 05004], lr: 0.208793, loss: 2.7989
2022-07-31 16:08:13 - train: epoch 0005, iter [01200, 05004], lr: 0.209592, loss: 2.7574
2022-07-31 16:09:11 - train: epoch 0005, iter [01300, 05004], lr: 0.210392, loss: 2.7163
2022-07-31 16:10:11 - train: epoch 0005, iter [01400, 05004], lr: 0.211191, loss: 3.0223
2022-07-31 16:11:08 - train: epoch 0005, iter [01500, 05004], lr: 0.211990, loss: 2.6267
2022-07-31 16:12:06 - train: epoch 0005, iter [01600, 05004], lr: 0.212790, loss: 2.4563
2022-07-31 16:13:04 - train: epoch 0005, iter [01700, 05004], lr: 0.213589, loss: 2.7746
2022-07-31 16:14:04 - train: epoch 0005, iter [01800, 05004], lr: 0.214388, loss: 2.8105
2022-07-31 16:15:01 - train: epoch 0005, iter [01900, 05004], lr: 0.215188, loss: 2.7757
2022-07-31 16:16:00 - train: epoch 0005, iter [02000, 05004], lr: 0.215987, loss: 2.6959
2022-07-31 16:17:00 - train: epoch 0005, iter [02100, 05004], lr: 0.216787, loss: 2.5368
2022-07-31 16:17:59 - train: epoch 0005, iter [02200, 05004], lr: 0.217586, loss: 2.7272
2022-07-31 16:18:56 - train: epoch 0005, iter [02300, 05004], lr: 0.218385, loss: 2.3816
2022-07-31 16:19:55 - train: epoch 0005, iter [02400, 05004], lr: 0.219185, loss: 2.7493
2022-07-31 16:20:53 - train: epoch 0005, iter [02500, 05004], lr: 0.219984, loss: 2.6590
2022-07-31 16:21:50 - train: epoch 0005, iter [02600, 05004], lr: 0.220783, loss: 2.8096
2022-07-31 16:22:50 - train: epoch 0005, iter [02700, 05004], lr: 0.221583, loss: 3.0061
2022-07-31 16:23:49 - train: epoch 0005, iter [02800, 05004], lr: 0.222382, loss: 2.8822
2022-07-31 16:24:47 - train: epoch 0005, iter [02900, 05004], lr: 0.223181, loss: 2.4754
2022-07-31 16:25:44 - train: epoch 0005, iter [03000, 05004], lr: 0.223981, loss: 2.7587
2022-07-31 16:26:40 - train: epoch 0005, iter [03100, 05004], lr: 0.224780, loss: 2.6625
2022-07-31 16:27:39 - train: epoch 0005, iter [03200, 05004], lr: 0.225580, loss: 2.6894
2022-07-31 16:28:36 - train: epoch 0005, iter [03300, 05004], lr: 0.226379, loss: 2.7575
2022-07-31 16:29:35 - train: epoch 0005, iter [03400, 05004], lr: 0.227178, loss: 2.6168
2022-07-31 16:30:33 - train: epoch 0005, iter [03500, 05004], lr: 0.227978, loss: 2.6988
2022-07-31 16:31:30 - train: epoch 0005, iter [03600, 05004], lr: 0.228777, loss: 2.9016
2022-07-31 16:32:27 - train: epoch 0005, iter [03700, 05004], lr: 0.229576, loss: 2.7662
2022-07-31 16:33:24 - train: epoch 0005, iter [03800, 05004], lr: 0.230376, loss: 2.9404
2022-07-31 16:34:25 - train: epoch 0005, iter [03900, 05004], lr: 0.231175, loss: 2.9123
2022-07-31 16:35:22 - train: epoch 0005, iter [04000, 05004], lr: 0.231974, loss: 2.8836
2022-07-31 16:36:20 - train: epoch 0005, iter [04100, 05004], lr: 0.232774, loss: 2.7407
2022-07-31 16:37:19 - train: epoch 0005, iter [04200, 05004], lr: 0.233573, loss: 2.8065
2022-07-31 16:38:17 - train: epoch 0005, iter [04300, 05004], lr: 0.234373, loss: 3.0347
2022-07-31 16:39:14 - train: epoch 0005, iter [04400, 05004], lr: 0.235172, loss: 2.9408
2022-07-31 16:40:13 - train: epoch 0005, iter [04500, 05004], lr: 0.235971, loss: 2.8565
2022-07-31 16:41:10 - train: epoch 0005, iter [04600, 05004], lr: 0.236771, loss: 2.7826
2022-07-31 16:42:10 - train: epoch 0005, iter [04700, 05004], lr: 0.237570, loss: 2.4265
2022-07-31 16:43:07 - train: epoch 0005, iter [04800, 05004], lr: 0.238369, loss: 2.4754
2022-07-31 16:44:07 - train: epoch 0005, iter [04900, 05004], lr: 0.239169, loss: 2.9529
2022-07-31 16:45:01 - train: epoch 0005, iter [05000, 05004], lr: 0.239968, loss: 2.6045
2022-07-31 16:45:02 - train: epoch 005, train_loss: 2.7662
2022-07-31 16:47:12 - eval: epoch: 005, acc1: 44.610%, acc5: 71.114%, test_loss: 2.4498, per_image_load_time: 4.429ms, per_image_inference_time: 0.598ms
2022-07-31 16:47:12 - until epoch: 005, best_acc1: 44.610%
2022-07-31 16:47:12 - epoch 006 lr: 0.200000
2022-07-31 16:48:22 - train: epoch 0006, iter [00100, 05004], lr: 0.200000, loss: 2.6725
2022-07-31 16:49:22 - train: epoch 0006, iter [00200, 05004], lr: 0.200000, loss: 2.5875
2022-07-31 16:50:19 - train: epoch 0006, iter [00300, 05004], lr: 0.200000, loss: 2.5016
2022-07-31 16:51:17 - train: epoch 0006, iter [00400, 05004], lr: 0.200000, loss: 2.6781
2022-07-31 16:52:15 - train: epoch 0006, iter [00500, 05004], lr: 0.199999, loss: 2.6343
2022-07-31 16:53:11 - train: epoch 0006, iter [00600, 05004], lr: 0.199999, loss: 2.5546
2022-07-31 16:54:10 - train: epoch 0006, iter [00700, 05004], lr: 0.199999, loss: 2.6958
2022-07-31 16:55:06 - train: epoch 0006, iter [00800, 05004], lr: 0.199999, loss: 2.4214
2022-07-31 16:56:06 - train: epoch 0006, iter [00900, 05004], lr: 0.199998, loss: 2.3916
2022-07-31 16:57:04 - train: epoch 0006, iter [01000, 05004], lr: 0.199998, loss: 2.4760
2022-07-31 16:58:01 - train: epoch 0006, iter [01100, 05004], lr: 0.199997, loss: 2.6035
2022-07-31 16:58:58 - train: epoch 0006, iter [01200, 05004], lr: 0.199997, loss: 2.7325
2022-07-31 16:59:57 - train: epoch 0006, iter [01300, 05004], lr: 0.199996, loss: 2.4951
2022-07-31 17:00:55 - train: epoch 0006, iter [01400, 05004], lr: 0.199996, loss: 2.6032
2022-07-31 17:01:54 - train: epoch 0006, iter [01500, 05004], lr: 0.199995, loss: 2.8232
2022-07-31 17:02:52 - train: epoch 0006, iter [01600, 05004], lr: 0.199994, loss: 2.5152
2022-07-31 17:03:51 - train: epoch 0006, iter [01700, 05004], lr: 0.199994, loss: 2.5148
2022-07-31 17:04:48 - train: epoch 0006, iter [01800, 05004], lr: 0.199993, loss: 2.5441
2022-07-31 17:05:47 - train: epoch 0006, iter [01900, 05004], lr: 0.199992, loss: 2.5817
2022-07-31 17:06:46 - train: epoch 0006, iter [02000, 05004], lr: 0.199991, loss: 2.9248
2022-07-31 17:07:46 - train: epoch 0006, iter [02100, 05004], lr: 0.199990, loss: 2.6859
2022-07-31 17:08:44 - train: epoch 0006, iter [02200, 05004], lr: 0.199989, loss: 2.3886
2022-07-31 17:09:41 - train: epoch 0006, iter [02300, 05004], lr: 0.199988, loss: 2.5539
2022-07-31 17:10:39 - train: epoch 0006, iter [02400, 05004], lr: 0.199987, loss: 2.7240
2022-07-31 17:11:38 - train: epoch 0006, iter [02500, 05004], lr: 0.199986, loss: 2.5700
2022-07-31 17:12:37 - train: epoch 0006, iter [02600, 05004], lr: 0.199985, loss: 2.3702
2022-07-31 17:13:36 - train: epoch 0006, iter [02700, 05004], lr: 0.199984, loss: 2.6399
2022-07-31 17:14:35 - train: epoch 0006, iter [02800, 05004], lr: 0.199983, loss: 2.3171
2022-07-31 17:15:31 - train: epoch 0006, iter [02900, 05004], lr: 0.199982, loss: 2.6000
2022-07-31 17:16:29 - train: epoch 0006, iter [03000, 05004], lr: 0.199980, loss: 2.5597
2022-07-31 17:17:27 - train: epoch 0006, iter [03100, 05004], lr: 0.199979, loss: 2.5091
2022-07-31 17:18:26 - train: epoch 0006, iter [03200, 05004], lr: 0.199978, loss: 2.4900
2022-07-31 17:19:23 - train: epoch 0006, iter [03300, 05004], lr: 0.199976, loss: 2.5204
2022-07-31 17:20:20 - train: epoch 0006, iter [03400, 05004], lr: 0.199975, loss: 2.6670
2022-07-31 17:21:19 - train: epoch 0006, iter [03500, 05004], lr: 0.199973, loss: 2.6770
2022-07-31 17:22:18 - train: epoch 0006, iter [03600, 05004], lr: 0.199972, loss: 2.6686
2022-07-31 17:23:15 - train: epoch 0006, iter [03700, 05004], lr: 0.199970, loss: 2.6357
2022-07-31 17:24:13 - train: epoch 0006, iter [03800, 05004], lr: 0.199968, loss: 2.5161
2022-07-31 17:25:12 - train: epoch 0006, iter [03900, 05004], lr: 0.199967, loss: 2.5279
2022-07-31 17:26:10 - train: epoch 0006, iter [04000, 05004], lr: 0.199965, loss: 2.6397
2022-07-31 17:27:08 - train: epoch 0006, iter [04100, 05004], lr: 0.199963, loss: 2.5650
2022-07-31 17:28:06 - train: epoch 0006, iter [04200, 05004], lr: 0.199961, loss: 2.3517
2022-07-31 17:29:04 - train: epoch 0006, iter [04300, 05004], lr: 0.199960, loss: 2.5414
2022-07-31 17:30:02 - train: epoch 0006, iter [04400, 05004], lr: 0.199958, loss: 2.3708
2022-07-31 17:31:00 - train: epoch 0006, iter [04500, 05004], lr: 0.199956, loss: 2.6733
2022-07-31 17:31:59 - train: epoch 0006, iter [04600, 05004], lr: 0.199954, loss: 2.5839
2022-07-31 17:32:57 - train: epoch 0006, iter [04700, 05004], lr: 0.199952, loss: 2.5892
2022-07-31 17:33:54 - train: epoch 0006, iter [04800, 05004], lr: 0.199950, loss: 2.5950
2022-07-31 17:34:53 - train: epoch 0006, iter [04900, 05004], lr: 0.199948, loss: 2.6916
2022-07-31 17:35:48 - train: epoch 0006, iter [05000, 05004], lr: 0.199945, loss: 2.5195
2022-07-31 17:35:49 - train: epoch 006, train_loss: 2.5894
2022-07-31 17:37:59 - eval: epoch: 006, acc1: 48.438%, acc5: 74.290%, test_loss: 2.2566, per_image_load_time: 3.666ms, per_image_inference_time: 0.613ms
2022-07-31 17:38:00 - until epoch: 006, best_acc1: 48.438%
2022-07-31 17:38:00 - epoch 007 lr: 0.199945
2022-07-31 17:39:09 - train: epoch 0007, iter [00100, 05004], lr: 0.199943, loss: 2.3125
2022-07-31 17:40:08 - train: epoch 0007, iter [00200, 05004], lr: 0.199941, loss: 2.7637
2022-07-31 17:41:07 - train: epoch 0007, iter [00300, 05004], lr: 0.199939, loss: 2.7594
2022-07-31 17:42:06 - train: epoch 0007, iter [00400, 05004], lr: 0.199936, loss: 2.6555
2022-07-31 17:43:03 - train: epoch 0007, iter [00500, 05004], lr: 0.199934, loss: 2.2052
2022-07-31 17:44:00 - train: epoch 0007, iter [00600, 05004], lr: 0.199931, loss: 2.4662
2022-07-31 17:44:59 - train: epoch 0007, iter [00700, 05004], lr: 0.199929, loss: 2.5451
2022-07-31 17:45:56 - train: epoch 0007, iter [00800, 05004], lr: 0.199926, loss: 2.5298
2022-07-31 17:46:55 - train: epoch 0007, iter [00900, 05004], lr: 0.199924, loss: 2.4806
2022-07-31 17:47:54 - train: epoch 0007, iter [01000, 05004], lr: 0.199921, loss: 2.7543
2022-07-31 17:48:51 - train: epoch 0007, iter [01100, 05004], lr: 0.199919, loss: 2.5362
2022-07-31 17:49:51 - train: epoch 0007, iter [01200, 05004], lr: 0.199916, loss: 2.5051
2022-07-31 17:50:49 - train: epoch 0007, iter [01300, 05004], lr: 0.199913, loss: 2.3210
2022-07-31 17:51:47 - train: epoch 0007, iter [01400, 05004], lr: 0.199910, loss: 2.5260
2022-07-31 17:52:45 - train: epoch 0007, iter [01500, 05004], lr: 0.199908, loss: 2.7154
2022-07-31 17:53:44 - train: epoch 0007, iter [01600, 05004], lr: 0.199905, loss: 2.6763
2022-07-31 17:54:43 - train: epoch 0007, iter [01700, 05004], lr: 0.199902, loss: 2.4106
2022-07-31 17:55:41 - train: epoch 0007, iter [01800, 05004], lr: 0.199899, loss: 2.5588
2022-07-31 17:56:40 - train: epoch 0007, iter [01900, 05004], lr: 0.199896, loss: 2.7266
2022-07-31 17:57:39 - train: epoch 0007, iter [02000, 05004], lr: 0.199893, loss: 2.4349
2022-07-31 17:58:38 - train: epoch 0007, iter [02100, 05004], lr: 0.199890, loss: 2.7408
2022-07-31 17:59:36 - train: epoch 0007, iter [02200, 05004], lr: 0.199887, loss: 2.3852
2022-07-31 18:00:37 - train: epoch 0007, iter [02300, 05004], lr: 0.199884, loss: 2.5447
2022-07-31 18:01:34 - train: epoch 0007, iter [02400, 05004], lr: 0.199880, loss: 2.7712
2022-07-31 18:02:32 - train: epoch 0007, iter [02500, 05004], lr: 0.199877, loss: 2.4820
2022-07-31 18:03:31 - train: epoch 0007, iter [02600, 05004], lr: 0.199874, loss: 2.4675
2022-07-31 18:04:31 - train: epoch 0007, iter [02700, 05004], lr: 0.199870, loss: 2.4396
2022-07-31 18:05:30 - train: epoch 0007, iter [02800, 05004], lr: 0.199867, loss: 2.6212
2022-07-31 18:06:30 - train: epoch 0007, iter [02900, 05004], lr: 0.199864, loss: 2.4306
2022-07-31 18:07:27 - train: epoch 0007, iter [03000, 05004], lr: 0.199860, loss: 2.5740
2022-07-31 18:08:27 - train: epoch 0007, iter [03100, 05004], lr: 0.199857, loss: 2.4793
2022-07-31 18:09:25 - train: epoch 0007, iter [03200, 05004], lr: 0.199853, loss: 2.5743
2022-07-31 18:10:23 - train: epoch 0007, iter [03300, 05004], lr: 0.199849, loss: 2.7487
2022-07-31 18:11:22 - train: epoch 0007, iter [03400, 05004], lr: 0.199846, loss: 2.6552
2022-07-31 18:12:21 - train: epoch 0007, iter [03500, 05004], lr: 0.199842, loss: 2.4418
2022-07-31 18:13:19 - train: epoch 0007, iter [03600, 05004], lr: 0.199838, loss: 2.2450
2022-07-31 18:14:16 - train: epoch 0007, iter [03700, 05004], lr: 0.199835, loss: 2.5395
2022-07-31 18:15:16 - train: epoch 0007, iter [03800, 05004], lr: 0.199831, loss: 2.6259
2022-07-31 18:16:15 - train: epoch 0007, iter [03900, 05004], lr: 0.199827, loss: 2.5704
2022-07-31 18:17:11 - train: epoch 0007, iter [04000, 05004], lr: 0.199823, loss: 2.4690
2022-07-31 18:18:09 - train: epoch 0007, iter [04100, 05004], lr: 0.199819, loss: 2.4699
2022-07-31 18:19:07 - train: epoch 0007, iter [04200, 05004], lr: 0.199815, loss: 2.4017
2022-07-31 18:20:06 - train: epoch 0007, iter [04300, 05004], lr: 0.199811, loss: 2.5054
2022-07-31 18:21:04 - train: epoch 0007, iter [04400, 05004], lr: 0.199807, loss: 2.4117
2022-07-31 18:22:03 - train: epoch 0007, iter [04500, 05004], lr: 0.199803, loss: 2.5179
2022-07-31 18:23:00 - train: epoch 0007, iter [04600, 05004], lr: 0.199799, loss: 2.6416
2022-07-31 18:23:59 - train: epoch 0007, iter [04700, 05004], lr: 0.199794, loss: 2.4326
2022-07-31 18:24:57 - train: epoch 0007, iter [04800, 05004], lr: 0.199790, loss: 2.7760
2022-07-31 18:25:55 - train: epoch 0007, iter [04900, 05004], lr: 0.199786, loss: 2.5780
2022-07-31 18:26:50 - train: epoch 0007, iter [05000, 05004], lr: 0.199782, loss: 2.6690
2022-07-31 18:26:52 - train: epoch 007, train_loss: 2.5125
2022-07-31 18:29:04 - eval: epoch: 007, acc1: 49.552%, acc5: 75.372%, test_loss: 2.1868, per_image_load_time: 3.837ms, per_image_inference_time: 0.636ms
2022-07-31 18:29:04 - until epoch: 007, best_acc1: 49.552%
2022-07-31 18:29:04 - epoch 008 lr: 0.199781
2022-07-31 18:30:11 - train: epoch 0008, iter [00100, 05004], lr: 0.199777, loss: 2.5526
2022-07-31 18:31:09 - train: epoch 0008, iter [00200, 05004], lr: 0.199773, loss: 2.5718
2022-07-31 18:32:06 - train: epoch 0008, iter [00300, 05004], lr: 0.199768, loss: 2.5425
2022-07-31 18:33:03 - train: epoch 0008, iter [00400, 05004], lr: 0.199764, loss: 2.5473
2022-07-31 18:34:01 - train: epoch 0008, iter [00500, 05004], lr: 0.199759, loss: 2.2921
2022-07-31 18:34:59 - train: epoch 0008, iter [00600, 05004], lr: 0.199754, loss: 2.5922
2022-07-31 18:35:56 - train: epoch 0008, iter [00700, 05004], lr: 0.199750, loss: 2.4704
2022-07-31 18:36:55 - train: epoch 0008, iter [00800, 05004], lr: 0.199745, loss: 2.3497
2022-07-31 18:37:53 - train: epoch 0008, iter [00900, 05004], lr: 0.199740, loss: 2.3499
2022-07-31 18:38:50 - train: epoch 0008, iter [01000, 05004], lr: 0.199736, loss: 2.3812
2022-07-31 18:39:49 - train: epoch 0008, iter [01100, 05004], lr: 0.199731, loss: 2.2804
2022-07-31 18:40:47 - train: epoch 0008, iter [01200, 05004], lr: 0.199726, loss: 2.3218
2022-07-31 18:41:45 - train: epoch 0008, iter [01300, 05004], lr: 0.199721, loss: 2.7206
2022-07-31 18:42:45 - train: epoch 0008, iter [01400, 05004], lr: 0.199716, loss: 2.2726
2022-07-31 18:43:43 - train: epoch 0008, iter [01500, 05004], lr: 0.199711, loss: 2.4267
2022-07-31 18:44:43 - train: epoch 0008, iter [01600, 05004], lr: 0.199706, loss: 2.4943
2022-07-31 18:45:41 - train: epoch 0008, iter [01700, 05004], lr: 0.199701, loss: 2.5364
2022-07-31 18:46:40 - train: epoch 0008, iter [01800, 05004], lr: 0.199696, loss: 2.4206
2022-07-31 18:47:38 - train: epoch 0008, iter [01900, 05004], lr: 0.199691, loss: 2.4367
2022-07-31 18:48:36 - train: epoch 0008, iter [02000, 05004], lr: 0.199685, loss: 2.4886
2022-07-31 18:49:35 - train: epoch 0008, iter [02100, 05004], lr: 0.199680, loss: 2.4193
2022-07-31 18:50:34 - train: epoch 0008, iter [02200, 05004], lr: 0.199675, loss: 2.4062
2022-07-31 18:51:32 - train: epoch 0008, iter [02300, 05004], lr: 0.199669, loss: 2.2211
2022-07-31 18:52:31 - train: epoch 0008, iter [02400, 05004], lr: 0.199664, loss: 2.2648
2022-07-31 18:53:28 - train: epoch 0008, iter [02500, 05004], lr: 0.199659, loss: 2.5063
2022-07-31 18:54:27 - train: epoch 0008, iter [02600, 05004], lr: 0.199653, loss: 2.4351
2022-07-31 18:55:25 - train: epoch 0008, iter [02700, 05004], lr: 0.199648, loss: 2.6726
2022-07-31 18:56:23 - train: epoch 0008, iter [02800, 05004], lr: 0.199642, loss: 2.5406
2022-07-31 18:57:21 - train: epoch 0008, iter [02900, 05004], lr: 0.199636, loss: 2.4742
2022-07-31 18:58:20 - train: epoch 0008, iter [03000, 05004], lr: 0.199631, loss: 2.4918
2022-07-31 18:59:17 - train: epoch 0008, iter [03100, 05004], lr: 0.199625, loss: 2.3878
2022-07-31 19:00:16 - train: epoch 0008, iter [03200, 05004], lr: 0.199619, loss: 2.8198
2022-07-31 19:01:14 - train: epoch 0008, iter [03300, 05004], lr: 0.199614, loss: 2.5593
2022-07-31 19:02:12 - train: epoch 0008, iter [03400, 05004], lr: 0.199608, loss: 2.4697
2022-07-31 19:03:11 - train: epoch 0008, iter [03500, 05004], lr: 0.199602, loss: 2.3936
2022-07-31 19:04:12 - train: epoch 0008, iter [03600, 05004], lr: 0.199596, loss: 2.4840
2022-07-31 19:05:09 - train: epoch 0008, iter [03700, 05004], lr: 0.199590, loss: 2.4283
2022-07-31 19:06:09 - train: epoch 0008, iter [03800, 05004], lr: 0.199584, loss: 2.3645
2022-07-31 19:07:06 - train: epoch 0008, iter [03900, 05004], lr: 0.199578, loss: 2.3958
2022-07-31 19:08:06 - train: epoch 0008, iter [04000, 05004], lr: 0.199572, loss: 2.8564
2022-07-31 19:09:05 - train: epoch 0008, iter [04100, 05004], lr: 0.199566, loss: 2.2743
2022-07-31 19:10:03 - train: epoch 0008, iter [04200, 05004], lr: 0.199560, loss: 2.6717
2022-07-31 19:11:02 - train: epoch 0008, iter [04300, 05004], lr: 0.199553, loss: 2.2750
2022-07-31 19:12:01 - train: epoch 0008, iter [04400, 05004], lr: 0.199547, loss: 2.3576
2022-07-31 19:12:59 - train: epoch 0008, iter [04500, 05004], lr: 0.199541, loss: 2.4437
2022-07-31 19:13:58 - train: epoch 0008, iter [04600, 05004], lr: 0.199534, loss: 2.3877
2022-07-31 19:14:58 - train: epoch 0008, iter [04700, 05004], lr: 0.199528, loss: 2.4754
2022-07-31 19:15:55 - train: epoch 0008, iter [04800, 05004], lr: 0.199522, loss: 2.4833
2022-07-31 19:16:52 - train: epoch 0008, iter [04900, 05004], lr: 0.199515, loss: 2.4825
2022-07-31 19:17:48 - train: epoch 0008, iter [05000, 05004], lr: 0.199509, loss: 2.4045
2022-07-31 19:17:50 - train: epoch 008, train_loss: 2.4572
2022-07-31 19:20:01 - eval: epoch: 008, acc1: 48.004%, acc5: 74.326%, test_loss: 2.2562, per_image_load_time: 4.428ms, per_image_inference_time: 0.608ms
2022-07-31 19:20:01 - until epoch: 008, best_acc1: 49.552%
2022-07-31 19:20:01 - epoch 009 lr: 0.199508
2022-07-31 19:21:11 - train: epoch 0009, iter [00100, 05004], lr: 0.199502, loss: 2.1200
2022-07-31 19:22:13 - train: epoch 0009, iter [00200, 05004], lr: 0.199495, loss: 2.4007
2022-07-31 19:23:10 - train: epoch 0009, iter [00300, 05004], lr: 0.199488, loss: 2.3929
2022-07-31 19:24:10 - train: epoch 0009, iter [00400, 05004], lr: 0.199482, loss: 2.7470
2022-07-31 19:25:09 - train: epoch 0009, iter [00500, 05004], lr: 0.199475, loss: 2.4158
2022-07-31 19:26:07 - train: epoch 0009, iter [00600, 05004], lr: 0.199468, loss: 2.4562
2022-07-31 19:27:06 - train: epoch 0009, iter [00700, 05004], lr: 0.199461, loss: 2.2864
2022-07-31 19:28:04 - train: epoch 0009, iter [00800, 05004], lr: 0.199455, loss: 2.5501
2022-07-31 19:29:01 - train: epoch 0009, iter [00900, 05004], lr: 0.199448, loss: 2.2571
2022-07-31 19:29:59 - train: epoch 0009, iter [01000, 05004], lr: 0.199441, loss: 2.2534
2022-07-31 19:30:59 - train: epoch 0009, iter [01100, 05004], lr: 0.199434, loss: 2.5970
2022-07-31 19:31:58 - train: epoch 0009, iter [01200, 05004], lr: 0.199427, loss: 2.6012
2022-07-31 19:32:57 - train: epoch 0009, iter [01300, 05004], lr: 0.199420, loss: 2.7221
2022-07-31 19:33:57 - train: epoch 0009, iter [01400, 05004], lr: 0.199412, loss: 2.1924
2022-07-31 19:34:56 - train: epoch 0009, iter [01500, 05004], lr: 0.199405, loss: 2.2887
2022-07-31 19:35:55 - train: epoch 0009, iter [01600, 05004], lr: 0.199398, loss: 2.5616
2022-07-31 19:36:54 - train: epoch 0009, iter [01700, 05004], lr: 0.199391, loss: 2.4231
2022-07-31 19:37:52 - train: epoch 0009, iter [01800, 05004], lr: 0.199383, loss: 2.4575
2022-07-31 19:38:52 - train: epoch 0009, iter [01900, 05004], lr: 0.199376, loss: 2.1365
2022-07-31 19:39:51 - train: epoch 0009, iter [02000, 05004], lr: 0.199369, loss: 2.4001
2022-07-31 19:40:52 - train: epoch 0009, iter [02100, 05004], lr: 0.199361, loss: 2.4839
2022-07-31 19:41:50 - train: epoch 0009, iter [02200, 05004], lr: 0.199354, loss: 2.3804
2022-07-31 19:42:49 - train: epoch 0009, iter [02300, 05004], lr: 0.199346, loss: 2.1585
2022-07-31 19:43:47 - train: epoch 0009, iter [02400, 05004], lr: 0.199339, loss: 2.5072
2022-07-31 19:44:46 - train: epoch 0009, iter [02500, 05004], lr: 0.199331, loss: 2.2686
2022-07-31 19:45:46 - train: epoch 0009, iter [02600, 05004], lr: 0.199323, loss: 2.5942
2022-07-31 19:46:43 - train: epoch 0009, iter [02700, 05004], lr: 0.199316, loss: 2.3056
2022-07-31 19:47:41 - train: epoch 0009, iter [02800, 05004], lr: 0.199308, loss: 2.4095
2022-07-31 19:48:39 - train: epoch 0009, iter [02900, 05004], lr: 0.199300, loss: 2.1254
2022-07-31 19:49:36 - train: epoch 0009, iter [03000, 05004], lr: 0.199292, loss: 2.2911
2022-07-31 19:50:34 - train: epoch 0009, iter [03100, 05004], lr: 0.199285, loss: 2.5159
2022-07-31 19:51:33 - train: epoch 0009, iter [03200, 05004], lr: 0.199277, loss: 2.3717
2022-07-31 19:52:28 - train: epoch 0009, iter [03300, 05004], lr: 0.199269, loss: 2.5651
2022-07-31 19:53:26 - train: epoch 0009, iter [03400, 05004], lr: 0.199261, loss: 2.5248
2022-07-31 19:54:23 - train: epoch 0009, iter [03500, 05004], lr: 0.199253, loss: 2.4557
2022-07-31 19:55:21 - train: epoch 0009, iter [03600, 05004], lr: 0.199245, loss: 2.2958
2022-07-31 19:56:19 - train: epoch 0009, iter [03700, 05004], lr: 0.199236, loss: 2.5894
2022-07-31 19:57:16 - train: epoch 0009, iter [03800, 05004], lr: 0.199228, loss: 2.5504
2022-07-31 19:58:15 - train: epoch 0009, iter [03900, 05004], lr: 0.199220, loss: 2.3247
2022-07-31 19:59:12 - train: epoch 0009, iter [04000, 05004], lr: 0.199212, loss: 2.5408
2022-07-31 20:00:11 - train: epoch 0009, iter [04100, 05004], lr: 0.199203, loss: 2.3742
2022-07-31 20:01:07 - train: epoch 0009, iter [04200, 05004], lr: 0.199195, loss: 2.3339
2022-07-31 20:02:04 - train: epoch 0009, iter [04300, 05004], lr: 0.199187, loss: 2.3597
2022-07-31 20:03:02 - train: epoch 0009, iter [04400, 05004], lr: 0.199178, loss: 2.6681
2022-07-31 20:04:00 - train: epoch 0009, iter [04500, 05004], lr: 0.199170, loss: 2.1883
2022-07-31 20:04:58 - train: epoch 0009, iter [04600, 05004], lr: 0.199161, loss: 2.4290
2022-07-31 20:05:54 - train: epoch 0009, iter [04700, 05004], lr: 0.199153, loss: 2.5518
2022-07-31 20:06:53 - train: epoch 0009, iter [04800, 05004], lr: 0.199144, loss: 2.4228
2022-07-31 20:07:51 - train: epoch 0009, iter [04900, 05004], lr: 0.199135, loss: 2.5934
2022-07-31 20:08:45 - train: epoch 0009, iter [05000, 05004], lr: 0.199127, loss: 2.3058
2022-07-31 20:08:46 - train: epoch 009, train_loss: 2.4112
2022-07-31 20:10:55 - eval: epoch: 009, acc1: 50.968%, acc5: 76.498%, test_loss: 2.1186, per_image_load_time: 4.369ms, per_image_inference_time: 0.566ms
2022-07-31 20:10:56 - until epoch: 009, best_acc1: 50.968%
2022-07-31 20:10:56 - epoch 010 lr: 0.199126
2022-07-31 20:12:05 - train: epoch 0010, iter [00100, 05004], lr: 0.199118, loss: 2.3788
2022-07-31 20:13:03 - train: epoch 0010, iter [00200, 05004], lr: 0.199109, loss: 2.4441
2022-07-31 20:13:59 - train: epoch 0010, iter [00300, 05004], lr: 0.199100, loss: 2.5262
2022-07-31 20:14:56 - train: epoch 0010, iter [00400, 05004], lr: 0.199091, loss: 2.5030
2022-07-31 20:15:53 - train: epoch 0010, iter [00500, 05004], lr: 0.199082, loss: 2.3526
2022-07-31 20:16:50 - train: epoch 0010, iter [00600, 05004], lr: 0.199073, loss: 2.4992
2022-07-31 20:17:49 - train: epoch 0010, iter [00700, 05004], lr: 0.199064, loss: 2.4101
2022-07-31 20:18:46 - train: epoch 0010, iter [00800, 05004], lr: 0.199055, loss: 2.2896
2022-07-31 20:19:43 - train: epoch 0010, iter [00900, 05004], lr: 0.199046, loss: 2.2204
2022-07-31 20:20:41 - train: epoch 0010, iter [01000, 05004], lr: 0.199037, loss: 2.3942
2022-07-31 20:21:39 - train: epoch 0010, iter [01100, 05004], lr: 0.199028, loss: 2.4956
2022-07-31 20:22:36 - train: epoch 0010, iter [01200, 05004], lr: 0.199019, loss: 2.2735
2022-07-31 20:23:35 - train: epoch 0010, iter [01300, 05004], lr: 0.199009, loss: 2.1979
2022-07-31 20:24:31 - train: epoch 0010, iter [01400, 05004], lr: 0.199000, loss: 2.6663
2022-07-31 20:25:29 - train: epoch 0010, iter [01500, 05004], lr: 0.198991, loss: 2.1441
2022-07-31 20:26:26 - train: epoch 0010, iter [01600, 05004], lr: 0.198981, loss: 2.3297
2022-07-31 20:27:25 - train: epoch 0010, iter [01700, 05004], lr: 0.198972, loss: 2.5964
2022-07-31 20:28:22 - train: epoch 0010, iter [01800, 05004], lr: 0.198963, loss: 2.1689
2022-07-31 20:29:20 - train: epoch 0010, iter [01900, 05004], lr: 0.198953, loss: 2.6170
2022-07-31 20:30:17 - train: epoch 0010, iter [02000, 05004], lr: 0.198943, loss: 2.2840
2022-07-31 20:31:15 - train: epoch 0010, iter [02100, 05004], lr: 0.198934, loss: 2.2592
2022-07-31 20:32:12 - train: epoch 0010, iter [02200, 05004], lr: 0.198924, loss: 2.4258
2022-07-31 20:33:09 - train: epoch 0010, iter [02300, 05004], lr: 0.198914, loss: 2.5861
2022-07-31 20:34:07 - train: epoch 0010, iter [02400, 05004], lr: 0.198905, loss: 2.4629
2022-07-31 20:35:07 - train: epoch 0010, iter [02500, 05004], lr: 0.198895, loss: 2.4688
2022-07-31 20:36:03 - train: epoch 0010, iter [02600, 05004], lr: 0.198885, loss: 2.5360
2022-07-31 20:37:02 - train: epoch 0010, iter [02700, 05004], lr: 0.198875, loss: 2.1611
2022-07-31 20:37:58 - train: epoch 0010, iter [02800, 05004], lr: 0.198865, loss: 2.4265
2022-07-31 20:38:55 - train: epoch 0010, iter [02900, 05004], lr: 0.198855, loss: 2.4034
2022-07-31 20:39:51 - train: epoch 0010, iter [03000, 05004], lr: 0.198845, loss: 2.3767
2022-07-31 20:40:48 - train: epoch 0010, iter [03100, 05004], lr: 0.198835, loss: 2.5436
2022-07-31 20:41:44 - train: epoch 0010, iter [03200, 05004], lr: 0.198825, loss: 2.4101
2022-07-31 20:42:41 - train: epoch 0010, iter [03300, 05004], lr: 0.198815, loss: 2.6137
2022-07-31 20:43:36 - train: epoch 0010, iter [03400, 05004], lr: 0.198805, loss: 2.4693
2022-07-31 20:44:33 - train: epoch 0010, iter [03500, 05004], lr: 0.198795, loss: 2.5670
2022-07-31 20:45:29 - train: epoch 0010, iter [03600, 05004], lr: 0.198785, loss: 2.6539
2022-07-31 20:46:26 - train: epoch 0010, iter [03700, 05004], lr: 0.198774, loss: 2.2704
2022-07-31 20:47:22 - train: epoch 0010, iter [03800, 05004], lr: 0.198764, loss: 2.4595
2022-07-31 20:48:20 - train: epoch 0010, iter [03900, 05004], lr: 0.198754, loss: 2.2043
2022-07-31 20:49:18 - train: epoch 0010, iter [04000, 05004], lr: 0.198743, loss: 2.2978
2022-07-31 20:50:14 - train: epoch 0010, iter [04100, 05004], lr: 0.198733, loss: 2.1782
2022-07-31 20:51:11 - train: epoch 0010, iter [04200, 05004], lr: 0.198722, loss: 2.4622
2022-07-31 20:52:07 - train: epoch 0010, iter [04300, 05004], lr: 0.198712, loss: 2.2721
2022-07-31 20:53:04 - train: epoch 0010, iter [04400, 05004], lr: 0.198701, loss: 2.3474
2022-07-31 20:54:02 - train: epoch 0010, iter [04500, 05004], lr: 0.198690, loss: 2.2519
2022-07-31 20:54:58 - train: epoch 0010, iter [04600, 05004], lr: 0.198680, loss: 2.2310
2022-07-31 20:55:56 - train: epoch 0010, iter [04700, 05004], lr: 0.198669, loss: 2.4267
2022-07-31 20:56:51 - train: epoch 0010, iter [04800, 05004], lr: 0.198658, loss: 2.1820
2022-07-31 20:57:48 - train: epoch 0010, iter [04900, 05004], lr: 0.198647, loss: 2.3134
2022-07-31 20:58:41 - train: epoch 0010, iter [05000, 05004], lr: 0.198637, loss: 2.2893
2022-07-31 20:58:43 - train: epoch 010, train_loss: 2.3777
2022-07-31 21:00:53 - eval: epoch: 010, acc1: 50.002%, acc5: 75.354%, test_loss: 2.1762, per_image_load_time: 4.467ms, per_image_inference_time: 0.576ms
2022-07-31 21:00:53 - until epoch: 010, best_acc1: 50.968%
2022-07-31 21:00:53 - epoch 011 lr: 0.198636
2022-07-31 21:02:00 - train: epoch 0011, iter [00100, 05004], lr: 0.198625, loss: 2.2464
2022-07-31 21:02:58 - train: epoch 0011, iter [00200, 05004], lr: 0.198614, loss: 2.4740
2022-07-31 21:03:56 - train: epoch 0011, iter [00300, 05004], lr: 0.198603, loss: 2.3176
2022-07-31 21:04:52 - train: epoch 0011, iter [00400, 05004], lr: 0.198592, loss: 2.4536
2022-07-31 21:05:49 - train: epoch 0011, iter [00500, 05004], lr: 0.198581, loss: 2.3323
2022-07-31 21:06:46 - train: epoch 0011, iter [00600, 05004], lr: 0.198570, loss: 2.3523
2022-07-31 21:07:44 - train: epoch 0011, iter [00700, 05004], lr: 0.198559, loss: 2.4806
2022-07-31 21:08:42 - train: epoch 0011, iter [00800, 05004], lr: 0.198548, loss: 2.6114
2022-07-31 21:09:37 - train: epoch 0011, iter [00900, 05004], lr: 0.198536, loss: 2.4910
2022-07-31 21:10:33 - train: epoch 0011, iter [01000, 05004], lr: 0.198525, loss: 2.3171
2022-07-31 21:11:32 - train: epoch 0011, iter [01100, 05004], lr: 0.198514, loss: 2.4939
2022-07-31 21:12:28 - train: epoch 0011, iter [01200, 05004], lr: 0.198503, loss: 2.4272
2022-07-31 21:13:26 - train: epoch 0011, iter [01300, 05004], lr: 0.198491, loss: 2.4720
2022-07-31 21:14:23 - train: epoch 0011, iter [01400, 05004], lr: 0.198480, loss: 2.3462
2022-07-31 21:15:21 - train: epoch 0011, iter [01500, 05004], lr: 0.198468, loss: 2.1192
2022-07-31 21:16:18 - train: epoch 0011, iter [01600, 05004], lr: 0.198457, loss: 2.3458
2022-07-31 21:17:14 - train: epoch 0011, iter [01700, 05004], lr: 0.198445, loss: 2.4313
2022-07-31 21:18:12 - train: epoch 0011, iter [01800, 05004], lr: 0.198433, loss: 2.2188
2022-07-31 21:19:11 - train: epoch 0011, iter [01900, 05004], lr: 0.198422, loss: 2.1954
2022-07-31 21:20:07 - train: epoch 0011, iter [02000, 05004], lr: 0.198410, loss: 2.3813
2022-07-31 21:21:04 - train: epoch 0011, iter [02100, 05004], lr: 0.198398, loss: 2.2720
2022-07-31 21:22:02 - train: epoch 0011, iter [02200, 05004], lr: 0.198386, loss: 2.0752
2022-07-31 21:23:00 - train: epoch 0011, iter [02300, 05004], lr: 0.198375, loss: 2.5531
2022-07-31 21:23:57 - train: epoch 0011, iter [02400, 05004], lr: 0.198363, loss: 2.3733
2022-07-31 21:24:55 - train: epoch 0011, iter [02500, 05004], lr: 0.198351, loss: 2.2963
2022-07-31 21:25:52 - train: epoch 0011, iter [02600, 05004], lr: 0.198339, loss: 2.3004
2022-07-31 21:26:50 - train: epoch 0011, iter [02700, 05004], lr: 0.198327, loss: 2.3353
2022-07-31 21:27:46 - train: epoch 0011, iter [02800, 05004], lr: 0.198315, loss: 2.0631
2022-07-31 21:28:46 - train: epoch 0011, iter [02900, 05004], lr: 0.198303, loss: 2.2290
2022-07-31 21:29:42 - train: epoch 0011, iter [03000, 05004], lr: 0.198290, loss: 2.7712
2022-07-31 21:30:39 - train: epoch 0011, iter [03100, 05004], lr: 0.198278, loss: 2.2606
2022-07-31 21:31:37 - train: epoch 0011, iter [03200, 05004], lr: 0.198266, loss: 2.4079
2022-07-31 21:32:35 - train: epoch 0011, iter [03300, 05004], lr: 0.198254, loss: 2.5072
2022-07-31 21:33:32 - train: epoch 0011, iter [03400, 05004], lr: 0.198241, loss: 2.2647
2022-07-31 21:34:31 - train: epoch 0011, iter [03500, 05004], lr: 0.198229, loss: 2.3704
2022-07-31 21:35:29 - train: epoch 0011, iter [03600, 05004], lr: 0.198217, loss: 2.2644
2022-07-31 21:36:27 - train: epoch 0011, iter [03700, 05004], lr: 0.198204, loss: 2.5224
2022-07-31 21:37:25 - train: epoch 0011, iter [03800, 05004], lr: 0.198192, loss: 2.3171
2022-07-31 21:38:24 - train: epoch 0011, iter [03900, 05004], lr: 0.198179, loss: 2.3174
2022-07-31 21:39:21 - train: epoch 0011, iter [04000, 05004], lr: 0.198167, loss: 2.2509
2022-07-31 21:40:20 - train: epoch 0011, iter [04100, 05004], lr: 0.198154, loss: 2.0153
2022-07-31 21:41:18 - train: epoch 0011, iter [04200, 05004], lr: 0.198141, loss: 2.2695
2022-07-31 21:42:17 - train: epoch 0011, iter [04300, 05004], lr: 0.198129, loss: 2.4118
2022-07-31 21:43:13 - train: epoch 0011, iter [04400, 05004], lr: 0.198116, loss: 2.4221
2022-07-31 21:44:11 - train: epoch 0011, iter [04500, 05004], lr: 0.198103, loss: 2.2277
2022-07-31 21:45:09 - train: epoch 0011, iter [04600, 05004], lr: 0.198090, loss: 2.2754
2022-07-31 21:46:08 - train: epoch 0011, iter [04700, 05004], lr: 0.198077, loss: 2.0523
2022-07-31 21:47:04 - train: epoch 0011, iter [04800, 05004], lr: 0.198064, loss: 2.3666
2022-07-31 21:48:02 - train: epoch 0011, iter [04900, 05004], lr: 0.198052, loss: 2.1936
2022-07-31 21:48:57 - train: epoch 0011, iter [05000, 05004], lr: 0.198039, loss: 2.3043
2022-07-31 21:48:58 - train: epoch 011, train_loss: 2.3468
2022-07-31 21:51:04 - eval: epoch: 011, acc1: 52.420%, acc5: 77.994%, test_loss: 2.0350, per_image_load_time: 3.786ms, per_image_inference_time: 0.609ms
2022-07-31 21:51:04 - until epoch: 011, best_acc1: 52.420%
2022-07-31 21:51:04 - epoch 012 lr: 0.198038
2022-07-31 21:52:14 - train: epoch 0012, iter [00100, 05004], lr: 0.198025, loss: 1.9406
2022-07-31 21:53:12 - train: epoch 0012, iter [00200, 05004], lr: 0.198012, loss: 2.3283
2022-07-31 21:54:11 - train: epoch 0012, iter [00300, 05004], lr: 0.197999, loss: 2.2035
2022-07-31 21:55:08 - train: epoch 0012, iter [00400, 05004], lr: 0.197986, loss: 2.5027
2022-07-31 21:56:05 - train: epoch 0012, iter [00500, 05004], lr: 0.197972, loss: 2.3907
2022-07-31 21:57:03 - train: epoch 0012, iter [00600, 05004], lr: 0.197959, loss: 2.1052
2022-07-31 21:58:00 - train: epoch 0012, iter [00700, 05004], lr: 0.197946, loss: 2.1301
2022-07-31 21:58:57 - train: epoch 0012, iter [00800, 05004], lr: 0.197932, loss: 2.3349
2022-07-31 21:59:54 - train: epoch 0012, iter [00900, 05004], lr: 0.197919, loss: 2.4148
2022-07-31 22:00:51 - train: epoch 0012, iter [01000, 05004], lr: 0.197906, loss: 2.1853
2022-07-31 22:01:50 - train: epoch 0012, iter [01100, 05004], lr: 0.197892, loss: 2.5041
2022-07-31 22:02:46 - train: epoch 0012, iter [01200, 05004], lr: 0.197879, loss: 2.1430
2022-07-31 22:03:44 - train: epoch 0012, iter [01300, 05004], lr: 0.197865, loss: 2.2329
2022-07-31 22:04:42 - train: epoch 0012, iter [01400, 05004], lr: 0.197851, loss: 2.4797
2022-07-31 22:05:40 - train: epoch 0012, iter [01500, 05004], lr: 0.197838, loss: 2.1852
2022-07-31 22:06:37 - train: epoch 0012, iter [01600, 05004], lr: 0.197824, loss: 2.3850
2022-07-31 22:07:35 - train: epoch 0012, iter [01700, 05004], lr: 0.197810, loss: 2.2496
2022-07-31 22:08:32 - train: epoch 0012, iter [01800, 05004], lr: 0.197797, loss: 2.3096
2022-07-31 22:09:30 - train: epoch 0012, iter [01900, 05004], lr: 0.197783, loss: 2.3420
2022-07-31 22:10:28 - train: epoch 0012, iter [02000, 05004], lr: 0.197769, loss: 2.5787
2022-07-31 22:11:26 - train: epoch 0012, iter [02100, 05004], lr: 0.197755, loss: 2.3820
2022-07-31 22:12:24 - train: epoch 0012, iter [02200, 05004], lr: 0.197741, loss: 2.3543
2022-07-31 22:13:22 - train: epoch 0012, iter [02300, 05004], lr: 0.197727, loss: 2.1477
2022-07-31 22:14:19 - train: epoch 0012, iter [02400, 05004], lr: 0.197713, loss: 2.3515
2022-07-31 22:15:18 - train: epoch 0012, iter [02500, 05004], lr: 0.197699, loss: 2.3018
2022-07-31 22:16:16 - train: epoch 0012, iter [02600, 05004], lr: 0.197685, loss: 2.1558
2022-07-31 22:17:13 - train: epoch 0012, iter [02700, 05004], lr: 0.197671, loss: 2.0793
2022-07-31 22:18:13 - train: epoch 0012, iter [02800, 05004], lr: 0.197656, loss: 2.3058
2022-07-31 22:19:09 - train: epoch 0012, iter [02900, 05004], lr: 0.197642, loss: 2.3481
2022-07-31 22:20:07 - train: epoch 0012, iter [03000, 05004], lr: 0.197628, loss: 1.9968
2022-07-31 22:21:06 - train: epoch 0012, iter [03100, 05004], lr: 0.197614, loss: 2.2310
2022-07-31 22:22:04 - train: epoch 0012, iter [03200, 05004], lr: 0.197599, loss: 2.2524
2022-07-31 22:23:02 - train: epoch 0012, iter [03300, 05004], lr: 0.197585, loss: 2.1803
2022-07-31 22:23:59 - train: epoch 0012, iter [03400, 05004], lr: 0.197570, loss: 2.2739
2022-07-31 22:24:56 - train: epoch 0012, iter [03500, 05004], lr: 0.197556, loss: 2.5232
2022-07-31 22:25:56 - train: epoch 0012, iter [03600, 05004], lr: 0.197541, loss: 2.3665
2022-07-31 22:26:52 - train: epoch 0012, iter [03700, 05004], lr: 0.197527, loss: 2.2602
2022-07-31 22:27:52 - train: epoch 0012, iter [03800, 05004], lr: 0.197512, loss: 2.4290
2022-07-31 22:28:50 - train: epoch 0012, iter [03900, 05004], lr: 0.197497, loss: 2.2962
2022-07-31 22:29:47 - train: epoch 0012, iter [04000, 05004], lr: 0.197483, loss: 2.4305
2022-07-31 22:30:44 - train: epoch 0012, iter [04100, 05004], lr: 0.197468, loss: 2.2996
2022-07-31 22:31:42 - train: epoch 0012, iter [04200, 05004], lr: 0.197453, loss: 2.1692
2022-07-31 22:32:41 - train: epoch 0012, iter [04300, 05004], lr: 0.197438, loss: 2.4389
2022-07-31 22:33:38 - train: epoch 0012, iter [04400, 05004], lr: 0.197423, loss: 2.1287
2022-07-31 22:34:36 - train: epoch 0012, iter [04500, 05004], lr: 0.197409, loss: 2.1944
2022-07-31 22:35:33 - train: epoch 0012, iter [04600, 05004], lr: 0.197394, loss: 2.4219
2022-07-31 22:36:32 - train: epoch 0012, iter [04700, 05004], lr: 0.197379, loss: 2.2530
2022-07-31 22:37:28 - train: epoch 0012, iter [04800, 05004], lr: 0.197364, loss: 2.3599
2022-07-31 22:38:26 - train: epoch 0012, iter [04900, 05004], lr: 0.197348, loss: 2.2593
2022-07-31 22:39:19 - train: epoch 0012, iter [05000, 05004], lr: 0.197333, loss: 1.8945
2022-07-31 22:39:20 - train: epoch 012, train_loss: 2.3256
2022-07-31 22:41:30 - eval: epoch: 012, acc1: 52.804%, acc5: 77.792%, test_loss: 2.0329, per_image_load_time: 4.310ms, per_image_inference_time: 0.636ms
2022-07-31 22:41:30 - until epoch: 012, best_acc1: 52.804%
2022-07-31 22:41:30 - epoch 013 lr: 0.197333
2022-07-31 22:42:38 - train: epoch 0013, iter [00100, 05004], lr: 0.197317, loss: 2.1698
2022-07-31 22:43:35 - train: epoch 0013, iter [00200, 05004], lr: 0.197302, loss: 2.1751
2022-07-31 22:44:32 - train: epoch 0013, iter [00300, 05004], lr: 0.197287, loss: 2.2352
2022-07-31 22:45:28 - train: epoch 0013, iter [00400, 05004], lr: 0.197272, loss: 2.0303
2022-07-31 22:46:25 - train: epoch 0013, iter [00500, 05004], lr: 0.197256, loss: 2.3355
2022-07-31 22:47:23 - train: epoch 0013, iter [00600, 05004], lr: 0.197241, loss: 2.1988
2022-07-31 22:48:19 - train: epoch 0013, iter [00700, 05004], lr: 0.197225, loss: 2.1738
2022-07-31 22:49:16 - train: epoch 0013, iter [00800, 05004], lr: 0.197210, loss: 2.5113
2022-07-31 22:50:12 - train: epoch 0013, iter [00900, 05004], lr: 0.197194, loss: 2.1394
2022-07-31 22:51:09 - train: epoch 0013, iter [01000, 05004], lr: 0.197179, loss: 2.2691
2022-07-31 22:52:06 - train: epoch 0013, iter [01100, 05004], lr: 0.197163, loss: 2.3743
2022-07-31 22:53:03 - train: epoch 0013, iter [01200, 05004], lr: 0.197148, loss: 2.4222
2022-07-31 22:54:00 - train: epoch 0013, iter [01300, 05004], lr: 0.197132, loss: 2.4380
2022-07-31 22:54:56 - train: epoch 0013, iter [01400, 05004], lr: 0.197116, loss: 2.2763
2022-07-31 22:55:53 - train: epoch 0013, iter [01500, 05004], lr: 0.197100, loss: 2.4955
2022-07-31 22:56:50 - train: epoch 0013, iter [01600, 05004], lr: 0.197085, loss: 2.0998
2022-07-31 22:57:48 - train: epoch 0013, iter [01700, 05004], lr: 0.197069, loss: 2.2237
2022-07-31 22:58:45 - train: epoch 0013, iter [01800, 05004], lr: 0.197053, loss: 2.1041
2022-07-31 22:59:43 - train: epoch 0013, iter [01900, 05004], lr: 0.197037, loss: 2.4327
2022-07-31 23:00:40 - train: epoch 0013, iter [02000, 05004], lr: 0.197021, loss: 2.4055
2022-07-31 23:01:37 - train: epoch 0013, iter [02100, 05004], lr: 0.197005, loss: 2.5316
2022-07-31 23:02:33 - train: epoch 0013, iter [02200, 05004], lr: 0.196989, loss: 2.2864
2022-07-31 23:03:31 - train: epoch 0013, iter [02300, 05004], lr: 0.196973, loss: 2.2163
2022-07-31 23:04:27 - train: epoch 0013, iter [02400, 05004], lr: 0.196957, loss: 2.5319
2022-07-31 23:05:24 - train: epoch 0013, iter [02500, 05004], lr: 0.196940, loss: 2.2980
2022-07-31 23:06:22 - train: epoch 0013, iter [02600, 05004], lr: 0.196924, loss: 2.2128
2022-07-31 23:07:19 - train: epoch 0013, iter [02700, 05004], lr: 0.196908, loss: 2.4305
2022-07-31 23:08:16 - train: epoch 0013, iter [02800, 05004], lr: 0.196891, loss: 2.3810
2022-07-31 23:09:13 - train: epoch 0013, iter [02900, 05004], lr: 0.196875, loss: 2.4166
2022-07-31 23:10:09 - train: epoch 0013, iter [03000, 05004], lr: 0.196859, loss: 2.1004
2022-07-31 23:11:05 - train: epoch 0013, iter [03100, 05004], lr: 0.196842, loss: 2.2006
2022-07-31 23:12:03 - train: epoch 0013, iter [03200, 05004], lr: 0.196826, loss: 2.4316
2022-07-31 23:13:00 - train: epoch 0013, iter [03300, 05004], lr: 0.196809, loss: 2.2605
2022-07-31 23:13:57 - train: epoch 0013, iter [03400, 05004], lr: 0.196793, loss: 2.3474
2022-07-31 23:14:53 - train: epoch 0013, iter [03500, 05004], lr: 0.196776, loss: 2.0135
2022-07-31 23:15:50 - train: epoch 0013, iter [03600, 05004], lr: 0.196759, loss: 2.3452
2022-07-31 23:16:46 - train: epoch 0013, iter [03700, 05004], lr: 0.196743, loss: 1.9882
2022-07-31 23:17:45 - train: epoch 0013, iter [03800, 05004], lr: 0.196726, loss: 2.3484
2022-07-31 23:18:42 - train: epoch 0013, iter [03900, 05004], lr: 0.196709, loss: 2.3441
2022-07-31 23:19:38 - train: epoch 0013, iter [04000, 05004], lr: 0.196692, loss: 2.3999
2022-07-31 23:20:35 - train: epoch 0013, iter [04100, 05004], lr: 0.196675, loss: 2.0516
2022-07-31 23:21:31 - train: epoch 0013, iter [04200, 05004], lr: 0.196658, loss: 2.1340
2022-07-31 23:22:30 - train: epoch 0013, iter [04300, 05004], lr: 0.196641, loss: 2.1958
2022-07-31 23:23:26 - train: epoch 0013, iter [04400, 05004], lr: 0.196624, loss: 2.2391
2022-07-31 23:24:24 - train: epoch 0013, iter [04500, 05004], lr: 0.196607, loss: 2.2159
2022-07-31 23:25:21 - train: epoch 0013, iter [04600, 05004], lr: 0.196590, loss: 2.2719
2022-07-31 23:26:19 - train: epoch 0013, iter [04700, 05004], lr: 0.196573, loss: 2.3408
2022-07-31 23:27:16 - train: epoch 0013, iter [04800, 05004], lr: 0.196556, loss: 2.5241
2022-07-31 23:28:13 - train: epoch 0013, iter [04900, 05004], lr: 0.196539, loss: 2.6770
2022-07-31 23:29:08 - train: epoch 0013, iter [05000, 05004], lr: 0.196522, loss: 2.3230
2022-07-31 23:29:09 - train: epoch 013, train_loss: 2.3043
2022-07-31 23:31:17 - eval: epoch: 013, acc1: 53.486%, acc5: 78.804%, test_loss: 1.9922, per_image_load_time: 4.051ms, per_image_inference_time: 0.615ms
2022-07-31 23:31:18 - until epoch: 013, best_acc1: 53.486%
2022-07-31 23:31:18 - epoch 014 lr: 0.196521
2022-07-31 23:32:26 - train: epoch 0014, iter [00100, 05004], lr: 0.196504, loss: 2.1985
2022-07-31 23:33:23 - train: epoch 0014, iter [00200, 05004], lr: 0.196486, loss: 2.2726
2022-07-31 23:34:19 - train: epoch 0014, iter [00300, 05004], lr: 0.196469, loss: 2.2942
2022-07-31 23:35:16 - train: epoch 0014, iter [00400, 05004], lr: 0.196451, loss: 2.3146
2022-07-31 23:36:13 - train: epoch 0014, iter [00500, 05004], lr: 0.196434, loss: 2.1510
2022-07-31 23:37:10 - train: epoch 0014, iter [00600, 05004], lr: 0.196416, loss: 2.3528
2022-07-31 23:38:07 - train: epoch 0014, iter [00700, 05004], lr: 0.196399, loss: 2.2018
2022-07-31 23:39:05 - train: epoch 0014, iter [00800, 05004], lr: 0.196381, loss: 2.1455
2022-07-31 23:40:00 - train: epoch 0014, iter [00900, 05004], lr: 0.196364, loss: 2.2594
2022-07-31 23:41:01 - train: epoch 0014, iter [01000, 05004], lr: 0.196346, loss: 2.4178
2022-07-31 23:41:56 - train: epoch 0014, iter [01100, 05004], lr: 0.196328, loss: 2.2287
2022-07-31 23:42:54 - train: epoch 0014, iter [01200, 05004], lr: 0.196310, loss: 2.3447
2022-07-31 23:43:51 - train: epoch 0014, iter [01300, 05004], lr: 0.196293, loss: 2.3869
2022-07-31 23:44:49 - train: epoch 0014, iter [01400, 05004], lr: 0.196275, loss: 2.2354
2022-07-31 23:45:46 - train: epoch 0014, iter [01500, 05004], lr: 0.196257, loss: 2.5872
2022-07-31 23:46:44 - train: epoch 0014, iter [01600, 05004], lr: 0.196239, loss: 2.3684
2022-07-31 23:47:41 - train: epoch 0014, iter [01700, 05004], lr: 0.196221, loss: 2.4292
2022-07-31 23:48:37 - train: epoch 0014, iter [01800, 05004], lr: 0.196203, loss: 2.4985
2022-07-31 23:49:34 - train: epoch 0014, iter [01900, 05004], lr: 0.196185, loss: 2.0777
2022-07-31 23:50:31 - train: epoch 0014, iter [02000, 05004], lr: 0.196167, loss: 2.1351
2022-07-31 23:51:28 - train: epoch 0014, iter [02100, 05004], lr: 0.196149, loss: 2.2843
2022-07-31 23:52:25 - train: epoch 0014, iter [02200, 05004], lr: 0.196131, loss: 2.3372
2022-07-31 23:53:22 - train: epoch 0014, iter [02300, 05004], lr: 0.196112, loss: 2.2445
2022-07-31 23:54:20 - train: epoch 0014, iter [02400, 05004], lr: 0.196094, loss: 2.3586
2022-07-31 23:55:18 - train: epoch 0014, iter [02500, 05004], lr: 0.196076, loss: 2.2245
2022-07-31 23:56:14 - train: epoch 0014, iter [02600, 05004], lr: 0.196057, loss: 2.2555
2022-07-31 23:57:12 - train: epoch 0014, iter [02700, 05004], lr: 0.196039, loss: 2.2010
2022-07-31 23:58:09 - train: epoch 0014, iter [02800, 05004], lr: 0.196021, loss: 2.2824
2022-07-31 23:59:06 - train: epoch 0014, iter [02900, 05004], lr: 0.196002, loss: 2.3315
2022-08-01 00:00:04 - train: epoch 0014, iter [03000, 05004], lr: 0.195984, loss: 2.5004
2022-08-01 00:01:02 - train: epoch 0014, iter [03100, 05004], lr: 0.195965, loss: 2.2163
2022-08-01 00:01:59 - train: epoch 0014, iter [03200, 05004], lr: 0.195946, loss: 2.4789
2022-08-01 00:02:57 - train: epoch 0014, iter [03300, 05004], lr: 0.195928, loss: 1.9790
2022-08-01 00:03:54 - train: epoch 0014, iter [03400, 05004], lr: 0.195909, loss: 2.3991
2022-08-01 00:04:51 - train: epoch 0014, iter [03500, 05004], lr: 0.195890, loss: 2.2056
2022-08-01 00:05:49 - train: epoch 0014, iter [03600, 05004], lr: 0.195872, loss: 2.0293
2022-08-01 00:06:45 - train: epoch 0014, iter [03700, 05004], lr: 0.195853, loss: 2.3260
2022-08-01 00:07:43 - train: epoch 0014, iter [03800, 05004], lr: 0.195834, loss: 2.5671
2022-08-01 00:08:41 - train: epoch 0014, iter [03900, 05004], lr: 0.195815, loss: 2.3541
2022-08-01 00:09:37 - train: epoch 0014, iter [04000, 05004], lr: 0.195796, loss: 2.4840
2022-08-01 00:10:34 - train: epoch 0014, iter [04100, 05004], lr: 0.195777, loss: 2.3170
2022-08-01 00:11:32 - train: epoch 0014, iter [04200, 05004], lr: 0.195758, loss: 2.1680
2022-08-01 00:12:29 - train: epoch 0014, iter [04300, 05004], lr: 0.195739, loss: 2.0502
2022-08-01 00:13:27 - train: epoch 0014, iter [04400, 05004], lr: 0.195720, loss: 2.3062
2022-08-01 00:14:24 - train: epoch 0014, iter [04500, 05004], lr: 0.195701, loss: 2.2064
2022-08-01 00:15:21 - train: epoch 0014, iter [04600, 05004], lr: 0.195682, loss: 2.4092
2022-08-01 00:16:18 - train: epoch 0014, iter [04700, 05004], lr: 0.195662, loss: 2.2715
2022-08-01 00:17:17 - train: epoch 0014, iter [04800, 05004], lr: 0.195643, loss: 2.2532
2022-08-01 00:18:13 - train: epoch 0014, iter [04900, 05004], lr: 0.195624, loss: 2.3129
2022-08-01 00:19:07 - train: epoch 0014, iter [05000, 05004], lr: 0.195604, loss: 2.3431
2022-08-01 00:19:09 - train: epoch 014, train_loss: 2.2856
2022-08-01 00:21:15 - eval: epoch: 014, acc1: 52.672%, acc5: 78.200%, test_loss: 2.0205, per_image_load_time: 2.465ms, per_image_inference_time: 0.636ms
2022-08-01 00:21:15 - until epoch: 014, best_acc1: 53.486%
2022-08-01 00:21:15 - epoch 015 lr: 0.195603
2022-08-01 00:22:24 - train: epoch 0015, iter [00100, 05004], lr: 0.195584, loss: 1.9855
2022-08-01 00:23:23 - train: epoch 0015, iter [00200, 05004], lr: 0.195565, loss: 2.3233
2022-08-01 00:24:21 - train: epoch 0015, iter [00300, 05004], lr: 0.195545, loss: 2.4047
2022-08-01 00:25:16 - train: epoch 0015, iter [00400, 05004], lr: 0.195526, loss: 2.3180
2022-08-01 00:26:13 - train: epoch 0015, iter [00500, 05004], lr: 0.195506, loss: 2.3606
2022-08-01 00:27:11 - train: epoch 0015, iter [00600, 05004], lr: 0.195487, loss: 2.3433
2022-08-01 00:28:08 - train: epoch 0015, iter [00700, 05004], lr: 0.195467, loss: 2.2791
2022-08-01 00:29:05 - train: epoch 0015, iter [00800, 05004], lr: 0.195447, loss: 2.1484
2022-08-01 00:30:02 - train: epoch 0015, iter [00900, 05004], lr: 0.195427, loss: 2.2543
2022-08-01 00:30:59 - train: epoch 0015, iter [01000, 05004], lr: 0.195408, loss: 2.0784
2022-08-01 00:31:55 - train: epoch 0015, iter [01100, 05004], lr: 0.195388, loss: 2.0263
2022-08-01 00:32:53 - train: epoch 0015, iter [01200, 05004], lr: 0.195368, loss: 2.1777
2022-08-01 00:33:52 - train: epoch 0015, iter [01300, 05004], lr: 0.195348, loss: 2.4642
2022-08-01 00:34:49 - train: epoch 0015, iter [01400, 05004], lr: 0.195328, loss: 2.3092
2022-08-01 00:35:45 - train: epoch 0015, iter [01500, 05004], lr: 0.195308, loss: 2.2404
2022-08-01 00:36:44 - train: epoch 0015, iter [01600, 05004], lr: 0.195288, loss: 2.3086
2022-08-01 00:37:40 - train: epoch 0015, iter [01700, 05004], lr: 0.195268, loss: 2.2891
2022-08-01 00:38:38 - train: epoch 0015, iter [01800, 05004], lr: 0.195248, loss: 2.0052
2022-08-01 00:39:33 - train: epoch 0015, iter [01900, 05004], lr: 0.195228, loss: 2.0737
2022-08-01 00:40:31 - train: epoch 0015, iter [02000, 05004], lr: 0.195208, loss: 2.1006
2022-08-01 00:41:30 - train: epoch 0015, iter [02100, 05004], lr: 0.195187, loss: 2.0212
2022-08-01 00:42:26 - train: epoch 0015, iter [02200, 05004], lr: 0.195167, loss: 2.3327
2022-08-01 00:43:24 - train: epoch 0015, iter [02300, 05004], lr: 0.195147, loss: 2.2025
2022-08-01 00:44:22 - train: epoch 0015, iter [02400, 05004], lr: 0.195126, loss: 2.2214
2022-08-01 00:45:19 - train: epoch 0015, iter [02500, 05004], lr: 0.195106, loss: 2.3191
2022-08-01 00:46:16 - train: epoch 0015, iter [02600, 05004], lr: 0.195086, loss: 2.2402
2022-08-01 00:47:12 - train: epoch 0015, iter [02700, 05004], lr: 0.195065, loss: 2.4389
2022-08-01 00:48:09 - train: epoch 0015, iter [02800, 05004], lr: 0.195045, loss: 2.2468
2022-08-01 00:49:08 - train: epoch 0015, iter [02900, 05004], lr: 0.195024, loss: 2.3224
2022-08-01 00:50:04 - train: epoch 0015, iter [03000, 05004], lr: 0.195003, loss: 2.1487
2022-08-01 00:51:01 - train: epoch 0015, iter [03100, 05004], lr: 0.194983, loss: 2.2112
2022-08-01 00:51:59 - train: epoch 0015, iter [03200, 05004], lr: 0.194962, loss: 2.3114
2022-08-01 00:52:57 - train: epoch 0015, iter [03300, 05004], lr: 0.194941, loss: 2.4112
2022-08-01 00:53:53 - train: epoch 0015, iter [03400, 05004], lr: 0.194921, loss: 2.1281
2022-08-01 00:54:50 - train: epoch 0015, iter [03500, 05004], lr: 0.194900, loss: 2.4052
2022-08-01 00:55:46 - train: epoch 0015, iter [03600, 05004], lr: 0.194879, loss: 2.4419
2022-08-01 00:56:43 - train: epoch 0015, iter [03700, 05004], lr: 0.194858, loss: 2.1475
2022-08-01 00:57:41 - train: epoch 0015, iter [03800, 05004], lr: 0.194837, loss: 2.1127
2022-08-01 00:58:39 - train: epoch 0015, iter [03900, 05004], lr: 0.194816, loss: 2.3318
2022-08-01 00:59:35 - train: epoch 0015, iter [04000, 05004], lr: 0.194795, loss: 2.2822
2022-08-01 01:00:33 - train: epoch 0015, iter [04100, 05004], lr: 0.194774, loss: 2.1581
2022-08-01 01:01:29 - train: epoch 0015, iter [04200, 05004], lr: 0.194753, loss: 2.1578
2022-08-01 01:02:26 - train: epoch 0015, iter [04300, 05004], lr: 0.194732, loss: 2.2430
2022-08-01 01:03:24 - train: epoch 0015, iter [04400, 05004], lr: 0.194711, loss: 2.2509
2022-08-01 01:04:21 - train: epoch 0015, iter [04500, 05004], lr: 0.194689, loss: 2.2226
2022-08-01 01:05:17 - train: epoch 0015, iter [04600, 05004], lr: 0.194668, loss: 2.0759
2022-08-01 01:06:16 - train: epoch 0015, iter [04700, 05004], lr: 0.194647, loss: 2.2525
2022-08-01 01:07:12 - train: epoch 0015, iter [04800, 05004], lr: 0.194625, loss: 2.1129
2022-08-01 01:08:09 - train: epoch 0015, iter [04900, 05004], lr: 0.194604, loss: 2.1774
2022-08-01 01:09:03 - train: epoch 0015, iter [05000, 05004], lr: 0.194583, loss: 2.4077
2022-08-01 01:09:04 - train: epoch 015, train_loss: 2.2691
2022-08-01 01:11:11 - eval: epoch: 015, acc1: 54.128%, acc5: 79.230%, test_loss: 1.9476, per_image_load_time: 3.997ms, per_image_inference_time: 0.618ms
2022-08-01 01:11:11 - until epoch: 015, best_acc1: 54.128%
2022-08-01 01:11:11 - epoch 016 lr: 0.194582
2022-08-01 01:12:18 - train: epoch 0016, iter [00100, 05004], lr: 0.194560, loss: 2.0987
2022-08-01 01:13:16 - train: epoch 0016, iter [00200, 05004], lr: 0.194539, loss: 2.1009
2022-08-01 01:14:11 - train: epoch 0016, iter [00300, 05004], lr: 0.194517, loss: 2.3866
2022-08-01 01:15:09 - train: epoch 0016, iter [00400, 05004], lr: 0.194496, loss: 2.5675
2022-08-01 01:16:08 - train: epoch 0016, iter [00500, 05004], lr: 0.194474, loss: 2.2399
2022-08-01 01:17:05 - train: epoch 0016, iter [00600, 05004], lr: 0.194452, loss: 2.3250
2022-08-01 01:18:01 - train: epoch 0016, iter [00700, 05004], lr: 0.194431, loss: 1.9692
2022-08-01 01:18:59 - train: epoch 0016, iter [00800, 05004], lr: 0.194409, loss: 2.3413
2022-08-01 01:19:56 - train: epoch 0016, iter [00900, 05004], lr: 0.194387, loss: 2.3268
2022-08-01 01:20:54 - train: epoch 0016, iter [01000, 05004], lr: 0.194365, loss: 2.1066
2022-08-01 01:21:49 - train: epoch 0016, iter [01100, 05004], lr: 0.194343, loss: 2.1186
2022-08-01 01:22:46 - train: epoch 0016, iter [01200, 05004], lr: 0.194321, loss: 2.1269
2022-08-01 01:23:44 - train: epoch 0016, iter [01300, 05004], lr: 0.194299, loss: 2.0395
2022-08-01 01:24:41 - train: epoch 0016, iter [01400, 05004], lr: 0.194277, loss: 2.0942
2022-08-01 01:25:37 - train: epoch 0016, iter [01500, 05004], lr: 0.194255, loss: 2.2960
2022-08-01 01:26:37 - train: epoch 0016, iter [01600, 05004], lr: 0.194233, loss: 2.5164
2022-08-01 01:27:32 - train: epoch 0016, iter [01700, 05004], lr: 0.194211, loss: 2.4014
2022-08-01 01:28:31 - train: epoch 0016, iter [01800, 05004], lr: 0.194189, loss: 2.5177
2022-08-01 01:29:27 - train: epoch 0016, iter [01900, 05004], lr: 0.194167, loss: 2.2118
2022-08-01 01:30:25 - train: epoch 0016, iter [02000, 05004], lr: 0.194144, loss: 2.2021
2022-08-01 01:31:22 - train: epoch 0016, iter [02100, 05004], lr: 0.194122, loss: 2.2905
2022-08-01 01:32:21 - train: epoch 0016, iter [02200, 05004], lr: 0.194100, loss: 2.2641
2022-08-01 01:33:17 - train: epoch 0016, iter [02300, 05004], lr: 0.194077, loss: 2.1638
2022-08-01 01:34:14 - train: epoch 0016, iter [02400, 05004], lr: 0.194055, loss: 2.3670
2022-08-01 01:35:11 - train: epoch 0016, iter [02500, 05004], lr: 0.194032, loss: 2.1809
2022-08-01 01:36:08 - train: epoch 0016, iter [02600, 05004], lr: 0.194010, loss: 2.4045
2022-08-01 01:37:04 - train: epoch 0016, iter [02700, 05004], lr: 0.193987, loss: 2.3502
2022-08-01 01:38:03 - train: epoch 0016, iter [02800, 05004], lr: 0.193965, loss: 2.1982
2022-08-01 01:38:58 - train: epoch 0016, iter [02900, 05004], lr: 0.193942, loss: 2.2193
2022-08-01 01:39:56 - train: epoch 0016, iter [03000, 05004], lr: 0.193919, loss: 2.3417
2022-08-01 01:40:53 - train: epoch 0016, iter [03100, 05004], lr: 0.193897, loss: 2.3179
2022-08-01 01:41:50 - train: epoch 0016, iter [03200, 05004], lr: 0.193874, loss: 2.3890
2022-08-01 01:42:47 - train: epoch 0016, iter [03300, 05004], lr: 0.193851, loss: 2.4554
2022-08-01 01:43:45 - train: epoch 0016, iter [03400, 05004], lr: 0.193828, loss: 2.2029
2022-08-01 01:44:41 - train: epoch 0016, iter [03500, 05004], lr: 0.193805, loss: 2.2684
2022-08-01 01:45:38 - train: epoch 0016, iter [03600, 05004], lr: 0.193783, loss: 2.0480
2022-08-01 01:46:36 - train: epoch 0016, iter [03700, 05004], lr: 0.193760, loss: 2.4566
2022-08-01 01:47:33 - train: epoch 0016, iter [03800, 05004], lr: 0.193737, loss: 2.5817
2022-08-01 01:48:30 - train: epoch 0016, iter [03900, 05004], lr: 0.193714, loss: 2.3471
2022-08-01 01:49:27 - train: epoch 0016, iter [04000, 05004], lr: 0.193690, loss: 2.2571
2022-08-01 01:50:24 - train: epoch 0016, iter [04100, 05004], lr: 0.193667, loss: 2.1887
2022-08-01 01:51:21 - train: epoch 0016, iter [04200, 05004], lr: 0.193644, loss: 2.0531
2022-08-01 01:52:18 - train: epoch 0016, iter [04300, 05004], lr: 0.193621, loss: 2.0470
2022-08-01 01:53:15 - train: epoch 0016, iter [04400, 05004], lr: 0.193598, loss: 2.1664
2022-08-01 01:54:11 - train: epoch 0016, iter [04500, 05004], lr: 0.193574, loss: 2.4644
2022-08-01 01:55:09 - train: epoch 0016, iter [04600, 05004], lr: 0.193551, loss: 2.3486
2022-08-01 01:56:05 - train: epoch 0016, iter [04700, 05004], lr: 0.193528, loss: 2.2820
2022-08-01 01:57:00 - train: epoch 0016, iter [04800, 05004], lr: 0.193504, loss: 2.1934
2022-08-01 01:57:58 - train: epoch 0016, iter [04900, 05004], lr: 0.193481, loss: 2.2550
2022-08-01 01:58:51 - train: epoch 0016, iter [05000, 05004], lr: 0.193457, loss: 2.2619
2022-08-01 01:58:53 - train: epoch 016, train_loss: 2.2558
2022-08-01 02:00:57 - eval: epoch: 016, acc1: 54.010%, acc5: 78.852%, test_loss: 1.9657, per_image_load_time: 4.209ms, per_image_inference_time: 0.569ms
2022-08-01 02:00:58 - until epoch: 016, best_acc1: 54.128%
2022-08-01 02:00:58 - epoch 017 lr: 0.193456
2022-08-01 02:02:05 - train: epoch 0017, iter [00100, 05004], lr: 0.193433, loss: 2.2338
2022-08-01 02:03:02 - train: epoch 0017, iter [00200, 05004], lr: 0.193409, loss: 2.3216
2022-08-01 02:04:00 - train: epoch 0017, iter [00300, 05004], lr: 0.193386, loss: 2.3458
2022-08-01 02:04:55 - train: epoch 0017, iter [00400, 05004], lr: 0.193362, loss: 1.8419
2022-08-01 02:05:52 - train: epoch 0017, iter [00500, 05004], lr: 0.193338, loss: 2.2449
2022-08-01 02:06:48 - train: epoch 0017, iter [00600, 05004], lr: 0.193315, loss: 2.4646
2022-08-01 02:07:45 - train: epoch 0017, iter [00700, 05004], lr: 0.193291, loss: 2.1500
2022-08-01 02:08:42 - train: epoch 0017, iter [00800, 05004], lr: 0.193267, loss: 2.1644
2022-08-01 02:09:39 - train: epoch 0017, iter [00900, 05004], lr: 0.193243, loss: 2.0777
2022-08-01 02:10:37 - train: epoch 0017, iter [01000, 05004], lr: 0.193219, loss: 2.0123
2022-08-01 02:11:35 - train: epoch 0017, iter [01100, 05004], lr: 0.193195, loss: 2.4755
2022-08-01 02:12:31 - train: epoch 0017, iter [01200, 05004], lr: 0.193171, loss: 2.0805
2022-08-01 02:13:30 - train: epoch 0017, iter [01300, 05004], lr: 0.193147, loss: 2.2381
2022-08-01 02:14:28 - train: epoch 0017, iter [01400, 05004], lr: 0.193123, loss: 2.2317
2022-08-01 02:15:24 - train: epoch 0017, iter [01500, 05004], lr: 0.193099, loss: 2.1002
2022-08-01 02:16:21 - train: epoch 0017, iter [01600, 05004], lr: 0.193075, loss: 2.2553
2022-08-01 02:17:19 - train: epoch 0017, iter [01700, 05004], lr: 0.193051, loss: 2.1979
2022-08-01 02:18:15 - train: epoch 0017, iter [01800, 05004], lr: 0.193027, loss: 2.2714
2022-08-01 02:19:14 - train: epoch 0017, iter [01900, 05004], lr: 0.193002, loss: 2.2839
2022-08-01 02:20:11 - train: epoch 0017, iter [02000, 05004], lr: 0.192978, loss: 2.2020
2022-08-01 02:21:08 - train: epoch 0017, iter [02100, 05004], lr: 0.192954, loss: 2.3126
2022-08-01 02:22:05 - train: epoch 0017, iter [02200, 05004], lr: 0.192929, loss: 2.2314
2022-08-01 02:23:02 - train: epoch 0017, iter [02300, 05004], lr: 0.192905, loss: 2.3093
2022-08-01 02:24:00 - train: epoch 0017, iter [02400, 05004], lr: 0.192880, loss: 2.3581
2022-08-01 02:24:56 - train: epoch 0017, iter [02500, 05004], lr: 0.192856, loss: 2.2126
2022-08-01 02:25:54 - train: epoch 0017, iter [02600, 05004], lr: 0.192831, loss: 2.2095
2022-08-01 02:26:51 - train: epoch 0017, iter [02700, 05004], lr: 0.192807, loss: 2.1836
2022-08-01 02:27:49 - train: epoch 0017, iter [02800, 05004], lr: 0.192782, loss: 2.1594
2022-08-01 02:28:45 - train: epoch 0017, iter [02900, 05004], lr: 0.192757, loss: 2.5086
2022-08-01 02:29:42 - train: epoch 0017, iter [03000, 05004], lr: 0.192733, loss: 2.0651
2022-08-01 02:30:39 - train: epoch 0017, iter [03100, 05004], lr: 0.192708, loss: 2.4002
2022-08-01 02:31:37 - train: epoch 0017, iter [03200, 05004], lr: 0.192683, loss: 2.1402
2022-08-01 02:32:34 - train: epoch 0017, iter [03300, 05004], lr: 0.192658, loss: 2.2287
2022-08-01 02:33:31 - train: epoch 0017, iter [03400, 05004], lr: 0.192633, loss: 1.9234
2022-08-01 02:34:27 - train: epoch 0017, iter [03500, 05004], lr: 0.192609, loss: 2.4226
2022-08-01 02:35:24 - train: epoch 0017, iter [03600, 05004], lr: 0.192584, loss: 2.5029
2022-08-01 02:36:22 - train: epoch 0017, iter [03700, 05004], lr: 0.192559, loss: 2.1716
2022-08-01 02:37:20 - train: epoch 0017, iter [03800, 05004], lr: 0.192534, loss: 2.3068
2022-08-01 02:38:17 - train: epoch 0017, iter [03900, 05004], lr: 0.192509, loss: 2.0603
2022-08-01 02:39:15 - train: epoch 0017, iter [04000, 05004], lr: 0.192483, loss: 2.2724
2022-08-01 02:40:12 - train: epoch 0017, iter [04100, 05004], lr: 0.192458, loss: 2.3752
2022-08-01 02:41:09 - train: epoch 0017, iter [04200, 05004], lr: 0.192433, loss: 2.1914
2022-08-01 02:42:07 - train: epoch 0017, iter [04300, 05004], lr: 0.192408, loss: 2.2941
2022-08-01 02:43:04 - train: epoch 0017, iter [04400, 05004], lr: 0.192383, loss: 2.3499
2022-08-01 02:44:02 - train: epoch 0017, iter [04500, 05004], lr: 0.192357, loss: 2.2991
2022-08-01 02:44:58 - train: epoch 0017, iter [04600, 05004], lr: 0.192332, loss: 2.1552
2022-08-01 02:45:55 - train: epoch 0017, iter [04700, 05004], lr: 0.192306, loss: 2.3249
2022-08-01 02:46:53 - train: epoch 0017, iter [04800, 05004], lr: 0.192281, loss: 2.2038
2022-08-01 02:47:50 - train: epoch 0017, iter [04900, 05004], lr: 0.192256, loss: 2.4558
2022-08-01 02:48:43 - train: epoch 0017, iter [05000, 05004], lr: 0.192230, loss: 1.9750
2022-08-01 02:48:44 - train: epoch 017, train_loss: 2.2404
2022-08-01 02:50:49 - eval: epoch: 017, acc1: 54.432%, acc5: 79.468%, test_loss: 1.9464, per_image_load_time: 3.339ms, per_image_inference_time: 0.581ms
2022-08-01 02:50:49 - until epoch: 017, best_acc1: 54.432%
2022-08-01 02:50:49 - epoch 018 lr: 0.192229
2022-08-01 02:51:55 - train: epoch 0018, iter [00100, 05004], lr: 0.192203, loss: 2.2236
2022-08-01 02:52:53 - train: epoch 0018, iter [00200, 05004], lr: 0.192178, loss: 2.3653
2022-08-01 02:53:49 - train: epoch 0018, iter [00300, 05004], lr: 0.192152, loss: 2.5975
2022-08-01 02:54:46 - train: epoch 0018, iter [00400, 05004], lr: 0.192126, loss: 2.5150
2022-08-01 02:55:44 - train: epoch 0018, iter [00500, 05004], lr: 0.192101, loss: 2.3692
2022-08-01 02:56:42 - train: epoch 0018, iter [00600, 05004], lr: 0.192075, loss: 2.3116
2022-08-01 02:57:39 - train: epoch 0018, iter [00700, 05004], lr: 0.192049, loss: 1.9507
2022-08-01 02:58:35 - train: epoch 0018, iter [00800, 05004], lr: 0.192023, loss: 2.1268
2022-08-01 02:59:34 - train: epoch 0018, iter [00900, 05004], lr: 0.191997, loss: 2.3243
2022-08-01 03:00:30 - train: epoch 0018, iter [01000, 05004], lr: 0.191972, loss: 2.0843
2022-08-01 03:01:27 - train: epoch 0018, iter [01100, 05004], lr: 0.191946, loss: 2.2960
2022-08-01 03:02:24 - train: epoch 0018, iter [01200, 05004], lr: 0.191920, loss: 2.1779
2022-08-01 03:03:20 - train: epoch 0018, iter [01300, 05004], lr: 0.191894, loss: 2.3759
2022-08-01 03:04:17 - train: epoch 0018, iter [01400, 05004], lr: 0.191867, loss: 2.2865
2022-08-01 03:05:14 - train: epoch 0018, iter [01500, 05004], lr: 0.191841, loss: 2.4239
2022-08-01 03:06:12 - train: epoch 0018, iter [01600, 05004], lr: 0.191815, loss: 2.1995
2022-08-01 03:07:07 - train: epoch 0018, iter [01700, 05004], lr: 0.191789, loss: 2.3206
2022-08-01 03:08:05 - train: epoch 0018, iter [01800, 05004], lr: 0.191763, loss: 1.9541
2022-08-01 03:09:03 - train: epoch 0018, iter [01900, 05004], lr: 0.191736, loss: 2.2033
2022-08-01 03:10:00 - train: epoch 0018, iter [02000, 05004], lr: 0.191710, loss: 2.6780
2022-08-01 03:10:56 - train: epoch 0018, iter [02100, 05004], lr: 0.191684, loss: 2.3829
2022-08-01 03:11:52 - train: epoch 0018, iter [02200, 05004], lr: 0.191657, loss: 2.1716
2022-08-01 03:12:49 - train: epoch 0018, iter [02300, 05004], lr: 0.191631, loss: 2.3961
2022-08-01 03:13:46 - train: epoch 0018, iter [02400, 05004], lr: 0.191604, loss: 2.2114
2022-08-01 03:14:43 - train: epoch 0018, iter [02500, 05004], lr: 0.191578, loss: 1.9889
2022-08-01 03:15:41 - train: epoch 0018, iter [02600, 05004], lr: 0.191551, loss: 2.2393
2022-08-01 03:16:36 - train: epoch 0018, iter [02700, 05004], lr: 0.191525, loss: 2.1541
2022-08-01 03:17:33 - train: epoch 0018, iter [02800, 05004], lr: 0.191498, loss: 1.9776
2022-08-01 03:18:29 - train: epoch 0018, iter [02900, 05004], lr: 0.191471, loss: 2.0680
2022-08-01 03:19:27 - train: epoch 0018, iter [03000, 05004], lr: 0.191445, loss: 2.0743
2022-08-01 03:20:23 - train: epoch 0018, iter [03100, 05004], lr: 0.191418, loss: 2.5784
2022-08-01 03:21:19 - train: epoch 0018, iter [03200, 05004], lr: 0.191391, loss: 2.1881
2022-08-01 03:22:16 - train: epoch 0018, iter [03300, 05004], lr: 0.191364, loss: 2.1862
2022-08-01 03:23:12 - train: epoch 0018, iter [03400, 05004], lr: 0.191337, loss: 2.2575
2022-08-01 03:24:09 - train: epoch 0018, iter [03500, 05004], lr: 0.191310, loss: 2.3313
2022-08-01 03:25:06 - train: epoch 0018, iter [03600, 05004], lr: 0.191283, loss: 2.2873
2022-08-01 03:26:02 - train: epoch 0018, iter [03700, 05004], lr: 0.191256, loss: 2.5268
2022-08-01 03:26:59 - train: epoch 0018, iter [03800, 05004], lr: 0.191229, loss: 2.4216
2022-08-01 03:27:56 - train: epoch 0018, iter [03900, 05004], lr: 0.191202, loss: 2.2979
2022-08-01 03:28:54 - train: epoch 0018, iter [04000, 05004], lr: 0.191175, loss: 2.0234
2022-08-01 03:29:51 - train: epoch 0018, iter [04100, 05004], lr: 0.191148, loss: 2.1978
2022-08-01 03:30:47 - train: epoch 0018, iter [04200, 05004], lr: 0.191121, loss: 2.0963
2022-08-01 03:31:44 - train: epoch 0018, iter [04300, 05004], lr: 0.191094, loss: 2.1230
2022-08-01 03:32:41 - train: epoch 0018, iter [04400, 05004], lr: 0.191066, loss: 2.1758
2022-08-01 03:33:38 - train: epoch 0018, iter [04500, 05004], lr: 0.191039, loss: 2.2563
2022-08-01 03:34:35 - train: epoch 0018, iter [04600, 05004], lr: 0.191012, loss: 2.1428
2022-08-01 03:35:31 - train: epoch 0018, iter [04700, 05004], lr: 0.190984, loss: 2.3535
2022-08-01 03:36:28 - train: epoch 0018, iter [04800, 05004], lr: 0.190957, loss: 2.3051
2022-08-01 03:37:25 - train: epoch 0018, iter [04900, 05004], lr: 0.190929, loss: 2.3245
2022-08-01 03:38:20 - train: epoch 0018, iter [05000, 05004], lr: 0.190902, loss: 2.5172
2022-08-01 03:38:21 - train: epoch 018, train_loss: 2.2281
2022-08-01 03:40:27 - eval: epoch: 018, acc1: 53.686%, acc5: 78.486%, test_loss: 1.9960, per_image_load_time: 3.782ms, per_image_inference_time: 0.616ms
2022-08-01 03:40:27 - until epoch: 018, best_acc1: 54.432%
2022-08-01 03:40:27 - epoch 019 lr: 0.190900
2022-08-01 03:41:34 - train: epoch 0019, iter [00100, 05004], lr: 0.190873, loss: 1.9576
2022-08-01 03:42:32 - train: epoch 0019, iter [00200, 05004], lr: 0.190845, loss: 2.2123
2022-08-01 03:43:28 - train: epoch 0019, iter [00300, 05004], lr: 0.190818, loss: 2.2920
2022-08-01 03:44:26 - train: epoch 0019, iter [00400, 05004], lr: 0.190790, loss: 2.2279
2022-08-01 03:45:23 - train: epoch 0019, iter [00500, 05004], lr: 0.190762, loss: 2.2273
2022-08-01 03:46:19 - train: epoch 0019, iter [00600, 05004], lr: 0.190735, loss: 2.3184
2022-08-01 03:47:16 - train: epoch 0019, iter [00700, 05004], lr: 0.190707, loss: 1.9041
2022-08-01 03:48:12 - train: epoch 0019, iter [00800, 05004], lr: 0.190679, loss: 2.2996
2022-08-01 03:49:11 - train: epoch 0019, iter [00900, 05004], lr: 0.190651, loss: 2.3053
2022-08-01 03:50:08 - train: epoch 0019, iter [01000, 05004], lr: 0.190623, loss: 2.2143
2022-08-01 03:51:05 - train: epoch 0019, iter [01100, 05004], lr: 0.190595, loss: 2.1546
2022-08-01 03:52:04 - train: epoch 0019, iter [01200, 05004], lr: 0.190567, loss: 2.3747
2022-08-01 03:53:00 - train: epoch 0019, iter [01300, 05004], lr: 0.190539, loss: 2.2445
2022-08-01 03:53:58 - train: epoch 0019, iter [01400, 05004], lr: 0.190511, loss: 2.2970
2022-08-01 03:54:56 - train: epoch 0019, iter [01500, 05004], lr: 0.190483, loss: 2.3687
2022-08-01 03:55:53 - train: epoch 0019, iter [01600, 05004], lr: 0.190455, loss: 2.1959
2022-08-01 03:56:49 - train: epoch 0019, iter [01700, 05004], lr: 0.190427, loss: 2.5247
2022-08-01 03:57:47 - train: epoch 0019, iter [01800, 05004], lr: 0.190398, loss: 2.0755
2022-08-01 03:58:42 - train: epoch 0019, iter [01900, 05004], lr: 0.190370, loss: 2.5667
2022-08-01 03:59:40 - train: epoch 0019, iter [02000, 05004], lr: 0.190342, loss: 2.1620
2022-08-01 04:00:37 - train: epoch 0019, iter [02100, 05004], lr: 0.190314, loss: 2.1608
2022-08-01 04:01:33 - train: epoch 0019, iter [02200, 05004], lr: 0.190285, loss: 2.2882
2022-08-01 04:02:29 - train: epoch 0019, iter [02300, 05004], lr: 0.190257, loss: 2.3534
2022-08-01 04:03:26 - train: epoch 0019, iter [02400, 05004], lr: 0.190228, loss: 2.3874
2022-08-01 04:04:23 - train: epoch 0019, iter [02500, 05004], lr: 0.190200, loss: 2.2474
2022-08-01 04:05:19 - train: epoch 0019, iter [02600, 05004], lr: 0.190171, loss: 2.1376
2022-08-01 04:06:16 - train: epoch 0019, iter [02700, 05004], lr: 0.190143, loss: 2.2020
2022-08-01 04:07:13 - train: epoch 0019, iter [02800, 05004], lr: 0.190114, loss: 2.5148
2022-08-01 04:08:10 - train: epoch 0019, iter [02900, 05004], lr: 0.190085, loss: 2.3446
2022-08-01 04:09:05 - train: epoch 0019, iter [03000, 05004], lr: 0.190057, loss: 2.2915
2022-08-01 04:10:01 - train: epoch 0019, iter [03100, 05004], lr: 0.190028, loss: 2.1480
2022-08-01 04:10:56 - train: epoch 0019, iter [03200, 05004], lr: 0.189999, loss: 1.8584
2022-08-01 04:11:53 - train: epoch 0019, iter [03300, 05004], lr: 0.189970, loss: 2.0234
2022-08-01 04:12:49 - train: epoch 0019, iter [03400, 05004], lr: 0.189941, loss: 2.2767
2022-08-01 04:13:45 - train: epoch 0019, iter [03500, 05004], lr: 0.189912, loss: 2.3141
2022-08-01 04:14:41 - train: epoch 0019, iter [03600, 05004], lr: 0.189883, loss: 2.0758
2022-08-01 04:15:36 - train: epoch 0019, iter [03700, 05004], lr: 0.189854, loss: 2.1708
2022-08-01 04:16:32 - train: epoch 0019, iter [03800, 05004], lr: 0.189825, loss: 2.4130
2022-08-01 04:17:28 - train: epoch 0019, iter [03900, 05004], lr: 0.189796, loss: 2.2640
2022-08-01 04:18:23 - train: epoch 0019, iter [04000, 05004], lr: 0.189767, loss: 2.3523
2022-08-01 04:19:20 - train: epoch 0019, iter [04100, 05004], lr: 0.189738, loss: 2.2852
2022-08-01 04:20:15 - train: epoch 0019, iter [04200, 05004], lr: 0.189709, loss: 2.3489
2022-08-01 04:21:11 - train: epoch 0019, iter [04300, 05004], lr: 0.189680, loss: 2.1042
2022-08-01 04:22:08 - train: epoch 0019, iter [04400, 05004], lr: 0.189650, loss: 2.4341
2022-08-01 04:23:02 - train: epoch 0019, iter [04500, 05004], lr: 0.189621, loss: 2.3155
2022-08-01 04:23:58 - train: epoch 0019, iter [04600, 05004], lr: 0.189592, loss: 2.3171
2022-08-01 04:24:54 - train: epoch 0019, iter [04700, 05004], lr: 0.189562, loss: 2.2343
2022-08-01 04:25:49 - train: epoch 0019, iter [04800, 05004], lr: 0.189533, loss: 2.3221
2022-08-01 04:26:44 - train: epoch 0019, iter [04900, 05004], lr: 0.189504, loss: 2.1654
2022-08-01 04:27:38 - train: epoch 0019, iter [05000, 05004], lr: 0.189474, loss: 2.1901
2022-08-01 04:27:39 - train: epoch 019, train_loss: 2.2160
2022-08-01 04:29:47 - eval: epoch: 019, acc1: 53.802%, acc5: 79.148%, test_loss: 1.9638, per_image_load_time: 4.298ms, per_image_inference_time: 0.580ms
2022-08-01 04:29:47 - until epoch: 019, best_acc1: 54.432%
2022-08-01 04:29:47 - epoch 020 lr: 0.189473
2022-08-01 04:30:54 - train: epoch 0020, iter [00100, 05004], lr: 0.189443, loss: 2.0902
2022-08-01 04:31:52 - train: epoch 0020, iter [00200, 05004], lr: 0.189414, loss: 2.0795
2022-08-01 04:32:51 - train: epoch 0020, iter [00300, 05004], lr: 0.189384, loss: 2.4030
2022-08-01 04:33:51 - train: epoch 0020, iter [00400, 05004], lr: 0.189355, loss: 2.0822
2022-08-01 04:34:50 - train: epoch 0020, iter [00500, 05004], lr: 0.189325, loss: 2.0409
2022-08-01 04:35:50 - train: epoch 0020, iter [00600, 05004], lr: 0.189295, loss: 2.2375
2022-08-01 04:36:47 - train: epoch 0020, iter [00700, 05004], lr: 0.189265, loss: 2.1649
2022-08-01 04:37:46 - train: epoch 0020, iter [00800, 05004], lr: 0.189236, loss: 2.2731
2022-08-01 04:38:43 - train: epoch 0020, iter [00900, 05004], lr: 0.189206, loss: 2.3158
2022-08-01 04:39:43 - train: epoch 0020, iter [01000, 05004], lr: 0.189176, loss: 2.3402
2022-08-01 04:40:39 - train: epoch 0020, iter [01100, 05004], lr: 0.189146, loss: 1.9768
2022-08-01 04:41:38 - train: epoch 0020, iter [01200, 05004], lr: 0.189116, loss: 2.0880
2022-08-01 04:42:35 - train: epoch 0020, iter [01300, 05004], lr: 0.189086, loss: 2.2190
2022-08-01 04:43:32 - train: epoch 0020, iter [01400, 05004], lr: 0.189056, loss: 2.2032
2022-08-01 04:44:29 - train: epoch 0020, iter [01500, 05004], lr: 0.189026, loss: 2.1959
2022-08-01 04:45:27 - train: epoch 0020, iter [01600, 05004], lr: 0.188996, loss: 2.3745
2022-08-01 04:46:24 - train: epoch 0020, iter [01700, 05004], lr: 0.188966, loss: 2.3210
2022-08-01 04:47:20 - train: epoch 0020, iter [01800, 05004], lr: 0.188935, loss: 2.1125
2022-08-01 04:48:16 - train: epoch 0020, iter [01900, 05004], lr: 0.188905, loss: 2.0837
2022-08-01 04:49:14 - train: epoch 0020, iter [02000, 05004], lr: 0.188875, loss: 2.0760
2022-08-01 04:50:09 - train: epoch 0020, iter [02100, 05004], lr: 0.188845, loss: 2.3576
2022-08-01 04:51:07 - train: epoch 0020, iter [02200, 05004], lr: 0.188814, loss: 2.1168
2022-08-01 04:52:03 - train: epoch 0020, iter [02300, 05004], lr: 0.188784, loss: 2.2005
2022-08-01 04:53:00 - train: epoch 0020, iter [02400, 05004], lr: 0.188753, loss: 2.4879
2022-08-01 04:53:58 - train: epoch 0020, iter [02500, 05004], lr: 0.188723, loss: 2.1254
2022-08-01 04:54:54 - train: epoch 0020, iter [02600, 05004], lr: 0.188692, loss: 1.9915
2022-08-01 04:55:49 - train: epoch 0020, iter [02700, 05004], lr: 0.188662, loss: 2.1238
2022-08-01 04:56:47 - train: epoch 0020, iter [02800, 05004], lr: 0.188631, loss: 2.3523
2022-08-01 04:57:43 - train: epoch 0020, iter [02900, 05004], lr: 0.188601, loss: 2.2525
2022-08-01 04:58:41 - train: epoch 0020, iter [03000, 05004], lr: 0.188570, loss: 2.3562
2022-08-01 04:59:37 - train: epoch 0020, iter [03100, 05004], lr: 0.188539, loss: 2.5097
2022-08-01 05:00:34 - train: epoch 0020, iter [03200, 05004], lr: 0.188509, loss: 2.2436
2022-08-01 05:01:30 - train: epoch 0020, iter [03300, 05004], lr: 0.188478, loss: 1.9036
2022-08-01 05:02:27 - train: epoch 0020, iter [03400, 05004], lr: 0.188447, loss: 2.3432
2022-08-01 05:03:22 - train: epoch 0020, iter [03500, 05004], lr: 0.188416, loss: 2.0244
2022-08-01 05:04:18 - train: epoch 0020, iter [03600, 05004], lr: 0.188385, loss: 2.0470
2022-08-01 05:05:14 - train: epoch 0020, iter [03700, 05004], lr: 0.188354, loss: 2.1587
2022-08-01 05:06:10 - train: epoch 0020, iter [03800, 05004], lr: 0.188323, loss: 2.3454
2022-08-01 05:07:05 - train: epoch 0020, iter [03900, 05004], lr: 0.188292, loss: 2.4679
2022-08-01 05:08:02 - train: epoch 0020, iter [04000, 05004], lr: 0.188261, loss: 2.2655
2022-08-01 05:08:57 - train: epoch 0020, iter [04100, 05004], lr: 0.188230, loss: 2.0807
2022-08-01 05:09:55 - train: epoch 0020, iter [04200, 05004], lr: 0.188199, loss: 2.0064
2022-08-01 05:10:49 - train: epoch 0020, iter [04300, 05004], lr: 0.188168, loss: 2.3234
2022-08-01 05:11:43 - train: epoch 0020, iter [04400, 05004], lr: 0.188137, loss: 2.1829
2022-08-01 05:12:39 - train: epoch 0020, iter [04500, 05004], lr: 0.188105, loss: 2.4649
2022-08-01 05:13:35 - train: epoch 0020, iter [04600, 05004], lr: 0.188074, loss: 2.1853
2022-08-01 05:14:31 - train: epoch 0020, iter [04700, 05004], lr: 0.188043, loss: 2.1431
2022-08-01 05:15:27 - train: epoch 0020, iter [04800, 05004], lr: 0.188011, loss: 2.1968
2022-08-01 05:16:22 - train: epoch 0020, iter [04900, 05004], lr: 0.187980, loss: 2.3550
2022-08-01 05:17:15 - train: epoch 0020, iter [05000, 05004], lr: 0.187949, loss: 2.1174
2022-08-01 05:17:16 - train: epoch 020, train_loss: 2.2017
2022-08-01 05:19:23 - eval: epoch: 020, acc1: 54.832%, acc5: 79.752%, test_loss: 1.9215, per_image_load_time: 3.831ms, per_image_inference_time: 0.586ms
2022-08-01 05:19:24 - until epoch: 020, best_acc1: 54.832%
2022-08-01 05:19:24 - epoch 021 lr: 0.187947
2022-08-01 05:20:31 - train: epoch 0021, iter [00100, 05004], lr: 0.187916, loss: 2.3183
2022-08-01 05:21:30 - train: epoch 0021, iter [00200, 05004], lr: 0.187884, loss: 2.1562
2022-08-01 05:22:27 - train: epoch 0021, iter [00300, 05004], lr: 0.187853, loss: 1.9676
2022-08-01 05:23:25 - train: epoch 0021, iter [00400, 05004], lr: 0.187821, loss: 2.1952
2022-08-01 05:24:23 - train: epoch 0021, iter [00500, 05004], lr: 0.187790, loss: 1.9851
2022-08-01 05:25:22 - train: epoch 0021, iter [00600, 05004], lr: 0.187758, loss: 2.2158
2022-08-01 05:26:20 - train: epoch 0021, iter [00700, 05004], lr: 0.187726, loss: 2.0462
2022-08-01 05:27:18 - train: epoch 0021, iter [00800, 05004], lr: 0.187695, loss: 2.2166
2022-08-01 05:28:16 - train: epoch 0021, iter [00900, 05004], lr: 0.187663, loss: 2.2170
2022-08-01 05:29:12 - train: epoch 0021, iter [01000, 05004], lr: 0.187631, loss: 2.0473
2022-08-01 05:30:10 - train: epoch 0021, iter [01100, 05004], lr: 0.187599, loss: 2.2060
2022-08-01 05:31:08 - train: epoch 0021, iter [01200, 05004], lr: 0.187567, loss: 2.1415
2022-08-01 05:32:05 - train: epoch 0021, iter [01300, 05004], lr: 0.187535, loss: 2.0104
2022-08-01 05:33:02 - train: epoch 0021, iter [01400, 05004], lr: 0.187503, loss: 1.8719
2022-08-01 05:33:59 - train: epoch 0021, iter [01500, 05004], lr: 0.187471, loss: 2.0771
2022-08-01 05:34:56 - train: epoch 0021, iter [01600, 05004], lr: 0.187439, loss: 2.0298
2022-08-01 05:35:53 - train: epoch 0021, iter [01700, 05004], lr: 0.187407, loss: 2.3513
2022-08-01 05:36:49 - train: epoch 0021, iter [01800, 05004], lr: 0.187375, loss: 2.0761
2022-08-01 05:37:45 - train: epoch 0021, iter [01900, 05004], lr: 0.187343, loss: 2.5974
2022-08-01 05:38:43 - train: epoch 0021, iter [02000, 05004], lr: 0.187311, loss: 2.3366
2022-08-01 05:39:40 - train: epoch 0021, iter [02100, 05004], lr: 0.187278, loss: 2.1015
2022-08-01 05:40:38 - train: epoch 0021, iter [02200, 05004], lr: 0.187246, loss: 2.2328
2022-08-01 05:41:34 - train: epoch 0021, iter [02300, 05004], lr: 0.187214, loss: 2.1216
2022-08-01 05:42:32 - train: epoch 0021, iter [02400, 05004], lr: 0.187181, loss: 1.9567
2022-08-01 05:43:27 - train: epoch 0021, iter [02500, 05004], lr: 0.187149, loss: 2.2190
2022-08-01 05:44:25 - train: epoch 0021, iter [02600, 05004], lr: 0.187117, loss: 2.3302
2022-08-01 05:45:23 - train: epoch 0021, iter [02700, 05004], lr: 0.187084, loss: 2.2536
2022-08-01 05:46:20 - train: epoch 0021, iter [02800, 05004], lr: 0.187052, loss: 2.1127
2022-08-01 05:47:15 - train: epoch 0021, iter [02900, 05004], lr: 0.187019, loss: 2.1537
2022-08-01 05:48:12 - train: epoch 0021, iter [03000, 05004], lr: 0.186987, loss: 2.2476
2022-08-01 05:49:09 - train: epoch 0021, iter [03100, 05004], lr: 0.186954, loss: 2.1384
2022-08-01 05:50:05 - train: epoch 0021, iter [03200, 05004], lr: 0.186921, loss: 1.9461
2022-08-01 05:51:01 - train: epoch 0021, iter [03300, 05004], lr: 0.186889, loss: 2.4792
2022-08-01 05:51:57 - train: epoch 0021, iter [03400, 05004], lr: 0.186856, loss: 2.4008
2022-08-01 05:52:53 - train: epoch 0021, iter [03500, 05004], lr: 0.186823, loss: 2.2128
2022-08-01 05:53:50 - train: epoch 0021, iter [03600, 05004], lr: 0.186790, loss: 2.1919
2022-08-01 05:54:47 - train: epoch 0021, iter [03700, 05004], lr: 0.186757, loss: 2.0656
2022-08-01 05:55:44 - train: epoch 0021, iter [03800, 05004], lr: 0.186725, loss: 2.2735
2022-08-01 05:56:39 - train: epoch 0021, iter [03900, 05004], lr: 0.186692, loss: 2.2811
2022-08-01 05:57:37 - train: epoch 0021, iter [04000, 05004], lr: 0.186659, loss: 2.3649
2022-08-01 05:58:32 - train: epoch 0021, iter [04100, 05004], lr: 0.186626, loss: 2.1739
2022-08-01 05:59:28 - train: epoch 0021, iter [04200, 05004], lr: 0.186593, loss: 2.0718
2022-08-01 06:00:25 - train: epoch 0021, iter [04300, 05004], lr: 0.186560, loss: 2.0835
2022-08-01 06:01:21 - train: epoch 0021, iter [04400, 05004], lr: 0.186526, loss: 2.1173
2022-08-01 06:02:17 - train: epoch 0021, iter [04500, 05004], lr: 0.186493, loss: 2.2733
2022-08-01 06:03:13 - train: epoch 0021, iter [04600, 05004], lr: 0.186460, loss: 2.0503
2022-08-01 06:04:11 - train: epoch 0021, iter [04700, 05004], lr: 0.186427, loss: 2.2116
2022-08-01 06:05:07 - train: epoch 0021, iter [04800, 05004], lr: 0.186394, loss: 2.1678
2022-08-01 06:06:05 - train: epoch 0021, iter [04900, 05004], lr: 0.186360, loss: 1.9912
2022-08-01 06:06:59 - train: epoch 0021, iter [05000, 05004], lr: 0.186327, loss: 2.2435
2022-08-01 06:07:00 - train: epoch 021, train_loss: 2.1934
2022-08-01 06:09:10 - eval: epoch: 021, acc1: 54.582%, acc5: 79.738%, test_loss: 1.9237, per_image_load_time: 4.249ms, per_image_inference_time: 0.674ms
2022-08-01 06:09:10 - until epoch: 021, best_acc1: 54.832%
2022-08-01 06:09:10 - epoch 022 lr: 0.186325
2022-08-01 06:10:17 - train: epoch 0022, iter [00100, 05004], lr: 0.186292, loss: 1.8036
2022-08-01 06:11:14 - train: epoch 0022, iter [00200, 05004], lr: 0.186259, loss: 1.9795
2022-08-01 06:12:12 - train: epoch 0022, iter [00300, 05004], lr: 0.186225, loss: 2.0953
2022-08-01 06:13:11 - train: epoch 0022, iter [00400, 05004], lr: 0.186192, loss: 2.1588
2022-08-01 06:14:08 - train: epoch 0022, iter [00500, 05004], lr: 0.186158, loss: 2.1504
2022-08-01 06:15:06 - train: epoch 0022, iter [00600, 05004], lr: 0.186125, loss: 2.1463
2022-08-01 06:16:03 - train: epoch 0022, iter [00700, 05004], lr: 0.186091, loss: 2.4493
2022-08-01 06:17:01 - train: epoch 0022, iter [00800, 05004], lr: 0.186058, loss: 2.3325
2022-08-01 06:17:59 - train: epoch 0022, iter [00900, 05004], lr: 0.186024, loss: 2.3234
2022-08-01 06:18:57 - train: epoch 0022, iter [01000, 05004], lr: 0.185990, loss: 2.0807
2022-08-01 06:19:54 - train: epoch 0022, iter [01100, 05004], lr: 0.185956, loss: 2.1784
2022-08-01 06:20:53 - train: epoch 0022, iter [01200, 05004], lr: 0.185923, loss: 1.9836
2022-08-01 06:21:51 - train: epoch 0022, iter [01300, 05004], lr: 0.185889, loss: 2.3152
2022-08-01 06:22:50 - train: epoch 0022, iter [01400, 05004], lr: 0.185855, loss: 2.1277
2022-08-01 06:23:47 - train: epoch 0022, iter [01500, 05004], lr: 0.185821, loss: 2.4152
2022-08-01 06:24:45 - train: epoch 0022, iter [01600, 05004], lr: 0.185787, loss: 2.0449
2022-08-01 06:25:43 - train: epoch 0022, iter [01700, 05004], lr: 0.185753, loss: 2.3281
2022-08-01 06:26:41 - train: epoch 0022, iter [01800, 05004], lr: 0.185719, loss: 2.6151
2022-08-01 06:27:39 - train: epoch 0022, iter [01900, 05004], lr: 0.185685, loss: 2.1053
2022-08-01 06:28:37 - train: epoch 0022, iter [02000, 05004], lr: 0.185651, loss: 2.0622
2022-08-01 06:29:34 - train: epoch 0022, iter [02100, 05004], lr: 0.185617, loss: 2.1539
2022-08-01 06:30:32 - train: epoch 0022, iter [02200, 05004], lr: 0.185583, loss: 1.8506
2022-08-01 06:31:31 - train: epoch 0022, iter [02300, 05004], lr: 0.185548, loss: 2.2875
2022-08-01 06:32:30 - train: epoch 0022, iter [02400, 05004], lr: 0.185514, loss: 2.2797
2022-08-01 06:33:27 - train: epoch 0022, iter [02500, 05004], lr: 0.185480, loss: 2.2318
2022-08-01 06:34:27 - train: epoch 0022, iter [02600, 05004], lr: 0.185446, loss: 2.0311
2022-08-01 06:35:25 - train: epoch 0022, iter [02700, 05004], lr: 0.185411, loss: 2.0701
2022-08-01 06:36:22 - train: epoch 0022, iter [02800, 05004], lr: 0.185377, loss: 2.4219
2022-08-01 06:37:20 - train: epoch 0022, iter [02900, 05004], lr: 0.185342, loss: 2.2323
2022-08-01 06:38:19 - train: epoch 0022, iter [03000, 05004], lr: 0.185308, loss: 2.1886
2022-08-01 06:39:18 - train: epoch 0022, iter [03100, 05004], lr: 0.185274, loss: 2.4809
2022-08-01 06:40:15 - train: epoch 0022, iter [03200, 05004], lr: 0.185239, loss: 2.0844
2022-08-01 06:41:14 - train: epoch 0022, iter [03300, 05004], lr: 0.185204, loss: 2.1590
2022-08-01 06:42:11 - train: epoch 0022, iter [03400, 05004], lr: 0.185170, loss: 2.1016
2022-08-01 06:43:09 - train: epoch 0022, iter [03500, 05004], lr: 0.185135, loss: 2.3727
2022-08-01 06:44:07 - train: epoch 0022, iter [03600, 05004], lr: 0.185100, loss: 2.2309
2022-08-01 06:45:06 - train: epoch 0022, iter [03700, 05004], lr: 0.185066, loss: 2.3153
2022-08-01 06:46:03 - train: epoch 0022, iter [03800, 05004], lr: 0.185031, loss: 2.2718
2022-08-01 06:47:01 - train: epoch 0022, iter [03900, 05004], lr: 0.184996, loss: 2.1161
2022-08-01 06:47:58 - train: epoch 0022, iter [04000, 05004], lr: 0.184961, loss: 2.1410
2022-08-01 06:48:57 - train: epoch 0022, iter [04100, 05004], lr: 0.184926, loss: 2.0984
2022-08-01 06:49:55 - train: epoch 0022, iter [04200, 05004], lr: 0.184892, loss: 2.2888
2022-08-01 06:50:54 - train: epoch 0022, iter [04300, 05004], lr: 0.184857, loss: 2.2410
2022-08-01 06:51:50 - train: epoch 0022, iter [04400, 05004], lr: 0.184822, loss: 1.9943
2022-08-01 06:52:49 - train: epoch 0022, iter [04500, 05004], lr: 0.184787, loss: 2.0124
2022-08-01 06:53:46 - train: epoch 0022, iter [04600, 05004], lr: 0.184752, loss: 2.5215
2022-08-01 06:54:45 - train: epoch 0022, iter [04700, 05004], lr: 0.184716, loss: 2.3288
2022-08-01 06:55:42 - train: epoch 0022, iter [04800, 05004], lr: 0.184681, loss: 2.1060
2022-08-01 06:56:37 - train: epoch 0022, iter [04900, 05004], lr: 0.184646, loss: 2.1562
2022-08-01 06:57:32 - train: epoch 0022, iter [05000, 05004], lr: 0.184611, loss: 2.0523
2022-08-01 06:57:34 - train: epoch 022, train_loss: 2.1808
2022-08-01 06:59:40 - eval: epoch: 022, acc1: 54.940%, acc5: 80.050%, test_loss: 1.8970, per_image_load_time: 4.314ms, per_image_inference_time: 0.600ms
2022-08-01 06:59:40 - until epoch: 022, best_acc1: 54.940%
2022-08-01 06:59:40 - epoch 023 lr: 0.184609
2022-08-01 07:00:49 - train: epoch 0023, iter [00100, 05004], lr: 0.184574, loss: 2.1484
2022-08-01 07:01:49 - train: epoch 0023, iter [00200, 05004], lr: 0.184539, loss: 2.0412
2022-08-01 07:02:52 - train: epoch 0023, iter [00300, 05004], lr: 0.184504, loss: 2.1811
2022-08-01 07:03:53 - train: epoch 0023, iter [00400, 05004], lr: 0.184468, loss: 2.0941
2022-08-01 07:04:53 - train: epoch 0023, iter [00500, 05004], lr: 0.184433, loss: 2.1591
2022-08-01 07:05:54 - train: epoch 0023, iter [00600, 05004], lr: 0.184398, loss: 2.0331
2022-08-01 07:06:51 - train: epoch 0023, iter [00700, 05004], lr: 0.184362, loss: 2.0986
2022-08-01 07:07:50 - train: epoch 0023, iter [00800, 05004], lr: 0.184327, loss: 2.4016
2022-08-01 07:08:49 - train: epoch 0023, iter [00900, 05004], lr: 0.184291, loss: 2.2124
2022-08-01 07:09:47 - train: epoch 0023, iter [01000, 05004], lr: 0.184255, loss: 1.9774
2022-08-01 07:10:45 - train: epoch 0023, iter [01100, 05004], lr: 0.184220, loss: 2.1938
2022-08-01 07:11:43 - train: epoch 0023, iter [01200, 05004], lr: 0.184184, loss: 2.0201
2022-08-01 07:12:41 - train: epoch 0023, iter [01300, 05004], lr: 0.184148, loss: 2.1881
2022-08-01 07:13:39 - train: epoch 0023, iter [01400, 05004], lr: 0.184113, loss: 2.1370
2022-08-01 07:14:36 - train: epoch 0023, iter [01500, 05004], lr: 0.184077, loss: 2.1439
2022-08-01 07:15:36 - train: epoch 0023, iter [01600, 05004], lr: 0.184041, loss: 2.0785
2022-08-01 07:16:32 - train: epoch 0023, iter [01700, 05004], lr: 0.184005, loss: 2.1424
2022-08-01 07:17:30 - train: epoch 0023, iter [01800, 05004], lr: 0.183969, loss: 2.0261
2022-08-01 07:18:27 - train: epoch 0023, iter [01900, 05004], lr: 0.183934, loss: 2.1704
2022-08-01 07:19:27 - train: epoch 0023, iter [02000, 05004], lr: 0.183898, loss: 1.9520
2022-08-01 07:20:24 - train: epoch 0023, iter [02100, 05004], lr: 0.183862, loss: 2.1541
2022-08-01 07:21:22 - train: epoch 0023, iter [02200, 05004], lr: 0.183826, loss: 2.0062
2022-08-01 07:22:19 - train: epoch 0023, iter [02300, 05004], lr: 0.183790, loss: 2.0654
2022-08-01 07:23:15 - train: epoch 0023, iter [02400, 05004], lr: 0.183753, loss: 2.1252
2022-08-01 07:24:12 - train: epoch 0023, iter [02500, 05004], lr: 0.183717, loss: 2.2631
2022-08-01 07:25:10 - train: epoch 0023, iter [02600, 05004], lr: 0.183681, loss: 2.1941
2022-08-01 07:26:06 - train: epoch 0023, iter [02700, 05004], lr: 0.183645, loss: 2.3017
2022-08-01 07:27:03 - train: epoch 0023, iter [02800, 05004], lr: 0.183609, loss: 2.2918
2022-08-01 07:28:00 - train: epoch 0023, iter [02900, 05004], lr: 0.183572, loss: 2.1120
2022-08-01 07:29:00 - train: epoch 0023, iter [03000, 05004], lr: 0.183536, loss: 2.2706
2022-08-01 07:29:58 - train: epoch 0023, iter [03100, 05004], lr: 0.183500, loss: 2.3041
2022-08-01 07:30:58 - train: epoch 0023, iter [03200, 05004], lr: 0.183463, loss: 2.2074
2022-08-01 07:31:53 - train: epoch 0023, iter [03300, 05004], lr: 0.183427, loss: 2.2051
2022-08-01 07:32:52 - train: epoch 0023, iter [03400, 05004], lr: 0.183391, loss: 2.3500
2022-08-01 07:33:50 - train: epoch 0023, iter [03500, 05004], lr: 0.183354, loss: 1.9941
2022-08-01 07:34:47 - train: epoch 0023, iter [03600, 05004], lr: 0.183318, loss: 2.2447
2022-08-01 07:35:45 - train: epoch 0023, iter [03700, 05004], lr: 0.183281, loss: 2.2108
2022-08-01 07:36:42 - train: epoch 0023, iter [03800, 05004], lr: 0.183244, loss: 2.0707
2022-08-01 07:37:38 - train: epoch 0023, iter [03900, 05004], lr: 0.183208, loss: 2.0997
2022-08-01 07:38:37 - train: epoch 0023, iter [04000, 05004], lr: 0.183171, loss: 1.9757
2022-08-01 07:39:34 - train: epoch 0023, iter [04100, 05004], lr: 0.183134, loss: 1.8706
2022-08-01 07:40:31 - train: epoch 0023, iter [04200, 05004], lr: 0.183098, loss: 1.9035
2022-08-01 07:41:29 - train: epoch 0023, iter [04300, 05004], lr: 0.183061, loss: 2.1542
2022-08-01 07:42:26 - train: epoch 0023, iter [04400, 05004], lr: 0.183024, loss: 2.1994
2022-08-01 07:43:25 - train: epoch 0023, iter [04500, 05004], lr: 0.182987, loss: 2.1823
2022-08-01 07:44:23 - train: epoch 0023, iter [04600, 05004], lr: 0.182950, loss: 2.2764
2022-08-01 07:45:21 - train: epoch 0023, iter [04700, 05004], lr: 0.182913, loss: 1.9859
2022-08-01 07:46:18 - train: epoch 0023, iter [04800, 05004], lr: 0.182876, loss: 2.0867
2022-08-01 07:47:16 - train: epoch 0023, iter [04900, 05004], lr: 0.182839, loss: 1.9076
2022-08-01 07:48:10 - train: epoch 0023, iter [05000, 05004], lr: 0.182802, loss: 2.2539
2022-08-01 07:48:12 - train: epoch 023, train_loss: 2.1699
2022-08-01 07:50:20 - eval: epoch: 023, acc1: 55.260%, acc5: 80.198%, test_loss: 1.8896, per_image_load_time: 2.987ms, per_image_inference_time: 0.630ms
2022-08-01 07:50:20 - until epoch: 023, best_acc1: 55.260%
2022-08-01 07:50:20 - epoch 024 lr: 0.182801
2022-08-01 07:51:29 - train: epoch 0024, iter [00100, 05004], lr: 0.182764, loss: 2.1324
2022-08-01 07:52:26 - train: epoch 0024, iter [00200, 05004], lr: 0.182727, loss: 1.9486
2022-08-01 07:53:23 - train: epoch 0024, iter [00300, 05004], lr: 0.182690, loss: 2.0162
2022-08-01 07:54:22 - train: epoch 0024, iter [00400, 05004], lr: 0.182652, loss: 2.2364
2022-08-01 07:55:19 - train: epoch 0024, iter [00500, 05004], lr: 0.182615, loss: 2.1719
2022-08-01 07:56:16 - train: epoch 0024, iter [00600, 05004], lr: 0.182578, loss: 2.1276
2022-08-01 07:57:14 - train: epoch 0024, iter [00700, 05004], lr: 0.182541, loss: 2.1900
2022-08-01 07:58:11 - train: epoch 0024, iter [00800, 05004], lr: 0.182503, loss: 2.0998
2022-08-01 07:59:09 - train: epoch 0024, iter [00900, 05004], lr: 0.182466, loss: 2.2348
2022-08-01 08:00:08 - train: epoch 0024, iter [01000, 05004], lr: 0.182429, loss: 2.0330
2022-08-01 08:01:06 - train: epoch 0024, iter [01100, 05004], lr: 0.182391, loss: 1.8020
2022-08-01 08:02:04 - train: epoch 0024, iter [01200, 05004], lr: 0.182354, loss: 1.9795
2022-08-01 08:03:02 - train: epoch 0024, iter [01300, 05004], lr: 0.182316, loss: 2.0734
2022-08-01 08:04:00 - train: epoch 0024, iter [01400, 05004], lr: 0.182279, loss: 2.0464
2022-08-01 08:04:59 - train: epoch 0024, iter [01500, 05004], lr: 0.182241, loss: 2.2770
2022-08-01 08:05:58 - train: epoch 0024, iter [01600, 05004], lr: 0.182203, loss: 2.0450
2022-08-01 08:06:54 - train: epoch 0024, iter [01700, 05004], lr: 0.182166, loss: 1.9249
2022-08-01 08:07:53 - train: epoch 0024, iter [01800, 05004], lr: 0.182128, loss: 2.3287
2022-08-01 08:08:52 - train: epoch 0024, iter [01900, 05004], lr: 0.182090, loss: 1.9139
2022-08-01 08:09:51 - train: epoch 0024, iter [02000, 05004], lr: 0.182053, loss: 2.3432
2022-08-01 08:10:48 - train: epoch 0024, iter [02100, 05004], lr: 0.182015, loss: 2.1614
2022-08-01 08:11:47 - train: epoch 0024, iter [02200, 05004], lr: 0.181977, loss: 2.0819
2022-08-01 08:12:44 - train: epoch 0024, iter [02300, 05004], lr: 0.181939, loss: 2.2018
2022-08-01 08:13:43 - train: epoch 0024, iter [02400, 05004], lr: 0.181901, loss: 2.2098
2022-08-01 08:14:40 - train: epoch 0024, iter [02500, 05004], lr: 0.181863, loss: 2.2266
2022-08-01 08:15:39 - train: epoch 0024, iter [02600, 05004], lr: 0.181825, loss: 2.3847
2022-08-01 08:16:36 - train: epoch 0024, iter [02700, 05004], lr: 0.181787, loss: 2.2022
2022-08-01 08:17:34 - train: epoch 0024, iter [02800, 05004], lr: 0.181749, loss: 2.1409
2022-08-01 08:18:31 - train: epoch 0024, iter [02900, 05004], lr: 0.181711, loss: 2.2996
2022-08-01 08:19:29 - train: epoch 0024, iter [03000, 05004], lr: 0.181673, loss: 2.1231
2022-08-01 08:20:26 - train: epoch 0024, iter [03100, 05004], lr: 0.181635, loss: 1.8594
2022-08-01 08:21:26 - train: epoch 0024, iter [03200, 05004], lr: 0.181597, loss: 2.3735
2022-08-01 08:22:23 - train: epoch 0024, iter [03300, 05004], lr: 0.181558, loss: 2.1493
2022-08-01 08:23:22 - train: epoch 0024, iter [03400, 05004], lr: 0.181520, loss: 2.0771
2022-08-01 08:24:20 - train: epoch 0024, iter [03500, 05004], lr: 0.181482, loss: 2.1372
2022-08-01 08:25:17 - train: epoch 0024, iter [03600, 05004], lr: 0.181444, loss: 2.1812
2022-08-01 08:26:15 - train: epoch 0024, iter [03700, 05004], lr: 0.181405, loss: 2.0857
2022-08-01 08:27:14 - train: epoch 0024, iter [03800, 05004], lr: 0.181367, loss: 2.2904
2022-08-01 08:28:11 - train: epoch 0024, iter [03900, 05004], lr: 0.181328, loss: 2.0099
2022-08-01 08:29:09 - train: epoch 0024, iter [04000, 05004], lr: 0.181290, loss: 2.0801
2022-08-01 08:30:08 - train: epoch 0024, iter [04100, 05004], lr: 0.181251, loss: 2.1424
2022-08-01 08:31:05 - train: epoch 0024, iter [04200, 05004], lr: 0.181213, loss: 2.0498
2022-08-01 08:32:02 - train: epoch 0024, iter [04300, 05004], lr: 0.181174, loss: 2.0217
2022-08-01 08:33:00 - train: epoch 0024, iter [04400, 05004], lr: 0.181136, loss: 2.1862
2022-08-01 08:33:58 - train: epoch 0024, iter [04500, 05004], lr: 0.181097, loss: 1.8869
2022-08-01 08:34:56 - train: epoch 0024, iter [04600, 05004], lr: 0.181058, loss: 2.0162
2022-08-01 08:35:54 - train: epoch 0024, iter [04700, 05004], lr: 0.181020, loss: 2.2735
2022-08-01 08:36:53 - train: epoch 0024, iter [04800, 05004], lr: 0.180981, loss: 2.0013
2022-08-01 08:37:51 - train: epoch 0024, iter [04900, 05004], lr: 0.180942, loss: 2.3094
2022-08-01 08:38:43 - train: epoch 0024, iter [05000, 05004], lr: 0.180903, loss: 2.1481
2022-08-01 08:38:44 - train: epoch 024, train_loss: 2.1610
2022-08-01 08:40:53 - eval: epoch: 024, acc1: 55.622%, acc5: 80.428%, test_loss: 1.8851, per_image_load_time: 1.638ms, per_image_inference_time: 0.663ms
2022-08-01 08:40:53 - until epoch: 024, best_acc1: 55.622%
2022-08-01 08:40:53 - epoch 025 lr: 0.180901
2022-08-01 08:42:00 - train: epoch 0025, iter [00100, 05004], lr: 0.180863, loss: 2.0220
2022-08-01 08:42:58 - train: epoch 0025, iter [00200, 05004], lr: 0.180824, loss: 1.8917
2022-08-01 08:43:55 - train: epoch 0025, iter [00300, 05004], lr: 0.180785, loss: 2.2258
2022-08-01 08:44:52 - train: epoch 0025, iter [00400, 05004], lr: 0.180746, loss: 2.1128
2022-08-01 08:45:49 - train: epoch 0025, iter [00500, 05004], lr: 0.180707, loss: 1.9893
2022-08-01 08:46:46 - train: epoch 0025, iter [00600, 05004], lr: 0.180668, loss: 2.0103
2022-08-01 08:47:44 - train: epoch 0025, iter [00700, 05004], lr: 0.180629, loss: 2.0884
2022-08-01 08:48:41 - train: epoch 0025, iter [00800, 05004], lr: 0.180590, loss: 2.1374
2022-08-01 08:49:37 - train: epoch 0025, iter [00900, 05004], lr: 0.180551, loss: 1.7769
2022-08-01 08:50:36 - train: epoch 0025, iter [01000, 05004], lr: 0.180511, loss: 2.3059
2022-08-01 08:51:32 - train: epoch 0025, iter [01100, 05004], lr: 0.180472, loss: 2.0779
2022-08-01 08:52:31 - train: epoch 0025, iter [01200, 05004], lr: 0.180433, loss: 2.3787
2022-08-01 08:53:29 - train: epoch 0025, iter [01300, 05004], lr: 0.180394, loss: 1.9496
2022-08-01 08:54:25 - train: epoch 0025, iter [01400, 05004], lr: 0.180354, loss: 2.1089
2022-08-01 08:55:24 - train: epoch 0025, iter [01500, 05004], lr: 0.180315, loss: 2.1735
2022-08-01 08:56:21 - train: epoch 0025, iter [01600, 05004], lr: 0.180276, loss: 1.9405
2022-08-01 08:57:18 - train: epoch 0025, iter [01700, 05004], lr: 0.180236, loss: 2.2354
2022-08-01 08:58:15 - train: epoch 0025, iter [01800, 05004], lr: 0.180197, loss: 1.9647
2022-08-01 08:59:15 - train: epoch 0025, iter [01900, 05004], lr: 0.180157, loss: 2.2480
2022-08-01 09:00:11 - train: epoch 0025, iter [02000, 05004], lr: 0.180118, loss: 2.2227
2022-08-01 09:01:07 - train: epoch 0025, iter [02100, 05004], lr: 0.180078, loss: 2.0193
2022-08-01 09:02:05 - train: epoch 0025, iter [02200, 05004], lr: 0.180039, loss: 1.9596
2022-08-01 09:03:01 - train: epoch 0025, iter [02300, 05004], lr: 0.179999, loss: 2.1437
2022-08-01 09:03:59 - train: epoch 0025, iter [02400, 05004], lr: 0.179959, loss: 1.9620
2022-08-01 09:04:57 - train: epoch 0025, iter [02500, 05004], lr: 0.179920, loss: 2.0895
2022-08-01 09:05:55 - train: epoch 0025, iter [02600, 05004], lr: 0.179880, loss: 2.2037
2022-08-01 09:06:55 - train: epoch 0025, iter [02700, 05004], lr: 0.179840, loss: 2.1782
2022-08-01 09:07:53 - train: epoch 0025, iter [02800, 05004], lr: 0.179800, loss: 1.9804
2022-08-01 09:08:52 - train: epoch 0025, iter [02900, 05004], lr: 0.179760, loss: 2.3122
2022-08-01 09:09:51 - train: epoch 0025, iter [03000, 05004], lr: 0.179721, loss: 2.1577
2022-08-01 09:10:51 - train: epoch 0025, iter [03100, 05004], lr: 0.179681, loss: 2.2067
2022-08-01 09:11:50 - train: epoch 0025, iter [03200, 05004], lr: 0.179641, loss: 1.9954
2022-08-01 09:12:49 - train: epoch 0025, iter [03300, 05004], lr: 0.179601, loss: 2.0506
2022-08-01 09:13:46 - train: epoch 0025, iter [03400, 05004], lr: 0.179561, loss: 2.1677
2022-08-01 09:14:45 - train: epoch 0025, iter [03500, 05004], lr: 0.179521, loss: 1.9688
2022-08-01 09:15:43 - train: epoch 0025, iter [03600, 05004], lr: 0.179481, loss: 2.2808
2022-08-01 09:16:41 - train: epoch 0025, iter [03700, 05004], lr: 0.179440, loss: 2.0653
2022-08-01 09:17:38 - train: epoch 0025, iter [03800, 05004], lr: 0.179400, loss: 2.1903
2022-08-01 09:18:38 - train: epoch 0025, iter [03900, 05004], lr: 0.179360, loss: 2.5234
2022-08-01 09:19:36 - train: epoch 0025, iter [04000, 05004], lr: 0.179320, loss: 2.1004
2022-08-01 09:20:35 - train: epoch 0025, iter [04100, 05004], lr: 0.179280, loss: 2.4484
2022-08-01 09:21:33 - train: epoch 0025, iter [04200, 05004], lr: 0.179239, loss: 2.2306
2022-08-01 09:22:29 - train: epoch 0025, iter [04300, 05004], lr: 0.179199, loss: 2.0909
2022-08-01 09:23:28 - train: epoch 0025, iter [04400, 05004], lr: 0.179159, loss: 1.9492
2022-08-01 09:24:28 - train: epoch 0025, iter [04500, 05004], lr: 0.179118, loss: 1.9931
2022-08-01 09:25:25 - train: epoch 0025, iter [04600, 05004], lr: 0.179078, loss: 1.7192
2022-08-01 09:26:24 - train: epoch 0025, iter [04700, 05004], lr: 0.179037, loss: 2.0922
2022-08-01 09:27:23 - train: epoch 0025, iter [04800, 05004], lr: 0.178997, loss: 2.1950
2022-08-01 09:28:20 - train: epoch 0025, iter [04900, 05004], lr: 0.178956, loss: 2.3296
2022-08-01 09:29:15 - train: epoch 0025, iter [05000, 05004], lr: 0.178916, loss: 2.1308
2022-08-01 09:29:17 - train: epoch 025, train_loss: 2.1500
2022-08-01 09:31:22 - eval: epoch: 025, acc1: 56.200%, acc5: 80.960%, test_loss: 1.8441, per_image_load_time: 2.404ms, per_image_inference_time: 0.633ms
2022-08-01 09:31:22 - until epoch: 025, best_acc1: 56.200%
2022-08-01 09:31:22 - epoch 026 lr: 0.178914
2022-08-01 09:32:34 - train: epoch 0026, iter [00100, 05004], lr: 0.178873, loss: 2.0744
2022-08-01 09:33:31 - train: epoch 0026, iter [00200, 05004], lr: 0.178833, loss: 1.8983
2022-08-01 09:34:27 - train: epoch 0026, iter [00300, 05004], lr: 0.178792, loss: 2.0929
2022-08-01 09:35:23 - train: epoch 0026, iter [00400, 05004], lr: 0.178751, loss: 2.1845
2022-08-01 09:36:19 - train: epoch 0026, iter [00500, 05004], lr: 0.178711, loss: 2.0974
2022-08-01 09:37:15 - train: epoch 0026, iter [00600, 05004], lr: 0.178670, loss: 2.4474
2022-08-01 09:38:11 - train: epoch 0026, iter [00700, 05004], lr: 0.178629, loss: 2.1857
2022-08-01 09:39:09 - train: epoch 0026, iter [00800, 05004], lr: 0.178588, loss: 1.9866
2022-08-01 09:40:03 - train: epoch 0026, iter [00900, 05004], lr: 0.178547, loss: 2.1746
2022-08-01 09:41:00 - train: epoch 0026, iter [01000, 05004], lr: 0.178506, loss: 2.1637
2022-08-01 09:41:56 - train: epoch 0026, iter [01100, 05004], lr: 0.178465, loss: 2.1771
2022-08-01 09:42:53 - train: epoch 0026, iter [01200, 05004], lr: 0.178424, loss: 2.1164
2022-08-01 09:43:50 - train: epoch 0026, iter [01300, 05004], lr: 0.178383, loss: 1.9666
2022-08-01 09:44:48 - train: epoch 0026, iter [01400, 05004], lr: 0.178342, loss: 2.1203
2022-08-01 09:45:44 - train: epoch 0026, iter [01500, 05004], lr: 0.178301, loss: 1.9488
2022-08-01 09:46:43 - train: epoch 0026, iter [01600, 05004], lr: 0.178260, loss: 2.2258
2022-08-01 09:47:39 - train: epoch 0026, iter [01700, 05004], lr: 0.178219, loss: 1.9549
2022-08-01 09:48:36 - train: epoch 0026, iter [01800, 05004], lr: 0.178178, loss: 2.3320
2022-08-01 09:49:33 - train: epoch 0026, iter [01900, 05004], lr: 0.178137, loss: 2.0944
2022-08-01 09:50:30 - train: epoch 0026, iter [02000, 05004], lr: 0.178095, loss: 2.2550
2022-08-01 09:51:27 - train: epoch 0026, iter [02100, 05004], lr: 0.178054, loss: 2.0593
2022-08-01 09:52:24 - train: epoch 0026, iter [02200, 05004], lr: 0.178013, loss: 2.2389
2022-08-01 09:53:21 - train: epoch 0026, iter [02300, 05004], lr: 0.177971, loss: 2.0166
2022-08-01 09:54:19 - train: epoch 0026, iter [02400, 05004], lr: 0.177930, loss: 2.3290
2022-08-01 09:55:15 - train: epoch 0026, iter [02500, 05004], lr: 0.177889, loss: 2.2022
2022-08-01 09:56:12 - train: epoch 0026, iter [02600, 05004], lr: 0.177847, loss: 2.2411
2022-08-01 09:57:09 - train: epoch 0026, iter [02700, 05004], lr: 0.177806, loss: 2.2536
2022-08-01 09:58:07 - train: epoch 0026, iter [02800, 05004], lr: 0.177764, loss: 2.2395
2022-08-01 09:59:03 - train: epoch 0026, iter [02900, 05004], lr: 0.177722, loss: 2.1211
2022-08-01 10:00:00 - train: epoch 0026, iter [03000, 05004], lr: 0.177681, loss: 2.1813
2022-08-01 10:00:56 - train: epoch 0026, iter [03100, 05004], lr: 0.177639, loss: 2.1607
2022-08-01 10:01:56 - train: epoch 0026, iter [03200, 05004], lr: 0.177598, loss: 2.1057
2022-08-01 10:02:51 - train: epoch 0026, iter [03300, 05004], lr: 0.177556, loss: 2.0393
2022-08-01 10:03:48 - train: epoch 0026, iter [03400, 05004], lr: 0.177514, loss: 2.0707
2022-08-01 10:04:46 - train: epoch 0026, iter [03500, 05004], lr: 0.177472, loss: 2.2818
2022-08-01 10:05:43 - train: epoch 0026, iter [03600, 05004], lr: 0.177431, loss: 1.9856
2022-08-01 10:06:40 - train: epoch 0026, iter [03700, 05004], lr: 0.177389, loss: 2.2654
2022-08-01 10:07:37 - train: epoch 0026, iter [03800, 05004], lr: 0.177347, loss: 2.3785
2022-08-01 10:08:34 - train: epoch 0026, iter [03900, 05004], lr: 0.177305, loss: 2.2075
2022-08-01 10:09:31 - train: epoch 0026, iter [04000, 05004], lr: 0.177263, loss: 2.2723
2022-08-01 10:10:26 - train: epoch 0026, iter [04100, 05004], lr: 0.177221, loss: 1.9752
2022-08-01 10:11:24 - train: epoch 0026, iter [04200, 05004], lr: 0.177179, loss: 2.3992
2022-08-01 10:12:22 - train: epoch 0026, iter [04300, 05004], lr: 0.177137, loss: 2.3122
2022-08-01 10:13:19 - train: epoch 0026, iter [04400, 05004], lr: 0.177095, loss: 2.3028
2022-08-01 10:14:16 - train: epoch 0026, iter [04500, 05004], lr: 0.177053, loss: 2.3894
2022-08-01 10:15:13 - train: epoch 0026, iter [04600, 05004], lr: 0.177011, loss: 2.3186
2022-08-01 10:16:11 - train: epoch 0026, iter [04700, 05004], lr: 0.176969, loss: 1.9942
2022-08-01 10:17:08 - train: epoch 0026, iter [04800, 05004], lr: 0.176926, loss: 1.9813
2022-08-01 10:18:05 - train: epoch 0026, iter [04900, 05004], lr: 0.176884, loss: 2.1533
2022-08-01 10:18:58 - train: epoch 0026, iter [05000, 05004], lr: 0.176842, loss: 2.3518
2022-08-01 10:18:59 - train: epoch 026, train_loss: 2.1409
2022-08-01 10:21:04 - eval: epoch: 026, acc1: 55.478%, acc5: 80.232%, test_loss: 1.8922, per_image_load_time: 2.679ms, per_image_inference_time: 0.592ms
2022-08-01 10:21:05 - until epoch: 026, best_acc1: 56.200%
2022-08-01 10:21:05 - epoch 027 lr: 0.176840
2022-08-01 10:22:09 - train: epoch 0027, iter [00100, 05004], lr: 0.176798, loss: 2.1146
2022-08-01 10:23:06 - train: epoch 0027, iter [00200, 05004], lr: 0.176755, loss: 1.9674
2022-08-01 10:24:03 - train: epoch 0027, iter [00300, 05004], lr: 0.176713, loss: 2.1880
2022-08-01 10:24:59 - train: epoch 0027, iter [00400, 05004], lr: 0.176671, loss: 2.2198
2022-08-01 10:25:55 - train: epoch 0027, iter [00500, 05004], lr: 0.176628, loss: 2.3256
2022-08-01 10:26:53 - train: epoch 0027, iter [00600, 05004], lr: 0.176586, loss: 2.3180
2022-08-01 10:27:48 - train: epoch 0027, iter [00700, 05004], lr: 0.176543, loss: 2.1637
2022-08-01 10:28:43 - train: epoch 0027, iter [00800, 05004], lr: 0.176501, loss: 2.1318
2022-08-01 10:29:40 - train: epoch 0027, iter [00900, 05004], lr: 0.176458, loss: 2.0315
2022-08-01 10:30:36 - train: epoch 0027, iter [01000, 05004], lr: 0.176416, loss: 2.1410
2022-08-01 10:31:33 - train: epoch 0027, iter [01100, 05004], lr: 0.176373, loss: 2.0278
2022-08-01 10:32:29 - train: epoch 0027, iter [01200, 05004], lr: 0.176330, loss: 2.3767
2022-08-01 10:33:26 - train: epoch 0027, iter [01300, 05004], lr: 0.176287, loss: 2.1919
2022-08-01 10:34:22 - train: epoch 0027, iter [01400, 05004], lr: 0.176245, loss: 2.1584
2022-08-01 10:35:17 - train: epoch 0027, iter [01500, 05004], lr: 0.176202, loss: 2.2727
2022-08-01 10:36:14 - train: epoch 0027, iter [01600, 05004], lr: 0.176159, loss: 2.1024
2022-08-01 10:37:11 - train: epoch 0027, iter [01700, 05004], lr: 0.176116, loss: 2.1129
2022-08-01 10:38:07 - train: epoch 0027, iter [01800, 05004], lr: 0.176073, loss: 2.1295
2022-08-01 10:39:05 - train: epoch 0027, iter [01900, 05004], lr: 0.176031, loss: 2.3563
2022-08-01 10:40:00 - train: epoch 0027, iter [02000, 05004], lr: 0.175988, loss: 2.1722
2022-08-01 10:40:56 - train: epoch 0027, iter [02100, 05004], lr: 0.175945, loss: 2.2567
2022-08-01 10:41:53 - train: epoch 0027, iter [02200, 05004], lr: 0.175902, loss: 2.3247
2022-08-01 10:42:50 - train: epoch 0027, iter [02300, 05004], lr: 0.175859, loss: 2.3726
2022-08-01 10:43:46 - train: epoch 0027, iter [02400, 05004], lr: 0.175815, loss: 2.1097
2022-08-01 10:44:42 - train: epoch 0027, iter [02500, 05004], lr: 0.175772, loss: 2.1337
2022-08-01 10:45:38 - train: epoch 0027, iter [02600, 05004], lr: 0.175729, loss: 2.2346
2022-08-01 10:46:34 - train: epoch 0027, iter [02700, 05004], lr: 0.175686, loss: 2.1871
2022-08-01 10:47:30 - train: epoch 0027, iter [02800, 05004], lr: 0.175643, loss: 2.3190
2022-08-01 10:48:26 - train: epoch 0027, iter [02900, 05004], lr: 0.175600, loss: 2.3468
2022-08-01 10:49:21 - train: epoch 0027, iter [03000, 05004], lr: 0.175556, loss: 2.0420
2022-08-01 10:50:18 - train: epoch 0027, iter [03100, 05004], lr: 0.175513, loss: 1.9953
2022-08-01 10:51:14 - train: epoch 0027, iter [03200, 05004], lr: 0.175470, loss: 2.0402
2022-08-01 10:52:11 - train: epoch 0027, iter [03300, 05004], lr: 0.175426, loss: 2.2736
2022-08-01 10:53:05 - train: epoch 0027, iter [03400, 05004], lr: 0.175383, loss: 2.0567
2022-08-01 10:54:01 - train: epoch 0027, iter [03500, 05004], lr: 0.175339, loss: 2.2259
2022-08-01 10:54:57 - train: epoch 0027, iter [03600, 05004], lr: 0.175296, loss: 2.3359
2022-08-01 10:55:54 - train: epoch 0027, iter [03700, 05004], lr: 0.175252, loss: 2.1661
2022-08-01 10:56:49 - train: epoch 0027, iter [03800, 05004], lr: 0.175209, loss: 2.1667
2022-08-01 10:57:46 - train: epoch 0027, iter [03900, 05004], lr: 0.175165, loss: 2.1366
2022-08-01 10:58:41 - train: epoch 0027, iter [04000, 05004], lr: 0.175122, loss: 2.0761
2022-08-01 10:59:37 - train: epoch 0027, iter [04100, 05004], lr: 0.175078, loss: 2.1806
2022-08-01 11:00:33 - train: epoch 0027, iter [04200, 05004], lr: 0.175034, loss: 2.1426
2022-08-01 11:01:29 - train: epoch 0027, iter [04300, 05004], lr: 0.174991, loss: 2.2122
2022-08-01 11:02:27 - train: epoch 0027, iter [04400, 05004], lr: 0.174947, loss: 2.1979
2022-08-01 11:03:22 - train: epoch 0027, iter [04500, 05004], lr: 0.174903, loss: 1.7594
2022-08-01 11:04:18 - train: epoch 0027, iter [04600, 05004], lr: 0.174859, loss: 2.0227
2022-08-01 11:05:15 - train: epoch 0027, iter [04700, 05004], lr: 0.174816, loss: 2.1955
2022-08-01 11:06:13 - train: epoch 0027, iter [04800, 05004], lr: 0.174772, loss: 2.2888
2022-08-01 11:07:10 - train: epoch 0027, iter [04900, 05004], lr: 0.174728, loss: 2.2078
2022-08-01 11:08:04 - train: epoch 0027, iter [05000, 05004], lr: 0.174684, loss: 1.9197
2022-08-01 11:08:05 - train: epoch 027, train_loss: 2.1325
2022-08-01 11:10:13 - eval: epoch: 027, acc1: 56.862%, acc5: 81.010%, test_loss: 1.8282, per_image_load_time: 4.289ms, per_image_inference_time: 0.633ms
2022-08-01 11:10:13 - until epoch: 027, best_acc1: 56.862%
2022-08-01 11:10:13 - epoch 028 lr: 0.174682
2022-08-01 11:11:20 - train: epoch 0028, iter [00100, 05004], lr: 0.174638, loss: 1.9939
2022-08-01 11:12:17 - train: epoch 0028, iter [00200, 05004], lr: 0.174594, loss: 1.9993
2022-08-01 11:13:15 - train: epoch 0028, iter [00300, 05004], lr: 0.174550, loss: 2.0685
2022-08-01 11:14:12 - train: epoch 0028, iter [00400, 05004], lr: 0.174506, loss: 2.3215
2022-08-01 11:15:10 - train: epoch 0028, iter [00500, 05004], lr: 0.174462, loss: 1.9589
2022-08-01 11:16:08 - train: epoch 0028, iter [00600, 05004], lr: 0.174418, loss: 2.3481
2022-08-01 11:17:04 - train: epoch 0028, iter [00700, 05004], lr: 0.174374, loss: 2.1425
2022-08-01 11:18:03 - train: epoch 0028, iter [00800, 05004], lr: 0.174330, loss: 1.9622
2022-08-01 11:19:01 - train: epoch 0028, iter [00900, 05004], lr: 0.174285, loss: 2.0046
2022-08-01 11:19:57 - train: epoch 0028, iter [01000, 05004], lr: 0.174241, loss: 2.1786
2022-08-01 11:20:54 - train: epoch 0028, iter [01100, 05004], lr: 0.174197, loss: 2.1243
2022-08-01 11:21:52 - train: epoch 0028, iter [01200, 05004], lr: 0.174152, loss: 2.0368
2022-08-01 11:22:50 - train: epoch 0028, iter [01300, 05004], lr: 0.174108, loss: 2.0409
2022-08-01 11:23:48 - train: epoch 0028, iter [01400, 05004], lr: 0.174064, loss: 2.2026
2022-08-01 11:24:44 - train: epoch 0028, iter [01500, 05004], lr: 0.174019, loss: 2.0372
2022-08-01 11:25:41 - train: epoch 0028, iter [01600, 05004], lr: 0.173975, loss: 2.1802
2022-08-01 11:26:38 - train: epoch 0028, iter [01700, 05004], lr: 0.173930, loss: 2.3221
2022-08-01 11:27:35 - train: epoch 0028, iter [01800, 05004], lr: 0.173886, loss: 2.0762
2022-08-01 11:28:33 - train: epoch 0028, iter [01900, 05004], lr: 0.173841, loss: 2.1428
2022-08-01 11:29:30 - train: epoch 0028, iter [02000, 05004], lr: 0.173797, loss: 2.2269
2022-08-01 11:30:29 - train: epoch 0028, iter [02100, 05004], lr: 0.173752, loss: 2.2363
2022-08-01 11:31:26 - train: epoch 0028, iter [02200, 05004], lr: 0.173707, loss: 2.1837
2022-08-01 11:32:22 - train: epoch 0028, iter [02300, 05004], lr: 0.173663, loss: 2.2319
2022-08-01 11:33:19 - train: epoch 0028, iter [02400, 05004], lr: 0.173618, loss: 2.2093
2022-08-01 11:34:16 - train: epoch 0028, iter [02500, 05004], lr: 0.173573, loss: 2.1684
2022-08-01 11:35:12 - train: epoch 0028, iter [02600, 05004], lr: 0.173529, loss: 2.0130
2022-08-01 11:36:11 - train: epoch 0028, iter [02700, 05004], lr: 0.173484, loss: 2.3510
2022-08-01 11:37:08 - train: epoch 0028, iter [02800, 05004], lr: 0.173439, loss: 2.2173
2022-08-01 11:38:06 - train: epoch 0028, iter [02900, 05004], lr: 0.173394, loss: 2.2255
2022-08-01 11:39:05 - train: epoch 0028, iter [03000, 05004], lr: 0.173349, loss: 2.2028
2022-08-01 11:40:00 - train: epoch 0028, iter [03100, 05004], lr: 0.173304, loss: 2.3283
2022-08-01 11:40:58 - train: epoch 0028, iter [03200, 05004], lr: 0.173259, loss: 1.8770
2022-08-01 11:41:55 - train: epoch 0028, iter [03300, 05004], lr: 0.173214, loss: 2.2616
2022-08-01 11:42:53 - train: epoch 0028, iter [03400, 05004], lr: 0.173169, loss: 2.3642
2022-08-01 11:43:50 - train: epoch 0028, iter [03500, 05004], lr: 0.173124, loss: 2.1551
2022-08-01 11:44:48 - train: epoch 0028, iter [03600, 05004], lr: 0.173079, loss: 2.2042
2022-08-01 11:45:46 - train: epoch 0028, iter [03700, 05004], lr: 0.173034, loss: 2.3371
2022-08-01 11:46:42 - train: epoch 0028, iter [03800, 05004], lr: 0.172989, loss: 1.8732
2022-08-01 11:47:39 - train: epoch 0028, iter [03900, 05004], lr: 0.172944, loss: 2.1502
2022-08-01 11:48:37 - train: epoch 0028, iter [04000, 05004], lr: 0.172898, loss: 2.0809
2022-08-01 11:49:32 - train: epoch 0028, iter [04100, 05004], lr: 0.172853, loss: 2.0113
2022-08-01 11:50:31 - train: epoch 0028, iter [04200, 05004], lr: 0.172808, loss: 2.3156
2022-08-01 11:51:28 - train: epoch 0028, iter [04300, 05004], lr: 0.172762, loss: 2.2537
2022-08-01 11:52:24 - train: epoch 0028, iter [04400, 05004], lr: 0.172717, loss: 2.2056
2022-08-01 11:53:22 - train: epoch 0028, iter [04500, 05004], lr: 0.172672, loss: 2.2189
2022-08-01 11:54:20 - train: epoch 0028, iter [04600, 05004], lr: 0.172626, loss: 2.1672
2022-08-01 11:55:16 - train: epoch 0028, iter [04700, 05004], lr: 0.172581, loss: 2.2294
2022-08-01 11:56:13 - train: epoch 0028, iter [04800, 05004], lr: 0.172535, loss: 2.1775
2022-08-01 11:57:10 - train: epoch 0028, iter [04900, 05004], lr: 0.172490, loss: 2.0200
2022-08-01 11:58:03 - train: epoch 0028, iter [05000, 05004], lr: 0.172444, loss: 1.9626
2022-08-01 11:58:05 - train: epoch 028, train_loss: 2.1197
2022-08-01 12:00:12 - eval: epoch: 028, acc1: 57.344%, acc5: 81.484%, test_loss: 1.8091, per_image_load_time: 2.569ms, per_image_inference_time: 0.599ms
2022-08-01 12:00:12 - until epoch: 028, best_acc1: 57.344%
2022-08-01 12:00:12 - epoch 029 lr: 0.172442
2022-08-01 12:01:22 - train: epoch 0029, iter [00100, 05004], lr: 0.172397, loss: 1.9740
2022-08-01 12:02:20 - train: epoch 0029, iter [00200, 05004], lr: 0.172351, loss: 2.1472
2022-08-01 12:03:18 - train: epoch 0029, iter [00300, 05004], lr: 0.172306, loss: 2.0251
2022-08-01 12:04:15 - train: epoch 0029, iter [00400, 05004], lr: 0.172260, loss: 1.9816
2022-08-01 12:05:15 - train: epoch 0029, iter [00500, 05004], lr: 0.172214, loss: 2.0366
2022-08-01 12:06:13 - train: epoch 0029, iter [00600, 05004], lr: 0.172169, loss: 2.2637
2022-08-01 12:07:10 - train: epoch 0029, iter [00700, 05004], lr: 0.172123, loss: 2.0568
2022-08-01 12:08:10 - train: epoch 0029, iter [00800, 05004], lr: 0.172077, loss: 2.2181
2022-08-01 12:09:08 - train: epoch 0029, iter [00900, 05004], lr: 0.172031, loss: 2.0682
2022-08-01 12:10:06 - train: epoch 0029, iter [01000, 05004], lr: 0.171985, loss: 1.9692
2022-08-01 12:11:03 - train: epoch 0029, iter [01100, 05004], lr: 0.171939, loss: 1.9889
2022-08-01 12:12:02 - train: epoch 0029, iter [01200, 05004], lr: 0.171894, loss: 2.0125
2022-08-01 12:12:59 - train: epoch 0029, iter [01300, 05004], lr: 0.171848, loss: 1.9947
2022-08-01 12:13:58 - train: epoch 0029, iter [01400, 05004], lr: 0.171802, loss: 2.1404
2022-08-01 12:14:56 - train: epoch 0029, iter [01500, 05004], lr: 0.171756, loss: 2.3191
2022-08-01 12:15:56 - train: epoch 0029, iter [01600, 05004], lr: 0.171710, loss: 2.1968
2022-08-01 12:16:54 - train: epoch 0029, iter [01700, 05004], lr: 0.171664, loss: 2.2262
2022-08-01 12:17:51 - train: epoch 0029, iter [01800, 05004], lr: 0.171617, loss: 2.1423
2022-08-01 12:18:50 - train: epoch 0029, iter [01900, 05004], lr: 0.171571, loss: 2.0734
2022-08-01 12:19:48 - train: epoch 0029, iter [02000, 05004], lr: 0.171525, loss: 1.9658
2022-08-01 12:20:47 - train: epoch 0029, iter [02100, 05004], lr: 0.171479, loss: 2.1949
2022-08-01 12:21:44 - train: epoch 0029, iter [02200, 05004], lr: 0.171433, loss: 2.1689
2022-08-01 12:22:43 - train: epoch 0029, iter [02300, 05004], lr: 0.171386, loss: 2.0364
2022-08-01 12:23:42 - train: epoch 0029, iter [02400, 05004], lr: 0.171340, loss: 2.0264
2022-08-01 12:24:39 - train: epoch 0029, iter [02500, 05004], lr: 0.171294, loss: 2.1674
2022-08-01 12:25:38 - train: epoch 0029, iter [02600, 05004], lr: 0.171247, loss: 2.4132
2022-08-01 12:26:35 - train: epoch 0029, iter [02700, 05004], lr: 0.171201, loss: 2.1693
2022-08-01 12:27:34 - train: epoch 0029, iter [02800, 05004], lr: 0.171155, loss: 2.0012
2022-08-01 12:28:31 - train: epoch 0029, iter [02900, 05004], lr: 0.171108, loss: 2.0404
2022-08-01 12:29:30 - train: epoch 0029, iter [03000, 05004], lr: 0.171062, loss: 2.0882
2022-08-01 12:30:30 - train: epoch 0029, iter [03100, 05004], lr: 0.171015, loss: 2.1040
2022-08-01 12:31:26 - train: epoch 0029, iter [03200, 05004], lr: 0.170969, loss: 2.1306
2022-08-01 12:32:24 - train: epoch 0029, iter [03300, 05004], lr: 0.170922, loss: 1.7827
2022-08-01 12:33:22 - train: epoch 0029, iter [03400, 05004], lr: 0.170875, loss: 1.9935
2022-08-01 12:34:22 - train: epoch 0029, iter [03500, 05004], lr: 0.170829, loss: 2.1609
2022-08-01 12:35:21 - train: epoch 0029, iter [03600, 05004], lr: 0.170782, loss: 1.9874
2022-08-01 12:36:18 - train: epoch 0029, iter [03700, 05004], lr: 0.170735, loss: 1.9518
2022-08-01 12:37:17 - train: epoch 0029, iter [03800, 05004], lr: 0.170689, loss: 1.9902
2022-08-01 12:38:15 - train: epoch 0029, iter [03900, 05004], lr: 0.170642, loss: 1.9392
2022-08-01 12:39:11 - train: epoch 0029, iter [04000, 05004], lr: 0.170595, loss: 2.3501
2022-08-01 12:40:08 - train: epoch 0029, iter [04100, 05004], lr: 0.170548, loss: 2.1014
2022-08-01 12:41:06 - train: epoch 0029, iter [04200, 05004], lr: 0.170501, loss: 2.0156
2022-08-01 12:42:03 - train: epoch 0029, iter [04300, 05004], lr: 0.170455, loss: 2.0130
2022-08-01 12:43:00 - train: epoch 0029, iter [04400, 05004], lr: 0.170408, loss: 2.1995
2022-08-01 12:43:58 - train: epoch 0029, iter [04500, 05004], lr: 0.170361, loss: 2.1881
2022-08-01 12:44:54 - train: epoch 0029, iter [04600, 05004], lr: 0.170314, loss: 2.2487
2022-08-01 12:45:54 - train: epoch 0029, iter [04700, 05004], lr: 0.170267, loss: 1.8940
2022-08-01 12:46:51 - train: epoch 0029, iter [04800, 05004], lr: 0.170220, loss: 2.0654
2022-08-01 12:47:49 - train: epoch 0029, iter [04900, 05004], lr: 0.170173, loss: 2.2695
2022-08-01 12:48:43 - train: epoch 0029, iter [05000, 05004], lr: 0.170126, loss: 1.8720
2022-08-01 12:48:45 - train: epoch 029, train_loss: 2.1117
2022-08-01 12:50:55 - eval: epoch: 029, acc1: 57.144%, acc5: 81.498%, test_loss: 1.7971, per_image_load_time: 3.237ms, per_image_inference_time: 0.646ms
2022-08-01 12:50:56 - until epoch: 029, best_acc1: 57.344%
2022-08-01 12:50:56 - epoch 030 lr: 0.170123
2022-08-01 12:52:04 - train: epoch 0030, iter [00100, 05004], lr: 0.170077, loss: 2.1379
2022-08-01 12:53:02 - train: epoch 0030, iter [00200, 05004], lr: 0.170029, loss: 2.1699
2022-08-01 12:54:00 - train: epoch 0030, iter [00300, 05004], lr: 0.169982, loss: 1.9841
2022-08-01 12:55:00 - train: epoch 0030, iter [00400, 05004], lr: 0.169935, loss: 1.9217
2022-08-01 12:55:56 - train: epoch 0030, iter [00500, 05004], lr: 0.169888, loss: 2.1346
2022-08-01 12:56:55 - train: epoch 0030, iter [00600, 05004], lr: 0.169840, loss: 2.0122
2022-08-01 12:57:52 - train: epoch 0030, iter [00700, 05004], lr: 0.169793, loss: 2.2377
2022-08-01 12:58:51 - train: epoch 0030, iter [00800, 05004], lr: 0.169746, loss: 2.0451
2022-08-01 12:59:50 - train: epoch 0030, iter [00900, 05004], lr: 0.169698, loss: 2.1553
2022-08-01 13:00:47 - train: epoch 0030, iter [01000, 05004], lr: 0.169651, loss: 1.9340
2022-08-01 13:01:46 - train: epoch 0030, iter [01100, 05004], lr: 0.169604, loss: 2.1041
2022-08-01 13:02:44 - train: epoch 0030, iter [01200, 05004], lr: 0.169556, loss: 2.1395
2022-08-01 13:03:42 - train: epoch 0030, iter [01300, 05004], lr: 0.169509, loss: 1.8681
2022-08-01 13:04:40 - train: epoch 0030, iter [01400, 05004], lr: 0.169461, loss: 2.0147
2022-08-01 13:05:39 - train: epoch 0030, iter [01500, 05004], lr: 0.169414, loss: 2.1216
2022-08-01 13:06:36 - train: epoch 0030, iter [01600, 05004], lr: 0.169366, loss: 1.9049
2022-08-01 13:07:34 - train: epoch 0030, iter [01700, 05004], lr: 0.169318, loss: 2.2593
2022-08-01 13:08:30 - train: epoch 0030, iter [01800, 05004], lr: 0.169271, loss: 2.1970
2022-08-01 13:09:32 - train: epoch 0030, iter [01900, 05004], lr: 0.169223, loss: 1.9987
2022-08-01 13:10:27 - train: epoch 0030, iter [02000, 05004], lr: 0.169175, loss: 1.9764
2022-08-01 13:11:28 - train: epoch 0030, iter [02100, 05004], lr: 0.169128, loss: 2.1488
2022-08-01 13:12:23 - train: epoch 0030, iter [02200, 05004], lr: 0.169080, loss: 2.0907
2022-08-01 13:13:22 - train: epoch 0030, iter [02300, 05004], lr: 0.169032, loss: 2.1214
2022-08-01 13:14:20 - train: epoch 0030, iter [02400, 05004], lr: 0.168984, loss: 2.1916
2022-08-01 13:15:17 - train: epoch 0030, iter [02500, 05004], lr: 0.168936, loss: 2.1522
2022-08-01 13:16:16 - train: epoch 0030, iter [02600, 05004], lr: 0.168888, loss: 2.0250
2022-08-01 13:17:13 - train: epoch 0030, iter [02700, 05004], lr: 0.168840, loss: 2.1276
2022-08-01 13:18:11 - train: epoch 0030, iter [02800, 05004], lr: 0.168793, loss: 2.0484
2022-08-01 13:19:08 - train: epoch 0030, iter [02900, 05004], lr: 0.168745, loss: 2.0867
2022-08-01 13:20:08 - train: epoch 0030, iter [03000, 05004], lr: 0.168697, loss: 2.0837
2022-08-01 13:21:02 - train: epoch 0030, iter [03100, 05004], lr: 0.168649, loss: 2.0587
2022-08-01 13:22:01 - train: epoch 0030, iter [03200, 05004], lr: 0.168600, loss: 2.0894
2022-08-01 13:22:59 - train: epoch 0030, iter [03300, 05004], lr: 0.168552, loss: 2.1013
2022-08-01 13:23:57 - train: epoch 0030, iter [03400, 05004], lr: 0.168504, loss: 1.9624
2022-08-01 13:24:54 - train: epoch 0030, iter [03500, 05004], lr: 0.168456, loss: 2.1429
2022-08-01 13:25:54 - train: epoch 0030, iter [03600, 05004], lr: 0.168408, loss: 2.1912
2022-08-01 13:26:49 - train: epoch 0030, iter [03700, 05004], lr: 0.168360, loss: 2.0179
2022-08-01 13:27:47 - train: epoch 0030, iter [03800, 05004], lr: 0.168311, loss: 2.1830
2022-08-01 13:28:43 - train: epoch 0030, iter [03900, 05004], lr: 0.168263, loss: 2.0887
2022-08-01 13:29:42 - train: epoch 0030, iter [04000, 05004], lr: 0.168215, loss: 1.8621
2022-08-01 13:30:38 - train: epoch 0030, iter [04100, 05004], lr: 0.168166, loss: 2.1132
2022-08-01 13:31:35 - train: epoch 0030, iter [04200, 05004], lr: 0.168118, loss: 2.3731
2022-08-01 13:32:36 - train: epoch 0030, iter [04300, 05004], lr: 0.168070, loss: 2.0704
2022-08-01 13:33:33 - train: epoch 0030, iter [04400, 05004], lr: 0.168021, loss: 2.0715
2022-08-01 13:34:29 - train: epoch 0030, iter [04500, 05004], lr: 0.167973, loss: 2.2814
2022-08-01 13:35:29 - train: epoch 0030, iter [04600, 05004], lr: 0.167924, loss: 1.8646
2022-08-01 13:36:24 - train: epoch 0030, iter [04700, 05004], lr: 0.167876, loss: 2.0663
2022-08-01 13:37:22 - train: epoch 0030, iter [04800, 05004], lr: 0.167827, loss: 2.2895
2022-08-01 13:38:20 - train: epoch 0030, iter [04900, 05004], lr: 0.167779, loss: 2.1724
2022-08-01 13:39:15 - train: epoch 0030, iter [05000, 05004], lr: 0.167730, loss: 2.0345
2022-08-01 13:39:17 - train: epoch 030, train_loss: 2.1032
2022-08-01 13:41:25 - eval: epoch: 030, acc1: 56.264%, acc5: 80.826%, test_loss: 1.8564, per_image_load_time: 4.330ms, per_image_inference_time: 0.569ms
2022-08-01 13:41:25 - until epoch: 030, best_acc1: 57.344%
2022-08-01 13:41:25 - epoch 031 lr: 0.167728
2022-08-01 13:42:31 - train: epoch 0031, iter [00100, 05004], lr: 0.167680, loss: 2.0804
2022-08-01 13:43:28 - train: epoch 0031, iter [00200, 05004], lr: 0.167631, loss: 2.0883
2022-08-01 13:44:27 - train: epoch 0031, iter [00300, 05004], lr: 0.167582, loss: 2.0526
2022-08-01 13:45:26 - train: epoch 0031, iter [00400, 05004], lr: 0.167533, loss: 2.0977
2022-08-01 13:46:26 - train: epoch 0031, iter [00500, 05004], lr: 0.167485, loss: 1.9607
2022-08-01 13:47:24 - train: epoch 0031, iter [00600, 05004], lr: 0.167436, loss: 2.0701
2022-08-01 13:48:24 - train: epoch 0031, iter [00700, 05004], lr: 0.167387, loss: 2.1010
2022-08-01 13:49:21 - train: epoch 0031, iter [00800, 05004], lr: 0.167338, loss: 2.0351
2022-08-01 13:50:20 - train: epoch 0031, iter [00900, 05004], lr: 0.167289, loss: 2.1302
2022-08-01 13:51:21 - train: epoch 0031, iter [01000, 05004], lr: 0.167240, loss: 2.2215
2022-08-01 13:52:19 - train: epoch 0031, iter [01100, 05004], lr: 0.167192, loss: 2.4265
2022-08-01 13:53:18 - train: epoch 0031, iter [01200, 05004], lr: 0.167143, loss: 2.2316
2022-08-01 13:54:17 - train: epoch 0031, iter [01300, 05004], lr: 0.167094, loss: 1.9708
2022-08-01 13:55:15 - train: epoch 0031, iter [01400, 05004], lr: 0.167045, loss: 2.0697
2022-08-01 13:56:13 - train: epoch 0031, iter [01500, 05004], lr: 0.166996, loss: 2.2578
2022-08-01 13:57:13 - train: epoch 0031, iter [01600, 05004], lr: 0.166946, loss: 2.2328
2022-08-01 13:58:12 - train: epoch 0031, iter [01700, 05004], lr: 0.166897, loss: 1.7987
2022-08-01 13:59:12 - train: epoch 0031, iter [01800, 05004], lr: 0.166848, loss: 1.8165
2022-08-01 14:00:10 - train: epoch 0031, iter [01900, 05004], lr: 0.166799, loss: 1.9991
2022-08-01 14:01:08 - train: epoch 0031, iter [02000, 05004], lr: 0.166750, loss: 2.2295
2022-08-01 14:02:08 - train: epoch 0031, iter [02100, 05004], lr: 0.166701, loss: 2.1888
2022-08-01 14:03:05 - train: epoch 0031, iter [02200, 05004], lr: 0.166651, loss: 1.9182
2022-08-01 14:04:04 - train: epoch 0031, iter [02300, 05004], lr: 0.166602, loss: 1.9632
2022-08-01 14:05:03 - train: epoch 0031, iter [02400, 05004], lr: 0.166553, loss: 2.1128
2022-08-01 14:06:02 - train: epoch 0031, iter [02500, 05004], lr: 0.166503, loss: 2.1750
2022-08-01 14:07:01 - train: epoch 0031, iter [02600, 05004], lr: 0.166454, loss: 2.0276
2022-08-01 14:08:01 - train: epoch 0031, iter [02700, 05004], lr: 0.166405, loss: 2.1574
2022-08-01 14:08:59 - train: epoch 0031, iter [02800, 05004], lr: 0.166355, loss: 2.3471
2022-08-01 14:09:59 - train: epoch 0031, iter [02900, 05004], lr: 0.166306, loss: 2.2260
2022-08-01 14:10:56 - train: epoch 0031, iter [03000, 05004], lr: 0.166256, loss: 2.4736
2022-08-01 14:11:54 - train: epoch 0031, iter [03100, 05004], lr: 0.166207, loss: 2.2308
2022-08-01 14:12:53 - train: epoch 0031, iter [03200, 05004], lr: 0.166157, loss: 2.2684
2022-08-01 14:13:52 - train: epoch 0031, iter [03300, 05004], lr: 0.166108, loss: 2.0105
2022-08-01 14:14:50 - train: epoch 0031, iter [03400, 05004], lr: 0.166058, loss: 2.1951
2022-08-01 14:15:49 - train: epoch 0031, iter [03500, 05004], lr: 0.166008, loss: 2.3161
2022-08-01 14:16:46 - train: epoch 0031, iter [03600, 05004], lr: 0.165959, loss: 2.1037
2022-08-01 14:17:45 - train: epoch 0031, iter [03700, 05004], lr: 0.165909, loss: 2.0253
2022-08-01 14:18:45 - train: epoch 0031, iter [03800, 05004], lr: 0.165859, loss: 2.0354
2022-08-01 14:19:43 - train: epoch 0031, iter [03900, 05004], lr: 0.165810, loss: 2.1782
2022-08-01 14:20:43 - train: epoch 0031, iter [04000, 05004], lr: 0.165760, loss: 2.0330
2022-08-01 14:21:41 - train: epoch 0031, iter [04100, 05004], lr: 0.165710, loss: 2.0455
2022-08-01 14:22:39 - train: epoch 0031, iter [04200, 05004], lr: 0.165660, loss: 2.2194
2022-08-01 14:23:38 - train: epoch 0031, iter [04300, 05004], lr: 0.165610, loss: 1.8900
2022-08-01 14:24:37 - train: epoch 0031, iter [04400, 05004], lr: 0.165561, loss: 2.2388
2022-08-01 14:25:36 - train: epoch 0031, iter [04500, 05004], lr: 0.165511, loss: 2.0206
2022-08-01 14:26:34 - train: epoch 0031, iter [04600, 05004], lr: 0.165461, loss: 2.2611
2022-08-01 14:27:34 - train: epoch 0031, iter [04700, 05004], lr: 0.165411, loss: 1.8693
2022-08-01 14:28:32 - train: epoch 0031, iter [04800, 05004], lr: 0.165361, loss: 2.1495
2022-08-01 14:29:32 - train: epoch 0031, iter [04900, 05004], lr: 0.165311, loss: 1.8533
2022-08-01 14:30:28 - train: epoch 0031, iter [05000, 05004], lr: 0.165261, loss: 2.2045
2022-08-01 14:30:29 - train: epoch 031, train_loss: 2.0924
2022-08-01 14:32:36 - eval: epoch: 031, acc1: 57.244%, acc5: 81.756%, test_loss: 1.8091, per_image_load_time: 4.115ms, per_image_inference_time: 0.518ms
2022-08-01 14:32:36 - until epoch: 031, best_acc1: 57.344%
2022-08-01 14:32:36 - epoch 032 lr: 0.165258
2022-08-01 14:33:47 - train: epoch 0032, iter [00100, 05004], lr: 0.165208, loss: 1.9007
2022-08-01 14:34:49 - train: epoch 0032, iter [00200, 05004], lr: 0.165158, loss: 2.1127
2022-08-01 14:35:51 - train: epoch 0032, iter [00300, 05004], lr: 0.165108, loss: 1.7308
2022-08-01 14:36:50 - train: epoch 0032, iter [00400, 05004], lr: 0.165058, loss: 1.9144
2022-08-01 14:37:49 - train: epoch 0032, iter [00500, 05004], lr: 0.165008, loss: 2.0840
2022-08-01 14:38:46 - train: epoch 0032, iter [00600, 05004], lr: 0.164958, loss: 1.9173
2022-08-01 14:39:44 - train: epoch 0032, iter [00700, 05004], lr: 0.164907, loss: 2.0566
2022-08-01 14:40:42 - train: epoch 0032, iter [00800, 05004], lr: 0.164857, loss: 2.2428
2022-08-01 14:41:39 - train: epoch 0032, iter [00900, 05004], lr: 0.164807, loss: 2.0628
2022-08-01 14:42:37 - train: epoch 0032, iter [01000, 05004], lr: 0.164756, loss: 2.0963
2022-08-01 14:43:34 - train: epoch 0032, iter [01100, 05004], lr: 0.164706, loss: 2.1195
2022-08-01 14:44:33 - train: epoch 0032, iter [01200, 05004], lr: 0.164656, loss: 2.2564
2022-08-01 14:45:30 - train: epoch 0032, iter [01300, 05004], lr: 0.164605, loss: 2.0837
2022-08-01 14:46:27 - train: epoch 0032, iter [01400, 05004], lr: 0.164555, loss: 2.3549
2022-08-01 14:47:25 - train: epoch 0032, iter [01500, 05004], lr: 0.164504, loss: 2.0057
2022-08-01 14:48:24 - train: epoch 0032, iter [01600, 05004], lr: 0.164454, loss: 2.0417
2022-08-01 14:49:23 - train: epoch 0032, iter [01700, 05004], lr: 0.164403, loss: 2.1196
2022-08-01 14:50:21 - train: epoch 0032, iter [01800, 05004], lr: 0.164353, loss: 2.4563
2022-08-01 14:51:17 - train: epoch 0032, iter [01900, 05004], lr: 0.164302, loss: 1.8857
2022-08-01 14:52:16 - train: epoch 0032, iter [02000, 05004], lr: 0.164251, loss: 2.1931
2022-08-01 14:53:14 - train: epoch 0032, iter [02100, 05004], lr: 0.164201, loss: 1.9466
2022-08-01 14:54:11 - train: epoch 0032, iter [02200, 05004], lr: 0.164150, loss: 1.9732
2022-08-01 14:55:09 - train: epoch 0032, iter [02300, 05004], lr: 0.164099, loss: 2.0767
2022-08-01 14:56:08 - train: epoch 0032, iter [02400, 05004], lr: 0.164049, loss: 2.0536
2022-08-01 14:57:05 - train: epoch 0032, iter [02500, 05004], lr: 0.163998, loss: 1.9763
2022-08-01 14:58:02 - train: epoch 0032, iter [02600, 05004], lr: 0.163947, loss: 1.9647
2022-08-01 14:59:00 - train: epoch 0032, iter [02700, 05004], lr: 0.163896, loss: 2.0747
2022-08-01 14:59:57 - train: epoch 0032, iter [02800, 05004], lr: 0.163845, loss: 2.2287
2022-08-01 15:00:56 - train: epoch 0032, iter [02900, 05004], lr: 0.163795, loss: 2.0182
2022-08-01 15:01:54 - train: epoch 0032, iter [03000, 05004], lr: 0.163744, loss: 2.0207
2022-08-01 15:02:52 - train: epoch 0032, iter [03100, 05004], lr: 0.163693, loss: 2.0640
2022-08-01 15:03:47 - train: epoch 0032, iter [03200, 05004], lr: 0.163642, loss: 2.1091
2022-08-01 15:04:44 - train: epoch 0032, iter [03300, 05004], lr: 0.163591, loss: 2.0226
2022-08-01 15:05:41 - train: epoch 0032, iter [03400, 05004], lr: 0.163540, loss: 2.0337
2022-08-01 15:06:35 - train: epoch 0032, iter [03500, 05004], lr: 0.163489, loss: 2.2569
2022-08-01 15:07:33 - train: epoch 0032, iter [03600, 05004], lr: 0.163438, loss: 2.1195
2022-08-01 15:08:29 - train: epoch 0032, iter [03700, 05004], lr: 0.163387, loss: 2.1663
2022-08-01 15:09:25 - train: epoch 0032, iter [03800, 05004], lr: 0.163335, loss: 1.8397
2022-08-01 15:10:23 - train: epoch 0032, iter [03900, 05004], lr: 0.163284, loss: 1.9455
2022-08-01 15:11:19 - train: epoch 0032, iter [04000, 05004], lr: 0.163233, loss: 2.1558
2022-08-01 15:12:17 - train: epoch 0032, iter [04100, 05004], lr: 0.163182, loss: 1.8776
2022-08-01 15:13:13 - train: epoch 0032, iter [04200, 05004], lr: 0.163131, loss: 1.8614
2022-08-01 15:14:13 - train: epoch 0032, iter [04300, 05004], lr: 0.163079, loss: 2.4767
2022-08-01 15:15:09 - train: epoch 0032, iter [04400, 05004], lr: 0.163028, loss: 2.1358
2022-08-01 15:16:08 - train: epoch 0032, iter [04500, 05004], lr: 0.162977, loss: 2.2384
2022-08-01 15:17:06 - train: epoch 0032, iter [04600, 05004], lr: 0.162925, loss: 2.0379
2022-08-01 15:18:04 - train: epoch 0032, iter [04700, 05004], lr: 0.162874, loss: 2.3036
2022-08-01 15:19:02 - train: epoch 0032, iter [04800, 05004], lr: 0.162823, loss: 2.1093
2022-08-01 15:19:58 - train: epoch 0032, iter [04900, 05004], lr: 0.162771, loss: 2.1782
2022-08-01 15:20:53 - train: epoch 0032, iter [05000, 05004], lr: 0.162720, loss: 2.0573
2022-08-01 15:20:55 - train: epoch 032, train_loss: 2.0832
2022-08-01 15:23:11 - eval: epoch: 032, acc1: 57.994%, acc5: 82.278%, test_loss: 1.7537, per_image_load_time: 2.632ms, per_image_inference_time: 0.538ms
2022-08-01 15:23:12 - until epoch: 032, best_acc1: 57.994%
2022-08-01 15:23:12 - epoch 033 lr: 0.162717
2022-08-01 15:24:23 - train: epoch 0033, iter [00100, 05004], lr: 0.162666, loss: 1.7846
2022-08-01 15:25:25 - train: epoch 0033, iter [00200, 05004], lr: 0.162615, loss: 2.1289
2022-08-01 15:26:28 - train: epoch 0033, iter [00300, 05004], lr: 0.162563, loss: 1.9916
2022-08-01 15:27:25 - train: epoch 0033, iter [00400, 05004], lr: 0.162512, loss: 1.8514
2022-08-01 15:28:22 - train: epoch 0033, iter [00500, 05004], lr: 0.162460, loss: 2.1659
2022-08-01 15:29:21 - train: epoch 0033, iter [00600, 05004], lr: 0.162408, loss: 1.9249
2022-08-01 15:30:20 - train: epoch 0033, iter [00700, 05004], lr: 0.162357, loss: 2.4008
2022-08-01 15:31:17 - train: epoch 0033, iter [00800, 05004], lr: 0.162305, loss: 2.2517
2022-08-01 15:32:14 - train: epoch 0033, iter [00900, 05004], lr: 0.162253, loss: 2.0423
2022-08-01 15:33:10 - train: epoch 0033, iter [01000, 05004], lr: 0.162202, loss: 2.0846
2022-08-01 15:34:09 - train: epoch 0033, iter [01100, 05004], lr: 0.162150, loss: 1.9852
2022-08-01 15:35:07 - train: epoch 0033, iter [01200, 05004], lr: 0.162098, loss: 2.2687
2022-08-01 15:36:04 - train: epoch 0033, iter [01300, 05004], lr: 0.162046, loss: 1.8511
2022-08-01 15:37:02 - train: epoch 0033, iter [01400, 05004], lr: 0.161994, loss: 1.9576
2022-08-01 15:37:59 - train: epoch 0033, iter [01500, 05004], lr: 0.161942, loss: 2.1413
2022-08-01 15:38:58 - train: epoch 0033, iter [01600, 05004], lr: 0.161891, loss: 2.2263
2022-08-01 15:39:55 - train: epoch 0033, iter [01700, 05004], lr: 0.161839, loss: 1.9044
2022-08-01 15:40:53 - train: epoch 0033, iter [01800, 05004], lr: 0.161787, loss: 2.1046
2022-08-01 15:41:51 - train: epoch 0033, iter [01900, 05004], lr: 0.161735, loss: 1.9019
2022-08-01 15:42:49 - train: epoch 0033, iter [02000, 05004], lr: 0.161683, loss: 1.8788
2022-08-01 15:43:47 - train: epoch 0033, iter [02100, 05004], lr: 0.161631, loss: 1.9383
2022-08-01 15:44:44 - train: epoch 0033, iter [02200, 05004], lr: 0.161579, loss: 2.3431
2022-08-01 15:45:43 - train: epoch 0033, iter [02300, 05004], lr: 0.161527, loss: 2.0493
2022-08-01 15:46:41 - train: epoch 0033, iter [02400, 05004], lr: 0.161474, loss: 2.1887
2022-08-01 15:47:38 - train: epoch 0033, iter [02500, 05004], lr: 0.161422, loss: 2.0337
2022-08-01 15:48:38 - train: epoch 0033, iter [02600, 05004], lr: 0.161370, loss: 1.9320
2022-08-01 15:49:34 - train: epoch 0033, iter [02700, 05004], lr: 0.161318, loss: 2.0940
2022-08-01 15:50:33 - train: epoch 0033, iter [02800, 05004], lr: 0.161266, loss: 2.1865
2022-08-01 15:51:29 - train: epoch 0033, iter [02900, 05004], lr: 0.161213, loss: 1.9633
2022-08-01 15:52:27 - train: epoch 0033, iter [03000, 05004], lr: 0.161161, loss: 2.0823
2022-08-01 15:53:25 - train: epoch 0033, iter [03100, 05004], lr: 0.161109, loss: 1.9135
2022-08-01 15:54:20 - train: epoch 0033, iter [03200, 05004], lr: 0.161057, loss: 1.9652
2022-08-01 15:55:17 - train: epoch 0033, iter [03300, 05004], lr: 0.161004, loss: 1.9672
2022-08-01 15:56:15 - train: epoch 0033, iter [03400, 05004], lr: 0.160952, loss: 1.9306
2022-08-01 15:57:10 - train: epoch 0033, iter [03500, 05004], lr: 0.160899, loss: 2.1472
2022-08-01 15:58:07 - train: epoch 0033, iter [03600, 05004], lr: 0.160847, loss: 2.2556
2022-08-01 15:59:02 - train: epoch 0033, iter [03700, 05004], lr: 0.160795, loss: 1.9923
2022-08-01 16:00:00 - train: epoch 0033, iter [03800, 05004], lr: 0.160742, loss: 2.0759
2022-08-01 16:00:57 - train: epoch 0033, iter [03900, 05004], lr: 0.160690, loss: 2.3851
2022-08-01 16:01:53 - train: epoch 0033, iter [04000, 05004], lr: 0.160637, loss: 2.1592
2022-08-01 16:02:50 - train: epoch 0033, iter [04100, 05004], lr: 0.160584, loss: 2.1247
2022-08-01 16:03:48 - train: epoch 0033, iter [04200, 05004], lr: 0.160532, loss: 2.0441
2022-08-01 16:04:44 - train: epoch 0033, iter [04300, 05004], lr: 0.160479, loss: 2.2444
2022-08-01 16:05:42 - train: epoch 0033, iter [04400, 05004], lr: 0.160427, loss: 2.0983
2022-08-01 16:06:39 - train: epoch 0033, iter [04500, 05004], lr: 0.160374, loss: 2.1440
2022-08-01 16:07:37 - train: epoch 0033, iter [04600, 05004], lr: 0.160321, loss: 2.1965
2022-08-01 16:08:34 - train: epoch 0033, iter [04700, 05004], lr: 0.160269, loss: 2.2034
2022-08-01 16:09:30 - train: epoch 0033, iter [04800, 05004], lr: 0.160216, loss: 2.2985
2022-08-01 16:10:28 - train: epoch 0033, iter [04900, 05004], lr: 0.160163, loss: 1.9349
2022-08-01 16:11:22 - train: epoch 0033, iter [05000, 05004], lr: 0.160110, loss: 2.1100
2022-08-01 16:11:23 - train: epoch 033, train_loss: 2.0707
2022-08-01 16:13:33 - eval: epoch: 033, acc1: 57.678%, acc5: 81.656%, test_loss: 1.7879, per_image_load_time: 4.068ms, per_image_inference_time: 0.636ms
2022-08-01 16:13:33 - until epoch: 033, best_acc1: 57.994%
2022-08-01 16:13:33 - epoch 034 lr: 0.160108
2022-08-01 16:14:43 - train: epoch 0034, iter [00100, 05004], lr: 0.160055, loss: 1.9224
2022-08-01 16:15:41 - train: epoch 0034, iter [00200, 05004], lr: 0.160002, loss: 1.9497
2022-08-01 16:16:40 - train: epoch 0034, iter [00300, 05004], lr: 0.159950, loss: 1.8412
2022-08-01 16:17:36 - train: epoch 0034, iter [00400, 05004], lr: 0.159897, loss: 1.9832
2022-08-01 16:18:35 - train: epoch 0034, iter [00500, 05004], lr: 0.159844, loss: 1.9085
2022-08-01 16:19:32 - train: epoch 0034, iter [00600, 05004], lr: 0.159791, loss: 2.1518
2022-08-01 16:20:30 - train: epoch 0034, iter [00700, 05004], lr: 0.159738, loss: 1.8946
2022-08-01 16:21:29 - train: epoch 0034, iter [00800, 05004], lr: 0.159685, loss: 2.3171
2022-08-01 16:22:27 - train: epoch 0034, iter [00900, 05004], lr: 0.159632, loss: 2.0557
2022-08-01 16:23:25 - train: epoch 0034, iter [01000, 05004], lr: 0.159579, loss: 2.0308
2022-08-01 16:24:22 - train: epoch 0034, iter [01100, 05004], lr: 0.159526, loss: 2.1907
2022-08-01 16:25:22 - train: epoch 0034, iter [01200, 05004], lr: 0.159472, loss: 2.2882
2022-08-01 16:26:22 - train: epoch 0034, iter [01300, 05004], lr: 0.159419, loss: 1.9408
2022-08-01 16:27:21 - train: epoch 0034, iter [01400, 05004], lr: 0.159366, loss: 2.1303
2022-08-01 16:28:18 - train: epoch 0034, iter [01500, 05004], lr: 0.159313, loss: 2.0635
2022-08-01 16:29:18 - train: epoch 0034, iter [01600, 05004], lr: 0.159260, loss: 2.1692
2022-08-01 16:30:17 - train: epoch 0034, iter [01700, 05004], lr: 0.159206, loss: 1.9851
2022-08-01 16:31:14 - train: epoch 0034, iter [01800, 05004], lr: 0.159153, loss: 2.4469
2022-08-01 16:32:12 - train: epoch 0034, iter [01900, 05004], lr: 0.159100, loss: 2.1286
2022-08-01 16:33:09 - train: epoch 0034, iter [02000, 05004], lr: 0.159047, loss: 1.8845
2022-08-01 16:34:06 - train: epoch 0034, iter [02100, 05004], lr: 0.158993, loss: 2.0988
2022-08-01 16:35:05 - train: epoch 0034, iter [02200, 05004], lr: 0.158940, loss: 1.8126
2022-08-01 16:36:02 - train: epoch 0034, iter [02300, 05004], lr: 0.158886, loss: 2.1805
2022-08-01 16:36:59 - train: epoch 0034, iter [02400, 05004], lr: 0.158833, loss: 1.8913
2022-08-01 16:37:55 - train: epoch 0034, iter [02500, 05004], lr: 0.158780, loss: 2.1125
2022-08-01 16:38:52 - train: epoch 0034, iter [02600, 05004], lr: 0.158726, loss: 2.2781
2022-08-01 16:39:51 - train: epoch 0034, iter [02700, 05004], lr: 0.158673, loss: 2.0493
2022-08-01 16:40:48 - train: epoch 0034, iter [02800, 05004], lr: 0.158619, loss: 1.8086
2022-08-01 16:41:45 - train: epoch 0034, iter [02900, 05004], lr: 0.158566, loss: 1.8721
2022-08-01 16:42:43 - train: epoch 0034, iter [03000, 05004], lr: 0.158512, loss: 1.9035
2022-08-01 16:43:41 - train: epoch 0034, iter [03100, 05004], lr: 0.158458, loss: 2.3036
2022-08-01 16:44:37 - train: epoch 0034, iter [03200, 05004], lr: 0.158405, loss: 1.9986
2022-08-01 16:45:35 - train: epoch 0034, iter [03300, 05004], lr: 0.158351, loss: 2.1446
2022-08-01 16:46:33 - train: epoch 0034, iter [03400, 05004], lr: 0.158297, loss: 2.2560
2022-08-01 16:47:30 - train: epoch 0034, iter [03500, 05004], lr: 0.158244, loss: 1.8466
2022-08-01 16:48:26 - train: epoch 0034, iter [03600, 05004], lr: 0.158190, loss: 1.8214
2022-08-01 16:49:24 - train: epoch 0034, iter [03700, 05004], lr: 0.158136, loss: 2.0185
2022-08-01 16:50:20 - train: epoch 0034, iter [03800, 05004], lr: 0.158082, loss: 2.0067
2022-08-01 16:51:17 - train: epoch 0034, iter [03900, 05004], lr: 0.158029, loss: 2.3197
2022-08-01 16:52:15 - train: epoch 0034, iter [04000, 05004], lr: 0.157975, loss: 2.0500
2022-08-01 16:53:12 - train: epoch 0034, iter [04100, 05004], lr: 0.157921, loss: 2.1225
2022-08-01 16:54:09 - train: epoch 0034, iter [04200, 05004], lr: 0.157867, loss: 2.0256
2022-08-01 16:55:05 - train: epoch 0034, iter [04300, 05004], lr: 0.157813, loss: 2.0618
2022-08-01 16:56:03 - train: epoch 0034, iter [04400, 05004], lr: 0.157759, loss: 2.0769
2022-08-01 16:57:00 - train: epoch 0034, iter [04500, 05004], lr: 0.157705, loss: 2.1808
2022-08-01 16:57:56 - train: epoch 0034, iter [04600, 05004], lr: 0.157651, loss: 2.2676
2022-08-01 16:58:53 - train: epoch 0034, iter [04700, 05004], lr: 0.157597, loss: 2.2006
2022-08-01 16:59:50 - train: epoch 0034, iter [04800, 05004], lr: 0.157543, loss: 2.0576
2022-08-01 17:00:46 - train: epoch 0034, iter [04900, 05004], lr: 0.157489, loss: 2.1148
2022-08-01 17:01:41 - train: epoch 0034, iter [05000, 05004], lr: 0.157435, loss: 2.0603
2022-08-01 17:01:42 - train: epoch 034, train_loss: 2.0619
2022-08-01 17:03:51 - eval: epoch: 034, acc1: 58.052%, acc5: 82.148%, test_loss: 1.7638, per_image_load_time: 2.852ms, per_image_inference_time: 0.598ms
2022-08-01 17:03:51 - until epoch: 034, best_acc1: 58.052%
2022-08-01 17:03:51 - epoch 035 lr: 0.157432
2022-08-01 17:04:58 - train: epoch 0035, iter [00100, 05004], lr: 0.157379, loss: 1.8599
2022-08-01 17:05:57 - train: epoch 0035, iter [00200, 05004], lr: 0.157325, loss: 1.9898
2022-08-01 17:06:57 - train: epoch 0035, iter [00300, 05004], lr: 0.157270, loss: 2.1646
2022-08-01 17:07:55 - train: epoch 0035, iter [00400, 05004], lr: 0.157216, loss: 1.9487
2022-08-01 17:08:53 - train: epoch 0035, iter [00500, 05004], lr: 0.157162, loss: 2.0577
2022-08-01 17:09:50 - train: epoch 0035, iter [00600, 05004], lr: 0.157108, loss: 2.1029
2022-08-01 17:10:47 - train: epoch 0035, iter [00700, 05004], lr: 0.157054, loss: 2.0885
2022-08-01 17:11:44 - train: epoch 0035, iter [00800, 05004], lr: 0.156999, loss: 2.1050
2022-08-01 17:12:42 - train: epoch 0035, iter [00900, 05004], lr: 0.156945, loss: 2.1283
2022-08-01 17:13:41 - train: epoch 0035, iter [01000, 05004], lr: 0.156891, loss: 1.8142
2022-08-01 17:14:37 - train: epoch 0035, iter [01100, 05004], lr: 0.156836, loss: 2.2212
2022-08-01 17:15:35 - train: epoch 0035, iter [01200, 05004], lr: 0.156782, loss: 1.8932
2022-08-01 17:16:33 - train: epoch 0035, iter [01300, 05004], lr: 0.156727, loss: 2.2414
2022-08-01 17:17:30 - train: epoch 0035, iter [01400, 05004], lr: 0.156673, loss: 1.9342
2022-08-01 17:18:28 - train: epoch 0035, iter [01500, 05004], lr: 0.156619, loss: 2.1907
2022-08-01 17:19:27 - train: epoch 0035, iter [01600, 05004], lr: 0.156564, loss: 1.8988
2022-08-01 17:20:24 - train: epoch 0035, iter [01700, 05004], lr: 0.156510, loss: 1.7842
2022-08-01 17:21:22 - train: epoch 0035, iter [01800, 05004], lr: 0.156455, loss: 2.0397
2022-08-01 17:22:19 - train: epoch 0035, iter [01900, 05004], lr: 0.156400, loss: 2.0382
2022-08-01 17:23:17 - train: epoch 0035, iter [02000, 05004], lr: 0.156346, loss: 2.2264
2022-08-01 17:24:15 - train: epoch 0035, iter [02100, 05004], lr: 0.156291, loss: 2.0465
2022-08-01 17:25:13 - train: epoch 0035, iter [02200, 05004], lr: 0.156237, loss: 2.2225
2022-08-01 17:26:12 - train: epoch 0035, iter [02300, 05004], lr: 0.156182, loss: 2.0430
2022-08-01 17:27:12 - train: epoch 0035, iter [02400, 05004], lr: 0.156127, loss: 2.2532
2022-08-01 17:28:09 - train: epoch 0035, iter [02500, 05004], lr: 0.156073, loss: 2.0568
2022-08-01 17:29:07 - train: epoch 0035, iter [02600, 05004], lr: 0.156018, loss: 2.3022
2022-08-01 17:30:05 - train: epoch 0035, iter [02700, 05004], lr: 0.155963, loss: 2.0422
2022-08-01 17:31:04 - train: epoch 0035, iter [02800, 05004], lr: 0.155908, loss: 2.0687
2022-08-01 17:32:01 - train: epoch 0035, iter [02900, 05004], lr: 0.155854, loss: 1.9033
2022-08-01 17:32:59 - train: epoch 0035, iter [03000, 05004], lr: 0.155799, loss: 1.8261
2022-08-01 17:33:55 - train: epoch 0035, iter [03100, 05004], lr: 0.155744, loss: 1.8993
2022-08-01 17:34:52 - train: epoch 0035, iter [03200, 05004], lr: 0.155689, loss: 2.0659
2022-08-01 17:35:50 - train: epoch 0035, iter [03300, 05004], lr: 0.155634, loss: 1.9979
2022-08-01 17:36:49 - train: epoch 0035, iter [03400, 05004], lr: 0.155579, loss: 1.9972
2022-08-01 17:37:45 - train: epoch 0035, iter [03500, 05004], lr: 0.155524, loss: 1.9638
2022-08-01 17:38:43 - train: epoch 0035, iter [03600, 05004], lr: 0.155469, loss: 2.2254
2022-08-01 17:39:41 - train: epoch 0035, iter [03700, 05004], lr: 0.155414, loss: 1.8908
2022-08-01 17:40:37 - train: epoch 0035, iter [03800, 05004], lr: 0.155359, loss: 1.9630
2022-08-01 17:41:36 - train: epoch 0035, iter [03900, 05004], lr: 0.155304, loss: 2.0229
2022-08-01 17:42:32 - train: epoch 0035, iter [04000, 05004], lr: 0.155249, loss: 1.7750
2022-08-01 17:43:29 - train: epoch 0035, iter [04100, 05004], lr: 0.155194, loss: 2.2136
2022-08-01 17:44:26 - train: epoch 0035, iter [04200, 05004], lr: 0.155139, loss: 1.9197
2022-08-01 17:45:23 - train: epoch 0035, iter [04300, 05004], lr: 0.155084, loss: 2.0796
2022-08-01 17:46:18 - train: epoch 0035, iter [04400, 05004], lr: 0.155029, loss: 2.2123
2022-08-01 17:47:15 - train: epoch 0035, iter [04500, 05004], lr: 0.154973, loss: 2.0621
2022-08-01 17:48:10 - train: epoch 0035, iter [04600, 05004], lr: 0.154918, loss: 1.9673
2022-08-01 17:49:06 - train: epoch 0035, iter [04700, 05004], lr: 0.154863, loss: 2.2703
2022-08-01 17:50:01 - train: epoch 0035, iter [04800, 05004], lr: 0.154808, loss: 2.1204
2022-08-01 17:50:56 - train: epoch 0035, iter [04900, 05004], lr: 0.154752, loss: 2.2603
2022-08-01 17:51:48 - train: epoch 0035, iter [05000, 05004], lr: 0.154697, loss: 2.1114
2022-08-01 17:51:50 - train: epoch 035, train_loss: 2.0540
2022-08-01 17:53:53 - eval: epoch: 035, acc1: 58.072%, acc5: 82.256%, test_loss: 1.7514, per_image_load_time: 4.303ms, per_image_inference_time: 0.504ms
2022-08-01 17:53:54 - until epoch: 035, best_acc1: 58.072%
2022-08-01 17:53:54 - epoch 036 lr: 0.154694
2022-08-01 17:55:01 - train: epoch 0036, iter [00100, 05004], lr: 0.154639, loss: 2.2016
2022-08-01 17:56:00 - train: epoch 0036, iter [00200, 05004], lr: 0.154584, loss: 2.0585
2022-08-01 17:56:54 - train: epoch 0036, iter [00300, 05004], lr: 0.154529, loss: 1.9128
2022-08-01 17:57:51 - train: epoch 0036, iter [00400, 05004], lr: 0.154473, loss: 1.9044
2022-08-01 17:58:46 - train: epoch 0036, iter [00500, 05004], lr: 0.154418, loss: 2.0574
2022-08-01 17:59:46 - train: epoch 0036, iter [00600, 05004], lr: 0.154362, loss: 2.0357
2022-08-01 18:00:39 - train: epoch 0036, iter [00700, 05004], lr: 0.154307, loss: 1.9036
2022-08-01 18:01:38 - train: epoch 0036, iter [00800, 05004], lr: 0.154251, loss: 1.9465
2022-08-01 18:02:32 - train: epoch 0036, iter [00900, 05004], lr: 0.154196, loss: 1.9620
2022-08-01 18:03:29 - train: epoch 0036, iter [01000, 05004], lr: 0.154140, loss: 1.9257
2022-08-01 18:04:26 - train: epoch 0036, iter [01100, 05004], lr: 0.154085, loss: 1.8649
2022-08-01 18:05:22 - train: epoch 0036, iter [01200, 05004], lr: 0.154029, loss: 1.9773
2022-08-01 18:06:19 - train: epoch 0036, iter [01300, 05004], lr: 0.153974, loss: 2.1003
2022-08-01 18:07:15 - train: epoch 0036, iter [01400, 05004], lr: 0.153918, loss: 2.0801
2022-08-01 18:08:10 - train: epoch 0036, iter [01500, 05004], lr: 0.153862, loss: 2.0023
2022-08-01 18:09:05 - train: epoch 0036, iter [01600, 05004], lr: 0.153807, loss: 2.0685
2022-08-01 18:10:04 - train: epoch 0036, iter [01700, 05004], lr: 0.153751, loss: 2.0455
2022-08-01 18:11:00 - train: epoch 0036, iter [01800, 05004], lr: 0.153695, loss: 1.7801
2022-08-01 18:11:55 - train: epoch 0036, iter [01900, 05004], lr: 0.153639, loss: 2.1816
2022-08-01 18:12:51 - train: epoch 0036, iter [02000, 05004], lr: 0.153584, loss: 1.9777
2022-08-01 18:13:44 - train: epoch 0036, iter [02100, 05004], lr: 0.153528, loss: 1.9446
2022-08-01 18:14:39 - train: epoch 0036, iter [02200, 05004], lr: 0.153472, loss: 2.0950
2022-08-01 18:15:35 - train: epoch 0036, iter [02300, 05004], lr: 0.153416, loss: 2.0230
2022-08-01 18:16:31 - train: epoch 0036, iter [02400, 05004], lr: 0.153360, loss: 2.2410
2022-08-01 18:17:26 - train: epoch 0036, iter [02500, 05004], lr: 0.153304, loss: 1.7673
2022-08-01 18:18:21 - train: epoch 0036, iter [02600, 05004], lr: 0.153248, loss: 2.1689
2022-08-01 18:19:15 - train: epoch 0036, iter [02700, 05004], lr: 0.153192, loss: 1.8372
2022-08-01 18:20:10 - train: epoch 0036, iter [02800, 05004], lr: 0.153136, loss: 1.8812
2022-08-01 18:21:05 - train: epoch 0036, iter [02900, 05004], lr: 0.153080, loss: 1.8390
2022-08-01 18:22:01 - train: epoch 0036, iter [03000, 05004], lr: 0.153024, loss: 2.0575
2022-08-01 18:22:57 - train: epoch 0036, iter [03100, 05004], lr: 0.152968, loss: 2.2194
2022-08-01 18:23:53 - train: epoch 0036, iter [03200, 05004], lr: 0.152912, loss: 2.2418
2022-08-01 18:24:46 - train: epoch 0036, iter [03300, 05004], lr: 0.152856, loss: 1.8627
2022-08-01 18:25:42 - train: epoch 0036, iter [03400, 05004], lr: 0.152800, loss: 1.9133
2022-08-01 18:26:37 - train: epoch 0036, iter [03500, 05004], lr: 0.152744, loss: 2.2053
2022-08-01 18:27:33 - train: epoch 0036, iter [03600, 05004], lr: 0.152688, loss: 2.1738
2022-08-01 18:28:29 - train: epoch 0036, iter [03700, 05004], lr: 0.152632, loss: 1.9349
2022-08-01 18:29:24 - train: epoch 0036, iter [03800, 05004], lr: 0.152575, loss: 2.0679
2022-08-01 18:30:20 - train: epoch 0036, iter [03900, 05004], lr: 0.152519, loss: 2.1156
2022-08-01 18:31:16 - train: epoch 0036, iter [04000, 05004], lr: 0.152463, loss: 1.9578
2022-08-01 18:32:10 - train: epoch 0036, iter [04100, 05004], lr: 0.152407, loss: 1.9647
2022-08-01 18:33:05 - train: epoch 0036, iter [04200, 05004], lr: 0.152350, loss: 2.0694
2022-08-01 18:33:59 - train: epoch 0036, iter [04300, 05004], lr: 0.152294, loss: 2.0454
2022-08-01 18:34:54 - train: epoch 0036, iter [04400, 05004], lr: 0.152238, loss: 2.2540
2022-08-01 18:35:50 - train: epoch 0036, iter [04500, 05004], lr: 0.152181, loss: 1.9367
2022-08-01 18:36:46 - train: epoch 0036, iter [04600, 05004], lr: 0.152125, loss: 2.0356
2022-08-01 18:37:40 - train: epoch 0036, iter [04700, 05004], lr: 0.152069, loss: 2.0694
2022-08-01 18:38:37 - train: epoch 0036, iter [04800, 05004], lr: 0.152012, loss: 1.9821
2022-08-01 18:39:33 - train: epoch 0036, iter [04900, 05004], lr: 0.151956, loss: 2.0430
2022-08-01 18:40:26 - train: epoch 0036, iter [05000, 05004], lr: 0.151899, loss: 2.0385
2022-08-01 18:40:27 - train: epoch 036, train_loss: 2.0421
2022-08-01 18:42:33 - eval: epoch: 036, acc1: 58.242%, acc5: 82.330%, test_loss: 1.7445, per_image_load_time: 4.336ms, per_image_inference_time: 0.487ms
2022-08-01 18:42:33 - until epoch: 036, best_acc1: 58.242%
2022-08-01 18:42:33 - epoch 037 lr: 0.151896
2022-08-01 18:43:39 - train: epoch 0037, iter [00100, 05004], lr: 0.151840, loss: 1.8609
2022-08-01 18:44:36 - train: epoch 0037, iter [00200, 05004], lr: 0.151784, loss: 1.7931
2022-08-01 18:45:34 - train: epoch 0037, iter [00300, 05004], lr: 0.151727, loss: 1.9215
2022-08-01 18:46:30 - train: epoch 0037, iter [00400, 05004], lr: 0.151671, loss: 2.0214
2022-08-01 18:47:24 - train: epoch 0037, iter [00500, 05004], lr: 0.151614, loss: 1.9388
2022-08-01 18:48:21 - train: epoch 0037, iter [00600, 05004], lr: 0.151558, loss: 2.3057
2022-08-01 18:49:16 - train: epoch 0037, iter [00700, 05004], lr: 0.151501, loss: 2.0605
2022-08-01 18:50:11 - train: epoch 0037, iter [00800, 05004], lr: 0.151444, loss: 2.1788
2022-08-01 18:51:06 - train: epoch 0037, iter [00900, 05004], lr: 0.151388, loss: 2.2189
2022-08-01 18:52:02 - train: epoch 0037, iter [01000, 05004], lr: 0.151331, loss: 1.8719
2022-08-01 18:52:57 - train: epoch 0037, iter [01100, 05004], lr: 0.151274, loss: 2.0109
2022-08-01 18:53:52 - train: epoch 0037, iter [01200, 05004], lr: 0.151217, loss: 2.2419
2022-08-01 18:54:47 - train: epoch 0037, iter [01300, 05004], lr: 0.151161, loss: 2.0238
2022-08-01 18:55:44 - train: epoch 0037, iter [01400, 05004], lr: 0.151104, loss: 2.1541
2022-08-01 18:56:38 - train: epoch 0037, iter [01500, 05004], lr: 0.151047, loss: 1.9968
2022-08-01 18:57:33 - train: epoch 0037, iter [01600, 05004], lr: 0.150990, loss: 1.8323
2022-08-01 18:58:29 - train: epoch 0037, iter [01700, 05004], lr: 0.150933, loss: 2.1574
2022-08-01 18:59:25 - train: epoch 0037, iter [01800, 05004], lr: 0.150876, loss: 1.7633
2022-08-01 19:00:21 - train: epoch 0037, iter [01900, 05004], lr: 0.150820, loss: 1.9668
2022-08-01 19:01:16 - train: epoch 0037, iter [02000, 05004], lr: 0.150763, loss: 1.9038
2022-08-01 19:02:13 - train: epoch 0037, iter [02100, 05004], lr: 0.150706, loss: 1.9395
2022-08-01 19:03:10 - train: epoch 0037, iter [02200, 05004], lr: 0.150649, loss: 2.0124
2022-08-01 19:04:04 - train: epoch 0037, iter [02300, 05004], lr: 0.150592, loss: 1.9801
2022-08-01 19:05:00 - train: epoch 0037, iter [02400, 05004], lr: 0.150535, loss: 2.1167
2022-08-01 19:05:54 - train: epoch 0037, iter [02500, 05004], lr: 0.150478, loss: 2.0591
2022-08-01 19:06:50 - train: epoch 0037, iter [02600, 05004], lr: 0.150421, loss: 2.2164
2022-08-01 19:07:46 - train: epoch 0037, iter [02700, 05004], lr: 0.150364, loss: 2.2379
2022-08-01 19:08:42 - train: epoch 0037, iter [02800, 05004], lr: 0.150306, loss: 1.9902
2022-08-01 19:09:37 - train: epoch 0037, iter [02900, 05004], lr: 0.150249, loss: 2.1666
2022-08-01 19:10:31 - train: epoch 0037, iter [03000, 05004], lr: 0.150192, loss: 2.1917
2022-08-01 19:11:29 - train: epoch 0037, iter [03100, 05004], lr: 0.150135, loss: 2.0586
2022-08-01 19:12:24 - train: epoch 0037, iter [03200, 05004], lr: 0.150078, loss: 2.1449
2022-08-01 19:13:21 - train: epoch 0037, iter [03300, 05004], lr: 0.150021, loss: 2.2543
2022-08-01 19:14:16 - train: epoch 0037, iter [03400, 05004], lr: 0.149963, loss: 1.8455
2022-08-01 19:15:15 - train: epoch 0037, iter [03500, 05004], lr: 0.149906, loss: 1.7971
2022-08-01 19:16:10 - train: epoch 0037, iter [03600, 05004], lr: 0.149849, loss: 1.9886
2022-08-01 19:17:06 - train: epoch 0037, iter [03700, 05004], lr: 0.149792, loss: 2.3627
2022-08-01 19:18:04 - train: epoch 0037, iter [03800, 05004], lr: 0.149734, loss: 2.0619
2022-08-01 19:19:02 - train: epoch 0037, iter [03900, 05004], lr: 0.149677, loss: 2.2115
2022-08-01 19:19:57 - train: epoch 0037, iter [04000, 05004], lr: 0.149619, loss: 2.1978
2022-08-01 19:20:52 - train: epoch 0037, iter [04100, 05004], lr: 0.149562, loss: 1.7318
2022-08-01 19:21:47 - train: epoch 0037, iter [04200, 05004], lr: 0.149505, loss: 1.9848
2022-08-01 19:22:44 - train: epoch 0037, iter [04300, 05004], lr: 0.149447, loss: 2.0929
2022-08-01 19:23:40 - train: epoch 0037, iter [04400, 05004], lr: 0.149390, loss: 1.8376
2022-08-01 19:24:37 - train: epoch 0037, iter [04500, 05004], lr: 0.149332, loss: 1.8323
2022-08-01 19:25:33 - train: epoch 0037, iter [04600, 05004], lr: 0.149275, loss: 2.0269
2022-08-01 19:26:30 - train: epoch 0037, iter [04700, 05004], lr: 0.149217, loss: 1.9992
2022-08-01 19:27:25 - train: epoch 0037, iter [04800, 05004], lr: 0.149160, loss: 2.0892
2022-08-01 19:28:21 - train: epoch 0037, iter [04900, 05004], lr: 0.149102, loss: 2.0451
2022-08-01 19:29:15 - train: epoch 0037, iter [05000, 05004], lr: 0.149045, loss: 2.0300
2022-08-01 19:29:17 - train: epoch 037, train_loss: 2.0317
2022-08-01 19:31:23 - eval: epoch: 037, acc1: 58.338%, acc5: 82.522%, test_loss: 1.7419, per_image_load_time: 3.333ms, per_image_inference_time: 0.520ms
2022-08-01 19:31:23 - until epoch: 037, best_acc1: 58.338%
2022-08-01 19:31:23 - epoch 038 lr: 0.149042
2022-08-01 19:32:31 - train: epoch 0038, iter [00100, 05004], lr: 0.148985, loss: 1.8951
2022-08-01 19:33:30 - train: epoch 0038, iter [00200, 05004], lr: 0.148927, loss: 1.9570
2022-08-01 19:34:26 - train: epoch 0038, iter [00300, 05004], lr: 0.148869, loss: 1.9536
2022-08-01 19:35:25 - train: epoch 0038, iter [00400, 05004], lr: 0.148812, loss: 1.8276
2022-08-01 19:36:22 - train: epoch 0038, iter [00500, 05004], lr: 0.148754, loss: 1.8722
2022-08-01 19:37:19 - train: epoch 0038, iter [00600, 05004], lr: 0.148696, loss: 2.1448
2022-08-01 19:38:17 - train: epoch 0038, iter [00700, 05004], lr: 0.148639, loss: 1.8882
2022-08-01 19:39:15 - train: epoch 0038, iter [00800, 05004], lr: 0.148581, loss: 2.0084
2022-08-01 19:40:12 - train: epoch 0038, iter [00900, 05004], lr: 0.148523, loss: 1.9778
2022-08-01 19:41:09 - train: epoch 0038, iter [01000, 05004], lr: 0.148465, loss: 2.0926
2022-08-01 19:42:07 - train: epoch 0038, iter [01100, 05004], lr: 0.148408, loss: 2.0909
2022-08-01 19:43:03 - train: epoch 0038, iter [01200, 05004], lr: 0.148350, loss: 2.0465
2022-08-01 19:43:59 - train: epoch 0038, iter [01300, 05004], lr: 0.148292, loss: 2.0665
2022-08-01 19:44:54 - train: epoch 0038, iter [01400, 05004], lr: 0.148234, loss: 1.9837
2022-08-01 19:45:51 - train: epoch 0038, iter [01500, 05004], lr: 0.148176, loss: 2.1130
2022-08-01 19:46:46 - train: epoch 0038, iter [01600, 05004], lr: 0.148118, loss: 2.2780
2022-08-01 19:47:42 - train: epoch 0038, iter [01700, 05004], lr: 0.148060, loss: 2.1744
2022-08-01 19:48:38 - train: epoch 0038, iter [01800, 05004], lr: 0.148002, loss: 2.1094
2022-08-01 19:49:34 - train: epoch 0038, iter [01900, 05004], lr: 0.147944, loss: 1.9523
2022-08-01 19:50:29 - train: epoch 0038, iter [02000, 05004], lr: 0.147886, loss: 2.0637
2022-08-01 19:51:25 - train: epoch 0038, iter [02100, 05004], lr: 0.147828, loss: 2.0748
2022-08-01 19:52:20 - train: epoch 0038, iter [02200, 05004], lr: 0.147770, loss: 1.8324
2022-08-01 19:53:16 - train: epoch 0038, iter [02300, 05004], lr: 0.147712, loss: 2.3045
2022-08-01 19:54:13 - train: epoch 0038, iter [02400, 05004], lr: 0.147654, loss: 2.3217
2022-08-01 19:55:10 - train: epoch 0038, iter [02500, 05004], lr: 0.147596, loss: 1.8378
2022-08-01 19:56:03 - train: epoch 0038, iter [02600, 05004], lr: 0.147538, loss: 2.0269
2022-08-01 19:57:00 - train: epoch 0038, iter [02700, 05004], lr: 0.147480, loss: 2.1651
2022-08-01 19:57:56 - train: epoch 0038, iter [02800, 05004], lr: 0.147421, loss: 2.3177
2022-08-01 19:58:54 - train: epoch 0038, iter [02900, 05004], lr: 0.147363, loss: 1.9713
2022-08-01 19:59:49 - train: epoch 0038, iter [03000, 05004], lr: 0.147305, loss: 1.9285
2022-08-01 20:00:42 - train: epoch 0038, iter [03100, 05004], lr: 0.147247, loss: 1.9891
2022-08-01 20:01:41 - train: epoch 0038, iter [03200, 05004], lr: 0.147189, loss: 1.7616
2022-08-01 20:02:37 - train: epoch 0038, iter [03300, 05004], lr: 0.147130, loss: 1.8897
2022-08-01 20:03:32 - train: epoch 0038, iter [03400, 05004], lr: 0.147072, loss: 1.7956
2022-08-01 20:04:29 - train: epoch 0038, iter [03500, 05004], lr: 0.147014, loss: 2.0689
2022-08-01 20:05:24 - train: epoch 0038, iter [03600, 05004], lr: 0.146955, loss: 2.0447
2022-08-01 20:06:20 - train: epoch 0038, iter [03700, 05004], lr: 0.146897, loss: 1.9924
2022-08-01 20:07:16 - train: epoch 0038, iter [03800, 05004], lr: 0.146839, loss: 2.1374
2022-08-01 20:08:13 - train: epoch 0038, iter [03900, 05004], lr: 0.146780, loss: 1.9925
2022-08-01 20:09:07 - train: epoch 0038, iter [04000, 05004], lr: 0.146722, loss: 2.0928
2022-08-01 20:10:03 - train: epoch 0038, iter [04100, 05004], lr: 0.146663, loss: 1.9279
2022-08-01 20:11:00 - train: epoch 0038, iter [04200, 05004], lr: 0.146605, loss: 1.8692
2022-08-01 20:11:57 - train: epoch 0038, iter [04300, 05004], lr: 0.146546, loss: 2.2497
2022-08-01 20:12:52 - train: epoch 0038, iter [04400, 05004], lr: 0.146488, loss: 1.9592
2022-08-01 20:13:47 - train: epoch 0038, iter [04500, 05004], lr: 0.146429, loss: 1.9771
2022-08-01 20:14:43 - train: epoch 0038, iter [04600, 05004], lr: 0.146371, loss: 1.9746
2022-08-01 20:15:39 - train: epoch 0038, iter [04700, 05004], lr: 0.146312, loss: 2.0065
2022-08-01 20:16:34 - train: epoch 0038, iter [04800, 05004], lr: 0.146254, loss: 2.1302
2022-08-01 20:17:30 - train: epoch 0038, iter [04900, 05004], lr: 0.146195, loss: 1.9500
2022-08-01 20:18:24 - train: epoch 0038, iter [05000, 05004], lr: 0.146136, loss: 1.9230
2022-08-01 20:18:25 - train: epoch 038, train_loss: 2.0217
2022-08-01 20:20:26 - eval: epoch: 038, acc1: 58.046%, acc5: 82.332%, test_loss: 1.7542, per_image_load_time: 4.140ms, per_image_inference_time: 0.522ms
2022-08-01 20:20:26 - until epoch: 038, best_acc1: 58.338%
2022-08-01 20:20:26 - epoch 039 lr: 0.146134
2022-08-01 20:21:30 - train: epoch 0039, iter [00100, 05004], lr: 0.146075, loss: 1.9767
2022-08-01 20:22:26 - train: epoch 0039, iter [00200, 05004], lr: 0.146017, loss: 2.0654
2022-08-01 20:23:23 - train: epoch 0039, iter [00300, 05004], lr: 0.145958, loss: 1.8339
2022-08-01 20:24:17 - train: epoch 0039, iter [00400, 05004], lr: 0.145899, loss: 2.1484
2022-08-01 20:25:12 - train: epoch 0039, iter [00500, 05004], lr: 0.145841, loss: 1.7944
2022-08-01 20:26:07 - train: epoch 0039, iter [00600, 05004], lr: 0.145782, loss: 2.0423
2022-08-01 20:27:02 - train: epoch 0039, iter [00700, 05004], lr: 0.145723, loss: 2.1040
2022-08-01 20:27:58 - train: epoch 0039, iter [00800, 05004], lr: 0.145664, loss: 2.0294
2022-08-01 20:28:53 - train: epoch 0039, iter [00900, 05004], lr: 0.145606, loss: 1.9668
2022-08-01 20:29:46 - train: epoch 0039, iter [01000, 05004], lr: 0.145547, loss: 1.9731
2022-08-01 20:30:42 - train: epoch 0039, iter [01100, 05004], lr: 0.145488, loss: 2.0741
2022-08-01 20:31:38 - train: epoch 0039, iter [01200, 05004], lr: 0.145429, loss: 2.2501
2022-08-01 20:32:32 - train: epoch 0039, iter [01300, 05004], lr: 0.145370, loss: 2.0793
2022-08-01 20:33:29 - train: epoch 0039, iter [01400, 05004], lr: 0.145311, loss: 2.1441
2022-08-01 20:34:21 - train: epoch 0039, iter [01500, 05004], lr: 0.145252, loss: 2.0704
2022-08-01 20:35:18 - train: epoch 0039, iter [01600, 05004], lr: 0.145193, loss: 2.1862
2022-08-01 20:36:13 - train: epoch 0039, iter [01700, 05004], lr: 0.145134, loss: 1.9907
2022-08-01 20:37:09 - train: epoch 0039, iter [01800, 05004], lr: 0.145075, loss: 2.0368
2022-08-01 20:38:05 - train: epoch 0039, iter [01900, 05004], lr: 0.145016, loss: 1.7907
2022-08-01 20:39:00 - train: epoch 0039, iter [02000, 05004], lr: 0.144957, loss: 1.9894
2022-08-01 20:39:57 - train: epoch 0039, iter [02100, 05004], lr: 0.144898, loss: 2.1057
2022-08-01 20:40:52 - train: epoch 0039, iter [02200, 05004], lr: 0.144839, loss: 2.0573
2022-08-01 20:41:46 - train: epoch 0039, iter [02300, 05004], lr: 0.144780, loss: 2.0837
2022-08-01 20:42:42 - train: epoch 0039, iter [02400, 05004], lr: 0.144721, loss: 2.1883
2022-08-01 20:43:36 - train: epoch 0039, iter [02500, 05004], lr: 0.144662, loss: 2.0464
2022-08-01 20:44:34 - train: epoch 0039, iter [02600, 05004], lr: 0.144603, loss: 1.9733
2022-08-01 20:45:27 - train: epoch 0039, iter [02700, 05004], lr: 0.144544, loss: 2.1176
2022-08-01 20:46:23 - train: epoch 0039, iter [02800, 05004], lr: 0.144485, loss: 2.1746
2022-08-01 20:47:20 - train: epoch 0039, iter [02900, 05004], lr: 0.144425, loss: 1.7576
2022-08-01 20:48:15 - train: epoch 0039, iter [03000, 05004], lr: 0.144366, loss: 1.9243
2022-08-01 20:49:11 - train: epoch 0039, iter [03100, 05004], lr: 0.144307, loss: 2.1074
2022-08-01 20:50:06 - train: epoch 0039, iter [03200, 05004], lr: 0.144248, loss: 2.1542
2022-08-01 20:51:02 - train: epoch 0039, iter [03300, 05004], lr: 0.144188, loss: 2.2591
2022-08-01 20:51:56 - train: epoch 0039, iter [03400, 05004], lr: 0.144129, loss: 2.1345
2022-08-01 20:52:52 - train: epoch 0039, iter [03500, 05004], lr: 0.144070, loss: 2.1818
2022-08-01 20:53:50 - train: epoch 0039, iter [03600, 05004], lr: 0.144010, loss: 2.0648
2022-08-01 20:54:44 - train: epoch 0039, iter [03700, 05004], lr: 0.143951, loss: 1.8972
2022-08-01 20:55:41 - train: epoch 0039, iter [03800, 05004], lr: 0.143892, loss: 1.8010
2022-08-01 20:56:37 - train: epoch 0039, iter [03900, 05004], lr: 0.143832, loss: 2.0224
2022-08-01 20:57:34 - train: epoch 0039, iter [04000, 05004], lr: 0.143773, loss: 2.1268
2022-08-01 20:58:28 - train: epoch 0039, iter [04100, 05004], lr: 0.143714, loss: 2.2111
2022-08-01 20:59:24 - train: epoch 0039, iter [04200, 05004], lr: 0.143654, loss: 2.1663
2022-08-01 21:00:21 - train: epoch 0039, iter [04300, 05004], lr: 0.143595, loss: 1.9491
2022-08-01 21:01:15 - train: epoch 0039, iter [04400, 05004], lr: 0.143535, loss: 1.9499
2022-08-01 21:02:13 - train: epoch 0039, iter [04500, 05004], lr: 0.143476, loss: 1.8823
2022-08-01 21:03:06 - train: epoch 0039, iter [04600, 05004], lr: 0.143416, loss: 2.1902
2022-08-01 21:04:03 - train: epoch 0039, iter [04700, 05004], lr: 0.143357, loss: 1.9662
2022-08-01 21:04:58 - train: epoch 0039, iter [04800, 05004], lr: 0.143297, loss: 1.9621
2022-08-01 21:05:54 - train: epoch 0039, iter [04900, 05004], lr: 0.143237, loss: 1.9575
2022-08-01 21:06:48 - train: epoch 0039, iter [05000, 05004], lr: 0.143178, loss: 2.0011
2022-08-01 21:06:49 - train: epoch 039, train_loss: 2.0115
2022-08-01 21:08:55 - eval: epoch: 039, acc1: 57.994%, acc5: 82.368%, test_loss: 1.7532, per_image_load_time: 4.366ms, per_image_inference_time: 0.500ms
2022-08-01 21:08:55 - until epoch: 039, best_acc1: 58.338%
2022-08-01 21:08:55 - epoch 040 lr: 0.143175
2022-08-01 21:10:02 - train: epoch 0040, iter [00100, 05004], lr: 0.143116, loss: 2.2723
2022-08-01 21:11:03 - train: epoch 0040, iter [00200, 05004], lr: 0.143056, loss: 2.0828
2022-08-01 21:12:03 - train: epoch 0040, iter [00300, 05004], lr: 0.142997, loss: 2.2367
2022-08-01 21:13:00 - train: epoch 0040, iter [00400, 05004], lr: 0.142937, loss: 1.6249
2022-08-01 21:13:59 - train: epoch 0040, iter [00500, 05004], lr: 0.142877, loss: 1.7024
2022-08-01 21:14:58 - train: epoch 0040, iter [00600, 05004], lr: 0.142817, loss: 2.1174
2022-08-01 21:15:54 - train: epoch 0040, iter [00700, 05004], lr: 0.142758, loss: 2.0419
2022-08-01 21:16:53 - train: epoch 0040, iter [00800, 05004], lr: 0.142698, loss: 1.8787
2022-08-01 21:17:49 - train: epoch 0040, iter [00900, 05004], lr: 0.142638, loss: 2.0290
2022-08-01 21:18:46 - train: epoch 0040, iter [01000, 05004], lr: 0.142578, loss: 2.0378
2022-08-01 21:19:45 - train: epoch 0040, iter [01100, 05004], lr: 0.142519, loss: 1.8024
2022-08-01 21:20:39 - train: epoch 0040, iter [01200, 05004], lr: 0.142459, loss: 2.1004
2022-08-01 21:21:39 - train: epoch 0040, iter [01300, 05004], lr: 0.142399, loss: 1.8545
2022-08-01 21:22:36 - train: epoch 0040, iter [01400, 05004], lr: 0.142339, loss: 2.1730
2022-08-01 21:23:33 - train: epoch 0040, iter [01500, 05004], lr: 0.142279, loss: 2.1272
2022-08-01 21:24:31 - train: epoch 0040, iter [01600, 05004], lr: 0.142219, loss: 2.2074
2022-08-01 21:25:28 - train: epoch 0040, iter [01700, 05004], lr: 0.142159, loss: 2.0160
2022-08-01 21:26:27 - train: epoch 0040, iter [01800, 05004], lr: 0.142099, loss: 1.9327
2022-08-01 21:27:23 - train: epoch 0040, iter [01900, 05004], lr: 0.142039, loss: 1.9782
2022-08-01 21:28:20 - train: epoch 0040, iter [02000, 05004], lr: 0.141980, loss: 2.0594
2022-08-01 21:29:17 - train: epoch 0040, iter [02100, 05004], lr: 0.141920, loss: 1.7457
2022-08-01 21:30:14 - train: epoch 0040, iter [02200, 05004], lr: 0.141860, loss: 1.9571
2022-08-01 21:31:09 - train: epoch 0040, iter [02300, 05004], lr: 0.141799, loss: 1.9711
2022-08-01 21:32:06 - train: epoch 0040, iter [02400, 05004], lr: 0.141739, loss: 2.1019
2022-08-01 21:33:01 - train: epoch 0040, iter [02500, 05004], lr: 0.141679, loss: 2.0885
2022-08-01 21:33:59 - train: epoch 0040, iter [02600, 05004], lr: 0.141619, loss: 2.0715
2022-08-01 21:34:54 - train: epoch 0040, iter [02700, 05004], lr: 0.141559, loss: 2.0932
2022-08-01 21:35:52 - train: epoch 0040, iter [02800, 05004], lr: 0.141499, loss: 2.0041
2022-08-01 21:36:47 - train: epoch 0040, iter [02900, 05004], lr: 0.141439, loss: 2.2618
2022-08-01 21:37:45 - train: epoch 0040, iter [03000, 05004], lr: 0.141379, loss: 1.9184
2022-08-01 21:38:40 - train: epoch 0040, iter [03100, 05004], lr: 0.141319, loss: 2.2714
2022-08-01 21:39:38 - train: epoch 0040, iter [03200, 05004], lr: 0.141258, loss: 2.2854
2022-08-01 21:40:32 - train: epoch 0040, iter [03300, 05004], lr: 0.141198, loss: 2.0895
2022-08-01 21:41:28 - train: epoch 0040, iter [03400, 05004], lr: 0.141138, loss: 2.0830
2022-08-01 21:42:24 - train: epoch 0040, iter [03500, 05004], lr: 0.141078, loss: 1.9812
2022-08-01 21:43:21 - train: epoch 0040, iter [03600, 05004], lr: 0.141017, loss: 2.0141
2022-08-01 21:44:17 - train: epoch 0040, iter [03700, 05004], lr: 0.140957, loss: 1.8448
2022-08-01 21:45:12 - train: epoch 0040, iter [03800, 05004], lr: 0.140897, loss: 1.8222
2022-08-01 21:46:08 - train: epoch 0040, iter [03900, 05004], lr: 0.140837, loss: 2.1048
2022-08-01 21:47:05 - train: epoch 0040, iter [04000, 05004], lr: 0.140776, loss: 2.0355
2022-08-01 21:48:02 - train: epoch 0040, iter [04100, 05004], lr: 0.140716, loss: 1.9426
2022-08-01 21:48:57 - train: epoch 0040, iter [04200, 05004], lr: 0.140656, loss: 1.8487
2022-08-01 21:49:52 - train: epoch 0040, iter [04300, 05004], lr: 0.140595, loss: 2.1714
2022-08-01 21:50:48 - train: epoch 0040, iter [04400, 05004], lr: 0.140535, loss: 2.0516
2022-08-01 21:51:44 - train: epoch 0040, iter [04500, 05004], lr: 0.140474, loss: 1.7776
2022-08-01 21:52:41 - train: epoch 0040, iter [04600, 05004], lr: 0.140414, loss: 2.0149
2022-08-01 21:53:39 - train: epoch 0040, iter [04700, 05004], lr: 0.140353, loss: 2.1890
2022-08-01 21:54:34 - train: epoch 0040, iter [04800, 05004], lr: 0.140293, loss: 1.6750
2022-08-01 21:55:30 - train: epoch 0040, iter [04900, 05004], lr: 0.140232, loss: 1.9736
2022-08-01 21:56:23 - train: epoch 0040, iter [05000, 05004], lr: 0.140172, loss: 1.9889
2022-08-01 21:56:25 - train: epoch 040, train_loss: 1.9994
2022-08-01 21:58:27 - eval: epoch: 040, acc1: 58.546%, acc5: 82.426%, test_loss: 1.7383, per_image_load_time: 2.656ms, per_image_inference_time: 0.585ms
2022-08-01 21:58:27 - until epoch: 040, best_acc1: 58.546%
2022-08-01 21:58:27 - epoch 041 lr: 0.140169
2022-08-01 21:59:33 - train: epoch 0041, iter [00100, 05004], lr: 0.140109, loss: 2.1685
2022-08-01 22:00:29 - train: epoch 0041, iter [00200, 05004], lr: 0.140048, loss: 2.3725
2022-08-01 22:01:27 - train: epoch 0041, iter [00300, 05004], lr: 0.139988, loss: 2.0955
2022-08-01 22:02:22 - train: epoch 0041, iter [00400, 05004], lr: 0.139927, loss: 2.0280
2022-08-01 22:03:18 - train: epoch 0041, iter [00500, 05004], lr: 0.139867, loss: 1.8075
2022-08-01 22:04:17 - train: epoch 0041, iter [00600, 05004], lr: 0.139806, loss: 2.0920
2022-08-01 22:05:13 - train: epoch 0041, iter [00700, 05004], lr: 0.139745, loss: 1.7766
2022-08-01 22:06:09 - train: epoch 0041, iter [00800, 05004], lr: 0.139685, loss: 1.7278
2022-08-01 22:07:05 - train: epoch 0041, iter [00900, 05004], lr: 0.139624, loss: 1.9050
2022-08-01 22:08:02 - train: epoch 0041, iter [01000, 05004], lr: 0.139563, loss: 2.1016
2022-08-01 22:08:59 - train: epoch 0041, iter [01100, 05004], lr: 0.139503, loss: 1.9906
2022-08-01 22:09:56 - train: epoch 0041, iter [01200, 05004], lr: 0.139442, loss: 1.7447
2022-08-01 22:10:52 - train: epoch 0041, iter [01300, 05004], lr: 0.139381, loss: 1.8005
2022-08-01 22:11:49 - train: epoch 0041, iter [01400, 05004], lr: 0.139321, loss: 2.1764
2022-08-01 22:12:45 - train: epoch 0041, iter [01500, 05004], lr: 0.139260, loss: 2.3480
2022-08-01 22:13:43 - train: epoch 0041, iter [01600, 05004], lr: 0.139199, loss: 2.0278
2022-08-01 22:14:38 - train: epoch 0041, iter [01700, 05004], lr: 0.139138, loss: 1.9671
2022-08-01 22:15:35 - train: epoch 0041, iter [01800, 05004], lr: 0.139077, loss: 1.8269
2022-08-01 22:16:32 - train: epoch 0041, iter [01900, 05004], lr: 0.139017, loss: 2.1691
2022-08-01 22:17:27 - train: epoch 0041, iter [02000, 05004], lr: 0.138956, loss: 1.8231
2022-08-01 22:18:24 - train: epoch 0041, iter [02100, 05004], lr: 0.138895, loss: 1.9094
2022-08-01 22:19:21 - train: epoch 0041, iter [02200, 05004], lr: 0.138834, loss: 1.7401
2022-08-01 22:20:17 - train: epoch 0041, iter [02300, 05004], lr: 0.138773, loss: 1.9211
2022-08-01 22:21:14 - train: epoch 0041, iter [02400, 05004], lr: 0.138712, loss: 2.2594
2022-08-01 22:22:09 - train: epoch 0041, iter [02500, 05004], lr: 0.138651, loss: 1.9020
2022-08-01 22:23:08 - train: epoch 0041, iter [02600, 05004], lr: 0.138590, loss: 2.0606
2022-08-01 22:24:04 - train: epoch 0041, iter [02700, 05004], lr: 0.138529, loss: 2.2118
2022-08-01 22:25:01 - train: epoch 0041, iter [02800, 05004], lr: 0.138468, loss: 2.0253
2022-08-01 22:25:57 - train: epoch 0041, iter [02900, 05004], lr: 0.138407, loss: 2.0795
2022-08-01 22:26:53 - train: epoch 0041, iter [03000, 05004], lr: 0.138346, loss: 2.0251
2022-08-01 22:27:50 - train: epoch 0041, iter [03100, 05004], lr: 0.138285, loss: 2.1105
2022-08-01 22:28:48 - train: epoch 0041, iter [03200, 05004], lr: 0.138224, loss: 1.9289
2022-08-01 22:29:44 - train: epoch 0041, iter [03300, 05004], lr: 0.138163, loss: 2.0192
2022-08-01 22:30:42 - train: epoch 0041, iter [03400, 05004], lr: 0.138102, loss: 1.9212
2022-08-01 22:31:37 - train: epoch 0041, iter [03500, 05004], lr: 0.138041, loss: 2.0389
2022-08-01 22:32:34 - train: epoch 0041, iter [03600, 05004], lr: 0.137980, loss: 2.0647
2022-08-01 22:33:30 - train: epoch 0041, iter [03700, 05004], lr: 0.137919, loss: 2.0778
2022-08-01 22:34:28 - train: epoch 0041, iter [03800, 05004], lr: 0.137857, loss: 1.9736
2022-08-01 22:35:25 - train: epoch 0041, iter [03900, 05004], lr: 0.137796, loss: 1.9939
2022-08-01 22:36:20 - train: epoch 0041, iter [04000, 05004], lr: 0.137735, loss: 1.8412
2022-08-01 22:37:15 - train: epoch 0041, iter [04100, 05004], lr: 0.137674, loss: 2.0735
2022-08-01 22:38:12 - train: epoch 0041, iter [04200, 05004], lr: 0.137613, loss: 1.7933
2022-08-01 22:39:09 - train: epoch 0041, iter [04300, 05004], lr: 0.137551, loss: 1.9451
2022-08-01 22:40:03 - train: epoch 0041, iter [04400, 05004], lr: 0.137490, loss: 1.8096
2022-08-01 22:40:59 - train: epoch 0041, iter [04500, 05004], lr: 0.137429, loss: 1.9642
2022-08-01 22:41:52 - train: epoch 0041, iter [04600, 05004], lr: 0.137368, loss: 2.0569
2022-08-01 22:42:49 - train: epoch 0041, iter [04700, 05004], lr: 0.137306, loss: 2.0675
2022-08-01 22:43:45 - train: epoch 0041, iter [04800, 05004], lr: 0.137245, loss: 2.1168
2022-08-01 22:44:39 - train: epoch 0041, iter [04900, 05004], lr: 0.137184, loss: 2.2072
2022-08-01 22:45:32 - train: epoch 0041, iter [05000, 05004], lr: 0.137122, loss: 1.9931
2022-08-01 22:45:33 - train: epoch 041, train_loss: 1.9881
2022-08-01 22:47:38 - eval: epoch: 041, acc1: 58.932%, acc5: 82.826%, test_loss: 1.7178, per_image_load_time: 4.241ms, per_image_inference_time: 0.511ms
2022-08-01 22:47:38 - until epoch: 041, best_acc1: 58.932%
2022-08-01 22:47:38 - epoch 042 lr: 0.137119
2022-08-01 22:48:43 - train: epoch 0042, iter [00100, 05004], lr: 0.137058, loss: 1.7692
2022-08-01 22:49:40 - train: epoch 0042, iter [00200, 05004], lr: 0.136997, loss: 1.8962
2022-08-01 22:50:37 - train: epoch 0042, iter [00300, 05004], lr: 0.136936, loss: 2.0926
2022-08-01 22:51:34 - train: epoch 0042, iter [00400, 05004], lr: 0.136874, loss: 1.8202
2022-08-01 22:52:29 - train: epoch 0042, iter [00500, 05004], lr: 0.136813, loss: 2.1151
2022-08-01 22:53:28 - train: epoch 0042, iter [00600, 05004], lr: 0.136751, loss: 1.8879
2022-08-01 22:54:22 - train: epoch 0042, iter [00700, 05004], lr: 0.136690, loss: 2.0101
2022-08-01 22:55:19 - train: epoch 0042, iter [00800, 05004], lr: 0.136628, loss: 2.0458
2022-08-01 22:56:16 - train: epoch 0042, iter [00900, 05004], lr: 0.136567, loss: 1.9268
2022-08-01 22:57:11 - train: epoch 0042, iter [01000, 05004], lr: 0.136505, loss: 2.0192
2022-08-01 22:58:08 - train: epoch 0042, iter [01100, 05004], lr: 0.136444, loss: 1.7016
2022-08-01 22:59:05 - train: epoch 0042, iter [01200, 05004], lr: 0.136382, loss: 1.7225
2022-08-01 23:00:00 - train: epoch 0042, iter [01300, 05004], lr: 0.136321, loss: 2.0512
2022-08-01 23:00:58 - train: epoch 0042, iter [01400, 05004], lr: 0.136259, loss: 2.0766
2022-08-01 23:01:55 - train: epoch 0042, iter [01500, 05004], lr: 0.136197, loss: 1.8314
2022-08-01 23:02:51 - train: epoch 0042, iter [01600, 05004], lr: 0.136136, loss: 2.0971
2022-08-01 23:03:47 - train: epoch 0042, iter [01700, 05004], lr: 0.136074, loss: 2.0155
2022-08-01 23:04:43 - train: epoch 0042, iter [01800, 05004], lr: 0.136013, loss: 1.8092
2022-08-01 23:05:41 - train: epoch 0042, iter [01900, 05004], lr: 0.135951, loss: 2.1332
2022-08-01 23:06:37 - train: epoch 0042, iter [02000, 05004], lr: 0.135889, loss: 2.0244
2022-08-01 23:07:32 - train: epoch 0042, iter [02100, 05004], lr: 0.135828, loss: 1.6469
2022-08-01 23:08:27 - train: epoch 0042, iter [02200, 05004], lr: 0.135766, loss: 1.9636
2022-08-01 23:09:23 - train: epoch 0042, iter [02300, 05004], lr: 0.135704, loss: 1.9953
2022-08-01 23:10:21 - train: epoch 0042, iter [02400, 05004], lr: 0.135642, loss: 2.0650
2022-08-01 23:11:15 - train: epoch 0042, iter [02500, 05004], lr: 0.135581, loss: 1.8513
2022-08-01 23:12:12 - train: epoch 0042, iter [02600, 05004], lr: 0.135519, loss: 2.1595
2022-08-01 23:13:09 - train: epoch 0042, iter [02700, 05004], lr: 0.135457, loss: 1.7516
2022-08-01 23:14:05 - train: epoch 0042, iter [02800, 05004], lr: 0.135395, loss: 1.7892
2022-08-01 23:15:00 - train: epoch 0042, iter [02900, 05004], lr: 0.135333, loss: 1.9181
2022-08-01 23:15:55 - train: epoch 0042, iter [03000, 05004], lr: 0.135272, loss: 2.0634
2022-08-01 23:16:53 - train: epoch 0042, iter [03100, 05004], lr: 0.135210, loss: 2.0686
2022-08-01 23:17:49 - train: epoch 0042, iter [03200, 05004], lr: 0.135148, loss: 2.1004
2022-08-01 23:18:45 - train: epoch 0042, iter [03300, 05004], lr: 0.135086, loss: 2.3042
2022-08-01 23:19:41 - train: epoch 0042, iter [03400, 05004], lr: 0.135024, loss: 2.0118
2022-08-01 23:20:37 - train: epoch 0042, iter [03500, 05004], lr: 0.134962, loss: 1.9148
2022-08-01 23:21:32 - train: epoch 0042, iter [03600, 05004], lr: 0.134900, loss: 2.2336
2022-08-01 23:22:29 - train: epoch 0042, iter [03700, 05004], lr: 0.134838, loss: 2.0506
2022-08-01 23:23:25 - train: epoch 0042, iter [03800, 05004], lr: 0.134776, loss: 1.6526
2022-08-01 23:24:20 - train: epoch 0042, iter [03900, 05004], lr: 0.134714, loss: 1.9110
2022-08-01 23:25:18 - train: epoch 0042, iter [04000, 05004], lr: 0.134652, loss: 1.6750
2022-08-01 23:26:14 - train: epoch 0042, iter [04100, 05004], lr: 0.134590, loss: 2.2012
2022-08-01 23:27:11 - train: epoch 0042, iter [04200, 05004], lr: 0.134528, loss: 2.0898
2022-08-01 23:28:08 - train: epoch 0042, iter [04300, 05004], lr: 0.134466, loss: 1.7291
2022-08-01 23:29:03 - train: epoch 0042, iter [04400, 05004], lr: 0.134404, loss: 1.7774
2022-08-01 23:30:00 - train: epoch 0042, iter [04500, 05004], lr: 0.134342, loss: 1.8530
2022-08-01 23:30:56 - train: epoch 0042, iter [04600, 05004], lr: 0.134280, loss: 2.1874
2022-08-01 23:31:52 - train: epoch 0042, iter [04700, 05004], lr: 0.134218, loss: 1.8451
2022-08-01 23:32:49 - train: epoch 0042, iter [04800, 05004], lr: 0.134156, loss: 1.9251
2022-08-01 23:33:43 - train: epoch 0042, iter [04900, 05004], lr: 0.134094, loss: 2.0852
2022-08-01 23:34:37 - train: epoch 0042, iter [05000, 05004], lr: 0.134032, loss: 1.8737
2022-08-01 23:34:38 - train: epoch 042, train_loss: 1.9800
2022-08-01 23:36:42 - eval: epoch: 042, acc1: 59.674%, acc5: 83.324%, test_loss: 1.6877, per_image_load_time: 4.285ms, per_image_inference_time: 0.560ms
2022-08-01 23:36:42 - until epoch: 042, best_acc1: 59.674%
2022-08-01 23:36:42 - epoch 043 lr: 0.134029
2022-08-01 23:37:48 - train: epoch 0043, iter [00100, 05004], lr: 0.133967, loss: 2.1353
2022-08-01 23:38:44 - train: epoch 0043, iter [00200, 05004], lr: 0.133905, loss: 2.2353
2022-08-01 23:39:40 - train: epoch 0043, iter [00300, 05004], lr: 0.133843, loss: 1.6685
2022-08-01 23:40:35 - train: epoch 0043, iter [00400, 05004], lr: 0.133781, loss: 1.7213
2022-08-01 23:41:30 - train: epoch 0043, iter [00500, 05004], lr: 0.133718, loss: 1.8633
2022-08-01 23:42:26 - train: epoch 0043, iter [00600, 05004], lr: 0.133656, loss: 1.8355
2022-08-01 23:43:23 - train: epoch 0043, iter [00700, 05004], lr: 0.133594, loss: 1.9541
2022-08-01 23:44:18 - train: epoch 0043, iter [00800, 05004], lr: 0.133532, loss: 2.0043
2022-08-01 23:45:15 - train: epoch 0043, iter [00900, 05004], lr: 0.133469, loss: 1.8955
2022-08-01 23:46:11 - train: epoch 0043, iter [01000, 05004], lr: 0.133407, loss: 2.0598
2022-08-01 23:47:07 - train: epoch 0043, iter [01100, 05004], lr: 0.133345, loss: 2.0214
2022-08-01 23:48:02 - train: epoch 0043, iter [01200, 05004], lr: 0.133283, loss: 1.7770
2022-08-01 23:48:59 - train: epoch 0043, iter [01300, 05004], lr: 0.133220, loss: 2.0790
2022-08-01 23:49:57 - train: epoch 0043, iter [01400, 05004], lr: 0.133158, loss: 1.8290
2022-08-01 23:50:51 - train: epoch 0043, iter [01500, 05004], lr: 0.133096, loss: 1.8290
2022-08-01 23:51:47 - train: epoch 0043, iter [01600, 05004], lr: 0.133033, loss: 1.8746
2022-08-01 23:52:43 - train: epoch 0043, iter [01700, 05004], lr: 0.132971, loss: 2.0649
2022-08-01 23:53:40 - train: epoch 0043, iter [01800, 05004], lr: 0.132908, loss: 2.0868
2022-08-01 23:54:35 - train: epoch 0043, iter [01900, 05004], lr: 0.132846, loss: 2.1005
2022-08-01 23:55:31 - train: epoch 0043, iter [02000, 05004], lr: 0.132784, loss: 1.8301
2022-08-01 23:56:27 - train: epoch 0043, iter [02100, 05004], lr: 0.132721, loss: 1.8560
2022-08-01 23:57:24 - train: epoch 0043, iter [02200, 05004], lr: 0.132659, loss: 1.9474
2022-08-01 23:58:20 - train: epoch 0043, iter [02300, 05004], lr: 0.132596, loss: 2.1977
2022-08-01 23:59:14 - train: epoch 0043, iter [02400, 05004], lr: 0.132534, loss: 1.7373
