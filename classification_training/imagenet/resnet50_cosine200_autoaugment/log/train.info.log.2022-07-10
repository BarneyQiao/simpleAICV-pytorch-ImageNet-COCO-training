2022-07-10 10:08:58 - eval: epoch: 048, acc1: 56.980%, acc5: 80.918%, test_loss: 1.8369, per_image_load_time: 2.428ms, per_image_inference_time: 0.473ms
2022-07-10 10:08:58 - until epoch: 048, best_acc1: 58.460%
2022-07-10 10:08:58 - epoch 049 lr: 0.088474
2022-07-10 10:09:38 - train: epoch 0049, iter [00100, 05004], lr: 0.088464, loss: 2.6313
2022-07-10 10:10:12 - train: epoch 0049, iter [00200, 05004], lr: 0.088454, loss: 2.4066
2022-07-10 10:10:47 - train: epoch 0049, iter [00300, 05004], lr: 0.088443, loss: 2.4553
2022-07-10 10:11:22 - train: epoch 0049, iter [00400, 05004], lr: 0.088433, loss: 2.6785
2022-07-10 10:11:56 - train: epoch 0049, iter [00500, 05004], lr: 0.088423, loss: 2.4292
2022-07-10 10:12:31 - train: epoch 0049, iter [00600, 05004], lr: 0.088413, loss: 2.3702
2022-07-10 10:13:06 - train: epoch 0049, iter [00700, 05004], lr: 0.088402, loss: 2.5893
2022-07-10 10:13:40 - train: epoch 0049, iter [00800, 05004], lr: 0.088392, loss: 2.7953
2022-07-10 10:14:15 - train: epoch 0049, iter [00900, 05004], lr: 0.088382, loss: 2.5776
2022-07-10 10:14:50 - train: epoch 0049, iter [01000, 05004], lr: 0.088371, loss: 2.5089
2022-07-10 10:15:25 - train: epoch 0049, iter [01100, 05004], lr: 0.088361, loss: 2.4544
2022-07-10 10:16:00 - train: epoch 0049, iter [01200, 05004], lr: 0.088351, loss: 2.2476
2022-07-10 10:16:35 - train: epoch 0049, iter [01300, 05004], lr: 0.088340, loss: 2.6416
2022-07-10 10:17:10 - train: epoch 0049, iter [01400, 05004], lr: 0.088330, loss: 2.5504
2022-07-10 10:17:45 - train: epoch 0049, iter [01500, 05004], lr: 0.088320, loss: 2.4221
2022-07-10 10:18:20 - train: epoch 0049, iter [01600, 05004], lr: 0.088309, loss: 2.6205
2022-07-10 10:18:55 - train: epoch 0049, iter [01700, 05004], lr: 0.088299, loss: 2.3598
2022-07-10 10:19:30 - train: epoch 0049, iter [01800, 05004], lr: 0.088289, loss: 2.4349
2022-07-10 10:20:05 - train: epoch 0049, iter [01900, 05004], lr: 0.088278, loss: 2.6334
2022-07-10 10:20:40 - train: epoch 0049, iter [02000, 05004], lr: 0.088268, loss: 2.3265
2022-07-10 10:21:15 - train: epoch 0049, iter [02100, 05004], lr: 0.088257, loss: 2.4458
2022-07-10 10:21:51 - train: epoch 0049, iter [02200, 05004], lr: 0.088247, loss: 2.3939
2022-07-10 10:22:25 - train: epoch 0049, iter [02300, 05004], lr: 0.088237, loss: 2.3697
2022-07-10 10:22:59 - train: epoch 0049, iter [02400, 05004], lr: 0.088226, loss: 2.7080
2022-07-10 10:23:36 - train: epoch 0049, iter [02500, 05004], lr: 0.088216, loss: 2.6446
2022-07-10 10:24:10 - train: epoch 0049, iter [02600, 05004], lr: 0.088206, loss: 2.6441
2022-07-10 10:24:46 - train: epoch 0049, iter [02700, 05004], lr: 0.088195, loss: 2.4968
2022-07-10 10:25:20 - train: epoch 0049, iter [02800, 05004], lr: 0.088185, loss: 2.6341
2022-07-10 10:25:55 - train: epoch 0049, iter [02900, 05004], lr: 0.088174, loss: 2.6184
2022-07-10 10:26:30 - train: epoch 0049, iter [03000, 05004], lr: 0.088164, loss: 2.6716
2022-07-10 10:27:06 - train: epoch 0049, iter [03100, 05004], lr: 0.088154, loss: 2.4239
2022-07-10 10:27:40 - train: epoch 0049, iter [03200, 05004], lr: 0.088143, loss: 2.6446
2022-07-10 10:28:16 - train: epoch 0049, iter [03300, 05004], lr: 0.088133, loss: 2.4466
2022-07-10 10:28:51 - train: epoch 0049, iter [03400, 05004], lr: 0.088122, loss: 2.7181
2022-07-10 10:29:26 - train: epoch 0049, iter [03500, 05004], lr: 0.088112, loss: 2.5441
2022-07-10 10:30:01 - train: epoch 0049, iter [03600, 05004], lr: 0.088102, loss: 2.6700
2022-07-10 10:30:36 - train: epoch 0049, iter [03700, 05004], lr: 0.088091, loss: 2.7389
2022-07-10 10:31:11 - train: epoch 0049, iter [03800, 05004], lr: 0.088081, loss: 2.8180
2022-07-10 10:31:46 - train: epoch 0049, iter [03900, 05004], lr: 0.088070, loss: 2.6082
2022-07-10 10:32:21 - train: epoch 0049, iter [04000, 05004], lr: 0.088060, loss: 2.4782
2022-07-10 10:32:55 - train: epoch 0049, iter [04100, 05004], lr: 0.088049, loss: 2.4558
2022-07-10 10:33:31 - train: epoch 0049, iter [04200, 05004], lr: 0.088039, loss: 2.6434
2022-07-10 10:34:06 - train: epoch 0049, iter [04300, 05004], lr: 0.088029, loss: 2.6923
2022-07-10 10:34:41 - train: epoch 0049, iter [04400, 05004], lr: 0.088018, loss: 2.5883
2022-07-10 10:35:16 - train: epoch 0049, iter [04500, 05004], lr: 0.088008, loss: 2.2811
2022-07-10 10:35:51 - train: epoch 0049, iter [04600, 05004], lr: 0.087997, loss: 2.6382
2022-07-10 10:36:27 - train: epoch 0049, iter [04700, 05004], lr: 0.087987, loss: 2.6465
2022-07-10 10:37:02 - train: epoch 0049, iter [04800, 05004], lr: 0.087976, loss: 2.4312
2022-07-10 10:37:37 - train: epoch 0049, iter [04900, 05004], lr: 0.087966, loss: 2.5682
2022-07-10 10:38:11 - train: epoch 0049, iter [05000, 05004], lr: 0.087955, loss: 2.2230
2022-07-10 10:38:12 - train: epoch 049, train_loss: 2.5138
2022-07-10 10:39:27 - eval: epoch: 049, acc1: 58.676%, acc5: 82.618%, test_loss: 1.7384, per_image_load_time: 2.041ms, per_image_inference_time: 0.507ms
2022-07-10 10:39:28 - until epoch: 049, best_acc1: 58.676%
2022-07-10 10:39:28 - epoch 050 lr: 0.087955
2022-07-10 10:40:07 - train: epoch 0050, iter [00100, 05004], lr: 0.087944, loss: 2.7521
2022-07-10 10:40:41 - train: epoch 0050, iter [00200, 05004], lr: 0.087934, loss: 2.4929
2022-07-10 10:41:16 - train: epoch 0050, iter [00300, 05004], lr: 0.087923, loss: 2.6134
2022-07-10 10:41:49 - train: epoch 0050, iter [00400, 05004], lr: 0.087913, loss: 2.4298
2022-07-10 10:42:24 - train: epoch 0050, iter [00500, 05004], lr: 0.087902, loss: 2.4911
2022-07-10 10:42:58 - train: epoch 0050, iter [00600, 05004], lr: 0.087892, loss: 2.7362
2022-07-10 10:43:34 - train: epoch 0050, iter [00700, 05004], lr: 0.087881, loss: 2.3717
2022-07-10 10:44:07 - train: epoch 0050, iter [00800, 05004], lr: 0.087871, loss: 2.3340
2022-07-10 10:44:43 - train: epoch 0050, iter [00900, 05004], lr: 0.087860, loss: 2.4287
2022-07-10 10:45:17 - train: epoch 0050, iter [01000, 05004], lr: 0.087850, loss: 2.7017
2022-07-10 10:45:51 - train: epoch 0050, iter [01100, 05004], lr: 0.087839, loss: 2.7667
2022-07-10 10:46:26 - train: epoch 0050, iter [01200, 05004], lr: 0.087829, loss: 2.2761
2022-07-10 10:47:01 - train: epoch 0050, iter [01300, 05004], lr: 0.087818, loss: 2.2520
2022-07-10 10:47:36 - train: epoch 0050, iter [01400, 05004], lr: 0.087808, loss: 2.6312
2022-07-10 10:48:10 - train: epoch 0050, iter [01500, 05004], lr: 0.087797, loss: 2.5421
2022-07-10 10:48:45 - train: epoch 0050, iter [01600, 05004], lr: 0.087787, loss: 2.4067
2022-07-10 10:49:19 - train: epoch 0050, iter [01700, 05004], lr: 0.087776, loss: 2.5685
2022-07-10 10:49:54 - train: epoch 0050, iter [01800, 05004], lr: 0.087766, loss: 2.4160
2022-07-10 10:50:29 - train: epoch 0050, iter [01900, 05004], lr: 0.087755, loss: 2.6643
2022-07-10 10:51:03 - train: epoch 0050, iter [02000, 05004], lr: 0.087744, loss: 2.3613
2022-07-10 10:51:37 - train: epoch 0050, iter [02100, 05004], lr: 0.087734, loss: 2.0922
2022-07-10 10:52:12 - train: epoch 0050, iter [02200, 05004], lr: 0.087723, loss: 2.6125
2022-07-10 10:52:47 - train: epoch 0050, iter [02300, 05004], lr: 0.087713, loss: 2.4767
2022-07-10 10:53:21 - train: epoch 0050, iter [02400, 05004], lr: 0.087702, loss: 2.2463
2022-07-10 10:53:55 - train: epoch 0050, iter [02500, 05004], lr: 0.087692, loss: 2.5190
2022-07-10 10:54:31 - train: epoch 0050, iter [02600, 05004], lr: 0.087681, loss: 2.3676
2022-07-10 10:55:05 - train: epoch 0050, iter [02700, 05004], lr: 0.087670, loss: 2.6885
2022-07-10 10:55:40 - train: epoch 0050, iter [02800, 05004], lr: 0.087660, loss: 2.7776
2022-07-10 10:56:15 - train: epoch 0050, iter [02900, 05004], lr: 0.087649, loss: 2.9355
2022-07-10 10:56:50 - train: epoch 0050, iter [03000, 05004], lr: 0.087639, loss: 2.6954
2022-07-10 10:57:26 - train: epoch 0050, iter [03100, 05004], lr: 0.087628, loss: 2.5480
2022-07-10 10:58:01 - train: epoch 0050, iter [03200, 05004], lr: 0.087617, loss: 2.4487
2022-07-10 10:58:36 - train: epoch 0050, iter [03300, 05004], lr: 0.087607, loss: 2.3940
2022-07-10 10:59:10 - train: epoch 0050, iter [03400, 05004], lr: 0.087596, loss: 2.4192
2022-07-10 10:59:45 - train: epoch 0050, iter [03500, 05004], lr: 0.087586, loss: 2.6664
2022-07-10 11:00:19 - train: epoch 0050, iter [03600, 05004], lr: 0.087575, loss: 2.5071
2022-07-10 11:00:54 - train: epoch 0050, iter [03700, 05004], lr: 0.087564, loss: 2.6909
2022-07-10 11:01:29 - train: epoch 0050, iter [03800, 05004], lr: 0.087554, loss: 2.3993
2022-07-10 11:02:03 - train: epoch 0050, iter [03900, 05004], lr: 0.087543, loss: 2.5543
2022-07-10 11:02:38 - train: epoch 0050, iter [04000, 05004], lr: 0.087533, loss: 2.7586
2022-07-10 11:03:12 - train: epoch 0050, iter [04100, 05004], lr: 0.087522, loss: 2.6705
2022-07-10 11:03:47 - train: epoch 0050, iter [04200, 05004], lr: 0.087511, loss: 2.5188
2022-07-10 11:04:22 - train: epoch 0050, iter [04300, 05004], lr: 0.087501, loss: 2.3558
2022-07-10 11:04:56 - train: epoch 0050, iter [04400, 05004], lr: 0.087490, loss: 2.4312
2022-07-10 11:05:30 - train: epoch 0050, iter [04500, 05004], lr: 0.087479, loss: 2.3458
2022-07-10 11:06:05 - train: epoch 0050, iter [04600, 05004], lr: 0.087469, loss: 2.5956
2022-07-10 11:06:39 - train: epoch 0050, iter [04700, 05004], lr: 0.087458, loss: 2.4262
2022-07-10 11:07:14 - train: epoch 0050, iter [04800, 05004], lr: 0.087447, loss: 2.3448
2022-07-10 11:07:48 - train: epoch 0050, iter [04900, 05004], lr: 0.087437, loss: 2.3128
2022-07-10 11:08:22 - train: epoch 0050, iter [05000, 05004], lr: 0.087426, loss: 2.5310
2022-07-10 11:08:23 - train: epoch 050, train_loss: 2.5097
2022-07-10 11:09:39 - eval: epoch: 050, acc1: 57.382%, acc5: 81.524%, test_loss: 1.8067, per_image_load_time: 2.482ms, per_image_inference_time: 0.465ms
2022-07-10 11:09:40 - until epoch: 050, best_acc1: 58.676%
2022-07-10 11:09:40 - epoch 051 lr: 0.087425
2022-07-10 11:10:20 - train: epoch 0051, iter [00100, 05004], lr: 0.087415, loss: 2.7041
2022-07-10 11:10:54 - train: epoch 0051, iter [00200, 05004], lr: 0.087404, loss: 2.5471
2022-07-10 11:11:28 - train: epoch 0051, iter [00300, 05004], lr: 0.087393, loss: 2.5023
2022-07-10 11:12:02 - train: epoch 0051, iter [00400, 05004], lr: 0.087383, loss: 2.2819
2022-07-10 11:12:37 - train: epoch 0051, iter [00500, 05004], lr: 0.087372, loss: 2.5812
2022-07-10 11:13:11 - train: epoch 0051, iter [00600, 05004], lr: 0.087361, loss: 2.3264
2022-07-10 11:13:45 - train: epoch 0051, iter [00700, 05004], lr: 0.087351, loss: 2.4841
2022-07-10 11:14:19 - train: epoch 0051, iter [00800, 05004], lr: 0.087340, loss: 2.7944
2022-07-10 11:14:54 - train: epoch 0051, iter [00900, 05004], lr: 0.087329, loss: 2.3461
2022-07-10 11:15:27 - train: epoch 0051, iter [01000, 05004], lr: 0.087319, loss: 2.7384
2022-07-10 11:16:02 - train: epoch 0051, iter [01100, 05004], lr: 0.087308, loss: 2.6321
2022-07-10 11:16:37 - train: epoch 0051, iter [01200, 05004], lr: 0.087297, loss: 2.3138
2022-07-10 11:17:11 - train: epoch 0051, iter [01300, 05004], lr: 0.087286, loss: 2.2045
2022-07-10 11:17:45 - train: epoch 0051, iter [01400, 05004], lr: 0.087276, loss: 2.3234
2022-07-10 11:18:19 - train: epoch 0051, iter [01500, 05004], lr: 0.087265, loss: 2.5377
2022-07-10 11:18:53 - train: epoch 0051, iter [01600, 05004], lr: 0.087254, loss: 2.2318
2022-07-10 11:19:27 - train: epoch 0051, iter [01700, 05004], lr: 0.087244, loss: 2.5204
2022-07-10 11:20:02 - train: epoch 0051, iter [01800, 05004], lr: 0.087233, loss: 2.3252
2022-07-10 11:20:35 - train: epoch 0051, iter [01900, 05004], lr: 0.087222, loss: 2.3752
2022-07-10 11:21:10 - train: epoch 0051, iter [02000, 05004], lr: 0.087211, loss: 2.6413
2022-07-10 11:21:44 - train: epoch 0051, iter [02100, 05004], lr: 0.087201, loss: 2.4034
2022-07-10 11:22:18 - train: epoch 0051, iter [02200, 05004], lr: 0.087190, loss: 2.5795
2022-07-10 11:22:52 - train: epoch 0051, iter [02300, 05004], lr: 0.087179, loss: 2.7750
2022-07-10 11:23:26 - train: epoch 0051, iter [02400, 05004], lr: 0.087168, loss: 2.7387
2022-07-10 11:24:00 - train: epoch 0051, iter [02500, 05004], lr: 0.087157, loss: 2.3809
2022-07-10 11:24:35 - train: epoch 0051, iter [02600, 05004], lr: 0.087147, loss: 2.4879
2022-07-10 11:25:09 - train: epoch 0051, iter [02700, 05004], lr: 0.087136, loss: 2.6437
2022-07-10 11:25:43 - train: epoch 0051, iter [02800, 05004], lr: 0.087125, loss: 2.4188
2022-07-10 11:26:18 - train: epoch 0051, iter [02900, 05004], lr: 0.087114, loss: 2.4641
2022-07-10 11:26:52 - train: epoch 0051, iter [03000, 05004], lr: 0.087104, loss: 2.2889
2022-07-10 11:27:26 - train: epoch 0051, iter [03100, 05004], lr: 0.087093, loss: 2.5364
2022-07-10 11:28:00 - train: epoch 0051, iter [03200, 05004], lr: 0.087082, loss: 2.4456
2022-07-10 11:28:34 - train: epoch 0051, iter [03300, 05004], lr: 0.087071, loss: 2.7799
2022-07-10 11:29:09 - train: epoch 0051, iter [03400, 05004], lr: 0.087060, loss: 2.6847
2022-07-10 11:29:43 - train: epoch 0051, iter [03500, 05004], lr: 0.087050, loss: 2.2966
2022-07-10 11:30:17 - train: epoch 0051, iter [03600, 05004], lr: 0.087039, loss: 2.4715
2022-07-10 11:30:51 - train: epoch 0051, iter [03700, 05004], lr: 0.087028, loss: 2.6092
2022-07-10 11:31:26 - train: epoch 0051, iter [03800, 05004], lr: 0.087017, loss: 2.6347
2022-07-10 11:32:00 - train: epoch 0051, iter [03900, 05004], lr: 0.087006, loss: 2.5801
2022-07-10 11:32:34 - train: epoch 0051, iter [04000, 05004], lr: 0.086995, loss: 2.5036
2022-07-10 11:33:08 - train: epoch 0051, iter [04100, 05004], lr: 0.086985, loss: 2.7034
2022-07-10 11:33:43 - train: epoch 0051, iter [04200, 05004], lr: 0.086974, loss: 2.6040
2022-07-10 11:34:18 - train: epoch 0051, iter [04300, 05004], lr: 0.086963, loss: 2.3775
2022-07-10 11:34:52 - train: epoch 0051, iter [04400, 05004], lr: 0.086952, loss: 2.5786
2022-07-10 11:35:26 - train: epoch 0051, iter [04500, 05004], lr: 0.086941, loss: 2.3462
2022-07-10 11:36:00 - train: epoch 0051, iter [04600, 05004], lr: 0.086930, loss: 2.4213
2022-07-10 11:36:34 - train: epoch 0051, iter [04700, 05004], lr: 0.086920, loss: 2.7485
2022-07-10 11:37:09 - train: epoch 0051, iter [04800, 05004], lr: 0.086909, loss: 2.7317
2022-07-10 11:37:43 - train: epoch 0051, iter [04900, 05004], lr: 0.086898, loss: 2.4980
2022-07-10 11:38:17 - train: epoch 0051, iter [05000, 05004], lr: 0.086887, loss: 2.2142
2022-07-10 11:38:18 - train: epoch 051, train_loss: 2.5054
2022-07-10 11:39:32 - eval: epoch: 051, acc1: 58.202%, acc5: 82.234%, test_loss: 1.7599, per_image_load_time: 2.096ms, per_image_inference_time: 0.473ms
2022-07-10 11:39:33 - until epoch: 051, best_acc1: 58.676%
2022-07-10 11:39:33 - epoch 052 lr: 0.086886
2022-07-10 11:40:12 - train: epoch 0052, iter [00100, 05004], lr: 0.086876, loss: 2.2006
2022-07-10 11:40:47 - train: epoch 0052, iter [00200, 05004], lr: 0.086865, loss: 2.5749
2022-07-10 11:41:21 - train: epoch 0052, iter [00300, 05004], lr: 0.086854, loss: 2.4600
2022-07-10 11:41:54 - train: epoch 0052, iter [00400, 05004], lr: 0.086843, loss: 2.6348
2022-07-10 11:42:29 - train: epoch 0052, iter [00500, 05004], lr: 0.086832, loss: 2.5084
2022-07-10 11:43:03 - train: epoch 0052, iter [00600, 05004], lr: 0.086821, loss: 2.3306
2022-07-10 11:43:37 - train: epoch 0052, iter [00700, 05004], lr: 0.086810, loss: 2.5685
2022-07-10 11:44:11 - train: epoch 0052, iter [00800, 05004], lr: 0.086799, loss: 2.8104
2022-07-10 11:44:46 - train: epoch 0052, iter [00900, 05004], lr: 0.086789, loss: 2.4515
2022-07-10 11:45:20 - train: epoch 0052, iter [01000, 05004], lr: 0.086778, loss: 2.8204
2022-07-10 11:45:54 - train: epoch 0052, iter [01100, 05004], lr: 0.086767, loss: 2.4584
2022-07-10 11:46:29 - train: epoch 0052, iter [01200, 05004], lr: 0.086756, loss: 2.2906
2022-07-10 11:47:03 - train: epoch 0052, iter [01300, 05004], lr: 0.086745, loss: 2.3758
2022-07-10 11:47:37 - train: epoch 0052, iter [01400, 05004], lr: 0.086734, loss: 2.8117
2022-07-10 11:48:11 - train: epoch 0052, iter [01500, 05004], lr: 0.086723, loss: 2.3566
2022-07-10 11:48:46 - train: epoch 0052, iter [01600, 05004], lr: 0.086712, loss: 2.4704
2022-07-10 11:49:20 - train: epoch 0052, iter [01700, 05004], lr: 0.086701, loss: 2.3071
2022-07-10 11:49:54 - train: epoch 0052, iter [01800, 05004], lr: 0.086690, loss: 2.4135
2022-07-10 11:50:28 - train: epoch 0052, iter [01900, 05004], lr: 0.086679, loss: 2.5642
2022-07-10 11:51:03 - train: epoch 0052, iter [02000, 05004], lr: 0.086668, loss: 2.8783
2022-07-10 11:51:37 - train: epoch 0052, iter [02100, 05004], lr: 0.086657, loss: 2.3893
2022-07-10 11:52:11 - train: epoch 0052, iter [02200, 05004], lr: 0.086647, loss: 2.4872
2022-07-10 11:52:45 - train: epoch 0052, iter [02300, 05004], lr: 0.086636, loss: 2.5450
2022-07-10 11:53:19 - train: epoch 0052, iter [02400, 05004], lr: 0.086625, loss: 2.3173
2022-07-10 11:53:53 - train: epoch 0052, iter [02500, 05004], lr: 0.086614, loss: 2.2792
2022-07-10 11:54:28 - train: epoch 0052, iter [02600, 05004], lr: 0.086603, loss: 2.0671
2022-07-10 11:55:01 - train: epoch 0052, iter [02700, 05004], lr: 0.086592, loss: 2.1966
2022-07-10 11:55:36 - train: epoch 0052, iter [02800, 05004], lr: 0.086581, loss: 2.7331
2022-07-10 11:56:10 - train: epoch 0052, iter [02900, 05004], lr: 0.086570, loss: 2.1737
2022-07-10 11:56:44 - train: epoch 0052, iter [03000, 05004], lr: 0.086559, loss: 2.5033
2022-07-10 11:57:19 - train: epoch 0052, iter [03100, 05004], lr: 0.086548, loss: 2.7841
2022-07-10 11:57:53 - train: epoch 0052, iter [03200, 05004], lr: 0.086537, loss: 2.5528
2022-07-10 11:58:27 - train: epoch 0052, iter [03300, 05004], lr: 0.086526, loss: 2.4091
2022-07-10 11:59:01 - train: epoch 0052, iter [03400, 05004], lr: 0.086515, loss: 2.5277
2022-07-10 11:59:35 - train: epoch 0052, iter [03500, 05004], lr: 0.086504, loss: 2.5568
2022-07-10 12:00:09 - train: epoch 0052, iter [03600, 05004], lr: 0.086493, loss: 2.5287
2022-07-10 12:00:43 - train: epoch 0052, iter [03700, 05004], lr: 0.086482, loss: 2.6667
2022-07-10 12:01:18 - train: epoch 0052, iter [03800, 05004], lr: 0.086471, loss: 2.4846
2022-07-10 12:01:52 - train: epoch 0052, iter [03900, 05004], lr: 0.086460, loss: 2.3476
2022-07-10 12:02:26 - train: epoch 0052, iter [04000, 05004], lr: 0.086449, loss: 2.6624
2022-07-10 12:03:00 - train: epoch 0052, iter [04100, 05004], lr: 0.086438, loss: 2.1960
2022-07-10 12:03:35 - train: epoch 0052, iter [04200, 05004], lr: 0.086427, loss: 2.2425
2022-07-10 12:04:09 - train: epoch 0052, iter [04300, 05004], lr: 0.086416, loss: 2.5429
2022-07-10 12:04:43 - train: epoch 0052, iter [04400, 05004], lr: 0.086405, loss: 2.5595
2022-07-10 12:05:17 - train: epoch 0052, iter [04500, 05004], lr: 0.086394, loss: 2.4547
2022-07-10 12:05:52 - train: epoch 0052, iter [04600, 05004], lr: 0.086383, loss: 2.5980
2022-07-10 12:06:26 - train: epoch 0052, iter [04700, 05004], lr: 0.086372, loss: 2.3908
2022-07-10 12:07:01 - train: epoch 0052, iter [04800, 05004], lr: 0.086361, loss: 2.2231
2022-07-10 12:07:35 - train: epoch 0052, iter [04900, 05004], lr: 0.086349, loss: 2.2255
2022-07-10 12:08:08 - train: epoch 0052, iter [05000, 05004], lr: 0.086338, loss: 2.3051
2022-07-10 12:08:10 - train: epoch 052, train_loss: 2.5008
2022-07-10 12:09:24 - eval: epoch: 052, acc1: 56.450%, acc5: 80.518%, test_loss: 1.8552, per_image_load_time: 1.783ms, per_image_inference_time: 0.473ms
2022-07-10 12:09:25 - until epoch: 052, best_acc1: 58.676%
2022-07-10 12:09:25 - epoch 053 lr: 0.086338
2022-07-10 12:10:04 - train: epoch 0053, iter [00100, 05004], lr: 0.086327, loss: 2.4818
2022-07-10 12:10:38 - train: epoch 0053, iter [00200, 05004], lr: 0.086316, loss: 2.6564
2022-07-10 12:11:12 - train: epoch 0053, iter [00300, 05004], lr: 0.086305, loss: 2.5258
2022-07-10 12:11:46 - train: epoch 0053, iter [00400, 05004], lr: 0.086294, loss: 2.7724
2022-07-10 12:12:20 - train: epoch 0053, iter [00500, 05004], lr: 0.086283, loss: 2.8426
2022-07-10 12:12:54 - train: epoch 0053, iter [00600, 05004], lr: 0.086272, loss: 2.4800
2022-07-10 12:13:28 - train: epoch 0053, iter [00700, 05004], lr: 0.086260, loss: 2.2162
2022-07-10 12:14:02 - train: epoch 0053, iter [00800, 05004], lr: 0.086249, loss: 2.5286
2022-07-10 12:14:36 - train: epoch 0053, iter [00900, 05004], lr: 0.086238, loss: 2.3510
2022-07-10 12:15:10 - train: epoch 0053, iter [01000, 05004], lr: 0.086227, loss: 2.5197
2022-07-10 12:15:44 - train: epoch 0053, iter [01100, 05004], lr: 0.086216, loss: 2.4364
2022-07-10 12:16:18 - train: epoch 0053, iter [01200, 05004], lr: 0.086205, loss: 2.2437
2022-07-10 12:16:51 - train: epoch 0053, iter [01300, 05004], lr: 0.086194, loss: 2.3549
2022-07-10 12:17:26 - train: epoch 0053, iter [01400, 05004], lr: 0.086183, loss: 2.9226
2022-07-10 12:18:00 - train: epoch 0053, iter [01500, 05004], lr: 0.086172, loss: 2.4729
2022-07-10 12:18:35 - train: epoch 0053, iter [01600, 05004], lr: 0.086161, loss: 2.7401
2022-07-10 12:19:09 - train: epoch 0053, iter [01700, 05004], lr: 0.086149, loss: 2.6617
2022-07-10 12:19:43 - train: epoch 0053, iter [01800, 05004], lr: 0.086138, loss: 2.4808
2022-07-10 12:20:17 - train: epoch 0053, iter [01900, 05004], lr: 0.086127, loss: 2.3029
2022-07-10 12:20:52 - train: epoch 0053, iter [02000, 05004], lr: 0.086116, loss: 2.4182
2022-07-10 12:21:26 - train: epoch 0053, iter [02100, 05004], lr: 0.086105, loss: 2.5497
2022-07-10 12:22:00 - train: epoch 0053, iter [02200, 05004], lr: 0.086094, loss: 2.5847
2022-07-10 12:22:34 - train: epoch 0053, iter [02300, 05004], lr: 0.086083, loss: 2.5866
2022-07-10 12:23:08 - train: epoch 0053, iter [02400, 05004], lr: 0.086071, loss: 2.4674
2022-07-10 12:23:42 - train: epoch 0053, iter [02500, 05004], lr: 0.086060, loss: 2.7798
2022-07-10 12:24:16 - train: epoch 0053, iter [02600, 05004], lr: 0.086049, loss: 2.5131
2022-07-10 12:24:50 - train: epoch 0053, iter [02700, 05004], lr: 0.086038, loss: 2.6172
2022-07-10 12:25:24 - train: epoch 0053, iter [02800, 05004], lr: 0.086027, loss: 2.8293
2022-07-10 12:25:59 - train: epoch 0053, iter [02900, 05004], lr: 0.086016, loss: 2.1623
2022-07-10 12:26:32 - train: epoch 0053, iter [03000, 05004], lr: 0.086005, loss: 2.3317
2022-07-10 12:27:06 - train: epoch 0053, iter [03100, 05004], lr: 0.085993, loss: 2.8097
2022-07-10 12:27:41 - train: epoch 0053, iter [03200, 05004], lr: 0.085982, loss: 2.7381
2022-07-10 12:28:15 - train: epoch 0053, iter [03300, 05004], lr: 0.085971, loss: 2.4221
2022-07-10 12:28:49 - train: epoch 0053, iter [03400, 05004], lr: 0.085960, loss: 2.6113
2022-07-10 12:29:23 - train: epoch 0053, iter [03500, 05004], lr: 0.085949, loss: 2.5549
2022-07-10 12:29:58 - train: epoch 0053, iter [03600, 05004], lr: 0.085937, loss: 2.4333
2022-07-10 12:30:32 - train: epoch 0053, iter [03700, 05004], lr: 0.085926, loss: 2.6787
2022-07-10 12:31:06 - train: epoch 0053, iter [03800, 05004], lr: 0.085915, loss: 2.2095
2022-07-10 12:31:41 - train: epoch 0053, iter [03900, 05004], lr: 0.085904, loss: 2.7415
2022-07-10 12:32:15 - train: epoch 0053, iter [04000, 05004], lr: 0.085893, loss: 2.4445
2022-07-10 12:32:49 - train: epoch 0053, iter [04100, 05004], lr: 0.085881, loss: 2.4684
2022-07-10 12:33:24 - train: epoch 0053, iter [04200, 05004], lr: 0.085870, loss: 2.4096
2022-07-10 12:33:58 - train: epoch 0053, iter [04300, 05004], lr: 0.085859, loss: 2.6854
2022-07-10 12:34:32 - train: epoch 0053, iter [04400, 05004], lr: 0.085848, loss: 2.5619
2022-07-10 12:35:07 - train: epoch 0053, iter [04500, 05004], lr: 0.085837, loss: 2.6232
2022-07-10 12:35:40 - train: epoch 0053, iter [04600, 05004], lr: 0.085825, loss: 2.3079
2022-07-10 12:36:15 - train: epoch 0053, iter [04700, 05004], lr: 0.085814, loss: 2.7707
2022-07-10 12:36:49 - train: epoch 0053, iter [04800, 05004], lr: 0.085803, loss: 2.7687
2022-07-10 12:37:24 - train: epoch 0053, iter [04900, 05004], lr: 0.085792, loss: 2.4582
2022-07-10 12:37:56 - train: epoch 0053, iter [05000, 05004], lr: 0.085780, loss: 2.5191
2022-07-10 12:37:57 - train: epoch 053, train_loss: 2.4971
2022-07-10 12:39:12 - eval: epoch: 053, acc1: 56.040%, acc5: 80.700%, test_loss: 1.8657, per_image_load_time: 2.141ms, per_image_inference_time: 0.487ms
2022-07-10 12:39:13 - until epoch: 053, best_acc1: 58.676%
2022-07-10 12:39:13 - epoch 054 lr: 0.085780
2022-07-10 12:39:52 - train: epoch 0054, iter [00100, 05004], lr: 0.085769, loss: 2.5224
2022-07-10 12:40:27 - train: epoch 0054, iter [00200, 05004], lr: 0.085757, loss: 2.8988
2022-07-10 12:41:01 - train: epoch 0054, iter [00300, 05004], lr: 0.085746, loss: 2.4941
2022-07-10 12:41:35 - train: epoch 0054, iter [00400, 05004], lr: 0.085735, loss: 2.3028
2022-07-10 12:42:10 - train: epoch 0054, iter [00500, 05004], lr: 0.085724, loss: 2.5634
2022-07-10 12:42:43 - train: epoch 0054, iter [00600, 05004], lr: 0.085712, loss: 2.3813
2022-07-10 12:43:17 - train: epoch 0054, iter [00700, 05004], lr: 0.085701, loss: 2.5017
2022-07-10 12:43:51 - train: epoch 0054, iter [00800, 05004], lr: 0.085690, loss: 2.6229
2022-07-10 12:44:27 - train: epoch 0054, iter [00900, 05004], lr: 0.085679, loss: 2.2316
2022-07-10 12:45:00 - train: epoch 0054, iter [01000, 05004], lr: 0.085667, loss: 2.2225
2022-07-10 12:45:35 - train: epoch 0054, iter [01100, 05004], lr: 0.085656, loss: 2.4013
2022-07-10 12:46:09 - train: epoch 0054, iter [01200, 05004], lr: 0.085645, loss: 2.7277
2022-07-10 12:46:43 - train: epoch 0054, iter [01300, 05004], lr: 0.085633, loss: 2.3395
2022-07-10 12:47:17 - train: epoch 0054, iter [01400, 05004], lr: 0.085622, loss: 2.4603
2022-07-10 12:47:52 - train: epoch 0054, iter [01500, 05004], lr: 0.085611, loss: 2.3195
2022-07-10 12:48:26 - train: epoch 0054, iter [01600, 05004], lr: 0.085600, loss: 2.2045
2022-07-10 12:49:00 - train: epoch 0054, iter [01700, 05004], lr: 0.085588, loss: 2.5665
2022-07-10 12:49:34 - train: epoch 0054, iter [01800, 05004], lr: 0.085577, loss: 2.4576
2022-07-10 12:50:08 - train: epoch 0054, iter [01900, 05004], lr: 0.085566, loss: 2.9195
2022-07-10 12:50:42 - train: epoch 0054, iter [02000, 05004], lr: 0.085554, loss: 2.5470
2022-07-10 12:51:17 - train: epoch 0054, iter [02100, 05004], lr: 0.085543, loss: 2.2969
2022-07-10 12:51:51 - train: epoch 0054, iter [02200, 05004], lr: 0.085532, loss: 2.3635
2022-07-10 12:52:25 - train: epoch 0054, iter [02300, 05004], lr: 0.085520, loss: 2.5744
2022-07-10 12:52:59 - train: epoch 0054, iter [02400, 05004], lr: 0.085509, loss: 2.6932
2022-07-10 12:53:34 - train: epoch 0054, iter [02500, 05004], lr: 0.085498, loss: 2.7209
2022-07-10 12:54:07 - train: epoch 0054, iter [02600, 05004], lr: 0.085486, loss: 2.3314
2022-07-10 12:54:42 - train: epoch 0054, iter [02700, 05004], lr: 0.085475, loss: 2.5048
2022-07-10 12:55:15 - train: epoch 0054, iter [02800, 05004], lr: 0.085464, loss: 2.7225
2022-07-10 12:55:51 - train: epoch 0054, iter [02900, 05004], lr: 0.085452, loss: 2.1645
2022-07-10 12:56:25 - train: epoch 0054, iter [03000, 05004], lr: 0.085441, loss: 2.7006
2022-07-10 12:56:59 - train: epoch 0054, iter [03100, 05004], lr: 0.085430, loss: 2.4694
2022-07-10 12:57:33 - train: epoch 0054, iter [03200, 05004], lr: 0.085418, loss: 2.6103
2022-07-10 12:58:08 - train: epoch 0054, iter [03300, 05004], lr: 0.085407, loss: 2.4991
2022-07-10 12:58:42 - train: epoch 0054, iter [03400, 05004], lr: 0.085395, loss: 2.6009
2022-07-10 12:59:16 - train: epoch 0054, iter [03500, 05004], lr: 0.085384, loss: 2.8319
2022-07-10 12:59:50 - train: epoch 0054, iter [03600, 05004], lr: 0.085373, loss: 2.4132
2022-07-10 13:00:24 - train: epoch 0054, iter [03700, 05004], lr: 0.085361, loss: 2.4482
2022-07-10 13:00:59 - train: epoch 0054, iter [03800, 05004], lr: 0.085350, loss: 2.3532
2022-07-10 13:01:33 - train: epoch 0054, iter [03900, 05004], lr: 0.085339, loss: 2.4321
2022-07-10 13:02:07 - train: epoch 0054, iter [04000, 05004], lr: 0.085327, loss: 2.3864
2022-07-10 13:02:42 - train: epoch 0054, iter [04100, 05004], lr: 0.085316, loss: 2.3856
2022-07-10 13:03:16 - train: epoch 0054, iter [04200, 05004], lr: 0.085304, loss: 2.4716
2022-07-10 13:03:50 - train: epoch 0054, iter [04300, 05004], lr: 0.085293, loss: 2.5999
2022-07-10 13:04:24 - train: epoch 0054, iter [04400, 05004], lr: 0.085282, loss: 2.3924
2022-07-10 13:04:59 - train: epoch 0054, iter [04500, 05004], lr: 0.085270, loss: 2.3631
2022-07-10 13:05:33 - train: epoch 0054, iter [04600, 05004], lr: 0.085259, loss: 2.5013
2022-07-10 13:06:07 - train: epoch 0054, iter [04700, 05004], lr: 0.085247, loss: 2.7688
2022-07-10 13:06:42 - train: epoch 0054, iter [04800, 05004], lr: 0.085236, loss: 2.7130
2022-07-10 13:07:17 - train: epoch 0054, iter [04900, 05004], lr: 0.085225, loss: 2.3749
2022-07-10 13:07:50 - train: epoch 0054, iter [05000, 05004], lr: 0.085213, loss: 2.6323
2022-07-10 13:07:51 - train: epoch 054, train_loss: 2.4950
2022-07-10 13:09:06 - eval: epoch: 054, acc1: 58.282%, acc5: 82.520%, test_loss: 1.7552, per_image_load_time: 2.256ms, per_image_inference_time: 0.491ms
2022-07-10 13:09:06 - until epoch: 054, best_acc1: 58.676%
2022-07-10 13:09:06 - epoch 055 lr: 0.085213
2022-07-10 13:09:46 - train: epoch 0055, iter [00100, 05004], lr: 0.085201, loss: 2.3252
2022-07-10 13:10:19 - train: epoch 0055, iter [00200, 05004], lr: 0.085190, loss: 2.5586
2022-07-10 13:10:53 - train: epoch 0055, iter [00300, 05004], lr: 0.085178, loss: 2.3143
2022-07-10 13:11:27 - train: epoch 0055, iter [00400, 05004], lr: 0.085167, loss: 2.3934
2022-07-10 13:12:02 - train: epoch 0055, iter [00500, 05004], lr: 0.085155, loss: 2.1041
2022-07-10 13:12:36 - train: epoch 0055, iter [00600, 05004], lr: 0.085144, loss: 2.4549
2022-07-10 13:13:09 - train: epoch 0055, iter [00700, 05004], lr: 0.085133, loss: 2.5689
2022-07-10 13:13:44 - train: epoch 0055, iter [00800, 05004], lr: 0.085121, loss: 2.4222
2022-07-10 13:14:17 - train: epoch 0055, iter [00900, 05004], lr: 0.085110, loss: 2.4600
2022-07-10 13:14:52 - train: epoch 0055, iter [01000, 05004], lr: 0.085098, loss: 2.5495
2022-07-10 13:15:26 - train: epoch 0055, iter [01100, 05004], lr: 0.085087, loss: 2.5983
2022-07-10 13:16:00 - train: epoch 0055, iter [01200, 05004], lr: 0.085075, loss: 2.4471
2022-07-10 13:16:35 - train: epoch 0055, iter [01300, 05004], lr: 0.085064, loss: 2.5464
2022-07-10 13:17:09 - train: epoch 0055, iter [01400, 05004], lr: 0.085052, loss: 2.5178
2022-07-10 13:17:43 - train: epoch 0055, iter [01500, 05004], lr: 0.085041, loss: 2.5615
2022-07-10 13:18:17 - train: epoch 0055, iter [01600, 05004], lr: 0.085029, loss: 2.5657
2022-07-10 13:18:51 - train: epoch 0055, iter [01700, 05004], lr: 0.085018, loss: 2.4608
2022-07-10 13:19:25 - train: epoch 0055, iter [01800, 05004], lr: 0.085006, loss: 2.7162
2022-07-10 13:19:58 - train: epoch 0055, iter [01900, 05004], lr: 0.084995, loss: 2.3738
2022-07-10 13:20:33 - train: epoch 0055, iter [02000, 05004], lr: 0.084983, loss: 2.5191
2022-07-10 13:21:06 - train: epoch 0055, iter [02100, 05004], lr: 0.084972, loss: 2.0807
2022-07-10 13:21:41 - train: epoch 0055, iter [02200, 05004], lr: 0.084960, loss: 2.6332
2022-07-10 13:22:14 - train: epoch 0055, iter [02300, 05004], lr: 0.084949, loss: 2.6076
2022-07-10 13:22:47 - train: epoch 0055, iter [02400, 05004], lr: 0.084937, loss: 2.3186
2022-07-10 13:23:22 - train: epoch 0055, iter [02500, 05004], lr: 0.084926, loss: 2.4874
2022-07-10 13:23:56 - train: epoch 0055, iter [02600, 05004], lr: 0.084914, loss: 2.1211
2022-07-10 13:24:30 - train: epoch 0055, iter [02700, 05004], lr: 0.084903, loss: 2.1922
2022-07-10 13:25:04 - train: epoch 0055, iter [02800, 05004], lr: 0.084891, loss: 2.2750
2022-07-10 13:25:38 - train: epoch 0055, iter [02900, 05004], lr: 0.084880, loss: 2.5177
2022-07-10 13:26:11 - train: epoch 0055, iter [03000, 05004], lr: 0.084868, loss: 2.3582
2022-07-10 13:26:45 - train: epoch 0055, iter [03100, 05004], lr: 0.084857, loss: 2.5370
2022-07-10 13:27:19 - train: epoch 0055, iter [03200, 05004], lr: 0.084845, loss: 2.4173
2022-07-10 13:27:53 - train: epoch 0055, iter [03300, 05004], lr: 0.084834, loss: 2.3556
2022-07-10 13:28:27 - train: epoch 0055, iter [03400, 05004], lr: 0.084822, loss: 2.4118
2022-07-10 13:29:02 - train: epoch 0055, iter [03500, 05004], lr: 0.084810, loss: 2.3610
2022-07-10 13:29:36 - train: epoch 0055, iter [03600, 05004], lr: 0.084799, loss: 2.5116
2022-07-10 13:30:10 - train: epoch 0055, iter [03700, 05004], lr: 0.084787, loss: 2.4657
2022-07-10 13:30:45 - train: epoch 0055, iter [03800, 05004], lr: 0.084776, loss: 2.2868
2022-07-10 13:31:19 - train: epoch 0055, iter [03900, 05004], lr: 0.084764, loss: 2.7773
2022-07-10 13:31:53 - train: epoch 0055, iter [04000, 05004], lr: 0.084753, loss: 2.4544
2022-07-10 13:32:28 - train: epoch 0055, iter [04100, 05004], lr: 0.084741, loss: 2.4531
2022-07-10 13:33:01 - train: epoch 0055, iter [04200, 05004], lr: 0.084729, loss: 2.5596
2022-07-10 13:33:36 - train: epoch 0055, iter [04300, 05004], lr: 0.084718, loss: 2.5619
2022-07-10 13:34:09 - train: epoch 0055, iter [04400, 05004], lr: 0.084706, loss: 2.5582
2022-07-10 13:34:44 - train: epoch 0055, iter [04500, 05004], lr: 0.084695, loss: 2.4596
2022-07-10 13:35:18 - train: epoch 0055, iter [04600, 05004], lr: 0.084683, loss: 2.4888
2022-07-10 13:35:52 - train: epoch 0055, iter [04700, 05004], lr: 0.084671, loss: 2.4383
2022-07-10 13:36:26 - train: epoch 0055, iter [04800, 05004], lr: 0.084660, loss: 2.4082
2022-07-10 13:37:01 - train: epoch 0055, iter [04900, 05004], lr: 0.084648, loss: 2.4351
2022-07-10 13:37:34 - train: epoch 0055, iter [05000, 05004], lr: 0.084637, loss: 2.5321
2022-07-10 13:37:35 - train: epoch 055, train_loss: 2.4838
2022-07-10 13:38:51 - eval: epoch: 055, acc1: 58.424%, acc5: 82.640%, test_loss: 1.7445, per_image_load_time: 2.057ms, per_image_inference_time: 0.460ms
2022-07-10 13:38:51 - until epoch: 055, best_acc1: 58.676%
2022-07-10 13:38:51 - epoch 056 lr: 0.084636
2022-07-10 13:39:31 - train: epoch 0056, iter [00100, 05004], lr: 0.084625, loss: 2.5476
2022-07-10 13:40:06 - train: epoch 0056, iter [00200, 05004], lr: 0.084613, loss: 2.5693
2022-07-10 13:40:40 - train: epoch 0056, iter [00300, 05004], lr: 0.084601, loss: 2.3774
2022-07-10 13:41:14 - train: epoch 0056, iter [00400, 05004], lr: 0.084590, loss: 2.3669
2022-07-10 13:41:49 - train: epoch 0056, iter [00500, 05004], lr: 0.084578, loss: 2.2489
2022-07-10 13:42:23 - train: epoch 0056, iter [00600, 05004], lr: 0.084566, loss: 2.4298
2022-07-10 13:42:58 - train: epoch 0056, iter [00700, 05004], lr: 0.084555, loss: 2.4791
2022-07-10 13:43:32 - train: epoch 0056, iter [00800, 05004], lr: 0.084543, loss: 2.5503
2022-07-10 13:44:07 - train: epoch 0056, iter [00900, 05004], lr: 0.084532, loss: 2.2532
2022-07-10 13:44:41 - train: epoch 0056, iter [01000, 05004], lr: 0.084520, loss: 2.3916
2022-07-10 13:45:16 - train: epoch 0056, iter [01100, 05004], lr: 0.084508, loss: 2.5453
2022-07-10 13:45:50 - train: epoch 0056, iter [01200, 05004], lr: 0.084497, loss: 2.4419
2022-07-10 13:46:24 - train: epoch 0056, iter [01300, 05004], lr: 0.084485, loss: 2.4742
2022-07-10 13:46:58 - train: epoch 0056, iter [01400, 05004], lr: 0.084473, loss: 2.4892
2022-07-10 13:47:33 - train: epoch 0056, iter [01500, 05004], lr: 0.084462, loss: 2.9055
2022-07-10 13:48:07 - train: epoch 0056, iter [01600, 05004], lr: 0.084450, loss: 2.3061
2022-07-10 13:48:42 - train: epoch 0056, iter [01700, 05004], lr: 0.084438, loss: 2.5369
2022-07-10 13:49:16 - train: epoch 0056, iter [01800, 05004], lr: 0.084427, loss: 2.6494
2022-07-10 13:49:51 - train: epoch 0056, iter [01900, 05004], lr: 0.084415, loss: 2.4870
2022-07-10 13:50:25 - train: epoch 0056, iter [02000, 05004], lr: 0.084403, loss: 2.6676
2022-07-10 13:51:00 - train: epoch 0056, iter [02100, 05004], lr: 0.084392, loss: 2.5296
2022-07-10 13:51:33 - train: epoch 0056, iter [02200, 05004], lr: 0.084380, loss: 2.5314
2022-07-10 13:52:08 - train: epoch 0056, iter [02300, 05004], lr: 0.084368, loss: 2.4820
2022-07-10 13:52:42 - train: epoch 0056, iter [02400, 05004], lr: 0.084357, loss: 2.4188
2022-07-10 13:53:17 - train: epoch 0056, iter [02500, 05004], lr: 0.084345, loss: 2.7701
2022-07-10 13:53:51 - train: epoch 0056, iter [02600, 05004], lr: 0.084333, loss: 2.6336
2022-07-10 13:54:26 - train: epoch 0056, iter [02700, 05004], lr: 0.084321, loss: 2.6298
2022-07-10 13:55:00 - train: epoch 0056, iter [02800, 05004], lr: 0.084310, loss: 2.2893
2022-07-10 13:55:35 - train: epoch 0056, iter [02900, 05004], lr: 0.084298, loss: 2.4743
2022-07-10 13:56:09 - train: epoch 0056, iter [03000, 05004], lr: 0.084286, loss: 2.7880
2022-07-10 13:56:44 - train: epoch 0056, iter [03100, 05004], lr: 0.084275, loss: 2.3237
2022-07-10 13:57:18 - train: epoch 0056, iter [03200, 05004], lr: 0.084263, loss: 2.3827
2022-07-10 13:57:53 - train: epoch 0056, iter [03300, 05004], lr: 0.084251, loss: 2.7905
2022-07-10 13:58:27 - train: epoch 0056, iter [03400, 05004], lr: 0.084239, loss: 2.5469
2022-07-10 13:59:01 - train: epoch 0056, iter [03500, 05004], lr: 0.084228, loss: 2.4410
2022-07-10 13:59:35 - train: epoch 0056, iter [03600, 05004], lr: 0.084216, loss: 2.1290
2022-07-10 14:00:10 - train: epoch 0056, iter [03700, 05004], lr: 0.084204, loss: 2.4113
2022-07-10 14:00:45 - train: epoch 0056, iter [03800, 05004], lr: 0.084192, loss: 2.1000
2022-07-10 14:01:19 - train: epoch 0056, iter [03900, 05004], lr: 0.084181, loss: 2.8430
2022-07-10 14:01:54 - train: epoch 0056, iter [04000, 05004], lr: 0.084169, loss: 2.6591
2022-07-10 14:02:29 - train: epoch 0056, iter [04100, 05004], lr: 0.084157, loss: 2.6162
2022-07-10 14:03:03 - train: epoch 0056, iter [04200, 05004], lr: 0.084145, loss: 2.4586
2022-07-10 14:03:38 - train: epoch 0056, iter [04300, 05004], lr: 0.084134, loss: 2.5818
2022-07-10 14:04:11 - train: epoch 0056, iter [04400, 05004], lr: 0.084122, loss: 2.4977
2022-07-10 14:04:45 - train: epoch 0056, iter [04500, 05004], lr: 0.084110, loss: 2.6388
2022-07-10 14:05:20 - train: epoch 0056, iter [04600, 05004], lr: 0.084098, loss: 2.6045
2022-07-10 14:05:55 - train: epoch 0056, iter [04700, 05004], lr: 0.084087, loss: 2.4787
2022-07-10 14:06:29 - train: epoch 0056, iter [04800, 05004], lr: 0.084075, loss: 2.7448
2022-07-10 14:07:04 - train: epoch 0056, iter [04900, 05004], lr: 0.084063, loss: 2.6109
2022-07-10 14:07:37 - train: epoch 0056, iter [05000, 05004], lr: 0.084051, loss: 2.7080
2022-07-10 14:07:38 - train: epoch 056, train_loss: 2.4759
2022-07-10 14:08:53 - eval: epoch: 056, acc1: 56.348%, acc5: 80.862%, test_loss: 1.8616, per_image_load_time: 1.649ms, per_image_inference_time: 0.460ms
2022-07-10 14:08:54 - until epoch: 056, best_acc1: 58.676%
2022-07-10 14:08:54 - epoch 057 lr: 0.084051
2022-07-10 14:09:33 - train: epoch 0057, iter [00100, 05004], lr: 0.084039, loss: 2.6295
2022-07-10 14:10:08 - train: epoch 0057, iter [00200, 05004], lr: 0.084027, loss: 2.4757
2022-07-10 14:10:43 - train: epoch 0057, iter [00300, 05004], lr: 0.084015, loss: 2.3061
2022-07-10 14:11:17 - train: epoch 0057, iter [00400, 05004], lr: 0.084004, loss: 2.3916
2022-07-10 14:11:52 - train: epoch 0057, iter [00500, 05004], lr: 0.083992, loss: 2.3839
2022-07-10 14:12:25 - train: epoch 0057, iter [00600, 05004], lr: 0.083980, loss: 2.5669
2022-07-10 14:13:00 - train: epoch 0057, iter [00700, 05004], lr: 0.083968, loss: 2.1696
2022-07-10 14:13:35 - train: epoch 0057, iter [00800, 05004], lr: 0.083956, loss: 2.3928
2022-07-10 14:14:09 - train: epoch 0057, iter [00900, 05004], lr: 0.083945, loss: 2.3960
2022-07-10 14:14:44 - train: epoch 0057, iter [01000, 05004], lr: 0.083933, loss: 2.3809
2022-07-10 14:15:18 - train: epoch 0057, iter [01100, 05004], lr: 0.083921, loss: 2.4707
2022-07-10 14:15:52 - train: epoch 0057, iter [01200, 05004], lr: 0.083909, loss: 2.2973
2022-07-10 14:16:26 - train: epoch 0057, iter [01300, 05004], lr: 0.083897, loss: 2.5339
2022-07-10 14:17:01 - train: epoch 0057, iter [01400, 05004], lr: 0.083885, loss: 2.5003
2022-07-10 14:17:36 - train: epoch 0057, iter [01500, 05004], lr: 0.083874, loss: 2.6502
2022-07-10 14:18:10 - train: epoch 0057, iter [01600, 05004], lr: 0.083862, loss: 2.6969
2022-07-10 14:18:45 - train: epoch 0057, iter [01700, 05004], lr: 0.083850, loss: 2.6872
2022-07-10 14:19:19 - train: epoch 0057, iter [01800, 05004], lr: 0.083838, loss: 2.5001
2022-07-10 14:19:54 - train: epoch 0057, iter [01900, 05004], lr: 0.083826, loss: 2.4762
2022-07-10 14:20:28 - train: epoch 0057, iter [02000, 05004], lr: 0.083814, loss: 2.4560
2022-07-10 14:21:02 - train: epoch 0057, iter [02100, 05004], lr: 0.083802, loss: 2.7108
2022-07-10 14:21:36 - train: epoch 0057, iter [02200, 05004], lr: 0.083791, loss: 2.7398
2022-07-10 14:22:11 - train: epoch 0057, iter [02300, 05004], lr: 0.083779, loss: 2.4735
2022-07-10 14:22:45 - train: epoch 0057, iter [02400, 05004], lr: 0.083767, loss: 2.3507
2022-07-10 14:23:19 - train: epoch 0057, iter [02500, 05004], lr: 0.083755, loss: 2.5736
2022-07-10 14:23:55 - train: epoch 0057, iter [02600, 05004], lr: 0.083743, loss: 2.3433
2022-07-10 14:24:29 - train: epoch 0057, iter [02700, 05004], lr: 0.083731, loss: 2.1500
2022-07-10 14:25:03 - train: epoch 0057, iter [02800, 05004], lr: 0.083719, loss: 1.9858
2022-07-10 14:25:38 - train: epoch 0057, iter [02900, 05004], lr: 0.083707, loss: 2.5305
2022-07-10 14:26:12 - train: epoch 0057, iter [03000, 05004], lr: 0.083696, loss: 2.5778
2022-07-10 14:26:46 - train: epoch 0057, iter [03100, 05004], lr: 0.083684, loss: 2.6120
2022-07-10 14:27:20 - train: epoch 0057, iter [03200, 05004], lr: 0.083672, loss: 2.6290
2022-07-10 14:27:54 - train: epoch 0057, iter [03300, 05004], lr: 0.083660, loss: 2.5399
2022-07-10 14:28:28 - train: epoch 0057, iter [03400, 05004], lr: 0.083648, loss: 2.5519
2022-07-10 14:29:03 - train: epoch 0057, iter [03500, 05004], lr: 0.083636, loss: 2.4643
2022-07-10 14:29:38 - train: epoch 0057, iter [03600, 05004], lr: 0.083624, loss: 2.2062
2022-07-10 14:30:13 - train: epoch 0057, iter [03700, 05004], lr: 0.083612, loss: 2.2513
2022-07-10 14:30:47 - train: epoch 0057, iter [03800, 05004], lr: 0.083600, loss: 2.4443
2022-07-10 14:31:21 - train: epoch 0057, iter [03900, 05004], lr: 0.083588, loss: 2.5772
2022-07-10 14:31:56 - train: epoch 0057, iter [04000, 05004], lr: 0.083576, loss: 2.3367
2022-07-10 14:32:30 - train: epoch 0057, iter [04100, 05004], lr: 0.083565, loss: 2.5256
2022-07-10 14:33:04 - train: epoch 0057, iter [04200, 05004], lr: 0.083553, loss: 2.3826
2022-07-10 14:33:38 - train: epoch 0057, iter [04300, 05004], lr: 0.083541, loss: 2.3785
2022-07-10 14:34:12 - train: epoch 0057, iter [04400, 05004], lr: 0.083529, loss: 2.4564
2022-07-10 14:34:47 - train: epoch 0057, iter [04500, 05004], lr: 0.083517, loss: 2.7418
2022-07-10 14:35:21 - train: epoch 0057, iter [04600, 05004], lr: 0.083505, loss: 2.8374
2022-07-10 14:35:55 - train: epoch 0057, iter [04700, 05004], lr: 0.083493, loss: 2.2746
2022-07-10 14:36:30 - train: epoch 0057, iter [04800, 05004], lr: 0.083481, loss: 2.6401
2022-07-10 14:37:04 - train: epoch 0057, iter [04900, 05004], lr: 0.083469, loss: 2.8052
2022-07-10 14:37:37 - train: epoch 0057, iter [05000, 05004], lr: 0.083457, loss: 2.7047
2022-07-10 14:37:38 - train: epoch 057, train_loss: 2.4721
2022-07-10 14:38:53 - eval: epoch: 057, acc1: 58.046%, acc5: 81.830%, test_loss: 1.7889, per_image_load_time: 2.221ms, per_image_inference_time: 0.468ms
2022-07-10 14:38:54 - until epoch: 057, best_acc1: 58.676%
2022-07-10 14:38:54 - epoch 058 lr: 0.083456
2022-07-10 14:39:33 - train: epoch 0058, iter [00100, 05004], lr: 0.083445, loss: 2.6182
2022-07-10 14:40:08 - train: epoch 0058, iter [00200, 05004], lr: 0.083433, loss: 2.3699
2022-07-10 14:40:42 - train: epoch 0058, iter [00300, 05004], lr: 0.083421, loss: 2.4422
2022-07-10 14:41:16 - train: epoch 0058, iter [00400, 05004], lr: 0.083409, loss: 2.4266
2022-07-10 14:41:51 - train: epoch 0058, iter [00500, 05004], lr: 0.083397, loss: 2.4778
2022-07-10 14:42:26 - train: epoch 0058, iter [00600, 05004], lr: 0.083385, loss: 2.7322
2022-07-10 14:43:00 - train: epoch 0058, iter [00700, 05004], lr: 0.083373, loss: 2.4461
2022-07-10 14:43:34 - train: epoch 0058, iter [00800, 05004], lr: 0.083361, loss: 2.2734
2022-07-10 14:44:08 - train: epoch 0058, iter [00900, 05004], lr: 0.083349, loss: 2.2355
2022-07-10 14:44:44 - train: epoch 0058, iter [01000, 05004], lr: 0.083337, loss: 2.6320
2022-07-10 14:45:18 - train: epoch 0058, iter [01100, 05004], lr: 0.083325, loss: 2.4503
2022-07-10 14:45:54 - train: epoch 0058, iter [01200, 05004], lr: 0.083313, loss: 2.2463
2022-07-10 14:46:28 - train: epoch 0058, iter [01300, 05004], lr: 0.083301, loss: 2.4511
2022-07-10 14:47:03 - train: epoch 0058, iter [01400, 05004], lr: 0.083289, loss: 2.6250
2022-07-10 14:47:37 - train: epoch 0058, iter [01500, 05004], lr: 0.083277, loss: 2.3628
2022-07-10 14:48:12 - train: epoch 0058, iter [01600, 05004], lr: 0.083265, loss: 2.3133
2022-07-10 14:48:47 - train: epoch 0058, iter [01700, 05004], lr: 0.083253, loss: 2.8217
2022-07-10 14:49:22 - train: epoch 0058, iter [01800, 05004], lr: 0.083241, loss: 2.5145
2022-07-10 14:49:56 - train: epoch 0058, iter [01900, 05004], lr: 0.083229, loss: 2.6573
2022-07-10 14:50:31 - train: epoch 0058, iter [02000, 05004], lr: 0.083217, loss: 2.5005
2022-07-10 14:51:06 - train: epoch 0058, iter [02100, 05004], lr: 0.083205, loss: 2.4144
2022-07-10 14:51:39 - train: epoch 0058, iter [02200, 05004], lr: 0.083193, loss: 2.1152
2022-07-10 14:52:14 - train: epoch 0058, iter [02300, 05004], lr: 0.083180, loss: 2.3824
2022-07-10 14:52:49 - train: epoch 0058, iter [02400, 05004], lr: 0.083168, loss: 2.5292
2022-07-10 14:53:23 - train: epoch 0058, iter [02500, 05004], lr: 0.083156, loss: 2.6439
2022-07-10 14:53:58 - train: epoch 0058, iter [02600, 05004], lr: 0.083144, loss: 2.5277
2022-07-10 14:54:33 - train: epoch 0058, iter [02700, 05004], lr: 0.083132, loss: 2.7743
2022-07-10 14:55:07 - train: epoch 0058, iter [02800, 05004], lr: 0.083120, loss: 2.0622
2022-07-10 14:55:42 - train: epoch 0058, iter [02900, 05004], lr: 0.083108, loss: 2.5091
2022-07-10 14:56:18 - train: epoch 0058, iter [03000, 05004], lr: 0.083096, loss: 2.6997
2022-07-10 14:56:53 - train: epoch 0058, iter [03100, 05004], lr: 0.083084, loss: 2.3127
2022-07-10 14:57:27 - train: epoch 0058, iter [03200, 05004], lr: 0.083072, loss: 2.2566
2022-07-10 14:58:03 - train: epoch 0058, iter [03300, 05004], lr: 0.083060, loss: 2.5048
2022-07-10 14:58:37 - train: epoch 0058, iter [03400, 05004], lr: 0.083048, loss: 2.2755
2022-07-10 14:59:13 - train: epoch 0058, iter [03500, 05004], lr: 0.083036, loss: 2.2721
2022-07-10 14:59:47 - train: epoch 0058, iter [03600, 05004], lr: 0.083024, loss: 2.2986
2022-07-10 15:00:22 - train: epoch 0058, iter [03700, 05004], lr: 0.083012, loss: 2.5583
2022-07-10 15:00:57 - train: epoch 0058, iter [03800, 05004], lr: 0.082999, loss: 2.3592
2022-07-10 15:01:32 - train: epoch 0058, iter [03900, 05004], lr: 0.082987, loss: 2.4770
2022-07-10 15:02:06 - train: epoch 0058, iter [04000, 05004], lr: 0.082975, loss: 2.6651
2022-07-10 15:02:42 - train: epoch 0058, iter [04100, 05004], lr: 0.082963, loss: 2.3776
2022-07-10 15:03:17 - train: epoch 0058, iter [04200, 05004], lr: 0.082951, loss: 2.3363
2022-07-10 15:03:52 - train: epoch 0058, iter [04300, 05004], lr: 0.082939, loss: 2.5938
2022-07-10 15:04:27 - train: epoch 0058, iter [04400, 05004], lr: 0.082927, loss: 2.3857
2022-07-10 15:05:02 - train: epoch 0058, iter [04500, 05004], lr: 0.082915, loss: 2.4789
2022-07-10 15:05:37 - train: epoch 0058, iter [04600, 05004], lr: 0.082903, loss: 2.1160
2022-07-10 15:06:11 - train: epoch 0058, iter [04700, 05004], lr: 0.082890, loss: 2.2639
2022-07-10 15:06:46 - train: epoch 0058, iter [04800, 05004], lr: 0.082878, loss: 2.5085
2022-07-10 15:07:22 - train: epoch 0058, iter [04900, 05004], lr: 0.082866, loss: 2.4136
2022-07-10 15:07:55 - train: epoch 0058, iter [05000, 05004], lr: 0.082854, loss: 2.3741
2022-07-10 15:07:56 - train: epoch 058, train_loss: 2.4664
2022-07-10 15:09:13 - eval: epoch: 058, acc1: 58.820%, acc5: 82.896%, test_loss: 1.7194, per_image_load_time: 2.460ms, per_image_inference_time: 0.483ms
2022-07-10 15:09:13 - until epoch: 058, best_acc1: 58.820%
2022-07-10 15:09:13 - epoch 059 lr: 0.082853
2022-07-10 15:09:54 - train: epoch 0059, iter [00100, 05004], lr: 0.082841, loss: 2.6077
2022-07-10 15:10:28 - train: epoch 0059, iter [00200, 05004], lr: 0.082829, loss: 2.2677
2022-07-10 15:11:02 - train: epoch 0059, iter [00300, 05004], lr: 0.082817, loss: 2.4786
2022-07-10 15:11:38 - train: epoch 0059, iter [00400, 05004], lr: 0.082805, loss: 2.7772
2022-07-10 15:12:14 - train: epoch 0059, iter [00500, 05004], lr: 0.082793, loss: 2.7701
2022-07-10 15:12:48 - train: epoch 0059, iter [00600, 05004], lr: 0.082781, loss: 2.3916
2022-07-10 15:13:23 - train: epoch 0059, iter [00700, 05004], lr: 0.082769, loss: 2.2819
2022-07-10 15:13:58 - train: epoch 0059, iter [00800, 05004], lr: 0.082756, loss: 2.5851
2022-07-10 15:14:34 - train: epoch 0059, iter [00900, 05004], lr: 0.082744, loss: 2.3119
2022-07-10 15:15:09 - train: epoch 0059, iter [01000, 05004], lr: 0.082732, loss: 2.5662
2022-07-10 15:15:43 - train: epoch 0059, iter [01100, 05004], lr: 0.082720, loss: 2.6704
2022-07-10 15:16:18 - train: epoch 0059, iter [01200, 05004], lr: 0.082708, loss: 2.2042
2022-07-10 15:16:53 - train: epoch 0059, iter [01300, 05004], lr: 0.082696, loss: 2.7851
2022-07-10 15:17:28 - train: epoch 0059, iter [01400, 05004], lr: 0.082683, loss: 2.6373
2022-07-10 15:18:03 - train: epoch 0059, iter [01500, 05004], lr: 0.082671, loss: 2.7231
2022-07-10 15:18:38 - train: epoch 0059, iter [01600, 05004], lr: 0.082659, loss: 2.2827
2022-07-10 15:19:12 - train: epoch 0059, iter [01700, 05004], lr: 0.082647, loss: 2.6627
2022-07-10 15:19:47 - train: epoch 0059, iter [01800, 05004], lr: 0.082635, loss: 2.5287
2022-07-10 15:20:22 - train: epoch 0059, iter [01900, 05004], lr: 0.082622, loss: 2.5249
2022-07-10 15:20:57 - train: epoch 0059, iter [02000, 05004], lr: 0.082610, loss: 2.5368
2022-07-10 15:21:33 - train: epoch 0059, iter [02100, 05004], lr: 0.082598, loss: 2.7488
2022-07-10 15:22:07 - train: epoch 0059, iter [02200, 05004], lr: 0.082586, loss: 2.5005
2022-07-10 15:22:42 - train: epoch 0059, iter [02300, 05004], lr: 0.082574, loss: 2.3019
2022-07-10 15:23:17 - train: epoch 0059, iter [02400, 05004], lr: 0.082561, loss: 2.4514
2022-07-10 15:23:52 - train: epoch 0059, iter [02500, 05004], lr: 0.082549, loss: 2.7115
2022-07-10 15:24:27 - train: epoch 0059, iter [02600, 05004], lr: 0.082537, loss: 2.5712
2022-07-10 15:25:02 - train: epoch 0059, iter [02700, 05004], lr: 0.082525, loss: 2.4540
2022-07-10 15:25:37 - train: epoch 0059, iter [02800, 05004], lr: 0.082512, loss: 2.6561
2022-07-10 15:26:12 - train: epoch 0059, iter [02900, 05004], lr: 0.082500, loss: 2.5072
2022-07-10 15:26:48 - train: epoch 0059, iter [03000, 05004], lr: 0.082488, loss: 2.7122
2022-07-10 15:27:22 - train: epoch 0059, iter [03100, 05004], lr: 0.082476, loss: 2.3826
2022-07-10 15:27:58 - train: epoch 0059, iter [03200, 05004], lr: 0.082464, loss: 2.1973
2022-07-10 15:28:33 - train: epoch 0059, iter [03300, 05004], lr: 0.082451, loss: 2.7098
2022-07-10 15:29:08 - train: epoch 0059, iter [03400, 05004], lr: 0.082439, loss: 2.8067
2022-07-10 15:29:43 - train: epoch 0059, iter [03500, 05004], lr: 0.082427, loss: 2.4007
2022-07-10 15:30:18 - train: epoch 0059, iter [03600, 05004], lr: 0.082415, loss: 2.4302
2022-07-10 15:30:54 - train: epoch 0059, iter [03700, 05004], lr: 0.082402, loss: 2.2697
2022-07-10 15:31:29 - train: epoch 0059, iter [03800, 05004], lr: 0.082390, loss: 2.6773
2022-07-10 15:32:05 - train: epoch 0059, iter [03900, 05004], lr: 0.082378, loss: 2.4624
2022-07-10 15:32:39 - train: epoch 0059, iter [04000, 05004], lr: 0.082365, loss: 2.6422
2022-07-10 15:33:14 - train: epoch 0059, iter [04100, 05004], lr: 0.082353, loss: 2.3699
2022-07-10 15:33:50 - train: epoch 0059, iter [04200, 05004], lr: 0.082341, loss: 2.5007
2022-07-10 15:34:25 - train: epoch 0059, iter [04300, 05004], lr: 0.082329, loss: 2.5613
2022-07-10 15:35:00 - train: epoch 0059, iter [04400, 05004], lr: 0.082316, loss: 2.6255
2022-07-10 15:35:35 - train: epoch 0059, iter [04500, 05004], lr: 0.082304, loss: 2.3339
2022-07-10 15:36:10 - train: epoch 0059, iter [04600, 05004], lr: 0.082292, loss: 2.4688
2022-07-10 15:36:45 - train: epoch 0059, iter [04700, 05004], lr: 0.082279, loss: 2.2010
2022-07-10 15:37:19 - train: epoch 0059, iter [04800, 05004], lr: 0.082267, loss: 2.2043
2022-07-10 15:37:55 - train: epoch 0059, iter [04900, 05004], lr: 0.082255, loss: 2.6453
2022-07-10 15:38:28 - train: epoch 0059, iter [05000, 05004], lr: 0.082243, loss: 2.5210
2022-07-10 15:38:29 - train: epoch 059, train_loss: 2.4613
2022-07-10 15:39:45 - eval: epoch: 059, acc1: 58.820%, acc5: 82.756%, test_loss: 1.7270, per_image_load_time: 2.412ms, per_image_inference_time: 0.474ms
2022-07-10 15:39:45 - until epoch: 059, best_acc1: 58.820%
2022-07-10 15:39:45 - epoch 060 lr: 0.082242
2022-07-10 15:40:25 - train: epoch 0060, iter [00100, 05004], lr: 0.082230, loss: 2.5987
2022-07-10 15:40:59 - train: epoch 0060, iter [00200, 05004], lr: 0.082217, loss: 2.5443
2022-07-10 15:41:33 - train: epoch 0060, iter [00300, 05004], lr: 0.082205, loss: 2.4057
2022-07-10 15:42:08 - train: epoch 0060, iter [00400, 05004], lr: 0.082193, loss: 2.4053
2022-07-10 15:42:43 - train: epoch 0060, iter [00500, 05004], lr: 0.082181, loss: 2.5567
2022-07-10 15:43:17 - train: epoch 0060, iter [00600, 05004], lr: 0.082168, loss: 2.4809
2022-07-10 15:43:51 - train: epoch 0060, iter [00700, 05004], lr: 0.082156, loss: 2.4925
2022-07-10 15:44:26 - train: epoch 0060, iter [00800, 05004], lr: 0.082144, loss: 2.6747
2022-07-10 15:45:01 - train: epoch 0060, iter [00900, 05004], lr: 0.082131, loss: 2.5063
2022-07-10 15:45:34 - train: epoch 0060, iter [01000, 05004], lr: 0.082119, loss: 2.2210
2022-07-10 15:46:09 - train: epoch 0060, iter [01100, 05004], lr: 0.082107, loss: 2.1306
2022-07-10 15:46:43 - train: epoch 0060, iter [01200, 05004], lr: 0.082094, loss: 2.4427
2022-07-10 15:47:18 - train: epoch 0060, iter [01300, 05004], lr: 0.082082, loss: 2.4057
2022-07-10 15:47:51 - train: epoch 0060, iter [01400, 05004], lr: 0.082070, loss: 2.6461
2022-07-10 15:48:26 - train: epoch 0060, iter [01500, 05004], lr: 0.082057, loss: 2.6379
2022-07-10 15:49:00 - train: epoch 0060, iter [01600, 05004], lr: 0.082045, loss: 2.3499
2022-07-10 15:49:34 - train: epoch 0060, iter [01700, 05004], lr: 0.082032, loss: 2.3798
2022-07-10 15:50:09 - train: epoch 0060, iter [01800, 05004], lr: 0.082020, loss: 2.3884
2022-07-10 15:50:43 - train: epoch 0060, iter [01900, 05004], lr: 0.082008, loss: 2.6903
2022-07-10 15:51:17 - train: epoch 0060, iter [02000, 05004], lr: 0.081995, loss: 2.1552
2022-07-10 15:51:51 - train: epoch 0060, iter [02100, 05004], lr: 0.081983, loss: 2.6546
2022-07-10 15:52:26 - train: epoch 0060, iter [02200, 05004], lr: 0.081971, loss: 2.4700
2022-07-10 15:53:00 - train: epoch 0060, iter [02300, 05004], lr: 0.081958, loss: 2.1225
2022-07-10 15:53:34 - train: epoch 0060, iter [02400, 05004], lr: 0.081946, loss: 2.2968
2022-07-10 15:54:08 - train: epoch 0060, iter [02500, 05004], lr: 0.081933, loss: 2.3910
2022-07-10 15:54:43 - train: epoch 0060, iter [02600, 05004], lr: 0.081921, loss: 2.5133
2022-07-10 15:55:17 - train: epoch 0060, iter [02700, 05004], lr: 0.081909, loss: 2.2952
2022-07-10 15:55:51 - train: epoch 0060, iter [02800, 05004], lr: 0.081896, loss: 2.4284
2022-07-10 15:56:25 - train: epoch 0060, iter [02900, 05004], lr: 0.081884, loss: 2.4910
2022-07-10 15:56:59 - train: epoch 0060, iter [03000, 05004], lr: 0.081871, loss: 2.4188
2022-07-10 15:57:33 - train: epoch 0060, iter [03100, 05004], lr: 0.081859, loss: 2.4098
2022-07-10 15:58:08 - train: epoch 0060, iter [03200, 05004], lr: 0.081847, loss: 2.4450
2022-07-10 15:58:41 - train: epoch 0060, iter [03300, 05004], lr: 0.081834, loss: 2.4981
2022-07-10 15:59:16 - train: epoch 0060, iter [03400, 05004], lr: 0.081822, loss: 2.3339
2022-07-10 15:59:50 - train: epoch 0060, iter [03500, 05004], lr: 0.081809, loss: 2.5819
2022-07-10 16:00:24 - train: epoch 0060, iter [03600, 05004], lr: 0.081797, loss: 2.4681
2022-07-10 16:00:59 - train: epoch 0060, iter [03700, 05004], lr: 0.081785, loss: 2.1711
2022-07-10 16:01:33 - train: epoch 0060, iter [03800, 05004], lr: 0.081772, loss: 2.7623
2022-07-10 16:02:08 - train: epoch 0060, iter [03900, 05004], lr: 0.081760, loss: 2.7310
2022-07-10 16:02:42 - train: epoch 0060, iter [04000, 05004], lr: 0.081747, loss: 2.5928
2022-07-10 16:03:16 - train: epoch 0060, iter [04100, 05004], lr: 0.081735, loss: 2.7402
2022-07-10 16:03:51 - train: epoch 0060, iter [04200, 05004], lr: 0.081722, loss: 2.4475
2022-07-10 16:04:25 - train: epoch 0060, iter [04300, 05004], lr: 0.081710, loss: 2.4650
2022-07-10 16:04:59 - train: epoch 0060, iter [04400, 05004], lr: 0.081698, loss: 2.6762
2022-07-10 16:05:34 - train: epoch 0060, iter [04500, 05004], lr: 0.081685, loss: 2.3742
2022-07-10 16:06:08 - train: epoch 0060, iter [04600, 05004], lr: 0.081673, loss: 2.3840
2022-07-10 16:06:43 - train: epoch 0060, iter [04700, 05004], lr: 0.081660, loss: 2.6348
2022-07-10 16:07:17 - train: epoch 0060, iter [04800, 05004], lr: 0.081648, loss: 2.2724
2022-07-10 16:07:52 - train: epoch 0060, iter [04900, 05004], lr: 0.081635, loss: 2.3679
2022-07-10 16:08:24 - train: epoch 0060, iter [05000, 05004], lr: 0.081623, loss: 2.6453
2022-07-10 16:08:25 - train: epoch 060, train_loss: 2.4601
2022-07-10 16:09:42 - eval: epoch: 060, acc1: 59.050%, acc5: 83.330%, test_loss: 1.7009, per_image_load_time: 2.441ms, per_image_inference_time: 0.467ms
2022-07-10 16:09:42 - until epoch: 060, best_acc1: 59.050%
2022-07-10 16:09:42 - epoch 061 lr: 0.081622
2022-07-10 16:10:22 - train: epoch 0061, iter [00100, 05004], lr: 0.081610, loss: 2.4412
2022-07-10 16:10:57 - train: epoch 0061, iter [00200, 05004], lr: 0.081597, loss: 2.5299
2022-07-10 16:11:31 - train: epoch 0061, iter [00300, 05004], lr: 0.081585, loss: 2.3014
2022-07-10 16:12:05 - train: epoch 0061, iter [00400, 05004], lr: 0.081572, loss: 2.6463
2022-07-10 16:12:40 - train: epoch 0061, iter [00500, 05004], lr: 0.081560, loss: 2.4720
2022-07-10 16:13:15 - train: epoch 0061, iter [00600, 05004], lr: 0.081547, loss: 2.4825
2022-07-10 16:13:49 - train: epoch 0061, iter [00700, 05004], lr: 0.081535, loss: 2.2129
2022-07-10 16:14:24 - train: epoch 0061, iter [00800, 05004], lr: 0.081522, loss: 2.4158
2022-07-10 16:14:58 - train: epoch 0061, iter [00900, 05004], lr: 0.081510, loss: 2.4427
2022-07-10 16:15:33 - train: epoch 0061, iter [01000, 05004], lr: 0.081497, loss: 2.2579
2022-07-10 16:16:07 - train: epoch 0061, iter [01100, 05004], lr: 0.081485, loss: 2.3918
2022-07-10 16:16:42 - train: epoch 0061, iter [01200, 05004], lr: 0.081472, loss: 2.3815
2022-07-10 16:17:17 - train: epoch 0061, iter [01300, 05004], lr: 0.081460, loss: 2.3844
2022-07-10 16:17:52 - train: epoch 0061, iter [01400, 05004], lr: 0.081447, loss: 2.3722
2022-07-10 16:18:28 - train: epoch 0061, iter [01500, 05004], lr: 0.081435, loss: 2.3766
2022-07-10 16:19:02 - train: epoch 0061, iter [01600, 05004], lr: 0.081422, loss: 2.1557
2022-07-10 16:19:37 - train: epoch 0061, iter [01700, 05004], lr: 0.081410, loss: 2.2515
2022-07-10 16:20:11 - train: epoch 0061, iter [01800, 05004], lr: 0.081397, loss: 2.5443
2022-07-10 16:20:46 - train: epoch 0061, iter [01900, 05004], lr: 0.081385, loss: 2.7504
2022-07-10 16:21:20 - train: epoch 0061, iter [02000, 05004], lr: 0.081372, loss: 2.4755
2022-07-10 16:21:55 - train: epoch 0061, iter [02100, 05004], lr: 0.081360, loss: 2.6910
2022-07-10 16:22:29 - train: epoch 0061, iter [02200, 05004], lr: 0.081347, loss: 2.3402
2022-07-10 16:23:05 - train: epoch 0061, iter [02300, 05004], lr: 0.081335, loss: 2.3388
2022-07-10 16:23:39 - train: epoch 0061, iter [02400, 05004], lr: 0.081322, loss: 2.3662
2022-07-10 16:24:14 - train: epoch 0061, iter [02500, 05004], lr: 0.081310, loss: 2.5319
2022-07-10 16:24:48 - train: epoch 0061, iter [02600, 05004], lr: 0.081297, loss: 2.5070
2022-07-10 16:25:22 - train: epoch 0061, iter [02700, 05004], lr: 0.081284, loss: 2.5097
2022-07-10 16:25:57 - train: epoch 0061, iter [02800, 05004], lr: 0.081272, loss: 2.3859
2022-07-10 16:26:32 - train: epoch 0061, iter [02900, 05004], lr: 0.081259, loss: 2.6507
2022-07-10 16:27:07 - train: epoch 0061, iter [03000, 05004], lr: 0.081247, loss: 2.5711
2022-07-10 16:27:42 - train: epoch 0061, iter [03100, 05004], lr: 0.081234, loss: 2.7131
2022-07-10 16:28:16 - train: epoch 0061, iter [03200, 05004], lr: 0.081222, loss: 2.4064
2022-07-10 16:28:51 - train: epoch 0061, iter [03300, 05004], lr: 0.081209, loss: 2.5183
2022-07-10 16:29:26 - train: epoch 0061, iter [03400, 05004], lr: 0.081196, loss: 2.3317
2022-07-10 16:30:01 - train: epoch 0061, iter [03500, 05004], lr: 0.081184, loss: 2.5134
2022-07-10 16:30:37 - train: epoch 0061, iter [03600, 05004], lr: 0.081171, loss: 2.5016
2022-07-10 16:31:11 - train: epoch 0061, iter [03700, 05004], lr: 0.081159, loss: 2.5083
2022-07-10 16:31:45 - train: epoch 0061, iter [03800, 05004], lr: 0.081146, loss: 2.4043
2022-07-10 16:32:20 - train: epoch 0061, iter [03900, 05004], lr: 0.081133, loss: 2.4176
2022-07-10 16:32:55 - train: epoch 0061, iter [04000, 05004], lr: 0.081121, loss: 2.6559
2022-07-10 16:33:29 - train: epoch 0061, iter [04100, 05004], lr: 0.081108, loss: 2.6391
2022-07-10 16:34:04 - train: epoch 0061, iter [04200, 05004], lr: 0.081096, loss: 2.4755
2022-07-10 16:34:38 - train: epoch 0061, iter [04300, 05004], lr: 0.081083, loss: 2.3775
2022-07-10 16:35:13 - train: epoch 0061, iter [04400, 05004], lr: 0.081070, loss: 2.5382
2022-07-10 16:35:47 - train: epoch 0061, iter [04500, 05004], lr: 0.081058, loss: 2.3053
2022-07-10 16:36:21 - train: epoch 0061, iter [04600, 05004], lr: 0.081045, loss: 2.6109
2022-07-10 16:36:54 - train: epoch 0061, iter [04700, 05004], lr: 0.081033, loss: 2.2705
2022-07-10 16:37:29 - train: epoch 0061, iter [04800, 05004], lr: 0.081020, loss: 2.3921
2022-07-10 16:38:01 - train: epoch 0061, iter [04900, 05004], lr: 0.081007, loss: 2.3466
2022-07-10 16:38:34 - train: epoch 0061, iter [05000, 05004], lr: 0.080995, loss: 2.6363
2022-07-10 16:38:35 - train: epoch 061, train_loss: 2.4543
2022-07-10 16:39:48 - eval: epoch: 061, acc1: 59.274%, acc5: 83.010%, test_loss: 1.7217, per_image_load_time: 2.315ms, per_image_inference_time: 0.418ms
2022-07-10 16:39:48 - until epoch: 061, best_acc1: 59.274%
2022-07-10 16:39:48 - epoch 062 lr: 0.080994
2022-07-10 16:40:26 - train: epoch 0062, iter [00100, 05004], lr: 0.080982, loss: 2.5578
2022-07-10 16:40:59 - train: epoch 0062, iter [00200, 05004], lr: 0.080969, loss: 2.6144
2022-07-10 16:41:34 - train: epoch 0062, iter [00300, 05004], lr: 0.080956, loss: 2.4606
2022-07-10 16:42:06 - train: epoch 0062, iter [00400, 05004], lr: 0.080944, loss: 2.4565
2022-07-10 16:42:42 - train: epoch 0062, iter [00500, 05004], lr: 0.080931, loss: 2.3717
2022-07-10 16:43:15 - train: epoch 0062, iter [00600, 05004], lr: 0.080918, loss: 2.2022
2022-07-10 16:43:49 - train: epoch 0062, iter [00700, 05004], lr: 0.080906, loss: 2.6943
2022-07-10 16:44:23 - train: epoch 0062, iter [00800, 05004], lr: 0.080893, loss: 2.4591
2022-07-10 16:44:57 - train: epoch 0062, iter [00900, 05004], lr: 0.080880, loss: 2.2915
2022-07-10 16:45:31 - train: epoch 0062, iter [01000, 05004], lr: 0.080868, loss: 2.5323
2022-07-10 16:46:04 - train: epoch 0062, iter [01100, 05004], lr: 0.080855, loss: 2.3477
2022-07-10 16:46:38 - train: epoch 0062, iter [01200, 05004], lr: 0.080842, loss: 2.5175
2022-07-10 16:47:13 - train: epoch 0062, iter [01300, 05004], lr: 0.080830, loss: 2.5455
2022-07-10 16:47:46 - train: epoch 0062, iter [01400, 05004], lr: 0.080817, loss: 2.4540
2022-07-10 16:48:21 - train: epoch 0062, iter [01500, 05004], lr: 0.080804, loss: 2.6457
2022-07-10 16:48:54 - train: epoch 0062, iter [01600, 05004], lr: 0.080792, loss: 2.6429
2022-07-10 16:49:29 - train: epoch 0062, iter [01700, 05004], lr: 0.080779, loss: 2.3319
2022-07-10 16:50:03 - train: epoch 0062, iter [01800, 05004], lr: 0.080766, loss: 2.3548
2022-07-10 16:50:38 - train: epoch 0062, iter [01900, 05004], lr: 0.080754, loss: 2.3972
2022-07-10 16:51:12 - train: epoch 0062, iter [02000, 05004], lr: 0.080741, loss: 2.2813
2022-07-10 16:51:47 - train: epoch 0062, iter [02100, 05004], lr: 0.080728, loss: 2.3687
2022-07-10 16:52:21 - train: epoch 0062, iter [02200, 05004], lr: 0.080716, loss: 2.1763
2022-07-10 16:52:56 - train: epoch 0062, iter [02300, 05004], lr: 0.080703, loss: 2.4590
2022-07-10 16:53:30 - train: epoch 0062, iter [02400, 05004], lr: 0.080690, loss: 2.3021
2022-07-10 16:54:05 - train: epoch 0062, iter [02500, 05004], lr: 0.080677, loss: 2.4884
2022-07-10 16:54:39 - train: epoch 0062, iter [02600, 05004], lr: 0.080665, loss: 2.2043
2022-07-10 16:55:13 - train: epoch 0062, iter [02700, 05004], lr: 0.080652, loss: 2.5131
2022-07-10 16:55:48 - train: epoch 0062, iter [02800, 05004], lr: 0.080639, loss: 2.6530
2022-07-10 16:56:22 - train: epoch 0062, iter [02900, 05004], lr: 0.080627, loss: 2.4832
2022-07-10 16:56:57 - train: epoch 0062, iter [03000, 05004], lr: 0.080614, loss: 2.4887
2022-07-10 16:57:31 - train: epoch 0062, iter [03100, 05004], lr: 0.080601, loss: 2.7563
2022-07-10 16:58:06 - train: epoch 0062, iter [03200, 05004], lr: 0.080588, loss: 2.3329
2022-07-10 16:58:40 - train: epoch 0062, iter [03300, 05004], lr: 0.080576, loss: 2.4236
2022-07-10 16:59:14 - train: epoch 0062, iter [03400, 05004], lr: 0.080563, loss: 2.4161
2022-07-10 16:59:49 - train: epoch 0062, iter [03500, 05004], lr: 0.080550, loss: 2.5378
2022-07-10 17:00:23 - train: epoch 0062, iter [03600, 05004], lr: 0.080537, loss: 2.4654
2022-07-10 17:00:58 - train: epoch 0062, iter [03700, 05004], lr: 0.080525, loss: 2.5970
2022-07-10 17:01:33 - train: epoch 0062, iter [03800, 05004], lr: 0.080512, loss: 2.5031
2022-07-10 17:02:08 - train: epoch 0062, iter [03900, 05004], lr: 0.080499, loss: 2.3331
2022-07-10 17:02:42 - train: epoch 0062, iter [04000, 05004], lr: 0.080486, loss: 2.1698
2022-07-10 17:03:17 - train: epoch 0062, iter [04100, 05004], lr: 0.080474, loss: 2.6686
2022-07-10 17:03:52 - train: epoch 0062, iter [04200, 05004], lr: 0.080461, loss: 2.3324
2022-07-10 17:04:26 - train: epoch 0062, iter [04300, 05004], lr: 0.080448, loss: 2.4613
2022-07-10 17:05:01 - train: epoch 0062, iter [04400, 05004], lr: 0.080435, loss: 2.4813
2022-07-10 17:05:35 - train: epoch 0062, iter [04500, 05004], lr: 0.080423, loss: 2.2850
2022-07-10 17:06:10 - train: epoch 0062, iter [04600, 05004], lr: 0.080410, loss: 2.2384
2022-07-10 17:06:45 - train: epoch 0062, iter [04700, 05004], lr: 0.080397, loss: 2.2977
2022-07-10 17:07:19 - train: epoch 0062, iter [04800, 05004], lr: 0.080384, loss: 2.4982
2022-07-10 17:07:54 - train: epoch 0062, iter [04900, 05004], lr: 0.080371, loss: 2.4140
2022-07-10 17:08:28 - train: epoch 0062, iter [05000, 05004], lr: 0.080359, loss: 2.5457
2022-07-10 17:08:29 - train: epoch 062, train_loss: 2.4476
2022-07-10 17:09:45 - eval: epoch: 062, acc1: 59.214%, acc5: 82.984%, test_loss: 1.7110, per_image_load_time: 2.454ms, per_image_inference_time: 0.443ms
2022-07-10 17:09:45 - until epoch: 062, best_acc1: 59.274%
2022-07-10 17:09:45 - epoch 063 lr: 0.080358
2022-07-10 17:10:25 - train: epoch 0063, iter [00100, 05004], lr: 0.080345, loss: 2.3558
2022-07-10 17:11:00 - train: epoch 0063, iter [00200, 05004], lr: 0.080333, loss: 2.2934
2022-07-10 17:11:35 - train: epoch 0063, iter [00300, 05004], lr: 0.080320, loss: 2.5119
2022-07-10 17:12:08 - train: epoch 0063, iter [00400, 05004], lr: 0.080307, loss: 2.3325
2022-07-10 17:12:43 - train: epoch 0063, iter [00500, 05004], lr: 0.080294, loss: 2.2375
2022-07-10 17:13:18 - train: epoch 0063, iter [00600, 05004], lr: 0.080281, loss: 2.4847
2022-07-10 17:13:53 - train: epoch 0063, iter [00700, 05004], lr: 0.080269, loss: 2.4117
2022-07-10 17:14:27 - train: epoch 0063, iter [00800, 05004], lr: 0.080256, loss: 2.2797
2022-07-10 17:15:02 - train: epoch 0063, iter [00900, 05004], lr: 0.080243, loss: 2.4227
2022-07-10 17:15:37 - train: epoch 0063, iter [01000, 05004], lr: 0.080230, loss: 2.6355
2022-07-10 17:16:12 - train: epoch 0063, iter [01100, 05004], lr: 0.080217, loss: 2.3133
2022-07-10 17:16:46 - train: epoch 0063, iter [01200, 05004], lr: 0.080204, loss: 2.4118
2022-07-10 17:17:21 - train: epoch 0063, iter [01300, 05004], lr: 0.080192, loss: 2.4287
2022-07-10 17:17:56 - train: epoch 0063, iter [01400, 05004], lr: 0.080179, loss: 2.6900
2022-07-10 17:18:31 - train: epoch 0063, iter [01500, 05004], lr: 0.080166, loss: 2.6585
2022-07-10 17:19:05 - train: epoch 0063, iter [01600, 05004], lr: 0.080153, loss: 2.4022
2022-07-10 17:19:40 - train: epoch 0063, iter [01700, 05004], lr: 0.080140, loss: 2.1991
2022-07-10 17:20:15 - train: epoch 0063, iter [01800, 05004], lr: 0.080127, loss: 2.6431
2022-07-10 17:20:51 - train: epoch 0063, iter [01900, 05004], lr: 0.080115, loss: 2.7275
2022-07-10 17:21:26 - train: epoch 0063, iter [02000, 05004], lr: 0.080102, loss: 2.4348
2022-07-10 17:22:01 - train: epoch 0063, iter [02100, 05004], lr: 0.080089, loss: 2.3893
2022-07-10 17:22:37 - train: epoch 0063, iter [02200, 05004], lr: 0.080076, loss: 2.7837
2022-07-10 17:23:11 - train: epoch 0063, iter [02300, 05004], lr: 0.080063, loss: 2.6033
2022-07-10 17:23:47 - train: epoch 0063, iter [02400, 05004], lr: 0.080050, loss: 2.7935
2022-07-10 17:24:22 - train: epoch 0063, iter [02500, 05004], lr: 0.080037, loss: 2.4850
2022-07-10 17:24:58 - train: epoch 0063, iter [02600, 05004], lr: 0.080024, loss: 2.3397
2022-07-10 17:25:32 - train: epoch 0063, iter [02700, 05004], lr: 0.080012, loss: 2.6243
2022-07-10 17:26:07 - train: epoch 0063, iter [02800, 05004], lr: 0.079999, loss: 2.3888
2022-07-10 17:26:43 - train: epoch 0063, iter [02900, 05004], lr: 0.079986, loss: 2.3004
2022-07-10 17:27:17 - train: epoch 0063, iter [03000, 05004], lr: 0.079973, loss: 2.4501
2022-07-10 17:27:52 - train: epoch 0063, iter [03100, 05004], lr: 0.079960, loss: 2.7605
2022-07-10 17:28:27 - train: epoch 0063, iter [03200, 05004], lr: 0.079947, loss: 2.5653
2022-07-10 17:29:02 - train: epoch 0063, iter [03300, 05004], lr: 0.079934, loss: 2.4376
2022-07-10 17:29:38 - train: epoch 0063, iter [03400, 05004], lr: 0.079921, loss: 2.1795
2022-07-10 17:30:12 - train: epoch 0063, iter [03500, 05004], lr: 0.079909, loss: 2.8124
2022-07-10 17:30:48 - train: epoch 0063, iter [03600, 05004], lr: 0.079896, loss: 2.3084
2022-07-10 17:31:22 - train: epoch 0063, iter [03700, 05004], lr: 0.079883, loss: 2.5242
2022-07-10 17:31:58 - train: epoch 0063, iter [03800, 05004], lr: 0.079870, loss: 2.5467
2022-07-10 17:32:33 - train: epoch 0063, iter [03900, 05004], lr: 0.079857, loss: 2.2042
2022-07-10 17:33:09 - train: epoch 0063, iter [04000, 05004], lr: 0.079844, loss: 2.2700
2022-07-10 17:33:44 - train: epoch 0063, iter [04100, 05004], lr: 0.079831, loss: 2.5469
2022-07-10 17:34:19 - train: epoch 0063, iter [04200, 05004], lr: 0.079818, loss: 2.2307
2022-07-10 17:34:54 - train: epoch 0063, iter [04300, 05004], lr: 0.079805, loss: 2.6647
2022-07-10 17:35:29 - train: epoch 0063, iter [04400, 05004], lr: 0.079792, loss: 2.4163
2022-07-10 17:36:04 - train: epoch 0063, iter [04500, 05004], lr: 0.079779, loss: 2.4238
2022-07-10 17:36:39 - train: epoch 0063, iter [04600, 05004], lr: 0.079766, loss: 2.4840
2022-07-10 17:37:15 - train: epoch 0063, iter [04700, 05004], lr: 0.079753, loss: 2.5368
2022-07-10 17:37:50 - train: epoch 0063, iter [04800, 05004], lr: 0.079741, loss: 2.3570
2022-07-10 17:38:25 - train: epoch 0063, iter [04900, 05004], lr: 0.079728, loss: 2.3821
2022-07-10 17:38:59 - train: epoch 0063, iter [05000, 05004], lr: 0.079715, loss: 2.6239
2022-07-10 17:39:00 - train: epoch 063, train_loss: 2.4433
2022-07-10 17:40:17 - eval: epoch: 063, acc1: 59.534%, acc5: 83.164%, test_loss: 1.7003, per_image_load_time: 2.457ms, per_image_inference_time: 0.473ms
2022-07-10 17:40:17 - until epoch: 063, best_acc1: 59.534%
2022-07-10 17:40:17 - epoch 064 lr: 0.079714
2022-07-10 17:40:58 - train: epoch 0064, iter [00100, 05004], lr: 0.079701, loss: 2.5400
2022-07-10 17:41:32 - train: epoch 0064, iter [00200, 05004], lr: 0.079688, loss: 2.3152
2022-07-10 17:42:06 - train: epoch 0064, iter [00300, 05004], lr: 0.079675, loss: 2.1081
2022-07-10 17:42:41 - train: epoch 0064, iter [00400, 05004], lr: 0.079662, loss: 2.6237
2022-07-10 17:43:16 - train: epoch 0064, iter [00500, 05004], lr: 0.079649, loss: 2.2188
2022-07-10 17:43:51 - train: epoch 0064, iter [00600, 05004], lr: 0.079636, loss: 2.4643
2022-07-10 17:44:26 - train: epoch 0064, iter [00700, 05004], lr: 0.079623, loss: 2.5431
2022-07-10 17:45:02 - train: epoch 0064, iter [00800, 05004], lr: 0.079610, loss: 2.4392
2022-07-10 17:45:36 - train: epoch 0064, iter [00900, 05004], lr: 0.079598, loss: 2.3536
2022-07-10 17:46:12 - train: epoch 0064, iter [01000, 05004], lr: 0.079585, loss: 2.4490
2022-07-10 17:46:48 - train: epoch 0064, iter [01100, 05004], lr: 0.079572, loss: 2.4177
2022-07-10 17:47:23 - train: epoch 0064, iter [01200, 05004], lr: 0.079559, loss: 2.2820
2022-07-10 17:47:59 - train: epoch 0064, iter [01300, 05004], lr: 0.079546, loss: 2.3156
2022-07-10 17:48:34 - train: epoch 0064, iter [01400, 05004], lr: 0.079533, loss: 2.5414
2022-07-10 17:49:08 - train: epoch 0064, iter [01500, 05004], lr: 0.079520, loss: 2.6521
2022-07-10 17:49:43 - train: epoch 0064, iter [01600, 05004], lr: 0.079507, loss: 2.3079
2022-07-10 17:50:19 - train: epoch 0064, iter [01700, 05004], lr: 0.079494, loss: 2.2304
2022-07-10 17:50:54 - train: epoch 0064, iter [01800, 05004], lr: 0.079481, loss: 2.3049
2022-07-10 17:51:30 - train: epoch 0064, iter [01900, 05004], lr: 0.079468, loss: 2.4395
2022-07-10 17:52:05 - train: epoch 0064, iter [02000, 05004], lr: 0.079455, loss: 2.1609
2022-07-10 17:52:41 - train: epoch 0064, iter [02100, 05004], lr: 0.079442, loss: 2.3059
2022-07-10 17:53:16 - train: epoch 0064, iter [02200, 05004], lr: 0.079429, loss: 2.5387
2022-07-10 17:53:51 - train: epoch 0064, iter [02300, 05004], lr: 0.079416, loss: 2.5305
2022-07-10 17:54:26 - train: epoch 0064, iter [02400, 05004], lr: 0.079403, loss: 2.3809
2022-07-10 17:55:01 - train: epoch 0064, iter [02500, 05004], lr: 0.079390, loss: 2.4192
2022-07-10 17:55:36 - train: epoch 0064, iter [02600, 05004], lr: 0.079376, loss: 2.1314
2022-07-10 17:56:11 - train: epoch 0064, iter [02700, 05004], lr: 0.079363, loss: 2.1592
2022-07-10 17:56:46 - train: epoch 0064, iter [02800, 05004], lr: 0.079350, loss: 2.2260
2022-07-10 17:57:22 - train: epoch 0064, iter [02900, 05004], lr: 0.079337, loss: 2.6696
2022-07-10 17:57:57 - train: epoch 0064, iter [03000, 05004], lr: 0.079324, loss: 2.4600
2022-07-10 17:58:32 - train: epoch 0064, iter [03100, 05004], lr: 0.079311, loss: 2.3274
2022-07-10 17:59:08 - train: epoch 0064, iter [03200, 05004], lr: 0.079298, loss: 2.4383
2022-07-10 17:59:42 - train: epoch 0064, iter [03300, 05004], lr: 0.079285, loss: 2.4597
2022-07-10 18:00:17 - train: epoch 0064, iter [03400, 05004], lr: 0.079272, loss: 2.5177
2022-07-10 18:00:52 - train: epoch 0064, iter [03500, 05004], lr: 0.079259, loss: 2.3094
2022-07-10 18:01:28 - train: epoch 0064, iter [03600, 05004], lr: 0.079246, loss: 2.4320
2022-07-10 18:02:03 - train: epoch 0064, iter [03700, 05004], lr: 0.079233, loss: 2.0960
2022-07-10 18:02:38 - train: epoch 0064, iter [03800, 05004], lr: 0.079220, loss: 2.5926
2022-07-10 18:03:13 - train: epoch 0064, iter [03900, 05004], lr: 0.079207, loss: 2.3774
2022-07-10 18:03:49 - train: epoch 0064, iter [04000, 05004], lr: 0.079194, loss: 2.3618
2022-07-10 18:04:24 - train: epoch 0064, iter [04100, 05004], lr: 0.079181, loss: 2.5342
2022-07-10 18:04:59 - train: epoch 0064, iter [04200, 05004], lr: 0.079168, loss: 2.4655
2022-07-10 18:05:35 - train: epoch 0064, iter [04300, 05004], lr: 0.079155, loss: 2.6558
2022-07-10 18:06:10 - train: epoch 0064, iter [04400, 05004], lr: 0.079142, loss: 2.3367
2022-07-10 18:06:45 - train: epoch 0064, iter [04500, 05004], lr: 0.079128, loss: 2.4052
2022-07-10 18:07:20 - train: epoch 0064, iter [04600, 05004], lr: 0.079115, loss: 2.7243
2022-07-10 18:07:55 - train: epoch 0064, iter [04700, 05004], lr: 0.079102, loss: 2.7016
2022-07-10 18:08:31 - train: epoch 0064, iter [04800, 05004], lr: 0.079089, loss: 2.5887
2022-07-10 18:09:05 - train: epoch 0064, iter [04900, 05004], lr: 0.079076, loss: 2.3913
2022-07-10 18:09:40 - train: epoch 0064, iter [05000, 05004], lr: 0.079063, loss: 2.5388
2022-07-10 18:09:41 - train: epoch 064, train_loss: 2.4416
2022-07-10 18:10:58 - eval: epoch: 064, acc1: 59.564%, acc5: 83.354%, test_loss: 1.6935, per_image_load_time: 2.446ms, per_image_inference_time: 0.477ms
2022-07-10 18:10:58 - until epoch: 064, best_acc1: 59.564%
2022-07-10 18:10:58 - epoch 065 lr: 0.079062
2022-07-10 18:11:38 - train: epoch 0065, iter [00100, 05004], lr: 0.079049, loss: 2.5023
2022-07-10 18:12:13 - train: epoch 0065, iter [00200, 05004], lr: 0.079036, loss: 2.3418
2022-07-10 18:12:48 - train: epoch 0065, iter [00300, 05004], lr: 0.079023, loss: 2.3204
2022-07-10 18:13:23 - train: epoch 0065, iter [00400, 05004], lr: 0.079010, loss: 2.5796
2022-07-10 18:13:58 - train: epoch 0065, iter [00500, 05004], lr: 0.078997, loss: 2.0938
2022-07-10 18:14:33 - train: epoch 0065, iter [00600, 05004], lr: 0.078984, loss: 2.5917
2022-07-10 18:15:08 - train: epoch 0065, iter [00700, 05004], lr: 0.078971, loss: 2.4594
2022-07-10 18:15:43 - train: epoch 0065, iter [00800, 05004], lr: 0.078958, loss: 2.1517
2022-07-10 18:16:18 - train: epoch 0065, iter [00900, 05004], lr: 0.078944, loss: 2.4566
2022-07-10 18:16:53 - train: epoch 0065, iter [01000, 05004], lr: 0.078931, loss: 2.3671
2022-07-10 18:17:29 - train: epoch 0065, iter [01100, 05004], lr: 0.078918, loss: 2.2085
2022-07-10 18:18:04 - train: epoch 0065, iter [01200, 05004], lr: 0.078905, loss: 2.7037
2022-07-10 18:18:40 - train: epoch 0065, iter [01300, 05004], lr: 0.078892, loss: 2.5115
2022-07-10 18:19:14 - train: epoch 0065, iter [01400, 05004], lr: 0.078879, loss: 2.3063
2022-07-10 18:19:48 - train: epoch 0065, iter [01500, 05004], lr: 0.078866, loss: 2.4275
2022-07-10 18:20:25 - train: epoch 0065, iter [01600, 05004], lr: 0.078852, loss: 2.6784
2022-07-10 18:20:59 - train: epoch 0065, iter [01700, 05004], lr: 0.078839, loss: 2.2780
2022-07-10 18:21:34 - train: epoch 0065, iter [01800, 05004], lr: 0.078826, loss: 2.2765
2022-07-10 18:22:09 - train: epoch 0065, iter [01900, 05004], lr: 0.078813, loss: 2.3845
2022-07-10 18:22:44 - train: epoch 0065, iter [02000, 05004], lr: 0.078800, loss: 2.5376
2022-07-10 18:23:20 - train: epoch 0065, iter [02100, 05004], lr: 0.078787, loss: 2.2708
2022-07-10 18:23:55 - train: epoch 0065, iter [02200, 05004], lr: 0.078774, loss: 2.4737
2022-07-10 18:24:30 - train: epoch 0065, iter [02300, 05004], lr: 0.078760, loss: 2.2571
2022-07-10 18:25:05 - train: epoch 0065, iter [02400, 05004], lr: 0.078747, loss: 2.3832
2022-07-10 18:25:41 - train: epoch 0065, iter [02500, 05004], lr: 0.078734, loss: 2.4899
2022-07-10 18:26:16 - train: epoch 0065, iter [02600, 05004], lr: 0.078721, loss: 2.4541
2022-07-10 18:26:51 - train: epoch 0065, iter [02700, 05004], lr: 0.078708, loss: 2.4246
2022-07-10 18:27:27 - train: epoch 0065, iter [02800, 05004], lr: 0.078695, loss: 2.1730
2022-07-10 18:28:02 - train: epoch 0065, iter [02900, 05004], lr: 0.078681, loss: 2.5783
2022-07-10 18:28:37 - train: epoch 0065, iter [03000, 05004], lr: 0.078668, loss: 2.4569
2022-07-10 18:29:13 - train: epoch 0065, iter [03100, 05004], lr: 0.078655, loss: 2.3315
2022-07-10 18:29:48 - train: epoch 0065, iter [03200, 05004], lr: 0.078642, loss: 2.4811
2022-07-10 18:30:23 - train: epoch 0065, iter [03300, 05004], lr: 0.078629, loss: 2.4819
2022-07-10 18:30:58 - train: epoch 0065, iter [03400, 05004], lr: 0.078615, loss: 2.2495
2022-07-10 18:31:34 - train: epoch 0065, iter [03500, 05004], lr: 0.078602, loss: 2.7141
2022-07-10 18:32:09 - train: epoch 0065, iter [03600, 05004], lr: 0.078589, loss: 2.4216
2022-07-10 18:32:44 - train: epoch 0065, iter [03700, 05004], lr: 0.078576, loss: 2.3346
2022-07-10 18:33:19 - train: epoch 0065, iter [03800, 05004], lr: 0.078563, loss: 2.1188
2022-07-10 18:33:54 - train: epoch 0065, iter [03900, 05004], lr: 0.078549, loss: 2.4860
2022-07-10 18:34:30 - train: epoch 0065, iter [04000, 05004], lr: 0.078536, loss: 2.2869
2022-07-10 18:35:04 - train: epoch 0065, iter [04100, 05004], lr: 0.078523, loss: 2.1731
2022-07-10 18:35:40 - train: epoch 0065, iter [04200, 05004], lr: 0.078510, loss: 2.5547
2022-07-10 18:36:14 - train: epoch 0065, iter [04300, 05004], lr: 0.078496, loss: 2.3311
2022-07-10 18:36:50 - train: epoch 0065, iter [04400, 05004], lr: 0.078483, loss: 2.4783
2022-07-10 18:37:26 - train: epoch 0065, iter [04500, 05004], lr: 0.078470, loss: 2.5728
2022-07-10 18:38:00 - train: epoch 0065, iter [04600, 05004], lr: 0.078457, loss: 2.4939
2022-07-10 18:38:35 - train: epoch 0065, iter [04700, 05004], lr: 0.078443, loss: 2.4655
2022-07-10 18:39:10 - train: epoch 0065, iter [04800, 05004], lr: 0.078430, loss: 2.5193
2022-07-10 18:39:45 - train: epoch 0065, iter [04900, 05004], lr: 0.078417, loss: 2.4876
2022-07-10 18:40:18 - train: epoch 0065, iter [05000, 05004], lr: 0.078404, loss: 2.4715
2022-07-10 18:40:19 - train: epoch 065, train_loss: 2.4348
2022-07-10 18:41:34 - eval: epoch: 065, acc1: 60.430%, acc5: 83.744%, test_loss: 1.6598, per_image_load_time: 2.284ms, per_image_inference_time: 0.503ms
2022-07-10 18:41:34 - until epoch: 065, best_acc1: 60.430%
2022-07-10 18:41:34 - epoch 066 lr: 0.078403
2022-07-10 18:42:15 - train: epoch 0066, iter [00100, 05004], lr: 0.078390, loss: 2.1763
2022-07-10 18:42:49 - train: epoch 0066, iter [00200, 05004], lr: 0.078377, loss: 2.7046
2022-07-10 18:43:24 - train: epoch 0066, iter [00300, 05004], lr: 0.078363, loss: 2.2080
2022-07-10 18:44:00 - train: epoch 0066, iter [00400, 05004], lr: 0.078350, loss: 2.2786
2022-07-10 18:44:34 - train: epoch 0066, iter [00500, 05004], lr: 0.078337, loss: 2.3609
2022-07-10 18:45:09 - train: epoch 0066, iter [00600, 05004], lr: 0.078324, loss: 2.3501
2022-07-10 18:45:44 - train: epoch 0066, iter [00700, 05004], lr: 0.078310, loss: 2.4221
2022-07-10 18:46:19 - train: epoch 0066, iter [00800, 05004], lr: 0.078297, loss: 2.6592
2022-07-10 18:46:53 - train: epoch 0066, iter [00900, 05004], lr: 0.078284, loss: 2.5041
2022-07-10 18:47:29 - train: epoch 0066, iter [01000, 05004], lr: 0.078271, loss: 2.6085
2022-07-10 18:48:03 - train: epoch 0066, iter [01100, 05004], lr: 0.078257, loss: 2.5099
2022-07-10 18:48:39 - train: epoch 0066, iter [01200, 05004], lr: 0.078244, loss: 2.5652
2022-07-10 18:49:13 - train: epoch 0066, iter [01300, 05004], lr: 0.078231, loss: 2.6248
2022-07-10 18:49:48 - train: epoch 0066, iter [01400, 05004], lr: 0.078217, loss: 2.1866
2022-07-10 18:50:24 - train: epoch 0066, iter [01500, 05004], lr: 0.078204, loss: 2.7266
2022-07-10 18:50:58 - train: epoch 0066, iter [01600, 05004], lr: 0.078191, loss: 2.4729
2022-07-10 18:51:34 - train: epoch 0066, iter [01700, 05004], lr: 0.078178, loss: 2.2708
2022-07-10 18:52:09 - train: epoch 0066, iter [01800, 05004], lr: 0.078164, loss: 2.2258
2022-07-10 18:52:44 - train: epoch 0066, iter [01900, 05004], lr: 0.078151, loss: 2.6931
2022-07-10 18:53:19 - train: epoch 0066, iter [02000, 05004], lr: 0.078138, loss: 2.4376
2022-07-10 18:53:53 - train: epoch 0066, iter [02100, 05004], lr: 0.078124, loss: 2.6541
2022-07-10 18:54:28 - train: epoch 0066, iter [02200, 05004], lr: 0.078111, loss: 2.3916
2022-07-10 18:55:03 - train: epoch 0066, iter [02300, 05004], lr: 0.078098, loss: 2.3039
2022-07-10 18:55:37 - train: epoch 0066, iter [02400, 05004], lr: 0.078084, loss: 2.2667
2022-07-10 18:56:12 - train: epoch 0066, iter [02500, 05004], lr: 0.078071, loss: 2.6148
2022-07-10 18:56:48 - train: epoch 0066, iter [02600, 05004], lr: 0.078058, loss: 2.2357
2022-07-10 18:57:23 - train: epoch 0066, iter [02700, 05004], lr: 0.078044, loss: 2.6287
2022-07-10 18:57:58 - train: epoch 0066, iter [02800, 05004], lr: 0.078031, loss: 2.4211
2022-07-10 18:58:34 - train: epoch 0066, iter [02900, 05004], lr: 0.078018, loss: 2.4688
2022-07-10 18:59:08 - train: epoch 0066, iter [03000, 05004], lr: 0.078004, loss: 2.6225
2022-07-10 18:59:44 - train: epoch 0066, iter [03100, 05004], lr: 0.077991, loss: 2.4175
2022-07-10 19:00:18 - train: epoch 0066, iter [03200, 05004], lr: 0.077978, loss: 2.1327
2022-07-10 19:00:54 - train: epoch 0066, iter [03300, 05004], lr: 0.077964, loss: 2.4441
2022-07-10 19:01:29 - train: epoch 0066, iter [03400, 05004], lr: 0.077951, loss: 2.8973
2022-07-10 19:02:04 - train: epoch 0066, iter [03500, 05004], lr: 0.077938, loss: 2.3962
2022-07-10 19:02:39 - train: epoch 0066, iter [03600, 05004], lr: 0.077924, loss: 2.6017
2022-07-10 19:03:15 - train: epoch 0066, iter [03700, 05004], lr: 0.077911, loss: 2.5885
2022-07-10 19:03:51 - train: epoch 0066, iter [03800, 05004], lr: 0.077898, loss: 2.2410
2022-07-10 19:04:26 - train: epoch 0066, iter [03900, 05004], lr: 0.077884, loss: 2.1480
2022-07-10 19:05:01 - train: epoch 0066, iter [04000, 05004], lr: 0.077871, loss: 2.4392
2022-07-10 19:05:36 - train: epoch 0066, iter [04100, 05004], lr: 0.077858, loss: 2.4387
2022-07-10 19:06:12 - train: epoch 0066, iter [04200, 05004], lr: 0.077844, loss: 2.3008
2022-07-10 19:06:46 - train: epoch 0066, iter [04300, 05004], lr: 0.077831, loss: 2.2633
2022-07-10 19:07:22 - train: epoch 0066, iter [04400, 05004], lr: 0.077817, loss: 2.3832
2022-07-10 19:07:57 - train: epoch 0066, iter [04500, 05004], lr: 0.077804, loss: 2.3277
2022-07-10 19:08:32 - train: epoch 0066, iter [04600, 05004], lr: 0.077791, loss: 2.5915
2022-07-10 19:09:07 - train: epoch 0066, iter [04700, 05004], lr: 0.077777, loss: 2.2083
2022-07-10 19:09:43 - train: epoch 0066, iter [04800, 05004], lr: 0.077764, loss: 2.3548
2022-07-10 19:10:18 - train: epoch 0066, iter [04900, 05004], lr: 0.077751, loss: 2.3710
2022-07-10 19:10:51 - train: epoch 0066, iter [05000, 05004], lr: 0.077737, loss: 2.2874
2022-07-10 19:10:52 - train: epoch 066, train_loss: 2.4286
2022-07-10 19:12:08 - eval: epoch: 066, acc1: 60.200%, acc5: 84.044%, test_loss: 1.6559, per_image_load_time: 2.168ms, per_image_inference_time: 0.479ms
2022-07-10 19:12:09 - until epoch: 066, best_acc1: 60.430%
2022-07-10 19:12:09 - epoch 067 lr: 0.077737
2022-07-10 19:12:48 - train: epoch 0067, iter [00100, 05004], lr: 0.077723, loss: 2.5309
2022-07-10 19:13:25 - train: epoch 0067, iter [00200, 05004], lr: 0.077710, loss: 2.4749
2022-07-10 19:14:00 - train: epoch 0067, iter [00300, 05004], lr: 0.077696, loss: 2.5322
2022-07-10 19:14:34 - train: epoch 0067, iter [00400, 05004], lr: 0.077683, loss: 2.3386
2022-07-10 19:15:09 - train: epoch 0067, iter [00500, 05004], lr: 0.077670, loss: 2.2206
2022-07-10 19:15:44 - train: epoch 0067, iter [00600, 05004], lr: 0.077656, loss: 2.3144
2022-07-10 19:16:19 - train: epoch 0067, iter [00700, 05004], lr: 0.077643, loss: 2.5770
2022-07-10 19:16:53 - train: epoch 0067, iter [00800, 05004], lr: 0.077629, loss: 2.4570
2022-07-10 19:17:29 - train: epoch 0067, iter [00900, 05004], lr: 0.077616, loss: 2.6610
2022-07-10 19:18:04 - train: epoch 0067, iter [01000, 05004], lr: 0.077603, loss: 2.3638
2022-07-10 19:18:39 - train: epoch 0067, iter [01100, 05004], lr: 0.077589, loss: 2.3777
2022-07-10 19:19:13 - train: epoch 0067, iter [01200, 05004], lr: 0.077576, loss: 2.4694
2022-07-10 19:19:49 - train: epoch 0067, iter [01300, 05004], lr: 0.077562, loss: 2.5491
2022-07-10 19:20:23 - train: epoch 0067, iter [01400, 05004], lr: 0.077549, loss: 2.5545
2022-07-10 19:20:59 - train: epoch 0067, iter [01500, 05004], lr: 0.077535, loss: 2.3826
2022-07-10 19:21:34 - train: epoch 0067, iter [01600, 05004], lr: 0.077522, loss: 2.4950
2022-07-10 19:22:09 - train: epoch 0067, iter [01700, 05004], lr: 0.077509, loss: 2.2189
2022-07-10 19:22:44 - train: epoch 0067, iter [01800, 05004], lr: 0.077495, loss: 2.5775
2022-07-10 19:23:18 - train: epoch 0067, iter [01900, 05004], lr: 0.077482, loss: 2.4295
2022-07-10 19:23:53 - train: epoch 0067, iter [02000, 05004], lr: 0.077468, loss: 2.5151
2022-07-10 19:24:28 - train: epoch 0067, iter [02100, 05004], lr: 0.077455, loss: 2.2928
2022-07-10 19:25:03 - train: epoch 0067, iter [02200, 05004], lr: 0.077441, loss: 2.4556
2022-07-10 19:25:38 - train: epoch 0067, iter [02300, 05004], lr: 0.077428, loss: 2.4545
2022-07-10 19:26:14 - train: epoch 0067, iter [02400, 05004], lr: 0.077414, loss: 2.3280
2022-07-10 19:26:48 - train: epoch 0067, iter [02500, 05004], lr: 0.077401, loss: 2.1911
2022-07-10 19:27:23 - train: epoch 0067, iter [02600, 05004], lr: 0.077387, loss: 2.1669
2022-07-10 19:27:58 - train: epoch 0067, iter [02700, 05004], lr: 0.077374, loss: 2.2967
2022-07-10 19:28:33 - train: epoch 0067, iter [02800, 05004], lr: 0.077360, loss: 2.4547
2022-07-10 19:29:08 - train: epoch 0067, iter [02900, 05004], lr: 0.077347, loss: 2.2715
2022-07-10 19:29:43 - train: epoch 0067, iter [03000, 05004], lr: 0.077334, loss: 2.1493
2022-07-10 19:30:18 - train: epoch 0067, iter [03100, 05004], lr: 0.077320, loss: 2.4653
2022-07-10 19:30:53 - train: epoch 0067, iter [03200, 05004], lr: 0.077307, loss: 2.4535
2022-07-10 19:31:28 - train: epoch 0067, iter [03300, 05004], lr: 0.077293, loss: 2.2996
2022-07-10 19:32:02 - train: epoch 0067, iter [03400, 05004], lr: 0.077280, loss: 2.2504
2022-07-10 19:32:38 - train: epoch 0067, iter [03500, 05004], lr: 0.077266, loss: 2.3074
2022-07-10 19:33:13 - train: epoch 0067, iter [03600, 05004], lr: 0.077253, loss: 2.5329
2022-07-10 19:33:48 - train: epoch 0067, iter [03700, 05004], lr: 0.077239, loss: 2.5694
2022-07-10 19:34:23 - train: epoch 0067, iter [03800, 05004], lr: 0.077226, loss: 2.2762
2022-07-10 19:34:59 - train: epoch 0067, iter [03900, 05004], lr: 0.077212, loss: 2.6127
2022-07-10 19:35:33 - train: epoch 0067, iter [04000, 05004], lr: 0.077199, loss: 2.4910
2022-07-10 19:36:08 - train: epoch 0067, iter [04100, 05004], lr: 0.077185, loss: 2.8632
2022-07-10 19:36:43 - train: epoch 0067, iter [04200, 05004], lr: 0.077172, loss: 2.5622
2022-07-10 19:37:19 - train: epoch 0067, iter [04300, 05004], lr: 0.077158, loss: 2.3152
2022-07-10 19:37:54 - train: epoch 0067, iter [04400, 05004], lr: 0.077145, loss: 2.6839
2022-07-10 19:38:30 - train: epoch 0067, iter [04500, 05004], lr: 0.077131, loss: 2.3388
2022-07-10 19:39:04 - train: epoch 0067, iter [04600, 05004], lr: 0.077117, loss: 2.0212
2022-07-10 19:39:41 - train: epoch 0067, iter [04700, 05004], lr: 0.077104, loss: 2.3757
2022-07-10 19:40:15 - train: epoch 0067, iter [04800, 05004], lr: 0.077090, loss: 2.1468
2022-07-10 19:40:51 - train: epoch 0067, iter [04900, 05004], lr: 0.077077, loss: 2.4482
2022-07-10 19:41:25 - train: epoch 0067, iter [05000, 05004], lr: 0.077063, loss: 2.4841
2022-07-10 19:41:26 - train: epoch 067, train_loss: 2.4230
2022-07-10 19:42:42 - eval: epoch: 067, acc1: 59.470%, acc5: 83.010%, test_loss: 1.7089, per_image_load_time: 2.493ms, per_image_inference_time: 0.445ms
2022-07-10 19:42:42 - until epoch: 067, best_acc1: 60.430%
2022-07-10 19:42:42 - epoch 068 lr: 0.077063
2022-07-10 19:43:22 - train: epoch 0068, iter [00100, 05004], lr: 0.077049, loss: 2.5123
2022-07-10 19:43:58 - train: epoch 0068, iter [00200, 05004], lr: 0.077036, loss: 2.4187
2022-07-10 19:44:32 - train: epoch 0068, iter [00300, 05004], lr: 0.077022, loss: 2.5421
2022-07-10 19:45:07 - train: epoch 0068, iter [00400, 05004], lr: 0.077009, loss: 2.2613
2022-07-10 19:45:42 - train: epoch 0068, iter [00500, 05004], lr: 0.076995, loss: 2.4551
2022-07-10 19:46:17 - train: epoch 0068, iter [00600, 05004], lr: 0.076982, loss: 2.2395
2022-07-10 19:46:52 - train: epoch 0068, iter [00700, 05004], lr: 0.076968, loss: 2.6944
2022-07-10 19:47:27 - train: epoch 0068, iter [00800, 05004], lr: 0.076954, loss: 2.3559
2022-07-10 19:48:01 - train: epoch 0068, iter [00900, 05004], lr: 0.076941, loss: 2.2364
2022-07-10 19:48:36 - train: epoch 0068, iter [01000, 05004], lr: 0.076927, loss: 2.4618
2022-07-10 19:49:11 - train: epoch 0068, iter [01100, 05004], lr: 0.076914, loss: 2.7368
2022-07-10 19:49:46 - train: epoch 0068, iter [01200, 05004], lr: 0.076900, loss: 2.2643
2022-07-10 19:50:20 - train: epoch 0068, iter [01300, 05004], lr: 0.076887, loss: 2.4944
2022-07-10 19:50:56 - train: epoch 0068, iter [01400, 05004], lr: 0.076873, loss: 2.4355
2022-07-10 19:51:31 - train: epoch 0068, iter [01500, 05004], lr: 0.076859, loss: 2.4740
2022-07-10 19:52:06 - train: epoch 0068, iter [01600, 05004], lr: 0.076846, loss: 2.4857
2022-07-10 19:52:40 - train: epoch 0068, iter [01700, 05004], lr: 0.076832, loss: 2.5211
2022-07-10 19:53:16 - train: epoch 0068, iter [01800, 05004], lr: 0.076819, loss: 2.5839
2022-07-10 19:53:49 - train: epoch 0068, iter [01900, 05004], lr: 0.076805, loss: 2.6656
2022-07-10 19:54:25 - train: epoch 0068, iter [02000, 05004], lr: 0.076792, loss: 2.4069
2022-07-10 19:55:00 - train: epoch 0068, iter [02100, 05004], lr: 0.076778, loss: 2.4630
2022-07-10 19:55:34 - train: epoch 0068, iter [02200, 05004], lr: 0.076764, loss: 2.3879
2022-07-10 19:56:10 - train: epoch 0068, iter [02300, 05004], lr: 0.076751, loss: 2.5219
2022-07-10 19:56:45 - train: epoch 0068, iter [02400, 05004], lr: 0.076737, loss: 2.6972
2022-07-10 19:57:20 - train: epoch 0068, iter [02500, 05004], lr: 0.076724, loss: 2.4135
2022-07-10 19:57:55 - train: epoch 0068, iter [02600, 05004], lr: 0.076710, loss: 2.3789
2022-07-10 19:58:29 - train: epoch 0068, iter [02700, 05004], lr: 0.076696, loss: 2.4960
2022-07-10 19:59:04 - train: epoch 0068, iter [02800, 05004], lr: 0.076683, loss: 2.6566
2022-07-10 19:59:39 - train: epoch 0068, iter [02900, 05004], lr: 0.076669, loss: 2.5198
2022-07-10 20:00:14 - train: epoch 0068, iter [03000, 05004], lr: 0.076656, loss: 2.6467
2022-07-10 20:00:49 - train: epoch 0068, iter [03100, 05004], lr: 0.076642, loss: 2.0778
2022-07-10 20:01:24 - train: epoch 0068, iter [03200, 05004], lr: 0.076628, loss: 2.5474
2022-07-10 20:01:59 - train: epoch 0068, iter [03300, 05004], lr: 0.076615, loss: 2.5234
2022-07-10 20:02:34 - train: epoch 0068, iter [03400, 05004], lr: 0.076601, loss: 2.3131
2022-07-10 20:03:08 - train: epoch 0068, iter [03500, 05004], lr: 0.076587, loss: 2.4044
2022-07-10 20:03:43 - train: epoch 0068, iter [03600, 05004], lr: 0.076574, loss: 2.3470
2022-07-10 20:04:17 - train: epoch 0068, iter [03700, 05004], lr: 0.076560, loss: 2.4198
2022-07-10 20:04:53 - train: epoch 0068, iter [03800, 05004], lr: 0.076546, loss: 2.3878
2022-07-10 20:05:27 - train: epoch 0068, iter [03900, 05004], lr: 0.076533, loss: 2.3728
2022-07-10 20:06:03 - train: epoch 0068, iter [04000, 05004], lr: 0.076519, loss: 2.3564
2022-07-10 20:06:38 - train: epoch 0068, iter [04100, 05004], lr: 0.076506, loss: 2.0522
2022-07-10 20:07:13 - train: epoch 0068, iter [04200, 05004], lr: 0.076492, loss: 2.6409
2022-07-10 20:07:47 - train: epoch 0068, iter [04300, 05004], lr: 0.076478, loss: 2.4274
2022-07-10 20:08:22 - train: epoch 0068, iter [04400, 05004], lr: 0.076465, loss: 2.3216
2022-07-10 20:08:57 - train: epoch 0068, iter [04500, 05004], lr: 0.076451, loss: 2.4292
2022-07-10 20:09:32 - train: epoch 0068, iter [04600, 05004], lr: 0.076437, loss: 2.5886
2022-07-10 20:10:07 - train: epoch 0068, iter [04700, 05004], lr: 0.076424, loss: 2.5198
2022-07-10 20:10:42 - train: epoch 0068, iter [04800, 05004], lr: 0.076410, loss: 2.6271
2022-07-10 20:11:17 - train: epoch 0068, iter [04900, 05004], lr: 0.076396, loss: 2.5400
2022-07-10 20:11:51 - train: epoch 0068, iter [05000, 05004], lr: 0.076383, loss: 2.5352
2022-07-10 20:11:52 - train: epoch 068, train_loss: 2.4164
2022-07-10 20:13:08 - eval: epoch: 068, acc1: 59.706%, acc5: 83.348%, test_loss: 1.6886, per_image_load_time: 1.753ms, per_image_inference_time: 0.478ms
2022-07-10 20:13:08 - until epoch: 068, best_acc1: 60.430%
2022-07-10 20:13:08 - epoch 069 lr: 0.076382
2022-07-10 20:13:48 - train: epoch 0069, iter [00100, 05004], lr: 0.076368, loss: 2.6133
2022-07-10 20:14:23 - train: epoch 0069, iter [00200, 05004], lr: 0.076355, loss: 2.5601
2022-07-10 20:14:57 - train: epoch 0069, iter [00300, 05004], lr: 0.076341, loss: 2.2682
2022-07-10 20:15:32 - train: epoch 0069, iter [00400, 05004], lr: 0.076327, loss: 2.2824
2022-07-10 20:16:07 - train: epoch 0069, iter [00500, 05004], lr: 0.076314, loss: 2.3694
2022-07-10 20:16:42 - train: epoch 0069, iter [00600, 05004], lr: 0.076300, loss: 2.2894
2022-07-10 20:17:17 - train: epoch 0069, iter [00700, 05004], lr: 0.076286, loss: 2.4306
2022-07-10 20:17:51 - train: epoch 0069, iter [00800, 05004], lr: 0.076273, loss: 2.4372
2022-07-10 20:18:25 - train: epoch 0069, iter [00900, 05004], lr: 0.076259, loss: 2.3137
2022-07-10 20:18:59 - train: epoch 0069, iter [01000, 05004], lr: 0.076245, loss: 2.5543
2022-07-10 20:19:35 - train: epoch 0069, iter [01100, 05004], lr: 0.076231, loss: 2.3149
2022-07-10 20:20:10 - train: epoch 0069, iter [01200, 05004], lr: 0.076218, loss: 2.2807
2022-07-10 20:20:44 - train: epoch 0069, iter [01300, 05004], lr: 0.076204, loss: 2.6135
2022-07-10 20:21:19 - train: epoch 0069, iter [01400, 05004], lr: 0.076190, loss: 2.4706
2022-07-10 20:21:53 - train: epoch 0069, iter [01500, 05004], lr: 0.076177, loss: 2.4036
2022-07-10 20:22:28 - train: epoch 0069, iter [01600, 05004], lr: 0.076163, loss: 2.5959
2022-07-10 20:23:03 - train: epoch 0069, iter [01700, 05004], lr: 0.076149, loss: 2.3257
2022-07-10 20:23:38 - train: epoch 0069, iter [01800, 05004], lr: 0.076135, loss: 2.0933
2022-07-10 20:24:12 - train: epoch 0069, iter [01900, 05004], lr: 0.076122, loss: 2.5553
2022-07-10 20:24:47 - train: epoch 0069, iter [02000, 05004], lr: 0.076108, loss: 2.4721
2022-07-10 20:25:22 - train: epoch 0069, iter [02100, 05004], lr: 0.076094, loss: 2.4291
2022-07-10 20:25:57 - train: epoch 0069, iter [02200, 05004], lr: 0.076081, loss: 2.4825
2022-07-10 20:26:30 - train: epoch 0069, iter [02300, 05004], lr: 0.076067, loss: 2.3385
2022-07-10 20:27:05 - train: epoch 0069, iter [02400, 05004], lr: 0.076053, loss: 2.3477
2022-07-10 20:27:40 - train: epoch 0069, iter [02500, 05004], lr: 0.076039, loss: 2.5225
2022-07-10 20:28:14 - train: epoch 0069, iter [02600, 05004], lr: 0.076026, loss: 2.5885
2022-07-10 20:28:49 - train: epoch 0069, iter [02700, 05004], lr: 0.076012, loss: 2.8199
2022-07-10 20:29:24 - train: epoch 0069, iter [02800, 05004], lr: 0.075998, loss: 2.5526
2022-07-10 20:29:59 - train: epoch 0069, iter [02900, 05004], lr: 0.075984, loss: 2.3315
2022-07-10 20:30:34 - train: epoch 0069, iter [03000, 05004], lr: 0.075971, loss: 2.4204
2022-07-10 20:31:09 - train: epoch 0069, iter [03100, 05004], lr: 0.075957, loss: 2.4536
2022-07-10 20:31:43 - train: epoch 0069, iter [03200, 05004], lr: 0.075943, loss: 2.2051
2022-07-10 20:32:18 - train: epoch 0069, iter [03300, 05004], lr: 0.075929, loss: 2.2191
2022-07-10 20:32:53 - train: epoch 0069, iter [03400, 05004], lr: 0.075916, loss: 2.2352
2022-07-10 20:33:27 - train: epoch 0069, iter [03500, 05004], lr: 0.075902, loss: 2.3261
2022-07-10 20:34:02 - train: epoch 0069, iter [03600, 05004], lr: 0.075888, loss: 2.5298
2022-07-10 20:34:37 - train: epoch 0069, iter [03700, 05004], lr: 0.075874, loss: 2.4376
2022-07-10 20:35:12 - train: epoch 0069, iter [03800, 05004], lr: 0.075860, loss: 2.5391
2022-07-10 20:35:47 - train: epoch 0069, iter [03900, 05004], lr: 0.075847, loss: 2.3828
2022-07-10 20:36:21 - train: epoch 0069, iter [04000, 05004], lr: 0.075833, loss: 2.6246
2022-07-10 20:36:55 - train: epoch 0069, iter [04100, 05004], lr: 0.075819, loss: 2.5290
2022-07-10 20:37:30 - train: epoch 0069, iter [04200, 05004], lr: 0.075805, loss: 2.3237
2022-07-10 20:38:05 - train: epoch 0069, iter [04300, 05004], lr: 0.075791, loss: 2.4675
2022-07-10 20:38:39 - train: epoch 0069, iter [04400, 05004], lr: 0.075778, loss: 2.4450
2022-07-10 20:39:14 - train: epoch 0069, iter [04500, 05004], lr: 0.075764, loss: 2.5162
2022-07-10 20:39:49 - train: epoch 0069, iter [04600, 05004], lr: 0.075750, loss: 2.5641
2022-07-10 20:40:23 - train: epoch 0069, iter [04700, 05004], lr: 0.075736, loss: 2.3530
2022-07-10 20:40:58 - train: epoch 0069, iter [04800, 05004], lr: 0.075723, loss: 2.5296
2022-07-10 20:41:33 - train: epoch 0069, iter [04900, 05004], lr: 0.075709, loss: 2.2389
2022-07-10 20:42:06 - train: epoch 0069, iter [05000, 05004], lr: 0.075695, loss: 2.5940
2022-07-10 20:42:07 - train: epoch 069, train_loss: 2.4135
2022-07-10 20:43:24 - eval: epoch: 069, acc1: 59.802%, acc5: 83.384%, test_loss: 1.6795, per_image_load_time: 2.464ms, per_image_inference_time: 0.467ms
2022-07-10 20:43:24 - until epoch: 069, best_acc1: 60.430%
2022-07-10 20:51:00 - epoch 070 lr: 0.075694
2022-07-10 20:51:41 - train: epoch 0070, iter [00100, 05004], lr: 0.075681, loss: 2.4171
2022-07-10 20:52:15 - train: epoch 0070, iter [00200, 05004], lr: 0.075667, loss: 2.3513
2022-07-10 20:52:49 - train: epoch 0070, iter [00300, 05004], lr: 0.075653, loss: 2.5492
2022-07-10 20:53:25 - train: epoch 0070, iter [00400, 05004], lr: 0.075639, loss: 2.3395
2022-07-10 20:53:59 - train: epoch 0070, iter [00500, 05004], lr: 0.075625, loss: 2.3457
2022-07-10 20:54:34 - train: epoch 0070, iter [00600, 05004], lr: 0.075611, loss: 2.1543
2022-07-10 20:55:08 - train: epoch 0070, iter [00700, 05004], lr: 0.075598, loss: 2.3066
2022-07-10 20:55:43 - train: epoch 0070, iter [00800, 05004], lr: 0.075584, loss: 2.2066
2022-07-10 20:56:17 - train: epoch 0070, iter [00900, 05004], lr: 0.075570, loss: 2.4016
2022-07-10 20:56:51 - train: epoch 0070, iter [01000, 05004], lr: 0.075556, loss: 2.3634
2022-07-10 20:57:27 - train: epoch 0070, iter [01100, 05004], lr: 0.075542, loss: 2.7684
2022-07-10 20:58:02 - train: epoch 0070, iter [01200, 05004], lr: 0.075528, loss: 2.3619
2022-07-10 20:58:35 - train: epoch 0070, iter [01300, 05004], lr: 0.075515, loss: 2.4499
2022-07-10 20:59:10 - train: epoch 0070, iter [01400, 05004], lr: 0.075501, loss: 2.3985
2022-07-10 20:59:45 - train: epoch 0070, iter [01500, 05004], lr: 0.075487, loss: 2.4410
2022-07-10 21:00:18 - train: epoch 0070, iter [01600, 05004], lr: 0.075473, loss: 2.6273
2022-07-10 21:00:53 - train: epoch 0070, iter [01700, 05004], lr: 0.075459, loss: 2.5615
2022-07-10 21:01:27 - train: epoch 0070, iter [01800, 05004], lr: 0.075445, loss: 2.1144
2022-07-10 21:02:01 - train: epoch 0070, iter [01900, 05004], lr: 0.075431, loss: 2.1724
2022-07-10 21:02:36 - train: epoch 0070, iter [02000, 05004], lr: 0.075418, loss: 2.3863
2022-07-10 21:03:11 - train: epoch 0070, iter [02100, 05004], lr: 0.075404, loss: 2.6277
2022-07-10 21:03:45 - train: epoch 0070, iter [02200, 05004], lr: 0.075390, loss: 2.4845
2022-07-10 21:04:20 - train: epoch 0070, iter [02300, 05004], lr: 0.075376, loss: 2.4470
2022-07-10 21:04:54 - train: epoch 0070, iter [02400, 05004], lr: 0.075362, loss: 2.4786
2022-07-10 21:05:28 - train: epoch 0070, iter [02500, 05004], lr: 0.075348, loss: 2.4970
2022-07-10 21:06:03 - train: epoch 0070, iter [02600, 05004], lr: 0.075334, loss: 2.5920
2022-07-10 21:06:37 - train: epoch 0070, iter [02700, 05004], lr: 0.075321, loss: 2.3663
2022-07-10 21:07:12 - train: epoch 0070, iter [02800, 05004], lr: 0.075307, loss: 2.5149
2022-07-10 21:07:46 - train: epoch 0070, iter [02900, 05004], lr: 0.075293, loss: 2.2606
2022-07-10 21:08:20 - train: epoch 0070, iter [03000, 05004], lr: 0.075279, loss: 2.5420
2022-07-10 21:08:55 - train: epoch 0070, iter [03100, 05004], lr: 0.075265, loss: 2.2977
2022-07-10 21:09:29 - train: epoch 0070, iter [03200, 05004], lr: 0.075251, loss: 2.6201
2022-07-10 21:10:04 - train: epoch 0070, iter [03300, 05004], lr: 0.075237, loss: 2.2810
2022-07-10 21:10:38 - train: epoch 0070, iter [03400, 05004], lr: 0.075223, loss: 2.5388
2022-07-10 21:11:13 - train: epoch 0070, iter [03500, 05004], lr: 0.075209, loss: 2.3688
2022-07-10 21:11:48 - train: epoch 0070, iter [03600, 05004], lr: 0.075195, loss: 2.6269
2022-07-10 21:12:22 - train: epoch 0070, iter [03700, 05004], lr: 0.075182, loss: 2.6056
2022-07-10 21:12:56 - train: epoch 0070, iter [03800, 05004], lr: 0.075168, loss: 2.2233
2022-07-10 21:13:31 - train: epoch 0070, iter [03900, 05004], lr: 0.075154, loss: 2.2187
2022-07-10 21:14:05 - train: epoch 0070, iter [04000, 05004], lr: 0.075140, loss: 2.6050
2022-07-10 21:14:40 - train: epoch 0070, iter [04100, 05004], lr: 0.075126, loss: 2.4758
2022-07-10 21:15:14 - train: epoch 0070, iter [04200, 05004], lr: 0.075112, loss: 2.3051
2022-07-10 21:15:49 - train: epoch 0070, iter [04300, 05004], lr: 0.075098, loss: 2.4445
2022-07-10 21:16:24 - train: epoch 0070, iter [04400, 05004], lr: 0.075084, loss: 2.6599
2022-07-10 21:16:58 - train: epoch 0070, iter [04500, 05004], lr: 0.075070, loss: 2.3519
2022-07-10 21:17:32 - train: epoch 0070, iter [04600, 05004], lr: 0.075056, loss: 2.6321
2022-07-10 21:18:07 - train: epoch 0070, iter [04700, 05004], lr: 0.075042, loss: 2.4611
2022-07-10 21:18:41 - train: epoch 0070, iter [04800, 05004], lr: 0.075028, loss: 2.5009
2022-07-10 21:19:16 - train: epoch 0070, iter [04900, 05004], lr: 0.075014, loss: 2.3960
2022-07-10 21:19:50 - train: epoch 0070, iter [05000, 05004], lr: 0.075001, loss: 2.2934
2022-07-10 21:19:51 - train: epoch 070, train_loss: 2.4079
2022-07-10 21:21:05 - eval: epoch: 070, acc1: 59.786%, acc5: 83.682%, test_loss: 1.6699, per_image_load_time: 2.385ms, per_image_inference_time: 0.465ms
2022-07-10 21:21:05 - until epoch: 070, best_acc1: 60.430%
2022-07-10 21:21:05 - epoch 071 lr: 0.075000
2022-07-10 21:21:45 - train: epoch 0071, iter [00100, 05004], lr: 0.074986, loss: 2.2784
2022-07-10 21:22:19 - train: epoch 0071, iter [00200, 05004], lr: 0.074972, loss: 2.3571
2022-07-10 21:22:53 - train: epoch 0071, iter [00300, 05004], lr: 0.074958, loss: 2.3594
2022-07-10 21:23:27 - train: epoch 0071, iter [00400, 05004], lr: 0.074944, loss: 2.3349
2022-07-10 21:24:02 - train: epoch 0071, iter [00500, 05004], lr: 0.074930, loss: 2.7135
2022-07-10 21:24:36 - train: epoch 0071, iter [00600, 05004], lr: 0.074916, loss: 2.5351
2022-07-10 21:25:10 - train: epoch 0071, iter [00700, 05004], lr: 0.074902, loss: 2.1610
2022-07-10 21:25:45 - train: epoch 0071, iter [00800, 05004], lr: 0.074888, loss: 2.3832
2022-07-10 21:26:20 - train: epoch 0071, iter [00900, 05004], lr: 0.074874, loss: 2.4397
2022-07-10 21:26:53 - train: epoch 0071, iter [01000, 05004], lr: 0.074860, loss: 2.3432
2022-07-10 21:27:29 - train: epoch 0071, iter [01100, 05004], lr: 0.074846, loss: 2.4065
2022-07-10 21:28:04 - train: epoch 0071, iter [01200, 05004], lr: 0.074833, loss: 2.2804
2022-07-10 21:28:38 - train: epoch 0071, iter [01300, 05004], lr: 0.074819, loss: 2.3170
2022-07-10 21:29:13 - train: epoch 0071, iter [01400, 05004], lr: 0.074805, loss: 2.2865
2022-07-10 21:29:47 - train: epoch 0071, iter [01500, 05004], lr: 0.074791, loss: 2.3408
2022-07-10 21:30:22 - train: epoch 0071, iter [01600, 05004], lr: 0.074777, loss: 2.1953
2022-07-10 21:30:57 - train: epoch 0071, iter [01700, 05004], lr: 0.074763, loss: 2.4159
2022-07-10 21:31:32 - train: epoch 0071, iter [01800, 05004], lr: 0.074749, loss: 2.6487
2022-07-10 21:32:06 - train: epoch 0071, iter [01900, 05004], lr: 0.074735, loss: 2.2224
2022-07-10 21:32:41 - train: epoch 0071, iter [02000, 05004], lr: 0.074721, loss: 2.3913
2022-07-10 21:33:16 - train: epoch 0071, iter [02100, 05004], lr: 0.074707, loss: 2.3373
2022-07-10 21:33:51 - train: epoch 0071, iter [02200, 05004], lr: 0.074693, loss: 2.3139
2022-07-10 21:34:25 - train: epoch 0071, iter [02300, 05004], lr: 0.074679, loss: 2.4457
2022-07-10 21:35:00 - train: epoch 0071, iter [02400, 05004], lr: 0.074665, loss: 2.3154
2022-07-10 21:35:35 - train: epoch 0071, iter [02500, 05004], lr: 0.074651, loss: 2.6220
2022-07-10 21:36:10 - train: epoch 0071, iter [02600, 05004], lr: 0.074637, loss: 2.2780
2022-07-10 21:36:44 - train: epoch 0071, iter [02700, 05004], lr: 0.074623, loss: 2.4822
2022-07-10 21:37:19 - train: epoch 0071, iter [02800, 05004], lr: 0.074609, loss: 2.4894
2022-07-10 21:37:54 - train: epoch 0071, iter [02900, 05004], lr: 0.074595, loss: 2.2194
2022-07-10 21:38:27 - train: epoch 0071, iter [03000, 05004], lr: 0.074581, loss: 2.4503
2022-07-10 21:39:01 - train: epoch 0071, iter [03100, 05004], lr: 0.074567, loss: 2.3120
2022-07-10 21:39:35 - train: epoch 0071, iter [03200, 05004], lr: 0.074553, loss: 2.4317
2022-07-10 21:40:09 - train: epoch 0071, iter [03300, 05004], lr: 0.074539, loss: 2.4876
2022-07-10 21:40:43 - train: epoch 0071, iter [03400, 05004], lr: 0.074525, loss: 2.4056
2022-07-10 21:41:16 - train: epoch 0071, iter [03500, 05004], lr: 0.074510, loss: 2.4597
2022-07-10 21:41:50 - train: epoch 0071, iter [03600, 05004], lr: 0.074496, loss: 2.5318
2022-07-10 21:42:24 - train: epoch 0071, iter [03700, 05004], lr: 0.074482, loss: 2.4969
2022-07-10 21:42:59 - train: epoch 0071, iter [03800, 05004], lr: 0.074468, loss: 2.3566
2022-07-10 21:43:32 - train: epoch 0071, iter [03900, 05004], lr: 0.074454, loss: 2.5139
2022-07-10 21:44:06 - train: epoch 0071, iter [04000, 05004], lr: 0.074440, loss: 2.2301
2022-07-10 21:44:40 - train: epoch 0071, iter [04100, 05004], lr: 0.074426, loss: 2.4822
2022-07-10 21:45:14 - train: epoch 0071, iter [04200, 05004], lr: 0.074412, loss: 2.7754
2022-07-10 21:45:47 - train: epoch 0071, iter [04300, 05004], lr: 0.074398, loss: 2.4343
2022-07-10 21:46:20 - train: epoch 0071, iter [04400, 05004], lr: 0.074384, loss: 2.6741
2022-07-10 21:46:54 - train: epoch 0071, iter [04500, 05004], lr: 0.074370, loss: 2.2748
2022-07-10 21:47:29 - train: epoch 0071, iter [04600, 05004], lr: 0.074356, loss: 2.2736
2022-07-10 21:48:03 - train: epoch 0071, iter [04700, 05004], lr: 0.074342, loss: 2.3933
2022-07-10 21:48:36 - train: epoch 0071, iter [04800, 05004], lr: 0.074328, loss: 2.5835
2022-07-10 21:49:10 - train: epoch 0071, iter [04900, 05004], lr: 0.074314, loss: 2.4145
2022-07-10 21:49:43 - train: epoch 0071, iter [05000, 05004], lr: 0.074300, loss: 2.2207
2022-07-10 21:49:44 - train: epoch 071, train_loss: 2.4030
2022-07-10 21:50:57 - eval: epoch: 071, acc1: 58.970%, acc5: 83.056%, test_loss: 1.7088, per_image_load_time: 2.151ms, per_image_inference_time: 0.429ms
2022-07-10 21:50:57 - until epoch: 071, best_acc1: 60.430%
2022-07-10 21:50:57 - epoch 072 lr: 0.074299
2022-07-10 21:51:36 - train: epoch 0072, iter [00100, 05004], lr: 0.074285, loss: 2.4178
2022-07-10 21:52:10 - train: epoch 0072, iter [00200, 05004], lr: 0.074271, loss: 2.2080
2022-07-10 21:52:43 - train: epoch 0072, iter [00300, 05004], lr: 0.074257, loss: 2.3052
2022-07-10 21:53:17 - train: epoch 0072, iter [00400, 05004], lr: 0.074243, loss: 2.6667
2022-07-10 21:53:51 - train: epoch 0072, iter [00500, 05004], lr: 0.074229, loss: 2.5642
2022-07-10 21:54:25 - train: epoch 0072, iter [00600, 05004], lr: 0.074215, loss: 2.3537
2022-07-10 21:54:58 - train: epoch 0072, iter [00700, 05004], lr: 0.074201, loss: 2.2778
2022-07-10 21:55:32 - train: epoch 0072, iter [00800, 05004], lr: 0.074187, loss: 2.5300
2022-07-10 21:56:06 - train: epoch 0072, iter [00900, 05004], lr: 0.074172, loss: 2.4932
2022-07-10 21:56:39 - train: epoch 0072, iter [01000, 05004], lr: 0.074158, loss: 2.3234
2022-07-10 21:57:14 - train: epoch 0072, iter [01100, 05004], lr: 0.074144, loss: 2.6271
2022-07-10 21:57:47 - train: epoch 0072, iter [01200, 05004], lr: 0.074130, loss: 2.3362
2022-07-10 21:58:21 - train: epoch 0072, iter [01300, 05004], lr: 0.074116, loss: 2.4208
2022-07-10 21:58:55 - train: epoch 0072, iter [01400, 05004], lr: 0.074102, loss: 2.4518
2022-07-10 21:59:29 - train: epoch 0072, iter [01500, 05004], lr: 0.074088, loss: 2.6031
2022-07-10 22:00:03 - train: epoch 0072, iter [01600, 05004], lr: 0.074074, loss: 2.3551
2022-07-10 22:00:37 - train: epoch 0072, iter [01700, 05004], lr: 0.074060, loss: 2.4565
2022-07-10 22:01:10 - train: epoch 0072, iter [01800, 05004], lr: 0.074046, loss: 2.4651
2022-07-10 22:01:45 - train: epoch 0072, iter [01900, 05004], lr: 0.074031, loss: 2.3854
2022-07-10 22:02:18 - train: epoch 0072, iter [02000, 05004], lr: 0.074017, loss: 2.6623
2022-07-10 22:02:52 - train: epoch 0072, iter [02100, 05004], lr: 0.074003, loss: 2.4532
2022-07-10 22:03:26 - train: epoch 0072, iter [02200, 05004], lr: 0.073989, loss: 2.5925
2022-07-10 22:04:00 - train: epoch 0072, iter [02300, 05004], lr: 0.073975, loss: 2.4154
2022-07-10 22:04:33 - train: epoch 0072, iter [02400, 05004], lr: 0.073961, loss: 2.5105
2022-07-10 22:05:08 - train: epoch 0072, iter [02500, 05004], lr: 0.073947, loss: 2.3660
2022-07-10 22:05:41 - train: epoch 0072, iter [02600, 05004], lr: 0.073933, loss: 2.4139
2022-07-10 22:06:15 - train: epoch 0072, iter [02700, 05004], lr: 0.073918, loss: 2.7545
2022-07-10 22:06:49 - train: epoch 0072, iter [02800, 05004], lr: 0.073904, loss: 2.4993
2022-07-10 22:07:22 - train: epoch 0072, iter [02900, 05004], lr: 0.073890, loss: 2.1880
2022-07-10 22:07:57 - train: epoch 0072, iter [03000, 05004], lr: 0.073876, loss: 2.0087
2022-07-10 22:08:31 - train: epoch 0072, iter [03100, 05004], lr: 0.073862, loss: 2.7103
2022-07-10 22:09:05 - train: epoch 0072, iter [03200, 05004], lr: 0.073848, loss: 2.3564
2022-07-10 22:09:39 - train: epoch 0072, iter [03300, 05004], lr: 0.073834, loss: 2.5051
2022-07-10 22:10:12 - train: epoch 0072, iter [03400, 05004], lr: 0.073819, loss: 2.7925
2022-07-10 22:10:46 - train: epoch 0072, iter [03500, 05004], lr: 0.073805, loss: 2.3448
2022-07-10 22:11:18 - train: epoch 0072, iter [03600, 05004], lr: 0.073791, loss: 2.5232
2022-07-10 22:11:52 - train: epoch 0072, iter [03700, 05004], lr: 0.073777, loss: 2.5437
2022-07-10 22:12:26 - train: epoch 0072, iter [03800, 05004], lr: 0.073763, loss: 2.8084
2022-07-10 22:13:00 - train: epoch 0072, iter [03900, 05004], lr: 0.073749, loss: 2.2475
2022-07-10 22:13:33 - train: epoch 0072, iter [04000, 05004], lr: 0.073734, loss: 2.3609
2022-07-10 22:14:07 - train: epoch 0072, iter [04100, 05004], lr: 0.073720, loss: 2.7737
2022-07-10 22:14:41 - train: epoch 0072, iter [04200, 05004], lr: 0.073706, loss: 2.3153
2022-07-10 22:15:14 - train: epoch 0072, iter [04300, 05004], lr: 0.073692, loss: 2.0380
2022-07-10 22:15:48 - train: epoch 0072, iter [04400, 05004], lr: 0.073678, loss: 2.4399
2022-07-10 22:16:22 - train: epoch 0072, iter [04500, 05004], lr: 0.073664, loss: 2.6560
2022-07-10 22:16:55 - train: epoch 0072, iter [04600, 05004], lr: 0.073649, loss: 2.1621
2022-07-10 22:17:29 - train: epoch 0072, iter [04700, 05004], lr: 0.073635, loss: 2.2422
2022-07-10 22:18:02 - train: epoch 0072, iter [04800, 05004], lr: 0.073621, loss: 2.4259
2022-07-10 22:18:36 - train: epoch 0072, iter [04900, 05004], lr: 0.073607, loss: 2.7110
2022-07-10 22:19:08 - train: epoch 0072, iter [05000, 05004], lr: 0.073593, loss: 2.2228
2022-07-10 22:19:09 - train: epoch 072, train_loss: 2.3968
2022-07-10 22:20:23 - eval: epoch: 072, acc1: 60.352%, acc5: 83.742%, test_loss: 1.6595, per_image_load_time: 2.441ms, per_image_inference_time: 0.431ms
2022-07-10 22:20:24 - until epoch: 072, best_acc1: 60.430%
2022-07-10 22:20:24 - epoch 073 lr: 0.073592
2022-07-10 22:21:03 - train: epoch 0073, iter [00100, 05004], lr: 0.073578, loss: 2.5085
2022-07-10 22:21:37 - train: epoch 0073, iter [00200, 05004], lr: 0.073564, loss: 2.3686
2022-07-10 22:22:10 - train: epoch 0073, iter [00300, 05004], lr: 0.073549, loss: 2.3005
2022-07-10 22:22:44 - train: epoch 0073, iter [00400, 05004], lr: 0.073535, loss: 2.2939
2022-07-10 22:23:18 - train: epoch 0073, iter [00500, 05004], lr: 0.073521, loss: 2.0963
2022-07-10 22:23:52 - train: epoch 0073, iter [00600, 05004], lr: 0.073507, loss: 2.3146
2022-07-10 22:24:26 - train: epoch 0073, iter [00700, 05004], lr: 0.073493, loss: 2.3151
2022-07-10 22:25:00 - train: epoch 0073, iter [00800, 05004], lr: 0.073478, loss: 2.2697
2022-07-10 22:25:33 - train: epoch 0073, iter [00900, 05004], lr: 0.073464, loss: 2.1044
2022-07-10 22:26:07 - train: epoch 0073, iter [01000, 05004], lr: 0.073450, loss: 2.4325
2022-07-10 22:26:40 - train: epoch 0073, iter [01100, 05004], lr: 0.073436, loss: 2.5725
2022-07-10 22:27:14 - train: epoch 0073, iter [01200, 05004], lr: 0.073422, loss: 2.4176
2022-07-10 22:27:48 - train: epoch 0073, iter [01300, 05004], lr: 0.073407, loss: 2.3381
2022-07-10 22:28:21 - train: epoch 0073, iter [01400, 05004], lr: 0.073393, loss: 2.2792
2022-07-10 22:28:55 - train: epoch 0073, iter [01500, 05004], lr: 0.073379, loss: 1.9936
2022-07-10 22:29:30 - train: epoch 0073, iter [01600, 05004], lr: 0.073365, loss: 2.5333
2022-07-10 22:30:03 - train: epoch 0073, iter [01700, 05004], lr: 0.073350, loss: 2.4708
2022-07-10 22:30:37 - train: epoch 0073, iter [01800, 05004], lr: 0.073336, loss: 1.9682
2022-07-10 22:31:11 - train: epoch 0073, iter [01900, 05004], lr: 0.073322, loss: 2.4694
2022-07-10 22:31:45 - train: epoch 0073, iter [02000, 05004], lr: 0.073308, loss: 2.0910
2022-07-10 22:32:18 - train: epoch 0073, iter [02100, 05004], lr: 0.073293, loss: 2.3395
2022-07-10 22:32:52 - train: epoch 0073, iter [02200, 05004], lr: 0.073279, loss: 2.3302
2022-07-10 22:33:26 - train: epoch 0073, iter [02300, 05004], lr: 0.073265, loss: 2.3911
2022-07-10 22:34:00 - train: epoch 0073, iter [02400, 05004], lr: 0.073251, loss: 2.3552
2022-07-10 22:34:34 - train: epoch 0073, iter [02500, 05004], lr: 0.073236, loss: 2.4104
2022-07-10 22:35:08 - train: epoch 0073, iter [02600, 05004], lr: 0.073222, loss: 2.3811
2022-07-10 22:35:42 - train: epoch 0073, iter [02700, 05004], lr: 0.073208, loss: 2.4402
2022-07-10 22:36:17 - train: epoch 0073, iter [02800, 05004], lr: 0.073194, loss: 2.3819
2022-07-10 22:36:50 - train: epoch 0073, iter [02900, 05004], lr: 0.073179, loss: 2.5346
2022-07-10 22:37:24 - train: epoch 0073, iter [03000, 05004], lr: 0.073165, loss: 2.2629
2022-07-10 22:37:58 - train: epoch 0073, iter [03100, 05004], lr: 0.073151, loss: 2.5623
2022-07-10 22:38:32 - train: epoch 0073, iter [03200, 05004], lr: 0.073137, loss: 2.1865
2022-07-10 22:39:05 - train: epoch 0073, iter [03300, 05004], lr: 0.073122, loss: 2.2338
2022-07-10 22:39:40 - train: epoch 0073, iter [03400, 05004], lr: 0.073108, loss: 2.4180
2022-07-10 22:40:13 - train: epoch 0073, iter [03500, 05004], lr: 0.073094, loss: 2.5629
2022-07-10 22:40:47 - train: epoch 0073, iter [03600, 05004], lr: 0.073080, loss: 2.3198
2022-07-10 22:41:21 - train: epoch 0073, iter [03700, 05004], lr: 0.073065, loss: 2.6164
2022-07-10 22:41:55 - train: epoch 0073, iter [03800, 05004], lr: 0.073051, loss: 2.5867
2022-07-10 22:42:30 - train: epoch 0073, iter [03900, 05004], lr: 0.073037, loss: 2.4544
2022-07-10 22:43:03 - train: epoch 0073, iter [04000, 05004], lr: 0.073022, loss: 2.6823
2022-07-10 22:43:37 - train: epoch 0073, iter [04100, 05004], lr: 0.073008, loss: 2.4763
2022-07-10 22:44:11 - train: epoch 0073, iter [04200, 05004], lr: 0.072994, loss: 2.3754
2022-07-10 22:44:45 - train: epoch 0073, iter [04300, 05004], lr: 0.072979, loss: 2.4675
2022-07-10 22:45:19 - train: epoch 0073, iter [04400, 05004], lr: 0.072965, loss: 2.5980
2022-07-10 22:45:52 - train: epoch 0073, iter [04500, 05004], lr: 0.072951, loss: 2.2848
2022-07-10 22:46:26 - train: epoch 0073, iter [04600, 05004], lr: 0.072937, loss: 2.4059
2022-07-10 22:47:00 - train: epoch 0073, iter [04700, 05004], lr: 0.072922, loss: 2.3107
2022-07-10 22:47:34 - train: epoch 0073, iter [04800, 05004], lr: 0.072908, loss: 2.3846
2022-07-10 22:48:07 - train: epoch 0073, iter [04900, 05004], lr: 0.072894, loss: 2.5427
2022-07-10 22:48:40 - train: epoch 0073, iter [05000, 05004], lr: 0.072879, loss: 2.5780
2022-07-10 22:48:41 - train: epoch 073, train_loss: 2.3895
2022-07-10 22:49:54 - eval: epoch: 073, acc1: 58.354%, acc5: 82.076%, test_loss: 1.7627, per_image_load_time: 1.809ms, per_image_inference_time: 0.437ms
2022-07-10 22:49:54 - until epoch: 073, best_acc1: 60.430%
2022-07-10 22:49:54 - epoch 074 lr: 0.072879
2022-07-10 22:50:34 - train: epoch 0074, iter [00100, 05004], lr: 0.072864, loss: 2.2385
2022-07-10 22:51:08 - train: epoch 0074, iter [00200, 05004], lr: 0.072850, loss: 2.4999
2022-07-10 22:51:41 - train: epoch 0074, iter [00300, 05004], lr: 0.072836, loss: 2.2447
2022-07-10 22:52:15 - train: epoch 0074, iter [00400, 05004], lr: 0.072822, loss: 2.3337
2022-07-10 22:52:48 - train: epoch 0074, iter [00500, 05004], lr: 0.072807, loss: 2.5650
2022-07-10 22:53:21 - train: epoch 0074, iter [00600, 05004], lr: 0.072793, loss: 2.0783
2022-07-10 22:53:54 - train: epoch 0074, iter [00700, 05004], lr: 0.072779, loss: 2.4597
2022-07-10 22:54:27 - train: epoch 0074, iter [00800, 05004], lr: 0.072764, loss: 2.5518
2022-07-10 22:55:01 - train: epoch 0074, iter [00900, 05004], lr: 0.072750, loss: 2.0583
2022-07-10 22:55:35 - train: epoch 0074, iter [01000, 05004], lr: 0.072736, loss: 2.5371
2022-07-10 22:56:08 - train: epoch 0074, iter [01100, 05004], lr: 0.072721, loss: 2.4365
2022-07-10 22:56:42 - train: epoch 0074, iter [01200, 05004], lr: 0.072707, loss: 2.3400
2022-07-10 22:57:14 - train: epoch 0074, iter [01300, 05004], lr: 0.072692, loss: 2.3551
2022-07-10 22:57:48 - train: epoch 0074, iter [01400, 05004], lr: 0.072678, loss: 2.1507
2022-07-10 22:58:21 - train: epoch 0074, iter [01500, 05004], lr: 0.072664, loss: 2.5213
2022-07-10 22:58:54 - train: epoch 0074, iter [01600, 05004], lr: 0.072649, loss: 2.1267
2022-07-10 22:59:27 - train: epoch 0074, iter [01700, 05004], lr: 0.072635, loss: 2.4266
2022-07-10 23:00:00 - train: epoch 0074, iter [01800, 05004], lr: 0.072621, loss: 2.5491
2022-07-10 23:00:33 - train: epoch 0074, iter [01900, 05004], lr: 0.072606, loss: 2.4316
2022-07-10 23:01:06 - train: epoch 0074, iter [02000, 05004], lr: 0.072592, loss: 1.9687
2022-07-10 23:01:38 - train: epoch 0074, iter [02100, 05004], lr: 0.072578, loss: 2.2445
2022-07-10 23:02:11 - train: epoch 0074, iter [02200, 05004], lr: 0.072563, loss: 2.2711
2022-07-10 23:02:44 - train: epoch 0074, iter [02300, 05004], lr: 0.072549, loss: 2.4241
2022-07-10 23:03:17 - train: epoch 0074, iter [02400, 05004], lr: 0.072535, loss: 2.4316
2022-07-10 23:03:50 - train: epoch 0074, iter [02500, 05004], lr: 0.072520, loss: 2.2465
2022-07-10 23:04:23 - train: epoch 0074, iter [02600, 05004], lr: 0.072506, loss: 1.8654
2022-07-10 23:04:56 - train: epoch 0074, iter [02700, 05004], lr: 0.072491, loss: 2.4404
2022-07-10 23:05:28 - train: epoch 0074, iter [02800, 05004], lr: 0.072477, loss: 2.7081
2022-07-10 23:06:01 - train: epoch 0074, iter [02900, 05004], lr: 0.072463, loss: 2.2186
2022-07-10 23:06:34 - train: epoch 0074, iter [03000, 05004], lr: 0.072448, loss: 2.4979
2022-07-10 23:07:06 - train: epoch 0074, iter [03100, 05004], lr: 0.072434, loss: 2.3600
2022-07-10 23:07:39 - train: epoch 0074, iter [03200, 05004], lr: 0.072420, loss: 2.2893
2022-07-10 23:08:12 - train: epoch 0074, iter [03300, 05004], lr: 0.072405, loss: 2.5216
2022-07-10 23:08:45 - train: epoch 0074, iter [03400, 05004], lr: 0.072391, loss: 2.2037
2022-07-10 23:09:18 - train: epoch 0074, iter [03500, 05004], lr: 0.072376, loss: 2.3552
2022-07-10 23:09:52 - train: epoch 0074, iter [03600, 05004], lr: 0.072362, loss: 2.4887
2022-07-10 23:10:24 - train: epoch 0074, iter [03700, 05004], lr: 0.072348, loss: 2.5856
2022-07-10 23:10:57 - train: epoch 0074, iter [03800, 05004], lr: 0.072333, loss: 2.2997
2022-07-10 23:11:30 - train: epoch 0074, iter [03900, 05004], lr: 0.072319, loss: 2.1373
2022-07-10 23:12:04 - train: epoch 0074, iter [04000, 05004], lr: 0.072304, loss: 2.5030
2022-07-10 23:12:36 - train: epoch 0074, iter [04100, 05004], lr: 0.072290, loss: 2.3587
2022-07-10 23:13:08 - train: epoch 0074, iter [04200, 05004], lr: 0.072276, loss: 2.3990
2022-07-10 23:13:42 - train: epoch 0074, iter [04300, 05004], lr: 0.072261, loss: 2.4026
2022-07-10 23:14:15 - train: epoch 0074, iter [04400, 05004], lr: 0.072247, loss: 2.4078
2022-07-10 23:14:49 - train: epoch 0074, iter [04500, 05004], lr: 0.072232, loss: 2.4808
2022-07-10 23:15:20 - train: epoch 0074, iter [04600, 05004], lr: 0.072218, loss: 2.4673
2022-07-10 23:15:53 - train: epoch 0074, iter [04700, 05004], lr: 0.072203, loss: 2.2595
2022-07-10 23:16:25 - train: epoch 0074, iter [04800, 05004], lr: 0.072189, loss: 2.4856
2022-07-10 23:16:59 - train: epoch 0074, iter [04900, 05004], lr: 0.072175, loss: 2.6628
2022-07-10 23:17:30 - train: epoch 0074, iter [05000, 05004], lr: 0.072160, loss: 2.3499
2022-07-10 23:17:31 - train: epoch 074, train_loss: 2.3877
2022-07-10 23:18:44 - eval: epoch: 074, acc1: 60.840%, acc5: 84.300%, test_loss: 1.6254, per_image_load_time: 2.013ms, per_image_inference_time: 0.359ms
2022-07-10 23:18:44 - until epoch: 074, best_acc1: 60.840%
2022-07-10 23:18:44 - epoch 075 lr: 0.072159
2022-07-10 23:19:24 - train: epoch 0075, iter [00100, 05004], lr: 0.072145, loss: 2.4720
2022-07-10 23:19:56 - train: epoch 0075, iter [00200, 05004], lr: 0.072131, loss: 2.3725
2022-07-10 23:20:28 - train: epoch 0075, iter [00300, 05004], lr: 0.072116, loss: 2.3172
2022-07-10 23:21:01 - train: epoch 0075, iter [00400, 05004], lr: 0.072102, loss: 2.4414
2022-07-10 23:21:34 - train: epoch 0075, iter [00500, 05004], lr: 0.072087, loss: 2.5364
2022-07-10 23:22:07 - train: epoch 0075, iter [00600, 05004], lr: 0.072073, loss: 2.4738
2022-07-10 23:22:40 - train: epoch 0075, iter [00700, 05004], lr: 0.072059, loss: 2.3402
2022-07-10 23:23:13 - train: epoch 0075, iter [00800, 05004], lr: 0.072044, loss: 2.5641
2022-07-10 23:23:45 - train: epoch 0075, iter [00900, 05004], lr: 0.072030, loss: 2.5537
2022-07-10 23:24:18 - train: epoch 0075, iter [01000, 05004], lr: 0.072015, loss: 2.1974
2022-07-10 23:24:50 - train: epoch 0075, iter [01100, 05004], lr: 0.072001, loss: 2.5367
2022-07-10 23:25:24 - train: epoch 0075, iter [01200, 05004], lr: 0.071986, loss: 2.3254
2022-07-10 23:25:56 - train: epoch 0075, iter [01300, 05004], lr: 0.071972, loss: 2.1733
2022-07-10 23:26:29 - train: epoch 0075, iter [01400, 05004], lr: 0.071957, loss: 2.3753
2022-07-10 23:27:01 - train: epoch 0075, iter [01500, 05004], lr: 0.071943, loss: 2.2859
2022-07-10 23:27:34 - train: epoch 0075, iter [01600, 05004], lr: 0.071928, loss: 2.4561
2022-07-10 23:28:06 - train: epoch 0075, iter [01700, 05004], lr: 0.071914, loss: 2.5034
2022-07-10 23:28:40 - train: epoch 0075, iter [01800, 05004], lr: 0.071899, loss: 2.4261
2022-07-10 23:29:12 - train: epoch 0075, iter [01900, 05004], lr: 0.071885, loss: 1.9624
2022-07-10 23:29:45 - train: epoch 0075, iter [02000, 05004], lr: 0.071871, loss: 2.4584
2022-07-10 23:30:18 - train: epoch 0075, iter [02100, 05004], lr: 0.071856, loss: 2.1784
2022-07-10 23:30:51 - train: epoch 0075, iter [02200, 05004], lr: 0.071842, loss: 2.3385
2022-07-10 23:31:23 - train: epoch 0075, iter [02300, 05004], lr: 0.071827, loss: 2.2999
2022-07-10 23:31:56 - train: epoch 0075, iter [02400, 05004], lr: 0.071813, loss: 2.4811
2022-07-10 23:32:28 - train: epoch 0075, iter [02500, 05004], lr: 0.071798, loss: 2.5514
2022-07-10 23:33:02 - train: epoch 0075, iter [02600, 05004], lr: 0.071784, loss: 2.4755
2022-07-10 23:33:35 - train: epoch 0075, iter [02700, 05004], lr: 0.071769, loss: 2.1953
2022-07-10 23:34:08 - train: epoch 0075, iter [02800, 05004], lr: 0.071755, loss: 2.4436
2022-07-10 23:34:40 - train: epoch 0075, iter [02900, 05004], lr: 0.071740, loss: 2.6122
2022-07-10 23:35:13 - train: epoch 0075, iter [03000, 05004], lr: 0.071726, loss: 2.3044
2022-07-10 23:35:47 - train: epoch 0075, iter [03100, 05004], lr: 0.071711, loss: 2.2671
2022-07-10 23:36:19 - train: epoch 0075, iter [03200, 05004], lr: 0.071697, loss: 2.3771
2022-07-10 23:36:52 - train: epoch 0075, iter [03300, 05004], lr: 0.071682, loss: 2.5128
2022-07-10 23:37:25 - train: epoch 0075, iter [03400, 05004], lr: 0.071668, loss: 2.4996
2022-07-10 23:37:58 - train: epoch 0075, iter [03500, 05004], lr: 0.071653, loss: 2.4101
2022-07-10 23:38:31 - train: epoch 0075, iter [03600, 05004], lr: 0.071639, loss: 2.7420
2022-07-10 23:39:04 - train: epoch 0075, iter [03700, 05004], lr: 0.071624, loss: 2.3937
2022-07-10 23:39:37 - train: epoch 0075, iter [03800, 05004], lr: 0.071610, loss: 2.4636
2022-07-10 23:40:10 - train: epoch 0075, iter [03900, 05004], lr: 0.071595, loss: 2.4837
2022-07-10 23:40:43 - train: epoch 0075, iter [04000, 05004], lr: 0.071581, loss: 2.2896
2022-07-10 23:41:16 - train: epoch 0075, iter [04100, 05004], lr: 0.071566, loss: 2.1935
2022-07-10 23:41:49 - train: epoch 0075, iter [04200, 05004], lr: 0.071551, loss: 2.2757
2022-07-10 23:42:22 - train: epoch 0075, iter [04300, 05004], lr: 0.071537, loss: 2.5321
2022-07-10 23:42:55 - train: epoch 0075, iter [04400, 05004], lr: 0.071522, loss: 2.2116
2022-07-10 23:43:28 - train: epoch 0075, iter [04500, 05004], lr: 0.071508, loss: 2.2193
2022-07-10 23:44:01 - train: epoch 0075, iter [04600, 05004], lr: 0.071493, loss: 2.3402
2022-07-10 23:44:34 - train: epoch 0075, iter [04700, 05004], lr: 0.071479, loss: 2.5169
2022-07-10 23:45:07 - train: epoch 0075, iter [04800, 05004], lr: 0.071464, loss: 2.2489
2022-07-10 23:45:41 - train: epoch 0075, iter [04900, 05004], lr: 0.071450, loss: 2.3061
2022-07-10 23:46:14 - train: epoch 0075, iter [05000, 05004], lr: 0.071435, loss: 2.4352
2022-07-10 23:46:15 - train: epoch 075, train_loss: 2.3810
2022-07-10 23:47:30 - eval: epoch: 075, acc1: 60.686%, acc5: 84.272%, test_loss: 1.6355, per_image_load_time: 2.321ms, per_image_inference_time: 0.469ms
2022-07-10 23:47:30 - until epoch: 075, best_acc1: 60.840%
2022-07-10 23:47:30 - epoch 076 lr: 0.071434
2022-07-10 23:48:10 - train: epoch 0076, iter [00100, 05004], lr: 0.071420, loss: 2.1568
2022-07-10 23:48:43 - train: epoch 0076, iter [00200, 05004], lr: 0.071406, loss: 1.9281
2022-07-10 23:49:17 - train: epoch 0076, iter [00300, 05004], lr: 0.071391, loss: 2.3610
2022-07-10 23:49:52 - train: epoch 0076, iter [00400, 05004], lr: 0.071376, loss: 2.6447
2022-07-10 23:50:26 - train: epoch 0076, iter [00500, 05004], lr: 0.071362, loss: 2.5562
2022-07-10 23:51:00 - train: epoch 0076, iter [00600, 05004], lr: 0.071347, loss: 2.2799
2022-07-10 23:51:34 - train: epoch 0076, iter [00700, 05004], lr: 0.071333, loss: 2.2423
2022-07-10 23:52:09 - train: epoch 0076, iter [00800, 05004], lr: 0.071318, loss: 2.3945
2022-07-10 23:52:44 - train: epoch 0076, iter [00900, 05004], lr: 0.071304, loss: 2.1744
2022-07-10 23:53:18 - train: epoch 0076, iter [01000, 05004], lr: 0.071289, loss: 2.4385
2022-07-10 23:53:52 - train: epoch 0076, iter [01100, 05004], lr: 0.071275, loss: 2.3104
2022-07-10 23:54:26 - train: epoch 0076, iter [01200, 05004], lr: 0.071260, loss: 2.5598
2022-07-10 23:55:01 - train: epoch 0076, iter [01300, 05004], lr: 0.071245, loss: 2.2518
2022-07-10 23:55:36 - train: epoch 0076, iter [01400, 05004], lr: 0.071231, loss: 2.3524
2022-07-10 23:56:10 - train: epoch 0076, iter [01500, 05004], lr: 0.071216, loss: 2.2612
2022-07-10 23:56:44 - train: epoch 0076, iter [01600, 05004], lr: 0.071202, loss: 2.1481
2022-07-10 23:57:18 - train: epoch 0076, iter [01700, 05004], lr: 0.071187, loss: 2.3095
2022-07-10 23:57:53 - train: epoch 0076, iter [01800, 05004], lr: 0.071172, loss: 2.1543
2022-07-10 23:58:27 - train: epoch 0076, iter [01900, 05004], lr: 0.071158, loss: 2.1493
2022-07-10 23:59:00 - train: epoch 0076, iter [02000, 05004], lr: 0.071143, loss: 2.6259
2022-07-10 23:59:33 - train: epoch 0076, iter [02100, 05004], lr: 0.071129, loss: 2.7030
2022-07-11 00:00:07 - train: epoch 0076, iter [02200, 05004], lr: 0.071114, loss: 2.2114
2022-07-11 00:00:42 - train: epoch 0076, iter [02300, 05004], lr: 0.071100, loss: 2.1803
2022-07-11 00:01:17 - train: epoch 0076, iter [02400, 05004], lr: 0.071085, loss: 2.4451
2022-07-11 00:01:50 - train: epoch 0076, iter [02500, 05004], lr: 0.071070, loss: 2.3987
2022-07-11 00:02:24 - train: epoch 0076, iter [02600, 05004], lr: 0.071056, loss: 2.5951
2022-07-11 00:02:58 - train: epoch 0076, iter [02700, 05004], lr: 0.071041, loss: 2.4105
2022-07-11 00:03:32 - train: epoch 0076, iter [02800, 05004], lr: 0.071027, loss: 2.2409
2022-07-11 00:04:06 - train: epoch 0076, iter [02900, 05004], lr: 0.071012, loss: 2.3843
2022-07-11 00:04:41 - train: epoch 0076, iter [03000, 05004], lr: 0.070997, loss: 2.0427
2022-07-11 00:05:16 - train: epoch 0076, iter [03100, 05004], lr: 0.070983, loss: 2.2345
2022-07-11 00:05:49 - train: epoch 0076, iter [03200, 05004], lr: 0.070968, loss: 2.3214
2022-07-11 00:06:23 - train: epoch 0076, iter [03300, 05004], lr: 0.070953, loss: 2.1351
2022-07-11 00:06:58 - train: epoch 0076, iter [03400, 05004], lr: 0.070939, loss: 2.2201
2022-07-11 00:07:32 - train: epoch 0076, iter [03500, 05004], lr: 0.070924, loss: 2.1981
2022-07-11 00:08:06 - train: epoch 0076, iter [03600, 05004], lr: 0.070910, loss: 2.1336
2022-07-11 00:08:40 - train: epoch 0076, iter [03700, 05004], lr: 0.070895, loss: 2.3603
2022-07-11 00:09:14 - train: epoch 0076, iter [03800, 05004], lr: 0.070880, loss: 2.3698
2022-07-11 00:09:48 - train: epoch 0076, iter [03900, 05004], lr: 0.070866, loss: 2.4365
2022-07-11 00:10:22 - train: epoch 0076, iter [04000, 05004], lr: 0.070851, loss: 2.4694
2022-07-11 00:10:56 - train: epoch 0076, iter [04100, 05004], lr: 0.070836, loss: 2.3687
2022-07-11 00:11:30 - train: epoch 0076, iter [04200, 05004], lr: 0.070822, loss: 2.3963
2022-07-11 00:12:05 - train: epoch 0076, iter [04300, 05004], lr: 0.070807, loss: 2.5546
2022-07-11 00:12:39 - train: epoch 0076, iter [04400, 05004], lr: 0.070793, loss: 2.3458
2022-07-11 00:13:13 - train: epoch 0076, iter [04500, 05004], lr: 0.070778, loss: 2.3585
2022-07-11 00:13:47 - train: epoch 0076, iter [04600, 05004], lr: 0.070763, loss: 2.5743
2022-07-11 00:14:21 - train: epoch 0076, iter [04700, 05004], lr: 0.070749, loss: 2.5755
2022-07-11 00:14:56 - train: epoch 0076, iter [04800, 05004], lr: 0.070734, loss: 2.3362
2022-07-11 00:15:30 - train: epoch 0076, iter [04900, 05004], lr: 0.070719, loss: 2.4050
2022-07-11 00:16:02 - train: epoch 0076, iter [05000, 05004], lr: 0.070705, loss: 2.6284
2022-07-11 00:16:03 - train: epoch 076, train_loss: 2.3730
2022-07-11 00:17:17 - eval: epoch: 076, acc1: 59.316%, acc5: 83.110%, test_loss: 1.7078, per_image_load_time: 2.377ms, per_image_inference_time: 0.403ms
2022-07-11 00:17:18 - until epoch: 076, best_acc1: 60.840%
2022-07-11 00:17:18 - epoch 077 lr: 0.070704
2022-07-11 00:17:57 - train: epoch 0077, iter [00100, 05004], lr: 0.070689, loss: 2.3607
2022-07-11 00:18:30 - train: epoch 0077, iter [00200, 05004], lr: 0.070675, loss: 2.4479
2022-07-11 00:19:03 - train: epoch 0077, iter [00300, 05004], lr: 0.070660, loss: 2.0584
2022-07-11 00:19:36 - train: epoch 0077, iter [00400, 05004], lr: 0.070645, loss: 2.6357
2022-07-11 00:20:11 - train: epoch 0077, iter [00500, 05004], lr: 0.070631, loss: 2.0947
2022-07-11 00:20:44 - train: epoch 0077, iter [00600, 05004], lr: 0.070616, loss: 2.1026
2022-07-11 00:21:19 - train: epoch 0077, iter [00700, 05004], lr: 0.070601, loss: 2.5468
2022-07-11 00:21:52 - train: epoch 0077, iter [00800, 05004], lr: 0.070587, loss: 2.2713
2022-07-11 00:22:25 - train: epoch 0077, iter [00900, 05004], lr: 0.070572, loss: 2.4952
2022-07-11 00:22:59 - train: epoch 0077, iter [01000, 05004], lr: 0.070557, loss: 2.2911
2022-07-11 00:23:33 - train: epoch 0077, iter [01100, 05004], lr: 0.070543, loss: 2.2628
2022-07-11 00:24:08 - train: epoch 0077, iter [01200, 05004], lr: 0.070528, loss: 2.2492
2022-07-11 00:24:41 - train: epoch 0077, iter [01300, 05004], lr: 0.070513, loss: 2.3062
2022-07-11 00:25:14 - train: epoch 0077, iter [01400, 05004], lr: 0.070499, loss: 2.3305
2022-07-11 00:25:49 - train: epoch 0077, iter [01500, 05004], lr: 0.070484, loss: 2.3285
2022-07-11 00:26:23 - train: epoch 0077, iter [01600, 05004], lr: 0.070469, loss: 2.1585
2022-07-11 00:26:57 - train: epoch 0077, iter [01700, 05004], lr: 0.070455, loss: 2.5472
2022-07-11 00:27:31 - train: epoch 0077, iter [01800, 05004], lr: 0.070440, loss: 2.2471
2022-07-11 00:28:04 - train: epoch 0077, iter [01900, 05004], lr: 0.070425, loss: 2.3850
2022-07-11 00:28:38 - train: epoch 0077, iter [02000, 05004], lr: 0.070411, loss: 2.4076
2022-07-11 00:29:12 - train: epoch 0077, iter [02100, 05004], lr: 0.070396, loss: 2.6692
2022-07-11 00:29:46 - train: epoch 0077, iter [02200, 05004], lr: 0.070381, loss: 2.1420
2022-07-11 00:30:20 - train: epoch 0077, iter [02300, 05004], lr: 0.070367, loss: 2.3459
2022-07-11 00:30:54 - train: epoch 0077, iter [02400, 05004], lr: 0.070352, loss: 2.4811
2022-07-11 00:31:29 - train: epoch 0077, iter [02500, 05004], lr: 0.070337, loss: 2.4038
2022-07-11 00:32:03 - train: epoch 0077, iter [02600, 05004], lr: 0.070322, loss: 2.4471
2022-07-11 00:32:37 - train: epoch 0077, iter [02700, 05004], lr: 0.070308, loss: 2.3558
2022-07-11 00:33:11 - train: epoch 0077, iter [02800, 05004], lr: 0.070293, loss: 2.3019
2022-07-11 00:33:45 - train: epoch 0077, iter [02900, 05004], lr: 0.070278, loss: 2.6755
2022-07-11 00:34:19 - train: epoch 0077, iter [03000, 05004], lr: 0.070264, loss: 2.1460
2022-07-11 00:34:53 - train: epoch 0077, iter [03100, 05004], lr: 0.070249, loss: 2.2339
2022-07-11 00:35:27 - train: epoch 0077, iter [03200, 05004], lr: 0.070234, loss: 2.7251
2022-07-11 00:36:01 - train: epoch 0077, iter [03300, 05004], lr: 0.070219, loss: 2.3486
2022-07-11 00:36:34 - train: epoch 0077, iter [03400, 05004], lr: 0.070205, loss: 2.4174
2022-07-11 00:37:09 - train: epoch 0077, iter [03500, 05004], lr: 0.070190, loss: 2.0496
2022-07-11 00:37:43 - train: epoch 0077, iter [03600, 05004], lr: 0.070175, loss: 2.2458
2022-07-11 00:38:16 - train: epoch 0077, iter [03700, 05004], lr: 0.070161, loss: 2.1050
2022-07-11 00:38:50 - train: epoch 0077, iter [03800, 05004], lr: 0.070146, loss: 2.2736
2022-07-11 00:39:25 - train: epoch 0077, iter [03900, 05004], lr: 0.070131, loss: 2.2484
2022-07-11 00:39:59 - train: epoch 0077, iter [04000, 05004], lr: 0.070116, loss: 2.3880
2022-07-11 00:40:34 - train: epoch 0077, iter [04100, 05004], lr: 0.070102, loss: 2.2906
2022-07-11 00:41:07 - train: epoch 0077, iter [04200, 05004], lr: 0.070087, loss: 2.2895
2022-07-11 00:41:41 - train: epoch 0077, iter [04300, 05004], lr: 0.070072, loss: 2.1912
2022-07-11 00:42:15 - train: epoch 0077, iter [04400, 05004], lr: 0.070057, loss: 2.3818
2022-07-11 00:42:49 - train: epoch 0077, iter [04500, 05004], lr: 0.070043, loss: 2.5038
2022-07-11 00:43:24 - train: epoch 0077, iter [04600, 05004], lr: 0.070028, loss: 2.2522
2022-07-11 00:43:57 - train: epoch 0077, iter [04700, 05004], lr: 0.070013, loss: 2.3804
2022-07-11 00:44:31 - train: epoch 0077, iter [04800, 05004], lr: 0.069998, loss: 2.0752
2022-07-11 00:45:06 - train: epoch 0077, iter [04900, 05004], lr: 0.069984, loss: 2.6012
2022-07-11 00:45:38 - train: epoch 0077, iter [05000, 05004], lr: 0.069969, loss: 2.3316
2022-07-11 00:45:39 - train: epoch 077, train_loss: 2.3696
2022-07-11 00:46:53 - eval: epoch: 077, acc1: 61.764%, acc5: 84.920%, test_loss: 1.5871, per_image_load_time: 1.973ms, per_image_inference_time: 0.428ms
2022-07-11 00:46:53 - until epoch: 077, best_acc1: 61.764%
2022-07-11 00:46:53 - epoch 078 lr: 0.069968
2022-07-11 00:47:33 - train: epoch 0078, iter [00100, 05004], lr: 0.069953, loss: 2.0652
2022-07-11 00:48:06 - train: epoch 0078, iter [00200, 05004], lr: 0.069939, loss: 2.3940
2022-07-11 00:48:40 - train: epoch 0078, iter [00300, 05004], lr: 0.069924, loss: 2.2284
2022-07-11 00:49:14 - train: epoch 0078, iter [00400, 05004], lr: 0.069909, loss: 2.4457
2022-07-11 00:49:47 - train: epoch 0078, iter [00500, 05004], lr: 0.069894, loss: 2.2395
2022-07-11 00:50:21 - train: epoch 0078, iter [00600, 05004], lr: 0.069880, loss: 2.1938
2022-07-11 00:50:55 - train: epoch 0078, iter [00700, 05004], lr: 0.069865, loss: 2.4367
2022-07-11 00:51:30 - train: epoch 0078, iter [00800, 05004], lr: 0.069850, loss: 2.2574
2022-07-11 00:52:02 - train: epoch 0078, iter [00900, 05004], lr: 0.069835, loss: 2.4071
2022-07-11 00:52:36 - train: epoch 0078, iter [01000, 05004], lr: 0.069821, loss: 2.2104
2022-07-11 00:53:11 - train: epoch 0078, iter [01100, 05004], lr: 0.069806, loss: 2.4459
2022-07-11 00:53:45 - train: epoch 0078, iter [01200, 05004], lr: 0.069791, loss: 2.4814
2022-07-11 00:54:19 - train: epoch 0078, iter [01300, 05004], lr: 0.069776, loss: 2.2931
2022-07-11 00:54:53 - train: epoch 0078, iter [01400, 05004], lr: 0.069761, loss: 1.8462
2022-07-11 00:55:26 - train: epoch 0078, iter [01500, 05004], lr: 0.069747, loss: 2.4978
2022-07-11 00:56:01 - train: epoch 0078, iter [01600, 05004], lr: 0.069732, loss: 2.4445
2022-07-11 00:56:35 - train: epoch 0078, iter [01700, 05004], lr: 0.069717, loss: 2.1394
2022-07-11 00:57:09 - train: epoch 0078, iter [01800, 05004], lr: 0.069702, loss: 2.1760
2022-07-11 00:57:42 - train: epoch 0078, iter [01900, 05004], lr: 0.069687, loss: 2.4847
2022-07-11 00:58:17 - train: epoch 0078, iter [02000, 05004], lr: 0.069673, loss: 2.5800
2022-07-11 00:58:52 - train: epoch 0078, iter [02100, 05004], lr: 0.069658, loss: 2.4659
2022-07-11 00:59:26 - train: epoch 0078, iter [02200, 05004], lr: 0.069643, loss: 2.3821
2022-07-11 01:00:00 - train: epoch 0078, iter [02300, 05004], lr: 0.069628, loss: 2.4988
2022-07-11 01:00:35 - train: epoch 0078, iter [02400, 05004], lr: 0.069613, loss: 2.5324
2022-07-11 01:01:09 - train: epoch 0078, iter [02500, 05004], lr: 0.069599, loss: 2.1689
2022-07-11 01:01:43 - train: epoch 0078, iter [02600, 05004], lr: 0.069584, loss: 2.3511
2022-07-11 01:02:17 - train: epoch 0078, iter [02700, 05004], lr: 0.069569, loss: 2.3296
2022-07-11 01:02:51 - train: epoch 0078, iter [02800, 05004], lr: 0.069554, loss: 2.3728
2022-07-11 01:03:24 - train: epoch 0078, iter [02900, 05004], lr: 0.069539, loss: 2.3819
2022-07-11 01:03:58 - train: epoch 0078, iter [03000, 05004], lr: 0.069525, loss: 2.4063
2022-07-11 01:04:32 - train: epoch 0078, iter [03100, 05004], lr: 0.069510, loss: 2.9214
2022-07-11 01:05:07 - train: epoch 0078, iter [03200, 05004], lr: 0.069495, loss: 2.4090
2022-07-11 01:05:40 - train: epoch 0078, iter [03300, 05004], lr: 0.069480, loss: 2.4364
2022-07-11 01:06:14 - train: epoch 0078, iter [03400, 05004], lr: 0.069465, loss: 2.3689
2022-07-11 01:06:49 - train: epoch 0078, iter [03500, 05004], lr: 0.069450, loss: 2.4706
2022-07-11 01:07:23 - train: epoch 0078, iter [03600, 05004], lr: 0.069436, loss: 2.4557
2022-07-11 01:07:57 - train: epoch 0078, iter [03700, 05004], lr: 0.069421, loss: 2.2832
2022-07-11 01:08:31 - train: epoch 0078, iter [03800, 05004], lr: 0.069406, loss: 2.3611
2022-07-11 01:09:05 - train: epoch 0078, iter [03900, 05004], lr: 0.069391, loss: 2.3232
2022-07-11 01:09:39 - train: epoch 0078, iter [04000, 05004], lr: 0.069376, loss: 2.1997
2022-07-11 01:10:13 - train: epoch 0078, iter [04100, 05004], lr: 0.069361, loss: 2.3220
2022-07-11 01:10:48 - train: epoch 0078, iter [04200, 05004], lr: 0.069347, loss: 2.4151
2022-07-11 01:11:22 - train: epoch 0078, iter [04300, 05004], lr: 0.069332, loss: 2.4051
2022-07-11 01:11:55 - train: epoch 0078, iter [04400, 05004], lr: 0.069317, loss: 2.1980
2022-07-11 01:12:29 - train: epoch 0078, iter [04500, 05004], lr: 0.069302, loss: 2.3229
2022-07-11 01:13:03 - train: epoch 0078, iter [04600, 05004], lr: 0.069287, loss: 2.1098
2022-07-11 01:13:38 - train: epoch 0078, iter [04700, 05004], lr: 0.069272, loss: 2.4471
2022-07-11 01:14:12 - train: epoch 0078, iter [04800, 05004], lr: 0.069257, loss: 2.5508
2022-07-11 01:14:46 - train: epoch 0078, iter [04900, 05004], lr: 0.069243, loss: 2.1534
2022-07-11 01:15:18 - train: epoch 0078, iter [05000, 05004], lr: 0.069228, loss: 2.5379
2022-07-11 01:15:19 - train: epoch 078, train_loss: 2.3624
2022-07-11 01:16:35 - eval: epoch: 078, acc1: 60.732%, acc5: 84.394%, test_loss: 1.6265, per_image_load_time: 2.488ms, per_image_inference_time: 0.445ms
2022-07-11 01:16:35 - until epoch: 078, best_acc1: 61.764%
2022-07-11 01:16:35 - epoch 079 lr: 0.069227
2022-07-11 01:17:14 - train: epoch 0079, iter [00100, 05004], lr: 0.069212, loss: 2.4045
2022-07-11 01:17:48 - train: epoch 0079, iter [00200, 05004], lr: 0.069197, loss: 2.1650
2022-07-11 01:18:22 - train: epoch 0079, iter [00300, 05004], lr: 0.069183, loss: 2.2028
2022-07-11 01:18:56 - train: epoch 0079, iter [00400, 05004], lr: 0.069168, loss: 2.6663
2022-07-11 01:19:30 - train: epoch 0079, iter [00500, 05004], lr: 0.069153, loss: 2.4162
2022-07-11 01:20:04 - train: epoch 0079, iter [00600, 05004], lr: 0.069138, loss: 2.6317
2022-07-11 01:20:38 - train: epoch 0079, iter [00700, 05004], lr: 0.069123, loss: 2.2707
2022-07-11 01:21:12 - train: epoch 0079, iter [00800, 05004], lr: 0.069108, loss: 2.1323
2022-07-11 01:21:46 - train: epoch 0079, iter [00900, 05004], lr: 0.069093, loss: 2.2578
2022-07-11 01:22:20 - train: epoch 0079, iter [01000, 05004], lr: 0.069078, loss: 2.2890
2022-07-11 01:22:54 - train: epoch 0079, iter [01100, 05004], lr: 0.069064, loss: 2.6656
2022-07-11 01:23:28 - train: epoch 0079, iter [01200, 05004], lr: 0.069049, loss: 2.6745
2022-07-11 01:24:02 - train: epoch 0079, iter [01300, 05004], lr: 0.069034, loss: 2.3512
2022-07-11 01:24:36 - train: epoch 0079, iter [01400, 05004], lr: 0.069019, loss: 2.1736
2022-07-11 01:25:10 - train: epoch 0079, iter [01500, 05004], lr: 0.069004, loss: 2.3886
2022-07-11 01:25:44 - train: epoch 0079, iter [01600, 05004], lr: 0.068989, loss: 2.2225
2022-07-11 01:26:19 - train: epoch 0079, iter [01700, 05004], lr: 0.068974, loss: 2.5730
2022-07-11 01:26:53 - train: epoch 0079, iter [01800, 05004], lr: 0.068959, loss: 2.3741
2022-07-11 01:27:27 - train: epoch 0079, iter [01900, 05004], lr: 0.068944, loss: 1.9843
2022-07-11 01:28:01 - train: epoch 0079, iter [02000, 05004], lr: 0.068930, loss: 2.5998
2022-07-11 01:28:35 - train: epoch 0079, iter [02100, 05004], lr: 0.068915, loss: 2.2222
2022-07-11 01:29:10 - train: epoch 0079, iter [02200, 05004], lr: 0.068900, loss: 2.5415
2022-07-11 01:29:45 - train: epoch 0079, iter [02300, 05004], lr: 0.068885, loss: 2.2912
2022-07-11 01:30:19 - train: epoch 0079, iter [02400, 05004], lr: 0.068870, loss: 2.4815
2022-07-11 01:30:53 - train: epoch 0079, iter [02500, 05004], lr: 0.068855, loss: 2.4317
2022-07-11 01:31:26 - train: epoch 0079, iter [02600, 05004], lr: 0.068840, loss: 2.2080
2022-07-11 01:32:01 - train: epoch 0079, iter [02700, 05004], lr: 0.068825, loss: 2.2005
2022-07-11 01:32:35 - train: epoch 0079, iter [02800, 05004], lr: 0.068810, loss: 2.0776
2022-07-11 01:33:09 - train: epoch 0079, iter [02900, 05004], lr: 0.068795, loss: 2.3321
2022-07-11 01:33:43 - train: epoch 0079, iter [03000, 05004], lr: 0.068780, loss: 2.1514
2022-07-11 01:34:17 - train: epoch 0079, iter [03100, 05004], lr: 0.068766, loss: 2.2149
2022-07-11 01:34:52 - train: epoch 0079, iter [03200, 05004], lr: 0.068751, loss: 2.5350
2022-07-11 01:35:26 - train: epoch 0079, iter [03300, 05004], lr: 0.068736, loss: 2.1103
2022-07-11 01:36:01 - train: epoch 0079, iter [03400, 05004], lr: 0.068721, loss: 2.4744
2022-07-11 01:36:35 - train: epoch 0079, iter [03500, 05004], lr: 0.068706, loss: 2.6870
2022-07-11 01:37:09 - train: epoch 0079, iter [03600, 05004], lr: 0.068691, loss: 2.3237
2022-07-11 01:37:43 - train: epoch 0079, iter [03700, 05004], lr: 0.068676, loss: 2.1542
2022-07-11 01:38:18 - train: epoch 0079, iter [03800, 05004], lr: 0.068661, loss: 2.4547
2022-07-11 01:38:53 - train: epoch 0079, iter [03900, 05004], lr: 0.068646, loss: 2.3258
2022-07-11 01:39:28 - train: epoch 0079, iter [04000, 05004], lr: 0.068631, loss: 2.2116
2022-07-11 01:40:02 - train: epoch 0079, iter [04100, 05004], lr: 0.068616, loss: 2.4012
2022-07-11 01:40:36 - train: epoch 0079, iter [04200, 05004], lr: 0.068601, loss: 2.1947
2022-07-11 01:41:12 - train: epoch 0079, iter [04300, 05004], lr: 0.068586, loss: 2.0593
2022-07-11 01:41:46 - train: epoch 0079, iter [04400, 05004], lr: 0.068571, loss: 2.6190
2022-07-11 01:42:20 - train: epoch 0079, iter [04500, 05004], lr: 0.068556, loss: 2.5343
2022-07-11 01:42:54 - train: epoch 0079, iter [04600, 05004], lr: 0.068542, loss: 2.2505
2022-07-11 01:43:28 - train: epoch 0079, iter [04700, 05004], lr: 0.068527, loss: 2.4725
2022-07-11 01:44:03 - train: epoch 0079, iter [04800, 05004], lr: 0.068512, loss: 2.2971
2022-07-11 01:44:38 - train: epoch 0079, iter [04900, 05004], lr: 0.068497, loss: 2.2788
2022-07-11 01:45:11 - train: epoch 0079, iter [05000, 05004], lr: 0.068482, loss: 2.0504
2022-07-11 01:45:12 - train: epoch 079, train_loss: 2.3602
2022-07-11 01:46:26 - eval: epoch: 079, acc1: 60.564%, acc5: 84.234%, test_loss: 1.6365, per_image_load_time: 1.717ms, per_image_inference_time: 0.434ms
2022-07-11 01:46:27 - until epoch: 079, best_acc1: 61.764%
2022-07-11 01:46:27 - epoch 080 lr: 0.068481
2022-07-11 01:47:07 - train: epoch 0080, iter [00100, 05004], lr: 0.068466, loss: 2.0995
2022-07-11 01:47:43 - train: epoch 0080, iter [00200, 05004], lr: 0.068451, loss: 2.3124
2022-07-11 01:48:16 - train: epoch 0080, iter [00300, 05004], lr: 0.068436, loss: 2.2143
2022-07-11 01:48:49 - train: epoch 0080, iter [00400, 05004], lr: 0.068421, loss: 2.1313
2022-07-11 01:49:24 - train: epoch 0080, iter [00500, 05004], lr: 0.068406, loss: 2.1539
2022-07-11 01:49:57 - train: epoch 0080, iter [00600, 05004], lr: 0.068391, loss: 2.3806
2022-07-11 01:50:31 - train: epoch 0080, iter [00700, 05004], lr: 0.068376, loss: 2.3217
2022-07-11 01:51:06 - train: epoch 0080, iter [00800, 05004], lr: 0.068361, loss: 2.1261
2022-07-11 01:51:40 - train: epoch 0080, iter [00900, 05004], lr: 0.068346, loss: 2.2766
2022-07-11 01:52:13 - train: epoch 0080, iter [01000, 05004], lr: 0.068331, loss: 2.0956
2022-07-11 01:52:48 - train: epoch 0080, iter [01100, 05004], lr: 0.068316, loss: 2.4858
2022-07-11 01:53:21 - train: epoch 0080, iter [01200, 05004], lr: 0.068301, loss: 2.3351
2022-07-11 01:53:56 - train: epoch 0080, iter [01300, 05004], lr: 0.068286, loss: 2.5221
2022-07-11 01:54:30 - train: epoch 0080, iter [01400, 05004], lr: 0.068272, loss: 2.2970
2022-07-11 01:55:05 - train: epoch 0080, iter [01500, 05004], lr: 0.068257, loss: 2.0232
2022-07-11 01:55:40 - train: epoch 0080, iter [01600, 05004], lr: 0.068242, loss: 2.4283
2022-07-11 01:56:13 - train: epoch 0080, iter [01700, 05004], lr: 0.068227, loss: 2.4117
2022-07-11 01:56:48 - train: epoch 0080, iter [01800, 05004], lr: 0.068212, loss: 2.4529
2022-07-11 01:57:23 - train: epoch 0080, iter [01900, 05004], lr: 0.068197, loss: 2.4436
2022-07-11 01:57:58 - train: epoch 0080, iter [02000, 05004], lr: 0.068182, loss: 2.2962
2022-07-11 01:58:32 - train: epoch 0080, iter [02100, 05004], lr: 0.068167, loss: 2.5821
2022-07-11 01:59:06 - train: epoch 0080, iter [02200, 05004], lr: 0.068152, loss: 2.4787
2022-07-11 01:59:40 - train: epoch 0080, iter [02300, 05004], lr: 0.068137, loss: 2.2153
2022-07-11 02:00:15 - train: epoch 0080, iter [02400, 05004], lr: 0.068122, loss: 2.6247
2022-07-11 02:00:50 - train: epoch 0080, iter [02500, 05004], lr: 0.068107, loss: 2.3344
2022-07-11 02:01:24 - train: epoch 0080, iter [02600, 05004], lr: 0.068092, loss: 2.1691
2022-07-11 02:01:59 - train: epoch 0080, iter [02700, 05004], lr: 0.068077, loss: 2.4290
2022-07-11 02:02:33 - train: epoch 0080, iter [02800, 05004], lr: 0.068062, loss: 2.5090
2022-07-11 02:03:08 - train: epoch 0080, iter [02900, 05004], lr: 0.068047, loss: 2.3236
2022-07-11 02:03:42 - train: epoch 0080, iter [03000, 05004], lr: 0.068032, loss: 2.4150
2022-07-11 02:04:16 - train: epoch 0080, iter [03100, 05004], lr: 0.068016, loss: 2.3681
2022-07-11 02:04:52 - train: epoch 0080, iter [03200, 05004], lr: 0.068001, loss: 2.2550
2022-07-11 02:05:26 - train: epoch 0080, iter [03300, 05004], lr: 0.067986, loss: 2.2511
2022-07-11 02:06:01 - train: epoch 0080, iter [03400, 05004], lr: 0.067971, loss: 2.3995
2022-07-11 02:06:36 - train: epoch 0080, iter [03500, 05004], lr: 0.067956, loss: 2.4601
2022-07-11 02:07:11 - train: epoch 0080, iter [03600, 05004], lr: 0.067941, loss: 2.6367
2022-07-11 02:07:46 - train: epoch 0080, iter [03700, 05004], lr: 0.067926, loss: 2.3845
2022-07-11 02:08:20 - train: epoch 0080, iter [03800, 05004], lr: 0.067911, loss: 2.5597
2022-07-11 02:08:55 - train: epoch 0080, iter [03900, 05004], lr: 0.067896, loss: 2.1424
2022-07-11 02:09:29 - train: epoch 0080, iter [04000, 05004], lr: 0.067881, loss: 2.3162
2022-07-11 02:10:04 - train: epoch 0080, iter [04100, 05004], lr: 0.067866, loss: 2.3585
2022-07-11 02:10:39 - train: epoch 0080, iter [04200, 05004], lr: 0.067851, loss: 2.4142
2022-07-11 02:11:14 - train: epoch 0080, iter [04300, 05004], lr: 0.067836, loss: 2.4441
2022-07-11 02:11:48 - train: epoch 0080, iter [04400, 05004], lr: 0.067821, loss: 2.4328
2022-07-11 02:12:23 - train: epoch 0080, iter [04500, 05004], lr: 0.067806, loss: 2.2145
2022-07-11 02:12:57 - train: epoch 0080, iter [04600, 05004], lr: 0.067791, loss: 2.5369
2022-07-11 02:13:32 - train: epoch 0080, iter [04700, 05004], lr: 0.067776, loss: 2.5516
2022-07-11 02:14:07 - train: epoch 0080, iter [04800, 05004], lr: 0.067761, loss: 2.3639
2022-07-11 02:14:41 - train: epoch 0080, iter [04900, 05004], lr: 0.067746, loss: 2.6663
2022-07-11 02:15:15 - train: epoch 0080, iter [05000, 05004], lr: 0.067731, loss: 2.2265
2022-07-11 02:15:16 - train: epoch 080, train_loss: 2.3532
2022-07-11 02:16:31 - eval: epoch: 080, acc1: 60.138%, acc5: 83.670%, test_loss: 1.6618, per_image_load_time: 2.309ms, per_image_inference_time: 0.462ms
2022-07-11 02:16:32 - until epoch: 080, best_acc1: 61.764%
2022-07-11 02:16:32 - epoch 081 lr: 0.067730
2022-07-11 02:17:11 - train: epoch 0081, iter [00100, 05004], lr: 0.067715, loss: 2.1516
2022-07-11 02:17:44 - train: epoch 0081, iter [00200, 05004], lr: 0.067700, loss: 2.2030
2022-07-11 02:18:18 - train: epoch 0081, iter [00300, 05004], lr: 0.067685, loss: 2.4405
2022-07-11 02:18:53 - train: epoch 0081, iter [00400, 05004], lr: 0.067670, loss: 2.3219
2022-07-11 02:19:27 - train: epoch 0081, iter [00500, 05004], lr: 0.067655, loss: 2.6147
2022-07-11 02:20:01 - train: epoch 0081, iter [00600, 05004], lr: 0.067640, loss: 2.1707
2022-07-11 02:20:34 - train: epoch 0081, iter [00700, 05004], lr: 0.067625, loss: 2.2885
2022-07-11 02:21:08 - train: epoch 0081, iter [00800, 05004], lr: 0.067610, loss: 2.2054
2022-07-11 02:21:43 - train: epoch 0081, iter [00900, 05004], lr: 0.067595, loss: 2.6030
2022-07-11 02:22:17 - train: epoch 0081, iter [01000, 05004], lr: 0.067580, loss: 2.3122
2022-07-11 02:22:52 - train: epoch 0081, iter [01100, 05004], lr: 0.067565, loss: 2.1391
2022-07-11 02:23:26 - train: epoch 0081, iter [01200, 05004], lr: 0.067549, loss: 2.5258
2022-07-11 02:23:59 - train: epoch 0081, iter [01300, 05004], lr: 0.067534, loss: 2.3250
2022-07-11 02:24:34 - train: epoch 0081, iter [01400, 05004], lr: 0.067519, loss: 2.4018
2022-07-11 02:25:08 - train: epoch 0081, iter [01500, 05004], lr: 0.067504, loss: 2.3925
2022-07-11 02:25:43 - train: epoch 0081, iter [01600, 05004], lr: 0.067489, loss: 2.2283
2022-07-11 02:26:17 - train: epoch 0081, iter [01700, 05004], lr: 0.067474, loss: 2.3826
2022-07-11 02:26:51 - train: epoch 0081, iter [01800, 05004], lr: 0.067459, loss: 2.4657
2022-07-11 02:27:26 - train: epoch 0081, iter [01900, 05004], lr: 0.067444, loss: 2.2540
2022-07-11 02:28:00 - train: epoch 0081, iter [02000, 05004], lr: 0.067429, loss: 2.6725
2022-07-11 02:28:35 - train: epoch 0081, iter [02100, 05004], lr: 0.067414, loss: 2.4306
2022-07-11 02:29:09 - train: epoch 0081, iter [02200, 05004], lr: 0.067399, loss: 2.2624
2022-07-11 02:29:42 - train: epoch 0081, iter [02300, 05004], lr: 0.067384, loss: 2.1478
2022-07-11 02:30:16 - train: epoch 0081, iter [02400, 05004], lr: 0.067368, loss: 2.2759
2022-07-11 02:30:50 - train: epoch 0081, iter [02500, 05004], lr: 0.067353, loss: 2.6083
2022-07-11 02:31:25 - train: epoch 0081, iter [02600, 05004], lr: 0.067338, loss: 2.5594
2022-07-11 02:32:00 - train: epoch 0081, iter [02700, 05004], lr: 0.067323, loss: 2.7387
2022-07-11 02:32:34 - train: epoch 0081, iter [02800, 05004], lr: 0.067308, loss: 2.4038
2022-07-11 02:33:08 - train: epoch 0081, iter [02900, 05004], lr: 0.067293, loss: 2.7314
2022-07-11 02:33:42 - train: epoch 0081, iter [03000, 05004], lr: 0.067278, loss: 2.3948
2022-07-11 02:34:16 - train: epoch 0081, iter [03100, 05004], lr: 0.067263, loss: 2.1965
2022-07-11 02:34:50 - train: epoch 0081, iter [03200, 05004], lr: 0.067248, loss: 2.1265
2022-07-11 02:35:25 - train: epoch 0081, iter [03300, 05004], lr: 0.067233, loss: 2.3817
2022-07-11 02:36:00 - train: epoch 0081, iter [03400, 05004], lr: 0.067217, loss: 2.5039
2022-07-11 02:36:34 - train: epoch 0081, iter [03500, 05004], lr: 0.067202, loss: 2.7321
2022-07-11 02:37:08 - train: epoch 0081, iter [03600, 05004], lr: 0.067187, loss: 2.0255
2022-07-11 02:37:42 - train: epoch 0081, iter [03700, 05004], lr: 0.067172, loss: 2.4814
2022-07-11 02:38:16 - train: epoch 0081, iter [03800, 05004], lr: 0.067157, loss: 2.2827
2022-07-11 02:38:51 - train: epoch 0081, iter [03900, 05004], lr: 0.067142, loss: 2.4897
2022-07-11 02:39:26 - train: epoch 0081, iter [04000, 05004], lr: 0.067127, loss: 2.2894
2022-07-11 02:40:00 - train: epoch 0081, iter [04100, 05004], lr: 0.067112, loss: 2.3913
2022-07-11 02:40:33 - train: epoch 0081, iter [04200, 05004], lr: 0.067096, loss: 2.2678
2022-07-11 02:41:08 - train: epoch 0081, iter [04300, 05004], lr: 0.067081, loss: 2.3130
2022-07-11 02:41:42 - train: epoch 0081, iter [04400, 05004], lr: 0.067066, loss: 2.5408
2022-07-11 02:42:17 - train: epoch 0081, iter [04500, 05004], lr: 0.067051, loss: 2.5029
2022-07-11 02:42:52 - train: epoch 0081, iter [04600, 05004], lr: 0.067036, loss: 2.3977
2022-07-11 02:43:25 - train: epoch 0081, iter [04700, 05004], lr: 0.067021, loss: 2.2857
2022-07-11 02:43:59 - train: epoch 0081, iter [04800, 05004], lr: 0.067006, loss: 2.3317
2022-07-11 02:44:33 - train: epoch 0081, iter [04900, 05004], lr: 0.066991, loss: 2.1429
2022-07-11 02:45:07 - train: epoch 0081, iter [05000, 05004], lr: 0.066975, loss: 2.2140
2022-07-11 02:45:08 - train: epoch 081, train_loss: 2.3467
2022-07-11 02:46:23 - eval: epoch: 081, acc1: 61.016%, acc5: 84.434%, test_loss: 1.6201, per_image_load_time: 1.733ms, per_image_inference_time: 0.481ms
2022-07-11 02:46:23 - until epoch: 081, best_acc1: 61.764%
2022-07-11 02:46:23 - epoch 082 lr: 0.066975
2022-07-11 02:47:03 - train: epoch 0082, iter [00100, 05004], lr: 0.066960, loss: 2.1645
2022-07-11 02:47:36 - train: epoch 0082, iter [00200, 05004], lr: 0.066944, loss: 2.1143
2022-07-11 02:48:10 - train: epoch 0082, iter [00300, 05004], lr: 0.066929, loss: 2.2089
2022-07-11 02:48:45 - train: epoch 0082, iter [00400, 05004], lr: 0.066914, loss: 2.3684
2022-07-11 02:49:19 - train: epoch 0082, iter [00500, 05004], lr: 0.066899, loss: 2.3909
2022-07-11 02:49:53 - train: epoch 0082, iter [00600, 05004], lr: 0.066884, loss: 2.3697
2022-07-11 02:50:26 - train: epoch 0082, iter [00700, 05004], lr: 0.066869, loss: 2.3803
2022-07-11 02:51:00 - train: epoch 0082, iter [00800, 05004], lr: 0.066854, loss: 2.3131
2022-07-11 02:51:34 - train: epoch 0082, iter [00900, 05004], lr: 0.066838, loss: 2.4486
2022-07-11 02:52:09 - train: epoch 0082, iter [01000, 05004], lr: 0.066823, loss: 2.4340
2022-07-11 02:52:42 - train: epoch 0082, iter [01100, 05004], lr: 0.066808, loss: 2.5452
2022-07-11 02:53:16 - train: epoch 0082, iter [01200, 05004], lr: 0.066793, loss: 2.4464
2022-07-11 02:53:50 - train: epoch 0082, iter [01300, 05004], lr: 0.066778, loss: 2.5621
2022-07-11 02:54:25 - train: epoch 0082, iter [01400, 05004], lr: 0.066763, loss: 2.5683
2022-07-11 02:54:59 - train: epoch 0082, iter [01500, 05004], lr: 0.066747, loss: 2.3175
2022-07-11 02:55:34 - train: epoch 0082, iter [01600, 05004], lr: 0.066732, loss: 2.3357
2022-07-11 02:56:08 - train: epoch 0082, iter [01700, 05004], lr: 0.066717, loss: 2.1590
2022-07-11 02:56:43 - train: epoch 0082, iter [01800, 05004], lr: 0.066702, loss: 2.3889
2022-07-11 02:57:19 - train: epoch 0082, iter [01900, 05004], lr: 0.066687, loss: 2.3657
2022-07-11 02:57:54 - train: epoch 0082, iter [02000, 05004], lr: 0.066672, loss: 2.0550
2022-07-11 02:58:28 - train: epoch 0082, iter [02100, 05004], lr: 0.066656, loss: 2.4962
2022-07-11 02:59:03 - train: epoch 0082, iter [02200, 05004], lr: 0.066641, loss: 2.4420
2022-07-11 02:59:38 - train: epoch 0082, iter [02300, 05004], lr: 0.066626, loss: 2.5904
2022-07-11 03:00:12 - train: epoch 0082, iter [02400, 05004], lr: 0.066611, loss: 2.5311
2022-07-11 03:00:48 - train: epoch 0082, iter [02500, 05004], lr: 0.066596, loss: 2.3438
2022-07-11 03:01:22 - train: epoch 0082, iter [02600, 05004], lr: 0.066581, loss: 2.2291
2022-07-11 03:01:57 - train: epoch 0082, iter [02700, 05004], lr: 0.066565, loss: 2.2388
2022-07-11 03:02:32 - train: epoch 0082, iter [02800, 05004], lr: 0.066550, loss: 1.9766
2022-07-11 03:03:06 - train: epoch 0082, iter [02900, 05004], lr: 0.066535, loss: 2.1155
2022-07-11 03:03:41 - train: epoch 0082, iter [03000, 05004], lr: 0.066520, loss: 2.3791
2022-07-11 03:04:16 - train: epoch 0082, iter [03100, 05004], lr: 0.066505, loss: 2.3462
2022-07-11 03:04:51 - train: epoch 0082, iter [03200, 05004], lr: 0.066489, loss: 2.3896
2022-07-11 03:05:26 - train: epoch 0082, iter [03300, 05004], lr: 0.066474, loss: 2.5145
2022-07-11 03:06:01 - train: epoch 0082, iter [03400, 05004], lr: 0.066459, loss: 2.2645
2022-07-11 03:06:36 - train: epoch 0082, iter [03500, 05004], lr: 0.066444, loss: 2.2679
2022-07-11 03:07:10 - train: epoch 0082, iter [03600, 05004], lr: 0.066429, loss: 2.5439
2022-07-11 03:07:45 - train: epoch 0082, iter [03700, 05004], lr: 0.066413, loss: 2.0929
2022-07-11 03:08:20 - train: epoch 0082, iter [03800, 05004], lr: 0.066398, loss: 2.4978
2022-07-11 03:08:55 - train: epoch 0082, iter [03900, 05004], lr: 0.066383, loss: 2.3782
2022-07-11 03:09:30 - train: epoch 0082, iter [04000, 05004], lr: 0.066368, loss: 2.2934
2022-07-11 03:10:04 - train: epoch 0082, iter [04100, 05004], lr: 0.066353, loss: 2.5162
2022-07-11 03:10:38 - train: epoch 0082, iter [04200, 05004], lr: 0.066337, loss: 2.4738
2022-07-11 03:11:14 - train: epoch 0082, iter [04300, 05004], lr: 0.066322, loss: 2.3209
2022-07-11 03:11:48 - train: epoch 0082, iter [04400, 05004], lr: 0.066307, loss: 2.2789
2022-07-11 03:12:23 - train: epoch 0082, iter [04500, 05004], lr: 0.066292, loss: 2.2128
2022-07-11 03:12:57 - train: epoch 0082, iter [04600, 05004], lr: 0.066276, loss: 2.3321
2022-07-11 03:13:31 - train: epoch 0082, iter [04700, 05004], lr: 0.066261, loss: 2.4168
2022-07-11 03:14:05 - train: epoch 0082, iter [04800, 05004], lr: 0.066246, loss: 2.3489
2022-07-11 03:14:39 - train: epoch 0082, iter [04900, 05004], lr: 0.066231, loss: 2.2294
2022-07-11 03:15:13 - train: epoch 0082, iter [05000, 05004], lr: 0.066216, loss: 2.4107
2022-07-11 03:15:14 - train: epoch 082, train_loss: 2.3386
2022-07-11 03:16:29 - eval: epoch: 082, acc1: 59.054%, acc5: 82.626%, test_loss: 1.7305, per_image_load_time: 2.200ms, per_image_inference_time: 0.479ms
2022-07-11 03:16:29 - until epoch: 082, best_acc1: 61.764%
2022-07-11 03:16:29 - epoch 083 lr: 0.066215
2022-07-11 03:17:09 - train: epoch 0083, iter [00100, 05004], lr: 0.066200, loss: 2.1256
2022-07-11 03:17:44 - train: epoch 0083, iter [00200, 05004], lr: 0.066184, loss: 2.1076
2022-07-11 03:18:19 - train: epoch 0083, iter [00300, 05004], lr: 0.066169, loss: 2.2545
2022-07-11 03:18:53 - train: epoch 0083, iter [00400, 05004], lr: 0.066154, loss: 2.2869
2022-07-11 03:19:28 - train: epoch 0083, iter [00500, 05004], lr: 0.066139, loss: 2.2880
2022-07-11 03:20:02 - train: epoch 0083, iter [00600, 05004], lr: 0.066124, loss: 2.3549
2022-07-11 03:20:37 - train: epoch 0083, iter [00700, 05004], lr: 0.066108, loss: 2.2131
2022-07-11 03:21:11 - train: epoch 0083, iter [00800, 05004], lr: 0.066093, loss: 2.4073
2022-07-11 03:21:46 - train: epoch 0083, iter [00900, 05004], lr: 0.066078, loss: 2.5481
2022-07-11 03:22:20 - train: epoch 0083, iter [01000, 05004], lr: 0.066063, loss: 2.2572
2022-07-11 03:22:56 - train: epoch 0083, iter [01100, 05004], lr: 0.066047, loss: 2.5049
2022-07-11 03:23:30 - train: epoch 0083, iter [01200, 05004], lr: 0.066032, loss: 2.4584
2022-07-11 03:24:05 - train: epoch 0083, iter [01300, 05004], lr: 0.066017, loss: 2.4602
2022-07-11 03:24:39 - train: epoch 0083, iter [01400, 05004], lr: 0.066002, loss: 2.1827
2022-07-11 03:25:14 - train: epoch 0083, iter [01500, 05004], lr: 0.065986, loss: 2.0397
2022-07-11 03:25:48 - train: epoch 0083, iter [01600, 05004], lr: 0.065971, loss: 2.2939
2022-07-11 03:26:24 - train: epoch 0083, iter [01700, 05004], lr: 0.065956, loss: 2.1850
2022-07-11 03:26:58 - train: epoch 0083, iter [01800, 05004], lr: 0.065941, loss: 2.5166
2022-07-11 03:27:31 - train: epoch 0083, iter [01900, 05004], lr: 0.065925, loss: 2.0872
2022-07-11 03:28:04 - train: epoch 0083, iter [02000, 05004], lr: 0.065910, loss: 2.2339
2022-07-11 03:28:38 - train: epoch 0083, iter [02100, 05004], lr: 0.065895, loss: 2.2631
2022-07-11 03:29:12 - train: epoch 0083, iter [02200, 05004], lr: 0.065880, loss: 2.3386
2022-07-11 03:29:46 - train: epoch 0083, iter [02300, 05004], lr: 0.065864, loss: 2.5530
2022-07-11 03:30:21 - train: epoch 0083, iter [02400, 05004], lr: 0.065849, loss: 2.1619
2022-07-11 03:30:56 - train: epoch 0083, iter [02500, 05004], lr: 0.065834, loss: 2.1674
2022-07-11 03:31:30 - train: epoch 0083, iter [02600, 05004], lr: 0.065818, loss: 2.4423
2022-07-11 03:32:04 - train: epoch 0083, iter [02700, 05004], lr: 0.065803, loss: 2.5851
2022-07-11 03:32:39 - train: epoch 0083, iter [02800, 05004], lr: 0.065788, loss: 2.4325
2022-07-11 03:33:13 - train: epoch 0083, iter [02900, 05004], lr: 0.065773, loss: 2.4709
2022-07-11 03:33:47 - train: epoch 0083, iter [03000, 05004], lr: 0.065757, loss: 2.2779
2022-07-11 03:34:21 - train: epoch 0083, iter [03100, 05004], lr: 0.065742, loss: 2.3458
2022-07-11 03:34:56 - train: epoch 0083, iter [03200, 05004], lr: 0.065727, loss: 2.1130
2022-07-11 03:35:30 - train: epoch 0083, iter [03300, 05004], lr: 0.065711, loss: 2.3998
2022-07-11 03:36:05 - train: epoch 0083, iter [03400, 05004], lr: 0.065696, loss: 2.4418
2022-07-11 03:36:40 - train: epoch 0083, iter [03500, 05004], lr: 0.065681, loss: 2.5427
2022-07-11 03:37:13 - train: epoch 0083, iter [03600, 05004], lr: 0.065666, loss: 2.4561
2022-07-11 03:37:47 - train: epoch 0083, iter [03700, 05004], lr: 0.065650, loss: 2.1258
2022-07-11 03:38:22 - train: epoch 0083, iter [03800, 05004], lr: 0.065635, loss: 2.2661
2022-07-11 03:38:56 - train: epoch 0083, iter [03900, 05004], lr: 0.065620, loss: 2.5232
2022-07-11 03:39:32 - train: epoch 0083, iter [04000, 05004], lr: 0.065604, loss: 2.4025
2022-07-11 03:40:07 - train: epoch 0083, iter [04100, 05004], lr: 0.065589, loss: 2.1591
2022-07-11 03:40:39 - train: epoch 0083, iter [04200, 05004], lr: 0.065574, loss: 2.7216
2022-07-11 03:41:13 - train: epoch 0083, iter [04300, 05004], lr: 0.065559, loss: 2.4041
2022-07-11 03:41:49 - train: epoch 0083, iter [04400, 05004], lr: 0.065543, loss: 2.3016
2022-07-11 03:42:24 - train: epoch 0083, iter [04500, 05004], lr: 0.065528, loss: 2.3989
2022-07-11 03:42:58 - train: epoch 0083, iter [04600, 05004], lr: 0.065513, loss: 2.2678
2022-07-11 03:43:34 - train: epoch 0083, iter [04700, 05004], lr: 0.065497, loss: 2.7422
2022-07-11 03:44:09 - train: epoch 0083, iter [04800, 05004], lr: 0.065482, loss: 2.4981
2022-07-11 03:44:44 - train: epoch 0083, iter [04900, 05004], lr: 0.065467, loss: 2.3124
2022-07-11 03:45:18 - train: epoch 0083, iter [05000, 05004], lr: 0.065451, loss: 2.2367
2022-07-11 03:45:19 - train: epoch 083, train_loss: 2.3364
2022-07-11 03:46:34 - eval: epoch: 083, acc1: 61.846%, acc5: 84.704%, test_loss: 1.5919, per_image_load_time: 1.985ms, per_image_inference_time: 0.462ms
2022-07-11 03:46:34 - until epoch: 083, best_acc1: 61.846%
2022-07-11 03:46:34 - epoch 084 lr: 0.065451
2022-07-11 03:47:14 - train: epoch 0084, iter [00100, 05004], lr: 0.065436, loss: 2.2870
2022-07-11 03:47:48 - train: epoch 0084, iter [00200, 05004], lr: 0.065420, loss: 2.3781
2022-07-11 03:48:22 - train: epoch 0084, iter [00300, 05004], lr: 0.065405, loss: 2.1054
2022-07-11 03:48:57 - train: epoch 0084, iter [00400, 05004], lr: 0.065390, loss: 2.2753
2022-07-11 03:49:32 - train: epoch 0084, iter [00500, 05004], lr: 0.065374, loss: 2.6647
2022-07-11 03:50:06 - train: epoch 0084, iter [00600, 05004], lr: 0.065359, loss: 2.4016
2022-07-11 03:50:40 - train: epoch 0084, iter [00700, 05004], lr: 0.065344, loss: 2.2397
2022-07-11 03:51:14 - train: epoch 0084, iter [00800, 05004], lr: 0.065328, loss: 2.3497
2022-07-11 03:51:50 - train: epoch 0084, iter [00900, 05004], lr: 0.065313, loss: 2.2390
2022-07-11 03:52:25 - train: epoch 0084, iter [01000, 05004], lr: 0.065298, loss: 2.4441
2022-07-11 03:52:58 - train: epoch 0084, iter [01100, 05004], lr: 0.065282, loss: 2.3204
2022-07-11 03:53:32 - train: epoch 0084, iter [01200, 05004], lr: 0.065267, loss: 2.5922
2022-07-11 03:54:07 - train: epoch 0084, iter [01300, 05004], lr: 0.065252, loss: 2.4349
2022-07-11 03:54:42 - train: epoch 0084, iter [01400, 05004], lr: 0.065236, loss: 2.0762
2022-07-11 03:55:17 - train: epoch 0084, iter [01500, 05004], lr: 0.065221, loss: 2.3368
2022-07-11 03:55:51 - train: epoch 0084, iter [01600, 05004], lr: 0.065206, loss: 2.1170
2022-07-11 03:56:26 - train: epoch 0084, iter [01700, 05004], lr: 0.065190, loss: 2.4332
2022-07-11 03:57:02 - train: epoch 0084, iter [01800, 05004], lr: 0.065175, loss: 2.3606
2022-07-11 03:57:36 - train: epoch 0084, iter [01900, 05004], lr: 0.065160, loss: 2.3261
2022-07-11 03:58:12 - train: epoch 0084, iter [02000, 05004], lr: 0.065144, loss: 2.5364
2022-07-11 03:58:46 - train: epoch 0084, iter [02100, 05004], lr: 0.065129, loss: 2.2336
2022-07-11 03:59:22 - train: epoch 0084, iter [02200, 05004], lr: 0.065114, loss: 2.0831
2022-07-11 03:59:57 - train: epoch 0084, iter [02300, 05004], lr: 0.065098, loss: 2.1750
2022-07-11 04:00:32 - train: epoch 0084, iter [02400, 05004], lr: 0.065083, loss: 2.3573
2022-07-11 04:01:08 - train: epoch 0084, iter [02500, 05004], lr: 0.065068, loss: 2.3022
2022-07-11 04:01:42 - train: epoch 0084, iter [02600, 05004], lr: 0.065052, loss: 2.1944
2022-07-11 04:02:18 - train: epoch 0084, iter [02700, 05004], lr: 0.065037, loss: 2.2309
2022-07-11 04:02:53 - train: epoch 0084, iter [02800, 05004], lr: 0.065022, loss: 2.5612
2022-07-11 04:03:28 - train: epoch 0084, iter [02900, 05004], lr: 0.065006, loss: 2.2507
2022-07-11 04:04:03 - train: epoch 0084, iter [03000, 05004], lr: 0.064991, loss: 2.2129
2022-07-11 04:04:37 - train: epoch 0084, iter [03100, 05004], lr: 0.064975, loss: 2.2339
2022-07-11 04:05:12 - train: epoch 0084, iter [03200, 05004], lr: 0.064960, loss: 2.1075
2022-07-11 04:05:47 - train: epoch 0084, iter [03300, 05004], lr: 0.064945, loss: 2.1497
2022-07-11 04:06:23 - train: epoch 0084, iter [03400, 05004], lr: 0.064929, loss: 2.3549
2022-07-11 04:06:57 - train: epoch 0084, iter [03500, 05004], lr: 0.064914, loss: 2.2240
2022-07-11 04:07:31 - train: epoch 0084, iter [03600, 05004], lr: 0.064899, loss: 2.2786
2022-07-11 04:08:06 - train: epoch 0084, iter [03700, 05004], lr: 0.064883, loss: 2.8200
2022-07-11 04:08:42 - train: epoch 0084, iter [03800, 05004], lr: 0.064868, loss: 2.5909
2022-07-11 04:09:16 - train: epoch 0084, iter [03900, 05004], lr: 0.064853, loss: 2.1658
2022-07-11 04:09:51 - train: epoch 0084, iter [04000, 05004], lr: 0.064837, loss: 2.5155
2022-07-11 04:10:26 - train: epoch 0084, iter [04100, 05004], lr: 0.064822, loss: 2.0851
2022-07-11 04:11:01 - train: epoch 0084, iter [04200, 05004], lr: 0.064806, loss: 2.3075
2022-07-11 04:11:36 - train: epoch 0084, iter [04300, 05004], lr: 0.064791, loss: 2.4262
2022-07-11 04:12:10 - train: epoch 0084, iter [04400, 05004], lr: 0.064776, loss: 2.6589
2022-07-11 04:12:44 - train: epoch 0084, iter [04500, 05004], lr: 0.064760, loss: 2.3187
2022-07-11 04:13:19 - train: epoch 0084, iter [04600, 05004], lr: 0.064745, loss: 2.1473
2022-07-11 04:13:53 - train: epoch 0084, iter [04700, 05004], lr: 0.064730, loss: 2.3915
2022-07-11 04:14:28 - train: epoch 0084, iter [04800, 05004], lr: 0.064714, loss: 2.1331
2022-07-11 04:15:02 - train: epoch 0084, iter [04900, 05004], lr: 0.064699, loss: 2.2878
2022-07-11 04:15:36 - train: epoch 0084, iter [05000, 05004], lr: 0.064683, loss: 2.1083
2022-07-11 04:15:37 - train: epoch 084, train_loss: 2.3266
2022-07-11 04:16:53 - eval: epoch: 084, acc1: 60.892%, acc5: 84.494%, test_loss: 1.6215, per_image_load_time: 1.406ms, per_image_inference_time: 0.461ms
2022-07-11 04:16:53 - until epoch: 084, best_acc1: 61.846%
2022-07-11 04:16:53 - epoch 085 lr: 0.064683
2022-07-11 04:17:32 - train: epoch 0085, iter [00100, 05004], lr: 0.064667, loss: 2.3677
2022-07-11 04:18:07 - train: epoch 0085, iter [00200, 05004], lr: 0.064652, loss: 2.3086
2022-07-11 04:18:41 - train: epoch 0085, iter [00300, 05004], lr: 0.064637, loss: 2.3522
2022-07-11 04:19:15 - train: epoch 0085, iter [00400, 05004], lr: 0.064621, loss: 2.4468
2022-07-11 04:19:50 - train: epoch 0085, iter [00500, 05004], lr: 0.064606, loss: 2.2741
2022-07-11 04:20:24 - train: epoch 0085, iter [00600, 05004], lr: 0.064590, loss: 1.8442
2022-07-11 04:20:59 - train: epoch 0085, iter [00700, 05004], lr: 0.064575, loss: 2.5835
2022-07-11 04:21:34 - train: epoch 0085, iter [00800, 05004], lr: 0.064560, loss: 2.2360
2022-07-11 04:22:07 - train: epoch 0085, iter [00900, 05004], lr: 0.064544, loss: 2.1422
2022-07-11 04:22:41 - train: epoch 0085, iter [01000, 05004], lr: 0.064529, loss: 2.0715
2022-07-11 04:23:16 - train: epoch 0085, iter [01100, 05004], lr: 0.064513, loss: 2.4125
2022-07-11 04:23:51 - train: epoch 0085, iter [01200, 05004], lr: 0.064498, loss: 2.1617
2022-07-11 04:24:25 - train: epoch 0085, iter [01300, 05004], lr: 0.064483, loss: 2.4259
2022-07-11 04:24:59 - train: epoch 0085, iter [01400, 05004], lr: 0.064467, loss: 2.2147
2022-07-11 04:25:34 - train: epoch 0085, iter [01500, 05004], lr: 0.064452, loss: 2.3400
2022-07-11 04:26:10 - train: epoch 0085, iter [01600, 05004], lr: 0.064436, loss: 2.4153
2022-07-11 04:26:43 - train: epoch 0085, iter [01700, 05004], lr: 0.064421, loss: 2.6089
2022-07-11 04:27:17 - train: epoch 0085, iter [01800, 05004], lr: 0.064406, loss: 2.1912
2022-07-11 04:27:52 - train: epoch 0085, iter [01900, 05004], lr: 0.064390, loss: 2.3328
2022-07-11 04:28:26 - train: epoch 0085, iter [02000, 05004], lr: 0.064375, loss: 2.3682
2022-07-11 04:29:00 - train: epoch 0085, iter [02100, 05004], lr: 0.064359, loss: 2.2652
2022-07-11 04:29:34 - train: epoch 0085, iter [02200, 05004], lr: 0.064344, loss: 2.3911
2022-07-11 04:30:08 - train: epoch 0085, iter [02300, 05004], lr: 0.064328, loss: 2.2343
2022-07-11 04:30:43 - train: epoch 0085, iter [02400, 05004], lr: 0.064313, loss: 2.4362
2022-07-11 04:31:18 - train: epoch 0085, iter [02500, 05004], lr: 0.064298, loss: 2.5466
2022-07-11 04:31:52 - train: epoch 0085, iter [02600, 05004], lr: 0.064282, loss: 2.5249
2022-07-11 04:32:26 - train: epoch 0085, iter [02700, 05004], lr: 0.064267, loss: 2.1920
2022-07-11 04:33:02 - train: epoch 0085, iter [02800, 05004], lr: 0.064251, loss: 2.5043
2022-07-11 04:33:36 - train: epoch 0085, iter [02900, 05004], lr: 0.064236, loss: 2.4517
2022-07-11 04:34:11 - train: epoch 0085, iter [03000, 05004], lr: 0.064220, loss: 2.6367
2022-07-11 04:34:44 - train: epoch 0085, iter [03100, 05004], lr: 0.064205, loss: 2.1358
2022-07-11 04:35:19 - train: epoch 0085, iter [03200, 05004], lr: 0.064190, loss: 2.3726
2022-07-11 04:35:55 - train: epoch 0085, iter [03300, 05004], lr: 0.064174, loss: 2.5240
2022-07-11 04:36:29 - train: epoch 0085, iter [03400, 05004], lr: 0.064159, loss: 2.4172
2022-07-11 04:37:03 - train: epoch 0085, iter [03500, 05004], lr: 0.064143, loss: 2.1849
2022-07-11 04:37:38 - train: epoch 0085, iter [03600, 05004], lr: 0.064128, loss: 2.5129
2022-07-11 04:38:13 - train: epoch 0085, iter [03700, 05004], lr: 0.064112, loss: 2.1206
2022-07-11 04:38:46 - train: epoch 0085, iter [03800, 05004], lr: 0.064097, loss: 2.7133
2022-07-11 04:39:21 - train: epoch 0085, iter [03900, 05004], lr: 0.064081, loss: 2.3568
2022-07-11 04:39:55 - train: epoch 0085, iter [04000, 05004], lr: 0.064066, loss: 2.6055
2022-07-11 04:40:29 - train: epoch 0085, iter [04100, 05004], lr: 0.064051, loss: 2.2788
2022-07-11 04:41:03 - train: epoch 0085, iter [04200, 05004], lr: 0.064035, loss: 2.4751
2022-07-11 04:41:37 - train: epoch 0085, iter [04300, 05004], lr: 0.064020, loss: 2.2177
2022-07-11 04:42:11 - train: epoch 0085, iter [04400, 05004], lr: 0.064004, loss: 2.2417
2022-07-11 04:42:46 - train: epoch 0085, iter [04500, 05004], lr: 0.063989, loss: 2.2166
2022-07-11 04:43:20 - train: epoch 0085, iter [04600, 05004], lr: 0.063973, loss: 2.4588
2022-07-11 04:43:54 - train: epoch 0085, iter [04700, 05004], lr: 0.063958, loss: 2.4790
2022-07-11 04:44:28 - train: epoch 0085, iter [04800, 05004], lr: 0.063942, loss: 2.2487
2022-07-11 04:45:03 - train: epoch 0085, iter [04900, 05004], lr: 0.063927, loss: 2.2669
2022-07-11 04:45:37 - train: epoch 0085, iter [05000, 05004], lr: 0.063911, loss: 2.0887
2022-07-11 04:45:38 - train: epoch 085, train_loss: 2.3249
2022-07-11 04:46:52 - eval: epoch: 085, acc1: 61.176%, acc5: 84.370%, test_loss: 1.6194, per_image_load_time: 2.349ms, per_image_inference_time: 0.460ms
2022-07-11 04:46:52 - until epoch: 085, best_acc1: 61.846%
2022-07-11 04:46:52 - epoch 086 lr: 0.063911
2022-07-11 04:47:33 - train: epoch 0086, iter [00100, 05004], lr: 0.063895, loss: 2.2160
2022-07-11 04:48:07 - train: epoch 0086, iter [00200, 05004], lr: 0.063880, loss: 1.8618
2022-07-11 04:48:41 - train: epoch 0086, iter [00300, 05004], lr: 0.063864, loss: 2.3198
2022-07-11 04:49:14 - train: epoch 0086, iter [00400, 05004], lr: 0.063849, loss: 2.3879
2022-07-11 04:49:49 - train: epoch 0086, iter [00500, 05004], lr: 0.063834, loss: 2.7938
2022-07-11 04:50:24 - train: epoch 0086, iter [00600, 05004], lr: 0.063818, loss: 2.3880
2022-07-11 04:50:57 - train: epoch 0086, iter [00700, 05004], lr: 0.063803, loss: 2.2517
2022-07-11 04:51:31 - train: epoch 0086, iter [00800, 05004], lr: 0.063787, loss: 2.0806
2022-07-11 04:52:06 - train: epoch 0086, iter [00900, 05004], lr: 0.063772, loss: 2.2200
2022-07-11 04:52:40 - train: epoch 0086, iter [01000, 05004], lr: 0.063756, loss: 2.3928
2022-07-11 04:53:14 - train: epoch 0086, iter [01100, 05004], lr: 0.063741, loss: 2.4311
2022-07-11 04:53:48 - train: epoch 0086, iter [01200, 05004], lr: 0.063725, loss: 2.4330
2022-07-11 04:54:22 - train: epoch 0086, iter [01300, 05004], lr: 0.063710, loss: 2.3466
2022-07-11 04:54:57 - train: epoch 0086, iter [01400, 05004], lr: 0.063694, loss: 2.1764
2022-07-11 04:55:32 - train: epoch 0086, iter [01500, 05004], lr: 0.063679, loss: 2.1150
2022-07-11 04:56:06 - train: epoch 0086, iter [01600, 05004], lr: 0.063663, loss: 2.0482
2022-07-11 04:56:42 - train: epoch 0086, iter [01700, 05004], lr: 0.063648, loss: 2.3238
2022-07-11 04:57:16 - train: epoch 0086, iter [01800, 05004], lr: 0.063632, loss: 2.4603
2022-07-11 04:57:51 - train: epoch 0086, iter [01900, 05004], lr: 0.063617, loss: 2.4089
2022-07-11 04:58:25 - train: epoch 0086, iter [02000, 05004], lr: 0.063601, loss: 2.4200
2022-07-11 04:59:00 - train: epoch 0086, iter [02100, 05004], lr: 0.063586, loss: 2.1187
2022-07-11 04:59:34 - train: epoch 0086, iter [02200, 05004], lr: 0.063570, loss: 2.4740
2022-07-11 05:00:10 - train: epoch 0086, iter [02300, 05004], lr: 0.063555, loss: 2.2553
2022-07-11 05:00:44 - train: epoch 0086, iter [02400, 05004], lr: 0.063539, loss: 1.9094
2022-07-11 05:01:19 - train: epoch 0086, iter [02500, 05004], lr: 0.063524, loss: 2.2151
2022-07-11 05:01:52 - train: epoch 0086, iter [02600, 05004], lr: 0.063508, loss: 2.4617
2022-07-11 05:02:26 - train: epoch 0086, iter [02700, 05004], lr: 0.063493, loss: 2.3219
2022-07-11 05:03:01 - train: epoch 0086, iter [02800, 05004], lr: 0.063477, loss: 2.3712
2022-07-11 05:03:36 - train: epoch 0086, iter [02900, 05004], lr: 0.063462, loss: 2.0981
2022-07-11 05:04:10 - train: epoch 0086, iter [03000, 05004], lr: 0.063446, loss: 2.2417
2022-07-11 05:04:44 - train: epoch 0086, iter [03100, 05004], lr: 0.063431, loss: 2.3568
2022-07-11 05:05:19 - train: epoch 0086, iter [03200, 05004], lr: 0.063415, loss: 2.3694
2022-07-11 05:05:54 - train: epoch 0086, iter [03300, 05004], lr: 0.063400, loss: 2.1445
2022-07-11 05:06:29 - train: epoch 0086, iter [03400, 05004], lr: 0.063384, loss: 2.4217
2022-07-11 05:07:04 - train: epoch 0086, iter [03500, 05004], lr: 0.063369, loss: 2.2919
2022-07-11 05:07:39 - train: epoch 0086, iter [03600, 05004], lr: 0.063353, loss: 2.3084
2022-07-11 05:08:14 - train: epoch 0086, iter [03700, 05004], lr: 0.063338, loss: 2.4832
2022-07-11 05:08:49 - train: epoch 0086, iter [03800, 05004], lr: 0.063322, loss: 2.3302
2022-07-11 05:09:24 - train: epoch 0086, iter [03900, 05004], lr: 0.063307, loss: 2.2689
2022-07-11 05:09:57 - train: epoch 0086, iter [04000, 05004], lr: 0.063291, loss: 2.2904
2022-07-11 05:10:33 - train: epoch 0086, iter [04100, 05004], lr: 0.063276, loss: 2.0598
2022-07-11 05:11:08 - train: epoch 0086, iter [04200, 05004], lr: 0.063260, loss: 2.3658
2022-07-11 05:11:43 - train: epoch 0086, iter [04300, 05004], lr: 0.063245, loss: 2.5605
2022-07-11 05:12:17 - train: epoch 0086, iter [04400, 05004], lr: 0.063229, loss: 2.2863
2022-07-11 05:12:52 - train: epoch 0086, iter [04500, 05004], lr: 0.063214, loss: 2.0428
2022-07-11 05:13:27 - train: epoch 0086, iter [04600, 05004], lr: 0.063198, loss: 2.4488
2022-07-11 05:14:02 - train: epoch 0086, iter [04700, 05004], lr: 0.063183, loss: 2.2425
2022-07-11 05:14:36 - train: epoch 0086, iter [04800, 05004], lr: 0.063167, loss: 2.2783
2022-07-11 05:15:11 - train: epoch 0086, iter [04900, 05004], lr: 0.063152, loss: 2.3537
2022-07-11 05:15:45 - train: epoch 0086, iter [05000, 05004], lr: 0.063136, loss: 2.2451
2022-07-11 05:15:46 - train: epoch 086, train_loss: 2.3154
2022-07-11 05:17:01 - eval: epoch: 086, acc1: 60.844%, acc5: 84.170%, test_loss: 1.6485, per_image_load_time: 2.426ms, per_image_inference_time: 0.464ms
2022-07-11 05:17:01 - until epoch: 086, best_acc1: 61.846%
2022-07-11 05:17:01 - epoch 087 lr: 0.063135
2022-07-11 05:17:41 - train: epoch 0087, iter [00100, 05004], lr: 0.063120, loss: 2.0786
2022-07-11 05:18:15 - train: epoch 0087, iter [00200, 05004], lr: 0.063104, loss: 2.1502
2022-07-11 05:18:49 - train: epoch 0087, iter [00300, 05004], lr: 0.063089, loss: 2.3621
2022-07-11 05:19:24 - train: epoch 0087, iter [00400, 05004], lr: 0.063073, loss: 2.6009
2022-07-11 05:19:58 - train: epoch 0087, iter [00500, 05004], lr: 0.063058, loss: 2.0186
2022-07-11 05:20:32 - train: epoch 0087, iter [00600, 05004], lr: 0.063042, loss: 2.5125
2022-07-11 05:21:07 - train: epoch 0087, iter [00700, 05004], lr: 0.063027, loss: 2.4661
2022-07-11 05:21:41 - train: epoch 0087, iter [00800, 05004], lr: 0.063011, loss: 2.4078
2022-07-11 05:22:15 - train: epoch 0087, iter [00900, 05004], lr: 0.062996, loss: 2.2322
2022-07-11 05:22:51 - train: epoch 0087, iter [01000, 05004], lr: 0.062980, loss: 2.2066
2022-07-11 05:23:25 - train: epoch 0087, iter [01100, 05004], lr: 0.062964, loss: 2.3989
2022-07-11 05:23:59 - train: epoch 0087, iter [01200, 05004], lr: 0.062949, loss: 2.2155
2022-07-11 05:24:33 - train: epoch 0087, iter [01300, 05004], lr: 0.062933, loss: 2.6038
2022-07-11 05:25:08 - train: epoch 0087, iter [01400, 05004], lr: 0.062918, loss: 2.2757
2022-07-11 05:25:43 - train: epoch 0087, iter [01500, 05004], lr: 0.062902, loss: 2.3020
2022-07-11 05:26:17 - train: epoch 0087, iter [01600, 05004], lr: 0.062887, loss: 2.1115
2022-07-11 05:26:51 - train: epoch 0087, iter [01700, 05004], lr: 0.062871, loss: 2.5195
2022-07-11 05:27:26 - train: epoch 0087, iter [01800, 05004], lr: 0.062856, loss: 2.4158
2022-07-11 05:28:01 - train: epoch 0087, iter [01900, 05004], lr: 0.062840, loss: 2.4016
2022-07-11 05:28:35 - train: epoch 0087, iter [02000, 05004], lr: 0.062824, loss: 2.5345
2022-07-11 05:29:10 - train: epoch 0087, iter [02100, 05004], lr: 0.062809, loss: 2.2821
2022-07-11 05:29:44 - train: epoch 0087, iter [02200, 05004], lr: 0.062793, loss: 2.5620
2022-07-11 05:30:19 - train: epoch 0087, iter [02300, 05004], lr: 0.062778, loss: 2.3566
2022-07-11 05:30:54 - train: epoch 0087, iter [02400, 05004], lr: 0.062762, loss: 2.2513
2022-07-11 05:31:29 - train: epoch 0087, iter [02500, 05004], lr: 0.062747, loss: 2.4378
2022-07-11 05:32:03 - train: epoch 0087, iter [02600, 05004], lr: 0.062731, loss: 2.6554
2022-07-11 05:32:37 - train: epoch 0087, iter [02700, 05004], lr: 0.062716, loss: 2.2008
2022-07-11 05:33:11 - train: epoch 0087, iter [02800, 05004], lr: 0.062700, loss: 2.0393
2022-07-11 05:33:46 - train: epoch 0087, iter [02900, 05004], lr: 0.062684, loss: 2.2989
2022-07-11 05:34:21 - train: epoch 0087, iter [03000, 05004], lr: 0.062669, loss: 2.3203
2022-07-11 05:34:56 - train: epoch 0087, iter [03100, 05004], lr: 0.062653, loss: 2.2357
2022-07-11 05:35:30 - train: epoch 0087, iter [03200, 05004], lr: 0.062638, loss: 2.3826
2022-07-11 05:36:05 - train: epoch 0087, iter [03300, 05004], lr: 0.062622, loss: 2.2145
2022-07-11 05:36:40 - train: epoch 0087, iter [03400, 05004], lr: 0.062606, loss: 2.7041
2022-07-11 05:37:14 - train: epoch 0087, iter [03500, 05004], lr: 0.062591, loss: 2.1005
2022-07-11 05:37:48 - train: epoch 0087, iter [03600, 05004], lr: 0.062575, loss: 2.1897
2022-07-11 05:38:23 - train: epoch 0087, iter [03700, 05004], lr: 0.062560, loss: 2.2756
2022-07-11 05:38:59 - train: epoch 0087, iter [03800, 05004], lr: 0.062544, loss: 2.3865
2022-07-11 05:39:33 - train: epoch 0087, iter [03900, 05004], lr: 0.062529, loss: 2.3292
2022-07-11 05:40:07 - train: epoch 0087, iter [04000, 05004], lr: 0.062513, loss: 2.2830
2022-07-11 05:40:42 - train: epoch 0087, iter [04100, 05004], lr: 0.062497, loss: 2.1897
2022-07-11 05:41:18 - train: epoch 0087, iter [04200, 05004], lr: 0.062482, loss: 2.3497
2022-07-11 05:41:52 - train: epoch 0087, iter [04300, 05004], lr: 0.062466, loss: 2.3352
2022-07-11 05:42:25 - train: epoch 0087, iter [04400, 05004], lr: 0.062451, loss: 2.2529
2022-07-11 05:43:01 - train: epoch 0087, iter [04500, 05004], lr: 0.062435, loss: 2.0215
2022-07-11 05:43:36 - train: epoch 0087, iter [04600, 05004], lr: 0.062419, loss: 2.1863
2022-07-11 05:44:11 - train: epoch 0087, iter [04700, 05004], lr: 0.062404, loss: 2.4207
2022-07-11 05:44:45 - train: epoch 0087, iter [04800, 05004], lr: 0.062388, loss: 2.1563
2022-07-11 05:45:20 - train: epoch 0087, iter [04900, 05004], lr: 0.062373, loss: 2.1393
2022-07-11 05:45:54 - train: epoch 0087, iter [05000, 05004], lr: 0.062357, loss: 2.2978
2022-07-11 05:45:55 - train: epoch 087, train_loss: 2.3114
2022-07-11 05:47:10 - eval: epoch: 087, acc1: 60.638%, acc5: 83.824%, test_loss: 1.6431, per_image_load_time: 2.366ms, per_image_inference_time: 0.460ms
2022-07-11 05:47:10 - until epoch: 087, best_acc1: 61.846%
2022-07-11 05:47:10 - epoch 088 lr: 0.062356
2022-07-11 05:47:50 - train: epoch 0088, iter [00100, 05004], lr: 0.062341, loss: 2.2036
2022-07-11 05:48:24 - train: epoch 0088, iter [00200, 05004], lr: 0.062325, loss: 2.3468
2022-07-11 05:48:57 - train: epoch 0088, iter [00300, 05004], lr: 0.062310, loss: 2.4796
2022-07-11 05:49:33 - train: epoch 0088, iter [00400, 05004], lr: 0.062294, loss: 2.2393
2022-07-11 05:50:06 - train: epoch 0088, iter [00500, 05004], lr: 0.062278, loss: 2.2903
2022-07-11 05:50:41 - train: epoch 0088, iter [00600, 05004], lr: 0.062263, loss: 2.4643
2022-07-11 05:51:16 - train: epoch 0088, iter [00700, 05004], lr: 0.062247, loss: 2.2988
2022-07-11 05:51:50 - train: epoch 0088, iter [00800, 05004], lr: 0.062232, loss: 2.3842
2022-07-11 05:52:26 - train: epoch 0088, iter [00900, 05004], lr: 0.062216, loss: 2.4204
2022-07-11 05:53:01 - train: epoch 0088, iter [01000, 05004], lr: 0.062200, loss: 2.2923
2022-07-11 05:53:36 - train: epoch 0088, iter [01100, 05004], lr: 0.062185, loss: 2.5681
2022-07-11 05:54:11 - train: epoch 0088, iter [01200, 05004], lr: 0.062169, loss: 2.3282
2022-07-11 05:54:46 - train: epoch 0088, iter [01300, 05004], lr: 0.062154, loss: 2.2857
2022-07-11 05:55:22 - train: epoch 0088, iter [01400, 05004], lr: 0.062138, loss: 2.0451
2022-07-11 05:55:57 - train: epoch 0088, iter [01500, 05004], lr: 0.062122, loss: 2.1466
2022-07-11 05:56:32 - train: epoch 0088, iter [01600, 05004], lr: 0.062107, loss: 2.0647
2022-07-11 05:57:08 - train: epoch 0088, iter [01700, 05004], lr: 0.062091, loss: 2.1670
2022-07-11 05:57:42 - train: epoch 0088, iter [01800, 05004], lr: 0.062075, loss: 2.0799
2022-07-11 05:58:18 - train: epoch 0088, iter [01900, 05004], lr: 0.062060, loss: 2.2928
2022-07-11 05:58:52 - train: epoch 0088, iter [02000, 05004], lr: 0.062044, loss: 2.2377
2022-07-11 05:59:27 - train: epoch 0088, iter [02100, 05004], lr: 0.062029, loss: 2.3271
2022-07-11 06:00:02 - train: epoch 0088, iter [02200, 05004], lr: 0.062013, loss: 2.2936
2022-07-11 06:00:37 - train: epoch 0088, iter [02300, 05004], lr: 0.061997, loss: 2.2728
2022-07-11 06:01:12 - train: epoch 0088, iter [02400, 05004], lr: 0.061982, loss: 2.5853
2022-07-11 06:01:47 - train: epoch 0088, iter [02500, 05004], lr: 0.061966, loss: 2.4847
2022-07-11 06:02:22 - train: epoch 0088, iter [02600, 05004], lr: 0.061950, loss: 2.1857
2022-07-11 06:02:57 - train: epoch 0088, iter [02700, 05004], lr: 0.061935, loss: 2.3219
2022-07-11 06:03:32 - train: epoch 0088, iter [02800, 05004], lr: 0.061919, loss: 2.6228
2022-07-11 06:04:07 - train: epoch 0088, iter [02900, 05004], lr: 0.061904, loss: 2.2623
2022-07-11 06:04:42 - train: epoch 0088, iter [03000, 05004], lr: 0.061888, loss: 2.4766
2022-07-11 06:05:17 - train: epoch 0088, iter [03100, 05004], lr: 0.061872, loss: 2.2687
2022-07-11 06:05:52 - train: epoch 0088, iter [03200, 05004], lr: 0.061857, loss: 2.3054
2022-07-11 06:06:27 - train: epoch 0088, iter [03300, 05004], lr: 0.061841, loss: 2.1676
2022-07-11 06:07:03 - train: epoch 0088, iter [03400, 05004], lr: 0.061825, loss: 2.1409
2022-07-11 06:07:39 - train: epoch 0088, iter [03500, 05004], lr: 0.061810, loss: 2.4889
2022-07-11 06:08:13 - train: epoch 0088, iter [03600, 05004], lr: 0.061794, loss: 2.2411
2022-07-11 06:08:48 - train: epoch 0088, iter [03700, 05004], lr: 0.061778, loss: 2.4517
2022-07-11 06:09:24 - train: epoch 0088, iter [03800, 05004], lr: 0.061763, loss: 2.6373
2022-07-11 06:09:59 - train: epoch 0088, iter [03900, 05004], lr: 0.061747, loss: 2.2334
2022-07-11 06:10:34 - train: epoch 0088, iter [04000, 05004], lr: 0.061732, loss: 1.9052
2022-07-11 06:11:09 - train: epoch 0088, iter [04100, 05004], lr: 0.061716, loss: 2.3532
2022-07-11 06:11:45 - train: epoch 0088, iter [04200, 05004], lr: 0.061700, loss: 2.3509
2022-07-11 06:12:20 - train: epoch 0088, iter [04300, 05004], lr: 0.061685, loss: 2.5976
2022-07-11 06:12:55 - train: epoch 0088, iter [04400, 05004], lr: 0.061669, loss: 2.3532
2022-07-11 06:13:29 - train: epoch 0088, iter [04500, 05004], lr: 0.061653, loss: 2.0686
2022-07-11 06:14:04 - train: epoch 0088, iter [04600, 05004], lr: 0.061638, loss: 2.4756
2022-07-11 06:14:39 - train: epoch 0088, iter [04700, 05004], lr: 0.061622, loss: 2.3564
2022-07-11 06:15:14 - train: epoch 0088, iter [04800, 05004], lr: 0.061606, loss: 2.2191
2022-07-11 06:15:48 - train: epoch 0088, iter [04900, 05004], lr: 0.061591, loss: 2.3310
2022-07-11 06:16:21 - train: epoch 0088, iter [05000, 05004], lr: 0.061575, loss: 2.3614
2022-07-11 06:16:22 - train: epoch 088, train_loss: 2.3062
2022-07-11 06:17:36 - eval: epoch: 088, acc1: 61.450%, acc5: 84.332%, test_loss: 1.6168, per_image_load_time: 2.401ms, per_image_inference_time: 0.447ms
2022-07-11 06:17:37 - until epoch: 088, best_acc1: 61.846%
2022-07-11 06:17:37 - epoch 089 lr: 0.061574
2022-07-11 06:18:17 - train: epoch 0089, iter [00100, 05004], lr: 0.061559, loss: 2.4474
2022-07-11 06:18:51 - train: epoch 0089, iter [00200, 05004], lr: 0.061543, loss: 2.2713
2022-07-11 06:19:25 - train: epoch 0089, iter [00300, 05004], lr: 0.061527, loss: 2.2464
2022-07-11 06:20:00 - train: epoch 0089, iter [00400, 05004], lr: 0.061512, loss: 2.1472
2022-07-11 06:20:35 - train: epoch 0089, iter [00500, 05004], lr: 0.061496, loss: 2.1276
2022-07-11 06:21:10 - train: epoch 0089, iter [00600, 05004], lr: 0.061480, loss: 2.1936
2022-07-11 06:21:44 - train: epoch 0089, iter [00700, 05004], lr: 0.061465, loss: 2.6335
2022-07-11 06:22:18 - train: epoch 0089, iter [00800, 05004], lr: 0.061449, loss: 2.7363
2022-07-11 06:22:53 - train: epoch 0089, iter [00900, 05004], lr: 0.061433, loss: 2.0514
2022-07-11 06:23:28 - train: epoch 0089, iter [01000, 05004], lr: 0.061418, loss: 2.2829
2022-07-11 06:24:03 - train: epoch 0089, iter [01100, 05004], lr: 0.061402, loss: 2.2388
2022-07-11 06:24:36 - train: epoch 0089, iter [01200, 05004], lr: 0.061386, loss: 2.4258
2022-07-11 06:25:12 - train: epoch 0089, iter [01300, 05004], lr: 0.061371, loss: 2.1708
2022-07-11 06:25:46 - train: epoch 0089, iter [01400, 05004], lr: 0.061355, loss: 2.3780
2022-07-11 06:26:21 - train: epoch 0089, iter [01500, 05004], lr: 0.061339, loss: 2.3812
2022-07-11 06:26:54 - train: epoch 0089, iter [01600, 05004], lr: 0.061324, loss: 2.1988
2022-07-11 06:27:30 - train: epoch 0089, iter [01700, 05004], lr: 0.061308, loss: 2.3142
2022-07-11 06:28:04 - train: epoch 0089, iter [01800, 05004], lr: 0.061292, loss: 2.4274
2022-07-11 06:28:39 - train: epoch 0089, iter [01900, 05004], lr: 0.061277, loss: 2.4459
2022-07-11 06:29:14 - train: epoch 0089, iter [02000, 05004], lr: 0.061261, loss: 2.4230
2022-07-11 06:29:48 - train: epoch 0089, iter [02100, 05004], lr: 0.061245, loss: 2.3365
2022-07-11 06:30:23 - train: epoch 0089, iter [02200, 05004], lr: 0.061230, loss: 2.2273
2022-07-11 06:30:57 - train: epoch 0089, iter [02300, 05004], lr: 0.061214, loss: 1.9572
2022-07-11 06:31:32 - train: epoch 0089, iter [02400, 05004], lr: 0.061198, loss: 2.3140
2022-07-11 06:32:06 - train: epoch 0089, iter [02500, 05004], lr: 0.061182, loss: 2.4702
2022-07-11 06:32:40 - train: epoch 0089, iter [02600, 05004], lr: 0.061167, loss: 2.2116
2022-07-11 06:33:15 - train: epoch 0089, iter [02700, 05004], lr: 0.061151, loss: 2.2498
2022-07-11 06:33:50 - train: epoch 0089, iter [02800, 05004], lr: 0.061135, loss: 2.2764
2022-07-11 06:34:24 - train: epoch 0089, iter [02900, 05004], lr: 0.061120, loss: 2.4606
2022-07-11 06:34:59 - train: epoch 0089, iter [03000, 05004], lr: 0.061104, loss: 2.5528
2022-07-11 06:35:34 - train: epoch 0089, iter [03100, 05004], lr: 0.061088, loss: 2.2040
2022-07-11 06:36:08 - train: epoch 0089, iter [03200, 05004], lr: 0.061073, loss: 2.4938
2022-07-11 06:36:43 - train: epoch 0089, iter [03300, 05004], lr: 0.061057, loss: 2.4826
2022-07-11 06:37:17 - train: epoch 0089, iter [03400, 05004], lr: 0.061041, loss: 2.3645
2022-07-11 06:37:52 - train: epoch 0089, iter [03500, 05004], lr: 0.061025, loss: 1.8437
2022-07-11 06:38:27 - train: epoch 0089, iter [03600, 05004], lr: 0.061010, loss: 2.1300
2022-07-11 06:39:02 - train: epoch 0089, iter [03700, 05004], lr: 0.060994, loss: 2.5007
2022-07-11 06:39:36 - train: epoch 0089, iter [03800, 05004], lr: 0.060978, loss: 1.9650
2022-07-11 06:40:11 - train: epoch 0089, iter [03900, 05004], lr: 0.060963, loss: 2.5700
2022-07-11 06:40:46 - train: epoch 0089, iter [04000, 05004], lr: 0.060947, loss: 2.4287
2022-07-11 06:41:21 - train: epoch 0089, iter [04100, 05004], lr: 0.060931, loss: 2.3400
2022-07-11 06:41:54 - train: epoch 0089, iter [04200, 05004], lr: 0.060916, loss: 2.4577
2022-07-11 06:42:30 - train: epoch 0089, iter [04300, 05004], lr: 0.060900, loss: 2.6162
2022-07-11 06:43:04 - train: epoch 0089, iter [04400, 05004], lr: 0.060884, loss: 2.5203
2022-07-11 06:43:39 - train: epoch 0089, iter [04500, 05004], lr: 0.060868, loss: 2.0637
2022-07-11 06:44:14 - train: epoch 0089, iter [04600, 05004], lr: 0.060853, loss: 2.2263
2022-07-11 06:44:49 - train: epoch 0089, iter [04700, 05004], lr: 0.060837, loss: 2.1749
2022-07-11 06:45:24 - train: epoch 0089, iter [04800, 05004], lr: 0.060821, loss: 2.3484
2022-07-11 06:45:59 - train: epoch 0089, iter [04900, 05004], lr: 0.060806, loss: 2.3834
2022-07-11 06:46:33 - train: epoch 0089, iter [05000, 05004], lr: 0.060790, loss: 2.0618
2022-07-11 06:46:34 - train: epoch 089, train_loss: 2.2986
2022-07-11 06:47:47 - eval: epoch: 089, acc1: 61.708%, acc5: 84.612%, test_loss: 1.6023, per_image_load_time: 1.438ms, per_image_inference_time: 0.477ms
2022-07-11 06:47:47 - until epoch: 089, best_acc1: 61.846%
2022-07-11 06:47:47 - epoch 090 lr: 0.060789
2022-07-11 06:48:27 - train: epoch 0090, iter [00100, 05004], lr: 0.060773, loss: 2.4331
2022-07-11 06:49:02 - train: epoch 0090, iter [00200, 05004], lr: 0.060758, loss: 2.3535
2022-07-11 06:49:36 - train: epoch 0090, iter [00300, 05004], lr: 0.060742, loss: 2.1889
2022-07-11 06:50:09 - train: epoch 0090, iter [00400, 05004], lr: 0.060726, loss: 2.1435
2022-07-11 06:50:44 - train: epoch 0090, iter [00500, 05004], lr: 0.060711, loss: 2.0114
2022-07-11 06:51:19 - train: epoch 0090, iter [00600, 05004], lr: 0.060695, loss: 2.4345
2022-07-11 06:51:53 - train: epoch 0090, iter [00700, 05004], lr: 0.060679, loss: 2.3755
2022-07-11 06:52:26 - train: epoch 0090, iter [00800, 05004], lr: 0.060663, loss: 2.2434
2022-07-11 06:53:02 - train: epoch 0090, iter [00900, 05004], lr: 0.060648, loss: 2.2226
2022-07-11 06:53:36 - train: epoch 0090, iter [01000, 05004], lr: 0.060632, loss: 2.1370
2022-07-11 06:54:12 - train: epoch 0090, iter [01100, 05004], lr: 0.060616, loss: 1.9080
2022-07-11 06:54:45 - train: epoch 0090, iter [01200, 05004], lr: 0.060601, loss: 1.8908
2022-07-11 06:55:20 - train: epoch 0090, iter [01300, 05004], lr: 0.060585, loss: 2.1644
2022-07-11 06:55:54 - train: epoch 0090, iter [01400, 05004], lr: 0.060569, loss: 2.3271
2022-07-11 06:56:29 - train: epoch 0090, iter [01500, 05004], lr: 0.060553, loss: 2.4594
2022-07-11 06:57:04 - train: epoch 0090, iter [01600, 05004], lr: 0.060538, loss: 2.2469
2022-07-11 06:57:38 - train: epoch 0090, iter [01700, 05004], lr: 0.060522, loss: 1.8887
2022-07-11 06:58:12 - train: epoch 0090, iter [01800, 05004], lr: 0.060506, loss: 2.2902
2022-07-11 06:58:47 - train: epoch 0090, iter [01900, 05004], lr: 0.060490, loss: 1.9899
2022-07-11 06:59:22 - train: epoch 0090, iter [02000, 05004], lr: 0.060475, loss: 2.1953
2022-07-11 06:59:56 - train: epoch 0090, iter [02100, 05004], lr: 0.060459, loss: 2.1932
2022-07-11 07:00:30 - train: epoch 0090, iter [02200, 05004], lr: 0.060443, loss: 2.3053
2022-07-11 07:01:05 - train: epoch 0090, iter [02300, 05004], lr: 0.060427, loss: 2.1099
2022-07-11 07:01:39 - train: epoch 0090, iter [02400, 05004], lr: 0.060412, loss: 2.2249
2022-07-11 07:02:14 - train: epoch 0090, iter [02500, 05004], lr: 0.060396, loss: 2.3312
2022-07-11 07:02:49 - train: epoch 0090, iter [02600, 05004], lr: 0.060380, loss: 2.1795
2022-07-11 07:03:23 - train: epoch 0090, iter [02700, 05004], lr: 0.060364, loss: 2.0745
2022-07-11 07:03:58 - train: epoch 0090, iter [02800, 05004], lr: 0.060349, loss: 2.1299
2022-07-11 07:04:32 - train: epoch 0090, iter [02900, 05004], lr: 0.060333, loss: 2.4442
2022-07-11 07:05:08 - train: epoch 0090, iter [03000, 05004], lr: 0.060317, loss: 2.5071
2022-07-11 07:05:42 - train: epoch 0090, iter [03100, 05004], lr: 0.060301, loss: 1.9123
2022-07-11 07:06:17 - train: epoch 0090, iter [03200, 05004], lr: 0.060286, loss: 2.2591
2022-07-11 07:06:52 - train: epoch 0090, iter [03300, 05004], lr: 0.060270, loss: 2.5163
2022-07-11 07:07:26 - train: epoch 0090, iter [03400, 05004], lr: 0.060254, loss: 2.2335
2022-07-11 07:08:01 - train: epoch 0090, iter [03500, 05004], lr: 0.060238, loss: 2.0590
2022-07-11 07:08:35 - train: epoch 0090, iter [03600, 05004], lr: 0.060223, loss: 1.9307
2022-07-11 07:09:10 - train: epoch 0090, iter [03700, 05004], lr: 0.060207, loss: 2.3664
2022-07-11 07:09:44 - train: epoch 0090, iter [03800, 05004], lr: 0.060191, loss: 2.1571
2022-07-11 07:10:19 - train: epoch 0090, iter [03900, 05004], lr: 0.060175, loss: 1.9068
2022-07-11 07:10:54 - train: epoch 0090, iter [04000, 05004], lr: 0.060160, loss: 2.1233
2022-07-11 07:11:29 - train: epoch 0090, iter [04100, 05004], lr: 0.060144, loss: 2.5153
2022-07-11 07:12:04 - train: epoch 0090, iter [04200, 05004], lr: 0.060128, loss: 2.2495
2022-07-11 07:12:38 - train: epoch 0090, iter [04300, 05004], lr: 0.060112, loss: 2.4786
2022-07-11 07:13:12 - train: epoch 0090, iter [04400, 05004], lr: 0.060097, loss: 2.2559
2022-07-11 07:13:47 - train: epoch 0090, iter [04500, 05004], lr: 0.060081, loss: 1.9470
2022-07-11 07:14:21 - train: epoch 0090, iter [04600, 05004], lr: 0.060065, loss: 2.3781
2022-07-11 07:14:56 - train: epoch 0090, iter [04700, 05004], lr: 0.060049, loss: 2.1515
2022-07-11 07:15:30 - train: epoch 0090, iter [04800, 05004], lr: 0.060033, loss: 2.6616
2022-07-11 07:16:06 - train: epoch 0090, iter [04900, 05004], lr: 0.060018, loss: 2.1234
2022-07-11 07:16:40 - train: epoch 0090, iter [05000, 05004], lr: 0.060002, loss: 2.2525
2022-07-11 07:16:41 - train: epoch 090, train_loss: 2.2906
2022-07-11 07:17:55 - eval: epoch: 090, acc1: 62.590%, acc5: 85.186%, test_loss: 1.5556, per_image_load_time: 0.820ms, per_image_inference_time: 0.458ms
2022-07-11 07:17:55 - until epoch: 090, best_acc1: 62.590%
2022-07-11 07:17:55 - epoch 091 lr: 0.060001
2022-07-11 07:18:35 - train: epoch 0091, iter [00100, 05004], lr: 0.059986, loss: 2.3393
2022-07-11 07:19:09 - train: epoch 0091, iter [00200, 05004], lr: 0.059970, loss: 2.0562
2022-07-11 07:19:44 - train: epoch 0091, iter [00300, 05004], lr: 0.059954, loss: 2.3173
2022-07-11 07:20:18 - train: epoch 0091, iter [00400, 05004], lr: 0.059938, loss: 2.4026
2022-07-11 07:20:53 - train: epoch 0091, iter [00500, 05004], lr: 0.059922, loss: 2.1838
2022-07-11 07:21:28 - train: epoch 0091, iter [00600, 05004], lr: 0.059907, loss: 2.5700
2022-07-11 07:22:03 - train: epoch 0091, iter [00700, 05004], lr: 0.059891, loss: 2.6002
2022-07-11 07:22:36 - train: epoch 0091, iter [00800, 05004], lr: 0.059875, loss: 2.3055
2022-07-11 07:23:12 - train: epoch 0091, iter [00900, 05004], lr: 0.059859, loss: 2.2613
2022-07-11 07:23:47 - train: epoch 0091, iter [01000, 05004], lr: 0.059844, loss: 2.1109
2022-07-11 07:24:21 - train: epoch 0091, iter [01100, 05004], lr: 0.059828, loss: 1.9343
2022-07-11 07:24:55 - train: epoch 0091, iter [01200, 05004], lr: 0.059812, loss: 2.2870
2022-07-11 07:25:31 - train: epoch 0091, iter [01300, 05004], lr: 0.059796, loss: 2.1326
2022-07-11 07:26:06 - train: epoch 0091, iter [01400, 05004], lr: 0.059780, loss: 2.3438
2022-07-11 07:26:41 - train: epoch 0091, iter [01500, 05004], lr: 0.059765, loss: 2.2476
2022-07-11 07:27:15 - train: epoch 0091, iter [01600, 05004], lr: 0.059749, loss: 2.3039
2022-07-11 07:27:50 - train: epoch 0091, iter [01700, 05004], lr: 0.059733, loss: 2.3321
2022-07-11 07:28:25 - train: epoch 0091, iter [01800, 05004], lr: 0.059717, loss: 2.4414
2022-07-11 07:29:00 - train: epoch 0091, iter [01900, 05004], lr: 0.059701, loss: 2.1823
2022-07-11 07:29:35 - train: epoch 0091, iter [02000, 05004], lr: 0.059686, loss: 2.4017
2022-07-11 07:30:09 - train: epoch 0091, iter [02100, 05004], lr: 0.059670, loss: 2.0970
2022-07-11 07:30:43 - train: epoch 0091, iter [02200, 05004], lr: 0.059654, loss: 2.3593
2022-07-11 07:31:18 - train: epoch 0091, iter [02300, 05004], lr: 0.059638, loss: 2.2993
2022-07-11 07:31:53 - train: epoch 0091, iter [02400, 05004], lr: 0.059622, loss: 2.0520
2022-07-11 07:32:28 - train: epoch 0091, iter [02500, 05004], lr: 0.059607, loss: 2.3705
2022-07-11 07:33:02 - train: epoch 0091, iter [02600, 05004], lr: 0.059591, loss: 2.3807
2022-07-11 07:33:37 - train: epoch 0091, iter [02700, 05004], lr: 0.059575, loss: 2.5894
2022-07-11 07:34:12 - train: epoch 0091, iter [02800, 05004], lr: 0.059559, loss: 2.4684
2022-07-11 07:34:47 - train: epoch 0091, iter [02900, 05004], lr: 0.059543, loss: 2.5410
2022-07-11 07:35:20 - train: epoch 0091, iter [03000, 05004], lr: 0.059528, loss: 2.4185
2022-07-11 07:35:55 - train: epoch 0091, iter [03100, 05004], lr: 0.059512, loss: 2.3367
2022-07-11 07:36:30 - train: epoch 0091, iter [03200, 05004], lr: 0.059496, loss: 2.2935
2022-07-11 07:37:05 - train: epoch 0091, iter [03300, 05004], lr: 0.059480, loss: 2.1881
2022-07-11 07:37:39 - train: epoch 0091, iter [03400, 05004], lr: 0.059464, loss: 2.2873
2022-07-11 07:38:14 - train: epoch 0091, iter [03500, 05004], lr: 0.059449, loss: 2.3538
2022-07-11 07:38:49 - train: epoch 0091, iter [03600, 05004], lr: 0.059433, loss: 2.2267
2022-07-11 07:39:24 - train: epoch 0091, iter [03700, 05004], lr: 0.059417, loss: 2.3813
2022-07-11 07:39:59 - train: epoch 0091, iter [03800, 05004], lr: 0.059401, loss: 2.3579
2022-07-11 07:40:34 - train: epoch 0091, iter [03900, 05004], lr: 0.059385, loss: 2.1816
2022-07-11 07:41:08 - train: epoch 0091, iter [04000, 05004], lr: 0.059370, loss: 2.1793
2022-07-11 07:41:43 - train: epoch 0091, iter [04100, 05004], lr: 0.059354, loss: 2.3612
2022-07-11 07:42:17 - train: epoch 0091, iter [04200, 05004], lr: 0.059338, loss: 2.2216
2022-07-11 07:42:52 - train: epoch 0091, iter [04300, 05004], lr: 0.059322, loss: 2.1954
2022-07-11 07:43:27 - train: epoch 0091, iter [04400, 05004], lr: 0.059306, loss: 2.1834
2022-07-11 07:44:01 - train: epoch 0091, iter [04500, 05004], lr: 0.059290, loss: 2.2240
2022-07-11 07:44:37 - train: epoch 0091, iter [04600, 05004], lr: 0.059275, loss: 2.3811
2022-07-11 07:45:11 - train: epoch 0091, iter [04700, 05004], lr: 0.059259, loss: 2.1885
2022-07-11 07:45:46 - train: epoch 0091, iter [04800, 05004], lr: 0.059243, loss: 2.2791
2022-07-11 07:46:21 - train: epoch 0091, iter [04900, 05004], lr: 0.059227, loss: 2.1893
2022-07-11 07:46:55 - train: epoch 0091, iter [05000, 05004], lr: 0.059211, loss: 2.4590
2022-07-11 07:46:56 - train: epoch 091, train_loss: 2.2849
2022-07-11 07:48:09 - eval: epoch: 091, acc1: 61.594%, acc5: 84.572%, test_loss: 1.6010, per_image_load_time: 1.728ms, per_image_inference_time: 0.456ms
2022-07-11 07:48:10 - until epoch: 091, best_acc1: 62.590%
2022-07-11 07:48:10 - epoch 092 lr: 0.059211
2022-07-11 07:48:50 - train: epoch 0092, iter [00100, 05004], lr: 0.059195, loss: 2.2095
2022-07-11 07:49:25 - train: epoch 0092, iter [00200, 05004], lr: 0.059179, loss: 2.3227
2022-07-11 07:49:58 - train: epoch 0092, iter [00300, 05004], lr: 0.059163, loss: 2.1906
2022-07-11 07:50:32 - train: epoch 0092, iter [00400, 05004], lr: 0.059147, loss: 2.3096
2022-07-11 07:51:06 - train: epoch 0092, iter [00500, 05004], lr: 0.059132, loss: 2.1480
2022-07-11 07:51:39 - train: epoch 0092, iter [00600, 05004], lr: 0.059116, loss: 2.0756
2022-07-11 07:52:13 - train: epoch 0092, iter [00700, 05004], lr: 0.059100, loss: 2.2931
2022-07-11 07:52:47 - train: epoch 0092, iter [00800, 05004], lr: 0.059084, loss: 2.5526
2022-07-11 07:53:21 - train: epoch 0092, iter [00900, 05004], lr: 0.059068, loss: 2.3509
2022-07-11 07:53:56 - train: epoch 0092, iter [01000, 05004], lr: 0.059052, loss: 2.4308
2022-07-11 07:54:31 - train: epoch 0092, iter [01100, 05004], lr: 0.059037, loss: 2.3053
2022-07-11 07:55:05 - train: epoch 0092, iter [01200, 05004], lr: 0.059021, loss: 2.2323
2022-07-11 07:55:39 - train: epoch 0092, iter [01300, 05004], lr: 0.059005, loss: 2.2161
2022-07-11 07:56:13 - train: epoch 0092, iter [01400, 05004], lr: 0.058989, loss: 2.2623
2022-07-11 07:56:49 - train: epoch 0092, iter [01500, 05004], lr: 0.058973, loss: 2.1948
2022-07-11 07:57:22 - train: epoch 0092, iter [01600, 05004], lr: 0.058957, loss: 2.4141
2022-07-11 07:57:56 - train: epoch 0092, iter [01700, 05004], lr: 0.058942, loss: 2.1268
2022-07-11 07:58:30 - train: epoch 0092, iter [01800, 05004], lr: 0.058926, loss: 2.4042
2022-07-11 07:59:05 - train: epoch 0092, iter [01900, 05004], lr: 0.058910, loss: 2.1306
2022-07-11 07:59:40 - train: epoch 0092, iter [02000, 05004], lr: 0.058894, loss: 2.1042
2022-07-11 08:00:14 - train: epoch 0092, iter [02100, 05004], lr: 0.058878, loss: 2.4419
2022-07-11 08:00:48 - train: epoch 0092, iter [02200, 05004], lr: 0.058862, loss: 2.3995
2022-07-11 08:01:22 - train: epoch 0092, iter [02300, 05004], lr: 0.058847, loss: 2.2205
2022-07-11 08:01:57 - train: epoch 0092, iter [02400, 05004], lr: 0.058831, loss: 2.2268
2022-07-11 08:02:31 - train: epoch 0092, iter [02500, 05004], lr: 0.058815, loss: 2.2019
2022-07-11 08:03:05 - train: epoch 0092, iter [02600, 05004], lr: 0.058799, loss: 2.3909
2022-07-11 08:03:40 - train: epoch 0092, iter [02700, 05004], lr: 0.058783, loss: 2.3245
2022-07-11 08:04:16 - train: epoch 0092, iter [02800, 05004], lr: 0.058767, loss: 2.1251
2022-07-11 08:04:50 - train: epoch 0092, iter [02900, 05004], lr: 0.058752, loss: 2.3430
2022-07-11 08:05:23 - train: epoch 0092, iter [03000, 05004], lr: 0.058736, loss: 2.3676
2022-07-11 08:05:58 - train: epoch 0092, iter [03100, 05004], lr: 0.058720, loss: 2.5420
2022-07-11 08:06:33 - train: epoch 0092, iter [03200, 05004], lr: 0.058704, loss: 2.1200
2022-07-11 08:07:08 - train: epoch 0092, iter [03300, 05004], lr: 0.058688, loss: 2.5115
2022-07-11 08:07:42 - train: epoch 0092, iter [03400, 05004], lr: 0.058672, loss: 2.4526
2022-07-11 08:08:15 - train: epoch 0092, iter [03500, 05004], lr: 0.058656, loss: 2.1728
2022-07-11 08:08:50 - train: epoch 0092, iter [03600, 05004], lr: 0.058641, loss: 2.5919
2022-07-11 08:09:25 - train: epoch 0092, iter [03700, 05004], lr: 0.058625, loss: 2.4498
2022-07-11 08:09:59 - train: epoch 0092, iter [03800, 05004], lr: 0.058609, loss: 2.5120
2022-07-11 08:10:34 - train: epoch 0092, iter [03900, 05004], lr: 0.058593, loss: 2.2946
2022-07-11 08:11:07 - train: epoch 0092, iter [04000, 05004], lr: 0.058577, loss: 2.3768
2022-07-11 08:11:43 - train: epoch 0092, iter [04100, 05004], lr: 0.058561, loss: 2.1590
2022-07-11 08:12:17 - train: epoch 0092, iter [04200, 05004], lr: 0.058545, loss: 2.2616
2022-07-11 08:12:51 - train: epoch 0092, iter [04300, 05004], lr: 0.058530, loss: 2.0754
2022-07-11 08:13:25 - train: epoch 0092, iter [04400, 05004], lr: 0.058514, loss: 2.1215
2022-07-11 08:14:01 - train: epoch 0092, iter [04500, 05004], lr: 0.058498, loss: 2.3325
2022-07-11 08:14:35 - train: epoch 0092, iter [04600, 05004], lr: 0.058482, loss: 2.3537
2022-07-11 08:15:10 - train: epoch 0092, iter [04700, 05004], lr: 0.058466, loss: 2.0723
2022-07-11 08:15:43 - train: epoch 0092, iter [04800, 05004], lr: 0.058450, loss: 2.1804
2022-07-11 08:16:18 - train: epoch 0092, iter [04900, 05004], lr: 0.058434, loss: 2.3575
2022-07-11 08:16:51 - train: epoch 0092, iter [05000, 05004], lr: 0.058418, loss: 2.0958
2022-07-11 08:16:52 - train: epoch 092, train_loss: 2.2805
2022-07-11 08:18:08 - eval: epoch: 092, acc1: 62.928%, acc5: 85.470%, test_loss: 1.5365, per_image_load_time: 1.719ms, per_image_inference_time: 0.469ms
2022-07-11 08:18:08 - until epoch: 092, best_acc1: 62.928%
2022-07-11 08:18:08 - epoch 093 lr: 0.058418
2022-07-11 08:18:48 - train: epoch 0093, iter [00100, 05004], lr: 0.058402, loss: 2.3245
2022-07-11 08:19:23 - train: epoch 0093, iter [00200, 05004], lr: 0.058386, loss: 2.1358
2022-07-11 08:19:58 - train: epoch 0093, iter [00300, 05004], lr: 0.058370, loss: 2.2206
2022-07-11 08:20:33 - train: epoch 0093, iter [00400, 05004], lr: 0.058354, loss: 2.4648
2022-07-11 08:21:06 - train: epoch 0093, iter [00500, 05004], lr: 0.058339, loss: 2.2407
2022-07-11 08:21:41 - train: epoch 0093, iter [00600, 05004], lr: 0.058323, loss: 2.2297
2022-07-11 08:22:16 - train: epoch 0093, iter [00700, 05004], lr: 0.058307, loss: 2.2112
2022-07-11 08:22:51 - train: epoch 0093, iter [00800, 05004], lr: 0.058291, loss: 2.2199
2022-07-11 08:23:24 - train: epoch 0093, iter [00900, 05004], lr: 0.058275, loss: 2.2329
2022-07-11 08:23:59 - train: epoch 0093, iter [01000, 05004], lr: 0.058259, loss: 2.0667
2022-07-11 08:24:34 - train: epoch 0093, iter [01100, 05004], lr: 0.058243, loss: 2.4362
2022-07-11 08:25:09 - train: epoch 0093, iter [01200, 05004], lr: 0.058227, loss: 2.3807
2022-07-11 08:25:44 - train: epoch 0093, iter [01300, 05004], lr: 0.058211, loss: 2.4109
2022-07-11 08:26:18 - train: epoch 0093, iter [01400, 05004], lr: 0.058196, loss: 2.3686
2022-07-11 08:26:52 - train: epoch 0093, iter [01500, 05004], lr: 0.058180, loss: 2.4077
2022-07-11 08:27:28 - train: epoch 0093, iter [01600, 05004], lr: 0.058164, loss: 2.4137
2022-07-11 08:28:02 - train: epoch 0093, iter [01700, 05004], lr: 0.058148, loss: 2.2536
2022-07-11 08:28:37 - train: epoch 0093, iter [01800, 05004], lr: 0.058132, loss: 2.2955
2022-07-11 08:29:11 - train: epoch 0093, iter [01900, 05004], lr: 0.058116, loss: 2.3058
2022-07-11 08:29:46 - train: epoch 0093, iter [02000, 05004], lr: 0.058100, loss: 2.1662
2022-07-11 08:30:22 - train: epoch 0093, iter [02100, 05004], lr: 0.058084, loss: 2.0132
2022-07-11 08:30:55 - train: epoch 0093, iter [02200, 05004], lr: 0.058069, loss: 2.3822
2022-07-11 08:31:30 - train: epoch 0093, iter [02300, 05004], lr: 0.058053, loss: 1.9619
2022-07-11 08:32:05 - train: epoch 0093, iter [02400, 05004], lr: 0.058037, loss: 2.1151
2022-07-11 08:32:40 - train: epoch 0093, iter [02500, 05004], lr: 0.058021, loss: 2.3226
2022-07-11 08:33:15 - train: epoch 0093, iter [02600, 05004], lr: 0.058005, loss: 2.6365
2022-07-11 08:33:49 - train: epoch 0093, iter [02700, 05004], lr: 0.057989, loss: 2.2972
2022-07-11 08:34:25 - train: epoch 0093, iter [02800, 05004], lr: 0.057973, loss: 2.1161
2022-07-11 08:34:59 - train: epoch 0093, iter [02900, 05004], lr: 0.057957, loss: 2.1767
2022-07-11 08:35:35 - train: epoch 0093, iter [03000, 05004], lr: 0.057941, loss: 2.0659
2022-07-11 08:36:10 - train: epoch 0093, iter [03100, 05004], lr: 0.057926, loss: 2.2516
2022-07-11 08:36:45 - train: epoch 0093, iter [03200, 05004], lr: 0.057910, loss: 2.0950
2022-07-11 08:37:20 - train: epoch 0093, iter [03300, 05004], lr: 0.057894, loss: 2.0073
2022-07-11 08:37:55 - train: epoch 0093, iter [03400, 05004], lr: 0.057878, loss: 2.2273
2022-07-11 08:38:30 - train: epoch 0093, iter [03500, 05004], lr: 0.057862, loss: 2.1971
2022-07-11 08:39:06 - train: epoch 0093, iter [03600, 05004], lr: 0.057846, loss: 2.2297
2022-07-11 08:39:39 - train: epoch 0093, iter [03700, 05004], lr: 0.057830, loss: 2.0904
2022-07-11 08:40:15 - train: epoch 0093, iter [03800, 05004], lr: 0.057814, loss: 2.2590
2022-07-11 08:40:50 - train: epoch 0093, iter [03900, 05004], lr: 0.057798, loss: 2.1396
2022-07-11 08:41:25 - train: epoch 0093, iter [04000, 05004], lr: 0.057782, loss: 2.4504
2022-07-11 08:42:00 - train: epoch 0093, iter [04100, 05004], lr: 0.057767, loss: 2.2689
2022-07-11 08:42:35 - train: epoch 0093, iter [04200, 05004], lr: 0.057751, loss: 2.2175
2022-07-11 08:43:09 - train: epoch 0093, iter [04300, 05004], lr: 0.057735, loss: 2.3764
2022-07-11 08:43:46 - train: epoch 0093, iter [04400, 05004], lr: 0.057719, loss: 2.3048
2022-07-11 08:44:20 - train: epoch 0093, iter [04500, 05004], lr: 0.057703, loss: 2.3205
2022-07-11 08:44:55 - train: epoch 0093, iter [04600, 05004], lr: 0.057687, loss: 2.5200
2022-07-11 08:45:31 - train: epoch 0093, iter [04700, 05004], lr: 0.057671, loss: 2.5046
2022-07-11 08:46:06 - train: epoch 0093, iter [04800, 05004], lr: 0.057655, loss: 2.2117
2022-07-11 08:46:41 - train: epoch 0093, iter [04900, 05004], lr: 0.057639, loss: 2.3011
2022-07-11 08:47:15 - train: epoch 0093, iter [05000, 05004], lr: 0.057623, loss: 2.4114
2022-07-11 08:47:16 - train: epoch 093, train_loss: 2.2749
2022-07-11 08:48:32 - eval: epoch: 093, acc1: 59.276%, acc5: 83.054%, test_loss: 1.6945, per_image_load_time: 2.438ms, per_image_inference_time: 0.493ms
2022-07-11 08:48:32 - until epoch: 093, best_acc1: 62.928%
2022-07-11 08:48:32 - epoch 094 lr: 0.057623
2022-07-11 08:49:12 - train: epoch 0094, iter [00100, 05004], lr: 0.057607, loss: 2.4792
2022-07-11 08:49:47 - train: epoch 0094, iter [00200, 05004], lr: 0.057591, loss: 2.3022
2022-07-11 08:50:21 - train: epoch 0094, iter [00300, 05004], lr: 0.057575, loss: 2.0781
2022-07-11 08:50:56 - train: epoch 0094, iter [00400, 05004], lr: 0.057559, loss: 2.3170
2022-07-11 08:51:31 - train: epoch 0094, iter [00500, 05004], lr: 0.057543, loss: 2.0221
2022-07-11 08:52:05 - train: epoch 0094, iter [00600, 05004], lr: 0.057527, loss: 2.3242
2022-07-11 08:52:40 - train: epoch 0094, iter [00700, 05004], lr: 0.057511, loss: 2.2504
2022-07-11 08:53:14 - train: epoch 0094, iter [00800, 05004], lr: 0.057495, loss: 2.2253
2022-07-11 08:53:49 - train: epoch 0094, iter [00900, 05004], lr: 0.057480, loss: 2.2425
2022-07-11 08:54:24 - train: epoch 0094, iter [01000, 05004], lr: 0.057464, loss: 2.1899
2022-07-11 08:55:00 - train: epoch 0094, iter [01100, 05004], lr: 0.057448, loss: 2.4914
2022-07-11 08:55:34 - train: epoch 0094, iter [01200, 05004], lr: 0.057432, loss: 2.1085
2022-07-11 08:56:09 - train: epoch 0094, iter [01300, 05004], lr: 0.057416, loss: 2.2524
2022-07-11 08:56:44 - train: epoch 0094, iter [01400, 05004], lr: 0.057400, loss: 2.1807
2022-07-11 08:57:19 - train: epoch 0094, iter [01500, 05004], lr: 0.057384, loss: 2.1069
2022-07-11 08:57:55 - train: epoch 0094, iter [01600, 05004], lr: 0.057368, loss: 2.5893
2022-07-11 08:58:29 - train: epoch 0094, iter [01700, 05004], lr: 0.057352, loss: 2.1209
2022-07-11 08:59:04 - train: epoch 0094, iter [01800, 05004], lr: 0.057336, loss: 2.2499
2022-07-11 08:59:40 - train: epoch 0094, iter [01900, 05004], lr: 0.057320, loss: 2.1583
2022-07-11 09:00:14 - train: epoch 0094, iter [02000, 05004], lr: 0.057304, loss: 2.0535
2022-07-11 09:00:49 - train: epoch 0094, iter [02100, 05004], lr: 0.057288, loss: 2.0325
2022-07-11 09:01:24 - train: epoch 0094, iter [02200, 05004], lr: 0.057273, loss: 2.2159
2022-07-11 09:01:59 - train: epoch 0094, iter [02300, 05004], lr: 0.057257, loss: 2.2635
2022-07-11 09:02:35 - train: epoch 0094, iter [02400, 05004], lr: 0.057241, loss: 2.0079
2022-07-11 09:03:09 - train: epoch 0094, iter [02500, 05004], lr: 0.057225, loss: 2.2164
2022-07-11 09:03:44 - train: epoch 0094, iter [02600, 05004], lr: 0.057209, loss: 2.0762
2022-07-11 09:04:20 - train: epoch 0094, iter [02700, 05004], lr: 0.057193, loss: 2.0716
2022-07-11 09:04:54 - train: epoch 0094, iter [02800, 05004], lr: 0.057177, loss: 2.2136
2022-07-11 09:05:30 - train: epoch 0094, iter [02900, 05004], lr: 0.057161, loss: 2.0950
2022-07-11 09:06:05 - train: epoch 0094, iter [03000, 05004], lr: 0.057145, loss: 2.5559
2022-07-11 09:06:41 - train: epoch 0094, iter [03100, 05004], lr: 0.057129, loss: 2.1266
2022-07-11 09:07:15 - train: epoch 0094, iter [03200, 05004], lr: 0.057113, loss: 2.6012
2022-07-11 09:07:51 - train: epoch 0094, iter [03300, 05004], lr: 0.057097, loss: 2.2475
2022-07-11 09:08:25 - train: epoch 0094, iter [03400, 05004], lr: 0.057081, loss: 2.3201
2022-07-11 09:09:01 - train: epoch 0094, iter [03500, 05004], lr: 0.057065, loss: 2.4795
2022-07-11 09:09:36 - train: epoch 0094, iter [03600, 05004], lr: 0.057050, loss: 2.0710
2022-07-11 09:10:11 - train: epoch 0094, iter [03700, 05004], lr: 0.057034, loss: 2.4867
2022-07-11 09:10:46 - train: epoch 0094, iter [03800, 05004], lr: 0.057018, loss: 2.3301
2022-07-11 09:11:22 - train: epoch 0094, iter [03900, 05004], lr: 0.057002, loss: 2.2427
2022-07-11 09:11:56 - train: epoch 0094, iter [04000, 05004], lr: 0.056986, loss: 2.1231
2022-07-11 09:12:31 - train: epoch 0094, iter [04100, 05004], lr: 0.056970, loss: 2.5529
2022-07-11 09:13:06 - train: epoch 0094, iter [04200, 05004], lr: 0.056954, loss: 2.1652
2022-07-11 09:13:41 - train: epoch 0094, iter [04300, 05004], lr: 0.056938, loss: 2.5460
2022-07-11 09:14:16 - train: epoch 0094, iter [04400, 05004], lr: 0.056922, loss: 2.3138
2022-07-11 09:14:52 - train: epoch 0094, iter [04500, 05004], lr: 0.056906, loss: 2.4464
2022-07-11 09:15:26 - train: epoch 0094, iter [04600, 05004], lr: 0.056890, loss: 2.3907
2022-07-11 09:16:02 - train: epoch 0094, iter [04700, 05004], lr: 0.056874, loss: 2.2539
2022-07-11 09:16:37 - train: epoch 0094, iter [04800, 05004], lr: 0.056858, loss: 2.2947
2022-07-11 09:17:12 - train: epoch 0094, iter [04900, 05004], lr: 0.056842, loss: 2.4874
2022-07-11 09:17:45 - train: epoch 0094, iter [05000, 05004], lr: 0.056826, loss: 2.2297
2022-07-11 09:17:47 - train: epoch 094, train_loss: 2.2696
2022-07-11 09:19:02 - eval: epoch: 094, acc1: 63.402%, acc5: 85.474%, test_loss: 1.5246, per_image_load_time: 2.344ms, per_image_inference_time: 0.512ms
2022-07-11 09:19:03 - until epoch: 094, best_acc1: 63.402%
2022-07-11 09:19:03 - epoch 095 lr: 0.056826
2022-07-11 09:19:42 - train: epoch 0095, iter [00100, 05004], lr: 0.056810, loss: 2.2053
2022-07-11 09:20:17 - train: epoch 0095, iter [00200, 05004], lr: 0.056794, loss: 2.4241
2022-07-11 09:20:52 - train: epoch 0095, iter [00300, 05004], lr: 0.056778, loss: 2.3183
2022-07-11 09:21:26 - train: epoch 0095, iter [00400, 05004], lr: 0.056762, loss: 2.3092
2022-07-11 09:22:01 - train: epoch 0095, iter [00500, 05004], lr: 0.056746, loss: 2.2749
2022-07-11 09:22:36 - train: epoch 0095, iter [00600, 05004], lr: 0.056730, loss: 2.0446
2022-07-11 09:23:11 - train: epoch 0095, iter [00700, 05004], lr: 0.056714, loss: 2.3470
2022-07-11 09:23:45 - train: epoch 0095, iter [00800, 05004], lr: 0.056698, loss: 2.2932
2022-07-11 09:24:21 - train: epoch 0095, iter [00900, 05004], lr: 0.056682, loss: 2.5629
2022-07-11 09:24:55 - train: epoch 0095, iter [01000, 05004], lr: 0.056666, loss: 2.2751
2022-07-11 09:25:30 - train: epoch 0095, iter [01100, 05004], lr: 0.056650, loss: 2.1861
2022-07-11 09:26:05 - train: epoch 0095, iter [01200, 05004], lr: 0.056634, loss: 2.4227
2022-07-11 09:26:40 - train: epoch 0095, iter [01300, 05004], lr: 0.056618, loss: 2.1616
2022-07-11 09:27:15 - train: epoch 0095, iter [01400, 05004], lr: 0.056602, loss: 2.1885
2022-07-11 09:27:49 - train: epoch 0095, iter [01500, 05004], lr: 0.056586, loss: 2.1885
2022-07-11 09:28:25 - train: epoch 0095, iter [01600, 05004], lr: 0.056570, loss: 2.1310
2022-07-11 09:28:59 - train: epoch 0095, iter [01700, 05004], lr: 0.056554, loss: 2.0143
2022-07-11 09:29:33 - train: epoch 0095, iter [01800, 05004], lr: 0.056539, loss: 2.3653
2022-07-11 09:30:09 - train: epoch 0095, iter [01900, 05004], lr: 0.056523, loss: 2.3022
2022-07-11 09:30:44 - train: epoch 0095, iter [02000, 05004], lr: 0.056507, loss: 2.3157
2022-07-11 09:31:19 - train: epoch 0095, iter [02100, 05004], lr: 0.056491, loss: 2.4431
2022-07-11 09:31:54 - train: epoch 0095, iter [02200, 05004], lr: 0.056475, loss: 1.8253
2022-07-11 09:32:29 - train: epoch 0095, iter [02300, 05004], lr: 0.056459, loss: 1.8746
2022-07-11 09:33:05 - train: epoch 0095, iter [02400, 05004], lr: 0.056443, loss: 2.2045
2022-07-11 09:33:39 - train: epoch 0095, iter [02500, 05004], lr: 0.056427, loss: 2.0601
2022-07-11 09:34:14 - train: epoch 0095, iter [02600, 05004], lr: 0.056411, loss: 2.4332
2022-07-11 09:34:49 - train: epoch 0095, iter [02700, 05004], lr: 0.056395, loss: 2.1705
2022-07-11 09:35:24 - train: epoch 0095, iter [02800, 05004], lr: 0.056379, loss: 1.9982
2022-07-11 09:35:59 - train: epoch 0095, iter [02900, 05004], lr: 0.056363, loss: 2.4100
2022-07-11 09:36:34 - train: epoch 0095, iter [03000, 05004], lr: 0.056347, loss: 2.4231
2022-07-11 09:37:09 - train: epoch 0095, iter [03100, 05004], lr: 0.056331, loss: 2.4520
2022-07-11 09:37:45 - train: epoch 0095, iter [03200, 05004], lr: 0.056315, loss: 2.1038
2022-07-11 09:38:19 - train: epoch 0095, iter [03300, 05004], lr: 0.056299, loss: 2.2093
2022-07-11 09:38:55 - train: epoch 0095, iter [03400, 05004], lr: 0.056283, loss: 1.9866
2022-07-11 09:39:29 - train: epoch 0095, iter [03500, 05004], lr: 0.056267, loss: 2.2196
2022-07-11 09:40:05 - train: epoch 0095, iter [03600, 05004], lr: 0.056251, loss: 1.9869
2022-07-11 09:40:40 - train: epoch 0095, iter [03700, 05004], lr: 0.056235, loss: 2.3286
2022-07-11 09:41:15 - train: epoch 0095, iter [03800, 05004], lr: 0.056219, loss: 2.2173
2022-07-11 09:41:51 - train: epoch 0095, iter [03900, 05004], lr: 0.056203, loss: 2.0344
2022-07-11 09:42:26 - train: epoch 0095, iter [04000, 05004], lr: 0.056187, loss: 2.2188
2022-07-11 09:43:01 - train: epoch 0095, iter [04100, 05004], lr: 0.056171, loss: 2.2544
2022-07-11 09:43:36 - train: epoch 0095, iter [04200, 05004], lr: 0.056155, loss: 2.1724
2022-07-11 09:44:11 - train: epoch 0095, iter [04300, 05004], lr: 0.056139, loss: 2.1985
2022-07-11 09:44:46 - train: epoch 0095, iter [04400, 05004], lr: 0.056123, loss: 2.5831
2022-07-11 09:45:22 - train: epoch 0095, iter [04500, 05004], lr: 0.056107, loss: 2.3023
2022-07-11 09:45:57 - train: epoch 0095, iter [04600, 05004], lr: 0.056091, loss: 2.2348
2022-07-11 09:46:31 - train: epoch 0095, iter [04700, 05004], lr: 0.056075, loss: 2.2351
2022-07-11 09:47:08 - train: epoch 0095, iter [04800, 05004], lr: 0.056059, loss: 2.2851
2022-07-11 09:47:42 - train: epoch 0095, iter [04900, 05004], lr: 0.056043, loss: 2.3171
2022-07-11 09:48:16 - train: epoch 0095, iter [05000, 05004], lr: 0.056027, loss: 2.0880
2022-07-11 09:48:17 - train: epoch 095, train_loss: 2.2616
2022-07-11 09:49:32 - eval: epoch: 095, acc1: 62.358%, acc5: 85.112%, test_loss: 1.5634, per_image_load_time: 1.954ms, per_image_inference_time: 0.492ms
2022-07-11 09:49:32 - until epoch: 095, best_acc1: 63.402%
2022-07-11 09:49:32 - epoch 096 lr: 0.056027
2022-07-11 09:50:12 - train: epoch 0096, iter [00100, 05004], lr: 0.056011, loss: 2.1573
2022-07-11 09:50:45 - train: epoch 0096, iter [00200, 05004], lr: 0.055995, loss: 2.4184
2022-07-11 09:51:20 - train: epoch 0096, iter [00300, 05004], lr: 0.055979, loss: 2.1967
2022-07-11 09:51:55 - train: epoch 0096, iter [00400, 05004], lr: 0.055963, loss: 2.0377
2022-07-11 09:52:30 - train: epoch 0096, iter [00500, 05004], lr: 0.055947, loss: 2.3625
2022-07-11 09:53:04 - train: epoch 0096, iter [00600, 05004], lr: 0.055931, loss: 2.3851
2022-07-11 09:53:38 - train: epoch 0096, iter [00700, 05004], lr: 0.055915, loss: 2.2496
2022-07-11 09:54:13 - train: epoch 0096, iter [00800, 05004], lr: 0.055899, loss: 2.2603
2022-07-11 09:54:48 - train: epoch 0096, iter [00900, 05004], lr: 0.055883, loss: 2.0987
2022-07-11 09:55:22 - train: epoch 0096, iter [01000, 05004], lr: 0.055867, loss: 2.3053
2022-07-11 09:55:57 - train: epoch 0096, iter [01100, 05004], lr: 0.055851, loss: 2.4468
2022-07-11 09:56:33 - train: epoch 0096, iter [01200, 05004], lr: 0.055835, loss: 2.0931
2022-07-11 09:57:07 - train: epoch 0096, iter [01300, 05004], lr: 0.055819, loss: 2.2480
2022-07-11 09:57:42 - train: epoch 0096, iter [01400, 05004], lr: 0.055803, loss: 2.1877
2022-07-11 09:58:17 - train: epoch 0096, iter [01500, 05004], lr: 0.055787, loss: 2.0634
2022-07-11 09:58:52 - train: epoch 0096, iter [01600, 05004], lr: 0.055771, loss: 2.2384
2022-07-11 09:59:27 - train: epoch 0096, iter [01700, 05004], lr: 0.055755, loss: 2.0245
2022-07-11 10:00:03 - train: epoch 0096, iter [01800, 05004], lr: 0.055739, loss: 2.3072
2022-07-11 10:00:37 - train: epoch 0096, iter [01900, 05004], lr: 0.055723, loss: 2.2135
2022-07-11 10:01:10 - train: epoch 0096, iter [02000, 05004], lr: 0.055707, loss: 2.1606
2022-07-11 10:01:45 - train: epoch 0096, iter [02100, 05004], lr: 0.055691, loss: 2.2557
2022-07-11 10:02:20 - train: epoch 0096, iter [02200, 05004], lr: 0.055675, loss: 2.0590
2022-07-11 10:02:55 - train: epoch 0096, iter [02300, 05004], lr: 0.055659, loss: 2.1908
2022-07-11 10:03:30 - train: epoch 0096, iter [02400, 05004], lr: 0.055643, loss: 2.2793
2022-07-11 10:04:04 - train: epoch 0096, iter [02500, 05004], lr: 0.055627, loss: 2.1736
2022-07-11 10:04:38 - train: epoch 0096, iter [02600, 05004], lr: 0.055611, loss: 2.4647
2022-07-11 10:05:12 - train: epoch 0096, iter [02700, 05004], lr: 0.055595, loss: 2.6628
2022-07-11 10:05:47 - train: epoch 0096, iter [02800, 05004], lr: 0.055579, loss: 2.2789
2022-07-11 10:06:21 - train: epoch 0096, iter [02900, 05004], lr: 0.055563, loss: 2.3391
2022-07-11 10:06:57 - train: epoch 0096, iter [03000, 05004], lr: 0.055547, loss: 2.1830
2022-07-11 10:07:32 - train: epoch 0096, iter [03100, 05004], lr: 0.055531, loss: 2.1374
2022-07-11 10:08:06 - train: epoch 0096, iter [03200, 05004], lr: 0.055515, loss: 2.6376
2022-07-11 10:08:41 - train: epoch 0096, iter [03300, 05004], lr: 0.055499, loss: 2.5463
2022-07-11 10:09:16 - train: epoch 0096, iter [03400, 05004], lr: 0.055483, loss: 2.2869
2022-07-11 10:09:50 - train: epoch 0096, iter [03500, 05004], lr: 0.055467, loss: 2.0029
2022-07-11 10:10:25 - train: epoch 0096, iter [03600, 05004], lr: 0.055451, loss: 2.2340
2022-07-11 10:11:00 - train: epoch 0096, iter [03700, 05004], lr: 0.055435, loss: 2.2102
2022-07-11 10:11:35 - train: epoch 0096, iter [03800, 05004], lr: 0.055419, loss: 2.1277
2022-07-11 10:12:09 - train: epoch 0096, iter [03900, 05004], lr: 0.055403, loss: 1.9765
2022-07-11 10:12:45 - train: epoch 0096, iter [04000, 05004], lr: 0.055387, loss: 2.3103
2022-07-11 10:13:20 - train: epoch 0096, iter [04100, 05004], lr: 0.055371, loss: 2.1363
2022-07-11 10:13:55 - train: epoch 0096, iter [04200, 05004], lr: 0.055355, loss: 2.4373
2022-07-11 10:14:29 - train: epoch 0096, iter [04300, 05004], lr: 0.055339, loss: 2.0264
2022-07-11 10:15:04 - train: epoch 0096, iter [04400, 05004], lr: 0.055323, loss: 2.2329
2022-07-11 10:15:40 - train: epoch 0096, iter [04500, 05004], lr: 0.055307, loss: 2.3519
2022-07-11 10:16:14 - train: epoch 0096, iter [04600, 05004], lr: 0.055291, loss: 2.1386
2022-07-11 10:16:49 - train: epoch 0096, iter [04700, 05004], lr: 0.055275, loss: 2.1121
2022-07-11 10:17:24 - train: epoch 0096, iter [04800, 05004], lr: 0.055259, loss: 2.4349
2022-07-11 10:17:59 - train: epoch 0096, iter [04900, 05004], lr: 0.055243, loss: 2.1115
2022-07-11 10:18:33 - train: epoch 0096, iter [05000, 05004], lr: 0.055227, loss: 2.2530
2022-07-11 10:18:34 - train: epoch 096, train_loss: 2.2538
2022-07-11 10:19:50 - eval: epoch: 096, acc1: 62.004%, acc5: 84.798%, test_loss: 1.5802, per_image_load_time: 2.441ms, per_image_inference_time: 0.487ms
2022-07-11 10:19:50 - until epoch: 096, best_acc1: 63.402%
2022-07-11 10:19:50 - epoch 097 lr: 0.055226
2022-07-11 10:20:29 - train: epoch 0097, iter [00100, 05004], lr: 0.055210, loss: 2.2028
2022-07-11 10:21:04 - train: epoch 0097, iter [00200, 05004], lr: 0.055194, loss: 2.4094
2022-07-11 10:21:37 - train: epoch 0097, iter [00300, 05004], lr: 0.055178, loss: 2.0895
2022-07-11 10:22:11 - train: epoch 0097, iter [00400, 05004], lr: 0.055162, loss: 2.4796
2022-07-11 10:22:46 - train: epoch 0097, iter [00500, 05004], lr: 0.055146, loss: 2.1029
2022-07-11 10:23:21 - train: epoch 0097, iter [00600, 05004], lr: 0.055130, loss: 2.0883
2022-07-11 10:23:55 - train: epoch 0097, iter [00700, 05004], lr: 0.055114, loss: 2.2325
2022-07-11 10:24:30 - train: epoch 0097, iter [00800, 05004], lr: 0.055098, loss: 2.0937
2022-07-11 10:25:04 - train: epoch 0097, iter [00900, 05004], lr: 0.055082, loss: 2.4078
2022-07-11 10:25:39 - train: epoch 0097, iter [01000, 05004], lr: 0.055066, loss: 2.4212
2022-07-11 10:26:13 - train: epoch 0097, iter [01100, 05004], lr: 0.055050, loss: 2.1879
2022-07-11 10:26:47 - train: epoch 0097, iter [01200, 05004], lr: 0.055034, loss: 2.1586
2022-07-11 10:27:22 - train: epoch 0097, iter [01300, 05004], lr: 0.055018, loss: 1.9745
2022-07-11 10:27:57 - train: epoch 0097, iter [01400, 05004], lr: 0.055002, loss: 1.9443
2022-07-11 10:28:32 - train: epoch 0097, iter [01500, 05004], lr: 0.054986, loss: 2.3843
2022-07-11 10:29:07 - train: epoch 0097, iter [01600, 05004], lr: 0.054970, loss: 1.9103
2022-07-11 10:29:43 - train: epoch 0097, iter [01700, 05004], lr: 0.054954, loss: 2.1362
2022-07-11 10:30:17 - train: epoch 0097, iter [01800, 05004], lr: 0.054938, loss: 2.2633
2022-07-11 10:30:53 - train: epoch 0097, iter [01900, 05004], lr: 0.054922, loss: 2.3580
2022-07-11 10:31:27 - train: epoch 0097, iter [02000, 05004], lr: 0.054906, loss: 2.1664
2022-07-11 10:32:02 - train: epoch 0097, iter [02100, 05004], lr: 0.054890, loss: 2.1885
2022-07-11 10:32:38 - train: epoch 0097, iter [02200, 05004], lr: 0.054874, loss: 2.3245
2022-07-11 10:33:12 - train: epoch 0097, iter [02300, 05004], lr: 0.054858, loss: 2.2690
2022-07-11 10:33:47 - train: epoch 0097, iter [02400, 05004], lr: 0.054842, loss: 2.3060
2022-07-11 10:34:22 - train: epoch 0097, iter [02500, 05004], lr: 0.054826, loss: 2.2049
2022-07-11 10:34:56 - train: epoch 0097, iter [02600, 05004], lr: 0.054810, loss: 2.2421
2022-07-11 10:35:32 - train: epoch 0097, iter [02700, 05004], lr: 0.054794, loss: 2.0187
2022-07-11 10:36:07 - train: epoch 0097, iter [02800, 05004], lr: 0.054778, loss: 2.3123
2022-07-11 10:36:42 - train: epoch 0097, iter [02900, 05004], lr: 0.054762, loss: 2.3947
2022-07-11 10:37:17 - train: epoch 0097, iter [03000, 05004], lr: 0.054746, loss: 2.1980
2022-07-11 10:37:52 - train: epoch 0097, iter [03100, 05004], lr: 0.054730, loss: 2.1591
2022-07-11 10:38:28 - train: epoch 0097, iter [03200, 05004], lr: 0.054714, loss: 2.3419
2022-07-11 10:39:02 - train: epoch 0097, iter [03300, 05004], lr: 0.054698, loss: 2.5640
2022-07-11 10:39:37 - train: epoch 0097, iter [03400, 05004], lr: 0.054682, loss: 2.4200
2022-07-11 10:40:12 - train: epoch 0097, iter [03500, 05004], lr: 0.054666, loss: 2.1966
2022-07-11 10:40:48 - train: epoch 0097, iter [03600, 05004], lr: 0.054650, loss: 2.3013
2022-07-11 10:41:22 - train: epoch 0097, iter [03700, 05004], lr: 0.054634, loss: 1.8688
2022-07-11 10:41:58 - train: epoch 0097, iter [03800, 05004], lr: 0.054618, loss: 2.1669
2022-07-11 10:42:32 - train: epoch 0097, iter [03900, 05004], lr: 0.054602, loss: 2.3599
2022-07-11 10:43:06 - train: epoch 0097, iter [04000, 05004], lr: 0.054586, loss: 2.2312
2022-07-11 10:43:42 - train: epoch 0097, iter [04100, 05004], lr: 0.054570, loss: 2.1453
2022-07-11 10:44:16 - train: epoch 0097, iter [04200, 05004], lr: 0.054554, loss: 2.1515
2022-07-11 10:44:52 - train: epoch 0097, iter [04300, 05004], lr: 0.054538, loss: 2.4662
2022-07-11 10:45:27 - train: epoch 0097, iter [04400, 05004], lr: 0.054521, loss: 2.2059
2022-07-11 10:46:02 - train: epoch 0097, iter [04500, 05004], lr: 0.054505, loss: 2.2452
2022-07-11 10:46:37 - train: epoch 0097, iter [04600, 05004], lr: 0.054489, loss: 1.9832
2022-07-11 10:47:12 - train: epoch 0097, iter [04700, 05004], lr: 0.054473, loss: 2.1808
2022-07-11 10:47:47 - train: epoch 0097, iter [04800, 05004], lr: 0.054457, loss: 2.2140
2022-07-11 10:48:23 - train: epoch 0097, iter [04900, 05004], lr: 0.054441, loss: 2.2381
2022-07-11 10:48:55 - train: epoch 0097, iter [05000, 05004], lr: 0.054425, loss: 2.1752
2022-07-11 10:48:56 - train: epoch 097, train_loss: 2.2489
2022-07-11 10:50:11 - eval: epoch: 097, acc1: 61.212%, acc5: 84.722%, test_loss: 1.6014, per_image_load_time: 2.395ms, per_image_inference_time: 0.497ms
2022-07-11 10:50:11 - until epoch: 097, best_acc1: 63.402%
2022-07-11 10:50:11 - epoch 098 lr: 0.054424
2022-07-11 10:50:51 - train: epoch 0098, iter [00100, 05004], lr: 0.054409, loss: 2.3322
2022-07-11 10:51:26 - train: epoch 0098, iter [00200, 05004], lr: 0.054393, loss: 2.4822
2022-07-11 10:52:01 - train: epoch 0098, iter [00300, 05004], lr: 0.054377, loss: 2.1944
2022-07-11 10:52:34 - train: epoch 0098, iter [00400, 05004], lr: 0.054361, loss: 1.8209
2022-07-11 10:53:10 - train: epoch 0098, iter [00500, 05004], lr: 0.054344, loss: 2.2188
2022-07-11 10:53:45 - train: epoch 0098, iter [00600, 05004], lr: 0.054328, loss: 2.3174
2022-07-11 10:54:20 - train: epoch 0098, iter [00700, 05004], lr: 0.054312, loss: 2.2637
2022-07-11 10:54:54 - train: epoch 0098, iter [00800, 05004], lr: 0.054296, loss: 2.3415
2022-07-11 10:55:29 - train: epoch 0098, iter [00900, 05004], lr: 0.054280, loss: 2.1494
2022-07-11 10:56:04 - train: epoch 0098, iter [01000, 05004], lr: 0.054264, loss: 2.2841
2022-07-11 10:56:39 - train: epoch 0098, iter [01100, 05004], lr: 0.054248, loss: 2.2459
2022-07-11 10:57:15 - train: epoch 0098, iter [01200, 05004], lr: 0.054232, loss: 2.2878
2022-07-11 10:57:49 - train: epoch 0098, iter [01300, 05004], lr: 0.054216, loss: 1.9126
2022-07-11 10:58:24 - train: epoch 0098, iter [01400, 05004], lr: 0.054200, loss: 2.3630
2022-07-11 10:58:59 - train: epoch 0098, iter [01500, 05004], lr: 0.054184, loss: 2.3155
2022-07-11 10:59:33 - train: epoch 0098, iter [01600, 05004], lr: 0.054168, loss: 2.0971
2022-07-11 11:00:08 - train: epoch 0098, iter [01700, 05004], lr: 0.054152, loss: 2.3192
2022-07-11 11:00:43 - train: epoch 0098, iter [01800, 05004], lr: 0.054136, loss: 2.3001
2022-07-11 11:01:18 - train: epoch 0098, iter [01900, 05004], lr: 0.054120, loss: 2.3322
2022-07-11 11:01:53 - train: epoch 0098, iter [02000, 05004], lr: 0.054104, loss: 2.3679
2022-07-11 11:02:28 - train: epoch 0098, iter [02100, 05004], lr: 0.054088, loss: 2.2730
2022-07-11 11:03:03 - train: epoch 0098, iter [02200, 05004], lr: 0.054072, loss: 2.1364
2022-07-11 11:03:38 - train: epoch 0098, iter [02300, 05004], lr: 0.054056, loss: 2.1878
2022-07-11 11:04:13 - train: epoch 0098, iter [02400, 05004], lr: 0.054040, loss: 2.2732
2022-07-11 11:04:48 - train: epoch 0098, iter [02500, 05004], lr: 0.054024, loss: 2.1420
2022-07-11 11:05:23 - train: epoch 0098, iter [02600, 05004], lr: 0.054008, loss: 2.0950
2022-07-11 11:05:59 - train: epoch 0098, iter [02700, 05004], lr: 0.053992, loss: 2.3973
2022-07-11 11:06:33 - train: epoch 0098, iter [02800, 05004], lr: 0.053976, loss: 2.3640
2022-07-11 11:07:07 - train: epoch 0098, iter [02900, 05004], lr: 0.053959, loss: 2.1333
2022-07-11 11:07:43 - train: epoch 0098, iter [03000, 05004], lr: 0.053943, loss: 2.1926
2022-07-11 11:08:18 - train: epoch 0098, iter [03100, 05004], lr: 0.053927, loss: 2.1870
2022-07-11 11:08:52 - train: epoch 0098, iter [03200, 05004], lr: 0.053911, loss: 2.1215
2022-07-11 11:09:27 - train: epoch 0098, iter [03300, 05004], lr: 0.053895, loss: 2.2074
2022-07-11 11:10:02 - train: epoch 0098, iter [03400, 05004], lr: 0.053879, loss: 2.4895
2022-07-11 11:10:38 - train: epoch 0098, iter [03500, 05004], lr: 0.053863, loss: 2.1353
2022-07-11 11:11:13 - train: epoch 0098, iter [03600, 05004], lr: 0.053847, loss: 2.3179
2022-07-11 11:11:48 - train: epoch 0098, iter [03700, 05004], lr: 0.053831, loss: 2.4294
2022-07-11 11:12:22 - train: epoch 0098, iter [03800, 05004], lr: 0.053815, loss: 1.9806
2022-07-11 11:12:57 - train: epoch 0098, iter [03900, 05004], lr: 0.053799, loss: 2.2380
2022-07-11 11:13:33 - train: epoch 0098, iter [04000, 05004], lr: 0.053783, loss: 2.2127
2022-07-11 11:14:06 - train: epoch 0098, iter [04100, 05004], lr: 0.053767, loss: 2.2989
2022-07-11 11:14:42 - train: epoch 0098, iter [04200, 05004], lr: 0.053751, loss: 2.0761
2022-07-11 11:15:17 - train: epoch 0098, iter [04300, 05004], lr: 0.053735, loss: 2.0501
2022-07-11 11:15:51 - train: epoch 0098, iter [04400, 05004], lr: 0.053719, loss: 2.3350
2022-07-11 11:16:26 - train: epoch 0098, iter [04500, 05004], lr: 0.053703, loss: 2.3404
2022-07-11 11:17:02 - train: epoch 0098, iter [04600, 05004], lr: 0.053687, loss: 2.2029
2022-07-11 11:17:36 - train: epoch 0098, iter [04700, 05004], lr: 0.053671, loss: 2.1196
2022-07-11 11:18:12 - train: epoch 0098, iter [04800, 05004], lr: 0.053654, loss: 1.8349
2022-07-11 11:18:47 - train: epoch 0098, iter [04900, 05004], lr: 0.053638, loss: 2.0913
2022-07-11 11:19:20 - train: epoch 0098, iter [05000, 05004], lr: 0.053622, loss: 2.1502
2022-07-11 11:19:21 - train: epoch 098, train_loss: 2.2386
2022-07-11 11:20:36 - eval: epoch: 098, acc1: 61.248%, acc5: 84.328%, test_loss: 1.6200, per_image_load_time: 1.438ms, per_image_inference_time: 0.491ms
2022-07-11 11:20:37 - until epoch: 098, best_acc1: 63.402%
2022-07-11 11:20:37 - epoch 099 lr: 0.053622
2022-07-11 11:21:16 - train: epoch 0099, iter [00100, 05004], lr: 0.053606, loss: 2.0458
2022-07-11 11:21:51 - train: epoch 0099, iter [00200, 05004], lr: 0.053590, loss: 2.0546
2022-07-11 11:22:25 - train: epoch 0099, iter [00300, 05004], lr: 0.053574, loss: 2.2546
2022-07-11 11:22:59 - train: epoch 0099, iter [00400, 05004], lr: 0.053558, loss: 2.0091
2022-07-11 11:23:34 - train: epoch 0099, iter [00500, 05004], lr: 0.053541, loss: 2.2746
2022-07-11 11:24:08 - train: epoch 0099, iter [00600, 05004], lr: 0.053525, loss: 2.3481
2022-07-11 11:24:42 - train: epoch 0099, iter [00700, 05004], lr: 0.053509, loss: 2.3635
2022-07-11 11:25:17 - train: epoch 0099, iter [00800, 05004], lr: 0.053493, loss: 2.2378
2022-07-11 11:25:50 - train: epoch 0099, iter [00900, 05004], lr: 0.053477, loss: 2.3517
2022-07-11 11:26:25 - train: epoch 0099, iter [01000, 05004], lr: 0.053461, loss: 2.2465
2022-07-11 11:27:00 - train: epoch 0099, iter [01100, 05004], lr: 0.053445, loss: 2.4700
2022-07-11 11:27:34 - train: epoch 0099, iter [01200, 05004], lr: 0.053429, loss: 2.1193
2022-07-11 11:28:09 - train: epoch 0099, iter [01300, 05004], lr: 0.053413, loss: 2.2278
2022-07-11 11:28:43 - train: epoch 0099, iter [01400, 05004], lr: 0.053397, loss: 2.2771
2022-07-11 11:29:18 - train: epoch 0099, iter [01500, 05004], lr: 0.053381, loss: 1.9839
2022-07-11 11:29:53 - train: epoch 0099, iter [01600, 05004], lr: 0.053365, loss: 2.2894
2022-07-11 11:30:27 - train: epoch 0099, iter [01700, 05004], lr: 0.053349, loss: 2.3092
2022-07-11 11:31:03 - train: epoch 0099, iter [01800, 05004], lr: 0.053333, loss: 2.3704
2022-07-11 11:31:37 - train: epoch 0099, iter [01900, 05004], lr: 0.053317, loss: 2.3554
2022-07-11 11:32:12 - train: epoch 0099, iter [02000, 05004], lr: 0.053301, loss: 2.3040
2022-07-11 11:32:46 - train: epoch 0099, iter [02100, 05004], lr: 0.053284, loss: 2.3232
2022-07-11 11:33:20 - train: epoch 0099, iter [02200, 05004], lr: 0.053268, loss: 2.2051
2022-07-11 11:33:54 - train: epoch 0099, iter [02300, 05004], lr: 0.053252, loss: 2.2425
2022-07-11 11:34:30 - train: epoch 0099, iter [02400, 05004], lr: 0.053236, loss: 2.7304
2022-07-11 11:35:04 - train: epoch 0099, iter [02500, 05004], lr: 0.053220, loss: 2.1575
2022-07-11 11:35:39 - train: epoch 0099, iter [02600, 05004], lr: 0.053204, loss: 2.0630
2022-07-11 11:36:13 - train: epoch 0099, iter [02700, 05004], lr: 0.053188, loss: 2.2918
2022-07-11 11:36:48 - train: epoch 0099, iter [02800, 05004], lr: 0.053172, loss: 2.4343
2022-07-11 11:37:23 - train: epoch 0099, iter [02900, 05004], lr: 0.053156, loss: 2.2300
2022-07-11 11:37:58 - train: epoch 0099, iter [03000, 05004], lr: 0.053140, loss: 2.6245
2022-07-11 11:38:33 - train: epoch 0099, iter [03100, 05004], lr: 0.053124, loss: 2.2983
2022-07-11 11:39:07 - train: epoch 0099, iter [03200, 05004], lr: 0.053108, loss: 2.3476
2022-07-11 11:39:42 - train: epoch 0099, iter [03300, 05004], lr: 0.053092, loss: 2.4042
2022-07-11 11:40:16 - train: epoch 0099, iter [03400, 05004], lr: 0.053076, loss: 2.1171
2022-07-11 11:40:51 - train: epoch 0099, iter [03500, 05004], lr: 0.053060, loss: 2.1790
2022-07-11 11:41:25 - train: epoch 0099, iter [03600, 05004], lr: 0.053044, loss: 2.1861
2022-07-11 11:42:01 - train: epoch 0099, iter [03700, 05004], lr: 0.053027, loss: 1.9694
2022-07-11 11:42:35 - train: epoch 0099, iter [03800, 05004], lr: 0.053011, loss: 2.0860
2022-07-11 11:43:10 - train: epoch 0099, iter [03900, 05004], lr: 0.052995, loss: 2.0728
2022-07-11 11:43:44 - train: epoch 0099, iter [04000, 05004], lr: 0.052979, loss: 2.3223
2022-07-11 11:44:19 - train: epoch 0099, iter [04100, 05004], lr: 0.052963, loss: 1.9730
2022-07-11 11:44:53 - train: epoch 0099, iter [04200, 05004], lr: 0.052947, loss: 2.3293
2022-07-11 11:45:29 - train: epoch 0099, iter [04300, 05004], lr: 0.052931, loss: 2.2252
2022-07-11 11:46:03 - train: epoch 0099, iter [04400, 05004], lr: 0.052915, loss: 2.1889
2022-07-11 11:46:38 - train: epoch 0099, iter [04500, 05004], lr: 0.052899, loss: 2.3425
2022-07-11 11:47:13 - train: epoch 0099, iter [04600, 05004], lr: 0.052883, loss: 2.4097
2022-07-11 11:47:48 - train: epoch 0099, iter [04700, 05004], lr: 0.052867, loss: 2.1426
2022-07-11 11:48:23 - train: epoch 0099, iter [04800, 05004], lr: 0.052851, loss: 2.4551
2022-07-11 11:48:57 - train: epoch 0099, iter [04900, 05004], lr: 0.052835, loss: 2.3075
2022-07-11 11:49:31 - train: epoch 0099, iter [05000, 05004], lr: 0.052819, loss: 2.2403
2022-07-11 11:49:32 - train: epoch 099, train_loss: 2.2328
2022-07-11 11:50:47 - eval: epoch: 099, acc1: 64.104%, acc5: 86.314%, test_loss: 1.4803, per_image_load_time: 1.269ms, per_image_inference_time: 0.465ms
2022-07-11 11:50:48 - until epoch: 099, best_acc1: 64.104%
2022-07-11 11:50:48 - epoch 100 lr: 0.052818
2022-07-11 11:51:27 - train: epoch 0100, iter [00100, 05004], lr: 0.052802, loss: 2.0200
2022-07-11 11:52:02 - train: epoch 0100, iter [00200, 05004], lr: 0.052786, loss: 2.0363
2022-07-11 11:52:37 - train: epoch 0100, iter [00300, 05004], lr: 0.052770, loss: 2.1054
2022-07-11 11:53:12 - train: epoch 0100, iter [00400, 05004], lr: 0.052754, loss: 2.0573
2022-07-11 11:53:47 - train: epoch 0100, iter [00500, 05004], lr: 0.052738, loss: 2.2695
2022-07-11 11:54:21 - train: epoch 0100, iter [00600, 05004], lr: 0.052721, loss: 2.3044
2022-07-11 11:54:56 - train: epoch 0100, iter [00700, 05004], lr: 0.052705, loss: 2.1816
2022-07-11 11:55:32 - train: epoch 0100, iter [00800, 05004], lr: 0.052689, loss: 2.2300
2022-07-11 11:56:06 - train: epoch 0100, iter [00900, 05004], lr: 0.052673, loss: 2.1558
2022-07-11 11:56:41 - train: epoch 0100, iter [01000, 05004], lr: 0.052657, loss: 2.3058
2022-07-11 11:57:15 - train: epoch 0100, iter [01100, 05004], lr: 0.052641, loss: 2.0954
2022-07-11 11:57:50 - train: epoch 0100, iter [01200, 05004], lr: 0.052625, loss: 2.1881
2022-07-11 11:58:24 - train: epoch 0100, iter [01300, 05004], lr: 0.052609, loss: 2.3347
2022-07-11 11:58:59 - train: epoch 0100, iter [01400, 05004], lr: 0.052593, loss: 2.1260
2022-07-11 11:59:34 - train: epoch 0100, iter [01500, 05004], lr: 0.052577, loss: 2.3473
2022-07-11 12:00:09 - train: epoch 0100, iter [01600, 05004], lr: 0.052561, loss: 2.2403
2022-07-11 12:00:43 - train: epoch 0100, iter [01700, 05004], lr: 0.052545, loss: 2.0855
2022-07-11 12:01:18 - train: epoch 0100, iter [01800, 05004], lr: 0.052529, loss: 2.1877
2022-07-11 12:01:52 - train: epoch 0100, iter [01900, 05004], lr: 0.052512, loss: 2.5399
2022-07-11 12:02:26 - train: epoch 0100, iter [02000, 05004], lr: 0.052496, loss: 2.4734
2022-07-11 12:03:02 - train: epoch 0100, iter [02100, 05004], lr: 0.052480, loss: 2.1007
2022-07-11 12:03:35 - train: epoch 0100, iter [02200, 05004], lr: 0.052464, loss: 2.3177
2022-07-11 12:04:10 - train: epoch 0100, iter [02300, 05004], lr: 0.052448, loss: 2.3030
2022-07-11 12:04:45 - train: epoch 0100, iter [02400, 05004], lr: 0.052432, loss: 2.3621
2022-07-11 12:05:20 - train: epoch 0100, iter [02500, 05004], lr: 0.052416, loss: 2.2835
2022-07-11 12:05:54 - train: epoch 0100, iter [02600, 05004], lr: 0.052400, loss: 2.1249
2022-07-11 12:06:29 - train: epoch 0100, iter [02700, 05004], lr: 0.052384, loss: 2.1786
2022-07-11 12:07:04 - train: epoch 0100, iter [02800, 05004], lr: 0.052368, loss: 2.1227
2022-07-11 12:07:39 - train: epoch 0100, iter [02900, 05004], lr: 0.052352, loss: 2.1498
2022-07-11 12:08:13 - train: epoch 0100, iter [03000, 05004], lr: 0.052336, loss: 2.0500
2022-07-11 12:08:48 - train: epoch 0100, iter [03100, 05004], lr: 0.052320, loss: 1.9554
2022-07-11 12:09:22 - train: epoch 0100, iter [03200, 05004], lr: 0.052303, loss: 2.4656
2022-07-11 12:09:57 - train: epoch 0100, iter [03300, 05004], lr: 0.052287, loss: 2.3080
2022-07-11 12:10:32 - train: epoch 0100, iter [03400, 05004], lr: 0.052271, loss: 2.1329
2022-07-11 12:11:08 - train: epoch 0100, iter [03500, 05004], lr: 0.052255, loss: 1.8558
2022-07-11 12:11:41 - train: epoch 0100, iter [03600, 05004], lr: 0.052239, loss: 1.8469
2022-07-11 12:12:16 - train: epoch 0100, iter [03700, 05004], lr: 0.052223, loss: 2.2596
2022-07-11 12:12:51 - train: epoch 0100, iter [03800, 05004], lr: 0.052207, loss: 2.0048
2022-07-11 12:13:26 - train: epoch 0100, iter [03900, 05004], lr: 0.052191, loss: 2.3235
2022-07-11 12:14:00 - train: epoch 0100, iter [04000, 05004], lr: 0.052175, loss: 2.2023
2022-07-11 12:14:35 - train: epoch 0100, iter [04100, 05004], lr: 0.052159, loss: 2.1779
2022-07-11 12:15:09 - train: epoch 0100, iter [04200, 05004], lr: 0.052143, loss: 2.4732
2022-07-11 12:15:45 - train: epoch 0100, iter [04300, 05004], lr: 0.052127, loss: 2.2147
2022-07-11 12:16:19 - train: epoch 0100, iter [04400, 05004], lr: 0.052110, loss: 2.4141
2022-07-11 12:16:53 - train: epoch 0100, iter [04500, 05004], lr: 0.052094, loss: 1.8622
2022-07-11 12:17:28 - train: epoch 0100, iter [04600, 05004], lr: 0.052078, loss: 2.1517
2022-07-11 12:18:03 - train: epoch 0100, iter [04700, 05004], lr: 0.052062, loss: 2.5326
2022-07-11 12:18:37 - train: epoch 0100, iter [04800, 05004], lr: 0.052046, loss: 2.2429
2022-07-11 12:19:12 - train: epoch 0100, iter [04900, 05004], lr: 0.052030, loss: 2.1688
2022-07-11 12:19:45 - train: epoch 0100, iter [05000, 05004], lr: 0.052014, loss: 2.4823
2022-07-11 12:19:46 - train: epoch 100, train_loss: 2.2264
2022-07-11 12:21:01 - eval: epoch: 100, acc1: 62.934%, acc5: 85.644%, test_loss: 1.5283, per_image_load_time: 1.577ms, per_image_inference_time: 0.473ms
2022-07-11 12:21:01 - until epoch: 100, best_acc1: 64.104%
2022-07-11 12:21:01 - epoch 101 lr: 0.052013
2022-07-11 12:21:40 - train: epoch 0101, iter [00100, 05004], lr: 0.051997, loss: 2.0191
2022-07-11 12:22:15 - train: epoch 0101, iter [00200, 05004], lr: 0.051981, loss: 2.3701
2022-07-11 12:22:49 - train: epoch 0101, iter [00300, 05004], lr: 0.051965, loss: 2.1102
2022-07-11 12:23:24 - train: epoch 0101, iter [00400, 05004], lr: 0.051949, loss: 1.9574
2022-07-11 12:23:58 - train: epoch 0101, iter [00500, 05004], lr: 0.051933, loss: 2.2937
2022-07-11 12:24:32 - train: epoch 0101, iter [00600, 05004], lr: 0.051917, loss: 2.3466
2022-07-11 12:25:07 - train: epoch 0101, iter [00700, 05004], lr: 0.051901, loss: 2.4634
2022-07-11 12:25:42 - train: epoch 0101, iter [00800, 05004], lr: 0.051885, loss: 2.0877
2022-07-11 12:26:16 - train: epoch 0101, iter [00900, 05004], lr: 0.051869, loss: 2.0605
2022-07-11 12:26:51 - train: epoch 0101, iter [01000, 05004], lr: 0.051852, loss: 2.2450
2022-07-11 12:27:25 - train: epoch 0101, iter [01100, 05004], lr: 0.051836, loss: 2.2444
2022-07-11 12:27:59 - train: epoch 0101, iter [01200, 05004], lr: 0.051820, loss: 2.2480
2022-07-11 12:28:33 - train: epoch 0101, iter [01300, 05004], lr: 0.051804, loss: 2.4001
2022-07-11 12:29:08 - train: epoch 0101, iter [01400, 05004], lr: 0.051788, loss: 2.0943
2022-07-11 12:29:42 - train: epoch 0101, iter [01500, 05004], lr: 0.051772, loss: 2.1543
2022-07-11 12:30:17 - train: epoch 0101, iter [01600, 05004], lr: 0.051756, loss: 1.9948
2022-07-11 12:30:52 - train: epoch 0101, iter [01700, 05004], lr: 0.051740, loss: 2.0540
2022-07-11 12:31:26 - train: epoch 0101, iter [01800, 05004], lr: 0.051724, loss: 2.2874
2022-07-11 12:32:01 - train: epoch 0101, iter [01900, 05004], lr: 0.051708, loss: 2.2228
2022-07-11 12:32:36 - train: epoch 0101, iter [02000, 05004], lr: 0.051692, loss: 2.2199
2022-07-11 12:33:10 - train: epoch 0101, iter [02100, 05004], lr: 0.051675, loss: 2.0075
2022-07-11 12:33:45 - train: epoch 0101, iter [02200, 05004], lr: 0.051659, loss: 2.0481
2022-07-11 12:34:19 - train: epoch 0101, iter [02300, 05004], lr: 0.051643, loss: 2.4123
2022-07-11 12:34:54 - train: epoch 0101, iter [02400, 05004], lr: 0.051627, loss: 2.0530
2022-07-11 12:35:29 - train: epoch 0101, iter [02500, 05004], lr: 0.051611, loss: 2.2357
2022-07-11 12:36:04 - train: epoch 0101, iter [02600, 05004], lr: 0.051595, loss: 2.4097
2022-07-11 12:36:38 - train: epoch 0101, iter [02700, 05004], lr: 0.051579, loss: 2.4186
2022-07-11 12:37:12 - train: epoch 0101, iter [02800, 05004], lr: 0.051563, loss: 2.2823
2022-07-11 12:37:47 - train: epoch 0101, iter [02900, 05004], lr: 0.051547, loss: 2.1825
2022-07-11 12:38:22 - train: epoch 0101, iter [03000, 05004], lr: 0.051531, loss: 2.0125
2022-07-11 12:38:57 - train: epoch 0101, iter [03100, 05004], lr: 0.051515, loss: 2.1949
2022-07-11 12:39:31 - train: epoch 0101, iter [03200, 05004], lr: 0.051498, loss: 2.1490
2022-07-11 12:40:05 - train: epoch 0101, iter [03300, 05004], lr: 0.051482, loss: 2.5856
2022-07-11 12:40:40 - train: epoch 0101, iter [03400, 05004], lr: 0.051466, loss: 2.3163
2022-07-11 12:41:15 - train: epoch 0101, iter [03500, 05004], lr: 0.051450, loss: 2.1845
2022-07-11 12:41:49 - train: epoch 0101, iter [03600, 05004], lr: 0.051434, loss: 2.1342
2022-07-11 12:42:24 - train: epoch 0101, iter [03700, 05004], lr: 0.051418, loss: 2.2137
2022-07-11 12:42:59 - train: epoch 0101, iter [03800, 05004], lr: 0.051402, loss: 2.3578
2022-07-11 12:43:35 - train: epoch 0101, iter [03900, 05004], lr: 0.051386, loss: 2.3331
2022-07-11 12:44:10 - train: epoch 0101, iter [04000, 05004], lr: 0.051370, loss: 2.1996
2022-07-11 12:44:45 - train: epoch 0101, iter [04100, 05004], lr: 0.051354, loss: 2.1057
2022-07-11 12:45:19 - train: epoch 0101, iter [04200, 05004], lr: 0.051338, loss: 2.2045
2022-07-11 12:45:55 - train: epoch 0101, iter [04300, 05004], lr: 0.051321, loss: 2.2393
2022-07-11 12:46:29 - train: epoch 0101, iter [04400, 05004], lr: 0.051305, loss: 2.1163
2022-07-11 12:47:04 - train: epoch 0101, iter [04500, 05004], lr: 0.051289, loss: 2.2315
2022-07-11 12:47:39 - train: epoch 0101, iter [04600, 05004], lr: 0.051273, loss: 2.3781
2022-07-11 12:48:14 - train: epoch 0101, iter [04700, 05004], lr: 0.051257, loss: 2.1425
2022-07-11 12:48:48 - train: epoch 0101, iter [04800, 05004], lr: 0.051241, loss: 2.1356
2022-07-11 12:49:24 - train: epoch 0101, iter [04900, 05004], lr: 0.051225, loss: 2.3566
2022-07-11 12:49:57 - train: epoch 0101, iter [05000, 05004], lr: 0.051209, loss: 2.4381
2022-07-11 12:49:58 - train: epoch 101, train_loss: 2.2197
2022-07-11 12:51:14 - eval: epoch: 101, acc1: 63.862%, acc5: 86.236%, test_loss: 1.4921, per_image_load_time: 2.485ms, per_image_inference_time: 0.465ms
2022-07-11 12:51:14 - until epoch: 101, best_acc1: 64.104%
2022-07-11 12:51:14 - epoch 102 lr: 0.051208
2022-07-11 12:51:54 - train: epoch 0102, iter [00100, 05004], lr: 0.051192, loss: 2.2609
2022-07-11 12:52:29 - train: epoch 0102, iter [00200, 05004], lr: 0.051176, loss: 2.3426
2022-07-11 12:53:04 - train: epoch 0102, iter [00300, 05004], lr: 0.051160, loss: 2.3412
2022-07-11 12:53:38 - train: epoch 0102, iter [00400, 05004], lr: 0.051144, loss: 2.1122
2022-07-11 12:54:13 - train: epoch 0102, iter [00500, 05004], lr: 0.051128, loss: 2.0318
2022-07-11 12:54:47 - train: epoch 0102, iter [00600, 05004], lr: 0.051112, loss: 2.1251
2022-07-11 12:55:22 - train: epoch 0102, iter [00700, 05004], lr: 0.051096, loss: 2.2666
2022-07-11 12:55:57 - train: epoch 0102, iter [00800, 05004], lr: 0.051079, loss: 2.2592
2022-07-11 12:56:31 - train: epoch 0102, iter [00900, 05004], lr: 0.051063, loss: 2.0062
2022-07-11 12:57:06 - train: epoch 0102, iter [01000, 05004], lr: 0.051047, loss: 2.3719
2022-07-11 12:57:40 - train: epoch 0102, iter [01100, 05004], lr: 0.051031, loss: 2.0245
2022-07-11 12:58:15 - train: epoch 0102, iter [01200, 05004], lr: 0.051015, loss: 2.2938
2022-07-11 12:58:48 - train: epoch 0102, iter [01300, 05004], lr: 0.050999, loss: 2.1540
2022-07-11 12:59:23 - train: epoch 0102, iter [01400, 05004], lr: 0.050983, loss: 2.3079
2022-07-11 12:59:59 - train: epoch 0102, iter [01500, 05004], lr: 0.050967, loss: 2.0588
2022-07-11 13:00:32 - train: epoch 0102, iter [01600, 05004], lr: 0.050951, loss: 2.2297
2022-07-11 13:01:07 - train: epoch 0102, iter [01700, 05004], lr: 0.050935, loss: 2.0043
2022-07-11 13:01:41 - train: epoch 0102, iter [01800, 05004], lr: 0.050918, loss: 2.2180
2022-07-11 13:02:16 - train: epoch 0102, iter [01900, 05004], lr: 0.050902, loss: 1.9393
2022-07-11 13:02:51 - train: epoch 0102, iter [02000, 05004], lr: 0.050886, loss: 2.0942
2022-07-11 13:03:26 - train: epoch 0102, iter [02100, 05004], lr: 0.050870, loss: 2.1607
2022-07-11 13:04:00 - train: epoch 0102, iter [02200, 05004], lr: 0.050854, loss: 2.1565
2022-07-11 13:04:35 - train: epoch 0102, iter [02300, 05004], lr: 0.050838, loss: 2.3432
2022-07-11 13:05:09 - train: epoch 0102, iter [02400, 05004], lr: 0.050822, loss: 2.2657
2022-07-11 13:05:45 - train: epoch 0102, iter [02500, 05004], lr: 0.050806, loss: 2.2593
2022-07-11 13:06:19 - train: epoch 0102, iter [02600, 05004], lr: 0.050790, loss: 2.1764
2022-07-11 13:06:54 - train: epoch 0102, iter [02700, 05004], lr: 0.050774, loss: 2.4903
2022-07-11 13:07:29 - train: epoch 0102, iter [02800, 05004], lr: 0.050758, loss: 2.1782
2022-07-11 13:08:03 - train: epoch 0102, iter [02900, 05004], lr: 0.050741, loss: 2.2714
2022-07-11 13:08:38 - train: epoch 0102, iter [03000, 05004], lr: 0.050725, loss: 2.2463
2022-07-11 13:09:12 - train: epoch 0102, iter [03100, 05004], lr: 0.050709, loss: 2.2587
2022-07-11 13:09:47 - train: epoch 0102, iter [03200, 05004], lr: 0.050693, loss: 2.2271
2022-07-11 13:10:22 - train: epoch 0102, iter [03300, 05004], lr: 0.050677, loss: 2.2362
2022-07-11 13:10:57 - train: epoch 0102, iter [03400, 05004], lr: 0.050661, loss: 2.2337
2022-07-11 13:11:31 - train: epoch 0102, iter [03500, 05004], lr: 0.050645, loss: 2.3746
2022-07-11 13:12:06 - train: epoch 0102, iter [03600, 05004], lr: 0.050629, loss: 2.1224
2022-07-11 13:12:41 - train: epoch 0102, iter [03700, 05004], lr: 0.050613, loss: 2.2643
2022-07-11 13:13:15 - train: epoch 0102, iter [03800, 05004], lr: 0.050597, loss: 2.2933
2022-07-11 13:13:51 - train: epoch 0102, iter [03900, 05004], lr: 0.050580, loss: 1.9987
2022-07-11 13:14:26 - train: epoch 0102, iter [04000, 05004], lr: 0.050564, loss: 2.0333
2022-07-11 13:15:00 - train: epoch 0102, iter [04100, 05004], lr: 0.050548, loss: 2.0551
2022-07-11 13:15:35 - train: epoch 0102, iter [04200, 05004], lr: 0.050532, loss: 2.5878
2022-07-11 13:16:10 - train: epoch 0102, iter [04300, 05004], lr: 0.050516, loss: 2.1273
2022-07-11 13:16:45 - train: epoch 0102, iter [04400, 05004], lr: 0.050500, loss: 2.3501
2022-07-11 13:17:19 - train: epoch 0102, iter [04500, 05004], lr: 0.050484, loss: 2.2997
2022-07-11 13:17:54 - train: epoch 0102, iter [04600, 05004], lr: 0.050468, loss: 2.2960
2022-07-11 13:18:29 - train: epoch 0102, iter [04700, 05004], lr: 0.050452, loss: 2.2791
2022-07-11 13:19:04 - train: epoch 0102, iter [04800, 05004], lr: 0.050436, loss: 2.1156
2022-07-11 13:19:39 - train: epoch 0102, iter [04900, 05004], lr: 0.050420, loss: 2.1640
2022-07-11 13:20:12 - train: epoch 0102, iter [05000, 05004], lr: 0.050403, loss: 2.1358
2022-07-11 13:20:14 - train: epoch 102, train_loss: 2.2138
2022-07-11 13:21:28 - eval: epoch: 102, acc1: 61.606%, acc5: 84.224%, test_loss: 1.6159, per_image_load_time: 1.789ms, per_image_inference_time: 0.472ms
2022-07-11 13:21:29 - until epoch: 102, best_acc1: 64.104%
2022-07-11 13:21:29 - epoch 103 lr: 0.050403
2022-07-11 13:22:08 - train: epoch 0103, iter [00100, 05004], lr: 0.050387, loss: 2.1476
2022-07-11 13:22:43 - train: epoch 0103, iter [00200, 05004], lr: 0.050371, loss: 2.1304
2022-07-11 13:23:17 - train: epoch 0103, iter [00300, 05004], lr: 0.050354, loss: 1.9773
2022-07-11 13:23:52 - train: epoch 0103, iter [00400, 05004], lr: 0.050338, loss: 2.1026
2022-07-11 13:24:26 - train: epoch 0103, iter [00500, 05004], lr: 0.050322, loss: 2.4250
2022-07-11 13:25:01 - train: epoch 0103, iter [00600, 05004], lr: 0.050306, loss: 2.1932
2022-07-11 13:25:35 - train: epoch 0103, iter [00700, 05004], lr: 0.050290, loss: 2.0953
2022-07-11 13:26:10 - train: epoch 0103, iter [00800, 05004], lr: 0.050274, loss: 2.1476
2022-07-11 13:26:46 - train: epoch 0103, iter [00900, 05004], lr: 0.050258, loss: 1.9371
2022-07-11 13:27:20 - train: epoch 0103, iter [01000, 05004], lr: 0.050242, loss: 2.5093
2022-07-11 13:27:56 - train: epoch 0103, iter [01100, 05004], lr: 0.050226, loss: 2.0967
2022-07-11 13:28:29 - train: epoch 0103, iter [01200, 05004], lr: 0.050210, loss: 2.1758
2022-07-11 13:29:04 - train: epoch 0103, iter [01300, 05004], lr: 0.050193, loss: 2.2816
2022-07-11 13:29:40 - train: epoch 0103, iter [01400, 05004], lr: 0.050177, loss: 2.1904
2022-07-11 13:30:14 - train: epoch 0103, iter [01500, 05004], lr: 0.050161, loss: 2.0741
2022-07-11 13:30:49 - train: epoch 0103, iter [01600, 05004], lr: 0.050145, loss: 1.9708
2022-07-11 13:31:24 - train: epoch 0103, iter [01700, 05004], lr: 0.050129, loss: 2.0752
2022-07-11 13:31:59 - train: epoch 0103, iter [01800, 05004], lr: 0.050113, loss: 1.9692
2022-07-11 13:32:33 - train: epoch 0103, iter [01900, 05004], lr: 0.050097, loss: 2.1317
2022-07-11 13:33:09 - train: epoch 0103, iter [02000, 05004], lr: 0.050081, loss: 2.1010
2022-07-11 13:33:43 - train: epoch 0103, iter [02100, 05004], lr: 0.050065, loss: 2.3980
2022-07-11 13:34:18 - train: epoch 0103, iter [02200, 05004], lr: 0.050049, loss: 2.1874
2022-07-11 13:34:53 - train: epoch 0103, iter [02300, 05004], lr: 0.050033, loss: 2.2563
2022-07-11 13:35:27 - train: epoch 0103, iter [02400, 05004], lr: 0.050016, loss: 2.2681
2022-07-11 13:36:03 - train: epoch 0103, iter [02500, 05004], lr: 0.050000, loss: 2.1330
2022-07-11 13:36:36 - train: epoch 0103, iter [02600, 05004], lr: 0.049984, loss: 1.9509
2022-07-11 13:37:11 - train: epoch 0103, iter [02700, 05004], lr: 0.049968, loss: 2.4181
2022-07-11 13:37:45 - train: epoch 0103, iter [02800, 05004], lr: 0.049952, loss: 1.9101
2022-07-11 13:38:21 - train: epoch 0103, iter [02900, 05004], lr: 0.049936, loss: 2.0889
2022-07-11 13:38:55 - train: epoch 0103, iter [03000, 05004], lr: 0.049920, loss: 2.3381
2022-07-11 13:39:30 - train: epoch 0103, iter [03100, 05004], lr: 0.049904, loss: 2.2775
2022-07-11 13:40:04 - train: epoch 0103, iter [03200, 05004], lr: 0.049888, loss: 2.1630
2022-07-11 13:40:39 - train: epoch 0103, iter [03300, 05004], lr: 0.049872, loss: 2.3970
2022-07-11 13:41:14 - train: epoch 0103, iter [03400, 05004], lr: 0.049855, loss: 2.0130
2022-07-11 13:41:49 - train: epoch 0103, iter [03500, 05004], lr: 0.049839, loss: 1.9623
2022-07-11 13:42:23 - train: epoch 0103, iter [03600, 05004], lr: 0.049823, loss: 2.1219
2022-07-11 13:42:59 - train: epoch 0103, iter [03700, 05004], lr: 0.049807, loss: 2.1176
2022-07-11 13:43:33 - train: epoch 0103, iter [03800, 05004], lr: 0.049791, loss: 1.9933
2022-07-11 13:44:08 - train: epoch 0103, iter [03900, 05004], lr: 0.049775, loss: 2.1523
2022-07-11 13:44:42 - train: epoch 0103, iter [04000, 05004], lr: 0.049759, loss: 2.0659
2022-07-11 13:45:18 - train: epoch 0103, iter [04100, 05004], lr: 0.049743, loss: 2.0113
2022-07-11 13:45:52 - train: epoch 0103, iter [04200, 05004], lr: 0.049727, loss: 2.1699
2022-07-11 13:46:27 - train: epoch 0103, iter [04300, 05004], lr: 0.049711, loss: 2.1903
2022-07-11 13:47:01 - train: epoch 0103, iter [04400, 05004], lr: 0.049694, loss: 2.2831
2022-07-11 13:47:36 - train: epoch 0103, iter [04500, 05004], lr: 0.049678, loss: 2.2717
2022-07-11 13:48:11 - train: epoch 0103, iter [04600, 05004], lr: 0.049662, loss: 2.3601
2022-07-11 13:48:46 - train: epoch 0103, iter [04700, 05004], lr: 0.049646, loss: 2.0472
2022-07-11 13:49:20 - train: epoch 0103, iter [04800, 05004], lr: 0.049630, loss: 2.4823
2022-07-11 13:49:55 - train: epoch 0103, iter [04900, 05004], lr: 0.049614, loss: 2.0395
2022-07-11 13:50:29 - train: epoch 0103, iter [05000, 05004], lr: 0.049598, loss: 2.3626
2022-07-11 13:50:30 - train: epoch 103, train_loss: 2.2054
2022-07-11 13:51:45 - eval: epoch: 103, acc1: 64.222%, acc5: 86.434%, test_loss: 1.4654, per_image_load_time: 2.488ms, per_image_inference_time: 0.463ms
2022-07-11 13:51:46 - until epoch: 103, best_acc1: 64.222%
2022-07-11 13:51:46 - epoch 104 lr: 0.049597
2022-07-11 13:52:26 - train: epoch 0104, iter [00100, 05004], lr: 0.049581, loss: 2.2651
2022-07-11 13:53:00 - train: epoch 0104, iter [00200, 05004], lr: 0.049565, loss: 2.2903
2022-07-11 13:53:35 - train: epoch 0104, iter [00300, 05004], lr: 0.049549, loss: 2.0378
2022-07-11 13:54:10 - train: epoch 0104, iter [00400, 05004], lr: 0.049533, loss: 2.2035
2022-07-11 13:54:44 - train: epoch 0104, iter [00500, 05004], lr: 0.049517, loss: 2.2213
2022-07-11 13:55:19 - train: epoch 0104, iter [00600, 05004], lr: 0.049501, loss: 2.2025
2022-07-11 13:55:54 - train: epoch 0104, iter [00700, 05004], lr: 0.049485, loss: 2.5516
2022-07-11 13:56:27 - train: epoch 0104, iter [00800, 05004], lr: 0.049468, loss: 2.3459
2022-07-11 13:57:03 - train: epoch 0104, iter [00900, 05004], lr: 0.049452, loss: 2.3027
2022-07-11 13:57:37 - train: epoch 0104, iter [01000, 05004], lr: 0.049436, loss: 2.0220
2022-07-11 13:58:13 - train: epoch 0104, iter [01100, 05004], lr: 0.049420, loss: 2.5353
2022-07-11 13:58:47 - train: epoch 0104, iter [01200, 05004], lr: 0.049404, loss: 2.2369
2022-07-11 13:59:22 - train: epoch 0104, iter [01300, 05004], lr: 0.049388, loss: 2.2969
2022-07-11 13:59:57 - train: epoch 0104, iter [01400, 05004], lr: 0.049372, loss: 2.1209
2022-07-11 14:00:33 - train: epoch 0104, iter [01500, 05004], lr: 0.049356, loss: 2.1451
2022-07-11 14:01:07 - train: epoch 0104, iter [01600, 05004], lr: 0.049340, loss: 2.1240
2022-07-11 14:01:42 - train: epoch 0104, iter [01700, 05004], lr: 0.049324, loss: 2.1241
2022-07-11 14:02:18 - train: epoch 0104, iter [01800, 05004], lr: 0.049307, loss: 2.1983
2022-07-11 14:02:53 - train: epoch 0104, iter [01900, 05004], lr: 0.049291, loss: 2.2814
2022-07-11 14:03:28 - train: epoch 0104, iter [02000, 05004], lr: 0.049275, loss: 2.0196
2022-07-11 14:04:03 - train: epoch 0104, iter [02100, 05004], lr: 0.049259, loss: 2.2408
2022-07-11 14:04:37 - train: epoch 0104, iter [02200, 05004], lr: 0.049243, loss: 2.3151
2022-07-11 14:05:12 - train: epoch 0104, iter [02300, 05004], lr: 0.049227, loss: 2.4476
2022-07-11 14:05:48 - train: epoch 0104, iter [02400, 05004], lr: 0.049211, loss: 2.3457
2022-07-11 14:06:22 - train: epoch 0104, iter [02500, 05004], lr: 0.049195, loss: 2.2304
2022-07-11 14:06:57 - train: epoch 0104, iter [02600, 05004], lr: 0.049179, loss: 2.2228
2022-07-11 14:07:33 - train: epoch 0104, iter [02700, 05004], lr: 0.049163, loss: 2.2802
2022-07-11 14:08:07 - train: epoch 0104, iter [02800, 05004], lr: 0.049147, loss: 2.2384
2022-07-11 14:08:43 - train: epoch 0104, iter [02900, 05004], lr: 0.049130, loss: 2.0169
2022-07-11 14:09:18 - train: epoch 0104, iter [03000, 05004], lr: 0.049114, loss: 2.2470
2022-07-11 14:09:53 - train: epoch 0104, iter [03100, 05004], lr: 0.049098, loss: 2.0704
2022-07-11 14:10:28 - train: epoch 0104, iter [03200, 05004], lr: 0.049082, loss: 2.0458
2022-07-11 14:11:03 - train: epoch 0104, iter [03300, 05004], lr: 0.049066, loss: 2.0509
2022-07-11 14:11:38 - train: epoch 0104, iter [03400, 05004], lr: 0.049050, loss: 2.1048
2022-07-11 14:12:13 - train: epoch 0104, iter [03500, 05004], lr: 0.049034, loss: 2.0580
2022-07-11 14:12:48 - train: epoch 0104, iter [03600, 05004], lr: 0.049018, loss: 2.2907
2022-07-11 14:13:23 - train: epoch 0104, iter [03700, 05004], lr: 0.049002, loss: 2.1808
2022-07-11 14:13:58 - train: epoch 0104, iter [03800, 05004], lr: 0.048986, loss: 1.9206
2022-07-11 14:14:33 - train: epoch 0104, iter [03900, 05004], lr: 0.048969, loss: 2.0617
2022-07-11 14:15:06 - train: epoch 0104, iter [04000, 05004], lr: 0.048953, loss: 2.0933
2022-07-11 14:15:41 - train: epoch 0104, iter [04100, 05004], lr: 0.048937, loss: 2.1491
2022-07-11 14:16:15 - train: epoch 0104, iter [04200, 05004], lr: 0.048921, loss: 2.2738
2022-07-11 14:16:50 - train: epoch 0104, iter [04300, 05004], lr: 0.048905, loss: 2.1221
2022-07-11 14:17:24 - train: epoch 0104, iter [04400, 05004], lr: 0.048889, loss: 1.9216
2022-07-11 14:17:58 - train: epoch 0104, iter [04500, 05004], lr: 0.048873, loss: 2.2806
2022-07-11 14:18:33 - train: epoch 0104, iter [04600, 05004], lr: 0.048857, loss: 2.2782
2022-07-11 14:19:07 - train: epoch 0104, iter [04700, 05004], lr: 0.048841, loss: 2.3630
2022-07-11 14:19:41 - train: epoch 0104, iter [04800, 05004], lr: 0.048825, loss: 2.2369
2022-07-11 14:20:15 - train: epoch 0104, iter [04900, 05004], lr: 0.048809, loss: 2.0889
2022-07-11 14:20:49 - train: epoch 0104, iter [05000, 05004], lr: 0.048792, loss: 2.2463
2022-07-11 14:20:50 - train: epoch 104, train_loss: 2.1971
2022-07-11 14:22:04 - eval: epoch: 104, acc1: 61.440%, acc5: 84.388%, test_loss: 1.6073, per_image_load_time: 2.429ms, per_image_inference_time: 0.441ms
2022-07-11 14:22:05 - until epoch: 104, best_acc1: 64.222%
2022-07-11 14:22:05 - epoch 105 lr: 0.048792
2022-07-11 14:22:45 - train: epoch 0105, iter [00100, 05004], lr: 0.048776, loss: 2.3906
2022-07-11 14:23:19 - train: epoch 0105, iter [00200, 05004], lr: 0.048760, loss: 2.1602
2022-07-11 14:23:53 - train: epoch 0105, iter [00300, 05004], lr: 0.048744, loss: 2.0392
2022-07-11 14:24:27 - train: epoch 0105, iter [00400, 05004], lr: 0.048727, loss: 2.1947
2022-07-11 14:25:00 - train: epoch 0105, iter [00500, 05004], lr: 0.048711, loss: 2.0287
2022-07-11 14:25:33 - train: epoch 0105, iter [00600, 05004], lr: 0.048695, loss: 2.2222
2022-07-11 14:26:08 - train: epoch 0105, iter [00700, 05004], lr: 0.048679, loss: 2.1287
2022-07-11 14:26:41 - train: epoch 0105, iter [00800, 05004], lr: 0.048663, loss: 2.3378
2022-07-11 14:27:15 - train: epoch 0105, iter [00900, 05004], lr: 0.048647, loss: 2.2079
2022-07-11 14:27:50 - train: epoch 0105, iter [01000, 05004], lr: 0.048631, loss: 2.2422
2022-07-11 14:28:23 - train: epoch 0105, iter [01100, 05004], lr: 0.048615, loss: 2.5712
2022-07-11 14:28:58 - train: epoch 0105, iter [01200, 05004], lr: 0.048599, loss: 2.2652
2022-07-11 14:29:32 - train: epoch 0105, iter [01300, 05004], lr: 0.048583, loss: 2.1341
2022-07-11 14:30:05 - train: epoch 0105, iter [01400, 05004], lr: 0.048567, loss: 2.1388
2022-07-11 14:30:40 - train: epoch 0105, iter [01500, 05004], lr: 0.048550, loss: 2.2814
2022-07-11 14:31:14 - train: epoch 0105, iter [01600, 05004], lr: 0.048534, loss: 1.9668
2022-07-11 14:31:48 - train: epoch 0105, iter [01700, 05004], lr: 0.048518, loss: 1.8881
2022-07-11 14:32:22 - train: epoch 0105, iter [01800, 05004], lr: 0.048502, loss: 2.1814
2022-07-11 14:32:57 - train: epoch 0105, iter [01900, 05004], lr: 0.048486, loss: 2.1332
2022-07-11 14:33:32 - train: epoch 0105, iter [02000, 05004], lr: 0.048470, loss: 2.4047
2022-07-11 14:34:06 - train: epoch 0105, iter [02100, 05004], lr: 0.048454, loss: 2.1457
2022-07-11 14:34:40 - train: epoch 0105, iter [02200, 05004], lr: 0.048438, loss: 2.3672
2022-07-11 14:35:14 - train: epoch 0105, iter [02300, 05004], lr: 0.048422, loss: 2.3937
2022-07-11 14:35:48 - train: epoch 0105, iter [02400, 05004], lr: 0.048406, loss: 2.1698
2022-07-11 14:36:23 - train: epoch 0105, iter [02500, 05004], lr: 0.048390, loss: 2.0372
2022-07-11 14:36:57 - train: epoch 0105, iter [02600, 05004], lr: 0.048373, loss: 2.3146
2022-07-11 14:37:31 - train: epoch 0105, iter [02700, 05004], lr: 0.048357, loss: 2.0765
2022-07-11 14:38:05 - train: epoch 0105, iter [02800, 05004], lr: 0.048341, loss: 2.3028
2022-07-11 14:38:39 - train: epoch 0105, iter [02900, 05004], lr: 0.048325, loss: 2.2561
2022-07-11 14:39:14 - train: epoch 0105, iter [03000, 05004], lr: 0.048309, loss: 2.1147
2022-07-11 14:39:49 - train: epoch 0105, iter [03100, 05004], lr: 0.048293, loss: 2.3052
2022-07-11 14:40:23 - train: epoch 0105, iter [03200, 05004], lr: 0.048277, loss: 2.4016
2022-07-11 14:40:57 - train: epoch 0105, iter [03300, 05004], lr: 0.048261, loss: 1.9047
2022-07-11 14:41:31 - train: epoch 0105, iter [03400, 05004], lr: 0.048245, loss: 2.0374
2022-07-11 14:42:06 - train: epoch 0105, iter [03500, 05004], lr: 0.048229, loss: 2.0645
2022-07-11 14:42:41 - train: epoch 0105, iter [03600, 05004], lr: 0.048213, loss: 2.2492
2022-07-11 14:43:14 - train: epoch 0105, iter [03700, 05004], lr: 0.048196, loss: 2.2853
2022-07-11 14:43:49 - train: epoch 0105, iter [03800, 05004], lr: 0.048180, loss: 2.5028
2022-07-11 14:44:23 - train: epoch 0105, iter [03900, 05004], lr: 0.048164, loss: 2.1799
2022-07-11 14:44:58 - train: epoch 0105, iter [04000, 05004], lr: 0.048148, loss: 2.1543
2022-07-11 14:45:32 - train: epoch 0105, iter [04100, 05004], lr: 0.048132, loss: 2.0616
2022-07-11 14:46:06 - train: epoch 0105, iter [04200, 05004], lr: 0.048116, loss: 1.9302
2022-07-11 14:46:41 - train: epoch 0105, iter [04300, 05004], lr: 0.048100, loss: 2.4693
2022-07-11 14:47:14 - train: epoch 0105, iter [04400, 05004], lr: 0.048084, loss: 1.9601
2022-07-11 14:47:48 - train: epoch 0105, iter [04500, 05004], lr: 0.048068, loss: 2.2837
2022-07-11 14:48:22 - train: epoch 0105, iter [04600, 05004], lr: 0.048052, loss: 2.3691
2022-07-11 14:48:57 - train: epoch 0105, iter [04700, 05004], lr: 0.048036, loss: 2.3382
2022-07-11 14:49:31 - train: epoch 0105, iter [04800, 05004], lr: 0.048020, loss: 2.1217
2022-07-11 14:50:06 - train: epoch 0105, iter [04900, 05004], lr: 0.048003, loss: 2.2255
2022-07-11 14:50:39 - train: epoch 0105, iter [05000, 05004], lr: 0.047987, loss: 2.0679
2022-07-11 14:50:40 - train: epoch 105, train_loss: 2.1914
2022-07-11 14:51:55 - eval: epoch: 105, acc1: 63.744%, acc5: 86.000%, test_loss: 1.4985, per_image_load_time: 2.446ms, per_image_inference_time: 0.440ms
2022-07-11 14:51:55 - until epoch: 105, best_acc1: 64.222%
2022-07-11 14:51:55 - epoch 106 lr: 0.047987
2022-07-11 14:52:35 - train: epoch 0106, iter [00100, 05004], lr: 0.047971, loss: 2.0474
2022-07-11 14:53:09 - train: epoch 0106, iter [00200, 05004], lr: 0.047955, loss: 2.0165
2022-07-11 14:53:43 - train: epoch 0106, iter [00300, 05004], lr: 0.047938, loss: 1.9396
2022-07-11 14:54:17 - train: epoch 0106, iter [00400, 05004], lr: 0.047922, loss: 2.2461
2022-07-11 14:54:52 - train: epoch 0106, iter [00500, 05004], lr: 0.047906, loss: 2.1700
2022-07-11 14:55:26 - train: epoch 0106, iter [00600, 05004], lr: 0.047890, loss: 2.1798
2022-07-11 14:55:59 - train: epoch 0106, iter [00700, 05004], lr: 0.047874, loss: 2.3565
2022-07-11 14:56:34 - train: epoch 0106, iter [00800, 05004], lr: 0.047858, loss: 2.2152
2022-07-11 14:57:08 - train: epoch 0106, iter [00900, 05004], lr: 0.047842, loss: 2.3600
2022-07-11 14:57:41 - train: epoch 0106, iter [01000, 05004], lr: 0.047826, loss: 2.3314
2022-07-11 14:58:16 - train: epoch 0106, iter [01100, 05004], lr: 0.047810, loss: 2.0011
2022-07-11 14:58:50 - train: epoch 0106, iter [01200, 05004], lr: 0.047794, loss: 1.9650
2022-07-11 14:59:24 - train: epoch 0106, iter [01300, 05004], lr: 0.047778, loss: 2.2015
2022-07-11 14:59:58 - train: epoch 0106, iter [01400, 05004], lr: 0.047762, loss: 2.0523
2022-07-11 15:00:33 - train: epoch 0106, iter [01500, 05004], lr: 0.047745, loss: 2.3284
2022-07-11 15:01:07 - train: epoch 0106, iter [01600, 05004], lr: 0.047729, loss: 2.0360
2022-07-11 15:01:40 - train: epoch 0106, iter [01700, 05004], lr: 0.047713, loss: 2.1394
2022-07-11 15:02:15 - train: epoch 0106, iter [01800, 05004], lr: 0.047697, loss: 1.9600
2022-07-11 15:02:50 - train: epoch 0106, iter [01900, 05004], lr: 0.047681, loss: 2.1152
2022-07-11 15:03:24 - train: epoch 0106, iter [02000, 05004], lr: 0.047665, loss: 2.0601
2022-07-11 15:03:58 - train: epoch 0106, iter [02100, 05004], lr: 0.047649, loss: 2.0502
2022-07-11 15:04:32 - train: epoch 0106, iter [02200, 05004], lr: 0.047633, loss: 2.2391
2022-07-11 15:05:06 - train: epoch 0106, iter [02300, 05004], lr: 0.047617, loss: 2.2591
2022-07-11 15:05:41 - train: epoch 0106, iter [02400, 05004], lr: 0.047601, loss: 2.4646
2022-07-11 15:06:15 - train: epoch 0106, iter [02500, 05004], lr: 0.047585, loss: 2.4219
2022-07-11 15:06:50 - train: epoch 0106, iter [02600, 05004], lr: 0.047569, loss: 1.9877
2022-07-11 15:07:23 - train: epoch 0106, iter [02700, 05004], lr: 0.047552, loss: 2.3281
2022-07-11 15:07:58 - train: epoch 0106, iter [02800, 05004], lr: 0.047536, loss: 1.8016
2022-07-11 15:08:32 - train: epoch 0106, iter [02900, 05004], lr: 0.047520, loss: 2.1824
2022-07-11 15:09:07 - train: epoch 0106, iter [03000, 05004], lr: 0.047504, loss: 2.5917
2022-07-11 15:09:41 - train: epoch 0106, iter [03100, 05004], lr: 0.047488, loss: 2.2927
2022-07-11 15:10:16 - train: epoch 0106, iter [03200, 05004], lr: 0.047472, loss: 2.1329
2022-07-11 15:10:50 - train: epoch 0106, iter [03300, 05004], lr: 0.047456, loss: 2.2716
2022-07-11 15:11:24 - train: epoch 0106, iter [03400, 05004], lr: 0.047440, loss: 2.1458
2022-07-11 15:11:59 - train: epoch 0106, iter [03500, 05004], lr: 0.047424, loss: 2.1958
2022-07-11 15:12:33 - train: epoch 0106, iter [03600, 05004], lr: 0.047408, loss: 2.1960
2022-07-11 15:13:07 - train: epoch 0106, iter [03700, 05004], lr: 0.047392, loss: 2.2366
2022-07-11 15:13:41 - train: epoch 0106, iter [03800, 05004], lr: 0.047376, loss: 2.5212
2022-07-11 15:14:16 - train: epoch 0106, iter [03900, 05004], lr: 0.047360, loss: 2.3166
2022-07-11 15:14:50 - train: epoch 0106, iter [04000, 05004], lr: 0.047343, loss: 2.2365
2022-07-11 15:15:25 - train: epoch 0106, iter [04100, 05004], lr: 0.047327, loss: 2.4201
2022-07-11 15:16:00 - train: epoch 0106, iter [04200, 05004], lr: 0.047311, loss: 2.1478
2022-07-11 15:16:35 - train: epoch 0106, iter [04300, 05004], lr: 0.047295, loss: 2.2197
2022-07-11 15:17:10 - train: epoch 0106, iter [04400, 05004], lr: 0.047279, loss: 2.2892
2022-07-11 15:17:44 - train: epoch 0106, iter [04500, 05004], lr: 0.047263, loss: 2.3148
2022-07-11 15:18:20 - train: epoch 0106, iter [04600, 05004], lr: 0.047247, loss: 2.3154
2022-07-11 15:18:54 - train: epoch 0106, iter [04700, 05004], lr: 0.047231, loss: 2.3077
2022-07-11 15:19:29 - train: epoch 0106, iter [04800, 05004], lr: 0.047215, loss: 2.2275
2022-07-11 15:20:04 - train: epoch 0106, iter [04900, 05004], lr: 0.047199, loss: 2.1015
2022-07-11 15:20:38 - train: epoch 0106, iter [05000, 05004], lr: 0.047183, loss: 2.5156
2022-07-11 15:20:39 - train: epoch 106, train_loss: 2.1817
2022-07-11 15:21:55 - eval: epoch: 106, acc1: 63.488%, acc5: 85.716%, test_loss: 1.5228, per_image_load_time: 2.421ms, per_image_inference_time: 0.477ms
2022-07-11 15:21:55 - until epoch: 106, best_acc1: 64.222%
2022-07-11 15:21:55 - epoch 107 lr: 0.047182
2022-07-11 15:22:36 - train: epoch 0107, iter [00100, 05004], lr: 0.047166, loss: 1.9928
2022-07-11 15:23:11 - train: epoch 0107, iter [00200, 05004], lr: 0.047150, loss: 2.3475
2022-07-11 15:23:45 - train: epoch 0107, iter [00300, 05004], lr: 0.047134, loss: 2.0398
2022-07-11 15:24:19 - train: epoch 0107, iter [00400, 05004], lr: 0.047118, loss: 2.1550
2022-07-11 15:24:54 - train: epoch 0107, iter [00500, 05004], lr: 0.047102, loss: 2.0945
2022-07-11 15:25:27 - train: epoch 0107, iter [00600, 05004], lr: 0.047086, loss: 1.8498
2022-07-11 15:26:01 - train: epoch 0107, iter [00700, 05004], lr: 0.047070, loss: 2.1957
2022-07-11 15:26:36 - train: epoch 0107, iter [00800, 05004], lr: 0.047054, loss: 1.9471
2022-07-11 15:27:08 - train: epoch 0107, iter [00900, 05004], lr: 0.047037, loss: 2.2755
2022-07-11 15:27:43 - train: epoch 0107, iter [01000, 05004], lr: 0.047021, loss: 2.1667
2022-07-11 15:28:16 - train: epoch 0107, iter [01100, 05004], lr: 0.047005, loss: 2.1722
2022-07-11 15:28:51 - train: epoch 0107, iter [01200, 05004], lr: 0.046989, loss: 1.9537
2022-07-11 15:29:25 - train: epoch 0107, iter [01300, 05004], lr: 0.046973, loss: 2.0444
2022-07-11 15:30:00 - train: epoch 0107, iter [01400, 05004], lr: 0.046957, loss: 2.0140
2022-07-11 15:30:33 - train: epoch 0107, iter [01500, 05004], lr: 0.046941, loss: 2.0284
2022-07-11 15:31:07 - train: epoch 0107, iter [01600, 05004], lr: 0.046925, loss: 2.1187
2022-07-11 15:31:42 - train: epoch 0107, iter [01700, 05004], lr: 0.046909, loss: 2.1397
2022-07-11 15:32:16 - train: epoch 0107, iter [01800, 05004], lr: 0.046893, loss: 2.2205
2022-07-11 15:32:50 - train: epoch 0107, iter [01900, 05004], lr: 0.046877, loss: 2.2934
2022-07-11 15:33:25 - train: epoch 0107, iter [02000, 05004], lr: 0.046861, loss: 2.1607
2022-07-11 15:33:59 - train: epoch 0107, iter [02100, 05004], lr: 0.046845, loss: 2.0663
2022-07-11 15:34:33 - train: epoch 0107, iter [02200, 05004], lr: 0.046829, loss: 2.1129
2022-07-11 15:35:07 - train: epoch 0107, iter [02300, 05004], lr: 0.046813, loss: 1.9819
2022-07-11 15:35:41 - train: epoch 0107, iter [02400, 05004], lr: 0.046796, loss: 2.1020
2022-07-11 15:36:15 - train: epoch 0107, iter [02500, 05004], lr: 0.046780, loss: 1.9348
2022-07-11 15:36:49 - train: epoch 0107, iter [02600, 05004], lr: 0.046764, loss: 2.2174
2022-07-11 15:37:24 - train: epoch 0107, iter [02700, 05004], lr: 0.046748, loss: 2.1143
2022-07-11 15:37:58 - train: epoch 0107, iter [02800, 05004], lr: 0.046732, loss: 2.1004
2022-07-11 15:38:33 - train: epoch 0107, iter [02900, 05004], lr: 0.046716, loss: 2.0716
2022-07-11 15:39:07 - train: epoch 0107, iter [03000, 05004], lr: 0.046700, loss: 1.9846
2022-07-11 15:39:42 - train: epoch 0107, iter [03100, 05004], lr: 0.046684, loss: 2.1070
2022-07-11 15:40:16 - train: epoch 0107, iter [03200, 05004], lr: 0.046668, loss: 2.1151
2022-07-11 15:40:51 - train: epoch 0107, iter [03300, 05004], lr: 0.046652, loss: 2.3524
2022-07-11 15:41:25 - train: epoch 0107, iter [03400, 05004], lr: 0.046636, loss: 2.0318
2022-07-11 15:42:00 - train: epoch 0107, iter [03500, 05004], lr: 0.046620, loss: 1.9666
2022-07-11 15:42:34 - train: epoch 0107, iter [03600, 05004], lr: 0.046604, loss: 2.3315
2022-07-11 15:43:09 - train: epoch 0107, iter [03700, 05004], lr: 0.046588, loss: 2.0896
2022-07-11 15:43:43 - train: epoch 0107, iter [03800, 05004], lr: 0.046572, loss: 2.2021
2022-07-11 15:44:18 - train: epoch 0107, iter [03900, 05004], lr: 0.046556, loss: 1.9815
2022-07-11 15:44:52 - train: epoch 0107, iter [04000, 05004], lr: 0.046539, loss: 2.2435
2022-07-11 15:45:27 - train: epoch 0107, iter [04100, 05004], lr: 0.046523, loss: 1.9900
2022-07-11 15:46:02 - train: epoch 0107, iter [04200, 05004], lr: 0.046507, loss: 2.2835
2022-07-11 15:46:35 - train: epoch 0107, iter [04300, 05004], lr: 0.046491, loss: 2.6173
2022-07-11 15:47:10 - train: epoch 0107, iter [04400, 05004], lr: 0.046475, loss: 1.9667
2022-07-11 15:47:44 - train: epoch 0107, iter [04500, 05004], lr: 0.046459, loss: 2.2084
2022-07-11 15:48:18 - train: epoch 0107, iter [04600, 05004], lr: 0.046443, loss: 2.2942
2022-07-11 15:48:53 - train: epoch 0107, iter [04700, 05004], lr: 0.046427, loss: 2.1544
2022-07-11 15:49:27 - train: epoch 0107, iter [04800, 05004], lr: 0.046411, loss: 1.9188
2022-07-11 15:50:01 - train: epoch 0107, iter [04900, 05004], lr: 0.046395, loss: 2.3525
2022-07-11 15:50:34 - train: epoch 0107, iter [05000, 05004], lr: 0.046379, loss: 1.9382
2022-07-11 15:50:35 - train: epoch 107, train_loss: 2.1761
2022-07-11 15:51:49 - eval: epoch: 107, acc1: 65.030%, acc5: 86.886%, test_loss: 1.4384, per_image_load_time: 2.121ms, per_image_inference_time: 0.472ms
2022-07-11 15:51:50 - until epoch: 107, best_acc1: 65.030%
2022-07-11 15:51:50 - epoch 108 lr: 0.046378
2022-07-11 15:52:29 - train: epoch 0108, iter [00100, 05004], lr: 0.046362, loss: 1.7228
2022-07-11 15:53:04 - train: epoch 0108, iter [00200, 05004], lr: 0.046346, loss: 2.0154
2022-07-11 15:53:38 - train: epoch 0108, iter [00300, 05004], lr: 0.046330, loss: 2.2929
2022-07-11 15:54:12 - train: epoch 0108, iter [00400, 05004], lr: 0.046314, loss: 2.4787
2022-07-11 15:54:47 - train: epoch 0108, iter [00500, 05004], lr: 0.046298, loss: 1.9977
2022-07-11 15:55:21 - train: epoch 0108, iter [00600, 05004], lr: 0.046282, loss: 2.1920
2022-07-11 15:55:55 - train: epoch 0108, iter [00700, 05004], lr: 0.046266, loss: 2.1741
2022-07-11 15:56:30 - train: epoch 0108, iter [00800, 05004], lr: 0.046250, loss: 2.2880
2022-07-11 15:57:03 - train: epoch 0108, iter [00900, 05004], lr: 0.046234, loss: 1.9893
2022-07-11 15:57:39 - train: epoch 0108, iter [01000, 05004], lr: 0.046218, loss: 2.3616
2022-07-11 15:58:12 - train: epoch 0108, iter [01100, 05004], lr: 0.046202, loss: 2.3777
2022-07-11 15:58:48 - train: epoch 0108, iter [01200, 05004], lr: 0.046186, loss: 2.1500
2022-07-11 15:59:21 - train: epoch 0108, iter [01300, 05004], lr: 0.046170, loss: 2.0435
2022-07-11 15:59:56 - train: epoch 0108, iter [01400, 05004], lr: 0.046154, loss: 2.2681
2022-07-11 16:00:31 - train: epoch 0108, iter [01500, 05004], lr: 0.046137, loss: 2.1468
2022-07-11 16:01:05 - train: epoch 0108, iter [01600, 05004], lr: 0.046121, loss: 2.1310
2022-07-11 16:01:40 - train: epoch 0108, iter [01700, 05004], lr: 0.046105, loss: 2.2480
2022-07-11 16:02:14 - train: epoch 0108, iter [01800, 05004], lr: 0.046089, loss: 2.2115
2022-07-11 16:02:49 - train: epoch 0108, iter [01900, 05004], lr: 0.046073, loss: 2.1930
2022-07-11 16:03:23 - train: epoch 0108, iter [02000, 05004], lr: 0.046057, loss: 2.1781
2022-07-11 16:03:57 - train: epoch 0108, iter [02100, 05004], lr: 0.046041, loss: 2.1874
2022-07-11 16:04:31 - train: epoch 0108, iter [02200, 05004], lr: 0.046025, loss: 2.0997
2022-07-11 16:05:06 - train: epoch 0108, iter [02300, 05004], lr: 0.046009, loss: 2.1392
2022-07-11 16:05:40 - train: epoch 0108, iter [02400, 05004], lr: 0.045993, loss: 2.0882
2022-07-11 16:06:15 - train: epoch 0108, iter [02500, 05004], lr: 0.045977, loss: 2.1275
2022-07-11 16:06:50 - train: epoch 0108, iter [02600, 05004], lr: 0.045961, loss: 2.5198
2022-07-11 16:07:24 - train: epoch 0108, iter [02700, 05004], lr: 0.045945, loss: 2.0267
2022-07-11 16:07:59 - train: epoch 0108, iter [02800, 05004], lr: 0.045929, loss: 2.2603
2022-07-11 16:08:33 - train: epoch 0108, iter [02900, 05004], lr: 0.045913, loss: 2.5614
2022-07-11 16:09:08 - train: epoch 0108, iter [03000, 05004], lr: 0.045897, loss: 2.1795
2022-07-11 16:09:42 - train: epoch 0108, iter [03100, 05004], lr: 0.045881, loss: 2.1271
2022-07-11 16:10:16 - train: epoch 0108, iter [03200, 05004], lr: 0.045865, loss: 2.0104
2022-07-11 16:10:51 - train: epoch 0108, iter [03300, 05004], lr: 0.045849, loss: 1.9531
2022-07-11 16:11:25 - train: epoch 0108, iter [03400, 05004], lr: 0.045833, loss: 2.2744
2022-07-11 16:12:00 - train: epoch 0108, iter [03500, 05004], lr: 0.045817, loss: 2.1461
2022-07-11 16:12:34 - train: epoch 0108, iter [03600, 05004], lr: 0.045801, loss: 2.1492
2022-07-11 16:13:08 - train: epoch 0108, iter [03700, 05004], lr: 0.045784, loss: 2.2000
2022-07-11 16:13:42 - train: epoch 0108, iter [03800, 05004], lr: 0.045768, loss: 2.0888
2022-07-11 16:14:17 - train: epoch 0108, iter [03900, 05004], lr: 0.045752, loss: 2.1234
2022-07-11 16:14:52 - train: epoch 0108, iter [04000, 05004], lr: 0.045736, loss: 2.2239
2022-07-11 16:15:26 - train: epoch 0108, iter [04100, 05004], lr: 0.045720, loss: 2.0392
2022-07-11 16:16:00 - train: epoch 0108, iter [04200, 05004], lr: 0.045704, loss: 2.1088
2022-07-11 16:16:34 - train: epoch 0108, iter [04300, 05004], lr: 0.045688, loss: 2.2749
2022-07-11 16:17:09 - train: epoch 0108, iter [04400, 05004], lr: 0.045672, loss: 2.0497
2022-07-11 16:17:44 - train: epoch 0108, iter [04500, 05004], lr: 0.045656, loss: 2.0626
2022-07-11 16:18:18 - train: epoch 0108, iter [04600, 05004], lr: 0.045640, loss: 2.2581
2022-07-11 16:18:52 - train: epoch 0108, iter [04700, 05004], lr: 0.045624, loss: 2.2325
2022-07-11 16:19:27 - train: epoch 0108, iter [04800, 05004], lr: 0.045608, loss: 1.9320
2022-07-11 16:20:01 - train: epoch 0108, iter [04900, 05004], lr: 0.045592, loss: 1.9439
2022-07-11 16:20:34 - train: epoch 0108, iter [05000, 05004], lr: 0.045576, loss: 1.8468
2022-07-11 16:20:35 - train: epoch 108, train_loss: 2.1698
2022-07-11 16:21:49 - eval: epoch: 108, acc1: 63.534%, acc5: 85.548%, test_loss: 1.5230, per_image_load_time: 1.116ms, per_image_inference_time: 0.463ms
2022-07-11 16:21:49 - until epoch: 108, best_acc1: 65.030%
2022-07-11 16:21:49 - epoch 109 lr: 0.045575
2022-07-11 16:22:29 - train: epoch 0109, iter [00100, 05004], lr: 0.045559, loss: 2.0850
2022-07-11 16:23:03 - train: epoch 0109, iter [00200, 05004], lr: 0.045543, loss: 2.0845
2022-07-11 16:23:36 - train: epoch 0109, iter [00300, 05004], lr: 0.045527, loss: 2.0356
2022-07-11 16:24:11 - train: epoch 0109, iter [00400, 05004], lr: 0.045511, loss: 2.1268
2022-07-11 16:24:44 - train: epoch 0109, iter [00500, 05004], lr: 0.045495, loss: 2.3226
2022-07-11 16:25:19 - train: epoch 0109, iter [00600, 05004], lr: 0.045479, loss: 2.1458
2022-07-11 16:25:53 - train: epoch 0109, iter [00700, 05004], lr: 0.045463, loss: 2.2343
2022-07-11 16:26:27 - train: epoch 0109, iter [00800, 05004], lr: 0.045447, loss: 1.8861
2022-07-11 16:27:01 - train: epoch 0109, iter [00900, 05004], lr: 0.045431, loss: 2.1050
2022-07-11 16:27:35 - train: epoch 0109, iter [01000, 05004], lr: 0.045415, loss: 1.7080
2022-07-11 16:28:09 - train: epoch 0109, iter [01100, 05004], lr: 0.045399, loss: 1.9150
2022-07-11 16:28:43 - train: epoch 0109, iter [01200, 05004], lr: 0.045383, loss: 2.1879
2022-07-11 16:29:17 - train: epoch 0109, iter [01300, 05004], lr: 0.045367, loss: 2.1082
2022-07-11 16:29:52 - train: epoch 0109, iter [01400, 05004], lr: 0.045351, loss: 1.9616
2022-07-11 16:30:26 - train: epoch 0109, iter [01500, 05004], lr: 0.045335, loss: 2.1969
2022-07-11 16:31:01 - train: epoch 0109, iter [01600, 05004], lr: 0.045319, loss: 2.0320
2022-07-11 16:31:36 - train: epoch 0109, iter [01700, 05004], lr: 0.045303, loss: 2.2616
2022-07-11 16:32:10 - train: epoch 0109, iter [01800, 05004], lr: 0.045287, loss: 2.1589
2022-07-11 16:32:44 - train: epoch 0109, iter [01900, 05004], lr: 0.045271, loss: 2.1195
2022-07-11 16:33:18 - train: epoch 0109, iter [02000, 05004], lr: 0.045255, loss: 2.1304
2022-07-11 16:33:52 - train: epoch 0109, iter [02100, 05004], lr: 0.045239, loss: 2.0598
2022-07-11 16:34:26 - train: epoch 0109, iter [02200, 05004], lr: 0.045223, loss: 2.4066
2022-07-11 16:35:01 - train: epoch 0109, iter [02300, 05004], lr: 0.045207, loss: 2.3570
2022-07-11 16:35:36 - train: epoch 0109, iter [02400, 05004], lr: 0.045191, loss: 2.1192
2022-07-11 16:36:09 - train: epoch 0109, iter [02500, 05004], lr: 0.045175, loss: 2.2178
2022-07-11 16:36:43 - train: epoch 0109, iter [02600, 05004], lr: 0.045159, loss: 2.4301
2022-07-11 16:37:18 - train: epoch 0109, iter [02700, 05004], lr: 0.045143, loss: 2.0122
2022-07-11 16:37:53 - train: epoch 0109, iter [02800, 05004], lr: 0.045127, loss: 1.9890
2022-07-11 16:38:27 - train: epoch 0109, iter [02900, 05004], lr: 0.045111, loss: 2.2064
2022-07-11 16:39:01 - train: epoch 0109, iter [03000, 05004], lr: 0.045095, loss: 2.4922
2022-07-11 16:39:36 - train: epoch 0109, iter [03100, 05004], lr: 0.045078, loss: 2.1712
2022-07-11 16:40:09 - train: epoch 0109, iter [03200, 05004], lr: 0.045062, loss: 1.8785
2022-07-11 16:40:44 - train: epoch 0109, iter [03300, 05004], lr: 0.045046, loss: 2.4175
2022-07-11 16:41:19 - train: epoch 0109, iter [03400, 05004], lr: 0.045030, loss: 2.2948
2022-07-11 16:41:53 - train: epoch 0109, iter [03500, 05004], lr: 0.045014, loss: 2.2148
2022-07-11 16:42:28 - train: epoch 0109, iter [03600, 05004], lr: 0.044998, loss: 1.9868
2022-07-11 16:43:02 - train: epoch 0109, iter [03700, 05004], lr: 0.044982, loss: 2.2194
2022-07-11 16:43:36 - train: epoch 0109, iter [03800, 05004], lr: 0.044966, loss: 2.3721
2022-07-11 16:44:11 - train: epoch 0109, iter [03900, 05004], lr: 0.044950, loss: 1.9698
2022-07-11 16:44:46 - train: epoch 0109, iter [04000, 05004], lr: 0.044934, loss: 2.2649
2022-07-11 16:45:20 - train: epoch 0109, iter [04100, 05004], lr: 0.044918, loss: 2.1101
2022-07-11 16:45:55 - train: epoch 0109, iter [04200, 05004], lr: 0.044902, loss: 2.2410
2022-07-11 16:46:29 - train: epoch 0109, iter [04300, 05004], lr: 0.044886, loss: 2.2488
2022-07-11 16:47:04 - train: epoch 0109, iter [04400, 05004], lr: 0.044870, loss: 2.1637
2022-07-11 16:47:39 - train: epoch 0109, iter [04500, 05004], lr: 0.044854, loss: 2.1936
2022-07-11 16:48:14 - train: epoch 0109, iter [04600, 05004], lr: 0.044838, loss: 2.1844
2022-07-11 16:48:49 - train: epoch 0109, iter [04700, 05004], lr: 0.044822, loss: 1.9755
2022-07-11 16:49:24 - train: epoch 0109, iter [04800, 05004], lr: 0.044806, loss: 2.4748
2022-07-11 16:49:59 - train: epoch 0109, iter [04900, 05004], lr: 0.044790, loss: 2.1803
2022-07-11 16:50:33 - train: epoch 0109, iter [05000, 05004], lr: 0.044774, loss: 1.9700
2022-07-11 16:50:34 - train: epoch 109, train_loss: 2.1590
2022-07-11 16:51:48 - eval: epoch: 109, acc1: 65.284%, acc5: 86.946%, test_loss: 1.4328, per_image_load_time: 2.259ms, per_image_inference_time: 0.479ms
2022-07-11 16:51:48 - until epoch: 109, best_acc1: 65.284%
2022-07-11 16:51:48 - epoch 110 lr: 0.044773
2022-07-11 16:52:28 - train: epoch 0110, iter [00100, 05004], lr: 0.044758, loss: 2.0405
2022-07-11 16:53:02 - train: epoch 0110, iter [00200, 05004], lr: 0.044742, loss: 2.0781
2022-07-11 16:53:36 - train: epoch 0110, iter [00300, 05004], lr: 0.044726, loss: 2.2901
2022-07-11 16:54:10 - train: epoch 0110, iter [00400, 05004], lr: 0.044710, loss: 2.1212
2022-07-11 16:54:45 - train: epoch 0110, iter [00500, 05004], lr: 0.044694, loss: 2.1314
2022-07-11 16:55:19 - train: epoch 0110, iter [00600, 05004], lr: 0.044678, loss: 2.2014
2022-07-11 16:55:53 - train: epoch 0110, iter [00700, 05004], lr: 0.044662, loss: 2.2957
2022-07-11 16:56:27 - train: epoch 0110, iter [00800, 05004], lr: 0.044646, loss: 2.2231
2022-07-11 16:57:02 - train: epoch 0110, iter [00900, 05004], lr: 0.044630, loss: 2.3731
2022-07-11 16:57:36 - train: epoch 0110, iter [01000, 05004], lr: 0.044614, loss: 2.0067
2022-07-11 16:58:10 - train: epoch 0110, iter [01100, 05004], lr: 0.044598, loss: 2.3491
2022-07-11 16:58:43 - train: epoch 0110, iter [01200, 05004], lr: 0.044582, loss: 2.0614
2022-07-11 16:59:17 - train: epoch 0110, iter [01300, 05004], lr: 0.044565, loss: 2.2119
2022-07-11 16:59:51 - train: epoch 0110, iter [01400, 05004], lr: 0.044549, loss: 2.0325
2022-07-11 17:00:25 - train: epoch 0110, iter [01500, 05004], lr: 0.044533, loss: 2.1345
2022-07-11 17:00:58 - train: epoch 0110, iter [01600, 05004], lr: 0.044517, loss: 1.9802
2022-07-11 17:01:33 - train: epoch 0110, iter [01700, 05004], lr: 0.044501, loss: 2.0874
2022-07-11 17:02:08 - train: epoch 0110, iter [01800, 05004], lr: 0.044485, loss: 2.0403
2022-07-11 17:02:41 - train: epoch 0110, iter [01900, 05004], lr: 0.044469, loss: 2.1774
2022-07-11 17:03:15 - train: epoch 0110, iter [02000, 05004], lr: 0.044453, loss: 2.0077
2022-07-11 17:03:49 - train: epoch 0110, iter [02100, 05004], lr: 0.044437, loss: 2.0233
2022-07-11 17:04:23 - train: epoch 0110, iter [02200, 05004], lr: 0.044421, loss: 2.2567
2022-07-11 17:04:58 - train: epoch 0110, iter [02300, 05004], lr: 0.044406, loss: 2.1933
2022-07-11 17:05:31 - train: epoch 0110, iter [02400, 05004], lr: 0.044390, loss: 2.0677
2022-07-11 17:06:05 - train: epoch 0110, iter [02500, 05004], lr: 0.044374, loss: 2.0924
2022-07-11 17:06:40 - train: epoch 0110, iter [02600, 05004], lr: 0.044358, loss: 2.4132
2022-07-11 17:07:14 - train: epoch 0110, iter [02700, 05004], lr: 0.044342, loss: 2.1948
2022-07-11 17:07:48 - train: epoch 0110, iter [02800, 05004], lr: 0.044326, loss: 2.1598
2022-07-11 17:08:23 - train: epoch 0110, iter [02900, 05004], lr: 0.044310, loss: 2.2661
2022-07-11 17:08:56 - train: epoch 0110, iter [03000, 05004], lr: 0.044294, loss: 1.7882
2022-07-11 17:09:31 - train: epoch 0110, iter [03100, 05004], lr: 0.044278, loss: 2.1893
2022-07-11 17:10:05 - train: epoch 0110, iter [03200, 05004], lr: 0.044262, loss: 2.2550
2022-07-11 17:10:40 - train: epoch 0110, iter [03300, 05004], lr: 0.044246, loss: 2.2471
2022-07-11 17:11:13 - train: epoch 0110, iter [03400, 05004], lr: 0.044230, loss: 2.0994
2022-07-11 17:11:48 - train: epoch 0110, iter [03500, 05004], lr: 0.044214, loss: 2.2230
2022-07-11 17:12:22 - train: epoch 0110, iter [03600, 05004], lr: 0.044198, loss: 2.0764
2022-07-11 17:12:55 - train: epoch 0110, iter [03700, 05004], lr: 0.044182, loss: 2.2495
2022-07-11 17:13:31 - train: epoch 0110, iter [03800, 05004], lr: 0.044166, loss: 2.1414
2022-07-11 17:14:04 - train: epoch 0110, iter [03900, 05004], lr: 0.044150, loss: 2.1082
2022-07-11 17:14:37 - train: epoch 0110, iter [04000, 05004], lr: 0.044134, loss: 2.0189
2022-07-11 17:15:12 - train: epoch 0110, iter [04100, 05004], lr: 0.044118, loss: 1.9964
2022-07-11 17:15:47 - train: epoch 0110, iter [04200, 05004], lr: 0.044102, loss: 2.1341
2022-07-11 17:16:20 - train: epoch 0110, iter [04300, 05004], lr: 0.044086, loss: 2.0327
2022-07-11 17:16:55 - train: epoch 0110, iter [04400, 05004], lr: 0.044070, loss: 2.3142
2022-07-11 17:17:29 - train: epoch 0110, iter [04500, 05004], lr: 0.044054, loss: 2.1649
2022-07-11 17:18:04 - train: epoch 0110, iter [04600, 05004], lr: 0.044038, loss: 2.0980
2022-07-11 17:18:38 - train: epoch 0110, iter [04700, 05004], lr: 0.044022, loss: 2.3279
2022-07-11 17:19:11 - train: epoch 0110, iter [04800, 05004], lr: 0.044006, loss: 2.3316
2022-07-11 17:19:45 - train: epoch 0110, iter [04900, 05004], lr: 0.043990, loss: 2.2802
2022-07-11 17:20:18 - train: epoch 0110, iter [05000, 05004], lr: 0.043974, loss: 2.0053
2022-07-11 17:20:19 - train: epoch 110, train_loss: 2.1558
2022-07-11 17:21:34 - eval: epoch: 110, acc1: 65.022%, acc5: 86.684%, test_loss: 1.4469, per_image_load_time: 2.383ms, per_image_inference_time: 0.449ms
2022-07-11 17:21:34 - until epoch: 110, best_acc1: 65.284%
2022-07-11 17:21:34 - epoch 111 lr: 0.043973
2022-07-11 17:22:15 - train: epoch 0111, iter [00100, 05004], lr: 0.043957, loss: 2.1504
2022-07-11 17:22:49 - train: epoch 0111, iter [00200, 05004], lr: 0.043941, loss: 2.2304
2022-07-11 17:23:23 - train: epoch 0111, iter [00300, 05004], lr: 0.043925, loss: 1.9652
2022-07-11 17:23:57 - train: epoch 0111, iter [00400, 05004], lr: 0.043909, loss: 2.1833
2022-07-11 17:24:31 - train: epoch 0111, iter [00500, 05004], lr: 0.043893, loss: 2.2020
2022-07-11 17:25:04 - train: epoch 0111, iter [00600, 05004], lr: 0.043877, loss: 2.4320
2022-07-11 17:25:38 - train: epoch 0111, iter [00700, 05004], lr: 0.043861, loss: 2.3095
2022-07-11 17:26:12 - train: epoch 0111, iter [00800, 05004], lr: 0.043845, loss: 1.8113
2022-07-11 17:26:45 - train: epoch 0111, iter [00900, 05004], lr: 0.043829, loss: 2.1038
2022-07-11 17:27:19 - train: epoch 0111, iter [01000, 05004], lr: 0.043813, loss: 1.9335
2022-07-11 17:27:53 - train: epoch 0111, iter [01100, 05004], lr: 0.043797, loss: 1.7842
2022-07-11 17:28:27 - train: epoch 0111, iter [01200, 05004], lr: 0.043781, loss: 2.2361
2022-07-11 17:29:01 - train: epoch 0111, iter [01300, 05004], lr: 0.043765, loss: 2.1001
2022-07-11 17:29:35 - train: epoch 0111, iter [01400, 05004], lr: 0.043750, loss: 2.3527
2022-07-11 17:30:10 - train: epoch 0111, iter [01500, 05004], lr: 0.043734, loss: 2.3606
2022-07-11 17:30:43 - train: epoch 0111, iter [01600, 05004], lr: 0.043718, loss: 1.9033
2022-07-11 17:31:18 - train: epoch 0111, iter [01700, 05004], lr: 0.043702, loss: 2.0820
2022-07-11 17:31:52 - train: epoch 0111, iter [01800, 05004], lr: 0.043686, loss: 2.0151
2022-07-11 17:32:26 - train: epoch 0111, iter [01900, 05004], lr: 0.043670, loss: 2.1957
2022-07-11 17:33:00 - train: epoch 0111, iter [02000, 05004], lr: 0.043654, loss: 1.9816
2022-07-11 17:33:34 - train: epoch 0111, iter [02100, 05004], lr: 0.043638, loss: 2.3384
2022-07-11 17:34:08 - train: epoch 0111, iter [02200, 05004], lr: 0.043622, loss: 2.3725
2022-07-11 17:34:43 - train: epoch 0111, iter [02300, 05004], lr: 0.043606, loss: 2.1242
2022-07-11 17:35:16 - train: epoch 0111, iter [02400, 05004], lr: 0.043590, loss: 2.2490
2022-07-11 17:35:51 - train: epoch 0111, iter [02500, 05004], lr: 0.043574, loss: 2.2689
2022-07-11 17:36:24 - train: epoch 0111, iter [02600, 05004], lr: 0.043558, loss: 1.9578
2022-07-11 17:36:59 - train: epoch 0111, iter [02700, 05004], lr: 0.043542, loss: 2.4966
2022-07-11 17:37:33 - train: epoch 0111, iter [02800, 05004], lr: 0.043526, loss: 2.0612
2022-07-11 17:38:07 - train: epoch 0111, iter [02900, 05004], lr: 0.043510, loss: 1.9634
2022-07-11 17:38:41 - train: epoch 0111, iter [03000, 05004], lr: 0.043494, loss: 2.0964
2022-07-11 17:39:15 - train: epoch 0111, iter [03100, 05004], lr: 0.043478, loss: 2.2828
2022-07-11 17:39:49 - train: epoch 0111, iter [03200, 05004], lr: 0.043462, loss: 2.2691
2022-07-11 17:40:23 - train: epoch 0111, iter [03300, 05004], lr: 0.043446, loss: 2.2992
2022-07-11 17:40:57 - train: epoch 0111, iter [03400, 05004], lr: 0.043430, loss: 2.3601
2022-07-11 17:41:31 - train: epoch 0111, iter [03500, 05004], lr: 0.043414, loss: 2.0700
2022-07-11 17:42:05 - train: epoch 0111, iter [03600, 05004], lr: 0.043398, loss: 1.8840
2022-07-11 17:42:38 - train: epoch 0111, iter [03700, 05004], lr: 0.043382, loss: 2.2006
2022-07-11 17:43:13 - train: epoch 0111, iter [03800, 05004], lr: 0.043366, loss: 2.0975
2022-07-11 17:43:46 - train: epoch 0111, iter [03900, 05004], lr: 0.043350, loss: 2.0265
2022-07-11 17:44:20 - train: epoch 0111, iter [04000, 05004], lr: 0.043334, loss: 2.1203
2022-07-11 17:44:54 - train: epoch 0111, iter [04100, 05004], lr: 0.043319, loss: 2.2610
2022-07-11 17:45:29 - train: epoch 0111, iter [04200, 05004], lr: 0.043303, loss: 2.2360
2022-07-11 17:46:02 - train: epoch 0111, iter [04300, 05004], lr: 0.043287, loss: 2.0697
2022-07-11 17:46:36 - train: epoch 0111, iter [04400, 05004], lr: 0.043271, loss: 2.1767
2022-07-11 17:47:10 - train: epoch 0111, iter [04500, 05004], lr: 0.043255, loss: 2.3817
2022-07-11 17:47:44 - train: epoch 0111, iter [04600, 05004], lr: 0.043239, loss: 2.0699
2022-07-11 17:48:17 - train: epoch 0111, iter [04700, 05004], lr: 0.043223, loss: 2.1281
2022-07-11 17:48:51 - train: epoch 0111, iter [04800, 05004], lr: 0.043207, loss: 1.9590
2022-07-11 17:49:26 - train: epoch 0111, iter [04900, 05004], lr: 0.043191, loss: 2.4014
2022-07-11 17:49:58 - train: epoch 0111, iter [05000, 05004], lr: 0.043175, loss: 2.2143
2022-07-11 17:49:59 - train: epoch 111, train_loss: 2.1487
2022-07-11 17:51:13 - eval: epoch: 111, acc1: 64.122%, acc5: 86.208%, test_loss: 1.4764, per_image_load_time: 2.232ms, per_image_inference_time: 0.452ms
2022-07-11 17:51:14 - until epoch: 111, best_acc1: 65.284%
2022-07-11 17:51:14 - epoch 112 lr: 0.043174
2022-07-11 17:51:53 - train: epoch 0112, iter [00100, 05004], lr: 0.043158, loss: 2.1809
2022-07-11 17:52:28 - train: epoch 0112, iter [00200, 05004], lr: 0.043142, loss: 1.8561
2022-07-11 17:53:02 - train: epoch 0112, iter [00300, 05004], lr: 0.043126, loss: 1.9241
2022-07-11 17:53:36 - train: epoch 0112, iter [00400, 05004], lr: 0.043111, loss: 2.1909
2022-07-11 17:54:10 - train: epoch 0112, iter [00500, 05004], lr: 0.043095, loss: 2.2874
2022-07-11 17:54:44 - train: epoch 0112, iter [00600, 05004], lr: 0.043079, loss: 2.0721
2022-07-11 17:55:18 - train: epoch 0112, iter [00700, 05004], lr: 0.043063, loss: 2.3164
2022-07-11 17:55:52 - train: epoch 0112, iter [00800, 05004], lr: 0.043047, loss: 1.9395
2022-07-11 17:56:27 - train: epoch 0112, iter [00900, 05004], lr: 0.043031, loss: 1.7850
2022-07-11 17:57:00 - train: epoch 0112, iter [01000, 05004], lr: 0.043015, loss: 1.8602
2022-07-11 17:57:35 - train: epoch 0112, iter [01100, 05004], lr: 0.042999, loss: 2.0267
2022-07-11 17:58:10 - train: epoch 0112, iter [01200, 05004], lr: 0.042983, loss: 2.2559
2022-07-11 17:58:43 - train: epoch 0112, iter [01300, 05004], lr: 0.042967, loss: 2.3671
2022-07-11 17:59:17 - train: epoch 0112, iter [01400, 05004], lr: 0.042951, loss: 2.2225
2022-07-11 17:59:51 - train: epoch 0112, iter [01500, 05004], lr: 0.042935, loss: 2.2829
2022-07-11 18:00:26 - train: epoch 0112, iter [01600, 05004], lr: 0.042919, loss: 2.4077
2022-07-11 18:01:00 - train: epoch 0112, iter [01700, 05004], lr: 0.042903, loss: 1.8096
2022-07-11 18:01:34 - train: epoch 0112, iter [01800, 05004], lr: 0.042887, loss: 1.8853
2022-07-11 18:02:08 - train: epoch 0112, iter [01900, 05004], lr: 0.042871, loss: 1.8646
2022-07-11 18:02:44 - train: epoch 0112, iter [02000, 05004], lr: 0.042856, loss: 2.0822
2022-07-11 18:03:18 - train: epoch 0112, iter [02100, 05004], lr: 0.042840, loss: 2.2033
2022-07-11 18:03:53 - train: epoch 0112, iter [02200, 05004], lr: 0.042824, loss: 1.9746
2022-07-11 18:04:27 - train: epoch 0112, iter [02300, 05004], lr: 0.042808, loss: 2.1075
2022-07-11 18:05:02 - train: epoch 0112, iter [02400, 05004], lr: 0.042792, loss: 1.9588
2022-07-11 18:05:36 - train: epoch 0112, iter [02500, 05004], lr: 0.042776, loss: 2.2789
2022-07-11 18:06:11 - train: epoch 0112, iter [02600, 05004], lr: 0.042760, loss: 2.2743
2022-07-11 18:06:45 - train: epoch 0112, iter [02700, 05004], lr: 0.042744, loss: 2.1936
2022-07-11 18:07:20 - train: epoch 0112, iter [02800, 05004], lr: 0.042728, loss: 1.9530
2022-07-11 18:07:53 - train: epoch 0112, iter [02900, 05004], lr: 0.042712, loss: 2.5151
2022-07-11 18:08:29 - train: epoch 0112, iter [03000, 05004], lr: 0.042696, loss: 2.3422
2022-07-11 18:09:04 - train: epoch 0112, iter [03100, 05004], lr: 0.042680, loss: 1.8958
2022-07-11 18:09:39 - train: epoch 0112, iter [03200, 05004], lr: 0.042664, loss: 2.2608
2022-07-11 18:10:12 - train: epoch 0112, iter [03300, 05004], lr: 0.042648, loss: 2.1403
2022-07-11 18:10:47 - train: epoch 0112, iter [03400, 05004], lr: 0.042633, loss: 2.1962
2022-07-11 18:11:21 - train: epoch 0112, iter [03500, 05004], lr: 0.042617, loss: 2.2584
2022-07-11 18:11:56 - train: epoch 0112, iter [03600, 05004], lr: 0.042601, loss: 2.0574
2022-07-11 18:12:29 - train: epoch 0112, iter [03700, 05004], lr: 0.042585, loss: 2.2455
2022-07-11 18:13:05 - train: epoch 0112, iter [03800, 05004], lr: 0.042569, loss: 2.1838
2022-07-11 18:13:38 - train: epoch 0112, iter [03900, 05004], lr: 0.042553, loss: 2.2066
2022-07-11 18:14:14 - train: epoch 0112, iter [04000, 05004], lr: 0.042537, loss: 1.9974
2022-07-11 18:14:48 - train: epoch 0112, iter [04100, 05004], lr: 0.042521, loss: 2.2781
2022-07-11 18:15:22 - train: epoch 0112, iter [04200, 05004], lr: 0.042505, loss: 2.0452
2022-07-11 18:15:56 - train: epoch 0112, iter [04300, 05004], lr: 0.042489, loss: 2.2837
2022-07-11 18:16:30 - train: epoch 0112, iter [04400, 05004], lr: 0.042473, loss: 2.5532
2022-07-11 18:17:05 - train: epoch 0112, iter [04500, 05004], lr: 0.042457, loss: 1.9240
2022-07-11 18:17:39 - train: epoch 0112, iter [04600, 05004], lr: 0.042442, loss: 2.0379
2022-07-11 18:18:13 - train: epoch 0112, iter [04700, 05004], lr: 0.042426, loss: 1.9966
2022-07-11 18:18:48 - train: epoch 0112, iter [04800, 05004], lr: 0.042410, loss: 2.4274
2022-07-11 18:19:22 - train: epoch 0112, iter [04900, 05004], lr: 0.042394, loss: 2.0974
2022-07-11 18:19:55 - train: epoch 0112, iter [05000, 05004], lr: 0.042378, loss: 2.0645
2022-07-11 18:19:57 - train: epoch 112, train_loss: 2.1363
2022-07-11 18:21:11 - eval: epoch: 112, acc1: 65.046%, acc5: 87.018%, test_loss: 1.4314, per_image_load_time: 1.913ms, per_image_inference_time: 0.467ms
2022-07-11 18:21:11 - until epoch: 112, best_acc1: 65.284%
2022-07-11 18:21:11 - epoch 113 lr: 0.042377
2022-07-11 18:21:50 - train: epoch 0113, iter [00100, 05004], lr: 0.042361, loss: 2.4094
2022-07-11 18:22:24 - train: epoch 0113, iter [00200, 05004], lr: 0.042345, loss: 1.9929
2022-07-11 18:22:59 - train: epoch 0113, iter [00300, 05004], lr: 0.042330, loss: 2.1799
2022-07-11 18:23:33 - train: epoch 0113, iter [00400, 05004], lr: 0.042314, loss: 1.9168
2022-07-11 18:24:07 - train: epoch 0113, iter [00500, 05004], lr: 0.042298, loss: 2.0681
2022-07-11 18:24:42 - train: epoch 0113, iter [00600, 05004], lr: 0.042282, loss: 2.1802
2022-07-11 18:25:16 - train: epoch 0113, iter [00700, 05004], lr: 0.042266, loss: 2.1274
2022-07-11 18:25:50 - train: epoch 0113, iter [00800, 05004], lr: 0.042250, loss: 2.0537
2022-07-11 18:26:25 - train: epoch 0113, iter [00900, 05004], lr: 0.042234, loss: 2.0389
2022-07-11 18:27:00 - train: epoch 0113, iter [01000, 05004], lr: 0.042218, loss: 2.3312
2022-07-11 18:27:33 - train: epoch 0113, iter [01100, 05004], lr: 0.042202, loss: 2.3177
2022-07-11 18:28:08 - train: epoch 0113, iter [01200, 05004], lr: 0.042186, loss: 2.2145
2022-07-11 18:28:43 - train: epoch 0113, iter [01300, 05004], lr: 0.042170, loss: 1.9647
2022-07-11 18:29:16 - train: epoch 0113, iter [01400, 05004], lr: 0.042155, loss: 2.0967
2022-07-11 18:29:50 - train: epoch 0113, iter [01500, 05004], lr: 0.042139, loss: 2.0974
2022-07-11 18:30:24 - train: epoch 0113, iter [01600, 05004], lr: 0.042123, loss: 1.9180
2022-07-11 18:30:59 - train: epoch 0113, iter [01700, 05004], lr: 0.042107, loss: 1.9866
2022-07-11 18:31:32 - train: epoch 0113, iter [01800, 05004], lr: 0.042091, loss: 2.3354
2022-07-11 18:32:08 - train: epoch 0113, iter [01900, 05004], lr: 0.042075, loss: 2.0933
2022-07-11 18:32:41 - train: epoch 0113, iter [02000, 05004], lr: 0.042059, loss: 2.1093
2022-07-11 18:33:15 - train: epoch 0113, iter [02100, 05004], lr: 0.042043, loss: 2.1040
2022-07-11 18:33:49 - train: epoch 0113, iter [02200, 05004], lr: 0.042027, loss: 2.2591
2022-07-11 18:34:22 - train: epoch 0113, iter [02300, 05004], lr: 0.042012, loss: 1.9418
2022-07-11 18:34:56 - train: epoch 0113, iter [02400, 05004], lr: 0.041996, loss: 2.1542
2022-07-11 18:35:29 - train: epoch 0113, iter [02500, 05004], lr: 0.041980, loss: 2.1060
2022-07-11 18:36:03 - train: epoch 0113, iter [02600, 05004], lr: 0.041964, loss: 1.8981
2022-07-11 18:36:37 - train: epoch 0113, iter [02700, 05004], lr: 0.041948, loss: 2.2320
2022-07-11 18:37:10 - train: epoch 0113, iter [02800, 05004], lr: 0.041932, loss: 1.9455
2022-07-11 18:37:45 - train: epoch 0113, iter [02900, 05004], lr: 0.041916, loss: 2.0576
2022-07-11 18:38:18 - train: epoch 0113, iter [03000, 05004], lr: 0.041900, loss: 2.0439
2022-07-11 18:38:52 - train: epoch 0113, iter [03100, 05004], lr: 0.041884, loss: 1.9581
2022-07-11 18:39:26 - train: epoch 0113, iter [03200, 05004], lr: 0.041869, loss: 1.9323
2022-07-11 18:40:00 - train: epoch 0113, iter [03300, 05004], lr: 0.041853, loss: 2.1850
2022-07-11 18:40:33 - train: epoch 0113, iter [03400, 05004], lr: 0.041837, loss: 2.0883
2022-07-11 18:41:07 - train: epoch 0113, iter [03500, 05004], lr: 0.041821, loss: 2.3014
2022-07-11 18:41:40 - train: epoch 0113, iter [03600, 05004], lr: 0.041805, loss: 2.2274
2022-07-11 18:42:16 - train: epoch 0113, iter [03700, 05004], lr: 0.041789, loss: 1.8795
2022-07-11 18:42:49 - train: epoch 0113, iter [03800, 05004], lr: 0.041773, loss: 2.3295
2022-07-11 18:43:24 - train: epoch 0113, iter [03900, 05004], lr: 0.041757, loss: 1.9920
2022-07-11 18:43:57 - train: epoch 0113, iter [04000, 05004], lr: 0.041742, loss: 1.8679
2022-07-11 18:44:31 - train: epoch 0113, iter [04100, 05004], lr: 0.041726, loss: 2.2742
2022-07-11 18:45:05 - train: epoch 0113, iter [04200, 05004], lr: 0.041710, loss: 2.1675
2022-07-11 18:45:39 - train: epoch 0113, iter [04300, 05004], lr: 0.041694, loss: 2.1547
2022-07-11 18:46:12 - train: epoch 0113, iter [04400, 05004], lr: 0.041678, loss: 2.1354
2022-07-11 18:46:46 - train: epoch 0113, iter [04500, 05004], lr: 0.041662, loss: 2.1122
2022-07-11 18:47:20 - train: epoch 0113, iter [04600, 05004], lr: 0.041646, loss: 2.0500
2022-07-11 18:47:55 - train: epoch 0113, iter [04700, 05004], lr: 0.041630, loss: 2.4184
2022-07-11 18:48:29 - train: epoch 0113, iter [04800, 05004], lr: 0.041615, loss: 2.3124
2022-07-11 18:49:04 - train: epoch 0113, iter [04900, 05004], lr: 0.041599, loss: 1.9266
2022-07-11 18:49:37 - train: epoch 0113, iter [05000, 05004], lr: 0.041583, loss: 2.2039
2022-07-11 18:49:38 - train: epoch 113, train_loss: 2.1293
2022-07-11 18:50:52 - eval: epoch: 113, acc1: 63.990%, acc5: 86.258%, test_loss: 1.4755, per_image_load_time: 2.454ms, per_image_inference_time: 0.440ms
2022-07-11 18:50:53 - until epoch: 113, best_acc1: 65.284%
2022-07-11 18:50:53 - epoch 114 lr: 0.041582
2022-07-11 18:51:32 - train: epoch 0114, iter [00100, 05004], lr: 0.041566, loss: 2.1941
2022-07-11 18:52:06 - train: epoch 0114, iter [00200, 05004], lr: 0.041550, loss: 1.8816
2022-07-11 18:52:40 - train: epoch 0114, iter [00300, 05004], lr: 0.041535, loss: 2.2103
2022-07-11 18:53:14 - train: epoch 0114, iter [00400, 05004], lr: 0.041519, loss: 2.1391
2022-07-11 18:53:47 - train: epoch 0114, iter [00500, 05004], lr: 0.041503, loss: 2.0713
2022-07-11 18:54:21 - train: epoch 0114, iter [00600, 05004], lr: 0.041487, loss: 2.0965
2022-07-11 18:54:54 - train: epoch 0114, iter [00700, 05004], lr: 0.041471, loss: 2.1455
2022-07-11 18:55:28 - train: epoch 0114, iter [00800, 05004], lr: 0.041455, loss: 2.3542
2022-07-11 18:56:02 - train: epoch 0114, iter [00900, 05004], lr: 0.041439, loss: 1.9006
2022-07-11 18:56:36 - train: epoch 0114, iter [01000, 05004], lr: 0.041424, loss: 2.0781
2022-07-11 18:57:09 - train: epoch 0114, iter [01100, 05004], lr: 0.041408, loss: 2.2032
2022-07-11 18:57:43 - train: epoch 0114, iter [01200, 05004], lr: 0.041392, loss: 2.1953
2022-07-11 18:58:17 - train: epoch 0114, iter [01300, 05004], lr: 0.041376, loss: 1.8128
2022-07-11 18:58:51 - train: epoch 0114, iter [01400, 05004], lr: 0.041360, loss: 2.2584
2022-07-11 18:59:25 - train: epoch 0114, iter [01500, 05004], lr: 0.041344, loss: 2.2183
2022-07-11 18:59:58 - train: epoch 0114, iter [01600, 05004], lr: 0.041328, loss: 1.8574
2022-07-11 19:00:32 - train: epoch 0114, iter [01700, 05004], lr: 0.041313, loss: 1.9700
2022-07-11 19:01:06 - train: epoch 0114, iter [01800, 05004], lr: 0.041297, loss: 2.2246
2022-07-11 19:01:40 - train: epoch 0114, iter [01900, 05004], lr: 0.041281, loss: 2.0570
2022-07-11 19:02:14 - train: epoch 0114, iter [02000, 05004], lr: 0.041265, loss: 2.0504
2022-07-11 19:02:48 - train: epoch 0114, iter [02100, 05004], lr: 0.041249, loss: 2.0089
2022-07-11 19:03:22 - train: epoch 0114, iter [02200, 05004], lr: 0.041233, loss: 2.0480
2022-07-11 19:03:56 - train: epoch 0114, iter [02300, 05004], lr: 0.041217, loss: 2.3948
2022-07-11 19:04:30 - train: epoch 0114, iter [02400, 05004], lr: 0.041202, loss: 2.2167
2022-07-11 19:05:05 - train: epoch 0114, iter [02500, 05004], lr: 0.041186, loss: 2.1997
2022-07-11 19:05:38 - train: epoch 0114, iter [02600, 05004], lr: 0.041170, loss: 2.1977
2022-07-11 19:06:12 - train: epoch 0114, iter [02700, 05004], lr: 0.041154, loss: 2.1751
2022-07-11 19:06:46 - train: epoch 0114, iter [02800, 05004], lr: 0.041138, loss: 2.1089
2022-07-11 19:07:19 - train: epoch 0114, iter [02900, 05004], lr: 0.041122, loss: 1.8528
2022-07-11 19:07:53 - train: epoch 0114, iter [03000, 05004], lr: 0.041107, loss: 1.9170
2022-07-11 19:08:27 - train: epoch 0114, iter [03100, 05004], lr: 0.041091, loss: 2.1661
2022-07-11 19:09:01 - train: epoch 0114, iter [03200, 05004], lr: 0.041075, loss: 2.0841
2022-07-11 19:09:35 - train: epoch 0114, iter [03300, 05004], lr: 0.041059, loss: 2.1028
2022-07-11 19:10:10 - train: epoch 0114, iter [03400, 05004], lr: 0.041043, loss: 2.1307
2022-07-11 19:10:45 - train: epoch 0114, iter [03500, 05004], lr: 0.041027, loss: 2.0609
2022-07-11 19:11:18 - train: epoch 0114, iter [03600, 05004], lr: 0.041011, loss: 2.0657
2022-07-11 19:11:52 - train: epoch 0114, iter [03700, 05004], lr: 0.040996, loss: 2.0025
2022-07-11 19:12:26 - train: epoch 0114, iter [03800, 05004], lr: 0.040980, loss: 2.1391
2022-07-11 19:13:00 - train: epoch 0114, iter [03900, 05004], lr: 0.040964, loss: 2.1881
2022-07-11 19:13:34 - train: epoch 0114, iter [04000, 05004], lr: 0.040948, loss: 1.7495
2022-07-11 19:14:08 - train: epoch 0114, iter [04100, 05004], lr: 0.040932, loss: 1.8975
2022-07-11 19:14:42 - train: epoch 0114, iter [04200, 05004], lr: 0.040916, loss: 2.2170
2022-07-11 19:15:16 - train: epoch 0114, iter [04300, 05004], lr: 0.040901, loss: 2.3058
2022-07-11 19:15:49 - train: epoch 0114, iter [04400, 05004], lr: 0.040885, loss: 2.3168
2022-07-11 19:16:23 - train: epoch 0114, iter [04500, 05004], lr: 0.040869, loss: 2.3514
2022-07-11 19:16:57 - train: epoch 0114, iter [04600, 05004], lr: 0.040853, loss: 2.0693
2022-07-11 19:17:32 - train: epoch 0114, iter [04700, 05004], lr: 0.040837, loss: 2.5843
2022-07-11 19:18:06 - train: epoch 0114, iter [04800, 05004], lr: 0.040822, loss: 2.2174
2022-07-11 19:18:39 - train: epoch 0114, iter [04900, 05004], lr: 0.040806, loss: 2.0790
2022-07-11 19:19:12 - train: epoch 0114, iter [05000, 05004], lr: 0.040790, loss: 2.1628
2022-07-11 19:19:14 - train: epoch 114, train_loss: 2.1215
2022-07-11 19:20:28 - eval: epoch: 114, acc1: 64.040%, acc5: 86.236%, test_loss: 1.4796, per_image_load_time: 0.674ms, per_image_inference_time: 0.448ms
2022-07-11 19:20:28 - until epoch: 114, best_acc1: 65.284%
2022-07-11 19:20:28 - epoch 115 lr: 0.040789
2022-07-11 19:21:07 - train: epoch 0115, iter [00100, 05004], lr: 0.040773, loss: 1.9751
2022-07-11 19:21:41 - train: epoch 0115, iter [00200, 05004], lr: 0.040758, loss: 1.9180
2022-07-11 19:22:15 - train: epoch 0115, iter [00300, 05004], lr: 0.040742, loss: 2.2463
2022-07-11 19:22:49 - train: epoch 0115, iter [00400, 05004], lr: 0.040726, loss: 1.9129
2022-07-11 19:23:22 - train: epoch 0115, iter [00500, 05004], lr: 0.040710, loss: 2.0544
2022-07-11 19:23:56 - train: epoch 0115, iter [00600, 05004], lr: 0.040694, loss: 1.9465
2022-07-11 19:24:29 - train: epoch 0115, iter [00700, 05004], lr: 0.040679, loss: 2.3979
2022-07-11 19:25:03 - train: epoch 0115, iter [00800, 05004], lr: 0.040663, loss: 2.1108
2022-07-11 19:25:37 - train: epoch 0115, iter [00900, 05004], lr: 0.040647, loss: 2.2295
2022-07-11 19:26:11 - train: epoch 0115, iter [01000, 05004], lr: 0.040631, loss: 2.1069
2022-07-11 19:26:45 - train: epoch 0115, iter [01100, 05004], lr: 0.040615, loss: 1.9047
2022-07-11 19:27:19 - train: epoch 0115, iter [01200, 05004], lr: 0.040599, loss: 1.9826
2022-07-11 19:27:53 - train: epoch 0115, iter [01300, 05004], lr: 0.040584, loss: 2.1570
2022-07-11 19:28:26 - train: epoch 0115, iter [01400, 05004], lr: 0.040568, loss: 1.8212
2022-07-11 19:29:00 - train: epoch 0115, iter [01500, 05004], lr: 0.040552, loss: 2.2152
2022-07-11 19:29:34 - train: epoch 0115, iter [01600, 05004], lr: 0.040536, loss: 2.0889
2022-07-11 19:30:09 - train: epoch 0115, iter [01700, 05004], lr: 0.040520, loss: 2.0956
2022-07-11 19:30:43 - train: epoch 0115, iter [01800, 05004], lr: 0.040505, loss: 2.2153
2022-07-11 19:31:16 - train: epoch 0115, iter [01900, 05004], lr: 0.040489, loss: 2.2262
2022-07-11 19:31:51 - train: epoch 0115, iter [02000, 05004], lr: 0.040473, loss: 2.1424
2022-07-11 19:32:25 - train: epoch 0115, iter [02100, 05004], lr: 0.040457, loss: 1.9938
2022-07-11 19:32:59 - train: epoch 0115, iter [02200, 05004], lr: 0.040441, loss: 2.1697
2022-07-11 19:33:33 - train: epoch 0115, iter [02300, 05004], lr: 0.040426, loss: 2.1458
2022-07-11 19:34:07 - train: epoch 0115, iter [02400, 05004], lr: 0.040410, loss: 2.2349
2022-07-11 19:34:41 - train: epoch 0115, iter [02500, 05004], lr: 0.040394, loss: 1.7782
2022-07-11 19:35:14 - train: epoch 0115, iter [02600, 05004], lr: 0.040378, loss: 1.9740
2022-07-11 19:35:48 - train: epoch 0115, iter [02700, 05004], lr: 0.040362, loss: 1.8210
2022-07-11 19:36:22 - train: epoch 0115, iter [02800, 05004], lr: 0.040347, loss: 2.2562
2022-07-11 19:36:55 - train: epoch 0115, iter [02900, 05004], lr: 0.040331, loss: 2.0893
2022-07-11 19:37:29 - train: epoch 0115, iter [03000, 05004], lr: 0.040315, loss: 1.9244
2022-07-11 19:38:03 - train: epoch 0115, iter [03100, 05004], lr: 0.040299, loss: 2.3969
2022-07-11 19:38:37 - train: epoch 0115, iter [03200, 05004], lr: 0.040283, loss: 2.1463
2022-07-11 19:39:10 - train: epoch 0115, iter [03300, 05004], lr: 0.040268, loss: 2.0197
2022-07-11 19:39:45 - train: epoch 0115, iter [03400, 05004], lr: 0.040252, loss: 2.2521
2022-07-11 19:40:18 - train: epoch 0115, iter [03500, 05004], lr: 0.040236, loss: 2.1016
2022-07-11 19:40:53 - train: epoch 0115, iter [03600, 05004], lr: 0.040220, loss: 2.1007
2022-07-11 19:41:26 - train: epoch 0115, iter [03700, 05004], lr: 0.040204, loss: 2.1517
2022-07-11 19:42:00 - train: epoch 0115, iter [03800, 05004], lr: 0.040189, loss: 2.1401
2022-07-11 19:42:34 - train: epoch 0115, iter [03900, 05004], lr: 0.040173, loss: 2.1652
2022-07-11 19:43:08 - train: epoch 0115, iter [04000, 05004], lr: 0.040157, loss: 2.1016
2022-07-11 19:43:42 - train: epoch 0115, iter [04100, 05004], lr: 0.040141, loss: 2.0142
2022-07-11 19:44:16 - train: epoch 0115, iter [04200, 05004], lr: 0.040126, loss: 2.0429
2022-07-11 19:44:50 - train: epoch 0115, iter [04300, 05004], lr: 0.040110, loss: 1.8338
2022-07-11 19:45:24 - train: epoch 0115, iter [04400, 05004], lr: 0.040094, loss: 2.2882
2022-07-11 19:45:57 - train: epoch 0115, iter [04500, 05004], lr: 0.040078, loss: 1.8313
2022-07-11 19:46:31 - train: epoch 0115, iter [04600, 05004], lr: 0.040062, loss: 2.1279
2022-07-11 19:47:05 - train: epoch 0115, iter [04700, 05004], lr: 0.040047, loss: 2.0141
2022-07-11 19:47:39 - train: epoch 0115, iter [04800, 05004], lr: 0.040031, loss: 2.0850
2022-07-11 19:48:12 - train: epoch 0115, iter [04900, 05004], lr: 0.040015, loss: 2.2460
2022-07-11 19:48:45 - train: epoch 0115, iter [05000, 05004], lr: 0.039999, loss: 2.3344
2022-07-11 19:48:46 - train: epoch 115, train_loss: 2.1190
2022-07-11 19:50:00 - eval: epoch: 115, acc1: 65.826%, acc5: 87.144%, test_loss: 1.4114, per_image_load_time: 2.291ms, per_image_inference_time: 0.460ms
2022-07-11 19:50:00 - until epoch: 115, best_acc1: 65.826%
2022-07-11 19:50:00 - epoch 116 lr: 0.039999
2022-07-11 19:50:40 - train: epoch 0116, iter [00100, 05004], lr: 0.039983, loss: 1.9357
2022-07-11 19:51:14 - train: epoch 0116, iter [00200, 05004], lr: 0.039967, loss: 2.0977
2022-07-11 19:51:48 - train: epoch 0116, iter [00300, 05004], lr: 0.039951, loss: 1.9618
2022-07-11 19:52:21 - train: epoch 0116, iter [00400, 05004], lr: 0.039936, loss: 2.0811
2022-07-11 19:52:55 - train: epoch 0116, iter [00500, 05004], lr: 0.039920, loss: 2.1951
2022-07-11 19:53:29 - train: epoch 0116, iter [00600, 05004], lr: 0.039904, loss: 2.1152
2022-07-11 19:54:02 - train: epoch 0116, iter [00700, 05004], lr: 0.039888, loss: 2.2069
2022-07-11 19:54:35 - train: epoch 0116, iter [00800, 05004], lr: 0.039873, loss: 2.1608
2022-07-11 19:55:09 - train: epoch 0116, iter [00900, 05004], lr: 0.039857, loss: 2.2029
2022-07-11 19:55:43 - train: epoch 0116, iter [01000, 05004], lr: 0.039841, loss: 2.0262
2022-07-11 19:56:15 - train: epoch 0116, iter [01100, 05004], lr: 0.039825, loss: 1.8584
2022-07-11 19:56:49 - train: epoch 0116, iter [01200, 05004], lr: 0.039810, loss: 2.3049
2022-07-11 19:57:23 - train: epoch 0116, iter [01300, 05004], lr: 0.039794, loss: 1.8874
2022-07-11 19:57:56 - train: epoch 0116, iter [01400, 05004], lr: 0.039778, loss: 2.2653
2022-07-11 19:58:30 - train: epoch 0116, iter [01500, 05004], lr: 0.039762, loss: 1.9222
2022-07-11 19:59:04 - train: epoch 0116, iter [01600, 05004], lr: 0.039746, loss: 1.9331
2022-07-11 19:59:37 - train: epoch 0116, iter [01700, 05004], lr: 0.039731, loss: 1.6012
2022-07-11 20:00:10 - train: epoch 0116, iter [01800, 05004], lr: 0.039715, loss: 2.0395
2022-07-11 20:00:44 - train: epoch 0116, iter [01900, 05004], lr: 0.039699, loss: 1.9906
2022-07-11 20:01:18 - train: epoch 0116, iter [02000, 05004], lr: 0.039683, loss: 1.9981
2022-07-11 20:01:52 - train: epoch 0116, iter [02100, 05004], lr: 0.039668, loss: 2.1726
2022-07-11 20:02:27 - train: epoch 0116, iter [02200, 05004], lr: 0.039652, loss: 2.3305
2022-07-11 20:03:00 - train: epoch 0116, iter [02300, 05004], lr: 0.039636, loss: 2.4285
2022-07-11 20:03:34 - train: epoch 0116, iter [02400, 05004], lr: 0.039620, loss: 2.1123
2022-07-11 20:04:07 - train: epoch 0116, iter [02500, 05004], lr: 0.039605, loss: 1.8834
2022-07-11 20:04:40 - train: epoch 0116, iter [02600, 05004], lr: 0.039589, loss: 2.3545
2022-07-11 20:05:15 - train: epoch 0116, iter [02700, 05004], lr: 0.039573, loss: 2.1748
2022-07-11 20:05:49 - train: epoch 0116, iter [02800, 05004], lr: 0.039557, loss: 2.3578
2022-07-11 20:06:23 - train: epoch 0116, iter [02900, 05004], lr: 0.039542, loss: 2.0690
2022-07-11 20:06:57 - train: epoch 0116, iter [03000, 05004], lr: 0.039526, loss: 2.1556
2022-07-11 20:07:31 - train: epoch 0116, iter [03100, 05004], lr: 0.039510, loss: 2.1604
2022-07-11 20:08:04 - train: epoch 0116, iter [03200, 05004], lr: 0.039495, loss: 2.1908
2022-07-11 20:08:40 - train: epoch 0116, iter [03300, 05004], lr: 0.039479, loss: 2.2376
2022-07-11 20:09:12 - train: epoch 0116, iter [03400, 05004], lr: 0.039463, loss: 2.1209
2022-07-11 20:09:46 - train: epoch 0116, iter [03500, 05004], lr: 0.039447, loss: 2.3118
2022-07-11 20:10:20 - train: epoch 0116, iter [03600, 05004], lr: 0.039432, loss: 2.0172
2022-07-11 20:10:54 - train: epoch 0116, iter [03700, 05004], lr: 0.039416, loss: 2.2591
2022-07-11 20:11:27 - train: epoch 0116, iter [03800, 05004], lr: 0.039400, loss: 1.9418
2022-07-11 20:12:02 - train: epoch 0116, iter [03900, 05004], lr: 0.039384, loss: 2.1595
2022-07-11 20:12:36 - train: epoch 0116, iter [04000, 05004], lr: 0.039369, loss: 2.1016
2022-07-11 20:13:10 - train: epoch 0116, iter [04100, 05004], lr: 0.039353, loss: 2.1501
2022-07-11 20:13:45 - train: epoch 0116, iter [04200, 05004], lr: 0.039337, loss: 2.0506
2022-07-11 20:14:18 - train: epoch 0116, iter [04300, 05004], lr: 0.039321, loss: 1.7607
2022-07-11 20:14:53 - train: epoch 0116, iter [04400, 05004], lr: 0.039306, loss: 2.0892
2022-07-11 20:15:27 - train: epoch 0116, iter [04500, 05004], lr: 0.039290, loss: 1.9328
2022-07-11 20:16:01 - train: epoch 0116, iter [04600, 05004], lr: 0.039274, loss: 2.3577
2022-07-11 20:16:36 - train: epoch 0116, iter [04700, 05004], lr: 0.039259, loss: 2.0511
2022-07-11 20:17:09 - train: epoch 0116, iter [04800, 05004], lr: 0.039243, loss: 1.7713
2022-07-11 20:17:44 - train: epoch 0116, iter [04900, 05004], lr: 0.039227, loss: 2.1112
2022-07-11 20:18:17 - train: epoch 0116, iter [05000, 05004], lr: 0.039211, loss: 1.8328
2022-07-11 20:18:18 - train: epoch 116, train_loss: 2.1049
2022-07-11 20:19:33 - eval: epoch: 116, acc1: 65.244%, acc5: 86.948%, test_loss: 1.4309, per_image_load_time: 2.448ms, per_image_inference_time: 0.473ms
2022-07-11 20:19:33 - until epoch: 116, best_acc1: 65.826%
2022-07-11 20:19:33 - epoch 117 lr: 0.039211
2022-07-11 20:20:14 - train: epoch 0117, iter [00100, 05004], lr: 0.039195, loss: 2.2797
2022-07-11 20:20:47 - train: epoch 0117, iter [00200, 05004], lr: 0.039179, loss: 1.9477
2022-07-11 20:21:21 - train: epoch 0117, iter [00300, 05004], lr: 0.039164, loss: 2.1882
2022-07-11 20:21:56 - train: epoch 0117, iter [00400, 05004], lr: 0.039148, loss: 2.1177
2022-07-11 20:22:30 - train: epoch 0117, iter [00500, 05004], lr: 0.039132, loss: 1.7240
2022-07-11 20:23:04 - train: epoch 0117, iter [00600, 05004], lr: 0.039116, loss: 2.1345
2022-07-11 20:23:38 - train: epoch 0117, iter [00700, 05004], lr: 0.039101, loss: 2.1594
2022-07-11 20:24:12 - train: epoch 0117, iter [00800, 05004], lr: 0.039085, loss: 1.9516
2022-07-11 20:24:47 - train: epoch 0117, iter [00900, 05004], lr: 0.039069, loss: 2.4947
2022-07-11 20:25:21 - train: epoch 0117, iter [01000, 05004], lr: 0.039054, loss: 1.9664
2022-07-11 20:25:56 - train: epoch 0117, iter [01100, 05004], lr: 0.039038, loss: 2.0918
2022-07-11 20:26:29 - train: epoch 0117, iter [01200, 05004], lr: 0.039022, loss: 2.3412
2022-07-11 20:27:03 - train: epoch 0117, iter [01300, 05004], lr: 0.039007, loss: 2.2923
2022-07-11 20:27:38 - train: epoch 0117, iter [01400, 05004], lr: 0.038991, loss: 2.3618
2022-07-11 20:28:12 - train: epoch 0117, iter [01500, 05004], lr: 0.038975, loss: 2.1133
2022-07-11 20:28:46 - train: epoch 0117, iter [01600, 05004], lr: 0.038959, loss: 1.6801
2022-07-11 20:29:22 - train: epoch 0117, iter [01700, 05004], lr: 0.038944, loss: 2.2625
2022-07-11 20:29:55 - train: epoch 0117, iter [01800, 05004], lr: 0.038928, loss: 2.0594
2022-07-11 20:30:29 - train: epoch 0117, iter [01900, 05004], lr: 0.038912, loss: 2.0791
2022-07-11 20:31:04 - train: epoch 0117, iter [02000, 05004], lr: 0.038897, loss: 2.3935
2022-07-11 20:31:38 - train: epoch 0117, iter [02100, 05004], lr: 0.038881, loss: 1.9641
2022-07-11 20:32:13 - train: epoch 0117, iter [02200, 05004], lr: 0.038865, loss: 1.8934
2022-07-11 20:32:48 - train: epoch 0117, iter [02300, 05004], lr: 0.038850, loss: 2.1194
2022-07-11 20:33:22 - train: epoch 0117, iter [02400, 05004], lr: 0.038834, loss: 1.9221
2022-07-11 20:33:56 - train: epoch 0117, iter [02500, 05004], lr: 0.038818, loss: 2.0901
2022-07-11 20:34:30 - train: epoch 0117, iter [02600, 05004], lr: 0.038802, loss: 2.1920
2022-07-11 20:35:05 - train: epoch 0117, iter [02700, 05004], lr: 0.038787, loss: 2.3106
2022-07-11 20:35:39 - train: epoch 0117, iter [02800, 05004], lr: 0.038771, loss: 1.9007
2022-07-11 20:36:13 - train: epoch 0117, iter [02900, 05004], lr: 0.038755, loss: 2.1612
2022-07-11 20:36:47 - train: epoch 0117, iter [03000, 05004], lr: 0.038740, loss: 2.1699
2022-07-11 20:37:22 - train: epoch 0117, iter [03100, 05004], lr: 0.038724, loss: 1.9719
2022-07-11 20:37:56 - train: epoch 0117, iter [03200, 05004], lr: 0.038708, loss: 1.9626
2022-07-11 20:38:31 - train: epoch 0117, iter [03300, 05004], lr: 0.038693, loss: 2.2389
2022-07-11 20:39:04 - train: epoch 0117, iter [03400, 05004], lr: 0.038677, loss: 2.1314
2022-07-11 20:39:39 - train: epoch 0117, iter [03500, 05004], lr: 0.038661, loss: 2.3135
2022-07-11 20:40:13 - train: epoch 0117, iter [03600, 05004], lr: 0.038646, loss: 2.0174
2022-07-11 20:40:47 - train: epoch 0117, iter [03700, 05004], lr: 0.038630, loss: 2.2381
2022-07-11 20:41:22 - train: epoch 0117, iter [03800, 05004], lr: 0.038614, loss: 2.3409
2022-07-11 20:41:57 - train: epoch 0117, iter [03900, 05004], lr: 0.038599, loss: 1.8749
2022-07-11 20:42:31 - train: epoch 0117, iter [04000, 05004], lr: 0.038583, loss: 1.9657
2022-07-11 20:43:05 - train: epoch 0117, iter [04100, 05004], lr: 0.038567, loss: 1.9588
2022-07-11 20:43:40 - train: epoch 0117, iter [04200, 05004], lr: 0.038552, loss: 2.3707
2022-07-11 20:44:14 - train: epoch 0117, iter [04300, 05004], lr: 0.038536, loss: 1.9919
2022-07-11 20:44:48 - train: epoch 0117, iter [04400, 05004], lr: 0.038520, loss: 2.0729
2022-07-11 20:45:22 - train: epoch 0117, iter [04500, 05004], lr: 0.038505, loss: 2.2060
2022-07-11 20:45:56 - train: epoch 0117, iter [04600, 05004], lr: 0.038489, loss: 2.2562
2022-07-11 20:46:30 - train: epoch 0117, iter [04700, 05004], lr: 0.038473, loss: 2.1166
2022-07-11 20:47:04 - train: epoch 0117, iter [04800, 05004], lr: 0.038458, loss: 2.0541
2022-07-11 20:47:38 - train: epoch 0117, iter [04900, 05004], lr: 0.038442, loss: 1.9866
2022-07-11 20:48:11 - train: epoch 0117, iter [05000, 05004], lr: 0.038426, loss: 2.2619
2022-07-11 20:48:12 - train: epoch 117, train_loss: 2.1024
2022-07-11 20:49:27 - eval: epoch: 117, acc1: 66.242%, acc5: 87.718%, test_loss: 1.3821, per_image_load_time: 1.206ms, per_image_inference_time: 0.459ms
2022-07-11 20:49:28 - until epoch: 117, best_acc1: 66.242%
2022-07-11 20:49:28 - epoch 118 lr: 0.038426
