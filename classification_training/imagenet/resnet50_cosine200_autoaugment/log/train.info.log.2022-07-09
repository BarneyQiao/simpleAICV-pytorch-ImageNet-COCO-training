2022-07-09 10:08:20 - network: resnet50
2022-07-09 10:08:20 - num_classes: 1000
2022-07-09 10:08:20 - input_image_size: 224
2022-07-09 10:08:20 - scale: 1.1428571428571428
2022-07-09 10:08:20 - trained_model_path: 
2022-07-09 10:08:20 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-09 10:08:20 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-09 10:08:20 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f05ad594e20>
2022-07-09 10:08:20 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f058f047130>
2022-07-09 10:08:20 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f058f047160>
2022-07-09 10:08:20 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f058f0471c0>
2022-07-09 10:08:20 - seed: 0
2022-07-09 10:08:20 - batch_size: 256
2022-07-09 10:08:20 - num_workers: 16
2022-07-09 10:08:20 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-07-09 10:08:20 - scheduler: ('CosineLR', {'warm_up_epochs': 5})
2022-07-09 10:08:20 - epochs: 200
2022-07-09 10:08:20 - print_interval: 100
2022-07-09 10:08:20 - accumulation_steps: 1
2022-07-09 10:08:20 - sync_bn: False
2022-07-09 10:08:20 - apex: True
2022-07-09 10:08:20 - use_ema_model: False
2022-07-09 10:08:20 - ema_model_decay: 0.9999
2022-07-09 10:08:20 - gpus_type: NVIDIA RTX A5000
2022-07-09 10:08:20 - gpus_num: 2
2022-07-09 10:08:20 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f058bd0fe30>
2022-07-09 10:08:20 - --------------------parameters--------------------
2022-07-09 10:08:20 - name: conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-07-09 10:08:20 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-07-09 10:08:20 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-07-09 10:08:20 - name: fc.weight, grad: True
2022-07-09 10:08:20 - name: fc.bias, grad: True
2022-07-09 10:08:20 - --------------------buffers--------------------
2022-07-09 10:08:20 - name: conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-07-09 10:08:20 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-07-09 10:08:20 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-09 10:08:20 - -----------no weight decay layers--------------
2022-07-09 10:08:20 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 10:08:20 - -------------weight decay layers---------------
2022-07-09 10:08:20 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer1.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer2.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer3.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 10:08:20 - epoch 001 lr: 0.100000
2022-07-09 10:09:01 - train: epoch 0001, iter [00100, 05004], lr: 0.020400, loss: 7.0154
2022-07-09 10:09:36 - train: epoch 0001, iter [00200, 05004], lr: 0.020799, loss: 6.9710
2022-07-09 10:10:11 - train: epoch 0001, iter [00300, 05004], lr: 0.021199, loss: 6.9251
2022-07-09 10:10:46 - train: epoch 0001, iter [00400, 05004], lr: 0.021599, loss: 6.9147
2022-07-09 10:11:21 - train: epoch 0001, iter [00500, 05004], lr: 0.021998, loss: 6.9114
2022-07-09 10:11:55 - train: epoch 0001, iter [00600, 05004], lr: 0.022398, loss: 6.8350
2022-07-09 10:12:29 - train: epoch 0001, iter [00700, 05004], lr: 0.022798, loss: 6.8276
2022-07-09 10:13:04 - train: epoch 0001, iter [00800, 05004], lr: 0.023197, loss: 6.7990
2022-07-09 10:13:38 - train: epoch 0001, iter [00900, 05004], lr: 0.023597, loss: 6.7439
2022-07-09 10:14:12 - train: epoch 0001, iter [01000, 05004], lr: 0.023997, loss: 6.7362
2022-07-09 10:14:47 - train: epoch 0001, iter [01100, 05004], lr: 0.024396, loss: 6.6923
2022-07-09 10:15:20 - train: epoch 0001, iter [01200, 05004], lr: 0.024796, loss: 6.5216
2022-07-09 10:15:54 - train: epoch 0001, iter [01300, 05004], lr: 0.025196, loss: 6.6566
2022-07-09 10:16:27 - train: epoch 0001, iter [01400, 05004], lr: 0.025596, loss: 6.5432
2022-07-09 10:17:01 - train: epoch 0001, iter [01500, 05004], lr: 0.025995, loss: 6.4221
2022-07-09 10:17:35 - train: epoch 0001, iter [01600, 05004], lr: 0.026395, loss: 6.4783
2022-07-09 10:18:09 - train: epoch 0001, iter [01700, 05004], lr: 0.026795, loss: 6.3093
2022-07-09 10:18:42 - train: epoch 0001, iter [01800, 05004], lr: 0.027194, loss: 6.3544
2022-07-09 10:19:16 - train: epoch 0001, iter [01900, 05004], lr: 0.027594, loss: 6.3033
2022-07-09 10:19:50 - train: epoch 0001, iter [02000, 05004], lr: 0.027994, loss: 6.2339
2022-07-09 10:20:23 - train: epoch 0001, iter [02100, 05004], lr: 0.028393, loss: 6.1810
2022-07-09 10:20:57 - train: epoch 0001, iter [02200, 05004], lr: 0.028793, loss: 6.1158
2022-07-09 10:21:30 - train: epoch 0001, iter [02300, 05004], lr: 0.029193, loss: 6.1569
2022-07-09 10:22:04 - train: epoch 0001, iter [02400, 05004], lr: 0.029592, loss: 6.1532
2022-07-09 10:22:38 - train: epoch 0001, iter [02500, 05004], lr: 0.029992, loss: 6.0383
2022-07-09 10:23:11 - train: epoch 0001, iter [02600, 05004], lr: 0.030392, loss: 5.9876
2022-07-09 10:23:45 - train: epoch 0001, iter [02700, 05004], lr: 0.030791, loss: 6.0830
2022-07-09 10:24:19 - train: epoch 0001, iter [02800, 05004], lr: 0.031191, loss: 5.8935
2022-07-09 10:24:52 - train: epoch 0001, iter [02900, 05004], lr: 0.031591, loss: 5.9253
2022-07-09 10:25:25 - train: epoch 0001, iter [03000, 05004], lr: 0.031990, loss: 5.9309
2022-07-09 10:25:59 - train: epoch 0001, iter [03100, 05004], lr: 0.032390, loss: 5.9950
2022-07-09 10:26:33 - train: epoch 0001, iter [03200, 05004], lr: 0.032790, loss: 6.0061
2022-07-09 10:27:07 - train: epoch 0001, iter [03300, 05004], lr: 0.033189, loss: 5.8429
2022-07-09 10:27:40 - train: epoch 0001, iter [03400, 05004], lr: 0.033589, loss: 5.8993
2022-07-09 10:28:13 - train: epoch 0001, iter [03500, 05004], lr: 0.033989, loss: 5.7398
2022-07-09 10:28:47 - train: epoch 0001, iter [03600, 05004], lr: 0.034388, loss: 5.8103
2022-07-09 10:29:20 - train: epoch 0001, iter [03700, 05004], lr: 0.034788, loss: 5.7210
2022-07-09 10:29:54 - train: epoch 0001, iter [03800, 05004], lr: 0.035188, loss: 5.6728
2022-07-09 10:30:26 - train: epoch 0001, iter [03900, 05004], lr: 0.035588, loss: 5.7141
2022-07-09 10:30:59 - train: epoch 0001, iter [04000, 05004], lr: 0.035987, loss: 5.6530
2022-07-09 10:31:32 - train: epoch 0001, iter [04100, 05004], lr: 0.036387, loss: 5.7104
2022-07-09 10:32:05 - train: epoch 0001, iter [04200, 05004], lr: 0.036787, loss: 5.5478
2022-07-09 10:32:38 - train: epoch 0001, iter [04300, 05004], lr: 0.037186, loss: 5.5595
2022-07-09 10:33:11 - train: epoch 0001, iter [04400, 05004], lr: 0.037586, loss: 5.2995
2022-07-09 10:33:44 - train: epoch 0001, iter [04500, 05004], lr: 0.037986, loss: 5.4999
2022-07-09 10:34:17 - train: epoch 0001, iter [04600, 05004], lr: 0.038385, loss: 5.6089
2022-07-09 10:34:50 - train: epoch 0001, iter [04700, 05004], lr: 0.038785, loss: 5.4245
2022-07-09 10:35:24 - train: epoch 0001, iter [04800, 05004], lr: 0.039185, loss: 5.6117
2022-07-09 10:35:56 - train: epoch 0001, iter [04900, 05004], lr: 0.039584, loss: 5.1706
2022-07-09 10:36:28 - train: epoch 0001, iter [05000, 05004], lr: 0.039984, loss: 5.4773
2022-07-09 10:36:29 - train: epoch 001, train_loss: 6.1313
2022-07-09 10:37:43 - eval: epoch: 001, acc1: 8.730%, acc5: 23.974%, test_loss: 4.9606, per_image_load_time: 2.030ms, per_image_inference_time: 0.466ms
2022-07-09 10:37:44 - until epoch: 001, best_acc1: 8.730%
2022-07-09 10:37:44 - epoch 002 lr: 0.040004
2022-07-09 10:38:23 - train: epoch 0002, iter [00100, 05004], lr: 0.040400, loss: 5.4124
2022-07-09 10:38:57 - train: epoch 0002, iter [00200, 05004], lr: 0.040799, loss: 5.2610
2022-07-09 10:39:31 - train: epoch 0002, iter [00300, 05004], lr: 0.041199, loss: 5.3401
2022-07-09 10:40:05 - train: epoch 0002, iter [00400, 05004], lr: 0.041599, loss: 5.3643
2022-07-09 10:40:39 - train: epoch 0002, iter [00500, 05004], lr: 0.041998, loss: 5.2459
2022-07-09 10:41:12 - train: epoch 0002, iter [00600, 05004], lr: 0.042398, loss: 5.1776
2022-07-09 10:41:46 - train: epoch 0002, iter [00700, 05004], lr: 0.042798, loss: 5.2344
2022-07-09 10:42:19 - train: epoch 0002, iter [00800, 05004], lr: 0.043197, loss: 5.0139
2022-07-09 10:42:53 - train: epoch 0002, iter [00900, 05004], lr: 0.043597, loss: 4.9378
2022-07-09 10:43:28 - train: epoch 0002, iter [01000, 05004], lr: 0.043997, loss: 5.1418
2022-07-09 10:44:01 - train: epoch 0002, iter [01100, 05004], lr: 0.044396, loss: 5.1425
2022-07-09 10:44:35 - train: epoch 0002, iter [01200, 05004], lr: 0.044796, loss: 5.1246
2022-07-09 10:45:09 - train: epoch 0002, iter [01300, 05004], lr: 0.045196, loss: 4.8582
2022-07-09 10:45:43 - train: epoch 0002, iter [01400, 05004], lr: 0.045596, loss: 5.0475
2022-07-09 10:46:17 - train: epoch 0002, iter [01500, 05004], lr: 0.045995, loss: 4.9534
2022-07-09 10:46:50 - train: epoch 0002, iter [01600, 05004], lr: 0.046395, loss: 4.8962
2022-07-09 10:47:23 - train: epoch 0002, iter [01700, 05004], lr: 0.046795, loss: 4.9891
2022-07-09 10:47:57 - train: epoch 0002, iter [01800, 05004], lr: 0.047194, loss: 4.9119
2022-07-09 10:48:30 - train: epoch 0002, iter [01900, 05004], lr: 0.047594, loss: 4.9552
2022-07-09 10:49:03 - train: epoch 0002, iter [02000, 05004], lr: 0.047994, loss: 4.7380
2022-07-09 10:49:37 - train: epoch 0002, iter [02100, 05004], lr: 0.048393, loss: 4.9731
2022-07-09 10:50:11 - train: epoch 0002, iter [02200, 05004], lr: 0.048793, loss: 4.6006
2022-07-09 10:50:45 - train: epoch 0002, iter [02300, 05004], lr: 0.049193, loss: 4.9050
2022-07-09 10:51:18 - train: epoch 0002, iter [02400, 05004], lr: 0.049592, loss: 4.5594
2022-07-09 10:51:51 - train: epoch 0002, iter [02500, 05004], lr: 0.049992, loss: 4.7230
2022-07-09 10:52:25 - train: epoch 0002, iter [02600, 05004], lr: 0.050392, loss: 4.6418
2022-07-09 10:52:58 - train: epoch 0002, iter [02700, 05004], lr: 0.050791, loss: 4.7949
2022-07-09 10:53:32 - train: epoch 0002, iter [02800, 05004], lr: 0.051191, loss: 4.7805
2022-07-09 10:54:06 - train: epoch 0002, iter [02900, 05004], lr: 0.051591, loss: 4.6807
2022-07-09 10:54:38 - train: epoch 0002, iter [03000, 05004], lr: 0.051990, loss: 4.5687
2022-07-09 10:55:13 - train: epoch 0002, iter [03100, 05004], lr: 0.052390, loss: 4.4755
2022-07-09 10:55:46 - train: epoch 0002, iter [03200, 05004], lr: 0.052790, loss: 4.5847
2022-07-09 10:56:20 - train: epoch 0002, iter [03300, 05004], lr: 0.053189, loss: 4.5809
2022-07-09 10:56:53 - train: epoch 0002, iter [03400, 05004], lr: 0.053589, loss: 4.6230
2022-07-09 10:57:26 - train: epoch 0002, iter [03500, 05004], lr: 0.053989, loss: 4.6047
2022-07-09 10:58:00 - train: epoch 0002, iter [03600, 05004], lr: 0.054388, loss: 4.4950
2022-07-09 10:58:34 - train: epoch 0002, iter [03700, 05004], lr: 0.054788, loss: 4.6468
2022-07-09 10:59:08 - train: epoch 0002, iter [03800, 05004], lr: 0.055188, loss: 4.2374
2022-07-09 10:59:41 - train: epoch 0002, iter [03900, 05004], lr: 0.055588, loss: 4.6452
2022-07-09 11:00:15 - train: epoch 0002, iter [04000, 05004], lr: 0.055987, loss: 4.4348
2022-07-09 11:00:49 - train: epoch 0002, iter [04100, 05004], lr: 0.056387, loss: 4.5600
2022-07-09 11:01:23 - train: epoch 0002, iter [04200, 05004], lr: 0.056787, loss: 4.4906
2022-07-09 11:01:56 - train: epoch 0002, iter [04300, 05004], lr: 0.057186, loss: 4.5605
2022-07-09 11:02:30 - train: epoch 0002, iter [04400, 05004], lr: 0.057586, loss: 4.3824
2022-07-09 11:03:03 - train: epoch 0002, iter [04500, 05004], lr: 0.057986, loss: 4.3213
2022-07-09 11:03:37 - train: epoch 0002, iter [04600, 05004], lr: 0.058385, loss: 4.3608
2022-07-09 11:04:10 - train: epoch 0002, iter [04700, 05004], lr: 0.058785, loss: 4.3864
2022-07-09 11:04:44 - train: epoch 0002, iter [04800, 05004], lr: 0.059185, loss: 4.3608
2022-07-09 11:05:18 - train: epoch 0002, iter [04900, 05004], lr: 0.059584, loss: 4.3172
2022-07-09 11:05:50 - train: epoch 0002, iter [05000, 05004], lr: 0.059984, loss: 4.3677
2022-07-09 11:05:51 - train: epoch 002, train_loss: 4.7905
2022-07-09 11:07:05 - eval: epoch: 002, acc1: 21.114%, acc5: 45.134%, test_loss: 3.9619, per_image_load_time: 2.034ms, per_image_inference_time: 0.483ms
2022-07-09 11:07:05 - until epoch: 002, best_acc1: 21.114%
2022-07-09 11:07:05 - epoch 003 lr: 0.060004
2022-07-09 11:07:45 - train: epoch 0003, iter [00100, 05004], lr: 0.060400, loss: 4.4778
2022-07-09 11:08:19 - train: epoch 0003, iter [00200, 05004], lr: 0.060799, loss: 4.2619
2022-07-09 11:08:53 - train: epoch 0003, iter [00300, 05004], lr: 0.061199, loss: 4.2233
2022-07-09 11:09:27 - train: epoch 0003, iter [00400, 05004], lr: 0.061599, loss: 4.3698
2022-07-09 11:10:00 - train: epoch 0003, iter [00500, 05004], lr: 0.061998, loss: 4.3169
2022-07-09 11:10:34 - train: epoch 0003, iter [00600, 05004], lr: 0.062398, loss: 4.1966
2022-07-09 11:11:08 - train: epoch 0003, iter [00700, 05004], lr: 0.062798, loss: 4.4236
2022-07-09 11:11:42 - train: epoch 0003, iter [00800, 05004], lr: 0.063197, loss: 4.3846
2022-07-09 11:12:15 - train: epoch 0003, iter [00900, 05004], lr: 0.063597, loss: 4.1397
2022-07-09 11:12:49 - train: epoch 0003, iter [01000, 05004], lr: 0.063997, loss: 4.1105
2022-07-09 11:13:23 - train: epoch 0003, iter [01100, 05004], lr: 0.064396, loss: 4.1831
2022-07-09 11:13:56 - train: epoch 0003, iter [01200, 05004], lr: 0.064796, loss: 4.0867
2022-07-09 11:14:30 - train: epoch 0003, iter [01300, 05004], lr: 0.065196, loss: 4.1561
2022-07-09 11:15:04 - train: epoch 0003, iter [01400, 05004], lr: 0.065596, loss: 3.9832
2022-07-09 11:15:38 - train: epoch 0003, iter [01500, 05004], lr: 0.065995, loss: 4.3489
2022-07-09 11:16:11 - train: epoch 0003, iter [01600, 05004], lr: 0.066395, loss: 4.0525
2022-07-09 11:16:46 - train: epoch 0003, iter [01700, 05004], lr: 0.066795, loss: 4.0037
2022-07-09 11:17:19 - train: epoch 0003, iter [01800, 05004], lr: 0.067194, loss: 4.0150
2022-07-09 11:17:53 - train: epoch 0003, iter [01900, 05004], lr: 0.067594, loss: 4.1128
2022-07-09 11:18:27 - train: epoch 0003, iter [02000, 05004], lr: 0.067994, loss: 4.3916
2022-07-09 11:19:00 - train: epoch 0003, iter [02100, 05004], lr: 0.068393, loss: 4.2711
2022-07-09 11:19:34 - train: epoch 0003, iter [02200, 05004], lr: 0.068793, loss: 4.4041
2022-07-09 11:20:07 - train: epoch 0003, iter [02300, 05004], lr: 0.069193, loss: 4.0812
2022-07-09 11:20:41 - train: epoch 0003, iter [02400, 05004], lr: 0.069592, loss: 4.0944
2022-07-09 11:21:15 - train: epoch 0003, iter [02500, 05004], lr: 0.069992, loss: 4.0090
2022-07-09 11:21:48 - train: epoch 0003, iter [02600, 05004], lr: 0.070392, loss: 3.7560
2022-07-09 11:22:23 - train: epoch 0003, iter [02700, 05004], lr: 0.070791, loss: 4.3710
2022-07-09 11:22:57 - train: epoch 0003, iter [02800, 05004], lr: 0.071191, loss: 3.9627
2022-07-09 11:23:31 - train: epoch 0003, iter [02900, 05004], lr: 0.071591, loss: 3.9685
2022-07-09 11:24:04 - train: epoch 0003, iter [03000, 05004], lr: 0.071990, loss: 4.1405
2022-07-09 11:24:39 - train: epoch 0003, iter [03100, 05004], lr: 0.072390, loss: 4.1052
2022-07-09 11:25:13 - train: epoch 0003, iter [03200, 05004], lr: 0.072790, loss: 4.1452
2022-07-09 11:25:47 - train: epoch 0003, iter [03300, 05004], lr: 0.073189, loss: 4.0706
2022-07-09 11:26:21 - train: epoch 0003, iter [03400, 05004], lr: 0.073589, loss: 3.9178
2022-07-09 11:26:55 - train: epoch 0003, iter [03500, 05004], lr: 0.073989, loss: 3.7661
2022-07-09 11:27:29 - train: epoch 0003, iter [03600, 05004], lr: 0.074388, loss: 3.9073
2022-07-09 11:28:03 - train: epoch 0003, iter [03700, 05004], lr: 0.074788, loss: 3.7806
2022-07-09 11:28:37 - train: epoch 0003, iter [03800, 05004], lr: 0.075188, loss: 4.2440
2022-07-09 11:29:11 - train: epoch 0003, iter [03900, 05004], lr: 0.075588, loss: 4.0342
2022-07-09 11:29:44 - train: epoch 0003, iter [04000, 05004], lr: 0.075987, loss: 3.9174
2022-07-09 11:30:19 - train: epoch 0003, iter [04100, 05004], lr: 0.076387, loss: 3.9435
2022-07-09 11:30:53 - train: epoch 0003, iter [04200, 05004], lr: 0.076787, loss: 3.8655
2022-07-09 11:31:26 - train: epoch 0003, iter [04300, 05004], lr: 0.077186, loss: 3.5968
2022-07-09 11:32:00 - train: epoch 0003, iter [04400, 05004], lr: 0.077586, loss: 3.7405
2022-07-09 11:32:34 - train: epoch 0003, iter [04500, 05004], lr: 0.077986, loss: 3.7454
2022-07-09 11:33:08 - train: epoch 0003, iter [04600, 05004], lr: 0.078385, loss: 3.8199
2022-07-09 11:33:42 - train: epoch 0003, iter [04700, 05004], lr: 0.078785, loss: 3.6787
2022-07-09 11:34:16 - train: epoch 0003, iter [04800, 05004], lr: 0.079185, loss: 4.0249
2022-07-09 11:34:49 - train: epoch 0003, iter [04900, 05004], lr: 0.079584, loss: 3.8847
2022-07-09 11:35:22 - train: epoch 0003, iter [05000, 05004], lr: 0.079984, loss: 3.7419
2022-07-09 11:35:23 - train: epoch 003, train_loss: 4.0533
2022-07-09 11:36:37 - eval: epoch: 003, acc1: 32.502%, acc5: 59.416%, test_loss: 3.1170, per_image_load_time: 2.166ms, per_image_inference_time: 0.443ms
2022-07-09 11:36:37 - until epoch: 003, best_acc1: 32.502%
2022-07-09 11:36:37 - epoch 004 lr: 0.080004
2022-07-09 11:37:16 - train: epoch 0004, iter [00100, 05004], lr: 0.080400, loss: 3.9382
2022-07-09 11:37:50 - train: epoch 0004, iter [00200, 05004], lr: 0.080799, loss: 3.7278
2022-07-09 11:38:25 - train: epoch 0004, iter [00300, 05004], lr: 0.081199, loss: 3.8771
2022-07-09 11:38:58 - train: epoch 0004, iter [00400, 05004], lr: 0.081599, loss: 3.8783
2022-07-09 11:39:32 - train: epoch 0004, iter [00500, 05004], lr: 0.081998, loss: 3.5726
2022-07-09 11:40:05 - train: epoch 0004, iter [00600, 05004], lr: 0.082398, loss: 3.9427
2022-07-09 11:40:39 - train: epoch 0004, iter [00700, 05004], lr: 0.082798, loss: 3.9955
2022-07-09 11:41:12 - train: epoch 0004, iter [00800, 05004], lr: 0.083197, loss: 3.7605
2022-07-09 11:41:46 - train: epoch 0004, iter [00900, 05004], lr: 0.083597, loss: 3.6485
2022-07-09 11:42:19 - train: epoch 0004, iter [01000, 05004], lr: 0.083997, loss: 3.7296
2022-07-09 11:42:53 - train: epoch 0004, iter [01100, 05004], lr: 0.084396, loss: 3.7905
2022-07-09 11:43:26 - train: epoch 0004, iter [01200, 05004], lr: 0.084796, loss: 3.3706
2022-07-09 11:44:01 - train: epoch 0004, iter [01300, 05004], lr: 0.085196, loss: 3.4580
2022-07-09 11:44:34 - train: epoch 0004, iter [01400, 05004], lr: 0.085596, loss: 3.6592
2022-07-09 11:45:08 - train: epoch 0004, iter [01500, 05004], lr: 0.085995, loss: 3.6843
2022-07-09 11:45:41 - train: epoch 0004, iter [01600, 05004], lr: 0.086395, loss: 3.6660
2022-07-09 11:46:15 - train: epoch 0004, iter [01700, 05004], lr: 0.086795, loss: 3.8798
2022-07-09 11:46:49 - train: epoch 0004, iter [01800, 05004], lr: 0.087194, loss: 3.9638
2022-07-09 11:47:22 - train: epoch 0004, iter [01900, 05004], lr: 0.087594, loss: 3.6897
2022-07-09 11:47:56 - train: epoch 0004, iter [02000, 05004], lr: 0.087994, loss: 3.8275
2022-07-09 11:48:30 - train: epoch 0004, iter [02100, 05004], lr: 0.088393, loss: 3.8097
2022-07-09 11:49:04 - train: epoch 0004, iter [02200, 05004], lr: 0.088793, loss: 3.6316
2022-07-09 11:49:38 - train: epoch 0004, iter [02300, 05004], lr: 0.089193, loss: 3.6032
2022-07-09 11:50:12 - train: epoch 0004, iter [02400, 05004], lr: 0.089592, loss: 3.5349
2022-07-09 11:50:46 - train: epoch 0004, iter [02500, 05004], lr: 0.089992, loss: 3.6632
2022-07-09 11:51:20 - train: epoch 0004, iter [02600, 05004], lr: 0.090392, loss: 3.6657
2022-07-09 11:51:54 - train: epoch 0004, iter [02700, 05004], lr: 0.090791, loss: 3.5210
2022-07-09 11:52:27 - train: epoch 0004, iter [02800, 05004], lr: 0.091191, loss: 3.6381
2022-07-09 11:53:01 - train: epoch 0004, iter [02900, 05004], lr: 0.091591, loss: 3.4816
2022-07-09 11:53:35 - train: epoch 0004, iter [03000, 05004], lr: 0.091990, loss: 3.5788
2022-07-09 11:54:08 - train: epoch 0004, iter [03100, 05004], lr: 0.092390, loss: 3.6747
2022-07-09 11:54:43 - train: epoch 0004, iter [03200, 05004], lr: 0.092790, loss: 3.6195
2022-07-09 11:55:16 - train: epoch 0004, iter [03300, 05004], lr: 0.093189, loss: 3.7571
2022-07-09 11:55:51 - train: epoch 0004, iter [03400, 05004], lr: 0.093589, loss: 3.6616
2022-07-09 11:56:24 - train: epoch 0004, iter [03500, 05004], lr: 0.093989, loss: 3.7836
2022-07-09 11:56:57 - train: epoch 0004, iter [03600, 05004], lr: 0.094388, loss: 3.2935
2022-07-09 11:57:32 - train: epoch 0004, iter [03700, 05004], lr: 0.094788, loss: 3.5638
2022-07-09 11:58:06 - train: epoch 0004, iter [03800, 05004], lr: 0.095188, loss: 3.5267
2022-07-09 11:58:40 - train: epoch 0004, iter [03900, 05004], lr: 0.095588, loss: 3.5193
2022-07-09 11:59:14 - train: epoch 0004, iter [04000, 05004], lr: 0.095987, loss: 3.3589
2022-07-09 11:59:48 - train: epoch 0004, iter [04100, 05004], lr: 0.096387, loss: 3.4755
2022-07-09 12:00:21 - train: epoch 0004, iter [04200, 05004], lr: 0.096787, loss: 3.4729
2022-07-09 12:00:55 - train: epoch 0004, iter [04300, 05004], lr: 0.097186, loss: 3.4433
2022-07-09 12:01:28 - train: epoch 0004, iter [04400, 05004], lr: 0.097586, loss: 3.3763
2022-07-09 12:02:02 - train: epoch 0004, iter [04500, 05004], lr: 0.097986, loss: 3.1378
2022-07-09 12:02:36 - train: epoch 0004, iter [04600, 05004], lr: 0.098385, loss: 3.3321
2022-07-09 12:03:10 - train: epoch 0004, iter [04700, 05004], lr: 0.098785, loss: 3.4648
2022-07-09 12:03:44 - train: epoch 0004, iter [04800, 05004], lr: 0.099185, loss: 3.3874
2022-07-09 12:04:18 - train: epoch 0004, iter [04900, 05004], lr: 0.099584, loss: 3.6349
2022-07-09 12:04:50 - train: epoch 0004, iter [05000, 05004], lr: 0.099984, loss: 3.7463
2022-07-09 12:04:51 - train: epoch 004, train_loss: 3.6768
2022-07-09 12:06:06 - eval: epoch: 004, acc1: 35.466%, acc5: 62.186%, test_loss: 2.9502, per_image_load_time: 2.315ms, per_image_inference_time: 0.495ms
2022-07-09 12:06:06 - until epoch: 004, best_acc1: 35.466%
2022-07-09 12:06:06 - epoch 005 lr: 0.100004
2022-07-09 12:06:46 - train: epoch 0005, iter [00100, 05004], lr: 0.100400, loss: 3.7486
2022-07-09 12:07:20 - train: epoch 0005, iter [00200, 05004], lr: 0.100799, loss: 3.5039
2022-07-09 12:07:54 - train: epoch 0005, iter [00300, 05004], lr: 0.101199, loss: 3.6923
2022-07-09 12:08:28 - train: epoch 0005, iter [00400, 05004], lr: 0.101599, loss: 3.5679
2022-07-09 12:09:02 - train: epoch 0005, iter [00500, 05004], lr: 0.101998, loss: 3.3956
2022-07-09 12:09:36 - train: epoch 0005, iter [00600, 05004], lr: 0.102398, loss: 3.5824
2022-07-09 12:10:11 - train: epoch 0005, iter [00700, 05004], lr: 0.102798, loss: 3.4514
2022-07-09 12:10:45 - train: epoch 0005, iter [00800, 05004], lr: 0.103197, loss: 3.6303
2022-07-09 12:11:19 - train: epoch 0005, iter [00900, 05004], lr: 0.103597, loss: 3.4049
2022-07-09 12:11:52 - train: epoch 0005, iter [01000, 05004], lr: 0.103997, loss: 3.4869
2022-07-09 12:12:26 - train: epoch 0005, iter [01100, 05004], lr: 0.104396, loss: 3.4568
2022-07-09 12:13:00 - train: epoch 0005, iter [01200, 05004], lr: 0.104796, loss: 3.5993
2022-07-09 12:13:34 - train: epoch 0005, iter [01300, 05004], lr: 0.105196, loss: 3.4093
2022-07-09 12:14:09 - train: epoch 0005, iter [01400, 05004], lr: 0.105596, loss: 3.5981
2022-07-09 12:14:43 - train: epoch 0005, iter [01500, 05004], lr: 0.105995, loss: 3.5806
2022-07-09 12:15:17 - train: epoch 0005, iter [01600, 05004], lr: 0.106395, loss: 3.2680
2022-07-09 12:15:52 - train: epoch 0005, iter [01700, 05004], lr: 0.106795, loss: 3.4677
2022-07-09 12:16:27 - train: epoch 0005, iter [01800, 05004], lr: 0.107194, loss: 3.5090
2022-07-09 12:17:01 - train: epoch 0005, iter [01900, 05004], lr: 0.107594, loss: 3.4835
2022-07-09 12:17:35 - train: epoch 0005, iter [02000, 05004], lr: 0.107994, loss: 3.4562
2022-07-09 12:18:10 - train: epoch 0005, iter [02100, 05004], lr: 0.108393, loss: 3.1247
2022-07-09 12:18:44 - train: epoch 0005, iter [02200, 05004], lr: 0.108793, loss: 3.2540
2022-07-09 12:19:18 - train: epoch 0005, iter [02300, 05004], lr: 0.109193, loss: 3.3197
2022-07-09 12:19:53 - train: epoch 0005, iter [02400, 05004], lr: 0.109592, loss: 3.4602
2022-07-09 12:20:28 - train: epoch 0005, iter [02500, 05004], lr: 0.109992, loss: 3.5943
2022-07-09 12:21:02 - train: epoch 0005, iter [02600, 05004], lr: 0.110392, loss: 3.6937
2022-07-09 12:21:37 - train: epoch 0005, iter [02700, 05004], lr: 0.110791, loss: 3.4179
2022-07-09 12:22:11 - train: epoch 0005, iter [02800, 05004], lr: 0.111191, loss: 3.4438
2022-07-09 12:22:45 - train: epoch 0005, iter [02900, 05004], lr: 0.111591, loss: 3.2701
2022-07-09 12:23:19 - train: epoch 0005, iter [03000, 05004], lr: 0.111990, loss: 3.5280
2022-07-09 12:23:54 - train: epoch 0005, iter [03100, 05004], lr: 0.112390, loss: 3.5318
2022-07-09 12:24:28 - train: epoch 0005, iter [03200, 05004], lr: 0.112790, loss: 3.5480
2022-07-09 12:25:03 - train: epoch 0005, iter [03300, 05004], lr: 0.113189, loss: 3.2914
2022-07-09 12:25:38 - train: epoch 0005, iter [03400, 05004], lr: 0.113589, loss: 3.3425
2022-07-09 12:26:12 - train: epoch 0005, iter [03500, 05004], lr: 0.113989, loss: 3.3954
2022-07-09 12:26:46 - train: epoch 0005, iter [03600, 05004], lr: 0.114388, loss: 3.5851
2022-07-09 12:27:21 - train: epoch 0005, iter [03700, 05004], lr: 0.114788, loss: 3.3026
2022-07-09 12:27:56 - train: epoch 0005, iter [03800, 05004], lr: 0.115188, loss: 3.3383
2022-07-09 12:28:30 - train: epoch 0005, iter [03900, 05004], lr: 0.115588, loss: 3.7225
2022-07-09 12:29:05 - train: epoch 0005, iter [04000, 05004], lr: 0.115987, loss: 3.3670
2022-07-09 12:29:40 - train: epoch 0005, iter [04100, 05004], lr: 0.116387, loss: 3.4800
2022-07-09 12:30:14 - train: epoch 0005, iter [04200, 05004], lr: 0.116787, loss: 3.3534
2022-07-09 12:30:49 - train: epoch 0005, iter [04300, 05004], lr: 0.117186, loss: 3.2033
2022-07-09 12:31:24 - train: epoch 0005, iter [04400, 05004], lr: 0.117586, loss: 3.4477
2022-07-09 12:31:58 - train: epoch 0005, iter [04500, 05004], lr: 0.117986, loss: 3.4992
2022-07-09 12:32:33 - train: epoch 0005, iter [04600, 05004], lr: 0.118385, loss: 3.5146
2022-07-09 12:33:08 - train: epoch 0005, iter [04700, 05004], lr: 0.118785, loss: 3.2155
2022-07-09 12:33:43 - train: epoch 0005, iter [04800, 05004], lr: 0.119185, loss: 3.3152
2022-07-09 12:34:16 - train: epoch 0005, iter [04900, 05004], lr: 0.119584, loss: 3.5825
2022-07-09 12:34:50 - train: epoch 0005, iter [05000, 05004], lr: 0.119984, loss: 3.3235
2022-07-09 12:34:51 - train: epoch 005, train_loss: 3.4856
2022-07-09 12:36:07 - eval: epoch: 005, acc1: 38.252%, acc5: 65.688%, test_loss: 2.7705, per_image_load_time: 2.447ms, per_image_inference_time: 0.463ms
2022-07-09 12:36:07 - until epoch: 005, best_acc1: 38.252%
2022-07-09 12:36:07 - epoch 006 lr: 0.100000
2022-07-09 12:36:47 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 3.3566
2022-07-09 12:37:21 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 3.2323
2022-07-09 12:37:56 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 3.0706
2022-07-09 12:38:29 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.4374
2022-07-09 12:39:03 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 3.3239
2022-07-09 12:39:37 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 3.3445
2022-07-09 12:40:12 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.4573
2022-07-09 12:40:47 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 3.2920
2022-07-09 12:41:20 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 3.2869
2022-07-09 12:41:54 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 3.4624
2022-07-09 12:42:29 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 3.1989
2022-07-09 12:43:03 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 3.3937
2022-07-09 12:43:38 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.2701
2022-07-09 12:44:12 - train: epoch 0006, iter [01400, 05004], lr: 0.099999, loss: 3.3197
2022-07-09 12:44:46 - train: epoch 0006, iter [01500, 05004], lr: 0.099999, loss: 3.2986
2022-07-09 12:45:20 - train: epoch 0006, iter [01600, 05004], lr: 0.099999, loss: 3.0212
2022-07-09 12:45:55 - train: epoch 0006, iter [01700, 05004], lr: 0.099999, loss: 3.3804
2022-07-09 12:46:29 - train: epoch 0006, iter [01800, 05004], lr: 0.099999, loss: 3.3275
2022-07-09 12:47:04 - train: epoch 0006, iter [01900, 05004], lr: 0.099999, loss: 2.9802
2022-07-09 12:47:38 - train: epoch 0006, iter [02000, 05004], lr: 0.099999, loss: 3.6590
2022-07-09 12:48:12 - train: epoch 0006, iter [02100, 05004], lr: 0.099999, loss: 3.3738
2022-07-09 12:48:47 - train: epoch 0006, iter [02200, 05004], lr: 0.099999, loss: 3.1888
2022-07-09 12:49:22 - train: epoch 0006, iter [02300, 05004], lr: 0.099999, loss: 3.1278
2022-07-09 12:49:56 - train: epoch 0006, iter [02400, 05004], lr: 0.099999, loss: 3.1941
2022-07-09 12:50:30 - train: epoch 0006, iter [02500, 05004], lr: 0.099998, loss: 3.2688
2022-07-09 12:51:04 - train: epoch 0006, iter [02600, 05004], lr: 0.099998, loss: 3.2099
2022-07-09 12:51:38 - train: epoch 0006, iter [02700, 05004], lr: 0.099998, loss: 3.2405
2022-07-09 12:52:13 - train: epoch 0006, iter [02800, 05004], lr: 0.099998, loss: 3.2320
2022-07-09 12:52:47 - train: epoch 0006, iter [02900, 05004], lr: 0.099998, loss: 3.2982
2022-07-09 12:53:21 - train: epoch 0006, iter [03000, 05004], lr: 0.099998, loss: 3.2571
2022-07-09 12:53:55 - train: epoch 0006, iter [03100, 05004], lr: 0.099998, loss: 3.2056
2022-07-09 12:54:30 - train: epoch 0006, iter [03200, 05004], lr: 0.099997, loss: 3.1282
2022-07-09 12:55:04 - train: epoch 0006, iter [03300, 05004], lr: 0.099997, loss: 2.8974
2022-07-09 12:55:38 - train: epoch 0006, iter [03400, 05004], lr: 0.099997, loss: 3.3493
2022-07-09 12:56:13 - train: epoch 0006, iter [03500, 05004], lr: 0.099997, loss: 3.2424
2022-07-09 12:56:47 - train: epoch 0006, iter [03600, 05004], lr: 0.099997, loss: 3.3922
2022-07-09 12:57:21 - train: epoch 0006, iter [03700, 05004], lr: 0.099996, loss: 3.2423
2022-07-09 12:57:56 - train: epoch 0006, iter [03800, 05004], lr: 0.099996, loss: 2.9917
2022-07-09 12:58:30 - train: epoch 0006, iter [03900, 05004], lr: 0.099996, loss: 3.2484
2022-07-09 12:59:05 - train: epoch 0006, iter [04000, 05004], lr: 0.099996, loss: 3.6197
2022-07-09 12:59:39 - train: epoch 0006, iter [04100, 05004], lr: 0.099996, loss: 3.2596
2022-07-09 13:00:14 - train: epoch 0006, iter [04200, 05004], lr: 0.099995, loss: 2.9018
2022-07-09 13:00:48 - train: epoch 0006, iter [04300, 05004], lr: 0.099995, loss: 3.1190
2022-07-09 13:01:23 - train: epoch 0006, iter [04400, 05004], lr: 0.099995, loss: 3.1947
2022-07-09 13:01:58 - train: epoch 0006, iter [04500, 05004], lr: 0.099995, loss: 3.1615
2022-07-09 13:02:31 - train: epoch 0006, iter [04600, 05004], lr: 0.099995, loss: 3.1804
2022-07-09 13:03:06 - train: epoch 0006, iter [04700, 05004], lr: 0.099994, loss: 3.2771
2022-07-09 13:03:40 - train: epoch 0006, iter [04800, 05004], lr: 0.099994, loss: 3.1846
2022-07-09 13:04:15 - train: epoch 0006, iter [04900, 05004], lr: 0.099994, loss: 3.1554
2022-07-09 13:04:48 - train: epoch 0006, iter [05000, 05004], lr: 0.099994, loss: 3.0517
2022-07-09 13:04:49 - train: epoch 006, train_loss: 3.2610
2022-07-09 13:06:04 - eval: epoch: 006, acc1: 42.172%, acc5: 68.862%, test_loss: 2.5857, per_image_load_time: 2.375ms, per_image_inference_time: 0.470ms
2022-07-09 13:06:04 - until epoch: 006, best_acc1: 42.172%
2022-07-09 13:06:04 - epoch 007 lr: 0.099994
2022-07-09 13:06:43 - train: epoch 0007, iter [00100, 05004], lr: 0.099993, loss: 3.2015
2022-07-09 13:07:18 - train: epoch 0007, iter [00200, 05004], lr: 0.099993, loss: 3.3776
2022-07-09 13:07:52 - train: epoch 0007, iter [00300, 05004], lr: 0.099993, loss: 3.5358
2022-07-09 13:08:27 - train: epoch 0007, iter [00400, 05004], lr: 0.099992, loss: 3.2120
2022-07-09 13:09:00 - train: epoch 0007, iter [00500, 05004], lr: 0.099992, loss: 3.1300
2022-07-09 13:09:35 - train: epoch 0007, iter [00600, 05004], lr: 0.099992, loss: 3.1577
2022-07-09 13:10:08 - train: epoch 0007, iter [00700, 05004], lr: 0.099992, loss: 3.3440
2022-07-09 13:10:42 - train: epoch 0007, iter [00800, 05004], lr: 0.099991, loss: 3.2572
2022-07-09 13:11:17 - train: epoch 0007, iter [00900, 05004], lr: 0.099991, loss: 3.2869
2022-07-09 13:11:51 - train: epoch 0007, iter [01000, 05004], lr: 0.099991, loss: 3.3335
2022-07-09 13:12:26 - train: epoch 0007, iter [01100, 05004], lr: 0.099990, loss: 3.0462
2022-07-09 13:13:00 - train: epoch 0007, iter [01200, 05004], lr: 0.099990, loss: 3.2185
2022-07-09 13:13:35 - train: epoch 0007, iter [01300, 05004], lr: 0.099990, loss: 3.0586
2022-07-09 13:14:10 - train: epoch 0007, iter [01400, 05004], lr: 0.099989, loss: 3.1449
2022-07-09 13:14:44 - train: epoch 0007, iter [01500, 05004], lr: 0.099989, loss: 3.2276
2022-07-09 13:15:18 - train: epoch 0007, iter [01600, 05004], lr: 0.099989, loss: 3.0868
2022-07-09 13:15:53 - train: epoch 0007, iter [01700, 05004], lr: 0.099988, loss: 3.1541
2022-07-09 13:16:27 - train: epoch 0007, iter [01800, 05004], lr: 0.099988, loss: 3.1710
2022-07-09 13:17:01 - train: epoch 0007, iter [01900, 05004], lr: 0.099988, loss: 3.1276
2022-07-09 13:17:36 - train: epoch 0007, iter [02000, 05004], lr: 0.099987, loss: 2.9596
2022-07-09 13:18:11 - train: epoch 0007, iter [02100, 05004], lr: 0.099987, loss: 3.1372
2022-07-09 13:18:45 - train: epoch 0007, iter [02200, 05004], lr: 0.099987, loss: 3.2756
2022-07-09 13:19:21 - train: epoch 0007, iter [02300, 05004], lr: 0.099986, loss: 3.3063
2022-07-09 13:19:55 - train: epoch 0007, iter [02400, 05004], lr: 0.099986, loss: 3.2495
2022-07-09 13:20:29 - train: epoch 0007, iter [02500, 05004], lr: 0.099985, loss: 3.1957
2022-07-09 13:21:04 - train: epoch 0007, iter [02600, 05004], lr: 0.099985, loss: 3.1013
2022-07-09 13:21:38 - train: epoch 0007, iter [02700, 05004], lr: 0.099985, loss: 3.2215
2022-07-09 13:22:13 - train: epoch 0007, iter [02800, 05004], lr: 0.099984, loss: 3.2219
2022-07-09 13:22:48 - train: epoch 0007, iter [02900, 05004], lr: 0.099984, loss: 3.1519
2022-07-09 13:23:22 - train: epoch 0007, iter [03000, 05004], lr: 0.099983, loss: 3.4753
2022-07-09 13:23:57 - train: epoch 0007, iter [03100, 05004], lr: 0.099983, loss: 3.1320
2022-07-09 13:24:31 - train: epoch 0007, iter [03200, 05004], lr: 0.099983, loss: 2.9022
2022-07-09 13:25:06 - train: epoch 0007, iter [03300, 05004], lr: 0.099982, loss: 3.2777
2022-07-09 13:25:40 - train: epoch 0007, iter [03400, 05004], lr: 0.099982, loss: 2.8059
2022-07-09 13:26:15 - train: epoch 0007, iter [03500, 05004], lr: 0.099981, loss: 3.2148
2022-07-09 13:26:50 - train: epoch 0007, iter [03600, 05004], lr: 0.099981, loss: 2.8731
2022-07-09 13:27:24 - train: epoch 0007, iter [03700, 05004], lr: 0.099980, loss: 3.1862
2022-07-09 13:27:58 - train: epoch 0007, iter [03800, 05004], lr: 0.099980, loss: 3.3247
2022-07-09 13:28:32 - train: epoch 0007, iter [03900, 05004], lr: 0.099979, loss: 2.9092
2022-07-09 13:29:07 - train: epoch 0007, iter [04000, 05004], lr: 0.099979, loss: 3.0865
2022-07-09 13:29:41 - train: epoch 0007, iter [04100, 05004], lr: 0.099979, loss: 3.0895
2022-07-09 13:30:16 - train: epoch 0007, iter [04200, 05004], lr: 0.099978, loss: 2.9672
2022-07-09 13:30:50 - train: epoch 0007, iter [04300, 05004], lr: 0.099978, loss: 3.1480
2022-07-09 13:31:24 - train: epoch 0007, iter [04400, 05004], lr: 0.099977, loss: 3.0183
2022-07-09 13:31:59 - train: epoch 0007, iter [04500, 05004], lr: 0.099977, loss: 3.3949
2022-07-09 13:32:33 - train: epoch 0007, iter [04600, 05004], lr: 0.099976, loss: 3.2495
2022-07-09 13:33:09 - train: epoch 0007, iter [04700, 05004], lr: 0.099976, loss: 3.2116
2022-07-09 13:33:43 - train: epoch 0007, iter [04800, 05004], lr: 0.099975, loss: 3.4656
2022-07-09 13:34:18 - train: epoch 0007, iter [04900, 05004], lr: 0.099975, loss: 3.0892
2022-07-09 13:34:51 - train: epoch 0007, iter [05000, 05004], lr: 0.099974, loss: 3.1754
2022-07-09 13:34:52 - train: epoch 007, train_loss: 3.1568
2022-07-09 13:36:06 - eval: epoch: 007, acc1: 43.792%, acc5: 70.290%, test_loss: 2.4914, per_image_load_time: 2.112ms, per_image_inference_time: 0.461ms
2022-07-09 13:36:07 - until epoch: 007, best_acc1: 43.792%
2022-07-09 13:36:07 - epoch 008 lr: 0.099974
2022-07-09 13:36:46 - train: epoch 0008, iter [00100, 05004], lr: 0.099974, loss: 2.9964
2022-07-09 13:37:20 - train: epoch 0008, iter [00200, 05004], lr: 0.099973, loss: 3.1218
2022-07-09 13:37:52 - train: epoch 0008, iter [00300, 05004], lr: 0.099972, loss: 2.8134
2022-07-09 13:38:27 - train: epoch 0008, iter [00400, 05004], lr: 0.099972, loss: 3.0591
2022-07-09 13:39:01 - train: epoch 0008, iter [00500, 05004], lr: 0.099971, loss: 3.0162
2022-07-09 13:39:35 - train: epoch 0008, iter [00600, 05004], lr: 0.099971, loss: 2.7702
2022-07-09 13:40:10 - train: epoch 0008, iter [00700, 05004], lr: 0.099970, loss: 3.4429
2022-07-09 13:40:44 - train: epoch 0008, iter [00800, 05004], lr: 0.099970, loss: 2.9735
2022-07-09 13:41:18 - train: epoch 0008, iter [00900, 05004], lr: 0.099969, loss: 3.0867
2022-07-09 13:41:52 - train: epoch 0008, iter [01000, 05004], lr: 0.099969, loss: 3.1560
2022-07-09 13:42:26 - train: epoch 0008, iter [01100, 05004], lr: 0.099968, loss: 2.7582
2022-07-09 13:43:01 - train: epoch 0008, iter [01200, 05004], lr: 0.099967, loss: 3.0725
2022-07-09 13:43:35 - train: epoch 0008, iter [01300, 05004], lr: 0.099967, loss: 3.0599
2022-07-09 13:44:10 - train: epoch 0008, iter [01400, 05004], lr: 0.099966, loss: 3.1069
2022-07-09 13:44:44 - train: epoch 0008, iter [01500, 05004], lr: 0.099966, loss: 2.9541
2022-07-09 13:45:18 - train: epoch 0008, iter [01600, 05004], lr: 0.099965, loss: 3.0774
2022-07-09 13:45:53 - train: epoch 0008, iter [01700, 05004], lr: 0.099964, loss: 3.0760
2022-07-09 13:46:27 - train: epoch 0008, iter [01800, 05004], lr: 0.099964, loss: 3.3370
2022-07-09 13:47:01 - train: epoch 0008, iter [01900, 05004], lr: 0.099963, loss: 2.9645
2022-07-09 13:47:36 - train: epoch 0008, iter [02000, 05004], lr: 0.099963, loss: 3.1168
2022-07-09 13:48:10 - train: epoch 0008, iter [02100, 05004], lr: 0.099962, loss: 3.2082
2022-07-09 13:48:43 - train: epoch 0008, iter [02200, 05004], lr: 0.099961, loss: 3.0922
2022-07-09 13:49:19 - train: epoch 0008, iter [02300, 05004], lr: 0.099961, loss: 3.0536
2022-07-09 13:49:54 - train: epoch 0008, iter [02400, 05004], lr: 0.099960, loss: 2.9130
2022-07-09 13:50:28 - train: epoch 0008, iter [02500, 05004], lr: 0.099959, loss: 3.2321
2022-07-09 13:51:03 - train: epoch 0008, iter [02600, 05004], lr: 0.099959, loss: 3.0126
2022-07-09 13:51:38 - train: epoch 0008, iter [02700, 05004], lr: 0.099958, loss: 3.0381
2022-07-09 13:52:12 - train: epoch 0008, iter [02800, 05004], lr: 0.099957, loss: 3.1531
2022-07-09 13:52:47 - train: epoch 0008, iter [02900, 05004], lr: 0.099957, loss: 2.9525
2022-07-09 13:53:21 - train: epoch 0008, iter [03000, 05004], lr: 0.099956, loss: 3.0851
2022-07-09 13:53:55 - train: epoch 0008, iter [03100, 05004], lr: 0.099955, loss: 3.0921
2022-07-09 13:54:31 - train: epoch 0008, iter [03200, 05004], lr: 0.099955, loss: 3.3815
2022-07-09 13:55:05 - train: epoch 0008, iter [03300, 05004], lr: 0.099954, loss: 3.3188
2022-07-09 13:55:39 - train: epoch 0008, iter [03400, 05004], lr: 0.099953, loss: 3.2918
2022-07-09 13:56:14 - train: epoch 0008, iter [03500, 05004], lr: 0.099953, loss: 3.1048
2022-07-09 13:56:49 - train: epoch 0008, iter [03600, 05004], lr: 0.099952, loss: 3.1263
2022-07-09 13:57:23 - train: epoch 0008, iter [03700, 05004], lr: 0.099951, loss: 2.9335
2022-07-09 13:57:58 - train: epoch 0008, iter [03800, 05004], lr: 0.099951, loss: 2.9273
2022-07-09 13:58:33 - train: epoch 0008, iter [03900, 05004], lr: 0.099950, loss: 3.2558
2022-07-09 13:59:08 - train: epoch 0008, iter [04000, 05004], lr: 0.099949, loss: 3.3291
2022-07-09 13:59:42 - train: epoch 0008, iter [04100, 05004], lr: 0.099948, loss: 3.0018
2022-07-09 14:00:17 - train: epoch 0008, iter [04200, 05004], lr: 0.099948, loss: 3.0653
2022-07-09 14:00:52 - train: epoch 0008, iter [04300, 05004], lr: 0.099947, loss: 2.7810
2022-07-09 14:01:26 - train: epoch 0008, iter [04400, 05004], lr: 0.099946, loss: 2.9367
2022-07-09 14:02:00 - train: epoch 0008, iter [04500, 05004], lr: 0.099945, loss: 3.1611
2022-07-09 14:02:35 - train: epoch 0008, iter [04600, 05004], lr: 0.099945, loss: 3.1017
2022-07-09 14:03:09 - train: epoch 0008, iter [04700, 05004], lr: 0.099944, loss: 2.8489
2022-07-09 14:03:43 - train: epoch 0008, iter [04800, 05004], lr: 0.099943, loss: 3.1279
2022-07-09 14:04:18 - train: epoch 0008, iter [04900, 05004], lr: 0.099942, loss: 3.1669
2022-07-09 14:04:51 - train: epoch 0008, iter [05000, 05004], lr: 0.099942, loss: 3.1768
2022-07-09 14:04:52 - train: epoch 008, train_loss: 3.0757
2022-07-09 14:06:08 - eval: epoch: 008, acc1: 46.568%, acc5: 73.530%, test_loss: 2.3225, per_image_load_time: 2.413ms, per_image_inference_time: 0.481ms
2022-07-09 14:06:08 - until epoch: 008, best_acc1: 46.568%
2022-07-09 14:06:08 - epoch 009 lr: 0.099942
2022-07-09 14:06:48 - train: epoch 0009, iter [00100, 05004], lr: 0.099941, loss: 2.7803
2022-07-09 14:07:22 - train: epoch 0009, iter [00200, 05004], lr: 0.099940, loss: 2.9711
2022-07-09 14:07:56 - train: epoch 0009, iter [00300, 05004], lr: 0.099939, loss: 2.6498
2022-07-09 14:08:31 - train: epoch 0009, iter [00400, 05004], lr: 0.099938, loss: 3.1334
2022-07-09 14:09:05 - train: epoch 0009, iter [00500, 05004], lr: 0.099938, loss: 2.9902
2022-07-09 14:09:40 - train: epoch 0009, iter [00600, 05004], lr: 0.099937, loss: 3.0638
2022-07-09 14:10:14 - train: epoch 0009, iter [00700, 05004], lr: 0.099936, loss: 2.9757
2022-07-09 14:10:47 - train: epoch 0009, iter [00800, 05004], lr: 0.099935, loss: 2.9754
2022-07-09 14:11:22 - train: epoch 0009, iter [00900, 05004], lr: 0.099934, loss: 2.6183
2022-07-09 14:11:56 - train: epoch 0009, iter [01000, 05004], lr: 0.099934, loss: 2.8211
2022-07-09 14:12:31 - train: epoch 0009, iter [01100, 05004], lr: 0.099933, loss: 3.3478
2022-07-09 14:13:05 - train: epoch 0009, iter [01200, 05004], lr: 0.099932, loss: 3.2258
2022-07-09 14:13:39 - train: epoch 0009, iter [01300, 05004], lr: 0.099931, loss: 3.3068
2022-07-09 14:14:13 - train: epoch 0009, iter [01400, 05004], lr: 0.099930, loss: 2.7606
2022-07-09 14:14:48 - train: epoch 0009, iter [01500, 05004], lr: 0.099929, loss: 2.9600
2022-07-09 14:15:22 - train: epoch 0009, iter [01600, 05004], lr: 0.099929, loss: 3.0388
2022-07-09 14:15:56 - train: epoch 0009, iter [01700, 05004], lr: 0.099928, loss: 3.3203
2022-07-09 14:16:31 - train: epoch 0009, iter [01800, 05004], lr: 0.099927, loss: 3.0350
2022-07-09 14:17:05 - train: epoch 0009, iter [01900, 05004], lr: 0.099926, loss: 2.7850
2022-07-09 14:17:40 - train: epoch 0009, iter [02000, 05004], lr: 0.099925, loss: 2.8100
2022-07-09 14:18:13 - train: epoch 0009, iter [02100, 05004], lr: 0.099924, loss: 3.0272
2022-07-09 14:18:48 - train: epoch 0009, iter [02200, 05004], lr: 0.099923, loss: 3.0125
2022-07-09 14:19:23 - train: epoch 0009, iter [02300, 05004], lr: 0.099922, loss: 2.9374
2022-07-09 14:19:57 - train: epoch 0009, iter [02400, 05004], lr: 0.099921, loss: 2.9035
2022-07-09 14:20:31 - train: epoch 0009, iter [02500, 05004], lr: 0.099921, loss: 2.7763
2022-07-09 14:21:06 - train: epoch 0009, iter [02600, 05004], lr: 0.099920, loss: 2.9129
2022-07-09 14:21:41 - train: epoch 0009, iter [02700, 05004], lr: 0.099919, loss: 2.9469
2022-07-09 14:22:15 - train: epoch 0009, iter [02800, 05004], lr: 0.099918, loss: 3.0786
2022-07-09 14:22:50 - train: epoch 0009, iter [02900, 05004], lr: 0.099917, loss: 2.7664
2022-07-09 14:23:24 - train: epoch 0009, iter [03000, 05004], lr: 0.099916, loss: 3.0695
2022-07-09 14:23:59 - train: epoch 0009, iter [03100, 05004], lr: 0.099915, loss: 2.9737
2022-07-09 14:24:33 - train: epoch 0009, iter [03200, 05004], lr: 0.099914, loss: 2.8418
2022-07-09 14:25:07 - train: epoch 0009, iter [03300, 05004], lr: 0.099913, loss: 2.9857
2022-07-09 14:25:43 - train: epoch 0009, iter [03400, 05004], lr: 0.099912, loss: 3.1635
2022-07-09 14:26:17 - train: epoch 0009, iter [03500, 05004], lr: 0.099911, loss: 3.1070
2022-07-09 14:26:52 - train: epoch 0009, iter [03600, 05004], lr: 0.099910, loss: 2.9380
2022-07-09 14:27:26 - train: epoch 0009, iter [03700, 05004], lr: 0.099909, loss: 3.2508
2022-07-09 14:28:00 - train: epoch 0009, iter [03800, 05004], lr: 0.099908, loss: 3.2028
2022-07-09 14:28:35 - train: epoch 0009, iter [03900, 05004], lr: 0.099907, loss: 2.6244
2022-07-09 14:29:09 - train: epoch 0009, iter [04000, 05004], lr: 0.099906, loss: 3.3342
2022-07-09 14:29:44 - train: epoch 0009, iter [04100, 05004], lr: 0.099905, loss: 3.1005
2022-07-09 14:30:18 - train: epoch 0009, iter [04200, 05004], lr: 0.099904, loss: 2.8265
2022-07-09 14:30:54 - train: epoch 0009, iter [04300, 05004], lr: 0.099903, loss: 2.9468
2022-07-09 14:31:28 - train: epoch 0009, iter [04400, 05004], lr: 0.099902, loss: 2.9274
2022-07-09 14:32:02 - train: epoch 0009, iter [04500, 05004], lr: 0.099901, loss: 2.8753
2022-07-09 14:32:37 - train: epoch 0009, iter [04600, 05004], lr: 0.099900, loss: 3.2791
2022-07-09 14:33:11 - train: epoch 0009, iter [04700, 05004], lr: 0.099899, loss: 3.0872
2022-07-09 14:33:46 - train: epoch 0009, iter [04800, 05004], lr: 0.099898, loss: 2.9540
2022-07-09 14:34:20 - train: epoch 0009, iter [04900, 05004], lr: 0.099897, loss: 2.9972
2022-07-09 14:34:53 - train: epoch 0009, iter [05000, 05004], lr: 0.099896, loss: 3.0344
2022-07-09 14:34:54 - train: epoch 009, train_loss: 3.0139
2022-07-09 14:36:09 - eval: epoch: 009, acc1: 48.178%, acc5: 74.538%, test_loss: 2.2529, per_image_load_time: 2.397ms, per_image_inference_time: 0.456ms
2022-07-09 14:36:09 - until epoch: 009, best_acc1: 48.178%
2022-07-09 14:36:09 - epoch 010 lr: 0.099896
2022-07-09 14:36:49 - train: epoch 0010, iter [00100, 05004], lr: 0.099895, loss: 2.7803
2022-07-09 14:37:23 - train: epoch 0010, iter [00200, 05004], lr: 0.099894, loss: 2.9037
2022-07-09 14:37:57 - train: epoch 0010, iter [00300, 05004], lr: 0.099893, loss: 3.0514
2022-07-09 14:38:32 - train: epoch 0010, iter [00400, 05004], lr: 0.099892, loss: 3.0656
2022-07-09 14:39:06 - train: epoch 0010, iter [00500, 05004], lr: 0.099891, loss: 2.8802
2022-07-09 14:39:39 - train: epoch 0010, iter [00600, 05004], lr: 0.099890, loss: 3.0476
2022-07-09 14:40:14 - train: epoch 0010, iter [00700, 05004], lr: 0.099889, loss: 2.9968
2022-07-09 14:40:48 - train: epoch 0010, iter [00800, 05004], lr: 0.099888, loss: 2.8274
2022-07-09 14:41:21 - train: epoch 0010, iter [00900, 05004], lr: 0.099887, loss: 2.7908
2022-07-09 14:41:56 - train: epoch 0010, iter [01000, 05004], lr: 0.099886, loss: 2.7808
2022-07-09 14:42:29 - train: epoch 0010, iter [01100, 05004], lr: 0.099884, loss: 2.9398
2022-07-09 14:43:03 - train: epoch 0010, iter [01200, 05004], lr: 0.099883, loss: 2.8417
2022-07-09 14:43:37 - train: epoch 0010, iter [01300, 05004], lr: 0.099882, loss: 2.8959
2022-07-09 14:44:11 - train: epoch 0010, iter [01400, 05004], lr: 0.099881, loss: 3.0265
2022-07-09 14:44:45 - train: epoch 0010, iter [01500, 05004], lr: 0.099880, loss: 2.8422
2022-07-09 14:45:19 - train: epoch 0010, iter [01600, 05004], lr: 0.099879, loss: 3.2262
2022-07-09 14:45:54 - train: epoch 0010, iter [01700, 05004], lr: 0.099878, loss: 3.1813
2022-07-09 14:46:27 - train: epoch 0010, iter [01800, 05004], lr: 0.099877, loss: 3.0462
2022-07-09 14:47:01 - train: epoch 0010, iter [01900, 05004], lr: 0.099876, loss: 3.1739
2022-07-09 14:47:36 - train: epoch 0010, iter [02000, 05004], lr: 0.099874, loss: 2.9226
2022-07-09 14:48:09 - train: epoch 0010, iter [02100, 05004], lr: 0.099873, loss: 2.8813
2022-07-09 14:48:44 - train: epoch 0010, iter [02200, 05004], lr: 0.099872, loss: 3.0822
2022-07-09 14:49:18 - train: epoch 0010, iter [02300, 05004], lr: 0.099871, loss: 3.0134
2022-07-09 14:49:52 - train: epoch 0010, iter [02400, 05004], lr: 0.099870, loss: 3.2086
2022-07-09 14:50:27 - train: epoch 0010, iter [02500, 05004], lr: 0.099869, loss: 2.8434
2022-07-09 14:51:02 - train: epoch 0010, iter [02600, 05004], lr: 0.099868, loss: 3.3094
2022-07-09 14:51:36 - train: epoch 0010, iter [02700, 05004], lr: 0.099866, loss: 2.7461
2022-07-09 14:52:10 - train: epoch 0010, iter [02800, 05004], lr: 0.099865, loss: 2.9448
2022-07-09 14:52:44 - train: epoch 0010, iter [02900, 05004], lr: 0.099864, loss: 3.0637
2022-07-09 14:53:18 - train: epoch 0010, iter [03000, 05004], lr: 0.099863, loss: 2.9254
2022-07-09 14:53:52 - train: epoch 0010, iter [03100, 05004], lr: 0.099862, loss: 3.0992
2022-07-09 14:54:26 - train: epoch 0010, iter [03200, 05004], lr: 0.099860, loss: 3.0755
2022-07-09 14:55:00 - train: epoch 0010, iter [03300, 05004], lr: 0.099859, loss: 3.1352
2022-07-09 14:55:35 - train: epoch 0010, iter [03400, 05004], lr: 0.099858, loss: 3.0670
2022-07-09 14:56:08 - train: epoch 0010, iter [03500, 05004], lr: 0.099857, loss: 3.0431
2022-07-09 14:56:43 - train: epoch 0010, iter [03600, 05004], lr: 0.099856, loss: 2.9342
2022-07-09 14:57:17 - train: epoch 0010, iter [03700, 05004], lr: 0.099854, loss: 2.6508
2022-07-09 14:57:51 - train: epoch 0010, iter [03800, 05004], lr: 0.099853, loss: 3.0306
2022-07-09 14:58:25 - train: epoch 0010, iter [03900, 05004], lr: 0.099852, loss: 2.6629
2022-07-09 14:58:59 - train: epoch 0010, iter [04000, 05004], lr: 0.099851, loss: 3.0080
2022-07-09 14:59:33 - train: epoch 0010, iter [04100, 05004], lr: 0.099849, loss: 2.6544
2022-07-09 15:00:07 - train: epoch 0010, iter [04200, 05004], lr: 0.099848, loss: 3.0877
2022-07-09 15:00:41 - train: epoch 0010, iter [04300, 05004], lr: 0.099847, loss: 3.1427
2022-07-09 15:01:15 - train: epoch 0010, iter [04400, 05004], lr: 0.099846, loss: 3.0217
2022-07-09 15:01:49 - train: epoch 0010, iter [04500, 05004], lr: 0.099844, loss: 2.8097
2022-07-09 15:02:24 - train: epoch 0010, iter [04600, 05004], lr: 0.099843, loss: 3.1482
2022-07-09 15:02:58 - train: epoch 0010, iter [04700, 05004], lr: 0.099842, loss: 3.0747
2022-07-09 15:03:32 - train: epoch 0010, iter [04800, 05004], lr: 0.099840, loss: 2.8097
2022-07-09 15:04:06 - train: epoch 0010, iter [04900, 05004], lr: 0.099839, loss: 2.8683
2022-07-09 15:04:39 - train: epoch 0010, iter [05000, 05004], lr: 0.099838, loss: 2.6420
2022-07-09 15:04:40 - train: epoch 010, train_loss: 2.9625
2022-07-09 15:05:55 - eval: epoch: 010, acc1: 47.900%, acc5: 74.208%, test_loss: 2.2837, per_image_load_time: 1.544ms, per_image_inference_time: 0.450ms
2022-07-09 15:05:55 - until epoch: 010, best_acc1: 48.178%
2022-07-09 15:05:55 - epoch 011 lr: 0.099838
2022-07-09 15:06:35 - train: epoch 0011, iter [00100, 05004], lr: 0.099837, loss: 2.6230
2022-07-09 15:07:09 - train: epoch 0011, iter [00200, 05004], lr: 0.099835, loss: 3.0612
2022-07-09 15:07:43 - train: epoch 0011, iter [00300, 05004], lr: 0.099834, loss: 2.7377
2022-07-09 15:08:17 - train: epoch 0011, iter [00400, 05004], lr: 0.099833, loss: 3.1657
2022-07-09 15:08:51 - train: epoch 0011, iter [00500, 05004], lr: 0.099831, loss: 2.8454
2022-07-09 15:09:25 - train: epoch 0011, iter [00600, 05004], lr: 0.099830, loss: 2.8604
2022-07-09 15:10:00 - train: epoch 0011, iter [00700, 05004], lr: 0.099829, loss: 2.9978
2022-07-09 15:10:34 - train: epoch 0011, iter [00800, 05004], lr: 0.099827, loss: 2.8417
2022-07-09 15:11:08 - train: epoch 0011, iter [00900, 05004], lr: 0.099826, loss: 3.1109
2022-07-09 15:11:42 - train: epoch 0011, iter [01000, 05004], lr: 0.099825, loss: 2.9588
2022-07-09 15:12:16 - train: epoch 0011, iter [01100, 05004], lr: 0.099823, loss: 3.0022
2022-07-09 15:12:51 - train: epoch 0011, iter [01200, 05004], lr: 0.099822, loss: 3.3082
2022-07-09 15:13:25 - train: epoch 0011, iter [01300, 05004], lr: 0.099821, loss: 2.9973
2022-07-09 15:13:59 - train: epoch 0011, iter [01400, 05004], lr: 0.099819, loss: 2.9234
2022-07-09 15:14:33 - train: epoch 0011, iter [01500, 05004], lr: 0.099818, loss: 2.9133
2022-07-09 15:15:07 - train: epoch 0011, iter [01600, 05004], lr: 0.099816, loss: 2.9437
2022-07-09 15:15:42 - train: epoch 0011, iter [01700, 05004], lr: 0.099815, loss: 3.0615
2022-07-09 15:16:17 - train: epoch 0011, iter [01800, 05004], lr: 0.099814, loss: 2.7875
2022-07-09 15:16:51 - train: epoch 0011, iter [01900, 05004], lr: 0.099812, loss: 2.7661
2022-07-09 15:17:25 - train: epoch 0011, iter [02000, 05004], lr: 0.099811, loss: 2.9313
2022-07-09 15:17:59 - train: epoch 0011, iter [02100, 05004], lr: 0.099810, loss: 2.9044
2022-07-09 15:18:33 - train: epoch 0011, iter [02200, 05004], lr: 0.099808, loss: 2.7405
2022-07-09 15:19:08 - train: epoch 0011, iter [02300, 05004], lr: 0.099807, loss: 3.2124
2022-07-09 15:19:42 - train: epoch 0011, iter [02400, 05004], lr: 0.099805, loss: 2.7059
2022-07-09 15:20:16 - train: epoch 0011, iter [02500, 05004], lr: 0.099804, loss: 3.5765
2022-07-09 15:20:51 - train: epoch 0011, iter [02600, 05004], lr: 0.099802, loss: 2.7796
2022-07-09 15:21:25 - train: epoch 0011, iter [02700, 05004], lr: 0.099801, loss: 2.9765
2022-07-09 15:21:59 - train: epoch 0011, iter [02800, 05004], lr: 0.099800, loss: 2.6171
2022-07-09 15:22:34 - train: epoch 0011, iter [02900, 05004], lr: 0.099798, loss: 2.8637
2022-07-09 15:23:09 - train: epoch 0011, iter [03000, 05004], lr: 0.099797, loss: 3.0063
2022-07-09 15:23:43 - train: epoch 0011, iter [03100, 05004], lr: 0.099795, loss: 2.8150
2022-07-09 15:24:17 - train: epoch 0011, iter [03200, 05004], lr: 0.099794, loss: 2.6506
2022-07-09 15:24:50 - train: epoch 0011, iter [03300, 05004], lr: 0.099792, loss: 3.0397
2022-07-09 15:25:25 - train: epoch 0011, iter [03400, 05004], lr: 0.099791, loss: 2.7530
2022-07-09 15:25:59 - train: epoch 0011, iter [03500, 05004], lr: 0.099789, loss: 2.7837
2022-07-09 15:26:33 - train: epoch 0011, iter [03600, 05004], lr: 0.099788, loss: 2.8445
2022-07-09 15:27:07 - train: epoch 0011, iter [03700, 05004], lr: 0.099786, loss: 2.9810
2022-07-09 15:27:41 - train: epoch 0011, iter [03800, 05004], lr: 0.099785, loss: 2.8357
2022-07-09 15:28:15 - train: epoch 0011, iter [03900, 05004], lr: 0.099783, loss: 2.8785
2022-07-09 15:28:49 - train: epoch 0011, iter [04000, 05004], lr: 0.099782, loss: 2.9218
2022-07-09 15:29:23 - train: epoch 0011, iter [04100, 05004], lr: 0.099780, loss: 2.7074
2022-07-09 15:29:58 - train: epoch 0011, iter [04200, 05004], lr: 0.099779, loss: 2.8638
2022-07-09 15:30:31 - train: epoch 0011, iter [04300, 05004], lr: 0.099777, loss: 2.9918
2022-07-09 15:31:06 - train: epoch 0011, iter [04400, 05004], lr: 0.099776, loss: 2.6756
2022-07-09 15:31:40 - train: epoch 0011, iter [04500, 05004], lr: 0.099774, loss: 2.8814
2022-07-09 15:32:14 - train: epoch 0011, iter [04600, 05004], lr: 0.099773, loss: 2.7874
2022-07-09 15:32:48 - train: epoch 0011, iter [04700, 05004], lr: 0.099771, loss: 2.5793
2022-07-09 15:33:22 - train: epoch 0011, iter [04800, 05004], lr: 0.099770, loss: 2.6411
2022-07-09 15:33:55 - train: epoch 0011, iter [04900, 05004], lr: 0.099768, loss: 2.6562
2022-07-09 15:34:28 - train: epoch 0011, iter [05000, 05004], lr: 0.099767, loss: 2.8593
2022-07-09 15:34:30 - train: epoch 011, train_loss: 2.9209
2022-07-09 15:35:45 - eval: epoch: 011, acc1: 49.286%, acc5: 75.410%, test_loss: 2.2017, per_image_load_time: 1.956ms, per_image_inference_time: 0.452ms
2022-07-09 15:35:45 - until epoch: 011, best_acc1: 49.286%
2022-07-09 15:35:45 - epoch 012 lr: 0.099767
2022-07-09 15:36:24 - train: epoch 0012, iter [00100, 05004], lr: 0.099765, loss: 2.9858
2022-07-09 15:36:59 - train: epoch 0012, iter [00200, 05004], lr: 0.099763, loss: 2.9267
2022-07-09 15:37:32 - train: epoch 0012, iter [00300, 05004], lr: 0.099762, loss: 2.8847
2022-07-09 15:38:07 - train: epoch 0012, iter [00400, 05004], lr: 0.099760, loss: 2.8350
2022-07-09 15:38:41 - train: epoch 0012, iter [00500, 05004], lr: 0.099759, loss: 3.0314
2022-07-09 15:39:15 - train: epoch 0012, iter [00600, 05004], lr: 0.099757, loss: 2.6304
2022-07-09 15:39:48 - train: epoch 0012, iter [00700, 05004], lr: 0.099756, loss: 2.8037
2022-07-09 15:40:22 - train: epoch 0012, iter [00800, 05004], lr: 0.099754, loss: 2.8489
2022-07-09 15:40:56 - train: epoch 0012, iter [00900, 05004], lr: 0.099752, loss: 2.9485
2022-07-09 15:41:30 - train: epoch 0012, iter [01000, 05004], lr: 0.099751, loss: 2.7185
2022-07-09 15:42:03 - train: epoch 0012, iter [01100, 05004], lr: 0.099749, loss: 3.1967
2022-07-09 15:42:37 - train: epoch 0012, iter [01200, 05004], lr: 0.099748, loss: 2.6905
2022-07-09 15:43:10 - train: epoch 0012, iter [01300, 05004], lr: 0.099746, loss: 2.8577
2022-07-09 15:43:43 - train: epoch 0012, iter [01400, 05004], lr: 0.099744, loss: 3.1037
2022-07-09 15:44:17 - train: epoch 0012, iter [01500, 05004], lr: 0.099743, loss: 2.5934
2022-07-09 15:44:51 - train: epoch 0012, iter [01600, 05004], lr: 0.099741, loss: 2.6189
2022-07-09 15:45:24 - train: epoch 0012, iter [01700, 05004], lr: 0.099739, loss: 2.7919
2022-07-09 15:45:59 - train: epoch 0012, iter [01800, 05004], lr: 0.099738, loss: 2.9949
2022-07-09 15:46:32 - train: epoch 0012, iter [01900, 05004], lr: 0.099736, loss: 3.0609
2022-07-09 15:47:07 - train: epoch 0012, iter [02000, 05004], lr: 0.099734, loss: 3.0729
2022-07-09 15:47:40 - train: epoch 0012, iter [02100, 05004], lr: 0.099733, loss: 2.7930
2022-07-09 15:48:13 - train: epoch 0012, iter [02200, 05004], lr: 0.099731, loss: 3.0313
2022-07-09 15:48:48 - train: epoch 0012, iter [02300, 05004], lr: 0.099729, loss: 2.7369
2022-07-09 15:49:22 - train: epoch 0012, iter [02400, 05004], lr: 0.099728, loss: 2.8167
2022-07-09 15:49:56 - train: epoch 0012, iter [02500, 05004], lr: 0.099726, loss: 2.7593
2022-07-09 15:50:30 - train: epoch 0012, iter [02600, 05004], lr: 0.099724, loss: 2.6399
2022-07-09 15:51:04 - train: epoch 0012, iter [02700, 05004], lr: 0.099723, loss: 2.8619
2022-07-09 15:51:38 - train: epoch 0012, iter [02800, 05004], lr: 0.099721, loss: 2.8469
2022-07-09 15:52:12 - train: epoch 0012, iter [02900, 05004], lr: 0.099719, loss: 2.7208
2022-07-09 15:52:46 - train: epoch 0012, iter [03000, 05004], lr: 0.099718, loss: 2.7291
2022-07-09 15:53:20 - train: epoch 0012, iter [03100, 05004], lr: 0.099716, loss: 3.1049
2022-07-09 15:53:53 - train: epoch 0012, iter [03200, 05004], lr: 0.099714, loss: 2.5019
2022-07-09 15:54:27 - train: epoch 0012, iter [03300, 05004], lr: 0.099713, loss: 3.0659
2022-07-09 15:55:01 - train: epoch 0012, iter [03400, 05004], lr: 0.099711, loss: 2.8885
2022-07-09 15:55:35 - train: epoch 0012, iter [03500, 05004], lr: 0.099709, loss: 2.9375
2022-07-09 15:56:10 - train: epoch 0012, iter [03600, 05004], lr: 0.099707, loss: 2.8494
2022-07-09 15:56:43 - train: epoch 0012, iter [03700, 05004], lr: 0.099706, loss: 2.6849
2022-07-09 15:57:17 - train: epoch 0012, iter [03800, 05004], lr: 0.099704, loss: 3.0930
2022-07-09 15:57:51 - train: epoch 0012, iter [03900, 05004], lr: 0.099702, loss: 2.6730
2022-07-09 15:58:26 - train: epoch 0012, iter [04000, 05004], lr: 0.099700, loss: 2.7282
2022-07-09 15:58:59 - train: epoch 0012, iter [04100, 05004], lr: 0.099699, loss: 2.4618
2022-07-09 15:59:33 - train: epoch 0012, iter [04200, 05004], lr: 0.099697, loss: 2.5365
2022-07-09 16:00:08 - train: epoch 0012, iter [04300, 05004], lr: 0.099695, loss: 2.9347
2022-07-09 16:00:42 - train: epoch 0012, iter [04400, 05004], lr: 0.099693, loss: 2.6823
2022-07-09 16:01:15 - train: epoch 0012, iter [04500, 05004], lr: 0.099691, loss: 2.5972
2022-07-09 16:01:49 - train: epoch 0012, iter [04600, 05004], lr: 0.099690, loss: 3.1137
2022-07-09 16:02:24 - train: epoch 0012, iter [04700, 05004], lr: 0.099688, loss: 2.9627
2022-07-09 16:02:57 - train: epoch 0012, iter [04800, 05004], lr: 0.099686, loss: 2.9147
2022-07-09 16:03:32 - train: epoch 0012, iter [04900, 05004], lr: 0.099684, loss: 2.9185
2022-07-09 16:04:04 - train: epoch 0012, iter [05000, 05004], lr: 0.099682, loss: 2.7740
2022-07-09 16:04:05 - train: epoch 012, train_loss: 2.8858
2022-07-09 16:05:21 - eval: epoch: 012, acc1: 51.440%, acc5: 77.210%, test_loss: 2.0859, per_image_load_time: 2.467ms, per_image_inference_time: 0.449ms
2022-07-09 16:05:21 - until epoch: 012, best_acc1: 51.440%
2022-07-09 16:05:21 - epoch 013 lr: 0.099682
2022-07-09 16:06:00 - train: epoch 0013, iter [00100, 05004], lr: 0.099681, loss: 2.6969
2022-07-09 16:06:34 - train: epoch 0013, iter [00200, 05004], lr: 0.099679, loss: 2.6637
2022-07-09 16:07:08 - train: epoch 0013, iter [00300, 05004], lr: 0.099677, loss: 2.8351
2022-07-09 16:07:42 - train: epoch 0013, iter [00400, 05004], lr: 0.099675, loss: 2.8524
2022-07-09 16:08:16 - train: epoch 0013, iter [00500, 05004], lr: 0.099673, loss: 2.8025
2022-07-09 16:08:50 - train: epoch 0013, iter [00600, 05004], lr: 0.099671, loss: 2.9084
2022-07-09 16:09:25 - train: epoch 0013, iter [00700, 05004], lr: 0.099670, loss: 2.7012
2022-07-09 16:09:59 - train: epoch 0013, iter [00800, 05004], lr: 0.099668, loss: 2.9149
2022-07-09 16:10:33 - train: epoch 0013, iter [00900, 05004], lr: 0.099666, loss: 2.7614
2022-07-09 16:11:07 - train: epoch 0013, iter [01000, 05004], lr: 0.099664, loss: 2.9293
2022-07-09 16:11:41 - train: epoch 0013, iter [01100, 05004], lr: 0.099662, loss: 2.7897
2022-07-09 16:12:15 - train: epoch 0013, iter [01200, 05004], lr: 0.099660, loss: 3.1160
2022-07-09 16:12:50 - train: epoch 0013, iter [01300, 05004], lr: 0.099658, loss: 3.0150
2022-07-09 16:13:25 - train: epoch 0013, iter [01400, 05004], lr: 0.099657, loss: 2.8192
2022-07-09 16:13:59 - train: epoch 0013, iter [01500, 05004], lr: 0.099655, loss: 2.7811
2022-07-09 16:14:32 - train: epoch 0013, iter [01600, 05004], lr: 0.099653, loss: 2.6319
2022-07-09 16:15:07 - train: epoch 0013, iter [01700, 05004], lr: 0.099651, loss: 2.8577
2022-07-09 16:15:41 - train: epoch 0013, iter [01800, 05004], lr: 0.099649, loss: 2.9534
2022-07-09 16:16:15 - train: epoch 0013, iter [01900, 05004], lr: 0.099647, loss: 2.8430
2022-07-09 16:16:49 - train: epoch 0013, iter [02000, 05004], lr: 0.099645, loss: 3.0434
2022-07-09 16:17:23 - train: epoch 0013, iter [02100, 05004], lr: 0.099643, loss: 3.0408
2022-07-09 16:17:57 - train: epoch 0013, iter [02200, 05004], lr: 0.099641, loss: 2.5467
2022-07-09 16:18:31 - train: epoch 0013, iter [02300, 05004], lr: 0.099639, loss: 3.0256
2022-07-09 16:19:05 - train: epoch 0013, iter [02400, 05004], lr: 0.099637, loss: 2.9339
2022-07-09 16:19:40 - train: epoch 0013, iter [02500, 05004], lr: 0.099635, loss: 2.9123
2022-07-09 16:20:14 - train: epoch 0013, iter [02600, 05004], lr: 0.099634, loss: 2.7521
2022-07-09 16:20:48 - train: epoch 0013, iter [02700, 05004], lr: 0.099632, loss: 2.7046
2022-07-09 16:21:22 - train: epoch 0013, iter [02800, 05004], lr: 0.099630, loss: 2.7241
2022-07-09 16:21:56 - train: epoch 0013, iter [02900, 05004], lr: 0.099628, loss: 3.0244
2022-07-09 16:22:31 - train: epoch 0013, iter [03000, 05004], lr: 0.099626, loss: 2.7968
2022-07-09 16:23:04 - train: epoch 0013, iter [03100, 05004], lr: 0.099624, loss: 2.8070
2022-07-09 16:23:39 - train: epoch 0013, iter [03200, 05004], lr: 0.099622, loss: 2.6905
2022-07-09 16:24:14 - train: epoch 0013, iter [03300, 05004], lr: 0.099620, loss: 2.6972
2022-07-09 16:24:49 - train: epoch 0013, iter [03400, 05004], lr: 0.099618, loss: 2.6754
2022-07-09 16:25:23 - train: epoch 0013, iter [03500, 05004], lr: 0.099616, loss: 2.6607
2022-07-09 16:25:57 - train: epoch 0013, iter [03600, 05004], lr: 0.099614, loss: 3.1705
2022-07-09 16:26:31 - train: epoch 0013, iter [03700, 05004], lr: 0.099612, loss: 2.6201
2022-07-09 16:27:06 - train: epoch 0013, iter [03800, 05004], lr: 0.099610, loss: 2.9701
2022-07-09 16:27:39 - train: epoch 0013, iter [03900, 05004], lr: 0.099608, loss: 2.8837
2022-07-09 16:28:14 - train: epoch 0013, iter [04000, 05004], lr: 0.099606, loss: 3.2119
2022-07-09 16:28:49 - train: epoch 0013, iter [04100, 05004], lr: 0.099604, loss: 2.6933
2022-07-09 16:29:24 - train: epoch 0013, iter [04200, 05004], lr: 0.099602, loss: 2.7857
2022-07-09 16:29:57 - train: epoch 0013, iter [04300, 05004], lr: 0.099600, loss: 2.8867
2022-07-09 16:30:32 - train: epoch 0013, iter [04400, 05004], lr: 0.099598, loss: 2.7215
2022-07-09 16:31:06 - train: epoch 0013, iter [04500, 05004], lr: 0.099596, loss: 2.7612
2022-07-09 16:31:41 - train: epoch 0013, iter [04600, 05004], lr: 0.099594, loss: 3.0010
2022-07-09 16:32:16 - train: epoch 0013, iter [04700, 05004], lr: 0.099592, loss: 3.0295
2022-07-09 16:32:50 - train: epoch 0013, iter [04800, 05004], lr: 0.099589, loss: 2.9082
2022-07-09 16:33:25 - train: epoch 0013, iter [04900, 05004], lr: 0.099587, loss: 2.9694
2022-07-09 16:33:59 - train: epoch 0013, iter [05000, 05004], lr: 0.099585, loss: 3.0065
2022-07-09 16:34:00 - train: epoch 013, train_loss: 2.8537
2022-07-09 16:35:13 - eval: epoch: 013, acc1: 52.028%, acc5: 77.830%, test_loss: 2.0610, per_image_load_time: 2.351ms, per_image_inference_time: 0.462ms
2022-07-09 16:35:14 - until epoch: 013, best_acc1: 52.028%
2022-07-09 16:35:14 - epoch 014 lr: 0.099585
2022-07-09 16:35:53 - train: epoch 0014, iter [00100, 05004], lr: 0.099583, loss: 2.7570
2022-07-09 16:36:28 - train: epoch 0014, iter [00200, 05004], lr: 0.099581, loss: 2.9331
2022-07-09 16:37:01 - train: epoch 0014, iter [00300, 05004], lr: 0.099579, loss: 2.3871
2022-07-09 16:37:35 - train: epoch 0014, iter [00400, 05004], lr: 0.099577, loss: 2.7317
2022-07-09 16:38:09 - train: epoch 0014, iter [00500, 05004], lr: 0.099575, loss: 2.8089
2022-07-09 16:38:43 - train: epoch 0014, iter [00600, 05004], lr: 0.099573, loss: 2.8084
2022-07-09 16:39:17 - train: epoch 0014, iter [00700, 05004], lr: 0.099571, loss: 2.8003
2022-07-09 16:39:50 - train: epoch 0014, iter [00800, 05004], lr: 0.099569, loss: 2.8389
2022-07-09 16:40:24 - train: epoch 0014, iter [00900, 05004], lr: 0.099566, loss: 3.0128
2022-07-09 16:40:59 - train: epoch 0014, iter [01000, 05004], lr: 0.099564, loss: 3.0423
2022-07-09 16:41:33 - train: epoch 0014, iter [01100, 05004], lr: 0.099562, loss: 2.7840
2022-07-09 16:42:07 - train: epoch 0014, iter [01200, 05004], lr: 0.099560, loss: 2.8732
2022-07-09 16:42:41 - train: epoch 0014, iter [01300, 05004], lr: 0.099558, loss: 2.9723
2022-07-09 16:43:16 - train: epoch 0014, iter [01400, 05004], lr: 0.099556, loss: 2.8747
2022-07-09 16:43:50 - train: epoch 0014, iter [01500, 05004], lr: 0.099554, loss: 3.1038
2022-07-09 16:44:25 - train: epoch 0014, iter [01600, 05004], lr: 0.099552, loss: 2.9800
2022-07-09 16:44:58 - train: epoch 0014, iter [01700, 05004], lr: 0.099549, loss: 3.0575
2022-07-09 16:45:33 - train: epoch 0014, iter [01800, 05004], lr: 0.099547, loss: 2.8784
2022-07-09 16:46:07 - train: epoch 0014, iter [01900, 05004], lr: 0.099545, loss: 2.6594
2022-07-09 16:46:41 - train: epoch 0014, iter [02000, 05004], lr: 0.099543, loss: 2.7002
2022-07-09 16:47:16 - train: epoch 0014, iter [02100, 05004], lr: 0.099541, loss: 2.8639
2022-07-09 16:47:50 - train: epoch 0014, iter [02200, 05004], lr: 0.099539, loss: 2.9132
2022-07-09 16:48:24 - train: epoch 0014, iter [02300, 05004], lr: 0.099536, loss: 2.8273
2022-07-09 16:48:58 - train: epoch 0014, iter [02400, 05004], lr: 0.099534, loss: 3.0250
2022-07-09 16:49:33 - train: epoch 0014, iter [02500, 05004], lr: 0.099532, loss: 2.7417
2022-07-09 16:50:06 - train: epoch 0014, iter [02600, 05004], lr: 0.099530, loss: 2.8151
2022-07-09 16:50:41 - train: epoch 0014, iter [02700, 05004], lr: 0.099528, loss: 2.7634
2022-07-09 16:51:15 - train: epoch 0014, iter [02800, 05004], lr: 0.099525, loss: 2.9028
2022-07-09 16:51:50 - train: epoch 0014, iter [02900, 05004], lr: 0.099523, loss: 2.6444
2022-07-09 16:52:24 - train: epoch 0014, iter [03000, 05004], lr: 0.099521, loss: 3.0590
2022-07-09 16:52:57 - train: epoch 0014, iter [03100, 05004], lr: 0.099519, loss: 2.6381
2022-07-09 16:53:32 - train: epoch 0014, iter [03200, 05004], lr: 0.099516, loss: 2.8081
2022-07-09 16:54:06 - train: epoch 0014, iter [03300, 05004], lr: 0.099514, loss: 2.8175
2022-07-09 16:54:40 - train: epoch 0014, iter [03400, 05004], lr: 0.099512, loss: 2.6804
2022-07-09 16:55:14 - train: epoch 0014, iter [03500, 05004], lr: 0.099510, loss: 2.9298
2022-07-09 16:55:49 - train: epoch 0014, iter [03600, 05004], lr: 0.099507, loss: 2.9318
2022-07-09 16:56:23 - train: epoch 0014, iter [03700, 05004], lr: 0.099505, loss: 2.5855
2022-07-09 16:56:56 - train: epoch 0014, iter [03800, 05004], lr: 0.099503, loss: 3.1457
2022-07-09 16:57:31 - train: epoch 0014, iter [03900, 05004], lr: 0.099501, loss: 2.6968
2022-07-09 16:58:05 - train: epoch 0014, iter [04000, 05004], lr: 0.099498, loss: 2.9534
2022-07-09 16:58:39 - train: epoch 0014, iter [04100, 05004], lr: 0.099496, loss: 2.6988
2022-07-09 16:59:14 - train: epoch 0014, iter [04200, 05004], lr: 0.099494, loss: 2.8730
2022-07-09 16:59:48 - train: epoch 0014, iter [04300, 05004], lr: 0.099492, loss: 2.5811
2022-07-09 17:00:22 - train: epoch 0014, iter [04400, 05004], lr: 0.099489, loss: 2.7820
2022-07-09 17:00:56 - train: epoch 0014, iter [04500, 05004], lr: 0.099487, loss: 2.7943
2022-07-09 17:01:31 - train: epoch 0014, iter [04600, 05004], lr: 0.099485, loss: 2.6302
2022-07-09 17:02:05 - train: epoch 0014, iter [04700, 05004], lr: 0.099482, loss: 2.7553
2022-07-09 17:02:40 - train: epoch 0014, iter [04800, 05004], lr: 0.099480, loss: 2.7497
2022-07-09 17:03:14 - train: epoch 0014, iter [04900, 05004], lr: 0.099478, loss: 2.5936
2022-07-09 17:03:47 - train: epoch 0014, iter [05000, 05004], lr: 0.099475, loss: 2.9562
2022-07-09 17:03:48 - train: epoch 014, train_loss: 2.8260
2022-07-09 17:05:02 - eval: epoch: 014, acc1: 49.314%, acc5: 75.148%, test_loss: 2.2196, per_image_load_time: 2.364ms, per_image_inference_time: 0.462ms
2022-07-09 17:05:02 - until epoch: 014, best_acc1: 52.028%
2022-07-09 17:05:02 - epoch 015 lr: 0.099475
2022-07-09 17:05:42 - train: epoch 0015, iter [00100, 05004], lr: 0.099473, loss: 2.7281
2022-07-09 17:06:16 - train: epoch 0015, iter [00200, 05004], lr: 0.099471, loss: 2.9099
2022-07-09 17:06:49 - train: epoch 0015, iter [00300, 05004], lr: 0.099468, loss: 3.0080
2022-07-09 17:07:23 - train: epoch 0015, iter [00400, 05004], lr: 0.099466, loss: 2.8505
2022-07-09 17:07:58 - train: epoch 0015, iter [00500, 05004], lr: 0.099464, loss: 2.6976
2022-07-09 17:08:32 - train: epoch 0015, iter [00600, 05004], lr: 0.099461, loss: 3.0472
2022-07-09 17:09:06 - train: epoch 0015, iter [00700, 05004], lr: 0.099459, loss: 2.6294
2022-07-09 17:09:40 - train: epoch 0015, iter [00800, 05004], lr: 0.099457, loss: 2.6565
2022-07-09 17:10:14 - train: epoch 0015, iter [00900, 05004], lr: 0.099454, loss: 2.7591
2022-07-09 17:10:48 - train: epoch 0015, iter [01000, 05004], lr: 0.099452, loss: 3.0012
2022-07-09 17:11:22 - train: epoch 0015, iter [01100, 05004], lr: 0.099449, loss: 2.6995
2022-07-09 17:11:56 - train: epoch 0015, iter [01200, 05004], lr: 0.099447, loss: 2.6421
2022-07-09 17:12:31 - train: epoch 0015, iter [01300, 05004], lr: 0.099445, loss: 3.1905
2022-07-09 17:13:04 - train: epoch 0015, iter [01400, 05004], lr: 0.099442, loss: 2.8528
2022-07-09 17:13:38 - train: epoch 0015, iter [01500, 05004], lr: 0.099440, loss: 2.6185
2022-07-09 17:14:12 - train: epoch 0015, iter [01600, 05004], lr: 0.099437, loss: 2.8997
2022-07-09 17:14:47 - train: epoch 0015, iter [01700, 05004], lr: 0.099435, loss: 3.0115
2022-07-09 17:15:20 - train: epoch 0015, iter [01800, 05004], lr: 0.099433, loss: 2.6700
2022-07-09 17:15:53 - train: epoch 0015, iter [01900, 05004], lr: 0.099430, loss: 2.7697
2022-07-09 17:16:27 - train: epoch 0015, iter [02000, 05004], lr: 0.099428, loss: 2.7371
2022-07-09 17:17:01 - train: epoch 0015, iter [02100, 05004], lr: 0.099425, loss: 2.6011
2022-07-09 17:17:35 - train: epoch 0015, iter [02200, 05004], lr: 0.099423, loss: 2.9232
2022-07-09 17:18:09 - train: epoch 0015, iter [02300, 05004], lr: 0.099420, loss: 2.7341
2022-07-09 17:18:43 - train: epoch 0015, iter [02400, 05004], lr: 0.099418, loss: 2.6804
2022-07-09 17:19:17 - train: epoch 0015, iter [02500, 05004], lr: 0.099416, loss: 2.7555
2022-07-09 17:19:51 - train: epoch 0015, iter [02600, 05004], lr: 0.099413, loss: 2.6242
2022-07-09 17:20:24 - train: epoch 0015, iter [02700, 05004], lr: 0.099411, loss: 2.9834
2022-07-09 17:20:58 - train: epoch 0015, iter [02800, 05004], lr: 0.099408, loss: 2.9936
2022-07-09 17:21:32 - train: epoch 0015, iter [02900, 05004], lr: 0.099406, loss: 2.6808
2022-07-09 17:22:06 - train: epoch 0015, iter [03000, 05004], lr: 0.099403, loss: 2.5753
2022-07-09 17:22:39 - train: epoch 0015, iter [03100, 05004], lr: 0.099401, loss: 2.7564
2022-07-09 17:23:13 - train: epoch 0015, iter [03200, 05004], lr: 0.099398, loss: 2.7644
2022-07-09 17:23:47 - train: epoch 0015, iter [03300, 05004], lr: 0.099396, loss: 2.4961
2022-07-09 17:24:21 - train: epoch 0015, iter [03400, 05004], lr: 0.099393, loss: 2.7601
2022-07-09 17:24:54 - train: epoch 0015, iter [03500, 05004], lr: 0.099391, loss: 2.9761
2022-07-09 17:25:28 - train: epoch 0015, iter [03600, 05004], lr: 0.099388, loss: 3.0043
2022-07-09 17:26:01 - train: epoch 0015, iter [03700, 05004], lr: 0.099386, loss: 2.7850
2022-07-09 17:26:35 - train: epoch 0015, iter [03800, 05004], lr: 0.099383, loss: 2.6926
2022-07-09 17:27:10 - train: epoch 0015, iter [03900, 05004], lr: 0.099381, loss: 3.0803
2022-07-09 17:27:43 - train: epoch 0015, iter [04000, 05004], lr: 0.099378, loss: 2.7801
2022-07-09 17:28:17 - train: epoch 0015, iter [04100, 05004], lr: 0.099376, loss: 3.0121
2022-07-09 17:28:49 - train: epoch 0015, iter [04200, 05004], lr: 0.099373, loss: 2.5841
2022-07-09 17:29:24 - train: epoch 0015, iter [04300, 05004], lr: 0.099371, loss: 2.7329
2022-07-09 17:29:57 - train: epoch 0015, iter [04400, 05004], lr: 0.099368, loss: 2.7201
2022-07-09 17:30:31 - train: epoch 0015, iter [04500, 05004], lr: 0.099365, loss: 2.7215
2022-07-09 17:31:05 - train: epoch 0015, iter [04600, 05004], lr: 0.099363, loss: 2.7209
2022-07-09 17:31:38 - train: epoch 0015, iter [04700, 05004], lr: 0.099360, loss: 2.8769
2022-07-09 17:32:12 - train: epoch 0015, iter [04800, 05004], lr: 0.099358, loss: 2.7207
2022-07-09 17:32:45 - train: epoch 0015, iter [04900, 05004], lr: 0.099355, loss: 2.8617
2022-07-09 17:33:17 - train: epoch 0015, iter [05000, 05004], lr: 0.099353, loss: 3.1133
2022-07-09 17:33:18 - train: epoch 015, train_loss: 2.8038
2022-07-09 17:34:32 - eval: epoch: 015, acc1: 53.408%, acc5: 78.716%, test_loss: 1.9911, per_image_load_time: 2.405ms, per_image_inference_time: 0.451ms
2022-07-09 17:34:33 - until epoch: 015, best_acc1: 53.408%
2022-07-09 17:34:33 - epoch 016 lr: 0.099352
2022-07-09 17:35:12 - train: epoch 0016, iter [00100, 05004], lr: 0.099350, loss: 2.8410
2022-07-09 17:35:47 - train: epoch 0016, iter [00200, 05004], lr: 0.099347, loss: 2.7340
2022-07-09 17:36:21 - train: epoch 0016, iter [00300, 05004], lr: 0.099345, loss: 2.9151
2022-07-09 17:36:55 - train: epoch 0016, iter [00400, 05004], lr: 0.099342, loss: 2.8142
2022-07-09 17:37:28 - train: epoch 0016, iter [00500, 05004], lr: 0.099340, loss: 2.5365
2022-07-09 17:38:02 - train: epoch 0016, iter [00600, 05004], lr: 0.099337, loss: 2.9953
2022-07-09 17:38:35 - train: epoch 0016, iter [00700, 05004], lr: 0.099334, loss: 2.5849
2022-07-09 17:39:09 - train: epoch 0016, iter [00800, 05004], lr: 0.099332, loss: 2.8103
2022-07-09 17:39:42 - train: epoch 0016, iter [00900, 05004], lr: 0.099329, loss: 2.6816
2022-07-09 17:40:16 - train: epoch 0016, iter [01000, 05004], lr: 0.099326, loss: 2.5728
2022-07-09 17:40:49 - train: epoch 0016, iter [01100, 05004], lr: 0.099324, loss: 2.6281
2022-07-09 17:41:22 - train: epoch 0016, iter [01200, 05004], lr: 0.099321, loss: 2.7352
2022-07-09 17:41:56 - train: epoch 0016, iter [01300, 05004], lr: 0.099319, loss: 3.0586
2022-07-09 17:42:29 - train: epoch 0016, iter [01400, 05004], lr: 0.099316, loss: 2.4993
2022-07-09 17:43:04 - train: epoch 0016, iter [01500, 05004], lr: 0.099313, loss: 2.9811
2022-07-09 17:43:38 - train: epoch 0016, iter [01600, 05004], lr: 0.099311, loss: 2.8527
2022-07-09 17:44:11 - train: epoch 0016, iter [01700, 05004], lr: 0.099308, loss: 2.5436
2022-07-09 17:44:45 - train: epoch 0016, iter [01800, 05004], lr: 0.099305, loss: 2.7201
2022-07-09 17:45:18 - train: epoch 0016, iter [01900, 05004], lr: 0.099303, loss: 2.6013
2022-07-09 17:45:52 - train: epoch 0016, iter [02000, 05004], lr: 0.099300, loss: 2.4636
2022-07-09 17:46:26 - train: epoch 0016, iter [02100, 05004], lr: 0.099297, loss: 2.8290
2022-07-09 17:47:00 - train: epoch 0016, iter [02200, 05004], lr: 0.099294, loss: 2.7935
2022-07-09 17:47:33 - train: epoch 0016, iter [02300, 05004], lr: 0.099292, loss: 3.0717
2022-07-09 17:48:07 - train: epoch 0016, iter [02400, 05004], lr: 0.099289, loss: 2.8596
2022-07-09 17:48:40 - train: epoch 0016, iter [02500, 05004], lr: 0.099286, loss: 2.8078
2022-07-09 17:49:15 - train: epoch 0016, iter [02600, 05004], lr: 0.099284, loss: 2.8288
2022-07-09 17:49:49 - train: epoch 0016, iter [02700, 05004], lr: 0.099281, loss: 2.8472
2022-07-09 17:50:22 - train: epoch 0016, iter [02800, 05004], lr: 0.099278, loss: 2.6647
2022-07-09 17:50:56 - train: epoch 0016, iter [02900, 05004], lr: 0.099275, loss: 2.7381
2022-07-09 17:51:30 - train: epoch 0016, iter [03000, 05004], lr: 0.099273, loss: 3.1451
2022-07-09 17:52:03 - train: epoch 0016, iter [03100, 05004], lr: 0.099270, loss: 3.1506
2022-07-09 17:52:38 - train: epoch 0016, iter [03200, 05004], lr: 0.099267, loss: 3.0462
2022-07-09 17:53:12 - train: epoch 0016, iter [03300, 05004], lr: 0.099265, loss: 2.8387
2022-07-09 17:53:45 - train: epoch 0016, iter [03400, 05004], lr: 0.099262, loss: 2.6625
2022-07-09 17:54:19 - train: epoch 0016, iter [03500, 05004], lr: 0.099259, loss: 2.6172
2022-07-09 17:54:53 - train: epoch 0016, iter [03600, 05004], lr: 0.099256, loss: 2.6282
2022-07-09 17:55:27 - train: epoch 0016, iter [03700, 05004], lr: 0.099253, loss: 2.9545
2022-07-09 17:56:01 - train: epoch 0016, iter [03800, 05004], lr: 0.099251, loss: 3.0722
2022-07-09 17:56:35 - train: epoch 0016, iter [03900, 05004], lr: 0.099248, loss: 2.9883
2022-07-09 17:57:09 - train: epoch 0016, iter [04000, 05004], lr: 0.099245, loss: 2.9365
2022-07-09 17:57:42 - train: epoch 0016, iter [04100, 05004], lr: 0.099242, loss: 2.6602
2022-07-09 17:58:16 - train: epoch 0016, iter [04200, 05004], lr: 0.099240, loss: 2.8218
2022-07-09 17:58:50 - train: epoch 0016, iter [04300, 05004], lr: 0.099237, loss: 2.5436
2022-07-09 17:59:24 - train: epoch 0016, iter [04400, 05004], lr: 0.099234, loss: 2.7935
2022-07-09 17:59:58 - train: epoch 0016, iter [04500, 05004], lr: 0.099231, loss: 2.9975
2022-07-09 18:00:32 - train: epoch 0016, iter [04600, 05004], lr: 0.099228, loss: 2.8055
2022-07-09 18:01:06 - train: epoch 0016, iter [04700, 05004], lr: 0.099226, loss: 2.9792
2022-07-09 18:01:40 - train: epoch 0016, iter [04800, 05004], lr: 0.099223, loss: 2.5741
2022-07-09 18:02:15 - train: epoch 0016, iter [04900, 05004], lr: 0.099220, loss: 2.6843
2022-07-09 18:02:47 - train: epoch 0016, iter [05000, 05004], lr: 0.099217, loss: 2.9411
2022-07-09 18:02:48 - train: epoch 016, train_loss: 2.7863
2022-07-09 18:04:03 - eval: epoch: 016, acc1: 50.900%, acc5: 76.608%, test_loss: 2.1310, per_image_load_time: 2.028ms, per_image_inference_time: 0.465ms
2022-07-09 18:04:03 - until epoch: 016, best_acc1: 53.408%
2022-07-09 18:04:03 - epoch 017 lr: 0.099217
2022-07-09 18:04:42 - train: epoch 0017, iter [00100, 05004], lr: 0.099214, loss: 2.8808
2022-07-09 18:05:16 - train: epoch 0017, iter [00200, 05004], lr: 0.099211, loss: 2.9634
2022-07-09 18:05:50 - train: epoch 0017, iter [00300, 05004], lr: 0.099208, loss: 2.8007
2022-07-09 18:06:25 - train: epoch 0017, iter [00400, 05004], lr: 0.099206, loss: 2.6601
2022-07-09 18:06:59 - train: epoch 0017, iter [00500, 05004], lr: 0.099203, loss: 2.7724
2022-07-09 18:07:33 - train: epoch 0017, iter [00600, 05004], lr: 0.099200, loss: 3.2593
2022-07-09 18:08:07 - train: epoch 0017, iter [00700, 05004], lr: 0.099197, loss: 2.7647
2022-07-09 18:08:42 - train: epoch 0017, iter [00800, 05004], lr: 0.099194, loss: 2.6816
2022-07-09 18:09:16 - train: epoch 0017, iter [00900, 05004], lr: 0.099191, loss: 2.8574
2022-07-09 18:09:51 - train: epoch 0017, iter [01000, 05004], lr: 0.099188, loss: 2.6029
2022-07-09 18:10:26 - train: epoch 0017, iter [01100, 05004], lr: 0.099185, loss: 2.9327
2022-07-09 18:10:59 - train: epoch 0017, iter [01200, 05004], lr: 0.099182, loss: 2.8958
2022-07-09 18:11:33 - train: epoch 0017, iter [01300, 05004], lr: 0.099180, loss: 2.7397
2022-07-09 18:12:09 - train: epoch 0017, iter [01400, 05004], lr: 0.099177, loss: 2.9119
2022-07-09 18:12:43 - train: epoch 0017, iter [01500, 05004], lr: 0.099174, loss: 2.6752
2022-07-09 18:13:17 - train: epoch 0017, iter [01600, 05004], lr: 0.099171, loss: 2.5923
2022-07-09 18:13:51 - train: epoch 0017, iter [01700, 05004], lr: 0.099168, loss: 2.8171
2022-07-09 18:14:25 - train: epoch 0017, iter [01800, 05004], lr: 0.099165, loss: 2.7238
2022-07-09 18:15:00 - train: epoch 0017, iter [01900, 05004], lr: 0.099162, loss: 2.5505
2022-07-09 18:15:34 - train: epoch 0017, iter [02000, 05004], lr: 0.099159, loss: 2.8570
2022-07-09 18:16:09 - train: epoch 0017, iter [02100, 05004], lr: 0.099156, loss: 2.8462
2022-07-09 18:16:43 - train: epoch 0017, iter [02200, 05004], lr: 0.099153, loss: 2.6632
2022-07-09 18:17:18 - train: epoch 0017, iter [02300, 05004], lr: 0.099150, loss: 2.6474
2022-07-09 18:17:51 - train: epoch 0017, iter [02400, 05004], lr: 0.099147, loss: 2.6559
2022-07-09 18:18:26 - train: epoch 0017, iter [02500, 05004], lr: 0.099144, loss: 2.9232
2022-07-09 18:19:01 - train: epoch 0017, iter [02600, 05004], lr: 0.099141, loss: 2.5687
2022-07-09 18:19:34 - train: epoch 0017, iter [02700, 05004], lr: 0.099138, loss: 2.8711
2022-07-09 18:20:09 - train: epoch 0017, iter [02800, 05004], lr: 0.099135, loss: 3.0358
2022-07-09 18:20:43 - train: epoch 0017, iter [02900, 05004], lr: 0.099132, loss: 2.9122
2022-07-09 18:21:17 - train: epoch 0017, iter [03000, 05004], lr: 0.099129, loss: 2.6468
2022-07-09 18:21:52 - train: epoch 0017, iter [03100, 05004], lr: 0.099126, loss: 2.8896
2022-07-09 18:22:26 - train: epoch 0017, iter [03200, 05004], lr: 0.099123, loss: 2.5892
2022-07-09 18:23:00 - train: epoch 0017, iter [03300, 05004], lr: 0.099120, loss: 2.7345
2022-07-09 18:23:34 - train: epoch 0017, iter [03400, 05004], lr: 0.099117, loss: 2.5168
2022-07-09 18:24:08 - train: epoch 0017, iter [03500, 05004], lr: 0.099114, loss: 2.8121
2022-07-09 18:24:43 - train: epoch 0017, iter [03600, 05004], lr: 0.099111, loss: 2.9941
2022-07-09 18:25:17 - train: epoch 0017, iter [03700, 05004], lr: 0.099108, loss: 2.7629
2022-07-09 18:25:51 - train: epoch 0017, iter [03800, 05004], lr: 0.099105, loss: 2.8710
2022-07-09 18:26:26 - train: epoch 0017, iter [03900, 05004], lr: 0.099102, loss: 2.6600
2022-07-09 18:27:00 - train: epoch 0017, iter [04000, 05004], lr: 0.099099, loss: 2.6974
2022-07-09 18:27:34 - train: epoch 0017, iter [04100, 05004], lr: 0.099096, loss: 2.7199
2022-07-09 18:28:09 - train: epoch 0017, iter [04200, 05004], lr: 0.099093, loss: 2.6409
2022-07-09 18:28:43 - train: epoch 0017, iter [04300, 05004], lr: 0.099090, loss: 2.7340
2022-07-09 18:29:17 - train: epoch 0017, iter [04400, 05004], lr: 0.099087, loss: 2.7958
2022-07-09 18:29:52 - train: epoch 0017, iter [04500, 05004], lr: 0.099084, loss: 2.7062
2022-07-09 18:30:27 - train: epoch 0017, iter [04600, 05004], lr: 0.099081, loss: 2.8857
2022-07-09 18:31:01 - train: epoch 0017, iter [04700, 05004], lr: 0.099078, loss: 2.8755
2022-07-09 18:31:35 - train: epoch 0017, iter [04800, 05004], lr: 0.099075, loss: 2.9194
2022-07-09 18:32:09 - train: epoch 0017, iter [04900, 05004], lr: 0.099072, loss: 2.5011
2022-07-09 18:32:43 - train: epoch 0017, iter [05000, 05004], lr: 0.099069, loss: 2.4262
2022-07-09 18:32:44 - train: epoch 017, train_loss: 2.7699
2022-07-09 18:33:58 - eval: epoch: 017, acc1: 54.048%, acc5: 79.380%, test_loss: 1.9595, per_image_load_time: 1.299ms, per_image_inference_time: 0.465ms
2022-07-09 18:33:59 - until epoch: 017, best_acc1: 54.048%
2022-07-09 18:33:59 - epoch 018 lr: 0.099068
2022-07-09 18:34:38 - train: epoch 0018, iter [00100, 05004], lr: 0.099065, loss: 2.7824
2022-07-09 18:35:13 - train: epoch 0018, iter [00200, 05004], lr: 0.099062, loss: 2.9883
2022-07-09 18:35:47 - train: epoch 0018, iter [00300, 05004], lr: 0.099059, loss: 2.8644
2022-07-09 18:36:20 - train: epoch 0018, iter [00400, 05004], lr: 0.099056, loss: 2.7583
2022-07-09 18:36:54 - train: epoch 0018, iter [00500, 05004], lr: 0.099053, loss: 2.7499
2022-07-09 18:37:29 - train: epoch 0018, iter [00600, 05004], lr: 0.099050, loss: 2.7164
2022-07-09 18:38:02 - train: epoch 0018, iter [00700, 05004], lr: 0.099047, loss: 2.4963
2022-07-09 18:38:36 - train: epoch 0018, iter [00800, 05004], lr: 0.099044, loss: 2.7561
2022-07-09 18:39:11 - train: epoch 0018, iter [00900, 05004], lr: 0.099040, loss: 2.9207
2022-07-09 18:39:44 - train: epoch 0018, iter [01000, 05004], lr: 0.099037, loss: 2.5494
2022-07-09 18:40:19 - train: epoch 0018, iter [01100, 05004], lr: 0.099034, loss: 3.0319
2022-07-09 18:40:53 - train: epoch 0018, iter [01200, 05004], lr: 0.099031, loss: 2.7342
2022-07-09 18:41:27 - train: epoch 0018, iter [01300, 05004], lr: 0.099028, loss: 3.0355
2022-07-09 18:42:01 - train: epoch 0018, iter [01400, 05004], lr: 0.099025, loss: 2.8030
2022-07-09 18:42:36 - train: epoch 0018, iter [01500, 05004], lr: 0.099022, loss: 2.9611
2022-07-09 18:43:09 - train: epoch 0018, iter [01600, 05004], lr: 0.099018, loss: 2.8769
2022-07-09 18:43:43 - train: epoch 0018, iter [01700, 05004], lr: 0.099015, loss: 2.5572
2022-07-09 18:44:18 - train: epoch 0018, iter [01800, 05004], lr: 0.099012, loss: 2.6197
2022-07-09 18:44:53 - train: epoch 0018, iter [01900, 05004], lr: 0.099009, loss: 2.7194
2022-07-09 18:45:26 - train: epoch 0018, iter [02000, 05004], lr: 0.099006, loss: 2.9963
2022-07-09 18:46:01 - train: epoch 0018, iter [02100, 05004], lr: 0.099002, loss: 3.0025
2022-07-09 18:46:35 - train: epoch 0018, iter [02200, 05004], lr: 0.098999, loss: 2.7676
2022-07-09 18:47:10 - train: epoch 0018, iter [02300, 05004], lr: 0.098996, loss: 2.7033
2022-07-09 18:47:44 - train: epoch 0018, iter [02400, 05004], lr: 0.098993, loss: 2.6774
2022-07-09 18:48:18 - train: epoch 0018, iter [02500, 05004], lr: 0.098990, loss: 2.5243
2022-07-09 18:48:51 - train: epoch 0018, iter [02600, 05004], lr: 0.098986, loss: 2.4501
2022-07-09 18:49:26 - train: epoch 0018, iter [02700, 05004], lr: 0.098983, loss: 2.7909
2022-07-09 18:50:00 - train: epoch 0018, iter [02800, 05004], lr: 0.098980, loss: 2.4484
2022-07-09 18:50:34 - train: epoch 0018, iter [02900, 05004], lr: 0.098977, loss: 2.8902
2022-07-09 18:51:09 - train: epoch 0018, iter [03000, 05004], lr: 0.098973, loss: 2.6794
2022-07-09 18:51:43 - train: epoch 0018, iter [03100, 05004], lr: 0.098970, loss: 3.1032
2022-07-09 18:52:17 - train: epoch 0018, iter [03200, 05004], lr: 0.098967, loss: 2.5058
2022-07-09 18:52:51 - train: epoch 0018, iter [03300, 05004], lr: 0.098964, loss: 2.7936
2022-07-09 18:53:25 - train: epoch 0018, iter [03400, 05004], lr: 0.098960, loss: 2.6805
2022-07-09 18:54:00 - train: epoch 0018, iter [03500, 05004], lr: 0.098957, loss: 3.0330
2022-07-09 18:54:35 - train: epoch 0018, iter [03600, 05004], lr: 0.098954, loss: 2.7979
2022-07-09 18:55:08 - train: epoch 0018, iter [03700, 05004], lr: 0.098951, loss: 3.1572
2022-07-09 18:55:44 - train: epoch 0018, iter [03800, 05004], lr: 0.098947, loss: 2.7259
2022-07-09 18:56:17 - train: epoch 0018, iter [03900, 05004], lr: 0.098944, loss: 2.8737
2022-07-09 18:56:52 - train: epoch 0018, iter [04000, 05004], lr: 0.098941, loss: 2.6215
2022-07-09 18:57:26 - train: epoch 0018, iter [04100, 05004], lr: 0.098937, loss: 2.5467
2022-07-09 18:58:01 - train: epoch 0018, iter [04200, 05004], lr: 0.098934, loss: 2.8575
2022-07-09 18:58:35 - train: epoch 0018, iter [04300, 05004], lr: 0.098931, loss: 2.4798
2022-07-09 18:59:09 - train: epoch 0018, iter [04400, 05004], lr: 0.098928, loss: 2.9884
2022-07-09 18:59:43 - train: epoch 0018, iter [04500, 05004], lr: 0.098924, loss: 2.7560
2022-07-09 19:00:18 - train: epoch 0018, iter [04600, 05004], lr: 0.098921, loss: 2.6178
2022-07-09 19:00:52 - train: epoch 0018, iter [04700, 05004], lr: 0.098918, loss: 2.8092
2022-07-09 19:01:26 - train: epoch 0018, iter [04800, 05004], lr: 0.098914, loss: 2.8936
2022-07-09 19:02:00 - train: epoch 0018, iter [04900, 05004], lr: 0.098911, loss: 2.7129
2022-07-09 19:02:33 - train: epoch 0018, iter [05000, 05004], lr: 0.098908, loss: 2.8633
2022-07-09 19:02:34 - train: epoch 018, train_loss: 2.7469
2022-07-09 19:03:48 - eval: epoch: 018, acc1: 51.398%, acc5: 76.966%, test_loss: 2.1125, per_image_load_time: 1.212ms, per_image_inference_time: 0.468ms
2022-07-09 19:03:48 - until epoch: 018, best_acc1: 54.048%
2022-07-09 19:03:48 - epoch 019 lr: 0.098907
2022-07-09 19:04:27 - train: epoch 0019, iter [00100, 05004], lr: 0.098904, loss: 2.4593
2022-07-09 19:05:01 - train: epoch 0019, iter [00200, 05004], lr: 0.098901, loss: 3.3405
2022-07-09 19:05:35 - train: epoch 0019, iter [00300, 05004], lr: 0.098897, loss: 3.1838
2022-07-09 19:06:08 - train: epoch 0019, iter [00400, 05004], lr: 0.098894, loss: 2.6887
2022-07-09 19:06:42 - train: epoch 0019, iter [00500, 05004], lr: 0.098891, loss: 2.6911
2022-07-09 19:07:15 - train: epoch 0019, iter [00600, 05004], lr: 0.098887, loss: 2.8463
2022-07-09 19:07:49 - train: epoch 0019, iter [00700, 05004], lr: 0.098884, loss: 2.5309
2022-07-09 19:08:23 - train: epoch 0019, iter [00800, 05004], lr: 0.098880, loss: 2.8526
2022-07-09 19:08:57 - train: epoch 0019, iter [00900, 05004], lr: 0.098877, loss: 2.8520
2022-07-09 19:09:31 - train: epoch 0019, iter [01000, 05004], lr: 0.098874, loss: 2.7408
2022-07-09 19:10:04 - train: epoch 0019, iter [01100, 05004], lr: 0.098870, loss: 2.7342
2022-07-09 19:10:38 - train: epoch 0019, iter [01200, 05004], lr: 0.098867, loss: 2.8312
2022-07-09 19:11:12 - train: epoch 0019, iter [01300, 05004], lr: 0.098863, loss: 3.0376
2022-07-09 19:11:46 - train: epoch 0019, iter [01400, 05004], lr: 0.098860, loss: 2.6207
2022-07-09 19:12:20 - train: epoch 0019, iter [01500, 05004], lr: 0.098857, loss: 2.9687
2022-07-09 19:12:53 - train: epoch 0019, iter [01600, 05004], lr: 0.098853, loss: 2.7237
2022-07-09 19:13:28 - train: epoch 0019, iter [01700, 05004], lr: 0.098850, loss: 3.0237
2022-07-09 19:14:02 - train: epoch 0019, iter [01800, 05004], lr: 0.098846, loss: 2.6655
2022-07-09 19:14:35 - train: epoch 0019, iter [01900, 05004], lr: 0.098843, loss: 2.8915
2022-07-09 19:15:10 - train: epoch 0019, iter [02000, 05004], lr: 0.098839, loss: 2.6721
2022-07-09 19:15:43 - train: epoch 0019, iter [02100, 05004], lr: 0.098836, loss: 2.5142
2022-07-09 19:16:17 - train: epoch 0019, iter [02200, 05004], lr: 0.098833, loss: 2.7046
2022-07-09 19:16:51 - train: epoch 0019, iter [02300, 05004], lr: 0.098829, loss: 2.5691
2022-07-09 19:17:25 - train: epoch 0019, iter [02400, 05004], lr: 0.098826, loss: 2.7914
2022-07-09 19:17:59 - train: epoch 0019, iter [02500, 05004], lr: 0.098822, loss: 2.6373
2022-07-09 19:18:33 - train: epoch 0019, iter [02600, 05004], lr: 0.098819, loss: 2.6966
2022-07-09 19:19:07 - train: epoch 0019, iter [02700, 05004], lr: 0.098815, loss: 2.9221
2022-07-09 19:19:41 - train: epoch 0019, iter [02800, 05004], lr: 0.098812, loss: 2.7642
2022-07-09 19:20:15 - train: epoch 0019, iter [02900, 05004], lr: 0.098808, loss: 2.6614
2022-07-09 19:20:49 - train: epoch 0019, iter [03000, 05004], lr: 0.098805, loss: 3.0342
2022-07-09 19:21:23 - train: epoch 0019, iter [03100, 05004], lr: 0.098801, loss: 2.6926
2022-07-09 19:21:56 - train: epoch 0019, iter [03200, 05004], lr: 0.098798, loss: 2.3512
2022-07-09 19:22:30 - train: epoch 0019, iter [03300, 05004], lr: 0.098794, loss: 2.5337
2022-07-09 19:23:04 - train: epoch 0019, iter [03400, 05004], lr: 0.098791, loss: 2.6524
2022-07-09 19:23:38 - train: epoch 0019, iter [03500, 05004], lr: 0.098787, loss: 2.7781
2022-07-09 19:24:11 - train: epoch 0019, iter [03600, 05004], lr: 0.098784, loss: 2.6596
2022-07-09 19:24:45 - train: epoch 0019, iter [03700, 05004], lr: 0.098780, loss: 2.6344
2022-07-09 19:25:19 - train: epoch 0019, iter [03800, 05004], lr: 0.098777, loss: 2.9710
2022-07-09 19:25:54 - train: epoch 0019, iter [03900, 05004], lr: 0.098773, loss: 2.6447
2022-07-09 19:26:27 - train: epoch 0019, iter [04000, 05004], lr: 0.098769, loss: 2.4844
2022-07-09 19:27:01 - train: epoch 0019, iter [04100, 05004], lr: 0.098766, loss: 2.7583
2022-07-09 19:27:35 - train: epoch 0019, iter [04200, 05004], lr: 0.098762, loss: 2.6433
2022-07-09 19:28:09 - train: epoch 0019, iter [04300, 05004], lr: 0.098759, loss: 2.7475
2022-07-09 19:28:43 - train: epoch 0019, iter [04400, 05004], lr: 0.098755, loss: 2.9342
2022-07-09 19:29:17 - train: epoch 0019, iter [04500, 05004], lr: 0.098752, loss: 2.9160
2022-07-09 19:29:51 - train: epoch 0019, iter [04600, 05004], lr: 0.098748, loss: 2.8010
2022-07-09 19:30:25 - train: epoch 0019, iter [04700, 05004], lr: 0.098744, loss: 2.7115
2022-07-09 19:30:59 - train: epoch 0019, iter [04800, 05004], lr: 0.098741, loss: 2.6692
2022-07-09 19:31:33 - train: epoch 0019, iter [04900, 05004], lr: 0.098737, loss: 2.7785
2022-07-09 19:32:06 - train: epoch 0019, iter [05000, 05004], lr: 0.098734, loss: 2.8880
2022-07-09 19:32:07 - train: epoch 019, train_loss: 2.7380
2022-07-09 19:33:22 - eval: epoch: 019, acc1: 53.294%, acc5: 78.534%, test_loss: 1.9967, per_image_load_time: 1.553ms, per_image_inference_time: 0.468ms
2022-07-09 19:33:22 - until epoch: 019, best_acc1: 54.048%
2022-07-09 19:33:22 - epoch 020 lr: 0.098734
2022-07-09 19:34:01 - train: epoch 0020, iter [00100, 05004], lr: 0.098730, loss: 2.9321
2022-07-09 19:34:36 - train: epoch 0020, iter [00200, 05004], lr: 0.098726, loss: 2.5513
2022-07-09 19:35:10 - train: epoch 0020, iter [00300, 05004], lr: 0.098723, loss: 2.7594
2022-07-09 19:35:43 - train: epoch 0020, iter [00400, 05004], lr: 0.098719, loss: 2.5446
2022-07-09 19:36:17 - train: epoch 0020, iter [00500, 05004], lr: 0.098715, loss: 2.6484
2022-07-09 19:36:52 - train: epoch 0020, iter [00600, 05004], lr: 0.098712, loss: 3.0208
2022-07-09 19:37:25 - train: epoch 0020, iter [00700, 05004], lr: 0.098708, loss: 2.5825
2022-07-09 19:37:59 - train: epoch 0020, iter [00800, 05004], lr: 0.098705, loss: 2.8175
2022-07-09 19:38:33 - train: epoch 0020, iter [00900, 05004], lr: 0.098701, loss: 2.9623
2022-07-09 19:39:06 - train: epoch 0020, iter [01000, 05004], lr: 0.098697, loss: 2.8347
2022-07-09 19:39:40 - train: epoch 0020, iter [01100, 05004], lr: 0.098694, loss: 2.6558
2022-07-09 19:40:13 - train: epoch 0020, iter [01200, 05004], lr: 0.098690, loss: 2.5696
2022-07-09 19:40:47 - train: epoch 0020, iter [01300, 05004], lr: 0.098686, loss: 2.5207
2022-07-09 19:41:20 - train: epoch 0020, iter [01400, 05004], lr: 0.098683, loss: 2.8176
2022-07-09 19:41:54 - train: epoch 0020, iter [01500, 05004], lr: 0.098679, loss: 2.9187
2022-07-09 19:42:28 - train: epoch 0020, iter [01600, 05004], lr: 0.098675, loss: 2.6034
2022-07-09 19:43:02 - train: epoch 0020, iter [01700, 05004], lr: 0.098672, loss: 2.4409
2022-07-09 19:43:35 - train: epoch 0020, iter [01800, 05004], lr: 0.098668, loss: 2.7747
2022-07-09 19:44:09 - train: epoch 0020, iter [01900, 05004], lr: 0.098664, loss: 2.3694
2022-07-09 19:44:42 - train: epoch 0020, iter [02000, 05004], lr: 0.098661, loss: 2.8796
2022-07-09 19:45:16 - train: epoch 0020, iter [02100, 05004], lr: 0.098657, loss: 2.7348
2022-07-09 19:45:50 - train: epoch 0020, iter [02200, 05004], lr: 0.098653, loss: 2.4756
2022-07-09 19:46:24 - train: epoch 0020, iter [02300, 05004], lr: 0.098649, loss: 2.7286
2022-07-09 19:46:57 - train: epoch 0020, iter [02400, 05004], lr: 0.098646, loss: 2.8547
2022-07-09 19:47:30 - train: epoch 0020, iter [02500, 05004], lr: 0.098642, loss: 2.7901
2022-07-09 19:48:05 - train: epoch 0020, iter [02600, 05004], lr: 0.098638, loss: 2.7222
2022-07-09 19:48:38 - train: epoch 0020, iter [02700, 05004], lr: 0.098635, loss: 2.7582
2022-07-09 19:49:13 - train: epoch 0020, iter [02800, 05004], lr: 0.098631, loss: 2.7387
2022-07-09 19:49:46 - train: epoch 0020, iter [02900, 05004], lr: 0.098627, loss: 2.7666
2022-07-09 19:50:20 - train: epoch 0020, iter [03000, 05004], lr: 0.098623, loss: 2.5774
2022-07-09 19:50:54 - train: epoch 0020, iter [03100, 05004], lr: 0.098620, loss: 2.8995
2022-07-09 19:51:28 - train: epoch 0020, iter [03200, 05004], lr: 0.098616, loss: 3.0650
2022-07-09 19:52:01 - train: epoch 0020, iter [03300, 05004], lr: 0.098612, loss: 2.7531
2022-07-09 19:52:35 - train: epoch 0020, iter [03400, 05004], lr: 0.098608, loss: 2.7895
2022-07-09 19:53:10 - train: epoch 0020, iter [03500, 05004], lr: 0.098604, loss: 2.6955
2022-07-09 19:53:43 - train: epoch 0020, iter [03600, 05004], lr: 0.098601, loss: 2.6002
2022-07-09 19:54:17 - train: epoch 0020, iter [03700, 05004], lr: 0.098597, loss: 2.5641
2022-07-09 19:54:52 - train: epoch 0020, iter [03800, 05004], lr: 0.098593, loss: 2.6201
2022-07-09 19:55:25 - train: epoch 0020, iter [03900, 05004], lr: 0.098589, loss: 2.9806
2022-07-09 19:55:58 - train: epoch 0020, iter [04000, 05004], lr: 0.098586, loss: 2.5764
2022-07-09 19:56:32 - train: epoch 0020, iter [04100, 05004], lr: 0.098582, loss: 2.6593
2022-07-09 19:57:07 - train: epoch 0020, iter [04200, 05004], lr: 0.098578, loss: 2.5222
2022-07-09 19:57:40 - train: epoch 0020, iter [04300, 05004], lr: 0.098574, loss: 2.7073
2022-07-09 19:58:15 - train: epoch 0020, iter [04400, 05004], lr: 0.098570, loss: 2.7428
2022-07-09 19:58:49 - train: epoch 0020, iter [04500, 05004], lr: 0.098566, loss: 2.9075
2022-07-09 19:59:23 - train: epoch 0020, iter [04600, 05004], lr: 0.098563, loss: 2.5352
2022-07-09 19:59:57 - train: epoch 0020, iter [04700, 05004], lr: 0.098559, loss: 2.5110
2022-07-09 20:00:31 - train: epoch 0020, iter [04800, 05004], lr: 0.098555, loss: 2.7097
2022-07-09 20:01:05 - train: epoch 0020, iter [04900, 05004], lr: 0.098551, loss: 2.8725
2022-07-09 20:01:38 - train: epoch 0020, iter [05000, 05004], lr: 0.098547, loss: 2.4756
2022-07-09 20:01:39 - train: epoch 020, train_loss: 2.7187
2022-07-09 20:02:54 - eval: epoch: 020, acc1: 51.732%, acc5: 76.984%, test_loss: 2.0986, per_image_load_time: 2.405ms, per_image_inference_time: 0.454ms
2022-07-09 20:02:55 - until epoch: 020, best_acc1: 54.048%
2022-07-09 20:02:55 - epoch 021 lr: 0.098547
2022-07-09 20:03:33 - train: epoch 0021, iter [00100, 05004], lr: 0.098543, loss: 2.6531
2022-07-09 20:04:08 - train: epoch 0021, iter [00200, 05004], lr: 0.098539, loss: 2.6986
2022-07-09 20:04:41 - train: epoch 0021, iter [00300, 05004], lr: 0.098536, loss: 2.4778
2022-07-09 20:05:14 - train: epoch 0021, iter [00400, 05004], lr: 0.098532, loss: 2.8943
2022-07-09 20:05:49 - train: epoch 0021, iter [00500, 05004], lr: 0.098528, loss: 2.6468
2022-07-09 20:06:22 - train: epoch 0021, iter [00600, 05004], lr: 0.098524, loss: 2.5601
2022-07-09 20:06:57 - train: epoch 0021, iter [00700, 05004], lr: 0.098520, loss: 2.5348
2022-07-09 20:07:31 - train: epoch 0021, iter [00800, 05004], lr: 0.098516, loss: 3.0877
2022-07-09 20:08:05 - train: epoch 0021, iter [00900, 05004], lr: 0.098512, loss: 2.8285
2022-07-09 20:08:39 - train: epoch 0021, iter [01000, 05004], lr: 0.098508, loss: 2.5327
2022-07-09 20:09:14 - train: epoch 0021, iter [01100, 05004], lr: 0.098504, loss: 2.7576
2022-07-09 20:09:47 - train: epoch 0021, iter [01200, 05004], lr: 0.098500, loss: 2.4324
2022-07-09 20:10:22 - train: epoch 0021, iter [01300, 05004], lr: 0.098497, loss: 2.4682
2022-07-09 20:10:56 - train: epoch 0021, iter [01400, 05004], lr: 0.098493, loss: 2.7055
2022-07-09 20:11:31 - train: epoch 0021, iter [01500, 05004], lr: 0.098489, loss: 2.4770
2022-07-09 20:12:04 - train: epoch 0021, iter [01600, 05004], lr: 0.098485, loss: 2.7265
2022-07-09 20:12:39 - train: epoch 0021, iter [01700, 05004], lr: 0.098481, loss: 2.5442
2022-07-09 20:13:12 - train: epoch 0021, iter [01800, 05004], lr: 0.098477, loss: 2.5712
2022-07-09 20:13:47 - train: epoch 0021, iter [01900, 05004], lr: 0.098473, loss: 2.7772
2022-07-09 20:14:21 - train: epoch 0021, iter [02000, 05004], lr: 0.098469, loss: 2.9227
2022-07-09 20:14:55 - train: epoch 0021, iter [02100, 05004], lr: 0.098465, loss: 2.5777
2022-07-09 20:15:29 - train: epoch 0021, iter [02200, 05004], lr: 0.098461, loss: 2.6772
2022-07-09 20:16:04 - train: epoch 0021, iter [02300, 05004], lr: 0.098457, loss: 2.5361
2022-07-09 20:16:38 - train: epoch 0021, iter [02400, 05004], lr: 0.098453, loss: 2.6234
2022-07-09 20:17:12 - train: epoch 0021, iter [02500, 05004], lr: 0.098449, loss: 2.6314
2022-07-09 20:17:46 - train: epoch 0021, iter [02600, 05004], lr: 0.098445, loss: 2.8079
2022-07-09 20:18:21 - train: epoch 0021, iter [02700, 05004], lr: 0.098441, loss: 2.5267
2022-07-09 20:18:55 - train: epoch 0021, iter [02800, 05004], lr: 0.098437, loss: 2.7746
2022-07-09 20:19:30 - train: epoch 0021, iter [02900, 05004], lr: 0.098433, loss: 2.5914
2022-07-09 20:20:04 - train: epoch 0021, iter [03000, 05004], lr: 0.098429, loss: 2.7377
2022-07-09 20:20:38 - train: epoch 0021, iter [03100, 05004], lr: 0.098425, loss: 2.8865
2022-07-09 20:21:11 - train: epoch 0021, iter [03200, 05004], lr: 0.098421, loss: 2.7327
2022-07-09 20:21:45 - train: epoch 0021, iter [03300, 05004], lr: 0.098417, loss: 2.9805
2022-07-09 20:22:19 - train: epoch 0021, iter [03400, 05004], lr: 0.098413, loss: 2.8199
2022-07-09 20:22:54 - train: epoch 0021, iter [03500, 05004], lr: 0.098409, loss: 2.6589
2022-07-09 20:23:27 - train: epoch 0021, iter [03600, 05004], lr: 0.098405, loss: 2.7715
2022-07-09 20:24:02 - train: epoch 0021, iter [03700, 05004], lr: 0.098401, loss: 2.6437
2022-07-09 20:24:36 - train: epoch 0021, iter [03800, 05004], lr: 0.098397, loss: 2.3805
2022-07-09 20:25:10 - train: epoch 0021, iter [03900, 05004], lr: 0.098393, loss: 2.6851
2022-07-09 20:25:45 - train: epoch 0021, iter [04000, 05004], lr: 0.098389, loss: 2.9635
2022-07-09 20:26:19 - train: epoch 0021, iter [04100, 05004], lr: 0.098385, loss: 2.7015
2022-07-09 20:26:54 - train: epoch 0021, iter [04200, 05004], lr: 0.098381, loss: 2.7522
2022-07-09 20:27:27 - train: epoch 0021, iter [04300, 05004], lr: 0.098377, loss: 2.6134
2022-07-09 20:28:01 - train: epoch 0021, iter [04400, 05004], lr: 0.098373, loss: 2.7002
2022-07-09 20:28:35 - train: epoch 0021, iter [04500, 05004], lr: 0.098369, loss: 2.9147
2022-07-09 20:29:09 - train: epoch 0021, iter [04600, 05004], lr: 0.098365, loss: 2.6858
2022-07-09 20:29:43 - train: epoch 0021, iter [04700, 05004], lr: 0.098360, loss: 2.7835
2022-07-09 20:30:18 - train: epoch 0021, iter [04800, 05004], lr: 0.098356, loss: 2.7442
2022-07-09 20:30:51 - train: epoch 0021, iter [04900, 05004], lr: 0.098352, loss: 2.4743
2022-07-09 20:31:24 - train: epoch 0021, iter [05000, 05004], lr: 0.098348, loss: 2.6446
2022-07-09 20:31:25 - train: epoch 021, train_loss: 2.7100
2022-07-09 20:32:39 - eval: epoch: 021, acc1: 54.250%, acc5: 79.524%, test_loss: 1.9434, per_image_load_time: 2.439ms, per_image_inference_time: 0.445ms
2022-07-09 20:32:39 - until epoch: 021, best_acc1: 54.250%
2022-07-09 20:32:39 - epoch 022 lr: 0.098348
2022-07-09 20:33:18 - train: epoch 0022, iter [00100, 05004], lr: 0.098344, loss: 2.4060
2022-07-09 20:33:52 - train: epoch 0022, iter [00200, 05004], lr: 0.098340, loss: 2.4533
2022-07-09 20:34:27 - train: epoch 0022, iter [00300, 05004], lr: 0.098336, loss: 2.5275
2022-07-09 20:35:01 - train: epoch 0022, iter [00400, 05004], lr: 0.098332, loss: 2.5749
2022-07-09 20:35:35 - train: epoch 0022, iter [00500, 05004], lr: 0.098327, loss: 2.6161
2022-07-09 20:36:09 - train: epoch 0022, iter [00600, 05004], lr: 0.098323, loss: 2.7042
2022-07-09 20:36:43 - train: epoch 0022, iter [00700, 05004], lr: 0.098319, loss: 2.5810
2022-07-09 20:37:16 - train: epoch 0022, iter [00800, 05004], lr: 0.098315, loss: 2.8847
2022-07-09 20:37:51 - train: epoch 0022, iter [00900, 05004], lr: 0.098311, loss: 2.8144
2022-07-09 20:38:24 - train: epoch 0022, iter [01000, 05004], lr: 0.098307, loss: 2.7281
2022-07-09 20:38:58 - train: epoch 0022, iter [01100, 05004], lr: 0.098303, loss: 2.6511
2022-07-09 20:39:32 - train: epoch 0022, iter [01200, 05004], lr: 0.098298, loss: 2.4035
2022-07-09 20:40:06 - train: epoch 0022, iter [01300, 05004], lr: 0.098294, loss: 2.7811
2022-07-09 20:40:39 - train: epoch 0022, iter [01400, 05004], lr: 0.098290, loss: 2.9606
2022-07-09 20:41:13 - train: epoch 0022, iter [01500, 05004], lr: 0.098286, loss: 2.5003
2022-07-09 20:41:46 - train: epoch 0022, iter [01600, 05004], lr: 0.098282, loss: 2.6166
2022-07-09 20:42:21 - train: epoch 0022, iter [01700, 05004], lr: 0.098278, loss: 2.4764
2022-07-09 20:42:54 - train: epoch 0022, iter [01800, 05004], lr: 0.098273, loss: 2.7774
2022-07-09 20:43:28 - train: epoch 0022, iter [01900, 05004], lr: 0.098269, loss: 2.3681
2022-07-09 20:44:02 - train: epoch 0022, iter [02000, 05004], lr: 0.098265, loss: 2.8642
2022-07-09 20:44:36 - train: epoch 0022, iter [02100, 05004], lr: 0.098261, loss: 2.8567
2022-07-09 20:45:10 - train: epoch 0022, iter [02200, 05004], lr: 0.098257, loss: 2.4684
2022-07-09 20:45:43 - train: epoch 0022, iter [02300, 05004], lr: 0.098252, loss: 2.8754
2022-07-09 20:46:17 - train: epoch 0022, iter [02400, 05004], lr: 0.098248, loss: 2.7670
2022-07-09 20:46:51 - train: epoch 0022, iter [02500, 05004], lr: 0.098244, loss: 2.5434
2022-07-09 20:47:25 - train: epoch 0022, iter [02600, 05004], lr: 0.098240, loss: 2.4966
2022-07-09 20:47:58 - train: epoch 0022, iter [02700, 05004], lr: 0.098235, loss: 2.5119
2022-07-09 20:48:32 - train: epoch 0022, iter [02800, 05004], lr: 0.098231, loss: 2.9794
2022-07-09 20:49:06 - train: epoch 0022, iter [02900, 05004], lr: 0.098227, loss: 2.5634
2022-07-09 20:49:40 - train: epoch 0022, iter [03000, 05004], lr: 0.098223, loss: 2.8523
2022-07-09 20:50:14 - train: epoch 0022, iter [03100, 05004], lr: 0.098218, loss: 2.9153
2022-07-09 20:50:47 - train: epoch 0022, iter [03200, 05004], lr: 0.098214, loss: 2.6058
2022-07-09 20:51:22 - train: epoch 0022, iter [03300, 05004], lr: 0.098210, loss: 2.7336
2022-07-09 20:51:55 - train: epoch 0022, iter [03400, 05004], lr: 0.098206, loss: 2.4956
2022-07-09 20:52:29 - train: epoch 0022, iter [03500, 05004], lr: 0.098201, loss: 2.8377
2022-07-09 20:53:04 - train: epoch 0022, iter [03600, 05004], lr: 0.098197, loss: 2.7149
2022-07-09 20:53:36 - train: epoch 0022, iter [03700, 05004], lr: 0.098193, loss: 2.7294
2022-07-09 20:54:11 - train: epoch 0022, iter [03800, 05004], lr: 0.098188, loss: 2.9153
2022-07-09 20:54:45 - train: epoch 0022, iter [03900, 05004], lr: 0.098184, loss: 2.7140
2022-07-09 20:55:18 - train: epoch 0022, iter [04000, 05004], lr: 0.098180, loss: 2.6063
2022-07-09 20:55:53 - train: epoch 0022, iter [04100, 05004], lr: 0.098176, loss: 2.4596
2022-07-09 20:56:27 - train: epoch 0022, iter [04200, 05004], lr: 0.098171, loss: 2.6248
2022-07-09 20:57:01 - train: epoch 0022, iter [04300, 05004], lr: 0.098167, loss: 2.9156
2022-07-09 20:57:35 - train: epoch 0022, iter [04400, 05004], lr: 0.098163, loss: 2.9466
2022-07-09 20:58:09 - train: epoch 0022, iter [04500, 05004], lr: 0.098158, loss: 2.7039
2022-07-09 20:58:43 - train: epoch 0022, iter [04600, 05004], lr: 0.098154, loss: 2.6735
2022-07-09 20:59:17 - train: epoch 0022, iter [04700, 05004], lr: 0.098150, loss: 2.7797
2022-07-09 20:59:51 - train: epoch 0022, iter [04800, 05004], lr: 0.098145, loss: 2.4676
2022-07-09 21:00:24 - train: epoch 0022, iter [04900, 05004], lr: 0.098141, loss: 2.5381
2022-07-09 21:00:57 - train: epoch 0022, iter [05000, 05004], lr: 0.098137, loss: 2.5224
2022-07-09 21:00:58 - train: epoch 022, train_loss: 2.6976
2022-07-09 21:02:13 - eval: epoch: 022, acc1: 51.770%, acc5: 77.216%, test_loss: 2.0993, per_image_load_time: 1.786ms, per_image_inference_time: 0.477ms
2022-07-09 21:02:13 - until epoch: 022, best_acc1: 54.250%
2022-07-09 21:02:13 - epoch 023 lr: 0.098136
2022-07-09 21:02:54 - train: epoch 0023, iter [00100, 05004], lr: 0.098132, loss: 2.5910
2022-07-09 21:03:27 - train: epoch 0023, iter [00200, 05004], lr: 0.098128, loss: 2.4980
2022-07-09 21:04:01 - train: epoch 0023, iter [00300, 05004], lr: 0.098123, loss: 2.4647
2022-07-09 21:04:34 - train: epoch 0023, iter [00400, 05004], lr: 0.098119, loss: 2.7069
2022-07-09 21:05:08 - train: epoch 0023, iter [00500, 05004], lr: 0.098115, loss: 2.7286
2022-07-09 21:05:42 - train: epoch 0023, iter [00600, 05004], lr: 0.098110, loss: 2.4668
2022-07-09 21:06:16 - train: epoch 0023, iter [00700, 05004], lr: 0.098106, loss: 2.4014
2022-07-09 21:06:50 - train: epoch 0023, iter [00800, 05004], lr: 0.098101, loss: 2.6308
2022-07-09 21:07:24 - train: epoch 0023, iter [00900, 05004], lr: 0.098097, loss: 2.6596
2022-07-09 21:07:59 - train: epoch 0023, iter [01000, 05004], lr: 0.098093, loss: 2.2778
2022-07-09 21:08:33 - train: epoch 0023, iter [01100, 05004], lr: 0.098088, loss: 2.9661
2022-07-09 21:09:07 - train: epoch 0023, iter [01200, 05004], lr: 0.098084, loss: 2.5064
2022-07-09 21:09:41 - train: epoch 0023, iter [01300, 05004], lr: 0.098079, loss: 2.7213
2022-07-09 21:10:15 - train: epoch 0023, iter [01400, 05004], lr: 0.098075, loss: 2.7166
2022-07-09 21:10:49 - train: epoch 0023, iter [01500, 05004], lr: 0.098071, loss: 2.5041
2022-07-09 21:11:24 - train: epoch 0023, iter [01600, 05004], lr: 0.098066, loss: 2.7169
2022-07-09 21:11:58 - train: epoch 0023, iter [01700, 05004], lr: 0.098062, loss: 2.8266
2022-07-09 21:12:32 - train: epoch 0023, iter [01800, 05004], lr: 0.098057, loss: 2.7209
2022-07-09 21:13:06 - train: epoch 0023, iter [01900, 05004], lr: 0.098053, loss: 2.6340
2022-07-09 21:13:40 - train: epoch 0023, iter [02000, 05004], lr: 0.098048, loss: 2.3430
2022-07-09 21:14:14 - train: epoch 0023, iter [02100, 05004], lr: 0.098044, loss: 2.7640
2022-07-09 21:14:48 - train: epoch 0023, iter [02200, 05004], lr: 0.098039, loss: 2.5052
2022-07-09 21:15:23 - train: epoch 0023, iter [02300, 05004], lr: 0.098035, loss: 2.6911
2022-07-09 21:15:57 - train: epoch 0023, iter [02400, 05004], lr: 0.098030, loss: 2.7333
2022-07-09 21:16:32 - train: epoch 0023, iter [02500, 05004], lr: 0.098026, loss: 2.7913
2022-07-09 21:17:06 - train: epoch 0023, iter [02600, 05004], lr: 0.098022, loss: 2.7610
2022-07-09 21:17:40 - train: epoch 0023, iter [02700, 05004], lr: 0.098017, loss: 2.5374
2022-07-09 21:18:15 - train: epoch 0023, iter [02800, 05004], lr: 0.098013, loss: 2.7208
2022-07-09 21:18:49 - train: epoch 0023, iter [02900, 05004], lr: 0.098008, loss: 2.6269
2022-07-09 21:19:22 - train: epoch 0023, iter [03000, 05004], lr: 0.098004, loss: 2.8821
2022-07-09 21:19:57 - train: epoch 0023, iter [03100, 05004], lr: 0.097999, loss: 2.7685
2022-07-09 21:20:31 - train: epoch 0023, iter [03200, 05004], lr: 0.097995, loss: 2.9352
2022-07-09 21:21:05 - train: epoch 0023, iter [03300, 05004], lr: 0.097990, loss: 2.6751
2022-07-09 21:21:40 - train: epoch 0023, iter [03400, 05004], lr: 0.097985, loss: 2.9638
2022-07-09 21:22:13 - train: epoch 0023, iter [03500, 05004], lr: 0.097981, loss: 2.4284
2022-07-09 21:22:48 - train: epoch 0023, iter [03600, 05004], lr: 0.097976, loss: 2.7305
2022-07-09 21:23:22 - train: epoch 0023, iter [03700, 05004], lr: 0.097972, loss: 2.6737
2022-07-09 21:23:56 - train: epoch 0023, iter [03800, 05004], lr: 0.097967, loss: 2.7198
2022-07-09 21:24:30 - train: epoch 0023, iter [03900, 05004], lr: 0.097963, loss: 2.9992
2022-07-09 21:25:05 - train: epoch 0023, iter [04000, 05004], lr: 0.097958, loss: 2.6848
2022-07-09 21:25:39 - train: epoch 0023, iter [04100, 05004], lr: 0.097954, loss: 2.5618
2022-07-09 21:26:13 - train: epoch 0023, iter [04200, 05004], lr: 0.097949, loss: 2.6300
2022-07-09 21:26:47 - train: epoch 0023, iter [04300, 05004], lr: 0.097945, loss: 2.4946
2022-07-09 21:27:20 - train: epoch 0023, iter [04400, 05004], lr: 0.097940, loss: 2.5877
2022-07-09 21:27:55 - train: epoch 0023, iter [04500, 05004], lr: 0.097935, loss: 2.5919
2022-07-09 21:28:29 - train: epoch 0023, iter [04600, 05004], lr: 0.097931, loss: 2.7458
2022-07-09 21:29:03 - train: epoch 0023, iter [04700, 05004], lr: 0.097926, loss: 2.4499
2022-07-09 21:29:37 - train: epoch 0023, iter [04800, 05004], lr: 0.097922, loss: 2.5355
2022-07-09 21:30:12 - train: epoch 0023, iter [04900, 05004], lr: 0.097917, loss: 2.6376
2022-07-09 21:30:44 - train: epoch 0023, iter [05000, 05004], lr: 0.097912, loss: 2.7226
2022-07-09 21:30:45 - train: epoch 023, train_loss: 2.6874
2022-07-09 21:32:00 - eval: epoch: 023, acc1: 56.022%, acc5: 80.654%, test_loss: 1.8673, per_image_load_time: 2.386ms, per_image_inference_time: 0.447ms
2022-07-09 21:32:00 - until epoch: 023, best_acc1: 56.022%
2022-07-09 21:32:00 - epoch 024 lr: 0.097912
2022-07-09 21:32:39 - train: epoch 0024, iter [00100, 05004], lr: 0.097908, loss: 2.5836
2022-07-09 21:33:14 - train: epoch 0024, iter [00200, 05004], lr: 0.097903, loss: 2.7594
2022-07-09 21:33:48 - train: epoch 0024, iter [00300, 05004], lr: 0.097898, loss: 2.5983
2022-07-09 21:34:23 - train: epoch 0024, iter [00400, 05004], lr: 0.097894, loss: 2.6935
2022-07-09 21:34:57 - train: epoch 0024, iter [00500, 05004], lr: 0.097889, loss: 2.6044
2022-07-09 21:35:32 - train: epoch 0024, iter [00600, 05004], lr: 0.097885, loss: 2.4762
2022-07-09 21:36:06 - train: epoch 0024, iter [00700, 05004], lr: 0.097880, loss: 2.6292
2022-07-09 21:36:40 - train: epoch 0024, iter [00800, 05004], lr: 0.097875, loss: 2.6534
2022-07-09 21:37:15 - train: epoch 0024, iter [00900, 05004], lr: 0.097871, loss: 2.6255
2022-07-09 21:37:48 - train: epoch 0024, iter [01000, 05004], lr: 0.097866, loss: 2.5821
2022-07-09 21:38:23 - train: epoch 0024, iter [01100, 05004], lr: 0.097861, loss: 2.3753
2022-07-09 21:38:56 - train: epoch 0024, iter [01200, 05004], lr: 0.097857, loss: 2.5296
2022-07-09 21:39:31 - train: epoch 0024, iter [01300, 05004], lr: 0.097852, loss: 2.7999
2022-07-09 21:40:05 - train: epoch 0024, iter [01400, 05004], lr: 0.097847, loss: 2.5067
2022-07-09 21:40:39 - train: epoch 0024, iter [01500, 05004], lr: 0.097843, loss: 2.9591
2022-07-09 21:41:13 - train: epoch 0024, iter [01600, 05004], lr: 0.097838, loss: 2.6278
2022-07-09 21:41:47 - train: epoch 0024, iter [01700, 05004], lr: 0.097833, loss: 2.6160
2022-07-09 21:42:21 - train: epoch 0024, iter [01800, 05004], lr: 0.097829, loss: 2.9406
2022-07-09 21:42:56 - train: epoch 0024, iter [01900, 05004], lr: 0.097824, loss: 2.5122
2022-07-09 21:43:30 - train: epoch 0024, iter [02000, 05004], lr: 0.097819, loss: 2.7665
2022-07-09 21:44:04 - train: epoch 0024, iter [02100, 05004], lr: 0.097815, loss: 2.6187
2022-07-09 21:44:39 - train: epoch 0024, iter [02200, 05004], lr: 0.097810, loss: 2.4529
2022-07-09 21:45:12 - train: epoch 0024, iter [02300, 05004], lr: 0.097805, loss: 2.6257
2022-07-09 21:45:47 - train: epoch 0024, iter [02400, 05004], lr: 0.097800, loss: 2.6192
2022-07-09 21:46:21 - train: epoch 0024, iter [02500, 05004], lr: 0.097796, loss: 2.6997
2022-07-09 21:46:56 - train: epoch 0024, iter [02600, 05004], lr: 0.097791, loss: 2.5080
2022-07-09 21:47:30 - train: epoch 0024, iter [02700, 05004], lr: 0.097786, loss: 2.7258
2022-07-09 21:48:04 - train: epoch 0024, iter [02800, 05004], lr: 0.097781, loss: 2.6517
2022-07-09 21:48:38 - train: epoch 0024, iter [02900, 05004], lr: 0.097777, loss: 2.6989
2022-07-09 21:49:13 - train: epoch 0024, iter [03000, 05004], lr: 0.097772, loss: 2.5495
2022-07-09 21:49:47 - train: epoch 0024, iter [03100, 05004], lr: 0.097767, loss: 2.4930
2022-07-09 21:50:22 - train: epoch 0024, iter [03200, 05004], lr: 0.097762, loss: 2.8595
2022-07-09 21:50:56 - train: epoch 0024, iter [03300, 05004], lr: 0.097758, loss: 2.4405
2022-07-09 21:51:30 - train: epoch 0024, iter [03400, 05004], lr: 0.097753, loss: 2.6380
2022-07-09 21:52:05 - train: epoch 0024, iter [03500, 05004], lr: 0.097748, loss: 2.6463
2022-07-09 21:52:39 - train: epoch 0024, iter [03600, 05004], lr: 0.097743, loss: 2.7997
2022-07-09 21:53:14 - train: epoch 0024, iter [03700, 05004], lr: 0.097739, loss: 2.5184
2022-07-09 21:53:47 - train: epoch 0024, iter [03800, 05004], lr: 0.097734, loss: 2.8019
2022-07-09 21:54:22 - train: epoch 0024, iter [03900, 05004], lr: 0.097729, loss: 2.5834
2022-07-09 21:54:56 - train: epoch 0024, iter [04000, 05004], lr: 0.097724, loss: 2.8593
2022-07-09 21:55:30 - train: epoch 0024, iter [04100, 05004], lr: 0.097719, loss: 2.8065
2022-07-09 21:56:05 - train: epoch 0024, iter [04200, 05004], lr: 0.097715, loss: 2.6958
2022-07-09 21:56:39 - train: epoch 0024, iter [04300, 05004], lr: 0.097710, loss: 2.7045
2022-07-09 21:57:13 - train: epoch 0024, iter [04400, 05004], lr: 0.097705, loss: 2.8456
2022-07-09 21:57:47 - train: epoch 0024, iter [04500, 05004], lr: 0.097700, loss: 2.4212
2022-07-09 21:58:22 - train: epoch 0024, iter [04600, 05004], lr: 0.097695, loss: 2.7100
2022-07-09 21:58:55 - train: epoch 0024, iter [04700, 05004], lr: 0.097690, loss: 2.6850
2022-07-09 21:59:30 - train: epoch 0024, iter [04800, 05004], lr: 0.097686, loss: 2.7138
2022-07-09 22:00:04 - train: epoch 0024, iter [04900, 05004], lr: 0.097681, loss: 2.7164
2022-07-09 22:00:37 - train: epoch 0024, iter [05000, 05004], lr: 0.097676, loss: 2.7403
2022-07-09 22:00:38 - train: epoch 024, train_loss: 2.6776
2022-07-09 22:01:53 - eval: epoch: 024, acc1: 54.458%, acc5: 79.774%, test_loss: 1.9346, per_image_load_time: 1.547ms, per_image_inference_time: 0.470ms
2022-07-09 22:01:53 - until epoch: 024, best_acc1: 56.022%
2022-07-09 22:01:53 - epoch 025 lr: 0.097676
2022-07-09 22:02:31 - train: epoch 0025, iter [00100, 05004], lr: 0.097671, loss: 2.6190
2022-07-09 22:03:06 - train: epoch 0025, iter [00200, 05004], lr: 0.097666, loss: 2.6405
2022-07-09 22:03:40 - train: epoch 0025, iter [00300, 05004], lr: 0.097661, loss: 2.7170
2022-07-09 22:04:14 - train: epoch 0025, iter [00400, 05004], lr: 0.097656, loss: 2.3675
2022-07-09 22:04:47 - train: epoch 0025, iter [00500, 05004], lr: 0.097651, loss: 2.5282
2022-07-09 22:05:20 - train: epoch 0025, iter [00600, 05004], lr: 0.097647, loss: 2.6298
2022-07-09 22:05:54 - train: epoch 0025, iter [00700, 05004], lr: 0.097642, loss: 2.4975
2022-07-09 22:06:28 - train: epoch 0025, iter [00800, 05004], lr: 0.097637, loss: 2.7483
2022-07-09 22:07:01 - train: epoch 0025, iter [00900, 05004], lr: 0.097632, loss: 2.6075
2022-07-09 22:07:35 - train: epoch 0025, iter [01000, 05004], lr: 0.097627, loss: 2.5287
2022-07-09 22:08:09 - train: epoch 0025, iter [01100, 05004], lr: 0.097622, loss: 2.5243
2022-07-09 22:08:43 - train: epoch 0025, iter [01200, 05004], lr: 0.097617, loss: 2.8227
2022-07-09 22:09:16 - train: epoch 0025, iter [01300, 05004], lr: 0.097612, loss: 2.8038
2022-07-09 22:09:50 - train: epoch 0025, iter [01400, 05004], lr: 0.097607, loss: 2.6772
2022-07-09 22:10:23 - train: epoch 0025, iter [01500, 05004], lr: 0.097602, loss: 2.6196
2022-07-09 22:10:57 - train: epoch 0025, iter [01600, 05004], lr: 0.097597, loss: 2.5442
2022-07-09 22:11:30 - train: epoch 0025, iter [01700, 05004], lr: 0.097593, loss: 2.4909
2022-07-09 22:12:04 - train: epoch 0025, iter [01800, 05004], lr: 0.097588, loss: 2.5410
2022-07-09 22:12:37 - train: epoch 0025, iter [01900, 05004], lr: 0.097583, loss: 2.5223
2022-07-09 22:13:11 - train: epoch 0025, iter [02000, 05004], lr: 0.097578, loss: 2.7725
2022-07-09 22:13:45 - train: epoch 0025, iter [02100, 05004], lr: 0.097573, loss: 2.5046
2022-07-09 22:14:19 - train: epoch 0025, iter [02200, 05004], lr: 0.097568, loss: 2.3670
2022-07-09 22:14:53 - train: epoch 0025, iter [02300, 05004], lr: 0.097563, loss: 2.6978
2022-07-09 22:15:26 - train: epoch 0025, iter [02400, 05004], lr: 0.097558, loss: 2.4374
2022-07-09 22:16:01 - train: epoch 0025, iter [02500, 05004], lr: 0.097553, loss: 2.8000
2022-07-09 22:16:35 - train: epoch 0025, iter [02600, 05004], lr: 0.097548, loss: 2.7428
2022-07-09 22:17:08 - train: epoch 0025, iter [02700, 05004], lr: 0.097543, loss: 2.7119
2022-07-09 22:17:42 - train: epoch 0025, iter [02800, 05004], lr: 0.097538, loss: 2.6857
2022-07-09 22:18:17 - train: epoch 0025, iter [02900, 05004], lr: 0.097533, loss: 2.8821
2022-07-09 22:18:51 - train: epoch 0025, iter [03000, 05004], lr: 0.097528, loss: 2.7478
2022-07-09 22:19:25 - train: epoch 0025, iter [03100, 05004], lr: 0.097523, loss: 2.5515
2022-07-09 22:19:59 - train: epoch 0025, iter [03200, 05004], lr: 0.097518, loss: 2.7719
2022-07-09 22:20:33 - train: epoch 0025, iter [03300, 05004], lr: 0.097513, loss: 2.4296
2022-07-09 22:21:07 - train: epoch 0025, iter [03400, 05004], lr: 0.097508, loss: 2.7414
2022-07-09 22:21:41 - train: epoch 0025, iter [03500, 05004], lr: 0.097503, loss: 2.2829
2022-07-09 22:22:15 - train: epoch 0025, iter [03600, 05004], lr: 0.097498, loss: 2.8020
2022-07-09 22:22:49 - train: epoch 0025, iter [03700, 05004], lr: 0.097493, loss: 2.6267
2022-07-09 22:23:24 - train: epoch 0025, iter [03800, 05004], lr: 0.097488, loss: 2.7574
2022-07-09 22:23:57 - train: epoch 0025, iter [03900, 05004], lr: 0.097483, loss: 3.0473
2022-07-09 22:24:31 - train: epoch 0025, iter [04000, 05004], lr: 0.097478, loss: 2.7622
2022-07-09 22:25:05 - train: epoch 0025, iter [04100, 05004], lr: 0.097473, loss: 2.9591
2022-07-09 22:25:39 - train: epoch 0025, iter [04200, 05004], lr: 0.097468, loss: 2.7325
2022-07-09 22:26:14 - train: epoch 0025, iter [04300, 05004], lr: 0.097463, loss: 2.5599
2022-07-09 22:26:48 - train: epoch 0025, iter [04400, 05004], lr: 0.097458, loss: 2.5803
2022-07-09 22:27:21 - train: epoch 0025, iter [04500, 05004], lr: 0.097452, loss: 2.6144
2022-07-09 22:27:56 - train: epoch 0025, iter [04600, 05004], lr: 0.097447, loss: 2.7291
2022-07-09 22:28:30 - train: epoch 0025, iter [04700, 05004], lr: 0.097442, loss: 2.4970
2022-07-09 22:29:04 - train: epoch 0025, iter [04800, 05004], lr: 0.097437, loss: 2.3891
2022-07-09 22:29:38 - train: epoch 0025, iter [04900, 05004], lr: 0.097432, loss: 2.7478
2022-07-09 22:30:11 - train: epoch 0025, iter [05000, 05004], lr: 0.097427, loss: 2.7604
2022-07-09 22:30:12 - train: epoch 025, train_loss: 2.6648
2022-07-09 22:31:26 - eval: epoch: 025, acc1: 55.032%, acc5: 80.434%, test_loss: 1.9029, per_image_load_time: 2.392ms, per_image_inference_time: 0.459ms
2022-07-09 22:31:27 - until epoch: 025, best_acc1: 56.022%
2022-07-09 22:31:27 - epoch 026 lr: 0.097427
2022-07-09 22:32:06 - train: epoch 0026, iter [00100, 05004], lr: 0.097422, loss: 2.6106
2022-07-09 22:32:41 - train: epoch 0026, iter [00200, 05004], lr: 0.097417, loss: 2.4754
2022-07-09 22:33:15 - train: epoch 0026, iter [00300, 05004], lr: 0.097412, loss: 2.6366
2022-07-09 22:33:49 - train: epoch 0026, iter [00400, 05004], lr: 0.097406, loss: 2.5020
2022-07-09 22:34:23 - train: epoch 0026, iter [00500, 05004], lr: 0.097401, loss: 2.5124
2022-07-09 22:34:56 - train: epoch 0026, iter [00600, 05004], lr: 0.097396, loss: 2.8164
2022-07-09 22:35:30 - train: epoch 0026, iter [00700, 05004], lr: 0.097391, loss: 2.5285
2022-07-09 22:36:05 - train: epoch 0026, iter [00800, 05004], lr: 0.097386, loss: 2.4939
2022-07-09 22:36:38 - train: epoch 0026, iter [00900, 05004], lr: 0.097381, loss: 2.8195
2022-07-09 22:37:12 - train: epoch 0026, iter [01000, 05004], lr: 0.097376, loss: 2.5543
2022-07-09 22:37:45 - train: epoch 0026, iter [01100, 05004], lr: 0.097370, loss: 2.5819
2022-07-09 22:38:19 - train: epoch 0026, iter [01200, 05004], lr: 0.097365, loss: 2.6427
2022-07-09 22:38:52 - train: epoch 0026, iter [01300, 05004], lr: 0.097360, loss: 2.3828
2022-07-09 22:39:26 - train: epoch 0026, iter [01400, 05004], lr: 0.097355, loss: 2.6367
2022-07-09 22:39:59 - train: epoch 0026, iter [01500, 05004], lr: 0.097350, loss: 2.4866
2022-07-09 22:40:32 - train: epoch 0026, iter [01600, 05004], lr: 0.097345, loss: 2.9665
2022-07-09 22:41:05 - train: epoch 0026, iter [01700, 05004], lr: 0.097339, loss: 2.7107
2022-07-09 22:41:39 - train: epoch 0026, iter [01800, 05004], lr: 0.097334, loss: 2.6996
2022-07-09 22:42:13 - train: epoch 0026, iter [01900, 05004], lr: 0.097329, loss: 2.9270
2022-07-09 22:42:46 - train: epoch 0026, iter [02000, 05004], lr: 0.097324, loss: 2.8573
2022-07-09 22:43:19 - train: epoch 0026, iter [02100, 05004], lr: 0.097319, loss: 2.7374
2022-07-09 22:43:53 - train: epoch 0026, iter [02200, 05004], lr: 0.097313, loss: 2.5669
2022-07-09 22:44:27 - train: epoch 0026, iter [02300, 05004], lr: 0.097308, loss: 2.4883
2022-07-09 22:45:00 - train: epoch 0026, iter [02400, 05004], lr: 0.097303, loss: 2.8144
2022-07-09 22:45:34 - train: epoch 0026, iter [02500, 05004], lr: 0.097298, loss: 2.9008
2022-07-09 22:46:07 - train: epoch 0026, iter [02600, 05004], lr: 0.097293, loss: 2.5317
2022-07-09 22:46:41 - train: epoch 0026, iter [02700, 05004], lr: 0.097287, loss: 2.4859
2022-07-09 22:47:14 - train: epoch 0026, iter [02800, 05004], lr: 0.097282, loss: 2.5593
2022-07-09 22:47:48 - train: epoch 0026, iter [02900, 05004], lr: 0.097277, loss: 2.7369
2022-07-09 22:48:21 - train: epoch 0026, iter [03000, 05004], lr: 0.097272, loss: 2.5003
2022-07-09 22:48:55 - train: epoch 0026, iter [03100, 05004], lr: 0.097266, loss: 2.8647
2022-07-09 22:49:30 - train: epoch 0026, iter [03200, 05004], lr: 0.097261, loss: 2.6897
2022-07-09 22:50:03 - train: epoch 0026, iter [03300, 05004], lr: 0.097256, loss: 2.5180
2022-07-09 22:50:37 - train: epoch 0026, iter [03400, 05004], lr: 0.097251, loss: 2.8132
2022-07-09 22:51:10 - train: epoch 0026, iter [03500, 05004], lr: 0.097245, loss: 2.7289
2022-07-09 22:51:44 - train: epoch 0026, iter [03600, 05004], lr: 0.097240, loss: 2.4237
2022-07-09 22:52:18 - train: epoch 0026, iter [03700, 05004], lr: 0.097235, loss: 2.8637
2022-07-09 22:52:52 - train: epoch 0026, iter [03800, 05004], lr: 0.097230, loss: 2.7053
2022-07-09 22:53:26 - train: epoch 0026, iter [03900, 05004], lr: 0.097224, loss: 2.9114
2022-07-09 22:53:59 - train: epoch 0026, iter [04000, 05004], lr: 0.097219, loss: 2.7790
2022-07-09 22:54:33 - train: epoch 0026, iter [04100, 05004], lr: 0.097214, loss: 2.6244
2022-07-09 22:55:07 - train: epoch 0026, iter [04200, 05004], lr: 0.097208, loss: 2.8012
2022-07-09 22:55:40 - train: epoch 0026, iter [04300, 05004], lr: 0.097203, loss: 2.7376
2022-07-09 22:56:14 - train: epoch 0026, iter [04400, 05004], lr: 0.097198, loss: 2.7302
2022-07-09 22:56:47 - train: epoch 0026, iter [04500, 05004], lr: 0.097192, loss: 2.9984
2022-07-09 22:57:21 - train: epoch 0026, iter [04600, 05004], lr: 0.097187, loss: 2.8694
2022-07-09 22:57:55 - train: epoch 0026, iter [04700, 05004], lr: 0.097182, loss: 2.4744
2022-07-09 22:58:30 - train: epoch 0026, iter [04800, 05004], lr: 0.097176, loss: 2.5254
2022-07-09 22:59:03 - train: epoch 0026, iter [04900, 05004], lr: 0.097171, loss: 2.6873
2022-07-09 22:59:36 - train: epoch 0026, iter [05000, 05004], lr: 0.097166, loss: 2.7591
2022-07-09 22:59:37 - train: epoch 026, train_loss: 2.6582
2022-07-09 23:00:52 - eval: epoch: 026, acc1: 55.612%, acc5: 80.488%, test_loss: 1.8731, per_image_load_time: 2.445ms, per_image_inference_time: 0.454ms
2022-07-09 23:00:52 - until epoch: 026, best_acc1: 56.022%
2022-07-09 23:00:52 - epoch 027 lr: 0.097166
2022-07-09 23:01:32 - train: epoch 0027, iter [00100, 05004], lr: 0.097160, loss: 2.6952
2022-07-09 23:02:06 - train: epoch 0027, iter [00200, 05004], lr: 0.097155, loss: 2.6676
2022-07-09 23:02:40 - train: epoch 0027, iter [00300, 05004], lr: 0.097150, loss: 2.6339
2022-07-09 23:03:14 - train: epoch 0027, iter [00400, 05004], lr: 0.097144, loss: 2.7596
2022-07-09 23:03:49 - train: epoch 0027, iter [00500, 05004], lr: 0.097139, loss: 2.7073
2022-07-09 23:04:23 - train: epoch 0027, iter [00600, 05004], lr: 0.097133, loss: 2.8385
2022-07-09 23:04:57 - train: epoch 0027, iter [00700, 05004], lr: 0.097128, loss: 2.8122
2022-07-09 23:05:31 - train: epoch 0027, iter [00800, 05004], lr: 0.097123, loss: 2.6564
2022-07-09 23:06:07 - train: epoch 0027, iter [00900, 05004], lr: 0.097117, loss: 2.6080
2022-07-09 23:06:41 - train: epoch 0027, iter [01000, 05004], lr: 0.097112, loss: 2.7030
2022-07-09 23:07:16 - train: epoch 0027, iter [01100, 05004], lr: 0.097107, loss: 2.5528
2022-07-09 23:07:50 - train: epoch 0027, iter [01200, 05004], lr: 0.097101, loss: 2.8235
2022-07-09 23:08:24 - train: epoch 0027, iter [01300, 05004], lr: 0.097096, loss: 2.7553
2022-07-09 23:08:59 - train: epoch 0027, iter [01400, 05004], lr: 0.097090, loss: 2.8502
2022-07-09 23:09:33 - train: epoch 0027, iter [01500, 05004], lr: 0.097085, loss: 2.6156
2022-07-09 23:10:08 - train: epoch 0027, iter [01600, 05004], lr: 0.097079, loss: 2.6340
2022-07-09 23:10:42 - train: epoch 0027, iter [01700, 05004], lr: 0.097074, loss: 2.7217
2022-07-09 23:11:16 - train: epoch 0027, iter [01800, 05004], lr: 0.097069, loss: 2.6765
2022-07-09 23:11:50 - train: epoch 0027, iter [01900, 05004], lr: 0.097063, loss: 2.8265
2022-07-09 23:12:25 - train: epoch 0027, iter [02000, 05004], lr: 0.097058, loss: 2.5010
2022-07-09 23:12:59 - train: epoch 0027, iter [02100, 05004], lr: 0.097052, loss: 2.5882
2022-07-09 23:13:33 - train: epoch 0027, iter [02200, 05004], lr: 0.097047, loss: 2.8905
2022-07-09 23:14:08 - train: epoch 0027, iter [02300, 05004], lr: 0.097041, loss: 2.9804
2022-07-09 23:14:41 - train: epoch 0027, iter [02400, 05004], lr: 0.097036, loss: 2.7458
2022-07-09 23:15:16 - train: epoch 0027, iter [02500, 05004], lr: 0.097030, loss: 2.6864
2022-07-09 23:15:50 - train: epoch 0027, iter [02600, 05004], lr: 0.097025, loss: 2.8558
2022-07-09 23:16:25 - train: epoch 0027, iter [02700, 05004], lr: 0.097020, loss: 2.5418
2022-07-09 23:16:59 - train: epoch 0027, iter [02800, 05004], lr: 0.097014, loss: 2.8777
2022-07-09 23:17:33 - train: epoch 0027, iter [02900, 05004], lr: 0.097009, loss: 2.8110
2022-07-09 23:18:07 - train: epoch 0027, iter [03000, 05004], lr: 0.097003, loss: 2.4248
2022-07-09 23:18:42 - train: epoch 0027, iter [03100, 05004], lr: 0.096998, loss: 2.5660
2022-07-09 23:19:16 - train: epoch 0027, iter [03200, 05004], lr: 0.096992, loss: 2.4734
2022-07-09 23:19:50 - train: epoch 0027, iter [03300, 05004], lr: 0.096987, loss: 2.6735
2022-07-09 23:20:25 - train: epoch 0027, iter [03400, 05004], lr: 0.096981, loss: 2.8429
2022-07-09 23:21:00 - train: epoch 0027, iter [03500, 05004], lr: 0.096976, loss: 2.9496
2022-07-09 23:21:34 - train: epoch 0027, iter [03600, 05004], lr: 0.096970, loss: 2.5037
2022-07-09 23:22:08 - train: epoch 0027, iter [03700, 05004], lr: 0.096965, loss: 2.4420
2022-07-09 23:22:42 - train: epoch 0027, iter [03800, 05004], lr: 0.096959, loss: 2.5985
2022-07-09 23:23:17 - train: epoch 0027, iter [03900, 05004], lr: 0.096954, loss: 2.4529
2022-07-09 23:23:51 - train: epoch 0027, iter [04000, 05004], lr: 0.096948, loss: 2.8219
2022-07-09 23:24:26 - train: epoch 0027, iter [04100, 05004], lr: 0.096942, loss: 2.5675
2022-07-09 23:25:00 - train: epoch 0027, iter [04200, 05004], lr: 0.096937, loss: 2.6342
2022-07-09 23:25:35 - train: epoch 0027, iter [04300, 05004], lr: 0.096931, loss: 2.6844
2022-07-09 23:26:09 - train: epoch 0027, iter [04400, 05004], lr: 0.096926, loss: 2.6533
2022-07-09 23:26:43 - train: epoch 0027, iter [04500, 05004], lr: 0.096920, loss: 2.4135
2022-07-09 23:27:18 - train: epoch 0027, iter [04600, 05004], lr: 0.096915, loss: 2.6830
2022-07-09 23:27:51 - train: epoch 0027, iter [04700, 05004], lr: 0.096909, loss: 2.7274
2022-07-09 23:28:27 - train: epoch 0027, iter [04800, 05004], lr: 0.096904, loss: 2.6348
2022-07-09 23:29:01 - train: epoch 0027, iter [04900, 05004], lr: 0.096898, loss: 2.8172
2022-07-09 23:29:34 - train: epoch 0027, iter [05000, 05004], lr: 0.096892, loss: 2.1509
2022-07-09 23:29:35 - train: epoch 027, train_loss: 2.6497
2022-07-09 23:30:50 - eval: epoch: 027, acc1: 55.688%, acc5: 80.576%, test_loss: 1.8816, per_image_load_time: 2.380ms, per_image_inference_time: 0.491ms
2022-07-09 23:30:51 - until epoch: 027, best_acc1: 56.022%
2022-07-09 23:30:51 - epoch 028 lr: 0.096892
2022-07-09 23:31:30 - train: epoch 0028, iter [00100, 05004], lr: 0.096887, loss: 2.4198
2022-07-09 23:32:05 - train: epoch 0028, iter [00200, 05004], lr: 0.096881, loss: 2.6420
2022-07-09 23:32:39 - train: epoch 0028, iter [00300, 05004], lr: 0.096875, loss: 2.6607
2022-07-09 23:33:13 - train: epoch 0028, iter [00400, 05004], lr: 0.096870, loss: 2.4999
2022-07-09 23:33:47 - train: epoch 0028, iter [00500, 05004], lr: 0.096864, loss: 2.6177
2022-07-09 23:34:22 - train: epoch 0028, iter [00600, 05004], lr: 0.096859, loss: 2.8565
2022-07-09 23:34:57 - train: epoch 0028, iter [00700, 05004], lr: 0.096853, loss: 2.7993
2022-07-09 23:35:31 - train: epoch 0028, iter [00800, 05004], lr: 0.096847, loss: 2.3553
2022-07-09 23:36:05 - train: epoch 0028, iter [00900, 05004], lr: 0.096842, loss: 2.3689
2022-07-09 23:36:39 - train: epoch 0028, iter [01000, 05004], lr: 0.096836, loss: 2.6452
2022-07-09 23:37:14 - train: epoch 0028, iter [01100, 05004], lr: 0.096830, loss: 2.6557
2022-07-09 23:37:48 - train: epoch 0028, iter [01200, 05004], lr: 0.096825, loss: 2.5460
2022-07-09 23:38:22 - train: epoch 0028, iter [01300, 05004], lr: 0.096819, loss: 2.8539
2022-07-09 23:38:57 - train: epoch 0028, iter [01400, 05004], lr: 0.096813, loss: 2.9161
2022-07-09 23:39:30 - train: epoch 0028, iter [01500, 05004], lr: 0.096808, loss: 2.5182
2022-07-09 23:40:05 - train: epoch 0028, iter [01600, 05004], lr: 0.096802, loss: 2.6573
2022-07-09 23:40:39 - train: epoch 0028, iter [01700, 05004], lr: 0.096796, loss: 2.4724
2022-07-09 23:41:14 - train: epoch 0028, iter [01800, 05004], lr: 0.096791, loss: 2.4782
2022-07-09 23:41:48 - train: epoch 0028, iter [01900, 05004], lr: 0.096785, loss: 2.6590
2022-07-09 23:42:22 - train: epoch 0028, iter [02000, 05004], lr: 0.096779, loss: 2.5027
2022-07-09 23:42:56 - train: epoch 0028, iter [02100, 05004], lr: 0.096774, loss: 2.5615
2022-07-09 23:43:31 - train: epoch 0028, iter [02200, 05004], lr: 0.096768, loss: 2.7424
2022-07-09 23:44:05 - train: epoch 0028, iter [02300, 05004], lr: 0.096762, loss: 2.5912
2022-07-09 23:44:40 - train: epoch 0028, iter [02400, 05004], lr: 0.096757, loss: 2.7864
2022-07-09 23:45:14 - train: epoch 0028, iter [02500, 05004], lr: 0.096751, loss: 2.7179
2022-07-09 23:45:49 - train: epoch 0028, iter [02600, 05004], lr: 0.096745, loss: 2.2389
2022-07-09 23:46:23 - train: epoch 0028, iter [02700, 05004], lr: 0.096740, loss: 2.5226
2022-07-09 23:46:57 - train: epoch 0028, iter [02800, 05004], lr: 0.096734, loss: 2.6125
2022-07-09 23:47:32 - train: epoch 0028, iter [02900, 05004], lr: 0.096728, loss: 2.6090
2022-07-09 23:48:07 - train: epoch 0028, iter [03000, 05004], lr: 0.096722, loss: 2.7468
2022-07-09 23:48:41 - train: epoch 0028, iter [03100, 05004], lr: 0.096717, loss: 2.9410
2022-07-09 23:49:15 - train: epoch 0028, iter [03200, 05004], lr: 0.096711, loss: 2.5122
2022-07-09 23:49:50 - train: epoch 0028, iter [03300, 05004], lr: 0.096705, loss: 2.6150
2022-07-09 23:50:25 - train: epoch 0028, iter [03400, 05004], lr: 0.096699, loss: 2.6493
2022-07-09 23:50:59 - train: epoch 0028, iter [03500, 05004], lr: 0.096694, loss: 2.5023
2022-07-09 23:51:33 - train: epoch 0028, iter [03600, 05004], lr: 0.096688, loss: 2.5755
2022-07-09 23:52:08 - train: epoch 0028, iter [03700, 05004], lr: 0.096682, loss: 2.6614
2022-07-09 23:52:42 - train: epoch 0028, iter [03800, 05004], lr: 0.096676, loss: 2.3451
2022-07-09 23:53:17 - train: epoch 0028, iter [03900, 05004], lr: 0.096671, loss: 2.6661
2022-07-09 23:53:52 - train: epoch 0028, iter [04000, 05004], lr: 0.096665, loss: 2.8725
2022-07-09 23:54:28 - train: epoch 0028, iter [04100, 05004], lr: 0.096659, loss: 2.7586
2022-07-09 23:55:03 - train: epoch 0028, iter [04200, 05004], lr: 0.096653, loss: 2.7147
2022-07-09 23:55:37 - train: epoch 0028, iter [04300, 05004], lr: 0.096647, loss: 2.5231
2022-07-09 23:56:12 - train: epoch 0028, iter [04400, 05004], lr: 0.096642, loss: 2.5499
2022-07-09 23:56:47 - train: epoch 0028, iter [04500, 05004], lr: 0.096636, loss: 2.6775
2022-07-09 23:57:22 - train: epoch 0028, iter [04600, 05004], lr: 0.096630, loss: 2.6210
2022-07-09 23:57:58 - train: epoch 0028, iter [04700, 05004], lr: 0.096624, loss: 2.5062
2022-07-09 23:58:32 - train: epoch 0028, iter [04800, 05004], lr: 0.096618, loss: 2.4904
2022-07-09 23:59:08 - train: epoch 0028, iter [04900, 05004], lr: 0.096613, loss: 2.4062
2022-07-09 23:59:42 - train: epoch 0028, iter [05000, 05004], lr: 0.096607, loss: 2.5591
2022-07-09 23:59:43 - train: epoch 028, train_loss: 2.6411
2022-07-10 00:00:58 - eval: epoch: 028, acc1: 54.972%, acc5: 80.048%, test_loss: 1.9196, per_image_load_time: 2.392ms, per_image_inference_time: 0.491ms
2022-07-10 00:00:58 - until epoch: 028, best_acc1: 56.022%
2022-07-10 00:00:58 - epoch 029 lr: 0.096606
2022-07-10 00:01:38 - train: epoch 0029, iter [00100, 05004], lr: 0.096601, loss: 2.7095
2022-07-10 00:02:13 - train: epoch 0029, iter [00200, 05004], lr: 0.096595, loss: 2.5396
2022-07-10 00:02:48 - train: epoch 0029, iter [00300, 05004], lr: 0.096589, loss: 2.8637
2022-07-10 00:03:22 - train: epoch 0029, iter [00400, 05004], lr: 0.096583, loss: 2.5667
2022-07-10 00:03:56 - train: epoch 0029, iter [00500, 05004], lr: 0.096577, loss: 2.6504
2022-07-10 00:04:31 - train: epoch 0029, iter [00600, 05004], lr: 0.096571, loss: 2.6561
2022-07-10 00:05:06 - train: epoch 0029, iter [00700, 05004], lr: 0.096566, loss: 2.0586
2022-07-10 00:05:41 - train: epoch 0029, iter [00800, 05004], lr: 0.096560, loss: 2.6212
2022-07-10 00:06:15 - train: epoch 0029, iter [00900, 05004], lr: 0.096554, loss: 2.5087
2022-07-10 00:06:49 - train: epoch 0029, iter [01000, 05004], lr: 0.096548, loss: 2.3676
2022-07-10 00:07:24 - train: epoch 0029, iter [01100, 05004], lr: 0.096542, loss: 2.5122
2022-07-10 00:07:58 - train: epoch 0029, iter [01200, 05004], lr: 0.096536, loss: 2.7238
2022-07-10 00:08:33 - train: epoch 0029, iter [01300, 05004], lr: 0.096530, loss: 2.6718
2022-07-10 00:09:08 - train: epoch 0029, iter [01400, 05004], lr: 0.096524, loss: 2.8627
2022-07-10 00:09:41 - train: epoch 0029, iter [01500, 05004], lr: 0.096518, loss: 2.7698
2022-07-10 00:10:16 - train: epoch 0029, iter [01600, 05004], lr: 0.096513, loss: 2.7077
2022-07-10 00:10:51 - train: epoch 0029, iter [01700, 05004], lr: 0.096507, loss: 2.4584
2022-07-10 00:11:25 - train: epoch 0029, iter [01800, 05004], lr: 0.096501, loss: 2.7627
2022-07-10 00:12:00 - train: epoch 0029, iter [01900, 05004], lr: 0.096495, loss: 2.3486
2022-07-10 00:12:35 - train: epoch 0029, iter [02000, 05004], lr: 0.096489, loss: 2.7821
2022-07-10 00:13:09 - train: epoch 0029, iter [02100, 05004], lr: 0.096483, loss: 2.6289
2022-07-10 00:13:44 - train: epoch 0029, iter [02200, 05004], lr: 0.096477, loss: 2.8120
2022-07-10 00:14:20 - train: epoch 0029, iter [02300, 05004], lr: 0.096471, loss: 2.7834
2022-07-10 00:14:55 - train: epoch 0029, iter [02400, 05004], lr: 0.096465, loss: 2.8781
2022-07-10 00:15:29 - train: epoch 0029, iter [02500, 05004], lr: 0.096459, loss: 2.5308
2022-07-10 00:16:04 - train: epoch 0029, iter [02600, 05004], lr: 0.096453, loss: 2.7014
2022-07-10 00:16:39 - train: epoch 0029, iter [02700, 05004], lr: 0.096447, loss: 2.5688
2022-07-10 00:17:14 - train: epoch 0029, iter [02800, 05004], lr: 0.096441, loss: 2.4024
2022-07-10 00:17:49 - train: epoch 0029, iter [02900, 05004], lr: 0.096435, loss: 2.6398
2022-07-10 00:18:24 - train: epoch 0029, iter [03000, 05004], lr: 0.096429, loss: 2.5807
2022-07-10 00:18:58 - train: epoch 0029, iter [03100, 05004], lr: 0.096423, loss: 2.5359
2022-07-10 00:19:33 - train: epoch 0029, iter [03200, 05004], lr: 0.096417, loss: 2.6273
2022-07-10 00:20:08 - train: epoch 0029, iter [03300, 05004], lr: 0.096411, loss: 2.5538
2022-07-10 00:20:43 - train: epoch 0029, iter [03400, 05004], lr: 0.096405, loss: 2.3343
2022-07-10 00:21:19 - train: epoch 0029, iter [03500, 05004], lr: 0.096399, loss: 2.6572
2022-07-10 00:21:53 - train: epoch 0029, iter [03600, 05004], lr: 0.096393, loss: 2.5829
2022-07-10 00:22:28 - train: epoch 0029, iter [03700, 05004], lr: 0.096387, loss: 2.5307
2022-07-10 00:23:03 - train: epoch 0029, iter [03800, 05004], lr: 0.096381, loss: 2.4109
2022-07-10 00:23:37 - train: epoch 0029, iter [03900, 05004], lr: 0.096375, loss: 2.4423
2022-07-10 00:24:13 - train: epoch 0029, iter [04000, 05004], lr: 0.096369, loss: 2.5226
2022-07-10 00:24:49 - train: epoch 0029, iter [04100, 05004], lr: 0.096363, loss: 2.6561
2022-07-10 00:25:23 - train: epoch 0029, iter [04200, 05004], lr: 0.096357, loss: 2.4356
2022-07-10 00:25:59 - train: epoch 0029, iter [04300, 05004], lr: 0.096351, loss: 2.6247
2022-07-10 00:26:34 - train: epoch 0029, iter [04400, 05004], lr: 0.096345, loss: 2.4894
2022-07-10 00:27:09 - train: epoch 0029, iter [04500, 05004], lr: 0.096339, loss: 2.8782
2022-07-10 00:27:43 - train: epoch 0029, iter [04600, 05004], lr: 0.096333, loss: 2.9661
2022-07-10 00:28:18 - train: epoch 0029, iter [04700, 05004], lr: 0.096327, loss: 2.3820
2022-07-10 00:28:53 - train: epoch 0029, iter [04800, 05004], lr: 0.096321, loss: 2.5584
2022-07-10 00:29:28 - train: epoch 0029, iter [04900, 05004], lr: 0.096315, loss: 3.0872
2022-07-10 00:30:02 - train: epoch 0029, iter [05000, 05004], lr: 0.096309, loss: 2.4674
2022-07-10 00:30:03 - train: epoch 029, train_loss: 2.6353
2022-07-10 00:31:19 - eval: epoch: 029, acc1: 56.924%, acc5: 81.288%, test_loss: 1.8159, per_image_load_time: 2.289ms, per_image_inference_time: 0.503ms
2022-07-10 00:31:19 - until epoch: 029, best_acc1: 56.924%
2022-07-10 00:31:19 - epoch 030 lr: 0.096309
2022-07-10 00:31:59 - train: epoch 0030, iter [00100, 05004], lr: 0.096303, loss: 2.7741
2022-07-10 00:32:33 - train: epoch 0030, iter [00200, 05004], lr: 0.096297, loss: 2.5495
2022-07-10 00:33:08 - train: epoch 0030, iter [00300, 05004], lr: 0.096290, loss: 2.4572
2022-07-10 00:33:42 - train: epoch 0030, iter [00400, 05004], lr: 0.096284, loss: 2.5472
2022-07-10 00:34:17 - train: epoch 0030, iter [00500, 05004], lr: 0.096278, loss: 2.8954
2022-07-10 00:34:53 - train: epoch 0030, iter [00600, 05004], lr: 0.096272, loss: 2.4840
2022-07-10 00:35:26 - train: epoch 0030, iter [00700, 05004], lr: 0.096266, loss: 2.5665
2022-07-10 00:36:02 - train: epoch 0030, iter [00800, 05004], lr: 0.096260, loss: 2.7570
2022-07-10 00:36:36 - train: epoch 0030, iter [00900, 05004], lr: 0.096254, loss: 2.9370
2022-07-10 00:37:10 - train: epoch 0030, iter [01000, 05004], lr: 0.096248, loss: 2.3796
2022-07-10 00:37:46 - train: epoch 0030, iter [01100, 05004], lr: 0.096242, loss: 2.3853
2022-07-10 00:38:20 - train: epoch 0030, iter [01200, 05004], lr: 0.096236, loss: 2.8212
2022-07-10 00:38:54 - train: epoch 0030, iter [01300, 05004], lr: 0.096229, loss: 2.4695
2022-07-10 00:39:30 - train: epoch 0030, iter [01400, 05004], lr: 0.096223, loss: 2.6210
2022-07-10 00:40:05 - train: epoch 0030, iter [01500, 05004], lr: 0.096217, loss: 2.5697
2022-07-10 00:40:40 - train: epoch 0030, iter [01600, 05004], lr: 0.096211, loss: 2.5741
2022-07-10 00:41:14 - train: epoch 0030, iter [01700, 05004], lr: 0.096205, loss: 2.8195
2022-07-10 00:41:49 - train: epoch 0030, iter [01800, 05004], lr: 0.096199, loss: 2.6871
2022-07-10 00:42:24 - train: epoch 0030, iter [01900, 05004], lr: 0.096193, loss: 2.7778
2022-07-10 00:42:59 - train: epoch 0030, iter [02000, 05004], lr: 0.096186, loss: 2.6352
2022-07-10 00:43:33 - train: epoch 0030, iter [02100, 05004], lr: 0.096180, loss: 3.0183
2022-07-10 00:44:08 - train: epoch 0030, iter [02200, 05004], lr: 0.096174, loss: 2.7549
2022-07-10 00:44:43 - train: epoch 0030, iter [02300, 05004], lr: 0.096168, loss: 2.6066
2022-07-10 00:45:18 - train: epoch 0030, iter [02400, 05004], lr: 0.096162, loss: 2.8477
2022-07-10 00:45:52 - train: epoch 0030, iter [02500, 05004], lr: 0.096155, loss: 2.5594
2022-07-10 00:46:28 - train: epoch 0030, iter [02600, 05004], lr: 0.096149, loss: 2.5668
2022-07-10 00:47:02 - train: epoch 0030, iter [02700, 05004], lr: 0.096143, loss: 2.7452
2022-07-10 00:47:37 - train: epoch 0030, iter [02800, 05004], lr: 0.096137, loss: 2.5418
2022-07-10 00:48:12 - train: epoch 0030, iter [02900, 05004], lr: 0.096131, loss: 2.5681
2022-07-10 00:48:46 - train: epoch 0030, iter [03000, 05004], lr: 0.096124, loss: 2.8792
2022-07-10 00:49:22 - train: epoch 0030, iter [03100, 05004], lr: 0.096118, loss: 2.6087
2022-07-10 00:49:57 - train: epoch 0030, iter [03200, 05004], lr: 0.096112, loss: 2.4652
2022-07-10 00:50:33 - train: epoch 0030, iter [03300, 05004], lr: 0.096106, loss: 2.7070
2022-07-10 00:51:07 - train: epoch 0030, iter [03400, 05004], lr: 0.096100, loss: 2.6165
2022-07-10 00:51:43 - train: epoch 0030, iter [03500, 05004], lr: 0.096093, loss: 2.7251
2022-07-10 00:52:18 - train: epoch 0030, iter [03600, 05004], lr: 0.096087, loss: 2.4658
2022-07-10 00:52:52 - train: epoch 0030, iter [03700, 05004], lr: 0.096081, loss: 2.6561
2022-07-10 00:53:28 - train: epoch 0030, iter [03800, 05004], lr: 0.096075, loss: 2.9878
2022-07-10 00:54:03 - train: epoch 0030, iter [03900, 05004], lr: 0.096068, loss: 2.6718
2022-07-10 00:54:38 - train: epoch 0030, iter [04000, 05004], lr: 0.096062, loss: 2.3185
2022-07-10 00:55:13 - train: epoch 0030, iter [04100, 05004], lr: 0.096056, loss: 2.5330
2022-07-10 00:55:48 - train: epoch 0030, iter [04200, 05004], lr: 0.096050, loss: 3.1288
2022-07-10 00:56:22 - train: epoch 0030, iter [04300, 05004], lr: 0.096043, loss: 2.4464
2022-07-10 00:56:58 - train: epoch 0030, iter [04400, 05004], lr: 0.096037, loss: 2.7049
2022-07-10 00:57:32 - train: epoch 0030, iter [04500, 05004], lr: 0.096031, loss: 2.9569
2022-07-10 00:58:08 - train: epoch 0030, iter [04600, 05004], lr: 0.096024, loss: 2.3516
2022-07-10 00:58:42 - train: epoch 0030, iter [04700, 05004], lr: 0.096018, loss: 2.8055
2022-07-10 00:59:17 - train: epoch 0030, iter [04800, 05004], lr: 0.096012, loss: 2.6385
2022-07-10 00:59:53 - train: epoch 0030, iter [04900, 05004], lr: 0.096006, loss: 2.8132
2022-07-10 01:00:26 - train: epoch 0030, iter [05000, 05004], lr: 0.095999, loss: 2.8420
2022-07-10 01:00:28 - train: epoch 030, train_loss: 2.6276
2022-07-10 01:01:44 - eval: epoch: 030, acc1: 56.236%, acc5: 80.910%, test_loss: 1.8531, per_image_load_time: 2.233ms, per_image_inference_time: 0.477ms
2022-07-10 01:01:44 - until epoch: 030, best_acc1: 56.924%
2022-07-10 01:01:44 - epoch 031 lr: 0.095999
2022-07-10 01:02:23 - train: epoch 0031, iter [00100, 05004], lr: 0.095993, loss: 2.7114
2022-07-10 01:02:59 - train: epoch 0031, iter [00200, 05004], lr: 0.095986, loss: 2.4203
2022-07-10 01:03:33 - train: epoch 0031, iter [00300, 05004], lr: 0.095980, loss: 2.5399
2022-07-10 01:04:08 - train: epoch 0031, iter [00400, 05004], lr: 0.095974, loss: 2.5246
2022-07-10 01:04:43 - train: epoch 0031, iter [00500, 05004], lr: 0.095967, loss: 2.5318
2022-07-10 01:05:17 - train: epoch 0031, iter [00600, 05004], lr: 0.095961, loss: 2.4956
2022-07-10 01:05:53 - train: epoch 0031, iter [00700, 05004], lr: 0.095955, loss: 2.4401
2022-07-10 01:06:27 - train: epoch 0031, iter [00800, 05004], lr: 0.095948, loss: 2.5332
2022-07-10 01:07:01 - train: epoch 0031, iter [00900, 05004], lr: 0.095942, loss: 2.4140
2022-07-10 01:07:36 - train: epoch 0031, iter [01000, 05004], lr: 0.095936, loss: 2.8872
2022-07-10 01:08:11 - train: epoch 0031, iter [01100, 05004], lr: 0.095929, loss: 2.7599
2022-07-10 01:08:47 - train: epoch 0031, iter [01200, 05004], lr: 0.095923, loss: 2.6009
2022-07-10 01:09:21 - train: epoch 0031, iter [01300, 05004], lr: 0.095917, loss: 2.3984
2022-07-10 01:09:55 - train: epoch 0031, iter [01400, 05004], lr: 0.095910, loss: 2.6352
2022-07-10 01:10:30 - train: epoch 0031, iter [01500, 05004], lr: 0.095904, loss: 2.7532
2022-07-10 01:11:04 - train: epoch 0031, iter [01600, 05004], lr: 0.095897, loss: 2.4990
2022-07-10 01:11:40 - train: epoch 0031, iter [01700, 05004], lr: 0.095891, loss: 2.3818
2022-07-10 01:12:13 - train: epoch 0031, iter [01800, 05004], lr: 0.095885, loss: 2.4694
2022-07-10 01:12:48 - train: epoch 0031, iter [01900, 05004], lr: 0.095878, loss: 2.6767
2022-07-10 01:13:24 - train: epoch 0031, iter [02000, 05004], lr: 0.095872, loss: 2.6356
2022-07-10 01:13:58 - train: epoch 0031, iter [02100, 05004], lr: 0.095865, loss: 2.4806
2022-07-10 01:14:34 - train: epoch 0031, iter [02200, 05004], lr: 0.095859, loss: 2.4687
2022-07-10 01:15:09 - train: epoch 0031, iter [02300, 05004], lr: 0.095853, loss: 2.3972
2022-07-10 01:15:44 - train: epoch 0031, iter [02400, 05004], lr: 0.095846, loss: 2.5249
2022-07-10 01:16:18 - train: epoch 0031, iter [02500, 05004], lr: 0.095840, loss: 2.4751
2022-07-10 01:16:53 - train: epoch 0031, iter [02600, 05004], lr: 0.095833, loss: 2.6673
2022-07-10 01:17:27 - train: epoch 0031, iter [02700, 05004], lr: 0.095827, loss: 2.6571
2022-07-10 01:18:02 - train: epoch 0031, iter [02800, 05004], lr: 0.095820, loss: 2.9871
2022-07-10 01:18:39 - train: epoch 0031, iter [02900, 05004], lr: 0.095814, loss: 2.6624
2022-07-10 01:19:13 - train: epoch 0031, iter [03000, 05004], lr: 0.095808, loss: 2.6542
2022-07-10 01:19:48 - train: epoch 0031, iter [03100, 05004], lr: 0.095801, loss: 2.6271
2022-07-10 01:20:23 - train: epoch 0031, iter [03200, 05004], lr: 0.095795, loss: 2.7284
2022-07-10 01:20:58 - train: epoch 0031, iter [03300, 05004], lr: 0.095788, loss: 2.4603
2022-07-10 01:21:33 - train: epoch 0031, iter [03400, 05004], lr: 0.095782, loss: 2.6131
2022-07-10 01:22:08 - train: epoch 0031, iter [03500, 05004], lr: 0.095775, loss: 2.8716
2022-07-10 01:22:42 - train: epoch 0031, iter [03600, 05004], lr: 0.095769, loss: 2.8713
2022-07-10 01:23:17 - train: epoch 0031, iter [03700, 05004], lr: 0.095762, loss: 2.4152
2022-07-10 01:23:52 - train: epoch 0031, iter [03800, 05004], lr: 0.095756, loss: 2.5297
2022-07-10 01:24:27 - train: epoch 0031, iter [03900, 05004], lr: 0.095749, loss: 2.4476
2022-07-10 01:25:03 - train: epoch 0031, iter [04000, 05004], lr: 0.095743, loss: 2.4426
2022-07-10 01:25:37 - train: epoch 0031, iter [04100, 05004], lr: 0.095736, loss: 2.4595
2022-07-10 01:26:12 - train: epoch 0031, iter [04200, 05004], lr: 0.095730, loss: 2.6982
2022-07-10 01:26:48 - train: epoch 0031, iter [04300, 05004], lr: 0.095723, loss: 2.6147
2022-07-10 01:27:23 - train: epoch 0031, iter [04400, 05004], lr: 0.095717, loss: 2.6908
2022-07-10 01:27:57 - train: epoch 0031, iter [04500, 05004], lr: 0.095710, loss: 2.6463
2022-07-10 01:28:32 - train: epoch 0031, iter [04600, 05004], lr: 0.095704, loss: 2.6477
2022-07-10 01:29:08 - train: epoch 0031, iter [04700, 05004], lr: 0.095697, loss: 2.7348
2022-07-10 01:29:43 - train: epoch 0031, iter [04800, 05004], lr: 0.095691, loss: 2.4256
2022-07-10 01:30:18 - train: epoch 0031, iter [04900, 05004], lr: 0.095684, loss: 2.4498
2022-07-10 01:30:51 - train: epoch 0031, iter [05000, 05004], lr: 0.095678, loss: 2.5767
2022-07-10 01:30:52 - train: epoch 031, train_loss: 2.6189
2022-07-10 01:32:08 - eval: epoch: 031, acc1: 54.172%, acc5: 79.320%, test_loss: 1.9479, per_image_load_time: 2.379ms, per_image_inference_time: 0.481ms
2022-07-10 01:32:09 - until epoch: 031, best_acc1: 56.924%
2022-07-10 01:32:09 - epoch 032 lr: 0.095677
2022-07-10 01:32:49 - train: epoch 0032, iter [00100, 05004], lr: 0.095671, loss: 2.2591
2022-07-10 01:33:23 - train: epoch 0032, iter [00200, 05004], lr: 0.095664, loss: 2.6486
2022-07-10 01:33:58 - train: epoch 0032, iter [00300, 05004], lr: 0.095658, loss: 2.5612
2022-07-10 01:34:32 - train: epoch 0032, iter [00400, 05004], lr: 0.095651, loss: 2.6443
2022-07-10 01:35:07 - train: epoch 0032, iter [00500, 05004], lr: 0.095644, loss: 2.7243
2022-07-10 01:35:41 - train: epoch 0032, iter [00600, 05004], lr: 0.095638, loss: 2.6924
2022-07-10 01:36:15 - train: epoch 0032, iter [00700, 05004], lr: 0.095631, loss: 2.5091
2022-07-10 01:36:49 - train: epoch 0032, iter [00800, 05004], lr: 0.095625, loss: 2.5226
2022-07-10 01:37:25 - train: epoch 0032, iter [00900, 05004], lr: 0.095618, loss: 2.5744
2022-07-10 01:38:00 - train: epoch 0032, iter [01000, 05004], lr: 0.095612, loss: 2.6408
2022-07-10 01:38:35 - train: epoch 0032, iter [01100, 05004], lr: 0.095605, loss: 2.6351
2022-07-10 01:39:10 - train: epoch 0032, iter [01200, 05004], lr: 0.095598, loss: 2.7305
2022-07-10 01:39:44 - train: epoch 0032, iter [01300, 05004], lr: 0.095592, loss: 2.4604
2022-07-10 01:40:19 - train: epoch 0032, iter [01400, 05004], lr: 0.095585, loss: 2.9610
2022-07-10 01:40:54 - train: epoch 0032, iter [01500, 05004], lr: 0.095579, loss: 2.6756
2022-07-10 01:41:28 - train: epoch 0032, iter [01600, 05004], lr: 0.095572, loss: 2.6286
2022-07-10 01:42:03 - train: epoch 0032, iter [01700, 05004], lr: 0.095565, loss: 2.7174
2022-07-10 01:42:38 - train: epoch 0032, iter [01800, 05004], lr: 0.095559, loss: 2.7123
2022-07-10 01:43:13 - train: epoch 0032, iter [01900, 05004], lr: 0.095552, loss: 2.6181
2022-07-10 01:43:48 - train: epoch 0032, iter [02000, 05004], lr: 0.095545, loss: 2.4939
2022-07-10 01:44:23 - train: epoch 0032, iter [02100, 05004], lr: 0.095539, loss: 2.5057
2022-07-10 01:44:59 - train: epoch 0032, iter [02200, 05004], lr: 0.095532, loss: 2.5606
2022-07-10 01:45:33 - train: epoch 0032, iter [02300, 05004], lr: 0.095525, loss: 2.6279
2022-07-10 01:46:07 - train: epoch 0032, iter [02400, 05004], lr: 0.095519, loss: 2.5925
2022-07-10 01:46:43 - train: epoch 0032, iter [02500, 05004], lr: 0.095512, loss: 2.7780
2022-07-10 01:47:17 - train: epoch 0032, iter [02600, 05004], lr: 0.095505, loss: 2.1789
2022-07-10 01:47:52 - train: epoch 0032, iter [02700, 05004], lr: 0.095499, loss: 2.5105
2022-07-10 01:48:28 - train: epoch 0032, iter [02800, 05004], lr: 0.095492, loss: 2.8180
2022-07-10 01:49:03 - train: epoch 0032, iter [02900, 05004], lr: 0.095485, loss: 2.4898
2022-07-10 01:49:38 - train: epoch 0032, iter [03000, 05004], lr: 0.095479, loss: 2.6142
2022-07-10 01:50:12 - train: epoch 0032, iter [03100, 05004], lr: 0.095472, loss: 2.8610
2022-07-10 01:50:48 - train: epoch 0032, iter [03200, 05004], lr: 0.095465, loss: 2.6319
2022-07-10 01:51:23 - train: epoch 0032, iter [03300, 05004], lr: 0.095459, loss: 2.4623
2022-07-10 01:51:57 - train: epoch 0032, iter [03400, 05004], lr: 0.095452, loss: 2.5224
2022-07-10 01:52:32 - train: epoch 0032, iter [03500, 05004], lr: 0.095445, loss: 2.6099
2022-07-10 01:53:07 - train: epoch 0032, iter [03600, 05004], lr: 0.095438, loss: 2.6960
2022-07-10 01:53:42 - train: epoch 0032, iter [03700, 05004], lr: 0.095432, loss: 2.5212
2022-07-10 01:54:17 - train: epoch 0032, iter [03800, 05004], lr: 0.095425, loss: 2.5346
2022-07-10 01:54:52 - train: epoch 0032, iter [03900, 05004], lr: 0.095418, loss: 2.7482
2022-07-10 01:55:27 - train: epoch 0032, iter [04000, 05004], lr: 0.095412, loss: 2.5815
2022-07-10 01:56:01 - train: epoch 0032, iter [04100, 05004], lr: 0.095405, loss: 2.3980
2022-07-10 01:56:37 - train: epoch 0032, iter [04200, 05004], lr: 0.095398, loss: 2.6061
2022-07-10 01:57:12 - train: epoch 0032, iter [04300, 05004], lr: 0.095391, loss: 2.7080
2022-07-10 01:57:47 - train: epoch 0032, iter [04400, 05004], lr: 0.095385, loss: 2.6677
2022-07-10 01:58:23 - train: epoch 0032, iter [04500, 05004], lr: 0.095378, loss: 2.9038
2022-07-10 01:58:58 - train: epoch 0032, iter [04600, 05004], lr: 0.095371, loss: 2.7869
2022-07-10 01:59:33 - train: epoch 0032, iter [04700, 05004], lr: 0.095364, loss: 2.6966
2022-07-10 02:00:08 - train: epoch 0032, iter [04800, 05004], lr: 0.095358, loss: 2.4991
2022-07-10 02:00:44 - train: epoch 0032, iter [04900, 05004], lr: 0.095351, loss: 2.7112
2022-07-10 02:01:17 - train: epoch 0032, iter [05000, 05004], lr: 0.095344, loss: 2.7035
2022-07-10 02:01:18 - train: epoch 032, train_loss: 2.6139
2022-07-10 02:02:33 - eval: epoch: 032, acc1: 54.390%, acc5: 79.578%, test_loss: 1.9492, per_image_load_time: 2.397ms, per_image_inference_time: 0.500ms
2022-07-10 02:02:33 - until epoch: 032, best_acc1: 56.924%
2022-07-10 02:02:33 - epoch 033 lr: 0.095344
2022-07-10 02:03:14 - train: epoch 0033, iter [00100, 05004], lr: 0.095337, loss: 2.6206
2022-07-10 02:03:48 - train: epoch 0033, iter [00200, 05004], lr: 0.095330, loss: 2.6323
2022-07-10 02:04:22 - train: epoch 0033, iter [00300, 05004], lr: 0.095323, loss: 2.3705
2022-07-10 02:04:57 - train: epoch 0033, iter [00400, 05004], lr: 0.095317, loss: 2.2753
2022-07-10 02:05:31 - train: epoch 0033, iter [00500, 05004], lr: 0.095310, loss: 2.4725
2022-07-10 02:06:06 - train: epoch 0033, iter [00600, 05004], lr: 0.095303, loss: 2.5427
2022-07-10 02:06:41 - train: epoch 0033, iter [00700, 05004], lr: 0.095296, loss: 2.6122
2022-07-10 02:07:16 - train: epoch 0033, iter [00800, 05004], lr: 0.095289, loss: 2.7285
2022-07-10 02:07:50 - train: epoch 0033, iter [00900, 05004], lr: 0.095282, loss: 2.7145
2022-07-10 02:08:25 - train: epoch 0033, iter [01000, 05004], lr: 0.095276, loss: 2.7502
2022-07-10 02:08:59 - train: epoch 0033, iter [01100, 05004], lr: 0.095269, loss: 2.5239
2022-07-10 02:09:34 - train: epoch 0033, iter [01200, 05004], lr: 0.095262, loss: 2.7695
2022-07-10 02:10:09 - train: epoch 0033, iter [01300, 05004], lr: 0.095255, loss: 2.6969
2022-07-10 02:10:44 - train: epoch 0033, iter [01400, 05004], lr: 0.095248, loss: 2.6427
2022-07-10 02:11:18 - train: epoch 0033, iter [01500, 05004], lr: 0.095241, loss: 2.8376
2022-07-10 02:11:53 - train: epoch 0033, iter [01600, 05004], lr: 0.095235, loss: 2.6428
2022-07-10 02:12:28 - train: epoch 0033, iter [01700, 05004], lr: 0.095228, loss: 2.5470
2022-07-10 02:13:03 - train: epoch 0033, iter [01800, 05004], lr: 0.095221, loss: 2.9300
2022-07-10 02:13:37 - train: epoch 0033, iter [01900, 05004], lr: 0.095214, loss: 2.6637
2022-07-10 02:14:12 - train: epoch 0033, iter [02000, 05004], lr: 0.095207, loss: 2.4512
2022-07-10 02:14:47 - train: epoch 0033, iter [02100, 05004], lr: 0.095200, loss: 2.5728
2022-07-10 02:15:21 - train: epoch 0033, iter [02200, 05004], lr: 0.095193, loss: 2.6056
2022-07-10 02:15:56 - train: epoch 0033, iter [02300, 05004], lr: 0.095186, loss: 2.5188
2022-07-10 02:16:32 - train: epoch 0033, iter [02400, 05004], lr: 0.095180, loss: 2.7121
2022-07-10 02:17:06 - train: epoch 0033, iter [02500, 05004], lr: 0.095173, loss: 2.7380
2022-07-10 02:17:40 - train: epoch 0033, iter [02600, 05004], lr: 0.095166, loss: 2.4033
2022-07-10 02:18:16 - train: epoch 0033, iter [02700, 05004], lr: 0.095159, loss: 2.7706
2022-07-10 02:18:50 - train: epoch 0033, iter [02800, 05004], lr: 0.095152, loss: 2.8147
2022-07-10 02:19:25 - train: epoch 0033, iter [02900, 05004], lr: 0.095145, loss: 2.7031
2022-07-10 02:19:59 - train: epoch 0033, iter [03000, 05004], lr: 0.095138, loss: 2.5178
2022-07-10 02:20:35 - train: epoch 0033, iter [03100, 05004], lr: 0.095131, loss: 2.5709
2022-07-10 02:21:11 - train: epoch 0033, iter [03200, 05004], lr: 0.095124, loss: 2.3961
2022-07-10 02:21:45 - train: epoch 0033, iter [03300, 05004], lr: 0.095117, loss: 2.4689
2022-07-10 02:22:20 - train: epoch 0033, iter [03400, 05004], lr: 0.095110, loss: 2.5917
2022-07-10 02:22:54 - train: epoch 0033, iter [03500, 05004], lr: 0.095103, loss: 2.8126
2022-07-10 02:23:29 - train: epoch 0033, iter [03600, 05004], lr: 0.095096, loss: 2.6900
2022-07-10 02:24:05 - train: epoch 0033, iter [03700, 05004], lr: 0.095090, loss: 2.7520
2022-07-10 02:24:39 - train: epoch 0033, iter [03800, 05004], lr: 0.095083, loss: 2.6383
2022-07-10 02:25:15 - train: epoch 0033, iter [03900, 05004], lr: 0.095076, loss: 2.7305
2022-07-10 02:25:49 - train: epoch 0033, iter [04000, 05004], lr: 0.095069, loss: 2.8608
2022-07-10 02:26:25 - train: epoch 0033, iter [04100, 05004], lr: 0.095062, loss: 2.7567
2022-07-10 02:27:00 - train: epoch 0033, iter [04200, 05004], lr: 0.095055, loss: 2.9370
2022-07-10 02:27:34 - train: epoch 0033, iter [04300, 05004], lr: 0.095048, loss: 2.6931
2022-07-10 02:28:10 - train: epoch 0033, iter [04400, 05004], lr: 0.095041, loss: 2.7270
2022-07-10 02:28:44 - train: epoch 0033, iter [04500, 05004], lr: 0.095034, loss: 2.9237
2022-07-10 02:29:20 - train: epoch 0033, iter [04600, 05004], lr: 0.095027, loss: 2.5350
2022-07-10 02:29:55 - train: epoch 0033, iter [04700, 05004], lr: 0.095020, loss: 2.5238
2022-07-10 02:30:30 - train: epoch 0033, iter [04800, 05004], lr: 0.095013, loss: 3.0809
2022-07-10 02:31:06 - train: epoch 0033, iter [04900, 05004], lr: 0.095006, loss: 2.6182
2022-07-10 02:31:39 - train: epoch 0033, iter [05000, 05004], lr: 0.094999, loss: 2.4665
2022-07-10 02:31:40 - train: epoch 033, train_loss: 2.6047
2022-07-10 02:32:56 - eval: epoch: 033, acc1: 56.586%, acc5: 81.160%, test_loss: 1.8351, per_image_load_time: 2.387ms, per_image_inference_time: 0.498ms
2022-07-10 02:32:56 - until epoch: 033, best_acc1: 56.924%
2022-07-10 02:32:56 - epoch 034 lr: 0.094998
2022-07-10 02:33:36 - train: epoch 0034, iter [00100, 05004], lr: 0.094991, loss: 2.5145
2022-07-10 02:34:11 - train: epoch 0034, iter [00200, 05004], lr: 0.094984, loss: 2.6078
2022-07-10 02:34:44 - train: epoch 0034, iter [00300, 05004], lr: 0.094977, loss: 2.5174
2022-07-10 02:35:19 - train: epoch 0034, iter [00400, 05004], lr: 0.094970, loss: 2.3100
2022-07-10 02:35:55 - train: epoch 0034, iter [00500, 05004], lr: 0.094963, loss: 2.4377
2022-07-10 02:36:29 - train: epoch 0034, iter [00600, 05004], lr: 0.094956, loss: 2.7794
2022-07-10 02:37:03 - train: epoch 0034, iter [00700, 05004], lr: 0.094949, loss: 2.5643
2022-07-10 02:37:38 - train: epoch 0034, iter [00800, 05004], lr: 0.094942, loss: 2.4700
2022-07-10 02:38:12 - train: epoch 0034, iter [00900, 05004], lr: 0.094935, loss: 2.6594
2022-07-10 02:38:47 - train: epoch 0034, iter [01000, 05004], lr: 0.094928, loss: 2.3612
2022-07-10 02:39:22 - train: epoch 0034, iter [01100, 05004], lr: 0.094921, loss: 2.6330
2022-07-10 02:39:57 - train: epoch 0034, iter [01200, 05004], lr: 0.094914, loss: 2.6607
2022-07-10 02:40:32 - train: epoch 0034, iter [01300, 05004], lr: 0.094907, loss: 2.4827
2022-07-10 02:41:07 - train: epoch 0034, iter [01400, 05004], lr: 0.094900, loss: 2.5577
2022-07-10 02:41:41 - train: epoch 0034, iter [01500, 05004], lr: 0.094893, loss: 2.4014
2022-07-10 02:42:16 - train: epoch 0034, iter [01600, 05004], lr: 0.094886, loss: 2.5859
2022-07-10 02:42:51 - train: epoch 0034, iter [01700, 05004], lr: 0.094878, loss: 2.5673
2022-07-10 02:43:26 - train: epoch 0034, iter [01800, 05004], lr: 0.094871, loss: 3.0457
2022-07-10 02:44:01 - train: epoch 0034, iter [01900, 05004], lr: 0.094864, loss: 2.7506
2022-07-10 02:44:36 - train: epoch 0034, iter [02000, 05004], lr: 0.094857, loss: 2.5089
2022-07-10 02:45:11 - train: epoch 0034, iter [02100, 05004], lr: 0.094850, loss: 2.7406
2022-07-10 02:45:46 - train: epoch 0034, iter [02200, 05004], lr: 0.094843, loss: 2.4019
2022-07-10 02:46:20 - train: epoch 0034, iter [02300, 05004], lr: 0.094836, loss: 2.5803
2022-07-10 02:46:56 - train: epoch 0034, iter [02400, 05004], lr: 0.094829, loss: 2.4973
2022-07-10 02:47:30 - train: epoch 0034, iter [02500, 05004], lr: 0.094821, loss: 2.6358
2022-07-10 02:48:05 - train: epoch 0034, iter [02600, 05004], lr: 0.094814, loss: 2.7215
2022-07-10 02:48:39 - train: epoch 0034, iter [02700, 05004], lr: 0.094807, loss: 2.6522
2022-07-10 02:49:15 - train: epoch 0034, iter [02800, 05004], lr: 0.094800, loss: 2.3742
2022-07-10 02:49:50 - train: epoch 0034, iter [02900, 05004], lr: 0.094793, loss: 2.4376
2022-07-10 02:50:25 - train: epoch 0034, iter [03000, 05004], lr: 0.094786, loss: 2.4500
2022-07-10 02:51:00 - train: epoch 0034, iter [03100, 05004], lr: 0.094779, loss: 2.5910
2022-07-10 02:51:35 - train: epoch 0034, iter [03200, 05004], lr: 0.094771, loss: 2.5008
2022-07-10 02:52:11 - train: epoch 0034, iter [03300, 05004], lr: 0.094764, loss: 2.3622
2022-07-10 02:52:45 - train: epoch 0034, iter [03400, 05004], lr: 0.094757, loss: 2.6917
2022-07-10 02:53:20 - train: epoch 0034, iter [03500, 05004], lr: 0.094750, loss: 2.6012
2022-07-10 02:53:55 - train: epoch 0034, iter [03600, 05004], lr: 0.094743, loss: 2.3964
2022-07-10 02:54:29 - train: epoch 0034, iter [03700, 05004], lr: 0.094736, loss: 2.4004
2022-07-10 02:55:05 - train: epoch 0034, iter [03800, 05004], lr: 0.094728, loss: 2.4856
2022-07-10 02:55:40 - train: epoch 0034, iter [03900, 05004], lr: 0.094721, loss: 2.7213
2022-07-10 02:56:15 - train: epoch 0034, iter [04000, 05004], lr: 0.094714, loss: 2.6917
2022-07-10 02:56:50 - train: epoch 0034, iter [04100, 05004], lr: 0.094707, loss: 2.5839
2022-07-10 02:57:25 - train: epoch 0034, iter [04200, 05004], lr: 0.094700, loss: 2.6218
2022-07-10 02:58:00 - train: epoch 0034, iter [04300, 05004], lr: 0.094692, loss: 2.3857
2022-07-10 02:58:35 - train: epoch 0034, iter [04400, 05004], lr: 0.094685, loss: 2.7184
2022-07-10 02:59:10 - train: epoch 0034, iter [04500, 05004], lr: 0.094678, loss: 2.7279
2022-07-10 02:59:46 - train: epoch 0034, iter [04600, 05004], lr: 0.094671, loss: 2.6716
2022-07-10 03:00:21 - train: epoch 0034, iter [04700, 05004], lr: 0.094663, loss: 2.6357
2022-07-10 03:00:55 - train: epoch 0034, iter [04800, 05004], lr: 0.094656, loss: 2.6957
2022-07-10 03:01:31 - train: epoch 0034, iter [04900, 05004], lr: 0.094649, loss: 2.7152
2022-07-10 03:02:04 - train: epoch 0034, iter [05000, 05004], lr: 0.094642, loss: 2.7467
2022-07-10 03:02:06 - train: epoch 034, train_loss: 2.5985
2022-07-10 03:03:21 - eval: epoch: 034, acc1: 54.164%, acc5: 79.342%, test_loss: 1.9788, per_image_load_time: 2.399ms, per_image_inference_time: 0.504ms
2022-07-10 03:03:21 - until epoch: 034, best_acc1: 56.924%
2022-07-10 03:03:21 - epoch 035 lr: 0.094641
2022-07-10 03:04:01 - train: epoch 0035, iter [00100, 05004], lr: 0.094634, loss: 2.2226
2022-07-10 03:04:35 - train: epoch 0035, iter [00200, 05004], lr: 0.094627, loss: 2.2767
2022-07-10 03:05:10 - train: epoch 0035, iter [00300, 05004], lr: 0.094620, loss: 2.7383
2022-07-10 03:05:44 - train: epoch 0035, iter [00400, 05004], lr: 0.094612, loss: 2.5733
2022-07-10 03:06:19 - train: epoch 0035, iter [00500, 05004], lr: 0.094605, loss: 2.5145
2022-07-10 03:06:53 - train: epoch 0035, iter [00600, 05004], lr: 0.094598, loss: 2.6032
2022-07-10 03:07:27 - train: epoch 0035, iter [00700, 05004], lr: 0.094591, loss: 2.7050
2022-07-10 03:08:02 - train: epoch 0035, iter [00800, 05004], lr: 0.094583, loss: 2.5599
2022-07-10 03:08:36 - train: epoch 0035, iter [00900, 05004], lr: 0.094576, loss: 2.6713
2022-07-10 03:09:12 - train: epoch 0035, iter [01000, 05004], lr: 0.094569, loss: 2.8043
2022-07-10 03:09:46 - train: epoch 0035, iter [01100, 05004], lr: 0.094561, loss: 2.7266
2022-07-10 03:10:21 - train: epoch 0035, iter [01200, 05004], lr: 0.094554, loss: 2.6072
2022-07-10 03:10:56 - train: epoch 0035, iter [01300, 05004], lr: 0.094547, loss: 2.6073
2022-07-10 03:11:31 - train: epoch 0035, iter [01400, 05004], lr: 0.094539, loss: 2.3855
2022-07-10 03:12:07 - train: epoch 0035, iter [01500, 05004], lr: 0.094532, loss: 2.5845
2022-07-10 03:12:42 - train: epoch 0035, iter [01600, 05004], lr: 0.094525, loss: 2.5847
2022-07-10 03:13:16 - train: epoch 0035, iter [01700, 05004], lr: 0.094517, loss: 2.4083
2022-07-10 03:13:51 - train: epoch 0035, iter [01800, 05004], lr: 0.094510, loss: 2.5015
2022-07-10 03:14:26 - train: epoch 0035, iter [01900, 05004], lr: 0.094503, loss: 2.6744
2022-07-10 03:15:02 - train: epoch 0035, iter [02000, 05004], lr: 0.094495, loss: 2.5857
2022-07-10 03:15:36 - train: epoch 0035, iter [02100, 05004], lr: 0.094488, loss: 2.4836
2022-07-10 03:16:11 - train: epoch 0035, iter [02200, 05004], lr: 0.094481, loss: 2.9003
2022-07-10 03:16:46 - train: epoch 0035, iter [02300, 05004], lr: 0.094473, loss: 2.5699
2022-07-10 03:17:21 - train: epoch 0035, iter [02400, 05004], lr: 0.094466, loss: 2.6099
2022-07-10 03:17:56 - train: epoch 0035, iter [02500, 05004], lr: 0.094459, loss: 2.4655
2022-07-10 03:18:32 - train: epoch 0035, iter [02600, 05004], lr: 0.094451, loss: 2.7080
2022-07-10 03:19:06 - train: epoch 0035, iter [02700, 05004], lr: 0.094444, loss: 2.7363
2022-07-10 03:19:42 - train: epoch 0035, iter [02800, 05004], lr: 0.094437, loss: 2.5442
2022-07-10 03:20:16 - train: epoch 0035, iter [02900, 05004], lr: 0.094429, loss: 2.7219
2022-07-10 03:20:51 - train: epoch 0035, iter [03000, 05004], lr: 0.094422, loss: 2.6515
2022-07-10 03:21:26 - train: epoch 0035, iter [03100, 05004], lr: 0.094414, loss: 2.6545
2022-07-10 03:22:01 - train: epoch 0035, iter [03200, 05004], lr: 0.094407, loss: 2.5156
2022-07-10 03:22:35 - train: epoch 0035, iter [03300, 05004], lr: 0.094400, loss: 2.5666
2022-07-10 03:23:11 - train: epoch 0035, iter [03400, 05004], lr: 0.094392, loss: 2.7109
2022-07-10 03:23:45 - train: epoch 0035, iter [03500, 05004], lr: 0.094385, loss: 2.2445
2022-07-10 03:24:20 - train: epoch 0035, iter [03600, 05004], lr: 0.094377, loss: 2.5874
2022-07-10 03:24:55 - train: epoch 0035, iter [03700, 05004], lr: 0.094370, loss: 2.5812
2022-07-10 03:25:30 - train: epoch 0035, iter [03800, 05004], lr: 0.094363, loss: 2.5508
2022-07-10 03:26:05 - train: epoch 0035, iter [03900, 05004], lr: 0.094355, loss: 2.8443
2022-07-10 03:26:41 - train: epoch 0035, iter [04000, 05004], lr: 0.094348, loss: 2.2232
2022-07-10 03:27:16 - train: epoch 0035, iter [04100, 05004], lr: 0.094340, loss: 2.8567
2022-07-10 03:27:51 - train: epoch 0035, iter [04200, 05004], lr: 0.094333, loss: 2.4707
2022-07-10 03:28:26 - train: epoch 0035, iter [04300, 05004], lr: 0.094325, loss: 2.6452
2022-07-10 03:29:00 - train: epoch 0035, iter [04400, 05004], lr: 0.094318, loss: 2.6836
2022-07-10 03:29:36 - train: epoch 0035, iter [04500, 05004], lr: 0.094310, loss: 2.9003
2022-07-10 03:30:11 - train: epoch 0035, iter [04600, 05004], lr: 0.094303, loss: 2.5050
2022-07-10 03:30:47 - train: epoch 0035, iter [04700, 05004], lr: 0.094296, loss: 2.8872
2022-07-10 03:31:21 - train: epoch 0035, iter [04800, 05004], lr: 0.094288, loss: 2.7654
2022-07-10 03:31:57 - train: epoch 0035, iter [04900, 05004], lr: 0.094281, loss: 2.5939
2022-07-10 03:32:30 - train: epoch 0035, iter [05000, 05004], lr: 0.094273, loss: 2.3439
2022-07-10 03:32:31 - train: epoch 035, train_loss: 2.5918
2022-07-10 03:33:46 - eval: epoch: 035, acc1: 54.656%, acc5: 79.662%, test_loss: 1.9399, per_image_load_time: 2.399ms, per_image_inference_time: 0.484ms
2022-07-10 03:33:46 - until epoch: 035, best_acc1: 56.924%
2022-07-10 03:33:46 - epoch 036 lr: 0.094273
2022-07-10 03:34:27 - train: epoch 0036, iter [00100, 05004], lr: 0.094265, loss: 2.6916
2022-07-10 03:35:01 - train: epoch 0036, iter [00200, 05004], lr: 0.094258, loss: 2.2833
2022-07-10 03:35:36 - train: epoch 0036, iter [00300, 05004], lr: 0.094250, loss: 2.5055
2022-07-10 03:36:11 - train: epoch 0036, iter [00400, 05004], lr: 0.094243, loss: 2.5951
2022-07-10 03:36:45 - train: epoch 0036, iter [00500, 05004], lr: 0.094235, loss: 2.6457
2022-07-10 03:37:20 - train: epoch 0036, iter [00600, 05004], lr: 0.094228, loss: 2.4155
2022-07-10 03:37:54 - train: epoch 0036, iter [00700, 05004], lr: 0.094220, loss: 2.5249
2022-07-10 03:38:29 - train: epoch 0036, iter [00800, 05004], lr: 0.094213, loss: 2.3621
2022-07-10 03:39:05 - train: epoch 0036, iter [00900, 05004], lr: 0.094205, loss: 2.5165
2022-07-10 03:39:39 - train: epoch 0036, iter [01000, 05004], lr: 0.094198, loss: 2.4808
2022-07-10 03:40:14 - train: epoch 0036, iter [01100, 05004], lr: 0.094190, loss: 2.7264
2022-07-10 03:40:49 - train: epoch 0036, iter [01200, 05004], lr: 0.094183, loss: 2.3742
2022-07-10 03:41:24 - train: epoch 0036, iter [01300, 05004], lr: 0.094175, loss: 2.6898
2022-07-10 03:41:59 - train: epoch 0036, iter [01400, 05004], lr: 0.094168, loss: 2.7733
2022-07-10 03:42:32 - train: epoch 0036, iter [01500, 05004], lr: 0.094160, loss: 2.7167
2022-07-10 03:43:08 - train: epoch 0036, iter [01600, 05004], lr: 0.094153, loss: 2.8011
2022-07-10 03:43:43 - train: epoch 0036, iter [01700, 05004], lr: 0.094145, loss: 2.3887
2022-07-10 03:44:18 - train: epoch 0036, iter [01800, 05004], lr: 0.094137, loss: 2.3792
2022-07-10 03:44:52 - train: epoch 0036, iter [01900, 05004], lr: 0.094130, loss: 2.6546
2022-07-10 03:45:27 - train: epoch 0036, iter [02000, 05004], lr: 0.094122, loss: 2.5507
2022-07-10 03:46:02 - train: epoch 0036, iter [02100, 05004], lr: 0.094115, loss: 2.4711
2022-07-10 03:46:37 - train: epoch 0036, iter [02200, 05004], lr: 0.094107, loss: 2.4695
2022-07-10 03:47:11 - train: epoch 0036, iter [02300, 05004], lr: 0.094100, loss: 2.7402
2022-07-10 03:47:46 - train: epoch 0036, iter [02400, 05004], lr: 0.094092, loss: 2.7007
2022-07-10 03:48:21 - train: epoch 0036, iter [02500, 05004], lr: 0.094084, loss: 2.2748
2022-07-10 03:48:56 - train: epoch 0036, iter [02600, 05004], lr: 0.094077, loss: 2.7122
2022-07-10 03:49:30 - train: epoch 0036, iter [02700, 05004], lr: 0.094069, loss: 2.4118
2022-07-10 03:50:05 - train: epoch 0036, iter [02800, 05004], lr: 0.094062, loss: 2.5171
2022-07-10 03:50:40 - train: epoch 0036, iter [02900, 05004], lr: 0.094054, loss: 2.2524
2022-07-10 03:51:15 - train: epoch 0036, iter [03000, 05004], lr: 0.094046, loss: 2.9090
2022-07-10 03:51:50 - train: epoch 0036, iter [03100, 05004], lr: 0.094039, loss: 2.6737
2022-07-10 03:52:25 - train: epoch 0036, iter [03200, 05004], lr: 0.094031, loss: 2.5873
2022-07-10 03:53:00 - train: epoch 0036, iter [03300, 05004], lr: 0.094023, loss: 2.4894
2022-07-10 03:53:34 - train: epoch 0036, iter [03400, 05004], lr: 0.094016, loss: 2.4970
2022-07-10 03:54:10 - train: epoch 0036, iter [03500, 05004], lr: 0.094008, loss: 2.6632
2022-07-10 03:54:45 - train: epoch 0036, iter [03600, 05004], lr: 0.094001, loss: 2.6643
2022-07-10 03:55:20 - train: epoch 0036, iter [03700, 05004], lr: 0.093993, loss: 2.6583
2022-07-10 03:55:55 - train: epoch 0036, iter [03800, 05004], lr: 0.093985, loss: 2.4889
2022-07-10 03:56:29 - train: epoch 0036, iter [03900, 05004], lr: 0.093978, loss: 2.5345
2022-07-10 03:57:04 - train: epoch 0036, iter [04000, 05004], lr: 0.093970, loss: 2.6230
2022-07-10 03:57:40 - train: epoch 0036, iter [04100, 05004], lr: 0.093962, loss: 2.3445
2022-07-10 03:58:15 - train: epoch 0036, iter [04200, 05004], lr: 0.093955, loss: 2.6234
2022-07-10 03:58:50 - train: epoch 0036, iter [04300, 05004], lr: 0.093947, loss: 2.5580
2022-07-10 03:59:23 - train: epoch 0036, iter [04400, 05004], lr: 0.093939, loss: 2.6093
2022-07-10 03:59:59 - train: epoch 0036, iter [04500, 05004], lr: 0.093932, loss: 2.5279
2022-07-10 04:00:34 - train: epoch 0036, iter [04600, 05004], lr: 0.093924, loss: 2.4254
2022-07-10 04:01:09 - train: epoch 0036, iter [04700, 05004], lr: 0.093916, loss: 2.6513
2022-07-10 04:01:44 - train: epoch 0036, iter [04800, 05004], lr: 0.093908, loss: 2.7081
2022-07-10 04:02:20 - train: epoch 0036, iter [04900, 05004], lr: 0.093901, loss: 2.6380
2022-07-10 04:02:53 - train: epoch 0036, iter [05000, 05004], lr: 0.093893, loss: 2.5911
2022-07-10 04:02:54 - train: epoch 036, train_loss: 2.5823
2022-07-10 04:04:10 - eval: epoch: 036, acc1: 54.636%, acc5: 79.948%, test_loss: 1.9298, per_image_load_time: 1.295ms, per_image_inference_time: 0.488ms
2022-07-10 04:04:10 - until epoch: 036, best_acc1: 56.924%
2022-07-10 04:04:10 - epoch 037 lr: 0.093893
2022-07-10 04:04:51 - train: epoch 0037, iter [00100, 05004], lr: 0.093885, loss: 2.3589
2022-07-10 04:05:25 - train: epoch 0037, iter [00200, 05004], lr: 0.093877, loss: 2.4093
2022-07-10 04:05:59 - train: epoch 0037, iter [00300, 05004], lr: 0.093870, loss: 2.4719
2022-07-10 04:06:34 - train: epoch 0037, iter [00400, 05004], lr: 0.093862, loss: 2.5344
2022-07-10 04:07:08 - train: epoch 0037, iter [00500, 05004], lr: 0.093854, loss: 2.6848
2022-07-10 04:07:43 - train: epoch 0037, iter [00600, 05004], lr: 0.093846, loss: 2.2903
2022-07-10 04:08:18 - train: epoch 0037, iter [00700, 05004], lr: 0.093839, loss: 2.6741
2022-07-10 04:08:53 - train: epoch 0037, iter [00800, 05004], lr: 0.093831, loss: 2.5127
2022-07-10 04:09:28 - train: epoch 0037, iter [00900, 05004], lr: 0.093823, loss: 2.7841
2022-07-10 04:10:02 - train: epoch 0037, iter [01000, 05004], lr: 0.093815, loss: 2.3715
2022-07-10 04:10:37 - train: epoch 0037, iter [01100, 05004], lr: 0.093808, loss: 2.6508
2022-07-10 04:11:12 - train: epoch 0037, iter [01200, 05004], lr: 0.093800, loss: 2.5406
2022-07-10 04:11:47 - train: epoch 0037, iter [01300, 05004], lr: 0.093792, loss: 2.4149
2022-07-10 04:12:22 - train: epoch 0037, iter [01400, 05004], lr: 0.093784, loss: 2.6640
2022-07-10 04:12:56 - train: epoch 0037, iter [01500, 05004], lr: 0.093777, loss: 2.4460
2022-07-10 04:13:31 - train: epoch 0037, iter [01600, 05004], lr: 0.093769, loss: 2.4791
2022-07-10 04:14:06 - train: epoch 0037, iter [01700, 05004], lr: 0.093761, loss: 2.7120
2022-07-10 04:14:41 - train: epoch 0037, iter [01800, 05004], lr: 0.093753, loss: 2.6903
2022-07-10 04:15:16 - train: epoch 0037, iter [01900, 05004], lr: 0.093745, loss: 2.6613
2022-07-10 04:15:50 - train: epoch 0037, iter [02000, 05004], lr: 0.093738, loss: 2.8109
2022-07-10 04:16:25 - train: epoch 0037, iter [02100, 05004], lr: 0.093730, loss: 2.5689
2022-07-10 04:17:00 - train: epoch 0037, iter [02200, 05004], lr: 0.093722, loss: 2.5324
2022-07-10 04:17:35 - train: epoch 0037, iter [02300, 05004], lr: 0.093714, loss: 2.5967
2022-07-10 04:18:09 - train: epoch 0037, iter [02400, 05004], lr: 0.093706, loss: 2.5389
2022-07-10 04:18:44 - train: epoch 0037, iter [02500, 05004], lr: 0.093699, loss: 2.6296
2022-07-10 04:19:19 - train: epoch 0037, iter [02600, 05004], lr: 0.093691, loss: 2.6422
2022-07-10 04:19:54 - train: epoch 0037, iter [02700, 05004], lr: 0.093683, loss: 2.6477
2022-07-10 04:20:29 - train: epoch 0037, iter [02800, 05004], lr: 0.093675, loss: 2.7154
2022-07-10 04:21:04 - train: epoch 0037, iter [02900, 05004], lr: 0.093667, loss: 2.7869
2022-07-10 04:21:38 - train: epoch 0037, iter [03000, 05004], lr: 0.093659, loss: 2.6129
2022-07-10 04:22:13 - train: epoch 0037, iter [03100, 05004], lr: 0.093652, loss: 2.6079
2022-07-10 04:22:48 - train: epoch 0037, iter [03200, 05004], lr: 0.093644, loss: 2.8954
2022-07-10 04:23:23 - train: epoch 0037, iter [03300, 05004], lr: 0.093636, loss: 2.5294
2022-07-10 04:23:58 - train: epoch 0037, iter [03400, 05004], lr: 0.093628, loss: 2.5726
2022-07-10 04:24:32 - train: epoch 0037, iter [03500, 05004], lr: 0.093620, loss: 2.7821
2022-07-10 04:25:08 - train: epoch 0037, iter [03600, 05004], lr: 0.093612, loss: 2.6864
2022-07-10 04:25:44 - train: epoch 0037, iter [03700, 05004], lr: 0.093604, loss: 2.6292
2022-07-10 04:26:18 - train: epoch 0037, iter [03800, 05004], lr: 0.093596, loss: 2.2643
2022-07-10 04:26:53 - train: epoch 0037, iter [03900, 05004], lr: 0.093589, loss: 2.6841
2022-07-10 04:27:29 - train: epoch 0037, iter [04000, 05004], lr: 0.093581, loss: 2.3835
2022-07-10 04:28:03 - train: epoch 0037, iter [04100, 05004], lr: 0.093573, loss: 2.6820
2022-07-10 04:28:38 - train: epoch 0037, iter [04200, 05004], lr: 0.093565, loss: 2.9361
2022-07-10 04:29:13 - train: epoch 0037, iter [04300, 05004], lr: 0.093557, loss: 2.6138
2022-07-10 04:29:48 - train: epoch 0037, iter [04400, 05004], lr: 0.093549, loss: 2.5471
2022-07-10 04:30:22 - train: epoch 0037, iter [04500, 05004], lr: 0.093541, loss: 2.5943
2022-07-10 04:30:57 - train: epoch 0037, iter [04600, 05004], lr: 0.093533, loss: 2.4428
2022-07-10 04:31:32 - train: epoch 0037, iter [04700, 05004], lr: 0.093525, loss: 2.5957
2022-07-10 04:32:08 - train: epoch 0037, iter [04800, 05004], lr: 0.093517, loss: 2.6555
2022-07-10 04:32:43 - train: epoch 0037, iter [04900, 05004], lr: 0.093509, loss: 2.6481
2022-07-10 04:33:16 - train: epoch 0037, iter [05000, 05004], lr: 0.093502, loss: 2.5380
2022-07-10 04:33:18 - train: epoch 037, train_loss: 2.5795
2022-07-10 04:34:33 - eval: epoch: 037, acc1: 54.724%, acc5: 79.916%, test_loss: 1.9233, per_image_load_time: 2.419ms, per_image_inference_time: 0.471ms
2022-07-10 04:34:33 - until epoch: 037, best_acc1: 56.924%
2022-07-10 04:34:33 - epoch 038 lr: 0.093501
2022-07-10 04:35:13 - train: epoch 0038, iter [00100, 05004], lr: 0.093493, loss: 2.5358
2022-07-10 04:35:48 - train: epoch 0038, iter [00200, 05004], lr: 0.093485, loss: 2.3757
2022-07-10 04:36:22 - train: epoch 0038, iter [00300, 05004], lr: 0.093477, loss: 2.1370
2022-07-10 04:36:57 - train: epoch 0038, iter [00400, 05004], lr: 0.093469, loss: 2.5364
2022-07-10 04:37:32 - train: epoch 0038, iter [00500, 05004], lr: 0.093462, loss: 2.3723
2022-07-10 04:38:07 - train: epoch 0038, iter [00600, 05004], lr: 0.093454, loss: 2.6810
2022-07-10 04:38:41 - train: epoch 0038, iter [00700, 05004], lr: 0.093446, loss: 2.3010
2022-07-10 04:39:16 - train: epoch 0038, iter [00800, 05004], lr: 0.093438, loss: 2.2576
2022-07-10 04:39:50 - train: epoch 0038, iter [00900, 05004], lr: 0.093430, loss: 2.5755
2022-07-10 04:40:25 - train: epoch 0038, iter [01000, 05004], lr: 0.093422, loss: 2.8372
2022-07-10 04:40:59 - train: epoch 0038, iter [01100, 05004], lr: 0.093414, loss: 2.4702
2022-07-10 04:41:34 - train: epoch 0038, iter [01200, 05004], lr: 0.093406, loss: 2.8303
2022-07-10 04:42:09 - train: epoch 0038, iter [01300, 05004], lr: 0.093398, loss: 2.5962
2022-07-10 04:42:44 - train: epoch 0038, iter [01400, 05004], lr: 0.093390, loss: 2.6872
2022-07-10 04:43:20 - train: epoch 0038, iter [01500, 05004], lr: 0.093382, loss: 2.8242
2022-07-10 04:43:55 - train: epoch 0038, iter [01600, 05004], lr: 0.093374, loss: 2.6934
2022-07-10 04:44:29 - train: epoch 0038, iter [01700, 05004], lr: 0.093366, loss: 2.6885
2022-07-10 04:45:04 - train: epoch 0038, iter [01800, 05004], lr: 0.093358, loss: 2.7936
2022-07-10 04:45:39 - train: epoch 0038, iter [01900, 05004], lr: 0.093350, loss: 2.6602
2022-07-10 04:46:13 - train: epoch 0038, iter [02000, 05004], lr: 0.093342, loss: 2.5246
2022-07-10 04:46:48 - train: epoch 0038, iter [02100, 05004], lr: 0.093334, loss: 2.6219
2022-07-10 04:47:23 - train: epoch 0038, iter [02200, 05004], lr: 0.093326, loss: 2.6058
2022-07-10 04:47:58 - train: epoch 0038, iter [02300, 05004], lr: 0.093318, loss: 2.8760
2022-07-10 04:48:34 - train: epoch 0038, iter [02400, 05004], lr: 0.093309, loss: 2.7691
2022-07-10 04:49:09 - train: epoch 0038, iter [02500, 05004], lr: 0.093301, loss: 2.5616
2022-07-10 04:49:43 - train: epoch 0038, iter [02600, 05004], lr: 0.093293, loss: 2.5372
2022-07-10 04:50:19 - train: epoch 0038, iter [02700, 05004], lr: 0.093285, loss: 2.5396
2022-07-10 04:50:54 - train: epoch 0038, iter [02800, 05004], lr: 0.093277, loss: 2.7670
2022-07-10 04:51:28 - train: epoch 0038, iter [02900, 05004], lr: 0.093269, loss: 2.7157
2022-07-10 04:52:04 - train: epoch 0038, iter [03000, 05004], lr: 0.093261, loss: 2.6607
2022-07-10 04:52:39 - train: epoch 0038, iter [03100, 05004], lr: 0.093253, loss: 2.7088
2022-07-10 04:53:15 - train: epoch 0038, iter [03200, 05004], lr: 0.093245, loss: 2.4100
2022-07-10 04:53:50 - train: epoch 0038, iter [03300, 05004], lr: 0.093237, loss: 2.3053
2022-07-10 04:54:25 - train: epoch 0038, iter [03400, 05004], lr: 0.093229, loss: 2.5256
2022-07-10 04:55:01 - train: epoch 0038, iter [03500, 05004], lr: 0.093221, loss: 2.5919
2022-07-10 04:55:36 - train: epoch 0038, iter [03600, 05004], lr: 0.093213, loss: 2.4880
2022-07-10 04:56:10 - train: epoch 0038, iter [03700, 05004], lr: 0.093205, loss: 2.2715
2022-07-10 04:56:46 - train: epoch 0038, iter [03800, 05004], lr: 0.093196, loss: 2.6056
2022-07-10 04:57:20 - train: epoch 0038, iter [03900, 05004], lr: 0.093188, loss: 2.6659
2022-07-10 04:57:55 - train: epoch 0038, iter [04000, 05004], lr: 0.093180, loss: 2.7341
2022-07-10 04:58:31 - train: epoch 0038, iter [04100, 05004], lr: 0.093172, loss: 2.4009
2022-07-10 04:59:06 - train: epoch 0038, iter [04200, 05004], lr: 0.093164, loss: 2.3608
2022-07-10 04:59:40 - train: epoch 0038, iter [04300, 05004], lr: 0.093156, loss: 2.7467
2022-07-10 05:00:16 - train: epoch 0038, iter [04400, 05004], lr: 0.093148, loss: 2.7339
2022-07-10 05:00:50 - train: epoch 0038, iter [04500, 05004], lr: 0.093140, loss: 2.6477
2022-07-10 05:01:25 - train: epoch 0038, iter [04600, 05004], lr: 0.093131, loss: 2.9611
2022-07-10 05:02:00 - train: epoch 0038, iter [04700, 05004], lr: 0.093123, loss: 2.5511
2022-07-10 05:02:36 - train: epoch 0038, iter [04800, 05004], lr: 0.093115, loss: 2.6704
2022-07-10 05:03:10 - train: epoch 0038, iter [04900, 05004], lr: 0.093107, loss: 2.5165
2022-07-10 05:03:44 - train: epoch 0038, iter [05000, 05004], lr: 0.093099, loss: 2.2870
2022-07-10 05:03:45 - train: epoch 038, train_loss: 2.5766
2022-07-10 05:05:00 - eval: epoch: 038, acc1: 55.938%, acc5: 80.332%, test_loss: 1.8812, per_image_load_time: 2.426ms, per_image_inference_time: 0.493ms
2022-07-10 05:05:01 - until epoch: 038, best_acc1: 56.924%
2022-07-10 05:05:01 - epoch 039 lr: 0.093098
2022-07-10 05:05:40 - train: epoch 0039, iter [00100, 05004], lr: 0.093090, loss: 2.8157
2022-07-10 05:06:15 - train: epoch 0039, iter [00200, 05004], lr: 0.093082, loss: 2.7836
2022-07-10 05:06:50 - train: epoch 0039, iter [00300, 05004], lr: 0.093074, loss: 2.5918
2022-07-10 05:07:23 - train: epoch 0039, iter [00400, 05004], lr: 0.093066, loss: 2.6166
2022-07-10 05:07:59 - train: epoch 0039, iter [00500, 05004], lr: 0.093058, loss: 2.4058
2022-07-10 05:08:34 - train: epoch 0039, iter [00600, 05004], lr: 0.093049, loss: 2.4158
2022-07-10 05:09:08 - train: epoch 0039, iter [00700, 05004], lr: 0.093041, loss: 2.8267
2022-07-10 05:09:43 - train: epoch 0039, iter [00800, 05004], lr: 0.093033, loss: 2.3946
2022-07-10 05:10:18 - train: epoch 0039, iter [00900, 05004], lr: 0.093025, loss: 2.6544
2022-07-10 05:10:53 - train: epoch 0039, iter [01000, 05004], lr: 0.093017, loss: 2.4791
2022-07-10 05:11:27 - train: epoch 0039, iter [01100, 05004], lr: 0.093008, loss: 2.5796
2022-07-10 05:12:01 - train: epoch 0039, iter [01200, 05004], lr: 0.093000, loss: 2.7502
2022-07-10 05:12:37 - train: epoch 0039, iter [01300, 05004], lr: 0.092992, loss: 2.8495
2022-07-10 05:13:12 - train: epoch 0039, iter [01400, 05004], lr: 0.092984, loss: 2.7866
2022-07-10 05:13:46 - train: epoch 0039, iter [01500, 05004], lr: 0.092976, loss: 2.5034
2022-07-10 05:14:21 - train: epoch 0039, iter [01600, 05004], lr: 0.092967, loss: 2.6933
2022-07-10 05:14:56 - train: epoch 0039, iter [01700, 05004], lr: 0.092959, loss: 2.2216
2022-07-10 05:15:31 - train: epoch 0039, iter [01800, 05004], lr: 0.092951, loss: 2.3531
2022-07-10 05:16:06 - train: epoch 0039, iter [01900, 05004], lr: 0.092943, loss: 2.3033
2022-07-10 05:16:40 - train: epoch 0039, iter [02000, 05004], lr: 0.092934, loss: 2.6367
2022-07-10 05:17:16 - train: epoch 0039, iter [02100, 05004], lr: 0.092926, loss: 2.7015
2022-07-10 05:17:50 - train: epoch 0039, iter [02200, 05004], lr: 0.092918, loss: 2.6011
2022-07-10 05:18:26 - train: epoch 0039, iter [02300, 05004], lr: 0.092910, loss: 2.7382
2022-07-10 05:19:01 - train: epoch 0039, iter [02400, 05004], lr: 0.092901, loss: 2.6406
2022-07-10 05:19:36 - train: epoch 0039, iter [02500, 05004], lr: 0.092893, loss: 2.4198
2022-07-10 05:20:11 - train: epoch 0039, iter [02600, 05004], lr: 0.092885, loss: 2.5941
2022-07-10 05:20:46 - train: epoch 0039, iter [02700, 05004], lr: 0.092877, loss: 2.7300
2022-07-10 05:21:21 - train: epoch 0039, iter [02800, 05004], lr: 0.092868, loss: 2.5068
2022-07-10 05:21:57 - train: epoch 0039, iter [02900, 05004], lr: 0.092860, loss: 2.2572
2022-07-10 05:22:31 - train: epoch 0039, iter [03000, 05004], lr: 0.092852, loss: 2.4518
2022-07-10 05:23:07 - train: epoch 0039, iter [03100, 05004], lr: 0.092843, loss: 2.4599
2022-07-10 05:23:42 - train: epoch 0039, iter [03200, 05004], lr: 0.092835, loss: 2.4567
2022-07-10 05:24:17 - train: epoch 0039, iter [03300, 05004], lr: 0.092827, loss: 2.5718
2022-07-10 05:24:52 - train: epoch 0039, iter [03400, 05004], lr: 0.092818, loss: 2.6901
2022-07-10 05:25:27 - train: epoch 0039, iter [03500, 05004], lr: 0.092810, loss: 2.9393
2022-07-10 05:26:02 - train: epoch 0039, iter [03600, 05004], lr: 0.092802, loss: 2.7133
2022-07-10 05:26:38 - train: epoch 0039, iter [03700, 05004], lr: 0.092793, loss: 2.5138
2022-07-10 05:27:13 - train: epoch 0039, iter [03800, 05004], lr: 0.092785, loss: 2.4979
2022-07-10 05:27:46 - train: epoch 0039, iter [03900, 05004], lr: 0.092777, loss: 2.9127
2022-07-10 05:28:23 - train: epoch 0039, iter [04000, 05004], lr: 0.092768, loss: 2.6136
2022-07-10 05:28:57 - train: epoch 0039, iter [04100, 05004], lr: 0.092760, loss: 2.6964
2022-07-10 05:29:32 - train: epoch 0039, iter [04200, 05004], lr: 0.092752, loss: 2.6031
2022-07-10 05:30:07 - train: epoch 0039, iter [04300, 05004], lr: 0.092743, loss: 2.5163
2022-07-10 05:30:42 - train: epoch 0039, iter [04400, 05004], lr: 0.092735, loss: 2.2799
2022-07-10 05:31:17 - train: epoch 0039, iter [04500, 05004], lr: 0.092727, loss: 2.5595
2022-07-10 05:31:53 - train: epoch 0039, iter [04600, 05004], lr: 0.092718, loss: 2.7354
2022-07-10 05:32:27 - train: epoch 0039, iter [04700, 05004], lr: 0.092710, loss: 2.4742
2022-07-10 05:33:03 - train: epoch 0039, iter [04800, 05004], lr: 0.092702, loss: 2.4918
2022-07-10 05:33:38 - train: epoch 0039, iter [04900, 05004], lr: 0.092693, loss: 2.3172
2022-07-10 05:34:12 - train: epoch 0039, iter [05000, 05004], lr: 0.092685, loss: 2.3038
2022-07-10 05:34:13 - train: epoch 039, train_loss: 2.5695
2022-07-10 05:35:28 - eval: epoch: 039, acc1: 55.170%, acc5: 80.376%, test_loss: 1.9001, per_image_load_time: 2.422ms, per_image_inference_time: 0.495ms
2022-07-10 05:35:29 - until epoch: 039, best_acc1: 56.924%
2022-07-10 05:35:29 - epoch 040 lr: 0.092684
2022-07-10 05:36:08 - train: epoch 0040, iter [00100, 05004], lr: 0.092676, loss: 2.9525
2022-07-10 05:36:43 - train: epoch 0040, iter [00200, 05004], lr: 0.092668, loss: 2.8733
2022-07-10 05:37:17 - train: epoch 0040, iter [00300, 05004], lr: 0.092659, loss: 2.5007
2022-07-10 05:37:52 - train: epoch 0040, iter [00400, 05004], lr: 0.092651, loss: 2.5018
2022-07-10 05:38:26 - train: epoch 0040, iter [00500, 05004], lr: 0.092643, loss: 2.4563
2022-07-10 05:39:00 - train: epoch 0040, iter [00600, 05004], lr: 0.092634, loss: 2.6765
2022-07-10 05:39:35 - train: epoch 0040, iter [00700, 05004], lr: 0.092626, loss: 2.6807
2022-07-10 05:40:10 - train: epoch 0040, iter [00800, 05004], lr: 0.092617, loss: 2.7158
2022-07-10 05:40:44 - train: epoch 0040, iter [00900, 05004], lr: 0.092609, loss: 2.4808
2022-07-10 05:41:18 - train: epoch 0040, iter [01000, 05004], lr: 0.092600, loss: 2.2171
2022-07-10 05:41:54 - train: epoch 0040, iter [01100, 05004], lr: 0.092592, loss: 2.4789
2022-07-10 05:42:28 - train: epoch 0040, iter [01200, 05004], lr: 0.092584, loss: 2.5373
2022-07-10 05:43:03 - train: epoch 0040, iter [01300, 05004], lr: 0.092575, loss: 2.4362
2022-07-10 05:43:38 - train: epoch 0040, iter [01400, 05004], lr: 0.092567, loss: 2.4006
2022-07-10 05:44:13 - train: epoch 0040, iter [01500, 05004], lr: 0.092558, loss: 2.4361
2022-07-10 05:44:47 - train: epoch 0040, iter [01600, 05004], lr: 0.092550, loss: 2.4924
2022-07-10 05:45:22 - train: epoch 0040, iter [01700, 05004], lr: 0.092541, loss: 2.5900
2022-07-10 05:45:56 - train: epoch 0040, iter [01800, 05004], lr: 0.092533, loss: 2.3845
2022-07-10 05:46:31 - train: epoch 0040, iter [01900, 05004], lr: 0.092524, loss: 2.4605
2022-07-10 05:47:07 - train: epoch 0040, iter [02000, 05004], lr: 0.092516, loss: 2.7103
2022-07-10 05:47:42 - train: epoch 0040, iter [02100, 05004], lr: 0.092508, loss: 2.3244
2022-07-10 05:48:17 - train: epoch 0040, iter [02200, 05004], lr: 0.092499, loss: 2.4182
2022-07-10 05:48:52 - train: epoch 0040, iter [02300, 05004], lr: 0.092491, loss: 2.4685
2022-07-10 05:49:27 - train: epoch 0040, iter [02400, 05004], lr: 0.092482, loss: 2.5422
2022-07-10 05:50:02 - train: epoch 0040, iter [02500, 05004], lr: 0.092474, loss: 2.6307
2022-07-10 05:50:36 - train: epoch 0040, iter [02600, 05004], lr: 0.092465, loss: 2.5763
2022-07-10 05:51:12 - train: epoch 0040, iter [02700, 05004], lr: 0.092457, loss: 2.8876
2022-07-10 05:51:47 - train: epoch 0040, iter [02800, 05004], lr: 0.092448, loss: 2.4548
2022-07-10 05:52:21 - train: epoch 0040, iter [02900, 05004], lr: 0.092440, loss: 2.8394
2022-07-10 05:52:57 - train: epoch 0040, iter [03000, 05004], lr: 0.092431, loss: 2.5705
2022-07-10 05:53:31 - train: epoch 0040, iter [03100, 05004], lr: 0.092423, loss: 2.4754
2022-07-10 05:54:06 - train: epoch 0040, iter [03200, 05004], lr: 0.092414, loss: 2.6732
2022-07-10 05:54:40 - train: epoch 0040, iter [03300, 05004], lr: 0.092405, loss: 2.5714
2022-07-10 05:55:16 - train: epoch 0040, iter [03400, 05004], lr: 0.092397, loss: 2.4336
2022-07-10 05:55:51 - train: epoch 0040, iter [03500, 05004], lr: 0.092388, loss: 2.5235
2022-07-10 05:56:25 - train: epoch 0040, iter [03600, 05004], lr: 0.092380, loss: 2.4564
2022-07-10 05:57:01 - train: epoch 0040, iter [03700, 05004], lr: 0.092371, loss: 2.5888
2022-07-10 05:57:36 - train: epoch 0040, iter [03800, 05004], lr: 0.092363, loss: 2.3926
2022-07-10 05:58:10 - train: epoch 0040, iter [03900, 05004], lr: 0.092354, loss: 2.6837
2022-07-10 05:58:45 - train: epoch 0040, iter [04000, 05004], lr: 0.092346, loss: 2.5587
2022-07-10 05:59:20 - train: epoch 0040, iter [04100, 05004], lr: 0.092337, loss: 2.7458
2022-07-10 05:59:55 - train: epoch 0040, iter [04200, 05004], lr: 0.092329, loss: 2.3988
2022-07-10 06:00:30 - train: epoch 0040, iter [04300, 05004], lr: 0.092320, loss: 2.5238
2022-07-10 06:01:04 - train: epoch 0040, iter [04400, 05004], lr: 0.092311, loss: 2.6627
2022-07-10 06:01:40 - train: epoch 0040, iter [04500, 05004], lr: 0.092303, loss: 2.5508
2022-07-10 06:02:15 - train: epoch 0040, iter [04600, 05004], lr: 0.092294, loss: 2.3006
2022-07-10 06:02:50 - train: epoch 0040, iter [04700, 05004], lr: 0.092286, loss: 2.6187
2022-07-10 06:03:25 - train: epoch 0040, iter [04800, 05004], lr: 0.092277, loss: 2.5745
2022-07-10 06:04:01 - train: epoch 0040, iter [04900, 05004], lr: 0.092268, loss: 2.6975
2022-07-10 06:04:33 - train: epoch 0040, iter [05000, 05004], lr: 0.092260, loss: 2.7483
2022-07-10 06:04:34 - train: epoch 040, train_loss: 2.5651
2022-07-10 06:05:50 - eval: epoch: 040, acc1: 56.298%, acc5: 81.372%, test_loss: 1.8365, per_image_load_time: 2.417ms, per_image_inference_time: 0.465ms
2022-07-10 06:05:50 - until epoch: 040, best_acc1: 56.924%
2022-07-10 06:05:50 - epoch 041 lr: 0.092259
2022-07-10 06:06:30 - train: epoch 0041, iter [00100, 05004], lr: 0.092251, loss: 2.8163
2022-07-10 06:07:04 - train: epoch 0041, iter [00200, 05004], lr: 0.092242, loss: 2.6306
2022-07-10 06:07:39 - train: epoch 0041, iter [00300, 05004], lr: 0.092234, loss: 2.4049
2022-07-10 06:08:14 - train: epoch 0041, iter [00400, 05004], lr: 0.092225, loss: 2.5722
2022-07-10 06:08:47 - train: epoch 0041, iter [00500, 05004], lr: 0.092216, loss: 2.3543
2022-07-10 06:09:22 - train: epoch 0041, iter [00600, 05004], lr: 0.092208, loss: 2.7144
2022-07-10 06:09:56 - train: epoch 0041, iter [00700, 05004], lr: 0.092199, loss: 2.4973
2022-07-10 06:10:31 - train: epoch 0041, iter [00800, 05004], lr: 0.092191, loss: 2.3036
2022-07-10 06:11:06 - train: epoch 0041, iter [00900, 05004], lr: 0.092182, loss: 2.3549
2022-07-10 06:11:40 - train: epoch 0041, iter [01000, 05004], lr: 0.092173, loss: 2.6855
2022-07-10 06:12:15 - train: epoch 0041, iter [01100, 05004], lr: 0.092165, loss: 2.4332
2022-07-10 06:12:50 - train: epoch 0041, iter [01200, 05004], lr: 0.092156, loss: 2.3513
2022-07-10 06:13:24 - train: epoch 0041, iter [01300, 05004], lr: 0.092147, loss: 2.6561
2022-07-10 06:13:59 - train: epoch 0041, iter [01400, 05004], lr: 0.092139, loss: 2.6463
2022-07-10 06:14:34 - train: epoch 0041, iter [01500, 05004], lr: 0.092130, loss: 2.8184
2022-07-10 06:15:09 - train: epoch 0041, iter [01600, 05004], lr: 0.092121, loss: 2.3627
2022-07-10 06:15:43 - train: epoch 0041, iter [01700, 05004], lr: 0.092113, loss: 2.6344
2022-07-10 06:16:18 - train: epoch 0041, iter [01800, 05004], lr: 0.092104, loss: 2.6530
2022-07-10 06:16:53 - train: epoch 0041, iter [01900, 05004], lr: 0.092095, loss: 2.8932
2022-07-10 06:17:27 - train: epoch 0041, iter [02000, 05004], lr: 0.092087, loss: 2.1874
2022-07-10 06:18:02 - train: epoch 0041, iter [02100, 05004], lr: 0.092078, loss: 2.3736
2022-07-10 06:18:36 - train: epoch 0041, iter [02200, 05004], lr: 0.092069, loss: 2.4165
2022-07-10 06:19:11 - train: epoch 0041, iter [02300, 05004], lr: 0.092060, loss: 2.3058
2022-07-10 06:19:46 - train: epoch 0041, iter [02400, 05004], lr: 0.092052, loss: 2.7732
2022-07-10 06:20:21 - train: epoch 0041, iter [02500, 05004], lr: 0.092043, loss: 2.2866
2022-07-10 06:20:56 - train: epoch 0041, iter [02600, 05004], lr: 0.092034, loss: 2.7172
2022-07-10 06:21:32 - train: epoch 0041, iter [02700, 05004], lr: 0.092026, loss: 3.0221
2022-07-10 06:22:06 - train: epoch 0041, iter [02800, 05004], lr: 0.092017, loss: 2.5066
2022-07-10 06:22:42 - train: epoch 0041, iter [02900, 05004], lr: 0.092008, loss: 2.5872
2022-07-10 06:23:15 - train: epoch 0041, iter [03000, 05004], lr: 0.091999, loss: 2.5698
2022-07-10 06:23:51 - train: epoch 0041, iter [03100, 05004], lr: 0.091991, loss: 2.7820
2022-07-10 06:24:26 - train: epoch 0041, iter [03200, 05004], lr: 0.091982, loss: 2.5097
2022-07-10 06:25:01 - train: epoch 0041, iter [03300, 05004], lr: 0.091973, loss: 2.2579
2022-07-10 06:25:36 - train: epoch 0041, iter [03400, 05004], lr: 0.091964, loss: 2.2966
2022-07-10 06:26:11 - train: epoch 0041, iter [03500, 05004], lr: 0.091956, loss: 2.6195
2022-07-10 06:26:47 - train: epoch 0041, iter [03600, 05004], lr: 0.091947, loss: 2.5966
2022-07-10 06:27:21 - train: epoch 0041, iter [03700, 05004], lr: 0.091938, loss: 2.6381
2022-07-10 06:27:56 - train: epoch 0041, iter [03800, 05004], lr: 0.091929, loss: 2.4394
2022-07-10 06:28:32 - train: epoch 0041, iter [03900, 05004], lr: 0.091921, loss: 2.4411
2022-07-10 06:29:06 - train: epoch 0041, iter [04000, 05004], lr: 0.091912, loss: 2.4058
2022-07-10 06:29:42 - train: epoch 0041, iter [04100, 05004], lr: 0.091903, loss: 2.7049
2022-07-10 06:30:16 - train: epoch 0041, iter [04200, 05004], lr: 0.091894, loss: 2.6634
2022-07-10 06:30:51 - train: epoch 0041, iter [04300, 05004], lr: 0.091886, loss: 2.5248
2022-07-10 06:31:27 - train: epoch 0041, iter [04400, 05004], lr: 0.091877, loss: 2.6042
2022-07-10 06:32:01 - train: epoch 0041, iter [04500, 05004], lr: 0.091868, loss: 2.6007
2022-07-10 06:32:36 - train: epoch 0041, iter [04600, 05004], lr: 0.091859, loss: 2.5978
2022-07-10 06:33:12 - train: epoch 0041, iter [04700, 05004], lr: 0.091850, loss: 2.7344
2022-07-10 06:33:47 - train: epoch 0041, iter [04800, 05004], lr: 0.091841, loss: 2.7280
2022-07-10 06:34:21 - train: epoch 0041, iter [04900, 05004], lr: 0.091833, loss: 2.4658
2022-07-10 06:34:56 - train: epoch 0041, iter [05000, 05004], lr: 0.091824, loss: 2.7283
2022-07-10 06:34:58 - train: epoch 041, train_loss: 2.5616
2022-07-10 06:36:13 - eval: epoch: 041, acc1: 55.192%, acc5: 80.096%, test_loss: 1.9055, per_image_load_time: 2.424ms, per_image_inference_time: 0.497ms
2022-07-10 06:36:13 - until epoch: 041, best_acc1: 56.924%
2022-07-10 06:36:13 - epoch 042 lr: 0.091823
2022-07-10 06:36:52 - train: epoch 0042, iter [00100, 05004], lr: 0.091815, loss: 2.3956
2022-07-10 06:37:28 - train: epoch 0042, iter [00200, 05004], lr: 0.091806, loss: 2.6703
2022-07-10 06:38:02 - train: epoch 0042, iter [00300, 05004], lr: 0.091797, loss: 2.4080
2022-07-10 06:38:37 - train: epoch 0042, iter [00400, 05004], lr: 0.091788, loss: 2.2149
2022-07-10 06:39:11 - train: epoch 0042, iter [00500, 05004], lr: 0.091779, loss: 2.6706
2022-07-10 06:39:46 - train: epoch 0042, iter [00600, 05004], lr: 0.091770, loss: 2.2728
2022-07-10 06:40:20 - train: epoch 0042, iter [00700, 05004], lr: 0.091762, loss: 2.7022
2022-07-10 06:40:54 - train: epoch 0042, iter [00800, 05004], lr: 0.091753, loss: 2.4224
2022-07-10 06:41:29 - train: epoch 0042, iter [00900, 05004], lr: 0.091744, loss: 2.8407
2022-07-10 06:42:04 - train: epoch 0042, iter [01000, 05004], lr: 0.091735, loss: 2.6124
2022-07-10 06:42:39 - train: epoch 0042, iter [01100, 05004], lr: 0.091726, loss: 2.3841
2022-07-10 06:43:13 - train: epoch 0042, iter [01200, 05004], lr: 0.091717, loss: 2.5298
2022-07-10 06:43:48 - train: epoch 0042, iter [01300, 05004], lr: 0.091708, loss: 2.5344
2022-07-10 06:44:23 - train: epoch 0042, iter [01400, 05004], lr: 0.091700, loss: 2.8450
2022-07-10 06:44:57 - train: epoch 0042, iter [01500, 05004], lr: 0.091691, loss: 2.5428
2022-07-10 06:45:32 - train: epoch 0042, iter [01600, 05004], lr: 0.091682, loss: 2.7579
2022-07-10 06:46:07 - train: epoch 0042, iter [01700, 05004], lr: 0.091673, loss: 2.6200
2022-07-10 06:46:42 - train: epoch 0042, iter [01800, 05004], lr: 0.091664, loss: 2.6564
2022-07-10 06:47:17 - train: epoch 0042, iter [01900, 05004], lr: 0.091655, loss: 2.8706
2022-07-10 06:47:51 - train: epoch 0042, iter [02000, 05004], lr: 0.091646, loss: 2.3366
2022-07-10 06:48:26 - train: epoch 0042, iter [02100, 05004], lr: 0.091637, loss: 2.2674
2022-07-10 06:49:02 - train: epoch 0042, iter [02200, 05004], lr: 0.091628, loss: 2.5294
2022-07-10 06:49:36 - train: epoch 0042, iter [02300, 05004], lr: 0.091619, loss: 2.5576
2022-07-10 06:50:11 - train: epoch 0042, iter [02400, 05004], lr: 0.091611, loss: 2.8218
2022-07-10 06:50:47 - train: epoch 0042, iter [02500, 05004], lr: 0.091602, loss: 2.7802
2022-07-10 06:51:21 - train: epoch 0042, iter [02600, 05004], lr: 0.091593, loss: 2.5932
2022-07-10 06:51:56 - train: epoch 0042, iter [02700, 05004], lr: 0.091584, loss: 2.2842
2022-07-10 06:52:31 - train: epoch 0042, iter [02800, 05004], lr: 0.091575, loss: 2.6250
2022-07-10 06:53:06 - train: epoch 0042, iter [02900, 05004], lr: 0.091566, loss: 2.6292
2022-07-10 06:53:41 - train: epoch 0042, iter [03000, 05004], lr: 0.091557, loss: 2.6545
2022-07-10 06:54:16 - train: epoch 0042, iter [03100, 05004], lr: 0.091548, loss: 2.3579
2022-07-10 06:54:51 - train: epoch 0042, iter [03200, 05004], lr: 0.091539, loss: 2.4946
2022-07-10 06:55:26 - train: epoch 0042, iter [03300, 05004], lr: 0.091530, loss: 2.7737
2022-07-10 06:56:01 - train: epoch 0042, iter [03400, 05004], lr: 0.091521, loss: 2.4265
2022-07-10 06:56:35 - train: epoch 0042, iter [03500, 05004], lr: 0.091512, loss: 2.4395
2022-07-10 06:57:09 - train: epoch 0042, iter [03600, 05004], lr: 0.091503, loss: 2.6097
2022-07-10 06:57:44 - train: epoch 0042, iter [03700, 05004], lr: 0.091494, loss: 2.5412
2022-07-10 06:58:20 - train: epoch 0042, iter [03800, 05004], lr: 0.091485, loss: 2.5009
2022-07-10 06:58:55 - train: epoch 0042, iter [03900, 05004], lr: 0.091476, loss: 2.5693
2022-07-10 06:59:29 - train: epoch 0042, iter [04000, 05004], lr: 0.091467, loss: 2.6002
2022-07-10 07:00:04 - train: epoch 0042, iter [04100, 05004], lr: 0.091458, loss: 2.7496
2022-07-10 07:00:39 - train: epoch 0042, iter [04200, 05004], lr: 0.091449, loss: 2.6411
2022-07-10 07:01:14 - train: epoch 0042, iter [04300, 05004], lr: 0.091440, loss: 2.6043
2022-07-10 07:01:49 - train: epoch 0042, iter [04400, 05004], lr: 0.091431, loss: 2.5241
2022-07-10 07:02:24 - train: epoch 0042, iter [04500, 05004], lr: 0.091422, loss: 2.3906
2022-07-10 07:02:59 - train: epoch 0042, iter [04600, 05004], lr: 0.091413, loss: 2.6724
2022-07-10 07:03:33 - train: epoch 0042, iter [04700, 05004], lr: 0.091404, loss: 2.3856
2022-07-10 07:04:08 - train: epoch 0042, iter [04800, 05004], lr: 0.091395, loss: 2.5802
2022-07-10 07:04:44 - train: epoch 0042, iter [04900, 05004], lr: 0.091386, loss: 2.7287
2022-07-10 07:05:18 - train: epoch 0042, iter [05000, 05004], lr: 0.091377, loss: 2.5808
2022-07-10 07:05:19 - train: epoch 042, train_loss: 2.5526
2022-07-10 07:06:34 - eval: epoch: 042, acc1: 55.600%, acc5: 80.114%, test_loss: 1.8939, per_image_load_time: 2.381ms, per_image_inference_time: 0.483ms
2022-07-10 07:06:34 - until epoch: 042, best_acc1: 56.924%
2022-07-10 07:06:34 - epoch 043 lr: 0.091377
2022-07-10 07:07:14 - train: epoch 0043, iter [00100, 05004], lr: 0.091368, loss: 2.6558
2022-07-10 07:07:49 - train: epoch 0043, iter [00200, 05004], lr: 0.091359, loss: 2.7082
2022-07-10 07:08:23 - train: epoch 0043, iter [00300, 05004], lr: 0.091350, loss: 2.2850
2022-07-10 07:08:58 - train: epoch 0043, iter [00400, 05004], lr: 0.091340, loss: 2.4652
2022-07-10 07:09:33 - train: epoch 0043, iter [00500, 05004], lr: 0.091331, loss: 2.3565
2022-07-10 07:10:07 - train: epoch 0043, iter [00600, 05004], lr: 0.091322, loss: 2.3748
2022-07-10 07:10:41 - train: epoch 0043, iter [00700, 05004], lr: 0.091313, loss: 2.6759
2022-07-10 07:11:15 - train: epoch 0043, iter [00800, 05004], lr: 0.091304, loss: 2.6244
2022-07-10 07:11:50 - train: epoch 0043, iter [00900, 05004], lr: 0.091295, loss: 2.5251
2022-07-10 07:12:25 - train: epoch 0043, iter [01000, 05004], lr: 0.091286, loss: 2.7681
2022-07-10 07:13:00 - train: epoch 0043, iter [01100, 05004], lr: 0.091277, loss: 2.5950
2022-07-10 07:13:35 - train: epoch 0043, iter [01200, 05004], lr: 0.091268, loss: 2.5483
2022-07-10 07:14:10 - train: epoch 0043, iter [01300, 05004], lr: 0.091259, loss: 2.5803
2022-07-10 07:14:44 - train: epoch 0043, iter [01400, 05004], lr: 0.091250, loss: 2.5381
2022-07-10 07:15:19 - train: epoch 0043, iter [01500, 05004], lr: 0.091241, loss: 2.3865
2022-07-10 07:15:54 - train: epoch 0043, iter [01600, 05004], lr: 0.091232, loss: 2.7214
2022-07-10 07:16:29 - train: epoch 0043, iter [01700, 05004], lr: 0.091222, loss: 2.5450
2022-07-10 07:17:04 - train: epoch 0043, iter [01800, 05004], lr: 0.091213, loss: 2.6392
2022-07-10 07:17:38 - train: epoch 0043, iter [01900, 05004], lr: 0.091204, loss: 2.5380
2022-07-10 07:18:13 - train: epoch 0043, iter [02000, 05004], lr: 0.091195, loss: 2.4562
2022-07-10 07:18:48 - train: epoch 0043, iter [02100, 05004], lr: 0.091186, loss: 2.2793
2022-07-10 07:19:23 - train: epoch 0043, iter [02200, 05004], lr: 0.091177, loss: 2.4944
2022-07-10 07:19:58 - train: epoch 0043, iter [02300, 05004], lr: 0.091168, loss: 2.6324
2022-07-10 07:20:33 - train: epoch 0043, iter [02400, 05004], lr: 0.091159, loss: 2.4650
2022-07-10 07:21:07 - train: epoch 0043, iter [02500, 05004], lr: 0.091149, loss: 2.6149
2022-07-10 07:21:43 - train: epoch 0043, iter [02600, 05004], lr: 0.091140, loss: 2.2459
2022-07-10 07:22:17 - train: epoch 0043, iter [02700, 05004], lr: 0.091131, loss: 2.5486
2022-07-10 07:22:52 - train: epoch 0043, iter [02800, 05004], lr: 0.091122, loss: 2.5112
2022-07-10 07:23:27 - train: epoch 0043, iter [02900, 05004], lr: 0.091113, loss: 2.7368
2022-07-10 07:24:02 - train: epoch 0043, iter [03000, 05004], lr: 0.091104, loss: 2.7998
2022-07-10 07:24:37 - train: epoch 0043, iter [03100, 05004], lr: 0.091094, loss: 2.5601
2022-07-10 07:25:13 - train: epoch 0043, iter [03200, 05004], lr: 0.091085, loss: 2.6152
2022-07-10 07:25:47 - train: epoch 0043, iter [03300, 05004], lr: 0.091076, loss: 2.6872
2022-07-10 07:26:21 - train: epoch 0043, iter [03400, 05004], lr: 0.091067, loss: 2.3936
2022-07-10 07:26:57 - train: epoch 0043, iter [03500, 05004], lr: 0.091058, loss: 2.4831
2022-07-10 07:27:32 - train: epoch 0043, iter [03600, 05004], lr: 0.091049, loss: 2.5809
2022-07-10 07:28:07 - train: epoch 0043, iter [03700, 05004], lr: 0.091039, loss: 2.4773
2022-07-10 07:28:43 - train: epoch 0043, iter [03800, 05004], lr: 0.091030, loss: 2.5723
2022-07-10 07:29:19 - train: epoch 0043, iter [03900, 05004], lr: 0.091021, loss: 2.8071
2022-07-10 07:29:54 - train: epoch 0043, iter [04000, 05004], lr: 0.091012, loss: 2.4300
2022-07-10 07:30:29 - train: epoch 0043, iter [04100, 05004], lr: 0.091003, loss: 3.0400
2022-07-10 07:31:04 - train: epoch 0043, iter [04200, 05004], lr: 0.090993, loss: 2.4768
2022-07-10 07:31:39 - train: epoch 0043, iter [04300, 05004], lr: 0.090984, loss: 2.7642
2022-07-10 07:32:14 - train: epoch 0043, iter [04400, 05004], lr: 0.090975, loss: 2.4664
2022-07-10 07:32:49 - train: epoch 0043, iter [04500, 05004], lr: 0.090966, loss: 2.5234
2022-07-10 07:33:24 - train: epoch 0043, iter [04600, 05004], lr: 0.090956, loss: 2.6049
2022-07-10 07:33:59 - train: epoch 0043, iter [04700, 05004], lr: 0.090947, loss: 2.5180
2022-07-10 07:34:34 - train: epoch 0043, iter [04800, 05004], lr: 0.090938, loss: 2.6266
2022-07-10 07:35:10 - train: epoch 0043, iter [04900, 05004], lr: 0.090929, loss: 2.5785
2022-07-10 07:35:43 - train: epoch 0043, iter [05000, 05004], lr: 0.090919, loss: 2.5831
2022-07-10 07:35:44 - train: epoch 043, train_loss: 2.5471
2022-07-10 07:36:59 - eval: epoch: 043, acc1: 58.212%, acc5: 82.338%, test_loss: 1.7649, per_image_load_time: 2.392ms, per_image_inference_time: 0.505ms
2022-07-10 07:37:00 - until epoch: 043, best_acc1: 58.212%
2022-07-10 07:37:00 - epoch 044 lr: 0.090919
2022-07-10 07:37:40 - train: epoch 0044, iter [00100, 05004], lr: 0.090910, loss: 2.5096
2022-07-10 07:38:15 - train: epoch 0044, iter [00200, 05004], lr: 0.090901, loss: 2.6345
2022-07-10 07:38:49 - train: epoch 0044, iter [00300, 05004], lr: 0.090891, loss: 2.4983
2022-07-10 07:39:24 - train: epoch 0044, iter [00400, 05004], lr: 0.090882, loss: 2.1863
2022-07-10 07:39:59 - train: epoch 0044, iter [00500, 05004], lr: 0.090873, loss: 2.4360
2022-07-10 07:40:34 - train: epoch 0044, iter [00600, 05004], lr: 0.090863, loss: 2.3632
2022-07-10 07:41:09 - train: epoch 0044, iter [00700, 05004], lr: 0.090854, loss: 2.2143
2022-07-10 07:41:43 - train: epoch 0044, iter [00800, 05004], lr: 0.090845, loss: 2.5393
2022-07-10 07:42:18 - train: epoch 0044, iter [00900, 05004], lr: 0.090836, loss: 2.5143
2022-07-10 07:42:52 - train: epoch 0044, iter [01000, 05004], lr: 0.090826, loss: 2.6373
2022-07-10 07:43:27 - train: epoch 0044, iter [01100, 05004], lr: 0.090817, loss: 2.3067
2022-07-10 07:44:02 - train: epoch 0044, iter [01200, 05004], lr: 0.090808, loss: 2.4627
2022-07-10 07:44:36 - train: epoch 0044, iter [01300, 05004], lr: 0.090798, loss: 2.6303
2022-07-10 07:45:11 - train: epoch 0044, iter [01400, 05004], lr: 0.090789, loss: 2.5532
2022-07-10 07:45:45 - train: epoch 0044, iter [01500, 05004], lr: 0.090780, loss: 2.5049
2022-07-10 07:46:20 - train: epoch 0044, iter [01600, 05004], lr: 0.090771, loss: 2.4667
2022-07-10 07:46:55 - train: epoch 0044, iter [01700, 05004], lr: 0.090761, loss: 2.4838
2022-07-10 07:47:30 - train: epoch 0044, iter [01800, 05004], lr: 0.090752, loss: 2.5347
2022-07-10 07:48:05 - train: epoch 0044, iter [01900, 05004], lr: 0.090743, loss: 2.8154
2022-07-10 07:48:40 - train: epoch 0044, iter [02000, 05004], lr: 0.090733, loss: 2.4233
2022-07-10 07:49:14 - train: epoch 0044, iter [02100, 05004], lr: 0.090724, loss: 2.7495
2022-07-10 07:49:49 - train: epoch 0044, iter [02200, 05004], lr: 0.090715, loss: 2.6759
2022-07-10 07:50:24 - train: epoch 0044, iter [02300, 05004], lr: 0.090705, loss: 2.6412
2022-07-10 07:50:59 - train: epoch 0044, iter [02400, 05004], lr: 0.090696, loss: 2.4540
2022-07-10 07:51:34 - train: epoch 0044, iter [02500, 05004], lr: 0.090686, loss: 2.6312
2022-07-10 07:52:09 - train: epoch 0044, iter [02600, 05004], lr: 0.090677, loss: 2.6241
2022-07-10 07:52:44 - train: epoch 0044, iter [02700, 05004], lr: 0.090668, loss: 2.6261
2022-07-10 07:53:18 - train: epoch 0044, iter [02800, 05004], lr: 0.090658, loss: 2.1654
2022-07-10 07:53:53 - train: epoch 0044, iter [02900, 05004], lr: 0.090649, loss: 2.4763
2022-07-10 07:54:28 - train: epoch 0044, iter [03000, 05004], lr: 0.090640, loss: 2.4688
2022-07-10 07:55:03 - train: epoch 0044, iter [03100, 05004], lr: 0.090630, loss: 2.4333
2022-07-10 07:55:37 - train: epoch 0044, iter [03200, 05004], lr: 0.090621, loss: 2.2033
2022-07-10 07:56:13 - train: epoch 0044, iter [03300, 05004], lr: 0.090611, loss: 2.3100
2022-07-10 07:56:47 - train: epoch 0044, iter [03400, 05004], lr: 0.090602, loss: 2.8650
2022-07-10 07:57:22 - train: epoch 0044, iter [03500, 05004], lr: 0.090593, loss: 2.4082
2022-07-10 07:57:58 - train: epoch 0044, iter [03600, 05004], lr: 0.090583, loss: 2.6532
2022-07-10 07:58:32 - train: epoch 0044, iter [03700, 05004], lr: 0.090574, loss: 2.6027
2022-07-10 07:59:07 - train: epoch 0044, iter [03800, 05004], lr: 0.090564, loss: 2.3312
2022-07-10 07:59:42 - train: epoch 0044, iter [03900, 05004], lr: 0.090555, loss: 2.5120
2022-07-10 08:00:17 - train: epoch 0044, iter [04000, 05004], lr: 0.090546, loss: 2.7297
2022-07-10 08:00:53 - train: epoch 0044, iter [04100, 05004], lr: 0.090536, loss: 2.3429
2022-07-10 08:01:27 - train: epoch 0044, iter [04200, 05004], lr: 0.090527, loss: 2.6043
2022-07-10 08:02:02 - train: epoch 0044, iter [04300, 05004], lr: 0.090517, loss: 2.6573
2022-07-10 08:02:37 - train: epoch 0044, iter [04400, 05004], lr: 0.090508, loss: 2.5476
2022-07-10 08:03:12 - train: epoch 0044, iter [04500, 05004], lr: 0.090498, loss: 2.5457
2022-07-10 08:03:47 - train: epoch 0044, iter [04600, 05004], lr: 0.090489, loss: 2.6788
2022-07-10 08:04:22 - train: epoch 0044, iter [04700, 05004], lr: 0.090480, loss: 2.5748
2022-07-10 08:04:57 - train: epoch 0044, iter [04800, 05004], lr: 0.090470, loss: 2.7089
2022-07-10 08:05:32 - train: epoch 0044, iter [04900, 05004], lr: 0.090461, loss: 2.5148
2022-07-10 08:06:05 - train: epoch 0044, iter [05000, 05004], lr: 0.090451, loss: 2.7829
2022-07-10 08:06:07 - train: epoch 044, train_loss: 2.5416
2022-07-10 08:07:22 - eval: epoch: 044, acc1: 57.222%, acc5: 81.574%, test_loss: 1.8109, per_image_load_time: 2.289ms, per_image_inference_time: 0.469ms
2022-07-10 08:07:22 - until epoch: 044, best_acc1: 58.212%
2022-07-10 08:07:22 - epoch 045 lr: 0.090451
2022-07-10 08:08:02 - train: epoch 0045, iter [00100, 05004], lr: 0.090441, loss: 2.6396
2022-07-10 08:08:36 - train: epoch 0045, iter [00200, 05004], lr: 0.090432, loss: 2.4865
2022-07-10 08:09:10 - train: epoch 0045, iter [00300, 05004], lr: 0.090422, loss: 2.5402
2022-07-10 08:09:45 - train: epoch 0045, iter [00400, 05004], lr: 0.090413, loss: 2.4117
2022-07-10 08:10:20 - train: epoch 0045, iter [00500, 05004], lr: 0.090403, loss: 2.7300
2022-07-10 08:10:55 - train: epoch 0045, iter [00600, 05004], lr: 0.090394, loss: 2.4860
2022-07-10 08:11:30 - train: epoch 0045, iter [00700, 05004], lr: 0.090385, loss: 2.3454
2022-07-10 08:12:04 - train: epoch 0045, iter [00800, 05004], lr: 0.090375, loss: 2.4550
2022-07-10 08:12:39 - train: epoch 0045, iter [00900, 05004], lr: 0.090366, loss: 2.5582
2022-07-10 08:13:13 - train: epoch 0045, iter [01000, 05004], lr: 0.090356, loss: 2.3995
2022-07-10 08:13:49 - train: epoch 0045, iter [01100, 05004], lr: 0.090347, loss: 2.6571
2022-07-10 08:14:23 - train: epoch 0045, iter [01200, 05004], lr: 0.090337, loss: 2.6453
2022-07-10 08:14:59 - train: epoch 0045, iter [01300, 05004], lr: 0.090327, loss: 2.6477
2022-07-10 08:15:34 - train: epoch 0045, iter [01400, 05004], lr: 0.090318, loss: 2.6816
2022-07-10 08:16:08 - train: epoch 0045, iter [01500, 05004], lr: 0.090308, loss: 2.5190
2022-07-10 08:16:42 - train: epoch 0045, iter [01600, 05004], lr: 0.090299, loss: 2.3604
2022-07-10 08:17:18 - train: epoch 0045, iter [01700, 05004], lr: 0.090289, loss: 2.4985
2022-07-10 08:17:54 - train: epoch 0045, iter [01800, 05004], lr: 0.090280, loss: 2.2812
2022-07-10 08:18:28 - train: epoch 0045, iter [01900, 05004], lr: 0.090270, loss: 2.6837
2022-07-10 08:19:04 - train: epoch 0045, iter [02000, 05004], lr: 0.090261, loss: 2.5879
2022-07-10 08:19:39 - train: epoch 0045, iter [02100, 05004], lr: 0.090251, loss: 2.5749
2022-07-10 08:20:14 - train: epoch 0045, iter [02200, 05004], lr: 0.090242, loss: 2.5873
2022-07-10 08:20:48 - train: epoch 0045, iter [02300, 05004], lr: 0.090232, loss: 2.4724
2022-07-10 08:21:24 - train: epoch 0045, iter [02400, 05004], lr: 0.090223, loss: 2.6128
2022-07-10 08:21:59 - train: epoch 0045, iter [02500, 05004], lr: 0.090213, loss: 2.5223
2022-07-10 08:22:34 - train: epoch 0045, iter [02600, 05004], lr: 0.090203, loss: 2.5212
2022-07-10 08:23:09 - train: epoch 0045, iter [02700, 05004], lr: 0.090194, loss: 2.6387
2022-07-10 08:23:44 - train: epoch 0045, iter [02800, 05004], lr: 0.090184, loss: 2.5592
2022-07-10 08:24:19 - train: epoch 0045, iter [02900, 05004], lr: 0.090175, loss: 2.7921
2022-07-10 08:24:54 - train: epoch 0045, iter [03000, 05004], lr: 0.090165, loss: 2.5129
2022-07-10 08:25:29 - train: epoch 0045, iter [03100, 05004], lr: 0.090156, loss: 2.4331
2022-07-10 08:26:04 - train: epoch 0045, iter [03200, 05004], lr: 0.090146, loss: 2.5904
2022-07-10 08:26:39 - train: epoch 0045, iter [03300, 05004], lr: 0.090136, loss: 2.6012
2022-07-10 08:27:14 - train: epoch 0045, iter [03400, 05004], lr: 0.090127, loss: 2.3898
2022-07-10 08:27:49 - train: epoch 0045, iter [03500, 05004], lr: 0.090117, loss: 2.7394
2022-07-10 08:28:24 - train: epoch 0045, iter [03600, 05004], lr: 0.090108, loss: 2.6306
2022-07-10 08:28:59 - train: epoch 0045, iter [03700, 05004], lr: 0.090098, loss: 2.6912
2022-07-10 08:29:34 - train: epoch 0045, iter [03800, 05004], lr: 0.090088, loss: 2.5549
2022-07-10 08:30:10 - train: epoch 0045, iter [03900, 05004], lr: 0.090079, loss: 2.7189
2022-07-10 08:30:44 - train: epoch 0045, iter [04000, 05004], lr: 0.090069, loss: 2.5887
2022-07-10 08:31:19 - train: epoch 0045, iter [04100, 05004], lr: 0.090059, loss: 2.4595
2022-07-10 08:31:53 - train: epoch 0045, iter [04200, 05004], lr: 0.090050, loss: 2.8172
2022-07-10 08:32:29 - train: epoch 0045, iter [04300, 05004], lr: 0.090040, loss: 2.7896
2022-07-10 08:33:04 - train: epoch 0045, iter [04400, 05004], lr: 0.090030, loss: 2.8204
2022-07-10 08:33:40 - train: epoch 0045, iter [04500, 05004], lr: 0.090021, loss: 2.3469
2022-07-10 08:34:15 - train: epoch 0045, iter [04600, 05004], lr: 0.090011, loss: 2.6160
2022-07-10 08:34:50 - train: epoch 0045, iter [04700, 05004], lr: 0.090002, loss: 2.3796
2022-07-10 08:35:25 - train: epoch 0045, iter [04800, 05004], lr: 0.089992, loss: 2.5686
2022-07-10 08:36:00 - train: epoch 0045, iter [04900, 05004], lr: 0.089982, loss: 2.3566
2022-07-10 08:36:34 - train: epoch 0045, iter [05000, 05004], lr: 0.089973, loss: 2.3611
2022-07-10 08:36:35 - train: epoch 045, train_loss: 2.5367
2022-07-10 08:37:51 - eval: epoch: 045, acc1: 58.460%, acc5: 82.720%, test_loss: 1.7370, per_image_load_time: 2.442ms, per_image_inference_time: 0.487ms
2022-07-10 08:37:52 - until epoch: 045, best_acc1: 58.460%
2022-07-10 08:37:52 - epoch 046 lr: 0.089972
2022-07-10 08:38:31 - train: epoch 0046, iter [00100, 05004], lr: 0.089962, loss: 2.1100
2022-07-10 08:39:06 - train: epoch 0046, iter [00200, 05004], lr: 0.089953, loss: 2.3745
2022-07-10 08:39:41 - train: epoch 0046, iter [00300, 05004], lr: 0.089943, loss: 2.5651
2022-07-10 08:40:15 - train: epoch 0046, iter [00400, 05004], lr: 0.089933, loss: 2.6124
2022-07-10 08:40:50 - train: epoch 0046, iter [00500, 05004], lr: 0.089924, loss: 2.4144
2022-07-10 08:41:25 - train: epoch 0046, iter [00600, 05004], lr: 0.089914, loss: 2.5847
2022-07-10 08:41:59 - train: epoch 0046, iter [00700, 05004], lr: 0.089904, loss: 2.4106
2022-07-10 08:42:34 - train: epoch 0046, iter [00800, 05004], lr: 0.089895, loss: 2.8038
2022-07-10 08:43:09 - train: epoch 0046, iter [00900, 05004], lr: 0.089885, loss: 2.5754
2022-07-10 08:43:44 - train: epoch 0046, iter [01000, 05004], lr: 0.089875, loss: 2.3443
2022-07-10 08:44:19 - train: epoch 0046, iter [01100, 05004], lr: 0.089866, loss: 2.6117
2022-07-10 08:44:53 - train: epoch 0046, iter [01200, 05004], lr: 0.089856, loss: 2.2947
2022-07-10 08:45:28 - train: epoch 0046, iter [01300, 05004], lr: 0.089846, loss: 2.4935
2022-07-10 08:46:03 - train: epoch 0046, iter [01400, 05004], lr: 0.089836, loss: 2.9357
2022-07-10 08:46:38 - train: epoch 0046, iter [01500, 05004], lr: 0.089827, loss: 2.4401
2022-07-10 08:47:12 - train: epoch 0046, iter [01600, 05004], lr: 0.089817, loss: 2.5395
2022-07-10 08:47:47 - train: epoch 0046, iter [01700, 05004], lr: 0.089807, loss: 2.2167
2022-07-10 08:48:22 - train: epoch 0046, iter [01800, 05004], lr: 0.089797, loss: 2.6274
2022-07-10 08:48:58 - train: epoch 0046, iter [01900, 05004], lr: 0.089788, loss: 2.4517
2022-07-10 08:49:32 - train: epoch 0046, iter [02000, 05004], lr: 0.089778, loss: 2.6712
2022-07-10 08:50:07 - train: epoch 0046, iter [02100, 05004], lr: 0.089768, loss: 2.6951
2022-07-10 08:50:41 - train: epoch 0046, iter [02200, 05004], lr: 0.089758, loss: 2.5646
2022-07-10 08:51:17 - train: epoch 0046, iter [02300, 05004], lr: 0.089749, loss: 2.6489
2022-07-10 08:51:51 - train: epoch 0046, iter [02400, 05004], lr: 0.089739, loss: 2.4152
2022-07-10 08:52:25 - train: epoch 0046, iter [02500, 05004], lr: 0.089729, loss: 2.6119
2022-07-10 08:53:00 - train: epoch 0046, iter [02600, 05004], lr: 0.089719, loss: 2.5437
2022-07-10 08:53:36 - train: epoch 0046, iter [02700, 05004], lr: 0.089710, loss: 2.4820
2022-07-10 08:54:10 - train: epoch 0046, iter [02800, 05004], lr: 0.089700, loss: 2.5023
2022-07-10 08:54:45 - train: epoch 0046, iter [02900, 05004], lr: 0.089690, loss: 2.6728
2022-07-10 08:55:20 - train: epoch 0046, iter [03000, 05004], lr: 0.089680, loss: 2.4855
2022-07-10 08:55:54 - train: epoch 0046, iter [03100, 05004], lr: 0.089670, loss: 2.2961
2022-07-10 08:56:29 - train: epoch 0046, iter [03200, 05004], lr: 0.089661, loss: 2.6101
2022-07-10 08:57:03 - train: epoch 0046, iter [03300, 05004], lr: 0.089651, loss: 2.7012
2022-07-10 08:57:39 - train: epoch 0046, iter [03400, 05004], lr: 0.089641, loss: 2.4992
2022-07-10 08:58:14 - train: epoch 0046, iter [03500, 05004], lr: 0.089631, loss: 2.5325
2022-07-10 08:58:48 - train: epoch 0046, iter [03600, 05004], lr: 0.089621, loss: 2.4035
2022-07-10 08:59:23 - train: epoch 0046, iter [03700, 05004], lr: 0.089611, loss: 2.4114
2022-07-10 08:59:58 - train: epoch 0046, iter [03800, 05004], lr: 0.089602, loss: 2.6637
2022-07-10 09:00:33 - train: epoch 0046, iter [03900, 05004], lr: 0.089592, loss: 2.4791
2022-07-10 09:01:07 - train: epoch 0046, iter [04000, 05004], lr: 0.089582, loss: 2.3694
2022-07-10 09:01:42 - train: epoch 0046, iter [04100, 05004], lr: 0.089572, loss: 2.6543
2022-07-10 09:02:17 - train: epoch 0046, iter [04200, 05004], lr: 0.089562, loss: 2.6152
2022-07-10 09:02:52 - train: epoch 0046, iter [04300, 05004], lr: 0.089552, loss: 2.7848
2022-07-10 09:03:27 - train: epoch 0046, iter [04400, 05004], lr: 0.089543, loss: 2.5785
2022-07-10 09:04:02 - train: epoch 0046, iter [04500, 05004], lr: 0.089533, loss: 2.6198
2022-07-10 09:04:37 - train: epoch 0046, iter [04600, 05004], lr: 0.089523, loss: 2.5740
2022-07-10 09:05:12 - train: epoch 0046, iter [04700, 05004], lr: 0.089513, loss: 2.4204
2022-07-10 09:05:47 - train: epoch 0046, iter [04800, 05004], lr: 0.089503, loss: 2.6086
2022-07-10 09:06:22 - train: epoch 0046, iter [04900, 05004], lr: 0.089493, loss: 2.4991
2022-07-10 09:06:56 - train: epoch 0046, iter [05000, 05004], lr: 0.089483, loss: 2.7042
2022-07-10 09:06:57 - train: epoch 046, train_loss: 2.5305
2022-07-10 09:08:12 - eval: epoch: 046, acc1: 58.404%, acc5: 82.486%, test_loss: 1.7491, per_image_load_time: 2.430ms, per_image_inference_time: 0.499ms
2022-07-10 09:08:13 - until epoch: 046, best_acc1: 58.460%
2022-07-10 09:08:13 - epoch 047 lr: 0.089483
2022-07-10 09:08:52 - train: epoch 0047, iter [00100, 05004], lr: 0.089473, loss: 2.2483
2022-07-10 09:09:26 - train: epoch 0047, iter [00200, 05004], lr: 0.089463, loss: 2.6163
2022-07-10 09:10:02 - train: epoch 0047, iter [00300, 05004], lr: 0.089453, loss: 2.5471
2022-07-10 09:10:36 - train: epoch 0047, iter [00400, 05004], lr: 0.089444, loss: 2.1147
2022-07-10 09:11:11 - train: epoch 0047, iter [00500, 05004], lr: 0.089434, loss: 2.6129
2022-07-10 09:11:45 - train: epoch 0047, iter [00600, 05004], lr: 0.089424, loss: 2.6082
2022-07-10 09:12:20 - train: epoch 0047, iter [00700, 05004], lr: 0.089414, loss: 2.6704
2022-07-10 09:12:55 - train: epoch 0047, iter [00800, 05004], lr: 0.089404, loss: 2.3928
2022-07-10 09:13:30 - train: epoch 0047, iter [00900, 05004], lr: 0.089394, loss: 2.7158
2022-07-10 09:14:04 - train: epoch 0047, iter [01000, 05004], lr: 0.089384, loss: 2.5504
2022-07-10 09:14:39 - train: epoch 0047, iter [01100, 05004], lr: 0.089374, loss: 2.5857
2022-07-10 09:15:14 - train: epoch 0047, iter [01200, 05004], lr: 0.089364, loss: 2.6697
2022-07-10 09:15:47 - train: epoch 0047, iter [01300, 05004], lr: 0.089354, loss: 2.6846
2022-07-10 09:16:23 - train: epoch 0047, iter [01400, 05004], lr: 0.089344, loss: 2.6962
2022-07-10 09:16:58 - train: epoch 0047, iter [01500, 05004], lr: 0.089334, loss: 2.1872
2022-07-10 09:17:32 - train: epoch 0047, iter [01600, 05004], lr: 0.089325, loss: 2.4768
2022-07-10 09:18:07 - train: epoch 0047, iter [01700, 05004], lr: 0.089315, loss: 2.4462
2022-07-10 09:18:42 - train: epoch 0047, iter [01800, 05004], lr: 0.089305, loss: 2.3643
2022-07-10 09:19:17 - train: epoch 0047, iter [01900, 05004], lr: 0.089295, loss: 2.6937
2022-07-10 09:19:52 - train: epoch 0047, iter [02000, 05004], lr: 0.089285, loss: 2.6549
2022-07-10 09:20:26 - train: epoch 0047, iter [02100, 05004], lr: 0.089275, loss: 2.6488
2022-07-10 09:21:01 - train: epoch 0047, iter [02200, 05004], lr: 0.089265, loss: 2.8549
2022-07-10 09:21:36 - train: epoch 0047, iter [02300, 05004], lr: 0.089255, loss: 2.4468
2022-07-10 09:22:10 - train: epoch 0047, iter [02400, 05004], lr: 0.089245, loss: 2.1889
2022-07-10 09:22:46 - train: epoch 0047, iter [02500, 05004], lr: 0.089235, loss: 2.6413
2022-07-10 09:23:20 - train: epoch 0047, iter [02600, 05004], lr: 0.089225, loss: 2.9730
2022-07-10 09:23:55 - train: epoch 0047, iter [02700, 05004], lr: 0.089215, loss: 2.5659
2022-07-10 09:24:29 - train: epoch 0047, iter [02800, 05004], lr: 0.089205, loss: 2.7884
2022-07-10 09:25:05 - train: epoch 0047, iter [02900, 05004], lr: 0.089195, loss: 2.4568
2022-07-10 09:25:39 - train: epoch 0047, iter [03000, 05004], lr: 0.089185, loss: 2.5443
2022-07-10 09:26:14 - train: epoch 0047, iter [03100, 05004], lr: 0.089175, loss: 2.2741
2022-07-10 09:26:49 - train: epoch 0047, iter [03200, 05004], lr: 0.089165, loss: 2.9485
2022-07-10 09:27:24 - train: epoch 0047, iter [03300, 05004], lr: 0.089155, loss: 2.3840
2022-07-10 09:28:00 - train: epoch 0047, iter [03400, 05004], lr: 0.089145, loss: 2.4303
2022-07-10 09:28:34 - train: epoch 0047, iter [03500, 05004], lr: 0.089135, loss: 2.8512
2022-07-10 09:29:10 - train: epoch 0047, iter [03600, 05004], lr: 0.089125, loss: 2.7009
2022-07-10 09:29:44 - train: epoch 0047, iter [03700, 05004], lr: 0.089115, loss: 2.6448
2022-07-10 09:30:19 - train: epoch 0047, iter [03800, 05004], lr: 0.089105, loss: 2.4376
2022-07-10 09:30:53 - train: epoch 0047, iter [03900, 05004], lr: 0.089095, loss: 2.4334
2022-07-10 09:31:29 - train: epoch 0047, iter [04000, 05004], lr: 0.089085, loss: 2.5608
2022-07-10 09:32:03 - train: epoch 0047, iter [04100, 05004], lr: 0.089075, loss: 2.7088
2022-07-10 09:32:38 - train: epoch 0047, iter [04200, 05004], lr: 0.089065, loss: 2.4518
2022-07-10 09:33:12 - train: epoch 0047, iter [04300, 05004], lr: 0.089055, loss: 2.4084
2022-07-10 09:33:48 - train: epoch 0047, iter [04400, 05004], lr: 0.089045, loss: 2.6148
2022-07-10 09:34:23 - train: epoch 0047, iter [04500, 05004], lr: 0.089034, loss: 2.4183
2022-07-10 09:34:58 - train: epoch 0047, iter [04600, 05004], lr: 0.089024, loss: 2.5786
2022-07-10 09:35:33 - train: epoch 0047, iter [04700, 05004], lr: 0.089014, loss: 2.5245
2022-07-10 09:36:07 - train: epoch 0047, iter [04800, 05004], lr: 0.089004, loss: 2.4301
2022-07-10 09:36:43 - train: epoch 0047, iter [04900, 05004], lr: 0.088994, loss: 2.4355
2022-07-10 09:37:16 - train: epoch 0047, iter [05000, 05004], lr: 0.088984, loss: 2.5077
2022-07-10 09:37:17 - train: epoch 047, train_loss: 2.5249
2022-07-10 09:38:32 - eval: epoch: 047, acc1: 57.516%, acc5: 81.838%, test_loss: 1.7931, per_image_load_time: 2.374ms, per_image_inference_time: 0.496ms
2022-07-10 09:38:33 - until epoch: 047, best_acc1: 58.460%
2022-07-10 09:38:33 - epoch 048 lr: 0.088984
2022-07-10 09:39:13 - train: epoch 0048, iter [00100, 05004], lr: 0.088974, loss: 2.5948
2022-07-10 09:39:47 - train: epoch 0048, iter [00200, 05004], lr: 0.088964, loss: 2.7896
2022-07-10 09:40:23 - train: epoch 0048, iter [00300, 05004], lr: 0.088953, loss: 2.7210
2022-07-10 09:40:56 - train: epoch 0048, iter [00400, 05004], lr: 0.088943, loss: 2.5501
2022-07-10 09:41:30 - train: epoch 0048, iter [00500, 05004], lr: 0.088933, loss: 2.4568
2022-07-10 09:42:06 - train: epoch 0048, iter [00600, 05004], lr: 0.088923, loss: 2.4391
2022-07-10 09:42:41 - train: epoch 0048, iter [00700, 05004], lr: 0.088913, loss: 2.6180
2022-07-10 09:43:15 - train: epoch 0048, iter [00800, 05004], lr: 0.088903, loss: 2.5190
2022-07-10 09:43:50 - train: epoch 0048, iter [00900, 05004], lr: 0.088893, loss: 2.9002
2022-07-10 09:44:24 - train: epoch 0048, iter [01000, 05004], lr: 0.088883, loss: 2.3839
2022-07-10 09:45:00 - train: epoch 0048, iter [01100, 05004], lr: 0.088873, loss: 2.5475
2022-07-10 09:45:33 - train: epoch 0048, iter [01200, 05004], lr: 0.088862, loss: 2.5308
2022-07-10 09:46:08 - train: epoch 0048, iter [01300, 05004], lr: 0.088852, loss: 2.3976
2022-07-10 09:46:43 - train: epoch 0048, iter [01400, 05004], lr: 0.088842, loss: 2.6168
2022-07-10 09:47:17 - train: epoch 0048, iter [01500, 05004], lr: 0.088832, loss: 2.6431
2022-07-10 09:47:51 - train: epoch 0048, iter [01600, 05004], lr: 0.088822, loss: 2.4675
2022-07-10 09:48:26 - train: epoch 0048, iter [01700, 05004], lr: 0.088812, loss: 2.5189
2022-07-10 09:49:02 - train: epoch 0048, iter [01800, 05004], lr: 0.088802, loss: 2.5696
2022-07-10 09:49:37 - train: epoch 0048, iter [01900, 05004], lr: 0.088791, loss: 2.6289
2022-07-10 09:50:11 - train: epoch 0048, iter [02000, 05004], lr: 0.088781, loss: 2.7302
2022-07-10 09:50:46 - train: epoch 0048, iter [02100, 05004], lr: 0.088771, loss: 2.4534
2022-07-10 09:51:21 - train: epoch 0048, iter [02200, 05004], lr: 0.088761, loss: 2.7776
2022-07-10 09:51:56 - train: epoch 0048, iter [02300, 05004], lr: 0.088751, loss: 2.2132
2022-07-10 09:52:30 - train: epoch 0048, iter [02400, 05004], lr: 0.088741, loss: 2.5917
2022-07-10 09:53:06 - train: epoch 0048, iter [02500, 05004], lr: 0.088730, loss: 2.5620
2022-07-10 09:53:41 - train: epoch 0048, iter [02600, 05004], lr: 0.088720, loss: 2.7142
2022-07-10 09:54:16 - train: epoch 0048, iter [02700, 05004], lr: 0.088710, loss: 2.7452
2022-07-10 09:54:51 - train: epoch 0048, iter [02800, 05004], lr: 0.088700, loss: 2.5020
2022-07-10 09:55:26 - train: epoch 0048, iter [02900, 05004], lr: 0.088690, loss: 2.5616
2022-07-10 09:56:01 - train: epoch 0048, iter [03000, 05004], lr: 0.088679, loss: 2.5823
2022-07-10 09:56:36 - train: epoch 0048, iter [03100, 05004], lr: 0.088669, loss: 2.5932
2022-07-10 09:57:11 - train: epoch 0048, iter [03200, 05004], lr: 0.088659, loss: 2.5062
2022-07-10 09:57:46 - train: epoch 0048, iter [03300, 05004], lr: 0.088649, loss: 2.6597
2022-07-10 09:58:20 - train: epoch 0048, iter [03400, 05004], lr: 0.088639, loss: 2.4392
2022-07-10 09:58:56 - train: epoch 0048, iter [03500, 05004], lr: 0.088628, loss: 2.6687
2022-07-10 09:59:31 - train: epoch 0048, iter [03600, 05004], lr: 0.088618, loss: 2.4992
2022-07-10 10:00:07 - train: epoch 0048, iter [03700, 05004], lr: 0.088608, loss: 2.6276
2022-07-10 10:00:42 - train: epoch 0048, iter [03800, 05004], lr: 0.088598, loss: 2.7356
2022-07-10 10:01:17 - train: epoch 0048, iter [03900, 05004], lr: 0.088588, loss: 2.5538
2022-07-10 10:01:52 - train: epoch 0048, iter [04000, 05004], lr: 0.088577, loss: 2.2666
2022-07-10 10:02:27 - train: epoch 0048, iter [04100, 05004], lr: 0.088567, loss: 2.5627
2022-07-10 10:03:02 - train: epoch 0048, iter [04200, 05004], lr: 0.088557, loss: 2.6800
2022-07-10 10:03:37 - train: epoch 0048, iter [04300, 05004], lr: 0.088547, loss: 2.5335
2022-07-10 10:04:13 - train: epoch 0048, iter [04400, 05004], lr: 0.088536, loss: 2.4482
2022-07-10 10:04:47 - train: epoch 0048, iter [04500, 05004], lr: 0.088526, loss: 2.5517
2022-07-10 10:05:22 - train: epoch 0048, iter [04600, 05004], lr: 0.088516, loss: 2.4555
2022-07-10 10:05:58 - train: epoch 0048, iter [04700, 05004], lr: 0.088506, loss: 2.6393
2022-07-10 10:06:33 - train: epoch 0048, iter [04800, 05004], lr: 0.088495, loss: 2.7148
2022-07-10 10:07:07 - train: epoch 0048, iter [04900, 05004], lr: 0.088485, loss: 2.6780
2022-07-10 10:07:41 - train: epoch 0048, iter [05000, 05004], lr: 0.088475, loss: 2.5527
2022-07-10 10:07:42 - train: epoch 048, train_loss: 2.5249
