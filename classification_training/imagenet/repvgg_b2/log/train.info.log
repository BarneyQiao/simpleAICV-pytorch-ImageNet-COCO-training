2022-07-08 07:54:12 - train: epoch 0074, iter [03200, 05004], lr: 0.032523, loss: 1.3000
2022-07-08 07:54:57 - train: epoch 0074, iter [03300, 05004], lr: 0.032498, loss: 1.4022
2022-07-08 07:55:42 - train: epoch 0074, iter [03400, 05004], lr: 0.032474, loss: 1.4602
2022-07-08 07:56:27 - train: epoch 0074, iter [03500, 05004], lr: 0.032449, loss: 1.2207
2022-07-08 07:57:13 - train: epoch 0074, iter [03600, 05004], lr: 0.032425, loss: 1.4107
2022-07-08 07:57:58 - train: epoch 0074, iter [03700, 05004], lr: 0.032400, loss: 1.4488
2022-07-08 07:58:43 - train: epoch 0074, iter [03800, 05004], lr: 0.032376, loss: 1.3224
2022-07-08 07:59:28 - train: epoch 0074, iter [03900, 05004], lr: 0.032352, loss: 1.1399
2022-07-08 08:00:14 - train: epoch 0074, iter [04000, 05004], lr: 0.032327, loss: 1.4338
2022-07-08 08:00:59 - train: epoch 0074, iter [04100, 05004], lr: 0.032303, loss: 1.4880
2022-07-08 08:01:44 - train: epoch 0074, iter [04200, 05004], lr: 0.032278, loss: 1.4058
2022-07-08 08:02:30 - train: epoch 0074, iter [04300, 05004], lr: 0.032254, loss: 1.3662
2022-07-08 08:03:15 - train: epoch 0074, iter [04400, 05004], lr: 0.032229, loss: 1.1946
2022-07-08 08:04:00 - train: epoch 0074, iter [04500, 05004], lr: 0.032205, loss: 1.2083
2022-07-08 08:04:45 - train: epoch 0074, iter [04600, 05004], lr: 0.032180, loss: 1.4234
2022-07-08 08:05:31 - train: epoch 0074, iter [04700, 05004], lr: 0.032156, loss: 1.3872
2022-07-08 08:06:16 - train: epoch 0074, iter [04800, 05004], lr: 0.032131, loss: 1.3283
2022-07-08 08:07:01 - train: epoch 0074, iter [04900, 05004], lr: 0.032107, loss: 1.6632
2022-07-08 08:07:46 - train: epoch 0074, iter [05000, 05004], lr: 0.032083, loss: 1.3636
2022-07-08 08:07:49 - train: epoch 074, train_loss: 1.3423
2022-07-08 08:09:06 - eval: epoch: 074, acc1: 70.078%, acc5: 89.928%, test_loss: 1.2151, per_image_load_time: 1.714ms, per_image_inference_time: 0.839ms
2022-07-08 08:09:07 - until epoch: 074, best_acc1: 70.366%
2022-07-08 08:09:07 - epoch 075 lr: 0.032081
2022-07-08 08:09:58 - train: epoch 0075, iter [00100, 05004], lr: 0.032057, loss: 1.4153
2022-07-08 08:10:43 - train: epoch 0075, iter [00200, 05004], lr: 0.032033, loss: 1.3321
2022-07-08 08:11:28 - train: epoch 0075, iter [00300, 05004], lr: 0.032008, loss: 1.2305
2022-07-08 08:12:13 - train: epoch 0075, iter [00400, 05004], lr: 0.031984, loss: 1.4288
2022-07-08 08:12:58 - train: epoch 0075, iter [00500, 05004], lr: 0.031960, loss: 1.2261
2022-07-08 08:13:43 - train: epoch 0075, iter [00600, 05004], lr: 0.031935, loss: 1.1422
2022-07-08 08:14:27 - train: epoch 0075, iter [00700, 05004], lr: 0.031911, loss: 1.5204
2022-07-08 08:15:13 - train: epoch 0075, iter [00800, 05004], lr: 0.031886, loss: 1.4491
2022-07-08 08:15:58 - train: epoch 0075, iter [00900, 05004], lr: 0.031862, loss: 1.4172
2022-07-08 08:16:43 - train: epoch 0075, iter [01000, 05004], lr: 0.031838, loss: 1.1513
2022-07-08 08:17:28 - train: epoch 0075, iter [01100, 05004], lr: 0.031813, loss: 1.3883
2022-07-08 08:18:13 - train: epoch 0075, iter [01200, 05004], lr: 0.031789, loss: 1.1842
2022-07-08 08:18:58 - train: epoch 0075, iter [01300, 05004], lr: 0.031765, loss: 1.1951
2022-07-08 08:19:43 - train: epoch 0075, iter [01400, 05004], lr: 0.031740, loss: 1.3773
2022-07-08 08:20:28 - train: epoch 0075, iter [01500, 05004], lr: 0.031716, loss: 1.3641
2022-07-08 08:21:13 - train: epoch 0075, iter [01600, 05004], lr: 0.031691, loss: 0.9523
2022-07-08 08:21:58 - train: epoch 0075, iter [01700, 05004], lr: 0.031667, loss: 1.2495
2022-07-08 08:22:43 - train: epoch 0075, iter [01800, 05004], lr: 0.031643, loss: 1.2693
2022-07-08 08:23:28 - train: epoch 0075, iter [01900, 05004], lr: 0.031618, loss: 1.1720
2022-07-08 08:24:13 - train: epoch 0075, iter [02000, 05004], lr: 0.031594, loss: 1.3133
2022-07-08 08:24:58 - train: epoch 0075, iter [02100, 05004], lr: 0.031570, loss: 1.2911
2022-07-08 08:25:43 - train: epoch 0075, iter [02200, 05004], lr: 0.031546, loss: 1.3710
2022-07-08 08:26:28 - train: epoch 0075, iter [02300, 05004], lr: 0.031521, loss: 1.3200
2022-07-08 08:27:13 - train: epoch 0075, iter [02400, 05004], lr: 0.031497, loss: 1.3425
2022-07-08 08:27:58 - train: epoch 0075, iter [02500, 05004], lr: 0.031473, loss: 1.4694
2022-07-08 08:28:43 - train: epoch 0075, iter [02600, 05004], lr: 0.031448, loss: 1.3883
2022-07-08 08:29:28 - train: epoch 0075, iter [02700, 05004], lr: 0.031424, loss: 1.1378
2022-07-08 08:30:13 - train: epoch 0075, iter [02800, 05004], lr: 0.031400, loss: 1.2670
2022-07-08 08:30:58 - train: epoch 0075, iter [02900, 05004], lr: 0.031375, loss: 1.4849
2022-07-08 08:31:43 - train: epoch 0075, iter [03000, 05004], lr: 0.031351, loss: 1.5276
2022-07-08 08:32:28 - train: epoch 0075, iter [03100, 05004], lr: 0.031327, loss: 1.3584
2022-07-08 08:33:13 - train: epoch 0075, iter [03200, 05004], lr: 0.031303, loss: 1.3802
2022-07-08 08:33:58 - train: epoch 0075, iter [03300, 05004], lr: 0.031278, loss: 1.2779
2022-07-08 08:34:43 - train: epoch 0075, iter [03400, 05004], lr: 0.031254, loss: 1.1850
2022-07-08 08:35:28 - train: epoch 0075, iter [03500, 05004], lr: 0.031230, loss: 1.2829
2022-07-08 08:36:13 - train: epoch 0075, iter [03600, 05004], lr: 0.031206, loss: 1.3138
2022-07-08 08:36:58 - train: epoch 0075, iter [03700, 05004], lr: 0.031181, loss: 1.3417
2022-07-08 08:37:44 - train: epoch 0075, iter [03800, 05004], lr: 0.031157, loss: 1.3050
2022-07-08 08:38:29 - train: epoch 0075, iter [03900, 05004], lr: 0.031133, loss: 1.5874
2022-07-08 08:39:14 - train: epoch 0075, iter [04000, 05004], lr: 0.031109, loss: 1.2167
2022-07-08 08:39:59 - train: epoch 0075, iter [04100, 05004], lr: 0.031085, loss: 1.0707
2022-07-08 08:40:44 - train: epoch 0075, iter [04200, 05004], lr: 0.031060, loss: 1.4778
2022-07-08 08:41:29 - train: epoch 0075, iter [04300, 05004], lr: 0.031036, loss: 1.6267
2022-07-08 08:42:14 - train: epoch 0075, iter [04400, 05004], lr: 0.031012, loss: 1.1856
2022-07-08 08:42:59 - train: epoch 0075, iter [04500, 05004], lr: 0.030988, loss: 1.3713
2022-07-08 08:43:45 - train: epoch 0075, iter [04600, 05004], lr: 0.030964, loss: 1.0928
2022-07-08 08:44:30 - train: epoch 0075, iter [04700, 05004], lr: 0.030939, loss: 1.4309
2022-07-08 08:45:15 - train: epoch 0075, iter [04800, 05004], lr: 0.030915, loss: 1.3453
2022-07-08 08:46:00 - train: epoch 0075, iter [04900, 05004], lr: 0.030891, loss: 1.1731
2022-07-08 08:46:45 - train: epoch 0075, iter [05000, 05004], lr: 0.030867, loss: 1.3587
2022-07-08 08:46:47 - train: epoch 075, train_loss: 1.3309
2022-07-08 08:48:03 - eval: epoch: 075, acc1: 70.522%, acc5: 90.030%, test_loss: 1.1988, per_image_load_time: 1.543ms, per_image_inference_time: 0.829ms
2022-07-08 08:48:04 - until epoch: 075, best_acc1: 70.522%
2022-07-08 08:48:04 - epoch 076 lr: 0.030866
2022-07-08 08:48:55 - train: epoch 0076, iter [00100, 05004], lr: 0.030842, loss: 1.0773
2022-07-08 08:49:40 - train: epoch 0076, iter [00200, 05004], lr: 0.030818, loss: 1.2345
2022-07-08 08:50:25 - train: epoch 0076, iter [00300, 05004], lr: 0.030793, loss: 1.4088
2022-07-08 08:51:10 - train: epoch 0076, iter [00400, 05004], lr: 0.030769, loss: 1.2981
2022-07-08 08:51:55 - train: epoch 0076, iter [00500, 05004], lr: 0.030745, loss: 1.2678
2022-07-08 08:52:40 - train: epoch 0076, iter [00600, 05004], lr: 0.030721, loss: 1.2770
2022-07-08 08:53:25 - train: epoch 0076, iter [00700, 05004], lr: 0.030697, loss: 1.3944
2022-07-08 08:54:10 - train: epoch 0076, iter [00800, 05004], lr: 0.030673, loss: 1.2269
2022-07-08 08:54:54 - train: epoch 0076, iter [00900, 05004], lr: 0.030649, loss: 1.2510
2022-07-08 08:55:40 - train: epoch 0076, iter [01000, 05004], lr: 0.030624, loss: 1.2033
2022-07-08 08:56:24 - train: epoch 0076, iter [01100, 05004], lr: 0.030600, loss: 1.1817
2022-07-08 08:57:09 - train: epoch 0076, iter [01200, 05004], lr: 0.030576, loss: 1.2706
2022-07-08 08:57:54 - train: epoch 0076, iter [01300, 05004], lr: 0.030552, loss: 1.2840
2022-07-08 08:58:39 - train: epoch 0076, iter [01400, 05004], lr: 0.030528, loss: 1.1877
2022-07-08 08:59:24 - train: epoch 0076, iter [01500, 05004], lr: 0.030504, loss: 1.1508
2022-07-08 09:00:09 - train: epoch 0076, iter [01600, 05004], lr: 0.030480, loss: 1.3171
2022-07-08 09:00:53 - train: epoch 0076, iter [01700, 05004], lr: 0.030456, loss: 1.3356
2022-07-08 09:01:38 - train: epoch 0076, iter [01800, 05004], lr: 0.030432, loss: 1.1677
2022-07-08 09:02:23 - train: epoch 0076, iter [01900, 05004], lr: 0.030408, loss: 1.4804
2022-07-08 09:03:08 - train: epoch 0076, iter [02000, 05004], lr: 0.030384, loss: 1.3358
2022-07-08 09:03:53 - train: epoch 0076, iter [02100, 05004], lr: 0.030359, loss: 1.4792
2022-07-08 09:04:38 - train: epoch 0076, iter [02200, 05004], lr: 0.030335, loss: 1.4808
2022-07-08 09:05:22 - train: epoch 0076, iter [02300, 05004], lr: 0.030311, loss: 1.2435
2022-07-08 09:06:07 - train: epoch 0076, iter [02400, 05004], lr: 0.030287, loss: 1.5107
2022-07-08 09:06:52 - train: epoch 0076, iter [02500, 05004], lr: 0.030263, loss: 1.3838
2022-07-08 09:07:37 - train: epoch 0076, iter [02600, 05004], lr: 0.030239, loss: 1.6138
2022-07-08 09:08:22 - train: epoch 0076, iter [02700, 05004], lr: 0.030215, loss: 1.2351
2022-07-08 09:09:07 - train: epoch 0076, iter [02800, 05004], lr: 0.030191, loss: 1.2971
2022-07-08 09:09:52 - train: epoch 0076, iter [02900, 05004], lr: 0.030167, loss: 1.4103
2022-07-08 09:10:37 - train: epoch 0076, iter [03000, 05004], lr: 0.030143, loss: 1.2612
2022-07-08 09:11:21 - train: epoch 0076, iter [03100, 05004], lr: 0.030119, loss: 1.3949
2022-07-08 09:12:06 - train: epoch 0076, iter [03200, 05004], lr: 0.030095, loss: 1.4014
2022-07-08 09:12:51 - train: epoch 0076, iter [03300, 05004], lr: 0.030071, loss: 1.2324
2022-07-08 09:13:36 - train: epoch 0076, iter [03400, 05004], lr: 0.030047, loss: 1.2910
2022-07-08 09:14:21 - train: epoch 0076, iter [03500, 05004], lr: 0.030023, loss: 1.1459
2022-07-08 09:15:06 - train: epoch 0076, iter [03600, 05004], lr: 0.029999, loss: 1.1593
2022-07-08 09:15:50 - train: epoch 0076, iter [03700, 05004], lr: 0.029975, loss: 1.2305
2022-07-08 09:16:35 - train: epoch 0076, iter [03800, 05004], lr: 0.029951, loss: 1.1745
2022-07-08 09:17:20 - train: epoch 0076, iter [03900, 05004], lr: 0.029927, loss: 1.1571
2022-07-08 09:18:05 - train: epoch 0076, iter [04000, 05004], lr: 0.029903, loss: 1.3165
2022-07-08 09:18:50 - train: epoch 0076, iter [04100, 05004], lr: 0.029879, loss: 1.2993
2022-07-08 09:19:35 - train: epoch 0076, iter [04200, 05004], lr: 0.029855, loss: 1.5674
2022-07-08 09:20:20 - train: epoch 0076, iter [04300, 05004], lr: 0.029832, loss: 1.3237
2022-07-08 09:21:05 - train: epoch 0076, iter [04400, 05004], lr: 0.029808, loss: 1.3957
2022-07-08 09:21:49 - train: epoch 0076, iter [04500, 05004], lr: 0.029784, loss: 1.3045
2022-07-08 09:22:34 - train: epoch 0076, iter [04600, 05004], lr: 0.029760, loss: 1.4089
2022-07-08 09:23:19 - train: epoch 0076, iter [04700, 05004], lr: 0.029736, loss: 1.5120
2022-07-08 09:24:04 - train: epoch 0076, iter [04800, 05004], lr: 0.029712, loss: 1.1374
2022-07-08 09:24:49 - train: epoch 0076, iter [04900, 05004], lr: 0.029688, loss: 1.3104
2022-07-08 09:25:33 - train: epoch 0076, iter [05000, 05004], lr: 0.029664, loss: 1.3675
2022-07-08 09:25:36 - train: epoch 076, train_loss: 1.3139
2022-07-08 09:26:52 - eval: epoch: 076, acc1: 70.770%, acc5: 90.386%, test_loss: 1.1848, per_image_load_time: 2.120ms, per_image_inference_time: 0.838ms
2022-07-08 09:26:54 - until epoch: 076, best_acc1: 70.770%
2022-07-08 09:26:54 - epoch 077 lr: 0.029663
2022-07-08 09:27:44 - train: epoch 0077, iter [00100, 05004], lr: 0.029639, loss: 1.3806
2022-07-08 09:28:29 - train: epoch 0077, iter [00200, 05004], lr: 0.029615, loss: 1.3144
2022-07-08 09:29:14 - train: epoch 0077, iter [00300, 05004], lr: 0.029592, loss: 1.0024
2022-07-08 09:29:59 - train: epoch 0077, iter [00400, 05004], lr: 0.029568, loss: 1.4224
2022-07-08 09:30:44 - train: epoch 0077, iter [00500, 05004], lr: 0.029544, loss: 1.1046
2022-07-08 09:31:28 - train: epoch 0077, iter [00600, 05004], lr: 0.029520, loss: 1.0765
2022-07-08 09:32:13 - train: epoch 0077, iter [00700, 05004], lr: 0.029496, loss: 1.3649
2022-07-08 09:32:58 - train: epoch 0077, iter [00800, 05004], lr: 0.029472, loss: 1.3661
2022-07-08 09:33:43 - train: epoch 0077, iter [00900, 05004], lr: 0.029448, loss: 1.4523
2022-07-08 09:34:28 - train: epoch 0077, iter [01000, 05004], lr: 0.029424, loss: 1.1012
2022-07-08 09:35:13 - train: epoch 0077, iter [01100, 05004], lr: 0.029401, loss: 1.3780
2022-07-08 09:35:57 - train: epoch 0077, iter [01200, 05004], lr: 0.029377, loss: 1.3095
2022-07-08 09:36:42 - train: epoch 0077, iter [01300, 05004], lr: 0.029353, loss: 1.0069
2022-07-08 09:37:27 - train: epoch 0077, iter [01400, 05004], lr: 0.029329, loss: 1.4240
2022-07-08 09:38:12 - train: epoch 0077, iter [01500, 05004], lr: 0.029305, loss: 1.2285
2022-07-08 09:38:57 - train: epoch 0077, iter [01600, 05004], lr: 0.029282, loss: 1.3067
2022-07-08 09:39:42 - train: epoch 0077, iter [01700, 05004], lr: 0.029258, loss: 1.4830
2022-07-08 09:40:27 - train: epoch 0077, iter [01800, 05004], lr: 0.029234, loss: 1.2473
2022-07-08 09:41:12 - train: epoch 0077, iter [01900, 05004], lr: 0.029210, loss: 1.3188
2022-07-08 09:41:57 - train: epoch 0077, iter [02000, 05004], lr: 0.029186, loss: 1.1071
2022-07-08 09:42:42 - train: epoch 0077, iter [02100, 05004], lr: 0.029163, loss: 1.3076
2022-07-08 09:43:26 - train: epoch 0077, iter [02200, 05004], lr: 0.029139, loss: 1.3550
2022-07-08 09:44:11 - train: epoch 0077, iter [02300, 05004], lr: 0.029115, loss: 1.5531
2022-07-08 09:44:56 - train: epoch 0077, iter [02400, 05004], lr: 0.029091, loss: 1.2904
2022-07-08 09:45:41 - train: epoch 0077, iter [02500, 05004], lr: 0.029067, loss: 1.5048
2022-07-08 09:46:26 - train: epoch 0077, iter [02600, 05004], lr: 0.029044, loss: 1.0709
2022-07-08 09:47:11 - train: epoch 0077, iter [02700, 05004], lr: 0.029020, loss: 1.3332
2022-07-08 09:47:56 - train: epoch 0077, iter [02800, 05004], lr: 0.028996, loss: 1.5016
2022-07-08 09:48:40 - train: epoch 0077, iter [02900, 05004], lr: 0.028973, loss: 1.6448
2022-07-08 09:49:25 - train: epoch 0077, iter [03000, 05004], lr: 0.028949, loss: 1.1878
2022-07-08 09:50:10 - train: epoch 0077, iter [03100, 05004], lr: 0.028925, loss: 1.3840
2022-07-08 09:50:54 - train: epoch 0077, iter [03200, 05004], lr: 0.028901, loss: 1.4237
2022-07-08 09:51:39 - train: epoch 0077, iter [03300, 05004], lr: 0.028878, loss: 1.2227
2022-07-08 09:52:24 - train: epoch 0077, iter [03400, 05004], lr: 0.028854, loss: 1.5302
2022-07-08 09:53:09 - train: epoch 0077, iter [03500, 05004], lr: 0.028830, loss: 1.1488
2022-07-08 09:53:54 - train: epoch 0077, iter [03600, 05004], lr: 0.028807, loss: 1.2285
2022-07-08 09:54:38 - train: epoch 0077, iter [03700, 05004], lr: 0.028783, loss: 1.3882
2022-07-08 09:55:23 - train: epoch 0077, iter [03800, 05004], lr: 0.028759, loss: 1.3483
2022-07-08 09:56:08 - train: epoch 0077, iter [03900, 05004], lr: 0.028735, loss: 1.4550
2022-07-08 09:56:53 - train: epoch 0077, iter [04000, 05004], lr: 0.028712, loss: 1.2113
2022-07-08 09:57:38 - train: epoch 0077, iter [04100, 05004], lr: 0.028688, loss: 1.5515
2022-07-08 09:58:23 - train: epoch 0077, iter [04200, 05004], lr: 0.028664, loss: 1.5127
2022-07-08 09:59:08 - train: epoch 0077, iter [04300, 05004], lr: 0.028641, loss: 1.2232
2022-07-08 09:59:52 - train: epoch 0077, iter [04400, 05004], lr: 0.028617, loss: 1.3036
2022-07-08 10:00:37 - train: epoch 0077, iter [04500, 05004], lr: 0.028594, loss: 1.3973
2022-07-08 10:01:22 - train: epoch 0077, iter [04600, 05004], lr: 0.028570, loss: 1.2408
2022-07-08 10:02:07 - train: epoch 0077, iter [04700, 05004], lr: 0.028546, loss: 1.1230
2022-07-08 10:02:52 - train: epoch 0077, iter [04800, 05004], lr: 0.028523, loss: 1.2820
2022-07-08 10:03:37 - train: epoch 0077, iter [04900, 05004], lr: 0.028499, loss: 1.4880
2022-07-08 10:04:21 - train: epoch 0077, iter [05000, 05004], lr: 0.028475, loss: 1.5421
2022-07-08 10:04:24 - train: epoch 077, train_loss: 1.2985
2022-07-08 10:05:40 - eval: epoch: 077, acc1: 71.110%, acc5: 90.526%, test_loss: 1.1639, per_image_load_time: 2.111ms, per_image_inference_time: 0.837ms
2022-07-08 10:05:42 - until epoch: 077, best_acc1: 71.110%
2022-07-08 10:05:42 - epoch 078 lr: 0.028474
2022-07-08 10:06:32 - train: epoch 0078, iter [00100, 05004], lr: 0.028451, loss: 1.1567
2022-07-08 10:07:17 - train: epoch 0078, iter [00200, 05004], lr: 0.028427, loss: 1.5024
2022-07-08 10:08:02 - train: epoch 0078, iter [00300, 05004], lr: 0.028404, loss: 1.2976
2022-07-08 10:08:47 - train: epoch 0078, iter [00400, 05004], lr: 0.028380, loss: 1.3321
2022-07-08 10:09:32 - train: epoch 0078, iter [00500, 05004], lr: 0.028356, loss: 1.1383
2022-07-08 10:10:17 - train: epoch 0078, iter [00600, 05004], lr: 0.028333, loss: 1.2382
2022-07-08 10:11:02 - train: epoch 0078, iter [00700, 05004], lr: 0.028309, loss: 1.3589
2022-07-08 10:11:46 - train: epoch 0078, iter [00800, 05004], lr: 0.028286, loss: 1.4445
2022-07-08 10:12:31 - train: epoch 0078, iter [00900, 05004], lr: 0.028262, loss: 1.3494
2022-07-08 10:13:16 - train: epoch 0078, iter [01000, 05004], lr: 0.028239, loss: 1.3459
2022-07-08 10:14:01 - train: epoch 0078, iter [01100, 05004], lr: 0.028215, loss: 1.0928
2022-07-08 10:14:45 - train: epoch 0078, iter [01200, 05004], lr: 0.028192, loss: 1.0803
2022-07-08 10:15:30 - train: epoch 0078, iter [01300, 05004], lr: 0.028168, loss: 1.2668
2022-07-08 10:16:15 - train: epoch 0078, iter [01400, 05004], lr: 0.028144, loss: 1.1019
2022-07-08 10:17:00 - train: epoch 0078, iter [01500, 05004], lr: 0.028121, loss: 1.1997
2022-07-08 10:17:45 - train: epoch 0078, iter [01600, 05004], lr: 0.028097, loss: 1.3372
2022-07-08 10:18:30 - train: epoch 0078, iter [01700, 05004], lr: 0.028074, loss: 1.2123
2022-07-08 10:19:14 - train: epoch 0078, iter [01800, 05004], lr: 0.028050, loss: 1.2330
2022-07-08 10:19:59 - train: epoch 0078, iter [01900, 05004], lr: 0.028027, loss: 1.0891
2022-07-08 10:20:44 - train: epoch 0078, iter [02000, 05004], lr: 0.028003, loss: 1.1851
2022-07-08 10:21:29 - train: epoch 0078, iter [02100, 05004], lr: 0.027980, loss: 1.5037
2022-07-08 10:22:14 - train: epoch 0078, iter [02200, 05004], lr: 0.027956, loss: 1.2384
2022-07-08 10:22:59 - train: epoch 0078, iter [02300, 05004], lr: 0.027933, loss: 1.4709
2022-07-08 10:23:43 - train: epoch 0078, iter [02400, 05004], lr: 0.027909, loss: 1.2440
2022-07-08 10:24:28 - train: epoch 0078, iter [02500, 05004], lr: 0.027886, loss: 1.3475
2022-07-08 10:25:13 - train: epoch 0078, iter [02600, 05004], lr: 0.027863, loss: 1.2790
2022-07-08 10:25:59 - train: epoch 0078, iter [02700, 05004], lr: 0.027839, loss: 1.3121
2022-07-08 10:26:44 - train: epoch 0078, iter [02800, 05004], lr: 0.027816, loss: 1.2980
2022-07-08 10:27:30 - train: epoch 0078, iter [02900, 05004], lr: 0.027792, loss: 1.1428
2022-07-08 10:28:15 - train: epoch 0078, iter [03000, 05004], lr: 0.027769, loss: 1.2624
2022-07-08 10:29:01 - train: epoch 0078, iter [03100, 05004], lr: 0.027745, loss: 1.3387
2022-07-08 10:29:47 - train: epoch 0078, iter [03200, 05004], lr: 0.027722, loss: 1.3443
2022-07-08 10:30:33 - train: epoch 0078, iter [03300, 05004], lr: 0.027699, loss: 1.4345
2022-07-08 10:31:18 - train: epoch 0078, iter [03400, 05004], lr: 0.027675, loss: 1.2745
2022-07-08 10:32:04 - train: epoch 0078, iter [03500, 05004], lr: 0.027652, loss: 1.1888
2022-07-08 10:32:50 - train: epoch 0078, iter [03600, 05004], lr: 0.027628, loss: 1.3654
2022-07-08 10:33:36 - train: epoch 0078, iter [03700, 05004], lr: 0.027605, loss: 1.2534
2022-07-08 10:34:22 - train: epoch 0078, iter [03800, 05004], lr: 0.027582, loss: 1.3609
2022-07-08 10:35:08 - train: epoch 0078, iter [03900, 05004], lr: 0.027558, loss: 1.3757
2022-07-08 10:35:53 - train: epoch 0078, iter [04000, 05004], lr: 0.027535, loss: 1.3192
2022-07-08 10:36:40 - train: epoch 0078, iter [04100, 05004], lr: 0.027511, loss: 1.3912
2022-07-08 10:37:26 - train: epoch 0078, iter [04200, 05004], lr: 0.027488, loss: 1.4819
2022-07-08 10:38:11 - train: epoch 0078, iter [04300, 05004], lr: 0.027465, loss: 1.3379
2022-07-08 10:38:57 - train: epoch 0078, iter [04400, 05004], lr: 0.027441, loss: 1.2626
2022-07-08 10:39:44 - train: epoch 0078, iter [04500, 05004], lr: 0.027418, loss: 1.2956
2022-07-08 10:40:30 - train: epoch 0078, iter [04600, 05004], lr: 0.027395, loss: 1.0240
2022-07-08 10:41:16 - train: epoch 0078, iter [04700, 05004], lr: 0.027371, loss: 1.3859
2022-07-08 10:42:02 - train: epoch 0078, iter [04800, 05004], lr: 0.027348, loss: 1.5710
2022-07-08 10:42:48 - train: epoch 0078, iter [04900, 05004], lr: 0.027325, loss: 1.3941
2022-07-08 10:43:34 - train: epoch 0078, iter [05000, 05004], lr: 0.027301, loss: 1.2277
2022-07-08 10:43:36 - train: epoch 078, train_loss: 1.2812
2022-07-08 10:44:51 - eval: epoch: 078, acc1: 71.024%, acc5: 90.444%, test_loss: 1.1658, per_image_load_time: 2.059ms, per_image_inference_time: 0.835ms
2022-07-08 10:44:52 - until epoch: 078, best_acc1: 71.110%
2022-07-08 10:44:52 - epoch 079 lr: 0.027300
2022-07-08 10:45:43 - train: epoch 0079, iter [00100, 05004], lr: 0.027277, loss: 1.2512
2022-07-08 10:46:30 - train: epoch 0079, iter [00200, 05004], lr: 0.027254, loss: 1.1376
2022-07-08 10:47:15 - train: epoch 0079, iter [00300, 05004], lr: 0.027231, loss: 1.2414
2022-07-08 10:48:01 - train: epoch 0079, iter [00400, 05004], lr: 0.027207, loss: 1.4006
2022-07-08 10:48:46 - train: epoch 0079, iter [00500, 05004], lr: 0.027184, loss: 1.2994
2022-07-08 10:49:32 - train: epoch 0079, iter [00600, 05004], lr: 0.027161, loss: 1.1463
2022-07-08 10:50:18 - train: epoch 0079, iter [00700, 05004], lr: 0.027137, loss: 1.1642
2022-07-08 10:51:03 - train: epoch 0079, iter [00800, 05004], lr: 0.027114, loss: 1.3961
2022-07-08 10:51:49 - train: epoch 0079, iter [00900, 05004], lr: 0.027091, loss: 1.1238
2022-07-08 10:52:34 - train: epoch 0079, iter [01000, 05004], lr: 0.027068, loss: 1.1238
2022-07-08 10:53:20 - train: epoch 0079, iter [01100, 05004], lr: 0.027044, loss: 1.3501
2022-07-08 10:54:05 - train: epoch 0079, iter [01200, 05004], lr: 0.027021, loss: 1.5106
2022-07-08 10:54:51 - train: epoch 0079, iter [01300, 05004], lr: 0.026998, loss: 1.0883
2022-07-08 10:55:36 - train: epoch 0079, iter [01400, 05004], lr: 0.026975, loss: 1.2676
2022-07-08 10:56:22 - train: epoch 0079, iter [01500, 05004], lr: 0.026952, loss: 1.0515
2022-07-08 10:57:07 - train: epoch 0079, iter [01600, 05004], lr: 0.026928, loss: 1.0572
2022-07-08 10:57:53 - train: epoch 0079, iter [01700, 05004], lr: 0.026905, loss: 1.2268
2022-07-08 10:58:39 - train: epoch 0079, iter [01800, 05004], lr: 0.026882, loss: 1.1408
2022-07-08 10:59:24 - train: epoch 0079, iter [01900, 05004], lr: 0.026859, loss: 1.3458
2022-07-08 11:00:10 - train: epoch 0079, iter [02000, 05004], lr: 0.026836, loss: 1.5216
2022-07-08 11:00:55 - train: epoch 0079, iter [02100, 05004], lr: 0.026812, loss: 1.0467
2022-07-08 11:01:41 - train: epoch 0079, iter [02200, 05004], lr: 0.026789, loss: 1.3277
2022-07-08 11:02:27 - train: epoch 0079, iter [02300, 05004], lr: 0.026766, loss: 1.1658
2022-07-08 11:03:12 - train: epoch 0079, iter [02400, 05004], lr: 0.026743, loss: 1.2867
2022-07-08 11:03:58 - train: epoch 0079, iter [02500, 05004], lr: 0.026720, loss: 1.1942
2022-07-08 11:04:43 - train: epoch 0079, iter [02600, 05004], lr: 0.026697, loss: 1.1807
2022-07-08 11:05:29 - train: epoch 0079, iter [02700, 05004], lr: 0.026673, loss: 1.0727
2022-07-08 11:06:15 - train: epoch 0079, iter [02800, 05004], lr: 0.026650, loss: 1.1713
2022-07-08 11:07:00 - train: epoch 0079, iter [02900, 05004], lr: 0.026627, loss: 1.1205
2022-07-08 11:07:46 - train: epoch 0079, iter [03000, 05004], lr: 0.026604, loss: 1.3014
2022-07-08 11:08:31 - train: epoch 0079, iter [03100, 05004], lr: 0.026581, loss: 1.2347
2022-07-08 11:09:17 - train: epoch 0079, iter [03200, 05004], lr: 0.026558, loss: 1.5186
2022-07-08 11:10:03 - train: epoch 0079, iter [03300, 05004], lr: 0.026535, loss: 1.1810
2022-07-08 11:10:48 - train: epoch 0079, iter [03400, 05004], lr: 0.026512, loss: 1.1578
2022-07-08 11:11:34 - train: epoch 0079, iter [03500, 05004], lr: 0.026489, loss: 1.4693
2022-07-08 11:12:19 - train: epoch 0079, iter [03600, 05004], lr: 0.026465, loss: 1.2563
2022-07-08 11:13:05 - train: epoch 0079, iter [03700, 05004], lr: 0.026442, loss: 1.0731
2022-07-08 11:13:50 - train: epoch 0079, iter [03800, 05004], lr: 0.026419, loss: 1.3383
2022-07-08 11:14:36 - train: epoch 0079, iter [03900, 05004], lr: 0.026396, loss: 1.0868
2022-07-08 11:15:22 - train: epoch 0079, iter [04000, 05004], lr: 0.026373, loss: 1.1651
2022-07-08 11:16:07 - train: epoch 0079, iter [04100, 05004], lr: 0.026350, loss: 1.5697
2022-07-08 11:16:53 - train: epoch 0079, iter [04200, 05004], lr: 0.026327, loss: 1.1985
2022-07-08 11:17:38 - train: epoch 0079, iter [04300, 05004], lr: 0.026304, loss: 0.9799
2022-07-08 11:18:24 - train: epoch 0079, iter [04400, 05004], lr: 0.026281, loss: 1.4891
2022-07-08 11:19:10 - train: epoch 0079, iter [04500, 05004], lr: 0.026258, loss: 1.5545
2022-07-08 11:19:55 - train: epoch 0079, iter [04600, 05004], lr: 0.026235, loss: 1.4794
2022-07-08 11:20:41 - train: epoch 0079, iter [04700, 05004], lr: 0.026212, loss: 1.2496
2022-07-08 11:21:27 - train: epoch 0079, iter [04800, 05004], lr: 0.026189, loss: 1.4294
2022-07-08 11:22:12 - train: epoch 0079, iter [04900, 05004], lr: 0.026166, loss: 1.3577
2022-07-08 11:22:58 - train: epoch 0079, iter [05000, 05004], lr: 0.026143, loss: 1.1127
2022-07-08 11:23:00 - train: epoch 079, train_loss: 1.2677
2022-07-08 11:24:16 - eval: epoch: 079, acc1: 70.982%, acc5: 90.644%, test_loss: 1.1635, per_image_load_time: 2.101ms, per_image_inference_time: 0.835ms
2022-07-08 11:24:17 - until epoch: 079, best_acc1: 71.110%
2022-07-08 11:24:17 - epoch 080 lr: 0.026142
2022-07-08 11:25:08 - train: epoch 0080, iter [00100, 05004], lr: 0.026119, loss: 0.9902
2022-07-08 11:25:54 - train: epoch 0080, iter [00200, 05004], lr: 0.026096, loss: 1.1595
2022-07-08 11:26:39 - train: epoch 0080, iter [00300, 05004], lr: 0.026073, loss: 1.2970
2022-07-08 11:27:25 - train: epoch 0080, iter [00400, 05004], lr: 0.026050, loss: 1.0834
2022-07-08 11:28:10 - train: epoch 0080, iter [00500, 05004], lr: 0.026027, loss: 1.2892
2022-07-08 11:28:56 - train: epoch 0080, iter [00600, 05004], lr: 0.026004, loss: 1.1233
2022-07-08 11:29:41 - train: epoch 0080, iter [00700, 05004], lr: 0.025981, loss: 1.2351
2022-07-08 11:30:27 - train: epoch 0080, iter [00800, 05004], lr: 0.025958, loss: 1.1118
2022-07-08 11:31:12 - train: epoch 0080, iter [00900, 05004], lr: 0.025935, loss: 1.2381
2022-07-08 11:31:57 - train: epoch 0080, iter [01000, 05004], lr: 0.025912, loss: 1.1512
2022-07-08 11:32:42 - train: epoch 0080, iter [01100, 05004], lr: 0.025890, loss: 1.2844
2022-07-08 11:33:28 - train: epoch 0080, iter [01200, 05004], lr: 0.025867, loss: 1.3343
2022-07-08 11:34:13 - train: epoch 0080, iter [01300, 05004], lr: 0.025844, loss: 1.2085
2022-07-08 11:34:58 - train: epoch 0080, iter [01400, 05004], lr: 0.025821, loss: 1.2152
2022-07-08 11:35:43 - train: epoch 0080, iter [01500, 05004], lr: 0.025798, loss: 1.1379
2022-07-08 11:36:28 - train: epoch 0080, iter [01600, 05004], lr: 0.025775, loss: 1.2241
2022-07-08 11:37:14 - train: epoch 0080, iter [01700, 05004], lr: 0.025752, loss: 1.2014
2022-07-08 11:37:59 - train: epoch 0080, iter [01800, 05004], lr: 0.025729, loss: 1.3781
2022-07-08 11:38:44 - train: epoch 0080, iter [01900, 05004], lr: 0.025706, loss: 1.1992
2022-07-08 11:39:30 - train: epoch 0080, iter [02000, 05004], lr: 0.025684, loss: 1.2713
2022-07-08 11:40:15 - train: epoch 0080, iter [02100, 05004], lr: 0.025661, loss: 1.3629
2022-07-08 11:41:01 - train: epoch 0080, iter [02200, 05004], lr: 0.025638, loss: 1.3238
2022-07-08 11:41:46 - train: epoch 0080, iter [02300, 05004], lr: 0.025615, loss: 1.1620
2022-07-08 11:42:32 - train: epoch 0080, iter [02400, 05004], lr: 0.025592, loss: 1.2311
2022-07-08 11:43:17 - train: epoch 0080, iter [02500, 05004], lr: 0.025569, loss: 1.1357
2022-07-08 11:44:03 - train: epoch 0080, iter [02600, 05004], lr: 0.025547, loss: 1.2341
2022-07-08 11:44:48 - train: epoch 0080, iter [02700, 05004], lr: 0.025524, loss: 1.4390
2022-07-08 11:45:34 - train: epoch 0080, iter [02800, 05004], lr: 0.025501, loss: 1.3623
2022-07-08 11:46:19 - train: epoch 0080, iter [02900, 05004], lr: 0.025478, loss: 1.1209
2022-07-08 11:47:05 - train: epoch 0080, iter [03000, 05004], lr: 0.025455, loss: 1.2573
2022-07-08 11:47:50 - train: epoch 0080, iter [03100, 05004], lr: 0.025433, loss: 1.4498
2022-07-08 11:48:36 - train: epoch 0080, iter [03200, 05004], lr: 0.025410, loss: 0.9558
2022-07-08 11:49:21 - train: epoch 0080, iter [03300, 05004], lr: 0.025387, loss: 1.2969
2022-07-08 11:50:07 - train: epoch 0080, iter [03400, 05004], lr: 0.025364, loss: 1.1236
2022-07-08 11:50:52 - train: epoch 0080, iter [03500, 05004], lr: 0.025341, loss: 1.0634
2022-07-08 11:51:38 - train: epoch 0080, iter [03600, 05004], lr: 0.025319, loss: 1.2362
2022-07-08 11:52:23 - train: epoch 0080, iter [03700, 05004], lr: 0.025296, loss: 1.1800
2022-07-08 11:53:09 - train: epoch 0080, iter [03800, 05004], lr: 0.025273, loss: 1.3351
2022-07-08 11:53:54 - train: epoch 0080, iter [03900, 05004], lr: 0.025251, loss: 1.2249
2022-07-08 11:54:40 - train: epoch 0080, iter [04000, 05004], lr: 0.025228, loss: 1.4229
2022-07-08 11:55:25 - train: epoch 0080, iter [04100, 05004], lr: 0.025205, loss: 1.2845
2022-07-08 11:56:11 - train: epoch 0080, iter [04200, 05004], lr: 0.025182, loss: 1.2296
2022-07-08 11:56:56 - train: epoch 0080, iter [04300, 05004], lr: 0.025160, loss: 1.3752
2022-07-08 11:57:42 - train: epoch 0080, iter [04400, 05004], lr: 0.025137, loss: 1.2488
2022-07-08 11:58:27 - train: epoch 0080, iter [04500, 05004], lr: 0.025114, loss: 1.2815
2022-07-08 11:59:13 - train: epoch 0080, iter [04600, 05004], lr: 0.025092, loss: 1.3868
2022-07-08 11:59:58 - train: epoch 0080, iter [04700, 05004], lr: 0.025069, loss: 1.3203
2022-07-08 12:00:44 - train: epoch 0080, iter [04800, 05004], lr: 0.025046, loss: 1.3025
2022-07-08 12:01:29 - train: epoch 0080, iter [04900, 05004], lr: 0.025024, loss: 1.4885
2022-07-08 12:02:15 - train: epoch 0080, iter [05000, 05004], lr: 0.025001, loss: 1.0523
2022-07-08 12:02:17 - train: epoch 080, train_loss: 1.2548
2022-07-08 12:03:32 - eval: epoch: 080, acc1: 71.392%, acc5: 90.806%, test_loss: 1.1467, per_image_load_time: 1.801ms, per_image_inference_time: 0.827ms
2022-07-08 12:03:34 - until epoch: 080, best_acc1: 71.392%
2022-07-08 12:03:34 - epoch 081 lr: 0.025000
2022-07-08 12:04:25 - train: epoch 0081, iter [00100, 05004], lr: 0.024977, loss: 1.0748
2022-07-08 12:05:10 - train: epoch 0081, iter [00200, 05004], lr: 0.024955, loss: 1.3402
2022-07-08 12:05:56 - train: epoch 0081, iter [00300, 05004], lr: 0.024932, loss: 1.1624
2022-07-08 12:06:42 - train: epoch 0081, iter [00400, 05004], lr: 0.024909, loss: 1.3836
2022-07-08 12:07:28 - train: epoch 0081, iter [00500, 05004], lr: 0.024887, loss: 1.2468
2022-07-08 12:08:14 - train: epoch 0081, iter [00600, 05004], lr: 0.024864, loss: 1.2044
2022-07-08 12:09:00 - train: epoch 0081, iter [00700, 05004], lr: 0.024842, loss: 1.1564
2022-07-08 12:09:45 - train: epoch 0081, iter [00800, 05004], lr: 0.024819, loss: 1.3202
2022-07-08 12:10:31 - train: epoch 0081, iter [00900, 05004], lr: 0.024796, loss: 1.1201
2022-07-08 12:11:17 - train: epoch 0081, iter [01000, 05004], lr: 0.024774, loss: 1.1901
2022-07-08 12:12:03 - train: epoch 0081, iter [01100, 05004], lr: 0.024751, loss: 1.1719
2022-07-08 12:12:48 - train: epoch 0081, iter [01200, 05004], lr: 0.024729, loss: 1.2355
2022-07-08 12:13:34 - train: epoch 0081, iter [01300, 05004], lr: 0.024706, loss: 1.1125
2022-07-08 12:14:20 - train: epoch 0081, iter [01400, 05004], lr: 0.024684, loss: 0.9005
2022-07-08 12:15:06 - train: epoch 0081, iter [01500, 05004], lr: 0.024661, loss: 1.3085
2022-07-08 12:15:52 - train: epoch 0081, iter [01600, 05004], lr: 0.024638, loss: 1.1289
2022-07-08 12:16:37 - train: epoch 0081, iter [01700, 05004], lr: 0.024616, loss: 1.3458
2022-07-08 12:17:23 - train: epoch 0081, iter [01800, 05004], lr: 0.024593, loss: 1.1570
2022-07-08 12:18:09 - train: epoch 0081, iter [01900, 05004], lr: 0.024571, loss: 1.1678
2022-07-08 12:18:55 - train: epoch 0081, iter [02000, 05004], lr: 0.024548, loss: 1.3142
2022-07-08 12:19:40 - train: epoch 0081, iter [02100, 05004], lr: 0.024526, loss: 1.1854
2022-07-08 12:20:26 - train: epoch 0081, iter [02200, 05004], lr: 0.024503, loss: 1.1038
2022-07-08 12:21:12 - train: epoch 0081, iter [02300, 05004], lr: 0.024481, loss: 1.1336
2022-07-08 12:21:58 - train: epoch 0081, iter [02400, 05004], lr: 0.024458, loss: 1.2764
2022-07-08 12:22:43 - train: epoch 0081, iter [02500, 05004], lr: 0.024436, loss: 1.3474
2022-07-08 12:23:29 - train: epoch 0081, iter [02600, 05004], lr: 0.024413, loss: 1.4508
2022-07-08 12:24:15 - train: epoch 0081, iter [02700, 05004], lr: 0.024391, loss: 1.1925
2022-07-08 12:25:00 - train: epoch 0081, iter [02800, 05004], lr: 0.024368, loss: 1.1647
2022-07-08 12:25:45 - train: epoch 0081, iter [02900, 05004], lr: 0.024346, loss: 1.0896
2022-07-08 12:26:30 - train: epoch 0081, iter [03000, 05004], lr: 0.024323, loss: 1.3176
2022-07-08 12:27:15 - train: epoch 0081, iter [03100, 05004], lr: 0.024301, loss: 1.0968
2022-07-08 12:28:00 - train: epoch 0081, iter [03200, 05004], lr: 0.024279, loss: 1.1942
2022-07-08 12:28:45 - train: epoch 0081, iter [03300, 05004], lr: 0.024256, loss: 1.2657
2022-07-08 12:29:30 - train: epoch 0081, iter [03400, 05004], lr: 0.024234, loss: 1.3390
2022-07-08 12:30:15 - train: epoch 0081, iter [03500, 05004], lr: 0.024211, loss: 1.2658
2022-07-08 12:31:00 - train: epoch 0081, iter [03600, 05004], lr: 0.024189, loss: 1.1322
2022-07-08 12:31:45 - train: epoch 0081, iter [03700, 05004], lr: 0.024167, loss: 1.4293
2022-07-08 12:32:30 - train: epoch 0081, iter [03800, 05004], lr: 0.024144, loss: 1.0894
2022-07-08 12:33:15 - train: epoch 0081, iter [03900, 05004], lr: 0.024122, loss: 1.3089
2022-07-08 12:34:00 - train: epoch 0081, iter [04000, 05004], lr: 0.024099, loss: 1.0768
2022-07-08 12:34:45 - train: epoch 0081, iter [04100, 05004], lr: 0.024077, loss: 1.3885
2022-07-08 12:35:30 - train: epoch 0081, iter [04200, 05004], lr: 0.024055, loss: 1.2708
2022-07-08 12:36:15 - train: epoch 0081, iter [04300, 05004], lr: 0.024032, loss: 1.2625
2022-07-08 12:37:00 - train: epoch 0081, iter [04400, 05004], lr: 0.024010, loss: 1.3915
2022-07-08 12:37:45 - train: epoch 0081, iter [04500, 05004], lr: 0.023988, loss: 1.2679
2022-07-08 12:38:30 - train: epoch 0081, iter [04600, 05004], lr: 0.023965, loss: 1.0940
2022-07-08 12:39:15 - train: epoch 0081, iter [04700, 05004], lr: 0.023943, loss: 1.3703
2022-07-08 12:40:00 - train: epoch 0081, iter [04800, 05004], lr: 0.023921, loss: 1.2968
2022-07-08 12:40:45 - train: epoch 0081, iter [04900, 05004], lr: 0.023898, loss: 1.2902
2022-07-08 12:41:30 - train: epoch 0081, iter [05000, 05004], lr: 0.023876, loss: 1.0871
2022-07-08 12:41:32 - train: epoch 081, train_loss: 1.2379
2022-07-08 12:42:49 - eval: epoch: 081, acc1: 72.078%, acc5: 91.060%, test_loss: 1.1362, per_image_load_time: 2.102ms, per_image_inference_time: 0.836ms
2022-07-08 12:42:50 - until epoch: 081, best_acc1: 72.078%
2022-07-08 12:42:50 - epoch 082 lr: 0.023875
2022-07-08 12:43:40 - train: epoch 0082, iter [00100, 05004], lr: 0.023853, loss: 0.9909
2022-07-08 12:44:25 - train: epoch 0082, iter [00200, 05004], lr: 0.023830, loss: 1.0505
2022-07-08 12:45:10 - train: epoch 0082, iter [00300, 05004], lr: 0.023808, loss: 1.2468
2022-07-08 12:45:55 - train: epoch 0082, iter [00400, 05004], lr: 0.023786, loss: 1.3132
2022-07-08 12:46:40 - train: epoch 0082, iter [00500, 05004], lr: 0.023764, loss: 1.2933
2022-07-08 12:47:25 - train: epoch 0082, iter [00600, 05004], lr: 0.023741, loss: 1.0491
2022-07-08 12:48:10 - train: epoch 0082, iter [00700, 05004], lr: 0.023719, loss: 1.3318
2022-07-08 12:48:55 - train: epoch 0082, iter [00800, 05004], lr: 0.023697, loss: 1.1855
2022-07-08 12:49:40 - train: epoch 0082, iter [00900, 05004], lr: 0.023675, loss: 1.3437
2022-07-08 12:50:25 - train: epoch 0082, iter [01000, 05004], lr: 0.023652, loss: 1.3510
2022-07-08 12:51:10 - train: epoch 0082, iter [01100, 05004], lr: 0.023630, loss: 1.3613
2022-07-08 12:51:55 - train: epoch 0082, iter [01200, 05004], lr: 0.023608, loss: 1.1793
2022-07-08 12:52:40 - train: epoch 0082, iter [01300, 05004], lr: 0.023586, loss: 1.3973
2022-07-08 12:53:25 - train: epoch 0082, iter [01400, 05004], lr: 0.023564, loss: 1.1119
2022-07-08 12:54:10 - train: epoch 0082, iter [01500, 05004], lr: 0.023541, loss: 1.0335
2022-07-08 12:54:55 - train: epoch 0082, iter [01600, 05004], lr: 0.023519, loss: 1.1925
2022-07-08 12:55:40 - train: epoch 0082, iter [01700, 05004], lr: 0.023497, loss: 1.0390
2022-07-08 12:56:25 - train: epoch 0082, iter [01800, 05004], lr: 0.023475, loss: 1.0722
2022-07-08 12:57:10 - train: epoch 0082, iter [01900, 05004], lr: 0.023453, loss: 1.0914
2022-07-08 12:57:55 - train: epoch 0082, iter [02000, 05004], lr: 0.023430, loss: 1.0192
2022-07-08 12:58:40 - train: epoch 0082, iter [02100, 05004], lr: 0.023408, loss: 1.1645
2022-07-08 12:59:25 - train: epoch 0082, iter [02200, 05004], lr: 0.023386, loss: 1.3295
2022-07-08 13:00:10 - train: epoch 0082, iter [02300, 05004], lr: 0.023364, loss: 1.2347
2022-07-08 13:00:55 - train: epoch 0082, iter [02400, 05004], lr: 0.023342, loss: 1.2271
2022-07-08 13:01:40 - train: epoch 0082, iter [02500, 05004], lr: 0.023320, loss: 1.1010
2022-07-08 13:02:25 - train: epoch 0082, iter [02600, 05004], lr: 0.023298, loss: 1.1369
2022-07-08 13:03:10 - train: epoch 0082, iter [02700, 05004], lr: 0.023275, loss: 1.2191
2022-07-08 13:03:55 - train: epoch 0082, iter [02800, 05004], lr: 0.023253, loss: 1.0418
2022-07-08 13:04:40 - train: epoch 0082, iter [02900, 05004], lr: 0.023231, loss: 1.1075
2022-07-08 13:05:25 - train: epoch 0082, iter [03000, 05004], lr: 0.023209, loss: 1.1912
2022-07-08 13:06:10 - train: epoch 0082, iter [03100, 05004], lr: 0.023187, loss: 1.1099
2022-07-08 13:06:55 - train: epoch 0082, iter [03200, 05004], lr: 0.023165, loss: 1.5302
2022-07-08 13:07:40 - train: epoch 0082, iter [03300, 05004], lr: 0.023143, loss: 1.1508
2022-07-08 13:08:25 - train: epoch 0082, iter [03400, 05004], lr: 0.023121, loss: 1.1841
2022-07-08 13:09:10 - train: epoch 0082, iter [03500, 05004], lr: 0.023099, loss: 1.0951
2022-07-08 13:09:55 - train: epoch 0082, iter [03600, 05004], lr: 0.023077, loss: 1.0919
2022-07-08 13:10:40 - train: epoch 0082, iter [03700, 05004], lr: 0.023055, loss: 1.1419
2022-07-08 13:11:25 - train: epoch 0082, iter [03800, 05004], lr: 0.023033, loss: 1.3940
2022-07-08 13:12:10 - train: epoch 0082, iter [03900, 05004], lr: 0.023011, loss: 1.3521
2022-07-08 13:12:55 - train: epoch 0082, iter [04000, 05004], lr: 0.022989, loss: 1.3098
2022-07-08 13:13:40 - train: epoch 0082, iter [04100, 05004], lr: 0.022967, loss: 1.2060
2022-07-08 13:14:25 - train: epoch 0082, iter [04200, 05004], lr: 0.022945, loss: 1.3234
2022-07-08 13:15:10 - train: epoch 0082, iter [04300, 05004], lr: 0.022923, loss: 1.0506
2022-07-08 13:15:55 - train: epoch 0082, iter [04400, 05004], lr: 0.022901, loss: 1.1953
2022-07-08 13:16:40 - train: epoch 0082, iter [04500, 05004], lr: 0.022879, loss: 1.2806
2022-07-08 13:17:25 - train: epoch 0082, iter [04600, 05004], lr: 0.022857, loss: 1.1968
2022-07-08 13:18:10 - train: epoch 0082, iter [04700, 05004], lr: 0.022835, loss: 1.1574
2022-07-08 13:18:55 - train: epoch 0082, iter [04800, 05004], lr: 0.022813, loss: 1.0665
2022-07-08 13:19:40 - train: epoch 0082, iter [04900, 05004], lr: 0.022791, loss: 1.2303
2022-07-08 13:20:25 - train: epoch 0082, iter [05000, 05004], lr: 0.022769, loss: 1.1468
2022-07-08 13:20:27 - train: epoch 082, train_loss: 1.2199
2022-07-08 13:21:44 - eval: epoch: 082, acc1: 72.012%, acc5: 90.918%, test_loss: 1.1356, per_image_load_time: 1.753ms, per_image_inference_time: 0.822ms
2022-07-08 13:21:45 - until epoch: 082, best_acc1: 72.078%
2022-07-08 13:21:45 - epoch 083 lr: 0.022768
2022-07-08 13:22:36 - train: epoch 0083, iter [00100, 05004], lr: 0.022746, loss: 1.1298
2022-07-08 13:23:21 - train: epoch 0083, iter [00200, 05004], lr: 0.022724, loss: 1.0750
2022-07-08 13:24:06 - train: epoch 0083, iter [00300, 05004], lr: 0.022702, loss: 1.1911
2022-07-08 13:24:52 - train: epoch 0083, iter [00400, 05004], lr: 0.022680, loss: 1.3809
2022-07-08 13:25:37 - train: epoch 0083, iter [00500, 05004], lr: 0.022658, loss: 1.1186
2022-07-08 13:26:22 - train: epoch 0083, iter [00600, 05004], lr: 0.022637, loss: 1.1688
2022-07-08 13:27:07 - train: epoch 0083, iter [00700, 05004], lr: 0.022615, loss: 1.0729
2022-07-08 13:27:52 - train: epoch 0083, iter [00800, 05004], lr: 0.022593, loss: 1.2135
2022-07-08 13:28:37 - train: epoch 0083, iter [00900, 05004], lr: 0.022571, loss: 1.1372
2022-07-08 13:29:22 - train: epoch 0083, iter [01000, 05004], lr: 0.022549, loss: 1.2887
2022-07-08 13:30:08 - train: epoch 0083, iter [01100, 05004], lr: 0.022527, loss: 1.2061
2022-07-08 13:30:53 - train: epoch 0083, iter [01200, 05004], lr: 0.022505, loss: 1.1182
2022-07-08 13:31:38 - train: epoch 0083, iter [01300, 05004], lr: 0.022483, loss: 1.1211
2022-07-08 13:32:23 - train: epoch 0083, iter [01400, 05004], lr: 0.022462, loss: 1.2744
2022-07-08 13:33:08 - train: epoch 0083, iter [01500, 05004], lr: 0.022440, loss: 1.1000
2022-07-08 13:33:53 - train: epoch 0083, iter [01600, 05004], lr: 0.022418, loss: 1.1939
2022-07-08 13:34:38 - train: epoch 0083, iter [01700, 05004], lr: 0.022396, loss: 1.3012
2022-07-08 13:35:23 - train: epoch 0083, iter [01800, 05004], lr: 0.022374, loss: 1.3401
2022-07-08 13:36:08 - train: epoch 0083, iter [01900, 05004], lr: 0.022353, loss: 1.0175
2022-07-08 13:36:53 - train: epoch 0083, iter [02000, 05004], lr: 0.022331, loss: 0.8604
2022-07-08 13:37:38 - train: epoch 0083, iter [02100, 05004], lr: 0.022309, loss: 1.1420
2022-07-08 13:38:24 - train: epoch 0083, iter [02200, 05004], lr: 0.022287, loss: 1.2514
2022-07-08 13:39:09 - train: epoch 0083, iter [02300, 05004], lr: 0.022265, loss: 1.2839
2022-07-08 13:39:54 - train: epoch 0083, iter [02400, 05004], lr: 0.022244, loss: 1.1555
2022-07-08 13:40:39 - train: epoch 0083, iter [02500, 05004], lr: 0.022222, loss: 1.2620
2022-07-08 13:41:24 - train: epoch 0083, iter [02600, 05004], lr: 0.022200, loss: 1.1347
2022-07-08 13:42:09 - train: epoch 0083, iter [02700, 05004], lr: 0.022178, loss: 1.1479
2022-07-08 13:42:54 - train: epoch 0083, iter [02800, 05004], lr: 0.022157, loss: 1.1318
2022-07-08 13:43:39 - train: epoch 0083, iter [02900, 05004], lr: 0.022135, loss: 1.1693
2022-07-08 13:44:24 - train: epoch 0083, iter [03000, 05004], lr: 0.022113, loss: 1.2875
2022-07-08 13:45:09 - train: epoch 0083, iter [03100, 05004], lr: 0.022092, loss: 1.1012
2022-07-08 13:45:54 - train: epoch 0083, iter [03200, 05004], lr: 0.022070, loss: 1.0451
2022-07-08 13:46:40 - train: epoch 0083, iter [03300, 05004], lr: 0.022048, loss: 1.4261
2022-07-08 13:47:25 - train: epoch 0083, iter [03400, 05004], lr: 0.022026, loss: 1.3369
2022-07-08 13:48:10 - train: epoch 0083, iter [03500, 05004], lr: 0.022005, loss: 1.3016
2022-07-08 13:48:55 - train: epoch 0083, iter [03600, 05004], lr: 0.021983, loss: 1.2435
2022-07-08 13:49:40 - train: epoch 0083, iter [03700, 05004], lr: 0.021961, loss: 1.2077
2022-07-08 13:50:25 - train: epoch 0083, iter [03800, 05004], lr: 0.021940, loss: 1.1381
2022-07-08 13:51:10 - train: epoch 0083, iter [03900, 05004], lr: 0.021918, loss: 1.1776
2022-07-08 13:51:56 - train: epoch 0083, iter [04000, 05004], lr: 0.021897, loss: 1.3824
2022-07-08 13:52:41 - train: epoch 0083, iter [04100, 05004], lr: 0.021875, loss: 1.2419
2022-07-08 13:53:26 - train: epoch 0083, iter [04200, 05004], lr: 0.021853, loss: 1.4960
2022-07-08 13:54:11 - train: epoch 0083, iter [04300, 05004], lr: 0.021832, loss: 1.2067
2022-07-08 13:54:56 - train: epoch 0083, iter [04400, 05004], lr: 0.021810, loss: 1.0907
2022-07-08 13:55:41 - train: epoch 0083, iter [04500, 05004], lr: 0.021788, loss: 1.3014
2022-07-08 13:56:26 - train: epoch 0083, iter [04600, 05004], lr: 0.021767, loss: 1.3469
2022-07-08 13:57:11 - train: epoch 0083, iter [04700, 05004], lr: 0.021745, loss: 1.2614
2022-07-08 13:57:56 - train: epoch 0083, iter [04800, 05004], lr: 0.021724, loss: 1.2582
2022-07-08 13:58:41 - train: epoch 0083, iter [04900, 05004], lr: 0.021702, loss: 1.4607
2022-07-08 13:59:26 - train: epoch 0083, iter [05000, 05004], lr: 0.021681, loss: 1.3273
2022-07-08 13:59:29 - train: epoch 083, train_loss: 1.2036
2022-07-08 14:00:44 - eval: epoch: 083, acc1: 72.450%, acc5: 91.166%, test_loss: 1.1117, per_image_load_time: 1.907ms, per_image_inference_time: 0.850ms
2022-07-08 14:00:46 - until epoch: 083, best_acc1: 72.450%
2022-07-08 14:00:46 - epoch 084 lr: 0.021679
2022-07-08 14:01:36 - train: epoch 0084, iter [00100, 05004], lr: 0.021658, loss: 1.1595
2022-07-08 14:02:21 - train: epoch 0084, iter [00200, 05004], lr: 0.021637, loss: 1.2171
2022-07-08 14:03:06 - train: epoch 0084, iter [00300, 05004], lr: 0.021615, loss: 0.9644
2022-07-08 14:03:52 - train: epoch 0084, iter [00400, 05004], lr: 0.021594, loss: 1.1716
2022-07-08 14:04:37 - train: epoch 0084, iter [00500, 05004], lr: 0.021572, loss: 1.1724
2022-07-08 14:05:22 - train: epoch 0084, iter [00600, 05004], lr: 0.021550, loss: 1.4223
2022-07-08 14:06:07 - train: epoch 0084, iter [00700, 05004], lr: 0.021529, loss: 1.1927
2022-07-08 14:06:52 - train: epoch 0084, iter [00800, 05004], lr: 0.021507, loss: 1.2313
2022-07-08 14:07:37 - train: epoch 0084, iter [00900, 05004], lr: 0.021486, loss: 1.1380
2022-07-08 14:08:22 - train: epoch 0084, iter [01000, 05004], lr: 0.021464, loss: 1.2718
2022-07-08 14:09:07 - train: epoch 0084, iter [01100, 05004], lr: 0.021443, loss: 1.0431
2022-07-08 14:09:52 - train: epoch 0084, iter [01200, 05004], lr: 0.021422, loss: 1.4715
2022-07-08 14:10:37 - train: epoch 0084, iter [01300, 05004], lr: 0.021400, loss: 1.1859
2022-07-08 14:11:22 - train: epoch 0084, iter [01400, 05004], lr: 0.021379, loss: 1.2253
2022-07-08 14:12:07 - train: epoch 0084, iter [01500, 05004], lr: 0.021357, loss: 1.1678
2022-07-08 14:12:52 - train: epoch 0084, iter [01600, 05004], lr: 0.021336, loss: 1.0104
2022-07-08 14:13:37 - train: epoch 0084, iter [01700, 05004], lr: 0.021314, loss: 1.2341
2022-07-08 14:14:22 - train: epoch 0084, iter [01800, 05004], lr: 0.021293, loss: 1.2235
2022-07-08 14:15:07 - train: epoch 0084, iter [01900, 05004], lr: 0.021271, loss: 1.2182
2022-07-08 14:15:52 - train: epoch 0084, iter [02000, 05004], lr: 0.021250, loss: 1.1903
2022-07-08 14:16:37 - train: epoch 0084, iter [02100, 05004], lr: 0.021229, loss: 1.1360
2022-07-08 14:17:22 - train: epoch 0084, iter [02200, 05004], lr: 0.021207, loss: 0.8968
2022-07-08 14:18:07 - train: epoch 0084, iter [02300, 05004], lr: 0.021186, loss: 1.1462
2022-07-08 14:18:52 - train: epoch 0084, iter [02400, 05004], lr: 0.021165, loss: 1.0510
2022-07-08 14:19:37 - train: epoch 0084, iter [02500, 05004], lr: 0.021143, loss: 1.2622
2022-07-08 14:20:22 - train: epoch 0084, iter [02600, 05004], lr: 0.021122, loss: 1.3679
2022-07-08 14:21:08 - train: epoch 0084, iter [02700, 05004], lr: 0.021100, loss: 1.1514
2022-07-08 14:21:53 - train: epoch 0084, iter [02800, 05004], lr: 0.021079, loss: 1.2206
2022-07-08 14:22:38 - train: epoch 0084, iter [02900, 05004], lr: 0.021058, loss: 1.1956
2022-07-08 14:23:23 - train: epoch 0084, iter [03000, 05004], lr: 0.021036, loss: 1.1604
2022-07-08 14:24:08 - train: epoch 0084, iter [03100, 05004], lr: 0.021015, loss: 1.1061
2022-07-08 14:24:53 - train: epoch 0084, iter [03200, 05004], lr: 0.020994, loss: 1.1074
2022-07-08 14:25:38 - train: epoch 0084, iter [03300, 05004], lr: 0.020973, loss: 1.1911
2022-07-08 14:26:23 - train: epoch 0084, iter [03400, 05004], lr: 0.020951, loss: 1.2016
2022-07-08 14:27:08 - train: epoch 0084, iter [03500, 05004], lr: 0.020930, loss: 1.0193
2022-07-08 14:27:53 - train: epoch 0084, iter [03600, 05004], lr: 0.020909, loss: 1.1883
2022-07-08 14:28:38 - train: epoch 0084, iter [03700, 05004], lr: 0.020887, loss: 1.3140
2022-07-08 14:29:23 - train: epoch 0084, iter [03800, 05004], lr: 0.020866, loss: 1.3338
2022-07-08 14:30:08 - train: epoch 0084, iter [03900, 05004], lr: 0.020845, loss: 1.3270
2022-07-08 14:30:53 - train: epoch 0084, iter [04000, 05004], lr: 0.020824, loss: 1.2338
2022-07-08 14:31:38 - train: epoch 0084, iter [04100, 05004], lr: 0.020802, loss: 1.2081
2022-07-08 14:32:23 - train: epoch 0084, iter [04200, 05004], lr: 0.020781, loss: 1.2794
2022-07-08 14:33:08 - train: epoch 0084, iter [04300, 05004], lr: 0.020760, loss: 1.2731
2022-07-08 14:33:53 - train: epoch 0084, iter [04400, 05004], lr: 0.020739, loss: 1.4701
2022-07-08 14:34:38 - train: epoch 0084, iter [04500, 05004], lr: 0.020718, loss: 1.1539
2022-07-08 14:35:23 - train: epoch 0084, iter [04600, 05004], lr: 0.020696, loss: 1.2577
2022-07-08 14:36:08 - train: epoch 0084, iter [04700, 05004], lr: 0.020675, loss: 1.4282
2022-07-08 14:36:53 - train: epoch 0084, iter [04800, 05004], lr: 0.020654, loss: 1.0274
2022-07-08 14:37:38 - train: epoch 0084, iter [04900, 05004], lr: 0.020633, loss: 1.0357
2022-07-08 14:38:24 - train: epoch 0084, iter [05000, 05004], lr: 0.020612, loss: 1.2245
2022-07-08 14:38:26 - train: epoch 084, train_loss: 1.1865
2022-07-08 14:39:42 - eval: epoch: 084, acc1: 72.674%, acc5: 91.420%, test_loss: 1.1032, per_image_load_time: 1.833ms, per_image_inference_time: 0.838ms
2022-07-08 14:39:44 - until epoch: 084, best_acc1: 72.674%
2022-07-08 14:39:44 - epoch 085 lr: 0.020611
2022-07-08 14:40:34 - train: epoch 0085, iter [00100, 05004], lr: 0.020590, loss: 0.9870
2022-07-08 14:41:19 - train: epoch 0085, iter [00200, 05004], lr: 0.020568, loss: 1.0186
2022-07-08 14:42:04 - train: epoch 0085, iter [00300, 05004], lr: 0.020547, loss: 1.2590
2022-07-08 14:42:49 - train: epoch 0085, iter [00400, 05004], lr: 0.020526, loss: 1.0364
2022-07-08 14:43:34 - train: epoch 0085, iter [00500, 05004], lr: 0.020505, loss: 1.3499
2022-07-08 14:44:19 - train: epoch 0085, iter [00600, 05004], lr: 0.020484, loss: 0.8868
2022-07-08 14:45:05 - train: epoch 0085, iter [00700, 05004], lr: 0.020463, loss: 1.1145
2022-07-08 14:45:50 - train: epoch 0085, iter [00800, 05004], lr: 0.020442, loss: 1.0427
2022-07-08 14:46:35 - train: epoch 0085, iter [00900, 05004], lr: 0.020421, loss: 1.0725
2022-07-08 14:47:20 - train: epoch 0085, iter [01000, 05004], lr: 0.020400, loss: 1.2637
2022-07-08 14:48:05 - train: epoch 0085, iter [01100, 05004], lr: 0.020378, loss: 1.1493
2022-07-08 14:48:50 - train: epoch 0085, iter [01200, 05004], lr: 0.020357, loss: 1.0837
2022-07-08 14:49:35 - train: epoch 0085, iter [01300, 05004], lr: 0.020336, loss: 1.1098
2022-07-08 14:50:20 - train: epoch 0085, iter [01400, 05004], lr: 0.020315, loss: 1.1903
2022-07-08 14:51:05 - train: epoch 0085, iter [01500, 05004], lr: 0.020294, loss: 1.0528
2022-07-08 14:51:50 - train: epoch 0085, iter [01600, 05004], lr: 0.020273, loss: 1.1385
2022-07-08 14:52:35 - train: epoch 0085, iter [01700, 05004], lr: 0.020252, loss: 1.3070
2022-07-08 14:53:20 - train: epoch 0085, iter [01800, 05004], lr: 0.020231, loss: 1.0058
2022-07-08 14:54:05 - train: epoch 0085, iter [01900, 05004], lr: 0.020210, loss: 1.0400
2022-07-08 14:54:50 - train: epoch 0085, iter [02000, 05004], lr: 0.020189, loss: 1.1340
2022-07-08 14:55:35 - train: epoch 0085, iter [02100, 05004], lr: 0.020168, loss: 0.9156
2022-07-08 14:56:20 - train: epoch 0085, iter [02200, 05004], lr: 0.020147, loss: 1.2454
2022-07-08 14:57:06 - train: epoch 0085, iter [02300, 05004], lr: 0.020126, loss: 0.9418
2022-07-08 14:57:51 - train: epoch 0085, iter [02400, 05004], lr: 0.020105, loss: 1.2739
2022-07-08 14:58:36 - train: epoch 0085, iter [02500, 05004], lr: 0.020084, loss: 1.4343
2022-07-08 14:59:21 - train: epoch 0085, iter [02600, 05004], lr: 0.020063, loss: 1.2119
2022-07-08 15:00:06 - train: epoch 0085, iter [02700, 05004], lr: 0.020042, loss: 1.0287
2022-07-08 15:00:51 - train: epoch 0085, iter [02800, 05004], lr: 0.020021, loss: 1.3612
2022-07-08 15:01:36 - train: epoch 0085, iter [02900, 05004], lr: 0.020000, loss: 1.3194
2022-07-08 15:02:21 - train: epoch 0085, iter [03000, 05004], lr: 0.019979, loss: 1.2801
2022-07-08 15:03:06 - train: epoch 0085, iter [03100, 05004], lr: 0.019959, loss: 1.1416
2022-07-08 15:03:51 - train: epoch 0085, iter [03200, 05004], lr: 0.019938, loss: 1.0982
2022-07-08 15:04:36 - train: epoch 0085, iter [03300, 05004], lr: 0.019917, loss: 1.1647
2022-07-08 15:05:21 - train: epoch 0085, iter [03400, 05004], lr: 0.019896, loss: 1.2780
2022-07-08 15:06:06 - train: epoch 0085, iter [03500, 05004], lr: 0.019875, loss: 1.2667
2022-07-08 15:06:51 - train: epoch 0085, iter [03600, 05004], lr: 0.019854, loss: 1.2537
2022-07-08 15:07:36 - train: epoch 0085, iter [03700, 05004], lr: 0.019833, loss: 1.0674
2022-07-08 15:08:21 - train: epoch 0085, iter [03800, 05004], lr: 0.019812, loss: 1.0665
2022-07-08 15:09:06 - train: epoch 0085, iter [03900, 05004], lr: 0.019792, loss: 1.1473
2022-07-08 15:09:51 - train: epoch 0085, iter [04000, 05004], lr: 0.019771, loss: 1.4364
2022-07-08 15:10:36 - train: epoch 0085, iter [04100, 05004], lr: 0.019750, loss: 1.1109
2022-07-08 15:11:21 - train: epoch 0085, iter [04200, 05004], lr: 0.019729, loss: 1.3110
2022-07-08 15:12:06 - train: epoch 0085, iter [04300, 05004], lr: 0.019708, loss: 1.0996
2022-07-08 15:12:51 - train: epoch 0085, iter [04400, 05004], lr: 0.019687, loss: 1.2038
2022-07-08 15:13:36 - train: epoch 0085, iter [04500, 05004], lr: 0.019667, loss: 1.2855
2022-07-08 15:14:22 - train: epoch 0085, iter [04600, 05004], lr: 0.019646, loss: 1.2952
2022-07-08 15:15:07 - train: epoch 0085, iter [04700, 05004], lr: 0.019625, loss: 1.5060
2022-07-08 15:15:52 - train: epoch 0085, iter [04800, 05004], lr: 0.019604, loss: 0.9747
2022-07-08 15:16:37 - train: epoch 0085, iter [04900, 05004], lr: 0.019584, loss: 1.1235
2022-07-08 15:17:22 - train: epoch 0085, iter [05000, 05004], lr: 0.019563, loss: 1.1364
2022-07-08 15:17:24 - train: epoch 085, train_loss: 1.1733
2022-07-08 15:18:39 - eval: epoch: 085, acc1: 72.956%, acc5: 91.370%, test_loss: 1.0962, per_image_load_time: 2.102ms, per_image_inference_time: 0.834ms
2022-07-08 15:18:41 - until epoch: 085, best_acc1: 72.956%
2022-07-08 15:18:41 - epoch 086 lr: 0.019562
2022-07-08 15:19:32 - train: epoch 0086, iter [00100, 05004], lr: 0.019541, loss: 1.0204
2022-07-08 15:20:17 - train: epoch 0086, iter [00200, 05004], lr: 0.019520, loss: 1.0603
2022-07-08 15:21:02 - train: epoch 0086, iter [00300, 05004], lr: 0.019500, loss: 1.2535
2022-07-08 15:21:47 - train: epoch 0086, iter [00400, 05004], lr: 0.019479, loss: 1.2182
2022-07-08 15:22:32 - train: epoch 0086, iter [00500, 05004], lr: 0.019458, loss: 1.1750
2022-07-08 15:23:17 - train: epoch 0086, iter [00600, 05004], lr: 0.019438, loss: 1.1932
2022-07-08 15:24:02 - train: epoch 0086, iter [00700, 05004], lr: 0.019417, loss: 1.0266
2022-07-08 15:24:47 - train: epoch 0086, iter [00800, 05004], lr: 0.019396, loss: 1.2736
2022-07-08 15:25:33 - train: epoch 0086, iter [00900, 05004], lr: 0.019375, loss: 1.2348
2022-07-08 15:26:18 - train: epoch 0086, iter [01000, 05004], lr: 0.019355, loss: 1.1376
2022-07-08 15:27:03 - train: epoch 0086, iter [01100, 05004], lr: 0.019334, loss: 1.1997
2022-07-08 15:27:48 - train: epoch 0086, iter [01200, 05004], lr: 0.019313, loss: 0.9622
2022-07-08 15:28:33 - train: epoch 0086, iter [01300, 05004], lr: 0.019293, loss: 1.1515
2022-07-08 15:29:18 - train: epoch 0086, iter [01400, 05004], lr: 0.019272, loss: 1.0919
2022-07-08 15:30:03 - train: epoch 0086, iter [01500, 05004], lr: 0.019252, loss: 1.0727
2022-07-08 15:30:48 - train: epoch 0086, iter [01600, 05004], lr: 0.019231, loss: 1.2806
2022-07-08 15:31:33 - train: epoch 0086, iter [01700, 05004], lr: 0.019210, loss: 1.0816
2022-07-08 15:32:18 - train: epoch 0086, iter [01800, 05004], lr: 0.019190, loss: 1.1632
2022-07-08 15:33:03 - train: epoch 0086, iter [01900, 05004], lr: 0.019169, loss: 1.2596
2022-07-08 15:33:48 - train: epoch 0086, iter [02000, 05004], lr: 0.019149, loss: 1.1490
2022-07-08 15:34:33 - train: epoch 0086, iter [02100, 05004], lr: 0.019128, loss: 1.0111
2022-07-08 15:35:18 - train: epoch 0086, iter [02200, 05004], lr: 0.019107, loss: 1.1874
2022-07-08 15:36:04 - train: epoch 0086, iter [02300, 05004], lr: 0.019087, loss: 1.1570
2022-07-08 15:36:49 - train: epoch 0086, iter [02400, 05004], lr: 0.019066, loss: 0.9277
2022-07-08 15:37:34 - train: epoch 0086, iter [02500, 05004], lr: 0.019046, loss: 1.0611
2022-07-08 15:38:19 - train: epoch 0086, iter [02600, 05004], lr: 0.019025, loss: 1.1398
2022-07-08 15:39:04 - train: epoch 0086, iter [02700, 05004], lr: 0.019005, loss: 1.0947
2022-07-08 15:39:49 - train: epoch 0086, iter [02800, 05004], lr: 0.018984, loss: 1.2468
2022-07-08 15:40:34 - train: epoch 0086, iter [02900, 05004], lr: 0.018964, loss: 1.0933
2022-07-08 15:41:19 - train: epoch 0086, iter [03000, 05004], lr: 0.018943, loss: 1.2537
2022-07-08 15:42:04 - train: epoch 0086, iter [03100, 05004], lr: 0.018923, loss: 1.1987
2022-07-08 15:42:49 - train: epoch 0086, iter [03200, 05004], lr: 0.018902, loss: 1.1878
2022-07-08 15:43:34 - train: epoch 0086, iter [03300, 05004], lr: 0.018882, loss: 1.2089
2022-07-08 15:44:19 - train: epoch 0086, iter [03400, 05004], lr: 0.018861, loss: 1.2691
2022-07-08 15:45:04 - train: epoch 0086, iter [03500, 05004], lr: 0.018841, loss: 1.2131
2022-07-08 15:45:49 - train: epoch 0086, iter [03600, 05004], lr: 0.018820, loss: 1.1584
2022-07-08 15:46:34 - train: epoch 0086, iter [03700, 05004], lr: 0.018800, loss: 1.3844
2022-07-08 15:47:20 - train: epoch 0086, iter [03800, 05004], lr: 0.018779, loss: 1.1284
2022-07-08 15:48:05 - train: epoch 0086, iter [03900, 05004], lr: 0.018759, loss: 1.1693
2022-07-08 15:48:50 - train: epoch 0086, iter [04000, 05004], lr: 0.018739, loss: 1.1524
2022-07-08 15:49:35 - train: epoch 0086, iter [04100, 05004], lr: 0.018718, loss: 1.2125
2022-07-08 15:50:20 - train: epoch 0086, iter [04200, 05004], lr: 0.018698, loss: 1.2529
2022-07-08 15:51:05 - train: epoch 0086, iter [04300, 05004], lr: 0.018677, loss: 1.2452
2022-07-08 15:51:50 - train: epoch 0086, iter [04400, 05004], lr: 0.018657, loss: 1.0992
2022-07-08 15:52:35 - train: epoch 0086, iter [04500, 05004], lr: 0.018637, loss: 1.0361
2022-07-08 15:53:20 - train: epoch 0086, iter [04600, 05004], lr: 0.018616, loss: 1.1204
2022-07-08 15:54:05 - train: epoch 0086, iter [04700, 05004], lr: 0.018596, loss: 1.1541
2022-07-08 15:54:50 - train: epoch 0086, iter [04800, 05004], lr: 0.018575, loss: 1.1455
2022-07-08 15:55:35 - train: epoch 0086, iter [04900, 05004], lr: 0.018555, loss: 1.2120
2022-07-08 15:56:20 - train: epoch 0086, iter [05000, 05004], lr: 0.018535, loss: 1.2144
2022-07-08 15:56:23 - train: epoch 086, train_loss: 1.1529
2022-07-08 15:57:39 - eval: epoch: 086, acc1: 73.078%, acc5: 91.508%, test_loss: 1.0846, per_image_load_time: 2.067ms, per_image_inference_time: 0.827ms
2022-07-08 15:57:40 - until epoch: 086, best_acc1: 73.078%
2022-07-08 15:57:40 - epoch 087 lr: 0.018534
2022-07-08 15:58:31 - train: epoch 0087, iter [00100, 05004], lr: 0.018514, loss: 1.2374
2022-07-08 15:59:16 - train: epoch 0087, iter [00200, 05004], lr: 0.018493, loss: 0.8457
2022-07-08 16:00:01 - train: epoch 0087, iter [00300, 05004], lr: 0.018473, loss: 1.3136
2022-07-08 16:00:47 - train: epoch 0087, iter [00400, 05004], lr: 0.018453, loss: 1.1613
2022-07-08 16:01:32 - train: epoch 0087, iter [00500, 05004], lr: 0.018432, loss: 0.8670
2022-07-08 16:02:17 - train: epoch 0087, iter [00600, 05004], lr: 0.018412, loss: 1.1275
2022-07-08 16:03:02 - train: epoch 0087, iter [00700, 05004], lr: 0.018392, loss: 0.9207
2022-07-08 16:03:47 - train: epoch 0087, iter [00800, 05004], lr: 0.018372, loss: 1.1782
2022-07-08 16:04:32 - train: epoch 0087, iter [00900, 05004], lr: 0.018351, loss: 1.1912
2022-07-08 16:05:17 - train: epoch 0087, iter [01000, 05004], lr: 0.018331, loss: 1.1324
2022-07-08 16:06:02 - train: epoch 0087, iter [01100, 05004], lr: 0.018311, loss: 1.2393
2022-07-08 16:06:47 - train: epoch 0087, iter [01200, 05004], lr: 0.018291, loss: 1.1426
2022-07-08 16:07:32 - train: epoch 0087, iter [01300, 05004], lr: 0.018270, loss: 1.3741
2022-07-08 16:08:17 - train: epoch 0087, iter [01400, 05004], lr: 0.018250, loss: 1.1117
2022-07-08 16:09:02 - train: epoch 0087, iter [01500, 05004], lr: 0.018230, loss: 0.9919
2022-07-08 16:09:47 - train: epoch 0087, iter [01600, 05004], lr: 0.018210, loss: 1.0645
2022-07-08 16:10:32 - train: epoch 0087, iter [01700, 05004], lr: 0.018190, loss: 1.1134
2022-07-08 16:11:17 - train: epoch 0087, iter [01800, 05004], lr: 0.018169, loss: 1.1784
2022-07-08 16:12:03 - train: epoch 0087, iter [01900, 05004], lr: 0.018149, loss: 1.1117
2022-07-08 16:12:48 - train: epoch 0087, iter [02000, 05004], lr: 0.018129, loss: 1.1587
2022-07-08 16:13:33 - train: epoch 0087, iter [02100, 05004], lr: 0.018109, loss: 1.2236
2022-07-08 16:14:18 - train: epoch 0087, iter [02200, 05004], lr: 0.018089, loss: 1.1283
2022-07-08 16:15:03 - train: epoch 0087, iter [02300, 05004], lr: 0.018069, loss: 1.1600
2022-07-08 16:15:48 - train: epoch 0087, iter [02400, 05004], lr: 0.018049, loss: 1.1050
2022-07-08 16:16:33 - train: epoch 0087, iter [02500, 05004], lr: 0.018028, loss: 1.1771
2022-07-08 16:17:18 - train: epoch 0087, iter [02600, 05004], lr: 0.018008, loss: 1.1505
2022-07-08 16:18:03 - train: epoch 0087, iter [02700, 05004], lr: 0.017988, loss: 1.0353
2022-07-08 16:18:48 - train: epoch 0087, iter [02800, 05004], lr: 0.017968, loss: 0.9690
2022-07-08 16:19:33 - train: epoch 0087, iter [02900, 05004], lr: 0.017948, loss: 1.0243
2022-07-08 16:20:18 - train: epoch 0087, iter [03000, 05004], lr: 0.017928, loss: 1.1817
2022-07-08 16:21:03 - train: epoch 0087, iter [03100, 05004], lr: 0.017908, loss: 1.1012
2022-07-08 16:21:49 - train: epoch 0087, iter [03200, 05004], lr: 0.017888, loss: 1.0821
2022-07-08 16:22:34 - train: epoch 0087, iter [03300, 05004], lr: 0.017868, loss: 1.1712
2022-07-08 16:23:19 - train: epoch 0087, iter [03400, 05004], lr: 0.017848, loss: 1.1649
2022-07-08 16:24:04 - train: epoch 0087, iter [03500, 05004], lr: 0.017828, loss: 0.9740
2022-07-08 16:24:49 - train: epoch 0087, iter [03600, 05004], lr: 0.017808, loss: 0.9379
2022-07-08 16:25:34 - train: epoch 0087, iter [03700, 05004], lr: 0.017788, loss: 1.0920
2022-07-08 16:26:19 - train: epoch 0087, iter [03800, 05004], lr: 0.017768, loss: 1.1377
2022-07-08 16:27:04 - train: epoch 0087, iter [03900, 05004], lr: 0.017748, loss: 1.1553
2022-07-08 16:27:49 - train: epoch 0087, iter [04000, 05004], lr: 0.017728, loss: 1.2225
2022-07-08 16:28:34 - train: epoch 0087, iter [04100, 05004], lr: 0.017708, loss: 1.2391
2022-07-08 16:29:19 - train: epoch 0087, iter [04200, 05004], lr: 0.017688, loss: 1.1336
2022-07-08 16:30:04 - train: epoch 0087, iter [04300, 05004], lr: 0.017668, loss: 1.1135
2022-07-08 16:30:49 - train: epoch 0087, iter [04400, 05004], lr: 0.017648, loss: 1.2168
2022-07-08 16:31:34 - train: epoch 0087, iter [04500, 05004], lr: 0.017628, loss: 1.2251
2022-07-08 16:32:19 - train: epoch 0087, iter [04600, 05004], lr: 0.017608, loss: 1.0954
2022-07-08 16:33:04 - train: epoch 0087, iter [04700, 05004], lr: 0.017588, loss: 1.1737
2022-07-08 16:33:49 - train: epoch 0087, iter [04800, 05004], lr: 0.017568, loss: 1.1466
2022-07-08 16:34:34 - train: epoch 0087, iter [04900, 05004], lr: 0.017548, loss: 1.1270
2022-07-08 16:35:19 - train: epoch 0087, iter [05000, 05004], lr: 0.017528, loss: 1.1735
2022-07-08 16:35:22 - train: epoch 087, train_loss: 1.1352
2022-07-08 16:36:37 - eval: epoch: 087, acc1: 73.264%, acc5: 91.564%, test_loss: 1.0810, per_image_load_time: 2.121ms, per_image_inference_time: 0.833ms
2022-07-08 16:36:39 - until epoch: 087, best_acc1: 73.264%
2022-07-08 16:36:39 - epoch 088 lr: 0.017527
2022-07-08 16:37:30 - train: epoch 0088, iter [00100, 05004], lr: 0.017508, loss: 1.0199
2022-07-08 16:38:15 - train: epoch 0088, iter [00200, 05004], lr: 0.017488, loss: 1.0163
2022-07-08 16:39:00 - train: epoch 0088, iter [00300, 05004], lr: 0.017468, loss: 1.2156
2022-07-08 16:39:46 - train: epoch 0088, iter [00400, 05004], lr: 0.017448, loss: 1.1076
2022-07-08 16:40:31 - train: epoch 0088, iter [00500, 05004], lr: 0.017428, loss: 0.9796
2022-07-08 16:41:16 - train: epoch 0088, iter [00600, 05004], lr: 0.017408, loss: 1.0490
2022-07-08 16:42:01 - train: epoch 0088, iter [00700, 05004], lr: 0.017389, loss: 0.9281
2022-07-08 16:42:46 - train: epoch 0088, iter [00800, 05004], lr: 0.017369, loss: 1.0721
2022-07-08 16:43:32 - train: epoch 0088, iter [00900, 05004], lr: 0.017349, loss: 1.1715
2022-07-08 16:44:17 - train: epoch 0088, iter [01000, 05004], lr: 0.017329, loss: 1.0148
2022-07-08 16:45:02 - train: epoch 0088, iter [01100, 05004], lr: 0.017309, loss: 1.0893
2022-07-08 16:45:47 - train: epoch 0088, iter [01200, 05004], lr: 0.017290, loss: 1.0760
2022-07-08 16:46:32 - train: epoch 0088, iter [01300, 05004], lr: 0.017270, loss: 1.2016
2022-07-08 16:47:17 - train: epoch 0088, iter [01400, 05004], lr: 0.017250, loss: 0.9592
2022-07-08 16:48:02 - train: epoch 0088, iter [01500, 05004], lr: 0.017230, loss: 1.1198
2022-07-08 16:48:47 - train: epoch 0088, iter [01600, 05004], lr: 0.017210, loss: 0.9768
2022-07-08 16:49:33 - train: epoch 0088, iter [01700, 05004], lr: 0.017191, loss: 1.1445
2022-07-08 16:50:18 - train: epoch 0088, iter [01800, 05004], lr: 0.017171, loss: 1.1065
2022-07-08 16:51:03 - train: epoch 0088, iter [01900, 05004], lr: 0.017151, loss: 1.1708
2022-07-08 16:51:48 - train: epoch 0088, iter [02000, 05004], lr: 0.017132, loss: 1.0511
2022-07-08 16:52:33 - train: epoch 0088, iter [02100, 05004], lr: 0.017112, loss: 1.1440
2022-07-08 16:53:18 - train: epoch 0088, iter [02200, 05004], lr: 0.017092, loss: 1.0650
2022-07-08 16:54:03 - train: epoch 0088, iter [02300, 05004], lr: 0.017072, loss: 0.9311
2022-07-08 16:54:48 - train: epoch 0088, iter [02400, 05004], lr: 0.017053, loss: 1.2264
2022-07-08 16:55:33 - train: epoch 0088, iter [02500, 05004], lr: 0.017033, loss: 1.1426
2022-07-08 16:56:18 - train: epoch 0088, iter [02600, 05004], lr: 0.017013, loss: 1.1868
2022-07-08 16:57:04 - train: epoch 0088, iter [02700, 05004], lr: 0.016994, loss: 1.2347
2022-07-08 16:57:49 - train: epoch 0088, iter [02800, 05004], lr: 0.016974, loss: 1.1299
2022-07-08 16:58:34 - train: epoch 0088, iter [02900, 05004], lr: 0.016955, loss: 1.1653
2022-07-08 16:59:19 - train: epoch 0088, iter [03000, 05004], lr: 0.016935, loss: 1.2261
2022-07-08 17:00:04 - train: epoch 0088, iter [03100, 05004], lr: 0.016915, loss: 1.0286
2022-07-08 17:00:49 - train: epoch 0088, iter [03200, 05004], lr: 0.016896, loss: 0.9893
2022-07-08 17:01:34 - train: epoch 0088, iter [03300, 05004], lr: 0.016876, loss: 1.0263
2022-07-08 17:02:19 - train: epoch 0088, iter [03400, 05004], lr: 0.016856, loss: 0.9519
2022-07-08 17:03:04 - train: epoch 0088, iter [03500, 05004], lr: 0.016837, loss: 1.0142
2022-07-08 17:03:49 - train: epoch 0088, iter [03600, 05004], lr: 0.016817, loss: 1.1666
2022-07-08 17:04:34 - train: epoch 0088, iter [03700, 05004], lr: 0.016798, loss: 1.1779
2022-07-08 17:05:19 - train: epoch 0088, iter [03800, 05004], lr: 0.016778, loss: 1.0833
2022-07-08 17:06:05 - train: epoch 0088, iter [03900, 05004], lr: 0.016759, loss: 1.2881
2022-07-08 17:06:50 - train: epoch 0088, iter [04000, 05004], lr: 0.016739, loss: 0.9430
2022-07-08 17:07:35 - train: epoch 0088, iter [04100, 05004], lr: 0.016720, loss: 1.2228
2022-07-08 17:08:20 - train: epoch 0088, iter [04200, 05004], lr: 0.016700, loss: 1.2868
2022-07-08 17:09:05 - train: epoch 0088, iter [04300, 05004], lr: 0.016681, loss: 1.0367
2022-07-08 17:09:50 - train: epoch 0088, iter [04400, 05004], lr: 0.016661, loss: 0.9757
2022-07-08 17:10:35 - train: epoch 0088, iter [04500, 05004], lr: 0.016642, loss: 1.1366
2022-07-08 17:11:20 - train: epoch 0088, iter [04600, 05004], lr: 0.016622, loss: 1.3046
2022-07-08 17:12:05 - train: epoch 0088, iter [04700, 05004], lr: 0.016603, loss: 1.1900
2022-07-08 17:12:50 - train: epoch 0088, iter [04800, 05004], lr: 0.016583, loss: 1.0719
2022-07-08 17:13:35 - train: epoch 0088, iter [04900, 05004], lr: 0.016564, loss: 0.9019
2022-07-08 17:14:21 - train: epoch 0088, iter [05000, 05004], lr: 0.016544, loss: 1.1553
2022-07-08 17:14:23 - train: epoch 088, train_loss: 1.1187
2022-07-08 17:15:40 - eval: epoch: 088, acc1: 73.524%, acc5: 91.800%, test_loss: 1.0631, per_image_load_time: 1.758ms, per_image_inference_time: 0.845ms
2022-07-08 17:15:41 - until epoch: 088, best_acc1: 73.524%
2022-07-08 17:15:41 - epoch 089 lr: 0.016543
2022-07-08 17:16:31 - train: epoch 0089, iter [00100, 05004], lr: 0.016524, loss: 1.2647
2022-07-08 17:17:17 - train: epoch 0089, iter [00200, 05004], lr: 0.016505, loss: 0.8622
2022-07-08 17:18:02 - train: epoch 0089, iter [00300, 05004], lr: 0.016485, loss: 1.1318
2022-07-08 17:18:48 - train: epoch 0089, iter [00400, 05004], lr: 0.016466, loss: 1.0143
2022-07-08 17:19:34 - train: epoch 0089, iter [00500, 05004], lr: 0.016446, loss: 1.0309
2022-07-08 17:20:20 - train: epoch 0089, iter [00600, 05004], lr: 0.016427, loss: 1.0858
2022-07-08 17:21:05 - train: epoch 0089, iter [00700, 05004], lr: 0.016408, loss: 1.2136
2022-07-08 17:21:51 - train: epoch 0089, iter [00800, 05004], lr: 0.016388, loss: 1.3216
2022-07-08 17:22:36 - train: epoch 0089, iter [00900, 05004], lr: 0.016369, loss: 1.1254
2022-07-08 17:23:22 - train: epoch 0089, iter [01000, 05004], lr: 0.016350, loss: 1.2375
2022-07-08 17:24:07 - train: epoch 0089, iter [01100, 05004], lr: 0.016330, loss: 1.0636
2022-07-08 17:24:53 - train: epoch 0089, iter [01200, 05004], lr: 0.016311, loss: 1.2029
2022-07-08 17:25:39 - train: epoch 0089, iter [01300, 05004], lr: 0.016292, loss: 1.0459
2022-07-08 17:26:24 - train: epoch 0089, iter [01400, 05004], lr: 0.016272, loss: 1.1620
2022-07-08 17:27:10 - train: epoch 0089, iter [01500, 05004], lr: 0.016253, loss: 1.0332
2022-07-08 17:27:55 - train: epoch 0089, iter [01600, 05004], lr: 0.016234, loss: 1.0301
2022-07-08 17:28:41 - train: epoch 0089, iter [01700, 05004], lr: 0.016214, loss: 1.0318
2022-07-08 17:29:26 - train: epoch 0089, iter [01800, 05004], lr: 0.016195, loss: 1.0164
2022-07-08 17:30:12 - train: epoch 0089, iter [01900, 05004], lr: 0.016176, loss: 1.0343
2022-07-08 17:30:57 - train: epoch 0089, iter [02000, 05004], lr: 0.016157, loss: 1.0375
2022-07-08 17:31:43 - train: epoch 0089, iter [02100, 05004], lr: 0.016137, loss: 1.1536
2022-07-08 17:32:28 - train: epoch 0089, iter [02200, 05004], lr: 0.016118, loss: 1.1278
2022-07-08 17:33:14 - train: epoch 0089, iter [02300, 05004], lr: 0.016099, loss: 0.9172
2022-07-08 17:34:00 - train: epoch 0089, iter [02400, 05004], lr: 0.016080, loss: 1.1427
2022-07-08 17:34:45 - train: epoch 0089, iter [02500, 05004], lr: 0.016060, loss: 1.0566
2022-07-08 17:35:31 - train: epoch 0089, iter [02600, 05004], lr: 0.016041, loss: 1.1369
2022-07-08 17:36:16 - train: epoch 0089, iter [02700, 05004], lr: 0.016022, loss: 0.9916
2022-07-08 17:37:02 - train: epoch 0089, iter [02800, 05004], lr: 0.016003, loss: 1.1394
2022-07-08 17:37:47 - train: epoch 0089, iter [02900, 05004], lr: 0.015984, loss: 1.0905
2022-07-08 17:38:33 - train: epoch 0089, iter [03000, 05004], lr: 0.015964, loss: 1.2042
2022-07-08 17:39:18 - train: epoch 0089, iter [03100, 05004], lr: 0.015945, loss: 1.1618
2022-07-08 17:40:04 - train: epoch 0089, iter [03200, 05004], lr: 0.015926, loss: 1.1810
2022-07-08 17:40:49 - train: epoch 0089, iter [03300, 05004], lr: 0.015907, loss: 1.2249
2022-07-08 17:41:35 - train: epoch 0089, iter [03400, 05004], lr: 0.015888, loss: 1.1548
2022-07-08 17:42:21 - train: epoch 0089, iter [03500, 05004], lr: 0.015869, loss: 0.9834
2022-07-08 17:43:06 - train: epoch 0089, iter [03600, 05004], lr: 0.015850, loss: 0.9504
2022-07-08 17:43:52 - train: epoch 0089, iter [03700, 05004], lr: 0.015831, loss: 1.3065
2022-07-08 17:44:37 - train: epoch 0089, iter [03800, 05004], lr: 0.015811, loss: 1.0515
2022-07-08 17:45:22 - train: epoch 0089, iter [03900, 05004], lr: 0.015792, loss: 1.1932
2022-07-08 17:46:08 - train: epoch 0089, iter [04000, 05004], lr: 0.015773, loss: 0.9230
2022-07-08 17:46:54 - train: epoch 0089, iter [04100, 05004], lr: 0.015754, loss: 1.1989
2022-07-08 17:47:39 - train: epoch 0089, iter [04200, 05004], lr: 0.015735, loss: 1.0510
2022-07-08 17:48:25 - train: epoch 0089, iter [04300, 05004], lr: 0.015716, loss: 1.1478
2022-07-08 17:49:10 - train: epoch 0089, iter [04400, 05004], lr: 0.015697, loss: 1.0643
2022-07-08 17:49:56 - train: epoch 0089, iter [04500, 05004], lr: 0.015678, loss: 0.9490
2022-07-08 17:50:41 - train: epoch 0089, iter [04600, 05004], lr: 0.015659, loss: 1.1262
2022-07-08 17:51:27 - train: epoch 0089, iter [04700, 05004], lr: 0.015640, loss: 1.1111
2022-07-08 17:52:12 - train: epoch 0089, iter [04800, 05004], lr: 0.015621, loss: 1.1031
2022-07-08 17:52:58 - train: epoch 0089, iter [04900, 05004], lr: 0.015602, loss: 1.0673
2022-07-08 17:53:43 - train: epoch 0089, iter [05000, 05004], lr: 0.015583, loss: 0.9171
2022-07-08 17:53:46 - train: epoch 089, train_loss: 1.1002
2022-07-08 17:55:02 - eval: epoch: 089, acc1: 73.928%, acc5: 92.022%, test_loss: 1.0450, per_image_load_time: 2.134ms, per_image_inference_time: 0.826ms
2022-07-08 17:55:04 - until epoch: 089, best_acc1: 73.928%
2022-07-08 17:55:04 - epoch 090 lr: 0.015582
2022-07-08 17:55:55 - train: epoch 0090, iter [00100, 05004], lr: 0.015563, loss: 1.1035
2022-07-08 17:56:40 - train: epoch 0090, iter [00200, 05004], lr: 0.015544, loss: 1.0632
2022-07-08 17:57:24 - train: epoch 0090, iter [00300, 05004], lr: 0.015525, loss: 0.9675
2022-07-08 17:58:09 - train: epoch 0090, iter [00400, 05004], lr: 0.015506, loss: 0.9438
2022-07-08 17:58:54 - train: epoch 0090, iter [00500, 05004], lr: 0.015488, loss: 1.0329
2022-07-08 17:59:39 - train: epoch 0090, iter [00600, 05004], lr: 0.015469, loss: 1.1956
2022-07-08 18:00:24 - train: epoch 0090, iter [00700, 05004], lr: 0.015450, loss: 1.1161
2022-07-08 18:01:08 - train: epoch 0090, iter [00800, 05004], lr: 0.015431, loss: 1.0145
2022-07-08 18:01:53 - train: epoch 0090, iter [00900, 05004], lr: 0.015412, loss: 0.9614
2022-07-08 18:02:38 - train: epoch 0090, iter [01000, 05004], lr: 0.015393, loss: 0.9885
2022-07-08 18:03:23 - train: epoch 0090, iter [01100, 05004], lr: 0.015374, loss: 1.2057
2022-07-08 18:04:08 - train: epoch 0090, iter [01200, 05004], lr: 0.015355, loss: 0.9997
2022-07-08 18:04:53 - train: epoch 0090, iter [01300, 05004], lr: 0.015336, loss: 1.2440
2022-07-08 18:05:37 - train: epoch 0090, iter [01400, 05004], lr: 0.015318, loss: 0.9272
2022-07-08 18:06:22 - train: epoch 0090, iter [01500, 05004], lr: 0.015299, loss: 1.3821
2022-07-08 18:07:08 - train: epoch 0090, iter [01600, 05004], lr: 0.015280, loss: 1.0454
2022-07-08 18:07:53 - train: epoch 0090, iter [01700, 05004], lr: 0.015261, loss: 0.9128
2022-07-08 18:08:38 - train: epoch 0090, iter [01800, 05004], lr: 0.015242, loss: 0.9601
2022-07-08 18:09:23 - train: epoch 0090, iter [01900, 05004], lr: 0.015223, loss: 0.9022
2022-07-08 18:10:08 - train: epoch 0090, iter [02000, 05004], lr: 0.015205, loss: 1.1877
2022-07-08 18:10:53 - train: epoch 0090, iter [02100, 05004], lr: 0.015186, loss: 1.2160
2022-07-08 18:11:38 - train: epoch 0090, iter [02200, 05004], lr: 0.015167, loss: 1.1162
2022-07-08 18:12:23 - train: epoch 0090, iter [02300, 05004], lr: 0.015148, loss: 1.2020
2022-07-08 18:13:08 - train: epoch 0090, iter [02400, 05004], lr: 0.015130, loss: 1.0872
2022-07-08 18:13:53 - train: epoch 0090, iter [02500, 05004], lr: 0.015111, loss: 1.1832
2022-07-08 18:14:38 - train: epoch 0090, iter [02600, 05004], lr: 0.015092, loss: 1.1456
2022-07-08 18:15:23 - train: epoch 0090, iter [02700, 05004], lr: 0.015073, loss: 1.0423
2022-07-08 18:16:08 - train: epoch 0090, iter [02800, 05004], lr: 0.015055, loss: 1.1220
2022-07-08 18:16:53 - train: epoch 0090, iter [02900, 05004], lr: 0.015036, loss: 1.0730
2022-07-08 18:17:39 - train: epoch 0090, iter [03000, 05004], lr: 0.015017, loss: 1.0782
2022-07-08 18:18:24 - train: epoch 0090, iter [03100, 05004], lr: 0.014999, loss: 0.9794
2022-07-08 18:19:09 - train: epoch 0090, iter [03200, 05004], lr: 0.014980, loss: 0.9458
2022-07-08 18:19:54 - train: epoch 0090, iter [03300, 05004], lr: 0.014961, loss: 1.2475
2022-07-08 18:20:39 - train: epoch 0090, iter [03400, 05004], lr: 0.014943, loss: 0.9059
2022-07-08 18:21:24 - train: epoch 0090, iter [03500, 05004], lr: 0.014924, loss: 1.0099
2022-07-08 18:22:10 - train: epoch 0090, iter [03600, 05004], lr: 0.014905, loss: 0.9470
2022-07-08 18:22:55 - train: epoch 0090, iter [03700, 05004], lr: 0.014887, loss: 1.0439
2022-07-08 18:23:40 - train: epoch 0090, iter [03800, 05004], lr: 0.014868, loss: 1.0331
2022-07-08 18:24:26 - train: epoch 0090, iter [03900, 05004], lr: 0.014849, loss: 0.9092
2022-07-08 18:25:11 - train: epoch 0090, iter [04000, 05004], lr: 0.014831, loss: 1.1297
2022-07-08 18:25:56 - train: epoch 0090, iter [04100, 05004], lr: 0.014812, loss: 1.3971
2022-07-08 18:26:41 - train: epoch 0090, iter [04200, 05004], lr: 0.014794, loss: 1.1939
2022-07-08 18:27:27 - train: epoch 0090, iter [04300, 05004], lr: 0.014775, loss: 1.1640
2022-07-08 18:28:12 - train: epoch 0090, iter [04400, 05004], lr: 0.014757, loss: 0.9958
2022-07-08 18:28:57 - train: epoch 0090, iter [04500, 05004], lr: 0.014738, loss: 0.9866
2022-07-08 18:29:42 - train: epoch 0090, iter [04600, 05004], lr: 0.014719, loss: 1.0498
2022-07-08 18:30:28 - train: epoch 0090, iter [04700, 05004], lr: 0.014701, loss: 1.0449
2022-07-08 18:31:13 - train: epoch 0090, iter [04800, 05004], lr: 0.014682, loss: 1.2276
2022-07-08 18:31:58 - train: epoch 0090, iter [04900, 05004], lr: 0.014664, loss: 1.0249
2022-07-08 18:32:43 - train: epoch 0090, iter [05000, 05004], lr: 0.014645, loss: 1.0998
2022-07-08 18:32:46 - train: epoch 090, train_loss: 1.0818
2022-07-08 18:34:01 - eval: epoch: 090, acc1: 73.320%, acc5: 91.680%, test_loss: 1.0699, per_image_load_time: 2.075ms, per_image_inference_time: 0.839ms
2022-07-08 18:34:02 - until epoch: 090, best_acc1: 73.928%
2022-07-08 18:34:02 - epoch 091 lr: 0.014644
2022-07-08 18:34:53 - train: epoch 0091, iter [00100, 05004], lr: 0.014626, loss: 0.9913
2022-07-08 18:35:38 - train: epoch 0091, iter [00200, 05004], lr: 0.014608, loss: 0.9700
2022-07-08 18:36:22 - train: epoch 0091, iter [00300, 05004], lr: 0.014589, loss: 1.0315
2022-07-08 18:37:07 - train: epoch 0091, iter [00400, 05004], lr: 0.014571, loss: 0.8509
2022-07-08 18:37:52 - train: epoch 0091, iter [00500, 05004], lr: 0.014552, loss: 1.0410
2022-07-08 18:38:37 - train: epoch 0091, iter [00600, 05004], lr: 0.014534, loss: 1.0146
2022-07-08 18:39:22 - train: epoch 0091, iter [00700, 05004], lr: 0.014515, loss: 0.9789
2022-07-08 18:40:07 - train: epoch 0091, iter [00800, 05004], lr: 0.014497, loss: 0.9398
2022-07-08 18:40:51 - train: epoch 0091, iter [00900, 05004], lr: 0.014479, loss: 1.1424
2022-07-08 18:41:36 - train: epoch 0091, iter [01000, 05004], lr: 0.014460, loss: 0.9259
2022-07-08 18:42:21 - train: epoch 0091, iter [01100, 05004], lr: 0.014442, loss: 0.9127
2022-07-08 18:43:06 - train: epoch 0091, iter [01200, 05004], lr: 0.014423, loss: 1.2077
2022-07-08 18:43:51 - train: epoch 0091, iter [01300, 05004], lr: 0.014405, loss: 0.9558
2022-07-08 18:44:36 - train: epoch 0091, iter [01400, 05004], lr: 0.014387, loss: 1.0196
2022-07-08 18:45:20 - train: epoch 0091, iter [01500, 05004], lr: 0.014368, loss: 1.0207
2022-07-08 18:46:05 - train: epoch 0091, iter [01600, 05004], lr: 0.014350, loss: 1.1041
2022-07-08 18:46:50 - train: epoch 0091, iter [01700, 05004], lr: 0.014332, loss: 1.0513
2022-07-08 18:47:35 - train: epoch 0091, iter [01800, 05004], lr: 0.014313, loss: 1.0860
2022-07-08 18:48:20 - train: epoch 0091, iter [01900, 05004], lr: 0.014295, loss: 0.8714
2022-07-08 18:49:05 - train: epoch 0091, iter [02000, 05004], lr: 0.014277, loss: 0.9982
2022-07-08 18:49:49 - train: epoch 0091, iter [02100, 05004], lr: 0.014258, loss: 0.8497
2022-07-08 18:50:34 - train: epoch 0091, iter [02200, 05004], lr: 0.014240, loss: 1.2058
2022-07-08 18:51:19 - train: epoch 0091, iter [02300, 05004], lr: 0.014222, loss: 1.1314
2022-07-08 18:52:04 - train: epoch 0091, iter [02400, 05004], lr: 0.014204, loss: 1.0319
2022-07-08 18:52:49 - train: epoch 0091, iter [02500, 05004], lr: 0.014185, loss: 1.0379
2022-07-08 18:53:34 - train: epoch 0091, iter [02600, 05004], lr: 0.014167, loss: 1.0862
2022-07-08 18:54:19 - train: epoch 0091, iter [02700, 05004], lr: 0.014149, loss: 0.8917
2022-07-08 18:55:03 - train: epoch 0091, iter [02800, 05004], lr: 0.014131, loss: 1.0737
2022-07-08 18:55:48 - train: epoch 0091, iter [02900, 05004], lr: 0.014112, loss: 1.2319
2022-07-08 18:56:33 - train: epoch 0091, iter [03000, 05004], lr: 0.014094, loss: 1.1711
2022-07-08 18:57:17 - train: epoch 0091, iter [03100, 05004], lr: 0.014076, loss: 0.9586
2022-07-08 18:58:02 - train: epoch 0091, iter [03200, 05004], lr: 0.014058, loss: 1.2412
2022-07-08 18:58:47 - train: epoch 0091, iter [03300, 05004], lr: 0.014040, loss: 0.9331
2022-07-08 18:59:32 - train: epoch 0091, iter [03400, 05004], lr: 0.014021, loss: 0.9883
2022-07-08 19:00:17 - train: epoch 0091, iter [03500, 05004], lr: 0.014003, loss: 1.0847
2022-07-08 19:01:02 - train: epoch 0091, iter [03600, 05004], lr: 0.013985, loss: 1.0504
2022-07-08 19:01:46 - train: epoch 0091, iter [03700, 05004], lr: 0.013967, loss: 1.1367
2022-07-08 19:02:31 - train: epoch 0091, iter [03800, 05004], lr: 0.013949, loss: 1.0665
2022-07-08 19:03:16 - train: epoch 0091, iter [03900, 05004], lr: 0.013931, loss: 1.0845
2022-07-08 19:04:01 - train: epoch 0091, iter [04000, 05004], lr: 0.013913, loss: 1.1191
2022-07-08 19:04:46 - train: epoch 0091, iter [04100, 05004], lr: 0.013894, loss: 1.1017
2022-07-08 19:05:31 - train: epoch 0091, iter [04200, 05004], lr: 0.013876, loss: 1.1201
2022-07-08 19:06:16 - train: epoch 0091, iter [04300, 05004], lr: 0.013858, loss: 1.1763
2022-07-08 19:07:01 - train: epoch 0091, iter [04400, 05004], lr: 0.013840, loss: 0.9692
2022-07-08 19:07:46 - train: epoch 0091, iter [04500, 05004], lr: 0.013822, loss: 1.0080
2022-07-08 19:08:31 - train: epoch 0091, iter [04600, 05004], lr: 0.013804, loss: 1.0406
2022-07-08 19:09:16 - train: epoch 0091, iter [04700, 05004], lr: 0.013786, loss: 1.1751
2022-07-08 19:10:01 - train: epoch 0091, iter [04800, 05004], lr: 0.013768, loss: 1.0735
2022-07-08 19:10:46 - train: epoch 0091, iter [04900, 05004], lr: 0.013750, loss: 1.0675
2022-07-08 19:11:31 - train: epoch 0091, iter [05000, 05004], lr: 0.013732, loss: 1.3045
2022-07-08 19:11:34 - train: epoch 091, train_loss: 1.0621
2022-07-08 19:12:51 - eval: epoch: 091, acc1: 74.272%, acc5: 92.192%, test_loss: 1.0391, per_image_load_time: 2.209ms, per_image_inference_time: 0.823ms
2022-07-08 19:12:53 - until epoch: 091, best_acc1: 74.272%
2022-07-08 19:12:53 - epoch 092 lr: 0.013731
2022-07-08 19:13:43 - train: epoch 0092, iter [00100, 05004], lr: 0.013713, loss: 1.0022
2022-07-08 19:14:28 - train: epoch 0092, iter [00200, 05004], lr: 0.013695, loss: 1.1267
2022-07-08 19:15:13 - train: epoch 0092, iter [00300, 05004], lr: 0.013677, loss: 0.9331
2022-07-08 19:15:58 - train: epoch 0092, iter [00400, 05004], lr: 0.013659, loss: 0.8556
2022-07-08 19:16:43 - train: epoch 0092, iter [00500, 05004], lr: 0.013641, loss: 1.1050
2022-07-08 19:17:29 - train: epoch 0092, iter [00600, 05004], lr: 0.013623, loss: 1.1198
2022-07-08 19:18:14 - train: epoch 0092, iter [00700, 05004], lr: 0.013605, loss: 1.0715
2022-07-08 19:18:59 - train: epoch 0092, iter [00800, 05004], lr: 0.013588, loss: 1.1798
2022-07-08 19:19:44 - train: epoch 0092, iter [00900, 05004], lr: 0.013570, loss: 0.9450
2022-07-08 19:20:29 - train: epoch 0092, iter [01000, 05004], lr: 0.013552, loss: 1.2575
2022-07-08 19:21:14 - train: epoch 0092, iter [01100, 05004], lr: 0.013534, loss: 0.9132
2022-07-08 19:21:59 - train: epoch 0092, iter [01200, 05004], lr: 0.013516, loss: 0.8996
2022-07-08 19:22:44 - train: epoch 0092, iter [01300, 05004], lr: 0.013498, loss: 1.0534
2022-07-08 19:23:29 - train: epoch 0092, iter [01400, 05004], lr: 0.013480, loss: 1.0345
2022-07-08 19:24:14 - train: epoch 0092, iter [01500, 05004], lr: 0.013462, loss: 0.9336
2022-07-08 19:24:59 - train: epoch 0092, iter [01600, 05004], lr: 0.013444, loss: 1.0904
2022-07-08 19:25:44 - train: epoch 0092, iter [01700, 05004], lr: 0.013427, loss: 1.0635
2022-07-08 19:26:29 - train: epoch 0092, iter [01800, 05004], lr: 0.013409, loss: 0.9580
2022-07-08 19:27:14 - train: epoch 0092, iter [01900, 05004], lr: 0.013391, loss: 0.9523
2022-07-08 19:27:59 - train: epoch 0092, iter [02000, 05004], lr: 0.013373, loss: 0.9169
2022-07-08 19:28:44 - train: epoch 0092, iter [02100, 05004], lr: 0.013355, loss: 1.0299
2022-07-08 19:29:29 - train: epoch 0092, iter [02200, 05004], lr: 0.013338, loss: 1.1053
2022-07-08 19:30:14 - train: epoch 0092, iter [02300, 05004], lr: 0.013320, loss: 1.1038
2022-07-08 19:30:59 - train: epoch 0092, iter [02400, 05004], lr: 0.013302, loss: 0.9429
2022-07-08 19:31:44 - train: epoch 0092, iter [02500, 05004], lr: 0.013284, loss: 1.0274
2022-07-08 19:32:30 - train: epoch 0092, iter [02600, 05004], lr: 0.013266, loss: 1.1780
2022-07-08 19:33:15 - train: epoch 0092, iter [02700, 05004], lr: 0.013249, loss: 0.9712
2022-07-08 19:33:59 - train: epoch 0092, iter [02800, 05004], lr: 0.013231, loss: 0.9793
2022-07-08 19:34:44 - train: epoch 0092, iter [02900, 05004], lr: 0.013213, loss: 1.0656
2022-07-08 19:35:29 - train: epoch 0092, iter [03000, 05004], lr: 0.013196, loss: 0.9244
2022-07-08 19:36:14 - train: epoch 0092, iter [03100, 05004], lr: 0.013178, loss: 1.0083
2022-07-08 19:36:59 - train: epoch 0092, iter [03200, 05004], lr: 0.013160, loss: 0.9269
2022-07-08 19:37:44 - train: epoch 0092, iter [03300, 05004], lr: 0.013142, loss: 1.0945
2022-07-08 19:38:29 - train: epoch 0092, iter [03400, 05004], lr: 0.013125, loss: 1.2707
2022-07-08 19:39:14 - train: epoch 0092, iter [03500, 05004], lr: 0.013107, loss: 0.8331
2022-07-08 19:39:59 - train: epoch 0092, iter [03600, 05004], lr: 0.013090, loss: 1.0332
2022-07-08 19:40:44 - train: epoch 0092, iter [03700, 05004], lr: 0.013072, loss: 0.8499
2022-07-08 19:41:29 - train: epoch 0092, iter [03800, 05004], lr: 0.013054, loss: 1.3208
2022-07-08 19:42:14 - train: epoch 0092, iter [03900, 05004], lr: 0.013037, loss: 1.1137
2022-07-08 19:42:59 - train: epoch 0092, iter [04000, 05004], lr: 0.013019, loss: 1.1713
2022-07-08 19:43:43 - train: epoch 0092, iter [04100, 05004], lr: 0.013001, loss: 0.9551
2022-07-08 19:44:29 - train: epoch 0092, iter [04200, 05004], lr: 0.012984, loss: 1.1232
2022-07-08 19:45:14 - train: epoch 0092, iter [04300, 05004], lr: 0.012966, loss: 1.0447
2022-07-08 19:45:59 - train: epoch 0092, iter [04400, 05004], lr: 0.012949, loss: 0.8920
2022-07-08 19:46:44 - train: epoch 0092, iter [04500, 05004], lr: 0.012931, loss: 1.0255
2022-07-08 19:47:30 - train: epoch 0092, iter [04600, 05004], lr: 0.012914, loss: 0.9702
2022-07-08 19:48:15 - train: epoch 0092, iter [04700, 05004], lr: 0.012896, loss: 0.7874
2022-07-08 19:49:00 - train: epoch 0092, iter [04800, 05004], lr: 0.012878, loss: 0.9217
2022-07-08 19:49:46 - train: epoch 0092, iter [04900, 05004], lr: 0.012861, loss: 1.0671
2022-07-08 19:50:31 - train: epoch 0092, iter [05000, 05004], lr: 0.012843, loss: 1.1847
2022-07-08 19:50:33 - train: epoch 092, train_loss: 1.0420
2022-07-08 19:51:50 - eval: epoch: 092, acc1: 74.440%, acc5: 92.264%, test_loss: 1.0366, per_image_load_time: 2.142ms, per_image_inference_time: 0.824ms
2022-07-08 19:51:51 - until epoch: 092, best_acc1: 74.440%
2022-07-08 19:51:51 - epoch 093 lr: 0.012843
2022-07-08 19:52:42 - train: epoch 0093, iter [00100, 05004], lr: 0.012825, loss: 0.8499
2022-07-08 19:53:27 - train: epoch 0093, iter [00200, 05004], lr: 0.012808, loss: 0.9063
2022-07-08 19:54:12 - train: epoch 0093, iter [00300, 05004], lr: 0.012790, loss: 1.0104
2022-07-08 19:54:57 - train: epoch 0093, iter [00400, 05004], lr: 0.012773, loss: 1.0258
2022-07-08 19:55:43 - train: epoch 0093, iter [00500, 05004], lr: 0.012755, loss: 1.1031
2022-07-08 19:56:28 - train: epoch 0093, iter [00600, 05004], lr: 0.012738, loss: 0.9902
2022-07-08 19:57:14 - train: epoch 0093, iter [00700, 05004], lr: 0.012720, loss: 1.0068
2022-07-08 19:57:59 - train: epoch 0093, iter [00800, 05004], lr: 0.012703, loss: 0.9045
2022-07-08 19:58:44 - train: epoch 0093, iter [00900, 05004], lr: 0.012686, loss: 0.9044
2022-07-08 19:59:30 - train: epoch 0093, iter [01000, 05004], lr: 0.012668, loss: 0.9200
2022-07-08 20:00:15 - train: epoch 0093, iter [01100, 05004], lr: 0.012651, loss: 0.8980
2022-07-08 20:01:01 - train: epoch 0093, iter [01200, 05004], lr: 0.012633, loss: 0.8424
2022-07-08 20:01:47 - train: epoch 0093, iter [01300, 05004], lr: 0.012616, loss: 1.1024
2022-07-08 20:02:32 - train: epoch 0093, iter [01400, 05004], lr: 0.012599, loss: 1.0175
2022-07-08 20:03:18 - train: epoch 0093, iter [01500, 05004], lr: 0.012581, loss: 1.2430
2022-07-08 20:04:03 - train: epoch 0093, iter [01600, 05004], lr: 0.012564, loss: 1.0781
2022-07-08 20:04:48 - train: epoch 0093, iter [01700, 05004], lr: 0.012547, loss: 0.9523
2022-07-08 20:05:34 - train: epoch 0093, iter [01800, 05004], lr: 0.012529, loss: 1.0019
2022-07-08 20:06:19 - train: epoch 0093, iter [01900, 05004], lr: 0.012512, loss: 0.8997
2022-07-08 20:07:05 - train: epoch 0093, iter [02000, 05004], lr: 0.012495, loss: 0.9364
2022-07-08 20:07:50 - train: epoch 0093, iter [02100, 05004], lr: 0.012477, loss: 0.8889
2022-07-08 20:08:35 - train: epoch 0093, iter [02200, 05004], lr: 0.012460, loss: 1.1729
2022-07-08 20:09:20 - train: epoch 0093, iter [02300, 05004], lr: 0.012443, loss: 1.0739
2022-07-08 20:10:06 - train: epoch 0093, iter [02400, 05004], lr: 0.012426, loss: 0.9304
2022-07-08 20:10:51 - train: epoch 0093, iter [02500, 05004], lr: 0.012408, loss: 0.9658
2022-07-08 20:11:36 - train: epoch 0093, iter [02600, 05004], lr: 0.012391, loss: 1.1968
2022-07-08 20:12:22 - train: epoch 0093, iter [02700, 05004], lr: 0.012374, loss: 0.9031
2022-07-08 20:13:07 - train: epoch 0093, iter [02800, 05004], lr: 0.012357, loss: 0.9461
2022-07-08 20:13:52 - train: epoch 0093, iter [02900, 05004], lr: 0.012339, loss: 1.0173
2022-07-08 20:14:38 - train: epoch 0093, iter [03000, 05004], lr: 0.012322, loss: 1.0199
2022-07-08 20:15:23 - train: epoch 0093, iter [03100, 05004], lr: 0.012305, loss: 1.2661
2022-07-08 20:16:08 - train: epoch 0093, iter [03200, 05004], lr: 0.012288, loss: 0.8987
2022-07-08 20:16:53 - train: epoch 0093, iter [03300, 05004], lr: 0.012271, loss: 1.0388
2022-07-08 20:17:38 - train: epoch 0093, iter [03400, 05004], lr: 0.012254, loss: 1.2404
2022-07-08 20:18:24 - train: epoch 0093, iter [03500, 05004], lr: 0.012236, loss: 0.8083
2022-07-08 20:19:09 - train: epoch 0093, iter [03600, 05004], lr: 0.012219, loss: 1.0666
2022-07-08 20:19:54 - train: epoch 0093, iter [03700, 05004], lr: 0.012202, loss: 0.9681
2022-07-08 20:20:40 - train: epoch 0093, iter [03800, 05004], lr: 0.012185, loss: 0.8484
2022-07-08 20:21:25 - train: epoch 0093, iter [03900, 05004], lr: 0.012168, loss: 1.0742
2022-07-08 20:22:10 - train: epoch 0093, iter [04000, 05004], lr: 0.012151, loss: 1.2911
2022-07-08 20:22:55 - train: epoch 0093, iter [04100, 05004], lr: 0.012134, loss: 1.1619
2022-07-08 20:23:41 - train: epoch 0093, iter [04200, 05004], lr: 0.012117, loss: 1.0631
2022-07-08 20:24:26 - train: epoch 0093, iter [04300, 05004], lr: 0.012100, loss: 1.2014
2022-07-08 20:25:11 - train: epoch 0093, iter [04400, 05004], lr: 0.012083, loss: 0.9623
2022-07-08 20:25:56 - train: epoch 0093, iter [04500, 05004], lr: 0.012065, loss: 1.0132
2022-07-08 20:26:42 - train: epoch 0093, iter [04600, 05004], lr: 0.012048, loss: 0.8561
2022-07-08 20:27:27 - train: epoch 0093, iter [04700, 05004], lr: 0.012031, loss: 1.2410
2022-07-08 20:28:12 - train: epoch 0093, iter [04800, 05004], lr: 0.012014, loss: 1.0401
2022-07-08 20:28:57 - train: epoch 0093, iter [04900, 05004], lr: 0.011997, loss: 1.2493
2022-07-08 20:29:43 - train: epoch 0093, iter [05000, 05004], lr: 0.011980, loss: 0.9618
2022-07-08 20:29:45 - train: epoch 093, train_loss: 1.0225
2022-07-08 20:31:03 - eval: epoch: 093, acc1: 74.736%, acc5: 92.340%, test_loss: 1.0273, per_image_load_time: 1.738ms, per_image_inference_time: 0.823ms
2022-07-08 20:31:04 - until epoch: 093, best_acc1: 74.736%
2022-07-08 20:31:04 - epoch 094 lr: 0.011980
2022-07-08 20:31:56 - train: epoch 0094, iter [00100, 05004], lr: 0.011963, loss: 0.9983
2022-07-08 20:32:41 - train: epoch 0094, iter [00200, 05004], lr: 0.011946, loss: 0.9685
2022-07-08 20:33:26 - train: epoch 0094, iter [00300, 05004], lr: 0.011929, loss: 1.0691
2022-07-08 20:34:11 - train: epoch 0094, iter [00400, 05004], lr: 0.011912, loss: 1.1472
2022-07-08 20:34:57 - train: epoch 0094, iter [00500, 05004], lr: 0.011895, loss: 0.9148
2022-07-08 20:35:42 - train: epoch 0094, iter [00600, 05004], lr: 0.011878, loss: 0.8005
2022-07-08 20:36:27 - train: epoch 0094, iter [00700, 05004], lr: 0.011861, loss: 1.1318
2022-07-08 20:37:13 - train: epoch 0094, iter [00800, 05004], lr: 0.011844, loss: 0.9204
2022-07-08 20:37:58 - train: epoch 0094, iter [00900, 05004], lr: 0.011827, loss: 0.9237
2022-07-08 20:38:43 - train: epoch 0094, iter [01000, 05004], lr: 0.011810, loss: 1.0201
2022-07-08 20:39:28 - train: epoch 0094, iter [01100, 05004], lr: 0.011793, loss: 1.1346
2022-07-08 20:40:13 - train: epoch 0094, iter [01200, 05004], lr: 0.011777, loss: 0.9467
2022-07-08 20:40:58 - train: epoch 0094, iter [01300, 05004], lr: 0.011760, loss: 1.0771
2022-07-08 20:41:43 - train: epoch 0094, iter [01400, 05004], lr: 0.011743, loss: 0.9694
2022-07-08 20:42:28 - train: epoch 0094, iter [01500, 05004], lr: 0.011726, loss: 1.1560
2022-07-08 20:43:13 - train: epoch 0094, iter [01600, 05004], lr: 0.011709, loss: 1.3908
2022-07-08 20:43:58 - train: epoch 0094, iter [01700, 05004], lr: 0.011692, loss: 0.9430
2022-07-08 20:44:43 - train: epoch 0094, iter [01800, 05004], lr: 0.011676, loss: 0.8575
2022-07-08 20:45:28 - train: epoch 0094, iter [01900, 05004], lr: 0.011659, loss: 0.9788
2022-07-08 20:46:13 - train: epoch 0094, iter [02000, 05004], lr: 0.011642, loss: 0.8803
2022-07-08 20:46:59 - train: epoch 0094, iter [02100, 05004], lr: 0.011625, loss: 1.0389
2022-07-08 20:47:44 - train: epoch 0094, iter [02200, 05004], lr: 0.011608, loss: 0.8558
2022-07-08 20:48:29 - train: epoch 0094, iter [02300, 05004], lr: 0.011592, loss: 0.7479
2022-07-08 20:49:14 - train: epoch 0094, iter [02400, 05004], lr: 0.011575, loss: 1.0334
2022-07-08 20:49:59 - train: epoch 0094, iter [02500, 05004], lr: 0.011558, loss: 0.9299
2022-07-08 20:50:44 - train: epoch 0094, iter [02600, 05004], lr: 0.011542, loss: 0.9055
2022-07-08 20:51:29 - train: epoch 0094, iter [02700, 05004], lr: 0.011525, loss: 0.9592
2022-07-08 20:52:14 - train: epoch 0094, iter [02800, 05004], lr: 0.011508, loss: 1.0368
2022-07-08 20:52:59 - train: epoch 0094, iter [02900, 05004], lr: 0.011491, loss: 1.0194
2022-07-08 20:53:44 - train: epoch 0094, iter [03000, 05004], lr: 0.011475, loss: 1.0013
2022-07-08 20:54:30 - train: epoch 0094, iter [03100, 05004], lr: 0.011458, loss: 1.1828
2022-07-08 20:55:15 - train: epoch 0094, iter [03200, 05004], lr: 0.011441, loss: 0.9528
2022-07-08 20:56:00 - train: epoch 0094, iter [03300, 05004], lr: 0.011425, loss: 1.0144
2022-07-08 20:56:45 - train: epoch 0094, iter [03400, 05004], lr: 0.011408, loss: 1.0885
2022-07-08 20:57:30 - train: epoch 0094, iter [03500, 05004], lr: 0.011391, loss: 1.2520
2022-07-08 20:58:15 - train: epoch 0094, iter [03600, 05004], lr: 0.011375, loss: 1.0176
2022-07-08 20:59:00 - train: epoch 0094, iter [03700, 05004], lr: 0.011358, loss: 1.0964
2022-07-08 20:59:46 - train: epoch 0094, iter [03800, 05004], lr: 0.011342, loss: 0.9652
2022-07-08 21:00:31 - train: epoch 0094, iter [03900, 05004], lr: 0.011325, loss: 0.9363
2022-07-08 21:01:16 - train: epoch 0094, iter [04000, 05004], lr: 0.011309, loss: 1.0568
2022-07-08 21:02:01 - train: epoch 0094, iter [04100, 05004], lr: 0.011292, loss: 1.2999
2022-07-08 21:02:46 - train: epoch 0094, iter [04200, 05004], lr: 0.011275, loss: 0.7624
2022-07-08 21:03:32 - train: epoch 0094, iter [04300, 05004], lr: 0.011259, loss: 1.1261
2022-07-08 21:04:17 - train: epoch 0094, iter [04400, 05004], lr: 0.011242, loss: 1.0642
2022-07-08 21:05:02 - train: epoch 0094, iter [04500, 05004], lr: 0.011226, loss: 1.0527
2022-07-08 21:05:47 - train: epoch 0094, iter [04600, 05004], lr: 0.011209, loss: 1.0208
2022-07-08 21:06:33 - train: epoch 0094, iter [04700, 05004], lr: 0.011193, loss: 1.0668
2022-07-08 21:07:18 - train: epoch 0094, iter [04800, 05004], lr: 0.011176, loss: 1.1062
2022-07-08 21:08:03 - train: epoch 0094, iter [04900, 05004], lr: 0.011160, loss: 1.0113
2022-07-08 21:08:48 - train: epoch 0094, iter [05000, 05004], lr: 0.011143, loss: 0.8840
2022-07-08 21:08:51 - train: epoch 094, train_loss: 1.0057
2022-07-08 21:10:09 - eval: epoch: 094, acc1: 74.526%, acc5: 92.328%, test_loss: 1.0210, per_image_load_time: 2.200ms, per_image_inference_time: 0.820ms
2022-07-08 21:10:09 - until epoch: 094, best_acc1: 74.736%
2022-07-08 21:10:09 - epoch 095 lr: 0.011143
2022-07-08 21:11:01 - train: epoch 0095, iter [00100, 05004], lr: 0.011126, loss: 0.7689
2022-07-08 21:11:46 - train: epoch 0095, iter [00200, 05004], lr: 0.011110, loss: 1.0585
2022-07-08 21:12:31 - train: epoch 0095, iter [00300, 05004], lr: 0.011093, loss: 0.8700
2022-07-08 21:13:16 - train: epoch 0095, iter [00400, 05004], lr: 0.011077, loss: 1.0259
2022-07-08 21:14:02 - train: epoch 0095, iter [00500, 05004], lr: 0.011061, loss: 0.9866
2022-07-08 21:14:47 - train: epoch 0095, iter [00600, 05004], lr: 0.011044, loss: 0.9510
2022-07-08 21:15:32 - train: epoch 0095, iter [00700, 05004], lr: 0.011028, loss: 1.0856
2022-07-08 21:16:17 - train: epoch 0095, iter [00800, 05004], lr: 0.011011, loss: 1.1404
2022-07-08 21:17:03 - train: epoch 0095, iter [00900, 05004], lr: 0.010995, loss: 1.1219
2022-07-08 21:17:48 - train: epoch 0095, iter [01000, 05004], lr: 0.010979, loss: 1.0655
2022-07-08 21:18:33 - train: epoch 0095, iter [01100, 05004], lr: 0.010962, loss: 0.8226
2022-07-08 21:19:19 - train: epoch 0095, iter [01200, 05004], lr: 0.010946, loss: 0.8768
2022-07-08 21:20:04 - train: epoch 0095, iter [01300, 05004], lr: 0.010930, loss: 0.9650
2022-07-08 21:20:49 - train: epoch 0095, iter [01400, 05004], lr: 0.010913, loss: 0.9356
2022-07-08 21:21:34 - train: epoch 0095, iter [01500, 05004], lr: 0.010897, loss: 0.9932
2022-07-08 21:22:19 - train: epoch 0095, iter [01600, 05004], lr: 0.010881, loss: 0.7101
2022-07-08 21:23:05 - train: epoch 0095, iter [01700, 05004], lr: 0.010864, loss: 1.0582
2022-07-08 21:23:50 - train: epoch 0095, iter [01800, 05004], lr: 0.010848, loss: 1.0223
2022-07-08 21:24:35 - train: epoch 0095, iter [01900, 05004], lr: 0.010832, loss: 0.9300
2022-07-08 21:25:20 - train: epoch 0095, iter [02000, 05004], lr: 0.010816, loss: 1.0229
2022-07-08 21:26:05 - train: epoch 0095, iter [02100, 05004], lr: 0.010799, loss: 0.9581
2022-07-08 21:26:51 - train: epoch 0095, iter [02200, 05004], lr: 0.010783, loss: 0.7310
2022-07-08 21:27:36 - train: epoch 0095, iter [02300, 05004], lr: 0.010767, loss: 0.8961
2022-07-08 21:28:21 - train: epoch 0095, iter [02400, 05004], lr: 0.010751, loss: 1.0649
2022-07-08 21:29:06 - train: epoch 0095, iter [02500, 05004], lr: 0.010734, loss: 0.8766
2022-07-08 21:29:51 - train: epoch 0095, iter [02600, 05004], lr: 0.010718, loss: 0.9470
2022-07-08 21:30:37 - train: epoch 0095, iter [02700, 05004], lr: 0.010702, loss: 1.0379
2022-07-08 21:31:22 - train: epoch 0095, iter [02800, 05004], lr: 0.010686, loss: 1.0041
2022-07-08 21:32:07 - train: epoch 0095, iter [02900, 05004], lr: 0.010670, loss: 0.9089
2022-07-08 21:32:52 - train: epoch 0095, iter [03000, 05004], lr: 0.010654, loss: 1.2070
2022-07-08 21:33:38 - train: epoch 0095, iter [03100, 05004], lr: 0.010638, loss: 1.1598
2022-07-08 21:34:23 - train: epoch 0095, iter [03200, 05004], lr: 0.010621, loss: 1.0370
2022-07-08 21:35:08 - train: epoch 0095, iter [03300, 05004], lr: 0.010605, loss: 1.0552
2022-07-08 21:35:54 - train: epoch 0095, iter [03400, 05004], lr: 0.010589, loss: 0.9312
2022-07-08 21:36:39 - train: epoch 0095, iter [03500, 05004], lr: 0.010573, loss: 1.0455
2022-07-08 21:37:24 - train: epoch 0095, iter [03600, 05004], lr: 0.010557, loss: 0.9586
2022-07-08 21:38:10 - train: epoch 0095, iter [03700, 05004], lr: 0.010541, loss: 0.9570
2022-07-08 21:38:55 - train: epoch 0095, iter [03800, 05004], lr: 0.010525, loss: 0.8360
2022-07-08 21:39:40 - train: epoch 0095, iter [03900, 05004], lr: 0.010509, loss: 1.1133
2022-07-08 21:40:26 - train: epoch 0095, iter [04000, 05004], lr: 0.010493, loss: 0.8568
2022-07-08 21:41:11 - train: epoch 0095, iter [04100, 05004], lr: 0.010477, loss: 1.0306
2022-07-08 21:41:56 - train: epoch 0095, iter [04200, 05004], lr: 0.010461, loss: 0.8388
2022-07-08 21:42:41 - train: epoch 0095, iter [04300, 05004], lr: 0.010445, loss: 1.0678
2022-07-08 21:43:26 - train: epoch 0095, iter [04400, 05004], lr: 0.010429, loss: 1.0908
2022-07-08 21:44:11 - train: epoch 0095, iter [04500, 05004], lr: 0.010413, loss: 0.8661
2022-07-08 21:44:56 - train: epoch 0095, iter [04600, 05004], lr: 0.010397, loss: 0.9975
2022-07-08 21:45:41 - train: epoch 0095, iter [04700, 05004], lr: 0.010381, loss: 0.9657
2022-07-08 21:46:26 - train: epoch 0095, iter [04800, 05004], lr: 0.010365, loss: 1.1036
2022-07-08 21:47:11 - train: epoch 0095, iter [04900, 05004], lr: 0.010349, loss: 0.9369
2022-07-08 21:47:57 - train: epoch 0095, iter [05000, 05004], lr: 0.010333, loss: 0.8272
2022-07-08 21:47:59 - train: epoch 095, train_loss: 0.9848
2022-07-08 21:49:16 - eval: epoch: 095, acc1: 74.908%, acc5: 92.546%, test_loss: 1.0066, per_image_load_time: 2.086ms, per_image_inference_time: 0.842ms
2022-07-08 21:49:17 - until epoch: 095, best_acc1: 74.908%
2022-07-08 21:49:17 - epoch 096 lr: 0.010332
2022-07-08 21:50:09 - train: epoch 0096, iter [00100, 05004], lr: 0.010316, loss: 0.9359
2022-07-08 21:50:54 - train: epoch 0096, iter [00200, 05004], lr: 0.010301, loss: 0.9880
2022-07-08 21:51:39 - train: epoch 0096, iter [00300, 05004], lr: 0.010285, loss: 1.0572
2022-07-08 21:52:24 - train: epoch 0096, iter [00400, 05004], lr: 0.010269, loss: 0.8233
2022-07-08 21:53:10 - train: epoch 0096, iter [00500, 05004], lr: 0.010253, loss: 0.8312
2022-07-08 21:53:55 - train: epoch 0096, iter [00600, 05004], lr: 0.010237, loss: 1.0454
2022-07-08 21:54:40 - train: epoch 0096, iter [00700, 05004], lr: 0.010221, loss: 0.7704
2022-07-08 21:55:25 - train: epoch 0096, iter [00800, 05004], lr: 0.010205, loss: 0.8384
2022-07-08 21:56:11 - train: epoch 0096, iter [00900, 05004], lr: 0.010189, loss: 0.9096
2022-07-08 21:56:56 - train: epoch 0096, iter [01000, 05004], lr: 0.010174, loss: 0.9125
2022-07-08 21:57:41 - train: epoch 0096, iter [01100, 05004], lr: 0.010158, loss: 0.9852
2022-07-08 21:58:27 - train: epoch 0096, iter [01200, 05004], lr: 0.010142, loss: 1.0336
2022-07-08 21:59:12 - train: epoch 0096, iter [01300, 05004], lr: 0.010126, loss: 0.9643
2022-07-08 21:59:57 - train: epoch 0096, iter [01400, 05004], lr: 0.010110, loss: 0.8835
2022-07-08 22:00:42 - train: epoch 0096, iter [01500, 05004], lr: 0.010095, loss: 0.9024
2022-07-08 22:01:27 - train: epoch 0096, iter [01600, 05004], lr: 0.010079, loss: 0.7176
2022-07-08 22:02:12 - train: epoch 0096, iter [01700, 05004], lr: 0.010063, loss: 1.0037
2022-07-08 22:02:57 - train: epoch 0096, iter [01800, 05004], lr: 0.010047, loss: 1.0901
2022-07-08 22:03:42 - train: epoch 0096, iter [01900, 05004], lr: 0.010032, loss: 1.0508
2022-07-08 22:04:28 - train: epoch 0096, iter [02000, 05004], lr: 0.010016, loss: 0.8686
2022-07-08 22:05:13 - train: epoch 0096, iter [02100, 05004], lr: 0.010000, loss: 0.8760
2022-07-08 22:05:58 - train: epoch 0096, iter [02200, 05004], lr: 0.009985, loss: 0.8085
2022-07-08 22:06:43 - train: epoch 0096, iter [02300, 05004], lr: 0.009969, loss: 0.8418
2022-07-08 22:07:29 - train: epoch 0096, iter [02400, 05004], lr: 0.009953, loss: 0.7807
2022-07-08 22:08:14 - train: epoch 0096, iter [02500, 05004], lr: 0.009938, loss: 0.9285
2022-07-08 22:08:59 - train: epoch 0096, iter [02600, 05004], lr: 0.009922, loss: 0.8430
2022-07-08 22:09:44 - train: epoch 0096, iter [02700, 05004], lr: 0.009906, loss: 0.9588
2022-07-08 22:10:30 - train: epoch 0096, iter [02800, 05004], lr: 0.009891, loss: 1.1544
2022-07-08 22:11:15 - train: epoch 0096, iter [02900, 05004], lr: 0.009875, loss: 0.8695
2022-07-08 22:12:00 - train: epoch 0096, iter [03000, 05004], lr: 0.009860, loss: 1.0009
2022-07-08 22:12:45 - train: epoch 0096, iter [03100, 05004], lr: 0.009844, loss: 1.0052
2022-07-08 22:13:31 - train: epoch 0096, iter [03200, 05004], lr: 0.009828, loss: 1.0064
2022-07-08 22:14:16 - train: epoch 0096, iter [03300, 05004], lr: 0.009813, loss: 1.1338
2022-07-08 22:15:01 - train: epoch 0096, iter [03400, 05004], lr: 0.009797, loss: 0.7388
2022-07-08 22:15:46 - train: epoch 0096, iter [03500, 05004], lr: 0.009782, loss: 0.8729
2022-07-08 22:16:32 - train: epoch 0096, iter [03600, 05004], lr: 0.009766, loss: 0.8327
2022-07-08 22:17:17 - train: epoch 0096, iter [03700, 05004], lr: 0.009751, loss: 1.0168
2022-07-08 22:18:02 - train: epoch 0096, iter [03800, 05004], lr: 0.009735, loss: 0.9657
2022-07-08 22:18:48 - train: epoch 0096, iter [03900, 05004], lr: 0.009720, loss: 0.9526
2022-07-08 22:19:33 - train: epoch 0096, iter [04000, 05004], lr: 0.009704, loss: 0.7734
2022-07-08 22:20:18 - train: epoch 0096, iter [04100, 05004], lr: 0.009689, loss: 0.9308
2022-07-08 22:21:03 - train: epoch 0096, iter [04200, 05004], lr: 0.009673, loss: 0.9757
2022-07-08 22:21:49 - train: epoch 0096, iter [04300, 05004], lr: 0.009658, loss: 0.6929
2022-07-08 22:22:34 - train: epoch 0096, iter [04400, 05004], lr: 0.009642, loss: 0.9036
2022-07-08 22:23:19 - train: epoch 0096, iter [04500, 05004], lr: 0.009627, loss: 0.9170
2022-07-08 22:24:05 - train: epoch 0096, iter [04600, 05004], lr: 0.009611, loss: 0.9311
2022-07-08 22:24:50 - train: epoch 0096, iter [04700, 05004], lr: 0.009596, loss: 0.9358
2022-07-08 22:25:35 - train: epoch 0096, iter [04800, 05004], lr: 0.009581, loss: 0.9921
2022-07-08 22:26:21 - train: epoch 0096, iter [04900, 05004], lr: 0.009565, loss: 1.1651
2022-07-08 22:27:06 - train: epoch 0096, iter [05000, 05004], lr: 0.009550, loss: 0.9993
2022-07-08 22:27:08 - train: epoch 096, train_loss: 0.9613
2022-07-08 22:28:26 - eval: epoch: 096, acc1: 74.962%, acc5: 92.634%, test_loss: 0.9977, per_image_load_time: 2.120ms, per_image_inference_time: 0.840ms
2022-07-08 22:28:27 - until epoch: 096, best_acc1: 74.962%
2022-07-08 22:28:27 - epoch 097 lr: 0.009549
2022-07-08 22:29:18 - train: epoch 0097, iter [00100, 05004], lr: 0.009534, loss: 0.9998
2022-07-08 22:30:04 - train: epoch 0097, iter [00200, 05004], lr: 0.009518, loss: 0.7352
2022-07-08 22:30:49 - train: epoch 0097, iter [00300, 05004], lr: 0.009503, loss: 1.0502
2022-07-08 22:31:35 - train: epoch 0097, iter [00400, 05004], lr: 0.009488, loss: 1.2016
2022-07-08 22:32:20 - train: epoch 0097, iter [00500, 05004], lr: 0.009472, loss: 0.9149
2022-07-08 22:33:06 - train: epoch 0097, iter [00600, 05004], lr: 0.009457, loss: 1.0085
2022-07-08 22:33:52 - train: epoch 0097, iter [00700, 05004], lr: 0.009442, loss: 0.9671
2022-07-08 22:34:37 - train: epoch 0097, iter [00800, 05004], lr: 0.009426, loss: 0.8227
2022-07-08 22:35:23 - train: epoch 0097, iter [00900, 05004], lr: 0.009411, loss: 0.8105
2022-07-08 22:36:08 - train: epoch 0097, iter [01000, 05004], lr: 0.009396, loss: 0.8695
2022-07-08 22:36:54 - train: epoch 0097, iter [01100, 05004], lr: 0.009381, loss: 0.8028
2022-07-08 22:37:39 - train: epoch 0097, iter [01200, 05004], lr: 0.009365, loss: 0.8906
2022-07-08 22:38:25 - train: epoch 0097, iter [01300, 05004], lr: 0.009350, loss: 0.8968
2022-07-08 22:39:10 - train: epoch 0097, iter [01400, 05004], lr: 0.009335, loss: 0.8731
2022-07-08 22:39:56 - train: epoch 0097, iter [01500, 05004], lr: 0.009320, loss: 0.9947
2022-07-08 22:40:41 - train: epoch 0097, iter [01600, 05004], lr: 0.009305, loss: 0.9229
2022-07-08 22:41:27 - train: epoch 0097, iter [01700, 05004], lr: 0.009289, loss: 0.8652
2022-07-08 22:42:12 - train: epoch 0097, iter [01800, 05004], lr: 0.009274, loss: 0.9113
2022-07-08 22:42:58 - train: epoch 0097, iter [01900, 05004], lr: 0.009259, loss: 1.0169
2022-07-08 22:43:44 - train: epoch 0097, iter [02000, 05004], lr: 0.009244, loss: 0.8814
2022-07-08 22:44:29 - train: epoch 0097, iter [02100, 05004], lr: 0.009229, loss: 1.0236
2022-07-08 22:45:15 - train: epoch 0097, iter [02200, 05004], lr: 0.009214, loss: 1.1365
2022-07-08 22:46:00 - train: epoch 0097, iter [02300, 05004], lr: 0.009198, loss: 0.8595
2022-07-08 22:46:45 - train: epoch 0097, iter [02400, 05004], lr: 0.009183, loss: 0.8787
2022-07-08 22:47:31 - train: epoch 0097, iter [02500, 05004], lr: 0.009168, loss: 0.9304
2022-07-08 22:48:16 - train: epoch 0097, iter [02600, 05004], lr: 0.009153, loss: 1.0776
2022-07-08 22:49:02 - train: epoch 0097, iter [02700, 05004], lr: 0.009138, loss: 0.8340
2022-07-08 22:49:47 - train: epoch 0097, iter [02800, 05004], lr: 0.009123, loss: 0.8556
2022-07-08 22:50:33 - train: epoch 0097, iter [02900, 05004], lr: 0.009108, loss: 1.1269
2022-07-08 22:51:18 - train: epoch 0097, iter [03000, 05004], lr: 0.009093, loss: 0.9266
2022-07-08 22:52:04 - train: epoch 0097, iter [03100, 05004], lr: 0.009078, loss: 0.9067
2022-07-08 22:52:49 - train: epoch 0097, iter [03200, 05004], lr: 0.009063, loss: 1.0424
2022-07-08 22:53:34 - train: epoch 0097, iter [03300, 05004], lr: 0.009048, loss: 1.0289
2022-07-08 22:54:20 - train: epoch 0097, iter [03400, 05004], lr: 0.009033, loss: 1.1611
2022-07-08 22:55:05 - train: epoch 0097, iter [03500, 05004], lr: 0.009018, loss: 1.0906
2022-07-08 22:55:50 - train: epoch 0097, iter [03600, 05004], lr: 0.009003, loss: 0.9070
2022-07-08 22:56:36 - train: epoch 0097, iter [03700, 05004], lr: 0.008988, loss: 0.8239
2022-07-08 22:57:21 - train: epoch 0097, iter [03800, 05004], lr: 0.008973, loss: 0.8813
2022-07-08 22:58:07 - train: epoch 0097, iter [03900, 05004], lr: 0.008958, loss: 1.0490
2022-07-08 22:58:52 - train: epoch 0097, iter [04000, 05004], lr: 0.008943, loss: 0.9485
2022-07-08 22:59:38 - train: epoch 0097, iter [04100, 05004], lr: 0.008928, loss: 0.9056
2022-07-08 23:00:23 - train: epoch 0097, iter [04200, 05004], lr: 0.008913, loss: 1.0167
2022-07-08 23:01:09 - train: epoch 0097, iter [04300, 05004], lr: 0.008898, loss: 0.8639
2022-07-08 23:01:54 - train: epoch 0097, iter [04400, 05004], lr: 0.008883, loss: 0.8903
2022-07-08 23:02:39 - train: epoch 0097, iter [04500, 05004], lr: 0.008869, loss: 0.9769
2022-07-08 23:03:25 - train: epoch 0097, iter [04600, 05004], lr: 0.008854, loss: 1.1001
2022-07-08 23:04:10 - train: epoch 0097, iter [04700, 05004], lr: 0.008839, loss: 1.0322
2022-07-08 23:04:55 - train: epoch 0097, iter [04800, 05004], lr: 0.008824, loss: 0.9863
2022-07-08 23:05:41 - train: epoch 0097, iter [04900, 05004], lr: 0.008809, loss: 1.1597
2022-07-08 23:06:26 - train: epoch 0097, iter [05000, 05004], lr: 0.008794, loss: 1.0267
2022-07-08 23:06:28 - train: epoch 097, train_loss: 0.9418
2022-07-08 23:07:45 - eval: epoch: 097, acc1: 75.516%, acc5: 92.666%, test_loss: 0.9802, per_image_load_time: 2.185ms, per_image_inference_time: 0.823ms
2022-07-08 23:07:47 - until epoch: 097, best_acc1: 75.516%
2022-07-08 23:29:48 - epoch 098 lr: 0.008794
2022-07-08 23:30:40 - train: epoch 0098, iter [00100, 05004], lr: 0.008779, loss: 0.9778
2022-07-08 23:31:26 - train: epoch 0098, iter [00200, 05004], lr: 0.008764, loss: 1.0224
2022-07-08 23:32:13 - train: epoch 0098, iter [00300, 05004], lr: 0.008749, loss: 0.9083
2022-07-08 23:32:59 - train: epoch 0098, iter [00400, 05004], lr: 0.008735, loss: 0.8396
2022-07-08 23:33:45 - train: epoch 0098, iter [00500, 05004], lr: 0.008720, loss: 0.9301
2022-07-08 23:34:31 - train: epoch 0098, iter [00600, 05004], lr: 0.008705, loss: 0.9050
2022-07-08 23:35:17 - train: epoch 0098, iter [00700, 05004], lr: 0.008690, loss: 0.9215
2022-07-08 23:36:03 - train: epoch 0098, iter [00800, 05004], lr: 0.008676, loss: 0.8528
2022-07-08 23:36:49 - train: epoch 0098, iter [00900, 05004], lr: 0.008661, loss: 0.8715
2022-07-08 23:37:35 - train: epoch 0098, iter [01000, 05004], lr: 0.008646, loss: 0.8444
2022-07-08 23:38:21 - train: epoch 0098, iter [01100, 05004], lr: 0.008631, loss: 0.8626
2022-07-08 23:39:07 - train: epoch 0098, iter [01200, 05004], lr: 0.008617, loss: 0.8897
2022-07-08 23:39:53 - train: epoch 0098, iter [01300, 05004], lr: 0.008602, loss: 0.9740
2022-07-08 23:40:39 - train: epoch 0098, iter [01400, 05004], lr: 0.008587, loss: 0.8668
2022-07-08 23:41:25 - train: epoch 0098, iter [01500, 05004], lr: 0.008573, loss: 0.8858
2022-07-08 23:42:11 - train: epoch 0098, iter [01600, 05004], lr: 0.008558, loss: 0.9001
2022-07-08 23:42:57 - train: epoch 0098, iter [01700, 05004], lr: 0.008543, loss: 0.8830
2022-07-08 23:43:43 - train: epoch 0098, iter [01800, 05004], lr: 0.008529, loss: 0.9495
2022-07-08 23:44:29 - train: epoch 0098, iter [01900, 05004], lr: 0.008514, loss: 0.8222
2022-07-08 23:45:15 - train: epoch 0098, iter [02000, 05004], lr: 0.008500, loss: 1.1229
2022-07-08 23:46:01 - train: epoch 0098, iter [02100, 05004], lr: 0.008485, loss: 0.8360
2022-07-08 23:46:46 - train: epoch 0098, iter [02200, 05004], lr: 0.008470, loss: 0.8696
2022-07-08 23:47:32 - train: epoch 0098, iter [02300, 05004], lr: 0.008456, loss: 0.8977
2022-07-08 23:48:18 - train: epoch 0098, iter [02400, 05004], lr: 0.008441, loss: 0.9041
2022-07-08 23:49:04 - train: epoch 0098, iter [02500, 05004], lr: 0.008427, loss: 0.9928
2022-07-08 23:49:50 - train: epoch 0098, iter [02600, 05004], lr: 0.008412, loss: 0.8141
2022-07-08 23:50:36 - train: epoch 0098, iter [02700, 05004], lr: 0.008398, loss: 0.9223
2022-07-08 23:51:22 - train: epoch 0098, iter [02800, 05004], lr: 0.008383, loss: 0.9667
2022-07-08 23:52:08 - train: epoch 0098, iter [02900, 05004], lr: 0.008369, loss: 0.8267
2022-07-08 23:52:53 - train: epoch 0098, iter [03000, 05004], lr: 0.008354, loss: 0.8422
2022-07-08 23:53:39 - train: epoch 0098, iter [03100, 05004], lr: 0.008340, loss: 0.9613
2022-07-08 23:54:25 - train: epoch 0098, iter [03200, 05004], lr: 0.008325, loss: 0.7939
2022-07-08 23:55:11 - train: epoch 0098, iter [03300, 05004], lr: 0.008311, loss: 0.9175
2022-07-08 23:55:56 - train: epoch 0098, iter [03400, 05004], lr: 0.008296, loss: 0.8559
2022-07-08 23:56:42 - train: epoch 0098, iter [03500, 05004], lr: 0.008282, loss: 0.9585
2022-07-08 23:57:27 - train: epoch 0098, iter [03600, 05004], lr: 0.008268, loss: 1.1785
2022-07-08 23:58:13 - train: epoch 0098, iter [03700, 05004], lr: 0.008253, loss: 0.9155
2022-07-08 23:58:58 - train: epoch 0098, iter [03800, 05004], lr: 0.008239, loss: 0.8404
2022-07-08 23:59:44 - train: epoch 0098, iter [03900, 05004], lr: 0.008224, loss: 1.0010
2022-07-09 00:00:29 - train: epoch 0098, iter [04000, 05004], lr: 0.008210, loss: 1.1722
2022-07-09 00:01:15 - train: epoch 0098, iter [04100, 05004], lr: 0.008196, loss: 1.0628
2022-07-09 00:02:01 - train: epoch 0098, iter [04200, 05004], lr: 0.008181, loss: 0.9188
2022-07-09 00:02:47 - train: epoch 0098, iter [04300, 05004], lr: 0.008167, loss: 0.7567
2022-07-09 00:03:33 - train: epoch 0098, iter [04400, 05004], lr: 0.008153, loss: 1.0197
2022-07-09 00:04:18 - train: epoch 0098, iter [04500, 05004], lr: 0.008138, loss: 1.0861
2022-07-09 00:05:04 - train: epoch 0098, iter [04600, 05004], lr: 0.008124, loss: 0.9473
2022-07-09 00:05:50 - train: epoch 0098, iter [04700, 05004], lr: 0.008110, loss: 1.1808
2022-07-09 00:06:36 - train: epoch 0098, iter [04800, 05004], lr: 0.008096, loss: 0.8542
2022-07-09 00:07:21 - train: epoch 0098, iter [04900, 05004], lr: 0.008081, loss: 0.9357
2022-07-09 00:08:07 - train: epoch 0098, iter [05000, 05004], lr: 0.008067, loss: 0.9306
2022-07-09 00:08:09 - train: epoch 098, train_loss: 0.9210
2022-07-09 00:09:26 - eval: epoch: 098, acc1: 75.278%, acc5: 92.798%, test_loss: 0.9835, per_image_load_time: 0.611ms, per_image_inference_time: 0.855ms
2022-07-09 00:09:27 - until epoch: 098, best_acc1: 75.516%
2022-07-09 00:09:27 - epoch 099 lr: 0.008066
2022-07-09 00:10:18 - train: epoch 0099, iter [00100, 05004], lr: 0.008052, loss: 1.0075
2022-07-09 00:11:03 - train: epoch 0099, iter [00200, 05004], lr: 0.008038, loss: 0.9227
2022-07-09 00:11:48 - train: epoch 0099, iter [00300, 05004], lr: 0.008024, loss: 0.7381
2022-07-09 00:12:33 - train: epoch 0099, iter [00400, 05004], lr: 0.008010, loss: 0.9384
2022-07-09 00:13:18 - train: epoch 0099, iter [00500, 05004], lr: 0.007995, loss: 0.7970
2022-07-09 00:14:02 - train: epoch 0099, iter [00600, 05004], lr: 0.007981, loss: 0.7234
2022-07-09 00:14:47 - train: epoch 0099, iter [00700, 05004], lr: 0.007967, loss: 0.9324
2022-07-09 00:15:31 - train: epoch 0099, iter [00800, 05004], lr: 0.007953, loss: 0.8942
2022-07-09 00:16:15 - train: epoch 0099, iter [00900, 05004], lr: 0.007939, loss: 1.0003
2022-07-09 00:17:00 - train: epoch 0099, iter [01000, 05004], lr: 0.007925, loss: 0.8390
2022-07-09 00:17:44 - train: epoch 0099, iter [01100, 05004], lr: 0.007910, loss: 0.8890
2022-07-09 00:18:29 - train: epoch 0099, iter [01200, 05004], lr: 0.007896, loss: 0.7567
2022-07-09 00:19:13 - train: epoch 0099, iter [01300, 05004], lr: 0.007882, loss: 0.8837
2022-07-09 00:19:58 - train: epoch 0099, iter [01400, 05004], lr: 0.007868, loss: 0.9194
2022-07-09 00:20:43 - train: epoch 0099, iter [01500, 05004], lr: 0.007854, loss: 0.8192
2022-07-09 00:21:27 - train: epoch 0099, iter [01600, 05004], lr: 0.007840, loss: 1.0015
2022-07-09 00:22:12 - train: epoch 0099, iter [01700, 05004], lr: 0.007826, loss: 0.8351
2022-07-09 00:22:57 - train: epoch 0099, iter [01800, 05004], lr: 0.007812, loss: 1.0106
2022-07-09 00:23:42 - train: epoch 0099, iter [01900, 05004], lr: 0.007798, loss: 0.9404
2022-07-09 00:24:27 - train: epoch 0099, iter [02000, 05004], lr: 0.007784, loss: 0.9111
2022-07-09 00:25:11 - train: epoch 0099, iter [02100, 05004], lr: 0.007770, loss: 0.9364
2022-07-09 00:25:56 - train: epoch 0099, iter [02200, 05004], lr: 0.007756, loss: 0.9171
2022-07-09 00:26:41 - train: epoch 0099, iter [02300, 05004], lr: 0.007742, loss: 0.9386
2022-07-09 00:27:26 - train: epoch 0099, iter [02400, 05004], lr: 0.007728, loss: 0.9179
2022-07-09 00:28:11 - train: epoch 0099, iter [02500, 05004], lr: 0.007714, loss: 0.8710
2022-07-09 00:28:55 - train: epoch 0099, iter [02600, 05004], lr: 0.007700, loss: 0.8334
2022-07-09 00:29:40 - train: epoch 0099, iter [02700, 05004], lr: 0.007686, loss: 0.9336
2022-07-09 00:30:25 - train: epoch 0099, iter [02800, 05004], lr: 0.007672, loss: 0.8236
2022-07-09 00:31:10 - train: epoch 0099, iter [02900, 05004], lr: 0.007658, loss: 0.7388
2022-07-09 00:31:55 - train: epoch 0099, iter [03000, 05004], lr: 0.007644, loss: 1.0267
2022-07-09 00:32:40 - train: epoch 0099, iter [03100, 05004], lr: 0.007630, loss: 0.8626
2022-07-09 00:33:25 - train: epoch 0099, iter [03200, 05004], lr: 0.007616, loss: 0.8088
2022-07-09 00:34:10 - train: epoch 0099, iter [03300, 05004], lr: 0.007603, loss: 0.8456
2022-07-09 00:34:54 - train: epoch 0099, iter [03400, 05004], lr: 0.007589, loss: 0.7855
2022-07-09 00:35:39 - train: epoch 0099, iter [03500, 05004], lr: 0.007575, loss: 0.9265
2022-07-09 00:36:24 - train: epoch 0099, iter [03600, 05004], lr: 0.007561, loss: 0.8827
2022-07-09 00:37:09 - train: epoch 0099, iter [03700, 05004], lr: 0.007547, loss: 0.8010
2022-07-09 00:37:54 - train: epoch 0099, iter [03800, 05004], lr: 0.007533, loss: 0.9248
2022-07-09 00:38:38 - train: epoch 0099, iter [03900, 05004], lr: 0.007520, loss: 0.9603
2022-07-09 00:39:23 - train: epoch 0099, iter [04000, 05004], lr: 0.007506, loss: 0.9199
2022-07-09 00:40:08 - train: epoch 0099, iter [04100, 05004], lr: 0.007492, loss: 0.7022
2022-07-09 00:40:53 - train: epoch 0099, iter [04200, 05004], lr: 0.007478, loss: 0.9539
2022-07-09 00:41:38 - train: epoch 0099, iter [04300, 05004], lr: 0.007465, loss: 1.1285
2022-07-09 00:42:23 - train: epoch 0099, iter [04400, 05004], lr: 0.007451, loss: 0.8608
2022-07-09 00:43:08 - train: epoch 0099, iter [04500, 05004], lr: 0.007437, loss: 0.9452
2022-07-09 00:43:53 - train: epoch 0099, iter [04600, 05004], lr: 0.007423, loss: 0.9913
2022-07-09 00:44:38 - train: epoch 0099, iter [04700, 05004], lr: 0.007410, loss: 0.7906
2022-07-09 00:45:23 - train: epoch 0099, iter [04800, 05004], lr: 0.007396, loss: 1.0434
2022-07-09 00:46:08 - train: epoch 0099, iter [04900, 05004], lr: 0.007382, loss: 0.9591
2022-07-09 00:46:53 - train: epoch 0099, iter [05000, 05004], lr: 0.007369, loss: 1.0171
2022-07-09 00:46:55 - train: epoch 099, train_loss: 0.8976
2022-07-09 00:48:12 - eval: epoch: 099, acc1: 75.676%, acc5: 92.866%, test_loss: 0.9742, per_image_load_time: 0.640ms, per_image_inference_time: 0.843ms
2022-07-09 00:48:14 - until epoch: 099, best_acc1: 75.676%
2022-07-09 00:48:14 - epoch 100 lr: 0.007368
2022-07-09 00:49:05 - train: epoch 0100, iter [00100, 05004], lr: 0.007354, loss: 0.8495
2022-07-09 00:49:49 - train: epoch 0100, iter [00200, 05004], lr: 0.007341, loss: 0.7689
2022-07-09 00:50:34 - train: epoch 0100, iter [00300, 05004], lr: 0.007327, loss: 0.7647
2022-07-09 00:51:18 - train: epoch 0100, iter [00400, 05004], lr: 0.007313, loss: 0.7598
2022-07-09 00:52:03 - train: epoch 0100, iter [00500, 05004], lr: 0.007300, loss: 0.8457
2022-07-09 00:52:48 - train: epoch 0100, iter [00600, 05004], lr: 0.007286, loss: 1.0830
2022-07-09 00:53:32 - train: epoch 0100, iter [00700, 05004], lr: 0.007273, loss: 0.6795
2022-07-09 00:54:17 - train: epoch 0100, iter [00800, 05004], lr: 0.007259, loss: 0.9020
2022-07-09 00:55:02 - train: epoch 0100, iter [00900, 05004], lr: 0.007245, loss: 0.8311
2022-07-09 00:55:46 - train: epoch 0100, iter [01000, 05004], lr: 0.007232, loss: 0.9509
2022-07-09 00:56:31 - train: epoch 0100, iter [01100, 05004], lr: 0.007218, loss: 0.8023
2022-07-09 00:57:16 - train: epoch 0100, iter [01200, 05004], lr: 0.007205, loss: 0.9054
2022-07-09 00:58:00 - train: epoch 0100, iter [01300, 05004], lr: 0.007191, loss: 0.8470
2022-07-09 00:58:45 - train: epoch 0100, iter [01400, 05004], lr: 0.007178, loss: 1.0633
2022-07-09 00:59:30 - train: epoch 0100, iter [01500, 05004], lr: 0.007164, loss: 0.8619
2022-07-09 01:00:14 - train: epoch 0100, iter [01600, 05004], lr: 0.007151, loss: 0.8042
2022-07-09 01:00:59 - train: epoch 0100, iter [01700, 05004], lr: 0.007137, loss: 0.8784
2022-07-09 01:01:44 - train: epoch 0100, iter [01800, 05004], lr: 0.007124, loss: 0.7942
2022-07-09 01:02:28 - train: epoch 0100, iter [01900, 05004], lr: 0.007110, loss: 1.0078
2022-07-09 01:03:13 - train: epoch 0100, iter [02000, 05004], lr: 0.007097, loss: 0.9797
2022-07-09 01:03:58 - train: epoch 0100, iter [02100, 05004], lr: 0.007084, loss: 0.7586
2022-07-09 01:04:42 - train: epoch 0100, iter [02200, 05004], lr: 0.007070, loss: 1.0391
2022-07-09 01:05:27 - train: epoch 0100, iter [02300, 05004], lr: 0.007057, loss: 1.0302
2022-07-09 01:06:12 - train: epoch 0100, iter [02400, 05004], lr: 0.007043, loss: 0.8605
2022-07-09 01:06:56 - train: epoch 0100, iter [02500, 05004], lr: 0.007030, loss: 0.7900
2022-07-09 01:07:41 - train: epoch 0100, iter [02600, 05004], lr: 0.007017, loss: 0.8999
2022-07-09 01:08:25 - train: epoch 0100, iter [02700, 05004], lr: 0.007003, loss: 0.7082
2022-07-09 01:09:10 - train: epoch 0100, iter [02800, 05004], lr: 0.006990, loss: 0.9985
2022-07-09 01:09:55 - train: epoch 0100, iter [02900, 05004], lr: 0.006977, loss: 0.7867
2022-07-09 01:10:40 - train: epoch 0100, iter [03000, 05004], lr: 0.006963, loss: 0.6715
2022-07-09 01:11:24 - train: epoch 0100, iter [03100, 05004], lr: 0.006950, loss: 0.6884
2022-07-09 01:12:09 - train: epoch 0100, iter [03200, 05004], lr: 0.006937, loss: 1.0868
2022-07-09 01:12:54 - train: epoch 0100, iter [03300, 05004], lr: 0.006923, loss: 0.9162
2022-07-09 01:13:39 - train: epoch 0100, iter [03400, 05004], lr: 0.006910, loss: 0.8757
2022-07-09 01:14:23 - train: epoch 0100, iter [03500, 05004], lr: 0.006897, loss: 0.7278
2022-07-09 01:15:08 - train: epoch 0100, iter [03600, 05004], lr: 0.006884, loss: 0.9239
2022-07-09 01:15:53 - train: epoch 0100, iter [03700, 05004], lr: 0.006870, loss: 0.7929
2022-07-09 01:16:38 - train: epoch 0100, iter [03800, 05004], lr: 0.006857, loss: 0.8129
2022-07-09 01:17:23 - train: epoch 0100, iter [03900, 05004], lr: 0.006844, loss: 0.8565
2022-07-09 01:18:08 - train: epoch 0100, iter [04000, 05004], lr: 0.006831, loss: 0.8104
2022-07-09 01:18:52 - train: epoch 0100, iter [04100, 05004], lr: 0.006817, loss: 1.0507
2022-07-09 01:19:37 - train: epoch 0100, iter [04200, 05004], lr: 0.006804, loss: 0.9101
2022-07-09 01:20:22 - train: epoch 0100, iter [04300, 05004], lr: 0.006791, loss: 0.8110
2022-07-09 01:21:07 - train: epoch 0100, iter [04400, 05004], lr: 0.006778, loss: 0.8569
2022-07-09 01:21:52 - train: epoch 0100, iter [04500, 05004], lr: 0.006765, loss: 0.7252
2022-07-09 01:22:36 - train: epoch 0100, iter [04600, 05004], lr: 0.006752, loss: 0.8574
2022-07-09 01:23:21 - train: epoch 0100, iter [04700, 05004], lr: 0.006739, loss: 0.9568
2022-07-09 01:24:06 - train: epoch 0100, iter [04800, 05004], lr: 0.006725, loss: 0.7774
2022-07-09 01:24:51 - train: epoch 0100, iter [04900, 05004], lr: 0.006712, loss: 0.8992
2022-07-09 01:25:36 - train: epoch 0100, iter [05000, 05004], lr: 0.006699, loss: 0.9662
2022-07-09 01:25:38 - train: epoch 100, train_loss: 0.8770
2022-07-09 01:26:55 - eval: epoch: 100, acc1: 76.050%, acc5: 93.066%, test_loss: 0.9505, per_image_load_time: 0.636ms, per_image_inference_time: 0.835ms
2022-07-09 01:26:56 - until epoch: 100, best_acc1: 76.050%
2022-07-09 01:26:56 - epoch 101 lr: 0.006699
2022-07-09 01:27:47 - train: epoch 0101, iter [00100, 05004], lr: 0.006686, loss: 0.6219
2022-07-09 01:28:32 - train: epoch 0101, iter [00200, 05004], lr: 0.006673, loss: 0.8024
2022-07-09 01:29:16 - train: epoch 0101, iter [00300, 05004], lr: 0.006660, loss: 0.7414
2022-07-09 01:30:01 - train: epoch 0101, iter [00400, 05004], lr: 0.006647, loss: 0.8557
2022-07-09 01:30:45 - train: epoch 0101, iter [00500, 05004], lr: 0.006633, loss: 0.7856
2022-07-09 01:31:30 - train: epoch 0101, iter [00600, 05004], lr: 0.006620, loss: 0.8087
2022-07-09 01:32:15 - train: epoch 0101, iter [00700, 05004], lr: 0.006607, loss: 0.8865
2022-07-09 01:32:59 - train: epoch 0101, iter [00800, 05004], lr: 0.006594, loss: 0.7856
2022-07-09 01:33:44 - train: epoch 0101, iter [00900, 05004], lr: 0.006581, loss: 0.9391
2022-07-09 01:34:28 - train: epoch 0101, iter [01000, 05004], lr: 0.006569, loss: 0.9911
2022-07-09 01:35:13 - train: epoch 0101, iter [01100, 05004], lr: 0.006556, loss: 1.0745
2022-07-09 01:35:57 - train: epoch 0101, iter [01200, 05004], lr: 0.006543, loss: 0.8031
2022-07-09 01:36:42 - train: epoch 0101, iter [01300, 05004], lr: 0.006530, loss: 0.8109
2022-07-09 01:37:26 - train: epoch 0101, iter [01400, 05004], lr: 0.006517, loss: 0.7295
2022-07-09 01:38:11 - train: epoch 0101, iter [01500, 05004], lr: 0.006504, loss: 0.7643
2022-07-09 01:38:56 - train: epoch 0101, iter [01600, 05004], lr: 0.006491, loss: 0.7455
2022-07-09 01:39:40 - train: epoch 0101, iter [01700, 05004], lr: 0.006478, loss: 0.9715
2022-07-09 01:40:25 - train: epoch 0101, iter [01800, 05004], lr: 0.006465, loss: 1.0229
2022-07-09 01:41:09 - train: epoch 0101, iter [01900, 05004], lr: 0.006452, loss: 0.7013
2022-07-09 01:41:54 - train: epoch 0101, iter [02000, 05004], lr: 0.006440, loss: 0.7762
2022-07-09 01:42:39 - train: epoch 0101, iter [02100, 05004], lr: 0.006427, loss: 0.8608
2022-07-09 01:43:23 - train: epoch 0101, iter [02200, 05004], lr: 0.006414, loss: 0.7420
2022-07-09 01:44:08 - train: epoch 0101, iter [02300, 05004], lr: 0.006401, loss: 0.8367
2022-07-09 01:44:53 - train: epoch 0101, iter [02400, 05004], lr: 0.006388, loss: 0.6968
2022-07-09 01:45:37 - train: epoch 0101, iter [02500, 05004], lr: 0.006375, loss: 0.8800
2022-07-09 01:46:22 - train: epoch 0101, iter [02600, 05004], lr: 0.006363, loss: 0.8494
2022-07-09 01:47:06 - train: epoch 0101, iter [02700, 05004], lr: 0.006350, loss: 0.8303
2022-07-09 01:47:51 - train: epoch 0101, iter [02800, 05004], lr: 0.006337, loss: 0.7835
2022-07-09 01:48:35 - train: epoch 0101, iter [02900, 05004], lr: 0.006324, loss: 0.9585
2022-07-09 01:49:20 - train: epoch 0101, iter [03000, 05004], lr: 0.006312, loss: 0.8592
2022-07-09 01:50:05 - train: epoch 0101, iter [03100, 05004], lr: 0.006299, loss: 1.1120
2022-07-09 01:50:49 - train: epoch 0101, iter [03200, 05004], lr: 0.006286, loss: 1.0353
2022-07-09 01:51:34 - train: epoch 0101, iter [03300, 05004], lr: 0.006274, loss: 0.9823
2022-07-09 01:52:19 - train: epoch 0101, iter [03400, 05004], lr: 0.006261, loss: 0.8438
2022-07-09 01:53:03 - train: epoch 0101, iter [03500, 05004], lr: 0.006248, loss: 0.6868
2022-07-09 01:53:48 - train: epoch 0101, iter [03600, 05004], lr: 0.006236, loss: 0.8759
2022-07-09 01:54:33 - train: epoch 0101, iter [03700, 05004], lr: 0.006223, loss: 0.8569
2022-07-09 01:55:17 - train: epoch 0101, iter [03800, 05004], lr: 0.006210, loss: 0.8172
2022-07-09 01:56:02 - train: epoch 0101, iter [03900, 05004], lr: 0.006198, loss: 0.8369
2022-07-09 01:56:47 - train: epoch 0101, iter [04000, 05004], lr: 0.006185, loss: 0.9588
2022-07-09 01:57:31 - train: epoch 0101, iter [04100, 05004], lr: 0.006172, loss: 0.8038
2022-07-09 01:58:16 - train: epoch 0101, iter [04200, 05004], lr: 0.006160, loss: 0.8916
2022-07-09 01:59:00 - train: epoch 0101, iter [04300, 05004], lr: 0.006147, loss: 0.7259
2022-07-09 01:59:45 - train: epoch 0101, iter [04400, 05004], lr: 0.006135, loss: 0.8850
2022-07-09 02:00:29 - train: epoch 0101, iter [04500, 05004], lr: 0.006122, loss: 1.0165
2022-07-09 02:01:14 - train: epoch 0101, iter [04600, 05004], lr: 0.006110, loss: 0.9122
2022-07-09 02:01:58 - train: epoch 0101, iter [04700, 05004], lr: 0.006097, loss: 0.9008
2022-07-09 02:02:43 - train: epoch 0101, iter [04800, 05004], lr: 0.006085, loss: 0.9074
2022-07-09 02:03:27 - train: epoch 0101, iter [04900, 05004], lr: 0.006072, loss: 0.7702
2022-07-09 02:04:12 - train: epoch 0101, iter [05000, 05004], lr: 0.006060, loss: 1.0298
2022-07-09 02:04:14 - train: epoch 101, train_loss: 0.8576
2022-07-09 02:05:31 - eval: epoch: 101, acc1: 76.006%, acc5: 92.988%, test_loss: 0.9577, per_image_load_time: 1.200ms, per_image_inference_time: 0.845ms
2022-07-09 02:05:32 - until epoch: 101, best_acc1: 76.050%
2022-07-09 02:05:32 - epoch 102 lr: 0.006059
2022-07-09 02:06:23 - train: epoch 0102, iter [00100, 05004], lr: 0.006047, loss: 0.8638
2022-07-09 02:07:07 - train: epoch 0102, iter [00200, 05004], lr: 0.006034, loss: 0.9534
2022-07-09 02:07:52 - train: epoch 0102, iter [00300, 05004], lr: 0.006022, loss: 0.8524
2022-07-09 02:08:37 - train: epoch 0102, iter [00400, 05004], lr: 0.006009, loss: 0.8565
2022-07-09 02:09:21 - train: epoch 0102, iter [00500, 05004], lr: 0.005997, loss: 0.7693
2022-07-09 02:10:06 - train: epoch 0102, iter [00600, 05004], lr: 0.005984, loss: 0.8389
2022-07-09 02:10:50 - train: epoch 0102, iter [00700, 05004], lr: 0.005972, loss: 0.7451
2022-07-09 02:11:35 - train: epoch 0102, iter [00800, 05004], lr: 0.005960, loss: 0.7592
2022-07-09 02:12:19 - train: epoch 0102, iter [00900, 05004], lr: 0.005947, loss: 0.7774
2022-07-09 02:13:04 - train: epoch 0102, iter [01000, 05004], lr: 0.005935, loss: 0.8611
2022-07-09 02:13:49 - train: epoch 0102, iter [01100, 05004], lr: 0.005923, loss: 0.7984
2022-07-09 02:14:33 - train: epoch 0102, iter [01200, 05004], lr: 0.005910, loss: 0.7660
2022-07-09 02:15:18 - train: epoch 0102, iter [01300, 05004], lr: 0.005898, loss: 0.7741
2022-07-09 02:16:02 - train: epoch 0102, iter [01400, 05004], lr: 0.005886, loss: 0.8370
2022-07-09 02:16:47 - train: epoch 0102, iter [01500, 05004], lr: 0.005873, loss: 0.8188
2022-07-09 02:17:31 - train: epoch 0102, iter [01600, 05004], lr: 0.005861, loss: 0.7880
2022-07-09 02:18:16 - train: epoch 0102, iter [01700, 05004], lr: 0.005849, loss: 0.9596
2022-07-09 02:19:00 - train: epoch 0102, iter [01800, 05004], lr: 0.005836, loss: 0.9706
2022-07-09 02:19:44 - train: epoch 0102, iter [01900, 05004], lr: 0.005824, loss: 0.7741
2022-07-09 02:20:29 - train: epoch 0102, iter [02000, 05004], lr: 0.005812, loss: 0.6831
2022-07-09 02:21:13 - train: epoch 0102, iter [02100, 05004], lr: 0.005800, loss: 0.8526
2022-07-09 02:21:58 - train: epoch 0102, iter [02200, 05004], lr: 0.005787, loss: 0.8376
2022-07-09 02:22:42 - train: epoch 0102, iter [02300, 05004], lr: 0.005775, loss: 0.7265
2022-07-09 02:23:27 - train: epoch 0102, iter [02400, 05004], lr: 0.005763, loss: 0.7905
2022-07-09 02:24:11 - train: epoch 0102, iter [02500, 05004], lr: 0.005751, loss: 0.8611
2022-07-09 02:24:56 - train: epoch 0102, iter [02600, 05004], lr: 0.005739, loss: 0.7323
2022-07-09 02:25:41 - train: epoch 0102, iter [02700, 05004], lr: 0.005727, loss: 1.0488
2022-07-09 02:26:25 - train: epoch 0102, iter [02800, 05004], lr: 0.005714, loss: 0.9607
2022-07-09 02:27:10 - train: epoch 0102, iter [02900, 05004], lr: 0.005702, loss: 1.0157
2022-07-09 02:27:55 - train: epoch 0102, iter [03000, 05004], lr: 0.005690, loss: 0.8736
2022-07-09 02:28:39 - train: epoch 0102, iter [03100, 05004], lr: 0.005678, loss: 0.8231
2022-07-09 02:29:24 - train: epoch 0102, iter [03200, 05004], lr: 0.005666, loss: 0.7629
2022-07-09 02:30:09 - train: epoch 0102, iter [03300, 05004], lr: 0.005654, loss: 0.6894
2022-07-09 02:30:54 - train: epoch 0102, iter [03400, 05004], lr: 0.005642, loss: 0.8169
2022-07-09 02:31:38 - train: epoch 0102, iter [03500, 05004], lr: 0.005630, loss: 0.9364
2022-07-09 02:32:23 - train: epoch 0102, iter [03600, 05004], lr: 0.005618, loss: 0.8408
2022-07-09 02:33:08 - train: epoch 0102, iter [03700, 05004], lr: 0.005606, loss: 0.8366
2022-07-09 02:33:52 - train: epoch 0102, iter [03800, 05004], lr: 0.005594, loss: 0.7827
2022-07-09 02:34:37 - train: epoch 0102, iter [03900, 05004], lr: 0.005582, loss: 0.8253
2022-07-09 02:35:22 - train: epoch 0102, iter [04000, 05004], lr: 0.005570, loss: 0.7547
2022-07-09 02:36:07 - train: epoch 0102, iter [04100, 05004], lr: 0.005558, loss: 0.7964
2022-07-09 02:36:51 - train: epoch 0102, iter [04200, 05004], lr: 0.005546, loss: 0.9011
2022-07-09 02:37:36 - train: epoch 0102, iter [04300, 05004], lr: 0.005534, loss: 0.7717
2022-07-09 02:38:21 - train: epoch 0102, iter [04400, 05004], lr: 0.005522, loss: 0.8555
2022-07-09 02:39:05 - train: epoch 0102, iter [04500, 05004], lr: 0.005510, loss: 0.9104
2022-07-09 02:39:50 - train: epoch 0102, iter [04600, 05004], lr: 0.005498, loss: 0.9190
2022-07-09 02:40:35 - train: epoch 0102, iter [04700, 05004], lr: 0.005486, loss: 1.0221
2022-07-09 02:41:19 - train: epoch 0102, iter [04800, 05004], lr: 0.005474, loss: 0.9025
2022-07-09 02:42:04 - train: epoch 0102, iter [04900, 05004], lr: 0.005462, loss: 0.8919
2022-07-09 02:42:49 - train: epoch 0102, iter [05000, 05004], lr: 0.005450, loss: 0.8100
2022-07-09 02:42:51 - train: epoch 102, train_loss: 0.8336
2022-07-09 02:44:08 - eval: epoch: 102, acc1: 76.408%, acc5: 93.288%, test_loss: 0.9434, per_image_load_time: 1.055ms, per_image_inference_time: 0.858ms
2022-07-09 02:44:09 - until epoch: 102, best_acc1: 76.408%
2022-07-09 02:44:09 - epoch 103 lr: 0.005450
2022-07-09 02:45:01 - train: epoch 0103, iter [00100, 05004], lr: 0.005438, loss: 0.8478
2022-07-09 02:45:45 - train: epoch 0103, iter [00200, 05004], lr: 0.005426, loss: 0.7785
2022-07-09 02:46:30 - train: epoch 0103, iter [00300, 05004], lr: 0.005414, loss: 0.6222
2022-07-09 02:47:15 - train: epoch 0103, iter [00400, 05004], lr: 0.005402, loss: 1.0432
2022-07-09 02:47:59 - train: epoch 0103, iter [00500, 05004], lr: 0.005390, loss: 0.5665
2022-07-09 02:48:44 - train: epoch 0103, iter [00600, 05004], lr: 0.005379, loss: 0.8178
2022-07-09 02:49:28 - train: epoch 0103, iter [00700, 05004], lr: 0.005367, loss: 0.8506
2022-07-09 02:50:13 - train: epoch 0103, iter [00800, 05004], lr: 0.005355, loss: 0.7314
2022-07-09 02:50:58 - train: epoch 0103, iter [00900, 05004], lr: 0.005343, loss: 0.8915
2022-07-09 02:51:42 - train: epoch 0103, iter [01000, 05004], lr: 0.005332, loss: 0.7723
2022-07-09 02:52:27 - train: epoch 0103, iter [01100, 05004], lr: 0.005320, loss: 0.7846
2022-07-09 02:53:12 - train: epoch 0103, iter [01200, 05004], lr: 0.005308, loss: 0.7216
2022-07-09 02:53:56 - train: epoch 0103, iter [01300, 05004], lr: 0.005296, loss: 1.0953
2022-07-09 02:54:41 - train: epoch 0103, iter [01400, 05004], lr: 0.005285, loss: 0.9631
2022-07-09 02:55:26 - train: epoch 0103, iter [01500, 05004], lr: 0.005273, loss: 0.6775
2022-07-09 02:56:11 - train: epoch 0103, iter [01600, 05004], lr: 0.005261, loss: 0.5971
2022-07-09 02:56:55 - train: epoch 0103, iter [01700, 05004], lr: 0.005250, loss: 0.8283
2022-07-09 02:57:40 - train: epoch 0103, iter [01800, 05004], lr: 0.005238, loss: 0.7317
2022-07-09 02:58:24 - train: epoch 0103, iter [01900, 05004], lr: 0.005226, loss: 0.8074
2022-07-09 02:59:09 - train: epoch 0103, iter [02000, 05004], lr: 0.005215, loss: 0.6637
2022-07-09 02:59:54 - train: epoch 0103, iter [02100, 05004], lr: 0.005203, loss: 0.8568
2022-07-09 03:00:38 - train: epoch 0103, iter [02200, 05004], lr: 0.005191, loss: 0.9302
2022-07-09 03:01:23 - train: epoch 0103, iter [02300, 05004], lr: 0.005180, loss: 0.7732
2022-07-09 03:02:08 - train: epoch 0103, iter [02400, 05004], lr: 0.005168, loss: 0.9902
2022-07-09 03:02:52 - train: epoch 0103, iter [02500, 05004], lr: 0.005157, loss: 0.8946
2022-07-09 03:03:37 - train: epoch 0103, iter [02600, 05004], lr: 0.005145, loss: 0.8393
2022-07-09 03:04:22 - train: epoch 0103, iter [02700, 05004], lr: 0.005133, loss: 0.7759
2022-07-09 03:05:07 - train: epoch 0103, iter [02800, 05004], lr: 0.005122, loss: 0.6462
2022-07-09 03:05:51 - train: epoch 0103, iter [02900, 05004], lr: 0.005110, loss: 0.8519
2022-07-09 03:06:36 - train: epoch 0103, iter [03000, 05004], lr: 0.005099, loss: 0.8588
2022-07-09 03:07:20 - train: epoch 0103, iter [03100, 05004], lr: 0.005087, loss: 0.7002
2022-07-09 03:08:05 - train: epoch 0103, iter [03200, 05004], lr: 0.005076, loss: 0.7940
2022-07-09 03:08:50 - train: epoch 0103, iter [03300, 05004], lr: 0.005064, loss: 0.9191
2022-07-09 03:09:34 - train: epoch 0103, iter [03400, 05004], lr: 0.005053, loss: 0.7960
2022-07-09 03:10:19 - train: epoch 0103, iter [03500, 05004], lr: 0.005042, loss: 0.7670
2022-07-09 03:11:04 - train: epoch 0103, iter [03600, 05004], lr: 0.005030, loss: 0.7843
2022-07-09 03:11:48 - train: epoch 0103, iter [03700, 05004], lr: 0.005019, loss: 0.9433
2022-07-09 03:12:33 - train: epoch 0103, iter [03800, 05004], lr: 0.005007, loss: 0.8233
2022-07-09 03:13:18 - train: epoch 0103, iter [03900, 05004], lr: 0.004996, loss: 0.8874
2022-07-09 03:14:02 - train: epoch 0103, iter [04000, 05004], lr: 0.004984, loss: 0.9769
2022-07-09 03:14:47 - train: epoch 0103, iter [04100, 05004], lr: 0.004973, loss: 0.6705
2022-07-09 03:15:32 - train: epoch 0103, iter [04200, 05004], lr: 0.004962, loss: 0.8463
2022-07-09 03:16:16 - train: epoch 0103, iter [04300, 05004], lr: 0.004950, loss: 0.7512
2022-07-09 03:17:01 - train: epoch 0103, iter [04400, 05004], lr: 0.004939, loss: 0.9297
2022-07-09 03:17:47 - train: epoch 0103, iter [04500, 05004], lr: 0.004928, loss: 0.8796
2022-07-09 03:18:32 - train: epoch 0103, iter [04600, 05004], lr: 0.004916, loss: 0.8830
2022-07-09 03:19:17 - train: epoch 0103, iter [04700, 05004], lr: 0.004905, loss: 0.7556
2022-07-09 03:20:02 - train: epoch 0103, iter [04800, 05004], lr: 0.004894, loss: 0.9956
2022-07-09 03:20:47 - train: epoch 0103, iter [04900, 05004], lr: 0.004882, loss: 0.8472
2022-07-09 03:21:32 - train: epoch 0103, iter [05000, 05004], lr: 0.004871, loss: 0.9760
2022-07-09 03:21:34 - train: epoch 103, train_loss: 0.8156
2022-07-09 03:22:50 - eval: epoch: 103, acc1: 76.622%, acc5: 93.378%, test_loss: 0.9428, per_image_load_time: 2.055ms, per_image_inference_time: 0.843ms
2022-07-09 03:22:51 - until epoch: 103, best_acc1: 76.622%
2022-07-09 03:22:51 - epoch 104 lr: 0.004871
2022-07-09 03:23:42 - train: epoch 0104, iter [00100, 05004], lr: 0.004859, loss: 0.7853
2022-07-09 03:24:27 - train: epoch 0104, iter [00200, 05004], lr: 0.004848, loss: 0.6595
2022-07-09 03:25:13 - train: epoch 0104, iter [00300, 05004], lr: 0.004837, loss: 0.7509
2022-07-09 03:25:58 - train: epoch 0104, iter [00400, 05004], lr: 0.004826, loss: 0.9717
2022-07-09 03:26:44 - train: epoch 0104, iter [00500, 05004], lr: 0.004815, loss: 0.6825
2022-07-09 03:27:30 - train: epoch 0104, iter [00600, 05004], lr: 0.004803, loss: 0.8891
2022-07-09 03:28:16 - train: epoch 0104, iter [00700, 05004], lr: 0.004792, loss: 0.9438
2022-07-09 03:29:02 - train: epoch 0104, iter [00800, 05004], lr: 0.004781, loss: 0.7965
2022-07-09 03:29:47 - train: epoch 0104, iter [00900, 05004], lr: 0.004770, loss: 0.7213
2022-07-09 03:30:33 - train: epoch 0104, iter [01000, 05004], lr: 0.004759, loss: 0.7998
2022-07-09 03:31:19 - train: epoch 0104, iter [01100, 05004], lr: 0.004748, loss: 0.7415
2022-07-09 03:32:05 - train: epoch 0104, iter [01200, 05004], lr: 0.004736, loss: 0.7377
2022-07-09 03:32:51 - train: epoch 0104, iter [01300, 05004], lr: 0.004725, loss: 0.7940
2022-07-09 03:33:37 - train: epoch 0104, iter [01400, 05004], lr: 0.004714, loss: 0.7864
2022-07-09 03:34:23 - train: epoch 0104, iter [01500, 05004], lr: 0.004703, loss: 0.8198
2022-07-09 03:35:09 - train: epoch 0104, iter [01600, 05004], lr: 0.004692, loss: 0.6380
2022-07-09 03:35:54 - train: epoch 0104, iter [01700, 05004], lr: 0.004681, loss: 0.7514
2022-07-09 03:36:40 - train: epoch 0104, iter [01800, 05004], lr: 0.004670, loss: 0.5945
2022-07-09 03:37:26 - train: epoch 0104, iter [01900, 05004], lr: 0.004659, loss: 0.5731
2022-07-09 03:38:11 - train: epoch 0104, iter [02000, 05004], lr: 0.004648, loss: 0.7431
2022-07-09 03:38:57 - train: epoch 0104, iter [02100, 05004], lr: 0.004637, loss: 0.7151
2022-07-09 03:39:43 - train: epoch 0104, iter [02200, 05004], lr: 0.004626, loss: 0.7755
2022-07-09 03:40:28 - train: epoch 0104, iter [02300, 05004], lr: 0.004615, loss: 0.7273
2022-07-09 03:41:14 - train: epoch 0104, iter [02400, 05004], lr: 0.004604, loss: 0.8469
2022-07-09 03:41:59 - train: epoch 0104, iter [02500, 05004], lr: 0.004593, loss: 0.9497
2022-07-09 03:42:45 - train: epoch 0104, iter [02600, 05004], lr: 0.004582, loss: 0.7344
2022-07-09 03:43:31 - train: epoch 0104, iter [02700, 05004], lr: 0.004571, loss: 0.9892
2022-07-09 03:44:16 - train: epoch 0104, iter [02800, 05004], lr: 0.004560, loss: 0.8166
2022-07-09 03:45:02 - train: epoch 0104, iter [02900, 05004], lr: 0.004549, loss: 0.9813
2022-07-09 03:45:48 - train: epoch 0104, iter [03000, 05004], lr: 0.004538, loss: 0.8762
2022-07-09 03:46:34 - train: epoch 0104, iter [03100, 05004], lr: 0.004528, loss: 0.7309
2022-07-09 03:47:20 - train: epoch 0104, iter [03200, 05004], lr: 0.004517, loss: 0.7437
2022-07-09 03:48:05 - train: epoch 0104, iter [03300, 05004], lr: 0.004506, loss: 0.7957
2022-07-09 03:48:51 - train: epoch 0104, iter [03400, 05004], lr: 0.004495, loss: 0.6402
2022-07-09 03:49:36 - train: epoch 0104, iter [03500, 05004], lr: 0.004484, loss: 0.7513
2022-07-09 03:50:22 - train: epoch 0104, iter [03600, 05004], lr: 0.004473, loss: 0.7881
2022-07-09 03:51:07 - train: epoch 0104, iter [03700, 05004], lr: 0.004463, loss: 0.8748
2022-07-09 03:51:53 - train: epoch 0104, iter [03800, 05004], lr: 0.004452, loss: 0.6983
2022-07-09 03:52:39 - train: epoch 0104, iter [03900, 05004], lr: 0.004441, loss: 0.7927
2022-07-09 03:53:24 - train: epoch 0104, iter [04000, 05004], lr: 0.004430, loss: 0.9510
2022-07-09 03:54:10 - train: epoch 0104, iter [04100, 05004], lr: 0.004419, loss: 0.6519
2022-07-09 03:54:55 - train: epoch 0104, iter [04200, 05004], lr: 0.004409, loss: 0.8331
2022-07-09 03:55:41 - train: epoch 0104, iter [04300, 05004], lr: 0.004398, loss: 0.8112
2022-07-09 03:56:26 - train: epoch 0104, iter [04400, 05004], lr: 0.004387, loss: 0.7857
2022-07-09 03:57:12 - train: epoch 0104, iter [04500, 05004], lr: 0.004377, loss: 0.9373
2022-07-09 03:57:58 - train: epoch 0104, iter [04600, 05004], lr: 0.004366, loss: 0.8417
2022-07-09 03:58:43 - train: epoch 0104, iter [04700, 05004], lr: 0.004355, loss: 0.8212
2022-07-09 03:59:29 - train: epoch 0104, iter [04800, 05004], lr: 0.004344, loss: 0.9584
2022-07-09 04:00:15 - train: epoch 0104, iter [04900, 05004], lr: 0.004334, loss: 0.6525
2022-07-09 04:01:01 - train: epoch 0104, iter [05000, 05004], lr: 0.004323, loss: 0.9126
2022-07-09 04:01:03 - train: epoch 104, train_loss: 0.7946
2022-07-09 04:02:19 - eval: epoch: 104, acc1: 76.772%, acc5: 93.434%, test_loss: 0.9358, per_image_load_time: 0.680ms, per_image_inference_time: 0.851ms
2022-07-09 04:02:20 - until epoch: 104, best_acc1: 76.772%
2022-07-09 04:02:20 - epoch 105 lr: 0.004323
2022-07-09 04:03:11 - train: epoch 0105, iter [00100, 05004], lr: 0.004312, loss: 0.7165
2022-07-09 04:03:55 - train: epoch 0105, iter [00200, 05004], lr: 0.004301, loss: 0.8961
2022-07-09 04:04:40 - train: epoch 0105, iter [00300, 05004], lr: 0.004291, loss: 0.6658
2022-07-09 04:05:25 - train: epoch 0105, iter [00400, 05004], lr: 0.004280, loss: 0.6988
2022-07-09 04:06:10 - train: epoch 0105, iter [00500, 05004], lr: 0.004270, loss: 0.8294
2022-07-09 04:06:55 - train: epoch 0105, iter [00600, 05004], lr: 0.004259, loss: 0.8462
2022-07-09 04:07:39 - train: epoch 0105, iter [00700, 05004], lr: 0.004249, loss: 0.6937
2022-07-09 04:08:24 - train: epoch 0105, iter [00800, 05004], lr: 0.004238, loss: 0.7817
2022-07-09 04:09:09 - train: epoch 0105, iter [00900, 05004], lr: 0.004227, loss: 0.7404
2022-07-09 04:09:54 - train: epoch 0105, iter [01000, 05004], lr: 0.004217, loss: 0.6319
2022-07-09 04:10:38 - train: epoch 0105, iter [01100, 05004], lr: 0.004206, loss: 0.8549
2022-07-09 04:11:23 - train: epoch 0105, iter [01200, 05004], lr: 0.004196, loss: 0.8174
2022-07-09 04:12:08 - train: epoch 0105, iter [01300, 05004], lr: 0.004185, loss: 0.5983
2022-07-09 04:12:53 - train: epoch 0105, iter [01400, 05004], lr: 0.004175, loss: 0.7961
2022-07-09 04:13:37 - train: epoch 0105, iter [01500, 05004], lr: 0.004165, loss: 0.8113
2022-07-09 04:14:22 - train: epoch 0105, iter [01600, 05004], lr: 0.004154, loss: 0.5887
2022-07-09 04:15:07 - train: epoch 0105, iter [01700, 05004], lr: 0.004144, loss: 0.7754
2022-07-09 04:15:52 - train: epoch 0105, iter [01800, 05004], lr: 0.004133, loss: 0.9694
2022-07-09 04:16:37 - train: epoch 0105, iter [01900, 05004], lr: 0.004123, loss: 0.6867
2022-07-09 04:17:22 - train: epoch 0105, iter [02000, 05004], lr: 0.004112, loss: 0.7400
2022-07-09 04:18:07 - train: epoch 0105, iter [02100, 05004], lr: 0.004102, loss: 0.7514
2022-07-09 04:18:52 - train: epoch 0105, iter [02200, 05004], lr: 0.004092, loss: 0.9908
2022-07-09 04:19:37 - train: epoch 0105, iter [02300, 05004], lr: 0.004081, loss: 0.7578
2022-07-09 04:20:22 - train: epoch 0105, iter [02400, 05004], lr: 0.004071, loss: 0.6434
2022-07-09 04:21:07 - train: epoch 0105, iter [02500, 05004], lr: 0.004061, loss: 0.7271
2022-07-09 04:21:52 - train: epoch 0105, iter [02600, 05004], lr: 0.004050, loss: 0.7674
2022-07-09 04:22:37 - train: epoch 0105, iter [02700, 05004], lr: 0.004040, loss: 0.7388
2022-07-09 04:23:22 - train: epoch 0105, iter [02800, 05004], lr: 0.004030, loss: 0.8643
2022-07-09 04:24:07 - train: epoch 0105, iter [02900, 05004], lr: 0.004019, loss: 0.6995
2022-07-09 04:24:52 - train: epoch 0105, iter [03000, 05004], lr: 0.004009, loss: 0.8479
2022-07-09 04:25:37 - train: epoch 0105, iter [03100, 05004], lr: 0.003999, loss: 1.0290
2022-07-09 04:26:22 - train: epoch 0105, iter [03200, 05004], lr: 0.003989, loss: 0.9474
2022-07-09 04:27:07 - train: epoch 0105, iter [03300, 05004], lr: 0.003978, loss: 0.8829
2022-07-09 04:27:52 - train: epoch 0105, iter [03400, 05004], lr: 0.003968, loss: 0.7506
2022-07-09 04:28:37 - train: epoch 0105, iter [03500, 05004], lr: 0.003958, loss: 0.7924
2022-07-09 04:29:22 - train: epoch 0105, iter [03600, 05004], lr: 0.003948, loss: 0.9185
2022-07-09 04:30:07 - train: epoch 0105, iter [03700, 05004], lr: 0.003938, loss: 0.7929
2022-07-09 04:30:52 - train: epoch 0105, iter [03800, 05004], lr: 0.003927, loss: 0.8683
2022-07-09 04:31:37 - train: epoch 0105, iter [03900, 05004], lr: 0.003917, loss: 0.8822
2022-07-09 04:32:23 - train: epoch 0105, iter [04000, 05004], lr: 0.003907, loss: 0.8898
2022-07-09 04:33:08 - train: epoch 0105, iter [04100, 05004], lr: 0.003897, loss: 0.7751
2022-07-09 04:33:52 - train: epoch 0105, iter [04200, 05004], lr: 0.003887, loss: 0.7424
2022-07-09 04:34:37 - train: epoch 0105, iter [04300, 05004], lr: 0.003877, loss: 0.6515
2022-07-09 04:35:22 - train: epoch 0105, iter [04400, 05004], lr: 0.003867, loss: 0.8222
2022-07-09 04:36:07 - train: epoch 0105, iter [04500, 05004], lr: 0.003857, loss: 0.8571
2022-07-09 04:36:52 - train: epoch 0105, iter [04600, 05004], lr: 0.003847, loss: 0.9467
2022-07-09 04:37:37 - train: epoch 0105, iter [04700, 05004], lr: 0.003837, loss: 0.7305
2022-07-09 04:38:22 - train: epoch 0105, iter [04800, 05004], lr: 0.003826, loss: 0.6034
2022-07-09 04:39:07 - train: epoch 0105, iter [04900, 05004], lr: 0.003816, loss: 0.9352
2022-07-09 04:39:52 - train: epoch 0105, iter [05000, 05004], lr: 0.003806, loss: 0.6668
2022-07-09 04:39:55 - train: epoch 105, train_loss: 0.7731
2022-07-09 04:41:10 - eval: epoch: 105, acc1: 77.156%, acc5: 93.454%, test_loss: 0.9199, per_image_load_time: 0.883ms, per_image_inference_time: 0.854ms
2022-07-09 04:41:12 - until epoch: 105, best_acc1: 77.156%
2022-07-09 04:41:12 - epoch 106 lr: 0.003806
2022-07-09 04:42:03 - train: epoch 0106, iter [00100, 05004], lr: 0.003796, loss: 0.7517
2022-07-09 04:42:47 - train: epoch 0106, iter [00200, 05004], lr: 0.003786, loss: 0.5717
2022-07-09 04:43:32 - train: epoch 0106, iter [00300, 05004], lr: 0.003776, loss: 0.6105
2022-07-09 04:44:17 - train: epoch 0106, iter [00400, 05004], lr: 0.003766, loss: 0.8024
2022-07-09 04:45:02 - train: epoch 0106, iter [00500, 05004], lr: 0.003756, loss: 0.8405
2022-07-09 04:45:47 - train: epoch 0106, iter [00600, 05004], lr: 0.003746, loss: 0.7819
2022-07-09 04:46:31 - train: epoch 0106, iter [00700, 05004], lr: 0.003736, loss: 0.7266
2022-07-09 04:47:16 - train: epoch 0106, iter [00800, 05004], lr: 0.003726, loss: 0.6135
2022-07-09 04:48:01 - train: epoch 0106, iter [00900, 05004], lr: 0.003716, loss: 0.7446
2022-07-09 04:48:46 - train: epoch 0106, iter [01000, 05004], lr: 0.003707, loss: 0.8092
2022-07-09 04:49:31 - train: epoch 0106, iter [01100, 05004], lr: 0.003697, loss: 0.6538
2022-07-09 04:50:15 - train: epoch 0106, iter [01200, 05004], lr: 0.003687, loss: 0.7974
2022-07-09 04:51:00 - train: epoch 0106, iter [01300, 05004], lr: 0.003677, loss: 0.6821
2022-07-09 04:51:45 - train: epoch 0106, iter [01400, 05004], lr: 0.003667, loss: 0.7584
2022-07-09 04:52:30 - train: epoch 0106, iter [01500, 05004], lr: 0.003657, loss: 0.8123
2022-07-09 04:53:15 - train: epoch 0106, iter [01600, 05004], lr: 0.003647, loss: 0.8462
2022-07-09 04:54:00 - train: epoch 0106, iter [01700, 05004], lr: 0.003638, loss: 0.7151
2022-07-09 04:54:44 - train: epoch 0106, iter [01800, 05004], lr: 0.003628, loss: 0.6428
2022-07-09 04:55:29 - train: epoch 0106, iter [01900, 05004], lr: 0.003618, loss: 0.6044
2022-07-09 04:56:14 - train: epoch 0106, iter [02000, 05004], lr: 0.003608, loss: 0.7110
2022-07-09 04:56:59 - train: epoch 0106, iter [02100, 05004], lr: 0.003599, loss: 0.6778
2022-07-09 04:57:43 - train: epoch 0106, iter [02200, 05004], lr: 0.003589, loss: 0.8550
2022-07-09 04:58:28 - train: epoch 0106, iter [02300, 05004], lr: 0.003579, loss: 0.7984
2022-07-09 04:59:13 - train: epoch 0106, iter [02400, 05004], lr: 0.003569, loss: 0.7112
2022-07-09 04:59:58 - train: epoch 0106, iter [02500, 05004], lr: 0.003560, loss: 0.8223
2022-07-09 05:00:43 - train: epoch 0106, iter [02600, 05004], lr: 0.003550, loss: 0.6604
2022-07-09 05:01:28 - train: epoch 0106, iter [02700, 05004], lr: 0.003540, loss: 0.7947
2022-07-09 05:02:13 - train: epoch 0106, iter [02800, 05004], lr: 0.003531, loss: 0.7081
2022-07-09 05:02:58 - train: epoch 0106, iter [02900, 05004], lr: 0.003521, loss: 0.6498
2022-07-09 05:03:43 - train: epoch 0106, iter [03000, 05004], lr: 0.003511, loss: 0.7287
2022-07-09 05:04:28 - train: epoch 0106, iter [03100, 05004], lr: 0.003502, loss: 0.7044
2022-07-09 05:05:13 - train: epoch 0106, iter [03200, 05004], lr: 0.003492, loss: 0.5045
2022-07-09 05:05:58 - train: epoch 0106, iter [03300, 05004], lr: 0.003483, loss: 0.8385
2022-07-09 05:06:43 - train: epoch 0106, iter [03400, 05004], lr: 0.003473, loss: 0.6681
2022-07-09 05:07:27 - train: epoch 0106, iter [03500, 05004], lr: 0.003463, loss: 0.6462
2022-07-09 05:08:12 - train: epoch 0106, iter [03600, 05004], lr: 0.003454, loss: 0.7358
2022-07-09 05:08:57 - train: epoch 0106, iter [03700, 05004], lr: 0.003444, loss: 0.7823
2022-07-09 05:09:42 - train: epoch 0106, iter [03800, 05004], lr: 0.003435, loss: 0.8373
2022-07-09 05:10:27 - train: epoch 0106, iter [03900, 05004], lr: 0.003425, loss: 0.7244
2022-07-09 05:11:12 - train: epoch 0106, iter [04000, 05004], lr: 0.003416, loss: 0.7367
2022-07-09 05:11:57 - train: epoch 0106, iter [04100, 05004], lr: 0.003406, loss: 0.8517
2022-07-09 05:12:42 - train: epoch 0106, iter [04200, 05004], lr: 0.003397, loss: 0.6810
2022-07-09 05:13:27 - train: epoch 0106, iter [04300, 05004], lr: 0.003387, loss: 0.6611
2022-07-09 05:14:12 - train: epoch 0106, iter [04400, 05004], lr: 0.003378, loss: 0.9280
2022-07-09 05:14:57 - train: epoch 0106, iter [04500, 05004], lr: 0.003368, loss: 0.7841
2022-07-09 05:15:41 - train: epoch 0106, iter [04600, 05004], lr: 0.003359, loss: 0.7810
2022-07-09 05:16:26 - train: epoch 0106, iter [04700, 05004], lr: 0.003350, loss: 0.6954
2022-07-09 05:17:11 - train: epoch 0106, iter [04800, 05004], lr: 0.003340, loss: 0.8408
2022-07-09 05:17:56 - train: epoch 0106, iter [04900, 05004], lr: 0.003331, loss: 0.6460
2022-07-09 05:18:41 - train: epoch 0106, iter [05000, 05004], lr: 0.003321, loss: 0.7126
2022-07-09 05:18:43 - train: epoch 106, train_loss: 0.7505
2022-07-09 05:19:59 - eval: epoch: 106, acc1: 77.268%, acc5: 93.668%, test_loss: 0.9166, per_image_load_time: 0.718ms, per_image_inference_time: 0.849ms
2022-07-09 05:20:01 - until epoch: 106, best_acc1: 77.268%
2022-07-09 05:20:01 - epoch 107 lr: 0.003321
2022-07-09 05:20:51 - train: epoch 0107, iter [00100, 05004], lr: 0.003312, loss: 0.6541
2022-07-09 05:21:36 - train: epoch 0107, iter [00200, 05004], lr: 0.003302, loss: 0.9253
2022-07-09 05:22:21 - train: epoch 0107, iter [00300, 05004], lr: 0.003293, loss: 0.8974
2022-07-09 05:23:06 - train: epoch 0107, iter [00400, 05004], lr: 0.003284, loss: 0.7357
2022-07-09 05:23:51 - train: epoch 0107, iter [00500, 05004], lr: 0.003274, loss: 0.7385
2022-07-09 05:24:36 - train: epoch 0107, iter [00600, 05004], lr: 0.003265, loss: 0.5777
2022-07-09 05:25:21 - train: epoch 0107, iter [00700, 05004], lr: 0.003256, loss: 0.5869
2022-07-09 05:26:06 - train: epoch 0107, iter [00800, 05004], lr: 0.003246, loss: 0.6833
2022-07-09 05:26:51 - train: epoch 0107, iter [00900, 05004], lr: 0.003237, loss: 0.8057
2022-07-09 05:27:36 - train: epoch 0107, iter [01000, 05004], lr: 0.003228, loss: 0.8075
2022-07-09 05:28:20 - train: epoch 0107, iter [01100, 05004], lr: 0.003219, loss: 0.7042
2022-07-09 05:29:06 - train: epoch 0107, iter [01200, 05004], lr: 0.003209, loss: 0.8083
2022-07-09 05:29:51 - train: epoch 0107, iter [01300, 05004], lr: 0.003200, loss: 0.6263
2022-07-09 05:30:35 - train: epoch 0107, iter [01400, 05004], lr: 0.003191, loss: 0.7125
2022-07-09 05:31:20 - train: epoch 0107, iter [01500, 05004], lr: 0.003182, loss: 0.7403
2022-07-09 05:32:05 - train: epoch 0107, iter [01600, 05004], lr: 0.003173, loss: 0.7599
2022-07-09 05:32:50 - train: epoch 0107, iter [01700, 05004], lr: 0.003163, loss: 0.6885
2022-07-09 05:33:35 - train: epoch 0107, iter [01800, 05004], lr: 0.003154, loss: 0.7916
2022-07-09 05:34:20 - train: epoch 0107, iter [01900, 05004], lr: 0.003145, loss: 0.6726
2022-07-09 05:35:05 - train: epoch 0107, iter [02000, 05004], lr: 0.003136, loss: 0.7977
2022-07-09 05:35:50 - train: epoch 0107, iter [02100, 05004], lr: 0.003127, loss: 0.6583
2022-07-09 05:36:34 - train: epoch 0107, iter [02200, 05004], lr: 0.003118, loss: 0.7826
2022-07-09 05:37:19 - train: epoch 0107, iter [02300, 05004], lr: 0.003109, loss: 0.7607
2022-07-09 05:38:04 - train: epoch 0107, iter [02400, 05004], lr: 0.003100, loss: 0.7434
2022-07-09 05:38:49 - train: epoch 0107, iter [02500, 05004], lr: 0.003091, loss: 0.6434
2022-07-09 05:39:34 - train: epoch 0107, iter [02600, 05004], lr: 0.003082, loss: 0.8491
2022-07-09 05:40:19 - train: epoch 0107, iter [02700, 05004], lr: 0.003073, loss: 0.5749
2022-07-09 05:41:04 - train: epoch 0107, iter [02800, 05004], lr: 0.003064, loss: 0.7905
2022-07-09 05:41:49 - train: epoch 0107, iter [02900, 05004], lr: 0.003054, loss: 0.6619
2022-07-09 05:42:34 - train: epoch 0107, iter [03000, 05004], lr: 0.003046, loss: 0.6674
2022-07-09 05:43:19 - train: epoch 0107, iter [03100, 05004], lr: 0.003037, loss: 0.8183
2022-07-09 05:44:04 - train: epoch 0107, iter [03200, 05004], lr: 0.003028, loss: 0.6567
2022-07-09 05:44:49 - train: epoch 0107, iter [03300, 05004], lr: 0.003019, loss: 0.9141
2022-07-09 05:45:33 - train: epoch 0107, iter [03400, 05004], lr: 0.003010, loss: 0.7802
2022-07-09 05:46:18 - train: epoch 0107, iter [03500, 05004], lr: 0.003001, loss: 0.6579
2022-07-09 05:47:03 - train: epoch 0107, iter [03600, 05004], lr: 0.002992, loss: 0.8171
2022-07-09 05:47:48 - train: epoch 0107, iter [03700, 05004], lr: 0.002983, loss: 0.7401
2022-07-09 05:48:33 - train: epoch 0107, iter [03800, 05004], lr: 0.002974, loss: 0.6573
2022-07-09 05:49:18 - train: epoch 0107, iter [03900, 05004], lr: 0.002965, loss: 0.6193
2022-07-09 05:50:03 - train: epoch 0107, iter [04000, 05004], lr: 0.002956, loss: 0.6831
2022-07-09 05:50:48 - train: epoch 0107, iter [04100, 05004], lr: 0.002947, loss: 0.8319
2022-07-09 05:51:33 - train: epoch 0107, iter [04200, 05004], lr: 0.002939, loss: 0.7359
2022-07-09 05:52:18 - train: epoch 0107, iter [04300, 05004], lr: 0.002930, loss: 0.7383
2022-07-09 05:53:03 - train: epoch 0107, iter [04400, 05004], lr: 0.002921, loss: 0.6168
2022-07-09 05:53:48 - train: epoch 0107, iter [04500, 05004], lr: 0.002912, loss: 0.6281
2022-07-09 05:54:33 - train: epoch 0107, iter [04600, 05004], lr: 0.002903, loss: 0.6834
2022-07-09 05:55:18 - train: epoch 0107, iter [04700, 05004], lr: 0.002895, loss: 0.7896
2022-07-09 05:56:03 - train: epoch 0107, iter [04800, 05004], lr: 0.002886, loss: 0.6035
2022-07-09 05:56:48 - train: epoch 0107, iter [04900, 05004], lr: 0.002877, loss: 0.6911
2022-07-09 05:57:32 - train: epoch 0107, iter [05000, 05004], lr: 0.002868, loss: 0.6730
2022-07-09 05:57:35 - train: epoch 107, train_loss: 0.7339
2022-07-09 05:58:51 - eval: epoch: 107, acc1: 77.164%, acc5: 93.662%, test_loss: 0.9147, per_image_load_time: 1.177ms, per_image_inference_time: 0.841ms
2022-07-09 05:58:52 - until epoch: 107, best_acc1: 77.268%
2022-07-09 05:58:52 - epoch 108 lr: 0.002868
2022-07-09 05:59:43 - train: epoch 0108, iter [00100, 05004], lr: 0.002859, loss: 0.6498
2022-07-09 06:00:28 - train: epoch 0108, iter [00200, 05004], lr: 0.002850, loss: 0.7400
2022-07-09 06:01:13 - train: epoch 0108, iter [00300, 05004], lr: 0.002842, loss: 0.6294
2022-07-09 06:01:58 - train: epoch 0108, iter [00400, 05004], lr: 0.002833, loss: 0.7982
2022-07-09 06:02:43 - train: epoch 0108, iter [00500, 05004], lr: 0.002824, loss: 0.6451
2022-07-09 06:03:28 - train: epoch 0108, iter [00600, 05004], lr: 0.002816, loss: 0.7948
2022-07-09 06:04:13 - train: epoch 0108, iter [00700, 05004], lr: 0.002807, loss: 0.7097
2022-07-09 06:04:58 - train: epoch 0108, iter [00800, 05004], lr: 0.002798, loss: 0.6849
2022-07-09 06:05:42 - train: epoch 0108, iter [00900, 05004], lr: 0.002790, loss: 0.6048
2022-07-09 06:06:27 - train: epoch 0108, iter [01000, 05004], lr: 0.002781, loss: 0.7758
2022-07-09 06:07:12 - train: epoch 0108, iter [01100, 05004], lr: 0.002773, loss: 0.6315
2022-07-09 06:07:57 - train: epoch 0108, iter [01200, 05004], lr: 0.002764, loss: 0.8968
2022-07-09 06:08:42 - train: epoch 0108, iter [01300, 05004], lr: 0.002755, loss: 0.6574
2022-07-09 06:09:27 - train: epoch 0108, iter [01400, 05004], lr: 0.002747, loss: 0.7195
2022-07-09 06:10:12 - train: epoch 0108, iter [01500, 05004], lr: 0.002738, loss: 0.7925
2022-07-09 06:10:57 - train: epoch 0108, iter [01600, 05004], lr: 0.002730, loss: 0.6371
2022-07-09 06:11:42 - train: epoch 0108, iter [01700, 05004], lr: 0.002721, loss: 0.7246
2022-07-09 06:12:27 - train: epoch 0108, iter [01800, 05004], lr: 0.002713, loss: 0.7997
2022-07-09 06:13:12 - train: epoch 0108, iter [01900, 05004], lr: 0.002704, loss: 0.7987
2022-07-09 06:13:57 - train: epoch 0108, iter [02000, 05004], lr: 0.002696, loss: 0.6919
2022-07-09 06:14:42 - train: epoch 0108, iter [02100, 05004], lr: 0.002687, loss: 0.7358
2022-07-09 06:15:27 - train: epoch 0108, iter [02200, 05004], lr: 0.002679, loss: 0.7155
2022-07-09 06:16:12 - train: epoch 0108, iter [02300, 05004], lr: 0.002671, loss: 0.6467
2022-07-09 06:16:57 - train: epoch 0108, iter [02400, 05004], lr: 0.002662, loss: 0.6791
2022-07-09 06:17:42 - train: epoch 0108, iter [02500, 05004], lr: 0.002654, loss: 0.7755
2022-07-09 06:18:26 - train: epoch 0108, iter [02600, 05004], lr: 0.002645, loss: 0.6307
2022-07-09 06:19:11 - train: epoch 0108, iter [02700, 05004], lr: 0.002637, loss: 0.7423
2022-07-09 06:19:56 - train: epoch 0108, iter [02800, 05004], lr: 0.002628, loss: 0.7586
2022-07-09 06:20:41 - train: epoch 0108, iter [02900, 05004], lr: 0.002620, loss: 0.6697
2022-07-09 06:21:26 - train: epoch 0108, iter [03000, 05004], lr: 0.002612, loss: 0.7485
2022-07-09 06:22:11 - train: epoch 0108, iter [03100, 05004], lr: 0.002603, loss: 0.8991
2022-07-09 06:22:56 - train: epoch 0108, iter [03200, 05004], lr: 0.002595, loss: 0.6691
2022-07-09 06:23:41 - train: epoch 0108, iter [03300, 05004], lr: 0.002587, loss: 0.5417
2022-07-09 06:24:26 - train: epoch 0108, iter [03400, 05004], lr: 0.002579, loss: 0.7664
2022-07-09 06:25:10 - train: epoch 0108, iter [03500, 05004], lr: 0.002570, loss: 0.6185
2022-07-09 06:25:55 - train: epoch 0108, iter [03600, 05004], lr: 0.002562, loss: 0.7868
2022-07-09 06:26:40 - train: epoch 0108, iter [03700, 05004], lr: 0.002554, loss: 0.8223
2022-07-09 06:27:25 - train: epoch 0108, iter [03800, 05004], lr: 0.002545, loss: 0.6589
2022-07-09 06:28:10 - train: epoch 0108, iter [03900, 05004], lr: 0.002537, loss: 0.7555
2022-07-09 06:28:55 - train: epoch 0108, iter [04000, 05004], lr: 0.002529, loss: 0.7421
2022-07-09 06:29:40 - train: epoch 0108, iter [04100, 05004], lr: 0.002521, loss: 0.6353
2022-07-09 06:30:25 - train: epoch 0108, iter [04200, 05004], lr: 0.002513, loss: 0.7181
2022-07-09 06:31:10 - train: epoch 0108, iter [04300, 05004], lr: 0.002504, loss: 0.7060
2022-07-09 06:31:54 - train: epoch 0108, iter [04400, 05004], lr: 0.002496, loss: 0.7559
2022-07-09 06:32:39 - train: epoch 0108, iter [04500, 05004], lr: 0.002488, loss: 0.6287
2022-07-09 06:33:24 - train: epoch 0108, iter [04600, 05004], lr: 0.002480, loss: 0.6773
2022-07-09 06:34:09 - train: epoch 0108, iter [04700, 05004], lr: 0.002472, loss: 0.8215
2022-07-09 06:34:54 - train: epoch 0108, iter [04800, 05004], lr: 0.002464, loss: 0.6307
2022-07-09 06:35:39 - train: epoch 0108, iter [04900, 05004], lr: 0.002456, loss: 0.7029
2022-07-09 06:36:24 - train: epoch 0108, iter [05000, 05004], lr: 0.002447, loss: 0.6908
2022-07-09 06:36:26 - train: epoch 108, train_loss: 0.7148
2022-07-09 06:37:43 - eval: epoch: 108, acc1: 77.424%, acc5: 93.730%, test_loss: 0.9051, per_image_load_time: 1.002ms, per_image_inference_time: 0.826ms
2022-07-09 06:37:44 - until epoch: 108, best_acc1: 77.424%
2022-07-09 06:37:44 - epoch 109 lr: 0.002447
2022-07-09 06:38:35 - train: epoch 0109, iter [00100, 05004], lr: 0.002439, loss: 0.6718
2022-07-09 06:39:20 - train: epoch 0109, iter [00200, 05004], lr: 0.002431, loss: 0.6647
2022-07-09 06:40:04 - train: epoch 0109, iter [00300, 05004], lr: 0.002423, loss: 0.8154
2022-07-09 06:40:49 - train: epoch 0109, iter [00400, 05004], lr: 0.002415, loss: 0.6961
2022-07-09 06:41:34 - train: epoch 0109, iter [00500, 05004], lr: 0.002407, loss: 0.7242
2022-07-09 06:42:19 - train: epoch 0109, iter [00600, 05004], lr: 0.002399, loss: 0.7454
2022-07-09 06:43:04 - train: epoch 0109, iter [00700, 05004], lr: 0.002391, loss: 0.6453
2022-07-09 06:43:48 - train: epoch 0109, iter [00800, 05004], lr: 0.002383, loss: 0.5961
2022-07-09 06:44:33 - train: epoch 0109, iter [00900, 05004], lr: 0.002375, loss: 0.7767
2022-07-09 06:45:18 - train: epoch 0109, iter [01000, 05004], lr: 0.002367, loss: 0.5309
2022-07-09 06:46:03 - train: epoch 0109, iter [01100, 05004], lr: 0.002359, loss: 0.6303
2022-07-09 06:46:48 - train: epoch 0109, iter [01200, 05004], lr: 0.002351, loss: 0.7088
2022-07-09 06:47:32 - train: epoch 0109, iter [01300, 05004], lr: 0.002343, loss: 0.5938
2022-07-09 06:48:17 - train: epoch 0109, iter [01400, 05004], lr: 0.002335, loss: 0.6874
2022-07-09 06:49:02 - train: epoch 0109, iter [01500, 05004], lr: 0.002327, loss: 0.7032
2022-07-09 06:49:47 - train: epoch 0109, iter [01600, 05004], lr: 0.002320, loss: 0.5483
2022-07-09 06:50:32 - train: epoch 0109, iter [01700, 05004], lr: 0.002312, loss: 0.7884
2022-07-09 06:51:16 - train: epoch 0109, iter [01800, 05004], lr: 0.002304, loss: 0.6535
2022-07-09 06:52:01 - train: epoch 0109, iter [01900, 05004], lr: 0.002296, loss: 0.7549
2022-07-09 06:52:46 - train: epoch 0109, iter [02000, 05004], lr: 0.002288, loss: 0.6424
2022-07-09 06:53:31 - train: epoch 0109, iter [02100, 05004], lr: 0.002280, loss: 0.7108
2022-07-09 06:54:16 - train: epoch 0109, iter [02200, 05004], lr: 0.002272, loss: 0.6302
2022-07-09 06:55:00 - train: epoch 0109, iter [02300, 05004], lr: 0.002265, loss: 0.7506
2022-07-09 06:55:45 - train: epoch 0109, iter [02400, 05004], lr: 0.002257, loss: 0.7667
2022-07-09 06:56:30 - train: epoch 0109, iter [02500, 05004], lr: 0.002249, loss: 0.7585
2022-07-09 06:57:15 - train: epoch 0109, iter [02600, 05004], lr: 0.002241, loss: 0.8166
2022-07-09 06:58:00 - train: epoch 0109, iter [02700, 05004], lr: 0.002234, loss: 0.7858
2022-07-09 06:58:45 - train: epoch 0109, iter [02800, 05004], lr: 0.002226, loss: 0.8040
2022-07-09 06:59:29 - train: epoch 0109, iter [02900, 05004], lr: 0.002218, loss: 0.7206
2022-07-09 07:00:14 - train: epoch 0109, iter [03000, 05004], lr: 0.002211, loss: 0.7292
2022-07-09 07:00:59 - train: epoch 0109, iter [03100, 05004], lr: 0.002203, loss: 0.8508
2022-07-09 07:01:44 - train: epoch 0109, iter [03200, 05004], lr: 0.002195, loss: 0.5385
2022-07-09 07:02:29 - train: epoch 0109, iter [03300, 05004], lr: 0.002188, loss: 0.6312
2022-07-09 07:03:14 - train: epoch 0109, iter [03400, 05004], lr: 0.002180, loss: 0.8101
2022-07-09 07:03:58 - train: epoch 0109, iter [03500, 05004], lr: 0.002172, loss: 0.7130
2022-07-09 07:04:43 - train: epoch 0109, iter [03600, 05004], lr: 0.002165, loss: 0.6159
2022-07-09 07:05:28 - train: epoch 0109, iter [03700, 05004], lr: 0.002157, loss: 0.6682
2022-07-09 07:06:13 - train: epoch 0109, iter [03800, 05004], lr: 0.002149, loss: 0.6886
2022-07-09 07:06:58 - train: epoch 0109, iter [03900, 05004], lr: 0.002142, loss: 0.7308
2022-07-09 07:07:43 - train: epoch 0109, iter [04000, 05004], lr: 0.002134, loss: 0.7052
2022-07-09 07:08:28 - train: epoch 0109, iter [04100, 05004], lr: 0.002127, loss: 0.6913
2022-07-09 07:09:13 - train: epoch 0109, iter [04200, 05004], lr: 0.002119, loss: 0.6664
2022-07-09 07:09:58 - train: epoch 0109, iter [04300, 05004], lr: 0.002112, loss: 0.8889
2022-07-09 07:10:42 - train: epoch 0109, iter [04400, 05004], lr: 0.002104, loss: 0.6534
2022-07-09 07:11:27 - train: epoch 0109, iter [04500, 05004], lr: 0.002097, loss: 0.5946
2022-07-09 07:12:12 - train: epoch 0109, iter [04600, 05004], lr: 0.002089, loss: 0.7308
2022-07-09 07:12:57 - train: epoch 0109, iter [04700, 05004], lr: 0.002082, loss: 0.7008
2022-07-09 07:13:42 - train: epoch 0109, iter [04800, 05004], lr: 0.002074, loss: 0.7998
2022-07-09 07:14:27 - train: epoch 0109, iter [04900, 05004], lr: 0.002067, loss: 0.7523
2022-07-09 07:15:12 - train: epoch 0109, iter [05000, 05004], lr: 0.002059, loss: 0.6064
2022-07-09 07:15:15 - train: epoch 109, train_loss: 0.6973
2022-07-09 07:16:30 - eval: epoch: 109, acc1: 77.688%, acc5: 93.878%, test_loss: 0.8997, per_image_load_time: 1.197ms, per_image_inference_time: 0.830ms
2022-07-09 07:16:32 - until epoch: 109, best_acc1: 77.688%
2022-07-09 07:16:32 - epoch 110 lr: 0.002059
2022-07-09 07:17:23 - train: epoch 0110, iter [00100, 05004], lr: 0.002052, loss: 0.6054
2022-07-09 07:18:08 - train: epoch 0110, iter [00200, 05004], lr: 0.002044, loss: 0.6846
2022-07-09 07:18:52 - train: epoch 0110, iter [00300, 05004], lr: 0.002037, loss: 0.6544
2022-07-09 07:19:37 - train: epoch 0110, iter [00400, 05004], lr: 0.002029, loss: 0.6729
2022-07-09 07:20:22 - train: epoch 0110, iter [00500, 05004], lr: 0.002022, loss: 0.4895
2022-07-09 07:21:07 - train: epoch 0110, iter [00600, 05004], lr: 0.002015, loss: 0.7958
2022-07-09 07:21:52 - train: epoch 0110, iter [00700, 05004], lr: 0.002007, loss: 0.6496
2022-07-09 07:22:36 - train: epoch 0110, iter [00800, 05004], lr: 0.002000, loss: 0.6537
2022-07-09 07:23:21 - train: epoch 0110, iter [00900, 05004], lr: 0.001993, loss: 0.6005
2022-07-09 07:24:06 - train: epoch 0110, iter [01000, 05004], lr: 0.001985, loss: 0.6755
2022-07-09 07:24:51 - train: epoch 0110, iter [01100, 05004], lr: 0.001978, loss: 0.9642
2022-07-09 07:25:35 - train: epoch 0110, iter [01200, 05004], lr: 0.001971, loss: 0.6963
2022-07-09 07:26:20 - train: epoch 0110, iter [01300, 05004], lr: 0.001964, loss: 0.7744
2022-07-09 07:27:05 - train: epoch 0110, iter [01400, 05004], lr: 0.001956, loss: 0.7010
2022-07-09 07:27:50 - train: epoch 0110, iter [01500, 05004], lr: 0.001949, loss: 0.6621
2022-07-09 07:28:35 - train: epoch 0110, iter [01600, 05004], lr: 0.001942, loss: 0.6982
2022-07-09 07:29:20 - train: epoch 0110, iter [01700, 05004], lr: 0.001935, loss: 0.5854
2022-07-09 07:30:05 - train: epoch 0110, iter [01800, 05004], lr: 0.001927, loss: 0.5774
2022-07-09 07:30:49 - train: epoch 0110, iter [01900, 05004], lr: 0.001920, loss: 0.6048
2022-07-09 07:31:34 - train: epoch 0110, iter [02000, 05004], lr: 0.001913, loss: 0.5631
2022-07-09 07:32:19 - train: epoch 0110, iter [02100, 05004], lr: 0.001906, loss: 0.7414
2022-07-09 07:33:04 - train: epoch 0110, iter [02200, 05004], lr: 0.001899, loss: 0.6036
2022-07-09 07:33:48 - train: epoch 0110, iter [02300, 05004], lr: 0.001892, loss: 0.6230
2022-07-09 07:34:33 - train: epoch 0110, iter [02400, 05004], lr: 0.001884, loss: 0.6113
2022-07-09 07:35:18 - train: epoch 0110, iter [02500, 05004], lr: 0.001877, loss: 0.6014
2022-07-09 07:36:03 - train: epoch 0110, iter [02600, 05004], lr: 0.001870, loss: 0.7498
2022-07-09 07:36:48 - train: epoch 0110, iter [02700, 05004], lr: 0.001863, loss: 0.6848
2022-07-09 07:37:32 - train: epoch 0110, iter [02800, 05004], lr: 0.001856, loss: 0.6355
2022-07-09 07:38:17 - train: epoch 0110, iter [02900, 05004], lr: 0.001849, loss: 0.6318
2022-07-09 07:39:02 - train: epoch 0110, iter [03000, 05004], lr: 0.001842, loss: 0.5910
2022-07-09 07:39:47 - train: epoch 0110, iter [03100, 05004], lr: 0.001835, loss: 0.6505
2022-07-09 07:40:32 - train: epoch 0110, iter [03200, 05004], lr: 0.001828, loss: 0.7795
2022-07-09 07:41:16 - train: epoch 0110, iter [03300, 05004], lr: 0.001821, loss: 0.8391
2022-07-09 07:42:01 - train: epoch 0110, iter [03400, 05004], lr: 0.001814, loss: 0.6947
2022-07-09 07:42:46 - train: epoch 0110, iter [03500, 05004], lr: 0.001807, loss: 0.8764
2022-07-09 07:43:31 - train: epoch 0110, iter [03600, 05004], lr: 0.001800, loss: 0.7797
2022-07-09 07:44:16 - train: epoch 0110, iter [03700, 05004], lr: 0.001793, loss: 0.7795
2022-07-09 07:45:00 - train: epoch 0110, iter [03800, 05004], lr: 0.001786, loss: 0.6605
2022-07-09 07:45:45 - train: epoch 0110, iter [03900, 05004], lr: 0.001779, loss: 0.5501
2022-07-09 07:46:30 - train: epoch 0110, iter [04000, 05004], lr: 0.001772, loss: 0.7878
2022-07-09 07:47:15 - train: epoch 0110, iter [04100, 05004], lr: 0.001765, loss: 0.5916
2022-07-09 07:48:00 - train: epoch 0110, iter [04200, 05004], lr: 0.001759, loss: 0.6830
2022-07-09 07:48:45 - train: epoch 0110, iter [04300, 05004], lr: 0.001752, loss: 0.6891
2022-07-09 07:49:30 - train: epoch 0110, iter [04400, 05004], lr: 0.001745, loss: 0.6826
2022-07-09 07:50:15 - train: epoch 0110, iter [04500, 05004], lr: 0.001738, loss: 0.5744
2022-07-09 07:51:00 - train: epoch 0110, iter [04600, 05004], lr: 0.001731, loss: 0.6299
2022-07-09 07:51:45 - train: epoch 0110, iter [04700, 05004], lr: 0.001724, loss: 0.7613
2022-07-09 07:52:30 - train: epoch 0110, iter [04800, 05004], lr: 0.001718, loss: 0.7647
2022-07-09 07:53:14 - train: epoch 0110, iter [04900, 05004], lr: 0.001711, loss: 0.6510
2022-07-09 07:53:59 - train: epoch 0110, iter [05000, 05004], lr: 0.001704, loss: 0.7314
2022-07-09 07:54:02 - train: epoch 110, train_loss: 0.6790
2022-07-09 07:55:18 - eval: epoch: 110, acc1: 77.724%, acc5: 93.888%, test_loss: 0.8996, per_image_load_time: 0.640ms, per_image_inference_time: 0.829ms
2022-07-09 07:55:19 - until epoch: 110, best_acc1: 77.724%
2022-07-09 07:55:19 - epoch 111 lr: 0.001704
2022-07-09 07:56:10 - train: epoch 0111, iter [00100, 05004], lr: 0.001697, loss: 0.6233
2022-07-09 07:56:54 - train: epoch 0111, iter [00200, 05004], lr: 0.001690, loss: 0.5981
2022-07-09 07:57:39 - train: epoch 0111, iter [00300, 05004], lr: 0.001683, loss: 0.5499
2022-07-09 07:58:24 - train: epoch 0111, iter [00400, 05004], lr: 0.001677, loss: 0.6778
2022-07-09 07:59:08 - train: epoch 0111, iter [00500, 05004], lr: 0.001670, loss: 0.6776
2022-07-09 07:59:53 - train: epoch 0111, iter [00600, 05004], lr: 0.001663, loss: 0.7406
2022-07-09 08:00:38 - train: epoch 0111, iter [00700, 05004], lr: 0.001657, loss: 0.7049
2022-07-09 08:01:23 - train: epoch 0111, iter [00800, 05004], lr: 0.001650, loss: 0.6567
2022-07-09 08:02:08 - train: epoch 0111, iter [00900, 05004], lr: 0.001643, loss: 0.6198
2022-07-09 08:02:52 - train: epoch 0111, iter [01000, 05004], lr: 0.001637, loss: 0.6168
2022-07-09 08:03:37 - train: epoch 0111, iter [01100, 05004], lr: 0.001630, loss: 0.6541
2022-07-09 08:04:22 - train: epoch 0111, iter [01200, 05004], lr: 0.001623, loss: 0.5648
2022-07-09 08:05:07 - train: epoch 0111, iter [01300, 05004], lr: 0.001617, loss: 0.6706
2022-07-09 08:05:52 - train: epoch 0111, iter [01400, 05004], lr: 0.001610, loss: 0.6897
2022-07-09 08:06:37 - train: epoch 0111, iter [01500, 05004], lr: 0.001604, loss: 0.7210
2022-07-09 08:07:22 - train: epoch 0111, iter [01600, 05004], lr: 0.001597, loss: 0.5604
2022-07-09 08:08:07 - train: epoch 0111, iter [01700, 05004], lr: 0.001591, loss: 0.7099
2022-07-09 08:08:51 - train: epoch 0111, iter [01800, 05004], lr: 0.001584, loss: 0.5732
2022-07-09 08:09:36 - train: epoch 0111, iter [01900, 05004], lr: 0.001577, loss: 0.5706
2022-07-09 08:10:21 - train: epoch 0111, iter [02000, 05004], lr: 0.001571, loss: 0.7273
2022-07-09 08:11:06 - train: epoch 0111, iter [02100, 05004], lr: 0.001564, loss: 0.7409
2022-07-09 08:11:51 - train: epoch 0111, iter [02200, 05004], lr: 0.001558, loss: 0.7639
2022-07-09 08:12:36 - train: epoch 0111, iter [02300, 05004], lr: 0.001551, loss: 0.7792
2022-07-09 08:13:20 - train: epoch 0111, iter [02400, 05004], lr: 0.001545, loss: 0.6906
2022-07-09 08:14:05 - train: epoch 0111, iter [02500, 05004], lr: 0.001539, loss: 0.6575
2022-07-09 08:14:50 - train: epoch 0111, iter [02600, 05004], lr: 0.001532, loss: 0.5186
2022-07-09 08:15:35 - train: epoch 0111, iter [02700, 05004], lr: 0.001526, loss: 0.7461
2022-07-09 08:16:20 - train: epoch 0111, iter [02800, 05004], lr: 0.001519, loss: 0.7499
2022-07-09 08:17:05 - train: epoch 0111, iter [02900, 05004], lr: 0.001513, loss: 0.5755
2022-07-09 08:17:49 - train: epoch 0111, iter [03000, 05004], lr: 0.001507, loss: 0.8761
2022-07-09 08:18:34 - train: epoch 0111, iter [03100, 05004], lr: 0.001500, loss: 0.7047
2022-07-09 08:19:19 - train: epoch 0111, iter [03200, 05004], lr: 0.001494, loss: 0.6613
2022-07-09 08:20:04 - train: epoch 0111, iter [03300, 05004], lr: 0.001487, loss: 0.7143
2022-07-09 08:20:49 - train: epoch 0111, iter [03400, 05004], lr: 0.001481, loss: 0.6875
2022-07-09 08:21:34 - train: epoch 0111, iter [03500, 05004], lr: 0.001475, loss: 0.7706
2022-07-09 08:22:20 - train: epoch 0111, iter [03600, 05004], lr: 0.001469, loss: 0.7034
2022-07-09 08:23:04 - train: epoch 0111, iter [03700, 05004], lr: 0.001462, loss: 0.7544
2022-07-09 08:23:49 - train: epoch 0111, iter [03800, 05004], lr: 0.001456, loss: 0.6445
2022-07-09 08:24:34 - train: epoch 0111, iter [03900, 05004], lr: 0.001450, loss: 0.6733
2022-07-09 08:25:19 - train: epoch 0111, iter [04000, 05004], lr: 0.001443, loss: 0.6511
2022-07-09 08:26:04 - train: epoch 0111, iter [04100, 05004], lr: 0.001437, loss: 0.7765
2022-07-09 08:26:49 - train: epoch 0111, iter [04200, 05004], lr: 0.001431, loss: 0.6418
2022-07-09 08:27:34 - train: epoch 0111, iter [04300, 05004], lr: 0.001425, loss: 0.7192
2022-07-09 08:28:18 - train: epoch 0111, iter [04400, 05004], lr: 0.001419, loss: 0.6293
2022-07-09 08:29:03 - train: epoch 0111, iter [04500, 05004], lr: 0.001412, loss: 0.5858
2022-07-09 08:29:48 - train: epoch 0111, iter [04600, 05004], lr: 0.001406, loss: 0.5305
2022-07-09 08:30:34 - train: epoch 0111, iter [04700, 05004], lr: 0.001400, loss: 0.6150
2022-07-09 08:31:19 - train: epoch 0111, iter [04800, 05004], lr: 0.001394, loss: 0.5358
2022-07-09 08:32:04 - train: epoch 0111, iter [04900, 05004], lr: 0.001388, loss: 0.6457
2022-07-09 08:32:49 - train: epoch 0111, iter [05000, 05004], lr: 0.001382, loss: 0.7103
2022-07-09 08:32:51 - train: epoch 111, train_loss: 0.6640
2022-07-09 08:34:06 - eval: epoch: 111, acc1: 77.742%, acc5: 93.918%, test_loss: 0.8903, per_image_load_time: 0.676ms, per_image_inference_time: 0.840ms
2022-07-09 08:34:07 - until epoch: 111, best_acc1: 77.742%
2022-07-09 08:34:07 - epoch 112 lr: 0.001381
2022-07-09 08:34:58 - train: epoch 0112, iter [00100, 05004], lr: 0.001375, loss: 0.6524
2022-07-09 08:35:44 - train: epoch 0112, iter [00200, 05004], lr: 0.001369, loss: 0.7136
2022-07-09 08:36:30 - train: epoch 0112, iter [00300, 05004], lr: 0.001363, loss: 0.5522
2022-07-09 08:37:16 - train: epoch 0112, iter [00400, 05004], lr: 0.001357, loss: 0.6589
2022-07-09 08:38:01 - train: epoch 0112, iter [00500, 05004], lr: 0.001351, loss: 0.6965
2022-07-09 08:38:47 - train: epoch 0112, iter [00600, 05004], lr: 0.001345, loss: 0.4136
2022-07-09 08:39:33 - train: epoch 0112, iter [00700, 05004], lr: 0.001339, loss: 0.8269
2022-07-09 08:40:19 - train: epoch 0112, iter [00800, 05004], lr: 0.001333, loss: 0.7369
2022-07-09 08:41:05 - train: epoch 0112, iter [00900, 05004], lr: 0.001327, loss: 0.5043
2022-07-09 08:41:50 - train: epoch 0112, iter [01000, 05004], lr: 0.001321, loss: 0.5733
2022-07-09 08:42:36 - train: epoch 0112, iter [01100, 05004], lr: 0.001315, loss: 0.4346
2022-07-09 08:43:22 - train: epoch 0112, iter [01200, 05004], lr: 0.001309, loss: 0.5349
2022-07-09 08:44:08 - train: epoch 0112, iter [01300, 05004], lr: 0.001303, loss: 0.6273
2022-07-09 08:44:54 - train: epoch 0112, iter [01400, 05004], lr: 0.001297, loss: 0.5579
2022-07-09 08:45:39 - train: epoch 0112, iter [01500, 05004], lr: 0.001291, loss: 0.6831
2022-07-09 08:46:25 - train: epoch 0112, iter [01600, 05004], lr: 0.001286, loss: 0.7226
2022-07-09 08:47:11 - train: epoch 0112, iter [01700, 05004], lr: 0.001280, loss: 0.8244
2022-07-09 08:47:57 - train: epoch 0112, iter [01800, 05004], lr: 0.001274, loss: 0.5128
2022-07-09 08:48:42 - train: epoch 0112, iter [01900, 05004], lr: 0.001268, loss: 0.6454
2022-07-09 08:49:28 - train: epoch 0112, iter [02000, 05004], lr: 0.001262, loss: 0.7341
2022-07-09 08:50:14 - train: epoch 0112, iter [02100, 05004], lr: 0.001256, loss: 0.7351
2022-07-09 08:50:59 - train: epoch 0112, iter [02200, 05004], lr: 0.001250, loss: 0.6265
2022-07-09 08:51:45 - train: epoch 0112, iter [02300, 05004], lr: 0.001245, loss: 0.6419
2022-07-09 08:52:31 - train: epoch 0112, iter [02400, 05004], lr: 0.001239, loss: 0.6302
2022-07-09 08:53:17 - train: epoch 0112, iter [02500, 05004], lr: 0.001233, loss: 0.6797
2022-07-09 08:54:03 - train: epoch 0112, iter [02600, 05004], lr: 0.001227, loss: 0.6031
2022-07-09 08:54:48 - train: epoch 0112, iter [02700, 05004], lr: 0.001221, loss: 0.6825
2022-07-09 08:55:34 - train: epoch 0112, iter [02800, 05004], lr: 0.001216, loss: 0.6554
2022-07-09 08:56:20 - train: epoch 0112, iter [02900, 05004], lr: 0.001210, loss: 0.7422
2022-07-09 08:57:06 - train: epoch 0112, iter [03000, 05004], lr: 0.001204, loss: 0.7553
2022-07-09 08:57:52 - train: epoch 0112, iter [03100, 05004], lr: 0.001199, loss: 0.5073
2022-07-09 08:58:37 - train: epoch 0112, iter [03200, 05004], lr: 0.001193, loss: 0.6031
2022-07-09 08:59:23 - train: epoch 0112, iter [03300, 05004], lr: 0.001187, loss: 0.5930
2022-07-09 09:00:09 - train: epoch 0112, iter [03400, 05004], lr: 0.001182, loss: 0.6927
2022-07-09 09:00:55 - train: epoch 0112, iter [03500, 05004], lr: 0.001176, loss: 0.5170
2022-07-09 09:01:40 - train: epoch 0112, iter [03600, 05004], lr: 0.001170, loss: 0.6391
2022-07-09 09:02:26 - train: epoch 0112, iter [03700, 05004], lr: 0.001165, loss: 0.7028
2022-07-09 09:03:12 - train: epoch 0112, iter [03800, 05004], lr: 0.001159, loss: 0.5856
2022-07-09 09:03:57 - train: epoch 0112, iter [03900, 05004], lr: 0.001153, loss: 0.6010
2022-07-09 09:04:43 - train: epoch 0112, iter [04000, 05004], lr: 0.001148, loss: 0.6695
2022-07-09 09:05:28 - train: epoch 0112, iter [04100, 05004], lr: 0.001142, loss: 0.6587
2022-07-09 09:06:14 - train: epoch 0112, iter [04200, 05004], lr: 0.001137, loss: 0.6537
2022-07-09 09:07:00 - train: epoch 0112, iter [04300, 05004], lr: 0.001131, loss: 0.7197
2022-07-09 09:07:45 - train: epoch 0112, iter [04400, 05004], lr: 0.001126, loss: 0.7866
2022-07-09 09:08:31 - train: epoch 0112, iter [04500, 05004], lr: 0.001120, loss: 0.6212
2022-07-09 09:09:17 - train: epoch 0112, iter [04600, 05004], lr: 0.001115, loss: 0.5095
2022-07-09 09:10:03 - train: epoch 0112, iter [04700, 05004], lr: 0.001109, loss: 0.8448
2022-07-09 09:10:48 - train: epoch 0112, iter [04800, 05004], lr: 0.001104, loss: 0.7308
2022-07-09 09:11:34 - train: epoch 0112, iter [04900, 05004], lr: 0.001098, loss: 0.6467
2022-07-09 09:12:19 - train: epoch 0112, iter [05000, 05004], lr: 0.001093, loss: 0.5422
2022-07-09 09:12:22 - train: epoch 112, train_loss: 0.6473
2022-07-09 09:13:38 - eval: epoch: 112, acc1: 77.766%, acc5: 93.902%, test_loss: 0.8899, per_image_load_time: 1.991ms, per_image_inference_time: 0.832ms
2022-07-09 09:13:40 - until epoch: 112, best_acc1: 77.766%
2022-07-09 20:02:16 - epoch 113 lr: 0.001093
2022-07-09 20:03:08 - train: epoch 0113, iter [00100, 05004], lr: 0.001087, loss: 0.4969
2022-07-09 20:03:54 - train: epoch 0113, iter [00200, 05004], lr: 0.001082, loss: 0.5409
2022-07-09 20:04:40 - train: epoch 0113, iter [00300, 05004], lr: 0.001076, loss: 0.6358
2022-07-09 20:05:25 - train: epoch 0113, iter [00400, 05004], lr: 0.001071, loss: 0.5974
2022-07-09 20:06:11 - train: epoch 0113, iter [00500, 05004], lr: 0.001066, loss: 0.7427
2022-07-09 20:06:57 - train: epoch 0113, iter [00600, 05004], lr: 0.001060, loss: 0.5802
2022-07-09 20:07:43 - train: epoch 0113, iter [00700, 05004], lr: 0.001055, loss: 0.4966
2022-07-09 20:08:29 - train: epoch 0113, iter [00800, 05004], lr: 0.001050, loss: 0.5595
2022-07-09 20:09:15 - train: epoch 0113, iter [00900, 05004], lr: 0.001044, loss: 0.6939
2022-07-09 20:10:01 - train: epoch 0113, iter [01000, 05004], lr: 0.001039, loss: 0.5858
2022-07-09 20:10:46 - train: epoch 0113, iter [01100, 05004], lr: 0.001034, loss: 0.5971
2022-07-09 20:11:32 - train: epoch 0113, iter [01200, 05004], lr: 0.001028, loss: 0.7635
2022-07-09 20:12:18 - train: epoch 0113, iter [01300, 05004], lr: 0.001023, loss: 0.4896
2022-07-09 20:13:04 - train: epoch 0113, iter [01400, 05004], lr: 0.001018, loss: 0.6661
2022-07-09 20:13:50 - train: epoch 0113, iter [01500, 05004], lr: 0.001013, loss: 0.6727
2022-07-09 20:14:36 - train: epoch 0113, iter [01600, 05004], lr: 0.001007, loss: 0.6712
2022-07-09 20:15:22 - train: epoch 0113, iter [01700, 05004], lr: 0.001002, loss: 0.5811
2022-07-09 20:16:07 - train: epoch 0113, iter [01800, 05004], lr: 0.000997, loss: 0.5507
2022-07-09 20:16:53 - train: epoch 0113, iter [01900, 05004], lr: 0.000992, loss: 0.4067
2022-07-09 20:17:38 - train: epoch 0113, iter [02000, 05004], lr: 0.000987, loss: 0.6398
2022-07-09 20:18:24 - train: epoch 0113, iter [02100, 05004], lr: 0.000981, loss: 0.7260
2022-07-09 20:19:10 - train: epoch 0113, iter [02200, 05004], lr: 0.000976, loss: 0.5064
2022-07-09 20:19:56 - train: epoch 0113, iter [02300, 05004], lr: 0.000971, loss: 0.6926
2022-07-09 20:20:41 - train: epoch 0113, iter [02400, 05004], lr: 0.000966, loss: 0.7138
2022-07-09 20:21:27 - train: epoch 0113, iter [02500, 05004], lr: 0.000961, loss: 0.6922
2022-07-09 20:22:13 - train: epoch 0113, iter [02600, 05004], lr: 0.000956, loss: 0.6077
2022-07-09 20:22:58 - train: epoch 0113, iter [02700, 05004], lr: 0.000951, loss: 0.5749
2022-07-09 20:23:44 - train: epoch 0113, iter [02800, 05004], lr: 0.000946, loss: 0.6259
2022-07-09 20:24:30 - train: epoch 0113, iter [02900, 05004], lr: 0.000941, loss: 0.5264
2022-07-09 20:25:15 - train: epoch 0113, iter [03000, 05004], lr: 0.000935, loss: 0.7239
2022-07-09 20:26:01 - train: epoch 0113, iter [03100, 05004], lr: 0.000930, loss: 0.5095
2022-07-09 20:26:46 - train: epoch 0113, iter [03200, 05004], lr: 0.000925, loss: 0.5701
2022-07-09 20:27:32 - train: epoch 0113, iter [03300, 05004], lr: 0.000920, loss: 0.5728
2022-07-09 20:28:18 - train: epoch 0113, iter [03400, 05004], lr: 0.000915, loss: 0.7209
2022-07-09 20:29:03 - train: epoch 0113, iter [03500, 05004], lr: 0.000910, loss: 0.7181
2022-07-09 20:29:49 - train: epoch 0113, iter [03600, 05004], lr: 0.000906, loss: 0.5378
2022-07-09 20:30:35 - train: epoch 0113, iter [03700, 05004], lr: 0.000901, loss: 0.4822
2022-07-09 20:31:20 - train: epoch 0113, iter [03800, 05004], lr: 0.000896, loss: 0.6230
2022-07-09 20:32:07 - train: epoch 0113, iter [03900, 05004], lr: 0.000891, loss: 0.7378
2022-07-09 20:32:52 - train: epoch 0113, iter [04000, 05004], lr: 0.000886, loss: 0.8701
2022-07-09 20:33:39 - train: epoch 0113, iter [04100, 05004], lr: 0.000881, loss: 0.7530
2022-07-09 20:34:25 - train: epoch 0113, iter [04200, 05004], lr: 0.000876, loss: 0.7864
2022-07-09 20:35:11 - train: epoch 0113, iter [04300, 05004], lr: 0.000871, loss: 0.6104
2022-07-09 20:35:57 - train: epoch 0113, iter [04400, 05004], lr: 0.000866, loss: 0.6172
2022-07-09 20:36:43 - train: epoch 0113, iter [04500, 05004], lr: 0.000861, loss: 0.6926
2022-07-09 20:37:29 - train: epoch 0113, iter [04600, 05004], lr: 0.000857, loss: 0.8171
2022-07-09 20:38:15 - train: epoch 0113, iter [04700, 05004], lr: 0.000852, loss: 0.5985
2022-07-09 20:39:01 - train: epoch 0113, iter [04800, 05004], lr: 0.000847, loss: 0.6268
2022-07-09 20:39:46 - train: epoch 0113, iter [04900, 05004], lr: 0.000842, loss: 0.6033
2022-07-09 20:40:32 - train: epoch 0113, iter [05000, 05004], lr: 0.000837, loss: 0.6137
2022-07-09 20:40:34 - train: epoch 113, train_loss: 0.6370
2022-07-09 20:41:50 - eval: epoch: 113, acc1: 77.960%, acc5: 93.906%, test_loss: 0.8893, per_image_load_time: 1.792ms, per_image_inference_time: 0.845ms
2022-07-09 20:41:51 - until epoch: 113, best_acc1: 77.960%
2022-07-09 20:41:51 - epoch 114 lr: 0.000837
2022-07-09 20:42:43 - train: epoch 0114, iter [00100, 05004], lr: 0.000832, loss: 0.7522
2022-07-09 20:43:29 - train: epoch 0114, iter [00200, 05004], lr: 0.000828, loss: 0.4891
2022-07-09 20:44:15 - train: epoch 0114, iter [00300, 05004], lr: 0.000823, loss: 0.4597
2022-07-09 20:45:01 - train: epoch 0114, iter [00400, 05004], lr: 0.000818, loss: 0.6627
2022-07-09 20:45:46 - train: epoch 0114, iter [00500, 05004], lr: 0.000814, loss: 0.6124
2022-07-09 20:46:32 - train: epoch 0114, iter [00600, 05004], lr: 0.000809, loss: 0.6020
2022-07-09 20:47:18 - train: epoch 0114, iter [00700, 05004], lr: 0.000804, loss: 0.5805
2022-07-09 20:48:04 - train: epoch 0114, iter [00800, 05004], lr: 0.000800, loss: 0.7156
2022-07-09 20:48:50 - train: epoch 0114, iter [00900, 05004], lr: 0.000795, loss: 0.6478
2022-07-09 20:49:36 - train: epoch 0114, iter [01000, 05004], lr: 0.000790, loss: 0.6470
2022-07-09 20:50:22 - train: epoch 0114, iter [01100, 05004], lr: 0.000786, loss: 0.6577
2022-07-09 20:51:07 - train: epoch 0114, iter [01200, 05004], lr: 0.000781, loss: 0.8007
2022-07-09 20:51:53 - train: epoch 0114, iter [01300, 05004], lr: 0.000776, loss: 0.4959
2022-07-09 20:52:39 - train: epoch 0114, iter [01400, 05004], lr: 0.000772, loss: 0.6286
2022-07-09 20:53:25 - train: epoch 0114, iter [01500, 05004], lr: 0.000767, loss: 0.5795
2022-07-09 20:54:11 - train: epoch 0114, iter [01600, 05004], lr: 0.000763, loss: 0.5638
2022-07-09 20:54:57 - train: epoch 0114, iter [01700, 05004], lr: 0.000758, loss: 0.5514
2022-07-09 20:55:42 - train: epoch 0114, iter [01800, 05004], lr: 0.000754, loss: 0.4972
2022-07-09 20:56:28 - train: epoch 0114, iter [01900, 05004], lr: 0.000749, loss: 0.5632
2022-07-09 20:57:14 - train: epoch 0114, iter [02000, 05004], lr: 0.000745, loss: 0.6433
2022-07-09 20:58:00 - train: epoch 0114, iter [02100, 05004], lr: 0.000740, loss: 0.5565
2022-07-09 20:58:46 - train: epoch 0114, iter [02200, 05004], lr: 0.000736, loss: 0.5283
2022-07-09 20:59:32 - train: epoch 0114, iter [02300, 05004], lr: 0.000731, loss: 0.6194
2022-07-09 21:00:18 - train: epoch 0114, iter [02400, 05004], lr: 0.000727, loss: 0.6107
2022-07-09 21:01:04 - train: epoch 0114, iter [02500, 05004], lr: 0.000722, loss: 0.5990
2022-07-09 21:01:50 - train: epoch 0114, iter [02600, 05004], lr: 0.000718, loss: 0.5773
2022-07-09 21:02:36 - train: epoch 0114, iter [02700, 05004], lr: 0.000713, loss: 0.6675
2022-07-09 21:03:21 - train: epoch 0114, iter [02800, 05004], lr: 0.000709, loss: 0.6957
2022-07-09 21:04:07 - train: epoch 0114, iter [02900, 05004], lr: 0.000705, loss: 0.4830
2022-07-09 21:04:53 - train: epoch 0114, iter [03000, 05004], lr: 0.000700, loss: 0.4858
2022-07-09 21:05:39 - train: epoch 0114, iter [03100, 05004], lr: 0.000696, loss: 0.7039
2022-07-09 21:06:25 - train: epoch 0114, iter [03200, 05004], lr: 0.000692, loss: 0.5203
2022-07-09 21:07:11 - train: epoch 0114, iter [03300, 05004], lr: 0.000687, loss: 0.6444
2022-07-09 21:07:57 - train: epoch 0114, iter [03400, 05004], lr: 0.000683, loss: 0.6883
2022-07-09 21:08:43 - train: epoch 0114, iter [03500, 05004], lr: 0.000679, loss: 0.5009
2022-07-09 21:09:28 - train: epoch 0114, iter [03600, 05004], lr: 0.000674, loss: 0.6263
2022-07-09 21:10:14 - train: epoch 0114, iter [03700, 05004], lr: 0.000670, loss: 0.4682
2022-07-09 21:11:00 - train: epoch 0114, iter [03800, 05004], lr: 0.000666, loss: 0.6024
2022-07-09 21:11:46 - train: epoch 0114, iter [03900, 05004], lr: 0.000662, loss: 0.7618
2022-07-09 21:12:32 - train: epoch 0114, iter [04000, 05004], lr: 0.000657, loss: 0.5448
2022-07-09 21:13:18 - train: epoch 0114, iter [04100, 05004], lr: 0.000653, loss: 0.5408
2022-07-09 21:14:04 - train: epoch 0114, iter [04200, 05004], lr: 0.000649, loss: 0.7551
2022-07-09 21:14:49 - train: epoch 0114, iter [04300, 05004], lr: 0.000645, loss: 0.6083
2022-07-09 21:15:35 - train: epoch 0114, iter [04400, 05004], lr: 0.000641, loss: 0.8294
2022-07-09 21:16:21 - train: epoch 0114, iter [04500, 05004], lr: 0.000636, loss: 0.6337
2022-07-09 21:17:07 - train: epoch 0114, iter [04600, 05004], lr: 0.000632, loss: 0.7458
2022-07-09 21:17:53 - train: epoch 0114, iter [04700, 05004], lr: 0.000628, loss: 0.6004
2022-07-09 21:18:39 - train: epoch 0114, iter [04800, 05004], lr: 0.000624, loss: 0.6150
2022-07-09 21:19:25 - train: epoch 0114, iter [04900, 05004], lr: 0.000620, loss: 0.6435
2022-07-09 21:20:11 - train: epoch 0114, iter [05000, 05004], lr: 0.000616, loss: 0.6873
2022-07-09 21:20:13 - train: epoch 114, train_loss: 0.6222
2022-07-09 21:21:28 - eval: epoch: 114, acc1: 78.004%, acc5: 94.006%, test_loss: 0.8848, per_image_load_time: 0.582ms, per_image_inference_time: 0.832ms
2022-07-09 21:21:29 - until epoch: 114, best_acc1: 78.004%
2022-07-09 21:21:29 - epoch 115 lr: 0.000616
2022-07-09 21:22:20 - train: epoch 0115, iter [00100, 05004], lr: 0.000611, loss: 0.5338
2022-07-09 21:23:06 - train: epoch 0115, iter [00200, 05004], lr: 0.000607, loss: 0.5922
2022-07-09 21:23:52 - train: epoch 0115, iter [00300, 05004], lr: 0.000603, loss: 0.5639
2022-07-09 21:24:37 - train: epoch 0115, iter [00400, 05004], lr: 0.000599, loss: 0.7064
2022-07-09 21:25:23 - train: epoch 0115, iter [00500, 05004], lr: 0.000595, loss: 0.5631
2022-07-09 21:26:09 - train: epoch 0115, iter [00600, 05004], lr: 0.000591, loss: 0.6223
2022-07-09 21:26:55 - train: epoch 0115, iter [00700, 05004], lr: 0.000587, loss: 0.8084
2022-07-09 21:27:41 - train: epoch 0115, iter [00800, 05004], lr: 0.000583, loss: 0.6345
2022-07-09 21:28:27 - train: epoch 0115, iter [00900, 05004], lr: 0.000579, loss: 0.6281
2022-07-09 21:29:13 - train: epoch 0115, iter [01000, 05004], lr: 0.000575, loss: 0.5806
2022-07-09 21:29:59 - train: epoch 0115, iter [01100, 05004], lr: 0.000571, loss: 0.5164
2022-07-09 21:30:45 - train: epoch 0115, iter [01200, 05004], lr: 0.000567, loss: 0.8059
2022-07-09 21:31:31 - train: epoch 0115, iter [01300, 05004], lr: 0.000564, loss: 0.6604
2022-07-09 21:32:17 - train: epoch 0115, iter [01400, 05004], lr: 0.000560, loss: 0.5676
2022-07-09 21:33:03 - train: epoch 0115, iter [01500, 05004], lr: 0.000556, loss: 0.7102
2022-07-09 21:33:48 - train: epoch 0115, iter [01600, 05004], lr: 0.000552, loss: 0.5258
2022-07-09 21:34:34 - train: epoch 0115, iter [01700, 05004], lr: 0.000548, loss: 0.6951
2022-07-09 21:35:20 - train: epoch 0115, iter [01800, 05004], lr: 0.000544, loss: 0.5748
2022-07-09 21:36:06 - train: epoch 0115, iter [01900, 05004], lr: 0.000540, loss: 0.6053
2022-07-09 21:36:52 - train: epoch 0115, iter [02000, 05004], lr: 0.000536, loss: 0.7998
2022-07-09 21:37:38 - train: epoch 0115, iter [02100, 05004], lr: 0.000533, loss: 0.5671
2022-07-09 21:38:24 - train: epoch 0115, iter [02200, 05004], lr: 0.000529, loss: 0.6804
2022-07-09 21:39:10 - train: epoch 0115, iter [02300, 05004], lr: 0.000525, loss: 0.6946
2022-07-09 21:39:56 - train: epoch 0115, iter [02400, 05004], lr: 0.000521, loss: 0.6439
2022-07-09 21:40:41 - train: epoch 0115, iter [02500, 05004], lr: 0.000518, loss: 0.5281
2022-07-09 21:41:27 - train: epoch 0115, iter [02600, 05004], lr: 0.000514, loss: 0.5582
2022-07-09 21:42:13 - train: epoch 0115, iter [02700, 05004], lr: 0.000510, loss: 0.4003
2022-07-09 21:42:59 - train: epoch 0115, iter [02800, 05004], lr: 0.000506, loss: 0.5696
2022-07-09 21:43:45 - train: epoch 0115, iter [02900, 05004], lr: 0.000503, loss: 0.4541
2022-07-09 21:44:31 - train: epoch 0115, iter [03000, 05004], lr: 0.000499, loss: 0.5486
2022-07-09 21:45:17 - train: epoch 0115, iter [03100, 05004], lr: 0.000495, loss: 0.6734
2022-07-09 21:46:03 - train: epoch 0115, iter [03200, 05004], lr: 0.000492, loss: 0.6117
2022-07-09 21:46:49 - train: epoch 0115, iter [03300, 05004], lr: 0.000488, loss: 0.5377
2022-07-09 21:47:35 - train: epoch 0115, iter [03400, 05004], lr: 0.000484, loss: 0.5423
2022-07-09 21:48:21 - train: epoch 0115, iter [03500, 05004], lr: 0.000481, loss: 0.5861
2022-07-09 21:49:07 - train: epoch 0115, iter [03600, 05004], lr: 0.000477, loss: 0.5638
2022-07-09 21:49:53 - train: epoch 0115, iter [03700, 05004], lr: 0.000473, loss: 0.6033
2022-07-09 21:50:38 - train: epoch 0115, iter [03800, 05004], lr: 0.000470, loss: 0.6655
2022-07-09 21:51:24 - train: epoch 0115, iter [03900, 05004], lr: 0.000466, loss: 0.6803
2022-07-09 21:52:11 - train: epoch 0115, iter [04000, 05004], lr: 0.000463, loss: 0.6434
2022-07-09 21:52:57 - train: epoch 0115, iter [04100, 05004], lr: 0.000459, loss: 0.5155
2022-07-09 21:53:43 - train: epoch 0115, iter [04200, 05004], lr: 0.000456, loss: 0.5890
2022-07-09 21:54:29 - train: epoch 0115, iter [04300, 05004], lr: 0.000452, loss: 0.6065
2022-07-09 21:55:15 - train: epoch 0115, iter [04400, 05004], lr: 0.000449, loss: 0.6044
2022-07-09 21:56:01 - train: epoch 0115, iter [04500, 05004], lr: 0.000445, loss: 0.5827
2022-07-09 21:56:47 - train: epoch 0115, iter [04600, 05004], lr: 0.000442, loss: 0.7140
2022-07-09 21:57:33 - train: epoch 0115, iter [04700, 05004], lr: 0.000438, loss: 0.5676
2022-07-09 21:58:19 - train: epoch 0115, iter [04800, 05004], lr: 0.000435, loss: 0.7427
2022-07-09 21:59:05 - train: epoch 0115, iter [04900, 05004], lr: 0.000431, loss: 0.5309
2022-07-09 21:59:51 - train: epoch 0115, iter [05000, 05004], lr: 0.000428, loss: 0.5748
2022-07-09 21:59:53 - train: epoch 115, train_loss: 0.6162
2022-07-09 22:01:08 - eval: epoch: 115, acc1: 78.144%, acc5: 94.020%, test_loss: 0.8839, per_image_load_time: 2.035ms, per_image_inference_time: 0.824ms
2022-07-09 22:01:09 - until epoch: 115, best_acc1: 78.144%
2022-07-09 22:01:09 - epoch 116 lr: 0.000428
2022-07-09 22:02:01 - train: epoch 0116, iter [00100, 05004], lr: 0.000424, loss: 0.5531
2022-07-09 22:02:47 - train: epoch 0116, iter [00200, 05004], lr: 0.000421, loss: 0.7103
2022-07-09 22:03:33 - train: epoch 0116, iter [00300, 05004], lr: 0.000418, loss: 0.6335
2022-07-09 22:04:19 - train: epoch 0116, iter [00400, 05004], lr: 0.000414, loss: 0.6818
2022-07-09 22:05:04 - train: epoch 0116, iter [00500, 05004], lr: 0.000411, loss: 0.6389
2022-07-09 22:05:50 - train: epoch 0116, iter [00600, 05004], lr: 0.000408, loss: 0.4699
2022-07-09 22:06:35 - train: epoch 0116, iter [00700, 05004], lr: 0.000404, loss: 0.5822
2022-07-09 22:07:21 - train: epoch 0116, iter [00800, 05004], lr: 0.000401, loss: 0.6208
2022-07-09 22:08:06 - train: epoch 0116, iter [00900, 05004], lr: 0.000398, loss: 0.6686
2022-07-09 22:08:52 - train: epoch 0116, iter [01000, 05004], lr: 0.000394, loss: 0.6570
2022-07-09 22:09:37 - train: epoch 0116, iter [01100, 05004], lr: 0.000391, loss: 0.4529
2022-07-09 22:10:22 - train: epoch 0116, iter [01200, 05004], lr: 0.000388, loss: 0.5561
2022-07-09 22:11:08 - train: epoch 0116, iter [01300, 05004], lr: 0.000385, loss: 0.4852
2022-07-09 22:11:54 - train: epoch 0116, iter [01400, 05004], lr: 0.000381, loss: 0.6564
2022-07-09 22:12:39 - train: epoch 0116, iter [01500, 05004], lr: 0.000378, loss: 0.5054
2022-07-09 22:13:25 - train: epoch 0116, iter [01600, 05004], lr: 0.000375, loss: 0.5836
2022-07-09 22:14:11 - train: epoch 0116, iter [01700, 05004], lr: 0.000372, loss: 0.5655
2022-07-09 22:14:57 - train: epoch 0116, iter [01800, 05004], lr: 0.000368, loss: 0.5509
2022-07-09 22:15:42 - train: epoch 0116, iter [01900, 05004], lr: 0.000365, loss: 0.4584
2022-07-09 22:16:28 - train: epoch 0116, iter [02000, 05004], lr: 0.000362, loss: 0.7158
2022-07-09 22:17:14 - train: epoch 0116, iter [02100, 05004], lr: 0.000359, loss: 0.5333
2022-07-09 22:17:59 - train: epoch 0116, iter [02200, 05004], lr: 0.000356, loss: 0.6275
2022-07-09 22:18:45 - train: epoch 0116, iter [02300, 05004], lr: 0.000353, loss: 0.5251
2022-07-09 22:19:31 - train: epoch 0116, iter [02400, 05004], lr: 0.000350, loss: 0.6500
2022-07-09 22:20:16 - train: epoch 0116, iter [02500, 05004], lr: 0.000347, loss: 0.5252
2022-07-09 22:21:02 - train: epoch 0116, iter [02600, 05004], lr: 0.000344, loss: 0.5465
2022-07-09 22:21:48 - train: epoch 0116, iter [02700, 05004], lr: 0.000341, loss: 0.6737
2022-07-09 22:22:33 - train: epoch 0116, iter [02800, 05004], lr: 0.000337, loss: 0.7487
2022-07-09 22:23:19 - train: epoch 0116, iter [02900, 05004], lr: 0.000334, loss: 0.7112
2022-07-09 22:24:05 - train: epoch 0116, iter [03000, 05004], lr: 0.000331, loss: 0.5872
2022-07-09 22:24:51 - train: epoch 0116, iter [03100, 05004], lr: 0.000328, loss: 0.6408
2022-07-09 22:25:36 - train: epoch 0116, iter [03200, 05004], lr: 0.000325, loss: 0.6172
2022-07-09 22:26:22 - train: epoch 0116, iter [03300, 05004], lr: 0.000322, loss: 0.7713
2022-07-09 22:27:09 - train: epoch 0116, iter [03400, 05004], lr: 0.000320, loss: 0.5683
2022-07-09 22:27:55 - train: epoch 0116, iter [03500, 05004], lr: 0.000317, loss: 0.7378
2022-07-09 22:28:42 - train: epoch 0116, iter [03600, 05004], lr: 0.000314, loss: 0.5641
2022-07-09 22:29:29 - train: epoch 0116, iter [03700, 05004], lr: 0.000311, loss: 0.4772
2022-07-09 22:30:15 - train: epoch 0116, iter [03800, 05004], lr: 0.000308, loss: 0.6232
2022-07-09 22:31:02 - train: epoch 0116, iter [03900, 05004], lr: 0.000305, loss: 0.5259
2022-07-09 22:31:49 - train: epoch 0116, iter [04000, 05004], lr: 0.000302, loss: 0.6087
2022-07-09 22:32:36 - train: epoch 0116, iter [04100, 05004], lr: 0.000299, loss: 0.7581
2022-07-09 22:33:23 - train: epoch 0116, iter [04200, 05004], lr: 0.000296, loss: 0.5463
2022-07-09 22:34:09 - train: epoch 0116, iter [04300, 05004], lr: 0.000293, loss: 0.4764
2022-07-09 22:34:56 - train: epoch 0116, iter [04400, 05004], lr: 0.000291, loss: 0.6597
2022-07-09 22:35:43 - train: epoch 0116, iter [04500, 05004], lr: 0.000288, loss: 0.5861
2022-07-09 22:36:29 - train: epoch 0116, iter [04600, 05004], lr: 0.000285, loss: 0.7975
2022-07-09 22:37:16 - train: epoch 0116, iter [04700, 05004], lr: 0.000282, loss: 0.6656
2022-07-09 22:38:03 - train: epoch 0116, iter [04800, 05004], lr: 0.000280, loss: 0.5268
2022-07-09 22:38:49 - train: epoch 0116, iter [04900, 05004], lr: 0.000277, loss: 0.5481
2022-07-09 22:39:36 - train: epoch 0116, iter [05000, 05004], lr: 0.000274, loss: 0.6476
2022-07-09 22:39:38 - train: epoch 116, train_loss: 0.6101
2022-07-09 22:40:54 - eval: epoch: 116, acc1: 78.042%, acc5: 94.076%, test_loss: 0.8829, per_image_load_time: 1.990ms, per_image_inference_time: 0.835ms
2022-07-09 22:40:55 - until epoch: 116, best_acc1: 78.144%
2022-07-09 22:40:55 - epoch 117 lr: 0.000274
2022-07-09 22:41:47 - train: epoch 0117, iter [00100, 05004], lr: 0.000271, loss: 0.6336
2022-07-09 22:42:34 - train: epoch 0117, iter [00200, 05004], lr: 0.000268, loss: 0.6386
2022-07-09 22:43:21 - train: epoch 0117, iter [00300, 05004], lr: 0.000266, loss: 0.5359
2022-07-09 22:44:08 - train: epoch 0117, iter [00400, 05004], lr: 0.000263, loss: 0.4875
2022-07-09 22:44:54 - train: epoch 0117, iter [00500, 05004], lr: 0.000260, loss: 0.7511
2022-07-09 22:45:41 - train: epoch 0117, iter [00600, 05004], lr: 0.000258, loss: 0.5060
2022-07-09 22:46:28 - train: epoch 0117, iter [00700, 05004], lr: 0.000255, loss: 0.5147
2022-07-09 22:47:14 - train: epoch 0117, iter [00800, 05004], lr: 0.000252, loss: 0.4982
2022-07-09 22:48:01 - train: epoch 0117, iter [00900, 05004], lr: 0.000250, loss: 0.5878
2022-07-09 22:48:47 - train: epoch 0117, iter [01000, 05004], lr: 0.000247, loss: 0.7029
2022-07-09 22:49:34 - train: epoch 0117, iter [01100, 05004], lr: 0.000245, loss: 0.5156
2022-07-09 22:50:20 - train: epoch 0117, iter [01200, 05004], lr: 0.000242, loss: 0.5690
2022-07-09 22:51:07 - train: epoch 0117, iter [01300, 05004], lr: 0.000240, loss: 0.6871
2022-07-09 22:51:53 - train: epoch 0117, iter [01400, 05004], lr: 0.000237, loss: 0.7432
2022-07-09 22:52:40 - train: epoch 0117, iter [01500, 05004], lr: 0.000234, loss: 0.5349
2022-07-09 22:53:26 - train: epoch 0117, iter [01600, 05004], lr: 0.000232, loss: 0.5327
2022-07-09 22:54:13 - train: epoch 0117, iter [01700, 05004], lr: 0.000229, loss: 0.6539
2022-07-09 22:54:59 - train: epoch 0117, iter [01800, 05004], lr: 0.000227, loss: 0.8287
2022-07-09 22:55:46 - train: epoch 0117, iter [01900, 05004], lr: 0.000224, loss: 0.6258
2022-07-09 22:56:32 - train: epoch 0117, iter [02000, 05004], lr: 0.000222, loss: 0.6111
2022-07-09 22:57:19 - train: epoch 0117, iter [02100, 05004], lr: 0.000219, loss: 0.5460
2022-07-09 22:58:05 - train: epoch 0117, iter [02200, 05004], lr: 0.000217, loss: 0.5350
2022-07-09 22:58:52 - train: epoch 0117, iter [02300, 05004], lr: 0.000215, loss: 0.5903
2022-07-09 22:59:38 - train: epoch 0117, iter [02400, 05004], lr: 0.000212, loss: 0.6021
2022-07-09 23:00:25 - train: epoch 0117, iter [02500, 05004], lr: 0.000210, loss: 0.6617
2022-07-09 23:01:11 - train: epoch 0117, iter [02600, 05004], lr: 0.000207, loss: 0.5711
2022-07-09 23:01:58 - train: epoch 0117, iter [02700, 05004], lr: 0.000205, loss: 0.4980
2022-07-09 23:02:44 - train: epoch 0117, iter [02800, 05004], lr: 0.000203, loss: 0.6090
2022-07-09 23:03:30 - train: epoch 0117, iter [02900, 05004], lr: 0.000200, loss: 0.6619
2022-07-09 23:04:17 - train: epoch 0117, iter [03000, 05004], lr: 0.000198, loss: 0.4499
2022-07-09 23:05:03 - train: epoch 0117, iter [03100, 05004], lr: 0.000196, loss: 0.4975
2022-07-09 23:05:50 - train: epoch 0117, iter [03200, 05004], lr: 0.000193, loss: 0.7502
2022-07-09 23:06:36 - train: epoch 0117, iter [03300, 05004], lr: 0.000191, loss: 0.5793
2022-07-09 23:07:23 - train: epoch 0117, iter [03400, 05004], lr: 0.000189, loss: 0.5259
2022-07-09 23:08:09 - train: epoch 0117, iter [03500, 05004], lr: 0.000187, loss: 0.7653
2022-07-09 23:08:56 - train: epoch 0117, iter [03600, 05004], lr: 0.000184, loss: 0.6525
2022-07-09 23:09:43 - train: epoch 0117, iter [03700, 05004], lr: 0.000182, loss: 0.5752
2022-07-09 23:10:29 - train: epoch 0117, iter [03800, 05004], lr: 0.000180, loss: 0.8020
2022-07-09 23:11:16 - train: epoch 0117, iter [03900, 05004], lr: 0.000178, loss: 0.6378
2022-07-09 23:12:02 - train: epoch 0117, iter [04000, 05004], lr: 0.000175, loss: 0.5665
2022-07-09 23:12:49 - train: epoch 0117, iter [04100, 05004], lr: 0.000173, loss: 0.6945
2022-07-09 23:13:35 - train: epoch 0117, iter [04200, 05004], lr: 0.000171, loss: 0.5275
2022-07-09 23:14:22 - train: epoch 0117, iter [04300, 05004], lr: 0.000169, loss: 0.5992
2022-07-09 23:15:08 - train: epoch 0117, iter [04400, 05004], lr: 0.000167, loss: 0.5547
2022-07-09 23:15:55 - train: epoch 0117, iter [04500, 05004], lr: 0.000165, loss: 0.7287
2022-07-09 23:16:41 - train: epoch 0117, iter [04600, 05004], lr: 0.000163, loss: 0.6265
2022-07-09 23:17:28 - train: epoch 0117, iter [04700, 05004], lr: 0.000160, loss: 0.5928
2022-07-09 23:18:14 - train: epoch 0117, iter [04800, 05004], lr: 0.000158, loss: 0.4936
2022-07-09 23:19:00 - train: epoch 0117, iter [04900, 05004], lr: 0.000156, loss: 0.6900
2022-07-09 23:19:47 - train: epoch 0117, iter [05000, 05004], lr: 0.000154, loss: 0.7430
2022-07-09 23:19:49 - train: epoch 117, train_loss: 0.6017
2022-07-09 23:21:05 - eval: epoch: 117, acc1: 78.128%, acc5: 94.130%, test_loss: 0.8818, per_image_load_time: 2.131ms, per_image_inference_time: 0.847ms
2022-07-09 23:21:06 - until epoch: 117, best_acc1: 78.144%
2022-07-09 23:21:06 - epoch 118 lr: 0.000154
2022-07-09 23:21:59 - train: epoch 0118, iter [00100, 05004], lr: 0.000152, loss: 0.7635
2022-07-09 23:22:45 - train: epoch 0118, iter [00200, 05004], lr: 0.000150, loss: 0.5075
2022-07-09 23:23:32 - train: epoch 0118, iter [00300, 05004], lr: 0.000148, loss: 0.5897
2022-07-09 23:24:19 - train: epoch 0118, iter [00400, 05004], lr: 0.000146, loss: 0.6109
2022-07-09 23:25:05 - train: epoch 0118, iter [00500, 05004], lr: 0.000144, loss: 0.6921
2022-07-09 23:25:52 - train: epoch 0118, iter [00600, 05004], lr: 0.000142, loss: 0.5958
2022-07-09 23:26:38 - train: epoch 0118, iter [00700, 05004], lr: 0.000140, loss: 0.8370
2022-07-09 23:27:25 - train: epoch 0118, iter [00800, 05004], lr: 0.000138, loss: 0.5098
2022-07-09 23:28:11 - train: epoch 0118, iter [00900, 05004], lr: 0.000136, loss: 0.5929
2022-07-09 23:28:58 - train: epoch 0118, iter [01000, 05004], lr: 0.000134, loss: 0.5873
2022-07-09 23:29:44 - train: epoch 0118, iter [01100, 05004], lr: 0.000132, loss: 0.7821
2022-07-09 23:30:31 - train: epoch 0118, iter [01200, 05004], lr: 0.000130, loss: 0.6927
2022-07-09 23:31:17 - train: epoch 0118, iter [01300, 05004], lr: 0.000129, loss: 0.6947
2022-07-09 23:32:04 - train: epoch 0118, iter [01400, 05004], lr: 0.000127, loss: 0.6811
2022-07-09 23:32:50 - train: epoch 0118, iter [01500, 05004], lr: 0.000125, loss: 0.7121
2022-07-09 23:33:37 - train: epoch 0118, iter [01600, 05004], lr: 0.000123, loss: 0.4439
2022-07-09 23:34:24 - train: epoch 0118, iter [01700, 05004], lr: 0.000121, loss: 0.6663
2022-07-09 23:35:10 - train: epoch 0118, iter [01800, 05004], lr: 0.000119, loss: 0.6056
2022-07-09 23:35:57 - train: epoch 0118, iter [01900, 05004], lr: 0.000118, loss: 0.7596
2022-07-09 23:36:43 - train: epoch 0118, iter [02000, 05004], lr: 0.000116, loss: 0.5223
2022-07-09 23:37:30 - train: epoch 0118, iter [02100, 05004], lr: 0.000114, loss: 0.6075
2022-07-09 23:38:16 - train: epoch 0118, iter [02200, 05004], lr: 0.000112, loss: 0.6147
2022-07-09 23:39:03 - train: epoch 0118, iter [02300, 05004], lr: 0.000111, loss: 0.5304
2022-07-09 23:39:49 - train: epoch 0118, iter [02400, 05004], lr: 0.000109, loss: 0.6277
2022-07-09 23:40:36 - train: epoch 0118, iter [02500, 05004], lr: 0.000107, loss: 0.5801
2022-07-09 23:41:23 - train: epoch 0118, iter [02600, 05004], lr: 0.000105, loss: 0.6982
2022-07-09 23:42:09 - train: epoch 0118, iter [02700, 05004], lr: 0.000104, loss: 0.6348
2022-07-09 23:42:56 - train: epoch 0118, iter [02800, 05004], lr: 0.000102, loss: 0.6793
2022-07-09 23:43:42 - train: epoch 0118, iter [02900, 05004], lr: 0.000100, loss: 0.5794
2022-07-09 23:44:29 - train: epoch 0118, iter [03000, 05004], lr: 0.000099, loss: 0.5365
2022-07-09 23:45:16 - train: epoch 0118, iter [03100, 05004], lr: 0.000097, loss: 0.6282
2022-07-09 23:46:02 - train: epoch 0118, iter [03200, 05004], lr: 0.000095, loss: 0.5478
2022-07-09 23:46:49 - train: epoch 0118, iter [03300, 05004], lr: 0.000094, loss: 0.5645
2022-07-09 23:47:35 - train: epoch 0118, iter [03400, 05004], lr: 0.000092, loss: 0.6594
2022-07-09 23:48:22 - train: epoch 0118, iter [03500, 05004], lr: 0.000091, loss: 0.6877
2022-07-09 23:49:08 - train: epoch 0118, iter [03600, 05004], lr: 0.000089, loss: 0.6218
2022-07-09 23:49:55 - train: epoch 0118, iter [03700, 05004], lr: 0.000088, loss: 0.6386
2022-07-09 23:50:41 - train: epoch 0118, iter [03800, 05004], lr: 0.000086, loss: 0.7093
2022-07-09 23:51:27 - train: epoch 0118, iter [03900, 05004], lr: 0.000084, loss: 0.5785
2022-07-09 23:52:14 - train: epoch 0118, iter [04000, 05004], lr: 0.000083, loss: 0.5535
2022-07-09 23:53:00 - train: epoch 0118, iter [04100, 05004], lr: 0.000081, loss: 0.7104
2022-07-09 23:53:47 - train: epoch 0118, iter [04200, 05004], lr: 0.000080, loss: 0.6356
2022-07-09 23:54:33 - train: epoch 0118, iter [04300, 05004], lr: 0.000079, loss: 0.6790
2022-07-09 23:55:20 - train: epoch 0118, iter [04400, 05004], lr: 0.000077, loss: 0.6322
2022-07-09 23:56:06 - train: epoch 0118, iter [04500, 05004], lr: 0.000076, loss: 0.6501
2022-07-09 23:56:53 - train: epoch 0118, iter [04600, 05004], lr: 0.000074, loss: 0.6293
2022-07-09 23:57:39 - train: epoch 0118, iter [04700, 05004], lr: 0.000073, loss: 0.4923
2022-07-09 23:58:26 - train: epoch 0118, iter [04800, 05004], lr: 0.000071, loss: 0.5322
2022-07-09 23:59:12 - train: epoch 0118, iter [04900, 05004], lr: 0.000070, loss: 0.4957
2022-07-09 23:59:58 - train: epoch 0118, iter [05000, 05004], lr: 0.000069, loss: 0.6381
2022-07-10 00:00:01 - train: epoch 118, train_loss: 0.5997
2022-07-10 00:01:16 - eval: epoch: 118, acc1: 78.214%, acc5: 94.056%, test_loss: 0.8806, per_image_load_time: 2.034ms, per_image_inference_time: 0.828ms
2022-07-10 00:01:17 - until epoch: 118, best_acc1: 78.214%
2022-07-10 00:01:17 - epoch 119 lr: 0.000069
2022-07-10 00:02:09 - train: epoch 0119, iter [00100, 05004], lr: 0.000067, loss: 0.6503
2022-07-10 00:02:55 - train: epoch 0119, iter [00200, 05004], lr: 0.000066, loss: 0.6900
2022-07-10 00:03:41 - train: epoch 0119, iter [00300, 05004], lr: 0.000064, loss: 0.6135
2022-07-10 00:04:27 - train: epoch 0119, iter [00400, 05004], lr: 0.000063, loss: 0.5868
2022-07-10 00:05:14 - train: epoch 0119, iter [00500, 05004], lr: 0.000062, loss: 0.4298
2022-07-10 00:06:00 - train: epoch 0119, iter [00600, 05004], lr: 0.000061, loss: 0.6573
2022-07-10 00:06:47 - train: epoch 0119, iter [00700, 05004], lr: 0.000059, loss: 0.5860
2022-07-10 00:07:33 - train: epoch 0119, iter [00800, 05004], lr: 0.000058, loss: 0.6050
2022-07-10 00:08:19 - train: epoch 0119, iter [00900, 05004], lr: 0.000057, loss: 0.5115
2022-07-10 00:09:05 - train: epoch 0119, iter [01000, 05004], lr: 0.000056, loss: 0.5695
2022-07-10 00:09:52 - train: epoch 0119, iter [01100, 05004], lr: 0.000054, loss: 0.6290
2022-07-10 00:10:38 - train: epoch 0119, iter [01200, 05004], lr: 0.000053, loss: 0.5460
2022-07-10 00:11:24 - train: epoch 0119, iter [01300, 05004], lr: 0.000052, loss: 0.7157
2022-07-10 00:12:11 - train: epoch 0119, iter [01400, 05004], lr: 0.000051, loss: 0.5782
2022-07-10 00:12:57 - train: epoch 0119, iter [01500, 05004], lr: 0.000050, loss: 0.5356
2022-07-10 00:13:43 - train: epoch 0119, iter [01600, 05004], lr: 0.000048, loss: 0.5455
2022-07-10 00:14:29 - train: epoch 0119, iter [01700, 05004], lr: 0.000047, loss: 0.5641
2022-07-10 00:15:16 - train: epoch 0119, iter [01800, 05004], lr: 0.000046, loss: 0.8192
2022-07-10 00:16:02 - train: epoch 0119, iter [01900, 05004], lr: 0.000045, loss: 0.3977
2022-07-10 00:16:49 - train: epoch 0119, iter [02000, 05004], lr: 0.000044, loss: 0.4924
2022-07-10 00:17:35 - train: epoch 0119, iter [02100, 05004], lr: 0.000043, loss: 0.7980
2022-07-10 00:18:21 - train: epoch 0119, iter [02200, 05004], lr: 0.000042, loss: 0.5284
2022-07-10 00:19:08 - train: epoch 0119, iter [02300, 05004], lr: 0.000041, loss: 0.4616
2022-07-10 00:19:54 - train: epoch 0119, iter [02400, 05004], lr: 0.000040, loss: 0.5857
2022-07-10 00:20:40 - train: epoch 0119, iter [02500, 05004], lr: 0.000039, loss: 0.7017
2022-07-10 00:21:27 - train: epoch 0119, iter [02600, 05004], lr: 0.000038, loss: 0.7169
2022-07-10 00:22:13 - train: epoch 0119, iter [02700, 05004], lr: 0.000037, loss: 0.7022
2022-07-10 00:22:59 - train: epoch 0119, iter [02800, 05004], lr: 0.000036, loss: 0.6319
2022-07-10 00:23:45 - train: epoch 0119, iter [02900, 05004], lr: 0.000035, loss: 0.6349
2022-07-10 00:24:32 - train: epoch 0119, iter [03000, 05004], lr: 0.000034, loss: 0.5816
2022-07-10 00:25:18 - train: epoch 0119, iter [03100, 05004], lr: 0.000033, loss: 0.6103
2022-07-10 00:26:04 - train: epoch 0119, iter [03200, 05004], lr: 0.000032, loss: 0.5656
2022-07-10 00:26:51 - train: epoch 0119, iter [03300, 05004], lr: 0.000031, loss: 0.5090
2022-07-10 00:27:37 - train: epoch 0119, iter [03400, 05004], lr: 0.000030, loss: 0.6592
2022-07-10 00:28:24 - train: epoch 0119, iter [03500, 05004], lr: 0.000029, loss: 0.5269
2022-07-10 00:29:10 - train: epoch 0119, iter [03600, 05004], lr: 0.000028, loss: 0.6303
2022-07-10 00:29:56 - train: epoch 0119, iter [03700, 05004], lr: 0.000027, loss: 0.6325
2022-07-10 00:30:42 - train: epoch 0119, iter [03800, 05004], lr: 0.000026, loss: 0.5362
2022-07-10 00:31:29 - train: epoch 0119, iter [03900, 05004], lr: 0.000026, loss: 0.7637
2022-07-10 00:32:15 - train: epoch 0119, iter [04000, 05004], lr: 0.000025, loss: 0.5973
2022-07-10 00:33:02 - train: epoch 0119, iter [04100, 05004], lr: 0.000024, loss: 0.6415
2022-07-10 00:33:48 - train: epoch 0119, iter [04200, 05004], lr: 0.000023, loss: 0.6001
2022-07-10 00:34:34 - train: epoch 0119, iter [04300, 05004], lr: 0.000022, loss: 0.5151
2022-07-10 00:35:21 - train: epoch 0119, iter [04400, 05004], lr: 0.000022, loss: 0.6713
2022-07-10 00:36:07 - train: epoch 0119, iter [04500, 05004], lr: 0.000021, loss: 0.6302
2022-07-10 00:36:54 - train: epoch 0119, iter [04600, 05004], lr: 0.000020, loss: 0.6091
2022-07-10 00:37:40 - train: epoch 0119, iter [04700, 05004], lr: 0.000019, loss: 0.6036
2022-07-10 00:38:27 - train: epoch 0119, iter [04800, 05004], lr: 0.000019, loss: 0.6558
2022-07-10 00:39:13 - train: epoch 0119, iter [04900, 05004], lr: 0.000018, loss: 0.5838
2022-07-10 00:40:00 - train: epoch 0119, iter [05000, 05004], lr: 0.000017, loss: 0.6051
2022-07-10 00:40:02 - train: epoch 119, train_loss: 0.5951
2022-07-10 00:41:17 - eval: epoch: 119, acc1: 78.178%, acc5: 94.070%, test_loss: 0.8806, per_image_load_time: 2.050ms, per_image_inference_time: 0.846ms
2022-07-10 00:41:18 - until epoch: 119, best_acc1: 78.214%
2022-07-10 00:41:18 - epoch 120 lr: 0.000017
2022-07-10 00:42:11 - train: epoch 0120, iter [00100, 05004], lr: 0.000016, loss: 0.6524
2022-07-10 00:42:57 - train: epoch 0120, iter [00200, 05004], lr: 0.000016, loss: 0.4751
2022-07-10 00:43:43 - train: epoch 0120, iter [00300, 05004], lr: 0.000015, loss: 0.6328
2022-07-10 00:44:29 - train: epoch 0120, iter [00400, 05004], lr: 0.000015, loss: 0.8110
2022-07-10 00:45:14 - train: epoch 0120, iter [00500, 05004], lr: 0.000014, loss: 0.6525
2022-07-10 00:46:00 - train: epoch 0120, iter [00600, 05004], lr: 0.000013, loss: 0.4272
2022-07-10 00:46:45 - train: epoch 0120, iter [00700, 05004], lr: 0.000013, loss: 0.6110
2022-07-10 00:47:30 - train: epoch 0120, iter [00800, 05004], lr: 0.000012, loss: 0.4956
2022-07-10 00:48:16 - train: epoch 0120, iter [00900, 05004], lr: 0.000012, loss: 0.6167
2022-07-10 00:49:01 - train: epoch 0120, iter [01000, 05004], lr: 0.000011, loss: 0.6033
2022-07-10 00:49:46 - train: epoch 0120, iter [01100, 05004], lr: 0.000010, loss: 0.5150
2022-07-10 00:50:32 - train: epoch 0120, iter [01200, 05004], lr: 0.000010, loss: 0.6074
2022-07-10 00:51:17 - train: epoch 0120, iter [01300, 05004], lr: 0.000009, loss: 0.5009
2022-07-10 00:52:02 - train: epoch 0120, iter [01400, 05004], lr: 0.000009, loss: 0.5874
2022-07-10 00:52:48 - train: epoch 0120, iter [01500, 05004], lr: 0.000008, loss: 0.5654
2022-07-10 00:53:33 - train: epoch 0120, iter [01600, 05004], lr: 0.000008, loss: 0.5291
2022-07-10 00:54:19 - train: epoch 0120, iter [01700, 05004], lr: 0.000007, loss: 0.5536
2022-07-10 00:55:04 - train: epoch 0120, iter [01800, 05004], lr: 0.000007, loss: 0.5410
2022-07-10 00:55:49 - train: epoch 0120, iter [01900, 05004], lr: 0.000007, loss: 0.6791
2022-07-10 00:56:35 - train: epoch 0120, iter [02000, 05004], lr: 0.000006, loss: 0.5462
2022-07-10 00:57:20 - train: epoch 0120, iter [02100, 05004], lr: 0.000006, loss: 0.6340
2022-07-10 00:58:05 - train: epoch 0120, iter [02200, 05004], lr: 0.000005, loss: 0.6688
2022-07-10 00:58:50 - train: epoch 0120, iter [02300, 05004], lr: 0.000005, loss: 0.6431
2022-07-10 00:59:35 - train: epoch 0120, iter [02400, 05004], lr: 0.000005, loss: 0.7226
2022-07-10 01:00:21 - train: epoch 0120, iter [02500, 05004], lr: 0.000004, loss: 0.5935
2022-07-10 01:01:06 - train: epoch 0120, iter [02600, 05004], lr: 0.000004, loss: 0.7159
2022-07-10 01:01:51 - train: epoch 0120, iter [02700, 05004], lr: 0.000004, loss: 0.6146
2022-07-10 01:02:37 - train: epoch 0120, iter [02800, 05004], lr: 0.000003, loss: 0.6447
2022-07-10 01:03:22 - train: epoch 0120, iter [02900, 05004], lr: 0.000003, loss: 0.5186
2022-07-10 01:04:07 - train: epoch 0120, iter [03000, 05004], lr: 0.000003, loss: 0.6474
2022-07-10 01:04:53 - train: epoch 0120, iter [03100, 05004], lr: 0.000002, loss: 0.4181
2022-07-10 01:05:38 - train: epoch 0120, iter [03200, 05004], lr: 0.000002, loss: 0.6938
2022-07-10 01:06:23 - train: epoch 0120, iter [03300, 05004], lr: 0.000002, loss: 0.6121
2022-07-10 01:07:09 - train: epoch 0120, iter [03400, 05004], lr: 0.000002, loss: 0.5883
2022-07-10 01:07:54 - train: epoch 0120, iter [03500, 05004], lr: 0.000002, loss: 0.8002
2022-07-10 01:08:39 - train: epoch 0120, iter [03600, 05004], lr: 0.000001, loss: 0.5522
2022-07-10 01:09:25 - train: epoch 0120, iter [03700, 05004], lr: 0.000001, loss: 0.5702
2022-07-10 01:10:10 - train: epoch 0120, iter [03800, 05004], lr: 0.000001, loss: 0.5747
2022-07-10 01:10:56 - train: epoch 0120, iter [03900, 05004], lr: 0.000001, loss: 0.4807
2022-07-10 01:11:41 - train: epoch 0120, iter [04000, 05004], lr: 0.000001, loss: 0.5950
2022-07-10 01:12:26 - train: epoch 0120, iter [04100, 05004], lr: 0.000001, loss: 0.5527
2022-07-10 01:13:12 - train: epoch 0120, iter [04200, 05004], lr: 0.000000, loss: 0.5914
2022-07-10 01:13:57 - train: epoch 0120, iter [04300, 05004], lr: 0.000000, loss: 0.5537
2022-07-10 01:14:42 - train: epoch 0120, iter [04400, 05004], lr: 0.000000, loss: 0.6915
2022-07-10 01:15:27 - train: epoch 0120, iter [04500, 05004], lr: 0.000000, loss: 0.7231
2022-07-10 01:16:13 - train: epoch 0120, iter [04600, 05004], lr: 0.000000, loss: 0.7320
2022-07-10 01:16:58 - train: epoch 0120, iter [04700, 05004], lr: 0.000000, loss: 0.6534
2022-07-10 01:17:43 - train: epoch 0120, iter [04800, 05004], lr: 0.000000, loss: 0.6301
2022-07-10 01:18:28 - train: epoch 0120, iter [04900, 05004], lr: 0.000000, loss: 0.5253
2022-07-10 01:19:14 - train: epoch 0120, iter [05000, 05004], lr: 0.000000, loss: 0.5901
2022-07-10 01:19:16 - train: epoch 120, train_loss: 0.5959
2022-07-10 01:20:32 - eval: epoch: 120, acc1: 78.140%, acc5: 94.104%, test_loss: 0.8802, per_image_load_time: 2.089ms, per_image_inference_time: 0.830ms
2022-07-10 01:20:32 - until epoch: 120, best_acc1: 78.214%
2022-07-10 01:20:32 - train done. model: RepVGG_B2, train time: 78.230 hours, best_acc1: 78.214%
