2022-07-06 07:53:35 - network: RepVGG_B2
2022-07-06 07:53:35 - num_classes: 1000
2022-07-06 07:53:35 - input_image_size: 224
2022-07-06 07:53:35 - scale: 1.1428571428571428
2022-07-06 07:53:35 - trained_model_path: 
2022-07-06 07:53:35 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-06 07:53:35 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-06 07:53:35 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f03f1f9ae80>
2022-07-06 07:53:35 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f03aef7f190>
2022-07-06 07:53:35 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f03aef7f1c0>
2022-07-06 07:53:35 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f03aef7f220>
2022-07-06 07:53:35 - seed: 0
2022-07-06 07:53:35 - batch_size: 256
2022-07-06 07:53:35 - num_workers: 16
2022-07-06 07:53:35 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-07-06 07:53:35 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-07-06 07:53:35 - epochs: 120
2022-07-06 07:53:35 - print_interval: 100
2022-07-06 07:53:35 - sync_bn: False
2022-07-06 07:53:35 - apex: True
2022-07-06 07:53:35 - use_ema_model: False
2022-07-06 07:53:35 - ema_model_decay: 0.9999
2022-07-06 07:53:35 - gpus_type: NVIDIA RTX A5000
2022-07-06 07:53:35 - gpus_num: 2
2022-07-06 07:53:35 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f03ab103d30>
2022-07-06 07:53:35 - --------------------parameters--------------------
2022-07-06 07:53:35 - name: stage0.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage0.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage0.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage0.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage0.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage0.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage1.0.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage1.0.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage1.0.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage1.0.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage1.0.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage1.0.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage1.1.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage1.1.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage1.1.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage1.1.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage1.1.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage1.1.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage1.1.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage1.1.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage1.2.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage1.2.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage1.2.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage1.2.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage1.2.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage1.2.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage1.2.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage1.2.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage1.3.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage1.3.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage1.3.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage1.3.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage1.3.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage1.3.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage1.3.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage1.3.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage2.0.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage2.0.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage2.0.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage2.0.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage2.0.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage2.0.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage2.1.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage2.1.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage2.1.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage2.1.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage2.1.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage2.1.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage2.1.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage2.1.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage2.2.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage2.2.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage2.2.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage2.2.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage2.2.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage2.2.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage2.2.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage2.2.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage2.3.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage2.3.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage2.3.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage2.3.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage2.3.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage2.3.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage2.3.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage2.3.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage2.4.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage2.4.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage2.4.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage2.4.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage2.4.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage2.4.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage2.4.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage2.4.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage2.5.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage2.5.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage2.5.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage2.5.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage2.5.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage2.5.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage2.5.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage2.5.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.0.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.0.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.0.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.0.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.0.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.0.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.1.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.1.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.1.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.1.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.1.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.1.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.1.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.1.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.2.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.2.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.2.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.2.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.2.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.2.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.2.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.2.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.3.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.3.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.3.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.3.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.3.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.3.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.3.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.3.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.4.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.4.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.4.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.4.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.4.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.4.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.4.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.4.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.5.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.5.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.5.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.5.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.5.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.5.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.5.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.5.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.6.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.6.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.6.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.6.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.6.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.6.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.6.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.6.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.7.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.7.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.7.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.7.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.7.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.7.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.7.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.7.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.8.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.8.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.8.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.8.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.8.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.8.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.8.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.8.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.9.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.9.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.9.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.9.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.9.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.9.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.9.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.9.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.10.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.10.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.10.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.10.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.10.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.10.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.10.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.10.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.11.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.11.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.11.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.11.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.11.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.11.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.11.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.11.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.12.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.12.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.12.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.12.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.12.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.12.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.12.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.12.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.13.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.13.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.13.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.13.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.13.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.13.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.13.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.13.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.14.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.14.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.14.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.14.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.14.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.14.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.14.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.14.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.15.identity.weight, grad: True
2022-07-06 07:53:35 - name: stage3.15.identity.bias, grad: True
2022-07-06 07:53:35 - name: stage3.15.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.15.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.15.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage3.15.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage3.15.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage3.15.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage4.0.conv3x3.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage4.0.conv3x3.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage4.0.conv3x3.bn.bias, grad: True
2022-07-06 07:53:35 - name: stage4.0.conv1x1.conv.weight, grad: True
2022-07-06 07:53:35 - name: stage4.0.conv1x1.bn.weight, grad: True
2022-07-06 07:53:35 - name: stage4.0.conv1x1.bn.bias, grad: True
2022-07-06 07:53:35 - name: fc.weight, grad: True
2022-07-06 07:53:35 - name: fc.bias, grad: True
2022-07-06 07:53:35 - --------------------buffers--------------------
2022-07-06 07:53:35 - name: stage0.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage0.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage0.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage0.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage1.0.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage1.0.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage1.0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage1.0.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage1.0.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage1.0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage1.1.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage1.1.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage1.1.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage1.1.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage1.1.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage1.1.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage1.1.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage1.1.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage1.1.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage1.2.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage1.2.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage1.2.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage1.2.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage1.2.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage1.2.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage1.2.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage1.2.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage1.2.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage1.3.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage1.3.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage1.3.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage1.3.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage1.3.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage1.3.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage1.3.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage1.3.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage1.3.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.0.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.0.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.0.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.0.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.1.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.1.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.1.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.1.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.1.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.1.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.1.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.1.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.1.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.2.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.2.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.2.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.2.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.2.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.2.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.2.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.2.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.2.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.3.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.3.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.3.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.3.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.3.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.3.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.3.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.3.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.3.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.4.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.4.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.4.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.4.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.4.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.4.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.4.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.4.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.4.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.5.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.5.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.5.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.5.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.5.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.5.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage2.5.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage2.5.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage2.5.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.0.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.0.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.0.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.0.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.1.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.1.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.1.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.1.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.1.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.1.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.1.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.1.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.1.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.2.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.2.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.2.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.2.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.2.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.2.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.2.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.2.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.2.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.3.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.3.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.3.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.3.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.3.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.3.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.3.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.3.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.3.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.4.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.4.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.4.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.4.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.4.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.4.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.4.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.4.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.4.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.5.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.5.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.5.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.5.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.5.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.5.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.5.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.5.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.5.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.6.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.6.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.6.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.6.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.6.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.6.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.6.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.6.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.6.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.7.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.7.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.7.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.7.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.7.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.7.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.7.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.7.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.7.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.8.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.8.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.8.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.8.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.8.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.8.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.8.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.8.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.8.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.9.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.9.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.9.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.9.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.9.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.9.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.9.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.9.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.9.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.10.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.10.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.10.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.10.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.10.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.10.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.10.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.10.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.10.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.11.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.11.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.11.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.11.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.11.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.11.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.11.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.11.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.11.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.12.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.12.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.12.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.12.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.12.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.12.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.12.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.12.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.12.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.13.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.13.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.13.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.13.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.13.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.13.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.13.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.13.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.13.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.14.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.14.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.14.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.14.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.14.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.14.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.14.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.14.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.14.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.15.identity.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.15.identity.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.15.identity.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.15.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.15.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.15.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage3.15.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage3.15.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage3.15.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage4.0.conv3x3.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage4.0.conv3x3.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage4.0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - name: stage4.0.conv1x1.bn.running_mean, grad: False
2022-07-06 07:53:35 - name: stage4.0.conv1x1.bn.running_var, grad: False
2022-07-06 07:53:35 - name: stage4.0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-06 07:53:35 - -----------no weight decay layers--------------
2022-07-06 07:53:35 - name: stage0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.1.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.1.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.1.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.1.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.1.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.1.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.2.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.2.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.2.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.2.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.2.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.2.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.3.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.3.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.3.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.3.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.3.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.3.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.1.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.1.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.1.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.1.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.1.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.1.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.2.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.2.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.2.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.2.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.2.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.2.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.3.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.3.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.3.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.3.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.3.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.3.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.4.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.4.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.4.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.4.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.4.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.4.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.5.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.5.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.5.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.5.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.5.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.5.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.1.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.1.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.1.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.1.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.1.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.1.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.2.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.2.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.2.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.2.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.2.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.2.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.3.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.3.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.3.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.3.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.3.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.3.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.4.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.4.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.4.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.4.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.4.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.4.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.5.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.5.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.5.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.5.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.5.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.5.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.6.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.6.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.6.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.6.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.6.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.6.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.7.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.7.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.7.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.7.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.7.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.7.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.8.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.8.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.8.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.8.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.8.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.8.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.9.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.9.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.9.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.9.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.9.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.9.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.10.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.10.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.10.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.10.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.10.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.10.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.11.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.11.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.11.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.11.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.11.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.11.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.12.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.12.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.12.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.12.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.12.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.12.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.13.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.13.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.13.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.13.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.13.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.13.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.14.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.14.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.14.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.14.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.14.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.14.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.15.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.15.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.15.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.15.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.15.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.15.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage4.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage4.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage4.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage4.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 07:53:35 - -------------weight decay layers---------------
2022-07-06 07:53:35 - name: stage0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.1.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.1.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.2.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.2.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.3.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage1.3.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.1.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.1.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.2.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.2.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.3.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.3.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.4.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.4.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.5.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage2.5.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.1.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.1.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.2.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.2.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.3.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.3.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.4.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.4.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.5.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.5.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.6.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.6.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.7.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.7.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.8.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.8.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.9.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.9.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.10.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.10.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.11.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.11.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.12.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.12.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.13.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.13.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.14.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.14.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.15.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage3.15.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage4.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: stage4.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 07:53:35 - epoch 001 lr: 0.100000
2022-07-06 07:54:26 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8823
2022-07-06 07:55:11 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.7602
2022-07-06 07:55:57 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.5750
2022-07-06 07:56:42 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.4606
2022-07-06 07:57:27 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.2568
2022-07-06 07:58:12 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.0539
2022-07-06 07:58:58 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.0613
2022-07-06 07:59:43 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 5.9883
2022-07-06 08:00:28 - train: epoch 0001, iter [00900, 05004], lr: 0.099999, loss: 5.8749
2022-07-06 08:01:13 - train: epoch 0001, iter [01000, 05004], lr: 0.099999, loss: 5.8278
2022-07-06 08:01:58 - train: epoch 0001, iter [01100, 05004], lr: 0.099999, loss: 5.8259
2022-07-06 08:02:44 - train: epoch 0001, iter [01200, 05004], lr: 0.099999, loss: 5.5528
2022-07-06 08:03:29 - train: epoch 0001, iter [01300, 05004], lr: 0.099999, loss: 5.4291
2022-07-06 08:04:14 - train: epoch 0001, iter [01400, 05004], lr: 0.099999, loss: 5.4249
2022-07-06 08:04:59 - train: epoch 0001, iter [01500, 05004], lr: 0.099998, loss: 5.3539
2022-07-06 08:05:44 - train: epoch 0001, iter [01600, 05004], lr: 0.099998, loss: 5.3629
2022-07-06 08:06:30 - train: epoch 0001, iter [01700, 05004], lr: 0.099998, loss: 5.1883
2022-07-06 08:07:15 - train: epoch 0001, iter [01800, 05004], lr: 0.099998, loss: 5.2634
2022-07-06 08:08:00 - train: epoch 0001, iter [01900, 05004], lr: 0.099998, loss: 5.1107
2022-07-06 08:08:46 - train: epoch 0001, iter [02000, 05004], lr: 0.099997, loss: 4.8942
2022-07-06 08:09:31 - train: epoch 0001, iter [02100, 05004], lr: 0.099997, loss: 4.9017
2022-07-06 08:10:16 - train: epoch 0001, iter [02200, 05004], lr: 0.099997, loss: 4.8560
2022-07-06 08:11:01 - train: epoch 0001, iter [02300, 05004], lr: 0.099996, loss: 4.7232
2022-07-06 08:11:46 - train: epoch 0001, iter [02400, 05004], lr: 0.099996, loss: 4.6949
2022-07-06 08:12:31 - train: epoch 0001, iter [02500, 05004], lr: 0.099996, loss: 4.7996
2022-07-06 08:13:17 - train: epoch 0001, iter [02600, 05004], lr: 0.099995, loss: 4.7643
2022-07-06 08:14:02 - train: epoch 0001, iter [02700, 05004], lr: 0.099995, loss: 4.7464
2022-07-06 08:14:47 - train: epoch 0001, iter [02800, 05004], lr: 0.099995, loss: 4.4817
2022-07-06 08:15:32 - train: epoch 0001, iter [02900, 05004], lr: 0.099994, loss: 4.3562
2022-07-06 08:16:17 - train: epoch 0001, iter [03000, 05004], lr: 0.099994, loss: 4.4975
2022-07-06 08:17:02 - train: epoch 0001, iter [03100, 05004], lr: 0.099993, loss: 4.6110
2022-07-06 08:17:48 - train: epoch 0001, iter [03200, 05004], lr: 0.099993, loss: 4.4088
2022-07-06 08:18:33 - train: epoch 0001, iter [03300, 05004], lr: 0.099993, loss: 4.1388
2022-07-06 08:19:18 - train: epoch 0001, iter [03400, 05004], lr: 0.099992, loss: 4.2093
2022-07-06 08:20:03 - train: epoch 0001, iter [03500, 05004], lr: 0.099992, loss: 4.2095
2022-07-06 08:20:49 - train: epoch 0001, iter [03600, 05004], lr: 0.099991, loss: 4.1577
2022-07-06 08:21:34 - train: epoch 0001, iter [03700, 05004], lr: 0.099991, loss: 4.2093
2022-07-06 08:22:19 - train: epoch 0001, iter [03800, 05004], lr: 0.099990, loss: 4.0260
2022-07-06 08:23:04 - train: epoch 0001, iter [03900, 05004], lr: 0.099990, loss: 4.1128
2022-07-06 08:23:49 - train: epoch 0001, iter [04000, 05004], lr: 0.099989, loss: 4.0390
2022-07-06 08:24:35 - train: epoch 0001, iter [04100, 05004], lr: 0.099988, loss: 4.1290
2022-07-06 08:25:20 - train: epoch 0001, iter [04200, 05004], lr: 0.099988, loss: 3.9674
2022-07-06 08:26:05 - train: epoch 0001, iter [04300, 05004], lr: 0.099987, loss: 3.9604
2022-07-06 08:26:50 - train: epoch 0001, iter [04400, 05004], lr: 0.099987, loss: 3.6586
2022-07-06 08:27:36 - train: epoch 0001, iter [04500, 05004], lr: 0.099986, loss: 3.8912
2022-07-06 08:28:21 - train: epoch 0001, iter [04600, 05004], lr: 0.099986, loss: 4.1023
2022-07-06 08:29:06 - train: epoch 0001, iter [04700, 05004], lr: 0.099985, loss: 3.7887
2022-07-06 08:29:51 - train: epoch 0001, iter [04800, 05004], lr: 0.099984, loss: 4.0137
2022-07-06 08:30:36 - train: epoch 0001, iter [04900, 05004], lr: 0.099984, loss: 3.6854
2022-07-06 08:31:21 - train: epoch 0001, iter [05000, 05004], lr: 0.099983, loss: 3.6689
2022-07-06 08:31:24 - train: epoch 001, train_loss: 4.8717
2022-07-06 08:32:42 - eval: epoch: 001, acc1: 26.536%, acc5: 51.832%, test_loss: 3.5125, per_image_load_time: 0.787ms, per_image_inference_time: 0.843ms
2022-07-06 08:32:43 - until epoch: 001, best_acc1: 26.536%
2022-07-06 08:32:43 - epoch 002 lr: 0.099983
2022-07-06 08:33:34 - train: epoch 0002, iter [00100, 05004], lr: 0.099982, loss: 3.8052
2022-07-06 08:34:19 - train: epoch 0002, iter [00200, 05004], lr: 0.099981, loss: 3.5589
2022-07-06 08:35:04 - train: epoch 0002, iter [00300, 05004], lr: 0.099981, loss: 3.7770
2022-07-06 08:35:50 - train: epoch 0002, iter [00400, 05004], lr: 0.099980, loss: 3.7648
2022-07-06 08:36:35 - train: epoch 0002, iter [00500, 05004], lr: 0.099979, loss: 3.4186
2022-07-06 08:37:20 - train: epoch 0002, iter [00600, 05004], lr: 0.099979, loss: 3.4217
2022-07-06 08:38:06 - train: epoch 0002, iter [00700, 05004], lr: 0.099978, loss: 3.6855
2022-07-06 08:38:51 - train: epoch 0002, iter [00800, 05004], lr: 0.099977, loss: 3.3322
2022-07-06 08:39:36 - train: epoch 0002, iter [00900, 05004], lr: 0.099976, loss: 3.2241
2022-07-06 08:40:21 - train: epoch 0002, iter [01000, 05004], lr: 0.099975, loss: 3.6489
2022-07-06 08:41:07 - train: epoch 0002, iter [01100, 05004], lr: 0.099975, loss: 3.7010
2022-07-06 08:41:52 - train: epoch 0002, iter [01200, 05004], lr: 0.099974, loss: 3.4586
2022-07-06 08:42:37 - train: epoch 0002, iter [01300, 05004], lr: 0.099973, loss: 3.3245
2022-07-06 08:43:22 - train: epoch 0002, iter [01400, 05004], lr: 0.099972, loss: 3.3936
2022-07-06 08:44:07 - train: epoch 0002, iter [01500, 05004], lr: 0.099971, loss: 3.5067
2022-07-06 08:44:53 - train: epoch 0002, iter [01600, 05004], lr: 0.099970, loss: 3.3003
2022-07-06 08:45:38 - train: epoch 0002, iter [01700, 05004], lr: 0.099969, loss: 3.4528
2022-07-06 08:46:23 - train: epoch 0002, iter [01800, 05004], lr: 0.099968, loss: 3.4411
2022-07-06 08:47:08 - train: epoch 0002, iter [01900, 05004], lr: 0.099967, loss: 3.2651
2022-07-06 08:47:53 - train: epoch 0002, iter [02000, 05004], lr: 0.099966, loss: 2.9975
2022-07-06 08:48:38 - train: epoch 0002, iter [02100, 05004], lr: 0.099965, loss: 3.2819
2022-07-06 08:49:24 - train: epoch 0002, iter [02200, 05004], lr: 0.099964, loss: 3.0131
2022-07-06 08:50:09 - train: epoch 0002, iter [02300, 05004], lr: 0.099963, loss: 3.3110
2022-07-06 08:50:54 - train: epoch 0002, iter [02400, 05004], lr: 0.099962, loss: 3.0319
2022-07-06 08:51:39 - train: epoch 0002, iter [02500, 05004], lr: 0.099961, loss: 3.0030
2022-07-06 08:52:24 - train: epoch 0002, iter [02600, 05004], lr: 0.099960, loss: 3.0822
2022-07-06 08:53:09 - train: epoch 0002, iter [02700, 05004], lr: 0.099959, loss: 3.3674
2022-07-06 08:53:54 - train: epoch 0002, iter [02800, 05004], lr: 0.099958, loss: 3.2556
2022-07-06 08:54:39 - train: epoch 0002, iter [02900, 05004], lr: 0.099957, loss: 3.0443
2022-07-06 08:55:25 - train: epoch 0002, iter [03000, 05004], lr: 0.099956, loss: 2.9830
2022-07-06 08:56:10 - train: epoch 0002, iter [03100, 05004], lr: 0.099955, loss: 3.0368
2022-07-06 08:56:55 - train: epoch 0002, iter [03200, 05004], lr: 0.099954, loss: 3.0281
2022-07-06 08:57:40 - train: epoch 0002, iter [03300, 05004], lr: 0.099953, loss: 2.9944
2022-07-06 08:58:26 - train: epoch 0002, iter [03400, 05004], lr: 0.099952, loss: 3.0780
2022-07-06 08:59:11 - train: epoch 0002, iter [03500, 05004], lr: 0.099951, loss: 3.0252
2022-07-06 08:59:56 - train: epoch 0002, iter [03600, 05004], lr: 0.099949, loss: 3.0691
2022-07-06 09:00:41 - train: epoch 0002, iter [03700, 05004], lr: 0.099948, loss: 3.1244
2022-07-06 09:01:27 - train: epoch 0002, iter [03800, 05004], lr: 0.099947, loss: 2.9565
2022-07-06 09:02:12 - train: epoch 0002, iter [03900, 05004], lr: 0.099946, loss: 3.0106
2022-07-06 09:02:58 - train: epoch 0002, iter [04000, 05004], lr: 0.099945, loss: 2.8449
2022-07-06 09:03:43 - train: epoch 0002, iter [04100, 05004], lr: 0.099943, loss: 3.1042
2022-07-06 09:04:28 - train: epoch 0002, iter [04200, 05004], lr: 0.099942, loss: 2.8978
2022-07-06 09:05:13 - train: epoch 0002, iter [04300, 05004], lr: 0.099941, loss: 2.8943
2022-07-06 09:05:59 - train: epoch 0002, iter [04400, 05004], lr: 0.099939, loss: 2.8775
2022-07-06 09:06:44 - train: epoch 0002, iter [04500, 05004], lr: 0.099938, loss: 2.7790
2022-07-06 09:07:29 - train: epoch 0002, iter [04600, 05004], lr: 0.099937, loss: 2.9001
2022-07-06 09:08:15 - train: epoch 0002, iter [04700, 05004], lr: 0.099936, loss: 2.9279
2022-07-06 09:09:00 - train: epoch 0002, iter [04800, 05004], lr: 0.099934, loss: 2.8754
2022-07-06 09:09:45 - train: epoch 0002, iter [04900, 05004], lr: 0.099933, loss: 2.8191
2022-07-06 09:10:31 - train: epoch 0002, iter [05000, 05004], lr: 0.099932, loss: 2.7808
2022-07-06 09:10:33 - train: epoch 002, train_loss: 3.2152
2022-07-06 09:11:50 - eval: epoch: 002, acc1: 40.778%, acc5: 67.594%, test_loss: 2.6458, per_image_load_time: 2.114ms, per_image_inference_time: 0.834ms
2022-07-06 09:11:52 - until epoch: 002, best_acc1: 40.778%
2022-07-06 09:11:52 - epoch 003 lr: 0.099931
2022-07-06 09:12:43 - train: epoch 0003, iter [00100, 05004], lr: 0.099930, loss: 2.9920
2022-07-06 09:13:29 - train: epoch 0003, iter [00200, 05004], lr: 0.099929, loss: 2.8253
2022-07-06 09:14:14 - train: epoch 0003, iter [00300, 05004], lr: 0.099927, loss: 2.7367
2022-07-06 09:15:00 - train: epoch 0003, iter [00400, 05004], lr: 0.099926, loss: 2.9711
2022-07-06 09:15:46 - train: epoch 0003, iter [00500, 05004], lr: 0.099924, loss: 3.0086
2022-07-06 09:16:32 - train: epoch 0003, iter [00600, 05004], lr: 0.099923, loss: 2.8275
2022-07-06 09:17:18 - train: epoch 0003, iter [00700, 05004], lr: 0.099922, loss: 2.8769
2022-07-06 09:18:03 - train: epoch 0003, iter [00800, 05004], lr: 0.099920, loss: 2.9053
2022-07-06 09:18:49 - train: epoch 0003, iter [00900, 05004], lr: 0.099919, loss: 2.6429
2022-07-06 09:19:34 - train: epoch 0003, iter [01000, 05004], lr: 0.099917, loss: 2.8568
2022-07-06 09:20:20 - train: epoch 0003, iter [01100, 05004], lr: 0.099916, loss: 2.8658
2022-07-06 09:21:06 - train: epoch 0003, iter [01200, 05004], lr: 0.099914, loss: 2.6489
2022-07-06 09:21:51 - train: epoch 0003, iter [01300, 05004], lr: 0.099913, loss: 2.7142
2022-07-06 09:22:37 - train: epoch 0003, iter [01400, 05004], lr: 0.099911, loss: 2.8185
2022-07-06 09:23:22 - train: epoch 0003, iter [01500, 05004], lr: 0.099909, loss: 3.0129
2022-07-06 09:24:08 - train: epoch 0003, iter [01600, 05004], lr: 0.099908, loss: 2.6816
2022-07-06 09:24:54 - train: epoch 0003, iter [01700, 05004], lr: 0.099906, loss: 2.6035
2022-07-06 09:25:39 - train: epoch 0003, iter [01800, 05004], lr: 0.099905, loss: 2.6720
2022-07-06 09:26:25 - train: epoch 0003, iter [01900, 05004], lr: 0.099903, loss: 2.6684
2022-07-06 09:27:11 - train: epoch 0003, iter [02000, 05004], lr: 0.099901, loss: 3.0925
2022-07-06 09:27:56 - train: epoch 0003, iter [02100, 05004], lr: 0.099900, loss: 3.0032
2022-07-06 09:28:42 - train: epoch 0003, iter [02200, 05004], lr: 0.099898, loss: 3.2442
2022-07-06 09:29:27 - train: epoch 0003, iter [02300, 05004], lr: 0.099896, loss: 2.6852
2022-07-06 09:30:13 - train: epoch 0003, iter [02400, 05004], lr: 0.099895, loss: 2.6403
2022-07-06 09:30:59 - train: epoch 0003, iter [02500, 05004], lr: 0.099893, loss: 2.6660
2022-07-06 09:31:44 - train: epoch 0003, iter [02600, 05004], lr: 0.099891, loss: 2.8186
2022-07-06 09:32:30 - train: epoch 0003, iter [02700, 05004], lr: 0.099890, loss: 3.1099
2022-07-06 09:33:15 - train: epoch 0003, iter [02800, 05004], lr: 0.099888, loss: 2.5726
2022-07-06 09:34:01 - train: epoch 0003, iter [02900, 05004], lr: 0.099886, loss: 2.7072
2022-07-06 09:34:47 - train: epoch 0003, iter [03000, 05004], lr: 0.099884, loss: 3.0000
2022-07-06 09:35:33 - train: epoch 0003, iter [03100, 05004], lr: 0.099882, loss: 2.8291
2022-07-06 09:36:18 - train: epoch 0003, iter [03200, 05004], lr: 0.099881, loss: 2.7138
2022-07-06 09:37:04 - train: epoch 0003, iter [03300, 05004], lr: 0.099879, loss: 2.7711
2022-07-06 09:37:49 - train: epoch 0003, iter [03400, 05004], lr: 0.099877, loss: 2.8101
2022-07-06 09:38:35 - train: epoch 0003, iter [03500, 05004], lr: 0.099875, loss: 2.5181
2022-07-06 09:39:20 - train: epoch 0003, iter [03600, 05004], lr: 0.099873, loss: 2.4820
2022-07-06 09:40:06 - train: epoch 0003, iter [03700, 05004], lr: 0.099871, loss: 2.6570
2022-07-06 09:40:51 - train: epoch 0003, iter [03800, 05004], lr: 0.099870, loss: 2.7358
2022-07-06 09:41:37 - train: epoch 0003, iter [03900, 05004], lr: 0.099868, loss: 2.8256
2022-07-06 09:42:23 - train: epoch 0003, iter [04000, 05004], lr: 0.099866, loss: 2.6789
2022-07-06 09:43:08 - train: epoch 0003, iter [04100, 05004], lr: 0.099864, loss: 2.6885
2022-07-06 09:43:54 - train: epoch 0003, iter [04200, 05004], lr: 0.099862, loss: 2.6386
2022-07-06 09:44:39 - train: epoch 0003, iter [04300, 05004], lr: 0.099860, loss: 2.3304
2022-07-06 09:45:24 - train: epoch 0003, iter [04400, 05004], lr: 0.099858, loss: 2.4927
2022-07-06 09:46:10 - train: epoch 0003, iter [04500, 05004], lr: 0.099856, loss: 2.5613
2022-07-06 09:46:55 - train: epoch 0003, iter [04600, 05004], lr: 0.099854, loss: 2.6358
2022-07-06 09:47:41 - train: epoch 0003, iter [04700, 05004], lr: 0.099852, loss: 2.4361
2022-07-06 09:48:26 - train: epoch 0003, iter [04800, 05004], lr: 0.099850, loss: 2.6422
2022-07-06 09:49:12 - train: epoch 0003, iter [04900, 05004], lr: 0.099848, loss: 2.6874
2022-07-06 09:49:57 - train: epoch 0003, iter [05000, 05004], lr: 0.099846, loss: 2.6633
2022-07-06 09:50:00 - train: epoch 003, train_loss: 2.7205
2022-07-06 09:51:17 - eval: epoch: 003, acc1: 49.598%, acc5: 75.258%, test_loss: 2.2043, per_image_load_time: 2.179ms, per_image_inference_time: 0.841ms
2022-07-06 09:51:19 - until epoch: 003, best_acc1: 49.598%
2022-07-06 09:51:19 - epoch 004 lr: 0.099846
2022-07-06 09:52:10 - train: epoch 0004, iter [00100, 05004], lr: 0.099844, loss: 2.6052
2022-07-06 09:52:56 - train: epoch 0004, iter [00200, 05004], lr: 0.099842, loss: 2.4805
2022-07-06 09:53:41 - train: epoch 0004, iter [00300, 05004], lr: 0.099840, loss: 2.5080
2022-07-06 09:54:26 - train: epoch 0004, iter [00400, 05004], lr: 0.099838, loss: 2.4553
2022-07-06 09:55:12 - train: epoch 0004, iter [00500, 05004], lr: 0.099835, loss: 2.4015
2022-07-06 09:55:57 - train: epoch 0004, iter [00600, 05004], lr: 0.099833, loss: 2.6477
2022-07-06 09:56:43 - train: epoch 0004, iter [00700, 05004], lr: 0.099831, loss: 2.5541
2022-07-06 09:57:29 - train: epoch 0004, iter [00800, 05004], lr: 0.099829, loss: 2.3209
2022-07-06 09:58:14 - train: epoch 0004, iter [00900, 05004], lr: 0.099827, loss: 2.2632
2022-07-06 09:59:00 - train: epoch 0004, iter [01000, 05004], lr: 0.099825, loss: 2.4956
2022-07-06 09:59:45 - train: epoch 0004, iter [01100, 05004], lr: 0.099822, loss: 2.6383
2022-07-06 10:00:31 - train: epoch 0004, iter [01200, 05004], lr: 0.099820, loss: 2.3212
2022-07-06 10:01:16 - train: epoch 0004, iter [01300, 05004], lr: 0.099818, loss: 2.2961
2022-07-06 10:02:02 - train: epoch 0004, iter [01400, 05004], lr: 0.099816, loss: 2.6409
2022-07-06 10:02:47 - train: epoch 0004, iter [01500, 05004], lr: 0.099814, loss: 2.6081
2022-07-06 10:03:33 - train: epoch 0004, iter [01600, 05004], lr: 0.099811, loss: 2.4673
2022-07-06 10:04:18 - train: epoch 0004, iter [01700, 05004], lr: 0.099809, loss: 2.5780
2022-07-06 10:05:04 - train: epoch 0004, iter [01800, 05004], lr: 0.099807, loss: 2.7235
2022-07-06 10:05:49 - train: epoch 0004, iter [01900, 05004], lr: 0.099804, loss: 2.6675
2022-07-06 10:06:35 - train: epoch 0004, iter [02000, 05004], lr: 0.099802, loss: 2.5185
2022-07-06 10:07:20 - train: epoch 0004, iter [02100, 05004], lr: 0.099800, loss: 2.4969
2022-07-06 10:08:06 - train: epoch 0004, iter [02200, 05004], lr: 0.099797, loss: 2.5148
2022-07-06 10:08:51 - train: epoch 0004, iter [02300, 05004], lr: 0.099795, loss: 2.3554
2022-07-06 10:09:37 - train: epoch 0004, iter [02400, 05004], lr: 0.099793, loss: 2.4668
2022-07-06 10:10:22 - train: epoch 0004, iter [02500, 05004], lr: 0.099790, loss: 2.4947
2022-07-06 10:11:08 - train: epoch 0004, iter [02600, 05004], lr: 0.099788, loss: 2.5849
2022-07-06 10:11:53 - train: epoch 0004, iter [02700, 05004], lr: 0.099785, loss: 2.3675
2022-07-06 10:12:39 - train: epoch 0004, iter [02800, 05004], lr: 0.099783, loss: 2.4713
2022-07-06 10:13:24 - train: epoch 0004, iter [02900, 05004], lr: 0.099781, loss: 2.3708
2022-07-06 10:14:10 - train: epoch 0004, iter [03000, 05004], lr: 0.099778, loss: 2.5027
2022-07-06 10:14:55 - train: epoch 0004, iter [03100, 05004], lr: 0.099776, loss: 2.5044
2022-07-06 10:15:40 - train: epoch 0004, iter [03200, 05004], lr: 0.099773, loss: 2.4588
2022-07-06 10:16:26 - train: epoch 0004, iter [03300, 05004], lr: 0.099771, loss: 2.5821
2022-07-06 10:17:11 - train: epoch 0004, iter [03400, 05004], lr: 0.099768, loss: 2.5286
2022-07-06 10:17:57 - train: epoch 0004, iter [03500, 05004], lr: 0.099766, loss: 2.4544
2022-07-06 10:18:42 - train: epoch 0004, iter [03600, 05004], lr: 0.099763, loss: 2.3304
2022-07-06 10:19:28 - train: epoch 0004, iter [03700, 05004], lr: 0.099761, loss: 2.3620
2022-07-06 10:20:14 - train: epoch 0004, iter [03800, 05004], lr: 0.099758, loss: 2.4355
2022-07-06 10:20:59 - train: epoch 0004, iter [03900, 05004], lr: 0.099755, loss: 2.4791
2022-07-06 10:21:45 - train: epoch 0004, iter [04000, 05004], lr: 0.099753, loss: 2.1847
2022-07-06 10:22:30 - train: epoch 0004, iter [04100, 05004], lr: 0.099750, loss: 2.4787
2022-07-06 10:23:16 - train: epoch 0004, iter [04200, 05004], lr: 0.099748, loss: 2.3154
2022-07-06 10:24:02 - train: epoch 0004, iter [04300, 05004], lr: 0.099745, loss: 2.1968
2022-07-06 10:24:47 - train: epoch 0004, iter [04400, 05004], lr: 0.099742, loss: 2.2538
2022-07-06 10:25:33 - train: epoch 0004, iter [04500, 05004], lr: 0.099740, loss: 1.9610
2022-07-06 10:26:18 - train: epoch 0004, iter [04600, 05004], lr: 0.099737, loss: 2.3779
2022-07-06 10:27:04 - train: epoch 0004, iter [04700, 05004], lr: 0.099734, loss: 2.2905
2022-07-06 10:27:49 - train: epoch 0004, iter [04800, 05004], lr: 0.099732, loss: 2.3186
2022-07-06 10:28:35 - train: epoch 0004, iter [04900, 05004], lr: 0.099729, loss: 2.4684
2022-07-06 10:29:20 - train: epoch 0004, iter [05000, 05004], lr: 0.099726, loss: 2.5783
2022-07-06 10:29:23 - train: epoch 004, train_loss: 2.4902
2022-07-06 10:30:40 - eval: epoch: 004, acc1: 51.072%, acc5: 76.466%, test_loss: 2.1237, per_image_load_time: 2.096ms, per_image_inference_time: 0.847ms
2022-07-06 10:30:41 - until epoch: 004, best_acc1: 51.072%
2022-07-06 10:30:41 - epoch 005 lr: 0.099726
2022-07-06 10:31:32 - train: epoch 0005, iter [00100, 05004], lr: 0.099723, loss: 2.4454
2022-07-06 10:32:18 - train: epoch 0005, iter [00200, 05004], lr: 0.099721, loss: 2.4544
2022-07-06 10:33:03 - train: epoch 0005, iter [00300, 05004], lr: 0.099718, loss: 2.5165
2022-07-06 10:33:49 - train: epoch 0005, iter [00400, 05004], lr: 0.099715, loss: 2.3423
2022-07-06 10:34:34 - train: epoch 0005, iter [00500, 05004], lr: 0.099712, loss: 2.1969
2022-07-06 10:35:19 - train: epoch 0005, iter [00600, 05004], lr: 0.099709, loss: 2.3998
2022-07-06 10:36:04 - train: epoch 0005, iter [00700, 05004], lr: 0.099707, loss: 2.4631
2022-07-06 10:36:50 - train: epoch 0005, iter [00800, 05004], lr: 0.099704, loss: 2.5811
2022-07-06 10:37:35 - train: epoch 0005, iter [00900, 05004], lr: 0.099701, loss: 2.3516
2022-07-06 10:38:20 - train: epoch 0005, iter [01000, 05004], lr: 0.099698, loss: 2.4462
2022-07-06 10:39:06 - train: epoch 0005, iter [01100, 05004], lr: 0.099695, loss: 2.5645
2022-07-06 10:39:51 - train: epoch 0005, iter [01200, 05004], lr: 0.099692, loss: 2.4024
2022-07-06 10:40:37 - train: epoch 0005, iter [01300, 05004], lr: 0.099689, loss: 2.4034
2022-07-06 10:41:22 - train: epoch 0005, iter [01400, 05004], lr: 0.099686, loss: 2.3922
2022-07-06 10:42:08 - train: epoch 0005, iter [01500, 05004], lr: 0.099684, loss: 2.1521
2022-07-06 10:42:53 - train: epoch 0005, iter [01600, 05004], lr: 0.099681, loss: 2.1387
2022-07-06 10:43:38 - train: epoch 0005, iter [01700, 05004], lr: 0.099678, loss: 2.3818
2022-07-06 10:44:24 - train: epoch 0005, iter [01800, 05004], lr: 0.099675, loss: 2.3716
2022-07-06 10:45:09 - train: epoch 0005, iter [01900, 05004], lr: 0.099672, loss: 2.2300
2022-07-06 10:45:55 - train: epoch 0005, iter [02000, 05004], lr: 0.099669, loss: 2.3976
2022-07-06 10:46:40 - train: epoch 0005, iter [02100, 05004], lr: 0.099666, loss: 2.1114
2022-07-06 10:47:26 - train: epoch 0005, iter [02200, 05004], lr: 0.099663, loss: 2.0802
2022-07-06 10:48:11 - train: epoch 0005, iter [02300, 05004], lr: 0.099660, loss: 2.1142
2022-07-06 10:48:56 - train: epoch 0005, iter [02400, 05004], lr: 0.099657, loss: 2.3417
2022-07-06 10:49:41 - train: epoch 0005, iter [02500, 05004], lr: 0.099653, loss: 2.4598
2022-07-06 10:50:27 - train: epoch 0005, iter [02600, 05004], lr: 0.099650, loss: 2.4999
2022-07-06 10:51:12 - train: epoch 0005, iter [02700, 05004], lr: 0.099647, loss: 2.4605
2022-07-06 10:51:57 - train: epoch 0005, iter [02800, 05004], lr: 0.099644, loss: 2.4493
2022-07-06 10:52:43 - train: epoch 0005, iter [02900, 05004], lr: 0.099641, loss: 2.3070
2022-07-06 10:53:28 - train: epoch 0005, iter [03000, 05004], lr: 0.099638, loss: 2.2768
2022-07-06 10:54:14 - train: epoch 0005, iter [03100, 05004], lr: 0.099635, loss: 2.3154
2022-07-06 10:54:59 - train: epoch 0005, iter [03200, 05004], lr: 0.099632, loss: 2.4615
2022-07-06 10:55:44 - train: epoch 0005, iter [03300, 05004], lr: 0.099628, loss: 2.1668
2022-07-06 10:56:30 - train: epoch 0005, iter [03400, 05004], lr: 0.099625, loss: 2.1118
2022-07-06 10:57:15 - train: epoch 0005, iter [03500, 05004], lr: 0.099622, loss: 2.3211
2022-07-06 10:58:01 - train: epoch 0005, iter [03600, 05004], lr: 0.099619, loss: 2.3406
2022-07-06 10:58:46 - train: epoch 0005, iter [03700, 05004], lr: 0.099616, loss: 2.2012
2022-07-06 10:59:31 - train: epoch 0005, iter [03800, 05004], lr: 0.099612, loss: 2.1526
2022-07-06 11:00:17 - train: epoch 0005, iter [03900, 05004], lr: 0.099609, loss: 2.7699
2022-07-06 11:01:02 - train: epoch 0005, iter [04000, 05004], lr: 0.099606, loss: 2.3192
2022-07-06 11:01:47 - train: epoch 0005, iter [04100, 05004], lr: 0.099603, loss: 2.4039
2022-07-06 11:02:33 - train: epoch 0005, iter [04200, 05004], lr: 0.099599, loss: 2.4063
2022-07-06 11:03:18 - train: epoch 0005, iter [04300, 05004], lr: 0.099596, loss: 2.2539
2022-07-06 11:04:03 - train: epoch 0005, iter [04400, 05004], lr: 0.099593, loss: 2.3190
2022-07-06 11:04:49 - train: epoch 0005, iter [04500, 05004], lr: 0.099589, loss: 2.3288
2022-07-06 11:05:34 - train: epoch 0005, iter [04600, 05004], lr: 0.099586, loss: 2.2004
2022-07-06 11:06:19 - train: epoch 0005, iter [04700, 05004], lr: 0.099583, loss: 2.0331
2022-07-06 11:07:05 - train: epoch 0005, iter [04800, 05004], lr: 0.099579, loss: 2.0588
2022-07-06 11:07:50 - train: epoch 0005, iter [04900, 05004], lr: 0.099576, loss: 2.4455
2022-07-06 11:08:35 - train: epoch 0005, iter [05000, 05004], lr: 0.099572, loss: 2.2516
2022-07-06 11:08:38 - train: epoch 005, train_loss: 2.3545
2022-07-06 11:09:55 - eval: epoch: 005, acc1: 52.904%, acc5: 78.328%, test_loss: 2.0233, per_image_load_time: 1.859ms, per_image_inference_time: 0.822ms
2022-07-06 11:09:56 - until epoch: 005, best_acc1: 52.904%
2022-07-06 11:09:56 - epoch 006 lr: 0.099572
2022-07-06 11:10:48 - train: epoch 0006, iter [00100, 05004], lr: 0.099569, loss: 2.2260
2022-07-06 11:11:33 - train: epoch 0006, iter [00200, 05004], lr: 0.099565, loss: 2.4585
2022-07-06 11:12:18 - train: epoch 0006, iter [00300, 05004], lr: 0.099562, loss: 2.1168
2022-07-06 11:13:04 - train: epoch 0006, iter [00400, 05004], lr: 0.099558, loss: 2.2498
2022-07-06 11:13:49 - train: epoch 0006, iter [00500, 05004], lr: 0.099555, loss: 2.3602
2022-07-06 11:14:34 - train: epoch 0006, iter [00600, 05004], lr: 0.099552, loss: 2.3531
2022-07-06 11:15:20 - train: epoch 0006, iter [00700, 05004], lr: 0.099548, loss: 2.3336
2022-07-06 11:16:05 - train: epoch 0006, iter [00800, 05004], lr: 0.099544, loss: 2.2673
2022-07-06 11:16:50 - train: epoch 0006, iter [00900, 05004], lr: 0.099541, loss: 2.1828
2022-07-06 11:17:35 - train: epoch 0006, iter [01000, 05004], lr: 0.099537, loss: 2.1878
2022-07-06 11:18:21 - train: epoch 0006, iter [01100, 05004], lr: 0.099534, loss: 2.2460
2022-07-06 11:19:06 - train: epoch 0006, iter [01200, 05004], lr: 0.099530, loss: 2.3407
2022-07-06 11:19:51 - train: epoch 0006, iter [01300, 05004], lr: 0.099527, loss: 2.3255
2022-07-06 11:20:36 - train: epoch 0006, iter [01400, 05004], lr: 0.099523, loss: 2.4019
2022-07-06 11:21:22 - train: epoch 0006, iter [01500, 05004], lr: 0.099520, loss: 2.4090
2022-07-06 11:22:07 - train: epoch 0006, iter [01600, 05004], lr: 0.099516, loss: 2.1150
2022-07-06 11:22:52 - train: epoch 0006, iter [01700, 05004], lr: 0.099512, loss: 2.4646
2022-07-06 11:23:37 - train: epoch 0006, iter [01800, 05004], lr: 0.099509, loss: 2.3384
2022-07-06 11:24:22 - train: epoch 0006, iter [01900, 05004], lr: 0.099505, loss: 2.1673
2022-07-06 11:25:08 - train: epoch 0006, iter [02000, 05004], lr: 0.099501, loss: 2.4366
2022-07-06 11:25:53 - train: epoch 0006, iter [02100, 05004], lr: 0.099498, loss: 2.4137
2022-07-06 11:26:38 - train: epoch 0006, iter [02200, 05004], lr: 0.099494, loss: 2.2073
2022-07-06 11:27:23 - train: epoch 0006, iter [02300, 05004], lr: 0.099490, loss: 2.1816
2022-07-06 11:28:08 - train: epoch 0006, iter [02400, 05004], lr: 0.099486, loss: 2.2160
2022-07-06 11:28:53 - train: epoch 0006, iter [02500, 05004], lr: 0.099483, loss: 2.5108
2022-07-06 11:29:39 - train: epoch 0006, iter [02600, 05004], lr: 0.099479, loss: 2.0754
2022-07-06 11:30:24 - train: epoch 0006, iter [02700, 05004], lr: 0.099475, loss: 2.4777
2022-07-06 11:31:09 - train: epoch 0006, iter [02800, 05004], lr: 0.099471, loss: 2.1774
2022-07-06 11:31:55 - train: epoch 0006, iter [02900, 05004], lr: 0.099468, loss: 2.4018
2022-07-06 11:32:40 - train: epoch 0006, iter [03000, 05004], lr: 0.099464, loss: 2.2406
2022-07-06 11:33:25 - train: epoch 0006, iter [03100, 05004], lr: 0.099460, loss: 2.0349
2022-07-06 11:34:10 - train: epoch 0006, iter [03200, 05004], lr: 0.099456, loss: 2.2130
2022-07-06 11:34:56 - train: epoch 0006, iter [03300, 05004], lr: 0.099452, loss: 2.0734
2022-07-06 11:35:41 - train: epoch 0006, iter [03400, 05004], lr: 0.099448, loss: 2.4650
2022-07-06 11:36:26 - train: epoch 0006, iter [03500, 05004], lr: 0.099444, loss: 2.3845
2022-07-06 11:37:11 - train: epoch 0006, iter [03600, 05004], lr: 0.099441, loss: 2.2553
2022-07-06 11:37:57 - train: epoch 0006, iter [03700, 05004], lr: 0.099437, loss: 2.3333
2022-07-06 11:38:42 - train: epoch 0006, iter [03800, 05004], lr: 0.099433, loss: 2.1255
2022-07-06 11:39:27 - train: epoch 0006, iter [03900, 05004], lr: 0.099429, loss: 2.2913
2022-07-06 11:40:12 - train: epoch 0006, iter [04000, 05004], lr: 0.099425, loss: 2.4851
2022-07-06 11:40:57 - train: epoch 0006, iter [04100, 05004], lr: 0.099421, loss: 2.2020
2022-07-06 11:41:42 - train: epoch 0006, iter [04200, 05004], lr: 0.099417, loss: 2.0768
2022-07-06 11:42:28 - train: epoch 0006, iter [04300, 05004], lr: 0.099413, loss: 2.2992
2022-07-06 11:43:13 - train: epoch 0006, iter [04400, 05004], lr: 0.099409, loss: 2.3206
2022-07-06 11:43:58 - train: epoch 0006, iter [04500, 05004], lr: 0.099405, loss: 2.2436
2022-07-06 11:44:43 - train: epoch 0006, iter [04600, 05004], lr: 0.099401, loss: 2.1975
2022-07-06 11:45:28 - train: epoch 0006, iter [04700, 05004], lr: 0.099397, loss: 2.2325
2022-07-06 11:46:13 - train: epoch 0006, iter [04800, 05004], lr: 0.099393, loss: 2.2131
2022-07-06 11:46:59 - train: epoch 0006, iter [04900, 05004], lr: 0.099389, loss: 2.2373
2022-07-06 11:47:44 - train: epoch 0006, iter [05000, 05004], lr: 0.099385, loss: 2.0522
2022-07-06 11:47:46 - train: epoch 006, train_loss: 2.2684
2022-07-06 11:49:02 - eval: epoch: 006, acc1: 54.466%, acc5: 79.268%, test_loss: 1.9511, per_image_load_time: 2.113ms, per_image_inference_time: 0.834ms
2022-07-06 11:49:03 - until epoch: 006, best_acc1: 54.466%
2022-07-06 11:49:03 - epoch 007 lr: 0.099384
2022-07-06 11:49:55 - train: epoch 0007, iter [00100, 05004], lr: 0.099380, loss: 2.1578
2022-07-06 11:50:40 - train: epoch 0007, iter [00200, 05004], lr: 0.099376, loss: 2.4037
2022-07-06 11:51:25 - train: epoch 0007, iter [00300, 05004], lr: 0.099372, loss: 2.3742
2022-07-06 11:52:10 - train: epoch 0007, iter [00400, 05004], lr: 0.099368, loss: 2.2423
2022-07-06 11:52:55 - train: epoch 0007, iter [00500, 05004], lr: 0.099364, loss: 2.1314
2022-07-06 11:53:40 - train: epoch 0007, iter [00600, 05004], lr: 0.099360, loss: 2.4268
2022-07-06 11:54:26 - train: epoch 0007, iter [00700, 05004], lr: 0.099355, loss: 2.2467
2022-07-06 11:55:11 - train: epoch 0007, iter [00800, 05004], lr: 0.099351, loss: 2.2179
2022-07-06 11:55:56 - train: epoch 0007, iter [00900, 05004], lr: 0.099347, loss: 2.2605
2022-07-06 11:56:41 - train: epoch 0007, iter [01000, 05004], lr: 0.099343, loss: 2.2255
2022-07-06 11:57:27 - train: epoch 0007, iter [01100, 05004], lr: 0.099339, loss: 2.1134
2022-07-06 11:58:12 - train: epoch 0007, iter [01200, 05004], lr: 0.099334, loss: 2.2137
2022-07-06 11:58:57 - train: epoch 0007, iter [01300, 05004], lr: 0.099330, loss: 2.1672
2022-07-06 11:59:42 - train: epoch 0007, iter [01400, 05004], lr: 0.099326, loss: 2.1162
2022-07-06 12:00:27 - train: epoch 0007, iter [01500, 05004], lr: 0.099322, loss: 2.3786
2022-07-06 12:01:12 - train: epoch 0007, iter [01600, 05004], lr: 0.099317, loss: 2.2382
2022-07-06 12:01:57 - train: epoch 0007, iter [01700, 05004], lr: 0.099313, loss: 2.2619
2022-07-06 12:02:42 - train: epoch 0007, iter [01800, 05004], lr: 0.099309, loss: 2.1734
2022-07-06 12:03:27 - train: epoch 0007, iter [01900, 05004], lr: 0.099304, loss: 2.2846
2022-07-06 12:04:12 - train: epoch 0007, iter [02000, 05004], lr: 0.099300, loss: 1.9176
2022-07-06 12:04:58 - train: epoch 0007, iter [02100, 05004], lr: 0.099296, loss: 2.2286
2022-07-06 12:05:43 - train: epoch 0007, iter [02200, 05004], lr: 0.099291, loss: 2.0841
2022-07-06 12:06:28 - train: epoch 0007, iter [02300, 05004], lr: 0.099287, loss: 2.2478
2022-07-06 12:07:13 - train: epoch 0007, iter [02400, 05004], lr: 0.099282, loss: 2.2066
2022-07-06 12:07:58 - train: epoch 0007, iter [02500, 05004], lr: 0.099278, loss: 2.0571
2022-07-06 12:08:43 - train: epoch 0007, iter [02600, 05004], lr: 0.099273, loss: 2.1416
2022-07-06 12:09:28 - train: epoch 0007, iter [02700, 05004], lr: 0.099269, loss: 2.1095
2022-07-06 12:10:14 - train: epoch 0007, iter [02800, 05004], lr: 0.099265, loss: 2.2019
2022-07-06 12:10:59 - train: epoch 0007, iter [02900, 05004], lr: 0.099260, loss: 2.1616
2022-07-06 12:11:44 - train: epoch 0007, iter [03000, 05004], lr: 0.099256, loss: 2.2099
2022-07-06 12:12:29 - train: epoch 0007, iter [03100, 05004], lr: 0.099251, loss: 2.1592
2022-07-06 12:13:15 - train: epoch 0007, iter [03200, 05004], lr: 0.099247, loss: 2.1859
2022-07-06 12:14:00 - train: epoch 0007, iter [03300, 05004], lr: 0.099242, loss: 2.4488
2022-07-06 12:14:45 - train: epoch 0007, iter [03400, 05004], lr: 0.099237, loss: 2.0606
2022-07-06 12:15:30 - train: epoch 0007, iter [03500, 05004], lr: 0.099233, loss: 2.3130
2022-07-06 12:16:15 - train: epoch 0007, iter [03600, 05004], lr: 0.099228, loss: 2.0125
2022-07-06 12:17:00 - train: epoch 0007, iter [03700, 05004], lr: 0.099224, loss: 2.1519
2022-07-06 12:17:46 - train: epoch 0007, iter [03800, 05004], lr: 0.099219, loss: 2.3110
2022-07-06 12:18:31 - train: epoch 0007, iter [03900, 05004], lr: 0.099215, loss: 2.0957
2022-07-06 12:19:16 - train: epoch 0007, iter [04000, 05004], lr: 0.099210, loss: 2.2593
2022-07-06 12:20:01 - train: epoch 0007, iter [04100, 05004], lr: 0.099205, loss: 2.1373
2022-07-06 12:20:47 - train: epoch 0007, iter [04200, 05004], lr: 0.099201, loss: 2.1612
2022-07-06 12:21:32 - train: epoch 0007, iter [04300, 05004], lr: 0.099196, loss: 2.3360
2022-07-06 12:22:17 - train: epoch 0007, iter [04400, 05004], lr: 0.099191, loss: 2.0022
2022-07-06 12:23:02 - train: epoch 0007, iter [04500, 05004], lr: 0.099187, loss: 2.2873
2022-07-06 12:23:47 - train: epoch 0007, iter [04600, 05004], lr: 0.099182, loss: 2.4057
2022-07-06 12:24:33 - train: epoch 0007, iter [04700, 05004], lr: 0.099177, loss: 2.2912
2022-07-06 12:25:18 - train: epoch 0007, iter [04800, 05004], lr: 0.099172, loss: 2.5174
2022-07-06 12:26:03 - train: epoch 0007, iter [04900, 05004], lr: 0.099168, loss: 2.1918
2022-07-06 12:26:48 - train: epoch 0007, iter [05000, 05004], lr: 0.099163, loss: 2.2390
2022-07-06 12:26:51 - train: epoch 007, train_loss: 2.2031
2022-07-06 12:28:08 - eval: epoch: 007, acc1: 56.288%, acc5: 80.592%, test_loss: 1.8535, per_image_load_time: 2.140ms, per_image_inference_time: 0.838ms
2022-07-06 12:28:10 - until epoch: 007, best_acc1: 56.288%
2022-07-06 12:28:10 - epoch 008 lr: 0.099163
2022-07-06 12:29:01 - train: epoch 0008, iter [00100, 05004], lr: 0.099158, loss: 2.1171
2022-07-06 12:29:46 - train: epoch 0008, iter [00200, 05004], lr: 0.099153, loss: 2.1851
2022-07-06 12:30:31 - train: epoch 0008, iter [00300, 05004], lr: 0.099148, loss: 1.9111
2022-07-06 12:31:17 - train: epoch 0008, iter [00400, 05004], lr: 0.099144, loss: 2.0688
2022-07-06 12:32:02 - train: epoch 0008, iter [00500, 05004], lr: 0.099139, loss: 2.0681
2022-07-06 12:32:47 - train: epoch 0008, iter [00600, 05004], lr: 0.099134, loss: 2.0802
2022-07-06 12:33:32 - train: epoch 0008, iter [00700, 05004], lr: 0.099129, loss: 2.5321
2022-07-06 12:34:18 - train: epoch 0008, iter [00800, 05004], lr: 0.099124, loss: 2.0006
2022-07-06 12:35:03 - train: epoch 0008, iter [00900, 05004], lr: 0.099119, loss: 2.2642
2022-07-06 12:35:48 - train: epoch 0008, iter [01000, 05004], lr: 0.099114, loss: 2.1643
2022-07-06 12:36:33 - train: epoch 0008, iter [01100, 05004], lr: 0.099109, loss: 2.0544
2022-07-06 12:37:19 - train: epoch 0008, iter [01200, 05004], lr: 0.099105, loss: 2.1482
2022-07-06 12:38:04 - train: epoch 0008, iter [01300, 05004], lr: 0.099100, loss: 2.1158
2022-07-06 12:38:49 - train: epoch 0008, iter [01400, 05004], lr: 0.099095, loss: 2.0500
2022-07-06 12:39:35 - train: epoch 0008, iter [01500, 05004], lr: 0.099090, loss: 2.2203
2022-07-06 12:40:20 - train: epoch 0008, iter [01600, 05004], lr: 0.099085, loss: 2.2613
2022-07-06 12:41:05 - train: epoch 0008, iter [01700, 05004], lr: 0.099080, loss: 2.0274
2022-07-06 12:41:50 - train: epoch 0008, iter [01800, 05004], lr: 0.099075, loss: 2.3405
2022-07-06 12:42:35 - train: epoch 0008, iter [01900, 05004], lr: 0.099070, loss: 1.9295
2022-07-06 12:43:21 - train: epoch 0008, iter [02000, 05004], lr: 0.099065, loss: 2.1279
2022-07-06 12:44:06 - train: epoch 0008, iter [02100, 05004], lr: 0.099060, loss: 2.1829
2022-07-06 12:44:51 - train: epoch 0008, iter [02200, 05004], lr: 0.099055, loss: 2.0520
2022-07-06 12:45:36 - train: epoch 0008, iter [02300, 05004], lr: 0.099050, loss: 2.2931
2022-07-06 12:46:21 - train: epoch 0008, iter [02400, 05004], lr: 0.099044, loss: 2.0465
2022-07-06 12:47:07 - train: epoch 0008, iter [02500, 05004], lr: 0.099039, loss: 2.1427
2022-07-06 12:47:52 - train: epoch 0008, iter [02600, 05004], lr: 0.099034, loss: 2.3346
2022-07-06 12:48:37 - train: epoch 0008, iter [02700, 05004], lr: 0.099029, loss: 2.2128
2022-07-06 12:49:23 - train: epoch 0008, iter [02800, 05004], lr: 0.099024, loss: 2.2675
2022-07-06 12:50:08 - train: epoch 0008, iter [02900, 05004], lr: 0.099019, loss: 2.0757
2022-07-06 12:50:53 - train: epoch 0008, iter [03000, 05004], lr: 0.099014, loss: 2.2237
2022-07-06 12:51:38 - train: epoch 0008, iter [03100, 05004], lr: 0.099009, loss: 2.0491
2022-07-06 12:52:23 - train: epoch 0008, iter [03200, 05004], lr: 0.099003, loss: 2.3583
2022-07-06 12:53:09 - train: epoch 0008, iter [03300, 05004], lr: 0.098998, loss: 2.3394
2022-07-06 12:53:54 - train: epoch 0008, iter [03400, 05004], lr: 0.098993, loss: 2.4006
2022-07-06 12:54:39 - train: epoch 0008, iter [03500, 05004], lr: 0.098988, loss: 2.0711
2022-07-06 12:55:24 - train: epoch 0008, iter [03600, 05004], lr: 0.098982, loss: 2.3027
2022-07-06 12:56:09 - train: epoch 0008, iter [03700, 05004], lr: 0.098977, loss: 2.0229
2022-07-06 12:56:55 - train: epoch 0008, iter [03800, 05004], lr: 0.098972, loss: 2.0543
2022-07-06 12:57:40 - train: epoch 0008, iter [03900, 05004], lr: 0.098967, loss: 2.1526
2022-07-06 12:58:25 - train: epoch 0008, iter [04000, 05004], lr: 0.098961, loss: 2.4792
2022-07-06 12:59:10 - train: epoch 0008, iter [04100, 05004], lr: 0.098956, loss: 2.1707
2022-07-06 12:59:56 - train: epoch 0008, iter [04200, 05004], lr: 0.098951, loss: 2.1037
2022-07-06 13:00:41 - train: epoch 0008, iter [04300, 05004], lr: 0.098945, loss: 1.8323
2022-07-06 13:01:26 - train: epoch 0008, iter [04400, 05004], lr: 0.098940, loss: 1.9730
2022-07-06 13:02:11 - train: epoch 0008, iter [04500, 05004], lr: 0.098935, loss: 2.2686
2022-07-06 13:02:57 - train: epoch 0008, iter [04600, 05004], lr: 0.098929, loss: 2.2140
2022-07-06 13:03:42 - train: epoch 0008, iter [04700, 05004], lr: 0.098924, loss: 2.0275
2022-07-06 13:04:27 - train: epoch 0008, iter [04800, 05004], lr: 0.098918, loss: 2.1826
2022-07-06 13:05:12 - train: epoch 0008, iter [04900, 05004], lr: 0.098913, loss: 2.2012
2022-07-06 13:05:57 - train: epoch 0008, iter [05000, 05004], lr: 0.098908, loss: 2.1458
2022-07-06 13:06:00 - train: epoch 008, train_loss: 2.1549
2022-07-06 13:07:16 - eval: epoch: 008, acc1: 56.192%, acc5: 80.722%, test_loss: 1.8838, per_image_load_time: 2.144ms, per_image_inference_time: 0.832ms
2022-07-06 13:07:17 - until epoch: 008, best_acc1: 56.288%
2022-07-06 13:07:17 - epoch 009 lr: 0.098907
2022-07-06 13:08:09 - train: epoch 0009, iter [00100, 05004], lr: 0.098902, loss: 1.9108
2022-07-06 13:08:54 - train: epoch 0009, iter [00200, 05004], lr: 0.098896, loss: 1.9303
2022-07-06 13:09:40 - train: epoch 0009, iter [00300, 05004], lr: 0.098891, loss: 1.8949
2022-07-06 13:10:25 - train: epoch 0009, iter [00400, 05004], lr: 0.098886, loss: 2.2400
2022-07-06 13:11:11 - train: epoch 0009, iter [00500, 05004], lr: 0.098880, loss: 2.1380
2022-07-06 13:11:56 - train: epoch 0009, iter [00600, 05004], lr: 0.098875, loss: 2.0186
2022-07-06 13:12:41 - train: epoch 0009, iter [00700, 05004], lr: 0.098869, loss: 2.0519
2022-07-06 13:13:27 - train: epoch 0009, iter [00800, 05004], lr: 0.098863, loss: 1.9815
2022-07-06 13:14:12 - train: epoch 0009, iter [00900, 05004], lr: 0.098858, loss: 1.9594
2022-07-06 13:14:58 - train: epoch 0009, iter [01000, 05004], lr: 0.098852, loss: 1.8692
2022-07-06 13:15:43 - train: epoch 0009, iter [01100, 05004], lr: 0.098847, loss: 2.3984
2022-07-06 13:16:29 - train: epoch 0009, iter [01200, 05004], lr: 0.098841, loss: 2.2397
2022-07-06 13:17:14 - train: epoch 0009, iter [01300, 05004], lr: 0.098836, loss: 2.3243
2022-07-06 13:17:59 - train: epoch 0009, iter [01400, 05004], lr: 0.098830, loss: 1.9054
2022-07-06 13:18:45 - train: epoch 0009, iter [01500, 05004], lr: 0.098824, loss: 2.0257
2022-07-06 13:19:31 - train: epoch 0009, iter [01600, 05004], lr: 0.098819, loss: 2.1219
2022-07-06 13:20:16 - train: epoch 0009, iter [01700, 05004], lr: 0.098813, loss: 2.2692
2022-07-06 13:21:02 - train: epoch 0009, iter [01800, 05004], lr: 0.098807, loss: 2.0884
2022-07-06 13:21:47 - train: epoch 0009, iter [01900, 05004], lr: 0.098802, loss: 1.8385
2022-07-06 13:22:33 - train: epoch 0009, iter [02000, 05004], lr: 0.098796, loss: 1.7666
2022-07-06 13:23:18 - train: epoch 0009, iter [02100, 05004], lr: 0.098790, loss: 2.0809
2022-07-06 13:24:04 - train: epoch 0009, iter [02200, 05004], lr: 0.098784, loss: 2.2024
2022-07-06 13:24:49 - train: epoch 0009, iter [02300, 05004], lr: 0.098779, loss: 1.9494
2022-07-06 13:25:35 - train: epoch 0009, iter [02400, 05004], lr: 0.098773, loss: 1.9803
2022-07-06 13:26:20 - train: epoch 0009, iter [02500, 05004], lr: 0.098767, loss: 1.9879
2022-07-06 13:27:06 - train: epoch 0009, iter [02600, 05004], lr: 0.098761, loss: 2.0690
2022-07-06 13:27:51 - train: epoch 0009, iter [02700, 05004], lr: 0.098756, loss: 2.0083
2022-07-06 13:28:37 - train: epoch 0009, iter [02800, 05004], lr: 0.098750, loss: 2.1301
2022-07-06 13:29:22 - train: epoch 0009, iter [02900, 05004], lr: 0.098744, loss: 1.8590
2022-07-06 13:30:08 - train: epoch 0009, iter [03000, 05004], lr: 0.098738, loss: 1.9618
2022-07-06 13:30:53 - train: epoch 0009, iter [03100, 05004], lr: 0.098732, loss: 2.1855
2022-07-06 13:31:39 - train: epoch 0009, iter [03200, 05004], lr: 0.098726, loss: 2.1271
2022-07-06 13:32:24 - train: epoch 0009, iter [03300, 05004], lr: 0.098721, loss: 2.0769
2022-07-06 13:33:09 - train: epoch 0009, iter [03400, 05004], lr: 0.098715, loss: 2.3378
2022-07-06 13:33:55 - train: epoch 0009, iter [03500, 05004], lr: 0.098709, loss: 2.2789
2022-07-06 13:34:40 - train: epoch 0009, iter [03600, 05004], lr: 0.098703, loss: 2.0772
2022-07-06 13:35:26 - train: epoch 0009, iter [03700, 05004], lr: 0.098697, loss: 2.2612
2022-07-06 13:36:11 - train: epoch 0009, iter [03800, 05004], lr: 0.098691, loss: 2.2206
2022-07-06 13:36:56 - train: epoch 0009, iter [03900, 05004], lr: 0.098685, loss: 1.7554
2022-07-06 13:37:42 - train: epoch 0009, iter [04000, 05004], lr: 0.098679, loss: 2.2711
2022-07-06 13:38:27 - train: epoch 0009, iter [04100, 05004], lr: 0.098673, loss: 2.0574
2022-07-06 13:39:13 - train: epoch 0009, iter [04200, 05004], lr: 0.098667, loss: 2.0991
2022-07-06 13:39:58 - train: epoch 0009, iter [04300, 05004], lr: 0.098661, loss: 2.1882
2022-07-06 13:40:44 - train: epoch 0009, iter [04400, 05004], lr: 0.098655, loss: 2.0306
2022-07-06 13:41:29 - train: epoch 0009, iter [04500, 05004], lr: 0.098649, loss: 2.0262
2022-07-06 13:42:15 - train: epoch 0009, iter [04600, 05004], lr: 0.098643, loss: 2.2798
2022-07-06 13:43:00 - train: epoch 0009, iter [04700, 05004], lr: 0.098637, loss: 2.2973
2022-07-06 13:43:46 - train: epoch 0009, iter [04800, 05004], lr: 0.098631, loss: 2.3185
2022-07-06 13:44:31 - train: epoch 0009, iter [04900, 05004], lr: 0.098625, loss: 2.1256
2022-07-06 13:45:16 - train: epoch 0009, iter [05000, 05004], lr: 0.098619, loss: 1.9783
2022-07-06 13:45:19 - train: epoch 009, train_loss: 2.1160
2022-07-06 13:46:36 - eval: epoch: 009, acc1: 56.604%, acc5: 80.944%, test_loss: 1.8486, per_image_load_time: 1.373ms, per_image_inference_time: 0.829ms
2022-07-06 13:46:38 - until epoch: 009, best_acc1: 56.604%
2022-07-06 13:46:38 - epoch 010 lr: 0.098618
2022-07-06 13:47:29 - train: epoch 0010, iter [00100, 05004], lr: 0.098612, loss: 2.0523
2022-07-06 13:48:14 - train: epoch 0010, iter [00200, 05004], lr: 0.098606, loss: 2.1188
2022-07-06 13:48:59 - train: epoch 0010, iter [00300, 05004], lr: 0.098600, loss: 2.0505
2022-07-06 13:49:44 - train: epoch 0010, iter [00400, 05004], lr: 0.098594, loss: 2.2046
2022-07-06 13:50:29 - train: epoch 0010, iter [00500, 05004], lr: 0.098588, loss: 1.9825
2022-07-06 13:51:14 - train: epoch 0010, iter [00600, 05004], lr: 0.098582, loss: 2.1269
2022-07-06 13:52:00 - train: epoch 0010, iter [00700, 05004], lr: 0.098575, loss: 2.1442
2022-07-06 13:52:45 - train: epoch 0010, iter [00800, 05004], lr: 0.098569, loss: 1.9408
2022-07-06 13:53:30 - train: epoch 0010, iter [00900, 05004], lr: 0.098563, loss: 1.9398
2022-07-06 13:54:15 - train: epoch 0010, iter [01000, 05004], lr: 0.098557, loss: 1.9246
2022-07-06 13:55:01 - train: epoch 0010, iter [01100, 05004], lr: 0.098551, loss: 1.9989
2022-07-06 13:55:46 - train: epoch 0010, iter [01200, 05004], lr: 0.098544, loss: 1.8809
2022-07-06 13:56:31 - train: epoch 0010, iter [01300, 05004], lr: 0.098538, loss: 2.0466
2022-07-06 13:57:16 - train: epoch 0010, iter [01400, 05004], lr: 0.098532, loss: 2.0639
2022-07-06 13:58:02 - train: epoch 0010, iter [01500, 05004], lr: 0.098525, loss: 1.8336
2022-07-06 13:58:47 - train: epoch 0010, iter [01600, 05004], lr: 0.098519, loss: 2.1711
2022-07-06 13:59:32 - train: epoch 0010, iter [01700, 05004], lr: 0.098513, loss: 2.2801
2022-07-06 14:00:18 - train: epoch 0010, iter [01800, 05004], lr: 0.098506, loss: 2.1302
2022-07-06 14:01:03 - train: epoch 0010, iter [01900, 05004], lr: 0.098500, loss: 2.1957
2022-07-06 14:01:48 - train: epoch 0010, iter [02000, 05004], lr: 0.098494, loss: 2.1781
2022-07-06 14:02:33 - train: epoch 0010, iter [02100, 05004], lr: 0.098487, loss: 1.8659
2022-07-06 14:03:19 - train: epoch 0010, iter [02200, 05004], lr: 0.098481, loss: 2.1860
2022-07-06 14:04:04 - train: epoch 0010, iter [02300, 05004], lr: 0.098475, loss: 2.2411
2022-07-06 14:04:49 - train: epoch 0010, iter [02400, 05004], lr: 0.098468, loss: 2.2505
2022-07-06 14:05:34 - train: epoch 0010, iter [02500, 05004], lr: 0.098462, loss: 2.0646
2022-07-06 14:06:20 - train: epoch 0010, iter [02600, 05004], lr: 0.098455, loss: 2.2004
2022-07-06 14:07:05 - train: epoch 0010, iter [02700, 05004], lr: 0.098449, loss: 1.8683
2022-07-06 14:07:50 - train: epoch 0010, iter [02800, 05004], lr: 0.098442, loss: 2.0694
2022-07-06 14:08:35 - train: epoch 0010, iter [02900, 05004], lr: 0.098436, loss: 2.0947
2022-07-06 14:09:20 - train: epoch 0010, iter [03000, 05004], lr: 0.098429, loss: 2.0557
2022-07-06 14:10:05 - train: epoch 0010, iter [03100, 05004], lr: 0.098423, loss: 2.3470
2022-07-06 14:10:51 - train: epoch 0010, iter [03200, 05004], lr: 0.098416, loss: 2.0079
2022-07-06 14:11:36 - train: epoch 0010, iter [03300, 05004], lr: 0.098410, loss: 2.2351
2022-07-06 14:12:21 - train: epoch 0010, iter [03400, 05004], lr: 0.098403, loss: 2.1610
2022-07-06 14:13:07 - train: epoch 0010, iter [03500, 05004], lr: 0.098397, loss: 2.3152
2022-07-06 14:13:52 - train: epoch 0010, iter [03600, 05004], lr: 0.098390, loss: 2.2709
2022-07-06 14:14:37 - train: epoch 0010, iter [03700, 05004], lr: 0.098383, loss: 2.0423
2022-07-06 14:15:22 - train: epoch 0010, iter [03800, 05004], lr: 0.098377, loss: 2.1696
2022-07-06 14:16:08 - train: epoch 0010, iter [03900, 05004], lr: 0.098370, loss: 1.7395
2022-07-06 14:16:53 - train: epoch 0010, iter [04000, 05004], lr: 0.098364, loss: 1.9534
2022-07-06 14:17:38 - train: epoch 0010, iter [04100, 05004], lr: 0.098357, loss: 1.8775
2022-07-06 14:18:23 - train: epoch 0010, iter [04200, 05004], lr: 0.098350, loss: 2.1524
2022-07-06 14:19:09 - train: epoch 0010, iter [04300, 05004], lr: 0.098344, loss: 2.0884
2022-07-06 14:19:54 - train: epoch 0010, iter [04400, 05004], lr: 0.098337, loss: 2.0218
2022-07-06 14:20:39 - train: epoch 0010, iter [04500, 05004], lr: 0.098330, loss: 1.9769
2022-07-06 14:21:24 - train: epoch 0010, iter [04600, 05004], lr: 0.098324, loss: 2.0677
2022-07-06 14:22:09 - train: epoch 0010, iter [04700, 05004], lr: 0.098317, loss: 2.1357
2022-07-06 14:22:55 - train: epoch 0010, iter [04800, 05004], lr: 0.098310, loss: 2.0441
2022-07-06 14:23:40 - train: epoch 0010, iter [04900, 05004], lr: 0.098303, loss: 1.9752
2022-07-06 14:24:25 - train: epoch 0010, iter [05000, 05004], lr: 0.098297, loss: 1.8347
2022-07-06 14:24:27 - train: epoch 010, train_loss: 2.0827
2022-07-06 14:25:46 - eval: epoch: 010, acc1: 57.742%, acc5: 81.726%, test_loss: 1.7914, per_image_load_time: 2.218ms, per_image_inference_time: 0.848ms
2022-07-06 14:25:47 - until epoch: 010, best_acc1: 57.742%
2022-07-06 14:25:47 - epoch 011 lr: 0.098296
2022-07-06 14:26:38 - train: epoch 0011, iter [00100, 05004], lr: 0.098290, loss: 1.8723
2022-07-06 14:27:23 - train: epoch 0011, iter [00200, 05004], lr: 0.098283, loss: 2.1610
2022-07-06 14:28:08 - train: epoch 0011, iter [00300, 05004], lr: 0.098276, loss: 1.9273
2022-07-06 14:28:52 - train: epoch 0011, iter [00400, 05004], lr: 0.098269, loss: 2.1586
2022-07-06 14:29:37 - train: epoch 0011, iter [00500, 05004], lr: 0.098262, loss: 2.0764
2022-07-06 14:30:22 - train: epoch 0011, iter [00600, 05004], lr: 0.098255, loss: 2.0598
2022-07-06 14:31:07 - train: epoch 0011, iter [00700, 05004], lr: 0.098249, loss: 2.0441
2022-07-06 14:31:52 - train: epoch 0011, iter [00800, 05004], lr: 0.098242, loss: 2.0329
2022-07-06 14:32:37 - train: epoch 0011, iter [00900, 05004], lr: 0.098235, loss: 2.1992
2022-07-06 14:33:22 - train: epoch 0011, iter [01000, 05004], lr: 0.098228, loss: 1.9940
2022-07-06 14:34:07 - train: epoch 0011, iter [01100, 05004], lr: 0.098221, loss: 2.1764
2022-07-06 14:34:52 - train: epoch 0011, iter [01200, 05004], lr: 0.098214, loss: 2.4036
2022-07-06 14:35:37 - train: epoch 0011, iter [01300, 05004], lr: 0.098207, loss: 2.2845
2022-07-06 14:36:22 - train: epoch 0011, iter [01400, 05004], lr: 0.098200, loss: 2.0607
2022-07-06 14:37:06 - train: epoch 0011, iter [01500, 05004], lr: 0.098193, loss: 1.9549
2022-07-06 14:37:51 - train: epoch 0011, iter [01600, 05004], lr: 0.098186, loss: 2.0640
2022-07-06 14:38:36 - train: epoch 0011, iter [01700, 05004], lr: 0.098179, loss: 2.0901
2022-07-06 14:39:21 - train: epoch 0011, iter [01800, 05004], lr: 0.098172, loss: 1.8860
2022-07-06 14:40:06 - train: epoch 0011, iter [01900, 05004], lr: 0.098165, loss: 1.7909
2022-07-06 14:40:51 - train: epoch 0011, iter [02000, 05004], lr: 0.098158, loss: 2.2172
2022-07-06 14:41:36 - train: epoch 0011, iter [02100, 05004], lr: 0.098151, loss: 2.0178
2022-07-06 14:42:21 - train: epoch 0011, iter [02200, 05004], lr: 0.098144, loss: 1.8860
2022-07-06 14:43:06 - train: epoch 0011, iter [02300, 05004], lr: 0.098137, loss: 2.3215
2022-07-06 14:43:51 - train: epoch 0011, iter [02400, 05004], lr: 0.098130, loss: 1.8724
2022-07-06 14:44:36 - train: epoch 0011, iter [02500, 05004], lr: 0.098123, loss: 2.3322
2022-07-06 14:45:21 - train: epoch 0011, iter [02600, 05004], lr: 0.098116, loss: 2.0533
2022-07-06 14:46:07 - train: epoch 0011, iter [02700, 05004], lr: 0.098109, loss: 2.1330
2022-07-06 14:46:51 - train: epoch 0011, iter [02800, 05004], lr: 0.098102, loss: 1.7648
2022-07-06 14:47:36 - train: epoch 0011, iter [02900, 05004], lr: 0.098094, loss: 2.1520
2022-07-06 14:48:21 - train: epoch 0011, iter [03000, 05004], lr: 0.098087, loss: 2.3069
2022-07-06 14:49:06 - train: epoch 0011, iter [03100, 05004], lr: 0.098080, loss: 2.0998
2022-07-06 14:49:51 - train: epoch 0011, iter [03200, 05004], lr: 0.098073, loss: 1.9284
2022-07-06 14:50:36 - train: epoch 0011, iter [03300, 05004], lr: 0.098066, loss: 2.0613
2022-07-06 14:51:21 - train: epoch 0011, iter [03400, 05004], lr: 0.098058, loss: 2.0546
2022-07-06 14:52:06 - train: epoch 0011, iter [03500, 05004], lr: 0.098051, loss: 2.0353
2022-07-06 14:52:51 - train: epoch 0011, iter [03600, 05004], lr: 0.098044, loss: 2.0979
2022-07-06 14:53:36 - train: epoch 0011, iter [03700, 05004], lr: 0.098037, loss: 2.1676
2022-07-06 14:54:22 - train: epoch 0011, iter [03800, 05004], lr: 0.098029, loss: 1.8691
2022-07-06 14:55:07 - train: epoch 0011, iter [03900, 05004], lr: 0.098022, loss: 2.1092
2022-07-06 14:55:52 - train: epoch 0011, iter [04000, 05004], lr: 0.098015, loss: 2.0969
2022-07-06 14:56:37 - train: epoch 0011, iter [04100, 05004], lr: 0.098008, loss: 1.8151
2022-07-06 14:57:22 - train: epoch 0011, iter [04200, 05004], lr: 0.098000, loss: 1.9671
2022-07-06 14:58:07 - train: epoch 0011, iter [04300, 05004], lr: 0.097993, loss: 2.1653
2022-07-06 14:58:52 - train: epoch 0011, iter [04400, 05004], lr: 0.097986, loss: 2.0130
2022-07-06 14:59:37 - train: epoch 0011, iter [04500, 05004], lr: 0.097978, loss: 1.8464
2022-07-06 15:00:22 - train: epoch 0011, iter [04600, 05004], lr: 0.097971, loss: 2.0019
2022-07-06 15:01:08 - train: epoch 0011, iter [04700, 05004], lr: 0.097964, loss: 1.7924
2022-07-06 15:01:53 - train: epoch 0011, iter [04800, 05004], lr: 0.097956, loss: 1.8949
2022-07-06 15:02:38 - train: epoch 0011, iter [04900, 05004], lr: 0.097949, loss: 1.8321
2022-07-06 15:03:23 - train: epoch 0011, iter [05000, 05004], lr: 0.097941, loss: 2.0782
2022-07-06 15:03:25 - train: epoch 011, train_loss: 2.0552
2022-07-06 15:04:43 - eval: epoch: 011, acc1: 57.356%, acc5: 81.628%, test_loss: 1.7893, per_image_load_time: 1.422ms, per_image_inference_time: 0.842ms
2022-07-06 15:04:44 - until epoch: 011, best_acc1: 57.742%
2022-07-06 15:04:44 - epoch 012 lr: 0.097941
2022-07-06 15:05:35 - train: epoch 0012, iter [00100, 05004], lr: 0.097934, loss: 1.9696
2022-07-06 15:06:20 - train: epoch 0012, iter [00200, 05004], lr: 0.097926, loss: 2.0344
2022-07-06 15:07:05 - train: epoch 0012, iter [00300, 05004], lr: 0.097919, loss: 2.0329
2022-07-06 15:07:51 - train: epoch 0012, iter [00400, 05004], lr: 0.097911, loss: 2.0343
2022-07-06 15:08:36 - train: epoch 0012, iter [00500, 05004], lr: 0.097904, loss: 2.2390
2022-07-06 15:09:21 - train: epoch 0012, iter [00600, 05004], lr: 0.097896, loss: 1.7607
2022-07-06 15:10:06 - train: epoch 0012, iter [00700, 05004], lr: 0.097889, loss: 1.9826
2022-07-06 15:10:51 - train: epoch 0012, iter [00800, 05004], lr: 0.097881, loss: 2.1313
2022-07-06 15:11:37 - train: epoch 0012, iter [00900, 05004], lr: 0.097874, loss: 2.1741
2022-07-06 15:12:22 - train: epoch 0012, iter [01000, 05004], lr: 0.097866, loss: 1.8479
2022-07-06 15:13:07 - train: epoch 0012, iter [01100, 05004], lr: 0.097858, loss: 2.3898
2022-07-06 15:13:52 - train: epoch 0012, iter [01200, 05004], lr: 0.097851, loss: 1.8037
2022-07-06 15:14:37 - train: epoch 0012, iter [01300, 05004], lr: 0.097843, loss: 2.0454
2022-07-06 15:15:22 - train: epoch 0012, iter [01400, 05004], lr: 0.097836, loss: 2.2038
2022-07-06 15:16:08 - train: epoch 0012, iter [01500, 05004], lr: 0.097828, loss: 1.8545
2022-07-06 15:16:53 - train: epoch 0012, iter [01600, 05004], lr: 0.097820, loss: 1.9316
2022-07-06 15:17:38 - train: epoch 0012, iter [01700, 05004], lr: 0.097813, loss: 1.8256
2022-07-06 15:18:23 - train: epoch 0012, iter [01800, 05004], lr: 0.097805, loss: 2.0638
2022-07-06 15:19:08 - train: epoch 0012, iter [01900, 05004], lr: 0.097797, loss: 2.0577
2022-07-06 15:19:53 - train: epoch 0012, iter [02000, 05004], lr: 0.097790, loss: 2.1948
2022-07-06 15:20:38 - train: epoch 0012, iter [02100, 05004], lr: 0.097782, loss: 1.9824
2022-07-06 15:21:23 - train: epoch 0012, iter [02200, 05004], lr: 0.097774, loss: 2.1494
2022-07-06 15:22:08 - train: epoch 0012, iter [02300, 05004], lr: 0.097767, loss: 2.0576
2022-07-06 15:22:54 - train: epoch 0012, iter [02400, 05004], lr: 0.097759, loss: 2.0405
2022-07-06 15:23:39 - train: epoch 0012, iter [02500, 05004], lr: 0.097751, loss: 1.8507
2022-07-06 15:24:24 - train: epoch 0012, iter [02600, 05004], lr: 0.097743, loss: 1.9099
2022-07-06 15:25:09 - train: epoch 0012, iter [02700, 05004], lr: 0.097736, loss: 1.8949
2022-07-06 15:25:54 - train: epoch 0012, iter [02800, 05004], lr: 0.097728, loss: 2.0101
2022-07-06 15:26:40 - train: epoch 0012, iter [02900, 05004], lr: 0.097720, loss: 1.9049
2022-07-06 15:27:25 - train: epoch 0012, iter [03000, 05004], lr: 0.097712, loss: 1.9341
2022-07-06 15:28:10 - train: epoch 0012, iter [03100, 05004], lr: 0.097704, loss: 2.2046
2022-07-06 15:28:55 - train: epoch 0012, iter [03200, 05004], lr: 0.097697, loss: 1.7985
2022-07-06 15:29:40 - train: epoch 0012, iter [03300, 05004], lr: 0.097689, loss: 2.0498
2022-07-06 15:30:26 - train: epoch 0012, iter [03400, 05004], lr: 0.097681, loss: 2.0144
2022-07-06 15:31:11 - train: epoch 0012, iter [03500, 05004], lr: 0.097673, loss: 2.1442
2022-07-06 15:31:56 - train: epoch 0012, iter [03600, 05004], lr: 0.097665, loss: 2.0778
2022-07-06 15:32:41 - train: epoch 0012, iter [03700, 05004], lr: 0.097657, loss: 1.9770
2022-07-06 15:33:26 - train: epoch 0012, iter [03800, 05004], lr: 0.097649, loss: 2.0287
2022-07-06 15:34:11 - train: epoch 0012, iter [03900, 05004], lr: 0.097641, loss: 1.9545
2022-07-06 15:34:56 - train: epoch 0012, iter [04000, 05004], lr: 0.097633, loss: 1.9506
2022-07-06 15:35:41 - train: epoch 0012, iter [04100, 05004], lr: 0.097625, loss: 1.9162
2022-07-06 15:36:27 - train: epoch 0012, iter [04200, 05004], lr: 0.097617, loss: 1.9580
2022-07-06 15:37:12 - train: epoch 0012, iter [04300, 05004], lr: 0.097609, loss: 2.1639
2022-07-06 15:37:57 - train: epoch 0012, iter [04400, 05004], lr: 0.097601, loss: 1.9074
2022-07-06 15:38:42 - train: epoch 0012, iter [04500, 05004], lr: 0.097593, loss: 1.9212
2022-07-06 15:39:28 - train: epoch 0012, iter [04600, 05004], lr: 0.097585, loss: 2.2371
2022-07-06 15:40:13 - train: epoch 0012, iter [04700, 05004], lr: 0.097577, loss: 2.0240
2022-07-06 15:40:58 - train: epoch 0012, iter [04800, 05004], lr: 0.097569, loss: 2.1385
2022-07-06 15:41:43 - train: epoch 0012, iter [04900, 05004], lr: 0.097561, loss: 2.0857
2022-07-06 15:42:29 - train: epoch 0012, iter [05000, 05004], lr: 0.097553, loss: 1.7646
2022-07-06 15:42:31 - train: epoch 012, train_loss: 2.0314
2022-07-06 15:43:49 - eval: epoch: 012, acc1: 59.366%, acc5: 83.208%, test_loss: 1.6939, per_image_load_time: 2.179ms, per_image_inference_time: 0.838ms
2022-07-06 15:43:50 - until epoch: 012, best_acc1: 59.366%
2022-07-06 15:43:50 - epoch 013 lr: 0.097553
2022-07-06 15:44:42 - train: epoch 0013, iter [00100, 05004], lr: 0.097545, loss: 1.8025
2022-07-06 15:45:27 - train: epoch 0013, iter [00200, 05004], lr: 0.097537, loss: 1.8733
2022-07-06 15:46:12 - train: epoch 0013, iter [00300, 05004], lr: 0.097529, loss: 2.0212
2022-07-06 15:46:57 - train: epoch 0013, iter [00400, 05004], lr: 0.097520, loss: 1.9187
2022-07-06 15:47:42 - train: epoch 0013, iter [00500, 05004], lr: 0.097512, loss: 2.0259
2022-07-06 15:48:27 - train: epoch 0013, iter [00600, 05004], lr: 0.097504, loss: 2.0820
2022-07-06 15:49:12 - train: epoch 0013, iter [00700, 05004], lr: 0.097496, loss: 1.9198
2022-07-06 15:49:57 - train: epoch 0013, iter [00800, 05004], lr: 0.097488, loss: 2.1136
2022-07-06 15:50:42 - train: epoch 0013, iter [00900, 05004], lr: 0.097480, loss: 1.9969
2022-07-06 15:51:27 - train: epoch 0013, iter [01000, 05004], lr: 0.097471, loss: 2.0825
2022-07-06 15:52:12 - train: epoch 0013, iter [01100, 05004], lr: 0.097463, loss: 2.1209
2022-07-06 15:52:57 - train: epoch 0013, iter [01200, 05004], lr: 0.097455, loss: 2.1838
2022-07-06 15:53:42 - train: epoch 0013, iter [01300, 05004], lr: 0.097447, loss: 2.0022
2022-07-06 15:54:27 - train: epoch 0013, iter [01400, 05004], lr: 0.097438, loss: 1.8640
2022-07-06 15:55:12 - train: epoch 0013, iter [01500, 05004], lr: 0.097430, loss: 2.1215
2022-07-06 15:55:57 - train: epoch 0013, iter [01600, 05004], lr: 0.097422, loss: 1.8577
2022-07-06 15:56:42 - train: epoch 0013, iter [01700, 05004], lr: 0.097414, loss: 1.9132
2022-07-06 15:57:27 - train: epoch 0013, iter [01800, 05004], lr: 0.097405, loss: 2.0620
2022-07-06 15:58:13 - train: epoch 0013, iter [01900, 05004], lr: 0.097397, loss: 2.0777
2022-07-06 15:58:58 - train: epoch 0013, iter [02000, 05004], lr: 0.097389, loss: 2.2172
2022-07-06 15:59:43 - train: epoch 0013, iter [02100, 05004], lr: 0.097380, loss: 2.1918
2022-07-06 16:00:28 - train: epoch 0013, iter [02200, 05004], lr: 0.097372, loss: 1.9661
2022-07-06 16:01:13 - train: epoch 0013, iter [02300, 05004], lr: 0.097363, loss: 1.9671
2022-07-06 16:01:58 - train: epoch 0013, iter [02400, 05004], lr: 0.097355, loss: 2.0089
2022-07-06 16:02:43 - train: epoch 0013, iter [02500, 05004], lr: 0.097347, loss: 1.9331
2022-07-06 16:03:28 - train: epoch 0013, iter [02600, 05004], lr: 0.097338, loss: 1.9408
2022-07-06 16:04:14 - train: epoch 0013, iter [02700, 05004], lr: 0.097330, loss: 1.8512
2022-07-06 16:04:59 - train: epoch 0013, iter [02800, 05004], lr: 0.097321, loss: 2.0624
2022-07-06 16:05:44 - train: epoch 0013, iter [02900, 05004], lr: 0.097313, loss: 2.1748
2022-07-06 16:06:29 - train: epoch 0013, iter [03000, 05004], lr: 0.097304, loss: 1.8390
2022-07-06 16:07:14 - train: epoch 0013, iter [03100, 05004], lr: 0.097296, loss: 1.8968
2022-07-06 16:07:59 - train: epoch 0013, iter [03200, 05004], lr: 0.097287, loss: 2.0371
2022-07-06 16:08:45 - train: epoch 0013, iter [03300, 05004], lr: 0.097279, loss: 1.8954
2022-07-06 16:09:30 - train: epoch 0013, iter [03400, 05004], lr: 0.097270, loss: 1.9938
2022-07-06 16:10:15 - train: epoch 0013, iter [03500, 05004], lr: 0.097262, loss: 1.9025
2022-07-06 16:11:00 - train: epoch 0013, iter [03600, 05004], lr: 0.097253, loss: 2.3564
2022-07-06 16:11:46 - train: epoch 0013, iter [03700, 05004], lr: 0.097245, loss: 1.7934
2022-07-06 16:12:31 - train: epoch 0013, iter [03800, 05004], lr: 0.097236, loss: 2.1960
2022-07-06 16:13:16 - train: epoch 0013, iter [03900, 05004], lr: 0.097228, loss: 2.0523
2022-07-06 16:14:01 - train: epoch 0013, iter [04000, 05004], lr: 0.097219, loss: 2.0021
2022-07-06 16:14:46 - train: epoch 0013, iter [04100, 05004], lr: 0.097210, loss: 1.9035
2022-07-06 16:15:32 - train: epoch 0013, iter [04200, 05004], lr: 0.097202, loss: 1.8905
2022-07-06 16:16:17 - train: epoch 0013, iter [04300, 05004], lr: 0.097193, loss: 1.8847
2022-07-06 16:17:02 - train: epoch 0013, iter [04400, 05004], lr: 0.097185, loss: 1.9469
2022-07-06 16:17:47 - train: epoch 0013, iter [04500, 05004], lr: 0.097176, loss: 1.8866
2022-07-06 16:18:32 - train: epoch 0013, iter [04600, 05004], lr: 0.097167, loss: 2.0405
2022-07-06 16:19:17 - train: epoch 0013, iter [04700, 05004], lr: 0.097159, loss: 2.1442
2022-07-06 16:20:02 - train: epoch 0013, iter [04800, 05004], lr: 0.097150, loss: 2.0762
2022-07-06 16:20:47 - train: epoch 0013, iter [04900, 05004], lr: 0.097141, loss: 2.0672
2022-07-06 16:21:32 - train: epoch 0013, iter [05000, 05004], lr: 0.097132, loss: 2.1984
2022-07-06 16:21:35 - train: epoch 013, train_loss: 2.0108
2022-07-06 16:22:51 - eval: epoch: 013, acc1: 59.238%, acc5: 83.306%, test_loss: 1.7078, per_image_load_time: 1.158ms, per_image_inference_time: 0.826ms
2022-07-06 16:22:52 - until epoch: 013, best_acc1: 59.366%
2022-07-06 16:22:52 - epoch 014 lr: 0.097132
2022-07-06 16:23:44 - train: epoch 0014, iter [00100, 05004], lr: 0.097123, loss: 1.9794
2022-07-06 16:24:30 - train: epoch 0014, iter [00200, 05004], lr: 0.097115, loss: 2.0743
2022-07-06 16:25:15 - train: epoch 0014, iter [00300, 05004], lr: 0.097106, loss: 1.7947
2022-07-06 16:26:01 - train: epoch 0014, iter [00400, 05004], lr: 0.097097, loss: 1.9174
2022-07-06 16:26:46 - train: epoch 0014, iter [00500, 05004], lr: 0.097088, loss: 1.8314
2022-07-06 16:27:31 - train: epoch 0014, iter [00600, 05004], lr: 0.097079, loss: 1.9354
2022-07-06 16:28:17 - train: epoch 0014, iter [00700, 05004], lr: 0.097071, loss: 1.9390
2022-07-06 16:29:02 - train: epoch 0014, iter [00800, 05004], lr: 0.097062, loss: 1.9580
2022-07-06 16:29:47 - train: epoch 0014, iter [00900, 05004], lr: 0.097053, loss: 2.0238
2022-07-06 16:30:33 - train: epoch 0014, iter [01000, 05004], lr: 0.097044, loss: 2.1779
2022-07-06 16:31:18 - train: epoch 0014, iter [01100, 05004], lr: 0.097035, loss: 2.0054
2022-07-06 16:32:03 - train: epoch 0014, iter [01200, 05004], lr: 0.097026, loss: 2.1389
2022-07-06 16:32:48 - train: epoch 0014, iter [01300, 05004], lr: 0.097017, loss: 1.9097
2022-07-06 16:33:34 - train: epoch 0014, iter [01400, 05004], lr: 0.097009, loss: 2.1216
2022-07-06 16:34:20 - train: epoch 0014, iter [01500, 05004], lr: 0.097000, loss: 2.1505
2022-07-06 16:35:06 - train: epoch 0014, iter [01600, 05004], lr: 0.096991, loss: 1.9368
2022-07-06 16:35:51 - train: epoch 0014, iter [01700, 05004], lr: 0.096982, loss: 2.2489
2022-07-06 16:36:37 - train: epoch 0014, iter [01800, 05004], lr: 0.096973, loss: 2.1919
2022-07-06 16:37:23 - train: epoch 0014, iter [01900, 05004], lr: 0.096964, loss: 1.9396
2022-07-06 16:38:09 - train: epoch 0014, iter [02000, 05004], lr: 0.096955, loss: 1.8690
2022-07-06 16:38:54 - train: epoch 0014, iter [02100, 05004], lr: 0.096946, loss: 2.1421
2022-07-06 16:39:40 - train: epoch 0014, iter [02200, 05004], lr: 0.096937, loss: 2.0023
2022-07-06 16:40:26 - train: epoch 0014, iter [02300, 05004], lr: 0.096928, loss: 1.9664
2022-07-06 16:41:11 - train: epoch 0014, iter [02400, 05004], lr: 0.096919, loss: 2.1331
2022-07-06 16:41:57 - train: epoch 0014, iter [02500, 05004], lr: 0.096910, loss: 1.9460
2022-07-06 16:42:43 - train: epoch 0014, iter [02600, 05004], lr: 0.096901, loss: 1.9526
2022-07-06 16:43:29 - train: epoch 0014, iter [02700, 05004], lr: 0.096892, loss: 1.9213
2022-07-06 16:44:14 - train: epoch 0014, iter [02800, 05004], lr: 0.096883, loss: 2.3338
2022-07-06 16:44:59 - train: epoch 0014, iter [02900, 05004], lr: 0.096873, loss: 1.9080
2022-07-06 16:45:45 - train: epoch 0014, iter [03000, 05004], lr: 0.096864, loss: 2.0348
2022-07-06 16:46:30 - train: epoch 0014, iter [03100, 05004], lr: 0.096855, loss: 1.7963
2022-07-06 16:47:16 - train: epoch 0014, iter [03200, 05004], lr: 0.096846, loss: 2.0148
2022-07-06 16:48:02 - train: epoch 0014, iter [03300, 05004], lr: 0.096837, loss: 1.8345
2022-07-06 16:48:47 - train: epoch 0014, iter [03400, 05004], lr: 0.096828, loss: 1.8886
2022-07-06 16:49:32 - train: epoch 0014, iter [03500, 05004], lr: 0.096819, loss: 1.9988
2022-07-06 16:50:17 - train: epoch 0014, iter [03600, 05004], lr: 0.096809, loss: 1.8932
2022-07-06 16:51:03 - train: epoch 0014, iter [03700, 05004], lr: 0.096800, loss: 1.9381
2022-07-06 16:51:48 - train: epoch 0014, iter [03800, 05004], lr: 0.096791, loss: 2.2799
2022-07-06 16:52:33 - train: epoch 0014, iter [03900, 05004], lr: 0.096782, loss: 1.9419
2022-07-06 16:53:18 - train: epoch 0014, iter [04000, 05004], lr: 0.096772, loss: 2.0852
2022-07-06 16:54:04 - train: epoch 0014, iter [04100, 05004], lr: 0.096763, loss: 1.9527
2022-07-06 16:54:49 - train: epoch 0014, iter [04200, 05004], lr: 0.096754, loss: 1.9796
2022-07-06 16:55:34 - train: epoch 0014, iter [04300, 05004], lr: 0.096745, loss: 1.9001
2022-07-06 16:56:19 - train: epoch 0014, iter [04400, 05004], lr: 0.096735, loss: 1.7660
2022-07-06 16:57:04 - train: epoch 0014, iter [04500, 05004], lr: 0.096726, loss: 1.9630
2022-07-06 16:57:49 - train: epoch 0014, iter [04600, 05004], lr: 0.096717, loss: 1.8990
2022-07-06 16:58:35 - train: epoch 0014, iter [04700, 05004], lr: 0.096707, loss: 1.9102
2022-07-06 16:59:20 - train: epoch 0014, iter [04800, 05004], lr: 0.096698, loss: 1.9668
2022-07-06 17:00:05 - train: epoch 0014, iter [04900, 05004], lr: 0.096689, loss: 1.7178
2022-07-06 17:00:50 - train: epoch 0014, iter [05000, 05004], lr: 0.096679, loss: 1.9151
2022-07-06 17:00:52 - train: epoch 014, train_loss: 1.9903
2022-07-06 17:02:10 - eval: epoch: 014, acc1: 57.630%, acc5: 81.996%, test_loss: 1.7970, per_image_load_time: 1.877ms, per_image_inference_time: 0.832ms
2022-07-06 17:02:10 - until epoch: 014, best_acc1: 59.366%
2022-07-06 17:02:10 - epoch 015 lr: 0.096679
2022-07-06 17:03:02 - train: epoch 0015, iter [00100, 05004], lr: 0.096670, loss: 1.7618
2022-07-06 17:03:48 - train: epoch 0015, iter [00200, 05004], lr: 0.096660, loss: 2.1581
2022-07-06 17:04:33 - train: epoch 0015, iter [00300, 05004], lr: 0.096651, loss: 2.0865
2022-07-06 17:05:19 - train: epoch 0015, iter [00400, 05004], lr: 0.096641, loss: 1.9266
2022-07-06 17:06:04 - train: epoch 0015, iter [00500, 05004], lr: 0.096632, loss: 2.0312
2022-07-06 17:06:50 - train: epoch 0015, iter [00600, 05004], lr: 0.096623, loss: 2.1508
2022-07-06 17:07:36 - train: epoch 0015, iter [00700, 05004], lr: 0.096613, loss: 1.9305
2022-07-06 17:08:21 - train: epoch 0015, iter [00800, 05004], lr: 0.096604, loss: 1.8130
2022-07-06 17:09:07 - train: epoch 0015, iter [00900, 05004], lr: 0.096594, loss: 1.9034
2022-07-06 17:09:53 - train: epoch 0015, iter [01000, 05004], lr: 0.096585, loss: 2.0286
2022-07-06 17:10:38 - train: epoch 0015, iter [01100, 05004], lr: 0.096575, loss: 1.8758
2022-07-06 17:11:24 - train: epoch 0015, iter [01200, 05004], lr: 0.096566, loss: 1.8931
2022-07-06 17:12:09 - train: epoch 0015, iter [01300, 05004], lr: 0.096556, loss: 2.2666
2022-07-06 17:12:55 - train: epoch 0015, iter [01400, 05004], lr: 0.096547, loss: 1.9366
2022-07-06 17:13:40 - train: epoch 0015, iter [01500, 05004], lr: 0.096537, loss: 1.7273
2022-07-06 17:14:26 - train: epoch 0015, iter [01600, 05004], lr: 0.096527, loss: 2.0282
2022-07-06 17:15:11 - train: epoch 0015, iter [01700, 05004], lr: 0.096518, loss: 2.0599
2022-07-06 17:15:57 - train: epoch 0015, iter [01800, 05004], lr: 0.096508, loss: 1.8462
2022-07-06 17:16:42 - train: epoch 0015, iter [01900, 05004], lr: 0.096499, loss: 1.8715
2022-07-06 17:17:28 - train: epoch 0015, iter [02000, 05004], lr: 0.096489, loss: 1.9109
2022-07-06 17:18:13 - train: epoch 0015, iter [02100, 05004], lr: 0.096479, loss: 1.9265
2022-07-06 17:18:58 - train: epoch 0015, iter [02200, 05004], lr: 0.096470, loss: 2.1094
2022-07-06 17:19:43 - train: epoch 0015, iter [02300, 05004], lr: 0.096460, loss: 1.8105
2022-07-06 17:20:28 - train: epoch 0015, iter [02400, 05004], lr: 0.096450, loss: 2.0624
2022-07-06 17:21:13 - train: epoch 0015, iter [02500, 05004], lr: 0.096441, loss: 2.0558
2022-07-06 17:21:59 - train: epoch 0015, iter [02600, 05004], lr: 0.096431, loss: 1.7121
2022-07-06 17:22:44 - train: epoch 0015, iter [02700, 05004], lr: 0.096421, loss: 2.0876
2022-07-06 17:23:29 - train: epoch 0015, iter [02800, 05004], lr: 0.096412, loss: 2.1662
2022-07-06 17:24:14 - train: epoch 0015, iter [02900, 05004], lr: 0.096402, loss: 1.9113
2022-07-06 17:24:59 - train: epoch 0015, iter [03000, 05004], lr: 0.096392, loss: 1.8155
2022-07-06 17:25:44 - train: epoch 0015, iter [03100, 05004], lr: 0.096382, loss: 1.9025
2022-07-06 17:26:30 - train: epoch 0015, iter [03200, 05004], lr: 0.096373, loss: 1.9001
2022-07-06 17:27:15 - train: epoch 0015, iter [03300, 05004], lr: 0.096363, loss: 1.8007
2022-07-06 17:28:00 - train: epoch 0015, iter [03400, 05004], lr: 0.096353, loss: 2.1182
2022-07-06 17:28:45 - train: epoch 0015, iter [03500, 05004], lr: 0.096343, loss: 2.1767
2022-07-06 17:29:30 - train: epoch 0015, iter [03600, 05004], lr: 0.096333, loss: 1.9930
2022-07-06 17:30:15 - train: epoch 0015, iter [03700, 05004], lr: 0.096323, loss: 1.8737
2022-07-06 17:31:00 - train: epoch 0015, iter [03800, 05004], lr: 0.096314, loss: 2.0129
2022-07-06 17:31:46 - train: epoch 0015, iter [03900, 05004], lr: 0.096304, loss: 2.0723
2022-07-06 17:32:31 - train: epoch 0015, iter [04000, 05004], lr: 0.096294, loss: 2.1226
2022-07-06 17:33:16 - train: epoch 0015, iter [04100, 05004], lr: 0.096284, loss: 2.1811
2022-07-06 17:34:02 - train: epoch 0015, iter [04200, 05004], lr: 0.096274, loss: 1.7084
2022-07-06 17:34:47 - train: epoch 0015, iter [04300, 05004], lr: 0.096264, loss: 2.0591
2022-07-06 17:35:33 - train: epoch 0015, iter [04400, 05004], lr: 0.096254, loss: 2.0350
2022-07-06 17:36:19 - train: epoch 0015, iter [04500, 05004], lr: 0.096244, loss: 1.9657
2022-07-06 17:37:05 - train: epoch 0015, iter [04600, 05004], lr: 0.096234, loss: 1.9686
2022-07-06 17:37:50 - train: epoch 0015, iter [04700, 05004], lr: 0.096224, loss: 2.0703
2022-07-06 17:38:36 - train: epoch 0015, iter [04800, 05004], lr: 0.096214, loss: 1.8345
2022-07-06 17:39:21 - train: epoch 0015, iter [04900, 05004], lr: 0.096204, loss: 1.8864
2022-07-06 17:40:07 - train: epoch 0015, iter [05000, 05004], lr: 0.096194, loss: 2.1217
2022-07-06 17:40:09 - train: epoch 015, train_loss: 1.9746
2022-07-06 17:41:28 - eval: epoch: 015, acc1: 60.544%, acc5: 83.790%, test_loss: 1.6538, per_image_load_time: 2.093ms, per_image_inference_time: 0.823ms
2022-07-06 17:41:29 - until epoch: 015, best_acc1: 60.544%
2022-07-06 17:41:29 - epoch 016 lr: 0.096194
2022-07-06 17:42:20 - train: epoch 0016, iter [00100, 05004], lr: 0.096184, loss: 1.9106
2022-07-06 17:43:06 - train: epoch 0016, iter [00200, 05004], lr: 0.096174, loss: 1.7362
2022-07-06 17:43:52 - train: epoch 0016, iter [00300, 05004], lr: 0.096164, loss: 1.9687
2022-07-06 17:44:38 - train: epoch 0016, iter [00400, 05004], lr: 0.096154, loss: 2.1630
2022-07-06 17:45:23 - train: epoch 0016, iter [00500, 05004], lr: 0.096144, loss: 1.7860
2022-07-06 17:46:09 - train: epoch 0016, iter [00600, 05004], lr: 0.096134, loss: 2.0557
2022-07-06 17:46:54 - train: epoch 0016, iter [00700, 05004], lr: 0.096124, loss: 1.7031
2022-07-06 17:47:39 - train: epoch 0016, iter [00800, 05004], lr: 0.096113, loss: 2.0388
2022-07-06 17:48:24 - train: epoch 0016, iter [00900, 05004], lr: 0.096103, loss: 2.0116
2022-07-06 17:49:09 - train: epoch 0016, iter [01000, 05004], lr: 0.096093, loss: 1.8882
2022-07-06 17:49:54 - train: epoch 0016, iter [01100, 05004], lr: 0.096083, loss: 1.9407
2022-07-06 17:50:39 - train: epoch 0016, iter [01200, 05004], lr: 0.096073, loss: 1.8623
2022-07-06 17:51:24 - train: epoch 0016, iter [01300, 05004], lr: 0.096063, loss: 2.1299
2022-07-06 17:52:09 - train: epoch 0016, iter [01400, 05004], lr: 0.096053, loss: 1.8519
2022-07-06 17:52:54 - train: epoch 0016, iter [01500, 05004], lr: 0.096042, loss: 2.0278
2022-07-06 17:53:39 - train: epoch 0016, iter [01600, 05004], lr: 0.096032, loss: 2.0761
2022-07-06 17:54:24 - train: epoch 0016, iter [01700, 05004], lr: 0.096022, loss: 1.8611
2022-07-06 17:55:09 - train: epoch 0016, iter [01800, 05004], lr: 0.096012, loss: 1.8172
2022-07-06 17:55:55 - train: epoch 0016, iter [01900, 05004], lr: 0.096001, loss: 1.9531
2022-07-06 17:56:40 - train: epoch 0016, iter [02000, 05004], lr: 0.095991, loss: 1.7349
2022-07-06 17:57:25 - train: epoch 0016, iter [02100, 05004], lr: 0.095981, loss: 2.0710
2022-07-06 17:58:10 - train: epoch 0016, iter [02200, 05004], lr: 0.095971, loss: 2.0643
2022-07-06 17:58:56 - train: epoch 0016, iter [02300, 05004], lr: 0.095960, loss: 2.1651
2022-07-06 17:59:41 - train: epoch 0016, iter [02400, 05004], lr: 0.095950, loss: 2.1323
2022-07-06 18:00:26 - train: epoch 0016, iter [02500, 05004], lr: 0.095940, loss: 1.9163
2022-07-06 18:01:11 - train: epoch 0016, iter [02600, 05004], lr: 0.095929, loss: 2.0619
2022-07-06 18:01:57 - train: epoch 0016, iter [02700, 05004], lr: 0.095919, loss: 2.0084
2022-07-06 18:02:42 - train: epoch 0016, iter [02800, 05004], lr: 0.095909, loss: 1.8305
2022-07-06 18:03:27 - train: epoch 0016, iter [02900, 05004], lr: 0.095898, loss: 2.0596
2022-07-06 18:04:12 - train: epoch 0016, iter [03000, 05004], lr: 0.095888, loss: 2.1520
2022-07-06 18:04:58 - train: epoch 0016, iter [03100, 05004], lr: 0.095878, loss: 2.0183
2022-07-06 18:05:43 - train: epoch 0016, iter [03200, 05004], lr: 0.095867, loss: 2.1096
2022-07-06 18:06:28 - train: epoch 0016, iter [03300, 05004], lr: 0.095857, loss: 2.0549
2022-07-06 18:07:13 - train: epoch 0016, iter [03400, 05004], lr: 0.095846, loss: 1.8605
2022-07-06 18:07:58 - train: epoch 0016, iter [03500, 05004], lr: 0.095836, loss: 1.9820
2022-07-06 18:08:44 - train: epoch 0016, iter [03600, 05004], lr: 0.095825, loss: 1.8576
2022-07-06 18:09:29 - train: epoch 0016, iter [03700, 05004], lr: 0.095815, loss: 2.0374
2022-07-06 18:10:14 - train: epoch 0016, iter [03800, 05004], lr: 0.095804, loss: 2.3701
2022-07-06 18:10:59 - train: epoch 0016, iter [03900, 05004], lr: 0.095794, loss: 1.9734
2022-07-06 18:11:44 - train: epoch 0016, iter [04000, 05004], lr: 0.095783, loss: 2.0434
2022-07-06 18:12:29 - train: epoch 0016, iter [04100, 05004], lr: 0.095773, loss: 1.9101
2022-07-06 18:13:15 - train: epoch 0016, iter [04200, 05004], lr: 0.095762, loss: 1.9580
2022-07-06 18:14:00 - train: epoch 0016, iter [04300, 05004], lr: 0.095752, loss: 1.8349
2022-07-06 18:14:45 - train: epoch 0016, iter [04400, 05004], lr: 0.095741, loss: 1.8666
2022-07-06 18:15:30 - train: epoch 0016, iter [04500, 05004], lr: 0.095731, loss: 2.0076
2022-07-06 18:16:16 - train: epoch 0016, iter [04600, 05004], lr: 0.095720, loss: 1.7720
2022-07-06 18:17:01 - train: epoch 0016, iter [04700, 05004], lr: 0.095710, loss: 2.2984
2022-07-06 18:17:46 - train: epoch 0016, iter [04800, 05004], lr: 0.095699, loss: 1.8578
2022-07-06 18:18:31 - train: epoch 0016, iter [04900, 05004], lr: 0.095688, loss: 1.9396
2022-07-06 18:19:16 - train: epoch 0016, iter [05000, 05004], lr: 0.095678, loss: 1.9990
2022-07-06 18:19:19 - train: epoch 016, train_loss: 1.9595
2022-07-06 18:20:36 - eval: epoch: 016, acc1: 59.484%, acc5: 83.226%, test_loss: 1.6955, per_image_load_time: 1.247ms, per_image_inference_time: 0.836ms
2022-07-06 18:20:37 - until epoch: 016, best_acc1: 60.544%
2022-07-06 18:20:37 - epoch 017 lr: 0.095677
2022-07-06 18:21:28 - train: epoch 0017, iter [00100, 05004], lr: 0.095667, loss: 1.7384
2022-07-06 18:22:14 - train: epoch 0017, iter [00200, 05004], lr: 0.095656, loss: 1.9435
2022-07-06 18:22:59 - train: epoch 0017, iter [00300, 05004], lr: 0.095645, loss: 2.2271
2022-07-06 18:23:45 - train: epoch 0017, iter [00400, 05004], lr: 0.095635, loss: 1.7251
2022-07-06 18:24:31 - train: epoch 0017, iter [00500, 05004], lr: 0.095624, loss: 2.0267
2022-07-06 18:25:17 - train: epoch 0017, iter [00600, 05004], lr: 0.095613, loss: 2.3158
2022-07-06 18:26:03 - train: epoch 0017, iter [00700, 05004], lr: 0.095602, loss: 1.9741
2022-07-06 18:26:48 - train: epoch 0017, iter [00800, 05004], lr: 0.095592, loss: 1.7943
2022-07-06 18:27:34 - train: epoch 0017, iter [00900, 05004], lr: 0.095581, loss: 1.8756
2022-07-06 18:28:20 - train: epoch 0017, iter [01000, 05004], lr: 0.095570, loss: 1.8642
2022-07-06 18:29:06 - train: epoch 0017, iter [01100, 05004], lr: 0.095559, loss: 2.2104
2022-07-06 18:29:51 - train: epoch 0017, iter [01200, 05004], lr: 0.095549, loss: 2.0996
2022-07-06 18:30:36 - train: epoch 0017, iter [01300, 05004], lr: 0.095538, loss: 1.9291
2022-07-06 18:31:21 - train: epoch 0017, iter [01400, 05004], lr: 0.095527, loss: 2.0149
2022-07-06 18:32:07 - train: epoch 0017, iter [01500, 05004], lr: 0.095516, loss: 1.6447
2022-07-06 18:32:52 - train: epoch 0017, iter [01600, 05004], lr: 0.095505, loss: 1.7842
2022-07-06 18:33:37 - train: epoch 0017, iter [01700, 05004], lr: 0.095495, loss: 1.8552
2022-07-06 18:34:22 - train: epoch 0017, iter [01800, 05004], lr: 0.095484, loss: 1.8949
2022-07-06 18:35:08 - train: epoch 0017, iter [01900, 05004], lr: 0.095473, loss: 1.8578
2022-07-06 18:35:53 - train: epoch 0017, iter [02000, 05004], lr: 0.095462, loss: 2.1601
2022-07-06 18:36:38 - train: epoch 0017, iter [02100, 05004], lr: 0.095451, loss: 1.9292
2022-07-06 18:37:24 - train: epoch 0017, iter [02200, 05004], lr: 0.095440, loss: 1.8840
2022-07-06 18:38:10 - train: epoch 0017, iter [02300, 05004], lr: 0.095429, loss: 1.9358
2022-07-06 18:38:55 - train: epoch 0017, iter [02400, 05004], lr: 0.095418, loss: 1.8362
2022-07-06 18:39:40 - train: epoch 0017, iter [02500, 05004], lr: 0.095407, loss: 2.0489
2022-07-06 18:40:26 - train: epoch 0017, iter [02600, 05004], lr: 0.095396, loss: 1.7718
2022-07-06 18:41:11 - train: epoch 0017, iter [02700, 05004], lr: 0.095385, loss: 1.7891
2022-07-06 18:41:56 - train: epoch 0017, iter [02800, 05004], lr: 0.095374, loss: 2.2197
2022-07-06 18:42:41 - train: epoch 0017, iter [02900, 05004], lr: 0.095363, loss: 2.0955
2022-07-06 18:43:26 - train: epoch 0017, iter [03000, 05004], lr: 0.095352, loss: 1.8305
2022-07-06 18:44:12 - train: epoch 0017, iter [03100, 05004], lr: 0.095341, loss: 2.0898
2022-07-06 18:44:57 - train: epoch 0017, iter [03200, 05004], lr: 0.095330, loss: 1.7987
2022-07-06 18:45:42 - train: epoch 0017, iter [03300, 05004], lr: 0.095319, loss: 2.0827
2022-07-06 18:46:27 - train: epoch 0017, iter [03400, 05004], lr: 0.095308, loss: 1.7777
2022-07-06 18:47:13 - train: epoch 0017, iter [03500, 05004], lr: 0.095297, loss: 1.9325
2022-07-06 18:47:58 - train: epoch 0017, iter [03600, 05004], lr: 0.095286, loss: 2.2035
2022-07-06 18:48:43 - train: epoch 0017, iter [03700, 05004], lr: 0.095275, loss: 1.9881
2022-07-06 18:49:28 - train: epoch 0017, iter [03800, 05004], lr: 0.095264, loss: 2.0669
2022-07-06 18:50:13 - train: epoch 0017, iter [03900, 05004], lr: 0.095253, loss: 1.8248
2022-07-06 18:50:59 - train: epoch 0017, iter [04000, 05004], lr: 0.095242, loss: 1.8304
2022-07-06 18:51:44 - train: epoch 0017, iter [04100, 05004], lr: 0.095231, loss: 2.0618
2022-07-06 18:52:29 - train: epoch 0017, iter [04200, 05004], lr: 0.095219, loss: 1.9415
2022-07-06 18:53:15 - train: epoch 0017, iter [04300, 05004], lr: 0.095208, loss: 1.9242
2022-07-06 18:54:00 - train: epoch 0017, iter [04400, 05004], lr: 0.095197, loss: 1.8653
2022-07-06 18:54:45 - train: epoch 0017, iter [04500, 05004], lr: 0.095186, loss: 2.1301
2022-07-06 18:55:31 - train: epoch 0017, iter [04600, 05004], lr: 0.095175, loss: 1.8614
2022-07-06 18:56:16 - train: epoch 0017, iter [04700, 05004], lr: 0.095163, loss: 2.1424
2022-07-06 18:57:02 - train: epoch 0017, iter [04800, 05004], lr: 0.095152, loss: 2.1141
2022-07-06 18:57:47 - train: epoch 0017, iter [04900, 05004], lr: 0.095141, loss: 1.7853
2022-07-06 18:58:32 - train: epoch 0017, iter [05000, 05004], lr: 0.095130, loss: 1.7792
2022-07-06 18:58:35 - train: epoch 017, train_loss: 1.9435
2022-07-06 18:59:53 - eval: epoch: 017, acc1: 60.666%, acc5: 84.282%, test_loss: 1.6382, per_image_load_time: 2.214ms, per_image_inference_time: 0.818ms
2022-07-06 18:59:54 - until epoch: 017, best_acc1: 60.666%
2022-07-06 18:59:54 - epoch 018 lr: 0.095129
2022-07-06 19:00:46 - train: epoch 0018, iter [00100, 05004], lr: 0.095118, loss: 1.8943
2022-07-06 19:01:32 - train: epoch 0018, iter [00200, 05004], lr: 0.095107, loss: 1.8750
2022-07-06 19:02:17 - train: epoch 0018, iter [00300, 05004], lr: 0.095095, loss: 2.0205
2022-07-06 19:03:03 - train: epoch 0018, iter [00400, 05004], lr: 0.095084, loss: 2.0515
2022-07-06 19:03:48 - train: epoch 0018, iter [00500, 05004], lr: 0.095073, loss: 1.8280
2022-07-06 19:04:33 - train: epoch 0018, iter [00600, 05004], lr: 0.095061, loss: 2.1133
2022-07-06 19:05:19 - train: epoch 0018, iter [00700, 05004], lr: 0.095050, loss: 1.6425
2022-07-06 19:06:04 - train: epoch 0018, iter [00800, 05004], lr: 0.095039, loss: 1.9602
2022-07-06 19:06:49 - train: epoch 0018, iter [00900, 05004], lr: 0.095027, loss: 2.0246
2022-07-06 19:07:34 - train: epoch 0018, iter [01000, 05004], lr: 0.095016, loss: 1.7496
2022-07-06 19:08:20 - train: epoch 0018, iter [01100, 05004], lr: 0.095005, loss: 2.1478
2022-07-06 19:09:05 - train: epoch 0018, iter [01200, 05004], lr: 0.094993, loss: 1.9597
2022-07-06 19:09:50 - train: epoch 0018, iter [01300, 05004], lr: 0.094982, loss: 2.2846
2022-07-06 19:10:35 - train: epoch 0018, iter [01400, 05004], lr: 0.094970, loss: 1.8674
2022-07-06 19:11:21 - train: epoch 0018, iter [01500, 05004], lr: 0.094959, loss: 2.1346
2022-07-06 19:12:06 - train: epoch 0018, iter [01600, 05004], lr: 0.094947, loss: 1.8238
2022-07-06 19:12:51 - train: epoch 0018, iter [01700, 05004], lr: 0.094936, loss: 1.9779
2022-07-06 19:13:37 - train: epoch 0018, iter [01800, 05004], lr: 0.094925, loss: 1.7073
2022-07-06 19:14:22 - train: epoch 0018, iter [01900, 05004], lr: 0.094913, loss: 2.0195
2022-07-06 19:15:07 - train: epoch 0018, iter [02000, 05004], lr: 0.094902, loss: 2.1279
2022-07-06 19:15:53 - train: epoch 0018, iter [02100, 05004], lr: 0.094890, loss: 2.0577
2022-07-06 19:16:38 - train: epoch 0018, iter [02200, 05004], lr: 0.094879, loss: 1.8934
2022-07-06 19:17:23 - train: epoch 0018, iter [02300, 05004], lr: 0.094867, loss: 1.8639
2022-07-06 19:18:09 - train: epoch 0018, iter [02400, 05004], lr: 0.094855, loss: 1.7850
2022-07-06 19:18:54 - train: epoch 0018, iter [02500, 05004], lr: 0.094844, loss: 1.7476
2022-07-06 19:19:39 - train: epoch 0018, iter [02600, 05004], lr: 0.094832, loss: 1.7983
2022-07-06 19:20:24 - train: epoch 0018, iter [02700, 05004], lr: 0.094821, loss: 2.0802
2022-07-06 19:21:10 - train: epoch 0018, iter [02800, 05004], lr: 0.094809, loss: 1.7022
2022-07-06 19:21:55 - train: epoch 0018, iter [02900, 05004], lr: 0.094797, loss: 1.9546
2022-07-06 19:22:40 - train: epoch 0018, iter [03000, 05004], lr: 0.094786, loss: 1.8944
2022-07-06 19:23:26 - train: epoch 0018, iter [03100, 05004], lr: 0.094774, loss: 2.2380
2022-07-06 19:24:11 - train: epoch 0018, iter [03200, 05004], lr: 0.094763, loss: 1.9020
2022-07-06 19:24:57 - train: epoch 0018, iter [03300, 05004], lr: 0.094751, loss: 1.8437
2022-07-06 19:25:42 - train: epoch 0018, iter [03400, 05004], lr: 0.094739, loss: 1.8558
2022-07-06 19:26:27 - train: epoch 0018, iter [03500, 05004], lr: 0.094728, loss: 2.2315
2022-07-06 19:27:13 - train: epoch 0018, iter [03600, 05004], lr: 0.094716, loss: 1.9429
2022-07-06 19:27:58 - train: epoch 0018, iter [03700, 05004], lr: 0.094704, loss: 2.2348
2022-07-06 19:28:43 - train: epoch 0018, iter [03800, 05004], lr: 0.094692, loss: 2.0394
2022-07-06 19:29:29 - train: epoch 0018, iter [03900, 05004], lr: 0.094681, loss: 2.0612
2022-07-06 19:30:14 - train: epoch 0018, iter [04000, 05004], lr: 0.094669, loss: 1.8787
2022-07-06 19:30:59 - train: epoch 0018, iter [04100, 05004], lr: 0.094657, loss: 1.9181
2022-07-06 19:31:44 - train: epoch 0018, iter [04200, 05004], lr: 0.094645, loss: 2.0375
2022-07-06 19:32:29 - train: epoch 0018, iter [04300, 05004], lr: 0.094634, loss: 1.6495
2022-07-06 19:33:15 - train: epoch 0018, iter [04400, 05004], lr: 0.094622, loss: 2.0829
2022-07-06 19:34:00 - train: epoch 0018, iter [04500, 05004], lr: 0.094610, loss: 1.9036
2022-07-06 19:34:45 - train: epoch 0018, iter [04600, 05004], lr: 0.094598, loss: 1.7730
2022-07-06 19:35:31 - train: epoch 0018, iter [04700, 05004], lr: 0.094586, loss: 2.1443
2022-07-06 19:36:16 - train: epoch 0018, iter [04800, 05004], lr: 0.094575, loss: 2.0248
2022-07-06 19:37:02 - train: epoch 0018, iter [04900, 05004], lr: 0.094563, loss: 1.8854
2022-07-06 19:37:47 - train: epoch 0018, iter [05000, 05004], lr: 0.094551, loss: 2.0427
2022-07-06 19:37:49 - train: epoch 018, train_loss: 1.9298
2022-07-06 19:39:06 - eval: epoch: 018, acc1: 60.210%, acc5: 83.568%, test_loss: 1.6728, per_image_load_time: 2.160ms, per_image_inference_time: 0.845ms
2022-07-06 19:39:07 - until epoch: 018, best_acc1: 60.666%
2022-07-06 19:39:07 - epoch 019 lr: 0.094550
2022-07-06 19:39:59 - train: epoch 0019, iter [00100, 05004], lr: 0.094538, loss: 1.7042
2022-07-06 19:40:44 - train: epoch 0019, iter [00200, 05004], lr: 0.094527, loss: 2.0571
2022-07-06 19:41:30 - train: epoch 0019, iter [00300, 05004], lr: 0.094515, loss: 2.1551
2022-07-06 19:42:15 - train: epoch 0019, iter [00400, 05004], lr: 0.094503, loss: 1.8455
2022-07-06 19:43:00 - train: epoch 0019, iter [00500, 05004], lr: 0.094491, loss: 1.9404
2022-07-06 19:43:46 - train: epoch 0019, iter [00600, 05004], lr: 0.094479, loss: 1.9663
2022-07-06 19:44:31 - train: epoch 0019, iter [00700, 05004], lr: 0.094467, loss: 1.6648
2022-07-06 19:45:16 - train: epoch 0019, iter [00800, 05004], lr: 0.094455, loss: 2.0497
2022-07-06 19:46:02 - train: epoch 0019, iter [00900, 05004], lr: 0.094443, loss: 2.0148
2022-07-06 19:46:47 - train: epoch 0019, iter [01000, 05004], lr: 0.094431, loss: 1.9565
2022-07-06 19:47:33 - train: epoch 0019, iter [01100, 05004], lr: 0.094419, loss: 1.7869
2022-07-06 19:48:18 - train: epoch 0019, iter [01200, 05004], lr: 0.094407, loss: 2.0028
2022-07-06 19:49:04 - train: epoch 0019, iter [01300, 05004], lr: 0.094395, loss: 2.0394
2022-07-06 19:49:49 - train: epoch 0019, iter [01400, 05004], lr: 0.094383, loss: 1.8473
2022-07-06 19:50:34 - train: epoch 0019, iter [01500, 05004], lr: 0.094371, loss: 2.2432
2022-07-06 19:51:20 - train: epoch 0019, iter [01600, 05004], lr: 0.094359, loss: 1.9470
2022-07-06 19:52:05 - train: epoch 0019, iter [01700, 05004], lr: 0.094347, loss: 1.9775
2022-07-06 19:52:50 - train: epoch 0019, iter [01800, 05004], lr: 0.094335, loss: 1.8441
2022-07-06 19:53:36 - train: epoch 0019, iter [01900, 05004], lr: 0.094322, loss: 2.0153
2022-07-06 19:54:21 - train: epoch 0019, iter [02000, 05004], lr: 0.094310, loss: 1.8003
2022-07-06 19:55:07 - train: epoch 0019, iter [02100, 05004], lr: 0.094298, loss: 1.7093
2022-07-06 19:55:52 - train: epoch 0019, iter [02200, 05004], lr: 0.094286, loss: 1.9931
2022-07-06 19:56:37 - train: epoch 0019, iter [02300, 05004], lr: 0.094274, loss: 1.8759
2022-07-06 19:57:23 - train: epoch 0019, iter [02400, 05004], lr: 0.094262, loss: 2.0188
2022-07-06 19:58:08 - train: epoch 0019, iter [02500, 05004], lr: 0.094250, loss: 1.7885
2022-07-06 19:58:54 - train: epoch 0019, iter [02600, 05004], lr: 0.094237, loss: 1.9907
2022-07-06 19:59:39 - train: epoch 0019, iter [02700, 05004], lr: 0.094225, loss: 2.0047
2022-07-06 20:00:24 - train: epoch 0019, iter [02800, 05004], lr: 0.094213, loss: 1.9862
2022-07-06 20:01:10 - train: epoch 0019, iter [02900, 05004], lr: 0.094201, loss: 1.9698
2022-07-06 20:01:55 - train: epoch 0019, iter [03000, 05004], lr: 0.094189, loss: 2.1585
2022-07-06 20:02:41 - train: epoch 0019, iter [03100, 05004], lr: 0.094176, loss: 1.9796
2022-07-06 20:03:26 - train: epoch 0019, iter [03200, 05004], lr: 0.094164, loss: 1.6226
2022-07-06 20:04:11 - train: epoch 0019, iter [03300, 05004], lr: 0.094152, loss: 1.8361
2022-07-06 20:04:57 - train: epoch 0019, iter [03400, 05004], lr: 0.094140, loss: 1.9450
2022-07-06 20:05:42 - train: epoch 0019, iter [03500, 05004], lr: 0.094127, loss: 1.8734
2022-07-06 20:06:27 - train: epoch 0019, iter [03600, 05004], lr: 0.094115, loss: 1.7451
2022-07-06 20:07:13 - train: epoch 0019, iter [03700, 05004], lr: 0.094103, loss: 1.9221
2022-07-06 20:07:58 - train: epoch 0019, iter [03800, 05004], lr: 0.094090, loss: 2.0747
2022-07-06 20:08:43 - train: epoch 0019, iter [03900, 05004], lr: 0.094078, loss: 1.9072
2022-07-06 20:09:29 - train: epoch 0019, iter [04000, 05004], lr: 0.094066, loss: 1.7544
2022-07-06 20:10:14 - train: epoch 0019, iter [04100, 05004], lr: 0.094053, loss: 2.0302
2022-07-06 20:10:59 - train: epoch 0019, iter [04200, 05004], lr: 0.094041, loss: 1.9334
2022-07-06 20:11:45 - train: epoch 0019, iter [04300, 05004], lr: 0.094028, loss: 1.8152
2022-07-06 20:12:30 - train: epoch 0019, iter [04400, 05004], lr: 0.094016, loss: 2.0408
2022-07-06 20:13:15 - train: epoch 0019, iter [04500, 05004], lr: 0.094004, loss: 2.1818
2022-07-06 20:14:01 - train: epoch 0019, iter [04600, 05004], lr: 0.093991, loss: 1.7850
2022-07-06 20:14:46 - train: epoch 0019, iter [04700, 05004], lr: 0.093979, loss: 1.7875
2022-07-06 20:15:31 - train: epoch 0019, iter [04800, 05004], lr: 0.093966, loss: 2.0201
2022-07-06 20:16:17 - train: epoch 0019, iter [04900, 05004], lr: 0.093954, loss: 1.9899
2022-07-06 20:17:02 - train: epoch 0019, iter [05000, 05004], lr: 0.093941, loss: 1.9230
2022-07-06 20:17:05 - train: epoch 019, train_loss: 1.9212
2022-07-06 20:18:21 - eval: epoch: 019, acc1: 60.764%, acc5: 84.126%, test_loss: 1.6414, per_image_load_time: 2.080ms, per_image_inference_time: 0.827ms
2022-07-06 20:18:22 - until epoch: 019, best_acc1: 60.764%
2022-07-06 20:18:22 - epoch 020 lr: 0.093941
2022-07-06 20:19:13 - train: epoch 0020, iter [00100, 05004], lr: 0.093928, loss: 2.0460
2022-07-06 20:19:59 - train: epoch 0020, iter [00200, 05004], lr: 0.093916, loss: 1.7169
2022-07-06 20:20:44 - train: epoch 0020, iter [00300, 05004], lr: 0.093903, loss: 1.8908
2022-07-06 20:21:30 - train: epoch 0020, iter [00400, 05004], lr: 0.093891, loss: 1.6935
2022-07-06 20:22:15 - train: epoch 0020, iter [00500, 05004], lr: 0.093878, loss: 1.7512
2022-07-06 20:23:00 - train: epoch 0020, iter [00600, 05004], lr: 0.093866, loss: 1.9943
2022-07-06 20:23:46 - train: epoch 0020, iter [00700, 05004], lr: 0.093853, loss: 1.6198
2022-07-06 20:24:31 - train: epoch 0020, iter [00800, 05004], lr: 0.093841, loss: 1.9189
2022-07-06 20:25:16 - train: epoch 0020, iter [00900, 05004], lr: 0.093828, loss: 2.2291
2022-07-06 20:26:02 - train: epoch 0020, iter [01000, 05004], lr: 0.093815, loss: 1.9498
2022-07-06 20:26:47 - train: epoch 0020, iter [01100, 05004], lr: 0.093803, loss: 1.6789
2022-07-06 20:27:32 - train: epoch 0020, iter [01200, 05004], lr: 0.093790, loss: 1.6240
2022-07-06 20:28:18 - train: epoch 0020, iter [01300, 05004], lr: 0.093778, loss: 1.7625
2022-07-06 20:29:03 - train: epoch 0020, iter [01400, 05004], lr: 0.093765, loss: 2.0289
2022-07-06 20:29:48 - train: epoch 0020, iter [01500, 05004], lr: 0.093752, loss: 2.0231
2022-07-06 20:30:33 - train: epoch 0020, iter [01600, 05004], lr: 0.093740, loss: 1.8229
2022-07-06 20:31:19 - train: epoch 0020, iter [01700, 05004], lr: 0.093727, loss: 1.6043
2022-07-06 20:32:04 - train: epoch 0020, iter [01800, 05004], lr: 0.093714, loss: 1.9045
2022-07-06 20:32:49 - train: epoch 0020, iter [01900, 05004], lr: 0.093702, loss: 1.8242
2022-07-06 20:33:34 - train: epoch 0020, iter [02000, 05004], lr: 0.093689, loss: 1.8459
2022-07-06 20:34:20 - train: epoch 0020, iter [02100, 05004], lr: 0.093676, loss: 2.1097
2022-07-06 20:35:05 - train: epoch 0020, iter [02200, 05004], lr: 0.093663, loss: 1.7430
2022-07-06 20:35:50 - train: epoch 0020, iter [02300, 05004], lr: 0.093651, loss: 1.8823
2022-07-06 20:36:35 - train: epoch 0020, iter [02400, 05004], lr: 0.093638, loss: 2.1298
2022-07-06 20:37:21 - train: epoch 0020, iter [02500, 05004], lr: 0.093625, loss: 1.8799
2022-07-06 20:38:06 - train: epoch 0020, iter [02600, 05004], lr: 0.093612, loss: 1.6895
2022-07-06 20:38:51 - train: epoch 0020, iter [02700, 05004], lr: 0.093599, loss: 2.0515
2022-07-06 20:39:37 - train: epoch 0020, iter [02800, 05004], lr: 0.093587, loss: 1.9522
2022-07-06 20:40:22 - train: epoch 0020, iter [02900, 05004], lr: 0.093574, loss: 2.1399
2022-07-06 20:41:07 - train: epoch 0020, iter [03000, 05004], lr: 0.093561, loss: 1.8694
2022-07-06 20:41:52 - train: epoch 0020, iter [03100, 05004], lr: 0.093548, loss: 2.0315
2022-07-06 20:42:38 - train: epoch 0020, iter [03200, 05004], lr: 0.093535, loss: 2.1111
2022-07-06 20:43:23 - train: epoch 0020, iter [03300, 05004], lr: 0.093522, loss: 1.7309
2022-07-06 20:44:08 - train: epoch 0020, iter [03400, 05004], lr: 0.093510, loss: 1.9481
2022-07-06 20:44:53 - train: epoch 0020, iter [03500, 05004], lr: 0.093497, loss: 1.8273
2022-07-06 20:45:38 - train: epoch 0020, iter [03600, 05004], lr: 0.093484, loss: 1.8834
2022-07-06 20:46:24 - train: epoch 0020, iter [03700, 05004], lr: 0.093471, loss: 1.7875
2022-07-06 20:47:09 - train: epoch 0020, iter [03800, 05004], lr: 0.093458, loss: 1.8754
2022-07-06 20:47:54 - train: epoch 0020, iter [03900, 05004], lr: 0.093445, loss: 2.0266
2022-07-06 20:48:39 - train: epoch 0020, iter [04000, 05004], lr: 0.093432, loss: 1.7618
2022-07-06 20:49:25 - train: epoch 0020, iter [04100, 05004], lr: 0.093419, loss: 1.7530
2022-07-06 20:50:10 - train: epoch 0020, iter [04200, 05004], lr: 0.093406, loss: 1.8820
2022-07-06 20:50:55 - train: epoch 0020, iter [04300, 05004], lr: 0.093393, loss: 1.9324
2022-07-06 20:51:41 - train: epoch 0020, iter [04400, 05004], lr: 0.093380, loss: 1.8383
2022-07-06 20:52:26 - train: epoch 0020, iter [04500, 05004], lr: 0.093367, loss: 2.0173
2022-07-06 20:53:11 - train: epoch 0020, iter [04600, 05004], lr: 0.093354, loss: 1.9413
2022-07-06 20:53:57 - train: epoch 0020, iter [04700, 05004], lr: 0.093341, loss: 1.8250
2022-07-06 20:54:42 - train: epoch 0020, iter [04800, 05004], lr: 0.093328, loss: 1.8815
2022-07-06 20:55:27 - train: epoch 0020, iter [04900, 05004], lr: 0.093315, loss: 2.0355
2022-07-06 20:56:12 - train: epoch 0020, iter [05000, 05004], lr: 0.093302, loss: 1.7878
2022-07-06 20:56:15 - train: epoch 020, train_loss: 1.9035
2022-07-06 20:57:31 - eval: epoch: 020, acc1: 61.400%, acc5: 84.476%, test_loss: 1.6207, per_image_load_time: 2.152ms, per_image_inference_time: 0.821ms
2022-07-06 20:57:32 - until epoch: 020, best_acc1: 61.400%
2022-07-06 20:57:32 - epoch 021 lr: 0.093301
2022-07-06 20:58:24 - train: epoch 0021, iter [00100, 05004], lr: 0.093288, loss: 1.7318
2022-07-06 20:59:09 - train: epoch 0021, iter [00200, 05004], lr: 0.093275, loss: 1.9048
2022-07-06 20:59:54 - train: epoch 0021, iter [00300, 05004], lr: 0.093262, loss: 1.6408
2022-07-06 21:00:40 - train: epoch 0021, iter [00400, 05004], lr: 0.093249, loss: 2.0110
2022-07-06 21:01:25 - train: epoch 0021, iter [00500, 05004], lr: 0.093236, loss: 1.8516
2022-07-06 21:02:11 - train: epoch 0021, iter [00600, 05004], lr: 0.093223, loss: 1.8035
2022-07-06 21:02:56 - train: epoch 0021, iter [00700, 05004], lr: 0.093209, loss: 1.7469
2022-07-06 21:03:41 - train: epoch 0021, iter [00800, 05004], lr: 0.093196, loss: 2.0464
2022-07-06 21:04:26 - train: epoch 0021, iter [00900, 05004], lr: 0.093183, loss: 1.9458
2022-07-06 21:05:12 - train: epoch 0021, iter [01000, 05004], lr: 0.093170, loss: 1.7330
2022-07-06 21:05:57 - train: epoch 0021, iter [01100, 05004], lr: 0.093157, loss: 1.6679
2022-07-06 21:06:42 - train: epoch 0021, iter [01200, 05004], lr: 0.093143, loss: 1.8039
2022-07-06 21:07:28 - train: epoch 0021, iter [01300, 05004], lr: 0.093130, loss: 1.7672
2022-07-06 21:08:13 - train: epoch 0021, iter [01400, 05004], lr: 0.093117, loss: 1.7803
2022-07-06 21:08:58 - train: epoch 0021, iter [01500, 05004], lr: 0.093104, loss: 1.6752
2022-07-06 21:09:44 - train: epoch 0021, iter [01600, 05004], lr: 0.093090, loss: 1.9128
2022-07-06 21:10:29 - train: epoch 0021, iter [01700, 05004], lr: 0.093077, loss: 1.8654
2022-07-06 21:11:15 - train: epoch 0021, iter [01800, 05004], lr: 0.093064, loss: 1.7833
2022-07-06 21:12:01 - train: epoch 0021, iter [01900, 05004], lr: 0.093051, loss: 2.0095
2022-07-06 21:12:46 - train: epoch 0021, iter [02000, 05004], lr: 0.093037, loss: 2.1507
2022-07-06 21:13:32 - train: epoch 0021, iter [02100, 05004], lr: 0.093024, loss: 1.7320
2022-07-06 21:14:17 - train: epoch 0021, iter [02200, 05004], lr: 0.093011, loss: 1.7999
2022-07-06 21:15:03 - train: epoch 0021, iter [02300, 05004], lr: 0.092997, loss: 1.7269
2022-07-06 21:15:49 - train: epoch 0021, iter [02400, 05004], lr: 0.092984, loss: 1.7536
2022-07-06 21:16:34 - train: epoch 0021, iter [02500, 05004], lr: 0.092971, loss: 1.8981
2022-07-06 21:17:19 - train: epoch 0021, iter [02600, 05004], lr: 0.092957, loss: 2.1984
2022-07-06 21:18:05 - train: epoch 0021, iter [02700, 05004], lr: 0.092944, loss: 1.7471
2022-07-06 21:18:50 - train: epoch 0021, iter [02800, 05004], lr: 0.092930, loss: 1.7835
2022-07-06 21:19:36 - train: epoch 0021, iter [02900, 05004], lr: 0.092917, loss: 1.8046
2022-07-06 21:20:21 - train: epoch 0021, iter [03000, 05004], lr: 0.092904, loss: 2.1162
2022-07-06 21:21:06 - train: epoch 0021, iter [03100, 05004], lr: 0.092890, loss: 1.9412
2022-07-06 21:21:52 - train: epoch 0021, iter [03200, 05004], lr: 0.092877, loss: 1.9360
2022-07-06 21:22:37 - train: epoch 0021, iter [03300, 05004], lr: 0.092863, loss: 2.1482
2022-07-06 21:23:22 - train: epoch 0021, iter [03400, 05004], lr: 0.092850, loss: 2.0172
2022-07-06 21:24:07 - train: epoch 0021, iter [03500, 05004], lr: 0.092836, loss: 1.7138
2022-07-06 21:24:53 - train: epoch 0021, iter [03600, 05004], lr: 0.092823, loss: 1.8856
2022-07-06 21:25:39 - train: epoch 0021, iter [03700, 05004], lr: 0.092809, loss: 1.9187
2022-07-06 21:26:24 - train: epoch 0021, iter [03800, 05004], lr: 0.092796, loss: 1.9166
2022-07-06 21:27:09 - train: epoch 0021, iter [03900, 05004], lr: 0.092782, loss: 1.7293
2022-07-06 21:27:55 - train: epoch 0021, iter [04000, 05004], lr: 0.092769, loss: 2.2016
2022-07-06 21:28:40 - train: epoch 0021, iter [04100, 05004], lr: 0.092755, loss: 1.8912
2022-07-06 21:29:25 - train: epoch 0021, iter [04200, 05004], lr: 0.092742, loss: 1.7940
2022-07-06 21:30:11 - train: epoch 0021, iter [04300, 05004], lr: 0.092728, loss: 1.8534
2022-07-06 21:30:56 - train: epoch 0021, iter [04400, 05004], lr: 0.092714, loss: 2.0032
2022-07-06 21:31:42 - train: epoch 0021, iter [04500, 05004], lr: 0.092701, loss: 2.0332
2022-07-06 21:32:27 - train: epoch 0021, iter [04600, 05004], lr: 0.092687, loss: 1.7975
2022-07-06 21:33:12 - train: epoch 0021, iter [04700, 05004], lr: 0.092674, loss: 2.0553
2022-07-06 21:33:58 - train: epoch 0021, iter [04800, 05004], lr: 0.092660, loss: 2.0368
2022-07-06 21:34:43 - train: epoch 0021, iter [04900, 05004], lr: 0.092646, loss: 1.8292
2022-07-06 21:35:28 - train: epoch 0021, iter [05000, 05004], lr: 0.092633, loss: 1.8751
2022-07-06 21:35:30 - train: epoch 021, train_loss: 1.8963
2022-07-06 21:36:48 - eval: epoch: 021, acc1: 61.114%, acc5: 84.352%, test_loss: 1.6364, per_image_load_time: 2.161ms, per_image_inference_time: 0.826ms
2022-07-06 21:36:49 - until epoch: 021, best_acc1: 61.400%
2022-07-06 21:36:49 - epoch 022 lr: 0.092632
2022-07-06 21:37:42 - train: epoch 0022, iter [00100, 05004], lr: 0.092618, loss: 1.6249
2022-07-06 21:38:28 - train: epoch 0022, iter [00200, 05004], lr: 0.092605, loss: 1.7237
2022-07-06 21:39:14 - train: epoch 0022, iter [00300, 05004], lr: 0.092591, loss: 1.6163
2022-07-06 21:40:01 - train: epoch 0022, iter [00400, 05004], lr: 0.092577, loss: 1.6825
2022-07-06 21:40:47 - train: epoch 0022, iter [00500, 05004], lr: 0.092564, loss: 1.8750
2022-07-06 21:41:33 - train: epoch 0022, iter [00600, 05004], lr: 0.092550, loss: 2.0553
2022-07-06 21:42:20 - train: epoch 0022, iter [00700, 05004], lr: 0.092536, loss: 1.9792
2022-07-06 21:43:06 - train: epoch 0022, iter [00800, 05004], lr: 0.092522, loss: 1.9360
2022-07-06 21:43:52 - train: epoch 0022, iter [00900, 05004], lr: 0.092509, loss: 2.0287
2022-07-06 21:44:38 - train: epoch 0022, iter [01000, 05004], lr: 0.092495, loss: 1.9213
2022-07-06 21:45:25 - train: epoch 0022, iter [01100, 05004], lr: 0.092481, loss: 1.8448
2022-07-06 21:46:11 - train: epoch 0022, iter [01200, 05004], lr: 0.092467, loss: 1.5575
2022-07-06 21:46:57 - train: epoch 0022, iter [01300, 05004], lr: 0.092453, loss: 1.8761
2022-07-06 21:47:43 - train: epoch 0022, iter [01400, 05004], lr: 0.092440, loss: 1.8622
2022-07-06 21:48:30 - train: epoch 0022, iter [01500, 05004], lr: 0.092426, loss: 1.7917
2022-07-06 21:49:15 - train: epoch 0022, iter [01600, 05004], lr: 0.092412, loss: 1.6558
2022-07-06 21:50:00 - train: epoch 0022, iter [01700, 05004], lr: 0.092398, loss: 1.6717
2022-07-06 21:50:45 - train: epoch 0022, iter [01800, 05004], lr: 0.092384, loss: 2.0747
2022-07-06 21:51:30 - train: epoch 0022, iter [01900, 05004], lr: 0.092370, loss: 1.6411
2022-07-06 21:52:15 - train: epoch 0022, iter [02000, 05004], lr: 0.092356, loss: 2.0062
2022-07-06 21:53:00 - train: epoch 0022, iter [02100, 05004], lr: 0.092342, loss: 2.0416
2022-07-06 21:53:45 - train: epoch 0022, iter [02200, 05004], lr: 0.092328, loss: 1.7227
2022-07-06 21:54:30 - train: epoch 0022, iter [02300, 05004], lr: 0.092315, loss: 2.0473
2022-07-06 21:55:15 - train: epoch 0022, iter [02400, 05004], lr: 0.092301, loss: 2.0071
2022-07-06 21:56:01 - train: epoch 0022, iter [02500, 05004], lr: 0.092287, loss: 1.9953
2022-07-06 21:56:46 - train: epoch 0022, iter [02600, 05004], lr: 0.092273, loss: 1.6634
2022-07-06 21:57:31 - train: epoch 0022, iter [02700, 05004], lr: 0.092259, loss: 1.6819
2022-07-06 21:58:16 - train: epoch 0022, iter [02800, 05004], lr: 0.092245, loss: 2.2990
2022-07-06 21:59:01 - train: epoch 0022, iter [02900, 05004], lr: 0.092231, loss: 1.7325
2022-07-06 21:59:46 - train: epoch 0022, iter [03000, 05004], lr: 0.092217, loss: 1.9758
2022-07-06 22:00:31 - train: epoch 0022, iter [03100, 05004], lr: 0.092203, loss: 1.9977
2022-07-06 22:01:16 - train: epoch 0022, iter [03200, 05004], lr: 0.092189, loss: 1.9921
2022-07-06 22:02:01 - train: epoch 0022, iter [03300, 05004], lr: 0.092175, loss: 1.9290
2022-07-06 22:02:46 - train: epoch 0022, iter [03400, 05004], lr: 0.092161, loss: 1.6394
2022-07-06 22:03:31 - train: epoch 0022, iter [03500, 05004], lr: 0.092147, loss: 2.0336
2022-07-06 22:04:16 - train: epoch 0022, iter [03600, 05004], lr: 0.092132, loss: 1.8910
2022-07-06 22:05:00 - train: epoch 0022, iter [03700, 05004], lr: 0.092118, loss: 1.9120
2022-07-06 22:05:45 - train: epoch 0022, iter [03800, 05004], lr: 0.092104, loss: 1.9956
2022-07-06 22:06:30 - train: epoch 0022, iter [03900, 05004], lr: 0.092090, loss: 1.8461
2022-07-06 22:07:15 - train: epoch 0022, iter [04000, 05004], lr: 0.092076, loss: 1.9688
2022-07-06 22:07:59 - train: epoch 0022, iter [04100, 05004], lr: 0.092062, loss: 1.8537
2022-07-06 22:08:44 - train: epoch 0022, iter [04200, 05004], lr: 0.092048, loss: 1.8817
2022-07-06 22:09:29 - train: epoch 0022, iter [04300, 05004], lr: 0.092034, loss: 2.1749
2022-07-06 22:10:14 - train: epoch 0022, iter [04400, 05004], lr: 0.092019, loss: 1.8404
2022-07-06 22:10:58 - train: epoch 0022, iter [04500, 05004], lr: 0.092005, loss: 1.9841
2022-07-06 22:11:43 - train: epoch 0022, iter [04600, 05004], lr: 0.091991, loss: 2.1230
2022-07-06 22:12:28 - train: epoch 0022, iter [04700, 05004], lr: 0.091977, loss: 1.9920
2022-07-06 22:13:12 - train: epoch 0022, iter [04800, 05004], lr: 0.091963, loss: 1.6637
2022-07-06 22:13:57 - train: epoch 0022, iter [04900, 05004], lr: 0.091948, loss: 1.7158
2022-07-06 22:14:42 - train: epoch 0022, iter [05000, 05004], lr: 0.091934, loss: 1.7679
2022-07-06 22:14:44 - train: epoch 022, train_loss: 1.8836
2022-07-06 22:16:02 - eval: epoch: 022, acc1: 60.494%, acc5: 83.962%, test_loss: 1.6532, per_image_load_time: 2.203ms, per_image_inference_time: 0.818ms
2022-07-06 22:16:03 - until epoch: 022, best_acc1: 61.400%
2022-07-06 22:16:03 - epoch 023 lr: 0.091933
2022-07-06 22:16:54 - train: epoch 0023, iter [00100, 05004], lr: 0.091919, loss: 1.7704
2022-07-06 22:17:39 - train: epoch 0023, iter [00200, 05004], lr: 0.091905, loss: 1.5567
2022-07-06 22:18:24 - train: epoch 0023, iter [00300, 05004], lr: 0.091891, loss: 1.7431
2022-07-06 22:19:09 - train: epoch 0023, iter [00400, 05004], lr: 0.091876, loss: 1.8672
2022-07-06 22:19:54 - train: epoch 0023, iter [00500, 05004], lr: 0.091862, loss: 1.9824
2022-07-06 22:20:39 - train: epoch 0023, iter [00600, 05004], lr: 0.091848, loss: 1.7808
2022-07-06 22:21:23 - train: epoch 0023, iter [00700, 05004], lr: 0.091834, loss: 1.5948
2022-07-06 22:22:08 - train: epoch 0023, iter [00800, 05004], lr: 0.091819, loss: 1.7518
2022-07-06 22:22:54 - train: epoch 0023, iter [00900, 05004], lr: 0.091805, loss: 1.8953
2022-07-06 22:23:39 - train: epoch 0023, iter [01000, 05004], lr: 0.091790, loss: 1.8002
2022-07-06 22:24:24 - train: epoch 0023, iter [01100, 05004], lr: 0.091776, loss: 2.0243
2022-07-06 22:25:08 - train: epoch 0023, iter [01200, 05004], lr: 0.091762, loss: 1.6749
2022-07-06 22:25:53 - train: epoch 0023, iter [01300, 05004], lr: 0.091747, loss: 1.8439
2022-07-06 22:26:38 - train: epoch 0023, iter [01400, 05004], lr: 0.091733, loss: 1.9704
2022-07-06 22:27:23 - train: epoch 0023, iter [01500, 05004], lr: 0.091719, loss: 1.7107
2022-07-06 22:28:07 - train: epoch 0023, iter [01600, 05004], lr: 0.091704, loss: 1.9556
2022-07-06 22:28:52 - train: epoch 0023, iter [01700, 05004], lr: 0.091690, loss: 2.0779
2022-07-06 22:29:37 - train: epoch 0023, iter [01800, 05004], lr: 0.091675, loss: 1.7388
2022-07-06 22:30:22 - train: epoch 0023, iter [01900, 05004], lr: 0.091661, loss: 1.9195
2022-07-06 22:31:07 - train: epoch 0023, iter [02000, 05004], lr: 0.091646, loss: 1.5890
2022-07-06 22:31:52 - train: epoch 0023, iter [02100, 05004], lr: 0.091632, loss: 2.0458
2022-07-06 22:32:37 - train: epoch 0023, iter [02200, 05004], lr: 0.091617, loss: 1.6269
2022-07-06 22:33:22 - train: epoch 0023, iter [02300, 05004], lr: 0.091603, loss: 1.8006
2022-07-06 22:34:07 - train: epoch 0023, iter [02400, 05004], lr: 0.091588, loss: 1.8141
2022-07-06 22:34:52 - train: epoch 0023, iter [02500, 05004], lr: 0.091574, loss: 2.0562
2022-07-06 22:35:37 - train: epoch 0023, iter [02600, 05004], lr: 0.091559, loss: 1.8986
2022-07-06 22:36:22 - train: epoch 0023, iter [02700, 05004], lr: 0.091545, loss: 1.8704
2022-07-06 22:37:07 - train: epoch 0023, iter [02800, 05004], lr: 0.091530, loss: 1.8117
2022-07-06 22:37:52 - train: epoch 0023, iter [02900, 05004], lr: 0.091516, loss: 2.0136
2022-07-06 22:38:37 - train: epoch 0023, iter [03000, 05004], lr: 0.091501, loss: 1.9780
2022-07-06 22:39:22 - train: epoch 0023, iter [03100, 05004], lr: 0.091486, loss: 1.9985
2022-07-06 22:40:07 - train: epoch 0023, iter [03200, 05004], lr: 0.091472, loss: 1.9933
2022-07-06 22:40:52 - train: epoch 0023, iter [03300, 05004], lr: 0.091457, loss: 1.8856
2022-07-06 22:41:37 - train: epoch 0023, iter [03400, 05004], lr: 0.091443, loss: 2.0306
2022-07-06 22:42:22 - train: epoch 0023, iter [03500, 05004], lr: 0.091428, loss: 1.7575
2022-07-06 22:43:06 - train: epoch 0023, iter [03600, 05004], lr: 0.091413, loss: 1.7061
2022-07-06 22:43:51 - train: epoch 0023, iter [03700, 05004], lr: 0.091399, loss: 1.8286
2022-07-06 22:44:36 - train: epoch 0023, iter [03800, 05004], lr: 0.091384, loss: 1.9599
2022-07-06 22:45:21 - train: epoch 0023, iter [03900, 05004], lr: 0.091369, loss: 1.8437
2022-07-06 22:46:06 - train: epoch 0023, iter [04000, 05004], lr: 0.091354, loss: 1.8323
2022-07-06 22:46:52 - train: epoch 0023, iter [04100, 05004], lr: 0.091340, loss: 1.8259
2022-07-06 22:47:37 - train: epoch 0023, iter [04200, 05004], lr: 0.091325, loss: 1.7348
2022-07-06 22:48:22 - train: epoch 0023, iter [04300, 05004], lr: 0.091310, loss: 1.7629
2022-07-06 22:49:07 - train: epoch 0023, iter [04400, 05004], lr: 0.091296, loss: 1.8100
2022-07-06 22:49:52 - train: epoch 0023, iter [04500, 05004], lr: 0.091281, loss: 1.8069
2022-07-06 22:50:37 - train: epoch 0023, iter [04600, 05004], lr: 0.091266, loss: 2.0420
2022-07-06 22:51:21 - train: epoch 0023, iter [04700, 05004], lr: 0.091251, loss: 1.5948
2022-07-06 22:52:06 - train: epoch 0023, iter [04800, 05004], lr: 0.091237, loss: 1.8275
2022-07-06 22:52:51 - train: epoch 0023, iter [04900, 05004], lr: 0.091222, loss: 1.8709
2022-07-06 22:53:36 - train: epoch 0023, iter [05000, 05004], lr: 0.091207, loss: 1.9594
2022-07-06 22:53:38 - train: epoch 023, train_loss: 1.8743
2022-07-06 22:54:54 - eval: epoch: 023, acc1: 62.034%, acc5: 84.892%, test_loss: 1.5836, per_image_load_time: 1.009ms, per_image_inference_time: 0.815ms
2022-07-06 22:54:56 - until epoch: 023, best_acc1: 62.034%
2022-07-06 22:54:56 - epoch 024 lr: 0.091206
2022-07-06 22:55:46 - train: epoch 0024, iter [00100, 05004], lr: 0.091191, loss: 1.8408
2022-07-06 22:56:32 - train: epoch 0024, iter [00200, 05004], lr: 0.091177, loss: 1.8968
2022-07-06 22:57:17 - train: epoch 0024, iter [00300, 05004], lr: 0.091162, loss: 1.7662
2022-07-06 22:58:03 - train: epoch 0024, iter [00400, 05004], lr: 0.091147, loss: 1.9411
2022-07-06 22:58:49 - train: epoch 0024, iter [00500, 05004], lr: 0.091132, loss: 1.7357
2022-07-06 22:59:34 - train: epoch 0024, iter [00600, 05004], lr: 0.091117, loss: 1.7287
2022-07-06 23:00:19 - train: epoch 0024, iter [00700, 05004], lr: 0.091102, loss: 1.7703
2022-07-06 23:01:04 - train: epoch 0024, iter [00800, 05004], lr: 0.091087, loss: 1.8039
2022-07-06 23:01:50 - train: epoch 0024, iter [00900, 05004], lr: 0.091073, loss: 1.8371
2022-07-06 23:02:35 - train: epoch 0024, iter [01000, 05004], lr: 0.091058, loss: 1.8013
2022-07-06 23:03:20 - train: epoch 0024, iter [01100, 05004], lr: 0.091043, loss: 1.5506
2022-07-06 23:04:05 - train: epoch 0024, iter [01200, 05004], lr: 0.091028, loss: 1.6670
2022-07-06 23:04:51 - train: epoch 0024, iter [01300, 05004], lr: 0.091013, loss: 2.1406
2022-07-06 23:05:36 - train: epoch 0024, iter [01400, 05004], lr: 0.090998, loss: 1.8205
2022-07-06 23:06:22 - train: epoch 0024, iter [01500, 05004], lr: 0.090983, loss: 2.0506
2022-07-06 23:07:07 - train: epoch 0024, iter [01600, 05004], lr: 0.090968, loss: 1.9244
2022-07-06 23:07:52 - train: epoch 0024, iter [01700, 05004], lr: 0.090953, loss: 1.8410
2022-07-06 23:08:37 - train: epoch 0024, iter [01800, 05004], lr: 0.090938, loss: 1.9847
2022-07-06 23:09:22 - train: epoch 0024, iter [01900, 05004], lr: 0.090923, loss: 1.6418
2022-07-06 23:10:07 - train: epoch 0024, iter [02000, 05004], lr: 0.090908, loss: 1.8994
2022-07-06 23:10:52 - train: epoch 0024, iter [02100, 05004], lr: 0.090893, loss: 1.7946
2022-07-06 23:11:37 - train: epoch 0024, iter [02200, 05004], lr: 0.090878, loss: 1.7179
2022-07-06 23:12:22 - train: epoch 0024, iter [02300, 05004], lr: 0.090863, loss: 2.0187
2022-07-06 23:13:07 - train: epoch 0024, iter [02400, 05004], lr: 0.090847, loss: 1.8759
2022-07-06 23:13:52 - train: epoch 0024, iter [02500, 05004], lr: 0.090832, loss: 1.7732
2022-07-06 23:14:37 - train: epoch 0024, iter [02600, 05004], lr: 0.090817, loss: 1.8089
2022-07-06 23:15:22 - train: epoch 0024, iter [02700, 05004], lr: 0.090802, loss: 1.9661
2022-07-06 23:16:07 - train: epoch 0024, iter [02800, 05004], lr: 0.090787, loss: 1.9959
2022-07-06 23:16:52 - train: epoch 0024, iter [02900, 05004], lr: 0.090772, loss: 1.8649
2022-07-06 23:17:38 - train: epoch 0024, iter [03000, 05004], lr: 0.090757, loss: 1.7515
2022-07-06 23:18:23 - train: epoch 0024, iter [03100, 05004], lr: 0.090742, loss: 1.7269
2022-07-06 23:19:09 - train: epoch 0024, iter [03200, 05004], lr: 0.090726, loss: 2.0522
2022-07-06 23:19:54 - train: epoch 0024, iter [03300, 05004], lr: 0.090711, loss: 1.5245
2022-07-06 23:20:39 - train: epoch 0024, iter [03400, 05004], lr: 0.090696, loss: 1.8390
2022-07-06 23:21:25 - train: epoch 0024, iter [03500, 05004], lr: 0.090681, loss: 1.8317
2022-07-06 23:22:10 - train: epoch 0024, iter [03600, 05004], lr: 0.090666, loss: 1.9961
2022-07-06 23:22:56 - train: epoch 0024, iter [03700, 05004], lr: 0.090650, loss: 1.7902
2022-07-06 23:23:41 - train: epoch 0024, iter [03800, 05004], lr: 0.090635, loss: 2.0491
2022-07-06 23:24:26 - train: epoch 0024, iter [03900, 05004], lr: 0.090620, loss: 1.8232
2022-07-06 23:25:11 - train: epoch 0024, iter [04000, 05004], lr: 0.090605, loss: 1.8817
2022-07-06 23:25:56 - train: epoch 0024, iter [04100, 05004], lr: 0.090589, loss: 1.8405
2022-07-06 23:26:41 - train: epoch 0024, iter [04200, 05004], lr: 0.090574, loss: 1.8878
2022-07-06 23:27:26 - train: epoch 0024, iter [04300, 05004], lr: 0.090559, loss: 1.7924
2022-07-06 23:28:12 - train: epoch 0024, iter [04400, 05004], lr: 0.090544, loss: 1.9542
2022-07-06 23:28:57 - train: epoch 0024, iter [04500, 05004], lr: 0.090528, loss: 1.7137
2022-07-06 23:29:42 - train: epoch 0024, iter [04600, 05004], lr: 0.090513, loss: 1.8291
2022-07-06 23:30:27 - train: epoch 0024, iter [04700, 05004], lr: 0.090498, loss: 1.8080
2022-07-06 23:31:12 - train: epoch 0024, iter [04800, 05004], lr: 0.090482, loss: 1.7307
2022-07-06 23:31:57 - train: epoch 0024, iter [04900, 05004], lr: 0.090467, loss: 1.8268
2022-07-06 23:32:41 - train: epoch 0024, iter [05000, 05004], lr: 0.090451, loss: 1.9922
2022-07-06 23:32:44 - train: epoch 024, train_loss: 1.8637
2022-07-06 23:34:01 - eval: epoch: 024, acc1: 60.378%, acc5: 83.706%, test_loss: 1.6703, per_image_load_time: 1.470ms, per_image_inference_time: 0.837ms
2022-07-06 23:34:02 - until epoch: 024, best_acc1: 62.034%
2022-07-06 23:34:02 - epoch 025 lr: 0.090451
2022-07-06 23:34:54 - train: epoch 0025, iter [00100, 05004], lr: 0.090435, loss: 1.6658
2022-07-06 23:35:39 - train: epoch 0025, iter [00200, 05004], lr: 0.090420, loss: 1.7156
2022-07-06 23:36:24 - train: epoch 0025, iter [00300, 05004], lr: 0.090405, loss: 1.6925
2022-07-06 23:37:10 - train: epoch 0025, iter [00400, 05004], lr: 0.090389, loss: 1.7778
2022-07-06 23:37:55 - train: epoch 0025, iter [00500, 05004], lr: 0.090374, loss: 1.6978
2022-07-06 23:38:40 - train: epoch 0025, iter [00600, 05004], lr: 0.090358, loss: 1.8565
2022-07-06 23:39:25 - train: epoch 0025, iter [00700, 05004], lr: 0.090343, loss: 1.8606
2022-07-06 23:40:11 - train: epoch 0025, iter [00800, 05004], lr: 0.090327, loss: 1.8607
2022-07-06 23:40:56 - train: epoch 0025, iter [00900, 05004], lr: 0.090312, loss: 1.7793
2022-07-06 23:41:42 - train: epoch 0025, iter [01000, 05004], lr: 0.090297, loss: 1.7973
2022-07-06 23:42:27 - train: epoch 0025, iter [01100, 05004], lr: 0.090281, loss: 1.8527
2022-07-06 23:43:12 - train: epoch 0025, iter [01200, 05004], lr: 0.090266, loss: 1.8537
2022-07-06 23:43:58 - train: epoch 0025, iter [01300, 05004], lr: 0.090250, loss: 1.8471
2022-07-06 23:44:43 - train: epoch 0025, iter [01400, 05004], lr: 0.090235, loss: 1.9633
2022-07-06 23:45:28 - train: epoch 0025, iter [01500, 05004], lr: 0.090219, loss: 1.8644
2022-07-06 23:46:14 - train: epoch 0025, iter [01600, 05004], lr: 0.090203, loss: 1.5474
2022-07-06 23:46:59 - train: epoch 0025, iter [01700, 05004], lr: 0.090188, loss: 1.7896
2022-07-06 23:47:45 - train: epoch 0025, iter [01800, 05004], lr: 0.090172, loss: 1.6070
2022-07-06 23:48:30 - train: epoch 0025, iter [01900, 05004], lr: 0.090157, loss: 1.7756
2022-07-06 23:49:15 - train: epoch 0025, iter [02000, 05004], lr: 0.090141, loss: 1.8839
2022-07-06 23:50:01 - train: epoch 0025, iter [02100, 05004], lr: 0.090126, loss: 1.8057
2022-07-06 23:50:46 - train: epoch 0025, iter [02200, 05004], lr: 0.090110, loss: 1.7364
2022-07-06 23:51:31 - train: epoch 0025, iter [02300, 05004], lr: 0.090094, loss: 1.8624
2022-07-06 23:52:16 - train: epoch 0025, iter [02400, 05004], lr: 0.090079, loss: 1.6398
2022-07-06 23:53:02 - train: epoch 0025, iter [02500, 05004], lr: 0.090063, loss: 1.9394
2022-07-06 23:53:47 - train: epoch 0025, iter [02600, 05004], lr: 0.090047, loss: 2.0019
2022-07-06 23:54:33 - train: epoch 0025, iter [02700, 05004], lr: 0.090032, loss: 1.8575
2022-07-06 23:55:18 - train: epoch 0025, iter [02800, 05004], lr: 0.090016, loss: 1.8076
2022-07-06 23:56:03 - train: epoch 0025, iter [02900, 05004], lr: 0.090000, loss: 2.1020
2022-07-06 23:56:48 - train: epoch 0025, iter [03000, 05004], lr: 0.089985, loss: 2.0197
2022-07-06 23:57:34 - train: epoch 0025, iter [03100, 05004], lr: 0.089969, loss: 1.9197
2022-07-06 23:58:19 - train: epoch 0025, iter [03200, 05004], lr: 0.089953, loss: 2.0389
2022-07-06 23:59:04 - train: epoch 0025, iter [03300, 05004], lr: 0.089937, loss: 1.7114
2022-07-06 23:59:50 - train: epoch 0025, iter [03400, 05004], lr: 0.089922, loss: 2.0055
2022-07-07 00:00:35 - train: epoch 0025, iter [03500, 05004], lr: 0.089906, loss: 1.5901
2022-07-07 00:01:20 - train: epoch 0025, iter [03600, 05004], lr: 0.089890, loss: 1.9973
2022-07-07 00:02:05 - train: epoch 0025, iter [03700, 05004], lr: 0.089874, loss: 1.8224
2022-07-07 00:02:50 - train: epoch 0025, iter [03800, 05004], lr: 0.089859, loss: 1.9607
2022-07-07 00:03:36 - train: epoch 0025, iter [03900, 05004], lr: 0.089843, loss: 2.0789
2022-07-07 00:04:21 - train: epoch 0025, iter [04000, 05004], lr: 0.089827, loss: 1.9370
2022-07-07 00:05:06 - train: epoch 0025, iter [04100, 05004], lr: 0.089811, loss: 2.1087
2022-07-07 00:05:51 - train: epoch 0025, iter [04200, 05004], lr: 0.089795, loss: 1.8188
2022-07-07 00:06:37 - train: epoch 0025, iter [04300, 05004], lr: 0.089780, loss: 1.7743
2022-07-07 00:07:22 - train: epoch 0025, iter [04400, 05004], lr: 0.089764, loss: 1.7569
2022-07-07 00:08:07 - train: epoch 0025, iter [04500, 05004], lr: 0.089748, loss: 1.7771
2022-07-07 00:08:52 - train: epoch 0025, iter [04600, 05004], lr: 0.089732, loss: 1.8547
2022-07-07 00:09:38 - train: epoch 0025, iter [04700, 05004], lr: 0.089716, loss: 1.7590
2022-07-07 00:10:23 - train: epoch 0025, iter [04800, 05004], lr: 0.089700, loss: 1.7041
2022-07-07 00:11:08 - train: epoch 0025, iter [04900, 05004], lr: 0.089684, loss: 1.8479
2022-07-07 00:11:53 - train: epoch 0025, iter [05000, 05004], lr: 0.089668, loss: 1.9925
2022-07-07 00:11:56 - train: epoch 025, train_loss: 1.8527
2022-07-07 00:13:12 - eval: epoch: 025, acc1: 62.186%, acc5: 85.180%, test_loss: 1.5653, per_image_load_time: 2.136ms, per_image_inference_time: 0.825ms
2022-07-07 00:13:13 - until epoch: 025, best_acc1: 62.186%
2022-07-07 00:13:13 - epoch 026 lr: 0.089668
2022-07-07 00:14:04 - train: epoch 0026, iter [00100, 05004], lr: 0.089652, loss: 1.6220
2022-07-07 00:14:50 - train: epoch 0026, iter [00200, 05004], lr: 0.089636, loss: 1.7047
2022-07-07 00:15:35 - train: epoch 0026, iter [00300, 05004], lr: 0.089620, loss: 1.6783
2022-07-07 00:16:21 - train: epoch 0026, iter [00400, 05004], lr: 0.089604, loss: 1.8662
2022-07-07 00:17:06 - train: epoch 0026, iter [00500, 05004], lr: 0.089588, loss: 1.8506
2022-07-07 00:17:51 - train: epoch 0026, iter [00600, 05004], lr: 0.089572, loss: 1.9559
2022-07-07 00:18:36 - train: epoch 0026, iter [00700, 05004], lr: 0.089556, loss: 1.6507
2022-07-07 00:19:21 - train: epoch 0026, iter [00800, 05004], lr: 0.089540, loss: 1.6973
2022-07-07 00:20:06 - train: epoch 0026, iter [00900, 05004], lr: 0.089524, loss: 2.0591
2022-07-07 00:20:51 - train: epoch 0026, iter [01000, 05004], lr: 0.089508, loss: 1.6663
2022-07-07 00:21:36 - train: epoch 0026, iter [01100, 05004], lr: 0.089492, loss: 1.7620
2022-07-07 00:22:21 - train: epoch 0026, iter [01200, 05004], lr: 0.089476, loss: 1.8346
2022-07-07 00:23:06 - train: epoch 0026, iter [01300, 05004], lr: 0.089460, loss: 1.6744
2022-07-07 00:23:51 - train: epoch 0026, iter [01400, 05004], lr: 0.089444, loss: 1.8653
2022-07-07 00:24:36 - train: epoch 0026, iter [01500, 05004], lr: 0.089428, loss: 1.7967
2022-07-07 00:25:21 - train: epoch 0026, iter [01600, 05004], lr: 0.089411, loss: 1.9066
2022-07-07 00:26:07 - train: epoch 0026, iter [01700, 05004], lr: 0.089395, loss: 1.7771
2022-07-07 00:26:51 - train: epoch 0026, iter [01800, 05004], lr: 0.089379, loss: 1.8878
2022-07-07 00:27:37 - train: epoch 0026, iter [01900, 05004], lr: 0.089363, loss: 2.0722
2022-07-07 00:28:22 - train: epoch 0026, iter [02000, 05004], lr: 0.089347, loss: 1.8852
2022-07-07 00:29:07 - train: epoch 0026, iter [02100, 05004], lr: 0.089331, loss: 1.8635
2022-07-07 00:29:52 - train: epoch 0026, iter [02200, 05004], lr: 0.089315, loss: 1.7292
2022-07-07 00:30:37 - train: epoch 0026, iter [02300, 05004], lr: 0.089299, loss: 1.7532
2022-07-07 00:31:22 - train: epoch 0026, iter [02400, 05004], lr: 0.089282, loss: 2.0580
2022-07-07 00:32:07 - train: epoch 0026, iter [02500, 05004], lr: 0.089266, loss: 1.9770
2022-07-07 00:32:52 - train: epoch 0026, iter [02600, 05004], lr: 0.089250, loss: 1.8556
2022-07-07 00:33:37 - train: epoch 0026, iter [02700, 05004], lr: 0.089234, loss: 1.7239
2022-07-07 00:34:22 - train: epoch 0026, iter [02800, 05004], lr: 0.089218, loss: 1.7471
2022-07-07 00:35:07 - train: epoch 0026, iter [02900, 05004], lr: 0.089201, loss: 1.9583
2022-07-07 00:35:52 - train: epoch 0026, iter [03000, 05004], lr: 0.089185, loss: 1.7866
2022-07-07 00:36:37 - train: epoch 0026, iter [03100, 05004], lr: 0.089169, loss: 1.8616
2022-07-07 00:37:22 - train: epoch 0026, iter [03200, 05004], lr: 0.089153, loss: 1.8140
2022-07-07 00:38:08 - train: epoch 0026, iter [03300, 05004], lr: 0.089136, loss: 1.7996
2022-07-07 00:38:53 - train: epoch 0026, iter [03400, 05004], lr: 0.089120, loss: 2.0171
2022-07-07 00:39:38 - train: epoch 0026, iter [03500, 05004], lr: 0.089104, loss: 2.0310
2022-07-07 00:40:23 - train: epoch 0026, iter [03600, 05004], lr: 0.089087, loss: 1.7969
2022-07-07 00:41:08 - train: epoch 0026, iter [03700, 05004], lr: 0.089071, loss: 1.8738
2022-07-07 00:41:53 - train: epoch 0026, iter [03800, 05004], lr: 0.089055, loss: 1.9646
2022-07-07 00:42:39 - train: epoch 0026, iter [03900, 05004], lr: 0.089038, loss: 2.0502
2022-07-07 00:43:24 - train: epoch 0026, iter [04000, 05004], lr: 0.089022, loss: 2.0237
2022-07-07 00:44:08 - train: epoch 0026, iter [04100, 05004], lr: 0.089006, loss: 1.8910
2022-07-07 00:44:53 - train: epoch 0026, iter [04200, 05004], lr: 0.088989, loss: 2.0318
2022-07-07 00:45:38 - train: epoch 0026, iter [04300, 05004], lr: 0.088973, loss: 1.9677
2022-07-07 00:46:23 - train: epoch 0026, iter [04400, 05004], lr: 0.088957, loss: 1.9857
2022-07-07 00:47:08 - train: epoch 0026, iter [04500, 05004], lr: 0.088940, loss: 2.1500
2022-07-07 00:47:53 - train: epoch 0026, iter [04600, 05004], lr: 0.088924, loss: 1.7551
2022-07-07 00:48:38 - train: epoch 0026, iter [04700, 05004], lr: 0.088907, loss: 1.7382
2022-07-07 00:49:24 - train: epoch 0026, iter [04800, 05004], lr: 0.088891, loss: 1.8096
2022-07-07 00:50:08 - train: epoch 0026, iter [04900, 05004], lr: 0.088874, loss: 1.8391
2022-07-07 00:50:54 - train: epoch 0026, iter [05000, 05004], lr: 0.088858, loss: 2.0604
2022-07-07 00:50:56 - train: epoch 026, train_loss: 1.8441
2022-07-07 00:52:12 - eval: epoch: 026, acc1: 62.282%, acc5: 85.254%, test_loss: 1.5679, per_image_load_time: 1.378ms, per_image_inference_time: 0.834ms
2022-07-07 00:52:14 - until epoch: 026, best_acc1: 62.282%
2022-07-07 00:52:14 - epoch 027 lr: 0.088857
2022-07-07 00:53:05 - train: epoch 0027, iter [00100, 05004], lr: 0.088841, loss: 1.9605
2022-07-07 00:53:50 - train: epoch 0027, iter [00200, 05004], lr: 0.088824, loss: 1.8201
2022-07-07 00:54:35 - train: epoch 0027, iter [00300, 05004], lr: 0.088808, loss: 1.7829
2022-07-07 00:55:21 - train: epoch 0027, iter [00400, 05004], lr: 0.088791, loss: 1.9881
2022-07-07 00:56:06 - train: epoch 0027, iter [00500, 05004], lr: 0.088775, loss: 1.8952
2022-07-07 00:56:51 - train: epoch 0027, iter [00600, 05004], lr: 0.088758, loss: 2.1219
2022-07-07 00:57:36 - train: epoch 0027, iter [00700, 05004], lr: 0.088742, loss: 1.9971
2022-07-07 00:58:21 - train: epoch 0027, iter [00800, 05004], lr: 0.088725, loss: 1.8399
2022-07-07 00:59:07 - train: epoch 0027, iter [00900, 05004], lr: 0.088709, loss: 1.7463
2022-07-07 00:59:52 - train: epoch 0027, iter [01000, 05004], lr: 0.088692, loss: 1.7749
2022-07-07 01:00:37 - train: epoch 0027, iter [01100, 05004], lr: 0.088676, loss: 1.7881
2022-07-07 01:01:22 - train: epoch 0027, iter [01200, 05004], lr: 0.088659, loss: 2.0529
2022-07-07 01:02:07 - train: epoch 0027, iter [01300, 05004], lr: 0.088642, loss: 1.7900
2022-07-07 01:02:53 - train: epoch 0027, iter [01400, 05004], lr: 0.088626, loss: 1.9727
2022-07-07 01:03:38 - train: epoch 0027, iter [01500, 05004], lr: 0.088609, loss: 1.8990
2022-07-07 01:04:23 - train: epoch 0027, iter [01600, 05004], lr: 0.088593, loss: 1.9603
2022-07-07 01:05:08 - train: epoch 0027, iter [01700, 05004], lr: 0.088576, loss: 1.8526
2022-07-07 01:05:53 - train: epoch 0027, iter [01800, 05004], lr: 0.088559, loss: 1.8671
2022-07-07 01:06:39 - train: epoch 0027, iter [01900, 05004], lr: 0.088543, loss: 1.8959
2022-07-07 01:07:24 - train: epoch 0027, iter [02000, 05004], lr: 0.088526, loss: 1.7842
2022-07-07 01:08:09 - train: epoch 0027, iter [02100, 05004], lr: 0.088509, loss: 2.1032
2022-07-07 01:08:54 - train: epoch 0027, iter [02200, 05004], lr: 0.088493, loss: 2.0236
2022-07-07 01:09:40 - train: epoch 0027, iter [02300, 05004], lr: 0.088476, loss: 2.0766
2022-07-07 01:10:25 - train: epoch 0027, iter [02400, 05004], lr: 0.088459, loss: 1.8346
2022-07-07 01:11:10 - train: epoch 0027, iter [02500, 05004], lr: 0.088442, loss: 1.6427
2022-07-07 01:11:56 - train: epoch 0027, iter [02600, 05004], lr: 0.088426, loss: 2.0472
2022-07-07 01:12:41 - train: epoch 0027, iter [02700, 05004], lr: 0.088409, loss: 1.8180
2022-07-07 01:13:26 - train: epoch 0027, iter [02800, 05004], lr: 0.088392, loss: 2.0067
2022-07-07 01:14:11 - train: epoch 0027, iter [02900, 05004], lr: 0.088375, loss: 1.8572
2022-07-07 01:14:56 - train: epoch 0027, iter [03000, 05004], lr: 0.088359, loss: 1.6986
2022-07-07 01:15:42 - train: epoch 0027, iter [03100, 05004], lr: 0.088342, loss: 1.6261
2022-07-07 01:16:27 - train: epoch 0027, iter [03200, 05004], lr: 0.088325, loss: 1.6519
2022-07-07 01:17:12 - train: epoch 0027, iter [03300, 05004], lr: 0.088308, loss: 1.9256
2022-07-07 01:17:57 - train: epoch 0027, iter [03400, 05004], lr: 0.088291, loss: 1.8347
2022-07-07 01:18:43 - train: epoch 0027, iter [03500, 05004], lr: 0.088275, loss: 2.0579
2022-07-07 01:19:28 - train: epoch 0027, iter [03600, 05004], lr: 0.088258, loss: 1.7332
2022-07-07 01:20:13 - train: epoch 0027, iter [03700, 05004], lr: 0.088241, loss: 1.7600
2022-07-07 01:20:59 - train: epoch 0027, iter [03800, 05004], lr: 0.088224, loss: 1.6806
2022-07-07 01:21:44 - train: epoch 0027, iter [03900, 05004], lr: 0.088207, loss: 1.6736
2022-07-07 01:22:29 - train: epoch 0027, iter [04000, 05004], lr: 0.088190, loss: 1.8203
2022-07-07 01:23:15 - train: epoch 0027, iter [04100, 05004], lr: 0.088173, loss: 1.9026
2022-07-07 01:24:00 - train: epoch 0027, iter [04200, 05004], lr: 0.088157, loss: 1.8404
2022-07-07 01:24:45 - train: epoch 0027, iter [04300, 05004], lr: 0.088140, loss: 1.8235
2022-07-07 01:25:30 - train: epoch 0027, iter [04400, 05004], lr: 0.088123, loss: 1.8368
2022-07-07 01:26:16 - train: epoch 0027, iter [04500, 05004], lr: 0.088106, loss: 1.6850
2022-07-07 01:27:01 - train: epoch 0027, iter [04600, 05004], lr: 0.088089, loss: 1.8373
2022-07-07 01:27:46 - train: epoch 0027, iter [04700, 05004], lr: 0.088072, loss: 1.8603
2022-07-07 01:28:32 - train: epoch 0027, iter [04800, 05004], lr: 0.088055, loss: 1.9787
2022-07-07 01:29:17 - train: epoch 0027, iter [04900, 05004], lr: 0.088038, loss: 1.9169
2022-07-07 01:30:02 - train: epoch 0027, iter [05000, 05004], lr: 0.088021, loss: 1.5345
2022-07-07 01:30:04 - train: epoch 027, train_loss: 1.8328
2022-07-07 01:31:20 - eval: epoch: 027, acc1: 62.510%, acc5: 85.342%, test_loss: 1.5559, per_image_load_time: 1.781ms, per_image_inference_time: 0.835ms
2022-07-07 01:31:22 - until epoch: 027, best_acc1: 62.510%
2022-07-07 01:31:22 - epoch 028 lr: 0.088020
2022-07-07 01:32:13 - train: epoch 0028, iter [00100, 05004], lr: 0.088003, loss: 1.5449
2022-07-07 01:32:57 - train: epoch 0028, iter [00200, 05004], lr: 0.087986, loss: 1.5861
2022-07-07 01:33:42 - train: epoch 0028, iter [00300, 05004], lr: 0.087969, loss: 1.8235
2022-07-07 01:34:27 - train: epoch 0028, iter [00400, 05004], lr: 0.087952, loss: 1.7180
2022-07-07 01:35:13 - train: epoch 0028, iter [00500, 05004], lr: 0.087935, loss: 1.6992
2022-07-07 01:35:58 - train: epoch 0028, iter [00600, 05004], lr: 0.087918, loss: 1.9087
2022-07-07 01:36:43 - train: epoch 0028, iter [00700, 05004], lr: 0.087901, loss: 1.9659
2022-07-07 01:37:28 - train: epoch 0028, iter [00800, 05004], lr: 0.087884, loss: 1.4109
2022-07-07 01:38:13 - train: epoch 0028, iter [00900, 05004], lr: 0.087867, loss: 1.5562
2022-07-07 01:38:59 - train: epoch 0028, iter [01000, 05004], lr: 0.087850, loss: 1.9012
2022-07-07 01:39:44 - train: epoch 0028, iter [01100, 05004], lr: 0.087833, loss: 1.8837
2022-07-07 01:40:30 - train: epoch 0028, iter [01200, 05004], lr: 0.087816, loss: 1.7008
2022-07-07 01:41:15 - train: epoch 0028, iter [01300, 05004], lr: 0.087799, loss: 1.7541
2022-07-07 01:42:01 - train: epoch 0028, iter [01400, 05004], lr: 0.087781, loss: 2.0597
2022-07-07 01:42:47 - train: epoch 0028, iter [01500, 05004], lr: 0.087764, loss: 1.7383
2022-07-07 01:43:32 - train: epoch 0028, iter [01600, 05004], lr: 0.087747, loss: 1.8762
2022-07-07 01:44:18 - train: epoch 0028, iter [01700, 05004], lr: 0.087730, loss: 1.6978
2022-07-07 01:45:04 - train: epoch 0028, iter [01800, 05004], lr: 0.087713, loss: 1.7383
2022-07-07 01:45:49 - train: epoch 0028, iter [01900, 05004], lr: 0.087696, loss: 1.7909
2022-07-07 01:46:35 - train: epoch 0028, iter [02000, 05004], lr: 0.087678, loss: 2.0316
2022-07-07 01:47:21 - train: epoch 0028, iter [02100, 05004], lr: 0.087661, loss: 1.8812
2022-07-07 01:48:07 - train: epoch 0028, iter [02200, 05004], lr: 0.087644, loss: 1.8689
2022-07-07 01:48:53 - train: epoch 0028, iter [02300, 05004], lr: 0.087627, loss: 1.9843
2022-07-07 01:49:38 - train: epoch 0028, iter [02400, 05004], lr: 0.087610, loss: 1.9790
2022-07-07 01:50:24 - train: epoch 0028, iter [02500, 05004], lr: 0.087592, loss: 1.8329
2022-07-07 01:51:10 - train: epoch 0028, iter [02600, 05004], lr: 0.087575, loss: 1.6909
2022-07-07 01:51:55 - train: epoch 0028, iter [02700, 05004], lr: 0.087558, loss: 1.8431
2022-07-07 01:52:41 - train: epoch 0028, iter [02800, 05004], lr: 0.087541, loss: 1.7933
2022-07-07 01:53:27 - train: epoch 0028, iter [02900, 05004], lr: 0.087523, loss: 1.9688
2022-07-07 01:54:13 - train: epoch 0028, iter [03000, 05004], lr: 0.087506, loss: 1.9863
2022-07-07 01:54:58 - train: epoch 0028, iter [03100, 05004], lr: 0.087489, loss: 2.0621
2022-07-07 01:55:44 - train: epoch 0028, iter [03200, 05004], lr: 0.087471, loss: 1.8139
2022-07-07 01:56:30 - train: epoch 0028, iter [03300, 05004], lr: 0.087454, loss: 1.8034
2022-07-07 01:57:15 - train: epoch 0028, iter [03400, 05004], lr: 0.087437, loss: 1.7602
2022-07-07 01:58:01 - train: epoch 0028, iter [03500, 05004], lr: 0.087419, loss: 1.8593
2022-07-07 01:58:47 - train: epoch 0028, iter [03600, 05004], lr: 0.087402, loss: 1.6894
2022-07-07 01:59:33 - train: epoch 0028, iter [03700, 05004], lr: 0.087385, loss: 1.7474
2022-07-07 02:00:18 - train: epoch 0028, iter [03800, 05004], lr: 0.087367, loss: 1.5068
2022-07-07 02:01:04 - train: epoch 0028, iter [03900, 05004], lr: 0.087350, loss: 1.7885
2022-07-07 02:01:50 - train: epoch 0028, iter [04000, 05004], lr: 0.087332, loss: 1.9720
2022-07-07 02:02:35 - train: epoch 0028, iter [04100, 05004], lr: 0.087315, loss: 1.9159
2022-07-07 02:03:21 - train: epoch 0028, iter [04200, 05004], lr: 0.087298, loss: 1.9933
2022-07-07 02:04:07 - train: epoch 0028, iter [04300, 05004], lr: 0.087280, loss: 1.7535
2022-07-07 02:04:52 - train: epoch 0028, iter [04400, 05004], lr: 0.087263, loss: 1.7635
2022-07-07 02:05:38 - train: epoch 0028, iter [04500, 05004], lr: 0.087245, loss: 1.7563
2022-07-07 02:06:23 - train: epoch 0028, iter [04600, 05004], lr: 0.087228, loss: 1.9095
2022-07-07 02:07:09 - train: epoch 0028, iter [04700, 05004], lr: 0.087210, loss: 1.8236
2022-07-07 02:07:55 - train: epoch 0028, iter [04800, 05004], lr: 0.087193, loss: 1.7188
2022-07-07 02:08:41 - train: epoch 0028, iter [04900, 05004], lr: 0.087175, loss: 1.6883
2022-07-07 02:09:27 - train: epoch 0028, iter [05000, 05004], lr: 0.087158, loss: 1.7757
2022-07-07 02:09:29 - train: epoch 028, train_loss: 1.8221
2022-07-07 02:10:45 - eval: epoch: 028, acc1: 63.088%, acc5: 85.634%, test_loss: 1.5395, per_image_load_time: 2.087ms, per_image_inference_time: 0.831ms
2022-07-07 02:10:46 - until epoch: 028, best_acc1: 63.088%
2022-07-07 02:10:46 - epoch 029 lr: 0.087157
2022-07-07 02:11:37 - train: epoch 0029, iter [00100, 05004], lr: 0.087140, loss: 1.8384
2022-07-07 02:12:22 - train: epoch 0029, iter [00200, 05004], lr: 0.087122, loss: 1.7479
2022-07-07 02:13:08 - train: epoch 0029, iter [00300, 05004], lr: 0.087105, loss: 2.0167
2022-07-07 02:13:54 - train: epoch 0029, iter [00400, 05004], lr: 0.087087, loss: 1.7683
2022-07-07 02:14:39 - train: epoch 0029, iter [00500, 05004], lr: 0.087070, loss: 1.7568
2022-07-07 02:15:25 - train: epoch 0029, iter [00600, 05004], lr: 0.087052, loss: 1.9893
2022-07-07 02:16:10 - train: epoch 0029, iter [00700, 05004], lr: 0.087034, loss: 1.3749
2022-07-07 02:16:56 - train: epoch 0029, iter [00800, 05004], lr: 0.087017, loss: 1.9883
2022-07-07 02:17:42 - train: epoch 0029, iter [00900, 05004], lr: 0.086999, loss: 1.6592
2022-07-07 02:18:27 - train: epoch 0029, iter [01000, 05004], lr: 0.086982, loss: 1.5763
2022-07-07 02:19:12 - train: epoch 0029, iter [01100, 05004], lr: 0.086964, loss: 1.7445
2022-07-07 02:19:58 - train: epoch 0029, iter [01200, 05004], lr: 0.086946, loss: 1.8505
2022-07-07 02:20:44 - train: epoch 0029, iter [01300, 05004], lr: 0.086929, loss: 1.7564
2022-07-07 02:21:30 - train: epoch 0029, iter [01400, 05004], lr: 0.086911, loss: 2.0356
2022-07-07 02:22:15 - train: epoch 0029, iter [01500, 05004], lr: 0.086894, loss: 1.9243
2022-07-07 02:23:01 - train: epoch 0029, iter [01600, 05004], lr: 0.086876, loss: 1.7057
2022-07-07 02:23:46 - train: epoch 0029, iter [01700, 05004], lr: 0.086858, loss: 1.8497
2022-07-07 02:24:32 - train: epoch 0029, iter [01800, 05004], lr: 0.086841, loss: 1.9245
2022-07-07 02:25:18 - train: epoch 0029, iter [01900, 05004], lr: 0.086823, loss: 1.6134
2022-07-07 02:26:04 - train: epoch 0029, iter [02000, 05004], lr: 0.086805, loss: 1.9889
2022-07-07 02:26:49 - train: epoch 0029, iter [02100, 05004], lr: 0.086787, loss: 1.7615
2022-07-07 02:27:35 - train: epoch 0029, iter [02200, 05004], lr: 0.086770, loss: 1.9168
2022-07-07 02:28:21 - train: epoch 0029, iter [02300, 05004], lr: 0.086752, loss: 1.7767
2022-07-07 02:29:07 - train: epoch 0029, iter [02400, 05004], lr: 0.086734, loss: 1.7446
2022-07-07 02:29:52 - train: epoch 0029, iter [02500, 05004], lr: 0.086716, loss: 1.6869
2022-07-07 02:30:38 - train: epoch 0029, iter [02600, 05004], lr: 0.086699, loss: 1.8260
2022-07-07 02:31:24 - train: epoch 0029, iter [02700, 05004], lr: 0.086681, loss: 1.8219
2022-07-07 02:32:10 - train: epoch 0029, iter [02800, 05004], lr: 0.086663, loss: 1.7150
2022-07-07 02:32:56 - train: epoch 0029, iter [02900, 05004], lr: 0.086645, loss: 1.7773
2022-07-07 02:33:42 - train: epoch 0029, iter [03000, 05004], lr: 0.086628, loss: 1.7753
2022-07-07 02:34:28 - train: epoch 0029, iter [03100, 05004], lr: 0.086610, loss: 1.7308
2022-07-07 02:35:14 - train: epoch 0029, iter [03200, 05004], lr: 0.086592, loss: 1.8838
2022-07-07 02:36:00 - train: epoch 0029, iter [03300, 05004], lr: 0.086574, loss: 1.7931
2022-07-07 02:36:46 - train: epoch 0029, iter [03400, 05004], lr: 0.086556, loss: 1.6500
2022-07-07 02:37:31 - train: epoch 0029, iter [03500, 05004], lr: 0.086538, loss: 1.8808
2022-07-07 02:38:17 - train: epoch 0029, iter [03600, 05004], lr: 0.086521, loss: 1.8449
2022-07-07 02:39:03 - train: epoch 0029, iter [03700, 05004], lr: 0.086503, loss: 1.7239
2022-07-07 02:39:49 - train: epoch 0029, iter [03800, 05004], lr: 0.086485, loss: 1.7814
2022-07-07 02:40:35 - train: epoch 0029, iter [03900, 05004], lr: 0.086467, loss: 1.7307
2022-07-07 02:41:21 - train: epoch 0029, iter [04000, 05004], lr: 0.086449, loss: 1.7605
2022-07-07 02:42:06 - train: epoch 0029, iter [04100, 05004], lr: 0.086431, loss: 1.5688
2022-07-07 02:42:52 - train: epoch 0029, iter [04200, 05004], lr: 0.086413, loss: 1.6523
2022-07-07 02:43:38 - train: epoch 0029, iter [04300, 05004], lr: 0.086395, loss: 1.9454
2022-07-07 02:44:23 - train: epoch 0029, iter [04400, 05004], lr: 0.086377, loss: 1.7047
2022-07-07 02:45:09 - train: epoch 0029, iter [04500, 05004], lr: 0.086359, loss: 2.0451
2022-07-07 02:45:55 - train: epoch 0029, iter [04600, 05004], lr: 0.086341, loss: 1.9811
2022-07-07 02:46:41 - train: epoch 0029, iter [04700, 05004], lr: 0.086323, loss: 1.5931
2022-07-07 02:47:27 - train: epoch 0029, iter [04800, 05004], lr: 0.086305, loss: 1.8479
2022-07-07 02:48:12 - train: epoch 0029, iter [04900, 05004], lr: 0.086287, loss: 2.1462
2022-07-07 02:48:58 - train: epoch 0029, iter [05000, 05004], lr: 0.086269, loss: 1.5400
2022-07-07 02:49:01 - train: epoch 029, train_loss: 1.8146
2022-07-07 02:50:16 - eval: epoch: 029, acc1: 62.490%, acc5: 85.464%, test_loss: 1.5465, per_image_load_time: 2.081ms, per_image_inference_time: 0.837ms
2022-07-07 02:50:17 - until epoch: 029, best_acc1: 63.088%
2022-07-07 02:50:17 - epoch 030 lr: 0.086269
2022-07-07 02:51:08 - train: epoch 0030, iter [00100, 05004], lr: 0.086251, loss: 1.8950
2022-07-07 02:51:54 - train: epoch 0030, iter [00200, 05004], lr: 0.086233, loss: 1.8779
2022-07-07 02:52:39 - train: epoch 0030, iter [00300, 05004], lr: 0.086215, loss: 1.6842
2022-07-07 02:53:25 - train: epoch 0030, iter [00400, 05004], lr: 0.086197, loss: 1.4793
2022-07-07 02:54:11 - train: epoch 0030, iter [00500, 05004], lr: 0.086179, loss: 2.0140
2022-07-07 02:54:57 - train: epoch 0030, iter [00600, 05004], lr: 0.086160, loss: 1.6227
2022-07-07 02:55:42 - train: epoch 0030, iter [00700, 05004], lr: 0.086142, loss: 1.7239
2022-07-07 02:56:28 - train: epoch 0030, iter [00800, 05004], lr: 0.086124, loss: 2.0311
2022-07-07 02:57:14 - train: epoch 0030, iter [00900, 05004], lr: 0.086106, loss: 1.8361
2022-07-07 02:58:00 - train: epoch 0030, iter [01000, 05004], lr: 0.086088, loss: 1.5844
2022-07-07 02:58:45 - train: epoch 0030, iter [01100, 05004], lr: 0.086070, loss: 1.7049
2022-07-07 02:59:31 - train: epoch 0030, iter [01200, 05004], lr: 0.086052, loss: 1.8302
2022-07-07 03:00:17 - train: epoch 0030, iter [01300, 05004], lr: 0.086034, loss: 1.6832
2022-07-07 03:01:02 - train: epoch 0030, iter [01400, 05004], lr: 0.086016, loss: 1.7194
2022-07-07 03:01:48 - train: epoch 0030, iter [01500, 05004], lr: 0.085998, loss: 1.7887
2022-07-07 03:02:34 - train: epoch 0030, iter [01600, 05004], lr: 0.085979, loss: 1.6859
2022-07-07 03:03:20 - train: epoch 0030, iter [01700, 05004], lr: 0.085961, loss: 2.0983
2022-07-07 03:04:05 - train: epoch 0030, iter [01800, 05004], lr: 0.085943, loss: 1.8419
2022-07-07 03:04:51 - train: epoch 0030, iter [01900, 05004], lr: 0.085925, loss: 1.9272
2022-07-07 03:05:37 - train: epoch 0030, iter [02000, 05004], lr: 0.085907, loss: 1.7938
2022-07-07 03:06:22 - train: epoch 0030, iter [02100, 05004], lr: 0.085888, loss: 1.9197
2022-07-07 03:07:08 - train: epoch 0030, iter [02200, 05004], lr: 0.085870, loss: 1.8057
2022-07-07 03:07:54 - train: epoch 0030, iter [02300, 05004], lr: 0.085852, loss: 1.7578
2022-07-07 03:08:39 - train: epoch 0030, iter [02400, 05004], lr: 0.085834, loss: 2.0155
2022-07-07 03:09:25 - train: epoch 0030, iter [02500, 05004], lr: 0.085815, loss: 1.8216
2022-07-07 03:10:10 - train: epoch 0030, iter [02600, 05004], lr: 0.085797, loss: 1.8117
2022-07-07 03:10:56 - train: epoch 0030, iter [02700, 05004], lr: 0.085779, loss: 1.7506
2022-07-07 03:11:42 - train: epoch 0030, iter [02800, 05004], lr: 0.085761, loss: 1.8108
2022-07-07 03:12:28 - train: epoch 0030, iter [02900, 05004], lr: 0.085742, loss: 1.8956
2022-07-07 03:13:13 - train: epoch 0030, iter [03000, 05004], lr: 0.085724, loss: 1.9871
2022-07-07 03:13:59 - train: epoch 0030, iter [03100, 05004], lr: 0.085706, loss: 1.6297
2022-07-07 03:14:45 - train: epoch 0030, iter [03200, 05004], lr: 0.085687, loss: 1.7366
2022-07-07 03:15:31 - train: epoch 0030, iter [03300, 05004], lr: 0.085669, loss: 2.0004
2022-07-07 03:16:17 - train: epoch 0030, iter [03400, 05004], lr: 0.085651, loss: 1.8908
2022-07-07 03:17:02 - train: epoch 0030, iter [03500, 05004], lr: 0.085632, loss: 1.9325
2022-07-07 03:17:48 - train: epoch 0030, iter [03600, 05004], lr: 0.085614, loss: 1.6829
2022-07-07 03:18:34 - train: epoch 0030, iter [03700, 05004], lr: 0.085596, loss: 1.8460
2022-07-07 03:19:19 - train: epoch 0030, iter [03800, 05004], lr: 0.085577, loss: 1.9177
2022-07-07 03:20:05 - train: epoch 0030, iter [03900, 05004], lr: 0.085559, loss: 1.7909
2022-07-07 03:20:51 - train: epoch 0030, iter [04000, 05004], lr: 0.085541, loss: 1.6757
2022-07-07 03:21:36 - train: epoch 0030, iter [04100, 05004], lr: 0.085522, loss: 1.7419
2022-07-07 03:22:22 - train: epoch 0030, iter [04200, 05004], lr: 0.085504, loss: 2.0718
2022-07-07 03:23:08 - train: epoch 0030, iter [04300, 05004], lr: 0.085485, loss: 1.8088
2022-07-07 03:23:53 - train: epoch 0030, iter [04400, 05004], lr: 0.085467, loss: 1.8172
2022-07-07 03:24:39 - train: epoch 0030, iter [04500, 05004], lr: 0.085448, loss: 2.0243
2022-07-07 03:25:25 - train: epoch 0030, iter [04600, 05004], lr: 0.085430, loss: 1.5631
2022-07-07 03:26:11 - train: epoch 0030, iter [04700, 05004], lr: 0.085412, loss: 1.8280
2022-07-07 03:26:56 - train: epoch 0030, iter [04800, 05004], lr: 0.085393, loss: 1.7640
2022-07-07 03:27:42 - train: epoch 0030, iter [04900, 05004], lr: 0.085375, loss: 1.9862
2022-07-07 03:28:28 - train: epoch 0030, iter [05000, 05004], lr: 0.085356, loss: 1.9190
2022-07-07 03:28:30 - train: epoch 030, train_loss: 1.8083
2022-07-07 03:29:46 - eval: epoch: 030, acc1: 62.114%, acc5: 85.078%, test_loss: 1.5806, per_image_load_time: 2.094ms, per_image_inference_time: 0.835ms
2022-07-07 03:29:47 - until epoch: 030, best_acc1: 63.088%
2022-07-07 03:29:47 - epoch 031 lr: 0.085355
2022-07-07 03:30:37 - train: epoch 0031, iter [00100, 05004], lr: 0.085337, loss: 1.9833
2022-07-07 03:31:22 - train: epoch 0031, iter [00200, 05004], lr: 0.085318, loss: 1.6290
2022-07-07 03:32:07 - train: epoch 0031, iter [00300, 05004], lr: 0.085300, loss: 1.6935
2022-07-07 03:32:52 - train: epoch 0031, iter [00400, 05004], lr: 0.085281, loss: 1.8124
2022-07-07 03:33:37 - train: epoch 0031, iter [00500, 05004], lr: 0.085263, loss: 1.7048
2022-07-07 03:34:21 - train: epoch 0031, iter [00600, 05004], lr: 0.085244, loss: 1.7332
2022-07-07 03:35:07 - train: epoch 0031, iter [00700, 05004], lr: 0.085226, loss: 1.8323
2022-07-07 03:35:52 - train: epoch 0031, iter [00800, 05004], lr: 0.085207, loss: 1.7353
2022-07-07 03:36:37 - train: epoch 0031, iter [00900, 05004], lr: 0.085188, loss: 1.7280
2022-07-07 03:37:22 - train: epoch 0031, iter [01000, 05004], lr: 0.085170, loss: 1.9492
2022-07-07 03:38:08 - train: epoch 0031, iter [01100, 05004], lr: 0.085151, loss: 2.0082
2022-07-07 03:38:53 - train: epoch 0031, iter [01200, 05004], lr: 0.085133, loss: 1.7982
2022-07-07 03:39:38 - train: epoch 0031, iter [01300, 05004], lr: 0.085114, loss: 1.5120
2022-07-07 03:40:24 - train: epoch 0031, iter [01400, 05004], lr: 0.085095, loss: 1.7853
2022-07-07 03:41:09 - train: epoch 0031, iter [01500, 05004], lr: 0.085077, loss: 1.8930
2022-07-07 03:41:54 - train: epoch 0031, iter [01600, 05004], lr: 0.085058, loss: 1.6769
2022-07-07 03:42:39 - train: epoch 0031, iter [01700, 05004], lr: 0.085039, loss: 1.7342
2022-07-07 03:43:25 - train: epoch 0031, iter [01800, 05004], lr: 0.085021, loss: 1.7182
2022-07-07 03:44:10 - train: epoch 0031, iter [01900, 05004], lr: 0.085002, loss: 1.7666
2022-07-07 03:44:55 - train: epoch 0031, iter [02000, 05004], lr: 0.084983, loss: 1.8329
2022-07-07 03:45:41 - train: epoch 0031, iter [02100, 05004], lr: 0.084965, loss: 1.6628
2022-07-07 03:46:26 - train: epoch 0031, iter [02200, 05004], lr: 0.084946, loss: 1.6844
2022-07-07 03:47:12 - train: epoch 0031, iter [02300, 05004], lr: 0.084927, loss: 1.6589
2022-07-07 03:47:57 - train: epoch 0031, iter [02400, 05004], lr: 0.084909, loss: 1.6912
2022-07-07 03:48:42 - train: epoch 0031, iter [02500, 05004], lr: 0.084890, loss: 1.7279
2022-07-07 03:49:28 - train: epoch 0031, iter [02600, 05004], lr: 0.084871, loss: 1.7788
2022-07-07 03:50:13 - train: epoch 0031, iter [02700, 05004], lr: 0.084852, loss: 1.9573
2022-07-07 03:50:59 - train: epoch 0031, iter [02800, 05004], lr: 0.084834, loss: 2.0649
2022-07-07 03:51:44 - train: epoch 0031, iter [02900, 05004], lr: 0.084815, loss: 1.8886
2022-07-07 03:52:30 - train: epoch 0031, iter [03000, 05004], lr: 0.084796, loss: 1.9539
2022-07-07 03:53:15 - train: epoch 0031, iter [03100, 05004], lr: 0.084777, loss: 1.6953
2022-07-07 03:54:00 - train: epoch 0031, iter [03200, 05004], lr: 0.084759, loss: 1.9401
2022-07-07 03:54:45 - train: epoch 0031, iter [03300, 05004], lr: 0.084740, loss: 1.9325
2022-07-07 03:55:31 - train: epoch 0031, iter [03400, 05004], lr: 0.084721, loss: 1.8858
2022-07-07 03:56:16 - train: epoch 0031, iter [03500, 05004], lr: 0.084702, loss: 1.9310
2022-07-07 03:57:01 - train: epoch 0031, iter [03600, 05004], lr: 0.084683, loss: 1.7908
2022-07-07 03:57:46 - train: epoch 0031, iter [03700, 05004], lr: 0.084664, loss: 1.8113
2022-07-07 03:58:31 - train: epoch 0031, iter [03800, 05004], lr: 0.084646, loss: 1.6869
2022-07-07 03:59:17 - train: epoch 0031, iter [03900, 05004], lr: 0.084627, loss: 1.7129
2022-07-07 04:00:02 - train: epoch 0031, iter [04000, 05004], lr: 0.084608, loss: 1.7622
2022-07-07 04:00:47 - train: epoch 0031, iter [04100, 05004], lr: 0.084589, loss: 1.7806
2022-07-07 04:01:33 - train: epoch 0031, iter [04200, 05004], lr: 0.084570, loss: 1.9009
2022-07-07 04:02:18 - train: epoch 0031, iter [04300, 05004], lr: 0.084551, loss: 1.7811
2022-07-07 04:03:03 - train: epoch 0031, iter [04400, 05004], lr: 0.084532, loss: 1.9906
2022-07-07 04:03:49 - train: epoch 0031, iter [04500, 05004], lr: 0.084513, loss: 1.9422
2022-07-07 04:04:34 - train: epoch 0031, iter [04600, 05004], lr: 0.084494, loss: 1.9043
2022-07-07 04:05:19 - train: epoch 0031, iter [04700, 05004], lr: 0.084475, loss: 1.8467
2022-07-07 04:06:04 - train: epoch 0031, iter [04800, 05004], lr: 0.084456, loss: 1.7931
2022-07-07 04:06:49 - train: epoch 0031, iter [04900, 05004], lr: 0.084437, loss: 1.6481
2022-07-07 04:07:34 - train: epoch 0031, iter [05000, 05004], lr: 0.084418, loss: 1.7504
2022-07-07 04:07:37 - train: epoch 031, train_loss: 1.7986
2022-07-07 04:08:54 - eval: epoch: 031, acc1: 62.866%, acc5: 85.570%, test_loss: 1.5353, per_image_load_time: 1.308ms, per_image_inference_time: 0.842ms
2022-07-07 04:08:55 - until epoch: 031, best_acc1: 63.088%
2022-07-07 04:08:55 - epoch 032 lr: 0.084418
2022-07-07 04:09:45 - train: epoch 0032, iter [00100, 05004], lr: 0.084399, loss: 1.7371
2022-07-07 04:10:30 - train: epoch 0032, iter [00200, 05004], lr: 0.084380, loss: 1.8683
2022-07-07 04:11:15 - train: epoch 0032, iter [00300, 05004], lr: 0.084361, loss: 1.7911
2022-07-07 04:12:01 - train: epoch 0032, iter [00400, 05004], lr: 0.084342, loss: 1.9683
2022-07-07 04:12:46 - train: epoch 0032, iter [00500, 05004], lr: 0.084323, loss: 1.8449
2022-07-07 04:13:32 - train: epoch 0032, iter [00600, 05004], lr: 0.084304, loss: 1.8580
2022-07-07 04:14:17 - train: epoch 0032, iter [00700, 05004], lr: 0.084285, loss: 1.7137
2022-07-07 04:15:02 - train: epoch 0032, iter [00800, 05004], lr: 0.084266, loss: 1.7729
2022-07-07 04:15:48 - train: epoch 0032, iter [00900, 05004], lr: 0.084247, loss: 1.7657
2022-07-07 04:16:33 - train: epoch 0032, iter [01000, 05004], lr: 0.084228, loss: 1.8439
2022-07-07 04:17:18 - train: epoch 0032, iter [01100, 05004], lr: 0.084208, loss: 1.8842
2022-07-07 04:18:04 - train: epoch 0032, iter [01200, 05004], lr: 0.084189, loss: 1.8467
2022-07-07 04:18:49 - train: epoch 0032, iter [01300, 05004], lr: 0.084170, loss: 1.7179
2022-07-07 04:19:34 - train: epoch 0032, iter [01400, 05004], lr: 0.084151, loss: 1.9143
2022-07-07 04:20:19 - train: epoch 0032, iter [01500, 05004], lr: 0.084132, loss: 1.8386
2022-07-07 04:21:04 - train: epoch 0032, iter [01600, 05004], lr: 0.084113, loss: 1.8716
2022-07-07 04:21:49 - train: epoch 0032, iter [01700, 05004], lr: 0.084094, loss: 1.8997
2022-07-07 04:22:34 - train: epoch 0032, iter [01800, 05004], lr: 0.084075, loss: 2.1166
2022-07-07 04:23:19 - train: epoch 0032, iter [01900, 05004], lr: 0.084056, loss: 1.6758
2022-07-07 04:24:05 - train: epoch 0032, iter [02000, 05004], lr: 0.084036, loss: 1.8054
2022-07-07 04:24:50 - train: epoch 0032, iter [02100, 05004], lr: 0.084017, loss: 1.5840
2022-07-07 04:25:35 - train: epoch 0032, iter [02200, 05004], lr: 0.083998, loss: 1.7373
2022-07-07 04:26:20 - train: epoch 0032, iter [02300, 05004], lr: 0.083979, loss: 1.8230
2022-07-07 04:27:06 - train: epoch 0032, iter [02400, 05004], lr: 0.083960, loss: 1.6309
2022-07-07 04:27:51 - train: epoch 0032, iter [02500, 05004], lr: 0.083940, loss: 1.9556
2022-07-07 04:28:36 - train: epoch 0032, iter [02600, 05004], lr: 0.083921, loss: 1.6054
2022-07-07 04:29:21 - train: epoch 0032, iter [02700, 05004], lr: 0.083902, loss: 1.7293
2022-07-07 04:30:06 - train: epoch 0032, iter [02800, 05004], lr: 0.083883, loss: 1.9593
2022-07-07 04:30:51 - train: epoch 0032, iter [02900, 05004], lr: 0.083864, loss: 1.6023
2022-07-07 04:31:36 - train: epoch 0032, iter [03000, 05004], lr: 0.083844, loss: 1.5738
2022-07-07 04:32:21 - train: epoch 0032, iter [03100, 05004], lr: 0.083825, loss: 1.9587
2022-07-07 04:33:06 - train: epoch 0032, iter [03200, 05004], lr: 0.083806, loss: 1.7889
2022-07-07 04:33:51 - train: epoch 0032, iter [03300, 05004], lr: 0.083786, loss: 1.6347
2022-07-07 04:34:35 - train: epoch 0032, iter [03400, 05004], lr: 0.083767, loss: 1.5844
2022-07-07 04:35:20 - train: epoch 0032, iter [03500, 05004], lr: 0.083748, loss: 1.7578
2022-07-07 04:36:05 - train: epoch 0032, iter [03600, 05004], lr: 0.083729, loss: 1.9768
2022-07-07 04:36:51 - train: epoch 0032, iter [03700, 05004], lr: 0.083709, loss: 1.9081
2022-07-07 04:37:36 - train: epoch 0032, iter [03800, 05004], lr: 0.083690, loss: 1.5832
2022-07-07 04:38:21 - train: epoch 0032, iter [03900, 05004], lr: 0.083671, loss: 1.9016
2022-07-07 04:39:06 - train: epoch 0032, iter [04000, 05004], lr: 0.083651, loss: 1.6238
2022-07-07 04:39:51 - train: epoch 0032, iter [04100, 05004], lr: 0.083632, loss: 1.7486
2022-07-07 04:40:36 - train: epoch 0032, iter [04200, 05004], lr: 0.083613, loss: 1.5790
2022-07-07 04:41:21 - train: epoch 0032, iter [04300, 05004], lr: 0.083593, loss: 2.0211
2022-07-07 04:42:06 - train: epoch 0032, iter [04400, 05004], lr: 0.083574, loss: 1.7905
2022-07-07 04:42:51 - train: epoch 0032, iter [04500, 05004], lr: 0.083554, loss: 1.9972
2022-07-07 04:43:36 - train: epoch 0032, iter [04600, 05004], lr: 0.083535, loss: 1.8278
2022-07-07 04:44:21 - train: epoch 0032, iter [04700, 05004], lr: 0.083516, loss: 1.9045
2022-07-07 04:45:06 - train: epoch 0032, iter [04800, 05004], lr: 0.083496, loss: 1.6299
2022-07-07 04:45:52 - train: epoch 0032, iter [04900, 05004], lr: 0.083477, loss: 1.8140
2022-07-07 04:46:37 - train: epoch 0032, iter [05000, 05004], lr: 0.083457, loss: 1.8938
2022-07-07 04:46:39 - train: epoch 032, train_loss: 1.7898
2022-07-07 04:47:56 - eval: epoch: 032, acc1: 63.638%, acc5: 85.976%, test_loss: 1.5037, per_image_load_time: 1.679ms, per_image_inference_time: 0.836ms
2022-07-07 04:47:57 - until epoch: 032, best_acc1: 63.638%
2022-07-07 04:47:57 - epoch 033 lr: 0.083456
2022-07-07 04:48:47 - train: epoch 0033, iter [00100, 05004], lr: 0.083437, loss: 1.6337
2022-07-07 04:49:32 - train: epoch 0033, iter [00200, 05004], lr: 0.083418, loss: 1.8311
2022-07-07 04:50:17 - train: epoch 0033, iter [00300, 05004], lr: 0.083398, loss: 1.6371
2022-07-07 04:51:02 - train: epoch 0033, iter [00400, 05004], lr: 0.083379, loss: 1.5484
2022-07-07 04:51:47 - train: epoch 0033, iter [00500, 05004], lr: 0.083359, loss: 1.9025
2022-07-07 04:52:32 - train: epoch 0033, iter [00600, 05004], lr: 0.083340, loss: 1.6692
2022-07-07 04:53:17 - train: epoch 0033, iter [00700, 05004], lr: 0.083320, loss: 1.8408
2022-07-07 04:54:02 - train: epoch 0033, iter [00800, 05004], lr: 0.083301, loss: 1.8064
2022-07-07 04:54:47 - train: epoch 0033, iter [00900, 05004], lr: 0.083281, loss: 1.8691
2022-07-07 04:55:32 - train: epoch 0033, iter [01000, 05004], lr: 0.083262, loss: 1.7915
2022-07-07 04:56:17 - train: epoch 0033, iter [01100, 05004], lr: 0.083242, loss: 1.6546
2022-07-07 04:57:02 - train: epoch 0033, iter [01200, 05004], lr: 0.083223, loss: 1.7881
2022-07-07 04:57:47 - train: epoch 0033, iter [01300, 05004], lr: 0.083203, loss: 1.6724
2022-07-07 04:58:32 - train: epoch 0033, iter [01400, 05004], lr: 0.083183, loss: 1.8612
2022-07-07 04:59:17 - train: epoch 0033, iter [01500, 05004], lr: 0.083164, loss: 2.1493
2022-07-07 05:00:02 - train: epoch 0033, iter [01600, 05004], lr: 0.083144, loss: 2.0741
2022-07-07 05:00:47 - train: epoch 0033, iter [01700, 05004], lr: 0.083125, loss: 1.7050
2022-07-07 05:01:32 - train: epoch 0033, iter [01800, 05004], lr: 0.083105, loss: 1.8650
2022-07-07 05:02:17 - train: epoch 0033, iter [01900, 05004], lr: 0.083086, loss: 1.8983
2022-07-07 05:03:03 - train: epoch 0033, iter [02000, 05004], lr: 0.083066, loss: 1.7048
2022-07-07 05:03:48 - train: epoch 0033, iter [02100, 05004], lr: 0.083046, loss: 1.7909
2022-07-07 05:04:33 - train: epoch 0033, iter [02200, 05004], lr: 0.083027, loss: 1.8252
2022-07-07 05:05:18 - train: epoch 0033, iter [02300, 05004], lr: 0.083007, loss: 1.6118
2022-07-07 05:06:03 - train: epoch 0033, iter [02400, 05004], lr: 0.082987, loss: 1.9785
2022-07-07 05:06:49 - train: epoch 0033, iter [02500, 05004], lr: 0.082968, loss: 1.8036
2022-07-07 05:07:34 - train: epoch 0033, iter [02600, 05004], lr: 0.082948, loss: 1.6789
2022-07-07 05:08:19 - train: epoch 0033, iter [02700, 05004], lr: 0.082928, loss: 1.9356
2022-07-07 05:09:04 - train: epoch 0033, iter [02800, 05004], lr: 0.082909, loss: 1.8289
2022-07-07 05:09:49 - train: epoch 0033, iter [02900, 05004], lr: 0.082889, loss: 1.9003
2022-07-07 05:10:35 - train: epoch 0033, iter [03000, 05004], lr: 0.082869, loss: 1.8410
2022-07-07 05:11:20 - train: epoch 0033, iter [03100, 05004], lr: 0.082850, loss: 1.8495
2022-07-07 05:12:05 - train: epoch 0033, iter [03200, 05004], lr: 0.082830, loss: 1.6361
2022-07-07 05:12:50 - train: epoch 0033, iter [03300, 05004], lr: 0.082810, loss: 1.8852
2022-07-07 05:13:35 - train: epoch 0033, iter [03400, 05004], lr: 0.082790, loss: 1.7470
2022-07-07 05:14:20 - train: epoch 0033, iter [03500, 05004], lr: 0.082771, loss: 1.7887
2022-07-07 05:15:05 - train: epoch 0033, iter [03600, 05004], lr: 0.082751, loss: 1.9193
2022-07-07 05:15:50 - train: epoch 0033, iter [03700, 05004], lr: 0.082731, loss: 1.8416
2022-07-07 05:16:35 - train: epoch 0033, iter [03800, 05004], lr: 0.082711, loss: 1.7738
2022-07-07 05:17:20 - train: epoch 0033, iter [03900, 05004], lr: 0.082691, loss: 1.8934
2022-07-07 05:18:05 - train: epoch 0033, iter [04000, 05004], lr: 0.082672, loss: 1.8722
2022-07-07 05:18:50 - train: epoch 0033, iter [04100, 05004], lr: 0.082652, loss: 1.8543
2022-07-07 05:19:35 - train: epoch 0033, iter [04200, 05004], lr: 0.082632, loss: 1.7833
2022-07-07 05:20:20 - train: epoch 0033, iter [04300, 05004], lr: 0.082612, loss: 2.0209
2022-07-07 05:21:05 - train: epoch 0033, iter [04400, 05004], lr: 0.082592, loss: 1.8319
2022-07-07 05:21:50 - train: epoch 0033, iter [04500, 05004], lr: 0.082573, loss: 2.0356
2022-07-07 05:22:35 - train: epoch 0033, iter [04600, 05004], lr: 0.082553, loss: 1.7584
2022-07-07 05:23:20 - train: epoch 0033, iter [04700, 05004], lr: 0.082533, loss: 1.6769
2022-07-07 05:24:05 - train: epoch 0033, iter [04800, 05004], lr: 0.082513, loss: 2.1668
2022-07-07 05:24:50 - train: epoch 0033, iter [04900, 05004], lr: 0.082493, loss: 1.6398
2022-07-07 05:25:35 - train: epoch 0033, iter [05000, 05004], lr: 0.082473, loss: 1.7637
2022-07-07 05:25:38 - train: epoch 033, train_loss: 1.7785
2022-07-07 05:26:54 - eval: epoch: 033, acc1: 63.502%, acc5: 86.046%, test_loss: 1.5007, per_image_load_time: 1.520ms, per_image_inference_time: 0.843ms
2022-07-07 05:26:55 - until epoch: 033, best_acc1: 63.638%
2022-07-07 05:26:55 - epoch 034 lr: 0.082472
2022-07-07 05:27:46 - train: epoch 0034, iter [00100, 05004], lr: 0.082453, loss: 1.6528
2022-07-07 05:28:31 - train: epoch 0034, iter [00200, 05004], lr: 0.082433, loss: 1.6959
2022-07-07 05:29:15 - train: epoch 0034, iter [00300, 05004], lr: 0.082413, loss: 1.7624
2022-07-07 05:30:00 - train: epoch 0034, iter [00400, 05004], lr: 0.082393, loss: 1.5292
2022-07-07 05:30:45 - train: epoch 0034, iter [00500, 05004], lr: 0.082373, loss: 1.7400
2022-07-07 05:31:30 - train: epoch 0034, iter [00600, 05004], lr: 0.082353, loss: 1.9442
2022-07-07 05:32:15 - train: epoch 0034, iter [00700, 05004], lr: 0.082333, loss: 1.7141
2022-07-07 05:33:00 - train: epoch 0034, iter [00800, 05004], lr: 0.082313, loss: 1.7009
2022-07-07 05:33:45 - train: epoch 0034, iter [00900, 05004], lr: 0.082293, loss: 1.7257
2022-07-07 05:34:30 - train: epoch 0034, iter [01000, 05004], lr: 0.082273, loss: 1.7918
2022-07-07 05:35:14 - train: epoch 0034, iter [01100, 05004], lr: 0.082253, loss: 1.7446
2022-07-07 05:35:59 - train: epoch 0034, iter [01200, 05004], lr: 0.082233, loss: 1.6908
2022-07-07 05:36:44 - train: epoch 0034, iter [01300, 05004], lr: 0.082213, loss: 1.6592
2022-07-07 05:37:29 - train: epoch 0034, iter [01400, 05004], lr: 0.082193, loss: 1.8586
2022-07-07 05:38:14 - train: epoch 0034, iter [01500, 05004], lr: 0.082173, loss: 1.7277
2022-07-07 05:38:59 - train: epoch 0034, iter [01600, 05004], lr: 0.082153, loss: 1.6827
2022-07-07 05:39:44 - train: epoch 0034, iter [01700, 05004], lr: 0.082133, loss: 1.6606
2022-07-07 05:40:29 - train: epoch 0034, iter [01800, 05004], lr: 0.082113, loss: 2.0480
2022-07-07 05:41:14 - train: epoch 0034, iter [01900, 05004], lr: 0.082093, loss: 1.7925
2022-07-07 05:41:58 - train: epoch 0034, iter [02000, 05004], lr: 0.082073, loss: 1.8245
2022-07-07 05:42:43 - train: epoch 0034, iter [02100, 05004], lr: 0.082053, loss: 1.9813
2022-07-07 05:43:28 - train: epoch 0034, iter [02200, 05004], lr: 0.082033, loss: 1.6556
2022-07-07 05:44:13 - train: epoch 0034, iter [02300, 05004], lr: 0.082013, loss: 1.9127
2022-07-07 05:44:58 - train: epoch 0034, iter [02400, 05004], lr: 0.081992, loss: 1.6996
2022-07-07 05:45:43 - train: epoch 0034, iter [02500, 05004], lr: 0.081972, loss: 1.8006
2022-07-07 05:46:28 - train: epoch 0034, iter [02600, 05004], lr: 0.081952, loss: 1.8259
2022-07-07 05:47:13 - train: epoch 0034, iter [02700, 05004], lr: 0.081932, loss: 1.7333
2022-07-07 05:47:58 - train: epoch 0034, iter [02800, 05004], lr: 0.081912, loss: 1.6059
2022-07-07 05:48:43 - train: epoch 0034, iter [02900, 05004], lr: 0.081892, loss: 1.4624
2022-07-07 05:49:28 - train: epoch 0034, iter [03000, 05004], lr: 0.081872, loss: 1.5337
2022-07-07 05:50:13 - train: epoch 0034, iter [03100, 05004], lr: 0.081852, loss: 1.5115
2022-07-07 05:50:58 - train: epoch 0034, iter [03200, 05004], lr: 0.081831, loss: 1.7373
2022-07-07 05:51:43 - train: epoch 0034, iter [03300, 05004], lr: 0.081811, loss: 1.7525
2022-07-07 05:52:28 - train: epoch 0034, iter [03400, 05004], lr: 0.081791, loss: 1.7838
2022-07-07 05:53:13 - train: epoch 0034, iter [03500, 05004], lr: 0.081771, loss: 1.7252
2022-07-07 05:53:58 - train: epoch 0034, iter [03600, 05004], lr: 0.081751, loss: 1.5892
2022-07-07 05:54:43 - train: epoch 0034, iter [03700, 05004], lr: 0.081730, loss: 1.6216
2022-07-07 05:55:28 - train: epoch 0034, iter [03800, 05004], lr: 0.081710, loss: 1.6562
2022-07-07 05:56:13 - train: epoch 0034, iter [03900, 05004], lr: 0.081690, loss: 1.8191
2022-07-07 05:56:58 - train: epoch 0034, iter [04000, 05004], lr: 0.081670, loss: 1.6642
2022-07-07 05:57:43 - train: epoch 0034, iter [04100, 05004], lr: 0.081649, loss: 1.7761
2022-07-07 05:58:28 - train: epoch 0034, iter [04200, 05004], lr: 0.081629, loss: 1.5887
2022-07-07 05:59:13 - train: epoch 0034, iter [04300, 05004], lr: 0.081609, loss: 1.7772
2022-07-07 05:59:58 - train: epoch 0034, iter [04400, 05004], lr: 0.081589, loss: 1.7600
2022-07-07 06:00:43 - train: epoch 0034, iter [04500, 05004], lr: 0.081568, loss: 1.8789
2022-07-07 06:01:28 - train: epoch 0034, iter [04600, 05004], lr: 0.081548, loss: 1.9424
2022-07-07 06:02:13 - train: epoch 0034, iter [04700, 05004], lr: 0.081528, loss: 1.7810
2022-07-07 06:02:58 - train: epoch 0034, iter [04800, 05004], lr: 0.081507, loss: 1.7638
2022-07-07 06:03:43 - train: epoch 0034, iter [04900, 05004], lr: 0.081487, loss: 1.7701
2022-07-07 06:04:28 - train: epoch 0034, iter [05000, 05004], lr: 0.081467, loss: 1.7598
2022-07-07 06:04:30 - train: epoch 034, train_loss: 1.7702
2022-07-07 06:05:46 - eval: epoch: 034, acc1: 63.574%, acc5: 86.232%, test_loss: 1.5042, per_image_load_time: 1.792ms, per_image_inference_time: 0.827ms
2022-07-07 06:05:47 - until epoch: 034, best_acc1: 63.638%
2022-07-07 06:05:47 - epoch 035 lr: 0.081466
2022-07-07 06:06:38 - train: epoch 0035, iter [00100, 05004], lr: 0.081446, loss: 1.5628
2022-07-07 06:07:23 - train: epoch 0035, iter [00200, 05004], lr: 0.081425, loss: 1.5632
2022-07-07 06:08:08 - train: epoch 0035, iter [00300, 05004], lr: 0.081405, loss: 1.9889
2022-07-07 06:08:53 - train: epoch 0035, iter [00400, 05004], lr: 0.081385, loss: 1.6136
2022-07-07 06:09:38 - train: epoch 0035, iter [00500, 05004], lr: 0.081364, loss: 1.7882
2022-07-07 06:10:23 - train: epoch 0035, iter [00600, 05004], lr: 0.081344, loss: 1.7183
2022-07-07 06:11:08 - train: epoch 0035, iter [00700, 05004], lr: 0.081324, loss: 1.7610
2022-07-07 06:11:52 - train: epoch 0035, iter [00800, 05004], lr: 0.081303, loss: 1.7250
2022-07-07 06:12:38 - train: epoch 0035, iter [00900, 05004], lr: 0.081283, loss: 1.8990
2022-07-07 06:13:23 - train: epoch 0035, iter [01000, 05004], lr: 0.081262, loss: 1.7865
2022-07-07 06:14:08 - train: epoch 0035, iter [01100, 05004], lr: 0.081242, loss: 1.9100
2022-07-07 06:14:52 - train: epoch 0035, iter [01200, 05004], lr: 0.081221, loss: 1.7466
2022-07-07 06:15:38 - train: epoch 0035, iter [01300, 05004], lr: 0.081201, loss: 1.7488
2022-07-07 06:16:23 - train: epoch 0035, iter [01400, 05004], lr: 0.081181, loss: 1.7869
2022-07-07 06:17:07 - train: epoch 0035, iter [01500, 05004], lr: 0.081160, loss: 1.7855
2022-07-07 06:17:53 - train: epoch 0035, iter [01600, 05004], lr: 0.081140, loss: 1.7420
2022-07-07 06:18:38 - train: epoch 0035, iter [01700, 05004], lr: 0.081119, loss: 1.5777
2022-07-07 06:19:23 - train: epoch 0035, iter [01800, 05004], lr: 0.081099, loss: 1.8236
2022-07-07 06:20:08 - train: epoch 0035, iter [01900, 05004], lr: 0.081078, loss: 1.7620
2022-07-07 06:20:53 - train: epoch 0035, iter [02000, 05004], lr: 0.081058, loss: 1.7135
2022-07-07 06:21:38 - train: epoch 0035, iter [02100, 05004], lr: 0.081037, loss: 1.6896
2022-07-07 06:22:23 - train: epoch 0035, iter [02200, 05004], lr: 0.081017, loss: 1.8481
2022-07-07 06:23:08 - train: epoch 0035, iter [02300, 05004], lr: 0.080996, loss: 1.7465
2022-07-07 06:23:53 - train: epoch 0035, iter [02400, 05004], lr: 0.080976, loss: 1.8575
2022-07-07 06:24:39 - train: epoch 0035, iter [02500, 05004], lr: 0.080955, loss: 1.7214
2022-07-07 06:25:24 - train: epoch 0035, iter [02600, 05004], lr: 0.080935, loss: 1.9445
2022-07-07 06:26:09 - train: epoch 0035, iter [02700, 05004], lr: 0.080914, loss: 1.9236
2022-07-07 06:26:54 - train: epoch 0035, iter [02800, 05004], lr: 0.080893, loss: 1.6978
2022-07-07 06:27:39 - train: epoch 0035, iter [02900, 05004], lr: 0.080873, loss: 1.7590
2022-07-07 06:28:25 - train: epoch 0035, iter [03000, 05004], lr: 0.080852, loss: 1.8824
2022-07-07 06:29:10 - train: epoch 0035, iter [03100, 05004], lr: 0.080832, loss: 1.7587
2022-07-07 06:29:55 - train: epoch 0035, iter [03200, 05004], lr: 0.080811, loss: 1.7306
2022-07-07 06:30:40 - train: epoch 0035, iter [03300, 05004], lr: 0.080790, loss: 1.6836
2022-07-07 06:31:25 - train: epoch 0035, iter [03400, 05004], lr: 0.080770, loss: 1.7434
2022-07-07 06:32:11 - train: epoch 0035, iter [03500, 05004], lr: 0.080749, loss: 1.4502
2022-07-07 06:32:56 - train: epoch 0035, iter [03600, 05004], lr: 0.080729, loss: 1.8552
2022-07-07 06:33:41 - train: epoch 0035, iter [03700, 05004], lr: 0.080708, loss: 1.5721
2022-07-07 06:34:26 - train: epoch 0035, iter [03800, 05004], lr: 0.080687, loss: 1.8240
2022-07-07 06:35:11 - train: epoch 0035, iter [03900, 05004], lr: 0.080667, loss: 1.9325
2022-07-07 06:35:57 - train: epoch 0035, iter [04000, 05004], lr: 0.080646, loss: 1.5474
2022-07-07 06:36:42 - train: epoch 0035, iter [04100, 05004], lr: 0.080625, loss: 2.0675
2022-07-07 06:37:27 - train: epoch 0035, iter [04200, 05004], lr: 0.080605, loss: 1.7908
2022-07-07 06:38:12 - train: epoch 0035, iter [04300, 05004], lr: 0.080584, loss: 1.7235
2022-07-07 06:38:57 - train: epoch 0035, iter [04400, 05004], lr: 0.080563, loss: 1.7975
2022-07-07 06:39:43 - train: epoch 0035, iter [04500, 05004], lr: 0.080543, loss: 1.8854
2022-07-07 06:40:28 - train: epoch 0035, iter [04600, 05004], lr: 0.080522, loss: 1.7102
2022-07-07 06:41:13 - train: epoch 0035, iter [04700, 05004], lr: 0.080501, loss: 2.0755
2022-07-07 06:41:58 - train: epoch 0035, iter [04800, 05004], lr: 0.080480, loss: 1.8902
2022-07-07 06:42:43 - train: epoch 0035, iter [04900, 05004], lr: 0.080460, loss: 1.8053
2022-07-07 06:43:28 - train: epoch 0035, iter [05000, 05004], lr: 0.080439, loss: 1.6134
2022-07-07 06:43:30 - train: epoch 035, train_loss: 1.7611
2022-07-07 06:44:46 - eval: epoch: 035, acc1: 63.546%, acc5: 85.976%, test_loss: 1.5087, per_image_load_time: 1.793ms, per_image_inference_time: 0.828ms
2022-07-07 06:44:47 - until epoch: 035, best_acc1: 63.638%
2022-07-07 06:44:47 - epoch 036 lr: 0.080438
2022-07-07 06:45:38 - train: epoch 0036, iter [00100, 05004], lr: 0.080417, loss: 1.8290
2022-07-07 06:46:22 - train: epoch 0036, iter [00200, 05004], lr: 0.080397, loss: 1.4937
2022-07-07 06:47:07 - train: epoch 0036, iter [00300, 05004], lr: 0.080376, loss: 1.5529
2022-07-07 06:47:52 - train: epoch 0036, iter [00400, 05004], lr: 0.080355, loss: 1.7640
2022-07-07 06:48:37 - train: epoch 0036, iter [00500, 05004], lr: 0.080334, loss: 1.7357
2022-07-07 06:49:22 - train: epoch 0036, iter [00600, 05004], lr: 0.080313, loss: 1.6185
2022-07-07 06:50:07 - train: epoch 0036, iter [00700, 05004], lr: 0.080293, loss: 1.5850
2022-07-07 06:50:52 - train: epoch 0036, iter [00800, 05004], lr: 0.080272, loss: 1.5223
2022-07-07 06:51:37 - train: epoch 0036, iter [00900, 05004], lr: 0.080251, loss: 1.7918
2022-07-07 06:52:22 - train: epoch 0036, iter [01000, 05004], lr: 0.080230, loss: 1.6891
2022-07-07 06:53:07 - train: epoch 0036, iter [01100, 05004], lr: 0.080209, loss: 1.6798
2022-07-07 06:53:52 - train: epoch 0036, iter [01200, 05004], lr: 0.080188, loss: 1.7324
2022-07-07 06:54:36 - train: epoch 0036, iter [01300, 05004], lr: 0.080168, loss: 1.7397
2022-07-07 06:55:21 - train: epoch 0036, iter [01400, 05004], lr: 0.080147, loss: 1.9492
2022-07-07 06:56:06 - train: epoch 0036, iter [01500, 05004], lr: 0.080126, loss: 1.9058
2022-07-07 06:56:51 - train: epoch 0036, iter [01600, 05004], lr: 0.080105, loss: 1.7890
2022-07-07 06:57:36 - train: epoch 0036, iter [01700, 05004], lr: 0.080084, loss: 1.7099
2022-07-07 06:58:21 - train: epoch 0036, iter [01800, 05004], lr: 0.080063, loss: 1.5422
2022-07-07 06:59:06 - train: epoch 0036, iter [01900, 05004], lr: 0.080042, loss: 1.7721
2022-07-07 06:59:51 - train: epoch 0036, iter [02000, 05004], lr: 0.080021, loss: 1.7737
2022-07-07 07:00:36 - train: epoch 0036, iter [02100, 05004], lr: 0.080000, loss: 1.6922
2022-07-07 07:01:21 - train: epoch 0036, iter [02200, 05004], lr: 0.079979, loss: 1.7027
2022-07-07 07:02:06 - train: epoch 0036, iter [02300, 05004], lr: 0.079959, loss: 1.7724
2022-07-07 07:02:51 - train: epoch 0036, iter [02400, 05004], lr: 0.079938, loss: 1.8960
2022-07-07 07:03:37 - train: epoch 0036, iter [02500, 05004], lr: 0.079917, loss: 1.5314
2022-07-07 07:04:22 - train: epoch 0036, iter [02600, 05004], lr: 0.079896, loss: 1.8064
2022-07-07 07:05:07 - train: epoch 0036, iter [02700, 05004], lr: 0.079875, loss: 1.6084
2022-07-07 07:05:52 - train: epoch 0036, iter [02800, 05004], lr: 0.079854, loss: 1.6150
2022-07-07 07:06:37 - train: epoch 0036, iter [02900, 05004], lr: 0.079833, loss: 1.5504
2022-07-07 07:07:22 - train: epoch 0036, iter [03000, 05004], lr: 0.079812, loss: 1.9613
2022-07-07 07:08:06 - train: epoch 0036, iter [03100, 05004], lr: 0.079791, loss: 1.8338
2022-07-07 07:08:52 - train: epoch 0036, iter [03200, 05004], lr: 0.079770, loss: 1.6613
2022-07-07 07:09:37 - train: epoch 0036, iter [03300, 05004], lr: 0.079749, loss: 1.6237
2022-07-07 07:10:22 - train: epoch 0036, iter [03400, 05004], lr: 0.079728, loss: 1.6539
2022-07-07 07:11:07 - train: epoch 0036, iter [03500, 05004], lr: 0.079707, loss: 1.8345
2022-07-07 07:11:52 - train: epoch 0036, iter [03600, 05004], lr: 0.079686, loss: 1.7965
2022-07-07 07:12:37 - train: epoch 0036, iter [03700, 05004], lr: 0.079665, loss: 1.8692
2022-07-07 07:13:22 - train: epoch 0036, iter [03800, 05004], lr: 0.079643, loss: 1.6114
2022-07-07 07:14:07 - train: epoch 0036, iter [03900, 05004], lr: 0.079622, loss: 1.7891
2022-07-07 07:14:52 - train: epoch 0036, iter [04000, 05004], lr: 0.079601, loss: 1.7770
2022-07-07 07:15:38 - train: epoch 0036, iter [04100, 05004], lr: 0.079580, loss: 1.8366
2022-07-07 07:16:23 - train: epoch 0036, iter [04200, 05004], lr: 0.079559, loss: 1.7368
2022-07-07 07:17:08 - train: epoch 0036, iter [04300, 05004], lr: 0.079538, loss: 1.7806
2022-07-07 07:17:53 - train: epoch 0036, iter [04400, 05004], lr: 0.079517, loss: 1.8333
2022-07-07 07:18:38 - train: epoch 0036, iter [04500, 05004], lr: 0.079496, loss: 1.4825
2022-07-07 07:19:23 - train: epoch 0036, iter [04600, 05004], lr: 0.079475, loss: 1.6596
2022-07-07 07:20:08 - train: epoch 0036, iter [04700, 05004], lr: 0.079454, loss: 1.6833
2022-07-07 07:20:53 - train: epoch 0036, iter [04800, 05004], lr: 0.079432, loss: 1.7907
2022-07-07 07:21:38 - train: epoch 0036, iter [04900, 05004], lr: 0.079411, loss: 1.7396
2022-07-07 07:22:23 - train: epoch 0036, iter [05000, 05004], lr: 0.079390, loss: 1.8458
2022-07-07 07:22:25 - train: epoch 036, train_loss: 1.7496
2022-07-07 07:23:42 - eval: epoch: 036, acc1: 63.490%, acc5: 85.900%, test_loss: 1.5197, per_image_load_time: 1.493ms, per_image_inference_time: 0.832ms
2022-07-07 07:23:43 - until epoch: 036, best_acc1: 63.638%
2022-07-07 07:23:43 - epoch 037 lr: 0.079389
2022-07-07 07:24:33 - train: epoch 0037, iter [00100, 05004], lr: 0.079368, loss: 1.5027
2022-07-07 07:25:18 - train: epoch 0037, iter [00200, 05004], lr: 0.079347, loss: 1.3405
2022-07-07 07:26:03 - train: epoch 0037, iter [00300, 05004], lr: 0.079326, loss: 1.6968
2022-07-07 07:26:48 - train: epoch 0037, iter [00400, 05004], lr: 0.079305, loss: 1.7374
2022-07-07 07:27:33 - train: epoch 0037, iter [00500, 05004], lr: 0.079283, loss: 1.7386
2022-07-07 07:28:18 - train: epoch 0037, iter [00600, 05004], lr: 0.079262, loss: 1.8273
2022-07-07 07:29:03 - train: epoch 0037, iter [00700, 05004], lr: 0.079241, loss: 1.7513
2022-07-07 07:29:48 - train: epoch 0037, iter [00800, 05004], lr: 0.079220, loss: 1.6921
2022-07-07 07:30:33 - train: epoch 0037, iter [00900, 05004], lr: 0.079198, loss: 2.0510
2022-07-07 07:31:18 - train: epoch 0037, iter [01000, 05004], lr: 0.079177, loss: 1.6535
2022-07-07 07:32:03 - train: epoch 0037, iter [01100, 05004], lr: 0.079156, loss: 1.7571
2022-07-07 07:32:48 - train: epoch 0037, iter [01200, 05004], lr: 0.079135, loss: 1.7177
2022-07-07 07:33:33 - train: epoch 0037, iter [01300, 05004], lr: 0.079113, loss: 1.7490
2022-07-07 07:34:18 - train: epoch 0037, iter [01400, 05004], lr: 0.079092, loss: 1.7127
2022-07-07 07:35:03 - train: epoch 0037, iter [01500, 05004], lr: 0.079071, loss: 1.6337
2022-07-07 07:35:47 - train: epoch 0037, iter [01600, 05004], lr: 0.079050, loss: 1.5353
2022-07-07 07:36:33 - train: epoch 0037, iter [01700, 05004], lr: 0.079028, loss: 1.8985
2022-07-07 07:37:18 - train: epoch 0037, iter [01800, 05004], lr: 0.079007, loss: 1.9110
2022-07-07 07:38:03 - train: epoch 0037, iter [01900, 05004], lr: 0.078986, loss: 1.8060
2022-07-07 07:38:48 - train: epoch 0037, iter [02000, 05004], lr: 0.078964, loss: 1.8699
2022-07-07 07:39:32 - train: epoch 0037, iter [02100, 05004], lr: 0.078943, loss: 1.8354
2022-07-07 07:40:17 - train: epoch 0037, iter [02200, 05004], lr: 0.078922, loss: 1.6904
2022-07-07 07:41:02 - train: epoch 0037, iter [02300, 05004], lr: 0.078900, loss: 1.5460
2022-07-07 07:41:47 - train: epoch 0037, iter [02400, 05004], lr: 0.078879, loss: 1.8045
2022-07-07 07:42:32 - train: epoch 0037, iter [02500, 05004], lr: 0.078858, loss: 1.6429
2022-07-07 07:43:17 - train: epoch 0037, iter [02600, 05004], lr: 0.078836, loss: 1.9121
2022-07-07 07:44:02 - train: epoch 0037, iter [02700, 05004], lr: 0.078815, loss: 1.8868
2022-07-07 07:44:47 - train: epoch 0037, iter [02800, 05004], lr: 0.078794, loss: 1.8766
2022-07-07 07:45:32 - train: epoch 0037, iter [02900, 05004], lr: 0.078772, loss: 1.8240
2022-07-07 07:46:17 - train: epoch 0037, iter [03000, 05004], lr: 0.078751, loss: 1.8869
2022-07-07 07:47:02 - train: epoch 0037, iter [03100, 05004], lr: 0.078729, loss: 1.6698
2022-07-07 07:47:47 - train: epoch 0037, iter [03200, 05004], lr: 0.078708, loss: 1.7991
2022-07-07 07:48:32 - train: epoch 0037, iter [03300, 05004], lr: 0.078687, loss: 1.6175
2022-07-07 07:49:18 - train: epoch 0037, iter [03400, 05004], lr: 0.078665, loss: 1.6615
2022-07-07 07:50:02 - train: epoch 0037, iter [03500, 05004], lr: 0.078644, loss: 1.7861
2022-07-07 07:50:47 - train: epoch 0037, iter [03600, 05004], lr: 0.078622, loss: 1.8767
2022-07-07 07:51:32 - train: epoch 0037, iter [03700, 05004], lr: 0.078601, loss: 1.8181
2022-07-07 07:52:17 - train: epoch 0037, iter [03800, 05004], lr: 0.078579, loss: 1.5775
2022-07-07 07:53:02 - train: epoch 0037, iter [03900, 05004], lr: 0.078558, loss: 1.8476
