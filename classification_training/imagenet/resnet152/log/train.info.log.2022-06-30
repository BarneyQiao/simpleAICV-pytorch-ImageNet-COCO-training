2022-06-30 01:34:52 - network: resnet152
2022-06-30 01:34:52 - num_classes: 1000
2022-06-30 01:34:52 - input_image_size: 224
2022-06-30 01:34:52 - scale: 1.1428571428571428
2022-06-30 01:34:52 - trained_model_path: 
2022-06-30 01:34:52 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-06-30 01:34:52 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-06-30 01:34:52 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f8bd4b1c670>
2022-06-30 01:34:52 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f8bd4b1c940>
2022-06-30 01:34:52 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f8bd4b1c970>
2022-06-30 01:34:52 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f8bd4b1c9d0>
2022-06-30 01:34:52 - seed: 0
2022-06-30 01:34:52 - batch_size: 256
2022-06-30 01:34:52 - num_workers: 16
2022-06-30 01:34:52 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-06-30 01:34:52 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-06-30 01:34:52 - epochs: 100
2022-06-30 01:34:52 - print_interval: 100
2022-06-30 01:34:52 - sync_bn: False
2022-06-30 01:34:52 - apex: True
2022-06-30 01:34:52 - use_ema_model: False
2022-06-30 01:34:52 - ema_model_decay: 0.9999
2022-06-30 01:34:52 - gpus_type: NVIDIA RTX A5000
2022-06-30 01:34:52 - gpus_num: 2
2022-06-30 01:34:52 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f8bb613c030>
2022-06-30 01:34:52 - --------------------parameters--------------------
2022-06-30 01:34:52 - name: conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.4.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.4.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.4.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.4.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.4.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.4.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.4.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.4.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.4.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.5.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.5.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.5.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.5.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.5.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.5.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.5.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.5.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.5.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.6.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.6.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.6.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.6.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.6.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.6.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.6.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.6.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.6.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.7.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.7.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.7.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.7.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.7.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.7.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer2.7.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer2.7.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer2.7.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.6.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.6.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.6.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.6.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.6.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.6.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.6.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.6.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.6.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.7.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.7.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.7.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.7.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.7.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.7.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.7.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.7.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.7.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.8.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.8.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.8.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.8.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.8.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.8.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.8.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.8.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.8.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.9.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.9.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.9.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.9.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.9.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.9.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.9.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.9.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.9.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.10.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.10.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.10.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.10.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.10.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.10.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.10.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.10.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.10.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.11.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.11.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.11.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.11.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.11.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.11.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.11.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.11.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.11.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.12.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.12.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.12.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.12.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.12.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.12.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.12.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.12.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.12.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.13.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.13.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.13.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.13.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.13.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.13.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.13.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.13.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.13.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.14.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.14.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.14.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.14.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.14.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.14.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.14.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.14.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.14.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.15.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.15.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.15.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.15.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.15.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.15.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.15.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.15.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.15.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.16.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.16.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.16.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.16.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.16.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.16.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.16.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.16.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.16.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.17.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.17.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.17.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.17.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.17.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.17.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.17.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.17.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.17.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.18.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.18.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.18.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.18.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.18.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.18.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.18.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.18.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.18.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.19.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.19.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.19.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.19.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.19.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.19.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.19.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.19.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.19.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.20.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.20.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.20.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.20.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.20.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.20.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.20.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.20.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.20.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.21.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.21.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.21.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.21.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.21.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.21.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.21.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.21.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.21.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.22.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.22.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.22.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.22.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.22.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.22.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.22.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.22.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.22.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.23.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.23.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.23.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.23.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.23.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.23.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.23.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.23.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.23.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.24.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.24.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.24.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.24.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.24.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.24.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.24.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.24.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.24.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.25.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.25.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.25.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.25.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.25.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.25.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.25.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.25.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.25.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.26.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.26.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.26.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.26.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.26.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.26.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.26.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.26.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.26.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.27.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.27.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.27.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.27.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.27.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.27.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.27.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.27.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.27.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.28.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.28.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.28.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.28.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.28.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.28.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.28.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.28.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.28.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.29.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.29.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.29.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.29.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.29.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.29.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.29.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.29.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.29.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.30.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.30.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.30.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.30.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.30.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.30.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.30.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.30.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.30.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.31.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.31.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.31.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.31.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.31.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.31.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.31.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.31.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.31.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.32.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.32.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.32.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.32.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.32.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.32.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.32.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.32.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.32.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.33.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.33.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.33.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.33.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.33.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.33.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.33.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.33.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.33.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.34.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.34.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.34.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.34.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.34.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.34.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.34.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.34.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.34.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.35.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.35.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.35.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.35.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.35.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.35.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer3.35.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer3.35.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer3.35.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-06-30 01:34:52 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-06-30 01:34:52 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-06-30 01:34:52 - name: fc.weight, grad: True
2022-06-30 01:34:52 - name: fc.bias, grad: True
2022-06-30 01:34:52 - --------------------buffers--------------------
2022-06-30 01:34:52 - name: conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.4.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.4.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.4.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.4.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.4.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.4.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.4.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.4.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.4.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.5.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.5.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.5.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.5.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.5.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.5.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.5.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.5.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.5.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.6.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.6.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.6.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.6.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.6.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.6.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.6.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.6.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.6.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.7.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.7.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.7.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.7.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.7.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.7.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer2.7.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer2.7.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer2.7.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.6.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.6.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.6.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.6.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.6.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.6.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.6.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.6.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.6.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.7.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.7.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.7.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.7.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.7.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.7.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.7.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.7.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.7.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.8.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.8.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.8.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.8.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.8.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.8.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.8.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.8.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.8.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.9.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.9.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.9.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.9.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.9.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.9.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.9.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.9.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.9.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.10.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.10.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.10.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.10.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.10.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.10.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.10.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.10.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.10.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.11.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.11.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.11.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.11.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.11.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.11.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.11.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.11.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.11.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.12.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.12.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.12.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.12.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.12.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.12.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.12.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.12.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.12.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.13.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.13.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.13.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.13.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.13.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.13.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.13.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.13.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.13.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.14.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.14.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.14.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.14.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.14.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.14.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.14.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.14.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.14.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.15.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.15.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.15.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.15.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.15.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.15.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.15.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.15.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.15.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.16.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.16.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.16.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.16.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.16.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.16.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.16.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.16.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.16.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.17.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.17.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.17.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.17.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.17.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.17.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.17.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.17.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.17.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.18.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.18.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.18.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.18.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.18.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.18.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.18.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.18.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.18.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.19.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.19.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.19.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.19.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.19.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.19.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.19.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.19.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.19.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.20.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.20.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.20.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.20.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.20.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.20.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.20.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.20.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.20.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.21.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.21.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.21.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.21.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.21.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.21.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.21.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.21.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.21.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.22.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.22.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.22.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.22.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.22.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.22.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.22.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.22.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.22.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.23.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.23.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.23.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.23.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.23.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.23.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.23.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.23.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.23.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.24.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.24.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.24.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.24.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.24.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.24.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.24.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.24.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.24.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.25.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.25.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.25.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.25.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.25.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.25.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.25.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.25.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.25.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.26.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.26.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.26.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.26.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.26.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.26.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.26.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.26.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.26.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.27.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.27.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.27.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.27.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.27.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.27.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.27.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.27.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.27.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.28.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.28.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.28.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.28.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.28.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.28.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.28.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.28.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.28.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.29.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.29.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.29.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.29.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.29.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.29.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.29.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.29.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.29.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.30.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.30.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.30.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.30.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.30.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.30.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.30.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.30.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.30.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.31.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.31.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.31.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.31.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.31.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.31.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.31.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.31.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.31.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.32.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.32.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.32.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.32.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.32.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.32.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.32.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.32.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.32.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.33.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.33.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.33.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.33.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.33.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.33.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.33.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.33.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.33.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.34.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.34.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.34.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.34.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.34.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.34.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.34.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.34.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.34.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.35.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.35.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.35.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.35.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.35.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.35.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer3.35.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer3.35.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer3.35.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-06-30 01:34:52 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-06-30 01:34:52 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-06-30 01:34:52 - -----------no weight decay layers--------------
2022-06-30 01:34:52 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.7.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.7.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.7.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.7.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.7.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.7.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.7.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.7.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.7.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.7.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.7.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.7.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.8.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.8.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.8.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.8.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.8.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.8.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.9.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.9.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.9.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.9.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.9.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.9.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.10.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.10.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.10.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.10.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.10.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.10.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.11.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.11.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.11.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.11.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.11.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.11.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.12.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.12.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.12.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.12.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.12.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.12.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.13.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.13.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.13.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.13.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.13.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.13.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.14.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.14.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.14.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.14.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.14.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.14.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.15.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.15.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.15.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.15.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.15.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.15.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.16.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.16.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.16.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.16.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.16.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.16.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.17.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.17.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.17.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.17.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.17.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.17.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.18.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.18.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.18.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.18.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.18.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.18.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.19.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.19.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.19.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.19.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.19.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.19.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.20.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.20.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.20.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.20.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.20.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.20.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.21.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.21.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.21.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.21.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.21.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.21.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.22.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.22.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.22.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.22.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.22.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.22.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.23.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.23.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.23.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.23.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.23.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.23.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.24.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.24.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.24.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.24.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.24.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.24.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.25.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.25.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.25.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.25.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.25.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.25.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.26.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.26.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.26.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.26.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.26.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.26.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.27.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.27.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.27.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.27.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.27.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.27.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.28.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.28.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.28.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.28.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.28.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.28.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.29.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.29.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.29.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.29.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.29.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.29.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.30.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.30.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.30.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.30.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.30.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.30.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.31.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.31.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.31.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.31.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.31.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.31.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.32.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.32.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.32.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.32.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.32.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.32.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.33.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.33.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.33.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.33.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.33.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.33.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.34.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.34.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.34.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.34.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.34.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.34.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.35.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.35.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.35.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.35.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.35.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.35.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-30 01:34:52 - -------------weight decay layers---------------
2022-06-30 01:34:52 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer1.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.6.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.6.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.6.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.7.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.7.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer2.7.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.6.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.6.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.6.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.7.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.7.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.7.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.8.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.8.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.8.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.9.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.9.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.9.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.10.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.10.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.10.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.11.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.11.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.11.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.12.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.12.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.12.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.13.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.13.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.13.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.14.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.14.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.14.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.15.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.15.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.15.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.16.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.16.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.16.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.17.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.17.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.17.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.18.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.18.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.18.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.19.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.19.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.19.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.20.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.20.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.20.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.21.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.21.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.21.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.22.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.22.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.22.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.23.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.23.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.23.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.24.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.24.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.24.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.25.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.25.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.25.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.26.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.26.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.26.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.27.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.27.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.27.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.28.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.28.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.28.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.29.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.29.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.29.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.30.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.30.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.30.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.31.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.31.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.31.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.32.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.32.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.32.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.33.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.33.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.33.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.34.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.34.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.34.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.35.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.35.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer3.35.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-30 01:34:52 - epoch 001 lr: 0.100000
2022-06-30 01:35:40 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9368
2022-06-30 01:36:21 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.9064
2022-06-30 01:37:02 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.9021
2022-06-30 01:37:44 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.8949
2022-06-30 01:38:25 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.8132
2022-06-30 01:39:07 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.8360
2022-06-30 01:39:48 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.7698
2022-06-30 01:40:29 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.7259
2022-06-30 01:41:11 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.5413
2022-06-30 01:41:52 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.5785
2022-06-30 01:42:34 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.3957
2022-06-30 01:43:15 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 6.3127
2022-06-30 01:43:56 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 6.4162
2022-06-30 01:44:38 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 6.3399
2022-06-30 01:45:19 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 6.2912
2022-06-30 01:46:01 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 6.3156
2022-06-30 01:46:42 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 6.1293
2022-06-30 01:47:24 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 6.1086
2022-06-30 01:48:05 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 6.0899
2022-06-30 01:48:47 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.9327
2022-06-30 01:49:28 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.9225
2022-06-30 01:50:10 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.8331
2022-06-30 01:50:51 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 5.8494
2022-06-30 01:51:33 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 5.7625
2022-06-30 01:52:14 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.6098
2022-06-30 01:52:56 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.7174
2022-06-30 01:53:37 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.7291
2022-06-30 01:54:19 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 5.5269
2022-06-30 01:55:00 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 5.4427
2022-06-30 01:55:42 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.5447
2022-06-30 01:56:23 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.5111
2022-06-30 01:57:05 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 5.3727
2022-06-30 01:57:47 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 5.3401
2022-06-30 01:58:28 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 5.1262
2022-06-30 01:59:10 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 5.1809
2022-06-30 01:59:52 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 5.2519
2022-06-30 02:00:33 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 5.2635
2022-06-30 02:01:15 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 5.0214
2022-06-30 02:01:56 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 5.1007
2022-06-30 02:02:38 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 5.0092
2022-06-30 02:03:19 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 5.1429
2022-06-30 02:04:01 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.9106
2022-06-30 02:04:43 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.8777
2022-06-30 02:05:24 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.6451
2022-06-30 02:06:06 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.9036
2022-06-30 02:06:48 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.9256
2022-06-30 02:07:29 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.6551
2022-06-30 02:08:11 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 5.0107
2022-06-30 02:08:52 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.7579
2022-06-30 02:09:34 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.5697
2022-06-30 02:09:36 - train: epoch 001, train_loss: 5.7562
2022-06-30 02:10:50 - eval: epoch: 001, acc1: 13.658%, acc5: 32.374%, test_loss: 4.5425, per_image_load_time: 2.007ms, per_image_inference_time: 0.876ms
2022-06-30 02:10:51 - until epoch: 001, best_acc1: 13.658%
2022-06-30 02:10:51 - epoch 002 lr: 0.100000
2022-06-30 02:11:38 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.5697
2022-06-30 02:12:20 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.4072
2022-06-30 02:13:02 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.8337
2022-06-30 02:13:44 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.5567
2022-06-30 02:14:25 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.3462
2022-06-30 02:15:07 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.3674
2022-06-30 02:15:48 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.5842
2022-06-30 02:16:30 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 4.0946
2022-06-30 02:17:12 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.9683
2022-06-30 02:17:53 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.4454
2022-06-30 02:18:35 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.4473
2022-06-30 02:19:17 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.3010
2022-06-30 02:19:58 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.1965
2022-06-30 02:20:40 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.3623
2022-06-30 02:21:22 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.1582
2022-06-30 02:22:03 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 4.0461
2022-06-30 02:22:45 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.1905
2022-06-30 02:23:27 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.2475
2022-06-30 02:24:09 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.9936
2022-06-30 02:24:51 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.7360
2022-06-30 02:25:32 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.9816
2022-06-30 02:26:14 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.7659
2022-06-30 02:26:56 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 4.0098
2022-06-30 02:27:38 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.8627
2022-06-30 02:28:20 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.8380
2022-06-30 02:29:02 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.7907
2022-06-30 02:29:44 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 4.0010
2022-06-30 02:30:26 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.8198
2022-06-30 02:31:08 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.8305
2022-06-30 02:31:50 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.7168
2022-06-30 02:32:32 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.7654
2022-06-30 02:33:14 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.7141
2022-06-30 02:33:56 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.7876
2022-06-30 02:34:38 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.8806
2022-06-30 02:35:20 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.6837
2022-06-30 02:36:02 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.7553
2022-06-30 02:36:44 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.7767
2022-06-30 02:37:26 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.5025
2022-06-30 02:38:09 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.6943
2022-06-30 02:38:51 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.5972
2022-06-30 02:39:33 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.6179
2022-06-30 02:40:15 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.4944
2022-06-30 02:40:57 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.5987
2022-06-30 02:41:39 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.4038
2022-06-30 02:42:21 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.4389
2022-06-30 02:43:03 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.5253
2022-06-30 02:43:45 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.5216
2022-06-30 02:44:28 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.4554
2022-06-30 02:45:10 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.3910
2022-06-30 02:45:52 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.3338
2022-06-30 02:45:54 - train: epoch 002, train_loss: 3.9357
2022-06-30 02:47:08 - eval: epoch: 002, acc1: 30.694%, acc5: 56.644%, test_loss: 3.6374, per_image_load_time: 2.006ms, per_image_inference_time: 0.880ms
2022-06-30 02:47:09 - until epoch: 002, best_acc1: 30.694%
2022-06-30 02:47:09 - epoch 003 lr: 0.100000
2022-06-30 02:47:57 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.5302
2022-06-30 02:48:38 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.5344
2022-06-30 02:49:21 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.3667
2022-06-30 02:50:03 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.3522
2022-06-30 02:50:45 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.4309
2022-06-30 02:51:27 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.2832
2022-06-30 02:52:08 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.5164
2022-06-30 02:52:50 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.5020
2022-06-30 02:53:32 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.2182
2022-06-30 02:54:14 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.1625
2022-06-30 02:54:56 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.1824
2022-06-30 02:55:38 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.1879
2022-06-30 02:56:20 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.2866
2022-06-30 02:57:02 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.0950
2022-06-30 02:57:44 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.5029
2022-06-30 02:58:26 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.2143
2022-06-30 02:59:07 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.1084
2022-06-30 02:59:49 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.0863
2022-06-30 03:00:31 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.3161
2022-06-30 03:01:13 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.4681
2022-06-30 03:01:55 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.3750
2022-06-30 03:02:37 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.5993
2022-06-30 03:03:19 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.2056
2022-06-30 03:04:01 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.0841
2022-06-30 03:04:43 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.1428
2022-06-30 03:05:24 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.1702
2022-06-30 03:06:06 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.4156
2022-06-30 03:06:48 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.0679
2022-06-30 03:07:30 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.1207
2022-06-30 03:08:12 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.2078
2022-06-30 03:08:53 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.2732
2022-06-30 03:09:35 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.2148
2022-06-30 03:10:17 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.0836
2022-06-30 03:10:59 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.2487
2022-06-30 03:11:41 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.8941
2022-06-30 03:12:23 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.0444
2022-06-30 03:13:05 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 2.9815
2022-06-30 03:13:47 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.1735
2022-06-30 03:14:28 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.2791
2022-06-30 03:15:11 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.0118
2022-06-30 03:15:53 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.1360
2022-06-30 03:16:34 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.9571
2022-06-30 03:17:16 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.7575
2022-06-30 03:17:58 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.9255
2022-06-30 03:18:40 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 2.9615
2022-06-30 03:19:22 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.0009
2022-06-30 03:20:04 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.7380
2022-06-30 03:20:46 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.0992
2022-06-30 03:21:28 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.0342
2022-06-30 03:22:09 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 2.9715
2022-06-30 03:22:12 - train: epoch 003, train_loss: 3.1523
2022-06-30 03:23:26 - eval: epoch: 003, acc1: 38.116%, acc5: 64.366%, test_loss: 2.9396, per_image_load_time: 1.756ms, per_image_inference_time: 0.878ms
2022-06-30 03:23:27 - until epoch: 003, best_acc1: 38.116%
2022-06-30 03:23:27 - epoch 004 lr: 0.100000
2022-06-30 03:24:14 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 2.9576
2022-06-30 03:24:56 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.8158
2022-06-30 03:25:37 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.8613
2022-06-30 03:26:18 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.7779
2022-06-30 03:27:00 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.7277
2022-06-30 03:27:41 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.0448
2022-06-30 03:28:23 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 2.9106
2022-06-30 03:29:04 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.7303
2022-06-30 03:29:46 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.5671
2022-06-30 03:30:27 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.7498
2022-06-30 03:31:09 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 2.8704
2022-06-30 03:31:51 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.6486
2022-06-30 03:32:33 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.6478
2022-06-30 03:33:14 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 2.8759
2022-06-30 03:33:56 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 2.8876
2022-06-30 03:34:37 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.7608
2022-06-30 03:35:19 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.8843
2022-06-30 03:36:01 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 2.9630
2022-06-30 03:36:43 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 2.8853
2022-06-30 03:37:24 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.8186
2022-06-30 03:38:06 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.8664
2022-06-30 03:38:48 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.7929
2022-06-30 03:39:29 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.5720
2022-06-30 03:40:11 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.6313
2022-06-30 03:40:53 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.8626
2022-06-30 03:41:35 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.8825
2022-06-30 03:42:16 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.5162
2022-06-30 03:42:58 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.7471
2022-06-30 03:43:40 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.6258
2022-06-30 03:44:21 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.6454
2022-06-30 03:45:03 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.7728
2022-06-30 03:45:45 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.6119
2022-06-30 03:46:27 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.8304
2022-06-30 03:47:08 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.8546
2022-06-30 03:47:50 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.7148
2022-06-30 03:48:32 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.5355
2022-06-30 03:49:13 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.7215
2022-06-30 03:49:55 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.6709
2022-06-30 03:50:37 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.6568
2022-06-30 03:51:19 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.4090
2022-06-30 03:52:01 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.7225
2022-06-30 03:52:42 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.4709
2022-06-30 03:53:24 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.4823
2022-06-30 03:54:06 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.5255
2022-06-30 03:54:48 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.1888
2022-06-30 03:55:29 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.6259
2022-06-30 03:56:11 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.4901
2022-06-30 03:56:53 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.5300
2022-06-30 03:57:35 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.7453
2022-06-30 03:58:16 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.8054
2022-06-30 03:58:18 - train: epoch 004, train_loss: 2.7667
2022-06-30 03:59:33 - eval: epoch: 004, acc1: 46.138%, acc5: 72.530%, test_loss: 2.3621, per_image_load_time: 1.994ms, per_image_inference_time: 0.902ms
2022-06-30 03:59:34 - until epoch: 004, best_acc1: 46.138%
2022-06-30 03:59:34 - epoch 005 lr: 0.100000
2022-06-30 04:00:21 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.7310
2022-06-30 04:01:03 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.7305
2022-06-30 04:01:45 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.6801
2022-06-30 04:02:26 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.5268
2022-06-30 04:03:08 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.4656
2022-06-30 04:03:50 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.6212
2022-06-30 04:04:31 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.7095
2022-06-30 04:05:13 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 2.7543
2022-06-30 04:05:54 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.5260
2022-06-30 04:06:36 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.6624
2022-06-30 04:07:18 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.6934
2022-06-30 04:08:00 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.6121
2022-06-30 04:08:41 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.5915
2022-06-30 04:09:23 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.6954
2022-06-30 04:10:04 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.4097
2022-06-30 04:10:46 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.3654
2022-06-30 04:11:28 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.4693
2022-06-30 04:12:09 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.5888
2022-06-30 04:12:51 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.4819
2022-06-30 04:13:32 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.5295
2022-06-30 04:14:14 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.3367
2022-06-30 04:14:56 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.3430
2022-06-30 04:15:37 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.2591
2022-06-30 04:16:19 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.4052
2022-06-30 04:17:01 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.7534
2022-06-30 04:17:42 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.6618
2022-06-30 04:18:24 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.6457
2022-06-30 04:19:05 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.4902
2022-06-30 04:19:47 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.3806
2022-06-30 04:20:28 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.5828
2022-06-30 04:21:10 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.5493
2022-06-30 04:21:52 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.6034
2022-06-30 04:22:34 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.3637
2022-06-30 04:23:15 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.3972
2022-06-30 04:23:57 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.4552
2022-06-30 04:24:39 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.6604
2022-06-30 04:25:20 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.3822
2022-06-30 04:26:02 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.2876
2022-06-30 04:26:44 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.8366
2022-06-30 04:27:26 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.4164
2022-06-30 04:28:07 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.4749
2022-06-30 04:28:49 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.5977
2022-06-30 04:29:31 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.3597
2022-06-30 04:30:13 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.5093
2022-06-30 04:30:54 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.5090
2022-06-30 04:31:36 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.4744
2022-06-30 04:32:18 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.2477
2022-06-30 04:33:00 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.3742
2022-06-30 04:33:41 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.6163
2022-06-30 04:34:23 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.3889
2022-06-30 04:34:25 - train: epoch 005, train_loss: 2.5450
2022-06-30 04:35:39 - eval: epoch: 005, acc1: 47.934%, acc5: 74.160%, test_loss: 2.2653, per_image_load_time: 1.995ms, per_image_inference_time: 0.879ms
2022-06-30 04:35:40 - until epoch: 005, best_acc1: 47.934%
2022-06-30 04:35:40 - epoch 006 lr: 0.100000
2022-06-30 04:36:28 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.4002
2022-06-30 04:37:10 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.4780
2022-06-30 04:37:51 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.3107
2022-06-30 04:38:33 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.5156
2022-06-30 04:39:14 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.4733
2022-06-30 04:39:56 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.5135
2022-06-30 04:40:37 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.5548
2022-06-30 04:41:19 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.4156
2022-06-30 04:42:00 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.3182
2022-06-30 04:42:42 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.3169
2022-06-30 04:43:23 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.3975
2022-06-30 04:44:05 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.4243
2022-06-30 04:44:47 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.5645
2022-06-30 04:45:28 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.5225
2022-06-30 04:46:10 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.5336
2022-06-30 04:46:51 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.3463
2022-06-30 04:47:33 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.5894
2022-06-30 04:48:14 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.4901
2022-06-30 04:48:56 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.3302
2022-06-30 04:49:37 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.5805
2022-06-30 04:50:19 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.5081
2022-06-30 04:51:00 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.3501
2022-06-30 04:51:42 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.2595
2022-06-30 04:52:24 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.4539
2022-06-30 04:53:05 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.6405
2022-06-30 04:53:47 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.3251
2022-06-30 04:54:28 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.5395
2022-06-30 04:55:10 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.2326
2022-06-30 04:55:51 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.5336
2022-06-30 04:56:33 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.3483
2022-06-30 04:57:15 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.2378
2022-06-30 04:57:56 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.2617
2022-06-30 04:58:38 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.2064
2022-06-30 04:59:19 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.5286
2022-06-30 05:00:01 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.3814
2022-06-30 05:00:42 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.3836
2022-06-30 05:01:24 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.4389
2022-06-30 05:02:06 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.2170
2022-06-30 05:02:48 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.3375
2022-06-30 05:03:30 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.5735
2022-06-30 05:04:12 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.3412
2022-06-30 05:04:53 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.2528
2022-06-30 05:05:35 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.4149
2022-06-30 05:06:17 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.3576
2022-06-30 05:06:59 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.3524
2022-06-30 05:07:41 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.3017
2022-06-30 05:08:22 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.3560
2022-06-30 05:09:04 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.4457
2022-06-30 05:09:46 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.4203
2022-06-30 05:10:28 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.1740
2022-06-30 05:10:30 - train: epoch 006, train_loss: 2.4036
2022-06-30 05:11:44 - eval: epoch: 006, acc1: 50.690%, acc5: 76.222%, test_loss: 2.1313, per_image_load_time: 1.985ms, per_image_inference_time: 0.882ms
2022-06-30 05:11:45 - until epoch: 006, best_acc1: 50.690%
2022-06-30 05:11:45 - epoch 007 lr: 0.100000
2022-06-30 05:12:33 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.1344
2022-06-30 05:13:14 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.4710
2022-06-30 05:13:56 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.6009
2022-06-30 05:14:38 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.4970
2022-06-30 05:15:19 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.3056
2022-06-30 05:16:01 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.4321
2022-06-30 05:16:42 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.2964
2022-06-30 05:17:24 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.2943
2022-06-30 05:18:05 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.3174
2022-06-30 05:18:47 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.2721
2022-06-30 05:19:28 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.1973
2022-06-30 05:20:10 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.3474
2022-06-30 05:20:52 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.2958
2022-06-30 05:21:33 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.2889
2022-06-30 05:22:15 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.4591
2022-06-30 05:22:57 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.3742
2022-06-30 05:23:38 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.4462
2022-06-30 05:24:20 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.2306
2022-06-30 05:25:02 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.3671
2022-06-30 05:25:43 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.1589
2022-06-30 05:26:25 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.4007
2022-06-30 05:27:07 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.1546
2022-06-30 05:27:49 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.3290
2022-06-30 05:28:30 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.3588
2022-06-30 05:29:12 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.2990
2022-06-30 05:29:54 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.2493
2022-06-30 05:30:36 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.2329
2022-06-30 05:31:17 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.3299
2022-06-30 05:31:59 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.2405
2022-06-30 05:32:41 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.3312
2022-06-30 05:33:22 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.1346
2022-06-30 05:34:04 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.2333
2022-06-30 05:34:46 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.5228
2022-06-30 05:35:28 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.1552
2022-06-30 05:36:09 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.4158
2022-06-30 05:36:51 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.1307
2022-06-30 05:37:32 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.3021
2022-06-30 05:38:14 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.4907
2022-06-30 05:38:56 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.1136
2022-06-30 05:39:38 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.3931
2022-06-30 05:40:19 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.2450
2022-06-30 05:41:01 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.2048
2022-06-30 05:41:43 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.3653
2022-06-30 05:42:24 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.1205
2022-06-30 05:43:06 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.5160
2022-06-30 05:43:48 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.4653
2022-06-30 05:44:29 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.5066
2022-06-30 05:45:11 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.5491
2022-06-30 05:45:53 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.2609
2022-06-30 05:46:35 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.3315
2022-06-30 05:46:37 - train: epoch 007, train_loss: 2.3023
2022-06-30 05:47:51 - eval: epoch: 007, acc1: 49.206%, acc5: 75.098%, test_loss: 2.2079, per_image_load_time: 1.962ms, per_image_inference_time: 0.885ms
2022-06-30 05:47:52 - until epoch: 007, best_acc1: 50.690%
2022-06-30 05:47:52 - epoch 008 lr: 0.100000
2022-06-30 05:48:40 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.1423
2022-06-30 05:49:21 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.3392
2022-06-30 05:50:03 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.0607
2022-06-30 05:50:44 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.0818
2022-06-30 05:51:26 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.1476
2022-06-30 05:52:07 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.1647
2022-06-30 05:52:49 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.4711
2022-06-30 05:53:30 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.0919
2022-06-30 05:54:12 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.2766
2022-06-30 05:54:53 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.2089
2022-06-30 05:55:35 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.0434
2022-06-30 05:56:16 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.0891
2022-06-30 05:56:58 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.1763
2022-06-30 05:57:40 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.1684
2022-06-30 05:58:21 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.3138
2022-06-30 05:59:03 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.2927
2022-06-30 05:59:44 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.2248
2022-06-30 06:00:26 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.3951
2022-06-30 06:01:07 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 1.9733
2022-06-30 06:01:49 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.2043
2022-06-30 06:02:30 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.2206
2022-06-30 06:03:12 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.2357
2022-06-30 06:03:53 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.1997
2022-06-30 06:04:35 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.1784
2022-06-30 06:05:16 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.2275
2022-06-30 06:05:58 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.3635
2022-06-30 06:06:39 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.3012
2022-06-30 06:07:21 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.2245
2022-06-30 06:08:02 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.1366
2022-06-30 06:08:44 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.2248
2022-06-30 06:09:25 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.1670
2022-06-30 06:10:07 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.4774
2022-06-30 06:10:48 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.4159
2022-06-30 06:11:30 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.3100
2022-06-30 06:12:11 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.2835
2022-06-30 06:12:53 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.4014
2022-06-30 06:13:34 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.1595
2022-06-30 06:14:16 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.0738
2022-06-30 06:14:57 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.2886
2022-06-30 06:15:39 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.4979
2022-06-30 06:16:21 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.1539
2022-06-30 06:17:02 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.0956
2022-06-30 06:17:44 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 1.8703
2022-06-30 06:18:25 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.2204
2022-06-30 06:19:07 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.3961
2022-06-30 06:19:49 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.3871
2022-06-30 06:20:30 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.0412
2022-06-30 06:21:12 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.2126
2022-06-30 06:21:53 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.3837
2022-06-30 06:22:35 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.1130
2022-06-30 06:22:37 - train: epoch 008, train_loss: 2.2231
2022-06-30 06:23:51 - eval: epoch: 008, acc1: 55.116%, acc5: 79.630%, test_loss: 1.9145, per_image_load_time: 1.984ms, per_image_inference_time: 0.882ms
2022-06-30 06:23:52 - until epoch: 008, best_acc1: 55.116%
2022-06-30 06:23:52 - epoch 009 lr: 0.100000
2022-06-30 06:24:39 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 1.9331
2022-06-30 06:25:21 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.0555
2022-06-30 06:26:02 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 1.8997
2022-06-30 06:26:44 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.3767
2022-06-30 06:27:25 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.1328
2022-06-30 06:28:07 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.1947
2022-06-30 06:28:49 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.0785
2022-06-30 06:29:30 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 1.9340
2022-06-30 06:30:12 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.0340
2022-06-30 06:30:54 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.0139
2022-06-30 06:31:35 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.4659
2022-06-30 06:32:17 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.2599
2022-06-30 06:32:58 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.3245
2022-06-30 06:33:40 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 1.8865
2022-06-30 06:34:21 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 1.9536
2022-06-30 06:35:03 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.1905
2022-06-30 06:35:45 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.3460
2022-06-30 06:36:26 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.0890
2022-06-30 06:37:08 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 1.9550
2022-06-30 06:37:50 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 1.7910
2022-06-30 06:38:31 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.2174
2022-06-30 06:39:13 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.1791
2022-06-30 06:39:55 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 1.9461
2022-06-30 06:40:37 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.0773
2022-06-30 06:41:18 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 1.9539
2022-06-30 06:42:00 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.1280
2022-06-30 06:42:42 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.0856
2022-06-30 06:43:24 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.1624
2022-06-30 06:44:05 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 1.9656
2022-06-30 06:44:47 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 1.9934
2022-06-30 06:45:29 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.2510
2022-06-30 06:46:11 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.2534
2022-06-30 06:46:53 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.1559
2022-06-30 06:47:34 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.3147
2022-06-30 06:48:16 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.3078
2022-06-30 06:48:58 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.0854
2022-06-30 06:49:40 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.2994
2022-06-30 06:50:22 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.3558
2022-06-30 06:51:03 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 1.8617
2022-06-30 06:51:45 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.3998
2022-06-30 06:52:27 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.0976
2022-06-30 06:53:09 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.1007
2022-06-30 06:53:51 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.1968
2022-06-30 06:54:33 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.2239
2022-06-30 06:55:14 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.0071
2022-06-30 06:55:56 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.2455
2022-06-30 06:56:38 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.4264
2022-06-30 06:57:20 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.2892
2022-06-30 06:58:02 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.1495
2022-06-30 06:58:43 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.0949
2022-06-30 06:58:45 - train: epoch 009, train_loss: 2.1650
2022-06-30 07:00:00 - eval: epoch: 009, acc1: 54.918%, acc5: 80.000%, test_loss: 1.9065, per_image_load_time: 2.011ms, per_image_inference_time: 0.893ms
2022-06-30 07:00:01 - until epoch: 009, best_acc1: 55.116%
2022-06-30 07:00:01 - epoch 010 lr: 0.100000
2022-06-30 07:00:48 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.0921
2022-06-30 07:01:30 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.1698
2022-06-30 07:02:11 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.1398
2022-06-30 07:02:52 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.2139
2022-06-30 07:03:34 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.0769
2022-06-30 07:04:15 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.1417
2022-06-30 07:04:57 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.1450
2022-06-30 07:05:38 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 1.9792
2022-06-30 07:06:20 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.0547
2022-06-30 07:07:01 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 1.8975
2022-06-30 07:07:43 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.0741
2022-06-30 07:08:25 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 1.9750
2022-06-30 07:09:07 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 1.9960
2022-06-30 07:09:48 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.1351
2022-06-30 07:10:30 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 1.8785
2022-06-30 07:11:11 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.2757
2022-06-30 07:11:53 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.0933
2022-06-30 07:12:35 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.1335
2022-06-30 07:13:16 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.1839
2022-06-30 07:13:58 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.0651
2022-06-30 07:14:39 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 1.9597
2022-06-30 07:15:21 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.4088
2022-06-30 07:16:03 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.3195
2022-06-30 07:16:44 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.2908
2022-06-30 07:17:26 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.1371
2022-06-30 07:18:07 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.2516
2022-06-30 07:18:49 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 1.8223
2022-06-30 07:19:30 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.1347
2022-06-30 07:20:12 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.1488
2022-06-30 07:20:54 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.0825
2022-06-30 07:21:36 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.2743
2022-06-30 07:22:17 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.1105
2022-06-30 07:22:58 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.2993
2022-06-30 07:23:40 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.2615
2022-06-30 07:24:21 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.4244
2022-06-30 07:25:03 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.2873
2022-06-30 07:25:44 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 1.9865
2022-06-30 07:26:26 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.1872
2022-06-30 07:27:08 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 1.8750
2022-06-30 07:27:49 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.0669
2022-06-30 07:28:31 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 1.9487
2022-06-30 07:29:13 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.1600
2022-06-30 07:29:54 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.0384
2022-06-30 07:30:36 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.0505
2022-06-30 07:31:17 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 1.8807
2022-06-30 07:31:59 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.1437
2022-06-30 07:32:41 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.1614
2022-06-30 07:33:23 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.1282
2022-06-30 07:34:04 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.0569
2022-06-30 07:34:46 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 1.8527
2022-06-30 07:34:48 - train: epoch 010, train_loss: 2.1178
2022-06-30 07:36:03 - eval: epoch: 010, acc1: 54.562%, acc5: 79.376%, test_loss: 1.9379, per_image_load_time: 1.956ms, per_image_inference_time: 0.872ms
2022-06-30 07:36:03 - until epoch: 010, best_acc1: 55.116%
2022-06-30 07:36:03 - epoch 011 lr: 0.100000
2022-06-30 07:36:51 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 1.8805
2022-06-30 07:37:32 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.2246
2022-06-30 07:38:14 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 1.9414
2022-06-30 07:38:55 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.2030
2022-06-30 07:39:37 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.0596
2022-06-30 07:40:18 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.1527
2022-06-30 07:41:00 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.0578
2022-06-30 07:41:41 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.1409
2022-06-30 07:42:23 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.2507
2022-06-30 07:43:05 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.0640
2022-06-30 07:43:46 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.1409
2022-06-30 07:44:28 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.4703
2022-06-30 07:45:09 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.3480
2022-06-30 07:45:51 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.1134
2022-06-30 07:46:32 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.0705
2022-06-30 07:47:14 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.1685
2022-06-30 07:47:56 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.0875
2022-06-30 07:48:37 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.0078
2022-06-30 07:49:19 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 1.8393
2022-06-30 07:50:01 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.2343
2022-06-30 07:50:42 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.0316
2022-06-30 07:51:24 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 1.9986
2022-06-30 07:52:06 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.2485
2022-06-30 07:52:47 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 1.8965
2022-06-30 07:53:29 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.3588
2022-06-30 07:54:10 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.1045
2022-06-30 07:54:52 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.1305
2022-06-30 07:55:34 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 1.7770
2022-06-30 07:56:15 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.1736
2022-06-30 07:56:57 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.3555
2022-06-30 07:57:38 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.1367
2022-06-30 07:58:20 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 1.9183
2022-06-30 07:59:01 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.2010
2022-06-30 07:59:43 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.0404
2022-06-30 08:00:24 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 1.9931
2022-06-30 08:01:06 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.1542
2022-06-30 08:01:47 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.1612
2022-06-30 08:02:29 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 1.8370
2022-06-30 08:03:10 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.2047
2022-06-30 08:03:52 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.0398
2022-06-30 08:04:34 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 1.8317
2022-06-30 08:05:15 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.0853
2022-06-30 08:05:57 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.2263
2022-06-30 08:06:38 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.0626
2022-06-30 08:07:20 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 1.9211
2022-06-30 08:08:02 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 1.9948
2022-06-30 08:08:43 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 1.8754
2022-06-30 08:09:25 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 1.9677
2022-06-30 08:10:07 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 1.9084
2022-06-30 08:10:48 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.0328
2022-06-30 08:10:50 - train: epoch 011, train_loss: 2.0878
2022-06-30 08:12:04 - eval: epoch: 011, acc1: 56.716%, acc5: 80.924%, test_loss: 1.8371, per_image_load_time: 1.997ms, per_image_inference_time: 0.887ms
2022-06-30 08:12:05 - until epoch: 011, best_acc1: 56.716%
2022-06-30 08:12:05 - epoch 012 lr: 0.100000
2022-06-30 08:12:52 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 1.9874
2022-06-30 08:13:34 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 1.9788
2022-06-30 08:14:15 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.0051
2022-06-30 08:14:57 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.0795
2022-06-30 08:15:39 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.2825
2022-06-30 08:16:20 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 1.8522
2022-06-30 08:17:02 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 1.8868
2022-06-30 08:17:44 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.1337
2022-06-30 08:18:25 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.2398
2022-06-30 08:19:07 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 1.9673
2022-06-30 08:19:49 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.4291
2022-06-30 08:20:30 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 1.7618
2022-06-30 08:21:12 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 1.9763
2022-06-30 08:21:53 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.1867
2022-06-30 08:22:35 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 1.9056
2022-06-30 08:23:16 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 1.9819
2022-06-30 08:23:58 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 1.9756
2022-06-30 08:24:40 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.0394
2022-06-30 08:25:21 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 1.9794
2022-06-30 08:26:03 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.1739
2022-06-30 08:26:45 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.1429
2022-06-30 08:27:26 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.2521
2022-06-30 08:28:08 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.0769
2022-06-30 08:28:49 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.0940
2022-06-30 08:29:31 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 1.8693
2022-06-30 08:30:13 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 1.7456
2022-06-30 08:30:54 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 1.9469
2022-06-30 08:31:36 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.1004
2022-06-30 08:32:18 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 1.8960
2022-06-30 08:32:59 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 1.9747
2022-06-30 08:33:41 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.2924
2022-06-30 08:34:23 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 1.8322
2022-06-30 08:35:04 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.0844
2022-06-30 08:35:46 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.0886
2022-06-30 08:36:27 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.1646
2022-06-30 08:37:09 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 1.9465
2022-06-30 08:37:51 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.0107
2022-06-30 08:38:32 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.0496
2022-06-30 08:39:14 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.0529
2022-06-30 08:39:55 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.0062
2022-06-30 08:40:37 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.0104
2022-06-30 08:41:18 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 1.9315
2022-06-30 08:42:00 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 1.9851
2022-06-30 08:42:42 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 1.8333
2022-06-30 08:43:23 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 1.9597
2022-06-30 08:44:05 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.2828
2022-06-30 08:44:47 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.0352
2022-06-30 08:45:28 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.1618
2022-06-30 08:46:10 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.0032
2022-06-30 08:46:52 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 1.8539
2022-06-30 08:46:54 - train: epoch 012, train_loss: 2.0652
2022-06-30 08:48:08 - eval: epoch: 012, acc1: 56.388%, acc5: 80.964%, test_loss: 1.8389, per_image_load_time: 1.750ms, per_image_inference_time: 0.857ms
2022-06-30 08:48:08 - until epoch: 012, best_acc1: 56.716%
2022-06-30 08:48:08 - epoch 013 lr: 0.100000
2022-06-30 08:48:56 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 1.7783
2022-06-30 08:49:37 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 1.9923
2022-06-30 08:50:19 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 1.9878
2022-06-30 08:51:00 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.0178
2022-06-30 08:51:41 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.1017
2022-06-30 08:52:23 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.0952
2022-06-30 08:53:04 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 1.9571
2022-06-30 08:53:46 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.1248
2022-06-30 08:54:27 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 1.9972
2022-06-30 08:55:09 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 1.9424
2022-06-30 08:55:50 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.0865
2022-06-30 08:56:32 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.1319
2022-06-30 08:57:13 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.1146
2022-06-30 08:57:55 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 1.7749
2022-06-30 08:58:36 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.0417
2022-06-30 08:59:18 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 1.7976
2022-06-30 08:59:59 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.0265
2022-06-30 09:00:41 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.1598
2022-06-30 09:01:23 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.1537
2022-06-30 09:02:04 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.2446
2022-06-30 09:02:46 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.3050
2022-06-30 09:03:27 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 1.8855
2022-06-30 09:04:09 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.0602
2022-06-30 09:04:50 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.0541
2022-06-30 09:05:32 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 1.9668
2022-06-30 09:06:14 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 1.9511
2022-06-30 09:06:55 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 1.9086
2022-06-30 09:07:37 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.0637
2022-06-30 09:08:18 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.1916
2022-06-30 09:09:00 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 1.9397
2022-06-30 09:09:41 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 1.8753
2022-06-30 09:10:23 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.0479
2022-06-30 09:11:05 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 1.8887
2022-06-30 09:11:46 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 1.9700
2022-06-30 09:12:28 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 1.9204
2022-06-30 09:13:09 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.3385
2022-06-30 09:13:51 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 1.7814
2022-06-30 09:14:32 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.1802
2022-06-30 09:15:13 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.1898
2022-06-30 09:15:55 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.0854
2022-06-30 09:16:36 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.0049
2022-06-30 09:17:18 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 1.9439
2022-06-30 09:18:00 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 1.8930
2022-06-30 09:18:41 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 1.8935
2022-06-30 09:19:23 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 1.9383
2022-06-30 09:20:05 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.1171
2022-06-30 09:20:46 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.1585
2022-06-30 09:21:28 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.1402
2022-06-30 09:22:10 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.1190
2022-06-30 09:22:52 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.1026
2022-06-30 09:22:54 - train: epoch 013, train_loss: 2.0360
2022-06-30 09:24:09 - eval: epoch: 013, acc1: 56.950%, acc5: 81.486%, test_loss: 1.8213, per_image_load_time: 2.028ms, per_image_inference_time: 0.875ms
2022-06-30 09:24:10 - until epoch: 013, best_acc1: 56.950%
2022-06-30 09:24:10 - epoch 014 lr: 0.100000
2022-06-30 09:24:57 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.0365
2022-06-30 09:25:39 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.0525
2022-06-30 09:26:20 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 1.6863
2022-06-30 09:27:02 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 1.9594
2022-06-30 09:27:43 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 1.9278
2022-06-30 09:28:25 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 1.9194
2022-06-30 09:29:06 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 1.9918
2022-06-30 09:29:48 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.0312
2022-06-30 09:30:29 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.0568
2022-06-30 09:31:11 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.2479
2022-06-30 09:31:53 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 1.9988
2022-06-30 09:32:34 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.0200
2022-06-30 09:33:16 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.0564
2022-06-30 09:33:57 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.1334
2022-06-30 09:34:39 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.0674
2022-06-30 09:35:20 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.0017
2022-06-30 09:36:02 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.2207
2022-06-30 09:36:44 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.2363
2022-06-30 09:37:25 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.0159
2022-06-30 09:38:07 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.0091
2022-06-30 09:38:48 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.1386
2022-06-30 09:39:30 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.0908
2022-06-30 09:40:12 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 1.9515
2022-06-30 09:40:53 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.2868
2022-06-30 09:41:35 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 1.9738
2022-06-30 09:42:16 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 1.9999
2022-06-30 09:42:58 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 1.9184
2022-06-30 09:43:39 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.2367
2022-06-30 09:44:21 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.0560
2022-06-30 09:45:02 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.0712
2022-06-30 09:45:44 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 1.8140
2022-06-30 09:46:25 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 1.9920
2022-06-30 09:47:07 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 1.9281
2022-06-30 09:47:48 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 1.9413
2022-06-30 09:48:30 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 1.9020
2022-06-30 09:49:12 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 1.8668
2022-06-30 09:49:53 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 1.8881
2022-06-30 09:50:35 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.2125
2022-06-30 09:51:16 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 1.8617
2022-06-30 09:51:58 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.1274
2022-06-30 09:52:39 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 1.9236
2022-06-30 09:53:21 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 1.8562
2022-06-30 09:54:03 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 1.8072
2022-06-30 09:54:44 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 1.8258
2022-06-30 09:55:26 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 1.9741
2022-06-30 09:56:08 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.0175
2022-06-30 09:56:49 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 1.8421
2022-06-30 09:57:31 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 1.9091
2022-06-30 09:58:12 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 1.8148
2022-06-30 09:58:54 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.0359
2022-06-30 09:58:56 - train: epoch 014, train_loss: 2.0153
2022-06-30 10:00:10 - eval: epoch: 014, acc1: 56.512%, acc5: 81.094%, test_loss: 1.8418, per_image_load_time: 2.012ms, per_image_inference_time: 0.878ms
2022-06-30 10:00:11 - until epoch: 014, best_acc1: 56.950%
2022-06-30 10:00:11 - epoch 015 lr: 0.100000
2022-06-30 10:00:59 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 1.6503
2022-06-30 10:01:40 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.1037
2022-06-30 10:02:22 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.1223
2022-06-30 10:03:03 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 1.9831
2022-06-30 10:03:44 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 1.9687
2022-06-30 10:04:26 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.1439
2022-06-30 10:05:07 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 1.9068
2022-06-30 10:05:49 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 1.8039
2022-06-30 10:06:30 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 1.9462
2022-06-30 10:07:12 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 1.9809
2022-06-30 10:07:53 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 1.8242
2022-06-30 10:08:35 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.0284
2022-06-30 10:09:16 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.2689
2022-06-30 10:09:58 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 1.9034
2022-06-30 10:10:39 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 1.7627
2022-06-30 10:11:21 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 1.9770
2022-06-30 10:12:02 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.1639
2022-06-30 10:12:44 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 1.8418
2022-06-30 10:13:25 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 1.8405
2022-06-30 10:14:07 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 1.9752
2022-06-30 10:14:49 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 1.8735
2022-06-30 10:15:31 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.1413
2022-06-30 10:16:12 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 1.7454
2022-06-30 10:16:54 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.1217
2022-06-30 10:17:36 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.1004
2022-06-30 10:18:17 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 1.8521
2022-06-30 10:18:59 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.0799
2022-06-30 10:19:40 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.1911
2022-06-30 10:20:22 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.0624
2022-06-30 10:21:03 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 1.7919
2022-06-30 10:21:45 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 1.9926
2022-06-30 10:22:27 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 1.9546
2022-06-30 10:23:08 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 1.8371
2022-06-30 10:23:50 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.0434
2022-06-30 10:24:32 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.1508
2022-06-30 10:25:14 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.0372
2022-06-30 10:25:55 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 1.8582
2022-06-30 10:26:37 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 1.9727
2022-06-30 10:27:18 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.1385
2022-06-30 10:28:00 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 1.9568
2022-06-30 10:28:41 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.1498
2022-06-30 10:29:23 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 1.6934
2022-06-30 10:30:04 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 1.9842
2022-06-30 10:30:46 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.0190
2022-06-30 10:31:28 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 1.9356
2022-06-30 10:32:10 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 1.9015
2022-06-30 10:32:51 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 1.9996
2022-06-30 10:33:33 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 1.9087
2022-06-30 10:34:15 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 1.9847
2022-06-30 10:34:57 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.0453
2022-06-30 10:34:59 - train: epoch 015, train_loss: 1.9910
2022-06-30 10:36:13 - eval: epoch: 015, acc1: 58.884%, acc5: 82.898%, test_loss: 1.7207, per_image_load_time: 2.007ms, per_image_inference_time: 0.863ms
2022-06-30 10:36:14 - until epoch: 015, best_acc1: 58.884%
2022-06-30 10:36:14 - epoch 016 lr: 0.100000
2022-06-30 10:37:01 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 1.9461
2022-06-30 10:37:43 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 1.7420
2022-06-30 10:38:24 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.0278
2022-06-30 10:39:06 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.1776
2022-06-30 10:39:48 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 1.8015
2022-06-30 10:40:29 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.1885
2022-06-30 10:41:10 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 1.7568
2022-06-30 10:41:52 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.0390
2022-06-30 10:42:33 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.1230
2022-06-30 10:43:15 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 1.8841
2022-06-30 10:43:56 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 1.8306
2022-06-30 10:44:38 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 1.8341
2022-06-30 10:45:19 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.0350
2022-06-30 10:46:01 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 1.8272
2022-06-30 10:46:43 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.0425
2022-06-30 10:47:24 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.0171
2022-06-30 10:48:06 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 1.7988
2022-06-30 10:48:48 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 1.8177
2022-06-30 10:49:29 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 1.9495
2022-06-30 10:50:11 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 1.6675
2022-06-30 10:50:53 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.0890
2022-06-30 10:51:35 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.0690
2022-06-30 10:52:16 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.1550
2022-06-30 10:52:58 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.0782
2022-06-30 10:53:40 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 1.8796
2022-06-30 10:54:22 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.1196
2022-06-30 10:55:03 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 1.9151
2022-06-30 10:55:45 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 1.7755
2022-06-30 10:56:27 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.1196
2022-06-30 10:57:08 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.2414
2022-06-30 10:57:50 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.1349
2022-06-30 10:58:32 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.1367
2022-06-30 10:59:14 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.0108
2022-06-30 10:59:56 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 1.8588
2022-06-30 11:00:38 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 1.9654
2022-06-30 11:01:20 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 1.8773
2022-06-30 11:02:01 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.0587
2022-06-30 11:02:43 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.3032
2022-06-30 11:03:25 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.0119
2022-06-30 11:04:07 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.0368
2022-06-30 11:04:49 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 1.9617
2022-06-30 11:05:30 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.1016
2022-06-30 11:06:12 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 1.6850
2022-06-30 11:06:54 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 1.7893
2022-06-30 11:07:36 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.0625
2022-06-30 11:08:18 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 1.8665
2022-06-30 11:08:59 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.1554
2022-06-30 11:09:41 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 1.9507
2022-06-30 11:10:23 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 1.8894
2022-06-30 11:11:05 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.1575
2022-06-30 11:11:07 - train: epoch 016, train_loss: 1.9751
2022-06-30 11:12:21 - eval: epoch: 016, acc1: 57.818%, acc5: 82.072%, test_loss: 1.7736, per_image_load_time: 1.380ms, per_image_inference_time: 0.900ms
2022-06-30 11:12:22 - until epoch: 016, best_acc1: 58.884%
2022-06-30 11:12:22 - epoch 017 lr: 0.100000
2022-06-30 11:13:09 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 1.9594
2022-06-30 11:13:50 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.0536
2022-06-30 11:14:32 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.1538
2022-06-30 11:15:13 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 1.6788
2022-06-30 11:15:55 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 1.9387
2022-06-30 11:16:37 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.2080
2022-06-30 11:17:18 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 1.8791
2022-06-30 11:18:00 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 1.8031
2022-06-30 11:18:41 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 1.8568
2022-06-30 11:19:23 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 1.8697
2022-06-30 11:20:05 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.2611
2022-06-30 11:20:47 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.0724
2022-06-30 11:21:28 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 1.9841
2022-06-30 11:22:10 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.0931
2022-06-30 11:22:52 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 1.6172
2022-06-30 11:23:33 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 1.7929
2022-06-30 11:24:15 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 1.9368
2022-06-30 11:24:56 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 1.9782
2022-06-30 11:25:38 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 1.9039
2022-06-30 11:26:19 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.1456
2022-06-30 11:27:01 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 1.9313
2022-06-30 11:27:43 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 1.8428
2022-06-30 11:28:24 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 1.9306
2022-06-30 11:29:06 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 1.9673
2022-06-30 11:29:47 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.0775
2022-06-30 11:30:29 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 1.8249
2022-06-30 11:31:11 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 1.8861
2022-06-30 11:31:52 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.1720
2022-06-30 11:32:34 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.1936
2022-06-30 11:33:15 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 1.8227
2022-06-30 11:33:57 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.1110
2022-06-30 11:34:39 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 1.8624
2022-06-30 11:35:20 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.0824
2022-06-30 11:36:02 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 1.7569
2022-06-30 11:36:44 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 1.9236
2022-06-30 11:37:25 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.1613
2022-06-30 11:38:07 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.0108
2022-06-30 11:38:48 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.1611
2022-06-30 11:39:30 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 1.8130
2022-06-30 11:40:11 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 1.8810
2022-06-30 11:40:53 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.0328
2022-06-30 11:41:35 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 1.9856
2022-06-30 11:42:16 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.0107
2022-06-30 11:42:58 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 1.9496
2022-06-30 11:43:40 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.0397
2022-06-30 11:44:22 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 1.8549
2022-06-30 11:45:03 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.0334
2022-06-30 11:45:45 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.0584
2022-06-30 11:46:27 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 1.7322
2022-06-30 11:47:09 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 1.7987
2022-06-30 11:47:11 - train: epoch 017, train_loss: 1.9610
2022-06-30 11:48:25 - eval: epoch: 017, acc1: 55.940%, acc5: 80.248%, test_loss: 1.8791, per_image_load_time: 1.152ms, per_image_inference_time: 0.866ms
2022-06-30 11:48:26 - until epoch: 017, best_acc1: 58.884%
2022-06-30 11:48:26 - epoch 018 lr: 0.100000
2022-06-30 11:49:14 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 1.9476
2022-06-30 11:49:55 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 1.9099
2022-06-30 11:50:37 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.1296
2022-06-30 11:51:18 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.0737
2022-06-30 11:52:00 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 1.8901
2022-06-30 11:52:42 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.0401
2022-06-30 11:53:24 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 1.6988
2022-06-30 11:54:06 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 1.9428
2022-06-30 11:54:48 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 1.9670
2022-06-30 11:55:29 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 1.7560
2022-06-30 11:56:11 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.0902
2022-06-30 11:56:53 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 1.8095
2022-06-30 11:57:35 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.2725
2022-06-30 11:58:17 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 1.9765
2022-06-30 11:58:59 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.1155
2022-06-30 11:59:41 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 1.9411
2022-06-30 12:00:23 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 1.8366
2022-06-30 12:01:05 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 1.7297
2022-06-30 12:01:46 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.0477
2022-06-30 12:02:28 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.1865
2022-06-30 12:03:10 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.1318
2022-06-30 12:03:52 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 1.9441
2022-06-30 12:04:34 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 1.9400
2022-06-30 12:05:16 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 1.7937
2022-06-30 12:05:58 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 1.7738
2022-06-30 12:06:39 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 1.7787
2022-06-30 12:07:21 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.1254
2022-06-30 12:08:03 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 1.7846
2022-06-30 12:08:45 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 1.9888
2022-06-30 12:09:27 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 1.9412
2022-06-30 12:10:09 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.2512
2022-06-30 12:10:51 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 1.8317
2022-06-30 12:11:33 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 1.8891
2022-06-30 12:12:15 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 1.9309
2022-06-30 12:12:56 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.2497
2022-06-30 12:13:38 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 1.9426
2022-06-30 12:14:20 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.3040
2022-06-30 12:15:02 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.0151
2022-06-30 12:15:43 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.0252
2022-06-30 12:16:25 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 1.8219
2022-06-30 12:17:07 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 1.8686
2022-06-30 12:17:49 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 1.9106
2022-06-30 12:18:31 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 1.7127
2022-06-30 12:19:12 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.0740
2022-06-30 12:19:54 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 1.8506
2022-06-30 12:20:36 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 1.7779
2022-06-30 12:21:18 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.1846
2022-06-30 12:22:00 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.0021
2022-06-30 12:22:42 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 1.9597
2022-06-30 12:23:23 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.0457
2022-06-30 12:23:26 - train: epoch 018, train_loss: 1.9413
2022-06-30 12:24:40 - eval: epoch: 018, acc1: 56.428%, acc5: 80.912%, test_loss: 1.8442, per_image_load_time: 1.064ms, per_image_inference_time: 0.877ms
2022-06-30 12:24:41 - until epoch: 018, best_acc1: 58.884%
2022-06-30 12:24:41 - epoch 019 lr: 0.100000
2022-06-30 12:25:28 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 1.7741
2022-06-30 12:26:10 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.0978
2022-06-30 12:26:52 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.1204
2022-06-30 12:27:33 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 1.8726
2022-06-30 12:28:15 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 1.8631
2022-06-30 12:28:57 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 1.7787
2022-06-30 12:29:38 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 1.6895
2022-06-30 12:30:20 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.0974
2022-06-30 12:31:02 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 1.8818
2022-06-30 12:31:43 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 1.9995
2022-06-30 12:32:25 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 1.8634
2022-06-30 12:33:07 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 1.9436
2022-06-30 12:33:48 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.1100
2022-06-30 12:34:30 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 1.8788
2022-06-30 12:35:12 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.2940
2022-06-30 12:35:54 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 1.9644
2022-06-30 12:36:35 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 1.9804
2022-06-30 12:37:17 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 1.8803
2022-06-30 12:37:59 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.0905
2022-06-30 12:38:40 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 1.8502
2022-06-30 12:39:22 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 1.8069
2022-06-30 12:40:04 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 1.9820
2022-06-30 12:40:45 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 1.9395
2022-06-30 12:41:27 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.0641
2022-06-30 12:42:09 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 1.7897
2022-06-30 12:42:51 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.0251
2022-06-30 12:43:33 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 1.9928
2022-06-30 12:44:14 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 1.9805
2022-06-30 12:44:56 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 1.9611
2022-06-30 12:45:38 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.2229
2022-06-30 12:46:19 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.1102
2022-06-30 12:47:01 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.7280
2022-06-30 12:47:43 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.0405
2022-06-30 12:48:24 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 1.8622
2022-06-30 12:49:06 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.0482
2022-06-30 12:49:48 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 1.8443
2022-06-30 12:50:30 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.0033
2022-06-30 12:51:11 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.0308
2022-06-30 12:51:53 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 1.9221
2022-06-30 12:52:35 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 1.7767
2022-06-30 12:53:16 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 1.9649
2022-06-30 12:53:58 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 1.9589
2022-06-30 12:54:40 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 1.6965
2022-06-30 12:55:22 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.0679
2022-06-30 12:56:03 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.1730
2022-06-30 12:56:45 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 1.8246
2022-06-30 12:57:27 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 1.7649
2022-06-30 12:58:09 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 1.9267
2022-06-30 12:58:50 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 1.9745
2022-06-30 12:59:32 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 1.9302
2022-06-30 12:59:34 - train: epoch 019, train_loss: 1.9328
2022-06-30 13:00:48 - eval: epoch: 019, acc1: 56.928%, acc5: 81.334%, test_loss: 1.8107, per_image_load_time: 1.087ms, per_image_inference_time: 0.885ms
2022-06-30 13:00:49 - until epoch: 019, best_acc1: 58.884%
2022-06-30 13:00:49 - epoch 020 lr: 0.100000
2022-06-30 13:01:36 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.0175
2022-06-30 13:02:18 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 1.7015
2022-06-30 13:03:00 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.0036
2022-06-30 13:03:41 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 1.7779
2022-06-30 13:04:23 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 1.7487
2022-06-30 13:05:05 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.0005
2022-06-30 13:05:46 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 1.6877
2022-06-30 13:06:28 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 1.9782
2022-06-30 13:07:10 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.1785
2022-06-30 13:07:52 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.0218
2022-06-30 13:08:34 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 1.8459
2022-06-30 13:09:16 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 1.6759
2022-06-30 13:09:57 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 1.8563
2022-06-30 13:10:39 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.1030
2022-06-30 13:11:21 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.0928
2022-06-30 13:12:02 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 1.7873
2022-06-30 13:12:44 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 1.6823
2022-06-30 13:13:26 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 1.8481
2022-06-30 13:14:08 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 1.8480
2022-06-30 13:14:49 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 1.8989
2022-06-30 13:15:31 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.1464
2022-06-30 13:16:13 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 1.6323
2022-06-30 13:16:55 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 1.8343
2022-06-30 13:17:37 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.1453
2022-06-30 13:18:18 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 1.7089
2022-06-30 13:19:00 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 1.7990
2022-06-30 13:19:42 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 1.9662
2022-06-30 13:20:24 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 1.9532
2022-06-30 13:21:06 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.0812
2022-06-30 13:21:48 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 1.9074
2022-06-30 13:22:29 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 1.9310
2022-06-30 13:23:11 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.0877
2022-06-30 13:23:53 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 1.6853
2022-06-30 13:24:35 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.0521
2022-06-30 13:25:17 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 1.8641
2022-06-30 13:25:58 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 1.9828
2022-06-30 13:26:40 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 1.8102
2022-06-30 13:27:22 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 1.9456
2022-06-30 13:28:03 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.0447
2022-06-30 13:28:45 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 1.8721
2022-06-30 13:29:27 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 1.8645
2022-06-30 13:30:09 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 1.9711
2022-06-30 13:30:51 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 1.7783
2022-06-30 13:31:32 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 1.9401
2022-06-30 13:32:14 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.0324
2022-06-30 13:32:56 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.0139
2022-06-30 13:33:38 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 1.7121
2022-06-30 13:34:20 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 1.9804
2022-06-30 13:35:02 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.0095
2022-06-30 13:35:43 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 1.7900
2022-06-30 13:35:45 - train: epoch 020, train_loss: 1.9142
2022-06-30 13:36:59 - eval: epoch: 020, acc1: 60.338%, acc5: 83.998%, test_loss: 1.6502, per_image_load_time: 1.311ms, per_image_inference_time: 0.886ms
2022-06-30 13:37:00 - until epoch: 020, best_acc1: 60.338%
2022-06-30 13:37:00 - epoch 021 lr: 0.100000
2022-06-30 13:37:48 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 1.7572
2022-06-30 13:38:30 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 1.8761
2022-06-30 13:39:12 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.5367
2022-06-30 13:39:54 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.0286
2022-06-30 13:40:36 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 1.8808
2022-06-30 13:41:18 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 1.8007
2022-06-30 13:41:59 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 1.8059
2022-06-30 13:42:41 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 1.9509
2022-06-30 13:43:23 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 1.8874
2022-06-30 13:44:05 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 1.7902
2022-06-30 13:44:47 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 1.7356
2022-06-30 13:45:29 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 1.8089
2022-06-30 13:46:11 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 1.7558
2022-06-30 13:46:53 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 1.7696
2022-06-30 13:47:35 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 1.6937
2022-06-30 13:48:17 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 1.8951
2022-06-30 13:48:58 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 1.9593
2022-06-30 13:49:40 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 1.7435
2022-06-30 13:50:22 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.0332
2022-06-30 13:51:04 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.1724
2022-06-30 13:51:46 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 1.7465
2022-06-30 13:52:28 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 1.8828
2022-06-30 13:53:10 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 1.7850
2022-06-30 13:53:52 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 1.7685
2022-06-30 13:54:34 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 1.8631
2022-06-30 13:55:16 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.1713
2022-06-30 13:55:58 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 1.8626
2022-06-30 13:56:40 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 1.7979
2022-06-30 13:57:22 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 1.7698
2022-06-30 13:58:04 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.0217
2022-06-30 13:58:46 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 1.9824
2022-06-30 13:59:28 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 1.8895
2022-06-30 14:00:09 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.1405
2022-06-30 14:00:51 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.1189
2022-06-30 14:01:33 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 1.8220
2022-06-30 14:02:15 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 1.8830
2022-06-30 14:02:57 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 1.9817
2022-06-30 14:03:39 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 1.8873
2022-06-30 14:04:21 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 1.8200
2022-06-30 14:05:03 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.1721
2022-06-30 14:05:45 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 1.8715
2022-06-30 14:06:27 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 1.8430
2022-06-30 14:07:09 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 1.7967
2022-06-30 14:07:50 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 1.9948
2022-06-30 14:08:32 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 1.9365
2022-06-30 14:09:14 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 1.7888
2022-06-30 14:09:56 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 1.9962
2022-06-30 14:10:38 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.0617
2022-06-30 14:11:20 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 1.7228
2022-06-30 14:12:02 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 1.7903
2022-06-30 14:12:04 - train: epoch 021, train_loss: 1.9046
2022-06-30 14:13:18 - eval: epoch: 021, acc1: 59.292%, acc5: 83.194%, test_loss: 1.7043, per_image_load_time: 1.283ms, per_image_inference_time: 0.878ms
2022-06-30 14:13:19 - until epoch: 021, best_acc1: 60.338%
2022-06-30 14:13:19 - epoch 022 lr: 0.100000
2022-06-30 14:14:07 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 1.6493
2022-06-30 14:14:48 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 1.7468
2022-06-30 14:15:30 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 1.6257
2022-06-30 14:16:12 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 1.8084
2022-06-30 14:16:53 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 1.8388
2022-06-30 14:17:35 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 1.9648
2022-06-30 14:18:17 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 1.9245
2022-06-30 14:18:59 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 1.9911
2022-06-30 14:19:40 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.0360
2022-06-30 14:20:22 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 1.8771
2022-06-30 14:21:04 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 1.8850
2022-06-30 14:21:45 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.6027
2022-06-30 14:22:27 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 1.8895
2022-06-30 14:23:09 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 1.9402
2022-06-30 14:23:50 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 1.8772
2022-06-30 14:24:32 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 1.5482
2022-06-30 14:25:14 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 1.7399
2022-06-30 14:25:55 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 1.9853
2022-06-30 14:26:37 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 1.8086
2022-06-30 14:27:19 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 1.9348
2022-06-30 14:28:01 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.0040
2022-06-30 14:28:42 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 1.7017
2022-06-30 14:29:24 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.0924
2022-06-30 14:30:06 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 1.9367
2022-06-30 14:30:47 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 1.9213
2022-06-30 14:31:29 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 1.6610
2022-06-30 14:32:11 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 1.6274
2022-06-30 14:32:53 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.2151
2022-06-30 14:33:34 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 1.7498
2022-06-30 14:34:16 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.0421
2022-06-30 14:34:58 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.0302
2022-06-30 14:35:39 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.0201
2022-06-30 14:36:21 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 1.8135
2022-06-30 14:37:03 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 1.7741
2022-06-30 14:37:44 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 1.9159
2022-06-30 14:38:26 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 1.8988
2022-06-30 14:39:07 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 1.9643
2022-06-30 14:39:49 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.0570
2022-06-30 14:40:31 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 1.8745
2022-06-30 14:41:13 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 1.9754
2022-06-30 14:41:54 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 1.8294
2022-06-30 14:42:36 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 1.8302
2022-06-30 14:43:18 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.0798
2022-06-30 14:44:00 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 1.8749
2022-06-30 14:44:41 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 1.8122
2022-06-30 14:45:23 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.0766
2022-06-30 14:46:05 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 1.9611
2022-06-30 14:46:47 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 1.7054
2022-06-30 14:47:29 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 1.7564
2022-06-30 14:48:10 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 1.8704
2022-06-30 14:48:12 - train: epoch 022, train_loss: 1.8935
2022-06-30 14:49:27 - eval: epoch: 022, acc1: 57.132%, acc5: 81.546%, test_loss: 1.8104, per_image_load_time: 1.945ms, per_image_inference_time: 0.868ms
2022-06-30 14:49:28 - until epoch: 022, best_acc1: 60.338%
2022-06-30 14:49:28 - epoch 023 lr: 0.100000
2022-06-30 14:50:15 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 1.7506
2022-06-30 14:50:56 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 1.6060
2022-06-30 14:51:38 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 1.7555
2022-06-30 14:52:19 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 1.8941
2022-06-30 14:53:01 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 1.8892
2022-06-30 14:53:43 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 1.8210
2022-06-30 14:54:25 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 1.5962
2022-06-30 14:55:06 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 1.8301
2022-06-30 14:55:48 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 1.8881
2022-06-30 14:56:29 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 1.8061
2022-06-30 14:57:11 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.0734
2022-06-30 14:57:53 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 1.6957
2022-06-30 14:58:34 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 1.8776
2022-06-30 14:59:16 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 1.9339
2022-06-30 14:59:58 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 1.6804
2022-06-30 15:00:40 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 1.9938
2022-06-30 15:01:22 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 1.9657
2022-06-30 15:02:04 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 1.8067
2022-06-30 15:02:45 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.0043
2022-06-30 15:03:27 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.5522
2022-06-30 15:04:09 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.1005
2022-06-30 15:04:51 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.6280
2022-06-30 15:05:33 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 1.7241
2022-06-30 15:06:15 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 1.7919
2022-06-30 15:06:57 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 1.8900
2022-06-30 15:07:38 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 1.9613
2022-06-30 15:08:20 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 1.8871
2022-06-30 15:09:02 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 1.8536
2022-06-30 15:09:44 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 1.9837
2022-06-30 15:10:26 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.1135
2022-06-30 15:11:08 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 1.9684
2022-06-30 15:11:49 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.0804
2022-06-30 15:12:31 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 1.8816
2022-06-30 15:13:13 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 1.9032
2022-06-30 15:13:55 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 1.7206
2022-06-30 15:14:37 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 1.6574
2022-06-30 15:15:19 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 1.8463
2022-06-30 15:16:01 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.0037
2022-06-30 15:16:43 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 1.8907
2022-06-30 15:17:25 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 1.8118
2022-06-30 15:18:07 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 1.8605
2022-06-30 15:18:49 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 1.7940
2022-06-30 15:19:31 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 1.6410
2022-06-30 15:20:12 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 1.7122
2022-06-30 15:20:54 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 1.7624
2022-06-30 15:21:36 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 1.9317
2022-06-30 15:22:18 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 1.6147
2022-06-30 15:23:00 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 1.9335
2022-06-30 15:23:42 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 1.7414
2022-06-30 15:24:24 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 1.9124
2022-06-30 15:24:26 - train: epoch 023, train_loss: 1.8849
2022-06-30 15:25:40 - eval: epoch: 023, acc1: 57.956%, acc5: 82.248%, test_loss: 1.7590, per_image_load_time: 1.014ms, per_image_inference_time: 0.853ms
2022-06-30 15:25:40 - until epoch: 023, best_acc1: 60.338%
2022-06-30 15:25:40 - epoch 024 lr: 0.100000
2022-06-30 15:26:28 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 1.7856
2022-06-30 15:27:09 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 1.9025
2022-06-30 15:27:51 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 1.7405
2022-06-30 15:28:33 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 1.9989
2022-06-30 15:29:15 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 1.7588
2022-06-30 15:29:56 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 1.7137
2022-06-30 15:30:38 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 1.7853
2022-06-30 15:31:20 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 1.8292
2022-06-30 15:32:02 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 1.8349
2022-06-30 15:32:44 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 1.8621
2022-06-30 15:33:25 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.5055
2022-06-30 15:34:07 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 1.8024
2022-06-30 15:34:49 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.1154
2022-06-30 15:35:30 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 1.8279
2022-06-30 15:36:12 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.0647
2022-06-30 15:36:54 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 1.8759
2022-06-30 15:37:35 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 1.9210
2022-06-30 15:38:17 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.1642
2022-06-30 15:38:59 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 1.6434
2022-06-30 15:39:40 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 1.8806
2022-06-30 15:40:22 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 1.8612
2022-06-30 15:41:04 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 1.7258
2022-06-30 15:41:45 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 1.9419
2022-06-30 15:42:27 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 1.8661
2022-06-30 15:43:09 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 1.7352
2022-06-30 15:43:51 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 1.8401
2022-06-30 15:44:32 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.1652
2022-06-30 15:45:14 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.0870
2022-06-30 15:45:56 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.1154
2022-06-30 15:46:37 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 1.8622
2022-06-30 15:47:19 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 1.7891
2022-06-30 15:48:01 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.0681
2022-06-30 15:48:42 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.4938
2022-06-30 15:49:24 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 1.9489
2022-06-30 15:50:05 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 1.8225
2022-06-30 15:50:47 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.0454
2022-06-30 15:51:29 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 1.8286
2022-06-30 15:52:10 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.1789
2022-06-30 15:52:52 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 1.9102
2022-06-30 15:53:34 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 1.9026
2022-06-30 15:54:15 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 1.9812
2022-06-30 15:54:57 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 1.8929
2022-06-30 15:55:39 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 1.9043
2022-06-30 15:56:21 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 1.9553
2022-06-30 15:57:02 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 1.8343
2022-06-30 15:57:44 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 1.8973
2022-06-30 15:58:26 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 1.8165
2022-06-30 15:59:08 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 1.7985
2022-06-30 15:59:49 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 1.8554
2022-06-30 16:00:31 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.0091
2022-06-30 16:00:33 - train: epoch 024, train_loss: 1.8822
2022-06-30 16:01:47 - eval: epoch: 024, acc1: 60.490%, acc5: 83.968%, test_loss: 1.6495, per_image_load_time: 1.587ms, per_image_inference_time: 0.858ms
2022-06-30 16:01:48 - until epoch: 024, best_acc1: 60.490%
2022-06-30 16:01:48 - epoch 025 lr: 0.100000
2022-06-30 16:02:35 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 1.7142
2022-06-30 16:03:16 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 1.8231
2022-06-30 16:03:58 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 1.6149
2022-06-30 16:04:40 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 1.7681
2022-06-30 16:05:21 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 1.7545
2022-06-30 16:06:03 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 1.8934
2022-06-30 16:06:45 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 1.9006
2022-06-30 16:07:26 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 1.8167
2022-06-30 16:08:08 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 1.7130
2022-06-30 16:08:50 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 1.8361
2022-06-30 16:09:32 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 1.8703
2022-06-30 16:10:13 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 1.9545
2022-06-30 16:10:55 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 1.8687
2022-06-30 16:11:37 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 1.9921
2022-06-30 16:12:19 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 1.9094
2022-06-30 16:13:01 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 1.5950
2022-06-30 16:13:43 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 1.7565
2022-06-30 16:14:25 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 1.6802
2022-06-30 16:15:07 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 1.7522
2022-06-30 16:15:48 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 1.8603
2022-06-30 16:16:30 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 1.7150
2022-06-30 16:17:12 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 1.7573
2022-06-30 16:17:54 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 1.9035
2022-06-30 16:18:36 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 1.6645
2022-06-30 16:19:18 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 1.9200
2022-06-30 16:19:59 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.0198
2022-06-30 16:20:41 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.0371
2022-06-30 16:21:23 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 1.8611
2022-06-30 16:22:05 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.0448
2022-06-30 16:22:47 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.1255
2022-06-30 16:23:29 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 1.9629
2022-06-30 16:24:11 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.1513
2022-06-30 16:24:53 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 1.7621
2022-06-30 16:25:34 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.0493
2022-06-30 16:26:16 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 1.6062
2022-06-30 16:26:58 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 1.9651
2022-06-30 16:27:40 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 1.9283
2022-06-30 16:28:21 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 1.9642
2022-06-30 16:29:03 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.1248
2022-06-30 16:29:45 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 1.9705
2022-06-30 16:30:27 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.1916
2022-06-30 16:31:09 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 1.8921
2022-06-30 16:31:51 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 1.7064
2022-06-30 16:32:33 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 1.8566
2022-06-30 16:33:15 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 1.8687
2022-06-30 16:33:56 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 1.9310
2022-06-30 16:34:38 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 1.7580
2022-06-30 16:35:20 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 1.6830
2022-06-30 16:36:02 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 1.8435
2022-06-30 16:36:44 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.0727
2022-06-30 16:36:46 - train: epoch 025, train_loss: 1.8744
2022-06-30 16:38:00 - eval: epoch: 025, acc1: 59.168%, acc5: 83.140%, test_loss: 1.6967, per_image_load_time: 1.309ms, per_image_inference_time: 0.871ms
2022-06-30 16:38:01 - until epoch: 025, best_acc1: 60.490%
2022-06-30 16:38:01 - epoch 026 lr: 0.100000
2022-06-30 16:38:49 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 1.6526
2022-06-30 16:39:31 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.6033
2022-06-30 16:40:14 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 1.8174
2022-06-30 16:40:55 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 1.8557
2022-06-30 16:41:37 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 1.9022
2022-06-30 16:42:19 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 1.9914
2022-06-30 16:43:01 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 1.8015
2022-06-30 16:43:43 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 1.7103
2022-06-30 16:44:24 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.0896
2022-06-30 16:45:06 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 1.6676
2022-06-30 16:45:48 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 1.8074
2022-06-30 16:46:30 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 1.8727
2022-06-30 16:47:12 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 1.7192
2022-06-30 16:47:53 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 1.8892
2022-06-30 16:48:35 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 1.8153
2022-06-30 16:49:17 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.0824
2022-06-30 16:49:59 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 1.8479
2022-06-30 16:50:41 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 1.9315
2022-06-30 16:51:23 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.1748
2022-06-30 16:52:04 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 1.9070
2022-06-30 16:52:46 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 1.9146
2022-06-30 16:53:28 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 1.8516
2022-06-30 16:54:10 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 1.8077
2022-06-30 16:54:51 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.1640
2022-06-30 16:55:33 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.0020
2022-06-30 16:56:15 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 1.8863
2022-06-30 16:56:57 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 1.8471
2022-06-30 16:57:39 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 1.7293
2022-06-30 16:58:20 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.0537
2022-06-30 16:59:02 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 1.8910
2022-06-30 16:59:44 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 1.9090
2022-06-30 17:00:26 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 1.9091
2022-06-30 17:01:07 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 1.8399
2022-06-30 17:01:49 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.0149
2022-06-30 17:02:31 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.0473
2022-06-30 17:03:13 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 1.7304
2022-06-30 17:03:55 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.0650
2022-06-30 17:04:37 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 1.9370
2022-06-30 17:05:18 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 1.9986
2022-06-30 17:06:00 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.1602
2022-06-30 17:06:42 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 1.9858
2022-06-30 17:07:24 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.0176
2022-06-30 17:08:06 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 1.9515
2022-06-30 17:08:48 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.0860
2022-06-30 17:09:29 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.0747
2022-06-30 17:10:11 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 1.8174
2022-06-30 17:10:53 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 1.8072
2022-06-30 17:11:35 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 1.8388
2022-06-30 17:12:17 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 1.8988
2022-06-30 17:12:59 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.0012
2022-06-30 17:13:01 - train: epoch 026, train_loss: 1.8659
2022-06-30 17:14:15 - eval: epoch: 026, acc1: 58.804%, acc5: 82.848%, test_loss: 1.7186, per_image_load_time: 1.608ms, per_image_inference_time: 0.869ms
2022-06-30 17:14:16 - until epoch: 026, best_acc1: 60.490%
2022-06-30 17:14:16 - epoch 027 lr: 0.100000
2022-06-30 17:15:03 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 1.9885
2022-06-30 17:15:45 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 1.8302
2022-06-30 17:16:26 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 1.8251
2022-06-30 17:17:08 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.0005
2022-06-30 17:17:50 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 1.9413
2022-06-30 17:18:31 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.1496
2022-06-30 17:19:13 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.0312
2022-06-30 17:19:54 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 1.9623
2022-06-30 17:20:36 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 1.7379
2022-06-30 17:21:18 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 1.8596
2022-06-30 17:21:59 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 1.7518
2022-06-30 17:22:41 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.0527
2022-06-30 17:23:23 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 1.8993
2022-06-30 17:24:04 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 1.9371
2022-06-30 17:24:46 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 1.8857
2022-06-30 17:25:28 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 1.8504
2022-06-30 17:26:10 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 1.8671
2022-06-30 17:26:51 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 1.8243
2022-06-30 17:27:33 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.0692
2022-06-30 17:28:15 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 1.7414
2022-06-30 17:28:57 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.2111
2022-06-30 17:29:38 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.1402
2022-06-30 17:30:20 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.1684
2022-06-30 17:31:02 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 1.9393
2022-06-30 17:31:44 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 1.7616
2022-06-30 17:32:25 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.0758
2022-06-30 17:33:07 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 1.8399
2022-06-30 17:33:49 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.0720
2022-06-30 17:34:30 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 1.8855
2022-06-30 17:35:12 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 1.6807
2022-06-30 17:35:54 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 1.7935
2022-06-30 17:36:36 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 1.6458
2022-06-30 17:37:18 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 1.9054
2022-06-30 17:37:59 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 1.7803
2022-06-30 17:38:41 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.1482
2022-06-30 17:39:23 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 1.7539
2022-06-30 17:40:05 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 1.8125
2022-06-30 17:40:47 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 1.6393
2022-06-30 17:41:28 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 1.7385
2022-06-30 17:42:10 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 1.9274
2022-06-30 17:42:52 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 1.8446
2022-06-30 17:43:34 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 1.8733
2022-06-30 17:44:16 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 1.7113
2022-06-30 17:44:57 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 1.9079
2022-06-30 17:45:39 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 1.6600
2022-06-30 17:46:21 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 1.7964
2022-06-30 17:47:03 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 1.8221
2022-06-30 17:47:44 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 1.9521
2022-06-30 17:48:26 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 1.9278
2022-06-30 17:49:08 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.5740
2022-06-30 17:49:10 - train: epoch 027, train_loss: 1.8537
2022-06-30 17:50:24 - eval: epoch: 027, acc1: 60.690%, acc5: 84.108%, test_loss: 1.6329, per_image_load_time: 1.704ms, per_image_inference_time: 0.874ms
2022-06-30 17:50:25 - until epoch: 027, best_acc1: 60.690%
2022-06-30 17:50:25 - epoch 028 lr: 0.100000
2022-06-30 17:51:12 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 1.5786
2022-06-30 17:51:54 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 1.7212
2022-06-30 17:52:36 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 1.7596
2022-06-30 17:53:18 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 1.7788
2022-06-30 17:54:00 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 1.7651
2022-06-30 17:54:41 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.0068
2022-06-30 17:55:23 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.0442
2022-06-30 17:56:05 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.5814
2022-06-30 17:56:47 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 1.6725
2022-06-30 17:57:29 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 1.8651
2022-06-30 17:58:11 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 1.8926
2022-06-30 17:58:52 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 1.7053
2022-06-30 17:59:34 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 1.7952
2022-06-30 18:00:15 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.1601
2022-06-30 18:00:57 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 1.7294
2022-06-30 18:01:39 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 1.9215
2022-06-30 18:02:20 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 1.8990
2022-06-30 18:03:02 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 1.7056
2022-06-30 18:03:43 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 1.8048
2022-06-30 18:04:25 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.0165
2022-06-30 18:05:07 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 1.9180
2022-06-30 18:05:48 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 1.8908
2022-06-30 18:06:30 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.0810
2022-06-30 18:07:11 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.0829
2022-06-30 18:07:53 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 1.9052
2022-06-30 18:08:35 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 1.6293
2022-06-30 18:09:16 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 1.9371
2022-06-30 18:09:58 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 1.9034
2022-06-30 18:10:40 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 1.8959
2022-06-30 18:11:21 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 1.9460
2022-06-30 18:12:03 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.0107
2022-06-30 18:12:44 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 1.8712
2022-06-30 18:13:26 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 1.9375
2022-06-30 18:14:08 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 1.8564
2022-06-30 18:14:49 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 1.7930
2022-06-30 18:15:31 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 1.7458
2022-06-30 18:16:13 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 1.7754
2022-06-30 18:16:54 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.6168
2022-06-30 18:17:36 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 1.7953
2022-06-30 18:18:18 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.0170
2022-06-30 18:18:59 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 1.9858
2022-06-30 18:19:41 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.0052
2022-06-30 18:20:23 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 1.7408
2022-06-30 18:21:04 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 1.8267
2022-06-30 18:21:46 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 1.8319
2022-06-30 18:22:27 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 1.9341
2022-06-30 18:23:09 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 1.8431
2022-06-30 18:23:50 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 1.7465
2022-06-30 18:24:32 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 1.7885
2022-06-30 18:25:13 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 1.7609
2022-06-30 18:25:15 - train: epoch 028, train_loss: 1.8518
2022-06-30 18:26:29 - eval: epoch: 028, acc1: 60.958%, acc5: 84.126%, test_loss: 1.6246, per_image_load_time: 1.492ms, per_image_inference_time: 0.866ms
2022-06-30 18:26:30 - until epoch: 028, best_acc1: 60.958%
2022-06-30 18:26:30 - epoch 029 lr: 0.100000
2022-06-30 18:27:17 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 1.8599
2022-06-30 18:27:59 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 1.8141
2022-06-30 18:28:40 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.0317
2022-06-30 18:29:22 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 1.7658
2022-06-30 18:30:03 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 1.8439
2022-06-30 18:30:45 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.0949
2022-06-30 18:31:27 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.3296
2022-06-30 18:32:08 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 1.9974
2022-06-30 18:32:50 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 1.7246
2022-06-30 18:33:32 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 1.6474
2022-06-30 18:34:14 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 1.7683
2022-06-30 18:34:55 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 1.8483
2022-06-30 18:35:37 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 1.7939
2022-06-30 18:36:19 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 1.9462
2022-06-30 18:37:01 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 1.8342
2022-06-30 18:37:43 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 1.6985
2022-06-30 18:38:25 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.0017
2022-06-30 18:39:06 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 1.8710
2022-06-30 18:39:48 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 1.6333
2022-06-30 18:40:30 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 1.9791
2022-06-30 18:41:12 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 1.8102
2022-06-30 18:41:54 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 1.8843
2022-06-30 18:42:36 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 1.9274
2022-06-30 18:43:18 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 1.7818
2022-06-30 18:44:00 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 1.6143
2022-06-30 18:44:42 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 1.9240
2022-06-30 18:45:24 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 1.8504
2022-06-30 18:46:06 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 1.7476
2022-06-30 18:46:47 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 1.8435
2022-06-30 18:47:29 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 1.7855
2022-06-30 18:48:11 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 1.8351
2022-06-30 18:48:53 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 1.9111
2022-06-30 18:49:35 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 1.8983
2022-06-30 18:50:17 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 1.6387
2022-06-30 18:50:59 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 1.9490
2022-06-30 18:51:41 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 1.8231
2022-06-30 18:52:23 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 1.7630
2022-06-30 18:53:05 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 1.8525
2022-06-30 18:53:47 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 1.6389
2022-06-30 18:54:29 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 1.7894
2022-06-30 18:55:11 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 1.9367
2022-06-30 18:55:53 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 1.7600
2022-06-30 18:56:34 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 1.9659
2022-06-30 18:57:16 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 1.9129
2022-06-30 18:57:58 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.0483
2022-06-30 18:58:40 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.0364
2022-06-30 18:59:22 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 1.5700
2022-06-30 19:00:04 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 1.8946
2022-06-30 19:00:46 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.1982
2022-06-30 19:01:28 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.6100
2022-06-30 19:01:30 - train: epoch 029, train_loss: 1.8507
2022-06-30 19:02:45 - eval: epoch: 029, acc1: 59.798%, acc5: 83.366%, test_loss: 1.6750, per_image_load_time: 1.042ms, per_image_inference_time: 0.875ms
2022-06-30 19:02:46 - until epoch: 029, best_acc1: 60.958%
2022-06-30 19:02:46 - epoch 030 lr: 0.100000
2022-06-30 19:03:33 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.0300
2022-06-30 19:04:15 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 1.9358
2022-06-30 19:04:56 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 1.8594
2022-06-30 19:05:38 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 1.5498
2022-06-30 19:06:19 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.0093
2022-06-30 19:07:01 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 1.6124
2022-06-30 19:07:43 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 1.7639
2022-06-30 19:08:24 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 1.9477
2022-06-30 19:09:06 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 1.8503
2022-06-30 19:09:48 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 1.5511
2022-06-30 19:10:29 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 1.6034
2022-06-30 19:11:11 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 1.9018
2022-06-30 19:11:52 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 1.7111
2022-06-30 19:12:34 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 1.8444
2022-06-30 19:13:16 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 1.8274
2022-06-30 19:13:58 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 1.7464
2022-06-30 19:14:39 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.0028
2022-06-30 19:15:21 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 1.8696
2022-06-30 19:16:02 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 1.9593
2022-06-30 19:16:44 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 1.7917
2022-06-30 19:17:25 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 1.9366
2022-06-30 19:18:07 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 1.8060
2022-06-30 19:18:49 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 1.7561
2022-06-30 19:19:30 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 1.9676
2022-06-30 19:20:12 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 1.8450
2022-06-30 19:20:53 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 1.8344
2022-06-30 19:21:35 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 1.7989
2022-06-30 19:22:17 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 1.8161
2022-06-30 19:22:58 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 1.8051
2022-06-30 19:23:40 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.0255
2022-06-30 19:24:22 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 1.6993
2022-06-30 19:25:03 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 1.8480
2022-06-30 19:25:45 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.1185
2022-06-30 19:26:26 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 1.8938
2022-06-30 19:27:08 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 1.9392
2022-06-30 19:27:50 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 1.7323
2022-06-30 19:28:32 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 1.8270
2022-06-30 19:29:13 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 1.8734
2022-06-30 19:29:55 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 1.8658
2022-06-30 19:30:37 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 1.6358
2022-06-30 19:31:19 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 1.6882
2022-06-30 19:32:00 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.0027
2022-06-30 19:32:42 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 1.7341
2022-06-30 19:33:24 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 1.8818
2022-06-30 19:34:05 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.0508
2022-06-30 19:34:47 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 1.6211
2022-06-30 19:35:29 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 1.8415
2022-06-30 19:36:11 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 1.8883
2022-06-30 19:36:53 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.1161
2022-06-30 19:37:34 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 1.9416
2022-06-30 19:37:36 - train: epoch 030, train_loss: 1.8469
2022-06-30 19:38:50 - eval: epoch: 030, acc1: 61.150%, acc5: 84.294%, test_loss: 1.6198, per_image_load_time: 1.674ms, per_image_inference_time: 0.881ms
2022-06-30 19:38:51 - until epoch: 030, best_acc1: 61.150%
2022-06-30 19:38:51 - epoch 031 lr: 0.010000
2022-06-30 19:39:38 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 1.8297
2022-06-30 19:40:19 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.4180
2022-06-30 19:41:01 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.4280
2022-06-30 19:41:43 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.6830
2022-06-30 19:42:24 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.4220
2022-06-30 19:43:06 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.3822
2022-06-30 19:43:47 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.3944
2022-06-30 19:44:29 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.4071
2022-06-30 19:45:10 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.4326
2022-06-30 19:45:52 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 1.5034
2022-06-30 19:46:33 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 1.6732
2022-06-30 19:47:15 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.4103
2022-06-30 19:47:56 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.1884
2022-06-30 19:48:38 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.3959
2022-06-30 19:49:19 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.4495
2022-06-30 19:50:01 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.2578
2022-06-30 19:50:42 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.2740
2022-06-30 19:51:23 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.2360
2022-06-30 19:52:05 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.4389
2022-06-30 19:52:46 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.3480
2022-06-30 19:53:28 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.1564
2022-06-30 19:54:09 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.1656
2022-06-30 19:54:51 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.2028
2022-06-30 19:55:32 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.2789
2022-06-30 19:56:14 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.2751
2022-06-30 19:56:56 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.3535
2022-06-30 19:57:37 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.2962
2022-06-30 19:58:18 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.6143
2022-06-30 19:59:00 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.4346
2022-06-30 19:59:41 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.4185
2022-06-30 20:00:22 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.1873
2022-06-30 20:01:04 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.4418
2022-06-30 20:01:45 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.4146
2022-06-30 20:02:27 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.3479
2022-06-30 20:03:09 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.4250
2022-06-30 20:03:50 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.3718
2022-06-30 20:04:32 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.2680
2022-06-30 20:05:13 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.2336
2022-06-30 20:05:55 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.1627
2022-06-30 20:06:36 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.3115
2022-06-30 20:07:18 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.2947
2022-06-30 20:07:59 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.3237
2022-06-30 20:08:41 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.2742
2022-06-30 20:09:23 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.4459
2022-06-30 20:10:04 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.4401
2022-06-30 20:10:46 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.2949
2022-06-30 20:11:27 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.3192
2022-06-30 20:12:09 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.2096
2022-06-30 20:12:50 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.2449
2022-06-30 20:13:32 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.2718
2022-06-30 20:13:34 - train: epoch 031, train_loss: 1.3629
2022-06-30 20:14:48 - eval: epoch: 031, acc1: 72.712%, acc5: 91.094%, test_loss: 1.0917, per_image_load_time: 1.167ms, per_image_inference_time: 0.901ms
2022-06-30 20:14:49 - until epoch: 031, best_acc1: 72.712%
2022-06-30 20:14:49 - epoch 032 lr: 0.010000
2022-06-30 20:15:36 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.2142
2022-06-30 20:16:17 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.2801
2022-06-30 20:16:59 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.3683
2022-06-30 20:17:41 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.3858
2022-06-30 20:18:22 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.2668
2022-06-30 20:19:04 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.3443
2022-06-30 20:19:45 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.2586
2022-06-30 20:20:27 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.2496
2022-06-30 20:21:08 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.2406
2022-06-30 20:21:50 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.4007
2022-06-30 20:22:31 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.4303
2022-06-30 20:23:13 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.1776
2022-06-30 20:23:54 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.2323
2022-06-30 20:24:36 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.3759
2022-06-30 20:25:17 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.2565
2022-06-30 20:25:59 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.3181
2022-06-30 20:26:41 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.3439
2022-06-30 20:27:22 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.4929
2022-06-30 20:28:04 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.1407
2022-06-30 20:28:45 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.2634
2022-06-30 20:29:27 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.1645
2022-06-30 20:30:08 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.1656
2022-06-30 20:30:50 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.1783
2022-06-30 20:31:31 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.1999
2022-06-30 20:32:13 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.3379
2022-06-30 20:32:54 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.0677
2022-06-30 20:33:36 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.2300
2022-06-30 20:34:18 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.3185
2022-06-30 20:34:59 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.0576
2022-06-30 20:35:40 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.0732
2022-06-30 20:36:22 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.2905
2022-06-30 20:37:04 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.1435
2022-06-30 20:37:45 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.1816
2022-06-30 20:38:27 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.0697
2022-06-30 20:39:09 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.2255
2022-06-30 20:39:50 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.3555
2022-06-30 20:40:32 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.2782
2022-06-30 20:41:14 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.0621
2022-06-30 20:41:55 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.3660
2022-06-30 20:42:37 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.1240
2022-06-30 20:43:19 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.2037
2022-06-30 20:44:00 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.0506
2022-06-30 20:44:42 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.4626
2022-06-30 20:45:24 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.1985
2022-06-30 20:46:05 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.3895
2022-06-30 20:46:47 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.2354
2022-06-30 20:47:28 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.2839
2022-06-30 20:48:10 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.1642
2022-06-30 20:48:52 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.3273
2022-06-30 20:49:33 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.1849
2022-06-30 20:49:35 - train: epoch 032, train_loss: 1.2400
2022-06-30 20:50:49 - eval: epoch: 032, acc1: 73.652%, acc5: 91.590%, test_loss: 1.0473, per_image_load_time: 1.899ms, per_image_inference_time: 0.900ms
2022-06-30 20:50:50 - until epoch: 032, best_acc1: 73.652%
2022-06-30 20:50:50 - epoch 033 lr: 0.010000
2022-06-30 20:51:37 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.0405
2022-06-30 20:52:18 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.3044
2022-06-30 20:53:00 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.0776
2022-06-30 20:53:41 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.1092
2022-06-30 20:54:23 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.3198
2022-06-30 20:55:04 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.0082
2022-06-30 20:55:46 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.3027
2022-06-30 20:56:27 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.1459
2022-06-30 20:57:09 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.2314
2022-06-30 20:57:51 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.2266
2022-06-30 20:58:32 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.2284
2022-06-30 20:59:14 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.1460
2022-06-30 20:59:55 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.1050
2022-06-30 21:00:37 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.3007
2022-06-30 21:01:19 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.4154
2022-06-30 21:02:00 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.2638
2022-06-30 21:02:42 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.0388
2022-06-30 21:03:24 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.3342
2022-06-30 21:04:05 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.2272
2022-06-30 21:04:47 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.0675
2022-06-30 21:05:28 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.2296
2022-06-30 21:06:10 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.2925
2022-06-30 21:06:52 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.1081
2022-06-30 21:07:33 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.4391
2022-06-30 21:08:15 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.1264
2022-06-30 21:08:57 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.0108
2022-06-30 21:09:38 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.2121
2022-06-30 21:10:20 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.1928
2022-06-30 21:11:02 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.3360
2022-06-30 21:11:43 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.2188
2022-06-30 21:12:25 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.2088
2022-06-30 21:13:07 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.2348
2022-06-30 21:13:48 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.2714
2022-06-30 21:14:30 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.0836
2022-06-30 21:15:12 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.1714
2022-06-30 21:15:53 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.4066
2022-06-30 21:16:35 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.2314
2022-06-30 21:17:17 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.0099
2022-06-30 21:17:59 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.2191
2022-06-30 21:18:41 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.2291
2022-06-30 21:19:22 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.1900
2022-06-30 21:20:04 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.0855
2022-06-30 21:20:46 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.3392
2022-06-30 21:21:27 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.2040
2022-06-30 21:22:09 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.3450
2022-06-30 21:22:51 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.1535
2022-06-30 21:23:33 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.0741
2022-06-30 21:24:15 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.5189
2022-06-30 21:24:57 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.0578
2022-06-30 21:25:38 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.1189
2022-06-30 21:25:40 - train: epoch 033, train_loss: 1.1871
2022-06-30 21:26:54 - eval: epoch: 033, acc1: 73.934%, acc5: 91.828%, test_loss: 1.0327, per_image_load_time: 1.241ms, per_image_inference_time: 0.887ms
2022-06-30 21:26:55 - until epoch: 033, best_acc1: 73.934%
2022-06-30 21:26:55 - epoch 034 lr: 0.010000
2022-06-30 21:27:42 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.0982
2022-06-30 21:28:24 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.1951
2022-06-30 21:29:05 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.1771
2022-06-30 21:29:47 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 0.9043
2022-06-30 21:30:28 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.2170
2022-06-30 21:31:10 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.3467
2022-06-30 21:31:51 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.1558
2022-06-30 21:32:33 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.0988
2022-06-30 21:33:15 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.0914
2022-06-30 21:33:56 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.0940
2022-06-30 21:34:38 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.1079
2022-06-30 21:35:19 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.1263
2022-06-30 21:36:01 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.1059
2022-06-30 21:36:43 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.1598
2022-06-30 21:37:24 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.0765
2022-06-30 21:38:05 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.0407
2022-06-30 21:38:47 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.0458
2022-06-30 21:39:28 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.2765
2022-06-30 21:40:10 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.1781
2022-06-30 21:40:51 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.2303
2022-06-30 21:41:33 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.2962
2022-06-30 21:42:15 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.0510
2022-06-30 21:42:56 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.2305
2022-06-30 21:43:38 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.0515
2022-06-30 21:44:19 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.1021
2022-06-30 21:45:00 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.1406
2022-06-30 21:45:42 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.1791
2022-06-30 21:46:23 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 0.9796
2022-06-30 21:47:05 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 0.9827
2022-06-30 21:47:47 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.0297
2022-06-30 21:48:28 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 0.9787
2022-06-30 21:49:10 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.0959
2022-06-30 21:49:51 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.0236
2022-06-30 21:50:33 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.1932
2022-06-30 21:51:14 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.0764
2022-06-30 21:51:56 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 0.9976
2022-06-30 21:52:38 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.0624
2022-06-30 21:53:19 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.0898
2022-06-30 21:54:01 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.2506
2022-06-30 21:54:42 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.0037
2022-06-30 21:55:24 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.1729
2022-06-30 21:56:05 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.1822
2022-06-30 21:56:47 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.1926
2022-06-30 21:57:28 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.1564
2022-06-30 21:58:10 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.3298
2022-06-30 21:58:51 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.3072
2022-06-30 21:59:33 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.0907
2022-06-30 22:00:15 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.0889
2022-06-30 22:00:56 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.1401
2022-06-30 22:01:38 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 0.9953
2022-06-30 22:01:40 - train: epoch 034, train_loss: 1.1548
2022-06-30 22:02:55 - eval: epoch: 034, acc1: 74.248%, acc5: 92.066%, test_loss: 1.0187, per_image_load_time: 1.565ms, per_image_inference_time: 0.901ms
2022-06-30 22:02:56 - until epoch: 034, best_acc1: 74.248%
2022-06-30 22:02:56 - epoch 035 lr: 0.010000
2022-06-30 22:03:43 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.0802
2022-06-30 22:04:25 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 0.9205
2022-06-30 22:05:06 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.2990
2022-06-30 22:05:48 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.0023
2022-06-30 22:06:30 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.1231
2022-06-30 22:07:11 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.0577
2022-06-30 22:07:53 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.1279
2022-06-30 22:08:35 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.0746
2022-06-30 22:09:16 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.2511
2022-06-30 22:09:58 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.3109
2022-06-30 22:10:40 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.2923
2022-06-30 22:11:21 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.1082
2022-06-30 22:12:03 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.2123
2022-06-30 22:12:45 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.1852
2022-06-30 22:13:26 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.1558
2022-06-30 22:14:08 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.1622
2022-06-30 22:14:50 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.0193
2022-06-30 22:15:32 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.1714
2022-06-30 22:16:13 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.1450
2022-06-30 22:16:55 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.0562
2022-06-30 22:17:37 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.0926
2022-06-30 22:18:18 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.1790
2022-06-30 22:19:00 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.1880
2022-06-30 22:19:42 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.1907
2022-06-30 22:20:24 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.0457
2022-06-30 22:21:06 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.2743
2022-06-30 22:21:47 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.2376
2022-06-30 22:22:29 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.1036
2022-06-30 22:23:11 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.0887
2022-06-30 22:23:53 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.3025
2022-06-30 22:24:35 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.0906
2022-06-30 22:25:17 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.0266
2022-06-30 22:25:59 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.0636
2022-06-30 22:26:41 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.0715
2022-06-30 22:27:22 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.0295
2022-06-30 22:28:04 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.1117
2022-06-30 22:28:46 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 0.9424
2022-06-30 22:29:28 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.1614
2022-06-30 22:30:10 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.2719
2022-06-30 22:30:52 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 0.9794
2022-06-30 22:31:34 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.2435
2022-06-30 22:32:16 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.1418
2022-06-30 22:32:58 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.1889
2022-06-30 22:33:40 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.2179
2022-06-30 22:34:22 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.2388
2022-06-30 22:35:04 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 0.9779
2022-06-30 22:35:45 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.2991
2022-06-30 22:36:28 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.2215
2022-06-30 22:37:10 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.1996
2022-06-30 22:37:52 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.0772
2022-06-30 22:37:54 - train: epoch 035, train_loss: 1.1300
2022-06-30 22:39:07 - eval: epoch: 035, acc1: 74.382%, acc5: 92.032%, test_loss: 1.0155, per_image_load_time: 1.492ms, per_image_inference_time: 0.908ms
2022-06-30 22:39:09 - until epoch: 035, best_acc1: 74.382%
2022-06-30 22:39:09 - epoch 036 lr: 0.010000
2022-06-30 22:39:56 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.2115
2022-06-30 22:40:37 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 0.8892
2022-06-30 22:41:19 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.0789
2022-06-30 22:42:01 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.0709
2022-06-30 22:42:43 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.0893
2022-06-30 22:43:24 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.0189
2022-06-30 22:44:06 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 0.9400
2022-06-30 22:44:48 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 0.9227
2022-06-30 22:45:29 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.0805
2022-06-30 22:46:11 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.1374
2022-06-30 22:46:53 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.1364
2022-06-30 22:47:34 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.1014
2022-06-30 22:48:16 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.1941
2022-06-30 22:48:58 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.2650
2022-06-30 22:49:39 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.2888
2022-06-30 22:50:21 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.0527
2022-06-30 22:51:03 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.0984
2022-06-30 22:51:44 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 0.9536
2022-06-30 22:52:26 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.1311
2022-06-30 22:53:08 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.1538
2022-06-30 22:53:50 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.0345
2022-06-30 22:54:31 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.1005
2022-06-30 22:55:13 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.2598
2022-06-30 22:55:55 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.1813
2022-06-30 22:56:37 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 0.9649
2022-06-30 22:57:19 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.0471
2022-06-30 22:58:00 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.0239
2022-06-30 22:58:42 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 0.9944
2022-06-30 22:59:24 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 0.9361
2022-06-30 23:00:06 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.1684
2022-06-30 23:00:48 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.0914
2022-06-30 23:01:29 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.1669
2022-06-30 23:02:11 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.0669
2022-06-30 23:02:53 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.0301
2022-06-30 23:03:35 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.0390
2022-06-30 23:04:17 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.1374
2022-06-30 23:04:59 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.1284
2022-06-30 23:05:41 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 0.9386
2022-06-30 23:06:22 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.1321
2022-06-30 23:07:04 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.1437
2022-06-30 23:07:46 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.0913
2022-06-30 23:08:28 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.0788
2022-06-30 23:09:10 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.0173
2022-06-30 23:09:51 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.1615
2022-06-30 23:10:33 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 0.8923
2022-06-30 23:11:15 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 0.9994
2022-06-30 23:11:57 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.0457
2022-06-30 23:12:39 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.1355
2022-06-30 23:13:21 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.0872
2022-06-30 23:14:02 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.0664
2022-06-30 23:14:05 - train: epoch 036, train_loss: 1.1081
2022-06-30 23:15:18 - eval: epoch: 036, acc1: 74.644%, acc5: 92.190%, test_loss: 1.0097, per_image_load_time: 1.291ms, per_image_inference_time: 0.874ms
2022-06-30 23:15:19 - until epoch: 036, best_acc1: 74.644%
2022-06-30 23:15:19 - epoch 037 lr: 0.010000
2022-06-30 23:16:06 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 0.9039
2022-06-30 23:16:48 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 0.8461
2022-06-30 23:17:29 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.0057
2022-06-30 23:18:11 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.2174
2022-06-30 23:18:53 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 0.9599
2022-06-30 23:19:34 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.1114
2022-06-30 23:20:16 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.1483
2022-06-30 23:20:58 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.0738
2022-06-30 23:21:39 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.4197
2022-06-30 23:22:21 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.0305
2022-06-30 23:23:02 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.1701
2022-06-30 23:23:44 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.0650
2022-06-30 23:24:26 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.0781
2022-06-30 23:25:08 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.0978
2022-06-30 23:25:49 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 0.9742
2022-06-30 23:26:31 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 0.9665
2022-06-30 23:27:12 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.3147
2022-06-30 23:27:54 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.2587
2022-06-30 23:28:35 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.2856
2022-06-30 23:29:17 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.0929
2022-06-30 23:29:59 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.1295
2022-06-30 23:30:40 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.1449
2022-06-30 23:31:22 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 0.9616
2022-06-30 23:32:04 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.1063
2022-06-30 23:32:45 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.0217
2022-06-30 23:33:27 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.3477
2022-06-30 23:34:08 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.0638
2022-06-30 23:34:50 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.0851
2022-06-30 23:35:32 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.1383
2022-06-30 23:36:14 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.1008
2022-06-30 23:36:56 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.0507
2022-06-30 23:37:38 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.1803
2022-06-30 23:38:20 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 0.9808
2022-06-30 23:39:01 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 0.9816
2022-06-30 23:39:43 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.1436
2022-06-30 23:40:25 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.2143
2022-06-30 23:41:07 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.0967
2022-06-30 23:41:49 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.0167
2022-06-30 23:42:31 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.2476
2022-06-30 23:43:13 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 0.9663
2022-06-30 23:43:54 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.1551
2022-06-30 23:44:36 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.2439
2022-06-30 23:45:18 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.2456
2022-06-30 23:46:00 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.0763
2022-06-30 23:46:42 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.0937
2022-06-30 23:47:23 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.1845
2022-06-30 23:48:05 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.0815
2022-06-30 23:48:47 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.0375
2022-06-30 23:49:29 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.1194
2022-06-30 23:50:10 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.0636
2022-06-30 23:50:12 - train: epoch 037, train_loss: 1.0974
2022-06-30 23:51:27 - eval: epoch: 037, acc1: 74.676%, acc5: 92.306%, test_loss: 1.0012, per_image_load_time: 1.945ms, per_image_inference_time: 0.870ms
2022-06-30 23:51:28 - until epoch: 037, best_acc1: 74.676%
2022-06-30 23:51:28 - epoch 038 lr: 0.010000
2022-06-30 23:52:15 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.0634
2022-06-30 23:52:57 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 0.8076
2022-06-30 23:53:39 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 0.7338
2022-06-30 23:54:20 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 0.9951
2022-06-30 23:55:02 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 0.9324
2022-06-30 23:55:44 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.0980
2022-06-30 23:56:25 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.0332
2022-06-30 23:57:07 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 0.9208
2022-06-30 23:57:49 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 0.9998
2022-06-30 23:58:30 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.1937
2022-06-30 23:59:12 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 0.9821
2022-06-30 23:59:53 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.0376
2022-07-01 00:00:35 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.2063
2022-07-01 00:01:16 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.1096
2022-07-01 00:01:58 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.1324
2022-07-01 00:02:40 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.1786
2022-07-01 00:03:22 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.1751
2022-07-01 00:04:03 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.1553
2022-07-01 00:04:45 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.1924
2022-07-01 00:05:27 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.0555
2022-07-01 00:06:09 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.1247
2022-07-01 00:06:50 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 0.9799
2022-07-01 00:07:32 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.0758
2022-07-01 00:08:13 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.2821
2022-07-01 00:08:55 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 0.9791
2022-07-01 00:09:37 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.1048
2022-07-01 00:10:19 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.2836
2022-07-01 00:11:00 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.1954
2022-07-01 00:11:42 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.2131
2022-07-01 00:12:23 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.1598
2022-07-01 00:13:04 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.0415
2022-07-01 00:13:46 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 0.8393
2022-07-01 00:14:28 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 0.9223
2022-07-01 00:15:09 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.0393
2022-07-01 00:15:51 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.1447
2022-07-01 00:16:32 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.0891
2022-07-01 00:17:14 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 0.9145
2022-07-01 00:17:55 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.1360
2022-07-01 00:18:37 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.1207
2022-07-01 00:19:19 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.0919
2022-07-01 00:20:00 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.1030
2022-07-01 00:20:42 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 0.9738
2022-07-01 00:21:24 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.2109
2022-07-01 00:22:05 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.2366
2022-07-01 00:22:47 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.1419
2022-07-01 00:23:29 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.2062
2022-07-01 00:24:10 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 0.9718
2022-07-01 00:24:52 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.1646
2022-07-01 00:25:34 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.1037
2022-07-01 00:26:15 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 0.8914
2022-07-01 00:26:17 - train: epoch 038, train_loss: 1.0839
2022-07-01 00:27:31 - eval: epoch: 038, acc1: 74.292%, acc5: 92.318%, test_loss: 1.0055, per_image_load_time: 1.848ms, per_image_inference_time: 0.896ms
2022-07-01 00:27:32 - until epoch: 038, best_acc1: 74.676%
2022-07-01 00:27:32 - epoch 039 lr: 0.010000
2022-07-01 00:28:19 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.1238
2022-07-01 00:29:01 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.2340
2022-07-01 00:29:43 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 0.9907
2022-07-01 00:30:24 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.1313
2022-07-01 00:31:06 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.0086
2022-07-01 00:31:48 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 0.9622
2022-07-01 00:32:30 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.2109
2022-07-01 00:33:12 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.0063
2022-07-01 00:33:53 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.1221
2022-07-01 00:34:35 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.0389
2022-07-01 00:35:16 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.1087
2022-07-01 00:35:58 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.0737
2022-07-01 00:36:39 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.2493
2022-07-01 00:37:21 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.1630
2022-07-01 00:38:03 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.0290
2022-07-01 00:38:45 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.1088
2022-07-01 00:39:27 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 0.9689
2022-07-01 00:40:08 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.0147
2022-07-01 00:40:50 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 0.8562
2022-07-01 00:41:32 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 0.9078
2022-07-01 00:42:13 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.0193
2022-07-01 00:42:55 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.0444
2022-07-01 00:43:36 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.3065
2022-07-01 00:44:18 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.1264
2022-07-01 00:45:00 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.0445
2022-07-01 00:45:41 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.1018
2022-07-01 00:46:23 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.2580
2022-07-01 00:47:04 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 0.9884
2022-07-01 00:47:46 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 0.9255
2022-07-01 00:48:27 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.1747
2022-07-01 00:49:09 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 0.9587
2022-07-01 00:49:51 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.0670
2022-07-01 00:50:32 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.0979
2022-07-01 00:51:14 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.1478
2022-07-01 00:51:56 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.2690
2022-07-01 00:52:37 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.2148
2022-07-01 00:53:19 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.0370
2022-07-01 00:54:01 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.0382
2022-07-01 00:54:43 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.1409
2022-07-01 00:55:24 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.1289
2022-07-01 00:56:06 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.1418
2022-07-01 00:56:48 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.0779
2022-07-01 00:57:29 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.0794
2022-07-01 00:58:11 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 0.9079
2022-07-01 00:58:53 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.0159
2022-07-01 00:59:34 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.2416
2022-07-01 01:00:16 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.0894
2022-07-01 01:00:58 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.1451
2022-07-01 01:01:39 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 0.9382
2022-07-01 01:02:21 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 0.9138
2022-07-01 01:02:23 - train: epoch 039, train_loss: 1.0768
2022-07-01 01:03:37 - eval: epoch: 039, acc1: 74.268%, acc5: 92.134%, test_loss: 1.0172, per_image_load_time: 1.976ms, per_image_inference_time: 0.897ms
2022-07-01 01:03:38 - until epoch: 039, best_acc1: 74.676%
2022-07-01 01:03:38 - epoch 040 lr: 0.010000
2022-07-01 01:04:25 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.1862
2022-07-01 01:05:07 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.3070
2022-07-01 01:05:48 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.1255
2022-07-01 01:06:30 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 0.9966
2022-07-01 01:07:11 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 0.9593
2022-07-01 01:07:53 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.1274
2022-07-01 01:08:34 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.0965
2022-07-01 01:09:16 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.1887
2022-07-01 01:09:58 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 0.9833
2022-07-01 01:10:39 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 0.7533
2022-07-01 01:11:21 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 0.9801
2022-07-01 01:12:02 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.0119
2022-07-01 01:12:44 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 0.9832
2022-07-01 01:13:26 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 0.8407
2022-07-01 01:14:07 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.1832
2022-07-01 01:14:49 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.1496
2022-07-01 01:15:30 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.1124
2022-07-01 01:16:12 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 0.9854
2022-07-01 01:16:54 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.0556
2022-07-01 01:17:36 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.1223
2022-07-01 01:18:17 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 0.9619
2022-07-01 01:18:59 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 0.8975
2022-07-01 01:19:41 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 0.9202
2022-07-01 01:20:22 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.0690
2022-07-01 01:21:04 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.1035
2022-07-01 01:21:46 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.0041
2022-07-01 01:22:27 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.1121
2022-07-01 01:23:09 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.0766
2022-07-01 01:23:51 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.2888
2022-07-01 01:24:33 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.2327
2022-07-01 01:25:14 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 0.9785
2022-07-01 01:25:56 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.1923
2022-07-01 01:26:38 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.1092
2022-07-01 01:27:19 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.0483
2022-07-01 01:28:01 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.1902
2022-07-01 01:28:43 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.0968
2022-07-01 01:29:24 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.0961
2022-07-01 01:30:06 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.0041
2022-07-01 01:30:48 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.1635
2022-07-01 01:31:30 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.1810
2022-07-01 01:32:11 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.1350
2022-07-01 01:32:53 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 0.9815
2022-07-01 01:33:35 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.2113
2022-07-01 01:34:16 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.0765
