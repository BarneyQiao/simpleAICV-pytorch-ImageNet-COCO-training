2022-02-22 11:08:20 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 0.8030
2022-02-22 11:09:02 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 0.8091
2022-02-22 11:09:44 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 0.6486
2022-02-22 11:10:26 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 0.8090
2022-02-22 11:11:08 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 0.7201
2022-02-22 11:11:50 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 0.8187
2022-02-22 11:12:32 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 0.7406
2022-02-22 11:13:15 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 0.6625
2022-02-22 11:13:57 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 0.6014
2022-02-22 11:14:39 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 0.6837
2022-02-22 11:15:21 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 0.6448
2022-02-22 11:16:03 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 0.8348
2022-02-22 11:16:45 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 0.8514
2022-02-22 11:17:27 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 0.9135
2022-02-22 11:18:09 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 0.6934
2022-02-22 11:18:51 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 0.5729
2022-02-22 11:19:33 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 0.9798
2022-02-22 11:20:15 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 0.6648
2022-02-22 11:20:56 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 0.6811
2022-02-22 11:21:38 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 0.7397
2022-02-22 11:22:20 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 0.6187
2022-02-22 11:23:02 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 0.6910
2022-02-22 11:23:44 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 0.8858
2022-02-22 11:24:26 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 0.6447
2022-02-22 11:25:08 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 0.5723
2022-02-22 11:25:50 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 0.8625
2022-02-22 11:26:32 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 0.8850
2022-02-22 11:27:15 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 0.8329
2022-02-22 11:27:58 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 0.7386
2022-02-22 11:28:42 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 0.8584
2022-02-22 11:29:27 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 0.9185
2022-02-22 11:30:15 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 0.6597
2022-02-22 11:30:19 - train: epoch 079, train_loss: 0.7459
2022-02-22 11:31:42 - eval: epoch: 079, acc1: 78.084%, acc5: 93.968%, test_loss: 0.8668, per_image_load_time: 0.941ms, per_image_inference_time: 0.870ms
2022-02-22 11:31:44 - until epoch: 079, best_acc1: 78.112%
2022-02-22 11:31:44 - epoch 080 lr: 0.0010000000000000002
2022-02-22 11:32:29 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 0.6265
2022-02-22 11:33:11 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 0.6811
2022-02-22 11:33:52 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 0.8295
2022-02-22 11:34:33 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 0.6274
2022-02-22 11:35:15 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 0.8378
2022-02-22 11:35:56 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 0.6774
2022-02-22 11:36:37 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 0.7109
2022-02-22 11:37:18 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 0.6574
2022-02-22 11:38:00 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 0.7526
2022-02-22 11:38:41 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 0.5876
2022-02-22 11:39:22 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 0.6944
2022-02-22 11:40:03 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 0.6404
2022-02-22 11:40:44 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 0.7091
2022-02-22 11:41:25 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 0.7760
2022-02-22 11:42:06 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 0.6885
2022-02-22 11:42:48 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 0.7562
2022-02-22 11:43:29 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 0.7997
2022-02-22 11:44:10 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 0.8553
2022-02-22 11:44:51 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 0.7192
2022-02-22 11:45:33 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 0.8080
2022-02-22 11:46:14 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 0.7342
2022-02-22 11:46:55 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 0.8167
2022-02-22 11:47:36 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 0.6775
2022-02-22 11:48:17 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 0.7623
2022-02-22 11:48:58 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 0.6954
2022-02-22 11:49:40 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 0.7227
2022-02-22 11:50:21 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 0.8943
2022-02-22 11:51:02 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 0.7883
2022-02-22 11:51:44 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 0.6389
2022-02-22 11:52:25 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 0.7228
2022-02-22 11:53:06 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 0.8615
2022-02-22 11:53:47 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 0.5827
2022-02-22 11:54:28 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 0.7776
2022-02-22 11:55:09 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 0.7150
2022-02-22 11:55:51 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 0.5962
2022-02-22 11:56:32 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 0.7621
2022-02-22 11:57:14 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 0.7262
2022-02-22 11:57:55 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 0.8060
2022-02-22 11:58:37 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 0.7621
2022-02-22 11:59:18 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 0.8806
2022-02-22 11:59:59 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 0.7974
2022-02-22 12:00:41 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 0.7427
2022-02-22 12:01:23 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 0.8273
2022-02-22 12:02:05 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 0.7354
2022-02-22 12:02:46 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 0.6977
2022-02-22 12:03:28 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 0.8034
2022-02-22 12:04:11 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 0.7550
2022-02-22 12:04:54 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 0.8108
2022-02-22 12:05:37 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 0.7599
2022-02-22 12:06:21 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 0.5055
2022-02-22 12:06:25 - train: epoch 080, train_loss: 0.7423
2022-02-22 12:07:45 - eval: epoch: 080, acc1: 77.952%, acc5: 93.892%, test_loss: 0.8742, per_image_load_time: 1.033ms, per_image_inference_time: 0.949ms
2022-02-22 12:07:46 - until epoch: 080, best_acc1: 78.112%
2022-02-22 12:07:46 - epoch 081 lr: 0.0010000000000000002
2022-02-22 12:08:33 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 0.5971
2022-02-22 12:09:14 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 0.7938
2022-02-22 12:09:55 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 0.6380
2022-02-22 12:10:36 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 0.8613
2022-02-22 12:11:17 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 0.7699
2022-02-22 12:11:58 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 0.7440
2022-02-22 12:12:39 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 0.7048
2022-02-22 12:13:21 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 0.8974
2022-02-22 12:14:02 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 0.6266
2022-02-22 12:14:43 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 0.7123
2022-02-22 12:15:24 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 0.7787
2022-02-22 12:16:05 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 0.7243
2022-02-22 12:16:46 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 0.6459
2022-02-22 12:17:27 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 0.4735
2022-02-22 12:18:08 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 0.7108
2022-02-22 12:18:50 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 0.6557
2022-02-22 12:19:31 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 0.8219
2022-02-22 12:20:12 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 0.6244
2022-02-22 12:20:53 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 0.7295
2022-02-22 12:21:34 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 0.8047
2022-02-22 12:22:15 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 0.6650
2022-02-22 12:22:57 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 0.7714
2022-02-22 12:23:38 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 0.7759
2022-02-22 12:24:19 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 0.6916
2022-02-22 12:25:00 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 0.8597
2022-02-22 12:25:41 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 0.7274
2022-02-22 12:26:22 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 0.7319
2022-02-22 12:27:03 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 0.6509
2022-02-22 12:27:45 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 0.7093
2022-02-22 12:28:26 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 0.7302
2022-02-22 12:29:07 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 0.5871
2022-02-22 12:29:48 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 0.7061
2022-02-22 12:30:29 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 0.7962
2022-02-22 12:31:10 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 0.8238
2022-02-22 12:31:51 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 0.7564
2022-02-22 12:32:33 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 0.6605
2022-02-22 12:33:14 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 0.8048
2022-02-22 12:33:55 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 0.6253
2022-02-22 12:34:36 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 0.7930
2022-02-22 12:35:18 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 0.5278
2022-02-22 12:35:59 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 0.8284
2022-02-22 12:36:40 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 0.7436
2022-02-22 12:37:22 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 0.7413
2022-02-22 12:38:09 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 0.8295
2022-02-22 12:38:56 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 0.6990
2022-02-22 12:39:41 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 0.6715
2022-02-22 12:40:24 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 0.7966
2022-02-22 12:41:08 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 0.7896
2022-02-22 12:41:54 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 0.6802
2022-02-22 12:42:43 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 0.5840
2022-02-22 12:42:47 - train: epoch 081, train_loss: 0.7370
2022-02-22 12:44:13 - eval: epoch: 081, acc1: 78.094%, acc5: 93.884%, test_loss: 0.8707, per_image_load_time: 0.834ms, per_image_inference_time: 0.921ms
2022-02-22 12:44:15 - until epoch: 081, best_acc1: 78.112%
2022-02-22 12:44:15 - epoch 082 lr: 0.0010000000000000002
2022-02-22 12:45:01 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 0.5890
2022-02-22 12:45:42 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 0.6340
2022-02-22 12:46:23 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 0.8360
2022-02-22 12:47:05 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 0.8051
2022-02-22 12:47:46 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 0.8057
2022-02-22 12:48:27 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 0.5638
2022-02-22 12:49:09 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 0.8280
2022-02-22 12:49:50 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 0.7672
2022-02-22 12:50:31 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 0.8229
2022-02-22 12:51:13 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 0.7988
2022-02-22 12:51:54 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 0.9175
2022-02-22 12:52:36 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 0.7045
2022-02-22 12:53:18 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 0.8332
2022-02-22 12:53:59 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 0.6796
2022-02-22 12:54:41 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 0.6679
2022-02-22 12:55:22 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 0.7228
2022-02-22 12:56:04 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 0.7018
2022-02-22 12:56:45 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 0.6439
2022-02-22 12:57:27 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 0.7560
2022-02-22 12:58:08 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 0.6694
2022-02-22 12:58:50 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 0.6824
2022-02-22 12:59:32 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 0.7275
2022-02-22 13:00:13 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 0.7451
2022-02-22 13:00:55 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 0.7819
2022-02-22 13:01:36 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 0.6475
2022-02-22 13:02:18 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 0.7088
2022-02-22 13:03:00 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 0.7114
2022-02-22 13:03:41 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 0.7575
2022-02-22 13:04:23 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 0.7275
2022-02-22 13:05:05 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 0.6485
2022-02-22 13:05:47 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 0.6458
2022-02-22 13:06:29 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 0.8559
2022-02-22 13:07:11 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 0.7192
2022-02-22 13:07:53 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 0.7419
2022-02-22 13:08:35 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 0.6678
2022-02-22 13:09:16 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 0.6507
2022-02-22 13:09:58 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 0.6808
2022-02-22 13:10:40 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 0.9724
2022-02-22 13:11:23 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 0.7163
2022-02-22 13:12:06 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 0.8718
2022-02-22 13:12:47 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 0.6751
2022-02-22 13:13:29 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 0.7631
2022-02-22 13:14:11 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 0.5993
2022-02-22 13:14:53 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 0.7200
2022-02-22 13:15:35 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 0.7939
2022-02-22 13:16:17 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 0.8521
2022-02-22 13:17:00 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 0.6622
2022-02-22 13:17:43 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 0.6903
2022-02-22 13:18:29 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 0.6752
2022-02-22 13:19:17 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 0.7596
2022-02-22 13:19:20 - train: epoch 082, train_loss: 0.7327
2022-02-22 13:20:43 - eval: epoch: 082, acc1: 78.000%, acc5: 93.924%, test_loss: 0.8710, per_image_load_time: 2.062ms, per_image_inference_time: 0.942ms
2022-02-22 13:20:45 - until epoch: 082, best_acc1: 78.112%
2022-02-22 13:20:45 - epoch 083 lr: 0.0010000000000000002
2022-02-22 13:21:32 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 0.5851
2022-02-22 13:22:14 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 0.6535
2022-02-22 13:22:56 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 0.8042
2022-02-22 13:23:37 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 0.8333
2022-02-22 13:24:19 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 0.5977
2022-02-22 13:25:01 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 0.6756
2022-02-22 13:25:42 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 0.7109
2022-02-22 13:26:24 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 0.6682
2022-02-22 13:27:05 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 0.7081
2022-02-22 13:27:46 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 0.8369
2022-02-22 13:28:28 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 0.6853
2022-02-22 13:29:10 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 0.7165
2022-02-22 13:29:52 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 0.6169
2022-02-22 13:30:33 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 0.7708
2022-02-22 13:31:15 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 0.6724
2022-02-22 13:31:56 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 0.7563
2022-02-22 13:32:38 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 0.8398
2022-02-22 13:33:20 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 0.8555
2022-02-22 13:34:02 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 0.5610
2022-02-22 13:34:43 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 0.4959
2022-02-22 13:35:26 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 0.6877
2022-02-22 13:36:07 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 0.7556
2022-02-22 13:36:49 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 0.7350
2022-02-22 13:37:31 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 0.7617
2022-02-22 13:38:13 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 0.8247
2022-02-22 13:38:55 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 0.6389
2022-02-22 13:39:36 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 0.7864
2022-02-22 13:40:18 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 0.6440
2022-02-22 13:40:59 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 0.6868
2022-02-22 13:41:41 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 0.8201
2022-02-22 13:42:23 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 0.6238
2022-02-22 13:43:05 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 0.6989
2022-02-22 13:43:47 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 0.9565
2022-02-22 13:44:29 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 0.8257
2022-02-22 13:45:11 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 0.7572
2022-02-22 13:45:53 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 0.7628
2022-02-22 13:46:36 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 0.7833
2022-02-22 13:47:18 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 0.6493
2022-02-22 13:48:00 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 0.6878
2022-02-22 13:48:42 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 0.8150
2022-02-22 13:49:24 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 0.7139
2022-02-22 13:50:06 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 0.8969
2022-02-22 13:50:48 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 0.7471
2022-02-22 13:51:30 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 0.5741
2022-02-22 13:52:12 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 0.8009
2022-02-22 13:52:55 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 0.9202
2022-02-22 13:53:38 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 0.7736
2022-02-22 13:54:23 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 0.7841
2022-02-22 13:55:07 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 0.9726
2022-02-22 13:55:54 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 0.8849
2022-02-22 13:55:57 - train: epoch 083, train_loss: 0.7293
2022-02-22 13:57:20 - eval: epoch: 083, acc1: 77.896%, acc5: 93.864%, test_loss: 0.8737, per_image_load_time: 1.650ms, per_image_inference_time: 0.919ms
2022-02-22 13:57:21 - until epoch: 083, best_acc1: 78.112%
2022-02-22 13:57:21 - epoch 084 lr: 0.0010000000000000002
2022-02-22 13:58:08 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 0.6721
2022-02-22 13:58:49 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 0.7773
2022-02-22 13:59:31 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 0.6465
2022-02-22 14:00:13 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 0.7331
2022-02-22 14:00:54 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 0.7094
2022-02-22 14:01:36 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 0.8861
2022-02-22 14:02:17 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 0.7845
2022-02-22 14:02:59 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 0.7939
2022-02-22 14:03:40 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 0.7007
2022-02-22 14:04:22 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 0.7654
2022-02-22 14:05:04 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 0.7079
2022-02-22 14:05:45 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 0.9545
2022-02-22 14:06:27 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 0.6300
2022-02-22 14:07:08 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 0.7640
2022-02-22 14:07:50 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 0.7015
2022-02-22 14:08:31 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 0.6145
2022-02-22 14:09:13 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 0.8062
2022-02-22 14:09:55 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 0.7717
2022-02-22 14:10:36 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 0.8334
2022-02-22 14:11:18 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 0.8423
2022-02-22 14:12:00 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 0.6387
2022-02-22 14:12:42 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 0.5565
2022-02-22 14:13:24 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 0.6333
2022-02-22 14:14:06 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 0.6287
2022-02-22 14:14:48 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 0.7586
2022-02-22 14:15:30 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 0.8728
2022-02-22 14:16:12 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 0.7119
2022-02-22 14:16:53 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 0.8387
2022-02-22 14:17:35 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 0.6745
2022-02-22 14:18:17 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 0.7259
2022-02-22 14:18:59 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 0.6120
2022-02-22 14:19:42 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 0.5634
2022-02-22 14:20:24 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 0.6362
2022-02-22 14:21:06 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 0.6017
2022-02-22 14:21:47 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 0.5479
2022-02-22 14:22:30 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 0.7548
2022-02-22 14:23:12 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 0.7546
2022-02-22 14:23:55 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 0.9068
2022-02-22 14:24:37 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 0.8675
2022-02-22 14:25:20 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 0.8050
2022-02-22 14:26:02 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 0.7742
2022-02-22 14:26:45 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 0.8083
2022-02-22 14:27:28 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 0.6890
2022-02-22 14:28:11 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 0.8253
2022-02-22 14:28:55 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 0.7358
2022-02-22 14:29:38 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 0.7252
2022-02-22 14:30:21 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 0.9730
2022-02-22 14:31:06 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 0.6195
2022-02-22 14:31:50 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 0.5071
2022-02-22 14:32:36 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 0.8149
2022-02-22 14:32:41 - train: epoch 084, train_loss: 0.7250
2022-02-22 14:34:05 - eval: epoch: 084, acc1: 78.070%, acc5: 93.888%, test_loss: 0.8724, per_image_load_time: 1.315ms, per_image_inference_time: 0.875ms
2022-02-22 14:34:06 - until epoch: 084, best_acc1: 78.112%
2022-02-22 14:34:06 - epoch 085 lr: 0.0010000000000000002
2022-02-22 14:34:54 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 0.5909
2022-02-22 14:35:36 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 0.5711
2022-02-22 14:36:19 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 0.8229
2022-02-22 14:37:01 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 0.6548
2022-02-22 14:37:44 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 0.9296
2022-02-22 14:38:27 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 0.5695
2022-02-22 14:39:09 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 0.6479
2022-02-22 14:39:52 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 0.6912
2022-02-22 14:40:34 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 0.7342
2022-02-22 14:41:17 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 0.7683
2022-02-22 14:42:00 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 0.7084
2022-02-22 14:42:43 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 0.6633
2022-02-22 14:43:25 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 0.6935
2022-02-22 14:44:08 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 0.8426
2022-02-22 14:44:51 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 0.6066
2022-02-22 14:45:34 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 0.7519
2022-02-22 14:46:17 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 0.8814
2022-02-22 14:47:00 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 0.6044
2022-02-22 14:47:43 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 0.6466
2022-02-22 14:48:26 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 0.7383
2022-02-22 14:49:09 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 0.5735
2022-02-22 14:49:52 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 0.7884
2022-02-22 14:50:34 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 0.5672
2022-02-22 14:51:17 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 0.8220
2022-02-22 14:52:00 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 0.8862
2022-02-22 14:52:43 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 0.8272
2022-02-22 14:53:26 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 0.6811
2022-02-22 14:54:08 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 0.8221
2022-02-22 14:54:51 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 0.9239
2022-02-22 14:55:34 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 0.8018
2022-02-22 14:56:17 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 0.6694
2022-02-22 14:57:00 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 0.6751
2022-02-22 14:57:42 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 0.7497
2022-02-22 14:58:25 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 0.8177
2022-02-22 14:59:08 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 0.7944
2022-02-22 14:59:51 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 0.7290
2022-02-22 15:00:33 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 0.6556
2022-02-22 15:01:16 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 0.6086
2022-02-22 15:01:59 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 0.6590
2022-02-22 15:02:42 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 0.9456
2022-02-22 15:03:25 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 0.7276
2022-02-22 15:04:08 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 0.8245
2022-02-22 15:04:50 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 0.7110
2022-02-22 15:05:33 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 0.7179
2022-02-22 15:06:16 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 0.7876
2022-02-22 15:07:00 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 0.7614
2022-02-22 15:07:43 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 0.9242
2022-02-22 15:08:26 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 0.6359
2022-02-22 15:09:12 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 0.6953
2022-02-22 15:10:03 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 0.7283
2022-02-22 15:10:07 - train: epoch 085, train_loss: 0.7236
2022-02-22 15:11:43 - eval: epoch: 085, acc1: 77.844%, acc5: 93.920%, test_loss: 0.8801, per_image_load_time: 2.706ms, per_image_inference_time: 0.838ms
2022-02-22 15:11:44 - until epoch: 085, best_acc1: 78.112%
2022-02-22 15:11:44 - epoch 086 lr: 0.0010000000000000002
2022-02-22 15:12:32 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 0.6308
2022-02-22 15:13:15 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 0.6886
2022-02-22 15:13:58 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 0.7629
2022-02-22 15:14:41 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 0.7829
2022-02-22 15:15:24 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 0.6677
2022-02-22 15:16:06 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 0.6942
2022-02-22 15:16:48 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 0.6300
2022-02-22 15:17:31 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 0.7853
2022-02-22 15:18:13 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 0.8023
2022-02-22 15:18:56 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 0.7182
2022-02-22 15:19:39 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 0.7416
2022-02-22 15:20:22 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 0.6439
2022-02-22 15:21:05 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 0.7267
2022-02-22 15:21:47 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 0.6188
2022-02-22 15:22:30 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 0.5732
2022-02-22 15:23:12 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 0.7015
2022-02-22 15:23:55 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 0.6647
2022-02-22 15:24:38 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 0.6842
2022-02-22 15:25:21 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 0.8275
2022-02-22 15:26:03 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 0.7280
2022-02-22 15:26:46 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 0.6567
2022-02-22 15:27:29 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 0.7676
2022-02-22 15:28:12 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 0.6832
2022-02-22 15:28:55 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 0.6138
2022-02-22 15:29:37 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 0.7298
2022-02-22 15:30:20 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 0.7856
2022-02-22 15:31:04 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 0.6000
2022-02-22 15:31:47 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 0.8695
2022-02-22 15:32:29 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 0.5904
2022-02-22 15:33:12 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 0.8289
2022-02-22 15:33:55 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 0.7436
2022-02-22 15:34:38 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 0.7993
2022-02-22 15:35:20 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 0.7807
2022-02-22 15:36:03 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 0.7257
2022-02-22 15:36:46 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 0.6899
2022-02-22 15:37:29 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 0.7155
2022-02-22 15:38:12 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 0.7681
2022-02-22 15:38:55 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 0.7529
2022-02-22 15:39:37 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 0.7551
2022-02-22 15:40:20 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 0.7891
2022-02-22 15:41:03 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 0.7386
2022-02-22 15:41:46 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 0.7828
2022-02-22 15:42:30 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 0.7239
2022-02-22 15:43:14 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 0.6991
2022-02-22 15:43:58 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 0.6464
2022-02-22 15:44:41 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 0.7020
2022-02-22 15:45:25 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 0.6719
2022-02-22 15:46:09 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 0.7205
2022-02-22 15:46:56 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 0.7793
2022-02-22 15:47:41 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 0.7634
2022-02-22 15:47:45 - train: epoch 086, train_loss: 0.7168
2022-02-22 15:49:11 - eval: epoch: 086, acc1: 77.990%, acc5: 93.932%, test_loss: 0.8808, per_image_load_time: 2.132ms, per_image_inference_time: 0.992ms
2022-02-22 15:49:12 - until epoch: 086, best_acc1: 78.112%
2022-02-22 15:49:12 - epoch 087 lr: 0.0010000000000000002
2022-02-22 15:50:01 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 0.8422
2022-02-22 15:50:43 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 0.5664
2022-02-22 15:51:25 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 0.7819
2022-02-22 15:52:08 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 0.7922
2022-02-22 15:52:50 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 0.5225
2022-02-22 15:53:32 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 0.7657
2022-02-22 15:54:15 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 0.5023
2022-02-22 15:54:57 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 0.7958
2022-02-22 15:55:40 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 0.7255
2022-02-22 15:56:22 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 0.6629
2022-02-22 15:57:05 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 0.7339
2022-02-22 15:57:48 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 0.7127
2022-02-22 15:58:30 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 0.7788
2022-02-22 15:59:12 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 0.7064
2022-02-22 15:59:55 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 0.6568
2022-02-22 16:00:38 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 0.5544
2022-02-22 16:01:20 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 0.9007
2022-02-22 16:02:03 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 0.7517
2022-02-22 16:02:46 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 0.7721
2022-02-22 16:03:28 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 0.6619
2022-02-22 16:04:11 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 0.7626
2022-02-22 16:04:53 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 0.6881
2022-02-22 16:05:36 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 0.7612
2022-02-22 16:06:18 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 0.6821
2022-02-22 16:07:01 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 0.6360
2022-02-22 16:07:44 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 0.7641
2022-02-22 16:08:26 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 0.6212
2022-02-22 16:09:09 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 0.5194
2022-02-22 16:09:52 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 0.5923
2022-02-22 16:10:34 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 0.7117
2022-02-22 16:11:17 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 0.7348
2022-02-22 16:11:59 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 0.6272
2022-02-22 16:12:41 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 0.8095
2022-02-22 16:13:24 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 0.8020
2022-02-22 16:14:06 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 0.5497
2022-02-22 16:14:49 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 0.6242
2022-02-22 16:15:32 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 0.6786
2022-02-22 16:16:14 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 0.6787
2022-02-22 16:16:57 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 0.6908
2022-02-22 16:17:40 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 0.7004
2022-02-22 16:18:22 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 0.8239
2022-02-22 16:19:05 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 0.7951
2022-02-22 16:19:50 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 0.7345
2022-02-22 16:20:36 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 0.7922
2022-02-22 16:21:19 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 0.7890
2022-02-22 16:22:02 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 0.6998
2022-02-22 16:22:45 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 0.7619
2022-02-22 16:23:31 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 0.6972
2022-02-22 16:24:32 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 0.6802
2022-02-22 16:25:24 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 0.6915
2022-02-22 16:25:29 - train: epoch 087, train_loss: 0.7129
2022-02-22 16:26:57 - eval: epoch: 087, acc1: 77.906%, acc5: 93.884%, test_loss: 0.8774, per_image_load_time: 0.631ms, per_image_inference_time: 0.923ms
2022-02-22 16:26:58 - until epoch: 087, best_acc1: 78.112%
2022-02-22 16:26:58 - epoch 088 lr: 0.0010000000000000002
2022-02-22 16:27:47 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 0.6633
2022-02-22 16:28:30 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 0.6288
2022-02-22 16:29:13 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 0.7864
2022-02-22 16:29:55 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 0.6910
2022-02-22 16:30:37 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 0.6118
2022-02-22 16:31:20 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 0.6671
2022-02-22 16:32:02 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 0.6264
2022-02-22 16:32:44 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 0.7495
2022-02-22 16:33:27 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 0.7702
2022-02-22 16:34:09 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 0.6944
2022-02-22 16:34:51 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 0.6886
2022-02-22 16:35:34 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 0.7366
2022-02-22 16:36:16 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 0.7344
2022-02-22 16:36:58 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 0.7213
2022-02-22 16:37:41 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 0.7138
2022-02-22 16:38:23 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 0.5784
2022-02-22 16:39:06 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 0.7432
2022-02-22 16:39:48 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 0.6604
2022-02-22 16:40:31 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 0.7103
2022-02-22 16:41:14 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 0.6051
2022-02-22 16:41:56 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 0.7356
2022-02-22 16:42:38 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 0.5909
2022-02-22 16:43:21 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 0.5594
2022-02-22 16:44:04 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 0.7571
2022-02-22 16:44:46 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 0.7086
2022-02-22 16:45:30 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 0.7246
2022-02-22 16:46:13 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 0.7596
2022-02-22 16:46:56 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 0.6245
2022-02-22 16:47:39 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 0.6594
2022-02-22 16:48:22 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 0.8002
2022-02-22 16:49:05 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 0.6131
2022-02-22 16:49:47 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 0.5704
2022-02-22 16:50:30 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 0.6829
2022-02-22 16:51:12 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 0.5516
2022-02-22 16:51:55 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 0.7035
2022-02-22 16:52:37 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 0.7464
2022-02-22 16:53:20 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 0.8097
2022-02-22 16:54:03 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 0.6916
2022-02-22 16:54:45 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 0.7681
2022-02-22 16:55:30 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 0.6876
2022-02-22 16:56:14 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 0.7139
2022-02-22 16:57:00 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 0.8291
2022-02-22 16:57:43 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 0.6781
2022-02-22 16:58:25 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 0.6584
2022-02-22 16:59:08 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 0.7265
2022-02-22 16:59:52 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 0.7604
2022-02-22 17:00:35 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 0.6658
2022-02-22 17:01:19 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 0.5817
2022-02-22 17:02:07 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 0.6166
2022-02-22 17:03:00 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 0.7881
2022-02-22 17:03:03 - train: epoch 088, train_loss: 0.7102
2022-02-22 17:04:30 - eval: epoch: 088, acc1: 77.944%, acc5: 93.908%, test_loss: 0.8786, per_image_load_time: 0.768ms, per_image_inference_time: 0.967ms
2022-02-22 17:04:32 - until epoch: 088, best_acc1: 78.112%
2022-02-22 17:04:32 - epoch 089 lr: 0.0010000000000000002
2022-02-22 17:05:19 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 0.8410
2022-02-22 17:06:01 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 0.5649
2022-02-22 17:06:44 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 0.7381
2022-02-22 17:07:26 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 0.6209
2022-02-22 17:08:08 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 0.7363
2022-02-22 17:08:51 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 0.6887
2022-02-22 17:09:33 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 0.8115
2022-02-22 17:10:15 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 0.8784
2022-02-22 17:10:58 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 0.7803
2022-02-22 17:11:40 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 0.8108
2022-02-22 17:12:22 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 0.7403
2022-02-22 17:13:05 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 0.8361
2022-02-22 17:13:48 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 0.6685
2022-02-22 17:14:30 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 0.8000
2022-02-22 17:15:12 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 0.7543
2022-02-22 17:15:55 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 0.7919
2022-02-22 17:16:37 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 0.7617
2022-02-22 17:17:20 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 0.6829
2022-02-22 17:18:02 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 0.6387
2022-02-22 17:18:44 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 0.6299
2022-02-22 17:19:27 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 0.8457
2022-02-22 17:20:09 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 0.7047
2022-02-22 17:20:52 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 0.5847
2022-02-22 17:21:35 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 0.6487
2022-02-22 17:22:17 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 0.5990
2022-02-22 17:23:00 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 0.7486
2022-02-22 17:23:43 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 0.6936
2022-02-22 17:24:25 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 0.7260
2022-02-22 17:25:08 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 0.7357
2022-02-22 17:25:50 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 0.7617
2022-02-22 17:26:33 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 0.7139
2022-02-22 17:27:15 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 0.7660
2022-02-22 17:27:58 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 0.7404
2022-02-22 17:28:41 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 0.7212
2022-02-22 17:29:23 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 0.6465
2022-02-22 17:30:07 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 0.6034
2022-02-22 17:30:50 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 0.8550
2022-02-22 17:31:33 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 0.6004
2022-02-22 17:32:16 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 0.6413
2022-02-22 17:32:59 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 0.5932
2022-02-22 17:33:42 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 0.9322
2022-02-22 17:34:24 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 0.6729
2022-02-22 17:35:07 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 0.7698
2022-02-22 17:35:50 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 0.5846
2022-02-22 17:36:34 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 0.6122
2022-02-22 17:37:17 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 0.7186
2022-02-22 17:38:02 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 0.7322
2022-02-22 17:38:45 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 0.7233
2022-02-22 17:39:30 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 0.7198
2022-02-22 17:40:20 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 0.5291
2022-02-22 17:40:24 - train: epoch 089, train_loss: 0.7083
2022-02-22 17:41:53 - eval: epoch: 089, acc1: 77.980%, acc5: 93.826%, test_loss: 0.8773, per_image_load_time: 1.698ms, per_image_inference_time: 0.933ms
2022-02-22 17:41:55 - until epoch: 089, best_acc1: 78.112%
2022-02-22 17:41:55 - epoch 090 lr: 0.0010000000000000002
2022-02-22 17:42:43 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 0.7496
2022-02-22 17:43:25 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 0.6806
2022-02-22 17:44:07 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 0.6948
2022-02-22 17:44:50 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 0.6955
2022-02-22 17:45:32 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 0.7584
2022-02-22 17:46:15 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 0.8255
2022-02-22 17:46:58 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 0.7376
2022-02-22 17:47:40 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 0.6314
2022-02-22 17:48:23 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 0.6407
2022-02-22 17:49:05 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 0.5906
2022-02-22 17:49:48 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 0.7102
2022-02-22 17:50:30 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 0.6657
2022-02-22 17:51:13 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 0.8459
2022-02-22 17:51:56 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 0.6096
2022-02-22 17:52:38 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 0.8519
2022-02-22 17:53:21 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 0.6394
2022-02-22 17:54:04 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 0.5839
2022-02-22 17:54:47 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 0.5725
2022-02-22 17:55:29 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 0.5974
2022-02-22 17:56:12 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 0.8709
2022-02-22 17:56:55 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 0.7640
2022-02-22 17:57:38 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 0.6628
2022-02-22 17:58:21 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 0.8626
2022-02-22 17:59:04 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 0.7366
2022-02-22 17:59:47 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 0.7764
2022-02-22 18:00:29 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 0.7777
2022-02-22 18:01:12 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 0.6177
2022-02-22 18:01:55 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 0.6973
2022-02-22 18:02:38 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 0.6586
2022-02-22 18:03:21 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 0.7332
2022-02-22 18:04:04 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 0.6765
2022-02-22 18:04:48 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 0.6435
2022-02-22 18:05:31 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 0.7828
2022-02-22 18:06:14 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 0.6062
2022-02-22 18:06:57 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 0.6543
2022-02-22 18:07:40 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 0.6599
2022-02-22 18:08:23 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 0.7034
2022-02-22 18:09:06 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 0.7139
2022-02-22 18:09:49 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 0.5405
2022-02-22 18:10:32 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 0.8306
2022-02-22 18:11:15 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 0.8475
2022-02-22 18:11:58 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 0.6682
2022-02-22 18:12:42 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 0.7717
2022-02-22 18:13:25 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 0.6823
2022-02-22 18:14:09 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 0.7151
2022-02-22 18:14:52 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 0.7030
2022-02-22 18:15:36 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 0.6807
2022-02-22 18:16:20 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 0.7331
2022-02-22 18:17:07 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 0.6278
2022-02-22 18:17:58 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 0.7382
2022-02-22 18:18:02 - train: epoch 090, train_loss: 0.7051
2022-02-22 18:19:36 - eval: epoch: 090, acc1: 77.780%, acc5: 93.840%, test_loss: 0.8836, per_image_load_time: 0.834ms, per_image_inference_time: 0.891ms
2022-02-22 18:19:38 - until epoch: 090, best_acc1: 78.112%
2022-02-22 18:19:38 - epoch 091 lr: 0.00010000000000000003
2022-02-22 18:20:26 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 0.6046
2022-02-22 18:21:09 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 0.6425
2022-02-22 18:21:52 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 0.5873
2022-02-22 18:22:35 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 0.5870
2022-02-22 18:23:18 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 0.6369
2022-02-22 18:24:01 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 0.6474
2022-02-22 18:24:44 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 0.6931
2022-02-22 18:25:27 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 0.6017
2022-02-22 18:26:10 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 0.7037
2022-02-22 18:26:53 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 0.6444
2022-02-22 18:27:36 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 0.6097
2022-02-22 18:28:19 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 0.8186
2022-02-22 18:29:02 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 0.5866
2022-02-22 18:29:45 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 0.7338
2022-02-22 18:30:28 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 0.6411
2022-02-22 18:31:11 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 0.6741
2022-02-22 18:31:54 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 0.7514
2022-02-22 18:32:37 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 0.6244
2022-02-22 18:33:19 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 0.5539
2022-02-22 18:34:03 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 0.6850
2022-02-22 18:34:45 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 0.5327
2022-02-22 18:35:28 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 0.7404
2022-02-22 18:36:11 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 0.6904
2022-02-22 18:36:54 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 0.6154
2022-02-22 18:37:37 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 0.6127
2022-02-22 18:38:19 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 0.6228
2022-02-22 18:39:02 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 0.5328
2022-02-22 18:39:45 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 0.7081
2022-02-22 18:40:28 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 0.7713
2022-02-22 18:41:10 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 0.7635
2022-02-22 18:41:53 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 0.5862
2022-02-22 18:42:36 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 0.6876
2022-02-22 18:43:19 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 0.5700
2022-02-22 18:44:02 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 0.5092
2022-02-22 18:44:45 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 0.6739
2022-02-22 18:45:28 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 0.5784
2022-02-22 18:46:11 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 0.6371
2022-02-22 18:46:53 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 0.7309
2022-02-22 18:47:36 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 0.7164
2022-02-22 18:48:18 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 0.7924
2022-02-22 18:49:01 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 0.7473
2022-02-22 18:49:44 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 0.7883
2022-02-22 18:50:27 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 0.7610
2022-02-22 18:51:10 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 0.5920
2022-02-22 18:51:53 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 0.5500
2022-02-22 18:52:37 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 0.6795
2022-02-22 18:53:20 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 0.6931
2022-02-22 18:54:05 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 0.6477
2022-02-22 18:54:52 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 0.6748
2022-02-22 18:55:40 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 0.7523
2022-02-22 18:55:44 - train: epoch 091, train_loss: 0.6713
2022-02-22 18:57:12 - eval: epoch: 091, acc1: 78.166%, acc5: 94.014%, test_loss: 0.8671, per_image_load_time: 1.054ms, per_image_inference_time: 0.887ms
2022-02-22 18:57:14 - until epoch: 091, best_acc1: 78.166%
2022-02-22 18:57:14 - epoch 092 lr: 0.00010000000000000003
2022-02-22 18:58:02 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 0.5982
2022-02-22 18:58:45 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 0.7635
2022-02-22 18:59:28 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 0.6072
2022-02-22 19:00:11 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 0.5087
2022-02-22 19:00:53 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 0.6968
2022-02-22 19:01:36 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 0.6999
2022-02-22 19:02:18 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 0.6924
2022-02-22 19:03:01 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 0.6530
2022-02-22 19:03:44 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 0.6376
2022-02-22 19:04:27 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 0.8179
2022-02-22 19:05:09 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 0.5035
2022-02-22 19:05:52 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 0.6015
2022-02-22 19:06:35 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 0.5755
2022-02-22 19:07:18 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 0.7177
2022-02-22 19:08:01 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 0.5790
2022-02-22 19:08:44 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 0.6263
2022-02-22 19:09:26 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 0.6645
2022-02-22 19:10:09 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 0.6682
2022-02-22 19:10:52 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 0.5747
2022-02-22 19:11:35 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 0.6407
2022-02-22 19:12:18 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 0.6701
2022-02-22 19:13:00 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 0.6936
2022-02-22 19:13:43 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 0.5817
2022-02-22 19:14:26 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 0.6311
2022-02-22 19:15:09 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 0.6031
2022-02-22 19:15:51 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 0.7451
2022-02-22 19:16:34 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 0.6692
2022-02-22 19:17:17 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 0.6381
2022-02-22 19:18:00 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 0.6557
2022-02-22 19:18:43 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 0.5362
2022-02-22 19:19:26 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 0.6483
2022-02-22 19:20:09 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 0.5290
2022-02-22 19:20:52 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 0.7044
2022-02-22 19:21:34 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 0.8284
2022-02-22 19:22:17 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 0.5386
2022-02-22 19:23:00 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 0.6037
2022-02-22 19:23:43 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 0.5739
2022-02-22 19:24:26 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 0.8343
2022-02-22 19:25:09 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 0.6988
2022-02-22 19:25:52 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 0.7282
2022-02-22 19:26:36 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 0.5682
2022-02-22 19:27:19 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 0.7140
2022-02-22 19:28:02 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 0.6510
2022-02-22 19:28:45 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 0.5410
2022-02-22 19:29:29 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 0.6146
2022-02-22 19:30:14 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 0.6220
2022-02-22 19:30:56 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 0.4321
2022-02-22 19:31:41 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 0.5773
2022-02-22 19:32:27 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 0.7038
2022-02-22 19:33:15 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 0.7302
2022-02-22 19:33:19 - train: epoch 092, train_loss: 0.6602
2022-02-22 19:34:41 - eval: epoch: 092, acc1: 78.298%, acc5: 94.026%, test_loss: 0.8660, per_image_load_time: 1.087ms, per_image_inference_time: 0.937ms
2022-02-22 19:34:43 - until epoch: 092, best_acc1: 78.298%
2022-02-22 19:34:43 - epoch 093 lr: 0.00010000000000000003
2022-02-22 19:35:31 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 0.5772
2022-02-22 19:36:14 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 0.6090
2022-02-22 19:36:57 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 0.5900
2022-02-22 19:37:40 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 0.5524
2022-02-22 19:38:23 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 0.6039
2022-02-22 19:39:06 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 0.6807
2022-02-22 19:39:49 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 0.6815
2022-02-22 19:40:32 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 0.5711
2022-02-22 19:41:15 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 0.5910
2022-02-22 19:41:59 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 0.5891
2022-02-22 19:42:41 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 0.5574
2022-02-22 19:43:24 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 0.5417
2022-02-22 19:44:07 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 0.6835
2022-02-22 19:44:50 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 0.6592
2022-02-22 19:45:33 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 0.8519
2022-02-22 19:46:15 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 0.6649
2022-02-22 19:46:59 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 0.5956
2022-02-22 19:47:42 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 0.6730
2022-02-22 19:48:25 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 0.6289
2022-02-22 19:49:07 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 0.5327
2022-02-22 19:49:50 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 0.5994
2022-02-22 19:50:33 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 0.7599
2022-02-22 19:51:16 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 0.7564
2022-02-22 19:51:59 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 0.6410
2022-02-22 19:52:42 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 0.7172
2022-02-22 19:53:25 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 0.7135
2022-02-22 19:54:08 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 0.5757
2022-02-22 19:54:50 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 0.6159
2022-02-22 19:55:33 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 0.7149
2022-02-22 19:56:16 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 0.6715
2022-02-22 19:56:59 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 0.8125
2022-02-22 19:57:42 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 0.6155
2022-02-22 19:58:26 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 0.7417
2022-02-22 19:59:09 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 0.8342
2022-02-22 19:59:52 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 0.5218
2022-02-22 20:00:35 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 0.7911
2022-02-22 20:01:18 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 0.5961
2022-02-22 20:02:01 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 0.5512
2022-02-22 20:02:44 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 0.7971
2022-02-22 20:03:28 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 0.7627
2022-02-22 20:04:11 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 0.6353
2022-02-22 20:04:54 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 0.7400
2022-02-22 20:05:37 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 0.8189
2022-02-22 20:06:22 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 0.6433
2022-02-22 20:07:06 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 0.6465
2022-02-22 20:07:49 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 0.4983
2022-02-22 20:08:35 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 0.7490
2022-02-22 20:09:19 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 0.6926
2022-02-22 20:10:04 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 0.7655
2022-02-22 20:10:51 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 0.6971
2022-02-22 20:10:55 - train: epoch 093, train_loss: 0.6558
2022-02-22 20:12:19 - eval: epoch: 093, acc1: 78.268%, acc5: 94.062%, test_loss: 0.8648, per_image_load_time: 1.208ms, per_image_inference_time: 0.882ms
2022-02-22 20:12:21 - until epoch: 093, best_acc1: 78.298%
2022-02-22 20:12:21 - epoch 094 lr: 0.00010000000000000003
2022-02-22 20:13:09 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 0.6287
2022-02-22 20:13:52 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 0.6407
2022-02-22 20:14:34 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 0.7147
2022-02-22 20:15:17 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 0.8665
2022-02-22 20:16:00 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 0.5860
2022-02-22 20:16:43 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 0.5735
2022-02-22 20:17:25 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 0.6877
2022-02-22 20:18:08 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 0.5915
2022-02-22 20:18:50 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 0.6039
2022-02-22 20:19:33 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 0.6592
2022-02-22 20:20:16 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 0.6576
2022-02-22 20:20:59 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 0.6969
2022-02-22 20:21:42 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 0.6635
2022-02-22 20:22:24 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 0.6558
2022-02-22 20:23:07 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 0.7883
2022-02-22 20:23:49 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.0768
2022-02-22 20:24:32 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 0.6361
2022-02-22 20:25:15 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 0.5938
2022-02-22 20:25:58 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 0.6980
2022-02-22 20:26:40 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 0.5154
2022-02-22 20:27:23 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 0.7345
2022-02-22 20:28:06 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 0.5837
2022-02-22 20:28:48 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 0.4481
2022-02-22 20:29:31 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 0.6885
2022-02-22 20:30:14 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 0.5510
2022-02-22 20:30:57 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 0.4787
2022-02-22 20:31:40 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 0.5923
2022-02-22 20:32:23 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 0.6259
2022-02-22 20:33:06 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 0.7347
2022-02-22 20:33:49 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 0.5557
2022-02-22 20:34:31 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 0.8121
2022-02-22 20:35:14 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 0.7176
2022-02-22 20:35:57 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 0.6474
2022-02-22 20:36:40 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 0.7154
2022-02-22 20:37:23 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 0.8162
2022-02-22 20:38:05 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 0.6287
2022-02-22 20:38:48 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 0.6852
2022-02-22 20:39:31 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 0.6146
2022-02-22 20:40:14 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 0.6130
2022-02-22 20:40:58 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 0.7829
2022-02-22 20:41:40 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 0.8507
2022-02-22 20:42:24 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 0.5481
2022-02-22 20:43:07 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 0.8067
2022-02-22 20:43:50 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 0.7638
2022-02-22 20:44:33 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 0.6892
2022-02-22 20:45:18 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 0.6465
2022-02-22 20:46:02 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 0.6806
2022-02-22 20:46:48 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 0.6101
2022-02-22 20:48:17 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 0.6026
2022-02-22 20:49:25 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 0.5640
2022-02-22 20:49:28 - train: epoch 094, train_loss: 0.6533
2022-02-22 20:50:57 - eval: epoch: 094, acc1: 78.348%, acc5: 94.024%, test_loss: 0.8651, per_image_load_time: 1.075ms, per_image_inference_time: 0.891ms
2022-02-22 20:51:00 - until epoch: 094, best_acc1: 78.348%
2022-02-22 20:51:00 - epoch 095 lr: 0.00010000000000000003
2022-02-22 20:51:46 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 0.5358
2022-02-22 20:52:29 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 0.7418
2022-02-22 20:53:12 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 0.5565
2022-02-22 20:53:54 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 0.6947
2022-02-22 20:54:37 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 0.7278
2022-02-22 20:55:19 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 0.6503
2022-02-22 20:56:02 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 0.7077
2022-02-22 20:56:44 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 0.8218
2022-02-22 20:57:27 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 0.8976
2022-02-22 20:58:09 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 0.6528
2022-02-22 20:58:51 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 0.5517
2022-02-22 20:59:34 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 0.5005
2022-02-22 21:00:16 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 0.6292
2022-02-22 21:00:59 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 0.6540
2022-02-22 21:01:41 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 0.5867
2022-02-22 21:02:24 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 0.4040
2022-02-22 21:03:06 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 0.6346
2022-02-22 21:03:49 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 0.6369
2022-02-22 21:04:32 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 0.5771
2022-02-22 21:05:14 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 0.6902
2022-02-22 21:05:57 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 0.6243
2022-02-22 21:06:40 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 0.5135
2022-02-22 21:07:22 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 0.6226
2022-02-22 21:08:05 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 0.7187
2022-02-22 21:08:47 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 0.6308
2022-02-22 21:09:30 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 0.5802
2022-02-22 21:10:13 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 0.6964
2022-02-22 21:10:56 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 0.6687
2022-02-22 21:11:39 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 0.5611
2022-02-22 21:12:21 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 0.7446
2022-02-22 21:13:04 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 0.7653
2022-02-22 21:13:47 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 0.6032
2022-02-22 21:14:29 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 0.7089
2022-02-22 21:15:12 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 0.6964
2022-02-22 21:15:54 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 0.6556
2022-02-22 21:16:37 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 0.5721
2022-02-22 21:17:19 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 0.6688
2022-02-22 21:18:02 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 0.4559
2022-02-22 21:18:45 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 0.6455
2022-02-22 21:19:27 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 0.5141
2022-02-22 21:20:10 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 0.6421
2022-02-22 21:20:52 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 0.5986
2022-02-22 21:21:35 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 0.6207
2022-02-22 21:22:18 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 0.6663
2022-02-22 21:23:01 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 0.5602
2022-02-22 21:23:45 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 0.6563
2022-02-22 21:24:30 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 0.6315
2022-02-22 21:25:14 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 0.6829
2022-02-22 21:26:00 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 0.5978
2022-02-22 21:26:52 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 0.5200
2022-02-22 21:26:56 - train: epoch 095, train_loss: 0.6522
2022-02-22 21:28:22 - eval: epoch: 095, acc1: 78.372%, acc5: 94.050%, test_loss: 0.8661, per_image_load_time: 2.063ms, per_image_inference_time: 1.045ms
2022-02-22 21:28:25 - until epoch: 095, best_acc1: 78.372%
2022-02-22 21:28:25 - epoch 096 lr: 0.00010000000000000003
2022-02-22 21:29:12 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 0.7006
2022-02-22 21:29:54 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 0.7427
2022-02-22 21:30:37 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 0.6888
2022-02-22 21:31:20 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 0.5301
2022-02-22 21:32:02 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 0.5903
2022-02-22 21:32:45 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 0.6818
2022-02-22 21:33:27 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 0.5518
2022-02-22 21:34:09 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 0.6020
2022-02-22 21:34:52 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 0.5971
2022-02-22 21:35:34 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 0.5779
2022-02-22 21:36:17 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 0.6290
2022-02-22 21:36:59 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 0.6851
2022-02-22 21:37:42 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 0.7015
2022-02-22 21:38:25 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 0.5931
2022-02-22 21:39:07 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 0.6423
2022-02-22 21:39:50 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 0.4426
2022-02-22 21:40:32 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 0.7406
2022-02-22 21:41:15 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 0.7472
2022-02-22 21:41:58 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 0.7566
2022-02-22 21:42:40 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 0.6124
2022-02-22 21:43:23 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 0.7018
2022-02-22 21:44:05 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 0.4913
2022-02-22 21:44:48 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 0.5614
2022-02-22 21:45:30 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 0.5559
2022-02-22 21:46:13 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 0.5675
2022-02-22 21:46:55 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 0.6206
2022-02-22 21:47:38 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 0.6323
2022-02-22 21:48:21 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 0.7916
2022-02-22 21:49:03 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 0.6449
2022-02-22 21:49:46 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 0.6118
2022-02-22 21:50:28 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 0.6427
2022-02-22 21:51:08 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 0.6627
2022-02-22 21:51:48 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 0.7890
2022-02-22 21:52:28 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 0.4608
2022-02-22 21:53:09 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 0.5577
2022-02-22 21:53:50 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 0.5232
2022-02-22 21:54:31 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 0.6613
2022-02-22 21:55:11 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 0.6808
2022-02-22 21:55:52 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 0.5758
2022-02-22 21:56:32 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 0.4910
2022-02-22 21:57:13 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 0.6241
2022-02-22 21:57:53 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 0.7396
2022-02-22 21:58:34 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 0.4808
2022-02-22 21:59:14 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 0.6854
2022-02-22 21:59:55 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 0.5726
2022-02-22 22:00:35 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 0.7052
2022-02-22 22:01:16 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 0.5681
2022-02-22 22:01:57 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 0.6892
2022-02-22 22:02:38 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 0.7851
2022-02-22 22:03:18 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 0.6109
2022-02-22 22:03:22 - train: epoch 096, train_loss: 0.6485
2022-02-22 22:04:39 - eval: epoch: 096, acc1: 78.240%, acc5: 94.058%, test_loss: 0.8661, per_image_load_time: 1.239ms, per_image_inference_time: 0.921ms
2022-02-22 22:04:40 - until epoch: 096, best_acc1: 78.372%
2022-02-22 22:04:40 - epoch 097 lr: 0.00010000000000000003
2022-02-22 22:05:26 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 0.7108
2022-02-22 22:06:06 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 0.5402
2022-02-22 22:06:47 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 0.7151
2022-02-22 22:07:27 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 0.8676
2022-02-22 22:08:08 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 0.6770
2022-02-22 22:08:49 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 0.7141
2022-02-22 22:09:29 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 0.6123
2022-02-22 22:10:10 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 0.5856
2022-02-22 22:10:51 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 0.5076
2022-02-22 22:11:32 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 0.5754
2022-02-22 22:12:13 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 0.5751
2022-02-22 22:12:53 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 0.5989
2022-02-22 22:13:34 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 0.4974
2022-02-22 22:14:15 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 0.6588
2022-02-22 22:14:55 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 0.6696
2022-02-22 22:15:36 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 0.6784
2022-02-22 22:16:17 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 0.6419
2022-02-22 22:16:58 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 0.6513
2022-02-22 22:17:39 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 0.6263
2022-02-22 22:18:20 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 0.6059
2022-02-22 22:19:01 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 0.7321
2022-02-22 22:19:43 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 0.7961
2022-02-22 22:20:24 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 0.5708
2022-02-22 22:21:05 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 0.6654
2022-02-22 22:21:46 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 0.7221
2022-02-22 22:22:26 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 0.7025
2022-02-22 22:23:07 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 0.5675
2022-02-22 22:23:48 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 0.5815
2022-02-22 22:24:28 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 0.7609
2022-02-22 22:25:09 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 0.6902
2022-02-22 22:25:50 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 0.6015
2022-02-22 22:26:30 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 0.6743
2022-02-22 22:27:12 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 0.6577
2022-02-22 22:27:53 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 0.8188
2022-02-22 22:28:34 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 0.7205
2022-02-22 22:29:14 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 0.5961
2022-02-22 22:29:55 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 0.5892
2022-02-22 22:30:36 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 0.5553
2022-02-22 22:31:17 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 0.6663
2022-02-22 22:31:58 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 0.6077
2022-02-22 22:32:39 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 0.5318
2022-02-22 22:33:20 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 0.6902
2022-02-22 22:34:02 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 0.5539
2022-02-22 22:34:43 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 0.6439
2022-02-22 22:35:25 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 0.6228
2022-02-22 22:36:07 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 0.8528
2022-02-22 22:36:50 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 0.7218
2022-02-22 22:37:35 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 0.6357
2022-02-22 22:38:18 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 0.7783
2022-02-22 22:39:13 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 0.7167
2022-02-22 22:39:18 - train: epoch 097, train_loss: 0.6469
2022-02-22 22:40:40 - eval: epoch: 097, acc1: 78.318%, acc5: 94.096%, test_loss: 0.8651, per_image_load_time: 0.472ms, per_image_inference_time: 0.887ms
2022-02-22 22:40:42 - until epoch: 097, best_acc1: 78.372%
2022-02-22 22:40:42 - epoch 098 lr: 0.00010000000000000003
2022-02-22 22:41:27 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 0.6743
2022-02-22 22:42:08 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 0.6950
2022-02-22 22:42:49 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 0.8350
2022-02-22 22:43:30 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 0.5379
2022-02-22 22:44:11 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 0.6504
2022-02-22 22:44:53 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 0.6868
2022-02-22 22:45:34 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 0.6015
2022-02-22 22:46:15 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 0.6600
2022-02-22 22:46:57 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 0.6816
2022-02-22 22:47:38 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 0.5562
2022-02-22 22:48:19 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 0.4947
2022-02-22 22:49:00 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 0.5563
2022-02-22 22:49:41 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 0.7110
2022-02-22 22:50:23 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 0.6482
2022-02-22 22:51:04 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 0.5614
2022-02-22 22:51:45 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 0.6965
2022-02-22 22:52:26 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 0.6693
2022-02-22 22:53:07 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 0.5409
2022-02-22 22:53:48 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 0.5200
2022-02-22 22:54:29 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 0.7651
2022-02-22 22:55:10 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 0.6431
2022-02-22 22:55:51 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 0.5780
2022-02-22 22:56:32 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 0.6697
2022-02-22 22:57:13 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 0.5748
2022-02-22 22:57:54 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 0.7497
2022-02-22 22:58:36 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 0.6187
2022-02-22 22:59:17 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 0.6701
2022-02-22 22:59:58 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 0.5794
2022-02-22 23:00:39 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 0.5859
2022-02-22 23:01:21 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 0.6285
2022-02-22 23:02:02 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 0.5901
2022-02-22 23:02:43 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 0.5971
2022-02-22 23:03:24 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 0.5066
2022-02-22 23:04:05 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 0.6511
2022-02-22 23:04:47 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 0.6205
2022-02-22 23:05:28 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 0.8182
2022-02-22 23:06:09 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 0.6609
2022-02-22 23:06:50 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 0.7096
2022-02-22 23:07:32 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 0.5912
2022-02-22 23:08:14 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 0.7058
2022-02-22 23:08:55 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 0.7163
2022-02-22 23:09:37 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 0.6531
2022-02-22 23:10:18 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 0.5087
2022-02-22 23:11:00 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 0.7206
2022-02-22 23:11:42 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 0.7463
2022-02-22 23:12:25 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 0.7482
2022-02-22 23:13:08 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 0.6042
2022-02-22 23:13:52 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 0.4611
2022-02-22 23:14:42 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 0.7094
2022-02-22 23:15:35 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 0.7403
2022-02-22 23:15:39 - train: epoch 098, train_loss: 0.6446
2022-02-22 23:17:06 - eval: epoch: 098, acc1: 78.262%, acc5: 94.058%, test_loss: 0.8659, per_image_load_time: 1.413ms, per_image_inference_time: 0.858ms
2022-02-22 23:17:08 - until epoch: 098, best_acc1: 78.372%
2022-02-22 23:17:08 - epoch 099 lr: 0.00010000000000000003
2022-02-22 23:17:55 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 0.5638
2022-02-22 23:18:36 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 0.5975
2022-02-22 23:19:18 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 0.6078
2022-02-22 23:20:00 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 0.6036
2022-02-22 23:20:41 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 0.7313
2022-02-22 23:21:22 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 0.5984
2022-02-22 23:22:04 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 0.6093
2022-02-22 23:22:45 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 0.6037
2022-02-22 23:23:26 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 0.6607
2022-02-22 23:24:08 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 0.7162
2022-02-22 23:24:49 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 0.6248
2022-02-22 23:25:31 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 0.6256
2022-02-22 23:26:12 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 0.6925
2022-02-22 23:26:53 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 0.6986
2022-02-22 23:27:35 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 0.5439
2022-02-22 23:28:16 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 0.8997
2022-02-22 23:28:58 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 0.5975
2022-02-22 23:29:39 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 0.6325
2022-02-22 23:30:21 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 0.6519
2022-02-22 23:31:02 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 0.6056
2022-02-22 23:31:44 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 0.5859
2022-02-22 23:32:26 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 0.6510
2022-02-22 23:33:07 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 0.6871
2022-02-22 23:33:49 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 0.7105
2022-02-22 23:34:30 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 0.6017
2022-02-22 23:35:11 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 0.5123
2022-02-22 23:35:53 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 0.6305
2022-02-22 23:36:34 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 0.7659
2022-02-22 23:37:16 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 0.6245
2022-02-22 23:37:57 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 0.6774
2022-02-22 23:38:39 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 0.6261
2022-02-22 23:39:20 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 0.7148
2022-02-22 23:40:02 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 0.5496
2022-02-22 23:40:43 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 0.7190
2022-02-22 23:41:25 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 0.6994
2022-02-22 23:42:07 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 0.5858
2022-02-22 23:42:48 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 0.5629
2022-02-22 23:43:30 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 0.8500
2022-02-22 23:44:11 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 0.6650
2022-02-22 23:44:53 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 0.7594
2022-02-22 23:45:35 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 0.5231
2022-02-22 23:46:16 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 0.6230
2022-02-22 23:46:58 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 0.6164
2022-02-22 23:47:40 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 0.6703
2022-02-22 23:48:22 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 0.6848
2022-02-22 23:49:04 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 0.7731
2022-02-22 23:49:47 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 0.5323
2022-02-22 23:50:35 - train: epoch 0099, iter [04800, 05004], lr: 0.000100, loss: 0.8347
2022-02-22 23:51:25 - train: epoch 0099, iter [04900, 05004], lr: 0.000100, loss: 0.6970
2022-02-22 23:52:19 - train: epoch 0099, iter [05000, 05004], lr: 0.000100, loss: 0.6118
2022-02-22 23:52:23 - train: epoch 099, train_loss: 0.6444
2022-02-22 23:53:54 - eval: epoch: 099, acc1: 78.302%, acc5: 94.038%, test_loss: 0.8662, per_image_load_time: 2.509ms, per_image_inference_time: 0.902ms
2022-02-22 23:53:56 - until epoch: 099, best_acc1: 78.372%
2022-02-22 23:53:56 - epoch 100 lr: 0.00010000000000000003
2022-02-22 23:54:41 - train: epoch 0100, iter [00100, 05004], lr: 0.000100, loss: 0.6279
2022-02-22 23:55:22 - train: epoch 0100, iter [00200, 05004], lr: 0.000100, loss: 0.7348
2022-02-22 23:56:04 - train: epoch 0100, iter [00300, 05004], lr: 0.000100, loss: 0.5211
2022-02-22 23:56:45 - train: epoch 0100, iter [00400, 05004], lr: 0.000100, loss: 0.4674
2022-02-22 23:57:25 - train: epoch 0100, iter [00500, 05004], lr: 0.000100, loss: 0.6939
2022-02-22 23:58:07 - train: epoch 0100, iter [00600, 05004], lr: 0.000100, loss: 0.8673
2022-02-22 23:58:47 - train: epoch 0100, iter [00700, 05004], lr: 0.000100, loss: 0.6197
2022-02-22 23:59:28 - train: epoch 0100, iter [00800, 05004], lr: 0.000100, loss: 0.6720
2022-02-23 00:00:09 - train: epoch 0100, iter [00900, 05004], lr: 0.000100, loss: 0.4838
2022-02-23 00:00:50 - train: epoch 0100, iter [01000, 05004], lr: 0.000100, loss: 0.6492
2022-02-23 00:01:31 - train: epoch 0100, iter [01100, 05004], lr: 0.000100, loss: 0.5764
2022-02-23 00:02:12 - train: epoch 0100, iter [01200, 05004], lr: 0.000100, loss: 0.6253
2022-02-23 00:02:53 - train: epoch 0100, iter [01300, 05004], lr: 0.000100, loss: 0.6411
2022-02-23 00:03:34 - train: epoch 0100, iter [01400, 05004], lr: 0.000100, loss: 0.6285
2022-02-23 00:04:15 - train: epoch 0100, iter [01500, 05004], lr: 0.000100, loss: 0.6508
2022-02-23 00:04:56 - train: epoch 0100, iter [01600, 05004], lr: 0.000100, loss: 0.5884
2022-02-23 00:05:37 - train: epoch 0100, iter [01700, 05004], lr: 0.000100, loss: 0.6944
2022-02-23 00:06:19 - train: epoch 0100, iter [01800, 05004], lr: 0.000100, loss: 0.7196
2022-02-23 00:07:00 - train: epoch 0100, iter [01900, 05004], lr: 0.000100, loss: 0.5521
2022-02-23 00:07:42 - train: epoch 0100, iter [02000, 05004], lr: 0.000100, loss: 0.6505
2022-02-23 00:08:23 - train: epoch 0100, iter [02100, 05004], lr: 0.000100, loss: 0.5353
2022-02-23 00:09:04 - train: epoch 0100, iter [02200, 05004], lr: 0.000100, loss: 0.6856
2022-02-23 00:09:46 - train: epoch 0100, iter [02300, 05004], lr: 0.000100, loss: 0.5962
2022-02-23 00:10:27 - train: epoch 0100, iter [02400, 05004], lr: 0.000100, loss: 0.6281
2022-02-23 00:11:08 - train: epoch 0100, iter [02500, 05004], lr: 0.000100, loss: 0.5902
2022-02-23 00:11:49 - train: epoch 0100, iter [02600, 05004], lr: 0.000100, loss: 0.7585
2022-02-23 00:12:31 - train: epoch 0100, iter [02700, 05004], lr: 0.000100, loss: 0.4915
2022-02-23 00:13:12 - train: epoch 0100, iter [02800, 05004], lr: 0.000100, loss: 0.5907
2022-02-23 00:13:53 - train: epoch 0100, iter [02900, 05004], lr: 0.000100, loss: 0.7294
2022-02-23 00:14:34 - train: epoch 0100, iter [03000, 05004], lr: 0.000100, loss: 0.6013
2022-02-23 00:15:16 - train: epoch 0100, iter [03100, 05004], lr: 0.000100, loss: 0.6149
2022-02-23 00:15:58 - train: epoch 0100, iter [03200, 05004], lr: 0.000100, loss: 0.6807
2022-02-23 00:16:39 - train: epoch 0100, iter [03300, 05004], lr: 0.000100, loss: 0.7277
2022-02-23 00:17:20 - train: epoch 0100, iter [03400, 05004], lr: 0.000100, loss: 0.7550
2022-02-23 00:18:02 - train: epoch 0100, iter [03500, 05004], lr: 0.000100, loss: 0.5628
2022-02-23 00:18:43 - train: epoch 0100, iter [03600, 05004], lr: 0.000100, loss: 0.6886
2022-02-23 00:19:24 - train: epoch 0100, iter [03700, 05004], lr: 0.000100, loss: 0.6509
2022-02-23 00:20:05 - train: epoch 0100, iter [03800, 05004], lr: 0.000100, loss: 0.6600
2022-02-23 00:20:47 - train: epoch 0100, iter [03900, 05004], lr: 0.000100, loss: 0.6971
2022-02-23 00:21:29 - train: epoch 0100, iter [04000, 05004], lr: 0.000100, loss: 0.5250
2022-02-23 00:22:10 - train: epoch 0100, iter [04100, 05004], lr: 0.000100, loss: 0.7407
2022-02-23 00:22:51 - train: epoch 0100, iter [04200, 05004], lr: 0.000100, loss: 0.6151
2022-02-23 00:23:33 - train: epoch 0100, iter [04300, 05004], lr: 0.000100, loss: 0.6232
2022-02-23 00:24:15 - train: epoch 0100, iter [04400, 05004], lr: 0.000100, loss: 0.8382
2022-02-23 00:24:57 - train: epoch 0100, iter [04500, 05004], lr: 0.000100, loss: 0.6075
2022-02-23 00:25:39 - train: epoch 0100, iter [04600, 05004], lr: 0.000100, loss: 0.5123
2022-02-23 00:26:22 - train: epoch 0100, iter [04700, 05004], lr: 0.000100, loss: 0.7141
2022-02-23 00:27:07 - train: epoch 0100, iter [04800, 05004], lr: 0.000100, loss: 0.5342
2022-02-23 00:27:54 - train: epoch 0100, iter [04900, 05004], lr: 0.000100, loss: 0.5954
2022-02-23 00:28:44 - train: epoch 0100, iter [05000, 05004], lr: 0.000100, loss: 0.7064
2022-02-23 00:28:47 - train: epoch 100, train_loss: 0.6423
2022-02-23 00:30:11 - eval: epoch: 100, acc1: 78.286%, acc5: 94.062%, test_loss: 0.8665, per_image_load_time: 1.629ms, per_image_inference_time: 1.008ms
2022-02-23 00:30:12 - until epoch: 100, best_acc1: 78.372%
2022-02-23 06:06:41 - train done. model: resnet152, train time: 61.321 hours, best_acc1: 78.372%
