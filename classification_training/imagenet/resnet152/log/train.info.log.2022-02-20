2022-02-20 11:07:58 - network: resnet152
2022-02-20 11:07:58 - num_classes: 1000
2022-02-20 11:07:58 - input_image_size: 224
2022-02-20 11:07:58 - scale: 1.1428571428571428
2022-02-20 11:07:58 - trained_model_path: 
2022-02-20 11:07:58 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-20 11:07:58 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7faeede220d0>
2022-02-20 11:07:58 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7faeede223a0>
2022-02-20 11:07:58 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7faeede223d0>
2022-02-20 11:07:58 - seed: 0
2022-02-20 11:07:58 - batch_size: 256
2022-02-20 11:07:58 - num_workers: 16
2022-02-20 11:07:58 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-20 11:07:58 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-20 11:07:58 - epochs: 100
2022-02-20 11:07:58 - print_interval: 100
2022-02-20 11:07:58 - distributed: True
2022-02-20 11:07:58 - sync_bn: False
2022-02-20 11:07:58 - apex: True
2022-02-20 11:07:58 - gpus_type: NVIDIA RTX A5000
2022-02-20 11:07:58 - gpus_num: 2
2022-02-20 11:07:58 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7faee4fe3e30>
2022-02-20 11:08:03 - --------------------parameters--------------------
2022-02-20 11:08:03 - name: conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.4.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.4.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.4.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.4.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.4.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.4.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.4.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.4.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.4.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.5.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.5.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.5.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.5.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.5.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.5.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.5.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.5.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.5.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.6.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.6.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.6.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.6.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.6.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.6.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.6.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.6.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.6.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.7.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.7.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.7.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.7.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.7.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.7.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer2.7.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer2.7.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer2.7.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.6.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.6.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.6.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.6.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.6.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.6.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.6.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.6.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.6.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.7.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.7.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.7.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.7.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.7.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.7.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.7.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.7.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.7.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.8.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.8.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.8.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.8.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.8.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.8.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.8.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.8.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.8.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.9.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.9.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.9.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.9.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.9.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.9.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.9.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.9.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.9.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.10.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.10.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.10.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.10.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.10.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.10.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.10.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.10.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.10.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.11.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.11.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.11.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.11.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.11.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.11.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.11.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.11.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.11.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.12.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.12.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.12.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.12.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.12.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.12.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.12.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.12.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.12.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.13.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.13.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.13.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.13.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.13.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.13.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.13.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.13.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.13.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.14.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.14.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.14.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.14.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.14.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.14.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.14.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.14.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.14.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.15.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.15.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.15.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.15.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.15.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.15.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.15.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.15.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.15.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.16.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.16.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.16.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.16.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.16.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.16.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.16.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.16.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.16.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.17.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.17.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.17.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.17.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.17.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.17.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.17.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.17.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.17.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.18.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.18.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.18.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.18.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.18.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.18.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.18.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.18.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.18.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.19.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.19.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.19.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.19.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.19.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.19.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.19.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.19.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.19.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.20.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.20.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.20.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.20.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.20.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.20.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.20.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.20.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.20.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.21.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.21.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.21.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.21.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.21.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.21.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.21.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.21.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.21.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.22.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.22.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.22.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.22.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.22.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.22.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.22.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.22.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.22.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.23.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.23.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.23.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.23.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.23.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.23.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.23.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.23.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.23.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.24.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.24.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.24.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.24.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.24.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.24.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.24.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.24.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.24.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.25.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.25.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.25.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.25.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.25.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.25.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.25.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.25.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.25.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.26.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.26.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.26.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.26.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.26.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.26.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.26.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.26.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.26.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.27.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.27.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.27.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.27.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.27.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.27.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.27.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.27.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.27.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.28.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.28.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.28.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.28.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.28.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.28.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.28.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.28.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.28.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.29.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.29.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.29.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.29.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.29.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.29.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.29.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.29.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.29.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.30.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.30.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.30.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.30.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.30.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.30.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.30.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.30.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.30.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.31.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.31.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.31.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.31.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.31.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.31.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.31.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.31.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.31.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.32.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.32.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.32.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.32.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.32.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.32.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.32.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.32.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.32.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.33.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.33.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.33.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.33.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.33.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.33.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.33.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.33.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.33.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.34.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.34.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.34.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.34.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.34.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.34.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.34.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.34.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.34.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.35.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.35.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.35.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.35.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.35.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.35.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer3.35.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer3.35.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer3.35.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-02-20 11:08:03 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-02-20 11:08:03 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-02-20 11:08:03 - name: fc.weight, grad: True
2022-02-20 11:08:03 - name: fc.bias, grad: True
2022-02-20 11:08:03 - --------------------buffers--------------------
2022-02-20 11:08:03 - name: conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.4.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.4.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.4.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.4.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.4.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.4.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.4.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.4.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.4.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.5.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.5.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.5.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.5.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.5.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.5.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.5.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.5.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.5.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.6.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.6.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.6.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.6.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.6.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.6.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.6.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.6.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.6.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.7.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.7.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.7.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.7.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.7.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.7.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer2.7.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer2.7.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer2.7.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.6.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.6.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.6.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.6.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.6.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.6.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.6.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.6.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.6.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.7.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.7.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.7.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.7.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.7.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.7.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.7.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.7.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.7.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.8.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.8.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.8.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.8.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.8.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.8.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.8.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.8.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.8.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.9.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.9.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.9.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.9.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.9.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.9.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.9.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.9.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.9.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.10.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.10.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.10.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.10.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.10.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.10.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.10.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.10.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.10.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.11.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.11.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.11.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.11.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.11.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.11.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.11.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.11.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.11.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.12.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.12.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.12.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.12.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.12.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.12.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.12.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.12.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.12.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.13.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.13.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.13.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.13.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.13.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.13.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.13.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.13.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.13.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.14.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.14.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.14.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.14.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.14.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.14.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.14.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.14.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.14.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.15.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.15.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.15.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.15.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.15.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.15.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.15.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.15.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.15.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.16.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.16.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.16.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.16.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.16.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.16.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.16.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.16.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.16.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.17.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.17.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.17.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.17.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.17.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.17.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.17.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.17.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.17.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.18.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.18.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.18.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.18.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.18.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.18.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.18.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.18.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.18.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.19.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.19.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.19.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.19.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.19.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.19.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.19.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.19.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.19.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.20.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.20.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.20.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.20.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.20.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.20.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.20.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.20.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.20.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.21.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.21.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.21.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.21.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.21.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.21.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.21.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.21.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.21.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.22.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.22.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.22.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.22.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.22.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.22.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.22.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.22.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.22.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.23.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.23.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.23.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.23.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.23.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.23.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.23.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.23.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.23.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.24.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.24.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.24.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.24.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.24.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.24.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.24.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.24.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.24.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.25.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.25.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.25.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.25.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.25.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.25.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.25.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.25.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.25.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.26.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.26.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.26.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.26.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.26.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.26.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.26.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.26.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.26.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.27.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.27.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.27.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.27.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.27.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.27.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.27.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.27.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.27.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.28.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.28.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.28.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.28.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.28.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.28.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.28.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.28.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.28.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.29.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.29.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.29.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.29.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.29.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.29.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.29.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.29.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.29.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.30.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.30.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.30.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.30.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.30.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.30.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.30.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.30.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.30.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.31.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.31.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.31.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.31.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.31.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.31.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.31.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.31.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.31.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.32.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.32.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.32.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.32.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.32.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.32.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.32.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.32.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.32.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.33.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.33.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.33.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.33.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.33.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.33.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.33.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.33.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.33.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.34.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.34.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.34.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.34.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.34.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.34.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.34.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.34.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.34.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.35.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.35.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.35.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.35.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.35.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.35.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer3.35.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer3.35.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer3.35.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-02-20 11:08:03 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-02-20 11:08:03 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-20 11:08:03 - epoch 001 lr: 0.1
2022-02-20 11:08:52 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9145
2022-02-20 11:09:35 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.8981
2022-02-20 11:10:17 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.8979
2022-02-20 11:10:59 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.9077
2022-02-20 11:11:41 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.8890
2022-02-20 11:12:24 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.8400
2022-02-20 11:13:06 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.8592
2022-02-20 11:13:48 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.7717
2022-02-20 11:14:30 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.6606
2022-02-20 11:15:12 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.6298
2022-02-20 11:15:54 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.6039
2022-02-20 11:16:36 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 6.4928
2022-02-20 11:17:18 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 6.4973
2022-02-20 11:18:00 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 6.4975
2022-02-20 11:18:42 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 6.2719
2022-02-20 11:19:24 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 6.4207
2022-02-20 11:20:07 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 6.2332
2022-02-20 11:20:49 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 6.1514
2022-02-20 11:21:31 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 6.1287
2022-02-20 11:22:13 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 6.0337
2022-02-20 11:22:56 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.9893
2022-02-20 11:23:38 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.8593
2022-02-20 11:24:20 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 5.8290
2022-02-20 11:25:02 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 5.7360
2022-02-20 11:25:44 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.6157
2022-02-20 11:26:26 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.7242
2022-02-20 11:27:08 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.6922
2022-02-20 11:27:51 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 5.5440
2022-02-20 11:28:33 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 5.4390
2022-02-20 11:29:15 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.4880
2022-02-20 11:29:57 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.5367
2022-02-20 11:30:40 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 5.4096
2022-02-20 11:31:22 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 5.2788
2022-02-20 11:32:05 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 5.2898
2022-02-20 11:32:47 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 5.1290
2022-02-20 11:33:29 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 5.2355
2022-02-20 11:34:11 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 5.3335
2022-02-20 11:34:54 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 5.0894
2022-02-20 11:35:36 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 5.1723
2022-02-20 11:36:18 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 5.0831
2022-02-20 11:37:01 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 5.1544
2022-02-20 11:37:43 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.9025
2022-02-20 11:38:25 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.9356
2022-02-20 11:39:07 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.6929
2022-02-20 11:39:50 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.9466
2022-02-20 11:40:32 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 5.0053
2022-02-20 11:41:15 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.5983
2022-02-20 11:41:57 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.9872
2022-02-20 11:42:40 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.7364
2022-02-20 11:43:22 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.6346
2022-02-20 11:43:24 - train: epoch 001, train_loss: 5.8146
2022-02-20 11:44:36 - eval: epoch: 001, acc1: 12.356%, acc5: 30.296%, test_loss: 4.6719, per_image_load_time: 1.295ms, per_image_inference_time: 0.817ms
2022-02-20 11:44:37 - until epoch: 001, best_acc1: 12.356%
2022-02-20 11:44:37 - epoch 002 lr: 0.1
2022-02-20 11:45:26 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.7056
2022-02-20 11:46:08 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.3797
2022-02-20 11:46:50 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.7898
2022-02-20 11:47:33 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.6097
2022-02-20 11:48:15 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.3942
2022-02-20 11:48:57 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.4523
2022-02-20 11:49:40 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.6521
2022-02-20 11:50:22 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 4.1729
2022-02-20 11:51:05 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 4.0430
2022-02-20 11:51:47 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.5187
2022-02-20 11:52:30 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.5677
2022-02-20 11:53:12 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.3387
2022-02-20 11:53:55 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.2738
2022-02-20 11:54:37 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.3883
2022-02-20 11:55:20 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.2754
2022-02-20 11:56:02 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 4.1603
2022-02-20 11:56:45 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.1969
2022-02-20 11:57:27 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.2859
2022-02-20 11:58:10 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 4.0769
2022-02-20 11:58:52 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.8770
2022-02-20 11:59:35 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 4.1263
2022-02-20 12:00:17 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.9687
2022-02-20 12:00:59 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 4.0819
2022-02-20 12:01:42 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.9175
2022-02-20 12:02:25 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.8689
2022-02-20 12:03:07 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.8676
2022-02-20 12:03:50 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.9983
2022-02-20 12:04:32 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.9614
2022-02-20 12:05:15 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.7769
2022-02-20 12:05:57 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.8634
2022-02-20 12:06:39 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.8442
2022-02-20 12:07:22 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.8023
2022-02-20 12:08:04 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.8114
2022-02-20 12:08:47 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.9641
2022-02-20 12:09:29 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.8044
2022-02-20 12:10:12 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.7702
2022-02-20 12:10:54 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.9079
2022-02-20 12:11:36 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.5827
2022-02-20 12:12:19 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.6955
2022-02-20 12:13:01 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.6562
2022-02-20 12:13:44 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.7374
2022-02-20 12:14:27 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.7095
2022-02-20 12:15:09 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.5701
2022-02-20 12:15:52 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.5259
2022-02-20 12:16:34 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.4595
2022-02-20 12:17:17 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.5003
2022-02-20 12:17:59 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.6423
2022-02-20 12:18:41 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.6106
2022-02-20 12:19:24 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.5402
2022-02-20 12:20:06 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.4760
2022-02-20 12:20:08 - train: epoch 002, train_loss: 4.0239
2022-02-20 12:21:23 - eval: epoch: 002, acc1: 27.768%, acc5: 53.162%, test_loss: 3.4598, per_image_load_time: 1.065ms, per_image_inference_time: 0.804ms
2022-02-20 12:21:25 - until epoch: 002, best_acc1: 27.768%
2022-02-20 12:21:25 - epoch 003 lr: 0.1
2022-02-20 12:22:12 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.5011
2022-02-20 12:22:55 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.5493
2022-02-20 12:23:37 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.3915
2022-02-20 12:24:20 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.4806
2022-02-20 12:25:03 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.5150
2022-02-20 12:25:45 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.4425
2022-02-20 12:26:28 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.6306
2022-02-20 12:27:10 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.4922
2022-02-20 12:27:52 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.3115
2022-02-20 12:28:35 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.4041
2022-02-20 12:29:17 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.2649
2022-02-20 12:30:00 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.3876
2022-02-20 12:30:42 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.3270
2022-02-20 12:31:25 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.2764
2022-02-20 12:32:08 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.7089
2022-02-20 12:32:50 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.2871
2022-02-20 12:33:32 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.2491
2022-02-20 12:34:15 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.1248
2022-02-20 12:34:58 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.3714
2022-02-20 12:35:41 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.5046
2022-02-20 12:36:23 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.4608
2022-02-20 12:37:06 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.7020
2022-02-20 12:37:48 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.2658
2022-02-20 12:38:31 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.2470
2022-02-20 12:39:13 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.2639
2022-02-20 12:39:56 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.2373
2022-02-20 12:40:38 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.5428
2022-02-20 12:41:21 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.1916
2022-02-20 12:42:03 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.2882
2022-02-20 12:42:46 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.2558
2022-02-20 12:43:29 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.4067
2022-02-20 12:44:11 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.2417
2022-02-20 12:44:54 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.1148
2022-02-20 12:45:36 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.3795
2022-02-20 12:46:19 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.0656
2022-02-20 12:47:02 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.1277
2022-02-20 12:47:44 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.2143
2022-02-20 12:48:27 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.2135
2022-02-20 12:49:10 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.2607
2022-02-20 12:49:52 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.2040
2022-02-20 12:50:35 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.1686
2022-02-20 12:51:18 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.9555
2022-02-20 12:52:00 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.7947
2022-02-20 12:52:43 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.9888
2022-02-20 12:53:26 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.0982
2022-02-20 12:54:08 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.0607
2022-02-20 12:54:52 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.9113
2022-02-20 12:55:34 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.2132
2022-02-20 12:56:17 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.1007
2022-02-20 12:57:00 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.1208
2022-02-20 12:57:03 - train: epoch 003, train_loss: 3.2616
2022-02-20 12:58:17 - eval: epoch: 003, acc1: 33.454%, acc5: 58.482%, test_loss: 11.3516, per_image_load_time: 2.079ms, per_image_inference_time: 0.803ms
2022-02-20 12:58:19 - until epoch: 003, best_acc1: 33.454%
2022-02-20 12:58:19 - epoch 004 lr: 0.1
2022-02-20 12:59:07 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.0641
2022-02-20 12:59:49 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.9171
2022-02-20 13:00:31 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.9576
2022-02-20 13:01:14 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.8944
2022-02-20 13:01:56 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.8011
2022-02-20 13:02:38 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.2049
2022-02-20 13:03:21 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.0507
2022-02-20 13:04:03 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.9132
2022-02-20 13:04:45 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.7981
2022-02-20 13:05:27 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.9858
2022-02-20 13:06:10 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.0351
2022-02-20 13:06:52 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.7277
2022-02-20 13:07:35 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.7284
2022-02-20 13:08:17 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 2.9326
2022-02-20 13:09:00 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 2.9829
2022-02-20 13:09:42 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.8101
2022-02-20 13:10:24 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.0024
2022-02-20 13:11:07 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.0446
2022-02-20 13:11:49 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.1118
2022-02-20 13:12:31 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.8383
2022-02-20 13:13:14 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.0667
2022-02-20 13:13:56 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.9092
2022-02-20 13:14:39 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.6804
2022-02-20 13:15:21 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.7724
2022-02-20 13:16:15 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.8683
2022-02-20 13:16:57 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.8948
2022-02-20 13:17:40 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.6339
2022-02-20 13:18:22 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.9001
2022-02-20 13:19:05 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.6871
2022-02-20 13:19:48 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.7903
2022-02-20 13:20:42 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.8540
2022-02-20 13:21:25 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.7290
2022-02-20 13:22:19 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.9907
2022-02-20 13:23:12 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.8669
2022-02-20 13:23:55 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.7452
2022-02-20 13:24:38 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.6392
2022-02-20 13:25:20 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.8038
2022-02-20 13:26:03 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.7240
2022-02-20 13:26:45 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.7474
2022-02-20 13:27:28 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.5326
2022-02-20 13:28:11 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.7720
2022-02-20 13:28:54 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.6308
2022-02-20 13:29:37 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.5406
2022-02-20 13:30:20 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.6740
2022-02-20 13:31:03 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.2401
2022-02-20 13:31:46 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.7188
2022-02-20 13:32:28 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.6245
2022-02-20 13:33:11 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.7336
2022-02-20 13:34:18 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.7425
2022-02-20 13:35:01 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.9019
2022-02-20 13:35:03 - train: epoch 004, train_loss: 2.8745
2022-02-20 13:36:36 - eval: epoch: 004, acc1: 42.836%, acc5: 69.408%, test_loss: 2.5570, per_image_load_time: 0.828ms, per_image_inference_time: 0.833ms
2022-02-20 13:36:38 - until epoch: 004, best_acc1: 42.836%
2022-02-20 13:36:38 - epoch 005 lr: 0.1
2022-02-20 13:37:26 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.7339
2022-02-20 13:38:09 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.8644
2022-02-20 13:38:57 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.9109
2022-02-20 13:39:48 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.6697
2022-02-20 13:40:40 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.5485
2022-02-20 13:41:23 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.6159
2022-02-20 13:42:18 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.7113
2022-02-20 13:43:08 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 2.9996
2022-02-20 13:44:01 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.6120
2022-02-20 13:45:00 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.6702
2022-02-20 13:45:45 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.6713
2022-02-20 13:46:37 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.6990
2022-02-20 13:47:20 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.7368
2022-02-20 13:48:03 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.7786
2022-02-20 13:48:46 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.4528
2022-02-20 13:49:28 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.5274
2022-02-20 13:50:11 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.6160
2022-02-20 13:50:54 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.7028
2022-02-20 13:51:37 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.4985
2022-02-20 13:52:20 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.6606
2022-02-20 13:53:03 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.4737
2022-02-20 13:53:46 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.5669
2022-02-20 13:54:29 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.5149
2022-02-20 13:55:11 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.5574
2022-02-20 13:55:54 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.7395
2022-02-20 13:56:37 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.8501
2022-02-20 13:57:20 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.7688
2022-02-20 13:58:03 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.5842
2022-02-20 13:58:46 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.5194
2022-02-20 13:59:29 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.4928
2022-02-20 14:00:17 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.6205
2022-02-20 14:01:07 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.6778
2022-02-20 14:02:03 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.4745
2022-02-20 14:02:47 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.4900
2022-02-20 14:03:31 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.6640
2022-02-20 14:04:25 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.7244
2022-02-20 14:05:08 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.5316
2022-02-20 14:05:51 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.3575
2022-02-20 14:06:35 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.8500
2022-02-20 14:07:18 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.5361
2022-02-20 14:08:11 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.7057
2022-02-20 14:08:56 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.6486
2022-02-20 14:09:39 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.5244
2022-02-20 14:10:23 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.5679
2022-02-20 14:11:06 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.6254
2022-02-20 14:11:50 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.5727
2022-02-20 14:12:45 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.4257
2022-02-20 14:13:28 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.4250
2022-02-20 14:14:12 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.6622
2022-02-20 14:15:06 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.3598
2022-02-20 14:15:08 - train: epoch 005, train_loss: 2.6422
2022-02-20 14:16:40 - eval: epoch: 005, acc1: 40.590%, acc5: 64.782%, test_loss: 20.5639, per_image_load_time: 0.936ms, per_image_inference_time: 0.829ms
2022-02-20 14:16:42 - until epoch: 005, best_acc1: 42.836%
2022-02-20 14:16:42 - epoch 006 lr: 0.1
2022-02-20 14:17:31 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.5717
2022-02-20 14:18:14 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.5479
2022-02-20 14:18:57 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.3858
2022-02-20 14:19:41 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.6757
2022-02-20 14:20:24 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.4962
2022-02-20 14:21:07 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.5792
2022-02-20 14:21:51 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.5902
2022-02-20 14:22:34 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.5412
2022-02-20 14:23:17 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.4510
2022-02-20 14:24:00 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.3675
2022-02-20 14:24:44 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.4543
2022-02-20 14:25:27 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.6645
2022-02-20 14:26:10 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.6414
2022-02-20 14:26:54 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.6273
2022-02-20 14:27:37 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.7457
2022-02-20 14:28:20 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.4041
2022-02-20 14:29:09 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.6889
2022-02-20 14:29:57 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.5937
2022-02-20 14:30:41 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.4931
2022-02-20 14:31:24 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.5906
2022-02-20 14:32:07 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.5949
2022-02-20 14:32:50 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.4460
2022-02-20 14:33:33 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.3430
2022-02-20 14:34:15 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.5590
2022-02-20 14:34:57 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.7544
2022-02-20 14:35:39 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.3486
2022-02-20 14:36:22 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.6330
2022-02-20 14:37:04 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.3920
2022-02-20 14:37:47 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.5778
2022-02-20 14:38:29 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.5210
2022-02-20 14:39:11 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.2001
2022-02-20 14:39:54 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.4174
2022-02-20 14:40:36 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.2762
2022-02-20 14:41:18 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.5414
2022-02-20 14:42:00 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.6058
2022-02-20 14:42:43 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.4315
2022-02-20 14:43:25 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.5670
2022-02-20 14:44:07 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.3352
2022-02-20 14:44:49 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.3670
2022-02-20 14:45:32 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.6803
2022-02-20 14:46:14 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.4405
2022-02-20 14:46:57 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.3343
2022-02-20 14:47:39 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.5771
2022-02-20 14:48:21 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.4377
2022-02-20 14:49:04 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.4964
2022-02-20 14:49:46 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.4234
2022-02-20 14:50:28 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.5160
2022-02-20 14:51:10 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.5117
2022-02-20 14:51:53 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.4671
2022-02-20 14:52:35 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.3807
2022-02-20 14:52:37 - train: epoch 006, train_loss: 2.5054
2022-02-20 14:53:52 - eval: epoch: 006, acc1: 48.550%, acc5: 74.472%, test_loss: 2.2515, per_image_load_time: 1.880ms, per_image_inference_time: 0.838ms
2022-02-20 14:53:54 - until epoch: 006, best_acc1: 48.550%
2022-02-20 14:53:54 - epoch 007 lr: 0.1
2022-02-20 14:54:42 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.3321
2022-02-20 14:55:24 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.5513
2022-02-20 14:56:06 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.7057
2022-02-20 14:56:48 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.4769
2022-02-20 14:57:31 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.3794
2022-02-20 14:58:13 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.6396
2022-02-20 14:58:55 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.3766
2022-02-20 14:59:38 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.4809
2022-02-20 15:00:20 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.3015
2022-02-20 15:01:02 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.4242
2022-02-20 15:01:44 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.2705
2022-02-20 15:02:26 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.3558
2022-02-20 15:03:08 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.3980
2022-02-20 15:03:51 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.3921
2022-02-20 15:04:33 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.5610
2022-02-20 15:05:15 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.4732
2022-02-20 15:05:57 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.4818
2022-02-20 15:06:39 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.3222
2022-02-20 15:07:21 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.4637
2022-02-20 15:08:04 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.3577
2022-02-20 15:08:46 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.5362
2022-02-20 15:09:28 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.2526
2022-02-20 15:10:10 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.5044
2022-02-20 15:10:53 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.4919
2022-02-20 15:11:35 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.3373
2022-02-20 15:12:17 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.4117
2022-02-20 15:12:59 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.2328
2022-02-20 15:13:42 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.2923
2022-02-20 15:14:24 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.2713
2022-02-20 15:15:06 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.4124
2022-02-20 15:15:48 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.2934
2022-02-20 15:16:30 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.2851
2022-02-20 15:17:13 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.6840
2022-02-20 15:17:55 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.2561
2022-02-20 15:18:37 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.4110
2022-02-20 15:19:19 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.2091
2022-02-20 15:20:02 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.3455
2022-02-20 15:20:44 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.5546
2022-02-20 15:21:26 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.2619
2022-02-20 15:22:09 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.4576
2022-02-20 15:22:51 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.3935
2022-02-20 15:23:33 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.2368
2022-02-20 15:24:16 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.5814
2022-02-20 15:24:58 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.1386
2022-02-20 15:25:40 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.5856
2022-02-20 15:26:23 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.3623
2022-02-20 15:27:05 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.4849
2022-02-20 15:27:47 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.5903
2022-02-20 15:28:30 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.3486
2022-02-20 15:29:12 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.2818
2022-02-20 15:29:14 - train: epoch 007, train_loss: 2.4034
2022-02-20 15:30:29 - eval: epoch: 007, acc1: 48.914%, acc5: 75.090%, test_loss: 2.2140, per_image_load_time: 1.795ms, per_image_inference_time: 0.848ms
2022-02-20 15:30:31 - until epoch: 007, best_acc1: 48.914%
2022-02-20 15:30:31 - epoch 008 lr: 0.1
2022-02-20 15:31:18 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.4079
2022-02-20 15:32:00 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.4760
2022-02-20 15:32:42 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.1407
2022-02-20 15:33:24 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.1998
2022-02-20 15:34:06 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.2005
2022-02-20 15:34:48 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.3131
2022-02-20 15:35:30 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.6141
2022-02-20 15:36:12 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.1828
2022-02-20 15:36:54 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.3366
2022-02-20 15:37:36 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.3620
2022-02-20 15:38:18 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.1838
2022-02-20 15:39:00 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.1309
2022-02-20 15:39:42 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.4095
2022-02-20 15:40:24 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.2962
2022-02-20 15:41:06 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.3503
2022-02-20 15:41:48 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.3723
2022-02-20 15:42:30 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.2297
2022-02-20 15:43:12 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.5410
2022-02-20 15:43:53 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.0667
2022-02-20 15:44:35 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.3366
2022-02-20 15:45:17 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.2929
2022-02-20 15:45:59 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.2147
2022-02-20 15:46:41 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.3638
2022-02-20 15:47:22 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.4058
2022-02-20 15:48:04 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.3165
2022-02-20 15:48:46 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.4222
2022-02-20 15:49:28 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.4149
2022-02-20 15:50:10 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.3943
2022-02-20 15:50:52 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.2686
2022-02-20 15:51:34 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.4628
2022-02-20 15:52:16 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.3079
2022-02-20 15:52:58 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.6069
2022-02-20 15:53:40 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.5383
2022-02-20 15:54:21 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.3921
2022-02-20 15:55:03 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.3171
2022-02-20 15:55:45 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.4805
2022-02-20 15:56:27 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.2153
2022-02-20 15:57:09 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.2073
2022-02-20 15:57:50 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.4153
2022-02-20 15:58:32 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.5756
2022-02-20 15:59:14 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.3218
2022-02-20 15:59:56 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.2511
2022-02-20 16:00:38 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.0216
2022-02-20 16:01:20 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.2378
2022-02-20 16:02:02 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.5454
2022-02-20 16:02:44 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.3722
2022-02-20 16:03:26 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.2338
2022-02-20 16:04:08 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.3161
2022-02-20 16:04:50 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.3929
2022-02-20 16:05:32 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.2419
2022-02-20 16:05:34 - train: epoch 008, train_loss: 2.3326
2022-02-20 16:06:50 - eval: epoch: 008, acc1: 48.624%, acc5: 74.718%, test_loss: 2.2226, per_image_load_time: 2.102ms, per_image_inference_time: 0.809ms
2022-02-20 16:06:51 - until epoch: 008, best_acc1: 48.914%
2022-02-20 16:06:51 - epoch 009 lr: 0.1
2022-02-20 16:07:39 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.0302
2022-02-20 16:08:21 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.1727
2022-02-20 16:09:02 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.0373
2022-02-20 16:09:44 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.5519
2022-02-20 16:10:26 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.3050
2022-02-20 16:11:08 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.3231
2022-02-20 16:11:50 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.2331
2022-02-20 16:12:32 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.1685
2022-02-20 16:13:14 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.1357
2022-02-20 16:13:56 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.2138
2022-02-20 16:14:38 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.5647
2022-02-20 16:15:20 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.3631
2022-02-20 16:16:02 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.4809
2022-02-20 16:16:44 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.0295
2022-02-20 16:17:26 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.0800
2022-02-20 16:18:07 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.3083
2022-02-20 16:18:49 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.4771
2022-02-20 16:19:31 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.1615
2022-02-20 16:20:13 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.0872
2022-02-20 16:20:55 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 1.9465
2022-02-20 16:21:37 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.2697
2022-02-20 16:22:19 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.4344
2022-02-20 16:23:01 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 1.9557
2022-02-20 16:23:43 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.2237
2022-02-20 16:24:25 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.1250
2022-02-20 16:25:07 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.2980
2022-02-20 16:25:49 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.2880
2022-02-20 16:26:31 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.2841
2022-02-20 16:27:13 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.0890
2022-02-20 16:27:55 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.2083
2022-02-20 16:28:37 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.2922
2022-02-20 16:29:19 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.2893
2022-02-20 16:30:01 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.2186
2022-02-20 16:30:43 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.4990
2022-02-20 16:31:25 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.4261
2022-02-20 16:32:07 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.2248
2022-02-20 16:32:50 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.4924
2022-02-20 16:33:32 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.4390
2022-02-20 16:34:14 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.0162
2022-02-20 16:34:56 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.4415
2022-02-20 16:35:38 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.2325
2022-02-20 16:36:21 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.2013
2022-02-20 16:37:03 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.3966
2022-02-20 16:37:45 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.2987
2022-02-20 16:38:27 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.2028
2022-02-20 16:39:09 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.4913
2022-02-20 16:39:51 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.5207
2022-02-20 16:40:34 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.4620
2022-02-20 16:41:16 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.2466
2022-02-20 16:41:58 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.2407
2022-02-20 16:42:00 - train: epoch 009, train_loss: 2.2798
2022-02-20 16:43:15 - eval: epoch: 009, acc1: 50.312%, acc5: 76.434%, test_loss: 2.1327, per_image_load_time: 2.037ms, per_image_inference_time: 0.818ms
2022-02-20 16:43:17 - until epoch: 009, best_acc1: 50.312%
2022-02-20 16:43:17 - epoch 010 lr: 0.1
2022-02-20 16:44:04 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.2582
2022-02-20 16:44:46 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.2398
2022-02-20 16:45:28 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.3094
2022-02-20 16:46:10 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.2944
2022-02-20 16:46:53 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.0692
2022-02-20 16:47:35 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.2530
2022-02-20 16:48:17 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.2602
2022-02-20 16:48:59 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.0648
2022-02-20 16:49:41 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.1801
2022-02-20 16:50:23 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.0994
2022-02-20 16:51:05 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.1460
2022-02-20 16:51:47 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.0528
2022-02-20 16:52:30 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.1282
2022-02-20 16:53:12 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.3135
2022-02-20 16:53:54 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 1.9455
2022-02-20 16:54:36 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.3521
2022-02-20 16:55:18 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.2919
2022-02-20 16:56:00 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.2326
2022-02-20 16:56:43 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.2987
2022-02-20 16:57:25 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.2670
2022-02-20 16:58:07 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.1317
2022-02-20 16:58:49 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.4307
2022-02-20 16:59:31 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.4071
2022-02-20 17:00:13 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.3436
2022-02-20 17:00:55 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.1571
2022-02-20 17:01:37 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.3062
2022-02-20 17:02:19 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.0689
2022-02-20 17:03:01 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.2263
2022-02-20 17:03:43 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.3339
2022-02-20 17:04:25 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.2840
2022-02-20 17:05:07 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.4716
2022-02-20 17:05:49 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.1402
2022-02-20 17:06:31 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.3400
2022-02-20 17:07:14 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.4505
2022-02-20 17:07:56 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.5442
2022-02-20 17:08:38 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.4203
2022-02-20 17:09:20 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.0812
2022-02-20 17:10:02 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.3112
2022-02-20 17:10:44 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 1.8665
2022-02-20 17:11:26 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.1868
2022-02-20 17:12:08 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.0730
2022-02-20 17:12:50 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.3658
2022-02-20 17:13:33 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.2659
2022-02-20 17:14:15 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.1961
2022-02-20 17:14:57 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.0005
2022-02-20 17:15:40 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.2603
2022-02-20 17:16:22 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.2690
2022-02-20 17:17:04 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.2061
2022-02-20 17:17:46 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.1440
2022-02-20 17:18:28 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.0289
2022-02-20 17:18:31 - train: epoch 010, train_loss: 2.2359
2022-02-20 17:19:44 - eval: epoch: 010, acc1: 48.816%, acc5: 75.468%, test_loss: 2.2104, per_image_load_time: 1.617ms, per_image_inference_time: 0.837ms
2022-02-20 17:19:45 - until epoch: 010, best_acc1: 50.312%
2022-02-20 17:19:45 - epoch 011 lr: 0.1
2022-02-20 17:20:33 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 1.9084
2022-02-20 17:21:15 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.3585
2022-02-20 17:21:57 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.0133
2022-02-20 17:22:39 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.3308
2022-02-20 17:23:21 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.1060
2022-02-20 17:24:03 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.1283
2022-02-20 17:24:45 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.1968
2022-02-20 17:25:27 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.2195
2022-02-20 17:26:09 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.3461
2022-02-20 17:26:51 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.0790
2022-02-20 17:27:33 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.3023
2022-02-20 17:28:15 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.5544
2022-02-20 17:28:57 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.4335
2022-02-20 17:29:39 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.2150
2022-02-20 17:30:21 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.0913
2022-02-20 17:31:03 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.3123
2022-02-20 17:31:44 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.2594
2022-02-20 17:32:27 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.0864
2022-02-20 17:33:08 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.0245
2022-02-20 17:33:51 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.4228
2022-02-20 17:34:33 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.2373
2022-02-20 17:35:15 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.1038
2022-02-20 17:35:57 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.5071
2022-02-20 17:36:39 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 1.9893
2022-02-20 17:37:21 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.3323
2022-02-20 17:38:03 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.3205
2022-02-20 17:38:45 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.2506
2022-02-20 17:39:27 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 1.9392
2022-02-20 17:40:09 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.2791
2022-02-20 17:40:51 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.4427
2022-02-20 17:41:33 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.3115
2022-02-20 17:42:15 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.0104
2022-02-20 17:42:57 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.2734
2022-02-20 17:43:39 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.2237
2022-02-20 17:44:21 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.2160
2022-02-20 17:45:03 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.1033
2022-02-20 17:45:45 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.2547
2022-02-20 17:46:27 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 1.9517
2022-02-20 17:47:10 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.2207
2022-02-20 17:47:52 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.0718
2022-02-20 17:48:34 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 1.9327
2022-02-20 17:49:16 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.1787
2022-02-20 17:49:58 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.2792
2022-02-20 17:50:40 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.1472
2022-02-20 17:51:22 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 1.9543
2022-02-20 17:52:04 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.0397
2022-02-20 17:52:46 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 1.9299
2022-02-20 17:53:29 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.0087
2022-02-20 17:54:10 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.0844
2022-02-20 17:54:53 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.1105
2022-02-20 17:54:55 - train: epoch 011, train_loss: 2.1994
2022-02-20 17:56:10 - eval: epoch: 011, acc1: 53.314%, acc5: 78.656%, test_loss: 1.9869, per_image_load_time: 2.074ms, per_image_inference_time: 0.825ms
2022-02-20 17:56:13 - until epoch: 011, best_acc1: 53.314%
2022-02-20 17:56:13 - epoch 012 lr: 0.1
2022-02-20 17:56:59 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.1343
2022-02-20 17:57:41 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.0836
2022-02-20 17:58:23 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.0983
2022-02-20 17:59:05 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.2999
2022-02-20 17:59:47 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.3371
2022-02-20 18:00:29 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 1.9322
2022-02-20 18:01:11 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.0173
2022-02-20 18:01:54 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.2072
2022-02-20 18:02:36 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.2726
2022-02-20 18:03:18 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.0220
2022-02-20 18:04:00 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.5502
2022-02-20 18:04:42 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.0259
2022-02-20 18:05:24 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.1249
2022-02-20 18:06:06 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.3199
2022-02-20 18:06:48 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 1.9781
2022-02-20 18:07:31 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.1521
2022-02-20 18:08:13 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.0646
2022-02-20 18:08:56 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.1900
2022-02-20 18:09:39 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.2349
2022-02-20 18:10:21 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.3246
2022-02-20 18:11:04 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.2156
2022-02-20 18:11:46 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.3387
2022-02-20 18:12:29 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.2564
2022-02-20 18:13:11 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.1565
2022-02-20 18:13:54 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 1.9816
2022-02-20 18:14:36 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 1.9599
2022-02-20 18:15:18 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.1009
2022-02-20 18:16:01 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.1225
2022-02-20 18:16:43 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.0290
2022-02-20 18:17:26 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.0645
2022-02-20 18:18:08 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.3567
2022-02-20 18:18:51 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 1.9983
2022-02-20 18:19:34 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.2071
2022-02-20 18:20:16 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.1010
2022-02-20 18:20:59 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.2670
2022-02-20 18:21:41 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.1859
2022-02-20 18:22:23 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.0405
2022-02-20 18:23:06 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.1510
2022-02-20 18:23:48 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.1171
2022-02-20 18:24:30 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.0108
2022-02-20 18:25:13 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.0522
2022-02-20 18:25:56 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.0700
2022-02-20 18:26:38 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.2453
2022-02-20 18:27:21 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 1.9816
2022-02-20 18:28:03 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.1065
2022-02-20 18:28:46 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.4925
2022-02-20 18:29:29 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.1012
2022-02-20 18:30:12 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.2273
2022-02-20 18:30:54 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.2441
2022-02-20 18:31:37 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 1.8725
2022-02-20 18:31:39 - train: epoch 012, train_loss: 2.1732
2022-02-20 18:32:55 - eval: epoch: 012, acc1: 54.330%, acc5: 79.486%, test_loss: 1.9402, per_image_load_time: 2.089ms, per_image_inference_time: 0.819ms
2022-02-20 18:32:57 - until epoch: 012, best_acc1: 54.330%
2022-02-20 18:32:57 - epoch 013 lr: 0.1
2022-02-20 18:33:44 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 1.9448
2022-02-20 18:34:26 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.1315
2022-02-20 18:35:09 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.0653
2022-02-20 18:35:51 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.0379
2022-02-20 18:36:33 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.1772
2022-02-20 18:37:16 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.2871
2022-02-20 18:37:58 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.1265
2022-02-20 18:38:41 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.2782
2022-02-20 18:39:23 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.0068
2022-02-20 18:40:06 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.1623
2022-02-20 18:40:48 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.2075
2022-02-20 18:41:30 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.3150
2022-02-20 18:42:13 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.2891
2022-02-20 18:42:55 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.0384
2022-02-20 18:43:38 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.2525
2022-02-20 18:44:20 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 1.9067
2022-02-20 18:45:03 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.1156
2022-02-20 18:45:45 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.2196
2022-02-20 18:46:28 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.1357
2022-02-20 18:47:10 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.3360
2022-02-20 18:47:53 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.3930
2022-02-20 18:48:35 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.1071
2022-02-20 18:49:17 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.1456
2022-02-20 18:50:00 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.2208
2022-02-20 18:50:42 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 1.9972
2022-02-20 18:51:24 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.0974
2022-02-20 18:52:07 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.1176
2022-02-20 18:52:49 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.1774
2022-02-20 18:53:32 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.2734
2022-02-20 18:54:14 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.0121
2022-02-20 18:54:56 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 1.9988
2022-02-20 18:55:39 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.1547
2022-02-20 18:56:21 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 1.9752
2022-02-20 18:57:04 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.0886
2022-02-20 18:57:47 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.0375
2022-02-20 18:58:30 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.3896
2022-02-20 18:59:13 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 1.8654
2022-02-20 18:59:56 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.2777
2022-02-20 19:00:39 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.2354
2022-02-20 19:01:22 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.2168
2022-02-20 19:02:05 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.0704
2022-02-20 19:02:48 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.0557
2022-02-20 19:03:31 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.0103
2022-02-20 19:04:14 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 1.9782
2022-02-20 19:04:57 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.1445
2022-02-20 19:05:40 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.1407
2022-02-20 19:06:23 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.2383
2022-02-20 19:07:05 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.2497
2022-02-20 19:07:48 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.2820
2022-02-20 19:08:31 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.2476
2022-02-20 19:08:33 - train: epoch 013, train_loss: 2.1497
2022-02-20 19:09:47 - eval: epoch: 013, acc1: 53.794%, acc5: 79.304%, test_loss: 1.9603, per_image_load_time: 1.731ms, per_image_inference_time: 0.823ms
2022-02-20 19:09:49 - until epoch: 013, best_acc1: 54.330%
2022-02-20 19:09:49 - epoch 014 lr: 0.1
2022-02-20 19:10:37 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.1023
2022-02-20 19:11:20 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.2540
2022-02-20 19:12:02 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 1.9132
2022-02-20 19:12:45 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 1.9585
2022-02-20 19:13:28 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.0441
2022-02-20 19:14:11 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.1138
2022-02-20 19:14:53 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.1256
2022-02-20 19:15:36 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.1308
2022-02-20 19:16:19 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.1471
2022-02-20 19:17:02 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.2952
2022-02-20 19:17:44 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 1.9908
2022-02-20 19:18:27 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.2276
2022-02-20 19:19:10 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.1647
2022-02-20 19:19:52 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.2293
2022-02-20 19:20:35 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.2249
2022-02-20 19:21:18 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.0293
2022-02-20 19:22:01 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.4005
2022-02-20 19:22:44 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.3191
2022-02-20 19:23:27 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.0820
2022-02-20 19:24:10 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.0142
2022-02-20 19:24:53 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.2889
2022-02-20 19:25:35 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.1745
2022-02-20 19:26:18 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.1663
2022-02-20 19:26:59 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.3044
2022-02-20 19:27:41 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.1467
2022-02-20 19:28:23 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.1314
2022-02-20 19:29:05 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.0435
2022-02-20 19:29:47 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.3874
2022-02-20 19:30:29 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.1303
2022-02-20 19:31:11 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.1604
2022-02-20 19:31:53 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 1.9504
2022-02-20 19:32:35 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.0126
2022-02-20 19:33:17 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.0288
2022-02-20 19:33:59 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.0695
2022-02-20 19:34:41 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.0560
2022-02-20 19:35:24 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 1.9656
2022-02-20 19:36:06 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 1.9813
2022-02-20 19:36:48 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.2542
2022-02-20 19:37:30 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 1.9714
2022-02-20 19:38:12 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.2023
2022-02-20 19:38:55 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.0039
2022-02-20 19:39:37 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 1.9400
2022-02-20 19:40:19 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.0202
2022-02-20 19:41:01 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 1.8960
2022-02-20 19:41:43 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.0444
2022-02-20 19:42:26 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.0610
2022-02-20 19:43:08 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.0546
2022-02-20 19:43:50 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.0317
2022-02-20 19:44:33 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 1.8735
2022-02-20 19:45:15 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.0978
2022-02-20 19:45:18 - train: epoch 014, train_loss: 2.1297
2022-02-20 19:46:32 - eval: epoch: 014, acc1: 48.434%, acc5: 74.606%, test_loss: 2.2534, per_image_load_time: 2.073ms, per_image_inference_time: 0.813ms
2022-02-20 19:46:34 - until epoch: 014, best_acc1: 54.330%
2022-02-20 19:46:34 - epoch 015 lr: 0.1
2022-02-20 19:47:38 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 1.8650
2022-02-20 19:48:46 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.3151
2022-02-20 19:49:49 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.2160
2022-02-20 19:50:55 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.0918
2022-02-20 19:51:57 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.0170
2022-02-20 19:52:47 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.2720
2022-02-20 19:53:29 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.1043
2022-02-20 19:54:11 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 1.9236
2022-02-20 19:55:01 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.0332
2022-02-20 19:56:12 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.1054
2022-02-20 19:57:16 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 1.9534
2022-02-20 19:57:59 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.0376
2022-02-20 19:58:42 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.4998
2022-02-20 19:59:26 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 1.9986
2022-02-20 20:00:08 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 1.8681
2022-02-20 20:00:51 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.1825
2022-02-20 20:01:33 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.2422
2022-02-20 20:02:15 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 1.9570
2022-02-20 20:02:57 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 1.9440
2022-02-20 20:03:40 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.0436
2022-02-20 20:04:23 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.0518
2022-02-20 20:05:06 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.4233
2022-02-20 20:05:49 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 1.9513
2022-02-20 20:06:31 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.1979
2022-02-20 20:07:13 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.1499
2022-02-20 20:07:56 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 1.9044
2022-02-20 20:08:38 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.1834
2022-02-20 20:09:21 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.2810
2022-02-20 20:10:03 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.0158
2022-02-20 20:10:45 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 1.8926
2022-02-20 20:11:27 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.0146
2022-02-20 20:12:09 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.0300
2022-02-20 20:12:52 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 1.8612
2022-02-20 20:13:34 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.2001
2022-02-20 20:14:16 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.2251
2022-02-20 20:14:59 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.0942
2022-02-20 20:15:41 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.0765
2022-02-20 20:16:23 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.0579
2022-02-20 20:17:06 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.2834
2022-02-20 20:17:48 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.2549
2022-02-20 20:18:31 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.3041
2022-02-20 20:19:13 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 1.8843
2022-02-20 20:19:55 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.1708
2022-02-20 20:20:38 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.1346
2022-02-20 20:21:20 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.0693
2022-02-20 20:22:02 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.0108
2022-02-20 20:22:44 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.0999
2022-02-20 20:23:27 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.0260
2022-02-20 20:24:09 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.0025
2022-02-20 20:24:51 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.2346
2022-02-20 20:24:54 - train: epoch 015, train_loss: 2.1147
2022-02-20 20:26:07 - eval: epoch: 015, acc1: 49.178%, acc5: 74.586%, test_loss: 2.2412, per_image_load_time: 1.962ms, per_image_inference_time: 0.837ms
2022-02-20 20:26:09 - until epoch: 015, best_acc1: 54.330%
2022-02-20 20:26:09 - epoch 016 lr: 0.1
2022-02-20 20:26:57 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.0659
2022-02-20 20:27:39 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 1.9276
2022-02-20 20:28:20 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.1870
2022-02-20 20:29:03 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.3134
2022-02-20 20:29:44 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 1.9123
2022-02-20 20:30:26 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.1793
2022-02-20 20:31:08 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 1.9119
2022-02-20 20:31:50 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.1425
2022-02-20 20:32:32 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.2232
2022-02-20 20:33:14 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 1.9896
2022-02-20 20:33:56 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.0115
2022-02-20 20:34:38 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 1.9888
2022-02-20 20:35:20 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.1151
2022-02-20 20:36:02 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.0244
2022-02-20 20:36:44 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.1098
2022-02-20 20:37:26 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.2286
2022-02-20 20:38:08 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.0889
2022-02-20 20:38:50 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.0435
2022-02-20 20:39:32 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.1570
2022-02-20 20:40:14 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 1.8165
2022-02-20 20:40:56 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.1430
2022-02-20 20:41:38 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.1865
2022-02-20 20:42:20 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.3558
2022-02-20 20:43:02 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.2375
2022-02-20 20:43:44 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.0171
2022-02-20 20:44:26 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.2459
2022-02-20 20:45:08 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.1536
2022-02-20 20:45:51 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 1.9163
2022-02-20 20:46:33 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.1493
2022-02-20 20:47:15 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.3391
2022-02-20 20:47:57 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.2813
2022-02-20 20:48:39 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.2607
2022-02-20 20:49:21 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.2246
2022-02-20 20:50:03 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 1.9777
2022-02-20 20:50:45 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.0230
2022-02-20 20:51:27 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 1.9914
2022-02-20 20:52:09 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.1882
2022-02-20 20:52:52 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.4076
2022-02-20 20:53:34 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.1707
2022-02-20 20:54:16 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.2507
2022-02-20 20:54:58 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.0453
2022-02-20 20:55:40 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.1209
2022-02-20 20:56:22 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 1.9244
2022-02-20 20:57:05 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 1.9570
2022-02-20 20:57:47 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.1221
2022-02-20 20:58:29 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.0149
2022-02-20 20:59:12 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.4222
2022-02-20 20:59:54 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.0326
2022-02-20 21:00:36 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.0857
2022-02-20 21:01:18 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.1699
2022-02-20 21:01:21 - train: epoch 016, train_loss: 2.0992
2022-02-20 21:02:35 - eval: epoch: 016, acc1: 53.940%, acc5: 79.308%, test_loss: 1.9536, per_image_load_time: 1.204ms, per_image_inference_time: 0.806ms
2022-02-20 21:02:37 - until epoch: 016, best_acc1: 54.330%
2022-02-20 21:02:37 - epoch 017 lr: 0.1
2022-02-20 21:03:25 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 1.9511
2022-02-20 21:04:07 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.0945
2022-02-20 21:04:49 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.3429
2022-02-20 21:05:31 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 1.8303
2022-02-20 21:06:13 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.1539
2022-02-20 21:06:55 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.3277
2022-02-20 21:07:37 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.0634
2022-02-20 21:08:30 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 1.9919
2022-02-20 21:09:12 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.0365
2022-02-20 21:10:06 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 1.9376
2022-02-20 21:10:48 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.4179
2022-02-20 21:11:40 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.2194
2022-02-20 21:12:22 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.0775
2022-02-20 21:13:04 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.2138
2022-02-20 21:13:46 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 1.6808
2022-02-20 21:14:28 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 1.8703
2022-02-20 21:15:10 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.0631
2022-02-20 21:15:51 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.0685
2022-02-20 21:16:34 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 1.9431
2022-02-20 21:17:16 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.2718
2022-02-20 21:17:58 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.0878
2022-02-20 21:18:40 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.0622
2022-02-20 21:19:45 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 1.9916
2022-02-20 21:20:50 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.0250
2022-02-20 21:21:58 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.2485
2022-02-20 21:23:06 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.0650
2022-02-20 21:23:58 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 1.9948
2022-02-20 21:25:06 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.2892
2022-02-20 21:26:20 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.3291
2022-02-20 21:27:02 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 1.8823
2022-02-20 21:27:44 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.3446
2022-02-20 21:28:27 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 1.9103
2022-02-20 21:29:09 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.2627
2022-02-20 21:29:51 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 1.9435
2022-02-20 21:30:33 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.1277
2022-02-20 21:31:16 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.1873
2022-02-20 21:31:57 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.0598
2022-02-20 21:32:39 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.2179
2022-02-20 21:33:21 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 1.9204
2022-02-20 21:34:03 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 1.9604
2022-02-20 21:34:45 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.1424
2022-02-20 21:35:27 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.0665
2022-02-20 21:36:09 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.0783
2022-02-20 21:36:51 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.0947
2022-02-20 21:37:33 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.1359
2022-02-20 21:38:15 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.1078
2022-02-20 21:38:57 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.2728
2022-02-20 21:39:39 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.0553
2022-02-20 21:40:21 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 1.9252
2022-02-20 21:41:03 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 1.9254
2022-02-20 21:41:06 - train: epoch 017, train_loss: 2.0869
2022-02-20 21:42:19 - eval: epoch: 017, acc1: 52.614%, acc5: 78.378%, test_loss: 2.0213, per_image_load_time: 0.875ms, per_image_inference_time: 0.829ms
2022-02-20 21:42:20 - until epoch: 017, best_acc1: 54.330%
2022-02-20 21:42:20 - epoch 018 lr: 0.1
2022-02-20 21:43:08 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.0831
2022-02-20 21:43:50 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.0762
2022-02-20 21:44:32 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.1934
2022-02-20 21:45:14 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.1814
2022-02-20 21:45:55 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 1.9854
2022-02-20 21:46:37 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.2165
2022-02-20 21:47:19 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 1.9243
2022-02-20 21:48:01 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 1.9972
2022-02-20 21:48:43 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.1695
2022-02-20 21:49:25 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 1.8621
2022-02-20 21:50:06 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.3437
2022-02-20 21:50:48 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.0955
2022-02-20 21:51:30 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.4157
2022-02-20 21:52:12 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.0541
2022-02-20 21:52:54 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.2164
2022-02-20 21:53:36 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.0683
2022-02-20 21:54:17 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 1.9572
2022-02-20 21:54:59 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 1.7927
2022-02-20 21:55:41 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.0686
2022-02-20 21:56:23 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.4080
2022-02-20 21:57:04 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.2252
2022-02-20 21:57:46 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.0861
2022-02-20 21:58:28 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.0888
2022-02-20 21:59:10 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 1.9596
2022-02-20 21:59:51 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 1.9275
2022-02-20 22:00:33 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 1.9324
2022-02-20 22:01:15 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.2370
2022-02-20 22:01:57 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 1.8925
2022-02-20 22:02:39 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.1953
2022-02-20 22:03:21 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.0346
2022-02-20 22:04:02 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.3498
2022-02-20 22:04:44 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 1.8718
2022-02-20 22:05:26 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 1.9502
2022-02-20 22:06:08 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.0531
2022-02-20 22:06:49 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.3003
2022-02-20 22:07:31 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.0847
2022-02-20 22:08:13 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.4546
2022-02-20 22:08:55 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.1001
2022-02-20 22:09:36 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.0958
2022-02-20 22:10:18 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 1.8626
2022-02-20 22:11:00 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.0392
2022-02-20 22:11:42 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 1.9514
2022-02-20 22:12:23 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 1.8388
2022-02-20 22:13:05 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.2903
2022-02-20 22:13:47 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.0266
2022-02-20 22:14:28 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 1.9106
2022-02-20 22:15:10 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.4024
2022-02-20 22:15:52 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.1516
2022-02-20 22:16:33 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.0433
2022-02-20 22:17:15 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.2136
2022-02-20 22:17:18 - train: epoch 018, train_loss: 2.0717
2022-02-20 22:18:33 - eval: epoch: 018, acc1: 54.372%, acc5: 79.312%, test_loss: 1.9491, per_image_load_time: 1.196ms, per_image_inference_time: 0.829ms
2022-02-20 22:18:35 - until epoch: 018, best_acc1: 54.372%
2022-02-20 22:18:35 - epoch 019 lr: 0.1
2022-02-20 22:19:21 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 1.8866
2022-02-20 22:20:03 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.2316
2022-02-20 22:20:45 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.3134
2022-02-20 22:21:27 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 1.8918
2022-02-20 22:22:08 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.0530
2022-02-20 22:22:50 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 1.9844
2022-02-20 22:23:32 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 1.8353
2022-02-20 22:24:14 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.2798
2022-02-20 22:24:56 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.0130
2022-02-20 22:25:38 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.0993
2022-02-20 22:26:19 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 1.9140
2022-02-20 22:27:01 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.0879
2022-02-20 22:27:43 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.2045
2022-02-20 22:28:25 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 1.9009
2022-02-20 22:29:07 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.4203
2022-02-20 22:29:49 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.0645
2022-02-20 22:30:30 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.1812
2022-02-20 22:31:12 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.0094
2022-02-20 22:31:54 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.1756
2022-02-20 22:32:36 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 1.9927
2022-02-20 22:33:17 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 1.8689
2022-02-20 22:33:59 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.0361
2022-02-20 22:34:41 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.0554
2022-02-20 22:35:23 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.1331
2022-02-20 22:36:04 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 1.9760
2022-02-20 22:36:46 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.0661
2022-02-20 22:37:28 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.0705
2022-02-20 22:38:10 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.1647
2022-02-20 22:38:52 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.0747
2022-02-20 22:39:33 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.3151
2022-02-20 22:40:15 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.1608
2022-02-20 22:40:57 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.7796
2022-02-20 22:41:39 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.0257
2022-02-20 22:42:21 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.1472
2022-02-20 22:43:03 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.1317
2022-02-20 22:43:44 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 1.9165
2022-02-20 22:44:26 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.1331
2022-02-20 22:45:08 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.2764
2022-02-20 22:45:49 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.1107
2022-02-20 22:46:31 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 1.9376
2022-02-20 22:47:13 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.0652
2022-02-20 22:47:55 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.0781
2022-02-20 22:48:37 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 1.9954
2022-02-20 22:49:18 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.1543
2022-02-20 22:50:00 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.2670
2022-02-20 22:50:42 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.0467
2022-02-20 22:51:24 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 1.9110
2022-02-20 22:52:06 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 1.9813
2022-02-20 22:52:48 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.1287
2022-02-20 22:53:30 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.0335
2022-02-20 22:53:33 - train: epoch 019, train_loss: 2.0662
2022-02-20 22:54:47 - eval: epoch: 019, acc1: 53.506%, acc5: 78.656%, test_loss: 1.9840, per_image_load_time: 1.266ms, per_image_inference_time: 0.813ms
2022-02-20 22:54:49 - until epoch: 019, best_acc1: 54.372%
2022-02-20 22:54:49 - epoch 020 lr: 0.1
2022-02-20 22:55:36 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.1407
2022-02-20 22:56:17 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 1.8971
2022-02-20 22:56:59 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.1012
2022-02-20 22:57:41 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 1.8046
2022-02-20 22:58:22 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 1.8460
2022-02-20 22:59:04 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.1412
2022-02-20 22:59:46 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 1.8870
2022-02-20 23:00:27 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.0920
2022-02-20 23:01:09 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.3576
2022-02-20 23:01:51 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.1317
2022-02-20 23:02:33 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 1.8942
2022-02-20 23:03:15 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 1.7799
2022-02-20 23:03:56 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 1.8952
2022-02-20 23:04:38 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.1700
2022-02-20 23:05:20 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.1504
2022-02-20 23:06:01 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 1.9278
2022-02-20 23:06:43 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 1.7962
2022-02-20 23:07:25 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.1013
2022-02-20 23:08:06 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 1.9149
2022-02-20 23:08:48 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 1.9390
2022-02-20 23:09:30 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.1910
2022-02-20 23:10:11 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 1.8007
2022-02-20 23:10:53 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 1.9990
2022-02-20 23:11:35 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.2128
2022-02-20 23:12:17 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 1.9712
2022-02-20 23:12:59 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 1.9021
2022-02-20 23:13:41 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.1090
2022-02-20 23:14:22 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.0494
2022-02-20 23:15:04 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.1409
2022-02-20 23:15:46 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.0060
2022-02-20 23:16:27 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.0644
2022-02-20 23:17:09 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.2446
2022-02-20 23:17:51 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 1.8381
2022-02-20 23:18:32 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.1458
2022-02-20 23:19:14 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 1.9780
2022-02-20 23:19:56 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.0711
2022-02-20 23:20:38 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 1.9745
2022-02-20 23:21:19 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.0368
2022-02-20 23:22:01 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.0803
2022-02-20 23:22:43 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 1.9164
2022-02-20 23:23:25 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 1.9968
2022-02-20 23:24:07 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.0552
2022-02-20 23:24:49 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 1.9970
2022-02-20 23:25:31 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.0732
2022-02-20 23:26:12 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.0869
2022-02-20 23:26:54 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.1306
2022-02-20 23:27:37 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 1.9169
2022-02-20 23:28:18 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.0304
2022-02-20 23:29:00 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.1578
2022-02-20 23:29:42 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 1.9078
2022-02-20 23:29:45 - train: epoch 020, train_loss: 2.0530
2022-02-20 23:31:00 - eval: epoch: 020, acc1: 53.690%, acc5: 79.090%, test_loss: 1.9807, per_image_load_time: 1.581ms, per_image_inference_time: 0.867ms
2022-02-20 23:31:02 - until epoch: 020, best_acc1: 54.372%
2022-02-20 23:31:02 - epoch 021 lr: 0.1
2022-02-20 23:31:49 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 1.9603
2022-02-20 23:32:31 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.0978
2022-02-20 23:33:13 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.8091
2022-02-20 23:33:55 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.1216
2022-02-20 23:34:37 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 1.9696
2022-02-20 23:35:19 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 1.8953
2022-02-20 23:36:01 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 1.9335
2022-02-20 23:36:43 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.2239
2022-02-20 23:37:25 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.0173
2022-02-20 23:38:06 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 1.9433
2022-02-20 23:38:48 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 1.9244
2022-02-20 23:39:30 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 1.9373
2022-02-20 23:40:12 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 1.9200
2022-02-20 23:40:54 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.0533
2022-02-20 23:41:36 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 1.8664
2022-02-20 23:42:18 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.0574
2022-02-20 23:43:00 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.0845
2022-02-20 23:43:42 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 1.9746
2022-02-20 23:44:23 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.1450
2022-02-20 23:45:05 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.2938
2022-02-20 23:45:47 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 1.8961
2022-02-20 23:46:29 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.0944
2022-02-20 23:47:11 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 1.8402
2022-02-20 23:47:53 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 1.9060
2022-02-20 23:48:35 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.0272
2022-02-20 23:49:17 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.2700
2022-02-20 23:49:59 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 1.9638
2022-02-20 23:50:41 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.0376
2022-02-20 23:51:23 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 1.9547
2022-02-20 23:52:04 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.2369
2022-02-20 23:52:46 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.0995
2022-02-20 23:53:28 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 1.9722
2022-02-20 23:54:10 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.2930
2022-02-20 23:54:52 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.2270
2022-02-20 23:55:34 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 1.9030
2022-02-20 23:56:16 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 1.9821
2022-02-20 23:56:58 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.1451
2022-02-20 23:57:39 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.0258
2022-02-20 23:58:21 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 1.9165
2022-02-20 23:59:03 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.2759
2022-02-20 23:59:45 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 1.9247
2022-02-21 00:00:27 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.0643
2022-02-21 00:01:09 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.0627
2022-02-21 00:01:51 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.0431
2022-02-21 00:02:32 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.2055
2022-02-21 00:03:14 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.0425
2022-02-21 00:03:56 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.1943
2022-02-21 00:04:38 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.2268
2022-02-21 00:05:20 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 1.8319
2022-02-21 00:06:02 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 1.9642
2022-02-21 00:06:05 - train: epoch 021, train_loss: 2.0485
2022-02-21 00:07:21 - eval: epoch: 021, acc1: 55.362%, acc5: 80.716%, test_loss: 1.8718, per_image_load_time: 1.407ms, per_image_inference_time: 0.888ms
2022-02-21 00:07:23 - until epoch: 021, best_acc1: 55.362%
2022-02-21 00:07:23 - epoch 022 lr: 0.1
2022-02-21 00:08:10 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 1.8188
2022-02-21 00:08:52 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 1.9236
2022-02-21 00:09:35 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 1.7745
2022-02-21 00:10:17 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 1.8954
2022-02-21 00:10:59 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.0030
2022-02-21 00:11:41 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.1301
2022-02-21 00:12:23 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.0751
2022-02-21 00:13:05 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.1259
2022-02-21 00:13:47 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.2844
2022-02-21 00:14:29 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.1841
2022-02-21 00:15:11 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.0578
2022-02-21 00:15:53 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.7391
2022-02-21 00:16:35 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.0522
2022-02-21 00:17:17 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.2209
2022-02-21 00:17:58 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.0497
2022-02-21 00:18:40 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 1.7731
2022-02-21 00:19:22 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 1.8314
2022-02-21 00:20:04 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.1827
2022-02-21 00:20:46 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 1.9555
2022-02-21 00:21:28 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.1077
2022-02-21 00:22:10 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.2004
2022-02-21 00:22:51 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 1.8320
2022-02-21 00:23:33 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.0950
2022-02-21 00:24:15 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.0844
2022-02-21 00:24:57 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.0469
2022-02-21 00:25:39 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 1.7451
2022-02-21 00:26:21 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 1.8317
2022-02-21 00:27:03 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.4009
2022-02-21 00:27:45 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 1.9409
2022-02-21 00:28:27 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.0968
2022-02-21 00:29:09 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.2997
2022-02-21 00:29:51 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.2031
2022-02-21 00:30:32 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 1.9878
2022-02-21 00:31:14 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 1.9653
2022-02-21 00:31:56 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.1019
2022-02-21 00:32:38 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 1.9998
2022-02-21 00:33:20 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.1455
2022-02-21 00:34:02 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.1655
2022-02-21 00:34:44 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 1.9982
2022-02-21 00:35:26 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.1027
2022-02-21 00:36:08 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 1.9680
2022-02-21 00:36:50 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 1.9921
2022-02-21 00:37:31 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.1851
2022-02-21 00:38:13 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 1.9970
2022-02-21 00:38:55 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.0156
2022-02-21 00:39:37 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.2207
2022-02-21 00:40:19 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.0572
2022-02-21 00:41:01 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 1.9261
2022-02-21 00:41:43 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 1.7584
2022-02-21 00:42:24 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 1.9991
2022-02-21 00:42:28 - train: epoch 022, train_loss: 2.0402
2022-02-21 00:43:43 - eval: epoch: 022, acc1: 56.320%, acc5: 81.414%, test_loss: 1.8298, per_image_load_time: 1.858ms, per_image_inference_time: 0.878ms
2022-02-21 00:43:45 - until epoch: 022, best_acc1: 56.320%
2022-02-21 00:43:45 - epoch 023 lr: 0.1
2022-02-21 00:44:32 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 1.9213
2022-02-21 00:45:14 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 1.7230
2022-02-21 00:45:56 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 1.8333
2022-02-21 00:46:37 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.1093
2022-02-21 00:47:19 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.1033
2022-02-21 00:48:01 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 1.8578
2022-02-21 00:48:43 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 1.7969
2022-02-21 00:49:25 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 1.9390
2022-02-21 00:50:07 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.1659
2022-02-21 00:50:49 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.0395
2022-02-21 00:51:30 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.0988
2022-02-21 00:52:12 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 1.9221
2022-02-21 00:52:54 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.0561
2022-02-21 00:53:36 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.0718
2022-02-21 00:54:17 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 1.9308
2022-02-21 00:54:59 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.0796
2022-02-21 00:55:41 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.1300
2022-02-21 00:56:23 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 1.9236
2022-02-21 00:57:05 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.1213
2022-02-21 00:57:46 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.7058
2022-02-21 00:58:28 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.1837
2022-02-21 00:59:10 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.7235
2022-02-21 00:59:52 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 1.9136
2022-02-21 01:00:33 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.0462
2022-02-21 01:01:15 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.1146
2022-02-21 01:01:57 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.0661
2022-02-21 01:02:39 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 1.8877
2022-02-21 01:03:20 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.0263
2022-02-21 01:04:02 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.0125
2022-02-21 01:04:44 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.2009
2022-02-21 01:05:26 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.0978
2022-02-21 01:06:08 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.1611
2022-02-21 01:06:50 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 1.9913
2022-02-21 01:07:31 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.1927
2022-02-21 01:08:13 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 1.9280
2022-02-21 01:08:55 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 1.8513
2022-02-21 01:09:37 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.0345
2022-02-21 01:10:18 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.1039
2022-02-21 01:11:00 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.0659
2022-02-21 01:11:42 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 1.9472
2022-02-21 01:12:24 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 1.8681
2022-02-21 01:13:06 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 1.9511
2022-02-21 01:13:47 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 1.8177
2022-02-21 01:14:29 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 1.8684
2022-02-21 01:15:11 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 1.8760
2022-02-21 01:15:53 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.0947
2022-02-21 01:16:35 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 1.8312
2022-02-21 01:17:17 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 1.9327
2022-02-21 01:17:59 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.0006
2022-02-21 01:18:41 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.0808
2022-02-21 01:18:45 - train: epoch 023, train_loss: 2.0319
2022-02-21 01:20:00 - eval: epoch: 023, acc1: 55.468%, acc5: 80.878%, test_loss: 1.8627, per_image_load_time: 1.243ms, per_image_inference_time: 0.839ms
2022-02-21 01:20:02 - until epoch: 023, best_acc1: 56.320%
2022-02-21 01:20:02 - epoch 024 lr: 0.1
2022-02-21 01:20:49 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 1.9427
2022-02-21 01:21:31 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.0801
2022-02-21 01:22:12 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 1.9295
2022-02-21 01:22:54 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.1177
2022-02-21 01:23:36 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 1.9786
2022-02-21 01:24:18 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 1.7457
2022-02-21 01:25:00 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 1.9655
2022-02-21 01:25:42 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 1.8391
2022-02-21 01:26:23 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 1.9978
2022-02-21 01:27:05 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.0180
2022-02-21 01:27:47 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.6549
2022-02-21 01:28:29 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 1.9124
2022-02-21 01:29:11 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.2759
2022-02-21 01:29:53 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 1.9698
2022-02-21 01:30:35 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.3112
2022-02-21 01:31:17 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.0396
2022-02-21 01:31:59 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.0443
2022-02-21 01:32:40 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.2590
2022-02-21 01:33:22 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 1.8298
2022-02-21 01:34:04 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 1.9899
2022-02-21 01:34:46 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.0749
2022-02-21 01:35:28 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 1.8160
2022-02-21 01:36:10 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.0470
2022-02-21 01:36:52 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.0190
2022-02-21 01:37:34 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 1.9732
2022-02-21 01:38:16 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 1.9836
2022-02-21 01:38:58 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.1067
2022-02-21 01:39:40 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.2321
2022-02-21 01:40:22 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.1341
2022-02-21 01:41:04 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 1.9957
2022-02-21 01:41:45 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 1.9148
2022-02-21 01:42:27 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.1504
2022-02-21 01:43:09 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.7300
2022-02-21 01:43:50 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.0145
2022-02-21 01:44:32 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 1.8761
2022-02-21 01:45:14 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.0948
2022-02-21 01:45:55 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 1.9808
2022-02-21 01:46:37 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.2917
2022-02-21 01:47:19 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.0062
2022-02-21 01:48:00 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.0049
2022-02-21 01:48:42 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 1.9573
2022-02-21 01:49:24 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.0004
2022-02-21 01:50:05 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.0672
2022-02-21 01:50:47 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.0319
2022-02-21 01:51:29 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 1.9148
2022-02-21 01:52:10 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.0594
2022-02-21 01:52:52 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.0447
2022-02-21 01:53:34 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 1.9147
2022-02-21 01:54:16 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.0827
2022-02-21 01:54:58 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.1768
2022-02-21 01:55:01 - train: epoch 024, train_loss: 2.0271
2022-02-21 01:56:16 - eval: epoch: 024, acc1: 56.032%, acc5: 80.940%, test_loss: 1.8474, per_image_load_time: 0.682ms, per_image_inference_time: 0.895ms
2022-02-21 01:56:18 - until epoch: 024, best_acc1: 56.320%
2022-02-21 01:56:18 - epoch 025 lr: 0.1
2022-02-21 01:57:05 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 1.8213
2022-02-21 01:57:47 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 1.7786
2022-02-21 01:58:28 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 1.8605
2022-02-21 01:59:10 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 1.9754
2022-02-21 01:59:52 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 1.9184
2022-02-21 02:00:34 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.0591
2022-02-21 02:01:15 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.0107
2022-02-21 02:01:57 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 1.9953
2022-02-21 02:02:39 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 1.8023
2022-02-21 02:03:21 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 1.9731
2022-02-21 02:04:03 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.0591
2022-02-21 02:04:45 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.0394
2022-02-21 02:05:27 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.0556
2022-02-21 02:06:08 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.2907
2022-02-21 02:06:50 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 1.9114
2022-02-21 02:07:32 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 1.7574
2022-02-21 02:08:14 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 1.9619
2022-02-21 02:08:56 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 1.9362
2022-02-21 02:09:38 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 1.9392
2022-02-21 02:10:20 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.0296
2022-02-21 02:11:02 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 1.8557
2022-02-21 02:11:44 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 1.8819
2022-02-21 02:12:26 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.0416
2022-02-21 02:13:08 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 1.7758
2022-02-21 02:13:49 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.0964
2022-02-21 02:14:31 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.1545
2022-02-21 02:15:13 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.1379
2022-02-21 02:15:55 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 1.9483
2022-02-21 02:16:37 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.2561
2022-02-21 02:17:19 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.2983
2022-02-21 02:18:01 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 1.9119
2022-02-21 02:18:43 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.2105
2022-02-21 02:19:26 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 1.9855
2022-02-21 02:20:08 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.1315
2022-02-21 02:20:50 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 1.7545
2022-02-21 02:21:31 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.2030
2022-02-21 02:22:13 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.0367
2022-02-21 02:22:55 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.1387
2022-02-21 02:23:38 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.2157
2022-02-21 02:24:19 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.0803
2022-02-21 02:25:01 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.1658
2022-02-21 02:25:43 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.1161
2022-02-21 02:26:25 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 1.9022
2022-02-21 02:27:07 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 1.9186
2022-02-21 02:27:49 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 1.9706
2022-02-21 02:28:31 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.0028
2022-02-21 02:29:13 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 1.9790
2022-02-21 02:29:55 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 1.7862
2022-02-21 02:30:38 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 1.9198
2022-02-21 02:31:20 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.2553
2022-02-21 02:31:23 - train: epoch 025, train_loss: 2.0229
2022-02-21 02:32:39 - eval: epoch: 025, acc1: 56.548%, acc5: 81.780%, test_loss: 1.8131, per_image_load_time: 0.719ms, per_image_inference_time: 0.852ms
2022-02-21 02:32:41 - until epoch: 025, best_acc1: 56.548%
2022-02-21 02:32:41 - epoch 026 lr: 0.1
2022-02-21 02:33:28 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 1.8364
2022-02-21 02:34:10 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.8777
2022-02-21 02:34:51 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 1.9461
2022-02-21 02:35:33 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 1.9787
2022-02-21 02:36:15 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.0716
2022-02-21 02:36:57 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.0928
2022-02-21 02:37:38 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 1.9477
2022-02-21 02:38:20 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 1.7111
2022-02-21 02:39:02 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.3382
2022-02-21 02:39:44 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 1.9513
2022-02-21 02:40:26 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 1.9663
2022-02-21 02:41:08 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.0428
2022-02-21 02:41:50 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 1.9050
2022-02-21 02:42:32 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.1707
2022-02-21 02:43:14 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 1.9189
2022-02-21 02:43:55 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.1512
2022-02-21 02:44:37 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.0200
2022-02-21 02:45:19 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.1643
2022-02-21 02:46:01 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.3452
2022-02-21 02:46:43 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.0831
2022-02-21 02:47:24 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.0202
2022-02-21 02:48:06 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 1.9278
2022-02-21 02:48:48 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 1.8920
2022-02-21 02:49:30 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.3522
2022-02-21 02:50:12 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.1230
2022-02-21 02:50:54 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.0661
2022-02-21 02:51:36 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 1.9461
2022-02-21 02:52:18 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 1.9121
2022-02-21 02:53:00 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.1230
2022-02-21 02:53:42 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 1.9213
2022-02-21 02:54:24 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 1.9634
2022-02-21 02:55:06 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 1.9894
2022-02-21 02:55:48 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 1.9456
2022-02-21 02:56:30 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.2468
2022-02-21 02:57:12 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.1916
2022-02-21 02:57:54 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 1.8349
2022-02-21 02:58:36 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.0687
2022-02-21 02:59:18 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.1284
2022-02-21 03:00:00 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.1659
2022-02-21 03:00:42 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.2960
2022-02-21 03:01:24 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.0314
2022-02-21 03:02:06 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.0716
2022-02-21 03:02:48 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.1669
2022-02-21 03:03:30 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.2273
2022-02-21 03:04:12 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.1745
2022-02-21 03:04:54 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.0354
2022-02-21 03:05:36 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 1.8415
2022-02-21 03:06:18 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.0117
2022-02-21 03:07:00 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.1011
2022-02-21 03:07:42 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.1847
2022-02-21 03:07:45 - train: epoch 026, train_loss: 2.0166
2022-02-21 03:09:01 - eval: epoch: 026, acc1: 56.504%, acc5: 81.678%, test_loss: 1.8075, per_image_load_time: 0.853ms, per_image_inference_time: 0.855ms
2022-02-21 03:09:03 - until epoch: 026, best_acc1: 56.548%
2022-02-21 03:09:03 - epoch 027 lr: 0.1
2022-02-21 03:09:50 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.1798
2022-02-21 03:10:32 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 1.9577
2022-02-21 03:11:14 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.0506
2022-02-21 03:11:56 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.1402
2022-02-21 03:12:38 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.1285
2022-02-21 03:13:19 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.2468
2022-02-21 03:14:01 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.2089
2022-02-21 03:14:43 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.0313
2022-02-21 03:15:25 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 1.9443
2022-02-21 03:16:07 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.0473
2022-02-21 03:16:48 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 1.9712
2022-02-21 03:17:30 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.2653
2022-02-21 03:18:12 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.0000
2022-02-21 03:18:54 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.2243
2022-02-21 03:19:36 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.0516
2022-02-21 03:20:18 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.0767
2022-02-21 03:21:00 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.0169
2022-02-21 03:21:42 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 1.8963
2022-02-21 03:22:23 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.2706
2022-02-21 03:23:05 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 1.9432
2022-02-21 03:23:47 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.1473
2022-02-21 03:24:29 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.2093
2022-02-21 03:25:11 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.2736
2022-02-21 03:25:53 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.0317
2022-02-21 03:26:35 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 1.8928
2022-02-21 03:27:17 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.2338
2022-02-21 03:27:59 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 1.8998
2022-02-21 03:28:41 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.1934
2022-02-21 03:29:23 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.0625
2022-02-21 03:30:05 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 1.8766
2022-02-21 03:30:47 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 1.8783
2022-02-21 03:31:29 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 1.7990
2022-02-21 03:32:11 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.0651
2022-02-21 03:32:53 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.0256
2022-02-21 03:33:35 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.1947
2022-02-21 03:34:17 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 1.8619
2022-02-21 03:34:59 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 1.9338
2022-02-21 03:35:41 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 1.7638
2022-02-21 03:36:23 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 1.8444
2022-02-21 03:37:05 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.0150
2022-02-21 03:37:47 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.0847
2022-02-21 03:38:29 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.0378
2022-02-21 03:39:11 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 1.9408
2022-02-21 03:39:53 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 1.9723
2022-02-21 03:40:35 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 1.8704
2022-02-21 03:41:17 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.0193
2022-02-21 03:41:59 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.0661
2022-02-21 03:42:41 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.0846
2022-02-21 03:43:23 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.1120
2022-02-21 03:44:05 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.7605
2022-02-21 03:44:09 - train: epoch 027, train_loss: 2.0134
2022-02-21 03:45:25 - eval: epoch: 027, acc1: 56.238%, acc5: 81.078%, test_loss: 1.8454, per_image_load_time: 0.828ms, per_image_inference_time: 0.836ms
2022-02-21 03:45:27 - until epoch: 027, best_acc1: 56.548%
2022-02-21 03:45:27 - epoch 028 lr: 0.1
2022-02-21 03:46:14 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 1.6889
2022-02-21 03:46:57 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 1.9367
2022-02-21 03:47:39 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 1.9282
2022-02-21 03:48:21 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 1.9098
2022-02-21 03:49:03 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 1.7881
2022-02-21 03:49:45 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.0340
2022-02-21 03:50:27 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.2424
2022-02-21 03:51:09 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.7373
2022-02-21 03:51:52 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 1.8516
2022-02-21 03:52:34 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.0279
2022-02-21 03:53:16 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.0644
2022-02-21 03:53:59 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 1.8110
2022-02-21 03:54:41 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.0864
2022-02-21 03:55:23 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.3059
2022-02-21 03:56:06 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 1.9337
2022-02-21 03:56:48 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.0056
2022-02-21 03:57:30 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 1.8453
2022-02-21 03:58:13 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 1.9320
2022-02-21 03:58:55 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 1.8929
2022-02-21 03:59:37 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.1625
2022-02-21 04:00:19 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.0493
2022-02-21 04:01:02 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.0303
2022-02-21 04:01:44 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.1931
2022-02-21 04:02:26 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.2080
2022-02-21 04:03:08 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 1.9898
2022-02-21 04:03:50 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 1.8914
2022-02-21 04:04:33 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.0221
2022-02-21 04:05:15 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 1.9120
2022-02-21 04:05:57 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.0958
2022-02-21 04:06:40 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.1051
2022-02-21 04:07:22 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.1901
2022-02-21 04:08:04 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 1.9066
2022-02-21 04:08:46 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.0263
2022-02-21 04:09:28 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 1.9585
2022-02-21 04:10:10 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 1.9579
2022-02-21 04:10:53 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 1.8632
2022-02-21 04:11:35 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 1.8795
2022-02-21 04:12:17 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.7184
2022-02-21 04:12:59 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 1.9903
2022-02-21 04:13:41 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.0896
2022-02-21 04:14:24 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.1169
2022-02-21 04:15:06 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.1824
2022-02-21 04:15:48 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 1.9743
2022-02-21 04:16:31 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.0692
2022-02-21 04:17:13 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 1.9650
2022-02-21 04:17:55 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.1238
2022-02-21 04:18:36 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.0861
2022-02-21 04:19:18 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 1.9959
2022-02-21 04:19:59 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 1.8233
2022-02-21 04:20:41 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 1.9607
2022-02-21 04:20:44 - train: epoch 028, train_loss: 2.0056
2022-02-21 04:21:59 - eval: epoch: 028, acc1: 57.810%, acc5: 81.966%, test_loss: 1.7819, per_image_load_time: 0.817ms, per_image_inference_time: 0.880ms
2022-02-21 04:22:01 - until epoch: 028, best_acc1: 57.810%
2022-02-21 04:22:01 - epoch 029 lr: 0.1
2022-02-21 04:22:47 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.1272
2022-02-21 04:23:29 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.0149
2022-02-21 04:24:11 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.1935
2022-02-21 04:24:52 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 1.9134
2022-02-21 04:25:34 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.0961
2022-02-21 04:26:16 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.3048
2022-02-21 04:26:57 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.5445
2022-02-21 04:27:39 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.1052
2022-02-21 04:28:21 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.0189
2022-02-21 04:29:03 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 1.8155
2022-02-21 04:29:44 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 1.9599
2022-02-21 04:30:26 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.0317
2022-02-21 04:31:08 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 1.9700
2022-02-21 04:31:50 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.2459
2022-02-21 04:32:32 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.0889
2022-02-21 04:33:13 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 1.9381
2022-02-21 04:33:55 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.0466
2022-02-21 04:34:37 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.0779
2022-02-21 04:35:18 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 1.8318
2022-02-21 04:36:00 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.2689
2022-02-21 04:36:42 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 1.9402
2022-02-21 04:37:24 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.0456
2022-02-21 04:38:05 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.0157
2022-02-21 04:38:47 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 1.9690
2022-02-21 04:39:29 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 1.8508
2022-02-21 04:40:11 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.0423
2022-02-21 04:40:52 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 1.9821
2022-02-21 04:41:34 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 1.9462
2022-02-21 04:42:16 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 1.9726
2022-02-21 04:42:57 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.0268
2022-02-21 04:43:39 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 1.9283
2022-02-21 04:44:20 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.0794
2022-02-21 04:45:02 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.0040
2022-02-21 04:45:44 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 1.7884
2022-02-21 04:46:26 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.1541
2022-02-21 04:47:07 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 1.9590
2022-02-21 04:47:49 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 1.9441
2022-02-21 04:48:30 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 1.9260
2022-02-21 04:49:12 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 1.8174
2022-02-21 04:49:54 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 1.9176
2022-02-21 04:50:36 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 1.8483
2022-02-21 04:51:17 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 1.8150
2022-02-21 04:51:59 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.1939
2022-02-21 04:52:41 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 1.9097
2022-02-21 04:53:23 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.1652
2022-02-21 04:54:04 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.1903
2022-02-21 04:54:46 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 1.8046
2022-02-21 04:55:28 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.1140
2022-02-21 04:56:10 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.4518
2022-02-21 04:56:52 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.7000
2022-02-21 04:56:55 - train: epoch 029, train_loss: 2.0026
2022-02-21 04:58:11 - eval: epoch: 029, acc1: 56.782%, acc5: 81.438%, test_loss: 1.8170, per_image_load_time: 1.234ms, per_image_inference_time: 0.879ms
2022-02-21 04:58:12 - until epoch: 029, best_acc1: 57.810%
2022-02-21 04:58:12 - epoch 030 lr: 0.1
2022-02-21 04:59:00 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.1505
2022-02-21 04:59:42 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.0542
2022-02-21 05:00:23 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.0284
2022-02-21 05:01:05 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 1.7750
2022-02-21 05:01:47 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.3103
2022-02-21 05:02:29 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 1.7625
2022-02-21 05:03:10 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 1.9611
2022-02-21 05:03:52 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.1679
2022-02-21 05:04:34 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.0509
2022-02-21 05:05:16 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 1.6689
2022-02-21 05:05:57 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 1.8321
2022-02-21 05:06:39 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.0212
2022-02-21 05:07:21 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 1.8538
2022-02-21 05:08:03 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 1.9773
2022-02-21 05:08:45 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 1.9388
2022-02-21 05:09:26 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 1.9447
2022-02-21 05:10:08 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.1408
2022-02-21 05:10:50 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.0319
2022-02-21 05:11:32 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.0913
2022-02-21 05:12:14 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.0872
2022-02-21 05:12:55 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.0637
2022-02-21 05:13:37 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 1.9527
2022-02-21 05:14:19 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 1.9670
2022-02-21 05:15:00 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.1611
2022-02-21 05:15:42 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.0639
2022-02-21 05:16:24 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 1.9969
2022-02-21 05:17:05 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 1.9144
2022-02-21 05:17:47 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 1.9204
2022-02-21 05:18:29 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.0312
2022-02-21 05:19:10 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.2603
2022-02-21 05:19:52 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.0975
2022-02-21 05:20:34 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 1.9316
2022-02-21 05:21:16 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.2517
2022-02-21 05:21:58 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.0063
2022-02-21 05:22:39 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.1315
2022-02-21 05:23:21 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 1.8670
2022-02-21 05:24:03 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 1.8927
2022-02-21 05:24:45 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.1201
2022-02-21 05:25:26 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.1205
2022-02-21 05:26:08 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 1.8676
2022-02-21 05:26:50 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 1.8892
2022-02-21 05:27:32 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.1998
2022-02-21 05:28:13 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 1.9222
2022-02-21 05:28:55 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.1559
2022-02-21 05:29:37 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.1557
2022-02-21 05:30:19 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 1.8225
2022-02-21 05:31:00 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.0358
2022-02-21 05:31:42 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 1.9438
2022-02-21 05:32:24 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.1958
2022-02-21 05:33:06 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.1113
2022-02-21 05:33:09 - train: epoch 030, train_loss: 2.0017
2022-02-21 05:34:24 - eval: epoch: 030, acc1: 56.384%, acc5: 81.364%, test_loss: 1.8344, per_image_load_time: 1.374ms, per_image_inference_time: 0.906ms
2022-02-21 05:34:26 - until epoch: 030, best_acc1: 57.810%
2022-02-21 05:34:26 - epoch 031 lr: 0.010000000000000002
2022-02-21 05:35:13 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 1.8143
2022-02-21 05:35:54 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.4793
2022-02-21 05:36:36 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.4236
2022-02-21 05:37:17 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.7703
2022-02-21 05:37:59 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.4732
2022-02-21 05:38:41 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.4559
2022-02-21 05:39:22 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.4652
2022-02-21 05:40:04 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.4129
2022-02-21 05:40:46 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.4575
2022-02-21 05:41:28 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 1.6520
2022-02-21 05:42:09 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 1.6778
2022-02-21 05:42:51 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.4871
2022-02-21 05:43:33 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.2829
2022-02-21 05:44:14 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.4732
2022-02-21 05:44:56 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.5958
2022-02-21 05:45:38 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.3373
2022-02-21 05:46:19 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.3064
2022-02-21 05:47:01 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.3987
2022-02-21 05:47:43 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.5516
2022-02-21 05:48:25 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.4241
2022-02-21 05:49:06 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.2798
2022-02-21 05:49:48 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.2587
2022-02-21 05:50:29 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.3012
2022-02-21 05:51:11 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.3991
2022-02-21 05:51:53 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.3160
2022-02-21 05:52:35 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.4836
2022-02-21 05:53:16 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.4279
2022-02-21 05:53:58 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.6890
2022-02-21 05:54:40 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.4731
2022-02-21 05:55:21 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.5491
2022-02-21 05:56:03 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.3452
2022-02-21 05:56:44 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.5612
2022-02-21 05:57:26 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.4846
2022-02-21 05:58:08 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.5209
2022-02-21 05:58:50 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.5475
2022-02-21 05:59:32 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.4222
2022-02-21 06:00:13 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.3686
2022-02-21 06:00:55 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.2923
2022-02-21 06:01:37 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.2808
2022-02-21 06:02:18 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.3239
2022-02-21 06:03:00 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.3266
2022-02-21 06:03:42 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.4733
2022-02-21 06:04:24 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.3600
2022-02-21 06:05:05 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.4824
2022-02-21 06:05:47 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.5289
2022-02-21 06:06:29 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.3928
2022-02-21 06:07:11 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.4572
2022-02-21 06:07:52 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.2851
2022-02-21 06:08:34 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.3744
2022-02-21 06:09:16 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.3738
2022-02-21 06:09:19 - train: epoch 031, train_loss: 1.4488
2022-02-21 06:10:35 - eval: epoch: 031, acc1: 71.756%, acc5: 90.694%, test_loss: 1.1220, per_image_load_time: 1.280ms, per_image_inference_time: 0.866ms
2022-02-21 06:10:37 - until epoch: 031, best_acc1: 71.756%
2022-02-21 06:10:37 - epoch 032 lr: 0.010000000000000002
2022-02-21 06:11:24 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.3235
2022-02-21 06:12:06 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.3491
2022-02-21 06:12:47 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.3995
2022-02-21 06:13:29 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.5569
2022-02-21 06:14:11 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.3967
2022-02-21 06:14:53 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.4061
2022-02-21 06:15:34 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.3468
2022-02-21 06:16:16 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.3979
2022-02-21 06:16:57 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.3002
2022-02-21 06:17:39 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.4256
2022-02-21 06:18:21 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.4266
2022-02-21 06:19:03 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.3237
2022-02-21 06:19:44 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.2679
2022-02-21 06:20:26 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.4630
2022-02-21 06:21:08 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.3606
2022-02-21 06:21:49 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.3884
2022-02-21 06:22:31 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.4887
2022-02-21 06:23:13 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.5986
2022-02-21 06:23:54 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.2069
2022-02-21 06:24:36 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.3785
2022-02-21 06:25:18 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.2498
2022-02-21 06:26:00 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.2024
2022-02-21 06:26:41 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.3359
2022-02-21 06:27:23 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.2557
2022-02-21 06:28:05 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.4899
2022-02-21 06:28:46 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.1443
2022-02-21 06:29:28 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.3787
2022-02-21 06:30:10 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.4216
2022-02-21 06:30:51 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.1398
2022-02-21 06:31:33 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.1314
2022-02-21 06:32:15 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.2891
2022-02-21 06:32:57 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.3181
2022-02-21 06:33:38 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.2763
2022-02-21 06:34:20 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.1765
2022-02-21 06:35:02 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.2776
2022-02-21 06:35:44 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.3757
2022-02-21 06:36:26 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.3501
2022-02-21 06:37:07 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.1841
2022-02-21 06:37:49 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.3544
2022-02-21 06:38:31 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.2408
2022-02-21 06:39:13 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.2772
2022-02-21 06:39:54 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.1623
2022-02-21 06:40:36 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.6012
2022-02-21 06:41:18 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.2730
2022-02-21 06:42:00 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.5204
2022-02-21 06:42:41 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.2828
2022-02-21 06:43:23 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.3866
2022-02-21 06:44:05 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.1900
2022-02-21 06:44:46 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.4297
2022-02-21 06:45:28 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.3342
2022-02-21 06:45:32 - train: epoch 032, train_loss: 1.3312
2022-02-21 06:46:47 - eval: epoch: 032, acc1: 72.390%, acc5: 91.238%, test_loss: 1.0819, per_image_load_time: 0.703ms, per_image_inference_time: 0.856ms
2022-02-21 06:46:49 - until epoch: 032, best_acc1: 72.390%
2022-02-21 06:46:49 - epoch 033 lr: 0.010000000000000002
2022-02-21 06:47:36 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.1161
2022-02-21 06:48:18 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.4180
2022-02-21 06:49:00 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.1825
2022-02-21 06:49:41 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.2079
2022-02-21 06:50:23 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.3703
2022-02-21 06:51:05 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.1131
2022-02-21 06:51:47 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.2809
2022-02-21 06:52:28 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.3677
2022-02-21 06:53:10 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.3054
2022-02-21 06:53:52 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.3298
2022-02-21 06:54:34 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.2655
2022-02-21 06:55:16 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.2810
2022-02-21 06:55:57 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.1538
2022-02-21 06:56:39 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.4511
2022-02-21 06:57:21 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.5276
2022-02-21 06:58:03 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.4346
2022-02-21 06:58:45 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.1993
2022-02-21 06:59:26 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.4986
2022-02-21 07:00:08 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.3664
2022-02-21 07:00:50 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.1735
2022-02-21 07:01:32 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.4099
2022-02-21 07:02:13 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.3886
2022-02-21 07:02:55 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.2332
2022-02-21 07:03:37 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.5273
2022-02-21 07:04:19 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.2727
2022-02-21 07:05:01 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.1152
2022-02-21 07:05:42 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.3750
2022-02-21 07:06:24 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.2951
2022-02-21 07:07:06 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.3568
2022-02-21 07:07:48 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.3169
2022-02-21 07:08:30 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.2960
2022-02-21 07:09:12 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.2933
2022-02-21 07:09:54 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.4197
2022-02-21 07:10:35 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.1771
2022-02-21 07:11:17 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.2382
2022-02-21 07:11:59 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.4466
2022-02-21 07:12:41 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.2532
2022-02-21 07:13:22 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.1979
2022-02-21 07:14:04 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.3040
2022-02-21 07:14:46 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.3284
2022-02-21 07:15:28 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.3212
2022-02-21 07:16:10 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.2158
2022-02-21 07:16:51 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.4196
2022-02-21 07:17:33 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.3249
2022-02-21 07:18:15 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.4858
2022-02-21 07:18:57 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.2962
2022-02-21 07:19:39 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.1894
2022-02-21 07:20:20 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.6575
2022-02-21 07:21:02 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.1615
2022-02-21 07:21:44 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.2456
2022-02-21 07:21:47 - train: epoch 033, train_loss: 1.2822
2022-02-21 07:23:03 - eval: epoch: 033, acc1: 72.432%, acc5: 91.236%, test_loss: 1.0819, per_image_load_time: 1.869ms, per_image_inference_time: 0.888ms
2022-02-21 07:23:05 - until epoch: 033, best_acc1: 72.432%
2022-02-21 07:23:05 - epoch 034 lr: 0.010000000000000002
2022-02-21 07:23:51 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.1860
2022-02-21 07:24:33 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.2843
2022-02-21 07:25:15 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.2126
2022-02-21 07:25:56 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.0496
2022-02-21 07:26:38 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.3163
2022-02-21 07:27:20 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.5261
2022-02-21 07:28:02 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.2607
2022-02-21 07:28:43 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.2036
2022-02-21 07:29:25 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.2508
2022-02-21 07:30:07 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.1249
2022-02-21 07:30:48 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.1559
2022-02-21 07:31:30 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.2502
2022-02-21 07:32:11 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.2148
2022-02-21 07:32:53 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.2533
2022-02-21 07:33:35 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.1780
2022-02-21 07:34:16 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.1382
2022-02-21 07:34:58 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.2280
2022-02-21 07:35:40 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.4134
2022-02-21 07:36:21 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.2690
2022-02-21 07:37:03 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.3480
2022-02-21 07:37:45 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.4772
2022-02-21 07:38:27 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.1892
2022-02-21 07:39:09 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.3681
2022-02-21 07:39:50 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.1402
2022-02-21 07:40:32 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.2288
2022-02-21 07:41:14 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.3282
2022-02-21 07:41:55 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.2762
2022-02-21 07:42:37 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.0965
2022-02-21 07:43:19 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.0511
2022-02-21 07:44:00 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.0481
2022-02-21 07:44:42 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.1402
2022-02-21 07:45:24 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.2206
2022-02-21 07:46:06 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.1831
2022-02-21 07:46:47 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.3166
2022-02-21 07:47:29 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.1066
2022-02-21 07:48:11 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.0309
2022-02-21 07:48:53 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.1165
2022-02-21 07:49:35 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.1972
2022-02-21 07:50:17 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.3838
2022-02-21 07:50:58 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.0798
2022-02-21 07:51:40 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.2209
2022-02-21 07:52:22 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.2521
2022-02-21 07:53:04 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.2334
2022-02-21 07:53:45 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.2908
2022-02-21 07:54:27 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.4070
2022-02-21 07:55:09 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.4475
2022-02-21 07:55:51 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.2133
2022-02-21 07:56:32 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.1947
2022-02-21 07:57:14 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.2646
2022-02-21 07:57:56 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.1208
2022-02-21 07:57:59 - train: epoch 034, train_loss: 1.2533
2022-02-21 07:59:16 - eval: epoch: 034, acc1: 73.296%, acc5: 91.636%, test_loss: 1.0523, per_image_load_time: 2.036ms, per_image_inference_time: 0.855ms
2022-02-21 07:59:18 - until epoch: 034, best_acc1: 73.296%
2022-02-21 07:59:18 - epoch 035 lr: 0.010000000000000002
2022-02-21 08:00:05 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.1346
2022-02-21 08:00:46 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.0294
2022-02-21 08:01:28 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.3786
2022-02-21 08:02:10 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.0469
2022-02-21 08:02:52 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.2492
2022-02-21 08:03:33 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.1898
2022-02-21 08:04:15 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.2032
2022-02-21 08:04:57 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.2106
2022-02-21 08:05:38 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.2885
2022-02-21 08:06:20 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.4085
2022-02-21 08:07:02 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.4069
2022-02-21 08:07:43 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.1845
2022-02-21 08:08:25 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.2950
2022-02-21 08:09:06 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.2467
2022-02-21 08:09:48 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.3582
2022-02-21 08:10:30 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.1892
2022-02-21 08:11:12 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.1113
2022-02-21 08:11:53 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.2709
2022-02-21 08:12:35 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.2869
2022-02-21 08:13:17 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.2709
2022-02-21 08:13:58 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.1806
2022-02-21 08:14:40 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.2867
2022-02-21 08:15:22 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.2866
2022-02-21 08:16:04 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.2846
2022-02-21 08:16:45 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.1466
2022-02-21 08:17:27 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.4000
2022-02-21 08:18:09 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.3362
2022-02-21 08:18:50 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.2056
2022-02-21 08:19:32 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.2044
2022-02-21 08:20:14 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.3947
2022-02-21 08:20:55 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.2650
2022-02-21 08:21:37 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.1498
2022-02-21 08:22:19 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.0573
2022-02-21 08:23:00 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.1938
2022-02-21 08:23:42 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.0422
2022-02-21 08:24:24 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.2465
2022-02-21 08:25:06 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 0.9673
2022-02-21 08:25:47 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.2942
2022-02-21 08:26:29 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.2896
2022-02-21 08:27:11 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.0805
2022-02-21 08:27:52 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.4995
2022-02-21 08:28:34 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.2017
2022-02-21 08:29:16 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.2349
2022-02-21 08:29:58 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.1893
2022-02-21 08:30:39 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.3624
2022-02-21 08:31:21 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.1000
2022-02-21 08:32:03 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.4465
2022-02-21 08:32:45 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.4438
2022-02-21 08:33:26 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.2422
2022-02-21 08:34:08 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.1559
2022-02-21 08:34:11 - train: epoch 035, train_loss: 1.2318
2022-02-21 08:35:27 - eval: epoch: 035, acc1: 73.062%, acc5: 91.408%, test_loss: 1.0648, per_image_load_time: 0.956ms, per_image_inference_time: 0.871ms
2022-02-21 08:35:29 - until epoch: 035, best_acc1: 73.296%
2022-02-21 08:35:29 - epoch 036 lr: 0.010000000000000002
2022-02-21 08:36:16 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.3628
2022-02-21 08:36:57 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.0180
2022-02-21 08:37:39 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.0630
2022-02-21 08:38:21 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.2481
2022-02-21 08:39:03 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.2250
2022-02-21 08:39:44 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.1176
2022-02-21 08:40:26 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.0364
2022-02-21 08:41:08 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.0409
2022-02-21 08:41:49 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.1554
2022-02-21 08:42:31 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.1866
2022-02-21 08:43:13 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.3311
2022-02-21 08:43:55 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.2270
2022-02-21 08:44:37 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.1712
2022-02-21 08:45:18 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.3638
2022-02-21 08:46:00 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.2911
2022-02-21 08:46:42 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.1725
2022-02-21 08:47:23 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.2110
2022-02-21 08:48:05 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.1363
2022-02-21 08:48:47 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.2213
2022-02-21 08:49:29 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.2818
2022-02-21 08:50:11 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.2619
2022-02-21 08:50:52 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.2084
2022-02-21 08:51:34 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.2579
2022-02-21 08:52:16 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.3046
2022-02-21 08:52:58 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.0843
2022-02-21 08:53:40 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.2671
2022-02-21 08:54:22 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.0988
2022-02-21 08:55:04 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.1465
2022-02-21 08:55:45 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.0506
2022-02-21 08:56:27 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.2636
2022-02-21 08:57:09 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.2166
2022-02-21 08:57:51 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.3311
2022-02-21 08:58:33 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.0996
2022-02-21 08:59:15 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.1629
2022-02-21 08:59:57 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.1989
2022-02-21 09:00:39 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.2631
2022-02-21 09:01:21 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.2543
2022-02-21 09:02:03 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.1006
2022-02-21 09:02:45 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.1836
2022-02-21 09:03:27 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.2799
2022-02-21 09:04:09 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.2156
2022-02-21 09:04:51 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.1555
2022-02-21 09:05:33 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.1927
2022-02-21 09:06:15 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.3053
2022-02-21 09:06:57 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.0040
2022-02-21 09:07:39 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.0667
2022-02-21 09:08:22 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.2374
2022-02-21 09:09:04 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.2056
2022-02-21 09:09:46 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.1992
2022-02-21 09:10:28 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.1444
2022-02-21 09:10:31 - train: epoch 036, train_loss: 1.2172
2022-02-21 09:11:48 - eval: epoch: 036, acc1: 73.298%, acc5: 91.620%, test_loss: 1.0528, per_image_load_time: 2.064ms, per_image_inference_time: 0.863ms
2022-02-21 09:11:50 - until epoch: 036, best_acc1: 73.298%
2022-02-21 09:11:50 - epoch 037 lr: 0.010000000000000002
2022-02-21 09:12:36 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 0.9411
2022-02-21 09:13:18 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 0.9341
2022-02-21 09:14:00 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.0981
2022-02-21 09:14:42 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.3127
2022-02-21 09:15:24 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.1276
2022-02-21 09:16:07 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.2291
2022-02-21 09:16:48 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.3039
2022-02-21 09:17:30 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.1685
2022-02-21 09:18:12 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.4723
2022-02-21 09:18:54 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.1566
2022-02-21 09:19:36 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.2176
2022-02-21 09:20:18 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.2158
2022-02-21 09:20:59 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.2672
2022-02-21 09:21:41 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.3114
2022-02-21 09:22:23 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.1367
2022-02-21 09:23:05 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.0449
2022-02-21 09:23:47 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.3586
2022-02-21 09:24:29 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.3260
2022-02-21 09:25:11 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.3581
2022-02-21 09:25:52 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.1294
2022-02-21 09:26:34 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.2698
2022-02-21 09:27:16 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.2742
2022-02-21 09:27:58 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.0732
2022-02-21 09:28:40 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.2340
2022-02-21 09:29:22 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.0884
2022-02-21 09:30:04 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.3998
2022-02-21 09:30:46 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.2117
2022-02-21 09:31:28 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.2682
2022-02-21 09:32:10 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.2941
2022-02-21 09:32:51 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.3154
2022-02-21 09:33:33 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.1162
2022-02-21 09:34:15 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.2023
2022-02-21 09:34:57 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.1271
2022-02-21 09:35:39 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.0814
2022-02-21 09:36:21 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.3024
2022-02-21 09:37:03 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.3607
2022-02-21 09:37:45 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.2793
2022-02-21 09:38:27 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.1106
2022-02-21 09:39:09 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.4679
2022-02-21 09:39:51 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.0931
2022-02-21 09:40:33 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.2783
2022-02-21 09:41:14 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.4326
2022-02-21 09:41:56 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.2691
2022-02-21 09:42:38 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.1962
2022-02-21 09:43:20 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.1146
2022-02-21 09:44:02 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.2550
2022-02-21 09:44:44 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.2198
2022-02-21 09:45:26 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.1678
2022-02-21 09:46:08 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.2307
2022-02-21 09:46:50 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.2071
2022-02-21 09:46:53 - train: epoch 037, train_loss: 1.2105
2022-02-21 09:48:09 - eval: epoch: 037, acc1: 73.526%, acc5: 91.682%, test_loss: 1.0445, per_image_load_time: 2.022ms, per_image_inference_time: 0.858ms
2022-02-21 09:48:11 - until epoch: 037, best_acc1: 73.526%
2022-02-21 09:48:11 - epoch 038 lr: 0.010000000000000002
2022-02-21 09:48:58 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.2160
2022-02-21 09:49:39 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 0.9814
2022-02-21 09:50:21 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 0.8492
2022-02-21 09:51:03 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.0803
2022-02-21 09:51:45 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.1004
2022-02-21 09:52:27 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.2641
2022-02-21 09:53:09 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.0749
2022-02-21 09:53:51 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.0663
2022-02-21 09:54:33 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.1707
2022-02-21 09:55:15 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.3630
2022-02-21 09:55:57 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.1758
2022-02-21 09:56:39 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.1981
2022-02-21 09:57:21 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.2470
2022-02-21 09:58:03 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.2930
2022-02-21 09:58:45 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.2435
2022-02-21 09:59:27 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.2544
2022-02-21 10:00:09 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.3322
2022-02-21 10:00:51 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.2379
2022-02-21 10:01:33 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.2033
2022-02-21 10:02:15 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.2017
2022-02-21 10:02:57 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.2676
2022-02-21 10:03:39 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.1258
2022-02-21 10:04:21 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.2006
2022-02-21 10:05:03 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.4558
2022-02-21 10:05:45 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.1198
2022-02-21 10:06:27 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.2801
2022-02-21 10:07:08 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.3860
2022-02-21 10:07:50 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.3249
2022-02-21 10:08:33 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.3976
2022-02-21 10:09:15 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.2559
2022-02-21 10:09:57 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.1785
2022-02-21 10:10:39 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 0.9190
2022-02-21 10:11:21 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.0068
2022-02-21 10:12:03 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.1767
2022-02-21 10:12:44 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.3148
2022-02-21 10:13:26 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.2766
2022-02-21 10:14:08 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.0401
2022-02-21 10:14:50 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.2921
2022-02-21 10:15:32 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.2259
2022-02-21 10:16:14 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.1024
2022-02-21 10:16:56 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.1283
2022-02-21 10:17:38 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.0702
2022-02-21 10:18:20 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.3168
2022-02-21 10:19:02 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.2885
2022-02-21 10:19:45 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.2928
2022-02-21 10:20:27 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.3207
2022-02-21 10:21:09 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.0563
2022-02-21 10:21:50 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.2792
2022-02-21 10:22:33 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.2482
2022-02-21 10:23:15 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.0148
2022-02-21 10:23:18 - train: epoch 038, train_loss: 1.2030
2022-02-21 10:24:34 - eval: epoch: 038, acc1: 73.132%, acc5: 91.550%, test_loss: 1.0573, per_image_load_time: 2.030ms, per_image_inference_time: 0.843ms
2022-02-21 10:24:35 - until epoch: 038, best_acc1: 73.526%
2022-02-21 10:24:35 - epoch 039 lr: 0.010000000000000002
2022-02-21 10:25:22 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.2938
2022-02-21 10:26:04 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.4181
2022-02-21 10:26:46 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.0674
2022-02-21 10:27:28 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.1942
2022-02-21 10:28:10 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.1500
2022-02-21 10:28:52 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.0636
2022-02-21 10:29:34 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.4138
2022-02-21 10:30:16 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.1194
2022-02-21 10:30:58 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.2765
2022-02-21 10:31:40 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.1610
2022-02-21 10:32:22 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.1768
2022-02-21 10:33:04 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.1449
2022-02-21 10:33:46 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.3784
2022-02-21 10:34:28 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.2202
2022-02-21 10:35:10 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.1673
2022-02-21 10:35:52 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.4370
2022-02-21 10:36:34 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.0358
2022-02-21 10:37:16 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.2175
2022-02-21 10:37:57 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 0.9608
2022-02-21 10:38:39 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.0777
2022-02-21 10:39:21 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.1601
2022-02-21 10:40:03 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.1096
2022-02-21 10:40:45 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.3221
2022-02-21 10:41:27 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.3629
2022-02-21 10:42:09 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.0911
2022-02-21 10:42:51 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.2020
2022-02-21 10:43:33 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.3838
2022-02-21 10:44:15 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.0817
2022-02-21 10:44:57 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.0761
2022-02-21 10:45:38 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.2779
2022-02-21 10:46:21 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.1238
2022-02-21 10:47:02 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.1877
2022-02-21 10:47:44 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.2749
2022-02-21 10:48:26 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.2922
2022-02-21 10:49:08 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.4062
2022-02-21 10:49:50 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.3685
2022-02-21 10:50:32 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.2031
2022-02-21 10:51:14 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.1178
2022-02-21 10:51:56 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.2697
2022-02-21 10:52:38 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.2796
2022-02-21 10:53:20 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.2472
2022-02-21 10:54:01 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.1327
2022-02-21 10:54:43 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.2000
2022-02-21 10:55:25 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 0.9528
2022-02-21 10:56:07 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.1539
2022-02-21 10:56:49 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.4057
2022-02-21 10:57:31 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.2307
2022-02-21 10:58:13 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.2634
2022-02-21 10:58:55 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.1131
2022-02-21 10:59:37 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.1445
2022-02-21 10:59:40 - train: epoch 039, train_loss: 1.2004
2022-02-21 11:00:56 - eval: epoch: 039, acc1: 73.074%, acc5: 91.716%, test_loss: 1.0547, per_image_load_time: 1.992ms, per_image_inference_time: 0.877ms
2022-02-21 11:00:58 - until epoch: 039, best_acc1: 73.526%
2022-02-21 11:00:58 - epoch 040 lr: 0.010000000000000002
2022-02-21 11:01:45 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.3317
2022-02-21 11:02:27 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.4457
2022-02-21 11:03:09 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.3163
2022-02-21 11:03:51 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.1014
2022-02-21 11:04:33 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.1022
2022-02-21 11:05:15 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.2476
2022-02-21 11:05:57 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.2369
2022-02-21 11:06:39 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.3295
2022-02-21 11:07:21 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.1460
