2022-02-21 11:08:02 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 0.9102
2022-02-21 11:08:44 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.0737
2022-02-21 11:09:26 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.1382
2022-02-21 11:10:08 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.0760
2022-02-21 11:10:50 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.1094
2022-02-21 11:11:32 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.3506
2022-02-21 11:12:14 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.2532
2022-02-21 11:12:56 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.2242
2022-02-21 11:13:38 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.0400
2022-02-21 11:14:20 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.1548
2022-02-21 11:15:02 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.2472
2022-02-21 11:15:43 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.1376
2022-02-21 11:16:25 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.0411
2022-02-21 11:17:07 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.1258
2022-02-21 11:17:49 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.2224
2022-02-21 11:18:31 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.2371
2022-02-21 11:19:13 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.0716
2022-02-21 11:19:55 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.2875
2022-02-21 11:20:37 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.1411
2022-02-21 11:21:19 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.4475
2022-02-21 11:22:00 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.3445
2022-02-21 11:22:43 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.0487
2022-02-21 11:23:25 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.3206
2022-02-21 11:24:07 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.2738
2022-02-21 11:24:49 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.2644
2022-02-21 11:25:30 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.4190
2022-02-21 11:26:12 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.2552
2022-02-21 11:26:54 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.2556
2022-02-21 11:27:36 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.1185
2022-02-21 11:28:18 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.2559
2022-02-21 11:29:00 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.3467
2022-02-21 11:29:42 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.2444
2022-02-21 11:30:24 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.1078
2022-02-21 11:31:06 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.2818
2022-02-21 11:31:47 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.2175
2022-02-21 11:32:29 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.0042
2022-02-21 11:33:11 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.1607
2022-02-21 11:33:53 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.1872
2022-02-21 11:34:35 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.1025
2022-02-21 11:35:17 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.3267
2022-02-21 11:35:59 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.3147
2022-02-21 11:36:03 - train: epoch 040, train_loss: 1.1976
2022-02-21 11:37:18 - eval: epoch: 040, acc1: 72.596%, acc5: 91.328%, test_loss: 1.0764, per_image_load_time: 1.876ms, per_image_inference_time: 0.871ms
2022-02-21 11:37:20 - until epoch: 040, best_acc1: 73.526%
2022-02-21 11:37:20 - epoch 041 lr: 0.010000000000000002
2022-02-21 11:38:07 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.3081
2022-02-21 11:38:49 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.2381
2022-02-21 11:39:31 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.1688
2022-02-21 11:40:13 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.2348
2022-02-21 11:40:55 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.0557
2022-02-21 11:41:37 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.2156
2022-02-21 11:42:19 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.0991
2022-02-21 11:43:01 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 0.9111
2022-02-21 11:43:43 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 0.9346
2022-02-21 11:44:25 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.3616
2022-02-21 11:45:07 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.0365
2022-02-21 11:45:49 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.0090
2022-02-21 11:46:31 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.1782
2022-02-21 11:47:13 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.3618
2022-02-21 11:47:55 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.4757
2022-02-21 11:48:37 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.0413
2022-02-21 11:49:18 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.2407
2022-02-21 11:50:00 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.2690
2022-02-21 11:50:42 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.1997
2022-02-21 11:51:24 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.1429
2022-02-21 11:52:06 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.0358
2022-02-21 11:52:48 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.1351
2022-02-21 11:53:30 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 0.9875
2022-02-21 11:54:12 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.3677
2022-02-21 11:54:54 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.1889
2022-02-21 11:55:36 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.3279
2022-02-21 11:56:18 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.3795
2022-02-21 11:57:00 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.2049
2022-02-21 11:57:42 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.2589
2022-02-21 11:58:23 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.2138
2022-02-21 11:59:05 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.3380
2022-02-21 11:59:47 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.0380
2022-02-21 12:00:29 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.0734
2022-02-21 12:01:11 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.0491
2022-02-21 12:01:53 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.2446
2022-02-21 12:02:35 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.3087
2022-02-21 12:03:17 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.2613
2022-02-21 12:03:58 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.0657
2022-02-21 12:04:40 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.2600
2022-02-21 12:05:22 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.0975
2022-02-21 12:06:04 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.2144
2022-02-21 12:06:46 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.0760
2022-02-21 12:07:28 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.2527
2022-02-21 12:08:10 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.0934
2022-02-21 12:08:52 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.2487
2022-02-21 12:09:34 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.2502
2022-02-21 12:10:16 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.4170
2022-02-21 12:10:58 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.1563
2022-02-21 12:11:40 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.1620
2022-02-21 12:12:22 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.2882
2022-02-21 12:12:26 - train: epoch 041, train_loss: 1.1987
2022-02-21 12:13:42 - eval: epoch: 041, acc1: 71.846%, acc5: 90.862%, test_loss: 1.1169, per_image_load_time: 1.996ms, per_image_inference_time: 0.885ms
2022-02-21 12:13:43 - until epoch: 041, best_acc1: 73.526%
2022-02-21 12:13:43 - epoch 042 lr: 0.010000000000000002
2022-02-21 12:14:31 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 0.9016
2022-02-21 12:15:12 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.1871
2022-02-21 12:15:55 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.1957
2022-02-21 12:16:37 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 0.9566
2022-02-21 12:17:19 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.2439
2022-02-21 12:18:01 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.0585
2022-02-21 12:18:43 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.1159
2022-02-21 12:19:25 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.0981
2022-02-21 12:20:07 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.3102
2022-02-21 12:20:49 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.2802
2022-02-21 12:21:31 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.0030
2022-02-21 12:22:13 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.2638
2022-02-21 12:22:55 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.2598
2022-02-21 12:23:37 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.5277
2022-02-21 12:24:19 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.1811
2022-02-21 12:25:01 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.2610
2022-02-21 12:25:43 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.1735
2022-02-21 12:26:25 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.0670
2022-02-21 12:27:07 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.3753
2022-02-21 12:27:49 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.0783
2022-02-21 12:28:31 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.0045
2022-02-21 12:29:13 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.0662
2022-02-21 12:29:55 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.1513
2022-02-21 12:30:37 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.3701
2022-02-21 12:31:19 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.3242
2022-02-21 12:32:01 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.2576
2022-02-21 12:32:43 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.0709
2022-02-21 12:33:25 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.1919
2022-02-21 12:34:07 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.1455
2022-02-21 12:34:48 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.2099
2022-02-21 12:35:30 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.1162
2022-02-21 12:36:12 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.2000
2022-02-21 12:36:54 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.3069
2022-02-21 12:37:36 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.1446
2022-02-21 12:38:18 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.2030
2022-02-21 12:39:00 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.3303
2022-02-21 12:39:42 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.1751
2022-02-21 12:40:24 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.0681
2022-02-21 12:41:06 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.1296
2022-02-21 12:41:48 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.3086
2022-02-21 12:42:29 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.3185
2022-02-21 12:43:11 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.2768
2022-02-21 12:43:53 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.0826
2022-02-21 12:44:35 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.1111
2022-02-21 12:45:17 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.2176
2022-02-21 12:45:59 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.4296
2022-02-21 12:46:41 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.1519
2022-02-21 12:47:22 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.2848
2022-02-21 12:48:04 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.2497
2022-02-21 12:48:46 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.1732
2022-02-21 12:48:50 - train: epoch 042, train_loss: 1.1985
2022-02-21 12:50:05 - eval: epoch: 042, acc1: 72.994%, acc5: 91.574%, test_loss: 1.0713, per_image_load_time: 1.780ms, per_image_inference_time: 0.867ms
2022-02-21 12:50:07 - until epoch: 042, best_acc1: 73.526%
2022-02-21 12:50:07 - epoch 043 lr: 0.010000000000000002
2022-02-21 12:50:54 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.1572
2022-02-21 12:51:36 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.2440
2022-02-21 12:52:18 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 0.9919
2022-02-21 12:53:00 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.1225
2022-02-21 12:53:42 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.2221
2022-02-21 12:54:23 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.1496
2022-02-21 12:55:05 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.3000
2022-02-21 12:55:47 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.3868
2022-02-21 12:56:29 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.1880
2022-02-21 12:57:11 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.3129
2022-02-21 12:57:52 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.2686
2022-02-21 12:58:34 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.3730
2022-02-21 12:59:16 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.2731
2022-02-21 12:59:58 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.2122
2022-02-21 13:00:40 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.0686
2022-02-21 13:01:22 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.2419
2022-02-21 13:02:04 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.2611
2022-02-21 13:02:46 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.2220
2022-02-21 13:03:28 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.2658
2022-02-21 13:04:09 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 0.9887
2022-02-21 13:04:51 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.1882
2022-02-21 13:05:33 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.2320
2022-02-21 13:06:15 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.3419
2022-02-21 13:06:56 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.0746
2022-02-21 13:07:38 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.3694
2022-02-21 13:08:20 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.0132
2022-02-21 13:09:02 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.2359
2022-02-21 13:09:44 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.1395
2022-02-21 13:10:26 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.2240
2022-02-21 13:11:07 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.4513
2022-02-21 13:11:50 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.3996
2022-02-21 13:12:32 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.2662
2022-02-21 13:13:13 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.1754
2022-02-21 13:13:55 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.2629
2022-02-21 13:14:37 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.1739
2022-02-21 13:15:19 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.1926
2022-02-21 13:16:01 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.2065
2022-02-21 13:16:43 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.3898
2022-02-21 13:17:24 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.2397
2022-02-21 13:18:06 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.2190
2022-02-21 13:18:48 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.5043
2022-02-21 13:19:30 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.2362
2022-02-21 13:20:12 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.2036
2022-02-21 13:20:54 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.0846
2022-02-21 13:21:36 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.2893
2022-02-21 13:22:18 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.2714
2022-02-21 13:23:00 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.1412
2022-02-21 13:23:41 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.1088
2022-02-21 13:24:23 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.1962
2022-02-21 13:25:05 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.1124
2022-02-21 13:25:09 - train: epoch 043, train_loss: 1.1958
2022-02-21 13:26:25 - eval: epoch: 043, acc1: 72.792%, acc5: 91.462%, test_loss: 1.0762, per_image_load_time: 1.654ms, per_image_inference_time: 0.851ms
2022-02-21 13:26:26 - until epoch: 043, best_acc1: 73.526%
2022-02-21 13:26:26 - epoch 044 lr: 0.010000000000000002
2022-02-21 13:27:13 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.2413
2022-02-21 13:27:55 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.2780
2022-02-21 13:28:37 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.2649
2022-02-21 13:29:19 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 0.9245
2022-02-21 13:30:01 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.1221
2022-02-21 13:30:43 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 0.9887
2022-02-21 13:31:25 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 0.9503
2022-02-21 13:32:06 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.1784
2022-02-21 13:32:48 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.1621
2022-02-21 13:33:30 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.2335
2022-02-21 13:34:12 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.1259
2022-02-21 13:34:53 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.0815
2022-02-21 13:35:35 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.2770
2022-02-21 13:36:17 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.0893
2022-02-21 13:36:59 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.1123
2022-02-21 13:37:41 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.0701
2022-02-21 13:38:23 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.1667
2022-02-21 13:39:04 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.0373
2022-02-21 13:39:46 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.3215
2022-02-21 13:40:28 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.1590
2022-02-21 13:41:10 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.3045
2022-02-21 13:41:52 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.1241
2022-02-21 13:42:34 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.2952
2022-02-21 13:43:15 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.2570
2022-02-21 13:43:57 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.2386
2022-02-21 13:44:39 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.2445
2022-02-21 13:45:21 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.3395
2022-02-21 13:46:03 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 0.9805
2022-02-21 13:46:44 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.0506
2022-02-21 13:47:26 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.1563
2022-02-21 13:48:08 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.2921
2022-02-21 13:48:50 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 0.9627
2022-02-21 13:49:32 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.0882
2022-02-21 13:50:14 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.3959
2022-02-21 13:50:56 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.0919
2022-02-21 13:51:38 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.3569
2022-02-21 13:52:20 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.3143
2022-02-21 13:53:03 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 0.9742
2022-02-21 13:53:45 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.3457
2022-02-21 13:54:27 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.3357
2022-02-21 13:55:09 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.1219
2022-02-21 13:55:52 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.1451
2022-02-21 13:56:34 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.3795
2022-02-21 13:57:16 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.0709
2022-02-21 13:57:58 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.2077
2022-02-21 13:58:40 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.1482
2022-02-21 13:59:23 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.2398
2022-02-21 14:00:05 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.3370
2022-02-21 14:00:47 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 0.9679
2022-02-21 14:01:29 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.1550
2022-02-21 14:01:33 - train: epoch 044, train_loss: 1.1976
2022-02-21 14:02:49 - eval: epoch: 044, acc1: 72.730%, acc5: 91.368%, test_loss: 1.0837, per_image_load_time: 2.034ms, per_image_inference_time: 0.877ms
2022-02-21 14:02:50 - until epoch: 044, best_acc1: 73.526%
2022-02-21 14:02:50 - epoch 045 lr: 0.010000000000000002
2022-02-21 14:03:38 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.0296
2022-02-21 14:04:20 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.1792
2022-02-21 14:05:03 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.2655
2022-02-21 14:05:45 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.0777
2022-02-21 14:06:27 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.2669
2022-02-21 14:07:09 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.1955
2022-02-21 14:07:52 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 0.9385
2022-02-21 14:08:34 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.0450
2022-02-21 14:09:16 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.1958
2022-02-21 14:09:59 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.1590
2022-02-21 14:10:41 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.2911
2022-02-21 14:11:23 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.3255
2022-02-21 14:12:06 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.3627
2022-02-21 14:12:48 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.0979
2022-02-21 14:13:31 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.3103
2022-02-21 14:14:13 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.1997
2022-02-21 14:14:55 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.0656
2022-02-21 14:15:38 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.0489
2022-02-21 14:16:20 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.2278
2022-02-21 14:17:03 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.1990
2022-02-21 14:17:45 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.2795
2022-02-21 14:18:28 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.2979
2022-02-21 14:19:10 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.1621
2022-02-21 14:19:53 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.3975
2022-02-21 14:20:35 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.3014
2022-02-21 14:21:17 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.1699
2022-02-21 14:22:00 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.1284
2022-02-21 14:22:42 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.1871
2022-02-21 14:23:24 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.2408
2022-02-21 14:24:06 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.3184
2022-02-21 14:24:48 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.2306
2022-02-21 14:25:29 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.3030
2022-02-21 14:26:11 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.2013
2022-02-21 14:26:52 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.0438
2022-02-21 14:27:34 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.4194
2022-02-21 14:28:15 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.3970
2022-02-21 14:28:57 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.1455
2022-02-21 14:29:39 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.1389
2022-02-21 14:30:21 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.2549
2022-02-21 14:31:03 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.2625
2022-02-21 14:31:45 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.1415
2022-02-21 14:32:26 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.3306
2022-02-21 14:33:08 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.3886
2022-02-21 14:33:50 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.3108
2022-02-21 14:34:32 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.1561
2022-02-21 14:35:14 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.3289
2022-02-21 14:35:56 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.3009
2022-02-21 14:36:38 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.0919
2022-02-21 14:37:20 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.1747
2022-02-21 14:38:02 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.2238
2022-02-21 14:38:06 - train: epoch 045, train_loss: 1.1941
2022-02-21 14:39:21 - eval: epoch: 045, acc1: 72.778%, acc5: 91.406%, test_loss: 1.0771, per_image_load_time: 1.999ms, per_image_inference_time: 0.889ms
2022-02-21 14:39:22 - until epoch: 045, best_acc1: 73.526%
2022-02-21 14:39:22 - epoch 046 lr: 0.010000000000000002
2022-02-21 14:40:10 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.0228
2022-02-21 14:40:52 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.1074
2022-02-21 14:41:34 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.2384
2022-02-21 14:42:16 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.3001
2022-02-21 14:42:58 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 0.9304
2022-02-21 14:43:39 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.2844
2022-02-21 14:44:21 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.1600
2022-02-21 14:45:03 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.1937
2022-02-21 14:45:45 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.2288
2022-02-21 14:46:27 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.0189
2022-02-21 14:47:09 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.2010
2022-02-21 14:47:51 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.2039
2022-02-21 14:48:33 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.2140
2022-02-21 14:49:14 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.2735
2022-02-21 14:49:56 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.1665
2022-02-21 14:50:38 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.2249
2022-02-21 14:51:20 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.0280
2022-02-21 14:52:02 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.2336
2022-02-21 14:52:44 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 0.9741
2022-02-21 14:53:26 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.2032
2022-02-21 14:54:08 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.2977
2022-02-21 14:54:49 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 0.9839
2022-02-21 14:55:31 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.3827
2022-02-21 14:56:13 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.2086
2022-02-21 14:56:55 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.3499
2022-02-21 14:57:37 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.3098
2022-02-21 14:58:19 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.0636
2022-02-21 14:59:01 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.0039
2022-02-21 14:59:43 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.2822
2022-02-21 15:00:25 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.0785
2022-02-21 15:01:07 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.0639
2022-02-21 15:01:49 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.4786
2022-02-21 15:02:31 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.2564
2022-02-21 15:03:13 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.2195
2022-02-21 15:03:55 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.1204
2022-02-21 15:04:37 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.0520
2022-02-21 15:05:19 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.0922
2022-02-21 15:06:01 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.3271
2022-02-21 15:06:43 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.2631
2022-02-21 15:07:24 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.0809
2022-02-21 15:08:06 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.3376
2022-02-21 15:08:49 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.1803
2022-02-21 15:09:30 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.3330
2022-02-21 15:10:13 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.1609
2022-02-21 15:10:54 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.0775
2022-02-21 15:11:36 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.3204
2022-02-21 15:12:18 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.1420
2022-02-21 15:13:00 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.2218
2022-02-21 15:13:42 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.2190
2022-02-21 15:14:23 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.2115
2022-02-21 15:14:27 - train: epoch 046, train_loss: 1.1934
2022-02-21 15:15:43 - eval: epoch: 046, acc1: 72.372%, acc5: 91.144%, test_loss: 1.0958, per_image_load_time: 1.999ms, per_image_inference_time: 0.859ms
2022-02-21 15:15:44 - until epoch: 046, best_acc1: 73.526%
2022-02-21 15:15:44 - epoch 047 lr: 0.010000000000000002
2022-02-21 15:16:31 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.1863
2022-02-21 15:17:13 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.2443
2022-02-21 15:17:55 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.0885
2022-02-21 15:18:37 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.1602
2022-02-21 15:19:19 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.3147
2022-02-21 15:20:01 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.0681
2022-02-21 15:20:43 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.3368
2022-02-21 15:21:25 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.0472
2022-02-21 15:22:06 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.3483
2022-02-21 15:22:48 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.2600
2022-02-21 15:23:30 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.2543
2022-02-21 15:24:12 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.1108
2022-02-21 15:24:53 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.1774
2022-02-21 15:25:35 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.1975
2022-02-21 15:26:17 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.0234
2022-02-21 15:26:59 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.1427
2022-02-21 15:27:41 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.0263
2022-02-21 15:28:22 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.1790
2022-02-21 15:29:04 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.0772
2022-02-21 15:29:46 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.2147
2022-02-21 15:30:28 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.3350
2022-02-21 15:31:10 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.3442
2022-02-21 15:31:52 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.2357
2022-02-21 15:32:33 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.0345
2022-02-21 15:33:15 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.2793
2022-02-21 15:33:57 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.4279
2022-02-21 15:34:39 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.2625
2022-02-21 15:35:21 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.3737
2022-02-21 15:36:03 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.1704
2022-02-21 15:36:45 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.2416
2022-02-21 15:37:26 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.1608
2022-02-21 15:38:08 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.4340
2022-02-21 15:38:50 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.1567
2022-02-21 15:39:32 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.1586
2022-02-21 15:40:14 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.3160
2022-02-21 15:40:56 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.2857
2022-02-21 15:41:37 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.3077
2022-02-21 15:42:18 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.2889
2022-02-21 15:42:58 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.1249
2022-02-21 15:43:39 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.2970
2022-02-21 15:44:20 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.2099
2022-02-21 15:45:01 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.1521
2022-02-21 15:45:42 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.0654
2022-02-21 15:46:23 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.1418
2022-02-21 15:47:04 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.3506
2022-02-21 15:47:45 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.2086
2022-02-21 15:48:26 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.3305
2022-02-21 15:49:07 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.1607
2022-02-21 15:49:49 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.1997
2022-02-21 15:50:30 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.1591
2022-02-21 15:50:34 - train: epoch 047, train_loss: 1.1938
2022-02-21 15:51:45 - eval: epoch: 047, acc1: 72.076%, acc5: 91.132%, test_loss: 1.0989, per_image_load_time: 1.776ms, per_image_inference_time: 0.847ms
2022-02-21 15:51:46 - until epoch: 047, best_acc1: 73.526%
2022-02-21 15:51:46 - epoch 048 lr: 0.010000000000000002
2022-02-21 15:52:33 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.3339
2022-02-21 15:53:14 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.4688
2022-02-21 15:53:55 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.0925
2022-02-21 15:54:36 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.2027
2022-02-21 15:55:18 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.0504
2022-02-21 15:55:59 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.1940
2022-02-21 15:56:41 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.1583
2022-02-21 15:57:22 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.2184
2022-02-21 15:58:04 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.2130
2022-02-21 15:58:46 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.1945
2022-02-21 15:59:27 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.1737
2022-02-21 16:00:09 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.3068
2022-02-21 16:00:51 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.0918
2022-02-21 16:01:33 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.1326
2022-02-21 16:02:15 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.2203
2022-02-21 16:02:58 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.1258
2022-02-21 16:03:41 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.2775
2022-02-21 16:04:24 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.2034
2022-02-21 16:05:07 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.2614
2022-02-21 16:05:50 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.4572
2022-02-21 16:06:32 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.2487
2022-02-21 16:07:14 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.3327
2022-02-21 16:07:54 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.0654
2022-02-21 16:08:35 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.2583
2022-02-21 16:09:16 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.0877
2022-02-21 16:09:57 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.4653
2022-02-21 16:10:38 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.2945
2022-02-21 16:11:19 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.1452
2022-02-21 16:12:00 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.3465
2022-02-21 16:12:41 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.1916
2022-02-21 16:13:23 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.0828
2022-02-21 16:14:06 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.1115
2022-02-21 16:14:50 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.3164
2022-02-21 16:15:33 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.1503
2022-02-21 16:16:16 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.4595
2022-02-21 16:16:59 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.3305
2022-02-21 16:17:42 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.2898
2022-02-21 16:18:26 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.1514
2022-02-21 16:19:09 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.3676
2022-02-21 16:19:52 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.0205
2022-02-21 16:20:35 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.3990
2022-02-21 16:21:19 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.1067
2022-02-21 16:22:00 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.2107
2022-02-21 16:22:42 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.0830
2022-02-21 16:23:25 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.2967
2022-02-21 16:24:08 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.0587
2022-02-21 16:24:51 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.1795
2022-02-21 16:25:35 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.4780
2022-02-21 16:26:19 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.2828
2022-02-21 16:27:06 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.2124
2022-02-21 16:27:09 - train: epoch 048, train_loss: 1.1935
2022-02-21 16:28:36 - eval: epoch: 048, acc1: 72.928%, acc5: 91.398%, test_loss: 1.0693, per_image_load_time: 1.763ms, per_image_inference_time: 0.902ms
2022-02-21 16:28:37 - until epoch: 048, best_acc1: 73.526%
2022-02-21 16:28:37 - epoch 049 lr: 0.010000000000000002
2022-02-21 16:29:25 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.3722
2022-02-21 16:30:08 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.2255
2022-02-21 16:30:51 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.2323
2022-02-21 16:31:33 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.2590
2022-02-21 16:32:16 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.1733
2022-02-21 16:32:58 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.1385
2022-02-21 16:33:42 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.2733
2022-02-21 16:34:25 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.4151
2022-02-21 16:35:08 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.0991
2022-02-21 16:35:50 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.1987
2022-02-21 16:36:33 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.1246
2022-02-21 16:37:16 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.0250
2022-02-21 16:37:58 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.2092
2022-02-21 16:38:41 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.3478
2022-02-21 16:39:23 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.1864
2022-02-21 16:40:06 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.3783
2022-02-21 16:40:49 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.2331
2022-02-21 16:41:32 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.0171
2022-02-21 16:42:15 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.1965
2022-02-21 16:42:57 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.0755
2022-02-21 16:43:40 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.0552
2022-02-21 16:44:23 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.2024
2022-02-21 16:45:05 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.1458
2022-02-21 16:45:48 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.2955
2022-02-21 16:46:31 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.2009
2022-02-21 16:47:13 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.2439
2022-02-21 16:47:56 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.1351
2022-02-21 16:48:39 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.1488
2022-02-21 16:49:22 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.3100
2022-02-21 16:50:05 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.2099
2022-02-21 16:50:48 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.2627
2022-02-21 16:51:31 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.2567
2022-02-21 16:52:14 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.1782
2022-02-21 16:52:56 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.3411
2022-02-21 16:53:39 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.2316
2022-02-21 16:54:22 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.3120
2022-02-21 16:55:05 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.2395
2022-02-21 16:55:48 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.3456
2022-02-21 16:56:31 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.2830
2022-02-21 16:57:14 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.1755
2022-02-21 16:57:56 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.1343
2022-02-21 16:58:39 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.2109
2022-02-21 16:59:22 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.3342
2022-02-21 17:00:05 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.2070
2022-02-21 17:00:48 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.1165
2022-02-21 17:01:40 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.3192
2022-02-21 17:02:23 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.2809
2022-02-21 17:03:06 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 0.9782
2022-02-21 17:03:53 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.0563
2022-02-21 17:04:38 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.1772
2022-02-21 17:04:42 - train: epoch 049, train_loss: 1.1890
2022-02-21 17:06:06 - eval: epoch: 049, acc1: 72.268%, acc5: 90.994%, test_loss: 1.1022, per_image_load_time: 2.257ms, per_image_inference_time: 0.906ms
2022-02-21 17:06:08 - until epoch: 049, best_acc1: 73.526%
2022-02-21 17:06:08 - epoch 050 lr: 0.010000000000000002
2022-02-21 17:06:56 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.3785
2022-02-21 17:07:39 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.1083
2022-02-21 17:08:21 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.1568
2022-02-21 17:09:04 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.1309
2022-02-21 17:09:47 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.2375
2022-02-21 17:10:30 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.2387
2022-02-21 17:11:13 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.0230
2022-02-21 17:11:56 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 0.9991
2022-02-21 17:12:39 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 0.9494
2022-02-21 17:13:21 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.2457
2022-02-21 17:14:04 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.2387
2022-02-21 17:14:47 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.1352
2022-02-21 17:15:30 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.0545
2022-02-21 17:16:12 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.1742
2022-02-21 17:16:55 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.1596
2022-02-21 17:17:38 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.1518
2022-02-21 17:18:21 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.2134
2022-02-21 17:19:04 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.1005
2022-02-21 17:19:46 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.1906
2022-02-21 17:20:29 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.1752
2022-02-21 17:21:12 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.0615
2022-02-21 17:21:55 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.3321
2022-02-21 17:22:38 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.0717
2022-02-21 17:23:21 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.0952
2022-02-21 17:24:03 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.2910
2022-02-21 17:24:46 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.0781
2022-02-21 17:25:29 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.2155
2022-02-21 17:26:12 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.3208
2022-02-21 17:26:54 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.3854
2022-02-21 17:27:37 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.3585
2022-02-21 17:28:20 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.1405
2022-02-21 17:29:03 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.1838
2022-02-21 17:29:46 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.0970
2022-02-21 17:30:28 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.1246
2022-02-21 17:31:11 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.1789
2022-02-21 17:31:54 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.1168
2022-02-21 17:32:37 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.2200
2022-02-21 17:33:20 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.0845
2022-02-21 17:34:02 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.0902
2022-02-21 17:34:45 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.3350
2022-02-21 17:35:28 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.0961
2022-02-21 17:36:11 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.2016
2022-02-21 17:36:54 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.1190
2022-02-21 17:37:37 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.0674
2022-02-21 17:38:20 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.1120
2022-02-21 17:39:04 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.2050
2022-02-21 17:39:47 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.1618
2022-02-21 17:40:31 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.0389
2022-02-21 17:41:16 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.1057
2022-02-21 17:42:01 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.1265
2022-02-21 17:42:05 - train: epoch 050, train_loss: 1.1867
2022-02-21 17:43:34 - eval: epoch: 050, acc1: 72.002%, acc5: 90.922%, test_loss: 1.1145, per_image_load_time: 1.451ms, per_image_inference_time: 0.899ms
2022-02-21 17:43:35 - until epoch: 050, best_acc1: 73.526%
2022-02-21 17:43:35 - epoch 051 lr: 0.010000000000000002
2022-02-21 17:44:23 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.2178
2022-02-21 17:45:06 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.4190
2022-02-21 17:45:48 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.2083
2022-02-21 17:46:31 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 0.9875
2022-02-21 17:47:14 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.2351
2022-02-21 17:47:57 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.2019
2022-02-21 17:48:40 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.0999
2022-02-21 17:49:23 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.3637
2022-02-21 17:50:05 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.0635
2022-02-21 17:50:48 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.4296
2022-02-21 17:51:32 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.2473
2022-02-21 17:52:15 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.0779
2022-02-21 17:52:58 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 0.8429
2022-02-21 17:53:40 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.0255
2022-02-21 17:54:23 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.1132
2022-02-21 17:55:06 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.1319
2022-02-21 17:55:50 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.2218
2022-02-21 17:56:32 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.2096
2022-02-21 17:57:15 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 0.9866
2022-02-21 17:57:58 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.1462
2022-02-21 17:58:41 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.1857
2022-02-21 17:59:23 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.2633
2022-02-21 18:00:06 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.1399
2022-02-21 18:00:49 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.3157
2022-02-21 18:01:31 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.0645
2022-02-21 18:02:14 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.0633
2022-02-21 18:02:56 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.2423
2022-02-21 18:03:39 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.0905
2022-02-21 18:04:22 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.2519
2022-02-21 18:05:05 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.0825
2022-02-21 18:05:47 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.0556
2022-02-21 18:06:30 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.1547
2022-02-21 18:07:13 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.3203
2022-02-21 18:07:55 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.2649
2022-02-21 18:08:38 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.1928
2022-02-21 18:09:21 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.1635
2022-02-21 18:10:03 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.3106
2022-02-21 18:10:46 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.1574
2022-02-21 18:11:29 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.2569
2022-02-21 18:12:12 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.0805
2022-02-21 18:12:55 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.2973
2022-02-21 18:13:38 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.4296
2022-02-21 18:14:21 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.1485
2022-02-21 18:15:04 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.2526
2022-02-21 18:15:47 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.0603
2022-02-21 18:16:30 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.3027
2022-02-21 18:17:14 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.2354
2022-02-21 18:17:58 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.1167
2022-02-21 18:18:42 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.2203
2022-02-21 18:19:29 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.1457
2022-02-21 18:19:32 - train: epoch 051, train_loss: 1.1861
2022-02-21 18:20:56 - eval: epoch: 051, acc1: 72.578%, acc5: 91.198%, test_loss: 1.0934, per_image_load_time: 1.529ms, per_image_inference_time: 0.948ms
2022-02-21 18:20:57 - until epoch: 051, best_acc1: 73.526%
2022-02-21 18:20:57 - epoch 052 lr: 0.010000000000000002
2022-02-21 18:21:46 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.0796
2022-02-21 18:22:29 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.1911
2022-02-21 18:23:12 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.2366
2022-02-21 18:23:55 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.2671
2022-02-21 18:24:38 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.3034
2022-02-21 18:25:20 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.0997
2022-02-21 18:26:03 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.2083
2022-02-21 18:26:46 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.2159
2022-02-21 18:27:28 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.3089
2022-02-21 18:28:11 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.3607
2022-02-21 18:28:54 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.0901
2022-02-21 18:29:37 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.0365
2022-02-21 18:30:19 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.1410
2022-02-21 18:31:02 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.4406
2022-02-21 18:31:45 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.0545
2022-02-21 18:32:27 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 0.9898
2022-02-21 18:33:10 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.0634
2022-02-21 18:33:53 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.0108
2022-02-21 18:34:36 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.2124
2022-02-21 18:35:19 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.2282
2022-02-21 18:36:01 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.0475
2022-02-21 18:36:44 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.2511
2022-02-21 18:37:27 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.0006
2022-02-21 18:38:09 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.0903
2022-02-21 18:38:52 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.1356
2022-02-21 18:39:35 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 0.9879
2022-02-21 18:40:18 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.1055
2022-02-21 18:41:01 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.1460
2022-02-21 18:41:44 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.0647
2022-02-21 18:42:27 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.1676
2022-02-21 18:43:10 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.1574
2022-02-21 18:43:53 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.3093
2022-02-21 18:44:36 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.2838
2022-02-21 18:45:19 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.2658
2022-02-21 18:46:02 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.3264
2022-02-21 18:46:46 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.2946
2022-02-21 18:47:29 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.3424
2022-02-21 18:48:12 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.1027
2022-02-21 18:48:54 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.1029
2022-02-21 18:49:37 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.2863
2022-02-21 18:50:20 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.0690
2022-02-21 18:51:03 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.1129
2022-02-21 18:51:46 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.2000
2022-02-21 18:52:29 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.0982
2022-02-21 18:53:12 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.0312
2022-02-21 18:53:55 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.3684
2022-02-21 18:54:39 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.2227
2022-02-21 18:55:23 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.0343
2022-02-21 18:56:08 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.0260
2022-02-21 18:56:54 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.1868
2022-02-21 18:56:58 - train: epoch 052, train_loss: 1.1815
2022-02-21 18:58:23 - eval: epoch: 052, acc1: 72.068%, acc5: 91.090%, test_loss: 1.1053, per_image_load_time: 2.373ms, per_image_inference_time: 0.858ms
2022-02-21 18:58:25 - until epoch: 052, best_acc1: 73.526%
2022-02-21 18:58:25 - epoch 053 lr: 0.010000000000000002
2022-02-21 18:59:13 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.1521
2022-02-21 18:59:56 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.3248
2022-02-21 19:00:39 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.1435
2022-02-21 19:01:21 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.2306
2022-02-21 19:02:04 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.3142
2022-02-21 19:02:47 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.0818
2022-02-21 19:03:29 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.1376
2022-02-21 19:04:11 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.3160
2022-02-21 19:04:54 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.1812
2022-02-21 19:05:37 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.1112
2022-02-21 19:06:20 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.1584
2022-02-21 19:07:03 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.0699
2022-02-21 19:07:46 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.2351
2022-02-21 19:08:29 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.3261
2022-02-21 19:09:12 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.1252
2022-02-21 19:09:55 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.4733
2022-02-21 19:10:37 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.4079
2022-02-21 19:11:20 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.2470
2022-02-21 19:12:03 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 0.9702
2022-02-21 19:12:45 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.1979
2022-02-21 19:13:28 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.1977
2022-02-21 19:14:11 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.0549
2022-02-21 19:14:54 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.1741
2022-02-21 19:15:36 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.2959
2022-02-21 19:16:19 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.4015
2022-02-21 19:17:02 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.2140
2022-02-21 19:17:45 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.3693
2022-02-21 19:18:28 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.3555
2022-02-21 19:19:11 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.0310
2022-02-21 19:19:54 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.0599
2022-02-21 19:20:37 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.3952
2022-02-21 19:21:19 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.3754
2022-02-21 19:22:02 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.1228
2022-02-21 19:22:45 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.3540
2022-02-21 19:23:28 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.2817
2022-02-21 19:24:11 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.3191
2022-02-21 19:24:54 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.3274
2022-02-21 19:25:37 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.2115
2022-02-21 19:26:20 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.3121
2022-02-21 19:27:03 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.2838
2022-02-21 19:27:46 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.3080
2022-02-21 19:28:29 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.2411
2022-02-21 19:29:12 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.3744
2022-02-21 19:29:55 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.2065
2022-02-21 19:30:39 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.2313
2022-02-21 19:31:22 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.0484
2022-02-21 19:32:06 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.3288
2022-02-21 19:32:51 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.3567
2022-02-21 19:33:36 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.0789
2022-02-21 19:34:23 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.1072
2022-02-21 19:34:26 - train: epoch 053, train_loss: 1.1806
2022-02-21 19:35:54 - eval: epoch: 053, acc1: 72.398%, acc5: 91.284%, test_loss: 1.0875, per_image_load_time: 2.369ms, per_image_inference_time: 0.873ms
2022-02-21 19:35:56 - until epoch: 053, best_acc1: 73.526%
2022-02-21 19:35:56 - epoch 054 lr: 0.010000000000000002
2022-02-21 19:36:44 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 0.9721
2022-02-21 19:37:27 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.4705
2022-02-21 19:38:10 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.1652
2022-02-21 19:38:53 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.0249
2022-02-21 19:39:35 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.2218
2022-02-21 19:40:18 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.0233
2022-02-21 19:41:01 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.2182
2022-02-21 19:41:44 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.2732
2022-02-21 19:42:26 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 0.9488
2022-02-21 19:43:10 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.0106
2022-02-21 19:43:53 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.0170
2022-02-21 19:44:35 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.2339
2022-02-21 19:45:18 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.1059
2022-02-21 19:46:01 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.1773
2022-02-21 19:46:44 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.3119
2022-02-21 19:47:26 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 0.8386
2022-02-21 19:48:09 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.1475
2022-02-21 19:48:52 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.2294
2022-02-21 19:49:35 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.4513
2022-02-21 19:50:17 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.2546
2022-02-21 19:51:00 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.0121
2022-02-21 19:51:43 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.1226
2022-02-21 19:52:26 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.2112
2022-02-21 19:53:09 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.0077
2022-02-21 19:53:52 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.2928
2022-02-21 19:54:34 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 0.9974
2022-02-21 19:55:17 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.2303
2022-02-21 19:56:00 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.4670
2022-02-21 19:56:43 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.0314
2022-02-21 19:57:26 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.2378
2022-02-21 19:58:09 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.1837
2022-02-21 19:58:52 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.3491
2022-02-21 19:59:35 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.1490
2022-02-21 20:00:18 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.2384
2022-02-21 20:01:01 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.2762
2022-02-21 20:01:44 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.0784
2022-02-21 20:02:27 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.1069
2022-02-21 20:03:10 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.1764
2022-02-21 20:03:53 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.2033
2022-02-21 20:04:36 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.0604
2022-02-21 20:05:19 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.2466
2022-02-21 20:06:02 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.1325
2022-02-21 20:06:45 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.1800
2022-02-21 20:07:28 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.0123
2022-02-21 20:08:11 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.0168
2022-02-21 20:08:54 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.2003
2022-02-21 20:09:38 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.3519
2022-02-21 20:10:22 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.4117
2022-02-21 20:11:06 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.0724
2022-02-21 20:11:51 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.2572
2022-02-21 20:11:55 - train: epoch 054, train_loss: 1.1804
2022-02-21 20:13:17 - eval: epoch: 054, acc1: 72.808%, acc5: 91.468%, test_loss: 1.0739, per_image_load_time: 1.785ms, per_image_inference_time: 0.944ms
2022-02-21 20:13:18 - until epoch: 054, best_acc1: 73.526%
2022-02-21 20:13:18 - epoch 055 lr: 0.010000000000000002
2022-02-21 20:14:06 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.1331
2022-02-21 20:14:49 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 0.9925
2022-02-21 20:15:32 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 0.9473
2022-02-21 20:16:14 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.0922
2022-02-21 20:16:57 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 0.9498
2022-02-21 20:17:39 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.0489
2022-02-21 20:18:22 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.2714
2022-02-21 20:19:05 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.0024
2022-02-21 20:19:48 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.0754
2022-02-21 20:20:31 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.1757
2022-02-21 20:21:14 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.0928
2022-02-21 20:21:57 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.1741
2022-02-21 20:22:39 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.2660
2022-02-21 20:23:22 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.0738
2022-02-21 20:24:05 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.2403
2022-02-21 20:24:48 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.3417
2022-02-21 20:25:31 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.2136
2022-02-21 20:26:14 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.2269
2022-02-21 20:26:56 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.2311
2022-02-21 20:27:39 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.1442
2022-02-21 20:28:22 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.0265
2022-02-21 20:29:04 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.2531
2022-02-21 20:29:47 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.1071
2022-02-21 20:30:30 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.0289
2022-02-21 20:31:12 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.1969
2022-02-21 20:31:55 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.0365
2022-02-21 20:32:38 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 0.9808
2022-02-21 20:33:21 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.1433
2022-02-21 20:34:04 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.1974
2022-02-21 20:34:48 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.0809
2022-02-21 20:35:31 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.2970
2022-02-21 20:36:14 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.0805
2022-02-21 20:36:57 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 0.9207
2022-02-21 20:37:40 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.1326
2022-02-21 20:38:23 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.0927
2022-02-21 20:39:05 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.1742
2022-02-21 20:39:48 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.1337
2022-02-21 20:40:31 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.2591
2022-02-21 20:41:14 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.4787
2022-02-21 20:41:57 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.2020
2022-02-21 20:42:40 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.1092
2022-02-21 20:43:23 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.2079
2022-02-21 20:44:06 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.2532
2022-02-21 20:44:49 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.4111
2022-02-21 20:45:32 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.2123
2022-02-21 20:46:16 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.3341
2022-02-21 20:46:59 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.0904
2022-02-21 20:47:44 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.2959
2022-02-21 20:48:28 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.1465
2022-02-21 20:49:15 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.2278
2022-02-21 20:49:18 - train: epoch 055, train_loss: 1.1758
2022-02-21 20:50:40 - eval: epoch: 055, acc1: 72.328%, acc5: 91.158%, test_loss: 1.0949, per_image_load_time: 2.236ms, per_image_inference_time: 0.854ms
2022-02-21 20:50:42 - until epoch: 055, best_acc1: 73.526%
2022-02-21 20:50:42 - epoch 056 lr: 0.010000000000000002
2022-02-21 20:51:30 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.2540
2022-02-21 20:52:13 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.2540
2022-02-21 20:52:56 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.1240
2022-02-21 20:53:38 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.1459
2022-02-21 20:54:21 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.1149
2022-02-21 20:55:04 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.1973
2022-02-21 20:55:46 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.1508
2022-02-21 20:56:29 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.3465
2022-02-21 20:57:12 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.1683
2022-02-21 20:57:55 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.1350
2022-02-21 20:58:38 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.0855
2022-02-21 20:59:21 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.0486
2022-02-21 21:00:04 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.2255
2022-02-21 21:00:46 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.1366
2022-02-21 21:01:29 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.3758
2022-02-21 21:02:12 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.0472
2022-02-21 21:02:54 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.1933
2022-02-21 21:03:37 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.3681
2022-02-21 21:04:20 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.1557
2022-02-21 21:05:03 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.1788
2022-02-21 21:05:45 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.1855
2022-02-21 21:06:28 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.3165
2022-02-21 21:07:11 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.2699
2022-02-21 21:07:54 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.2641
2022-02-21 21:08:37 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.3517
2022-02-21 21:09:19 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.0963
2022-02-21 21:10:01 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.1356
2022-02-21 21:10:44 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.1731
2022-02-21 21:11:26 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.2799
2022-02-21 21:12:09 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.2484
2022-02-21 21:12:52 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.0935
2022-02-21 21:13:34 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.1046
2022-02-21 21:14:17 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.3604
2022-02-21 21:14:59 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.0210
2022-02-21 21:15:42 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.1874
2022-02-21 21:16:24 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 0.9260
2022-02-21 21:17:06 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.0555
2022-02-21 21:17:49 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.0135
2022-02-21 21:18:32 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.4322
2022-02-21 21:19:14 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.1603
2022-02-21 21:19:57 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.3786
2022-02-21 21:20:40 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.1355
2022-02-21 21:21:23 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.1182
2022-02-21 21:22:05 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.2906
2022-02-21 21:22:48 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.2154
2022-02-21 21:23:31 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.2127
2022-02-21 21:24:15 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.3420
2022-02-21 21:24:59 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.3056
2022-02-21 21:25:42 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.0589
2022-02-21 21:26:26 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.2150
2022-02-21 21:26:30 - train: epoch 056, train_loss: 1.1690
2022-02-21 21:27:50 - eval: epoch: 056, acc1: 72.818%, acc5: 91.406%, test_loss: 1.0783, per_image_load_time: 1.563ms, per_image_inference_time: 0.921ms
2022-02-21 21:27:51 - until epoch: 056, best_acc1: 73.526%
2022-02-21 21:27:51 - epoch 057 lr: 0.010000000000000002
2022-02-21 21:28:39 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.2643
2022-02-21 21:29:21 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 0.9851
2022-02-21 21:30:04 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 0.9984
2022-02-21 21:30:46 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.1061
2022-02-21 21:31:29 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.0390
2022-02-21 21:32:11 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.3618
2022-02-21 21:32:53 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 0.9468
2022-02-21 21:33:36 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.1411
2022-02-21 21:34:18 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.2115
2022-02-21 21:35:01 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.1069
2022-02-21 21:35:43 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.1458
2022-02-21 21:36:25 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 0.9990
2022-02-21 21:37:08 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.1038
2022-02-21 21:37:50 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.1279
2022-02-21 21:38:33 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.3203
2022-02-21 21:39:16 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.3204
2022-02-21 21:39:58 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.4584
2022-02-21 21:40:41 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.2471
2022-02-21 21:41:23 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.0664
2022-02-21 21:42:06 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.1705
2022-02-21 21:42:49 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.1680
2022-02-21 21:43:32 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.2189
2022-02-21 21:44:14 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.0873
2022-02-21 21:44:57 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.0974
2022-02-21 21:45:39 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.1505
2022-02-21 21:46:22 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.0489
2022-02-21 21:47:05 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 0.9735
2022-02-21 21:47:47 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.0318
2022-02-21 21:48:30 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.2952
2022-02-21 21:49:12 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.2540
2022-02-21 21:49:55 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.3445
2022-02-21 21:50:37 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.2433
2022-02-21 21:51:20 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.1861
2022-02-21 21:52:02 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.2039
2022-02-21 21:52:45 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.1635
2022-02-21 21:53:27 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.2051
2022-02-21 21:54:10 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.1286
2022-02-21 21:54:52 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.1104
2022-02-21 21:55:35 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.3172
2022-02-21 21:56:17 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.1647
2022-02-21 21:57:00 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.3149
2022-02-21 21:57:42 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.2391
2022-02-21 21:58:25 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.1807
2022-02-21 21:59:08 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.2510
2022-02-21 21:59:51 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.2439
2022-02-21 22:00:34 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.3345
2022-02-21 22:01:17 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.0851
2022-02-21 22:02:01 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.3196
2022-02-21 22:02:46 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.4265
2022-02-21 22:03:36 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.3518
2022-02-21 22:03:39 - train: epoch 057, train_loss: 1.1681
2022-02-21 22:05:03 - eval: epoch: 057, acc1: 72.754%, acc5: 91.352%, test_loss: 1.0842, per_image_load_time: 1.866ms, per_image_inference_time: 0.915ms
2022-02-21 22:05:05 - until epoch: 057, best_acc1: 73.526%
2022-02-21 22:05:05 - epoch 058 lr: 0.010000000000000002
2022-02-21 22:05:51 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.1229
2022-02-21 22:06:33 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.0270
2022-02-21 22:07:15 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.2831
2022-02-21 22:07:56 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.2169
2022-02-21 22:08:37 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.0124
2022-02-21 22:09:18 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.2737
2022-02-21 22:09:58 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.1368
2022-02-21 22:10:39 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.0388
2022-02-21 22:11:20 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.0299
2022-02-21 22:12:01 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.1614
2022-02-21 22:12:42 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.0216
2022-02-21 22:13:23 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.0756
2022-02-21 22:14:04 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.1337
2022-02-21 22:14:45 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.1239
2022-02-21 22:15:25 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.1792
2022-02-21 22:16:06 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.1757
2022-02-21 22:16:47 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.3951
2022-02-21 22:17:28 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.2558
2022-02-21 22:18:09 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.3500
2022-02-21 22:18:51 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.2272
2022-02-21 22:19:33 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 0.9998
2022-02-21 22:20:15 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 0.8903
2022-02-21 22:20:57 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.1379
2022-02-21 22:21:38 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.1332
2022-02-21 22:22:20 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.3289
2022-02-21 22:23:02 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.0937
2022-02-21 22:23:44 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.2791
2022-02-21 22:24:25 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.0053
2022-02-21 22:25:06 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.1060
2022-02-21 22:25:47 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.2340
2022-02-21 22:26:28 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 0.9868
2022-02-21 22:27:09 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.0288
2022-02-21 22:27:50 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.2257
2022-02-21 22:28:31 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.1406
2022-02-21 22:29:12 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.1510
2022-02-21 22:29:52 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.0749
2022-02-21 22:30:33 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.1434
2022-02-21 22:31:14 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.2114
2022-02-21 22:31:55 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.1864
2022-02-21 22:32:36 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.2791
2022-02-21 22:33:17 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.2566
2022-02-21 22:33:58 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.0458
2022-02-21 22:34:39 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.4181
2022-02-21 22:35:20 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 0.9659
2022-02-21 22:36:00 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.2444
2022-02-21 22:36:41 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.0649
2022-02-21 22:37:22 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.1793
2022-02-21 22:38:03 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.2380
2022-02-21 22:38:44 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.1620
2022-02-21 22:39:25 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.1527
2022-02-21 22:39:29 - train: epoch 058, train_loss: 1.1649
2022-02-21 22:40:44 - eval: epoch: 058, acc1: 73.096%, acc5: 91.444%, test_loss: 1.0695, per_image_load_time: 2.012ms, per_image_inference_time: 0.829ms
2022-02-21 22:40:46 - until epoch: 058, best_acc1: 73.526%
2022-02-21 22:40:46 - epoch 059 lr: 0.010000000000000002
2022-02-21 22:41:32 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.1877
2022-02-21 22:42:13 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.1403
2022-02-21 22:42:53 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.2178
2022-02-21 22:43:34 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.1463
2022-02-21 22:44:15 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.3444
2022-02-21 22:44:56 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.1107
2022-02-21 22:45:37 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.1071
2022-02-21 22:46:18 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.2023
2022-02-21 22:46:59 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.1062
2022-02-21 22:47:40 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.1198
2022-02-21 22:48:20 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 1.2965
2022-02-21 22:49:01 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.0415
2022-02-21 22:49:42 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.2569
2022-02-21 22:50:23 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.2743
2022-02-21 22:51:05 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.2507
2022-02-21 22:51:46 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.0827
2022-02-21 22:52:28 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.1767
2022-02-21 22:53:10 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.1206
2022-02-21 22:53:52 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.1971
2022-02-21 22:54:34 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.0559
2022-02-21 22:55:16 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.2441
2022-02-21 22:55:58 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.3522
2022-02-21 22:56:40 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.1070
2022-02-21 22:57:22 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.3011
2022-02-21 22:58:04 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.2077
2022-02-21 22:58:46 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.0865
2022-02-21 22:59:28 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.0798
2022-02-21 23:00:10 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.3358
2022-02-21 23:00:52 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.0948
2022-02-21 23:01:34 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 1.4672
2022-02-21 23:02:16 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.0705
2022-02-21 23:02:58 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.1569
2022-02-21 23:03:40 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.1563
2022-02-21 23:04:23 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 1.4744
2022-02-21 23:05:05 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 0.9898
2022-02-21 23:05:47 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.1135
2022-02-21 23:06:29 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.0956
2022-02-21 23:07:11 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.2263
2022-02-21 23:07:53 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.0187
2022-02-21 23:08:35 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 1.5485
2022-02-21 23:09:17 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.2372
2022-02-21 23:10:00 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.2410
2022-02-21 23:10:42 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.3391
2022-02-21 23:11:25 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.3966
2022-02-21 23:12:07 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.1293
2022-02-21 23:12:50 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.2524
2022-02-21 23:13:34 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.0929
2022-02-21 23:14:16 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.0566
2022-02-21 23:15:00 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.2755
2022-02-21 23:15:49 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.2728
2022-02-21 23:15:53 - train: epoch 059, train_loss: 1.1628
2022-02-21 23:17:19 - eval: epoch: 059, acc1: 73.154%, acc5: 91.530%, test_loss: 1.0659, per_image_load_time: 2.373ms, per_image_inference_time: 0.852ms
2022-02-21 23:17:20 - until epoch: 059, best_acc1: 73.526%
2022-02-21 23:17:20 - epoch 060 lr: 0.010000000000000002
2022-02-21 23:18:07 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.1152
2022-02-21 23:18:49 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.1836
2022-02-21 23:19:31 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.1824
2022-02-21 23:20:14 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.1669
2022-02-21 23:20:56 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.3408
2022-02-21 23:21:37 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.1912
2022-02-21 23:22:19 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.1594
2022-02-21 23:23:01 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.2940
2022-02-21 23:23:43 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 0.9909
2022-02-21 23:24:25 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 0.9189
2022-02-21 23:25:07 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 0.8594
2022-02-21 23:25:49 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.0486
2022-02-21 23:26:31 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.1458
2022-02-21 23:27:13 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.1864
2022-02-21 23:27:55 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.0650
2022-02-21 23:28:37 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.1583
2022-02-21 23:29:19 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.0804
2022-02-21 23:30:01 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.1970
2022-02-21 23:30:43 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.2795
2022-02-21 23:31:24 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.1167
2022-02-21 23:32:06 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.2055
2022-02-21 23:32:48 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.2520
2022-02-21 23:33:30 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 0.9193
2022-02-21 23:34:12 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.0742
2022-02-21 23:34:54 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.0959
2022-02-21 23:35:36 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.2295
2022-02-21 23:36:18 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.2440
2022-02-21 23:37:00 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.1175
2022-02-21 23:37:42 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.1409
2022-02-21 23:38:24 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.3716
2022-02-21 23:39:06 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.1615
2022-02-21 23:39:48 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.1540
2022-02-21 23:40:30 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 0.9472
2022-02-21 23:41:13 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.1488
2022-02-21 23:41:54 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.0892
2022-02-21 23:42:36 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.0585
2022-02-21 23:43:18 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.2796
2022-02-21 23:44:00 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.1223
2022-02-21 23:44:42 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.5089
2022-02-21 23:45:24 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.1485
2022-02-21 23:46:06 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.3626
2022-02-21 23:46:48 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.3114
2022-02-21 23:47:31 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.0994
2022-02-21 23:48:13 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.2798
2022-02-21 23:48:56 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.1529
2022-02-21 23:49:38 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.1277
2022-02-21 23:50:21 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.2329
2022-02-21 23:51:04 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.0158
2022-02-21 23:51:51 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.1755
2022-02-21 23:52:40 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.2490
2022-02-21 23:52:43 - train: epoch 060, train_loss: 1.1591
2022-02-21 23:54:05 - eval: epoch: 060, acc1: 72.122%, acc5: 91.094%, test_loss: 1.1044, per_image_load_time: 0.762ms, per_image_inference_time: 0.886ms
2022-02-21 23:54:07 - until epoch: 060, best_acc1: 73.526%
2022-02-21 23:54:07 - epoch 061 lr: 0.0010000000000000002
2022-02-21 23:54:54 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 0.9994
2022-02-21 23:55:36 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.1370
2022-02-21 23:56:18 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 0.9301
2022-02-21 23:57:01 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.2443
2022-02-21 23:57:43 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.0585
2022-02-21 23:58:25 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.1235
2022-02-21 23:59:07 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 0.7356
2022-02-21 23:59:49 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.0638
2022-02-22 00:00:32 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 0.9180
2022-02-22 00:01:14 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 0.8199
2022-02-22 00:01:57 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 0.8529
2022-02-22 00:02:39 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.0057
2022-02-22 00:03:22 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 0.9130
2022-02-22 00:04:05 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 0.8182
2022-02-22 00:04:47 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.0560
2022-02-22 00:05:30 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 0.9083
2022-02-22 00:06:12 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.0015
2022-02-22 00:06:55 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 0.9848
2022-02-22 00:07:38 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.0732
2022-02-22 00:08:20 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 0.8807
2022-02-22 00:09:03 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.2445
2022-02-22 00:09:45 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 0.8856
2022-02-22 00:10:28 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 0.9504
2022-02-22 00:11:11 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 0.7610
2022-02-22 00:11:53 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 0.8918
2022-02-22 00:12:36 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 0.9445
2022-02-22 00:13:18 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.1429
2022-02-22 00:14:00 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 0.9683
2022-02-22 00:14:43 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.0351
2022-02-22 00:15:26 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 0.8736
2022-02-22 00:16:08 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.0768
2022-02-22 00:16:51 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.0214
2022-02-22 00:17:34 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 0.8864
2022-02-22 00:18:16 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 0.6968
2022-02-22 00:18:59 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 0.9864
2022-02-22 00:19:41 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 0.9382
2022-02-22 00:20:24 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.0077
2022-02-22 00:21:07 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.0646
2022-02-22 00:21:49 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 0.9095
2022-02-22 00:22:32 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 0.9598
2022-02-22 00:23:15 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.1011
2022-02-22 00:23:58 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 0.9016
2022-02-22 00:24:40 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.0031
2022-02-22 00:25:23 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 0.8983
2022-02-22 00:26:07 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 0.8645
2022-02-22 00:26:51 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.0770
2022-02-22 00:27:34 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 0.9014
2022-02-22 00:28:18 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 0.9519
2022-02-22 00:29:04 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.0778
2022-02-22 00:29:54 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 0.9193
2022-02-22 00:29:57 - train: epoch 061, train_loss: 0.9490
2022-02-22 00:31:18 - eval: epoch: 061, acc1: 77.150%, acc5: 93.544%, test_loss: 0.8939, per_image_load_time: 1.168ms, per_image_inference_time: 0.907ms
2022-02-22 00:31:20 - until epoch: 061, best_acc1: 77.150%
2022-02-22 00:31:20 - epoch 062 lr: 0.0010000000000000002
2022-02-22 00:32:06 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.0657
2022-02-22 00:32:48 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 0.9733
2022-02-22 00:33:29 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 0.8734
2022-02-22 00:34:11 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 0.7473
2022-02-22 00:34:53 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 0.7714
2022-02-22 00:35:35 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 0.9294
2022-02-22 00:36:17 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.0532
2022-02-22 00:36:58 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 0.9430
2022-02-22 00:37:40 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 0.7655
2022-02-22 00:38:22 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.0453
2022-02-22 00:39:04 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 0.8050
2022-02-22 00:39:46 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.0869
2022-02-22 00:40:27 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 0.9933
2022-02-22 00:41:10 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 0.8862
2022-02-22 00:41:52 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 0.9335
2022-02-22 00:42:34 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 0.9973
2022-02-22 00:43:16 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.0180
2022-02-22 00:43:58 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 0.8166
2022-02-22 00:44:40 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 0.8738
2022-02-22 00:45:22 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 0.8201
2022-02-22 00:46:04 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 0.9880
2022-02-22 00:46:46 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 0.8263
2022-02-22 00:47:29 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 0.9413
2022-02-22 00:48:10 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 0.7891
2022-02-22 00:48:52 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 0.9134
2022-02-22 00:49:34 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 0.8106
2022-02-22 00:50:16 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 0.9178
2022-02-22 00:50:58 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.0186
2022-02-22 00:51:41 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 0.9897
2022-02-22 00:52:23 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 0.9284
2022-02-22 00:53:05 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 0.9448
2022-02-22 00:53:47 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 0.7915
2022-02-22 00:54:30 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.0902
2022-02-22 00:55:12 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.0001
2022-02-22 00:55:54 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 0.9544
2022-02-22 00:56:36 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 0.9808
2022-02-22 00:57:19 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 0.8646
2022-02-22 00:58:01 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 0.9226
2022-02-22 00:58:43 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 0.8583
2022-02-22 00:59:25 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 0.6768
2022-02-22 01:00:08 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 0.9481
2022-02-22 01:00:51 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 0.7658
2022-02-22 01:01:33 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 0.8286
2022-02-22 01:02:16 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 0.8199
2022-02-22 01:02:59 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 0.7629
2022-02-22 01:03:43 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 0.7219
2022-02-22 01:04:28 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 0.8523
2022-02-22 01:05:11 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 0.8535
2022-02-22 01:05:56 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 0.7935
2022-02-22 01:06:42 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 0.8635
2022-02-22 01:06:45 - train: epoch 062, train_loss: 0.8919
2022-02-22 01:08:09 - eval: epoch: 062, acc1: 77.420%, acc5: 93.748%, test_loss: 0.8781, per_image_load_time: 2.281ms, per_image_inference_time: 0.870ms
2022-02-22 01:08:12 - until epoch: 062, best_acc1: 77.420%
2022-02-22 01:08:12 - epoch 063 lr: 0.0010000000000000002
2022-02-22 01:08:58 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 0.8292
2022-02-22 01:09:41 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 0.8145
2022-02-22 01:10:23 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.0426
2022-02-22 01:11:06 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 0.9959
2022-02-22 01:11:48 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 0.7727
2022-02-22 01:12:30 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 0.9351
2022-02-22 01:13:12 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 0.7782
2022-02-22 01:13:55 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 0.8915
2022-02-22 01:14:37 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 0.9135
2022-02-22 01:15:20 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 0.9826
2022-02-22 01:16:02 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 0.7575
2022-02-22 01:16:45 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 0.7378
2022-02-22 01:17:27 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 0.9324
2022-02-22 01:18:09 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.0658
2022-02-22 01:18:52 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 0.9316
2022-02-22 01:19:34 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 0.8503
2022-02-22 01:20:16 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 0.7803
2022-02-22 01:20:59 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 0.9753
2022-02-22 01:21:41 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 0.9258
2022-02-22 01:22:24 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 0.7912
2022-02-22 01:23:06 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 0.7436
2022-02-22 01:23:49 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.0837
2022-02-22 01:24:31 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 0.8924
2022-02-22 01:25:14 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 0.9561
2022-02-22 01:25:56 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 0.8392
2022-02-22 01:26:39 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 0.8502
2022-02-22 01:27:21 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.0818
2022-02-22 01:28:03 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 0.8416
2022-02-22 01:28:46 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 0.8236
2022-02-22 01:29:28 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 0.9755
2022-02-22 01:30:11 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.0884
2022-02-22 01:30:53 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 0.9459
2022-02-22 01:31:35 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 0.8589
2022-02-22 01:32:18 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 0.7256
2022-02-22 01:33:00 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 0.9778
2022-02-22 01:33:43 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 0.7744
2022-02-22 01:34:26 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.0602
2022-02-22 01:35:08 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.0677
2022-02-22 01:35:51 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 0.6644
2022-02-22 01:36:33 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 0.6911
2022-02-22 01:37:16 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 0.9994
2022-02-22 01:37:59 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 0.8681
2022-02-22 01:38:42 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 0.9581
2022-02-22 01:39:25 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 0.8555
2022-02-22 01:40:08 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 0.8010
2022-02-22 01:40:51 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 0.8249
2022-02-22 01:41:34 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 0.7994
2022-02-22 01:42:19 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 0.8183
2022-02-22 01:43:03 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 0.8316
2022-02-22 01:43:46 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 0.8430
2022-02-22 01:43:49 - train: epoch 063, train_loss: 0.8697
2022-02-22 01:45:09 - eval: epoch: 063, acc1: 77.556%, acc5: 93.750%, test_loss: 0.8738, per_image_load_time: 2.024ms, per_image_inference_time: 0.935ms
2022-02-22 01:45:11 - until epoch: 063, best_acc1: 77.556%
2022-02-22 01:45:11 - epoch 064 lr: 0.0010000000000000002
2022-02-22 01:45:58 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 0.7860
2022-02-22 01:46:41 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 0.9012
2022-02-22 01:47:24 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 0.6720
2022-02-22 01:48:06 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 0.9719
2022-02-22 01:48:49 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 0.7692
2022-02-22 01:49:32 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.0306
2022-02-22 01:50:14 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 0.9427
2022-02-22 01:50:57 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 0.7761
2022-02-22 01:51:39 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 0.8701
2022-02-22 01:52:21 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 0.7675
2022-02-22 01:53:04 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 0.7817
2022-02-22 01:53:46 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 0.7104
2022-02-22 01:54:29 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 0.8286
2022-02-22 01:55:11 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.0308
2022-02-22 01:55:53 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 0.8488
2022-02-22 01:56:36 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 0.8030
2022-02-22 01:57:19 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 0.7954
2022-02-22 01:58:01 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 0.7625
2022-02-22 01:58:44 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 0.7641
2022-02-22 01:59:26 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 0.8262
2022-02-22 02:00:09 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 0.9478
2022-02-22 02:00:51 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 0.8669
2022-02-22 02:01:34 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 0.9766
2022-02-22 02:02:16 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 0.7550
2022-02-22 02:02:59 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 0.6741
2022-02-22 02:03:42 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 0.7567
2022-02-22 02:04:24 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 0.7693
2022-02-22 02:05:07 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 0.7554
2022-02-22 02:05:49 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.1290
2022-02-22 02:06:32 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 0.9098
2022-02-22 02:07:15 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 0.7870
2022-02-22 02:07:57 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 0.8329
2022-02-22 02:08:40 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 0.8813
2022-02-22 02:09:22 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 0.9820
2022-02-22 02:10:05 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 0.7527
2022-02-22 02:10:48 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 0.7754
2022-02-22 02:11:30 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 0.6745
2022-02-22 02:12:13 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.0037
2022-02-22 02:12:56 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 0.7783
2022-02-22 02:13:39 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 0.7149
2022-02-22 02:14:21 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 0.9014
2022-02-22 02:15:04 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 0.7990
2022-02-22 02:15:47 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.0213
2022-02-22 02:16:31 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 0.8665
2022-02-22 02:17:14 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 0.7606
2022-02-22 02:17:58 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 0.9577
2022-02-22 02:18:42 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.1591
2022-02-22 02:19:26 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 0.8088
2022-02-22 02:20:10 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.0493
2022-02-22 02:20:57 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 0.6923
2022-02-22 02:21:00 - train: epoch 064, train_loss: 0.8537
2022-02-22 02:22:25 - eval: epoch: 064, acc1: 77.710%, acc5: 93.798%, test_loss: 0.8695, per_image_load_time: 0.598ms, per_image_inference_time: 0.866ms
2022-02-22 02:22:27 - until epoch: 064, best_acc1: 77.710%
2022-02-22 02:22:27 - epoch 065 lr: 0.0010000000000000002
2022-02-22 02:23:14 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 0.9591
2022-02-22 02:23:56 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 0.7902
2022-02-22 02:24:39 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 0.8198
2022-02-22 02:25:22 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 0.9461
2022-02-22 02:26:05 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 0.8104
2022-02-22 02:26:47 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 0.9291
2022-02-22 02:27:30 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 0.7036
2022-02-22 02:28:12 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 0.8378
2022-02-22 02:28:55 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 0.7913
2022-02-22 02:29:38 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 0.8651
2022-02-22 02:30:21 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 0.8838
2022-02-22 02:31:03 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.0457
2022-02-22 02:31:46 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 0.8692
2022-02-22 02:32:28 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 0.6198
2022-02-22 02:33:11 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 0.8353
2022-02-22 02:33:53 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 0.7694
2022-02-22 02:34:35 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 0.7554
2022-02-22 02:35:18 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 0.7908
2022-02-22 02:36:00 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 0.7637
2022-02-22 02:36:43 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 0.8089
2022-02-22 02:37:25 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 0.8098
2022-02-22 02:38:08 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 0.9805
2022-02-22 02:38:50 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 0.7725
2022-02-22 02:39:32 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 0.7399
2022-02-22 02:40:15 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 0.8334
2022-02-22 02:40:58 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 0.8877
2022-02-22 02:41:40 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 0.8539
2022-02-22 02:42:23 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 0.7440
2022-02-22 02:43:05 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 0.9148
2022-02-22 02:43:48 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 0.7740
2022-02-22 02:44:31 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 0.8982
2022-02-22 02:45:13 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 0.9135
2022-02-22 02:45:56 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 0.7679
2022-02-22 02:46:38 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 0.7484
2022-02-22 02:47:21 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.0600
2022-02-22 02:48:03 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 0.8924
2022-02-22 02:48:46 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 0.7590
2022-02-22 02:49:29 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 0.6663
2022-02-22 02:50:12 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 0.8996
2022-02-22 02:50:54 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 0.8993
2022-02-22 02:51:37 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 0.7362
2022-02-22 02:52:21 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 0.8447
2022-02-22 02:53:04 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 0.6928
2022-02-22 02:53:46 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 0.7587
2022-02-22 02:54:30 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 0.7849
2022-02-22 02:55:13 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 0.8104
2022-02-22 02:55:57 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 0.9016
2022-02-22 02:56:41 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 0.5938
2022-02-22 02:57:27 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 0.7276
2022-02-22 02:58:13 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 0.9110
2022-02-22 02:58:16 - train: epoch 065, train_loss: 0.8392
2022-02-22 02:59:37 - eval: epoch: 065, acc1: 77.890%, acc5: 93.876%, test_loss: 0.8691, per_image_load_time: 0.755ms, per_image_inference_time: 0.940ms
2022-02-22 02:59:39 - until epoch: 065, best_acc1: 77.890%
2022-02-22 02:59:39 - epoch 066 lr: 0.0010000000000000002
2022-02-22 03:00:27 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 0.7360
2022-02-22 03:01:10 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 0.9481
2022-02-22 03:01:52 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 0.6361
2022-02-22 03:02:35 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 0.6529
2022-02-22 03:03:17 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 0.8968
2022-02-22 03:04:00 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 0.8464
2022-02-22 03:04:42 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 0.8356
2022-02-22 03:05:25 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.0219
2022-02-22 03:06:08 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 0.9128
2022-02-22 03:06:50 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 0.9079
2022-02-22 03:07:33 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 0.8636
2022-02-22 03:08:16 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 0.9117
2022-02-22 03:08:58 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 0.8038
2022-02-22 03:09:41 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 0.7051
2022-02-22 03:10:24 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 0.8748
2022-02-22 03:11:06 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 0.7708
2022-02-22 03:11:49 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 0.8225
2022-02-22 03:12:31 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 0.7714
2022-02-22 03:13:14 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.0455
2022-02-22 03:13:57 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 0.9650
2022-02-22 03:14:40 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 0.8355
2022-02-22 03:15:22 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 0.7923
2022-02-22 03:16:05 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.0491
2022-02-22 03:16:47 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 0.7255
2022-02-22 03:17:30 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 0.7817
2022-02-22 03:18:13 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 0.7557
2022-02-22 03:18:55 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.0341
2022-02-22 03:19:38 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.0086
2022-02-22 03:20:21 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 0.7417
2022-02-22 03:21:04 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 0.7814
2022-02-22 03:21:46 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 0.8761
2022-02-22 03:22:29 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 0.6715
2022-02-22 03:23:12 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 0.7529
2022-02-22 03:23:54 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.0115
2022-02-22 03:24:37 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 0.8551
2022-02-22 03:25:20 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 0.8905
2022-02-22 03:26:03 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 0.8515
2022-02-22 03:26:45 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 0.7113
2022-02-22 03:27:28 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 0.7267
2022-02-22 03:28:10 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 0.9767
2022-02-22 03:28:53 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 0.6534
2022-02-22 03:29:36 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 0.6260
2022-02-22 03:30:19 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 0.6724
2022-02-22 03:31:02 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 0.8477
2022-02-22 03:31:45 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 0.8546
2022-02-22 03:32:28 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 0.9748
2022-02-22 03:33:19 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 0.7486
2022-02-22 03:34:03 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 0.8163
2022-02-22 03:34:46 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 0.8184
2022-02-22 03:36:05 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 0.8169
2022-02-22 03:36:12 - train: epoch 066, train_loss: 0.8296
2022-02-22 03:37:32 - eval: epoch: 066, acc1: 77.924%, acc5: 93.836%, test_loss: 0.8655, per_image_load_time: 0.970ms, per_image_inference_time: 0.974ms
2022-02-22 03:37:34 - until epoch: 066, best_acc1: 77.924%
2022-02-22 03:37:34 - epoch 067 lr: 0.0010000000000000002
2022-02-22 03:38:20 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 0.6974
2022-02-22 03:39:02 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 0.7499
2022-02-22 03:39:44 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.1468
2022-02-22 03:40:26 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 0.8498
2022-02-22 03:41:08 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 0.8524
2022-02-22 03:41:50 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 0.7520
2022-02-22 03:42:32 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 0.8766
2022-02-22 03:43:14 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 0.9497
2022-02-22 03:43:56 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 0.8398
2022-02-22 03:44:38 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 0.6651
2022-02-22 03:45:19 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 0.7807
2022-02-22 03:46:02 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 0.7518
2022-02-22 03:46:44 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 0.9831
2022-02-22 03:47:26 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 0.8078
2022-02-22 03:48:08 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 0.9015
2022-02-22 03:48:50 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 0.9251
2022-02-22 03:49:31 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 0.7525
2022-02-22 03:50:13 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 0.9957
2022-02-22 03:50:55 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 0.7056
2022-02-22 03:51:37 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 0.9507
2022-02-22 03:52:19 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 0.6943
2022-02-22 03:53:01 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 0.8880
2022-02-22 03:53:44 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 0.7791
2022-02-22 03:54:26 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 0.8170
2022-02-22 03:55:07 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 0.7137
2022-02-22 03:55:50 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 0.7127
2022-02-22 03:56:32 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 0.7629
2022-02-22 03:57:14 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 0.9294
2022-02-22 03:57:57 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 0.7930
2022-02-22 03:58:39 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 0.8439
2022-02-22 03:59:21 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 0.7396
2022-02-22 04:00:03 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 0.9078
2022-02-22 04:00:45 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 0.7095
2022-02-22 04:01:27 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 0.8604
2022-02-22 04:02:09 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 0.6623
2022-02-22 04:02:51 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 0.7286
2022-02-22 04:03:33 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 0.9705
2022-02-22 04:04:15 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 0.7719
2022-02-22 04:04:57 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 0.8420
2022-02-22 04:05:40 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.0238
2022-02-22 04:06:22 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.0043
2022-02-22 04:07:04 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 0.9245
2022-02-22 04:07:47 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 0.7263
2022-02-22 04:08:30 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 0.8612
2022-02-22 04:09:12 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 0.7967
2022-02-22 04:09:55 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 0.5845
2022-02-22 04:10:38 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 0.7627
2022-02-22 04:11:22 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 0.6211
2022-02-22 04:12:07 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 0.7837
2022-02-22 04:12:56 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 0.8379
2022-02-22 04:12:59 - train: epoch 067, train_loss: 0.8199
2022-02-22 04:14:21 - eval: epoch: 067, acc1: 77.858%, acc5: 93.902%, test_loss: 0.8641, per_image_load_time: 1.404ms, per_image_inference_time: 0.955ms
2022-02-22 04:14:23 - until epoch: 067, best_acc1: 77.924%
2022-02-22 04:14:23 - epoch 068 lr: 0.0010000000000000002
2022-02-22 04:15:10 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 0.8594
2022-02-22 04:15:52 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 0.9274
2022-02-22 04:16:34 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 0.8785
2022-02-22 04:17:16 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 0.9054
2022-02-22 04:17:58 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 0.8015
2022-02-22 04:18:40 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 0.8519
2022-02-22 04:19:21 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.0570
2022-02-22 04:20:03 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 0.7634
2022-02-22 04:20:45 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 0.7454
2022-02-22 04:21:27 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 0.8340
2022-02-22 04:22:09 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 0.9214
2022-02-22 04:22:51 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 0.7830
2022-02-22 04:23:33 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 0.6401
2022-02-22 04:24:14 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 0.8194
2022-02-22 04:24:56 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 0.9245
2022-02-22 04:25:38 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 0.8896
2022-02-22 04:26:20 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 0.9139
2022-02-22 04:27:01 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 0.8303
2022-02-22 04:27:43 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 0.8872
2022-02-22 04:28:25 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 0.9173
2022-02-22 04:29:07 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 0.7648
2022-02-22 04:29:49 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 0.8470
2022-02-22 04:30:31 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 0.7019
2022-02-22 04:31:12 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 0.9306
2022-02-22 04:31:54 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 0.7684
2022-02-22 04:32:36 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 0.8764
2022-02-22 04:33:18 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 0.8331
2022-02-22 04:34:00 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 0.9082
2022-02-22 04:34:42 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 0.8749
2022-02-22 04:35:24 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.0049
2022-02-22 04:36:06 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 0.6564
2022-02-22 04:36:47 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 0.8017
2022-02-22 04:37:29 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 0.8413
2022-02-22 04:38:12 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 0.7425
2022-02-22 04:38:54 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 0.8631
2022-02-22 04:39:36 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 0.7484
2022-02-22 04:40:18 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 0.9740
2022-02-22 04:41:00 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 0.8847
2022-02-22 04:41:42 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 0.8313
2022-02-22 04:42:24 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 0.8091
2022-02-22 04:43:06 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 0.6649
2022-02-22 04:43:48 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 0.9731
2022-02-22 04:44:30 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 0.9128
2022-02-22 04:45:12 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 0.7688
2022-02-22 04:45:54 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 0.8533
2022-02-22 04:46:37 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 0.9517
2022-02-22 04:47:20 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.0458
2022-02-22 04:48:03 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 0.9820
2022-02-22 04:48:46 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 0.8922
2022-02-22 04:49:34 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 0.7310
2022-02-22 04:49:37 - train: epoch 068, train_loss: 0.8119
2022-02-22 04:51:01 - eval: epoch: 068, acc1: 78.004%, acc5: 93.966%, test_loss: 0.8622, per_image_load_time: 1.200ms, per_image_inference_time: 0.954ms
2022-02-22 04:51:04 - until epoch: 068, best_acc1: 78.004%
2022-02-22 04:51:04 - epoch 069 lr: 0.0010000000000000002
2022-02-22 04:51:50 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 0.9642
2022-02-22 04:52:31 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 0.8904
2022-02-22 04:53:13 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 0.8955
2022-02-22 04:53:54 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 0.8221
2022-02-22 04:54:36 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 0.7144
2022-02-22 04:55:18 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 0.6935
2022-02-22 04:55:59 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 0.7887
2022-02-22 04:56:41 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 0.8448
2022-02-22 04:57:22 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 0.8044
2022-02-22 04:58:04 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 0.8132
2022-02-22 04:58:45 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 0.7544
2022-02-22 04:59:27 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 0.8037
2022-02-22 05:00:09 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 0.9773
2022-02-22 05:00:50 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 0.9119
2022-02-22 05:01:32 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 0.8657
2022-02-22 05:02:13 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 0.8495
2022-02-22 05:02:55 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 0.7815
2022-02-22 05:03:36 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 0.6590
2022-02-22 05:04:17 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 0.6901
2022-02-22 05:04:59 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 0.5957
2022-02-22 05:05:40 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 0.8431
2022-02-22 05:06:22 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 0.8898
2022-02-22 05:07:03 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 0.8549
2022-02-22 05:07:45 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 0.8937
2022-02-22 05:08:26 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 0.8299
2022-02-22 05:09:08 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 0.8718
2022-02-22 05:09:49 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 0.9782
2022-02-22 05:10:31 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 0.9787
2022-02-22 05:11:12 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 0.6632
2022-02-22 05:11:54 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 0.7839
2022-02-22 05:12:35 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 0.6492
2022-02-22 05:13:17 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 0.7367
2022-02-22 05:13:59 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 0.6487
2022-02-22 05:14:40 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 0.6457
2022-02-22 05:15:22 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 0.7378
2022-02-22 05:16:04 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 0.7323
2022-02-22 05:16:45 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 0.8708
2022-02-22 05:17:27 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 0.8492
2022-02-22 05:18:09 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 0.8913
2022-02-22 05:18:51 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 0.8406
2022-02-22 05:19:33 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 0.9479
2022-02-22 05:20:14 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 0.6990
2022-02-22 05:20:56 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 0.7960
2022-02-22 05:21:39 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 0.8306
2022-02-22 05:22:21 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 0.7997
2022-02-22 05:23:03 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 0.9486
2022-02-22 05:23:47 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 0.7837
2022-02-22 05:24:29 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 0.7876
2022-02-22 05:25:13 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 0.7155
2022-02-22 05:26:02 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 0.8868
2022-02-22 05:26:05 - train: epoch 069, train_loss: 0.8024
2022-02-22 05:27:29 - eval: epoch: 069, acc1: 77.916%, acc5: 93.886%, test_loss: 0.8632, per_image_load_time: 0.815ms, per_image_inference_time: 0.944ms
2022-02-22 05:27:31 - until epoch: 069, best_acc1: 78.004%
2022-02-22 05:27:31 - epoch 070 lr: 0.0010000000000000002
2022-02-22 05:28:18 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 0.8164
2022-02-22 05:28:59 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 0.8916
2022-02-22 05:29:40 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 0.9216
2022-02-22 05:30:22 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 0.7270
2022-02-22 05:31:04 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 0.8878
2022-02-22 05:31:45 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 0.7713
2022-02-22 05:32:27 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 0.8244
2022-02-22 05:33:08 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 0.6684
2022-02-22 05:33:50 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 0.8951
2022-02-22 05:34:31 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 0.7697
2022-02-22 05:35:13 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.0579
2022-02-22 05:35:54 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 0.6160
2022-02-22 05:36:36 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 0.7462
2022-02-22 05:37:17 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 0.8204
2022-02-22 05:37:59 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 0.7736
2022-02-22 05:38:40 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 0.7694
2022-02-22 05:39:22 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 0.8789
2022-02-22 05:40:03 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 0.7120
2022-02-22 05:40:44 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 0.8242
2022-02-22 05:41:26 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 0.8310
2022-02-22 05:42:07 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 0.8008
2022-02-22 05:42:49 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 0.7974
2022-02-22 05:43:31 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 0.7368
2022-02-22 05:44:12 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 0.9579
2022-02-22 05:44:54 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 0.8505
2022-02-22 05:45:36 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 0.7308
2022-02-22 05:46:17 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 0.7723
2022-02-22 05:46:59 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 0.8944
2022-02-22 05:47:41 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 0.9897
2022-02-22 05:48:23 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 0.7537
2022-02-22 05:49:04 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 0.7084
2022-02-22 05:49:46 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 0.8800
2022-02-22 05:50:28 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 0.7943
2022-02-22 05:51:09 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 0.8145
2022-02-22 05:51:51 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 0.8348
2022-02-22 05:52:33 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 0.8578
2022-02-22 05:53:14 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 0.7568
2022-02-22 05:53:57 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 0.7676
2022-02-22 05:54:38 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 0.6576
2022-02-22 05:55:20 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 0.6681
2022-02-22 05:56:02 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 0.7245
2022-02-22 05:56:44 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 0.7937
2022-02-22 05:57:26 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 0.7905
2022-02-22 05:58:08 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 0.8905
2022-02-22 05:58:50 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 0.7493
2022-02-22 05:59:32 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 0.8953
2022-02-22 06:00:15 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 0.8903
2022-02-22 06:00:58 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 0.7665
2022-02-22 06:01:45 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 0.8233
2022-02-22 06:02:33 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 0.7599
2022-02-22 06:02:37 - train: epoch 070, train_loss: 0.7970
2022-02-22 06:03:57 - eval: epoch: 070, acc1: 77.980%, acc5: 93.974%, test_loss: 0.8624, per_image_load_time: 2.099ms, per_image_inference_time: 0.949ms
2022-02-22 06:03:59 - until epoch: 070, best_acc1: 78.004%
2022-02-22 06:03:59 - epoch 071 lr: 0.0010000000000000002
2022-02-22 06:04:45 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 0.6530
2022-02-22 06:05:27 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 0.7396
2022-02-22 06:06:08 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 0.7389
2022-02-22 06:06:49 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 0.8024
2022-02-22 06:07:31 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 0.7398
2022-02-22 06:08:12 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 0.8490
2022-02-22 06:08:54 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 0.9841
2022-02-22 06:09:35 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 0.7703
2022-02-22 06:10:17 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 0.7982
2022-02-22 06:10:58 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 0.8470
2022-02-22 06:11:40 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 0.9169
2022-02-22 06:12:21 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 0.7018
2022-02-22 06:13:03 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 0.8715
2022-02-22 06:13:44 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 0.8579
2022-02-22 06:14:26 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 0.6377
2022-02-22 06:15:08 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 0.8127
2022-02-22 06:15:49 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 0.7650
2022-02-22 06:16:31 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 0.8128
2022-02-22 06:17:12 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 0.6942
2022-02-22 06:17:54 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 0.8921
2022-02-22 06:18:36 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 0.6516
2022-02-22 06:19:18 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 0.7577
2022-02-22 06:19:59 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 0.6734
2022-02-22 06:20:41 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 0.7010
2022-02-22 06:21:22 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 0.7667
2022-02-22 06:22:04 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 0.7143
2022-02-22 06:22:46 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 0.7856
2022-02-22 06:23:27 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 0.8433
2022-02-22 06:24:09 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 0.8222
2022-02-22 06:24:50 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 0.7645
2022-02-22 06:25:32 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 0.7886
2022-02-22 06:26:13 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 0.7230
2022-02-22 06:26:55 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 0.7054
2022-02-22 06:27:37 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 0.7304
2022-02-22 06:28:18 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 0.9321
2022-02-22 06:29:00 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 0.9141
2022-02-22 06:29:42 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 0.8420
2022-02-22 06:30:23 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 0.7839
2022-02-22 06:31:05 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 0.9179
2022-02-22 06:31:47 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 0.9244
2022-02-22 06:32:29 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 0.7659
2022-02-22 06:33:11 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 0.8719
2022-02-22 06:33:53 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 0.8850
2022-02-22 06:34:35 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 0.7921
2022-02-22 06:35:17 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 0.7839
2022-02-22 06:36:00 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 0.8114
2022-02-22 06:36:43 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 0.6344
2022-02-22 06:37:27 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 0.6157
2022-02-22 06:38:09 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 0.5815
2022-02-22 06:38:55 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 0.6990
2022-02-22 06:38:58 - train: epoch 071, train_loss: 0.7892
2022-02-22 06:40:26 - eval: epoch: 071, acc1: 78.112%, acc5: 93.936%, test_loss: 0.8603, per_image_load_time: 1.305ms, per_image_inference_time: 0.874ms
2022-02-22 06:40:28 - until epoch: 071, best_acc1: 78.112%
2022-02-22 06:40:28 - epoch 072 lr: 0.0010000000000000002
2022-02-22 06:41:14 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 0.8842
2022-02-22 06:41:55 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 0.7208
2022-02-22 06:42:36 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 0.6466
2022-02-22 06:43:17 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 0.7285
2022-02-22 06:43:59 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 0.7590
2022-02-22 06:44:40 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 0.7756
2022-02-22 06:45:21 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 0.7957
2022-02-22 06:46:02 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 0.9036
2022-02-22 06:46:44 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 0.6731
2022-02-22 06:47:25 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 0.6888
2022-02-22 06:48:06 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 0.7422
2022-02-22 06:48:48 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 0.7599
2022-02-22 06:49:29 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 0.8539
2022-02-22 06:50:11 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 0.8814
2022-02-22 06:50:53 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 0.6831
2022-02-22 06:51:34 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 0.8874
2022-02-22 06:52:15 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 0.7364
2022-02-22 06:52:57 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 0.6873
2022-02-22 06:53:38 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 0.6751
2022-02-22 06:54:20 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 0.8520
2022-02-22 06:55:02 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 0.8020
2022-02-22 06:55:43 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 0.8209
2022-02-22 06:56:25 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 0.8920
2022-02-22 06:57:06 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 0.6814
2022-02-22 06:57:48 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 0.7125
2022-02-22 06:58:29 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 0.6371
2022-02-22 06:59:11 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 0.8106
2022-02-22 06:59:52 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 0.8812
2022-02-22 07:00:34 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 0.7693
2022-02-22 07:01:15 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 0.6951
2022-02-22 07:01:57 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 0.9200
2022-02-22 07:02:38 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 0.7152
2022-02-22 07:03:20 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 0.7295
2022-02-22 07:04:01 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 0.7911
2022-02-22 07:04:43 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 0.7788
2022-02-22 07:05:24 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 0.6904
2022-02-22 07:06:06 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 0.9056
2022-02-22 07:06:48 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 0.7569
2022-02-22 07:07:30 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 0.8783
2022-02-22 07:08:12 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 0.7468
2022-02-22 07:08:54 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 0.8933
2022-02-22 07:09:36 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 0.7365
2022-02-22 07:10:18 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 0.7634
2022-02-22 07:11:00 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 0.6737
2022-02-22 07:11:42 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 0.8852
2022-02-22 07:12:24 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 0.8067
2022-02-22 07:13:07 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 0.8384
2022-02-22 07:13:51 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 0.8816
2022-02-22 07:14:36 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 0.8514
2022-02-22 07:15:21 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 0.7894
2022-02-22 07:15:24 - train: epoch 072, train_loss: 0.7840
2022-02-22 07:16:44 - eval: epoch: 072, acc1: 78.112%, acc5: 93.892%, test_loss: 0.8630, per_image_load_time: 2.052ms, per_image_inference_time: 0.953ms
2022-02-22 07:16:46 - until epoch: 072, best_acc1: 78.112%
2022-02-22 07:16:46 - epoch 073 lr: 0.0010000000000000002
2022-02-22 07:17:32 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 0.9103
2022-02-22 07:18:13 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 0.8419
2022-02-22 07:18:54 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 0.8886
2022-02-22 07:19:36 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 0.5566
2022-02-22 07:20:17 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 0.6899
2022-02-22 07:20:58 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 0.6540
2022-02-22 07:21:40 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 0.7960
2022-02-22 07:22:21 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 0.7033
2022-02-22 07:23:03 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 0.6237
2022-02-22 07:23:44 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 0.6755
2022-02-22 07:24:26 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 0.8405
2022-02-22 07:25:07 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 0.8098
2022-02-22 07:25:48 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 0.7689
2022-02-22 07:26:30 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 0.6595
2022-02-22 07:27:11 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 0.6991
2022-02-22 07:27:53 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 0.7830
2022-02-22 07:28:34 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 0.9814
2022-02-22 07:29:16 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 0.6249
2022-02-22 07:29:58 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 0.8036
2022-02-22 07:30:39 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 0.5666
2022-02-22 07:31:21 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 0.7633
2022-02-22 07:32:03 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 0.9159
2022-02-22 07:32:44 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 0.8464
2022-02-22 07:33:26 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 0.7714
2022-02-22 07:34:08 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 0.8879
2022-02-22 07:34:49 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 0.8758
2022-02-22 07:35:31 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 0.8169
2022-02-22 07:36:12 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 0.8406
2022-02-22 07:36:53 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 0.8833
2022-02-22 07:37:35 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 0.6789
2022-02-22 07:38:16 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 0.7046
2022-02-22 07:38:58 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 0.7461
2022-02-22 07:39:39 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 0.7867
2022-02-22 07:40:21 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 0.7897
2022-02-22 07:41:02 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 0.7965
2022-02-22 07:41:44 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 0.7540
2022-02-22 07:42:25 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 0.8065
2022-02-22 07:43:07 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 0.9803
2022-02-22 07:43:49 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 0.7350
2022-02-22 07:44:31 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 0.8113
2022-02-22 07:45:12 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 0.7408
2022-02-22 07:45:54 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 0.8176
2022-02-22 07:46:36 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 0.6957
2022-02-22 07:47:18 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 0.8078
2022-02-22 07:48:00 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 0.7682
2022-02-22 07:48:43 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 0.9397
2022-02-22 07:49:25 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 0.6045
2022-02-22 07:50:08 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 0.8593
2022-02-22 07:50:52 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 0.6754
2022-02-22 07:51:37 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 0.9052
2022-02-22 07:51:41 - train: epoch 073, train_loss: 0.7791
2022-02-22 07:52:59 - eval: epoch: 073, acc1: 78.038%, acc5: 93.940%, test_loss: 0.8663, per_image_load_time: 1.453ms, per_image_inference_time: 0.949ms
2022-02-22 07:53:00 - until epoch: 073, best_acc1: 78.112%
2022-02-22 07:53:00 - epoch 074 lr: 0.0010000000000000002
2022-02-22 07:53:46 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 0.7517
2022-02-22 07:54:27 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 0.7862
2022-02-22 07:55:08 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 0.8402
2022-02-22 07:55:50 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 0.8204
2022-02-22 07:56:31 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 0.8030
2022-02-22 07:57:12 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 0.8188
2022-02-22 07:57:53 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 0.8005
2022-02-22 07:58:35 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 0.9712
2022-02-22 07:59:16 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 0.8094
2022-02-22 07:59:58 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 0.8109
2022-02-22 08:00:39 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 0.8660
2022-02-22 08:01:20 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 0.7393
2022-02-22 08:02:02 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 0.8778
2022-02-22 08:02:43 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 0.6725
2022-02-22 08:03:24 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 0.7511
2022-02-22 08:04:06 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 0.5788
2022-02-22 08:04:47 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 0.8313
2022-02-22 08:05:29 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.1358
2022-02-22 08:06:10 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 0.9118
2022-02-22 08:06:52 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 0.6399
2022-02-22 08:07:34 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 0.7421
2022-02-22 08:08:15 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.0997
2022-02-22 08:08:56 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 0.8084
2022-02-22 08:09:38 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 0.7660
2022-02-22 08:10:20 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 0.7534
2022-02-22 08:11:01 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 0.6650
2022-02-22 08:11:42 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 0.6554
2022-02-22 08:12:24 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 0.9652
2022-02-22 08:13:05 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 0.6906
2022-02-22 08:13:47 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 0.8357
2022-02-22 08:14:28 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 0.7537
2022-02-22 08:15:09 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 0.7493
2022-02-22 08:15:51 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 0.8186
2022-02-22 08:16:32 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 0.8235
2022-02-22 08:17:14 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 0.5710
2022-02-22 08:17:55 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 0.7613
2022-02-22 08:18:37 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 0.8234
2022-02-22 08:19:18 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 0.7083
2022-02-22 08:20:00 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 0.6221
2022-02-22 08:20:41 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 0.6833
2022-02-22 08:21:23 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 0.9512
2022-02-22 08:22:04 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 0.7084
2022-02-22 08:22:46 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 0.6705
2022-02-22 08:23:28 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 0.6714
2022-02-22 08:24:10 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 0.6941
2022-02-22 08:24:52 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 0.8655
2022-02-22 08:25:34 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 0.8293
2022-02-22 08:26:18 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 0.7969
2022-02-22 08:27:03 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 0.9386
2022-02-22 08:27:49 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 0.7903
2022-02-22 08:27:52 - train: epoch 074, train_loss: 0.7717
2022-02-22 08:29:14 - eval: epoch: 074, acc1: 78.102%, acc5: 93.910%, test_loss: 0.8613, per_image_load_time: 0.859ms, per_image_inference_time: 0.944ms
2022-02-22 08:29:16 - until epoch: 074, best_acc1: 78.112%
2022-02-22 08:29:16 - epoch 075 lr: 0.0010000000000000002
2022-02-22 08:30:02 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 0.8188
2022-02-22 08:30:43 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 0.7456
2022-02-22 08:31:25 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 0.7228
2022-02-22 08:32:06 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 0.7392
2022-02-22 08:32:48 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 0.6238
2022-02-22 08:33:29 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 0.7192
2022-02-22 08:34:10 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 0.8389
2022-02-22 08:34:52 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 0.8635
2022-02-22 08:35:33 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 0.9724
2022-02-22 08:36:14 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 0.5889
2022-02-22 08:36:56 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 0.7503
2022-02-22 08:37:37 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 0.7448
2022-02-22 08:38:19 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 0.7262
2022-02-22 08:39:00 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 0.8015
2022-02-22 08:39:42 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 0.8514
2022-02-22 08:40:23 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 0.5747
2022-02-22 08:41:04 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 0.7021
2022-02-22 08:41:46 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 0.7013
2022-02-22 08:42:28 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 0.6850
2022-02-22 08:43:09 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 0.8531
2022-02-22 08:43:51 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 0.7814
2022-02-22 08:44:32 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 0.8180
2022-02-22 08:45:13 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 0.8770
2022-02-22 08:45:54 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 0.7612
2022-02-22 08:46:36 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 0.7999
2022-02-22 08:47:17 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 0.8229
2022-02-22 08:47:58 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 0.5366
2022-02-22 08:48:40 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 0.7355
2022-02-22 08:49:21 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 0.8124
2022-02-22 08:50:03 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 0.8530
2022-02-22 08:50:44 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 0.7885
2022-02-22 08:51:26 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 0.7505
2022-02-22 08:52:07 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 0.7920
2022-02-22 08:52:49 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 0.5499
2022-02-22 08:53:30 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 0.6429
2022-02-22 08:54:12 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 0.7174
2022-02-22 08:54:54 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 0.7718
2022-02-22 08:55:35 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 0.7660
2022-02-22 08:56:17 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 0.9125
2022-02-22 08:56:59 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 0.6926
2022-02-22 08:57:41 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 0.5215
2022-02-22 08:58:23 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 0.8225
2022-02-22 08:59:04 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 0.9048
2022-02-22 08:59:46 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 0.6336
2022-02-22 09:00:29 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 0.8346
2022-02-22 09:01:11 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 0.6742
2022-02-22 09:01:54 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 0.9236
2022-02-22 09:02:38 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 0.7963
2022-02-22 09:03:21 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 0.6445
2022-02-22 09:04:05 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 0.8949
2022-02-22 09:04:08 - train: epoch 075, train_loss: 0.7677
2022-02-22 09:05:31 - eval: epoch: 075, acc1: 78.080%, acc5: 93.916%, test_loss: 0.8647, per_image_load_time: 0.611ms, per_image_inference_time: 0.896ms
2022-02-22 09:05:33 - until epoch: 075, best_acc1: 78.112%
2022-02-22 09:05:33 - epoch 076 lr: 0.0010000000000000002
2022-02-22 09:06:19 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 0.7085
2022-02-22 09:07:01 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 0.8249
2022-02-22 09:07:43 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 0.8392
2022-02-22 09:08:24 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 0.8377
2022-02-22 09:09:06 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 0.8130
2022-02-22 09:09:47 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 0.8554
2022-02-22 09:10:29 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 0.7595
2022-02-22 09:11:11 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 0.7128
2022-02-22 09:11:52 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 0.6408
2022-02-22 09:12:34 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 0.5597
2022-02-22 09:13:15 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 0.6786
2022-02-22 09:13:57 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 0.7836
2022-02-22 09:14:39 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 0.7741
2022-02-22 09:15:21 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 0.7413
2022-02-22 09:16:03 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 0.6398
2022-02-22 09:16:45 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 0.7420
2022-02-22 09:17:26 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 0.7515
2022-02-22 09:18:08 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 0.5921
2022-02-22 09:18:49 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 0.8781
2022-02-22 09:19:31 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 0.8349
2022-02-22 09:20:13 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 0.9066
2022-02-22 09:20:55 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 0.8541
2022-02-22 09:21:37 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 0.8095
2022-02-22 09:22:18 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 0.8692
2022-02-22 09:23:00 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 0.7661
2022-02-22 09:23:41 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 0.8819
2022-02-22 09:24:23 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 0.8066
2022-02-22 09:25:05 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 0.6936
2022-02-22 09:25:47 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 0.7269
2022-02-22 09:26:29 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 0.7137
2022-02-22 09:27:10 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 0.7678
2022-02-22 09:27:52 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 0.7894
2022-02-22 09:28:33 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 0.8007
2022-02-22 09:29:15 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 0.8450
2022-02-22 09:29:57 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 0.6930
2022-02-22 09:30:39 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 0.7688
2022-02-22 09:31:21 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 0.6556
2022-02-22 09:32:03 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 0.6295
2022-02-22 09:32:45 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 0.5893
2022-02-22 09:33:26 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 0.7524
2022-02-22 09:34:08 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 0.6950
2022-02-22 09:34:50 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 0.9378
2022-02-22 09:35:32 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 0.7340
2022-02-22 09:36:14 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 0.7985
2022-02-22 09:36:56 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 0.7488
2022-02-22 09:37:39 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 0.6448
2022-02-22 09:38:21 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 0.8421
2022-02-22 09:39:04 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 0.6068
2022-02-22 09:39:49 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 0.7200
2022-02-22 09:40:36 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 0.7880
2022-02-22 09:40:40 - train: epoch 076, train_loss: 0.7616
2022-02-22 09:42:00 - eval: epoch: 076, acc1: 78.016%, acc5: 93.904%, test_loss: 0.8694, per_image_load_time: 0.850ms, per_image_inference_time: 0.894ms
2022-02-22 09:42:01 - until epoch: 076, best_acc1: 78.112%
2022-02-22 09:42:01 - epoch 077 lr: 0.0010000000000000002
2022-02-22 09:42:48 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 0.7511
2022-02-22 09:43:30 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 0.7573
2022-02-22 09:44:12 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 0.6176
2022-02-22 09:44:53 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 0.8158
2022-02-22 09:45:35 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 0.6427
2022-02-22 09:46:16 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 0.5279
2022-02-22 09:46:58 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 0.7694
2022-02-22 09:47:39 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 0.7960
2022-02-22 09:48:20 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 0.9062
2022-02-22 09:49:02 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 0.6489
2022-02-22 09:49:43 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 0.7285
2022-02-22 09:50:25 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 0.7942
2022-02-22 09:51:06 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 0.6093
2022-02-22 09:51:48 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 0.8524
2022-02-22 09:52:29 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 0.6893
2022-02-22 09:53:10 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 0.7231
2022-02-22 09:53:52 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 0.9197
2022-02-22 09:54:33 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 0.6645
2022-02-22 09:55:15 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 0.7214
2022-02-22 09:55:56 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 0.6491
2022-02-22 09:56:37 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 0.7111
2022-02-22 09:57:19 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 0.7868
2022-02-22 09:58:00 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 0.9102
2022-02-22 09:58:42 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 0.8022
2022-02-22 09:59:24 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 0.9270
2022-02-22 10:00:05 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 0.5805
2022-02-22 10:00:47 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 0.7697
2022-02-22 10:01:29 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 0.8341
2022-02-22 10:02:11 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 0.9818
2022-02-22 10:02:53 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 0.6608
2022-02-22 10:03:34 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 0.8591
2022-02-22 10:04:16 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 0.8406
2022-02-22 10:04:58 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 0.6871
2022-02-22 10:05:40 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 0.9550
2022-02-22 10:06:23 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 0.6546
2022-02-22 10:07:05 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 0.6637
2022-02-22 10:07:47 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 0.7767
2022-02-22 10:08:28 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 0.8854
2022-02-22 10:09:10 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 0.8356
2022-02-22 10:09:52 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 0.7422
2022-02-22 10:10:34 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 0.9413
2022-02-22 10:11:16 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 0.8492
2022-02-22 10:11:58 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 0.6971
2022-02-22 10:12:40 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 0.7259
2022-02-22 10:13:22 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 0.7943
2022-02-22 10:14:05 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 0.7397
2022-02-22 10:14:47 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 0.6066
2022-02-22 10:15:30 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 0.7998
2022-02-22 10:16:16 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 0.8873
2022-02-22 10:17:04 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 0.9349
2022-02-22 10:17:07 - train: epoch 077, train_loss: 0.7552
2022-02-22 10:18:29 - eval: epoch: 077, acc1: 78.064%, acc5: 93.868%, test_loss: 0.8684, per_image_load_time: 0.723ms, per_image_inference_time: 0.933ms
2022-02-22 10:18:30 - until epoch: 077, best_acc1: 78.112%
2022-02-22 10:18:30 - epoch 078 lr: 0.0010000000000000002
2022-02-22 10:19:17 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 0.6984
2022-02-22 10:19:59 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 0.8641
2022-02-22 10:20:40 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 0.8930
2022-02-22 10:21:21 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 0.8395
2022-02-22 10:22:03 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 0.7501
2022-02-22 10:22:44 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 0.7955
2022-02-22 10:23:26 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 0.7613
2022-02-22 10:24:07 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 0.8810
2022-02-22 10:24:48 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 0.8538
2022-02-22 10:25:29 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 0.8015
2022-02-22 10:26:11 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 0.6111
2022-02-22 10:26:52 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 0.6049
2022-02-22 10:27:33 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 0.8545
2022-02-22 10:28:14 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 0.6967
2022-02-22 10:28:56 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 0.7586
2022-02-22 10:29:37 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 0.8076
2022-02-22 10:30:18 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 0.7500
2022-02-22 10:31:00 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 0.7479
2022-02-22 10:31:41 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 0.6780
2022-02-22 10:32:22 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 0.6444
2022-02-22 10:33:04 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 0.8989
2022-02-22 10:33:45 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 0.6895
2022-02-22 10:34:27 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 0.8696
2022-02-22 10:35:09 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 0.7309
2022-02-22 10:35:50 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 0.6732
2022-02-22 10:36:32 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 0.6531
2022-02-22 10:37:14 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 0.7308
2022-02-22 10:37:55 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 0.8427
2022-02-22 10:38:37 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 0.6636
2022-02-22 10:39:19 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 0.6702
2022-02-22 10:40:01 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 0.8502
2022-02-22 10:40:43 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 0.7136
2022-02-22 10:41:25 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 0.7246
2022-02-22 10:42:07 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 0.7388
2022-02-22 10:42:49 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 0.7894
2022-02-22 10:43:30 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 0.8074
2022-02-22 10:44:12 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 0.5894
2022-02-22 10:44:54 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 0.7493
2022-02-22 10:45:36 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 0.7593
2022-02-22 10:46:18 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 0.6825
2022-02-22 10:47:00 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 0.8072
2022-02-22 10:47:42 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 0.8801
2022-02-22 10:48:24 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 0.7418
2022-02-22 10:49:06 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 0.7298
2022-02-22 10:49:48 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 0.8172
2022-02-22 10:50:32 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 0.5960
2022-02-22 10:51:16 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 0.7538
2022-02-22 10:51:58 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 0.9493
2022-02-22 10:52:44 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 0.7667
2022-02-22 10:53:31 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 0.7914
2022-02-22 10:53:34 - train: epoch 078, train_loss: 0.7486
2022-02-22 10:54:57 - eval: epoch: 078, acc1: 77.952%, acc5: 93.974%, test_loss: 0.8689, per_image_load_time: 0.560ms, per_image_inference_time: 0.867ms
2022-02-22 10:54:58 - until epoch: 078, best_acc1: 78.112%
2022-02-22 10:54:58 - epoch 079 lr: 0.0010000000000000002
2022-02-22 10:55:46 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 0.7116
2022-02-22 10:56:28 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 0.6599
2022-02-22 10:57:10 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 0.7887
2022-02-22 10:57:51 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 0.7377
2022-02-22 10:58:33 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 0.8200
2022-02-22 10:59:15 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 0.7536
2022-02-22 10:59:58 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 0.6631
2022-02-22 11:00:40 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 0.8388
2022-02-22 11:01:22 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 0.6362
2022-02-22 11:02:03 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 0.6517
2022-02-22 11:02:45 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 0.8033
2022-02-22 11:03:27 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 0.9426
2022-02-22 11:04:09 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 0.6482
2022-02-22 11:04:51 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 0.7069
2022-02-22 11:05:32 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 0.6262
2022-02-22 11:06:14 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 0.6935
2022-02-22 11:06:56 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 0.5851
2022-02-22 11:07:38 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 0.7075
