2022-07-05 07:41:01 - network: resnet50
2022-07-05 07:41:01 - num_classes: 1000
2022-07-05 07:41:01 - input_image_size: 224
2022-07-05 07:41:01 - scale: 1.1428571428571428
2022-07-05 07:41:01 - trained_model_path: 
2022-07-05 07:41:01 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-05 07:41:01 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-05 07:41:01 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f35c0bf9d60>
2022-07-05 07:41:01 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f35a281a070>
2022-07-05 07:41:01 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f35a281a0a0>
2022-07-05 07:41:01 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f35a281a100>
2022-07-05 07:41:01 - seed: 0
2022-07-05 07:41:01 - batch_size: 256
2022-07-05 07:41:01 - num_workers: 16
2022-07-05 07:41:01 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-07-05 07:41:01 - scheduler: ('CosineLR', {'warm_up_epochs': 5})
2022-07-05 07:41:01 - epochs: 200
2022-07-05 07:41:01 - print_interval: 100
2022-07-05 07:41:01 - sync_bn: False
2022-07-05 07:41:01 - apex: True
2022-07-05 07:41:01 - use_ema_model: False
2022-07-05 07:41:01 - ema_model_decay: 0.9999
2022-07-05 07:41:01 - gpus_type: NVIDIA RTX A5000
2022-07-05 07:41:01 - gpus_num: 2
2022-07-05 07:41:01 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f359f3944b0>
2022-07-05 07:41:01 - --------------------parameters--------------------
2022-07-05 07:41:01 - name: conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-07-05 07:41:01 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-07-05 07:41:01 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-07-05 07:41:01 - name: fc.weight, grad: True
2022-07-05 07:41:01 - name: fc.bias, grad: True
2022-07-05 07:41:01 - --------------------buffers--------------------
2022-07-05 07:41:01 - name: conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-07-05 07:41:01 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-07-05 07:41:01 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-05 07:41:01 - -----------no weight decay layers--------------
2022-07-05 07:41:01 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-05 07:41:01 - -------------weight decay layers---------------
2022-07-05 07:41:01 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer1.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer2.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer3.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-05 07:41:01 - epoch 001 lr: 0.100000
2022-07-05 07:41:41 - train: epoch 0001, iter [00100, 05004], lr: 0.020400, loss: 6.9923
2022-07-05 07:42:14 - train: epoch 0001, iter [00200, 05004], lr: 0.020799, loss: 6.8999
2022-07-05 07:42:48 - train: epoch 0001, iter [00300, 05004], lr: 0.021199, loss: 6.7626
2022-07-05 07:43:21 - train: epoch 0001, iter [00400, 05004], lr: 0.021599, loss: 6.7691
2022-07-05 07:43:55 - train: epoch 0001, iter [00500, 05004], lr: 0.021998, loss: 6.6871
2022-07-05 07:44:29 - train: epoch 0001, iter [00600, 05004], lr: 0.022398, loss: 6.5274
2022-07-05 07:45:02 - train: epoch 0001, iter [00700, 05004], lr: 0.022798, loss: 6.5826
2022-07-05 07:45:36 - train: epoch 0001, iter [00800, 05004], lr: 0.023197, loss: 6.3897
2022-07-05 07:46:09 - train: epoch 0001, iter [00900, 05004], lr: 0.023597, loss: 6.3148
2022-07-05 07:46:44 - train: epoch 0001, iter [01000, 05004], lr: 0.023997, loss: 6.2939
2022-07-05 07:47:17 - train: epoch 0001, iter [01100, 05004], lr: 0.024396, loss: 6.1441
2022-07-05 07:47:52 - train: epoch 0001, iter [01200, 05004], lr: 0.024796, loss: 6.0691
2022-07-05 07:48:25 - train: epoch 0001, iter [01300, 05004], lr: 0.025196, loss: 6.0442
2022-07-05 07:48:59 - train: epoch 0001, iter [01400, 05004], lr: 0.025596, loss: 5.9416
2022-07-05 07:49:33 - train: epoch 0001, iter [01500, 05004], lr: 0.025995, loss: 5.8203
2022-07-05 07:50:07 - train: epoch 0001, iter [01600, 05004], lr: 0.026395, loss: 5.8946
2022-07-05 07:50:41 - train: epoch 0001, iter [01700, 05004], lr: 0.026795, loss: 5.6571
2022-07-05 07:51:15 - train: epoch 0001, iter [01800, 05004], lr: 0.027194, loss: 5.7148
2022-07-05 07:51:50 - train: epoch 0001, iter [01900, 05004], lr: 0.027594, loss: 5.6232
2022-07-05 07:52:23 - train: epoch 0001, iter [02000, 05004], lr: 0.027994, loss: 5.5038
2022-07-05 07:52:57 - train: epoch 0001, iter [02100, 05004], lr: 0.028393, loss: 5.5164
2022-07-05 07:53:31 - train: epoch 0001, iter [02200, 05004], lr: 0.028793, loss: 5.4547
2022-07-05 07:54:06 - train: epoch 0001, iter [02300, 05004], lr: 0.029193, loss: 5.3154
2022-07-05 07:54:38 - train: epoch 0001, iter [02400, 05004], lr: 0.029592, loss: 5.3197
2022-07-05 07:55:13 - train: epoch 0001, iter [02500, 05004], lr: 0.029992, loss: 5.3267
2022-07-05 07:55:47 - train: epoch 0001, iter [02600, 05004], lr: 0.030392, loss: 5.4576
2022-07-05 07:56:21 - train: epoch 0001, iter [02700, 05004], lr: 0.030791, loss: 5.3559
2022-07-05 07:56:56 - train: epoch 0001, iter [02800, 05004], lr: 0.031191, loss: 5.1936
2022-07-05 07:57:29 - train: epoch 0001, iter [02900, 05004], lr: 0.031591, loss: 5.1630
2022-07-05 07:58:03 - train: epoch 0001, iter [03000, 05004], lr: 0.031990, loss: 5.1851
2022-07-05 07:58:37 - train: epoch 0001, iter [03100, 05004], lr: 0.032390, loss: 5.2701
2022-07-05 07:59:11 - train: epoch 0001, iter [03200, 05004], lr: 0.032790, loss: 5.1015
2022-07-05 07:59:45 - train: epoch 0001, iter [03300, 05004], lr: 0.033189, loss: 4.9898
2022-07-05 08:00:19 - train: epoch 0001, iter [03400, 05004], lr: 0.033589, loss: 4.9974
2022-07-05 08:00:54 - train: epoch 0001, iter [03500, 05004], lr: 0.033989, loss: 4.9512
2022-07-05 08:01:29 - train: epoch 0001, iter [03600, 05004], lr: 0.034388, loss: 5.0198
2022-07-05 08:02:02 - train: epoch 0001, iter [03700, 05004], lr: 0.034788, loss: 5.1592
2022-07-05 08:02:36 - train: epoch 0001, iter [03800, 05004], lr: 0.035188, loss: 4.8270
2022-07-05 08:03:10 - train: epoch 0001, iter [03900, 05004], lr: 0.035588, loss: 4.8883
2022-07-05 08:03:43 - train: epoch 0001, iter [04000, 05004], lr: 0.035987, loss: 4.9208
2022-07-05 08:04:18 - train: epoch 0001, iter [04100, 05004], lr: 0.036387, loss: 4.9178
2022-07-05 08:04:52 - train: epoch 0001, iter [04200, 05004], lr: 0.036787, loss: 4.7527
2022-07-05 08:05:26 - train: epoch 0001, iter [04300, 05004], lr: 0.037186, loss: 4.6845
2022-07-05 08:06:01 - train: epoch 0001, iter [04400, 05004], lr: 0.037586, loss: 4.4771
2022-07-05 08:06:34 - train: epoch 0001, iter [04500, 05004], lr: 0.037986, loss: 4.7239
2022-07-05 08:07:09 - train: epoch 0001, iter [04600, 05004], lr: 0.038385, loss: 4.8697
2022-07-05 08:07:42 - train: epoch 0001, iter [04700, 05004], lr: 0.038785, loss: 4.4676
2022-07-05 08:08:17 - train: epoch 0001, iter [04800, 05004], lr: 0.039185, loss: 4.7053
2022-07-05 08:08:51 - train: epoch 0001, iter [04900, 05004], lr: 0.039584, loss: 4.5135
2022-07-05 08:09:24 - train: epoch 0001, iter [05000, 05004], lr: 0.039984, loss: 4.5643
2022-07-05 08:09:25 - train: epoch 001, train_loss: 5.4859
2022-07-05 08:10:40 - eval: epoch: 001, acc1: 13.694%, acc5: 32.272%, test_loss: 4.7912, per_image_load_time: 1.799ms, per_image_inference_time: 0.511ms
2022-07-05 08:10:40 - until epoch: 001, best_acc1: 13.694%
2022-07-05 08:10:40 - epoch 002 lr: 0.040004
2022-07-05 08:11:20 - train: epoch 0002, iter [00100, 05004], lr: 0.040400, loss: 4.4742
2022-07-05 08:11:53 - train: epoch 0002, iter [00200, 05004], lr: 0.040799, loss: 4.3196
2022-07-05 08:12:28 - train: epoch 0002, iter [00300, 05004], lr: 0.041199, loss: 4.6048
2022-07-05 08:13:02 - train: epoch 0002, iter [00400, 05004], lr: 0.041599, loss: 4.4603
2022-07-05 08:13:36 - train: epoch 0002, iter [00500, 05004], lr: 0.041998, loss: 4.2201
2022-07-05 08:14:09 - train: epoch 0002, iter [00600, 05004], lr: 0.042398, loss: 4.2898
2022-07-05 08:14:44 - train: epoch 0002, iter [00700, 05004], lr: 0.042798, loss: 4.4016
2022-07-05 08:15:18 - train: epoch 0002, iter [00800, 05004], lr: 0.043197, loss: 4.0547
2022-07-05 08:15:53 - train: epoch 0002, iter [00900, 05004], lr: 0.043597, loss: 3.8726
2022-07-05 08:16:28 - train: epoch 0002, iter [01000, 05004], lr: 0.043997, loss: 4.3401
2022-07-05 08:17:01 - train: epoch 0002, iter [01100, 05004], lr: 0.044396, loss: 4.4083
2022-07-05 08:17:35 - train: epoch 0002, iter [01200, 05004], lr: 0.044796, loss: 4.2040
2022-07-05 08:18:10 - train: epoch 0002, iter [01300, 05004], lr: 0.045196, loss: 4.1545
2022-07-05 08:18:44 - train: epoch 0002, iter [01400, 05004], lr: 0.045596, loss: 4.2233
2022-07-05 08:19:18 - train: epoch 0002, iter [01500, 05004], lr: 0.045995, loss: 4.1320
2022-07-05 08:19:53 - train: epoch 0002, iter [01600, 05004], lr: 0.046395, loss: 4.0435
2022-07-05 08:20:26 - train: epoch 0002, iter [01700, 05004], lr: 0.046795, loss: 4.0696
2022-07-05 08:21:01 - train: epoch 0002, iter [01800, 05004], lr: 0.047194, loss: 4.1636
2022-07-05 08:21:36 - train: epoch 0002, iter [01900, 05004], lr: 0.047594, loss: 3.8935
2022-07-05 08:22:09 - train: epoch 0002, iter [02000, 05004], lr: 0.047994, loss: 3.6813
2022-07-05 08:22:44 - train: epoch 0002, iter [02100, 05004], lr: 0.048393, loss: 3.9864
2022-07-05 08:23:18 - train: epoch 0002, iter [02200, 05004], lr: 0.048793, loss: 3.7877
2022-07-05 08:23:52 - train: epoch 0002, iter [02300, 05004], lr: 0.049193, loss: 3.9875
2022-07-05 08:24:27 - train: epoch 0002, iter [02400, 05004], lr: 0.049592, loss: 3.7925
2022-07-05 08:25:01 - train: epoch 0002, iter [02500, 05004], lr: 0.049992, loss: 3.7936
2022-07-05 08:25:35 - train: epoch 0002, iter [02600, 05004], lr: 0.050392, loss: 3.7646
2022-07-05 08:26:10 - train: epoch 0002, iter [02700, 05004], lr: 0.050791, loss: 3.9524
2022-07-05 08:26:44 - train: epoch 0002, iter [02800, 05004], lr: 0.051191, loss: 3.7971
2022-07-05 08:27:19 - train: epoch 0002, iter [02900, 05004], lr: 0.051591, loss: 3.7413
2022-07-05 08:27:53 - train: epoch 0002, iter [03000, 05004], lr: 0.051990, loss: 3.6424
2022-07-05 08:28:28 - train: epoch 0002, iter [03100, 05004], lr: 0.052390, loss: 3.7981
2022-07-05 08:29:02 - train: epoch 0002, iter [03200, 05004], lr: 0.052790, loss: 3.7832
2022-07-05 08:29:36 - train: epoch 0002, iter [03300, 05004], lr: 0.053189, loss: 3.7556
2022-07-05 08:30:11 - train: epoch 0002, iter [03400, 05004], lr: 0.053589, loss: 3.9123
2022-07-05 08:30:46 - train: epoch 0002, iter [03500, 05004], lr: 0.053989, loss: 3.7229
2022-07-05 08:31:20 - train: epoch 0002, iter [03600, 05004], lr: 0.054388, loss: 3.7292
2022-07-05 08:31:54 - train: epoch 0002, iter [03700, 05004], lr: 0.054788, loss: 3.7933
2022-07-05 08:32:29 - train: epoch 0002, iter [03800, 05004], lr: 0.055188, loss: 3.4597
2022-07-05 08:33:04 - train: epoch 0002, iter [03900, 05004], lr: 0.055588, loss: 3.6393
2022-07-05 08:33:37 - train: epoch 0002, iter [04000, 05004], lr: 0.055987, loss: 3.5967
2022-07-05 08:34:12 - train: epoch 0002, iter [04100, 05004], lr: 0.056387, loss: 3.6924
2022-07-05 08:34:47 - train: epoch 0002, iter [04200, 05004], lr: 0.056787, loss: 3.5020
2022-07-05 08:35:22 - train: epoch 0002, iter [04300, 05004], lr: 0.057186, loss: 3.6160
2022-07-05 08:35:57 - train: epoch 0002, iter [04400, 05004], lr: 0.057586, loss: 3.4478
2022-07-05 08:36:32 - train: epoch 0002, iter [04500, 05004], lr: 0.057986, loss: 3.4390
2022-07-05 08:37:07 - train: epoch 0002, iter [04600, 05004], lr: 0.058385, loss: 3.5321
2022-07-05 08:37:40 - train: epoch 0002, iter [04700, 05004], lr: 0.058785, loss: 5.6161
2022-07-05 08:38:15 - train: epoch 0002, iter [04800, 05004], lr: 0.059185, loss: 3.9182
2022-07-05 08:38:50 - train: epoch 0002, iter [04900, 05004], lr: 0.059584, loss: 3.8003
2022-07-05 08:39:22 - train: epoch 0002, iter [05000, 05004], lr: 0.059984, loss: 3.6840
2022-07-05 08:39:23 - train: epoch 002, train_loss: 3.9479
2022-07-05 08:40:38 - eval: epoch: 002, acc1: 24.212%, acc5: 48.404%, test_loss: 3.7168, per_image_load_time: 1.336ms, per_image_inference_time: 0.500ms
2022-07-05 08:40:38 - until epoch: 002, best_acc1: 24.212%
2022-07-05 08:40:38 - epoch 003 lr: 0.060004
2022-07-05 08:41:17 - train: epoch 0003, iter [00100, 05004], lr: 0.060400, loss: 3.6444
2022-07-05 08:41:51 - train: epoch 0003, iter [00200, 05004], lr: 0.060799, loss: 3.7076
2022-07-05 08:42:25 - train: epoch 0003, iter [00300, 05004], lr: 0.061199, loss: 3.5989
2022-07-05 08:42:59 - train: epoch 0003, iter [00400, 05004], lr: 0.061599, loss: 3.5652
2022-07-05 08:43:33 - train: epoch 0003, iter [00500, 05004], lr: 0.061998, loss: 3.6672
2022-07-05 08:44:08 - train: epoch 0003, iter [00600, 05004], lr: 0.062398, loss: 3.5857
2022-07-05 08:44:42 - train: epoch 0003, iter [00700, 05004], lr: 0.062798, loss: 3.7414
2022-07-05 08:45:16 - train: epoch 0003, iter [00800, 05004], lr: 0.063197, loss: 3.6205
2022-07-05 08:45:50 - train: epoch 0003, iter [00900, 05004], lr: 0.063597, loss: 3.4412
2022-07-05 08:46:23 - train: epoch 0003, iter [01000, 05004], lr: 0.063997, loss: 3.3531
2022-07-05 08:46:58 - train: epoch 0003, iter [01100, 05004], lr: 0.064396, loss: 3.4144
2022-07-05 08:47:33 - train: epoch 0003, iter [01200, 05004], lr: 0.064796, loss: 3.3089
2022-07-05 08:48:06 - train: epoch 0003, iter [01300, 05004], lr: 0.065196, loss: 3.4890
2022-07-05 08:48:40 - train: epoch 0003, iter [01400, 05004], lr: 0.065596, loss: 3.2922
2022-07-05 08:49:15 - train: epoch 0003, iter [01500, 05004], lr: 0.065995, loss: 3.6631
2022-07-05 08:49:50 - train: epoch 0003, iter [01600, 05004], lr: 0.066395, loss: 3.2714
2022-07-05 08:50:24 - train: epoch 0003, iter [01700, 05004], lr: 0.066795, loss: 3.2990
2022-07-05 08:50:58 - train: epoch 0003, iter [01800, 05004], lr: 0.067194, loss: 3.1863
2022-07-05 08:51:32 - train: epoch 0003, iter [01900, 05004], lr: 0.067594, loss: 3.4366
2022-07-05 08:52:07 - train: epoch 0003, iter [02000, 05004], lr: 0.067994, loss: 3.6766
2022-07-05 08:52:41 - train: epoch 0003, iter [02100, 05004], lr: 0.068393, loss: 3.6086
2022-07-05 08:53:15 - train: epoch 0003, iter [02200, 05004], lr: 0.068793, loss: 3.8007
2022-07-05 08:53:49 - train: epoch 0003, iter [02300, 05004], lr: 0.069193, loss: 3.3067
2022-07-05 08:54:23 - train: epoch 0003, iter [02400, 05004], lr: 0.069592, loss: 3.1912
2022-07-05 08:54:57 - train: epoch 0003, iter [02500, 05004], lr: 0.069992, loss: 3.2753
2022-07-05 08:55:31 - train: epoch 0003, iter [02600, 05004], lr: 0.070392, loss: 3.2414
2022-07-05 08:56:06 - train: epoch 0003, iter [02700, 05004], lr: 0.070791, loss: 3.6093
2022-07-05 08:56:41 - train: epoch 0003, iter [02800, 05004], lr: 0.071191, loss: 3.1541
2022-07-05 08:57:15 - train: epoch 0003, iter [02900, 05004], lr: 0.071591, loss: 3.2955
2022-07-05 08:57:49 - train: epoch 0003, iter [03000, 05004], lr: 0.071990, loss: 3.3667
2022-07-05 08:58:25 - train: epoch 0003, iter [03100, 05004], lr: 0.072390, loss: 3.4235
2022-07-05 08:58:58 - train: epoch 0003, iter [03200, 05004], lr: 0.072790, loss: 3.2604
2022-07-05 08:59:33 - train: epoch 0003, iter [03300, 05004], lr: 0.073189, loss: 3.3428
2022-07-05 09:00:07 - train: epoch 0003, iter [03400, 05004], lr: 0.073589, loss: 3.3036
2022-07-05 09:00:41 - train: epoch 0003, iter [03500, 05004], lr: 0.073989, loss: 3.0968
2022-07-05 09:01:16 - train: epoch 0003, iter [03600, 05004], lr: 0.074388, loss: 3.1364
2022-07-05 09:01:51 - train: epoch 0003, iter [03700, 05004], lr: 0.074788, loss: 3.1716
2022-07-05 09:02:25 - train: epoch 0003, iter [03800, 05004], lr: 0.075188, loss: 3.2187
2022-07-05 09:02:59 - train: epoch 0003, iter [03900, 05004], lr: 0.075588, loss: 3.5039
2022-07-05 09:03:33 - train: epoch 0003, iter [04000, 05004], lr: 0.075987, loss: 3.1475
2022-07-05 09:04:08 - train: epoch 0003, iter [04100, 05004], lr: 0.076387, loss: 3.1443
2022-07-05 09:04:43 - train: epoch 0003, iter [04200, 05004], lr: 0.076787, loss: 3.0693
2022-07-05 09:05:18 - train: epoch 0003, iter [04300, 05004], lr: 0.077186, loss: 2.8773
2022-07-05 09:05:52 - train: epoch 0003, iter [04400, 05004], lr: 0.077586, loss: 3.0059
2022-07-05 09:06:26 - train: epoch 0003, iter [04500, 05004], lr: 0.077986, loss: 3.1078
2022-07-05 09:07:01 - train: epoch 0003, iter [04600, 05004], lr: 0.078385, loss: 3.0541
2022-07-05 09:07:35 - train: epoch 0003, iter [04700, 05004], lr: 0.078785, loss: 2.9557
2022-07-05 09:08:10 - train: epoch 0003, iter [04800, 05004], lr: 0.079185, loss: 3.2492
2022-07-05 09:08:44 - train: epoch 0003, iter [04900, 05004], lr: 0.079584, loss: 3.2054
2022-07-05 09:09:17 - train: epoch 0003, iter [05000, 05004], lr: 0.079984, loss: 3.1554
2022-07-05 09:09:18 - train: epoch 003, train_loss: 3.3172
2022-07-05 09:10:34 - eval: epoch: 003, acc1: 33.092%, acc5: 58.720%, test_loss: 3.1693, per_image_load_time: 2.380ms, per_image_inference_time: 0.483ms
2022-07-05 09:10:34 - until epoch: 003, best_acc1: 33.092%
2022-07-05 09:10:34 - epoch 004 lr: 0.080004
2022-07-05 09:11:14 - train: epoch 0004, iter [00100, 05004], lr: 0.080400, loss: 3.1709
2022-07-05 09:11:47 - train: epoch 0004, iter [00200, 05004], lr: 0.080799, loss: 3.0285
2022-07-05 09:12:21 - train: epoch 0004, iter [00300, 05004], lr: 0.081199, loss: 3.1116
2022-07-05 09:12:55 - train: epoch 0004, iter [00400, 05004], lr: 0.081599, loss: 3.0159
2022-07-05 09:13:29 - train: epoch 0004, iter [00500, 05004], lr: 0.081998, loss: 2.9349
2022-07-05 09:14:03 - train: epoch 0004, iter [00600, 05004], lr: 0.082398, loss: 3.2625
2022-07-05 09:14:37 - train: epoch 0004, iter [00700, 05004], lr: 0.082798, loss: 3.1840
2022-07-05 09:15:11 - train: epoch 0004, iter [00800, 05004], lr: 0.083197, loss: 2.9143
2022-07-05 09:15:46 - train: epoch 0004, iter [00900, 05004], lr: 0.083597, loss: 2.7928
2022-07-05 09:16:20 - train: epoch 0004, iter [01000, 05004], lr: 0.083997, loss: 3.0202
2022-07-05 09:16:54 - train: epoch 0004, iter [01100, 05004], lr: 0.084396, loss: 3.1121
2022-07-05 09:17:29 - train: epoch 0004, iter [01200, 05004], lr: 0.084796, loss: 2.8241
2022-07-05 09:18:03 - train: epoch 0004, iter [01300, 05004], lr: 0.085196, loss: 2.8437
2022-07-05 09:18:37 - train: epoch 0004, iter [01400, 05004], lr: 0.085596, loss: 3.1017
2022-07-05 09:19:12 - train: epoch 0004, iter [01500, 05004], lr: 0.085995, loss: 3.0771
2022-07-05 09:19:46 - train: epoch 0004, iter [01600, 05004], lr: 0.086395, loss: 2.9641
2022-07-05 09:20:20 - train: epoch 0004, iter [01700, 05004], lr: 0.086795, loss: 3.0563
2022-07-05 09:20:56 - train: epoch 0004, iter [01800, 05004], lr: 0.087194, loss: 3.1479
2022-07-05 09:21:31 - train: epoch 0004, iter [01900, 05004], lr: 0.087594, loss: 2.9640
2022-07-05 09:22:05 - train: epoch 0004, iter [02000, 05004], lr: 0.087994, loss: 2.8624
2022-07-05 09:22:39 - train: epoch 0004, iter [02100, 05004], lr: 0.088393, loss: 2.9349
2022-07-05 09:23:14 - train: epoch 0004, iter [02200, 05004], lr: 0.088793, loss: 2.9773
2022-07-05 09:23:48 - train: epoch 0004, iter [02300, 05004], lr: 0.089193, loss: 2.8534
2022-07-05 09:24:22 - train: epoch 0004, iter [02400, 05004], lr: 0.089592, loss: 2.8688
2022-07-05 09:24:57 - train: epoch 0004, iter [02500, 05004], lr: 0.089992, loss: 2.9761
2022-07-05 09:25:32 - train: epoch 0004, iter [02600, 05004], lr: 0.090392, loss: 2.9783
2022-07-05 09:26:06 - train: epoch 0004, iter [02700, 05004], lr: 0.090791, loss: 2.7892
2022-07-05 09:26:41 - train: epoch 0004, iter [02800, 05004], lr: 0.091191, loss: 2.9120
2022-07-05 09:27:15 - train: epoch 0004, iter [02900, 05004], lr: 0.091591, loss: 2.7368
2022-07-05 09:27:49 - train: epoch 0004, iter [03000, 05004], lr: 0.091990, loss: 2.9621
2022-07-05 09:28:23 - train: epoch 0004, iter [03100, 05004], lr: 0.092390, loss: 2.9033
2022-07-05 09:28:58 - train: epoch 0004, iter [03200, 05004], lr: 0.092790, loss: 2.8901
2022-07-05 09:29:32 - train: epoch 0004, iter [03300, 05004], lr: 0.093189, loss: 2.9948
2022-07-05 09:30:07 - train: epoch 0004, iter [03400, 05004], lr: 0.093589, loss: 3.0447
2022-07-05 09:30:42 - train: epoch 0004, iter [03500, 05004], lr: 0.093989, loss: 2.7685
2022-07-05 09:31:15 - train: epoch 0004, iter [03600, 05004], lr: 0.094388, loss: 2.7657
2022-07-05 09:31:50 - train: epoch 0004, iter [03700, 05004], lr: 0.094788, loss: 2.9506
2022-07-05 09:32:25 - train: epoch 0004, iter [03800, 05004], lr: 0.095188, loss: 2.8819
2022-07-05 09:32:59 - train: epoch 0004, iter [03900, 05004], lr: 0.095588, loss: 2.8122
2022-07-05 09:33:34 - train: epoch 0004, iter [04000, 05004], lr: 0.095987, loss: 2.6165
2022-07-05 09:34:08 - train: epoch 0004, iter [04100, 05004], lr: 0.096387, loss: 2.9188
2022-07-05 09:34:43 - train: epoch 0004, iter [04200, 05004], lr: 0.096787, loss: 2.7712
2022-07-05 09:35:18 - train: epoch 0004, iter [04300, 05004], lr: 0.097186, loss: 2.6695
2022-07-05 09:35:52 - train: epoch 0004, iter [04400, 05004], lr: 0.097586, loss: 2.7826
2022-07-05 09:36:28 - train: epoch 0004, iter [04500, 05004], lr: 0.097986, loss: 2.3284
2022-07-05 09:37:02 - train: epoch 0004, iter [04600, 05004], lr: 0.098385, loss: 2.8879
2022-07-05 09:37:36 - train: epoch 0004, iter [04700, 05004], lr: 0.098785, loss: 2.7533
2022-07-05 09:38:11 - train: epoch 0004, iter [04800, 05004], lr: 0.099185, loss: 2.6603
2022-07-05 09:38:45 - train: epoch 0004, iter [04900, 05004], lr: 0.099584, loss: 2.8357
2022-07-05 09:39:18 - train: epoch 0004, iter [05000, 05004], lr: 0.099984, loss: 3.0047
2022-07-05 09:39:19 - train: epoch 004, train_loss: 2.9596
2022-07-05 09:40:35 - eval: epoch: 004, acc1: 37.160%, acc5: 63.490%, test_loss: 2.9154, per_image_load_time: 2.429ms, per_image_inference_time: 0.486ms
2022-07-05 09:40:35 - until epoch: 004, best_acc1: 37.160%
2022-07-05 09:40:35 - epoch 005 lr: 0.100004
2022-07-05 09:41:14 - train: epoch 0005, iter [00100, 05004], lr: 0.100400, loss: 2.9027
2022-07-05 09:41:49 - train: epoch 0005, iter [00200, 05004], lr: 0.100799, loss: 2.8922
2022-07-05 09:42:23 - train: epoch 0005, iter [00300, 05004], lr: 0.101199, loss: 2.9696
2022-07-05 09:42:57 - train: epoch 0005, iter [00400, 05004], lr: 0.101599, loss: 2.8266
2022-07-05 09:43:31 - train: epoch 0005, iter [00500, 05004], lr: 0.101998, loss: 2.6000
2022-07-05 09:44:05 - train: epoch 0005, iter [00600, 05004], lr: 0.102398, loss: 2.8169
2022-07-05 09:44:39 - train: epoch 0005, iter [00700, 05004], lr: 0.102798, loss: 2.8864
2022-07-05 09:45:14 - train: epoch 0005, iter [00800, 05004], lr: 0.103197, loss: 3.0657
2022-07-05 09:45:48 - train: epoch 0005, iter [00900, 05004], lr: 0.103597, loss: 2.7678
2022-07-05 09:46:22 - train: epoch 0005, iter [01000, 05004], lr: 0.103997, loss: 2.9060
2022-07-05 09:46:56 - train: epoch 0005, iter [01100, 05004], lr: 0.104396, loss: 2.9103
2022-07-05 09:47:30 - train: epoch 0005, iter [01200, 05004], lr: 0.104796, loss: 2.8991
2022-07-05 09:48:05 - train: epoch 0005, iter [01300, 05004], lr: 0.105196, loss: 2.7215
2022-07-05 09:48:39 - train: epoch 0005, iter [01400, 05004], lr: 0.105596, loss: 2.9289
2022-07-05 09:49:14 - train: epoch 0005, iter [01500, 05004], lr: 0.105995, loss: 2.6751
2022-07-05 09:49:47 - train: epoch 0005, iter [01600, 05004], lr: 0.106395, loss: 2.6069
2022-07-05 09:50:21 - train: epoch 0005, iter [01700, 05004], lr: 0.106795, loss: 2.6840
2022-07-05 09:50:56 - train: epoch 0005, iter [01800, 05004], lr: 0.107194, loss: 2.8744
2022-07-05 09:51:30 - train: epoch 0005, iter [01900, 05004], lr: 0.107594, loss: 2.6335
2022-07-05 09:52:04 - train: epoch 0005, iter [02000, 05004], lr: 0.107994, loss: 2.8532
2022-07-05 09:52:38 - train: epoch 0005, iter [02100, 05004], lr: 0.108393, loss: 2.6954
2022-07-05 09:53:12 - train: epoch 0005, iter [02200, 05004], lr: 0.108793, loss: 2.5379
2022-07-05 09:53:46 - train: epoch 0005, iter [02300, 05004], lr: 0.109193, loss: 2.5596
2022-07-05 09:54:19 - train: epoch 0005, iter [02400, 05004], lr: 0.109592, loss: 2.7590
2022-07-05 09:54:53 - train: epoch 0005, iter [02500, 05004], lr: 0.109992, loss: 2.9133
2022-07-05 09:55:26 - train: epoch 0005, iter [02600, 05004], lr: 0.110392, loss: 2.9594
2022-07-05 09:56:01 - train: epoch 0005, iter [02700, 05004], lr: 0.110791, loss: 2.9102
2022-07-05 09:56:35 - train: epoch 0005, iter [02800, 05004], lr: 0.111191, loss: 2.7110
2022-07-05 09:57:09 - train: epoch 0005, iter [02900, 05004], lr: 0.111591, loss: 2.7208
2022-07-05 09:57:42 - train: epoch 0005, iter [03000, 05004], lr: 0.111990, loss: 2.7933
2022-07-05 09:58:16 - train: epoch 0005, iter [03100, 05004], lr: 0.112390, loss: 2.8384
2022-07-05 09:58:49 - train: epoch 0005, iter [03200, 05004], lr: 0.112790, loss: 2.8693
2022-07-05 09:59:24 - train: epoch 0005, iter [03300, 05004], lr: 0.113189, loss: 2.6086
2022-07-05 09:59:58 - train: epoch 0005, iter [03400, 05004], lr: 0.113589, loss: 2.5772
2022-07-05 10:00:32 - train: epoch 0005, iter [03500, 05004], lr: 0.113989, loss: 2.6982
2022-07-05 10:01:06 - train: epoch 0005, iter [03600, 05004], lr: 0.114388, loss: 2.8278
2022-07-05 10:01:40 - train: epoch 0005, iter [03700, 05004], lr: 0.114788, loss: 2.5658
2022-07-05 10:02:14 - train: epoch 0005, iter [03800, 05004], lr: 0.115188, loss: 2.6390
2022-07-05 10:02:48 - train: epoch 0005, iter [03900, 05004], lr: 0.115588, loss: 3.0023
2022-07-05 10:03:23 - train: epoch 0005, iter [04000, 05004], lr: 0.115987, loss: 2.7538
2022-07-05 10:03:57 - train: epoch 0005, iter [04100, 05004], lr: 0.116387, loss: 2.7153
2022-07-05 10:04:30 - train: epoch 0005, iter [04200, 05004], lr: 0.116787, loss: 2.8568
2022-07-05 10:05:05 - train: epoch 0005, iter [04300, 05004], lr: 0.117186, loss: 2.6176
2022-07-05 10:05:39 - train: epoch 0005, iter [04400, 05004], lr: 0.117586, loss: 2.8339
2022-07-05 10:06:13 - train: epoch 0005, iter [04500, 05004], lr: 0.117986, loss: 2.8335
2022-07-05 10:06:47 - train: epoch 0005, iter [04600, 05004], lr: 0.118385, loss: 2.6830
2022-07-05 10:07:21 - train: epoch 0005, iter [04700, 05004], lr: 0.118785, loss: 2.5626
2022-07-05 10:07:56 - train: epoch 0005, iter [04800, 05004], lr: 0.119185, loss: 2.5821
2022-07-05 10:08:30 - train: epoch 0005, iter [04900, 05004], lr: 0.119584, loss: 2.8108
2022-07-05 10:09:03 - train: epoch 0005, iter [05000, 05004], lr: 0.119984, loss: 2.6826
2022-07-05 10:09:04 - train: epoch 005, train_loss: 2.7877
2022-07-05 10:10:19 - eval: epoch: 005, acc1: 41.794%, acc5: 68.764%, test_loss: 2.5960, per_image_load_time: 2.475ms, per_image_inference_time: 0.456ms
2022-07-05 10:10:20 - until epoch: 005, best_acc1: 41.794%
2022-07-05 10:10:20 - epoch 006 lr: 0.100000
2022-07-05 10:10:59 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.5554
2022-07-05 10:11:33 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.6189
2022-07-05 10:12:07 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.4785
2022-07-05 10:12:42 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.6774
2022-07-05 10:13:15 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.6444
2022-07-05 10:13:49 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.6022
2022-07-05 10:14:23 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.6492
2022-07-05 10:14:58 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.5413
2022-07-05 10:15:31 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.4993
2022-07-05 10:16:05 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.4852
2022-07-05 10:16:39 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.5389
2022-07-05 10:17:12 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.6818
2022-07-05 10:17:47 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.7138
2022-07-05 10:18:21 - train: epoch 0006, iter [01400, 05004], lr: 0.099999, loss: 2.6651
2022-07-05 10:18:54 - train: epoch 0006, iter [01500, 05004], lr: 0.099999, loss: 2.7481
2022-07-05 10:19:28 - train: epoch 0006, iter [01600, 05004], lr: 0.099999, loss: 2.3190
2022-07-05 10:20:03 - train: epoch 0006, iter [01700, 05004], lr: 0.099999, loss: 2.6984
2022-07-05 10:20:36 - train: epoch 0006, iter [01800, 05004], lr: 0.099999, loss: 2.6257
2022-07-05 10:21:10 - train: epoch 0006, iter [01900, 05004], lr: 0.099999, loss: 2.5643
2022-07-05 10:21:44 - train: epoch 0006, iter [02000, 05004], lr: 0.099999, loss: 2.7231
2022-07-05 10:22:19 - train: epoch 0006, iter [02100, 05004], lr: 0.099999, loss: 2.6589
2022-07-05 10:22:53 - train: epoch 0006, iter [02200, 05004], lr: 0.099999, loss: 2.4333
2022-07-05 10:23:28 - train: epoch 0006, iter [02300, 05004], lr: 0.099999, loss: 2.3994
2022-07-05 10:24:01 - train: epoch 0006, iter [02400, 05004], lr: 0.099999, loss: 2.5614
2022-07-05 10:24:35 - train: epoch 0006, iter [02500, 05004], lr: 0.099998, loss: 2.8038
2022-07-05 10:25:10 - train: epoch 0006, iter [02600, 05004], lr: 0.099998, loss: 2.5384
2022-07-05 10:25:44 - train: epoch 0006, iter [02700, 05004], lr: 0.099998, loss: 2.7351
2022-07-05 10:26:17 - train: epoch 0006, iter [02800, 05004], lr: 0.099998, loss: 2.3989
2022-07-05 10:26:52 - train: epoch 0006, iter [02900, 05004], lr: 0.099998, loss: 2.6632
2022-07-05 10:27:27 - train: epoch 0006, iter [03000, 05004], lr: 0.099998, loss: 2.5401
2022-07-05 10:28:00 - train: epoch 0006, iter [03100, 05004], lr: 0.099998, loss: 2.3206
2022-07-05 10:28:35 - train: epoch 0006, iter [03200, 05004], lr: 0.099997, loss: 2.4657
2022-07-05 10:29:09 - train: epoch 0006, iter [03300, 05004], lr: 0.099997, loss: 2.4221
2022-07-05 10:29:43 - train: epoch 0006, iter [03400, 05004], lr: 0.099997, loss: 2.7940
2022-07-05 10:30:16 - train: epoch 0006, iter [03500, 05004], lr: 0.099997, loss: 2.6399
2022-07-05 10:30:51 - train: epoch 0006, iter [03600, 05004], lr: 0.099997, loss: 2.5935
2022-07-05 10:31:25 - train: epoch 0006, iter [03700, 05004], lr: 0.099996, loss: 2.5022
2022-07-05 10:32:00 - train: epoch 0006, iter [03800, 05004], lr: 0.099996, loss: 2.3940
2022-07-05 10:32:33 - train: epoch 0006, iter [03900, 05004], lr: 0.099996, loss: 2.4483
2022-07-05 10:33:07 - train: epoch 0006, iter [04000, 05004], lr: 0.099996, loss: 2.7930
2022-07-05 10:33:41 - train: epoch 0006, iter [04100, 05004], lr: 0.099996, loss: 2.4025
2022-07-05 10:34:16 - train: epoch 0006, iter [04200, 05004], lr: 0.099995, loss: 2.3617
2022-07-05 10:34:50 - train: epoch 0006, iter [04300, 05004], lr: 0.099995, loss: 2.4642
2022-07-05 10:35:25 - train: epoch 0006, iter [04400, 05004], lr: 0.099995, loss: 2.5111
2022-07-05 10:35:59 - train: epoch 0006, iter [04500, 05004], lr: 0.099995, loss: 2.4914
2022-07-05 10:36:32 - train: epoch 0006, iter [04600, 05004], lr: 0.099995, loss: 2.5297
2022-07-05 10:37:07 - train: epoch 0006, iter [04700, 05004], lr: 0.099994, loss: 2.4952
2022-07-05 10:37:41 - train: epoch 0006, iter [04800, 05004], lr: 0.099994, loss: 2.4831
2022-07-05 10:38:15 - train: epoch 0006, iter [04900, 05004], lr: 0.099994, loss: 2.5348
2022-07-05 10:38:48 - train: epoch 0006, iter [05000, 05004], lr: 0.099994, loss: 2.4028
2022-07-05 10:38:49 - train: epoch 006, train_loss: 2.5642
2022-07-05 10:40:04 - eval: epoch: 006, acc1: 45.056%, acc5: 71.294%, test_loss: 2.4604, per_image_load_time: 1.816ms, per_image_inference_time: 0.502ms
2022-07-05 10:40:04 - until epoch: 006, best_acc1: 45.056%
2022-07-05 10:40:04 - epoch 007 lr: 0.099994
2022-07-05 10:40:43 - train: epoch 0007, iter [00100, 05004], lr: 0.099993, loss: 2.3600
2022-07-05 10:41:18 - train: epoch 0007, iter [00200, 05004], lr: 0.099993, loss: 2.6228
2022-07-05 10:41:52 - train: epoch 0007, iter [00300, 05004], lr: 0.099993, loss: 2.8007
2022-07-05 10:42:27 - train: epoch 0007, iter [00400, 05004], lr: 0.099992, loss: 2.5925
2022-07-05 10:43:01 - train: epoch 0007, iter [00500, 05004], lr: 0.099992, loss: 2.3925
2022-07-05 10:43:34 - train: epoch 0007, iter [00600, 05004], lr: 0.099992, loss: 2.6644
2022-07-05 10:44:09 - train: epoch 0007, iter [00700, 05004], lr: 0.099992, loss: 2.5711
2022-07-05 10:44:43 - train: epoch 0007, iter [00800, 05004], lr: 0.099991, loss: 2.4973
2022-07-05 10:45:17 - train: epoch 0007, iter [00900, 05004], lr: 0.099991, loss: 2.3447
2022-07-05 10:45:50 - train: epoch 0007, iter [01000, 05004], lr: 0.099991, loss: 2.4884
2022-07-05 10:46:26 - train: epoch 0007, iter [01100, 05004], lr: 0.099990, loss: 2.2449
2022-07-05 10:47:00 - train: epoch 0007, iter [01200, 05004], lr: 0.099990, loss: 2.4726
2022-07-05 10:47:34 - train: epoch 0007, iter [01300, 05004], lr: 0.099990, loss: 2.4832
2022-07-05 10:48:07 - train: epoch 0007, iter [01400, 05004], lr: 0.099989, loss: 2.4923
2022-07-05 10:48:41 - train: epoch 0007, iter [01500, 05004], lr: 0.099989, loss: 2.5469
2022-07-05 10:49:15 - train: epoch 0007, iter [01600, 05004], lr: 0.099989, loss: 2.4656
2022-07-05 10:49:50 - train: epoch 0007, iter [01700, 05004], lr: 0.099988, loss: 2.4915
2022-07-05 10:50:25 - train: epoch 0007, iter [01800, 05004], lr: 0.099988, loss: 2.4367
2022-07-05 10:50:59 - train: epoch 0007, iter [01900, 05004], lr: 0.099988, loss: 2.5021
2022-07-05 10:51:33 - train: epoch 0007, iter [02000, 05004], lr: 0.099987, loss: 2.2796
2022-07-05 10:52:07 - train: epoch 0007, iter [02100, 05004], lr: 0.099987, loss: 2.5700
2022-07-05 10:52:41 - train: epoch 0007, iter [02200, 05004], lr: 0.099987, loss: 2.3378
2022-07-05 10:53:16 - train: epoch 0007, iter [02300, 05004], lr: 0.099986, loss: 2.5040
2022-07-05 10:53:50 - train: epoch 0007, iter [02400, 05004], lr: 0.099986, loss: 2.4882
2022-07-05 10:54:24 - train: epoch 0007, iter [02500, 05004], lr: 0.099985, loss: 2.3916
2022-07-05 10:54:59 - train: epoch 0007, iter [02600, 05004], lr: 0.099985, loss: 2.4922
2022-07-05 10:55:34 - train: epoch 0007, iter [02700, 05004], lr: 0.099985, loss: 2.3154
2022-07-05 10:56:07 - train: epoch 0007, iter [02800, 05004], lr: 0.099984, loss: 2.4465
2022-07-05 10:56:42 - train: epoch 0007, iter [02900, 05004], lr: 0.099984, loss: 2.4510
2022-07-05 10:57:16 - train: epoch 0007, iter [03000, 05004], lr: 0.099983, loss: 2.5713
2022-07-05 10:57:51 - train: epoch 0007, iter [03100, 05004], lr: 0.099983, loss: 2.3313
2022-07-05 10:58:25 - train: epoch 0007, iter [03200, 05004], lr: 0.099983, loss: 2.4043
2022-07-05 10:58:59 - train: epoch 0007, iter [03300, 05004], lr: 0.099982, loss: 2.7918
2022-07-05 10:59:34 - train: epoch 0007, iter [03400, 05004], lr: 0.099982, loss: 2.3062
2022-07-05 11:00:09 - train: epoch 0007, iter [03500, 05004], lr: 0.099981, loss: 2.5288
2022-07-05 11:00:43 - train: epoch 0007, iter [03600, 05004], lr: 0.099981, loss: 2.2993
2022-07-05 11:01:17 - train: epoch 0007, iter [03700, 05004], lr: 0.099980, loss: 2.4788
2022-07-05 11:01:51 - train: epoch 0007, iter [03800, 05004], lr: 0.099980, loss: 2.7233
2022-07-05 11:02:26 - train: epoch 0007, iter [03900, 05004], lr: 0.099979, loss: 2.2897
2022-07-05 11:03:00 - train: epoch 0007, iter [04000, 05004], lr: 0.099979, loss: 2.5037
2022-07-05 11:03:35 - train: epoch 0007, iter [04100, 05004], lr: 0.099979, loss: 2.3278
2022-07-05 11:04:09 - train: epoch 0007, iter [04200, 05004], lr: 0.099978, loss: 2.4256
2022-07-05 11:04:43 - train: epoch 0007, iter [04300, 05004], lr: 0.099978, loss: 2.5193
2022-07-05 11:05:18 - train: epoch 0007, iter [04400, 05004], lr: 0.099977, loss: 2.2789
2022-07-05 11:05:52 - train: epoch 0007, iter [04500, 05004], lr: 0.099977, loss: 2.6330
2022-07-05 11:06:26 - train: epoch 0007, iter [04600, 05004], lr: 0.099976, loss: 2.5837
2022-07-05 11:07:01 - train: epoch 0007, iter [04700, 05004], lr: 0.099976, loss: 2.5347
2022-07-05 11:07:36 - train: epoch 0007, iter [04800, 05004], lr: 0.099975, loss: 2.7952
2022-07-05 11:08:09 - train: epoch 0007, iter [04900, 05004], lr: 0.099975, loss: 2.4818
2022-07-05 11:08:43 - train: epoch 0007, iter [05000, 05004], lr: 0.099974, loss: 2.4224
2022-07-05 11:08:44 - train: epoch 007, train_loss: 2.4689
2022-07-05 11:10:00 - eval: epoch: 007, acc1: 44.368%, acc5: 70.780%, test_loss: 2.4661, per_image_load_time: 2.410ms, per_image_inference_time: 0.511ms
2022-07-05 11:10:00 - until epoch: 007, best_acc1: 45.056%
2022-07-05 11:10:00 - epoch 008 lr: 0.099974
2022-07-05 11:10:39 - train: epoch 0008, iter [00100, 05004], lr: 0.099974, loss: 2.3203
2022-07-05 11:11:14 - train: epoch 0008, iter [00200, 05004], lr: 0.099973, loss: 2.6162
2022-07-05 11:11:45 - train: epoch 0008, iter [00300, 05004], lr: 0.099972, loss: 2.1413
2022-07-05 11:12:20 - train: epoch 0008, iter [00400, 05004], lr: 0.099972, loss: 2.2560
2022-07-05 11:12:53 - train: epoch 0008, iter [00500, 05004], lr: 0.099971, loss: 2.2547
2022-07-05 11:13:27 - train: epoch 0008, iter [00600, 05004], lr: 0.099971, loss: 2.2895
2022-07-05 11:14:01 - train: epoch 0008, iter [00700, 05004], lr: 0.099970, loss: 2.7171
2022-07-05 11:14:35 - train: epoch 0008, iter [00800, 05004], lr: 0.099970, loss: 2.2402
2022-07-05 11:15:10 - train: epoch 0008, iter [00900, 05004], lr: 0.099969, loss: 2.3993
2022-07-05 11:15:44 - train: epoch 0008, iter [01000, 05004], lr: 0.099969, loss: 2.3951
2022-07-05 11:16:17 - train: epoch 0008, iter [01100, 05004], lr: 0.099968, loss: 2.2432
2022-07-05 11:16:52 - train: epoch 0008, iter [01200, 05004], lr: 0.099967, loss: 2.2724
2022-07-05 11:17:26 - train: epoch 0008, iter [01300, 05004], lr: 0.099967, loss: 2.4925
2022-07-05 11:18:00 - train: epoch 0008, iter [01400, 05004], lr: 0.099966, loss: 2.3279
2022-07-05 11:18:35 - train: epoch 0008, iter [01500, 05004], lr: 0.099966, loss: 2.4829
2022-07-05 11:19:09 - train: epoch 0008, iter [01600, 05004], lr: 0.099965, loss: 2.5155
2022-07-05 11:19:43 - train: epoch 0008, iter [01700, 05004], lr: 0.099964, loss: 2.3163
2022-07-05 11:20:18 - train: epoch 0008, iter [01800, 05004], lr: 0.099964, loss: 2.5859
2022-07-05 11:20:52 - train: epoch 0008, iter [01900, 05004], lr: 0.099963, loss: 2.2021
2022-07-05 11:21:26 - train: epoch 0008, iter [02000, 05004], lr: 0.099963, loss: 2.4452
2022-07-05 11:22:01 - train: epoch 0008, iter [02100, 05004], lr: 0.099962, loss: 2.3691
2022-07-05 11:22:35 - train: epoch 0008, iter [02200, 05004], lr: 0.099961, loss: 2.2571
2022-07-05 11:23:09 - train: epoch 0008, iter [02300, 05004], lr: 0.099961, loss: 2.5135
2022-07-05 11:23:43 - train: epoch 0008, iter [02400, 05004], lr: 0.099960, loss: 2.3519
2022-07-05 11:24:18 - train: epoch 0008, iter [02500, 05004], lr: 0.099959, loss: 2.3396
2022-07-05 11:24:52 - train: epoch 0008, iter [02600, 05004], lr: 0.099959, loss: 2.6044
2022-07-05 11:25:27 - train: epoch 0008, iter [02700, 05004], lr: 0.099958, loss: 2.4875
2022-07-05 11:26:01 - train: epoch 0008, iter [02800, 05004], lr: 0.099957, loss: 2.4904
2022-07-05 11:26:34 - train: epoch 0008, iter [02900, 05004], lr: 0.099957, loss: 2.3404
2022-07-05 11:27:08 - train: epoch 0008, iter [03000, 05004], lr: 0.099956, loss: 2.4805
2022-07-05 11:27:43 - train: epoch 0008, iter [03100, 05004], lr: 0.099955, loss: 2.4438
2022-07-05 11:28:17 - train: epoch 0008, iter [03200, 05004], lr: 0.099955, loss: 2.6672
2022-07-05 11:28:52 - train: epoch 0008, iter [03300, 05004], lr: 0.099954, loss: 2.5554
2022-07-05 11:29:26 - train: epoch 0008, iter [03400, 05004], lr: 0.099953, loss: 2.5867
2022-07-05 11:30:01 - train: epoch 0008, iter [03500, 05004], lr: 0.099953, loss: 2.2653
2022-07-05 11:30:35 - train: epoch 0008, iter [03600, 05004], lr: 0.099952, loss: 2.5809
2022-07-05 11:31:09 - train: epoch 0008, iter [03700, 05004], lr: 0.099951, loss: 2.3690
2022-07-05 11:31:44 - train: epoch 0008, iter [03800, 05004], lr: 0.099951, loss: 2.2376
2022-07-05 11:32:19 - train: epoch 0008, iter [03900, 05004], lr: 0.099950, loss: 2.4612
2022-07-05 11:32:53 - train: epoch 0008, iter [04000, 05004], lr: 0.099949, loss: 2.6170
2022-07-05 11:33:27 - train: epoch 0008, iter [04100, 05004], lr: 0.099948, loss: 2.4500
2022-07-05 11:34:01 - train: epoch 0008, iter [04200, 05004], lr: 0.099948, loss: 2.2635
2022-07-05 11:34:36 - train: epoch 0008, iter [04300, 05004], lr: 0.099947, loss: 2.1224
2022-07-05 11:35:10 - train: epoch 0008, iter [04400, 05004], lr: 0.099946, loss: 2.2946
2022-07-05 11:35:44 - train: epoch 0008, iter [04500, 05004], lr: 0.099945, loss: 2.5937
2022-07-05 11:36:18 - train: epoch 0008, iter [04600, 05004], lr: 0.099945, loss: 2.5200
2022-07-05 11:36:53 - train: epoch 0008, iter [04700, 05004], lr: 0.099944, loss: 2.2658
2022-07-05 11:37:28 - train: epoch 0008, iter [04800, 05004], lr: 0.099943, loss: 2.4492
2022-07-05 11:38:02 - train: epoch 0008, iter [04900, 05004], lr: 0.099942, loss: 2.5067
2022-07-05 11:38:35 - train: epoch 0008, iter [05000, 05004], lr: 0.099942, loss: 2.3453
2022-07-05 11:38:37 - train: epoch 008, train_loss: 2.3936
2022-07-05 11:39:52 - eval: epoch: 008, acc1: 49.154%, acc5: 74.952%, test_loss: 2.2124, per_image_load_time: 1.567ms, per_image_inference_time: 0.492ms
2022-07-05 11:39:52 - until epoch: 008, best_acc1: 49.154%
2022-07-05 11:39:52 - epoch 009 lr: 0.099942
2022-07-05 11:40:32 - train: epoch 0009, iter [00100, 05004], lr: 0.099941, loss: 2.0908
2022-07-05 11:41:06 - train: epoch 0009, iter [00200, 05004], lr: 0.099940, loss: 2.1100
2022-07-05 11:41:39 - train: epoch 0009, iter [00300, 05004], lr: 0.099939, loss: 1.9898
2022-07-05 11:42:14 - train: epoch 0009, iter [00400, 05004], lr: 0.099938, loss: 2.4499
2022-07-05 11:42:47 - train: epoch 0009, iter [00500, 05004], lr: 0.099938, loss: 2.3903
2022-07-05 11:43:21 - train: epoch 0009, iter [00600, 05004], lr: 0.099937, loss: 2.3406
2022-07-05 11:43:55 - train: epoch 0009, iter [00700, 05004], lr: 0.099936, loss: 2.3222
2022-07-05 11:44:28 - train: epoch 0009, iter [00800, 05004], lr: 0.099935, loss: 2.2537
2022-07-05 11:45:02 - train: epoch 0009, iter [00900, 05004], lr: 0.099934, loss: 2.1811
2022-07-05 11:45:36 - train: epoch 0009, iter [01000, 05004], lr: 0.099934, loss: 2.1987
2022-07-05 11:46:10 - train: epoch 0009, iter [01100, 05004], lr: 0.099933, loss: 2.6372
2022-07-05 11:46:44 - train: epoch 0009, iter [01200, 05004], lr: 0.099932, loss: 2.4273
2022-07-05 11:47:18 - train: epoch 0009, iter [01300, 05004], lr: 0.099931, loss: 2.6172
2022-07-05 11:47:52 - train: epoch 0009, iter [01400, 05004], lr: 0.099930, loss: 2.0713
2022-07-05 11:48:27 - train: epoch 0009, iter [01500, 05004], lr: 0.099929, loss: 2.1915
2022-07-05 11:49:01 - train: epoch 0009, iter [01600, 05004], lr: 0.099929, loss: 2.3620
2022-07-05 11:49:34 - train: epoch 0009, iter [01700, 05004], lr: 0.099928, loss: 2.4826
2022-07-05 11:50:08 - train: epoch 0009, iter [01800, 05004], lr: 0.099927, loss: 2.3548
2022-07-05 11:50:43 - train: epoch 0009, iter [01900, 05004], lr: 0.099926, loss: 2.0467
2022-07-05 11:51:16 - train: epoch 0009, iter [02000, 05004], lr: 0.099925, loss: 2.1512
2022-07-05 11:51:51 - train: epoch 0009, iter [02100, 05004], lr: 0.099924, loss: 2.3578
2022-07-05 11:52:25 - train: epoch 0009, iter [02200, 05004], lr: 0.099923, loss: 2.4293
2022-07-05 11:52:59 - train: epoch 0009, iter [02300, 05004], lr: 0.099922, loss: 2.1082
2022-07-05 11:53:34 - train: epoch 0009, iter [02400, 05004], lr: 0.099921, loss: 2.2690
2022-07-05 11:54:07 - train: epoch 0009, iter [02500, 05004], lr: 0.099921, loss: 2.1979
2022-07-05 11:54:42 - train: epoch 0009, iter [02600, 05004], lr: 0.099920, loss: 2.3051
2022-07-05 11:55:16 - train: epoch 0009, iter [02700, 05004], lr: 0.099919, loss: 2.3503
2022-07-05 11:55:50 - train: epoch 0009, iter [02800, 05004], lr: 0.099918, loss: 2.3614
2022-07-05 11:56:24 - train: epoch 0009, iter [02900, 05004], lr: 0.099917, loss: 2.1428
2022-07-05 11:56:58 - train: epoch 0009, iter [03000, 05004], lr: 0.099916, loss: 2.2097
2022-07-05 11:57:32 - train: epoch 0009, iter [03100, 05004], lr: 0.099915, loss: 2.3743
2022-07-05 11:58:06 - train: epoch 0009, iter [03200, 05004], lr: 0.099914, loss: 2.3646
2022-07-05 11:58:40 - train: epoch 0009, iter [03300, 05004], lr: 0.099913, loss: 2.3224
2022-07-05 11:59:15 - train: epoch 0009, iter [03400, 05004], lr: 0.099912, loss: 2.5396
2022-07-05 11:59:49 - train: epoch 0009, iter [03500, 05004], lr: 0.099911, loss: 2.4275
2022-07-05 12:00:24 - train: epoch 0009, iter [03600, 05004], lr: 0.099910, loss: 2.2459
2022-07-05 12:00:57 - train: epoch 0009, iter [03700, 05004], lr: 0.099909, loss: 2.5107
2022-07-05 12:01:31 - train: epoch 0009, iter [03800, 05004], lr: 0.099908, loss: 2.5545
2022-07-05 12:02:05 - train: epoch 0009, iter [03900, 05004], lr: 0.099907, loss: 2.0082
2022-07-05 12:02:39 - train: epoch 0009, iter [04000, 05004], lr: 0.099906, loss: 2.4662
2022-07-05 12:03:13 - train: epoch 0009, iter [04100, 05004], lr: 0.099905, loss: 2.4702
2022-07-05 12:03:47 - train: epoch 0009, iter [04200, 05004], lr: 0.099904, loss: 2.1601
2022-07-05 12:04:22 - train: epoch 0009, iter [04300, 05004], lr: 0.099903, loss: 2.4962
2022-07-05 12:04:56 - train: epoch 0009, iter [04400, 05004], lr: 0.099902, loss: 2.2729
2022-07-05 12:05:29 - train: epoch 0009, iter [04500, 05004], lr: 0.099901, loss: 2.2779
2022-07-05 12:06:03 - train: epoch 0009, iter [04600, 05004], lr: 0.099900, loss: 2.3772
2022-07-05 12:06:37 - train: epoch 0009, iter [04700, 05004], lr: 0.099899, loss: 2.4914
2022-07-05 12:07:11 - train: epoch 0009, iter [04800, 05004], lr: 0.099898, loss: 2.5392
2022-07-05 12:07:44 - train: epoch 0009, iter [04900, 05004], lr: 0.099897, loss: 2.4296
2022-07-05 12:08:17 - train: epoch 0009, iter [05000, 05004], lr: 0.099896, loss: 2.2270
2022-07-05 12:08:18 - train: epoch 009, train_loss: 2.3344
2022-07-05 12:09:33 - eval: epoch: 009, acc1: 49.614%, acc5: 75.722%, test_loss: 2.1951, per_image_load_time: 2.417ms, per_image_inference_time: 0.488ms
2022-07-05 12:09:33 - until epoch: 009, best_acc1: 49.614%
2022-07-05 12:09:33 - epoch 010 lr: 0.099896
2022-07-05 12:10:14 - train: epoch 0010, iter [00100, 05004], lr: 0.099895, loss: 2.3105
2022-07-05 12:10:46 - train: epoch 0010, iter [00200, 05004], lr: 0.099894, loss: 2.3382
2022-07-05 12:11:21 - train: epoch 0010, iter [00300, 05004], lr: 0.099893, loss: 2.2312
2022-07-05 12:11:56 - train: epoch 0010, iter [00400, 05004], lr: 0.099892, loss: 2.3520
2022-07-05 12:12:29 - train: epoch 0010, iter [00500, 05004], lr: 0.099891, loss: 2.1708
2022-07-05 12:13:04 - train: epoch 0010, iter [00600, 05004], lr: 0.099890, loss: 2.4167
2022-07-05 12:13:38 - train: epoch 0010, iter [00700, 05004], lr: 0.099889, loss: 2.2975
2022-07-05 12:14:12 - train: epoch 0010, iter [00800, 05004], lr: 0.099888, loss: 2.0849
2022-07-05 12:14:46 - train: epoch 0010, iter [00900, 05004], lr: 0.099887, loss: 2.2098
2022-07-05 12:15:21 - train: epoch 0010, iter [01000, 05004], lr: 0.099886, loss: 2.1134
2022-07-05 12:15:55 - train: epoch 0010, iter [01100, 05004], lr: 0.099884, loss: 2.3086
2022-07-05 12:16:29 - train: epoch 0010, iter [01200, 05004], lr: 0.099883, loss: 2.1418
2022-07-05 12:17:04 - train: epoch 0010, iter [01300, 05004], lr: 0.099882, loss: 2.1084
2022-07-05 12:17:38 - train: epoch 0010, iter [01400, 05004], lr: 0.099881, loss: 2.3244
2022-07-05 12:18:12 - train: epoch 0010, iter [01500, 05004], lr: 0.099880, loss: 2.0294
2022-07-05 12:18:46 - train: epoch 0010, iter [01600, 05004], lr: 0.099879, loss: 2.3578
2022-07-05 12:19:20 - train: epoch 0010, iter [01700, 05004], lr: 0.099878, loss: 2.4450
2022-07-05 12:19:53 - train: epoch 0010, iter [01800, 05004], lr: 0.099877, loss: 2.2319
2022-07-05 12:20:28 - train: epoch 0010, iter [01900, 05004], lr: 0.099876, loss: 2.3839
2022-07-05 12:21:02 - train: epoch 0010, iter [02000, 05004], lr: 0.099874, loss: 2.3907
2022-07-05 12:21:36 - train: epoch 0010, iter [02100, 05004], lr: 0.099873, loss: 2.1540
2022-07-05 12:22:11 - train: epoch 0010, iter [02200, 05004], lr: 0.099872, loss: 2.4348
2022-07-05 12:22:45 - train: epoch 0010, iter [02300, 05004], lr: 0.099871, loss: 2.5528
2022-07-05 12:23:19 - train: epoch 0010, iter [02400, 05004], lr: 0.099870, loss: 2.4464
2022-07-05 12:23:55 - train: epoch 0010, iter [02500, 05004], lr: 0.099869, loss: 2.2505
2022-07-05 12:24:29 - train: epoch 0010, iter [02600, 05004], lr: 0.099868, loss: 2.3617
2022-07-05 12:25:03 - train: epoch 0010, iter [02700, 05004], lr: 0.099866, loss: 2.0629
2022-07-05 12:25:37 - train: epoch 0010, iter [02800, 05004], lr: 0.099865, loss: 2.2557
2022-07-05 12:26:11 - train: epoch 0010, iter [02900, 05004], lr: 0.099864, loss: 2.3675
2022-07-05 12:26:46 - train: epoch 0010, iter [03000, 05004], lr: 0.099863, loss: 2.2073
2022-07-05 12:27:20 - train: epoch 0010, iter [03100, 05004], lr: 0.099862, loss: 2.6324
2022-07-05 12:27:55 - train: epoch 0010, iter [03200, 05004], lr: 0.099860, loss: 2.3058
2022-07-05 12:28:30 - train: epoch 0010, iter [03300, 05004], lr: 0.099859, loss: 2.4094
2022-07-05 12:29:04 - train: epoch 0010, iter [03400, 05004], lr: 0.099858, loss: 2.4923
2022-07-05 12:29:39 - train: epoch 0010, iter [03500, 05004], lr: 0.099857, loss: 2.5908
2022-07-05 12:30:13 - train: epoch 0010, iter [03600, 05004], lr: 0.099856, loss: 2.5020
2022-07-05 12:30:48 - train: epoch 0010, iter [03700, 05004], lr: 0.099854, loss: 2.1273
2022-07-05 12:31:22 - train: epoch 0010, iter [03800, 05004], lr: 0.099853, loss: 2.3672
2022-07-05 12:31:56 - train: epoch 0010, iter [03900, 05004], lr: 0.099852, loss: 1.9740
2022-07-05 12:32:32 - train: epoch 0010, iter [04000, 05004], lr: 0.099851, loss: 2.2799
2022-07-05 12:33:05 - train: epoch 0010, iter [04100, 05004], lr: 0.099849, loss: 2.0838
2022-07-05 12:33:40 - train: epoch 0010, iter [04200, 05004], lr: 0.099848, loss: 2.3404
2022-07-05 12:34:14 - train: epoch 0010, iter [04300, 05004], lr: 0.099847, loss: 2.3778
2022-07-05 12:34:49 - train: epoch 0010, iter [04400, 05004], lr: 0.099846, loss: 2.3914
2022-07-05 12:35:23 - train: epoch 0010, iter [04500, 05004], lr: 0.099844, loss: 2.0262
2022-07-05 12:35:58 - train: epoch 0010, iter [04600, 05004], lr: 0.099843, loss: 2.3240
2022-07-05 12:36:33 - train: epoch 0010, iter [04700, 05004], lr: 0.099842, loss: 2.2966
2022-07-05 12:37:07 - train: epoch 0010, iter [04800, 05004], lr: 0.099840, loss: 2.1528
2022-07-05 12:37:43 - train: epoch 0010, iter [04900, 05004], lr: 0.099839, loss: 2.1959
2022-07-05 12:38:16 - train: epoch 0010, iter [05000, 05004], lr: 0.099838, loss: 2.1048
2022-07-05 12:38:17 - train: epoch 010, train_loss: 2.2862
2022-07-05 12:39:33 - eval: epoch: 010, acc1: 51.630%, acc5: 77.410%, test_loss: 2.0740, per_image_load_time: 2.487ms, per_image_inference_time: 0.460ms
2022-07-05 12:39:34 - until epoch: 010, best_acc1: 51.630%
2022-07-05 12:39:34 - epoch 011 lr: 0.099838
2022-07-05 12:40:13 - train: epoch 0011, iter [00100, 05004], lr: 0.099837, loss: 2.0507
2022-07-05 12:40:47 - train: epoch 0011, iter [00200, 05004], lr: 0.099835, loss: 2.3779
2022-07-05 12:41:21 - train: epoch 0011, iter [00300, 05004], lr: 0.099834, loss: 2.0057
2022-07-05 12:41:55 - train: epoch 0011, iter [00400, 05004], lr: 0.099833, loss: 2.4232
2022-07-05 12:42:29 - train: epoch 0011, iter [00500, 05004], lr: 0.099831, loss: 2.1294
2022-07-05 12:43:03 - train: epoch 0011, iter [00600, 05004], lr: 0.099830, loss: 2.2459
2022-07-05 12:43:37 - train: epoch 0011, iter [00700, 05004], lr: 0.099829, loss: 2.2366
2022-07-05 12:44:10 - train: epoch 0011, iter [00800, 05004], lr: 0.099827, loss: 2.3083
2022-07-05 12:44:44 - train: epoch 0011, iter [00900, 05004], lr: 0.099826, loss: 2.3449
2022-07-05 12:45:18 - train: epoch 0011, iter [01000, 05004], lr: 0.099825, loss: 2.2644
2022-07-05 12:45:53 - train: epoch 0011, iter [01100, 05004], lr: 0.099823, loss: 2.3159
2022-07-05 12:46:27 - train: epoch 0011, iter [01200, 05004], lr: 0.099822, loss: 2.6146
2022-07-05 12:47:01 - train: epoch 0011, iter [01300, 05004], lr: 0.099821, loss: 2.4758
2022-07-05 12:47:36 - train: epoch 0011, iter [01400, 05004], lr: 0.099819, loss: 2.4397
2022-07-05 12:48:10 - train: epoch 0011, iter [01500, 05004], lr: 0.099818, loss: 2.1755
2022-07-05 12:48:45 - train: epoch 0011, iter [01600, 05004], lr: 0.099816, loss: 2.3411
2022-07-05 12:49:19 - train: epoch 0011, iter [01700, 05004], lr: 0.099815, loss: 2.1863
2022-07-05 12:49:53 - train: epoch 0011, iter [01800, 05004], lr: 0.099814, loss: 2.1245
2022-07-05 12:50:28 - train: epoch 0011, iter [01900, 05004], lr: 0.099812, loss: 2.0627
2022-07-05 12:51:02 - train: epoch 0011, iter [02000, 05004], lr: 0.099811, loss: 2.3865
2022-07-05 12:51:36 - train: epoch 0011, iter [02100, 05004], lr: 0.099810, loss: 2.2009
2022-07-05 12:52:11 - train: epoch 0011, iter [02200, 05004], lr: 0.099808, loss: 2.1699
2022-07-05 12:52:46 - train: epoch 0011, iter [02300, 05004], lr: 0.099807, loss: 2.5681
2022-07-05 12:53:19 - train: epoch 0011, iter [02400, 05004], lr: 0.099805, loss: 2.0329
2022-07-05 12:53:54 - train: epoch 0011, iter [02500, 05004], lr: 0.099804, loss: 2.4800
2022-07-05 12:54:29 - train: epoch 0011, iter [02600, 05004], lr: 0.099802, loss: 2.2131
2022-07-05 12:55:03 - train: epoch 0011, iter [02700, 05004], lr: 0.099801, loss: 2.2972
2022-07-05 12:55:38 - train: epoch 0011, iter [02800, 05004], lr: 0.099800, loss: 1.9130
2022-07-05 12:56:12 - train: epoch 0011, iter [02900, 05004], lr: 0.099798, loss: 2.3413
2022-07-05 12:56:47 - train: epoch 0011, iter [03000, 05004], lr: 0.099797, loss: 2.4627
2022-07-05 12:57:22 - train: epoch 0011, iter [03100, 05004], lr: 0.099795, loss: 2.2425
2022-07-05 12:57:56 - train: epoch 0011, iter [03200, 05004], lr: 0.099794, loss: 2.0640
2022-07-05 12:58:31 - train: epoch 0011, iter [03300, 05004], lr: 0.099792, loss: 2.2836
2022-07-05 12:59:06 - train: epoch 0011, iter [03400, 05004], lr: 0.099791, loss: 2.1799
2022-07-05 12:59:40 - train: epoch 0011, iter [03500, 05004], lr: 0.099789, loss: 2.2227
2022-07-05 13:00:14 - train: epoch 0011, iter [03600, 05004], lr: 0.099788, loss: 2.1879
2022-07-05 13:00:48 - train: epoch 0011, iter [03700, 05004], lr: 0.099786, loss: 2.3030
2022-07-05 13:01:22 - train: epoch 0011, iter [03800, 05004], lr: 0.099785, loss: 2.0013
2022-07-05 13:01:56 - train: epoch 0011, iter [03900, 05004], lr: 0.099783, loss: 2.2951
2022-07-05 13:02:30 - train: epoch 0011, iter [04000, 05004], lr: 0.099782, loss: 2.1845
2022-07-05 13:03:04 - train: epoch 0011, iter [04100, 05004], lr: 0.099780, loss: 2.0166
2022-07-05 13:03:38 - train: epoch 0011, iter [04200, 05004], lr: 0.099779, loss: 2.1989
2022-07-05 13:04:13 - train: epoch 0011, iter [04300, 05004], lr: 0.099777, loss: 2.2371
2022-07-05 13:04:47 - train: epoch 0011, iter [04400, 05004], lr: 0.099776, loss: 2.1144
2022-07-05 13:05:21 - train: epoch 0011, iter [04500, 05004], lr: 0.099774, loss: 2.0843
2022-07-05 13:05:56 - train: epoch 0011, iter [04600, 05004], lr: 0.099773, loss: 2.1262
2022-07-05 13:06:30 - train: epoch 0011, iter [04700, 05004], lr: 0.099771, loss: 2.0488
2022-07-05 13:07:04 - train: epoch 0011, iter [04800, 05004], lr: 0.099770, loss: 2.0520
2022-07-05 13:07:38 - train: epoch 0011, iter [04900, 05004], lr: 0.099768, loss: 2.0800
2022-07-05 13:08:11 - train: epoch 0011, iter [05000, 05004], lr: 0.099767, loss: 2.0863
2022-07-05 13:08:12 - train: epoch 011, train_loss: 2.2436
2022-07-05 13:09:27 - eval: epoch: 011, acc1: 52.198%, acc5: 77.840%, test_loss: 2.0421, per_image_load_time: 1.039ms, per_image_inference_time: 0.492ms
2022-07-05 13:09:28 - until epoch: 011, best_acc1: 52.198%
2022-07-05 13:09:28 - epoch 012 lr: 0.099767
2022-07-05 13:10:06 - train: epoch 0012, iter [00100, 05004], lr: 0.099765, loss: 2.1687
2022-07-05 13:10:40 - train: epoch 0012, iter [00200, 05004], lr: 0.099763, loss: 2.1129
2022-07-05 13:11:13 - train: epoch 0012, iter [00300, 05004], lr: 0.099762, loss: 2.1870
2022-07-05 13:11:48 - train: epoch 0012, iter [00400, 05004], lr: 0.099760, loss: 2.1875
2022-07-05 13:12:21 - train: epoch 0012, iter [00500, 05004], lr: 0.099759, loss: 2.3794
2022-07-05 13:12:55 - train: epoch 0012, iter [00600, 05004], lr: 0.099757, loss: 1.9425
2022-07-05 13:13:29 - train: epoch 0012, iter [00700, 05004], lr: 0.099756, loss: 2.0981
2022-07-05 13:14:03 - train: epoch 0012, iter [00800, 05004], lr: 0.099754, loss: 2.2027
2022-07-05 13:14:37 - train: epoch 0012, iter [00900, 05004], lr: 0.099752, loss: 2.3755
2022-07-05 13:15:11 - train: epoch 0012, iter [01000, 05004], lr: 0.099751, loss: 2.0618
2022-07-05 13:15:45 - train: epoch 0012, iter [01100, 05004], lr: 0.099749, loss: 2.4664
2022-07-05 13:16:19 - train: epoch 0012, iter [01200, 05004], lr: 0.099748, loss: 2.0599
2022-07-05 13:16:53 - train: epoch 0012, iter [01300, 05004], lr: 0.099746, loss: 2.2099
2022-07-05 13:17:27 - train: epoch 0012, iter [01400, 05004], lr: 0.099744, loss: 2.3309
2022-07-05 13:18:02 - train: epoch 0012, iter [01500, 05004], lr: 0.099743, loss: 2.0066
2022-07-05 13:18:36 - train: epoch 0012, iter [01600, 05004], lr: 0.099741, loss: 2.1438
2022-07-05 13:19:10 - train: epoch 0012, iter [01700, 05004], lr: 0.099739, loss: 2.0687
2022-07-05 13:19:44 - train: epoch 0012, iter [01800, 05004], lr: 0.099738, loss: 2.2432
2022-07-05 13:20:19 - train: epoch 0012, iter [01900, 05004], lr: 0.099736, loss: 2.2033
2022-07-05 13:20:53 - train: epoch 0012, iter [02000, 05004], lr: 0.099734, loss: 2.3027
2022-07-05 13:21:28 - train: epoch 0012, iter [02100, 05004], lr: 0.099733, loss: 2.1426
2022-07-05 13:22:01 - train: epoch 0012, iter [02200, 05004], lr: 0.099731, loss: 2.4171
2022-07-05 13:22:36 - train: epoch 0012, iter [02300, 05004], lr: 0.099729, loss: 2.2135
2022-07-05 13:23:11 - train: epoch 0012, iter [02400, 05004], lr: 0.099728, loss: 2.2347
2022-07-05 13:23:45 - train: epoch 0012, iter [02500, 05004], lr: 0.099726, loss: 2.0197
2022-07-05 13:24:19 - train: epoch 0012, iter [02600, 05004], lr: 0.099724, loss: 2.0073
2022-07-05 13:24:54 - train: epoch 0012, iter [02700, 05004], lr: 0.099723, loss: 2.1972
2022-07-05 13:25:29 - train: epoch 0012, iter [02800, 05004], lr: 0.099721, loss: 2.1602
2022-07-05 13:26:03 - train: epoch 0012, iter [02900, 05004], lr: 0.099719, loss: 2.0516
2022-07-05 13:26:36 - train: epoch 0012, iter [03000, 05004], lr: 0.099718, loss: 2.1241
2022-07-05 13:27:11 - train: epoch 0012, iter [03100, 05004], lr: 0.099716, loss: 2.3352
2022-07-05 13:27:45 - train: epoch 0012, iter [03200, 05004], lr: 0.099714, loss: 1.9639
2022-07-05 13:28:19 - train: epoch 0012, iter [03300, 05004], lr: 0.099713, loss: 2.3013
2022-07-05 13:28:53 - train: epoch 0012, iter [03400, 05004], lr: 0.099711, loss: 2.1653
2022-07-05 13:29:28 - train: epoch 0012, iter [03500, 05004], lr: 0.099709, loss: 2.3578
2022-07-05 13:30:03 - train: epoch 0012, iter [03600, 05004], lr: 0.099707, loss: 2.2381
2022-07-05 13:30:36 - train: epoch 0012, iter [03700, 05004], lr: 0.099706, loss: 2.1914
2022-07-05 13:31:11 - train: epoch 0012, iter [03800, 05004], lr: 0.099704, loss: 2.1788
2022-07-05 13:31:45 - train: epoch 0012, iter [03900, 05004], lr: 0.099702, loss: 2.0806
2022-07-05 13:32:19 - train: epoch 0012, iter [04000, 05004], lr: 0.099700, loss: 2.1109
2022-07-05 13:32:53 - train: epoch 0012, iter [04100, 05004], lr: 0.099699, loss: 2.0454
2022-07-05 13:33:27 - train: epoch 0012, iter [04200, 05004], lr: 0.099697, loss: 2.0022
2022-07-05 13:34:01 - train: epoch 0012, iter [04300, 05004], lr: 0.099695, loss: 2.3216
2022-07-05 13:34:36 - train: epoch 0012, iter [04400, 05004], lr: 0.099693, loss: 2.0779
2022-07-05 13:35:10 - train: epoch 0012, iter [04500, 05004], lr: 0.099691, loss: 2.1086
2022-07-05 13:35:44 - train: epoch 0012, iter [04600, 05004], lr: 0.099690, loss: 2.3907
2022-07-05 13:36:18 - train: epoch 0012, iter [04700, 05004], lr: 0.099688, loss: 2.1980
2022-07-05 13:36:53 - train: epoch 0012, iter [04800, 05004], lr: 0.099686, loss: 2.3676
2022-07-05 13:37:27 - train: epoch 0012, iter [04900, 05004], lr: 0.099684, loss: 2.2003
2022-07-05 13:38:00 - train: epoch 0012, iter [05000, 05004], lr: 0.099682, loss: 2.0345
2022-07-05 13:38:01 - train: epoch 012, train_loss: 2.2108
2022-07-05 13:39:17 - eval: epoch: 012, acc1: 53.396%, acc5: 78.640%, test_loss: 1.9918, per_image_load_time: 2.376ms, per_image_inference_time: 0.472ms
2022-07-05 13:39:17 - until epoch: 012, best_acc1: 53.396%
2022-07-05 13:39:17 - epoch 013 lr: 0.099682
2022-07-05 13:39:57 - train: epoch 0013, iter [00100, 05004], lr: 0.099681, loss: 1.9512
2022-07-05 13:40:32 - train: epoch 0013, iter [00200, 05004], lr: 0.099679, loss: 2.1150
2022-07-05 13:41:06 - train: epoch 0013, iter [00300, 05004], lr: 0.099677, loss: 2.1986
2022-07-05 13:41:40 - train: epoch 0013, iter [00400, 05004], lr: 0.099675, loss: 2.0294
2022-07-05 13:42:13 - train: epoch 0013, iter [00500, 05004], lr: 0.099673, loss: 2.2256
2022-07-05 13:42:46 - train: epoch 0013, iter [00600, 05004], lr: 0.099671, loss: 2.2947
2022-07-05 13:43:20 - train: epoch 0013, iter [00700, 05004], lr: 0.099670, loss: 2.1338
2022-07-05 13:43:55 - train: epoch 0013, iter [00800, 05004], lr: 0.099668, loss: 2.2057
2022-07-05 13:44:29 - train: epoch 0013, iter [00900, 05004], lr: 0.099666, loss: 2.2155
2022-07-05 13:45:03 - train: epoch 0013, iter [01000, 05004], lr: 0.099664, loss: 2.1606
2022-07-05 13:45:37 - train: epoch 0013, iter [01100, 05004], lr: 0.099662, loss: 2.1418
2022-07-05 13:46:11 - train: epoch 0013, iter [01200, 05004], lr: 0.099660, loss: 2.3636
2022-07-05 13:46:45 - train: epoch 0013, iter [01300, 05004], lr: 0.099658, loss: 2.3016
2022-07-05 13:47:19 - train: epoch 0013, iter [01400, 05004], lr: 0.099657, loss: 2.0104
2022-07-05 13:47:54 - train: epoch 0013, iter [01500, 05004], lr: 0.099655, loss: 2.2624
2022-07-05 13:48:28 - train: epoch 0013, iter [01600, 05004], lr: 0.099653, loss: 1.8973
2022-07-05 13:49:03 - train: epoch 0013, iter [01700, 05004], lr: 0.099651, loss: 2.1203
2022-07-05 13:49:36 - train: epoch 0013, iter [01800, 05004], lr: 0.099649, loss: 2.2385
2022-07-05 13:50:11 - train: epoch 0013, iter [01900, 05004], lr: 0.099647, loss: 2.1939
2022-07-05 13:50:45 - train: epoch 0013, iter [02000, 05004], lr: 0.099645, loss: 2.4088
2022-07-05 13:51:19 - train: epoch 0013, iter [02100, 05004], lr: 0.099643, loss: 2.3765
2022-07-05 13:51:54 - train: epoch 0013, iter [02200, 05004], lr: 0.099641, loss: 2.0393
2022-07-05 13:52:28 - train: epoch 0013, iter [02300, 05004], lr: 0.099639, loss: 2.2394
2022-07-05 13:53:02 - train: epoch 0013, iter [02400, 05004], lr: 0.099637, loss: 2.1901
2022-07-05 13:53:37 - train: epoch 0013, iter [02500, 05004], lr: 0.099635, loss: 2.1822
2022-07-05 13:54:11 - train: epoch 0013, iter [02600, 05004], lr: 0.099634, loss: 2.1583
2022-07-05 13:54:45 - train: epoch 0013, iter [02700, 05004], lr: 0.099632, loss: 1.9883
2022-07-05 13:55:19 - train: epoch 0013, iter [02800, 05004], lr: 0.099630, loss: 2.2663
2022-07-05 13:55:55 - train: epoch 0013, iter [02900, 05004], lr: 0.099628, loss: 2.2747
2022-07-05 13:56:29 - train: epoch 0013, iter [03000, 05004], lr: 0.099626, loss: 2.0177
2022-07-05 13:57:02 - train: epoch 0013, iter [03100, 05004], lr: 0.099624, loss: 2.0403
2022-07-05 13:57:37 - train: epoch 0013, iter [03200, 05004], lr: 0.099622, loss: 2.2700
2022-07-05 13:58:11 - train: epoch 0013, iter [03300, 05004], lr: 0.099620, loss: 1.9921
2022-07-05 13:58:45 - train: epoch 0013, iter [03400, 05004], lr: 0.099618, loss: 2.1247
2022-07-05 13:59:18 - train: epoch 0013, iter [03500, 05004], lr: 0.099616, loss: 2.0025
2022-07-05 13:59:53 - train: epoch 0013, iter [03600, 05004], lr: 0.099614, loss: 2.5753
2022-07-05 14:00:27 - train: epoch 0013, iter [03700, 05004], lr: 0.099612, loss: 1.8466
2022-07-05 14:01:01 - train: epoch 0013, iter [03800, 05004], lr: 0.099610, loss: 2.3499
2022-07-05 14:01:35 - train: epoch 0013, iter [03900, 05004], lr: 0.099608, loss: 2.2405
2022-07-05 14:02:09 - train: epoch 0013, iter [04000, 05004], lr: 0.099606, loss: 2.2306
2022-07-05 14:02:44 - train: epoch 0013, iter [04100, 05004], lr: 0.099604, loss: 2.0730
2022-07-05 14:03:17 - train: epoch 0013, iter [04200, 05004], lr: 0.099602, loss: 2.0973
2022-07-05 14:03:52 - train: epoch 0013, iter [04300, 05004], lr: 0.099600, loss: 2.0478
2022-07-05 14:04:26 - train: epoch 0013, iter [04400, 05004], lr: 0.099598, loss: 2.0552
2022-07-05 14:05:00 - train: epoch 0013, iter [04500, 05004], lr: 0.099596, loss: 2.1572
2022-07-05 14:05:35 - train: epoch 0013, iter [04600, 05004], lr: 0.099594, loss: 2.2354
2022-07-05 14:06:09 - train: epoch 0013, iter [04700, 05004], lr: 0.099592, loss: 2.3252
2022-07-05 14:06:44 - train: epoch 0013, iter [04800, 05004], lr: 0.099589, loss: 2.3288
2022-07-05 14:07:18 - train: epoch 0013, iter [04900, 05004], lr: 0.099587, loss: 2.2707
2022-07-05 14:07:51 - train: epoch 0013, iter [05000, 05004], lr: 0.099585, loss: 2.3059
2022-07-05 14:07:52 - train: epoch 013, train_loss: 2.1837
2022-07-05 14:09:07 - eval: epoch: 013, acc1: 53.582%, acc5: 78.596%, test_loss: 2.0015, per_image_load_time: 2.346ms, per_image_inference_time: 0.481ms
2022-07-05 14:09:07 - until epoch: 013, best_acc1: 53.582%
2022-07-05 14:09:07 - epoch 014 lr: 0.099585
2022-07-05 14:09:47 - train: epoch 0014, iter [00100, 05004], lr: 0.099583, loss: 2.2139
2022-07-05 14:10:21 - train: epoch 0014, iter [00200, 05004], lr: 0.099581, loss: 2.2537
2022-07-05 14:10:55 - train: epoch 0014, iter [00300, 05004], lr: 0.099579, loss: 1.8617
2022-07-05 14:11:29 - train: epoch 0014, iter [00400, 05004], lr: 0.099577, loss: 2.0481
2022-07-05 14:12:03 - train: epoch 0014, iter [00500, 05004], lr: 0.099575, loss: 2.0524
2022-07-05 14:12:37 - train: epoch 0014, iter [00600, 05004], lr: 0.099573, loss: 2.1130
2022-07-05 14:13:11 - train: epoch 0014, iter [00700, 05004], lr: 0.099571, loss: 2.1424
2022-07-05 14:13:45 - train: epoch 0014, iter [00800, 05004], lr: 0.099569, loss: 2.1870
2022-07-05 14:14:19 - train: epoch 0014, iter [00900, 05004], lr: 0.099566, loss: 2.2980
2022-07-05 14:14:54 - train: epoch 0014, iter [01000, 05004], lr: 0.099564, loss: 2.2771
2022-07-05 14:15:28 - train: epoch 0014, iter [01100, 05004], lr: 0.099562, loss: 2.1628
2022-07-05 14:16:03 - train: epoch 0014, iter [01200, 05004], lr: 0.099560, loss: 2.1381
2022-07-05 14:16:37 - train: epoch 0014, iter [01300, 05004], lr: 0.099558, loss: 2.1722
2022-07-05 14:17:11 - train: epoch 0014, iter [01400, 05004], lr: 0.099556, loss: 2.2313
2022-07-05 14:17:45 - train: epoch 0014, iter [01500, 05004], lr: 0.099554, loss: 2.3342
2022-07-05 14:18:19 - train: epoch 0014, iter [01600, 05004], lr: 0.099552, loss: 2.1272
2022-07-05 14:18:53 - train: epoch 0014, iter [01700, 05004], lr: 0.099549, loss: 2.4020
2022-07-05 14:19:28 - train: epoch 0014, iter [01800, 05004], lr: 0.099547, loss: 2.3581
2022-07-05 14:20:03 - train: epoch 0014, iter [01900, 05004], lr: 0.099545, loss: 2.0233
2022-07-05 14:20:36 - train: epoch 0014, iter [02000, 05004], lr: 0.099543, loss: 2.1569
2022-07-05 14:21:11 - train: epoch 0014, iter [02100, 05004], lr: 0.099541, loss: 2.2867
2022-07-05 14:21:46 - train: epoch 0014, iter [02200, 05004], lr: 0.099539, loss: 2.1689
2022-07-05 14:22:20 - train: epoch 0014, iter [02300, 05004], lr: 0.099536, loss: 2.1509
2022-07-05 14:22:54 - train: epoch 0014, iter [02400, 05004], lr: 0.099534, loss: 2.3368
2022-07-05 14:23:28 - train: epoch 0014, iter [02500, 05004], lr: 0.099532, loss: 2.0663
2022-07-05 14:24:02 - train: epoch 0014, iter [02600, 05004], lr: 0.099530, loss: 2.1390
2022-07-05 14:24:38 - train: epoch 0014, iter [02700, 05004], lr: 0.099528, loss: 2.1395
2022-07-05 14:25:12 - train: epoch 0014, iter [02800, 05004], lr: 0.099525, loss: 2.5232
2022-07-05 14:25:46 - train: epoch 0014, iter [02900, 05004], lr: 0.099523, loss: 2.0399
2022-07-05 14:26:21 - train: epoch 0014, iter [03000, 05004], lr: 0.099521, loss: 2.2472
2022-07-05 14:26:55 - train: epoch 0014, iter [03100, 05004], lr: 0.099519, loss: 1.9640
2022-07-05 14:27:29 - train: epoch 0014, iter [03200, 05004], lr: 0.099516, loss: 2.0914
2022-07-05 14:28:04 - train: epoch 0014, iter [03300, 05004], lr: 0.099514, loss: 1.9941
2022-07-05 14:28:38 - train: epoch 0014, iter [03400, 05004], lr: 0.099512, loss: 2.0615
2022-07-05 14:29:12 - train: epoch 0014, iter [03500, 05004], lr: 0.099510, loss: 2.0493
2022-07-05 14:29:46 - train: epoch 0014, iter [03600, 05004], lr: 0.099507, loss: 2.0300
2022-07-05 14:30:21 - train: epoch 0014, iter [03700, 05004], lr: 0.099505, loss: 2.0345
2022-07-05 14:30:55 - train: epoch 0014, iter [03800, 05004], lr: 0.099503, loss: 2.2984
2022-07-05 14:31:29 - train: epoch 0014, iter [03900, 05004], lr: 0.099501, loss: 2.0110
2022-07-05 14:32:04 - train: epoch 0014, iter [04000, 05004], lr: 0.099498, loss: 2.2882
2022-07-05 14:32:38 - train: epoch 0014, iter [04100, 05004], lr: 0.099496, loss: 2.0521
2022-07-05 14:33:12 - train: epoch 0014, iter [04200, 05004], lr: 0.099494, loss: 2.0184
2022-07-05 14:33:47 - train: epoch 0014, iter [04300, 05004], lr: 0.099492, loss: 2.0006
2022-07-05 14:34:22 - train: epoch 0014, iter [04400, 05004], lr: 0.099489, loss: 1.8884
2022-07-05 14:34:56 - train: epoch 0014, iter [04500, 05004], lr: 0.099487, loss: 2.0375
2022-07-05 14:35:30 - train: epoch 0014, iter [04600, 05004], lr: 0.099485, loss: 2.0435
2022-07-05 14:36:04 - train: epoch 0014, iter [04700, 05004], lr: 0.099482, loss: 2.0515
2022-07-05 14:36:39 - train: epoch 0014, iter [04800, 05004], lr: 0.099480, loss: 2.0332
2022-07-05 14:37:13 - train: epoch 0014, iter [04900, 05004], lr: 0.099478, loss: 1.9431
2022-07-05 14:37:46 - train: epoch 0014, iter [05000, 05004], lr: 0.099475, loss: 2.2161
2022-07-05 14:37:47 - train: epoch 014, train_loss: 2.1571
2022-07-05 14:39:03 - eval: epoch: 014, acc1: 53.766%, acc5: 79.104%, test_loss: 1.9802, per_image_load_time: 2.171ms, per_image_inference_time: 0.479ms
2022-07-05 14:39:04 - until epoch: 014, best_acc1: 53.766%
2022-07-05 14:39:04 - epoch 015 lr: 0.099475
2022-07-05 14:39:44 - train: epoch 0015, iter [00100, 05004], lr: 0.099473, loss: 1.8779
2022-07-05 14:40:17 - train: epoch 0015, iter [00200, 05004], lr: 0.099471, loss: 2.2623
2022-07-05 14:40:51 - train: epoch 0015, iter [00300, 05004], lr: 0.099468, loss: 2.2609
2022-07-05 14:41:25 - train: epoch 0015, iter [00400, 05004], lr: 0.099466, loss: 2.0727
2022-07-05 14:41:58 - train: epoch 0015, iter [00500, 05004], lr: 0.099464, loss: 2.1362
2022-07-05 14:42:32 - train: epoch 0015, iter [00600, 05004], lr: 0.099461, loss: 2.2824
2022-07-05 14:43:06 - train: epoch 0015, iter [00700, 05004], lr: 0.099459, loss: 2.0133
2022-07-05 14:43:40 - train: epoch 0015, iter [00800, 05004], lr: 0.099457, loss: 1.8965
2022-07-05 14:44:15 - train: epoch 0015, iter [00900, 05004], lr: 0.099454, loss: 2.0295
2022-07-05 14:44:48 - train: epoch 0015, iter [01000, 05004], lr: 0.099452, loss: 2.1958
2022-07-05 14:45:22 - train: epoch 0015, iter [01100, 05004], lr: 0.099449, loss: 1.9986
2022-07-05 14:45:56 - train: epoch 0015, iter [01200, 05004], lr: 0.099447, loss: 2.0394
2022-07-05 14:46:30 - train: epoch 0015, iter [01300, 05004], lr: 0.099445, loss: 2.5093
2022-07-05 14:47:04 - train: epoch 0015, iter [01400, 05004], lr: 0.099442, loss: 2.0269
2022-07-05 14:47:38 - train: epoch 0015, iter [01500, 05004], lr: 0.099440, loss: 1.8327
2022-07-05 14:48:12 - train: epoch 0015, iter [01600, 05004], lr: 0.099437, loss: 2.1173
2022-07-05 14:48:46 - train: epoch 0015, iter [01700, 05004], lr: 0.099435, loss: 2.1749
2022-07-05 14:49:20 - train: epoch 0015, iter [01800, 05004], lr: 0.099433, loss: 1.9419
2022-07-05 14:49:54 - train: epoch 0015, iter [01900, 05004], lr: 0.099430, loss: 2.0165
2022-07-05 14:50:29 - train: epoch 0015, iter [02000, 05004], lr: 0.099428, loss: 2.1629
2022-07-05 14:51:03 - train: epoch 0015, iter [02100, 05004], lr: 0.099425, loss: 2.0514
2022-07-05 14:51:37 - train: epoch 0015, iter [02200, 05004], lr: 0.099423, loss: 2.4291
2022-07-05 14:52:12 - train: epoch 0015, iter [02300, 05004], lr: 0.099420, loss: 2.0088
2022-07-05 14:52:46 - train: epoch 0015, iter [02400, 05004], lr: 0.099418, loss: 2.3318
2022-07-05 14:53:20 - train: epoch 0015, iter [02500, 05004], lr: 0.099416, loss: 2.1893
2022-07-05 14:53:55 - train: epoch 0015, iter [02600, 05004], lr: 0.099413, loss: 2.0489
2022-07-05 14:54:28 - train: epoch 0015, iter [02700, 05004], lr: 0.099411, loss: 2.1826
2022-07-05 14:55:03 - train: epoch 0015, iter [02800, 05004], lr: 0.099408, loss: 2.2814
2022-07-05 14:55:37 - train: epoch 0015, iter [02900, 05004], lr: 0.099406, loss: 2.1202
2022-07-05 14:56:12 - train: epoch 0015, iter [03000, 05004], lr: 0.099403, loss: 1.9318
2022-07-05 14:56:46 - train: epoch 0015, iter [03100, 05004], lr: 0.099401, loss: 2.1500
2022-07-05 14:57:21 - train: epoch 0015, iter [03200, 05004], lr: 0.099398, loss: 2.0875
2022-07-05 14:57:56 - train: epoch 0015, iter [03300, 05004], lr: 0.099396, loss: 1.9255
2022-07-05 14:58:30 - train: epoch 0015, iter [03400, 05004], lr: 0.099393, loss: 2.2323
2022-07-05 14:59:04 - train: epoch 0015, iter [03500, 05004], lr: 0.099391, loss: 2.3574
2022-07-05 14:59:38 - train: epoch 0015, iter [03600, 05004], lr: 0.099388, loss: 2.1458
2022-07-05 15:00:12 - train: epoch 0015, iter [03700, 05004], lr: 0.099386, loss: 1.9752
2022-07-05 15:00:47 - train: epoch 0015, iter [03800, 05004], lr: 0.099383, loss: 2.1901
2022-07-05 15:01:21 - train: epoch 0015, iter [03900, 05004], lr: 0.099381, loss: 2.2950
2022-07-05 15:01:55 - train: epoch 0015, iter [04000, 05004], lr: 0.099378, loss: 2.1870
2022-07-05 15:02:29 - train: epoch 0015, iter [04100, 05004], lr: 0.099376, loss: 2.3319
2022-07-05 15:03:03 - train: epoch 0015, iter [04200, 05004], lr: 0.099373, loss: 1.8390
2022-07-05 15:03:37 - train: epoch 0015, iter [04300, 05004], lr: 0.099371, loss: 2.0570
2022-07-05 15:04:12 - train: epoch 0015, iter [04400, 05004], lr: 0.099368, loss: 2.1500
2022-07-05 15:04:46 - train: epoch 0015, iter [04500, 05004], lr: 0.099365, loss: 2.1709
2022-07-05 15:05:19 - train: epoch 0015, iter [04600, 05004], lr: 0.099363, loss: 2.0581
2022-07-05 15:05:53 - train: epoch 0015, iter [04700, 05004], lr: 0.099360, loss: 2.1667
2022-07-05 15:06:27 - train: epoch 0015, iter [04800, 05004], lr: 0.099358, loss: 2.0706
2022-07-05 15:07:01 - train: epoch 0015, iter [04900, 05004], lr: 0.099355, loss: 2.0629
2022-07-05 15:07:34 - train: epoch 0015, iter [05000, 05004], lr: 0.099353, loss: 2.2130
2022-07-05 15:07:35 - train: epoch 015, train_loss: 2.1384
2022-07-05 15:08:50 - eval: epoch: 015, acc1: 53.194%, acc5: 78.276%, test_loss: 2.0081, per_image_load_time: 2.345ms, per_image_inference_time: 0.510ms
2022-07-05 15:08:50 - until epoch: 015, best_acc1: 53.766%
2022-07-05 15:08:50 - epoch 016 lr: 0.099352
2022-07-05 15:09:28 - train: epoch 0016, iter [00100, 05004], lr: 0.099350, loss: 2.0376
2022-07-05 15:10:03 - train: epoch 0016, iter [00200, 05004], lr: 0.099347, loss: 1.9772
2022-07-05 15:10:37 - train: epoch 0016, iter [00300, 05004], lr: 0.099345, loss: 2.0674
2022-07-05 15:11:11 - train: epoch 0016, iter [00400, 05004], lr: 0.099342, loss: 2.3045
2022-07-05 15:11:45 - train: epoch 0016, iter [00500, 05004], lr: 0.099340, loss: 1.8837
2022-07-05 15:12:19 - train: epoch 0016, iter [00600, 05004], lr: 0.099337, loss: 2.2411
2022-07-05 15:12:54 - train: epoch 0016, iter [00700, 05004], lr: 0.099334, loss: 1.8138
2022-07-05 15:13:28 - train: epoch 0016, iter [00800, 05004], lr: 0.099332, loss: 2.1054
2022-07-05 15:14:02 - train: epoch 0016, iter [00900, 05004], lr: 0.099329, loss: 2.1290
2022-07-05 15:14:37 - train: epoch 0016, iter [01000, 05004], lr: 0.099326, loss: 2.0938
2022-07-05 15:15:11 - train: epoch 0016, iter [01100, 05004], lr: 0.099324, loss: 1.9961
2022-07-05 15:15:45 - train: epoch 0016, iter [01200, 05004], lr: 0.099321, loss: 2.0323
2022-07-05 15:16:20 - train: epoch 0016, iter [01300, 05004], lr: 0.099319, loss: 2.3017
2022-07-05 15:16:54 - train: epoch 0016, iter [01400, 05004], lr: 0.099316, loss: 2.0005
2022-07-05 15:17:30 - train: epoch 0016, iter [01500, 05004], lr: 0.099313, loss: 2.2063
2022-07-05 15:18:04 - train: epoch 0016, iter [01600, 05004], lr: 0.099311, loss: 2.2234
2022-07-05 15:18:37 - train: epoch 0016, iter [01700, 05004], lr: 0.099308, loss: 2.0264
2022-07-05 15:19:12 - train: epoch 0016, iter [01800, 05004], lr: 0.099305, loss: 1.9587
2022-07-05 15:19:46 - train: epoch 0016, iter [01900, 05004], lr: 0.099303, loss: 2.1496
2022-07-05 15:20:21 - train: epoch 0016, iter [02000, 05004], lr: 0.099300, loss: 1.8731
2022-07-05 15:20:55 - train: epoch 0016, iter [02100, 05004], lr: 0.099297, loss: 2.1235
2022-07-05 15:21:29 - train: epoch 0016, iter [02200, 05004], lr: 0.099294, loss: 2.1261
2022-07-05 15:22:03 - train: epoch 0016, iter [02300, 05004], lr: 0.099292, loss: 2.2440
2022-07-05 15:22:38 - train: epoch 0016, iter [02400, 05004], lr: 0.099289, loss: 2.1767
2022-07-05 15:23:12 - train: epoch 0016, iter [02500, 05004], lr: 0.099286, loss: 2.0474
2022-07-05 15:23:47 - train: epoch 0016, iter [02600, 05004], lr: 0.099284, loss: 2.2736
2022-07-05 15:24:22 - train: epoch 0016, iter [02700, 05004], lr: 0.099281, loss: 2.1153
2022-07-05 15:24:56 - train: epoch 0016, iter [02800, 05004], lr: 0.099278, loss: 1.9983
2022-07-05 15:25:30 - train: epoch 0016, iter [02900, 05004], lr: 0.099275, loss: 2.2189
2022-07-05 15:26:05 - train: epoch 0016, iter [03000, 05004], lr: 0.099273, loss: 2.3176
2022-07-05 15:26:40 - train: epoch 0016, iter [03100, 05004], lr: 0.099270, loss: 2.2729
2022-07-05 15:27:14 - train: epoch 0016, iter [03200, 05004], lr: 0.099267, loss: 2.2496
2022-07-05 15:27:49 - train: epoch 0016, iter [03300, 05004], lr: 0.099265, loss: 2.1712
2022-07-05 15:28:23 - train: epoch 0016, iter [03400, 05004], lr: 0.099262, loss: 1.9911
2022-07-05 15:28:58 - train: epoch 0016, iter [03500, 05004], lr: 0.099259, loss: 2.1280
2022-07-05 15:29:32 - train: epoch 0016, iter [03600, 05004], lr: 0.099256, loss: 2.0015
2022-07-05 15:30:07 - train: epoch 0016, iter [03700, 05004], lr: 0.099253, loss: 2.1905
2022-07-05 15:30:41 - train: epoch 0016, iter [03800, 05004], lr: 0.099251, loss: 2.4928
2022-07-05 15:31:16 - train: epoch 0016, iter [03900, 05004], lr: 0.099248, loss: 2.1804
2022-07-05 15:31:50 - train: epoch 0016, iter [04000, 05004], lr: 0.099245, loss: 2.1800
2022-07-05 15:32:24 - train: epoch 0016, iter [04100, 05004], lr: 0.099242, loss: 2.0475
2022-07-05 15:33:00 - train: epoch 0016, iter [04200, 05004], lr: 0.099240, loss: 2.1223
2022-07-05 15:33:34 - train: epoch 0016, iter [04300, 05004], lr: 0.099237, loss: 1.9502
2022-07-05 15:34:09 - train: epoch 0016, iter [04400, 05004], lr: 0.099234, loss: 2.0555
2022-07-05 15:34:43 - train: epoch 0016, iter [04500, 05004], lr: 0.099231, loss: 2.2784
2022-07-05 15:35:17 - train: epoch 0016, iter [04600, 05004], lr: 0.099228, loss: 2.0948
2022-07-05 15:35:52 - train: epoch 0016, iter [04700, 05004], lr: 0.099226, loss: 2.3188
2022-07-05 15:36:26 - train: epoch 0016, iter [04800, 05004], lr: 0.099223, loss: 2.0333
2022-07-05 15:37:00 - train: epoch 0016, iter [04900, 05004], lr: 0.099220, loss: 2.2324
2022-07-05 15:37:32 - train: epoch 0016, iter [05000, 05004], lr: 0.099217, loss: 2.2054
2022-07-05 15:37:33 - train: epoch 016, train_loss: 2.1191
2022-07-05 15:38:47 - eval: epoch: 016, acc1: 55.236%, acc5: 80.194%, test_loss: 1.9109, per_image_load_time: 1.300ms, per_image_inference_time: 0.433ms
2022-07-05 15:38:47 - until epoch: 016, best_acc1: 55.236%
2022-07-05 15:38:47 - epoch 017 lr: 0.099217
2022-07-05 15:39:26 - train: epoch 0017, iter [00100, 05004], lr: 0.099214, loss: 1.9767
2022-07-05 15:40:00 - train: epoch 0017, iter [00200, 05004], lr: 0.099211, loss: 2.0380
2022-07-05 15:40:33 - train: epoch 0017, iter [00300, 05004], lr: 0.099208, loss: 2.2657
2022-07-05 15:41:07 - train: epoch 0017, iter [00400, 05004], lr: 0.099206, loss: 1.8674
2022-07-05 15:41:40 - train: epoch 0017, iter [00500, 05004], lr: 0.099203, loss: 2.0422
2022-07-05 15:42:13 - train: epoch 0017, iter [00600, 05004], lr: 0.099200, loss: 2.3504
2022-07-05 15:42:47 - train: epoch 0017, iter [00700, 05004], lr: 0.099197, loss: 2.1135
2022-07-05 15:43:20 - train: epoch 0017, iter [00800, 05004], lr: 0.099194, loss: 1.9550
2022-07-05 15:43:53 - train: epoch 0017, iter [00900, 05004], lr: 0.099191, loss: 1.9746
2022-07-05 15:44:27 - train: epoch 0017, iter [01000, 05004], lr: 0.099188, loss: 1.9067
2022-07-05 15:45:01 - train: epoch 0017, iter [01100, 05004], lr: 0.099185, loss: 2.3568
2022-07-05 15:45:36 - train: epoch 0017, iter [01200, 05004], lr: 0.099182, loss: 2.3319
2022-07-05 15:46:09 - train: epoch 0017, iter [01300, 05004], lr: 0.099180, loss: 2.0762
2022-07-05 15:46:44 - train: epoch 0017, iter [01400, 05004], lr: 0.099177, loss: 2.1386
2022-07-05 15:47:18 - train: epoch 0017, iter [01500, 05004], lr: 0.099174, loss: 1.7150
2022-07-05 15:47:51 - train: epoch 0017, iter [01600, 05004], lr: 0.099171, loss: 2.0194
2022-07-05 15:48:26 - train: epoch 0017, iter [01700, 05004], lr: 0.099168, loss: 1.9448
2022-07-05 15:49:00 - train: epoch 0017, iter [01800, 05004], lr: 0.099165, loss: 2.1546
2022-07-05 15:49:35 - train: epoch 0017, iter [01900, 05004], lr: 0.099162, loss: 2.0795
2022-07-05 15:50:08 - train: epoch 0017, iter [02000, 05004], lr: 0.099159, loss: 2.2273
2022-07-05 15:50:43 - train: epoch 0017, iter [02100, 05004], lr: 0.099156, loss: 2.1243
2022-07-05 15:51:16 - train: epoch 0017, iter [02200, 05004], lr: 0.099153, loss: 2.0277
2022-07-05 15:51:51 - train: epoch 0017, iter [02300, 05004], lr: 0.099150, loss: 2.0606
2022-07-05 15:52:25 - train: epoch 0017, iter [02400, 05004], lr: 0.099147, loss: 2.0951
2022-07-05 15:52:59 - train: epoch 0017, iter [02500, 05004], lr: 0.099144, loss: 2.2226
2022-07-05 15:53:33 - train: epoch 0017, iter [02600, 05004], lr: 0.099141, loss: 2.0501
2022-07-05 15:54:07 - train: epoch 0017, iter [02700, 05004], lr: 0.099138, loss: 1.9986
2022-07-05 15:54:41 - train: epoch 0017, iter [02800, 05004], lr: 0.099135, loss: 2.2147
2022-07-05 15:55:16 - train: epoch 0017, iter [02900, 05004], lr: 0.099132, loss: 2.2964
2022-07-05 15:55:50 - train: epoch 0017, iter [03000, 05004], lr: 0.099129, loss: 1.9102
2022-07-05 15:56:24 - train: epoch 0017, iter [03100, 05004], lr: 0.099126, loss: 2.2407
2022-07-05 15:56:59 - train: epoch 0017, iter [03200, 05004], lr: 0.099123, loss: 1.9302
2022-07-05 15:57:33 - train: epoch 0017, iter [03300, 05004], lr: 0.099120, loss: 2.1791
2022-07-05 15:58:07 - train: epoch 0017, iter [03400, 05004], lr: 0.099117, loss: 1.8787
2022-07-05 15:58:41 - train: epoch 0017, iter [03500, 05004], lr: 0.099114, loss: 2.0450
2022-07-05 15:59:16 - train: epoch 0017, iter [03600, 05004], lr: 0.099111, loss: 2.3892
2022-07-05 15:59:50 - train: epoch 0017, iter [03700, 05004], lr: 0.099108, loss: 2.0583
2022-07-05 16:00:24 - train: epoch 0017, iter [03800, 05004], lr: 0.099105, loss: 2.3315
2022-07-05 16:00:59 - train: epoch 0017, iter [03900, 05004], lr: 0.099102, loss: 1.9867
2022-07-05 16:01:33 - train: epoch 0017, iter [04000, 05004], lr: 0.099099, loss: 1.9382
2022-07-05 16:02:06 - train: epoch 0017, iter [04100, 05004], lr: 0.099096, loss: 2.0766
2022-07-05 16:02:42 - train: epoch 0017, iter [04200, 05004], lr: 0.099093, loss: 2.0937
2022-07-05 16:03:16 - train: epoch 0017, iter [04300, 05004], lr: 0.099090, loss: 2.1403
2022-07-05 16:03:51 - train: epoch 0017, iter [04400, 05004], lr: 0.099087, loss: 2.1044
2022-07-05 16:04:25 - train: epoch 0017, iter [04500, 05004], lr: 0.099084, loss: 2.1871
2022-07-05 16:04:59 - train: epoch 0017, iter [04600, 05004], lr: 0.099081, loss: 2.0086
2022-07-05 16:05:34 - train: epoch 0017, iter [04700, 05004], lr: 0.099078, loss: 2.2321
2022-07-05 16:06:08 - train: epoch 0017, iter [04800, 05004], lr: 0.099075, loss: 2.1988
2022-07-05 16:06:42 - train: epoch 0017, iter [04900, 05004], lr: 0.099072, loss: 1.9024
2022-07-05 16:07:15 - train: epoch 0017, iter [05000, 05004], lr: 0.099069, loss: 1.8871
2022-07-05 16:07:16 - train: epoch 017, train_loss: 2.1014
2022-07-05 16:08:32 - eval: epoch: 017, acc1: 52.110%, acc5: 77.970%, test_loss: 2.0590, per_image_load_time: 1.993ms, per_image_inference_time: 0.470ms
2022-07-05 16:08:32 - until epoch: 017, best_acc1: 55.236%
2022-07-05 16:08:32 - epoch 018 lr: 0.099068
2022-07-05 16:09:12 - train: epoch 0018, iter [00100, 05004], lr: 0.099065, loss: 2.0764
2022-07-05 16:09:45 - train: epoch 0018, iter [00200, 05004], lr: 0.099062, loss: 2.0819
2022-07-05 16:10:20 - train: epoch 0018, iter [00300, 05004], lr: 0.099059, loss: 2.3017
2022-07-05 16:10:53 - train: epoch 0018, iter [00400, 05004], lr: 0.099056, loss: 2.2159
2022-07-05 16:11:27 - train: epoch 0018, iter [00500, 05004], lr: 0.099053, loss: 1.9627
2022-07-05 16:12:01 - train: epoch 0018, iter [00600, 05004], lr: 0.099050, loss: 2.1904
2022-07-05 16:12:34 - train: epoch 0018, iter [00700, 05004], lr: 0.099047, loss: 1.8184
2022-07-05 16:13:08 - train: epoch 0018, iter [00800, 05004], lr: 0.099044, loss: 2.1132
2022-07-05 16:13:42 - train: epoch 0018, iter [00900, 05004], lr: 0.099040, loss: 2.1351
2022-07-05 16:14:15 - train: epoch 0018, iter [01000, 05004], lr: 0.099037, loss: 1.8772
2022-07-05 16:14:49 - train: epoch 0018, iter [01100, 05004], lr: 0.099034, loss: 2.2990
2022-07-05 16:15:23 - train: epoch 0018, iter [01200, 05004], lr: 0.099031, loss: 2.0735
2022-07-05 16:15:57 - train: epoch 0018, iter [01300, 05004], lr: 0.099028, loss: 2.4343
2022-07-05 16:16:30 - train: epoch 0018, iter [01400, 05004], lr: 0.099025, loss: 2.0881
2022-07-05 16:17:04 - train: epoch 0018, iter [01500, 05004], lr: 0.099022, loss: 2.2597
2022-07-05 16:17:38 - train: epoch 0018, iter [01600, 05004], lr: 0.099018, loss: 2.0539
2022-07-05 16:18:12 - train: epoch 0018, iter [01700, 05004], lr: 0.099015, loss: 2.0618
2022-07-05 16:18:46 - train: epoch 0018, iter [01800, 05004], lr: 0.099012, loss: 1.7607
2022-07-05 16:19:20 - train: epoch 0018, iter [01900, 05004], lr: 0.099009, loss: 2.1374
2022-07-05 16:19:51 - train: epoch 0018, iter [02000, 05004], lr: 0.099006, loss: 2.3689
2022-07-05 16:20:23 - train: epoch 0018, iter [02100, 05004], lr: 0.099002, loss: 2.3182
2022-07-05 16:20:56 - train: epoch 0018, iter [02200, 05004], lr: 0.098999, loss: 2.0386
2022-07-05 16:21:30 - train: epoch 0018, iter [02300, 05004], lr: 0.098996, loss: 2.0090
2022-07-05 16:22:04 - train: epoch 0018, iter [02400, 05004], lr: 0.098993, loss: 1.9984
2022-07-05 16:22:38 - train: epoch 0018, iter [02500, 05004], lr: 0.098990, loss: 1.8969
2022-07-05 16:23:13 - train: epoch 0018, iter [02600, 05004], lr: 0.098986, loss: 1.9393
2022-07-05 16:23:45 - train: epoch 0018, iter [02700, 05004], lr: 0.098983, loss: 2.2039
2022-07-05 16:24:19 - train: epoch 0018, iter [02800, 05004], lr: 0.098980, loss: 1.8553
2022-07-05 16:24:53 - train: epoch 0018, iter [02900, 05004], lr: 0.098977, loss: 2.1209
2022-07-05 16:25:26 - train: epoch 0018, iter [03000, 05004], lr: 0.098973, loss: 2.0420
2022-07-05 16:26:00 - train: epoch 0018, iter [03100, 05004], lr: 0.098970, loss: 2.3875
2022-07-05 16:26:34 - train: epoch 0018, iter [03200, 05004], lr: 0.098967, loss: 2.0073
2022-07-05 16:27:08 - train: epoch 0018, iter [03300, 05004], lr: 0.098964, loss: 1.9984
2022-07-05 16:27:42 - train: epoch 0018, iter [03400, 05004], lr: 0.098960, loss: 2.1257
2022-07-05 16:28:16 - train: epoch 0018, iter [03500, 05004], lr: 0.098957, loss: 2.3475
2022-07-05 16:28:50 - train: epoch 0018, iter [03600, 05004], lr: 0.098954, loss: 2.1292
2022-07-05 16:29:24 - train: epoch 0018, iter [03700, 05004], lr: 0.098951, loss: 2.4407
2022-07-05 16:29:58 - train: epoch 0018, iter [03800, 05004], lr: 0.098947, loss: 2.1548
2022-07-05 16:30:32 - train: epoch 0018, iter [03900, 05004], lr: 0.098944, loss: 2.1075
2022-07-05 16:31:06 - train: epoch 0018, iter [04000, 05004], lr: 0.098941, loss: 1.8967
2022-07-05 16:31:39 - train: epoch 0018, iter [04100, 05004], lr: 0.098937, loss: 2.1193
2022-07-05 16:32:13 - train: epoch 0018, iter [04200, 05004], lr: 0.098934, loss: 2.0957
2022-07-05 16:32:47 - train: epoch 0018, iter [04300, 05004], lr: 0.098931, loss: 1.8170
2022-07-05 16:33:22 - train: epoch 0018, iter [04400, 05004], lr: 0.098928, loss: 2.1179
2022-07-05 16:33:56 - train: epoch 0018, iter [04500, 05004], lr: 0.098924, loss: 2.0667
2022-07-05 16:34:30 - train: epoch 0018, iter [04600, 05004], lr: 0.098921, loss: 2.0321
2022-07-05 16:35:04 - train: epoch 0018, iter [04700, 05004], lr: 0.098918, loss: 2.3513
2022-07-05 16:35:38 - train: epoch 0018, iter [04800, 05004], lr: 0.098914, loss: 2.1173
2022-07-05 16:36:12 - train: epoch 0018, iter [04900, 05004], lr: 0.098911, loss: 2.0981
2022-07-05 16:36:45 - train: epoch 0018, iter [05000, 05004], lr: 0.098908, loss: 2.1402
2022-07-05 16:36:46 - train: epoch 018, train_loss: 2.0861
2022-07-05 16:38:01 - eval: epoch: 018, acc1: 55.658%, acc5: 80.350%, test_loss: 1.8923, per_image_load_time: 1.636ms, per_image_inference_time: 0.495ms
2022-07-05 16:38:02 - until epoch: 018, best_acc1: 55.658%
2022-07-05 16:38:02 - epoch 019 lr: 0.098907
2022-07-05 16:38:42 - train: epoch 0019, iter [00100, 05004], lr: 0.098904, loss: 1.8445
2022-07-05 16:39:15 - train: epoch 0019, iter [00200, 05004], lr: 0.098901, loss: 2.1689
2022-07-05 16:39:50 - train: epoch 0019, iter [00300, 05004], lr: 0.098897, loss: 2.3798
2022-07-05 16:40:24 - train: epoch 0019, iter [00400, 05004], lr: 0.098894, loss: 1.9658
2022-07-05 16:40:57 - train: epoch 0019, iter [00500, 05004], lr: 0.098891, loss: 2.0246
2022-07-05 16:41:31 - train: epoch 0019, iter [00600, 05004], lr: 0.098887, loss: 1.9115
2022-07-05 16:42:05 - train: epoch 0019, iter [00700, 05004], lr: 0.098884, loss: 1.8088
2022-07-05 16:42:39 - train: epoch 0019, iter [00800, 05004], lr: 0.098880, loss: 2.2391
2022-07-05 16:43:13 - train: epoch 0019, iter [00900, 05004], lr: 0.098877, loss: 2.0920
2022-07-05 16:43:47 - train: epoch 0019, iter [01000, 05004], lr: 0.098874, loss: 2.1825
2022-07-05 16:44:21 - train: epoch 0019, iter [01100, 05004], lr: 0.098870, loss: 1.9718
2022-07-05 16:44:55 - train: epoch 0019, iter [01200, 05004], lr: 0.098867, loss: 2.1538
2022-07-05 16:45:29 - train: epoch 0019, iter [01300, 05004], lr: 0.098863, loss: 2.1675
2022-07-05 16:46:03 - train: epoch 0019, iter [01400, 05004], lr: 0.098860, loss: 1.9771
2022-07-05 16:46:37 - train: epoch 0019, iter [01500, 05004], lr: 0.098857, loss: 2.5234
2022-07-05 16:47:11 - train: epoch 0019, iter [01600, 05004], lr: 0.098853, loss: 2.0483
2022-07-05 16:47:46 - train: epoch 0019, iter [01700, 05004], lr: 0.098850, loss: 2.2600
2022-07-05 16:48:20 - train: epoch 0019, iter [01800, 05004], lr: 0.098846, loss: 1.9386
2022-07-05 16:48:54 - train: epoch 0019, iter [01900, 05004], lr: 0.098843, loss: 2.2144
2022-07-05 16:49:29 - train: epoch 0019, iter [02000, 05004], lr: 0.098839, loss: 1.9934
2022-07-05 16:50:02 - train: epoch 0019, iter [02100, 05004], lr: 0.098836, loss: 1.9244
2022-07-05 16:50:37 - train: epoch 0019, iter [02200, 05004], lr: 0.098833, loss: 2.0540
2022-07-05 16:51:10 - train: epoch 0019, iter [02300, 05004], lr: 0.098829, loss: 2.0827
2022-07-05 16:51:44 - train: epoch 0019, iter [02400, 05004], lr: 0.098826, loss: 2.1517
2022-07-05 16:52:17 - train: epoch 0019, iter [02500, 05004], lr: 0.098822, loss: 2.0378
2022-07-05 16:52:51 - train: epoch 0019, iter [02600, 05004], lr: 0.098819, loss: 2.1061
2022-07-05 16:53:24 - train: epoch 0019, iter [02700, 05004], lr: 0.098815, loss: 2.0834
2022-07-05 16:53:58 - train: epoch 0019, iter [02800, 05004], lr: 0.098812, loss: 2.0961
2022-07-05 16:54:33 - train: epoch 0019, iter [02900, 05004], lr: 0.098808, loss: 2.1009
2022-07-05 16:55:06 - train: epoch 0019, iter [03000, 05004], lr: 0.098805, loss: 2.3138
2022-07-05 16:55:40 - train: epoch 0019, iter [03100, 05004], lr: 0.098801, loss: 2.2344
2022-07-05 16:56:13 - train: epoch 0019, iter [03200, 05004], lr: 0.098798, loss: 1.8388
2022-07-05 16:56:47 - train: epoch 0019, iter [03300, 05004], lr: 0.098794, loss: 1.9494
2022-07-05 16:57:21 - train: epoch 0019, iter [03400, 05004], lr: 0.098791, loss: 2.1526
2022-07-05 16:57:55 - train: epoch 0019, iter [03500, 05004], lr: 0.098787, loss: 2.0764
2022-07-05 16:58:29 - train: epoch 0019, iter [03600, 05004], lr: 0.098784, loss: 1.8405
2022-07-05 16:59:03 - train: epoch 0019, iter [03700, 05004], lr: 0.098780, loss: 2.0733
2022-07-05 16:59:37 - train: epoch 0019, iter [03800, 05004], lr: 0.098777, loss: 2.1936
2022-07-05 17:00:11 - train: epoch 0019, iter [03900, 05004], lr: 0.098773, loss: 1.9816
2022-07-05 17:00:45 - train: epoch 0019, iter [04000, 05004], lr: 0.098769, loss: 1.9465
2022-07-05 17:01:19 - train: epoch 0019, iter [04100, 05004], lr: 0.098766, loss: 2.1930
2022-07-05 17:01:52 - train: epoch 0019, iter [04200, 05004], lr: 0.098762, loss: 2.0377
2022-07-05 17:02:26 - train: epoch 0019, iter [04300, 05004], lr: 0.098759, loss: 2.0054
2022-07-05 17:03:00 - train: epoch 0019, iter [04400, 05004], lr: 0.098755, loss: 2.1477
2022-07-05 17:03:35 - train: epoch 0019, iter [04500, 05004], lr: 0.098752, loss: 2.3327
2022-07-05 17:04:08 - train: epoch 0019, iter [04600, 05004], lr: 0.098748, loss: 2.0246
2022-07-05 17:04:41 - train: epoch 0019, iter [04700, 05004], lr: 0.098744, loss: 2.0931
2022-07-05 17:05:16 - train: epoch 0019, iter [04800, 05004], lr: 0.098741, loss: 2.1267
2022-07-05 17:05:49 - train: epoch 0019, iter [04900, 05004], lr: 0.098737, loss: 2.0703
2022-07-05 17:06:22 - train: epoch 0019, iter [05000, 05004], lr: 0.098734, loss: 2.0656
2022-07-05 17:06:23 - train: epoch 019, train_loss: 2.0762
2022-07-05 17:07:38 - eval: epoch: 019, acc1: 55.190%, acc5: 80.212%, test_loss: 1.9003, per_image_load_time: 1.552ms, per_image_inference_time: 0.469ms
2022-07-05 17:07:38 - until epoch: 019, best_acc1: 55.658%
2022-07-05 17:07:38 - epoch 020 lr: 0.098734
2022-07-05 17:08:16 - train: epoch 0020, iter [00100, 05004], lr: 0.098730, loss: 2.2351
2022-07-05 17:08:50 - train: epoch 0020, iter [00200, 05004], lr: 0.098726, loss: 1.8772
2022-07-05 17:09:25 - train: epoch 0020, iter [00300, 05004], lr: 0.098723, loss: 2.0706
2022-07-05 17:09:58 - train: epoch 0020, iter [00400, 05004], lr: 0.098719, loss: 1.8913
2022-07-05 17:10:33 - train: epoch 0020, iter [00500, 05004], lr: 0.098715, loss: 1.9168
2022-07-05 17:11:05 - train: epoch 0020, iter [00600, 05004], lr: 0.098712, loss: 2.1764
2022-07-05 17:11:38 - train: epoch 0020, iter [00700, 05004], lr: 0.098708, loss: 1.8860
2022-07-05 17:12:13 - train: epoch 0020, iter [00800, 05004], lr: 0.098705, loss: 2.0601
2022-07-05 17:12:46 - train: epoch 0020, iter [00900, 05004], lr: 0.098701, loss: 2.3425
2022-07-05 17:13:20 - train: epoch 0020, iter [01000, 05004], lr: 0.098697, loss: 2.0984
2022-07-05 17:13:53 - train: epoch 0020, iter [01100, 05004], lr: 0.098694, loss: 2.0441
2022-07-05 17:14:26 - train: epoch 0020, iter [01200, 05004], lr: 0.098690, loss: 1.8348
2022-07-05 17:15:00 - train: epoch 0020, iter [01300, 05004], lr: 0.098686, loss: 1.9933
2022-07-05 17:15:33 - train: epoch 0020, iter [01400, 05004], lr: 0.098683, loss: 2.2168
2022-07-05 17:16:07 - train: epoch 0020, iter [01500, 05004], lr: 0.098679, loss: 2.1238
2022-07-05 17:16:41 - train: epoch 0020, iter [01600, 05004], lr: 0.098675, loss: 1.9907
2022-07-05 17:17:14 - train: epoch 0020, iter [01700, 05004], lr: 0.098672, loss: 1.7679
2022-07-05 17:17:48 - train: epoch 0020, iter [01800, 05004], lr: 0.098668, loss: 2.0361
2022-07-05 17:18:22 - train: epoch 0020, iter [01900, 05004], lr: 0.098664, loss: 1.9342
2022-07-05 17:18:55 - train: epoch 0020, iter [02000, 05004], lr: 0.098661, loss: 2.0421
2022-07-05 17:19:28 - train: epoch 0020, iter [02100, 05004], lr: 0.098657, loss: 2.1892
2022-07-05 17:20:02 - train: epoch 0020, iter [02200, 05004], lr: 0.098653, loss: 1.8557
2022-07-05 17:20:36 - train: epoch 0020, iter [02300, 05004], lr: 0.098649, loss: 1.9297
2022-07-05 17:21:09 - train: epoch 0020, iter [02400, 05004], lr: 0.098646, loss: 2.2722
2022-07-05 17:21:42 - train: epoch 0020, iter [02500, 05004], lr: 0.098642, loss: 1.9215
2022-07-05 17:22:16 - train: epoch 0020, iter [02600, 05004], lr: 0.098638, loss: 1.8507
2022-07-05 17:22:50 - train: epoch 0020, iter [02700, 05004], lr: 0.098635, loss: 2.1740
2022-07-05 17:23:23 - train: epoch 0020, iter [02800, 05004], lr: 0.098631, loss: 2.0443
2022-07-05 17:23:57 - train: epoch 0020, iter [02900, 05004], lr: 0.098627, loss: 2.3064
2022-07-05 17:24:31 - train: epoch 0020, iter [03000, 05004], lr: 0.098623, loss: 2.0209
2022-07-05 17:25:05 - train: epoch 0020, iter [03100, 05004], lr: 0.098620, loss: 2.1159
2022-07-05 17:25:38 - train: epoch 0020, iter [03200, 05004], lr: 0.098616, loss: 2.1361
2022-07-05 17:26:12 - train: epoch 0020, iter [03300, 05004], lr: 0.098612, loss: 1.8513
2022-07-05 17:26:45 - train: epoch 0020, iter [03400, 05004], lr: 0.098608, loss: 2.1084
2022-07-05 17:27:19 - train: epoch 0020, iter [03500, 05004], lr: 0.098604, loss: 1.9245
2022-07-05 17:27:53 - train: epoch 0020, iter [03600, 05004], lr: 0.098601, loss: 2.0870
2022-07-05 17:28:27 - train: epoch 0020, iter [03700, 05004], lr: 0.098597, loss: 1.9362
2022-07-05 17:29:01 - train: epoch 0020, iter [03800, 05004], lr: 0.098593, loss: 2.0362
2022-07-05 17:29:35 - train: epoch 0020, iter [03900, 05004], lr: 0.098589, loss: 2.1755
2022-07-05 17:30:07 - train: epoch 0020, iter [04000, 05004], lr: 0.098586, loss: 1.9547
2022-07-05 17:30:41 - train: epoch 0020, iter [04100, 05004], lr: 0.098582, loss: 1.9932
2022-07-05 17:31:16 - train: epoch 0020, iter [04200, 05004], lr: 0.098578, loss: 2.0657
2022-07-05 17:31:49 - train: epoch 0020, iter [04300, 05004], lr: 0.098574, loss: 2.1047
2022-07-05 17:32:23 - train: epoch 0020, iter [04400, 05004], lr: 0.098570, loss: 2.0950
2022-07-05 17:32:56 - train: epoch 0020, iter [04500, 05004], lr: 0.098566, loss: 2.1841
2022-07-05 17:33:30 - train: epoch 0020, iter [04600, 05004], lr: 0.098563, loss: 2.1321
2022-07-05 17:34:04 - train: epoch 0020, iter [04700, 05004], lr: 0.098559, loss: 1.8765
2022-07-05 17:34:38 - train: epoch 0020, iter [04800, 05004], lr: 0.098555, loss: 2.0964
2022-07-05 17:35:11 - train: epoch 0020, iter [04900, 05004], lr: 0.098551, loss: 2.1913
2022-07-05 17:35:44 - train: epoch 0020, iter [05000, 05004], lr: 0.098547, loss: 1.9185
2022-07-05 17:35:46 - train: epoch 020, train_loss: 2.0587
2022-07-05 17:37:00 - eval: epoch: 020, acc1: 56.118%, acc5: 80.656%, test_loss: 1.8653, per_image_load_time: 1.466ms, per_image_inference_time: 0.440ms
2022-07-05 17:37:01 - until epoch: 020, best_acc1: 56.118%
2022-07-05 17:37:01 - epoch 021 lr: 0.098547
2022-07-05 17:37:40 - train: epoch 0021, iter [00100, 05004], lr: 0.098543, loss: 1.9303
2022-07-05 17:38:13 - train: epoch 0021, iter [00200, 05004], lr: 0.098539, loss: 2.0923
2022-07-05 17:38:45 - train: epoch 0021, iter [00300, 05004], lr: 0.098536, loss: 1.7383
2022-07-05 17:39:18 - train: epoch 0021, iter [00400, 05004], lr: 0.098532, loss: 2.1418
2022-07-05 17:39:51 - train: epoch 0021, iter [00500, 05004], lr: 0.098528, loss: 1.9800
2022-07-05 17:40:23 - train: epoch 0021, iter [00600, 05004], lr: 0.098524, loss: 1.9649
2022-07-05 17:40:56 - train: epoch 0021, iter [00700, 05004], lr: 0.098520, loss: 1.8951
2022-07-05 17:41:30 - train: epoch 0021, iter [00800, 05004], lr: 0.098516, loss: 2.1113
2022-07-05 17:42:04 - train: epoch 0021, iter [00900, 05004], lr: 0.098512, loss: 1.9640
2022-07-05 17:42:36 - train: epoch 0021, iter [01000, 05004], lr: 0.098508, loss: 1.9770
2022-07-05 17:43:10 - train: epoch 0021, iter [01100, 05004], lr: 0.098504, loss: 2.0260
2022-07-05 17:43:43 - train: epoch 0021, iter [01200, 05004], lr: 0.098500, loss: 1.8601
2022-07-05 17:44:17 - train: epoch 0021, iter [01300, 05004], lr: 0.098497, loss: 1.8511
2022-07-05 17:44:50 - train: epoch 0021, iter [01400, 05004], lr: 0.098493, loss: 2.0075
2022-07-05 17:45:23 - train: epoch 0021, iter [01500, 05004], lr: 0.098489, loss: 1.7963
2022-07-05 17:45:57 - train: epoch 0021, iter [01600, 05004], lr: 0.098485, loss: 2.0652
2022-07-05 17:46:31 - train: epoch 0021, iter [01700, 05004], lr: 0.098481, loss: 2.0405
2022-07-05 17:47:05 - train: epoch 0021, iter [01800, 05004], lr: 0.098477, loss: 1.8826
2022-07-05 17:47:39 - train: epoch 0021, iter [01900, 05004], lr: 0.098473, loss: 2.1549
2022-07-05 17:48:12 - train: epoch 0021, iter [02000, 05004], lr: 0.098469, loss: 2.2713
2022-07-05 17:48:46 - train: epoch 0021, iter [02100, 05004], lr: 0.098465, loss: 1.9262
2022-07-05 17:49:19 - train: epoch 0021, iter [02200, 05004], lr: 0.098461, loss: 2.0086
2022-07-05 17:49:53 - train: epoch 0021, iter [02300, 05004], lr: 0.098457, loss: 1.9967
2022-07-05 17:50:27 - train: epoch 0021, iter [02400, 05004], lr: 0.098453, loss: 1.8849
2022-07-05 17:51:01 - train: epoch 0021, iter [02500, 05004], lr: 0.098449, loss: 2.0855
2022-07-05 17:51:35 - train: epoch 0021, iter [02600, 05004], lr: 0.098445, loss: 2.3044
2022-07-05 17:52:09 - train: epoch 0021, iter [02700, 05004], lr: 0.098441, loss: 1.9022
2022-07-05 17:52:43 - train: epoch 0021, iter [02800, 05004], lr: 0.098437, loss: 2.0609
2022-07-05 17:53:16 - train: epoch 0021, iter [02900, 05004], lr: 0.098433, loss: 1.9933
2022-07-05 17:53:51 - train: epoch 0021, iter [03000, 05004], lr: 0.098429, loss: 2.2304
2022-07-05 17:54:24 - train: epoch 0021, iter [03100, 05004], lr: 0.098425, loss: 2.1217
2022-07-05 17:54:58 - train: epoch 0021, iter [03200, 05004], lr: 0.098421, loss: 2.0016
2022-07-05 17:55:30 - train: epoch 0021, iter [03300, 05004], lr: 0.098417, loss: 2.3799
2022-07-05 17:56:02 - train: epoch 0021, iter [03400, 05004], lr: 0.098413, loss: 2.2690
2022-07-05 17:56:34 - train: epoch 0021, iter [03500, 05004], lr: 0.098409, loss: 1.9648
2022-07-05 17:57:07 - train: epoch 0021, iter [03600, 05004], lr: 0.098405, loss: 2.1034
2022-07-05 17:57:39 - train: epoch 0021, iter [03700, 05004], lr: 0.098401, loss: 2.0541
2022-07-05 17:58:13 - train: epoch 0021, iter [03800, 05004], lr: 0.098397, loss: 1.8835
2022-07-05 17:58:47 - train: epoch 0021, iter [03900, 05004], lr: 0.098393, loss: 1.9185
2022-07-05 17:59:21 - train: epoch 0021, iter [04000, 05004], lr: 0.098389, loss: 2.2507
2022-07-05 17:59:55 - train: epoch 0021, iter [04100, 05004], lr: 0.098385, loss: 2.0145
2022-07-05 18:00:29 - train: epoch 0021, iter [04200, 05004], lr: 0.098381, loss: 2.0087
2022-07-05 18:01:02 - train: epoch 0021, iter [04300, 05004], lr: 0.098377, loss: 2.0684
2022-07-05 18:01:36 - train: epoch 0021, iter [04400, 05004], lr: 0.098373, loss: 2.1468
2022-07-05 18:02:10 - train: epoch 0021, iter [04500, 05004], lr: 0.098369, loss: 2.2121
2022-07-05 18:02:43 - train: epoch 0021, iter [04600, 05004], lr: 0.098365, loss: 1.9029
2022-07-05 18:03:16 - train: epoch 0021, iter [04700, 05004], lr: 0.098360, loss: 2.2093
2022-07-05 18:03:50 - train: epoch 0021, iter [04800, 05004], lr: 0.098356, loss: 2.2347
2022-07-05 18:04:24 - train: epoch 0021, iter [04900, 05004], lr: 0.098352, loss: 1.7764
2022-07-05 18:04:56 - train: epoch 0021, iter [05000, 05004], lr: 0.098348, loss: 2.0376
2022-07-05 18:04:58 - train: epoch 021, train_loss: 2.0498
2022-07-05 18:06:13 - eval: epoch: 021, acc1: 56.740%, acc5: 81.644%, test_loss: 1.8184, per_image_load_time: 0.711ms, per_image_inference_time: 0.457ms
2022-07-05 18:06:13 - until epoch: 021, best_acc1: 56.740%
2022-07-05 18:06:13 - epoch 022 lr: 0.098348
2022-07-05 18:06:52 - train: epoch 0022, iter [00100, 05004], lr: 0.098344, loss: 1.8230
2022-07-05 18:07:26 - train: epoch 0022, iter [00200, 05004], lr: 0.098340, loss: 1.8541
2022-07-05 18:08:00 - train: epoch 0022, iter [00300, 05004], lr: 0.098336, loss: 1.6988
2022-07-05 18:08:33 - train: epoch 0022, iter [00400, 05004], lr: 0.098332, loss: 1.9292
2022-07-05 18:09:07 - train: epoch 0022, iter [00500, 05004], lr: 0.098327, loss: 1.9950
2022-07-05 18:09:41 - train: epoch 0022, iter [00600, 05004], lr: 0.098323, loss: 2.1477
2022-07-05 18:10:15 - train: epoch 0022, iter [00700, 05004], lr: 0.098319, loss: 2.1150
2022-07-05 18:10:49 - train: epoch 0022, iter [00800, 05004], lr: 0.098315, loss: 2.1513
2022-07-05 18:11:24 - train: epoch 0022, iter [00900, 05004], lr: 0.098311, loss: 2.0635
2022-07-05 18:11:57 - train: epoch 0022, iter [01000, 05004], lr: 0.098307, loss: 2.0180
2022-07-05 18:12:32 - train: epoch 0022, iter [01100, 05004], lr: 0.098303, loss: 2.0869
2022-07-05 18:13:07 - train: epoch 0022, iter [01200, 05004], lr: 0.098298, loss: 1.6763
2022-07-05 18:13:40 - train: epoch 0022, iter [01300, 05004], lr: 0.098294, loss: 1.9480
2022-07-05 18:14:14 - train: epoch 0022, iter [01400, 05004], lr: 0.098290, loss: 2.0194
2022-07-05 18:14:48 - train: epoch 0022, iter [01500, 05004], lr: 0.098286, loss: 1.8971
2022-07-05 18:15:23 - train: epoch 0022, iter [01600, 05004], lr: 0.098282, loss: 1.7734
2022-07-05 18:15:58 - train: epoch 0022, iter [01700, 05004], lr: 0.098278, loss: 1.8486
2022-07-05 18:16:32 - train: epoch 0022, iter [01800, 05004], lr: 0.098273, loss: 2.1293
2022-07-05 18:17:06 - train: epoch 0022, iter [01900, 05004], lr: 0.098269, loss: 1.8706
2022-07-05 18:17:40 - train: epoch 0022, iter [02000, 05004], lr: 0.098265, loss: 2.0877
2022-07-05 18:18:15 - train: epoch 0022, iter [02100, 05004], lr: 0.098261, loss: 2.1635
2022-07-05 18:18:48 - train: epoch 0022, iter [02200, 05004], lr: 0.098257, loss: 1.8725
2022-07-05 18:19:23 - train: epoch 0022, iter [02300, 05004], lr: 0.098252, loss: 2.2216
2022-07-05 18:19:57 - train: epoch 0022, iter [02400, 05004], lr: 0.098248, loss: 2.1332
2022-07-05 18:20:32 - train: epoch 0022, iter [02500, 05004], lr: 0.098244, loss: 2.1054
2022-07-05 18:21:06 - train: epoch 0022, iter [02600, 05004], lr: 0.098240, loss: 1.8636
2022-07-05 18:21:41 - train: epoch 0022, iter [02700, 05004], lr: 0.098235, loss: 1.8202
2022-07-05 18:22:15 - train: epoch 0022, iter [02800, 05004], lr: 0.098231, loss: 2.3345
2022-07-05 18:22:49 - train: epoch 0022, iter [02900, 05004], lr: 0.098227, loss: 1.9012
2022-07-05 18:23:23 - train: epoch 0022, iter [03000, 05004], lr: 0.098223, loss: 2.1508
2022-07-05 18:23:58 - train: epoch 0022, iter [03100, 05004], lr: 0.098218, loss: 2.1727
2022-07-05 18:24:32 - train: epoch 0022, iter [03200, 05004], lr: 0.098214, loss: 2.1908
2022-07-05 18:25:07 - train: epoch 0022, iter [03300, 05004], lr: 0.098210, loss: 2.0503
2022-07-05 18:25:42 - train: epoch 0022, iter [03400, 05004], lr: 0.098206, loss: 1.8787
2022-07-05 18:26:15 - train: epoch 0022, iter [03500, 05004], lr: 0.098201, loss: 2.1556
2022-07-05 18:26:50 - train: epoch 0022, iter [03600, 05004], lr: 0.098197, loss: 2.0183
2022-07-05 18:27:24 - train: epoch 0022, iter [03700, 05004], lr: 0.098193, loss: 2.2266
2022-07-05 18:27:58 - train: epoch 0022, iter [03800, 05004], lr: 0.098188, loss: 2.1421
2022-07-05 18:28:33 - train: epoch 0022, iter [03900, 05004], lr: 0.098184, loss: 2.0234
2022-07-05 18:29:08 - train: epoch 0022, iter [04000, 05004], lr: 0.098180, loss: 2.1330
2022-07-05 18:29:42 - train: epoch 0022, iter [04100, 05004], lr: 0.098176, loss: 1.9317
2022-07-05 18:30:16 - train: epoch 0022, iter [04200, 05004], lr: 0.098171, loss: 1.9832
2022-07-05 18:30:51 - train: epoch 0022, iter [04300, 05004], lr: 0.098167, loss: 2.2812
2022-07-05 18:31:25 - train: epoch 0022, iter [04400, 05004], lr: 0.098163, loss: 2.0951
2022-07-05 18:31:59 - train: epoch 0022, iter [04500, 05004], lr: 0.098158, loss: 2.0526
2022-07-05 18:32:34 - train: epoch 0022, iter [04600, 05004], lr: 0.098154, loss: 2.3289
2022-07-05 18:33:08 - train: epoch 0022, iter [04700, 05004], lr: 0.098150, loss: 2.1113
2022-07-05 18:33:42 - train: epoch 0022, iter [04800, 05004], lr: 0.098145, loss: 1.8819
2022-07-05 18:34:16 - train: epoch 0022, iter [04900, 05004], lr: 0.098141, loss: 1.8385
2022-07-05 18:34:50 - train: epoch 0022, iter [05000, 05004], lr: 0.098137, loss: 1.9299
2022-07-05 18:34:51 - train: epoch 022, train_loss: 2.0392
2022-07-05 18:36:07 - eval: epoch: 022, acc1: 54.218%, acc5: 79.206%, test_loss: 1.9676, per_image_load_time: 0.571ms, per_image_inference_time: 0.469ms
2022-07-05 18:36:08 - until epoch: 022, best_acc1: 56.740%
2022-07-05 18:36:08 - epoch 023 lr: 0.098136
2022-07-05 18:36:48 - train: epoch 0023, iter [00100, 05004], lr: 0.098132, loss: 1.8927
2022-07-05 18:37:21 - train: epoch 0023, iter [00200, 05004], lr: 0.098128, loss: 1.7214
2022-07-05 18:37:55 - train: epoch 0023, iter [00300, 05004], lr: 0.098123, loss: 1.8904
2022-07-05 18:38:29 - train: epoch 0023, iter [00400, 05004], lr: 0.098119, loss: 2.0215
2022-07-05 18:39:03 - train: epoch 0023, iter [00500, 05004], lr: 0.098115, loss: 1.9916
2022-07-05 18:39:38 - train: epoch 0023, iter [00600, 05004], lr: 0.098110, loss: 1.9248
2022-07-05 18:40:12 - train: epoch 0023, iter [00700, 05004], lr: 0.098106, loss: 1.7457
2022-07-05 18:40:46 - train: epoch 0023, iter [00800, 05004], lr: 0.098101, loss: 1.9882
2022-07-05 18:41:21 - train: epoch 0023, iter [00900, 05004], lr: 0.098097, loss: 2.0109
2022-07-05 18:41:56 - train: epoch 0023, iter [01000, 05004], lr: 0.098093, loss: 1.9623
2022-07-05 18:42:31 - train: epoch 0023, iter [01100, 05004], lr: 0.098088, loss: 2.1563
2022-07-05 18:43:05 - train: epoch 0023, iter [01200, 05004], lr: 0.098084, loss: 1.8501
2022-07-05 18:43:40 - train: epoch 0023, iter [01300, 05004], lr: 0.098079, loss: 1.9763
2022-07-05 18:44:15 - train: epoch 0023, iter [01400, 05004], lr: 0.098075, loss: 1.9844
2022-07-05 18:44:49 - train: epoch 0023, iter [01500, 05004], lr: 0.098071, loss: 1.8205
2022-07-05 18:45:24 - train: epoch 0023, iter [01600, 05004], lr: 0.098066, loss: 2.0727
2022-07-05 18:45:58 - train: epoch 0023, iter [01700, 05004], lr: 0.098062, loss: 2.1493
2022-07-05 18:46:32 - train: epoch 0023, iter [01800, 05004], lr: 0.098057, loss: 1.8335
2022-07-05 18:47:06 - train: epoch 0023, iter [01900, 05004], lr: 0.098053, loss: 2.1294
2022-07-05 18:47:40 - train: epoch 0023, iter [02000, 05004], lr: 0.098048, loss: 1.7833
2022-07-05 18:48:15 - train: epoch 0023, iter [02100, 05004], lr: 0.098044, loss: 2.1646
2022-07-05 18:48:49 - train: epoch 0023, iter [02200, 05004], lr: 0.098039, loss: 1.7779
2022-07-05 18:49:24 - train: epoch 0023, iter [02300, 05004], lr: 0.098035, loss: 1.8426
2022-07-05 18:49:59 - train: epoch 0023, iter [02400, 05004], lr: 0.098030, loss: 2.0715
2022-07-05 18:50:33 - train: epoch 0023, iter [02500, 05004], lr: 0.098026, loss: 2.1305
2022-07-05 18:51:08 - train: epoch 0023, iter [02600, 05004], lr: 0.098022, loss: 2.1562
2022-07-05 18:51:42 - train: epoch 0023, iter [02700, 05004], lr: 0.098017, loss: 1.9472
2022-07-05 18:52:17 - train: epoch 0023, iter [02800, 05004], lr: 0.098013, loss: 2.0574
2022-07-05 18:52:52 - train: epoch 0023, iter [02900, 05004], lr: 0.098008, loss: 2.1732
2022-07-05 18:53:26 - train: epoch 0023, iter [03000, 05004], lr: 0.098004, loss: 2.2801
2022-07-05 18:54:01 - train: epoch 0023, iter [03100, 05004], lr: 0.097999, loss: 2.2108
2022-07-05 18:54:36 - train: epoch 0023, iter [03200, 05004], lr: 0.097995, loss: 2.1471
2022-07-05 18:55:10 - train: epoch 0023, iter [03300, 05004], lr: 0.097990, loss: 2.0775
2022-07-05 18:55:45 - train: epoch 0023, iter [03400, 05004], lr: 0.097985, loss: 2.2036
2022-07-05 18:56:19 - train: epoch 0023, iter [03500, 05004], lr: 0.097981, loss: 1.8841
2022-07-05 18:56:54 - train: epoch 0023, iter [03600, 05004], lr: 0.097976, loss: 1.8270
2022-07-05 18:57:28 - train: epoch 0023, iter [03700, 05004], lr: 0.097972, loss: 1.9696
2022-07-05 18:58:03 - train: epoch 0023, iter [03800, 05004], lr: 0.097967, loss: 2.0501
2022-07-05 18:58:37 - train: epoch 0023, iter [03900, 05004], lr: 0.097963, loss: 2.0107
2022-07-05 18:59:11 - train: epoch 0023, iter [04000, 05004], lr: 0.097958, loss: 1.9154
2022-07-05 18:59:47 - train: epoch 0023, iter [04100, 05004], lr: 0.097954, loss: 1.9193
2022-07-05 19:00:21 - train: epoch 0023, iter [04200, 05004], lr: 0.097949, loss: 1.9134
2022-07-05 19:00:55 - train: epoch 0023, iter [04300, 05004], lr: 0.097945, loss: 1.8496
2022-07-05 19:01:29 - train: epoch 0023, iter [04400, 05004], lr: 0.097940, loss: 1.9504
2022-07-05 19:02:04 - train: epoch 0023, iter [04500, 05004], lr: 0.097935, loss: 1.8360
2022-07-05 19:02:39 - train: epoch 0023, iter [04600, 05004], lr: 0.097931, loss: 2.1260
2022-07-05 19:03:13 - train: epoch 0023, iter [04700, 05004], lr: 0.097926, loss: 1.8179
2022-07-05 19:03:47 - train: epoch 0023, iter [04800, 05004], lr: 0.097922, loss: 2.0485
2022-07-05 19:04:22 - train: epoch 0023, iter [04900, 05004], lr: 0.097917, loss: 1.9636
2022-07-05 19:04:55 - train: epoch 0023, iter [05000, 05004], lr: 0.097912, loss: 2.2057
2022-07-05 19:04:56 - train: epoch 023, train_loss: 2.0293
2022-07-05 19:06:13 - eval: epoch: 023, acc1: 49.452%, acc5: 74.912%, test_loss: 2.2406, per_image_load_time: 0.894ms, per_image_inference_time: 0.471ms
2022-07-05 19:06:13 - until epoch: 023, best_acc1: 56.740%
2022-07-05 19:06:13 - epoch 024 lr: 0.097912
2022-07-05 19:06:53 - train: epoch 0024, iter [00100, 05004], lr: 0.097908, loss: 1.8846
2022-07-05 19:07:27 - train: epoch 0024, iter [00200, 05004], lr: 0.097903, loss: 2.0570
2022-07-05 19:08:01 - train: epoch 0024, iter [00300, 05004], lr: 0.097898, loss: 1.9431
2022-07-05 19:08:36 - train: epoch 0024, iter [00400, 05004], lr: 0.097894, loss: 1.9840
2022-07-05 19:09:11 - train: epoch 0024, iter [00500, 05004], lr: 0.097889, loss: 1.9205
2022-07-05 19:09:45 - train: epoch 0024, iter [00600, 05004], lr: 0.097885, loss: 1.8639
2022-07-05 19:10:18 - train: epoch 0024, iter [00700, 05004], lr: 0.097880, loss: 1.9138
2022-07-05 19:10:52 - train: epoch 0024, iter [00800, 05004], lr: 0.097875, loss: 1.8615
2022-07-05 19:11:27 - train: epoch 0024, iter [00900, 05004], lr: 0.097871, loss: 1.9402
2022-07-05 19:12:01 - train: epoch 0024, iter [01000, 05004], lr: 0.097866, loss: 1.9223
2022-07-05 19:12:35 - train: epoch 0024, iter [01100, 05004], lr: 0.097861, loss: 1.7224
2022-07-05 19:13:09 - train: epoch 0024, iter [01200, 05004], lr: 0.097857, loss: 1.8708
2022-07-05 19:13:44 - train: epoch 0024, iter [01300, 05004], lr: 0.097852, loss: 2.3582
2022-07-05 19:14:19 - train: epoch 0024, iter [01400, 05004], lr: 0.097847, loss: 1.9172
2022-07-05 19:14:53 - train: epoch 0024, iter [01500, 05004], lr: 0.097843, loss: 2.2141
2022-07-05 19:15:27 - train: epoch 0024, iter [01600, 05004], lr: 0.097838, loss: 2.0830
2022-07-05 19:16:00 - train: epoch 0024, iter [01700, 05004], lr: 0.097833, loss: 2.0726
2022-07-05 19:16:34 - train: epoch 0024, iter [01800, 05004], lr: 0.097829, loss: 2.3587
2022-07-05 19:17:09 - train: epoch 0024, iter [01900, 05004], lr: 0.097824, loss: 1.7942
2022-07-05 19:17:44 - train: epoch 0024, iter [02000, 05004], lr: 0.097819, loss: 1.9821
2022-07-05 19:18:18 - train: epoch 0024, iter [02100, 05004], lr: 0.097815, loss: 1.9821
2022-07-05 19:18:53 - train: epoch 0024, iter [02200, 05004], lr: 0.097810, loss: 1.8001
2022-07-05 19:19:27 - train: epoch 0024, iter [02300, 05004], lr: 0.097805, loss: 1.9409
2022-07-05 19:20:02 - train: epoch 0024, iter [02400, 05004], lr: 0.097800, loss: 2.0262
2022-07-05 19:20:36 - train: epoch 0024, iter [02500, 05004], lr: 0.097796, loss: 1.9582
2022-07-05 19:21:11 - train: epoch 0024, iter [02600, 05004], lr: 0.097791, loss: 1.9599
2022-07-05 19:21:45 - train: epoch 0024, iter [02700, 05004], lr: 0.097786, loss: 2.1923
2022-07-05 19:22:19 - train: epoch 0024, iter [02800, 05004], lr: 0.097781, loss: 2.1034
2022-07-05 19:22:54 - train: epoch 0024, iter [02900, 05004], lr: 0.097777, loss: 1.9950
2022-07-05 19:23:28 - train: epoch 0024, iter [03000, 05004], lr: 0.097772, loss: 1.8428
2022-07-05 19:24:03 - train: epoch 0024, iter [03100, 05004], lr: 0.097767, loss: 1.9473
2022-07-05 19:24:38 - train: epoch 0024, iter [03200, 05004], lr: 0.097762, loss: 2.1465
2022-07-05 19:25:13 - train: epoch 0024, iter [03300, 05004], lr: 0.097758, loss: 1.6873
2022-07-05 19:25:47 - train: epoch 0024, iter [03400, 05004], lr: 0.097753, loss: 1.9703
2022-07-05 19:26:22 - train: epoch 0024, iter [03500, 05004], lr: 0.097748, loss: 1.9562
2022-07-05 19:26:56 - train: epoch 0024, iter [03600, 05004], lr: 0.097743, loss: 2.1918
2022-07-05 19:27:31 - train: epoch 0024, iter [03700, 05004], lr: 0.097739, loss: 1.9643
2022-07-05 19:28:06 - train: epoch 0024, iter [03800, 05004], lr: 0.097734, loss: 2.2013
2022-07-05 19:28:41 - train: epoch 0024, iter [03900, 05004], lr: 0.097729, loss: 1.9199
2022-07-05 19:29:15 - train: epoch 0024, iter [04000, 05004], lr: 0.097724, loss: 2.0943
2022-07-05 19:29:49 - train: epoch 0024, iter [04100, 05004], lr: 0.097719, loss: 1.9878
2022-07-05 19:30:24 - train: epoch 0024, iter [04200, 05004], lr: 0.097715, loss: 2.0022
2022-07-05 19:30:59 - train: epoch 0024, iter [04300, 05004], lr: 0.097710, loss: 2.0216
2022-07-05 19:31:34 - train: epoch 0024, iter [04400, 05004], lr: 0.097705, loss: 2.0019
2022-07-05 19:32:08 - train: epoch 0024, iter [04500, 05004], lr: 0.097700, loss: 1.9226
2022-07-05 19:32:43 - train: epoch 0024, iter [04600, 05004], lr: 0.097695, loss: 2.1708
2022-07-05 19:33:18 - train: epoch 0024, iter [04700, 05004], lr: 0.097690, loss: 2.0513
2022-07-05 19:33:53 - train: epoch 0024, iter [04800, 05004], lr: 0.097686, loss: 1.9305
2022-07-05 19:34:28 - train: epoch 0024, iter [04900, 05004], lr: 0.097681, loss: 2.0985
2022-07-05 19:35:01 - train: epoch 0024, iter [05000, 05004], lr: 0.097676, loss: 2.0419
2022-07-05 19:35:02 - train: epoch 024, train_loss: 2.0214
2022-07-05 19:36:18 - eval: epoch: 024, acc1: 54.340%, acc5: 79.320%, test_loss: 1.9474, per_image_load_time: 0.690ms, per_image_inference_time: 0.487ms
2022-07-05 19:36:18 - until epoch: 024, best_acc1: 56.740%
2022-07-05 19:36:18 - epoch 025 lr: 0.097676
2022-07-05 19:36:58 - train: epoch 0025, iter [00100, 05004], lr: 0.097671, loss: 1.8754
2022-07-05 19:37:32 - train: epoch 0025, iter [00200, 05004], lr: 0.097666, loss: 1.8569
2022-07-05 19:38:06 - train: epoch 0025, iter [00300, 05004], lr: 0.097661, loss: 1.8001
2022-07-05 19:38:40 - train: epoch 0025, iter [00400, 05004], lr: 0.097656, loss: 1.8979
2022-07-05 19:39:15 - train: epoch 0025, iter [00500, 05004], lr: 0.097651, loss: 1.8590
2022-07-05 19:39:50 - train: epoch 0025, iter [00600, 05004], lr: 0.097647, loss: 2.0053
2022-07-05 19:40:24 - train: epoch 0025, iter [00700, 05004], lr: 0.097642, loss: 2.0207
2022-07-05 19:40:58 - train: epoch 0025, iter [00800, 05004], lr: 0.097637, loss: 1.9747
2022-07-05 19:41:32 - train: epoch 0025, iter [00900, 05004], lr: 0.097632, loss: 1.8946
2022-07-05 19:42:06 - train: epoch 0025, iter [01000, 05004], lr: 0.097627, loss: 1.9947
2022-07-05 19:42:41 - train: epoch 0025, iter [01100, 05004], lr: 0.097622, loss: 2.0077
2022-07-05 19:43:15 - train: epoch 0025, iter [01200, 05004], lr: 0.097617, loss: 2.1118
2022-07-05 19:43:49 - train: epoch 0025, iter [01300, 05004], lr: 0.097612, loss: 2.1107
2022-07-05 19:44:24 - train: epoch 0025, iter [01400, 05004], lr: 0.097607, loss: 2.1664
2022-07-05 19:45:00 - train: epoch 0025, iter [01500, 05004], lr: 0.097602, loss: 1.9549
2022-07-05 19:45:34 - train: epoch 0025, iter [01600, 05004], lr: 0.097597, loss: 1.7040
2022-07-05 19:46:08 - train: epoch 0025, iter [01700, 05004], lr: 0.097593, loss: 1.9623
2022-07-05 19:46:43 - train: epoch 0025, iter [01800, 05004], lr: 0.097588, loss: 1.7884
2022-07-05 19:47:18 - train: epoch 0025, iter [01900, 05004], lr: 0.097583, loss: 1.8627
2022-07-05 19:47:52 - train: epoch 0025, iter [02000, 05004], lr: 0.097578, loss: 1.9787
2022-07-05 19:48:27 - train: epoch 0025, iter [02100, 05004], lr: 0.097573, loss: 1.8216
2022-07-05 19:49:02 - train: epoch 0025, iter [02200, 05004], lr: 0.097568, loss: 1.8279
2022-07-05 19:49:36 - train: epoch 0025, iter [02300, 05004], lr: 0.097563, loss: 2.0045
2022-07-05 19:50:10 - train: epoch 0025, iter [02400, 05004], lr: 0.097558, loss: 1.8756
2022-07-05 19:50:45 - train: epoch 0025, iter [02500, 05004], lr: 0.097553, loss: 2.0060
2022-07-05 19:51:20 - train: epoch 0025, iter [02600, 05004], lr: 0.097548, loss: 2.1744
2022-07-05 19:51:55 - train: epoch 0025, iter [02700, 05004], lr: 0.097543, loss: 2.0758
2022-07-05 19:52:28 - train: epoch 0025, iter [02800, 05004], lr: 0.097538, loss: 1.9890
2022-07-05 19:53:02 - train: epoch 0025, iter [02900, 05004], lr: 0.097533, loss: 2.2300
2022-07-05 19:53:36 - train: epoch 0025, iter [03000, 05004], lr: 0.097528, loss: 2.1872
2022-07-05 19:54:11 - train: epoch 0025, iter [03100, 05004], lr: 0.097523, loss: 1.9515
2022-07-05 19:54:45 - train: epoch 0025, iter [03200, 05004], lr: 0.097518, loss: 2.2737
2022-07-05 19:55:20 - train: epoch 0025, iter [03300, 05004], lr: 0.097513, loss: 1.8960
2022-07-05 19:55:53 - train: epoch 0025, iter [03400, 05004], lr: 0.097508, loss: 2.1692
2022-07-05 19:56:29 - train: epoch 0025, iter [03500, 05004], lr: 0.097503, loss: 1.7699
2022-07-05 19:57:03 - train: epoch 0025, iter [03600, 05004], lr: 0.097498, loss: 2.1377
2022-07-05 19:57:37 - train: epoch 0025, iter [03700, 05004], lr: 0.097493, loss: 1.9782
2022-07-05 19:58:13 - train: epoch 0025, iter [03800, 05004], lr: 0.097488, loss: 2.0732
2022-07-05 19:58:47 - train: epoch 0025, iter [03900, 05004], lr: 0.097483, loss: 2.2893
2022-07-05 19:59:21 - train: epoch 0025, iter [04000, 05004], lr: 0.097478, loss: 2.2062
2022-07-05 19:59:56 - train: epoch 0025, iter [04100, 05004], lr: 0.097473, loss: 2.3289
2022-07-05 20:00:31 - train: epoch 0025, iter [04200, 05004], lr: 0.097468, loss: 2.0288
2022-07-05 20:01:05 - train: epoch 0025, iter [04300, 05004], lr: 0.097463, loss: 1.8698
2022-07-05 20:01:40 - train: epoch 0025, iter [04400, 05004], lr: 0.097458, loss: 1.9392
2022-07-05 20:02:15 - train: epoch 0025, iter [04500, 05004], lr: 0.097452, loss: 1.9933
2022-07-05 20:02:50 - train: epoch 0025, iter [04600, 05004], lr: 0.097447, loss: 2.0419
2022-07-05 20:03:24 - train: epoch 0025, iter [04700, 05004], lr: 0.097442, loss: 1.8542
2022-07-05 20:03:59 - train: epoch 0025, iter [04800, 05004], lr: 0.097437, loss: 1.7013
2022-07-05 20:04:34 - train: epoch 0025, iter [04900, 05004], lr: 0.097432, loss: 1.9599
2022-07-05 20:05:06 - train: epoch 0025, iter [05000, 05004], lr: 0.097427, loss: 2.1588
2022-07-05 20:05:08 - train: epoch 025, train_loss: 2.0110
2022-07-05 20:06:24 - eval: epoch: 025, acc1: 57.840%, acc5: 82.220%, test_loss: 1.7734, per_image_load_time: 1.209ms, per_image_inference_time: 0.478ms
2022-07-05 20:06:24 - until epoch: 025, best_acc1: 57.840%
2022-07-05 20:06:24 - epoch 026 lr: 0.097427
2022-07-05 20:07:03 - train: epoch 0026, iter [00100, 05004], lr: 0.097422, loss: 1.7804
2022-07-05 20:07:38 - train: epoch 0026, iter [00200, 05004], lr: 0.097417, loss: 1.7435
2022-07-05 20:08:12 - train: epoch 0026, iter [00300, 05004], lr: 0.097412, loss: 1.9215
2022-07-05 20:08:46 - train: epoch 0026, iter [00400, 05004], lr: 0.097406, loss: 1.9219
2022-07-05 20:09:20 - train: epoch 0026, iter [00500, 05004], lr: 0.097401, loss: 2.0387
2022-07-05 20:09:54 - train: epoch 0026, iter [00600, 05004], lr: 0.097396, loss: 2.1289
2022-07-05 20:10:28 - train: epoch 0026, iter [00700, 05004], lr: 0.097391, loss: 1.8583
2022-07-05 20:11:03 - train: epoch 0026, iter [00800, 05004], lr: 0.097386, loss: 1.7735
2022-07-05 20:11:37 - train: epoch 0026, iter [00900, 05004], lr: 0.097381, loss: 2.1674
2022-07-05 20:12:11 - train: epoch 0026, iter [01000, 05004], lr: 0.097376, loss: 1.8826
2022-07-05 20:12:46 - train: epoch 0026, iter [01100, 05004], lr: 0.097370, loss: 1.9185
2022-07-05 20:13:20 - train: epoch 0026, iter [01200, 05004], lr: 0.097365, loss: 2.0303
2022-07-05 20:13:54 - train: epoch 0026, iter [01300, 05004], lr: 0.097360, loss: 1.8543
2022-07-05 20:14:28 - train: epoch 0026, iter [01400, 05004], lr: 0.097355, loss: 2.1221
2022-07-05 20:15:03 - train: epoch 0026, iter [01500, 05004], lr: 0.097350, loss: 1.8745
2022-07-05 20:15:37 - train: epoch 0026, iter [01600, 05004], lr: 0.097345, loss: 2.2290
2022-07-05 20:16:12 - train: epoch 0026, iter [01700, 05004], lr: 0.097339, loss: 1.8851
2022-07-05 20:16:47 - train: epoch 0026, iter [01800, 05004], lr: 0.097334, loss: 2.0426
2022-07-05 20:17:21 - train: epoch 0026, iter [01900, 05004], lr: 0.097329, loss: 2.2742
2022-07-05 20:17:55 - train: epoch 0026, iter [02000, 05004], lr: 0.097324, loss: 2.1957
2022-07-05 20:18:29 - train: epoch 0026, iter [02100, 05004], lr: 0.097319, loss: 2.0642
2022-07-05 20:19:04 - train: epoch 0026, iter [02200, 05004], lr: 0.097313, loss: 2.0156
2022-07-05 20:19:38 - train: epoch 0026, iter [02300, 05004], lr: 0.097308, loss: 1.9420
2022-07-05 20:20:13 - train: epoch 0026, iter [02400, 05004], lr: 0.097303, loss: 2.2031
2022-07-05 20:20:47 - train: epoch 0026, iter [02500, 05004], lr: 0.097298, loss: 2.0764
2022-07-05 20:21:22 - train: epoch 0026, iter [02600, 05004], lr: 0.097293, loss: 1.9700
2022-07-05 20:21:56 - train: epoch 0026, iter [02700, 05004], lr: 0.097287, loss: 1.8465
2022-07-05 20:22:30 - train: epoch 0026, iter [02800, 05004], lr: 0.097282, loss: 1.7973
2022-07-05 20:23:04 - train: epoch 0026, iter [02900, 05004], lr: 0.097277, loss: 2.1586
2022-07-05 20:23:39 - train: epoch 0026, iter [03000, 05004], lr: 0.097272, loss: 2.0056
2022-07-05 20:24:14 - train: epoch 0026, iter [03100, 05004], lr: 0.097266, loss: 2.0428
2022-07-05 20:24:48 - train: epoch 0026, iter [03200, 05004], lr: 0.097261, loss: 2.0665
2022-07-05 20:25:22 - train: epoch 0026, iter [03300, 05004], lr: 0.097256, loss: 1.9213
2022-07-05 20:25:56 - train: epoch 0026, iter [03400, 05004], lr: 0.097251, loss: 2.2391
2022-07-05 20:26:31 - train: epoch 0026, iter [03500, 05004], lr: 0.097245, loss: 2.1400
2022-07-05 20:27:05 - train: epoch 0026, iter [03600, 05004], lr: 0.097240, loss: 1.9166
2022-07-05 20:27:39 - train: epoch 0026, iter [03700, 05004], lr: 0.097235, loss: 2.1098
2022-07-05 20:28:14 - train: epoch 0026, iter [03800, 05004], lr: 0.097230, loss: 2.2720
2022-07-05 20:28:48 - train: epoch 0026, iter [03900, 05004], lr: 0.097224, loss: 2.1821
2022-07-05 20:29:22 - train: epoch 0026, iter [04000, 05004], lr: 0.097219, loss: 2.2650
2022-07-05 20:29:57 - train: epoch 0026, iter [04100, 05004], lr: 0.097214, loss: 2.1213
2022-07-05 20:30:31 - train: epoch 0026, iter [04200, 05004], lr: 0.097208, loss: 2.1179
2022-07-05 20:31:06 - train: epoch 0026, iter [04300, 05004], lr: 0.097203, loss: 2.0773
2022-07-05 20:31:40 - train: epoch 0026, iter [04400, 05004], lr: 0.097198, loss: 2.2691
2022-07-05 20:32:15 - train: epoch 0026, iter [04500, 05004], lr: 0.097192, loss: 2.2095
2022-07-05 20:32:50 - train: epoch 0026, iter [04600, 05004], lr: 0.097187, loss: 1.9777
2022-07-05 20:33:24 - train: epoch 0026, iter [04700, 05004], lr: 0.097182, loss: 1.8407
2022-07-05 20:33:58 - train: epoch 0026, iter [04800, 05004], lr: 0.097176, loss: 1.9627
2022-07-05 20:34:33 - train: epoch 0026, iter [04900, 05004], lr: 0.097171, loss: 1.9870
2022-07-05 20:35:06 - train: epoch 0026, iter [05000, 05004], lr: 0.097166, loss: 2.2036
2022-07-05 20:35:07 - train: epoch 026, train_loss: 2.0032
2022-07-05 20:36:23 - eval: epoch: 026, acc1: 56.124%, acc5: 80.854%, test_loss: 1.8556, per_image_load_time: 0.599ms, per_image_inference_time: 0.475ms
2022-07-05 20:36:23 - until epoch: 026, best_acc1: 57.840%
2022-07-05 20:36:23 - epoch 027 lr: 0.097166
2022-07-05 20:37:03 - train: epoch 0027, iter [00100, 05004], lr: 0.097160, loss: 2.1012
2022-07-05 20:37:38 - train: epoch 0027, iter [00200, 05004], lr: 0.097155, loss: 1.8887
2022-07-05 20:38:11 - train: epoch 0027, iter [00300, 05004], lr: 0.097150, loss: 2.0275
2022-07-05 20:38:45 - train: epoch 0027, iter [00400, 05004], lr: 0.097144, loss: 2.1205
2022-07-05 20:39:19 - train: epoch 0027, iter [00500, 05004], lr: 0.097139, loss: 2.0760
2022-07-05 20:39:54 - train: epoch 0027, iter [00600, 05004], lr: 0.097133, loss: 2.1582
2022-07-05 20:40:29 - train: epoch 0027, iter [00700, 05004], lr: 0.097128, loss: 2.1721
2022-07-05 20:41:03 - train: epoch 0027, iter [00800, 05004], lr: 0.097123, loss: 1.9993
2022-07-05 20:41:37 - train: epoch 0027, iter [00900, 05004], lr: 0.097117, loss: 1.8928
2022-07-05 20:42:11 - train: epoch 0027, iter [01000, 05004], lr: 0.097112, loss: 2.0650
2022-07-05 20:42:46 - train: epoch 0027, iter [01100, 05004], lr: 0.097107, loss: 1.9932
2022-07-05 20:43:20 - train: epoch 0027, iter [01200, 05004], lr: 0.097101, loss: 2.1747
2022-07-05 20:43:54 - train: epoch 0027, iter [01300, 05004], lr: 0.097096, loss: 2.0231
2022-07-05 20:44:28 - train: epoch 0027, iter [01400, 05004], lr: 0.097090, loss: 2.1470
2022-07-05 20:45:03 - train: epoch 0027, iter [01500, 05004], lr: 0.097085, loss: 2.0137
2022-07-05 20:45:37 - train: epoch 0027, iter [01600, 05004], lr: 0.097079, loss: 2.0540
2022-07-05 20:46:11 - train: epoch 0027, iter [01700, 05004], lr: 0.097074, loss: 2.0023
2022-07-05 20:46:46 - train: epoch 0027, iter [01800, 05004], lr: 0.097069, loss: 1.9033
2022-07-05 20:47:20 - train: epoch 0027, iter [01900, 05004], lr: 0.097063, loss: 2.1213
2022-07-05 20:47:55 - train: epoch 0027, iter [02000, 05004], lr: 0.097058, loss: 1.9386
2022-07-05 20:48:29 - train: epoch 0027, iter [02100, 05004], lr: 0.097052, loss: 2.2372
2022-07-05 20:49:04 - train: epoch 0027, iter [02200, 05004], lr: 0.097047, loss: 2.1420
2022-07-05 20:49:38 - train: epoch 0027, iter [02300, 05004], lr: 0.097041, loss: 2.2511
2022-07-05 20:50:14 - train: epoch 0027, iter [02400, 05004], lr: 0.097036, loss: 2.0347
2022-07-05 20:50:48 - train: epoch 0027, iter [02500, 05004], lr: 0.097030, loss: 1.8864
2022-07-05 20:51:22 - train: epoch 0027, iter [02600, 05004], lr: 0.097025, loss: 2.1828
2022-07-05 20:51:57 - train: epoch 0027, iter [02700, 05004], lr: 0.097020, loss: 1.9111
2022-07-05 20:52:31 - train: epoch 0027, iter [02800, 05004], lr: 0.097014, loss: 2.1617
2022-07-05 20:53:06 - train: epoch 0027, iter [02900, 05004], lr: 0.097009, loss: 2.0689
2022-07-05 20:53:41 - train: epoch 0027, iter [03000, 05004], lr: 0.097003, loss: 1.8057
2022-07-05 20:54:15 - train: epoch 0027, iter [03100, 05004], lr: 0.096998, loss: 1.9125
2022-07-05 20:54:49 - train: epoch 0027, iter [03200, 05004], lr: 0.096992, loss: 1.8580
2022-07-05 20:55:24 - train: epoch 0027, iter [03300, 05004], lr: 0.096987, loss: 2.1132
2022-07-05 20:55:58 - train: epoch 0027, iter [03400, 05004], lr: 0.096981, loss: 1.9644
2022-07-05 20:56:32 - train: epoch 0027, iter [03500, 05004], lr: 0.096976, loss: 2.2370
2022-07-05 20:57:07 - train: epoch 0027, iter [03600, 05004], lr: 0.096970, loss: 1.8058
2022-07-05 20:57:40 - train: epoch 0027, iter [03700, 05004], lr: 0.096965, loss: 1.9319
2022-07-05 20:58:16 - train: epoch 0027, iter [03800, 05004], lr: 0.096959, loss: 1.7423
2022-07-05 20:58:50 - train: epoch 0027, iter [03900, 05004], lr: 0.096954, loss: 1.8260
2022-07-05 20:59:24 - train: epoch 0027, iter [04000, 05004], lr: 0.096948, loss: 2.0411
2022-07-05 20:59:59 - train: epoch 0027, iter [04100, 05004], lr: 0.096942, loss: 1.9602
2022-07-05 21:00:34 - train: epoch 0027, iter [04200, 05004], lr: 0.096937, loss: 2.0680
2022-07-05 21:01:08 - train: epoch 0027, iter [04300, 05004], lr: 0.096931, loss: 1.9530
2022-07-05 21:01:42 - train: epoch 0027, iter [04400, 05004], lr: 0.096926, loss: 1.9544
2022-07-05 21:02:17 - train: epoch 0027, iter [04500, 05004], lr: 0.096920, loss: 1.8181
2022-07-05 21:02:51 - train: epoch 0027, iter [04600, 05004], lr: 0.096915, loss: 1.9109
2022-07-05 21:03:26 - train: epoch 0027, iter [04700, 05004], lr: 0.096909, loss: 2.0021
2022-07-05 21:04:01 - train: epoch 0027, iter [04800, 05004], lr: 0.096904, loss: 2.0194
2022-07-05 21:04:35 - train: epoch 0027, iter [04900, 05004], lr: 0.096898, loss: 2.0360
2022-07-05 21:05:08 - train: epoch 0027, iter [05000, 05004], lr: 0.096892, loss: 1.6433
2022-07-05 21:05:09 - train: epoch 027, train_loss: 1.9942
2022-07-05 21:06:24 - eval: epoch: 027, acc1: 58.566%, acc5: 82.514%, test_loss: 1.7385, per_image_load_time: 0.745ms, per_image_inference_time: 0.491ms
2022-07-05 21:06:24 - until epoch: 027, best_acc1: 58.566%
2022-07-05 21:06:24 - epoch 028 lr: 0.096892
2022-07-05 21:07:04 - train: epoch 0028, iter [00100, 05004], lr: 0.096887, loss: 1.7477
2022-07-05 21:07:38 - train: epoch 0028, iter [00200, 05004], lr: 0.096881, loss: 1.8170
2022-07-05 21:08:12 - train: epoch 0028, iter [00300, 05004], lr: 0.096875, loss: 2.0032
2022-07-05 21:08:46 - train: epoch 0028, iter [00400, 05004], lr: 0.096870, loss: 1.8290
2022-07-05 21:09:20 - train: epoch 0028, iter [00500, 05004], lr: 0.096864, loss: 1.9280
2022-07-05 21:09:55 - train: epoch 0028, iter [00600, 05004], lr: 0.096859, loss: 2.1120
2022-07-05 21:10:29 - train: epoch 0028, iter [00700, 05004], lr: 0.096853, loss: 2.2389
2022-07-05 21:11:03 - train: epoch 0028, iter [00800, 05004], lr: 0.096847, loss: 1.7059
2022-07-05 21:11:37 - train: epoch 0028, iter [00900, 05004], lr: 0.096842, loss: 1.8074
2022-07-05 21:12:13 - train: epoch 0028, iter [01000, 05004], lr: 0.096836, loss: 1.9617
2022-07-05 21:12:46 - train: epoch 0028, iter [01100, 05004], lr: 0.096830, loss: 2.0654
2022-07-05 21:13:20 - train: epoch 0028, iter [01200, 05004], lr: 0.096825, loss: 1.8393
2022-07-05 21:13:55 - train: epoch 0028, iter [01300, 05004], lr: 0.096819, loss: 1.9903
2022-07-05 21:14:30 - train: epoch 0028, iter [01400, 05004], lr: 0.096813, loss: 2.2541
2022-07-05 21:15:03 - train: epoch 0028, iter [01500, 05004], lr: 0.096808, loss: 1.9678
2022-07-05 21:15:38 - train: epoch 0028, iter [01600, 05004], lr: 0.096802, loss: 1.9991
2022-07-05 21:16:12 - train: epoch 0028, iter [01700, 05004], lr: 0.096796, loss: 1.9688
2022-07-05 21:16:47 - train: epoch 0028, iter [01800, 05004], lr: 0.096791, loss: 1.9083
2022-07-05 21:17:21 - train: epoch 0028, iter [01900, 05004], lr: 0.096785, loss: 2.0258
2022-07-05 21:17:56 - train: epoch 0028, iter [02000, 05004], lr: 0.096779, loss: 2.0971
2022-07-05 21:18:30 - train: epoch 0028, iter [02100, 05004], lr: 0.096774, loss: 2.0379
2022-07-05 21:19:05 - train: epoch 0028, iter [02200, 05004], lr: 0.096768, loss: 2.0315
2022-07-05 21:19:39 - train: epoch 0028, iter [02300, 05004], lr: 0.096762, loss: 2.1841
2022-07-05 21:20:14 - train: epoch 0028, iter [02400, 05004], lr: 0.096757, loss: 2.1489
2022-07-05 21:20:49 - train: epoch 0028, iter [02500, 05004], lr: 0.096751, loss: 2.0607
2022-07-05 21:21:24 - train: epoch 0028, iter [02600, 05004], lr: 0.096745, loss: 1.8873
2022-07-05 21:21:59 - train: epoch 0028, iter [02700, 05004], lr: 0.096740, loss: 1.9831
2022-07-05 21:22:33 - train: epoch 0028, iter [02800, 05004], lr: 0.096734, loss: 1.9643
2022-07-05 21:23:08 - train: epoch 0028, iter [02900, 05004], lr: 0.096728, loss: 2.0763
2022-07-05 21:23:43 - train: epoch 0028, iter [03000, 05004], lr: 0.096722, loss: 2.0892
2022-07-05 21:24:18 - train: epoch 0028, iter [03100, 05004], lr: 0.096717, loss: 2.1930
2022-07-05 21:24:51 - train: epoch 0028, iter [03200, 05004], lr: 0.096711, loss: 1.9680
2022-07-05 21:25:27 - train: epoch 0028, iter [03300, 05004], lr: 0.096705, loss: 2.0320
2022-07-05 21:26:02 - train: epoch 0028, iter [03400, 05004], lr: 0.096699, loss: 1.9879
2022-07-05 21:26:36 - train: epoch 0028, iter [03500, 05004], lr: 0.096694, loss: 1.9005
2022-07-05 21:27:11 - train: epoch 0028, iter [03600, 05004], lr: 0.096688, loss: 1.8228
2022-07-05 21:27:45 - train: epoch 0028, iter [03700, 05004], lr: 0.096682, loss: 2.0119
2022-07-05 21:28:20 - train: epoch 0028, iter [03800, 05004], lr: 0.096676, loss: 1.6423
2022-07-05 21:28:55 - train: epoch 0028, iter [03900, 05004], lr: 0.096671, loss: 1.9160
2022-07-05 21:29:29 - train: epoch 0028, iter [04000, 05004], lr: 0.096665, loss: 2.0345
2022-07-05 21:30:03 - train: epoch 0028, iter [04100, 05004], lr: 0.096659, loss: 2.0602
2022-07-05 21:30:39 - train: epoch 0028, iter [04200, 05004], lr: 0.096653, loss: 2.1519
2022-07-05 21:31:13 - train: epoch 0028, iter [04300, 05004], lr: 0.096647, loss: 1.8567
2022-07-05 21:31:47 - train: epoch 0028, iter [04400, 05004], lr: 0.096642, loss: 1.9535
2022-07-05 21:32:22 - train: epoch 0028, iter [04500, 05004], lr: 0.096636, loss: 1.9465
2022-07-05 21:32:56 - train: epoch 0028, iter [04600, 05004], lr: 0.096630, loss: 2.1012
2022-07-05 21:33:31 - train: epoch 0028, iter [04700, 05004], lr: 0.096624, loss: 2.0677
2022-07-05 21:34:05 - train: epoch 0028, iter [04800, 05004], lr: 0.096618, loss: 1.9457
2022-07-05 21:34:41 - train: epoch 0028, iter [04900, 05004], lr: 0.096613, loss: 1.8341
2022-07-05 21:35:14 - train: epoch 0028, iter [05000, 05004], lr: 0.096607, loss: 1.8913
2022-07-05 21:35:15 - train: epoch 028, train_loss: 1.9853
2022-07-05 21:36:31 - eval: epoch: 028, acc1: 57.484%, acc5: 81.358%, test_loss: 1.8069, per_image_load_time: 0.678ms, per_image_inference_time: 0.479ms
2022-07-05 21:36:32 - until epoch: 028, best_acc1: 58.566%
2022-07-05 21:36:32 - epoch 029 lr: 0.096606
2022-07-05 21:37:12 - train: epoch 0029, iter [00100, 05004], lr: 0.096601, loss: 2.1268
2022-07-05 21:37:46 - train: epoch 0029, iter [00200, 05004], lr: 0.096595, loss: 1.9642
2022-07-05 21:38:21 - train: epoch 0029, iter [00300, 05004], lr: 0.096589, loss: 2.1768
2022-07-05 21:38:54 - train: epoch 0029, iter [00400, 05004], lr: 0.096583, loss: 2.0096
2022-07-05 21:39:29 - train: epoch 0029, iter [00500, 05004], lr: 0.096577, loss: 2.0424
2022-07-05 21:40:02 - train: epoch 0029, iter [00600, 05004], lr: 0.096571, loss: 2.0616
2022-07-05 21:40:37 - train: epoch 0029, iter [00700, 05004], lr: 0.096566, loss: 1.4568
2022-07-05 21:41:11 - train: epoch 0029, iter [00800, 05004], lr: 0.096560, loss: 2.1070
2022-07-05 21:41:45 - train: epoch 0029, iter [00900, 05004], lr: 0.096554, loss: 1.9642
2022-07-05 21:42:19 - train: epoch 0029, iter [01000, 05004], lr: 0.096548, loss: 1.7558
2022-07-05 21:42:54 - train: epoch 0029, iter [01100, 05004], lr: 0.096542, loss: 1.8482
2022-07-05 21:43:28 - train: epoch 0029, iter [01200, 05004], lr: 0.096536, loss: 2.0087
2022-07-05 21:44:03 - train: epoch 0029, iter [01300, 05004], lr: 0.096530, loss: 1.8836
2022-07-05 21:44:37 - train: epoch 0029, iter [01400, 05004], lr: 0.096524, loss: 2.1951
2022-07-05 21:45:12 - train: epoch 0029, iter [01500, 05004], lr: 0.096518, loss: 2.1547
2022-07-05 21:45:46 - train: epoch 0029, iter [01600, 05004], lr: 0.096513, loss: 1.9576
2022-07-05 21:46:20 - train: epoch 0029, iter [01700, 05004], lr: 0.096507, loss: 2.0625
2022-07-05 21:46:55 - train: epoch 0029, iter [01800, 05004], lr: 0.096501, loss: 2.0735
2022-07-05 21:47:30 - train: epoch 0029, iter [01900, 05004], lr: 0.096495, loss: 1.8439
2022-07-05 21:48:04 - train: epoch 0029, iter [02000, 05004], lr: 0.096489, loss: 2.1467
2022-07-05 21:48:38 - train: epoch 0029, iter [02100, 05004], lr: 0.096483, loss: 1.9845
2022-07-05 21:49:13 - train: epoch 0029, iter [02200, 05004], lr: 0.096477, loss: 2.0492
2022-07-05 21:49:48 - train: epoch 0029, iter [02300, 05004], lr: 0.096471, loss: 2.0786
2022-07-05 21:50:22 - train: epoch 0029, iter [02400, 05004], lr: 0.096465, loss: 1.8458
2022-07-05 21:50:57 - train: epoch 0029, iter [02500, 05004], lr: 0.096459, loss: 1.8556
2022-07-05 21:51:31 - train: epoch 0029, iter [02600, 05004], lr: 0.096453, loss: 1.9699
2022-07-05 21:52:05 - train: epoch 0029, iter [02700, 05004], lr: 0.096447, loss: 2.0074
2022-07-05 21:52:40 - train: epoch 0029, iter [02800, 05004], lr: 0.096441, loss: 1.8123
2022-07-05 21:53:15 - train: epoch 0029, iter [02900, 05004], lr: 0.096435, loss: 1.8907
2022-07-05 21:53:49 - train: epoch 0029, iter [03000, 05004], lr: 0.096429, loss: 1.8867
2022-07-05 21:54:24 - train: epoch 0029, iter [03100, 05004], lr: 0.096423, loss: 1.9572
2022-07-05 21:54:59 - train: epoch 0029, iter [03200, 05004], lr: 0.096417, loss: 1.9502
2022-07-05 21:55:34 - train: epoch 0029, iter [03300, 05004], lr: 0.096411, loss: 1.9763
2022-07-05 21:56:08 - train: epoch 0029, iter [03400, 05004], lr: 0.096405, loss: 1.7688
2022-07-05 21:56:43 - train: epoch 0029, iter [03500, 05004], lr: 0.096399, loss: 2.1500
2022-07-05 21:57:17 - train: epoch 0029, iter [03600, 05004], lr: 0.096393, loss: 1.9675
2022-07-05 21:57:52 - train: epoch 0029, iter [03700, 05004], lr: 0.096387, loss: 1.9256
2022-07-05 21:58:27 - train: epoch 0029, iter [03800, 05004], lr: 0.096381, loss: 1.9810
2022-07-05 21:59:01 - train: epoch 0029, iter [03900, 05004], lr: 0.096375, loss: 1.7635
2022-07-05 21:59:36 - train: epoch 0029, iter [04000, 05004], lr: 0.096369, loss: 1.8526
2022-07-05 22:00:11 - train: epoch 0029, iter [04100, 05004], lr: 0.096363, loss: 1.8042
2022-07-05 22:00:44 - train: epoch 0029, iter [04200, 05004], lr: 0.096357, loss: 1.8390
2022-07-05 22:01:19 - train: epoch 0029, iter [04300, 05004], lr: 0.096351, loss: 2.0127
2022-07-05 22:01:54 - train: epoch 0029, iter [04400, 05004], lr: 0.096345, loss: 1.9174
2022-07-05 22:02:28 - train: epoch 0029, iter [04500, 05004], lr: 0.096339, loss: 2.0755
2022-07-05 22:03:03 - train: epoch 0029, iter [04600, 05004], lr: 0.096333, loss: 2.1568
2022-07-05 22:03:38 - train: epoch 0029, iter [04700, 05004], lr: 0.096327, loss: 1.7240
2022-07-05 22:04:12 - train: epoch 0029, iter [04800, 05004], lr: 0.096321, loss: 2.0778
2022-07-05 22:04:47 - train: epoch 0029, iter [04900, 05004], lr: 0.096315, loss: 2.3228
2022-07-05 22:05:20 - train: epoch 0029, iter [05000, 05004], lr: 0.096309, loss: 1.7322
2022-07-05 22:05:21 - train: epoch 029, train_loss: 1.9801
2022-07-05 22:06:38 - eval: epoch: 029, acc1: 57.446%, acc5: 81.932%, test_loss: 1.7857, per_image_load_time: 0.676ms, per_image_inference_time: 0.467ms
2022-07-05 22:06:38 - until epoch: 029, best_acc1: 58.566%
2022-07-05 22:06:38 - epoch 030 lr: 0.096309
2022-07-05 22:07:18 - train: epoch 0030, iter [00100, 05004], lr: 0.096303, loss: 2.0995
2022-07-05 22:07:51 - train: epoch 0030, iter [00200, 05004], lr: 0.096297, loss: 1.9780
2022-07-05 22:08:26 - train: epoch 0030, iter [00300, 05004], lr: 0.096290, loss: 1.9206
2022-07-05 22:08:59 - train: epoch 0030, iter [00400, 05004], lr: 0.096284, loss: 1.7362
2022-07-05 22:09:34 - train: epoch 0030, iter [00500, 05004], lr: 0.096278, loss: 2.2614
2022-07-05 22:10:09 - train: epoch 0030, iter [00600, 05004], lr: 0.096272, loss: 1.7910
2022-07-05 22:10:43 - train: epoch 0030, iter [00700, 05004], lr: 0.096266, loss: 1.8800
2022-07-05 22:11:17 - train: epoch 0030, iter [00800, 05004], lr: 0.096260, loss: 2.1786
2022-07-05 22:11:52 - train: epoch 0030, iter [00900, 05004], lr: 0.096254, loss: 1.9606
2022-07-05 22:12:26 - train: epoch 0030, iter [01000, 05004], lr: 0.096248, loss: 1.7002
2022-07-05 22:13:01 - train: epoch 0030, iter [01100, 05004], lr: 0.096242, loss: 1.8527
2022-07-05 22:13:34 - train: epoch 0030, iter [01200, 05004], lr: 0.096236, loss: 2.0381
2022-07-05 22:14:09 - train: epoch 0030, iter [01300, 05004], lr: 0.096229, loss: 1.8267
2022-07-05 22:14:43 - train: epoch 0030, iter [01400, 05004], lr: 0.096223, loss: 1.9057
2022-07-05 22:15:18 - train: epoch 0030, iter [01500, 05004], lr: 0.096217, loss: 1.9718
2022-07-05 22:15:52 - train: epoch 0030, iter [01600, 05004], lr: 0.096211, loss: 1.9638
2022-07-05 22:16:26 - train: epoch 0030, iter [01700, 05004], lr: 0.096205, loss: 2.1677
2022-07-05 22:17:01 - train: epoch 0030, iter [01800, 05004], lr: 0.096199, loss: 1.9367
2022-07-05 22:17:35 - train: epoch 0030, iter [01900, 05004], lr: 0.096193, loss: 2.1919
2022-07-05 22:18:10 - train: epoch 0030, iter [02000, 05004], lr: 0.096186, loss: 1.9523
2022-07-05 22:18:45 - train: epoch 0030, iter [02100, 05004], lr: 0.096180, loss: 2.0080
2022-07-05 22:19:18 - train: epoch 0030, iter [02200, 05004], lr: 0.096174, loss: 1.8534
2022-07-05 22:19:53 - train: epoch 0030, iter [02300, 05004], lr: 0.096168, loss: 1.9223
2022-07-05 22:20:28 - train: epoch 0030, iter [02400, 05004], lr: 0.096162, loss: 2.1653
2022-07-05 22:21:02 - train: epoch 0030, iter [02500, 05004], lr: 0.096155, loss: 2.0311
2022-07-05 22:21:36 - train: epoch 0030, iter [02600, 05004], lr: 0.096149, loss: 1.9086
2022-07-05 22:22:11 - train: epoch 0030, iter [02700, 05004], lr: 0.096143, loss: 1.8653
2022-07-05 22:22:46 - train: epoch 0030, iter [02800, 05004], lr: 0.096137, loss: 1.9826
2022-07-05 22:23:19 - train: epoch 0030, iter [02900, 05004], lr: 0.096131, loss: 2.0459
2022-07-05 22:23:54 - train: epoch 0030, iter [03000, 05004], lr: 0.096124, loss: 2.1981
2022-07-05 22:24:29 - train: epoch 0030, iter [03100, 05004], lr: 0.096118, loss: 1.9254
2022-07-05 22:25:03 - train: epoch 0030, iter [03200, 05004], lr: 0.096112, loss: 1.9874
2022-07-05 22:25:36 - train: epoch 0030, iter [03300, 05004], lr: 0.096106, loss: 2.2248
2022-07-05 22:26:12 - train: epoch 0030, iter [03400, 05004], lr: 0.096100, loss: 1.9534
2022-07-05 22:26:47 - train: epoch 0030, iter [03500, 05004], lr: 0.096093, loss: 2.0569
2022-07-05 22:27:22 - train: epoch 0030, iter [03600, 05004], lr: 0.096087, loss: 1.8800
2022-07-05 22:27:57 - train: epoch 0030, iter [03700, 05004], lr: 0.096081, loss: 1.9431
2022-07-05 22:28:31 - train: epoch 0030, iter [03800, 05004], lr: 0.096075, loss: 2.1166
2022-07-05 22:29:05 - train: epoch 0030, iter [03900, 05004], lr: 0.096068, loss: 2.1017
2022-07-05 22:29:40 - train: epoch 0030, iter [04000, 05004], lr: 0.096062, loss: 1.8428
2022-07-05 22:30:14 - train: epoch 0030, iter [04100, 05004], lr: 0.096056, loss: 1.8808
2022-07-05 22:30:48 - train: epoch 0030, iter [04200, 05004], lr: 0.096050, loss: 2.2105
2022-07-05 22:31:24 - train: epoch 0030, iter [04300, 05004], lr: 0.096043, loss: 2.0087
2022-07-05 22:31:58 - train: epoch 0030, iter [04400, 05004], lr: 0.096037, loss: 1.9767
2022-07-05 22:32:33 - train: epoch 0030, iter [04500, 05004], lr: 0.096031, loss: 2.2338
2022-07-05 22:33:06 - train: epoch 0030, iter [04600, 05004], lr: 0.096024, loss: 1.7577
2022-07-05 22:33:41 - train: epoch 0030, iter [04700, 05004], lr: 0.096018, loss: 1.9998
2022-07-05 22:34:16 - train: epoch 0030, iter [04800, 05004], lr: 0.096012, loss: 2.0220
2022-07-05 22:34:51 - train: epoch 0030, iter [04900, 05004], lr: 0.096006, loss: 2.1704
2022-07-05 22:35:24 - train: epoch 0030, iter [05000, 05004], lr: 0.095999, loss: 2.0647
2022-07-05 22:35:26 - train: epoch 030, train_loss: 1.9765
2022-07-05 22:36:42 - eval: epoch: 030, acc1: 58.522%, acc5: 82.542%, test_loss: 1.7415, per_image_load_time: 0.740ms, per_image_inference_time: 0.478ms
2022-07-05 22:36:42 - until epoch: 030, best_acc1: 58.566%
2022-07-05 22:36:42 - epoch 031 lr: 0.095999
2022-07-05 22:37:22 - train: epoch 0031, iter [00100, 05004], lr: 0.095993, loss: 1.9973
2022-07-05 22:37:57 - train: epoch 0031, iter [00200, 05004], lr: 0.095986, loss: 1.8190
2022-07-05 22:38:30 - train: epoch 0031, iter [00300, 05004], lr: 0.095980, loss: 1.9206
2022-07-05 22:39:05 - train: epoch 0031, iter [00400, 05004], lr: 0.095974, loss: 2.0400
2022-07-05 22:39:40 - train: epoch 0031, iter [00500, 05004], lr: 0.095967, loss: 1.8537
2022-07-05 22:40:15 - train: epoch 0031, iter [00600, 05004], lr: 0.095961, loss: 1.9168
2022-07-05 22:40:49 - train: epoch 0031, iter [00700, 05004], lr: 0.095955, loss: 2.0230
2022-07-05 22:41:24 - train: epoch 0031, iter [00800, 05004], lr: 0.095948, loss: 1.8804
2022-07-05 22:41:58 - train: epoch 0031, iter [00900, 05004], lr: 0.095942, loss: 1.8944
2022-07-05 22:42:32 - train: epoch 0031, iter [01000, 05004], lr: 0.095936, loss: 2.1584
2022-07-05 22:43:07 - train: epoch 0031, iter [01100, 05004], lr: 0.095929, loss: 2.1432
2022-07-05 22:43:41 - train: epoch 0031, iter [01200, 05004], lr: 0.095923, loss: 1.9912
2022-07-05 22:44:15 - train: epoch 0031, iter [01300, 05004], lr: 0.095917, loss: 1.7092
2022-07-05 22:44:50 - train: epoch 0031, iter [01400, 05004], lr: 0.095910, loss: 2.0860
2022-07-05 22:45:24 - train: epoch 0031, iter [01500, 05004], lr: 0.095904, loss: 2.1484
2022-07-05 22:45:58 - train: epoch 0031, iter [01600, 05004], lr: 0.095897, loss: 1.8388
2022-07-05 22:46:32 - train: epoch 0031, iter [01700, 05004], lr: 0.095891, loss: 1.8667
2022-07-05 22:47:07 - train: epoch 0031, iter [01800, 05004], lr: 0.095885, loss: 1.9236
2022-07-05 22:47:41 - train: epoch 0031, iter [01900, 05004], lr: 0.095878, loss: 1.9402
2022-07-05 22:48:16 - train: epoch 0031, iter [02000, 05004], lr: 0.095872, loss: 2.0422
2022-07-05 22:48:51 - train: epoch 0031, iter [02100, 05004], lr: 0.095865, loss: 1.7965
2022-07-05 22:49:25 - train: epoch 0031, iter [02200, 05004], lr: 0.095859, loss: 1.9023
2022-07-05 22:50:00 - train: epoch 0031, iter [02300, 05004], lr: 0.095853, loss: 1.7505
2022-07-05 22:50:34 - train: epoch 0031, iter [02400, 05004], lr: 0.095846, loss: 1.9539
2022-07-05 22:51:10 - train: epoch 0031, iter [02500, 05004], lr: 0.095840, loss: 1.8852
2022-07-05 22:51:44 - train: epoch 0031, iter [02600, 05004], lr: 0.095833, loss: 1.8368
2022-07-05 22:52:18 - train: epoch 0031, iter [02700, 05004], lr: 0.095827, loss: 2.1194
2022-07-05 22:52:53 - train: epoch 0031, iter [02800, 05004], lr: 0.095820, loss: 2.3189
2022-07-05 22:53:28 - train: epoch 0031, iter [02900, 05004], lr: 0.095814, loss: 2.0384
2022-07-05 22:54:02 - train: epoch 0031, iter [03000, 05004], lr: 0.095808, loss: 2.1218
2022-07-05 22:54:36 - train: epoch 0031, iter [03100, 05004], lr: 0.095801, loss: 1.9632
2022-07-05 22:55:11 - train: epoch 0031, iter [03200, 05004], lr: 0.095795, loss: 2.1959
2022-07-05 22:55:45 - train: epoch 0031, iter [03300, 05004], lr: 0.095788, loss: 2.0801
2022-07-05 22:56:20 - train: epoch 0031, iter [03400, 05004], lr: 0.095782, loss: 2.0856
2022-07-05 22:56:56 - train: epoch 0031, iter [03500, 05004], lr: 0.095775, loss: 2.1004
2022-07-05 22:57:29 - train: epoch 0031, iter [03600, 05004], lr: 0.095769, loss: 1.9510
2022-07-05 22:58:05 - train: epoch 0031, iter [03700, 05004], lr: 0.095762, loss: 1.8399
2022-07-05 22:58:40 - train: epoch 0031, iter [03800, 05004], lr: 0.095756, loss: 1.8446
2022-07-05 22:59:16 - train: epoch 0031, iter [03900, 05004], lr: 0.095749, loss: 1.9491
2022-07-05 22:59:51 - train: epoch 0031, iter [04000, 05004], lr: 0.095743, loss: 1.9899
2022-07-05 23:00:25 - train: epoch 0031, iter [04100, 05004], lr: 0.095736, loss: 2.0182
2022-07-05 23:01:00 - train: epoch 0031, iter [04200, 05004], lr: 0.095730, loss: 2.0386
2022-07-05 23:01:35 - train: epoch 0031, iter [04300, 05004], lr: 0.095723, loss: 1.9402
2022-07-05 23:02:09 - train: epoch 0031, iter [04400, 05004], lr: 0.095717, loss: 2.1343
2022-07-05 23:02:44 - train: epoch 0031, iter [04500, 05004], lr: 0.095710, loss: 2.1685
2022-07-05 23:03:19 - train: epoch 0031, iter [04600, 05004], lr: 0.095704, loss: 2.0628
2022-07-05 23:03:53 - train: epoch 0031, iter [04700, 05004], lr: 0.095697, loss: 1.9740
2022-07-05 23:04:29 - train: epoch 0031, iter [04800, 05004], lr: 0.095691, loss: 1.8929
2022-07-05 23:05:04 - train: epoch 0031, iter [04900, 05004], lr: 0.095684, loss: 1.7499
2022-07-05 23:05:37 - train: epoch 0031, iter [05000, 05004], lr: 0.095678, loss: 1.8130
2022-07-05 23:05:38 - train: epoch 031, train_loss: 1.9685
2022-07-05 23:06:54 - eval: epoch: 031, acc1: 56.190%, acc5: 80.868%, test_loss: 1.8589, per_image_load_time: 0.611ms, per_image_inference_time: 0.462ms
2022-07-05 23:06:55 - until epoch: 031, best_acc1: 58.566%
2022-07-05 23:06:55 - epoch 032 lr: 0.095677
2022-07-05 23:07:35 - train: epoch 0032, iter [00100, 05004], lr: 0.095671, loss: 1.8442
2022-07-05 23:08:09 - train: epoch 0032, iter [00200, 05004], lr: 0.095664, loss: 2.0496
2022-07-05 23:08:43 - train: epoch 0032, iter [00300, 05004], lr: 0.095658, loss: 1.9575
2022-07-05 23:09:18 - train: epoch 0032, iter [00400, 05004], lr: 0.095651, loss: 2.1924
2022-07-05 23:09:52 - train: epoch 0032, iter [00500, 05004], lr: 0.095644, loss: 2.0990
2022-07-05 23:10:27 - train: epoch 0032, iter [00600, 05004], lr: 0.095638, loss: 1.9347
2022-07-05 23:11:00 - train: epoch 0032, iter [00700, 05004], lr: 0.095631, loss: 1.9516
2022-07-05 23:11:35 - train: epoch 0032, iter [00800, 05004], lr: 0.095625, loss: 1.9377
2022-07-05 23:12:09 - train: epoch 0032, iter [00900, 05004], lr: 0.095618, loss: 1.8645
2022-07-05 23:12:43 - train: epoch 0032, iter [01000, 05004], lr: 0.095612, loss: 2.0536
2022-07-05 23:13:18 - train: epoch 0032, iter [01100, 05004], lr: 0.095605, loss: 2.1235
2022-07-05 23:13:52 - train: epoch 0032, iter [01200, 05004], lr: 0.095598, loss: 1.9810
2022-07-05 23:14:27 - train: epoch 0032, iter [01300, 05004], lr: 0.095592, loss: 1.9664
2022-07-05 23:15:02 - train: epoch 0032, iter [01400, 05004], lr: 0.095585, loss: 2.0685
2022-07-05 23:15:36 - train: epoch 0032, iter [01500, 05004], lr: 0.095579, loss: 1.9741
2022-07-05 23:16:11 - train: epoch 0032, iter [01600, 05004], lr: 0.095572, loss: 2.0495
2022-07-05 23:16:46 - train: epoch 0032, iter [01700, 05004], lr: 0.095565, loss: 2.1338
2022-07-05 23:17:20 - train: epoch 0032, iter [01800, 05004], lr: 0.095559, loss: 2.2308
2022-07-05 23:17:55 - train: epoch 0032, iter [01900, 05004], lr: 0.095552, loss: 1.7853
2022-07-05 23:18:30 - train: epoch 0032, iter [02000, 05004], lr: 0.095545, loss: 1.9173
2022-07-05 23:19:04 - train: epoch 0032, iter [02100, 05004], lr: 0.095539, loss: 1.8470
2022-07-05 23:19:39 - train: epoch 0032, iter [02200, 05004], lr: 0.095532, loss: 1.8939
2022-07-05 23:20:13 - train: epoch 0032, iter [02300, 05004], lr: 0.095525, loss: 1.9869
2022-07-05 23:20:48 - train: epoch 0032, iter [02400, 05004], lr: 0.095519, loss: 1.8582
2022-07-05 23:21:22 - train: epoch 0032, iter [02500, 05004], lr: 0.095512, loss: 2.0045
2022-07-05 23:21:58 - train: epoch 0032, iter [02600, 05004], lr: 0.095505, loss: 1.7067
2022-07-05 23:22:31 - train: epoch 0032, iter [02700, 05004], lr: 0.095499, loss: 1.9374
2022-07-05 23:23:06 - train: epoch 0032, iter [02800, 05004], lr: 0.095492, loss: 2.1706
2022-07-05 23:23:41 - train: epoch 0032, iter [02900, 05004], lr: 0.095485, loss: 1.7119
2022-07-05 23:24:16 - train: epoch 0032, iter [03000, 05004], lr: 0.095479, loss: 1.7127
2022-07-05 23:24:49 - train: epoch 0032, iter [03100, 05004], lr: 0.095472, loss: 2.1225
2022-07-05 23:25:25 - train: epoch 0032, iter [03200, 05004], lr: 0.095465, loss: 2.0893
2022-07-05 23:26:00 - train: epoch 0032, iter [03300, 05004], lr: 0.095459, loss: 1.8718
2022-07-05 23:26:34 - train: epoch 0032, iter [03400, 05004], lr: 0.095452, loss: 1.7297
2022-07-05 23:27:09 - train: epoch 0032, iter [03500, 05004], lr: 0.095445, loss: 1.9209
2022-07-05 23:27:43 - train: epoch 0032, iter [03600, 05004], lr: 0.095438, loss: 2.0288
2022-07-05 23:28:18 - train: epoch 0032, iter [03700, 05004], lr: 0.095432, loss: 2.0714
2022-07-05 23:28:53 - train: epoch 0032, iter [03800, 05004], lr: 0.095425, loss: 1.8040
2022-07-05 23:29:28 - train: epoch 0032, iter [03900, 05004], lr: 0.095418, loss: 2.0411
2022-07-05 23:30:02 - train: epoch 0032, iter [04000, 05004], lr: 0.095412, loss: 1.8294
2022-07-05 23:30:37 - train: epoch 0032, iter [04100, 05004], lr: 0.095405, loss: 1.8881
2022-07-05 23:31:11 - train: epoch 0032, iter [04200, 05004], lr: 0.095398, loss: 1.8480
2022-07-05 23:31:46 - train: epoch 0032, iter [04300, 05004], lr: 0.095391, loss: 2.2085
2022-07-05 23:32:21 - train: epoch 0032, iter [04400, 05004], lr: 0.095385, loss: 2.0061
2022-07-05 23:32:56 - train: epoch 0032, iter [04500, 05004], lr: 0.095378, loss: 1.9929
2022-07-05 23:33:31 - train: epoch 0032, iter [04600, 05004], lr: 0.095371, loss: 1.9608
2022-07-05 23:34:05 - train: epoch 0032, iter [04700, 05004], lr: 0.095364, loss: 2.0189
2022-07-05 23:34:40 - train: epoch 0032, iter [04800, 05004], lr: 0.095358, loss: 1.8046
2022-07-05 23:35:13 - train: epoch 0032, iter [04900, 05004], lr: 0.095351, loss: 2.2092
2022-07-05 23:35:47 - train: epoch 0032, iter [05000, 05004], lr: 0.095344, loss: 2.0071
2022-07-05 23:35:48 - train: epoch 032, train_loss: 1.9618
2022-07-05 23:37:05 - eval: epoch: 032, acc1: 55.894%, acc5: 80.820%, test_loss: 1.8657, per_image_load_time: 0.611ms, per_image_inference_time: 0.464ms
2022-07-05 23:37:05 - until epoch: 032, best_acc1: 58.566%
2022-07-05 23:37:05 - epoch 033 lr: 0.095344
2022-07-05 23:37:45 - train: epoch 0033, iter [00100, 05004], lr: 0.095337, loss: 1.8603
2022-07-05 23:38:19 - train: epoch 0033, iter [00200, 05004], lr: 0.095330, loss: 2.0367
2022-07-05 23:38:53 - train: epoch 0033, iter [00300, 05004], lr: 0.095323, loss: 1.8397
2022-07-05 23:39:27 - train: epoch 0033, iter [00400, 05004], lr: 0.095317, loss: 1.6947
2022-07-05 23:40:01 - train: epoch 0033, iter [00500, 05004], lr: 0.095310, loss: 2.0438
2022-07-05 23:40:36 - train: epoch 0033, iter [00600, 05004], lr: 0.095303, loss: 1.8773
2022-07-05 23:41:10 - train: epoch 0033, iter [00700, 05004], lr: 0.095296, loss: 1.9420
2022-07-05 23:41:44 - train: epoch 0033, iter [00800, 05004], lr: 0.095289, loss: 2.0592
2022-07-05 23:42:18 - train: epoch 0033, iter [00900, 05004], lr: 0.095282, loss: 2.0842
2022-07-05 23:42:54 - train: epoch 0033, iter [01000, 05004], lr: 0.095276, loss: 1.8766
2022-07-05 23:43:28 - train: epoch 0033, iter [01100, 05004], lr: 0.095269, loss: 1.9110
2022-07-05 23:44:02 - train: epoch 0033, iter [01200, 05004], lr: 0.095262, loss: 1.9354
2022-07-05 23:44:37 - train: epoch 0033, iter [01300, 05004], lr: 0.095255, loss: 1.9360
2022-07-05 23:45:11 - train: epoch 0033, iter [01400, 05004], lr: 0.095248, loss: 2.0625
2022-07-05 23:45:45 - train: epoch 0033, iter [01500, 05004], lr: 0.095241, loss: 2.2556
2022-07-05 23:46:20 - train: epoch 0033, iter [01600, 05004], lr: 0.095235, loss: 2.0845
2022-07-05 23:46:54 - train: epoch 0033, iter [01700, 05004], lr: 0.095228, loss: 1.8359
2022-07-05 23:47:28 - train: epoch 0033, iter [01800, 05004], lr: 0.095221, loss: 2.3021
2022-07-05 23:48:03 - train: epoch 0033, iter [01900, 05004], lr: 0.095214, loss: 1.9948
2022-07-05 23:48:37 - train: epoch 0033, iter [02000, 05004], lr: 0.095207, loss: 1.8964
2022-07-05 23:49:11 - train: epoch 0033, iter [02100, 05004], lr: 0.095200, loss: 1.9160
2022-07-05 23:49:46 - train: epoch 0033, iter [02200, 05004], lr: 0.095193, loss: 2.0547
2022-07-05 23:50:19 - train: epoch 0033, iter [02300, 05004], lr: 0.095186, loss: 1.8280
2022-07-05 23:50:55 - train: epoch 0033, iter [02400, 05004], lr: 0.095180, loss: 2.2683
2022-07-05 23:51:29 - train: epoch 0033, iter [02500, 05004], lr: 0.095173, loss: 1.8901
2022-07-05 23:52:03 - train: epoch 0033, iter [02600, 05004], lr: 0.095166, loss: 1.8802
2022-07-05 23:52:36 - train: epoch 0033, iter [02700, 05004], lr: 0.095159, loss: 2.2191
2022-07-05 23:53:11 - train: epoch 0033, iter [02800, 05004], lr: 0.095152, loss: 2.0926
2022-07-05 23:53:46 - train: epoch 0033, iter [02900, 05004], lr: 0.095145, loss: 2.1483
2022-07-05 23:54:21 - train: epoch 0033, iter [03000, 05004], lr: 0.095138, loss: 2.0127
2022-07-05 23:54:55 - train: epoch 0033, iter [03100, 05004], lr: 0.095131, loss: 1.9883
2022-07-05 23:55:29 - train: epoch 0033, iter [03200, 05004], lr: 0.095124, loss: 1.8856
2022-07-05 23:56:04 - train: epoch 0033, iter [03300, 05004], lr: 0.095117, loss: 2.0518
2022-07-05 23:56:38 - train: epoch 0033, iter [03400, 05004], lr: 0.095110, loss: 1.9249
2022-07-05 23:57:13 - train: epoch 0033, iter [03500, 05004], lr: 0.095103, loss: 2.0526
2022-07-05 23:57:47 - train: epoch 0033, iter [03600, 05004], lr: 0.095096, loss: 2.0952
2022-07-05 23:58:21 - train: epoch 0033, iter [03700, 05004], lr: 0.095090, loss: 2.0401
2022-07-05 23:58:55 - train: epoch 0033, iter [03800, 05004], lr: 0.095083, loss: 1.9134
2022-07-05 23:59:30 - train: epoch 0033, iter [03900, 05004], lr: 0.095076, loss: 2.0201
2022-07-06 00:00:04 - train: epoch 0033, iter [04000, 05004], lr: 0.095069, loss: 2.0630
2022-07-06 00:00:39 - train: epoch 0033, iter [04100, 05004], lr: 0.095062, loss: 2.0857
2022-07-06 00:01:14 - train: epoch 0033, iter [04200, 05004], lr: 0.095055, loss: 1.9414
2022-07-06 00:01:48 - train: epoch 0033, iter [04300, 05004], lr: 0.095048, loss: 2.1058
2022-07-06 00:02:23 - train: epoch 0033, iter [04400, 05004], lr: 0.095041, loss: 2.0447
2022-07-06 00:02:57 - train: epoch 0033, iter [04500, 05004], lr: 0.095034, loss: 2.2693
2022-07-06 00:03:31 - train: epoch 0033, iter [04600, 05004], lr: 0.095027, loss: 1.8574
2022-07-06 00:04:07 - train: epoch 0033, iter [04700, 05004], lr: 0.095020, loss: 1.8288
2022-07-06 00:04:42 - train: epoch 0033, iter [04800, 05004], lr: 0.095013, loss: 2.4117
2022-07-06 00:05:17 - train: epoch 0033, iter [04900, 05004], lr: 0.095006, loss: 1.8557
2022-07-06 00:05:51 - train: epoch 0033, iter [05000, 05004], lr: 0.094999, loss: 1.8531
2022-07-06 00:05:52 - train: epoch 033, train_loss: 1.9528
2022-07-06 00:07:08 - eval: epoch: 033, acc1: 57.070%, acc5: 81.576%, test_loss: 1.8156, per_image_load_time: 0.760ms, per_image_inference_time: 0.460ms
2022-07-06 00:07:08 - until epoch: 033, best_acc1: 58.566%
2022-07-06 00:07:08 - epoch 034 lr: 0.094998
2022-07-06 00:07:48 - train: epoch 0034, iter [00100, 05004], lr: 0.094991, loss: 1.8499
2022-07-06 00:08:22 - train: epoch 0034, iter [00200, 05004], lr: 0.094984, loss: 1.7885
2022-07-06 00:08:56 - train: epoch 0034, iter [00300, 05004], lr: 0.094977, loss: 1.9326
2022-07-06 00:09:30 - train: epoch 0034, iter [00400, 05004], lr: 0.094970, loss: 1.7323
2022-07-06 00:10:05 - train: epoch 0034, iter [00500, 05004], lr: 0.094963, loss: 1.8671
2022-07-06 00:10:38 - train: epoch 0034, iter [00600, 05004], lr: 0.094956, loss: 2.1648
2022-07-06 00:11:13 - train: epoch 0034, iter [00700, 05004], lr: 0.094949, loss: 1.9783
2022-07-06 00:11:47 - train: epoch 0034, iter [00800, 05004], lr: 0.094942, loss: 1.8590
2022-07-06 00:12:22 - train: epoch 0034, iter [00900, 05004], lr: 0.094935, loss: 1.8938
2022-07-06 00:12:56 - train: epoch 0034, iter [01000, 05004], lr: 0.094928, loss: 1.8183
2022-07-06 00:13:30 - train: epoch 0034, iter [01100, 05004], lr: 0.094921, loss: 1.9733
2022-07-06 00:14:06 - train: epoch 0034, iter [01200, 05004], lr: 0.094914, loss: 1.8821
2022-07-06 00:14:40 - train: epoch 0034, iter [01300, 05004], lr: 0.094907, loss: 1.8175
2022-07-06 00:15:13 - train: epoch 0034, iter [01400, 05004], lr: 0.094900, loss: 2.0572
2022-07-06 00:15:48 - train: epoch 0034, iter [01500, 05004], lr: 0.094893, loss: 1.8733
2022-07-06 00:16:22 - train: epoch 0034, iter [01600, 05004], lr: 0.094886, loss: 1.8022
2022-07-06 00:16:56 - train: epoch 0034, iter [01700, 05004], lr: 0.094878, loss: 1.8487
2022-07-06 00:17:31 - train: epoch 0034, iter [01800, 05004], lr: 0.094871, loss: 2.0878
2022-07-06 00:18:04 - train: epoch 0034, iter [01900, 05004], lr: 0.094864, loss: 2.0806
2022-07-06 00:18:39 - train: epoch 0034, iter [02000, 05004], lr: 0.094857, loss: 1.9356
2022-07-06 00:19:13 - train: epoch 0034, iter [02100, 05004], lr: 0.094850, loss: 2.1451
2022-07-06 00:19:47 - train: epoch 0034, iter [02200, 05004], lr: 0.094843, loss: 1.7692
2022-07-06 00:20:21 - train: epoch 0034, iter [02300, 05004], lr: 0.094836, loss: 1.9697
2022-07-06 00:20:55 - train: epoch 0034, iter [02400, 05004], lr: 0.094829, loss: 1.7983
2022-07-06 00:21:29 - train: epoch 0034, iter [02500, 05004], lr: 0.094821, loss: 1.9898
2022-07-06 00:22:02 - train: epoch 0034, iter [02600, 05004], lr: 0.094814, loss: 1.9453
2022-07-06 00:22:35 - train: epoch 0034, iter [02700, 05004], lr: 0.094807, loss: 1.9231
2022-07-06 00:23:10 - train: epoch 0034, iter [02800, 05004], lr: 0.094800, loss: 1.7891
2022-07-06 00:23:43 - train: epoch 0034, iter [02900, 05004], lr: 0.094793, loss: 1.5678
2022-07-06 00:24:17 - train: epoch 0034, iter [03000, 05004], lr: 0.094786, loss: 1.7652
2022-07-06 00:24:50 - train: epoch 0034, iter [03100, 05004], lr: 0.094779, loss: 1.7708
2022-07-06 00:25:24 - train: epoch 0034, iter [03200, 05004], lr: 0.094771, loss: 1.9950
2022-07-06 00:25:57 - train: epoch 0034, iter [03300, 05004], lr: 0.094764, loss: 1.8390
2022-07-06 00:26:31 - train: epoch 0034, iter [03400, 05004], lr: 0.094757, loss: 2.0976
2022-07-06 00:27:05 - train: epoch 0034, iter [03500, 05004], lr: 0.094750, loss: 1.9219
2022-07-06 00:27:38 - train: epoch 0034, iter [03600, 05004], lr: 0.094743, loss: 1.7892
2022-07-06 00:28:12 - train: epoch 0034, iter [03700, 05004], lr: 0.094736, loss: 1.7655
2022-07-06 00:28:45 - train: epoch 0034, iter [03800, 05004], lr: 0.094728, loss: 1.9495
2022-07-06 00:29:19 - train: epoch 0034, iter [03900, 05004], lr: 0.094721, loss: 1.9725
2022-07-06 00:29:53 - train: epoch 0034, iter [04000, 05004], lr: 0.094714, loss: 1.7793
2022-07-06 00:30:27 - train: epoch 0034, iter [04100, 05004], lr: 0.094707, loss: 2.0008
2022-07-06 00:31:00 - train: epoch 0034, iter [04200, 05004], lr: 0.094700, loss: 1.8636
2022-07-06 00:31:33 - train: epoch 0034, iter [04300, 05004], lr: 0.094692, loss: 2.0034
2022-07-06 00:32:07 - train: epoch 0034, iter [04400, 05004], lr: 0.094685, loss: 2.0232
2022-07-06 00:32:41 - train: epoch 0034, iter [04500, 05004], lr: 0.094678, loss: 2.0364
2022-07-06 00:33:14 - train: epoch 0034, iter [04600, 05004], lr: 0.094671, loss: 2.0269
2022-07-06 00:33:48 - train: epoch 0034, iter [04700, 05004], lr: 0.094663, loss: 1.8718
2022-07-06 00:34:21 - train: epoch 0034, iter [04800, 05004], lr: 0.094656, loss: 1.9094
2022-07-06 00:34:55 - train: epoch 0034, iter [04900, 05004], lr: 0.094649, loss: 1.8636
2022-07-06 00:35:28 - train: epoch 0034, iter [05000, 05004], lr: 0.094642, loss: 1.8644
2022-07-06 00:35:29 - train: epoch 034, train_loss: 1.9477
2022-07-06 00:36:43 - eval: epoch: 034, acc1: 53.546%, acc5: 78.766%, test_loss: 2.0066, per_image_load_time: 0.778ms, per_image_inference_time: 0.463ms
2022-07-06 00:36:43 - until epoch: 034, best_acc1: 58.566%
2022-07-06 00:36:43 - epoch 035 lr: 0.094641
2022-07-06 00:37:22 - train: epoch 0035, iter [00100, 05004], lr: 0.094634, loss: 1.7201
2022-07-06 00:37:55 - train: epoch 0035, iter [00200, 05004], lr: 0.094627, loss: 1.8388
2022-07-06 00:38:28 - train: epoch 0035, iter [00300, 05004], lr: 0.094620, loss: 2.1594
2022-07-06 00:39:02 - train: epoch 0035, iter [00400, 05004], lr: 0.094612, loss: 1.8309
2022-07-06 00:39:35 - train: epoch 0035, iter [00500, 05004], lr: 0.094605, loss: 1.9051
2022-07-06 00:40:07 - train: epoch 0035, iter [00600, 05004], lr: 0.094598, loss: 2.0487
2022-07-06 00:40:41 - train: epoch 0035, iter [00700, 05004], lr: 0.094591, loss: 1.9162
2022-07-06 00:41:14 - train: epoch 0035, iter [00800, 05004], lr: 0.094583, loss: 1.8970
2022-07-06 00:41:46 - train: epoch 0035, iter [00900, 05004], lr: 0.094576, loss: 2.1524
2022-07-06 00:42:19 - train: epoch 0035, iter [01000, 05004], lr: 0.094569, loss: 2.0366
2022-07-06 00:42:53 - train: epoch 0035, iter [01100, 05004], lr: 0.094561, loss: 2.2581
2022-07-06 00:43:26 - train: epoch 0035, iter [01200, 05004], lr: 0.094554, loss: 1.9224
2022-07-06 00:43:59 - train: epoch 0035, iter [01300, 05004], lr: 0.094547, loss: 2.0359
2022-07-06 00:44:32 - train: epoch 0035, iter [01400, 05004], lr: 0.094539, loss: 2.0070
2022-07-06 00:45:05 - train: epoch 0035, iter [01500, 05004], lr: 0.094532, loss: 1.9784
2022-07-06 00:45:39 - train: epoch 0035, iter [01600, 05004], lr: 0.094525, loss: 1.9079
2022-07-06 00:46:12 - train: epoch 0035, iter [01700, 05004], lr: 0.094517, loss: 1.7654
2022-07-06 00:46:45 - train: epoch 0035, iter [01800, 05004], lr: 0.094510, loss: 1.9528
2022-07-06 00:47:18 - train: epoch 0035, iter [01900, 05004], lr: 0.094503, loss: 2.0018
2022-07-06 00:47:51 - train: epoch 0035, iter [02000, 05004], lr: 0.094495, loss: 2.0232
2022-07-06 00:48:24 - train: epoch 0035, iter [02100, 05004], lr: 0.094488, loss: 1.8812
2022-07-06 00:48:56 - train: epoch 0035, iter [02200, 05004], lr: 0.094481, loss: 2.1208
2022-07-06 00:49:30 - train: epoch 0035, iter [02300, 05004], lr: 0.094473, loss: 1.9710
2022-07-06 00:50:02 - train: epoch 0035, iter [02400, 05004], lr: 0.094466, loss: 2.0610
2022-07-06 00:50:36 - train: epoch 0035, iter [02500, 05004], lr: 0.094459, loss: 1.9025
2022-07-06 00:51:09 - train: epoch 0035, iter [02600, 05004], lr: 0.094451, loss: 2.1207
2022-07-06 00:51:42 - train: epoch 0035, iter [02700, 05004], lr: 0.094444, loss: 2.0643
2022-07-06 00:52:15 - train: epoch 0035, iter [02800, 05004], lr: 0.094437, loss: 1.8982
2022-07-06 00:52:48 - train: epoch 0035, iter [02900, 05004], lr: 0.094429, loss: 1.9148
2022-07-06 00:53:22 - train: epoch 0035, iter [03000, 05004], lr: 0.094422, loss: 2.0300
2022-07-06 00:53:55 - train: epoch 0035, iter [03100, 05004], lr: 0.094414, loss: 1.9652
2022-07-06 00:54:29 - train: epoch 0035, iter [03200, 05004], lr: 0.094407, loss: 1.8267
2022-07-06 00:55:02 - train: epoch 0035, iter [03300, 05004], lr: 0.094400, loss: 1.9172
2022-07-06 00:55:34 - train: epoch 0035, iter [03400, 05004], lr: 0.094392, loss: 1.9281
2022-07-06 00:56:08 - train: epoch 0035, iter [03500, 05004], lr: 0.094385, loss: 1.7238
2022-07-06 00:56:41 - train: epoch 0035, iter [03600, 05004], lr: 0.094377, loss: 1.9865
2022-07-06 00:57:15 - train: epoch 0035, iter [03700, 05004], lr: 0.094370, loss: 1.8204
2022-07-06 00:57:48 - train: epoch 0035, iter [03800, 05004], lr: 0.094363, loss: 1.9576
2022-07-06 00:58:21 - train: epoch 0035, iter [03900, 05004], lr: 0.094355, loss: 2.2521
2022-07-06 00:58:55 - train: epoch 0035, iter [04000, 05004], lr: 0.094348, loss: 1.7217
2022-07-06 00:59:27 - train: epoch 0035, iter [04100, 05004], lr: 0.094340, loss: 2.2165
2022-07-06 01:00:01 - train: epoch 0035, iter [04200, 05004], lr: 0.094333, loss: 1.9012
2022-07-06 01:00:34 - train: epoch 0035, iter [04300, 05004], lr: 0.094325, loss: 1.9202
2022-07-06 01:01:08 - train: epoch 0035, iter [04400, 05004], lr: 0.094318, loss: 2.0086
2022-07-06 01:01:41 - train: epoch 0035, iter [04500, 05004], lr: 0.094310, loss: 1.9949
2022-07-06 01:02:14 - train: epoch 0035, iter [04600, 05004], lr: 0.094303, loss: 1.8880
2022-07-06 01:02:47 - train: epoch 0035, iter [04700, 05004], lr: 0.094296, loss: 2.1874
2022-07-06 01:03:20 - train: epoch 0035, iter [04800, 05004], lr: 0.094288, loss: 2.1286
2022-07-06 01:03:53 - train: epoch 0035, iter [04900, 05004], lr: 0.094281, loss: 2.0544
2022-07-06 01:04:26 - train: epoch 0035, iter [05000, 05004], lr: 0.094273, loss: 1.8108
2022-07-06 01:04:27 - train: epoch 035, train_loss: 1.9423
2022-07-06 01:05:41 - eval: epoch: 035, acc1: 53.912%, acc5: 78.726%, test_loss: 1.9912, per_image_load_time: 0.735ms, per_image_inference_time: 0.476ms
2022-07-06 01:05:41 - until epoch: 035, best_acc1: 58.566%
2022-07-06 01:05:41 - epoch 036 lr: 0.094273
2022-07-06 01:06:21 - train: epoch 0036, iter [00100, 05004], lr: 0.094265, loss: 2.0289
2022-07-06 01:06:55 - train: epoch 0036, iter [00200, 05004], lr: 0.094258, loss: 1.6884
2022-07-06 01:07:27 - train: epoch 0036, iter [00300, 05004], lr: 0.094250, loss: 1.8088
2022-07-06 01:08:01 - train: epoch 0036, iter [00400, 05004], lr: 0.094243, loss: 1.9371
2022-07-06 01:08:35 - train: epoch 0036, iter [00500, 05004], lr: 0.094235, loss: 1.8912
2022-07-06 01:09:09 - train: epoch 0036, iter [00600, 05004], lr: 0.094228, loss: 1.7758
2022-07-06 01:09:42 - train: epoch 0036, iter [00700, 05004], lr: 0.094220, loss: 1.7368
2022-07-06 01:10:17 - train: epoch 0036, iter [00800, 05004], lr: 0.094213, loss: 1.7997
2022-07-06 01:10:51 - train: epoch 0036, iter [00900, 05004], lr: 0.094205, loss: 1.9375
2022-07-06 01:11:24 - train: epoch 0036, iter [01000, 05004], lr: 0.094198, loss: 1.8784
2022-07-06 01:11:56 - train: epoch 0036, iter [01100, 05004], lr: 0.094190, loss: 1.9140
2022-07-06 01:12:29 - train: epoch 0036, iter [01200, 05004], lr: 0.094183, loss: 2.0004
2022-07-06 01:13:03 - train: epoch 0036, iter [01300, 05004], lr: 0.094175, loss: 1.9653
2022-07-06 01:13:37 - train: epoch 0036, iter [01400, 05004], lr: 0.094168, loss: 2.0602
2022-07-06 01:14:09 - train: epoch 0036, iter [01500, 05004], lr: 0.094160, loss: 2.1047
2022-07-06 01:14:43 - train: epoch 0036, iter [01600, 05004], lr: 0.094153, loss: 1.9801
2022-07-06 01:15:16 - train: epoch 0036, iter [01700, 05004], lr: 0.094145, loss: 1.8880
2022-07-06 01:15:50 - train: epoch 0036, iter [01800, 05004], lr: 0.094137, loss: 1.8516
2022-07-06 01:16:23 - train: epoch 0036, iter [01900, 05004], lr: 0.094130, loss: 1.9518
2022-07-06 01:16:55 - train: epoch 0036, iter [02000, 05004], lr: 0.094122, loss: 1.8392
2022-07-06 01:17:29 - train: epoch 0036, iter [02100, 05004], lr: 0.094115, loss: 1.9020
2022-07-06 01:18:03 - train: epoch 0036, iter [02200, 05004], lr: 0.094107, loss: 1.8560
2022-07-06 01:18:36 - train: epoch 0036, iter [02300, 05004], lr: 0.094100, loss: 2.0577
2022-07-06 01:19:09 - train: epoch 0036, iter [02400, 05004], lr: 0.094092, loss: 2.0174
2022-07-06 01:19:43 - train: epoch 0036, iter [02500, 05004], lr: 0.094084, loss: 1.7577
2022-07-06 01:20:17 - train: epoch 0036, iter [02600, 05004], lr: 0.094077, loss: 2.0527
2022-07-06 01:20:50 - train: epoch 0036, iter [02700, 05004], lr: 0.094069, loss: 1.8011
2022-07-06 01:21:24 - train: epoch 0036, iter [02800, 05004], lr: 0.094062, loss: 1.7107
2022-07-06 01:21:58 - train: epoch 0036, iter [02900, 05004], lr: 0.094054, loss: 1.8314
2022-07-06 01:22:31 - train: epoch 0036, iter [03000, 05004], lr: 0.094046, loss: 2.1437
2022-07-06 01:23:05 - train: epoch 0036, iter [03100, 05004], lr: 0.094039, loss: 1.9855
2022-07-06 01:23:38 - train: epoch 0036, iter [03200, 05004], lr: 0.094031, loss: 1.9735
2022-07-06 01:24:12 - train: epoch 0036, iter [03300, 05004], lr: 0.094023, loss: 1.9017
2022-07-06 01:24:45 - train: epoch 0036, iter [03400, 05004], lr: 0.094016, loss: 1.8610
2022-07-06 01:25:19 - train: epoch 0036, iter [03500, 05004], lr: 0.094008, loss: 2.0190
2022-07-06 01:25:52 - train: epoch 0036, iter [03600, 05004], lr: 0.094001, loss: 2.1130
2022-07-06 01:26:27 - train: epoch 0036, iter [03700, 05004], lr: 0.093993, loss: 1.9901
2022-07-06 01:27:00 - train: epoch 0036, iter [03800, 05004], lr: 0.093985, loss: 1.7837
2022-07-06 01:27:33 - train: epoch 0036, iter [03900, 05004], lr: 0.093978, loss: 1.9187
2022-07-06 01:28:07 - train: epoch 0036, iter [04000, 05004], lr: 0.093970, loss: 1.9017
2022-07-06 01:28:39 - train: epoch 0036, iter [04100, 05004], lr: 0.093962, loss: 1.9079
2022-07-06 01:29:13 - train: epoch 0036, iter [04200, 05004], lr: 0.093955, loss: 1.9725
2022-07-06 01:29:46 - train: epoch 0036, iter [04300, 05004], lr: 0.093947, loss: 1.8975
2022-07-06 01:30:19 - train: epoch 0036, iter [04400, 05004], lr: 0.093939, loss: 1.9430
2022-07-06 01:30:52 - train: epoch 0036, iter [04500, 05004], lr: 0.093932, loss: 1.6512
2022-07-06 01:31:26 - train: epoch 0036, iter [04600, 05004], lr: 0.093924, loss: 1.8832
2022-07-06 01:31:59 - train: epoch 0036, iter [04700, 05004], lr: 0.093916, loss: 1.8580
2022-07-06 01:32:33 - train: epoch 0036, iter [04800, 05004], lr: 0.093908, loss: 1.9787
2022-07-06 01:33:06 - train: epoch 0036, iter [04900, 05004], lr: 0.093901, loss: 1.9584
2022-07-06 01:33:38 - train: epoch 0036, iter [05000, 05004], lr: 0.093893, loss: 1.9581
2022-07-06 01:33:40 - train: epoch 036, train_loss: 1.9330
2022-07-06 01:34:54 - eval: epoch: 036, acc1: 57.424%, acc5: 81.918%, test_loss: 1.7834, per_image_load_time: 2.386ms, per_image_inference_time: 0.470ms
2022-07-06 01:34:54 - until epoch: 036, best_acc1: 58.566%
2022-07-06 01:34:54 - epoch 037 lr: 0.093893
2022-07-06 01:35:32 - train: epoch 0037, iter [00100, 05004], lr: 0.093885, loss: 1.7271
2022-07-06 01:36:06 - train: epoch 0037, iter [00200, 05004], lr: 0.093877, loss: 1.5620
2022-07-06 01:36:40 - train: epoch 0037, iter [00300, 05004], lr: 0.093870, loss: 1.9078
2022-07-06 01:37:13 - train: epoch 0037, iter [00400, 05004], lr: 0.093862, loss: 1.9486
2022-07-06 01:37:46 - train: epoch 0037, iter [00500, 05004], lr: 0.093854, loss: 1.9756
2022-07-06 01:38:19 - train: epoch 0037, iter [00600, 05004], lr: 0.093846, loss: 2.0944
2022-07-06 01:38:52 - train: epoch 0037, iter [00700, 05004], lr: 0.093839, loss: 1.9422
2022-07-06 01:39:25 - train: epoch 0037, iter [00800, 05004], lr: 0.093831, loss: 1.9795
2022-07-06 01:39:58 - train: epoch 0037, iter [00900, 05004], lr: 0.093823, loss: 2.2047
2022-07-06 01:40:32 - train: epoch 0037, iter [01000, 05004], lr: 0.093815, loss: 1.8826
2022-07-06 01:41:05 - train: epoch 0037, iter [01100, 05004], lr: 0.093808, loss: 2.0216
2022-07-06 01:41:38 - train: epoch 0037, iter [01200, 05004], lr: 0.093800, loss: 1.9162
2022-07-06 01:42:11 - train: epoch 0037, iter [01300, 05004], lr: 0.093792, loss: 1.9615
2022-07-06 01:42:44 - train: epoch 0037, iter [01400, 05004], lr: 0.093784, loss: 1.9572
2022-07-06 01:43:17 - train: epoch 0037, iter [01500, 05004], lr: 0.093777, loss: 1.7713
2022-07-06 01:43:51 - train: epoch 0037, iter [01600, 05004], lr: 0.093769, loss: 1.7050
2022-07-06 01:44:25 - train: epoch 0037, iter [01700, 05004], lr: 0.093761, loss: 2.0587
2022-07-06 01:44:58 - train: epoch 0037, iter [01800, 05004], lr: 0.093753, loss: 1.9347
2022-07-06 01:45:32 - train: epoch 0037, iter [01900, 05004], lr: 0.093745, loss: 2.0060
2022-07-06 01:46:05 - train: epoch 0037, iter [02000, 05004], lr: 0.093738, loss: 1.9613
2022-07-06 01:46:38 - train: epoch 0037, iter [02100, 05004], lr: 0.093730, loss: 1.8850
2022-07-06 01:47:12 - train: epoch 0037, iter [02200, 05004], lr: 0.093722, loss: 1.8197
2022-07-06 01:47:45 - train: epoch 0037, iter [02300, 05004], lr: 0.093714, loss: 1.7798
2022-07-06 01:48:18 - train: epoch 0037, iter [02400, 05004], lr: 0.093706, loss: 1.8980
2022-07-06 01:48:52 - train: epoch 0037, iter [02500, 05004], lr: 0.093699, loss: 1.8685
2022-07-06 01:49:25 - train: epoch 0037, iter [02600, 05004], lr: 0.093691, loss: 2.1354
2022-07-06 01:49:58 - train: epoch 0037, iter [02700, 05004], lr: 0.093683, loss: 2.0775
2022-07-06 01:50:32 - train: epoch 0037, iter [02800, 05004], lr: 0.093675, loss: 1.9943
2022-07-06 01:51:05 - train: epoch 0037, iter [02900, 05004], lr: 0.093667, loss: 2.0286
2022-07-06 01:51:38 - train: epoch 0037, iter [03000, 05004], lr: 0.093659, loss: 2.1026
2022-07-06 01:52:12 - train: epoch 0037, iter [03100, 05004], lr: 0.093652, loss: 1.8422
2022-07-06 01:52:45 - train: epoch 0037, iter [03200, 05004], lr: 0.093644, loss: 2.0157
2022-07-06 01:53:19 - train: epoch 0037, iter [03300, 05004], lr: 0.093636, loss: 1.8131
2022-07-06 01:53:52 - train: epoch 0037, iter [03400, 05004], lr: 0.093628, loss: 1.8106
2022-07-06 01:54:25 - train: epoch 0037, iter [03500, 05004], lr: 0.093620, loss: 1.9397
2022-07-06 01:54:58 - train: epoch 0037, iter [03600, 05004], lr: 0.093612, loss: 2.0746
2022-07-06 01:55:31 - train: epoch 0037, iter [03700, 05004], lr: 0.093604, loss: 2.0188
2022-07-06 01:56:05 - train: epoch 0037, iter [03800, 05004], lr: 0.093596, loss: 1.8010
2022-07-06 01:56:37 - train: epoch 0037, iter [03900, 05004], lr: 0.093589, loss: 2.0667
2022-07-06 01:57:11 - train: epoch 0037, iter [04000, 05004], lr: 0.093581, loss: 1.9001
2022-07-06 01:57:44 - train: epoch 0037, iter [04100, 05004], lr: 0.093573, loss: 1.8891
2022-07-06 01:58:18 - train: epoch 0037, iter [04200, 05004], lr: 0.093565, loss: 2.1501
2022-07-06 01:58:51 - train: epoch 0037, iter [04300, 05004], lr: 0.093557, loss: 2.0353
2022-07-06 01:59:25 - train: epoch 0037, iter [04400, 05004], lr: 0.093549, loss: 1.9403
2022-07-06 01:59:58 - train: epoch 0037, iter [04500, 05004], lr: 0.093541, loss: 1.8824
2022-07-06 02:00:32 - train: epoch 0037, iter [04600, 05004], lr: 0.093533, loss: 1.9421
2022-07-06 02:01:06 - train: epoch 0037, iter [04700, 05004], lr: 0.093525, loss: 1.9373
2022-07-06 02:01:39 - train: epoch 0037, iter [04800, 05004], lr: 0.093517, loss: 1.8799
2022-07-06 02:02:13 - train: epoch 0037, iter [04900, 05004], lr: 0.093509, loss: 2.0255
2022-07-06 02:02:45 - train: epoch 0037, iter [05000, 05004], lr: 0.093502, loss: 1.9315
2022-07-06 02:02:46 - train: epoch 037, train_loss: 1.9309
2022-07-06 02:04:01 - eval: epoch: 037, acc1: 56.796%, acc5: 81.042%, test_loss: 1.8379, per_image_load_time: 0.672ms, per_image_inference_time: 0.463ms
2022-07-06 02:04:01 - until epoch: 037, best_acc1: 58.566%
2022-07-06 02:04:01 - epoch 038 lr: 0.093501
2022-07-06 02:04:39 - train: epoch 0038, iter [00100, 05004], lr: 0.093493, loss: 1.8761
2022-07-06 02:05:12 - train: epoch 0038, iter [00200, 05004], lr: 0.093485, loss: 1.7101
2022-07-06 02:05:44 - train: epoch 0038, iter [00300, 05004], lr: 0.093477, loss: 1.6297
2022-07-06 02:06:18 - train: epoch 0038, iter [00400, 05004], lr: 0.093469, loss: 1.8558
2022-07-06 02:06:51 - train: epoch 0038, iter [00500, 05004], lr: 0.093462, loss: 1.8138
2022-07-06 02:07:25 - train: epoch 0038, iter [00600, 05004], lr: 0.093454, loss: 1.8728
2022-07-06 02:07:58 - train: epoch 0038, iter [00700, 05004], lr: 0.093446, loss: 1.7669
2022-07-06 02:08:31 - train: epoch 0038, iter [00800, 05004], lr: 0.093438, loss: 1.7237
2022-07-06 02:09:04 - train: epoch 0038, iter [00900, 05004], lr: 0.093430, loss: 1.7830
2022-07-06 02:09:37 - train: epoch 0038, iter [01000, 05004], lr: 0.093422, loss: 2.1873
2022-07-06 02:10:10 - train: epoch 0038, iter [01100, 05004], lr: 0.093414, loss: 1.8631
2022-07-06 02:10:44 - train: epoch 0038, iter [01200, 05004], lr: 0.093406, loss: 1.9390
2022-07-06 02:11:17 - train: epoch 0038, iter [01300, 05004], lr: 0.093398, loss: 2.1285
2022-07-06 02:11:51 - train: epoch 0038, iter [01400, 05004], lr: 0.093390, loss: 1.9574
2022-07-06 02:12:23 - train: epoch 0038, iter [01500, 05004], lr: 0.093382, loss: 2.1229
2022-07-06 02:12:57 - train: epoch 0038, iter [01600, 05004], lr: 0.093374, loss: 2.0008
2022-07-06 02:13:30 - train: epoch 0038, iter [01700, 05004], lr: 0.093366, loss: 2.1437
2022-07-06 02:14:03 - train: epoch 0038, iter [01800, 05004], lr: 0.093358, loss: 1.9836
2022-07-06 02:14:36 - train: epoch 0038, iter [01900, 05004], lr: 0.093350, loss: 1.9138
2022-07-06 02:15:10 - train: epoch 0038, iter [02000, 05004], lr: 0.093342, loss: 1.9207
2022-07-06 02:15:44 - train: epoch 0038, iter [02100, 05004], lr: 0.093334, loss: 1.9883
2022-07-06 02:16:17 - train: epoch 0038, iter [02200, 05004], lr: 0.093326, loss: 1.9163
2022-07-06 02:16:51 - train: epoch 0038, iter [02300, 05004], lr: 0.093318, loss: 2.0423
2022-07-06 02:17:25 - train: epoch 0038, iter [02400, 05004], lr: 0.093309, loss: 2.2228
2022-07-06 02:17:58 - train: epoch 0038, iter [02500, 05004], lr: 0.093301, loss: 1.9233
2022-07-06 02:18:32 - train: epoch 0038, iter [02600, 05004], lr: 0.093293, loss: 1.9447
2022-07-06 02:19:05 - train: epoch 0038, iter [02700, 05004], lr: 0.093285, loss: 2.0352
2022-07-06 02:19:38 - train: epoch 0038, iter [02800, 05004], lr: 0.093277, loss: 2.1010
2022-07-06 02:20:12 - train: epoch 0038, iter [02900, 05004], lr: 0.093269, loss: 2.0710
2022-07-06 02:20:45 - train: epoch 0038, iter [03000, 05004], lr: 0.093261, loss: 1.8922
2022-07-06 02:21:18 - train: epoch 0038, iter [03100, 05004], lr: 0.093253, loss: 1.8741
2022-07-06 02:21:51 - train: epoch 0038, iter [03200, 05004], lr: 0.093245, loss: 1.6308
2022-07-06 02:22:24 - train: epoch 0038, iter [03300, 05004], lr: 0.093237, loss: 1.8045
2022-07-06 02:22:58 - train: epoch 0038, iter [03400, 05004], lr: 0.093229, loss: 1.7776
2022-07-06 02:23:31 - train: epoch 0038, iter [03500, 05004], lr: 0.093221, loss: 1.9698
2022-07-06 02:24:04 - train: epoch 0038, iter [03600, 05004], lr: 0.093213, loss: 1.9280
2022-07-06 02:24:38 - train: epoch 0038, iter [03700, 05004], lr: 0.093205, loss: 1.7363
2022-07-06 02:25:11 - train: epoch 0038, iter [03800, 05004], lr: 0.093196, loss: 1.9892
2022-07-06 02:25:44 - train: epoch 0038, iter [03900, 05004], lr: 0.093188, loss: 1.9531
2022-07-06 02:26:18 - train: epoch 0038, iter [04000, 05004], lr: 0.093180, loss: 1.9444
2022-07-06 02:26:50 - train: epoch 0038, iter [04100, 05004], lr: 0.093172, loss: 1.9234
2022-07-06 02:27:23 - train: epoch 0038, iter [04200, 05004], lr: 0.093164, loss: 1.6781
2022-07-06 02:27:56 - train: epoch 0038, iter [04300, 05004], lr: 0.093156, loss: 2.1424
2022-07-06 02:28:30 - train: epoch 0038, iter [04400, 05004], lr: 0.093148, loss: 1.9476
2022-07-06 02:29:03 - train: epoch 0038, iter [04500, 05004], lr: 0.093140, loss: 1.9284
2022-07-06 02:29:37 - train: epoch 0038, iter [04600, 05004], lr: 0.093131, loss: 2.0910
2022-07-06 02:30:09 - train: epoch 0038, iter [04700, 05004], lr: 0.093123, loss: 1.6569
2022-07-06 02:30:42 - train: epoch 0038, iter [04800, 05004], lr: 0.093115, loss: 1.8991
2022-07-06 02:31:14 - train: epoch 0038, iter [04900, 05004], lr: 0.093107, loss: 1.9054
2022-07-06 02:31:46 - train: epoch 0038, iter [05000, 05004], lr: 0.093099, loss: 1.6830
2022-07-06 02:31:47 - train: epoch 038, train_loss: 1.9239
2022-07-06 02:33:01 - eval: epoch: 038, acc1: 57.802%, acc5: 81.892%, test_loss: 1.7789, per_image_load_time: 1.584ms, per_image_inference_time: 0.473ms
2022-07-06 02:33:02 - until epoch: 038, best_acc1: 58.566%
2022-07-06 02:33:02 - epoch 039 lr: 0.093098
2022-07-06 02:33:40 - train: epoch 0039, iter [00100, 05004], lr: 0.093090, loss: 1.9124
2022-07-06 02:34:14 - train: epoch 0039, iter [00200, 05004], lr: 0.093082, loss: 2.0506
2022-07-06 02:34:47 - train: epoch 0039, iter [00300, 05004], lr: 0.093074, loss: 1.7416
2022-07-06 02:35:20 - train: epoch 0039, iter [00400, 05004], lr: 0.093066, loss: 1.9852
2022-07-06 02:35:53 - train: epoch 0039, iter [00500, 05004], lr: 0.093058, loss: 1.7878
2022-07-06 02:36:26 - train: epoch 0039, iter [00600, 05004], lr: 0.093049, loss: 1.7521
2022-07-06 02:36:58 - train: epoch 0039, iter [00700, 05004], lr: 0.093041, loss: 2.1117
2022-07-06 02:37:31 - train: epoch 0039, iter [00800, 05004], lr: 0.093033, loss: 1.8984
2022-07-06 02:38:05 - train: epoch 0039, iter [00900, 05004], lr: 0.093025, loss: 2.0403
2022-07-06 02:38:38 - train: epoch 0039, iter [01000, 05004], lr: 0.093017, loss: 1.8112
2022-07-06 02:39:11 - train: epoch 0039, iter [01100, 05004], lr: 0.093008, loss: 1.8784
2022-07-06 02:39:44 - train: epoch 0039, iter [01200, 05004], lr: 0.093000, loss: 2.0910
2022-07-06 02:40:18 - train: epoch 0039, iter [01300, 05004], lr: 0.092992, loss: 2.1662
2022-07-06 02:40:51 - train: epoch 0039, iter [01400, 05004], lr: 0.092984, loss: 2.0717
2022-07-06 02:41:24 - train: epoch 0039, iter [01500, 05004], lr: 0.092976, loss: 1.8523
2022-07-06 02:41:57 - train: epoch 0039, iter [01600, 05004], lr: 0.092967, loss: 1.9723
2022-07-06 02:42:31 - train: epoch 0039, iter [01700, 05004], lr: 0.092959, loss: 1.6003
2022-07-06 02:43:04 - train: epoch 0039, iter [01800, 05004], lr: 0.092951, loss: 1.7392
2022-07-06 02:43:36 - train: epoch 0039, iter [01900, 05004], lr: 0.092943, loss: 1.6463
2022-07-06 02:44:10 - train: epoch 0039, iter [02000, 05004], lr: 0.092934, loss: 1.8726
2022-07-06 02:44:43 - train: epoch 0039, iter [02100, 05004], lr: 0.092926, loss: 2.0618
2022-07-06 02:45:16 - train: epoch 0039, iter [02200, 05004], lr: 0.092918, loss: 1.8010
2022-07-06 02:45:50 - train: epoch 0039, iter [02300, 05004], lr: 0.092910, loss: 2.1327
2022-07-06 02:46:23 - train: epoch 0039, iter [02400, 05004], lr: 0.092901, loss: 2.0785
2022-07-06 02:46:56 - train: epoch 0039, iter [02500, 05004], lr: 0.092893, loss: 1.8661
2022-07-06 02:47:30 - train: epoch 0039, iter [02600, 05004], lr: 0.092885, loss: 1.8820
2022-07-06 02:48:04 - train: epoch 0039, iter [02700, 05004], lr: 0.092877, loss: 2.1000
2022-07-06 02:48:38 - train: epoch 0039, iter [02800, 05004], lr: 0.092868, loss: 1.7769
2022-07-06 02:49:11 - train: epoch 0039, iter [02900, 05004], lr: 0.092860, loss: 1.7087
2022-07-06 02:49:45 - train: epoch 0039, iter [03000, 05004], lr: 0.092852, loss: 1.9855
2022-07-06 02:50:18 - train: epoch 0039, iter [03100, 05004], lr: 0.092843, loss: 1.8321
2022-07-06 02:50:52 - train: epoch 0039, iter [03200, 05004], lr: 0.092835, loss: 1.9584
2022-07-06 02:51:24 - train: epoch 0039, iter [03300, 05004], lr: 0.092827, loss: 1.8963
2022-07-06 02:51:58 - train: epoch 0039, iter [03400, 05004], lr: 0.092818, loss: 2.0772
2022-07-06 02:52:32 - train: epoch 0039, iter [03500, 05004], lr: 0.092810, loss: 2.1301
2022-07-06 02:53:05 - train: epoch 0039, iter [03600, 05004], lr: 0.092802, loss: 2.1772
2022-07-06 02:53:39 - train: epoch 0039, iter [03700, 05004], lr: 0.092793, loss: 1.8514
2022-07-06 02:54:12 - train: epoch 0039, iter [03800, 05004], lr: 0.092785, loss: 1.7684
2022-07-06 02:54:46 - train: epoch 0039, iter [03900, 05004], lr: 0.092777, loss: 1.9743
2022-07-06 02:55:20 - train: epoch 0039, iter [04000, 05004], lr: 0.092768, loss: 1.8591
2022-07-06 02:55:53 - train: epoch 0039, iter [04100, 05004], lr: 0.092760, loss: 1.9814
2022-07-06 02:56:27 - train: epoch 0039, iter [04200, 05004], lr: 0.092752, loss: 1.9449
2022-07-06 02:57:01 - train: epoch 0039, iter [04300, 05004], lr: 0.092743, loss: 1.9671
2022-07-06 02:57:34 - train: epoch 0039, iter [04400, 05004], lr: 0.092735, loss: 1.6510
2022-07-06 02:58:08 - train: epoch 0039, iter [04500, 05004], lr: 0.092727, loss: 1.7571
2022-07-06 02:58:42 - train: epoch 0039, iter [04600, 05004], lr: 0.092718, loss: 2.0655
2022-07-06 02:59:15 - train: epoch 0039, iter [04700, 05004], lr: 0.092710, loss: 1.9565
2022-07-06 02:59:49 - train: epoch 0039, iter [04800, 05004], lr: 0.092702, loss: 1.9793
2022-07-06 03:00:23 - train: epoch 0039, iter [04900, 05004], lr: 0.092693, loss: 1.7756
2022-07-06 03:00:55 - train: epoch 0039, iter [05000, 05004], lr: 0.092685, loss: 1.7366
2022-07-06 03:00:56 - train: epoch 039, train_loss: 1.9193
2022-07-06 03:02:11 - eval: epoch: 039, acc1: 55.586%, acc5: 80.792%, test_loss: 1.8626, per_image_load_time: 0.718ms, per_image_inference_time: 0.489ms
2022-07-06 03:02:11 - until epoch: 039, best_acc1: 58.566%
2022-07-06 03:02:11 - epoch 040 lr: 0.092684
2022-07-06 03:02:49 - train: epoch 0040, iter [00100, 05004], lr: 0.092676, loss: 2.1379
2022-07-06 03:03:23 - train: epoch 0040, iter [00200, 05004], lr: 0.092668, loss: 2.2026
2022-07-06 03:03:57 - train: epoch 0040, iter [00300, 05004], lr: 0.092659, loss: 2.0016
2022-07-06 03:04:30 - train: epoch 0040, iter [00400, 05004], lr: 0.092651, loss: 1.8748
2022-07-06 03:05:04 - train: epoch 0040, iter [00500, 05004], lr: 0.092643, loss: 1.7815
2022-07-06 03:05:37 - train: epoch 0040, iter [00600, 05004], lr: 0.092634, loss: 2.0261
2022-07-06 03:06:11 - train: epoch 0040, iter [00700, 05004], lr: 0.092626, loss: 1.8057
2022-07-06 03:06:45 - train: epoch 0040, iter [00800, 05004], lr: 0.092617, loss: 2.0354
2022-07-06 03:07:18 - train: epoch 0040, iter [00900, 05004], lr: 0.092609, loss: 1.8144
2022-07-06 03:07:51 - train: epoch 0040, iter [01000, 05004], lr: 0.092600, loss: 1.6686
2022-07-06 03:08:25 - train: epoch 0040, iter [01100, 05004], lr: 0.092592, loss: 1.8314
2022-07-06 03:08:59 - train: epoch 0040, iter [01200, 05004], lr: 0.092584, loss: 1.7650
2022-07-06 03:09:32 - train: epoch 0040, iter [01300, 05004], lr: 0.092575, loss: 1.7777
2022-07-06 03:10:06 - train: epoch 0040, iter [01400, 05004], lr: 0.092567, loss: 1.8523
2022-07-06 03:10:39 - train: epoch 0040, iter [01500, 05004], lr: 0.092558, loss: 2.0749
2022-07-06 03:11:12 - train: epoch 0040, iter [01600, 05004], lr: 0.092550, loss: 1.8556
2022-07-06 03:11:45 - train: epoch 0040, iter [01700, 05004], lr: 0.092541, loss: 2.0513
2022-07-06 03:12:18 - train: epoch 0040, iter [01800, 05004], lr: 0.092533, loss: 1.7625
2022-07-06 03:12:52 - train: epoch 0040, iter [01900, 05004], lr: 0.092524, loss: 1.7592
2022-07-06 03:13:25 - train: epoch 0040, iter [02000, 05004], lr: 0.092516, loss: 1.9422
2022-07-06 03:13:59 - train: epoch 0040, iter [02100, 05004], lr: 0.092508, loss: 1.7990
2022-07-06 03:14:32 - train: epoch 0040, iter [02200, 05004], lr: 0.092499, loss: 1.6686
2022-07-06 03:15:05 - train: epoch 0040, iter [02300, 05004], lr: 0.092491, loss: 1.8356
2022-07-06 03:15:38 - train: epoch 0040, iter [02400, 05004], lr: 0.092482, loss: 1.9180
2022-07-06 03:16:12 - train: epoch 0040, iter [02500, 05004], lr: 0.092474, loss: 1.9485
2022-07-06 03:16:46 - train: epoch 0040, iter [02600, 05004], lr: 0.092465, loss: 1.7467
2022-07-06 03:17:19 - train: epoch 0040, iter [02700, 05004], lr: 0.092457, loss: 1.9775
2022-07-06 03:17:53 - train: epoch 0040, iter [02800, 05004], lr: 0.092448, loss: 1.9692
2022-07-06 03:18:26 - train: epoch 0040, iter [02900, 05004], lr: 0.092440, loss: 2.1914
2022-07-06 03:19:00 - train: epoch 0040, iter [03000, 05004], lr: 0.092431, loss: 2.0622
2022-07-06 03:19:33 - train: epoch 0040, iter [03100, 05004], lr: 0.092423, loss: 1.8414
2022-07-06 03:20:07 - train: epoch 0040, iter [03200, 05004], lr: 0.092414, loss: 2.1325
2022-07-06 03:20:42 - train: epoch 0040, iter [03300, 05004], lr: 0.092405, loss: 1.9874
2022-07-06 03:21:15 - train: epoch 0040, iter [03400, 05004], lr: 0.092397, loss: 2.0171
2022-07-06 03:21:48 - train: epoch 0040, iter [03500, 05004], lr: 0.092388, loss: 1.8813
2022-07-06 03:22:21 - train: epoch 0040, iter [03600, 05004], lr: 0.092380, loss: 1.9455
2022-07-06 03:22:55 - train: epoch 0040, iter [03700, 05004], lr: 0.092371, loss: 1.8185
2022-07-06 03:23:28 - train: epoch 0040, iter [03800, 05004], lr: 0.092363, loss: 1.8205
2022-07-06 03:24:01 - train: epoch 0040, iter [03900, 05004], lr: 0.092354, loss: 2.0374
2022-07-06 03:24:35 - train: epoch 0040, iter [04000, 05004], lr: 0.092346, loss: 2.1009
2022-07-06 03:25:08 - train: epoch 0040, iter [04100, 05004], lr: 0.092337, loss: 1.9804
2022-07-06 03:25:42 - train: epoch 0040, iter [04200, 05004], lr: 0.092329, loss: 1.8609
2022-07-06 03:26:14 - train: epoch 0040, iter [04300, 05004], lr: 0.092320, loss: 1.8819
2022-07-06 03:26:48 - train: epoch 0040, iter [04400, 05004], lr: 0.092311, loss: 1.9759
2022-07-06 03:27:21 - train: epoch 0040, iter [04500, 05004], lr: 0.092303, loss: 1.7495
2022-07-06 03:27:55 - train: epoch 0040, iter [04600, 05004], lr: 0.092294, loss: 1.9959
2022-07-06 03:28:29 - train: epoch 0040, iter [04700, 05004], lr: 0.092286, loss: 1.9313
2022-07-06 03:29:02 - train: epoch 0040, iter [04800, 05004], lr: 0.092277, loss: 1.8982
2022-07-06 03:29:35 - train: epoch 0040, iter [04900, 05004], lr: 0.092268, loss: 2.0393
2022-07-06 03:30:08 - train: epoch 0040, iter [05000, 05004], lr: 0.092260, loss: 1.9872
2022-07-06 03:30:09 - train: epoch 040, train_loss: 1.9138
2022-07-06 03:31:24 - eval: epoch: 040, acc1: 58.042%, acc5: 81.932%, test_loss: 1.7750, per_image_load_time: 0.855ms, per_image_inference_time: 0.467ms
2022-07-06 03:31:24 - until epoch: 040, best_acc1: 58.566%
2022-07-06 03:31:24 - epoch 041 lr: 0.092259
2022-07-06 03:32:04 - train: epoch 0041, iter [00100, 05004], lr: 0.092251, loss: 2.0476
2022-07-06 03:32:38 - train: epoch 0041, iter [00200, 05004], lr: 0.092242, loss: 1.9814
2022-07-06 03:33:12 - train: epoch 0041, iter [00300, 05004], lr: 0.092234, loss: 1.8419
2022-07-06 03:33:45 - train: epoch 0041, iter [00400, 05004], lr: 0.092225, loss: 1.8892
2022-07-06 03:34:18 - train: epoch 0041, iter [00500, 05004], lr: 0.092216, loss: 1.6147
2022-07-06 03:34:52 - train: epoch 0041, iter [00600, 05004], lr: 0.092208, loss: 1.9518
2022-07-06 03:35:26 - train: epoch 0041, iter [00700, 05004], lr: 0.092199, loss: 1.8262
2022-07-06 03:36:00 - train: epoch 0041, iter [00800, 05004], lr: 0.092191, loss: 1.6701
2022-07-06 03:36:35 - train: epoch 0041, iter [00900, 05004], lr: 0.092182, loss: 1.6568
2022-07-06 03:37:08 - train: epoch 0041, iter [01000, 05004], lr: 0.092173, loss: 2.0390
2022-07-06 03:37:42 - train: epoch 0041, iter [01100, 05004], lr: 0.092165, loss: 1.8797
2022-07-06 03:38:16 - train: epoch 0041, iter [01200, 05004], lr: 0.092156, loss: 1.6341
2022-07-06 03:38:49 - train: epoch 0041, iter [01300, 05004], lr: 0.092147, loss: 1.8901
2022-07-06 03:39:23 - train: epoch 0041, iter [01400, 05004], lr: 0.092139, loss: 1.9601
2022-07-06 03:39:56 - train: epoch 0041, iter [01500, 05004], lr: 0.092130, loss: 2.2052
2022-07-06 03:40:30 - train: epoch 0041, iter [01600, 05004], lr: 0.092121, loss: 1.7370
2022-07-06 03:41:04 - train: epoch 0041, iter [01700, 05004], lr: 0.092113, loss: 1.9669
2022-07-06 03:41:37 - train: epoch 0041, iter [01800, 05004], lr: 0.092104, loss: 1.9494
2022-07-06 03:42:12 - train: epoch 0041, iter [01900, 05004], lr: 0.092095, loss: 2.2030
2022-07-06 03:42:44 - train: epoch 0041, iter [02000, 05004], lr: 0.092087, loss: 1.7786
2022-07-06 03:43:18 - train: epoch 0041, iter [02100, 05004], lr: 0.092078, loss: 1.7108
2022-07-06 03:43:52 - train: epoch 0041, iter [02200, 05004], lr: 0.092069, loss: 1.6862
2022-07-06 03:44:26 - train: epoch 0041, iter [02300, 05004], lr: 0.092060, loss: 1.6768
2022-07-06 03:45:00 - train: epoch 0041, iter [02400, 05004], lr: 0.092052, loss: 2.0917
2022-07-06 03:45:32 - train: epoch 0041, iter [02500, 05004], lr: 0.092043, loss: 1.7587
2022-07-06 03:46:07 - train: epoch 0041, iter [02600, 05004], lr: 0.092034, loss: 2.0641
2022-07-06 03:46:40 - train: epoch 0041, iter [02700, 05004], lr: 0.092026, loss: 2.0810
2022-07-06 03:47:13 - train: epoch 0041, iter [02800, 05004], lr: 0.092017, loss: 1.8682
2022-07-06 03:47:46 - train: epoch 0041, iter [02900, 05004], lr: 0.092008, loss: 2.0892
2022-07-06 03:48:20 - train: epoch 0041, iter [03000, 05004], lr: 0.091999, loss: 2.0163
2022-07-06 03:48:54 - train: epoch 0041, iter [03100, 05004], lr: 0.091991, loss: 2.0464
2022-07-06 03:49:28 - train: epoch 0041, iter [03200, 05004], lr: 0.091982, loss: 1.8660
2022-07-06 03:50:02 - train: epoch 0041, iter [03300, 05004], lr: 0.091973, loss: 1.6769
2022-07-06 03:50:35 - train: epoch 0041, iter [03400, 05004], lr: 0.091964, loss: 1.7483
2022-07-06 03:51:09 - train: epoch 0041, iter [03500, 05004], lr: 0.091956, loss: 1.9028
2022-07-06 03:51:42 - train: epoch 0041, iter [03600, 05004], lr: 0.091947, loss: 2.0122
2022-07-06 03:52:15 - train: epoch 0041, iter [03700, 05004], lr: 0.091938, loss: 1.9286
2022-07-06 03:52:49 - train: epoch 0041, iter [03800, 05004], lr: 0.091929, loss: 1.7077
2022-07-06 03:53:24 - train: epoch 0041, iter [03900, 05004], lr: 0.091921, loss: 1.9969
2022-07-06 03:53:57 - train: epoch 0041, iter [04000, 05004], lr: 0.091912, loss: 1.8707
2022-07-06 03:54:31 - train: epoch 0041, iter [04100, 05004], lr: 0.091903, loss: 1.8425
2022-07-06 03:55:04 - train: epoch 0041, iter [04200, 05004], lr: 0.091894, loss: 1.9008
2022-07-06 03:55:37 - train: epoch 0041, iter [04300, 05004], lr: 0.091886, loss: 1.9270
2022-07-06 03:56:11 - train: epoch 0041, iter [04400, 05004], lr: 0.091877, loss: 1.9556
2022-07-06 03:56:44 - train: epoch 0041, iter [04500, 05004], lr: 0.091868, loss: 1.9829
2022-07-06 03:57:17 - train: epoch 0041, iter [04600, 05004], lr: 0.091859, loss: 2.0342
2022-07-06 03:57:51 - train: epoch 0041, iter [04700, 05004], lr: 0.091850, loss: 2.1150
2022-07-06 03:58:25 - train: epoch 0041, iter [04800, 05004], lr: 0.091841, loss: 1.9981
2022-07-06 03:58:58 - train: epoch 0041, iter [04900, 05004], lr: 0.091833, loss: 1.9439
2022-07-06 03:59:30 - train: epoch 0041, iter [05000, 05004], lr: 0.091824, loss: 1.9663
2022-07-06 03:59:31 - train: epoch 041, train_loss: 1.9115
2022-07-06 04:00:46 - eval: epoch: 041, acc1: 57.110%, acc5: 81.544%, test_loss: 1.8168, per_image_load_time: 1.360ms, per_image_inference_time: 0.466ms
2022-07-06 04:00:46 - until epoch: 041, best_acc1: 58.566%
2022-07-06 04:00:46 - epoch 042 lr: 0.091823
2022-07-06 04:01:26 - train: epoch 0042, iter [00100, 05004], lr: 0.091815, loss: 1.6338
2022-07-06 04:01:59 - train: epoch 0042, iter [00200, 05004], lr: 0.091806, loss: 1.9807
2022-07-06 04:02:33 - train: epoch 0042, iter [00300, 05004], lr: 0.091797, loss: 1.8617
2022-07-06 04:03:06 - train: epoch 0042, iter [00400, 05004], lr: 0.091788, loss: 1.5921
2022-07-06 04:03:40 - train: epoch 0042, iter [00500, 05004], lr: 0.091779, loss: 1.9421
2022-07-06 04:04:15 - train: epoch 0042, iter [00600, 05004], lr: 0.091770, loss: 1.6928
2022-07-06 04:04:47 - train: epoch 0042, iter [00700, 05004], lr: 0.091762, loss: 1.8864
2022-07-06 04:05:22 - train: epoch 0042, iter [00800, 05004], lr: 0.091753, loss: 1.7561
2022-07-06 04:05:55 - train: epoch 0042, iter [00900, 05004], lr: 0.091744, loss: 2.1693
2022-07-06 04:06:29 - train: epoch 0042, iter [01000, 05004], lr: 0.091735, loss: 1.9185
2022-07-06 04:07:03 - train: epoch 0042, iter [01100, 05004], lr: 0.091726, loss: 1.7240
2022-07-06 04:07:36 - train: epoch 0042, iter [01200, 05004], lr: 0.091717, loss: 1.8322
2022-07-06 04:08:10 - train: epoch 0042, iter [01300, 05004], lr: 0.091708, loss: 1.9307
2022-07-06 04:08:44 - train: epoch 0042, iter [01400, 05004], lr: 0.091700, loss: 2.3130
2022-07-06 04:09:18 - train: epoch 0042, iter [01500, 05004], lr: 0.091691, loss: 1.7584
2022-07-06 04:09:52 - train: epoch 0042, iter [01600, 05004], lr: 0.091682, loss: 1.9492
2022-07-06 04:10:25 - train: epoch 0042, iter [01700, 05004], lr: 0.091673, loss: 1.8987
2022-07-06 04:10:59 - train: epoch 0042, iter [01800, 05004], lr: 0.091664, loss: 1.8556
2022-07-06 04:11:33 - train: epoch 0042, iter [01900, 05004], lr: 0.091655, loss: 2.0996
2022-07-06 04:12:06 - train: epoch 0042, iter [02000, 05004], lr: 0.091646, loss: 1.8543
2022-07-06 04:12:40 - train: epoch 0042, iter [02100, 05004], lr: 0.091637, loss: 1.6490
2022-07-06 04:13:14 - train: epoch 0042, iter [02200, 05004], lr: 0.091628, loss: 1.8413
2022-07-06 04:13:48 - train: epoch 0042, iter [02300, 05004], lr: 0.091619, loss: 1.8460
2022-07-06 04:14:22 - train: epoch 0042, iter [02400, 05004], lr: 0.091611, loss: 2.2042
2022-07-06 04:14:56 - train: epoch 0042, iter [02500, 05004], lr: 0.091602, loss: 2.0982
2022-07-06 04:15:30 - train: epoch 0042, iter [02600, 05004], lr: 0.091593, loss: 2.1724
2022-07-06 04:16:04 - train: epoch 0042, iter [02700, 05004], lr: 0.091584, loss: 1.7784
2022-07-06 04:16:38 - train: epoch 0042, iter [02800, 05004], lr: 0.091575, loss: 1.8027
2022-07-06 04:17:11 - train: epoch 0042, iter [02900, 05004], lr: 0.091566, loss: 1.8124
2022-07-06 04:17:46 - train: epoch 0042, iter [03000, 05004], lr: 0.091557, loss: 1.9399
2022-07-06 04:18:20 - train: epoch 0042, iter [03100, 05004], lr: 0.091548, loss: 1.9673
2022-07-06 04:18:53 - train: epoch 0042, iter [03200, 05004], lr: 0.091539, loss: 1.9701
2022-07-06 04:19:28 - train: epoch 0042, iter [03300, 05004], lr: 0.091530, loss: 2.0494
2022-07-06 04:20:01 - train: epoch 0042, iter [03400, 05004], lr: 0.091521, loss: 1.7478
2022-07-06 04:20:35 - train: epoch 0042, iter [03500, 05004], lr: 0.091512, loss: 1.9953
2022-07-06 04:21:09 - train: epoch 0042, iter [03600, 05004], lr: 0.091503, loss: 2.0011
2022-07-06 04:21:43 - train: epoch 0042, iter [03700, 05004], lr: 0.091494, loss: 1.9585
2022-07-06 04:22:17 - train: epoch 0042, iter [03800, 05004], lr: 0.091485, loss: 1.6696
2022-07-06 04:22:51 - train: epoch 0042, iter [03900, 05004], lr: 0.091476, loss: 1.7968
2022-07-06 04:23:26 - train: epoch 0042, iter [04000, 05004], lr: 0.091467, loss: 1.9132
2022-07-06 04:24:00 - train: epoch 0042, iter [04100, 05004], lr: 0.091458, loss: 2.0309
2022-07-06 04:24:33 - train: epoch 0042, iter [04200, 05004], lr: 0.091449, loss: 2.0133
2022-07-06 04:25:08 - train: epoch 0042, iter [04300, 05004], lr: 0.091440, loss: 1.7500
2022-07-06 04:25:43 - train: epoch 0042, iter [04400, 05004], lr: 0.091431, loss: 1.8715
2022-07-06 04:26:18 - train: epoch 0042, iter [04500, 05004], lr: 0.091422, loss: 1.9550
2022-07-06 04:26:50 - train: epoch 0042, iter [04600, 05004], lr: 0.091413, loss: 2.2184
2022-07-06 04:27:24 - train: epoch 0042, iter [04700, 05004], lr: 0.091404, loss: 1.8182
2022-07-06 04:27:58 - train: epoch 0042, iter [04800, 05004], lr: 0.091395, loss: 1.9888
2022-07-06 04:28:32 - train: epoch 0042, iter [04900, 05004], lr: 0.091386, loss: 1.9459
2022-07-06 04:29:04 - train: epoch 0042, iter [05000, 05004], lr: 0.091377, loss: 1.8833
2022-07-06 04:29:05 - train: epoch 042, train_loss: 1.9052
2022-07-06 04:30:20 - eval: epoch: 042, acc1: 57.618%, acc5: 81.738%, test_loss: 1.8004, per_image_load_time: 1.192ms, per_image_inference_time: 0.489ms
2022-07-06 04:30:20 - until epoch: 042, best_acc1: 58.566%
2022-07-06 04:30:20 - epoch 043 lr: 0.091377
2022-07-06 04:30:59 - train: epoch 0043, iter [00100, 05004], lr: 0.091368, loss: 1.8572
2022-07-06 04:31:32 - train: epoch 0043, iter [00200, 05004], lr: 0.091359, loss: 1.9511
2022-07-06 04:32:06 - train: epoch 0043, iter [00300, 05004], lr: 0.091350, loss: 1.6304
2022-07-06 04:32:39 - train: epoch 0043, iter [00400, 05004], lr: 0.091340, loss: 1.7888
2022-07-06 04:33:14 - train: epoch 0043, iter [00500, 05004], lr: 0.091331, loss: 1.7772
2022-07-06 04:33:47 - train: epoch 0043, iter [00600, 05004], lr: 0.091322, loss: 1.8224
2022-07-06 04:34:21 - train: epoch 0043, iter [00700, 05004], lr: 0.091313, loss: 1.9985
2022-07-06 04:34:55 - train: epoch 0043, iter [00800, 05004], lr: 0.091304, loss: 1.9921
2022-07-06 04:35:29 - train: epoch 0043, iter [00900, 05004], lr: 0.091295, loss: 1.8979
2022-07-06 04:36:02 - train: epoch 0043, iter [01000, 05004], lr: 0.091286, loss: 2.0349
2022-07-06 04:36:36 - train: epoch 0043, iter [01100, 05004], lr: 0.091277, loss: 1.9488
2022-07-06 04:37:10 - train: epoch 0043, iter [01200, 05004], lr: 0.091268, loss: 1.8378
2022-07-06 04:37:43 - train: epoch 0043, iter [01300, 05004], lr: 0.091259, loss: 1.9912
2022-07-06 04:38:18 - train: epoch 0043, iter [01400, 05004], lr: 0.091250, loss: 1.8777
2022-07-06 04:38:51 - train: epoch 0043, iter [01500, 05004], lr: 0.091241, loss: 1.7702
2022-07-06 04:39:25 - train: epoch 0043, iter [01600, 05004], lr: 0.091232, loss: 2.0788
2022-07-06 04:39:59 - train: epoch 0043, iter [01700, 05004], lr: 0.091222, loss: 1.9736
2022-07-06 04:40:33 - train: epoch 0043, iter [01800, 05004], lr: 0.091213, loss: 1.9666
2022-07-06 04:41:07 - train: epoch 0043, iter [01900, 05004], lr: 0.091204, loss: 1.8792
2022-07-06 04:41:41 - train: epoch 0043, iter [02000, 05004], lr: 0.091195, loss: 1.8000
2022-07-06 04:42:14 - train: epoch 0043, iter [02100, 05004], lr: 0.091186, loss: 1.8309
2022-07-06 04:42:47 - train: epoch 0043, iter [02200, 05004], lr: 0.091177, loss: 2.0220
2022-07-06 04:43:21 - train: epoch 0043, iter [02300, 05004], lr: 0.091168, loss: 1.9869
2022-07-06 04:43:55 - train: epoch 0043, iter [02400, 05004], lr: 0.091159, loss: 1.9132
2022-07-06 04:44:29 - train: epoch 0043, iter [02500, 05004], lr: 0.091149, loss: 2.1117
2022-07-06 04:45:03 - train: epoch 0043, iter [02600, 05004], lr: 0.091140, loss: 1.6124
2022-07-06 04:45:36 - train: epoch 0043, iter [02700, 05004], lr: 0.091131, loss: 1.9179
2022-07-06 04:46:11 - train: epoch 0043, iter [02800, 05004], lr: 0.091122, loss: 1.7004
2022-07-06 04:46:44 - train: epoch 0043, iter [02900, 05004], lr: 0.091113, loss: 1.8961
2022-07-06 04:47:18 - train: epoch 0043, iter [03000, 05004], lr: 0.091104, loss: 2.0885
2022-07-06 04:47:51 - train: epoch 0043, iter [03100, 05004], lr: 0.091094, loss: 1.9937
2022-07-06 04:48:25 - train: epoch 0043, iter [03200, 05004], lr: 0.091085, loss: 2.0431
2022-07-06 04:48:58 - train: epoch 0043, iter [03300, 05004], lr: 0.091076, loss: 1.8981
2022-07-06 04:49:31 - train: epoch 0043, iter [03400, 05004], lr: 0.091067, loss: 1.9207
2022-07-06 04:50:06 - train: epoch 0043, iter [03500, 05004], lr: 0.091058, loss: 1.8134
2022-07-06 04:50:39 - train: epoch 0043, iter [03600, 05004], lr: 0.091049, loss: 1.9198
2022-07-06 04:51:12 - train: epoch 0043, iter [03700, 05004], lr: 0.091039, loss: 1.8765
2022-07-06 04:51:45 - train: epoch 0043, iter [03800, 05004], lr: 0.091030, loss: 2.0258
2022-07-06 04:52:19 - train: epoch 0043, iter [03900, 05004], lr: 0.091021, loss: 2.0848
2022-07-06 04:52:53 - train: epoch 0043, iter [04000, 05004], lr: 0.091012, loss: 1.7934
2022-07-06 04:53:27 - train: epoch 0043, iter [04100, 05004], lr: 0.091003, loss: 2.3616
2022-07-06 04:54:00 - train: epoch 0043, iter [04200, 05004], lr: 0.090993, loss: 1.9812
2022-07-06 04:54:33 - train: epoch 0043, iter [04300, 05004], lr: 0.090984, loss: 1.8566
2022-07-06 04:55:07 - train: epoch 0043, iter [04400, 05004], lr: 0.090975, loss: 1.8594
2022-07-06 04:55:40 - train: epoch 0043, iter [04500, 05004], lr: 0.090966, loss: 1.7836
2022-07-06 04:56:14 - train: epoch 0043, iter [04600, 05004], lr: 0.090956, loss: 1.8435
2022-07-06 04:56:47 - train: epoch 0043, iter [04700, 05004], lr: 0.090947, loss: 1.8538
2022-07-06 04:57:21 - train: epoch 0043, iter [04800, 05004], lr: 0.090938, loss: 1.9853
2022-07-06 04:57:55 - train: epoch 0043, iter [04900, 05004], lr: 0.090929, loss: 1.8374
2022-07-06 04:58:27 - train: epoch 0043, iter [05000, 05004], lr: 0.090919, loss: 1.8472
2022-07-06 04:58:28 - train: epoch 043, train_loss: 1.8996
2022-07-06 04:59:43 - eval: epoch: 043, acc1: 53.466%, acc5: 78.200%, test_loss: 2.0267, per_image_load_time: 1.898ms, per_image_inference_time: 0.467ms
2022-07-06 04:59:43 - until epoch: 043, best_acc1: 58.566%
2022-07-06 04:59:43 - epoch 044 lr: 0.090919
2022-07-06 05:00:22 - train: epoch 0044, iter [00100, 05004], lr: 0.090910, loss: 1.9807
2022-07-06 05:00:55 - train: epoch 0044, iter [00200, 05004], lr: 0.090901, loss: 1.9643
2022-07-06 05:01:29 - train: epoch 0044, iter [00300, 05004], lr: 0.090891, loss: 1.8577
2022-07-06 05:02:03 - train: epoch 0044, iter [00400, 05004], lr: 0.090882, loss: 1.4687
2022-07-06 05:02:37 - train: epoch 0044, iter [00500, 05004], lr: 0.090873, loss: 1.8624
2022-07-06 05:03:11 - train: epoch 0044, iter [00600, 05004], lr: 0.090863, loss: 1.6217
2022-07-06 05:03:43 - train: epoch 0044, iter [00700, 05004], lr: 0.090854, loss: 1.6529
2022-07-06 05:04:17 - train: epoch 0044, iter [00800, 05004], lr: 0.090845, loss: 1.8490
2022-07-06 05:04:49 - train: epoch 0044, iter [00900, 05004], lr: 0.090836, loss: 1.8656
2022-07-06 05:05:22 - train: epoch 0044, iter [01000, 05004], lr: 0.090826, loss: 1.8628
2022-07-06 05:05:56 - train: epoch 0044, iter [01100, 05004], lr: 0.090817, loss: 1.7960
2022-07-06 05:06:29 - train: epoch 0044, iter [01200, 05004], lr: 0.090808, loss: 1.6939
2022-07-06 05:07:03 - train: epoch 0044, iter [01300, 05004], lr: 0.090798, loss: 1.9403
2022-07-06 05:07:36 - train: epoch 0044, iter [01400, 05004], lr: 0.090789, loss: 1.7988
2022-07-06 05:08:10 - train: epoch 0044, iter [01500, 05004], lr: 0.090780, loss: 1.7899
2022-07-06 05:08:43 - train: epoch 0044, iter [01600, 05004], lr: 0.090771, loss: 1.8567
2022-07-06 05:09:17 - train: epoch 0044, iter [01700, 05004], lr: 0.090761, loss: 1.7435
2022-07-06 05:09:51 - train: epoch 0044, iter [01800, 05004], lr: 0.090752, loss: 1.8160
2022-07-06 05:10:24 - train: epoch 0044, iter [01900, 05004], lr: 0.090743, loss: 2.0297
2022-07-06 05:10:57 - train: epoch 0044, iter [02000, 05004], lr: 0.090733, loss: 1.7367
2022-07-06 05:11:31 - train: epoch 0044, iter [02100, 05004], lr: 0.090724, loss: 2.0698
2022-07-06 05:12:05 - train: epoch 0044, iter [02200, 05004], lr: 0.090715, loss: 1.8997
2022-07-06 05:12:38 - train: epoch 0044, iter [02300, 05004], lr: 0.090705, loss: 1.9701
2022-07-06 05:13:12 - train: epoch 0044, iter [02400, 05004], lr: 0.090696, loss: 1.8832
2022-07-06 05:13:46 - train: epoch 0044, iter [02500, 05004], lr: 0.090686, loss: 1.9282
2022-07-06 05:14:20 - train: epoch 0044, iter [02600, 05004], lr: 0.090677, loss: 1.8705
2022-07-06 05:14:53 - train: epoch 0044, iter [02700, 05004], lr: 0.090668, loss: 1.9174
2022-07-06 05:15:26 - train: epoch 0044, iter [02800, 05004], lr: 0.090658, loss: 1.6471
2022-07-06 05:16:00 - train: epoch 0044, iter [02900, 05004], lr: 0.090649, loss: 1.7030
2022-07-06 05:16:34 - train: epoch 0044, iter [03000, 05004], lr: 0.090640, loss: 1.7653
2022-07-06 05:17:07 - train: epoch 0044, iter [03100, 05004], lr: 0.090630, loss: 1.8781
2022-07-06 05:17:40 - train: epoch 0044, iter [03200, 05004], lr: 0.090621, loss: 1.5589
2022-07-06 05:18:14 - train: epoch 0044, iter [03300, 05004], lr: 0.090611, loss: 1.8019
2022-07-06 05:18:47 - train: epoch 0044, iter [03400, 05004], lr: 0.090602, loss: 2.2069
2022-07-06 05:19:21 - train: epoch 0044, iter [03500, 05004], lr: 0.090593, loss: 1.7645
2022-07-06 05:19:55 - train: epoch 0044, iter [03600, 05004], lr: 0.090583, loss: 2.1131
2022-07-06 05:20:29 - train: epoch 0044, iter [03700, 05004], lr: 0.090574, loss: 1.8867
2022-07-06 05:21:02 - train: epoch 0044, iter [03800, 05004], lr: 0.090564, loss: 1.5289
2022-07-06 05:21:35 - train: epoch 0044, iter [03900, 05004], lr: 0.090555, loss: 1.9294
2022-07-06 05:22:09 - train: epoch 0044, iter [04000, 05004], lr: 0.090546, loss: 1.8961
2022-07-06 05:22:43 - train: epoch 0044, iter [04100, 05004], lr: 0.090536, loss: 1.8285
2022-07-06 05:23:17 - train: epoch 0044, iter [04200, 05004], lr: 0.090527, loss: 1.9769
2022-07-06 05:23:49 - train: epoch 0044, iter [04300, 05004], lr: 0.090517, loss: 2.1316
2022-07-06 05:24:23 - train: epoch 0044, iter [04400, 05004], lr: 0.090508, loss: 1.6369
2022-07-06 05:24:57 - train: epoch 0044, iter [04500, 05004], lr: 0.090498, loss: 2.0409
2022-07-06 05:25:31 - train: epoch 0044, iter [04600, 05004], lr: 0.090489, loss: 1.8773
2022-07-06 05:26:04 - train: epoch 0044, iter [04700, 05004], lr: 0.090480, loss: 1.9061
2022-07-06 05:26:37 - train: epoch 0044, iter [04800, 05004], lr: 0.090470, loss: 2.0165
2022-07-06 05:27:11 - train: epoch 0044, iter [04900, 05004], lr: 0.090461, loss: 1.8023
2022-07-06 05:27:43 - train: epoch 0044, iter [05000, 05004], lr: 0.090451, loss: 2.0909
2022-07-06 05:27:45 - train: epoch 044, train_loss: 1.8956
2022-07-06 05:28:59 - eval: epoch: 044, acc1: 58.552%, acc5: 82.566%, test_loss: 1.7432, per_image_load_time: 0.933ms, per_image_inference_time: 0.471ms
2022-07-06 05:29:00 - until epoch: 044, best_acc1: 58.566%
2022-07-06 05:29:00 - epoch 045 lr: 0.090451
2022-07-06 05:29:40 - train: epoch 0045, iter [00100, 05004], lr: 0.090441, loss: 1.7689
2022-07-06 05:30:13 - train: epoch 0045, iter [00200, 05004], lr: 0.090432, loss: 1.9232
2022-07-06 05:30:48 - train: epoch 0045, iter [00300, 05004], lr: 0.090422, loss: 1.9085
2022-07-06 05:31:21 - train: epoch 0045, iter [00400, 05004], lr: 0.090413, loss: 1.6802
2022-07-06 05:31:55 - train: epoch 0045, iter [00500, 05004], lr: 0.090403, loss: 2.0328
2022-07-06 05:32:29 - train: epoch 0045, iter [00600, 05004], lr: 0.090394, loss: 1.8974
2022-07-06 05:33:03 - train: epoch 0045, iter [00700, 05004], lr: 0.090385, loss: 1.5830
2022-07-06 05:33:36 - train: epoch 0045, iter [00800, 05004], lr: 0.090375, loss: 1.8082
2022-07-06 05:34:09 - train: epoch 0045, iter [00900, 05004], lr: 0.090366, loss: 1.9718
2022-07-06 05:34:43 - train: epoch 0045, iter [01000, 05004], lr: 0.090356, loss: 1.8212
2022-07-06 05:35:17 - train: epoch 0045, iter [01100, 05004], lr: 0.090347, loss: 2.0015
2022-07-06 05:35:50 - train: epoch 0045, iter [01200, 05004], lr: 0.090337, loss: 1.9084
2022-07-06 05:36:24 - train: epoch 0045, iter [01300, 05004], lr: 0.090327, loss: 2.0085
2022-07-06 05:36:58 - train: epoch 0045, iter [01400, 05004], lr: 0.090318, loss: 1.8974
2022-07-06 05:37:32 - train: epoch 0045, iter [01500, 05004], lr: 0.090308, loss: 2.0415
2022-07-06 05:38:05 - train: epoch 0045, iter [01600, 05004], lr: 0.090299, loss: 1.8321
2022-07-06 05:38:39 - train: epoch 0045, iter [01700, 05004], lr: 0.090289, loss: 1.7656
2022-07-06 05:39:13 - train: epoch 0045, iter [01800, 05004], lr: 0.090280, loss: 1.7747
2022-07-06 05:39:47 - train: epoch 0045, iter [01900, 05004], lr: 0.090270, loss: 1.8861
2022-07-06 05:40:21 - train: epoch 0045, iter [02000, 05004], lr: 0.090261, loss: 2.0272
2022-07-06 05:40:55 - train: epoch 0045, iter [02100, 05004], lr: 0.090251, loss: 2.0527
2022-07-06 05:41:28 - train: epoch 0045, iter [02200, 05004], lr: 0.090242, loss: 1.9295
2022-07-06 05:42:02 - train: epoch 0045, iter [02300, 05004], lr: 0.090232, loss: 1.7696
2022-07-06 05:42:36 - train: epoch 0045, iter [02400, 05004], lr: 0.090223, loss: 2.0199
2022-07-06 05:43:09 - train: epoch 0045, iter [02500, 05004], lr: 0.090213, loss: 1.8600
2022-07-06 05:43:43 - train: epoch 0045, iter [02600, 05004], lr: 0.090203, loss: 1.8558
2022-07-06 05:44:17 - train: epoch 0045, iter [02700, 05004], lr: 0.090194, loss: 1.7648
2022-07-06 05:44:51 - train: epoch 0045, iter [02800, 05004], lr: 0.090184, loss: 1.8418
2022-07-06 05:45:23 - train: epoch 0045, iter [02900, 05004], lr: 0.090175, loss: 2.0472
2022-07-06 05:45:57 - train: epoch 0045, iter [03000, 05004], lr: 0.090165, loss: 2.0306
2022-07-06 05:46:31 - train: epoch 0045, iter [03100, 05004], lr: 0.090156, loss: 1.8769
2022-07-06 05:47:05 - train: epoch 0045, iter [03200, 05004], lr: 0.090146, loss: 1.9696
2022-07-06 05:47:39 - train: epoch 0045, iter [03300, 05004], lr: 0.090136, loss: 1.9365
2022-07-06 05:48:12 - train: epoch 0045, iter [03400, 05004], lr: 0.090127, loss: 1.6686
2022-07-06 05:48:45 - train: epoch 0045, iter [03500, 05004], lr: 0.090117, loss: 2.0632
2022-07-06 05:49:20 - train: epoch 0045, iter [03600, 05004], lr: 0.090108, loss: 2.0566
2022-07-06 05:49:53 - train: epoch 0045, iter [03700, 05004], lr: 0.090098, loss: 1.9267
2022-07-06 05:50:26 - train: epoch 0045, iter [03800, 05004], lr: 0.090088, loss: 1.7585
2022-07-06 05:51:00 - train: epoch 0045, iter [03900, 05004], lr: 0.090079, loss: 1.9641
2022-07-06 05:51:33 - train: epoch 0045, iter [04000, 05004], lr: 0.090069, loss: 2.0723
2022-07-06 05:52:07 - train: epoch 0045, iter [04100, 05004], lr: 0.090059, loss: 1.8010
2022-07-06 05:52:40 - train: epoch 0045, iter [04200, 05004], lr: 0.090050, loss: 2.1330
2022-07-06 05:53:14 - train: epoch 0045, iter [04300, 05004], lr: 0.090040, loss: 2.1317
2022-07-06 05:53:48 - train: epoch 0045, iter [04400, 05004], lr: 0.090030, loss: 1.9936
2022-07-06 05:54:22 - train: epoch 0045, iter [04500, 05004], lr: 0.090021, loss: 1.8441
2022-07-06 05:54:55 - train: epoch 0045, iter [04600, 05004], lr: 0.090011, loss: 2.0623
2022-07-06 05:55:29 - train: epoch 0045, iter [04700, 05004], lr: 0.090002, loss: 1.9406
2022-07-06 05:56:02 - train: epoch 0045, iter [04800, 05004], lr: 0.089992, loss: 1.8132
2022-07-06 05:56:37 - train: epoch 0045, iter [04900, 05004], lr: 0.089982, loss: 1.9123
2022-07-06 05:57:09 - train: epoch 0045, iter [05000, 05004], lr: 0.089973, loss: 1.8703
2022-07-06 05:57:10 - train: epoch 045, train_loss: 1.8880
2022-07-06 05:58:24 - eval: epoch: 045, acc1: 59.832%, acc5: 83.372%, test_loss: 1.6898, per_image_load_time: 0.885ms, per_image_inference_time: 0.470ms
2022-07-06 05:58:25 - until epoch: 045, best_acc1: 59.832%
2022-07-06 05:58:25 - epoch 046 lr: 0.089972
2022-07-06 05:59:04 - train: epoch 0046, iter [00100, 05004], lr: 0.089962, loss: 1.5011
2022-07-06 05:59:38 - train: epoch 0046, iter [00200, 05004], lr: 0.089953, loss: 1.7486
2022-07-06 06:00:11 - train: epoch 0046, iter [00300, 05004], lr: 0.089943, loss: 1.9136
2022-07-06 06:00:45 - train: epoch 0046, iter [00400, 05004], lr: 0.089933, loss: 2.0365
2022-07-06 06:01:18 - train: epoch 0046, iter [00500, 05004], lr: 0.089924, loss: 1.6067
2022-07-06 06:01:52 - train: epoch 0046, iter [00600, 05004], lr: 0.089914, loss: 2.0057
2022-07-06 06:02:25 - train: epoch 0046, iter [00700, 05004], lr: 0.089904, loss: 1.8143
2022-07-06 06:03:00 - train: epoch 0046, iter [00800, 05004], lr: 0.089895, loss: 1.9583
2022-07-06 06:03:34 - train: epoch 0046, iter [00900, 05004], lr: 0.089885, loss: 1.8489
2022-07-06 06:04:07 - train: epoch 0046, iter [01000, 05004], lr: 0.089875, loss: 1.6942
2022-07-06 06:04:41 - train: epoch 0046, iter [01100, 05004], lr: 0.089866, loss: 1.8405
2022-07-06 06:05:15 - train: epoch 0046, iter [01200, 05004], lr: 0.089856, loss: 1.8428
2022-07-06 06:05:48 - train: epoch 0046, iter [01300, 05004], lr: 0.089846, loss: 1.8707
2022-07-06 06:06:22 - train: epoch 0046, iter [01400, 05004], lr: 0.089836, loss: 2.1124
2022-07-06 06:06:55 - train: epoch 0046, iter [01500, 05004], lr: 0.089827, loss: 1.8611
2022-07-06 06:07:29 - train: epoch 0046, iter [01600, 05004], lr: 0.089817, loss: 1.8471
2022-07-06 06:08:03 - train: epoch 0046, iter [01700, 05004], lr: 0.089807, loss: 1.7248
2022-07-06 06:08:36 - train: epoch 0046, iter [01800, 05004], lr: 0.089797, loss: 1.8507
2022-07-06 06:09:10 - train: epoch 0046, iter [01900, 05004], lr: 0.089788, loss: 1.6844
2022-07-06 06:09:43 - train: epoch 0046, iter [02000, 05004], lr: 0.089778, loss: 1.9028
2022-07-06 06:10:16 - train: epoch 0046, iter [02100, 05004], lr: 0.089768, loss: 2.0495
2022-07-06 06:10:50 - train: epoch 0046, iter [02200, 05004], lr: 0.089758, loss: 1.6035
2022-07-06 06:11:23 - train: epoch 0046, iter [02300, 05004], lr: 0.089749, loss: 2.0963
2022-07-06 06:11:57 - train: epoch 0046, iter [02400, 05004], lr: 0.089739, loss: 1.8542
2022-07-06 06:12:31 - train: epoch 0046, iter [02500, 05004], lr: 0.089729, loss: 1.9143
2022-07-06 06:13:04 - train: epoch 0046, iter [02600, 05004], lr: 0.089719, loss: 1.9908
2022-07-06 06:13:37 - train: epoch 0046, iter [02700, 05004], lr: 0.089710, loss: 1.7339
2022-07-06 06:14:11 - train: epoch 0046, iter [02800, 05004], lr: 0.089700, loss: 1.7660
2022-07-06 06:14:44 - train: epoch 0046, iter [02900, 05004], lr: 0.089690, loss: 1.8780
2022-07-06 06:15:18 - train: epoch 0046, iter [03000, 05004], lr: 0.089680, loss: 1.7499
2022-07-06 06:15:51 - train: epoch 0046, iter [03100, 05004], lr: 0.089670, loss: 1.7113
2022-07-06 06:16:25 - train: epoch 0046, iter [03200, 05004], lr: 0.089661, loss: 2.0876
2022-07-06 06:16:58 - train: epoch 0046, iter [03300, 05004], lr: 0.089651, loss: 1.9524
2022-07-06 06:17:32 - train: epoch 0046, iter [03400, 05004], lr: 0.089641, loss: 1.9164
2022-07-06 06:18:05 - train: epoch 0046, iter [03500, 05004], lr: 0.089631, loss: 1.8603
2022-07-06 06:18:38 - train: epoch 0046, iter [03600, 05004], lr: 0.089621, loss: 1.7073
2022-07-06 06:19:12 - train: epoch 0046, iter [03700, 05004], lr: 0.089611, loss: 1.6402
2022-07-06 06:19:46 - train: epoch 0046, iter [03800, 05004], lr: 0.089602, loss: 2.0328
2022-07-06 06:20:18 - train: epoch 0046, iter [03900, 05004], lr: 0.089592, loss: 1.9012
2022-07-06 06:20:53 - train: epoch 0046, iter [04000, 05004], lr: 0.089582, loss: 1.7632
2022-07-06 06:21:26 - train: epoch 0046, iter [04100, 05004], lr: 0.089572, loss: 2.0896
2022-07-06 06:21:59 - train: epoch 0046, iter [04200, 05004], lr: 0.089562, loss: 1.8815
2022-07-06 06:22:33 - train: epoch 0046, iter [04300, 05004], lr: 0.089552, loss: 2.0666
2022-07-06 06:23:07 - train: epoch 0046, iter [04400, 05004], lr: 0.089543, loss: 2.0286
2022-07-06 06:23:40 - train: epoch 0046, iter [04500, 05004], lr: 0.089533, loss: 1.8370
2022-07-06 06:24:14 - train: epoch 0046, iter [04600, 05004], lr: 0.089523, loss: 1.8636
2022-07-06 06:24:47 - train: epoch 0046, iter [04700, 05004], lr: 0.089513, loss: 1.8454
2022-07-06 06:25:20 - train: epoch 0046, iter [04800, 05004], lr: 0.089503, loss: 1.8495
2022-07-06 06:25:54 - train: epoch 0046, iter [04900, 05004], lr: 0.089493, loss: 1.9229
2022-07-06 06:26:27 - train: epoch 0046, iter [05000, 05004], lr: 0.089483, loss: 1.8215
2022-07-06 06:26:28 - train: epoch 046, train_loss: 1.8818
2022-07-06 06:27:43 - eval: epoch: 046, acc1: 56.654%, acc5: 80.590%, test_loss: 1.8593, per_image_load_time: 1.263ms, per_image_inference_time: 0.456ms
2022-07-06 06:27:43 - until epoch: 046, best_acc1: 59.832%
2022-07-06 06:27:43 - epoch 047 lr: 0.089483
2022-07-06 06:28:22 - train: epoch 0047, iter [00100, 05004], lr: 0.089473, loss: 1.7381
2022-07-06 06:28:56 - train: epoch 0047, iter [00200, 05004], lr: 0.089463, loss: 1.9568
2022-07-06 06:29:30 - train: epoch 0047, iter [00300, 05004], lr: 0.089453, loss: 1.8033
2022-07-06 06:30:04 - train: epoch 0047, iter [00400, 05004], lr: 0.089444, loss: 1.6743
2022-07-06 06:30:38 - train: epoch 0047, iter [00500, 05004], lr: 0.089434, loss: 1.8906
2022-07-06 06:31:11 - train: epoch 0047, iter [00600, 05004], lr: 0.089424, loss: 1.9272
2022-07-06 06:31:44 - train: epoch 0047, iter [00700, 05004], lr: 0.089414, loss: 2.0311
2022-07-06 06:32:18 - train: epoch 0047, iter [00800, 05004], lr: 0.089404, loss: 1.7711
2022-07-06 06:32:50 - train: epoch 0047, iter [00900, 05004], lr: 0.089394, loss: 2.0860
2022-07-06 06:33:25 - train: epoch 0047, iter [01000, 05004], lr: 0.089384, loss: 1.9896
2022-07-06 06:33:58 - train: epoch 0047, iter [01100, 05004], lr: 0.089374, loss: 1.9198
2022-07-06 06:34:32 - train: epoch 0047, iter [01200, 05004], lr: 0.089364, loss: 1.7598
2022-07-06 06:35:05 - train: epoch 0047, iter [01300, 05004], lr: 0.089354, loss: 1.9423
2022-07-06 06:35:40 - train: epoch 0047, iter [01400, 05004], lr: 0.089344, loss: 1.8408
2022-07-06 06:36:13 - train: epoch 0047, iter [01500, 05004], lr: 0.089334, loss: 1.7865
2022-07-06 06:36:46 - train: epoch 0047, iter [01600, 05004], lr: 0.089325, loss: 1.8160
2022-07-06 06:37:20 - train: epoch 0047, iter [01700, 05004], lr: 0.089315, loss: 1.7040
2022-07-06 06:37:55 - train: epoch 0047, iter [01800, 05004], lr: 0.089305, loss: 1.8049
2022-07-06 06:38:28 - train: epoch 0047, iter [01900, 05004], lr: 0.089295, loss: 1.7866
2022-07-06 06:39:01 - train: epoch 0047, iter [02000, 05004], lr: 0.089285, loss: 1.8602
2022-07-06 06:39:33 - train: epoch 0047, iter [02100, 05004], lr: 0.089275, loss: 2.0306
2022-07-06 06:40:07 - train: epoch 0047, iter [02200, 05004], lr: 0.089265, loss: 1.9965
2022-07-06 06:40:40 - train: epoch 0047, iter [02300, 05004], lr: 0.089255, loss: 1.8593
2022-07-06 06:41:14 - train: epoch 0047, iter [02400, 05004], lr: 0.089245, loss: 1.7273
2022-07-06 06:41:48 - train: epoch 0047, iter [02500, 05004], lr: 0.089235, loss: 1.8347
2022-07-06 06:42:22 - train: epoch 0047, iter [02600, 05004], lr: 0.089225, loss: 2.2010
2022-07-06 06:42:55 - train: epoch 0047, iter [02700, 05004], lr: 0.089215, loss: 1.8742
2022-07-06 06:43:30 - train: epoch 0047, iter [02800, 05004], lr: 0.089205, loss: 2.1073
2022-07-06 06:44:03 - train: epoch 0047, iter [02900, 05004], lr: 0.089195, loss: 1.8009
2022-07-06 06:44:37 - train: epoch 0047, iter [03000, 05004], lr: 0.089185, loss: 1.9155
2022-07-06 06:45:10 - train: epoch 0047, iter [03100, 05004], lr: 0.089175, loss: 1.8484
2022-07-06 06:45:45 - train: epoch 0047, iter [03200, 05004], lr: 0.089165, loss: 2.1468
2022-07-06 06:46:18 - train: epoch 0047, iter [03300, 05004], lr: 0.089155, loss: 1.7959
2022-07-06 06:46:52 - train: epoch 0047, iter [03400, 05004], lr: 0.089145, loss: 1.8924
2022-07-06 06:47:26 - train: epoch 0047, iter [03500, 05004], lr: 0.089135, loss: 2.0586
2022-07-06 06:47:59 - train: epoch 0047, iter [03600, 05004], lr: 0.089125, loss: 2.0238
2022-07-06 06:48:33 - train: epoch 0047, iter [03700, 05004], lr: 0.089115, loss: 2.0757
2022-07-06 06:49:07 - train: epoch 0047, iter [03800, 05004], lr: 0.089105, loss: 1.9544
2022-07-06 06:49:40 - train: epoch 0047, iter [03900, 05004], lr: 0.089095, loss: 1.7079
2022-07-06 06:50:14 - train: epoch 0047, iter [04000, 05004], lr: 0.089085, loss: 2.0345
2022-07-06 06:50:48 - train: epoch 0047, iter [04100, 05004], lr: 0.089075, loss: 1.9688
2022-07-06 06:51:22 - train: epoch 0047, iter [04200, 05004], lr: 0.089065, loss: 1.8484
2022-07-06 06:51:55 - train: epoch 0047, iter [04300, 05004], lr: 0.089055, loss: 1.7585
2022-07-06 06:52:30 - train: epoch 0047, iter [04400, 05004], lr: 0.089045, loss: 1.8304
2022-07-06 06:53:03 - train: epoch 0047, iter [04500, 05004], lr: 0.089034, loss: 1.9790
2022-07-06 06:53:35 - train: epoch 0047, iter [04600, 05004], lr: 0.089024, loss: 1.9325
2022-07-06 06:54:09 - train: epoch 0047, iter [04700, 05004], lr: 0.089014, loss: 2.0463
2022-07-06 06:54:43 - train: epoch 0047, iter [04800, 05004], lr: 0.089004, loss: 1.8051
2022-07-06 06:55:16 - train: epoch 0047, iter [04900, 05004], lr: 0.088994, loss: 2.0420
2022-07-06 06:55:49 - train: epoch 0047, iter [05000, 05004], lr: 0.088984, loss: 1.7417
2022-07-06 06:55:50 - train: epoch 047, train_loss: 1.8805
2022-07-06 06:57:04 - eval: epoch: 047, acc1: 60.430%, acc5: 83.824%, test_loss: 1.6542, per_image_load_time: 0.788ms, per_image_inference_time: 0.485ms
2022-07-06 06:57:05 - until epoch: 047, best_acc1: 60.430%
2022-07-06 06:57:05 - epoch 048 lr: 0.088984
2022-07-06 06:57:43 - train: epoch 0048, iter [00100, 05004], lr: 0.088974, loss: 2.0123
2022-07-06 06:58:18 - train: epoch 0048, iter [00200, 05004], lr: 0.088964, loss: 2.1929
2022-07-06 06:58:52 - train: epoch 0048, iter [00300, 05004], lr: 0.088953, loss: 1.8015
2022-07-06 06:59:24 - train: epoch 0048, iter [00400, 05004], lr: 0.088943, loss: 1.8235
2022-07-06 06:59:59 - train: epoch 0048, iter [00500, 05004], lr: 0.088933, loss: 1.7732
2022-07-06 07:00:32 - train: epoch 0048, iter [00600, 05004], lr: 0.088923, loss: 1.8312
2022-07-06 07:01:06 - train: epoch 0048, iter [00700, 05004], lr: 0.088913, loss: 1.8663
2022-07-06 07:01:40 - train: epoch 0048, iter [00800, 05004], lr: 0.088903, loss: 1.9512
2022-07-06 07:02:14 - train: epoch 0048, iter [00900, 05004], lr: 0.088893, loss: 1.8649
2022-07-06 07:02:47 - train: epoch 0048, iter [01000, 05004], lr: 0.088883, loss: 1.8800
2022-07-06 07:03:21 - train: epoch 0048, iter [01100, 05004], lr: 0.088873, loss: 1.9314
2022-07-06 07:03:54 - train: epoch 0048, iter [01200, 05004], lr: 0.088862, loss: 1.9018
2022-07-06 07:04:29 - train: epoch 0048, iter [01300, 05004], lr: 0.088852, loss: 1.7370
2022-07-06 07:05:02 - train: epoch 0048, iter [01400, 05004], lr: 0.088842, loss: 1.7672
2022-07-06 07:05:36 - train: epoch 0048, iter [01500, 05004], lr: 0.088832, loss: 2.0303
2022-07-06 07:06:10 - train: epoch 0048, iter [01600, 05004], lr: 0.088822, loss: 1.7955
2022-07-06 07:06:44 - train: epoch 0048, iter [01700, 05004], lr: 0.088812, loss: 2.0842
2022-07-06 07:07:18 - train: epoch 0048, iter [01800, 05004], lr: 0.088802, loss: 1.8880
2022-07-06 07:07:51 - train: epoch 0048, iter [01900, 05004], lr: 0.088791, loss: 1.9514
2022-07-06 07:08:25 - train: epoch 0048, iter [02000, 05004], lr: 0.088781, loss: 2.0475
2022-07-06 07:08:59 - train: epoch 0048, iter [02100, 05004], lr: 0.088771, loss: 1.8690
2022-07-06 07:09:33 - train: epoch 0048, iter [02200, 05004], lr: 0.088761, loss: 2.1493
2022-07-06 07:10:07 - train: epoch 0048, iter [02300, 05004], lr: 0.088751, loss: 1.6679
2022-07-06 07:10:41 - train: epoch 0048, iter [02400, 05004], lr: 0.088741, loss: 1.9618
2022-07-06 07:11:15 - train: epoch 0048, iter [02500, 05004], lr: 0.088730, loss: 1.9202
2022-07-06 07:11:49 - train: epoch 0048, iter [02600, 05004], lr: 0.088720, loss: 2.0940
2022-07-06 07:12:23 - train: epoch 0048, iter [02700, 05004], lr: 0.088710, loss: 2.0599
2022-07-06 07:12:57 - train: epoch 0048, iter [02800, 05004], lr: 0.088700, loss: 1.8604
2022-07-06 07:13:31 - train: epoch 0048, iter [02900, 05004], lr: 0.088690, loss: 2.0050
2022-07-06 07:14:04 - train: epoch 0048, iter [03000, 05004], lr: 0.088679, loss: 1.9042
2022-07-06 07:14:38 - train: epoch 0048, iter [03100, 05004], lr: 0.088669, loss: 1.8969
2022-07-06 07:15:12 - train: epoch 0048, iter [03200, 05004], lr: 0.088659, loss: 1.7301
2022-07-06 07:15:46 - train: epoch 0048, iter [03300, 05004], lr: 0.088649, loss: 1.9706
2022-07-06 07:16:19 - train: epoch 0048, iter [03400, 05004], lr: 0.088639, loss: 1.9191
2022-07-06 07:16:53 - train: epoch 0048, iter [03500, 05004], lr: 0.088628, loss: 2.0700
2022-07-06 07:17:28 - train: epoch 0048, iter [03600, 05004], lr: 0.088618, loss: 1.8937
2022-07-06 07:18:01 - train: epoch 0048, iter [03700, 05004], lr: 0.088608, loss: 2.0044
2022-07-06 07:18:35 - train: epoch 0048, iter [03800, 05004], lr: 0.088598, loss: 1.8030
2022-07-06 07:19:09 - train: epoch 0048, iter [03900, 05004], lr: 0.088588, loss: 1.9380
2022-07-06 07:19:43 - train: epoch 0048, iter [04000, 05004], lr: 0.088577, loss: 1.7284
2022-07-06 07:20:18 - train: epoch 0048, iter [04100, 05004], lr: 0.088567, loss: 2.1525
2022-07-06 07:20:52 - train: epoch 0048, iter [04200, 05004], lr: 0.088557, loss: 1.8052
2022-07-06 07:21:24 - train: epoch 0048, iter [04300, 05004], lr: 0.088547, loss: 1.9307
2022-07-06 07:21:59 - train: epoch 0048, iter [04400, 05004], lr: 0.088536, loss: 1.7393
2022-07-06 07:22:34 - train: epoch 0048, iter [04500, 05004], lr: 0.088526, loss: 1.9169
2022-07-06 07:23:07 - train: epoch 0048, iter [04600, 05004], lr: 0.088516, loss: 1.8430
2022-07-06 07:23:42 - train: epoch 0048, iter [04700, 05004], lr: 0.088506, loss: 1.8760
2022-07-06 07:24:15 - train: epoch 0048, iter [04800, 05004], lr: 0.088495, loss: 2.2433
2022-07-06 07:24:50 - train: epoch 0048, iter [04900, 05004], lr: 0.088485, loss: 1.9705
2022-07-06 07:25:22 - train: epoch 0048, iter [05000, 05004], lr: 0.088475, loss: 1.7851
2022-07-06 07:25:23 - train: epoch 048, train_loss: 1.8784
2022-07-06 07:26:38 - eval: epoch: 048, acc1: 56.690%, acc5: 80.948%, test_loss: 1.8427, per_image_load_time: 1.147ms, per_image_inference_time: 0.480ms
2022-07-06 07:26:39 - until epoch: 048, best_acc1: 60.430%
2022-07-06 07:26:39 - epoch 049 lr: 0.088474
2022-07-06 07:27:18 - train: epoch 0049, iter [00100, 05004], lr: 0.088464, loss: 1.9884
2022-07-06 07:27:51 - train: epoch 0049, iter [00200, 05004], lr: 0.088454, loss: 1.8578
2022-07-06 07:28:26 - train: epoch 0049, iter [00300, 05004], lr: 0.088443, loss: 1.8564
2022-07-06 07:28:59 - train: epoch 0049, iter [00400, 05004], lr: 0.088433, loss: 1.9963
2022-07-06 07:29:33 - train: epoch 0049, iter [00500, 05004], lr: 0.088423, loss: 1.9135
2022-07-06 07:30:07 - train: epoch 0049, iter [00600, 05004], lr: 0.088413, loss: 1.9025
2022-07-06 07:30:40 - train: epoch 0049, iter [00700, 05004], lr: 0.088402, loss: 1.9298
2022-07-06 07:31:14 - train: epoch 0049, iter [00800, 05004], lr: 0.088392, loss: 2.2006
2022-07-06 07:31:48 - train: epoch 0049, iter [00900, 05004], lr: 0.088382, loss: 1.8030
2022-07-06 07:32:21 - train: epoch 0049, iter [01000, 05004], lr: 0.088371, loss: 1.8845
2022-07-06 07:32:55 - train: epoch 0049, iter [01100, 05004], lr: 0.088361, loss: 1.6034
2022-07-06 07:33:29 - train: epoch 0049, iter [01200, 05004], lr: 0.088351, loss: 1.5513
2022-07-06 07:34:03 - train: epoch 0049, iter [01300, 05004], lr: 0.088340, loss: 2.0324
2022-07-06 07:34:36 - train: epoch 0049, iter [01400, 05004], lr: 0.088330, loss: 1.9940
2022-07-06 07:35:10 - train: epoch 0049, iter [01500, 05004], lr: 0.088320, loss: 1.7697
2022-07-06 07:35:45 - train: epoch 0049, iter [01600, 05004], lr: 0.088309, loss: 2.0821
2022-07-06 07:36:19 - train: epoch 0049, iter [01700, 05004], lr: 0.088299, loss: 1.8611
2022-07-06 07:36:52 - train: epoch 0049, iter [01800, 05004], lr: 0.088289, loss: 1.7684
2022-07-06 07:37:25 - train: epoch 0049, iter [01900, 05004], lr: 0.088278, loss: 1.9071
2022-07-06 07:38:00 - train: epoch 0049, iter [02000, 05004], lr: 0.088268, loss: 1.6475
2022-07-06 07:38:34 - train: epoch 0049, iter [02100, 05004], lr: 0.088257, loss: 1.8166
2022-07-06 07:39:09 - train: epoch 0049, iter [02200, 05004], lr: 0.088247, loss: 1.8890
2022-07-06 07:39:42 - train: epoch 0049, iter [02300, 05004], lr: 0.088237, loss: 1.7704
2022-07-06 07:40:17 - train: epoch 0049, iter [02400, 05004], lr: 0.088226, loss: 2.1292
2022-07-06 07:40:51 - train: epoch 0049, iter [02500, 05004], lr: 0.088216, loss: 1.9404
