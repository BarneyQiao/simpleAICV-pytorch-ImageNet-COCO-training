2022-07-06 07:41:24 - train: epoch 0049, iter [02600, 05004], lr: 0.088206, loss: 2.0554
2022-07-06 07:41:59 - train: epoch 0049, iter [02700, 05004], lr: 0.088195, loss: 1.7987
2022-07-06 07:42:32 - train: epoch 0049, iter [02800, 05004], lr: 0.088185, loss: 1.7892
2022-07-06 07:43:06 - train: epoch 0049, iter [02900, 05004], lr: 0.088174, loss: 1.9344
2022-07-06 07:43:40 - train: epoch 0049, iter [03000, 05004], lr: 0.088164, loss: 2.0050
2022-07-06 07:44:14 - train: epoch 0049, iter [03100, 05004], lr: 0.088154, loss: 2.0330
2022-07-06 07:44:48 - train: epoch 0049, iter [03200, 05004], lr: 0.088143, loss: 2.0126
2022-07-06 07:45:23 - train: epoch 0049, iter [03300, 05004], lr: 0.088133, loss: 1.8893
2022-07-06 07:45:58 - train: epoch 0049, iter [03400, 05004], lr: 0.088122, loss: 1.9655
2022-07-06 07:46:31 - train: epoch 0049, iter [03500, 05004], lr: 0.088112, loss: 2.0079
2022-07-06 07:47:06 - train: epoch 0049, iter [03600, 05004], lr: 0.088102, loss: 1.9579
2022-07-06 07:47:40 - train: epoch 0049, iter [03700, 05004], lr: 0.088091, loss: 1.8386
2022-07-06 07:48:14 - train: epoch 0049, iter [03800, 05004], lr: 0.088081, loss: 2.2234
2022-07-06 07:48:48 - train: epoch 0049, iter [03900, 05004], lr: 0.088070, loss: 2.1924
2022-07-06 07:49:22 - train: epoch 0049, iter [04000, 05004], lr: 0.088060, loss: 1.8443
2022-07-06 07:49:56 - train: epoch 0049, iter [04100, 05004], lr: 0.088049, loss: 1.7014
2022-07-06 07:50:31 - train: epoch 0049, iter [04200, 05004], lr: 0.088039, loss: 1.9391
2022-07-06 07:51:04 - train: epoch 0049, iter [04300, 05004], lr: 0.088029, loss: 2.2048
2022-07-06 07:51:38 - train: epoch 0049, iter [04400, 05004], lr: 0.088018, loss: 1.8693
2022-07-06 07:52:12 - train: epoch 0049, iter [04500, 05004], lr: 0.088008, loss: 1.8351
2022-07-06 07:52:47 - train: epoch 0049, iter [04600, 05004], lr: 0.087997, loss: 1.9770
2022-07-06 07:53:21 - train: epoch 0049, iter [04700, 05004], lr: 0.087987, loss: 2.0163
2022-07-06 07:53:55 - train: epoch 0049, iter [04800, 05004], lr: 0.087976, loss: 1.7173
2022-07-06 07:54:29 - train: epoch 0049, iter [04900, 05004], lr: 0.087966, loss: 1.7486
2022-07-06 07:55:01 - train: epoch 0049, iter [05000, 05004], lr: 0.087955, loss: 1.7039
2022-07-06 07:55:03 - train: epoch 049, train_loss: 1.8704
2022-07-06 07:56:17 - eval: epoch: 049, acc1: 57.774%, acc5: 81.964%, test_loss: 1.7894, per_image_load_time: 1.850ms, per_image_inference_time: 0.472ms
2022-07-06 07:56:18 - until epoch: 049, best_acc1: 60.430%
2022-07-06 07:56:18 - epoch 050 lr: 0.087955
2022-07-06 07:56:56 - train: epoch 0050, iter [00100, 05004], lr: 0.087944, loss: 1.9743
2022-07-06 07:57:30 - train: epoch 0050, iter [00200, 05004], lr: 0.087934, loss: 1.8866
2022-07-06 07:58:05 - train: epoch 0050, iter [00300, 05004], lr: 0.087923, loss: 1.8733
2022-07-06 07:58:38 - train: epoch 0050, iter [00400, 05004], lr: 0.087913, loss: 1.6665
2022-07-06 07:59:13 - train: epoch 0050, iter [00500, 05004], lr: 0.087902, loss: 1.9010
2022-07-06 07:59:46 - train: epoch 0050, iter [00600, 05004], lr: 0.087892, loss: 1.9579
2022-07-06 08:00:21 - train: epoch 0050, iter [00700, 05004], lr: 0.087881, loss: 1.7668
2022-07-06 08:00:54 - train: epoch 0050, iter [00800, 05004], lr: 0.087871, loss: 1.5425
2022-07-06 08:01:28 - train: epoch 0050, iter [00900, 05004], lr: 0.087860, loss: 1.7273
2022-07-06 08:02:01 - train: epoch 0050, iter [01000, 05004], lr: 0.087850, loss: 1.9807
2022-07-06 08:02:36 - train: epoch 0050, iter [01100, 05004], lr: 0.087839, loss: 1.9425
2022-07-06 08:03:10 - train: epoch 0050, iter [01200, 05004], lr: 0.087829, loss: 1.8571
2022-07-06 08:03:44 - train: epoch 0050, iter [01300, 05004], lr: 0.087818, loss: 1.6854
2022-07-06 08:04:17 - train: epoch 0050, iter [01400, 05004], lr: 0.087808, loss: 1.8844
2022-07-06 08:04:51 - train: epoch 0050, iter [01500, 05004], lr: 0.087797, loss: 1.8772
2022-07-06 08:05:25 - train: epoch 0050, iter [01600, 05004], lr: 0.087787, loss: 1.8189
2022-07-06 08:05:59 - train: epoch 0050, iter [01700, 05004], lr: 0.087776, loss: 2.0488
2022-07-06 08:06:32 - train: epoch 0050, iter [01800, 05004], lr: 0.087766, loss: 2.0178
2022-07-06 08:07:06 - train: epoch 0050, iter [01900, 05004], lr: 0.087755, loss: 1.8297
2022-07-06 08:07:40 - train: epoch 0050, iter [02000, 05004], lr: 0.087744, loss: 1.8606
2022-07-06 08:08:13 - train: epoch 0050, iter [02100, 05004], lr: 0.087734, loss: 1.5152
2022-07-06 08:08:47 - train: epoch 0050, iter [02200, 05004], lr: 0.087723, loss: 1.9780
2022-07-06 08:09:21 - train: epoch 0050, iter [02300, 05004], lr: 0.087713, loss: 1.8385
2022-07-06 08:09:55 - train: epoch 0050, iter [02400, 05004], lr: 0.087702, loss: 1.8082
2022-07-06 08:10:28 - train: epoch 0050, iter [02500, 05004], lr: 0.087692, loss: 1.9610
2022-07-06 08:11:02 - train: epoch 0050, iter [02600, 05004], lr: 0.087681, loss: 1.7185
2022-07-06 08:11:35 - train: epoch 0050, iter [02700, 05004], lr: 0.087670, loss: 1.9046
2022-07-06 08:12:09 - train: epoch 0050, iter [02800, 05004], lr: 0.087660, loss: 2.1085
2022-07-06 08:12:43 - train: epoch 0050, iter [02900, 05004], lr: 0.087649, loss: 2.1665
2022-07-06 08:13:17 - train: epoch 0050, iter [03000, 05004], lr: 0.087639, loss: 2.1007
2022-07-06 08:13:50 - train: epoch 0050, iter [03100, 05004], lr: 0.087628, loss: 1.8255
2022-07-06 08:14:24 - train: epoch 0050, iter [03200, 05004], lr: 0.087617, loss: 1.7797
2022-07-06 08:14:58 - train: epoch 0050, iter [03300, 05004], lr: 0.087607, loss: 1.7638
2022-07-06 08:15:32 - train: epoch 0050, iter [03400, 05004], lr: 0.087596, loss: 1.7846
2022-07-06 08:16:06 - train: epoch 0050, iter [03500, 05004], lr: 0.087586, loss: 1.8006
2022-07-06 08:16:40 - train: epoch 0050, iter [03600, 05004], lr: 0.087575, loss: 1.9984
2022-07-06 08:17:13 - train: epoch 0050, iter [03700, 05004], lr: 0.087564, loss: 1.9294
2022-07-06 08:17:47 - train: epoch 0050, iter [03800, 05004], lr: 0.087554, loss: 1.7407
2022-07-06 08:18:21 - train: epoch 0050, iter [03900, 05004], lr: 0.087543, loss: 1.6741
2022-07-06 08:18:53 - train: epoch 0050, iter [04000, 05004], lr: 0.087533, loss: 1.9664
2022-07-06 08:19:28 - train: epoch 0050, iter [04100, 05004], lr: 0.087522, loss: 1.8686
2022-07-06 08:20:00 - train: epoch 0050, iter [04200, 05004], lr: 0.087511, loss: 1.9390
2022-07-06 08:20:34 - train: epoch 0050, iter [04300, 05004], lr: 0.087501, loss: 1.8494
2022-07-06 08:21:07 - train: epoch 0050, iter [04400, 05004], lr: 0.087490, loss: 1.7640
2022-07-06 08:21:42 - train: epoch 0050, iter [04500, 05004], lr: 0.087479, loss: 1.9638
2022-07-06 08:22:15 - train: epoch 0050, iter [04600, 05004], lr: 0.087469, loss: 1.8563
2022-07-06 08:22:49 - train: epoch 0050, iter [04700, 05004], lr: 0.087458, loss: 1.9163
2022-07-06 08:23:23 - train: epoch 0050, iter [04800, 05004], lr: 0.087447, loss: 1.6647
2022-07-06 08:23:56 - train: epoch 0050, iter [04900, 05004], lr: 0.087437, loss: 1.6978
2022-07-06 08:24:29 - train: epoch 0050, iter [05000, 05004], lr: 0.087426, loss: 1.8984
2022-07-06 08:24:30 - train: epoch 050, train_loss: 1.8634
2022-07-06 08:25:44 - eval: epoch: 050, acc1: 58.804%, acc5: 82.604%, test_loss: 1.7463, per_image_load_time: 1.883ms, per_image_inference_time: 0.467ms
2022-07-06 08:25:44 - until epoch: 050, best_acc1: 60.430%
2022-07-06 08:25:44 - epoch 051 lr: 0.087425
2022-07-06 08:26:24 - train: epoch 0051, iter [00100, 05004], lr: 0.087415, loss: 2.0583
2022-07-06 08:26:58 - train: epoch 0051, iter [00200, 05004], lr: 0.087404, loss: 2.3223
2022-07-06 08:27:31 - train: epoch 0051, iter [00300, 05004], lr: 0.087393, loss: 1.8153
2022-07-06 08:28:05 - train: epoch 0051, iter [00400, 05004], lr: 0.087383, loss: 1.7073
2022-07-06 08:28:40 - train: epoch 0051, iter [00500, 05004], lr: 0.087372, loss: 1.8442
2022-07-06 08:29:12 - train: epoch 0051, iter [00600, 05004], lr: 0.087361, loss: 1.7749
2022-07-06 08:29:46 - train: epoch 0051, iter [00700, 05004], lr: 0.087351, loss: 1.8738
2022-07-06 08:30:18 - train: epoch 0051, iter [00800, 05004], lr: 0.087340, loss: 2.0637
2022-07-06 08:30:52 - train: epoch 0051, iter [00900, 05004], lr: 0.087329, loss: 1.7655
2022-07-06 08:31:25 - train: epoch 0051, iter [01000, 05004], lr: 0.087319, loss: 2.1586
2022-07-06 08:31:59 - train: epoch 0051, iter [01100, 05004], lr: 0.087308, loss: 2.0276
2022-07-06 08:32:32 - train: epoch 0051, iter [01200, 05004], lr: 0.087297, loss: 1.8298
2022-07-06 08:33:06 - train: epoch 0051, iter [01300, 05004], lr: 0.087286, loss: 1.5487
2022-07-06 08:33:39 - train: epoch 0051, iter [01400, 05004], lr: 0.087276, loss: 1.7022
2022-07-06 08:34:14 - train: epoch 0051, iter [01500, 05004], lr: 0.087265, loss: 1.7996
2022-07-06 08:34:47 - train: epoch 0051, iter [01600, 05004], lr: 0.087254, loss: 1.7414
2022-07-06 08:35:20 - train: epoch 0051, iter [01700, 05004], lr: 0.087244, loss: 1.8878
2022-07-06 08:35:54 - train: epoch 0051, iter [01800, 05004], lr: 0.087233, loss: 1.8378
2022-07-06 08:36:28 - train: epoch 0051, iter [01900, 05004], lr: 0.087222, loss: 1.6340
2022-07-06 08:37:01 - train: epoch 0051, iter [02000, 05004], lr: 0.087211, loss: 1.8770
2022-07-06 08:37:35 - train: epoch 0051, iter [02100, 05004], lr: 0.087201, loss: 1.7784
2022-07-06 08:38:08 - train: epoch 0051, iter [02200, 05004], lr: 0.087190, loss: 1.9210
2022-07-06 08:38:42 - train: epoch 0051, iter [02300, 05004], lr: 0.087179, loss: 2.0460
2022-07-06 08:39:15 - train: epoch 0051, iter [02400, 05004], lr: 0.087168, loss: 1.9989
2022-07-06 08:39:49 - train: epoch 0051, iter [02500, 05004], lr: 0.087157, loss: 1.7776
2022-07-06 08:40:23 - train: epoch 0051, iter [02600, 05004], lr: 0.087147, loss: 1.7550
2022-07-06 08:40:57 - train: epoch 0051, iter [02700, 05004], lr: 0.087136, loss: 1.9633
2022-07-06 08:41:31 - train: epoch 0051, iter [02800, 05004], lr: 0.087125, loss: 1.6703
2022-07-06 08:42:05 - train: epoch 0051, iter [02900, 05004], lr: 0.087114, loss: 1.7638
2022-07-06 08:42:38 - train: epoch 0051, iter [03000, 05004], lr: 0.087104, loss: 1.8107
2022-07-06 08:43:12 - train: epoch 0051, iter [03100, 05004], lr: 0.087093, loss: 1.8485
2022-07-06 08:43:46 - train: epoch 0051, iter [03200, 05004], lr: 0.087082, loss: 1.8757
2022-07-06 08:44:19 - train: epoch 0051, iter [03300, 05004], lr: 0.087071, loss: 2.0236
2022-07-06 08:44:53 - train: epoch 0051, iter [03400, 05004], lr: 0.087060, loss: 1.8816
2022-07-06 08:45:27 - train: epoch 0051, iter [03500, 05004], lr: 0.087050, loss: 1.8311
2022-07-06 08:46:01 - train: epoch 0051, iter [03600, 05004], lr: 0.087039, loss: 1.9060
2022-07-06 08:46:34 - train: epoch 0051, iter [03700, 05004], lr: 0.087028, loss: 2.0363
2022-07-06 08:47:08 - train: epoch 0051, iter [03800, 05004], lr: 0.087017, loss: 1.8895
2022-07-06 08:47:41 - train: epoch 0051, iter [03900, 05004], lr: 0.087006, loss: 1.9932
2022-07-06 08:48:15 - train: epoch 0051, iter [04000, 05004], lr: 0.086995, loss: 1.9428
2022-07-06 08:48:50 - train: epoch 0051, iter [04100, 05004], lr: 0.086985, loss: 1.9445
2022-07-06 08:49:23 - train: epoch 0051, iter [04200, 05004], lr: 0.086974, loss: 2.0537
2022-07-06 08:49:56 - train: epoch 0051, iter [04300, 05004], lr: 0.086963, loss: 1.6757
2022-07-06 08:50:30 - train: epoch 0051, iter [04400, 05004], lr: 0.086952, loss: 2.0091
2022-07-06 08:51:04 - train: epoch 0051, iter [04500, 05004], lr: 0.086941, loss: 1.7562
2022-07-06 08:51:38 - train: epoch 0051, iter [04600, 05004], lr: 0.086930, loss: 1.9372
2022-07-06 08:52:11 - train: epoch 0051, iter [04700, 05004], lr: 0.086920, loss: 1.8717
2022-07-06 08:52:44 - train: epoch 0051, iter [04800, 05004], lr: 0.086909, loss: 1.8981
2022-07-06 08:53:18 - train: epoch 0051, iter [04900, 05004], lr: 0.086898, loss: 2.0068
2022-07-06 08:53:50 - train: epoch 0051, iter [05000, 05004], lr: 0.086887, loss: 1.7036
2022-07-06 08:53:51 - train: epoch 051, train_loss: 1.8607
2022-07-06 08:55:06 - eval: epoch: 051, acc1: 59.726%, acc5: 83.146%, test_loss: 1.6822, per_image_load_time: 2.191ms, per_image_inference_time: 0.483ms
2022-07-06 08:55:06 - until epoch: 051, best_acc1: 60.430%
2022-07-06 08:55:06 - epoch 052 lr: 0.086886
2022-07-06 08:55:45 - train: epoch 0052, iter [00100, 05004], lr: 0.086876, loss: 1.7351
2022-07-06 08:56:19 - train: epoch 0052, iter [00200, 05004], lr: 0.086865, loss: 1.8605
2022-07-06 08:56:52 - train: epoch 0052, iter [00300, 05004], lr: 0.086854, loss: 1.8550
2022-07-06 08:57:25 - train: epoch 0052, iter [00400, 05004], lr: 0.086843, loss: 1.9343
2022-07-06 08:57:59 - train: epoch 0052, iter [00500, 05004], lr: 0.086832, loss: 1.8985
2022-07-06 08:58:33 - train: epoch 0052, iter [00600, 05004], lr: 0.086821, loss: 1.7157
2022-07-06 08:59:07 - train: epoch 0052, iter [00700, 05004], lr: 0.086810, loss: 1.8280
2022-07-06 08:59:40 - train: epoch 0052, iter [00800, 05004], lr: 0.086799, loss: 1.9368
2022-07-06 09:00:14 - train: epoch 0052, iter [00900, 05004], lr: 0.086789, loss: 1.9534
2022-07-06 09:00:48 - train: epoch 0052, iter [01000, 05004], lr: 0.086778, loss: 2.0787
2022-07-06 09:01:21 - train: epoch 0052, iter [01100, 05004], lr: 0.086767, loss: 1.7900
2022-07-06 09:01:55 - train: epoch 0052, iter [01200, 05004], lr: 0.086756, loss: 1.6894
2022-07-06 09:02:28 - train: epoch 0052, iter [01300, 05004], lr: 0.086745, loss: 1.7193
2022-07-06 09:03:02 - train: epoch 0052, iter [01400, 05004], lr: 0.086734, loss: 2.1608
2022-07-06 09:03:35 - train: epoch 0052, iter [01500, 05004], lr: 0.086723, loss: 1.6855
2022-07-06 09:04:10 - train: epoch 0052, iter [01600, 05004], lr: 0.086712, loss: 1.6987
2022-07-06 09:04:44 - train: epoch 0052, iter [01700, 05004], lr: 0.086701, loss: 1.6091
2022-07-06 09:05:17 - train: epoch 0052, iter [01800, 05004], lr: 0.086690, loss: 1.7305
2022-07-06 09:05:50 - train: epoch 0052, iter [01900, 05004], lr: 0.086679, loss: 1.8220
2022-07-06 09:06:24 - train: epoch 0052, iter [02000, 05004], lr: 0.086668, loss: 2.1288
2022-07-06 09:06:57 - train: epoch 0052, iter [02100, 05004], lr: 0.086657, loss: 1.7811
2022-07-06 09:07:32 - train: epoch 0052, iter [02200, 05004], lr: 0.086647, loss: 1.9598
2022-07-06 09:08:05 - train: epoch 0052, iter [02300, 05004], lr: 0.086636, loss: 1.6850
2022-07-06 09:08:40 - train: epoch 0052, iter [02400, 05004], lr: 0.086625, loss: 1.6406
2022-07-06 09:09:13 - train: epoch 0052, iter [02500, 05004], lr: 0.086614, loss: 1.7269
2022-07-06 09:09:47 - train: epoch 0052, iter [02600, 05004], lr: 0.086603, loss: 1.5885
2022-07-06 09:10:20 - train: epoch 0052, iter [02700, 05004], lr: 0.086592, loss: 1.8467
2022-07-06 09:10:54 - train: epoch 0052, iter [02800, 05004], lr: 0.086581, loss: 1.8914
2022-07-06 09:11:28 - train: epoch 0052, iter [02900, 05004], lr: 0.086570, loss: 1.6332
2022-07-06 09:12:02 - train: epoch 0052, iter [03000, 05004], lr: 0.086559, loss: 1.8456
2022-07-06 09:12:36 - train: epoch 0052, iter [03100, 05004], lr: 0.086548, loss: 1.9786
2022-07-06 09:13:10 - train: epoch 0052, iter [03200, 05004], lr: 0.086537, loss: 1.9840
2022-07-06 09:13:43 - train: epoch 0052, iter [03300, 05004], lr: 0.086526, loss: 1.9323
2022-07-06 09:14:17 - train: epoch 0052, iter [03400, 05004], lr: 0.086515, loss: 1.9871
2022-07-06 09:14:52 - train: epoch 0052, iter [03500, 05004], lr: 0.086504, loss: 1.9966
2022-07-06 09:15:25 - train: epoch 0052, iter [03600, 05004], lr: 0.086493, loss: 1.9761
2022-07-06 09:16:00 - train: epoch 0052, iter [03700, 05004], lr: 0.086482, loss: 2.0777
2022-07-06 09:16:34 - train: epoch 0052, iter [03800, 05004], lr: 0.086471, loss: 1.8476
2022-07-06 09:17:07 - train: epoch 0052, iter [03900, 05004], lr: 0.086460, loss: 1.7686
2022-07-06 09:17:41 - train: epoch 0052, iter [04000, 05004], lr: 0.086449, loss: 2.0804
2022-07-06 09:18:15 - train: epoch 0052, iter [04100, 05004], lr: 0.086438, loss: 1.6516
2022-07-06 09:18:49 - train: epoch 0052, iter [04200, 05004], lr: 0.086427, loss: 1.7316
2022-07-06 09:19:23 - train: epoch 0052, iter [04300, 05004], lr: 0.086416, loss: 1.9377
2022-07-06 09:19:57 - train: epoch 0052, iter [04400, 05004], lr: 0.086405, loss: 1.9345
2022-07-06 09:20:31 - train: epoch 0052, iter [04500, 05004], lr: 0.086394, loss: 1.7588
2022-07-06 09:21:05 - train: epoch 0052, iter [04600, 05004], lr: 0.086383, loss: 1.9864
2022-07-06 09:21:39 - train: epoch 0052, iter [04700, 05004], lr: 0.086372, loss: 1.9084
2022-07-06 09:22:13 - train: epoch 0052, iter [04800, 05004], lr: 0.086361, loss: 1.5953
2022-07-06 09:22:46 - train: epoch 0052, iter [04900, 05004], lr: 0.086349, loss: 1.8105
2022-07-06 09:23:19 - train: epoch 0052, iter [05000, 05004], lr: 0.086338, loss: 1.7462
2022-07-06 09:23:20 - train: epoch 052, train_loss: 1.8553
2022-07-06 09:24:34 - eval: epoch: 052, acc1: 59.528%, acc5: 83.528%, test_loss: 1.6805, per_image_load_time: 1.186ms, per_image_inference_time: 0.474ms
2022-07-06 09:24:35 - until epoch: 052, best_acc1: 60.430%
2022-07-06 09:24:35 - epoch 053 lr: 0.086338
2022-07-06 09:25:13 - train: epoch 0053, iter [00100, 05004], lr: 0.086327, loss: 1.8322
2022-07-06 09:25:47 - train: epoch 0053, iter [00200, 05004], lr: 0.086316, loss: 2.0343
2022-07-06 09:26:21 - train: epoch 0053, iter [00300, 05004], lr: 0.086305, loss: 1.8668
2022-07-06 09:26:54 - train: epoch 0053, iter [00400, 05004], lr: 0.086294, loss: 1.9446
2022-07-06 09:27:28 - train: epoch 0053, iter [00500, 05004], lr: 0.086283, loss: 2.0076
2022-07-06 09:28:02 - train: epoch 0053, iter [00600, 05004], lr: 0.086272, loss: 1.8816
2022-07-06 09:28:35 - train: epoch 0053, iter [00700, 05004], lr: 0.086260, loss: 1.6757
2022-07-06 09:29:08 - train: epoch 0053, iter [00800, 05004], lr: 0.086249, loss: 1.9360
2022-07-06 09:29:41 - train: epoch 0053, iter [00900, 05004], lr: 0.086238, loss: 1.6969
2022-07-06 09:30:14 - train: epoch 0053, iter [01000, 05004], lr: 0.086227, loss: 1.7455
2022-07-06 09:30:48 - train: epoch 0053, iter [01100, 05004], lr: 0.086216, loss: 1.8440
2022-07-06 09:31:21 - train: epoch 0053, iter [01200, 05004], lr: 0.086205, loss: 1.8162
2022-07-06 09:31:53 - train: epoch 0053, iter [01300, 05004], lr: 0.086194, loss: 1.7870
2022-07-06 09:32:27 - train: epoch 0053, iter [01400, 05004], lr: 0.086183, loss: 2.1426
2022-07-06 09:33:00 - train: epoch 0053, iter [01500, 05004], lr: 0.086172, loss: 1.7629
2022-07-06 09:33:34 - train: epoch 0053, iter [01600, 05004], lr: 0.086161, loss: 2.0890
2022-07-06 09:34:07 - train: epoch 0053, iter [01700, 05004], lr: 0.086149, loss: 1.9899
2022-07-06 09:34:40 - train: epoch 0053, iter [01800, 05004], lr: 0.086138, loss: 2.0177
2022-07-06 09:35:14 - train: epoch 0053, iter [01900, 05004], lr: 0.086127, loss: 1.6911
2022-07-06 09:35:48 - train: epoch 0053, iter [02000, 05004], lr: 0.086116, loss: 1.8580
2022-07-06 09:36:20 - train: epoch 0053, iter [02100, 05004], lr: 0.086105, loss: 1.9568
2022-07-06 09:36:54 - train: epoch 0053, iter [02200, 05004], lr: 0.086094, loss: 1.8170
2022-07-06 09:37:28 - train: epoch 0053, iter [02300, 05004], lr: 0.086083, loss: 1.8435
2022-07-06 09:38:01 - train: epoch 0053, iter [02400, 05004], lr: 0.086071, loss: 1.9522
2022-07-06 09:38:35 - train: epoch 0053, iter [02500, 05004], lr: 0.086060, loss: 2.0830
2022-07-06 09:39:07 - train: epoch 0053, iter [02600, 05004], lr: 0.086049, loss: 1.9015
2022-07-06 09:39:41 - train: epoch 0053, iter [02700, 05004], lr: 0.086038, loss: 2.1368
2022-07-06 09:40:15 - train: epoch 0053, iter [02800, 05004], lr: 0.086027, loss: 1.9843
2022-07-06 09:40:48 - train: epoch 0053, iter [02900, 05004], lr: 0.086016, loss: 1.6500
2022-07-06 09:41:21 - train: epoch 0053, iter [03000, 05004], lr: 0.086005, loss: 1.6277
2022-07-06 09:41:55 - train: epoch 0053, iter [03100, 05004], lr: 0.085993, loss: 2.0341
2022-07-06 09:42:28 - train: epoch 0053, iter [03200, 05004], lr: 0.085982, loss: 2.0699
2022-07-06 09:43:03 - train: epoch 0053, iter [03300, 05004], lr: 0.085971, loss: 1.7572
2022-07-06 09:43:35 - train: epoch 0053, iter [03400, 05004], lr: 0.085960, loss: 2.0273
2022-07-06 09:44:09 - train: epoch 0053, iter [03500, 05004], lr: 0.085949, loss: 1.8529
2022-07-06 09:44:43 - train: epoch 0053, iter [03600, 05004], lr: 0.085937, loss: 1.9052
2022-07-06 09:45:16 - train: epoch 0053, iter [03700, 05004], lr: 0.085926, loss: 2.1022
2022-07-06 09:45:50 - train: epoch 0053, iter [03800, 05004], lr: 0.085915, loss: 1.7750
2022-07-06 09:46:24 - train: epoch 0053, iter [03900, 05004], lr: 0.085904, loss: 2.0700
2022-07-06 09:46:57 - train: epoch 0053, iter [04000, 05004], lr: 0.085893, loss: 1.7779
2022-07-06 09:47:31 - train: epoch 0053, iter [04100, 05004], lr: 0.085881, loss: 1.8606
2022-07-06 09:48:04 - train: epoch 0053, iter [04200, 05004], lr: 0.085870, loss: 1.8542
2022-07-06 09:48:38 - train: epoch 0053, iter [04300, 05004], lr: 0.085859, loss: 2.0917
2022-07-06 09:49:12 - train: epoch 0053, iter [04400, 05004], lr: 0.085848, loss: 1.8400
2022-07-06 09:49:45 - train: epoch 0053, iter [04500, 05004], lr: 0.085837, loss: 1.9600
2022-07-06 09:50:19 - train: epoch 0053, iter [04600, 05004], lr: 0.085825, loss: 1.7247
2022-07-06 09:50:52 - train: epoch 0053, iter [04700, 05004], lr: 0.085814, loss: 2.0565
2022-07-06 09:51:26 - train: epoch 0053, iter [04800, 05004], lr: 0.085803, loss: 2.0486
2022-07-06 09:52:00 - train: epoch 0053, iter [04900, 05004], lr: 0.085792, loss: 1.7484
2022-07-06 09:52:31 - train: epoch 0053, iter [05000, 05004], lr: 0.085780, loss: 1.7980
2022-07-06 09:52:33 - train: epoch 053, train_loss: 1.8503
2022-07-06 09:53:47 - eval: epoch: 053, acc1: 56.830%, acc5: 81.028%, test_loss: 1.8408, per_image_load_time: 1.005ms, per_image_inference_time: 0.454ms
2022-07-06 09:53:47 - until epoch: 053, best_acc1: 60.430%
2022-07-06 09:53:47 - epoch 054 lr: 0.085780
2022-07-06 09:54:26 - train: epoch 0054, iter [00100, 05004], lr: 0.085769, loss: 1.6649
2022-07-06 09:54:59 - train: epoch 0054, iter [00200, 05004], lr: 0.085757, loss: 2.1745
2022-07-06 09:55:32 - train: epoch 0054, iter [00300, 05004], lr: 0.085746, loss: 1.8517
2022-07-06 09:56:05 - train: epoch 0054, iter [00400, 05004], lr: 0.085735, loss: 1.7144
2022-07-06 09:56:38 - train: epoch 0054, iter [00500, 05004], lr: 0.085724, loss: 1.7524
2022-07-06 09:57:10 - train: epoch 0054, iter [00600, 05004], lr: 0.085712, loss: 1.6399
2022-07-06 09:57:43 - train: epoch 0054, iter [00700, 05004], lr: 0.085701, loss: 1.9280
2022-07-06 09:58:16 - train: epoch 0054, iter [00800, 05004], lr: 0.085690, loss: 1.8566
2022-07-06 09:58:50 - train: epoch 0054, iter [00900, 05004], lr: 0.085679, loss: 1.6193
2022-07-06 09:59:23 - train: epoch 0054, iter [01000, 05004], lr: 0.085667, loss: 1.5545
2022-07-06 09:59:56 - train: epoch 0054, iter [01100, 05004], lr: 0.085656, loss: 1.7831
2022-07-06 10:00:29 - train: epoch 0054, iter [01200, 05004], lr: 0.085645, loss: 1.9695
2022-07-06 10:01:02 - train: epoch 0054, iter [01300, 05004], lr: 0.085633, loss: 1.7506
2022-07-06 10:01:35 - train: epoch 0054, iter [01400, 05004], lr: 0.085622, loss: 1.8799
2022-07-06 10:02:08 - train: epoch 0054, iter [01500, 05004], lr: 0.085611, loss: 1.8614
2022-07-06 10:02:41 - train: epoch 0054, iter [01600, 05004], lr: 0.085600, loss: 1.4588
2022-07-06 10:03:15 - train: epoch 0054, iter [01700, 05004], lr: 0.085588, loss: 2.0234
2022-07-06 10:03:48 - train: epoch 0054, iter [01800, 05004], lr: 0.085577, loss: 1.8222
2022-07-06 10:04:21 - train: epoch 0054, iter [01900, 05004], lr: 0.085566, loss: 2.2924
2022-07-06 10:04:54 - train: epoch 0054, iter [02000, 05004], lr: 0.085554, loss: 1.9795
2022-07-06 10:05:27 - train: epoch 0054, iter [02100, 05004], lr: 0.085543, loss: 1.6214
2022-07-06 10:06:01 - train: epoch 0054, iter [02200, 05004], lr: 0.085532, loss: 1.9051
2022-07-06 10:06:34 - train: epoch 0054, iter [02300, 05004], lr: 0.085520, loss: 1.9218
2022-07-06 10:07:08 - train: epoch 0054, iter [02400, 05004], lr: 0.085509, loss: 1.8399
2022-07-06 10:07:41 - train: epoch 0054, iter [02500, 05004], lr: 0.085498, loss: 1.9426
2022-07-06 10:08:14 - train: epoch 0054, iter [02600, 05004], lr: 0.085486, loss: 1.7570
2022-07-06 10:08:48 - train: epoch 0054, iter [02700, 05004], lr: 0.085475, loss: 1.8758
2022-07-06 10:09:21 - train: epoch 0054, iter [02800, 05004], lr: 0.085464, loss: 2.1987
2022-07-06 10:09:55 - train: epoch 0054, iter [02900, 05004], lr: 0.085452, loss: 1.7566
2022-07-06 10:10:27 - train: epoch 0054, iter [03000, 05004], lr: 0.085441, loss: 1.9943
2022-07-06 10:11:01 - train: epoch 0054, iter [03100, 05004], lr: 0.085430, loss: 1.7486
2022-07-06 10:11:35 - train: epoch 0054, iter [03200, 05004], lr: 0.085418, loss: 1.9985
2022-07-06 10:12:08 - train: epoch 0054, iter [03300, 05004], lr: 0.085407, loss: 1.7660
2022-07-06 10:12:42 - train: epoch 0054, iter [03400, 05004], lr: 0.085395, loss: 1.8330
2022-07-06 10:13:15 - train: epoch 0054, iter [03500, 05004], lr: 0.085384, loss: 2.0484
2022-07-06 10:13:49 - train: epoch 0054, iter [03600, 05004], lr: 0.085373, loss: 1.8139
2022-07-06 10:14:23 - train: epoch 0054, iter [03700, 05004], lr: 0.085361, loss: 1.7473
2022-07-06 10:14:56 - train: epoch 0054, iter [03800, 05004], lr: 0.085350, loss: 1.8914
2022-07-06 10:15:29 - train: epoch 0054, iter [03900, 05004], lr: 0.085339, loss: 1.8386
2022-07-06 10:16:03 - train: epoch 0054, iter [04000, 05004], lr: 0.085327, loss: 1.7501
2022-07-06 10:16:37 - train: epoch 0054, iter [04100, 05004], lr: 0.085316, loss: 2.0046
2022-07-06 10:17:10 - train: epoch 0054, iter [04200, 05004], lr: 0.085304, loss: 1.9030
2022-07-06 10:17:43 - train: epoch 0054, iter [04300, 05004], lr: 0.085293, loss: 1.7897
2022-07-06 10:18:17 - train: epoch 0054, iter [04400, 05004], lr: 0.085282, loss: 1.5293
2022-07-06 10:18:49 - train: epoch 0054, iter [04500, 05004], lr: 0.085270, loss: 1.7927
2022-07-06 10:19:23 - train: epoch 0054, iter [04600, 05004], lr: 0.085259, loss: 1.9288
2022-07-06 10:19:56 - train: epoch 0054, iter [04700, 05004], lr: 0.085247, loss: 2.1750
2022-07-06 10:20:29 - train: epoch 0054, iter [04800, 05004], lr: 0.085236, loss: 2.0127
2022-07-06 10:21:03 - train: epoch 0054, iter [04900, 05004], lr: 0.085225, loss: 1.6616
2022-07-06 10:21:35 - train: epoch 0054, iter [05000, 05004], lr: 0.085213, loss: 1.9479
2022-07-06 10:21:36 - train: epoch 054, train_loss: 1.8488
2022-07-06 10:22:50 - eval: epoch: 054, acc1: 59.980%, acc5: 83.852%, test_loss: 1.6695, per_image_load_time: 1.153ms, per_image_inference_time: 0.466ms
2022-07-06 10:22:50 - until epoch: 054, best_acc1: 60.430%
2022-07-06 10:22:50 - epoch 055 lr: 0.085213
2022-07-06 10:23:30 - train: epoch 0055, iter [00100, 05004], lr: 0.085201, loss: 1.8267
2022-07-06 10:24:03 - train: epoch 0055, iter [00200, 05004], lr: 0.085190, loss: 1.7631
2022-07-06 10:24:36 - train: epoch 0055, iter [00300, 05004], lr: 0.085178, loss: 1.6770
2022-07-06 10:25:09 - train: epoch 0055, iter [00400, 05004], lr: 0.085167, loss: 1.6966
2022-07-06 10:25:42 - train: epoch 0055, iter [00500, 05004], lr: 0.085155, loss: 1.6748
2022-07-06 10:26:16 - train: epoch 0055, iter [00600, 05004], lr: 0.085144, loss: 1.7279
2022-07-06 10:26:50 - train: epoch 0055, iter [00700, 05004], lr: 0.085133, loss: 1.9875
2022-07-06 10:27:22 - train: epoch 0055, iter [00800, 05004], lr: 0.085121, loss: 1.7369
2022-07-06 10:27:56 - train: epoch 0055, iter [00900, 05004], lr: 0.085110, loss: 1.8399
2022-07-06 10:28:30 - train: epoch 0055, iter [01000, 05004], lr: 0.085098, loss: 1.8154
2022-07-06 10:29:03 - train: epoch 0055, iter [01100, 05004], lr: 0.085087, loss: 1.8552
2022-07-06 10:29:37 - train: epoch 0055, iter [01200, 05004], lr: 0.085075, loss: 1.8763
2022-07-06 10:30:10 - train: epoch 0055, iter [01300, 05004], lr: 0.085064, loss: 1.9515
2022-07-06 10:30:44 - train: epoch 0055, iter [01400, 05004], lr: 0.085052, loss: 1.8049
2022-07-06 10:31:17 - train: epoch 0055, iter [01500, 05004], lr: 0.085041, loss: 1.8100
2022-07-06 10:31:51 - train: epoch 0055, iter [01600, 05004], lr: 0.085029, loss: 2.0902
2022-07-06 10:32:24 - train: epoch 0055, iter [01700, 05004], lr: 0.085018, loss: 2.0009
2022-07-06 10:32:58 - train: epoch 0055, iter [01800, 05004], lr: 0.085006, loss: 1.9347
2022-07-06 10:33:31 - train: epoch 0055, iter [01900, 05004], lr: 0.084995, loss: 1.8508
2022-07-06 10:34:04 - train: epoch 0055, iter [02000, 05004], lr: 0.084983, loss: 1.8412
2022-07-06 10:34:38 - train: epoch 0055, iter [02100, 05004], lr: 0.084972, loss: 1.6319
2022-07-06 10:35:10 - train: epoch 0055, iter [02200, 05004], lr: 0.084960, loss: 1.9863
2022-07-06 10:35:43 - train: epoch 0055, iter [02300, 05004], lr: 0.084949, loss: 1.7547
2022-07-06 10:36:16 - train: epoch 0055, iter [02400, 05004], lr: 0.084937, loss: 1.6271
2022-07-06 10:36:50 - train: epoch 0055, iter [02500, 05004], lr: 0.084926, loss: 1.9256
2022-07-06 10:37:23 - train: epoch 0055, iter [02600, 05004], lr: 0.084914, loss: 1.6889
2022-07-06 10:37:56 - train: epoch 0055, iter [02700, 05004], lr: 0.084903, loss: 1.7018
2022-07-06 10:38:29 - train: epoch 0055, iter [02800, 05004], lr: 0.084891, loss: 1.8603
2022-07-06 10:39:01 - train: epoch 0055, iter [02900, 05004], lr: 0.084880, loss: 1.9530
2022-07-06 10:39:34 - train: epoch 0055, iter [03000, 05004], lr: 0.084868, loss: 1.7653
2022-07-06 10:40:07 - train: epoch 0055, iter [03100, 05004], lr: 0.084857, loss: 1.9420
2022-07-06 10:40:41 - train: epoch 0055, iter [03200, 05004], lr: 0.084845, loss: 1.7044
2022-07-06 10:41:13 - train: epoch 0055, iter [03300, 05004], lr: 0.084834, loss: 1.5626
2022-07-06 10:41:47 - train: epoch 0055, iter [03400, 05004], lr: 0.084822, loss: 1.7981
2022-07-06 10:42:20 - train: epoch 0055, iter [03500, 05004], lr: 0.084810, loss: 1.6817
2022-07-06 10:42:53 - train: epoch 0055, iter [03600, 05004], lr: 0.084799, loss: 1.7981
2022-07-06 10:43:25 - train: epoch 0055, iter [03700, 05004], lr: 0.084787, loss: 1.8366
2022-07-06 10:43:59 - train: epoch 0055, iter [03800, 05004], lr: 0.084776, loss: 1.7859
2022-07-06 10:44:32 - train: epoch 0055, iter [03900, 05004], lr: 0.084764, loss: 2.2094
2022-07-06 10:45:05 - train: epoch 0055, iter [04000, 05004], lr: 0.084753, loss: 1.8591
2022-07-06 10:45:38 - train: epoch 0055, iter [04100, 05004], lr: 0.084741, loss: 1.8668
2022-07-06 10:46:11 - train: epoch 0055, iter [04200, 05004], lr: 0.084729, loss: 1.9132
2022-07-06 10:46:44 - train: epoch 0055, iter [04300, 05004], lr: 0.084718, loss: 1.9659
2022-07-06 10:47:17 - train: epoch 0055, iter [04400, 05004], lr: 0.084706, loss: 2.0025
2022-07-06 10:47:51 - train: epoch 0055, iter [04500, 05004], lr: 0.084695, loss: 1.9294
2022-07-06 10:48:24 - train: epoch 0055, iter [04600, 05004], lr: 0.084683, loss: 1.9365
2022-07-06 10:48:57 - train: epoch 0055, iter [04700, 05004], lr: 0.084671, loss: 1.6148
2022-07-06 10:49:30 - train: epoch 0055, iter [04800, 05004], lr: 0.084660, loss: 1.9493
2022-07-06 10:50:04 - train: epoch 0055, iter [04900, 05004], lr: 0.084648, loss: 1.7449
2022-07-06 10:50:35 - train: epoch 0055, iter [05000, 05004], lr: 0.084637, loss: 1.9550
2022-07-06 10:50:37 - train: epoch 055, train_loss: 1.8417
2022-07-06 10:51:51 - eval: epoch: 055, acc1: 59.318%, acc5: 82.954%, test_loss: 1.7192, per_image_load_time: 0.631ms, per_image_inference_time: 0.447ms
2022-07-06 10:51:51 - until epoch: 055, best_acc1: 60.430%
2022-07-06 10:51:51 - epoch 056 lr: 0.084636
2022-07-06 10:52:30 - train: epoch 0056, iter [00100, 05004], lr: 0.084625, loss: 1.9156
2022-07-06 10:53:03 - train: epoch 0056, iter [00200, 05004], lr: 0.084613, loss: 1.9406
2022-07-06 10:53:36 - train: epoch 0056, iter [00300, 05004], lr: 0.084601, loss: 1.7971
2022-07-06 10:54:09 - train: epoch 0056, iter [00400, 05004], lr: 0.084590, loss: 1.7287
2022-07-06 10:54:43 - train: epoch 0056, iter [00500, 05004], lr: 0.084578, loss: 1.7109
2022-07-06 10:55:16 - train: epoch 0056, iter [00600, 05004], lr: 0.084566, loss: 1.7561
2022-07-06 10:55:50 - train: epoch 0056, iter [00700, 05004], lr: 0.084555, loss: 1.7155
2022-07-06 10:56:23 - train: epoch 0056, iter [00800, 05004], lr: 0.084543, loss: 2.0325
2022-07-06 10:56:56 - train: epoch 0056, iter [00900, 05004], lr: 0.084532, loss: 1.9084
2022-07-06 10:57:28 - train: epoch 0056, iter [01000, 05004], lr: 0.084520, loss: 1.8002
2022-07-06 10:58:02 - train: epoch 0056, iter [01100, 05004], lr: 0.084508, loss: 1.7033
2022-07-06 10:58:35 - train: epoch 0056, iter [01200, 05004], lr: 0.084497, loss: 1.7718
2022-07-06 10:59:08 - train: epoch 0056, iter [01300, 05004], lr: 0.084485, loss: 1.9302
2022-07-06 10:59:42 - train: epoch 0056, iter [01400, 05004], lr: 0.084473, loss: 1.7964
2022-07-06 11:00:15 - train: epoch 0056, iter [01500, 05004], lr: 0.084462, loss: 2.1773
2022-07-06 11:00:49 - train: epoch 0056, iter [01600, 05004], lr: 0.084450, loss: 1.7045
2022-07-06 11:01:22 - train: epoch 0056, iter [01700, 05004], lr: 0.084438, loss: 1.8134
2022-07-06 11:01:55 - train: epoch 0056, iter [01800, 05004], lr: 0.084427, loss: 2.1338
2022-07-06 11:02:29 - train: epoch 0056, iter [01900, 05004], lr: 0.084415, loss: 1.8905
2022-07-06 11:03:03 - train: epoch 0056, iter [02000, 05004], lr: 0.084403, loss: 1.7673
2022-07-06 11:03:35 - train: epoch 0056, iter [02100, 05004], lr: 0.084392, loss: 1.8612
2022-07-06 11:04:10 - train: epoch 0056, iter [02200, 05004], lr: 0.084380, loss: 2.0279
2022-07-06 11:04:44 - train: epoch 0056, iter [02300, 05004], lr: 0.084368, loss: 1.9645
2022-07-06 11:05:17 - train: epoch 0056, iter [02400, 05004], lr: 0.084357, loss: 1.8275
2022-07-06 11:05:51 - train: epoch 0056, iter [02500, 05004], lr: 0.084345, loss: 2.0662
2022-07-06 11:06:23 - train: epoch 0056, iter [02600, 05004], lr: 0.084333, loss: 1.7800
2022-07-06 11:06:58 - train: epoch 0056, iter [02700, 05004], lr: 0.084321, loss: 1.8177
2022-07-06 11:07:31 - train: epoch 0056, iter [02800, 05004], lr: 0.084310, loss: 1.7205
2022-07-06 11:08:05 - train: epoch 0056, iter [02900, 05004], lr: 0.084298, loss: 1.9189
2022-07-06 11:08:39 - train: epoch 0056, iter [03000, 05004], lr: 0.084286, loss: 2.0240
2022-07-06 11:09:12 - train: epoch 0056, iter [03100, 05004], lr: 0.084275, loss: 1.6978
2022-07-06 11:09:45 - train: epoch 0056, iter [03200, 05004], lr: 0.084263, loss: 1.6738
2022-07-06 11:10:18 - train: epoch 0056, iter [03300, 05004], lr: 0.084251, loss: 2.2867
2022-07-06 11:10:52 - train: epoch 0056, iter [03400, 05004], lr: 0.084239, loss: 1.8379
2022-07-06 11:11:25 - train: epoch 0056, iter [03500, 05004], lr: 0.084228, loss: 1.7530
2022-07-06 11:11:58 - train: epoch 0056, iter [03600, 05004], lr: 0.084216, loss: 1.5924
2022-07-06 11:12:31 - train: epoch 0056, iter [03700, 05004], lr: 0.084204, loss: 1.8356
2022-07-06 11:13:05 - train: epoch 0056, iter [03800, 05004], lr: 0.084192, loss: 1.7263
2022-07-06 11:13:39 - train: epoch 0056, iter [03900, 05004], lr: 0.084181, loss: 2.1102
2022-07-06 11:14:13 - train: epoch 0056, iter [04000, 05004], lr: 0.084169, loss: 1.7715
2022-07-06 11:14:46 - train: epoch 0056, iter [04100, 05004], lr: 0.084157, loss: 2.0212
2022-07-06 11:15:19 - train: epoch 0056, iter [04200, 05004], lr: 0.084145, loss: 1.8159
2022-07-06 11:15:53 - train: epoch 0056, iter [04300, 05004], lr: 0.084134, loss: 1.8168
2022-07-06 11:16:27 - train: epoch 0056, iter [04400, 05004], lr: 0.084122, loss: 1.9396
2022-07-06 11:17:00 - train: epoch 0056, iter [04500, 05004], lr: 0.084110, loss: 1.8706
2022-07-06 11:17:34 - train: epoch 0056, iter [04600, 05004], lr: 0.084098, loss: 1.9939
2022-07-06 11:18:08 - train: epoch 0056, iter [04700, 05004], lr: 0.084087, loss: 2.0043
2022-07-06 11:18:40 - train: epoch 0056, iter [04800, 05004], lr: 0.084075, loss: 2.1046
2022-07-06 11:19:14 - train: epoch 0056, iter [04900, 05004], lr: 0.084063, loss: 1.8292
2022-07-06 11:19:47 - train: epoch 0056, iter [05000, 05004], lr: 0.084051, loss: 1.9864
2022-07-06 11:19:48 - train: epoch 056, train_loss: 1.8358
2022-07-06 11:21:02 - eval: epoch: 056, acc1: 56.632%, acc5: 81.122%, test_loss: 1.8643, per_image_load_time: 2.005ms, per_image_inference_time: 0.471ms
2022-07-06 11:21:02 - until epoch: 056, best_acc1: 60.430%
2022-07-06 11:21:02 - epoch 057 lr: 0.084051
2022-07-06 11:21:41 - train: epoch 0057, iter [00100, 05004], lr: 0.084039, loss: 1.9162
2022-07-06 11:22:14 - train: epoch 0057, iter [00200, 05004], lr: 0.084027, loss: 1.7829
2022-07-06 11:22:48 - train: epoch 0057, iter [00300, 05004], lr: 0.084015, loss: 1.6108
2022-07-06 11:23:20 - train: epoch 0057, iter [00400, 05004], lr: 0.084004, loss: 1.7989
2022-07-06 11:23:55 - train: epoch 0057, iter [00500, 05004], lr: 0.083992, loss: 1.5998
2022-07-06 11:24:28 - train: epoch 0057, iter [00600, 05004], lr: 0.083980, loss: 1.9137
2022-07-06 11:25:01 - train: epoch 0057, iter [00700, 05004], lr: 0.083968, loss: 1.5871
2022-07-06 11:25:35 - train: epoch 0057, iter [00800, 05004], lr: 0.083956, loss: 1.7626
2022-07-06 11:26:08 - train: epoch 0057, iter [00900, 05004], lr: 0.083945, loss: 1.8263
2022-07-06 11:26:41 - train: epoch 0057, iter [01000, 05004], lr: 0.083933, loss: 1.7790
2022-07-06 11:27:15 - train: epoch 0057, iter [01100, 05004], lr: 0.083921, loss: 1.8299
2022-07-06 11:27:48 - train: epoch 0057, iter [01200, 05004], lr: 0.083909, loss: 1.7264
2022-07-06 11:28:22 - train: epoch 0057, iter [01300, 05004], lr: 0.083897, loss: 1.8842
2022-07-06 11:28:55 - train: epoch 0057, iter [01400, 05004], lr: 0.083885, loss: 1.9267
2022-07-06 11:29:29 - train: epoch 0057, iter [01500, 05004], lr: 0.083874, loss: 2.0973
2022-07-06 11:30:02 - train: epoch 0057, iter [01600, 05004], lr: 0.083862, loss: 2.0772
2022-07-06 11:30:35 - train: epoch 0057, iter [01700, 05004], lr: 0.083850, loss: 2.0071
2022-07-06 11:31:09 - train: epoch 0057, iter [01800, 05004], lr: 0.083838, loss: 1.9778
2022-07-06 11:31:42 - train: epoch 0057, iter [01900, 05004], lr: 0.083826, loss: 1.7039
2022-07-06 11:32:16 - train: epoch 0057, iter [02000, 05004], lr: 0.083814, loss: 1.9026
2022-07-06 11:32:50 - train: epoch 0057, iter [02100, 05004], lr: 0.083802, loss: 1.9068
2022-07-06 11:33:23 - train: epoch 0057, iter [02200, 05004], lr: 0.083791, loss: 1.8641
2022-07-06 11:33:58 - train: epoch 0057, iter [02300, 05004], lr: 0.083779, loss: 1.7502
2022-07-06 11:34:31 - train: epoch 0057, iter [02400, 05004], lr: 0.083767, loss: 1.6960
2022-07-06 11:35:04 - train: epoch 0057, iter [02500, 05004], lr: 0.083755, loss: 1.8504
2022-07-06 11:35:39 - train: epoch 0057, iter [02600, 05004], lr: 0.083743, loss: 1.6618
2022-07-06 11:36:13 - train: epoch 0057, iter [02700, 05004], lr: 0.083731, loss: 1.5152
2022-07-06 11:36:46 - train: epoch 0057, iter [02800, 05004], lr: 0.083719, loss: 1.4175
2022-07-06 11:37:20 - train: epoch 0057, iter [02900, 05004], lr: 0.083707, loss: 1.9935
2022-07-06 11:37:54 - train: epoch 0057, iter [03000, 05004], lr: 0.083696, loss: 2.0698
2022-07-06 11:38:27 - train: epoch 0057, iter [03100, 05004], lr: 0.083684, loss: 2.0717
2022-07-06 11:39:01 - train: epoch 0057, iter [03200, 05004], lr: 0.083672, loss: 2.0823
2022-07-06 11:39:35 - train: epoch 0057, iter [03300, 05004], lr: 0.083660, loss: 1.7754
2022-07-06 11:40:09 - train: epoch 0057, iter [03400, 05004], lr: 0.083648, loss: 1.8502
2022-07-06 11:40:43 - train: epoch 0057, iter [03500, 05004], lr: 0.083636, loss: 1.8466
2022-07-06 11:41:17 - train: epoch 0057, iter [03600, 05004], lr: 0.083624, loss: 1.6675
2022-07-06 11:41:51 - train: epoch 0057, iter [03700, 05004], lr: 0.083612, loss: 1.7043
2022-07-06 11:42:24 - train: epoch 0057, iter [03800, 05004], lr: 0.083600, loss: 1.8543
2022-07-06 11:42:58 - train: epoch 0057, iter [03900, 05004], lr: 0.083588, loss: 2.0054
2022-07-06 11:43:32 - train: epoch 0057, iter [04000, 05004], lr: 0.083576, loss: 1.8362
2022-07-06 11:44:05 - train: epoch 0057, iter [04100, 05004], lr: 0.083565, loss: 1.8376
2022-07-06 11:44:38 - train: epoch 0057, iter [04200, 05004], lr: 0.083553, loss: 1.9017
2022-07-06 11:45:12 - train: epoch 0057, iter [04300, 05004], lr: 0.083541, loss: 1.7225
2022-07-06 11:45:45 - train: epoch 0057, iter [04400, 05004], lr: 0.083529, loss: 1.7845
2022-07-06 11:46:19 - train: epoch 0057, iter [04500, 05004], lr: 0.083517, loss: 2.0157
2022-07-06 11:46:52 - train: epoch 0057, iter [04600, 05004], lr: 0.083505, loss: 1.9423
2022-07-06 11:47:26 - train: epoch 0057, iter [04700, 05004], lr: 0.083493, loss: 1.7000
2022-07-06 11:48:01 - train: epoch 0057, iter [04800, 05004], lr: 0.083481, loss: 2.0641
2022-07-06 11:48:34 - train: epoch 0057, iter [04900, 05004], lr: 0.083469, loss: 1.9341
2022-07-06 11:49:07 - train: epoch 0057, iter [05000, 05004], lr: 0.083457, loss: 1.9813
2022-07-06 11:49:08 - train: epoch 057, train_loss: 1.8327
2022-07-06 11:50:23 - eval: epoch: 057, acc1: 58.830%, acc5: 82.684%, test_loss: 1.7375, per_image_load_time: 0.635ms, per_image_inference_time: 0.444ms
2022-07-06 11:50:23 - until epoch: 057, best_acc1: 60.430%
2022-07-06 11:50:23 - epoch 058 lr: 0.083456
2022-07-06 11:51:01 - train: epoch 0058, iter [00100, 05004], lr: 0.083445, loss: 1.8360
2022-07-06 11:51:35 - train: epoch 0058, iter [00200, 05004], lr: 0.083433, loss: 1.5885
2022-07-06 11:52:08 - train: epoch 0058, iter [00300, 05004], lr: 0.083421, loss: 1.8489
2022-07-06 11:52:41 - train: epoch 0058, iter [00400, 05004], lr: 0.083409, loss: 1.8641
2022-07-06 11:53:15 - train: epoch 0058, iter [00500, 05004], lr: 0.083397, loss: 1.6661
2022-07-06 11:53:49 - train: epoch 0058, iter [00600, 05004], lr: 0.083385, loss: 1.9390
2022-07-06 11:54:23 - train: epoch 0058, iter [00700, 05004], lr: 0.083373, loss: 1.8147
2022-07-06 11:54:56 - train: epoch 0058, iter [00800, 05004], lr: 0.083361, loss: 1.6923
2022-07-06 11:55:29 - train: epoch 0058, iter [00900, 05004], lr: 0.083349, loss: 1.6490
2022-07-06 11:56:03 - train: epoch 0058, iter [01000, 05004], lr: 0.083337, loss: 1.9489
2022-07-06 11:56:36 - train: epoch 0058, iter [01100, 05004], lr: 0.083325, loss: 1.5735
2022-07-06 11:57:10 - train: epoch 0058, iter [01200, 05004], lr: 0.083313, loss: 1.7275
2022-07-06 11:57:43 - train: epoch 0058, iter [01300, 05004], lr: 0.083301, loss: 1.9185
2022-07-06 11:58:17 - train: epoch 0058, iter [01400, 05004], lr: 0.083289, loss: 1.8191
2022-07-06 11:58:49 - train: epoch 0058, iter [01500, 05004], lr: 0.083277, loss: 1.8001
2022-07-06 11:59:24 - train: epoch 0058, iter [01600, 05004], lr: 0.083265, loss: 1.7332
2022-07-06 11:59:56 - train: epoch 0058, iter [01700, 05004], lr: 0.083253, loss: 1.9869
2022-07-06 12:00:30 - train: epoch 0058, iter [01800, 05004], lr: 0.083241, loss: 1.8593
2022-07-06 12:01:04 - train: epoch 0058, iter [01900, 05004], lr: 0.083229, loss: 1.9441
2022-07-06 12:01:37 - train: epoch 0058, iter [02000, 05004], lr: 0.083217, loss: 2.0205
2022-07-06 12:02:10 - train: epoch 0058, iter [02100, 05004], lr: 0.083205, loss: 1.6753
2022-07-06 12:02:44 - train: epoch 0058, iter [02200, 05004], lr: 0.083193, loss: 1.6432
2022-07-06 12:03:18 - train: epoch 0058, iter [02300, 05004], lr: 0.083180, loss: 1.7880
2022-07-06 12:03:51 - train: epoch 0058, iter [02400, 05004], lr: 0.083168, loss: 1.8917
2022-07-06 12:04:24 - train: epoch 0058, iter [02500, 05004], lr: 0.083156, loss: 1.9283
2022-07-06 12:04:58 - train: epoch 0058, iter [02600, 05004], lr: 0.083144, loss: 1.6963
2022-07-06 12:05:32 - train: epoch 0058, iter [02700, 05004], lr: 0.083132, loss: 2.0019
2022-07-06 12:06:05 - train: epoch 0058, iter [02800, 05004], lr: 0.083120, loss: 1.5894
2022-07-06 12:06:39 - train: epoch 0058, iter [02900, 05004], lr: 0.083108, loss: 1.6040
2022-07-06 12:07:12 - train: epoch 0058, iter [03000, 05004], lr: 0.083096, loss: 1.8263
2022-07-06 12:07:45 - train: epoch 0058, iter [03100, 05004], lr: 0.083084, loss: 1.6244
2022-07-06 12:08:19 - train: epoch 0058, iter [03200, 05004], lr: 0.083072, loss: 1.7144
2022-07-06 12:08:53 - train: epoch 0058, iter [03300, 05004], lr: 0.083060, loss: 1.6974
2022-07-06 12:09:26 - train: epoch 0058, iter [03400, 05004], lr: 0.083048, loss: 1.7178
2022-07-06 12:09:59 - train: epoch 0058, iter [03500, 05004], lr: 0.083036, loss: 1.7659
2022-07-06 12:10:33 - train: epoch 0058, iter [03600, 05004], lr: 0.083024, loss: 1.6768
2022-07-06 12:11:06 - train: epoch 0058, iter [03700, 05004], lr: 0.083012, loss: 1.8869
2022-07-06 12:11:40 - train: epoch 0058, iter [03800, 05004], lr: 0.082999, loss: 1.9465
2022-07-06 12:12:13 - train: epoch 0058, iter [03900, 05004], lr: 0.082987, loss: 1.8123
2022-07-06 12:12:47 - train: epoch 0058, iter [04000, 05004], lr: 0.082975, loss: 1.9184
2022-07-06 12:13:21 - train: epoch 0058, iter [04100, 05004], lr: 0.082963, loss: 2.0144
2022-07-06 12:13:54 - train: epoch 0058, iter [04200, 05004], lr: 0.082951, loss: 1.5481
2022-07-06 12:14:28 - train: epoch 0058, iter [04300, 05004], lr: 0.082939, loss: 2.1310
2022-07-06 12:15:01 - train: epoch 0058, iter [04400, 05004], lr: 0.082927, loss: 1.6358
2022-07-06 12:15:35 - train: epoch 0058, iter [04500, 05004], lr: 0.082915, loss: 1.8532
2022-07-06 12:16:08 - train: epoch 0058, iter [04600, 05004], lr: 0.082903, loss: 1.6052
2022-07-06 12:16:42 - train: epoch 0058, iter [04700, 05004], lr: 0.082890, loss: 1.8756
2022-07-06 12:17:15 - train: epoch 0058, iter [04800, 05004], lr: 0.082878, loss: 1.8800
2022-07-06 12:17:49 - train: epoch 0058, iter [04900, 05004], lr: 0.082866, loss: 1.6321
2022-07-06 12:18:22 - train: epoch 0058, iter [05000, 05004], lr: 0.082854, loss: 1.7643
2022-07-06 12:18:23 - train: epoch 058, train_loss: 1.8258
2022-07-06 12:19:36 - eval: epoch: 058, acc1: 60.394%, acc5: 83.708%, test_loss: 1.6548, per_image_load_time: 1.856ms, per_image_inference_time: 0.468ms
2022-07-06 12:19:36 - until epoch: 058, best_acc1: 60.430%
2022-07-06 12:19:36 - epoch 059 lr: 0.082853
2022-07-06 12:20:15 - train: epoch 0059, iter [00100, 05004], lr: 0.082841, loss: 1.9047
2022-07-06 12:20:48 - train: epoch 0059, iter [00200, 05004], lr: 0.082829, loss: 1.7165
2022-07-06 12:21:21 - train: epoch 0059, iter [00300, 05004], lr: 0.082817, loss: 1.8896
2022-07-06 12:21:55 - train: epoch 0059, iter [00400, 05004], lr: 0.082805, loss: 1.8894
2022-07-06 12:22:27 - train: epoch 0059, iter [00500, 05004], lr: 0.082793, loss: 2.1288
2022-07-06 12:23:01 - train: epoch 0059, iter [00600, 05004], lr: 0.082781, loss: 1.7162
2022-07-06 12:23:34 - train: epoch 0059, iter [00700, 05004], lr: 0.082769, loss: 1.7964
2022-07-06 12:24:08 - train: epoch 0059, iter [00800, 05004], lr: 0.082756, loss: 1.7873
2022-07-06 12:24:41 - train: epoch 0059, iter [00900, 05004], lr: 0.082744, loss: 1.8135
2022-07-06 12:25:15 - train: epoch 0059, iter [01000, 05004], lr: 0.082732, loss: 1.7430
2022-07-06 12:25:48 - train: epoch 0059, iter [01100, 05004], lr: 0.082720, loss: 1.9579
2022-07-06 12:26:22 - train: epoch 0059, iter [01200, 05004], lr: 0.082708, loss: 1.5520
2022-07-06 12:26:56 - train: epoch 0059, iter [01300, 05004], lr: 0.082696, loss: 2.0386
2022-07-06 12:27:29 - train: epoch 0059, iter [01400, 05004], lr: 0.082683, loss: 1.9810
2022-07-06 12:28:02 - train: epoch 0059, iter [01500, 05004], lr: 0.082671, loss: 1.8690
2022-07-06 12:28:35 - train: epoch 0059, iter [01600, 05004], lr: 0.082659, loss: 1.6629
2022-07-06 12:29:09 - train: epoch 0059, iter [01700, 05004], lr: 0.082647, loss: 1.8672
2022-07-06 12:29:42 - train: epoch 0059, iter [01800, 05004], lr: 0.082635, loss: 1.7469
2022-07-06 12:30:16 - train: epoch 0059, iter [01900, 05004], lr: 0.082622, loss: 1.9293
2022-07-06 12:30:49 - train: epoch 0059, iter [02000, 05004], lr: 0.082610, loss: 1.7487
2022-07-06 12:31:24 - train: epoch 0059, iter [02100, 05004], lr: 0.082598, loss: 2.0793
2022-07-06 12:31:57 - train: epoch 0059, iter [02200, 05004], lr: 0.082586, loss: 2.0225
2022-07-06 12:32:31 - train: epoch 0059, iter [02300, 05004], lr: 0.082574, loss: 1.7751
2022-07-06 12:33:04 - train: epoch 0059, iter [02400, 05004], lr: 0.082561, loss: 1.8599
2022-07-06 12:33:38 - train: epoch 0059, iter [02500, 05004], lr: 0.082549, loss: 1.9196
2022-07-06 12:34:11 - train: epoch 0059, iter [02600, 05004], lr: 0.082537, loss: 1.7133
2022-07-06 12:34:44 - train: epoch 0059, iter [02700, 05004], lr: 0.082525, loss: 1.8001
2022-07-06 12:35:18 - train: epoch 0059, iter [02800, 05004], lr: 0.082512, loss: 2.0165
2022-07-06 12:35:51 - train: epoch 0059, iter [02900, 05004], lr: 0.082500, loss: 1.8419
2022-07-06 12:36:25 - train: epoch 0059, iter [03000, 05004], lr: 0.082488, loss: 2.1756
2022-07-06 12:36:59 - train: epoch 0059, iter [03100, 05004], lr: 0.082476, loss: 1.8104
2022-07-06 12:37:32 - train: epoch 0059, iter [03200, 05004], lr: 0.082464, loss: 1.7106
2022-07-06 12:38:06 - train: epoch 0059, iter [03300, 05004], lr: 0.082451, loss: 1.9485
2022-07-06 12:38:39 - train: epoch 0059, iter [03400, 05004], lr: 0.082439, loss: 2.1234
2022-07-06 12:39:12 - train: epoch 0059, iter [03500, 05004], lr: 0.082427, loss: 1.6586
2022-07-06 12:39:46 - train: epoch 0059, iter [03600, 05004], lr: 0.082415, loss: 1.8474
2022-07-06 12:40:21 - train: epoch 0059, iter [03700, 05004], lr: 0.082402, loss: 1.7202
2022-07-06 12:40:53 - train: epoch 0059, iter [03800, 05004], lr: 0.082390, loss: 1.7830
2022-07-06 12:41:27 - train: epoch 0059, iter [03900, 05004], lr: 0.082378, loss: 1.8254
2022-07-06 12:42:00 - train: epoch 0059, iter [04000, 05004], lr: 0.082365, loss: 2.1710
2022-07-06 12:42:34 - train: epoch 0059, iter [04100, 05004], lr: 0.082353, loss: 1.8113
2022-07-06 12:43:08 - train: epoch 0059, iter [04200, 05004], lr: 0.082341, loss: 1.9449
2022-07-06 12:43:41 - train: epoch 0059, iter [04300, 05004], lr: 0.082329, loss: 2.0171
2022-07-06 12:44:15 - train: epoch 0059, iter [04400, 05004], lr: 0.082316, loss: 2.0056
2022-07-06 12:44:49 - train: epoch 0059, iter [04500, 05004], lr: 0.082304, loss: 1.9215
2022-07-06 12:45:22 - train: epoch 0059, iter [04600, 05004], lr: 0.082292, loss: 1.9481
2022-07-06 12:45:55 - train: epoch 0059, iter [04700, 05004], lr: 0.082279, loss: 1.8516
2022-07-06 12:46:29 - train: epoch 0059, iter [04800, 05004], lr: 0.082267, loss: 1.7027
2022-07-06 12:47:02 - train: epoch 0059, iter [04900, 05004], lr: 0.082255, loss: 1.9593
2022-07-06 12:47:35 - train: epoch 0059, iter [05000, 05004], lr: 0.082243, loss: 2.0712
2022-07-06 12:47:36 - train: epoch 059, train_loss: 1.8210
2022-07-06 12:48:50 - eval: epoch: 059, acc1: 57.662%, acc5: 81.686%, test_loss: 1.7916, per_image_load_time: 1.787ms, per_image_inference_time: 0.470ms
2022-07-06 12:48:50 - until epoch: 059, best_acc1: 60.430%
2022-07-06 12:48:50 - epoch 060 lr: 0.082242
2022-07-06 12:49:29 - train: epoch 0060, iter [00100, 05004], lr: 0.082230, loss: 1.6294
2022-07-06 12:50:02 - train: epoch 0060, iter [00200, 05004], lr: 0.082217, loss: 1.8858
2022-07-06 12:50:36 - train: epoch 0060, iter [00300, 05004], lr: 0.082205, loss: 1.7704
2022-07-06 12:51:09 - train: epoch 0060, iter [00400, 05004], lr: 0.082193, loss: 1.9040
2022-07-06 12:51:43 - train: epoch 0060, iter [00500, 05004], lr: 0.082181, loss: 1.9092
2022-07-06 12:52:15 - train: epoch 0060, iter [00600, 05004], lr: 0.082168, loss: 1.8891
2022-07-06 12:52:49 - train: epoch 0060, iter [00700, 05004], lr: 0.082156, loss: 1.8303
2022-07-06 12:53:22 - train: epoch 0060, iter [00800, 05004], lr: 0.082144, loss: 2.0214
2022-07-06 12:53:55 - train: epoch 0060, iter [00900, 05004], lr: 0.082131, loss: 1.5661
2022-07-06 12:54:28 - train: epoch 0060, iter [01000, 05004], lr: 0.082119, loss: 1.5082
2022-07-06 12:55:01 - train: epoch 0060, iter [01100, 05004], lr: 0.082107, loss: 1.4734
2022-07-06 12:55:34 - train: epoch 0060, iter [01200, 05004], lr: 0.082094, loss: 1.7260
2022-07-06 12:56:08 - train: epoch 0060, iter [01300, 05004], lr: 0.082082, loss: 1.6664
2022-07-06 12:56:42 - train: epoch 0060, iter [01400, 05004], lr: 0.082070, loss: 1.8749
2022-07-06 12:57:16 - train: epoch 0060, iter [01500, 05004], lr: 0.082057, loss: 1.8208
2022-07-06 12:57:49 - train: epoch 0060, iter [01600, 05004], lr: 0.082045, loss: 1.7836
2022-07-06 12:58:22 - train: epoch 0060, iter [01700, 05004], lr: 0.082032, loss: 1.8516
2022-07-06 12:58:56 - train: epoch 0060, iter [01800, 05004], lr: 0.082020, loss: 1.9029
2022-07-06 12:59:28 - train: epoch 0060, iter [01900, 05004], lr: 0.082008, loss: 2.0483
2022-07-06 13:00:01 - train: epoch 0060, iter [02000, 05004], lr: 0.081995, loss: 1.5902
2022-07-06 13:00:35 - train: epoch 0060, iter [02100, 05004], lr: 0.081983, loss: 1.9640
2022-07-06 13:01:09 - train: epoch 0060, iter [02200, 05004], lr: 0.081971, loss: 2.0652
2022-07-06 13:01:42 - train: epoch 0060, iter [02300, 05004], lr: 0.081958, loss: 1.5062
2022-07-06 13:02:15 - train: epoch 0060, iter [02400, 05004], lr: 0.081946, loss: 1.7861
2022-07-06 13:02:49 - train: epoch 0060, iter [02500, 05004], lr: 0.081933, loss: 1.8081
2022-07-06 13:03:22 - train: epoch 0060, iter [02600, 05004], lr: 0.081921, loss: 1.9104
2022-07-06 13:03:56 - train: epoch 0060, iter [02700, 05004], lr: 0.081909, loss: 1.8126
2022-07-06 13:04:29 - train: epoch 0060, iter [02800, 05004], lr: 0.081896, loss: 1.7167
2022-07-06 13:05:03 - train: epoch 0060, iter [02900, 05004], lr: 0.081884, loss: 1.9925
2022-07-06 13:05:36 - train: epoch 0060, iter [03000, 05004], lr: 0.081871, loss: 2.1553
2022-07-06 13:06:10 - train: epoch 0060, iter [03100, 05004], lr: 0.081859, loss: 1.9448
2022-07-06 13:06:44 - train: epoch 0060, iter [03200, 05004], lr: 0.081847, loss: 1.8351
2022-07-06 13:07:17 - train: epoch 0060, iter [03300, 05004], lr: 0.081834, loss: 1.6565
2022-07-06 13:07:51 - train: epoch 0060, iter [03400, 05004], lr: 0.081822, loss: 1.8711
2022-07-06 13:08:24 - train: epoch 0060, iter [03500, 05004], lr: 0.081809, loss: 1.9468
2022-07-06 13:08:57 - train: epoch 0060, iter [03600, 05004], lr: 0.081797, loss: 1.8913
2022-07-06 13:09:31 - train: epoch 0060, iter [03700, 05004], lr: 0.081785, loss: 1.8072
2022-07-06 13:10:04 - train: epoch 0060, iter [03800, 05004], lr: 0.081772, loss: 1.8568
2022-07-06 13:10:38 - train: epoch 0060, iter [03900, 05004], lr: 0.081760, loss: 2.2135
2022-07-06 13:11:12 - train: epoch 0060, iter [04000, 05004], lr: 0.081747, loss: 1.8633
2022-07-06 13:11:46 - train: epoch 0060, iter [04100, 05004], lr: 0.081735, loss: 1.9765
2022-07-06 13:12:19 - train: epoch 0060, iter [04200, 05004], lr: 0.081722, loss: 1.8894
2022-07-06 13:12:52 - train: epoch 0060, iter [04300, 05004], lr: 0.081710, loss: 1.7720
2022-07-06 13:13:25 - train: epoch 0060, iter [04400, 05004], lr: 0.081698, loss: 1.9397
2022-07-06 13:13:58 - train: epoch 0060, iter [04500, 05004], lr: 0.081685, loss: 1.7665
2022-07-06 13:14:32 - train: epoch 0060, iter [04600, 05004], lr: 0.081673, loss: 1.7085
2022-07-06 13:15:06 - train: epoch 0060, iter [04700, 05004], lr: 0.081660, loss: 1.9428
2022-07-06 13:15:38 - train: epoch 0060, iter [04800, 05004], lr: 0.081648, loss: 1.7428
2022-07-06 13:16:12 - train: epoch 0060, iter [04900, 05004], lr: 0.081635, loss: 1.7343
2022-07-06 13:16:45 - train: epoch 0060, iter [05000, 05004], lr: 0.081623, loss: 1.8936
2022-07-06 13:16:46 - train: epoch 060, train_loss: 1.8170
2022-07-06 13:18:01 - eval: epoch: 060, acc1: 59.876%, acc5: 83.860%, test_loss: 1.6572, per_image_load_time: 1.016ms, per_image_inference_time: 0.455ms
2022-07-06 13:18:01 - until epoch: 060, best_acc1: 60.430%
2022-07-06 13:18:01 - epoch 061 lr: 0.081622
2022-07-06 13:18:39 - train: epoch 0061, iter [00100, 05004], lr: 0.081610, loss: 1.7007
2022-07-06 13:19:13 - train: epoch 0061, iter [00200, 05004], lr: 0.081597, loss: 1.8136
2022-07-06 13:19:45 - train: epoch 0061, iter [00300, 05004], lr: 0.081585, loss: 1.5271
2022-07-06 13:20:19 - train: epoch 0061, iter [00400, 05004], lr: 0.081572, loss: 1.9957
2022-07-06 13:20:53 - train: epoch 0061, iter [00500, 05004], lr: 0.081560, loss: 1.7702
2022-07-06 13:21:26 - train: epoch 0061, iter [00600, 05004], lr: 0.081547, loss: 1.8540
2022-07-06 13:21:59 - train: epoch 0061, iter [00700, 05004], lr: 0.081535, loss: 1.4543
2022-07-06 13:22:34 - train: epoch 0061, iter [00800, 05004], lr: 0.081522, loss: 1.8379
2022-07-06 13:23:07 - train: epoch 0061, iter [00900, 05004], lr: 0.081510, loss: 1.7325
2022-07-06 13:23:41 - train: epoch 0061, iter [01000, 05004], lr: 0.081497, loss: 1.6213
2022-07-06 13:24:14 - train: epoch 0061, iter [01100, 05004], lr: 0.081485, loss: 1.6347
2022-07-06 13:24:47 - train: epoch 0061, iter [01200, 05004], lr: 0.081472, loss: 1.6949
2022-07-06 13:25:22 - train: epoch 0061, iter [01300, 05004], lr: 0.081460, loss: 1.7208
2022-07-06 13:25:55 - train: epoch 0061, iter [01400, 05004], lr: 0.081447, loss: 1.6868
2022-07-06 13:26:28 - train: epoch 0061, iter [01500, 05004], lr: 0.081435, loss: 1.8847
2022-07-06 13:27:02 - train: epoch 0061, iter [01600, 05004], lr: 0.081422, loss: 1.6284
2022-07-06 13:27:35 - train: epoch 0061, iter [01700, 05004], lr: 0.081410, loss: 1.8500
2022-07-06 13:28:09 - train: epoch 0061, iter [01800, 05004], lr: 0.081397, loss: 1.9117
2022-07-06 13:28:42 - train: epoch 0061, iter [01900, 05004], lr: 0.081385, loss: 1.9573
2022-07-06 13:29:16 - train: epoch 0061, iter [02000, 05004], lr: 0.081372, loss: 1.7314
2022-07-06 13:29:50 - train: epoch 0061, iter [02100, 05004], lr: 0.081360, loss: 2.1971
2022-07-06 13:30:23 - train: epoch 0061, iter [02200, 05004], lr: 0.081347, loss: 1.7067
2022-07-06 13:30:57 - train: epoch 0061, iter [02300, 05004], lr: 0.081335, loss: 1.6922
2022-07-06 13:31:30 - train: epoch 0061, iter [02400, 05004], lr: 0.081322, loss: 1.6207
2022-07-06 13:32:04 - train: epoch 0061, iter [02500, 05004], lr: 0.081310, loss: 1.6968
2022-07-06 13:32:38 - train: epoch 0061, iter [02600, 05004], lr: 0.081297, loss: 1.9177
2022-07-06 13:33:12 - train: epoch 0061, iter [02700, 05004], lr: 0.081284, loss: 1.9523
2022-07-06 13:33:45 - train: epoch 0061, iter [02800, 05004], lr: 0.081272, loss: 1.7969
2022-07-06 13:34:19 - train: epoch 0061, iter [02900, 05004], lr: 0.081259, loss: 1.8726
2022-07-06 13:34:53 - train: epoch 0061, iter [03000, 05004], lr: 0.081247, loss: 1.8148
2022-07-06 13:35:27 - train: epoch 0061, iter [03100, 05004], lr: 0.081234, loss: 1.9835
2022-07-06 13:36:00 - train: epoch 0061, iter [03200, 05004], lr: 0.081222, loss: 1.8026
2022-07-06 13:36:34 - train: epoch 0061, iter [03300, 05004], lr: 0.081209, loss: 1.8172
2022-07-06 13:37:08 - train: epoch 0061, iter [03400, 05004], lr: 0.081196, loss: 1.6243
2022-07-06 13:37:42 - train: epoch 0061, iter [03500, 05004], lr: 0.081184, loss: 2.0142
2022-07-06 13:38:16 - train: epoch 0061, iter [03600, 05004], lr: 0.081171, loss: 1.7827
2022-07-06 13:38:49 - train: epoch 0061, iter [03700, 05004], lr: 0.081159, loss: 1.9857
2022-07-06 13:39:22 - train: epoch 0061, iter [03800, 05004], lr: 0.081146, loss: 1.8581
2022-07-06 13:39:57 - train: epoch 0061, iter [03900, 05004], lr: 0.081133, loss: 1.7565
2022-07-06 13:40:30 - train: epoch 0061, iter [04000, 05004], lr: 0.081121, loss: 1.8802
2022-07-06 13:41:04 - train: epoch 0061, iter [04100, 05004], lr: 0.081108, loss: 2.1599
2022-07-06 13:41:37 - train: epoch 0061, iter [04200, 05004], lr: 0.081096, loss: 1.7028
2022-07-06 13:42:12 - train: epoch 0061, iter [04300, 05004], lr: 0.081083, loss: 1.8769
2022-07-06 13:42:45 - train: epoch 0061, iter [04400, 05004], lr: 0.081070, loss: 2.0208
2022-07-06 13:43:19 - train: epoch 0061, iter [04500, 05004], lr: 0.081058, loss: 1.9253
2022-07-06 13:43:53 - train: epoch 0061, iter [04600, 05004], lr: 0.081045, loss: 1.9674
2022-07-06 13:44:26 - train: epoch 0061, iter [04700, 05004], lr: 0.081033, loss: 1.7562
2022-07-06 13:45:00 - train: epoch 0061, iter [04800, 05004], lr: 0.081020, loss: 1.7533
2022-07-06 13:45:34 - train: epoch 0061, iter [04900, 05004], lr: 0.081007, loss: 1.7658
2022-07-06 13:46:06 - train: epoch 0061, iter [05000, 05004], lr: 0.080995, loss: 2.0104
2022-07-06 13:46:07 - train: epoch 061, train_loss: 1.8114
2022-07-06 13:47:21 - eval: epoch: 061, acc1: 59.020%, acc5: 82.740%, test_loss: 1.7316, per_image_load_time: 2.002ms, per_image_inference_time: 0.462ms
2022-07-06 13:47:22 - until epoch: 061, best_acc1: 60.430%
2022-07-06 13:47:22 - epoch 062 lr: 0.080994
2022-07-06 13:48:00 - train: epoch 0062, iter [00100, 05004], lr: 0.080982, loss: 1.9003
2022-07-06 13:48:34 - train: epoch 0062, iter [00200, 05004], lr: 0.080969, loss: 1.9626
2022-07-06 13:49:07 - train: epoch 0062, iter [00300, 05004], lr: 0.080956, loss: 1.7165
2022-07-06 13:49:40 - train: epoch 0062, iter [00400, 05004], lr: 0.080944, loss: 1.7990
2022-07-06 13:50:14 - train: epoch 0062, iter [00500, 05004], lr: 0.080931, loss: 1.7582
2022-07-06 13:50:47 - train: epoch 0062, iter [00600, 05004], lr: 0.080918, loss: 1.7173
2022-07-06 13:51:21 - train: epoch 0062, iter [00700, 05004], lr: 0.080906, loss: 1.9251
2022-07-06 13:51:54 - train: epoch 0062, iter [00800, 05004], lr: 0.080893, loss: 1.7892
2022-07-06 13:52:28 - train: epoch 0062, iter [00900, 05004], lr: 0.080880, loss: 1.7074
2022-07-06 13:53:01 - train: epoch 0062, iter [01000, 05004], lr: 0.080868, loss: 1.9243
2022-07-06 13:53:34 - train: epoch 0062, iter [01100, 05004], lr: 0.080855, loss: 1.7596
2022-07-06 13:54:09 - train: epoch 0062, iter [01200, 05004], lr: 0.080842, loss: 2.0861
2022-07-06 13:54:42 - train: epoch 0062, iter [01300, 05004], lr: 0.080830, loss: 1.9726
2022-07-06 13:55:16 - train: epoch 0062, iter [01400, 05004], lr: 0.080817, loss: 1.6663
2022-07-06 13:55:51 - train: epoch 0062, iter [01500, 05004], lr: 0.080804, loss: 1.8617
2022-07-06 13:56:23 - train: epoch 0062, iter [01600, 05004], lr: 0.080792, loss: 1.8636
2022-07-06 13:56:57 - train: epoch 0062, iter [01700, 05004], lr: 0.080779, loss: 1.8020
2022-07-06 13:57:30 - train: epoch 0062, iter [01800, 05004], lr: 0.080766, loss: 1.7374
2022-07-06 13:58:03 - train: epoch 0062, iter [01900, 05004], lr: 0.080754, loss: 1.7897
2022-07-06 13:58:37 - train: epoch 0062, iter [02000, 05004], lr: 0.080741, loss: 1.7459
2022-07-06 13:59:11 - train: epoch 0062, iter [02100, 05004], lr: 0.080728, loss: 1.9500
2022-07-06 13:59:44 - train: epoch 0062, iter [02200, 05004], lr: 0.080716, loss: 1.6427
2022-07-06 14:00:19 - train: epoch 0062, iter [02300, 05004], lr: 0.080703, loss: 1.7763
2022-07-06 14:00:52 - train: epoch 0062, iter [02400, 05004], lr: 0.080690, loss: 1.7602
2022-07-06 14:01:25 - train: epoch 0062, iter [02500, 05004], lr: 0.080677, loss: 1.9953
2022-07-06 14:01:58 - train: epoch 0062, iter [02600, 05004], lr: 0.080665, loss: 1.5592
2022-07-06 14:02:32 - train: epoch 0062, iter [02700, 05004], lr: 0.080652, loss: 1.9013
2022-07-06 14:03:06 - train: epoch 0062, iter [02800, 05004], lr: 0.080639, loss: 1.9418
2022-07-06 14:03:40 - train: epoch 0062, iter [02900, 05004], lr: 0.080627, loss: 1.7913
2022-07-06 14:04:13 - train: epoch 0062, iter [03000, 05004], lr: 0.080614, loss: 1.8903
2022-07-06 14:04:47 - train: epoch 0062, iter [03100, 05004], lr: 0.080601, loss: 1.9370
2022-07-06 14:05:19 - train: epoch 0062, iter [03200, 05004], lr: 0.080588, loss: 1.6417
2022-07-06 14:05:53 - train: epoch 0062, iter [03300, 05004], lr: 0.080576, loss: 1.9701
2022-07-06 14:06:27 - train: epoch 0062, iter [03400, 05004], lr: 0.080563, loss: 1.8818
2022-07-06 14:07:00 - train: epoch 0062, iter [03500, 05004], lr: 0.080550, loss: 1.8961
2022-07-06 14:07:34 - train: epoch 0062, iter [03600, 05004], lr: 0.080537, loss: 1.9280
2022-07-06 14:08:07 - train: epoch 0062, iter [03700, 05004], lr: 0.080525, loss: 1.8084
2022-07-06 14:08:41 - train: epoch 0062, iter [03800, 05004], lr: 0.080512, loss: 1.8411
2022-07-06 14:09:14 - train: epoch 0062, iter [03900, 05004], lr: 0.080499, loss: 1.9070
2022-07-06 14:09:48 - train: epoch 0062, iter [04000, 05004], lr: 0.080486, loss: 1.5670
2022-07-06 14:10:22 - train: epoch 0062, iter [04100, 05004], lr: 0.080474, loss: 1.8585
2022-07-06 14:10:56 - train: epoch 0062, iter [04200, 05004], lr: 0.080461, loss: 1.7297
2022-07-06 14:11:29 - train: epoch 0062, iter [04300, 05004], lr: 0.080448, loss: 1.8374
2022-07-06 14:12:03 - train: epoch 0062, iter [04400, 05004], lr: 0.080435, loss: 1.7888
2022-07-06 14:12:36 - train: epoch 0062, iter [04500, 05004], lr: 0.080423, loss: 1.6875
2022-07-06 14:13:10 - train: epoch 0062, iter [04600, 05004], lr: 0.080410, loss: 1.4957
2022-07-06 14:13:44 - train: epoch 0062, iter [04700, 05004], lr: 0.080397, loss: 1.7792
2022-07-06 14:14:18 - train: epoch 0062, iter [04800, 05004], lr: 0.080384, loss: 1.7641
2022-07-06 14:14:52 - train: epoch 0062, iter [04900, 05004], lr: 0.080371, loss: 1.7764
2022-07-06 14:15:24 - train: epoch 0062, iter [05000, 05004], lr: 0.080359, loss: 1.8586
2022-07-06 14:15:25 - train: epoch 062, train_loss: 1.8062
2022-07-06 14:16:40 - eval: epoch: 062, acc1: 58.796%, acc5: 82.580%, test_loss: 1.7357, per_image_load_time: 0.741ms, per_image_inference_time: 0.447ms
2022-07-06 14:16:40 - until epoch: 062, best_acc1: 60.430%
2022-07-06 14:16:40 - epoch 063 lr: 0.080358
2022-07-06 14:17:20 - train: epoch 0063, iter [00100, 05004], lr: 0.080345, loss: 1.6874
2022-07-06 14:17:53 - train: epoch 0063, iter [00200, 05004], lr: 0.080333, loss: 1.6506
2022-07-06 14:18:26 - train: epoch 0063, iter [00300, 05004], lr: 0.080320, loss: 1.9036
2022-07-06 14:19:01 - train: epoch 0063, iter [00400, 05004], lr: 0.080307, loss: 1.8410
2022-07-06 14:19:34 - train: epoch 0063, iter [00500, 05004], lr: 0.080294, loss: 1.7124
2022-07-06 14:20:06 - train: epoch 0063, iter [00600, 05004], lr: 0.080281, loss: 1.8393
2022-07-06 14:20:40 - train: epoch 0063, iter [00700, 05004], lr: 0.080269, loss: 1.8252
2022-07-06 14:21:13 - train: epoch 0063, iter [00800, 05004], lr: 0.080256, loss: 1.7388
2022-07-06 14:21:47 - train: epoch 0063, iter [00900, 05004], lr: 0.080243, loss: 1.8087
2022-07-06 14:22:19 - train: epoch 0063, iter [01000, 05004], lr: 0.080230, loss: 1.9424
2022-07-06 14:22:54 - train: epoch 0063, iter [01100, 05004], lr: 0.080217, loss: 1.7990
2022-07-06 14:23:26 - train: epoch 0063, iter [01200, 05004], lr: 0.080204, loss: 1.7701
2022-07-06 14:24:00 - train: epoch 0063, iter [01300, 05004], lr: 0.080192, loss: 1.7563
2022-07-06 14:24:33 - train: epoch 0063, iter [01400, 05004], lr: 0.080179, loss: 2.0859
2022-07-06 14:25:07 - train: epoch 0063, iter [01500, 05004], lr: 0.080166, loss: 1.8426
2022-07-06 14:25:40 - train: epoch 0063, iter [01600, 05004], lr: 0.080153, loss: 1.7980
2022-07-06 14:26:13 - train: epoch 0063, iter [01700, 05004], lr: 0.080140, loss: 1.6328
2022-07-06 14:26:47 - train: epoch 0063, iter [01800, 05004], lr: 0.080127, loss: 1.9776
2022-07-06 14:27:20 - train: epoch 0063, iter [01900, 05004], lr: 0.080115, loss: 1.9499
2022-07-06 14:27:54 - train: epoch 0063, iter [02000, 05004], lr: 0.080102, loss: 1.6530
2022-07-06 14:28:27 - train: epoch 0063, iter [02100, 05004], lr: 0.080089, loss: 1.6561
2022-07-06 14:29:02 - train: epoch 0063, iter [02200, 05004], lr: 0.080076, loss: 2.1151
2022-07-06 14:29:35 - train: epoch 0063, iter [02300, 05004], lr: 0.080063, loss: 1.9001
2022-07-06 14:30:09 - train: epoch 0063, iter [02400, 05004], lr: 0.080050, loss: 2.0274
2022-07-06 14:30:43 - train: epoch 0063, iter [02500, 05004], lr: 0.080037, loss: 1.8467
2022-07-06 14:31:17 - train: epoch 0063, iter [02600, 05004], lr: 0.080024, loss: 1.7145
2022-07-06 14:31:50 - train: epoch 0063, iter [02700, 05004], lr: 0.080012, loss: 1.8934
2022-07-06 14:32:24 - train: epoch 0063, iter [02800, 05004], lr: 0.079999, loss: 1.8440
2022-07-06 14:32:57 - train: epoch 0063, iter [02900, 05004], lr: 0.079986, loss: 1.7767
2022-07-06 14:33:31 - train: epoch 0063, iter [03000, 05004], lr: 0.079973, loss: 1.9479
2022-07-06 14:34:04 - train: epoch 0063, iter [03100, 05004], lr: 0.079960, loss: 2.1889
2022-07-06 14:34:38 - train: epoch 0063, iter [03200, 05004], lr: 0.079947, loss: 1.9180
2022-07-06 14:35:11 - train: epoch 0063, iter [03300, 05004], lr: 0.079934, loss: 1.7616
2022-07-06 14:35:45 - train: epoch 0063, iter [03400, 05004], lr: 0.079921, loss: 1.6564
2022-07-06 14:36:18 - train: epoch 0063, iter [03500, 05004], lr: 0.079909, loss: 1.9328
2022-07-06 14:36:51 - train: epoch 0063, iter [03600, 05004], lr: 0.079896, loss: 1.7193
2022-07-06 14:37:25 - train: epoch 0063, iter [03700, 05004], lr: 0.079883, loss: 1.9019
2022-07-06 14:37:58 - train: epoch 0063, iter [03800, 05004], lr: 0.079870, loss: 2.0201
2022-07-06 14:38:31 - train: epoch 0063, iter [03900, 05004], lr: 0.079857, loss: 1.5517
2022-07-06 14:39:05 - train: epoch 0063, iter [04000, 05004], lr: 0.079844, loss: 1.5022
2022-07-06 14:39:39 - train: epoch 0063, iter [04100, 05004], lr: 0.079831, loss: 1.9112
2022-07-06 14:40:12 - train: epoch 0063, iter [04200, 05004], lr: 0.079818, loss: 1.7505
2022-07-06 14:40:46 - train: epoch 0063, iter [04300, 05004], lr: 0.079805, loss: 1.9908
2022-07-06 14:41:20 - train: epoch 0063, iter [04400, 05004], lr: 0.079792, loss: 1.8296
2022-07-06 14:41:53 - train: epoch 0063, iter [04500, 05004], lr: 0.079779, loss: 1.6917
2022-07-06 14:42:27 - train: epoch 0063, iter [04600, 05004], lr: 0.079766, loss: 1.6745
2022-07-06 14:43:00 - train: epoch 0063, iter [04700, 05004], lr: 0.079753, loss: 1.7271
2022-07-06 14:43:35 - train: epoch 0063, iter [04800, 05004], lr: 0.079741, loss: 1.8520
2022-07-06 14:44:08 - train: epoch 0063, iter [04900, 05004], lr: 0.079728, loss: 1.7099
2022-07-06 14:44:40 - train: epoch 0063, iter [05000, 05004], lr: 0.079715, loss: 1.8736
2022-07-06 14:44:42 - train: epoch 063, train_loss: 1.8039
2022-07-06 14:45:57 - eval: epoch: 063, acc1: 60.828%, acc5: 84.086%, test_loss: 1.6413, per_image_load_time: 0.858ms, per_image_inference_time: 0.443ms
2022-07-06 14:45:58 - until epoch: 063, best_acc1: 60.828%
2022-07-06 14:45:58 - epoch 064 lr: 0.079714
2022-07-06 14:46:36 - train: epoch 0064, iter [00100, 05004], lr: 0.079701, loss: 1.6181
2022-07-06 14:47:09 - train: epoch 0064, iter [00200, 05004], lr: 0.079688, loss: 1.8089
2022-07-06 14:47:43 - train: epoch 0064, iter [00300, 05004], lr: 0.079675, loss: 1.5643
2022-07-06 14:48:16 - train: epoch 0064, iter [00400, 05004], lr: 0.079662, loss: 1.7310
2022-07-06 14:48:50 - train: epoch 0064, iter [00500, 05004], lr: 0.079649, loss: 1.6344
2022-07-06 14:49:23 - train: epoch 0064, iter [00600, 05004], lr: 0.079636, loss: 1.8525
2022-07-06 14:49:56 - train: epoch 0064, iter [00700, 05004], lr: 0.079623, loss: 1.8650
2022-07-06 14:50:30 - train: epoch 0064, iter [00800, 05004], lr: 0.079610, loss: 1.5877
2022-07-06 14:51:03 - train: epoch 0064, iter [00900, 05004], lr: 0.079598, loss: 1.8469
2022-07-06 14:51:37 - train: epoch 0064, iter [01000, 05004], lr: 0.079585, loss: 1.7312
2022-07-06 14:52:10 - train: epoch 0064, iter [01100, 05004], lr: 0.079572, loss: 1.6925
2022-07-06 14:52:44 - train: epoch 0064, iter [01200, 05004], lr: 0.079559, loss: 1.6533
2022-07-06 14:53:17 - train: epoch 0064, iter [01300, 05004], lr: 0.079546, loss: 1.8816
2022-07-06 14:53:50 - train: epoch 0064, iter [01400, 05004], lr: 0.079533, loss: 2.0333
2022-07-06 14:54:24 - train: epoch 0064, iter [01500, 05004], lr: 0.079520, loss: 1.9440
2022-07-06 14:54:57 - train: epoch 0064, iter [01600, 05004], lr: 0.079507, loss: 1.6314
2022-07-06 14:55:31 - train: epoch 0064, iter [01700, 05004], lr: 0.079494, loss: 1.5163
2022-07-06 14:56:04 - train: epoch 0064, iter [01800, 05004], lr: 0.079481, loss: 1.7644
2022-07-06 14:56:38 - train: epoch 0064, iter [01900, 05004], lr: 0.079468, loss: 1.7978
2022-07-06 14:57:11 - train: epoch 0064, iter [02000, 05004], lr: 0.079455, loss: 1.6557
2022-07-06 14:57:45 - train: epoch 0064, iter [02100, 05004], lr: 0.079442, loss: 1.7243
2022-07-06 14:58:18 - train: epoch 0064, iter [02200, 05004], lr: 0.079429, loss: 1.8504
2022-07-06 14:58:52 - train: epoch 0064, iter [02300, 05004], lr: 0.079416, loss: 1.8518
2022-07-06 14:59:25 - train: epoch 0064, iter [02400, 05004], lr: 0.079403, loss: 1.8623
2022-07-06 14:59:58 - train: epoch 0064, iter [02500, 05004], lr: 0.079390, loss: 1.6255
2022-07-06 15:00:31 - train: epoch 0064, iter [02600, 05004], lr: 0.079376, loss: 1.7829
2022-07-06 15:01:05 - train: epoch 0064, iter [02700, 05004], lr: 0.079363, loss: 1.7708
2022-07-06 15:01:38 - train: epoch 0064, iter [02800, 05004], lr: 0.079350, loss: 1.6458
2022-07-06 15:02:12 - train: epoch 0064, iter [02900, 05004], lr: 0.079337, loss: 2.1037
2022-07-06 15:02:46 - train: epoch 0064, iter [03000, 05004], lr: 0.079324, loss: 1.9487
2022-07-06 15:03:19 - train: epoch 0064, iter [03100, 05004], lr: 0.079311, loss: 1.7908
2022-07-06 15:03:53 - train: epoch 0064, iter [03200, 05004], lr: 0.079298, loss: 1.8179
2022-07-06 15:04:26 - train: epoch 0064, iter [03300, 05004], lr: 0.079285, loss: 1.7476
2022-07-06 15:05:00 - train: epoch 0064, iter [03400, 05004], lr: 0.079272, loss: 2.0125
2022-07-06 15:05:34 - train: epoch 0064, iter [03500, 05004], lr: 0.079259, loss: 1.6957
2022-07-06 15:06:08 - train: epoch 0064, iter [03600, 05004], lr: 0.079246, loss: 1.7895
2022-07-06 15:06:41 - train: epoch 0064, iter [03700, 05004], lr: 0.079233, loss: 1.4044
2022-07-06 15:07:15 - train: epoch 0064, iter [03800, 05004], lr: 0.079220, loss: 2.0027
2022-07-06 15:07:48 - train: epoch 0064, iter [03900, 05004], lr: 0.079207, loss: 1.7152
2022-07-06 15:08:21 - train: epoch 0064, iter [04000, 05004], lr: 0.079194, loss: 1.6186
2022-07-06 15:08:55 - train: epoch 0064, iter [04100, 05004], lr: 0.079181, loss: 1.9117
2022-07-06 15:09:28 - train: epoch 0064, iter [04200, 05004], lr: 0.079168, loss: 1.6979
2022-07-06 15:10:02 - train: epoch 0064, iter [04300, 05004], lr: 0.079155, loss: 1.9421
2022-07-06 15:10:36 - train: epoch 0064, iter [04400, 05004], lr: 0.079142, loss: 1.7695
2022-07-06 15:11:08 - train: epoch 0064, iter [04500, 05004], lr: 0.079128, loss: 1.7288
2022-07-06 15:11:42 - train: epoch 0064, iter [04600, 05004], lr: 0.079115, loss: 1.9837
2022-07-06 15:12:16 - train: epoch 0064, iter [04700, 05004], lr: 0.079102, loss: 2.1831
2022-07-06 15:12:49 - train: epoch 0064, iter [04800, 05004], lr: 0.079089, loss: 1.8903
2022-07-06 15:13:23 - train: epoch 0064, iter [04900, 05004], lr: 0.079076, loss: 1.9736
2022-07-06 15:13:55 - train: epoch 0064, iter [05000, 05004], lr: 0.079063, loss: 1.7198
2022-07-06 15:13:56 - train: epoch 064, train_loss: 1.7983
2022-07-06 15:15:10 - eval: epoch: 064, acc1: 60.630%, acc5: 84.010%, test_loss: 1.6441, per_image_load_time: 1.070ms, per_image_inference_time: 0.446ms
2022-07-06 15:15:10 - until epoch: 064, best_acc1: 60.828%
2022-07-06 15:15:10 - epoch 065 lr: 0.079062
2022-07-06 15:15:49 - train: epoch 0065, iter [00100, 05004], lr: 0.079049, loss: 1.8182
2022-07-06 15:16:23 - train: epoch 0065, iter [00200, 05004], lr: 0.079036, loss: 1.7773
2022-07-06 15:16:56 - train: epoch 0065, iter [00300, 05004], lr: 0.079023, loss: 1.7171
2022-07-06 15:17:30 - train: epoch 0065, iter [00400, 05004], lr: 0.079010, loss: 1.9437
2022-07-06 15:18:02 - train: epoch 0065, iter [00500, 05004], lr: 0.078997, loss: 1.7027
2022-07-06 15:18:36 - train: epoch 0065, iter [00600, 05004], lr: 0.078984, loss: 1.7597
2022-07-06 15:19:09 - train: epoch 0065, iter [00700, 05004], lr: 0.078971, loss: 1.8329
2022-07-06 15:19:43 - train: epoch 0065, iter [00800, 05004], lr: 0.078958, loss: 1.6723
2022-07-06 15:20:16 - train: epoch 0065, iter [00900, 05004], lr: 0.078944, loss: 1.7870
2022-07-06 15:20:50 - train: epoch 0065, iter [01000, 05004], lr: 0.078931, loss: 1.7324
2022-07-06 15:21:24 - train: epoch 0065, iter [01100, 05004], lr: 0.078918, loss: 1.7353
2022-07-06 15:21:57 - train: epoch 0065, iter [01200, 05004], lr: 0.078905, loss: 2.0020
2022-07-06 15:22:30 - train: epoch 0065, iter [01300, 05004], lr: 0.078892, loss: 1.7572
2022-07-06 15:23:04 - train: epoch 0065, iter [01400, 05004], lr: 0.078879, loss: 1.4959
2022-07-06 15:23:36 - train: epoch 0065, iter [01500, 05004], lr: 0.078866, loss: 1.9507
2022-07-06 15:24:11 - train: epoch 0065, iter [01600, 05004], lr: 0.078852, loss: 1.9834
2022-07-06 15:24:43 - train: epoch 0065, iter [01700, 05004], lr: 0.078839, loss: 1.6934
2022-07-06 15:25:17 - train: epoch 0065, iter [01800, 05004], lr: 0.078826, loss: 1.6779
2022-07-06 15:25:50 - train: epoch 0065, iter [01900, 05004], lr: 0.078813, loss: 1.6601
2022-07-06 15:26:24 - train: epoch 0065, iter [02000, 05004], lr: 0.078800, loss: 1.7513
2022-07-06 15:26:57 - train: epoch 0065, iter [02100, 05004], lr: 0.078787, loss: 1.7549
2022-07-06 15:27:31 - train: epoch 0065, iter [02200, 05004], lr: 0.078774, loss: 1.8462
2022-07-06 15:28:05 - train: epoch 0065, iter [02300, 05004], lr: 0.078760, loss: 1.6751
2022-07-06 15:28:39 - train: epoch 0065, iter [02400, 05004], lr: 0.078747, loss: 1.7089
2022-07-06 15:29:13 - train: epoch 0065, iter [02500, 05004], lr: 0.078734, loss: 1.6656
2022-07-06 15:29:46 - train: epoch 0065, iter [02600, 05004], lr: 0.078721, loss: 1.9925
2022-07-06 15:30:19 - train: epoch 0065, iter [02700, 05004], lr: 0.078708, loss: 1.9033
2022-07-06 15:30:53 - train: epoch 0065, iter [02800, 05004], lr: 0.078695, loss: 1.6783
2022-07-06 15:31:26 - train: epoch 0065, iter [02900, 05004], lr: 0.078681, loss: 1.9236
2022-07-06 15:32:00 - train: epoch 0065, iter [03000, 05004], lr: 0.078668, loss: 1.7130
2022-07-06 15:32:33 - train: epoch 0065, iter [03100, 05004], lr: 0.078655, loss: 1.8006
2022-07-06 15:33:06 - train: epoch 0065, iter [03200, 05004], lr: 0.078642, loss: 1.9849
2022-07-06 15:33:40 - train: epoch 0065, iter [03300, 05004], lr: 0.078629, loss: 1.9253
2022-07-06 15:34:14 - train: epoch 0065, iter [03400, 05004], lr: 0.078615, loss: 1.7605
2022-07-06 15:34:47 - train: epoch 0065, iter [03500, 05004], lr: 0.078602, loss: 2.0895
2022-07-06 15:35:21 - train: epoch 0065, iter [03600, 05004], lr: 0.078589, loss: 1.8533
2022-07-06 15:35:54 - train: epoch 0065, iter [03700, 05004], lr: 0.078576, loss: 1.6618
2022-07-06 15:36:28 - train: epoch 0065, iter [03800, 05004], lr: 0.078563, loss: 1.7468
2022-07-06 15:37:02 - train: epoch 0065, iter [03900, 05004], lr: 0.078549, loss: 1.9308
2022-07-06 15:37:35 - train: epoch 0065, iter [04000, 05004], lr: 0.078536, loss: 1.7916
2022-07-06 15:38:08 - train: epoch 0065, iter [04100, 05004], lr: 0.078523, loss: 1.6093
2022-07-06 15:38:42 - train: epoch 0065, iter [04200, 05004], lr: 0.078510, loss: 1.8474
2022-07-06 15:39:16 - train: epoch 0065, iter [04300, 05004], lr: 0.078496, loss: 1.7850
2022-07-06 15:39:50 - train: epoch 0065, iter [04400, 05004], lr: 0.078483, loss: 1.7883
2022-07-06 15:40:23 - train: epoch 0065, iter [04500, 05004], lr: 0.078470, loss: 1.8634
2022-07-06 15:40:56 - train: epoch 0065, iter [04600, 05004], lr: 0.078457, loss: 1.8496
2022-07-06 15:41:31 - train: epoch 0065, iter [04700, 05004], lr: 0.078443, loss: 1.8497
2022-07-06 15:42:04 - train: epoch 0065, iter [04800, 05004], lr: 0.078430, loss: 1.4732
2022-07-06 15:42:38 - train: epoch 0065, iter [04900, 05004], lr: 0.078417, loss: 1.7614
2022-07-06 15:43:10 - train: epoch 0065, iter [05000, 05004], lr: 0.078404, loss: 1.8112
2022-07-06 15:43:11 - train: epoch 065, train_loss: 1.7937
2022-07-06 15:44:25 - eval: epoch: 065, acc1: 62.268%, acc5: 85.122%, test_loss: 1.5651, per_image_load_time: 1.583ms, per_image_inference_time: 0.465ms
2022-07-06 15:44:26 - until epoch: 065, best_acc1: 62.268%
2022-07-06 15:44:26 - epoch 066 lr: 0.078403
2022-07-06 15:45:05 - train: epoch 0066, iter [00100, 05004], lr: 0.078390, loss: 1.5759
2022-07-06 15:45:38 - train: epoch 0066, iter [00200, 05004], lr: 0.078377, loss: 1.8755
2022-07-06 15:46:12 - train: epoch 0066, iter [00300, 05004], lr: 0.078363, loss: 1.7302
2022-07-06 15:46:44 - train: epoch 0066, iter [00400, 05004], lr: 0.078350, loss: 1.4821
2022-07-06 15:47:18 - train: epoch 0066, iter [00500, 05004], lr: 0.078337, loss: 1.8882
2022-07-06 15:47:51 - train: epoch 0066, iter [00600, 05004], lr: 0.078324, loss: 1.7741
2022-07-06 15:48:25 - train: epoch 0066, iter [00700, 05004], lr: 0.078310, loss: 1.8026
2022-07-06 15:48:59 - train: epoch 0066, iter [00800, 05004], lr: 0.078297, loss: 2.1441
2022-07-06 15:49:31 - train: epoch 0066, iter [00900, 05004], lr: 0.078284, loss: 1.8607
2022-07-06 15:50:05 - train: epoch 0066, iter [01000, 05004], lr: 0.078271, loss: 1.8250
2022-07-06 15:50:39 - train: epoch 0066, iter [01100, 05004], lr: 0.078257, loss: 1.8831
2022-07-06 15:51:13 - train: epoch 0066, iter [01200, 05004], lr: 0.078244, loss: 2.0581
2022-07-06 15:51:45 - train: epoch 0066, iter [01300, 05004], lr: 0.078231, loss: 1.9426
2022-07-06 15:52:19 - train: epoch 0066, iter [01400, 05004], lr: 0.078217, loss: 1.6269
2022-07-06 15:52:53 - train: epoch 0066, iter [01500, 05004], lr: 0.078204, loss: 1.7843
2022-07-06 15:53:26 - train: epoch 0066, iter [01600, 05004], lr: 0.078191, loss: 1.8056
2022-07-06 15:54:00 - train: epoch 0066, iter [01700, 05004], lr: 0.078178, loss: 1.7607
2022-07-06 15:54:33 - train: epoch 0066, iter [01800, 05004], lr: 0.078164, loss: 1.5730
2022-07-06 15:55:06 - train: epoch 0066, iter [01900, 05004], lr: 0.078151, loss: 2.0405
2022-07-06 15:55:39 - train: epoch 0066, iter [02000, 05004], lr: 0.078138, loss: 1.9094
2022-07-06 15:56:12 - train: epoch 0066, iter [02100, 05004], lr: 0.078124, loss: 1.9081
2022-07-06 15:56:45 - train: epoch 0066, iter [02200, 05004], lr: 0.078111, loss: 1.8146
2022-07-06 15:57:19 - train: epoch 0066, iter [02300, 05004], lr: 0.078098, loss: 1.9521
2022-07-06 15:57:52 - train: epoch 0066, iter [02400, 05004], lr: 0.078084, loss: 1.6749
2022-07-06 15:58:26 - train: epoch 0066, iter [02500, 05004], lr: 0.078071, loss: 1.8298
2022-07-06 15:59:00 - train: epoch 0066, iter [02600, 05004], lr: 0.078058, loss: 1.7516
2022-07-06 15:59:33 - train: epoch 0066, iter [02700, 05004], lr: 0.078044, loss: 2.0370
2022-07-06 16:00:06 - train: epoch 0066, iter [02800, 05004], lr: 0.078031, loss: 1.8709
2022-07-06 16:00:40 - train: epoch 0066, iter [02900, 05004], lr: 0.078018, loss: 1.8955
2022-07-06 16:01:13 - train: epoch 0066, iter [03000, 05004], lr: 0.078004, loss: 1.7503
2022-07-06 16:01:47 - train: epoch 0066, iter [03100, 05004], lr: 0.077991, loss: 2.0289
2022-07-06 16:02:20 - train: epoch 0066, iter [03200, 05004], lr: 0.077978, loss: 1.6044
2022-07-06 16:02:54 - train: epoch 0066, iter [03300, 05004], lr: 0.077964, loss: 1.7324
2022-07-06 16:03:27 - train: epoch 0066, iter [03400, 05004], lr: 0.077951, loss: 2.0602
2022-07-06 16:04:01 - train: epoch 0066, iter [03500, 05004], lr: 0.077938, loss: 1.8971
2022-07-06 16:04:33 - train: epoch 0066, iter [03600, 05004], lr: 0.077924, loss: 1.8998
2022-07-06 16:05:07 - train: epoch 0066, iter [03700, 05004], lr: 0.077911, loss: 1.7667
2022-07-06 16:05:40 - train: epoch 0066, iter [03800, 05004], lr: 0.077898, loss: 1.6956
2022-07-06 16:06:14 - train: epoch 0066, iter [03900, 05004], lr: 0.077884, loss: 1.5958
2022-07-06 16:06:47 - train: epoch 0066, iter [04000, 05004], lr: 0.077871, loss: 1.8772
2022-07-06 16:07:20 - train: epoch 0066, iter [04100, 05004], lr: 0.077858, loss: 1.6172
2022-07-06 16:07:53 - train: epoch 0066, iter [04200, 05004], lr: 0.077844, loss: 1.6771
2022-07-06 16:08:26 - train: epoch 0066, iter [04300, 05004], lr: 0.077831, loss: 1.6769
2022-07-06 16:09:01 - train: epoch 0066, iter [04400, 05004], lr: 0.077817, loss: 1.7113
2022-07-06 16:09:34 - train: epoch 0066, iter [04500, 05004], lr: 0.077804, loss: 1.8860
2022-07-06 16:10:08 - train: epoch 0066, iter [04600, 05004], lr: 0.077791, loss: 2.1020
2022-07-06 16:10:41 - train: epoch 0066, iter [04700, 05004], lr: 0.077777, loss: 1.7354
2022-07-06 16:11:15 - train: epoch 0066, iter [04800, 05004], lr: 0.077764, loss: 1.8102
2022-07-06 16:11:50 - train: epoch 0066, iter [04900, 05004], lr: 0.077751, loss: 1.8264
2022-07-06 16:12:22 - train: epoch 0066, iter [05000, 05004], lr: 0.077737, loss: 1.7792
2022-07-06 16:12:23 - train: epoch 066, train_loss: 1.7887
2022-07-06 16:13:37 - eval: epoch: 066, acc1: 61.360%, acc5: 84.702%, test_loss: 1.6020, per_image_load_time: 1.511ms, per_image_inference_time: 0.470ms
2022-07-06 16:13:38 - until epoch: 066, best_acc1: 62.268%
2022-07-06 16:13:38 - epoch 067 lr: 0.077737
2022-07-06 16:14:16 - train: epoch 0067, iter [00100, 05004], lr: 0.077723, loss: 1.6464
2022-07-06 16:14:50 - train: epoch 0067, iter [00200, 05004], lr: 0.077710, loss: 1.7424
2022-07-06 16:15:24 - train: epoch 0067, iter [00300, 05004], lr: 0.077696, loss: 1.9258
2022-07-06 16:15:56 - train: epoch 0067, iter [00400, 05004], lr: 0.077683, loss: 1.7434
2022-07-06 16:16:30 - train: epoch 0067, iter [00500, 05004], lr: 0.077670, loss: 1.6372
2022-07-06 16:17:03 - train: epoch 0067, iter [00600, 05004], lr: 0.077656, loss: 1.6439
2022-07-06 16:17:37 - train: epoch 0067, iter [00700, 05004], lr: 0.077643, loss: 1.8636
2022-07-06 16:18:10 - train: epoch 0067, iter [00800, 05004], lr: 0.077629, loss: 1.9055
2022-07-06 16:18:44 - train: epoch 0067, iter [00900, 05004], lr: 0.077616, loss: 2.0428
2022-07-06 16:19:17 - train: epoch 0067, iter [01000, 05004], lr: 0.077603, loss: 1.5027
2022-07-06 16:19:50 - train: epoch 0067, iter [01100, 05004], lr: 0.077589, loss: 1.7216
2022-07-06 16:20:23 - train: epoch 0067, iter [01200, 05004], lr: 0.077576, loss: 1.6682
2022-07-06 16:20:56 - train: epoch 0067, iter [01300, 05004], lr: 0.077562, loss: 1.9292
2022-07-06 16:21:29 - train: epoch 0067, iter [01400, 05004], lr: 0.077549, loss: 1.8486
2022-07-06 16:22:03 - train: epoch 0067, iter [01500, 05004], lr: 0.077535, loss: 1.7817
2022-07-06 16:22:36 - train: epoch 0067, iter [01600, 05004], lr: 0.077522, loss: 1.8233
2022-07-06 16:23:10 - train: epoch 0067, iter [01700, 05004], lr: 0.077509, loss: 1.7000
2022-07-06 16:23:43 - train: epoch 0067, iter [01800, 05004], lr: 0.077495, loss: 1.9320
2022-07-06 16:24:16 - train: epoch 0067, iter [01900, 05004], lr: 0.077482, loss: 1.6512
2022-07-06 16:24:49 - train: epoch 0067, iter [02000, 05004], lr: 0.077468, loss: 1.8098
2022-07-06 16:25:23 - train: epoch 0067, iter [02100, 05004], lr: 0.077455, loss: 1.6286
2022-07-06 16:25:56 - train: epoch 0067, iter [02200, 05004], lr: 0.077441, loss: 1.7609
2022-07-06 16:26:30 - train: epoch 0067, iter [02300, 05004], lr: 0.077428, loss: 1.6701
2022-07-06 16:27:04 - train: epoch 0067, iter [02400, 05004], lr: 0.077414, loss: 1.7239
2022-07-06 16:27:38 - train: epoch 0067, iter [02500, 05004], lr: 0.077401, loss: 1.6838
2022-07-06 16:28:11 - train: epoch 0067, iter [02600, 05004], lr: 0.077387, loss: 1.6691
2022-07-06 16:28:44 - train: epoch 0067, iter [02700, 05004], lr: 0.077374, loss: 1.7266
2022-07-06 16:29:19 - train: epoch 0067, iter [02800, 05004], lr: 0.077360, loss: 1.8988
2022-07-06 16:29:52 - train: epoch 0067, iter [02900, 05004], lr: 0.077347, loss: 1.7422
2022-07-06 16:30:26 - train: epoch 0067, iter [03000, 05004], lr: 0.077334, loss: 1.7813
2022-07-06 16:30:59 - train: epoch 0067, iter [03100, 05004], lr: 0.077320, loss: 1.6076
2022-07-06 16:31:32 - train: epoch 0067, iter [03200, 05004], lr: 0.077307, loss: 1.9567
2022-07-06 16:32:06 - train: epoch 0067, iter [03300, 05004], lr: 0.077293, loss: 1.6077
2022-07-06 16:32:39 - train: epoch 0067, iter [03400, 05004], lr: 0.077280, loss: 1.7526
2022-07-06 16:33:13 - train: epoch 0067, iter [03500, 05004], lr: 0.077266, loss: 1.7426
2022-07-06 16:33:47 - train: epoch 0067, iter [03600, 05004], lr: 0.077253, loss: 1.9011
2022-07-06 16:34:20 - train: epoch 0067, iter [03700, 05004], lr: 0.077239, loss: 1.8532
2022-07-06 16:34:53 - train: epoch 0067, iter [03800, 05004], lr: 0.077226, loss: 1.7528
2022-07-06 16:35:27 - train: epoch 0067, iter [03900, 05004], lr: 0.077212, loss: 1.9768
2022-07-06 16:36:00 - train: epoch 0067, iter [04000, 05004], lr: 0.077199, loss: 1.7955
2022-07-06 16:36:33 - train: epoch 0067, iter [04100, 05004], lr: 0.077185, loss: 1.8840
2022-07-06 16:37:07 - train: epoch 0067, iter [04200, 05004], lr: 0.077172, loss: 1.8906
2022-07-06 16:37:41 - train: epoch 0067, iter [04300, 05004], lr: 0.077158, loss: 1.6668
2022-07-06 16:38:15 - train: epoch 0067, iter [04400, 05004], lr: 0.077145, loss: 1.9150
2022-07-06 16:38:48 - train: epoch 0067, iter [04500, 05004], lr: 0.077131, loss: 1.7044
2022-07-06 16:39:22 - train: epoch 0067, iter [04600, 05004], lr: 0.077117, loss: 1.4628
2022-07-06 16:39:56 - train: epoch 0067, iter [04700, 05004], lr: 0.077104, loss: 1.8040
2022-07-06 16:40:29 - train: epoch 0067, iter [04800, 05004], lr: 0.077090, loss: 1.4830
2022-07-06 16:41:03 - train: epoch 0067, iter [04900, 05004], lr: 0.077077, loss: 1.8443
2022-07-06 16:41:35 - train: epoch 0067, iter [05000, 05004], lr: 0.077063, loss: 1.8162
2022-07-06 16:41:36 - train: epoch 067, train_loss: 1.7812
2022-07-06 16:42:49 - eval: epoch: 067, acc1: 58.728%, acc5: 82.524%, test_loss: 1.7460, per_image_load_time: 2.215ms, per_image_inference_time: 0.460ms
2022-07-06 16:42:50 - until epoch: 067, best_acc1: 62.268%
2022-07-06 16:42:50 - epoch 068 lr: 0.077063
2022-07-06 16:43:27 - train: epoch 0068, iter [00100, 05004], lr: 0.077049, loss: 1.8030
2022-07-06 16:44:02 - train: epoch 0068, iter [00200, 05004], lr: 0.077036, loss: 1.7914
2022-07-06 16:44:35 - train: epoch 0068, iter [00300, 05004], lr: 0.077022, loss: 1.8899
2022-07-06 16:45:09 - train: epoch 0068, iter [00400, 05004], lr: 0.077009, loss: 1.7219
2022-07-06 16:45:41 - train: epoch 0068, iter [00500, 05004], lr: 0.076995, loss: 1.5557
2022-07-06 16:46:15 - train: epoch 0068, iter [00600, 05004], lr: 0.076982, loss: 1.7854
2022-07-06 16:46:48 - train: epoch 0068, iter [00700, 05004], lr: 0.076968, loss: 2.1054
2022-07-06 16:47:22 - train: epoch 0068, iter [00800, 05004], lr: 0.076954, loss: 1.7012
2022-07-06 16:47:55 - train: epoch 0068, iter [00900, 05004], lr: 0.076941, loss: 1.6125
2022-07-06 16:48:28 - train: epoch 0068, iter [01000, 05004], lr: 0.076927, loss: 1.7482
2022-07-06 16:49:02 - train: epoch 0068, iter [01100, 05004], lr: 0.076914, loss: 2.1155
2022-07-06 16:49:35 - train: epoch 0068, iter [01200, 05004], lr: 0.076900, loss: 1.7396
2022-07-06 16:50:09 - train: epoch 0068, iter [01300, 05004], lr: 0.076887, loss: 1.6609
2022-07-06 16:50:42 - train: epoch 0068, iter [01400, 05004], lr: 0.076873, loss: 1.7891
2022-07-06 16:51:16 - train: epoch 0068, iter [01500, 05004], lr: 0.076859, loss: 1.9064
2022-07-06 16:51:49 - train: epoch 0068, iter [01600, 05004], lr: 0.076846, loss: 1.7528
2022-07-06 16:52:23 - train: epoch 0068, iter [01700, 05004], lr: 0.076832, loss: 1.9481
2022-07-06 16:52:56 - train: epoch 0068, iter [01800, 05004], lr: 0.076819, loss: 1.8287
2022-07-06 16:53:29 - train: epoch 0068, iter [01900, 05004], lr: 0.076805, loss: 1.8727
2022-07-06 16:54:03 - train: epoch 0068, iter [02000, 05004], lr: 0.076792, loss: 1.7841
2022-07-06 16:54:37 - train: epoch 0068, iter [02100, 05004], lr: 0.076778, loss: 1.8143
2022-07-06 16:55:11 - train: epoch 0068, iter [02200, 05004], lr: 0.076764, loss: 1.7719
2022-07-06 16:55:45 - train: epoch 0068, iter [02300, 05004], lr: 0.076751, loss: 1.7092
2022-07-06 16:56:18 - train: epoch 0068, iter [02400, 05004], lr: 0.076737, loss: 1.8866
2022-07-06 16:56:51 - train: epoch 0068, iter [02500, 05004], lr: 0.076724, loss: 1.7849
2022-07-06 16:57:26 - train: epoch 0068, iter [02600, 05004], lr: 0.076710, loss: 1.7590
2022-07-06 16:57:59 - train: epoch 0068, iter [02700, 05004], lr: 0.076696, loss: 1.7646
2022-07-06 16:58:33 - train: epoch 0068, iter [02800, 05004], lr: 0.076683, loss: 1.9232
2022-07-06 16:59:06 - train: epoch 0068, iter [02900, 05004], lr: 0.076669, loss: 1.8634
2022-07-06 16:59:39 - train: epoch 0068, iter [03000, 05004], lr: 0.076656, loss: 1.9880
2022-07-06 17:00:12 - train: epoch 0068, iter [03100, 05004], lr: 0.076642, loss: 1.4736
2022-07-06 17:00:46 - train: epoch 0068, iter [03200, 05004], lr: 0.076628, loss: 1.9440
2022-07-06 17:01:21 - train: epoch 0068, iter [03300, 05004], lr: 0.076615, loss: 1.6971
2022-07-06 17:01:54 - train: epoch 0068, iter [03400, 05004], lr: 0.076601, loss: 1.6628
2022-07-06 17:02:27 - train: epoch 0068, iter [03500, 05004], lr: 0.076587, loss: 1.8479
2022-07-06 17:03:01 - train: epoch 0068, iter [03600, 05004], lr: 0.076574, loss: 1.6025
2022-07-06 17:03:35 - train: epoch 0068, iter [03700, 05004], lr: 0.076560, loss: 1.8926
2022-07-06 17:04:08 - train: epoch 0068, iter [03800, 05004], lr: 0.076546, loss: 1.8541
2022-07-06 17:04:41 - train: epoch 0068, iter [03900, 05004], lr: 0.076533, loss: 1.8291
2022-07-06 17:05:15 - train: epoch 0068, iter [04000, 05004], lr: 0.076519, loss: 1.7957
2022-07-06 17:05:49 - train: epoch 0068, iter [04100, 05004], lr: 0.076506, loss: 1.5634
2022-07-06 17:06:23 - train: epoch 0068, iter [04200, 05004], lr: 0.076492, loss: 1.9417
2022-07-06 17:06:57 - train: epoch 0068, iter [04300, 05004], lr: 0.076478, loss: 1.8587
2022-07-06 17:07:30 - train: epoch 0068, iter [04400, 05004], lr: 0.076465, loss: 1.7965
2022-07-06 17:08:04 - train: epoch 0068, iter [04500, 05004], lr: 0.076451, loss: 1.8303
2022-07-06 17:08:38 - train: epoch 0068, iter [04600, 05004], lr: 0.076437, loss: 1.9796
2022-07-06 17:09:11 - train: epoch 0068, iter [04700, 05004], lr: 0.076424, loss: 2.0288
2022-07-06 17:09:45 - train: epoch 0068, iter [04800, 05004], lr: 0.076410, loss: 1.9117
2022-07-06 17:10:18 - train: epoch 0068, iter [04900, 05004], lr: 0.076396, loss: 1.9765
2022-07-06 17:10:50 - train: epoch 0068, iter [05000, 05004], lr: 0.076383, loss: 1.7782
2022-07-06 17:10:51 - train: epoch 068, train_loss: 1.7781
2022-07-06 17:12:05 - eval: epoch: 068, acc1: 59.778%, acc5: 83.556%, test_loss: 1.6900, per_image_load_time: 1.676ms, per_image_inference_time: 0.464ms
2022-07-06 17:12:05 - until epoch: 068, best_acc1: 62.268%
2022-07-06 17:12:05 - epoch 069 lr: 0.076382
2022-07-06 17:12:44 - train: epoch 0069, iter [00100, 05004], lr: 0.076368, loss: 2.0348
2022-07-06 17:13:17 - train: epoch 0069, iter [00200, 05004], lr: 0.076355, loss: 1.9807
2022-07-06 17:13:51 - train: epoch 0069, iter [00300, 05004], lr: 0.076341, loss: 1.7123
2022-07-06 17:14:23 - train: epoch 0069, iter [00400, 05004], lr: 0.076327, loss: 1.6527
2022-07-06 17:14:56 - train: epoch 0069, iter [00500, 05004], lr: 0.076314, loss: 1.5969
2022-07-06 17:15:30 - train: epoch 0069, iter [00600, 05004], lr: 0.076300, loss: 1.5980
2022-07-06 17:16:04 - train: epoch 0069, iter [00700, 05004], lr: 0.076286, loss: 1.6735
2022-07-06 17:16:36 - train: epoch 0069, iter [00800, 05004], lr: 0.076273, loss: 1.8235
2022-07-06 17:17:10 - train: epoch 0069, iter [00900, 05004], lr: 0.076259, loss: 1.7199
2022-07-06 17:17:44 - train: epoch 0069, iter [01000, 05004], lr: 0.076245, loss: 1.7328
2022-07-06 17:18:18 - train: epoch 0069, iter [01100, 05004], lr: 0.076231, loss: 1.7119
2022-07-06 17:18:51 - train: epoch 0069, iter [01200, 05004], lr: 0.076218, loss: 1.6904
2022-07-06 17:19:24 - train: epoch 0069, iter [01300, 05004], lr: 0.076204, loss: 2.0234
2022-07-06 17:19:58 - train: epoch 0069, iter [01400, 05004], lr: 0.076190, loss: 1.7825
2022-07-06 17:20:31 - train: epoch 0069, iter [01500, 05004], lr: 0.076177, loss: 1.8317
2022-07-06 17:21:05 - train: epoch 0069, iter [01600, 05004], lr: 0.076163, loss: 1.9608
2022-07-06 17:21:39 - train: epoch 0069, iter [01700, 05004], lr: 0.076149, loss: 1.7796
2022-07-06 17:22:11 - train: epoch 0069, iter [01800, 05004], lr: 0.076135, loss: 1.6426
2022-07-06 17:22:45 - train: epoch 0069, iter [01900, 05004], lr: 0.076122, loss: 1.7863
2022-07-06 17:23:19 - train: epoch 0069, iter [02000, 05004], lr: 0.076108, loss: 1.6283
2022-07-06 17:23:52 - train: epoch 0069, iter [02100, 05004], lr: 0.076094, loss: 1.8084
2022-07-06 17:24:27 - train: epoch 0069, iter [02200, 05004], lr: 0.076081, loss: 1.8619
2022-07-06 17:24:59 - train: epoch 0069, iter [02300, 05004], lr: 0.076067, loss: 1.7528
2022-07-06 17:25:33 - train: epoch 0069, iter [02400, 05004], lr: 0.076053, loss: 1.9151
2022-07-06 17:26:06 - train: epoch 0069, iter [02500, 05004], lr: 0.076039, loss: 1.7946
2022-07-06 17:26:40 - train: epoch 0069, iter [02600, 05004], lr: 0.076026, loss: 1.8190
2022-07-06 17:27:13 - train: epoch 0069, iter [02700, 05004], lr: 0.076012, loss: 2.0287
2022-07-06 17:27:47 - train: epoch 0069, iter [02800, 05004], lr: 0.075998, loss: 1.8046
2022-07-06 17:28:20 - train: epoch 0069, iter [02900, 05004], lr: 0.075984, loss: 1.6806
2022-07-06 17:28:53 - train: epoch 0069, iter [03000, 05004], lr: 0.075971, loss: 1.7865
2022-07-06 17:29:27 - train: epoch 0069, iter [03100, 05004], lr: 0.075957, loss: 1.7801
2022-07-06 17:30:00 - train: epoch 0069, iter [03200, 05004], lr: 0.075943, loss: 1.7366
2022-07-06 17:30:34 - train: epoch 0069, iter [03300, 05004], lr: 0.075929, loss: 1.6280
2022-07-06 17:31:07 - train: epoch 0069, iter [03400, 05004], lr: 0.075916, loss: 1.6257
2022-07-06 17:31:40 - train: epoch 0069, iter [03500, 05004], lr: 0.075902, loss: 1.5726
2022-07-06 17:32:13 - train: epoch 0069, iter [03600, 05004], lr: 0.075888, loss: 1.5938
2022-07-06 17:32:47 - train: epoch 0069, iter [03700, 05004], lr: 0.075874, loss: 1.9178
2022-07-06 17:33:20 - train: epoch 0069, iter [03800, 05004], lr: 0.075860, loss: 1.8588
2022-07-06 17:33:53 - train: epoch 0069, iter [03900, 05004], lr: 0.075847, loss: 1.8679
2022-07-06 17:34:27 - train: epoch 0069, iter [04000, 05004], lr: 0.075833, loss: 1.8465
2022-07-06 17:34:59 - train: epoch 0069, iter [04100, 05004], lr: 0.075819, loss: 1.9989
2022-07-06 17:35:33 - train: epoch 0069, iter [04200, 05004], lr: 0.075805, loss: 1.5838
2022-07-06 17:36:06 - train: epoch 0069, iter [04300, 05004], lr: 0.075791, loss: 1.8761
2022-07-06 17:36:40 - train: epoch 0069, iter [04400, 05004], lr: 0.075778, loss: 1.7633
2022-07-06 17:37:13 - train: epoch 0069, iter [04500, 05004], lr: 0.075764, loss: 1.7368
2022-07-06 17:37:47 - train: epoch 0069, iter [04600, 05004], lr: 0.075750, loss: 1.9723
2022-07-06 17:38:21 - train: epoch 0069, iter [04700, 05004], lr: 0.075736, loss: 1.8179
2022-07-06 17:38:54 - train: epoch 0069, iter [04800, 05004], lr: 0.075723, loss: 1.7858
2022-07-06 17:39:27 - train: epoch 0069, iter [04900, 05004], lr: 0.075709, loss: 1.6370
2022-07-06 17:39:59 - train: epoch 0069, iter [05000, 05004], lr: 0.075695, loss: 1.8764
2022-07-06 17:40:00 - train: epoch 069, train_loss: 1.7733
2022-07-06 17:41:14 - eval: epoch: 069, acc1: 57.452%, acc5: 81.394%, test_loss: 1.8114, per_image_load_time: 0.812ms, per_image_inference_time: 0.441ms
2022-07-06 17:41:15 - until epoch: 069, best_acc1: 62.268%
2022-07-06 17:41:15 - epoch 070 lr: 0.075694
2022-07-06 17:41:53 - train: epoch 0070, iter [00100, 05004], lr: 0.075681, loss: 1.8195
2022-07-06 17:42:26 - train: epoch 0070, iter [00200, 05004], lr: 0.075667, loss: 1.7312
2022-07-06 17:43:00 - train: epoch 0070, iter [00300, 05004], lr: 0.075653, loss: 1.7882
2022-07-06 17:43:33 - train: epoch 0070, iter [00400, 05004], lr: 0.075639, loss: 1.7394
2022-07-06 17:44:06 - train: epoch 0070, iter [00500, 05004], lr: 0.075625, loss: 1.8790
2022-07-06 17:44:39 - train: epoch 0070, iter [00600, 05004], lr: 0.075611, loss: 1.6978
2022-07-06 17:45:13 - train: epoch 0070, iter [00700, 05004], lr: 0.075598, loss: 1.6185
2022-07-06 17:45:46 - train: epoch 0070, iter [00800, 05004], lr: 0.075584, loss: 1.5421
2022-07-06 17:46:19 - train: epoch 0070, iter [00900, 05004], lr: 0.075570, loss: 1.7921
2022-07-06 17:46:52 - train: epoch 0070, iter [01000, 05004], lr: 0.075556, loss: 1.7025
2022-07-06 17:47:26 - train: epoch 0070, iter [01100, 05004], lr: 0.075542, loss: 2.1042
2022-07-06 17:47:59 - train: epoch 0070, iter [01200, 05004], lr: 0.075528, loss: 1.4854
2022-07-06 17:48:32 - train: epoch 0070, iter [01300, 05004], lr: 0.075515, loss: 1.6801
2022-07-06 17:49:06 - train: epoch 0070, iter [01400, 05004], lr: 0.075501, loss: 1.7648
2022-07-06 17:49:39 - train: epoch 0070, iter [01500, 05004], lr: 0.075487, loss: 1.7488
2022-07-06 17:50:12 - train: epoch 0070, iter [01600, 05004], lr: 0.075473, loss: 1.7686
2022-07-06 17:50:46 - train: epoch 0070, iter [01700, 05004], lr: 0.075459, loss: 1.6962
2022-07-06 17:51:20 - train: epoch 0070, iter [01800, 05004], lr: 0.075445, loss: 1.6615
2022-07-06 17:51:53 - train: epoch 0070, iter [01900, 05004], lr: 0.075431, loss: 1.6848
2022-07-06 17:52:25 - train: epoch 0070, iter [02000, 05004], lr: 0.075418, loss: 1.8614
2022-07-06 17:52:59 - train: epoch 0070, iter [02100, 05004], lr: 0.075404, loss: 1.8931
2022-07-06 17:53:33 - train: epoch 0070, iter [02200, 05004], lr: 0.075390, loss: 1.7779
2022-07-06 17:54:06 - train: epoch 0070, iter [02300, 05004], lr: 0.075376, loss: 1.7741
2022-07-06 17:54:40 - train: epoch 0070, iter [02400, 05004], lr: 0.075362, loss: 1.8773
2022-07-06 17:55:13 - train: epoch 0070, iter [02500, 05004], lr: 0.075348, loss: 1.7853
2022-07-06 17:55:46 - train: epoch 0070, iter [02600, 05004], lr: 0.075334, loss: 1.7250
2022-07-06 17:56:20 - train: epoch 0070, iter [02700, 05004], lr: 0.075321, loss: 1.8199
2022-07-06 17:56:54 - train: epoch 0070, iter [02800, 05004], lr: 0.075307, loss: 1.7883
2022-07-06 17:57:26 - train: epoch 0070, iter [02900, 05004], lr: 0.075293, loss: 1.9990
2022-07-06 17:58:00 - train: epoch 0070, iter [03000, 05004], lr: 0.075279, loss: 1.7184
2022-07-06 17:58:33 - train: epoch 0070, iter [03100, 05004], lr: 0.075265, loss: 1.7680
2022-07-06 17:59:07 - train: epoch 0070, iter [03200, 05004], lr: 0.075251, loss: 1.9337
2022-07-06 17:59:42 - train: epoch 0070, iter [03300, 05004], lr: 0.075237, loss: 1.8202
2022-07-06 18:00:15 - train: epoch 0070, iter [03400, 05004], lr: 0.075223, loss: 1.7562
2022-07-06 18:00:49 - train: epoch 0070, iter [03500, 05004], lr: 0.075209, loss: 1.7053
2022-07-06 18:01:22 - train: epoch 0070, iter [03600, 05004], lr: 0.075195, loss: 1.8645
2022-07-06 18:01:56 - train: epoch 0070, iter [03700, 05004], lr: 0.075182, loss: 1.8320
2022-07-06 18:02:29 - train: epoch 0070, iter [03800, 05004], lr: 0.075168, loss: 1.7706
2022-07-06 18:03:02 - train: epoch 0070, iter [03900, 05004], lr: 0.075154, loss: 1.6484
2022-07-06 18:03:35 - train: epoch 0070, iter [04000, 05004], lr: 0.075140, loss: 1.7783
2022-07-06 18:04:09 - train: epoch 0070, iter [04100, 05004], lr: 0.075126, loss: 1.7110
2022-07-06 18:04:43 - train: epoch 0070, iter [04200, 05004], lr: 0.075112, loss: 1.8009
2022-07-06 18:05:16 - train: epoch 0070, iter [04300, 05004], lr: 0.075098, loss: 1.8904
2022-07-06 18:05:50 - train: epoch 0070, iter [04400, 05004], lr: 0.075084, loss: 1.7374
2022-07-06 18:06:24 - train: epoch 0070, iter [04500, 05004], lr: 0.075070, loss: 1.9035
2022-07-06 18:06:57 - train: epoch 0070, iter [04600, 05004], lr: 0.075056, loss: 1.8338
2022-07-06 18:07:30 - train: epoch 0070, iter [04700, 05004], lr: 0.075042, loss: 1.8729
2022-07-06 18:08:04 - train: epoch 0070, iter [04800, 05004], lr: 0.075028, loss: 1.8641
2022-07-06 18:08:37 - train: epoch 0070, iter [04900, 05004], lr: 0.075014, loss: 1.8716
2022-07-06 18:09:09 - train: epoch 0070, iter [05000, 05004], lr: 0.075001, loss: 1.8876
2022-07-06 18:09:10 - train: epoch 070, train_loss: 1.7697
2022-07-06 18:10:24 - eval: epoch: 070, acc1: 59.282%, acc5: 82.996%, test_loss: 1.7090, per_image_load_time: 1.229ms, per_image_inference_time: 0.479ms
2022-07-06 18:10:24 - until epoch: 070, best_acc1: 62.268%
2022-07-06 18:10:24 - epoch 071 lr: 0.075000
2022-07-06 18:11:04 - train: epoch 0071, iter [00100, 05004], lr: 0.074986, loss: 1.5140
2022-07-06 18:11:36 - train: epoch 0071, iter [00200, 05004], lr: 0.074972, loss: 1.6367
2022-07-06 18:12:10 - train: epoch 0071, iter [00300, 05004], lr: 0.074958, loss: 1.6965
2022-07-06 18:12:43 - train: epoch 0071, iter [00400, 05004], lr: 0.074944, loss: 1.8898
2022-07-06 18:13:17 - train: epoch 0071, iter [00500, 05004], lr: 0.074930, loss: 1.7803
2022-07-06 18:13:49 - train: epoch 0071, iter [00600, 05004], lr: 0.074916, loss: 1.9140
2022-07-06 18:14:23 - train: epoch 0071, iter [00700, 05004], lr: 0.074902, loss: 1.8814
2022-07-06 18:14:56 - train: epoch 0071, iter [00800, 05004], lr: 0.074888, loss: 1.8160
2022-07-06 18:15:30 - train: epoch 0071, iter [00900, 05004], lr: 0.074874, loss: 1.8676
2022-07-06 18:16:03 - train: epoch 0071, iter [01000, 05004], lr: 0.074860, loss: 1.9270
2022-07-06 18:16:36 - train: epoch 0071, iter [01100, 05004], lr: 0.074846, loss: 1.8635
2022-07-06 18:17:10 - train: epoch 0071, iter [01200, 05004], lr: 0.074833, loss: 1.6909
2022-07-06 18:17:43 - train: epoch 0071, iter [01300, 05004], lr: 0.074819, loss: 1.8902
2022-07-06 18:18:16 - train: epoch 0071, iter [01400, 05004], lr: 0.074805, loss: 1.7682
2022-07-06 18:18:50 - train: epoch 0071, iter [01500, 05004], lr: 0.074791, loss: 1.4569
2022-07-06 18:19:23 - train: epoch 0071, iter [01600, 05004], lr: 0.074777, loss: 1.5639
2022-07-06 18:19:56 - train: epoch 0071, iter [01700, 05004], lr: 0.074763, loss: 1.6769
2022-07-06 18:20:30 - train: epoch 0071, iter [01800, 05004], lr: 0.074749, loss: 1.7959
2022-07-06 18:21:04 - train: epoch 0071, iter [01900, 05004], lr: 0.074735, loss: 1.7846
2022-07-06 18:21:37 - train: epoch 0071, iter [02000, 05004], lr: 0.074721, loss: 1.8158
2022-07-06 18:22:11 - train: epoch 0071, iter [02100, 05004], lr: 0.074707, loss: 1.5959
2022-07-06 18:22:44 - train: epoch 0071, iter [02200, 05004], lr: 0.074693, loss: 1.5830
2022-07-06 18:23:17 - train: epoch 0071, iter [02300, 05004], lr: 0.074679, loss: 1.7490
2022-07-06 18:23:51 - train: epoch 0071, iter [02400, 05004], lr: 0.074665, loss: 1.6834
2022-07-06 18:24:24 - train: epoch 0071, iter [02500, 05004], lr: 0.074651, loss: 1.8515
2022-07-06 18:24:58 - train: epoch 0071, iter [02600, 05004], lr: 0.074637, loss: 1.5498
2022-07-06 18:25:30 - train: epoch 0071, iter [02700, 05004], lr: 0.074623, loss: 1.8601
2022-07-06 18:26:05 - train: epoch 0071, iter [02800, 05004], lr: 0.074609, loss: 1.8081
2022-07-06 18:26:38 - train: epoch 0071, iter [02900, 05004], lr: 0.074595, loss: 1.6313
2022-07-06 18:27:13 - train: epoch 0071, iter [03000, 05004], lr: 0.074581, loss: 1.8497
2022-07-06 18:27:46 - train: epoch 0071, iter [03100, 05004], lr: 0.074567, loss: 1.6583
2022-07-06 18:28:20 - train: epoch 0071, iter [03200, 05004], lr: 0.074553, loss: 1.6017
2022-07-06 18:28:52 - train: epoch 0071, iter [03300, 05004], lr: 0.074539, loss: 1.7410
2022-07-06 18:29:26 - train: epoch 0071, iter [03400, 05004], lr: 0.074525, loss: 1.6775
2022-07-06 18:30:00 - train: epoch 0071, iter [03500, 05004], lr: 0.074510, loss: 1.9726
2022-07-06 18:30:33 - train: epoch 0071, iter [03600, 05004], lr: 0.074496, loss: 1.9815
2022-07-06 18:31:07 - train: epoch 0071, iter [03700, 05004], lr: 0.074482, loss: 1.8867
2022-07-06 18:31:39 - train: epoch 0071, iter [03800, 05004], lr: 0.074468, loss: 1.6873
2022-07-06 18:32:13 - train: epoch 0071, iter [03900, 05004], lr: 0.074454, loss: 1.8292
2022-07-06 18:32:47 - train: epoch 0071, iter [04000, 05004], lr: 0.074440, loss: 1.9623
2022-07-06 18:33:20 - train: epoch 0071, iter [04100, 05004], lr: 0.074426, loss: 1.6892
2022-07-06 18:33:54 - train: epoch 0071, iter [04200, 05004], lr: 0.074412, loss: 1.8927
2022-07-06 18:34:27 - train: epoch 0071, iter [04300, 05004], lr: 0.074398, loss: 1.8199
2022-07-06 18:35:00 - train: epoch 0071, iter [04400, 05004], lr: 0.074384, loss: 1.8461
2022-07-06 18:35:33 - train: epoch 0071, iter [04500, 05004], lr: 0.074370, loss: 1.8771
2022-07-06 18:36:07 - train: epoch 0071, iter [04600, 05004], lr: 0.074356, loss: 1.7914
2022-07-06 18:36:41 - train: epoch 0071, iter [04700, 05004], lr: 0.074342, loss: 1.7165
2022-07-06 18:37:14 - train: epoch 0071, iter [04800, 05004], lr: 0.074328, loss: 1.6678
2022-07-06 18:37:47 - train: epoch 0071, iter [04900, 05004], lr: 0.074314, loss: 1.6657
2022-07-06 18:38:20 - train: epoch 0071, iter [05000, 05004], lr: 0.074300, loss: 1.6220
2022-07-06 18:38:21 - train: epoch 071, train_loss: 1.7624
2022-07-06 18:39:36 - eval: epoch: 071, acc1: 61.346%, acc5: 84.614%, test_loss: 1.6040, per_image_load_time: 0.710ms, per_image_inference_time: 0.460ms
2022-07-06 18:39:36 - until epoch: 071, best_acc1: 62.268%
2022-07-06 18:39:36 - epoch 072 lr: 0.074299
2022-07-06 18:40:15 - train: epoch 0072, iter [00100, 05004], lr: 0.074285, loss: 1.8174
2022-07-06 18:40:48 - train: epoch 0072, iter [00200, 05004], lr: 0.074271, loss: 1.5296
2022-07-06 18:41:22 - train: epoch 0072, iter [00300, 05004], lr: 0.074257, loss: 1.5158
2022-07-06 18:41:55 - train: epoch 0072, iter [00400, 05004], lr: 0.074243, loss: 1.5914
2022-07-06 18:42:28 - train: epoch 0072, iter [00500, 05004], lr: 0.074229, loss: 1.6011
2022-07-06 18:43:02 - train: epoch 0072, iter [00600, 05004], lr: 0.074215, loss: 1.6219
2022-07-06 18:43:34 - train: epoch 0072, iter [00700, 05004], lr: 0.074201, loss: 1.6812
2022-07-06 18:44:08 - train: epoch 0072, iter [00800, 05004], lr: 0.074187, loss: 2.0696
2022-07-06 18:44:41 - train: epoch 0072, iter [00900, 05004], lr: 0.074172, loss: 1.5861
2022-07-06 18:45:15 - train: epoch 0072, iter [01000, 05004], lr: 0.074158, loss: 1.7358
2022-07-06 18:45:48 - train: epoch 0072, iter [01100, 05004], lr: 0.074144, loss: 1.9811
2022-07-06 18:46:21 - train: epoch 0072, iter [01200, 05004], lr: 0.074130, loss: 1.6571
2022-07-06 18:46:55 - train: epoch 0072, iter [01300, 05004], lr: 0.074116, loss: 1.6896
2022-07-06 18:47:28 - train: epoch 0072, iter [01400, 05004], lr: 0.074102, loss: 1.8678
2022-07-06 18:48:02 - train: epoch 0072, iter [01500, 05004], lr: 0.074088, loss: 1.7514
2022-07-06 18:48:35 - train: epoch 0072, iter [01600, 05004], lr: 0.074074, loss: 1.7974
2022-07-06 18:49:09 - train: epoch 0072, iter [01700, 05004], lr: 0.074060, loss: 1.6541
2022-07-06 18:49:42 - train: epoch 0072, iter [01800, 05004], lr: 0.074046, loss: 1.5578
2022-07-06 18:50:14 - train: epoch 0072, iter [01900, 05004], lr: 0.074031, loss: 1.6622
2022-07-06 18:50:49 - train: epoch 0072, iter [02000, 05004], lr: 0.074017, loss: 1.8672
2022-07-06 18:51:22 - train: epoch 0072, iter [02100, 05004], lr: 0.074003, loss: 1.8475
2022-07-06 18:51:56 - train: epoch 0072, iter [02200, 05004], lr: 0.073989, loss: 1.9277
2022-07-06 18:52:28 - train: epoch 0072, iter [02300, 05004], lr: 0.073975, loss: 2.0048
2022-07-06 18:53:03 - train: epoch 0072, iter [02400, 05004], lr: 0.073961, loss: 1.6269
2022-07-06 18:53:36 - train: epoch 0072, iter [02500, 05004], lr: 0.073947, loss: 1.5548
2022-07-06 18:54:10 - train: epoch 0072, iter [02600, 05004], lr: 0.073933, loss: 1.6313
2022-07-06 18:54:43 - train: epoch 0072, iter [02700, 05004], lr: 0.073918, loss: 1.8049
2022-07-06 18:55:17 - train: epoch 0072, iter [02800, 05004], lr: 0.073904, loss: 1.9075
2022-07-06 18:55:51 - train: epoch 0072, iter [02900, 05004], lr: 0.073890, loss: 1.6015
2022-07-06 18:56:25 - train: epoch 0072, iter [03000, 05004], lr: 0.073876, loss: 1.8277
2022-07-06 18:56:58 - train: epoch 0072, iter [03100, 05004], lr: 0.073862, loss: 1.9238
2022-07-06 18:57:32 - train: epoch 0072, iter [03200, 05004], lr: 0.073848, loss: 1.6738
2022-07-06 18:58:05 - train: epoch 0072, iter [03300, 05004], lr: 0.073834, loss: 1.7898
2022-07-06 18:58:39 - train: epoch 0072, iter [03400, 05004], lr: 0.073819, loss: 1.8892
2022-07-06 18:59:13 - train: epoch 0072, iter [03500, 05004], lr: 0.073805, loss: 1.7958
2022-07-06 18:59:45 - train: epoch 0072, iter [03600, 05004], lr: 0.073791, loss: 1.7598
2022-07-06 19:00:20 - train: epoch 0072, iter [03700, 05004], lr: 0.073777, loss: 2.0123
2022-07-06 19:00:53 - train: epoch 0072, iter [03800, 05004], lr: 0.073763, loss: 1.8276
2022-07-06 19:01:26 - train: epoch 0072, iter [03900, 05004], lr: 0.073749, loss: 1.9044
2022-07-06 19:02:00 - train: epoch 0072, iter [04000, 05004], lr: 0.073734, loss: 1.7071
2022-07-06 19:02:33 - train: epoch 0072, iter [04100, 05004], lr: 0.073720, loss: 1.9271
2022-07-06 19:03:07 - train: epoch 0072, iter [04200, 05004], lr: 0.073706, loss: 1.7817
2022-07-06 19:03:41 - train: epoch 0072, iter [04300, 05004], lr: 0.073692, loss: 1.8105
2022-07-06 19:04:15 - train: epoch 0072, iter [04400, 05004], lr: 0.073678, loss: 1.6013
2022-07-06 19:04:49 - train: epoch 0072, iter [04500, 05004], lr: 0.073664, loss: 1.9767
2022-07-06 19:05:23 - train: epoch 0072, iter [04600, 05004], lr: 0.073649, loss: 1.7081
2022-07-06 19:05:56 - train: epoch 0072, iter [04700, 05004], lr: 0.073635, loss: 1.9086
2022-07-06 19:06:29 - train: epoch 0072, iter [04800, 05004], lr: 0.073621, loss: 1.9885
2022-07-06 19:07:03 - train: epoch 0072, iter [04900, 05004], lr: 0.073607, loss: 1.8424
2022-07-06 19:07:35 - train: epoch 0072, iter [05000, 05004], lr: 0.073593, loss: 1.7474
2022-07-06 19:07:36 - train: epoch 072, train_loss: 1.7601
2022-07-06 19:08:50 - eval: epoch: 072, acc1: 61.528%, acc5: 84.588%, test_loss: 1.5981, per_image_load_time: 1.391ms, per_image_inference_time: 0.463ms
2022-07-06 19:08:50 - until epoch: 072, best_acc1: 62.268%
2022-07-06 19:08:50 - epoch 073 lr: 0.073592
2022-07-06 19:09:28 - train: epoch 0073, iter [00100, 05004], lr: 0.073578, loss: 2.0031
2022-07-06 19:10:01 - train: epoch 0073, iter [00200, 05004], lr: 0.073564, loss: 1.7010
2022-07-06 19:10:35 - train: epoch 0073, iter [00300, 05004], lr: 0.073549, loss: 1.7284
2022-07-06 19:11:08 - train: epoch 0073, iter [00400, 05004], lr: 0.073535, loss: 1.5102
2022-07-06 19:11:42 - train: epoch 0073, iter [00500, 05004], lr: 0.073521, loss: 1.5249
2022-07-06 19:12:15 - train: epoch 0073, iter [00600, 05004], lr: 0.073507, loss: 1.8203
2022-07-06 19:12:49 - train: epoch 0073, iter [00700, 05004], lr: 0.073493, loss: 1.8439
2022-07-06 19:13:22 - train: epoch 0073, iter [00800, 05004], lr: 0.073478, loss: 1.6297
2022-07-06 19:13:56 - train: epoch 0073, iter [00900, 05004], lr: 0.073464, loss: 1.5400
2022-07-06 19:14:30 - train: epoch 0073, iter [01000, 05004], lr: 0.073450, loss: 1.6417
2022-07-06 19:15:03 - train: epoch 0073, iter [01100, 05004], lr: 0.073436, loss: 1.8527
2022-07-06 19:15:37 - train: epoch 0073, iter [01200, 05004], lr: 0.073422, loss: 1.7248
2022-07-06 19:16:11 - train: epoch 0073, iter [01300, 05004], lr: 0.073407, loss: 1.7004
2022-07-06 19:16:44 - train: epoch 0073, iter [01400, 05004], lr: 0.073393, loss: 1.6237
2022-07-06 19:17:17 - train: epoch 0073, iter [01500, 05004], lr: 0.073379, loss: 1.7154
2022-07-06 19:17:51 - train: epoch 0073, iter [01600, 05004], lr: 0.073365, loss: 1.7856
2022-07-06 19:18:24 - train: epoch 0073, iter [01700, 05004], lr: 0.073350, loss: 2.0197
2022-07-06 19:18:57 - train: epoch 0073, iter [01800, 05004], lr: 0.073336, loss: 1.5228
2022-07-06 19:19:31 - train: epoch 0073, iter [01900, 05004], lr: 0.073322, loss: 1.8623
2022-07-06 19:20:05 - train: epoch 0073, iter [02000, 05004], lr: 0.073308, loss: 1.4939
2022-07-06 19:20:38 - train: epoch 0073, iter [02100, 05004], lr: 0.073293, loss: 1.7737
2022-07-06 19:21:11 - train: epoch 0073, iter [02200, 05004], lr: 0.073279, loss: 1.8860
2022-07-06 19:21:44 - train: epoch 0073, iter [02300, 05004], lr: 0.073265, loss: 1.8725
2022-07-06 19:22:18 - train: epoch 0073, iter [02400, 05004], lr: 0.073251, loss: 1.7111
2022-07-06 19:22:51 - train: epoch 0073, iter [02500, 05004], lr: 0.073236, loss: 1.9588
2022-07-06 19:23:25 - train: epoch 0073, iter [02600, 05004], lr: 0.073222, loss: 1.8877
2022-07-06 19:23:59 - train: epoch 0073, iter [02700, 05004], lr: 0.073208, loss: 1.7642
2022-07-06 19:24:32 - train: epoch 0073, iter [02800, 05004], lr: 0.073194, loss: 1.6834
2022-07-06 19:25:06 - train: epoch 0073, iter [02900, 05004], lr: 0.073179, loss: 1.8672
2022-07-06 19:25:40 - train: epoch 0073, iter [03000, 05004], lr: 0.073165, loss: 1.5298
2022-07-06 19:26:13 - train: epoch 0073, iter [03100, 05004], lr: 0.073151, loss: 1.6466
2022-07-06 19:26:46 - train: epoch 0073, iter [03200, 05004], lr: 0.073137, loss: 1.5739
2022-07-06 19:27:19 - train: epoch 0073, iter [03300, 05004], lr: 0.073122, loss: 1.8250
2022-07-06 19:27:53 - train: epoch 0073, iter [03400, 05004], lr: 0.073108, loss: 1.7597
2022-07-06 19:28:27 - train: epoch 0073, iter [03500, 05004], lr: 0.073094, loss: 1.7841
2022-07-06 19:29:00 - train: epoch 0073, iter [03600, 05004], lr: 0.073080, loss: 1.8356
2022-07-06 19:29:33 - train: epoch 0073, iter [03700, 05004], lr: 0.073065, loss: 1.7805
2022-07-06 19:30:07 - train: epoch 0073, iter [03800, 05004], lr: 0.073051, loss: 2.0180
2022-07-06 19:30:40 - train: epoch 0073, iter [03900, 05004], lr: 0.073037, loss: 1.8010
2022-07-06 19:31:13 - train: epoch 0073, iter [04000, 05004], lr: 0.073022, loss: 1.9748
2022-07-06 19:31:46 - train: epoch 0073, iter [04100, 05004], lr: 0.073008, loss: 1.7479
2022-07-06 19:32:20 - train: epoch 0073, iter [04200, 05004], lr: 0.072994, loss: 1.9305
2022-07-06 19:32:54 - train: epoch 0073, iter [04300, 05004], lr: 0.072979, loss: 1.7099
2022-07-06 19:33:28 - train: epoch 0073, iter [04400, 05004], lr: 0.072965, loss: 1.8554
2022-07-06 19:34:01 - train: epoch 0073, iter [04500, 05004], lr: 0.072951, loss: 1.7618
2022-07-06 19:34:34 - train: epoch 0073, iter [04600, 05004], lr: 0.072937, loss: 1.9550
2022-07-06 19:35:07 - train: epoch 0073, iter [04700, 05004], lr: 0.072922, loss: 1.5599
2022-07-06 19:35:40 - train: epoch 0073, iter [04800, 05004], lr: 0.072908, loss: 1.9062
2022-07-06 19:36:14 - train: epoch 0073, iter [04900, 05004], lr: 0.072894, loss: 1.6866
2022-07-06 19:36:47 - train: epoch 0073, iter [05000, 05004], lr: 0.072879, loss: 1.8815
2022-07-06 19:36:48 - train: epoch 073, train_loss: 1.7537
2022-07-06 19:38:02 - eval: epoch: 073, acc1: 61.448%, acc5: 84.472%, test_loss: 1.6017, per_image_load_time: 0.995ms, per_image_inference_time: 0.456ms
2022-07-06 19:38:02 - until epoch: 073, best_acc1: 62.268%
2022-07-06 19:38:02 - epoch 074 lr: 0.072879
2022-07-06 19:38:42 - train: epoch 0074, iter [00100, 05004], lr: 0.072864, loss: 1.4983
2022-07-06 19:39:15 - train: epoch 0074, iter [00200, 05004], lr: 0.072850, loss: 1.6555
2022-07-06 19:39:49 - train: epoch 0074, iter [00300, 05004], lr: 0.072836, loss: 1.8086
2022-07-06 19:40:22 - train: epoch 0074, iter [00400, 05004], lr: 0.072822, loss: 1.9821
2022-07-06 19:40:56 - train: epoch 0074, iter [00500, 05004], lr: 0.072807, loss: 1.6437
2022-07-06 19:41:30 - train: epoch 0074, iter [00600, 05004], lr: 0.072793, loss: 1.7836
2022-07-06 19:42:02 - train: epoch 0074, iter [00700, 05004], lr: 0.072779, loss: 1.7764
2022-07-06 19:42:36 - train: epoch 0074, iter [00800, 05004], lr: 0.072764, loss: 1.8499
2022-07-06 19:43:10 - train: epoch 0074, iter [00900, 05004], lr: 0.072750, loss: 1.6132
2022-07-06 19:43:45 - train: epoch 0074, iter [01000, 05004], lr: 0.072736, loss: 1.9032
2022-07-06 19:44:17 - train: epoch 0074, iter [01100, 05004], lr: 0.072721, loss: 1.9645
2022-07-06 19:44:51 - train: epoch 0074, iter [01200, 05004], lr: 0.072707, loss: 1.6089
2022-07-06 19:45:24 - train: epoch 0074, iter [01300, 05004], lr: 0.072692, loss: 1.8785
2022-07-06 19:45:57 - train: epoch 0074, iter [01400, 05004], lr: 0.072678, loss: 1.6632
2022-07-06 19:46:31 - train: epoch 0074, iter [01500, 05004], lr: 0.072664, loss: 1.8786
2022-07-06 19:47:05 - train: epoch 0074, iter [01600, 05004], lr: 0.072649, loss: 1.5016
2022-07-06 19:47:39 - train: epoch 0074, iter [01700, 05004], lr: 0.072635, loss: 1.9466
2022-07-06 19:48:13 - train: epoch 0074, iter [01800, 05004], lr: 0.072621, loss: 2.0852
2022-07-06 19:48:46 - train: epoch 0074, iter [01900, 05004], lr: 0.072606, loss: 1.8488
2022-07-06 19:49:20 - train: epoch 0074, iter [02000, 05004], lr: 0.072592, loss: 1.4606
2022-07-06 19:49:54 - train: epoch 0074, iter [02100, 05004], lr: 0.072578, loss: 1.7191
2022-07-06 19:50:27 - train: epoch 0074, iter [02200, 05004], lr: 0.072563, loss: 2.1067
2022-07-06 19:51:00 - train: epoch 0074, iter [02300, 05004], lr: 0.072549, loss: 1.8457
2022-07-06 19:51:34 - train: epoch 0074, iter [02400, 05004], lr: 0.072535, loss: 1.7372
2022-07-06 19:52:07 - train: epoch 0074, iter [02500, 05004], lr: 0.072520, loss: 1.6237
2022-07-06 19:52:40 - train: epoch 0074, iter [02600, 05004], lr: 0.072506, loss: 1.4381
2022-07-06 19:53:15 - train: epoch 0074, iter [02700, 05004], lr: 0.072491, loss: 1.7633
2022-07-06 19:53:48 - train: epoch 0074, iter [02800, 05004], lr: 0.072477, loss: 2.0880
2022-07-06 19:54:21 - train: epoch 0074, iter [02900, 05004], lr: 0.072463, loss: 1.7669
2022-07-06 19:54:54 - train: epoch 0074, iter [03000, 05004], lr: 0.072448, loss: 1.7517
2022-07-06 19:55:28 - train: epoch 0074, iter [03100, 05004], lr: 0.072434, loss: 1.7831
2022-07-06 19:56:01 - train: epoch 0074, iter [03200, 05004], lr: 0.072420, loss: 1.6828
2022-07-06 19:56:34 - train: epoch 0074, iter [03300, 05004], lr: 0.072405, loss: 1.8797
2022-07-06 19:57:08 - train: epoch 0074, iter [03400, 05004], lr: 0.072391, loss: 1.8414
2022-07-06 19:57:42 - train: epoch 0074, iter [03500, 05004], lr: 0.072376, loss: 1.5737
2022-07-06 19:58:16 - train: epoch 0074, iter [03600, 05004], lr: 0.072362, loss: 1.7639
2022-07-06 19:58:49 - train: epoch 0074, iter [03700, 05004], lr: 0.072348, loss: 1.8372
2022-07-06 19:59:23 - train: epoch 0074, iter [03800, 05004], lr: 0.072333, loss: 1.7346
2022-07-06 19:59:57 - train: epoch 0074, iter [03900, 05004], lr: 0.072319, loss: 1.5089
2022-07-06 20:00:31 - train: epoch 0074, iter [04000, 05004], lr: 0.072304, loss: 1.6432
2022-07-06 20:01:04 - train: epoch 0074, iter [04100, 05004], lr: 0.072290, loss: 1.9558
2022-07-06 20:01:37 - train: epoch 0074, iter [04200, 05004], lr: 0.072276, loss: 1.8049
2022-07-06 20:02:11 - train: epoch 0074, iter [04300, 05004], lr: 0.072261, loss: 1.8155
2022-07-06 20:02:45 - train: epoch 0074, iter [04400, 05004], lr: 0.072247, loss: 1.5946
2022-07-06 20:03:19 - train: epoch 0074, iter [04500, 05004], lr: 0.072232, loss: 1.6394
2022-07-06 20:03:53 - train: epoch 0074, iter [04600, 05004], lr: 0.072218, loss: 1.8187
2022-07-06 20:04:26 - train: epoch 0074, iter [04700, 05004], lr: 0.072203, loss: 1.8432
2022-07-06 20:05:00 - train: epoch 0074, iter [04800, 05004], lr: 0.072189, loss: 1.8371
2022-07-06 20:05:33 - train: epoch 0074, iter [04900, 05004], lr: 0.072175, loss: 1.9911
2022-07-06 20:06:06 - train: epoch 0074, iter [05000, 05004], lr: 0.072160, loss: 1.6760
2022-07-06 20:06:07 - train: epoch 074, train_loss: 1.7466
2022-07-06 20:07:22 - eval: epoch: 074, acc1: 61.448%, acc5: 84.564%, test_loss: 1.5973, per_image_load_time: 0.845ms, per_image_inference_time: 0.449ms
2022-07-06 20:07:22 - until epoch: 074, best_acc1: 62.268%
2022-07-06 20:07:22 - epoch 075 lr: 0.072159
2022-07-06 20:08:00 - train: epoch 0075, iter [00100, 05004], lr: 0.072145, loss: 1.8467
2022-07-06 20:08:34 - train: epoch 0075, iter [00200, 05004], lr: 0.072131, loss: 1.6606
2022-07-06 20:09:08 - train: epoch 0075, iter [00300, 05004], lr: 0.072116, loss: 1.6178
2022-07-06 20:09:41 - train: epoch 0075, iter [00400, 05004], lr: 0.072102, loss: 1.6955
2022-07-06 20:10:14 - train: epoch 0075, iter [00500, 05004], lr: 0.072087, loss: 1.5271
2022-07-06 20:10:47 - train: epoch 0075, iter [00600, 05004], lr: 0.072073, loss: 1.5923
2022-07-06 20:11:20 - train: epoch 0075, iter [00700, 05004], lr: 0.072059, loss: 1.8476
2022-07-06 20:11:53 - train: epoch 0075, iter [00800, 05004], lr: 0.072044, loss: 1.8684
2022-07-06 20:12:27 - train: epoch 0075, iter [00900, 05004], lr: 0.072030, loss: 1.8259
2022-07-06 20:13:00 - train: epoch 0075, iter [01000, 05004], lr: 0.072015, loss: 1.5950
2022-07-06 20:13:33 - train: epoch 0075, iter [01100, 05004], lr: 0.072001, loss: 1.7772
2022-07-06 20:14:07 - train: epoch 0075, iter [01200, 05004], lr: 0.071986, loss: 1.6536
2022-07-06 20:14:41 - train: epoch 0075, iter [01300, 05004], lr: 0.071972, loss: 1.5314
2022-07-06 20:15:13 - train: epoch 0075, iter [01400, 05004], lr: 0.071957, loss: 1.6571
2022-07-06 20:15:48 - train: epoch 0075, iter [01500, 05004], lr: 0.071943, loss: 1.8698
2022-07-06 20:16:20 - train: epoch 0075, iter [01600, 05004], lr: 0.071928, loss: 1.3581
2022-07-06 20:16:54 - train: epoch 0075, iter [01700, 05004], lr: 0.071914, loss: 1.7506
2022-07-06 20:17:28 - train: epoch 0075, iter [01800, 05004], lr: 0.071899, loss: 1.6705
2022-07-06 20:18:01 - train: epoch 0075, iter [01900, 05004], lr: 0.071885, loss: 1.5656
2022-07-06 20:18:35 - train: epoch 0075, iter [02000, 05004], lr: 0.071871, loss: 1.7489
2022-07-06 20:19:08 - train: epoch 0075, iter [02100, 05004], lr: 0.071856, loss: 1.5557
2022-07-06 20:19:42 - train: epoch 0075, iter [02200, 05004], lr: 0.071842, loss: 1.7690
2022-07-06 20:20:15 - train: epoch 0075, iter [02300, 05004], lr: 0.071827, loss: 1.6283
2022-07-06 20:20:49 - train: epoch 0075, iter [02400, 05004], lr: 0.071813, loss: 1.7715
2022-07-06 20:21:22 - train: epoch 0075, iter [02500, 05004], lr: 0.071798, loss: 1.9661
2022-07-06 20:21:56 - train: epoch 0075, iter [02600, 05004], lr: 0.071784, loss: 1.8781
2022-07-06 20:22:30 - train: epoch 0075, iter [02700, 05004], lr: 0.071769, loss: 1.6334
2022-07-06 20:23:02 - train: epoch 0075, iter [02800, 05004], lr: 0.071755, loss: 1.7619
2022-07-06 20:23:36 - train: epoch 0075, iter [02900, 05004], lr: 0.071740, loss: 1.9273
2022-07-06 20:24:10 - train: epoch 0075, iter [03000, 05004], lr: 0.071726, loss: 2.0260
2022-07-06 20:24:43 - train: epoch 0075, iter [03100, 05004], lr: 0.071711, loss: 1.8038
2022-07-06 20:25:17 - train: epoch 0075, iter [03200, 05004], lr: 0.071697, loss: 1.7633
2022-07-06 20:25:51 - train: epoch 0075, iter [03300, 05004], lr: 0.071682, loss: 1.7969
2022-07-06 20:26:24 - train: epoch 0075, iter [03400, 05004], lr: 0.071668, loss: 1.6160
2022-07-06 20:26:59 - train: epoch 0075, iter [03500, 05004], lr: 0.071653, loss: 1.7531
2022-07-06 20:27:32 - train: epoch 0075, iter [03600, 05004], lr: 0.071639, loss: 1.7266
2022-07-06 20:28:05 - train: epoch 0075, iter [03700, 05004], lr: 0.071624, loss: 1.8142
2022-07-06 20:28:39 - train: epoch 0075, iter [03800, 05004], lr: 0.071610, loss: 1.7091
2022-07-06 20:29:13 - train: epoch 0075, iter [03900, 05004], lr: 0.071595, loss: 1.9510
2022-07-06 20:29:46 - train: epoch 0075, iter [04000, 05004], lr: 0.071581, loss: 1.6049
2022-07-06 20:30:20 - train: epoch 0075, iter [04100, 05004], lr: 0.071566, loss: 1.4228
2022-07-06 20:30:54 - train: epoch 0075, iter [04200, 05004], lr: 0.071551, loss: 1.8036
2022-07-06 20:31:28 - train: epoch 0075, iter [04300, 05004], lr: 0.071537, loss: 1.9988
2022-07-06 20:32:02 - train: epoch 0075, iter [04400, 05004], lr: 0.071522, loss: 1.4691
2022-07-06 20:32:36 - train: epoch 0075, iter [04500, 05004], lr: 0.071508, loss: 1.6848
2022-07-06 20:33:09 - train: epoch 0075, iter [04600, 05004], lr: 0.071493, loss: 1.5819
2022-07-06 20:33:42 - train: epoch 0075, iter [04700, 05004], lr: 0.071479, loss: 1.8585
2022-07-06 20:34:16 - train: epoch 0075, iter [04800, 05004], lr: 0.071464, loss: 1.8202
2022-07-06 20:34:49 - train: epoch 0075, iter [04900, 05004], lr: 0.071450, loss: 1.6577
2022-07-06 20:35:22 - train: epoch 0075, iter [05000, 05004], lr: 0.071435, loss: 1.7379
2022-07-06 20:35:23 - train: epoch 075, train_loss: 1.7445
2022-07-06 20:36:37 - eval: epoch: 075, acc1: 60.724%, acc5: 83.788%, test_loss: 1.6531, per_image_load_time: 0.643ms, per_image_inference_time: 0.454ms
2022-07-06 20:36:37 - until epoch: 075, best_acc1: 62.268%
2022-07-06 20:36:37 - epoch 076 lr: 0.071434
2022-07-06 20:37:16 - train: epoch 0076, iter [00100, 05004], lr: 0.071420, loss: 1.5304
2022-07-06 20:37:49 - train: epoch 0076, iter [00200, 05004], lr: 0.071406, loss: 1.7085
2022-07-06 20:38:23 - train: epoch 0076, iter [00300, 05004], lr: 0.071391, loss: 1.7990
2022-07-06 20:38:56 - train: epoch 0076, iter [00400, 05004], lr: 0.071376, loss: 1.6073
2022-07-06 20:39:29 - train: epoch 0076, iter [00500, 05004], lr: 0.071362, loss: 1.7298
2022-07-06 20:40:02 - train: epoch 0076, iter [00600, 05004], lr: 0.071347, loss: 1.7799
2022-07-06 20:40:35 - train: epoch 0076, iter [00700, 05004], lr: 0.071333, loss: 1.8290
2022-07-06 20:41:10 - train: epoch 0076, iter [00800, 05004], lr: 0.071318, loss: 1.7370
2022-07-06 20:41:42 - train: epoch 0076, iter [00900, 05004], lr: 0.071304, loss: 1.6505
2022-07-06 20:42:16 - train: epoch 0076, iter [01000, 05004], lr: 0.071289, loss: 1.6735
2022-07-06 20:42:49 - train: epoch 0076, iter [01100, 05004], lr: 0.071275, loss: 1.5886
2022-07-06 20:43:22 - train: epoch 0076, iter [01200, 05004], lr: 0.071260, loss: 1.6521
2022-07-06 20:43:56 - train: epoch 0076, iter [01300, 05004], lr: 0.071245, loss: 1.7532
2022-07-06 20:44:30 - train: epoch 0076, iter [01400, 05004], lr: 0.071231, loss: 1.5937
2022-07-06 20:45:03 - train: epoch 0076, iter [01500, 05004], lr: 0.071216, loss: 1.6646
2022-07-06 20:45:37 - train: epoch 0076, iter [01600, 05004], lr: 0.071202, loss: 1.6124
2022-07-06 20:46:10 - train: epoch 0076, iter [01700, 05004], lr: 0.071187, loss: 1.6969
2022-07-06 20:46:44 - train: epoch 0076, iter [01800, 05004], lr: 0.071172, loss: 1.5726
2022-07-06 20:47:17 - train: epoch 0076, iter [01900, 05004], lr: 0.071158, loss: 1.8640
2022-07-06 20:47:51 - train: epoch 0076, iter [02000, 05004], lr: 0.071143, loss: 1.8199
2022-07-06 20:48:25 - train: epoch 0076, iter [02100, 05004], lr: 0.071129, loss: 1.8412
2022-07-06 20:48:58 - train: epoch 0076, iter [02200, 05004], lr: 0.071114, loss: 1.8545
2022-07-06 20:49:31 - train: epoch 0076, iter [02300, 05004], lr: 0.071100, loss: 1.6531
2022-07-06 20:50:06 - train: epoch 0076, iter [02400, 05004], lr: 0.071085, loss: 1.8022
2022-07-06 20:50:38 - train: epoch 0076, iter [02500, 05004], lr: 0.071070, loss: 1.7090
2022-07-06 20:51:12 - train: epoch 0076, iter [02600, 05004], lr: 0.071056, loss: 2.0108
2022-07-06 20:51:45 - train: epoch 0076, iter [02700, 05004], lr: 0.071041, loss: 1.7981
2022-07-06 20:52:18 - train: epoch 0076, iter [02800, 05004], lr: 0.071027, loss: 1.6750
2022-07-06 20:52:51 - train: epoch 0076, iter [02900, 05004], lr: 0.071012, loss: 1.7871
2022-07-06 20:53:25 - train: epoch 0076, iter [03000, 05004], lr: 0.070997, loss: 1.6406
2022-07-06 20:53:58 - train: epoch 0076, iter [03100, 05004], lr: 0.070983, loss: 1.9683
2022-07-06 20:54:32 - train: epoch 0076, iter [03200, 05004], lr: 0.070968, loss: 1.7439
2022-07-06 20:55:05 - train: epoch 0076, iter [03300, 05004], lr: 0.070953, loss: 1.5994
2022-07-06 20:55:39 - train: epoch 0076, iter [03400, 05004], lr: 0.070939, loss: 1.7131
2022-07-06 20:56:12 - train: epoch 0076, iter [03500, 05004], lr: 0.070924, loss: 1.6444
2022-07-06 20:56:45 - train: epoch 0076, iter [03600, 05004], lr: 0.070910, loss: 1.5483
2022-07-06 20:57:18 - train: epoch 0076, iter [03700, 05004], lr: 0.070895, loss: 1.6399
2022-07-06 20:57:52 - train: epoch 0076, iter [03800, 05004], lr: 0.070880, loss: 1.6581
2022-07-06 20:58:25 - train: epoch 0076, iter [03900, 05004], lr: 0.070866, loss: 1.7231
2022-07-06 20:58:59 - train: epoch 0076, iter [04000, 05004], lr: 0.070851, loss: 1.7427
2022-07-06 20:59:32 - train: epoch 0076, iter [04100, 05004], lr: 0.070836, loss: 1.8536
2022-07-06 21:00:06 - train: epoch 0076, iter [04200, 05004], lr: 0.070822, loss: 1.8452
2022-07-06 21:00:40 - train: epoch 0076, iter [04300, 05004], lr: 0.070807, loss: 1.8046
2022-07-06 21:01:13 - train: epoch 0076, iter [04400, 05004], lr: 0.070793, loss: 1.7285
2022-07-06 21:01:47 - train: epoch 0076, iter [04500, 05004], lr: 0.070778, loss: 1.7805
2022-07-06 21:02:19 - train: epoch 0076, iter [04600, 05004], lr: 0.070763, loss: 1.8026
2022-07-06 21:02:53 - train: epoch 0076, iter [04700, 05004], lr: 0.070749, loss: 1.8506
2022-07-06 21:03:27 - train: epoch 0076, iter [04800, 05004], lr: 0.070734, loss: 1.5842
2022-07-06 21:04:01 - train: epoch 0076, iter [04900, 05004], lr: 0.070719, loss: 1.7117
2022-07-06 21:04:33 - train: epoch 0076, iter [05000, 05004], lr: 0.070705, loss: 1.7950
2022-07-06 21:04:34 - train: epoch 076, train_loss: 1.7377
2022-07-06 21:05:48 - eval: epoch: 076, acc1: 59.658%, acc5: 83.008%, test_loss: 1.6999, per_image_load_time: 1.191ms, per_image_inference_time: 0.473ms
2022-07-06 21:05:48 - until epoch: 076, best_acc1: 62.268%
2022-07-06 21:05:48 - epoch 077 lr: 0.070704
2022-07-06 21:06:27 - train: epoch 0077, iter [00100, 05004], lr: 0.070689, loss: 1.9227
2022-07-06 21:07:00 - train: epoch 0077, iter [00200, 05004], lr: 0.070675, loss: 1.8473
2022-07-06 21:07:33 - train: epoch 0077, iter [00300, 05004], lr: 0.070660, loss: 1.5352
2022-07-06 21:08:06 - train: epoch 0077, iter [00400, 05004], lr: 0.070645, loss: 1.8545
2022-07-06 21:08:40 - train: epoch 0077, iter [00500, 05004], lr: 0.070631, loss: 1.4498
2022-07-06 21:09:13 - train: epoch 0077, iter [00600, 05004], lr: 0.070616, loss: 1.5057
2022-07-06 21:09:47 - train: epoch 0077, iter [00700, 05004], lr: 0.070601, loss: 1.5971
2022-07-06 21:10:20 - train: epoch 0077, iter [00800, 05004], lr: 0.070587, loss: 1.7692
2022-07-06 21:10:54 - train: epoch 0077, iter [00900, 05004], lr: 0.070572, loss: 1.9626
2022-07-06 21:11:27 - train: epoch 0077, iter [01000, 05004], lr: 0.070557, loss: 1.5115
2022-07-06 21:12:01 - train: epoch 0077, iter [01100, 05004], lr: 0.070543, loss: 1.8983
2022-07-06 21:12:34 - train: epoch 0077, iter [01200, 05004], lr: 0.070528, loss: 1.7611
2022-07-06 21:13:07 - train: epoch 0077, iter [01300, 05004], lr: 0.070513, loss: 1.5027
2022-07-06 21:13:41 - train: epoch 0077, iter [01400, 05004], lr: 0.070499, loss: 1.8389
2022-07-06 21:14:15 - train: epoch 0077, iter [01500, 05004], lr: 0.070484, loss: 1.5382
2022-07-06 21:14:49 - train: epoch 0077, iter [01600, 05004], lr: 0.070469, loss: 1.7275
2022-07-06 21:15:23 - train: epoch 0077, iter [01700, 05004], lr: 0.070455, loss: 2.0491
2022-07-06 21:15:57 - train: epoch 0077, iter [01800, 05004], lr: 0.070440, loss: 1.5965
2022-07-06 21:16:30 - train: epoch 0077, iter [01900, 05004], lr: 0.070425, loss: 1.7042
2022-07-06 21:17:03 - train: epoch 0077, iter [02000, 05004], lr: 0.070411, loss: 1.5817
2022-07-06 21:17:38 - train: epoch 0077, iter [02100, 05004], lr: 0.070396, loss: 1.8457
2022-07-06 21:18:11 - train: epoch 0077, iter [02200, 05004], lr: 0.070381, loss: 1.8352
2022-07-06 21:18:44 - train: epoch 0077, iter [02300, 05004], lr: 0.070367, loss: 1.8714
2022-07-06 21:19:18 - train: epoch 0077, iter [02400, 05004], lr: 0.070352, loss: 1.6636
2022-07-06 21:19:51 - train: epoch 0077, iter [02500, 05004], lr: 0.070337, loss: 1.9532
2022-07-06 21:20:25 - train: epoch 0077, iter [02600, 05004], lr: 0.070322, loss: 1.4892
2022-07-06 21:20:59 - train: epoch 0077, iter [02700, 05004], lr: 0.070308, loss: 1.6704
2022-07-06 21:21:32 - train: epoch 0077, iter [02800, 05004], lr: 0.070293, loss: 1.9298
2022-07-06 21:22:05 - train: epoch 0077, iter [02900, 05004], lr: 0.070278, loss: 2.1329
2022-07-06 21:22:39 - train: epoch 0077, iter [03000, 05004], lr: 0.070264, loss: 1.5470
2022-07-06 21:23:12 - train: epoch 0077, iter [03100, 05004], lr: 0.070249, loss: 1.6933
2022-07-06 21:23:46 - train: epoch 0077, iter [03200, 05004], lr: 0.070234, loss: 1.9171
2022-07-06 21:24:19 - train: epoch 0077, iter [03300, 05004], lr: 0.070219, loss: 1.5576
2022-07-06 21:24:51 - train: epoch 0077, iter [03400, 05004], lr: 0.070205, loss: 1.9258
2022-07-06 21:25:26 - train: epoch 0077, iter [03500, 05004], lr: 0.070190, loss: 1.5153
2022-07-06 21:26:00 - train: epoch 0077, iter [03600, 05004], lr: 0.070175, loss: 1.6875
2022-07-06 21:26:33 - train: epoch 0077, iter [03700, 05004], lr: 0.070161, loss: 1.8001
2022-07-06 21:27:07 - train: epoch 0077, iter [03800, 05004], lr: 0.070146, loss: 1.7644
2022-07-06 21:27:41 - train: epoch 0077, iter [03900, 05004], lr: 0.070131, loss: 1.7879
2022-07-06 21:28:14 - train: epoch 0077, iter [04000, 05004], lr: 0.070116, loss: 1.7317
2022-07-06 21:28:47 - train: epoch 0077, iter [04100, 05004], lr: 0.070102, loss: 1.9545
2022-07-06 21:29:21 - train: epoch 0077, iter [04200, 05004], lr: 0.070087, loss: 1.9075
2022-07-06 21:29:55 - train: epoch 0077, iter [04300, 05004], lr: 0.070072, loss: 1.7394
2022-07-06 21:30:28 - train: epoch 0077, iter [04400, 05004], lr: 0.070057, loss: 1.6968
2022-07-06 21:31:03 - train: epoch 0077, iter [04500, 05004], lr: 0.070043, loss: 1.8775
2022-07-06 21:31:36 - train: epoch 0077, iter [04600, 05004], lr: 0.070028, loss: 1.6865
2022-07-06 21:32:10 - train: epoch 0077, iter [04700, 05004], lr: 0.070013, loss: 1.5804
2022-07-06 21:32:43 - train: epoch 0077, iter [04800, 05004], lr: 0.069998, loss: 1.7414
2022-07-06 21:33:17 - train: epoch 0077, iter [04900, 05004], lr: 0.069984, loss: 1.9084
2022-07-06 21:33:50 - train: epoch 0077, iter [05000, 05004], lr: 0.069969, loss: 1.9767
2022-07-06 21:33:51 - train: epoch 077, train_loss: 1.7303
2022-07-06 21:35:05 - eval: epoch: 077, acc1: 60.714%, acc5: 84.100%, test_loss: 1.6408, per_image_load_time: 1.085ms, per_image_inference_time: 0.456ms
2022-07-06 21:35:05 - until epoch: 077, best_acc1: 62.268%
2022-07-06 21:35:05 - epoch 078 lr: 0.069968
2022-07-06 21:35:44 - train: epoch 0078, iter [00100, 05004], lr: 0.069953, loss: 1.7308
2022-07-06 21:36:17 - train: epoch 0078, iter [00200, 05004], lr: 0.069939, loss: 1.7756
2022-07-06 21:36:51 - train: epoch 0078, iter [00300, 05004], lr: 0.069924, loss: 1.7380
2022-07-06 21:37:25 - train: epoch 0078, iter [00400, 05004], lr: 0.069909, loss: 1.7592
2022-07-06 21:37:57 - train: epoch 0078, iter [00500, 05004], lr: 0.069894, loss: 1.6157
2022-07-06 21:38:31 - train: epoch 0078, iter [00600, 05004], lr: 0.069880, loss: 1.7746
2022-07-06 21:39:04 - train: epoch 0078, iter [00700, 05004], lr: 0.069865, loss: 1.7680
2022-07-06 21:39:37 - train: epoch 0078, iter [00800, 05004], lr: 0.069850, loss: 1.8810
2022-07-06 21:40:11 - train: epoch 0078, iter [00900, 05004], lr: 0.069835, loss: 1.8538
2022-07-06 21:40:44 - train: epoch 0078, iter [01000, 05004], lr: 0.069821, loss: 1.8413
2022-07-06 21:41:17 - train: epoch 0078, iter [01100, 05004], lr: 0.069806, loss: 1.4939
2022-07-06 21:41:51 - train: epoch 0078, iter [01200, 05004], lr: 0.069791, loss: 1.5234
2022-07-06 21:42:24 - train: epoch 0078, iter [01300, 05004], lr: 0.069776, loss: 1.7645
2022-07-06 21:42:58 - train: epoch 0078, iter [01400, 05004], lr: 0.069761, loss: 1.5325
2022-07-06 21:43:30 - train: epoch 0078, iter [01500, 05004], lr: 0.069747, loss: 1.7422
2022-07-06 21:44:04 - train: epoch 0078, iter [01600, 05004], lr: 0.069732, loss: 1.8837
2022-07-06 21:44:38 - train: epoch 0078, iter [01700, 05004], lr: 0.069717, loss: 1.6982
2022-07-06 21:45:11 - train: epoch 0078, iter [01800, 05004], lr: 0.069702, loss: 1.8609
2022-07-06 21:45:45 - train: epoch 0078, iter [01900, 05004], lr: 0.069687, loss: 1.4889
2022-07-06 21:46:19 - train: epoch 0078, iter [02000, 05004], lr: 0.069673, loss: 1.7795
2022-07-06 21:46:52 - train: epoch 0078, iter [02100, 05004], lr: 0.069658, loss: 2.0633
2022-07-06 21:47:27 - train: epoch 0078, iter [02200, 05004], lr: 0.069643, loss: 1.7018
2022-07-06 21:48:00 - train: epoch 0078, iter [02300, 05004], lr: 0.069628, loss: 1.9561
2022-07-06 21:48:34 - train: epoch 0078, iter [02400, 05004], lr: 0.069613, loss: 1.7698
2022-07-06 21:49:08 - train: epoch 0078, iter [02500, 05004], lr: 0.069599, loss: 1.7468
2022-07-06 21:49:42 - train: epoch 0078, iter [02600, 05004], lr: 0.069584, loss: 1.6830
2022-07-06 21:50:16 - train: epoch 0078, iter [02700, 05004], lr: 0.069569, loss: 1.6698
2022-07-06 21:50:50 - train: epoch 0078, iter [02800, 05004], lr: 0.069554, loss: 1.7003
2022-07-06 21:51:24 - train: epoch 0078, iter [02900, 05004], lr: 0.069539, loss: 1.7474
2022-07-06 21:51:58 - train: epoch 0078, iter [03000, 05004], lr: 0.069525, loss: 1.6650
2022-07-06 21:52:32 - train: epoch 0078, iter [03100, 05004], lr: 0.069510, loss: 1.8913
2022-07-06 21:53:06 - train: epoch 0078, iter [03200, 05004], lr: 0.069495, loss: 1.7170
2022-07-06 21:53:40 - train: epoch 0078, iter [03300, 05004], lr: 0.069480, loss: 1.8268
2022-07-06 21:54:14 - train: epoch 0078, iter [03400, 05004], lr: 0.069465, loss: 1.7418
2022-07-06 21:54:48 - train: epoch 0078, iter [03500, 05004], lr: 0.069450, loss: 1.6701
2022-07-06 21:55:22 - train: epoch 0078, iter [03600, 05004], lr: 0.069436, loss: 1.9279
2022-07-06 21:55:56 - train: epoch 0078, iter [03700, 05004], lr: 0.069421, loss: 1.6061
2022-07-06 21:56:29 - train: epoch 0078, iter [03800, 05004], lr: 0.069406, loss: 1.6831
2022-07-06 21:57:03 - train: epoch 0078, iter [03900, 05004], lr: 0.069391, loss: 1.6893
2022-07-06 21:57:38 - train: epoch 0078, iter [04000, 05004], lr: 0.069376, loss: 1.7226
2022-07-06 21:58:12 - train: epoch 0078, iter [04100, 05004], lr: 0.069361, loss: 1.8436
2022-07-06 21:58:46 - train: epoch 0078, iter [04200, 05004], lr: 0.069347, loss: 1.8875
2022-07-06 21:59:20 - train: epoch 0078, iter [04300, 05004], lr: 0.069332, loss: 1.7408
2022-07-06 21:59:55 - train: epoch 0078, iter [04400, 05004], lr: 0.069317, loss: 1.7193
2022-07-06 22:00:28 - train: epoch 0078, iter [04500, 05004], lr: 0.069302, loss: 1.7489
2022-07-06 22:01:02 - train: epoch 0078, iter [04600, 05004], lr: 0.069287, loss: 1.4222
2022-07-06 22:01:36 - train: epoch 0078, iter [04700, 05004], lr: 0.069272, loss: 1.7975
2022-07-06 22:02:09 - train: epoch 0078, iter [04800, 05004], lr: 0.069257, loss: 1.9886
2022-07-06 22:02:44 - train: epoch 0078, iter [04900, 05004], lr: 0.069243, loss: 1.7807
2022-07-06 22:03:16 - train: epoch 0078, iter [05000, 05004], lr: 0.069228, loss: 1.6476
2022-07-06 22:03:17 - train: epoch 078, train_loss: 1.7241
2022-07-06 22:04:32 - eval: epoch: 078, acc1: 58.576%, acc5: 82.644%, test_loss: 1.7388, per_image_load_time: 0.940ms, per_image_inference_time: 0.483ms
2022-07-06 22:04:32 - until epoch: 078, best_acc1: 62.268%
2022-07-06 22:04:32 - epoch 079 lr: 0.069227
2022-07-06 22:05:11 - train: epoch 0079, iter [00100, 05004], lr: 0.069212, loss: 1.6644
2022-07-06 22:05:45 - train: epoch 0079, iter [00200, 05004], lr: 0.069197, loss: 1.6271
2022-07-06 22:06:19 - train: epoch 0079, iter [00300, 05004], lr: 0.069183, loss: 1.7438
2022-07-06 22:06:52 - train: epoch 0079, iter [00400, 05004], lr: 0.069168, loss: 1.8026
2022-07-06 22:07:26 - train: epoch 0079, iter [00500, 05004], lr: 0.069153, loss: 1.7436
2022-07-06 22:08:00 - train: epoch 0079, iter [00600, 05004], lr: 0.069138, loss: 1.5625
2022-07-06 22:08:34 - train: epoch 0079, iter [00700, 05004], lr: 0.069123, loss: 1.6043
2022-07-06 22:09:07 - train: epoch 0079, iter [00800, 05004], lr: 0.069108, loss: 1.7755
2022-07-06 22:09:41 - train: epoch 0079, iter [00900, 05004], lr: 0.069093, loss: 1.6874
2022-07-06 22:10:15 - train: epoch 0079, iter [01000, 05004], lr: 0.069078, loss: 1.6923
2022-07-06 22:10:49 - train: epoch 0079, iter [01100, 05004], lr: 0.069064, loss: 1.8886
2022-07-06 22:11:23 - train: epoch 0079, iter [01200, 05004], lr: 0.069049, loss: 2.0578
2022-07-06 22:11:57 - train: epoch 0079, iter [01300, 05004], lr: 0.069034, loss: 1.5176
2022-07-06 22:12:30 - train: epoch 0079, iter [01400, 05004], lr: 0.069019, loss: 1.7049
2022-07-06 22:13:04 - train: epoch 0079, iter [01500, 05004], lr: 0.069004, loss: 1.5952
2022-07-06 22:13:38 - train: epoch 0079, iter [01600, 05004], lr: 0.068989, loss: 1.3882
2022-07-06 22:14:12 - train: epoch 0079, iter [01700, 05004], lr: 0.068974, loss: 1.7358
2022-07-06 22:14:46 - train: epoch 0079, iter [01800, 05004], lr: 0.068959, loss: 1.7034
2022-07-06 22:15:20 - train: epoch 0079, iter [01900, 05004], lr: 0.068944, loss: 1.7375
2022-07-06 22:15:54 - train: epoch 0079, iter [02000, 05004], lr: 0.068930, loss: 1.9394
2022-07-06 22:16:27 - train: epoch 0079, iter [02100, 05004], lr: 0.068915, loss: 1.5585
2022-07-06 22:17:01 - train: epoch 0079, iter [02200, 05004], lr: 0.068900, loss: 1.8058
2022-07-06 22:17:36 - train: epoch 0079, iter [02300, 05004], lr: 0.068885, loss: 1.6894
2022-07-06 22:18:10 - train: epoch 0079, iter [02400, 05004], lr: 0.068870, loss: 1.7397
2022-07-06 22:18:44 - train: epoch 0079, iter [02500, 05004], lr: 0.068855, loss: 1.6989
2022-07-06 22:19:17 - train: epoch 0079, iter [02600, 05004], lr: 0.068840, loss: 1.7156
2022-07-06 22:19:51 - train: epoch 0079, iter [02700, 05004], lr: 0.068825, loss: 1.4606
2022-07-06 22:20:25 - train: epoch 0079, iter [02800, 05004], lr: 0.068810, loss: 1.5652
2022-07-06 22:20:59 - train: epoch 0079, iter [02900, 05004], lr: 0.068795, loss: 1.5300
2022-07-06 22:21:32 - train: epoch 0079, iter [03000, 05004], lr: 0.068780, loss: 1.7687
2022-07-06 22:22:06 - train: epoch 0079, iter [03100, 05004], lr: 0.068766, loss: 1.6367
2022-07-06 22:22:40 - train: epoch 0079, iter [03200, 05004], lr: 0.068751, loss: 2.0294
2022-07-06 22:23:13 - train: epoch 0079, iter [03300, 05004], lr: 0.068736, loss: 1.7013
2022-07-06 22:23:47 - train: epoch 0079, iter [03400, 05004], lr: 0.068721, loss: 1.5470
2022-07-06 22:24:21 - train: epoch 0079, iter [03500, 05004], lr: 0.068706, loss: 1.8642
2022-07-06 22:24:54 - train: epoch 0079, iter [03600, 05004], lr: 0.068691, loss: 1.7101
2022-07-06 22:25:29 - train: epoch 0079, iter [03700, 05004], lr: 0.068676, loss: 1.5940
2022-07-06 22:26:03 - train: epoch 0079, iter [03800, 05004], lr: 0.068661, loss: 1.8072
2022-07-06 22:26:37 - train: epoch 0079, iter [03900, 05004], lr: 0.068646, loss: 1.6132
2022-07-06 22:27:11 - train: epoch 0079, iter [04000, 05004], lr: 0.068631, loss: 1.5897
2022-07-06 22:27:45 - train: epoch 0079, iter [04100, 05004], lr: 0.068616, loss: 1.8027
2022-07-06 22:28:18 - train: epoch 0079, iter [04200, 05004], lr: 0.068601, loss: 1.5337
2022-07-06 22:28:52 - train: epoch 0079, iter [04300, 05004], lr: 0.068586, loss: 1.4306
2022-07-06 22:29:25 - train: epoch 0079, iter [04400, 05004], lr: 0.068571, loss: 1.9451
2022-07-06 22:29:59 - train: epoch 0079, iter [04500, 05004], lr: 0.068556, loss: 2.0266
2022-07-06 22:30:33 - train: epoch 0079, iter [04600, 05004], lr: 0.068542, loss: 1.9623
2022-07-06 22:31:07 - train: epoch 0079, iter [04700, 05004], lr: 0.068527, loss: 1.8065
2022-07-06 22:31:42 - train: epoch 0079, iter [04800, 05004], lr: 0.068512, loss: 1.8044
2022-07-06 22:32:16 - train: epoch 0079, iter [04900, 05004], lr: 0.068497, loss: 1.7970
2022-07-06 22:32:48 - train: epoch 0079, iter [05000, 05004], lr: 0.068482, loss: 1.6111
2022-07-06 22:32:49 - train: epoch 079, train_loss: 1.7213
2022-07-06 22:34:04 - eval: epoch: 079, acc1: 63.036%, acc5: 85.778%, test_loss: 1.5356, per_image_load_time: 1.333ms, per_image_inference_time: 0.468ms
2022-07-06 22:34:05 - until epoch: 079, best_acc1: 63.036%
2022-07-06 22:34:05 - epoch 080 lr: 0.068481
2022-07-06 22:34:43 - train: epoch 0080, iter [00100, 05004], lr: 0.068466, loss: 1.4854
2022-07-06 22:35:17 - train: epoch 0080, iter [00200, 05004], lr: 0.068451, loss: 1.6898
2022-07-06 22:35:51 - train: epoch 0080, iter [00300, 05004], lr: 0.068436, loss: 1.7906
2022-07-06 22:36:24 - train: epoch 0080, iter [00400, 05004], lr: 0.068421, loss: 1.3984
2022-07-06 22:36:58 - train: epoch 0080, iter [00500, 05004], lr: 0.068406, loss: 1.6876
2022-07-06 22:37:31 - train: epoch 0080, iter [00600, 05004], lr: 0.068391, loss: 1.5455
2022-07-06 22:38:05 - train: epoch 0080, iter [00700, 05004], lr: 0.068376, loss: 1.6172
2022-07-06 22:38:38 - train: epoch 0080, iter [00800, 05004], lr: 0.068361, loss: 1.5862
2022-07-06 22:39:12 - train: epoch 0080, iter [00900, 05004], lr: 0.068346, loss: 1.6744
2022-07-06 22:39:45 - train: epoch 0080, iter [01000, 05004], lr: 0.068331, loss: 1.5789
2022-07-06 22:40:19 - train: epoch 0080, iter [01100, 05004], lr: 0.068316, loss: 1.6630
2022-07-06 22:40:53 - train: epoch 0080, iter [01200, 05004], lr: 0.068301, loss: 1.6030
2022-07-06 22:41:27 - train: epoch 0080, iter [01300, 05004], lr: 0.068286, loss: 1.7203
2022-07-06 22:42:01 - train: epoch 0080, iter [01400, 05004], lr: 0.068272, loss: 1.6934
2022-07-06 22:42:35 - train: epoch 0080, iter [01500, 05004], lr: 0.068257, loss: 1.7063
2022-07-06 22:43:09 - train: epoch 0080, iter [01600, 05004], lr: 0.068242, loss: 1.6406
2022-07-06 22:43:42 - train: epoch 0080, iter [01700, 05004], lr: 0.068227, loss: 1.8067
2022-07-06 22:44:16 - train: epoch 0080, iter [01800, 05004], lr: 0.068212, loss: 1.7773
2022-07-06 22:44:49 - train: epoch 0080, iter [01900, 05004], lr: 0.068197, loss: 1.7073
2022-07-06 22:45:23 - train: epoch 0080, iter [02000, 05004], lr: 0.068182, loss: 1.8524
2022-07-06 22:45:56 - train: epoch 0080, iter [02100, 05004], lr: 0.068167, loss: 1.7447
2022-07-06 22:46:30 - train: epoch 0080, iter [02200, 05004], lr: 0.068152, loss: 1.7458
2022-07-06 22:47:04 - train: epoch 0080, iter [02300, 05004], lr: 0.068137, loss: 1.6537
2022-07-06 22:47:38 - train: epoch 0080, iter [02400, 05004], lr: 0.068122, loss: 1.6875
2022-07-06 22:48:12 - train: epoch 0080, iter [02500, 05004], lr: 0.068107, loss: 1.5347
2022-07-06 22:48:46 - train: epoch 0080, iter [02600, 05004], lr: 0.068092, loss: 1.6366
2022-07-06 22:49:20 - train: epoch 0080, iter [02700, 05004], lr: 0.068077, loss: 1.8576
2022-07-06 22:49:54 - train: epoch 0080, iter [02800, 05004], lr: 0.068062, loss: 1.7668
2022-07-06 22:50:28 - train: epoch 0080, iter [02900, 05004], lr: 0.068047, loss: 1.6672
2022-07-06 22:51:02 - train: epoch 0080, iter [03000, 05004], lr: 0.068032, loss: 1.7724
2022-07-06 22:51:36 - train: epoch 0080, iter [03100, 05004], lr: 0.068016, loss: 1.9091
2022-07-06 22:52:11 - train: epoch 0080, iter [03200, 05004], lr: 0.068001, loss: 1.2589
2022-07-06 22:52:45 - train: epoch 0080, iter [03300, 05004], lr: 0.067986, loss: 1.7149
2022-07-06 22:53:18 - train: epoch 0080, iter [03400, 05004], lr: 0.067971, loss: 1.5939
2022-07-06 22:53:53 - train: epoch 0080, iter [03500, 05004], lr: 0.067956, loss: 1.5783
2022-07-06 22:54:26 - train: epoch 0080, iter [03600, 05004], lr: 0.067941, loss: 1.7040
2022-07-06 22:55:00 - train: epoch 0080, iter [03700, 05004], lr: 0.067926, loss: 1.6634
2022-07-06 22:55:34 - train: epoch 0080, iter [03800, 05004], lr: 0.067911, loss: 1.8534
2022-07-06 22:56:08 - train: epoch 0080, iter [03900, 05004], lr: 0.067896, loss: 1.6665
2022-07-06 22:56:42 - train: epoch 0080, iter [04000, 05004], lr: 0.067881, loss: 1.8650
2022-07-06 22:57:15 - train: epoch 0080, iter [04100, 05004], lr: 0.067866, loss: 1.8402
2022-07-06 22:57:50 - train: epoch 0080, iter [04200, 05004], lr: 0.067851, loss: 1.7458
2022-07-06 22:58:24 - train: epoch 0080, iter [04300, 05004], lr: 0.067836, loss: 1.9122
2022-07-06 22:58:57 - train: epoch 0080, iter [04400, 05004], lr: 0.067821, loss: 1.6917
2022-07-06 22:59:31 - train: epoch 0080, iter [04500, 05004], lr: 0.067806, loss: 1.6346
2022-07-06 23:00:04 - train: epoch 0080, iter [04600, 05004], lr: 0.067791, loss: 1.8690
2022-07-06 23:00:39 - train: epoch 0080, iter [04700, 05004], lr: 0.067776, loss: 1.7938
2022-07-06 23:01:13 - train: epoch 0080, iter [04800, 05004], lr: 0.067761, loss: 1.6439
2022-07-06 23:01:47 - train: epoch 0080, iter [04900, 05004], lr: 0.067746, loss: 1.8413
2022-07-06 23:02:20 - train: epoch 0080, iter [05000, 05004], lr: 0.067731, loss: 1.6222
2022-07-06 23:02:21 - train: epoch 080, train_loss: 1.7161
2022-07-06 23:03:36 - eval: epoch: 080, acc1: 58.278%, acc5: 82.462%, test_loss: 1.7514, per_image_load_time: 0.804ms, per_image_inference_time: 0.465ms
2022-07-06 23:03:37 - until epoch: 080, best_acc1: 63.036%
2022-07-06 23:03:37 - epoch 081 lr: 0.067730
2022-07-06 23:04:16 - train: epoch 0081, iter [00100, 05004], lr: 0.067715, loss: 1.5462
2022-07-06 23:04:49 - train: epoch 0081, iter [00200, 05004], lr: 0.067700, loss: 1.7250
2022-07-06 23:05:22 - train: epoch 0081, iter [00300, 05004], lr: 0.067685, loss: 1.6079
2022-07-06 23:05:56 - train: epoch 0081, iter [00400, 05004], lr: 0.067670, loss: 1.8902
2022-07-06 23:06:29 - train: epoch 0081, iter [00500, 05004], lr: 0.067655, loss: 1.8483
2022-07-06 23:07:03 - train: epoch 0081, iter [00600, 05004], lr: 0.067640, loss: 1.8276
2022-07-06 23:07:37 - train: epoch 0081, iter [00700, 05004], lr: 0.067625, loss: 1.5541
2022-07-06 23:08:10 - train: epoch 0081, iter [00800, 05004], lr: 0.067610, loss: 1.9472
2022-07-06 23:08:43 - train: epoch 0081, iter [00900, 05004], lr: 0.067595, loss: 1.5397
2022-07-06 23:09:17 - train: epoch 0081, iter [01000, 05004], lr: 0.067580, loss: 1.6536
2022-07-06 23:09:50 - train: epoch 0081, iter [01100, 05004], lr: 0.067565, loss: 1.6005
2022-07-06 23:10:24 - train: epoch 0081, iter [01200, 05004], lr: 0.067549, loss: 1.7244
2022-07-06 23:10:58 - train: epoch 0081, iter [01300, 05004], lr: 0.067534, loss: 1.4375
2022-07-06 23:11:32 - train: epoch 0081, iter [01400, 05004], lr: 0.067519, loss: 1.3207
2022-07-06 23:12:06 - train: epoch 0081, iter [01500, 05004], lr: 0.067504, loss: 1.7600
2022-07-06 23:12:40 - train: epoch 0081, iter [01600, 05004], lr: 0.067489, loss: 1.6782
2022-07-06 23:13:13 - train: epoch 0081, iter [01700, 05004], lr: 0.067474, loss: 1.7991
2022-07-06 23:13:47 - train: epoch 0081, iter [01800, 05004], lr: 0.067459, loss: 1.6242
2022-07-06 23:14:21 - train: epoch 0081, iter [01900, 05004], lr: 0.067444, loss: 1.6184
2022-07-06 23:14:54 - train: epoch 0081, iter [02000, 05004], lr: 0.067429, loss: 1.7899
2022-07-06 23:15:28 - train: epoch 0081, iter [02100, 05004], lr: 0.067414, loss: 1.7964
2022-07-06 23:16:02 - train: epoch 0081, iter [02200, 05004], lr: 0.067399, loss: 1.7075
2022-07-06 23:16:35 - train: epoch 0081, iter [02300, 05004], lr: 0.067384, loss: 1.6307
2022-07-06 23:17:09 - train: epoch 0081, iter [02400, 05004], lr: 0.067368, loss: 1.7065
2022-07-06 23:17:42 - train: epoch 0081, iter [02500, 05004], lr: 0.067353, loss: 1.8838
2022-07-06 23:18:16 - train: epoch 0081, iter [02600, 05004], lr: 0.067338, loss: 1.8436
2022-07-06 23:18:50 - train: epoch 0081, iter [02700, 05004], lr: 0.067323, loss: 1.6424
2022-07-06 23:19:24 - train: epoch 0081, iter [02800, 05004], lr: 0.067308, loss: 1.6677
2022-07-06 23:19:58 - train: epoch 0081, iter [02900, 05004], lr: 0.067293, loss: 1.5305
2022-07-06 23:20:32 - train: epoch 0081, iter [03000, 05004], lr: 0.067278, loss: 1.7325
2022-07-06 23:21:06 - train: epoch 0081, iter [03100, 05004], lr: 0.067263, loss: 1.6345
2022-07-06 23:21:39 - train: epoch 0081, iter [03200, 05004], lr: 0.067248, loss: 1.7974
2022-07-06 23:22:13 - train: epoch 0081, iter [03300, 05004], lr: 0.067233, loss: 1.6876
2022-07-06 23:22:48 - train: epoch 0081, iter [03400, 05004], lr: 0.067217, loss: 1.9695
2022-07-06 23:23:21 - train: epoch 0081, iter [03500, 05004], lr: 0.067202, loss: 1.7779
2022-07-06 23:23:56 - train: epoch 0081, iter [03600, 05004], lr: 0.067187, loss: 1.5488
2022-07-06 23:24:30 - train: epoch 0081, iter [03700, 05004], lr: 0.067172, loss: 1.7729
2022-07-06 23:25:03 - train: epoch 0081, iter [03800, 05004], lr: 0.067157, loss: 1.6480
2022-07-06 23:25:37 - train: epoch 0081, iter [03900, 05004], lr: 0.067142, loss: 1.7889
2022-07-06 23:26:11 - train: epoch 0081, iter [04000, 05004], lr: 0.067127, loss: 1.5265
2022-07-06 23:26:44 - train: epoch 0081, iter [04100, 05004], lr: 0.067112, loss: 1.8307
2022-07-06 23:27:18 - train: epoch 0081, iter [04200, 05004], lr: 0.067096, loss: 1.7646
2022-07-06 23:27:51 - train: epoch 0081, iter [04300, 05004], lr: 0.067081, loss: 1.6422
2022-07-06 23:28:25 - train: epoch 0081, iter [04400, 05004], lr: 0.067066, loss: 1.9006
2022-07-06 23:28:58 - train: epoch 0081, iter [04500, 05004], lr: 0.067051, loss: 1.8211
2022-07-06 23:29:32 - train: epoch 0081, iter [04600, 05004], lr: 0.067036, loss: 1.6116
2022-07-06 23:30:06 - train: epoch 0081, iter [04700, 05004], lr: 0.067021, loss: 1.7322
2022-07-06 23:30:39 - train: epoch 0081, iter [04800, 05004], lr: 0.067006, loss: 1.7844
2022-07-06 23:31:13 - train: epoch 0081, iter [04900, 05004], lr: 0.066991, loss: 1.7733
2022-07-06 23:31:45 - train: epoch 0081, iter [05000, 05004], lr: 0.066975, loss: 1.4989
2022-07-06 23:31:46 - train: epoch 081, train_loss: 1.7103
2022-07-06 23:33:01 - eval: epoch: 081, acc1: 62.832%, acc5: 85.460%, test_loss: 1.5434, per_image_load_time: 0.999ms, per_image_inference_time: 0.473ms
2022-07-06 23:33:02 - until epoch: 081, best_acc1: 63.036%
2022-07-06 23:33:02 - epoch 082 lr: 0.066975
2022-07-06 23:33:41 - train: epoch 0082, iter [00100, 05004], lr: 0.066960, loss: 1.4288
2022-07-06 23:34:14 - train: epoch 0082, iter [00200, 05004], lr: 0.066944, loss: 1.4817
2022-07-06 23:34:48 - train: epoch 0082, iter [00300, 05004], lr: 0.066929, loss: 1.6159
2022-07-06 23:35:21 - train: epoch 0082, iter [00400, 05004], lr: 0.066914, loss: 1.7851
2022-07-06 23:35:54 - train: epoch 0082, iter [00500, 05004], lr: 0.066899, loss: 1.8167
2022-07-06 23:36:28 - train: epoch 0082, iter [00600, 05004], lr: 0.066884, loss: 1.5943
2022-07-06 23:37:01 - train: epoch 0082, iter [00700, 05004], lr: 0.066869, loss: 1.8463
2022-07-06 23:37:34 - train: epoch 0082, iter [00800, 05004], lr: 0.066854, loss: 1.6692
2022-07-06 23:38:08 - train: epoch 0082, iter [00900, 05004], lr: 0.066838, loss: 1.9193
2022-07-06 23:38:41 - train: epoch 0082, iter [01000, 05004], lr: 0.066823, loss: 1.7736
2022-07-06 23:39:14 - train: epoch 0082, iter [01100, 05004], lr: 0.066808, loss: 1.7955
2022-07-06 23:39:47 - train: epoch 0082, iter [01200, 05004], lr: 0.066793, loss: 1.7491
2022-07-06 23:40:21 - train: epoch 0082, iter [01300, 05004], lr: 0.066778, loss: 1.9204
2022-07-06 23:40:54 - train: epoch 0082, iter [01400, 05004], lr: 0.066763, loss: 1.5573
2022-07-06 23:41:27 - train: epoch 0082, iter [01500, 05004], lr: 0.066747, loss: 1.5676
2022-07-06 23:42:01 - train: epoch 0082, iter [01600, 05004], lr: 0.066732, loss: 1.6573
2022-07-06 23:42:35 - train: epoch 0082, iter [01700, 05004], lr: 0.066717, loss: 1.5079
2022-07-06 23:43:09 - train: epoch 0082, iter [01800, 05004], lr: 0.066702, loss: 1.5363
2022-07-06 23:43:42 - train: epoch 0082, iter [01900, 05004], lr: 0.066687, loss: 1.6560
2022-07-06 23:44:16 - train: epoch 0082, iter [02000, 05004], lr: 0.066672, loss: 1.4634
2022-07-06 23:44:50 - train: epoch 0082, iter [02100, 05004], lr: 0.066656, loss: 1.5736
2022-07-06 23:45:24 - train: epoch 0082, iter [02200, 05004], lr: 0.066641, loss: 1.6592
2022-07-06 23:45:58 - train: epoch 0082, iter [02300, 05004], lr: 0.066626, loss: 1.7178
2022-07-06 23:46:31 - train: epoch 0082, iter [02400, 05004], lr: 0.066611, loss: 1.6848
2022-07-06 23:47:05 - train: epoch 0082, iter [02500, 05004], lr: 0.066596, loss: 1.6712
2022-07-06 23:47:38 - train: epoch 0082, iter [02600, 05004], lr: 0.066581, loss: 1.6511
2022-07-06 23:48:13 - train: epoch 0082, iter [02700, 05004], lr: 0.066565, loss: 1.5741
2022-07-06 23:48:46 - train: epoch 0082, iter [02800, 05004], lr: 0.066550, loss: 1.4849
2022-07-06 23:49:20 - train: epoch 0082, iter [02900, 05004], lr: 0.066535, loss: 1.4997
2022-07-06 23:49:54 - train: epoch 0082, iter [03000, 05004], lr: 0.066520, loss: 1.6712
2022-07-06 23:50:28 - train: epoch 0082, iter [03100, 05004], lr: 0.066505, loss: 1.6245
2022-07-06 23:51:01 - train: epoch 0082, iter [03200, 05004], lr: 0.066489, loss: 1.9956
2022-07-06 23:51:35 - train: epoch 0082, iter [03300, 05004], lr: 0.066474, loss: 1.6507
2022-07-06 23:52:09 - train: epoch 0082, iter [03400, 05004], lr: 0.066459, loss: 1.5700
2022-07-06 23:52:43 - train: epoch 0082, iter [03500, 05004], lr: 0.066444, loss: 1.6558
2022-07-06 23:53:16 - train: epoch 0082, iter [03600, 05004], lr: 0.066429, loss: 1.6581
2022-07-06 23:53:51 - train: epoch 0082, iter [03700, 05004], lr: 0.066413, loss: 1.5653
2022-07-06 23:54:25 - train: epoch 0082, iter [03800, 05004], lr: 0.066398, loss: 2.0059
2022-07-06 23:55:00 - train: epoch 0082, iter [03900, 05004], lr: 0.066383, loss: 1.8616
2022-07-06 23:55:32 - train: epoch 0082, iter [04000, 05004], lr: 0.066368, loss: 1.7856
2022-07-06 23:56:06 - train: epoch 0082, iter [04100, 05004], lr: 0.066353, loss: 1.7223
2022-07-06 23:56:41 - train: epoch 0082, iter [04200, 05004], lr: 0.066337, loss: 1.8952
2022-07-06 23:57:15 - train: epoch 0082, iter [04300, 05004], lr: 0.066322, loss: 1.5717
2022-07-06 23:57:49 - train: epoch 0082, iter [04400, 05004], lr: 0.066307, loss: 1.5988
2022-07-06 23:58:23 - train: epoch 0082, iter [04500, 05004], lr: 0.066292, loss: 1.8181
2022-07-06 23:58:57 - train: epoch 0082, iter [04600, 05004], lr: 0.066276, loss: 1.6824
2022-07-06 23:59:30 - train: epoch 0082, iter [04700, 05004], lr: 0.066261, loss: 1.5828
2022-07-07 00:00:04 - train: epoch 0082, iter [04800, 05004], lr: 0.066246, loss: 1.6960
2022-07-07 00:00:38 - train: epoch 0082, iter [04900, 05004], lr: 0.066231, loss: 1.7442
2022-07-07 00:01:10 - train: epoch 0082, iter [05000, 05004], lr: 0.066216, loss: 1.7806
2022-07-07 00:01:12 - train: epoch 082, train_loss: 1.7024
2022-07-07 00:02:26 - eval: epoch: 082, acc1: 61.444%, acc5: 84.364%, test_loss: 1.6116, per_image_load_time: 1.626ms, per_image_inference_time: 0.502ms
2022-07-07 00:02:26 - until epoch: 082, best_acc1: 63.036%
2022-07-07 00:02:26 - epoch 083 lr: 0.066215
2022-07-07 00:03:05 - train: epoch 0083, iter [00100, 05004], lr: 0.066200, loss: 1.6399
2022-07-07 00:03:39 - train: epoch 0083, iter [00200, 05004], lr: 0.066184, loss: 1.5647
2022-07-07 00:04:13 - train: epoch 0083, iter [00300, 05004], lr: 0.066169, loss: 1.7508
2022-07-07 00:04:46 - train: epoch 0083, iter [00400, 05004], lr: 0.066154, loss: 1.7876
2022-07-07 00:05:20 - train: epoch 0083, iter [00500, 05004], lr: 0.066139, loss: 1.6635
2022-07-07 00:05:53 - train: epoch 0083, iter [00600, 05004], lr: 0.066124, loss: 1.6312
2022-07-07 00:06:27 - train: epoch 0083, iter [00700, 05004], lr: 0.066108, loss: 1.6265
2022-07-07 00:07:02 - train: epoch 0083, iter [00800, 05004], lr: 0.066093, loss: 1.7608
2022-07-07 00:07:35 - train: epoch 0083, iter [00900, 05004], lr: 0.066078, loss: 1.7256
2022-07-07 00:08:09 - train: epoch 0083, iter [01000, 05004], lr: 0.066063, loss: 1.7980
2022-07-07 00:08:43 - train: epoch 0083, iter [01100, 05004], lr: 0.066047, loss: 1.6508
2022-07-07 00:09:16 - train: epoch 0083, iter [01200, 05004], lr: 0.066032, loss: 1.6855
2022-07-07 00:09:50 - train: epoch 0083, iter [01300, 05004], lr: 0.066017, loss: 1.5032
2022-07-07 00:10:24 - train: epoch 0083, iter [01400, 05004], lr: 0.066002, loss: 1.6711
2022-07-07 00:10:57 - train: epoch 0083, iter [01500, 05004], lr: 0.065986, loss: 1.5325
2022-07-07 00:11:31 - train: epoch 0083, iter [01600, 05004], lr: 0.065971, loss: 1.6535
2022-07-07 00:12:05 - train: epoch 0083, iter [01700, 05004], lr: 0.065956, loss: 1.7964
2022-07-07 00:12:38 - train: epoch 0083, iter [01800, 05004], lr: 0.065941, loss: 1.8482
2022-07-07 00:13:11 - train: epoch 0083, iter [01900, 05004], lr: 0.065925, loss: 1.5350
2022-07-07 00:13:45 - train: epoch 0083, iter [02000, 05004], lr: 0.065910, loss: 1.3749
2022-07-07 00:14:19 - train: epoch 0083, iter [02100, 05004], lr: 0.065895, loss: 1.5581
2022-07-07 00:14:53 - train: epoch 0083, iter [02200, 05004], lr: 0.065880, loss: 1.8182
2022-07-07 00:15:26 - train: epoch 0083, iter [02300, 05004], lr: 0.065864, loss: 1.7718
2022-07-07 00:15:59 - train: epoch 0083, iter [02400, 05004], lr: 0.065849, loss: 1.6394
2022-07-07 00:16:34 - train: epoch 0083, iter [02500, 05004], lr: 0.065834, loss: 1.6742
2022-07-07 00:17:07 - train: epoch 0083, iter [02600, 05004], lr: 0.065818, loss: 1.6733
2022-07-07 00:17:41 - train: epoch 0083, iter [02700, 05004], lr: 0.065803, loss: 1.5726
2022-07-07 00:18:15 - train: epoch 0083, iter [02800, 05004], lr: 0.065788, loss: 1.6354
2022-07-07 00:18:48 - train: epoch 0083, iter [02900, 05004], lr: 0.065773, loss: 1.8085
2022-07-07 00:19:21 - train: epoch 0083, iter [03000, 05004], lr: 0.065757, loss: 1.7663
2022-07-07 00:19:55 - train: epoch 0083, iter [03100, 05004], lr: 0.065742, loss: 1.5338
2022-07-07 00:20:29 - train: epoch 0083, iter [03200, 05004], lr: 0.065727, loss: 1.6070
2022-07-07 00:21:02 - train: epoch 0083, iter [03300, 05004], lr: 0.065711, loss: 1.9412
2022-07-07 00:21:36 - train: epoch 0083, iter [03400, 05004], lr: 0.065696, loss: 1.8028
2022-07-07 00:22:10 - train: epoch 0083, iter [03500, 05004], lr: 0.065681, loss: 1.8238
2022-07-07 00:22:44 - train: epoch 0083, iter [03600, 05004], lr: 0.065666, loss: 1.8122
2022-07-07 00:23:17 - train: epoch 0083, iter [03700, 05004], lr: 0.065650, loss: 1.7402
2022-07-07 00:23:51 - train: epoch 0083, iter [03800, 05004], lr: 0.065635, loss: 1.5648
2022-07-07 00:24:25 - train: epoch 0083, iter [03900, 05004], lr: 0.065620, loss: 1.8096
2022-07-07 00:24:58 - train: epoch 0083, iter [04000, 05004], lr: 0.065604, loss: 2.0305
2022-07-07 00:25:33 - train: epoch 0083, iter [04100, 05004], lr: 0.065589, loss: 1.6856
2022-07-07 00:26:06 - train: epoch 0083, iter [04200, 05004], lr: 0.065574, loss: 2.0518
2022-07-07 00:26:40 - train: epoch 0083, iter [04300, 05004], lr: 0.065559, loss: 1.6945
2022-07-07 00:27:14 - train: epoch 0083, iter [04400, 05004], lr: 0.065543, loss: 1.5768
2022-07-07 00:27:48 - train: epoch 0083, iter [04500, 05004], lr: 0.065528, loss: 1.7961
2022-07-07 00:28:22 - train: epoch 0083, iter [04600, 05004], lr: 0.065513, loss: 1.7937
2022-07-07 00:28:56 - train: epoch 0083, iter [04700, 05004], lr: 0.065497, loss: 1.8828
2022-07-07 00:29:30 - train: epoch 0083, iter [04800, 05004], lr: 0.065482, loss: 1.8560
2022-07-07 00:30:03 - train: epoch 0083, iter [04900, 05004], lr: 0.065467, loss: 1.9370
2022-07-07 00:30:36 - train: epoch 0083, iter [05000, 05004], lr: 0.065451, loss: 1.7205
2022-07-07 00:30:37 - train: epoch 083, train_loss: 1.6996
2022-07-07 00:31:51 - eval: epoch: 083, acc1: 62.348%, acc5: 85.076%, test_loss: 1.5659, per_image_load_time: 1.996ms, per_image_inference_time: 0.486ms
2022-07-07 00:31:52 - until epoch: 083, best_acc1: 63.036%
2022-07-07 00:31:52 - epoch 084 lr: 0.065451
2022-07-07 00:32:31 - train: epoch 0084, iter [00100, 05004], lr: 0.065436, loss: 1.6069
2022-07-07 00:33:04 - train: epoch 0084, iter [00200, 05004], lr: 0.065420, loss: 1.7923
2022-07-07 00:33:38 - train: epoch 0084, iter [00300, 05004], lr: 0.065405, loss: 1.6026
2022-07-07 00:34:11 - train: epoch 0084, iter [00400, 05004], lr: 0.065390, loss: 1.6625
2022-07-07 00:34:45 - train: epoch 0084, iter [00500, 05004], lr: 0.065374, loss: 1.6612
2022-07-07 00:35:18 - train: epoch 0084, iter [00600, 05004], lr: 0.065359, loss: 2.0131
2022-07-07 00:35:51 - train: epoch 0084, iter [00700, 05004], lr: 0.065344, loss: 1.7531
2022-07-07 00:36:26 - train: epoch 0084, iter [00800, 05004], lr: 0.065328, loss: 1.7200
2022-07-07 00:37:00 - train: epoch 0084, iter [00900, 05004], lr: 0.065313, loss: 1.6855
2022-07-07 00:37:33 - train: epoch 0084, iter [01000, 05004], lr: 0.065298, loss: 1.7034
2022-07-07 00:38:07 - train: epoch 0084, iter [01100, 05004], lr: 0.065282, loss: 1.7081
2022-07-07 00:38:40 - train: epoch 0084, iter [01200, 05004], lr: 0.065267, loss: 2.0259
2022-07-07 00:39:14 - train: epoch 0084, iter [01300, 05004], lr: 0.065252, loss: 1.6122
2022-07-07 00:39:49 - train: epoch 0084, iter [01400, 05004], lr: 0.065236, loss: 1.8141
2022-07-07 00:40:21 - train: epoch 0084, iter [01500, 05004], lr: 0.065221, loss: 1.7885
2022-07-07 00:40:56 - train: epoch 0084, iter [01600, 05004], lr: 0.065206, loss: 1.5020
2022-07-07 00:41:30 - train: epoch 0084, iter [01700, 05004], lr: 0.065190, loss: 1.7547
2022-07-07 00:42:03 - train: epoch 0084, iter [01800, 05004], lr: 0.065175, loss: 1.7172
2022-07-07 00:42:38 - train: epoch 0084, iter [01900, 05004], lr: 0.065160, loss: 1.6429
2022-07-07 00:43:11 - train: epoch 0084, iter [02000, 05004], lr: 0.065144, loss: 1.7024
2022-07-07 00:43:45 - train: epoch 0084, iter [02100, 05004], lr: 0.065129, loss: 1.5950
2022-07-07 00:44:18 - train: epoch 0084, iter [02200, 05004], lr: 0.065114, loss: 1.4909
2022-07-07 00:44:53 - train: epoch 0084, iter [02300, 05004], lr: 0.065098, loss: 1.7389
2022-07-07 00:45:26 - train: epoch 0084, iter [02400, 05004], lr: 0.065083, loss: 1.5717
2022-07-07 00:46:00 - train: epoch 0084, iter [02500, 05004], lr: 0.065068, loss: 1.7058
2022-07-07 00:46:34 - train: epoch 0084, iter [02600, 05004], lr: 0.065052, loss: 1.8189
2022-07-07 00:47:08 - train: epoch 0084, iter [02700, 05004], lr: 0.065037, loss: 1.6519
2022-07-07 00:47:41 - train: epoch 0084, iter [02800, 05004], lr: 0.065022, loss: 1.8809
2022-07-07 00:48:15 - train: epoch 0084, iter [02900, 05004], lr: 0.065006, loss: 1.7423
2022-07-07 00:48:48 - train: epoch 0084, iter [03000, 05004], lr: 0.064991, loss: 1.6911
2022-07-07 00:49:22 - train: epoch 0084, iter [03100, 05004], lr: 0.064975, loss: 1.6736
2022-07-07 00:49:57 - train: epoch 0084, iter [03200, 05004], lr: 0.064960, loss: 1.5687
2022-07-07 00:50:30 - train: epoch 0084, iter [03300, 05004], lr: 0.064945, loss: 1.8104
2022-07-07 00:51:04 - train: epoch 0084, iter [03400, 05004], lr: 0.064929, loss: 1.6337
2022-07-07 00:51:39 - train: epoch 0084, iter [03500, 05004], lr: 0.064914, loss: 1.6327
2022-07-07 00:52:11 - train: epoch 0084, iter [03600, 05004], lr: 0.064899, loss: 1.7886
2022-07-07 00:52:45 - train: epoch 0084, iter [03700, 05004], lr: 0.064883, loss: 1.8226
2022-07-07 00:53:20 - train: epoch 0084, iter [03800, 05004], lr: 0.064868, loss: 1.9593
2022-07-07 00:53:54 - train: epoch 0084, iter [03900, 05004], lr: 0.064853, loss: 1.7477
2022-07-07 00:54:27 - train: epoch 0084, iter [04000, 05004], lr: 0.064837, loss: 1.6284
2022-07-07 00:55:02 - train: epoch 0084, iter [04100, 05004], lr: 0.064822, loss: 1.7419
2022-07-07 00:55:36 - train: epoch 0084, iter [04200, 05004], lr: 0.064806, loss: 1.8337
2022-07-07 00:56:10 - train: epoch 0084, iter [04300, 05004], lr: 0.064791, loss: 1.7806
2022-07-07 00:56:43 - train: epoch 0084, iter [04400, 05004], lr: 0.064776, loss: 2.0419
2022-07-07 00:57:17 - train: epoch 0084, iter [04500, 05004], lr: 0.064760, loss: 1.5873
2022-07-07 00:57:51 - train: epoch 0084, iter [04600, 05004], lr: 0.064745, loss: 1.7223
2022-07-07 00:58:25 - train: epoch 0084, iter [04700, 05004], lr: 0.064730, loss: 1.9788
2022-07-07 00:58:59 - train: epoch 0084, iter [04800, 05004], lr: 0.064714, loss: 1.5544
2022-07-07 00:59:33 - train: epoch 0084, iter [04900, 05004], lr: 0.064699, loss: 1.5554
2022-07-07 01:00:06 - train: epoch 0084, iter [05000, 05004], lr: 0.064683, loss: 1.6261
2022-07-07 01:00:07 - train: epoch 084, train_loss: 1.6922
2022-07-07 01:01:21 - eval: epoch: 084, acc1: 61.552%, acc5: 84.680%, test_loss: 1.5967, per_image_load_time: 0.852ms, per_image_inference_time: 0.473ms
2022-07-07 01:01:21 - until epoch: 084, best_acc1: 63.036%
2022-07-07 01:01:21 - epoch 085 lr: 0.064683
2022-07-07 01:02:00 - train: epoch 0085, iter [00100, 05004], lr: 0.064667, loss: 1.5514
2022-07-07 01:02:33 - train: epoch 0085, iter [00200, 05004], lr: 0.064652, loss: 1.4960
2022-07-07 01:03:07 - train: epoch 0085, iter [00300, 05004], lr: 0.064637, loss: 1.7593
2022-07-07 01:03:41 - train: epoch 0085, iter [00400, 05004], lr: 0.064621, loss: 1.6029
2022-07-07 01:04:14 - train: epoch 0085, iter [00500, 05004], lr: 0.064606, loss: 1.8962
2022-07-07 01:04:47 - train: epoch 0085, iter [00600, 05004], lr: 0.064590, loss: 1.3590
2022-07-07 01:05:21 - train: epoch 0085, iter [00700, 05004], lr: 0.064575, loss: 1.4976
2022-07-07 01:05:55 - train: epoch 0085, iter [00800, 05004], lr: 0.064560, loss: 1.5414
2022-07-07 01:06:28 - train: epoch 0085, iter [00900, 05004], lr: 0.064544, loss: 1.5383
2022-07-07 01:07:02 - train: epoch 0085, iter [01000, 05004], lr: 0.064529, loss: 1.8109
2022-07-07 01:07:35 - train: epoch 0085, iter [01100, 05004], lr: 0.064513, loss: 1.7811
2022-07-07 01:08:09 - train: epoch 0085, iter [01200, 05004], lr: 0.064498, loss: 1.5993
2022-07-07 01:08:43 - train: epoch 0085, iter [01300, 05004], lr: 0.064483, loss: 1.5413
2022-07-07 01:09:16 - train: epoch 0085, iter [01400, 05004], lr: 0.064467, loss: 1.6402
2022-07-07 01:09:50 - train: epoch 0085, iter [01500, 05004], lr: 0.064452, loss: 1.5393
2022-07-07 01:10:24 - train: epoch 0085, iter [01600, 05004], lr: 0.064436, loss: 1.7120
2022-07-07 01:10:57 - train: epoch 0085, iter [01700, 05004], lr: 0.064421, loss: 1.9299
2022-07-07 01:11:31 - train: epoch 0085, iter [01800, 05004], lr: 0.064406, loss: 1.5596
2022-07-07 01:12:05 - train: epoch 0085, iter [01900, 05004], lr: 0.064390, loss: 1.5130
2022-07-07 01:12:39 - train: epoch 0085, iter [02000, 05004], lr: 0.064375, loss: 1.6519
2022-07-07 01:13:12 - train: epoch 0085, iter [02100, 05004], lr: 0.064359, loss: 1.3203
2022-07-07 01:13:47 - train: epoch 0085, iter [02200, 05004], lr: 0.064344, loss: 1.8733
2022-07-07 01:14:20 - train: epoch 0085, iter [02300, 05004], lr: 0.064328, loss: 1.5395
2022-07-07 01:14:54 - train: epoch 0085, iter [02400, 05004], lr: 0.064313, loss: 1.7398
2022-07-07 01:15:28 - train: epoch 0085, iter [02500, 05004], lr: 0.064298, loss: 2.0002
2022-07-07 01:16:02 - train: epoch 0085, iter [02600, 05004], lr: 0.064282, loss: 1.7833
2022-07-07 01:16:35 - train: epoch 0085, iter [02700, 05004], lr: 0.064267, loss: 1.5007
2022-07-07 01:17:10 - train: epoch 0085, iter [02800, 05004], lr: 0.064251, loss: 1.8782
2022-07-07 01:17:43 - train: epoch 0085, iter [02900, 05004], lr: 0.064236, loss: 1.8137
2022-07-07 01:18:17 - train: epoch 0085, iter [03000, 05004], lr: 0.064220, loss: 1.8717
2022-07-07 01:18:51 - train: epoch 0085, iter [03100, 05004], lr: 0.064205, loss: 1.6824
2022-07-07 01:19:24 - train: epoch 0085, iter [03200, 05004], lr: 0.064190, loss: 1.6777
2022-07-07 01:19:58 - train: epoch 0085, iter [03300, 05004], lr: 0.064174, loss: 1.7616
2022-07-07 01:20:31 - train: epoch 0085, iter [03400, 05004], lr: 0.064159, loss: 1.8246
2022-07-07 01:21:06 - train: epoch 0085, iter [03500, 05004], lr: 0.064143, loss: 1.7798
2022-07-07 01:21:39 - train: epoch 0085, iter [03600, 05004], lr: 0.064128, loss: 1.8196
2022-07-07 01:22:13 - train: epoch 0085, iter [03700, 05004], lr: 0.064112, loss: 1.6589
2022-07-07 01:22:46 - train: epoch 0085, iter [03800, 05004], lr: 0.064097, loss: 1.6732
2022-07-07 01:23:20 - train: epoch 0085, iter [03900, 05004], lr: 0.064081, loss: 1.6951
2022-07-07 01:23:54 - train: epoch 0085, iter [04000, 05004], lr: 0.064066, loss: 2.0117
2022-07-07 01:24:28 - train: epoch 0085, iter [04100, 05004], lr: 0.064051, loss: 1.7053
2022-07-07 01:25:02 - train: epoch 0085, iter [04200, 05004], lr: 0.064035, loss: 1.7961
2022-07-07 01:25:35 - train: epoch 0085, iter [04300, 05004], lr: 0.064020, loss: 1.7467
2022-07-07 01:26:09 - train: epoch 0085, iter [04400, 05004], lr: 0.064004, loss: 1.6618
2022-07-07 01:26:44 - train: epoch 0085, iter [04500, 05004], lr: 0.063989, loss: 1.8051
2022-07-07 01:27:17 - train: epoch 0085, iter [04600, 05004], lr: 0.063973, loss: 1.7712
2022-07-07 01:27:51 - train: epoch 0085, iter [04700, 05004], lr: 0.063958, loss: 1.9257
2022-07-07 01:28:24 - train: epoch 0085, iter [04800, 05004], lr: 0.063942, loss: 1.4866
2022-07-07 01:28:58 - train: epoch 0085, iter [04900, 05004], lr: 0.063927, loss: 1.7083
2022-07-07 01:29:31 - train: epoch 0085, iter [05000, 05004], lr: 0.063911, loss: 1.6721
2022-07-07 01:29:32 - train: epoch 085, train_loss: 1.6889
2022-07-07 01:30:47 - eval: epoch: 085, acc1: 64.008%, acc5: 86.078%, test_loss: 1.4902, per_image_load_time: 1.196ms, per_image_inference_time: 0.468ms
2022-07-07 01:30:48 - until epoch: 085, best_acc1: 64.008%
2022-07-07 01:30:48 - epoch 086 lr: 0.063911
2022-07-07 01:31:27 - train: epoch 0086, iter [00100, 05004], lr: 0.063895, loss: 1.5444
2022-07-07 01:32:01 - train: epoch 0086, iter [00200, 05004], lr: 0.063880, loss: 1.5631
2022-07-07 01:32:34 - train: epoch 0086, iter [00300, 05004], lr: 0.063864, loss: 1.9287
2022-07-07 01:33:07 - train: epoch 0086, iter [00400, 05004], lr: 0.063849, loss: 1.7486
2022-07-07 01:33:41 - train: epoch 0086, iter [00500, 05004], lr: 0.063834, loss: 1.6434
2022-07-07 01:34:14 - train: epoch 0086, iter [00600, 05004], lr: 0.063818, loss: 1.5943
2022-07-07 01:34:48 - train: epoch 0086, iter [00700, 05004], lr: 0.063803, loss: 1.5088
2022-07-07 01:35:22 - train: epoch 0086, iter [00800, 05004], lr: 0.063787, loss: 1.8966
2022-07-07 01:35:56 - train: epoch 0086, iter [00900, 05004], lr: 0.063772, loss: 1.7769
2022-07-07 01:36:30 - train: epoch 0086, iter [01000, 05004], lr: 0.063756, loss: 1.7504
2022-07-07 01:37:03 - train: epoch 0086, iter [01100, 05004], lr: 0.063741, loss: 1.7154
2022-07-07 01:37:37 - train: epoch 0086, iter [01200, 05004], lr: 0.063725, loss: 1.4957
2022-07-07 01:38:11 - train: epoch 0086, iter [01300, 05004], lr: 0.063710, loss: 1.7308
2022-07-07 01:38:45 - train: epoch 0086, iter [01400, 05004], lr: 0.063694, loss: 1.6730
2022-07-07 01:39:19 - train: epoch 0086, iter [01500, 05004], lr: 0.063679, loss: 1.4180
2022-07-07 01:39:53 - train: epoch 0086, iter [01600, 05004], lr: 0.063663, loss: 1.7721
2022-07-07 01:40:26 - train: epoch 0086, iter [01700, 05004], lr: 0.063648, loss: 1.5906
2022-07-07 01:40:59 - train: epoch 0086, iter [01800, 05004], lr: 0.063632, loss: 1.8561
2022-07-07 01:41:33 - train: epoch 0086, iter [01900, 05004], lr: 0.063617, loss: 1.7561
2022-07-07 01:42:06 - train: epoch 0086, iter [02000, 05004], lr: 0.063601, loss: 1.6052
2022-07-07 01:42:41 - train: epoch 0086, iter [02100, 05004], lr: 0.063586, loss: 1.5198
2022-07-07 01:43:15 - train: epoch 0086, iter [02200, 05004], lr: 0.063570, loss: 1.7842
2022-07-07 01:43:50 - train: epoch 0086, iter [02300, 05004], lr: 0.063555, loss: 1.6390
2022-07-07 01:44:23 - train: epoch 0086, iter [02400, 05004], lr: 0.063539, loss: 1.4535
2022-07-07 01:44:56 - train: epoch 0086, iter [02500, 05004], lr: 0.063524, loss: 1.5867
2022-07-07 01:45:30 - train: epoch 0086, iter [02600, 05004], lr: 0.063508, loss: 1.7239
2022-07-07 01:46:04 - train: epoch 0086, iter [02700, 05004], lr: 0.063493, loss: 1.5532
2022-07-07 01:46:38 - train: epoch 0086, iter [02800, 05004], lr: 0.063477, loss: 1.7538
2022-07-07 01:47:12 - train: epoch 0086, iter [02900, 05004], lr: 0.063462, loss: 1.4722
2022-07-07 01:47:46 - train: epoch 0086, iter [03000, 05004], lr: 0.063446, loss: 1.8103
2022-07-07 01:48:19 - train: epoch 0086, iter [03100, 05004], lr: 0.063431, loss: 1.7720
2022-07-07 01:48:54 - train: epoch 0086, iter [03200, 05004], lr: 0.063415, loss: 1.8024
2022-07-07 01:49:28 - train: epoch 0086, iter [03300, 05004], lr: 0.063400, loss: 1.8781
2022-07-07 01:50:01 - train: epoch 0086, iter [03400, 05004], lr: 0.063384, loss: 1.7505
2022-07-07 01:50:36 - train: epoch 0086, iter [03500, 05004], lr: 0.063369, loss: 1.7662
2022-07-07 01:51:10 - train: epoch 0086, iter [03600, 05004], lr: 0.063353, loss: 1.7397
2022-07-07 01:51:44 - train: epoch 0086, iter [03700, 05004], lr: 0.063338, loss: 1.6940
2022-07-07 01:52:18 - train: epoch 0086, iter [03800, 05004], lr: 0.063322, loss: 1.6374
2022-07-07 01:52:52 - train: epoch 0086, iter [03900, 05004], lr: 0.063307, loss: 1.7360
2022-07-07 01:53:25 - train: epoch 0086, iter [04000, 05004], lr: 0.063291, loss: 1.7796
2022-07-07 01:53:59 - train: epoch 0086, iter [04100, 05004], lr: 0.063276, loss: 1.7177
2022-07-07 01:54:33 - train: epoch 0086, iter [04200, 05004], lr: 0.063260, loss: 1.7810
2022-07-07 01:55:07 - train: epoch 0086, iter [04300, 05004], lr: 0.063245, loss: 1.7782
2022-07-07 01:55:40 - train: epoch 0086, iter [04400, 05004], lr: 0.063229, loss: 1.5412
2022-07-07 01:56:14 - train: epoch 0086, iter [04500, 05004], lr: 0.063214, loss: 1.5116
2022-07-07 01:56:48 - train: epoch 0086, iter [04600, 05004], lr: 0.063198, loss: 1.5656
2022-07-07 01:57:22 - train: epoch 0086, iter [04700, 05004], lr: 0.063183, loss: 1.6891
2022-07-07 01:57:56 - train: epoch 0086, iter [04800, 05004], lr: 0.063167, loss: 1.7037
2022-07-07 01:58:30 - train: epoch 0086, iter [04900, 05004], lr: 0.063152, loss: 1.6759
2022-07-07 01:59:02 - train: epoch 0086, iter [05000, 05004], lr: 0.063136, loss: 1.6799
2022-07-07 01:59:03 - train: epoch 086, train_loss: 1.6814
2022-07-07 02:00:18 - eval: epoch: 086, acc1: 63.908%, acc5: 86.264%, test_loss: 1.4930, per_image_load_time: 1.787ms, per_image_inference_time: 0.498ms
2022-07-07 02:00:18 - until epoch: 086, best_acc1: 64.008%
2022-07-07 02:00:18 - epoch 087 lr: 0.063135
2022-07-07 02:00:58 - train: epoch 0087, iter [00100, 05004], lr: 0.063120, loss: 1.6886
2022-07-07 02:01:31 - train: epoch 0087, iter [00200, 05004], lr: 0.063104, loss: 1.3567
2022-07-07 02:02:05 - train: epoch 0087, iter [00300, 05004], lr: 0.063089, loss: 1.8059
2022-07-07 02:02:39 - train: epoch 0087, iter [00400, 05004], lr: 0.063073, loss: 1.8128
2022-07-07 02:03:12 - train: epoch 0087, iter [00500, 05004], lr: 0.063058, loss: 1.3531
2022-07-07 02:03:46 - train: epoch 0087, iter [00600, 05004], lr: 0.063042, loss: 1.7353
2022-07-07 02:04:19 - train: epoch 0087, iter [00700, 05004], lr: 0.063027, loss: 1.4097
2022-07-07 02:04:53 - train: epoch 0087, iter [00800, 05004], lr: 0.063011, loss: 1.6828
2022-07-07 02:05:27 - train: epoch 0087, iter [00900, 05004], lr: 0.062996, loss: 1.7643
2022-07-07 02:06:01 - train: epoch 0087, iter [01000, 05004], lr: 0.062980, loss: 1.6435
2022-07-07 02:06:35 - train: epoch 0087, iter [01100, 05004], lr: 0.062964, loss: 1.6964
2022-07-07 02:07:08 - train: epoch 0087, iter [01200, 05004], lr: 0.062949, loss: 1.7152
2022-07-07 02:07:42 - train: epoch 0087, iter [01300, 05004], lr: 0.062933, loss: 1.9382
2022-07-07 02:08:16 - train: epoch 0087, iter [01400, 05004], lr: 0.062918, loss: 1.5625
2022-07-07 02:08:50 - train: epoch 0087, iter [01500, 05004], lr: 0.062902, loss: 1.5372
2022-07-07 02:09:23 - train: epoch 0087, iter [01600, 05004], lr: 0.062887, loss: 1.4921
2022-07-07 02:09:57 - train: epoch 0087, iter [01700, 05004], lr: 0.062871, loss: 1.8207
2022-07-07 02:10:30 - train: epoch 0087, iter [01800, 05004], lr: 0.062856, loss: 1.6943
2022-07-07 02:11:04 - train: epoch 0087, iter [01900, 05004], lr: 0.062840, loss: 1.7036
2022-07-07 02:11:37 - train: epoch 0087, iter [02000, 05004], lr: 0.062824, loss: 1.6993
2022-07-07 02:12:11 - train: epoch 0087, iter [02100, 05004], lr: 0.062809, loss: 1.7328
2022-07-07 02:12:46 - train: epoch 0087, iter [02200, 05004], lr: 0.062793, loss: 1.7853
2022-07-07 02:13:20 - train: epoch 0087, iter [02300, 05004], lr: 0.062778, loss: 1.7647
2022-07-07 02:13:53 - train: epoch 0087, iter [02400, 05004], lr: 0.062762, loss: 1.6210
2022-07-07 02:14:27 - train: epoch 0087, iter [02500, 05004], lr: 0.062747, loss: 1.6236
2022-07-07 02:15:01 - train: epoch 0087, iter [02600, 05004], lr: 0.062731, loss: 1.7215
2022-07-07 02:15:35 - train: epoch 0087, iter [02700, 05004], lr: 0.062716, loss: 1.6877
2022-07-07 02:16:09 - train: epoch 0087, iter [02800, 05004], lr: 0.062700, loss: 1.4981
2022-07-07 02:16:42 - train: epoch 0087, iter [02900, 05004], lr: 0.062684, loss: 1.5411
2022-07-07 02:17:16 - train: epoch 0087, iter [03000, 05004], lr: 0.062669, loss: 1.8211
2022-07-07 02:17:50 - train: epoch 0087, iter [03100, 05004], lr: 0.062653, loss: 1.7534
2022-07-07 02:18:24 - train: epoch 0087, iter [03200, 05004], lr: 0.062638, loss: 1.7035
2022-07-07 02:18:58 - train: epoch 0087, iter [03300, 05004], lr: 0.062622, loss: 1.6291
2022-07-07 02:19:32 - train: epoch 0087, iter [03400, 05004], lr: 0.062606, loss: 1.7155
2022-07-07 02:20:05 - train: epoch 0087, iter [03500, 05004], lr: 0.062591, loss: 1.5292
2022-07-07 02:20:39 - train: epoch 0087, iter [03600, 05004], lr: 0.062575, loss: 1.4918
2022-07-07 02:21:14 - train: epoch 0087, iter [03700, 05004], lr: 0.062560, loss: 1.6237
2022-07-07 02:21:47 - train: epoch 0087, iter [03800, 05004], lr: 0.062544, loss: 1.7360
2022-07-07 02:22:22 - train: epoch 0087, iter [03900, 05004], lr: 0.062529, loss: 1.6265
2022-07-07 02:22:55 - train: epoch 0087, iter [04000, 05004], lr: 0.062513, loss: 1.8590
2022-07-07 02:23:29 - train: epoch 0087, iter [04100, 05004], lr: 0.062497, loss: 1.8124
2022-07-07 02:24:03 - train: epoch 0087, iter [04200, 05004], lr: 0.062482, loss: 1.6973
2022-07-07 02:24:38 - train: epoch 0087, iter [04300, 05004], lr: 0.062466, loss: 1.6519
2022-07-07 02:25:12 - train: epoch 0087, iter [04400, 05004], lr: 0.062451, loss: 1.7206
2022-07-07 02:25:46 - train: epoch 0087, iter [04500, 05004], lr: 0.062435, loss: 1.6987
2022-07-07 02:26:19 - train: epoch 0087, iter [04600, 05004], lr: 0.062419, loss: 1.6859
2022-07-07 02:26:54 - train: epoch 0087, iter [04700, 05004], lr: 0.062404, loss: 1.7629
2022-07-07 02:27:27 - train: epoch 0087, iter [04800, 05004], lr: 0.062388, loss: 1.6783
2022-07-07 02:28:01 - train: epoch 0087, iter [04900, 05004], lr: 0.062373, loss: 1.7421
2022-07-07 02:28:34 - train: epoch 0087, iter [05000, 05004], lr: 0.062357, loss: 1.7797
2022-07-07 02:28:35 - train: epoch 087, train_loss: 1.6760
2022-07-07 02:29:49 - eval: epoch: 087, acc1: 63.438%, acc5: 85.812%, test_loss: 1.5145, per_image_load_time: 1.345ms, per_image_inference_time: 0.483ms
2022-07-07 02:29:50 - until epoch: 087, best_acc1: 64.008%
2022-07-07 02:29:50 - epoch 088 lr: 0.062356
2022-07-07 02:30:29 - train: epoch 0088, iter [00100, 05004], lr: 0.062341, loss: 1.4912
2022-07-07 02:31:04 - train: epoch 0088, iter [00200, 05004], lr: 0.062325, loss: 1.6614
2022-07-07 02:31:36 - train: epoch 0088, iter [00300, 05004], lr: 0.062310, loss: 1.6760
2022-07-07 02:32:10 - train: epoch 0088, iter [00400, 05004], lr: 0.062294, loss: 1.6720
2022-07-07 02:32:43 - train: epoch 0088, iter [00500, 05004], lr: 0.062278, loss: 1.5485
2022-07-07 02:33:17 - train: epoch 0088, iter [00600, 05004], lr: 0.062263, loss: 1.6476
2022-07-07 02:33:51 - train: epoch 0088, iter [00700, 05004], lr: 0.062247, loss: 1.6451
2022-07-07 02:34:25 - train: epoch 0088, iter [00800, 05004], lr: 0.062232, loss: 1.5473
2022-07-07 02:34:59 - train: epoch 0088, iter [00900, 05004], lr: 0.062216, loss: 1.7598
2022-07-07 02:35:32 - train: epoch 0088, iter [01000, 05004], lr: 0.062200, loss: 1.5336
2022-07-07 02:36:06 - train: epoch 0088, iter [01100, 05004], lr: 0.062185, loss: 1.7733
2022-07-07 02:36:40 - train: epoch 0088, iter [01200, 05004], lr: 0.062169, loss: 1.6121
2022-07-07 02:37:14 - train: epoch 0088, iter [01300, 05004], lr: 0.062154, loss: 1.7415
2022-07-07 02:37:47 - train: epoch 0088, iter [01400, 05004], lr: 0.062138, loss: 1.4972
2022-07-07 02:38:21 - train: epoch 0088, iter [01500, 05004], lr: 0.062122, loss: 1.6407
2022-07-07 02:38:55 - train: epoch 0088, iter [01600, 05004], lr: 0.062107, loss: 1.5976
2022-07-07 02:39:29 - train: epoch 0088, iter [01700, 05004], lr: 0.062091, loss: 1.6214
2022-07-07 02:40:03 - train: epoch 0088, iter [01800, 05004], lr: 0.062075, loss: 1.6825
2022-07-07 02:40:37 - train: epoch 0088, iter [01900, 05004], lr: 0.062060, loss: 1.8123
2022-07-07 02:41:11 - train: epoch 0088, iter [02000, 05004], lr: 0.062044, loss: 1.5753
2022-07-07 02:41:44 - train: epoch 0088, iter [02100, 05004], lr: 0.062029, loss: 1.6306
2022-07-07 02:42:18 - train: epoch 0088, iter [02200, 05004], lr: 0.062013, loss: 1.6873
2022-07-07 02:42:52 - train: epoch 0088, iter [02300, 05004], lr: 0.061997, loss: 1.4187
2022-07-07 02:43:25 - train: epoch 0088, iter [02400, 05004], lr: 0.061982, loss: 1.7974
2022-07-07 02:43:59 - train: epoch 0088, iter [02500, 05004], lr: 0.061966, loss: 1.7012
2022-07-07 02:44:34 - train: epoch 0088, iter [02600, 05004], lr: 0.061950, loss: 1.6360
2022-07-07 02:45:08 - train: epoch 0088, iter [02700, 05004], lr: 0.061935, loss: 1.7108
2022-07-07 02:45:41 - train: epoch 0088, iter [02800, 05004], lr: 0.061919, loss: 1.7283
2022-07-07 02:46:15 - train: epoch 0088, iter [02900, 05004], lr: 0.061904, loss: 1.7342
2022-07-07 02:46:49 - train: epoch 0088, iter [03000, 05004], lr: 0.061888, loss: 1.8219
2022-07-07 02:47:24 - train: epoch 0088, iter [03100, 05004], lr: 0.061872, loss: 1.5314
2022-07-07 02:47:58 - train: epoch 0088, iter [03200, 05004], lr: 0.061857, loss: 1.5645
2022-07-07 02:48:31 - train: epoch 0088, iter [03300, 05004], lr: 0.061841, loss: 1.5764
2022-07-07 02:49:05 - train: epoch 0088, iter [03400, 05004], lr: 0.061825, loss: 1.5006
2022-07-07 02:49:39 - train: epoch 0088, iter [03500, 05004], lr: 0.061810, loss: 1.5145
2022-07-07 02:50:13 - train: epoch 0088, iter [03600, 05004], lr: 0.061794, loss: 1.5954
2022-07-07 02:50:47 - train: epoch 0088, iter [03700, 05004], lr: 0.061778, loss: 1.7673
2022-07-07 02:51:21 - train: epoch 0088, iter [03800, 05004], lr: 0.061763, loss: 1.7730
2022-07-07 02:51:55 - train: epoch 0088, iter [03900, 05004], lr: 0.061747, loss: 1.9038
2022-07-07 02:52:30 - train: epoch 0088, iter [04000, 05004], lr: 0.061732, loss: 1.5188
2022-07-07 02:53:03 - train: epoch 0088, iter [04100, 05004], lr: 0.061716, loss: 1.8653
2022-07-07 02:53:37 - train: epoch 0088, iter [04200, 05004], lr: 0.061700, loss: 1.8851
2022-07-07 02:54:11 - train: epoch 0088, iter [04300, 05004], lr: 0.061685, loss: 1.7145
2022-07-07 02:54:45 - train: epoch 0088, iter [04400, 05004], lr: 0.061669, loss: 1.5714
2022-07-07 02:55:19 - train: epoch 0088, iter [04500, 05004], lr: 0.061653, loss: 1.6399
2022-07-07 02:55:52 - train: epoch 0088, iter [04600, 05004], lr: 0.061638, loss: 1.8940
2022-07-07 02:56:26 - train: epoch 0088, iter [04700, 05004], lr: 0.061622, loss: 1.8134
2022-07-07 02:57:01 - train: epoch 0088, iter [04800, 05004], lr: 0.061606, loss: 1.6579
2022-07-07 02:57:34 - train: epoch 0088, iter [04900, 05004], lr: 0.061591, loss: 1.4304
2022-07-07 02:58:07 - train: epoch 0088, iter [05000, 05004], lr: 0.061575, loss: 1.6555
2022-07-07 02:58:08 - train: epoch 088, train_loss: 1.6699
2022-07-07 02:59:23 - eval: epoch: 088, acc1: 61.284%, acc5: 84.492%, test_loss: 1.6090, per_image_load_time: 1.661ms, per_image_inference_time: 0.501ms
2022-07-07 02:59:23 - until epoch: 088, best_acc1: 64.008%
2022-07-07 02:59:23 - epoch 089 lr: 0.061574
2022-07-07 03:00:01 - train: epoch 0089, iter [00100, 05004], lr: 0.061559, loss: 1.8576
2022-07-07 03:00:36 - train: epoch 0089, iter [00200, 05004], lr: 0.061543, loss: 1.3720
2022-07-07 03:01:10 - train: epoch 0089, iter [00300, 05004], lr: 0.061527, loss: 1.6469
2022-07-07 03:01:43 - train: epoch 0089, iter [00400, 05004], lr: 0.061512, loss: 1.5654
2022-07-07 03:02:16 - train: epoch 0089, iter [00500, 05004], lr: 0.061496, loss: 1.5743
2022-07-07 03:02:49 - train: epoch 0089, iter [00600, 05004], lr: 0.061480, loss: 1.5596
2022-07-07 03:03:23 - train: epoch 0089, iter [00700, 05004], lr: 0.061465, loss: 1.6919
2022-07-07 03:03:56 - train: epoch 0089, iter [00800, 05004], lr: 0.061449, loss: 1.9016
2022-07-07 03:04:30 - train: epoch 0089, iter [00900, 05004], lr: 0.061433, loss: 1.6516
2022-07-07 03:05:03 - train: epoch 0089, iter [01000, 05004], lr: 0.061418, loss: 1.7821
2022-07-07 03:05:37 - train: epoch 0089, iter [01100, 05004], lr: 0.061402, loss: 1.6564
2022-07-07 03:06:10 - train: epoch 0089, iter [01200, 05004], lr: 0.061386, loss: 1.7892
2022-07-07 03:06:45 - train: epoch 0089, iter [01300, 05004], lr: 0.061371, loss: 1.6892
2022-07-07 03:07:17 - train: epoch 0089, iter [01400, 05004], lr: 0.061355, loss: 1.6402
2022-07-07 03:07:51 - train: epoch 0089, iter [01500, 05004], lr: 0.061339, loss: 1.7198
2022-07-07 03:08:26 - train: epoch 0089, iter [01600, 05004], lr: 0.061324, loss: 1.5021
2022-07-07 03:08:59 - train: epoch 0089, iter [01700, 05004], lr: 0.061308, loss: 1.7285
2022-07-07 03:09:33 - train: epoch 0089, iter [01800, 05004], lr: 0.061292, loss: 1.6242
2022-07-07 03:10:06 - train: epoch 0089, iter [01900, 05004], lr: 0.061277, loss: 1.5488
2022-07-07 03:10:41 - train: epoch 0089, iter [02000, 05004], lr: 0.061261, loss: 1.5441
2022-07-07 03:11:14 - train: epoch 0089, iter [02100, 05004], lr: 0.061245, loss: 1.7517
2022-07-07 03:11:48 - train: epoch 0089, iter [02200, 05004], lr: 0.061230, loss: 1.7998
2022-07-07 03:12:22 - train: epoch 0089, iter [02300, 05004], lr: 0.061214, loss: 1.5538
2022-07-07 03:12:56 - train: epoch 0089, iter [02400, 05004], lr: 0.061198, loss: 1.7783
2022-07-07 03:13:30 - train: epoch 0089, iter [02500, 05004], lr: 0.061182, loss: 1.6393
2022-07-07 03:14:04 - train: epoch 0089, iter [02600, 05004], lr: 0.061167, loss: 1.6378
2022-07-07 03:14:38 - train: epoch 0089, iter [02700, 05004], lr: 0.061151, loss: 1.6231
2022-07-07 03:15:11 - train: epoch 0089, iter [02800, 05004], lr: 0.061135, loss: 1.7052
2022-07-07 03:15:46 - train: epoch 0089, iter [02900, 05004], lr: 0.061120, loss: 1.5866
2022-07-07 03:16:19 - train: epoch 0089, iter [03000, 05004], lr: 0.061104, loss: 1.7781
2022-07-07 03:16:53 - train: epoch 0089, iter [03100, 05004], lr: 0.061088, loss: 1.7694
2022-07-07 03:17:26 - train: epoch 0089, iter [03200, 05004], lr: 0.061073, loss: 1.8056
2022-07-07 03:18:01 - train: epoch 0089, iter [03300, 05004], lr: 0.061057, loss: 1.8653
2022-07-07 03:18:35 - train: epoch 0089, iter [03400, 05004], lr: 0.061041, loss: 1.6960
2022-07-07 03:19:09 - train: epoch 0089, iter [03500, 05004], lr: 0.061025, loss: 1.3770
2022-07-07 03:19:42 - train: epoch 0089, iter [03600, 05004], lr: 0.061010, loss: 1.6125
2022-07-07 03:20:16 - train: epoch 0089, iter [03700, 05004], lr: 0.060994, loss: 1.8711
2022-07-07 03:20:51 - train: epoch 0089, iter [03800, 05004], lr: 0.060978, loss: 1.6184
2022-07-07 03:21:24 - train: epoch 0089, iter [03900, 05004], lr: 0.060963, loss: 1.8387
2022-07-07 03:21:58 - train: epoch 0089, iter [04000, 05004], lr: 0.060947, loss: 1.5749
2022-07-07 03:22:32 - train: epoch 0089, iter [04100, 05004], lr: 0.060931, loss: 1.9148
2022-07-07 03:23:05 - train: epoch 0089, iter [04200, 05004], lr: 0.060916, loss: 1.7478
2022-07-07 03:23:40 - train: epoch 0089, iter [04300, 05004], lr: 0.060900, loss: 1.6768
2022-07-07 03:24:14 - train: epoch 0089, iter [04400, 05004], lr: 0.060884, loss: 1.6288
2022-07-07 03:24:48 - train: epoch 0089, iter [04500, 05004], lr: 0.060868, loss: 1.5180
2022-07-07 03:25:21 - train: epoch 0089, iter [04600, 05004], lr: 0.060853, loss: 1.6208
2022-07-07 03:25:56 - train: epoch 0089, iter [04700, 05004], lr: 0.060837, loss: 1.6470
2022-07-07 03:26:30 - train: epoch 0089, iter [04800, 05004], lr: 0.060821, loss: 1.6833
2022-07-07 03:27:04 - train: epoch 0089, iter [04900, 05004], lr: 0.060806, loss: 1.6438
2022-07-07 03:27:37 - train: epoch 0089, iter [05000, 05004], lr: 0.060790, loss: 1.4293
2022-07-07 03:27:38 - train: epoch 089, train_loss: 1.6639
2022-07-07 03:28:53 - eval: epoch: 089, acc1: 61.858%, acc5: 84.784%, test_loss: 1.5855, per_image_load_time: 0.719ms, per_image_inference_time: 0.480ms
2022-07-07 03:28:53 - until epoch: 089, best_acc1: 64.008%
2022-07-07 03:28:53 - epoch 090 lr: 0.060789
2022-07-07 03:29:33 - train: epoch 0090, iter [00100, 05004], lr: 0.060773, loss: 1.5602
2022-07-07 03:30:06 - train: epoch 0090, iter [00200, 05004], lr: 0.060758, loss: 1.6541
2022-07-07 03:30:40 - train: epoch 0090, iter [00300, 05004], lr: 0.060742, loss: 1.5750
2022-07-07 03:31:13 - train: epoch 0090, iter [00400, 05004], lr: 0.060726, loss: 1.5007
2022-07-07 03:31:47 - train: epoch 0090, iter [00500, 05004], lr: 0.060711, loss: 1.5706
2022-07-07 03:32:20 - train: epoch 0090, iter [00600, 05004], lr: 0.060695, loss: 1.7434
2022-07-07 03:32:54 - train: epoch 0090, iter [00700, 05004], lr: 0.060679, loss: 1.5970
2022-07-07 03:33:28 - train: epoch 0090, iter [00800, 05004], lr: 0.060663, loss: 1.6402
2022-07-07 03:34:01 - train: epoch 0090, iter [00900, 05004], lr: 0.060648, loss: 1.5544
2022-07-07 03:34:35 - train: epoch 0090, iter [01000, 05004], lr: 0.060632, loss: 1.5776
2022-07-07 03:35:09 - train: epoch 0090, iter [01100, 05004], lr: 0.060616, loss: 1.7211
2022-07-07 03:35:43 - train: epoch 0090, iter [01200, 05004], lr: 0.060601, loss: 1.6838
2022-07-07 03:36:16 - train: epoch 0090, iter [01300, 05004], lr: 0.060585, loss: 1.8108
2022-07-07 03:36:49 - train: epoch 0090, iter [01400, 05004], lr: 0.060569, loss: 1.4814
2022-07-07 03:37:23 - train: epoch 0090, iter [01500, 05004], lr: 0.060553, loss: 1.8791
2022-07-07 03:37:57 - train: epoch 0090, iter [01600, 05004], lr: 0.060538, loss: 1.6334
2022-07-07 03:38:30 - train: epoch 0090, iter [01700, 05004], lr: 0.060522, loss: 1.4290
2022-07-07 03:39:04 - train: epoch 0090, iter [01800, 05004], lr: 0.060506, loss: 1.4843
2022-07-07 03:39:38 - train: epoch 0090, iter [01900, 05004], lr: 0.060490, loss: 1.4738
2022-07-07 03:40:11 - train: epoch 0090, iter [02000, 05004], lr: 0.060475, loss: 1.7576
2022-07-07 03:40:45 - train: epoch 0090, iter [02100, 05004], lr: 0.060459, loss: 1.8776
2022-07-07 03:41:19 - train: epoch 0090, iter [02200, 05004], lr: 0.060443, loss: 1.6622
2022-07-07 03:41:53 - train: epoch 0090, iter [02300, 05004], lr: 0.060427, loss: 1.9055
2022-07-07 03:42:27 - train: epoch 0090, iter [02400, 05004], lr: 0.060412, loss: 1.6188
2022-07-07 03:43:01 - train: epoch 0090, iter [02500, 05004], lr: 0.060396, loss: 1.7617
2022-07-07 03:43:34 - train: epoch 0090, iter [02600, 05004], lr: 0.060380, loss: 1.7075
2022-07-07 03:44:08 - train: epoch 0090, iter [02700, 05004], lr: 0.060364, loss: 1.6024
2022-07-07 03:44:43 - train: epoch 0090, iter [02800, 05004], lr: 0.060349, loss: 1.7274
2022-07-07 03:45:17 - train: epoch 0090, iter [02900, 05004], lr: 0.060333, loss: 1.5936
2022-07-07 03:45:51 - train: epoch 0090, iter [03000, 05004], lr: 0.060317, loss: 1.6612
2022-07-07 03:46:25 - train: epoch 0090, iter [03100, 05004], lr: 0.060301, loss: 1.4737
2022-07-07 03:46:59 - train: epoch 0090, iter [03200, 05004], lr: 0.060286, loss: 1.5460
2022-07-07 03:47:33 - train: epoch 0090, iter [03300, 05004], lr: 0.060270, loss: 1.8756
2022-07-07 03:48:08 - train: epoch 0090, iter [03400, 05004], lr: 0.060254, loss: 1.5112
2022-07-07 03:48:41 - train: epoch 0090, iter [03500, 05004], lr: 0.060238, loss: 1.6269
2022-07-07 03:49:15 - train: epoch 0090, iter [03600, 05004], lr: 0.060223, loss: 1.4907
2022-07-07 03:49:48 - train: epoch 0090, iter [03700, 05004], lr: 0.060207, loss: 1.6890
2022-07-07 03:50:23 - train: epoch 0090, iter [03800, 05004], lr: 0.060191, loss: 1.7381
2022-07-07 03:50:56 - train: epoch 0090, iter [03900, 05004], lr: 0.060175, loss: 1.5243
2022-07-07 03:51:30 - train: epoch 0090, iter [04000, 05004], lr: 0.060160, loss: 1.6586
2022-07-07 03:52:04 - train: epoch 0090, iter [04100, 05004], lr: 0.060144, loss: 2.0050
2022-07-07 03:52:39 - train: epoch 0090, iter [04200, 05004], lr: 0.060128, loss: 1.8533
2022-07-07 03:53:13 - train: epoch 0090, iter [04300, 05004], lr: 0.060112, loss: 1.7483
2022-07-07 03:53:47 - train: epoch 0090, iter [04400, 05004], lr: 0.060097, loss: 1.6834
2022-07-07 03:54:20 - train: epoch 0090, iter [04500, 05004], lr: 0.060081, loss: 1.5028
2022-07-07 03:54:54 - train: epoch 0090, iter [04600, 05004], lr: 0.060065, loss: 1.6032
2022-07-07 03:55:29 - train: epoch 0090, iter [04700, 05004], lr: 0.060049, loss: 1.6394
2022-07-07 03:56:03 - train: epoch 0090, iter [04800, 05004], lr: 0.060033, loss: 1.7357
2022-07-07 03:56:36 - train: epoch 0090, iter [04900, 05004], lr: 0.060018, loss: 1.5913
2022-07-07 03:57:09 - train: epoch 0090, iter [05000, 05004], lr: 0.060002, loss: 1.6871
2022-07-07 03:57:10 - train: epoch 090, train_loss: 1.6568
2022-07-07 03:58:24 - eval: epoch: 090, acc1: 61.736%, acc5: 84.720%, test_loss: 1.5949, per_image_load_time: 1.388ms, per_image_inference_time: 0.480ms
2022-07-07 03:58:25 - until epoch: 090, best_acc1: 64.008%
2022-07-07 03:58:25 - epoch 091 lr: 0.060001
2022-07-07 03:59:04 - train: epoch 0091, iter [00100, 05004], lr: 0.059986, loss: 1.5591
2022-07-07 03:59:37 - train: epoch 0091, iter [00200, 05004], lr: 0.059970, loss: 1.5767
2022-07-07 04:00:11 - train: epoch 0091, iter [00300, 05004], lr: 0.059954, loss: 1.5761
2022-07-07 04:00:45 - train: epoch 0091, iter [00400, 05004], lr: 0.059938, loss: 1.4730
2022-07-07 04:01:18 - train: epoch 0091, iter [00500, 05004], lr: 0.059922, loss: 1.5431
2022-07-07 04:01:52 - train: epoch 0091, iter [00600, 05004], lr: 0.059907, loss: 1.6982
2022-07-07 04:02:24 - train: epoch 0091, iter [00700, 05004], lr: 0.059891, loss: 1.7184
2022-07-07 04:02:58 - train: epoch 0091, iter [00800, 05004], lr: 0.059875, loss: 1.3835
2022-07-07 04:03:32 - train: epoch 0091, iter [00900, 05004], lr: 0.059859, loss: 1.6392
2022-07-07 04:04:06 - train: epoch 0091, iter [01000, 05004], lr: 0.059844, loss: 1.5034
2022-07-07 04:04:39 - train: epoch 0091, iter [01100, 05004], lr: 0.059828, loss: 1.3764
2022-07-07 04:05:13 - train: epoch 0091, iter [01200, 05004], lr: 0.059812, loss: 1.5534
2022-07-07 04:05:47 - train: epoch 0091, iter [01300, 05004], lr: 0.059796, loss: 1.4857
2022-07-07 04:06:20 - train: epoch 0091, iter [01400, 05004], lr: 0.059780, loss: 1.5609
2022-07-07 04:06:54 - train: epoch 0091, iter [01500, 05004], lr: 0.059765, loss: 1.6063
2022-07-07 04:07:27 - train: epoch 0091, iter [01600, 05004], lr: 0.059749, loss: 1.7083
2022-07-07 04:08:01 - train: epoch 0091, iter [01700, 05004], lr: 0.059733, loss: 1.7638
2022-07-07 04:08:34 - train: epoch 0091, iter [01800, 05004], lr: 0.059717, loss: 1.7248
2022-07-07 04:09:09 - train: epoch 0091, iter [01900, 05004], lr: 0.059701, loss: 1.5105
2022-07-07 04:09:42 - train: epoch 0091, iter [02000, 05004], lr: 0.059686, loss: 1.6570
2022-07-07 04:10:15 - train: epoch 0091, iter [02100, 05004], lr: 0.059670, loss: 1.4117
2022-07-07 04:10:49 - train: epoch 0091, iter [02200, 05004], lr: 0.059654, loss: 1.7222
2022-07-07 04:11:23 - train: epoch 0091, iter [02300, 05004], lr: 0.059638, loss: 1.6038
2022-07-07 04:11:56 - train: epoch 0091, iter [02400, 05004], lr: 0.059622, loss: 1.6684
2022-07-07 04:12:31 - train: epoch 0091, iter [02500, 05004], lr: 0.059607, loss: 1.6510
2022-07-07 04:13:04 - train: epoch 0091, iter [02600, 05004], lr: 0.059591, loss: 1.7140
2022-07-07 04:13:38 - train: epoch 0091, iter [02700, 05004], lr: 0.059575, loss: 1.5079
2022-07-07 04:14:10 - train: epoch 0091, iter [02800, 05004], lr: 0.059559, loss: 1.6584
2022-07-07 04:14:45 - train: epoch 0091, iter [02900, 05004], lr: 0.059543, loss: 1.6956
2022-07-07 04:15:20 - train: epoch 0091, iter [03000, 05004], lr: 0.059528, loss: 1.8471
2022-07-07 04:15:53 - train: epoch 0091, iter [03100, 05004], lr: 0.059512, loss: 1.5871
2022-07-07 04:16:26 - train: epoch 0091, iter [03200, 05004], lr: 0.059496, loss: 1.8492
2022-07-07 04:16:59 - train: epoch 0091, iter [03300, 05004], lr: 0.059480, loss: 1.5422
2022-07-07 04:17:33 - train: epoch 0091, iter [03400, 05004], lr: 0.059464, loss: 1.6013
2022-07-07 04:18:07 - train: epoch 0091, iter [03500, 05004], lr: 0.059449, loss: 1.7244
2022-07-07 04:18:41 - train: epoch 0091, iter [03600, 05004], lr: 0.059433, loss: 1.6650
2022-07-07 04:19:15 - train: epoch 0091, iter [03700, 05004], lr: 0.059417, loss: 1.7691
2022-07-07 04:19:49 - train: epoch 0091, iter [03800, 05004], lr: 0.059401, loss: 1.6127
2022-07-07 04:20:22 - train: epoch 0091, iter [03900, 05004], lr: 0.059385, loss: 1.6449
2022-07-07 04:20:57 - train: epoch 0091, iter [04000, 05004], lr: 0.059370, loss: 1.7022
2022-07-07 04:21:30 - train: epoch 0091, iter [04100, 05004], lr: 0.059354, loss: 1.6845
2022-07-07 04:22:04 - train: epoch 0091, iter [04200, 05004], lr: 0.059338, loss: 1.7607
2022-07-07 04:22:38 - train: epoch 0091, iter [04300, 05004], lr: 0.059322, loss: 1.7992
2022-07-07 04:23:11 - train: epoch 0091, iter [04400, 05004], lr: 0.059306, loss: 1.4497
2022-07-07 04:23:45 - train: epoch 0091, iter [04500, 05004], lr: 0.059290, loss: 1.5891
2022-07-07 04:24:18 - train: epoch 0091, iter [04600, 05004], lr: 0.059275, loss: 1.6699
2022-07-07 04:24:52 - train: epoch 0091, iter [04700, 05004], lr: 0.059259, loss: 1.8430
2022-07-07 04:25:27 - train: epoch 0091, iter [04800, 05004], lr: 0.059243, loss: 1.6581
2022-07-07 04:26:00 - train: epoch 0091, iter [04900, 05004], lr: 0.059227, loss: 1.5752
2022-07-07 04:26:33 - train: epoch 0091, iter [05000, 05004], lr: 0.059211, loss: 2.0218
2022-07-07 04:26:34 - train: epoch 091, train_loss: 1.6517
2022-07-07 04:27:48 - eval: epoch: 091, acc1: 59.270%, acc5: 82.756%, test_loss: 1.7267, per_image_load_time: 1.922ms, per_image_inference_time: 0.482ms
2022-07-07 04:27:48 - until epoch: 091, best_acc1: 64.008%
2022-07-07 04:27:48 - epoch 092 lr: 0.059211
2022-07-07 04:28:27 - train: epoch 0092, iter [00100, 05004], lr: 0.059195, loss: 1.6434
2022-07-07 04:29:00 - train: epoch 0092, iter [00200, 05004], lr: 0.059179, loss: 1.6396
2022-07-07 04:29:34 - train: epoch 0092, iter [00300, 05004], lr: 0.059163, loss: 1.5512
2022-07-07 04:30:08 - train: epoch 0092, iter [00400, 05004], lr: 0.059147, loss: 1.4737
2022-07-07 04:30:41 - train: epoch 0092, iter [00500, 05004], lr: 0.059132, loss: 1.7112
2022-07-07 04:31:14 - train: epoch 0092, iter [00600, 05004], lr: 0.059116, loss: 1.5848
2022-07-07 04:31:48 - train: epoch 0092, iter [00700, 05004], lr: 0.059100, loss: 1.6488
2022-07-07 04:32:23 - train: epoch 0092, iter [00800, 05004], lr: 0.059084, loss: 1.7330
2022-07-07 04:32:56 - train: epoch 0092, iter [00900, 05004], lr: 0.059068, loss: 1.7108
2022-07-07 04:33:30 - train: epoch 0092, iter [01000, 05004], lr: 0.059052, loss: 1.7961
2022-07-07 04:34:03 - train: epoch 0092, iter [01100, 05004], lr: 0.059037, loss: 1.6142
2022-07-07 04:34:37 - train: epoch 0092, iter [01200, 05004], lr: 0.059021, loss: 1.4425
2022-07-07 04:35:10 - train: epoch 0092, iter [01300, 05004], lr: 0.059005, loss: 1.6020
2022-07-07 04:35:43 - train: epoch 0092, iter [01400, 05004], lr: 0.058989, loss: 1.4729
2022-07-07 04:36:17 - train: epoch 0092, iter [01500, 05004], lr: 0.058973, loss: 1.5523
2022-07-07 04:36:50 - train: epoch 0092, iter [01600, 05004], lr: 0.058957, loss: 1.5745
2022-07-07 04:37:24 - train: epoch 0092, iter [01700, 05004], lr: 0.058942, loss: 1.6840
2022-07-07 04:37:57 - train: epoch 0092, iter [01800, 05004], lr: 0.058926, loss: 1.7425
2022-07-07 04:38:31 - train: epoch 0092, iter [01900, 05004], lr: 0.058910, loss: 1.5172
2022-07-07 04:39:05 - train: epoch 0092, iter [02000, 05004], lr: 0.058894, loss: 1.6144
2022-07-07 04:39:37 - train: epoch 0092, iter [02100, 05004], lr: 0.058878, loss: 1.5996
2022-07-07 04:40:11 - train: epoch 0092, iter [02200, 05004], lr: 0.058862, loss: 1.6348
2022-07-07 04:40:45 - train: epoch 0092, iter [02300, 05004], lr: 0.058847, loss: 1.6809
2022-07-07 04:41:18 - train: epoch 0092, iter [02400, 05004], lr: 0.058831, loss: 1.4902
2022-07-07 04:41:52 - train: epoch 0092, iter [02500, 05004], lr: 0.058815, loss: 1.7279
2022-07-07 04:42:26 - train: epoch 0092, iter [02600, 05004], lr: 0.058799, loss: 1.6957
2022-07-07 04:43:00 - train: epoch 0092, iter [02700, 05004], lr: 0.058783, loss: 1.7025
2022-07-07 04:43:33 - train: epoch 0092, iter [02800, 05004], lr: 0.058767, loss: 1.5419
2022-07-07 04:44:08 - train: epoch 0092, iter [02900, 05004], lr: 0.058752, loss: 1.7861
2022-07-07 04:44:40 - train: epoch 0092, iter [03000, 05004], lr: 0.058736, loss: 1.5414
2022-07-07 04:45:14 - train: epoch 0092, iter [03100, 05004], lr: 0.058720, loss: 1.6163
2022-07-07 04:45:48 - train: epoch 0092, iter [03200, 05004], lr: 0.058704, loss: 1.5177
2022-07-07 04:46:22 - train: epoch 0092, iter [03300, 05004], lr: 0.058688, loss: 1.6677
2022-07-07 04:46:55 - train: epoch 0092, iter [03400, 05004], lr: 0.058672, loss: 1.8666
2022-07-07 04:47:29 - train: epoch 0092, iter [03500, 05004], lr: 0.058656, loss: 1.5109
2022-07-07 04:48:02 - train: epoch 0092, iter [03600, 05004], lr: 0.058641, loss: 1.6631
2022-07-07 04:48:36 - train: epoch 0092, iter [03700, 05004], lr: 0.058625, loss: 1.5802
2022-07-07 04:49:10 - train: epoch 0092, iter [03800, 05004], lr: 0.058609, loss: 1.8560
2022-07-07 04:49:43 - train: epoch 0092, iter [03900, 05004], lr: 0.058593, loss: 1.7340
2022-07-07 04:50:16 - train: epoch 0092, iter [04000, 05004], lr: 0.058577, loss: 1.7448
2022-07-07 04:50:50 - train: epoch 0092, iter [04100, 05004], lr: 0.058561, loss: 1.3890
2022-07-07 04:51:23 - train: epoch 0092, iter [04200, 05004], lr: 0.058545, loss: 1.7026
2022-07-07 04:51:57 - train: epoch 0092, iter [04300, 05004], lr: 0.058530, loss: 1.6237
2022-07-07 04:52:31 - train: epoch 0092, iter [04400, 05004], lr: 0.058514, loss: 1.5299
2022-07-07 04:53:05 - train: epoch 0092, iter [04500, 05004], lr: 0.058498, loss: 1.6287
2022-07-07 04:53:39 - train: epoch 0092, iter [04600, 05004], lr: 0.058482, loss: 1.6860
2022-07-07 04:54:13 - train: epoch 0092, iter [04700, 05004], lr: 0.058466, loss: 1.3339
2022-07-07 04:54:47 - train: epoch 0092, iter [04800, 05004], lr: 0.058450, loss: 1.5413
2022-07-07 04:55:21 - train: epoch 0092, iter [04900, 05004], lr: 0.058434, loss: 1.5722
2022-07-07 04:55:54 - train: epoch 0092, iter [05000, 05004], lr: 0.058418, loss: 1.7776
2022-07-07 04:55:55 - train: epoch 092, train_loss: 1.6432
2022-07-07 04:57:09 - eval: epoch: 092, acc1: 64.272%, acc5: 86.282%, test_loss: 1.4833, per_image_load_time: 0.577ms, per_image_inference_time: 0.452ms
2022-07-07 04:57:09 - until epoch: 092, best_acc1: 64.272%
2022-07-07 04:57:09 - epoch 093 lr: 0.058418
2022-07-07 04:57:49 - train: epoch 0093, iter [00100, 05004], lr: 0.058402, loss: 1.4830
2022-07-07 04:58:22 - train: epoch 0093, iter [00200, 05004], lr: 0.058386, loss: 1.3780
2022-07-07 04:58:55 - train: epoch 0093, iter [00300, 05004], lr: 0.058370, loss: 1.6393
2022-07-07 04:59:29 - train: epoch 0093, iter [00400, 05004], lr: 0.058354, loss: 1.5315
2022-07-07 05:00:03 - train: epoch 0093, iter [00500, 05004], lr: 0.058339, loss: 1.6620
2022-07-07 05:00:35 - train: epoch 0093, iter [00600, 05004], lr: 0.058323, loss: 1.5451
2022-07-07 05:01:10 - train: epoch 0093, iter [00700, 05004], lr: 0.058307, loss: 1.7861
2022-07-07 05:01:44 - train: epoch 0093, iter [00800, 05004], lr: 0.058291, loss: 1.4962
2022-07-07 05:02:17 - train: epoch 0093, iter [00900, 05004], lr: 0.058275, loss: 1.5238
2022-07-07 05:02:51 - train: epoch 0093, iter [01000, 05004], lr: 0.058259, loss: 1.3657
2022-07-07 05:03:24 - train: epoch 0093, iter [01100, 05004], lr: 0.058243, loss: 1.5150
2022-07-07 05:03:58 - train: epoch 0093, iter [01200, 05004], lr: 0.058227, loss: 1.5985
2022-07-07 05:04:31 - train: epoch 0093, iter [01300, 05004], lr: 0.058211, loss: 1.5872
2022-07-07 05:05:06 - train: epoch 0093, iter [01400, 05004], lr: 0.058196, loss: 1.6535
2022-07-07 05:05:39 - train: epoch 0093, iter [01500, 05004], lr: 0.058180, loss: 1.7354
2022-07-07 05:06:13 - train: epoch 0093, iter [01600, 05004], lr: 0.058164, loss: 1.7119
2022-07-07 05:06:47 - train: epoch 0093, iter [01700, 05004], lr: 0.058148, loss: 1.4768
2022-07-07 05:07:21 - train: epoch 0093, iter [01800, 05004], lr: 0.058132, loss: 1.7578
2022-07-07 05:07:54 - train: epoch 0093, iter [01900, 05004], lr: 0.058116, loss: 1.5964
2022-07-07 05:08:28 - train: epoch 0093, iter [02000, 05004], lr: 0.058100, loss: 1.5389
2022-07-07 05:09:01 - train: epoch 0093, iter [02100, 05004], lr: 0.058084, loss: 1.4531
2022-07-07 05:09:35 - train: epoch 0093, iter [02200, 05004], lr: 0.058069, loss: 1.8326
2022-07-07 05:10:08 - train: epoch 0093, iter [02300, 05004], lr: 0.058053, loss: 1.6563
2022-07-07 05:10:42 - train: epoch 0093, iter [02400, 05004], lr: 0.058037, loss: 1.4877
2022-07-07 05:11:16 - train: epoch 0093, iter [02500, 05004], lr: 0.058021, loss: 1.5627
2022-07-07 05:11:49 - train: epoch 0093, iter [02600, 05004], lr: 0.058005, loss: 1.9310
2022-07-07 05:12:22 - train: epoch 0093, iter [02700, 05004], lr: 0.057989, loss: 1.4934
2022-07-07 05:12:57 - train: epoch 0093, iter [02800, 05004], lr: 0.057973, loss: 1.5859
2022-07-07 05:13:30 - train: epoch 0093, iter [02900, 05004], lr: 0.057957, loss: 1.5724
2022-07-07 05:14:04 - train: epoch 0093, iter [03000, 05004], lr: 0.057941, loss: 1.4857
2022-07-07 05:14:38 - train: epoch 0093, iter [03100, 05004], lr: 0.057926, loss: 1.9157
2022-07-07 05:15:12 - train: epoch 0093, iter [03200, 05004], lr: 0.057910, loss: 1.5232
2022-07-07 05:15:45 - train: epoch 0093, iter [03300, 05004], lr: 0.057894, loss: 1.5655
2022-07-07 05:16:19 - train: epoch 0093, iter [03400, 05004], lr: 0.057878, loss: 1.9206
2022-07-07 05:16:53 - train: epoch 0093, iter [03500, 05004], lr: 0.057862, loss: 1.5224
2022-07-07 05:17:26 - train: epoch 0093, iter [03600, 05004], lr: 0.057846, loss: 1.7187
2022-07-07 05:17:59 - train: epoch 0093, iter [03700, 05004], lr: 0.057830, loss: 1.5308
2022-07-07 05:18:33 - train: epoch 0093, iter [03800, 05004], lr: 0.057814, loss: 1.4647
2022-07-07 05:19:08 - train: epoch 0093, iter [03900, 05004], lr: 0.057798, loss: 1.7225
2022-07-07 05:19:41 - train: epoch 0093, iter [04000, 05004], lr: 0.057782, loss: 1.8821
2022-07-07 05:20:15 - train: epoch 0093, iter [04100, 05004], lr: 0.057767, loss: 1.7483
2022-07-07 05:20:49 - train: epoch 0093, iter [04200, 05004], lr: 0.057751, loss: 1.7687
2022-07-07 05:21:22 - train: epoch 0093, iter [04300, 05004], lr: 0.057735, loss: 1.8949
2022-07-07 05:21:57 - train: epoch 0093, iter [04400, 05004], lr: 0.057719, loss: 1.5406
2022-07-07 05:22:30 - train: epoch 0093, iter [04500, 05004], lr: 0.057703, loss: 1.5359
2022-07-07 05:23:04 - train: epoch 0093, iter [04600, 05004], lr: 0.057687, loss: 1.4397
2022-07-07 05:23:38 - train: epoch 0093, iter [04700, 05004], lr: 0.057671, loss: 2.0633
2022-07-07 05:24:11 - train: epoch 0093, iter [04800, 05004], lr: 0.057655, loss: 1.6259
2022-07-07 05:24:46 - train: epoch 0093, iter [04900, 05004], lr: 0.057639, loss: 1.7933
2022-07-07 05:25:18 - train: epoch 0093, iter [05000, 05004], lr: 0.057623, loss: 1.5755
2022-07-07 05:25:19 - train: epoch 093, train_loss: 1.6386
2022-07-07 05:26:34 - eval: epoch: 093, acc1: 63.030%, acc5: 85.502%, test_loss: 1.5342, per_image_load_time: 0.614ms, per_image_inference_time: 0.464ms
2022-07-07 05:26:35 - until epoch: 093, best_acc1: 64.272%
2022-07-07 05:26:35 - epoch 094 lr: 0.057623
2022-07-07 05:27:13 - train: epoch 0094, iter [00100, 05004], lr: 0.057607, loss: 1.6900
2022-07-07 05:27:47 - train: epoch 0094, iter [00200, 05004], lr: 0.057591, loss: 1.6346
2022-07-07 05:28:21 - train: epoch 0094, iter [00300, 05004], lr: 0.057575, loss: 1.7568
2022-07-07 05:28:54 - train: epoch 0094, iter [00400, 05004], lr: 0.057559, loss: 1.8199
2022-07-07 05:29:29 - train: epoch 0094, iter [00500, 05004], lr: 0.057543, loss: 1.5062
2022-07-07 05:30:02 - train: epoch 0094, iter [00600, 05004], lr: 0.057527, loss: 1.3789
2022-07-07 05:30:36 - train: epoch 0094, iter [00700, 05004], lr: 0.057511, loss: 1.6482
2022-07-07 05:31:09 - train: epoch 0094, iter [00800, 05004], lr: 0.057495, loss: 1.5618
2022-07-07 05:31:43 - train: epoch 0094, iter [00900, 05004], lr: 0.057480, loss: 1.6201
2022-07-07 05:32:17 - train: epoch 0094, iter [01000, 05004], lr: 0.057464, loss: 1.8078
2022-07-07 05:32:50 - train: epoch 0094, iter [01100, 05004], lr: 0.057448, loss: 1.9198
2022-07-07 05:33:24 - train: epoch 0094, iter [01200, 05004], lr: 0.057432, loss: 1.5838
2022-07-07 05:33:58 - train: epoch 0094, iter [01300, 05004], lr: 0.057416, loss: 1.6324
2022-07-07 05:34:31 - train: epoch 0094, iter [01400, 05004], lr: 0.057400, loss: 1.5406
2022-07-07 05:35:04 - train: epoch 0094, iter [01500, 05004], lr: 0.057384, loss: 1.7104
2022-07-07 05:35:40 - train: epoch 0094, iter [01600, 05004], lr: 0.057368, loss: 1.9449
2022-07-07 05:36:12 - train: epoch 0094, iter [01700, 05004], lr: 0.057352, loss: 1.5494
2022-07-07 05:36:46 - train: epoch 0094, iter [01800, 05004], lr: 0.057336, loss: 1.5267
2022-07-07 05:37:20 - train: epoch 0094, iter [01900, 05004], lr: 0.057320, loss: 1.5461
2022-07-07 05:37:52 - train: epoch 0094, iter [02000, 05004], lr: 0.057304, loss: 1.4747
2022-07-07 05:38:26 - train: epoch 0094, iter [02100, 05004], lr: 0.057288, loss: 1.5933
2022-07-07 05:39:00 - train: epoch 0094, iter [02200, 05004], lr: 0.057273, loss: 1.4561
2022-07-07 05:39:34 - train: epoch 0094, iter [02300, 05004], lr: 0.057257, loss: 1.5108
2022-07-07 05:40:09 - train: epoch 0094, iter [02400, 05004], lr: 0.057241, loss: 1.6056
2022-07-07 05:40:42 - train: epoch 0094, iter [02500, 05004], lr: 0.057225, loss: 1.5525
2022-07-07 05:41:15 - train: epoch 0094, iter [02600, 05004], lr: 0.057209, loss: 1.4834
2022-07-07 05:41:49 - train: epoch 0094, iter [02700, 05004], lr: 0.057193, loss: 1.5828
2022-07-07 05:42:24 - train: epoch 0094, iter [02800, 05004], lr: 0.057177, loss: 1.6652
2022-07-07 05:42:58 - train: epoch 0094, iter [02900, 05004], lr: 0.057161, loss: 1.6093
2022-07-07 05:43:31 - train: epoch 0094, iter [03000, 05004], lr: 0.057145, loss: 1.5812
2022-07-07 05:44:05 - train: epoch 0094, iter [03100, 05004], lr: 0.057129, loss: 1.9054
2022-07-07 05:44:39 - train: epoch 0094, iter [03200, 05004], lr: 0.057113, loss: 1.5957
2022-07-07 05:45:13 - train: epoch 0094, iter [03300, 05004], lr: 0.057097, loss: 1.5462
2022-07-07 05:45:46 - train: epoch 0094, iter [03400, 05004], lr: 0.057081, loss: 1.7403
2022-07-07 05:46:21 - train: epoch 0094, iter [03500, 05004], lr: 0.057065, loss: 1.8406
2022-07-07 05:46:54 - train: epoch 0094, iter [03600, 05004], lr: 0.057050, loss: 1.6708
2022-07-07 05:47:28 - train: epoch 0094, iter [03700, 05004], lr: 0.057034, loss: 1.8466
2022-07-07 05:48:02 - train: epoch 0094, iter [03800, 05004], lr: 0.057018, loss: 1.5870
2022-07-07 05:48:36 - train: epoch 0094, iter [03900, 05004], lr: 0.057002, loss: 1.5964
2022-07-07 05:49:10 - train: epoch 0094, iter [04000, 05004], lr: 0.056986, loss: 1.7241
2022-07-07 05:49:43 - train: epoch 0094, iter [04100, 05004], lr: 0.056970, loss: 1.9058
2022-07-07 05:50:18 - train: epoch 0094, iter [04200, 05004], lr: 0.056954, loss: 1.4851
2022-07-07 05:50:52 - train: epoch 0094, iter [04300, 05004], lr: 0.056938, loss: 1.8257
2022-07-07 05:51:25 - train: epoch 0094, iter [04400, 05004], lr: 0.056922, loss: 1.6995
2022-07-07 05:51:59 - train: epoch 0094, iter [04500, 05004], lr: 0.056906, loss: 1.7319
2022-07-07 05:52:33 - train: epoch 0094, iter [04600, 05004], lr: 0.056890, loss: 1.6969
2022-07-07 05:53:06 - train: epoch 0094, iter [04700, 05004], lr: 0.056874, loss: 1.7559
2022-07-07 05:53:40 - train: epoch 0094, iter [04800, 05004], lr: 0.056858, loss: 1.6553
2022-07-07 05:54:15 - train: epoch 0094, iter [04900, 05004], lr: 0.056842, loss: 1.6261
2022-07-07 05:54:47 - train: epoch 0094, iter [05000, 05004], lr: 0.056826, loss: 1.4845
2022-07-07 05:54:48 - train: epoch 094, train_loss: 1.6346
2022-07-07 05:56:02 - eval: epoch: 094, acc1: 61.262%, acc5: 84.314%, test_loss: 1.6245, per_image_load_time: 0.945ms, per_image_inference_time: 0.481ms
2022-07-07 05:56:02 - until epoch: 094, best_acc1: 64.272%
2022-07-07 05:56:02 - epoch 095 lr: 0.056826
2022-07-07 05:56:42 - train: epoch 0095, iter [00100, 05004], lr: 0.056810, loss: 1.4718
2022-07-07 05:57:15 - train: epoch 0095, iter [00200, 05004], lr: 0.056794, loss: 1.7642
2022-07-07 05:57:47 - train: epoch 0095, iter [00300, 05004], lr: 0.056778, loss: 1.5438
2022-07-07 05:58:22 - train: epoch 0095, iter [00400, 05004], lr: 0.056762, loss: 1.7126
2022-07-07 05:58:55 - train: epoch 0095, iter [00500, 05004], lr: 0.056746, loss: 1.6189
2022-07-07 05:59:28 - train: epoch 0095, iter [00600, 05004], lr: 0.056730, loss: 1.5062
2022-07-07 06:00:02 - train: epoch 0095, iter [00700, 05004], lr: 0.056714, loss: 1.7489
2022-07-07 06:00:37 - train: epoch 0095, iter [00800, 05004], lr: 0.056698, loss: 1.8719
2022-07-07 06:01:11 - train: epoch 0095, iter [00900, 05004], lr: 0.056682, loss: 1.8985
2022-07-07 06:01:44 - train: epoch 0095, iter [01000, 05004], lr: 0.056666, loss: 1.7082
2022-07-07 06:02:18 - train: epoch 0095, iter [01100, 05004], lr: 0.056650, loss: 1.4704
2022-07-07 06:02:51 - train: epoch 0095, iter [01200, 05004], lr: 0.056634, loss: 1.4954
2022-07-07 06:03:25 - train: epoch 0095, iter [01300, 05004], lr: 0.056618, loss: 1.5249
2022-07-07 06:03:59 - train: epoch 0095, iter [01400, 05004], lr: 0.056602, loss: 1.7076
2022-07-07 06:04:34 - train: epoch 0095, iter [01500, 05004], lr: 0.056586, loss: 1.7243
2022-07-07 06:05:07 - train: epoch 0095, iter [01600, 05004], lr: 0.056570, loss: 1.3553
2022-07-07 06:05:41 - train: epoch 0095, iter [01700, 05004], lr: 0.056554, loss: 1.5851
2022-07-07 06:06:15 - train: epoch 0095, iter [01800, 05004], lr: 0.056539, loss: 1.5894
2022-07-07 06:06:48 - train: epoch 0095, iter [01900, 05004], lr: 0.056523, loss: 1.5635
2022-07-07 06:07:23 - train: epoch 0095, iter [02000, 05004], lr: 0.056507, loss: 1.7447
2022-07-07 06:07:57 - train: epoch 0095, iter [02100, 05004], lr: 0.056491, loss: 1.7043
2022-07-07 06:08:31 - train: epoch 0095, iter [02200, 05004], lr: 0.056475, loss: 1.2752
2022-07-07 06:09:05 - train: epoch 0095, iter [02300, 05004], lr: 0.056459, loss: 1.5237
2022-07-07 06:09:39 - train: epoch 0095, iter [02400, 05004], lr: 0.056443, loss: 1.7937
2022-07-07 06:10:13 - train: epoch 0095, iter [02500, 05004], lr: 0.056427, loss: 1.5111
2022-07-07 06:10:47 - train: epoch 0095, iter [02600, 05004], lr: 0.056411, loss: 1.7035
2022-07-07 06:11:21 - train: epoch 0095, iter [02700, 05004], lr: 0.056395, loss: 1.5655
2022-07-07 06:11:55 - train: epoch 0095, iter [02800, 05004], lr: 0.056379, loss: 1.3909
2022-07-07 06:12:29 - train: epoch 0095, iter [02900, 05004], lr: 0.056363, loss: 1.5286
2022-07-07 06:13:03 - train: epoch 0095, iter [03000, 05004], lr: 0.056347, loss: 1.7957
2022-07-07 06:13:37 - train: epoch 0095, iter [03100, 05004], lr: 0.056331, loss: 1.8758
2022-07-07 06:14:10 - train: epoch 0095, iter [03200, 05004], lr: 0.056315, loss: 1.5900
2022-07-07 06:14:44 - train: epoch 0095, iter [03300, 05004], lr: 0.056299, loss: 1.6271
2022-07-07 06:15:18 - train: epoch 0095, iter [03400, 05004], lr: 0.056283, loss: 1.4792
2022-07-07 06:15:52 - train: epoch 0095, iter [03500, 05004], lr: 0.056267, loss: 1.5207
2022-07-07 06:16:26 - train: epoch 0095, iter [03600, 05004], lr: 0.056251, loss: 1.6104
2022-07-07 06:17:00 - train: epoch 0095, iter [03700, 05004], lr: 0.056235, loss: 1.4600
2022-07-07 06:17:33 - train: epoch 0095, iter [03800, 05004], lr: 0.056219, loss: 1.4682
2022-07-07 06:18:08 - train: epoch 0095, iter [03900, 05004], lr: 0.056203, loss: 1.6440
2022-07-07 06:18:41 - train: epoch 0095, iter [04000, 05004], lr: 0.056187, loss: 1.3671
2022-07-07 06:19:15 - train: epoch 0095, iter [04100, 05004], lr: 0.056171, loss: 1.6823
2022-07-07 06:19:49 - train: epoch 0095, iter [04200, 05004], lr: 0.056155, loss: 1.5490
2022-07-07 06:20:23 - train: epoch 0095, iter [04300, 05004], lr: 0.056139, loss: 1.7015
2022-07-07 06:20:56 - train: epoch 0095, iter [04400, 05004], lr: 0.056123, loss: 1.6949
2022-07-07 06:21:31 - train: epoch 0095, iter [04500, 05004], lr: 0.056107, loss: 1.4648
2022-07-07 06:22:05 - train: epoch 0095, iter [04600, 05004], lr: 0.056091, loss: 1.4913
2022-07-07 06:22:39 - train: epoch 0095, iter [04700, 05004], lr: 0.056075, loss: 1.5844
2022-07-07 06:23:12 - train: epoch 0095, iter [04800, 05004], lr: 0.056059, loss: 1.6979
2022-07-07 06:23:46 - train: epoch 0095, iter [04900, 05004], lr: 0.056043, loss: 1.5176
2022-07-07 06:24:19 - train: epoch 0095, iter [05000, 05004], lr: 0.056027, loss: 1.4667
2022-07-07 06:24:20 - train: epoch 095, train_loss: 1.6282
2022-07-07 06:25:34 - eval: epoch: 095, acc1: 61.092%, acc5: 84.118%, test_loss: 1.6273, per_image_load_time: 1.738ms, per_image_inference_time: 0.495ms
2022-07-07 06:25:35 - until epoch: 095, best_acc1: 64.272%
2022-07-07 06:25:35 - epoch 096 lr: 0.056027
2022-07-07 06:26:14 - train: epoch 0096, iter [00100, 05004], lr: 0.056011, loss: 1.5864
2022-07-07 06:26:47 - train: epoch 0096, iter [00200, 05004], lr: 0.055995, loss: 1.4338
2022-07-07 06:27:21 - train: epoch 0096, iter [00300, 05004], lr: 0.055979, loss: 1.6840
2022-07-07 06:27:55 - train: epoch 0096, iter [00400, 05004], lr: 0.055963, loss: 1.4292
2022-07-07 06:28:28 - train: epoch 0096, iter [00500, 05004], lr: 0.055947, loss: 1.6050
2022-07-07 06:29:02 - train: epoch 0096, iter [00600, 05004], lr: 0.055931, loss: 1.7163
2022-07-07 06:29:35 - train: epoch 0096, iter [00700, 05004], lr: 0.055915, loss: 1.4638
2022-07-07 06:30:10 - train: epoch 0096, iter [00800, 05004], lr: 0.055899, loss: 1.5018
2022-07-07 06:30:43 - train: epoch 0096, iter [00900, 05004], lr: 0.055883, loss: 1.5596
2022-07-07 06:31:17 - train: epoch 0096, iter [01000, 05004], lr: 0.055867, loss: 1.5272
2022-07-07 06:31:50 - train: epoch 0096, iter [01100, 05004], lr: 0.055851, loss: 1.6604
2022-07-07 06:32:24 - train: epoch 0096, iter [01200, 05004], lr: 0.055835, loss: 1.6415
2022-07-07 06:32:58 - train: epoch 0096, iter [01300, 05004], lr: 0.055819, loss: 1.6602
2022-07-07 06:33:32 - train: epoch 0096, iter [01400, 05004], lr: 0.055803, loss: 1.6914
2022-07-07 06:34:05 - train: epoch 0096, iter [01500, 05004], lr: 0.055787, loss: 1.5271
2022-07-07 06:34:39 - train: epoch 0096, iter [01600, 05004], lr: 0.055771, loss: 1.2796
2022-07-07 06:35:12 - train: epoch 0096, iter [01700, 05004], lr: 0.055755, loss: 1.5777
2022-07-07 06:35:47 - train: epoch 0096, iter [01800, 05004], lr: 0.055739, loss: 1.7672
2022-07-07 06:36:19 - train: epoch 0096, iter [01900, 05004], lr: 0.055723, loss: 1.7053
2022-07-07 06:36:53 - train: epoch 0096, iter [02000, 05004], lr: 0.055707, loss: 1.5088
2022-07-07 06:37:27 - train: epoch 0096, iter [02100, 05004], lr: 0.055691, loss: 1.5078
2022-07-07 06:38:01 - train: epoch 0096, iter [02200, 05004], lr: 0.055675, loss: 1.4096
2022-07-07 06:38:35 - train: epoch 0096, iter [02300, 05004], lr: 0.055659, loss: 1.4973
2022-07-07 06:39:08 - train: epoch 0096, iter [02400, 05004], lr: 0.055643, loss: 1.4950
2022-07-07 06:39:42 - train: epoch 0096, iter [02500, 05004], lr: 0.055627, loss: 1.5390
2022-07-07 06:40:16 - train: epoch 0096, iter [02600, 05004], lr: 0.055611, loss: 1.4656
2022-07-07 06:40:50 - train: epoch 0096, iter [02700, 05004], lr: 0.055595, loss: 1.6850
2022-07-07 06:41:23 - train: epoch 0096, iter [02800, 05004], lr: 0.055579, loss: 1.8023
2022-07-07 06:41:57 - train: epoch 0096, iter [02900, 05004], lr: 0.055563, loss: 1.5894
2022-07-07 06:42:31 - train: epoch 0096, iter [03000, 05004], lr: 0.055547, loss: 1.6719
2022-07-07 06:43:04 - train: epoch 0096, iter [03100, 05004], lr: 0.055531, loss: 1.7812
2022-07-07 06:43:38 - train: epoch 0096, iter [03200, 05004], lr: 0.055515, loss: 1.7287
2022-07-07 06:44:12 - train: epoch 0096, iter [03300, 05004], lr: 0.055499, loss: 1.8207
2022-07-07 06:44:45 - train: epoch 0096, iter [03400, 05004], lr: 0.055483, loss: 1.3645
2022-07-07 06:45:19 - train: epoch 0096, iter [03500, 05004], lr: 0.055467, loss: 1.4926
2022-07-07 06:45:54 - train: epoch 0096, iter [03600, 05004], lr: 0.055451, loss: 1.4630
2022-07-07 06:46:27 - train: epoch 0096, iter [03700, 05004], lr: 0.055435, loss: 1.6178
2022-07-07 06:47:01 - train: epoch 0096, iter [03800, 05004], lr: 0.055419, loss: 1.4785
2022-07-07 06:47:35 - train: epoch 0096, iter [03900, 05004], lr: 0.055403, loss: 1.5738
2022-07-07 06:48:09 - train: epoch 0096, iter [04000, 05004], lr: 0.055387, loss: 1.4237
2022-07-07 06:48:42 - train: epoch 0096, iter [04100, 05004], lr: 0.055371, loss: 1.5167
2022-07-07 06:49:17 - train: epoch 0096, iter [04200, 05004], lr: 0.055355, loss: 1.6689
2022-07-07 06:49:50 - train: epoch 0096, iter [04300, 05004], lr: 0.055339, loss: 1.3139
2022-07-07 06:50:24 - train: epoch 0096, iter [04400, 05004], lr: 0.055323, loss: 1.4984
2022-07-07 06:50:57 - train: epoch 0096, iter [04500, 05004], lr: 0.055307, loss: 1.5100
2022-07-07 06:51:31 - train: epoch 0096, iter [04600, 05004], lr: 0.055291, loss: 1.5343
2022-07-07 06:52:05 - train: epoch 0096, iter [04700, 05004], lr: 0.055275, loss: 1.6624
2022-07-07 06:52:38 - train: epoch 0096, iter [04800, 05004], lr: 0.055259, loss: 1.6182
2022-07-07 06:53:12 - train: epoch 0096, iter [04900, 05004], lr: 0.055243, loss: 1.7931
2022-07-07 06:53:45 - train: epoch 0096, iter [05000, 05004], lr: 0.055227, loss: 1.7032
2022-07-07 06:53:46 - train: epoch 096, train_loss: 1.6197
2022-07-07 06:55:01 - eval: epoch: 096, acc1: 64.952%, acc5: 86.798%, test_loss: 1.4362, per_image_load_time: 1.104ms, per_image_inference_time: 0.478ms
2022-07-07 06:55:01 - until epoch: 096, best_acc1: 64.952%
2022-07-07 06:55:01 - epoch 097 lr: 0.055226
2022-07-07 06:55:41 - train: epoch 0097, iter [00100, 05004], lr: 0.055210, loss: 1.7614
2022-07-07 06:56:13 - train: epoch 0097, iter [00200, 05004], lr: 0.055194, loss: 1.3230
2022-07-07 06:56:48 - train: epoch 0097, iter [00300, 05004], lr: 0.055178, loss: 1.7857
2022-07-07 06:57:21 - train: epoch 0097, iter [00400, 05004], lr: 0.055162, loss: 1.7822
2022-07-07 06:57:55 - train: epoch 0097, iter [00500, 05004], lr: 0.055146, loss: 1.5895
2022-07-07 06:58:29 - train: epoch 0097, iter [00600, 05004], lr: 0.055130, loss: 1.7906
2022-07-07 06:59:02 - train: epoch 0097, iter [00700, 05004], lr: 0.055114, loss: 1.5398
2022-07-07 06:59:36 - train: epoch 0097, iter [00800, 05004], lr: 0.055098, loss: 1.4147
2022-07-07 07:00:10 - train: epoch 0097, iter [00900, 05004], lr: 0.055082, loss: 1.5390
2022-07-07 07:00:44 - train: epoch 0097, iter [01000, 05004], lr: 0.055066, loss: 1.5790
2022-07-07 07:01:18 - train: epoch 0097, iter [01100, 05004], lr: 0.055050, loss: 1.4083
2022-07-07 07:01:51 - train: epoch 0097, iter [01200, 05004], lr: 0.055034, loss: 1.4843
2022-07-07 07:02:26 - train: epoch 0097, iter [01300, 05004], lr: 0.055018, loss: 1.5579
2022-07-07 07:03:00 - train: epoch 0097, iter [01400, 05004], lr: 0.055002, loss: 1.5299
2022-07-07 07:03:33 - train: epoch 0097, iter [01500, 05004], lr: 0.054986, loss: 1.6294
2022-07-07 07:04:08 - train: epoch 0097, iter [01600, 05004], lr: 0.054970, loss: 1.5777
2022-07-07 07:04:41 - train: epoch 0097, iter [01700, 05004], lr: 0.054954, loss: 1.5167
2022-07-07 07:05:15 - train: epoch 0097, iter [01800, 05004], lr: 0.054938, loss: 1.5902
2022-07-07 07:05:48 - train: epoch 0097, iter [01900, 05004], lr: 0.054922, loss: 1.5113
2022-07-07 07:06:22 - train: epoch 0097, iter [02000, 05004], lr: 0.054906, loss: 1.6187
2022-07-07 07:06:57 - train: epoch 0097, iter [02100, 05004], lr: 0.054890, loss: 1.6166
2022-07-07 07:07:30 - train: epoch 0097, iter [02200, 05004], lr: 0.054874, loss: 1.7481
2022-07-07 07:08:03 - train: epoch 0097, iter [02300, 05004], lr: 0.054858, loss: 1.5594
2022-07-07 07:08:37 - train: epoch 0097, iter [02400, 05004], lr: 0.054842, loss: 1.6332
2022-07-07 07:09:10 - train: epoch 0097, iter [02500, 05004], lr: 0.054826, loss: 1.6283
2022-07-07 07:09:45 - train: epoch 0097, iter [02600, 05004], lr: 0.054810, loss: 1.7150
2022-07-07 07:10:19 - train: epoch 0097, iter [02700, 05004], lr: 0.054794, loss: 1.4641
2022-07-07 07:10:52 - train: epoch 0097, iter [02800, 05004], lr: 0.054778, loss: 1.4558
2022-07-07 07:11:27 - train: epoch 0097, iter [02900, 05004], lr: 0.054762, loss: 1.7132
2022-07-07 07:11:59 - train: epoch 0097, iter [03000, 05004], lr: 0.054746, loss: 1.6857
2022-07-07 07:12:33 - train: epoch 0097, iter [03100, 05004], lr: 0.054730, loss: 1.5832
2022-07-07 07:13:09 - train: epoch 0097, iter [03200, 05004], lr: 0.054714, loss: 1.6120
2022-07-07 07:13:43 - train: epoch 0097, iter [03300, 05004], lr: 0.054698, loss: 1.7974
2022-07-07 07:14:16 - train: epoch 0097, iter [03400, 05004], lr: 0.054682, loss: 1.7965
2022-07-07 07:14:50 - train: epoch 0097, iter [03500, 05004], lr: 0.054666, loss: 1.7159
2022-07-07 07:15:25 - train: epoch 0097, iter [03600, 05004], lr: 0.054650, loss: 1.6373
2022-07-07 07:15:58 - train: epoch 0097, iter [03700, 05004], lr: 0.054634, loss: 1.4320
2022-07-07 07:16:32 - train: epoch 0097, iter [03800, 05004], lr: 0.054618, loss: 1.4794
2022-07-07 07:17:05 - train: epoch 0097, iter [03900, 05004], lr: 0.054602, loss: 1.6897
2022-07-07 07:17:39 - train: epoch 0097, iter [04000, 05004], lr: 0.054586, loss: 1.7375
2022-07-07 07:18:12 - train: epoch 0097, iter [04100, 05004], lr: 0.054570, loss: 1.5625
2022-07-07 07:18:46 - train: epoch 0097, iter [04200, 05004], lr: 0.054554, loss: 1.5134
2022-07-07 07:19:20 - train: epoch 0097, iter [04300, 05004], lr: 0.054538, loss: 1.5286
2022-07-07 07:19:54 - train: epoch 0097, iter [04400, 05004], lr: 0.054521, loss: 1.5954
2022-07-07 07:20:28 - train: epoch 0097, iter [04500, 05004], lr: 0.054505, loss: 1.7003
2022-07-07 07:21:02 - train: epoch 0097, iter [04600, 05004], lr: 0.054489, loss: 1.7621
2022-07-07 07:21:36 - train: epoch 0097, iter [04700, 05004], lr: 0.054473, loss: 1.5593
2022-07-07 07:22:10 - train: epoch 0097, iter [04800, 05004], lr: 0.054457, loss: 1.6483
2022-07-07 07:22:44 - train: epoch 0097, iter [04900, 05004], lr: 0.054441, loss: 1.7924
2022-07-07 07:23:16 - train: epoch 0097, iter [05000, 05004], lr: 0.054425, loss: 1.7186
2022-07-07 07:23:17 - train: epoch 097, train_loss: 1.6136
2022-07-07 07:24:32 - eval: epoch: 097, acc1: 65.240%, acc5: 87.186%, test_loss: 1.4207, per_image_load_time: 0.533ms, per_image_inference_time: 0.439ms
2022-07-07 07:24:33 - until epoch: 097, best_acc1: 65.240%
2022-07-07 07:24:33 - epoch 098 lr: 0.054424
2022-07-07 07:25:12 - train: epoch 0098, iter [00100, 05004], lr: 0.054409, loss: 1.8318
2022-07-07 07:25:45 - train: epoch 0098, iter [00200, 05004], lr: 0.054393, loss: 1.6932
2022-07-07 07:26:19 - train: epoch 0098, iter [00300, 05004], lr: 0.054377, loss: 1.8533
2022-07-07 07:26:52 - train: epoch 0098, iter [00400, 05004], lr: 0.054361, loss: 1.3540
2022-07-07 07:27:25 - train: epoch 0098, iter [00500, 05004], lr: 0.054344, loss: 1.5660
2022-07-07 07:28:00 - train: epoch 0098, iter [00600, 05004], lr: 0.054328, loss: 1.7370
2022-07-07 07:28:34 - train: epoch 0098, iter [00700, 05004], lr: 0.054312, loss: 1.5543
2022-07-07 07:29:06 - train: epoch 0098, iter [00800, 05004], lr: 0.054296, loss: 1.7212
2022-07-07 07:29:40 - train: epoch 0098, iter [00900, 05004], lr: 0.054280, loss: 1.5501
2022-07-07 07:30:14 - train: epoch 0098, iter [01000, 05004], lr: 0.054264, loss: 1.4064
2022-07-07 07:30:48 - train: epoch 0098, iter [01100, 05004], lr: 0.054248, loss: 1.3000
2022-07-07 07:31:20 - train: epoch 0098, iter [01200, 05004], lr: 0.054232, loss: 1.5964
2022-07-07 07:31:55 - train: epoch 0098, iter [01300, 05004], lr: 0.054216, loss: 1.7839
2022-07-07 07:32:28 - train: epoch 0098, iter [01400, 05004], lr: 0.054200, loss: 1.5943
2022-07-07 07:33:01 - train: epoch 0098, iter [01500, 05004], lr: 0.054184, loss: 1.5540
2022-07-07 07:33:36 - train: epoch 0098, iter [01600, 05004], lr: 0.054168, loss: 1.5358
2022-07-07 07:34:10 - train: epoch 0098, iter [01700, 05004], lr: 0.054152, loss: 1.7102
2022-07-07 07:34:45 - train: epoch 0098, iter [01800, 05004], lr: 0.054136, loss: 1.5826
2022-07-07 07:35:18 - train: epoch 0098, iter [01900, 05004], lr: 0.054120, loss: 1.4732
2022-07-07 07:35:52 - train: epoch 0098, iter [02000, 05004], lr: 0.054104, loss: 1.7879
2022-07-07 07:36:25 - train: epoch 0098, iter [02100, 05004], lr: 0.054088, loss: 1.5903
2022-07-07 07:37:00 - train: epoch 0098, iter [02200, 05004], lr: 0.054072, loss: 1.5308
2022-07-07 07:37:33 - train: epoch 0098, iter [02300, 05004], lr: 0.054056, loss: 1.5164
2022-07-07 07:38:08 - train: epoch 0098, iter [02400, 05004], lr: 0.054040, loss: 1.5673
2022-07-07 07:38:42 - train: epoch 0098, iter [02500, 05004], lr: 0.054024, loss: 1.8041
2022-07-07 07:39:16 - train: epoch 0098, iter [02600, 05004], lr: 0.054008, loss: 1.5900
2022-07-07 07:39:50 - train: epoch 0098, iter [02700, 05004], lr: 0.053992, loss: 1.7309
2022-07-07 07:40:23 - train: epoch 0098, iter [02800, 05004], lr: 0.053976, loss: 1.5930
2022-07-07 07:40:57 - train: epoch 0098, iter [02900, 05004], lr: 0.053959, loss: 1.5486
