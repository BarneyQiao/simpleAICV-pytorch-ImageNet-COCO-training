2022-07-08 07:41:40 - train: epoch 0147, iter [03400, 05004], lr: 0.017341, loss: 1.3717
2022-07-08 07:42:14 - train: epoch 0147, iter [03500, 05004], lr: 0.017329, loss: 1.1268
2022-07-08 07:42:47 - train: epoch 0147, iter [03600, 05004], lr: 0.017317, loss: 1.1877
2022-07-08 07:43:21 - train: epoch 0147, iter [03700, 05004], lr: 0.017305, loss: 1.1920
2022-07-08 07:43:55 - train: epoch 0147, iter [03800, 05004], lr: 0.017293, loss: 0.9254
2022-07-08 07:44:29 - train: epoch 0147, iter [03900, 05004], lr: 0.017281, loss: 1.3514
2022-07-08 07:45:03 - train: epoch 0147, iter [04000, 05004], lr: 0.017268, loss: 1.1208
2022-07-08 07:45:36 - train: epoch 0147, iter [04100, 05004], lr: 0.017256, loss: 1.0974
2022-07-08 07:46:10 - train: epoch 0147, iter [04200, 05004], lr: 0.017244, loss: 1.4109
2022-07-08 07:46:44 - train: epoch 0147, iter [04300, 05004], lr: 0.017232, loss: 1.1234
2022-07-08 07:47:18 - train: epoch 0147, iter [04400, 05004], lr: 0.017220, loss: 1.1220
2022-07-08 07:47:52 - train: epoch 0147, iter [04500, 05004], lr: 0.017208, loss: 1.2099
2022-07-08 07:48:27 - train: epoch 0147, iter [04600, 05004], lr: 0.017195, loss: 1.2063
2022-07-08 07:49:00 - train: epoch 0147, iter [04700, 05004], lr: 0.017183, loss: 1.2514
2022-07-08 07:49:34 - train: epoch 0147, iter [04800, 05004], lr: 0.017171, loss: 1.0893
2022-07-08 07:50:09 - train: epoch 0147, iter [04900, 05004], lr: 0.017159, loss: 0.9681
2022-07-08 07:50:41 - train: epoch 0147, iter [05000, 05004], lr: 0.017147, loss: 1.2212
2022-07-08 07:50:42 - train: epoch 147, train_loss: 1.1841
2022-07-08 07:51:55 - eval: epoch: 147, acc1: 71.114%, acc5: 90.528%, test_loss: 1.1568, per_image_load_time: 1.444ms, per_image_inference_time: 0.484ms
2022-07-08 07:51:56 - until epoch: 147, best_acc1: 71.284%
2022-07-08 07:51:56 - epoch 148 lr: 0.017146
2022-07-08 07:52:34 - train: epoch 0148, iter [00100, 05004], lr: 0.017134, loss: 1.1379
2022-07-08 07:53:08 - train: epoch 0148, iter [00200, 05004], lr: 0.017122, loss: 0.9920
2022-07-08 07:53:41 - train: epoch 0148, iter [00300, 05004], lr: 0.017110, loss: 1.1523
2022-07-08 07:54:14 - train: epoch 0148, iter [00400, 05004], lr: 0.017098, loss: 1.1476
2022-07-08 07:54:47 - train: epoch 0148, iter [00500, 05004], lr: 0.017086, loss: 1.1385
2022-07-08 07:55:20 - train: epoch 0148, iter [00600, 05004], lr: 0.017074, loss: 1.0958
2022-07-08 07:55:54 - train: epoch 0148, iter [00700, 05004], lr: 0.017062, loss: 0.9983
2022-07-08 07:56:28 - train: epoch 0148, iter [00800, 05004], lr: 0.017049, loss: 1.0916
2022-07-08 07:57:02 - train: epoch 0148, iter [00900, 05004], lr: 0.017037, loss: 1.0546
2022-07-08 07:57:35 - train: epoch 0148, iter [01000, 05004], lr: 0.017025, loss: 0.9994
2022-07-08 07:58:09 - train: epoch 0148, iter [01100, 05004], lr: 0.017013, loss: 0.9678
2022-07-08 07:58:42 - train: epoch 0148, iter [01200, 05004], lr: 0.017001, loss: 1.1887
2022-07-08 07:59:16 - train: epoch 0148, iter [01300, 05004], lr: 0.016989, loss: 1.0660
2022-07-08 07:59:51 - train: epoch 0148, iter [01400, 05004], lr: 0.016977, loss: 1.1177
2022-07-08 08:00:24 - train: epoch 0148, iter [01500, 05004], lr: 0.016965, loss: 1.0359
2022-07-08 08:00:58 - train: epoch 0148, iter [01600, 05004], lr: 0.016953, loss: 1.1172
2022-07-08 08:01:32 - train: epoch 0148, iter [01700, 05004], lr: 0.016941, loss: 1.3692
2022-07-08 08:02:06 - train: epoch 0148, iter [01800, 05004], lr: 0.016929, loss: 1.1742
2022-07-08 08:02:39 - train: epoch 0148, iter [01900, 05004], lr: 0.016916, loss: 1.1509
2022-07-08 08:03:14 - train: epoch 0148, iter [02000, 05004], lr: 0.016904, loss: 1.2275
2022-07-08 08:03:48 - train: epoch 0148, iter [02100, 05004], lr: 0.016892, loss: 1.2228
2022-07-08 08:04:21 - train: epoch 0148, iter [02200, 05004], lr: 0.016880, loss: 0.9819
2022-07-08 08:04:54 - train: epoch 0148, iter [02300, 05004], lr: 0.016868, loss: 1.1809
2022-07-08 08:05:29 - train: epoch 0148, iter [02400, 05004], lr: 0.016856, loss: 1.1395
2022-07-08 08:06:03 - train: epoch 0148, iter [02500, 05004], lr: 0.016844, loss: 1.2188
2022-07-08 08:06:36 - train: epoch 0148, iter [02600, 05004], lr: 0.016832, loss: 1.1709
2022-07-08 08:07:11 - train: epoch 0148, iter [02700, 05004], lr: 0.016820, loss: 1.1739
2022-07-08 08:07:44 - train: epoch 0148, iter [02800, 05004], lr: 0.016808, loss: 1.3219
2022-07-08 08:08:18 - train: epoch 0148, iter [02900, 05004], lr: 0.016796, loss: 1.2326
2022-07-08 08:08:52 - train: epoch 0148, iter [03000, 05004], lr: 0.016784, loss: 1.0720
2022-07-08 08:09:27 - train: epoch 0148, iter [03100, 05004], lr: 0.016772, loss: 1.1550
2022-07-08 08:10:00 - train: epoch 0148, iter [03200, 05004], lr: 0.016760, loss: 1.0857
2022-07-08 08:10:35 - train: epoch 0148, iter [03300, 05004], lr: 0.016748, loss: 1.0189
2022-07-08 08:11:09 - train: epoch 0148, iter [03400, 05004], lr: 0.016736, loss: 1.2095
2022-07-08 08:11:42 - train: epoch 0148, iter [03500, 05004], lr: 0.016724, loss: 1.1531
2022-07-08 08:12:16 - train: epoch 0148, iter [03600, 05004], lr: 0.016712, loss: 1.1201
2022-07-08 08:12:50 - train: epoch 0148, iter [03700, 05004], lr: 0.016700, loss: 1.2240
2022-07-08 08:13:24 - train: epoch 0148, iter [03800, 05004], lr: 0.016688, loss: 1.2133
2022-07-08 08:13:58 - train: epoch 0148, iter [03900, 05004], lr: 0.016676, loss: 1.0278
2022-07-08 08:14:31 - train: epoch 0148, iter [04000, 05004], lr: 0.016664, loss: 1.1685
2022-07-08 08:15:06 - train: epoch 0148, iter [04100, 05004], lr: 0.016652, loss: 1.2128
2022-07-08 08:15:39 - train: epoch 0148, iter [04200, 05004], lr: 0.016640, loss: 1.0420
2022-07-08 08:16:13 - train: epoch 0148, iter [04300, 05004], lr: 0.016628, loss: 1.2035
2022-07-08 08:16:47 - train: epoch 0148, iter [04400, 05004], lr: 0.016616, loss: 0.9906
2022-07-08 08:17:21 - train: epoch 0148, iter [04500, 05004], lr: 0.016604, loss: 1.1646
2022-07-08 08:17:54 - train: epoch 0148, iter [04600, 05004], lr: 0.016592, loss: 1.2001
2022-07-08 08:18:29 - train: epoch 0148, iter [04700, 05004], lr: 0.016580, loss: 1.0884
2022-07-08 08:19:02 - train: epoch 0148, iter [04800, 05004], lr: 0.016568, loss: 1.0400
2022-07-08 08:19:37 - train: epoch 0148, iter [04900, 05004], lr: 0.016556, loss: 1.3689
2022-07-08 08:20:10 - train: epoch 0148, iter [05000, 05004], lr: 0.016544, loss: 1.2441
2022-07-08 08:20:11 - train: epoch 148, train_loss: 1.1703
2022-07-08 08:21:25 - eval: epoch: 148, acc1: 70.378%, acc5: 90.130%, test_loss: 1.2006, per_image_load_time: 1.569ms, per_image_inference_time: 0.489ms
2022-07-08 08:21:25 - until epoch: 148, best_acc1: 71.284%
2022-07-08 08:21:25 - epoch 149 lr: 0.016543
2022-07-08 08:22:04 - train: epoch 0149, iter [00100, 05004], lr: 0.016532, loss: 1.1554
2022-07-08 08:22:38 - train: epoch 0149, iter [00200, 05004], lr: 0.016520, loss: 1.1023
2022-07-08 08:23:11 - train: epoch 0149, iter [00300, 05004], lr: 0.016508, loss: 1.1699
2022-07-08 08:23:45 - train: epoch 0149, iter [00400, 05004], lr: 0.016496, loss: 1.0881
2022-07-08 08:24:19 - train: epoch 0149, iter [00500, 05004], lr: 0.016484, loss: 1.0075
2022-07-08 08:24:53 - train: epoch 0149, iter [00600, 05004], lr: 0.016472, loss: 1.2190
2022-07-08 08:25:27 - train: epoch 0149, iter [00700, 05004], lr: 0.016460, loss: 1.1856
2022-07-08 08:26:01 - train: epoch 0149, iter [00800, 05004], lr: 0.016448, loss: 1.2146
2022-07-08 08:26:34 - train: epoch 0149, iter [00900, 05004], lr: 0.016436, loss: 1.1845
2022-07-08 08:27:08 - train: epoch 0149, iter [01000, 05004], lr: 0.016424, loss: 1.2395
2022-07-08 08:27:43 - train: epoch 0149, iter [01100, 05004], lr: 0.016412, loss: 1.2151
2022-07-08 08:28:16 - train: epoch 0149, iter [01200, 05004], lr: 0.016400, loss: 1.0454
2022-07-08 08:28:50 - train: epoch 0149, iter [01300, 05004], lr: 0.016388, loss: 1.2732
2022-07-08 08:29:25 - train: epoch 0149, iter [01400, 05004], lr: 0.016376, loss: 0.9696
2022-07-08 08:29:58 - train: epoch 0149, iter [01500, 05004], lr: 0.016364, loss: 1.0537
2022-07-08 08:30:31 - train: epoch 0149, iter [01600, 05004], lr: 0.016353, loss: 1.1350
2022-07-08 08:31:05 - train: epoch 0149, iter [01700, 05004], lr: 0.016341, loss: 1.0855
2022-07-08 08:31:39 - train: epoch 0149, iter [01800, 05004], lr: 0.016329, loss: 1.1457
2022-07-08 08:32:12 - train: epoch 0149, iter [01900, 05004], lr: 0.016317, loss: 1.0976
2022-07-08 08:32:46 - train: epoch 0149, iter [02000, 05004], lr: 0.016305, loss: 1.3131
2022-07-08 08:33:20 - train: epoch 0149, iter [02100, 05004], lr: 0.016293, loss: 1.2706
2022-07-08 08:33:54 - train: epoch 0149, iter [02200, 05004], lr: 0.016281, loss: 1.0765
2022-07-08 08:34:28 - train: epoch 0149, iter [02300, 05004], lr: 0.016269, loss: 1.0721
2022-07-08 08:35:02 - train: epoch 0149, iter [02400, 05004], lr: 0.016257, loss: 1.0921
2022-07-08 08:35:36 - train: epoch 0149, iter [02500, 05004], lr: 0.016245, loss: 1.0996
2022-07-08 08:36:10 - train: epoch 0149, iter [02600, 05004], lr: 0.016234, loss: 1.1341
2022-07-08 08:36:43 - train: epoch 0149, iter [02700, 05004], lr: 0.016222, loss: 1.3268
2022-07-08 08:37:17 - train: epoch 0149, iter [02800, 05004], lr: 0.016210, loss: 0.9688
2022-07-08 08:37:52 - train: epoch 0149, iter [02900, 05004], lr: 0.016198, loss: 1.2799
2022-07-08 08:38:26 - train: epoch 0149, iter [03000, 05004], lr: 0.016186, loss: 1.0922
2022-07-08 08:38:59 - train: epoch 0149, iter [03100, 05004], lr: 0.016174, loss: 1.1966
2022-07-08 08:39:34 - train: epoch 0149, iter [03200, 05004], lr: 0.016162, loss: 1.0429
2022-07-08 08:40:07 - train: epoch 0149, iter [03300, 05004], lr: 0.016151, loss: 1.2848
2022-07-08 08:40:41 - train: epoch 0149, iter [03400, 05004], lr: 0.016139, loss: 1.0841
2022-07-08 08:41:15 - train: epoch 0149, iter [03500, 05004], lr: 0.016127, loss: 1.2151
2022-07-08 08:41:49 - train: epoch 0149, iter [03600, 05004], lr: 0.016115, loss: 1.0002
2022-07-08 08:42:23 - train: epoch 0149, iter [03700, 05004], lr: 0.016103, loss: 1.0648
2022-07-08 08:42:58 - train: epoch 0149, iter [03800, 05004], lr: 0.016091, loss: 1.2789
2022-07-08 08:43:31 - train: epoch 0149, iter [03900, 05004], lr: 0.016080, loss: 1.4146
2022-07-08 08:44:05 - train: epoch 0149, iter [04000, 05004], lr: 0.016068, loss: 1.1907
2022-07-08 08:44:39 - train: epoch 0149, iter [04100, 05004], lr: 0.016056, loss: 1.1034
2022-07-08 08:45:13 - train: epoch 0149, iter [04200, 05004], lr: 0.016044, loss: 0.8972
2022-07-08 08:45:47 - train: epoch 0149, iter [04300, 05004], lr: 0.016032, loss: 1.1617
2022-07-08 08:46:21 - train: epoch 0149, iter [04400, 05004], lr: 0.016020, loss: 1.2557
2022-07-08 08:46:55 - train: epoch 0149, iter [04500, 05004], lr: 0.016009, loss: 1.0790
2022-07-08 08:47:30 - train: epoch 0149, iter [04600, 05004], lr: 0.015997, loss: 1.0734
2022-07-08 08:48:04 - train: epoch 0149, iter [04700, 05004], lr: 0.015985, loss: 1.1610
2022-07-08 08:48:38 - train: epoch 0149, iter [04800, 05004], lr: 0.015973, loss: 1.2232
2022-07-08 08:49:11 - train: epoch 0149, iter [04900, 05004], lr: 0.015961, loss: 1.1246
2022-07-08 08:49:44 - train: epoch 0149, iter [05000, 05004], lr: 0.015950, loss: 1.2780
2022-07-08 08:49:45 - train: epoch 149, train_loss: 1.1604
2022-07-08 08:51:00 - eval: epoch: 149, acc1: 71.744%, acc5: 90.802%, test_loss: 1.1418, per_image_load_time: 0.775ms, per_image_inference_time: 0.443ms
2022-07-08 08:51:00 - until epoch: 149, best_acc1: 71.744%
2022-07-08 08:51:00 - epoch 150 lr: 0.015949
2022-07-08 08:51:39 - train: epoch 0150, iter [00100, 05004], lr: 0.015937, loss: 1.0069
2022-07-08 08:52:13 - train: epoch 0150, iter [00200, 05004], lr: 0.015926, loss: 1.0450
2022-07-08 08:52:45 - train: epoch 0150, iter [00300, 05004], lr: 0.015914, loss: 1.0032
2022-07-08 08:53:19 - train: epoch 0150, iter [00400, 05004], lr: 0.015902, loss: 1.1626
2022-07-08 08:53:53 - train: epoch 0150, iter [00500, 05004], lr: 0.015890, loss: 1.1287
2022-07-08 08:54:26 - train: epoch 0150, iter [00600, 05004], lr: 0.015879, loss: 1.0575
2022-07-08 08:54:59 - train: epoch 0150, iter [00700, 05004], lr: 0.015867, loss: 1.1040
2022-07-08 08:55:33 - train: epoch 0150, iter [00800, 05004], lr: 0.015855, loss: 1.3659
2022-07-08 08:56:07 - train: epoch 0150, iter [00900, 05004], lr: 0.015843, loss: 1.2432
2022-07-08 08:56:41 - train: epoch 0150, iter [01000, 05004], lr: 0.015832, loss: 1.2162
2022-07-08 08:57:14 - train: epoch 0150, iter [01100, 05004], lr: 0.015820, loss: 1.3256
2022-07-08 08:57:48 - train: epoch 0150, iter [01200, 05004], lr: 0.015808, loss: 1.0463
2022-07-08 08:58:22 - train: epoch 0150, iter [01300, 05004], lr: 0.015796, loss: 1.2088
2022-07-08 08:58:55 - train: epoch 0150, iter [01400, 05004], lr: 0.015785, loss: 1.0342
2022-07-08 08:59:29 - train: epoch 0150, iter [01500, 05004], lr: 0.015773, loss: 1.2155
2022-07-08 09:00:04 - train: epoch 0150, iter [01600, 05004], lr: 0.015761, loss: 1.1077
2022-07-08 09:00:37 - train: epoch 0150, iter [01700, 05004], lr: 0.015749, loss: 1.2073
2022-07-08 09:01:11 - train: epoch 0150, iter [01800, 05004], lr: 0.015738, loss: 1.1869
2022-07-08 09:01:44 - train: epoch 0150, iter [01900, 05004], lr: 0.015726, loss: 1.0310
2022-07-08 09:02:19 - train: epoch 0150, iter [02000, 05004], lr: 0.015714, loss: 1.1195
2022-07-08 09:02:53 - train: epoch 0150, iter [02100, 05004], lr: 0.015702, loss: 1.0177
2022-07-08 09:03:27 - train: epoch 0150, iter [02200, 05004], lr: 0.015691, loss: 1.0954
2022-07-08 09:04:00 - train: epoch 0150, iter [02300, 05004], lr: 0.015679, loss: 1.2432
2022-07-08 09:04:34 - train: epoch 0150, iter [02400, 05004], lr: 0.015667, loss: 1.1240
2022-07-08 09:05:08 - train: epoch 0150, iter [02500, 05004], lr: 0.015656, loss: 1.0533
2022-07-08 09:05:42 - train: epoch 0150, iter [02600, 05004], lr: 0.015644, loss: 0.8692
2022-07-08 09:06:15 - train: epoch 0150, iter [02700, 05004], lr: 0.015632, loss: 1.0375
2022-07-08 09:06:49 - train: epoch 0150, iter [02800, 05004], lr: 0.015621, loss: 1.3163
2022-07-08 09:07:23 - train: epoch 0150, iter [02900, 05004], lr: 0.015609, loss: 1.0861
2022-07-08 09:07:57 - train: epoch 0150, iter [03000, 05004], lr: 0.015597, loss: 1.1631
2022-07-08 09:08:30 - train: epoch 0150, iter [03100, 05004], lr: 0.015585, loss: 1.0837
2022-07-08 09:09:05 - train: epoch 0150, iter [03200, 05004], lr: 0.015574, loss: 1.1242
2022-07-08 09:09:39 - train: epoch 0150, iter [03300, 05004], lr: 0.015562, loss: 1.1083
2022-07-08 09:10:14 - train: epoch 0150, iter [03400, 05004], lr: 0.015550, loss: 1.0578
2022-07-08 09:10:47 - train: epoch 0150, iter [03500, 05004], lr: 0.015539, loss: 1.0797
2022-07-08 09:11:22 - train: epoch 0150, iter [03600, 05004], lr: 0.015527, loss: 1.2677
2022-07-08 09:11:55 - train: epoch 0150, iter [03700, 05004], lr: 0.015515, loss: 1.3037
2022-07-08 09:12:30 - train: epoch 0150, iter [03800, 05004], lr: 0.015504, loss: 0.9986
2022-07-08 09:13:04 - train: epoch 0150, iter [03900, 05004], lr: 0.015492, loss: 1.1960
2022-07-08 09:13:38 - train: epoch 0150, iter [04000, 05004], lr: 0.015481, loss: 1.0346
2022-07-08 09:14:12 - train: epoch 0150, iter [04100, 05004], lr: 0.015469, loss: 1.1458
2022-07-08 09:14:46 - train: epoch 0150, iter [04200, 05004], lr: 0.015457, loss: 1.3215
2022-07-08 09:15:20 - train: epoch 0150, iter [04300, 05004], lr: 0.015446, loss: 1.2202
2022-07-08 09:15:54 - train: epoch 0150, iter [04400, 05004], lr: 0.015434, loss: 1.2222
2022-07-08 09:16:29 - train: epoch 0150, iter [04500, 05004], lr: 0.015422, loss: 1.1580
2022-07-08 09:17:03 - train: epoch 0150, iter [04600, 05004], lr: 0.015411, loss: 1.2000
2022-07-08 09:17:37 - train: epoch 0150, iter [04700, 05004], lr: 0.015399, loss: 1.1921
2022-07-08 09:18:11 - train: epoch 0150, iter [04800, 05004], lr: 0.015387, loss: 1.2451
2022-07-08 09:18:45 - train: epoch 0150, iter [04900, 05004], lr: 0.015376, loss: 1.1193
2022-07-08 09:19:18 - train: epoch 0150, iter [05000, 05004], lr: 0.015364, loss: 1.3334
2022-07-08 09:19:19 - train: epoch 150, train_loss: 1.1499
2022-07-08 09:20:32 - eval: epoch: 150, acc1: 71.764%, acc5: 90.974%, test_loss: 1.1340, per_image_load_time: 1.872ms, per_image_inference_time: 0.502ms
2022-07-08 09:20:33 - until epoch: 150, best_acc1: 71.764%
2022-07-08 09:20:33 - epoch 151 lr: 0.015364
2022-07-08 09:21:12 - train: epoch 0151, iter [00100, 05004], lr: 0.015352, loss: 1.1629
2022-07-08 09:21:46 - train: epoch 0151, iter [00200, 05004], lr: 0.015341, loss: 1.0805
2022-07-08 09:22:19 - train: epoch 0151, iter [00300, 05004], lr: 0.015329, loss: 1.1454
2022-07-08 09:22:54 - train: epoch 0151, iter [00400, 05004], lr: 0.015317, loss: 1.0098
2022-07-08 09:23:28 - train: epoch 0151, iter [00500, 05004], lr: 0.015306, loss: 1.0808
2022-07-08 09:24:01 - train: epoch 0151, iter [00600, 05004], lr: 0.015294, loss: 1.2002
2022-07-08 09:24:35 - train: epoch 0151, iter [00700, 05004], lr: 0.015283, loss: 1.1970
2022-07-08 09:25:09 - train: epoch 0151, iter [00800, 05004], lr: 0.015271, loss: 1.0615
2022-07-08 09:25:43 - train: epoch 0151, iter [00900, 05004], lr: 0.015259, loss: 1.1114
2022-07-08 09:26:17 - train: epoch 0151, iter [01000, 05004], lr: 0.015248, loss: 1.1159
2022-07-08 09:26:50 - train: epoch 0151, iter [01100, 05004], lr: 0.015236, loss: 1.0427
2022-07-08 09:27:24 - train: epoch 0151, iter [01200, 05004], lr: 0.015225, loss: 1.2103
2022-07-08 09:27:58 - train: epoch 0151, iter [01300, 05004], lr: 0.015213, loss: 0.9777
2022-07-08 09:28:32 - train: epoch 0151, iter [01400, 05004], lr: 0.015202, loss: 1.0067
2022-07-08 09:29:06 - train: epoch 0151, iter [01500, 05004], lr: 0.015190, loss: 0.9337
2022-07-08 09:29:39 - train: epoch 0151, iter [01600, 05004], lr: 0.015178, loss: 1.0374
2022-07-08 09:30:14 - train: epoch 0151, iter [01700, 05004], lr: 0.015167, loss: 1.0466
2022-07-08 09:30:48 - train: epoch 0151, iter [01800, 05004], lr: 0.015155, loss: 1.2216
2022-07-08 09:31:21 - train: epoch 0151, iter [01900, 05004], lr: 0.015144, loss: 1.1236
2022-07-08 09:31:55 - train: epoch 0151, iter [02000, 05004], lr: 0.015132, loss: 1.2815
2022-07-08 09:32:29 - train: epoch 0151, iter [02100, 05004], lr: 0.015121, loss: 0.8625
2022-07-08 09:33:03 - train: epoch 0151, iter [02200, 05004], lr: 0.015109, loss: 1.1866
2022-07-08 09:33:37 - train: epoch 0151, iter [02300, 05004], lr: 0.015098, loss: 1.1916
2022-07-08 09:34:11 - train: epoch 0151, iter [02400, 05004], lr: 0.015086, loss: 1.0863
2022-07-08 09:34:44 - train: epoch 0151, iter [02500, 05004], lr: 0.015075, loss: 1.0128
2022-07-08 09:35:18 - train: epoch 0151, iter [02600, 05004], lr: 0.015063, loss: 1.0968
2022-07-08 09:35:52 - train: epoch 0151, iter [02700, 05004], lr: 0.015052, loss: 1.1254
2022-07-08 09:36:26 - train: epoch 0151, iter [02800, 05004], lr: 0.015040, loss: 1.3124
2022-07-08 09:37:00 - train: epoch 0151, iter [02900, 05004], lr: 0.015029, loss: 1.2141
2022-07-08 09:37:33 - train: epoch 0151, iter [03000, 05004], lr: 0.015017, loss: 1.0732
2022-07-08 09:38:08 - train: epoch 0151, iter [03100, 05004], lr: 0.015006, loss: 1.0723
2022-07-08 09:38:41 - train: epoch 0151, iter [03200, 05004], lr: 0.014994, loss: 1.0590
2022-07-08 09:39:16 - train: epoch 0151, iter [03300, 05004], lr: 0.014983, loss: 0.9556
2022-07-08 09:39:49 - train: epoch 0151, iter [03400, 05004], lr: 0.014971, loss: 1.1129
2022-07-08 09:40:24 - train: epoch 0151, iter [03500, 05004], lr: 0.014960, loss: 1.2162
2022-07-08 09:40:57 - train: epoch 0151, iter [03600, 05004], lr: 0.014948, loss: 1.1584
2022-07-08 09:41:32 - train: epoch 0151, iter [03700, 05004], lr: 0.014937, loss: 1.0396
2022-07-08 09:42:05 - train: epoch 0151, iter [03800, 05004], lr: 0.014925, loss: 1.2159
2022-07-08 09:42:39 - train: epoch 0151, iter [03900, 05004], lr: 0.014914, loss: 1.2566
2022-07-08 09:43:14 - train: epoch 0151, iter [04000, 05004], lr: 0.014902, loss: 1.2542
2022-07-08 09:43:48 - train: epoch 0151, iter [04100, 05004], lr: 0.014891, loss: 1.2272
2022-07-08 09:44:22 - train: epoch 0151, iter [04200, 05004], lr: 0.014879, loss: 1.0397
2022-07-08 09:44:56 - train: epoch 0151, iter [04300, 05004], lr: 0.014868, loss: 1.0746
2022-07-08 09:45:30 - train: epoch 0151, iter [04400, 05004], lr: 0.014856, loss: 1.3000
2022-07-08 09:46:05 - train: epoch 0151, iter [04500, 05004], lr: 0.014845, loss: 1.1190
2022-07-08 09:46:39 - train: epoch 0151, iter [04600, 05004], lr: 0.014834, loss: 1.2305
2022-07-08 09:47:13 - train: epoch 0151, iter [04700, 05004], lr: 0.014822, loss: 0.9197
2022-07-08 09:47:48 - train: epoch 0151, iter [04800, 05004], lr: 0.014811, loss: 1.2485
2022-07-08 09:48:22 - train: epoch 0151, iter [04900, 05004], lr: 0.014799, loss: 1.2280
2022-07-08 09:48:55 - train: epoch 0151, iter [05000, 05004], lr: 0.014788, loss: 1.2349
2022-07-08 09:48:56 - train: epoch 151, train_loss: 1.1377
2022-07-08 09:50:11 - eval: epoch: 151, acc1: 70.884%, acc5: 90.246%, test_loss: 1.1874, per_image_load_time: 0.609ms, per_image_inference_time: 0.488ms
2022-07-08 09:50:11 - until epoch: 151, best_acc1: 71.764%
2022-07-08 09:50:11 - epoch 152 lr: 0.014787
2022-07-08 09:50:51 - train: epoch 0152, iter [00100, 05004], lr: 0.014776, loss: 1.1539
2022-07-08 09:51:24 - train: epoch 0152, iter [00200, 05004], lr: 0.014764, loss: 1.1105
2022-07-08 09:51:58 - train: epoch 0152, iter [00300, 05004], lr: 0.014753, loss: 1.1032
2022-07-08 09:52:32 - train: epoch 0152, iter [00400, 05004], lr: 0.014742, loss: 1.0243
2022-07-08 09:53:05 - train: epoch 0152, iter [00500, 05004], lr: 0.014730, loss: 0.9957
2022-07-08 09:53:39 - train: epoch 0152, iter [00600, 05004], lr: 0.014719, loss: 1.0376
2022-07-08 09:54:13 - train: epoch 0152, iter [00700, 05004], lr: 0.014707, loss: 1.0407
2022-07-08 09:54:47 - train: epoch 0152, iter [00800, 05004], lr: 0.014696, loss: 1.0230
2022-07-08 09:55:21 - train: epoch 0152, iter [00900, 05004], lr: 0.014685, loss: 1.2453
2022-07-08 09:55:54 - train: epoch 0152, iter [01000, 05004], lr: 0.014673, loss: 1.2976
2022-07-08 09:56:29 - train: epoch 0152, iter [01100, 05004], lr: 0.014662, loss: 1.2292
2022-07-08 09:57:02 - train: epoch 0152, iter [01200, 05004], lr: 0.014650, loss: 1.0436
2022-07-08 09:57:36 - train: epoch 0152, iter [01300, 05004], lr: 0.014639, loss: 1.0652
2022-07-08 09:58:10 - train: epoch 0152, iter [01400, 05004], lr: 0.014628, loss: 1.1554
2022-07-08 09:58:45 - train: epoch 0152, iter [01500, 05004], lr: 0.014616, loss: 1.0161
2022-07-08 09:59:18 - train: epoch 0152, iter [01600, 05004], lr: 0.014605, loss: 1.0891
2022-07-08 09:59:52 - train: epoch 0152, iter [01700, 05004], lr: 0.014594, loss: 1.2354
2022-07-08 10:00:26 - train: epoch 0152, iter [01800, 05004], lr: 0.014582, loss: 1.0777
2022-07-08 10:01:01 - train: epoch 0152, iter [01900, 05004], lr: 0.014571, loss: 0.9275
2022-07-08 10:01:34 - train: epoch 0152, iter [02000, 05004], lr: 0.014560, loss: 1.2176
2022-07-08 10:02:08 - train: epoch 0152, iter [02100, 05004], lr: 0.014548, loss: 0.9864
2022-07-08 10:02:42 - train: epoch 0152, iter [02200, 05004], lr: 0.014537, loss: 1.4367
2022-07-08 10:03:15 - train: epoch 0152, iter [02300, 05004], lr: 0.014525, loss: 1.2274
2022-07-08 10:03:50 - train: epoch 0152, iter [02400, 05004], lr: 0.014514, loss: 1.1137
2022-07-08 10:04:24 - train: epoch 0152, iter [02500, 05004], lr: 0.014503, loss: 1.2530
2022-07-08 10:04:57 - train: epoch 0152, iter [02600, 05004], lr: 0.014491, loss: 1.2106
2022-07-08 10:05:31 - train: epoch 0152, iter [02700, 05004], lr: 0.014480, loss: 1.1665
2022-07-08 10:06:06 - train: epoch 0152, iter [02800, 05004], lr: 0.014469, loss: 1.0141
2022-07-08 10:06:40 - train: epoch 0152, iter [02900, 05004], lr: 0.014457, loss: 1.1695
2022-07-08 10:07:13 - train: epoch 0152, iter [03000, 05004], lr: 0.014446, loss: 1.2076
2022-07-08 10:07:47 - train: epoch 0152, iter [03100, 05004], lr: 0.014435, loss: 1.0461
2022-07-08 10:08:21 - train: epoch 0152, iter [03200, 05004], lr: 0.014424, loss: 1.0340
2022-07-08 10:08:55 - train: epoch 0152, iter [03300, 05004], lr: 0.014412, loss: 1.2907
2022-07-08 10:09:30 - train: epoch 0152, iter [03400, 05004], lr: 0.014401, loss: 1.0104
2022-07-08 10:10:03 - train: epoch 0152, iter [03500, 05004], lr: 0.014390, loss: 1.0209
2022-07-08 10:10:37 - train: epoch 0152, iter [03600, 05004], lr: 0.014378, loss: 1.1396
2022-07-08 10:11:11 - train: epoch 0152, iter [03700, 05004], lr: 0.014367, loss: 1.0770
2022-07-08 10:11:45 - train: epoch 0152, iter [03800, 05004], lr: 0.014356, loss: 1.0419
2022-07-08 10:12:19 - train: epoch 0152, iter [03900, 05004], lr: 0.014344, loss: 1.1603
2022-07-08 10:12:52 - train: epoch 0152, iter [04000, 05004], lr: 0.014333, loss: 0.9859
2022-07-08 10:13:27 - train: epoch 0152, iter [04100, 05004], lr: 0.014322, loss: 1.1695
2022-07-08 10:14:01 - train: epoch 0152, iter [04200, 05004], lr: 0.014311, loss: 1.0186
2022-07-08 10:14:34 - train: epoch 0152, iter [04300, 05004], lr: 0.014299, loss: 1.0995
2022-07-08 10:15:09 - train: epoch 0152, iter [04400, 05004], lr: 0.014288, loss: 1.2595
2022-07-08 10:15:42 - train: epoch 0152, iter [04500, 05004], lr: 0.014277, loss: 1.1246
2022-07-08 10:16:16 - train: epoch 0152, iter [04600, 05004], lr: 0.014266, loss: 0.9922
2022-07-08 10:16:50 - train: epoch 0152, iter [04700, 05004], lr: 0.014254, loss: 1.0931
2022-07-08 10:17:24 - train: epoch 0152, iter [04800, 05004], lr: 0.014243, loss: 1.2884
2022-07-08 10:17:57 - train: epoch 0152, iter [04900, 05004], lr: 0.014232, loss: 1.2339
2022-07-08 10:18:29 - train: epoch 0152, iter [05000, 05004], lr: 0.014221, loss: 1.0576
2022-07-08 10:18:31 - train: epoch 152, train_loss: 1.1258
2022-07-08 10:19:45 - eval: epoch: 152, acc1: 71.166%, acc5: 90.346%, test_loss: 1.1666, per_image_load_time: 0.565ms, per_image_inference_time: 0.469ms
2022-07-08 10:19:45 - until epoch: 152, best_acc1: 71.764%
2022-07-08 10:19:45 - epoch 153 lr: 0.014220
2022-07-08 10:20:24 - train: epoch 0153, iter [00100, 05004], lr: 0.014209, loss: 1.1887
2022-07-08 10:20:58 - train: epoch 0153, iter [00200, 05004], lr: 0.014198, loss: 1.0645
2022-07-08 10:21:32 - train: epoch 0153, iter [00300, 05004], lr: 0.014186, loss: 1.1476
2022-07-08 10:22:05 - train: epoch 0153, iter [00400, 05004], lr: 0.014175, loss: 1.0407
2022-07-08 10:22:39 - train: epoch 0153, iter [00500, 05004], lr: 0.014164, loss: 1.2662
2022-07-08 10:23:13 - train: epoch 0153, iter [00600, 05004], lr: 0.014153, loss: 1.1858
2022-07-08 10:23:46 - train: epoch 0153, iter [00700, 05004], lr: 0.014141, loss: 1.1033
2022-07-08 10:24:20 - train: epoch 0153, iter [00800, 05004], lr: 0.014130, loss: 0.8939
2022-07-08 10:24:54 - train: epoch 0153, iter [00900, 05004], lr: 0.014119, loss: 1.1821
2022-07-08 10:25:27 - train: epoch 0153, iter [01000, 05004], lr: 0.014108, loss: 0.9806
2022-07-08 10:26:01 - train: epoch 0153, iter [01100, 05004], lr: 0.014097, loss: 1.0556
2022-07-08 10:26:35 - train: epoch 0153, iter [01200, 05004], lr: 0.014085, loss: 1.1858
2022-07-08 10:27:09 - train: epoch 0153, iter [01300, 05004], lr: 0.014074, loss: 1.1169
2022-07-08 10:27:42 - train: epoch 0153, iter [01400, 05004], lr: 0.014063, loss: 1.1558
2022-07-08 10:28:16 - train: epoch 0153, iter [01500, 05004], lr: 0.014052, loss: 1.0512
2022-07-08 10:28:50 - train: epoch 0153, iter [01600, 05004], lr: 0.014041, loss: 1.1126
2022-07-08 10:29:23 - train: epoch 0153, iter [01700, 05004], lr: 0.014029, loss: 1.1202
2022-07-08 10:29:57 - train: epoch 0153, iter [01800, 05004], lr: 0.014018, loss: 1.0204
2022-07-08 10:30:32 - train: epoch 0153, iter [01900, 05004], lr: 0.014007, loss: 1.0313
2022-07-08 10:31:06 - train: epoch 0153, iter [02000, 05004], lr: 0.013996, loss: 1.0233
2022-07-08 10:31:39 - train: epoch 0153, iter [02100, 05004], lr: 0.013985, loss: 1.1716
2022-07-08 10:32:13 - train: epoch 0153, iter [02200, 05004], lr: 0.013974, loss: 1.0193
2022-07-08 10:32:47 - train: epoch 0153, iter [02300, 05004], lr: 0.013962, loss: 1.1464
2022-07-08 10:33:21 - train: epoch 0153, iter [02400, 05004], lr: 0.013951, loss: 1.2279
2022-07-08 10:33:55 - train: epoch 0153, iter [02500, 05004], lr: 0.013940, loss: 1.1495
2022-07-08 10:34:29 - train: epoch 0153, iter [02600, 05004], lr: 0.013929, loss: 0.8761
2022-07-08 10:35:03 - train: epoch 0153, iter [02700, 05004], lr: 0.013918, loss: 1.0753
2022-07-08 10:35:36 - train: epoch 0153, iter [02800, 05004], lr: 0.013907, loss: 1.0117
2022-07-08 10:36:10 - train: epoch 0153, iter [02900, 05004], lr: 0.013896, loss: 1.0614
2022-07-08 10:36:45 - train: epoch 0153, iter [03000, 05004], lr: 0.013884, loss: 1.2107
2022-07-08 10:37:19 - train: epoch 0153, iter [03100, 05004], lr: 0.013873, loss: 1.0870
2022-07-08 10:37:52 - train: epoch 0153, iter [03200, 05004], lr: 0.013862, loss: 1.2592
2022-07-08 10:38:26 - train: epoch 0153, iter [03300, 05004], lr: 0.013851, loss: 1.1807
2022-07-08 10:38:59 - train: epoch 0153, iter [03400, 05004], lr: 0.013840, loss: 1.0384
2022-07-08 10:39:33 - train: epoch 0153, iter [03500, 05004], lr: 0.013829, loss: 1.0908
2022-07-08 10:40:07 - train: epoch 0153, iter [03600, 05004], lr: 0.013818, loss: 1.1950
2022-07-08 10:40:41 - train: epoch 0153, iter [03700, 05004], lr: 0.013807, loss: 1.2350
2022-07-08 10:41:15 - train: epoch 0153, iter [03800, 05004], lr: 0.013795, loss: 1.1923
2022-07-08 10:41:49 - train: epoch 0153, iter [03900, 05004], lr: 0.013784, loss: 1.0315
2022-07-08 10:42:22 - train: epoch 0153, iter [04000, 05004], lr: 0.013773, loss: 1.3312
2022-07-08 10:42:57 - train: epoch 0153, iter [04100, 05004], lr: 0.013762, loss: 1.0973
2022-07-08 10:43:30 - train: epoch 0153, iter [04200, 05004], lr: 0.013751, loss: 1.3521
2022-07-08 10:44:04 - train: epoch 0153, iter [04300, 05004], lr: 0.013740, loss: 1.1088
2022-07-08 10:44:38 - train: epoch 0153, iter [04400, 05004], lr: 0.013729, loss: 1.0507
2022-07-08 10:45:12 - train: epoch 0153, iter [04500, 05004], lr: 0.013718, loss: 1.0839
2022-07-08 10:45:46 - train: epoch 0153, iter [04600, 05004], lr: 0.013707, loss: 1.0049
2022-07-08 10:46:20 - train: epoch 0153, iter [04700, 05004], lr: 0.013696, loss: 1.0995
2022-07-08 10:46:55 - train: epoch 0153, iter [04800, 05004], lr: 0.013685, loss: 1.1676
2022-07-08 10:47:28 - train: epoch 0153, iter [04900, 05004], lr: 0.013674, loss: 0.9918
2022-07-08 10:48:00 - train: epoch 0153, iter [05000, 05004], lr: 0.013662, loss: 0.9807
2022-07-08 10:48:01 - train: epoch 153, train_loss: 1.1135
2022-07-08 10:49:16 - eval: epoch: 153, acc1: 71.476%, acc5: 90.534%, test_loss: 1.1547, per_image_load_time: 0.883ms, per_image_inference_time: 0.477ms
2022-07-08 10:49:16 - until epoch: 153, best_acc1: 71.764%
2022-07-08 10:49:16 - epoch 154 lr: 0.013662
2022-07-08 10:49:56 - train: epoch 0154, iter [00100, 05004], lr: 0.013651, loss: 1.1017
2022-07-08 10:50:29 - train: epoch 0154, iter [00200, 05004], lr: 0.013640, loss: 1.0591
2022-07-08 10:51:02 - train: epoch 0154, iter [00300, 05004], lr: 0.013629, loss: 1.1320
2022-07-08 10:51:36 - train: epoch 0154, iter [00400, 05004], lr: 0.013618, loss: 0.9986
2022-07-08 10:52:10 - train: epoch 0154, iter [00500, 05004], lr: 0.013607, loss: 1.1190
2022-07-08 10:52:44 - train: epoch 0154, iter [00600, 05004], lr: 0.013596, loss: 1.1797
2022-07-08 10:53:17 - train: epoch 0154, iter [00700, 05004], lr: 0.013585, loss: 1.0202
2022-07-08 10:53:51 - train: epoch 0154, iter [00800, 05004], lr: 0.013574, loss: 0.8551
2022-07-08 10:54:24 - train: epoch 0154, iter [00900, 05004], lr: 0.013563, loss: 1.1424
2022-07-08 10:54:57 - train: epoch 0154, iter [01000, 05004], lr: 0.013552, loss: 1.0065
2022-07-08 10:55:30 - train: epoch 0154, iter [01100, 05004], lr: 0.013541, loss: 1.0964
2022-07-08 10:56:03 - train: epoch 0154, iter [01200, 05004], lr: 0.013530, loss: 1.1295
2022-07-08 10:56:36 - train: epoch 0154, iter [01300, 05004], lr: 0.013519, loss: 0.9458
2022-07-08 10:57:10 - train: epoch 0154, iter [01400, 05004], lr: 0.013508, loss: 1.1431
2022-07-08 10:57:44 - train: epoch 0154, iter [01500, 05004], lr: 0.013497, loss: 1.0435
2022-07-08 10:58:18 - train: epoch 0154, iter [01600, 05004], lr: 0.013486, loss: 1.0835
2022-07-08 10:58:51 - train: epoch 0154, iter [01700, 05004], lr: 0.013475, loss: 1.1001
2022-07-08 10:59:25 - train: epoch 0154, iter [01800, 05004], lr: 0.013464, loss: 0.9430
2022-07-08 10:59:59 - train: epoch 0154, iter [01900, 05004], lr: 0.013453, loss: 1.0660
2022-07-08 11:00:32 - train: epoch 0154, iter [02000, 05004], lr: 0.013442, loss: 0.9425
2022-07-08 11:01:07 - train: epoch 0154, iter [02100, 05004], lr: 0.013431, loss: 1.0564
2022-07-08 11:01:40 - train: epoch 0154, iter [02200, 05004], lr: 0.013420, loss: 1.0280
2022-07-08 11:02:14 - train: epoch 0154, iter [02300, 05004], lr: 0.013409, loss: 1.1201
2022-07-08 11:02:47 - train: epoch 0154, iter [02400, 05004], lr: 0.013398, loss: 1.2085
2022-07-08 11:03:22 - train: epoch 0154, iter [02500, 05004], lr: 0.013387, loss: 1.0810
2022-07-08 11:03:55 - train: epoch 0154, iter [02600, 05004], lr: 0.013376, loss: 1.0506
2022-07-08 11:04:29 - train: epoch 0154, iter [02700, 05004], lr: 0.013365, loss: 1.0274
2022-07-08 11:05:04 - train: epoch 0154, iter [02800, 05004], lr: 0.013354, loss: 1.2477
2022-07-08 11:05:37 - train: epoch 0154, iter [02900, 05004], lr: 0.013343, loss: 1.1849
2022-07-08 11:06:11 - train: epoch 0154, iter [03000, 05004], lr: 0.013332, loss: 1.0800
2022-07-08 11:06:45 - train: epoch 0154, iter [03100, 05004], lr: 0.013321, loss: 1.0685
2022-07-08 11:07:19 - train: epoch 0154, iter [03200, 05004], lr: 0.013310, loss: 1.0895
2022-07-08 11:07:53 - train: epoch 0154, iter [03300, 05004], lr: 0.013299, loss: 1.0587
2022-07-08 11:08:27 - train: epoch 0154, iter [03400, 05004], lr: 0.013288, loss: 0.9871
2022-07-08 11:09:01 - train: epoch 0154, iter [03500, 05004], lr: 0.013277, loss: 0.9990
2022-07-08 11:09:35 - train: epoch 0154, iter [03600, 05004], lr: 0.013266, loss: 1.0674
2022-07-08 11:10:08 - train: epoch 0154, iter [03700, 05004], lr: 0.013256, loss: 1.0720
2022-07-08 11:10:42 - train: epoch 0154, iter [03800, 05004], lr: 0.013245, loss: 1.0837
2022-07-08 11:11:17 - train: epoch 0154, iter [03900, 05004], lr: 0.013234, loss: 1.0847
2022-07-08 11:11:51 - train: epoch 0154, iter [04000, 05004], lr: 0.013223, loss: 1.0753
2022-07-08 11:12:25 - train: epoch 0154, iter [04100, 05004], lr: 0.013212, loss: 1.0092
2022-07-08 11:12:58 - train: epoch 0154, iter [04200, 05004], lr: 0.013201, loss: 1.0681
2022-07-08 11:13:32 - train: epoch 0154, iter [04300, 05004], lr: 0.013190, loss: 1.2466
2022-07-08 11:14:07 - train: epoch 0154, iter [04400, 05004], lr: 0.013179, loss: 0.9394
2022-07-08 11:14:40 - train: epoch 0154, iter [04500, 05004], lr: 0.013168, loss: 1.0346
2022-07-08 11:15:14 - train: epoch 0154, iter [04600, 05004], lr: 0.013157, loss: 1.0297
2022-07-08 11:15:48 - train: epoch 0154, iter [04700, 05004], lr: 0.013147, loss: 1.1423
2022-07-08 11:16:22 - train: epoch 0154, iter [04800, 05004], lr: 0.013136, loss: 1.5202
2022-07-08 11:16:57 - train: epoch 0154, iter [04900, 05004], lr: 0.013125, loss: 1.4737
2022-07-08 11:17:29 - train: epoch 0154, iter [05000, 05004], lr: 0.013114, loss: 0.9328
2022-07-08 11:17:30 - train: epoch 154, train_loss: 1.1004
2022-07-08 11:18:45 - eval: epoch: 154, acc1: 72.852%, acc5: 91.262%, test_loss: 1.1043, per_image_load_time: 0.878ms, per_image_inference_time: 0.474ms
2022-07-08 11:18:45 - until epoch: 154, best_acc1: 72.852%
2022-07-08 11:18:45 - epoch 155 lr: 0.013113
2022-07-08 11:19:24 - train: epoch 0155, iter [00100, 05004], lr: 0.013103, loss: 1.1014
2022-07-08 11:19:58 - train: epoch 0155, iter [00200, 05004], lr: 0.013092, loss: 1.1435
2022-07-08 11:20:30 - train: epoch 0155, iter [00300, 05004], lr: 0.013081, loss: 1.0596
2022-07-08 11:21:04 - train: epoch 0155, iter [00400, 05004], lr: 0.013070, loss: 0.9743
2022-07-08 11:21:38 - train: epoch 0155, iter [00500, 05004], lr: 0.013059, loss: 1.2080
2022-07-08 11:22:11 - train: epoch 0155, iter [00600, 05004], lr: 0.013048, loss: 0.9533
2022-07-08 11:22:45 - train: epoch 0155, iter [00700, 05004], lr: 0.013037, loss: 0.9269
2022-07-08 11:23:18 - train: epoch 0155, iter [00800, 05004], lr: 0.013027, loss: 0.8586
2022-07-08 11:23:53 - train: epoch 0155, iter [00900, 05004], lr: 0.013016, loss: 1.0969
2022-07-08 11:24:26 - train: epoch 0155, iter [01000, 05004], lr: 0.013005, loss: 1.0840
2022-07-08 11:24:59 - train: epoch 0155, iter [01100, 05004], lr: 0.012994, loss: 1.1394
2022-07-08 11:25:33 - train: epoch 0155, iter [01200, 05004], lr: 0.012983, loss: 1.1465
2022-07-08 11:26:07 - train: epoch 0155, iter [01300, 05004], lr: 0.012973, loss: 1.0005
2022-07-08 11:26:41 - train: epoch 0155, iter [01400, 05004], lr: 0.012962, loss: 1.1814
2022-07-08 11:27:15 - train: epoch 0155, iter [01500, 05004], lr: 0.012951, loss: 1.0186
2022-07-08 11:27:49 - train: epoch 0155, iter [01600, 05004], lr: 0.012940, loss: 1.1560
2022-07-08 11:28:24 - train: epoch 0155, iter [01700, 05004], lr: 0.012929, loss: 1.0833
2022-07-08 11:28:57 - train: epoch 0155, iter [01800, 05004], lr: 0.012918, loss: 1.0890
2022-07-08 11:29:31 - train: epoch 0155, iter [01900, 05004], lr: 0.012908, loss: 0.9836
2022-07-08 11:30:06 - train: epoch 0155, iter [02000, 05004], lr: 0.012897, loss: 1.1431
2022-07-08 11:30:40 - train: epoch 0155, iter [02100, 05004], lr: 0.012886, loss: 0.9970
2022-07-08 11:31:13 - train: epoch 0155, iter [02200, 05004], lr: 0.012875, loss: 0.9873
2022-07-08 11:31:48 - train: epoch 0155, iter [02300, 05004], lr: 0.012865, loss: 0.9923
2022-07-08 11:32:22 - train: epoch 0155, iter [02400, 05004], lr: 0.012854, loss: 1.2577
2022-07-08 11:32:56 - train: epoch 0155, iter [02500, 05004], lr: 0.012843, loss: 1.0630
2022-07-08 11:33:30 - train: epoch 0155, iter [02600, 05004], lr: 0.012832, loss: 1.0405
2022-07-08 11:34:04 - train: epoch 0155, iter [02700, 05004], lr: 0.012821, loss: 1.1306
2022-07-08 11:34:38 - train: epoch 0155, iter [02800, 05004], lr: 0.012811, loss: 1.0827
2022-07-08 11:35:13 - train: epoch 0155, iter [02900, 05004], lr: 0.012800, loss: 1.0035
2022-07-08 11:35:47 - train: epoch 0155, iter [03000, 05004], lr: 0.012789, loss: 1.0801
2022-07-08 11:36:21 - train: epoch 0155, iter [03100, 05004], lr: 0.012778, loss: 1.0814
2022-07-08 11:36:55 - train: epoch 0155, iter [03200, 05004], lr: 0.012768, loss: 1.1004
2022-07-08 11:37:29 - train: epoch 0155, iter [03300, 05004], lr: 0.012757, loss: 1.1315
2022-07-08 11:38:03 - train: epoch 0155, iter [03400, 05004], lr: 0.012746, loss: 1.1206
2022-07-08 11:38:38 - train: epoch 0155, iter [03500, 05004], lr: 0.012735, loss: 1.1136
2022-07-08 11:39:11 - train: epoch 0155, iter [03600, 05004], lr: 0.012725, loss: 1.2046
2022-07-08 11:39:45 - train: epoch 0155, iter [03700, 05004], lr: 0.012714, loss: 0.9915
2022-07-08 11:40:19 - train: epoch 0155, iter [03800, 05004], lr: 0.012703, loss: 1.4047
2022-07-08 11:40:54 - train: epoch 0155, iter [03900, 05004], lr: 0.012693, loss: 1.0121
2022-07-08 11:41:28 - train: epoch 0155, iter [04000, 05004], lr: 0.012682, loss: 1.0720
2022-07-08 11:42:03 - train: epoch 0155, iter [04100, 05004], lr: 0.012671, loss: 1.1682
2022-07-08 11:42:36 - train: epoch 0155, iter [04200, 05004], lr: 0.012660, loss: 0.9898
2022-07-08 11:43:09 - train: epoch 0155, iter [04300, 05004], lr: 0.012650, loss: 1.2271
2022-07-08 11:43:43 - train: epoch 0155, iter [04400, 05004], lr: 0.012639, loss: 1.3873
2022-07-08 11:44:18 - train: epoch 0155, iter [04500, 05004], lr: 0.012628, loss: 0.9875
2022-07-08 11:44:53 - train: epoch 0155, iter [04600, 05004], lr: 0.012618, loss: 1.0042
2022-07-08 11:45:26 - train: epoch 0155, iter [04700, 05004], lr: 0.012607, loss: 1.1107
2022-07-08 11:46:00 - train: epoch 0155, iter [04800, 05004], lr: 0.012596, loss: 1.0536
2022-07-08 11:46:34 - train: epoch 0155, iter [04900, 05004], lr: 0.012586, loss: 1.1543
2022-07-08 11:47:07 - train: epoch 0155, iter [05000, 05004], lr: 0.012575, loss: 1.0456
2022-07-08 11:47:08 - train: epoch 155, train_loss: 1.0864
2022-07-08 11:48:23 - eval: epoch: 155, acc1: 72.624%, acc5: 91.030%, test_loss: 1.1089, per_image_load_time: 2.230ms, per_image_inference_time: 0.475ms
2022-07-08 11:48:23 - until epoch: 155, best_acc1: 72.852%
2022-07-08 11:48:23 - epoch 156 lr: 0.012574
2022-07-08 11:49:02 - train: epoch 0156, iter [00100, 05004], lr: 0.012564, loss: 1.1446
2022-07-08 11:49:36 - train: epoch 0156, iter [00200, 05004], lr: 0.012553, loss: 1.0207
2022-07-08 11:50:10 - train: epoch 0156, iter [00300, 05004], lr: 0.012542, loss: 0.9077
2022-07-08 11:50:43 - train: epoch 0156, iter [00400, 05004], lr: 0.012532, loss: 0.9422
2022-07-08 11:51:17 - train: epoch 0156, iter [00500, 05004], lr: 0.012521, loss: 0.9474
2022-07-08 11:51:51 - train: epoch 0156, iter [00600, 05004], lr: 0.012510, loss: 0.9405
2022-07-08 11:52:24 - train: epoch 0156, iter [00700, 05004], lr: 0.012500, loss: 1.0058
2022-07-08 11:52:58 - train: epoch 0156, iter [00800, 05004], lr: 0.012489, loss: 1.2634
2022-07-08 11:53:31 - train: epoch 0156, iter [00900, 05004], lr: 0.012479, loss: 1.1791
2022-07-08 11:54:05 - train: epoch 0156, iter [01000, 05004], lr: 0.012468, loss: 1.1564
2022-07-08 11:54:38 - train: epoch 0156, iter [01100, 05004], lr: 0.012457, loss: 1.0772
2022-07-08 11:55:12 - train: epoch 0156, iter [01200, 05004], lr: 0.012447, loss: 0.8274
2022-07-08 11:55:45 - train: epoch 0156, iter [01300, 05004], lr: 0.012436, loss: 1.0650
2022-07-08 11:56:19 - train: epoch 0156, iter [01400, 05004], lr: 0.012425, loss: 1.2996
2022-07-08 11:56:53 - train: epoch 0156, iter [01500, 05004], lr: 0.012415, loss: 1.1049
2022-07-08 11:57:27 - train: epoch 0156, iter [01600, 05004], lr: 0.012404, loss: 0.9332
2022-07-08 11:58:01 - train: epoch 0156, iter [01700, 05004], lr: 0.012394, loss: 1.1211
2022-07-08 11:58:34 - train: epoch 0156, iter [01800, 05004], lr: 0.012383, loss: 1.0191
2022-07-08 11:59:08 - train: epoch 0156, iter [01900, 05004], lr: 0.012372, loss: 0.9091
2022-07-08 11:59:42 - train: epoch 0156, iter [02000, 05004], lr: 0.012362, loss: 0.8630
2022-07-08 12:00:16 - train: epoch 0156, iter [02100, 05004], lr: 0.012351, loss: 1.2450
2022-07-08 12:00:51 - train: epoch 0156, iter [02200, 05004], lr: 0.012341, loss: 1.0890
2022-07-08 12:01:24 - train: epoch 0156, iter [02300, 05004], lr: 0.012330, loss: 1.0764
2022-07-08 12:01:58 - train: epoch 0156, iter [02400, 05004], lr: 0.012319, loss: 1.0584
2022-07-08 12:02:32 - train: epoch 0156, iter [02500, 05004], lr: 0.012309, loss: 0.9452
2022-07-08 12:03:06 - train: epoch 0156, iter [02600, 05004], lr: 0.012298, loss: 1.1752
2022-07-08 12:03:40 - train: epoch 0156, iter [02700, 05004], lr: 0.012288, loss: 0.9839
2022-07-08 12:04:15 - train: epoch 0156, iter [02800, 05004], lr: 0.012277, loss: 1.1438
2022-07-08 12:04:48 - train: epoch 0156, iter [02900, 05004], lr: 0.012267, loss: 1.0340
2022-07-08 12:05:22 - train: epoch 0156, iter [03000, 05004], lr: 0.012256, loss: 1.2076
2022-07-08 12:05:56 - train: epoch 0156, iter [03100, 05004], lr: 0.012245, loss: 1.0691
2022-07-08 12:06:30 - train: epoch 0156, iter [03200, 05004], lr: 0.012235, loss: 0.9043
2022-07-08 12:07:04 - train: epoch 0156, iter [03300, 05004], lr: 0.012224, loss: 1.0005
2022-07-08 12:07:39 - train: epoch 0156, iter [03400, 05004], lr: 0.012214, loss: 0.9807
2022-07-08 12:08:13 - train: epoch 0156, iter [03500, 05004], lr: 0.012203, loss: 1.0798
2022-07-08 12:08:47 - train: epoch 0156, iter [03600, 05004], lr: 0.012193, loss: 1.4053
2022-07-08 12:09:21 - train: epoch 0156, iter [03700, 05004], lr: 0.012182, loss: 1.1039
2022-07-08 12:09:54 - train: epoch 0156, iter [03800, 05004], lr: 0.012172, loss: 1.2581
2022-07-08 12:10:30 - train: epoch 0156, iter [03900, 05004], lr: 0.012161, loss: 1.1249
2022-07-08 12:11:03 - train: epoch 0156, iter [04000, 05004], lr: 0.012151, loss: 1.0966
2022-07-08 12:11:36 - train: epoch 0156, iter [04100, 05004], lr: 0.012140, loss: 1.4272
2022-07-08 12:12:10 - train: epoch 0156, iter [04200, 05004], lr: 0.012130, loss: 1.0827
2022-07-08 12:12:45 - train: epoch 0156, iter [04300, 05004], lr: 0.012119, loss: 1.0880
2022-07-08 12:13:19 - train: epoch 0156, iter [04400, 05004], lr: 0.012109, loss: 0.9976
2022-07-08 12:13:53 - train: epoch 0156, iter [04500, 05004], lr: 0.012098, loss: 1.0519
2022-07-08 12:14:28 - train: epoch 0156, iter [04600, 05004], lr: 0.012088, loss: 1.0556
2022-07-08 12:15:01 - train: epoch 0156, iter [04700, 05004], lr: 0.012077, loss: 1.0782
2022-07-08 12:15:36 - train: epoch 0156, iter [04800, 05004], lr: 0.012067, loss: 1.0837
2022-07-08 12:16:10 - train: epoch 0156, iter [04900, 05004], lr: 0.012056, loss: 1.2629
2022-07-08 12:16:42 - train: epoch 0156, iter [05000, 05004], lr: 0.012046, loss: 1.0484
2022-07-08 12:16:43 - train: epoch 156, train_loss: 1.0728
2022-07-08 12:17:58 - eval: epoch: 156, acc1: 71.618%, acc5: 90.802%, test_loss: 1.1460, per_image_load_time: 0.696ms, per_image_inference_time: 0.489ms
2022-07-08 12:17:58 - until epoch: 156, best_acc1: 72.852%
2022-07-08 12:17:58 - epoch 157 lr: 0.012045
2022-07-08 12:18:37 - train: epoch 0157, iter [00100, 05004], lr: 0.012035, loss: 1.0135
2022-07-08 12:19:11 - train: epoch 0157, iter [00200, 05004], lr: 0.012024, loss: 0.9269
2022-07-08 12:19:46 - train: epoch 0157, iter [00300, 05004], lr: 0.012014, loss: 1.0165
2022-07-08 12:20:19 - train: epoch 0157, iter [00400, 05004], lr: 0.012003, loss: 0.9334
2022-07-08 12:20:53 - train: epoch 0157, iter [00500, 05004], lr: 0.011993, loss: 0.9336
2022-07-08 12:21:27 - train: epoch 0157, iter [00600, 05004], lr: 0.011982, loss: 1.0729
2022-07-08 12:22:01 - train: epoch 0157, iter [00700, 05004], lr: 0.011972, loss: 1.0677
2022-07-08 12:22:35 - train: epoch 0157, iter [00800, 05004], lr: 0.011961, loss: 1.0274
2022-07-08 12:23:09 - train: epoch 0157, iter [00900, 05004], lr: 0.011951, loss: 1.0379
2022-07-08 12:23:42 - train: epoch 0157, iter [01000, 05004], lr: 0.011941, loss: 1.0601
2022-07-08 12:24:16 - train: epoch 0157, iter [01100, 05004], lr: 0.011930, loss: 0.8829
2022-07-08 12:24:50 - train: epoch 0157, iter [01200, 05004], lr: 0.011920, loss: 1.0091
2022-07-08 12:25:24 - train: epoch 0157, iter [01300, 05004], lr: 0.011909, loss: 1.0432
2022-07-08 12:25:57 - train: epoch 0157, iter [01400, 05004], lr: 0.011899, loss: 1.1377
2022-07-08 12:26:31 - train: epoch 0157, iter [01500, 05004], lr: 0.011888, loss: 1.1322
2022-07-08 12:27:04 - train: epoch 0157, iter [01600, 05004], lr: 0.011878, loss: 0.8782
2022-07-08 12:27:38 - train: epoch 0157, iter [01700, 05004], lr: 0.011868, loss: 0.8616
2022-07-08 12:28:11 - train: epoch 0157, iter [01800, 05004], lr: 0.011857, loss: 0.9910
2022-07-08 12:28:46 - train: epoch 0157, iter [01900, 05004], lr: 0.011847, loss: 1.2280
2022-07-08 12:29:20 - train: epoch 0157, iter [02000, 05004], lr: 0.011836, loss: 1.1884
2022-07-08 12:29:54 - train: epoch 0157, iter [02100, 05004], lr: 0.011826, loss: 1.1341
2022-07-08 12:30:28 - train: epoch 0157, iter [02200, 05004], lr: 0.011816, loss: 0.9974
2022-07-08 12:31:01 - train: epoch 0157, iter [02300, 05004], lr: 0.011805, loss: 0.9743
2022-07-08 12:31:35 - train: epoch 0157, iter [02400, 05004], lr: 0.011795, loss: 0.9507
2022-07-08 12:32:09 - train: epoch 0157, iter [02500, 05004], lr: 0.011784, loss: 0.9858
2022-07-08 12:32:42 - train: epoch 0157, iter [02600, 05004], lr: 0.011774, loss: 1.0136
2022-07-08 12:33:16 - train: epoch 0157, iter [02700, 05004], lr: 0.011764, loss: 1.1638
2022-07-08 12:33:50 - train: epoch 0157, iter [02800, 05004], lr: 0.011753, loss: 1.0967
2022-07-08 12:34:24 - train: epoch 0157, iter [02900, 05004], lr: 0.011743, loss: 0.9155
2022-07-08 12:34:57 - train: epoch 0157, iter [03000, 05004], lr: 0.011733, loss: 1.3709
2022-07-08 12:35:32 - train: epoch 0157, iter [03100, 05004], lr: 0.011722, loss: 1.0070
2022-07-08 12:36:05 - train: epoch 0157, iter [03200, 05004], lr: 0.011712, loss: 0.9043
2022-07-08 12:36:38 - train: epoch 0157, iter [03300, 05004], lr: 0.011702, loss: 1.1844
2022-07-08 12:37:12 - train: epoch 0157, iter [03400, 05004], lr: 0.011691, loss: 0.8818
2022-07-08 12:37:46 - train: epoch 0157, iter [03500, 05004], lr: 0.011681, loss: 1.2399
2022-07-08 12:38:21 - train: epoch 0157, iter [03600, 05004], lr: 0.011670, loss: 1.0684
2022-07-08 12:38:54 - train: epoch 0157, iter [03700, 05004], lr: 0.011660, loss: 0.8834
2022-07-08 12:39:28 - train: epoch 0157, iter [03800, 05004], lr: 0.011650, loss: 1.0716
2022-07-08 12:40:02 - train: epoch 0157, iter [03900, 05004], lr: 0.011639, loss: 1.1465
2022-07-08 12:40:36 - train: epoch 0157, iter [04000, 05004], lr: 0.011629, loss: 0.9602
2022-07-08 12:41:09 - train: epoch 0157, iter [04100, 05004], lr: 0.011619, loss: 0.9193
2022-07-08 12:41:44 - train: epoch 0157, iter [04200, 05004], lr: 0.011609, loss: 1.0693
2022-07-08 12:42:18 - train: epoch 0157, iter [04300, 05004], lr: 0.011598, loss: 1.1983
2022-07-08 12:42:51 - train: epoch 0157, iter [04400, 05004], lr: 0.011588, loss: 1.0966
2022-07-08 12:43:25 - train: epoch 0157, iter [04500, 05004], lr: 0.011578, loss: 1.1929
2022-07-08 12:43:59 - train: epoch 0157, iter [04600, 05004], lr: 0.011567, loss: 1.1854
2022-07-08 12:44:33 - train: epoch 0157, iter [04700, 05004], lr: 0.011557, loss: 1.0419
2022-07-08 12:45:07 - train: epoch 0157, iter [04800, 05004], lr: 0.011547, loss: 1.0967
2022-07-08 12:45:41 - train: epoch 0157, iter [04900, 05004], lr: 0.011536, loss: 1.0963
2022-07-08 12:46:13 - train: epoch 0157, iter [05000, 05004], lr: 0.011526, loss: 1.0609
2022-07-08 12:46:14 - train: epoch 157, train_loss: 1.0628
2022-07-08 12:47:29 - eval: epoch: 157, acc1: 72.148%, acc5: 91.002%, test_loss: 1.1246, per_image_load_time: 1.677ms, per_image_inference_time: 0.485ms
2022-07-08 12:47:29 - until epoch: 157, best_acc1: 72.852%
2022-07-08 12:47:29 - epoch 158 lr: 0.011526
2022-07-08 12:48:08 - train: epoch 0158, iter [00100, 05004], lr: 0.011515, loss: 1.1872
2022-07-08 12:48:42 - train: epoch 0158, iter [00200, 05004], lr: 0.011505, loss: 0.9283
2022-07-08 12:49:15 - train: epoch 0158, iter [00300, 05004], lr: 0.011495, loss: 0.9510
2022-07-08 12:49:49 - train: epoch 0158, iter [00400, 05004], lr: 0.011485, loss: 0.9187
2022-07-08 12:50:21 - train: epoch 0158, iter [00500, 05004], lr: 0.011474, loss: 0.9228
2022-07-08 12:50:56 - train: epoch 0158, iter [00600, 05004], lr: 0.011464, loss: 0.9267
2022-07-08 12:51:30 - train: epoch 0158, iter [00700, 05004], lr: 0.011454, loss: 0.9239
2022-07-08 12:52:03 - train: epoch 0158, iter [00800, 05004], lr: 0.011444, loss: 1.1885
2022-07-08 12:52:37 - train: epoch 0158, iter [00900, 05004], lr: 0.011433, loss: 1.1708
2022-07-08 12:53:11 - train: epoch 0158, iter [01000, 05004], lr: 0.011423, loss: 1.0912
2022-07-08 12:53:45 - train: epoch 0158, iter [01100, 05004], lr: 0.011413, loss: 0.8546
2022-07-08 12:54:18 - train: epoch 0158, iter [01200, 05004], lr: 0.011403, loss: 0.8981
2022-07-08 12:54:52 - train: epoch 0158, iter [01300, 05004], lr: 0.011392, loss: 1.0585
2022-07-08 12:55:25 - train: epoch 0158, iter [01400, 05004], lr: 0.011382, loss: 0.9944
2022-07-08 12:55:59 - train: epoch 0158, iter [01500, 05004], lr: 0.011372, loss: 0.9295
2022-07-08 12:56:33 - train: epoch 0158, iter [01600, 05004], lr: 0.011362, loss: 1.0623
2022-07-08 12:57:07 - train: epoch 0158, iter [01700, 05004], lr: 0.011352, loss: 1.0404
2022-07-08 12:57:41 - train: epoch 0158, iter [01800, 05004], lr: 0.011341, loss: 1.0797
2022-07-08 12:58:15 - train: epoch 0158, iter [01900, 05004], lr: 0.011331, loss: 1.2865
2022-07-08 12:58:47 - train: epoch 0158, iter [02000, 05004], lr: 0.011321, loss: 1.2336
2022-07-08 12:59:21 - train: epoch 0158, iter [02100, 05004], lr: 0.011311, loss: 1.1302
2022-07-08 12:59:56 - train: epoch 0158, iter [02200, 05004], lr: 0.011301, loss: 1.2235
2022-07-08 13:00:29 - train: epoch 0158, iter [02300, 05004], lr: 0.011290, loss: 1.0798
2022-07-08 13:01:02 - train: epoch 0158, iter [02400, 05004], lr: 0.011280, loss: 1.1027
2022-07-08 13:01:36 - train: epoch 0158, iter [02500, 05004], lr: 0.011270, loss: 1.1114
2022-07-08 13:02:10 - train: epoch 0158, iter [02600, 05004], lr: 0.011260, loss: 0.9306
2022-07-08 13:02:44 - train: epoch 0158, iter [02700, 05004], lr: 0.011250, loss: 0.9747
2022-07-08 13:03:17 - train: epoch 0158, iter [02800, 05004], lr: 0.011239, loss: 1.1154
2022-07-08 13:03:51 - train: epoch 0158, iter [02900, 05004], lr: 0.011229, loss: 1.0622
2022-07-08 13:04:25 - train: epoch 0158, iter [03000, 05004], lr: 0.011219, loss: 1.0029
2022-07-08 13:04:58 - train: epoch 0158, iter [03100, 05004], lr: 0.011209, loss: 0.9932
2022-07-08 13:05:32 - train: epoch 0158, iter [03200, 05004], lr: 0.011199, loss: 1.0915
2022-07-08 13:06:05 - train: epoch 0158, iter [03300, 05004], lr: 0.011189, loss: 0.9232
2022-07-08 13:06:40 - train: epoch 0158, iter [03400, 05004], lr: 0.011178, loss: 0.9697
2022-07-08 13:07:13 - train: epoch 0158, iter [03500, 05004], lr: 0.011168, loss: 1.0819
2022-07-08 13:07:47 - train: epoch 0158, iter [03600, 05004], lr: 0.011158, loss: 1.0353
2022-07-08 13:08:20 - train: epoch 0158, iter [03700, 05004], lr: 0.011148, loss: 1.0932
2022-07-08 13:08:55 - train: epoch 0158, iter [03800, 05004], lr: 0.011138, loss: 1.2139
2022-07-08 13:09:29 - train: epoch 0158, iter [03900, 05004], lr: 0.011128, loss: 1.0562
2022-07-08 13:10:02 - train: epoch 0158, iter [04000, 05004], lr: 0.011118, loss: 0.9650
2022-07-08 13:10:36 - train: epoch 0158, iter [04100, 05004], lr: 0.011108, loss: 1.0521
2022-07-08 13:11:10 - train: epoch 0158, iter [04200, 05004], lr: 0.011097, loss: 1.0805
2022-07-08 13:11:44 - train: epoch 0158, iter [04300, 05004], lr: 0.011087, loss: 1.1848
2022-07-08 13:12:17 - train: epoch 0158, iter [04400, 05004], lr: 0.011077, loss: 1.0746
2022-07-08 13:12:51 - train: epoch 0158, iter [04500, 05004], lr: 0.011067, loss: 1.0331
2022-07-08 13:13:25 - train: epoch 0158, iter [04600, 05004], lr: 0.011057, loss: 0.9468
2022-07-08 13:13:59 - train: epoch 0158, iter [04700, 05004], lr: 0.011047, loss: 0.9280
2022-07-08 13:14:33 - train: epoch 0158, iter [04800, 05004], lr: 0.011037, loss: 1.0872
2022-07-08 13:15:07 - train: epoch 0158, iter [04900, 05004], lr: 0.011027, loss: 0.9965
2022-07-08 13:15:38 - train: epoch 0158, iter [05000, 05004], lr: 0.011017, loss: 1.1436
2022-07-08 13:15:40 - train: epoch 158, train_loss: 1.0471
2022-07-08 13:16:55 - eval: epoch: 158, acc1: 72.866%, acc5: 91.310%, test_loss: 1.0943, per_image_load_time: 0.809ms, per_image_inference_time: 0.464ms
2022-07-08 13:16:55 - until epoch: 158, best_acc1: 72.866%
2022-07-08 13:16:55 - epoch 159 lr: 0.011016
2022-07-08 13:17:34 - train: epoch 0159, iter [00100, 05004], lr: 0.011006, loss: 0.8978
2022-07-08 13:18:07 - train: epoch 0159, iter [00200, 05004], lr: 0.010996, loss: 0.9880
2022-07-08 13:18:41 - train: epoch 0159, iter [00300, 05004], lr: 0.010986, loss: 1.1021
2022-07-08 13:19:15 - train: epoch 0159, iter [00400, 05004], lr: 0.010976, loss: 1.0498
2022-07-08 13:19:48 - train: epoch 0159, iter [00500, 05004], lr: 0.010966, loss: 1.1538
2022-07-08 13:20:22 - train: epoch 0159, iter [00600, 05004], lr: 0.010956, loss: 0.9261
2022-07-08 13:20:55 - train: epoch 0159, iter [00700, 05004], lr: 0.010946, loss: 1.1506
2022-07-08 13:21:29 - train: epoch 0159, iter [00800, 05004], lr: 0.010936, loss: 0.8335
2022-07-08 13:22:03 - train: epoch 0159, iter [00900, 05004], lr: 0.010926, loss: 0.8457
2022-07-08 13:22:36 - train: epoch 0159, iter [01000, 05004], lr: 0.010916, loss: 1.0475
2022-07-08 13:23:10 - train: epoch 0159, iter [01100, 05004], lr: 0.010906, loss: 1.1589
2022-07-08 13:23:43 - train: epoch 0159, iter [01200, 05004], lr: 0.010896, loss: 0.7801
2022-07-08 13:24:17 - train: epoch 0159, iter [01300, 05004], lr: 0.010886, loss: 1.1916
2022-07-08 13:24:51 - train: epoch 0159, iter [01400, 05004], lr: 0.010876, loss: 1.0339
2022-07-08 13:25:24 - train: epoch 0159, iter [01500, 05004], lr: 0.010866, loss: 1.0991
2022-07-08 13:25:58 - train: epoch 0159, iter [01600, 05004], lr: 0.010856, loss: 1.1788
2022-07-08 13:26:32 - train: epoch 0159, iter [01700, 05004], lr: 0.010846, loss: 1.0036
2022-07-08 13:27:05 - train: epoch 0159, iter [01800, 05004], lr: 0.010835, loss: 0.9614
2022-07-08 13:27:38 - train: epoch 0159, iter [01900, 05004], lr: 0.010825, loss: 0.9281
2022-07-08 13:28:12 - train: epoch 0159, iter [02000, 05004], lr: 0.010815, loss: 1.1051
2022-07-08 13:28:46 - train: epoch 0159, iter [02100, 05004], lr: 0.010805, loss: 0.9209
2022-07-08 13:29:19 - train: epoch 0159, iter [02200, 05004], lr: 0.010795, loss: 1.0364
2022-07-08 13:29:53 - train: epoch 0159, iter [02300, 05004], lr: 0.010786, loss: 0.8757
2022-07-08 13:30:27 - train: epoch 0159, iter [02400, 05004], lr: 0.010776, loss: 0.8957
2022-07-08 13:31:00 - train: epoch 0159, iter [02500, 05004], lr: 0.010766, loss: 0.8199
2022-07-08 13:31:33 - train: epoch 0159, iter [02600, 05004], lr: 0.010756, loss: 1.0631
2022-07-08 13:32:07 - train: epoch 0159, iter [02700, 05004], lr: 0.010746, loss: 1.0175
2022-07-08 13:32:41 - train: epoch 0159, iter [02800, 05004], lr: 0.010736, loss: 1.1214
2022-07-08 13:33:15 - train: epoch 0159, iter [02900, 05004], lr: 0.010726, loss: 1.0895
2022-07-08 13:33:48 - train: epoch 0159, iter [03000, 05004], lr: 0.010716, loss: 0.9467
2022-07-08 13:34:22 - train: epoch 0159, iter [03100, 05004], lr: 0.010706, loss: 1.0095
2022-07-08 13:34:56 - train: epoch 0159, iter [03200, 05004], lr: 0.010696, loss: 1.0332
2022-07-08 13:35:29 - train: epoch 0159, iter [03300, 05004], lr: 0.010686, loss: 1.0738
2022-07-08 13:36:03 - train: epoch 0159, iter [03400, 05004], lr: 0.010676, loss: 1.1185
2022-07-08 13:36:36 - train: epoch 0159, iter [03500, 05004], lr: 0.010666, loss: 1.0413
2022-07-08 13:37:10 - train: epoch 0159, iter [03600, 05004], lr: 0.010656, loss: 0.9267
2022-07-08 13:37:44 - train: epoch 0159, iter [03700, 05004], lr: 0.010646, loss: 1.0048
2022-07-08 13:38:17 - train: epoch 0159, iter [03800, 05004], lr: 0.010636, loss: 0.9849
2022-07-08 13:38:50 - train: epoch 0159, iter [03900, 05004], lr: 0.010626, loss: 1.1022
2022-07-08 13:39:24 - train: epoch 0159, iter [04000, 05004], lr: 0.010616, loss: 1.2060
2022-07-08 13:39:58 - train: epoch 0159, iter [04100, 05004], lr: 0.010606, loss: 1.0727
2022-07-08 13:40:31 - train: epoch 0159, iter [04200, 05004], lr: 0.010596, loss: 1.0687
2022-07-08 13:41:05 - train: epoch 0159, iter [04300, 05004], lr: 0.010587, loss: 1.0311
2022-07-08 13:41:39 - train: epoch 0159, iter [04400, 05004], lr: 0.010577, loss: 0.9331
2022-07-08 13:42:12 - train: epoch 0159, iter [04500, 05004], lr: 0.010567, loss: 1.2490
2022-07-08 13:42:45 - train: epoch 0159, iter [04600, 05004], lr: 0.010557, loss: 1.0394
2022-07-08 13:43:19 - train: epoch 0159, iter [04700, 05004], lr: 0.010547, loss: 0.9369
2022-07-08 13:43:52 - train: epoch 0159, iter [04800, 05004], lr: 0.010537, loss: 1.0329
2022-07-08 13:44:25 - train: epoch 0159, iter [04900, 05004], lr: 0.010527, loss: 1.0702
2022-07-08 13:44:58 - train: epoch 0159, iter [05000, 05004], lr: 0.010517, loss: 1.0078
2022-07-08 13:44:59 - train: epoch 159, train_loss: 1.0362
2022-07-08 13:46:14 - eval: epoch: 159, acc1: 73.126%, acc5: 91.544%, test_loss: 1.0793, per_image_load_time: 1.367ms, per_image_inference_time: 0.474ms
2022-07-08 13:46:14 - until epoch: 159, best_acc1: 73.126%
2022-07-08 13:46:14 - epoch 160 lr: 0.010517
2022-07-08 13:46:53 - train: epoch 0160, iter [00100, 05004], lr: 0.010507, loss: 0.9398
2022-07-08 13:47:27 - train: epoch 0160, iter [00200, 05004], lr: 0.010497, loss: 0.9116
2022-07-08 13:48:01 - train: epoch 0160, iter [00300, 05004], lr: 0.010487, loss: 1.0708
2022-07-08 13:48:35 - train: epoch 0160, iter [00400, 05004], lr: 0.010477, loss: 0.8841
2022-07-08 13:49:09 - train: epoch 0160, iter [00500, 05004], lr: 0.010468, loss: 1.1290
2022-07-08 13:49:41 - train: epoch 0160, iter [00600, 05004], lr: 0.010458, loss: 1.0703
2022-07-08 13:50:15 - train: epoch 0160, iter [00700, 05004], lr: 0.010448, loss: 0.9765
2022-07-08 13:50:48 - train: epoch 0160, iter [00800, 05004], lr: 0.010438, loss: 0.9530
2022-07-08 13:51:22 - train: epoch 0160, iter [00900, 05004], lr: 0.010428, loss: 1.2311
2022-07-08 13:51:56 - train: epoch 0160, iter [01000, 05004], lr: 0.010418, loss: 1.0167
2022-07-08 13:52:29 - train: epoch 0160, iter [01100, 05004], lr: 0.010409, loss: 1.0249
2022-07-08 13:53:02 - train: epoch 0160, iter [01200, 05004], lr: 0.010399, loss: 0.9713
2022-07-08 13:53:36 - train: epoch 0160, iter [01300, 05004], lr: 0.010389, loss: 1.1641
2022-07-08 13:54:10 - train: epoch 0160, iter [01400, 05004], lr: 0.010379, loss: 0.9892
2022-07-08 13:54:43 - train: epoch 0160, iter [01500, 05004], lr: 0.010369, loss: 0.9675
2022-07-08 13:55:17 - train: epoch 0160, iter [01600, 05004], lr: 0.010359, loss: 1.0791
2022-07-08 13:55:51 - train: epoch 0160, iter [01700, 05004], lr: 0.010350, loss: 0.8860
2022-07-08 13:56:24 - train: epoch 0160, iter [01800, 05004], lr: 0.010340, loss: 1.0570
2022-07-08 13:56:58 - train: epoch 0160, iter [01900, 05004], lr: 0.010330, loss: 1.0295
2022-07-08 13:57:30 - train: epoch 0160, iter [02000, 05004], lr: 0.010320, loss: 0.8524
2022-07-08 13:58:04 - train: epoch 0160, iter [02100, 05004], lr: 0.010310, loss: 1.1066
2022-07-08 13:58:38 - train: epoch 0160, iter [02200, 05004], lr: 0.010301, loss: 1.0171
2022-07-08 13:59:11 - train: epoch 0160, iter [02300, 05004], lr: 0.010291, loss: 0.9855
2022-07-08 13:59:45 - train: epoch 0160, iter [02400, 05004], lr: 0.010281, loss: 0.9629
2022-07-08 14:00:18 - train: epoch 0160, iter [02500, 05004], lr: 0.010271, loss: 0.9546
2022-07-08 14:00:51 - train: epoch 0160, iter [02600, 05004], lr: 0.010262, loss: 0.9648
2022-07-08 14:01:25 - train: epoch 0160, iter [02700, 05004], lr: 0.010252, loss: 0.9046
2022-07-08 14:01:58 - train: epoch 0160, iter [02800, 05004], lr: 0.010242, loss: 1.0639
2022-07-08 14:02:32 - train: epoch 0160, iter [02900, 05004], lr: 0.010232, loss: 1.0638
2022-07-08 14:03:06 - train: epoch 0160, iter [03000, 05004], lr: 0.010222, loss: 0.9966
2022-07-08 14:03:39 - train: epoch 0160, iter [03100, 05004], lr: 0.010213, loss: 1.0551
2022-07-08 14:04:13 - train: epoch 0160, iter [03200, 05004], lr: 0.010203, loss: 0.9033
2022-07-08 14:04:46 - train: epoch 0160, iter [03300, 05004], lr: 0.010193, loss: 1.0567
2022-07-08 14:05:19 - train: epoch 0160, iter [03400, 05004], lr: 0.010184, loss: 1.1526
2022-07-08 14:05:54 - train: epoch 0160, iter [03500, 05004], lr: 0.010174, loss: 1.1010
2022-07-08 14:06:27 - train: epoch 0160, iter [03600, 05004], lr: 0.010164, loss: 0.9733
2022-07-08 14:07:01 - train: epoch 0160, iter [03700, 05004], lr: 0.010154, loss: 1.0425
2022-07-08 14:07:34 - train: epoch 0160, iter [03800, 05004], lr: 0.010145, loss: 1.0489
2022-07-08 14:08:10 - train: epoch 0160, iter [03900, 05004], lr: 0.010135, loss: 1.1592
2022-07-08 14:08:42 - train: epoch 0160, iter [04000, 05004], lr: 0.010125, loss: 1.0854
2022-07-08 14:09:15 - train: epoch 0160, iter [04100, 05004], lr: 0.010115, loss: 1.0330
2022-07-08 14:09:50 - train: epoch 0160, iter [04200, 05004], lr: 0.010106, loss: 1.0752
2022-07-08 14:10:24 - train: epoch 0160, iter [04300, 05004], lr: 0.010096, loss: 0.8464
2022-07-08 14:10:56 - train: epoch 0160, iter [04400, 05004], lr: 0.010086, loss: 0.9626
2022-07-08 14:11:30 - train: epoch 0160, iter [04500, 05004], lr: 0.010077, loss: 1.1207
2022-07-08 14:12:04 - train: epoch 0160, iter [04600, 05004], lr: 0.010067, loss: 1.0084
2022-07-08 14:12:38 - train: epoch 0160, iter [04700, 05004], lr: 0.010057, loss: 1.1020
2022-07-08 14:13:11 - train: epoch 0160, iter [04800, 05004], lr: 0.010048, loss: 1.1322
2022-07-08 14:13:45 - train: epoch 0160, iter [04900, 05004], lr: 0.010038, loss: 0.9389
2022-07-08 14:14:17 - train: epoch 0160, iter [05000, 05004], lr: 0.010028, loss: 0.9852
2022-07-08 14:14:18 - train: epoch 160, train_loss: 1.0214
2022-07-08 14:15:33 - eval: epoch: 160, acc1: 73.188%, acc5: 91.640%, test_loss: 1.0900, per_image_load_time: 1.913ms, per_image_inference_time: 0.475ms
2022-07-08 14:15:33 - until epoch: 160, best_acc1: 73.188%
2022-07-08 14:15:33 - epoch 161 lr: 0.010028
2022-07-08 14:16:13 - train: epoch 0161, iter [00100, 05004], lr: 0.010018, loss: 0.9773
2022-07-08 14:16:46 - train: epoch 0161, iter [00200, 05004], lr: 0.010009, loss: 0.9577
2022-07-08 14:17:20 - train: epoch 0161, iter [00300, 05004], lr: 0.009999, loss: 0.8710
2022-07-08 14:17:54 - train: epoch 0161, iter [00400, 05004], lr: 0.009989, loss: 0.9653
2022-07-08 14:18:27 - train: epoch 0161, iter [00500, 05004], lr: 0.009980, loss: 1.0998
2022-07-08 14:18:59 - train: epoch 0161, iter [00600, 05004], lr: 0.009970, loss: 0.8906
2022-07-08 14:19:32 - train: epoch 0161, iter [00700, 05004], lr: 0.009960, loss: 0.9582
2022-07-08 14:20:06 - train: epoch 0161, iter [00800, 05004], lr: 0.009951, loss: 0.9254
2022-07-08 14:20:38 - train: epoch 0161, iter [00900, 05004], lr: 0.009941, loss: 0.8512
2022-07-08 14:21:12 - train: epoch 0161, iter [01000, 05004], lr: 0.009931, loss: 1.1578
2022-07-08 14:21:45 - train: epoch 0161, iter [01100, 05004], lr: 0.009922, loss: 0.9992
2022-07-08 14:22:18 - train: epoch 0161, iter [01200, 05004], lr: 0.009912, loss: 0.9433
2022-07-08 14:22:52 - train: epoch 0161, iter [01300, 05004], lr: 0.009902, loss: 1.0229
2022-07-08 14:23:25 - train: epoch 0161, iter [01400, 05004], lr: 0.009893, loss: 1.0844
2022-07-08 14:23:58 - train: epoch 0161, iter [01500, 05004], lr: 0.009883, loss: 0.9102
2022-07-08 14:24:31 - train: epoch 0161, iter [01600, 05004], lr: 0.009874, loss: 0.9567
2022-07-08 14:25:05 - train: epoch 0161, iter [01700, 05004], lr: 0.009864, loss: 0.9264
2022-07-08 14:25:38 - train: epoch 0161, iter [01800, 05004], lr: 0.009854, loss: 1.0493
2022-07-08 14:26:12 - train: epoch 0161, iter [01900, 05004], lr: 0.009845, loss: 0.8474
2022-07-08 14:26:46 - train: epoch 0161, iter [02000, 05004], lr: 0.009835, loss: 1.0321
2022-07-08 14:27:19 - train: epoch 0161, iter [02100, 05004], lr: 0.009826, loss: 1.0555
2022-07-08 14:27:53 - train: epoch 0161, iter [02200, 05004], lr: 0.009816, loss: 0.9317
2022-07-08 14:28:26 - train: epoch 0161, iter [02300, 05004], lr: 0.009807, loss: 1.0233
2022-07-08 14:28:59 - train: epoch 0161, iter [02400, 05004], lr: 0.009797, loss: 1.1784
2022-07-08 14:29:33 - train: epoch 0161, iter [02500, 05004], lr: 0.009787, loss: 1.2430
2022-07-08 14:30:07 - train: epoch 0161, iter [02600, 05004], lr: 0.009778, loss: 1.0607
2022-07-08 14:30:40 - train: epoch 0161, iter [02700, 05004], lr: 0.009768, loss: 1.0327
2022-07-08 14:31:14 - train: epoch 0161, iter [02800, 05004], lr: 0.009759, loss: 0.8649
2022-07-08 14:31:47 - train: epoch 0161, iter [02900, 05004], lr: 0.009749, loss: 0.9840
2022-07-08 14:32:21 - train: epoch 0161, iter [03000, 05004], lr: 0.009740, loss: 0.7761
2022-07-08 14:32:54 - train: epoch 0161, iter [03100, 05004], lr: 0.009730, loss: 0.9711
2022-07-08 14:33:27 - train: epoch 0161, iter [03200, 05004], lr: 0.009721, loss: 1.0398
2022-07-08 14:34:02 - train: epoch 0161, iter [03300, 05004], lr: 0.009711, loss: 0.8664
2022-07-08 14:34:35 - train: epoch 0161, iter [03400, 05004], lr: 0.009701, loss: 0.9504
2022-07-08 14:35:08 - train: epoch 0161, iter [03500, 05004], lr: 0.009692, loss: 1.0936
2022-07-08 14:35:42 - train: epoch 0161, iter [03600, 05004], lr: 0.009682, loss: 1.0610
2022-07-08 14:36:16 - train: epoch 0161, iter [03700, 05004], lr: 0.009673, loss: 1.0154
2022-07-08 14:36:49 - train: epoch 0161, iter [03800, 05004], lr: 0.009663, loss: 1.1404
2022-07-08 14:37:23 - train: epoch 0161, iter [03900, 05004], lr: 0.009654, loss: 1.0151
2022-07-08 14:37:56 - train: epoch 0161, iter [04000, 05004], lr: 0.009644, loss: 1.2451
2022-07-08 14:38:30 - train: epoch 0161, iter [04100, 05004], lr: 0.009635, loss: 0.9932
2022-07-08 14:39:04 - train: epoch 0161, iter [04200, 05004], lr: 0.009625, loss: 1.1215
2022-07-08 14:39:37 - train: epoch 0161, iter [04300, 05004], lr: 0.009616, loss: 0.9176
2022-07-08 14:40:11 - train: epoch 0161, iter [04400, 05004], lr: 0.009606, loss: 1.1067
2022-07-08 14:40:44 - train: epoch 0161, iter [04500, 05004], lr: 0.009597, loss: 0.9062
2022-07-08 14:41:18 - train: epoch 0161, iter [04600, 05004], lr: 0.009587, loss: 0.9719
2022-07-08 14:41:52 - train: epoch 0161, iter [04700, 05004], lr: 0.009578, loss: 0.9926
2022-07-08 14:42:26 - train: epoch 0161, iter [04800, 05004], lr: 0.009568, loss: 1.0864
2022-07-08 14:42:59 - train: epoch 0161, iter [04900, 05004], lr: 0.009559, loss: 0.8467
2022-07-08 14:43:32 - train: epoch 0161, iter [05000, 05004], lr: 0.009550, loss: 0.8726
2022-07-08 14:43:33 - train: epoch 161, train_loss: 1.0088
2022-07-08 14:44:47 - eval: epoch: 161, acc1: 72.836%, acc5: 91.354%, test_loss: 1.0972, per_image_load_time: 1.441ms, per_image_inference_time: 0.482ms
2022-07-08 14:44:47 - until epoch: 161, best_acc1: 73.188%
2022-07-08 14:44:47 - epoch 162 lr: 0.009549
2022-07-08 14:45:27 - train: epoch 0162, iter [00100, 05004], lr: 0.009540, loss: 0.8593
2022-07-08 14:46:01 - train: epoch 0162, iter [00200, 05004], lr: 0.009530, loss: 0.8809
2022-07-08 14:46:34 - train: epoch 0162, iter [00300, 05004], lr: 0.009521, loss: 1.1278
2022-07-08 14:47:07 - train: epoch 0162, iter [00400, 05004], lr: 0.009511, loss: 1.0281
2022-07-08 14:47:42 - train: epoch 0162, iter [00500, 05004], lr: 0.009502, loss: 1.1019
2022-07-08 14:48:15 - train: epoch 0162, iter [00600, 05004], lr: 0.009492, loss: 0.9003
2022-07-08 14:48:49 - train: epoch 0162, iter [00700, 05004], lr: 0.009483, loss: 0.9833
2022-07-08 14:49:23 - train: epoch 0162, iter [00800, 05004], lr: 0.009474, loss: 0.8296
2022-07-08 14:49:57 - train: epoch 0162, iter [00900, 05004], lr: 0.009464, loss: 1.0588
2022-07-08 14:50:30 - train: epoch 0162, iter [01000, 05004], lr: 0.009455, loss: 0.9590
2022-07-08 14:51:03 - train: epoch 0162, iter [01100, 05004], lr: 0.009445, loss: 1.0239
2022-07-08 14:51:37 - train: epoch 0162, iter [01200, 05004], lr: 0.009436, loss: 1.0846
2022-07-08 14:52:10 - train: epoch 0162, iter [01300, 05004], lr: 0.009426, loss: 1.0807
2022-07-08 14:52:44 - train: epoch 0162, iter [01400, 05004], lr: 0.009417, loss: 0.9402
2022-07-08 14:53:17 - train: epoch 0162, iter [01500, 05004], lr: 0.009408, loss: 0.9471
2022-07-08 14:53:50 - train: epoch 0162, iter [01600, 05004], lr: 0.009398, loss: 1.0822
2022-07-08 14:54:24 - train: epoch 0162, iter [01700, 05004], lr: 0.009389, loss: 0.8861
2022-07-08 14:54:58 - train: epoch 0162, iter [01800, 05004], lr: 0.009380, loss: 0.9888
2022-07-08 14:55:31 - train: epoch 0162, iter [01900, 05004], lr: 0.009370, loss: 1.1262
2022-07-08 14:56:04 - train: epoch 0162, iter [02000, 05004], lr: 0.009361, loss: 1.0075
2022-07-08 14:56:37 - train: epoch 0162, iter [02100, 05004], lr: 0.009351, loss: 0.9814
2022-07-08 14:57:10 - train: epoch 0162, iter [02200, 05004], lr: 0.009342, loss: 0.8840
2022-07-08 14:57:44 - train: epoch 0162, iter [02300, 05004], lr: 0.009333, loss: 1.0907
2022-07-08 14:58:18 - train: epoch 0162, iter [02400, 05004], lr: 0.009323, loss: 0.9200
2022-07-08 14:58:51 - train: epoch 0162, iter [02500, 05004], lr: 0.009314, loss: 1.1547
2022-07-08 14:59:25 - train: epoch 0162, iter [02600, 05004], lr: 0.009305, loss: 0.9530
2022-07-08 14:59:59 - train: epoch 0162, iter [02700, 05004], lr: 0.009295, loss: 0.9873
2022-07-08 15:00:33 - train: epoch 0162, iter [02800, 05004], lr: 0.009286, loss: 0.9536
2022-07-08 15:01:07 - train: epoch 0162, iter [02900, 05004], lr: 0.009277, loss: 0.8981
2022-07-08 15:01:41 - train: epoch 0162, iter [03000, 05004], lr: 0.009267, loss: 0.9078
2022-07-08 15:02:13 - train: epoch 0162, iter [03100, 05004], lr: 0.009258, loss: 1.0333
2022-07-08 15:02:47 - train: epoch 0162, iter [03200, 05004], lr: 0.009249, loss: 0.9161
2022-07-08 15:03:21 - train: epoch 0162, iter [03300, 05004], lr: 0.009239, loss: 1.1038
2022-07-08 15:03:55 - train: epoch 0162, iter [03400, 05004], lr: 0.009230, loss: 0.9121
2022-07-08 15:04:29 - train: epoch 0162, iter [03500, 05004], lr: 0.009221, loss: 1.0396
2022-07-08 15:05:02 - train: epoch 0162, iter [03600, 05004], lr: 0.009211, loss: 1.2658
2022-07-08 15:05:36 - train: epoch 0162, iter [03700, 05004], lr: 0.009202, loss: 1.0356
2022-07-08 15:06:10 - train: epoch 0162, iter [03800, 05004], lr: 0.009193, loss: 1.1772
2022-07-08 15:06:44 - train: epoch 0162, iter [03900, 05004], lr: 0.009183, loss: 1.0342
2022-07-08 15:07:17 - train: epoch 0162, iter [04000, 05004], lr: 0.009174, loss: 0.8371
2022-07-08 15:07:50 - train: epoch 0162, iter [04100, 05004], lr: 0.009165, loss: 1.1360
2022-07-08 15:08:24 - train: epoch 0162, iter [04200, 05004], lr: 0.009155, loss: 1.0777
2022-07-08 15:08:58 - train: epoch 0162, iter [04300, 05004], lr: 0.009146, loss: 1.0788
2022-07-08 15:09:32 - train: epoch 0162, iter [04400, 05004], lr: 0.009137, loss: 1.0199
2022-07-08 15:10:06 - train: epoch 0162, iter [04500, 05004], lr: 0.009128, loss: 1.1096
2022-07-08 15:10:40 - train: epoch 0162, iter [04600, 05004], lr: 0.009118, loss: 1.0446
2022-07-08 15:11:14 - train: epoch 0162, iter [04700, 05004], lr: 0.009109, loss: 0.8895
2022-07-08 15:11:48 - train: epoch 0162, iter [04800, 05004], lr: 0.009100, loss: 1.1553
2022-07-08 15:12:21 - train: epoch 0162, iter [04900, 05004], lr: 0.009091, loss: 1.0397
2022-07-08 15:12:53 - train: epoch 0162, iter [05000, 05004], lr: 0.009081, loss: 1.0990
2022-07-08 15:12:55 - train: epoch 162, train_loss: 0.9963
2022-07-08 15:14:10 - eval: epoch: 162, acc1: 73.610%, acc5: 91.924%, test_loss: 1.0709, per_image_load_time: 1.370ms, per_image_inference_time: 0.481ms
2022-07-08 15:14:10 - until epoch: 162, best_acc1: 73.610%
2022-07-08 15:14:10 - epoch 163 lr: 0.009081
2022-07-08 15:14:49 - train: epoch 0163, iter [00100, 05004], lr: 0.009072, loss: 1.0333
2022-07-08 15:15:23 - train: epoch 0163, iter [00200, 05004], lr: 0.009062, loss: 0.9438
2022-07-08 15:15:57 - train: epoch 0163, iter [00300, 05004], lr: 0.009053, loss: 1.0593
2022-07-08 15:16:30 - train: epoch 0163, iter [00400, 05004], lr: 0.009044, loss: 0.9445
2022-07-08 15:17:04 - train: epoch 0163, iter [00500, 05004], lr: 0.009035, loss: 0.9650
2022-07-08 15:17:38 - train: epoch 0163, iter [00600, 05004], lr: 0.009026, loss: 1.0139
2022-07-08 15:18:11 - train: epoch 0163, iter [00700, 05004], lr: 0.009016, loss: 0.8905
2022-07-08 15:18:45 - train: epoch 0163, iter [00800, 05004], lr: 0.009007, loss: 0.7424
2022-07-08 15:19:18 - train: epoch 0163, iter [00900, 05004], lr: 0.008998, loss: 0.9786
2022-07-08 15:19:53 - train: epoch 0163, iter [01000, 05004], lr: 0.008989, loss: 1.0115
2022-07-08 15:20:26 - train: epoch 0163, iter [01100, 05004], lr: 0.008979, loss: 0.9883
2022-07-08 15:21:00 - train: epoch 0163, iter [01200, 05004], lr: 0.008970, loss: 0.9908
2022-07-08 15:21:34 - train: epoch 0163, iter [01300, 05004], lr: 0.008961, loss: 0.8574
2022-07-08 15:22:08 - train: epoch 0163, iter [01400, 05004], lr: 0.008952, loss: 0.9120
2022-07-08 15:22:41 - train: epoch 0163, iter [01500, 05004], lr: 0.008943, loss: 0.8831
2022-07-08 15:23:16 - train: epoch 0163, iter [01600, 05004], lr: 0.008933, loss: 1.0859
2022-07-08 15:23:49 - train: epoch 0163, iter [01700, 05004], lr: 0.008924, loss: 1.0065
2022-07-08 15:24:23 - train: epoch 0163, iter [01800, 05004], lr: 0.008915, loss: 0.9885
2022-07-08 15:24:57 - train: epoch 0163, iter [01900, 05004], lr: 0.008906, loss: 1.0578
2022-07-08 15:25:30 - train: epoch 0163, iter [02000, 05004], lr: 0.008897, loss: 0.7966
2022-07-08 15:26:05 - train: epoch 0163, iter [02100, 05004], lr: 0.008888, loss: 0.9479
2022-07-08 15:26:38 - train: epoch 0163, iter [02200, 05004], lr: 0.008878, loss: 1.0384
2022-07-08 15:27:12 - train: epoch 0163, iter [02300, 05004], lr: 0.008869, loss: 0.9853
2022-07-08 15:27:45 - train: epoch 0163, iter [02400, 05004], lr: 0.008860, loss: 1.1554
2022-07-08 15:28:19 - train: epoch 0163, iter [02500, 05004], lr: 0.008851, loss: 0.9946
2022-07-08 15:28:53 - train: epoch 0163, iter [02600, 05004], lr: 0.008842, loss: 0.8871
2022-07-08 15:29:28 - train: epoch 0163, iter [02700, 05004], lr: 0.008833, loss: 1.2016
2022-07-08 15:30:01 - train: epoch 0163, iter [02800, 05004], lr: 0.008824, loss: 1.0423
2022-07-08 15:30:35 - train: epoch 0163, iter [02900, 05004], lr: 0.008814, loss: 0.8278
2022-07-08 15:31:09 - train: epoch 0163, iter [03000, 05004], lr: 0.008805, loss: 1.1160
2022-07-08 15:31:42 - train: epoch 0163, iter [03100, 05004], lr: 0.008796, loss: 1.0040
2022-07-08 15:32:16 - train: epoch 0163, iter [03200, 05004], lr: 0.008787, loss: 0.9522
2022-07-08 15:32:51 - train: epoch 0163, iter [03300, 05004], lr: 0.008778, loss: 0.9505
2022-07-08 15:33:25 - train: epoch 0163, iter [03400, 05004], lr: 0.008769, loss: 0.9667
2022-07-08 15:33:58 - train: epoch 0163, iter [03500, 05004], lr: 0.008760, loss: 0.9843
2022-07-08 15:34:32 - train: epoch 0163, iter [03600, 05004], lr: 0.008751, loss: 0.8651
2022-07-08 15:35:06 - train: epoch 0163, iter [03700, 05004], lr: 0.008742, loss: 1.0295
2022-07-08 15:35:39 - train: epoch 0163, iter [03800, 05004], lr: 0.008732, loss: 0.9387
2022-07-08 15:36:14 - train: epoch 0163, iter [03900, 05004], lr: 0.008723, loss: 0.8749
2022-07-08 15:36:47 - train: epoch 0163, iter [04000, 05004], lr: 0.008714, loss: 0.9358
2022-07-08 15:37:22 - train: epoch 0163, iter [04100, 05004], lr: 0.008705, loss: 0.8701
2022-07-08 15:37:55 - train: epoch 0163, iter [04200, 05004], lr: 0.008696, loss: 1.0083
2022-07-08 15:38:29 - train: epoch 0163, iter [04300, 05004], lr: 0.008687, loss: 0.8613
2022-07-08 15:39:02 - train: epoch 0163, iter [04400, 05004], lr: 0.008678, loss: 0.9690
2022-07-08 15:39:36 - train: epoch 0163, iter [04500, 05004], lr: 0.008669, loss: 0.8175
2022-07-08 15:40:10 - train: epoch 0163, iter [04600, 05004], lr: 0.008660, loss: 1.0378
2022-07-08 15:40:45 - train: epoch 0163, iter [04700, 05004], lr: 0.008651, loss: 1.0087
2022-07-08 15:41:18 - train: epoch 0163, iter [04800, 05004], lr: 0.008642, loss: 1.0513
2022-07-08 15:41:52 - train: epoch 0163, iter [04900, 05004], lr: 0.008633, loss: 0.9600
2022-07-08 15:42:24 - train: epoch 0163, iter [05000, 05004], lr: 0.008624, loss: 0.8603
2022-07-08 15:42:26 - train: epoch 163, train_loss: 0.9809
2022-07-08 15:43:41 - eval: epoch: 163, acc1: 73.684%, acc5: 91.752%, test_loss: 1.0646, per_image_load_time: 1.445ms, per_image_inference_time: 0.473ms
2022-07-08 15:43:41 - until epoch: 163, best_acc1: 73.684%
2022-07-08 15:43:41 - epoch 164 lr: 0.008623
2022-07-08 15:44:20 - train: epoch 0164, iter [00100, 05004], lr: 0.008614, loss: 0.8053
2022-07-08 15:44:54 - train: epoch 0164, iter [00200, 05004], lr: 0.008605, loss: 0.8301
2022-07-08 15:45:27 - train: epoch 0164, iter [00300, 05004], lr: 0.008596, loss: 1.0891
2022-07-08 15:46:00 - train: epoch 0164, iter [00400, 05004], lr: 0.008587, loss: 0.8288
2022-07-08 15:46:33 - train: epoch 0164, iter [00500, 05004], lr: 0.008578, loss: 0.8719
2022-07-08 15:47:08 - train: epoch 0164, iter [00600, 05004], lr: 0.008569, loss: 0.9618
2022-07-08 15:47:41 - train: epoch 0164, iter [00700, 05004], lr: 0.008560, loss: 0.9207
2022-07-08 15:48:14 - train: epoch 0164, iter [00800, 05004], lr: 0.008551, loss: 0.8591
2022-07-08 15:48:48 - train: epoch 0164, iter [00900, 05004], lr: 0.008542, loss: 0.8976
2022-07-08 15:49:21 - train: epoch 0164, iter [01000, 05004], lr: 0.008533, loss: 0.9893
2022-07-08 15:49:54 - train: epoch 0164, iter [01100, 05004], lr: 0.008524, loss: 0.9147
2022-07-08 15:50:28 - train: epoch 0164, iter [01200, 05004], lr: 0.008515, loss: 0.7446
2022-07-08 15:51:01 - train: epoch 0164, iter [01300, 05004], lr: 0.008506, loss: 0.7656
2022-07-08 15:51:34 - train: epoch 0164, iter [01400, 05004], lr: 0.008497, loss: 0.9925
2022-07-08 15:52:07 - train: epoch 0164, iter [01500, 05004], lr: 0.008488, loss: 1.2232
2022-07-08 15:52:41 - train: epoch 0164, iter [01600, 05004], lr: 0.008479, loss: 0.9182
2022-07-08 15:53:14 - train: epoch 0164, iter [01700, 05004], lr: 0.008470, loss: 1.0156
2022-07-08 15:53:47 - train: epoch 0164, iter [01800, 05004], lr: 0.008461, loss: 0.8809
2022-07-08 15:54:21 - train: epoch 0164, iter [01900, 05004], lr: 0.008452, loss: 0.9666
2022-07-08 15:54:54 - train: epoch 0164, iter [02000, 05004], lr: 0.008443, loss: 1.1206
2022-07-08 15:55:28 - train: epoch 0164, iter [02100, 05004], lr: 0.008435, loss: 0.9887
2022-07-08 15:56:01 - train: epoch 0164, iter [02200, 05004], lr: 0.008426, loss: 0.9755
2022-07-08 15:56:35 - train: epoch 0164, iter [02300, 05004], lr: 0.008417, loss: 1.0279
2022-07-08 15:57:08 - train: epoch 0164, iter [02400, 05004], lr: 0.008408, loss: 0.9981
2022-07-08 15:57:41 - train: epoch 0164, iter [02500, 05004], lr: 0.008399, loss: 1.0362
2022-07-08 15:58:14 - train: epoch 0164, iter [02600, 05004], lr: 0.008390, loss: 1.1835
2022-07-08 15:58:47 - train: epoch 0164, iter [02700, 05004], lr: 0.008381, loss: 1.0872
2022-07-08 15:59:20 - train: epoch 0164, iter [02800, 05004], lr: 0.008372, loss: 0.8694
2022-07-08 15:59:53 - train: epoch 0164, iter [02900, 05004], lr: 0.008363, loss: 0.8408
2022-07-08 16:00:27 - train: epoch 0164, iter [03000, 05004], lr: 0.008354, loss: 1.0032
2022-07-08 16:01:01 - train: epoch 0164, iter [03100, 05004], lr: 0.008345, loss: 1.0026
2022-07-08 16:01:33 - train: epoch 0164, iter [03200, 05004], lr: 0.008336, loss: 1.1774
2022-07-08 16:02:06 - train: epoch 0164, iter [03300, 05004], lr: 0.008327, loss: 1.0347
2022-07-08 16:02:40 - train: epoch 0164, iter [03400, 05004], lr: 0.008319, loss: 0.8857
2022-07-08 16:03:14 - train: epoch 0164, iter [03500, 05004], lr: 0.008310, loss: 0.9117
2022-07-08 16:03:47 - train: epoch 0164, iter [03600, 05004], lr: 0.008301, loss: 0.9684
2022-07-08 16:04:20 - train: epoch 0164, iter [03700, 05004], lr: 0.008292, loss: 1.0472
2022-07-08 16:04:54 - train: epoch 0164, iter [03800, 05004], lr: 0.008283, loss: 0.8771
2022-07-08 16:05:28 - train: epoch 0164, iter [03900, 05004], lr: 0.008274, loss: 0.8257
2022-07-08 16:06:01 - train: epoch 0164, iter [04000, 05004], lr: 0.008265, loss: 0.9258
2022-07-08 16:06:34 - train: epoch 0164, iter [04100, 05004], lr: 0.008256, loss: 0.9584
2022-07-08 16:07:08 - train: epoch 0164, iter [04200, 05004], lr: 0.008248, loss: 1.1174
2022-07-08 16:07:41 - train: epoch 0164, iter [04300, 05004], lr: 0.008239, loss: 0.9193
2022-07-08 16:08:15 - train: epoch 0164, iter [04400, 05004], lr: 0.008230, loss: 0.9465
2022-07-08 16:08:48 - train: epoch 0164, iter [04500, 05004], lr: 0.008221, loss: 1.0540
2022-07-08 16:09:21 - train: epoch 0164, iter [04600, 05004], lr: 0.008212, loss: 1.0914
2022-07-08 16:09:55 - train: epoch 0164, iter [04700, 05004], lr: 0.008203, loss: 0.8813
2022-07-08 16:10:28 - train: epoch 0164, iter [04800, 05004], lr: 0.008194, loss: 1.0354
2022-07-08 16:11:02 - train: epoch 0164, iter [04900, 05004], lr: 0.008186, loss: 0.9235
2022-07-08 16:11:34 - train: epoch 0164, iter [05000, 05004], lr: 0.008177, loss: 0.9297
2022-07-08 16:11:35 - train: epoch 164, train_loss: 0.9682
2022-07-08 16:12:50 - eval: epoch: 164, acc1: 73.844%, acc5: 91.778%, test_loss: 1.0617, per_image_load_time: 1.548ms, per_image_inference_time: 0.477ms
2022-07-08 16:12:50 - until epoch: 164, best_acc1: 73.844%
2022-07-08 16:12:50 - epoch 165 lr: 0.008176
2022-07-08 16:13:29 - train: epoch 0165, iter [00100, 05004], lr: 0.008168, loss: 0.8900
2022-07-08 16:14:03 - train: epoch 0165, iter [00200, 05004], lr: 0.008159, loss: 0.9198
2022-07-08 16:14:36 - train: epoch 0165, iter [00300, 05004], lr: 0.008150, loss: 1.1258
2022-07-08 16:15:09 - train: epoch 0165, iter [00400, 05004], lr: 0.008141, loss: 0.8639
2022-07-08 16:15:44 - train: epoch 0165, iter [00500, 05004], lr: 0.008132, loss: 0.9794
2022-07-08 16:16:17 - train: epoch 0165, iter [00600, 05004], lr: 0.008124, loss: 0.9848
2022-07-08 16:16:51 - train: epoch 0165, iter [00700, 05004], lr: 0.008115, loss: 1.0847
2022-07-08 16:17:24 - train: epoch 0165, iter [00800, 05004], lr: 0.008106, loss: 0.9329
2022-07-08 16:17:58 - train: epoch 0165, iter [00900, 05004], lr: 0.008097, loss: 0.9117
2022-07-08 16:18:31 - train: epoch 0165, iter [01000, 05004], lr: 0.008088, loss: 0.7736
2022-07-08 16:19:06 - train: epoch 0165, iter [01100, 05004], lr: 0.008080, loss: 0.8810
2022-07-08 16:19:39 - train: epoch 0165, iter [01200, 05004], lr: 0.008071, loss: 0.9159
2022-07-08 16:20:13 - train: epoch 0165, iter [01300, 05004], lr: 0.008062, loss: 0.9127
2022-07-08 16:20:47 - train: epoch 0165, iter [01400, 05004], lr: 0.008053, loss: 0.8196
2022-07-08 16:21:22 - train: epoch 0165, iter [01500, 05004], lr: 0.008045, loss: 0.9193
2022-07-08 16:21:55 - train: epoch 0165, iter [01600, 05004], lr: 0.008036, loss: 0.8582
2022-07-08 16:22:29 - train: epoch 0165, iter [01700, 05004], lr: 0.008027, loss: 1.0754
2022-07-08 16:23:02 - train: epoch 0165, iter [01800, 05004], lr: 0.008018, loss: 0.7708
2022-07-08 16:23:36 - train: epoch 0165, iter [01900, 05004], lr: 0.008010, loss: 0.8747
2022-07-08 16:24:11 - train: epoch 0165, iter [02000, 05004], lr: 0.008001, loss: 0.8363
2022-07-08 16:24:45 - train: epoch 0165, iter [02100, 05004], lr: 0.007992, loss: 0.9307
2022-07-08 16:25:19 - train: epoch 0165, iter [02200, 05004], lr: 0.007983, loss: 1.0303
2022-07-08 16:25:52 - train: epoch 0165, iter [02300, 05004], lr: 0.007975, loss: 0.9849
2022-07-08 16:26:27 - train: epoch 0165, iter [02400, 05004], lr: 0.007966, loss: 1.0463
2022-07-08 16:27:00 - train: epoch 0165, iter [02500, 05004], lr: 0.007957, loss: 0.9370
2022-07-08 16:27:34 - train: epoch 0165, iter [02600, 05004], lr: 0.007949, loss: 0.9033
2022-07-08 16:28:09 - train: epoch 0165, iter [02700, 05004], lr: 0.007940, loss: 1.0021
2022-07-08 16:28:43 - train: epoch 0165, iter [02800, 05004], lr: 0.007931, loss: 0.9140
2022-07-08 16:29:17 - train: epoch 0165, iter [02900, 05004], lr: 0.007922, loss: 1.1277
2022-07-08 16:29:51 - train: epoch 0165, iter [03000, 05004], lr: 0.007914, loss: 1.0422
2022-07-08 16:30:26 - train: epoch 0165, iter [03100, 05004], lr: 0.007905, loss: 0.9345
2022-07-08 16:30:59 - train: epoch 0165, iter [03200, 05004], lr: 0.007896, loss: 0.8995
2022-07-08 16:31:33 - train: epoch 0165, iter [03300, 05004], lr: 0.007888, loss: 1.0828
2022-07-08 16:32:07 - train: epoch 0165, iter [03400, 05004], lr: 0.007879, loss: 1.0181
2022-07-08 16:32:41 - train: epoch 0165, iter [03500, 05004], lr: 0.007870, loss: 0.9515
2022-07-08 16:33:15 - train: epoch 0165, iter [03600, 05004], lr: 0.007862, loss: 1.2634
2022-07-08 16:33:49 - train: epoch 0165, iter [03700, 05004], lr: 0.007853, loss: 0.8873
2022-07-08 16:34:22 - train: epoch 0165, iter [03800, 05004], lr: 0.007844, loss: 0.9816
2022-07-08 16:34:56 - train: epoch 0165, iter [03900, 05004], lr: 0.007836, loss: 0.8287
2022-07-08 16:35:30 - train: epoch 0165, iter [04000, 05004], lr: 0.007827, loss: 0.9539
2022-07-08 16:36:04 - train: epoch 0165, iter [04100, 05004], lr: 0.007818, loss: 0.8533
2022-07-08 16:36:37 - train: epoch 0165, iter [04200, 05004], lr: 0.007810, loss: 1.0947
2022-07-08 16:37:12 - train: epoch 0165, iter [04300, 05004], lr: 0.007801, loss: 0.9301
2022-07-08 16:37:45 - train: epoch 0165, iter [04400, 05004], lr: 0.007793, loss: 1.0085
2022-07-08 16:38:19 - train: epoch 0165, iter [04500, 05004], lr: 0.007784, loss: 0.8960
2022-07-08 16:38:53 - train: epoch 0165, iter [04600, 05004], lr: 0.007775, loss: 1.0231
2022-07-08 16:39:27 - train: epoch 0165, iter [04700, 05004], lr: 0.007767, loss: 1.0645
2022-07-08 16:40:01 - train: epoch 0165, iter [04800, 05004], lr: 0.007758, loss: 0.8591
2022-07-08 16:40:34 - train: epoch 0165, iter [04900, 05004], lr: 0.007749, loss: 0.7995
2022-07-08 16:41:07 - train: epoch 0165, iter [05000, 05004], lr: 0.007741, loss: 0.6838
2022-07-08 16:41:08 - train: epoch 165, train_loss: 0.9538
2022-07-08 16:42:23 - eval: epoch: 165, acc1: 73.990%, acc5: 92.062%, test_loss: 1.0510, per_image_load_time: 1.197ms, per_image_inference_time: 0.452ms
2022-07-08 16:42:23 - until epoch: 165, best_acc1: 73.990%
2022-07-08 16:42:23 - epoch 166 lr: 0.007740
2022-07-08 16:43:01 - train: epoch 0166, iter [00100, 05004], lr: 0.007732, loss: 0.7555
2022-07-08 16:43:36 - train: epoch 0166, iter [00200, 05004], lr: 0.007723, loss: 0.8648
2022-07-08 16:44:09 - train: epoch 0166, iter [00300, 05004], lr: 0.007715, loss: 0.9818
2022-07-08 16:44:43 - train: epoch 0166, iter [00400, 05004], lr: 0.007706, loss: 1.0320
2022-07-08 16:45:17 - train: epoch 0166, iter [00500, 05004], lr: 0.007698, loss: 0.9971
2022-07-08 16:45:50 - train: epoch 0166, iter [00600, 05004], lr: 0.007689, loss: 0.8070
2022-07-08 16:46:23 - train: epoch 0166, iter [00700, 05004], lr: 0.007680, loss: 0.8516
2022-07-08 16:46:57 - train: epoch 0166, iter [00800, 05004], lr: 0.007672, loss: 1.0662
2022-07-08 16:47:30 - train: epoch 0166, iter [00900, 05004], lr: 0.007663, loss: 0.9353
2022-07-08 16:48:04 - train: epoch 0166, iter [01000, 05004], lr: 0.007655, loss: 0.9685
2022-07-08 16:48:37 - train: epoch 0166, iter [01100, 05004], lr: 0.007646, loss: 0.9724
2022-07-08 16:49:11 - train: epoch 0166, iter [01200, 05004], lr: 0.007638, loss: 0.8696
2022-07-08 16:49:44 - train: epoch 0166, iter [01300, 05004], lr: 0.007629, loss: 0.9042
2022-07-08 16:50:19 - train: epoch 0166, iter [01400, 05004], lr: 0.007620, loss: 0.8968
2022-07-08 16:50:52 - train: epoch 0166, iter [01500, 05004], lr: 0.007612, loss: 0.8708
2022-07-08 16:51:27 - train: epoch 0166, iter [01600, 05004], lr: 0.007603, loss: 1.0584
2022-07-08 16:52:00 - train: epoch 0166, iter [01700, 05004], lr: 0.007595, loss: 1.0936
2022-07-08 16:52:34 - train: epoch 0166, iter [01800, 05004], lr: 0.007586, loss: 0.8163
2022-07-08 16:53:08 - train: epoch 0166, iter [01900, 05004], lr: 0.007578, loss: 1.0461
2022-07-08 16:53:41 - train: epoch 0166, iter [02000, 05004], lr: 0.007569, loss: 0.8229
2022-07-08 16:54:15 - train: epoch 0166, iter [02100, 05004], lr: 0.007561, loss: 0.7353
2022-07-08 16:54:48 - train: epoch 0166, iter [02200, 05004], lr: 0.007552, loss: 0.9299
2022-07-08 16:55:22 - train: epoch 0166, iter [02300, 05004], lr: 0.007544, loss: 1.0377
2022-07-08 16:55:56 - train: epoch 0166, iter [02400, 05004], lr: 0.007535, loss: 1.1064
2022-07-08 16:56:29 - train: epoch 0166, iter [02500, 05004], lr: 0.007527, loss: 0.9753
2022-07-08 16:57:03 - train: epoch 0166, iter [02600, 05004], lr: 0.007518, loss: 0.8418
2022-07-08 16:57:36 - train: epoch 0166, iter [02700, 05004], lr: 0.007510, loss: 0.8145
2022-07-08 16:58:10 - train: epoch 0166, iter [02800, 05004], lr: 0.007501, loss: 0.9815
2022-07-08 16:58:44 - train: epoch 0166, iter [02900, 05004], lr: 0.007493, loss: 0.9900
2022-07-08 16:59:18 - train: epoch 0166, iter [03000, 05004], lr: 0.007484, loss: 1.1877
2022-07-08 16:59:51 - train: epoch 0166, iter [03100, 05004], lr: 0.007476, loss: 1.2017
2022-07-08 17:00:26 - train: epoch 0166, iter [03200, 05004], lr: 0.007467, loss: 0.8957
2022-07-08 17:00:59 - train: epoch 0166, iter [03300, 05004], lr: 0.007459, loss: 0.8673
2022-07-08 17:01:32 - train: epoch 0166, iter [03400, 05004], lr: 0.007451, loss: 0.8596
2022-07-08 17:02:06 - train: epoch 0166, iter [03500, 05004], lr: 0.007442, loss: 1.0762
2022-07-08 17:02:39 - train: epoch 0166, iter [03600, 05004], lr: 0.007434, loss: 0.8134
2022-07-08 17:03:12 - train: epoch 0166, iter [03700, 05004], lr: 0.007425, loss: 0.7797
2022-07-08 17:03:45 - train: epoch 0166, iter [03800, 05004], lr: 0.007417, loss: 0.8554
2022-07-08 17:04:19 - train: epoch 0166, iter [03900, 05004], lr: 0.007408, loss: 0.8359
2022-07-08 17:04:52 - train: epoch 0166, iter [04000, 05004], lr: 0.007400, loss: 1.0468
2022-07-08 17:05:26 - train: epoch 0166, iter [04100, 05004], lr: 0.007391, loss: 0.7872
2022-07-08 17:05:59 - train: epoch 0166, iter [04200, 05004], lr: 0.007383, loss: 1.0048
2022-07-08 17:06:33 - train: epoch 0166, iter [04300, 05004], lr: 0.007375, loss: 1.0212
2022-07-08 17:07:05 - train: epoch 0166, iter [04400, 05004], lr: 0.007366, loss: 0.8387
2022-07-08 17:07:40 - train: epoch 0166, iter [04500, 05004], lr: 0.007358, loss: 0.9813
2022-07-08 17:08:13 - train: epoch 0166, iter [04600, 05004], lr: 0.007349, loss: 1.0206
2022-07-08 17:08:47 - train: epoch 0166, iter [04700, 05004], lr: 0.007341, loss: 1.0437
2022-07-08 17:09:20 - train: epoch 0166, iter [04800, 05004], lr: 0.007333, loss: 0.8377
2022-07-08 17:09:54 - train: epoch 0166, iter [04900, 05004], lr: 0.007324, loss: 0.8208
2022-07-08 17:10:25 - train: epoch 0166, iter [05000, 05004], lr: 0.007316, loss: 1.2844
2022-07-08 17:10:27 - train: epoch 166, train_loss: 0.9416
2022-07-08 17:11:40 - eval: epoch: 166, acc1: 74.038%, acc5: 91.914%, test_loss: 1.0503, per_image_load_time: 2.419ms, per_image_inference_time: 0.419ms
2022-07-08 17:11:41 - until epoch: 166, best_acc1: 74.038%
2022-07-08 17:11:41 - epoch 167 lr: 0.007315
2022-07-08 17:12:20 - train: epoch 0167, iter [00100, 05004], lr: 0.007307, loss: 1.0272
2022-07-08 17:12:54 - train: epoch 0167, iter [00200, 05004], lr: 0.007299, loss: 0.8456
2022-07-08 17:13:27 - train: epoch 0167, iter [00300, 05004], lr: 0.007290, loss: 0.9162
2022-07-08 17:14:01 - train: epoch 0167, iter [00400, 05004], lr: 0.007282, loss: 0.7533
2022-07-08 17:14:36 - train: epoch 0167, iter [00500, 05004], lr: 0.007274, loss: 0.9434
2022-07-08 17:15:09 - train: epoch 0167, iter [00600, 05004], lr: 0.007265, loss: 1.0554
2022-07-08 17:15:42 - train: epoch 0167, iter [00700, 05004], lr: 0.007257, loss: 0.8699
2022-07-08 17:16:16 - train: epoch 0167, iter [00800, 05004], lr: 0.007249, loss: 1.1396
2022-07-08 17:16:50 - train: epoch 0167, iter [00900, 05004], lr: 0.007240, loss: 0.8115
2022-07-08 17:17:24 - train: epoch 0167, iter [01000, 05004], lr: 0.007232, loss: 0.7985
2022-07-08 17:17:58 - train: epoch 0167, iter [01100, 05004], lr: 0.007224, loss: 0.8279
2022-07-08 17:18:32 - train: epoch 0167, iter [01200, 05004], lr: 0.007215, loss: 0.8635
2022-07-08 17:19:05 - train: epoch 0167, iter [01300, 05004], lr: 0.007207, loss: 1.0433
2022-07-08 17:19:39 - train: epoch 0167, iter [01400, 05004], lr: 0.007199, loss: 0.7956
2022-07-08 17:20:13 - train: epoch 0167, iter [01500, 05004], lr: 0.007190, loss: 0.8998
2022-07-08 17:20:46 - train: epoch 0167, iter [01600, 05004], lr: 0.007182, loss: 0.9714
2022-07-08 17:21:21 - train: epoch 0167, iter [01700, 05004], lr: 0.007174, loss: 0.8528
2022-07-08 17:21:55 - train: epoch 0167, iter [01800, 05004], lr: 0.007165, loss: 1.0238
2022-07-08 17:22:28 - train: epoch 0167, iter [01900, 05004], lr: 0.007157, loss: 1.0014
2022-07-08 17:23:03 - train: epoch 0167, iter [02000, 05004], lr: 0.007149, loss: 1.0030
2022-07-08 17:23:36 - train: epoch 0167, iter [02100, 05004], lr: 0.007140, loss: 0.7925
2022-07-08 17:24:10 - train: epoch 0167, iter [02200, 05004], lr: 0.007132, loss: 0.8742
2022-07-08 17:24:42 - train: epoch 0167, iter [02300, 05004], lr: 0.007124, loss: 1.1091
2022-07-08 17:25:16 - train: epoch 0167, iter [02400, 05004], lr: 0.007116, loss: 0.8948
2022-07-08 17:25:50 - train: epoch 0167, iter [02500, 05004], lr: 0.007107, loss: 0.8790
2022-07-08 17:26:23 - train: epoch 0167, iter [02600, 05004], lr: 0.007099, loss: 0.9257
2022-07-08 17:26:57 - train: epoch 0167, iter [02700, 05004], lr: 0.007091, loss: 1.0105
2022-07-08 17:27:30 - train: epoch 0167, iter [02800, 05004], lr: 0.007082, loss: 0.9357
2022-07-08 17:28:04 - train: epoch 0167, iter [02900, 05004], lr: 0.007074, loss: 0.7853
2022-07-08 17:28:37 - train: epoch 0167, iter [03000, 05004], lr: 0.007066, loss: 0.7183
2022-07-08 17:29:11 - train: epoch 0167, iter [03100, 05004], lr: 0.007058, loss: 0.8535
2022-07-08 17:29:43 - train: epoch 0167, iter [03200, 05004], lr: 0.007049, loss: 0.8978
2022-07-08 17:30:17 - train: epoch 0167, iter [03300, 05004], lr: 0.007041, loss: 0.9708
2022-07-08 17:30:51 - train: epoch 0167, iter [03400, 05004], lr: 0.007033, loss: 0.9935
2022-07-08 17:31:24 - train: epoch 0167, iter [03500, 05004], lr: 0.007025, loss: 0.9723
2022-07-08 17:31:57 - train: epoch 0167, iter [03600, 05004], lr: 0.007017, loss: 0.8732
2022-07-08 17:32:31 - train: epoch 0167, iter [03700, 05004], lr: 0.007008, loss: 0.8933
2022-07-08 17:33:05 - train: epoch 0167, iter [03800, 05004], lr: 0.007000, loss: 1.0085
2022-07-08 17:33:39 - train: epoch 0167, iter [03900, 05004], lr: 0.006992, loss: 0.8835
2022-07-08 17:34:12 - train: epoch 0167, iter [04000, 05004], lr: 0.006984, loss: 0.9574
2022-07-08 17:34:46 - train: epoch 0167, iter [04100, 05004], lr: 0.006975, loss: 0.8849
2022-07-08 17:35:19 - train: epoch 0167, iter [04200, 05004], lr: 0.006967, loss: 0.9360
2022-07-08 17:35:53 - train: epoch 0167, iter [04300, 05004], lr: 0.006959, loss: 0.9355
2022-07-08 17:36:27 - train: epoch 0167, iter [04400, 05004], lr: 0.006951, loss: 0.7037
2022-07-08 17:37:00 - train: epoch 0167, iter [04500, 05004], lr: 0.006943, loss: 0.6889
2022-07-08 17:37:34 - train: epoch 0167, iter [04600, 05004], lr: 0.006935, loss: 0.9776
2022-07-08 17:38:07 - train: epoch 0167, iter [04700, 05004], lr: 0.006926, loss: 1.1110
2022-07-08 17:38:41 - train: epoch 0167, iter [04800, 05004], lr: 0.006918, loss: 0.8779
2022-07-08 17:39:14 - train: epoch 0167, iter [04900, 05004], lr: 0.006910, loss: 0.9555
2022-07-08 17:39:47 - train: epoch 0167, iter [05000, 05004], lr: 0.006902, loss: 0.8973
2022-07-08 17:39:48 - train: epoch 167, train_loss: 0.9239
2022-07-08 17:41:02 - eval: epoch: 167, acc1: 74.410%, acc5: 92.060%, test_loss: 1.0466, per_image_load_time: 1.335ms, per_image_inference_time: 0.472ms
2022-07-08 17:41:02 - until epoch: 167, best_acc1: 74.410%
2022-07-08 17:41:02 - epoch 168 lr: 0.006901
2022-07-08 17:41:41 - train: epoch 0168, iter [00100, 05004], lr: 0.006893, loss: 0.8047
2022-07-08 17:42:14 - train: epoch 0168, iter [00200, 05004], lr: 0.006885, loss: 0.9668
2022-07-08 17:42:46 - train: epoch 0168, iter [00300, 05004], lr: 0.006877, loss: 0.9855
2022-07-08 17:43:20 - train: epoch 0168, iter [00400, 05004], lr: 0.006869, loss: 0.7861
2022-07-08 17:43:53 - train: epoch 0168, iter [00500, 05004], lr: 0.006861, loss: 0.9039
2022-07-08 17:44:26 - train: epoch 0168, iter [00600, 05004], lr: 0.006853, loss: 0.9476
2022-07-08 17:44:59 - train: epoch 0168, iter [00700, 05004], lr: 0.006844, loss: 0.8402
2022-07-08 17:45:33 - train: epoch 0168, iter [00800, 05004], lr: 0.006836, loss: 1.2228
2022-07-08 17:46:06 - train: epoch 0168, iter [00900, 05004], lr: 0.006828, loss: 0.9222
2022-07-08 17:46:39 - train: epoch 0168, iter [01000, 05004], lr: 0.006820, loss: 0.8089
2022-07-08 17:47:13 - train: epoch 0168, iter [01100, 05004], lr: 0.006812, loss: 0.8402
2022-07-08 17:47:47 - train: epoch 0168, iter [01200, 05004], lr: 0.006804, loss: 0.9049
2022-07-08 17:48:20 - train: epoch 0168, iter [01300, 05004], lr: 0.006796, loss: 0.9118
2022-07-08 17:48:54 - train: epoch 0168, iter [01400, 05004], lr: 0.006788, loss: 0.8373
2022-07-08 17:49:27 - train: epoch 0168, iter [01500, 05004], lr: 0.006780, loss: 0.9358
2022-07-08 17:50:00 - train: epoch 0168, iter [01600, 05004], lr: 0.006772, loss: 0.9301
2022-07-08 17:50:34 - train: epoch 0168, iter [01700, 05004], lr: 0.006763, loss: 0.8688
2022-07-08 17:51:08 - train: epoch 0168, iter [01800, 05004], lr: 0.006755, loss: 0.8075
2022-07-08 17:51:41 - train: epoch 0168, iter [01900, 05004], lr: 0.006747, loss: 0.9409
2022-07-08 17:52:15 - train: epoch 0168, iter [02000, 05004], lr: 0.006739, loss: 0.7638
2022-07-08 17:52:49 - train: epoch 0168, iter [02100, 05004], lr: 0.006731, loss: 0.9460
2022-07-08 17:53:22 - train: epoch 0168, iter [02200, 05004], lr: 0.006723, loss: 0.9502
2022-07-08 17:53:55 - train: epoch 0168, iter [02300, 05004], lr: 0.006715, loss: 0.8787
2022-07-08 17:54:28 - train: epoch 0168, iter [02400, 05004], lr: 0.006707, loss: 1.0029
2022-07-08 17:55:01 - train: epoch 0168, iter [02500, 05004], lr: 0.006699, loss: 1.0169
2022-07-08 17:55:34 - train: epoch 0168, iter [02600, 05004], lr: 0.006691, loss: 0.8141
2022-07-08 17:56:07 - train: epoch 0168, iter [02700, 05004], lr: 0.006683, loss: 0.8222
2022-07-08 17:56:40 - train: epoch 0168, iter [02800, 05004], lr: 0.006675, loss: 0.9484
2022-07-08 17:57:13 - train: epoch 0168, iter [02900, 05004], lr: 0.006667, loss: 1.0024
2022-07-08 17:57:46 - train: epoch 0168, iter [03000, 05004], lr: 0.006659, loss: 0.7934
2022-07-08 17:58:20 - train: epoch 0168, iter [03100, 05004], lr: 0.006651, loss: 0.8851
2022-07-08 17:58:53 - train: epoch 0168, iter [03200, 05004], lr: 0.006643, loss: 0.8636
2022-07-08 17:59:26 - train: epoch 0168, iter [03300, 05004], lr: 0.006635, loss: 0.7949
2022-07-08 17:59:59 - train: epoch 0168, iter [03400, 05004], lr: 0.006627, loss: 1.0694
2022-07-08 18:00:32 - train: epoch 0168, iter [03500, 05004], lr: 0.006619, loss: 0.7774
2022-07-08 18:01:05 - train: epoch 0168, iter [03600, 05004], lr: 0.006611, loss: 0.9264
2022-07-08 18:01:38 - train: epoch 0168, iter [03700, 05004], lr: 0.006603, loss: 0.9074
2022-07-08 18:02:11 - train: epoch 0168, iter [03800, 05004], lr: 0.006595, loss: 0.9929
2022-07-08 18:02:45 - train: epoch 0168, iter [03900, 05004], lr: 0.006587, loss: 0.9597
2022-07-08 18:03:18 - train: epoch 0168, iter [04000, 05004], lr: 0.006579, loss: 1.0296
2022-07-08 18:03:51 - train: epoch 0168, iter [04100, 05004], lr: 0.006571, loss: 1.1004
2022-07-08 18:04:24 - train: epoch 0168, iter [04200, 05004], lr: 0.006563, loss: 0.9153
2022-07-08 18:04:57 - train: epoch 0168, iter [04300, 05004], lr: 0.006555, loss: 0.8624
2022-07-08 18:05:30 - train: epoch 0168, iter [04400, 05004], lr: 0.006547, loss: 1.0490
2022-07-08 18:06:04 - train: epoch 0168, iter [04500, 05004], lr: 0.006539, loss: 1.0801
2022-07-08 18:06:38 - train: epoch 0168, iter [04600, 05004], lr: 0.006531, loss: 0.9467
2022-07-08 18:07:10 - train: epoch 0168, iter [04700, 05004], lr: 0.006523, loss: 0.8480
2022-07-08 18:07:44 - train: epoch 0168, iter [04800, 05004], lr: 0.006515, loss: 0.9492
2022-07-08 18:08:17 - train: epoch 0168, iter [04900, 05004], lr: 0.006507, loss: 0.8079
2022-07-08 18:08:49 - train: epoch 0168, iter [05000, 05004], lr: 0.006499, loss: 0.9028
2022-07-08 18:08:50 - train: epoch 168, train_loss: 0.9106
2022-07-08 18:10:03 - eval: epoch: 168, acc1: 74.660%, acc5: 92.258%, test_loss: 1.0291, per_image_load_time: 2.090ms, per_image_inference_time: 0.461ms
2022-07-08 18:10:04 - until epoch: 168, best_acc1: 74.660%
2022-07-08 18:10:04 - epoch 169 lr: 0.006499
2022-07-08 18:10:43 - train: epoch 0169, iter [00100, 05004], lr: 0.006491, loss: 0.9112
2022-07-08 18:11:15 - train: epoch 0169, iter [00200, 05004], lr: 0.006483, loss: 1.0021
2022-07-08 18:11:48 - train: epoch 0169, iter [00300, 05004], lr: 0.006475, loss: 0.9662
2022-07-08 18:12:21 - train: epoch 0169, iter [00400, 05004], lr: 0.006467, loss: 0.8581
2022-07-08 18:12:54 - train: epoch 0169, iter [00500, 05004], lr: 0.006459, loss: 0.9793
2022-07-08 18:13:27 - train: epoch 0169, iter [00600, 05004], lr: 0.006451, loss: 0.9130
2022-07-08 18:13:59 - train: epoch 0169, iter [00700, 05004], lr: 0.006443, loss: 0.9725
2022-07-08 18:14:32 - train: epoch 0169, iter [00800, 05004], lr: 0.006435, loss: 0.8338
2022-07-08 18:15:05 - train: epoch 0169, iter [00900, 05004], lr: 0.006428, loss: 1.0120
2022-07-08 18:15:38 - train: epoch 0169, iter [01000, 05004], lr: 0.006420, loss: 0.9382
2022-07-08 18:16:10 - train: epoch 0169, iter [01100, 05004], lr: 0.006412, loss: 0.9633
2022-07-08 18:16:44 - train: epoch 0169, iter [01200, 05004], lr: 0.006404, loss: 0.8031
2022-07-08 18:17:17 - train: epoch 0169, iter [01300, 05004], lr: 0.006396, loss: 0.9451
2022-07-08 18:17:50 - train: epoch 0169, iter [01400, 05004], lr: 0.006388, loss: 0.8601
2022-07-08 18:18:22 - train: epoch 0169, iter [01500, 05004], lr: 0.006380, loss: 0.6914
2022-07-08 18:18:55 - train: epoch 0169, iter [01600, 05004], lr: 0.006372, loss: 0.8427
2022-07-08 18:19:28 - train: epoch 0169, iter [01700, 05004], lr: 0.006364, loss: 0.8561
2022-07-08 18:20:02 - train: epoch 0169, iter [01800, 05004], lr: 0.006357, loss: 0.9534
2022-07-08 18:20:35 - train: epoch 0169, iter [01900, 05004], lr: 0.006349, loss: 0.9567
2022-07-08 18:21:08 - train: epoch 0169, iter [02000, 05004], lr: 0.006341, loss: 0.8907
2022-07-08 18:21:40 - train: epoch 0169, iter [02100, 05004], lr: 0.006333, loss: 0.8550
2022-07-08 18:22:13 - train: epoch 0169, iter [02200, 05004], lr: 0.006325, loss: 0.9661
2022-07-08 18:22:47 - train: epoch 0169, iter [02300, 05004], lr: 0.006317, loss: 0.9802
2022-07-08 18:23:19 - train: epoch 0169, iter [02400, 05004], lr: 0.006310, loss: 0.6920
2022-07-08 18:23:53 - train: epoch 0169, iter [02500, 05004], lr: 0.006302, loss: 0.8832
2022-07-08 18:24:25 - train: epoch 0169, iter [02600, 05004], lr: 0.006294, loss: 0.8112
2022-07-08 18:24:59 - train: epoch 0169, iter [02700, 05004], lr: 0.006286, loss: 0.7993
2022-07-08 18:25:33 - train: epoch 0169, iter [02800, 05004], lr: 0.006278, loss: 0.9160
2022-07-08 18:26:06 - train: epoch 0169, iter [02900, 05004], lr: 0.006270, loss: 0.9321
2022-07-08 18:26:40 - train: epoch 0169, iter [03000, 05004], lr: 0.006263, loss: 0.8617
2022-07-08 18:27:13 - train: epoch 0169, iter [03100, 05004], lr: 0.006255, loss: 0.7559
2022-07-08 18:27:47 - train: epoch 0169, iter [03200, 05004], lr: 0.006247, loss: 0.9542
2022-07-08 18:28:20 - train: epoch 0169, iter [03300, 05004], lr: 0.006239, loss: 0.6805
2022-07-08 18:28:53 - train: epoch 0169, iter [03400, 05004], lr: 0.006232, loss: 0.8344
2022-07-08 18:29:27 - train: epoch 0169, iter [03500, 05004], lr: 0.006224, loss: 0.8480
2022-07-08 18:29:59 - train: epoch 0169, iter [03600, 05004], lr: 0.006216, loss: 0.8964
2022-07-08 18:30:33 - train: epoch 0169, iter [03700, 05004], lr: 0.006208, loss: 0.9638
2022-07-08 18:31:06 - train: epoch 0169, iter [03800, 05004], lr: 0.006200, loss: 0.9967
2022-07-08 18:31:40 - train: epoch 0169, iter [03900, 05004], lr: 0.006193, loss: 0.7541
2022-07-08 18:32:13 - train: epoch 0169, iter [04000, 05004], lr: 0.006185, loss: 0.7431
2022-07-08 18:32:47 - train: epoch 0169, iter [04100, 05004], lr: 0.006177, loss: 0.7856
2022-07-08 18:33:20 - train: epoch 0169, iter [04200, 05004], lr: 0.006169, loss: 1.0176
2022-07-08 18:33:53 - train: epoch 0169, iter [04300, 05004], lr: 0.006162, loss: 0.7667
2022-07-08 18:34:27 - train: epoch 0169, iter [04400, 05004], lr: 0.006154, loss: 0.9410
2022-07-08 18:35:00 - train: epoch 0169, iter [04500, 05004], lr: 0.006146, loss: 1.0337
2022-07-08 18:35:34 - train: epoch 0169, iter [04600, 05004], lr: 0.006138, loss: 0.8426
2022-07-08 18:36:08 - train: epoch 0169, iter [04700, 05004], lr: 0.006131, loss: 0.7372
2022-07-08 18:36:41 - train: epoch 0169, iter [04800, 05004], lr: 0.006123, loss: 0.9224
2022-07-08 18:37:15 - train: epoch 0169, iter [04900, 05004], lr: 0.006115, loss: 0.8406
2022-07-08 18:37:47 - train: epoch 0169, iter [05000, 05004], lr: 0.006108, loss: 0.9210
2022-07-08 18:37:48 - train: epoch 169, train_loss: 0.8975
2022-07-08 18:39:02 - eval: epoch: 169, acc1: 74.704%, acc5: 92.254%, test_loss: 1.0226, per_image_load_time: 2.220ms, per_image_inference_time: 0.489ms
2022-07-08 18:39:03 - until epoch: 169, best_acc1: 74.704%
2022-07-08 18:39:03 - epoch 170 lr: 0.006107
2022-07-08 18:39:41 - train: epoch 0170, iter [00100, 05004], lr: 0.006100, loss: 0.9235
2022-07-08 18:40:15 - train: epoch 0170, iter [00200, 05004], lr: 0.006092, loss: 0.8852
2022-07-08 18:40:49 - train: epoch 0170, iter [00300, 05004], lr: 0.006084, loss: 0.8999
2022-07-08 18:41:22 - train: epoch 0170, iter [00400, 05004], lr: 0.006076, loss: 0.9127
2022-07-08 18:41:55 - train: epoch 0170, iter [00500, 05004], lr: 0.006069, loss: 0.9726
2022-07-08 18:42:28 - train: epoch 0170, iter [00600, 05004], lr: 0.006061, loss: 0.9753
2022-07-08 18:43:02 - train: epoch 0170, iter [00700, 05004], lr: 0.006053, loss: 0.7408
2022-07-08 18:43:35 - train: epoch 0170, iter [00800, 05004], lr: 0.006046, loss: 0.9322
2022-07-08 18:44:09 - train: epoch 0170, iter [00900, 05004], lr: 0.006038, loss: 0.9057
2022-07-08 18:44:42 - train: epoch 0170, iter [01000, 05004], lr: 0.006030, loss: 0.8241
2022-07-08 18:45:15 - train: epoch 0170, iter [01100, 05004], lr: 0.006023, loss: 0.8164
2022-07-08 18:45:49 - train: epoch 0170, iter [01200, 05004], lr: 0.006015, loss: 0.8554
2022-07-08 18:46:22 - train: epoch 0170, iter [01300, 05004], lr: 0.006007, loss: 0.9256
2022-07-08 18:46:56 - train: epoch 0170, iter [01400, 05004], lr: 0.006000, loss: 0.9589
2022-07-08 18:47:30 - train: epoch 0170, iter [01500, 05004], lr: 0.005992, loss: 0.8604
2022-07-08 18:48:04 - train: epoch 0170, iter [01600, 05004], lr: 0.005985, loss: 0.9420
2022-07-08 18:48:37 - train: epoch 0170, iter [01700, 05004], lr: 0.005977, loss: 1.0545
2022-07-08 18:49:10 - train: epoch 0170, iter [01800, 05004], lr: 0.005969, loss: 0.6401
2022-07-08 18:49:45 - train: epoch 0170, iter [01900, 05004], lr: 0.005962, loss: 1.1292
2022-07-08 18:50:18 - train: epoch 0170, iter [02000, 05004], lr: 0.005954, loss: 0.8281
2022-07-08 18:50:52 - train: epoch 0170, iter [02100, 05004], lr: 0.005946, loss: 0.7533
2022-07-08 18:51:25 - train: epoch 0170, iter [02200, 05004], lr: 0.005939, loss: 0.7725
2022-07-08 18:51:59 - train: epoch 0170, iter [02300, 05004], lr: 0.005931, loss: 0.9071
2022-07-08 18:52:33 - train: epoch 0170, iter [02400, 05004], lr: 0.005924, loss: 1.1309
2022-07-08 18:53:06 - train: epoch 0170, iter [02500, 05004], lr: 0.005916, loss: 0.8048
2022-07-08 18:53:41 - train: epoch 0170, iter [02600, 05004], lr: 0.005908, loss: 0.9876
2022-07-08 18:54:14 - train: epoch 0170, iter [02700, 05004], lr: 0.005901, loss: 0.8593
2022-07-08 18:54:47 - train: epoch 0170, iter [02800, 05004], lr: 0.005893, loss: 0.8668
2022-07-08 18:55:21 - train: epoch 0170, iter [02900, 05004], lr: 0.005886, loss: 0.9031
2022-07-08 18:55:54 - train: epoch 0170, iter [03000, 05004], lr: 0.005878, loss: 0.7467
2022-07-08 18:56:28 - train: epoch 0170, iter [03100, 05004], lr: 0.005870, loss: 0.9358
2022-07-08 18:57:01 - train: epoch 0170, iter [03200, 05004], lr: 0.005863, loss: 0.8078
2022-07-08 18:57:35 - train: epoch 0170, iter [03300, 05004], lr: 0.005855, loss: 0.8487
2022-07-08 18:58:08 - train: epoch 0170, iter [03400, 05004], lr: 0.005848, loss: 1.0821
2022-07-08 18:58:42 - train: epoch 0170, iter [03500, 05004], lr: 0.005840, loss: 0.8640
2022-07-08 18:59:15 - train: epoch 0170, iter [03600, 05004], lr: 0.005833, loss: 0.7425
2022-07-08 18:59:49 - train: epoch 0170, iter [03700, 05004], lr: 0.005825, loss: 0.9600
2022-07-08 19:00:23 - train: epoch 0170, iter [03800, 05004], lr: 0.005818, loss: 1.0050
2022-07-08 19:00:56 - train: epoch 0170, iter [03900, 05004], lr: 0.005810, loss: 1.1094
2022-07-08 19:01:30 - train: epoch 0170, iter [04000, 05004], lr: 0.005803, loss: 0.7999
2022-07-08 19:02:03 - train: epoch 0170, iter [04100, 05004], lr: 0.005795, loss: 0.7894
2022-07-08 19:02:37 - train: epoch 0170, iter [04200, 05004], lr: 0.005787, loss: 0.9801
2022-07-08 19:03:11 - train: epoch 0170, iter [04300, 05004], lr: 0.005780, loss: 0.9427
2022-07-08 19:03:43 - train: epoch 0170, iter [04400, 05004], lr: 0.005772, loss: 0.8128
2022-07-08 19:04:17 - train: epoch 0170, iter [04500, 05004], lr: 0.005765, loss: 0.9266
2022-07-08 19:04:50 - train: epoch 0170, iter [04600, 05004], lr: 0.005757, loss: 0.7216
2022-07-08 19:05:23 - train: epoch 0170, iter [04700, 05004], lr: 0.005750, loss: 1.0073
2022-07-08 19:05:57 - train: epoch 0170, iter [04800, 05004], lr: 0.005742, loss: 0.9692
2022-07-08 19:06:32 - train: epoch 0170, iter [04900, 05004], lr: 0.005735, loss: 0.9822
2022-07-08 19:07:04 - train: epoch 0170, iter [05000, 05004], lr: 0.005727, loss: 0.8932
2022-07-08 19:07:05 - train: epoch 170, train_loss: 0.8825
2022-07-08 19:08:19 - eval: epoch: 170, acc1: 74.534%, acc5: 92.270%, test_loss: 1.0304, per_image_load_time: 1.752ms, per_image_inference_time: 0.450ms
2022-07-08 19:08:20 - until epoch: 170, best_acc1: 74.704%
2022-07-08 19:08:20 - epoch 171 lr: 0.005727
2022-07-08 19:08:58 - train: epoch 0171, iter [00100, 05004], lr: 0.005720, loss: 0.7543
2022-07-08 19:09:31 - train: epoch 0171, iter [00200, 05004], lr: 0.005712, loss: 0.9574
2022-07-08 19:10:06 - train: epoch 0171, iter [00300, 05004], lr: 0.005705, loss: 0.7465
2022-07-08 19:10:39 - train: epoch 0171, iter [00400, 05004], lr: 0.005697, loss: 0.9557
2022-07-08 19:11:13 - train: epoch 0171, iter [00500, 05004], lr: 0.005690, loss: 0.8190
2022-07-08 19:11:47 - train: epoch 0171, iter [00600, 05004], lr: 0.005682, loss: 0.9024
2022-07-08 19:12:20 - train: epoch 0171, iter [00700, 05004], lr: 0.005675, loss: 0.7728
2022-07-08 19:12:54 - train: epoch 0171, iter [00800, 05004], lr: 0.005667, loss: 0.8838
2022-07-08 19:13:26 - train: epoch 0171, iter [00900, 05004], lr: 0.005660, loss: 0.7283
2022-07-08 19:14:00 - train: epoch 0171, iter [01000, 05004], lr: 0.005653, loss: 0.7303
2022-07-08 19:14:34 - train: epoch 0171, iter [01100, 05004], lr: 0.005645, loss: 0.7930
2022-07-08 19:15:07 - train: epoch 0171, iter [01200, 05004], lr: 0.005638, loss: 1.0693
2022-07-08 19:15:41 - train: epoch 0171, iter [01300, 05004], lr: 0.005630, loss: 0.8628
2022-07-08 19:16:14 - train: epoch 0171, iter [01400, 05004], lr: 0.005623, loss: 0.8718
2022-07-08 19:16:48 - train: epoch 0171, iter [01500, 05004], lr: 0.005615, loss: 0.7084
2022-07-08 19:17:22 - train: epoch 0171, iter [01600, 05004], lr: 0.005608, loss: 1.0031
2022-07-08 19:17:55 - train: epoch 0171, iter [01700, 05004], lr: 0.005601, loss: 0.7921
2022-07-08 19:18:29 - train: epoch 0171, iter [01800, 05004], lr: 0.005593, loss: 0.8808
2022-07-08 19:19:01 - train: epoch 0171, iter [01900, 05004], lr: 0.005586, loss: 0.8272
2022-07-08 19:19:35 - train: epoch 0171, iter [02000, 05004], lr: 0.005578, loss: 0.7218
2022-07-08 19:20:08 - train: epoch 0171, iter [02100, 05004], lr: 0.005571, loss: 0.9212
2022-07-08 19:20:42 - train: epoch 0171, iter [02200, 05004], lr: 0.005564, loss: 0.7756
2022-07-08 19:21:16 - train: epoch 0171, iter [02300, 05004], lr: 0.005556, loss: 0.9476
2022-07-08 19:21:49 - train: epoch 0171, iter [02400, 05004], lr: 0.005549, loss: 0.8956
2022-07-08 19:22:22 - train: epoch 0171, iter [02500, 05004], lr: 0.005542, loss: 0.7727
2022-07-08 19:22:55 - train: epoch 0171, iter [02600, 05004], lr: 0.005534, loss: 0.7457
2022-07-08 19:23:28 - train: epoch 0171, iter [02700, 05004], lr: 0.005527, loss: 0.8904
2022-07-08 19:24:01 - train: epoch 0171, iter [02800, 05004], lr: 0.005520, loss: 0.7891
2022-07-08 19:24:34 - train: epoch 0171, iter [02900, 05004], lr: 0.005512, loss: 0.8536
2022-07-08 19:25:08 - train: epoch 0171, iter [03000, 05004], lr: 0.005505, loss: 1.0944
2022-07-08 19:25:40 - train: epoch 0171, iter [03100, 05004], lr: 0.005497, loss: 0.9124
2022-07-08 19:26:15 - train: epoch 0171, iter [03200, 05004], lr: 0.005490, loss: 0.8575
2022-07-08 19:26:48 - train: epoch 0171, iter [03300, 05004], lr: 0.005483, loss: 0.8649
2022-07-08 19:27:22 - train: epoch 0171, iter [03400, 05004], lr: 0.005476, loss: 0.8435
2022-07-08 19:27:55 - train: epoch 0171, iter [03500, 05004], lr: 0.005468, loss: 0.9164
2022-07-08 19:28:28 - train: epoch 0171, iter [03600, 05004], lr: 0.005461, loss: 1.0761
2022-07-08 19:29:03 - train: epoch 0171, iter [03700, 05004], lr: 0.005454, loss: 1.1018
2022-07-08 19:29:36 - train: epoch 0171, iter [03800, 05004], lr: 0.005446, loss: 0.7353
2022-07-08 19:30:09 - train: epoch 0171, iter [03900, 05004], lr: 0.005439, loss: 0.8826
2022-07-08 19:30:44 - train: epoch 0171, iter [04000, 05004], lr: 0.005432, loss: 0.7462
2022-07-08 19:31:18 - train: epoch 0171, iter [04100, 05004], lr: 0.005424, loss: 0.9257
2022-07-08 19:31:51 - train: epoch 0171, iter [04200, 05004], lr: 0.005417, loss: 0.8158
2022-07-08 19:32:24 - train: epoch 0171, iter [04300, 05004], lr: 0.005410, loss: 1.0413
2022-07-08 19:32:58 - train: epoch 0171, iter [04400, 05004], lr: 0.005402, loss: 0.9066
2022-07-08 19:33:31 - train: epoch 0171, iter [04500, 05004], lr: 0.005395, loss: 1.0373
2022-07-08 19:34:04 - train: epoch 0171, iter [04600, 05004], lr: 0.005388, loss: 0.7190
2022-07-08 19:34:37 - train: epoch 0171, iter [04700, 05004], lr: 0.005381, loss: 1.0027
2022-07-08 19:35:12 - train: epoch 0171, iter [04800, 05004], lr: 0.005373, loss: 0.8556
2022-07-08 19:35:45 - train: epoch 0171, iter [04900, 05004], lr: 0.005366, loss: 0.8690
2022-07-08 19:36:17 - train: epoch 0171, iter [05000, 05004], lr: 0.005359, loss: 1.0393
2022-07-08 19:36:18 - train: epoch 171, train_loss: 0.8705
2022-07-08 19:37:32 - eval: epoch: 171, acc1: 74.140%, acc5: 92.160%, test_loss: 1.0457, per_image_load_time: 2.425ms, per_image_inference_time: 0.473ms
2022-07-08 19:37:32 - until epoch: 171, best_acc1: 74.704%
2022-07-08 19:37:32 - epoch 172 lr: 0.005359
2022-07-08 19:38:11 - train: epoch 0172, iter [00100, 05004], lr: 0.005351, loss: 0.9404
2022-07-08 19:38:45 - train: epoch 0172, iter [00200, 05004], lr: 0.005344, loss: 0.8535
2022-07-08 19:39:20 - train: epoch 0172, iter [00300, 05004], lr: 0.005337, loss: 0.9890
2022-07-08 19:39:53 - train: epoch 0172, iter [00400, 05004], lr: 0.005330, loss: 0.8288
2022-07-08 19:40:26 - train: epoch 0172, iter [00500, 05004], lr: 0.005322, loss: 0.9782
2022-07-08 19:40:59 - train: epoch 0172, iter [00600, 05004], lr: 0.005315, loss: 0.8601
2022-07-08 19:41:33 - train: epoch 0172, iter [00700, 05004], lr: 0.005308, loss: 0.5990
2022-07-08 19:42:06 - train: epoch 0172, iter [00800, 05004], lr: 0.005301, loss: 0.8579
2022-07-08 19:42:40 - train: epoch 0172, iter [00900, 05004], lr: 0.005294, loss: 0.7422
2022-07-08 19:43:13 - train: epoch 0172, iter [01000, 05004], lr: 0.005286, loss: 0.8060
2022-07-08 19:43:46 - train: epoch 0172, iter [01100, 05004], lr: 0.005279, loss: 0.8934
2022-07-08 19:44:19 - train: epoch 0172, iter [01200, 05004], lr: 0.005272, loss: 1.0597
2022-07-08 19:44:53 - train: epoch 0172, iter [01300, 05004], lr: 0.005265, loss: 0.8026
2022-07-08 19:45:27 - train: epoch 0172, iter [01400, 05004], lr: 0.005258, loss: 0.8256
2022-07-08 19:46:01 - train: epoch 0172, iter [01500, 05004], lr: 0.005250, loss: 0.8097
2022-07-08 19:46:35 - train: epoch 0172, iter [01600, 05004], lr: 0.005243, loss: 0.9487
2022-07-08 19:47:08 - train: epoch 0172, iter [01700, 05004], lr: 0.005236, loss: 0.8925
2022-07-08 19:47:43 - train: epoch 0172, iter [01800, 05004], lr: 0.005229, loss: 1.0468
2022-07-08 19:48:16 - train: epoch 0172, iter [01900, 05004], lr: 0.005222, loss: 0.7266
2022-07-08 19:48:50 - train: epoch 0172, iter [02000, 05004], lr: 0.005215, loss: 0.8802
2022-07-08 19:49:24 - train: epoch 0172, iter [02100, 05004], lr: 0.005207, loss: 0.8162
2022-07-08 19:49:57 - train: epoch 0172, iter [02200, 05004], lr: 0.005200, loss: 0.8481
2022-07-08 19:50:31 - train: epoch 0172, iter [02300, 05004], lr: 0.005193, loss: 0.8873
2022-07-08 19:51:05 - train: epoch 0172, iter [02400, 05004], lr: 0.005186, loss: 1.0501
2022-07-08 19:51:38 - train: epoch 0172, iter [02500, 05004], lr: 0.005179, loss: 0.8362
2022-07-08 19:52:12 - train: epoch 0172, iter [02600, 05004], lr: 0.005172, loss: 0.8736
2022-07-08 19:52:45 - train: epoch 0172, iter [02700, 05004], lr: 0.005165, loss: 0.7144
2022-07-08 19:53:19 - train: epoch 0172, iter [02800, 05004], lr: 0.005157, loss: 0.8459
2022-07-08 19:53:52 - train: epoch 0172, iter [02900, 05004], lr: 0.005150, loss: 0.8211
2022-07-08 19:54:26 - train: epoch 0172, iter [03000, 05004], lr: 0.005143, loss: 0.7663
2022-07-08 19:54:59 - train: epoch 0172, iter [03100, 05004], lr: 0.005136, loss: 0.9938
2022-07-08 19:55:33 - train: epoch 0172, iter [03200, 05004], lr: 0.005129, loss: 0.8253
2022-07-08 19:56:06 - train: epoch 0172, iter [03300, 05004], lr: 0.005122, loss: 1.0439
2022-07-08 19:56:40 - train: epoch 0172, iter [03400, 05004], lr: 0.005115, loss: 0.9059
2022-07-08 19:57:13 - train: epoch 0172, iter [03500, 05004], lr: 0.005108, loss: 0.7419
2022-07-08 19:57:46 - train: epoch 0172, iter [03600, 05004], lr: 0.005101, loss: 0.8747
2022-07-08 19:58:20 - train: epoch 0172, iter [03700, 05004], lr: 0.005094, loss: 0.7463
2022-07-08 19:58:53 - train: epoch 0172, iter [03800, 05004], lr: 0.005086, loss: 0.7412
2022-07-08 19:59:26 - train: epoch 0172, iter [03900, 05004], lr: 0.005079, loss: 0.7489
2022-07-08 19:59:59 - train: epoch 0172, iter [04000, 05004], lr: 0.005072, loss: 0.9350
2022-07-08 20:00:33 - train: epoch 0172, iter [04100, 05004], lr: 0.005065, loss: 1.0603
2022-07-08 20:01:06 - train: epoch 0172, iter [04200, 05004], lr: 0.005058, loss: 0.9270
2022-07-08 20:01:39 - train: epoch 0172, iter [04300, 05004], lr: 0.005051, loss: 0.6810
2022-07-08 20:02:13 - train: epoch 0172, iter [04400, 05004], lr: 0.005044, loss: 0.9219
2022-07-08 20:02:46 - train: epoch 0172, iter [04500, 05004], lr: 0.005037, loss: 0.9547
2022-07-08 20:03:18 - train: epoch 0172, iter [04600, 05004], lr: 0.005030, loss: 0.8960
2022-07-08 20:03:53 - train: epoch 0172, iter [04700, 05004], lr: 0.005023, loss: 0.9611
2022-07-08 20:04:25 - train: epoch 0172, iter [04800, 05004], lr: 0.005016, loss: 0.8067
2022-07-08 20:04:59 - train: epoch 0172, iter [04900, 05004], lr: 0.005009, loss: 0.9453
2022-07-08 20:05:31 - train: epoch 0172, iter [05000, 05004], lr: 0.005002, loss: 0.8221
2022-07-08 20:05:32 - train: epoch 172, train_loss: 0.8532
2022-07-08 20:06:47 - eval: epoch: 172, acc1: 74.884%, acc5: 92.428%, test_loss: 1.0247, per_image_load_time: 2.380ms, per_image_inference_time: 0.426ms
2022-07-08 20:06:47 - until epoch: 172, best_acc1: 74.884%
2022-07-08 20:06:47 - epoch 173 lr: 0.005002
2022-07-08 20:07:25 - train: epoch 0173, iter [00100, 05004], lr: 0.004995, loss: 0.9180
2022-07-08 20:07:59 - train: epoch 0173, iter [00200, 05004], lr: 0.004988, loss: 0.9064
2022-07-08 20:08:32 - train: epoch 0173, iter [00300, 05004], lr: 0.004981, loss: 0.7899
2022-07-08 20:09:05 - train: epoch 0173, iter [00400, 05004], lr: 0.004974, loss: 0.9015
2022-07-08 20:09:38 - train: epoch 0173, iter [00500, 05004], lr: 0.004967, loss: 0.9262
2022-07-08 20:10:11 - train: epoch 0173, iter [00600, 05004], lr: 0.004960, loss: 0.7729
2022-07-08 20:10:45 - train: epoch 0173, iter [00700, 05004], lr: 0.004953, loss: 0.7670
2022-07-08 20:11:17 - train: epoch 0173, iter [00800, 05004], lr: 0.004946, loss: 0.7693
2022-07-08 20:11:50 - train: epoch 0173, iter [00900, 05004], lr: 0.004939, loss: 0.8584
2022-07-08 20:12:24 - train: epoch 0173, iter [01000, 05004], lr: 0.004932, loss: 1.0060
2022-07-08 20:12:56 - train: epoch 0173, iter [01100, 05004], lr: 0.004925, loss: 0.8413
2022-07-08 20:13:30 - train: epoch 0173, iter [01200, 05004], lr: 0.004918, loss: 0.8387
2022-07-08 20:14:03 - train: epoch 0173, iter [01300, 05004], lr: 0.004911, loss: 0.8913
2022-07-08 20:14:37 - train: epoch 0173, iter [01400, 05004], lr: 0.004904, loss: 1.0253
2022-07-08 20:15:10 - train: epoch 0173, iter [01500, 05004], lr: 0.004897, loss: 0.6938
2022-07-08 20:15:43 - train: epoch 0173, iter [01600, 05004], lr: 0.004890, loss: 0.8809
2022-07-08 20:16:18 - train: epoch 0173, iter [01700, 05004], lr: 0.004883, loss: 0.7918
2022-07-08 20:16:51 - train: epoch 0173, iter [01800, 05004], lr: 0.004876, loss: 0.7440
2022-07-08 20:17:24 - train: epoch 0173, iter [01900, 05004], lr: 0.004869, loss: 0.8882
2022-07-08 20:17:57 - train: epoch 0173, iter [02000, 05004], lr: 0.004862, loss: 0.9946
2022-07-08 20:18:30 - train: epoch 0173, iter [02100, 05004], lr: 0.004855, loss: 0.7312
2022-07-08 20:19:04 - train: epoch 0173, iter [02200, 05004], lr: 0.004848, loss: 0.8778
2022-07-08 20:19:37 - train: epoch 0173, iter [02300, 05004], lr: 0.004841, loss: 0.9284
2022-07-08 20:20:10 - train: epoch 0173, iter [02400, 05004], lr: 0.004835, loss: 0.7817
2022-07-08 20:20:43 - train: epoch 0173, iter [02500, 05004], lr: 0.004828, loss: 0.7772
2022-07-08 20:21:16 - train: epoch 0173, iter [02600, 05004], lr: 0.004821, loss: 0.9815
2022-07-08 20:21:50 - train: epoch 0173, iter [02700, 05004], lr: 0.004814, loss: 0.8337
2022-07-08 20:22:22 - train: epoch 0173, iter [02800, 05004], lr: 0.004807, loss: 0.9038
2022-07-08 20:22:55 - train: epoch 0173, iter [02900, 05004], lr: 0.004800, loss: 0.9646
2022-07-08 20:23:29 - train: epoch 0173, iter [03000, 05004], lr: 0.004793, loss: 0.9252
2022-07-08 20:24:03 - train: epoch 0173, iter [03100, 05004], lr: 0.004786, loss: 0.8589
2022-07-08 20:24:36 - train: epoch 0173, iter [03200, 05004], lr: 0.004779, loss: 0.8336
2022-07-08 20:25:09 - train: epoch 0173, iter [03300, 05004], lr: 0.004773, loss: 0.8689
2022-07-08 20:25:41 - train: epoch 0173, iter [03400, 05004], lr: 0.004766, loss: 0.8336
2022-07-08 20:26:14 - train: epoch 0173, iter [03500, 05004], lr: 0.004759, loss: 0.9082
2022-07-08 20:26:48 - train: epoch 0173, iter [03600, 05004], lr: 0.004752, loss: 0.8158
2022-07-08 20:27:22 - train: epoch 0173, iter [03700, 05004], lr: 0.004745, loss: 0.8962
2022-07-08 20:27:55 - train: epoch 0173, iter [03800, 05004], lr: 0.004738, loss: 0.7871
2022-07-08 20:28:28 - train: epoch 0173, iter [03900, 05004], lr: 0.004731, loss: 0.6355
2022-07-08 20:29:01 - train: epoch 0173, iter [04000, 05004], lr: 0.004725, loss: 0.6189
2022-07-08 20:29:34 - train: epoch 0173, iter [04100, 05004], lr: 0.004718, loss: 0.9248
2022-07-08 20:30:07 - train: epoch 0173, iter [04200, 05004], lr: 0.004711, loss: 0.8505
2022-07-08 20:30:40 - train: epoch 0173, iter [04300, 05004], lr: 0.004704, loss: 0.8890
2022-07-08 20:31:14 - train: epoch 0173, iter [04400, 05004], lr: 0.004697, loss: 0.7726
2022-07-08 20:31:47 - train: epoch 0173, iter [04500, 05004], lr: 0.004691, loss: 0.8737
2022-07-08 20:32:20 - train: epoch 0173, iter [04600, 05004], lr: 0.004684, loss: 0.8457
2022-07-08 20:32:53 - train: epoch 0173, iter [04700, 05004], lr: 0.004677, loss: 0.9300
2022-07-08 20:33:27 - train: epoch 0173, iter [04800, 05004], lr: 0.004670, loss: 0.8769
2022-07-08 20:34:01 - train: epoch 0173, iter [04900, 05004], lr: 0.004663, loss: 0.8894
2022-07-08 20:34:33 - train: epoch 0173, iter [05000, 05004], lr: 0.004657, loss: 0.8865
2022-07-08 20:34:34 - train: epoch 173, train_loss: 0.8405
2022-07-08 20:35:48 - eval: epoch: 173, acc1: 74.868%, acc5: 92.354%, test_loss: 1.0128, per_image_load_time: 2.297ms, per_image_inference_time: 0.450ms
2022-07-08 20:35:48 - until epoch: 173, best_acc1: 74.884%
2022-07-08 20:35:48 - epoch 174 lr: 0.004656
2022-07-08 20:36:27 - train: epoch 0174, iter [00100, 05004], lr: 0.004650, loss: 0.6890
2022-07-08 20:37:00 - train: epoch 0174, iter [00200, 05004], lr: 0.004643, loss: 0.7821
2022-07-08 20:37:33 - train: epoch 0174, iter [00300, 05004], lr: 0.004636, loss: 0.8163
2022-07-08 20:38:05 - train: epoch 0174, iter [00400, 05004], lr: 0.004629, loss: 0.7639
2022-07-08 20:38:38 - train: epoch 0174, iter [00500, 05004], lr: 0.004622, loss: 0.7986
2022-07-08 20:39:11 - train: epoch 0174, iter [00600, 05004], lr: 0.004616, loss: 0.8329
2022-07-08 20:39:44 - train: epoch 0174, iter [00700, 05004], lr: 0.004609, loss: 0.7607
2022-07-08 20:40:17 - train: epoch 0174, iter [00800, 05004], lr: 0.004602, loss: 0.8933
2022-07-08 20:40:50 - train: epoch 0174, iter [00900, 05004], lr: 0.004595, loss: 0.8056
2022-07-08 20:41:23 - train: epoch 0174, iter [01000, 05004], lr: 0.004589, loss: 0.7348
2022-07-08 20:41:57 - train: epoch 0174, iter [01100, 05004], lr: 0.004582, loss: 0.7848
2022-07-08 20:42:29 - train: epoch 0174, iter [01200, 05004], lr: 0.004575, loss: 0.8790
2022-07-08 20:43:02 - train: epoch 0174, iter [01300, 05004], lr: 0.004568, loss: 0.7946
2022-07-08 20:43:36 - train: epoch 0174, iter [01400, 05004], lr: 0.004562, loss: 0.8049
2022-07-08 20:44:09 - train: epoch 0174, iter [01500, 05004], lr: 0.004555, loss: 0.7567
2022-07-08 20:44:42 - train: epoch 0174, iter [01600, 05004], lr: 0.004548, loss: 0.7518
2022-07-08 20:45:16 - train: epoch 0174, iter [01700, 05004], lr: 0.004542, loss: 0.7657
2022-07-08 20:45:49 - train: epoch 0174, iter [01800, 05004], lr: 0.004535, loss: 0.7672
2022-07-08 20:46:22 - train: epoch 0174, iter [01900, 05004], lr: 0.004528, loss: 0.8400
2022-07-08 20:46:54 - train: epoch 0174, iter [02000, 05004], lr: 0.004522, loss: 0.9142
2022-07-08 20:47:29 - train: epoch 0174, iter [02100, 05004], lr: 0.004515, loss: 0.7855
2022-07-08 20:48:02 - train: epoch 0174, iter [02200, 05004], lr: 0.004508, loss: 0.8282
2022-07-08 20:48:35 - train: epoch 0174, iter [02300, 05004], lr: 0.004502, loss: 0.9824
2022-07-08 20:49:09 - train: epoch 0174, iter [02400, 05004], lr: 0.004495, loss: 0.8019
2022-07-08 20:49:41 - train: epoch 0174, iter [02500, 05004], lr: 0.004488, loss: 0.8551
2022-07-08 20:50:16 - train: epoch 0174, iter [02600, 05004], lr: 0.004481, loss: 0.7017
2022-07-08 20:50:49 - train: epoch 0174, iter [02700, 05004], lr: 0.004475, loss: 0.6061
2022-07-08 20:51:23 - train: epoch 0174, iter [02800, 05004], lr: 0.004468, loss: 0.8676
2022-07-08 20:51:56 - train: epoch 0174, iter [02900, 05004], lr: 0.004462, loss: 0.8735
2022-07-08 20:52:30 - train: epoch 0174, iter [03000, 05004], lr: 0.004455, loss: 0.6894
2022-07-08 20:53:03 - train: epoch 0174, iter [03100, 05004], lr: 0.004448, loss: 0.9715
2022-07-08 20:53:37 - train: epoch 0174, iter [03200, 05004], lr: 0.004442, loss: 0.7988
2022-07-08 20:54:11 - train: epoch 0174, iter [03300, 05004], lr: 0.004435, loss: 0.8390
2022-07-08 20:54:44 - train: epoch 0174, iter [03400, 05004], lr: 0.004428, loss: 0.7204
2022-07-08 20:55:18 - train: epoch 0174, iter [03500, 05004], lr: 0.004422, loss: 0.8383
2022-07-08 20:55:50 - train: epoch 0174, iter [03600, 05004], lr: 0.004415, loss: 0.9250
2022-07-08 20:56:24 - train: epoch 0174, iter [03700, 05004], lr: 0.004409, loss: 0.7878
2022-07-08 20:56:59 - train: epoch 0174, iter [03800, 05004], lr: 0.004402, loss: 0.8358
2022-07-08 20:57:32 - train: epoch 0174, iter [03900, 05004], lr: 0.004395, loss: 0.8383
2022-07-08 20:58:05 - train: epoch 0174, iter [04000, 05004], lr: 0.004389, loss: 0.8756
2022-07-08 20:58:39 - train: epoch 0174, iter [04100, 05004], lr: 0.004382, loss: 0.8017
2022-07-08 20:59:12 - train: epoch 0174, iter [04200, 05004], lr: 0.004376, loss: 0.8140
2022-07-08 20:59:46 - train: epoch 0174, iter [04300, 05004], lr: 0.004369, loss: 0.9177
2022-07-08 21:00:19 - train: epoch 0174, iter [04400, 05004], lr: 0.004362, loss: 0.8466
2022-07-08 21:00:53 - train: epoch 0174, iter [04500, 05004], lr: 0.004356, loss: 0.9532
2022-07-08 21:01:26 - train: epoch 0174, iter [04600, 05004], lr: 0.004349, loss: 0.6839
2022-07-08 21:02:00 - train: epoch 0174, iter [04700, 05004], lr: 0.004343, loss: 1.1484
2022-07-08 21:02:33 - train: epoch 0174, iter [04800, 05004], lr: 0.004336, loss: 1.1107
2022-07-08 21:03:06 - train: epoch 0174, iter [04900, 05004], lr: 0.004330, loss: 0.7375
2022-07-08 21:03:39 - train: epoch 0174, iter [05000, 05004], lr: 0.004323, loss: 0.8954
2022-07-08 21:03:40 - train: epoch 174, train_loss: 0.8242
2022-07-08 21:04:54 - eval: epoch: 174, acc1: 75.172%, acc5: 92.552%, test_loss: 1.0099, per_image_load_time: 0.747ms, per_image_inference_time: 0.483ms
2022-07-08 21:04:54 - until epoch: 174, best_acc1: 75.172%
2022-07-08 21:04:54 - epoch 175 lr: 0.004323
2022-07-08 21:05:32 - train: epoch 0175, iter [00100, 05004], lr: 0.004316, loss: 0.9011
2022-07-08 21:06:05 - train: epoch 0175, iter [00200, 05004], lr: 0.004310, loss: 0.7970
2022-07-08 21:06:38 - train: epoch 0175, iter [00300, 05004], lr: 0.004303, loss: 0.7251
2022-07-08 21:07:11 - train: epoch 0175, iter [00400, 05004], lr: 0.004297, loss: 0.7601
2022-07-08 21:07:44 - train: epoch 0175, iter [00500, 05004], lr: 0.004290, loss: 0.7257
2022-07-08 21:08:17 - train: epoch 0175, iter [00600, 05004], lr: 0.004284, loss: 0.7229
2022-07-08 21:08:50 - train: epoch 0175, iter [00700, 05004], lr: 0.004277, loss: 0.9571
2022-07-08 21:09:22 - train: epoch 0175, iter [00800, 05004], lr: 0.004270, loss: 0.9637
2022-07-08 21:09:56 - train: epoch 0175, iter [00900, 05004], lr: 0.004264, loss: 0.9570
2022-07-08 21:10:29 - train: epoch 0175, iter [01000, 05004], lr: 0.004257, loss: 0.9227
2022-07-08 21:11:02 - train: epoch 0175, iter [01100, 05004], lr: 0.004251, loss: 0.5839
2022-07-08 21:11:35 - train: epoch 0175, iter [01200, 05004], lr: 0.004244, loss: 0.9221
2022-07-08 21:12:08 - train: epoch 0175, iter [01300, 05004], lr: 0.004238, loss: 0.8117
2022-07-08 21:12:42 - train: epoch 0175, iter [01400, 05004], lr: 0.004232, loss: 0.9423
2022-07-08 21:13:15 - train: epoch 0175, iter [01500, 05004], lr: 0.004225, loss: 0.8678
2022-07-08 21:13:48 - train: epoch 0175, iter [01600, 05004], lr: 0.004219, loss: 0.6967
2022-07-08 21:14:21 - train: epoch 0175, iter [01700, 05004], lr: 0.004212, loss: 0.8322
2022-07-08 21:14:54 - train: epoch 0175, iter [01800, 05004], lr: 0.004206, loss: 0.9383
2022-07-08 21:15:28 - train: epoch 0175, iter [01900, 05004], lr: 0.004199, loss: 0.7640
2022-07-08 21:16:01 - train: epoch 0175, iter [02000, 05004], lr: 0.004193, loss: 0.7157
2022-07-08 21:16:34 - train: epoch 0175, iter [02100, 05004], lr: 0.004186, loss: 0.7882
2022-07-08 21:17:08 - train: epoch 0175, iter [02200, 05004], lr: 0.004180, loss: 0.7667
2022-07-08 21:17:41 - train: epoch 0175, iter [02300, 05004], lr: 0.004173, loss: 0.8352
2022-07-08 21:18:13 - train: epoch 0175, iter [02400, 05004], lr: 0.004167, loss: 0.8107
2022-07-08 21:18:46 - train: epoch 0175, iter [02500, 05004], lr: 0.004161, loss: 0.8269
2022-07-08 21:19:20 - train: epoch 0175, iter [02600, 05004], lr: 0.004154, loss: 0.8236
2022-07-08 21:19:53 - train: epoch 0175, iter [02700, 05004], lr: 0.004148, loss: 0.8337
2022-07-08 21:20:26 - train: epoch 0175, iter [02800, 05004], lr: 0.004141, loss: 0.7334
2022-07-08 21:20:59 - train: epoch 0175, iter [02900, 05004], lr: 0.004135, loss: 0.7796
2022-07-08 21:21:33 - train: epoch 0175, iter [03000, 05004], lr: 0.004128, loss: 0.7231
2022-07-08 21:22:05 - train: epoch 0175, iter [03100, 05004], lr: 0.004122, loss: 0.7271
2022-07-08 21:22:39 - train: epoch 0175, iter [03200, 05004], lr: 0.004116, loss: 0.7600
2022-07-08 21:23:13 - train: epoch 0175, iter [03300, 05004], lr: 0.004109, loss: 0.7133
2022-07-08 21:23:46 - train: epoch 0175, iter [03400, 05004], lr: 0.004103, loss: 0.9875
2022-07-08 21:24:20 - train: epoch 0175, iter [03500, 05004], lr: 0.004096, loss: 0.6374
2022-07-08 21:24:53 - train: epoch 0175, iter [03600, 05004], lr: 0.004090, loss: 0.8062
2022-07-08 21:25:25 - train: epoch 0175, iter [03700, 05004], lr: 0.004084, loss: 0.9029
2022-07-08 21:25:59 - train: epoch 0175, iter [03800, 05004], lr: 0.004077, loss: 0.7144
2022-07-08 21:26:33 - train: epoch 0175, iter [03900, 05004], lr: 0.004071, loss: 0.9129
2022-07-08 21:27:06 - train: epoch 0175, iter [04000, 05004], lr: 0.004065, loss: 0.7390
2022-07-08 21:27:39 - train: epoch 0175, iter [04100, 05004], lr: 0.004058, loss: 0.9330
2022-07-08 21:28:13 - train: epoch 0175, iter [04200, 05004], lr: 0.004052, loss: 0.7060
2022-07-08 21:28:46 - train: epoch 0175, iter [04300, 05004], lr: 0.004046, loss: 0.7646
2022-07-08 21:29:19 - train: epoch 0175, iter [04400, 05004], lr: 0.004039, loss: 0.7374
2022-07-08 21:29:53 - train: epoch 0175, iter [04500, 05004], lr: 0.004033, loss: 0.8539
2022-07-08 21:30:26 - train: epoch 0175, iter [04600, 05004], lr: 0.004027, loss: 0.8314
2022-07-08 21:30:59 - train: epoch 0175, iter [04700, 05004], lr: 0.004020, loss: 0.8987
2022-07-08 21:31:33 - train: epoch 0175, iter [04800, 05004], lr: 0.004014, loss: 0.9985
2022-07-08 21:32:06 - train: epoch 0175, iter [04900, 05004], lr: 0.004008, loss: 0.9644
2022-07-08 21:32:38 - train: epoch 0175, iter [05000, 05004], lr: 0.004001, loss: 0.6331
2022-07-08 21:32:39 - train: epoch 175, train_loss: 0.8105
2022-07-08 21:33:53 - eval: epoch: 175, acc1: 75.570%, acc5: 92.750%, test_loss: 1.0005, per_image_load_time: 2.375ms, per_image_inference_time: 0.477ms
2022-07-08 21:33:53 - until epoch: 175, best_acc1: 75.570%
2022-07-08 21:33:53 - epoch 176 lr: 0.004001
2022-07-08 21:34:32 - train: epoch 0176, iter [00100, 05004], lr: 0.003995, loss: 0.7585
2022-07-08 21:35:05 - train: epoch 0176, iter [00200, 05004], lr: 0.003988, loss: 0.6488
2022-07-08 21:35:38 - train: epoch 0176, iter [00300, 05004], lr: 0.003982, loss: 0.8720
2022-07-08 21:36:11 - train: epoch 0176, iter [00400, 05004], lr: 0.003976, loss: 0.6789
2022-07-08 21:36:44 - train: epoch 0176, iter [00500, 05004], lr: 0.003970, loss: 0.8139
2022-07-08 21:37:17 - train: epoch 0176, iter [00600, 05004], lr: 0.003963, loss: 0.8226
2022-07-08 21:37:50 - train: epoch 0176, iter [00700, 05004], lr: 0.003957, loss: 0.7611
2022-07-08 21:38:23 - train: epoch 0176, iter [00800, 05004], lr: 0.003951, loss: 1.0049
2022-07-08 21:38:57 - train: epoch 0176, iter [00900, 05004], lr: 0.003944, loss: 0.8283
2022-07-08 21:39:30 - train: epoch 0176, iter [01000, 05004], lr: 0.003938, loss: 0.7034
2022-07-08 21:40:03 - train: epoch 0176, iter [01100, 05004], lr: 0.003932, loss: 0.6822
2022-07-08 21:40:36 - train: epoch 0176, iter [01200, 05004], lr: 0.003926, loss: 0.8327
2022-07-08 21:41:09 - train: epoch 0176, iter [01300, 05004], lr: 0.003919, loss: 0.8014
2022-07-08 21:41:43 - train: epoch 0176, iter [01400, 05004], lr: 0.003913, loss: 0.8486
2022-07-08 21:42:16 - train: epoch 0176, iter [01500, 05004], lr: 0.003907, loss: 0.6712
2022-07-08 21:42:48 - train: epoch 0176, iter [01600, 05004], lr: 0.003901, loss: 0.7718
2022-07-08 21:43:22 - train: epoch 0176, iter [01700, 05004], lr: 0.003894, loss: 0.8354
2022-07-08 21:43:56 - train: epoch 0176, iter [01800, 05004], lr: 0.003888, loss: 0.9008
2022-07-08 21:44:29 - train: epoch 0176, iter [01900, 05004], lr: 0.003882, loss: 0.6916
2022-07-08 21:45:02 - train: epoch 0176, iter [02000, 05004], lr: 0.003876, loss: 0.7130
2022-07-08 21:45:35 - train: epoch 0176, iter [02100, 05004], lr: 0.003870, loss: 0.7938
2022-07-08 21:46:09 - train: epoch 0176, iter [02200, 05004], lr: 0.003863, loss: 0.7474
2022-07-08 21:46:42 - train: epoch 0176, iter [02300, 05004], lr: 0.003857, loss: 0.6161
2022-07-08 21:47:16 - train: epoch 0176, iter [02400, 05004], lr: 0.003851, loss: 0.7139
2022-07-08 21:47:49 - train: epoch 0176, iter [02500, 05004], lr: 0.003845, loss: 0.8683
2022-07-08 21:48:23 - train: epoch 0176, iter [02600, 05004], lr: 0.003839, loss: 0.6476
2022-07-08 21:48:56 - train: epoch 0176, iter [02700, 05004], lr: 0.003832, loss: 0.8319
2022-07-08 21:49:30 - train: epoch 0176, iter [02800, 05004], lr: 0.003826, loss: 0.7846
2022-07-08 21:50:03 - train: epoch 0176, iter [02900, 05004], lr: 0.003820, loss: 0.8176
2022-07-08 21:50:37 - train: epoch 0176, iter [03000, 05004], lr: 0.003814, loss: 0.7792
2022-07-08 21:51:10 - train: epoch 0176, iter [03100, 05004], lr: 0.003808, loss: 0.7352
2022-07-08 21:51:44 - train: epoch 0176, iter [03200, 05004], lr: 0.003802, loss: 0.7553
2022-07-08 21:52:18 - train: epoch 0176, iter [03300, 05004], lr: 0.003795, loss: 0.8710
2022-07-08 21:52:51 - train: epoch 0176, iter [03400, 05004], lr: 0.003789, loss: 0.9962
2022-07-08 21:53:25 - train: epoch 0176, iter [03500, 05004], lr: 0.003783, loss: 0.7991
2022-07-08 21:53:58 - train: epoch 0176, iter [03600, 05004], lr: 0.003777, loss: 0.9341
2022-07-08 21:54:31 - train: epoch 0176, iter [03700, 05004], lr: 0.003771, loss: 0.7163
2022-07-08 21:55:04 - train: epoch 0176, iter [03800, 05004], lr: 0.003765, loss: 0.7739
2022-07-08 21:55:37 - train: epoch 0176, iter [03900, 05004], lr: 0.003759, loss: 0.8327
2022-07-08 21:56:11 - train: epoch 0176, iter [04000, 05004], lr: 0.003752, loss: 0.9544
2022-07-08 21:56:45 - train: epoch 0176, iter [04100, 05004], lr: 0.003746, loss: 0.6881
2022-07-08 21:57:18 - train: epoch 0176, iter [04200, 05004], lr: 0.003740, loss: 0.8223
2022-07-08 21:57:52 - train: epoch 0176, iter [04300, 05004], lr: 0.003734, loss: 0.8435
2022-07-08 21:58:25 - train: epoch 0176, iter [04400, 05004], lr: 0.003728, loss: 0.6965
2022-07-08 21:58:58 - train: epoch 0176, iter [04500, 05004], lr: 0.003722, loss: 0.8764
2022-07-08 21:59:32 - train: epoch 0176, iter [04600, 05004], lr: 0.003716, loss: 0.8084
2022-07-08 22:00:04 - train: epoch 0176, iter [04700, 05004], lr: 0.003710, loss: 0.8751
2022-07-08 22:00:39 - train: epoch 0176, iter [04800, 05004], lr: 0.003704, loss: 0.7750
2022-07-08 22:01:12 - train: epoch 0176, iter [04900, 05004], lr: 0.003698, loss: 0.8027
2022-07-08 22:01:44 - train: epoch 0176, iter [05000, 05004], lr: 0.003692, loss: 0.9282
2022-07-08 22:01:45 - train: epoch 176, train_loss: 0.7944
2022-07-08 22:03:01 - eval: epoch: 176, acc1: 75.186%, acc5: 92.566%, test_loss: 1.0176, per_image_load_time: 1.925ms, per_image_inference_time: 0.482ms
2022-07-08 22:03:01 - until epoch: 176, best_acc1: 75.570%
2022-07-08 22:03:01 - epoch 177 lr: 0.003691
2022-07-08 22:03:40 - train: epoch 0177, iter [00100, 05004], lr: 0.003685, loss: 0.6781
2022-07-08 22:04:14 - train: epoch 0177, iter [00200, 05004], lr: 0.003679, loss: 0.7609
2022-07-08 22:04:48 - train: epoch 0177, iter [00300, 05004], lr: 0.003673, loss: 1.0366
2022-07-08 22:05:21 - train: epoch 0177, iter [00400, 05004], lr: 0.003667, loss: 1.0304
2022-07-08 22:05:55 - train: epoch 0177, iter [00500, 05004], lr: 0.003661, loss: 0.8516
2022-07-08 22:06:29 - train: epoch 0177, iter [00600, 05004], lr: 0.003655, loss: 0.7105
2022-07-08 22:07:03 - train: epoch 0177, iter [00700, 05004], lr: 0.003649, loss: 0.7506
2022-07-08 22:07:36 - train: epoch 0177, iter [00800, 05004], lr: 0.003643, loss: 0.7687
2022-07-08 22:08:09 - train: epoch 0177, iter [00900, 05004], lr: 0.003637, loss: 0.6723
2022-07-08 22:08:42 - train: epoch 0177, iter [01000, 05004], lr: 0.003631, loss: 0.8627
2022-07-08 22:09:16 - train: epoch 0177, iter [01100, 05004], lr: 0.003625, loss: 0.7315
2022-07-08 22:09:49 - train: epoch 0177, iter [01200, 05004], lr: 0.003619, loss: 0.7344
2022-07-08 22:10:23 - train: epoch 0177, iter [01300, 05004], lr: 0.003613, loss: 0.7847
2022-07-08 22:10:56 - train: epoch 0177, iter [01400, 05004], lr: 0.003607, loss: 0.9419
2022-07-08 22:11:29 - train: epoch 0177, iter [01500, 05004], lr: 0.003601, loss: 0.8832
2022-07-08 22:12:03 - train: epoch 0177, iter [01600, 05004], lr: 0.003595, loss: 0.8252
2022-07-08 22:12:36 - train: epoch 0177, iter [01700, 05004], lr: 0.003589, loss: 0.7982
2022-07-08 22:13:10 - train: epoch 0177, iter [01800, 05004], lr: 0.003583, loss: 0.8987
2022-07-08 22:13:43 - train: epoch 0177, iter [01900, 05004], lr: 0.003577, loss: 0.6841
2022-07-08 22:14:16 - train: epoch 0177, iter [02000, 05004], lr: 0.003571, loss: 0.6283
2022-07-08 22:14:50 - train: epoch 0177, iter [02100, 05004], lr: 0.003565, loss: 0.8681
2022-07-08 22:15:23 - train: epoch 0177, iter [02200, 05004], lr: 0.003559, loss: 0.9046
2022-07-08 22:15:56 - train: epoch 0177, iter [02300, 05004], lr: 0.003553, loss: 0.7893
2022-07-08 22:16:29 - train: epoch 0177, iter [02400, 05004], lr: 0.003547, loss: 0.8792
2022-07-08 22:17:02 - train: epoch 0177, iter [02500, 05004], lr: 0.003541, loss: 0.7981
2022-07-08 22:17:36 - train: epoch 0177, iter [02600, 05004], lr: 0.003535, loss: 0.6761
2022-07-08 22:18:09 - train: epoch 0177, iter [02700, 05004], lr: 0.003529, loss: 0.6024
2022-07-08 22:18:42 - train: epoch 0177, iter [02800, 05004], lr: 0.003523, loss: 0.5838
2022-07-08 22:19:16 - train: epoch 0177, iter [02900, 05004], lr: 0.003517, loss: 0.8420
2022-07-08 22:19:49 - train: epoch 0177, iter [03000, 05004], lr: 0.003511, loss: 0.7313
2022-07-08 22:20:22 - train: epoch 0177, iter [03100, 05004], lr: 0.003505, loss: 0.7099
2022-07-08 22:20:55 - train: epoch 0177, iter [03200, 05004], lr: 0.003499, loss: 0.7718
2022-07-08 22:21:29 - train: epoch 0177, iter [03300, 05004], lr: 0.003494, loss: 0.7726
2022-07-08 22:22:01 - train: epoch 0177, iter [03400, 05004], lr: 0.003488, loss: 0.8344
2022-07-08 22:22:35 - train: epoch 0177, iter [03500, 05004], lr: 0.003482, loss: 0.6043
2022-07-08 22:23:08 - train: epoch 0177, iter [03600, 05004], lr: 0.003476, loss: 0.8635
2022-07-08 22:23:41 - train: epoch 0177, iter [03700, 05004], lr: 0.003470, loss: 0.7702
2022-07-08 22:24:14 - train: epoch 0177, iter [03800, 05004], lr: 0.003464, loss: 0.7577
2022-07-08 22:24:48 - train: epoch 0177, iter [03900, 05004], lr: 0.003458, loss: 0.6024
2022-07-08 22:25:21 - train: epoch 0177, iter [04000, 05004], lr: 0.003452, loss: 0.6948
2022-07-08 22:25:54 - train: epoch 0177, iter [04100, 05004], lr: 0.003446, loss: 0.7461
2022-07-08 22:26:28 - train: epoch 0177, iter [04200, 05004], lr: 0.003441, loss: 0.9537
2022-07-08 22:27:01 - train: epoch 0177, iter [04300, 05004], lr: 0.003435, loss: 0.9919
2022-07-08 22:27:34 - train: epoch 0177, iter [04400, 05004], lr: 0.003429, loss: 0.6925
2022-07-08 22:28:08 - train: epoch 0177, iter [04500, 05004], lr: 0.003423, loss: 0.7475
2022-07-08 22:28:41 - train: epoch 0177, iter [04600, 05004], lr: 0.003417, loss: 0.8533
2022-07-08 22:29:14 - train: epoch 0177, iter [04700, 05004], lr: 0.003411, loss: 0.6357
2022-07-08 22:29:47 - train: epoch 0177, iter [04800, 05004], lr: 0.003405, loss: 0.8602
2022-07-08 22:30:21 - train: epoch 0177, iter [04900, 05004], lr: 0.003400, loss: 0.8555
2022-07-08 22:30:53 - train: epoch 0177, iter [05000, 05004], lr: 0.003394, loss: 0.9287
2022-07-08 22:30:54 - train: epoch 177, train_loss: 0.7813
2022-07-08 22:32:08 - eval: epoch: 177, acc1: 75.684%, acc5: 92.770%, test_loss: 0.9948, per_image_load_time: 1.013ms, per_image_inference_time: 0.473ms
2022-07-08 22:32:09 - until epoch: 177, best_acc1: 75.684%
2022-07-08 22:32:09 - epoch 178 lr: 0.003393
2022-07-08 22:32:47 - train: epoch 0178, iter [00100, 05004], lr: 0.003388, loss: 0.7197
2022-07-08 22:33:21 - train: epoch 0178, iter [00200, 05004], lr: 0.003382, loss: 0.8318
2022-07-08 22:33:55 - train: epoch 0178, iter [00300, 05004], lr: 0.003376, loss: 0.8775
2022-07-08 22:34:28 - train: epoch 0178, iter [00400, 05004], lr: 0.003370, loss: 0.7260
2022-07-08 22:35:00 - train: epoch 0178, iter [00500, 05004], lr: 0.003364, loss: 0.6417
2022-07-08 22:35:34 - train: epoch 0178, iter [00600, 05004], lr: 0.003359, loss: 0.7040
2022-07-08 22:36:06 - train: epoch 0178, iter [00700, 05004], lr: 0.003353, loss: 0.6227
2022-07-08 22:36:39 - train: epoch 0178, iter [00800, 05004], lr: 0.003347, loss: 0.7940
2022-07-08 22:37:12 - train: epoch 0178, iter [00900, 05004], lr: 0.003341, loss: 0.8110
2022-07-08 22:37:45 - train: epoch 0178, iter [01000, 05004], lr: 0.003335, loss: 0.7418
2022-07-08 22:38:18 - train: epoch 0178, iter [01100, 05004], lr: 0.003330, loss: 0.7241
2022-07-08 22:38:51 - train: epoch 0178, iter [01200, 05004], lr: 0.003324, loss: 0.7566
2022-07-08 22:39:24 - train: epoch 0178, iter [01300, 05004], lr: 0.003318, loss: 0.7619
2022-07-08 22:39:57 - train: epoch 0178, iter [01400, 05004], lr: 0.003312, loss: 0.7364
2022-07-08 22:40:30 - train: epoch 0178, iter [01500, 05004], lr: 0.003307, loss: 0.8001
2022-07-08 22:41:02 - train: epoch 0178, iter [01600, 05004], lr: 0.003301, loss: 0.7058
2022-07-08 22:41:35 - train: epoch 0178, iter [01700, 05004], lr: 0.003295, loss: 0.7710
2022-07-08 22:42:08 - train: epoch 0178, iter [01800, 05004], lr: 0.003289, loss: 0.7578
2022-07-08 22:42:42 - train: epoch 0178, iter [01900, 05004], lr: 0.003284, loss: 0.7883
2022-07-08 22:43:15 - train: epoch 0178, iter [02000, 05004], lr: 0.003278, loss: 0.7326
2022-07-08 22:43:48 - train: epoch 0178, iter [02100, 05004], lr: 0.003272, loss: 0.6466
2022-07-08 22:44:21 - train: epoch 0178, iter [02200, 05004], lr: 0.003266, loss: 0.7518
2022-07-08 22:44:54 - train: epoch 0178, iter [02300, 05004], lr: 0.003261, loss: 0.8470
2022-07-08 22:45:27 - train: epoch 0178, iter [02400, 05004], lr: 0.003255, loss: 0.8024
2022-07-08 22:46:00 - train: epoch 0178, iter [02500, 05004], lr: 0.003249, loss: 0.5705
2022-07-08 22:46:33 - train: epoch 0178, iter [02600, 05004], lr: 0.003244, loss: 0.8441
2022-07-08 22:47:07 - train: epoch 0178, iter [02700, 05004], lr: 0.003238, loss: 0.8091
2022-07-08 22:47:39 - train: epoch 0178, iter [02800, 05004], lr: 0.003232, loss: 0.6031
2022-07-08 22:48:13 - train: epoch 0178, iter [02900, 05004], lr: 0.003227, loss: 0.8011
2022-07-08 22:48:46 - train: epoch 0178, iter [03000, 05004], lr: 0.003221, loss: 0.7820
2022-07-08 22:49:19 - train: epoch 0178, iter [03100, 05004], lr: 0.003215, loss: 0.6706
2022-07-08 22:49:52 - train: epoch 0178, iter [03200, 05004], lr: 0.003209, loss: 0.8385
2022-07-08 22:50:26 - train: epoch 0178, iter [03300, 05004], lr: 0.003204, loss: 0.6851
2022-07-08 22:50:58 - train: epoch 0178, iter [03400, 05004], lr: 0.003198, loss: 0.8024
2022-07-08 22:51:32 - train: epoch 0178, iter [03500, 05004], lr: 0.003192, loss: 0.8945
2022-07-08 22:52:05 - train: epoch 0178, iter [03600, 05004], lr: 0.003187, loss: 0.8074
2022-07-08 22:52:38 - train: epoch 0178, iter [03700, 05004], lr: 0.003181, loss: 0.7365
2022-07-08 22:53:11 - train: epoch 0178, iter [03800, 05004], lr: 0.003176, loss: 0.9100
2022-07-08 22:53:44 - train: epoch 0178, iter [03900, 05004], lr: 0.003170, loss: 0.9190
2022-07-08 22:54:18 - train: epoch 0178, iter [04000, 05004], lr: 0.003164, loss: 0.8082
2022-07-08 22:54:50 - train: epoch 0178, iter [04100, 05004], lr: 0.003159, loss: 0.7865
2022-07-08 22:55:24 - train: epoch 0178, iter [04200, 05004], lr: 0.003153, loss: 0.7914
2022-07-08 22:55:57 - train: epoch 0178, iter [04300, 05004], lr: 0.003147, loss: 0.8377
2022-07-08 22:56:31 - train: epoch 0178, iter [04400, 05004], lr: 0.003142, loss: 0.6608
2022-07-08 22:57:03 - train: epoch 0178, iter [04500, 05004], lr: 0.003136, loss: 0.8476
2022-07-08 22:57:36 - train: epoch 0178, iter [04600, 05004], lr: 0.003130, loss: 0.7729
2022-07-08 22:58:09 - train: epoch 0178, iter [04700, 05004], lr: 0.003125, loss: 0.6088
2022-07-08 22:58:43 - train: epoch 0178, iter [04800, 05004], lr: 0.003119, loss: 0.7072
2022-07-08 22:59:16 - train: epoch 0178, iter [04900, 05004], lr: 0.003114, loss: 0.9108
2022-07-08 22:59:48 - train: epoch 0178, iter [05000, 05004], lr: 0.003108, loss: 0.7667
2022-07-08 22:59:49 - train: epoch 178, train_loss: 0.7650
2022-07-08 23:01:04 - eval: epoch: 178, acc1: 75.492%, acc5: 92.666%, test_loss: 0.9929, per_image_load_time: 2.057ms, per_image_inference_time: 0.472ms
2022-07-08 23:01:04 - until epoch: 178, best_acc1: 75.684%
2022-07-08 23:01:04 - epoch 179 lr: 0.003108
2022-07-08 23:01:43 - train: epoch 0179, iter [00100, 05004], lr: 0.003102, loss: 0.8174
2022-07-08 23:02:16 - train: epoch 0179, iter [00200, 05004], lr: 0.003097, loss: 0.7153
2022-07-08 23:02:50 - train: epoch 0179, iter [00300, 05004], lr: 0.003091, loss: 0.9382
2022-07-08 23:03:23 - train: epoch 0179, iter [00400, 05004], lr: 0.003086, loss: 0.7654
2022-07-08 23:03:56 - train: epoch 0179, iter [00500, 05004], lr: 0.003080, loss: 0.8250
2022-07-08 23:04:29 - train: epoch 0179, iter [00600, 05004], lr: 0.003074, loss: 0.8282
2022-07-08 23:05:03 - train: epoch 0179, iter [00700, 05004], lr: 0.003069, loss: 0.6528
2022-07-08 23:05:36 - train: epoch 0179, iter [00800, 05004], lr: 0.003063, loss: 0.6283
2022-07-08 23:06:09 - train: epoch 0179, iter [00900, 05004], lr: 0.003058, loss: 0.6567
2022-07-08 23:06:43 - train: epoch 0179, iter [01000, 05004], lr: 0.003052, loss: 0.7044
2022-07-08 23:07:15 - train: epoch 0179, iter [01100, 05004], lr: 0.003047, loss: 0.6216
2022-07-08 23:07:49 - train: epoch 0179, iter [01200, 05004], lr: 0.003041, loss: 0.8912
2022-07-08 23:08:22 - train: epoch 0179, iter [01300, 05004], lr: 0.003036, loss: 0.6955
2022-07-08 23:08:56 - train: epoch 0179, iter [01400, 05004], lr: 0.003030, loss: 0.6774
2022-07-08 23:09:29 - train: epoch 0179, iter [01500, 05004], lr: 0.003025, loss: 0.8151
2022-07-08 23:10:02 - train: epoch 0179, iter [01600, 05004], lr: 0.003019, loss: 0.5833
2022-07-08 23:10:35 - train: epoch 0179, iter [01700, 05004], lr: 0.003014, loss: 0.8375
2022-07-08 23:11:09 - train: epoch 0179, iter [01800, 05004], lr: 0.003008, loss: 0.7542
2022-07-08 23:11:42 - train: epoch 0179, iter [01900, 05004], lr: 0.003003, loss: 0.6715
2022-07-08 23:12:16 - train: epoch 0179, iter [02000, 05004], lr: 0.002997, loss: 0.7532
2022-07-08 23:12:50 - train: epoch 0179, iter [02100, 05004], lr: 0.002992, loss: 0.8937
2022-07-08 23:13:23 - train: epoch 0179, iter [02200, 05004], lr: 0.002986, loss: 0.8496
2022-07-08 23:13:57 - train: epoch 0179, iter [02300, 05004], lr: 0.002981, loss: 0.5899
2022-07-08 23:14:30 - train: epoch 0179, iter [02400, 05004], lr: 0.002975, loss: 0.7431
2022-07-08 23:15:04 - train: epoch 0179, iter [02500, 05004], lr: 0.002970, loss: 0.8442
2022-07-08 23:15:37 - train: epoch 0179, iter [02600, 05004], lr: 0.002964, loss: 0.6968
2022-07-08 23:16:11 - train: epoch 0179, iter [02700, 05004], lr: 0.002959, loss: 0.9106
2022-07-08 23:16:44 - train: epoch 0179, iter [02800, 05004], lr: 0.002953, loss: 0.8401
2022-07-08 23:17:18 - train: epoch 0179, iter [02900, 05004], lr: 0.002948, loss: 0.9846
2022-07-08 23:17:52 - train: epoch 0179, iter [03000, 05004], lr: 0.002942, loss: 0.6466
2022-07-08 23:18:26 - train: epoch 0179, iter [03100, 05004], lr: 0.002937, loss: 0.7903
2022-07-08 23:18:59 - train: epoch 0179, iter [03200, 05004], lr: 0.002932, loss: 0.8830
2022-07-08 23:19:32 - train: epoch 0179, iter [03300, 05004], lr: 0.002926, loss: 0.8418
2022-07-08 23:20:06 - train: epoch 0179, iter [03400, 05004], lr: 0.002921, loss: 0.8008
2022-07-08 23:20:39 - train: epoch 0179, iter [03500, 05004], lr: 0.002915, loss: 0.8164
2022-07-08 23:21:13 - train: epoch 0179, iter [03600, 05004], lr: 0.002910, loss: 0.8422
2022-07-08 23:21:47 - train: epoch 0179, iter [03700, 05004], lr: 0.002904, loss: 0.5979
2022-07-08 23:22:21 - train: epoch 0179, iter [03800, 05004], lr: 0.002899, loss: 0.7148
2022-07-08 23:22:54 - train: epoch 0179, iter [03900, 05004], lr: 0.002894, loss: 0.8611
2022-07-08 23:23:27 - train: epoch 0179, iter [04000, 05004], lr: 0.002888, loss: 0.7732
2022-07-08 23:24:01 - train: epoch 0179, iter [04100, 05004], lr: 0.002883, loss: 0.8010
2022-07-08 23:24:35 - train: epoch 0179, iter [04200, 05004], lr: 0.002878, loss: 0.8674
2022-07-08 23:25:08 - train: epoch 0179, iter [04300, 05004], lr: 0.002872, loss: 0.6845
2022-07-08 23:25:42 - train: epoch 0179, iter [04400, 05004], lr: 0.002867, loss: 0.9171
2022-07-08 23:26:16 - train: epoch 0179, iter [04500, 05004], lr: 0.002861, loss: 0.8009
2022-07-08 23:26:49 - train: epoch 0179, iter [04600, 05004], lr: 0.002856, loss: 0.7078
2022-07-08 23:27:22 - train: epoch 0179, iter [04700, 05004], lr: 0.002851, loss: 0.7210
2022-07-08 23:27:55 - train: epoch 0179, iter [04800, 05004], lr: 0.002845, loss: 0.6860
2022-07-08 23:28:29 - train: epoch 0179, iter [04900, 05004], lr: 0.002840, loss: 0.7815
2022-07-08 23:29:01 - train: epoch 0179, iter [05000, 05004], lr: 0.002835, loss: 0.6710
2022-07-08 23:29:02 - train: epoch 179, train_loss: 0.7534
2022-07-08 23:30:16 - eval: epoch: 179, acc1: 75.578%, acc5: 92.874%, test_loss: 0.9939, per_image_load_time: 1.674ms, per_image_inference_time: 0.458ms
2022-07-08 23:30:17 - until epoch: 179, best_acc1: 75.684%
2022-07-08 23:38:50 - epoch 180 lr: 0.002834
2022-07-08 23:39:29 - train: epoch 0180, iter [00100, 05004], lr: 0.002829, loss: 0.8358
2022-07-08 23:40:02 - train: epoch 0180, iter [00200, 05004], lr: 0.002824, loss: 0.6613
2022-07-08 23:40:36 - train: epoch 0180, iter [00300, 05004], lr: 0.002818, loss: 0.7698
2022-07-08 23:41:09 - train: epoch 0180, iter [00400, 05004], lr: 0.002813, loss: 0.5635
2022-07-08 23:41:42 - train: epoch 0180, iter [00500, 05004], lr: 0.002808, loss: 0.6917
2022-07-08 23:42:14 - train: epoch 0180, iter [00600, 05004], lr: 0.002802, loss: 0.6853
2022-07-08 23:42:48 - train: epoch 0180, iter [00700, 05004], lr: 0.002797, loss: 0.6067
2022-07-08 23:43:20 - train: epoch 0180, iter [00800, 05004], lr: 0.002792, loss: 0.6082
2022-07-08 23:43:53 - train: epoch 0180, iter [00900, 05004], lr: 0.002787, loss: 0.7196
2022-07-08 23:44:26 - train: epoch 0180, iter [01000, 05004], lr: 0.002781, loss: 0.6397
2022-07-08 23:44:59 - train: epoch 0180, iter [01100, 05004], lr: 0.002776, loss: 0.8302
2022-07-08 23:45:32 - train: epoch 0180, iter [01200, 05004], lr: 0.002771, loss: 0.7668
2022-07-08 23:46:06 - train: epoch 0180, iter [01300, 05004], lr: 0.002765, loss: 0.7645
2022-07-08 23:46:38 - train: epoch 0180, iter [01400, 05004], lr: 0.002760, loss: 0.7375
2022-07-08 23:47:10 - train: epoch 0180, iter [01500, 05004], lr: 0.002755, loss: 0.8342
2022-07-08 23:47:43 - train: epoch 0180, iter [01600, 05004], lr: 0.002750, loss: 0.7977
2022-07-08 23:48:17 - train: epoch 0180, iter [01700, 05004], lr: 0.002744, loss: 0.7067
2022-07-08 23:48:50 - train: epoch 0180, iter [01800, 05004], lr: 0.002739, loss: 0.7049
2022-07-08 23:49:23 - train: epoch 0180, iter [01900, 05004], lr: 0.002734, loss: 0.5434
2022-07-08 23:49:57 - train: epoch 0180, iter [02000, 05004], lr: 0.002729, loss: 0.7914
2022-07-08 23:50:30 - train: epoch 0180, iter [02100, 05004], lr: 0.002723, loss: 0.7220
2022-07-08 23:51:03 - train: epoch 0180, iter [02200, 05004], lr: 0.002718, loss: 0.6931
2022-07-08 23:51:37 - train: epoch 0180, iter [02300, 05004], lr: 0.002713, loss: 0.7293
2022-07-08 23:52:10 - train: epoch 0180, iter [02400, 05004], lr: 0.002708, loss: 0.6713
2022-07-08 23:52:43 - train: epoch 0180, iter [02500, 05004], lr: 0.002702, loss: 0.7756
2022-07-08 23:53:16 - train: epoch 0180, iter [02600, 05004], lr: 0.002697, loss: 0.7853
2022-07-08 23:53:49 - train: epoch 0180, iter [02700, 05004], lr: 0.002692, loss: 0.7881
2022-07-08 23:54:21 - train: epoch 0180, iter [02800, 05004], lr: 0.002687, loss: 0.7189
2022-07-08 23:54:54 - train: epoch 0180, iter [02900, 05004], lr: 0.002682, loss: 0.6846
2022-07-08 23:55:28 - train: epoch 0180, iter [03000, 05004], lr: 0.002676, loss: 0.6980
2022-07-08 23:56:00 - train: epoch 0180, iter [03100, 05004], lr: 0.002671, loss: 0.7745
2022-07-08 23:56:34 - train: epoch 0180, iter [03200, 05004], lr: 0.002666, loss: 0.6050
2022-07-08 23:57:07 - train: epoch 0180, iter [03300, 05004], lr: 0.002661, loss: 0.7430
2022-07-08 23:57:40 - train: epoch 0180, iter [03400, 05004], lr: 0.002656, loss: 0.9574
2022-07-08 23:58:13 - train: epoch 0180, iter [03500, 05004], lr: 0.002650, loss: 0.6309
2022-07-08 23:58:46 - train: epoch 0180, iter [03600, 05004], lr: 0.002645, loss: 0.8311
2022-07-08 23:59:19 - train: epoch 0180, iter [03700, 05004], lr: 0.002640, loss: 0.7378
2022-07-08 23:59:52 - train: epoch 0180, iter [03800, 05004], lr: 0.002635, loss: 0.8584
2022-07-09 00:00:26 - train: epoch 0180, iter [03900, 05004], lr: 0.002630, loss: 0.8620
2022-07-09 00:00:59 - train: epoch 0180, iter [04000, 05004], lr: 0.002625, loss: 0.7364
2022-07-09 00:01:32 - train: epoch 0180, iter [04100, 05004], lr: 0.002619, loss: 0.6658
2022-07-09 00:02:04 - train: epoch 0180, iter [04200, 05004], lr: 0.002614, loss: 0.7687
2022-07-09 00:02:37 - train: epoch 0180, iter [04300, 05004], lr: 0.002609, loss: 0.9329
2022-07-09 00:03:10 - train: epoch 0180, iter [04400, 05004], lr: 0.002604, loss: 0.7373
2022-07-09 00:03:43 - train: epoch 0180, iter [04500, 05004], lr: 0.002599, loss: 0.6576
2022-07-09 00:04:16 - train: epoch 0180, iter [04600, 05004], lr: 0.002594, loss: 0.9494
2022-07-09 00:04:49 - train: epoch 0180, iter [04700, 05004], lr: 0.002589, loss: 0.6345
2022-07-09 00:05:22 - train: epoch 0180, iter [04800, 05004], lr: 0.002584, loss: 0.8413
2022-07-09 00:05:54 - train: epoch 0180, iter [04900, 05004], lr: 0.002578, loss: 0.6855
2022-07-09 00:06:26 - train: epoch 0180, iter [05000, 05004], lr: 0.002573, loss: 0.6905
2022-07-09 00:06:27 - train: epoch 180, train_loss: 0.7421
2022-07-09 00:07:41 - eval: epoch: 180, acc1: 75.906%, acc5: 92.866%, test_loss: 0.9847, per_image_load_time: 2.009ms, per_image_inference_time: 0.472ms
2022-07-09 00:07:41 - until epoch: 180, best_acc1: 75.906%
2022-07-09 00:07:41 - epoch 181 lr: 0.002573
2022-07-09 00:08:20 - train: epoch 0181, iter [00100, 05004], lr: 0.002568, loss: 0.8215
2022-07-09 00:08:54 - train: epoch 0181, iter [00200, 05004], lr: 0.002563, loss: 0.8687
2022-07-09 00:09:26 - train: epoch 0181, iter [00300, 05004], lr: 0.002558, loss: 0.6649
2022-07-09 00:09:59 - train: epoch 0181, iter [00400, 05004], lr: 0.002553, loss: 0.6086
2022-07-09 00:10:32 - train: epoch 0181, iter [00500, 05004], lr: 0.002548, loss: 0.7555
2022-07-09 00:11:06 - train: epoch 0181, iter [00600, 05004], lr: 0.002543, loss: 0.5597
2022-07-09 00:11:39 - train: epoch 0181, iter [00700, 05004], lr: 0.002538, loss: 0.6845
2022-07-09 00:12:12 - train: epoch 0181, iter [00800, 05004], lr: 0.002533, loss: 0.8225
2022-07-09 00:12:46 - train: epoch 0181, iter [00900, 05004], lr: 0.002527, loss: 0.6116
2022-07-09 00:13:20 - train: epoch 0181, iter [01000, 05004], lr: 0.002522, loss: 0.6777
2022-07-09 00:13:54 - train: epoch 0181, iter [01100, 05004], lr: 0.002517, loss: 0.7614
2022-07-09 00:14:27 - train: epoch 0181, iter [01200, 05004], lr: 0.002512, loss: 0.9224
2022-07-09 00:15:00 - train: epoch 0181, iter [01300, 05004], lr: 0.002507, loss: 0.7715
2022-07-09 00:15:35 - train: epoch 0181, iter [01400, 05004], lr: 0.002502, loss: 0.6337
2022-07-09 00:16:09 - train: epoch 0181, iter [01500, 05004], lr: 0.002497, loss: 0.8343
2022-07-09 00:16:43 - train: epoch 0181, iter [01600, 05004], lr: 0.002492, loss: 0.6327
2022-07-09 00:17:17 - train: epoch 0181, iter [01700, 05004], lr: 0.002487, loss: 1.0613
2022-07-09 00:17:51 - train: epoch 0181, iter [01800, 05004], lr: 0.002482, loss: 0.9067
2022-07-09 00:18:25 - train: epoch 0181, iter [01900, 05004], lr: 0.002477, loss: 0.5980
2022-07-09 00:18:59 - train: epoch 0181, iter [02000, 05004], lr: 0.002472, loss: 0.8443
2022-07-09 00:19:33 - train: epoch 0181, iter [02100, 05004], lr: 0.002467, loss: 0.7924
2022-07-09 00:20:06 - train: epoch 0181, iter [02200, 05004], lr: 0.002462, loss: 0.8209
2022-07-09 00:20:40 - train: epoch 0181, iter [02300, 05004], lr: 0.002457, loss: 0.8023
2022-07-09 00:21:14 - train: epoch 0181, iter [02400, 05004], lr: 0.002452, loss: 0.6220
2022-07-09 00:21:48 - train: epoch 0181, iter [02500, 05004], lr: 0.002447, loss: 0.6109
2022-07-09 00:22:21 - train: epoch 0181, iter [02600, 05004], lr: 0.002442, loss: 0.8147
2022-07-09 00:22:56 - train: epoch 0181, iter [02700, 05004], lr: 0.002437, loss: 0.7873
2022-07-09 00:23:29 - train: epoch 0181, iter [02800, 05004], lr: 0.002432, loss: 0.7482
2022-07-09 00:24:03 - train: epoch 0181, iter [02900, 05004], lr: 0.002427, loss: 0.8039
2022-07-09 00:24:37 - train: epoch 0181, iter [03000, 05004], lr: 0.002422, loss: 0.6852
2022-07-09 00:25:11 - train: epoch 0181, iter [03100, 05004], lr: 0.002418, loss: 0.8454
2022-07-09 00:25:45 - train: epoch 0181, iter [03200, 05004], lr: 0.002413, loss: 0.6706
2022-07-09 00:26:19 - train: epoch 0181, iter [03300, 05004], lr: 0.002408, loss: 0.8096
2022-07-09 00:26:53 - train: epoch 0181, iter [03400, 05004], lr: 0.002403, loss: 0.6180
2022-07-09 00:27:27 - train: epoch 0181, iter [03500, 05004], lr: 0.002398, loss: 0.7402
2022-07-09 00:28:00 - train: epoch 0181, iter [03600, 05004], lr: 0.002393, loss: 0.6347
2022-07-09 00:28:35 - train: epoch 0181, iter [03700, 05004], lr: 0.002388, loss: 0.7205
2022-07-09 00:29:08 - train: epoch 0181, iter [03800, 05004], lr: 0.002383, loss: 0.7254
2022-07-09 00:29:42 - train: epoch 0181, iter [03900, 05004], lr: 0.002378, loss: 0.9230
2022-07-09 00:30:17 - train: epoch 0181, iter [04000, 05004], lr: 0.002373, loss: 0.6257
2022-07-09 00:30:50 - train: epoch 0181, iter [04100, 05004], lr: 0.002368, loss: 0.7186
2022-07-09 00:31:23 - train: epoch 0181, iter [04200, 05004], lr: 0.002363, loss: 0.6870
2022-07-09 00:31:57 - train: epoch 0181, iter [04300, 05004], lr: 0.002359, loss: 0.7120
2022-07-09 00:32:32 - train: epoch 0181, iter [04400, 05004], lr: 0.002354, loss: 0.7389
2022-07-09 00:33:05 - train: epoch 0181, iter [04500, 05004], lr: 0.002349, loss: 0.8761
2022-07-09 00:33:39 - train: epoch 0181, iter [04600, 05004], lr: 0.002344, loss: 0.8226
2022-07-09 00:34:13 - train: epoch 0181, iter [04700, 05004], lr: 0.002339, loss: 0.9318
2022-07-09 00:34:48 - train: epoch 0181, iter [04800, 05004], lr: 0.002334, loss: 0.7060
2022-07-09 00:35:21 - train: epoch 0181, iter [04900, 05004], lr: 0.002329, loss: 0.6530
2022-07-09 00:35:53 - train: epoch 0181, iter [05000, 05004], lr: 0.002324, loss: 0.6505
2022-07-09 00:35:55 - train: epoch 181, train_loss: 0.7278
2022-07-09 00:37:08 - eval: epoch: 181, acc1: 75.962%, acc5: 92.972%, test_loss: 0.9824, per_image_load_time: 2.006ms, per_image_inference_time: 0.499ms
2022-07-09 00:37:08 - until epoch: 181, best_acc1: 75.962%
2022-07-09 00:37:08 - epoch 182 lr: 0.002324
2022-07-09 00:37:48 - train: epoch 0182, iter [00100, 05004], lr: 0.002319, loss: 0.7044
2022-07-09 00:38:21 - train: epoch 0182, iter [00200, 05004], lr: 0.002315, loss: 0.7208
2022-07-09 00:38:53 - train: epoch 0182, iter [00300, 05004], lr: 0.002310, loss: 0.6768
2022-07-09 00:39:26 - train: epoch 0182, iter [00400, 05004], lr: 0.002305, loss: 0.7163
2022-07-09 00:40:00 - train: epoch 0182, iter [00500, 05004], lr: 0.002300, loss: 0.7155
2022-07-09 00:40:34 - train: epoch 0182, iter [00600, 05004], lr: 0.002295, loss: 0.8026
2022-07-09 00:41:07 - train: epoch 0182, iter [00700, 05004], lr: 0.002290, loss: 0.6721
2022-07-09 00:41:41 - train: epoch 0182, iter [00800, 05004], lr: 0.002286, loss: 0.6839
2022-07-09 00:42:14 - train: epoch 0182, iter [00900, 05004], lr: 0.002281, loss: 0.7618
2022-07-09 00:42:48 - train: epoch 0182, iter [01000, 05004], lr: 0.002276, loss: 0.8671
2022-07-09 00:43:22 - train: epoch 0182, iter [01100, 05004], lr: 0.002271, loss: 0.7662
2022-07-09 00:43:55 - train: epoch 0182, iter [01200, 05004], lr: 0.002266, loss: 0.6494
2022-07-09 00:44:30 - train: epoch 0182, iter [01300, 05004], lr: 0.002262, loss: 0.4997
2022-07-09 00:45:02 - train: epoch 0182, iter [01400, 05004], lr: 0.002257, loss: 0.8302
2022-07-09 00:45:37 - train: epoch 0182, iter [01500, 05004], lr: 0.002252, loss: 0.7945
2022-07-09 00:46:11 - train: epoch 0182, iter [01600, 05004], lr: 0.002247, loss: 0.8903
2022-07-09 00:46:45 - train: epoch 0182, iter [01700, 05004], lr: 0.002243, loss: 0.7935
2022-07-09 00:47:19 - train: epoch 0182, iter [01800, 05004], lr: 0.002238, loss: 0.6273
2022-07-09 00:47:53 - train: epoch 0182, iter [01900, 05004], lr: 0.002233, loss: 0.7088
2022-07-09 00:48:26 - train: epoch 0182, iter [02000, 05004], lr: 0.002228, loss: 0.8930
2022-07-09 00:49:01 - train: epoch 0182, iter [02100, 05004], lr: 0.002223, loss: 0.6012
2022-07-09 00:49:34 - train: epoch 0182, iter [02200, 05004], lr: 0.002219, loss: 0.7212
2022-07-09 00:50:09 - train: epoch 0182, iter [02300, 05004], lr: 0.002214, loss: 0.6649
2022-07-09 00:50:42 - train: epoch 0182, iter [02400, 05004], lr: 0.002209, loss: 0.6538
2022-07-09 00:51:18 - train: epoch 0182, iter [02500, 05004], lr: 0.002205, loss: 0.7085
2022-07-09 00:51:50 - train: epoch 0182, iter [02600, 05004], lr: 0.002200, loss: 0.5449
2022-07-09 00:52:24 - train: epoch 0182, iter [02700, 05004], lr: 0.002195, loss: 0.5517
2022-07-09 00:52:58 - train: epoch 0182, iter [02800, 05004], lr: 0.002190, loss: 0.5749
2022-07-09 00:53:31 - train: epoch 0182, iter [02900, 05004], lr: 0.002186, loss: 0.7557
2022-07-09 00:54:05 - train: epoch 0182, iter [03000, 05004], lr: 0.002181, loss: 0.7598
2022-07-09 00:54:39 - train: epoch 0182, iter [03100, 05004], lr: 0.002176, loss: 0.8064
2022-07-09 00:55:14 - train: epoch 0182, iter [03200, 05004], lr: 0.002172, loss: 0.7058
2022-07-09 00:55:47 - train: epoch 0182, iter [03300, 05004], lr: 0.002167, loss: 0.6889
2022-07-09 00:56:20 - train: epoch 0182, iter [03400, 05004], lr: 0.002162, loss: 0.7475
2022-07-09 00:56:54 - train: epoch 0182, iter [03500, 05004], lr: 0.002158, loss: 0.7614
2022-07-09 00:57:29 - train: epoch 0182, iter [03600, 05004], lr: 0.002153, loss: 0.7357
2022-07-09 00:58:02 - train: epoch 0182, iter [03700, 05004], lr: 0.002148, loss: 0.6156
2022-07-09 00:58:36 - train: epoch 0182, iter [03800, 05004], lr: 0.002143, loss: 0.8566
2022-07-09 00:59:10 - train: epoch 0182, iter [03900, 05004], lr: 0.002139, loss: 0.7563
2022-07-09 00:59:44 - train: epoch 0182, iter [04000, 05004], lr: 0.002134, loss: 0.7010
2022-07-09 01:00:18 - train: epoch 0182, iter [04100, 05004], lr: 0.002130, loss: 0.8240
2022-07-09 01:00:51 - train: epoch 0182, iter [04200, 05004], lr: 0.002125, loss: 0.6783
2022-07-09 01:01:25 - train: epoch 0182, iter [04300, 05004], lr: 0.002120, loss: 0.6813
2022-07-09 01:01:59 - train: epoch 0182, iter [04400, 05004], lr: 0.002116, loss: 0.5720
2022-07-09 01:02:32 - train: epoch 0182, iter [04500, 05004], lr: 0.002111, loss: 0.7780
2022-07-09 01:03:06 - train: epoch 0182, iter [04600, 05004], lr: 0.002106, loss: 0.7042
2022-07-09 01:03:40 - train: epoch 0182, iter [04700, 05004], lr: 0.002102, loss: 0.7470
2022-07-09 01:04:14 - train: epoch 0182, iter [04800, 05004], lr: 0.002097, loss: 0.7385
2022-07-09 01:04:47 - train: epoch 0182, iter [04900, 05004], lr: 0.002092, loss: 0.5432
2022-07-09 01:05:20 - train: epoch 0182, iter [05000, 05004], lr: 0.002088, loss: 0.6699
2022-07-09 01:05:21 - train: epoch 182, train_loss: 0.7150
2022-07-09 01:06:34 - eval: epoch: 182, acc1: 76.108%, acc5: 92.946%, test_loss: 0.9778, per_image_load_time: 1.263ms, per_image_inference_time: 0.479ms
2022-07-09 01:06:35 - until epoch: 182, best_acc1: 76.108%
2022-07-09 01:06:35 - epoch 183 lr: 0.002088
2022-07-09 01:07:14 - train: epoch 0183, iter [00100, 05004], lr: 0.002083, loss: 0.7273
2022-07-09 01:07:46 - train: epoch 0183, iter [00200, 05004], lr: 0.002079, loss: 0.7764
2022-07-09 01:08:19 - train: epoch 0183, iter [00300, 05004], lr: 0.002074, loss: 0.7006
2022-07-09 01:08:53 - train: epoch 0183, iter [00400, 05004], lr: 0.002069, loss: 0.7041
2022-07-09 01:09:27 - train: epoch 0183, iter [00500, 05004], lr: 0.002065, loss: 0.7366
2022-07-09 01:10:00 - train: epoch 0183, iter [00600, 05004], lr: 0.002060, loss: 0.5543
2022-07-09 01:10:33 - train: epoch 0183, iter [00700, 05004], lr: 0.002056, loss: 0.7178
2022-07-09 01:11:07 - train: epoch 0183, iter [00800, 05004], lr: 0.002051, loss: 0.8257
2022-07-09 01:11:40 - train: epoch 0183, iter [00900, 05004], lr: 0.002046, loss: 0.6751
2022-07-09 01:12:14 - train: epoch 0183, iter [01000, 05004], lr: 0.002042, loss: 0.6409
2022-07-09 01:12:47 - train: epoch 0183, iter [01100, 05004], lr: 0.002037, loss: 0.7409
2022-07-09 01:13:21 - train: epoch 0183, iter [01200, 05004], lr: 0.002033, loss: 0.9065
2022-07-09 01:13:54 - train: epoch 0183, iter [01300, 05004], lr: 0.002028, loss: 0.5908
2022-07-09 01:14:27 - train: epoch 0183, iter [01400, 05004], lr: 0.002024, loss: 0.7147
2022-07-09 01:15:01 - train: epoch 0183, iter [01500, 05004], lr: 0.002019, loss: 0.6136
2022-07-09 01:15:35 - train: epoch 0183, iter [01600, 05004], lr: 0.002015, loss: 0.6083
2022-07-09 01:16:08 - train: epoch 0183, iter [01700, 05004], lr: 0.002010, loss: 0.6833
2022-07-09 01:16:41 - train: epoch 0183, iter [01800, 05004], lr: 0.002006, loss: 0.5941
2022-07-09 01:17:15 - train: epoch 0183, iter [01900, 05004], lr: 0.002001, loss: 0.6240
2022-07-09 01:17:48 - train: epoch 0183, iter [02000, 05004], lr: 0.001997, loss: 0.6093
2022-07-09 01:18:22 - train: epoch 0183, iter [02100, 05004], lr: 0.001992, loss: 0.8900
2022-07-09 01:18:55 - train: epoch 0183, iter [02200, 05004], lr: 0.001988, loss: 0.9213
2022-07-09 01:19:29 - train: epoch 0183, iter [02300, 05004], lr: 0.001983, loss: 0.7055
2022-07-09 01:20:03 - train: epoch 0183, iter [02400, 05004], lr: 0.001979, loss: 0.7205
2022-07-09 01:20:37 - train: epoch 0183, iter [02500, 05004], lr: 0.001974, loss: 0.7109
2022-07-09 01:21:11 - train: epoch 0183, iter [02600, 05004], lr: 0.001970, loss: 0.8170
2022-07-09 01:21:45 - train: epoch 0183, iter [02700, 05004], lr: 0.001965, loss: 0.7687
2022-07-09 01:22:18 - train: epoch 0183, iter [02800, 05004], lr: 0.001961, loss: 0.7259
2022-07-09 01:22:52 - train: epoch 0183, iter [02900, 05004], lr: 0.001956, loss: 0.6847
2022-07-09 01:23:26 - train: epoch 0183, iter [03000, 05004], lr: 0.001952, loss: 0.7651
2022-07-09 01:23:59 - train: epoch 0183, iter [03100, 05004], lr: 0.001947, loss: 0.6166
2022-07-09 01:24:33 - train: epoch 0183, iter [03200, 05004], lr: 0.001943, loss: 0.5912
2022-07-09 01:25:07 - train: epoch 0183, iter [03300, 05004], lr: 0.001939, loss: 0.6125
2022-07-09 01:25:42 - train: epoch 0183, iter [03400, 05004], lr: 0.001934, loss: 0.6419
2022-07-09 01:26:15 - train: epoch 0183, iter [03500, 05004], lr: 0.001930, loss: 0.7251
2022-07-09 01:26:48 - train: epoch 0183, iter [03600, 05004], lr: 0.001925, loss: 0.6597
2022-07-09 01:27:23 - train: epoch 0183, iter [03700, 05004], lr: 0.001921, loss: 0.9105
2022-07-09 01:27:55 - train: epoch 0183, iter [03800, 05004], lr: 0.001916, loss: 0.5559
2022-07-09 01:28:31 - train: epoch 0183, iter [03900, 05004], lr: 0.001912, loss: 0.7021
2022-07-09 01:29:04 - train: epoch 0183, iter [04000, 05004], lr: 0.001908, loss: 0.8253
2022-07-09 01:29:39 - train: epoch 0183, iter [04100, 05004], lr: 0.001903, loss: 0.7679
2022-07-09 01:30:12 - train: epoch 0183, iter [04200, 05004], lr: 0.001899, loss: 0.5476
2022-07-09 01:30:46 - train: epoch 0183, iter [04300, 05004], lr: 0.001894, loss: 0.6408
2022-07-09 01:31:20 - train: epoch 0183, iter [04400, 05004], lr: 0.001890, loss: 0.5871
2022-07-09 01:31:53 - train: epoch 0183, iter [04500, 05004], lr: 0.001886, loss: 0.6132
2022-07-09 01:32:27 - train: epoch 0183, iter [04600, 05004], lr: 0.001881, loss: 0.6490
2022-07-09 01:33:00 - train: epoch 0183, iter [04700, 05004], lr: 0.001877, loss: 0.6740
2022-07-09 01:33:34 - train: epoch 0183, iter [04800, 05004], lr: 0.001872, loss: 0.7294
2022-07-09 01:34:07 - train: epoch 0183, iter [04900, 05004], lr: 0.001868, loss: 0.5934
2022-07-09 01:34:40 - train: epoch 0183, iter [05000, 05004], lr: 0.001864, loss: 0.6315
2022-07-09 01:34:42 - train: epoch 183, train_loss: 0.7000
2022-07-09 01:35:56 - eval: epoch: 183, acc1: 76.068%, acc5: 92.902%, test_loss: 0.9773, per_image_load_time: 1.677ms, per_image_inference_time: 0.498ms
2022-07-09 01:35:56 - until epoch: 183, best_acc1: 76.108%
2022-07-09 01:35:56 - epoch 184 lr: 0.001864
2022-07-09 01:36:34 - train: epoch 0184, iter [00100, 05004], lr: 0.001859, loss: 0.6541
2022-07-09 01:37:08 - train: epoch 0184, iter [00200, 05004], lr: 0.001855, loss: 0.6826
2022-07-09 01:37:41 - train: epoch 0184, iter [00300, 05004], lr: 0.001851, loss: 0.6285
2022-07-09 01:38:16 - train: epoch 0184, iter [00400, 05004], lr: 0.001846, loss: 0.7364
2022-07-09 01:38:48 - train: epoch 0184, iter [00500, 05004], lr: 0.001842, loss: 0.6365
2022-07-09 01:39:21 - train: epoch 0184, iter [00600, 05004], lr: 0.001838, loss: 0.6490
2022-07-09 01:39:54 - train: epoch 0184, iter [00700, 05004], lr: 0.001833, loss: 0.6260
2022-07-09 01:40:28 - train: epoch 0184, iter [00800, 05004], lr: 0.001829, loss: 0.7472
2022-07-09 01:41:01 - train: epoch 0184, iter [00900, 05004], lr: 0.001825, loss: 0.5390
2022-07-09 01:41:34 - train: epoch 0184, iter [01000, 05004], lr: 0.001820, loss: 0.6990
2022-07-09 01:42:08 - train: epoch 0184, iter [01100, 05004], lr: 0.001816, loss: 0.7257
2022-07-09 01:42:40 - train: epoch 0184, iter [01200, 05004], lr: 0.001812, loss: 0.6539
2022-07-09 01:43:14 - train: epoch 0184, iter [01300, 05004], lr: 0.001807, loss: 0.5321
2022-07-09 01:43:49 - train: epoch 0184, iter [01400, 05004], lr: 0.001803, loss: 0.6506
2022-07-09 01:44:22 - train: epoch 0184, iter [01500, 05004], lr: 0.001799, loss: 0.6271
2022-07-09 01:44:56 - train: epoch 0184, iter [01600, 05004], lr: 0.001795, loss: 0.6011
2022-07-09 01:45:29 - train: epoch 0184, iter [01700, 05004], lr: 0.001790, loss: 0.6826
2022-07-09 01:46:03 - train: epoch 0184, iter [01800, 05004], lr: 0.001786, loss: 0.8839
2022-07-09 01:46:36 - train: epoch 0184, iter [01900, 05004], lr: 0.001782, loss: 0.5522
2022-07-09 01:47:10 - train: epoch 0184, iter [02000, 05004], lr: 0.001778, loss: 0.6681
2022-07-09 01:47:43 - train: epoch 0184, iter [02100, 05004], lr: 0.001773, loss: 0.5606
2022-07-09 01:48:18 - train: epoch 0184, iter [02200, 05004], lr: 0.001769, loss: 0.7570
2022-07-09 01:48:51 - train: epoch 0184, iter [02300, 05004], lr: 0.001765, loss: 0.8228
2022-07-09 01:49:24 - train: epoch 0184, iter [02400, 05004], lr: 0.001761, loss: 0.6639
2022-07-09 01:49:58 - train: epoch 0184, iter [02500, 05004], lr: 0.001756, loss: 0.7312
2022-07-09 01:50:33 - train: epoch 0184, iter [02600, 05004], lr: 0.001752, loss: 0.8206
2022-07-09 01:51:06 - train: epoch 0184, iter [02700, 05004], lr: 0.001748, loss: 0.9146
2022-07-09 01:51:39 - train: epoch 0184, iter [02800, 05004], lr: 0.001744, loss: 0.6705
2022-07-09 01:52:13 - train: epoch 0184, iter [02900, 05004], lr: 0.001739, loss: 0.6355
2022-07-09 01:52:46 - train: epoch 0184, iter [03000, 05004], lr: 0.001735, loss: 0.5820
2022-07-09 01:53:21 - train: epoch 0184, iter [03100, 05004], lr: 0.001731, loss: 0.7492
2022-07-09 01:53:55 - train: epoch 0184, iter [03200, 05004], lr: 0.001727, loss: 0.7623
2022-07-09 01:54:28 - train: epoch 0184, iter [03300, 05004], lr: 0.001723, loss: 0.6817
2022-07-09 01:55:02 - train: epoch 0184, iter [03400, 05004], lr: 0.001718, loss: 0.6845
2022-07-09 01:55:35 - train: epoch 0184, iter [03500, 05004], lr: 0.001714, loss: 0.6984
2022-07-09 01:56:09 - train: epoch 0184, iter [03600, 05004], lr: 0.001710, loss: 0.8480
2022-07-09 01:56:43 - train: epoch 0184, iter [03700, 05004], lr: 0.001706, loss: 0.6889
2022-07-09 01:57:16 - train: epoch 0184, iter [03800, 05004], lr: 0.001702, loss: 0.5480
2022-07-09 01:57:50 - train: epoch 0184, iter [03900, 05004], lr: 0.001698, loss: 0.5501
2022-07-09 01:58:24 - train: epoch 0184, iter [04000, 05004], lr: 0.001693, loss: 0.9156
2022-07-09 01:58:57 - train: epoch 0184, iter [04100, 05004], lr: 0.001689, loss: 0.7267
2022-07-09 01:59:31 - train: epoch 0184, iter [04200, 05004], lr: 0.001685, loss: 0.5840
2022-07-09 02:00:05 - train: epoch 0184, iter [04300, 05004], lr: 0.001681, loss: 0.7460
2022-07-09 02:00:39 - train: epoch 0184, iter [04400, 05004], lr: 0.001677, loss: 0.7037
2022-07-09 02:01:13 - train: epoch 0184, iter [04500, 05004], lr: 0.001673, loss: 0.8688
2022-07-09 02:01:47 - train: epoch 0184, iter [04600, 05004], lr: 0.001669, loss: 0.7341
2022-07-09 02:02:20 - train: epoch 0184, iter [04700, 05004], lr: 0.001664, loss: 0.7918
2022-07-09 02:02:54 - train: epoch 0184, iter [04800, 05004], lr: 0.001660, loss: 0.7241
2022-07-09 02:03:28 - train: epoch 0184, iter [04900, 05004], lr: 0.001656, loss: 0.6505
2022-07-09 02:04:00 - train: epoch 0184, iter [05000, 05004], lr: 0.001652, loss: 0.7473
2022-07-09 02:04:01 - train: epoch 184, train_loss: 0.6879
2022-07-09 02:05:15 - eval: epoch: 184, acc1: 76.152%, acc5: 93.002%, test_loss: 0.9734, per_image_load_time: 2.300ms, per_image_inference_time: 0.480ms
2022-07-09 02:05:15 - until epoch: 184, best_acc1: 76.152%
2022-07-09 02:05:15 - epoch 185 lr: 0.001652
2022-07-09 02:05:54 - train: epoch 0185, iter [00100, 05004], lr: 0.001648, loss: 0.6301
2022-07-09 02:06:27 - train: epoch 0185, iter [00200, 05004], lr: 0.001644, loss: 0.5585
2022-07-09 02:06:59 - train: epoch 0185, iter [00300, 05004], lr: 0.001640, loss: 0.6810
2022-07-09 02:07:33 - train: epoch 0185, iter [00400, 05004], lr: 0.001636, loss: 0.7116
2022-07-09 02:08:06 - train: epoch 0185, iter [00500, 05004], lr: 0.001632, loss: 0.8572
2022-07-09 02:08:39 - train: epoch 0185, iter [00600, 05004], lr: 0.001627, loss: 0.6342
2022-07-09 02:09:13 - train: epoch 0185, iter [00700, 05004], lr: 0.001623, loss: 0.7387
2022-07-09 02:09:46 - train: epoch 0185, iter [00800, 05004], lr: 0.001619, loss: 0.7906
2022-07-09 02:10:19 - train: epoch 0185, iter [00900, 05004], lr: 0.001615, loss: 0.6175
2022-07-09 02:10:52 - train: epoch 0185, iter [01000, 05004], lr: 0.001611, loss: 0.7903
2022-07-09 02:11:25 - train: epoch 0185, iter [01100, 05004], lr: 0.001607, loss: 0.6279
2022-07-09 02:11:59 - train: epoch 0185, iter [01200, 05004], lr: 0.001603, loss: 0.6624
2022-07-09 02:12:32 - train: epoch 0185, iter [01300, 05004], lr: 0.001599, loss: 0.8226
2022-07-09 02:13:05 - train: epoch 0185, iter [01400, 05004], lr: 0.001595, loss: 0.6136
2022-07-09 02:13:39 - train: epoch 0185, iter [01500, 05004], lr: 0.001591, loss: 0.5531
2022-07-09 02:14:12 - train: epoch 0185, iter [01600, 05004], lr: 0.001587, loss: 0.5589
2022-07-09 02:14:45 - train: epoch 0185, iter [01700, 05004], lr: 0.001583, loss: 0.6878
2022-07-09 02:15:19 - train: epoch 0185, iter [01800, 05004], lr: 0.001579, loss: 0.6621
2022-07-09 02:15:53 - train: epoch 0185, iter [01900, 05004], lr: 0.001575, loss: 0.5546
2022-07-09 02:16:26 - train: epoch 0185, iter [02000, 05004], lr: 0.001571, loss: 0.5945
2022-07-09 02:17:00 - train: epoch 0185, iter [02100, 05004], lr: 0.001567, loss: 0.8089
2022-07-09 02:17:34 - train: epoch 0185, iter [02200, 05004], lr: 0.001563, loss: 0.7438
2022-07-09 02:18:07 - train: epoch 0185, iter [02300, 05004], lr: 0.001559, loss: 0.6404
2022-07-09 02:18:40 - train: epoch 0185, iter [02400, 05004], lr: 0.001555, loss: 0.9010
2022-07-09 02:19:14 - train: epoch 0185, iter [02500, 05004], lr: 0.001551, loss: 0.7155
2022-07-09 02:19:47 - train: epoch 0185, iter [02600, 05004], lr: 0.001547, loss: 0.6434
2022-07-09 02:20:21 - train: epoch 0185, iter [02700, 05004], lr: 0.001543, loss: 0.7238
2022-07-09 02:20:55 - train: epoch 0185, iter [02800, 05004], lr: 0.001539, loss: 0.8310
2022-07-09 02:21:28 - train: epoch 0185, iter [02900, 05004], lr: 0.001535, loss: 0.6276
2022-07-09 02:22:02 - train: epoch 0185, iter [03000, 05004], lr: 0.001531, loss: 0.5361
2022-07-09 02:22:35 - train: epoch 0185, iter [03100, 05004], lr: 0.001527, loss: 0.6903
2022-07-09 02:23:09 - train: epoch 0185, iter [03200, 05004], lr: 0.001523, loss: 0.6048
2022-07-09 02:23:42 - train: epoch 0185, iter [03300, 05004], lr: 0.001519, loss: 0.5999
2022-07-09 02:24:15 - train: epoch 0185, iter [03400, 05004], lr: 0.001515, loss: 0.5609
2022-07-09 02:24:49 - train: epoch 0185, iter [03500, 05004], lr: 0.001511, loss: 0.8371
2022-07-09 02:25:23 - train: epoch 0185, iter [03600, 05004], lr: 0.001507, loss: 0.6498
2022-07-09 02:25:55 - train: epoch 0185, iter [03700, 05004], lr: 0.001504, loss: 0.7104
2022-07-09 02:26:29 - train: epoch 0185, iter [03800, 05004], lr: 0.001500, loss: 0.6722
2022-07-09 02:27:03 - train: epoch 0185, iter [03900, 05004], lr: 0.001496, loss: 0.6344
2022-07-09 02:27:36 - train: epoch 0185, iter [04000, 05004], lr: 0.001492, loss: 0.8002
2022-07-09 02:28:11 - train: epoch 0185, iter [04100, 05004], lr: 0.001488, loss: 0.5703
2022-07-09 02:28:43 - train: epoch 0185, iter [04200, 05004], lr: 0.001484, loss: 0.6436
2022-07-09 02:29:18 - train: epoch 0185, iter [04300, 05004], lr: 0.001480, loss: 0.9132
2022-07-09 02:29:51 - train: epoch 0185, iter [04400, 05004], lr: 0.001476, loss: 0.5941
2022-07-09 02:30:24 - train: epoch 0185, iter [04500, 05004], lr: 0.001472, loss: 0.7366
2022-07-09 02:30:56 - train: epoch 0185, iter [04600, 05004], lr: 0.001469, loss: 0.5447
2022-07-09 02:31:31 - train: epoch 0185, iter [04700, 05004], lr: 0.001465, loss: 0.6368
2022-07-09 02:32:05 - train: epoch 0185, iter [04800, 05004], lr: 0.001461, loss: 0.6350
2022-07-09 02:32:38 - train: epoch 0185, iter [04900, 05004], lr: 0.001457, loss: 0.8991
2022-07-09 02:33:10 - train: epoch 0185, iter [05000, 05004], lr: 0.001453, loss: 0.7227
2022-07-09 02:33:11 - train: epoch 185, train_loss: 0.6753
2022-07-09 02:34:25 - eval: epoch: 185, acc1: 76.274%, acc5: 93.094%, test_loss: 0.9718, per_image_load_time: 2.354ms, per_image_inference_time: 0.483ms
2022-07-09 02:34:26 - until epoch: 185, best_acc1: 76.274%
2022-07-09 02:34:26 - epoch 186 lr: 0.001453
2022-07-09 02:35:04 - train: epoch 0186, iter [00100, 05004], lr: 0.001449, loss: 0.5401
2022-07-09 02:35:37 - train: epoch 0186, iter [00200, 05004], lr: 0.001445, loss: 0.7583
2022-07-09 02:36:11 - train: epoch 0186, iter [00300, 05004], lr: 0.001441, loss: 0.6737
2022-07-09 02:36:44 - train: epoch 0186, iter [00400, 05004], lr: 0.001438, loss: 0.6446
2022-07-09 02:37:18 - train: epoch 0186, iter [00500, 05004], lr: 0.001434, loss: 0.7085
2022-07-09 02:37:50 - train: epoch 0186, iter [00600, 05004], lr: 0.001430, loss: 0.7483
2022-07-09 02:38:24 - train: epoch 0186, iter [00700, 05004], lr: 0.001426, loss: 0.6118
2022-07-09 02:38:57 - train: epoch 0186, iter [00800, 05004], lr: 0.001422, loss: 0.7817
2022-07-09 02:39:30 - train: epoch 0186, iter [00900, 05004], lr: 0.001418, loss: 0.7302
2022-07-09 02:40:04 - train: epoch 0186, iter [01000, 05004], lr: 0.001415, loss: 0.6544
2022-07-09 02:40:38 - train: epoch 0186, iter [01100, 05004], lr: 0.001411, loss: 0.6443
2022-07-09 02:41:12 - train: epoch 0186, iter [01200, 05004], lr: 0.001407, loss: 0.5793
2022-07-09 02:41:45 - train: epoch 0186, iter [01300, 05004], lr: 0.001403, loss: 0.4554
2022-07-09 02:42:18 - train: epoch 0186, iter [01400, 05004], lr: 0.001399, loss: 0.6997
2022-07-09 02:42:52 - train: epoch 0186, iter [01500, 05004], lr: 0.001396, loss: 0.6085
2022-07-09 02:43:25 - train: epoch 0186, iter [01600, 05004], lr: 0.001392, loss: 0.5153
2022-07-09 02:43:59 - train: epoch 0186, iter [01700, 05004], lr: 0.001388, loss: 0.5599
2022-07-09 02:44:33 - train: epoch 0186, iter [01800, 05004], lr: 0.001384, loss: 0.7113
2022-07-09 02:45:07 - train: epoch 0186, iter [01900, 05004], lr: 0.001381, loss: 0.6302
2022-07-09 02:45:40 - train: epoch 0186, iter [02000, 05004], lr: 0.001377, loss: 0.7379
2022-07-09 02:46:13 - train: epoch 0186, iter [02100, 05004], lr: 0.001373, loss: 0.6870
2022-07-09 02:46:47 - train: epoch 0186, iter [02200, 05004], lr: 0.001369, loss: 0.5519
2022-07-09 02:47:21 - train: epoch 0186, iter [02300, 05004], lr: 0.001366, loss: 0.4724
2022-07-09 02:47:55 - train: epoch 0186, iter [02400, 05004], lr: 0.001362, loss: 0.6119
2022-07-09 02:48:29 - train: epoch 0186, iter [02500, 05004], lr: 0.001358, loss: 0.7510
2022-07-09 02:49:03 - train: epoch 0186, iter [02600, 05004], lr: 0.001354, loss: 0.6056
2022-07-09 02:49:36 - train: epoch 0186, iter [02700, 05004], lr: 0.001351, loss: 0.8208
2022-07-09 02:50:09 - train: epoch 0186, iter [02800, 05004], lr: 0.001347, loss: 0.6156
2022-07-09 02:50:44 - train: epoch 0186, iter [02900, 05004], lr: 0.001343, loss: 0.7200
2022-07-09 02:51:17 - train: epoch 0186, iter [03000, 05004], lr: 0.001340, loss: 0.6403
2022-07-09 02:51:51 - train: epoch 0186, iter [03100, 05004], lr: 0.001336, loss: 0.6626
2022-07-09 02:52:25 - train: epoch 0186, iter [03200, 05004], lr: 0.001332, loss: 0.8088
2022-07-09 02:52:58 - train: epoch 0186, iter [03300, 05004], lr: 0.001329, loss: 0.6114
2022-07-09 02:53:33 - train: epoch 0186, iter [03400, 05004], lr: 0.001325, loss: 0.6727
2022-07-09 02:54:06 - train: epoch 0186, iter [03500, 05004], lr: 0.001321, loss: 0.5886
2022-07-09 02:54:40 - train: epoch 0186, iter [03600, 05004], lr: 0.001317, loss: 0.5797
2022-07-09 02:55:14 - train: epoch 0186, iter [03700, 05004], lr: 0.001314, loss: 0.6069
2022-07-09 02:55:49 - train: epoch 0186, iter [03800, 05004], lr: 0.001310, loss: 0.6181
2022-07-09 02:56:22 - train: epoch 0186, iter [03900, 05004], lr: 0.001306, loss: 0.5829
2022-07-09 02:56:56 - train: epoch 0186, iter [04000, 05004], lr: 0.001303, loss: 0.5382
2022-07-09 02:57:30 - train: epoch 0186, iter [04100, 05004], lr: 0.001299, loss: 0.5092
2022-07-09 02:58:03 - train: epoch 0186, iter [04200, 05004], lr: 0.001296, loss: 0.6125
2022-07-09 02:58:38 - train: epoch 0186, iter [04300, 05004], lr: 0.001292, loss: 0.7959
2022-07-09 02:59:12 - train: epoch 0186, iter [04400, 05004], lr: 0.001288, loss: 0.5943
2022-07-09 02:59:46 - train: epoch 0186, iter [04500, 05004], lr: 0.001285, loss: 0.7814
2022-07-09 03:00:20 - train: epoch 0186, iter [04600, 05004], lr: 0.001281, loss: 0.6946
2022-07-09 03:00:53 - train: epoch 0186, iter [04700, 05004], lr: 0.001277, loss: 0.7935
2022-07-09 03:01:28 - train: epoch 0186, iter [04800, 05004], lr: 0.001274, loss: 0.8347
2022-07-09 03:02:02 - train: epoch 0186, iter [04900, 05004], lr: 0.001270, loss: 0.6035
2022-07-09 03:02:35 - train: epoch 0186, iter [05000, 05004], lr: 0.001267, loss: 0.8344
2022-07-09 03:02:36 - train: epoch 186, train_loss: 0.6635
2022-07-09 03:03:50 - eval: epoch: 186, acc1: 76.494%, acc5: 93.128%, test_loss: 0.9669, per_image_load_time: 2.374ms, per_image_inference_time: 0.474ms
2022-07-09 03:03:50 - until epoch: 186, best_acc1: 76.494%
2022-07-09 03:03:50 - epoch 187 lr: 0.001266
2022-07-09 03:04:29 - train: epoch 0187, iter [00100, 05004], lr: 0.001263, loss: 0.7519
2022-07-09 03:05:02 - train: epoch 0187, iter [00200, 05004], lr: 0.001259, loss: 0.5776
2022-07-09 03:05:34 - train: epoch 0187, iter [00300, 05004], lr: 0.001256, loss: 0.5780
2022-07-09 03:06:08 - train: epoch 0187, iter [00400, 05004], lr: 0.001252, loss: 0.6593
2022-07-09 03:06:42 - train: epoch 0187, iter [00500, 05004], lr: 0.001249, loss: 0.6431
2022-07-09 03:07:15 - train: epoch 0187, iter [00600, 05004], lr: 0.001245, loss: 0.7674
2022-07-09 03:07:48 - train: epoch 0187, iter [00700, 05004], lr: 0.001241, loss: 0.6054
2022-07-09 03:08:22 - train: epoch 0187, iter [00800, 05004], lr: 0.001238, loss: 0.7259
2022-07-09 03:08:55 - train: epoch 0187, iter [00900, 05004], lr: 0.001234, loss: 0.5174
2022-07-09 03:09:29 - train: epoch 0187, iter [01000, 05004], lr: 0.001231, loss: 0.6095
2022-07-09 03:10:03 - train: epoch 0187, iter [01100, 05004], lr: 0.001227, loss: 0.5869
2022-07-09 03:10:37 - train: epoch 0187, iter [01200, 05004], lr: 0.001224, loss: 0.6333
2022-07-09 03:11:11 - train: epoch 0187, iter [01300, 05004], lr: 0.001220, loss: 0.5866
2022-07-09 03:11:44 - train: epoch 0187, iter [01400, 05004], lr: 0.001217, loss: 0.6451
2022-07-09 03:12:18 - train: epoch 0187, iter [01500, 05004], lr: 0.001213, loss: 0.8269
2022-07-09 03:12:51 - train: epoch 0187, iter [01600, 05004], lr: 0.001209, loss: 0.6063
2022-07-09 03:13:25 - train: epoch 0187, iter [01700, 05004], lr: 0.001206, loss: 0.6887
2022-07-09 03:13:59 - train: epoch 0187, iter [01800, 05004], lr: 0.001202, loss: 0.6669
2022-07-09 03:14:33 - train: epoch 0187, iter [01900, 05004], lr: 0.001199, loss: 0.5810
2022-07-09 03:15:06 - train: epoch 0187, iter [02000, 05004], lr: 0.001195, loss: 0.7006
2022-07-09 03:15:40 - train: epoch 0187, iter [02100, 05004], lr: 0.001192, loss: 0.9340
2022-07-09 03:16:14 - train: epoch 0187, iter [02200, 05004], lr: 0.001188, loss: 0.7816
2022-07-09 03:16:49 - train: epoch 0187, iter [02300, 05004], lr: 0.001185, loss: 0.5574
2022-07-09 03:17:22 - train: epoch 0187, iter [02400, 05004], lr: 0.001181, loss: 0.8202
2022-07-09 03:17:56 - train: epoch 0187, iter [02500, 05004], lr: 0.001178, loss: 0.5415
2022-07-09 03:18:29 - train: epoch 0187, iter [02600, 05004], lr: 0.001175, loss: 0.6470
2022-07-09 03:19:03 - train: epoch 0187, iter [02700, 05004], lr: 0.001171, loss: 0.7028
2022-07-09 03:19:37 - train: epoch 0187, iter [02800, 05004], lr: 0.001168, loss: 0.6985
2022-07-09 03:20:10 - train: epoch 0187, iter [02900, 05004], lr: 0.001164, loss: 0.5371
2022-07-09 03:20:44 - train: epoch 0187, iter [03000, 05004], lr: 0.001161, loss: 0.6849
2022-07-09 03:21:17 - train: epoch 0187, iter [03100, 05004], lr: 0.001157, loss: 0.6942
2022-07-09 03:21:51 - train: epoch 0187, iter [03200, 05004], lr: 0.001154, loss: 0.7550
2022-07-09 03:22:25 - train: epoch 0187, iter [03300, 05004], lr: 0.001150, loss: 0.6455
2022-07-09 03:22:59 - train: epoch 0187, iter [03400, 05004], lr: 0.001147, loss: 0.9422
2022-07-09 03:23:33 - train: epoch 0187, iter [03500, 05004], lr: 0.001144, loss: 0.7498
2022-07-09 03:24:06 - train: epoch 0187, iter [03600, 05004], lr: 0.001140, loss: 0.7528
2022-07-09 03:24:40 - train: epoch 0187, iter [03700, 05004], lr: 0.001137, loss: 0.5523
2022-07-09 03:25:14 - train: epoch 0187, iter [03800, 05004], lr: 0.001133, loss: 0.5757
2022-07-09 03:25:47 - train: epoch 0187, iter [03900, 05004], lr: 0.001130, loss: 0.7505
2022-07-09 03:26:21 - train: epoch 0187, iter [04000, 05004], lr: 0.001126, loss: 0.6262
2022-07-09 03:26:55 - train: epoch 0187, iter [04100, 05004], lr: 0.001123, loss: 0.8374
2022-07-09 03:27:28 - train: epoch 0187, iter [04200, 05004], lr: 0.001120, loss: 0.7531
2022-07-09 03:28:03 - train: epoch 0187, iter [04300, 05004], lr: 0.001116, loss: 0.7108
2022-07-09 03:28:36 - train: epoch 0187, iter [04400, 05004], lr: 0.001113, loss: 0.6329
2022-07-09 03:29:10 - train: epoch 0187, iter [04500, 05004], lr: 0.001110, loss: 0.6073
2022-07-09 03:29:44 - train: epoch 0187, iter [04600, 05004], lr: 0.001106, loss: 0.8311
2022-07-09 03:30:18 - train: epoch 0187, iter [04700, 05004], lr: 0.001103, loss: 0.6497
2022-07-09 03:30:51 - train: epoch 0187, iter [04800, 05004], lr: 0.001099, loss: 0.7172
2022-07-09 03:31:26 - train: epoch 0187, iter [04900, 05004], lr: 0.001096, loss: 0.6384
2022-07-09 03:31:58 - train: epoch 0187, iter [05000, 05004], lr: 0.001093, loss: 0.5628
2022-07-09 03:31:59 - train: epoch 187, train_loss: 0.6543
2022-07-09 03:33:13 - eval: epoch: 187, acc1: 76.472%, acc5: 93.142%, test_loss: 0.9671, per_image_load_time: 1.114ms, per_image_inference_time: 0.478ms
2022-07-09 03:33:13 - until epoch: 187, best_acc1: 76.494%
2022-07-09 03:33:13 - epoch 188 lr: 0.001093
2022-07-09 03:33:52 - train: epoch 0188, iter [00100, 05004], lr: 0.001089, loss: 0.6249
2022-07-09 03:34:25 - train: epoch 0188, iter [00200, 05004], lr: 0.001086, loss: 0.6329
2022-07-09 03:34:58 - train: epoch 0188, iter [00300, 05004], lr: 0.001083, loss: 0.4343
2022-07-09 03:35:31 - train: epoch 0188, iter [00400, 05004], lr: 0.001079, loss: 0.6204
2022-07-09 03:36:05 - train: epoch 0188, iter [00500, 05004], lr: 0.001076, loss: 0.5531
2022-07-09 03:36:38 - train: epoch 0188, iter [00600, 05004], lr: 0.001073, loss: 0.5821
2022-07-09 03:37:11 - train: epoch 0188, iter [00700, 05004], lr: 0.001069, loss: 0.5917
2022-07-09 03:37:45 - train: epoch 0188, iter [00800, 05004], lr: 0.001066, loss: 0.6270
2022-07-09 03:38:18 - train: epoch 0188, iter [00900, 05004], lr: 0.001063, loss: 0.7101
2022-07-09 03:38:52 - train: epoch 0188, iter [01000, 05004], lr: 0.001059, loss: 0.6239
2022-07-09 03:39:25 - train: epoch 0188, iter [01100, 05004], lr: 0.001056, loss: 0.4086
2022-07-09 03:40:00 - train: epoch 0188, iter [01200, 05004], lr: 0.001053, loss: 0.6001
2022-07-09 03:40:34 - train: epoch 0188, iter [01300, 05004], lr: 0.001050, loss: 0.4237
2022-07-09 03:41:07 - train: epoch 0188, iter [01400, 05004], lr: 0.001046, loss: 0.6350
2022-07-09 03:41:41 - train: epoch 0188, iter [01500, 05004], lr: 0.001043, loss: 0.6418
2022-07-09 03:42:15 - train: epoch 0188, iter [01600, 05004], lr: 0.001040, loss: 0.6405
2022-07-09 03:42:48 - train: epoch 0188, iter [01700, 05004], lr: 0.001036, loss: 0.7084
2022-07-09 03:43:22 - train: epoch 0188, iter [01800, 05004], lr: 0.001033, loss: 0.6011
2022-07-09 03:43:56 - train: epoch 0188, iter [01900, 05004], lr: 0.001030, loss: 0.6971
2022-07-09 03:44:29 - train: epoch 0188, iter [02000, 05004], lr: 0.001027, loss: 0.6049
2022-07-09 03:45:03 - train: epoch 0188, iter [02100, 05004], lr: 0.001023, loss: 0.5232
2022-07-09 03:45:37 - train: epoch 0188, iter [02200, 05004], lr: 0.001020, loss: 0.5813
2022-07-09 03:46:11 - train: epoch 0188, iter [02300, 05004], lr: 0.001017, loss: 0.6181
2022-07-09 03:46:44 - train: epoch 0188, iter [02400, 05004], lr: 0.001014, loss: 0.5789
2022-07-09 03:47:18 - train: epoch 0188, iter [02500, 05004], lr: 0.001011, loss: 0.5249
2022-07-09 03:47:52 - train: epoch 0188, iter [02600, 05004], lr: 0.001007, loss: 0.8452
2022-07-09 03:48:26 - train: epoch 0188, iter [02700, 05004], lr: 0.001004, loss: 0.7596
2022-07-09 03:49:00 - train: epoch 0188, iter [02800, 05004], lr: 0.001001, loss: 0.5899
2022-07-09 03:49:34 - train: epoch 0188, iter [02900, 05004], lr: 0.000998, loss: 0.6174
2022-07-09 03:50:07 - train: epoch 0188, iter [03000, 05004], lr: 0.000994, loss: 0.5519
2022-07-09 03:50:41 - train: epoch 0188, iter [03100, 05004], lr: 0.000991, loss: 0.6740
2022-07-09 03:51:15 - train: epoch 0188, iter [03200, 05004], lr: 0.000988, loss: 0.5367
2022-07-09 03:51:48 - train: epoch 0188, iter [03300, 05004], lr: 0.000985, loss: 0.6519
2022-07-09 03:52:21 - train: epoch 0188, iter [03400, 05004], lr: 0.000982, loss: 0.5062
2022-07-09 03:52:55 - train: epoch 0188, iter [03500, 05004], lr: 0.000979, loss: 0.5592
2022-07-09 03:53:29 - train: epoch 0188, iter [03600, 05004], lr: 0.000975, loss: 0.6270
2022-07-09 03:54:02 - train: epoch 0188, iter [03700, 05004], lr: 0.000972, loss: 0.6662
2022-07-09 03:54:35 - train: epoch 0188, iter [03800, 05004], lr: 0.000969, loss: 0.5554
2022-07-09 03:55:08 - train: epoch 0188, iter [03900, 05004], lr: 0.000966, loss: 0.6866
2022-07-09 03:55:42 - train: epoch 0188, iter [04000, 05004], lr: 0.000963, loss: 0.6461
2022-07-09 03:56:15 - train: epoch 0188, iter [04100, 05004], lr: 0.000960, loss: 0.6965
2022-07-09 03:56:49 - train: epoch 0188, iter [04200, 05004], lr: 0.000957, loss: 0.6949
2022-07-09 03:57:23 - train: epoch 0188, iter [04300, 05004], lr: 0.000953, loss: 0.6973
2022-07-09 03:57:57 - train: epoch 0188, iter [04400, 05004], lr: 0.000950, loss: 0.8600
2022-07-09 03:58:31 - train: epoch 0188, iter [04500, 05004], lr: 0.000947, loss: 0.5955
2022-07-09 03:59:04 - train: epoch 0188, iter [04600, 05004], lr: 0.000944, loss: 0.7597
2022-07-09 03:59:38 - train: epoch 0188, iter [04700, 05004], lr: 0.000941, loss: 0.6171
2022-07-09 04:00:12 - train: epoch 0188, iter [04800, 05004], lr: 0.000938, loss: 0.7665
2022-07-09 04:00:46 - train: epoch 0188, iter [04900, 05004], lr: 0.000935, loss: 0.6074
2022-07-09 04:01:18 - train: epoch 0188, iter [05000, 05004], lr: 0.000932, loss: 0.5763
2022-07-09 04:01:19 - train: epoch 188, train_loss: 0.6427
2022-07-09 04:02:34 - eval: epoch: 188, acc1: 76.494%, acc5: 93.252%, test_loss: 0.9631, per_image_load_time: 1.014ms, per_image_inference_time: 0.482ms
2022-07-09 04:02:34 - until epoch: 188, best_acc1: 76.494%
2022-07-09 04:02:34 - epoch 189 lr: 0.000931
2022-07-09 04:03:13 - train: epoch 0189, iter [00100, 05004], lr: 0.000928, loss: 0.5647
2022-07-09 04:03:46 - train: epoch 0189, iter [00200, 05004], lr: 0.000925, loss: 0.6116
2022-07-09 04:04:20 - train: epoch 0189, iter [00300, 05004], lr: 0.000922, loss: 0.6535
2022-07-09 04:04:54 - train: epoch 0189, iter [00400, 05004], lr: 0.000919, loss: 0.4674
2022-07-09 04:05:27 - train: epoch 0189, iter [00500, 05004], lr: 0.000916, loss: 0.5376
2022-07-09 04:06:01 - train: epoch 0189, iter [00600, 05004], lr: 0.000913, loss: 0.8112
2022-07-09 04:06:34 - train: epoch 0189, iter [00700, 05004], lr: 0.000910, loss: 0.5618
2022-07-09 04:07:08 - train: epoch 0189, iter [00800, 05004], lr: 0.000907, loss: 0.4159
2022-07-09 04:07:42 - train: epoch 0189, iter [00900, 05004], lr: 0.000904, loss: 0.6315
2022-07-09 04:08:14 - train: epoch 0189, iter [01000, 05004], lr: 0.000901, loss: 0.6642
2022-07-09 04:08:48 - train: epoch 0189, iter [01100, 05004], lr: 0.000898, loss: 0.7533
2022-07-09 04:09:22 - train: epoch 0189, iter [01200, 05004], lr: 0.000895, loss: 0.7277
2022-07-09 04:09:56 - train: epoch 0189, iter [01300, 05004], lr: 0.000892, loss: 0.6245
2022-07-09 04:10:29 - train: epoch 0189, iter [01400, 05004], lr: 0.000889, loss: 0.7107
2022-07-09 04:11:02 - train: epoch 0189, iter [01500, 05004], lr: 0.000886, loss: 0.5570
2022-07-09 04:11:37 - train: epoch 0189, iter [01600, 05004], lr: 0.000883, loss: 0.7494
2022-07-09 04:12:10 - train: epoch 0189, iter [01700, 05004], lr: 0.000880, loss: 0.7731
2022-07-09 04:12:43 - train: epoch 0189, iter [01800, 05004], lr: 0.000877, loss: 0.8872
2022-07-09 04:13:18 - train: epoch 0189, iter [01900, 05004], lr: 0.000874, loss: 0.5314
2022-07-09 04:13:51 - train: epoch 0189, iter [02000, 05004], lr: 0.000871, loss: 0.6564
2022-07-09 04:14:25 - train: epoch 0189, iter [02100, 05004], lr: 0.000868, loss: 0.6752
2022-07-09 04:14:59 - train: epoch 0189, iter [02200, 05004], lr: 0.000865, loss: 0.6055
2022-07-09 04:15:32 - train: epoch 0189, iter [02300, 05004], lr: 0.000862, loss: 0.5877
2022-07-09 04:16:06 - train: epoch 0189, iter [02400, 05004], lr: 0.000859, loss: 0.5816
2022-07-09 04:16:39 - train: epoch 0189, iter [02500, 05004], lr: 0.000856, loss: 0.5581
2022-07-09 04:17:13 - train: epoch 0189, iter [02600, 05004], lr: 0.000853, loss: 0.5970
2022-07-09 04:17:47 - train: epoch 0189, iter [02700, 05004], lr: 0.000850, loss: 0.6170
2022-07-09 04:18:19 - train: epoch 0189, iter [02800, 05004], lr: 0.000847, loss: 0.5487
2022-07-09 04:18:53 - train: epoch 0189, iter [02900, 05004], lr: 0.000844, loss: 0.6295
2022-07-09 04:19:26 - train: epoch 0189, iter [03000, 05004], lr: 0.000841, loss: 0.6429
2022-07-09 04:20:00 - train: epoch 0189, iter [03100, 05004], lr: 0.000838, loss: 0.6173
2022-07-09 04:20:34 - train: epoch 0189, iter [03200, 05004], lr: 0.000835, loss: 0.5909
2022-07-09 04:21:07 - train: epoch 0189, iter [03300, 05004], lr: 0.000832, loss: 0.6365
2022-07-09 04:21:41 - train: epoch 0189, iter [03400, 05004], lr: 0.000829, loss: 0.7235
2022-07-09 04:22:15 - train: epoch 0189, iter [03500, 05004], lr: 0.000826, loss: 0.6328
2022-07-09 04:22:48 - train: epoch 0189, iter [03600, 05004], lr: 0.000823, loss: 0.6012
2022-07-09 04:23:21 - train: epoch 0189, iter [03700, 05004], lr: 0.000821, loss: 0.5222
2022-07-09 04:23:55 - train: epoch 0189, iter [03800, 05004], lr: 0.000818, loss: 0.6893
2022-07-09 04:24:28 - train: epoch 0189, iter [03900, 05004], lr: 0.000815, loss: 0.7649
2022-07-09 04:25:01 - train: epoch 0189, iter [04000, 05004], lr: 0.000812, loss: 0.6394
2022-07-09 04:25:34 - train: epoch 0189, iter [04100, 05004], lr: 0.000809, loss: 0.6975
2022-07-09 04:26:08 - train: epoch 0189, iter [04200, 05004], lr: 0.000806, loss: 0.6138
2022-07-09 04:26:41 - train: epoch 0189, iter [04300, 05004], lr: 0.000803, loss: 0.5679
2022-07-09 04:27:15 - train: epoch 0189, iter [04400, 05004], lr: 0.000800, loss: 0.5566
2022-07-09 04:27:49 - train: epoch 0189, iter [04500, 05004], lr: 0.000797, loss: 0.5866
2022-07-09 04:28:23 - train: epoch 0189, iter [04600, 05004], lr: 0.000795, loss: 0.6421
2022-07-09 04:28:57 - train: epoch 0189, iter [04700, 05004], lr: 0.000792, loss: 0.6945
2022-07-09 04:29:29 - train: epoch 0189, iter [04800, 05004], lr: 0.000789, loss: 0.6513
2022-07-09 04:30:03 - train: epoch 0189, iter [04900, 05004], lr: 0.000786, loss: 0.5612
2022-07-09 04:30:35 - train: epoch 0189, iter [05000, 05004], lr: 0.000783, loss: 0.7233
2022-07-09 04:30:36 - train: epoch 189, train_loss: 0.6356
2022-07-09 04:31:52 - eval: epoch: 189, acc1: 76.558%, acc5: 93.154%, test_loss: 0.9622, per_image_load_time: 2.476ms, per_image_inference_time: 0.459ms
2022-07-09 04:31:53 - until epoch: 189, best_acc1: 76.558%
2022-07-09 04:31:53 - epoch 190 lr: 0.000783
2022-07-09 04:32:32 - train: epoch 0190, iter [00100, 05004], lr: 0.000780, loss: 0.6133
2022-07-09 04:33:05 - train: epoch 0190, iter [00200, 05004], lr: 0.000777, loss: 0.4888
2022-07-09 04:33:38 - train: epoch 0190, iter [00300, 05004], lr: 0.000775, loss: 0.6826
2022-07-09 04:34:11 - train: epoch 0190, iter [00400, 05004], lr: 0.000772, loss: 0.7572
2022-07-09 04:34:44 - train: epoch 0190, iter [00500, 05004], lr: 0.000769, loss: 0.6014
2022-07-09 04:35:18 - train: epoch 0190, iter [00600, 05004], lr: 0.000766, loss: 0.6929
2022-07-09 04:35:50 - train: epoch 0190, iter [00700, 05004], lr: 0.000763, loss: 0.6331
2022-07-09 04:36:24 - train: epoch 0190, iter [00800, 05004], lr: 0.000761, loss: 0.5375
2022-07-09 04:36:58 - train: epoch 0190, iter [00900, 05004], lr: 0.000758, loss: 0.5296
2022-07-09 04:37:31 - train: epoch 0190, iter [01000, 05004], lr: 0.000755, loss: 0.6852
2022-07-09 04:38:04 - train: epoch 0190, iter [01100, 05004], lr: 0.000752, loss: 0.6168
2022-07-09 04:38:37 - train: epoch 0190, iter [01200, 05004], lr: 0.000749, loss: 0.6453
2022-07-09 04:39:12 - train: epoch 0190, iter [01300, 05004], lr: 0.000747, loss: 0.7442
2022-07-09 04:39:45 - train: epoch 0190, iter [01400, 05004], lr: 0.000744, loss: 0.6889
2022-07-09 04:40:19 - train: epoch 0190, iter [01500, 05004], lr: 0.000741, loss: 0.5989
2022-07-09 04:40:52 - train: epoch 0190, iter [01600, 05004], lr: 0.000738, loss: 0.5787
2022-07-09 04:41:26 - train: epoch 0190, iter [01700, 05004], lr: 0.000736, loss: 0.7167
2022-07-09 04:41:59 - train: epoch 0190, iter [01800, 05004], lr: 0.000733, loss: 0.7621
2022-07-09 04:42:33 - train: epoch 0190, iter [01900, 05004], lr: 0.000730, loss: 0.7844
2022-07-09 04:43:07 - train: epoch 0190, iter [02000, 05004], lr: 0.000727, loss: 0.6048
2022-07-09 04:43:40 - train: epoch 0190, iter [02100, 05004], lr: 0.000725, loss: 0.6579
2022-07-09 04:44:14 - train: epoch 0190, iter [02200, 05004], lr: 0.000722, loss: 0.7268
2022-07-09 04:44:49 - train: epoch 0190, iter [02300, 05004], lr: 0.000719, loss: 0.7293
2022-07-09 04:45:23 - train: epoch 0190, iter [02400, 05004], lr: 0.000716, loss: 0.5904
2022-07-09 04:45:56 - train: epoch 0190, iter [02500, 05004], lr: 0.000714, loss: 0.5099
2022-07-09 04:46:30 - train: epoch 0190, iter [02600, 05004], lr: 0.000711, loss: 0.4960
2022-07-09 04:47:04 - train: epoch 0190, iter [02700, 05004], lr: 0.000708, loss: 0.5553
2022-07-09 04:47:38 - train: epoch 0190, iter [02800, 05004], lr: 0.000706, loss: 0.7162
2022-07-09 04:48:11 - train: epoch 0190, iter [02900, 05004], lr: 0.000703, loss: 0.7352
2022-07-09 04:48:45 - train: epoch 0190, iter [03000, 05004], lr: 0.000700, loss: 0.4613
2022-07-09 04:49:19 - train: epoch 0190, iter [03100, 05004], lr: 0.000698, loss: 0.6149
2022-07-09 04:49:53 - train: epoch 0190, iter [03200, 05004], lr: 0.000695, loss: 0.8630
2022-07-09 04:50:27 - train: epoch 0190, iter [03300, 05004], lr: 0.000692, loss: 0.5547
2022-07-09 04:51:01 - train: epoch 0190, iter [03400, 05004], lr: 0.000690, loss: 0.7493
2022-07-09 04:51:34 - train: epoch 0190, iter [03500, 05004], lr: 0.000687, loss: 0.5972
2022-07-09 04:52:09 - train: epoch 0190, iter [03600, 05004], lr: 0.000684, loss: 0.8701
2022-07-09 04:52:42 - train: epoch 0190, iter [03700, 05004], lr: 0.000682, loss: 0.6577
2022-07-09 04:53:16 - train: epoch 0190, iter [03800, 05004], lr: 0.000679, loss: 0.6942
2022-07-09 04:53:50 - train: epoch 0190, iter [03900, 05004], lr: 0.000676, loss: 0.5698
2022-07-09 04:54:24 - train: epoch 0190, iter [04000, 05004], lr: 0.000674, loss: 0.7150
2022-07-09 04:54:58 - train: epoch 0190, iter [04100, 05004], lr: 0.000671, loss: 0.8237
2022-07-09 04:55:32 - train: epoch 0190, iter [04200, 05004], lr: 0.000668, loss: 0.4918
2022-07-09 04:56:05 - train: epoch 0190, iter [04300, 05004], lr: 0.000666, loss: 0.5651
2022-07-09 04:56:38 - train: epoch 0190, iter [04400, 05004], lr: 0.000663, loss: 0.6945
2022-07-09 04:57:13 - train: epoch 0190, iter [04500, 05004], lr: 0.000661, loss: 0.4409
2022-07-09 04:57:47 - train: epoch 0190, iter [04600, 05004], lr: 0.000658, loss: 0.5936
2022-07-09 04:58:22 - train: epoch 0190, iter [04700, 05004], lr: 0.000655, loss: 0.6374
2022-07-09 04:58:55 - train: epoch 0190, iter [04800, 05004], lr: 0.000653, loss: 0.6856
2022-07-09 04:59:29 - train: epoch 0190, iter [04900, 05004], lr: 0.000650, loss: 0.5763
2022-07-09 05:00:01 - train: epoch 0190, iter [05000, 05004], lr: 0.000648, loss: 0.7432
2022-07-09 05:00:02 - train: epoch 190, train_loss: 0.6272
2022-07-09 05:01:15 - eval: epoch: 190, acc1: 76.648%, acc5: 93.270%, test_loss: 0.9610, per_image_load_time: 2.261ms, per_image_inference_time: 0.488ms
2022-07-09 05:01:16 - until epoch: 190, best_acc1: 76.648%
2022-07-09 05:01:16 - epoch 191 lr: 0.000647
2022-07-09 05:01:54 - train: epoch 0191, iter [00100, 05004], lr: 0.000645, loss: 0.6833
2022-07-09 05:02:27 - train: epoch 0191, iter [00200, 05004], lr: 0.000642, loss: 0.7725
2022-07-09 05:03:00 - train: epoch 0191, iter [00300, 05004], lr: 0.000640, loss: 0.6242
2022-07-09 05:03:33 - train: epoch 0191, iter [00400, 05004], lr: 0.000637, loss: 0.6714
2022-07-09 05:04:06 - train: epoch 0191, iter [00500, 05004], lr: 0.000635, loss: 0.6606
2022-07-09 05:04:40 - train: epoch 0191, iter [00600, 05004], lr: 0.000632, loss: 0.6305
2022-07-09 05:05:12 - train: epoch 0191, iter [00700, 05004], lr: 0.000630, loss: 0.7437
2022-07-09 05:05:46 - train: epoch 0191, iter [00800, 05004], lr: 0.000627, loss: 0.5511
2022-07-09 05:06:19 - train: epoch 0191, iter [00900, 05004], lr: 0.000624, loss: 0.8199
2022-07-09 05:06:53 - train: epoch 0191, iter [01000, 05004], lr: 0.000622, loss: 0.6626
2022-07-09 05:07:25 - train: epoch 0191, iter [01100, 05004], lr: 0.000619, loss: 0.6217
2022-07-09 05:07:59 - train: epoch 0191, iter [01200, 05004], lr: 0.000617, loss: 0.7456
2022-07-09 05:08:32 - train: epoch 0191, iter [01300, 05004], lr: 0.000614, loss: 0.8294
2022-07-09 05:09:05 - train: epoch 0191, iter [01400, 05004], lr: 0.000612, loss: 0.5234
2022-07-09 05:09:38 - train: epoch 0191, iter [01500, 05004], lr: 0.000609, loss: 0.7152
2022-07-09 05:10:12 - train: epoch 0191, iter [01600, 05004], lr: 0.000607, loss: 0.5728
2022-07-09 05:10:45 - train: epoch 0191, iter [01700, 05004], lr: 0.000604, loss: 0.5956
2022-07-09 05:11:19 - train: epoch 0191, iter [01800, 05004], lr: 0.000602, loss: 0.5519
2022-07-09 05:11:52 - train: epoch 0191, iter [01900, 05004], lr: 0.000599, loss: 0.5438
2022-07-09 05:12:26 - train: epoch 0191, iter [02000, 05004], lr: 0.000597, loss: 0.5515
2022-07-09 05:12:58 - train: epoch 0191, iter [02100, 05004], lr: 0.000594, loss: 0.6233
2022-07-09 05:13:32 - train: epoch 0191, iter [02200, 05004], lr: 0.000592, loss: 0.5329
2022-07-09 05:14:06 - train: epoch 0191, iter [02300, 05004], lr: 0.000589, loss: 0.7118
2022-07-09 05:14:39 - train: epoch 0191, iter [02400, 05004], lr: 0.000587, loss: 0.5753
2022-07-09 05:15:12 - train: epoch 0191, iter [02500, 05004], lr: 0.000585, loss: 0.5950
2022-07-09 05:15:46 - train: epoch 0191, iter [02600, 05004], lr: 0.000582, loss: 0.6792
2022-07-09 05:16:20 - train: epoch 0191, iter [02700, 05004], lr: 0.000580, loss: 0.6920
2022-07-09 05:16:53 - train: epoch 0191, iter [02800, 05004], lr: 0.000577, loss: 0.7095
2022-07-09 05:17:26 - train: epoch 0191, iter [02900, 05004], lr: 0.000575, loss: 0.4941
2022-07-09 05:17:58 - train: epoch 0191, iter [03000, 05004], lr: 0.000572, loss: 0.5737
2022-07-09 05:18:32 - train: epoch 0191, iter [03100, 05004], lr: 0.000570, loss: 0.5458
2022-07-09 05:19:05 - train: epoch 0191, iter [03200, 05004], lr: 0.000567, loss: 0.6078
2022-07-09 05:19:38 - train: epoch 0191, iter [03300, 05004], lr: 0.000565, loss: 0.5336
2022-07-09 05:20:12 - train: epoch 0191, iter [03400, 05004], lr: 0.000563, loss: 0.6925
2022-07-09 05:20:45 - train: epoch 0191, iter [03500, 05004], lr: 0.000560, loss: 0.6757
2022-07-09 05:21:19 - train: epoch 0191, iter [03600, 05004], lr: 0.000558, loss: 0.5985
2022-07-09 05:21:52 - train: epoch 0191, iter [03700, 05004], lr: 0.000555, loss: 0.6811
2022-07-09 05:22:26 - train: epoch 0191, iter [03800, 05004], lr: 0.000553, loss: 0.6825
2022-07-09 05:22:58 - train: epoch 0191, iter [03900, 05004], lr: 0.000551, loss: 0.6740
2022-07-09 05:23:32 - train: epoch 0191, iter [04000, 05004], lr: 0.000548, loss: 0.5651
2022-07-09 05:24:06 - train: epoch 0191, iter [04100, 05004], lr: 0.000546, loss: 0.6921
2022-07-09 05:24:39 - train: epoch 0191, iter [04200, 05004], lr: 0.000544, loss: 0.5006
2022-07-09 05:25:11 - train: epoch 0191, iter [04300, 05004], lr: 0.000541, loss: 0.6350
2022-07-09 05:25:45 - train: epoch 0191, iter [04400, 05004], lr: 0.000539, loss: 0.6166
2022-07-09 05:26:19 - train: epoch 0191, iter [04500, 05004], lr: 0.000536, loss: 0.5846
2022-07-09 05:26:52 - train: epoch 0191, iter [04600, 05004], lr: 0.000534, loss: 0.6046
2022-07-09 05:27:26 - train: epoch 0191, iter [04700, 05004], lr: 0.000532, loss: 0.5995
2022-07-09 05:27:59 - train: epoch 0191, iter [04800, 05004], lr: 0.000529, loss: 0.6381
2022-07-09 05:28:31 - train: epoch 0191, iter [04900, 05004], lr: 0.000527, loss: 0.6550
2022-07-09 05:29:03 - train: epoch 0191, iter [05000, 05004], lr: 0.000525, loss: 0.5566
2022-07-09 05:29:04 - train: epoch 191, train_loss: 0.6195
2022-07-09 05:30:19 - eval: epoch: 191, acc1: 76.844%, acc5: 93.314%, test_loss: 0.9573, per_image_load_time: 1.363ms, per_image_inference_time: 0.467ms
2022-07-09 05:30:19 - until epoch: 191, best_acc1: 76.844%
2022-07-09 05:30:19 - epoch 192 lr: 0.000525
2022-07-09 05:30:57 - train: epoch 0192, iter [00100, 05004], lr: 0.000522, loss: 0.6702
2022-07-09 05:31:31 - train: epoch 0192, iter [00200, 05004], lr: 0.000520, loss: 0.7062
2022-07-09 05:32:04 - train: epoch 0192, iter [00300, 05004], lr: 0.000518, loss: 0.7606
2022-07-09 05:32:37 - train: epoch 0192, iter [00400, 05004], lr: 0.000515, loss: 0.5331
2022-07-09 05:33:10 - train: epoch 0192, iter [00500, 05004], lr: 0.000513, loss: 0.5549
2022-07-09 05:33:43 - train: epoch 0192, iter [00600, 05004], lr: 0.000511, loss: 0.7103
2022-07-09 05:34:15 - train: epoch 0192, iter [00700, 05004], lr: 0.000509, loss: 0.7798
2022-07-09 05:34:49 - train: epoch 0192, iter [00800, 05004], lr: 0.000506, loss: 0.5699
2022-07-09 05:35:21 - train: epoch 0192, iter [00900, 05004], lr: 0.000504, loss: 0.7193
2022-07-09 05:35:55 - train: epoch 0192, iter [01000, 05004], lr: 0.000502, loss: 0.6615
2022-07-09 05:36:28 - train: epoch 0192, iter [01100, 05004], lr: 0.000499, loss: 0.7415
2022-07-09 05:37:01 - train: epoch 0192, iter [01200, 05004], lr: 0.000497, loss: 0.5601
2022-07-09 05:37:35 - train: epoch 0192, iter [01300, 05004], lr: 0.000495, loss: 0.6825
2022-07-09 05:38:08 - train: epoch 0192, iter [01400, 05004], lr: 0.000493, loss: 0.5063
2022-07-09 05:38:42 - train: epoch 0192, iter [01500, 05004], lr: 0.000490, loss: 0.5276
2022-07-09 05:39:15 - train: epoch 0192, iter [01600, 05004], lr: 0.000488, loss: 0.5403
2022-07-09 05:39:49 - train: epoch 0192, iter [01700, 05004], lr: 0.000486, loss: 0.5397
2022-07-09 05:40:22 - train: epoch 0192, iter [01800, 05004], lr: 0.000484, loss: 0.6915
2022-07-09 05:40:55 - train: epoch 0192, iter [01900, 05004], lr: 0.000481, loss: 0.7766
2022-07-09 05:41:28 - train: epoch 0192, iter [02000, 05004], lr: 0.000479, loss: 0.6843
2022-07-09 05:42:01 - train: epoch 0192, iter [02100, 05004], lr: 0.000477, loss: 0.5913
2022-07-09 05:42:35 - train: epoch 0192, iter [02200, 05004], lr: 0.000475, loss: 0.5644
2022-07-09 05:43:09 - train: epoch 0192, iter [02300, 05004], lr: 0.000473, loss: 0.4431
2022-07-09 05:43:42 - train: epoch 0192, iter [02400, 05004], lr: 0.000470, loss: 0.6890
2022-07-09 05:44:15 - train: epoch 0192, iter [02500, 05004], lr: 0.000468, loss: 0.5014
2022-07-09 05:44:50 - train: epoch 0192, iter [02600, 05004], lr: 0.000466, loss: 0.6278
2022-07-09 05:45:23 - train: epoch 0192, iter [02700, 05004], lr: 0.000464, loss: 0.4943
2022-07-09 05:45:56 - train: epoch 0192, iter [02800, 05004], lr: 0.000462, loss: 0.6952
2022-07-09 05:46:30 - train: epoch 0192, iter [02900, 05004], lr: 0.000459, loss: 0.5953
2022-07-09 05:47:03 - train: epoch 0192, iter [03000, 05004], lr: 0.000457, loss: 0.5157
2022-07-09 05:47:36 - train: epoch 0192, iter [03100, 05004], lr: 0.000455, loss: 0.4462
2022-07-09 05:48:10 - train: epoch 0192, iter [03200, 05004], lr: 0.000453, loss: 0.5164
2022-07-09 05:48:43 - train: epoch 0192, iter [03300, 05004], lr: 0.000451, loss: 0.7042
2022-07-09 05:49:17 - train: epoch 0192, iter [03400, 05004], lr: 0.000449, loss: 0.6334
2022-07-09 05:49:50 - train: epoch 0192, iter [03500, 05004], lr: 0.000446, loss: 0.5868
2022-07-09 05:50:23 - train: epoch 0192, iter [03600, 05004], lr: 0.000444, loss: 0.5136
2022-07-09 05:50:56 - train: epoch 0192, iter [03700, 05004], lr: 0.000442, loss: 0.5641
2022-07-09 05:51:30 - train: epoch 0192, iter [03800, 05004], lr: 0.000440, loss: 0.5006
2022-07-09 05:52:04 - train: epoch 0192, iter [03900, 05004], lr: 0.000438, loss: 0.6160
2022-07-09 05:52:37 - train: epoch 0192, iter [04000, 05004], lr: 0.000436, loss: 0.6321
2022-07-09 05:53:11 - train: epoch 0192, iter [04100, 05004], lr: 0.000434, loss: 0.8200
2022-07-09 05:53:44 - train: epoch 0192, iter [04200, 05004], lr: 0.000432, loss: 0.5964
2022-07-09 05:54:18 - train: epoch 0192, iter [04300, 05004], lr: 0.000429, loss: 0.6450
2022-07-09 05:54:51 - train: epoch 0192, iter [04400, 05004], lr: 0.000427, loss: 0.5035
2022-07-09 05:55:25 - train: epoch 0192, iter [04500, 05004], lr: 0.000425, loss: 0.6837
2022-07-09 05:55:59 - train: epoch 0192, iter [04600, 05004], lr: 0.000423, loss: 0.6015
2022-07-09 05:56:32 - train: epoch 0192, iter [04700, 05004], lr: 0.000421, loss: 0.5858
2022-07-09 05:57:06 - train: epoch 0192, iter [04800, 05004], lr: 0.000419, loss: 0.5968
2022-07-09 05:57:39 - train: epoch 0192, iter [04900, 05004], lr: 0.000417, loss: 0.6106
2022-07-09 05:58:11 - train: epoch 0192, iter [05000, 05004], lr: 0.000415, loss: 0.5076
2022-07-09 05:58:12 - train: epoch 192, train_loss: 0.6094
2022-07-09 05:59:26 - eval: epoch: 192, acc1: 76.720%, acc5: 93.298%, test_loss: 0.9564, per_image_load_time: 2.343ms, per_image_inference_time: 0.478ms
2022-07-09 05:59:26 - until epoch: 192, best_acc1: 76.844%
2022-07-09 05:59:26 - epoch 193 lr: 0.000415
2022-07-09 06:00:04 - train: epoch 0193, iter [00100, 05004], lr: 0.000413, loss: 0.6832
2022-07-09 06:00:38 - train: epoch 0193, iter [00200, 05004], lr: 0.000411, loss: 0.6374
2022-07-09 06:01:12 - train: epoch 0193, iter [00300, 05004], lr: 0.000409, loss: 0.5494
2022-07-09 06:01:44 - train: epoch 0193, iter [00400, 05004], lr: 0.000406, loss: 0.6386
2022-07-09 06:02:16 - train: epoch 0193, iter [00500, 05004], lr: 0.000404, loss: 0.5083
2022-07-09 06:02:49 - train: epoch 0193, iter [00600, 05004], lr: 0.000402, loss: 0.6254
2022-07-09 06:03:22 - train: epoch 0193, iter [00700, 05004], lr: 0.000400, loss: 0.6320
2022-07-09 06:03:55 - train: epoch 0193, iter [00800, 05004], lr: 0.000398, loss: 0.6387
2022-07-09 06:04:28 - train: epoch 0193, iter [00900, 05004], lr: 0.000396, loss: 0.5977
2022-07-09 06:05:00 - train: epoch 0193, iter [01000, 05004], lr: 0.000394, loss: 0.5032
2022-07-09 06:05:33 - train: epoch 0193, iter [01100, 05004], lr: 0.000392, loss: 0.5312
2022-07-09 06:06:07 - train: epoch 0193, iter [01200, 05004], lr: 0.000390, loss: 0.6331
2022-07-09 06:06:40 - train: epoch 0193, iter [01300, 05004], lr: 0.000388, loss: 0.5387
2022-07-09 06:07:12 - train: epoch 0193, iter [01400, 05004], lr: 0.000386, loss: 0.6798
2022-07-09 06:07:46 - train: epoch 0193, iter [01500, 05004], lr: 0.000384, loss: 0.4362
2022-07-09 06:08:19 - train: epoch 0193, iter [01600, 05004], lr: 0.000382, loss: 0.4589
2022-07-09 06:08:53 - train: epoch 0193, iter [01700, 05004], lr: 0.000380, loss: 0.5402
2022-07-09 06:09:26 - train: epoch 0193, iter [01800, 05004], lr: 0.000378, loss: 0.6510
2022-07-09 06:09:59 - train: epoch 0193, iter [01900, 05004], lr: 0.000376, loss: 0.5594
2022-07-09 06:10:33 - train: epoch 0193, iter [02000, 05004], lr: 0.000374, loss: 0.6517
2022-07-09 06:11:06 - train: epoch 0193, iter [02100, 05004], lr: 0.000372, loss: 0.6050
2022-07-09 06:11:40 - train: epoch 0193, iter [02200, 05004], lr: 0.000370, loss: 0.5642
2022-07-09 06:12:13 - train: epoch 0193, iter [02300, 05004], lr: 0.000368, loss: 0.6552
2022-07-09 06:12:47 - train: epoch 0193, iter [02400, 05004], lr: 0.000367, loss: 0.5761
2022-07-09 06:13:20 - train: epoch 0193, iter [02500, 05004], lr: 0.000365, loss: 0.6350
2022-07-09 06:13:54 - train: epoch 0193, iter [02600, 05004], lr: 0.000363, loss: 0.6949
2022-07-09 06:14:28 - train: epoch 0193, iter [02700, 05004], lr: 0.000361, loss: 0.5363
2022-07-09 06:15:01 - train: epoch 0193, iter [02800, 05004], lr: 0.000359, loss: 0.7205
2022-07-09 06:15:35 - train: epoch 0193, iter [02900, 05004], lr: 0.000357, loss: 0.5643
2022-07-09 06:16:09 - train: epoch 0193, iter [03000, 05004], lr: 0.000355, loss: 0.6827
2022-07-09 06:16:42 - train: epoch 0193, iter [03100, 05004], lr: 0.000353, loss: 0.6219
2022-07-09 06:17:15 - train: epoch 0193, iter [03200, 05004], lr: 0.000351, loss: 0.6382
2022-07-09 06:17:50 - train: epoch 0193, iter [03300, 05004], lr: 0.000349, loss: 0.4145
2022-07-09 06:18:22 - train: epoch 0193, iter [03400, 05004], lr: 0.000347, loss: 0.5073
2022-07-09 06:18:55 - train: epoch 0193, iter [03500, 05004], lr: 0.000345, loss: 0.5985
2022-07-09 06:19:29 - train: epoch 0193, iter [03600, 05004], lr: 0.000344, loss: 0.7301
2022-07-09 06:20:03 - train: epoch 0193, iter [03700, 05004], lr: 0.000342, loss: 0.4236
2022-07-09 06:20:37 - train: epoch 0193, iter [03800, 05004], lr: 0.000340, loss: 0.6136
2022-07-09 06:21:10 - train: epoch 0193, iter [03900, 05004], lr: 0.000338, loss: 0.6621
2022-07-09 06:21:43 - train: epoch 0193, iter [04000, 05004], lr: 0.000336, loss: 0.6680
2022-07-09 06:22:17 - train: epoch 0193, iter [04100, 05004], lr: 0.000334, loss: 0.5298
2022-07-09 06:22:51 - train: epoch 0193, iter [04200, 05004], lr: 0.000332, loss: 0.5592
2022-07-09 06:23:24 - train: epoch 0193, iter [04300, 05004], lr: 0.000331, loss: 0.5001
2022-07-09 06:23:57 - train: epoch 0193, iter [04400, 05004], lr: 0.000329, loss: 0.5129
2022-07-09 06:24:30 - train: epoch 0193, iter [04500, 05004], lr: 0.000327, loss: 0.5150
2022-07-09 06:25:03 - train: epoch 0193, iter [04600, 05004], lr: 0.000325, loss: 0.6757
2022-07-09 06:25:36 - train: epoch 0193, iter [04700, 05004], lr: 0.000323, loss: 0.6322
2022-07-09 06:26:09 - train: epoch 0193, iter [04800, 05004], lr: 0.000321, loss: 0.7155
2022-07-09 06:26:43 - train: epoch 0193, iter [04900, 05004], lr: 0.000320, loss: 0.4299
2022-07-09 06:27:15 - train: epoch 0193, iter [05000, 05004], lr: 0.000318, loss: 0.5109
2022-07-09 06:27:16 - train: epoch 193, train_loss: 0.6052
2022-07-09 06:28:31 - eval: epoch: 193, acc1: 76.696%, acc5: 93.280%, test_loss: 0.9580, per_image_load_time: 2.439ms, per_image_inference_time: 0.451ms
2022-07-09 06:28:31 - until epoch: 193, best_acc1: 76.844%
2022-07-09 06:28:31 - epoch 194 lr: 0.000318
2022-07-09 06:29:11 - train: epoch 0194, iter [00100, 05004], lr: 0.000316, loss: 0.5591
2022-07-09 06:29:43 - train: epoch 0194, iter [00200, 05004], lr: 0.000314, loss: 0.5445
2022-07-09 06:30:16 - train: epoch 0194, iter [00300, 05004], lr: 0.000312, loss: 0.5097
2022-07-09 06:30:50 - train: epoch 0194, iter [00400, 05004], lr: 0.000310, loss: 0.6046
2022-07-09 06:31:23 - train: epoch 0194, iter [00500, 05004], lr: 0.000309, loss: 0.6244
2022-07-09 06:31:57 - train: epoch 0194, iter [00600, 05004], lr: 0.000307, loss: 0.4521
2022-07-09 06:32:30 - train: epoch 0194, iter [00700, 05004], lr: 0.000305, loss: 0.6442
2022-07-09 06:33:03 - train: epoch 0194, iter [00800, 05004], lr: 0.000303, loss: 0.8171
2022-07-09 06:33:37 - train: epoch 0194, iter [00900, 05004], lr: 0.000302, loss: 0.4870
2022-07-09 06:34:10 - train: epoch 0194, iter [01000, 05004], lr: 0.000300, loss: 0.3631
2022-07-09 06:34:44 - train: epoch 0194, iter [01100, 05004], lr: 0.000298, loss: 0.5424
2022-07-09 06:35:17 - train: epoch 0194, iter [01200, 05004], lr: 0.000296, loss: 0.6492
2022-07-09 06:35:50 - train: epoch 0194, iter [01300, 05004], lr: 0.000295, loss: 0.4022
2022-07-09 06:36:24 - train: epoch 0194, iter [01400, 05004], lr: 0.000293, loss: 0.5458
2022-07-09 06:36:58 - train: epoch 0194, iter [01500, 05004], lr: 0.000291, loss: 0.7092
2022-07-09 06:37:30 - train: epoch 0194, iter [01600, 05004], lr: 0.000289, loss: 0.6458
2022-07-09 06:38:04 - train: epoch 0194, iter [01700, 05004], lr: 0.000288, loss: 0.5979
2022-07-09 06:38:38 - train: epoch 0194, iter [01800, 05004], lr: 0.000286, loss: 0.5620
2022-07-09 06:39:11 - train: epoch 0194, iter [01900, 05004], lr: 0.000284, loss: 0.6074
2022-07-09 06:39:44 - train: epoch 0194, iter [02000, 05004], lr: 0.000282, loss: 0.6102
2022-07-09 06:40:18 - train: epoch 0194, iter [02100, 05004], lr: 0.000281, loss: 0.7080
2022-07-09 06:40:52 - train: epoch 0194, iter [02200, 05004], lr: 0.000279, loss: 0.7970
2022-07-09 06:41:25 - train: epoch 0194, iter [02300, 05004], lr: 0.000277, loss: 0.5506
2022-07-09 06:41:59 - train: epoch 0194, iter [02400, 05004], lr: 0.000276, loss: 0.4924
2022-07-09 06:42:31 - train: epoch 0194, iter [02500, 05004], lr: 0.000274, loss: 0.6891
2022-07-09 06:43:05 - train: epoch 0194, iter [02600, 05004], lr: 0.000272, loss: 0.5552
2022-07-09 06:43:39 - train: epoch 0194, iter [02700, 05004], lr: 0.000271, loss: 0.4699
2022-07-09 06:44:12 - train: epoch 0194, iter [02800, 05004], lr: 0.000269, loss: 0.6372
2022-07-09 06:44:46 - train: epoch 0194, iter [02900, 05004], lr: 0.000267, loss: 0.6656
2022-07-09 06:45:19 - train: epoch 0194, iter [03000, 05004], lr: 0.000266, loss: 0.4176
2022-07-09 06:45:52 - train: epoch 0194, iter [03100, 05004], lr: 0.000264, loss: 0.6706
2022-07-09 06:46:26 - train: epoch 0194, iter [03200, 05004], lr: 0.000262, loss: 0.4547
2022-07-09 06:46:59 - train: epoch 0194, iter [03300, 05004], lr: 0.000261, loss: 0.5951
2022-07-09 06:47:33 - train: epoch 0194, iter [03400, 05004], lr: 0.000259, loss: 0.5336
2022-07-09 06:48:06 - train: epoch 0194, iter [03500, 05004], lr: 0.000257, loss: 0.5833
2022-07-09 06:48:40 - train: epoch 0194, iter [03600, 05004], lr: 0.000256, loss: 0.6049
2022-07-09 06:49:13 - train: epoch 0194, iter [03700, 05004], lr: 0.000254, loss: 0.5566
2022-07-09 06:49:47 - train: epoch 0194, iter [03800, 05004], lr: 0.000252, loss: 0.5152
2022-07-09 06:50:21 - train: epoch 0194, iter [03900, 05004], lr: 0.000251, loss: 0.5862
2022-07-09 06:50:55 - train: epoch 0194, iter [04000, 05004], lr: 0.000249, loss: 0.5585
2022-07-09 06:51:29 - train: epoch 0194, iter [04100, 05004], lr: 0.000248, loss: 0.6623
2022-07-09 06:52:02 - train: epoch 0194, iter [04200, 05004], lr: 0.000246, loss: 0.4527
2022-07-09 06:52:35 - train: epoch 0194, iter [04300, 05004], lr: 0.000244, loss: 0.6026
2022-07-09 06:53:08 - train: epoch 0194, iter [04400, 05004], lr: 0.000243, loss: 0.5895
2022-07-09 06:53:43 - train: epoch 0194, iter [04500, 05004], lr: 0.000241, loss: 0.6085
2022-07-09 06:54:17 - train: epoch 0194, iter [04600, 05004], lr: 0.000240, loss: 0.5048
2022-07-09 06:54:50 - train: epoch 0194, iter [04700, 05004], lr: 0.000238, loss: 0.5461
2022-07-09 06:55:24 - train: epoch 0194, iter [04800, 05004], lr: 0.000237, loss: 0.6393
2022-07-09 06:55:57 - train: epoch 0194, iter [04900, 05004], lr: 0.000235, loss: 0.6827
2022-07-09 06:56:30 - train: epoch 0194, iter [05000, 05004], lr: 0.000233, loss: 0.6248
2022-07-09 06:56:31 - train: epoch 194, train_loss: 0.5989
2022-07-09 06:57:44 - eval: epoch: 194, acc1: 76.872%, acc5: 93.350%, test_loss: 0.9526, per_image_load_time: 1.704ms, per_image_inference_time: 0.494ms
2022-07-09 06:57:45 - until epoch: 194, best_acc1: 76.872%
2022-07-09 06:57:45 - epoch 195 lr: 0.000233
2022-07-09 06:58:23 - train: epoch 0195, iter [00100, 05004], lr: 0.000232, loss: 0.7089
2022-07-09 06:58:57 - train: epoch 0195, iter [00200, 05004], lr: 0.000230, loss: 0.5310
2022-07-09 06:59:30 - train: epoch 0195, iter [00300, 05004], lr: 0.000229, loss: 0.5818
2022-07-09 07:00:03 - train: epoch 0195, iter [00400, 05004], lr: 0.000227, loss: 0.5948
2022-07-09 07:00:36 - train: epoch 0195, iter [00500, 05004], lr: 0.000226, loss: 0.7028
2022-07-09 07:01:09 - train: epoch 0195, iter [00600, 05004], lr: 0.000224, loss: 0.7053
2022-07-09 07:01:43 - train: epoch 0195, iter [00700, 05004], lr: 0.000223, loss: 0.8066
2022-07-09 07:02:15 - train: epoch 0195, iter [00800, 05004], lr: 0.000221, loss: 0.6969
2022-07-09 07:02:50 - train: epoch 0195, iter [00900, 05004], lr: 0.000220, loss: 0.4790
2022-07-09 07:03:23 - train: epoch 0195, iter [01000, 05004], lr: 0.000218, loss: 0.5431
2022-07-09 07:03:56 - train: epoch 0195, iter [01100, 05004], lr: 0.000217, loss: 0.5418
2022-07-09 07:04:31 - train: epoch 0195, iter [01200, 05004], lr: 0.000215, loss: 0.5977
2022-07-09 07:05:04 - train: epoch 0195, iter [01300, 05004], lr: 0.000214, loss: 0.5313
2022-07-09 07:05:37 - train: epoch 0195, iter [01400, 05004], lr: 0.000212, loss: 0.4792
2022-07-09 07:06:10 - train: epoch 0195, iter [01500, 05004], lr: 0.000211, loss: 0.4985
2022-07-09 07:06:44 - train: epoch 0195, iter [01600, 05004], lr: 0.000209, loss: 0.7482
2022-07-09 07:07:17 - train: epoch 0195, iter [01700, 05004], lr: 0.000208, loss: 0.4542
2022-07-09 07:07:51 - train: epoch 0195, iter [01800, 05004], lr: 0.000206, loss: 0.6583
2022-07-09 07:08:24 - train: epoch 0195, iter [01900, 05004], lr: 0.000205, loss: 0.6775
2022-07-09 07:08:57 - train: epoch 0195, iter [02000, 05004], lr: 0.000203, loss: 0.6147
2022-07-09 07:09:32 - train: epoch 0195, iter [02100, 05004], lr: 0.000202, loss: 0.6787
2022-07-09 07:10:05 - train: epoch 0195, iter [02200, 05004], lr: 0.000200, loss: 0.5834
2022-07-09 07:10:39 - train: epoch 0195, iter [02300, 05004], lr: 0.000199, loss: 0.6611
2022-07-09 07:11:12 - train: epoch 0195, iter [02400, 05004], lr: 0.000198, loss: 0.5945
2022-07-09 07:11:46 - train: epoch 0195, iter [02500, 05004], lr: 0.000196, loss: 0.6050
2022-07-09 07:12:19 - train: epoch 0195, iter [02600, 05004], lr: 0.000195, loss: 0.4904
2022-07-09 07:12:52 - train: epoch 0195, iter [02700, 05004], lr: 0.000193, loss: 0.6391
2022-07-09 07:13:26 - train: epoch 0195, iter [02800, 05004], lr: 0.000192, loss: 0.5827
2022-07-09 07:14:00 - train: epoch 0195, iter [02900, 05004], lr: 0.000191, loss: 0.5446
2022-07-09 07:14:34 - train: epoch 0195, iter [03000, 05004], lr: 0.000189, loss: 0.6584
2022-07-09 07:15:08 - train: epoch 0195, iter [03100, 05004], lr: 0.000188, loss: 0.6442
2022-07-09 07:15:41 - train: epoch 0195, iter [03200, 05004], lr: 0.000186, loss: 0.5419
2022-07-09 07:16:14 - train: epoch 0195, iter [03300, 05004], lr: 0.000185, loss: 0.5626
2022-07-09 07:16:48 - train: epoch 0195, iter [03400, 05004], lr: 0.000184, loss: 0.6560
2022-07-09 07:17:21 - train: epoch 0195, iter [03500, 05004], lr: 0.000182, loss: 0.5421
2022-07-09 07:17:54 - train: epoch 0195, iter [03600, 05004], lr: 0.000181, loss: 0.5922
2022-07-09 07:18:28 - train: epoch 0195, iter [03700, 05004], lr: 0.000179, loss: 0.7668
2022-07-09 07:19:01 - train: epoch 0195, iter [03800, 05004], lr: 0.000178, loss: 0.5114
2022-07-09 07:19:35 - train: epoch 0195, iter [03900, 05004], lr: 0.000177, loss: 0.6569
2022-07-09 07:20:08 - train: epoch 0195, iter [04000, 05004], lr: 0.000175, loss: 0.5677
2022-07-09 07:20:42 - train: epoch 0195, iter [04100, 05004], lr: 0.000174, loss: 0.6118
2022-07-09 07:21:15 - train: epoch 0195, iter [04200, 05004], lr: 0.000173, loss: 0.6053
2022-07-09 07:21:49 - train: epoch 0195, iter [04300, 05004], lr: 0.000171, loss: 0.6311
2022-07-09 07:22:23 - train: epoch 0195, iter [04400, 05004], lr: 0.000170, loss: 0.5639
2022-07-09 07:22:57 - train: epoch 0195, iter [04500, 05004], lr: 0.000169, loss: 0.5650
2022-07-09 07:23:31 - train: epoch 0195, iter [04600, 05004], lr: 0.000167, loss: 0.5287
2022-07-09 07:24:05 - train: epoch 0195, iter [04700, 05004], lr: 0.000166, loss: 0.6810
2022-07-09 07:24:38 - train: epoch 0195, iter [04800, 05004], lr: 0.000165, loss: 0.7099
2022-07-09 07:25:11 - train: epoch 0195, iter [04900, 05004], lr: 0.000163, loss: 0.6533
2022-07-09 07:25:45 - train: epoch 0195, iter [05000, 05004], lr: 0.000162, loss: 0.4591
2022-07-09 07:25:46 - train: epoch 195, train_loss: 0.5978
2022-07-09 07:27:00 - eval: epoch: 195, acc1: 76.934%, acc5: 93.342%, test_loss: 0.9550, per_image_load_time: 2.377ms, per_image_inference_time: 0.477ms
2022-07-09 07:27:01 - until epoch: 195, best_acc1: 76.934%
2022-07-09 07:27:01 - epoch 196 lr: 0.000162
2022-07-09 07:27:39 - train: epoch 0196, iter [00100, 05004], lr: 0.000161, loss: 0.6309
2022-07-09 07:28:13 - train: epoch 0196, iter [00200, 05004], lr: 0.000160, loss: 0.4825
2022-07-09 07:28:46 - train: epoch 0196, iter [00300, 05004], lr: 0.000158, loss: 0.5661
2022-07-09 07:29:19 - train: epoch 0196, iter [00400, 05004], lr: 0.000157, loss: 0.6202
2022-07-09 07:29:54 - train: epoch 0196, iter [00500, 05004], lr: 0.000156, loss: 0.7463
2022-07-09 07:30:27 - train: epoch 0196, iter [00600, 05004], lr: 0.000154, loss: 0.4838
2022-07-09 07:31:01 - train: epoch 0196, iter [00700, 05004], lr: 0.000153, loss: 0.5987
2022-07-09 07:31:34 - train: epoch 0196, iter [00800, 05004], lr: 0.000152, loss: 0.5616
2022-07-09 07:32:08 - train: epoch 0196, iter [00900, 05004], lr: 0.000151, loss: 0.5176
2022-07-09 07:32:41 - train: epoch 0196, iter [01000, 05004], lr: 0.000149, loss: 0.7407
2022-07-09 07:33:15 - train: epoch 0196, iter [01100, 05004], lr: 0.000148, loss: 0.6883
2022-07-09 07:33:49 - train: epoch 0196, iter [01200, 05004], lr: 0.000147, loss: 0.6619
2022-07-09 07:34:22 - train: epoch 0196, iter [01300, 05004], lr: 0.000146, loss: 0.6966
2022-07-09 07:34:56 - train: epoch 0196, iter [01400, 05004], lr: 0.000145, loss: 0.5833
2022-07-09 07:35:29 - train: epoch 0196, iter [01500, 05004], lr: 0.000143, loss: 0.6774
2022-07-09 07:36:03 - train: epoch 0196, iter [01600, 05004], lr: 0.000142, loss: 0.6269
2022-07-09 07:36:36 - train: epoch 0196, iter [01700, 05004], lr: 0.000141, loss: 0.6269
2022-07-09 07:37:11 - train: epoch 0196, iter [01800, 05004], lr: 0.000140, loss: 0.6120
2022-07-09 07:37:44 - train: epoch 0196, iter [01900, 05004], lr: 0.000138, loss: 0.6394
2022-07-09 07:38:18 - train: epoch 0196, iter [02000, 05004], lr: 0.000137, loss: 0.8155
2022-07-09 07:38:51 - train: epoch 0196, iter [02100, 05004], lr: 0.000136, loss: 0.6670
2022-07-09 07:39:25 - train: epoch 0196, iter [02200, 05004], lr: 0.000135, loss: 0.6045
2022-07-09 07:39:58 - train: epoch 0196, iter [02300, 05004], lr: 0.000134, loss: 0.5120
2022-07-09 07:40:32 - train: epoch 0196, iter [02400, 05004], lr: 0.000133, loss: 0.5664
2022-07-09 07:41:06 - train: epoch 0196, iter [02500, 05004], lr: 0.000131, loss: 0.6268
2022-07-09 07:41:40 - train: epoch 0196, iter [02600, 05004], lr: 0.000130, loss: 0.4921
2022-07-09 07:42:14 - train: epoch 0196, iter [02700, 05004], lr: 0.000129, loss: 0.7048
2022-07-09 07:42:47 - train: epoch 0196, iter [02800, 05004], lr: 0.000128, loss: 0.5766
2022-07-09 07:43:21 - train: epoch 0196, iter [02900, 05004], lr: 0.000127, loss: 0.6355
2022-07-09 07:43:55 - train: epoch 0196, iter [03000, 05004], lr: 0.000126, loss: 0.6504
2022-07-09 07:44:28 - train: epoch 0196, iter [03100, 05004], lr: 0.000124, loss: 0.5483
2022-07-09 07:45:02 - train: epoch 0196, iter [03200, 05004], lr: 0.000123, loss: 0.5566
2022-07-09 07:45:36 - train: epoch 0196, iter [03300, 05004], lr: 0.000122, loss: 0.4926
2022-07-09 07:46:10 - train: epoch 0196, iter [03400, 05004], lr: 0.000121, loss: 0.5537
2022-07-09 07:46:43 - train: epoch 0196, iter [03500, 05004], lr: 0.000120, loss: 0.6198
2022-07-09 07:47:17 - train: epoch 0196, iter [03600, 05004], lr: 0.000119, loss: 0.5015
2022-07-09 07:47:51 - train: epoch 0196, iter [03700, 05004], lr: 0.000118, loss: 0.6129
2022-07-09 07:48:24 - train: epoch 0196, iter [03800, 05004], lr: 0.000117, loss: 0.5977
2022-07-09 07:48:58 - train: epoch 0196, iter [03900, 05004], lr: 0.000116, loss: 0.4570
2022-07-09 07:49:32 - train: epoch 0196, iter [04000, 05004], lr: 0.000114, loss: 0.5285
2022-07-09 07:50:07 - train: epoch 0196, iter [04100, 05004], lr: 0.000113, loss: 0.6180
2022-07-09 07:50:40 - train: epoch 0196, iter [04200, 05004], lr: 0.000112, loss: 0.5294
2022-07-09 07:51:15 - train: epoch 0196, iter [04300, 05004], lr: 0.000111, loss: 0.7211
2022-07-09 07:51:48 - train: epoch 0196, iter [04400, 05004], lr: 0.000110, loss: 0.6379
2022-07-09 07:52:22 - train: epoch 0196, iter [04500, 05004], lr: 0.000109, loss: 0.6786
2022-07-09 07:52:56 - train: epoch 0196, iter [04600, 05004], lr: 0.000108, loss: 0.6519
2022-07-09 07:53:29 - train: epoch 0196, iter [04700, 05004], lr: 0.000107, loss: 0.5723
2022-07-09 07:54:03 - train: epoch 0196, iter [04800, 05004], lr: 0.000106, loss: 0.6360
2022-07-09 07:54:37 - train: epoch 0196, iter [04900, 05004], lr: 0.000105, loss: 0.6403
2022-07-09 07:55:10 - train: epoch 0196, iter [05000, 05004], lr: 0.000104, loss: 0.5247
2022-07-09 07:55:11 - train: epoch 196, train_loss: 0.5936
2022-07-09 07:56:26 - eval: epoch: 196, acc1: 76.900%, acc5: 93.288%, test_loss: 0.9534, per_image_load_time: 2.263ms, per_image_inference_time: 0.482ms
2022-07-09 07:56:26 - until epoch: 196, best_acc1: 76.934%
2022-07-09 07:56:26 - epoch 197 lr: 0.000104
2022-07-09 07:57:04 - train: epoch 0197, iter [00100, 05004], lr: 0.000103, loss: 0.6702
2022-07-09 07:57:37 - train: epoch 0197, iter [00200, 05004], lr: 0.000102, loss: 0.5016
2022-07-09 07:58:10 - train: epoch 0197, iter [00300, 05004], lr: 0.000101, loss: 0.5581
2022-07-09 07:58:42 - train: epoch 0197, iter [00400, 05004], lr: 0.000100, loss: 0.6221
2022-07-09 07:59:15 - train: epoch 0197, iter [00500, 05004], lr: 0.000099, loss: 0.5025
2022-07-09 07:59:49 - train: epoch 0197, iter [00600, 05004], lr: 0.000098, loss: 0.6330
2022-07-09 08:00:22 - train: epoch 0197, iter [00700, 05004], lr: 0.000097, loss: 0.5260
2022-07-09 08:00:55 - train: epoch 0197, iter [00800, 05004], lr: 0.000096, loss: 0.5705
2022-07-09 08:01:28 - train: epoch 0197, iter [00900, 05004], lr: 0.000095, loss: 0.5998
2022-07-09 08:02:02 - train: epoch 0197, iter [01000, 05004], lr: 0.000094, loss: 0.6983
2022-07-09 08:02:35 - train: epoch 0197, iter [01100, 05004], lr: 0.000093, loss: 0.5938
2022-07-09 08:03:08 - train: epoch 0197, iter [01200, 05004], lr: 0.000092, loss: 0.6454
2022-07-09 08:03:41 - train: epoch 0197, iter [01300, 05004], lr: 0.000091, loss: 0.6397
2022-07-09 08:04:15 - train: epoch 0197, iter [01400, 05004], lr: 0.000090, loss: 0.5983
2022-07-09 08:04:49 - train: epoch 0197, iter [01500, 05004], lr: 0.000089, loss: 0.5812
2022-07-09 08:05:22 - train: epoch 0197, iter [01600, 05004], lr: 0.000088, loss: 0.5432
2022-07-09 08:05:56 - train: epoch 0197, iter [01700, 05004], lr: 0.000087, loss: 0.5401
2022-07-09 08:06:28 - train: epoch 0197, iter [01800, 05004], lr: 0.000086, loss: 0.5168
2022-07-09 08:07:01 - train: epoch 0197, iter [01900, 05004], lr: 0.000085, loss: 0.6248
2022-07-09 08:07:35 - train: epoch 0197, iter [02000, 05004], lr: 0.000084, loss: 0.6536
2022-07-09 08:08:08 - train: epoch 0197, iter [02100, 05004], lr: 0.000083, loss: 0.7510
2022-07-09 08:08:41 - train: epoch 0197, iter [02200, 05004], lr: 0.000082, loss: 0.5412
2022-07-09 08:09:14 - train: epoch 0197, iter [02300, 05004], lr: 0.000081, loss: 0.6631
2022-07-09 08:09:47 - train: epoch 0197, iter [02400, 05004], lr: 0.000080, loss: 0.5684
2022-07-09 08:10:20 - train: epoch 0197, iter [02500, 05004], lr: 0.000079, loss: 0.5322
2022-07-09 08:10:53 - train: epoch 0197, iter [02600, 05004], lr: 0.000079, loss: 0.5385
2022-07-09 08:11:28 - train: epoch 0197, iter [02700, 05004], lr: 0.000078, loss: 0.6475
2022-07-09 08:12:00 - train: epoch 0197, iter [02800, 05004], lr: 0.000077, loss: 0.6218
2022-07-09 08:12:34 - train: epoch 0197, iter [02900, 05004], lr: 0.000076, loss: 0.5310
2022-07-09 08:13:07 - train: epoch 0197, iter [03000, 05004], lr: 0.000075, loss: 0.6288
2022-07-09 08:13:40 - train: epoch 0197, iter [03100, 05004], lr: 0.000074, loss: 0.6252
2022-07-09 08:14:14 - train: epoch 0197, iter [03200, 05004], lr: 0.000073, loss: 0.4478
2022-07-09 08:14:47 - train: epoch 0197, iter [03300, 05004], lr: 0.000072, loss: 0.6525
2022-07-09 08:15:22 - train: epoch 0197, iter [03400, 05004], lr: 0.000072, loss: 0.6107
2022-07-09 08:15:55 - train: epoch 0197, iter [03500, 05004], lr: 0.000071, loss: 0.6414
2022-07-09 08:16:28 - train: epoch 0197, iter [03600, 05004], lr: 0.000070, loss: 0.5292
2022-07-09 08:17:02 - train: epoch 0197, iter [03700, 05004], lr: 0.000069, loss: 0.5112
2022-07-09 08:17:35 - train: epoch 0197, iter [03800, 05004], lr: 0.000068, loss: 0.5726
2022-07-09 08:18:09 - train: epoch 0197, iter [03900, 05004], lr: 0.000067, loss: 0.6759
2022-07-09 08:18:42 - train: epoch 0197, iter [04000, 05004], lr: 0.000066, loss: 0.5829
2022-07-09 08:19:16 - train: epoch 0197, iter [04100, 05004], lr: 0.000066, loss: 0.5213
2022-07-09 08:19:50 - train: epoch 0197, iter [04200, 05004], lr: 0.000065, loss: 0.5599
2022-07-09 08:20:23 - train: epoch 0197, iter [04300, 05004], lr: 0.000064, loss: 0.6830
2022-07-09 08:20:57 - train: epoch 0197, iter [04400, 05004], lr: 0.000063, loss: 0.7913
2022-07-09 08:21:30 - train: epoch 0197, iter [04500, 05004], lr: 0.000062, loss: 0.6061
2022-07-09 08:22:03 - train: epoch 0197, iter [04600, 05004], lr: 0.000062, loss: 0.5935
2022-07-09 08:22:37 - train: epoch 0197, iter [04700, 05004], lr: 0.000061, loss: 0.6442
2022-07-09 08:23:11 - train: epoch 0197, iter [04800, 05004], lr: 0.000060, loss: 0.5339
2022-07-09 08:23:43 - train: epoch 0197, iter [04900, 05004], lr: 0.000059, loss: 0.5976
2022-07-09 08:24:16 - train: epoch 0197, iter [05000, 05004], lr: 0.000058, loss: 0.4654
2022-07-09 08:24:18 - train: epoch 197, train_loss: 0.5912
2022-07-09 08:25:32 - eval: epoch: 197, acc1: 76.940%, acc5: 93.380%, test_loss: 0.9543, per_image_load_time: 1.489ms, per_image_inference_time: 0.473ms
2022-07-09 08:25:33 - until epoch: 197, best_acc1: 76.940%
2022-07-09 08:25:33 - epoch 198 lr: 0.000058
2022-07-09 08:26:12 - train: epoch 0198, iter [00100, 05004], lr: 0.000058, loss: 0.4641
2022-07-09 08:26:45 - train: epoch 0198, iter [00200, 05004], lr: 0.000057, loss: 0.6060
2022-07-09 08:27:18 - train: epoch 0198, iter [00300, 05004], lr: 0.000056, loss: 0.5100
2022-07-09 08:27:51 - train: epoch 0198, iter [00400, 05004], lr: 0.000055, loss: 0.5190
2022-07-09 08:28:24 - train: epoch 0198, iter [00500, 05004], lr: 0.000055, loss: 0.6161
2022-07-09 08:28:58 - train: epoch 0198, iter [00600, 05004], lr: 0.000054, loss: 0.4516
2022-07-09 08:29:31 - train: epoch 0198, iter [00700, 05004], lr: 0.000053, loss: 0.6295
2022-07-09 08:30:05 - train: epoch 0198, iter [00800, 05004], lr: 0.000052, loss: 0.6017
2022-07-09 08:30:38 - train: epoch 0198, iter [00900, 05004], lr: 0.000052, loss: 0.6313
2022-07-09 08:31:12 - train: epoch 0198, iter [01000, 05004], lr: 0.000051, loss: 0.6072
2022-07-09 08:31:46 - train: epoch 0198, iter [01100, 05004], lr: 0.000050, loss: 0.5299
2022-07-09 08:32:20 - train: epoch 0198, iter [01200, 05004], lr: 0.000049, loss: 0.8310
2022-07-09 08:32:53 - train: epoch 0198, iter [01300, 05004], lr: 0.000049, loss: 0.5136
2022-07-09 08:33:27 - train: epoch 0198, iter [01400, 05004], lr: 0.000048, loss: 0.6382
2022-07-09 08:34:00 - train: epoch 0198, iter [01500, 05004], lr: 0.000047, loss: 0.5346
2022-07-09 08:34:34 - train: epoch 0198, iter [01600, 05004], lr: 0.000047, loss: 0.6892
2022-07-09 08:35:07 - train: epoch 0198, iter [01700, 05004], lr: 0.000046, loss: 0.5059
2022-07-09 08:35:41 - train: epoch 0198, iter [01800, 05004], lr: 0.000045, loss: 0.5076
2022-07-09 08:36:15 - train: epoch 0198, iter [01900, 05004], lr: 0.000045, loss: 0.6407
2022-07-09 08:36:49 - train: epoch 0198, iter [02000, 05004], lr: 0.000044, loss: 0.6309
2022-07-09 08:37:21 - train: epoch 0198, iter [02100, 05004], lr: 0.000043, loss: 0.5293
2022-07-09 08:37:55 - train: epoch 0198, iter [02200, 05004], lr: 0.000043, loss: 0.5131
2022-07-09 08:38:28 - train: epoch 0198, iter [02300, 05004], lr: 0.000042, loss: 0.5173
2022-07-09 08:39:02 - train: epoch 0198, iter [02400, 05004], lr: 0.000041, loss: 0.5034
2022-07-09 08:39:36 - train: epoch 0198, iter [02500, 05004], lr: 0.000041, loss: 0.7087
2022-07-09 08:40:08 - train: epoch 0198, iter [02600, 05004], lr: 0.000040, loss: 0.5776
2022-07-09 08:40:43 - train: epoch 0198, iter [02700, 05004], lr: 0.000039, loss: 0.8497
2022-07-09 08:41:16 - train: epoch 0198, iter [02800, 05004], lr: 0.000039, loss: 0.6035
2022-07-09 08:41:50 - train: epoch 0198, iter [02900, 05004], lr: 0.000038, loss: 0.5982
2022-07-09 08:42:24 - train: epoch 0198, iter [03000, 05004], lr: 0.000037, loss: 0.6612
2022-07-09 08:42:58 - train: epoch 0198, iter [03100, 05004], lr: 0.000037, loss: 0.7198
2022-07-09 08:43:32 - train: epoch 0198, iter [03200, 05004], lr: 0.000036, loss: 0.5404
2022-07-09 08:44:05 - train: epoch 0198, iter [03300, 05004], lr: 0.000036, loss: 0.5016
2022-07-09 08:44:39 - train: epoch 0198, iter [03400, 05004], lr: 0.000035, loss: 0.4742
2022-07-09 08:45:13 - train: epoch 0198, iter [03500, 05004], lr: 0.000034, loss: 0.8152
2022-07-09 08:45:47 - train: epoch 0198, iter [03600, 05004], lr: 0.000034, loss: 0.5354
2022-07-09 08:46:21 - train: epoch 0198, iter [03700, 05004], lr: 0.000033, loss: 0.6165
2022-07-09 08:46:55 - train: epoch 0198, iter [03800, 05004], lr: 0.000033, loss: 0.6788
2022-07-09 08:47:29 - train: epoch 0198, iter [03900, 05004], lr: 0.000032, loss: 0.6873
2022-07-09 08:48:02 - train: epoch 0198, iter [04000, 05004], lr: 0.000031, loss: 0.5796
2022-07-09 08:48:35 - train: epoch 0198, iter [04100, 05004], lr: 0.000031, loss: 0.6124
2022-07-09 08:49:09 - train: epoch 0198, iter [04200, 05004], lr: 0.000030, loss: 0.5113
2022-07-09 08:49:42 - train: epoch 0198, iter [04300, 05004], lr: 0.000030, loss: 0.5220
2022-07-09 08:50:16 - train: epoch 0198, iter [04400, 05004], lr: 0.000029, loss: 0.6864
2022-07-09 08:50:49 - train: epoch 0198, iter [04500, 05004], lr: 0.000029, loss: 0.6111
2022-07-09 08:51:22 - train: epoch 0198, iter [04600, 05004], lr: 0.000028, loss: 0.5274
2022-07-09 08:51:57 - train: epoch 0198, iter [04700, 05004], lr: 0.000028, loss: 0.5465
2022-07-09 08:52:29 - train: epoch 0198, iter [04800, 05004], lr: 0.000027, loss: 0.6984
2022-07-09 08:53:04 - train: epoch 0198, iter [04900, 05004], lr: 0.000026, loss: 0.6167
2022-07-09 08:53:36 - train: epoch 0198, iter [05000, 05004], lr: 0.000026, loss: 0.5877
2022-07-09 08:53:37 - train: epoch 198, train_loss: 0.5897
2022-07-09 08:54:52 - eval: epoch: 198, acc1: 76.942%, acc5: 93.394%, test_loss: 0.9540, per_image_load_time: 1.307ms, per_image_inference_time: 0.480ms
2022-07-09 08:54:52 - until epoch: 198, best_acc1: 76.942%
2022-07-09 08:54:52 - epoch 199 lr: 0.000026
2022-07-09 08:55:31 - train: epoch 0199, iter [00100, 05004], lr: 0.000025, loss: 0.5075
2022-07-09 08:56:04 - train: epoch 0199, iter [00200, 05004], lr: 0.000025, loss: 0.4974
2022-07-09 08:56:37 - train: epoch 0199, iter [00300, 05004], lr: 0.000024, loss: 0.6693
2022-07-09 08:57:11 - train: epoch 0199, iter [00400, 05004], lr: 0.000024, loss: 0.5541
2022-07-09 08:57:44 - train: epoch 0199, iter [00500, 05004], lr: 0.000023, loss: 0.4849
2022-07-09 08:58:17 - train: epoch 0199, iter [00600, 05004], lr: 0.000023, loss: 0.5883
2022-07-09 08:58:50 - train: epoch 0199, iter [00700, 05004], lr: 0.000022, loss: 0.5141
2022-07-09 08:59:24 - train: epoch 0199, iter [00800, 05004], lr: 0.000022, loss: 0.5051
2022-07-09 08:59:58 - train: epoch 0199, iter [00900, 05004], lr: 0.000021, loss: 0.6761
2022-07-09 09:00:31 - train: epoch 0199, iter [01000, 05004], lr: 0.000021, loss: 0.6958
2022-07-09 09:01:05 - train: epoch 0199, iter [01100, 05004], lr: 0.000021, loss: 0.5757
2022-07-09 09:01:38 - train: epoch 0199, iter [01200, 05004], lr: 0.000020, loss: 0.5041
2022-07-09 09:02:11 - train: epoch 0199, iter [01300, 05004], lr: 0.000020, loss: 0.6403
2022-07-09 09:02:45 - train: epoch 0199, iter [01400, 05004], lr: 0.000019, loss: 0.6615
2022-07-09 09:03:18 - train: epoch 0199, iter [01500, 05004], lr: 0.000019, loss: 0.6818
2022-07-09 09:03:52 - train: epoch 0199, iter [01600, 05004], lr: 0.000018, loss: 0.5775
2022-07-09 09:04:25 - train: epoch 0199, iter [01700, 05004], lr: 0.000018, loss: 0.5806
2022-07-09 09:04:59 - train: epoch 0199, iter [01800, 05004], lr: 0.000017, loss: 0.6927
2022-07-09 09:05:32 - train: epoch 0199, iter [01900, 05004], lr: 0.000017, loss: 0.5287
2022-07-09 09:06:05 - train: epoch 0199, iter [02000, 05004], lr: 0.000017, loss: 0.7149
2022-07-09 09:06:37 - train: epoch 0199, iter [02100, 05004], lr: 0.000016, loss: 0.5194
2022-07-09 09:07:11 - train: epoch 0199, iter [02200, 05004], lr: 0.000016, loss: 0.5340
2022-07-09 09:07:44 - train: epoch 0199, iter [02300, 05004], lr: 0.000015, loss: 0.4408
2022-07-09 09:08:18 - train: epoch 0199, iter [02400, 05004], lr: 0.000015, loss: 0.5059
2022-07-09 09:08:51 - train: epoch 0199, iter [02500, 05004], lr: 0.000015, loss: 0.5481
2022-07-09 09:09:24 - train: epoch 0199, iter [02600, 05004], lr: 0.000014, loss: 0.5702
2022-07-09 09:09:58 - train: epoch 0199, iter [02700, 05004], lr: 0.000014, loss: 0.7711
2022-07-09 09:10:31 - train: epoch 0199, iter [02800, 05004], lr: 0.000013, loss: 0.5604
2022-07-09 09:11:05 - train: epoch 0199, iter [02900, 05004], lr: 0.000013, loss: 0.4638
2022-07-09 09:11:39 - train: epoch 0199, iter [03000, 05004], lr: 0.000013, loss: 0.4929
2022-07-09 09:12:12 - train: epoch 0199, iter [03100, 05004], lr: 0.000012, loss: 0.6285
2022-07-09 09:12:46 - train: epoch 0199, iter [03200, 05004], lr: 0.000012, loss: 0.5473
2022-07-09 09:13:19 - train: epoch 0199, iter [03300, 05004], lr: 0.000012, loss: 0.4955
2022-07-09 09:13:52 - train: epoch 0199, iter [03400, 05004], lr: 0.000011, loss: 0.6036
2022-07-09 09:14:25 - train: epoch 0199, iter [03500, 05004], lr: 0.000011, loss: 0.6347
2022-07-09 09:14:59 - train: epoch 0199, iter [03600, 05004], lr: 0.000011, loss: 0.6243
2022-07-09 09:15:32 - train: epoch 0199, iter [03700, 05004], lr: 0.000010, loss: 0.6304
2022-07-09 09:16:05 - train: epoch 0199, iter [03800, 05004], lr: 0.000010, loss: 0.5770
2022-07-09 09:16:38 - train: epoch 0199, iter [03900, 05004], lr: 0.000010, loss: 0.5703
2022-07-09 09:17:11 - train: epoch 0199, iter [04000, 05004], lr: 0.000009, loss: 0.4986
2022-07-09 09:17:45 - train: epoch 0199, iter [04100, 05004], lr: 0.000009, loss: 0.7990
2022-07-09 09:18:18 - train: epoch 0199, iter [04200, 05004], lr: 0.000009, loss: 0.5425
2022-07-09 09:18:50 - train: epoch 0199, iter [04300, 05004], lr: 0.000008, loss: 0.5696
2022-07-09 09:19:24 - train: epoch 0199, iter [04400, 05004], lr: 0.000008, loss: 0.5626
2022-07-09 09:19:57 - train: epoch 0199, iter [04500, 05004], lr: 0.000008, loss: 0.5724
2022-07-09 09:20:30 - train: epoch 0199, iter [04600, 05004], lr: 0.000008, loss: 0.6209
2022-07-09 09:21:03 - train: epoch 0199, iter [04700, 05004], lr: 0.000007, loss: 0.5963
2022-07-09 09:21:36 - train: epoch 0199, iter [04800, 05004], lr: 0.000007, loss: 0.6297
2022-07-09 09:22:09 - train: epoch 0199, iter [04900, 05004], lr: 0.000007, loss: 0.6436
2022-07-09 09:22:40 - train: epoch 0199, iter [05000, 05004], lr: 0.000006, loss: 0.6044
2022-07-09 09:22:42 - train: epoch 199, train_loss: 0.5882
2022-07-09 09:23:55 - eval: epoch: 199, acc1: 76.864%, acc5: 93.356%, test_loss: 0.9548, per_image_load_time: 1.069ms, per_image_inference_time: 0.456ms
2022-07-09 09:23:55 - until epoch: 199, best_acc1: 76.942%
2022-07-09 09:23:55 - epoch 200 lr: 0.000006
2022-07-09 09:24:33 - train: epoch 0200, iter [00100, 05004], lr: 0.000006, loss: 0.5281
2022-07-09 09:25:07 - train: epoch 0200, iter [00200, 05004], lr: 0.000006, loss: 0.6338
2022-07-09 09:25:39 - train: epoch 0200, iter [00300, 05004], lr: 0.000006, loss: 0.7370
2022-07-09 09:26:11 - train: epoch 0200, iter [00400, 05004], lr: 0.000005, loss: 0.6764
2022-07-09 09:26:44 - train: epoch 0200, iter [00500, 05004], lr: 0.000005, loss: 0.6454
2022-07-09 09:27:16 - train: epoch 0200, iter [00600, 05004], lr: 0.000005, loss: 0.5823
2022-07-09 09:27:49 - train: epoch 0200, iter [00700, 05004], lr: 0.000005, loss: 0.5467
2022-07-09 09:28:22 - train: epoch 0200, iter [00800, 05004], lr: 0.000005, loss: 0.5420
2022-07-09 09:28:54 - train: epoch 0200, iter [00900, 05004], lr: 0.000004, loss: 0.4920
2022-07-09 09:29:27 - train: epoch 0200, iter [01000, 05004], lr: 0.000004, loss: 0.5079
2022-07-09 09:30:00 - train: epoch 0200, iter [01100, 05004], lr: 0.000004, loss: 0.5708
2022-07-09 09:30:33 - train: epoch 0200, iter [01200, 05004], lr: 0.000004, loss: 0.6862
2022-07-09 09:31:06 - train: epoch 0200, iter [01300, 05004], lr: 0.000004, loss: 0.4746
2022-07-09 09:31:39 - train: epoch 0200, iter [01400, 05004], lr: 0.000003, loss: 0.6032
2022-07-09 09:32:12 - train: epoch 0200, iter [01500, 05004], lr: 0.000003, loss: 0.6044
2022-07-09 09:32:45 - train: epoch 0200, iter [01600, 05004], lr: 0.000003, loss: 0.6145
2022-07-09 09:33:18 - train: epoch 0200, iter [01700, 05004], lr: 0.000003, loss: 0.5355
2022-07-09 09:33:51 - train: epoch 0200, iter [01800, 05004], lr: 0.000003, loss: 0.6273
2022-07-09 09:34:24 - train: epoch 0200, iter [01900, 05004], lr: 0.000002, loss: 0.5659
2022-07-09 09:34:58 - train: epoch 0200, iter [02000, 05004], lr: 0.000002, loss: 0.6259
2022-07-09 09:35:30 - train: epoch 0200, iter [02100, 05004], lr: 0.000002, loss: 0.6261
2022-07-09 09:36:04 - train: epoch 0200, iter [02200, 05004], lr: 0.000002, loss: 0.5986
2022-07-09 09:36:37 - train: epoch 0200, iter [02300, 05004], lr: 0.000002, loss: 0.7527
2022-07-09 09:37:11 - train: epoch 0200, iter [02400, 05004], lr: 0.000002, loss: 0.5677
2022-07-09 09:37:44 - train: epoch 0200, iter [02500, 05004], lr: 0.000002, loss: 0.5288
2022-07-09 09:38:17 - train: epoch 0200, iter [02600, 05004], lr: 0.000001, loss: 0.5290
2022-07-09 09:38:51 - train: epoch 0200, iter [02700, 05004], lr: 0.000001, loss: 0.4848
2022-07-09 09:39:24 - train: epoch 0200, iter [02800, 05004], lr: 0.000001, loss: 0.6284
2022-07-09 09:39:56 - train: epoch 0200, iter [02900, 05004], lr: 0.000001, loss: 0.5573
2022-07-09 09:40:30 - train: epoch 0200, iter [03000, 05004], lr: 0.000001, loss: 0.5138
2022-07-09 09:41:03 - train: epoch 0200, iter [03100, 05004], lr: 0.000001, loss: 0.6116
2022-07-09 09:41:36 - train: epoch 0200, iter [03200, 05004], lr: 0.000001, loss: 0.6610
2022-07-09 09:42:10 - train: epoch 0200, iter [03300, 05004], lr: 0.000001, loss: 0.6501
2022-07-09 09:42:43 - train: epoch 0200, iter [03400, 05004], lr: 0.000001, loss: 0.4325
2022-07-09 09:43:16 - train: epoch 0200, iter [03500, 05004], lr: 0.000001, loss: 0.4912
2022-07-09 09:43:50 - train: epoch 0200, iter [03600, 05004], lr: 0.000001, loss: 0.6340
2022-07-09 09:44:23 - train: epoch 0200, iter [03700, 05004], lr: 0.000000, loss: 0.6351
2022-07-09 09:44:56 - train: epoch 0200, iter [03800, 05004], lr: 0.000000, loss: 0.5729
2022-07-09 09:45:30 - train: epoch 0200, iter [03900, 05004], lr: 0.000000, loss: 0.5301
2022-07-09 09:46:03 - train: epoch 0200, iter [04000, 05004], lr: 0.000000, loss: 0.6503
2022-07-09 09:46:36 - train: epoch 0200, iter [04100, 05004], lr: 0.000000, loss: 0.4645
2022-07-09 09:47:09 - train: epoch 0200, iter [04200, 05004], lr: 0.000000, loss: 0.5948
2022-07-09 09:47:43 - train: epoch 0200, iter [04300, 05004], lr: 0.000000, loss: 0.6581
2022-07-09 09:48:17 - train: epoch 0200, iter [04400, 05004], lr: 0.000000, loss: 0.5577
2022-07-09 09:48:50 - train: epoch 0200, iter [04500, 05004], lr: 0.000000, loss: 0.5940
2022-07-09 09:49:23 - train: epoch 0200, iter [04600, 05004], lr: 0.000000, loss: 0.5465
2022-07-09 09:49:56 - train: epoch 0200, iter [04700, 05004], lr: 0.000000, loss: 0.5899
2022-07-09 09:50:30 - train: epoch 0200, iter [04800, 05004], lr: 0.000000, loss: 0.6768
2022-07-09 09:51:03 - train: epoch 0200, iter [04900, 05004], lr: 0.000000, loss: 0.7152
2022-07-09 09:51:35 - train: epoch 0200, iter [05000, 05004], lr: 0.000000, loss: 0.6883
2022-07-09 09:51:37 - train: epoch 200, train_loss: 0.5870
2022-07-09 09:52:50 - eval: epoch: 200, acc1: 76.986%, acc5: 93.350%, test_loss: 0.9533, per_image_load_time: 1.055ms, per_image_inference_time: 0.467ms
2022-07-09 09:52:51 - until epoch: 200, best_acc1: 76.986%
2022-07-09 09:52:51 - train done. model: resnet50, train time: 98.038 hours, best_acc1: 76.986%
