2022-08-26 05:07:12 - network: resnet50
2022-08-26 05:07:12 - num_classes: 1000
2022-08-26 05:07:12 - input_image_size: 224
2022-08-26 05:07:12 - scale: 1.1428571428571428
2022-08-26 05:07:12 - trained_model_path: 
2022-08-26 05:07:12 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-26 05:07:12 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-26 05:07:12 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f07bd3a9820>
2022-08-26 05:07:12 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f07bd3a9ac0>
2022-08-26 05:07:12 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f07bd3a9af0>
2022-08-26 05:07:12 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f07bd3a9b80>
2022-08-26 05:07:12 - seed: 0
2022-08-26 05:07:12 - batch_size: 256
2022-08-26 05:07:12 - num_workers: 16
2022-08-26 05:07:12 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-26 05:07:12 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-26 05:07:12 - epochs: 100
2022-08-26 05:07:12 - print_interval: 100
2022-08-26 05:07:12 - accumulation_steps: 1
2022-08-26 05:07:12 - sync_bn: False
2022-08-26 05:07:12 - apex: True
2022-08-26 05:07:12 - use_ema_model: False
2022-08-26 05:07:12 - ema_model_decay: 0.9999
2022-08-26 05:07:12 - gpus_type: NVIDIA RTX A5000
2022-08-26 05:07:12 - gpus_num: 2
2022-08-26 05:07:12 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f079910c4b0>
2022-08-26 05:07:12 - --------------------parameters--------------------
2022-08-26 05:07:12 - name: conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-26 05:07:12 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-26 05:07:12 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-26 05:07:12 - name: fc.weight, grad: True
2022-08-26 05:07:12 - name: fc.bias, grad: True
2022-08-26 05:07:12 - --------------------buffers--------------------
2022-08-26 05:07:12 - name: conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-26 05:07:12 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-26 05:07:12 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-26 05:07:12 - -----------no weight decay layers--------------
2022-08-26 05:07:12 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-26 05:07:12 - -------------weight decay layers---------------
2022-08-26 05:07:12 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer1.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer2.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer3.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-26 05:07:12 - epoch 001 lr: 0.100000
2022-08-26 05:07:52 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9135
2022-08-26 05:08:26 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.9083
2022-08-26 05:09:01 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.8955
2022-08-26 05:09:36 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.8647
2022-08-26 05:10:11 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.8066
2022-08-26 05:10:46 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.6360
2022-08-26 05:11:20 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.6553
2022-08-26 05:11:53 - train: epoch 0001, iter [00800, 05004], lr: 0.099999, loss: 6.5785
2022-08-26 05:12:28 - train: epoch 0001, iter [00900, 05004], lr: 0.099999, loss: 6.4905
2022-08-26 05:13:02 - train: epoch 0001, iter [01000, 05004], lr: 0.099999, loss: 6.4246
2022-08-26 05:13:36 - train: epoch 0001, iter [01100, 05004], lr: 0.099999, loss: 6.3649
2022-08-26 05:14:10 - train: epoch 0001, iter [01200, 05004], lr: 0.099999, loss: 6.1997
2022-08-26 05:14:44 - train: epoch 0001, iter [01300, 05004], lr: 0.099998, loss: 6.2666
2022-08-26 05:15:18 - train: epoch 0001, iter [01400, 05004], lr: 0.099998, loss: 6.1340
2022-08-26 05:15:53 - train: epoch 0001, iter [01500, 05004], lr: 0.099998, loss: 6.0123
2022-08-26 05:16:26 - train: epoch 0001, iter [01600, 05004], lr: 0.099997, loss: 5.9960
2022-08-26 05:17:00 - train: epoch 0001, iter [01700, 05004], lr: 0.099997, loss: 5.7893
2022-08-26 05:17:34 - train: epoch 0001, iter [01800, 05004], lr: 0.099997, loss: 5.8213
2022-08-26 05:18:08 - train: epoch 0001, iter [01900, 05004], lr: 0.099996, loss: 5.6708
2022-08-26 05:18:42 - train: epoch 0001, iter [02000, 05004], lr: 0.099996, loss: 5.6315
2022-08-26 05:19:17 - train: epoch 0001, iter [02100, 05004], lr: 0.099996, loss: 5.5115
2022-08-26 05:19:49 - train: epoch 0001, iter [02200, 05004], lr: 0.099995, loss: 5.4956
2022-08-26 05:20:24 - train: epoch 0001, iter [02300, 05004], lr: 0.099995, loss: 5.4003
2022-08-26 05:20:57 - train: epoch 0001, iter [02400, 05004], lr: 0.099994, loss: 5.3780
2022-08-26 05:21:31 - train: epoch 0001, iter [02500, 05004], lr: 0.099994, loss: 5.3171
2022-08-26 05:22:04 - train: epoch 0001, iter [02600, 05004], lr: 0.099993, loss: 5.4727
2022-08-26 05:22:38 - train: epoch 0001, iter [02700, 05004], lr: 0.099993, loss: 5.4086
2022-08-26 05:23:12 - train: epoch 0001, iter [02800, 05004], lr: 0.099992, loss: 5.1990
2022-08-26 05:23:46 - train: epoch 0001, iter [02900, 05004], lr: 0.099992, loss: 5.0685
2022-08-26 05:24:20 - train: epoch 0001, iter [03000, 05004], lr: 0.099991, loss: 5.1762
2022-08-26 05:24:54 - train: epoch 0001, iter [03100, 05004], lr: 0.099991, loss: 5.3009
2022-08-26 05:25:28 - train: epoch 0001, iter [03200, 05004], lr: 0.099990, loss: 5.1139
2022-08-26 05:26:02 - train: epoch 0001, iter [03300, 05004], lr: 0.099989, loss: 4.9370
2022-08-26 05:26:35 - train: epoch 0001, iter [03400, 05004], lr: 0.099989, loss: 4.8725
2022-08-26 05:27:10 - train: epoch 0001, iter [03500, 05004], lr: 0.099988, loss: 4.8219
2022-08-26 05:27:43 - train: epoch 0001, iter [03600, 05004], lr: 0.099987, loss: 4.8604
2022-08-26 05:28:17 - train: epoch 0001, iter [03700, 05004], lr: 0.099987, loss: 4.9532
2022-08-26 05:28:51 - train: epoch 0001, iter [03800, 05004], lr: 0.099986, loss: 4.7354
2022-08-26 05:29:25 - train: epoch 0001, iter [03900, 05004], lr: 0.099985, loss: 4.7618
2022-08-26 05:30:00 - train: epoch 0001, iter [04000, 05004], lr: 0.099984, loss: 4.7194
2022-08-26 05:30:34 - train: epoch 0001, iter [04100, 05004], lr: 0.099983, loss: 4.8039
2022-08-26 05:31:07 - train: epoch 0001, iter [04200, 05004], lr: 0.099983, loss: 4.6978
2022-08-26 05:31:41 - train: epoch 0001, iter [04300, 05004], lr: 0.099982, loss: 4.5211
2022-08-26 05:32:14 - train: epoch 0001, iter [04400, 05004], lr: 0.099981, loss: 4.3187
2022-08-26 05:32:49 - train: epoch 0001, iter [04500, 05004], lr: 0.099980, loss: 4.6009
2022-08-26 05:33:23 - train: epoch 0001, iter [04600, 05004], lr: 0.099979, loss: 4.8416
2022-08-26 05:33:56 - train: epoch 0001, iter [04700, 05004], lr: 0.099978, loss: 4.2668
2022-08-26 05:34:30 - train: epoch 0001, iter [04800, 05004], lr: 0.099977, loss: 4.6238
2022-08-26 05:35:04 - train: epoch 0001, iter [04900, 05004], lr: 0.099976, loss: 4.3754
2022-08-26 05:35:37 - train: epoch 0001, iter [05000, 05004], lr: 0.099975, loss: 4.2400
2022-08-26 05:35:38 - train: epoch 001, train_loss: 5.4969
2022-08-26 05:36:54 - eval: epoch: 001, acc1: 15.452%, acc5: 35.212%, test_loss: 4.4114, per_image_load_time: 2.424ms, per_image_inference_time: 0.483ms
2022-08-26 05:36:54 - until epoch: 001, best_acc1: 15.452%
2022-08-26 05:36:54 - epoch 002 lr: 0.099975
2022-08-26 05:37:34 - train: epoch 0002, iter [00100, 05004], lr: 0.099974, loss: 4.3671
2022-08-26 05:38:08 - train: epoch 0002, iter [00200, 05004], lr: 0.099973, loss: 4.0747
2022-08-26 05:38:42 - train: epoch 0002, iter [00300, 05004], lr: 0.099972, loss: 4.5066
2022-08-26 05:39:15 - train: epoch 0002, iter [00400, 05004], lr: 0.099971, loss: 4.2692
2022-08-26 05:39:50 - train: epoch 0002, iter [00500, 05004], lr: 0.099970, loss: 4.0271
2022-08-26 05:40:23 - train: epoch 0002, iter [00600, 05004], lr: 0.099969, loss: 4.1131
2022-08-26 05:40:58 - train: epoch 0002, iter [00700, 05004], lr: 0.099968, loss: 4.2793
2022-08-26 05:41:32 - train: epoch 0002, iter [00800, 05004], lr: 0.099967, loss: 3.8896
2022-08-26 05:42:06 - train: epoch 0002, iter [00900, 05004], lr: 0.099966, loss: 3.7476
2022-08-26 05:42:41 - train: epoch 0002, iter [01000, 05004], lr: 0.099964, loss: 4.2038
2022-08-26 05:43:15 - train: epoch 0002, iter [01100, 05004], lr: 0.099963, loss: 4.1640
2022-08-26 05:43:49 - train: epoch 0002, iter [01200, 05004], lr: 0.099962, loss: 3.9920
2022-08-26 05:44:23 - train: epoch 0002, iter [01300, 05004], lr: 0.099961, loss: 4.0290
2022-08-26 05:44:57 - train: epoch 0002, iter [01400, 05004], lr: 0.099960, loss: 3.9639
2022-08-26 05:45:32 - train: epoch 0002, iter [01500, 05004], lr: 0.099958, loss: 3.9733
2022-08-26 05:46:06 - train: epoch 0002, iter [01600, 05004], lr: 0.099957, loss: 3.8780
2022-08-26 05:46:39 - train: epoch 0002, iter [01700, 05004], lr: 0.099956, loss: 4.0057
2022-08-26 05:47:14 - train: epoch 0002, iter [01800, 05004], lr: 0.099954, loss: 3.9865
2022-08-26 05:47:48 - train: epoch 0002, iter [01900, 05004], lr: 0.099953, loss: 3.7095
2022-08-26 05:48:23 - train: epoch 0002, iter [02000, 05004], lr: 0.099952, loss: 3.5592
2022-08-26 05:48:57 - train: epoch 0002, iter [02100, 05004], lr: 0.099950, loss: 3.7970
2022-08-26 05:49:32 - train: epoch 0002, iter [02200, 05004], lr: 0.099949, loss: 3.4931
2022-08-26 05:50:06 - train: epoch 0002, iter [02300, 05004], lr: 0.099947, loss: 3.7179
2022-08-26 05:50:41 - train: epoch 0002, iter [02400, 05004], lr: 0.099946, loss: 3.6174
2022-08-26 05:51:15 - train: epoch 0002, iter [02500, 05004], lr: 0.099945, loss: 3.6163
2022-08-26 05:51:50 - train: epoch 0002, iter [02600, 05004], lr: 0.099943, loss: 3.5541
2022-08-26 05:52:24 - train: epoch 0002, iter [02700, 05004], lr: 0.099942, loss: 3.8250
2022-08-26 05:52:58 - train: epoch 0002, iter [02800, 05004], lr: 0.099940, loss: 3.6356
2022-08-26 05:53:33 - train: epoch 0002, iter [02900, 05004], lr: 0.099938, loss: 3.5475
2022-08-26 05:54:07 - train: epoch 0002, iter [03000, 05004], lr: 0.099937, loss: 3.4455
2022-08-26 05:54:42 - train: epoch 0002, iter [03100, 05004], lr: 0.099935, loss: 3.5753
2022-08-26 05:55:16 - train: epoch 0002, iter [03200, 05004], lr: 0.099934, loss: 3.5462
2022-08-26 05:55:50 - train: epoch 0002, iter [03300, 05004], lr: 0.099932, loss: 3.5067
2022-08-26 05:56:25 - train: epoch 0002, iter [03400, 05004], lr: 0.099930, loss: 3.6489
2022-08-26 05:56:59 - train: epoch 0002, iter [03500, 05004], lr: 0.099929, loss: 3.4659
2022-08-26 05:57:34 - train: epoch 0002, iter [03600, 05004], lr: 0.099927, loss: 3.6024
2022-08-26 05:58:08 - train: epoch 0002, iter [03700, 05004], lr: 0.099925, loss: 3.5891
2022-08-26 05:58:43 - train: epoch 0002, iter [03800, 05004], lr: 0.099924, loss: 3.2401
2022-08-26 05:59:17 - train: epoch 0002, iter [03900, 05004], lr: 0.099922, loss: 3.4842
2022-08-26 05:59:52 - train: epoch 0002, iter [04000, 05004], lr: 0.099920, loss: 3.4763
2022-08-26 06:00:26 - train: epoch 0002, iter [04100, 05004], lr: 0.099918, loss: 3.4871
2022-08-26 06:01:01 - train: epoch 0002, iter [04200, 05004], lr: 0.099917, loss: 3.3298
2022-08-26 06:01:35 - train: epoch 0002, iter [04300, 05004], lr: 0.099915, loss: 3.3769
2022-08-26 06:02:09 - train: epoch 0002, iter [04400, 05004], lr: 0.099913, loss: 3.1990
2022-08-26 06:02:44 - train: epoch 0002, iter [04500, 05004], lr: 0.099911, loss: 3.2061
2022-08-26 06:03:19 - train: epoch 0002, iter [04600, 05004], lr: 0.099909, loss: 3.2367
2022-08-26 06:03:54 - train: epoch 0002, iter [04700, 05004], lr: 0.099907, loss: 3.3550
2022-08-26 06:04:28 - train: epoch 0002, iter [04800, 05004], lr: 0.099905, loss: 3.3769
2022-08-26 06:05:01 - train: epoch 0002, iter [04900, 05004], lr: 0.099903, loss: 3.1561
2022-08-26 06:05:35 - train: epoch 0002, iter [05000, 05004], lr: 0.099901, loss: 3.2516
2022-08-26 06:05:36 - train: epoch 002, train_loss: 3.7163
2022-08-26 06:06:52 - eval: epoch: 002, acc1: 29.940%, acc5: 55.300%, test_loss: 4.5565, per_image_load_time: 1.287ms, per_image_inference_time: 0.504ms
2022-08-26 06:06:52 - until epoch: 002, best_acc1: 29.940%
2022-08-26 06:06:52 - epoch 003 lr: 0.099901
2022-08-26 06:07:31 - train: epoch 0003, iter [00100, 05004], lr: 0.099899, loss: 3.4070
2022-08-26 06:08:06 - train: epoch 0003, iter [00200, 05004], lr: 0.099897, loss: 3.3230
2022-08-26 06:08:39 - train: epoch 0003, iter [00300, 05004], lr: 0.099895, loss: 3.2184
2022-08-26 06:09:13 - train: epoch 0003, iter [00400, 05004], lr: 0.099893, loss: 3.2860
2022-08-26 06:09:46 - train: epoch 0003, iter [00500, 05004], lr: 0.099891, loss: 3.3470
2022-08-26 06:10:20 - train: epoch 0003, iter [00600, 05004], lr: 0.099889, loss: 3.1833
2022-08-26 06:10:54 - train: epoch 0003, iter [00700, 05004], lr: 0.099887, loss: 3.4013
2022-08-26 06:11:28 - train: epoch 0003, iter [00800, 05004], lr: 0.099885, loss: 3.2532
2022-08-26 06:12:01 - train: epoch 0003, iter [00900, 05004], lr: 0.099883, loss: 3.0931
2022-08-26 06:12:35 - train: epoch 0003, iter [01000, 05004], lr: 0.099881, loss: 3.1376
2022-08-26 06:13:09 - train: epoch 0003, iter [01100, 05004], lr: 0.099878, loss: 3.1106
2022-08-26 06:13:43 - train: epoch 0003, iter [01200, 05004], lr: 0.099876, loss: 3.0610
2022-08-26 06:14:17 - train: epoch 0003, iter [01300, 05004], lr: 0.099874, loss: 3.1351
2022-08-26 06:14:51 - train: epoch 0003, iter [01400, 05004], lr: 0.099872, loss: 3.0859
2022-08-26 06:15:25 - train: epoch 0003, iter [01500, 05004], lr: 0.099870, loss: 3.4000
2022-08-26 06:15:59 - train: epoch 0003, iter [01600, 05004], lr: 0.099867, loss: 3.0455
2022-08-26 06:16:33 - train: epoch 0003, iter [01700, 05004], lr: 0.099865, loss: 3.0430
2022-08-26 06:17:07 - train: epoch 0003, iter [01800, 05004], lr: 0.099863, loss: 3.0139
2022-08-26 06:17:41 - train: epoch 0003, iter [01900, 05004], lr: 0.099860, loss: 3.1220
2022-08-26 06:18:16 - train: epoch 0003, iter [02000, 05004], lr: 0.099858, loss: 3.4213
2022-08-26 06:18:50 - train: epoch 0003, iter [02100, 05004], lr: 0.099856, loss: 3.3240
2022-08-26 06:19:24 - train: epoch 0003, iter [02200, 05004], lr: 0.099853, loss: 3.5680
2022-08-26 06:19:59 - train: epoch 0003, iter [02300, 05004], lr: 0.099851, loss: 2.9853
2022-08-26 06:20:33 - train: epoch 0003, iter [02400, 05004], lr: 0.099848, loss: 2.9403
2022-08-26 06:21:06 - train: epoch 0003, iter [02500, 05004], lr: 0.099846, loss: 3.0795
2022-08-26 06:21:40 - train: epoch 0003, iter [02600, 05004], lr: 0.099843, loss: 3.0685
2022-08-26 06:22:15 - train: epoch 0003, iter [02700, 05004], lr: 0.099841, loss: 3.3347
2022-08-26 06:22:48 - train: epoch 0003, iter [02800, 05004], lr: 0.099838, loss: 2.9330
2022-08-26 06:23:23 - train: epoch 0003, iter [02900, 05004], lr: 0.099836, loss: 2.9683
2022-08-26 06:23:56 - train: epoch 0003, iter [03000, 05004], lr: 0.099833, loss: 3.2014
2022-08-26 06:24:31 - train: epoch 0003, iter [03100, 05004], lr: 0.099831, loss: 3.1194
2022-08-26 06:25:05 - train: epoch 0003, iter [03200, 05004], lr: 0.099828, loss: 3.0917
2022-08-26 06:25:39 - train: epoch 0003, iter [03300, 05004], lr: 0.099826, loss: 2.9844
2022-08-26 06:26:14 - train: epoch 0003, iter [03400, 05004], lr: 0.099823, loss: 3.0997
2022-08-26 06:26:48 - train: epoch 0003, iter [03500, 05004], lr: 0.099820, loss: 2.7564
2022-08-26 06:27:22 - train: epoch 0003, iter [03600, 05004], lr: 0.099818, loss: 2.8775
2022-08-26 06:27:57 - train: epoch 0003, iter [03700, 05004], lr: 0.099815, loss: 3.0333
2022-08-26 06:28:31 - train: epoch 0003, iter [03800, 05004], lr: 0.099812, loss: 2.9861
2022-08-26 06:29:04 - train: epoch 0003, iter [03900, 05004], lr: 0.099810, loss: 3.0949
2022-08-26 06:29:39 - train: epoch 0003, iter [04000, 05004], lr: 0.099807, loss: 2.8471
2022-08-26 06:30:13 - train: epoch 0003, iter [04100, 05004], lr: 0.099804, loss: 3.0926
2022-08-26 06:30:46 - train: epoch 0003, iter [04200, 05004], lr: 0.099801, loss: 2.8824
2022-08-26 06:31:20 - train: epoch 0003, iter [04300, 05004], lr: 0.099798, loss: 2.6203
2022-08-26 06:31:54 - train: epoch 0003, iter [04400, 05004], lr: 0.099796, loss: 2.8637
2022-08-26 06:32:28 - train: epoch 0003, iter [04500, 05004], lr: 0.099793, loss: 2.8449
2022-08-26 06:33:03 - train: epoch 0003, iter [04600, 05004], lr: 0.099790, loss: 2.9189
2022-08-26 06:33:36 - train: epoch 0003, iter [04700, 05004], lr: 0.099787, loss: 2.7584
2022-08-26 06:34:11 - train: epoch 0003, iter [04800, 05004], lr: 0.099784, loss: 2.9912
2022-08-26 06:34:45 - train: epoch 0003, iter [04900, 05004], lr: 0.099781, loss: 2.9572
2022-08-26 06:35:17 - train: epoch 0003, iter [05000, 05004], lr: 0.099778, loss: 2.9217
2022-08-26 06:35:19 - train: epoch 003, train_loss: 3.0516
2022-08-26 06:36:34 - eval: epoch: 003, acc1: 38.884%, acc5: 64.746%, test_loss: 3.2578, per_image_load_time: 2.432ms, per_image_inference_time: 0.494ms
2022-08-26 06:36:35 - until epoch: 003, best_acc1: 38.884%
2022-08-26 06:36:35 - epoch 004 lr: 0.099778
2022-08-26 06:37:14 - train: epoch 0004, iter [00100, 05004], lr: 0.099775, loss: 2.8805
2022-08-26 06:37:48 - train: epoch 0004, iter [00200, 05004], lr: 0.099772, loss: 2.7841
2022-08-26 06:38:22 - train: epoch 0004, iter [00300, 05004], lr: 0.099769, loss: 2.7709
2022-08-26 06:38:56 - train: epoch 0004, iter [00400, 05004], lr: 0.099766, loss: 2.6880
2022-08-26 06:39:30 - train: epoch 0004, iter [00500, 05004], lr: 0.099763, loss: 2.6978
2022-08-26 06:40:04 - train: epoch 0004, iter [00600, 05004], lr: 0.099760, loss: 3.0095
2022-08-26 06:40:38 - train: epoch 0004, iter [00700, 05004], lr: 0.099757, loss: 2.9299
2022-08-26 06:41:12 - train: epoch 0004, iter [00800, 05004], lr: 0.099754, loss: 2.7541
2022-08-26 06:41:46 - train: epoch 0004, iter [00900, 05004], lr: 0.099751, loss: 2.5952
2022-08-26 06:42:20 - train: epoch 0004, iter [01000, 05004], lr: 0.099748, loss: 2.7587
2022-08-26 06:42:54 - train: epoch 0004, iter [01100, 05004], lr: 0.099744, loss: 2.9327
2022-08-26 06:43:29 - train: epoch 0004, iter [01200, 05004], lr: 0.099741, loss: 2.5658
2022-08-26 06:44:04 - train: epoch 0004, iter [01300, 05004], lr: 0.099738, loss: 2.5372
2022-08-26 06:44:37 - train: epoch 0004, iter [01400, 05004], lr: 0.099735, loss: 2.8706
2022-08-26 06:45:12 - train: epoch 0004, iter [01500, 05004], lr: 0.099732, loss: 2.9230
2022-08-26 06:45:46 - train: epoch 0004, iter [01600, 05004], lr: 0.099728, loss: 2.6671
2022-08-26 06:46:20 - train: epoch 0004, iter [01700, 05004], lr: 0.099725, loss: 2.8359
2022-08-26 06:46:54 - train: epoch 0004, iter [01800, 05004], lr: 0.099722, loss: 2.9320
2022-08-26 06:47:29 - train: epoch 0004, iter [01900, 05004], lr: 0.099718, loss: 2.8758
2022-08-26 06:48:04 - train: epoch 0004, iter [02000, 05004], lr: 0.099715, loss: 2.7525
2022-08-26 06:48:39 - train: epoch 0004, iter [02100, 05004], lr: 0.099712, loss: 2.8802
2022-08-26 06:49:13 - train: epoch 0004, iter [02200, 05004], lr: 0.099708, loss: 2.7406
2022-08-26 06:49:48 - train: epoch 0004, iter [02300, 05004], lr: 0.099705, loss: 2.5506
2022-08-26 06:50:22 - train: epoch 0004, iter [02400, 05004], lr: 0.099702, loss: 2.5477
2022-08-26 06:50:57 - train: epoch 0004, iter [02500, 05004], lr: 0.099698, loss: 2.8365
2022-08-26 06:51:31 - train: epoch 0004, iter [02600, 05004], lr: 0.099695, loss: 2.7784
2022-08-26 06:52:05 - train: epoch 0004, iter [02700, 05004], lr: 0.099691, loss: 2.5819
2022-08-26 06:52:40 - train: epoch 0004, iter [02800, 05004], lr: 0.099688, loss: 2.6963
2022-08-26 06:53:14 - train: epoch 0004, iter [02900, 05004], lr: 0.099684, loss: 2.6088
2022-08-26 06:53:49 - train: epoch 0004, iter [03000, 05004], lr: 0.099681, loss: 2.7064
2022-08-26 06:54:24 - train: epoch 0004, iter [03100, 05004], lr: 0.099677, loss: 2.8484
2022-08-26 06:54:58 - train: epoch 0004, iter [03200, 05004], lr: 0.099674, loss: 2.7415
2022-08-26 06:55:33 - train: epoch 0004, iter [03300, 05004], lr: 0.099670, loss: 2.8091
2022-08-26 06:56:07 - train: epoch 0004, iter [03400, 05004], lr: 0.099666, loss: 2.7170
2022-08-26 06:56:41 - train: epoch 0004, iter [03500, 05004], lr: 0.099663, loss: 2.6146
2022-08-26 06:57:15 - train: epoch 0004, iter [03600, 05004], lr: 0.099659, loss: 2.4510
2022-08-26 06:57:50 - train: epoch 0004, iter [03700, 05004], lr: 0.099655, loss: 2.6803
2022-08-26 06:58:25 - train: epoch 0004, iter [03800, 05004], lr: 0.099652, loss: 2.6296
2022-08-26 06:58:59 - train: epoch 0004, iter [03900, 05004], lr: 0.099648, loss: 2.6495
2022-08-26 06:59:34 - train: epoch 0004, iter [04000, 05004], lr: 0.099644, loss: 2.3363
2022-08-26 07:00:08 - train: epoch 0004, iter [04100, 05004], lr: 0.099641, loss: 2.7166
2022-08-26 07:00:44 - train: epoch 0004, iter [04200, 05004], lr: 0.099637, loss: 2.5402
2022-08-26 07:01:18 - train: epoch 0004, iter [04300, 05004], lr: 0.099633, loss: 2.4239
2022-08-26 07:01:52 - train: epoch 0004, iter [04400, 05004], lr: 0.099629, loss: 2.4780
2022-08-26 07:02:27 - train: epoch 0004, iter [04500, 05004], lr: 0.099625, loss: 2.1407
2022-08-26 07:03:01 - train: epoch 0004, iter [04600, 05004], lr: 0.099621, loss: 2.6727
2022-08-26 07:03:35 - train: epoch 0004, iter [04700, 05004], lr: 0.099618, loss: 2.5061
2022-08-26 07:04:10 - train: epoch 0004, iter [04800, 05004], lr: 0.099614, loss: 2.5470
2022-08-26 07:04:44 - train: epoch 0004, iter [04900, 05004], lr: 0.099610, loss: 2.7634
2022-08-26 07:05:17 - train: epoch 0004, iter [05000, 05004], lr: 0.099606, loss: 2.8205
2022-08-26 07:05:18 - train: epoch 004, train_loss: 2.7254
2022-08-26 07:06:34 - eval: epoch: 004, acc1: 44.744%, acc5: 71.006%, test_loss: 2.4453, per_image_load_time: 2.493ms, per_image_inference_time: 0.471ms
2022-08-26 07:06:35 - until epoch: 004, best_acc1: 44.744%
2022-08-26 07:06:35 - epoch 005 lr: 0.099606
2022-08-26 07:07:13 - train: epoch 0005, iter [00100, 05004], lr: 0.099602, loss: 2.6769
2022-08-26 07:07:48 - train: epoch 0005, iter [00200, 05004], lr: 0.099598, loss: 2.6479
2022-08-26 07:08:22 - train: epoch 0005, iter [00300, 05004], lr: 0.099594, loss: 2.7079
2022-08-26 07:08:56 - train: epoch 0005, iter [00400, 05004], lr: 0.099590, loss: 2.5611
2022-08-26 07:09:31 - train: epoch 0005, iter [00500, 05004], lr: 0.099586, loss: 2.4079
2022-08-26 07:10:05 - train: epoch 0005, iter [00600, 05004], lr: 0.099582, loss: 2.5843
2022-08-26 07:10:39 - train: epoch 0005, iter [00700, 05004], lr: 0.099578, loss: 2.6222
2022-08-26 07:11:14 - train: epoch 0005, iter [00800, 05004], lr: 0.099574, loss: 2.7573
2022-08-26 07:11:48 - train: epoch 0005, iter [00900, 05004], lr: 0.099570, loss: 2.5121
2022-08-26 07:12:21 - train: epoch 0005, iter [01000, 05004], lr: 0.099565, loss: 2.6007
2022-08-26 07:12:56 - train: epoch 0005, iter [01100, 05004], lr: 0.099561, loss: 2.6820
2022-08-26 07:13:30 - train: epoch 0005, iter [01200, 05004], lr: 0.099557, loss: 2.6683
2022-08-26 07:14:05 - train: epoch 0005, iter [01300, 05004], lr: 0.099553, loss: 2.5237
2022-08-26 07:14:39 - train: epoch 0005, iter [01400, 05004], lr: 0.099549, loss: 2.6442
2022-08-26 07:15:13 - train: epoch 0005, iter [01500, 05004], lr: 0.099545, loss: 2.3564
2022-08-26 07:15:48 - train: epoch 0005, iter [01600, 05004], lr: 0.099540, loss: 2.3367
2022-08-26 07:16:22 - train: epoch 0005, iter [01700, 05004], lr: 0.099536, loss: 2.4604
2022-08-26 07:16:57 - train: epoch 0005, iter [01800, 05004], lr: 0.099532, loss: 2.6095
2022-08-26 07:17:31 - train: epoch 0005, iter [01900, 05004], lr: 0.099527, loss: 2.5034
2022-08-26 07:18:05 - train: epoch 0005, iter [02000, 05004], lr: 0.099523, loss: 2.5922
2022-08-26 07:18:40 - train: epoch 0005, iter [02100, 05004], lr: 0.099519, loss: 2.2575
2022-08-26 07:19:15 - train: epoch 0005, iter [02200, 05004], lr: 0.099514, loss: 2.2712
2022-08-26 07:19:49 - train: epoch 0005, iter [02300, 05004], lr: 0.099510, loss: 2.3069
2022-08-26 07:20:24 - train: epoch 0005, iter [02400, 05004], lr: 0.099506, loss: 2.4776
2022-08-26 07:20:58 - train: epoch 0005, iter [02500, 05004], lr: 0.099501, loss: 2.6832
2022-08-26 07:21:32 - train: epoch 0005, iter [02600, 05004], lr: 0.099497, loss: 2.7086
2022-08-26 07:22:07 - train: epoch 0005, iter [02700, 05004], lr: 0.099492, loss: 2.6450
2022-08-26 07:22:42 - train: epoch 0005, iter [02800, 05004], lr: 0.099488, loss: 2.5654
2022-08-26 07:23:17 - train: epoch 0005, iter [02900, 05004], lr: 0.099483, loss: 2.3491
2022-08-26 07:23:50 - train: epoch 0005, iter [03000, 05004], lr: 0.099479, loss: 2.5261
2022-08-26 07:24:25 - train: epoch 0005, iter [03100, 05004], lr: 0.099474, loss: 2.5838
2022-08-26 07:25:00 - train: epoch 0005, iter [03200, 05004], lr: 0.099470, loss: 2.6463
2022-08-26 07:25:35 - train: epoch 0005, iter [03300, 05004], lr: 0.099465, loss: 2.3801
2022-08-26 07:26:10 - train: epoch 0005, iter [03400, 05004], lr: 0.099461, loss: 2.4184
2022-08-26 07:26:44 - train: epoch 0005, iter [03500, 05004], lr: 0.099456, loss: 2.5181
2022-08-26 07:27:19 - train: epoch 0005, iter [03600, 05004], lr: 0.099451, loss: 2.6649
2022-08-26 07:27:54 - train: epoch 0005, iter [03700, 05004], lr: 0.099447, loss: 2.3225
2022-08-26 07:28:28 - train: epoch 0005, iter [03800, 05004], lr: 0.099442, loss: 2.3423
2022-08-26 07:29:03 - train: epoch 0005, iter [03900, 05004], lr: 0.099437, loss: 2.8500
2022-08-26 07:29:38 - train: epoch 0005, iter [04000, 05004], lr: 0.099433, loss: 2.4605
2022-08-26 07:30:12 - train: epoch 0005, iter [04100, 05004], lr: 0.099428, loss: 2.4526
2022-08-26 07:30:47 - train: epoch 0005, iter [04200, 05004], lr: 0.099423, loss: 2.4772
2022-08-26 07:31:21 - train: epoch 0005, iter [04300, 05004], lr: 0.099419, loss: 2.4428
2022-08-26 07:31:56 - train: epoch 0005, iter [04400, 05004], lr: 0.099414, loss: 2.5456
2022-08-26 07:32:30 - train: epoch 0005, iter [04500, 05004], lr: 0.099409, loss: 2.6096
2022-08-26 07:33:05 - train: epoch 0005, iter [04600, 05004], lr: 0.099404, loss: 2.4717
2022-08-26 07:33:40 - train: epoch 0005, iter [04700, 05004], lr: 0.099399, loss: 2.2186
2022-08-26 07:34:14 - train: epoch 0005, iter [04800, 05004], lr: 0.099394, loss: 2.2190
2022-08-26 07:34:48 - train: epoch 0005, iter [04900, 05004], lr: 0.099390, loss: 2.5721
2022-08-26 07:35:21 - train: epoch 0005, iter [05000, 05004], lr: 0.099385, loss: 2.4044
2022-08-26 07:35:22 - train: epoch 005, train_loss: 2.5394
2022-08-26 07:36:37 - eval: epoch: 005, acc1: 46.014%, acc5: 72.488%, test_loss: 2.3744, per_image_load_time: 1.678ms, per_image_inference_time: 0.484ms
2022-08-26 07:36:38 - until epoch: 005, best_acc1: 46.014%
2022-08-26 07:36:38 - epoch 006 lr: 0.099384
2022-08-26 07:37:17 - train: epoch 0006, iter [00100, 05004], lr: 0.099379, loss: 2.3850
2022-08-26 07:37:51 - train: epoch 0006, iter [00200, 05004], lr: 0.099375, loss: 2.5360
2022-08-26 07:38:26 - train: epoch 0006, iter [00300, 05004], lr: 0.099370, loss: 2.2552
2022-08-26 07:38:59 - train: epoch 0006, iter [00400, 05004], lr: 0.099365, loss: 2.4944
2022-08-26 07:39:34 - train: epoch 0006, iter [00500, 05004], lr: 0.099360, loss: 2.3351
2022-08-26 07:40:08 - train: epoch 0006, iter [00600, 05004], lr: 0.099355, loss: 2.4508
2022-08-26 07:40:43 - train: epoch 0006, iter [00700, 05004], lr: 0.099350, loss: 2.4306
2022-08-26 07:41:17 - train: epoch 0006, iter [00800, 05004], lr: 0.099345, loss: 2.4231
2022-08-26 07:41:52 - train: epoch 0006, iter [00900, 05004], lr: 0.099339, loss: 2.3831
2022-08-26 07:42:26 - train: epoch 0006, iter [01000, 05004], lr: 0.099334, loss: 2.3631
2022-08-26 07:43:00 - train: epoch 0006, iter [01100, 05004], lr: 0.099329, loss: 2.3895
2022-08-26 07:43:35 - train: epoch 0006, iter [01200, 05004], lr: 0.099324, loss: 2.5288
2022-08-26 07:44:09 - train: epoch 0006, iter [01300, 05004], lr: 0.099319, loss: 2.6279
2022-08-26 07:44:44 - train: epoch 0006, iter [01400, 05004], lr: 0.099314, loss: 2.5335
2022-08-26 07:45:19 - train: epoch 0006, iter [01500, 05004], lr: 0.099309, loss: 2.5933
2022-08-26 07:45:53 - train: epoch 0006, iter [01600, 05004], lr: 0.099303, loss: 2.2485
2022-08-26 07:46:28 - train: epoch 0006, iter [01700, 05004], lr: 0.099298, loss: 2.5931
2022-08-26 07:47:02 - train: epoch 0006, iter [01800, 05004], lr: 0.099293, loss: 2.5500
2022-08-26 07:47:36 - train: epoch 0006, iter [01900, 05004], lr: 0.099288, loss: 2.3610
2022-08-26 07:48:11 - train: epoch 0006, iter [02000, 05004], lr: 0.099282, loss: 2.6821
2022-08-26 07:48:46 - train: epoch 0006, iter [02100, 05004], lr: 0.099277, loss: 2.5980
2022-08-26 07:49:20 - train: epoch 0006, iter [02200, 05004], lr: 0.099272, loss: 2.3329
2022-08-26 07:49:54 - train: epoch 0006, iter [02300, 05004], lr: 0.099266, loss: 2.3090
2022-08-26 07:50:29 - train: epoch 0006, iter [02400, 05004], lr: 0.099261, loss: 2.3557
2022-08-26 07:51:03 - train: epoch 0006, iter [02500, 05004], lr: 0.099256, loss: 2.6259
2022-08-26 07:51:38 - train: epoch 0006, iter [02600, 05004], lr: 0.099250, loss: 2.4223
2022-08-26 07:52:13 - train: epoch 0006, iter [02700, 05004], lr: 0.099245, loss: 2.5407
2022-08-26 07:52:46 - train: epoch 0006, iter [02800, 05004], lr: 0.099239, loss: 2.2827
2022-08-26 07:53:21 - train: epoch 0006, iter [02900, 05004], lr: 0.099234, loss: 2.4432
2022-08-26 07:53:56 - train: epoch 0006, iter [03000, 05004], lr: 0.099228, loss: 2.4662
2022-08-26 07:54:30 - train: epoch 0006, iter [03100, 05004], lr: 0.099223, loss: 2.1548
2022-08-26 07:55:04 - train: epoch 0006, iter [03200, 05004], lr: 0.099217, loss: 2.4608
2022-08-26 07:55:38 - train: epoch 0006, iter [03300, 05004], lr: 0.099212, loss: 2.2346
2022-08-26 07:56:14 - train: epoch 0006, iter [03400, 05004], lr: 0.099206, loss: 2.6599
2022-08-26 07:56:48 - train: epoch 0006, iter [03500, 05004], lr: 0.099201, loss: 2.4421
2022-08-26 07:57:23 - train: epoch 0006, iter [03600, 05004], lr: 0.099195, loss: 2.3915
2022-08-26 07:57:58 - train: epoch 0006, iter [03700, 05004], lr: 0.099189, loss: 2.4166
2022-08-26 07:58:32 - train: epoch 0006, iter [03800, 05004], lr: 0.099184, loss: 2.3214
2022-08-26 07:59:07 - train: epoch 0006, iter [03900, 05004], lr: 0.099178, loss: 2.3479
2022-08-26 07:59:42 - train: epoch 0006, iter [04000, 05004], lr: 0.099172, loss: 2.6277
2022-08-26 08:00:15 - train: epoch 0006, iter [04100, 05004], lr: 0.099167, loss: 2.3691
2022-08-26 08:00:50 - train: epoch 0006, iter [04200, 05004], lr: 0.099161, loss: 2.2695
2022-08-26 08:01:25 - train: epoch 0006, iter [04300, 05004], lr: 0.099155, loss: 2.3969
2022-08-26 08:02:00 - train: epoch 0006, iter [04400, 05004], lr: 0.099150, loss: 2.3929
2022-08-26 08:02:34 - train: epoch 0006, iter [04500, 05004], lr: 0.099144, loss: 2.4501
2022-08-26 08:03:08 - train: epoch 0006, iter [04600, 05004], lr: 0.099138, loss: 2.3543
2022-08-26 08:03:42 - train: epoch 0006, iter [04700, 05004], lr: 0.099132, loss: 2.3509
2022-08-26 08:04:17 - train: epoch 0006, iter [04800, 05004], lr: 0.099126, loss: 2.3851
2022-08-26 08:04:52 - train: epoch 0006, iter [04900, 05004], lr: 0.099120, loss: 2.4068
2022-08-26 08:05:25 - train: epoch 0006, iter [05000, 05004], lr: 0.099115, loss: 2.3727
2022-08-26 08:05:26 - train: epoch 006, train_loss: 2.4231
2022-08-26 08:06:41 - eval: epoch: 006, acc1: 50.842%, acc5: 76.346%, test_loss: 2.1322, per_image_load_time: 2.167ms, per_image_inference_time: 0.513ms
2022-08-26 08:06:42 - until epoch: 006, best_acc1: 50.842%
2022-08-26 08:06:42 - epoch 007 lr: 0.099114
2022-08-26 08:07:21 - train: epoch 0007, iter [00100, 05004], lr: 0.099108, loss: 2.2251
2022-08-26 08:07:55 - train: epoch 0007, iter [00200, 05004], lr: 0.099103, loss: 2.5119
2022-08-26 08:08:29 - train: epoch 0007, iter [00300, 05004], lr: 0.099097, loss: 2.6148
2022-08-26 08:09:04 - train: epoch 0007, iter [00400, 05004], lr: 0.099091, loss: 2.4030
2022-08-26 08:09:37 - train: epoch 0007, iter [00500, 05004], lr: 0.099085, loss: 2.3056
2022-08-26 08:10:11 - train: epoch 0007, iter [00600, 05004], lr: 0.099079, loss: 2.4461
2022-08-26 08:10:46 - train: epoch 0007, iter [00700, 05004], lr: 0.099073, loss: 2.3919
2022-08-26 08:11:20 - train: epoch 0007, iter [00800, 05004], lr: 0.099067, loss: 2.4145
2022-08-26 08:11:55 - train: epoch 0007, iter [00900, 05004], lr: 0.099061, loss: 2.3833
2022-08-26 08:12:29 - train: epoch 0007, iter [01000, 05004], lr: 0.099055, loss: 2.4068
2022-08-26 08:13:04 - train: epoch 0007, iter [01100, 05004], lr: 0.099048, loss: 2.2439
2022-08-26 08:13:38 - train: epoch 0007, iter [01200, 05004], lr: 0.099042, loss: 2.3486
2022-08-26 08:14:12 - train: epoch 0007, iter [01300, 05004], lr: 0.099036, loss: 2.3143
2022-08-26 08:14:47 - train: epoch 0007, iter [01400, 05004], lr: 0.099030, loss: 2.3098
2022-08-26 08:15:21 - train: epoch 0007, iter [01500, 05004], lr: 0.099024, loss: 2.4273
2022-08-26 08:15:55 - train: epoch 0007, iter [01600, 05004], lr: 0.099018, loss: 2.3127
2022-08-26 08:16:30 - train: epoch 0007, iter [01700, 05004], lr: 0.099012, loss: 2.4293
2022-08-26 08:17:04 - train: epoch 0007, iter [01800, 05004], lr: 0.099005, loss: 2.2580
2022-08-26 08:17:39 - train: epoch 0007, iter [01900, 05004], lr: 0.098999, loss: 2.3860
2022-08-26 08:18:13 - train: epoch 0007, iter [02000, 05004], lr: 0.098993, loss: 2.1933
2022-08-26 08:18:48 - train: epoch 0007, iter [02100, 05004], lr: 0.098987, loss: 2.4450
2022-08-26 08:19:22 - train: epoch 0007, iter [02200, 05004], lr: 0.098980, loss: 2.3499
2022-08-26 08:19:57 - train: epoch 0007, iter [02300, 05004], lr: 0.098974, loss: 2.4320
2022-08-26 08:20:31 - train: epoch 0007, iter [02400, 05004], lr: 0.098968, loss: 2.3580
2022-08-26 08:21:05 - train: epoch 0007, iter [02500, 05004], lr: 0.098961, loss: 2.2212
2022-08-26 08:21:39 - train: epoch 0007, iter [02600, 05004], lr: 0.098955, loss: 2.2637
2022-08-26 08:22:14 - train: epoch 0007, iter [02700, 05004], lr: 0.098948, loss: 2.1950
2022-08-26 08:22:48 - train: epoch 0007, iter [02800, 05004], lr: 0.098942, loss: 2.3224
2022-08-26 08:23:23 - train: epoch 0007, iter [02900, 05004], lr: 0.098936, loss: 2.2817
2022-08-26 08:23:57 - train: epoch 0007, iter [03000, 05004], lr: 0.098929, loss: 2.3738
2022-08-26 08:24:32 - train: epoch 0007, iter [03100, 05004], lr: 0.098923, loss: 2.0606
2022-08-26 08:25:05 - train: epoch 0007, iter [03200, 05004], lr: 0.098916, loss: 2.2625
2022-08-26 08:25:41 - train: epoch 0007, iter [03300, 05004], lr: 0.098910, loss: 2.5971
2022-08-26 08:26:15 - train: epoch 0007, iter [03400, 05004], lr: 0.098903, loss: 2.1728
2022-08-26 08:26:49 - train: epoch 0007, iter [03500, 05004], lr: 0.098897, loss: 2.3763
2022-08-26 08:27:25 - train: epoch 0007, iter [03600, 05004], lr: 0.098890, loss: 2.0638
2022-08-26 08:27:58 - train: epoch 0007, iter [03700, 05004], lr: 0.098883, loss: 2.3576
2022-08-26 08:28:32 - train: epoch 0007, iter [03800, 05004], lr: 0.098877, loss: 2.5922
2022-08-26 08:29:07 - train: epoch 0007, iter [03900, 05004], lr: 0.098870, loss: 2.1989
2022-08-26 08:29:42 - train: epoch 0007, iter [04000, 05004], lr: 0.098864, loss: 2.3883
2022-08-26 08:30:16 - train: epoch 0007, iter [04100, 05004], lr: 0.098857, loss: 2.2346
2022-08-26 08:30:51 - train: epoch 0007, iter [04200, 05004], lr: 0.098850, loss: 2.2652
2022-08-26 08:31:26 - train: epoch 0007, iter [04300, 05004], lr: 0.098844, loss: 2.4602
2022-08-26 08:32:00 - train: epoch 0007, iter [04400, 05004], lr: 0.098837, loss: 2.1426
2022-08-26 08:32:35 - train: epoch 0007, iter [04500, 05004], lr: 0.098830, loss: 2.4781
2022-08-26 08:33:09 - train: epoch 0007, iter [04600, 05004], lr: 0.098823, loss: 2.4060
2022-08-26 08:33:43 - train: epoch 0007, iter [04700, 05004], lr: 0.098817, loss: 2.4791
2022-08-26 08:34:19 - train: epoch 0007, iter [04800, 05004], lr: 0.098810, loss: 2.6013
2022-08-26 08:34:54 - train: epoch 0007, iter [04900, 05004], lr: 0.098803, loss: 2.3058
2022-08-26 08:35:27 - train: epoch 0007, iter [05000, 05004], lr: 0.098796, loss: 2.3240
2022-08-26 08:35:28 - train: epoch 007, train_loss: 2.3389
2022-08-26 08:36:42 - eval: epoch: 007, acc1: 51.912%, acc5: 77.412%, test_loss: 2.0656, per_image_load_time: 2.375ms, per_image_inference_time: 0.525ms
2022-08-26 08:36:43 - until epoch: 007, best_acc1: 51.912%
2022-08-26 08:36:43 - epoch 008 lr: 0.098796
2022-08-26 08:37:22 - train: epoch 0008, iter [00100, 05004], lr: 0.098789, loss: 2.2455
2022-08-26 08:37:56 - train: epoch 0008, iter [00200, 05004], lr: 0.098782, loss: 2.4517
2022-08-26 08:38:30 - train: epoch 0008, iter [00300, 05004], lr: 0.098775, loss: 2.1331
2022-08-26 08:39:03 - train: epoch 0008, iter [00400, 05004], lr: 0.098768, loss: 2.0896
2022-08-26 08:39:38 - train: epoch 0008, iter [00500, 05004], lr: 0.098761, loss: 2.1700
2022-08-26 08:40:12 - train: epoch 0008, iter [00600, 05004], lr: 0.098754, loss: 2.1605
2022-08-26 08:40:47 - train: epoch 0008, iter [00700, 05004], lr: 0.098747, loss: 2.5771
2022-08-26 08:41:21 - train: epoch 0008, iter [00800, 05004], lr: 0.098740, loss: 2.0987
2022-08-26 08:41:55 - train: epoch 0008, iter [00900, 05004], lr: 0.098733, loss: 2.2294
2022-08-26 08:42:30 - train: epoch 0008, iter [01000, 05004], lr: 0.098726, loss: 2.3343
2022-08-26 08:43:04 - train: epoch 0008, iter [01100, 05004], lr: 0.098719, loss: 2.1133
2022-08-26 08:43:38 - train: epoch 0008, iter [01200, 05004], lr: 0.098712, loss: 2.1729
2022-08-26 08:44:13 - train: epoch 0008, iter [01300, 05004], lr: 0.098705, loss: 2.3087
2022-08-26 08:44:47 - train: epoch 0008, iter [01400, 05004], lr: 0.098698, loss: 2.2452
2022-08-26 08:45:21 - train: epoch 0008, iter [01500, 05004], lr: 0.098691, loss: 2.3388
2022-08-26 08:45:55 - train: epoch 0008, iter [01600, 05004], lr: 0.098684, loss: 2.2834
2022-08-26 08:46:30 - train: epoch 0008, iter [01700, 05004], lr: 0.098677, loss: 2.1816
2022-08-26 08:47:05 - train: epoch 0008, iter [01800, 05004], lr: 0.098669, loss: 2.4445
2022-08-26 08:47:38 - train: epoch 0008, iter [01900, 05004], lr: 0.098662, loss: 2.0665
2022-08-26 08:48:13 - train: epoch 0008, iter [02000, 05004], lr: 0.098655, loss: 2.3267
2022-08-26 08:48:47 - train: epoch 0008, iter [02100, 05004], lr: 0.098648, loss: 2.2408
2022-08-26 08:49:21 - train: epoch 0008, iter [02200, 05004], lr: 0.098641, loss: 2.2088
2022-08-26 08:49:55 - train: epoch 0008, iter [02300, 05004], lr: 0.098633, loss: 2.3048
2022-08-26 08:50:29 - train: epoch 0008, iter [02400, 05004], lr: 0.098626, loss: 2.2045
2022-08-26 08:51:03 - train: epoch 0008, iter [02500, 05004], lr: 0.098619, loss: 2.2517
2022-08-26 08:51:36 - train: epoch 0008, iter [02600, 05004], lr: 0.098611, loss: 2.4709
2022-08-26 08:52:10 - train: epoch 0008, iter [02700, 05004], lr: 0.098604, loss: 2.3666
2022-08-26 08:52:45 - train: epoch 0008, iter [02800, 05004], lr: 0.098597, loss: 2.2732
2022-08-26 08:53:17 - train: epoch 0008, iter [02900, 05004], lr: 0.098589, loss: 2.2052
2022-08-26 08:53:51 - train: epoch 0008, iter [03000, 05004], lr: 0.098582, loss: 2.3463
2022-08-26 08:54:25 - train: epoch 0008, iter [03100, 05004], lr: 0.098574, loss: 2.2036
2022-08-26 08:55:00 - train: epoch 0008, iter [03200, 05004], lr: 0.098567, loss: 2.5216
2022-08-26 08:55:33 - train: epoch 0008, iter [03300, 05004], lr: 0.098559, loss: 2.5325
2022-08-26 08:56:07 - train: epoch 0008, iter [03400, 05004], lr: 0.098552, loss: 2.5157
2022-08-26 08:56:42 - train: epoch 0008, iter [03500, 05004], lr: 0.098544, loss: 2.2268
2022-08-26 08:57:16 - train: epoch 0008, iter [03600, 05004], lr: 0.098537, loss: 2.3518
2022-08-26 08:57:50 - train: epoch 0008, iter [03700, 05004], lr: 0.098529, loss: 2.2950
2022-08-26 08:58:24 - train: epoch 0008, iter [03800, 05004], lr: 0.098522, loss: 2.1343
2022-08-26 08:58:58 - train: epoch 0008, iter [03900, 05004], lr: 0.098514, loss: 2.3437
2022-08-26 08:59:32 - train: epoch 0008, iter [04000, 05004], lr: 0.098507, loss: 2.5562
2022-08-26 09:00:06 - train: epoch 0008, iter [04100, 05004], lr: 0.098499, loss: 2.2266
2022-08-26 09:00:40 - train: epoch 0008, iter [04200, 05004], lr: 0.098491, loss: 2.1770
2022-08-26 09:01:15 - train: epoch 0008, iter [04300, 05004], lr: 0.098484, loss: 1.9978
2022-08-26 09:01:49 - train: epoch 0008, iter [04400, 05004], lr: 0.098476, loss: 2.2391
2022-08-26 09:02:22 - train: epoch 0008, iter [04500, 05004], lr: 0.098468, loss: 2.4613
2022-08-26 09:02:57 - train: epoch 0008, iter [04600, 05004], lr: 0.098461, loss: 2.3263
2022-08-26 09:03:31 - train: epoch 0008, iter [04700, 05004], lr: 0.098453, loss: 2.2062
2022-08-26 09:04:05 - train: epoch 0008, iter [04800, 05004], lr: 0.098445, loss: 2.3035
2022-08-26 09:04:39 - train: epoch 0008, iter [04900, 05004], lr: 0.098437, loss: 2.4199
2022-08-26 09:05:11 - train: epoch 0008, iter [05000, 05004], lr: 0.098429, loss: 2.2206
2022-08-26 09:05:13 - train: epoch 008, train_loss: 2.2742
2022-08-26 09:06:28 - eval: epoch: 008, acc1: 52.838%, acc5: 78.096%, test_loss: 2.0179, per_image_load_time: 1.121ms, per_image_inference_time: 0.491ms
2022-08-26 09:06:29 - until epoch: 008, best_acc1: 52.838%
2022-08-26 09:06:29 - epoch 009 lr: 0.098429
2022-08-26 09:07:07 - train: epoch 0009, iter [00100, 05004], lr: 0.098421, loss: 1.9163
2022-08-26 09:07:41 - train: epoch 0009, iter [00200, 05004], lr: 0.098414, loss: 2.0334
2022-08-26 09:08:15 - train: epoch 0009, iter [00300, 05004], lr: 0.098406, loss: 1.9123
2022-08-26 09:08:48 - train: epoch 0009, iter [00400, 05004], lr: 0.098398, loss: 2.3979
2022-08-26 09:09:22 - train: epoch 0009, iter [00500, 05004], lr: 0.098390, loss: 2.2116
2022-08-26 09:09:56 - train: epoch 0009, iter [00600, 05004], lr: 0.098382, loss: 2.2235
2022-08-26 09:10:30 - train: epoch 0009, iter [00700, 05004], lr: 0.098374, loss: 2.2112
2022-08-26 09:11:03 - train: epoch 0009, iter [00800, 05004], lr: 0.098366, loss: 2.0874
2022-08-26 09:11:36 - train: epoch 0009, iter [00900, 05004], lr: 0.098358, loss: 2.1331
2022-08-26 09:12:10 - train: epoch 0009, iter [01000, 05004], lr: 0.098350, loss: 2.0763
2022-08-26 09:12:44 - train: epoch 0009, iter [01100, 05004], lr: 0.098342, loss: 2.5509
2022-08-26 09:13:18 - train: epoch 0009, iter [01200, 05004], lr: 0.098334, loss: 2.3393
2022-08-26 09:13:52 - train: epoch 0009, iter [01300, 05004], lr: 0.098326, loss: 2.3714
2022-08-26 09:14:26 - train: epoch 0009, iter [01400, 05004], lr: 0.098318, loss: 1.8973
2022-08-26 09:14:59 - train: epoch 0009, iter [01500, 05004], lr: 0.098310, loss: 2.0841
2022-08-26 09:15:33 - train: epoch 0009, iter [01600, 05004], lr: 0.098302, loss: 2.2466
2022-08-26 09:16:07 - train: epoch 0009, iter [01700, 05004], lr: 0.098294, loss: 2.3577
2022-08-26 09:16:41 - train: epoch 0009, iter [01800, 05004], lr: 0.098286, loss: 2.1739
2022-08-26 09:17:14 - train: epoch 0009, iter [01900, 05004], lr: 0.098277, loss: 1.9302
2022-08-26 09:17:49 - train: epoch 0009, iter [02000, 05004], lr: 0.098269, loss: 1.9088
2022-08-26 09:18:22 - train: epoch 0009, iter [02100, 05004], lr: 0.098261, loss: 2.2799
2022-08-26 09:18:56 - train: epoch 0009, iter [02200, 05004], lr: 0.098253, loss: 2.3160
2022-08-26 09:19:30 - train: epoch 0009, iter [02300, 05004], lr: 0.098245, loss: 2.0127
2022-08-26 09:20:03 - train: epoch 0009, iter [02400, 05004], lr: 0.098236, loss: 2.1297
2022-08-26 09:20:37 - train: epoch 0009, iter [02500, 05004], lr: 0.098228, loss: 2.0945
2022-08-26 09:21:11 - train: epoch 0009, iter [02600, 05004], lr: 0.098220, loss: 2.1971
2022-08-26 09:21:44 - train: epoch 0009, iter [02700, 05004], lr: 0.098211, loss: 2.1844
2022-08-26 09:22:18 - train: epoch 0009, iter [02800, 05004], lr: 0.098203, loss: 2.2946
2022-08-26 09:22:52 - train: epoch 0009, iter [02900, 05004], lr: 0.098195, loss: 1.9503
2022-08-26 09:23:25 - train: epoch 0009, iter [03000, 05004], lr: 0.098186, loss: 2.1847
2022-08-26 09:23:59 - train: epoch 0009, iter [03100, 05004], lr: 0.098178, loss: 2.2202
2022-08-26 09:24:32 - train: epoch 0009, iter [03200, 05004], lr: 0.098170, loss: 2.2752
2022-08-26 09:25:06 - train: epoch 0009, iter [03300, 05004], lr: 0.098161, loss: 2.2485
2022-08-26 09:25:40 - train: epoch 0009, iter [03400, 05004], lr: 0.098153, loss: 2.3819
2022-08-26 09:26:14 - train: epoch 0009, iter [03500, 05004], lr: 0.098144, loss: 2.3091
2022-08-26 09:26:47 - train: epoch 0009, iter [03600, 05004], lr: 0.098136, loss: 2.1419
2022-08-26 09:27:21 - train: epoch 0009, iter [03700, 05004], lr: 0.098127, loss: 2.4490
2022-08-26 09:27:55 - train: epoch 0009, iter [03800, 05004], lr: 0.098119, loss: 2.4233
2022-08-26 09:28:29 - train: epoch 0009, iter [03900, 05004], lr: 0.098110, loss: 1.9144
2022-08-26 09:29:02 - train: epoch 0009, iter [04000, 05004], lr: 0.098102, loss: 2.4207
2022-08-26 09:29:36 - train: epoch 0009, iter [04100, 05004], lr: 0.098093, loss: 2.1965
2022-08-26 09:30:11 - train: epoch 0009, iter [04200, 05004], lr: 0.098084, loss: 2.0826
2022-08-26 09:30:44 - train: epoch 0009, iter [04300, 05004], lr: 0.098076, loss: 2.3391
2022-08-26 09:31:18 - train: epoch 0009, iter [04400, 05004], lr: 0.098067, loss: 2.1544
2022-08-26 09:31:53 - train: epoch 0009, iter [04500, 05004], lr: 0.098059, loss: 2.0722
2022-08-26 09:32:26 - train: epoch 0009, iter [04600, 05004], lr: 0.098050, loss: 2.3596
2022-08-26 09:33:00 - train: epoch 0009, iter [04700, 05004], lr: 0.098041, loss: 2.4204
2022-08-26 09:33:34 - train: epoch 0009, iter [04800, 05004], lr: 0.098033, loss: 2.4922
2022-08-26 09:34:08 - train: epoch 0009, iter [04900, 05004], lr: 0.098024, loss: 2.2870
2022-08-26 09:34:40 - train: epoch 0009, iter [05000, 05004], lr: 0.098015, loss: 2.1436
2022-08-26 09:34:41 - train: epoch 009, train_loss: 2.2253
2022-08-26 09:35:56 - eval: epoch: 009, acc1: 49.718%, acc5: 74.266%, test_loss: 2.9874, per_image_load_time: 1.310ms, per_image_inference_time: 0.484ms
2022-08-26 09:35:56 - until epoch: 009, best_acc1: 52.838%
2022-08-26 09:35:56 - epoch 010 lr: 0.098015
2022-08-26 09:36:35 - train: epoch 0010, iter [00100, 05004], lr: 0.098006, loss: 2.1343
2022-08-26 09:37:08 - train: epoch 0010, iter [00200, 05004], lr: 0.097997, loss: 2.3102
2022-08-26 09:37:42 - train: epoch 0010, iter [00300, 05004], lr: 0.097988, loss: 2.1564
2022-08-26 09:38:17 - train: epoch 0010, iter [00400, 05004], lr: 0.097980, loss: 2.2401
2022-08-26 09:38:50 - train: epoch 0010, iter [00500, 05004], lr: 0.097971, loss: 2.0738
2022-08-26 09:39:25 - train: epoch 0010, iter [00600, 05004], lr: 0.097962, loss: 2.1958
2022-08-26 09:39:58 - train: epoch 0010, iter [00700, 05004], lr: 0.097953, loss: 2.2442
2022-08-26 09:40:32 - train: epoch 0010, iter [00800, 05004], lr: 0.097944, loss: 2.0654
2022-08-26 09:41:05 - train: epoch 0010, iter [00900, 05004], lr: 0.097935, loss: 1.9863
2022-08-26 09:41:39 - train: epoch 0010, iter [01000, 05004], lr: 0.097926, loss: 2.0070
2022-08-26 09:42:14 - train: epoch 0010, iter [01100, 05004], lr: 0.097917, loss: 2.1447
2022-08-26 09:42:47 - train: epoch 0010, iter [01200, 05004], lr: 0.097908, loss: 1.9307
2022-08-26 09:43:21 - train: epoch 0010, iter [01300, 05004], lr: 0.097899, loss: 2.0693
2022-08-26 09:43:56 - train: epoch 0010, iter [01400, 05004], lr: 0.097890, loss: 2.2172
2022-08-26 09:44:29 - train: epoch 0010, iter [01500, 05004], lr: 0.097881, loss: 1.9340
2022-08-26 09:45:03 - train: epoch 0010, iter [01600, 05004], lr: 0.097872, loss: 2.2658
2022-08-26 09:45:36 - train: epoch 0010, iter [01700, 05004], lr: 0.097863, loss: 2.3053
2022-08-26 09:46:10 - train: epoch 0010, iter [01800, 05004], lr: 0.097854, loss: 2.2474
2022-08-26 09:46:44 - train: epoch 0010, iter [01900, 05004], lr: 0.097845, loss: 2.2436
2022-08-26 09:47:18 - train: epoch 0010, iter [02000, 05004], lr: 0.097836, loss: 2.2017
2022-08-26 09:47:52 - train: epoch 0010, iter [02100, 05004], lr: 0.097827, loss: 2.0952
2022-08-26 09:48:25 - train: epoch 0010, iter [02200, 05004], lr: 0.097817, loss: 2.3075
2022-08-26 09:48:59 - train: epoch 0010, iter [02300, 05004], lr: 0.097808, loss: 2.4147
2022-08-26 09:49:32 - train: epoch 0010, iter [02400, 05004], lr: 0.097799, loss: 2.2988
2022-08-26 09:50:06 - train: epoch 0010, iter [02500, 05004], lr: 0.097790, loss: 2.1741
2022-08-26 09:50:40 - train: epoch 0010, iter [02600, 05004], lr: 0.097781, loss: 2.3089
2022-08-26 09:51:14 - train: epoch 0010, iter [02700, 05004], lr: 0.097771, loss: 1.9691
2022-08-26 09:51:48 - train: epoch 0010, iter [02800, 05004], lr: 0.097762, loss: 2.3325
2022-08-26 09:52:22 - train: epoch 0010, iter [02900, 05004], lr: 0.097753, loss: 2.1415
2022-08-26 09:52:57 - train: epoch 0010, iter [03000, 05004], lr: 0.097743, loss: 2.1679
2022-08-26 09:53:30 - train: epoch 0010, iter [03100, 05004], lr: 0.097734, loss: 2.4245
2022-08-26 09:54:04 - train: epoch 0010, iter [03200, 05004], lr: 0.097725, loss: 2.1637
2022-08-26 09:54:38 - train: epoch 0010, iter [03300, 05004], lr: 0.097715, loss: 2.3156
2022-08-26 09:55:12 - train: epoch 0010, iter [03400, 05004], lr: 0.097706, loss: 2.3498
2022-08-26 09:55:45 - train: epoch 0010, iter [03500, 05004], lr: 0.097697, loss: 2.3732
2022-08-26 09:56:19 - train: epoch 0010, iter [03600, 05004], lr: 0.097687, loss: 2.3900
2022-08-26 09:56:53 - train: epoch 0010, iter [03700, 05004], lr: 0.097678, loss: 2.0329
2022-08-26 09:57:27 - train: epoch 0010, iter [03800, 05004], lr: 0.097668, loss: 2.2677
2022-08-26 09:58:00 - train: epoch 0010, iter [03900, 05004], lr: 0.097659, loss: 1.8743
2022-08-26 09:58:34 - train: epoch 0010, iter [04000, 05004], lr: 0.097649, loss: 2.1213
2022-08-26 09:59:08 - train: epoch 0010, iter [04100, 05004], lr: 0.097640, loss: 2.0349
2022-08-26 09:59:42 - train: epoch 0010, iter [04200, 05004], lr: 0.097630, loss: 2.2362
2022-08-26 10:00:16 - train: epoch 0010, iter [04300, 05004], lr: 0.097621, loss: 2.1573
2022-08-26 10:00:50 - train: epoch 0010, iter [04400, 05004], lr: 0.097611, loss: 2.1398
2022-08-26 10:01:24 - train: epoch 0010, iter [04500, 05004], lr: 0.097601, loss: 2.0375
2022-08-26 10:01:57 - train: epoch 0010, iter [04600, 05004], lr: 0.097592, loss: 2.2567
2022-08-26 10:02:32 - train: epoch 0010, iter [04700, 05004], lr: 0.097582, loss: 2.2377
2022-08-26 10:03:05 - train: epoch 0010, iter [04800, 05004], lr: 0.097573, loss: 2.0788
2022-08-26 10:03:39 - train: epoch 0010, iter [04900, 05004], lr: 0.097563, loss: 2.0507
2022-08-26 10:04:11 - train: epoch 0010, iter [05000, 05004], lr: 0.097553, loss: 1.8922
2022-08-26 10:04:12 - train: epoch 010, train_loss: 2.1847
2022-08-26 10:05:27 - eval: epoch: 010, acc1: 55.218%, acc5: 79.806%, test_loss: 1.9052, per_image_load_time: 1.802ms, per_image_inference_time: 0.498ms
2022-08-26 10:05:28 - until epoch: 010, best_acc1: 55.218%
2022-08-26 10:05:28 - epoch 011 lr: 0.097553
2022-08-26 10:06:06 - train: epoch 0011, iter [00100, 05004], lr: 0.097543, loss: 1.9596
2022-08-26 10:06:39 - train: epoch 0011, iter [00200, 05004], lr: 0.097533, loss: 2.3180
2022-08-26 10:07:14 - train: epoch 0011, iter [00300, 05004], lr: 0.097524, loss: 1.9555
2022-08-26 10:07:46 - train: epoch 0011, iter [00400, 05004], lr: 0.097514, loss: 2.2080
2022-08-26 10:08:21 - train: epoch 0011, iter [00500, 05004], lr: 0.097504, loss: 2.1194
2022-08-26 10:08:54 - train: epoch 0011, iter [00600, 05004], lr: 0.097494, loss: 2.1235
2022-08-26 10:09:29 - train: epoch 0011, iter [00700, 05004], lr: 0.097484, loss: 2.1547
2022-08-26 10:10:03 - train: epoch 0011, iter [00800, 05004], lr: 0.097475, loss: 2.1865
2022-08-26 10:10:36 - train: epoch 0011, iter [00900, 05004], lr: 0.097465, loss: 2.2825
2022-08-26 10:11:11 - train: epoch 0011, iter [01000, 05004], lr: 0.097455, loss: 2.0847
2022-08-26 10:11:44 - train: epoch 0011, iter [01100, 05004], lr: 0.097445, loss: 2.2249
2022-08-26 10:12:18 - train: epoch 0011, iter [01200, 05004], lr: 0.097435, loss: 2.4740
2022-08-26 10:12:52 - train: epoch 0011, iter [01300, 05004], lr: 0.097425, loss: 2.4208
2022-08-26 10:13:26 - train: epoch 0011, iter [01400, 05004], lr: 0.097415, loss: 2.3193
2022-08-26 10:14:00 - train: epoch 0011, iter [01500, 05004], lr: 0.097405, loss: 2.0437
2022-08-26 10:14:33 - train: epoch 0011, iter [01600, 05004], lr: 0.097395, loss: 2.3241
2022-08-26 10:15:07 - train: epoch 0011, iter [01700, 05004], lr: 0.097385, loss: 2.1417
2022-08-26 10:15:41 - train: epoch 0011, iter [01800, 05004], lr: 0.097375, loss: 2.0175
2022-08-26 10:16:14 - train: epoch 0011, iter [01900, 05004], lr: 0.097365, loss: 2.0237
2022-08-26 10:16:48 - train: epoch 0011, iter [02000, 05004], lr: 0.097355, loss: 2.2931
2022-08-26 10:17:21 - train: epoch 0011, iter [02100, 05004], lr: 0.097345, loss: 2.2409
2022-08-26 10:17:56 - train: epoch 0011, iter [02200, 05004], lr: 0.097335, loss: 2.1319
2022-08-26 10:18:29 - train: epoch 0011, iter [02300, 05004], lr: 0.097325, loss: 2.3813
2022-08-26 10:19:03 - train: epoch 0011, iter [02400, 05004], lr: 0.097315, loss: 1.9470
2022-08-26 10:19:36 - train: epoch 0011, iter [02500, 05004], lr: 0.097304, loss: 2.4043
2022-08-26 10:20:10 - train: epoch 0011, iter [02600, 05004], lr: 0.097294, loss: 2.0550
2022-08-26 10:20:44 - train: epoch 0011, iter [02700, 05004], lr: 0.097284, loss: 2.2221
2022-08-26 10:21:18 - train: epoch 0011, iter [02800, 05004], lr: 0.097274, loss: 1.9251
2022-08-26 10:21:52 - train: epoch 0011, iter [02900, 05004], lr: 0.097264, loss: 2.2341
2022-08-26 10:22:25 - train: epoch 0011, iter [03000, 05004], lr: 0.097253, loss: 2.4154
2022-08-26 10:22:59 - train: epoch 0011, iter [03100, 05004], lr: 0.097243, loss: 2.1941
2022-08-26 10:23:32 - train: epoch 0011, iter [03200, 05004], lr: 0.097233, loss: 2.0103
2022-08-26 10:24:06 - train: epoch 0011, iter [03300, 05004], lr: 0.097223, loss: 2.2239
2022-08-26 10:24:40 - train: epoch 0011, iter [03400, 05004], lr: 0.097212, loss: 2.1506
2022-08-26 10:25:14 - train: epoch 0011, iter [03500, 05004], lr: 0.097202, loss: 2.1706
2022-08-26 10:25:48 - train: epoch 0011, iter [03600, 05004], lr: 0.097191, loss: 2.1466
2022-08-26 10:26:21 - train: epoch 0011, iter [03700, 05004], lr: 0.097181, loss: 2.2811
2022-08-26 10:26:55 - train: epoch 0011, iter [03800, 05004], lr: 0.097171, loss: 1.8970
2022-08-26 10:27:28 - train: epoch 0011, iter [03900, 05004], lr: 0.097160, loss: 2.2360
2022-08-26 10:28:02 - train: epoch 0011, iter [04000, 05004], lr: 0.097150, loss: 2.0932
2022-08-26 10:28:35 - train: epoch 0011, iter [04100, 05004], lr: 0.097139, loss: 1.9344
2022-08-26 10:29:09 - train: epoch 0011, iter [04200, 05004], lr: 0.097129, loss: 2.0888
2022-08-26 10:29:43 - train: epoch 0011, iter [04300, 05004], lr: 0.097118, loss: 2.2437
2022-08-26 10:30:16 - train: epoch 0011, iter [04400, 05004], lr: 0.097108, loss: 2.0606
2022-08-26 10:30:50 - train: epoch 0011, iter [04500, 05004], lr: 0.097097, loss: 2.0389
2022-08-26 10:31:24 - train: epoch 0011, iter [04600, 05004], lr: 0.097087, loss: 2.0495
2022-08-26 10:31:57 - train: epoch 0011, iter [04700, 05004], lr: 0.097076, loss: 1.8939
2022-08-26 10:32:31 - train: epoch 0011, iter [04800, 05004], lr: 0.097066, loss: 1.9134
2022-08-26 10:33:05 - train: epoch 0011, iter [04900, 05004], lr: 0.097055, loss: 2.0320
2022-08-26 10:33:37 - train: epoch 0011, iter [05000, 05004], lr: 0.097044, loss: 2.1099
2022-08-26 10:33:39 - train: epoch 011, train_loss: 2.1494
2022-08-26 10:34:55 - eval: epoch: 011, acc1: 52.862%, acc5: 78.184%, test_loss: 2.0837, per_image_load_time: 2.416ms, per_image_inference_time: 0.497ms
2022-08-26 10:34:55 - until epoch: 011, best_acc1: 55.218%
2022-08-26 10:34:55 - epoch 012 lr: 0.097044
2022-08-26 10:35:34 - train: epoch 0012, iter [00100, 05004], lr: 0.097033, loss: 2.1166
2022-08-26 10:36:08 - train: epoch 0012, iter [00200, 05004], lr: 0.097023, loss: 2.0310
2022-08-26 10:36:42 - train: epoch 0012, iter [00300, 05004], lr: 0.097012, loss: 2.1453
2022-08-26 10:37:16 - train: epoch 0012, iter [00400, 05004], lr: 0.097001, loss: 2.2351
2022-08-26 10:37:50 - train: epoch 0012, iter [00500, 05004], lr: 0.096991, loss: 2.3408
2022-08-26 10:38:23 - train: epoch 0012, iter [00600, 05004], lr: 0.096980, loss: 1.8120
2022-08-26 10:38:58 - train: epoch 0012, iter [00700, 05004], lr: 0.096969, loss: 2.0210
2022-08-26 10:39:32 - train: epoch 0012, iter [00800, 05004], lr: 0.096958, loss: 2.1804
2022-08-26 10:40:06 - train: epoch 0012, iter [00900, 05004], lr: 0.096948, loss: 2.3071
2022-08-26 10:40:40 - train: epoch 0012, iter [01000, 05004], lr: 0.096937, loss: 1.9361
2022-08-26 10:41:13 - train: epoch 0012, iter [01100, 05004], lr: 0.096926, loss: 2.4050
2022-08-26 10:41:47 - train: epoch 0012, iter [01200, 05004], lr: 0.096915, loss: 1.9489
2022-08-26 10:42:20 - train: epoch 0012, iter [01300, 05004], lr: 0.096904, loss: 2.0958
2022-08-26 10:42:54 - train: epoch 0012, iter [01400, 05004], lr: 0.096893, loss: 2.2906
2022-08-26 10:43:27 - train: epoch 0012, iter [01500, 05004], lr: 0.096882, loss: 1.9226
2022-08-26 10:44:02 - train: epoch 0012, iter [01600, 05004], lr: 0.096872, loss: 2.0143
2022-08-26 10:44:35 - train: epoch 0012, iter [01700, 05004], lr: 0.096861, loss: 1.9813
2022-08-26 10:45:09 - train: epoch 0012, iter [01800, 05004], lr: 0.096850, loss: 2.1209
2022-08-26 10:45:44 - train: epoch 0012, iter [01900, 05004], lr: 0.096839, loss: 2.1124
2022-08-26 10:46:17 - train: epoch 0012, iter [02000, 05004], lr: 0.096828, loss: 2.3582
2022-08-26 10:46:51 - train: epoch 0012, iter [02100, 05004], lr: 0.096817, loss: 2.0762
2022-08-26 10:47:24 - train: epoch 0012, iter [02200, 05004], lr: 0.096806, loss: 2.3036
2022-08-26 10:47:58 - train: epoch 0012, iter [02300, 05004], lr: 0.096795, loss: 2.0872
2022-08-26 10:48:32 - train: epoch 0012, iter [02400, 05004], lr: 0.096784, loss: 2.1919
2022-08-26 10:49:05 - train: epoch 0012, iter [02500, 05004], lr: 0.096772, loss: 1.8371
2022-08-26 10:49:40 - train: epoch 0012, iter [02600, 05004], lr: 0.096761, loss: 2.0385
2022-08-26 10:50:14 - train: epoch 0012, iter [02700, 05004], lr: 0.096750, loss: 2.0844
2022-08-26 10:50:48 - train: epoch 0012, iter [02800, 05004], lr: 0.096739, loss: 2.1126
2022-08-26 10:51:21 - train: epoch 0012, iter [02900, 05004], lr: 0.096728, loss: 1.9278
2022-08-26 10:51:55 - train: epoch 0012, iter [03000, 05004], lr: 0.096717, loss: 1.9955
2022-08-26 10:52:29 - train: epoch 0012, iter [03100, 05004], lr: 0.096706, loss: 2.3138
2022-08-26 10:53:02 - train: epoch 0012, iter [03200, 05004], lr: 0.096694, loss: 1.9122
2022-08-26 10:53:36 - train: epoch 0012, iter [03300, 05004], lr: 0.096683, loss: 2.1610
2022-08-26 10:54:10 - train: epoch 0012, iter [03400, 05004], lr: 0.096672, loss: 2.0729
2022-08-26 10:54:43 - train: epoch 0012, iter [03500, 05004], lr: 0.096661, loss: 2.2340
2022-08-26 10:55:19 - train: epoch 0012, iter [03600, 05004], lr: 0.096649, loss: 2.1527
2022-08-26 10:55:51 - train: epoch 0012, iter [03700, 05004], lr: 0.096638, loss: 2.1211
2022-08-26 10:56:25 - train: epoch 0012, iter [03800, 05004], lr: 0.096627, loss: 2.1204
2022-08-26 10:57:00 - train: epoch 0012, iter [03900, 05004], lr: 0.096615, loss: 1.9748
2022-08-26 10:57:34 - train: epoch 0012, iter [04000, 05004], lr: 0.096604, loss: 2.1432
2022-08-26 10:58:08 - train: epoch 0012, iter [04100, 05004], lr: 0.096593, loss: 1.9662
2022-08-26 10:58:42 - train: epoch 0012, iter [04200, 05004], lr: 0.096581, loss: 2.0394
2022-08-26 10:59:16 - train: epoch 0012, iter [04300, 05004], lr: 0.096570, loss: 2.1676
2022-08-26 10:59:49 - train: epoch 0012, iter [04400, 05004], lr: 0.096558, loss: 1.9415
2022-08-26 11:00:24 - train: epoch 0012, iter [04500, 05004], lr: 0.096547, loss: 1.9722
2022-08-26 11:00:58 - train: epoch 0012, iter [04600, 05004], lr: 0.096535, loss: 2.3572
2022-08-26 11:01:32 - train: epoch 0012, iter [04700, 05004], lr: 0.096524, loss: 2.0712
2022-08-26 11:02:06 - train: epoch 0012, iter [04800, 05004], lr: 0.096512, loss: 2.3496
2022-08-26 11:02:40 - train: epoch 0012, iter [04900, 05004], lr: 0.096501, loss: 2.1094
2022-08-26 11:03:12 - train: epoch 0012, iter [05000, 05004], lr: 0.096489, loss: 1.8658
2022-08-26 11:03:13 - train: epoch 012, train_loss: 2.1234
2022-08-26 11:04:29 - eval: epoch: 012, acc1: 44.450%, acc5: 67.586%, test_loss: 4.7448, per_image_load_time: 2.355ms, per_image_inference_time: 0.494ms
2022-08-26 11:04:29 - until epoch: 012, best_acc1: 55.218%
2022-08-26 11:04:29 - epoch 013 lr: 0.096489
2022-08-26 11:05:07 - train: epoch 0013, iter [00100, 05004], lr: 0.096477, loss: 1.8080
2022-08-26 11:05:43 - train: epoch 0013, iter [00200, 05004], lr: 0.096466, loss: 2.0248
2022-08-26 11:06:16 - train: epoch 0013, iter [00300, 05004], lr: 0.096454, loss: 2.0220
2022-08-26 11:06:50 - train: epoch 0013, iter [00400, 05004], lr: 0.096442, loss: 2.0115
2022-08-26 11:07:23 - train: epoch 0013, iter [00500, 05004], lr: 0.096431, loss: 2.1049
2022-08-26 11:07:57 - train: epoch 0013, iter [00600, 05004], lr: 0.096419, loss: 2.2229
2022-08-26 11:08:31 - train: epoch 0013, iter [00700, 05004], lr: 0.096407, loss: 2.0192
2022-08-26 11:09:05 - train: epoch 0013, iter [00800, 05004], lr: 0.096396, loss: 2.2073
2022-08-26 11:09:39 - train: epoch 0013, iter [00900, 05004], lr: 0.096384, loss: 1.9960
2022-08-26 11:10:13 - train: epoch 0013, iter [01000, 05004], lr: 0.096372, loss: 2.0917
2022-08-26 11:10:46 - train: epoch 0013, iter [01100, 05004], lr: 0.096361, loss: 2.1525
2022-08-26 11:11:21 - train: epoch 0013, iter [01200, 05004], lr: 0.096349, loss: 2.2508
2022-08-26 11:11:54 - train: epoch 0013, iter [01300, 05004], lr: 0.096337, loss: 2.1441
2022-08-26 11:12:28 - train: epoch 0013, iter [01400, 05004], lr: 0.096325, loss: 1.8587
2022-08-26 11:13:02 - train: epoch 0013, iter [01500, 05004], lr: 0.096313, loss: 2.0766
2022-08-26 11:13:35 - train: epoch 0013, iter [01600, 05004], lr: 0.096302, loss: 1.9068
2022-08-26 11:14:09 - train: epoch 0013, iter [01700, 05004], lr: 0.096290, loss: 2.1883
2022-08-26 11:14:43 - train: epoch 0013, iter [01800, 05004], lr: 0.096278, loss: 2.1053
2022-08-26 11:15:16 - train: epoch 0013, iter [01900, 05004], lr: 0.096266, loss: 2.1354
2022-08-26 11:15:51 - train: epoch 0013, iter [02000, 05004], lr: 0.096254, loss: 2.3279
2022-08-26 11:16:25 - train: epoch 0013, iter [02100, 05004], lr: 0.096242, loss: 2.3633
2022-08-26 11:16:58 - train: epoch 0013, iter [02200, 05004], lr: 0.096230, loss: 1.9826
2022-08-26 11:17:33 - train: epoch 0013, iter [02300, 05004], lr: 0.096218, loss: 2.1853
2022-08-26 11:18:06 - train: epoch 0013, iter [02400, 05004], lr: 0.096206, loss: 2.1039
2022-08-26 11:18:40 - train: epoch 0013, iter [02500, 05004], lr: 0.096194, loss: 1.8923
2022-08-26 11:19:14 - train: epoch 0013, iter [02600, 05004], lr: 0.096182, loss: 2.0591
2022-08-26 11:19:47 - train: epoch 0013, iter [02700, 05004], lr: 0.096170, loss: 1.8772
2022-08-26 11:20:22 - train: epoch 0013, iter [02800, 05004], lr: 0.096158, loss: 2.1564
2022-08-26 11:20:55 - train: epoch 0013, iter [02900, 05004], lr: 0.096146, loss: 2.1939
2022-08-26 11:21:29 - train: epoch 0013, iter [03000, 05004], lr: 0.096134, loss: 2.0711
2022-08-26 11:22:03 - train: epoch 0013, iter [03100, 05004], lr: 0.096122, loss: 1.9216
2022-08-26 11:22:37 - train: epoch 0013, iter [03200, 05004], lr: 0.096110, loss: 2.1064
2022-08-26 11:23:11 - train: epoch 0013, iter [03300, 05004], lr: 0.096098, loss: 1.9319
2022-08-26 11:23:45 - train: epoch 0013, iter [03400, 05004], lr: 0.096085, loss: 2.0452
2022-08-26 11:24:19 - train: epoch 0013, iter [03500, 05004], lr: 0.096073, loss: 2.0334
2022-08-26 11:24:52 - train: epoch 0013, iter [03600, 05004], lr: 0.096061, loss: 2.3830
2022-08-26 11:25:26 - train: epoch 0013, iter [03700, 05004], lr: 0.096049, loss: 1.8137
2022-08-26 11:26:00 - train: epoch 0013, iter [03800, 05004], lr: 0.096037, loss: 2.2525
2022-08-26 11:26:34 - train: epoch 0013, iter [03900, 05004], lr: 0.096024, loss: 2.2237
2022-08-26 11:27:08 - train: epoch 0013, iter [04000, 05004], lr: 0.096012, loss: 2.1745
2022-08-26 11:27:42 - train: epoch 0013, iter [04100, 05004], lr: 0.096000, loss: 2.0531
2022-08-26 11:28:15 - train: epoch 0013, iter [04200, 05004], lr: 0.095987, loss: 2.0970
2022-08-26 11:28:49 - train: epoch 0013, iter [04300, 05004], lr: 0.095975, loss: 1.9540
2022-08-26 11:29:23 - train: epoch 0013, iter [04400, 05004], lr: 0.095963, loss: 2.0416
2022-08-26 11:29:57 - train: epoch 0013, iter [04500, 05004], lr: 0.095950, loss: 1.9595
2022-08-26 11:30:31 - train: epoch 0013, iter [04600, 05004], lr: 0.095938, loss: 2.1119
2022-08-26 11:31:05 - train: epoch 0013, iter [04700, 05004], lr: 0.095926, loss: 2.2038
2022-08-26 11:31:40 - train: epoch 0013, iter [04800, 05004], lr: 0.095913, loss: 2.2362
2022-08-26 11:32:14 - train: epoch 0013, iter [04900, 05004], lr: 0.095901, loss: 2.1951
2022-08-26 11:32:46 - train: epoch 0013, iter [05000, 05004], lr: 0.095888, loss: 2.2279
2022-08-26 11:32:47 - train: epoch 013, train_loss: 2.0968
2022-08-26 11:34:03 - eval: epoch: 013, acc1: 53.174%, acc5: 78.090%, test_loss: 2.2334, per_image_load_time: 2.438ms, per_image_inference_time: 0.497ms
2022-08-26 11:34:03 - until epoch: 013, best_acc1: 55.218%
2022-08-26 11:34:03 - epoch 014 lr: 0.095888
2022-08-26 11:34:43 - train: epoch 0014, iter [00100, 05004], lr: 0.095875, loss: 2.1181
2022-08-26 11:35:16 - train: epoch 0014, iter [00200, 05004], lr: 0.095863, loss: 2.0886
2022-08-26 11:35:50 - train: epoch 0014, iter [00300, 05004], lr: 0.095850, loss: 1.7988
2022-08-26 11:36:24 - train: epoch 0014, iter [00400, 05004], lr: 0.095838, loss: 1.9581
2022-08-26 11:36:58 - train: epoch 0014, iter [00500, 05004], lr: 0.095825, loss: 2.0029
2022-08-26 11:37:32 - train: epoch 0014, iter [00600, 05004], lr: 0.095813, loss: 2.0872
2022-08-26 11:38:06 - train: epoch 0014, iter [00700, 05004], lr: 0.095800, loss: 2.0216
2022-08-26 11:38:41 - train: epoch 0014, iter [00800, 05004], lr: 0.095787, loss: 2.0257
2022-08-26 11:39:14 - train: epoch 0014, iter [00900, 05004], lr: 0.095775, loss: 2.0673
2022-08-26 11:39:48 - train: epoch 0014, iter [01000, 05004], lr: 0.095762, loss: 2.2664
2022-08-26 11:40:22 - train: epoch 0014, iter [01100, 05004], lr: 0.095750, loss: 2.0632
2022-08-26 11:40:56 - train: epoch 0014, iter [01200, 05004], lr: 0.095737, loss: 2.0639
2022-08-26 11:41:30 - train: epoch 0014, iter [01300, 05004], lr: 0.095724, loss: 2.1380
2022-08-26 11:42:04 - train: epoch 0014, iter [01400, 05004], lr: 0.095711, loss: 2.1815
2022-08-26 11:42:38 - train: epoch 0014, iter [01500, 05004], lr: 0.095699, loss: 2.1293
2022-08-26 11:43:11 - train: epoch 0014, iter [01600, 05004], lr: 0.095686, loss: 2.1302
2022-08-26 11:43:46 - train: epoch 0014, iter [01700, 05004], lr: 0.095673, loss: 2.2762
2022-08-26 11:44:20 - train: epoch 0014, iter [01800, 05004], lr: 0.095660, loss: 2.2584
2022-08-26 11:44:53 - train: epoch 0014, iter [01900, 05004], lr: 0.095648, loss: 1.9603
2022-08-26 11:45:27 - train: epoch 0014, iter [02000, 05004], lr: 0.095635, loss: 1.9799
2022-08-26 11:46:01 - train: epoch 0014, iter [02100, 05004], lr: 0.095622, loss: 2.1610
2022-08-26 11:46:35 - train: epoch 0014, iter [02200, 05004], lr: 0.095609, loss: 2.1159
2022-08-26 11:47:09 - train: epoch 0014, iter [02300, 05004], lr: 0.095596, loss: 2.1108
2022-08-26 11:47:43 - train: epoch 0014, iter [02400, 05004], lr: 0.095583, loss: 2.1786
2022-08-26 11:48:17 - train: epoch 0014, iter [02500, 05004], lr: 0.095570, loss: 1.9789
2022-08-26 11:48:51 - train: epoch 0014, iter [02600, 05004], lr: 0.095557, loss: 2.0470
2022-08-26 11:49:25 - train: epoch 0014, iter [02700, 05004], lr: 0.095545, loss: 2.0988
2022-08-26 11:49:59 - train: epoch 0014, iter [02800, 05004], lr: 0.095532, loss: 2.3877
2022-08-26 11:50:33 - train: epoch 0014, iter [02900, 05004], lr: 0.095519, loss: 1.9462
2022-08-26 11:51:07 - train: epoch 0014, iter [03000, 05004], lr: 0.095506, loss: 2.1652
2022-08-26 11:51:41 - train: epoch 0014, iter [03100, 05004], lr: 0.095493, loss: 1.8859
2022-08-26 11:52:14 - train: epoch 0014, iter [03200, 05004], lr: 0.095480, loss: 2.0372
2022-08-26 11:52:48 - train: epoch 0014, iter [03300, 05004], lr: 0.095467, loss: 1.8986
2022-08-26 11:53:22 - train: epoch 0014, iter [03400, 05004], lr: 0.095453, loss: 2.0102
2022-08-26 11:53:56 - train: epoch 0014, iter [03500, 05004], lr: 0.095440, loss: 1.9652
2022-08-26 11:54:30 - train: epoch 0014, iter [03600, 05004], lr: 0.095427, loss: 1.9624
2022-08-26 11:55:04 - train: epoch 0014, iter [03700, 05004], lr: 0.095414, loss: 1.9594
2022-08-26 11:55:38 - train: epoch 0014, iter [03800, 05004], lr: 0.095401, loss: 2.2889
2022-08-26 11:56:12 - train: epoch 0014, iter [03900, 05004], lr: 0.095388, loss: 2.0547
2022-08-26 11:56:47 - train: epoch 0014, iter [04000, 05004], lr: 0.095375, loss: 2.2094
2022-08-26 11:57:20 - train: epoch 0014, iter [04100, 05004], lr: 0.095361, loss: 1.9877
2022-08-26 11:57:55 - train: epoch 0014, iter [04200, 05004], lr: 0.095348, loss: 1.9358
2022-08-26 11:58:28 - train: epoch 0014, iter [04300, 05004], lr: 0.095335, loss: 1.9846
2022-08-26 11:59:02 - train: epoch 0014, iter [04400, 05004], lr: 0.095322, loss: 1.8797
2022-08-26 11:59:37 - train: epoch 0014, iter [04500, 05004], lr: 0.095308, loss: 2.0164
2022-08-26 12:00:11 - train: epoch 0014, iter [04600, 05004], lr: 0.095295, loss: 1.8682
2022-08-26 12:00:45 - train: epoch 0014, iter [04700, 05004], lr: 0.095282, loss: 1.9653
2022-08-26 12:01:19 - train: epoch 0014, iter [04800, 05004], lr: 0.095269, loss: 1.9099
2022-08-26 12:01:53 - train: epoch 0014, iter [04900, 05004], lr: 0.095255, loss: 1.9256
2022-08-26 12:02:25 - train: epoch 0014, iter [05000, 05004], lr: 0.095242, loss: 2.1329
2022-08-26 12:02:26 - train: epoch 014, train_loss: 2.0755
2022-08-26 12:03:42 - eval: epoch: 014, acc1: 55.698%, acc5: 80.348%, test_loss: 1.8917, per_image_load_time: 2.389ms, per_image_inference_time: 0.478ms
2022-08-26 12:03:43 - until epoch: 014, best_acc1: 55.698%
2022-08-26 12:03:43 - epoch 015 lr: 0.095241
2022-08-26 12:04:22 - train: epoch 0015, iter [00100, 05004], lr: 0.095228, loss: 1.7735
2022-08-26 12:04:56 - train: epoch 0015, iter [00200, 05004], lr: 0.095215, loss: 2.2338
2022-08-26 12:05:29 - train: epoch 0015, iter [00300, 05004], lr: 0.095201, loss: 2.1102
2022-08-26 12:06:04 - train: epoch 0015, iter [00400, 05004], lr: 0.095188, loss: 1.9991
2022-08-26 12:06:38 - train: epoch 0015, iter [00500, 05004], lr: 0.095174, loss: 2.0741
2022-08-26 12:07:11 - train: epoch 0015, iter [00600, 05004], lr: 0.095161, loss: 2.1052
2022-08-26 12:07:46 - train: epoch 0015, iter [00700, 05004], lr: 0.095147, loss: 1.9879
2022-08-26 12:08:20 - train: epoch 0015, iter [00800, 05004], lr: 0.095134, loss: 1.9094
2022-08-26 12:08:54 - train: epoch 0015, iter [00900, 05004], lr: 0.095120, loss: 1.9967
2022-08-26 12:09:28 - train: epoch 0015, iter [01000, 05004], lr: 0.095107, loss: 2.1708
2022-08-26 12:10:01 - train: epoch 0015, iter [01100, 05004], lr: 0.095093, loss: 1.9251
2022-08-26 12:10:35 - train: epoch 0015, iter [01200, 05004], lr: 0.095080, loss: 1.9937
2022-08-26 12:11:09 - train: epoch 0015, iter [01300, 05004], lr: 0.095066, loss: 2.4084
2022-08-26 12:11:43 - train: epoch 0015, iter [01400, 05004], lr: 0.095052, loss: 1.9300
2022-08-26 12:12:16 - train: epoch 0015, iter [01500, 05004], lr: 0.095039, loss: 1.7666
2022-08-26 12:12:50 - train: epoch 0015, iter [01600, 05004], lr: 0.095025, loss: 2.0857
2022-08-26 12:13:24 - train: epoch 0015, iter [01700, 05004], lr: 0.095012, loss: 2.2183
2022-08-26 12:13:57 - train: epoch 0015, iter [01800, 05004], lr: 0.094998, loss: 1.9138
2022-08-26 12:14:31 - train: epoch 0015, iter [01900, 05004], lr: 0.094984, loss: 1.8267
2022-08-26 12:15:05 - train: epoch 0015, iter [02000, 05004], lr: 0.094970, loss: 2.0620
2022-08-26 12:15:39 - train: epoch 0015, iter [02100, 05004], lr: 0.094957, loss: 1.9659
2022-08-26 12:16:12 - train: epoch 0015, iter [02200, 05004], lr: 0.094943, loss: 2.2679
2022-08-26 12:16:46 - train: epoch 0015, iter [02300, 05004], lr: 0.094929, loss: 1.8955
2022-08-26 12:17:21 - train: epoch 0015, iter [02400, 05004], lr: 0.094915, loss: 2.1426
2022-08-26 12:17:54 - train: epoch 0015, iter [02500, 05004], lr: 0.094902, loss: 2.0907
2022-08-26 12:18:29 - train: epoch 0015, iter [02600, 05004], lr: 0.094888, loss: 1.8871
2022-08-26 12:19:03 - train: epoch 0015, iter [02700, 05004], lr: 0.094874, loss: 2.1542
2022-08-26 12:19:37 - train: epoch 0015, iter [02800, 05004], lr: 0.094860, loss: 2.1456
2022-08-26 12:20:10 - train: epoch 0015, iter [02900, 05004], lr: 0.094846, loss: 1.9904
2022-08-26 12:20:44 - train: epoch 0015, iter [03000, 05004], lr: 0.094832, loss: 1.9194
2022-08-26 12:21:18 - train: epoch 0015, iter [03100, 05004], lr: 0.094818, loss: 1.9721
2022-08-26 12:21:52 - train: epoch 0015, iter [03200, 05004], lr: 0.094805, loss: 2.0396
2022-08-26 12:22:26 - train: epoch 0015, iter [03300, 05004], lr: 0.094791, loss: 1.8324
2022-08-26 12:23:01 - train: epoch 0015, iter [03400, 05004], lr: 0.094777, loss: 2.1108
2022-08-26 12:23:33 - train: epoch 0015, iter [03500, 05004], lr: 0.094763, loss: 2.2410
2022-08-26 12:24:08 - train: epoch 0015, iter [03600, 05004], lr: 0.094749, loss: 2.1380
2022-08-26 12:24:42 - train: epoch 0015, iter [03700, 05004], lr: 0.094735, loss: 1.8620
2022-08-26 12:25:16 - train: epoch 0015, iter [03800, 05004], lr: 0.094721, loss: 2.0265
2022-08-26 12:25:49 - train: epoch 0015, iter [03900, 05004], lr: 0.094707, loss: 2.2058
2022-08-26 12:26:23 - train: epoch 0015, iter [04000, 05004], lr: 0.094693, loss: 2.0860
2022-08-26 12:26:57 - train: epoch 0015, iter [04100, 05004], lr: 0.094678, loss: 2.2203
2022-08-26 12:27:31 - train: epoch 0015, iter [04200, 05004], lr: 0.094664, loss: 1.8750
2022-08-26 12:28:05 - train: epoch 0015, iter [04300, 05004], lr: 0.094650, loss: 2.0181
2022-08-26 12:28:39 - train: epoch 0015, iter [04400, 05004], lr: 0.094636, loss: 2.0796
2022-08-26 12:29:13 - train: epoch 0015, iter [04500, 05004], lr: 0.094622, loss: 2.0910
2022-08-26 12:29:47 - train: epoch 0015, iter [04600, 05004], lr: 0.094608, loss: 2.0174
2022-08-26 12:30:21 - train: epoch 0015, iter [04700, 05004], lr: 0.094594, loss: 2.1264
2022-08-26 12:30:55 - train: epoch 0015, iter [04800, 05004], lr: 0.094579, loss: 1.9825
2022-08-26 12:31:29 - train: epoch 0015, iter [04900, 05004], lr: 0.094565, loss: 2.0094
2022-08-26 12:32:01 - train: epoch 0015, iter [05000, 05004], lr: 0.094551, loss: 2.1168
2022-08-26 12:32:02 - train: epoch 015, train_loss: 2.0567
2022-08-26 12:33:17 - eval: epoch: 015, acc1: 55.984%, acc5: 80.360%, test_loss: 1.8844, per_image_load_time: 2.264ms, per_image_inference_time: 0.501ms
2022-08-26 12:33:18 - until epoch: 015, best_acc1: 55.984%
2022-08-26 12:33:18 - epoch 016 lr: 0.094550
2022-08-26 12:33:56 - train: epoch 0016, iter [00100, 05004], lr: 0.094536, loss: 2.0364
2022-08-26 12:34:31 - train: epoch 0016, iter [00200, 05004], lr: 0.094522, loss: 1.9195
2022-08-26 12:35:05 - train: epoch 0016, iter [00300, 05004], lr: 0.094507, loss: 2.0729
2022-08-26 12:35:39 - train: epoch 0016, iter [00400, 05004], lr: 0.094493, loss: 2.2398
2022-08-26 12:36:12 - train: epoch 0016, iter [00500, 05004], lr: 0.094479, loss: 1.8498
2022-08-26 12:36:46 - train: epoch 0016, iter [00600, 05004], lr: 0.094465, loss: 2.2134
2022-08-26 12:37:20 - train: epoch 0016, iter [00700, 05004], lr: 0.094450, loss: 1.8177
2022-08-26 12:37:54 - train: epoch 0016, iter [00800, 05004], lr: 0.094436, loss: 2.0614
2022-08-26 12:38:27 - train: epoch 0016, iter [00900, 05004], lr: 0.094421, loss: 2.0590
2022-08-26 12:39:01 - train: epoch 0016, iter [01000, 05004], lr: 0.094407, loss: 1.8388
2022-08-26 12:39:34 - train: epoch 0016, iter [01100, 05004], lr: 0.094393, loss: 1.9162
2022-08-26 12:40:08 - train: epoch 0016, iter [01200, 05004], lr: 0.094378, loss: 1.9243
2022-08-26 12:40:42 - train: epoch 0016, iter [01300, 05004], lr: 0.094364, loss: 2.1983
2022-08-26 12:41:17 - train: epoch 0016, iter [01400, 05004], lr: 0.094349, loss: 1.8975
2022-08-26 12:41:50 - train: epoch 0016, iter [01500, 05004], lr: 0.094335, loss: 2.0267
2022-08-26 12:42:24 - train: epoch 0016, iter [01600, 05004], lr: 0.094320, loss: 2.1912
2022-08-26 12:42:58 - train: epoch 0016, iter [01700, 05004], lr: 0.094306, loss: 1.9786
2022-08-26 12:43:31 - train: epoch 0016, iter [01800, 05004], lr: 0.094291, loss: 1.9568
2022-08-26 12:44:05 - train: epoch 0016, iter [01900, 05004], lr: 0.094276, loss: 2.0508
2022-08-26 12:44:39 - train: epoch 0016, iter [02000, 05004], lr: 0.094262, loss: 1.7510
2022-08-26 12:45:13 - train: epoch 0016, iter [02100, 05004], lr: 0.094247, loss: 2.1890
2022-08-26 12:45:47 - train: epoch 0016, iter [02200, 05004], lr: 0.094233, loss: 2.1055
2022-08-26 12:46:20 - train: epoch 0016, iter [02300, 05004], lr: 0.094218, loss: 2.2287
2022-08-26 12:46:54 - train: epoch 0016, iter [02400, 05004], lr: 0.094203, loss: 2.2126
2022-08-26 12:47:27 - train: epoch 0016, iter [02500, 05004], lr: 0.094189, loss: 1.9859
2022-08-26 12:48:01 - train: epoch 0016, iter [02600, 05004], lr: 0.094174, loss: 2.2546
2022-08-26 12:48:35 - train: epoch 0016, iter [02700, 05004], lr: 0.094159, loss: 1.9184
2022-08-26 12:49:08 - train: epoch 0016, iter [02800, 05004], lr: 0.094144, loss: 1.9567
2022-08-26 12:49:42 - train: epoch 0016, iter [02900, 05004], lr: 0.094130, loss: 2.1031
2022-08-26 12:50:17 - train: epoch 0016, iter [03000, 05004], lr: 0.094115, loss: 2.3237
2022-08-26 12:50:51 - train: epoch 0016, iter [03100, 05004], lr: 0.094100, loss: 2.2852
2022-08-26 12:51:24 - train: epoch 0016, iter [03200, 05004], lr: 0.094085, loss: 2.3450
2022-08-26 12:51:58 - train: epoch 0016, iter [03300, 05004], lr: 0.094071, loss: 2.1031
2022-08-26 12:52:32 - train: epoch 0016, iter [03400, 05004], lr: 0.094056, loss: 2.0359
2022-08-26 12:53:06 - train: epoch 0016, iter [03500, 05004], lr: 0.094041, loss: 2.0291
2022-08-26 12:53:40 - train: epoch 0016, iter [03600, 05004], lr: 0.094026, loss: 1.8658
2022-08-26 12:54:14 - train: epoch 0016, iter [03700, 05004], lr: 0.094011, loss: 2.1561
2022-08-26 12:54:48 - train: epoch 0016, iter [03800, 05004], lr: 0.093996, loss: 2.3714
2022-08-26 12:55:21 - train: epoch 0016, iter [03900, 05004], lr: 0.093981, loss: 2.0967
2022-08-26 12:55:56 - train: epoch 0016, iter [04000, 05004], lr: 0.093966, loss: 2.0577
2022-08-26 12:56:29 - train: epoch 0016, iter [04100, 05004], lr: 0.093951, loss: 1.8890
2022-08-26 12:57:03 - train: epoch 0016, iter [04200, 05004], lr: 0.093936, loss: 2.1700
2022-08-26 12:57:37 - train: epoch 0016, iter [04300, 05004], lr: 0.093921, loss: 1.8196
2022-08-26 12:58:11 - train: epoch 0016, iter [04400, 05004], lr: 0.093906, loss: 1.9259
2022-08-26 12:58:45 - train: epoch 0016, iter [04500, 05004], lr: 0.093891, loss: 2.1729
2022-08-26 12:59:19 - train: epoch 0016, iter [04600, 05004], lr: 0.093876, loss: 1.9427
2022-08-26 12:59:52 - train: epoch 0016, iter [04700, 05004], lr: 0.093861, loss: 2.2773
2022-08-26 13:00:27 - train: epoch 0016, iter [04800, 05004], lr: 0.093846, loss: 2.0871
2022-08-26 13:01:00 - train: epoch 0016, iter [04900, 05004], lr: 0.093831, loss: 2.1407
2022-08-26 13:01:33 - train: epoch 0016, iter [05000, 05004], lr: 0.093816, loss: 2.1037
2022-08-26 13:01:34 - train: epoch 016, train_loss: 2.0390
2022-08-26 13:02:50 - eval: epoch: 016, acc1: 55.248%, acc5: 79.608%, test_loss: 1.9765, per_image_load_time: 1.694ms, per_image_inference_time: 0.495ms
2022-08-26 13:02:50 - until epoch: 016, best_acc1: 55.984%
2022-08-26 13:02:50 - epoch 017 lr: 0.093815
2022-08-26 13:03:29 - train: epoch 0017, iter [00100, 05004], lr: 0.093800, loss: 1.9166
2022-08-26 13:04:04 - train: epoch 0017, iter [00200, 05004], lr: 0.093785, loss: 1.9902
2022-08-26 13:04:38 - train: epoch 0017, iter [00300, 05004], lr: 0.093770, loss: 2.2616
2022-08-26 13:05:12 - train: epoch 0017, iter [00400, 05004], lr: 0.093755, loss: 1.7734
2022-08-26 13:05:46 - train: epoch 0017, iter [00500, 05004], lr: 0.093740, loss: 2.0638
2022-08-26 13:06:19 - train: epoch 0017, iter [00600, 05004], lr: 0.093724, loss: 2.3486
2022-08-26 13:06:53 - train: epoch 0017, iter [00700, 05004], lr: 0.093709, loss: 2.0634
2022-08-26 13:07:27 - train: epoch 0017, iter [00800, 05004], lr: 0.093694, loss: 1.8813
2022-08-26 13:08:00 - train: epoch 0017, iter [00900, 05004], lr: 0.093679, loss: 1.9549
2022-08-26 13:08:34 - train: epoch 0017, iter [01000, 05004], lr: 0.093663, loss: 1.9570
2022-08-26 13:09:08 - train: epoch 0017, iter [01100, 05004], lr: 0.093648, loss: 2.3414
2022-08-26 13:09:42 - train: epoch 0017, iter [01200, 05004], lr: 0.093633, loss: 2.1959
2022-08-26 13:10:16 - train: epoch 0017, iter [01300, 05004], lr: 0.093617, loss: 2.0122
2022-08-26 13:10:49 - train: epoch 0017, iter [01400, 05004], lr: 0.093602, loss: 2.0235
2022-08-26 13:11:23 - train: epoch 0017, iter [01500, 05004], lr: 0.093587, loss: 1.6603
2022-08-26 13:11:57 - train: epoch 0017, iter [01600, 05004], lr: 0.093571, loss: 1.8823
2022-08-26 13:12:31 - train: epoch 0017, iter [01700, 05004], lr: 0.093556, loss: 2.0181
2022-08-26 13:13:05 - train: epoch 0017, iter [01800, 05004], lr: 0.093540, loss: 1.9920
2022-08-26 13:13:39 - train: epoch 0017, iter [01900, 05004], lr: 0.093525, loss: 1.9622
2022-08-26 13:14:12 - train: epoch 0017, iter [02000, 05004], lr: 0.093509, loss: 2.2549
2022-08-26 13:14:46 - train: epoch 0017, iter [02100, 05004], lr: 0.093494, loss: 1.9270
2022-08-26 13:15:20 - train: epoch 0017, iter [02200, 05004], lr: 0.093478, loss: 1.9501
2022-08-26 13:15:54 - train: epoch 0017, iter [02300, 05004], lr: 0.093463, loss: 1.9217
2022-08-26 13:16:27 - train: epoch 0017, iter [02400, 05004], lr: 0.093447, loss: 1.9474
2022-08-26 13:17:01 - train: epoch 0017, iter [02500, 05004], lr: 0.093432, loss: 2.1739
2022-08-26 13:17:34 - train: epoch 0017, iter [02600, 05004], lr: 0.093416, loss: 1.9810
2022-08-26 13:18:08 - train: epoch 0017, iter [02700, 05004], lr: 0.093401, loss: 1.8934
2022-08-26 13:18:42 - train: epoch 0017, iter [02800, 05004], lr: 0.093385, loss: 2.2120
2022-08-26 13:19:15 - train: epoch 0017, iter [02900, 05004], lr: 0.093370, loss: 2.1840
2022-08-26 13:19:49 - train: epoch 0017, iter [03000, 05004], lr: 0.093354, loss: 1.8015
2022-08-26 13:20:23 - train: epoch 0017, iter [03100, 05004], lr: 0.093338, loss: 2.1346
2022-08-26 13:20:57 - train: epoch 0017, iter [03200, 05004], lr: 0.093323, loss: 1.9105
2022-08-26 13:21:31 - train: epoch 0017, iter [03300, 05004], lr: 0.093307, loss: 2.1275
2022-08-26 13:22:05 - train: epoch 0017, iter [03400, 05004], lr: 0.093291, loss: 1.8441
2022-08-26 13:22:39 - train: epoch 0017, iter [03500, 05004], lr: 0.093276, loss: 1.9957
2022-08-26 13:23:13 - train: epoch 0017, iter [03600, 05004], lr: 0.093260, loss: 2.3544
2022-08-26 13:23:47 - train: epoch 0017, iter [03700, 05004], lr: 0.093244, loss: 2.0179
2022-08-26 13:24:21 - train: epoch 0017, iter [03800, 05004], lr: 0.093228, loss: 2.2981
2022-08-26 13:24:56 - train: epoch 0017, iter [03900, 05004], lr: 0.093212, loss: 1.8334
2022-08-26 13:25:29 - train: epoch 0017, iter [04000, 05004], lr: 0.093197, loss: 1.9257
2022-08-26 13:26:03 - train: epoch 0017, iter [04100, 05004], lr: 0.093181, loss: 2.0740
2022-08-26 13:26:37 - train: epoch 0017, iter [04200, 05004], lr: 0.093165, loss: 2.0205
2022-08-26 13:27:11 - train: epoch 0017, iter [04300, 05004], lr: 0.093149, loss: 2.0298
2022-08-26 13:27:45 - train: epoch 0017, iter [04400, 05004], lr: 0.093133, loss: 1.9633
2022-08-26 13:28:19 - train: epoch 0017, iter [04500, 05004], lr: 0.093117, loss: 2.2205
2022-08-26 13:28:53 - train: epoch 0017, iter [04600, 05004], lr: 0.093102, loss: 2.0270
2022-08-26 13:29:27 - train: epoch 0017, iter [04700, 05004], lr: 0.093086, loss: 2.1146
2022-08-26 13:30:01 - train: epoch 0017, iter [04800, 05004], lr: 0.093070, loss: 2.1647
2022-08-26 13:30:35 - train: epoch 0017, iter [04900, 05004], lr: 0.093054, loss: 1.8474
2022-08-26 13:31:08 - train: epoch 0017, iter [05000, 05004], lr: 0.093038, loss: 1.7964
2022-08-26 13:31:09 - train: epoch 017, train_loss: 2.0217
2022-08-26 13:32:25 - eval: epoch: 017, acc1: 54.968%, acc5: 79.780%, test_loss: 1.9202, per_image_load_time: 1.511ms, per_image_inference_time: 0.503ms
2022-08-26 13:32:25 - until epoch: 017, best_acc1: 55.984%
2022-08-26 13:32:25 - epoch 018 lr: 0.093037
2022-08-26 13:33:04 - train: epoch 0018, iter [00100, 05004], lr: 0.093021, loss: 1.9862
2022-08-26 13:33:39 - train: epoch 0018, iter [00200, 05004], lr: 0.093005, loss: 1.9755
2022-08-26 13:34:13 - train: epoch 0018, iter [00300, 05004], lr: 0.092989, loss: 2.0626
2022-08-26 13:34:46 - train: epoch 0018, iter [00400, 05004], lr: 0.092973, loss: 2.0999
2022-08-26 13:35:20 - train: epoch 0018, iter [00500, 05004], lr: 0.092957, loss: 1.9242
2022-08-26 13:35:54 - train: epoch 0018, iter [00600, 05004], lr: 0.092941, loss: 2.1073
2022-08-26 13:36:28 - train: epoch 0018, iter [00700, 05004], lr: 0.092925, loss: 1.7379
2022-08-26 13:37:01 - train: epoch 0018, iter [00800, 05004], lr: 0.092909, loss: 1.9918
2022-08-26 13:37:34 - train: epoch 0018, iter [00900, 05004], lr: 0.092893, loss: 2.1274
2022-08-26 13:38:08 - train: epoch 0018, iter [01000, 05004], lr: 0.092876, loss: 1.8229
2022-08-26 13:38:42 - train: epoch 0018, iter [01100, 05004], lr: 0.092860, loss: 2.2378
2022-08-26 13:39:16 - train: epoch 0018, iter [01200, 05004], lr: 0.092844, loss: 2.0078
2022-08-26 13:39:50 - train: epoch 0018, iter [01300, 05004], lr: 0.092828, loss: 2.4562
2022-08-26 13:40:24 - train: epoch 0018, iter [01400, 05004], lr: 0.092812, loss: 1.9491
2022-08-26 13:40:57 - train: epoch 0018, iter [01500, 05004], lr: 0.092796, loss: 2.2511
2022-08-26 13:41:31 - train: epoch 0018, iter [01600, 05004], lr: 0.092779, loss: 1.9734
2022-08-26 13:42:04 - train: epoch 0018, iter [01700, 05004], lr: 0.092763, loss: 1.8898
2022-08-26 13:42:38 - train: epoch 0018, iter [01800, 05004], lr: 0.092747, loss: 1.8187
2022-08-26 13:43:12 - train: epoch 0018, iter [01900, 05004], lr: 0.092730, loss: 2.0292
2022-08-26 13:43:46 - train: epoch 0018, iter [02000, 05004], lr: 0.092714, loss: 2.2140
2022-08-26 13:44:20 - train: epoch 0018, iter [02100, 05004], lr: 0.092698, loss: 2.1562
2022-08-26 13:44:54 - train: epoch 0018, iter [02200, 05004], lr: 0.092681, loss: 1.9023
2022-08-26 13:45:28 - train: epoch 0018, iter [02300, 05004], lr: 0.092665, loss: 2.0181
2022-08-26 13:46:02 - train: epoch 0018, iter [02400, 05004], lr: 0.092649, loss: 1.8817
2022-08-26 13:46:35 - train: epoch 0018, iter [02500, 05004], lr: 0.092632, loss: 1.8034
2022-08-26 13:47:08 - train: epoch 0018, iter [02600, 05004], lr: 0.092616, loss: 1.8732
2022-08-26 13:47:43 - train: epoch 0018, iter [02700, 05004], lr: 0.092600, loss: 2.1627
2022-08-26 13:48:17 - train: epoch 0018, iter [02800, 05004], lr: 0.092583, loss: 1.7632
2022-08-26 13:48:50 - train: epoch 0018, iter [02900, 05004], lr: 0.092567, loss: 2.0259
2022-08-26 13:49:24 - train: epoch 0018, iter [03000, 05004], lr: 0.092550, loss: 1.9367
2022-08-26 13:49:57 - train: epoch 0018, iter [03100, 05004], lr: 0.092534, loss: 2.2609
2022-08-26 13:50:31 - train: epoch 0018, iter [03200, 05004], lr: 0.092517, loss: 1.7634
2022-08-26 13:51:05 - train: epoch 0018, iter [03300, 05004], lr: 0.092501, loss: 1.8828
2022-08-26 13:51:38 - train: epoch 0018, iter [03400, 05004], lr: 0.092484, loss: 1.9861
2022-08-26 13:52:13 - train: epoch 0018, iter [03500, 05004], lr: 0.092467, loss: 2.2580
2022-08-26 13:52:47 - train: epoch 0018, iter [03600, 05004], lr: 0.092451, loss: 2.0671
2022-08-26 13:53:22 - train: epoch 0018, iter [03700, 05004], lr: 0.092434, loss: 2.3472
2022-08-26 13:53:55 - train: epoch 0018, iter [03800, 05004], lr: 0.092418, loss: 2.0951
2022-08-26 13:54:29 - train: epoch 0018, iter [03900, 05004], lr: 0.092401, loss: 2.0835
2022-08-26 13:55:03 - train: epoch 0018, iter [04000, 05004], lr: 0.092384, loss: 1.8526
2022-08-26 13:55:37 - train: epoch 0018, iter [04100, 05004], lr: 0.092368, loss: 2.0581
2022-08-26 13:56:11 - train: epoch 0018, iter [04200, 05004], lr: 0.092351, loss: 2.0898
2022-08-26 13:56:44 - train: epoch 0018, iter [04300, 05004], lr: 0.092334, loss: 1.6966
2022-08-26 13:57:18 - train: epoch 0018, iter [04400, 05004], lr: 0.092318, loss: 2.1729
2022-08-26 13:57:52 - train: epoch 0018, iter [04500, 05004], lr: 0.092301, loss: 1.9848
2022-08-26 13:58:26 - train: epoch 0018, iter [04600, 05004], lr: 0.092284, loss: 1.8732
2022-08-26 13:58:59 - train: epoch 0018, iter [04700, 05004], lr: 0.092267, loss: 2.2978
2022-08-26 13:59:33 - train: epoch 0018, iter [04800, 05004], lr: 0.092251, loss: 2.1308
2022-08-26 14:00:07 - train: epoch 0018, iter [04900, 05004], lr: 0.092234, loss: 2.0112
2022-08-26 14:00:39 - train: epoch 0018, iter [05000, 05004], lr: 0.092217, loss: 2.0863
2022-08-26 14:00:40 - train: epoch 018, train_loss: 2.0061
2022-08-26 14:01:56 - eval: epoch: 018, acc1: 57.470%, acc5: 81.408%, test_loss: 1.8050, per_image_load_time: 2.298ms, per_image_inference_time: 0.486ms
2022-08-26 14:01:56 - until epoch: 018, best_acc1: 57.470%
2022-08-26 14:01:56 - epoch 019 lr: 0.092216
2022-08-26 14:02:36 - train: epoch 0019, iter [00100, 05004], lr: 0.092200, loss: 1.7915
2022-08-26 14:03:09 - train: epoch 0019, iter [00200, 05004], lr: 0.092183, loss: 2.2903
2022-08-26 14:03:43 - train: epoch 0019, iter [00300, 05004], lr: 0.092166, loss: 2.2207
2022-08-26 14:04:17 - train: epoch 0019, iter [00400, 05004], lr: 0.092149, loss: 1.8583
2022-08-26 14:04:51 - train: epoch 0019, iter [00500, 05004], lr: 0.092132, loss: 1.9822
2022-08-26 14:05:25 - train: epoch 0019, iter [00600, 05004], lr: 0.092115, loss: 1.9161
2022-08-26 14:05:59 - train: epoch 0019, iter [00700, 05004], lr: 0.092098, loss: 1.8050
2022-08-26 14:06:33 - train: epoch 0019, iter [00800, 05004], lr: 0.092081, loss: 2.0686
2022-08-26 14:07:06 - train: epoch 0019, iter [00900, 05004], lr: 0.092064, loss: 1.9779
2022-08-26 14:07:40 - train: epoch 0019, iter [01000, 05004], lr: 0.092047, loss: 2.0831
2022-08-26 14:08:14 - train: epoch 0019, iter [01100, 05004], lr: 0.092030, loss: 1.9131
2022-08-26 14:08:48 - train: epoch 0019, iter [01200, 05004], lr: 0.092013, loss: 2.0375
2022-08-26 14:09:22 - train: epoch 0019, iter [01300, 05004], lr: 0.091996, loss: 2.1939
2022-08-26 14:09:55 - train: epoch 0019, iter [01400, 05004], lr: 0.091979, loss: 1.9239
2022-08-26 14:10:29 - train: epoch 0019, iter [01500, 05004], lr: 0.091962, loss: 2.2786
2022-08-26 14:11:03 - train: epoch 0019, iter [01600, 05004], lr: 0.091945, loss: 1.9352
2022-08-26 14:11:37 - train: epoch 0019, iter [01700, 05004], lr: 0.091928, loss: 2.0275
2022-08-26 14:12:10 - train: epoch 0019, iter [01800, 05004], lr: 0.091911, loss: 1.9346
2022-08-26 14:12:44 - train: epoch 0019, iter [01900, 05004], lr: 0.091894, loss: 2.1541
2022-08-26 14:13:19 - train: epoch 0019, iter [02000, 05004], lr: 0.091877, loss: 1.9744
2022-08-26 14:13:52 - train: epoch 0019, iter [02100, 05004], lr: 0.091860, loss: 1.8194
2022-08-26 14:14:26 - train: epoch 0019, iter [02200, 05004], lr: 0.091842, loss: 2.0370
2022-08-26 14:14:59 - train: epoch 0019, iter [02300, 05004], lr: 0.091825, loss: 2.0536
2022-08-26 14:15:34 - train: epoch 0019, iter [02400, 05004], lr: 0.091808, loss: 2.0744
2022-08-26 14:16:07 - train: epoch 0019, iter [02500, 05004], lr: 0.091791, loss: 1.9235
2022-08-26 14:16:42 - train: epoch 0019, iter [02600, 05004], lr: 0.091773, loss: 2.1020
2022-08-26 14:17:15 - train: epoch 0019, iter [02700, 05004], lr: 0.091756, loss: 2.0971
2022-08-26 14:17:49 - train: epoch 0019, iter [02800, 05004], lr: 0.091739, loss: 2.0375
2022-08-26 14:18:23 - train: epoch 0019, iter [02900, 05004], lr: 0.091722, loss: 1.9782
2022-08-26 14:18:57 - train: epoch 0019, iter [03000, 05004], lr: 0.091704, loss: 2.3210
2022-08-26 14:19:31 - train: epoch 0019, iter [03100, 05004], lr: 0.091687, loss: 2.0830
2022-08-26 14:20:04 - train: epoch 0019, iter [03200, 05004], lr: 0.091670, loss: 1.6609
2022-08-26 14:20:38 - train: epoch 0019, iter [03300, 05004], lr: 0.091652, loss: 1.9476
2022-08-26 14:21:12 - train: epoch 0019, iter [03400, 05004], lr: 0.091635, loss: 2.0955
2022-08-26 14:21:46 - train: epoch 0019, iter [03500, 05004], lr: 0.091618, loss: 1.9648
2022-08-26 14:22:19 - train: epoch 0019, iter [03600, 05004], lr: 0.091600, loss: 1.7776
2022-08-26 14:22:53 - train: epoch 0019, iter [03700, 05004], lr: 0.091583, loss: 2.0615
2022-08-26 14:23:27 - train: epoch 0019, iter [03800, 05004], lr: 0.091565, loss: 2.0751
2022-08-26 14:24:00 - train: epoch 0019, iter [03900, 05004], lr: 0.091548, loss: 2.0317
2022-08-26 14:24:34 - train: epoch 0019, iter [04000, 05004], lr: 0.091530, loss: 1.8566
2022-08-26 14:25:08 - train: epoch 0019, iter [04100, 05004], lr: 0.091513, loss: 2.0966
2022-08-26 14:25:42 - train: epoch 0019, iter [04200, 05004], lr: 0.091495, loss: 1.9296
2022-08-26 14:26:16 - train: epoch 0019, iter [04300, 05004], lr: 0.091478, loss: 1.8054
2022-08-26 14:26:50 - train: epoch 0019, iter [04400, 05004], lr: 0.091460, loss: 2.1043
2022-08-26 14:27:24 - train: epoch 0019, iter [04500, 05004], lr: 0.091443, loss: 2.2909
2022-08-26 14:27:58 - train: epoch 0019, iter [04600, 05004], lr: 0.091425, loss: 1.9526
2022-08-26 14:28:31 - train: epoch 0019, iter [04700, 05004], lr: 0.091408, loss: 1.8352
2022-08-26 14:29:05 - train: epoch 0019, iter [04800, 05004], lr: 0.091390, loss: 2.1198
2022-08-26 14:29:38 - train: epoch 0019, iter [04900, 05004], lr: 0.091372, loss: 1.9948
2022-08-26 14:30:11 - train: epoch 0019, iter [05000, 05004], lr: 0.091355, loss: 2.0360
2022-08-26 14:30:13 - train: epoch 019, train_loss: 1.9968
2022-08-26 14:31:28 - eval: epoch: 019, acc1: 58.194%, acc5: 82.264%, test_loss: 1.7540, per_image_load_time: 1.667ms, per_image_inference_time: 0.481ms
2022-08-26 14:31:29 - until epoch: 019, best_acc1: 58.194%
2022-08-26 14:31:29 - epoch 020 lr: 0.091354
2022-08-26 14:32:08 - train: epoch 0020, iter [00100, 05004], lr: 0.091336, loss: 2.0516
2022-08-26 14:32:42 - train: epoch 0020, iter [00200, 05004], lr: 0.091319, loss: 1.8599
2022-08-26 14:33:17 - train: epoch 0020, iter [00300, 05004], lr: 0.091301, loss: 1.9249
2022-08-26 14:33:50 - train: epoch 0020, iter [00400, 05004], lr: 0.091283, loss: 1.7392
2022-08-26 14:34:24 - train: epoch 0020, iter [00500, 05004], lr: 0.091266, loss: 1.8908
2022-08-26 14:34:57 - train: epoch 0020, iter [00600, 05004], lr: 0.091248, loss: 2.1152
2022-08-26 14:35:31 - train: epoch 0020, iter [00700, 05004], lr: 0.091230, loss: 1.7679
2022-08-26 14:36:05 - train: epoch 0020, iter [00800, 05004], lr: 0.091212, loss: 2.0649
2022-08-26 14:36:39 - train: epoch 0020, iter [00900, 05004], lr: 0.091195, loss: 2.2996
2022-08-26 14:37:12 - train: epoch 0020, iter [01000, 05004], lr: 0.091177, loss: 2.0454
2022-08-26 14:37:46 - train: epoch 0020, iter [01100, 05004], lr: 0.091159, loss: 1.9547
2022-08-26 14:38:20 - train: epoch 0020, iter [01200, 05004], lr: 0.091141, loss: 1.7558
2022-08-26 14:38:54 - train: epoch 0020, iter [01300, 05004], lr: 0.091123, loss: 1.8421
2022-08-26 14:39:28 - train: epoch 0020, iter [01400, 05004], lr: 0.091105, loss: 2.2788
2022-08-26 14:40:02 - train: epoch 0020, iter [01500, 05004], lr: 0.091088, loss: 2.0609
2022-08-26 14:40:36 - train: epoch 0020, iter [01600, 05004], lr: 0.091070, loss: 1.8731
2022-08-26 14:41:10 - train: epoch 0020, iter [01700, 05004], lr: 0.091052, loss: 1.7299
2022-08-26 14:41:43 - train: epoch 0020, iter [01800, 05004], lr: 0.091034, loss: 1.9669
2022-08-26 14:42:17 - train: epoch 0020, iter [01900, 05004], lr: 0.091016, loss: 1.8175
2022-08-26 14:42:51 - train: epoch 0020, iter [02000, 05004], lr: 0.090998, loss: 1.9146
2022-08-26 14:43:25 - train: epoch 0020, iter [02100, 05004], lr: 0.090980, loss: 2.1670
2022-08-26 14:43:59 - train: epoch 0020, iter [02200, 05004], lr: 0.090962, loss: 1.8074
2022-08-26 14:44:33 - train: epoch 0020, iter [02300, 05004], lr: 0.090944, loss: 1.9582
2022-08-26 14:45:07 - train: epoch 0020, iter [02400, 05004], lr: 0.090926, loss: 2.1021
2022-08-26 14:45:40 - train: epoch 0020, iter [02500, 05004], lr: 0.090908, loss: 1.8388
2022-08-26 14:46:14 - train: epoch 0020, iter [02600, 05004], lr: 0.090890, loss: 1.7867
2022-08-26 14:46:48 - train: epoch 0020, iter [02700, 05004], lr: 0.090872, loss: 2.0507
2022-08-26 14:47:22 - train: epoch 0020, iter [02800, 05004], lr: 0.090854, loss: 2.0180
2022-08-26 14:47:56 - train: epoch 0020, iter [02900, 05004], lr: 0.090836, loss: 2.0914
2022-08-26 14:48:29 - train: epoch 0020, iter [03000, 05004], lr: 0.090817, loss: 1.9307
2022-08-26 14:49:04 - train: epoch 0020, iter [03100, 05004], lr: 0.090799, loss: 2.1043
2022-08-26 14:49:37 - train: epoch 0020, iter [03200, 05004], lr: 0.090781, loss: 2.1725
2022-08-26 14:50:12 - train: epoch 0020, iter [03300, 05004], lr: 0.090763, loss: 1.8563
2022-08-26 14:50:45 - train: epoch 0020, iter [03400, 05004], lr: 0.090745, loss: 2.0767
2022-08-26 14:51:19 - train: epoch 0020, iter [03500, 05004], lr: 0.090727, loss: 1.9080
2022-08-26 14:51:53 - train: epoch 0020, iter [03600, 05004], lr: 0.090708, loss: 2.0747
2022-08-26 14:52:27 - train: epoch 0020, iter [03700, 05004], lr: 0.090690, loss: 1.8867
2022-08-26 14:53:01 - train: epoch 0020, iter [03800, 05004], lr: 0.090672, loss: 1.9651
2022-08-26 14:53:35 - train: epoch 0020, iter [03900, 05004], lr: 0.090654, loss: 2.1083
2022-08-26 14:54:09 - train: epoch 0020, iter [04000, 05004], lr: 0.090635, loss: 1.8288
2022-08-26 14:54:43 - train: epoch 0020, iter [04100, 05004], lr: 0.090617, loss: 1.8731
2022-08-26 14:55:17 - train: epoch 0020, iter [04200, 05004], lr: 0.090599, loss: 1.9295
2022-08-26 14:55:50 - train: epoch 0020, iter [04300, 05004], lr: 0.090580, loss: 2.0731
2022-08-26 14:56:24 - train: epoch 0020, iter [04400, 05004], lr: 0.090562, loss: 2.1222
2022-08-26 14:56:59 - train: epoch 0020, iter [04500, 05004], lr: 0.090544, loss: 2.0390
2022-08-26 14:57:32 - train: epoch 0020, iter [04600, 05004], lr: 0.090525, loss: 2.0671
2022-08-26 14:58:07 - train: epoch 0020, iter [04700, 05004], lr: 0.090507, loss: 1.7980
2022-08-26 14:58:41 - train: epoch 0020, iter [04800, 05004], lr: 0.090488, loss: 1.9855
2022-08-26 14:59:14 - train: epoch 0020, iter [04900, 05004], lr: 0.090470, loss: 2.0305
2022-08-26 14:59:47 - train: epoch 0020, iter [05000, 05004], lr: 0.090452, loss: 1.8346
2022-08-26 14:59:48 - train: epoch 020, train_loss: 1.9756
2022-08-26 15:01:04 - eval: epoch: 020, acc1: 57.144%, acc5: 81.402%, test_loss: 1.8243, per_image_load_time: 1.724ms, per_image_inference_time: 0.501ms
2022-08-26 15:01:04 - until epoch: 020, best_acc1: 58.194%
2022-08-26 15:01:04 - epoch 021 lr: 0.090451
2022-08-26 15:01:43 - train: epoch 0021, iter [00100, 05004], lr: 0.090432, loss: 1.8067
2022-08-26 15:02:17 - train: epoch 0021, iter [00200, 05004], lr: 0.090414, loss: 2.0328
2022-08-26 15:02:51 - train: epoch 0021, iter [00300, 05004], lr: 0.090395, loss: 1.6889
2022-08-26 15:03:25 - train: epoch 0021, iter [00400, 05004], lr: 0.090377, loss: 2.1058
2022-08-26 15:03:59 - train: epoch 0021, iter [00500, 05004], lr: 0.090358, loss: 1.8733
2022-08-26 15:04:32 - train: epoch 0021, iter [00600, 05004], lr: 0.090340, loss: 1.8355
2022-08-26 15:05:06 - train: epoch 0021, iter [00700, 05004], lr: 0.090321, loss: 1.8856
2022-08-26 15:05:40 - train: epoch 0021, iter [00800, 05004], lr: 0.090303, loss: 2.0765
2022-08-26 15:06:14 - train: epoch 0021, iter [00900, 05004], lr: 0.090284, loss: 1.8772
2022-08-26 15:06:47 - train: epoch 0021, iter [01000, 05004], lr: 0.090266, loss: 1.7949
2022-08-26 15:07:22 - train: epoch 0021, iter [01100, 05004], lr: 0.090247, loss: 1.7855
2022-08-26 15:07:54 - train: epoch 0021, iter [01200, 05004], lr: 0.090228, loss: 1.8847
2022-08-26 15:08:28 - train: epoch 0021, iter [01300, 05004], lr: 0.090210, loss: 1.8481
2022-08-26 15:09:02 - train: epoch 0021, iter [01400, 05004], lr: 0.090191, loss: 1.8422
2022-08-26 15:09:36 - train: epoch 0021, iter [01500, 05004], lr: 0.090172, loss: 1.7813
2022-08-26 15:10:09 - train: epoch 0021, iter [01600, 05004], lr: 0.090154, loss: 1.9957
2022-08-26 15:10:44 - train: epoch 0021, iter [01700, 05004], lr: 0.090135, loss: 1.8704
2022-08-26 15:11:17 - train: epoch 0021, iter [01800, 05004], lr: 0.090116, loss: 1.8026
2022-08-26 15:11:51 - train: epoch 0021, iter [01900, 05004], lr: 0.090097, loss: 2.0783
2022-08-26 15:12:24 - train: epoch 0021, iter [02000, 05004], lr: 0.090079, loss: 2.2076
2022-08-26 15:12:58 - train: epoch 0021, iter [02100, 05004], lr: 0.090060, loss: 1.7506
2022-08-26 15:13:32 - train: epoch 0021, iter [02200, 05004], lr: 0.090041, loss: 1.9664
2022-08-26 15:14:05 - train: epoch 0021, iter [02300, 05004], lr: 0.090022, loss: 1.8084
2022-08-26 15:14:39 - train: epoch 0021, iter [02400, 05004], lr: 0.090003, loss: 1.7771
2022-08-26 15:15:13 - train: epoch 0021, iter [02500, 05004], lr: 0.089985, loss: 1.9355
2022-08-26 15:15:47 - train: epoch 0021, iter [02600, 05004], lr: 0.089966, loss: 2.1638
2022-08-26 15:16:22 - train: epoch 0021, iter [02700, 05004], lr: 0.089947, loss: 1.8570
2022-08-26 15:16:55 - train: epoch 0021, iter [02800, 05004], lr: 0.089928, loss: 1.9612
2022-08-26 15:17:28 - train: epoch 0021, iter [02900, 05004], lr: 0.089909, loss: 1.9477
2022-08-26 15:18:02 - train: epoch 0021, iter [03000, 05004], lr: 0.089890, loss: 2.2118
2022-08-26 15:18:36 - train: epoch 0021, iter [03100, 05004], lr: 0.089871, loss: 2.0792
2022-08-26 15:19:10 - train: epoch 0021, iter [03200, 05004], lr: 0.089852, loss: 1.9696
2022-08-26 15:19:43 - train: epoch 0021, iter [03300, 05004], lr: 0.089833, loss: 2.1857
2022-08-26 15:20:17 - train: epoch 0021, iter [03400, 05004], lr: 0.089814, loss: 2.0203
2022-08-26 15:20:51 - train: epoch 0021, iter [03500, 05004], lr: 0.089795, loss: 1.8462
2022-08-26 15:21:26 - train: epoch 0021, iter [03600, 05004], lr: 0.089776, loss: 1.9007
2022-08-26 15:21:59 - train: epoch 0021, iter [03700, 05004], lr: 0.089757, loss: 1.9845
2022-08-26 15:22:34 - train: epoch 0021, iter [03800, 05004], lr: 0.089738, loss: 1.9235
2022-08-26 15:23:07 - train: epoch 0021, iter [03900, 05004], lr: 0.089719, loss: 1.8078
2022-08-26 15:23:42 - train: epoch 0021, iter [04000, 05004], lr: 0.089700, loss: 2.1832
2022-08-26 15:24:15 - train: epoch 0021, iter [04100, 05004], lr: 0.089681, loss: 1.9683
2022-08-26 15:24:49 - train: epoch 0021, iter [04200, 05004], lr: 0.089662, loss: 1.8639
2022-08-26 15:25:23 - train: epoch 0021, iter [04300, 05004], lr: 0.089643, loss: 1.9114
2022-08-26 15:25:57 - train: epoch 0021, iter [04400, 05004], lr: 0.089624, loss: 2.0997
2022-08-26 15:26:30 - train: epoch 0021, iter [04500, 05004], lr: 0.089605, loss: 2.0873
2022-08-26 15:27:04 - train: epoch 0021, iter [04600, 05004], lr: 0.089585, loss: 1.8550
2022-08-26 15:27:37 - train: epoch 0021, iter [04700, 05004], lr: 0.089566, loss: 2.0924
2022-08-26 15:28:12 - train: epoch 0021, iter [04800, 05004], lr: 0.089547, loss: 2.1009
2022-08-26 15:28:46 - train: epoch 0021, iter [04900, 05004], lr: 0.089528, loss: 1.7690
2022-08-26 15:29:19 - train: epoch 0021, iter [05000, 05004], lr: 0.089509, loss: 1.9587
2022-08-26 15:29:20 - train: epoch 021, train_loss: 1.9672
2022-08-26 15:30:36 - eval: epoch: 021, acc1: 51.948%, acc5: 75.790%, test_loss: 2.4627, per_image_load_time: 2.353ms, per_image_inference_time: 0.484ms
2022-08-26 15:30:36 - until epoch: 021, best_acc1: 58.194%
2022-08-26 15:30:36 - epoch 022 lr: 0.089508
2022-08-26 15:31:15 - train: epoch 0022, iter [00100, 05004], lr: 0.089489, loss: 1.7241
2022-08-26 15:31:49 - train: epoch 0022, iter [00200, 05004], lr: 0.089469, loss: 1.7724
2022-08-26 15:32:23 - train: epoch 0022, iter [00300, 05004], lr: 0.089450, loss: 1.7644
2022-08-26 15:32:57 - train: epoch 0022, iter [00400, 05004], lr: 0.089431, loss: 1.7916
2022-08-26 15:33:32 - train: epoch 0022, iter [00500, 05004], lr: 0.089411, loss: 1.8978
2022-08-26 15:34:06 - train: epoch 0022, iter [00600, 05004], lr: 0.089392, loss: 1.9752
2022-08-26 15:34:39 - train: epoch 0022, iter [00700, 05004], lr: 0.089373, loss: 2.0003
2022-08-26 15:35:13 - train: epoch 0022, iter [00800, 05004], lr: 0.089353, loss: 2.0538
2022-08-26 15:35:47 - train: epoch 0022, iter [00900, 05004], lr: 0.089334, loss: 2.1362
2022-08-26 15:36:20 - train: epoch 0022, iter [01000, 05004], lr: 0.089315, loss: 1.9934
2022-08-26 15:36:54 - train: epoch 0022, iter [01100, 05004], lr: 0.089295, loss: 2.0084
2022-08-26 15:37:28 - train: epoch 0022, iter [01200, 05004], lr: 0.089276, loss: 1.5970
2022-08-26 15:38:02 - train: epoch 0022, iter [01300, 05004], lr: 0.089256, loss: 1.9178
2022-08-26 15:38:36 - train: epoch 0022, iter [01400, 05004], lr: 0.089237, loss: 1.9219
2022-08-26 15:39:10 - train: epoch 0022, iter [01500, 05004], lr: 0.089217, loss: 1.9049
2022-08-26 15:39:43 - train: epoch 0022, iter [01600, 05004], lr: 0.089198, loss: 1.7753
2022-08-26 15:40:17 - train: epoch 0022, iter [01700, 05004], lr: 0.089178, loss: 1.7747
2022-08-26 15:40:52 - train: epoch 0022, iter [01800, 05004], lr: 0.089159, loss: 2.0608
2022-08-26 15:41:26 - train: epoch 0022, iter [01900, 05004], lr: 0.089139, loss: 1.7898
2022-08-26 15:42:01 - train: epoch 0022, iter [02000, 05004], lr: 0.089120, loss: 1.9915
2022-08-26 15:42:33 - train: epoch 0022, iter [02100, 05004], lr: 0.089100, loss: 2.0375
2022-08-26 15:43:08 - train: epoch 0022, iter [02200, 05004], lr: 0.089081, loss: 1.7716
2022-08-26 15:43:41 - train: epoch 0022, iter [02300, 05004], lr: 0.089061, loss: 2.0611
2022-08-26 15:44:15 - train: epoch 0022, iter [02400, 05004], lr: 0.089042, loss: 1.9430
2022-08-26 15:44:49 - train: epoch 0022, iter [02500, 05004], lr: 0.089022, loss: 1.9678
2022-08-26 15:45:23 - train: epoch 0022, iter [02600, 05004], lr: 0.089002, loss: 1.7501
2022-08-26 15:45:57 - train: epoch 0022, iter [02700, 05004], lr: 0.088983, loss: 1.6657
2022-08-26 15:46:30 - train: epoch 0022, iter [02800, 05004], lr: 0.088963, loss: 2.1695
2022-08-26 15:47:05 - train: epoch 0022, iter [02900, 05004], lr: 0.088943, loss: 1.8571
2022-08-26 15:47:39 - train: epoch 0022, iter [03000, 05004], lr: 0.088924, loss: 2.0650
2022-08-26 15:48:13 - train: epoch 0022, iter [03100, 05004], lr: 0.088904, loss: 2.1143
2022-08-26 15:48:47 - train: epoch 0022, iter [03200, 05004], lr: 0.088884, loss: 2.1209
2022-08-26 15:49:21 - train: epoch 0022, iter [03300, 05004], lr: 0.088864, loss: 1.9612
2022-08-26 15:49:54 - train: epoch 0022, iter [03400, 05004], lr: 0.088845, loss: 1.7707
2022-08-26 15:50:28 - train: epoch 0022, iter [03500, 05004], lr: 0.088825, loss: 1.9609
2022-08-26 15:51:03 - train: epoch 0022, iter [03600, 05004], lr: 0.088805, loss: 2.0239
2022-08-26 15:51:36 - train: epoch 0022, iter [03700, 05004], lr: 0.088785, loss: 2.0819
2022-08-26 15:52:11 - train: epoch 0022, iter [03800, 05004], lr: 0.088765, loss: 2.0745
2022-08-26 15:52:44 - train: epoch 0022, iter [03900, 05004], lr: 0.088746, loss: 1.9281
2022-08-26 15:53:19 - train: epoch 0022, iter [04000, 05004], lr: 0.088726, loss: 1.9857
2022-08-26 15:53:52 - train: epoch 0022, iter [04100, 05004], lr: 0.088706, loss: 1.8958
2022-08-26 15:54:27 - train: epoch 0022, iter [04200, 05004], lr: 0.088686, loss: 1.8987
2022-08-26 15:55:01 - train: epoch 0022, iter [04300, 05004], lr: 0.088666, loss: 2.2382
2022-08-26 15:55:34 - train: epoch 0022, iter [04400, 05004], lr: 0.088646, loss: 1.9495
2022-08-26 15:56:09 - train: epoch 0022, iter [04500, 05004], lr: 0.088626, loss: 1.9769
2022-08-26 15:56:43 - train: epoch 0022, iter [04600, 05004], lr: 0.088606, loss: 2.2539
2022-08-26 15:57:16 - train: epoch 0022, iter [04700, 05004], lr: 0.088586, loss: 2.0930
2022-08-26 15:57:50 - train: epoch 0022, iter [04800, 05004], lr: 0.088566, loss: 1.7224
2022-08-26 15:58:24 - train: epoch 0022, iter [04900, 05004], lr: 0.088546, loss: 1.7809
2022-08-26 15:58:57 - train: epoch 0022, iter [05000, 05004], lr: 0.088526, loss: 1.9467
2022-08-26 15:58:58 - train: epoch 022, train_loss: 1.9559
2022-08-26 16:00:14 - eval: epoch: 022, acc1: 55.506%, acc5: 80.176%, test_loss: 1.8921, per_image_load_time: 1.579ms, per_image_inference_time: 0.478ms
2022-08-26 16:00:14 - until epoch: 022, best_acc1: 58.194%
2022-08-26 16:00:14 - epoch 023 lr: 0.088525
2022-08-26 16:00:53 - train: epoch 0023, iter [00100, 05004], lr: 0.088506, loss: 1.8343
2022-08-26 16:01:27 - train: epoch 0023, iter [00200, 05004], lr: 0.088486, loss: 1.6508
2022-08-26 16:02:01 - train: epoch 0023, iter [00300, 05004], lr: 0.088466, loss: 1.6888
2022-08-26 16:02:35 - train: epoch 0023, iter [00400, 05004], lr: 0.088446, loss: 1.9415
2022-08-26 16:03:09 - train: epoch 0023, iter [00500, 05004], lr: 0.088425, loss: 1.8942
2022-08-26 16:03:43 - train: epoch 0023, iter [00600, 05004], lr: 0.088405, loss: 1.8623
2022-08-26 16:04:16 - train: epoch 0023, iter [00700, 05004], lr: 0.088385, loss: 1.6949
2022-08-26 16:04:50 - train: epoch 0023, iter [00800, 05004], lr: 0.088365, loss: 1.7498
2022-08-26 16:05:23 - train: epoch 0023, iter [00900, 05004], lr: 0.088345, loss: 1.9518
2022-08-26 16:05:56 - train: epoch 0023, iter [01000, 05004], lr: 0.088325, loss: 1.8675
2022-08-26 16:06:30 - train: epoch 0023, iter [01100, 05004], lr: 0.088305, loss: 2.0382
2022-08-26 16:07:04 - train: epoch 0023, iter [01200, 05004], lr: 0.088284, loss: 1.7209
2022-08-26 16:07:37 - train: epoch 0023, iter [01300, 05004], lr: 0.088264, loss: 1.8791
2022-08-26 16:08:11 - train: epoch 0023, iter [01400, 05004], lr: 0.088244, loss: 2.0016
2022-08-26 16:08:43 - train: epoch 0023, iter [01500, 05004], lr: 0.088224, loss: 1.7335
2022-08-26 16:09:17 - train: epoch 0023, iter [01600, 05004], lr: 0.088204, loss: 1.9837
2022-08-26 16:09:50 - train: epoch 0023, iter [01700, 05004], lr: 0.088183, loss: 2.1338
2022-08-26 16:10:22 - train: epoch 0023, iter [01800, 05004], lr: 0.088163, loss: 1.7784
2022-08-26 16:10:56 - train: epoch 0023, iter [01900, 05004], lr: 0.088143, loss: 1.9426
2022-08-26 16:11:29 - train: epoch 0023, iter [02000, 05004], lr: 0.088122, loss: 1.6771
2022-08-26 16:12:02 - train: epoch 0023, iter [02100, 05004], lr: 0.088102, loss: 2.0655
2022-08-26 16:12:35 - train: epoch 0023, iter [02200, 05004], lr: 0.088082, loss: 1.6813
2022-08-26 16:13:08 - train: epoch 0023, iter [02300, 05004], lr: 0.088061, loss: 1.8302
2022-08-26 16:13:41 - train: epoch 0023, iter [02400, 05004], lr: 0.088041, loss: 1.9331
2022-08-26 16:14:15 - train: epoch 0023, iter [02500, 05004], lr: 0.088021, loss: 1.9549
2022-08-26 16:14:49 - train: epoch 0023, iter [02600, 05004], lr: 0.088000, loss: 2.0217
2022-08-26 16:15:22 - train: epoch 0023, iter [02700, 05004], lr: 0.087980, loss: 1.9552
2022-08-26 16:15:56 - train: epoch 0023, iter [02800, 05004], lr: 0.087959, loss: 1.9438
2022-08-26 16:16:29 - train: epoch 0023, iter [02900, 05004], lr: 0.087939, loss: 1.9877
2022-08-26 16:17:03 - train: epoch 0023, iter [03000, 05004], lr: 0.087919, loss: 2.1693
2022-08-26 16:17:36 - train: epoch 0023, iter [03100, 05004], lr: 0.087898, loss: 2.0707
2022-08-26 16:18:10 - train: epoch 0023, iter [03200, 05004], lr: 0.087878, loss: 2.1598
2022-08-26 16:18:44 - train: epoch 0023, iter [03300, 05004], lr: 0.087857, loss: 1.9649
2022-08-26 16:19:18 - train: epoch 0023, iter [03400, 05004], lr: 0.087837, loss: 2.0215
2022-08-26 16:19:52 - train: epoch 0023, iter [03500, 05004], lr: 0.087816, loss: 1.9063
2022-08-26 16:20:25 - train: epoch 0023, iter [03600, 05004], lr: 0.087796, loss: 1.7763
2022-08-26 16:21:00 - train: epoch 0023, iter [03700, 05004], lr: 0.087775, loss: 1.9427
2022-08-26 16:21:34 - train: epoch 0023, iter [03800, 05004], lr: 0.087754, loss: 1.9625
2022-08-26 16:22:07 - train: epoch 0023, iter [03900, 05004], lr: 0.087734, loss: 1.8429
2022-08-26 16:22:41 - train: epoch 0023, iter [04000, 05004], lr: 0.087713, loss: 1.8162
2022-08-26 16:23:15 - train: epoch 0023, iter [04100, 05004], lr: 0.087693, loss: 1.9020
2022-08-26 16:23:49 - train: epoch 0023, iter [04200, 05004], lr: 0.087672, loss: 1.7991
2022-08-26 16:24:23 - train: epoch 0023, iter [04300, 05004], lr: 0.087651, loss: 1.7444
2022-08-26 16:24:57 - train: epoch 0023, iter [04400, 05004], lr: 0.087631, loss: 1.7655
2022-08-26 16:25:31 - train: epoch 0023, iter [04500, 05004], lr: 0.087610, loss: 1.8142
2022-08-26 16:26:05 - train: epoch 0023, iter [04600, 05004], lr: 0.087589, loss: 2.0465
2022-08-26 16:26:39 - train: epoch 0023, iter [04700, 05004], lr: 0.087569, loss: 1.7342
2022-08-26 16:27:13 - train: epoch 0023, iter [04800, 05004], lr: 0.087548, loss: 1.8989
2022-08-26 16:27:46 - train: epoch 0023, iter [04900, 05004], lr: 0.087527, loss: 1.8970
2022-08-26 16:28:19 - train: epoch 0023, iter [05000, 05004], lr: 0.087506, loss: 2.0474
2022-08-26 16:28:20 - train: epoch 023, train_loss: 1.9418
2022-08-26 16:29:34 - eval: epoch: 023, acc1: 56.556%, acc5: 81.080%, test_loss: 1.8356, per_image_load_time: 1.497ms, per_image_inference_time: 0.512ms
2022-08-26 16:29:35 - until epoch: 023, best_acc1: 58.194%
2022-08-26 16:29:35 - epoch 024 lr: 0.087505
2022-08-26 16:30:13 - train: epoch 0024, iter [00100, 05004], lr: 0.087485, loss: 1.9321
2022-08-26 16:30:47 - train: epoch 0024, iter [00200, 05004], lr: 0.087464, loss: 1.9832
2022-08-26 16:31:20 - train: epoch 0024, iter [00300, 05004], lr: 0.087443, loss: 1.8891
2022-08-26 16:31:53 - train: epoch 0024, iter [00400, 05004], lr: 0.087422, loss: 1.9453
2022-08-26 16:32:26 - train: epoch 0024, iter [00500, 05004], lr: 0.087402, loss: 1.7846
2022-08-26 16:32:59 - train: epoch 0024, iter [00600, 05004], lr: 0.087381, loss: 1.7111
2022-08-26 16:33:33 - train: epoch 0024, iter [00700, 05004], lr: 0.087360, loss: 1.8851
2022-08-26 16:34:07 - train: epoch 0024, iter [00800, 05004], lr: 0.087339, loss: 1.7764
2022-08-26 16:34:40 - train: epoch 0024, iter [00900, 05004], lr: 0.087318, loss: 1.9562
2022-08-26 16:35:14 - train: epoch 0024, iter [01000, 05004], lr: 0.087297, loss: 1.8152
2022-08-26 16:35:47 - train: epoch 0024, iter [01100, 05004], lr: 0.087276, loss: 1.5859
2022-08-26 16:36:20 - train: epoch 0024, iter [01200, 05004], lr: 0.087255, loss: 1.8081
2022-08-26 16:36:54 - train: epoch 0024, iter [01300, 05004], lr: 0.087234, loss: 2.1976
2022-08-26 16:37:28 - train: epoch 0024, iter [01400, 05004], lr: 0.087213, loss: 1.8326
2022-08-26 16:38:02 - train: epoch 0024, iter [01500, 05004], lr: 0.087193, loss: 2.1142
2022-08-26 16:38:36 - train: epoch 0024, iter [01600, 05004], lr: 0.087172, loss: 1.9950
2022-08-26 16:39:09 - train: epoch 0024, iter [01700, 05004], lr: 0.087151, loss: 1.9886
2022-08-26 16:39:42 - train: epoch 0024, iter [01800, 05004], lr: 0.087130, loss: 2.2307
2022-08-26 16:40:16 - train: epoch 0024, iter [01900, 05004], lr: 0.087108, loss: 1.7206
2022-08-26 16:40:50 - train: epoch 0024, iter [02000, 05004], lr: 0.087087, loss: 2.0452
2022-08-26 16:41:24 - train: epoch 0024, iter [02100, 05004], lr: 0.087066, loss: 1.9748
2022-08-26 16:41:58 - train: epoch 0024, iter [02200, 05004], lr: 0.087045, loss: 1.8213
2022-08-26 16:42:31 - train: epoch 0024, iter [02300, 05004], lr: 0.087024, loss: 1.9057
2022-08-26 16:43:05 - train: epoch 0024, iter [02400, 05004], lr: 0.087003, loss: 1.8157
2022-08-26 16:43:38 - train: epoch 0024, iter [02500, 05004], lr: 0.086982, loss: 1.8566
2022-08-26 16:44:12 - train: epoch 0024, iter [02600, 05004], lr: 0.086961, loss: 1.8611
2022-08-26 16:44:45 - train: epoch 0024, iter [02700, 05004], lr: 0.086940, loss: 2.0637
2022-08-26 16:45:20 - train: epoch 0024, iter [02800, 05004], lr: 0.086919, loss: 1.9738
2022-08-26 16:45:53 - train: epoch 0024, iter [02900, 05004], lr: 0.086897, loss: 1.9447
2022-08-26 16:46:26 - train: epoch 0024, iter [03000, 05004], lr: 0.086876, loss: 1.9319
2022-08-26 16:47:01 - train: epoch 0024, iter [03100, 05004], lr: 0.086855, loss: 1.8606
2022-08-26 16:47:35 - train: epoch 0024, iter [03200, 05004], lr: 0.086834, loss: 2.0581
2022-08-26 16:48:09 - train: epoch 0024, iter [03300, 05004], lr: 0.086813, loss: 1.6355
2022-08-26 16:48:42 - train: epoch 0024, iter [03400, 05004], lr: 0.086791, loss: 1.8962
2022-08-26 16:49:16 - train: epoch 0024, iter [03500, 05004], lr: 0.086770, loss: 1.8700
2022-08-26 16:49:49 - train: epoch 0024, iter [03600, 05004], lr: 0.086749, loss: 2.0989
2022-08-26 16:50:23 - train: epoch 0024, iter [03700, 05004], lr: 0.086727, loss: 1.8015
2022-08-26 16:50:56 - train: epoch 0024, iter [03800, 05004], lr: 0.086706, loss: 2.1433
2022-08-26 16:51:29 - train: epoch 0024, iter [03900, 05004], lr: 0.086685, loss: 1.7940
2022-08-26 16:52:02 - train: epoch 0024, iter [04000, 05004], lr: 0.086663, loss: 2.0269
2022-08-26 16:52:36 - train: epoch 0024, iter [04100, 05004], lr: 0.086642, loss: 1.9371
2022-08-26 16:53:09 - train: epoch 0024, iter [04200, 05004], lr: 0.086621, loss: 1.9530
2022-08-26 16:53:43 - train: epoch 0024, iter [04300, 05004], lr: 0.086599, loss: 1.9537
2022-08-26 16:54:16 - train: epoch 0024, iter [04400, 05004], lr: 0.086578, loss: 1.9900
2022-08-26 16:54:50 - train: epoch 0024, iter [04500, 05004], lr: 0.086557, loss: 1.8222
2022-08-26 16:55:22 - train: epoch 0024, iter [04600, 05004], lr: 0.086535, loss: 2.0032
2022-08-26 16:55:56 - train: epoch 0024, iter [04700, 05004], lr: 0.086514, loss: 1.9282
2022-08-26 16:56:29 - train: epoch 0024, iter [04800, 05004], lr: 0.086492, loss: 1.8480
2022-08-26 16:57:03 - train: epoch 0024, iter [04900, 05004], lr: 0.086471, loss: 1.9002
2022-08-26 16:57:35 - train: epoch 0024, iter [05000, 05004], lr: 0.086449, loss: 1.9730
2022-08-26 16:57:36 - train: epoch 024, train_loss: 1.9323
2022-08-26 16:58:50 - eval: epoch: 024, acc1: 56.788%, acc5: 81.148%, test_loss: 1.8264, per_image_load_time: 2.296ms, per_image_inference_time: 0.457ms
2022-08-26 16:58:51 - until epoch: 024, best_acc1: 58.194%
2022-08-26 16:58:51 - epoch 025 lr: 0.086448
2022-08-26 16:59:29 - train: epoch 0025, iter [00100, 05004], lr: 0.086427, loss: 1.7615
2022-08-26 17:00:02 - train: epoch 0025, iter [00200, 05004], lr: 0.086405, loss: 1.7459
2022-08-26 17:00:36 - train: epoch 0025, iter [00300, 05004], lr: 0.086384, loss: 1.8433
2022-08-26 17:01:09 - train: epoch 0025, iter [00400, 05004], lr: 0.086362, loss: 1.8351
2022-08-26 17:01:43 - train: epoch 0025, iter [00500, 05004], lr: 0.086341, loss: 1.7334
2022-08-26 17:02:16 - train: epoch 0025, iter [00600, 05004], lr: 0.086319, loss: 1.9239
2022-08-26 17:02:49 - train: epoch 0025, iter [00700, 05004], lr: 0.086298, loss: 1.9914
2022-08-26 17:03:22 - train: epoch 0025, iter [00800, 05004], lr: 0.086276, loss: 1.8754
2022-08-26 17:03:56 - train: epoch 0025, iter [00900, 05004], lr: 0.086254, loss: 1.8241
2022-08-26 17:04:30 - train: epoch 0025, iter [01000, 05004], lr: 0.086233, loss: 1.8756
2022-08-26 17:05:02 - train: epoch 0025, iter [01100, 05004], lr: 0.086211, loss: 1.9112
2022-08-26 17:05:36 - train: epoch 0025, iter [01200, 05004], lr: 0.086190, loss: 1.9469
2022-08-26 17:06:09 - train: epoch 0025, iter [01300, 05004], lr: 0.086168, loss: 1.9650
2022-08-26 17:06:43 - train: epoch 0025, iter [01400, 05004], lr: 0.086146, loss: 1.9766
2022-08-26 17:07:17 - train: epoch 0025, iter [01500, 05004], lr: 0.086124, loss: 1.8421
2022-08-26 17:07:51 - train: epoch 0025, iter [01600, 05004], lr: 0.086103, loss: 1.6545
2022-08-26 17:08:24 - train: epoch 0025, iter [01700, 05004], lr: 0.086081, loss: 1.9261
2022-08-26 17:08:58 - train: epoch 0025, iter [01800, 05004], lr: 0.086059, loss: 1.7780
2022-08-26 17:09:32 - train: epoch 0025, iter [01900, 05004], lr: 0.086038, loss: 1.8245
2022-08-26 17:10:06 - train: epoch 0025, iter [02000, 05004], lr: 0.086016, loss: 1.9061
2022-08-26 17:10:40 - train: epoch 0025, iter [02100, 05004], lr: 0.085994, loss: 1.7404
2022-08-26 17:11:15 - train: epoch 0025, iter [02200, 05004], lr: 0.085972, loss: 1.7946
2022-08-26 17:11:48 - train: epoch 0025, iter [02300, 05004], lr: 0.085950, loss: 1.8784
2022-08-26 17:12:21 - train: epoch 0025, iter [02400, 05004], lr: 0.085929, loss: 1.7992
2022-08-26 17:12:56 - train: epoch 0025, iter [02500, 05004], lr: 0.085907, loss: 1.9397
2022-08-26 17:13:29 - train: epoch 0025, iter [02600, 05004], lr: 0.085885, loss: 2.1329
2022-08-26 17:14:03 - train: epoch 0025, iter [02700, 05004], lr: 0.085863, loss: 1.9052
2022-08-26 17:14:37 - train: epoch 0025, iter [02800, 05004], lr: 0.085841, loss: 1.8813
2022-08-26 17:15:11 - train: epoch 0025, iter [02900, 05004], lr: 0.085819, loss: 2.1752
2022-08-26 17:15:45 - train: epoch 0025, iter [03000, 05004], lr: 0.085797, loss: 2.1299
2022-08-26 17:16:18 - train: epoch 0025, iter [03100, 05004], lr: 0.085775, loss: 1.8266
2022-08-26 17:16:52 - train: epoch 0025, iter [03200, 05004], lr: 0.085753, loss: 2.1554
2022-08-26 17:17:25 - train: epoch 0025, iter [03300, 05004], lr: 0.085732, loss: 1.8214
2022-08-26 17:17:58 - train: epoch 0025, iter [03400, 05004], lr: 0.085710, loss: 2.0225
2022-08-26 17:18:31 - train: epoch 0025, iter [03500, 05004], lr: 0.085688, loss: 1.7448
2022-08-26 17:19:05 - train: epoch 0025, iter [03600, 05004], lr: 0.085666, loss: 2.1952
2022-08-26 17:19:39 - train: epoch 0025, iter [03700, 05004], lr: 0.085644, loss: 1.8795
2022-08-26 17:20:13 - train: epoch 0025, iter [03800, 05004], lr: 0.085622, loss: 1.9872
2022-08-26 17:20:46 - train: epoch 0025, iter [03900, 05004], lr: 0.085600, loss: 2.1318
2022-08-26 17:21:21 - train: epoch 0025, iter [04000, 05004], lr: 0.085577, loss: 2.0046
2022-08-26 17:21:54 - train: epoch 0025, iter [04100, 05004], lr: 0.085555, loss: 2.2003
2022-08-26 17:22:28 - train: epoch 0025, iter [04200, 05004], lr: 0.085533, loss: 1.9571
2022-08-26 17:23:02 - train: epoch 0025, iter [04300, 05004], lr: 0.085511, loss: 1.8377
2022-08-26 17:23:36 - train: epoch 0025, iter [04400, 05004], lr: 0.085489, loss: 1.8347
2022-08-26 17:24:09 - train: epoch 0025, iter [04500, 05004], lr: 0.085467, loss: 1.7982
2022-08-26 17:24:43 - train: epoch 0025, iter [04600, 05004], lr: 0.085445, loss: 1.9817
2022-08-26 17:25:18 - train: epoch 0025, iter [04700, 05004], lr: 0.085423, loss: 1.8432
2022-08-26 17:25:52 - train: epoch 0025, iter [04800, 05004], lr: 0.085401, loss: 1.7239
2022-08-26 17:26:25 - train: epoch 0025, iter [04900, 05004], lr: 0.085378, loss: 1.9017
2022-08-26 17:26:58 - train: epoch 0025, iter [05000, 05004], lr: 0.085356, loss: 2.1184
2022-08-26 17:26:59 - train: epoch 025, train_loss: 1.9197
2022-08-26 17:28:15 - eval: epoch: 025, acc1: 56.854%, acc5: 80.898%, test_loss: 1.8470, per_image_load_time: 2.448ms, per_image_inference_time: 0.488ms
2022-08-26 17:28:16 - until epoch: 025, best_acc1: 58.194%
2022-08-26 17:28:16 - epoch 026 lr: 0.085355
2022-08-26 17:28:55 - train: epoch 0026, iter [00100, 05004], lr: 0.085333, loss: 1.7025
2022-08-26 17:29:29 - train: epoch 0026, iter [00200, 05004], lr: 0.085311, loss: 1.7503
2022-08-26 17:30:03 - train: epoch 0026, iter [00300, 05004], lr: 0.085289, loss: 1.8512
2022-08-26 17:30:36 - train: epoch 0026, iter [00400, 05004], lr: 0.085266, loss: 1.8734
2022-08-26 17:31:10 - train: epoch 0026, iter [00500, 05004], lr: 0.085244, loss: 1.9014
2022-08-26 17:31:44 - train: epoch 0026, iter [00600, 05004], lr: 0.085222, loss: 2.0717
2022-08-26 17:32:17 - train: epoch 0026, iter [00700, 05004], lr: 0.085200, loss: 1.8182
2022-08-26 17:32:52 - train: epoch 0026, iter [00800, 05004], lr: 0.085177, loss: 1.6644
2022-08-26 17:33:26 - train: epoch 0026, iter [00900, 05004], lr: 0.085155, loss: 2.1594
2022-08-26 17:33:59 - train: epoch 0026, iter [01000, 05004], lr: 0.085133, loss: 1.8025
2022-08-26 17:34:33 - train: epoch 0026, iter [01100, 05004], lr: 0.085110, loss: 1.8417
2022-08-26 17:35:07 - train: epoch 0026, iter [01200, 05004], lr: 0.085088, loss: 1.9053
2022-08-26 17:35:40 - train: epoch 0026, iter [01300, 05004], lr: 0.085066, loss: 1.8561
2022-08-26 17:36:15 - train: epoch 0026, iter [01400, 05004], lr: 0.085043, loss: 2.0099
2022-08-26 17:36:47 - train: epoch 0026, iter [01500, 05004], lr: 0.085021, loss: 1.8056
2022-08-26 17:37:22 - train: epoch 0026, iter [01600, 05004], lr: 0.084998, loss: 1.9757
2022-08-26 17:37:55 - train: epoch 0026, iter [01700, 05004], lr: 0.084976, loss: 1.8890
2022-08-26 17:38:29 - train: epoch 0026, iter [01800, 05004], lr: 0.084954, loss: 1.9725
2022-08-26 17:39:03 - train: epoch 0026, iter [01900, 05004], lr: 0.084931, loss: 2.1805
2022-08-26 17:39:36 - train: epoch 0026, iter [02000, 05004], lr: 0.084909, loss: 1.9547
2022-08-26 17:40:10 - train: epoch 0026, iter [02100, 05004], lr: 0.084886, loss: 2.0311
2022-08-26 17:40:43 - train: epoch 0026, iter [02200, 05004], lr: 0.084864, loss: 1.8968
2022-08-26 17:41:18 - train: epoch 0026, iter [02300, 05004], lr: 0.084841, loss: 1.9131
2022-08-26 17:41:52 - train: epoch 0026, iter [02400, 05004], lr: 0.084819, loss: 2.1327
2022-08-26 17:42:25 - train: epoch 0026, iter [02500, 05004], lr: 0.084796, loss: 1.9625
2022-08-26 17:42:59 - train: epoch 0026, iter [02600, 05004], lr: 0.084774, loss: 1.9754
2022-08-26 17:43:33 - train: epoch 0026, iter [02700, 05004], lr: 0.084751, loss: 1.9470
2022-08-26 17:44:07 - train: epoch 0026, iter [02800, 05004], lr: 0.084728, loss: 1.8186
2022-08-26 17:44:41 - train: epoch 0026, iter [02900, 05004], lr: 0.084706, loss: 1.9620
2022-08-26 17:45:15 - train: epoch 0026, iter [03000, 05004], lr: 0.084683, loss: 1.8642
2022-08-26 17:45:48 - train: epoch 0026, iter [03100, 05004], lr: 0.084661, loss: 1.9155
2022-08-26 17:46:22 - train: epoch 0026, iter [03200, 05004], lr: 0.084638, loss: 1.9651
2022-08-26 17:46:56 - train: epoch 0026, iter [03300, 05004], lr: 0.084615, loss: 1.8401
2022-08-26 17:47:30 - train: epoch 0026, iter [03400, 05004], lr: 0.084593, loss: 1.9777
2022-08-26 17:48:04 - train: epoch 0026, iter [03500, 05004], lr: 0.084570, loss: 2.1490
2022-08-26 17:48:37 - train: epoch 0026, iter [03600, 05004], lr: 0.084547, loss: 1.6410
2022-08-26 17:49:11 - train: epoch 0026, iter [03700, 05004], lr: 0.084525, loss: 2.0376
2022-08-26 17:49:45 - train: epoch 0026, iter [03800, 05004], lr: 0.084502, loss: 2.0228
2022-08-26 17:50:19 - train: epoch 0026, iter [03900, 05004], lr: 0.084479, loss: 2.0417
2022-08-26 17:50:52 - train: epoch 0026, iter [04000, 05004], lr: 0.084456, loss: 2.1046
2022-08-26 17:51:26 - train: epoch 0026, iter [04100, 05004], lr: 0.084434, loss: 2.0816
2022-08-26 17:52:00 - train: epoch 0026, iter [04200, 05004], lr: 0.084411, loss: 1.9700
2022-08-26 17:52:33 - train: epoch 0026, iter [04300, 05004], lr: 0.084388, loss: 2.0404
2022-08-26 17:53:06 - train: epoch 0026, iter [04400, 05004], lr: 0.084365, loss: 2.1991
2022-08-26 17:53:41 - train: epoch 0026, iter [04500, 05004], lr: 0.084343, loss: 2.2001
2022-08-26 17:54:15 - train: epoch 0026, iter [04600, 05004], lr: 0.084320, loss: 1.8910
2022-08-26 17:54:49 - train: epoch 0026, iter [04700, 05004], lr: 0.084297, loss: 1.7853
2022-08-26 17:55:22 - train: epoch 0026, iter [04800, 05004], lr: 0.084274, loss: 1.8687
2022-08-26 17:55:57 - train: epoch 0026, iter [04900, 05004], lr: 0.084251, loss: 1.8773
2022-08-26 17:56:29 - train: epoch 0026, iter [05000, 05004], lr: 0.084228, loss: 2.1481
2022-08-26 17:56:30 - train: epoch 026, train_loss: 1.9082
2022-08-26 17:57:46 - eval: epoch: 026, acc1: 59.426%, acc5: 83.180%, test_loss: 1.6952, per_image_load_time: 1.399ms, per_image_inference_time: 0.517ms
2022-08-26 17:57:46 - until epoch: 026, best_acc1: 59.426%
2022-08-26 17:57:46 - epoch 027 lr: 0.084227
2022-08-26 17:58:26 - train: epoch 0027, iter [00100, 05004], lr: 0.084204, loss: 2.0114
2022-08-26 17:58:59 - train: epoch 0027, iter [00200, 05004], lr: 0.084182, loss: 1.8538
2022-08-26 17:59:33 - train: epoch 0027, iter [00300, 05004], lr: 0.084159, loss: 1.9014
2022-08-26 18:00:07 - train: epoch 0027, iter [00400, 05004], lr: 0.084136, loss: 2.1108
2022-08-26 18:00:42 - train: epoch 0027, iter [00500, 05004], lr: 0.084113, loss: 1.9163
2022-08-26 18:01:16 - train: epoch 0027, iter [00600, 05004], lr: 0.084090, loss: 2.2113
2022-08-26 18:01:50 - train: epoch 0027, iter [00700, 05004], lr: 0.084067, loss: 2.0056
2022-08-26 18:02:24 - train: epoch 0027, iter [00800, 05004], lr: 0.084044, loss: 1.9958
2022-08-26 18:02:58 - train: epoch 0027, iter [00900, 05004], lr: 0.084021, loss: 1.7528
2022-08-26 18:03:32 - train: epoch 0027, iter [01000, 05004], lr: 0.083998, loss: 1.9661
2022-08-26 18:04:06 - train: epoch 0027, iter [01100, 05004], lr: 0.083975, loss: 1.8530
2022-08-26 18:04:40 - train: epoch 0027, iter [01200, 05004], lr: 0.083952, loss: 2.1675
2022-08-26 18:05:14 - train: epoch 0027, iter [01300, 05004], lr: 0.083929, loss: 1.8537
2022-08-26 18:05:48 - train: epoch 0027, iter [01400, 05004], lr: 0.083906, loss: 2.0705
2022-08-26 18:06:22 - train: epoch 0027, iter [01500, 05004], lr: 0.083883, loss: 1.9301
2022-08-26 18:06:56 - train: epoch 0027, iter [01600, 05004], lr: 0.083860, loss: 1.9860
2022-08-26 18:07:30 - train: epoch 0027, iter [01700, 05004], lr: 0.083836, loss: 1.8729
2022-08-26 18:08:04 - train: epoch 0027, iter [01800, 05004], lr: 0.083813, loss: 1.8561
2022-08-26 18:08:37 - train: epoch 0027, iter [01900, 05004], lr: 0.083790, loss: 2.1060
2022-08-26 18:09:11 - train: epoch 0027, iter [02000, 05004], lr: 0.083767, loss: 1.8392
2022-08-26 18:09:44 - train: epoch 0027, iter [02100, 05004], lr: 0.083744, loss: 2.1722
2022-08-26 18:10:18 - train: epoch 0027, iter [02200, 05004], lr: 0.083721, loss: 2.0263
2022-08-26 18:10:52 - train: epoch 0027, iter [02300, 05004], lr: 0.083697, loss: 2.2171
2022-08-26 18:11:26 - train: epoch 0027, iter [02400, 05004], lr: 0.083674, loss: 1.9543
2022-08-26 18:12:00 - train: epoch 0027, iter [02500, 05004], lr: 0.083651, loss: 1.7506
2022-08-26 18:12:33 - train: epoch 0027, iter [02600, 05004], lr: 0.083628, loss: 2.1555
2022-08-26 18:13:08 - train: epoch 0027, iter [02700, 05004], lr: 0.083605, loss: 1.9374
2022-08-26 18:13:42 - train: epoch 0027, iter [02800, 05004], lr: 0.083581, loss: 2.0969
2022-08-26 18:14:15 - train: epoch 0027, iter [02900, 05004], lr: 0.083558, loss: 1.9165
2022-08-26 18:14:49 - train: epoch 0027, iter [03000, 05004], lr: 0.083535, loss: 1.7381
2022-08-26 18:15:23 - train: epoch 0027, iter [03100, 05004], lr: 0.083512, loss: 1.8491
2022-08-26 18:15:57 - train: epoch 0027, iter [03200, 05004], lr: 0.083488, loss: 1.7072
2022-08-26 18:16:30 - train: epoch 0027, iter [03300, 05004], lr: 0.083465, loss: 1.9386
2022-08-26 18:17:03 - train: epoch 0027, iter [03400, 05004], lr: 0.083442, loss: 1.8309
2022-08-26 18:17:38 - train: epoch 0027, iter [03500, 05004], lr: 0.083418, loss: 2.1504
2022-08-26 18:18:12 - train: epoch 0027, iter [03600, 05004], lr: 0.083395, loss: 1.7430
2022-08-26 18:18:46 - train: epoch 0027, iter [03700, 05004], lr: 0.083372, loss: 1.8613
2022-08-26 18:19:20 - train: epoch 0027, iter [03800, 05004], lr: 0.083348, loss: 1.6715
2022-08-26 18:19:53 - train: epoch 0027, iter [03900, 05004], lr: 0.083325, loss: 1.7763
2022-08-26 18:20:27 - train: epoch 0027, iter [04000, 05004], lr: 0.083301, loss: 1.9369
2022-08-26 18:21:01 - train: epoch 0027, iter [04100, 05004], lr: 0.083278, loss: 1.8773
2022-08-26 18:21:35 - train: epoch 0027, iter [04200, 05004], lr: 0.083254, loss: 1.9077
2022-08-26 18:22:09 - train: epoch 0027, iter [04300, 05004], lr: 0.083231, loss: 1.8062
2022-08-26 18:22:42 - train: epoch 0027, iter [04400, 05004], lr: 0.083208, loss: 1.9210
2022-08-26 18:23:16 - train: epoch 0027, iter [04500, 05004], lr: 0.083184, loss: 1.7089
2022-08-26 18:23:49 - train: epoch 0027, iter [04600, 05004], lr: 0.083161, loss: 1.8159
2022-08-26 18:24:23 - train: epoch 0027, iter [04700, 05004], lr: 0.083137, loss: 1.8911
2022-08-26 18:24:58 - train: epoch 0027, iter [04800, 05004], lr: 0.083114, loss: 2.0061
2022-08-26 18:25:31 - train: epoch 0027, iter [04900, 05004], lr: 0.083090, loss: 1.9623
2022-08-26 18:26:04 - train: epoch 0027, iter [05000, 05004], lr: 0.083067, loss: 1.5717
2022-08-26 18:26:05 - train: epoch 027, train_loss: 1.8989
2022-08-26 18:27:20 - eval: epoch: 027, acc1: 59.434%, acc5: 83.072%, test_loss: 1.6981, per_image_load_time: 1.847ms, per_image_inference_time: 0.482ms
2022-08-26 18:27:21 - until epoch: 027, best_acc1: 59.434%
2022-08-26 18:27:21 - epoch 028 lr: 0.083065
2022-08-26 18:28:00 - train: epoch 0028, iter [00100, 05004], lr: 0.083042, loss: 1.6789
2022-08-26 18:28:34 - train: epoch 0028, iter [00200, 05004], lr: 0.083018, loss: 1.7467
2022-08-26 18:29:08 - train: epoch 0028, iter [00300, 05004], lr: 0.082995, loss: 1.8744
2022-08-26 18:29:41 - train: epoch 0028, iter [00400, 05004], lr: 0.082971, loss: 1.7850
2022-08-26 18:30:15 - train: epoch 0028, iter [00500, 05004], lr: 0.082948, loss: 1.9200
2022-08-26 18:30:49 - train: epoch 0028, iter [00600, 05004], lr: 0.082924, loss: 2.0695
2022-08-26 18:31:22 - train: epoch 0028, iter [00700, 05004], lr: 0.082900, loss: 1.9980
2022-08-26 18:31:56 - train: epoch 0028, iter [00800, 05004], lr: 0.082877, loss: 1.5990
2022-08-26 18:32:30 - train: epoch 0028, iter [00900, 05004], lr: 0.082853, loss: 1.7097
2022-08-26 18:33:04 - train: epoch 0028, iter [01000, 05004], lr: 0.082829, loss: 1.9274
2022-08-26 18:33:38 - train: epoch 0028, iter [01100, 05004], lr: 0.082806, loss: 2.0238
2022-08-26 18:34:11 - train: epoch 0028, iter [01200, 05004], lr: 0.082782, loss: 1.7210
2022-08-26 18:34:46 - train: epoch 0028, iter [01300, 05004], lr: 0.082758, loss: 1.8193
2022-08-26 18:35:20 - train: epoch 0028, iter [01400, 05004], lr: 0.082735, loss: 2.2608
2022-08-26 18:35:53 - train: epoch 0028, iter [01500, 05004], lr: 0.082711, loss: 1.8433
2022-08-26 18:36:27 - train: epoch 0028, iter [01600, 05004], lr: 0.082687, loss: 1.9559
2022-08-26 18:37:01 - train: epoch 0028, iter [01700, 05004], lr: 0.082663, loss: 1.8654
2022-08-26 18:37:35 - train: epoch 0028, iter [01800, 05004], lr: 0.082640, loss: 1.7949
2022-08-26 18:38:08 - train: epoch 0028, iter [01900, 05004], lr: 0.082616, loss: 1.8473
2022-08-26 18:38:43 - train: epoch 0028, iter [02000, 05004], lr: 0.082592, loss: 2.0388
2022-08-26 18:39:17 - train: epoch 0028, iter [02100, 05004], lr: 0.082568, loss: 1.9412
2022-08-26 18:39:51 - train: epoch 0028, iter [02200, 05004], lr: 0.082544, loss: 1.9321
2022-08-26 18:40:24 - train: epoch 0028, iter [02300, 05004], lr: 0.082521, loss: 2.0489
2022-08-26 18:40:58 - train: epoch 0028, iter [02400, 05004], lr: 0.082497, loss: 2.1297
2022-08-26 18:41:32 - train: epoch 0028, iter [02500, 05004], lr: 0.082473, loss: 1.9772
2022-08-26 18:42:06 - train: epoch 0028, iter [02600, 05004], lr: 0.082449, loss: 1.6719
2022-08-26 18:42:40 - train: epoch 0028, iter [02700, 05004], lr: 0.082425, loss: 2.0178
2022-08-26 18:43:13 - train: epoch 0028, iter [02800, 05004], lr: 0.082401, loss: 1.8071
2022-08-26 18:43:48 - train: epoch 0028, iter [02900, 05004], lr: 0.082377, loss: 1.9051
2022-08-26 18:44:21 - train: epoch 0028, iter [03000, 05004], lr: 0.082353, loss: 2.0266
2022-08-26 18:44:55 - train: epoch 0028, iter [03100, 05004], lr: 0.082329, loss: 2.0427
2022-08-26 18:45:29 - train: epoch 0028, iter [03200, 05004], lr: 0.082305, loss: 1.8655
2022-08-26 18:46:03 - train: epoch 0028, iter [03300, 05004], lr: 0.082282, loss: 1.9374
2022-08-26 18:46:37 - train: epoch 0028, iter [03400, 05004], lr: 0.082258, loss: 1.8012
2022-08-26 18:47:10 - train: epoch 0028, iter [03500, 05004], lr: 0.082234, loss: 1.7889
2022-08-26 18:47:44 - train: epoch 0028, iter [03600, 05004], lr: 0.082210, loss: 1.7815
2022-08-26 18:48:18 - train: epoch 0028, iter [03700, 05004], lr: 0.082186, loss: 1.8633
2022-08-26 18:48:51 - train: epoch 0028, iter [03800, 05004], lr: 0.082161, loss: 1.6377
2022-08-26 18:49:25 - train: epoch 0028, iter [03900, 05004], lr: 0.082137, loss: 1.9008
2022-08-26 18:49:59 - train: epoch 0028, iter [04000, 05004], lr: 0.082113, loss: 2.0408
2022-08-26 18:50:33 - train: epoch 0028, iter [04100, 05004], lr: 0.082089, loss: 2.0152
2022-08-26 18:51:08 - train: epoch 0028, iter [04200, 05004], lr: 0.082065, loss: 2.0870
2022-08-26 18:51:41 - train: epoch 0028, iter [04300, 05004], lr: 0.082041, loss: 1.7917
2022-08-26 18:52:16 - train: epoch 0028, iter [04400, 05004], lr: 0.082017, loss: 1.8390
2022-08-26 18:52:49 - train: epoch 0028, iter [04500, 05004], lr: 0.081993, loss: 1.9001
2022-08-26 18:53:24 - train: epoch 0028, iter [04600, 05004], lr: 0.081969, loss: 1.9682
2022-08-26 18:53:57 - train: epoch 0028, iter [04700, 05004], lr: 0.081945, loss: 2.0000
2022-08-26 18:54:31 - train: epoch 0028, iter [04800, 05004], lr: 0.081921, loss: 1.8682
2022-08-26 18:55:05 - train: epoch 0028, iter [04900, 05004], lr: 0.081896, loss: 1.7126
2022-08-26 18:55:37 - train: epoch 0028, iter [05000, 05004], lr: 0.081872, loss: 1.8434
2022-08-26 18:55:38 - train: epoch 028, train_loss: 1.8834
2022-08-26 18:56:54 - eval: epoch: 028, acc1: 60.388%, acc5: 83.576%, test_loss: 1.6640, per_image_load_time: 2.391ms, per_image_inference_time: 0.512ms
2022-08-26 18:56:54 - until epoch: 028, best_acc1: 60.388%
2022-08-26 18:56:54 - epoch 029 lr: 0.081871
2022-08-26 18:57:33 - train: epoch 0029, iter [00100, 05004], lr: 0.081847, loss: 1.9743
2022-08-26 18:58:07 - train: epoch 0029, iter [00200, 05004], lr: 0.081823, loss: 1.9957
2022-08-26 18:58:41 - train: epoch 0029, iter [00300, 05004], lr: 0.081799, loss: 1.9798
2022-08-26 18:59:15 - train: epoch 0029, iter [00400, 05004], lr: 0.081774, loss: 1.7566
2022-08-26 18:59:48 - train: epoch 0029, iter [00500, 05004], lr: 0.081750, loss: 1.8837
2022-08-26 19:00:23 - train: epoch 0029, iter [00600, 05004], lr: 0.081726, loss: 2.0692
2022-08-26 19:00:57 - train: epoch 0029, iter [00700, 05004], lr: 0.081702, loss: 1.3371
2022-08-26 19:01:31 - train: epoch 0029, iter [00800, 05004], lr: 0.081677, loss: 1.9389
2022-08-26 19:02:05 - train: epoch 0029, iter [00900, 05004], lr: 0.081653, loss: 1.7242
2022-08-26 19:02:39 - train: epoch 0029, iter [01000, 05004], lr: 0.081629, loss: 1.6543
2022-08-26 19:03:13 - train: epoch 0029, iter [01100, 05004], lr: 0.081604, loss: 1.6611
2022-08-26 19:03:47 - train: epoch 0029, iter [01200, 05004], lr: 0.081580, loss: 1.9382
2022-08-26 19:04:20 - train: epoch 0029, iter [01300, 05004], lr: 0.081556, loss: 1.7895
2022-08-26 19:04:54 - train: epoch 0029, iter [01400, 05004], lr: 0.081531, loss: 2.0353
2022-08-26 19:05:28 - train: epoch 0029, iter [01500, 05004], lr: 0.081507, loss: 1.8874
2022-08-26 19:06:02 - train: epoch 0029, iter [01600, 05004], lr: 0.081483, loss: 1.8352
2022-08-26 19:06:35 - train: epoch 0029, iter [01700, 05004], lr: 0.081458, loss: 1.9397
2022-08-26 19:07:09 - train: epoch 0029, iter [01800, 05004], lr: 0.081434, loss: 1.9902
2022-08-26 19:07:42 - train: epoch 0029, iter [01900, 05004], lr: 0.081409, loss: 1.6996
2022-08-26 19:08:16 - train: epoch 0029, iter [02000, 05004], lr: 0.081385, loss: 2.0587
2022-08-26 19:08:50 - train: epoch 0029, iter [02100, 05004], lr: 0.081361, loss: 1.8692
2022-08-26 19:09:24 - train: epoch 0029, iter [02200, 05004], lr: 0.081336, loss: 1.9743
2022-08-26 19:09:57 - train: epoch 0029, iter [02300, 05004], lr: 0.081312, loss: 1.8749
2022-08-26 19:10:31 - train: epoch 0029, iter [02400, 05004], lr: 0.081287, loss: 1.7407
2022-08-26 19:11:05 - train: epoch 0029, iter [02500, 05004], lr: 0.081263, loss: 1.7780
2022-08-26 19:11:38 - train: epoch 0029, iter [02600, 05004], lr: 0.081238, loss: 1.9457
2022-08-26 19:12:12 - train: epoch 0029, iter [02700, 05004], lr: 0.081214, loss: 1.8099
2022-08-26 19:12:45 - train: epoch 0029, iter [02800, 05004], lr: 0.081189, loss: 1.7358
2022-08-26 19:13:19 - train: epoch 0029, iter [02900, 05004], lr: 0.081165, loss: 1.8069
2022-08-26 19:13:53 - train: epoch 0029, iter [03000, 05004], lr: 0.081140, loss: 1.8438
2022-08-26 19:14:26 - train: epoch 0029, iter [03100, 05004], lr: 0.081115, loss: 1.8442
2022-08-26 19:14:59 - train: epoch 0029, iter [03200, 05004], lr: 0.081091, loss: 1.8625
2022-08-26 19:15:33 - train: epoch 0029, iter [03300, 05004], lr: 0.081066, loss: 1.8598
2022-08-26 19:16:07 - train: epoch 0029, iter [03400, 05004], lr: 0.081042, loss: 1.7913
2022-08-26 19:16:40 - train: epoch 0029, iter [03500, 05004], lr: 0.081017, loss: 1.8969
2022-08-26 19:17:14 - train: epoch 0029, iter [03600, 05004], lr: 0.080992, loss: 1.8292
2022-08-26 19:17:48 - train: epoch 0029, iter [03700, 05004], lr: 0.080968, loss: 1.7195
2022-08-26 19:18:21 - train: epoch 0029, iter [03800, 05004], lr: 0.080943, loss: 1.8640
2022-08-26 19:18:56 - train: epoch 0029, iter [03900, 05004], lr: 0.080918, loss: 1.6919
2022-08-26 19:19:30 - train: epoch 0029, iter [04000, 05004], lr: 0.080894, loss: 1.7565
2022-08-26 19:20:04 - train: epoch 0029, iter [04100, 05004], lr: 0.080869, loss: 1.8299
2022-08-26 19:20:39 - train: epoch 0029, iter [04200, 05004], lr: 0.080844, loss: 1.7585
2022-08-26 19:21:13 - train: epoch 0029, iter [04300, 05004], lr: 0.080820, loss: 1.9841
2022-08-26 19:21:48 - train: epoch 0029, iter [04400, 05004], lr: 0.080795, loss: 1.8718
2022-08-26 19:22:27 - train: epoch 0029, iter [04500, 05004], lr: 0.080770, loss: 2.0300
2022-08-26 19:24:12 - train: epoch 0029, iter [04600, 05004], lr: 0.080745, loss: 2.0570
2022-08-26 19:25:58 - train: epoch 0029, iter [04700, 05004], lr: 0.080721, loss: 1.6480
2022-08-26 19:27:57 - train: epoch 0029, iter [04800, 05004], lr: 0.080696, loss: 1.8987
2022-08-26 19:29:38 - train: epoch 0029, iter [04900, 05004], lr: 0.080671, loss: 2.1815
2022-08-26 19:31:21 - train: epoch 0029, iter [05000, 05004], lr: 0.080646, loss: 1.6522
2022-08-26 19:31:24 - train: epoch 029, train_loss: 1.8729
2022-08-26 19:35:16 - eval: epoch: 029, acc1: 57.050%, acc5: 81.096%, test_loss: 1.8276, per_image_load_time: 3.338ms, per_image_inference_time: 0.822ms
2022-08-26 19:35:17 - until epoch: 029, best_acc1: 60.388%
2022-08-26 19:35:17 - epoch 030 lr: 0.080645
2022-08-26 19:37:11 - train: epoch 0030, iter [00100, 05004], lr: 0.080621, loss: 2.0435
2022-08-26 19:38:48 - train: epoch 0030, iter [00200, 05004], lr: 0.080596, loss: 1.9079
2022-08-26 19:40:23 - train: epoch 0030, iter [00300, 05004], lr: 0.080571, loss: 1.7847
2022-08-26 19:41:58 - train: epoch 0030, iter [00400, 05004], lr: 0.080546, loss: 1.5555
2022-08-26 19:43:39 - train: epoch 0030, iter [00500, 05004], lr: 0.080521, loss: 2.0419
2022-08-26 19:45:21 - train: epoch 0030, iter [00600, 05004], lr: 0.080496, loss: 1.6272
2022-08-26 19:47:03 - train: epoch 0030, iter [00700, 05004], lr: 0.080471, loss: 1.8676
2022-08-26 19:48:49 - train: epoch 0030, iter [00800, 05004], lr: 0.080447, loss: 2.0634
2022-08-26 19:50:28 - train: epoch 0030, iter [00900, 05004], lr: 0.080422, loss: 1.8949
2022-08-26 19:52:10 - train: epoch 0030, iter [01000, 05004], lr: 0.080397, loss: 1.7231
2022-08-26 19:53:52 - train: epoch 0030, iter [01100, 05004], lr: 0.080372, loss: 1.6969
2022-08-26 19:55:29 - train: epoch 0030, iter [01200, 05004], lr: 0.080347, loss: 1.8531
2022-08-26 19:57:19 - train: epoch 0030, iter [01300, 05004], lr: 0.080322, loss: 1.6613
2022-08-26 19:59:01 - train: epoch 0030, iter [01400, 05004], lr: 0.080297, loss: 1.8277
2022-08-26 20:00:43 - train: epoch 0030, iter [01500, 05004], lr: 0.080272, loss: 1.8580
2022-08-26 20:02:21 - train: epoch 0030, iter [01600, 05004], lr: 0.080247, loss: 1.7856
2022-08-26 20:04:10 - train: epoch 0030, iter [01700, 05004], lr: 0.080222, loss: 2.0760
2022-08-26 20:05:53 - train: epoch 0030, iter [01800, 05004], lr: 0.080197, loss: 1.8911
2022-08-26 20:07:39 - train: epoch 0030, iter [01900, 05004], lr: 0.080172, loss: 1.9997
2022-08-26 20:08:46 - train: epoch 0030, iter [02000, 05004], lr: 0.080147, loss: 1.8760
2022-08-26 20:09:43 - train: epoch 0030, iter [02100, 05004], lr: 0.080122, loss: 1.9157
2022-08-26 20:10:33 - train: epoch 0030, iter [02200, 05004], lr: 0.080097, loss: 1.8191
2022-08-26 20:11:07 - train: epoch 0030, iter [02300, 05004], lr: 0.080072, loss: 1.6869
2022-08-26 20:11:40 - train: epoch 0030, iter [02400, 05004], lr: 0.080047, loss: 2.0144
2022-08-26 20:12:14 - train: epoch 0030, iter [02500, 05004], lr: 0.080022, loss: 1.9066
2022-08-26 20:12:47 - train: epoch 0030, iter [02600, 05004], lr: 0.079996, loss: 1.8107
2022-08-26 20:13:21 - train: epoch 0030, iter [02700, 05004], lr: 0.079971, loss: 1.8129
2022-08-26 20:13:54 - train: epoch 0030, iter [02800, 05004], lr: 0.079946, loss: 1.8046
2022-08-26 20:14:28 - train: epoch 0030, iter [02900, 05004], lr: 0.079921, loss: 1.8816
2022-08-26 20:15:02 - train: epoch 0030, iter [03000, 05004], lr: 0.079896, loss: 2.0076
2022-08-26 20:15:36 - train: epoch 0030, iter [03100, 05004], lr: 0.079871, loss: 1.7914
2022-08-26 20:16:09 - train: epoch 0030, iter [03200, 05004], lr: 0.079846, loss: 1.8630
2022-08-26 20:16:43 - train: epoch 0030, iter [03300, 05004], lr: 0.079820, loss: 2.1701
2022-08-26 20:17:17 - train: epoch 0030, iter [03400, 05004], lr: 0.079795, loss: 1.8494
2022-08-26 20:17:51 - train: epoch 0030, iter [03500, 05004], lr: 0.079770, loss: 2.0528
2022-08-26 20:18:25 - train: epoch 0030, iter [03600, 05004], lr: 0.079745, loss: 1.7758
2022-08-26 20:18:58 - train: epoch 0030, iter [03700, 05004], lr: 0.079719, loss: 1.8316
2022-08-26 20:19:32 - train: epoch 0030, iter [03800, 05004], lr: 0.079694, loss: 1.9360
2022-08-26 20:20:06 - train: epoch 0030, iter [03900, 05004], lr: 0.079669, loss: 1.9248
2022-08-26 20:20:40 - train: epoch 0030, iter [04000, 05004], lr: 0.079644, loss: 1.7357
2022-08-26 20:21:13 - train: epoch 0030, iter [04100, 05004], lr: 0.079618, loss: 1.7909
2022-08-26 20:21:47 - train: epoch 0030, iter [04200, 05004], lr: 0.079593, loss: 2.0259
2022-08-26 20:22:21 - train: epoch 0030, iter [04300, 05004], lr: 0.079568, loss: 1.9478
2022-08-26 20:22:55 - train: epoch 0030, iter [04400, 05004], lr: 0.079542, loss: 1.9122
2022-08-26 20:23:28 - train: epoch 0030, iter [04500, 05004], lr: 0.079517, loss: 2.0840
2022-08-26 20:24:02 - train: epoch 0030, iter [04600, 05004], lr: 0.079492, loss: 1.6133
2022-08-26 20:24:35 - train: epoch 0030, iter [04700, 05004], lr: 0.079466, loss: 1.8042
2022-08-26 20:25:09 - train: epoch 0030, iter [04800, 05004], lr: 0.079441, loss: 1.8102
2022-08-26 20:25:43 - train: epoch 0030, iter [04900, 05004], lr: 0.079416, loss: 2.0791
2022-08-26 20:26:16 - train: epoch 0030, iter [05000, 05004], lr: 0.079390, loss: 2.0009
2022-08-26 20:26:17 - train: epoch 030, train_loss: 1.8659
2022-08-26 20:27:32 - eval: epoch: 030, acc1: 59.098%, acc5: 82.868%, test_loss: 1.7127, per_image_load_time: 2.418ms, per_image_inference_time: 0.494ms
2022-08-26 20:27:32 - until epoch: 030, best_acc1: 60.388%
2022-08-26 20:27:32 - epoch 031 lr: 0.079389
2022-08-26 20:28:11 - train: epoch 0031, iter [00100, 05004], lr: 0.079364, loss: 1.9252
2022-08-26 20:28:46 - train: epoch 0031, iter [00200, 05004], lr: 0.079338, loss: 1.7177
2022-08-26 20:29:19 - train: epoch 0031, iter [00300, 05004], lr: 0.079313, loss: 1.6944
2022-08-26 20:29:53 - train: epoch 0031, iter [00400, 05004], lr: 0.079288, loss: 1.9792
2022-08-26 20:30:26 - train: epoch 0031, iter [00500, 05004], lr: 0.079262, loss: 1.7891
2022-08-26 20:31:00 - train: epoch 0031, iter [00600, 05004], lr: 0.079237, loss: 1.8331
2022-08-26 20:31:35 - train: epoch 0031, iter [00700, 05004], lr: 0.079211, loss: 1.9148
2022-08-26 20:32:07 - train: epoch 0031, iter [00800, 05004], lr: 0.079186, loss: 1.7440
2022-08-26 20:32:41 - train: epoch 0031, iter [00900, 05004], lr: 0.079160, loss: 1.7649
2022-08-26 20:33:15 - train: epoch 0031, iter [01000, 05004], lr: 0.079135, loss: 2.1558
2022-08-26 20:33:49 - train: epoch 0031, iter [01100, 05004], lr: 0.079109, loss: 2.0924
2022-08-26 20:34:22 - train: epoch 0031, iter [01200, 05004], lr: 0.079084, loss: 1.8437
2022-08-26 20:34:56 - train: epoch 0031, iter [01300, 05004], lr: 0.079058, loss: 1.6243
2022-08-26 20:35:30 - train: epoch 0031, iter [01400, 05004], lr: 0.079033, loss: 1.8817
2022-08-26 20:36:03 - train: epoch 0031, iter [01500, 05004], lr: 0.079007, loss: 2.0280
2022-08-26 20:36:36 - train: epoch 0031, iter [01600, 05004], lr: 0.078981, loss: 1.7973
2022-08-26 20:37:09 - train: epoch 0031, iter [01700, 05004], lr: 0.078956, loss: 1.7564
2022-08-26 20:37:43 - train: epoch 0031, iter [01800, 05004], lr: 0.078930, loss: 1.7089
2022-08-26 20:38:16 - train: epoch 0031, iter [01900, 05004], lr: 0.078905, loss: 1.8871
2022-08-26 20:38:50 - train: epoch 0031, iter [02000, 05004], lr: 0.078879, loss: 1.9255
2022-08-26 20:39:24 - train: epoch 0031, iter [02100, 05004], lr: 0.078853, loss: 1.6507
2022-08-26 20:39:57 - train: epoch 0031, iter [02200, 05004], lr: 0.078828, loss: 1.6552
2022-08-26 20:40:31 - train: epoch 0031, iter [02300, 05004], lr: 0.078802, loss: 1.6179
2022-08-26 20:41:04 - train: epoch 0031, iter [02400, 05004], lr: 0.078776, loss: 1.8375
2022-08-26 20:41:38 - train: epoch 0031, iter [02500, 05004], lr: 0.078751, loss: 1.7151
2022-08-26 20:42:12 - train: epoch 0031, iter [02600, 05004], lr: 0.078725, loss: 1.8423
2022-08-26 20:42:45 - train: epoch 0031, iter [02700, 05004], lr: 0.078699, loss: 1.9566
2022-08-26 20:43:18 - train: epoch 0031, iter [02800, 05004], lr: 0.078674, loss: 2.1706
2022-08-26 20:43:53 - train: epoch 0031, iter [02900, 05004], lr: 0.078648, loss: 1.9867
2022-08-26 20:44:26 - train: epoch 0031, iter [03000, 05004], lr: 0.078622, loss: 2.0797
2022-08-26 20:44:59 - train: epoch 0031, iter [03100, 05004], lr: 0.078596, loss: 1.7682
2022-08-26 20:45:33 - train: epoch 0031, iter [03200, 05004], lr: 0.078571, loss: 2.0310
2022-08-26 20:46:06 - train: epoch 0031, iter [03300, 05004], lr: 0.078545, loss: 1.8842
2022-08-26 20:46:40 - train: epoch 0031, iter [03400, 05004], lr: 0.078519, loss: 1.9368
2022-08-26 20:47:15 - train: epoch 0031, iter [03500, 05004], lr: 0.078493, loss: 1.9293
2022-08-26 20:47:48 - train: epoch 0031, iter [03600, 05004], lr: 0.078468, loss: 1.9043
2022-08-26 20:48:21 - train: epoch 0031, iter [03700, 05004], lr: 0.078442, loss: 1.8140
2022-08-26 20:48:55 - train: epoch 0031, iter [03800, 05004], lr: 0.078416, loss: 1.7474
2022-08-26 20:49:28 - train: epoch 0031, iter [03900, 05004], lr: 0.078390, loss: 1.8257
2022-08-26 20:50:03 - train: epoch 0031, iter [04000, 05004], lr: 0.078364, loss: 1.8245
2022-08-26 20:50:37 - train: epoch 0031, iter [04100, 05004], lr: 0.078338, loss: 1.8409
2022-08-26 20:51:11 - train: epoch 0031, iter [04200, 05004], lr: 0.078313, loss: 1.9642
2022-08-26 20:51:44 - train: epoch 0031, iter [04300, 05004], lr: 0.078287, loss: 1.9111
2022-08-26 20:52:18 - train: epoch 0031, iter [04400, 05004], lr: 0.078261, loss: 2.0094
2022-08-26 20:52:52 - train: epoch 0031, iter [04500, 05004], lr: 0.078235, loss: 2.0903
2022-08-26 20:53:25 - train: epoch 0031, iter [04600, 05004], lr: 0.078209, loss: 1.8763
2022-08-26 20:54:00 - train: epoch 0031, iter [04700, 05004], lr: 0.078183, loss: 1.9176
2022-08-26 20:54:33 - train: epoch 0031, iter [04800, 05004], lr: 0.078157, loss: 1.8062
2022-08-26 20:55:07 - train: epoch 0031, iter [04900, 05004], lr: 0.078131, loss: 1.6948
2022-08-26 20:55:40 - train: epoch 0031, iter [05000, 05004], lr: 0.078105, loss: 1.7977
2022-08-26 20:55:41 - train: epoch 031, train_loss: 1.8545
2022-08-26 20:56:56 - eval: epoch: 031, acc1: 57.398%, acc5: 81.200%, test_loss: 1.8570, per_image_load_time: 2.375ms, per_image_inference_time: 0.472ms
2022-08-26 20:56:56 - until epoch: 031, best_acc1: 60.388%
2022-08-26 20:56:56 - epoch 032 lr: 0.078104
2022-08-26 20:57:35 - train: epoch 0032, iter [00100, 05004], lr: 0.078078, loss: 1.7368
2022-08-26 20:58:09 - train: epoch 0032, iter [00200, 05004], lr: 0.078052, loss: 1.8969
2022-08-26 20:58:43 - train: epoch 0032, iter [00300, 05004], lr: 0.078026, loss: 1.8766
2022-08-26 20:59:16 - train: epoch 0032, iter [00400, 05004], lr: 0.078000, loss: 2.0449
2022-08-26 20:59:50 - train: epoch 0032, iter [00500, 05004], lr: 0.077974, loss: 1.8551
2022-08-26 21:00:24 - train: epoch 0032, iter [00600, 05004], lr: 0.077948, loss: 1.8126
2022-08-26 21:00:57 - train: epoch 0032, iter [00700, 05004], lr: 0.077922, loss: 1.8551
2022-08-26 21:01:31 - train: epoch 0032, iter [00800, 05004], lr: 0.077896, loss: 1.7931
2022-08-26 21:02:05 - train: epoch 0032, iter [00900, 05004], lr: 0.077870, loss: 1.7531
2022-08-26 21:02:38 - train: epoch 0032, iter [01000, 05004], lr: 0.077844, loss: 2.0161
2022-08-26 21:03:12 - train: epoch 0032, iter [01100, 05004], lr: 0.077818, loss: 1.9052
2022-08-26 21:03:46 - train: epoch 0032, iter [01200, 05004], lr: 0.077792, loss: 1.8363
2022-08-26 21:04:19 - train: epoch 0032, iter [01300, 05004], lr: 0.077766, loss: 1.8086
2022-08-26 21:04:54 - train: epoch 0032, iter [01400, 05004], lr: 0.077740, loss: 1.9881
2022-08-26 21:05:27 - train: epoch 0032, iter [01500, 05004], lr: 0.077713, loss: 1.9029
2022-08-26 21:06:01 - train: epoch 0032, iter [01600, 05004], lr: 0.077687, loss: 1.9376
2022-08-26 21:06:35 - train: epoch 0032, iter [01700, 05004], lr: 0.077661, loss: 2.0489
2022-08-26 21:07:08 - train: epoch 0032, iter [01800, 05004], lr: 0.077635, loss: 2.1103
2022-08-26 21:07:42 - train: epoch 0032, iter [01900, 05004], lr: 0.077609, loss: 1.7484
2022-08-26 21:08:16 - train: epoch 0032, iter [02000, 05004], lr: 0.077583, loss: 1.8651
2022-08-26 21:08:50 - train: epoch 0032, iter [02100, 05004], lr: 0.077557, loss: 1.8119
2022-08-26 21:09:22 - train: epoch 0032, iter [02200, 05004], lr: 0.077530, loss: 1.7002
2022-08-26 21:09:56 - train: epoch 0032, iter [02300, 05004], lr: 0.077504, loss: 1.9001
2022-08-26 21:10:30 - train: epoch 0032, iter [02400, 05004], lr: 0.077478, loss: 1.7344
2022-08-26 21:11:04 - train: epoch 0032, iter [02500, 05004], lr: 0.077452, loss: 2.0193
2022-08-26 21:11:37 - train: epoch 0032, iter [02600, 05004], lr: 0.077425, loss: 1.5603
2022-08-26 21:12:11 - train: epoch 0032, iter [02700, 05004], lr: 0.077399, loss: 1.7828
2022-08-26 21:12:45 - train: epoch 0032, iter [02800, 05004], lr: 0.077373, loss: 2.0687
2022-08-26 21:13:19 - train: epoch 0032, iter [02900, 05004], lr: 0.077347, loss: 1.6761
2022-08-26 21:13:52 - train: epoch 0032, iter [03000, 05004], lr: 0.077320, loss: 1.6054
2022-08-26 21:14:25 - train: epoch 0032, iter [03100, 05004], lr: 0.077294, loss: 1.9094
2022-08-26 21:14:59 - train: epoch 0032, iter [03200, 05004], lr: 0.077268, loss: 1.8814
2022-08-26 21:15:33 - train: epoch 0032, iter [03300, 05004], lr: 0.077241, loss: 1.6672
2022-08-26 21:16:07 - train: epoch 0032, iter [03400, 05004], lr: 0.077215, loss: 1.7300
2022-08-26 21:16:40 - train: epoch 0032, iter [03500, 05004], lr: 0.077189, loss: 1.7785
2022-08-26 21:17:13 - train: epoch 0032, iter [03600, 05004], lr: 0.077162, loss: 1.8742
2022-08-26 21:17:48 - train: epoch 0032, iter [03700, 05004], lr: 0.077136, loss: 1.9519
2022-08-26 21:18:21 - train: epoch 0032, iter [03800, 05004], lr: 0.077110, loss: 1.7486
2022-08-26 21:18:55 - train: epoch 0032, iter [03900, 05004], lr: 0.077083, loss: 1.9175
2022-08-26 21:19:28 - train: epoch 0032, iter [04000, 05004], lr: 0.077057, loss: 1.7070
2022-08-26 21:20:02 - train: epoch 0032, iter [04100, 05004], lr: 0.077031, loss: 1.7457
2022-08-26 21:20:35 - train: epoch 0032, iter [04200, 05004], lr: 0.077004, loss: 1.7171
2022-08-26 21:21:09 - train: epoch 0032, iter [04300, 05004], lr: 0.076978, loss: 2.0645
2022-08-26 21:21:42 - train: epoch 0032, iter [04400, 05004], lr: 0.076951, loss: 1.8200
2022-08-26 21:22:16 - train: epoch 0032, iter [04500, 05004], lr: 0.076925, loss: 1.9941
2022-08-26 21:22:49 - train: epoch 0032, iter [04600, 05004], lr: 0.076898, loss: 1.9861
2022-08-26 21:23:23 - train: epoch 0032, iter [04700, 05004], lr: 0.076872, loss: 1.9777
2022-08-26 21:23:57 - train: epoch 0032, iter [04800, 05004], lr: 0.076845, loss: 1.7248
2022-08-26 21:24:30 - train: epoch 0032, iter [04900, 05004], lr: 0.076819, loss: 2.0640
2022-08-26 21:25:03 - train: epoch 0032, iter [05000, 05004], lr: 0.076792, loss: 1.8007
2022-08-26 21:25:04 - train: epoch 032, train_loss: 1.8431
2022-08-26 21:26:20 - eval: epoch: 032, acc1: 59.616%, acc5: 83.212%, test_loss: 1.6992, per_image_load_time: 2.332ms, per_image_inference_time: 0.485ms
2022-08-26 21:26:20 - until epoch: 032, best_acc1: 60.388%
2022-08-26 21:26:20 - epoch 033 lr: 0.076791
2022-08-26 21:26:59 - train: epoch 0033, iter [00100, 05004], lr: 0.076765, loss: 1.6717
2022-08-26 21:27:33 - train: epoch 0033, iter [00200, 05004], lr: 0.076738, loss: 1.9725
2022-08-26 21:28:08 - train: epoch 0033, iter [00300, 05004], lr: 0.076712, loss: 1.6666
2022-08-26 21:28:41 - train: epoch 0033, iter [00400, 05004], lr: 0.076685, loss: 1.6357
2022-08-26 21:29:15 - train: epoch 0033, iter [00500, 05004], lr: 0.076659, loss: 1.8844
2022-08-26 21:29:49 - train: epoch 0033, iter [00600, 05004], lr: 0.076632, loss: 1.7685
2022-08-26 21:30:22 - train: epoch 0033, iter [00700, 05004], lr: 0.076606, loss: 1.8357
2022-08-26 21:30:55 - train: epoch 0033, iter [00800, 05004], lr: 0.076579, loss: 1.9374
2022-08-26 21:31:30 - train: epoch 0033, iter [00900, 05004], lr: 0.076552, loss: 1.8806
2022-08-26 21:32:04 - train: epoch 0033, iter [01000, 05004], lr: 0.076526, loss: 1.8053
2022-08-26 21:32:37 - train: epoch 0033, iter [01100, 05004], lr: 0.076499, loss: 1.8016
2022-08-26 21:33:11 - train: epoch 0033, iter [01200, 05004], lr: 0.076473, loss: 1.7916
2022-08-26 21:33:45 - train: epoch 0033, iter [01300, 05004], lr: 0.076446, loss: 1.8044
2022-08-26 21:34:19 - train: epoch 0033, iter [01400, 05004], lr: 0.076419, loss: 1.9526
2022-08-26 21:34:52 - train: epoch 0033, iter [01500, 05004], lr: 0.076393, loss: 2.0696
2022-08-26 21:35:25 - train: epoch 0033, iter [01600, 05004], lr: 0.076366, loss: 2.0048
2022-08-26 21:35:59 - train: epoch 0033, iter [01700, 05004], lr: 0.076339, loss: 1.7321
2022-08-26 21:36:33 - train: epoch 0033, iter [01800, 05004], lr: 0.076313, loss: 2.0878
2022-08-26 21:37:07 - train: epoch 0033, iter [01900, 05004], lr: 0.076286, loss: 1.9456
2022-08-26 21:37:41 - train: epoch 0033, iter [02000, 05004], lr: 0.076259, loss: 1.7324
2022-08-26 21:38:13 - train: epoch 0033, iter [02100, 05004], lr: 0.076232, loss: 1.7552
2022-08-26 21:38:46 - train: epoch 0033, iter [02200, 05004], lr: 0.076206, loss: 1.9493
2022-08-26 21:39:19 - train: epoch 0033, iter [02300, 05004], lr: 0.076179, loss: 1.6892
2022-08-26 21:39:52 - train: epoch 0033, iter [02400, 05004], lr: 0.076152, loss: 2.0265
2022-08-26 21:40:25 - train: epoch 0033, iter [02500, 05004], lr: 0.076125, loss: 1.8833
2022-08-26 21:40:58 - train: epoch 0033, iter [02600, 05004], lr: 0.076099, loss: 1.6877
2022-08-26 21:41:33 - train: epoch 0033, iter [02700, 05004], lr: 0.076072, loss: 1.9242
2022-08-26 21:42:19 - train: epoch 0033, iter [02800, 05004], lr: 0.076045, loss: 1.8527
2022-08-26 21:43:28 - train: epoch 0033, iter [02900, 05004], lr: 0.076018, loss: 2.0036
2022-08-26 21:44:53 - train: epoch 0033, iter [03000, 05004], lr: 0.075992, loss: 1.8428
2022-08-26 21:46:16 - train: epoch 0033, iter [03100, 05004], lr: 0.075965, loss: 1.8756
2022-08-26 21:47:46 - train: epoch 0033, iter [03200, 05004], lr: 0.075938, loss: 1.7649
2022-08-26 21:49:22 - train: epoch 0033, iter [03300, 05004], lr: 0.075911, loss: 1.9950
2022-08-26 21:50:41 - train: epoch 0033, iter [03400, 05004], lr: 0.075884, loss: 1.7642
2022-08-26 21:52:03 - train: epoch 0033, iter [03500, 05004], lr: 0.075857, loss: 1.9056
2022-08-26 21:53:29 - train: epoch 0033, iter [03600, 05004], lr: 0.075830, loss: 2.0869
2022-08-26 21:55:04 - train: epoch 0033, iter [03700, 05004], lr: 0.075804, loss: 1.8826
2022-08-26 21:56:38 - train: epoch 0033, iter [03800, 05004], lr: 0.075777, loss: 1.7881
2022-08-26 21:58:02 - train: epoch 0033, iter [03900, 05004], lr: 0.075750, loss: 1.9158
2022-08-26 21:59:27 - train: epoch 0033, iter [04000, 05004], lr: 0.075723, loss: 1.8628
2022-08-26 22:00:50 - train: epoch 0033, iter [04100, 05004], lr: 0.075696, loss: 1.9482
2022-08-26 22:02:26 - train: epoch 0033, iter [04200, 05004], lr: 0.075669, loss: 1.8928
2022-08-26 22:03:45 - train: epoch 0033, iter [04300, 05004], lr: 0.075642, loss: 1.9877
2022-08-26 22:05:16 - train: epoch 0033, iter [04400, 05004], lr: 0.075615, loss: 1.8711
2022-08-26 22:06:38 - train: epoch 0033, iter [04500, 05004], lr: 0.075588, loss: 2.1962
2022-08-26 22:08:04 - train: epoch 0033, iter [04600, 05004], lr: 0.075561, loss: 1.8335
2022-08-26 22:09:35 - train: epoch 0033, iter [04700, 05004], lr: 0.075534, loss: 1.7026
2022-08-26 22:11:01 - train: epoch 0033, iter [04800, 05004], lr: 0.075507, loss: 2.2856
2022-08-26 22:12:35 - train: epoch 0033, iter [04900, 05004], lr: 0.075480, loss: 1.6616
2022-08-26 22:14:06 - train: epoch 0033, iter [05000, 05004], lr: 0.075453, loss: 1.7407
2022-08-26 22:14:09 - train: epoch 033, train_loss: 1.8296
2022-08-26 22:17:45 - eval: epoch: 033, acc1: 58.270%, acc5: 81.968%, test_loss: 1.7898, per_image_load_time: 7.604ms, per_image_inference_time: 0.763ms
2022-08-26 22:17:46 - until epoch: 033, best_acc1: 60.388%
2022-08-26 22:17:46 - epoch 034 lr: 0.075452
2022-08-26 22:19:17 - train: epoch 0034, iter [00100, 05004], lr: 0.075425, loss: 1.7752
2022-08-26 22:20:42 - train: epoch 0034, iter [00200, 05004], lr: 0.075398, loss: 1.6687
2022-08-26 22:22:14 - train: epoch 0034, iter [00300, 05004], lr: 0.075371, loss: 1.7380
2022-08-26 22:23:39 - train: epoch 0034, iter [00400, 05004], lr: 0.075344, loss: 1.5810
2022-08-26 22:25:18 - train: epoch 0034, iter [00500, 05004], lr: 0.075317, loss: 1.7022
2022-08-26 22:26:44 - train: epoch 0034, iter [00600, 05004], lr: 0.075290, loss: 2.0096
2022-08-26 22:28:11 - train: epoch 0034, iter [00700, 05004], lr: 0.075263, loss: 1.8507
2022-08-26 22:29:52 - train: epoch 0034, iter [00800, 05004], lr: 0.075236, loss: 1.7790
2022-08-26 22:31:23 - train: epoch 0034, iter [00900, 05004], lr: 0.075208, loss: 1.7395
2022-08-26 22:32:46 - train: epoch 0034, iter [01000, 05004], lr: 0.075181, loss: 1.7385
2022-08-26 22:34:13 - train: epoch 0034, iter [01100, 05004], lr: 0.075154, loss: 1.8670
2022-08-26 22:35:47 - train: epoch 0034, iter [01200, 05004], lr: 0.075127, loss: 1.7423
2022-08-26 22:37:17 - train: epoch 0034, iter [01300, 05004], lr: 0.075100, loss: 1.7318
2022-08-26 22:38:48 - train: epoch 0034, iter [01400, 05004], lr: 0.075073, loss: 1.8219
2022-08-26 22:40:15 - train: epoch 0034, iter [01500, 05004], lr: 0.075046, loss: 1.7006
2022-08-26 22:41:52 - train: epoch 0034, iter [01600, 05004], lr: 0.075018, loss: 1.6875
2022-08-26 22:43:18 - train: epoch 0034, iter [01700, 05004], lr: 0.074991, loss: 1.8010
2022-08-26 22:44:43 - train: epoch 0034, iter [01800, 05004], lr: 0.074964, loss: 1.9540
2022-08-26 22:46:11 - train: epoch 0034, iter [01900, 05004], lr: 0.074937, loss: 1.8970
2022-08-26 22:47:43 - train: epoch 0034, iter [02000, 05004], lr: 0.074910, loss: 1.9071
2022-08-26 22:49:07 - train: epoch 0034, iter [02100, 05004], lr: 0.074882, loss: 2.0949
2022-08-26 22:50:35 - train: epoch 0034, iter [02200, 05004], lr: 0.074855, loss: 1.6970
2022-08-26 22:51:57 - train: epoch 0034, iter [02300, 05004], lr: 0.074828, loss: 1.8713
2022-08-26 22:53:33 - train: epoch 0034, iter [02400, 05004], lr: 0.074801, loss: 1.7308
2022-08-26 22:55:10 - train: epoch 0034, iter [02500, 05004], lr: 0.074773, loss: 1.8614
2022-08-26 22:56:34 - train: epoch 0034, iter [02600, 05004], lr: 0.074746, loss: 1.8429
2022-08-26 22:58:13 - train: epoch 0034, iter [02700, 05004], lr: 0.074719, loss: 1.7430
2022-08-26 22:59:46 - train: epoch 0034, iter [02800, 05004], lr: 0.074692, loss: 1.5951
2022-08-26 23:01:15 - train: epoch 0034, iter [02900, 05004], lr: 0.074664, loss: 1.5931
2022-08-26 23:02:48 - train: epoch 0034, iter [03000, 05004], lr: 0.074637, loss: 1.6001
2022-08-26 23:04:04 - train: epoch 0034, iter [03100, 05004], lr: 0.074610, loss: 1.6138
2022-08-26 23:05:36 - train: epoch 0034, iter [03200, 05004], lr: 0.074582, loss: 1.8332
2022-08-26 23:06:59 - train: epoch 0034, iter [03300, 05004], lr: 0.074555, loss: 1.7032
2022-08-26 23:08:34 - train: epoch 0034, iter [03400, 05004], lr: 0.074528, loss: 1.8934
2022-08-26 23:10:00 - train: epoch 0034, iter [03500, 05004], lr: 0.074500, loss: 1.7065
2022-08-26 23:11:32 - train: epoch 0034, iter [03600, 05004], lr: 0.074473, loss: 1.7329
2022-08-26 23:12:52 - train: epoch 0034, iter [03700, 05004], lr: 0.074446, loss: 1.7046
2022-08-26 23:14:20 - train: epoch 0034, iter [03800, 05004], lr: 0.074418, loss: 1.7305
2022-08-26 23:15:50 - train: epoch 0034, iter [03900, 05004], lr: 0.074391, loss: 1.8444
2022-08-26 23:17:23 - train: epoch 0034, iter [04000, 05004], lr: 0.074363, loss: 1.6521
2022-08-26 23:18:56 - train: epoch 0034, iter [04100, 05004], lr: 0.074336, loss: 1.7490
2022-08-26 23:20:29 - train: epoch 0034, iter [04200, 05004], lr: 0.074309, loss: 1.7548
2022-08-26 23:22:19 - train: epoch 0034, iter [04300, 05004], lr: 0.074281, loss: 1.8658
2022-08-26 23:23:44 - train: epoch 0034, iter [04400, 05004], lr: 0.074254, loss: 1.8660
2022-08-26 23:24:58 - train: epoch 0034, iter [04500, 05004], lr: 0.074226, loss: 1.9044
2022-08-26 23:26:24 - train: epoch 0034, iter [04600, 05004], lr: 0.074199, loss: 1.9520
2022-08-26 23:28:03 - train: epoch 0034, iter [04700, 05004], lr: 0.074171, loss: 1.7601
2022-08-26 23:29:31 - train: epoch 0034, iter [04800, 05004], lr: 0.074144, loss: 1.8616
2022-08-26 23:30:54 - train: epoch 0034, iter [04900, 05004], lr: 0.074116, loss: 1.7958
2022-08-26 23:32:30 - train: epoch 0034, iter [05000, 05004], lr: 0.074089, loss: 1.7910
2022-08-26 23:32:32 - train: epoch 034, train_loss: 1.8197
2022-08-26 23:35:47 - eval: epoch: 034, acc1: 55.824%, acc5: 80.356%, test_loss: 1.8951, per_image_load_time: 6.146ms, per_image_inference_time: 0.740ms
2022-08-26 23:35:48 - until epoch: 034, best_acc1: 60.388%
2022-08-26 23:35:48 - epoch 035 lr: 0.074087
2022-08-26 23:37:25 - train: epoch 0035, iter [00100, 05004], lr: 0.074060, loss: 1.6781
2022-08-26 23:38:54 - train: epoch 0035, iter [00200, 05004], lr: 0.074033, loss: 1.5377
2022-08-26 23:40:25 - train: epoch 0035, iter [00300, 05004], lr: 0.074005, loss: 1.9859
2022-08-26 23:41:51 - train: epoch 0035, iter [00400, 05004], lr: 0.073978, loss: 1.6964
2022-08-26 23:43:19 - train: epoch 0035, iter [00500, 05004], lr: 0.073950, loss: 1.7558
2022-08-26 23:44:49 - train: epoch 0035, iter [00600, 05004], lr: 0.073922, loss: 1.8131
2022-08-26 23:46:19 - train: epoch 0035, iter [00700, 05004], lr: 0.073895, loss: 1.7435
2022-08-26 23:48:00 - train: epoch 0035, iter [00800, 05004], lr: 0.073867, loss: 1.6979
2022-08-26 23:49:28 - train: epoch 0035, iter [00900, 05004], lr: 0.073840, loss: 1.9655
2022-08-26 23:50:59 - train: epoch 0035, iter [01000, 05004], lr: 0.073812, loss: 1.8440
2022-08-26 23:52:31 - train: epoch 0035, iter [01100, 05004], lr: 0.073785, loss: 2.0177
2022-08-26 23:54:02 - train: epoch 0035, iter [01200, 05004], lr: 0.073757, loss: 1.8686
2022-08-26 23:55:26 - train: epoch 0035, iter [01300, 05004], lr: 0.073729, loss: 1.8105
2022-08-26 23:56:58 - train: epoch 0035, iter [01400, 05004], lr: 0.073702, loss: 1.8986
2022-08-26 23:58:25 - train: epoch 0035, iter [01500, 05004], lr: 0.073674, loss: 1.7992
2022-08-26 23:59:46 - train: epoch 0035, iter [01600, 05004], lr: 0.073646, loss: 1.7802
2022-08-27 00:01:12 - train: epoch 0035, iter [01700, 05004], lr: 0.073619, loss: 1.5685
2022-08-27 00:02:48 - train: epoch 0035, iter [01800, 05004], lr: 0.073591, loss: 1.8521
2022-08-27 00:04:22 - train: epoch 0035, iter [01900, 05004], lr: 0.073563, loss: 1.8363
2022-08-27 00:05:51 - train: epoch 0035, iter [02000, 05004], lr: 0.073536, loss: 1.8123
2022-08-27 00:07:17 - train: epoch 0035, iter [02100, 05004], lr: 0.073508, loss: 1.6615
2022-08-27 00:09:00 - train: epoch 0035, iter [02200, 05004], lr: 0.073480, loss: 1.8898
2022-08-27 00:10:32 - train: epoch 0035, iter [02300, 05004], lr: 0.073453, loss: 1.8435
2022-08-27 00:11:49 - train: epoch 0035, iter [02400, 05004], lr: 0.073425, loss: 1.8882
2022-08-27 00:13:18 - train: epoch 0035, iter [02500, 05004], lr: 0.073397, loss: 1.6861
2022-08-27 00:14:54 - train: epoch 0035, iter [02600, 05004], lr: 0.073369, loss: 2.0102
2022-08-27 00:16:18 - train: epoch 0035, iter [02700, 05004], lr: 0.073342, loss: 1.9146
2022-08-27 00:17:45 - train: epoch 0035, iter [02800, 05004], lr: 0.073314, loss: 1.7547
2022-08-27 00:19:20 - train: epoch 0035, iter [02900, 05004], lr: 0.073286, loss: 1.7771
2022-08-27 00:20:50 - train: epoch 0035, iter [03000, 05004], lr: 0.073258, loss: 1.8390
2022-08-27 00:22:12 - train: epoch 0035, iter [03100, 05004], lr: 0.073230, loss: 1.8217
2022-08-27 00:23:47 - train: epoch 0035, iter [03200, 05004], lr: 0.073203, loss: 1.6941
2022-08-27 00:25:13 - train: epoch 0035, iter [03300, 05004], lr: 0.073175, loss: 1.7045
2022-08-27 00:26:37 - train: epoch 0035, iter [03400, 05004], lr: 0.073147, loss: 1.7175
2022-08-27 00:28:02 - train: epoch 0035, iter [03500, 05004], lr: 0.073119, loss: 1.5468
2022-08-27 00:29:35 - train: epoch 0035, iter [03600, 05004], lr: 0.073091, loss: 1.9745
2022-08-27 00:31:05 - train: epoch 0035, iter [03700, 05004], lr: 0.073063, loss: 1.6695
2022-08-27 00:32:40 - train: epoch 0035, iter [03800, 05004], lr: 0.073036, loss: 1.8742
2022-08-27 00:34:00 - train: epoch 0035, iter [03900, 05004], lr: 0.073008, loss: 2.0444
2022-08-27 00:35:47 - train: epoch 0035, iter [04000, 05004], lr: 0.072980, loss: 1.5432
2022-08-27 00:37:11 - train: epoch 0035, iter [04100, 05004], lr: 0.072952, loss: 2.0665
2022-08-27 00:38:42 - train: epoch 0035, iter [04200, 05004], lr: 0.072924, loss: 1.7314
2022-08-27 00:40:09 - train: epoch 0035, iter [04300, 05004], lr: 0.072896, loss: 1.7908
2022-08-27 00:41:34 - train: epoch 0035, iter [04400, 05004], lr: 0.072868, loss: 1.9142
2022-08-27 00:43:06 - train: epoch 0035, iter [04500, 05004], lr: 0.072840, loss: 1.9140
2022-08-27 00:44:34 - train: epoch 0035, iter [04600, 05004], lr: 0.072812, loss: 1.7453
2022-08-27 00:46:14 - train: epoch 0035, iter [04700, 05004], lr: 0.072785, loss: 2.1279
2022-08-27 00:47:47 - train: epoch 0035, iter [04800, 05004], lr: 0.072757, loss: 1.9662
2022-08-27 00:49:15 - train: epoch 0035, iter [04900, 05004], lr: 0.072729, loss: 1.8388
2022-08-27 00:50:42 - train: epoch 0035, iter [05000, 05004], lr: 0.072701, loss: 1.7970
2022-08-27 00:50:44 - train: epoch 035, train_loss: 1.8082
2022-08-27 00:54:26 - eval: epoch: 035, acc1: 57.282%, acc5: 80.940%, test_loss: 1.8342, per_image_load_time: 6.805ms, per_image_inference_time: 0.763ms
2022-08-27 00:54:26 - until epoch: 035, best_acc1: 60.388%
2022-08-27 00:54:26 - epoch 036 lr: 0.072699
2022-08-27 00:56:05 - train: epoch 0036, iter [00100, 05004], lr: 0.072672, loss: 1.8364
2022-08-27 00:57:34 - train: epoch 0036, iter [00200, 05004], lr: 0.072644, loss: 1.4930
2022-08-27 00:59:04 - train: epoch 0036, iter [00300, 05004], lr: 0.072616, loss: 1.6507
2022-08-27 01:00:28 - train: epoch 0036, iter [00400, 05004], lr: 0.072588, loss: 1.7765
2022-08-27 01:01:53 - train: epoch 0036, iter [00500, 05004], lr: 0.072560, loss: 1.8050
2022-08-27 01:03:21 - train: epoch 0036, iter [00600, 05004], lr: 0.072532, loss: 1.7343
2022-08-27 01:04:45 - train: epoch 0036, iter [00700, 05004], lr: 0.072504, loss: 1.5940
2022-08-27 01:06:25 - train: epoch 0036, iter [00800, 05004], lr: 0.072475, loss: 1.5909
2022-08-27 01:08:04 - train: epoch 0036, iter [00900, 05004], lr: 0.072447, loss: 1.8233
2022-08-27 01:09:26 - train: epoch 0036, iter [01000, 05004], lr: 0.072419, loss: 1.7313
2022-08-27 01:10:52 - train: epoch 0036, iter [01100, 05004], lr: 0.072391, loss: 1.8365
2022-08-27 01:12:23 - train: epoch 0036, iter [01200, 05004], lr: 0.072363, loss: 1.7643
2022-08-27 01:13:54 - train: epoch 0036, iter [01300, 05004], lr: 0.072335, loss: 1.7434
2022-08-27 01:15:20 - train: epoch 0036, iter [01400, 05004], lr: 0.072307, loss: 1.9173
2022-08-27 01:16:47 - train: epoch 0036, iter [01500, 05004], lr: 0.072279, loss: 1.9525
2022-08-27 01:18:18 - train: epoch 0036, iter [01600, 05004], lr: 0.072251, loss: 1.8960
2022-08-27 01:19:40 - train: epoch 0036, iter [01700, 05004], lr: 0.072223, loss: 1.7610
2022-08-27 01:21:16 - train: epoch 0036, iter [01800, 05004], lr: 0.072195, loss: 1.6771
2022-08-27 01:22:36 - train: epoch 0036, iter [01900, 05004], lr: 0.072167, loss: 1.7454
2022-08-27 01:24:16 - train: epoch 0036, iter [02000, 05004], lr: 0.072138, loss: 1.7385
2022-08-27 01:25:35 - train: epoch 0036, iter [02100, 05004], lr: 0.072110, loss: 1.7819
2022-08-27 01:27:04 - train: epoch 0036, iter [02200, 05004], lr: 0.072082, loss: 1.7581
2022-08-27 01:28:38 - train: epoch 0036, iter [02300, 05004], lr: 0.072054, loss: 1.9107
2022-08-27 01:30:09 - train: epoch 0036, iter [02400, 05004], lr: 0.072026, loss: 1.8886
2022-08-27 01:31:40 - train: epoch 0036, iter [02500, 05004], lr: 0.071998, loss: 1.6288
2022-08-27 01:33:09 - train: epoch 0036, iter [02600, 05004], lr: 0.071969, loss: 1.9906
2022-08-27 01:34:26 - train: epoch 0036, iter [02700, 05004], lr: 0.071941, loss: 1.6348
2022-08-27 01:35:53 - train: epoch 0036, iter [02800, 05004], lr: 0.071913, loss: 1.7398
2022-08-27 01:37:19 - train: epoch 0036, iter [02900, 05004], lr: 0.071885, loss: 1.5642
2022-08-27 01:38:53 - train: epoch 0036, iter [03000, 05004], lr: 0.071856, loss: 2.0566
2022-08-27 01:40:27 - train: epoch 0036, iter [03100, 05004], lr: 0.071828, loss: 1.8113
2022-08-27 01:41:54 - train: epoch 0036, iter [03200, 05004], lr: 0.071800, loss: 1.7742
2022-08-27 01:43:21 - train: epoch 0036, iter [03300, 05004], lr: 0.071772, loss: 1.6857
2022-08-27 01:44:38 - train: epoch 0036, iter [03400, 05004], lr: 0.071743, loss: 1.6374
2022-08-27 01:46:02 - train: epoch 0036, iter [03500, 05004], lr: 0.071715, loss: 1.8089
2022-08-27 01:47:34 - train: epoch 0036, iter [03600, 05004], lr: 0.071687, loss: 1.9447
2022-08-27 01:49:14 - train: epoch 0036, iter [03700, 05004], lr: 0.071659, loss: 1.8540
2022-08-27 01:50:41 - train: epoch 0036, iter [03800, 05004], lr: 0.071630, loss: 1.6790
2022-08-27 01:52:18 - train: epoch 0036, iter [03900, 05004], lr: 0.071602, loss: 1.7407
2022-08-27 01:53:49 - train: epoch 0036, iter [04000, 05004], lr: 0.071574, loss: 1.9013
2022-08-27 01:55:18 - train: epoch 0036, iter [04100, 05004], lr: 0.071545, loss: 1.6888
2022-08-27 01:56:44 - train: epoch 0036, iter [04200, 05004], lr: 0.071517, loss: 1.7656
2022-08-27 01:58:17 - train: epoch 0036, iter [04300, 05004], lr: 0.071489, loss: 1.8917
2022-08-27 01:59:36 - train: epoch 0036, iter [04400, 05004], lr: 0.071460, loss: 1.7546
2022-08-27 02:00:59 - train: epoch 0036, iter [04500, 05004], lr: 0.071432, loss: 1.5328
2022-08-27 02:02:30 - train: epoch 0036, iter [04600, 05004], lr: 0.071404, loss: 1.6942
2022-08-27 02:04:01 - train: epoch 0036, iter [04700, 05004], lr: 0.071375, loss: 1.7324
2022-08-27 02:05:39 - train: epoch 0036, iter [04800, 05004], lr: 0.071347, loss: 1.7366
2022-08-27 02:07:05 - train: epoch 0036, iter [04900, 05004], lr: 0.071318, loss: 1.7744
2022-08-27 02:08:30 - train: epoch 0036, iter [05000, 05004], lr: 0.071290, loss: 1.7276
2022-08-27 02:08:32 - train: epoch 036, train_loss: 1.7951
2022-08-27 02:12:05 - eval: epoch: 036, acc1: 57.980%, acc5: 81.784%, test_loss: 1.7848, per_image_load_time: 7.449ms, per_image_inference_time: 0.757ms
2022-08-27 02:12:06 - until epoch: 036, best_acc1: 60.388%
2022-08-27 02:12:06 - epoch 037 lr: 0.071289
2022-08-27 02:13:45 - train: epoch 0037, iter [00100, 05004], lr: 0.071261, loss: 1.5552
2022-08-27 02:15:23 - train: epoch 0037, iter [00200, 05004], lr: 0.071232, loss: 1.4568
2022-08-27 02:16:46 - train: epoch 0037, iter [00300, 05004], lr: 0.071204, loss: 1.7603
2022-08-27 02:18:02 - train: epoch 0037, iter [00400, 05004], lr: 0.071175, loss: 1.8112
2022-08-27 02:19:31 - train: epoch 0037, iter [00500, 05004], lr: 0.071147, loss: 1.8070
2022-08-27 02:21:14 - train: epoch 0037, iter [00600, 05004], lr: 0.071118, loss: 1.8446
2022-08-27 02:22:46 - train: epoch 0037, iter [00700, 05004], lr: 0.071090, loss: 1.9132
2022-08-27 02:24:24 - train: epoch 0037, iter [00800, 05004], lr: 0.071061, loss: 1.7606
2022-08-27 02:26:08 - train: epoch 0037, iter [00900, 05004], lr: 0.071033, loss: 2.1157
2022-08-27 02:27:42 - train: epoch 0037, iter [01000, 05004], lr: 0.071005, loss: 1.6990
2022-08-27 02:29:18 - train: epoch 0037, iter [01100, 05004], lr: 0.070976, loss: 1.8340
2022-08-27 02:31:01 - train: epoch 0037, iter [01200, 05004], lr: 0.070948, loss: 1.8049
2022-08-27 02:32:44 - train: epoch 0037, iter [01300, 05004], lr: 0.070919, loss: 1.7483
2022-08-27 02:34:25 - train: epoch 0037, iter [01400, 05004], lr: 0.070891, loss: 1.8419
2022-08-27 02:35:58 - train: epoch 0037, iter [01500, 05004], lr: 0.070862, loss: 1.7190
2022-08-27 02:37:41 - train: epoch 0037, iter [01600, 05004], lr: 0.070833, loss: 1.5845
2022-08-27 02:39:12 - train: epoch 0037, iter [01700, 05004], lr: 0.070805, loss: 1.9117
2022-08-27 02:40:41 - train: epoch 0037, iter [01800, 05004], lr: 0.070776, loss: 1.8591
2022-08-27 02:42:19 - train: epoch 0037, iter [01900, 05004], lr: 0.070748, loss: 1.8013
2022-08-27 02:43:49 - train: epoch 0037, iter [02000, 05004], lr: 0.070719, loss: 1.8487
2022-08-27 02:45:27 - train: epoch 0037, iter [02100, 05004], lr: 0.070691, loss: 1.8262
2022-08-27 02:47:03 - train: epoch 0037, iter [02200, 05004], lr: 0.070662, loss: 1.7313
2022-08-27 02:48:35 - train: epoch 0037, iter [02300, 05004], lr: 0.070633, loss: 1.6753
2022-08-27 02:50:01 - train: epoch 0037, iter [02400, 05004], lr: 0.070605, loss: 1.8377
2022-08-27 02:51:30 - train: epoch 0037, iter [02500, 05004], lr: 0.070576, loss: 1.7188
2022-08-27 02:53:10 - train: epoch 0037, iter [02600, 05004], lr: 0.070548, loss: 1.9125
2022-08-27 02:54:53 - train: epoch 0037, iter [02700, 05004], lr: 0.070519, loss: 1.8708
2022-08-27 02:56:39 - train: epoch 0037, iter [02800, 05004], lr: 0.070490, loss: 1.8737
2022-08-27 02:58:21 - train: epoch 0037, iter [02900, 05004], lr: 0.070462, loss: 1.8792
2022-08-27 02:59:52 - train: epoch 0037, iter [03000, 05004], lr: 0.070433, loss: 2.0017
2022-08-27 03:01:20 - train: epoch 0037, iter [03100, 05004], lr: 0.070404, loss: 1.6772
2022-08-27 03:03:06 - train: epoch 0037, iter [03200, 05004], lr: 0.070376, loss: 1.8852
2022-08-27 03:04:42 - train: epoch 0037, iter [03300, 05004], lr: 0.070347, loss: 1.7872
2022-08-27 03:06:17 - train: epoch 0037, iter [03400, 05004], lr: 0.070318, loss: 1.6109
2022-08-27 03:07:51 - train: epoch 0037, iter [03500, 05004], lr: 0.070290, loss: 1.6765
2022-08-27 03:09:32 - train: epoch 0037, iter [03600, 05004], lr: 0.070261, loss: 1.9086
2022-08-27 03:11:05 - train: epoch 0037, iter [03700, 05004], lr: 0.070232, loss: 1.8601
2022-08-27 03:12:38 - train: epoch 0037, iter [03800, 05004], lr: 0.070204, loss: 1.7349
2022-08-27 03:14:25 - train: epoch 0037, iter [03900, 05004], lr: 0.070175, loss: 1.9182
2022-08-27 03:16:09 - train: epoch 0037, iter [04000, 05004], lr: 0.070146, loss: 1.7115
2022-08-27 03:17:31 - train: epoch 0037, iter [04100, 05004], lr: 0.070118, loss: 1.8462
2022-08-27 03:19:08 - train: epoch 0037, iter [04200, 05004], lr: 0.070089, loss: 2.0893
2022-08-27 03:20:46 - train: epoch 0037, iter [04300, 05004], lr: 0.070060, loss: 1.8607
2022-08-27 03:22:23 - train: epoch 0037, iter [04400, 05004], lr: 0.070031, loss: 1.7668
2022-08-27 03:24:07 - train: epoch 0037, iter [04500, 05004], lr: 0.070002, loss: 1.8209
2022-08-27 03:25:45 - train: epoch 0037, iter [04600, 05004], lr: 0.069974, loss: 1.7256
2022-08-27 03:27:33 - train: epoch 0037, iter [04700, 05004], lr: 0.069945, loss: 1.7618
2022-08-27 03:29:02 - train: epoch 0037, iter [04800, 05004], lr: 0.069916, loss: 1.7638
2022-08-27 03:30:35 - train: epoch 0037, iter [04900, 05004], lr: 0.069887, loss: 1.8697
2022-08-27 03:32:04 - train: epoch 0037, iter [05000, 05004], lr: 0.069859, loss: 1.8109
2022-08-27 03:32:06 - train: epoch 037, train_loss: 1.7876
2022-08-27 03:35:46 - eval: epoch: 037, acc1: 60.472%, acc5: 83.714%, test_loss: 1.6518, per_image_load_time: 7.133ms, per_image_inference_time: 0.766ms
2022-08-27 03:35:46 - until epoch: 037, best_acc1: 60.472%
2022-08-27 03:35:46 - epoch 038 lr: 0.069857
2022-08-27 03:37:25 - train: epoch 0038, iter [00100, 05004], lr: 0.069829, loss: 1.7459
2022-08-27 03:39:04 - train: epoch 0038, iter [00200, 05004], lr: 0.069800, loss: 1.5011
2022-08-27 03:40:31 - train: epoch 0038, iter [00300, 05004], lr: 0.069771, loss: 1.4334
2022-08-27 03:42:10 - train: epoch 0038, iter [00400, 05004], lr: 0.069742, loss: 1.6477
2022-08-27 03:43:46 - train: epoch 0038, iter [00500, 05004], lr: 0.069713, loss: 1.5585
2022-08-27 03:45:23 - train: epoch 0038, iter [00600, 05004], lr: 0.069684, loss: 1.7893
2022-08-27 03:46:48 - train: epoch 0038, iter [00700, 05004], lr: 0.069656, loss: 1.5900
2022-08-27 03:48:33 - train: epoch 0038, iter [00800, 05004], lr: 0.069627, loss: 1.5579
2022-08-27 03:50:08 - train: epoch 0038, iter [00900, 05004], lr: 0.069598, loss: 1.7177
2022-08-27 03:51:39 - train: epoch 0038, iter [01000, 05004], lr: 0.069569, loss: 2.0641
2022-08-27 03:53:15 - train: epoch 0038, iter [01100, 05004], lr: 0.069540, loss: 1.7302
2022-08-27 03:54:52 - train: epoch 0038, iter [01200, 05004], lr: 0.069511, loss: 1.8519
2022-08-27 03:56:28 - train: epoch 0038, iter [01300, 05004], lr: 0.069482, loss: 1.8814
2022-08-27 03:58:25 - train: epoch 0038, iter [01400, 05004], lr: 0.069453, loss: 1.7785
2022-08-27 04:00:00 - train: epoch 0038, iter [01500, 05004], lr: 0.069424, loss: 1.8632
2022-08-27 04:01:34 - train: epoch 0038, iter [01600, 05004], lr: 0.069395, loss: 1.8695
2022-08-27 04:03:11 - train: epoch 0038, iter [01700, 05004], lr: 0.069367, loss: 1.9757
2022-08-27 04:04:49 - train: epoch 0038, iter [01800, 05004], lr: 0.069338, loss: 1.9014
2022-08-27 04:06:34 - train: epoch 0038, iter [01900, 05004], lr: 0.069309, loss: 1.8053
2022-08-27 04:08:03 - train: epoch 0038, iter [02000, 05004], lr: 0.069280, loss: 1.6976
2022-08-27 04:09:37 - train: epoch 0038, iter [02100, 05004], lr: 0.069251, loss: 1.8955
2022-08-27 04:11:22 - train: epoch 0038, iter [02200, 05004], lr: 0.069222, loss: 1.6947
2022-08-27 04:12:48 - train: epoch 0038, iter [02300, 05004], lr: 0.069193, loss: 1.9203
2022-08-27 04:14:34 - train: epoch 0038, iter [02400, 05004], lr: 0.069164, loss: 2.0117
2022-08-27 04:16:13 - train: epoch 0038, iter [02500, 05004], lr: 0.069135, loss: 1.7654
2022-08-27 04:17:48 - train: epoch 0038, iter [02600, 05004], lr: 0.069106, loss: 1.8044
2022-08-27 04:19:24 - train: epoch 0038, iter [02700, 05004], lr: 0.069077, loss: 1.9297
2022-08-27 04:21:05 - train: epoch 0038, iter [02800, 05004], lr: 0.069048, loss: 1.9870
2022-08-27 04:22:40 - train: epoch 0038, iter [02900, 05004], lr: 0.069019, loss: 1.8544
2022-08-27 04:24:18 - train: epoch 0038, iter [03000, 05004], lr: 0.068990, loss: 1.7303
2022-08-27 04:25:58 - train: epoch 0038, iter [03100, 05004], lr: 0.068961, loss: 1.7308
2022-08-27 04:27:41 - train: epoch 0038, iter [03200, 05004], lr: 0.068932, loss: 1.5285
2022-08-27 04:29:13 - train: epoch 0038, iter [03300, 05004], lr: 0.068903, loss: 1.4778
2022-08-27 04:30:58 - train: epoch 0038, iter [03400, 05004], lr: 0.068873, loss: 1.6348
2022-08-27 04:32:44 - train: epoch 0038, iter [03500, 05004], lr: 0.068844, loss: 1.7546
2022-08-27 04:34:25 - train: epoch 0038, iter [03600, 05004], lr: 0.068815, loss: 1.8365
2022-08-27 04:36:05 - train: epoch 0038, iter [03700, 05004], lr: 0.068786, loss: 1.4775
2022-08-27 04:37:43 - train: epoch 0038, iter [03800, 05004], lr: 0.068757, loss: 1.8631
2022-08-27 04:39:09 - train: epoch 0038, iter [03900, 05004], lr: 0.068728, loss: 1.8332
2022-08-27 04:40:40 - train: epoch 0038, iter [04000, 05004], lr: 0.068699, loss: 1.8409
2022-08-27 04:42:25 - train: epoch 0038, iter [04100, 05004], lr: 0.068670, loss: 1.6525
2022-08-27 04:44:02 - train: epoch 0038, iter [04200, 05004], lr: 0.068641, loss: 1.5863
2022-08-27 04:45:42 - train: epoch 0038, iter [04300, 05004], lr: 0.068612, loss: 1.9721
2022-08-27 04:47:06 - train: epoch 0038, iter [04400, 05004], lr: 0.068582, loss: 1.9608
2022-08-27 04:48:38 - train: epoch 0038, iter [04500, 05004], lr: 0.068553, loss: 1.8473
2022-08-27 04:50:18 - train: epoch 0038, iter [04600, 05004], lr: 0.068524, loss: 1.9786
2022-08-27 04:51:57 - train: epoch 0038, iter [04700, 05004], lr: 0.068495, loss: 1.6410
2022-08-27 04:53:27 - train: epoch 0038, iter [04800, 05004], lr: 0.068466, loss: 1.9234
2022-08-27 04:55:12 - train: epoch 0038, iter [04900, 05004], lr: 0.068437, loss: 1.8565
2022-08-27 04:56:58 - train: epoch 0038, iter [05000, 05004], lr: 0.068407, loss: 1.5238
2022-08-27 04:57:01 - train: epoch 038, train_loss: 1.7744
2022-08-27 05:00:45 - eval: epoch: 038, acc1: 60.568%, acc5: 83.840%, test_loss: 1.6420, per_image_load_time: 7.765ms, per_image_inference_time: 0.773ms
2022-08-27 05:00:45 - until epoch: 038, best_acc1: 60.568%
2022-08-27 05:00:45 - epoch 039 lr: 0.068406
2022-08-27 05:02:22 - train: epoch 0039, iter [00100, 05004], lr: 0.068377, loss: 1.6823
2022-08-27 05:04:02 - train: epoch 0039, iter [00200, 05004], lr: 0.068348, loss: 1.9137
2022-08-27 05:05:20 - train: epoch 0039, iter [00300, 05004], lr: 0.068319, loss: 1.5220
2022-08-27 05:06:51 - train: epoch 0039, iter [00400, 05004], lr: 0.068289, loss: 1.7541
2022-08-27 05:08:21 - train: epoch 0039, iter [00500, 05004], lr: 0.068260, loss: 1.7591
2022-08-27 05:09:56 - train: epoch 0039, iter [00600, 05004], lr: 0.068231, loss: 1.6915
2022-08-27 05:11:33 - train: epoch 0039, iter [00700, 05004], lr: 0.068202, loss: 1.8568
2022-08-27 05:13:09 - train: epoch 0039, iter [00800, 05004], lr: 0.068173, loss: 1.6605
2022-08-27 05:14:52 - train: epoch 0039, iter [00900, 05004], lr: 0.068143, loss: 1.8075
2022-08-27 05:16:22 - train: epoch 0039, iter [01000, 05004], lr: 0.068114, loss: 1.6833
2022-08-27 05:18:07 - train: epoch 0039, iter [01100, 05004], lr: 0.068085, loss: 1.7670
2022-08-27 05:19:40 - train: epoch 0039, iter [01200, 05004], lr: 0.068055, loss: 1.8729
2022-08-27 05:21:18 - train: epoch 0039, iter [01300, 05004], lr: 0.068026, loss: 2.0591
2022-08-27 05:22:55 - train: epoch 0039, iter [01400, 05004], lr: 0.067997, loss: 1.8435
2022-08-27 05:24:32 - train: epoch 0039, iter [01500, 05004], lr: 0.067968, loss: 1.7244
2022-08-27 05:26:14 - train: epoch 0039, iter [01600, 05004], lr: 0.067938, loss: 1.8435
2022-08-27 05:27:45 - train: epoch 0039, iter [01700, 05004], lr: 0.067909, loss: 1.4677
2022-08-27 05:29:24 - train: epoch 0039, iter [01800, 05004], lr: 0.067880, loss: 1.7033
2022-08-27 05:31:04 - train: epoch 0039, iter [01900, 05004], lr: 0.067850, loss: 1.4574
2022-08-27 05:32:34 - train: epoch 0039, iter [02000, 05004], lr: 0.067821, loss: 1.6642
2022-08-27 05:34:19 - train: epoch 0039, iter [02100, 05004], lr: 0.067792, loss: 1.7959
2022-08-27 05:36:08 - train: epoch 0039, iter [02200, 05004], lr: 0.067762, loss: 1.6619
2022-08-27 05:37:30 - train: epoch 0039, iter [02300, 05004], lr: 0.067733, loss: 2.0339
2022-08-27 05:39:05 - train: epoch 0039, iter [02400, 05004], lr: 0.067704, loss: 1.8913
2022-08-27 05:40:44 - train: epoch 0039, iter [02500, 05004], lr: 0.067674, loss: 1.5973
2022-08-27 05:42:20 - train: epoch 0039, iter [02600, 05004], lr: 0.067645, loss: 1.7393
2022-08-27 05:43:57 - train: epoch 0039, iter [02700, 05004], lr: 0.067616, loss: 1.8830
2022-08-27 05:45:15 - train: epoch 0039, iter [02800, 05004], lr: 0.067586, loss: 1.6327
2022-08-27 05:46:56 - train: epoch 0039, iter [02900, 05004], lr: 0.067557, loss: 1.5870
2022-08-27 05:48:17 - train: epoch 0039, iter [03000, 05004], lr: 0.067527, loss: 1.7659
2022-08-27 05:49:51 - train: epoch 0039, iter [03100, 05004], lr: 0.067498, loss: 1.6784
2022-08-27 05:51:19 - train: epoch 0039, iter [03200, 05004], lr: 0.067469, loss: 1.7782
2022-08-27 05:52:53 - train: epoch 0039, iter [03300, 05004], lr: 0.067439, loss: 1.8143
2022-08-27 05:54:30 - train: epoch 0039, iter [03400, 05004], lr: 0.067410, loss: 1.7892
2022-08-27 05:56:06 - train: epoch 0039, iter [03500, 05004], lr: 0.067380, loss: 2.0234
2022-08-27 05:57:52 - train: epoch 0039, iter [03600, 05004], lr: 0.067351, loss: 1.8980
2022-08-27 05:59:25 - train: epoch 0039, iter [03700, 05004], lr: 0.067321, loss: 1.6909
2022-08-27 06:01:04 - train: epoch 0039, iter [03800, 05004], lr: 0.067292, loss: 1.6266
2022-08-27 06:02:51 - train: epoch 0039, iter [03900, 05004], lr: 0.067263, loss: 1.7654
2022-08-27 06:04:28 - train: epoch 0039, iter [04000, 05004], lr: 0.067233, loss: 1.8237
2022-08-27 06:05:57 - train: epoch 0039, iter [04100, 05004], lr: 0.067204, loss: 1.7709
2022-08-27 06:07:30 - train: epoch 0039, iter [04200, 05004], lr: 0.067174, loss: 1.7776
2022-08-27 06:09:12 - train: epoch 0039, iter [04300, 05004], lr: 0.067145, loss: 1.7465
2022-08-27 06:10:41 - train: epoch 0039, iter [04400, 05004], lr: 0.067115, loss: 1.4583
2022-08-27 06:12:25 - train: epoch 0039, iter [04500, 05004], lr: 0.067086, loss: 1.7095
2022-08-27 06:14:14 - train: epoch 0039, iter [04600, 05004], lr: 0.067056, loss: 1.9295
2022-08-27 06:15:47 - train: epoch 0039, iter [04700, 05004], lr: 0.067027, loss: 1.7840
2022-08-27 06:17:24 - train: epoch 0039, iter [04800, 05004], lr: 0.066997, loss: 1.8723
2022-08-27 06:19:07 - train: epoch 0039, iter [04900, 05004], lr: 0.066968, loss: 1.6313
2022-08-27 06:20:41 - train: epoch 0039, iter [05000, 05004], lr: 0.066938, loss: 1.6592
2022-08-27 06:20:45 - train: epoch 039, train_loss: 1.7623
2022-08-27 06:24:51 - eval: epoch: 039, acc1: 61.902%, acc5: 84.844%, test_loss: 1.5699, per_image_load_time: 8.454ms, per_image_inference_time: 0.793ms
2022-08-27 06:24:52 - until epoch: 039, best_acc1: 61.902%
2022-08-27 06:24:52 - epoch 040 lr: 0.066937
2022-08-27 06:26:40 - train: epoch 0040, iter [00100, 05004], lr: 0.066907, loss: 1.9536
2022-08-27 06:28:12 - train: epoch 0040, iter [00200, 05004], lr: 0.066878, loss: 2.1055
2022-08-27 06:30:02 - train: epoch 0040, iter [00300, 05004], lr: 0.066848, loss: 1.8737
2022-08-27 06:31:38 - train: epoch 0040, iter [00400, 05004], lr: 0.066819, loss: 1.7286
2022-08-27 06:33:14 - train: epoch 0040, iter [00500, 05004], lr: 0.066789, loss: 1.5290
2022-08-27 06:34:56 - train: epoch 0040, iter [00600, 05004], lr: 0.066760, loss: 1.8311
2022-08-27 06:36:40 - train: epoch 0040, iter [00700, 05004], lr: 0.066730, loss: 1.6851
2022-08-27 06:38:19 - train: epoch 0040, iter [00800, 05004], lr: 0.066700, loss: 1.7965
2022-08-27 06:40:01 - train: epoch 0040, iter [00900, 05004], lr: 0.066671, loss: 1.6585
2022-08-27 06:41:41 - train: epoch 0040, iter [01000, 05004], lr: 0.066641, loss: 1.4642
2022-08-27 06:43:21 - train: epoch 0040, iter [01100, 05004], lr: 0.066612, loss: 1.7415
2022-08-27 06:45:02 - train: epoch 0040, iter [01200, 05004], lr: 0.066582, loss: 1.6526
2022-08-27 06:46:43 - train: epoch 0040, iter [01300, 05004], lr: 0.066552, loss: 1.5903
2022-08-27 06:48:14 - train: epoch 0040, iter [01400, 05004], lr: 0.066523, loss: 1.5833
2022-08-27 06:49:59 - train: epoch 0040, iter [01500, 05004], lr: 0.066493, loss: 1.8916
2022-08-27 06:51:23 - train: epoch 0040, iter [01600, 05004], lr: 0.066463, loss: 1.8526
2022-08-27 06:53:08 - train: epoch 0040, iter [01700, 05004], lr: 0.066434, loss: 1.8777
2022-08-27 06:54:49 - train: epoch 0040, iter [01800, 05004], lr: 0.066404, loss: 1.5471
2022-08-27 06:56:29 - train: epoch 0040, iter [01900, 05004], lr: 0.066375, loss: 1.6681
2022-08-27 06:57:51 - train: epoch 0040, iter [02000, 05004], lr: 0.066345, loss: 1.7038
2022-08-27 06:59:19 - train: epoch 0040, iter [02100, 05004], lr: 0.066315, loss: 1.5806
2022-08-27 07:00:43 - train: epoch 0040, iter [02200, 05004], lr: 0.066286, loss: 1.6129
2022-08-27 07:02:16 - train: epoch 0040, iter [02300, 05004], lr: 0.066256, loss: 1.6904
2022-08-27 07:03:46 - train: epoch 0040, iter [02400, 05004], lr: 0.066226, loss: 1.8817
2022-08-27 07:05:31 - train: epoch 0040, iter [02500, 05004], lr: 0.066196, loss: 1.8724
2022-08-27 07:06:58 - train: epoch 0040, iter [02600, 05004], lr: 0.066167, loss: 1.6053
2022-08-27 07:08:37 - train: epoch 0040, iter [02700, 05004], lr: 0.066137, loss: 1.8213
2022-08-27 07:10:24 - train: epoch 0040, iter [02800, 05004], lr: 0.066107, loss: 1.8040
2022-08-27 07:12:02 - train: epoch 0040, iter [02900, 05004], lr: 0.066078, loss: 2.1112
2022-08-27 07:13:39 - train: epoch 0040, iter [03000, 05004], lr: 0.066048, loss: 1.8748
2022-08-27 07:15:02 - train: epoch 0040, iter [03100, 05004], lr: 0.066018, loss: 1.6086
2022-08-27 07:16:30 - train: epoch 0040, iter [03200, 05004], lr: 0.065988, loss: 1.9339
2022-08-27 07:18:06 - train: epoch 0040, iter [03300, 05004], lr: 0.065959, loss: 1.8488
2022-08-27 07:19:41 - train: epoch 0040, iter [03400, 05004], lr: 0.065929, loss: 1.8408
2022-08-27 07:21:14 - train: epoch 0040, iter [03500, 05004], lr: 0.065899, loss: 1.7902
2022-08-27 07:22:52 - train: epoch 0040, iter [03600, 05004], lr: 0.065869, loss: 1.7880
2022-08-27 07:24:22 - train: epoch 0040, iter [03700, 05004], lr: 0.065840, loss: 1.8393
2022-08-27 07:25:53 - train: epoch 0040, iter [03800, 05004], lr: 0.065810, loss: 1.6033
2022-08-27 07:27:30 - train: epoch 0040, iter [03900, 05004], lr: 0.065780, loss: 1.8970
2022-08-27 07:29:21 - train: epoch 0040, iter [04000, 05004], lr: 0.065750, loss: 1.9128
2022-08-27 07:31:00 - train: epoch 0040, iter [04100, 05004], lr: 0.065720, loss: 1.9118
2022-08-27 07:32:26 - train: epoch 0040, iter [04200, 05004], lr: 0.065691, loss: 1.6917
2022-08-27 07:34:00 - train: epoch 0040, iter [04300, 05004], lr: 0.065661, loss: 1.8457
2022-08-27 07:35:41 - train: epoch 0040, iter [04400, 05004], lr: 0.065631, loss: 1.7214
2022-08-27 07:37:17 - train: epoch 0040, iter [04500, 05004], lr: 0.065601, loss: 1.5238
2022-08-27 07:38:56 - train: epoch 0040, iter [04600, 05004], lr: 0.065571, loss: 1.8440
2022-08-27 07:40:42 - train: epoch 0040, iter [04700, 05004], lr: 0.065542, loss: 1.8151
2022-08-27 07:42:12 - train: epoch 0040, iter [04800, 05004], lr: 0.065512, loss: 1.7263
2022-08-27 07:43:59 - train: epoch 0040, iter [04900, 05004], lr: 0.065482, loss: 1.9583
2022-08-27 07:45:27 - train: epoch 0040, iter [05000, 05004], lr: 0.065452, loss: 1.9168
2022-08-27 07:45:30 - train: epoch 040, train_loss: 1.7503
2022-08-27 07:49:00 - eval: epoch: 040, acc1: 57.170%, acc5: 81.242%, test_loss: 1.8266, per_image_load_time: 7.194ms, per_image_inference_time: 0.810ms
2022-08-27 07:49:00 - until epoch: 040, best_acc1: 61.902%
2022-08-27 07:49:00 - epoch 041 lr: 0.065451
2022-08-27 07:50:45 - train: epoch 0041, iter [00100, 05004], lr: 0.065421, loss: 1.8112
2022-08-27 07:52:23 - train: epoch 0041, iter [00200, 05004], lr: 0.065391, loss: 1.9595
2022-08-27 07:54:03 - train: epoch 0041, iter [00300, 05004], lr: 0.065361, loss: 1.5981
2022-08-27 07:55:25 - train: epoch 0041, iter [00400, 05004], lr: 0.065331, loss: 1.6649
2022-08-27 07:57:07 - train: epoch 0041, iter [00500, 05004], lr: 0.065302, loss: 1.5063
2022-08-27 07:58:35 - train: epoch 0041, iter [00600, 05004], lr: 0.065272, loss: 1.8549
2022-08-27 08:00:13 - train: epoch 0041, iter [00700, 05004], lr: 0.065242, loss: 1.5338
2022-08-27 08:01:54 - train: epoch 0041, iter [00800, 05004], lr: 0.065212, loss: 1.5045
2022-08-27 08:03:33 - train: epoch 0041, iter [00900, 05004], lr: 0.065182, loss: 1.4967
2022-08-27 08:05:01 - train: epoch 0041, iter [01000, 05004], lr: 0.065152, loss: 1.9703
2022-08-27 08:06:34 - train: epoch 0041, iter [01100, 05004], lr: 0.065122, loss: 1.5667
2022-08-27 08:08:16 - train: epoch 0041, iter [01200, 05004], lr: 0.065092, loss: 1.4361
2022-08-27 08:10:00 - train: epoch 0041, iter [01300, 05004], lr: 0.065062, loss: 1.7505
2022-08-27 08:11:42 - train: epoch 0041, iter [01400, 05004], lr: 0.065032, loss: 1.8853
2022-08-27 08:13:21 - train: epoch 0041, iter [01500, 05004], lr: 0.065002, loss: 2.0131
2022-08-27 08:14:57 - train: epoch 0041, iter [01600, 05004], lr: 0.064972, loss: 1.4705
2022-08-27 08:16:31 - train: epoch 0041, iter [01700, 05004], lr: 0.064942, loss: 1.7590
2022-08-27 08:18:08 - train: epoch 0041, iter [01800, 05004], lr: 0.064912, loss: 1.7912
2022-08-27 08:19:47 - train: epoch 0041, iter [01900, 05004], lr: 0.064883, loss: 1.9108
2022-08-27 08:21:08 - train: epoch 0041, iter [02000, 05004], lr: 0.064853, loss: 1.6937
2022-08-27 08:22:50 - train: epoch 0041, iter [02100, 05004], lr: 0.064823, loss: 1.5504
2022-08-27 08:24:28 - train: epoch 0041, iter [02200, 05004], lr: 0.064793, loss: 1.5993
2022-08-27 08:26:05 - train: epoch 0041, iter [02300, 05004], lr: 0.064763, loss: 1.4607
2022-08-27 08:27:45 - train: epoch 0041, iter [02400, 05004], lr: 0.064733, loss: 1.9370
2022-08-27 08:29:28 - train: epoch 0041, iter [02500, 05004], lr: 0.064703, loss: 1.5479
2022-08-27 08:31:09 - train: epoch 0041, iter [02600, 05004], lr: 0.064673, loss: 1.9199
2022-08-27 08:32:56 - train: epoch 0041, iter [02700, 05004], lr: 0.064643, loss: 1.8862
2022-08-27 08:34:33 - train: epoch 0041, iter [02800, 05004], lr: 0.064613, loss: 1.6992
2022-08-27 08:36:26 - train: epoch 0041, iter [02900, 05004], lr: 0.064583, loss: 1.8867
2022-08-27 08:38:02 - train: epoch 0041, iter [03000, 05004], lr: 0.064553, loss: 1.8138
2022-08-27 08:39:36 - train: epoch 0041, iter [03100, 05004], lr: 0.064522, loss: 1.9015
2022-08-27 08:40:59 - train: epoch 0041, iter [03200, 05004], lr: 0.064492, loss: 1.7027
2022-08-27 08:42:29 - train: epoch 0041, iter [03300, 05004], lr: 0.064462, loss: 1.5921
2022-08-27 08:44:04 - train: epoch 0041, iter [03400, 05004], lr: 0.064432, loss: 1.5844
2022-08-27 08:45:45 - train: epoch 0041, iter [03500, 05004], lr: 0.064402, loss: 1.8648
2022-08-27 08:47:08 - train: epoch 0041, iter [03600, 05004], lr: 0.064372, loss: 1.7670
2022-08-27 08:48:39 - train: epoch 0041, iter [03700, 05004], lr: 0.064342, loss: 1.7883
2022-08-27 08:50:20 - train: epoch 0041, iter [03800, 05004], lr: 0.064312, loss: 1.5723
2022-08-27 08:51:51 - train: epoch 0041, iter [03900, 05004], lr: 0.064282, loss: 1.8469
2022-08-27 08:53:28 - train: epoch 0041, iter [04000, 05004], lr: 0.064252, loss: 1.6342
2022-08-27 08:55:01 - train: epoch 0041, iter [04100, 05004], lr: 0.064222, loss: 1.7979
2022-08-27 08:56:41 - train: epoch 0041, iter [04200, 05004], lr: 0.064192, loss: 1.6573
2022-08-27 08:58:13 - train: epoch 0041, iter [04300, 05004], lr: 0.064162, loss: 1.7527
2022-08-27 08:59:59 - train: epoch 0041, iter [04400, 05004], lr: 0.064132, loss: 1.6600
2022-08-27 09:01:37 - train: epoch 0041, iter [04500, 05004], lr: 0.064101, loss: 1.8990
2022-08-27 09:03:12 - train: epoch 0041, iter [04600, 05004], lr: 0.064071, loss: 1.8001
2022-08-27 09:04:46 - train: epoch 0041, iter [04700, 05004], lr: 0.064041, loss: 2.0046
2022-08-27 09:06:34 - train: epoch 0041, iter [04800, 05004], lr: 0.064011, loss: 1.7623
2022-08-27 09:08:11 - train: epoch 0041, iter [04900, 05004], lr: 0.063981, loss: 1.7736
2022-08-27 09:09:42 - train: epoch 0041, iter [05000, 05004], lr: 0.063951, loss: 1.7819
2022-08-27 09:09:44 - train: epoch 041, train_loss: 1.7417
2022-08-27 09:13:13 - eval: epoch: 041, acc1: 59.370%, acc5: 83.060%, test_loss: 1.7032, per_image_load_time: 7.216ms, per_image_inference_time: 0.851ms
2022-08-27 09:13:14 - until epoch: 041, best_acc1: 61.902%
2022-08-27 09:13:14 - epoch 042 lr: 0.063949
2022-08-27 09:14:55 - train: epoch 0042, iter [00100, 05004], lr: 0.063919, loss: 1.4398
2022-08-27 09:16:32 - train: epoch 0042, iter [00200, 05004], lr: 0.063889, loss: 1.7932
2022-08-27 09:18:13 - train: epoch 0042, iter [00300, 05004], lr: 0.063859, loss: 1.8624
2022-08-27 09:19:49 - train: epoch 0042, iter [00400, 05004], lr: 0.063829, loss: 1.4109
2022-08-27 09:21:25 - train: epoch 0042, iter [00500, 05004], lr: 0.063799, loss: 1.7471
2022-08-27 09:22:53 - train: epoch 0042, iter [00600, 05004], lr: 0.063769, loss: 1.5135
2022-08-27 09:24:32 - train: epoch 0042, iter [00700, 05004], lr: 0.063738, loss: 1.7288
2022-08-27 09:26:13 - train: epoch 0042, iter [00800, 05004], lr: 0.063708, loss: 1.6908
2022-08-27 09:27:56 - train: epoch 0042, iter [00900, 05004], lr: 0.063678, loss: 1.9554
2022-08-27 09:29:41 - train: epoch 0042, iter [01000, 05004], lr: 0.063648, loss: 1.8538
2022-08-27 09:31:18 - train: epoch 0042, iter [01100, 05004], lr: 0.063618, loss: 1.5876
2022-08-27 09:32:53 - train: epoch 0042, iter [01200, 05004], lr: 0.063587, loss: 1.6670
2022-08-27 09:34:39 - train: epoch 0042, iter [01300, 05004], lr: 0.063557, loss: 1.7466
2022-08-27 09:36:04 - train: epoch 0042, iter [01400, 05004], lr: 0.063527, loss: 2.0062
2022-08-27 09:37:42 - train: epoch 0042, iter [01500, 05004], lr: 0.063497, loss: 1.6601
2022-08-27 09:39:05 - train: epoch 0042, iter [01600, 05004], lr: 0.063467, loss: 1.8677
2022-08-27 09:40:40 - train: epoch 0042, iter [01700, 05004], lr: 0.063436, loss: 1.7222
2022-08-27 09:42:21 - train: epoch 0042, iter [01800, 05004], lr: 0.063406, loss: 1.6898
2022-08-27 09:43:49 - train: epoch 0042, iter [01900, 05004], lr: 0.063376, loss: 1.8431
2022-08-27 09:45:28 - train: epoch 0042, iter [02000, 05004], lr: 0.063346, loss: 1.6171
2022-08-27 09:46:53 - train: epoch 0042, iter [02100, 05004], lr: 0.063315, loss: 1.5350
2022-08-27 09:48:25 - train: epoch 0042, iter [02200, 05004], lr: 0.063285, loss: 1.6689
2022-08-27 09:49:52 - train: epoch 0042, iter [02300, 05004], lr: 0.063255, loss: 1.6460
2022-08-27 09:51:27 - train: epoch 0042, iter [02400, 05004], lr: 0.063225, loss: 2.0217
2022-08-27 09:53:09 - train: epoch 0042, iter [02500, 05004], lr: 0.063194, loss: 1.8357
2022-08-27 09:54:47 - train: epoch 0042, iter [02600, 05004], lr: 0.063164, loss: 1.7946
2022-08-27 09:56:19 - train: epoch 0042, iter [02700, 05004], lr: 0.063134, loss: 1.6475
2022-08-27 09:57:49 - train: epoch 0042, iter [02800, 05004], lr: 0.063103, loss: 1.6395
2022-08-27 09:59:23 - train: epoch 0042, iter [02900, 05004], lr: 0.063073, loss: 1.6876
2022-08-27 10:01:00 - train: epoch 0042, iter [03000, 05004], lr: 0.063043, loss: 1.7798
2022-08-27 10:02:33 - train: epoch 0042, iter [03100, 05004], lr: 0.063012, loss: 1.7882
2022-08-27 10:04:02 - train: epoch 0042, iter [03200, 05004], lr: 0.062982, loss: 1.7870
2022-08-27 10:05:43 - train: epoch 0042, iter [03300, 05004], lr: 0.062952, loss: 1.9613
2022-08-27 10:07:09 - train: epoch 0042, iter [03400, 05004], lr: 0.062922, loss: 1.5941
2022-08-27 10:08:33 - train: epoch 0042, iter [03500, 05004], lr: 0.062891, loss: 1.6796
2022-08-27 10:10:10 - train: epoch 0042, iter [03600, 05004], lr: 0.062861, loss: 1.8644
2022-08-27 10:11:45 - train: epoch 0042, iter [03700, 05004], lr: 0.062831, loss: 1.7462
2022-08-27 10:13:15 - train: epoch 0042, iter [03800, 05004], lr: 0.062800, loss: 1.5718
2022-08-27 10:15:00 - train: epoch 0042, iter [03900, 05004], lr: 0.062770, loss: 1.7252
2022-08-27 10:16:39 - train: epoch 0042, iter [04000, 05004], lr: 0.062740, loss: 1.7987
2022-08-27 10:17:56 - train: epoch 0042, iter [04100, 05004], lr: 0.062709, loss: 1.9115
2022-08-27 10:19:32 - train: epoch 0042, iter [04200, 05004], lr: 0.062679, loss: 1.7979
2022-08-27 10:20:59 - train: epoch 0042, iter [04300, 05004], lr: 0.062648, loss: 1.6269
2022-08-27 10:22:29 - train: epoch 0042, iter [04400, 05004], lr: 0.062618, loss: 1.6685
2022-08-27 10:24:09 - train: epoch 0042, iter [04500, 05004], lr: 0.062588, loss: 1.8195
2022-08-27 10:25:48 - train: epoch 0042, iter [04600, 05004], lr: 0.062557, loss: 2.0465
2022-08-27 10:27:24 - train: epoch 0042, iter [04700, 05004], lr: 0.062527, loss: 1.5727
2022-08-27 10:29:03 - train: epoch 0042, iter [04800, 05004], lr: 0.062497, loss: 1.8777
2022-08-27 10:30:38 - train: epoch 0042, iter [04900, 05004], lr: 0.062466, loss: 1.7325
2022-08-27 10:32:18 - train: epoch 0042, iter [05000, 05004], lr: 0.062436, loss: 1.7224
2022-08-27 10:32:19 - train: epoch 042, train_loss: 1.7288
2022-08-27 10:35:58 - eval: epoch: 042, acc1: 58.998%, acc5: 82.476%, test_loss: 1.7339, per_image_load_time: 6.553ms, per_image_inference_time: 0.843ms
2022-08-27 10:35:59 - until epoch: 042, best_acc1: 61.902%
2022-08-27 10:35:59 - epoch 043 lr: 0.062434
2022-08-27 10:37:26 - train: epoch 0043, iter [00100, 05004], lr: 0.062404, loss: 1.6761
2022-08-27 10:38:49 - train: epoch 0043, iter [00200, 05004], lr: 0.062374, loss: 1.7455
2022-08-27 10:40:13 - train: epoch 0043, iter [00300, 05004], lr: 0.062343, loss: 1.4427
2022-08-27 10:41:18 - train: epoch 0043, iter [00400, 05004], lr: 0.062313, loss: 1.6094
2022-08-27 10:42:12 - train: epoch 0043, iter [00500, 05004], lr: 0.062282, loss: 1.6520
2022-08-27 10:42:54 - train: epoch 0043, iter [00600, 05004], lr: 0.062252, loss: 1.5960
2022-08-27 10:43:28 - train: epoch 0043, iter [00700, 05004], lr: 0.062222, loss: 1.7720
2022-08-27 10:44:01 - train: epoch 0043, iter [00800, 05004], lr: 0.062191, loss: 1.8720
2022-08-27 10:44:35 - train: epoch 0043, iter [00900, 05004], lr: 0.062161, loss: 1.6111
2022-08-27 10:45:08 - train: epoch 0043, iter [01000, 05004], lr: 0.062130, loss: 1.8742
2022-08-27 10:45:42 - train: epoch 0043, iter [01100, 05004], lr: 0.062100, loss: 1.7805
2022-08-27 10:46:17 - train: epoch 0043, iter [01200, 05004], lr: 0.062069, loss: 1.6233
2022-08-27 10:46:50 - train: epoch 0043, iter [01300, 05004], lr: 0.062039, loss: 1.7979
2022-08-27 10:47:23 - train: epoch 0043, iter [01400, 05004], lr: 0.062008, loss: 1.6749
2022-08-27 10:47:58 - train: epoch 0043, iter [01500, 05004], lr: 0.061978, loss: 1.5250
2022-08-27 10:48:31 - train: epoch 0043, iter [01600, 05004], lr: 0.061947, loss: 1.7952
2022-08-27 10:49:05 - train: epoch 0043, iter [01700, 05004], lr: 0.061917, loss: 1.7387
2022-08-27 10:49:39 - train: epoch 0043, iter [01800, 05004], lr: 0.061886, loss: 1.8565
2022-08-27 10:50:13 - train: epoch 0043, iter [01900, 05004], lr: 0.061856, loss: 1.6922
2022-08-27 10:50:47 - train: epoch 0043, iter [02000, 05004], lr: 0.061825, loss: 1.5681
2022-08-27 10:51:21 - train: epoch 0043, iter [02100, 05004], lr: 0.061795, loss: 1.6243
2022-08-27 10:51:55 - train: epoch 0043, iter [02200, 05004], lr: 0.061764, loss: 1.7312
2022-08-27 10:52:30 - train: epoch 0043, iter [02300, 05004], lr: 0.061734, loss: 1.8912
2022-08-27 10:53:03 - train: epoch 0043, iter [02400, 05004], lr: 0.061703, loss: 1.7085
2022-08-27 10:53:38 - train: epoch 0043, iter [02500, 05004], lr: 0.061673, loss: 1.9104
2022-08-27 10:54:11 - train: epoch 0043, iter [02600, 05004], lr: 0.061642, loss: 1.4139
2022-08-27 10:54:45 - train: epoch 0043, iter [02700, 05004], lr: 0.061612, loss: 1.7367
2022-08-27 10:55:20 - train: epoch 0043, iter [02800, 05004], lr: 0.061581, loss: 1.5965
2022-08-27 10:55:54 - train: epoch 0043, iter [02900, 05004], lr: 0.061551, loss: 1.7780
2022-08-27 10:56:28 - train: epoch 0043, iter [03000, 05004], lr: 0.061520, loss: 1.8942
2022-08-27 10:57:02 - train: epoch 0043, iter [03100, 05004], lr: 0.061490, loss: 1.9168
2022-08-27 10:57:36 - train: epoch 0043, iter [03200, 05004], lr: 0.061459, loss: 1.8301
2022-08-27 10:58:10 - train: epoch 0043, iter [03300, 05004], lr: 0.061429, loss: 1.7467
2022-08-27 10:58:43 - train: epoch 0043, iter [03400, 05004], lr: 0.061398, loss: 1.7960
2022-08-27 10:59:18 - train: epoch 0043, iter [03500, 05004], lr: 0.061367, loss: 1.7046
2022-08-27 10:59:52 - train: epoch 0043, iter [03600, 05004], lr: 0.061337, loss: 1.6997
2022-08-27 11:00:26 - train: epoch 0043, iter [03700, 05004], lr: 0.061306, loss: 1.8309
2022-08-27 11:01:00 - train: epoch 0043, iter [03800, 05004], lr: 0.061276, loss: 1.8826
2022-08-27 11:01:34 - train: epoch 0043, iter [03900, 05004], lr: 0.061245, loss: 1.8743
2022-08-27 11:02:07 - train: epoch 0043, iter [04000, 05004], lr: 0.061215, loss: 1.7612
2022-08-27 11:02:41 - train: epoch 0043, iter [04100, 05004], lr: 0.061184, loss: 2.1114
2022-08-27 11:03:15 - train: epoch 0043, iter [04200, 05004], lr: 0.061153, loss: 1.8210
2022-08-27 11:03:50 - train: epoch 0043, iter [04300, 05004], lr: 0.061123, loss: 1.7230
2022-08-27 11:04:24 - train: epoch 0043, iter [04400, 05004], lr: 0.061092, loss: 1.5864
2022-08-27 11:04:58 - train: epoch 0043, iter [04500, 05004], lr: 0.061062, loss: 1.7499
2022-08-27 11:05:31 - train: epoch 0043, iter [04600, 05004], lr: 0.061031, loss: 1.7210
2022-08-27 11:06:05 - train: epoch 0043, iter [04700, 05004], lr: 0.061000, loss: 1.6619
2022-08-27 11:06:39 - train: epoch 0043, iter [04800, 05004], lr: 0.060970, loss: 1.7509
2022-08-27 11:07:13 - train: epoch 0043, iter [04900, 05004], lr: 0.060939, loss: 1.7549
2022-08-27 11:07:47 - train: epoch 0043, iter [05000, 05004], lr: 0.060908, loss: 1.6607
2022-08-27 11:07:48 - train: epoch 043, train_loss: 1.7165
2022-08-27 11:09:03 - eval: epoch: 043, acc1: 61.908%, acc5: 84.730%, test_loss: 1.5874, per_image_load_time: 2.426ms, per_image_inference_time: 0.493ms
2022-08-27 11:09:03 - until epoch: 043, best_acc1: 61.908%
2022-08-27 11:09:03 - epoch 044 lr: 0.060907
2022-08-27 11:09:42 - train: epoch 0044, iter [00100, 05004], lr: 0.060877, loss: 1.8559
2022-08-27 11:10:14 - train: epoch 0044, iter [00200, 05004], lr: 0.060846, loss: 1.8606
2022-08-27 11:10:49 - train: epoch 0044, iter [00300, 05004], lr: 0.060815, loss: 1.6279
2022-08-27 11:11:23 - train: epoch 0044, iter [00400, 05004], lr: 0.060785, loss: 1.3784
2022-08-27 11:11:56 - train: epoch 0044, iter [00500, 05004], lr: 0.060754, loss: 1.5846
2022-08-27 11:12:30 - train: epoch 0044, iter [00600, 05004], lr: 0.060723, loss: 1.4385
2022-08-27 11:13:03 - train: epoch 0044, iter [00700, 05004], lr: 0.060693, loss: 1.4851
2022-08-27 11:13:37 - train: epoch 0044, iter [00800, 05004], lr: 0.060662, loss: 1.6410
2022-08-27 11:14:10 - train: epoch 0044, iter [00900, 05004], lr: 0.060631, loss: 1.7188
2022-08-27 11:14:44 - train: epoch 0044, iter [01000, 05004], lr: 0.060601, loss: 1.6390
2022-08-27 11:15:18 - train: epoch 0044, iter [01100, 05004], lr: 0.060570, loss: 1.6203
2022-08-27 11:15:53 - train: epoch 0044, iter [01200, 05004], lr: 0.060539, loss: 1.6650
2022-08-27 11:16:26 - train: epoch 0044, iter [01300, 05004], lr: 0.060509, loss: 1.7328
2022-08-27 11:17:01 - train: epoch 0044, iter [01400, 05004], lr: 0.060478, loss: 1.5396
2022-08-27 11:17:35 - train: epoch 0044, iter [01500, 05004], lr: 0.060447, loss: 1.5102
2022-08-27 11:18:09 - train: epoch 0044, iter [01600, 05004], lr: 0.060416, loss: 1.5795
2022-08-27 11:18:44 - train: epoch 0044, iter [01700, 05004], lr: 0.060386, loss: 1.5500
2022-08-27 11:19:17 - train: epoch 0044, iter [01800, 05004], lr: 0.060355, loss: 1.7416
2022-08-27 11:19:51 - train: epoch 0044, iter [01900, 05004], lr: 0.060324, loss: 1.8431
2022-08-27 11:20:25 - train: epoch 0044, iter [02000, 05004], lr: 0.060294, loss: 1.5961
2022-08-27 11:20:59 - train: epoch 0044, iter [02100, 05004], lr: 0.060263, loss: 1.8478
2022-08-27 11:21:33 - train: epoch 0044, iter [02200, 05004], lr: 0.060232, loss: 1.6377
2022-08-27 11:22:06 - train: epoch 0044, iter [02300, 05004], lr: 0.060201, loss: 1.8508
2022-08-27 11:22:42 - train: epoch 0044, iter [02400, 05004], lr: 0.060171, loss: 1.6144
2022-08-27 11:23:16 - train: epoch 0044, iter [02500, 05004], lr: 0.060140, loss: 1.7492
2022-08-27 11:23:50 - train: epoch 0044, iter [02600, 05004], lr: 0.060109, loss: 1.7226
2022-08-27 11:24:24 - train: epoch 0044, iter [02700, 05004], lr: 0.060078, loss: 1.7828
2022-08-27 11:24:58 - train: epoch 0044, iter [02800, 05004], lr: 0.060048, loss: 1.4270
2022-08-27 11:25:32 - train: epoch 0044, iter [02900, 05004], lr: 0.060017, loss: 1.5170
2022-08-27 11:26:06 - train: epoch 0044, iter [03000, 05004], lr: 0.059986, loss: 1.4819
2022-08-27 11:26:40 - train: epoch 0044, iter [03100, 05004], lr: 0.059955, loss: 1.7086
2022-08-27 11:27:13 - train: epoch 0044, iter [03200, 05004], lr: 0.059925, loss: 1.3967
2022-08-27 11:27:48 - train: epoch 0044, iter [03300, 05004], lr: 0.059894, loss: 1.5463
2022-08-27 11:28:21 - train: epoch 0044, iter [03400, 05004], lr: 0.059863, loss: 1.9690
2022-08-27 11:28:56 - train: epoch 0044, iter [03500, 05004], lr: 0.059832, loss: 1.5879
2022-08-27 11:29:31 - train: epoch 0044, iter [03600, 05004], lr: 0.059802, loss: 1.9400
2022-08-27 11:30:04 - train: epoch 0044, iter [03700, 05004], lr: 0.059771, loss: 1.7354
2022-08-27 11:30:38 - train: epoch 0044, iter [03800, 05004], lr: 0.059740, loss: 1.3508
2022-08-27 11:31:12 - train: epoch 0044, iter [03900, 05004], lr: 0.059709, loss: 1.7496
2022-08-27 11:31:46 - train: epoch 0044, iter [04000, 05004], lr: 0.059678, loss: 1.7583
2022-08-27 11:32:20 - train: epoch 0044, iter [04100, 05004], lr: 0.059648, loss: 1.5577
2022-08-27 11:32:54 - train: epoch 0044, iter [04200, 05004], lr: 0.059617, loss: 1.7653
2022-08-27 11:33:28 - train: epoch 0044, iter [04300, 05004], lr: 0.059586, loss: 1.9105
2022-08-27 11:34:02 - train: epoch 0044, iter [04400, 05004], lr: 0.059555, loss: 1.4657
2022-08-27 11:34:35 - train: epoch 0044, iter [04500, 05004], lr: 0.059524, loss: 1.8010
2022-08-27 11:35:10 - train: epoch 0044, iter [04600, 05004], lr: 0.059494, loss: 1.7597
2022-08-27 11:35:44 - train: epoch 0044, iter [04700, 05004], lr: 0.059463, loss: 1.8002
2022-08-27 11:36:18 - train: epoch 0044, iter [04800, 05004], lr: 0.059432, loss: 1.9593
2022-08-27 11:36:51 - train: epoch 0044, iter [04900, 05004], lr: 0.059401, loss: 1.5521
2022-08-27 11:37:24 - train: epoch 0044, iter [05000, 05004], lr: 0.059370, loss: 1.7951
2022-08-27 11:37:25 - train: epoch 044, train_loss: 1.7053
2022-08-27 11:38:39 - eval: epoch: 044, acc1: 63.222%, acc5: 85.716%, test_loss: 1.5174, per_image_load_time: 2.266ms, per_image_inference_time: 0.522ms
2022-08-27 11:38:40 - until epoch: 044, best_acc1: 63.222%
2022-08-27 11:38:40 - epoch 045 lr: 0.059369
2022-08-27 11:39:18 - train: epoch 0045, iter [00100, 05004], lr: 0.059338, loss: 1.5488
2022-08-27 11:39:52 - train: epoch 0045, iter [00200, 05004], lr: 0.059307, loss: 1.7444
2022-08-27 11:40:25 - train: epoch 0045, iter [00300, 05004], lr: 0.059277, loss: 1.7393
2022-08-27 11:40:59 - train: epoch 0045, iter [00400, 05004], lr: 0.059246, loss: 1.4612
2022-08-27 11:41:33 - train: epoch 0045, iter [00500, 05004], lr: 0.059215, loss: 1.7925
2022-08-27 11:42:08 - train: epoch 0045, iter [00600, 05004], lr: 0.059184, loss: 1.7393
2022-08-27 11:42:42 - train: epoch 0045, iter [00700, 05004], lr: 0.059153, loss: 1.4014
2022-08-27 11:43:16 - train: epoch 0045, iter [00800, 05004], lr: 0.059122, loss: 1.6477
2022-08-27 11:43:50 - train: epoch 0045, iter [00900, 05004], lr: 0.059091, loss: 1.7953
2022-08-27 11:44:24 - train: epoch 0045, iter [01000, 05004], lr: 0.059061, loss: 1.6211
2022-08-27 11:44:58 - train: epoch 0045, iter [01100, 05004], lr: 0.059030, loss: 1.8169
2022-08-27 11:45:32 - train: epoch 0045, iter [01200, 05004], lr: 0.058999, loss: 1.7252
2022-08-27 11:46:07 - train: epoch 0045, iter [01300, 05004], lr: 0.058968, loss: 1.7325
2022-08-27 11:46:40 - train: epoch 0045, iter [01400, 05004], lr: 0.058937, loss: 1.6041
2022-08-27 11:47:14 - train: epoch 0045, iter [01500, 05004], lr: 0.058906, loss: 1.8104
2022-08-27 11:47:48 - train: epoch 0045, iter [01600, 05004], lr: 0.058875, loss: 1.6236
2022-08-27 11:48:22 - train: epoch 0045, iter [01700, 05004], lr: 0.058844, loss: 1.5316
2022-08-27 11:48:55 - train: epoch 0045, iter [01800, 05004], lr: 0.058813, loss: 1.5708
2022-08-27 11:49:29 - train: epoch 0045, iter [01900, 05004], lr: 0.058783, loss: 1.7234
2022-08-27 11:50:04 - train: epoch 0045, iter [02000, 05004], lr: 0.058752, loss: 1.7639
2022-08-27 11:50:38 - train: epoch 0045, iter [02100, 05004], lr: 0.058721, loss: 1.8098
2022-08-27 11:51:11 - train: epoch 0045, iter [02200, 05004], lr: 0.058690, loss: 1.7558
2022-08-27 11:51:46 - train: epoch 0045, iter [02300, 05004], lr: 0.058659, loss: 1.6135
2022-08-27 11:52:21 - train: epoch 0045, iter [02400, 05004], lr: 0.058628, loss: 1.8357
2022-08-27 11:52:55 - train: epoch 0045, iter [02500, 05004], lr: 0.058597, loss: 1.7402
2022-08-27 11:53:28 - train: epoch 0045, iter [02600, 05004], lr: 0.058566, loss: 1.6807
2022-08-27 11:54:03 - train: epoch 0045, iter [02700, 05004], lr: 0.058535, loss: 1.5620
2022-08-27 11:54:38 - train: epoch 0045, iter [02800, 05004], lr: 0.058504, loss: 1.6294
2022-08-27 11:55:12 - train: epoch 0045, iter [02900, 05004], lr: 0.058473, loss: 1.8545
2022-08-27 11:55:46 - train: epoch 0045, iter [03000, 05004], lr: 0.058442, loss: 1.8933
2022-08-27 11:56:21 - train: epoch 0045, iter [03100, 05004], lr: 0.058411, loss: 1.6991
2022-08-27 11:56:55 - train: epoch 0045, iter [03200, 05004], lr: 0.058381, loss: 1.7825
2022-08-27 11:57:29 - train: epoch 0045, iter [03300, 05004], lr: 0.058350, loss: 1.7152
2022-08-27 11:58:04 - train: epoch 0045, iter [03400, 05004], lr: 0.058319, loss: 1.3988
2022-08-27 11:58:38 - train: epoch 0045, iter [03500, 05004], lr: 0.058288, loss: 1.9251
2022-08-27 11:59:12 - train: epoch 0045, iter [03600, 05004], lr: 0.058257, loss: 1.8190
2022-08-27 11:59:46 - train: epoch 0045, iter [03700, 05004], lr: 0.058226, loss: 1.6457
2022-08-27 12:00:20 - train: epoch 0045, iter [03800, 05004], lr: 0.058195, loss: 1.6185
2022-08-27 12:00:56 - train: epoch 0045, iter [03900, 05004], lr: 0.058164, loss: 1.8400
2022-08-27 12:01:29 - train: epoch 0045, iter [04000, 05004], lr: 0.058133, loss: 1.7922
2022-08-27 12:02:04 - train: epoch 0045, iter [04100, 05004], lr: 0.058102, loss: 1.6625
2022-08-27 12:02:38 - train: epoch 0045, iter [04200, 05004], lr: 0.058071, loss: 1.8936
2022-08-27 12:03:12 - train: epoch 0045, iter [04300, 05004], lr: 0.058040, loss: 1.9670
2022-08-27 12:03:47 - train: epoch 0045, iter [04400, 05004], lr: 0.058009, loss: 1.8730
2022-08-27 12:04:21 - train: epoch 0045, iter [04500, 05004], lr: 0.057978, loss: 1.6270
2022-08-27 12:04:56 - train: epoch 0045, iter [04600, 05004], lr: 0.057947, loss: 1.8169
2022-08-27 12:05:30 - train: epoch 0045, iter [04700, 05004], lr: 0.057916, loss: 1.7825
2022-08-27 12:06:04 - train: epoch 0045, iter [04800, 05004], lr: 0.057885, loss: 1.6199
2022-08-27 12:06:38 - train: epoch 0045, iter [04900, 05004], lr: 0.057854, loss: 1.6560
2022-08-27 12:07:10 - train: epoch 0045, iter [05000, 05004], lr: 0.057823, loss: 1.6408
2022-08-27 12:07:11 - train: epoch 045, train_loss: 1.6901
2022-08-27 12:08:26 - eval: epoch: 045, acc1: 63.040%, acc5: 85.688%, test_loss: 1.5279, per_image_load_time: 2.366ms, per_image_inference_time: 0.489ms
2022-08-27 12:08:26 - until epoch: 045, best_acc1: 63.222%
2022-08-27 12:08:26 - epoch 046 lr: 0.057821
2022-08-27 12:09:05 - train: epoch 0046, iter [00100, 05004], lr: 0.057791, loss: 1.3319
2022-08-27 12:09:38 - train: epoch 0046, iter [00200, 05004], lr: 0.057760, loss: 1.4865
2022-08-27 12:10:12 - train: epoch 0046, iter [00300, 05004], lr: 0.057729, loss: 1.6852
2022-08-27 12:10:46 - train: epoch 0046, iter [00400, 05004], lr: 0.057698, loss: 1.9191
2022-08-27 12:11:20 - train: epoch 0046, iter [00500, 05004], lr: 0.057667, loss: 1.3954
2022-08-27 12:11:54 - train: epoch 0046, iter [00600, 05004], lr: 0.057636, loss: 1.7556
2022-08-27 12:12:27 - train: epoch 0046, iter [00700, 05004], lr: 0.057605, loss: 1.6285
2022-08-27 12:13:02 - train: epoch 0046, iter [00800, 05004], lr: 0.057574, loss: 1.7807
2022-08-27 12:13:36 - train: epoch 0046, iter [00900, 05004], lr: 0.057543, loss: 1.6800
2022-08-27 12:14:09 - train: epoch 0046, iter [01000, 05004], lr: 0.057512, loss: 1.5315
2022-08-27 12:14:43 - train: epoch 0046, iter [01100, 05004], lr: 0.057480, loss: 1.6494
2022-08-27 12:15:18 - train: epoch 0046, iter [01200, 05004], lr: 0.057449, loss: 1.6738
2022-08-27 12:15:52 - train: epoch 0046, iter [01300, 05004], lr: 0.057418, loss: 1.6178
2022-08-27 12:16:25 - train: epoch 0046, iter [01400, 05004], lr: 0.057387, loss: 1.9080
2022-08-27 12:17:00 - train: epoch 0046, iter [01500, 05004], lr: 0.057356, loss: 1.5375
2022-08-27 12:17:34 - train: epoch 0046, iter [01600, 05004], lr: 0.057325, loss: 1.6862
2022-08-27 12:18:09 - train: epoch 0046, iter [01700, 05004], lr: 0.057294, loss: 1.5120
2022-08-27 12:18:43 - train: epoch 0046, iter [01800, 05004], lr: 0.057263, loss: 1.6666
2022-08-27 12:19:17 - train: epoch 0046, iter [01900, 05004], lr: 0.057232, loss: 1.4934
2022-08-27 12:19:51 - train: epoch 0046, iter [02000, 05004], lr: 0.057201, loss: 1.6476
2022-08-27 12:20:25 - train: epoch 0046, iter [02100, 05004], lr: 0.057170, loss: 1.8806
2022-08-27 12:20:59 - train: epoch 0046, iter [02200, 05004], lr: 0.057139, loss: 1.5188
2022-08-27 12:21:34 - train: epoch 0046, iter [02300, 05004], lr: 0.057108, loss: 1.8940
2022-08-27 12:22:08 - train: epoch 0046, iter [02400, 05004], lr: 0.057077, loss: 1.7249
2022-08-27 12:22:43 - train: epoch 0046, iter [02500, 05004], lr: 0.057046, loss: 1.7157
2022-08-27 12:23:16 - train: epoch 0046, iter [02600, 05004], lr: 0.057015, loss: 1.7940
2022-08-27 12:23:51 - train: epoch 0046, iter [02700, 05004], lr: 0.056984, loss: 1.6419
2022-08-27 12:24:25 - train: epoch 0046, iter [02800, 05004], lr: 0.056952, loss: 1.4358
2022-08-27 12:25:00 - train: epoch 0046, iter [02900, 05004], lr: 0.056921, loss: 1.7776
2022-08-27 12:25:34 - train: epoch 0046, iter [03000, 05004], lr: 0.056890, loss: 1.4950
2022-08-27 12:26:08 - train: epoch 0046, iter [03100, 05004], lr: 0.056859, loss: 1.5401
2022-08-27 12:26:43 - train: epoch 0046, iter [03200, 05004], lr: 0.056828, loss: 1.8177
2022-08-27 12:27:18 - train: epoch 0046, iter [03300, 05004], lr: 0.056797, loss: 1.7197
2022-08-27 12:27:52 - train: epoch 0046, iter [03400, 05004], lr: 0.056766, loss: 1.6850
2022-08-27 12:28:26 - train: epoch 0046, iter [03500, 05004], lr: 0.056735, loss: 1.6243
2022-08-27 12:29:01 - train: epoch 0046, iter [03600, 05004], lr: 0.056704, loss: 1.5388
2022-08-27 12:29:35 - train: epoch 0046, iter [03700, 05004], lr: 0.056673, loss: 1.5490
2022-08-27 12:30:09 - train: epoch 0046, iter [03800, 05004], lr: 0.056641, loss: 1.8685
2022-08-27 12:30:44 - train: epoch 0046, iter [03900, 05004], lr: 0.056610, loss: 1.7986
2022-08-27 12:31:18 - train: epoch 0046, iter [04000, 05004], lr: 0.056579, loss: 1.5135
2022-08-27 12:31:53 - train: epoch 0046, iter [04100, 05004], lr: 0.056548, loss: 1.7401
2022-08-27 12:32:27 - train: epoch 0046, iter [04200, 05004], lr: 0.056517, loss: 1.6953
2022-08-27 12:33:02 - train: epoch 0046, iter [04300, 05004], lr: 0.056486, loss: 1.8397
2022-08-27 12:33:36 - train: epoch 0046, iter [04400, 05004], lr: 0.056455, loss: 1.7225
2022-08-27 12:34:11 - train: epoch 0046, iter [04500, 05004], lr: 0.056424, loss: 1.6140
2022-08-27 12:34:45 - train: epoch 0046, iter [04600, 05004], lr: 0.056392, loss: 1.7933
2022-08-27 12:35:19 - train: epoch 0046, iter [04700, 05004], lr: 0.056361, loss: 1.5954
2022-08-27 12:35:53 - train: epoch 0046, iter [04800, 05004], lr: 0.056330, loss: 1.6234
2022-08-27 12:36:28 - train: epoch 0046, iter [04900, 05004], lr: 0.056299, loss: 1.7437
2022-08-27 12:37:01 - train: epoch 0046, iter [05000, 05004], lr: 0.056268, loss: 1.5783
2022-08-27 12:37:02 - train: epoch 046, train_loss: 1.6761
2022-08-27 12:38:17 - eval: epoch: 046, acc1: 63.070%, acc5: 85.474%, test_loss: 1.5261, per_image_load_time: 1.508ms, per_image_inference_time: 0.522ms
2022-08-27 12:38:17 - until epoch: 046, best_acc1: 63.222%
2022-08-27 12:38:17 - epoch 047 lr: 0.056266
2022-08-27 12:38:55 - train: epoch 0047, iter [00100, 05004], lr: 0.056236, loss: 1.5869
2022-08-27 12:39:29 - train: epoch 0047, iter [00200, 05004], lr: 0.056204, loss: 1.7583
2022-08-27 12:40:03 - train: epoch 0047, iter [00300, 05004], lr: 0.056173, loss: 1.5897
2022-08-27 12:40:37 - train: epoch 0047, iter [00400, 05004], lr: 0.056142, loss: 1.4689
2022-08-27 12:41:10 - train: epoch 0047, iter [00500, 05004], lr: 0.056111, loss: 1.6491
2022-08-27 12:41:44 - train: epoch 0047, iter [00600, 05004], lr: 0.056080, loss: 1.6762
2022-08-27 12:42:18 - train: epoch 0047, iter [00700, 05004], lr: 0.056049, loss: 1.7693
2022-08-27 12:42:52 - train: epoch 0047, iter [00800, 05004], lr: 0.056017, loss: 1.5583
2022-08-27 12:43:24 - train: epoch 0047, iter [00900, 05004], lr: 0.055986, loss: 1.8060
2022-08-27 12:43:58 - train: epoch 0047, iter [01000, 05004], lr: 0.055955, loss: 1.7213
2022-08-27 12:44:32 - train: epoch 0047, iter [01100, 05004], lr: 0.055924, loss: 1.7987
2022-08-27 12:45:06 - train: epoch 0047, iter [01200, 05004], lr: 0.055893, loss: 1.6075
2022-08-27 12:45:40 - train: epoch 0047, iter [01300, 05004], lr: 0.055862, loss: 1.7733
2022-08-27 12:46:14 - train: epoch 0047, iter [01400, 05004], lr: 0.055830, loss: 1.6493
2022-08-27 12:46:48 - train: epoch 0047, iter [01500, 05004], lr: 0.055799, loss: 1.5111
2022-08-27 12:47:22 - train: epoch 0047, iter [01600, 05004], lr: 0.055768, loss: 1.6920
2022-08-27 12:47:55 - train: epoch 0047, iter [01700, 05004], lr: 0.055737, loss: 1.5135
2022-08-27 12:48:30 - train: epoch 0047, iter [01800, 05004], lr: 0.055706, loss: 1.6020
2022-08-27 12:49:04 - train: epoch 0047, iter [01900, 05004], lr: 0.055675, loss: 1.5368
2022-08-27 12:49:38 - train: epoch 0047, iter [02000, 05004], lr: 0.055643, loss: 1.6103
2022-08-27 12:50:13 - train: epoch 0047, iter [02100, 05004], lr: 0.055612, loss: 1.8132
2022-08-27 12:50:47 - train: epoch 0047, iter [02200, 05004], lr: 0.055581, loss: 1.8241
2022-08-27 12:51:20 - train: epoch 0047, iter [02300, 05004], lr: 0.055550, loss: 1.7349
2022-08-27 12:51:54 - train: epoch 0047, iter [02400, 05004], lr: 0.055519, loss: 1.4869
2022-08-27 12:52:28 - train: epoch 0047, iter [02500, 05004], lr: 0.055487, loss: 1.7152
2022-08-27 12:53:01 - train: epoch 0047, iter [02600, 05004], lr: 0.055456, loss: 1.9979
2022-08-27 12:53:35 - train: epoch 0047, iter [02700, 05004], lr: 0.055425, loss: 1.6844
2022-08-27 12:54:09 - train: epoch 0047, iter [02800, 05004], lr: 0.055394, loss: 1.8437
2022-08-27 12:54:43 - train: epoch 0047, iter [02900, 05004], lr: 0.055363, loss: 1.6106
2022-08-27 12:55:17 - train: epoch 0047, iter [03000, 05004], lr: 0.055331, loss: 1.7935
2022-08-27 12:55:52 - train: epoch 0047, iter [03100, 05004], lr: 0.055300, loss: 1.5832
2022-08-27 12:56:25 - train: epoch 0047, iter [03200, 05004], lr: 0.055269, loss: 1.9258
2022-08-27 12:56:59 - train: epoch 0047, iter [03300, 05004], lr: 0.055238, loss: 1.5400
2022-08-27 12:57:34 - train: epoch 0047, iter [03400, 05004], lr: 0.055206, loss: 1.7188
2022-08-27 12:58:08 - train: epoch 0047, iter [03500, 05004], lr: 0.055175, loss: 1.9015
2022-08-27 12:58:42 - train: epoch 0047, iter [03600, 05004], lr: 0.055144, loss: 1.7161
2022-08-27 12:59:15 - train: epoch 0047, iter [03700, 05004], lr: 0.055113, loss: 1.7993
2022-08-27 12:59:49 - train: epoch 0047, iter [03800, 05004], lr: 0.055082, loss: 1.7028
2022-08-27 13:00:24 - train: epoch 0047, iter [03900, 05004], lr: 0.055050, loss: 1.6075
2022-08-27 13:00:58 - train: epoch 0047, iter [04000, 05004], lr: 0.055019, loss: 1.7551
2022-08-27 13:01:32 - train: epoch 0047, iter [04100, 05004], lr: 0.054988, loss: 1.7532
2022-08-27 13:02:06 - train: epoch 0047, iter [04200, 05004], lr: 0.054957, loss: 1.6635
2022-08-27 13:02:40 - train: epoch 0047, iter [04300, 05004], lr: 0.054925, loss: 1.5593
2022-08-27 13:03:14 - train: epoch 0047, iter [04400, 05004], lr: 0.054894, loss: 1.6429
2022-08-27 13:03:47 - train: epoch 0047, iter [04500, 05004], lr: 0.054863, loss: 1.8174
2022-08-27 13:04:21 - train: epoch 0047, iter [04600, 05004], lr: 0.054832, loss: 1.6952
2022-08-27 13:04:55 - train: epoch 0047, iter [04700, 05004], lr: 0.054800, loss: 1.7637
2022-08-27 13:05:30 - train: epoch 0047, iter [04800, 05004], lr: 0.054769, loss: 1.5851
2022-08-27 13:06:04 - train: epoch 0047, iter [04900, 05004], lr: 0.054738, loss: 1.8394
2022-08-27 13:06:36 - train: epoch 0047, iter [05000, 05004], lr: 0.054707, loss: 1.5677
2022-08-27 13:06:37 - train: epoch 047, train_loss: 1.6673
2022-08-27 13:07:52 - eval: epoch: 047, acc1: 62.112%, acc5: 85.244%, test_loss: 1.5613, per_image_load_time: 1.663ms, per_image_inference_time: 0.527ms
2022-08-27 13:07:53 - until epoch: 047, best_acc1: 63.222%
2022-08-27 13:07:53 - epoch 048 lr: 0.054705
2022-08-27 13:08:32 - train: epoch 0048, iter [00100, 05004], lr: 0.054674, loss: 1.7627
2022-08-27 13:09:05 - train: epoch 0048, iter [00200, 05004], lr: 0.054643, loss: 1.9530
2022-08-27 13:09:40 - train: epoch 0048, iter [00300, 05004], lr: 0.054612, loss: 1.6453
2022-08-27 13:10:13 - train: epoch 0048, iter [00400, 05004], lr: 0.054580, loss: 1.6550
2022-08-27 13:10:47 - train: epoch 0048, iter [00500, 05004], lr: 0.054549, loss: 1.5575
2022-08-27 13:11:20 - train: epoch 0048, iter [00600, 05004], lr: 0.054518, loss: 1.7170
2022-08-27 13:11:56 - train: epoch 0048, iter [00700, 05004], lr: 0.054487, loss: 1.6577
2022-08-27 13:12:29 - train: epoch 0048, iter [00800, 05004], lr: 0.054455, loss: 1.6514
2022-08-27 13:13:04 - train: epoch 0048, iter [00900, 05004], lr: 0.054424, loss: 1.7798
2022-08-27 13:13:37 - train: epoch 0048, iter [01000, 05004], lr: 0.054393, loss: 1.7491
2022-08-27 13:14:12 - train: epoch 0048, iter [01100, 05004], lr: 0.054362, loss: 1.7136
2022-08-27 13:14:46 - train: epoch 0048, iter [01200, 05004], lr: 0.054330, loss: 1.6687
2022-08-27 13:15:20 - train: epoch 0048, iter [01300, 05004], lr: 0.054299, loss: 1.5157
2022-08-27 13:15:55 - train: epoch 0048, iter [01400, 05004], lr: 0.054268, loss: 1.5733
2022-08-27 13:16:29 - train: epoch 0048, iter [01500, 05004], lr: 0.054236, loss: 1.7034
2022-08-27 13:17:02 - train: epoch 0048, iter [01600, 05004], lr: 0.054205, loss: 1.5837
2022-08-27 13:17:37 - train: epoch 0048, iter [01700, 05004], lr: 0.054174, loss: 1.7980
2022-08-27 13:18:12 - train: epoch 0048, iter [01800, 05004], lr: 0.054143, loss: 1.6594
2022-08-27 13:18:45 - train: epoch 0048, iter [01900, 05004], lr: 0.054111, loss: 1.7722
2022-08-27 13:19:19 - train: epoch 0048, iter [02000, 05004], lr: 0.054080, loss: 1.8611
2022-08-27 13:19:54 - train: epoch 0048, iter [02100, 05004], lr: 0.054049, loss: 1.6457
2022-08-27 13:20:28 - train: epoch 0048, iter [02200, 05004], lr: 0.054017, loss: 1.8773
2022-08-27 13:21:03 - train: epoch 0048, iter [02300, 05004], lr: 0.053986, loss: 1.5394
2022-08-27 13:21:36 - train: epoch 0048, iter [02400, 05004], lr: 0.053955, loss: 1.8506
2022-08-27 13:22:12 - train: epoch 0048, iter [02500, 05004], lr: 0.053924, loss: 1.7488
2022-08-27 13:22:46 - train: epoch 0048, iter [02600, 05004], lr: 0.053892, loss: 1.8804
2022-08-27 13:23:20 - train: epoch 0048, iter [02700, 05004], lr: 0.053861, loss: 1.8061
2022-08-27 13:23:54 - train: epoch 0048, iter [02800, 05004], lr: 0.053830, loss: 1.5130
2022-08-27 13:24:28 - train: epoch 0048, iter [02900, 05004], lr: 0.053798, loss: 1.8123
2022-08-27 13:25:03 - train: epoch 0048, iter [03000, 05004], lr: 0.053767, loss: 1.6853
2022-08-27 13:25:37 - train: epoch 0048, iter [03100, 05004], lr: 0.053736, loss: 1.6435
2022-08-27 13:26:11 - train: epoch 0048, iter [03200, 05004], lr: 0.053704, loss: 1.4993
2022-08-27 13:26:46 - train: epoch 0048, iter [03300, 05004], lr: 0.053673, loss: 1.7079
2022-08-27 13:27:20 - train: epoch 0048, iter [03400, 05004], lr: 0.053642, loss: 1.6491
2022-08-27 13:27:54 - train: epoch 0048, iter [03500, 05004], lr: 0.053611, loss: 1.9233
2022-08-27 13:28:29 - train: epoch 0048, iter [03600, 05004], lr: 0.053579, loss: 1.7548
2022-08-27 13:29:03 - train: epoch 0048, iter [03700, 05004], lr: 0.053548, loss: 1.7304
2022-08-27 13:29:38 - train: epoch 0048, iter [03800, 05004], lr: 0.053517, loss: 1.6074
2022-08-27 13:30:12 - train: epoch 0048, iter [03900, 05004], lr: 0.053485, loss: 1.8044
2022-08-27 13:30:46 - train: epoch 0048, iter [04000, 05004], lr: 0.053454, loss: 1.5370
2022-08-27 13:31:20 - train: epoch 0048, iter [04100, 05004], lr: 0.053423, loss: 1.8505
2022-08-27 13:31:55 - train: epoch 0048, iter [04200, 05004], lr: 0.053391, loss: 1.6522
2022-08-27 13:32:29 - train: epoch 0048, iter [04300, 05004], lr: 0.053360, loss: 1.6705
2022-08-27 13:33:03 - train: epoch 0048, iter [04400, 05004], lr: 0.053329, loss: 1.5172
2022-08-27 13:33:39 - train: epoch 0048, iter [04500, 05004], lr: 0.053297, loss: 1.6629
2022-08-27 13:34:13 - train: epoch 0048, iter [04600, 05004], lr: 0.053266, loss: 1.6027
2022-08-27 13:34:47 - train: epoch 0048, iter [04700, 05004], lr: 0.053235, loss: 1.6927
2022-08-27 13:35:21 - train: epoch 0048, iter [04800, 05004], lr: 0.053203, loss: 1.9622
2022-08-27 13:35:55 - train: epoch 0048, iter [04900, 05004], lr: 0.053172, loss: 1.7129
2022-08-27 13:36:29 - train: epoch 0048, iter [05000, 05004], lr: 0.053141, loss: 1.6442
2022-08-27 13:36:30 - train: epoch 048, train_loss: 1.6563
2022-08-27 13:37:45 - eval: epoch: 048, acc1: 61.162%, acc5: 84.054%, test_loss: 1.6239, per_image_load_time: 1.784ms, per_image_inference_time: 0.508ms
2022-08-27 13:37:45 - until epoch: 048, best_acc1: 63.222%
2022-08-27 13:37:45 - epoch 049 lr: 0.053139
2022-08-27 13:38:24 - train: epoch 0049, iter [00100, 05004], lr: 0.053108, loss: 1.8279
2022-08-27 13:38:58 - train: epoch 0049, iter [00200, 05004], lr: 0.053077, loss: 1.5984
2022-08-27 13:39:32 - train: epoch 0049, iter [00300, 05004], lr: 0.053046, loss: 1.6588
2022-08-27 13:40:06 - train: epoch 0049, iter [00400, 05004], lr: 0.053014, loss: 1.7422
2022-08-27 13:40:40 - train: epoch 0049, iter [00500, 05004], lr: 0.052983, loss: 1.5183
2022-08-27 13:41:14 - train: epoch 0049, iter [00600, 05004], lr: 0.052952, loss: 1.5894
2022-08-27 13:41:48 - train: epoch 0049, iter [00700, 05004], lr: 0.052920, loss: 1.7048
2022-08-27 13:42:23 - train: epoch 0049, iter [00800, 05004], lr: 0.052889, loss: 1.8833
2022-08-27 13:42:56 - train: epoch 0049, iter [00900, 05004], lr: 0.052858, loss: 1.5786
2022-08-27 13:43:31 - train: epoch 0049, iter [01000, 05004], lr: 0.052826, loss: 1.6993
2022-08-27 13:44:05 - train: epoch 0049, iter [01100, 05004], lr: 0.052795, loss: 1.5002
2022-08-27 13:44:39 - train: epoch 0049, iter [01200, 05004], lr: 0.052763, loss: 1.3643
2022-08-27 13:45:13 - train: epoch 0049, iter [01300, 05004], lr: 0.052732, loss: 1.7281
2022-08-27 13:45:47 - train: epoch 0049, iter [01400, 05004], lr: 0.052701, loss: 1.7951
2022-08-27 13:46:21 - train: epoch 0049, iter [01500, 05004], lr: 0.052669, loss: 1.6805
2022-08-27 13:46:55 - train: epoch 0049, iter [01600, 05004], lr: 0.052638, loss: 1.7947
2022-08-27 13:47:30 - train: epoch 0049, iter [01700, 05004], lr: 0.052607, loss: 1.6947
2022-08-27 13:48:04 - train: epoch 0049, iter [01800, 05004], lr: 0.052575, loss: 1.4876
2022-08-27 13:48:38 - train: epoch 0049, iter [01900, 05004], lr: 0.052544, loss: 1.6612
2022-08-27 13:49:12 - train: epoch 0049, iter [02000, 05004], lr: 0.052513, loss: 1.5236
2022-08-27 13:49:47 - train: epoch 0049, iter [02100, 05004], lr: 0.052481, loss: 1.5431
2022-08-27 13:50:21 - train: epoch 0049, iter [02200, 05004], lr: 0.052450, loss: 1.5999
2022-08-27 13:50:55 - train: epoch 0049, iter [02300, 05004], lr: 0.052419, loss: 1.5413
2022-08-27 13:51:30 - train: epoch 0049, iter [02400, 05004], lr: 0.052387, loss: 1.6984
2022-08-27 13:52:04 - train: epoch 0049, iter [02500, 05004], lr: 0.052356, loss: 1.6037
2022-08-27 13:52:38 - train: epoch 0049, iter [02600, 05004], lr: 0.052325, loss: 1.7298
2022-08-27 13:53:12 - train: epoch 0049, iter [02700, 05004], lr: 0.052293, loss: 1.4202
2022-08-27 13:53:46 - train: epoch 0049, iter [02800, 05004], lr: 0.052262, loss: 1.5253
2022-08-27 13:54:21 - train: epoch 0049, iter [02900, 05004], lr: 0.052231, loss: 1.7057
2022-08-27 13:54:55 - train: epoch 0049, iter [03000, 05004], lr: 0.052199, loss: 1.8229
2022-08-27 13:55:30 - train: epoch 0049, iter [03100, 05004], lr: 0.052168, loss: 1.6731
2022-08-27 13:56:03 - train: epoch 0049, iter [03200, 05004], lr: 0.052136, loss: 1.7417
2022-08-27 13:56:38 - train: epoch 0049, iter [03300, 05004], lr: 0.052105, loss: 1.6491
2022-08-27 13:57:12 - train: epoch 0049, iter [03400, 05004], lr: 0.052074, loss: 1.8428
2022-08-27 13:57:47 - train: epoch 0049, iter [03500, 05004], lr: 0.052042, loss: 1.7430
2022-08-27 13:58:21 - train: epoch 0049, iter [03600, 05004], lr: 0.052011, loss: 1.8102
2022-08-27 13:58:54 - train: epoch 0049, iter [03700, 05004], lr: 0.051980, loss: 1.6117
2022-08-27 13:59:29 - train: epoch 0049, iter [03800, 05004], lr: 0.051948, loss: 1.9040
2022-08-27 14:00:04 - train: epoch 0049, iter [03900, 05004], lr: 0.051917, loss: 1.9207
2022-08-27 14:00:38 - train: epoch 0049, iter [04000, 05004], lr: 0.051886, loss: 1.6101
2022-08-27 14:01:11 - train: epoch 0049, iter [04100, 05004], lr: 0.051854, loss: 1.4883
2022-08-27 14:01:46 - train: epoch 0049, iter [04200, 05004], lr: 0.051823, loss: 1.6984
2022-08-27 14:02:20 - train: epoch 0049, iter [04300, 05004], lr: 0.051791, loss: 1.9320
2022-08-27 14:02:54 - train: epoch 0049, iter [04400, 05004], lr: 0.051760, loss: 1.7160
2022-08-27 14:03:28 - train: epoch 0049, iter [04500, 05004], lr: 0.051729, loss: 1.5838
2022-08-27 14:04:03 - train: epoch 0049, iter [04600, 05004], lr: 0.051697, loss: 1.7639
2022-08-27 14:04:37 - train: epoch 0049, iter [04700, 05004], lr: 0.051666, loss: 1.7893
2022-08-27 14:05:12 - train: epoch 0049, iter [04800, 05004], lr: 0.051635, loss: 1.4174
2022-08-27 14:05:46 - train: epoch 0049, iter [04900, 05004], lr: 0.051603, loss: 1.4603
2022-08-27 14:06:19 - train: epoch 0049, iter [05000, 05004], lr: 0.051572, loss: 1.5585
2022-08-27 14:06:20 - train: epoch 049, train_loss: 1.6404
2022-08-27 14:07:34 - eval: epoch: 049, acc1: 60.442%, acc5: 83.748%, test_loss: 1.6550, per_image_load_time: 0.890ms, per_image_inference_time: 0.511ms
2022-08-27 14:07:34 - until epoch: 049, best_acc1: 63.222%
2022-08-27 14:07:34 - epoch 050 lr: 0.051570
2022-08-27 14:08:13 - train: epoch 0050, iter [00100, 05004], lr: 0.051539, loss: 1.8320
2022-08-27 14:08:46 - train: epoch 0050, iter [00200, 05004], lr: 0.051508, loss: 1.5905
2022-08-27 14:09:20 - train: epoch 0050, iter [00300, 05004], lr: 0.051476, loss: 1.6372
2022-08-27 14:09:54 - train: epoch 0050, iter [00400, 05004], lr: 0.051445, loss: 1.4273
2022-08-27 14:10:28 - train: epoch 0050, iter [00500, 05004], lr: 0.051414, loss: 1.6775
2022-08-27 14:11:02 - train: epoch 0050, iter [00600, 05004], lr: 0.051382, loss: 1.7232
2022-08-27 14:11:36 - train: epoch 0050, iter [00700, 05004], lr: 0.051351, loss: 1.4536
2022-08-27 14:12:10 - train: epoch 0050, iter [00800, 05004], lr: 0.051320, loss: 1.2662
2022-08-27 14:12:45 - train: epoch 0050, iter [00900, 05004], lr: 0.051288, loss: 1.4779
2022-08-27 14:13:18 - train: epoch 0050, iter [01000, 05004], lr: 0.051257, loss: 1.7266
2022-08-27 14:13:52 - train: epoch 0050, iter [01100, 05004], lr: 0.051225, loss: 1.7039
2022-08-27 14:14:27 - train: epoch 0050, iter [01200, 05004], lr: 0.051194, loss: 1.5317
2022-08-27 14:15:01 - train: epoch 0050, iter [01300, 05004], lr: 0.051163, loss: 1.4994
2022-08-27 14:15:34 - train: epoch 0050, iter [01400, 05004], lr: 0.051131, loss: 1.6230
2022-08-27 14:16:08 - train: epoch 0050, iter [01500, 05004], lr: 0.051100, loss: 1.5640
2022-08-27 14:16:42 - train: epoch 0050, iter [01600, 05004], lr: 0.051068, loss: 1.4978
2022-08-27 14:17:16 - train: epoch 0050, iter [01700, 05004], lr: 0.051037, loss: 1.7575
2022-08-27 14:17:50 - train: epoch 0050, iter [01800, 05004], lr: 0.051006, loss: 1.7212
2022-08-27 14:18:23 - train: epoch 0050, iter [01900, 05004], lr: 0.050974, loss: 1.5855
2022-08-27 14:18:57 - train: epoch 0050, iter [02000, 05004], lr: 0.050943, loss: 1.6911
2022-08-27 14:19:31 - train: epoch 0050, iter [02100, 05004], lr: 0.050912, loss: 1.4049
2022-08-27 14:20:05 - train: epoch 0050, iter [02200, 05004], lr: 0.050880, loss: 1.7003
2022-08-27 14:20:39 - train: epoch 0050, iter [02300, 05004], lr: 0.050849, loss: 1.5674
2022-08-27 14:21:13 - train: epoch 0050, iter [02400, 05004], lr: 0.050817, loss: 1.5386
2022-08-27 14:21:46 - train: epoch 0050, iter [02500, 05004], lr: 0.050786, loss: 1.6951
2022-08-27 14:22:20 - train: epoch 0050, iter [02600, 05004], lr: 0.050755, loss: 1.5585
2022-08-27 14:22:53 - train: epoch 0050, iter [02700, 05004], lr: 0.050723, loss: 1.7002
2022-08-27 14:23:28 - train: epoch 0050, iter [02800, 05004], lr: 0.050692, loss: 1.7647
2022-08-27 14:24:01 - train: epoch 0050, iter [02900, 05004], lr: 0.050660, loss: 1.8501
2022-08-27 14:24:36 - train: epoch 0050, iter [03000, 05004], lr: 0.050629, loss: 1.7518
2022-08-27 14:25:09 - train: epoch 0050, iter [03100, 05004], lr: 0.050598, loss: 1.5569
2022-08-27 14:25:43 - train: epoch 0050, iter [03200, 05004], lr: 0.050566, loss: 1.6509
2022-08-27 14:26:17 - train: epoch 0050, iter [03300, 05004], lr: 0.050535, loss: 1.4942
2022-08-27 14:26:52 - train: epoch 0050, iter [03400, 05004], lr: 0.050504, loss: 1.4656
2022-08-27 14:27:25 - train: epoch 0050, iter [03500, 05004], lr: 0.050472, loss: 1.5663
2022-08-27 14:28:00 - train: epoch 0050, iter [03600, 05004], lr: 0.050441, loss: 1.7806
2022-08-27 14:28:34 - train: epoch 0050, iter [03700, 05004], lr: 0.050409, loss: 1.7177
2022-08-27 14:29:07 - train: epoch 0050, iter [03800, 05004], lr: 0.050378, loss: 1.5197
2022-08-27 14:29:41 - train: epoch 0050, iter [03900, 05004], lr: 0.050347, loss: 1.4617
2022-08-27 14:30:16 - train: epoch 0050, iter [04000, 05004], lr: 0.050315, loss: 1.7043
2022-08-27 14:30:49 - train: epoch 0050, iter [04100, 05004], lr: 0.050284, loss: 1.5694
2022-08-27 14:31:24 - train: epoch 0050, iter [04200, 05004], lr: 0.050252, loss: 1.7561
2022-08-27 14:31:58 - train: epoch 0050, iter [04300, 05004], lr: 0.050221, loss: 1.6101
2022-08-27 14:32:32 - train: epoch 0050, iter [04400, 05004], lr: 0.050190, loss: 1.5942
2022-08-27 14:33:06 - train: epoch 0050, iter [04500, 05004], lr: 0.050158, loss: 1.6303
2022-08-27 14:33:41 - train: epoch 0050, iter [04600, 05004], lr: 0.050127, loss: 1.6082
2022-08-27 14:34:14 - train: epoch 0050, iter [04700, 05004], lr: 0.050095, loss: 1.7051
2022-08-27 14:34:49 - train: epoch 0050, iter [04800, 05004], lr: 0.050064, loss: 1.3817
2022-08-27 14:35:23 - train: epoch 0050, iter [04900, 05004], lr: 0.050033, loss: 1.4668
2022-08-27 14:35:56 - train: epoch 0050, iter [05000, 05004], lr: 0.050001, loss: 1.5859
2022-08-27 14:35:57 - train: epoch 050, train_loss: 1.6250
2022-08-27 14:37:12 - eval: epoch: 050, acc1: 64.468%, acc5: 86.092%, test_loss: 1.4760, per_image_load_time: 1.550ms, per_image_inference_time: 0.519ms
2022-08-27 14:37:12 - until epoch: 050, best_acc1: 64.468%
2022-08-27 14:37:12 - epoch 051 lr: 0.050000
2022-08-27 14:37:51 - train: epoch 0051, iter [00100, 05004], lr: 0.049969, loss: 1.7569
2022-08-27 14:38:25 - train: epoch 0051, iter [00200, 05004], lr: 0.049937, loss: 1.9618
2022-08-27 14:38:59 - train: epoch 0051, iter [00300, 05004], lr: 0.049906, loss: 1.6282
2022-08-27 14:39:33 - train: epoch 0051, iter [00400, 05004], lr: 0.049874, loss: 1.4436
2022-08-27 14:40:07 - train: epoch 0051, iter [00500, 05004], lr: 0.049843, loss: 1.5337
2022-08-27 14:40:42 - train: epoch 0051, iter [00600, 05004], lr: 0.049812, loss: 1.5908
2022-08-27 14:41:17 - train: epoch 0051, iter [00700, 05004], lr: 0.049780, loss: 1.6231
2022-08-27 14:41:51 - train: epoch 0051, iter [00800, 05004], lr: 0.049749, loss: 1.9438
2022-08-27 14:42:24 - train: epoch 0051, iter [00900, 05004], lr: 0.049717, loss: 1.5206
2022-08-27 14:42:58 - train: epoch 0051, iter [01000, 05004], lr: 0.049686, loss: 1.8261
2022-08-27 14:43:32 - train: epoch 0051, iter [01100, 05004], lr: 0.049655, loss: 1.7830
2022-08-27 14:44:07 - train: epoch 0051, iter [01200, 05004], lr: 0.049623, loss: 1.5596
2022-08-27 14:44:42 - train: epoch 0051, iter [01300, 05004], lr: 0.049592, loss: 1.3019
2022-08-27 14:45:16 - train: epoch 0051, iter [01400, 05004], lr: 0.049561, loss: 1.4046
2022-08-27 14:45:50 - train: epoch 0051, iter [01500, 05004], lr: 0.049529, loss: 1.6061
2022-08-27 14:46:23 - train: epoch 0051, iter [01600, 05004], lr: 0.049498, loss: 1.5103
2022-08-27 14:46:57 - train: epoch 0051, iter [01700, 05004], lr: 0.049466, loss: 1.6755
2022-08-27 14:47:31 - train: epoch 0051, iter [01800, 05004], lr: 0.049435, loss: 1.6051
2022-08-27 14:48:06 - train: epoch 0051, iter [01900, 05004], lr: 0.049404, loss: 1.3515
2022-08-27 14:48:40 - train: epoch 0051, iter [02000, 05004], lr: 0.049372, loss: 1.5900
2022-08-27 14:49:14 - train: epoch 0051, iter [02100, 05004], lr: 0.049341, loss: 1.5859
2022-08-27 14:49:48 - train: epoch 0051, iter [02200, 05004], lr: 0.049309, loss: 1.6594
2022-08-27 14:50:23 - train: epoch 0051, iter [02300, 05004], lr: 0.049278, loss: 1.6951
2022-08-27 14:50:57 - train: epoch 0051, iter [02400, 05004], lr: 0.049247, loss: 1.7697
2022-08-27 14:51:31 - train: epoch 0051, iter [02500, 05004], lr: 0.049215, loss: 1.5342
2022-08-27 14:52:06 - train: epoch 0051, iter [02600, 05004], lr: 0.049184, loss: 1.4739
2022-08-27 14:52:41 - train: epoch 0051, iter [02700, 05004], lr: 0.049152, loss: 1.7049
2022-08-27 14:53:15 - train: epoch 0051, iter [02800, 05004], lr: 0.049121, loss: 1.4727
2022-08-27 14:53:50 - train: epoch 0051, iter [02900, 05004], lr: 0.049090, loss: 1.5266
2022-08-27 14:54:24 - train: epoch 0051, iter [03000, 05004], lr: 0.049058, loss: 1.5309
2022-08-27 14:54:58 - train: epoch 0051, iter [03100, 05004], lr: 0.049027, loss: 1.4627
2022-08-27 14:55:33 - train: epoch 0051, iter [03200, 05004], lr: 0.048996, loss: 1.5379
2022-08-27 14:56:07 - train: epoch 0051, iter [03300, 05004], lr: 0.048964, loss: 1.6795
2022-08-27 14:56:41 - train: epoch 0051, iter [03400, 05004], lr: 0.048933, loss: 1.5931
2022-08-27 14:57:16 - train: epoch 0051, iter [03500, 05004], lr: 0.048901, loss: 1.5660
2022-08-27 14:57:50 - train: epoch 0051, iter [03600, 05004], lr: 0.048870, loss: 1.5262
2022-08-27 14:58:25 - train: epoch 0051, iter [03700, 05004], lr: 0.048839, loss: 1.7630
2022-08-27 14:58:59 - train: epoch 0051, iter [03800, 05004], lr: 0.048807, loss: 1.5908
2022-08-27 14:59:33 - train: epoch 0051, iter [03900, 05004], lr: 0.048776, loss: 1.7399
2022-08-27 15:00:08 - train: epoch 0051, iter [04000, 05004], lr: 0.048744, loss: 1.5534
2022-08-27 15:00:42 - train: epoch 0051, iter [04100, 05004], lr: 0.048713, loss: 1.7187
2022-08-27 15:01:17 - train: epoch 0051, iter [04200, 05004], lr: 0.048682, loss: 1.8294
2022-08-27 15:01:51 - train: epoch 0051, iter [04300, 05004], lr: 0.048650, loss: 1.4677
2022-08-27 15:02:25 - train: epoch 0051, iter [04400, 05004], lr: 0.048619, loss: 1.6464
2022-08-27 15:03:00 - train: epoch 0051, iter [04500, 05004], lr: 0.048588, loss: 1.5548
2022-08-27 15:03:34 - train: epoch 0051, iter [04600, 05004], lr: 0.048556, loss: 1.6566
2022-08-27 15:04:09 - train: epoch 0051, iter [04700, 05004], lr: 0.048525, loss: 1.7293
2022-08-27 15:04:44 - train: epoch 0051, iter [04800, 05004], lr: 0.048493, loss: 1.4945
2022-08-27 15:05:18 - train: epoch 0051, iter [04900, 05004], lr: 0.048462, loss: 1.7082
2022-08-27 15:05:50 - train: epoch 0051, iter [05000, 05004], lr: 0.048431, loss: 1.5429
2022-08-27 15:05:51 - train: epoch 051, train_loss: 1.6123
2022-08-27 15:07:06 - eval: epoch: 051, acc1: 64.874%, acc5: 86.878%, test_loss: 1.4305, per_image_load_time: 2.388ms, per_image_inference_time: 0.503ms
2022-08-27 15:07:07 - until epoch: 051, best_acc1: 64.874%
2022-08-27 15:07:07 - epoch 052 lr: 0.048429
2022-08-27 15:07:45 - train: epoch 0052, iter [00100, 05004], lr: 0.048398, loss: 1.4580
2022-08-27 15:08:18 - train: epoch 0052, iter [00200, 05004], lr: 0.048367, loss: 1.6003
2022-08-27 15:08:52 - train: epoch 0052, iter [00300, 05004], lr: 0.048335, loss: 1.7602
2022-08-27 15:09:26 - train: epoch 0052, iter [00400, 05004], lr: 0.048304, loss: 1.7097
2022-08-27 15:10:00 - train: epoch 0052, iter [00500, 05004], lr: 0.048273, loss: 1.6533
2022-08-27 15:10:34 - train: epoch 0052, iter [00600, 05004], lr: 0.048241, loss: 1.4914
2022-08-27 15:11:09 - train: epoch 0052, iter [00700, 05004], lr: 0.048210, loss: 1.5887
2022-08-27 15:11:43 - train: epoch 0052, iter [00800, 05004], lr: 0.048178, loss: 1.6744
2022-08-27 15:12:17 - train: epoch 0052, iter [00900, 05004], lr: 0.048147, loss: 1.6584
2022-08-27 15:12:51 - train: epoch 0052, iter [01000, 05004], lr: 0.048116, loss: 1.7720
2022-08-27 15:13:25 - train: epoch 0052, iter [01100, 05004], lr: 0.048084, loss: 1.6060
2022-08-27 15:13:59 - train: epoch 0052, iter [01200, 05004], lr: 0.048053, loss: 1.3995
2022-08-27 15:14:33 - train: epoch 0052, iter [01300, 05004], lr: 0.048022, loss: 1.4984
2022-08-27 15:15:08 - train: epoch 0052, iter [01400, 05004], lr: 0.047990, loss: 1.9360
2022-08-27 15:15:42 - train: epoch 0052, iter [01500, 05004], lr: 0.047959, loss: 1.4965
2022-08-27 15:16:16 - train: epoch 0052, iter [01600, 05004], lr: 0.047928, loss: 1.4669
2022-08-27 15:16:49 - train: epoch 0052, iter [01700, 05004], lr: 0.047896, loss: 1.4655
2022-08-27 15:17:24 - train: epoch 0052, iter [01800, 05004], lr: 0.047865, loss: 1.4661
2022-08-27 15:17:58 - train: epoch 0052, iter [01900, 05004], lr: 0.047833, loss: 1.5477
2022-08-27 15:18:32 - train: epoch 0052, iter [02000, 05004], lr: 0.047802, loss: 1.8505
2022-08-27 15:19:06 - train: epoch 0052, iter [02100, 05004], lr: 0.047771, loss: 1.5697
2022-08-27 15:19:40 - train: epoch 0052, iter [02200, 05004], lr: 0.047739, loss: 1.6866
2022-08-27 15:20:15 - train: epoch 0052, iter [02300, 05004], lr: 0.047708, loss: 1.4000
2022-08-27 15:20:48 - train: epoch 0052, iter [02400, 05004], lr: 0.047677, loss: 1.3728
2022-08-27 15:21:22 - train: epoch 0052, iter [02500, 05004], lr: 0.047645, loss: 1.4487
2022-08-27 15:21:57 - train: epoch 0052, iter [02600, 05004], lr: 0.047614, loss: 1.3331
2022-08-27 15:22:30 - train: epoch 0052, iter [02700, 05004], lr: 0.047583, loss: 1.5244
2022-08-27 15:23:04 - train: epoch 0052, iter [02800, 05004], lr: 0.047551, loss: 1.6171
2022-08-27 15:23:39 - train: epoch 0052, iter [02900, 05004], lr: 0.047520, loss: 1.4566
2022-08-27 15:24:12 - train: epoch 0052, iter [03000, 05004], lr: 0.047489, loss: 1.5846
2022-08-27 15:24:46 - train: epoch 0052, iter [03100, 05004], lr: 0.047457, loss: 1.7671
2022-08-27 15:25:21 - train: epoch 0052, iter [03200, 05004], lr: 0.047426, loss: 1.7063
2022-08-27 15:25:55 - train: epoch 0052, iter [03300, 05004], lr: 0.047394, loss: 1.6480
2022-08-27 15:26:29 - train: epoch 0052, iter [03400, 05004], lr: 0.047363, loss: 1.6843
2022-08-27 15:27:02 - train: epoch 0052, iter [03500, 05004], lr: 0.047332, loss: 1.7068
2022-08-27 15:27:37 - train: epoch 0052, iter [03600, 05004], lr: 0.047300, loss: 1.6229
2022-08-27 15:28:12 - train: epoch 0052, iter [03700, 05004], lr: 0.047269, loss: 1.8122
2022-08-27 15:28:45 - train: epoch 0052, iter [03800, 05004], lr: 0.047238, loss: 1.4549
2022-08-27 15:29:19 - train: epoch 0052, iter [03900, 05004], lr: 0.047206, loss: 1.5240
2022-08-27 15:29:53 - train: epoch 0052, iter [04000, 05004], lr: 0.047175, loss: 1.7407
2022-08-27 15:30:26 - train: epoch 0052, iter [04100, 05004], lr: 0.047144, loss: 1.4177
2022-08-27 15:31:01 - train: epoch 0052, iter [04200, 05004], lr: 0.047112, loss: 1.5293
2022-08-27 15:31:35 - train: epoch 0052, iter [04300, 05004], lr: 0.047081, loss: 1.6249
2022-08-27 15:32:10 - train: epoch 0052, iter [04400, 05004], lr: 0.047050, loss: 1.6587
2022-08-27 15:32:44 - train: epoch 0052, iter [04500, 05004], lr: 0.047018, loss: 1.5416
2022-08-27 15:33:18 - train: epoch 0052, iter [04600, 05004], lr: 0.046987, loss: 1.7298
2022-08-27 15:33:51 - train: epoch 0052, iter [04700, 05004], lr: 0.046956, loss: 1.6410
2022-08-27 15:34:26 - train: epoch 0052, iter [04800, 05004], lr: 0.046924, loss: 1.3648
2022-08-27 15:35:00 - train: epoch 0052, iter [04900, 05004], lr: 0.046893, loss: 1.4937
2022-08-27 15:35:33 - train: epoch 0052, iter [05000, 05004], lr: 0.046862, loss: 1.6103
2022-08-27 15:35:34 - train: epoch 052, train_loss: 1.5982
2022-08-27 15:36:48 - eval: epoch: 052, acc1: 64.818%, acc5: 86.848%, test_loss: 1.4405, per_image_load_time: 2.293ms, per_image_inference_time: 0.534ms
2022-08-27 15:36:49 - until epoch: 052, best_acc1: 64.874%
2022-08-27 15:36:49 - epoch 053 lr: 0.046860
2022-08-27 15:37:27 - train: epoch 0053, iter [00100, 05004], lr: 0.046829, loss: 1.4103
2022-08-27 15:38:01 - train: epoch 0053, iter [00200, 05004], lr: 0.046798, loss: 1.8454
2022-08-27 15:38:34 - train: epoch 0053, iter [00300, 05004], lr: 0.046766, loss: 1.5247
2022-08-27 15:39:07 - train: epoch 0053, iter [00400, 05004], lr: 0.046735, loss: 1.7049
2022-08-27 15:39:40 - train: epoch 0053, iter [00500, 05004], lr: 0.046704, loss: 1.7287
2022-08-27 15:40:14 - train: epoch 0053, iter [00600, 05004], lr: 0.046673, loss: 1.6026
2022-08-27 15:40:47 - train: epoch 0053, iter [00700, 05004], lr: 0.046641, loss: 1.4106
2022-08-27 15:41:21 - train: epoch 0053, iter [00800, 05004], lr: 0.046610, loss: 1.6245
2022-08-27 15:41:55 - train: epoch 0053, iter [00900, 05004], lr: 0.046579, loss: 1.5170
2022-08-27 15:42:30 - train: epoch 0053, iter [01000, 05004], lr: 0.046547, loss: 1.4946
2022-08-27 15:43:03 - train: epoch 0053, iter [01100, 05004], lr: 0.046516, loss: 1.5099
2022-08-27 15:43:36 - train: epoch 0053, iter [01200, 05004], lr: 0.046485, loss: 1.4953
2022-08-27 15:44:10 - train: epoch 0053, iter [01300, 05004], lr: 0.046453, loss: 1.5352
2022-08-27 15:44:43 - train: epoch 0053, iter [01400, 05004], lr: 0.046422, loss: 1.9954
2022-08-27 15:45:17 - train: epoch 0053, iter [01500, 05004], lr: 0.046391, loss: 1.5421
2022-08-27 15:45:50 - train: epoch 0053, iter [01600, 05004], lr: 0.046359, loss: 1.7827
2022-08-27 15:46:24 - train: epoch 0053, iter [01700, 05004], lr: 0.046328, loss: 1.7341
2022-08-27 15:46:58 - train: epoch 0053, iter [01800, 05004], lr: 0.046297, loss: 1.8176
2022-08-27 15:47:32 - train: epoch 0053, iter [01900, 05004], lr: 0.046265, loss: 1.4321
2022-08-27 15:48:05 - train: epoch 0053, iter [02000, 05004], lr: 0.046234, loss: 1.6121
2022-08-27 15:48:39 - train: epoch 0053, iter [02100, 05004], lr: 0.046203, loss: 1.6277
2022-08-27 15:49:13 - train: epoch 0053, iter [02200, 05004], lr: 0.046172, loss: 1.5777
2022-08-27 15:49:47 - train: epoch 0053, iter [02300, 05004], lr: 0.046140, loss: 1.4920
2022-08-27 15:50:21 - train: epoch 0053, iter [02400, 05004], lr: 0.046109, loss: 1.6621
2022-08-27 15:50:55 - train: epoch 0053, iter [02500, 05004], lr: 0.046078, loss: 1.7567
2022-08-27 15:51:28 - train: epoch 0053, iter [02600, 05004], lr: 0.046046, loss: 1.6328
2022-08-27 15:52:02 - train: epoch 0053, iter [02700, 05004], lr: 0.046015, loss: 1.8909
2022-08-27 15:52:37 - train: epoch 0053, iter [02800, 05004], lr: 0.045984, loss: 1.6974
2022-08-27 15:53:10 - train: epoch 0053, iter [02900, 05004], lr: 0.045953, loss: 1.4661
2022-08-27 15:53:45 - train: epoch 0053, iter [03000, 05004], lr: 0.045921, loss: 1.4548
2022-08-27 15:54:19 - train: epoch 0053, iter [03100, 05004], lr: 0.045890, loss: 1.8131
2022-08-27 15:54:52 - train: epoch 0053, iter [03200, 05004], lr: 0.045859, loss: 1.8812
2022-08-27 15:55:26 - train: epoch 0053, iter [03300, 05004], lr: 0.045827, loss: 1.5082
2022-08-27 15:56:00 - train: epoch 0053, iter [03400, 05004], lr: 0.045796, loss: 1.6854
2022-08-27 15:56:34 - train: epoch 0053, iter [03500, 05004], lr: 0.045765, loss: 1.6062
2022-08-27 15:57:08 - train: epoch 0053, iter [03600, 05004], lr: 0.045734, loss: 1.7182
2022-08-27 15:57:42 - train: epoch 0053, iter [03700, 05004], lr: 0.045702, loss: 1.7503
2022-08-27 15:58:16 - train: epoch 0053, iter [03800, 05004], lr: 0.045671, loss: 1.6129
2022-08-27 15:58:50 - train: epoch 0053, iter [03900, 05004], lr: 0.045640, loss: 1.7640
2022-08-27 15:59:24 - train: epoch 0053, iter [04000, 05004], lr: 0.045608, loss: 1.5306
2022-08-27 15:59:58 - train: epoch 0053, iter [04100, 05004], lr: 0.045577, loss: 1.6174
2022-08-27 16:00:31 - train: epoch 0053, iter [04200, 05004], lr: 0.045546, loss: 1.5936
2022-08-27 16:01:06 - train: epoch 0053, iter [04300, 05004], lr: 0.045515, loss: 1.8039
2022-08-27 16:01:40 - train: epoch 0053, iter [04400, 05004], lr: 0.045483, loss: 1.5615
2022-08-27 16:02:14 - train: epoch 0053, iter [04500, 05004], lr: 0.045452, loss: 1.7601
2022-08-27 16:02:49 - train: epoch 0053, iter [04600, 05004], lr: 0.045421, loss: 1.4403
2022-08-27 16:03:23 - train: epoch 0053, iter [04700, 05004], lr: 0.045390, loss: 1.7788
2022-08-27 16:03:57 - train: epoch 0053, iter [04800, 05004], lr: 0.045358, loss: 1.8159
2022-08-27 16:04:31 - train: epoch 0053, iter [04900, 05004], lr: 0.045327, loss: 1.5418
2022-08-27 16:05:04 - train: epoch 0053, iter [05000, 05004], lr: 0.045296, loss: 1.5046
2022-08-27 16:05:05 - train: epoch 053, train_loss: 1.5845
2022-08-27 16:06:20 - eval: epoch: 053, acc1: 65.716%, acc5: 87.444%, test_loss: 1.3996, per_image_load_time: 2.048ms, per_image_inference_time: 0.523ms
2022-08-27 16:06:21 - until epoch: 053, best_acc1: 65.716%
2022-08-27 16:06:21 - epoch 054 lr: 0.045294
2022-08-27 16:07:00 - train: epoch 0054, iter [00100, 05004], lr: 0.045263, loss: 1.4530
2022-08-27 16:07:33 - train: epoch 0054, iter [00200, 05004], lr: 0.045232, loss: 1.9069
2022-08-27 16:08:05 - train: epoch 0054, iter [00300, 05004], lr: 0.045201, loss: 1.6390
2022-08-27 16:08:39 - train: epoch 0054, iter [00400, 05004], lr: 0.045170, loss: 1.3526
2022-08-27 16:09:12 - train: epoch 0054, iter [00500, 05004], lr: 0.045138, loss: 1.5461
2022-08-27 16:09:44 - train: epoch 0054, iter [00600, 05004], lr: 0.045107, loss: 1.4255
2022-08-27 16:10:19 - train: epoch 0054, iter [00700, 05004], lr: 0.045076, loss: 1.6589
2022-08-27 16:10:52 - train: epoch 0054, iter [00800, 05004], lr: 0.045045, loss: 1.7201
2022-08-27 16:11:26 - train: epoch 0054, iter [00900, 05004], lr: 0.045013, loss: 1.4550
2022-08-27 16:12:00 - train: epoch 0054, iter [01000, 05004], lr: 0.044982, loss: 1.3539
2022-08-27 16:12:33 - train: epoch 0054, iter [01100, 05004], lr: 0.044951, loss: 1.4781
2022-08-27 16:13:07 - train: epoch 0054, iter [01200, 05004], lr: 0.044920, loss: 1.6803
2022-08-27 16:13:40 - train: epoch 0054, iter [01300, 05004], lr: 0.044888, loss: 1.5188
2022-08-27 16:14:15 - train: epoch 0054, iter [01400, 05004], lr: 0.044857, loss: 1.5552
2022-08-27 16:14:48 - train: epoch 0054, iter [01500, 05004], lr: 0.044826, loss: 1.6109
2022-08-27 16:15:22 - train: epoch 0054, iter [01600, 05004], lr: 0.044795, loss: 1.2345
2022-08-27 16:15:56 - train: epoch 0054, iter [01700, 05004], lr: 0.044764, loss: 1.5744
2022-08-27 16:16:30 - train: epoch 0054, iter [01800, 05004], lr: 0.044732, loss: 1.6187
2022-08-27 16:17:04 - train: epoch 0054, iter [01900, 05004], lr: 0.044701, loss: 1.9810
2022-08-27 16:17:38 - train: epoch 0054, iter [02000, 05004], lr: 0.044670, loss: 1.6355
2022-08-27 16:18:12 - train: epoch 0054, iter [02100, 05004], lr: 0.044639, loss: 1.3845
2022-08-27 16:18:46 - train: epoch 0054, iter [02200, 05004], lr: 0.044608, loss: 1.6534
2022-08-27 16:19:19 - train: epoch 0054, iter [02300, 05004], lr: 0.044576, loss: 1.6177
2022-08-27 16:19:54 - train: epoch 0054, iter [02400, 05004], lr: 0.044545, loss: 1.4664
2022-08-27 16:20:29 - train: epoch 0054, iter [02500, 05004], lr: 0.044514, loss: 1.6581
2022-08-27 16:21:01 - train: epoch 0054, iter [02600, 05004], lr: 0.044483, loss: 1.4072
2022-08-27 16:21:37 - train: epoch 0054, iter [02700, 05004], lr: 0.044452, loss: 1.5846
2022-08-27 16:22:10 - train: epoch 0054, iter [02800, 05004], lr: 0.044420, loss: 1.8183
2022-08-27 16:22:44 - train: epoch 0054, iter [02900, 05004], lr: 0.044389, loss: 1.5461
2022-08-27 16:23:19 - train: epoch 0054, iter [03000, 05004], lr: 0.044358, loss: 1.6316
2022-08-27 16:23:53 - train: epoch 0054, iter [03100, 05004], lr: 0.044327, loss: 1.5379
2022-08-27 16:24:27 - train: epoch 0054, iter [03200, 05004], lr: 0.044296, loss: 1.6703
2022-08-27 16:25:01 - train: epoch 0054, iter [03300, 05004], lr: 0.044264, loss: 1.5368
2022-08-27 16:25:35 - train: epoch 0054, iter [03400, 05004], lr: 0.044233, loss: 1.6000
2022-08-27 16:26:09 - train: epoch 0054, iter [03500, 05004], lr: 0.044202, loss: 1.7459
2022-08-27 16:26:42 - train: epoch 0054, iter [03600, 05004], lr: 0.044171, loss: 1.5132
2022-08-27 16:27:17 - train: epoch 0054, iter [03700, 05004], lr: 0.044140, loss: 1.4798
2022-08-27 16:27:51 - train: epoch 0054, iter [03800, 05004], lr: 0.044108, loss: 1.5679
2022-08-27 16:28:26 - train: epoch 0054, iter [03900, 05004], lr: 0.044077, loss: 1.6107
2022-08-27 16:28:59 - train: epoch 0054, iter [04000, 05004], lr: 0.044046, loss: 1.4251
2022-08-27 16:29:33 - train: epoch 0054, iter [04100, 05004], lr: 0.044015, loss: 1.6821
2022-08-27 16:30:08 - train: epoch 0054, iter [04200, 05004], lr: 0.043984, loss: 1.6337
2022-08-27 16:30:42 - train: epoch 0054, iter [04300, 05004], lr: 0.043953, loss: 1.5559
2022-08-27 16:31:16 - train: epoch 0054, iter [04400, 05004], lr: 0.043921, loss: 1.4358
2022-08-27 16:31:50 - train: epoch 0054, iter [04500, 05004], lr: 0.043890, loss: 1.4725
2022-08-27 16:32:24 - train: epoch 0054, iter [04600, 05004], lr: 0.043859, loss: 1.5919
2022-08-27 16:32:58 - train: epoch 0054, iter [04700, 05004], lr: 0.043828, loss: 1.8227
2022-08-27 16:33:32 - train: epoch 0054, iter [04800, 05004], lr: 0.043797, loss: 1.6570
2022-08-27 16:34:06 - train: epoch 0054, iter [04900, 05004], lr: 0.043766, loss: 1.4544
2022-08-27 16:34:39 - train: epoch 0054, iter [05000, 05004], lr: 0.043735, loss: 1.6358
2022-08-27 16:34:40 - train: epoch 054, train_loss: 1.5715
2022-08-27 16:35:56 - eval: epoch: 054, acc1: 60.514%, acc5: 83.818%, test_loss: 1.6684, per_image_load_time: 2.406ms, per_image_inference_time: 0.525ms
2022-08-27 16:35:56 - until epoch: 054, best_acc1: 65.716%
2022-08-27 16:35:56 - epoch 055 lr: 0.043733
2022-08-27 16:36:36 - train: epoch 0055, iter [00100, 05004], lr: 0.043702, loss: 1.4638
2022-08-27 16:37:09 - train: epoch 0055, iter [00200, 05004], lr: 0.043671, loss: 1.3641
2022-08-27 16:37:43 - train: epoch 0055, iter [00300, 05004], lr: 0.043640, loss: 1.3661
2022-08-27 16:38:18 - train: epoch 0055, iter [00400, 05004], lr: 0.043609, loss: 1.4959
2022-08-27 16:38:50 - train: epoch 0055, iter [00500, 05004], lr: 0.043578, loss: 1.4045
2022-08-27 16:39:25 - train: epoch 0055, iter [00600, 05004], lr: 0.043547, loss: 1.4399
2022-08-27 16:39:59 - train: epoch 0055, iter [00700, 05004], lr: 0.043515, loss: 1.7146
2022-08-27 16:40:33 - train: epoch 0055, iter [00800, 05004], lr: 0.043484, loss: 1.3905
2022-08-27 16:41:07 - train: epoch 0055, iter [00900, 05004], lr: 0.043453, loss: 1.4990
2022-08-27 16:41:40 - train: epoch 0055, iter [01000, 05004], lr: 0.043422, loss: 1.5325
2022-08-27 16:42:14 - train: epoch 0055, iter [01100, 05004], lr: 0.043391, loss: 1.6625
2022-08-27 16:42:48 - train: epoch 0055, iter [01200, 05004], lr: 0.043360, loss: 1.5948
2022-08-27 16:43:23 - train: epoch 0055, iter [01300, 05004], lr: 0.043329, loss: 1.6833
2022-08-27 16:43:57 - train: epoch 0055, iter [01400, 05004], lr: 0.043298, loss: 1.4310
2022-08-27 16:44:31 - train: epoch 0055, iter [01500, 05004], lr: 0.043266, loss: 1.5797
2022-08-27 16:45:06 - train: epoch 0055, iter [01600, 05004], lr: 0.043235, loss: 1.6455
2022-08-27 16:45:40 - train: epoch 0055, iter [01700, 05004], lr: 0.043204, loss: 1.6033
2022-08-27 16:46:14 - train: epoch 0055, iter [01800, 05004], lr: 0.043173, loss: 1.6441
2022-08-27 16:46:48 - train: epoch 0055, iter [01900, 05004], lr: 0.043142, loss: 1.5961
2022-08-27 16:47:23 - train: epoch 0055, iter [02000, 05004], lr: 0.043111, loss: 1.5312
2022-08-27 16:47:57 - train: epoch 0055, iter [02100, 05004], lr: 0.043080, loss: 1.3660
2022-08-27 16:48:30 - train: epoch 0055, iter [02200, 05004], lr: 0.043049, loss: 1.6292
2022-08-27 16:49:05 - train: epoch 0055, iter [02300, 05004], lr: 0.043018, loss: 1.4441
2022-08-27 16:49:40 - train: epoch 0055, iter [02400, 05004], lr: 0.042987, loss: 1.3826
2022-08-27 16:50:14 - train: epoch 0055, iter [02500, 05004], lr: 0.042956, loss: 1.6012
2022-08-27 16:50:48 - train: epoch 0055, iter [02600, 05004], lr: 0.042924, loss: 1.4274
2022-08-27 16:51:22 - train: epoch 0055, iter [02700, 05004], lr: 0.042893, loss: 1.5117
2022-08-27 16:51:57 - train: epoch 0055, iter [02800, 05004], lr: 0.042862, loss: 1.5725
2022-08-27 16:52:31 - train: epoch 0055, iter [02900, 05004], lr: 0.042831, loss: 1.6572
2022-08-27 16:53:05 - train: epoch 0055, iter [03000, 05004], lr: 0.042800, loss: 1.5795
2022-08-27 16:53:39 - train: epoch 0055, iter [03100, 05004], lr: 0.042769, loss: 1.5542
2022-08-27 16:54:13 - train: epoch 0055, iter [03200, 05004], lr: 0.042738, loss: 1.4159
2022-08-27 16:54:46 - train: epoch 0055, iter [03300, 05004], lr: 0.042707, loss: 1.2827
2022-08-27 16:55:21 - train: epoch 0055, iter [03400, 05004], lr: 0.042676, loss: 1.4806
2022-08-27 16:55:55 - train: epoch 0055, iter [03500, 05004], lr: 0.042645, loss: 1.3434
2022-08-27 16:56:29 - train: epoch 0055, iter [03600, 05004], lr: 0.042614, loss: 1.5572
2022-08-27 16:57:03 - train: epoch 0055, iter [03700, 05004], lr: 0.042583, loss: 1.5543
2022-08-27 16:57:36 - train: epoch 0055, iter [03800, 05004], lr: 0.042552, loss: 1.6135
2022-08-27 16:58:11 - train: epoch 0055, iter [03900, 05004], lr: 0.042521, loss: 1.9197
2022-08-27 16:58:45 - train: epoch 0055, iter [04000, 05004], lr: 0.042490, loss: 1.6025
2022-08-27 16:59:19 - train: epoch 0055, iter [04100, 05004], lr: 0.042459, loss: 1.5808
2022-08-27 16:59:53 - train: epoch 0055, iter [04200, 05004], lr: 0.042428, loss: 1.6068
2022-08-27 17:00:27 - train: epoch 0055, iter [04300, 05004], lr: 0.042397, loss: 1.6811
2022-08-27 17:01:01 - train: epoch 0055, iter [04400, 05004], lr: 0.042366, loss: 1.7781
2022-08-27 17:01:35 - train: epoch 0055, iter [04500, 05004], lr: 0.042335, loss: 1.5707
2022-08-27 17:02:09 - train: epoch 0055, iter [04600, 05004], lr: 0.042304, loss: 1.7157
2022-08-27 17:02:43 - train: epoch 0055, iter [04700, 05004], lr: 0.042273, loss: 1.3338
2022-08-27 17:03:18 - train: epoch 0055, iter [04800, 05004], lr: 0.042242, loss: 1.5222
2022-08-27 17:03:53 - train: epoch 0055, iter [04900, 05004], lr: 0.042211, loss: 1.4370
2022-08-27 17:04:25 - train: epoch 0055, iter [05000, 05004], lr: 0.042180, loss: 1.5680
2022-08-27 17:04:26 - train: epoch 055, train_loss: 1.5558
2022-08-27 17:05:41 - eval: epoch: 055, acc1: 66.620%, acc5: 87.740%, test_loss: 1.3623, per_image_load_time: 2.260ms, per_image_inference_time: 0.512ms
2022-08-27 17:05:41 - until epoch: 055, best_acc1: 66.620%
2022-08-27 17:05:41 - epoch 056 lr: 0.042178
2022-08-27 17:06:20 - train: epoch 0056, iter [00100, 05004], lr: 0.042147, loss: 1.5446
2022-08-27 17:06:54 - train: epoch 0056, iter [00200, 05004], lr: 0.042116, loss: 1.6760
2022-08-27 17:07:27 - train: epoch 0056, iter [00300, 05004], lr: 0.042085, loss: 1.5448
2022-08-27 17:08:01 - train: epoch 0056, iter [00400, 05004], lr: 0.042054, loss: 1.4970
2022-08-27 17:08:34 - train: epoch 0056, iter [00500, 05004], lr: 0.042023, loss: 1.3856
2022-08-27 17:09:08 - train: epoch 0056, iter [00600, 05004], lr: 0.041992, loss: 1.4445
2022-08-27 17:09:43 - train: epoch 0056, iter [00700, 05004], lr: 0.041961, loss: 1.5278
2022-08-27 17:10:16 - train: epoch 0056, iter [00800, 05004], lr: 0.041930, loss: 1.7162
2022-08-27 17:10:50 - train: epoch 0056, iter [00900, 05004], lr: 0.041899, loss: 1.5925
2022-08-27 17:11:24 - train: epoch 0056, iter [01000, 05004], lr: 0.041868, loss: 1.5621
2022-08-27 17:11:59 - train: epoch 0056, iter [01100, 05004], lr: 0.041837, loss: 1.4739
2022-08-27 17:12:32 - train: epoch 0056, iter [01200, 05004], lr: 0.041806, loss: 1.5363
2022-08-27 17:13:06 - train: epoch 0056, iter [01300, 05004], lr: 0.041775, loss: 1.6286
2022-08-27 17:13:40 - train: epoch 0056, iter [01400, 05004], lr: 0.041745, loss: 1.5048
2022-08-27 17:14:14 - train: epoch 0056, iter [01500, 05004], lr: 0.041714, loss: 1.7330
2022-08-27 17:14:48 - train: epoch 0056, iter [01600, 05004], lr: 0.041683, loss: 1.3717
2022-08-27 17:15:23 - train: epoch 0056, iter [01700, 05004], lr: 0.041652, loss: 1.6575
2022-08-27 17:15:57 - train: epoch 0056, iter [01800, 05004], lr: 0.041621, loss: 1.7830
2022-08-27 17:16:31 - train: epoch 0056, iter [01900, 05004], lr: 0.041590, loss: 1.5617
2022-08-27 17:17:04 - train: epoch 0056, iter [02000, 05004], lr: 0.041559, loss: 1.5108
2022-08-27 17:17:39 - train: epoch 0056, iter [02100, 05004], lr: 0.041528, loss: 1.5443
2022-08-27 17:18:13 - train: epoch 0056, iter [02200, 05004], lr: 0.041497, loss: 1.7424
2022-08-27 17:18:46 - train: epoch 0056, iter [02300, 05004], lr: 0.041466, loss: 1.6501
2022-08-27 17:19:20 - train: epoch 0056, iter [02400, 05004], lr: 0.041435, loss: 1.4895
2022-08-27 17:19:54 - train: epoch 0056, iter [02500, 05004], lr: 0.041404, loss: 1.7249
2022-08-27 17:20:28 - train: epoch 0056, iter [02600, 05004], lr: 0.041373, loss: 1.5000
2022-08-27 17:21:02 - train: epoch 0056, iter [02700, 05004], lr: 0.041342, loss: 1.6284
2022-08-27 17:21:35 - train: epoch 0056, iter [02800, 05004], lr: 0.041311, loss: 1.5393
2022-08-27 17:22:10 - train: epoch 0056, iter [02900, 05004], lr: 0.041280, loss: 1.7479
2022-08-27 17:22:44 - train: epoch 0056, iter [03000, 05004], lr: 0.041250, loss: 1.6422
2022-08-27 17:23:18 - train: epoch 0056, iter [03100, 05004], lr: 0.041219, loss: 1.3957
2022-08-27 17:23:52 - train: epoch 0056, iter [03200, 05004], lr: 0.041188, loss: 1.4411
2022-08-27 17:24:26 - train: epoch 0056, iter [03300, 05004], lr: 0.041157, loss: 1.8824
2022-08-27 17:25:00 - train: epoch 0056, iter [03400, 05004], lr: 0.041126, loss: 1.5594
2022-08-27 17:25:34 - train: epoch 0056, iter [03500, 05004], lr: 0.041095, loss: 1.4369
2022-08-27 17:26:08 - train: epoch 0056, iter [03600, 05004], lr: 0.041064, loss: 1.2691
2022-08-27 17:26:42 - train: epoch 0056, iter [03700, 05004], lr: 0.041033, loss: 1.4896
2022-08-27 17:27:16 - train: epoch 0056, iter [03800, 05004], lr: 0.041002, loss: 1.4306
2022-08-27 17:27:49 - train: epoch 0056, iter [03900, 05004], lr: 0.040972, loss: 1.7472
2022-08-27 17:28:24 - train: epoch 0056, iter [04000, 05004], lr: 0.040941, loss: 1.5908
2022-08-27 17:28:57 - train: epoch 0056, iter [04100, 05004], lr: 0.040910, loss: 1.7204
2022-08-27 17:29:31 - train: epoch 0056, iter [04200, 05004], lr: 0.040879, loss: 1.5071
2022-08-27 17:30:06 - train: epoch 0056, iter [04300, 05004], lr: 0.040848, loss: 1.4617
2022-08-27 17:30:40 - train: epoch 0056, iter [04400, 05004], lr: 0.040817, loss: 1.6533
2022-08-27 17:31:13 - train: epoch 0056, iter [04500, 05004], lr: 0.040786, loss: 1.5567
2022-08-27 17:31:48 - train: epoch 0056, iter [04600, 05004], lr: 0.040756, loss: 1.6303
2022-08-27 17:32:21 - train: epoch 0056, iter [04700, 05004], lr: 0.040725, loss: 1.6228
2022-08-27 17:32:55 - train: epoch 0056, iter [04800, 05004], lr: 0.040694, loss: 1.7458
2022-08-27 17:33:29 - train: epoch 0056, iter [04900, 05004], lr: 0.040663, loss: 1.5378
2022-08-27 17:34:02 - train: epoch 0056, iter [05000, 05004], lr: 0.040632, loss: 1.7247
2022-08-27 17:34:03 - train: epoch 056, train_loss: 1.5383
2022-08-27 17:35:18 - eval: epoch: 056, acc1: 65.756%, acc5: 87.238%, test_loss: 1.4117, per_image_load_time: 2.311ms, per_image_inference_time: 0.502ms
2022-08-27 17:35:19 - until epoch: 056, best_acc1: 66.620%
2022-08-27 17:35:19 - epoch 057 lr: 0.040631
2022-08-27 17:35:58 - train: epoch 0057, iter [00100, 05004], lr: 0.040600, loss: 1.6027
2022-08-27 17:36:32 - train: epoch 0057, iter [00200, 05004], lr: 0.040569, loss: 1.4051
2022-08-27 17:37:06 - train: epoch 0057, iter [00300, 05004], lr: 0.040538, loss: 1.4274
2022-08-27 17:37:40 - train: epoch 0057, iter [00400, 05004], lr: 0.040508, loss: 1.5259
2022-08-27 17:38:14 - train: epoch 0057, iter [00500, 05004], lr: 0.040477, loss: 1.2359
2022-08-27 17:38:47 - train: epoch 0057, iter [00600, 05004], lr: 0.040446, loss: 1.6430
2022-08-27 17:39:22 - train: epoch 0057, iter [00700, 05004], lr: 0.040415, loss: 1.2795
2022-08-27 17:39:56 - train: epoch 0057, iter [00800, 05004], lr: 0.040384, loss: 1.5063
2022-08-27 17:40:30 - train: epoch 0057, iter [00900, 05004], lr: 0.040354, loss: 1.4493
2022-08-27 17:41:05 - train: epoch 0057, iter [01000, 05004], lr: 0.040323, loss: 1.4652
2022-08-27 17:41:39 - train: epoch 0057, iter [01100, 05004], lr: 0.040292, loss: 1.5680
2022-08-27 17:42:12 - train: epoch 0057, iter [01200, 05004], lr: 0.040261, loss: 1.4011
2022-08-27 17:42:47 - train: epoch 0057, iter [01300, 05004], lr: 0.040230, loss: 1.5570
2022-08-27 17:43:21 - train: epoch 0057, iter [01400, 05004], lr: 0.040200, loss: 1.5977
2022-08-27 17:43:55 - train: epoch 0057, iter [01500, 05004], lr: 0.040169, loss: 1.7496
2022-08-27 17:44:29 - train: epoch 0057, iter [01600, 05004], lr: 0.040138, loss: 1.7717
2022-08-27 17:45:02 - train: epoch 0057, iter [01700, 05004], lr: 0.040107, loss: 1.7058
2022-08-27 17:45:37 - train: epoch 0057, iter [01800, 05004], lr: 0.040077, loss: 1.6775
2022-08-27 17:46:11 - train: epoch 0057, iter [01900, 05004], lr: 0.040046, loss: 1.4129
2022-08-27 17:46:45 - train: epoch 0057, iter [02000, 05004], lr: 0.040015, loss: 1.4906
2022-08-27 17:47:18 - train: epoch 0057, iter [02100, 05004], lr: 0.039984, loss: 1.5437
2022-08-27 17:47:52 - train: epoch 0057, iter [02200, 05004], lr: 0.039953, loss: 1.5349
2022-08-27 17:48:26 - train: epoch 0057, iter [02300, 05004], lr: 0.039923, loss: 1.3930
2022-08-27 17:48:59 - train: epoch 0057, iter [02400, 05004], lr: 0.039892, loss: 1.4684
2022-08-27 17:49:33 - train: epoch 0057, iter [02500, 05004], lr: 0.039861, loss: 1.6441
2022-08-27 17:50:07 - train: epoch 0057, iter [02600, 05004], lr: 0.039831, loss: 1.4480
2022-08-27 17:50:42 - train: epoch 0057, iter [02700, 05004], lr: 0.039800, loss: 1.2527
2022-08-27 17:51:15 - train: epoch 0057, iter [02800, 05004], lr: 0.039769, loss: 1.1583
2022-08-27 17:51:48 - train: epoch 0057, iter [02900, 05004], lr: 0.039738, loss: 1.6631
2022-08-27 17:52:23 - train: epoch 0057, iter [03000, 05004], lr: 0.039708, loss: 1.7364
2022-08-27 17:52:57 - train: epoch 0057, iter [03100, 05004], lr: 0.039677, loss: 1.8471
2022-08-27 17:53:31 - train: epoch 0057, iter [03200, 05004], lr: 0.039646, loss: 1.7407
2022-08-27 17:54:05 - train: epoch 0057, iter [03300, 05004], lr: 0.039615, loss: 1.3620
2022-08-27 17:54:39 - train: epoch 0057, iter [03400, 05004], lr: 0.039585, loss: 1.4912
2022-08-27 17:55:13 - train: epoch 0057, iter [03500, 05004], lr: 0.039554, loss: 1.5462
2022-08-27 17:55:47 - train: epoch 0057, iter [03600, 05004], lr: 0.039523, loss: 1.4150
2022-08-27 17:56:21 - train: epoch 0057, iter [03700, 05004], lr: 0.039493, loss: 1.4271
2022-08-27 17:56:55 - train: epoch 0057, iter [03800, 05004], lr: 0.039462, loss: 1.5259
2022-08-27 17:57:29 - train: epoch 0057, iter [03900, 05004], lr: 0.039431, loss: 1.6151
2022-08-27 17:58:03 - train: epoch 0057, iter [04000, 05004], lr: 0.039401, loss: 1.4940
2022-08-27 17:58:37 - train: epoch 0057, iter [04100, 05004], lr: 0.039370, loss: 1.5237
2022-08-27 17:59:11 - train: epoch 0057, iter [04200, 05004], lr: 0.039339, loss: 1.6180
2022-08-27 17:59:45 - train: epoch 0057, iter [04300, 05004], lr: 0.039309, loss: 1.4664
2022-08-27 18:00:19 - train: epoch 0057, iter [04400, 05004], lr: 0.039278, loss: 1.5844
2022-08-27 18:00:53 - train: epoch 0057, iter [04500, 05004], lr: 0.039247, loss: 1.6681
2022-08-27 18:01:27 - train: epoch 0057, iter [04600, 05004], lr: 0.039217, loss: 1.6287
2022-08-27 18:02:00 - train: epoch 0057, iter [04700, 05004], lr: 0.039186, loss: 1.4145
2022-08-27 18:02:35 - train: epoch 0057, iter [04800, 05004], lr: 0.039155, loss: 1.7533
2022-08-27 18:03:10 - train: epoch 0057, iter [04900, 05004], lr: 0.039125, loss: 1.7606
2022-08-27 18:03:42 - train: epoch 0057, iter [05000, 05004], lr: 0.039094, loss: 1.6502
2022-08-27 18:03:43 - train: epoch 057, train_loss: 1.5253
2022-08-27 18:04:58 - eval: epoch: 057, acc1: 65.150%, acc5: 87.040%, test_loss: 1.4298, per_image_load_time: 2.434ms, per_image_inference_time: 0.511ms
2022-08-27 18:04:59 - until epoch: 057, best_acc1: 66.620%
2022-08-27 18:04:59 - epoch 058 lr: 0.039093
2022-08-27 18:05:37 - train: epoch 0058, iter [00100, 05004], lr: 0.039062, loss: 1.5278
2022-08-27 18:06:12 - train: epoch 0058, iter [00200, 05004], lr: 0.039032, loss: 1.3094
2022-08-27 18:06:47 - train: epoch 0058, iter [00300, 05004], lr: 0.039001, loss: 1.5335
2022-08-27 18:07:20 - train: epoch 0058, iter [00400, 05004], lr: 0.038970, loss: 1.5382
2022-08-27 18:07:54 - train: epoch 0058, iter [00500, 05004], lr: 0.038940, loss: 1.3285
2022-08-27 18:08:28 - train: epoch 0058, iter [00600, 05004], lr: 0.038909, loss: 1.7570
2022-08-27 18:09:03 - train: epoch 0058, iter [00700, 05004], lr: 0.038879, loss: 1.4692
2022-08-27 18:09:36 - train: epoch 0058, iter [00800, 05004], lr: 0.038848, loss: 1.3725
2022-08-27 18:10:09 - train: epoch 0058, iter [00900, 05004], lr: 0.038817, loss: 1.3167
2022-08-27 18:10:44 - train: epoch 0058, iter [01000, 05004], lr: 0.038787, loss: 1.5927
2022-08-27 18:11:18 - train: epoch 0058, iter [01100, 05004], lr: 0.038756, loss: 1.2896
2022-08-27 18:11:52 - train: epoch 0058, iter [01200, 05004], lr: 0.038726, loss: 1.3958
2022-08-27 18:12:27 - train: epoch 0058, iter [01300, 05004], lr: 0.038695, loss: 1.5619
2022-08-27 18:13:01 - train: epoch 0058, iter [01400, 05004], lr: 0.038664, loss: 1.5316
2022-08-27 18:13:35 - train: epoch 0058, iter [01500, 05004], lr: 0.038634, loss: 1.5041
2022-08-27 18:14:09 - train: epoch 0058, iter [01600, 05004], lr: 0.038603, loss: 1.4433
2022-08-27 18:14:43 - train: epoch 0058, iter [01700, 05004], lr: 0.038573, loss: 1.8064
2022-08-27 18:15:17 - train: epoch 0058, iter [01800, 05004], lr: 0.038542, loss: 1.5123
2022-08-27 18:15:51 - train: epoch 0058, iter [01900, 05004], lr: 0.038512, loss: 1.6741
2022-08-27 18:16:25 - train: epoch 0058, iter [02000, 05004], lr: 0.038481, loss: 1.6230
2022-08-27 18:16:59 - train: epoch 0058, iter [02100, 05004], lr: 0.038450, loss: 1.3624
2022-08-27 18:17:34 - train: epoch 0058, iter [02200, 05004], lr: 0.038420, loss: 1.2897
2022-08-27 18:18:08 - train: epoch 0058, iter [02300, 05004], lr: 0.038389, loss: 1.5476
2022-08-27 18:18:41 - train: epoch 0058, iter [02400, 05004], lr: 0.038359, loss: 1.5915
2022-08-27 18:19:15 - train: epoch 0058, iter [02500, 05004], lr: 0.038328, loss: 1.6349
2022-08-27 18:19:50 - train: epoch 0058, iter [02600, 05004], lr: 0.038298, loss: 1.3623
2022-08-27 18:20:24 - train: epoch 0058, iter [02700, 05004], lr: 0.038267, loss: 1.7394
2022-08-27 18:20:58 - train: epoch 0058, iter [02800, 05004], lr: 0.038237, loss: 1.3227
2022-08-27 18:21:33 - train: epoch 0058, iter [02900, 05004], lr: 0.038206, loss: 1.3613
2022-08-27 18:22:06 - train: epoch 0058, iter [03000, 05004], lr: 0.038176, loss: 1.5536
2022-08-27 18:22:41 - train: epoch 0058, iter [03100, 05004], lr: 0.038145, loss: 1.4604
2022-08-27 18:23:13 - train: epoch 0058, iter [03200, 05004], lr: 0.038115, loss: 1.3564
2022-08-27 18:23:49 - train: epoch 0058, iter [03300, 05004], lr: 0.038084, loss: 1.4427
2022-08-27 18:24:23 - train: epoch 0058, iter [03400, 05004], lr: 0.038054, loss: 1.3417
2022-08-27 18:24:57 - train: epoch 0058, iter [03500, 05004], lr: 0.038023, loss: 1.4447
2022-08-27 18:25:30 - train: epoch 0058, iter [03600, 05004], lr: 0.037993, loss: 1.3946
2022-08-27 18:26:05 - train: epoch 0058, iter [03700, 05004], lr: 0.037962, loss: 1.5786
2022-08-27 18:26:40 - train: epoch 0058, iter [03800, 05004], lr: 0.037932, loss: 1.6650
2022-08-27 18:27:15 - train: epoch 0058, iter [03900, 05004], lr: 0.037901, loss: 1.5870
2022-08-27 18:27:48 - train: epoch 0058, iter [04000, 05004], lr: 0.037871, loss: 1.6352
2022-08-27 18:28:23 - train: epoch 0058, iter [04100, 05004], lr: 0.037841, loss: 1.6142
2022-08-27 18:28:57 - train: epoch 0058, iter [04200, 05004], lr: 0.037810, loss: 1.2214
2022-08-27 18:29:30 - train: epoch 0058, iter [04300, 05004], lr: 0.037780, loss: 1.8349
2022-08-27 18:30:05 - train: epoch 0058, iter [04400, 05004], lr: 0.037749, loss: 1.2908
2022-08-27 18:30:39 - train: epoch 0058, iter [04500, 05004], lr: 0.037719, loss: 1.5996
2022-08-27 18:31:16 - train: epoch 0058, iter [04600, 05004], lr: 0.037688, loss: 1.3806
2022-08-27 18:31:49 - train: epoch 0058, iter [04700, 05004], lr: 0.037658, loss: 1.5009
2022-08-27 18:32:23 - train: epoch 0058, iter [04800, 05004], lr: 0.037628, loss: 1.5812
2022-08-27 18:32:58 - train: epoch 0058, iter [04900, 05004], lr: 0.037597, loss: 1.4344
2022-08-27 18:33:30 - train: epoch 0058, iter [05000, 05004], lr: 0.037567, loss: 1.5281
2022-08-27 18:33:31 - train: epoch 058, train_loss: 1.5074
2022-08-27 18:34:46 - eval: epoch: 058, acc1: 67.348%, acc5: 88.356%, test_loss: 1.3277, per_image_load_time: 2.361ms, per_image_inference_time: 0.513ms
2022-08-27 18:34:47 - until epoch: 058, best_acc1: 67.348%
2022-08-27 18:34:47 - epoch 059 lr: 0.037565
2022-08-27 18:35:25 - train: epoch 0059, iter [00100, 05004], lr: 0.037535, loss: 1.6019
2022-08-27 18:35:59 - train: epoch 0059, iter [00200, 05004], lr: 0.037505, loss: 1.4715
2022-08-27 18:36:33 - train: epoch 0059, iter [00300, 05004], lr: 0.037474, loss: 1.5573
2022-08-27 18:37:07 - train: epoch 0059, iter [00400, 05004], lr: 0.037444, loss: 1.4935
2022-08-27 18:37:41 - train: epoch 0059, iter [00500, 05004], lr: 0.037414, loss: 1.6809
2022-08-27 18:38:14 - train: epoch 0059, iter [00600, 05004], lr: 0.037383, loss: 1.4707
2022-08-27 18:38:49 - train: epoch 0059, iter [00700, 05004], lr: 0.037353, loss: 1.3530
2022-08-27 18:39:22 - train: epoch 0059, iter [00800, 05004], lr: 0.037322, loss: 1.5185
2022-08-27 18:39:57 - train: epoch 0059, iter [00900, 05004], lr: 0.037292, loss: 1.4305
2022-08-27 18:40:30 - train: epoch 0059, iter [01000, 05004], lr: 0.037262, loss: 1.5051
2022-08-27 18:41:04 - train: epoch 0059, iter [01100, 05004], lr: 0.037231, loss: 1.6527
2022-08-27 18:41:38 - train: epoch 0059, iter [01200, 05004], lr: 0.037201, loss: 1.2479
2022-08-27 18:42:12 - train: epoch 0059, iter [01300, 05004], lr: 0.037171, loss: 1.6429
2022-08-27 18:42:47 - train: epoch 0059, iter [01400, 05004], lr: 0.037140, loss: 1.6467
2022-08-27 18:43:21 - train: epoch 0059, iter [01500, 05004], lr: 0.037110, loss: 1.6733
2022-08-27 18:43:55 - train: epoch 0059, iter [01600, 05004], lr: 0.037080, loss: 1.3335
2022-08-27 18:44:30 - train: epoch 0059, iter [01700, 05004], lr: 0.037049, loss: 1.6344
2022-08-27 18:45:04 - train: epoch 0059, iter [01800, 05004], lr: 0.037019, loss: 1.4206
2022-08-27 18:45:38 - train: epoch 0059, iter [01900, 05004], lr: 0.036989, loss: 1.5317
2022-08-27 18:46:12 - train: epoch 0059, iter [02000, 05004], lr: 0.036958, loss: 1.4717
2022-08-27 18:46:46 - train: epoch 0059, iter [02100, 05004], lr: 0.036928, loss: 1.6598
2022-08-27 18:47:20 - train: epoch 0059, iter [02200, 05004], lr: 0.036898, loss: 1.6486
2022-08-27 18:47:55 - train: epoch 0059, iter [02300, 05004], lr: 0.036868, loss: 1.4497
2022-08-27 18:48:29 - train: epoch 0059, iter [02400, 05004], lr: 0.036837, loss: 1.5211
2022-08-27 18:49:03 - train: epoch 0059, iter [02500, 05004], lr: 0.036807, loss: 1.4785
2022-08-27 18:49:38 - train: epoch 0059, iter [02600, 05004], lr: 0.036777, loss: 1.4424
2022-08-27 18:50:12 - train: epoch 0059, iter [02700, 05004], lr: 0.036746, loss: 1.4827
2022-08-27 18:50:46 - train: epoch 0059, iter [02800, 05004], lr: 0.036716, loss: 1.7269
2022-08-27 18:51:20 - train: epoch 0059, iter [02900, 05004], lr: 0.036686, loss: 1.5633
2022-08-27 18:51:54 - train: epoch 0059, iter [03000, 05004], lr: 0.036656, loss: 1.8740
2022-08-27 18:52:28 - train: epoch 0059, iter [03100, 05004], lr: 0.036625, loss: 1.3554
2022-08-27 18:53:03 - train: epoch 0059, iter [03200, 05004], lr: 0.036595, loss: 1.4406
2022-08-27 18:53:37 - train: epoch 0059, iter [03300, 05004], lr: 0.036565, loss: 1.5663
2022-08-27 18:54:11 - train: epoch 0059, iter [03400, 05004], lr: 0.036535, loss: 1.8007
2022-08-27 18:54:45 - train: epoch 0059, iter [03500, 05004], lr: 0.036504, loss: 1.4400
2022-08-27 18:55:20 - train: epoch 0059, iter [03600, 05004], lr: 0.036474, loss: 1.4568
2022-08-27 18:55:55 - train: epoch 0059, iter [03700, 05004], lr: 0.036444, loss: 1.3813
2022-08-27 18:56:29 - train: epoch 0059, iter [03800, 05004], lr: 0.036414, loss: 1.5074
2022-08-27 18:57:03 - train: epoch 0059, iter [03900, 05004], lr: 0.036384, loss: 1.3933
2022-08-27 18:57:37 - train: epoch 0059, iter [04000, 05004], lr: 0.036353, loss: 1.7844
2022-08-27 18:58:11 - train: epoch 0059, iter [04100, 05004], lr: 0.036323, loss: 1.5133
2022-08-27 18:58:45 - train: epoch 0059, iter [04200, 05004], lr: 0.036293, loss: 1.5072
2022-08-27 18:59:20 - train: epoch 0059, iter [04300, 05004], lr: 0.036263, loss: 1.7416
2022-08-27 18:59:54 - train: epoch 0059, iter [04400, 05004], lr: 0.036233, loss: 1.6810
2022-08-27 19:00:29 - train: epoch 0059, iter [04500, 05004], lr: 0.036202, loss: 1.5967
2022-08-27 19:01:03 - train: epoch 0059, iter [04600, 05004], lr: 0.036172, loss: 1.4833
2022-08-27 19:01:38 - train: epoch 0059, iter [04700, 05004], lr: 0.036142, loss: 1.5093
2022-08-27 19:02:12 - train: epoch 0059, iter [04800, 05004], lr: 0.036112, loss: 1.3606
2022-08-27 19:02:46 - train: epoch 0059, iter [04900, 05004], lr: 0.036082, loss: 1.6861
2022-08-27 19:03:19 - train: epoch 0059, iter [05000, 05004], lr: 0.036052, loss: 1.6958
2022-08-27 19:03:20 - train: epoch 059, train_loss: 1.4929
2022-08-27 19:04:36 - eval: epoch: 059, acc1: 65.432%, acc5: 87.346%, test_loss: 1.4059, per_image_load_time: 2.414ms, per_image_inference_time: 0.492ms
2022-08-27 19:04:37 - until epoch: 059, best_acc1: 67.348%
2022-08-27 19:04:37 - epoch 060 lr: 0.036050
2022-08-27 19:05:15 - train: epoch 0060, iter [00100, 05004], lr: 0.036020, loss: 1.3534
2022-08-27 19:05:49 - train: epoch 0060, iter [00200, 05004], lr: 0.035990, loss: 1.6309
2022-08-27 19:06:21 - train: epoch 0060, iter [00300, 05004], lr: 0.035960, loss: 1.4901
2022-08-27 19:06:55 - train: epoch 0060, iter [00400, 05004], lr: 0.035930, loss: 1.5321
2022-08-27 19:07:30 - train: epoch 0060, iter [00500, 05004], lr: 0.035900, loss: 1.6305
2022-08-27 19:08:04 - train: epoch 0060, iter [00600, 05004], lr: 0.035870, loss: 1.5227
2022-08-27 19:08:36 - train: epoch 0060, iter [00700, 05004], lr: 0.035840, loss: 1.4589
2022-08-27 19:09:10 - train: epoch 0060, iter [00800, 05004], lr: 0.035809, loss: 1.7417
2022-08-27 19:09:44 - train: epoch 0060, iter [00900, 05004], lr: 0.035779, loss: 1.2131
2022-08-27 19:10:18 - train: epoch 0060, iter [01000, 05004], lr: 0.035749, loss: 1.1379
2022-08-27 19:10:51 - train: epoch 0060, iter [01100, 05004], lr: 0.035719, loss: 1.2683
2022-08-27 19:11:26 - train: epoch 0060, iter [01200, 05004], lr: 0.035689, loss: 1.3792
2022-08-27 19:11:59 - train: epoch 0060, iter [01300, 05004], lr: 0.035659, loss: 1.3653
2022-08-27 19:12:33 - train: epoch 0060, iter [01400, 05004], lr: 0.035629, loss: 1.4301
2022-08-27 19:13:07 - train: epoch 0060, iter [01500, 05004], lr: 0.035599, loss: 1.5155
2022-08-27 19:13:40 - train: epoch 0060, iter [01600, 05004], lr: 0.035569, loss: 1.5114
2022-08-27 19:14:14 - train: epoch 0060, iter [01700, 05004], lr: 0.035539, loss: 1.4958
2022-08-27 19:14:49 - train: epoch 0060, iter [01800, 05004], lr: 0.035509, loss: 1.5688
2022-08-27 19:15:22 - train: epoch 0060, iter [01900, 05004], lr: 0.035479, loss: 1.6628
2022-08-27 19:15:57 - train: epoch 0060, iter [02000, 05004], lr: 0.035449, loss: 1.3695
2022-08-27 19:16:30 - train: epoch 0060, iter [02100, 05004], lr: 0.035419, loss: 1.5402
2022-08-27 19:17:04 - train: epoch 0060, iter [02200, 05004], lr: 0.035389, loss: 1.6043
2022-08-27 19:17:37 - train: epoch 0060, iter [02300, 05004], lr: 0.035359, loss: 1.2616
2022-08-27 19:18:11 - train: epoch 0060, iter [02400, 05004], lr: 0.035329, loss: 1.4346
2022-08-27 19:18:44 - train: epoch 0060, iter [02500, 05004], lr: 0.035299, loss: 1.5547
2022-08-27 19:19:18 - train: epoch 0060, iter [02600, 05004], lr: 0.035269, loss: 1.5249
2022-08-27 19:19:52 - train: epoch 0060, iter [02700, 05004], lr: 0.035239, loss: 1.5396
2022-08-27 19:20:26 - train: epoch 0060, iter [02800, 05004], lr: 0.035209, loss: 1.3054
2022-08-27 19:21:00 - train: epoch 0060, iter [02900, 05004], lr: 0.035179, loss: 1.6016
2022-08-27 19:21:33 - train: epoch 0060, iter [03000, 05004], lr: 0.035149, loss: 1.6849
2022-08-27 19:22:07 - train: epoch 0060, iter [03100, 05004], lr: 0.035119, loss: 1.5839
2022-08-27 19:22:42 - train: epoch 0060, iter [03200, 05004], lr: 0.035089, loss: 1.5237
2022-08-27 19:23:14 - train: epoch 0060, iter [03300, 05004], lr: 0.035059, loss: 1.3086
2022-08-27 19:23:49 - train: epoch 0060, iter [03400, 05004], lr: 0.035029, loss: 1.4366
2022-08-27 19:24:22 - train: epoch 0060, iter [03500, 05004], lr: 0.034999, loss: 1.5964
2022-08-27 19:24:56 - train: epoch 0060, iter [03600, 05004], lr: 0.034969, loss: 1.5014
2022-08-27 19:25:30 - train: epoch 0060, iter [03700, 05004], lr: 0.034939, loss: 1.4860
2022-08-27 19:26:03 - train: epoch 0060, iter [03800, 05004], lr: 0.034909, loss: 1.4666
2022-08-27 19:26:37 - train: epoch 0060, iter [03900, 05004], lr: 0.034879, loss: 1.8615
2022-08-27 19:27:11 - train: epoch 0060, iter [04000, 05004], lr: 0.034849, loss: 1.5470
2022-08-27 19:27:45 - train: epoch 0060, iter [04100, 05004], lr: 0.034819, loss: 1.6937
2022-08-27 19:28:19 - train: epoch 0060, iter [04200, 05004], lr: 0.034789, loss: 1.5473
2022-08-27 19:28:53 - train: epoch 0060, iter [04300, 05004], lr: 0.034759, loss: 1.4665
2022-08-27 19:29:27 - train: epoch 0060, iter [04400, 05004], lr: 0.034730, loss: 1.6560
2022-08-27 19:30:01 - train: epoch 0060, iter [04500, 05004], lr: 0.034700, loss: 1.4622
2022-08-27 19:30:35 - train: epoch 0060, iter [04600, 05004], lr: 0.034670, loss: 1.4847
2022-08-27 19:31:09 - train: epoch 0060, iter [04700, 05004], lr: 0.034640, loss: 1.5319
2022-08-27 19:31:43 - train: epoch 0060, iter [04800, 05004], lr: 0.034610, loss: 1.3549
2022-08-27 19:32:17 - train: epoch 0060, iter [04900, 05004], lr: 0.034580, loss: 1.4269
2022-08-27 19:32:49 - train: epoch 0060, iter [05000, 05004], lr: 0.034550, loss: 1.6361
2022-08-27 19:32:51 - train: epoch 060, train_loss: 1.4768
2022-08-27 19:34:06 - eval: epoch: 060, acc1: 67.038%, acc5: 88.016%, test_loss: 1.3409, per_image_load_time: 2.402ms, per_image_inference_time: 0.535ms
2022-08-27 19:34:07 - until epoch: 060, best_acc1: 67.348%
2022-08-27 19:34:07 - epoch 061 lr: 0.034549
2022-08-27 19:34:45 - train: epoch 0061, iter [00100, 05004], lr: 0.034519, loss: 1.3077
2022-08-27 19:35:19 - train: epoch 0061, iter [00200, 05004], lr: 0.034489, loss: 1.4903
2022-08-27 19:35:52 - train: epoch 0061, iter [00300, 05004], lr: 0.034460, loss: 1.3057
2022-08-27 19:36:26 - train: epoch 0061, iter [00400, 05004], lr: 0.034430, loss: 1.6816
2022-08-27 19:37:00 - train: epoch 0061, iter [00500, 05004], lr: 0.034400, loss: 1.4712
2022-08-27 19:37:34 - train: epoch 0061, iter [00600, 05004], lr: 0.034370, loss: 1.5528
2022-08-27 19:38:07 - train: epoch 0061, iter [00700, 05004], lr: 0.034340, loss: 1.1569
2022-08-27 19:38:41 - train: epoch 0061, iter [00800, 05004], lr: 0.034311, loss: 1.5375
2022-08-27 19:39:15 - train: epoch 0061, iter [00900, 05004], lr: 0.034281, loss: 1.3818
2022-08-27 19:39:50 - train: epoch 0061, iter [01000, 05004], lr: 0.034251, loss: 1.2949
2022-08-27 19:40:24 - train: epoch 0061, iter [01100, 05004], lr: 0.034221, loss: 1.3009
2022-08-27 19:40:58 - train: epoch 0061, iter [01200, 05004], lr: 0.034191, loss: 1.3737
2022-08-27 19:41:31 - train: epoch 0061, iter [01300, 05004], lr: 0.034162, loss: 1.4162
2022-08-27 19:42:06 - train: epoch 0061, iter [01400, 05004], lr: 0.034132, loss: 1.4230
2022-08-27 19:42:40 - train: epoch 0061, iter [01500, 05004], lr: 0.034102, loss: 1.4473
2022-08-27 19:43:13 - train: epoch 0061, iter [01600, 05004], lr: 0.034072, loss: 1.1975
2022-08-27 19:43:46 - train: epoch 0061, iter [01700, 05004], lr: 0.034043, loss: 1.4998
2022-08-27 19:44:20 - train: epoch 0061, iter [01800, 05004], lr: 0.034013, loss: 1.5219
2022-08-27 19:44:54 - train: epoch 0061, iter [01900, 05004], lr: 0.033983, loss: 1.5474
2022-08-27 19:45:28 - train: epoch 0061, iter [02000, 05004], lr: 0.033953, loss: 1.4210
2022-08-27 19:46:03 - train: epoch 0061, iter [02100, 05004], lr: 0.033924, loss: 1.8091
2022-08-27 19:46:36 - train: epoch 0061, iter [02200, 05004], lr: 0.033894, loss: 1.3901
2022-08-27 19:47:09 - train: epoch 0061, iter [02300, 05004], lr: 0.033864, loss: 1.4005
2022-08-27 19:47:43 - train: epoch 0061, iter [02400, 05004], lr: 0.033834, loss: 1.2663
2022-08-27 19:48:17 - train: epoch 0061, iter [02500, 05004], lr: 0.033805, loss: 1.4094
2022-08-27 19:48:51 - train: epoch 0061, iter [02600, 05004], lr: 0.033775, loss: 1.6113
2022-08-27 19:49:25 - train: epoch 0061, iter [02700, 05004], lr: 0.033745, loss: 1.6472
2022-08-27 19:49:59 - train: epoch 0061, iter [02800, 05004], lr: 0.033716, loss: 1.4528
2022-08-27 19:50:33 - train: epoch 0061, iter [02900, 05004], lr: 0.033686, loss: 1.5687
2022-08-27 19:51:05 - train: epoch 0061, iter [03000, 05004], lr: 0.033656, loss: 1.4188
2022-08-27 19:51:40 - train: epoch 0061, iter [03100, 05004], lr: 0.033627, loss: 1.6390
2022-08-27 19:52:13 - train: epoch 0061, iter [03200, 05004], lr: 0.033597, loss: 1.4912
2022-08-27 19:52:47 - train: epoch 0061, iter [03300, 05004], lr: 0.033567, loss: 1.3581
2022-08-27 19:53:21 - train: epoch 0061, iter [03400, 05004], lr: 0.033538, loss: 1.2912
2022-08-27 19:53:55 - train: epoch 0061, iter [03500, 05004], lr: 0.033508, loss: 1.4568
2022-08-27 19:54:29 - train: epoch 0061, iter [03600, 05004], lr: 0.033478, loss: 1.4822
2022-08-27 19:55:04 - train: epoch 0061, iter [03700, 05004], lr: 0.033449, loss: 1.6285
2022-08-27 19:55:38 - train: epoch 0061, iter [03800, 05004], lr: 0.033419, loss: 1.4284
2022-08-27 19:56:11 - train: epoch 0061, iter [03900, 05004], lr: 0.033390, loss: 1.5572
2022-08-27 19:56:45 - train: epoch 0061, iter [04000, 05004], lr: 0.033360, loss: 1.4355
2022-08-27 19:57:20 - train: epoch 0061, iter [04100, 05004], lr: 0.033330, loss: 1.7565
2022-08-27 19:57:54 - train: epoch 0061, iter [04200, 05004], lr: 0.033301, loss: 1.4718
2022-08-27 19:58:28 - train: epoch 0061, iter [04300, 05004], lr: 0.033271, loss: 1.5378
2022-08-27 19:59:01 - train: epoch 0061, iter [04400, 05004], lr: 0.033242, loss: 1.5524
2022-08-27 19:59:35 - train: epoch 0061, iter [04500, 05004], lr: 0.033212, loss: 1.5293
2022-08-27 20:00:09 - train: epoch 0061, iter [04600, 05004], lr: 0.033182, loss: 1.5409
2022-08-27 20:00:44 - train: epoch 0061, iter [04700, 05004], lr: 0.033153, loss: 1.3676
2022-08-27 20:01:17 - train: epoch 0061, iter [04800, 05004], lr: 0.033123, loss: 1.4972
2022-08-27 20:01:52 - train: epoch 0061, iter [04900, 05004], lr: 0.033094, loss: 1.5201
2022-08-27 20:02:24 - train: epoch 0061, iter [05000, 05004], lr: 0.033064, loss: 1.6465
2022-08-27 20:02:25 - train: epoch 061, train_loss: 1.4593
2022-08-27 20:03:40 - eval: epoch: 061, acc1: 67.526%, acc5: 88.224%, test_loss: 1.3248, per_image_load_time: 2.387ms, per_image_inference_time: 0.510ms
2022-08-27 20:03:41 - until epoch: 061, best_acc1: 67.526%
2022-08-27 20:03:41 - epoch 062 lr: 0.033063
2022-08-27 20:04:19 - train: epoch 0062, iter [00100, 05004], lr: 0.033034, loss: 1.4886
2022-08-27 20:04:53 - train: epoch 0062, iter [00200, 05004], lr: 0.033004, loss: 1.5682
2022-08-27 20:05:27 - train: epoch 0062, iter [00300, 05004], lr: 0.032975, loss: 1.3913
2022-08-27 20:06:00 - train: epoch 0062, iter [00400, 05004], lr: 0.032945, loss: 1.3425
2022-08-27 20:06:34 - train: epoch 0062, iter [00500, 05004], lr: 0.032916, loss: 1.3054
2022-08-27 20:07:07 - train: epoch 0062, iter [00600, 05004], lr: 0.032886, loss: 1.3548
2022-08-27 20:07:41 - train: epoch 0062, iter [00700, 05004], lr: 0.032857, loss: 1.6321
2022-08-27 20:08:14 - train: epoch 0062, iter [00800, 05004], lr: 0.032827, loss: 1.5275
2022-08-27 20:08:48 - train: epoch 0062, iter [00900, 05004], lr: 0.032798, loss: 1.3614
2022-08-27 20:09:22 - train: epoch 0062, iter [01000, 05004], lr: 0.032768, loss: 1.4814
2022-08-27 20:09:55 - train: epoch 0062, iter [01100, 05004], lr: 0.032739, loss: 1.3513
2022-08-27 20:10:29 - train: epoch 0062, iter [01200, 05004], lr: 0.032709, loss: 1.6737
2022-08-27 20:11:03 - train: epoch 0062, iter [01300, 05004], lr: 0.032680, loss: 1.5304
2022-08-27 20:11:37 - train: epoch 0062, iter [01400, 05004], lr: 0.032650, loss: 1.3492
2022-08-27 20:12:10 - train: epoch 0062, iter [01500, 05004], lr: 0.032621, loss: 1.4513
2022-08-27 20:12:44 - train: epoch 0062, iter [01600, 05004], lr: 0.032591, loss: 1.5785
2022-08-27 20:13:17 - train: epoch 0062, iter [01700, 05004], lr: 0.032562, loss: 1.5333
2022-08-27 20:13:52 - train: epoch 0062, iter [01800, 05004], lr: 0.032533, loss: 1.3757
2022-08-27 20:14:25 - train: epoch 0062, iter [01900, 05004], lr: 0.032503, loss: 1.4026
2022-08-27 20:14:58 - train: epoch 0062, iter [02000, 05004], lr: 0.032474, loss: 1.4182
2022-08-27 20:15:32 - train: epoch 0062, iter [02100, 05004], lr: 0.032444, loss: 1.5315
2022-08-27 20:16:05 - train: epoch 0062, iter [02200, 05004], lr: 0.032415, loss: 1.3525
2022-08-27 20:16:39 - train: epoch 0062, iter [02300, 05004], lr: 0.032386, loss: 1.4790
2022-08-27 20:17:13 - train: epoch 0062, iter [02400, 05004], lr: 0.032356, loss: 1.3999
2022-08-27 20:17:47 - train: epoch 0062, iter [02500, 05004], lr: 0.032327, loss: 1.5381
2022-08-27 20:18:21 - train: epoch 0062, iter [02600, 05004], lr: 0.032297, loss: 1.2730
2022-08-27 20:18:55 - train: epoch 0062, iter [02700, 05004], lr: 0.032268, loss: 1.3941
2022-08-27 20:19:29 - train: epoch 0062, iter [02800, 05004], lr: 0.032239, loss: 1.5812
2022-08-27 20:20:02 - train: epoch 0062, iter [02900, 05004], lr: 0.032209, loss: 1.5009
2022-08-27 20:20:35 - train: epoch 0062, iter [03000, 05004], lr: 0.032180, loss: 1.4812
2022-08-27 20:21:10 - train: epoch 0062, iter [03100, 05004], lr: 0.032151, loss: 1.5412
2022-08-27 20:21:44 - train: epoch 0062, iter [03200, 05004], lr: 0.032121, loss: 1.3496
2022-08-27 20:22:17 - train: epoch 0062, iter [03300, 05004], lr: 0.032092, loss: 1.5805
2022-08-27 20:22:52 - train: epoch 0062, iter [03400, 05004], lr: 0.032063, loss: 1.4519
2022-08-27 20:23:26 - train: epoch 0062, iter [03500, 05004], lr: 0.032034, loss: 1.5032
2022-08-27 20:24:00 - train: epoch 0062, iter [03600, 05004], lr: 0.032004, loss: 1.5460
2022-08-27 20:24:34 - train: epoch 0062, iter [03700, 05004], lr: 0.031975, loss: 1.4815
2022-08-27 20:25:08 - train: epoch 0062, iter [03800, 05004], lr: 0.031946, loss: 1.5049
2022-08-27 20:25:42 - train: epoch 0062, iter [03900, 05004], lr: 0.031916, loss: 1.4160
2022-08-27 20:26:16 - train: epoch 0062, iter [04000, 05004], lr: 0.031887, loss: 1.2592
2022-08-27 20:26:50 - train: epoch 0062, iter [04100, 05004], lr: 0.031858, loss: 1.4921
2022-08-27 20:27:24 - train: epoch 0062, iter [04200, 05004], lr: 0.031829, loss: 1.3516
2022-08-27 20:27:58 - train: epoch 0062, iter [04300, 05004], lr: 0.031799, loss: 1.4469
2022-08-27 20:28:32 - train: epoch 0062, iter [04400, 05004], lr: 0.031770, loss: 1.4067
2022-08-27 20:29:06 - train: epoch 0062, iter [04500, 05004], lr: 0.031741, loss: 1.3196
2022-08-27 20:29:39 - train: epoch 0062, iter [04600, 05004], lr: 0.031712, loss: 1.1431
2022-08-27 20:30:13 - train: epoch 0062, iter [04700, 05004], lr: 0.031683, loss: 1.4804
2022-08-27 20:30:48 - train: epoch 0062, iter [04800, 05004], lr: 0.031653, loss: 1.3873
2022-08-27 20:31:21 - train: epoch 0062, iter [04900, 05004], lr: 0.031624, loss: 1.4685
2022-08-27 20:31:54 - train: epoch 0062, iter [05000, 05004], lr: 0.031595, loss: 1.4219
2022-08-27 20:31:55 - train: epoch 062, train_loss: 1.4422
2022-08-27 20:33:11 - eval: epoch: 062, acc1: 66.094%, acc5: 87.466%, test_loss: 1.3819, per_image_load_time: 2.415ms, per_image_inference_time: 0.474ms
2022-08-27 20:33:11 - until epoch: 062, best_acc1: 67.526%
2022-08-27 20:33:11 - epoch 063 lr: 0.031593
2022-08-27 20:33:50 - train: epoch 0063, iter [00100, 05004], lr: 0.031565, loss: 1.3197
2022-08-27 20:34:23 - train: epoch 0063, iter [00200, 05004], lr: 0.031535, loss: 1.2630
2022-08-27 20:34:58 - train: epoch 0063, iter [00300, 05004], lr: 0.031506, loss: 1.5739
2022-08-27 20:35:31 - train: epoch 0063, iter [00400, 05004], lr: 0.031477, loss: 1.4218
2022-08-27 20:36:05 - train: epoch 0063, iter [00500, 05004], lr: 0.031448, loss: 1.2967
2022-08-27 20:36:38 - train: epoch 0063, iter [00600, 05004], lr: 0.031419, loss: 1.5484
2022-08-27 20:37:12 - train: epoch 0063, iter [00700, 05004], lr: 0.031390, loss: 1.4104
2022-08-27 20:37:46 - train: epoch 0063, iter [00800, 05004], lr: 0.031361, loss: 1.4016
2022-08-27 20:38:20 - train: epoch 0063, iter [00900, 05004], lr: 0.031331, loss: 1.4375
2022-08-27 20:38:53 - train: epoch 0063, iter [01000, 05004], lr: 0.031302, loss: 1.5353
2022-08-27 20:39:28 - train: epoch 0063, iter [01100, 05004], lr: 0.031273, loss: 1.3512
2022-08-27 20:40:01 - train: epoch 0063, iter [01200, 05004], lr: 0.031244, loss: 1.3581
2022-08-27 20:40:35 - train: epoch 0063, iter [01300, 05004], lr: 0.031215, loss: 1.4032
2022-08-27 20:41:09 - train: epoch 0063, iter [01400, 05004], lr: 0.031186, loss: 1.7383
2022-08-27 20:41:43 - train: epoch 0063, iter [01500, 05004], lr: 0.031157, loss: 1.5126
2022-08-27 20:42:17 - train: epoch 0063, iter [01600, 05004], lr: 0.031128, loss: 1.3756
2022-08-27 20:42:50 - train: epoch 0063, iter [01700, 05004], lr: 0.031099, loss: 1.2494
2022-08-27 20:43:24 - train: epoch 0063, iter [01800, 05004], lr: 0.031070, loss: 1.5579
2022-08-27 20:43:58 - train: epoch 0063, iter [01900, 05004], lr: 0.031041, loss: 1.5376
2022-08-27 20:44:31 - train: epoch 0063, iter [02000, 05004], lr: 0.031012, loss: 1.3165
2022-08-27 20:45:06 - train: epoch 0063, iter [02100, 05004], lr: 0.030982, loss: 1.2853
2022-08-27 20:45:40 - train: epoch 0063, iter [02200, 05004], lr: 0.030953, loss: 1.8094
2022-08-27 20:46:14 - train: epoch 0063, iter [02300, 05004], lr: 0.030924, loss: 1.4851
2022-08-27 20:46:48 - train: epoch 0063, iter [02400, 05004], lr: 0.030895, loss: 1.5369
2022-08-27 20:47:22 - train: epoch 0063, iter [02500, 05004], lr: 0.030866, loss: 1.3774
2022-08-27 20:47:57 - train: epoch 0063, iter [02600, 05004], lr: 0.030837, loss: 1.3769
2022-08-27 20:48:30 - train: epoch 0063, iter [02700, 05004], lr: 0.030808, loss: 1.5284
2022-08-27 20:49:05 - train: epoch 0063, iter [02800, 05004], lr: 0.030779, loss: 1.4467
2022-08-27 20:49:38 - train: epoch 0063, iter [02900, 05004], lr: 0.030750, loss: 1.4164
2022-08-27 20:50:12 - train: epoch 0063, iter [03000, 05004], lr: 0.030721, loss: 1.4246
2022-08-27 20:50:46 - train: epoch 0063, iter [03100, 05004], lr: 0.030693, loss: 1.7974
2022-08-27 20:51:20 - train: epoch 0063, iter [03200, 05004], lr: 0.030664, loss: 1.4784
2022-08-27 20:51:53 - train: epoch 0063, iter [03300, 05004], lr: 0.030635, loss: 1.4873
2022-08-27 20:52:27 - train: epoch 0063, iter [03400, 05004], lr: 0.030606, loss: 1.2034
2022-08-27 20:53:01 - train: epoch 0063, iter [03500, 05004], lr: 0.030577, loss: 1.4943
2022-08-27 20:53:36 - train: epoch 0063, iter [03600, 05004], lr: 0.030548, loss: 1.3046
2022-08-27 20:54:09 - train: epoch 0063, iter [03700, 05004], lr: 0.030519, loss: 1.5015
2022-08-27 20:54:43 - train: epoch 0063, iter [03800, 05004], lr: 0.030490, loss: 1.6893
2022-08-27 20:55:17 - train: epoch 0063, iter [03900, 05004], lr: 0.030461, loss: 1.1352
2022-08-27 20:55:52 - train: epoch 0063, iter [04000, 05004], lr: 0.030432, loss: 1.1104
2022-08-27 20:56:25 - train: epoch 0063, iter [04100, 05004], lr: 0.030403, loss: 1.5882
2022-08-27 20:56:59 - train: epoch 0063, iter [04200, 05004], lr: 0.030374, loss: 1.3486
2022-08-27 20:57:33 - train: epoch 0063, iter [04300, 05004], lr: 0.030346, loss: 1.4784
2022-08-27 20:58:07 - train: epoch 0063, iter [04400, 05004], lr: 0.030317, loss: 1.4807
2022-08-27 20:58:42 - train: epoch 0063, iter [04500, 05004], lr: 0.030288, loss: 1.3361
2022-08-27 20:59:15 - train: epoch 0063, iter [04600, 05004], lr: 0.030259, loss: 1.3777
2022-08-27 20:59:49 - train: epoch 0063, iter [04700, 05004], lr: 0.030230, loss: 1.3931
2022-08-27 21:00:24 - train: epoch 0063, iter [04800, 05004], lr: 0.030201, loss: 1.4383
2022-08-27 21:00:59 - train: epoch 0063, iter [04900, 05004], lr: 0.030173, loss: 1.4004
2022-08-27 21:01:31 - train: epoch 0063, iter [05000, 05004], lr: 0.030144, loss: 1.4077
2022-08-27 21:01:32 - train: epoch 063, train_loss: 1.4263
2022-08-27 21:02:47 - eval: epoch: 063, acc1: 67.192%, acc5: 88.124%, test_loss: 1.3484, per_image_load_time: 2.367ms, per_image_inference_time: 0.517ms
2022-08-27 21:02:47 - until epoch: 063, best_acc1: 67.526%
2022-08-27 21:02:47 - epoch 064 lr: 0.030142
2022-08-27 21:03:26 - train: epoch 0064, iter [00100, 05004], lr: 0.030114, loss: 1.3567
2022-08-27 21:03:59 - train: epoch 0064, iter [00200, 05004], lr: 0.030085, loss: 1.3759
2022-08-27 21:04:32 - train: epoch 0064, iter [00300, 05004], lr: 0.030056, loss: 1.1728
2022-08-27 21:05:06 - train: epoch 0064, iter [00400, 05004], lr: 0.030027, loss: 1.3224
2022-08-27 21:05:40 - train: epoch 0064, iter [00500, 05004], lr: 0.029999, loss: 1.3084
2022-08-27 21:06:13 - train: epoch 0064, iter [00600, 05004], lr: 0.029970, loss: 1.4904
2022-08-27 21:06:47 - train: epoch 0064, iter [00700, 05004], lr: 0.029941, loss: 1.4689
2022-08-27 21:07:20 - train: epoch 0064, iter [00800, 05004], lr: 0.029912, loss: 1.2337
2022-08-27 21:07:54 - train: epoch 0064, iter [00900, 05004], lr: 0.029884, loss: 1.4320
2022-08-27 21:08:28 - train: epoch 0064, iter [01000, 05004], lr: 0.029855, loss: 1.3450
2022-08-27 21:09:02 - train: epoch 0064, iter [01100, 05004], lr: 0.029826, loss: 1.3824
2022-08-27 21:09:36 - train: epoch 0064, iter [01200, 05004], lr: 0.029797, loss: 1.3347
2022-08-27 21:10:11 - train: epoch 0064, iter [01300, 05004], lr: 0.029769, loss: 1.4188
2022-08-27 21:10:44 - train: epoch 0064, iter [01400, 05004], lr: 0.029740, loss: 1.6776
2022-08-27 21:11:17 - train: epoch 0064, iter [01500, 05004], lr: 0.029711, loss: 1.4242
2022-08-27 21:11:51 - train: epoch 0064, iter [01600, 05004], lr: 0.029683, loss: 1.2642
2022-08-27 21:12:25 - train: epoch 0064, iter [01700, 05004], lr: 0.029654, loss: 1.2370
2022-08-27 21:12:59 - train: epoch 0064, iter [01800, 05004], lr: 0.029625, loss: 1.3038
2022-08-27 21:13:33 - train: epoch 0064, iter [01900, 05004], lr: 0.029597, loss: 1.3261
2022-08-27 21:14:07 - train: epoch 0064, iter [02000, 05004], lr: 0.029568, loss: 1.3628
2022-08-27 21:14:40 - train: epoch 0064, iter [02100, 05004], lr: 0.029539, loss: 1.3805
2022-08-27 21:15:14 - train: epoch 0064, iter [02200, 05004], lr: 0.029511, loss: 1.4829
2022-08-27 21:15:49 - train: epoch 0064, iter [02300, 05004], lr: 0.029482, loss: 1.4716
2022-08-27 21:16:21 - train: epoch 0064, iter [02400, 05004], lr: 0.029453, loss: 1.4624
2022-08-27 21:16:55 - train: epoch 0064, iter [02500, 05004], lr: 0.029425, loss: 1.2347
2022-08-27 21:17:29 - train: epoch 0064, iter [02600, 05004], lr: 0.029396, loss: 1.3900
2022-08-27 21:18:03 - train: epoch 0064, iter [02700, 05004], lr: 0.029368, loss: 1.3882
2022-08-27 21:18:37 - train: epoch 0064, iter [02800, 05004], lr: 0.029339, loss: 1.2945
2022-08-27 21:19:11 - train: epoch 0064, iter [02900, 05004], lr: 0.029310, loss: 1.6746
2022-08-27 21:19:45 - train: epoch 0064, iter [03000, 05004], lr: 0.029282, loss: 1.4452
2022-08-27 21:20:19 - train: epoch 0064, iter [03100, 05004], lr: 0.029253, loss: 1.3694
2022-08-27 21:20:53 - train: epoch 0064, iter [03200, 05004], lr: 0.029225, loss: 1.4560
2022-08-27 21:21:27 - train: epoch 0064, iter [03300, 05004], lr: 0.029196, loss: 1.3811
2022-08-27 21:22:00 - train: epoch 0064, iter [03400, 05004], lr: 0.029168, loss: 1.4919
2022-08-27 21:22:34 - train: epoch 0064, iter [03500, 05004], lr: 0.029139, loss: 1.3515
2022-08-27 21:23:09 - train: epoch 0064, iter [03600, 05004], lr: 0.029111, loss: 1.4187
2022-08-27 21:23:42 - train: epoch 0064, iter [03700, 05004], lr: 0.029082, loss: 1.1048
2022-08-27 21:24:16 - train: epoch 0064, iter [03800, 05004], lr: 0.029054, loss: 1.5915
2022-08-27 21:24:50 - train: epoch 0064, iter [03900, 05004], lr: 0.029025, loss: 1.3440
2022-08-27 21:25:24 - train: epoch 0064, iter [04000, 05004], lr: 0.028997, loss: 1.2569
2022-08-27 21:25:59 - train: epoch 0064, iter [04100, 05004], lr: 0.028968, loss: 1.3486
2022-08-27 21:26:32 - train: epoch 0064, iter [04200, 05004], lr: 0.028940, loss: 1.2580
2022-08-27 21:27:07 - train: epoch 0064, iter [04300, 05004], lr: 0.028911, loss: 1.5384
2022-08-27 21:27:41 - train: epoch 0064, iter [04400, 05004], lr: 0.028883, loss: 1.4312
2022-08-27 21:28:15 - train: epoch 0064, iter [04500, 05004], lr: 0.028854, loss: 1.3262
2022-08-27 21:28:49 - train: epoch 0064, iter [04600, 05004], lr: 0.028826, loss: 1.6411
2022-08-27 21:29:23 - train: epoch 0064, iter [04700, 05004], lr: 0.028797, loss: 1.8074
2022-08-27 21:29:57 - train: epoch 0064, iter [04800, 05004], lr: 0.028769, loss: 1.4410
2022-08-27 21:30:31 - train: epoch 0064, iter [04900, 05004], lr: 0.028741, loss: 1.7095
2022-08-27 21:31:03 - train: epoch 0064, iter [05000, 05004], lr: 0.028712, loss: 1.2232
2022-08-27 21:31:05 - train: epoch 064, train_loss: 1.4088
2022-08-27 21:32:20 - eval: epoch: 064, acc1: 67.838%, acc5: 88.462%, test_loss: 1.3228, per_image_load_time: 2.386ms, per_image_inference_time: 0.494ms
2022-08-27 21:32:20 - until epoch: 064, best_acc1: 67.838%
2022-08-27 21:32:20 - epoch 065 lr: 0.028711
2022-08-27 21:32:59 - train: epoch 0065, iter [00100, 05004], lr: 0.028683, loss: 1.4577
2022-08-27 21:33:33 - train: epoch 0065, iter [00200, 05004], lr: 0.028654, loss: 1.3465
2022-08-27 21:34:07 - train: epoch 0065, iter [00300, 05004], lr: 0.028626, loss: 1.3469
2022-08-27 21:34:40 - train: epoch 0065, iter [00400, 05004], lr: 0.028597, loss: 1.4664
2022-08-27 21:35:14 - train: epoch 0065, iter [00500, 05004], lr: 0.028569, loss: 1.3228
2022-08-27 21:35:48 - train: epoch 0065, iter [00600, 05004], lr: 0.028541, loss: 1.5460
2022-08-27 21:36:22 - train: epoch 0065, iter [00700, 05004], lr: 0.028512, loss: 1.3176
2022-08-27 21:36:55 - train: epoch 0065, iter [00800, 05004], lr: 0.028484, loss: 1.3131
2022-08-27 21:37:28 - train: epoch 0065, iter [00900, 05004], lr: 0.028456, loss: 1.3012
2022-08-27 21:38:03 - train: epoch 0065, iter [01000, 05004], lr: 0.028427, loss: 1.3596
2022-08-27 21:38:36 - train: epoch 0065, iter [01100, 05004], lr: 0.028399, loss: 1.3275
2022-08-27 21:39:11 - train: epoch 0065, iter [01200, 05004], lr: 0.028371, loss: 1.5837
2022-08-27 21:39:44 - train: epoch 0065, iter [01300, 05004], lr: 0.028343, loss: 1.3627
2022-08-27 21:40:17 - train: epoch 0065, iter [01400, 05004], lr: 0.028314, loss: 1.2110
2022-08-27 21:40:52 - train: epoch 0065, iter [01500, 05004], lr: 0.028286, loss: 1.3568
2022-08-27 21:41:27 - train: epoch 0065, iter [01600, 05004], lr: 0.028258, loss: 1.4200
2022-08-27 21:42:01 - train: epoch 0065, iter [01700, 05004], lr: 0.028229, loss: 1.3669
2022-08-27 21:42:34 - train: epoch 0065, iter [01800, 05004], lr: 0.028201, loss: 1.2506
2022-08-27 21:43:07 - train: epoch 0065, iter [01900, 05004], lr: 0.028173, loss: 1.2244
2022-08-27 21:43:41 - train: epoch 0065, iter [02000, 05004], lr: 0.028145, loss: 1.3728
2022-08-27 21:44:16 - train: epoch 0065, iter [02100, 05004], lr: 0.028116, loss: 1.2358
2022-08-27 21:44:50 - train: epoch 0065, iter [02200, 05004], lr: 0.028088, loss: 1.5411
2022-08-27 21:45:24 - train: epoch 0065, iter [02300, 05004], lr: 0.028060, loss: 1.3192
2022-08-27 21:45:58 - train: epoch 0065, iter [02400, 05004], lr: 0.028032, loss: 1.2465
2022-08-27 21:46:32 - train: epoch 0065, iter [02500, 05004], lr: 0.028004, loss: 1.4640
2022-08-27 21:47:06 - train: epoch 0065, iter [02600, 05004], lr: 0.027975, loss: 1.5565
2022-08-27 21:47:40 - train: epoch 0065, iter [02700, 05004], lr: 0.027947, loss: 1.3983
2022-08-27 21:48:14 - train: epoch 0065, iter [02800, 05004], lr: 0.027919, loss: 1.3163
2022-08-27 21:48:48 - train: epoch 0065, iter [02900, 05004], lr: 0.027891, loss: 1.4430
2022-08-27 21:49:22 - train: epoch 0065, iter [03000, 05004], lr: 0.027863, loss: 1.3679
2022-08-27 21:49:56 - train: epoch 0065, iter [03100, 05004], lr: 0.027835, loss: 1.4427
2022-08-27 21:50:30 - train: epoch 0065, iter [03200, 05004], lr: 0.027806, loss: 1.5665
2022-08-27 21:51:04 - train: epoch 0065, iter [03300, 05004], lr: 0.027778, loss: 1.4116
2022-08-27 21:51:38 - train: epoch 0065, iter [03400, 05004], lr: 0.027750, loss: 1.3463
2022-08-27 21:52:11 - train: epoch 0065, iter [03500, 05004], lr: 0.027722, loss: 1.5798
2022-08-27 21:52:45 - train: epoch 0065, iter [03600, 05004], lr: 0.027694, loss: 1.4660
2022-08-27 21:53:19 - train: epoch 0065, iter [03700, 05004], lr: 0.027666, loss: 1.2581
2022-08-27 21:53:53 - train: epoch 0065, iter [03800, 05004], lr: 0.027638, loss: 1.3025
2022-08-27 21:54:26 - train: epoch 0065, iter [03900, 05004], lr: 0.027610, loss: 1.4405
2022-08-27 21:55:00 - train: epoch 0065, iter [04000, 05004], lr: 0.027582, loss: 1.3636
2022-08-27 21:55:34 - train: epoch 0065, iter [04100, 05004], lr: 0.027554, loss: 1.2420
2022-08-27 21:56:08 - train: epoch 0065, iter [04200, 05004], lr: 0.027526, loss: 1.3911
2022-08-27 21:56:42 - train: epoch 0065, iter [04300, 05004], lr: 0.027498, loss: 1.3951
2022-08-27 21:57:16 - train: epoch 0065, iter [04400, 05004], lr: 0.027470, loss: 1.3844
2022-08-27 21:57:50 - train: epoch 0065, iter [04500, 05004], lr: 0.027442, loss: 1.4204
2022-08-27 21:58:24 - train: epoch 0065, iter [04600, 05004], lr: 0.027414, loss: 1.4821
2022-08-27 21:58:57 - train: epoch 0065, iter [04700, 05004], lr: 0.027386, loss: 1.4432
2022-08-27 21:59:31 - train: epoch 0065, iter [04800, 05004], lr: 0.027358, loss: 1.1027
2022-08-27 22:00:05 - train: epoch 0065, iter [04900, 05004], lr: 0.027330, loss: 1.2914
2022-08-27 22:00:38 - train: epoch 0065, iter [05000, 05004], lr: 0.027302, loss: 1.4708
2022-08-27 22:00:39 - train: epoch 065, train_loss: 1.3899
2022-08-27 22:01:55 - eval: epoch: 065, acc1: 68.016%, acc5: 88.500%, test_loss: 1.3089, per_image_load_time: 2.425ms, per_image_inference_time: 0.512ms
2022-08-27 22:01:55 - until epoch: 065, best_acc1: 68.016%
2022-08-27 22:01:55 - epoch 066 lr: 0.027300
2022-08-27 22:02:34 - train: epoch 0066, iter [00100, 05004], lr: 0.027273, loss: 1.1818
2022-08-27 22:03:07 - train: epoch 0066, iter [00200, 05004], lr: 0.027245, loss: 1.5072
2022-08-27 22:03:41 - train: epoch 0066, iter [00300, 05004], lr: 0.027217, loss: 1.2421
2022-08-27 22:04:15 - train: epoch 0066, iter [00400, 05004], lr: 0.027189, loss: 1.1064
2022-08-27 22:04:49 - train: epoch 0066, iter [00500, 05004], lr: 0.027161, loss: 1.4407
2022-08-27 22:05:24 - train: epoch 0066, iter [00600, 05004], lr: 0.027133, loss: 1.3417
2022-08-27 22:05:57 - train: epoch 0066, iter [00700, 05004], lr: 0.027105, loss: 1.2809
2022-08-27 22:06:32 - train: epoch 0066, iter [00800, 05004], lr: 0.027077, loss: 1.7530
2022-08-27 22:07:06 - train: epoch 0066, iter [00900, 05004], lr: 0.027049, loss: 1.4046
2022-08-27 22:07:40 - train: epoch 0066, iter [01000, 05004], lr: 0.027021, loss: 1.4839
2022-08-27 22:08:14 - train: epoch 0066, iter [01100, 05004], lr: 0.026993, loss: 1.4475
2022-08-27 22:08:48 - train: epoch 0066, iter [01200, 05004], lr: 0.026965, loss: 1.5645
2022-08-27 22:09:22 - train: epoch 0066, iter [01300, 05004], lr: 0.026938, loss: 1.4459
2022-08-27 22:09:57 - train: epoch 0066, iter [01400, 05004], lr: 0.026910, loss: 1.1339
2022-08-27 22:10:31 - train: epoch 0066, iter [01500, 05004], lr: 0.026882, loss: 1.2996
2022-08-27 22:11:05 - train: epoch 0066, iter [01600, 05004], lr: 0.026854, loss: 1.4173
2022-08-27 22:11:40 - train: epoch 0066, iter [01700, 05004], lr: 0.026826, loss: 1.3426
2022-08-27 22:12:14 - train: epoch 0066, iter [01800, 05004], lr: 0.026798, loss: 1.1836
2022-08-27 22:12:48 - train: epoch 0066, iter [01900, 05004], lr: 0.026771, loss: 1.6192
2022-08-27 22:13:22 - train: epoch 0066, iter [02000, 05004], lr: 0.026743, loss: 1.5098
2022-08-27 22:13:56 - train: epoch 0066, iter [02100, 05004], lr: 0.026715, loss: 1.3724
2022-08-27 22:14:31 - train: epoch 0066, iter [02200, 05004], lr: 0.026687, loss: 1.3308
2022-08-27 22:15:05 - train: epoch 0066, iter [02300, 05004], lr: 0.026660, loss: 1.5367
2022-08-27 22:15:39 - train: epoch 0066, iter [02400, 05004], lr: 0.026632, loss: 1.2606
2022-08-27 22:16:13 - train: epoch 0066, iter [02500, 05004], lr: 0.026604, loss: 1.3614
2022-08-27 22:16:47 - train: epoch 0066, iter [02600, 05004], lr: 0.026576, loss: 1.2261
2022-08-27 22:17:22 - train: epoch 0066, iter [02700, 05004], lr: 0.026549, loss: 1.6015
2022-08-27 22:17:56 - train: epoch 0066, iter [02800, 05004], lr: 0.026521, loss: 1.4378
2022-08-27 22:18:31 - train: epoch 0066, iter [02900, 05004], lr: 0.026493, loss: 1.3920
2022-08-27 22:19:06 - train: epoch 0066, iter [03000, 05004], lr: 0.026465, loss: 1.3146
2022-08-27 22:19:40 - train: epoch 0066, iter [03100, 05004], lr: 0.026438, loss: 1.5101
2022-08-27 22:20:13 - train: epoch 0066, iter [03200, 05004], lr: 0.026410, loss: 1.0884
2022-08-27 22:20:48 - train: epoch 0066, iter [03300, 05004], lr: 0.026382, loss: 1.3144
2022-08-27 22:21:23 - train: epoch 0066, iter [03400, 05004], lr: 0.026355, loss: 1.6554
2022-08-27 22:21:56 - train: epoch 0066, iter [03500, 05004], lr: 0.026327, loss: 1.3756
2022-08-27 22:22:31 - train: epoch 0066, iter [03600, 05004], lr: 0.026299, loss: 1.4637
2022-08-27 22:23:05 - train: epoch 0066, iter [03700, 05004], lr: 0.026272, loss: 1.4239
2022-08-27 22:23:40 - train: epoch 0066, iter [03800, 05004], lr: 0.026244, loss: 1.2769
2022-08-27 22:24:14 - train: epoch 0066, iter [03900, 05004], lr: 0.026217, loss: 1.2083
2022-08-27 22:24:48 - train: epoch 0066, iter [04000, 05004], lr: 0.026189, loss: 1.5547
2022-08-27 22:25:21 - train: epoch 0066, iter [04100, 05004], lr: 0.026161, loss: 1.2232
2022-08-27 22:25:56 - train: epoch 0066, iter [04200, 05004], lr: 0.026134, loss: 1.1779
2022-08-27 22:26:31 - train: epoch 0066, iter [04300, 05004], lr: 0.026106, loss: 1.2829
2022-08-27 22:27:05 - train: epoch 0066, iter [04400, 05004], lr: 0.026079, loss: 1.3418
2022-08-27 22:27:39 - train: epoch 0066, iter [04500, 05004], lr: 0.026051, loss: 1.4742
2022-08-27 22:28:14 - train: epoch 0066, iter [04600, 05004], lr: 0.026024, loss: 1.6521
2022-08-27 22:28:48 - train: epoch 0066, iter [04700, 05004], lr: 0.025996, loss: 1.3586
2022-08-27 22:29:22 - train: epoch 0066, iter [04800, 05004], lr: 0.025968, loss: 1.3973
2022-08-27 22:29:57 - train: epoch 0066, iter [04900, 05004], lr: 0.025941, loss: 1.4477
2022-08-27 22:30:30 - train: epoch 0066, iter [05000, 05004], lr: 0.025913, loss: 1.3009
2022-08-27 22:30:31 - train: epoch 066, train_loss: 1.3714
2022-08-27 22:31:47 - eval: epoch: 066, acc1: 65.668%, acc5: 87.304%, test_loss: 1.4075, per_image_load_time: 1.976ms, per_image_inference_time: 0.508ms
2022-08-27 22:31:47 - until epoch: 066, best_acc1: 68.016%
2022-08-27 22:31:47 - epoch 067 lr: 0.025912
2022-08-27 22:32:25 - train: epoch 0067, iter [00100, 05004], lr: 0.025885, loss: 1.2231
2022-08-27 22:33:00 - train: epoch 0067, iter [00200, 05004], lr: 0.025857, loss: 1.2839
2022-08-27 22:33:34 - train: epoch 0067, iter [00300, 05004], lr: 0.025830, loss: 1.5891
2022-08-27 22:34:07 - train: epoch 0067, iter [00400, 05004], lr: 0.025802, loss: 1.3699
2022-08-27 22:34:42 - train: epoch 0067, iter [00500, 05004], lr: 0.025775, loss: 1.3207
2022-08-27 22:35:16 - train: epoch 0067, iter [00600, 05004], lr: 0.025747, loss: 1.2328
2022-08-27 22:35:50 - train: epoch 0067, iter [00700, 05004], lr: 0.025720, loss: 1.3726
2022-08-27 22:36:24 - train: epoch 0067, iter [00800, 05004], lr: 0.025693, loss: 1.4105
2022-08-27 22:36:58 - train: epoch 0067, iter [00900, 05004], lr: 0.025665, loss: 1.4937
2022-08-27 22:37:32 - train: epoch 0067, iter [01000, 05004], lr: 0.025638, loss: 1.1832
2022-08-27 22:38:06 - train: epoch 0067, iter [01100, 05004], lr: 0.025610, loss: 1.3399
2022-08-27 22:38:40 - train: epoch 0067, iter [01200, 05004], lr: 0.025583, loss: 1.3182
2022-08-27 22:39:14 - train: epoch 0067, iter [01300, 05004], lr: 0.025556, loss: 1.5881
2022-08-27 22:39:49 - train: epoch 0067, iter [01400, 05004], lr: 0.025528, loss: 1.4061
2022-08-27 22:40:23 - train: epoch 0067, iter [01500, 05004], lr: 0.025501, loss: 1.4341
2022-08-27 22:40:57 - train: epoch 0067, iter [01600, 05004], lr: 0.025473, loss: 1.3531
2022-08-27 22:41:30 - train: epoch 0067, iter [01700, 05004], lr: 0.025446, loss: 1.1933
2022-08-27 22:42:05 - train: epoch 0067, iter [01800, 05004], lr: 0.025419, loss: 1.6093
2022-08-27 22:42:39 - train: epoch 0067, iter [01900, 05004], lr: 0.025391, loss: 1.2970
2022-08-27 22:43:14 - train: epoch 0067, iter [02000, 05004], lr: 0.025364, loss: 1.4355
2022-08-27 22:43:48 - train: epoch 0067, iter [02100, 05004], lr: 0.025337, loss: 1.1468
2022-08-27 22:44:21 - train: epoch 0067, iter [02200, 05004], lr: 0.025309, loss: 1.4506
2022-08-27 22:44:57 - train: epoch 0067, iter [02300, 05004], lr: 0.025282, loss: 1.2996
2022-08-27 22:45:30 - train: epoch 0067, iter [02400, 05004], lr: 0.025255, loss: 1.3320
2022-08-27 22:46:05 - train: epoch 0067, iter [02500, 05004], lr: 0.025228, loss: 1.2903
2022-08-27 22:46:38 - train: epoch 0067, iter [02600, 05004], lr: 0.025200, loss: 1.2227
2022-08-27 22:47:12 - train: epoch 0067, iter [02700, 05004], lr: 0.025173, loss: 1.3369
2022-08-27 22:47:47 - train: epoch 0067, iter [02800, 05004], lr: 0.025146, loss: 1.4638
2022-08-27 22:48:23 - train: epoch 0067, iter [02900, 05004], lr: 0.025119, loss: 1.2592
2022-08-27 22:48:57 - train: epoch 0067, iter [03000, 05004], lr: 0.025091, loss: 1.3732
2022-08-27 22:49:32 - train: epoch 0067, iter [03100, 05004], lr: 0.025064, loss: 1.1870
2022-08-27 22:50:07 - train: epoch 0067, iter [03200, 05004], lr: 0.025037, loss: 1.4240
2022-08-27 22:50:42 - train: epoch 0067, iter [03300, 05004], lr: 0.025010, loss: 1.1970
2022-08-27 22:51:17 - train: epoch 0067, iter [03400, 05004], lr: 0.024983, loss: 1.3324
2022-08-27 22:51:52 - train: epoch 0067, iter [03500, 05004], lr: 0.024955, loss: 1.2721
2022-08-27 22:52:27 - train: epoch 0067, iter [03600, 05004], lr: 0.024928, loss: 1.3835
2022-08-27 22:53:01 - train: epoch 0067, iter [03700, 05004], lr: 0.024901, loss: 1.4992
2022-08-27 22:53:35 - train: epoch 0067, iter [03800, 05004], lr: 0.024874, loss: 1.2747
2022-08-27 22:54:10 - train: epoch 0067, iter [03900, 05004], lr: 0.024847, loss: 1.3832
2022-08-27 22:54:45 - train: epoch 0067, iter [04000, 05004], lr: 0.024820, loss: 1.3563
2022-08-27 22:55:20 - train: epoch 0067, iter [04100, 05004], lr: 0.024793, loss: 1.3860
2022-08-27 22:55:55 - train: epoch 0067, iter [04200, 05004], lr: 0.024765, loss: 1.5133
2022-08-27 22:56:30 - train: epoch 0067, iter [04300, 05004], lr: 0.024738, loss: 1.2460
2022-08-27 22:57:05 - train: epoch 0067, iter [04400, 05004], lr: 0.024711, loss: 1.4513
2022-08-27 22:57:39 - train: epoch 0067, iter [04500, 05004], lr: 0.024684, loss: 1.2486
2022-08-27 22:58:14 - train: epoch 0067, iter [04600, 05004], lr: 0.024657, loss: 1.0296
2022-08-27 22:58:50 - train: epoch 0067, iter [04700, 05004], lr: 0.024630, loss: 1.2719
2022-08-27 22:59:24 - train: epoch 0067, iter [04800, 05004], lr: 0.024603, loss: 1.1106
2022-08-27 22:59:59 - train: epoch 0067, iter [04900, 05004], lr: 0.024576, loss: 1.4362
2022-08-27 23:00:32 - train: epoch 0067, iter [05000, 05004], lr: 0.024549, loss: 1.5497
2022-08-27 23:00:33 - train: epoch 067, train_loss: 1.3528
2022-08-27 23:01:50 - eval: epoch: 067, acc1: 67.890%, acc5: 88.548%, test_loss: 1.3032, per_image_load_time: 2.493ms, per_image_inference_time: 0.488ms
2022-08-27 23:01:50 - until epoch: 067, best_acc1: 68.016%
2022-08-27 23:01:50 - epoch 068 lr: 0.024548
2022-08-27 23:02:30 - train: epoch 0068, iter [00100, 05004], lr: 0.024521, loss: 1.3259
2022-08-27 23:03:05 - train: epoch 0068, iter [00200, 05004], lr: 0.024494, loss: 1.3454
2022-08-27 23:03:39 - train: epoch 0068, iter [00300, 05004], lr: 0.024467, loss: 1.3682
2022-08-27 23:04:13 - train: epoch 0068, iter [00400, 05004], lr: 0.024440, loss: 1.3853
2022-08-27 23:04:48 - train: epoch 0068, iter [00500, 05004], lr: 0.024413, loss: 1.2328
2022-08-27 23:05:22 - train: epoch 0068, iter [00600, 05004], lr: 0.024386, loss: 1.2995
2022-08-27 23:05:56 - train: epoch 0068, iter [00700, 05004], lr: 0.024359, loss: 1.5890
2022-08-27 23:06:31 - train: epoch 0068, iter [00800, 05004], lr: 0.024332, loss: 1.4009
2022-08-27 23:07:05 - train: epoch 0068, iter [00900, 05004], lr: 0.024305, loss: 1.1943
2022-08-27 23:07:39 - train: epoch 0068, iter [01000, 05004], lr: 0.024278, loss: 1.3564
2022-08-27 23:08:13 - train: epoch 0068, iter [01100, 05004], lr: 0.024251, loss: 1.5524
2022-08-27 23:08:47 - train: epoch 0068, iter [01200, 05004], lr: 0.024224, loss: 1.3068
2022-08-27 23:09:23 - train: epoch 0068, iter [01300, 05004], lr: 0.024198, loss: 1.1790
2022-08-27 23:09:57 - train: epoch 0068, iter [01400, 05004], lr: 0.024171, loss: 1.2929
2022-08-27 23:10:31 - train: epoch 0068, iter [01500, 05004], lr: 0.024144, loss: 1.4641
2022-08-27 23:11:05 - train: epoch 0068, iter [01600, 05004], lr: 0.024117, loss: 1.3508
2022-08-27 23:11:40 - train: epoch 0068, iter [01700, 05004], lr: 0.024090, loss: 1.4335
2022-08-27 23:12:15 - train: epoch 0068, iter [01800, 05004], lr: 0.024063, loss: 1.3945
2022-08-27 23:12:50 - train: epoch 0068, iter [01900, 05004], lr: 0.024036, loss: 1.4436
2022-08-27 23:13:24 - train: epoch 0068, iter [02000, 05004], lr: 0.024010, loss: 1.2602
2022-08-27 23:13:59 - train: epoch 0068, iter [02100, 05004], lr: 0.023983, loss: 1.3112
2022-08-27 23:14:33 - train: epoch 0068, iter [02200, 05004], lr: 0.023956, loss: 1.3593
2022-08-27 23:15:08 - train: epoch 0068, iter [02300, 05004], lr: 0.023929, loss: 1.2184
2022-08-27 23:15:41 - train: epoch 0068, iter [02400, 05004], lr: 0.023902, loss: 1.4266
2022-08-27 23:16:16 - train: epoch 0068, iter [02500, 05004], lr: 0.023876, loss: 1.2279
2022-08-27 23:16:50 - train: epoch 0068, iter [02600, 05004], lr: 0.023849, loss: 1.3198
2022-08-27 23:17:25 - train: epoch 0068, iter [02700, 05004], lr: 0.023822, loss: 1.3579
2022-08-27 23:17:59 - train: epoch 0068, iter [02800, 05004], lr: 0.023795, loss: 1.4478
2022-08-27 23:18:32 - train: epoch 0068, iter [02900, 05004], lr: 0.023769, loss: 1.4395
2022-08-27 23:19:07 - train: epoch 0068, iter [03000, 05004], lr: 0.023742, loss: 1.5546
2022-08-27 23:19:42 - train: epoch 0068, iter [03100, 05004], lr: 0.023715, loss: 1.1952
2022-08-27 23:20:16 - train: epoch 0068, iter [03200, 05004], lr: 0.023689, loss: 1.4446
2022-08-27 23:20:51 - train: epoch 0068, iter [03300, 05004], lr: 0.023662, loss: 1.3113
2022-08-27 23:21:26 - train: epoch 0068, iter [03400, 05004], lr: 0.023635, loss: 1.2018
2022-08-27 23:21:59 - train: epoch 0068, iter [03500, 05004], lr: 0.023608, loss: 1.4103
2022-08-27 23:22:33 - train: epoch 0068, iter [03600, 05004], lr: 0.023582, loss: 1.2590
2022-08-27 23:23:07 - train: epoch 0068, iter [03700, 05004], lr: 0.023555, loss: 1.4571
2022-08-27 23:23:43 - train: epoch 0068, iter [03800, 05004], lr: 0.023529, loss: 1.4914
2022-08-27 23:24:17 - train: epoch 0068, iter [03900, 05004], lr: 0.023502, loss: 1.4999
2022-08-27 23:24:52 - train: epoch 0068, iter [04000, 05004], lr: 0.023475, loss: 1.3007
2022-08-27 23:25:26 - train: epoch 0068, iter [04100, 05004], lr: 0.023449, loss: 1.1476
2022-08-27 23:26:01 - train: epoch 0068, iter [04200, 05004], lr: 0.023422, loss: 1.4683
2022-08-27 23:26:35 - train: epoch 0068, iter [04300, 05004], lr: 0.023396, loss: 1.4649
2022-08-27 23:27:10 - train: epoch 0068, iter [04400, 05004], lr: 0.023369, loss: 1.3452
2022-08-27 23:27:44 - train: epoch 0068, iter [04500, 05004], lr: 0.023342, loss: 1.4584
2022-08-27 23:28:19 - train: epoch 0068, iter [04600, 05004], lr: 0.023316, loss: 1.5175
2022-08-27 23:28:54 - train: epoch 0068, iter [04700, 05004], lr: 0.023289, loss: 1.6366
2022-08-27 23:29:28 - train: epoch 0068, iter [04800, 05004], lr: 0.023263, loss: 1.4171
2022-08-27 23:30:03 - train: epoch 0068, iter [04900, 05004], lr: 0.023236, loss: 1.5107
2022-08-27 23:30:37 - train: epoch 0068, iter [05000, 05004], lr: 0.023210, loss: 1.2788
2022-08-27 23:30:38 - train: epoch 068, train_loss: 1.3345
2022-08-27 23:31:55 - eval: epoch: 068, acc1: 68.112%, acc5: 88.814%, test_loss: 1.2897, per_image_load_time: 2.460ms, per_image_inference_time: 0.490ms
2022-08-27 23:31:56 - until epoch: 068, best_acc1: 68.112%
2022-08-27 23:31:56 - epoch 069 lr: 0.023208
2022-08-27 23:32:35 - train: epoch 0069, iter [00100, 05004], lr: 0.023182, loss: 1.4717
2022-08-27 23:33:10 - train: epoch 0069, iter [00200, 05004], lr: 0.023156, loss: 1.5314
2022-08-27 23:33:45 - train: epoch 0069, iter [00300, 05004], lr: 0.023129, loss: 1.3212
2022-08-27 23:34:19 - train: epoch 0069, iter [00400, 05004], lr: 0.023103, loss: 1.2087
2022-08-27 23:34:54 - train: epoch 0069, iter [00500, 05004], lr: 0.023076, loss: 1.1978
2022-08-27 23:35:29 - train: epoch 0069, iter [00600, 05004], lr: 0.023050, loss: 1.1878
2022-08-27 23:36:03 - train: epoch 0069, iter [00700, 05004], lr: 0.023023, loss: 1.1633
2022-08-27 23:36:37 - train: epoch 0069, iter [00800, 05004], lr: 0.022997, loss: 1.3541
2022-08-27 23:37:11 - train: epoch 0069, iter [00900, 05004], lr: 0.022971, loss: 1.2073
2022-08-27 23:37:47 - train: epoch 0069, iter [01000, 05004], lr: 0.022944, loss: 1.2081
2022-08-27 23:38:22 - train: epoch 0069, iter [01100, 05004], lr: 0.022918, loss: 1.2546
2022-08-27 23:38:56 - train: epoch 0069, iter [01200, 05004], lr: 0.022891, loss: 1.2484
2022-08-27 23:39:31 - train: epoch 0069, iter [01300, 05004], lr: 0.022865, loss: 1.4981
2022-08-27 23:40:05 - train: epoch 0069, iter [01400, 05004], lr: 0.022839, loss: 1.3602
2022-08-27 23:40:39 - train: epoch 0069, iter [01500, 05004], lr: 0.022812, loss: 1.3744
2022-08-27 23:41:14 - train: epoch 0069, iter [01600, 05004], lr: 0.022786, loss: 1.3752
2022-08-27 23:41:49 - train: epoch 0069, iter [01700, 05004], lr: 0.022760, loss: 1.2114
2022-08-27 23:42:22 - train: epoch 0069, iter [01800, 05004], lr: 0.022733, loss: 1.1689
2022-08-27 23:42:57 - train: epoch 0069, iter [01900, 05004], lr: 0.022707, loss: 1.3046
2022-08-27 23:43:32 - train: epoch 0069, iter [02000, 05004], lr: 0.022681, loss: 1.1152
2022-08-27 23:44:06 - train: epoch 0069, iter [02100, 05004], lr: 0.022654, loss: 1.3243
2022-08-27 23:44:41 - train: epoch 0069, iter [02200, 05004], lr: 0.022628, loss: 1.3773
2022-08-27 23:45:15 - train: epoch 0069, iter [02300, 05004], lr: 0.022602, loss: 1.2986
2022-08-27 23:45:50 - train: epoch 0069, iter [02400, 05004], lr: 0.022576, loss: 1.4354
2022-08-27 23:46:24 - train: epoch 0069, iter [02500, 05004], lr: 0.022549, loss: 1.3655
2022-08-27 23:46:59 - train: epoch 0069, iter [02600, 05004], lr: 0.022523, loss: 1.4198
2022-08-27 23:47:33 - train: epoch 0069, iter [02700, 05004], lr: 0.022497, loss: 1.4967
2022-08-27 23:48:08 - train: epoch 0069, iter [02800, 05004], lr: 0.022471, loss: 1.4338
2022-08-27 23:48:43 - train: epoch 0069, iter [02900, 05004], lr: 0.022445, loss: 1.1435
2022-08-27 23:49:18 - train: epoch 0069, iter [03000, 05004], lr: 0.022418, loss: 1.3039
2022-08-27 23:49:53 - train: epoch 0069, iter [03100, 05004], lr: 0.022392, loss: 1.3076
2022-08-27 23:50:28 - train: epoch 0069, iter [03200, 05004], lr: 0.022366, loss: 1.2776
2022-08-27 23:51:02 - train: epoch 0069, iter [03300, 05004], lr: 0.022340, loss: 1.2538
2022-08-27 23:51:38 - train: epoch 0069, iter [03400, 05004], lr: 0.022314, loss: 1.1296
2022-08-27 23:52:12 - train: epoch 0069, iter [03500, 05004], lr: 0.022288, loss: 1.2006
2022-08-27 23:52:46 - train: epoch 0069, iter [03600, 05004], lr: 0.022261, loss: 1.1947
2022-08-27 23:53:20 - train: epoch 0069, iter [03700, 05004], lr: 0.022235, loss: 1.4586
2022-08-27 23:53:55 - train: epoch 0069, iter [03800, 05004], lr: 0.022209, loss: 1.2992
2022-08-27 23:54:30 - train: epoch 0069, iter [03900, 05004], lr: 0.022183, loss: 1.3230
2022-08-27 23:55:04 - train: epoch 0069, iter [04000, 05004], lr: 0.022157, loss: 1.3642
2022-08-27 23:55:39 - train: epoch 0069, iter [04100, 05004], lr: 0.022131, loss: 1.4887
2022-08-27 23:56:14 - train: epoch 0069, iter [04200, 05004], lr: 0.022105, loss: 1.1731
2022-08-27 23:56:48 - train: epoch 0069, iter [04300, 05004], lr: 0.022079, loss: 1.3585
2022-08-27 23:57:23 - train: epoch 0069, iter [04400, 05004], lr: 0.022053, loss: 1.3864
2022-08-27 23:57:57 - train: epoch 0069, iter [04500, 05004], lr: 0.022027, loss: 1.3201
2022-08-27 23:58:31 - train: epoch 0069, iter [04600, 05004], lr: 0.022001, loss: 1.5923
2022-08-27 23:59:06 - train: epoch 0069, iter [04700, 05004], lr: 0.021975, loss: 1.3013
2022-08-27 23:59:42 - train: epoch 0069, iter [04800, 05004], lr: 0.021949, loss: 1.3320
2022-08-28 00:00:16 - train: epoch 0069, iter [04900, 05004], lr: 0.021923, loss: 1.2323
2022-08-28 00:00:49 - train: epoch 0069, iter [05000, 05004], lr: 0.021897, loss: 1.3570
2022-08-28 00:00:50 - train: epoch 069, train_loss: 1.3137
2022-08-28 00:02:06 - eval: epoch: 069, acc1: 69.226%, acc5: 89.534%, test_loss: 1.2385, per_image_load_time: 2.401ms, per_image_inference_time: 0.502ms
2022-08-28 00:02:07 - until epoch: 069, best_acc1: 69.226%
2022-08-28 00:02:07 - epoch 070 lr: 0.021896
2022-08-28 00:02:46 - train: epoch 0070, iter [00100, 05004], lr: 0.021870, loss: 1.3049
2022-08-28 00:03:21 - train: epoch 0070, iter [00200, 05004], lr: 0.021844, loss: 1.3241
2022-08-28 00:03:55 - train: epoch 0070, iter [00300, 05004], lr: 0.021818, loss: 1.3555
2022-08-28 00:04:30 - train: epoch 0070, iter [00400, 05004], lr: 0.021792, loss: 1.1919
2022-08-28 00:05:05 - train: epoch 0070, iter [00500, 05004], lr: 0.021766, loss: 1.3058
2022-08-28 00:05:39 - train: epoch 0070, iter [00600, 05004], lr: 0.021740, loss: 1.2543
2022-08-28 00:06:14 - train: epoch 0070, iter [00700, 05004], lr: 0.021714, loss: 1.2290
2022-08-28 00:06:49 - train: epoch 0070, iter [00800, 05004], lr: 0.021688, loss: 1.1127
2022-08-28 00:07:23 - train: epoch 0070, iter [00900, 05004], lr: 0.021663, loss: 1.3977
2022-08-28 00:07:57 - train: epoch 0070, iter [01000, 05004], lr: 0.021637, loss: 1.2069
2022-08-28 00:08:32 - train: epoch 0070, iter [01100, 05004], lr: 0.021611, loss: 1.6357
2022-08-28 00:09:06 - train: epoch 0070, iter [01200, 05004], lr: 0.021585, loss: 1.0911
2022-08-28 00:09:41 - train: epoch 0070, iter [01300, 05004], lr: 0.021559, loss: 1.2873
2022-08-28 00:10:15 - train: epoch 0070, iter [01400, 05004], lr: 0.021533, loss: 1.3213
2022-08-28 00:10:49 - train: epoch 0070, iter [01500, 05004], lr: 0.021508, loss: 1.1584
2022-08-28 00:11:23 - train: epoch 0070, iter [01600, 05004], lr: 0.021482, loss: 1.3622
2022-08-28 00:11:59 - train: epoch 0070, iter [01700, 05004], lr: 0.021456, loss: 1.3842
2022-08-28 00:12:34 - train: epoch 0070, iter [01800, 05004], lr: 0.021430, loss: 1.2698
2022-08-28 00:13:08 - train: epoch 0070, iter [01900, 05004], lr: 0.021405, loss: 1.2483
2022-08-28 00:13:42 - train: epoch 0070, iter [02000, 05004], lr: 0.021379, loss: 1.2482
2022-08-28 00:14:18 - train: epoch 0070, iter [02100, 05004], lr: 0.021353, loss: 1.3108
2022-08-28 00:14:53 - train: epoch 0070, iter [02200, 05004], lr: 0.021327, loss: 1.2900
2022-08-28 00:15:27 - train: epoch 0070, iter [02300, 05004], lr: 0.021302, loss: 1.2383
2022-08-28 00:16:02 - train: epoch 0070, iter [02400, 05004], lr: 0.021276, loss: 1.3801
2022-08-28 00:16:37 - train: epoch 0070, iter [02500, 05004], lr: 0.021250, loss: 1.3390
2022-08-28 00:17:11 - train: epoch 0070, iter [02600, 05004], lr: 0.021225, loss: 1.2326
2022-08-28 00:17:46 - train: epoch 0070, iter [02700, 05004], lr: 0.021199, loss: 1.2905
2022-08-28 00:18:20 - train: epoch 0070, iter [02800, 05004], lr: 0.021173, loss: 1.3898
2022-08-28 00:18:55 - train: epoch 0070, iter [02900, 05004], lr: 0.021148, loss: 1.5204
2022-08-28 00:19:29 - train: epoch 0070, iter [03000, 05004], lr: 0.021122, loss: 1.2781
2022-08-28 00:20:04 - train: epoch 0070, iter [03100, 05004], lr: 0.021096, loss: 1.2628
2022-08-28 00:20:38 - train: epoch 0070, iter [03200, 05004], lr: 0.021071, loss: 1.4014
2022-08-28 00:21:14 - train: epoch 0070, iter [03300, 05004], lr: 0.021045, loss: 1.2561
2022-08-28 00:21:49 - train: epoch 0070, iter [03400, 05004], lr: 0.021020, loss: 1.3764
2022-08-28 00:22:24 - train: epoch 0070, iter [03500, 05004], lr: 0.020994, loss: 1.2643
2022-08-28 00:22:58 - train: epoch 0070, iter [03600, 05004], lr: 0.020968, loss: 1.4218
2022-08-28 00:23:34 - train: epoch 0070, iter [03700, 05004], lr: 0.020943, loss: 1.3309
2022-08-28 00:24:07 - train: epoch 0070, iter [03800, 05004], lr: 0.020917, loss: 1.2645
2022-08-28 00:24:42 - train: epoch 0070, iter [03900, 05004], lr: 0.020892, loss: 1.1884
2022-08-28 00:25:18 - train: epoch 0070, iter [04000, 05004], lr: 0.020866, loss: 1.1603
2022-08-28 00:25:53 - train: epoch 0070, iter [04100, 05004], lr: 0.020841, loss: 1.2055
2022-08-28 00:26:27 - train: epoch 0070, iter [04200, 05004], lr: 0.020815, loss: 1.3234
2022-08-28 00:27:02 - train: epoch 0070, iter [04300, 05004], lr: 0.020790, loss: 1.3460
2022-08-28 00:27:36 - train: epoch 0070, iter [04400, 05004], lr: 0.020764, loss: 1.2925
2022-08-28 00:28:10 - train: epoch 0070, iter [04500, 05004], lr: 0.020739, loss: 1.3104
2022-08-28 00:28:45 - train: epoch 0070, iter [04600, 05004], lr: 0.020713, loss: 1.4787
2022-08-28 00:29:19 - train: epoch 0070, iter [04700, 05004], lr: 0.020688, loss: 1.4262
2022-08-28 00:29:53 - train: epoch 0070, iter [04800, 05004], lr: 0.020663, loss: 1.4438
2022-08-28 00:30:27 - train: epoch 0070, iter [04900, 05004], lr: 0.020637, loss: 1.4336
2022-08-28 00:31:00 - train: epoch 0070, iter [05000, 05004], lr: 0.020612, loss: 1.2518
2022-08-28 00:31:01 - train: epoch 070, train_loss: 1.2950
2022-08-28 00:32:17 - eval: epoch: 070, acc1: 70.218%, acc5: 89.848%, test_loss: 1.2008, per_image_load_time: 2.416ms, per_image_inference_time: 0.512ms
2022-08-28 00:32:18 - until epoch: 070, best_acc1: 70.218%
2022-08-28 00:32:18 - epoch 071 lr: 0.020610
2022-08-28 00:32:57 - train: epoch 0071, iter [00100, 05004], lr: 0.020585, loss: 1.0335
2022-08-28 00:33:30 - train: epoch 0071, iter [00200, 05004], lr: 0.020560, loss: 1.2159
2022-08-28 00:34:04 - train: epoch 0071, iter [00300, 05004], lr: 0.020535, loss: 1.1855
2022-08-28 00:34:39 - train: epoch 0071, iter [00400, 05004], lr: 0.020509, loss: 1.3521
2022-08-28 00:35:13 - train: epoch 0071, iter [00500, 05004], lr: 0.020484, loss: 1.3003
2022-08-28 00:35:46 - train: epoch 0071, iter [00600, 05004], lr: 0.020459, loss: 1.4016
2022-08-28 00:36:20 - train: epoch 0071, iter [00700, 05004], lr: 0.020433, loss: 1.3926
2022-08-28 00:36:53 - train: epoch 0071, iter [00800, 05004], lr: 0.020408, loss: 1.2126
2022-08-28 00:37:27 - train: epoch 0071, iter [00900, 05004], lr: 0.020383, loss: 1.3763
2022-08-28 00:38:01 - train: epoch 0071, iter [01000, 05004], lr: 0.020357, loss: 1.3750
2022-08-28 00:38:35 - train: epoch 0071, iter [01100, 05004], lr: 0.020332, loss: 1.4280
2022-08-28 00:39:08 - train: epoch 0071, iter [01200, 05004], lr: 0.020307, loss: 1.2388
2022-08-28 00:39:43 - train: epoch 0071, iter [01300, 05004], lr: 0.020282, loss: 1.3420
2022-08-28 00:40:17 - train: epoch 0071, iter [01400, 05004], lr: 0.020256, loss: 1.3054
2022-08-28 00:40:51 - train: epoch 0071, iter [01500, 05004], lr: 0.020231, loss: 1.0585
2022-08-28 00:41:26 - train: epoch 0071, iter [01600, 05004], lr: 0.020206, loss: 1.2012
2022-08-28 00:42:00 - train: epoch 0071, iter [01700, 05004], lr: 0.020181, loss: 1.2223
2022-08-28 00:42:35 - train: epoch 0071, iter [01800, 05004], lr: 0.020156, loss: 1.2167
2022-08-28 00:43:08 - train: epoch 0071, iter [01900, 05004], lr: 0.020130, loss: 1.2148
2022-08-28 00:43:43 - train: epoch 0071, iter [02000, 05004], lr: 0.020105, loss: 1.3372
2022-08-28 00:44:17 - train: epoch 0071, iter [02100, 05004], lr: 0.020080, loss: 1.0394
2022-08-28 00:44:51 - train: epoch 0071, iter [02200, 05004], lr: 0.020055, loss: 1.2343
2022-08-28 00:45:27 - train: epoch 0071, iter [02300, 05004], lr: 0.020030, loss: 1.2081
2022-08-28 00:46:02 - train: epoch 0071, iter [02400, 05004], lr: 0.020005, loss: 1.1921
2022-08-28 00:46:37 - train: epoch 0071, iter [02500, 05004], lr: 0.019979, loss: 1.3557
2022-08-28 00:47:11 - train: epoch 0071, iter [02600, 05004], lr: 0.019954, loss: 1.2156
2022-08-28 00:47:47 - train: epoch 0071, iter [02700, 05004], lr: 0.019929, loss: 1.3680
2022-08-28 00:48:21 - train: epoch 0071, iter [02800, 05004], lr: 0.019904, loss: 1.3489
2022-08-28 00:48:56 - train: epoch 0071, iter [02900, 05004], lr: 0.019879, loss: 1.2811
2022-08-28 00:49:30 - train: epoch 0071, iter [03000, 05004], lr: 0.019854, loss: 1.3589
2022-08-28 00:50:06 - train: epoch 0071, iter [03100, 05004], lr: 0.019829, loss: 1.3497
2022-08-28 00:50:40 - train: epoch 0071, iter [03200, 05004], lr: 0.019804, loss: 1.1588
2022-08-28 00:51:16 - train: epoch 0071, iter [03300, 05004], lr: 0.019779, loss: 1.1578
2022-08-28 00:51:51 - train: epoch 0071, iter [03400, 05004], lr: 0.019754, loss: 1.1965
2022-08-28 00:52:25 - train: epoch 0071, iter [03500, 05004], lr: 0.019729, loss: 1.4566
2022-08-28 00:52:59 - train: epoch 0071, iter [03600, 05004], lr: 0.019704, loss: 1.4665
2022-08-28 00:53:35 - train: epoch 0071, iter [03700, 05004], lr: 0.019679, loss: 1.3355
2022-08-28 00:54:09 - train: epoch 0071, iter [03800, 05004], lr: 0.019654, loss: 1.2810
2022-08-28 00:54:44 - train: epoch 0071, iter [03900, 05004], lr: 0.019629, loss: 1.3318
2022-08-28 00:55:20 - train: epoch 0071, iter [04000, 05004], lr: 0.019604, loss: 1.4775
2022-08-28 00:55:54 - train: epoch 0071, iter [04100, 05004], lr: 0.019579, loss: 1.2839
2022-08-28 00:56:30 - train: epoch 0071, iter [04200, 05004], lr: 0.019554, loss: 1.4039
2022-08-28 00:57:04 - train: epoch 0071, iter [04300, 05004], lr: 0.019530, loss: 1.3664
2022-08-28 00:57:39 - train: epoch 0071, iter [04400, 05004], lr: 0.019505, loss: 1.3181
2022-08-28 00:58:14 - train: epoch 0071, iter [04500, 05004], lr: 0.019480, loss: 1.2636
2022-08-28 00:58:48 - train: epoch 0071, iter [04600, 05004], lr: 0.019455, loss: 1.3393
2022-08-28 00:59:24 - train: epoch 0071, iter [04700, 05004], lr: 0.019430, loss: 1.1792
2022-08-28 00:59:59 - train: epoch 0071, iter [04800, 05004], lr: 0.019405, loss: 1.1577
2022-08-28 01:00:34 - train: epoch 0071, iter [04900, 05004], lr: 0.019380, loss: 1.1324
2022-08-28 01:01:08 - train: epoch 0071, iter [05000, 05004], lr: 0.019356, loss: 1.1157
2022-08-28 01:01:09 - train: epoch 071, train_loss: 1.2741
2022-08-28 01:02:26 - eval: epoch: 071, acc1: 70.446%, acc5: 90.070%, test_loss: 1.1910, per_image_load_time: 2.072ms, per_image_inference_time: 0.510ms
2022-08-28 01:02:26 - until epoch: 071, best_acc1: 70.446%
2022-08-28 01:02:26 - epoch 072 lr: 0.019354
2022-08-28 01:03:06 - train: epoch 0072, iter [00100, 05004], lr: 0.019330, loss: 1.3165
2022-08-28 01:03:42 - train: epoch 0072, iter [00200, 05004], lr: 0.019305, loss: 1.0878
2022-08-28 01:04:15 - train: epoch 0072, iter [00300, 05004], lr: 0.019280, loss: 1.1499
2022-08-28 01:04:50 - train: epoch 0072, iter [00400, 05004], lr: 0.019256, loss: 1.1551
2022-08-28 01:05:25 - train: epoch 0072, iter [00500, 05004], lr: 0.019231, loss: 1.1389
2022-08-28 01:06:00 - train: epoch 0072, iter [00600, 05004], lr: 0.019206, loss: 1.1283
2022-08-28 01:06:34 - train: epoch 0072, iter [00700, 05004], lr: 0.019181, loss: 1.2647
2022-08-28 01:07:09 - train: epoch 0072, iter [00800, 05004], lr: 0.019157, loss: 1.4292
2022-08-28 01:07:42 - train: epoch 0072, iter [00900, 05004], lr: 0.019132, loss: 1.0862
2022-08-28 01:08:16 - train: epoch 0072, iter [01000, 05004], lr: 0.019107, loss: 1.2107
2022-08-28 01:08:50 - train: epoch 0072, iter [01100, 05004], lr: 0.019083, loss: 1.3488
2022-08-28 01:09:25 - train: epoch 0072, iter [01200, 05004], lr: 0.019058, loss: 1.1827
2022-08-28 01:10:00 - train: epoch 0072, iter [01300, 05004], lr: 0.019033, loss: 1.2110
2022-08-28 01:10:34 - train: epoch 0072, iter [01400, 05004], lr: 0.019009, loss: 1.3159
2022-08-28 01:11:09 - train: epoch 0072, iter [01500, 05004], lr: 0.018984, loss: 1.1856
2022-08-28 01:11:43 - train: epoch 0072, iter [01600, 05004], lr: 0.018959, loss: 1.3093
2022-08-28 01:12:19 - train: epoch 0072, iter [01700, 05004], lr: 0.018935, loss: 1.2021
2022-08-28 01:12:53 - train: epoch 0072, iter [01800, 05004], lr: 0.018910, loss: 1.0886
2022-08-28 01:13:27 - train: epoch 0072, iter [01900, 05004], lr: 0.018886, loss: 1.1919
2022-08-28 01:14:02 - train: epoch 0072, iter [02000, 05004], lr: 0.018861, loss: 1.3002
2022-08-28 01:14:37 - train: epoch 0072, iter [02100, 05004], lr: 0.018836, loss: 1.3967
2022-08-28 01:15:13 - train: epoch 0072, iter [02200, 05004], lr: 0.018812, loss: 1.3659
2022-08-28 01:15:47 - train: epoch 0072, iter [02300, 05004], lr: 0.018787, loss: 1.4510
2022-08-28 01:16:21 - train: epoch 0072, iter [02400, 05004], lr: 0.018763, loss: 1.1132
2022-08-28 01:16:57 - train: epoch 0072, iter [02500, 05004], lr: 0.018738, loss: 1.0695
2022-08-28 01:17:32 - train: epoch 0072, iter [02600, 05004], lr: 0.018714, loss: 1.0856
2022-08-28 01:18:06 - train: epoch 0072, iter [02700, 05004], lr: 0.018689, loss: 1.3046
2022-08-28 01:18:41 - train: epoch 0072, iter [02800, 05004], lr: 0.018665, loss: 1.3403
2022-08-28 01:19:15 - train: epoch 0072, iter [02900, 05004], lr: 0.018640, loss: 1.2056
2022-08-28 01:19:51 - train: epoch 0072, iter [03000, 05004], lr: 0.018616, loss: 1.3079
2022-08-28 01:20:25 - train: epoch 0072, iter [03100, 05004], lr: 0.018592, loss: 1.3925
2022-08-28 01:21:00 - train: epoch 0072, iter [03200, 05004], lr: 0.018567, loss: 1.2016
2022-08-28 01:21:34 - train: epoch 0072, iter [03300, 05004], lr: 0.018543, loss: 1.2374
2022-08-28 01:22:10 - train: epoch 0072, iter [03400, 05004], lr: 0.018518, loss: 1.3597
2022-08-28 01:22:45 - train: epoch 0072, iter [03500, 05004], lr: 0.018494, loss: 1.2366
2022-08-28 01:23:19 - train: epoch 0072, iter [03600, 05004], lr: 0.018470, loss: 1.2569
2022-08-28 01:23:54 - train: epoch 0072, iter [03700, 05004], lr: 0.018445, loss: 1.2838
2022-08-28 01:24:29 - train: epoch 0072, iter [03800, 05004], lr: 0.018421, loss: 1.3339
2022-08-28 01:25:03 - train: epoch 0072, iter [03900, 05004], lr: 0.018397, loss: 1.4218
2022-08-28 01:25:39 - train: epoch 0072, iter [04000, 05004], lr: 0.018372, loss: 1.1692
2022-08-28 01:26:13 - train: epoch 0072, iter [04100, 05004], lr: 0.018348, loss: 1.4584
2022-08-28 01:26:48 - train: epoch 0072, iter [04200, 05004], lr: 0.018324, loss: 1.2645
2022-08-28 01:27:21 - train: epoch 0072, iter [04300, 05004], lr: 0.018299, loss: 1.3239
2022-08-28 01:27:57 - train: epoch 0072, iter [04400, 05004], lr: 0.018275, loss: 1.1176
2022-08-28 01:28:31 - train: epoch 0072, iter [04500, 05004], lr: 0.018251, loss: 1.4718
2022-08-28 01:29:06 - train: epoch 0072, iter [04600, 05004], lr: 0.018227, loss: 1.1268
2022-08-28 01:29:41 - train: epoch 0072, iter [04700, 05004], lr: 0.018202, loss: 1.4037
2022-08-28 01:30:15 - train: epoch 0072, iter [04800, 05004], lr: 0.018178, loss: 1.4129
2022-08-28 01:30:50 - train: epoch 0072, iter [04900, 05004], lr: 0.018154, loss: 1.4185
2022-08-28 01:31:23 - train: epoch 0072, iter [05000, 05004], lr: 0.018130, loss: 1.2600
2022-08-28 01:31:24 - train: epoch 072, train_loss: 1.2544
2022-08-28 01:32:41 - eval: epoch: 072, acc1: 70.390%, acc5: 90.090%, test_loss: 1.2000, per_image_load_time: 2.460ms, per_image_inference_time: 0.490ms
2022-08-28 01:32:42 - until epoch: 072, best_acc1: 70.446%
2022-08-28 01:32:42 - epoch 073 lr: 0.018129
2022-08-28 01:33:22 - train: epoch 0073, iter [00100, 05004], lr: 0.018105, loss: 1.4287
2022-08-28 01:33:56 - train: epoch 0073, iter [00200, 05004], lr: 0.018080, loss: 1.2452
2022-08-28 01:34:31 - train: epoch 0073, iter [00300, 05004], lr: 0.018056, loss: 1.3088
2022-08-28 01:35:06 - train: epoch 0073, iter [00400, 05004], lr: 0.018032, loss: 0.9400
2022-08-28 01:35:40 - train: epoch 0073, iter [00500, 05004], lr: 0.018008, loss: 1.0798
2022-08-28 01:36:15 - train: epoch 0073, iter [00600, 05004], lr: 0.017984, loss: 1.0608
2022-08-28 01:36:50 - train: epoch 0073, iter [00700, 05004], lr: 0.017960, loss: 1.2592
2022-08-28 01:37:24 - train: epoch 0073, iter [00800, 05004], lr: 0.017936, loss: 1.1847
2022-08-28 01:37:59 - train: epoch 0073, iter [00900, 05004], lr: 0.017912, loss: 1.0315
2022-08-28 01:38:33 - train: epoch 0073, iter [01000, 05004], lr: 0.017888, loss: 1.0793
2022-08-28 01:39:08 - train: epoch 0073, iter [01100, 05004], lr: 0.017864, loss: 1.3041
2022-08-28 01:39:43 - train: epoch 0073, iter [01200, 05004], lr: 0.017839, loss: 1.1676
2022-08-28 01:40:18 - train: epoch 0073, iter [01300, 05004], lr: 0.017815, loss: 1.2488
2022-08-28 01:40:52 - train: epoch 0073, iter [01400, 05004], lr: 0.017791, loss: 1.1314
2022-08-28 01:41:28 - train: epoch 0073, iter [01500, 05004], lr: 0.017767, loss: 1.2193
2022-08-28 01:42:02 - train: epoch 0073, iter [01600, 05004], lr: 0.017743, loss: 1.2510
2022-08-28 01:42:36 - train: epoch 0073, iter [01700, 05004], lr: 0.017719, loss: 1.4711
2022-08-28 01:43:11 - train: epoch 0073, iter [01800, 05004], lr: 0.017695, loss: 1.1035
2022-08-28 01:43:45 - train: epoch 0073, iter [01900, 05004], lr: 0.017672, loss: 1.2960
2022-08-28 01:44:20 - train: epoch 0073, iter [02000, 05004], lr: 0.017648, loss: 1.0293
2022-08-28 01:44:55 - train: epoch 0073, iter [02100, 05004], lr: 0.017624, loss: 1.2461
2022-08-28 01:45:30 - train: epoch 0073, iter [02200, 05004], lr: 0.017600, loss: 1.3886
2022-08-28 01:46:05 - train: epoch 0073, iter [02300, 05004], lr: 0.017576, loss: 1.3660
2022-08-28 01:46:40 - train: epoch 0073, iter [02400, 05004], lr: 0.017552, loss: 1.1922
2022-08-28 01:47:15 - train: epoch 0073, iter [02500, 05004], lr: 0.017528, loss: 1.4341
2022-08-28 01:47:50 - train: epoch 0073, iter [02600, 05004], lr: 0.017504, loss: 1.3121
2022-08-28 01:48:24 - train: epoch 0073, iter [02700, 05004], lr: 0.017480, loss: 1.2014
2022-08-28 01:48:59 - train: epoch 0073, iter [02800, 05004], lr: 0.017457, loss: 1.2045
2022-08-28 01:49:33 - train: epoch 0073, iter [02900, 05004], lr: 0.017433, loss: 1.2410
2022-08-28 01:50:08 - train: epoch 0073, iter [03000, 05004], lr: 0.017409, loss: 1.1418
2022-08-28 01:50:42 - train: epoch 0073, iter [03100, 05004], lr: 0.017385, loss: 1.1361
2022-08-28 01:51:17 - train: epoch 0073, iter [03200, 05004], lr: 0.017361, loss: 1.0931
2022-08-28 01:51:51 - train: epoch 0073, iter [03300, 05004], lr: 0.017338, loss: 1.1904
2022-08-28 01:52:27 - train: epoch 0073, iter [03400, 05004], lr: 0.017314, loss: 1.2041
2022-08-28 01:53:01 - train: epoch 0073, iter [03500, 05004], lr: 0.017290, loss: 1.2997
2022-08-28 01:53:35 - train: epoch 0073, iter [03600, 05004], lr: 0.017266, loss: 1.2288
2022-08-28 01:54:10 - train: epoch 0073, iter [03700, 05004], lr: 0.017243, loss: 1.2641
2022-08-28 01:54:44 - train: epoch 0073, iter [03800, 05004], lr: 0.017219, loss: 1.5496
2022-08-28 01:55:20 - train: epoch 0073, iter [03900, 05004], lr: 0.017195, loss: 1.2267
2022-08-28 01:55:53 - train: epoch 0073, iter [04000, 05004], lr: 0.017171, loss: 1.3844
2022-08-28 01:56:28 - train: epoch 0073, iter [04100, 05004], lr: 0.017148, loss: 1.2302
2022-08-28 01:57:03 - train: epoch 0073, iter [04200, 05004], lr: 0.017124, loss: 1.3352
2022-08-28 01:57:38 - train: epoch 0073, iter [04300, 05004], lr: 0.017100, loss: 1.2792
2022-08-28 01:58:13 - train: epoch 0073, iter [04400, 05004], lr: 0.017077, loss: 1.3161
2022-08-28 01:58:47 - train: epoch 0073, iter [04500, 05004], lr: 0.017053, loss: 1.1363
2022-08-28 01:59:22 - train: epoch 0073, iter [04600, 05004], lr: 0.017030, loss: 1.3873
2022-08-28 01:59:57 - train: epoch 0073, iter [04700, 05004], lr: 0.017006, loss: 1.0277
2022-08-28 02:00:32 - train: epoch 0073, iter [04800, 05004], lr: 0.016982, loss: 1.3852
2022-08-28 02:01:06 - train: epoch 0073, iter [04900, 05004], lr: 0.016959, loss: 1.1634
2022-08-28 02:01:41 - train: epoch 0073, iter [05000, 05004], lr: 0.016935, loss: 1.2886
2022-08-28 02:01:42 - train: epoch 073, train_loss: 1.2334
2022-08-28 02:02:59 - eval: epoch: 073, acc1: 70.846%, acc5: 90.194%, test_loss: 1.1753, per_image_load_time: 2.437ms, per_image_inference_time: 0.496ms
2022-08-28 02:02:59 - until epoch: 073, best_acc1: 70.846%
2022-08-28 02:02:59 - epoch 074 lr: 0.016934
2022-08-28 02:03:40 - train: epoch 0074, iter [00100, 05004], lr: 0.016911, loss: 1.0284
2022-08-28 02:04:14 - train: epoch 0074, iter [00200, 05004], lr: 0.016887, loss: 1.2142
2022-08-28 02:04:49 - train: epoch 0074, iter [00300, 05004], lr: 0.016864, loss: 1.2782
2022-08-28 02:05:22 - train: epoch 0074, iter [00400, 05004], lr: 0.016840, loss: 1.3514
2022-08-28 02:05:57 - train: epoch 0074, iter [00500, 05004], lr: 0.016817, loss: 1.2216
2022-08-28 02:06:32 - train: epoch 0074, iter [00600, 05004], lr: 0.016793, loss: 1.3106
2022-08-28 02:07:07 - train: epoch 0074, iter [00700, 05004], lr: 0.016770, loss: 1.1291
2022-08-28 02:07:41 - train: epoch 0074, iter [00800, 05004], lr: 0.016746, loss: 1.3384
2022-08-28 02:08:16 - train: epoch 0074, iter [00900, 05004], lr: 0.016723, loss: 1.1619
2022-08-28 02:08:50 - train: epoch 0074, iter [01000, 05004], lr: 0.016700, loss: 1.3218
2022-08-28 02:09:25 - train: epoch 0074, iter [01100, 05004], lr: 0.016676, loss: 1.2957
2022-08-28 02:09:59 - train: epoch 0074, iter [01200, 05004], lr: 0.016653, loss: 1.1358
2022-08-28 02:10:34 - train: epoch 0074, iter [01300, 05004], lr: 0.016629, loss: 1.2544
2022-08-28 02:11:08 - train: epoch 0074, iter [01400, 05004], lr: 0.016606, loss: 1.1760
2022-08-28 02:11:42 - train: epoch 0074, iter [01500, 05004], lr: 0.016583, loss: 1.2646
2022-08-28 02:12:17 - train: epoch 0074, iter [01600, 05004], lr: 0.016559, loss: 1.0633
2022-08-28 02:12:51 - train: epoch 0074, iter [01700, 05004], lr: 0.016536, loss: 1.2529
2022-08-28 02:13:25 - train: epoch 0074, iter [01800, 05004], lr: 0.016513, loss: 1.5229
2022-08-28 02:14:01 - train: epoch 0074, iter [01900, 05004], lr: 0.016489, loss: 1.3496
2022-08-28 02:14:35 - train: epoch 0074, iter [02000, 05004], lr: 0.016466, loss: 1.0225
2022-08-28 02:15:11 - train: epoch 0074, iter [02100, 05004], lr: 0.016443, loss: 1.2474
2022-08-28 02:15:45 - train: epoch 0074, iter [02200, 05004], lr: 0.016420, loss: 1.4484
2022-08-28 02:16:19 - train: epoch 0074, iter [02300, 05004], lr: 0.016396, loss: 1.3547
2022-08-28 02:16:53 - train: epoch 0074, iter [02400, 05004], lr: 0.016373, loss: 1.2355
2022-08-28 02:17:28 - train: epoch 0074, iter [02500, 05004], lr: 0.016350, loss: 1.1042
2022-08-28 02:18:02 - train: epoch 0074, iter [02600, 05004], lr: 0.016327, loss: 0.9576
2022-08-28 02:18:37 - train: epoch 0074, iter [02700, 05004], lr: 0.016303, loss: 1.1740
2022-08-28 02:19:13 - train: epoch 0074, iter [02800, 05004], lr: 0.016280, loss: 1.5118
2022-08-28 02:19:46 - train: epoch 0074, iter [02900, 05004], lr: 0.016257, loss: 1.2479
2022-08-28 02:20:21 - train: epoch 0074, iter [03000, 05004], lr: 0.016234, loss: 1.2763
2022-08-28 02:20:56 - train: epoch 0074, iter [03100, 05004], lr: 0.016211, loss: 1.1608
2022-08-28 02:21:30 - train: epoch 0074, iter [03200, 05004], lr: 0.016188, loss: 1.1668
2022-08-28 02:22:05 - train: epoch 0074, iter [03300, 05004], lr: 0.016165, loss: 1.2392
2022-08-28 02:22:40 - train: epoch 0074, iter [03400, 05004], lr: 0.016141, loss: 1.2910
2022-08-28 02:23:14 - train: epoch 0074, iter [03500, 05004], lr: 0.016118, loss: 1.0681
2022-08-28 02:23:49 - train: epoch 0074, iter [03600, 05004], lr: 0.016095, loss: 1.2737
2022-08-28 02:24:23 - train: epoch 0074, iter [03700, 05004], lr: 0.016072, loss: 1.3294
2022-08-28 02:24:58 - train: epoch 0074, iter [03800, 05004], lr: 0.016049, loss: 1.2400
2022-08-28 02:25:33 - train: epoch 0074, iter [03900, 05004], lr: 0.016026, loss: 0.9918
2022-08-28 02:26:08 - train: epoch 0074, iter [04000, 05004], lr: 0.016003, loss: 1.1560
2022-08-28 02:26:42 - train: epoch 0074, iter [04100, 05004], lr: 0.015980, loss: 1.4933
2022-08-28 02:27:17 - train: epoch 0074, iter [04200, 05004], lr: 0.015957, loss: 1.2814
2022-08-28 02:27:52 - train: epoch 0074, iter [04300, 05004], lr: 0.015934, loss: 1.2332
2022-08-28 02:28:27 - train: epoch 0074, iter [04400, 05004], lr: 0.015911, loss: 1.0703
2022-08-28 02:29:02 - train: epoch 0074, iter [04500, 05004], lr: 0.015888, loss: 1.1385
2022-08-28 02:29:36 - train: epoch 0074, iter [04600, 05004], lr: 0.015865, loss: 1.3193
2022-08-28 02:30:11 - train: epoch 0074, iter [04700, 05004], lr: 0.015842, loss: 1.2827
2022-08-28 02:30:46 - train: epoch 0074, iter [04800, 05004], lr: 0.015819, loss: 1.1845
2022-08-28 02:31:20 - train: epoch 0074, iter [04900, 05004], lr: 0.015796, loss: 1.4244
2022-08-28 02:31:53 - train: epoch 0074, iter [05000, 05004], lr: 0.015774, loss: 1.1445
2022-08-28 02:31:55 - train: epoch 074, train_loss: 1.2095
2022-08-28 02:33:12 - eval: epoch: 074, acc1: 71.260%, acc5: 90.532%, test_loss: 1.1588, per_image_load_time: 2.462ms, per_image_inference_time: 0.491ms
2022-08-28 02:33:12 - until epoch: 074, best_acc1: 71.260%
2022-08-28 02:33:12 - epoch 075 lr: 0.015772
2022-08-28 02:33:53 - train: epoch 0075, iter [00100, 05004], lr: 0.015750, loss: 1.2288
2022-08-28 02:34:27 - train: epoch 0075, iter [00200, 05004], lr: 0.015727, loss: 1.2318
2022-08-28 02:35:01 - train: epoch 0075, iter [00300, 05004], lr: 0.015704, loss: 1.0631
2022-08-28 02:35:35 - train: epoch 0075, iter [00400, 05004], lr: 0.015681, loss: 1.2730
2022-08-28 02:36:11 - train: epoch 0075, iter [00500, 05004], lr: 0.015658, loss: 1.0383
2022-08-28 02:36:46 - train: epoch 0075, iter [00600, 05004], lr: 0.015636, loss: 1.1119
2022-08-28 02:37:20 - train: epoch 0075, iter [00700, 05004], lr: 0.015613, loss: 1.2675
2022-08-28 02:37:55 - train: epoch 0075, iter [00800, 05004], lr: 0.015590, loss: 1.3177
2022-08-28 02:38:29 - train: epoch 0075, iter [00900, 05004], lr: 0.015567, loss: 1.3678
2022-08-28 02:39:04 - train: epoch 0075, iter [01000, 05004], lr: 0.015544, loss: 1.0564
2022-08-28 02:39:39 - train: epoch 0075, iter [01100, 05004], lr: 0.015522, loss: 1.1664
2022-08-28 02:40:13 - train: epoch 0075, iter [01200, 05004], lr: 0.015499, loss: 1.1308
2022-08-28 02:40:48 - train: epoch 0075, iter [01300, 05004], lr: 0.015476, loss: 1.0254
2022-08-28 02:41:21 - train: epoch 0075, iter [01400, 05004], lr: 0.015454, loss: 1.2148
2022-08-28 02:41:57 - train: epoch 0075, iter [01500, 05004], lr: 0.015431, loss: 1.1933
2022-08-28 02:42:31 - train: epoch 0075, iter [01600, 05004], lr: 0.015408, loss: 1.0024
2022-08-28 02:43:06 - train: epoch 0075, iter [01700, 05004], lr: 0.015386, loss: 1.2356
2022-08-28 02:43:40 - train: epoch 0075, iter [01800, 05004], lr: 0.015363, loss: 1.2132
2022-08-28 02:44:15 - train: epoch 0075, iter [01900, 05004], lr: 0.015340, loss: 1.0668
2022-08-28 02:44:49 - train: epoch 0075, iter [02000, 05004], lr: 0.015318, loss: 1.2819
2022-08-28 02:45:24 - train: epoch 0075, iter [02100, 05004], lr: 0.015295, loss: 1.0570
2022-08-28 02:45:59 - train: epoch 0075, iter [02200, 05004], lr: 0.015273, loss: 1.2556
2022-08-28 02:46:33 - train: epoch 0075, iter [02300, 05004], lr: 0.015250, loss: 1.1187
2022-08-28 02:47:09 - train: epoch 0075, iter [02400, 05004], lr: 0.015227, loss: 1.1261
2022-08-28 02:47:42 - train: epoch 0075, iter [02500, 05004], lr: 0.015205, loss: 1.3250
2022-08-28 02:48:17 - train: epoch 0075, iter [02600, 05004], lr: 0.015182, loss: 1.3118
2022-08-28 02:48:53 - train: epoch 0075, iter [02700, 05004], lr: 0.015160, loss: 1.1058
2022-08-28 02:49:27 - train: epoch 0075, iter [02800, 05004], lr: 0.015137, loss: 1.1629
2022-08-28 02:50:02 - train: epoch 0075, iter [02900, 05004], lr: 0.015115, loss: 1.2734
2022-08-28 02:50:36 - train: epoch 0075, iter [03000, 05004], lr: 0.015092, loss: 1.3033
2022-08-28 02:51:12 - train: epoch 0075, iter [03100, 05004], lr: 0.015070, loss: 1.2458
2022-08-28 02:51:46 - train: epoch 0075, iter [03200, 05004], lr: 0.015047, loss: 1.1597
2022-08-28 02:52:21 - train: epoch 0075, iter [03300, 05004], lr: 0.015025, loss: 1.2632
2022-08-28 02:52:55 - train: epoch 0075, iter [03400, 05004], lr: 0.015002, loss: 1.1123
2022-08-28 02:53:31 - train: epoch 0075, iter [03500, 05004], lr: 0.014980, loss: 1.1079
2022-08-28 02:54:05 - train: epoch 0075, iter [03600, 05004], lr: 0.014958, loss: 1.2396
2022-08-28 02:54:41 - train: epoch 0075, iter [03700, 05004], lr: 0.014935, loss: 1.2500
2022-08-28 02:55:15 - train: epoch 0075, iter [03800, 05004], lr: 0.014913, loss: 1.1816
2022-08-28 02:55:49 - train: epoch 0075, iter [03900, 05004], lr: 0.014891, loss: 1.3309
2022-08-28 02:56:24 - train: epoch 0075, iter [04000, 05004], lr: 0.014868, loss: 1.1173
2022-08-28 02:56:58 - train: epoch 0075, iter [04100, 05004], lr: 0.014846, loss: 0.9071
2022-08-28 02:57:33 - train: epoch 0075, iter [04200, 05004], lr: 0.014824, loss: 1.2330
2022-08-28 02:58:08 - train: epoch 0075, iter [04300, 05004], lr: 0.014801, loss: 1.3747
2022-08-28 02:58:43 - train: epoch 0075, iter [04400, 05004], lr: 0.014779, loss: 0.9883
2022-08-28 02:59:17 - train: epoch 0075, iter [04500, 05004], lr: 0.014757, loss: 1.1540
2022-08-28 02:59:52 - train: epoch 0075, iter [04600, 05004], lr: 0.014734, loss: 0.9994
2022-08-28 03:00:27 - train: epoch 0075, iter [04700, 05004], lr: 0.014712, loss: 1.3442
2022-08-28 03:01:02 - train: epoch 0075, iter [04800, 05004], lr: 0.014690, loss: 1.1634
2022-08-28 03:01:37 - train: epoch 0075, iter [04900, 05004], lr: 0.014668, loss: 1.0192
2022-08-28 03:02:10 - train: epoch 0075, iter [05000, 05004], lr: 0.014646, loss: 1.2495
2022-08-28 03:02:11 - train: epoch 075, train_loss: 1.1889
2022-08-28 03:03:29 - eval: epoch: 075, acc1: 71.184%, acc5: 90.326%, test_loss: 1.1611, per_image_load_time: 2.448ms, per_image_inference_time: 0.493ms
2022-08-28 03:03:29 - until epoch: 075, best_acc1: 71.260%
2022-08-28 03:03:29 - epoch 076 lr: 0.014644
2022-08-28 03:04:09 - train: epoch 0076, iter [00100, 05004], lr: 0.014622, loss: 0.9556
2022-08-28 03:04:43 - train: epoch 0076, iter [00200, 05004], lr: 0.014600, loss: 1.1617
2022-08-28 03:05:17 - train: epoch 0076, iter [00300, 05004], lr: 0.014578, loss: 1.1752
2022-08-28 03:05:51 - train: epoch 0076, iter [00400, 05004], lr: 0.014556, loss: 1.1983
2022-08-28 03:06:26 - train: epoch 0076, iter [00500, 05004], lr: 0.014534, loss: 1.1358
2022-08-28 03:07:01 - train: epoch 0076, iter [00600, 05004], lr: 0.014512, loss: 1.1976
2022-08-28 03:07:36 - train: epoch 0076, iter [00700, 05004], lr: 0.014490, loss: 1.1452
2022-08-28 03:08:10 - train: epoch 0076, iter [00800, 05004], lr: 0.014468, loss: 1.1228
2022-08-28 03:08:45 - train: epoch 0076, iter [00900, 05004], lr: 0.014445, loss: 0.9697
2022-08-28 03:09:19 - train: epoch 0076, iter [01000, 05004], lr: 0.014423, loss: 1.0211
2022-08-28 03:09:54 - train: epoch 0076, iter [01100, 05004], lr: 0.014401, loss: 1.0273
2022-08-28 03:10:29 - train: epoch 0076, iter [01200, 05004], lr: 0.014379, loss: 1.1330
2022-08-28 03:11:03 - train: epoch 0076, iter [01300, 05004], lr: 0.014357, loss: 1.1550
2022-08-28 03:11:38 - train: epoch 0076, iter [01400, 05004], lr: 0.014335, loss: 1.0305
2022-08-28 03:12:13 - train: epoch 0076, iter [01500, 05004], lr: 0.014313, loss: 1.0552
2022-08-28 03:12:48 - train: epoch 0076, iter [01600, 05004], lr: 0.014291, loss: 1.0999
2022-08-28 03:13:21 - train: epoch 0076, iter [01700, 05004], lr: 0.014269, loss: 1.0908
2022-08-28 03:13:56 - train: epoch 0076, iter [01800, 05004], lr: 0.014247, loss: 1.0695
2022-08-28 03:14:31 - train: epoch 0076, iter [01900, 05004], lr: 0.014225, loss: 1.2691
2022-08-28 03:15:05 - train: epoch 0076, iter [02000, 05004], lr: 0.014204, loss: 1.1865
2022-08-28 03:15:40 - train: epoch 0076, iter [02100, 05004], lr: 0.014182, loss: 1.2863
2022-08-28 03:16:15 - train: epoch 0076, iter [02200, 05004], lr: 0.014160, loss: 1.3456
2022-08-28 03:16:50 - train: epoch 0076, iter [02300, 05004], lr: 0.014138, loss: 1.1049
2022-08-28 03:17:25 - train: epoch 0076, iter [02400, 05004], lr: 0.014116, loss: 1.3046
2022-08-28 03:17:58 - train: epoch 0076, iter [02500, 05004], lr: 0.014094, loss: 1.1789
2022-08-28 03:18:34 - train: epoch 0076, iter [02600, 05004], lr: 0.014072, loss: 1.4252
2022-08-28 03:19:08 - train: epoch 0076, iter [02700, 05004], lr: 0.014050, loss: 1.1543
2022-08-28 03:19:42 - train: epoch 0076, iter [02800, 05004], lr: 0.014029, loss: 1.1369
2022-08-28 03:20:17 - train: epoch 0076, iter [02900, 05004], lr: 0.014007, loss: 1.1551
2022-08-28 03:20:51 - train: epoch 0076, iter [03000, 05004], lr: 0.013985, loss: 1.1327
2022-08-28 03:21:24 - train: epoch 0076, iter [03100, 05004], lr: 0.013963, loss: 1.2926
2022-08-28 03:21:59 - train: epoch 0076, iter [03200, 05004], lr: 0.013942, loss: 1.2021
2022-08-28 03:22:33 - train: epoch 0076, iter [03300, 05004], lr: 0.013920, loss: 1.0633
2022-08-28 03:23:07 - train: epoch 0076, iter [03400, 05004], lr: 0.013898, loss: 1.1692
2022-08-28 03:23:41 - train: epoch 0076, iter [03500, 05004], lr: 0.013876, loss: 1.0823
2022-08-28 03:24:15 - train: epoch 0076, iter [03600, 05004], lr: 0.013855, loss: 1.0025
2022-08-28 03:24:50 - train: epoch 0076, iter [03700, 05004], lr: 0.013833, loss: 1.1128
2022-08-28 03:25:23 - train: epoch 0076, iter [03800, 05004], lr: 0.013811, loss: 1.0005
2022-08-28 03:25:58 - train: epoch 0076, iter [03900, 05004], lr: 0.013790, loss: 1.0150
2022-08-28 03:26:32 - train: epoch 0076, iter [04000, 05004], lr: 0.013768, loss: 1.1915
2022-08-28 03:27:06 - train: epoch 0076, iter [04100, 05004], lr: 0.013746, loss: 1.1547
2022-08-28 03:27:40 - train: epoch 0076, iter [04200, 05004], lr: 0.013725, loss: 1.4045
2022-08-28 03:28:14 - train: epoch 0076, iter [04300, 05004], lr: 0.013703, loss: 1.2154
2022-08-28 03:28:48 - train: epoch 0076, iter [04400, 05004], lr: 0.013682, loss: 1.1652
2022-08-28 03:29:22 - train: epoch 0076, iter [04500, 05004], lr: 0.013660, loss: 1.1471
2022-08-28 03:29:56 - train: epoch 0076, iter [04600, 05004], lr: 0.013638, loss: 1.2618
2022-08-28 03:30:30 - train: epoch 0076, iter [04700, 05004], lr: 0.013617, loss: 1.2522
2022-08-28 03:31:05 - train: epoch 0076, iter [04800, 05004], lr: 0.013595, loss: 1.0277
2022-08-28 03:31:39 - train: epoch 0076, iter [04900, 05004], lr: 0.013574, loss: 1.0983
2022-08-28 03:32:12 - train: epoch 0076, iter [05000, 05004], lr: 0.013552, loss: 1.2299
2022-08-28 03:32:13 - train: epoch 076, train_loss: 1.1656
2022-08-28 03:33:28 - eval: epoch: 076, acc1: 70.914%, acc5: 90.298%, test_loss: 1.1654, per_image_load_time: 2.389ms, per_image_inference_time: 0.521ms
2022-08-28 03:33:29 - until epoch: 076, best_acc1: 71.260%
2022-08-28 03:33:29 - epoch 077 lr: 0.013551
2022-08-28 03:34:08 - train: epoch 0077, iter [00100, 05004], lr: 0.013530, loss: 1.2273
2022-08-28 03:34:41 - train: epoch 0077, iter [00200, 05004], lr: 0.013509, loss: 1.2785
2022-08-28 03:35:14 - train: epoch 0077, iter [00300, 05004], lr: 0.013487, loss: 0.9141
2022-08-28 03:35:48 - train: epoch 0077, iter [00400, 05004], lr: 0.013466, loss: 1.2689
2022-08-28 03:36:21 - train: epoch 0077, iter [00500, 05004], lr: 0.013444, loss: 1.0459
2022-08-28 03:36:55 - train: epoch 0077, iter [00600, 05004], lr: 0.013423, loss: 0.9076
2022-08-28 03:37:29 - train: epoch 0077, iter [00700, 05004], lr: 0.013402, loss: 1.0794
2022-08-28 03:38:03 - train: epoch 0077, iter [00800, 05004], lr: 0.013380, loss: 1.2213
2022-08-28 03:38:37 - train: epoch 0077, iter [00900, 05004], lr: 0.013359, loss: 1.2929
2022-08-28 03:39:10 - train: epoch 0077, iter [01000, 05004], lr: 0.013337, loss: 0.9796
2022-08-28 03:39:45 - train: epoch 0077, iter [01100, 05004], lr: 0.013316, loss: 1.2009
2022-08-28 03:40:19 - train: epoch 0077, iter [01200, 05004], lr: 0.013295, loss: 1.2529
2022-08-28 03:40:53 - train: epoch 0077, iter [01300, 05004], lr: 0.013273, loss: 0.9140
2022-08-28 03:41:26 - train: epoch 0077, iter [01400, 05004], lr: 0.013252, loss: 1.3208
2022-08-28 03:42:00 - train: epoch 0077, iter [01500, 05004], lr: 0.013231, loss: 1.0856
2022-08-28 03:42:34 - train: epoch 0077, iter [01600, 05004], lr: 0.013210, loss: 1.1470
2022-08-28 03:43:08 - train: epoch 0077, iter [01700, 05004], lr: 0.013188, loss: 1.3522
2022-08-28 03:43:41 - train: epoch 0077, iter [01800, 05004], lr: 0.013167, loss: 1.0256
2022-08-28 03:44:16 - train: epoch 0077, iter [01900, 05004], lr: 0.013146, loss: 1.1303
2022-08-28 03:44:50 - train: epoch 0077, iter [02000, 05004], lr: 0.013125, loss: 0.9198
2022-08-28 03:45:24 - train: epoch 0077, iter [02100, 05004], lr: 0.013103, loss: 1.1355
2022-08-28 03:45:58 - train: epoch 0077, iter [02200, 05004], lr: 0.013082, loss: 1.2255
2022-08-28 03:46:32 - train: epoch 0077, iter [02300, 05004], lr: 0.013061, loss: 1.2805
2022-08-28 03:47:07 - train: epoch 0077, iter [02400, 05004], lr: 0.013040, loss: 1.1704
2022-08-28 03:47:40 - train: epoch 0077, iter [02500, 05004], lr: 0.013019, loss: 1.3191
2022-08-28 03:48:15 - train: epoch 0077, iter [02600, 05004], lr: 0.012998, loss: 0.9186
2022-08-28 03:48:49 - train: epoch 0077, iter [02700, 05004], lr: 0.012977, loss: 1.1429
2022-08-28 03:49:23 - train: epoch 0077, iter [02800, 05004], lr: 0.012956, loss: 1.2206
2022-08-28 03:49:57 - train: epoch 0077, iter [02900, 05004], lr: 0.012934, loss: 1.4294
2022-08-28 03:50:29 - train: epoch 0077, iter [03000, 05004], lr: 0.012913, loss: 1.0903
2022-08-28 03:51:04 - train: epoch 0077, iter [03100, 05004], lr: 0.012892, loss: 1.1718
2022-08-28 03:51:38 - train: epoch 0077, iter [03200, 05004], lr: 0.012871, loss: 1.3167
2022-08-28 03:52:12 - train: epoch 0077, iter [03300, 05004], lr: 0.012850, loss: 1.0206
2022-08-28 03:52:46 - train: epoch 0077, iter [03400, 05004], lr: 0.012829, loss: 1.3642
2022-08-28 03:53:20 - train: epoch 0077, iter [03500, 05004], lr: 0.012808, loss: 0.9844
2022-08-28 03:53:55 - train: epoch 0077, iter [03600, 05004], lr: 0.012787, loss: 1.0790
2022-08-28 03:54:28 - train: epoch 0077, iter [03700, 05004], lr: 0.012766, loss: 1.2275
2022-08-28 03:55:03 - train: epoch 0077, iter [03800, 05004], lr: 0.012745, loss: 1.1343
2022-08-28 03:55:37 - train: epoch 0077, iter [03900, 05004], lr: 0.012725, loss: 1.1551
2022-08-28 03:56:11 - train: epoch 0077, iter [04000, 05004], lr: 0.012704, loss: 1.1116
2022-08-28 03:56:46 - train: epoch 0077, iter [04100, 05004], lr: 0.012683, loss: 1.3101
2022-08-28 03:57:19 - train: epoch 0077, iter [04200, 05004], lr: 0.012662, loss: 1.3174
2022-08-28 03:57:53 - train: epoch 0077, iter [04300, 05004], lr: 0.012641, loss: 1.1731
2022-08-28 03:58:27 - train: epoch 0077, iter [04400, 05004], lr: 0.012620, loss: 1.0657
2022-08-28 03:59:02 - train: epoch 0077, iter [04500, 05004], lr: 0.012599, loss: 1.1567
2022-08-28 03:59:37 - train: epoch 0077, iter [04600, 05004], lr: 0.012578, loss: 1.0826
2022-08-28 04:00:11 - train: epoch 0077, iter [04700, 05004], lr: 0.012558, loss: 0.9147
2022-08-28 04:00:44 - train: epoch 0077, iter [04800, 05004], lr: 0.012537, loss: 1.1141
2022-08-28 04:01:19 - train: epoch 0077, iter [04900, 05004], lr: 0.012516, loss: 1.3820
2022-08-28 04:01:52 - train: epoch 0077, iter [05000, 05004], lr: 0.012495, loss: 1.3802
2022-08-28 04:01:53 - train: epoch 077, train_loss: 1.1414
2022-08-28 04:03:09 - eval: epoch: 077, acc1: 72.204%, acc5: 91.024%, test_loss: 1.1151, per_image_load_time: 2.449ms, per_image_inference_time: 0.498ms
2022-08-28 04:03:09 - until epoch: 077, best_acc1: 72.204%
2022-08-28 04:03:09 - epoch 078 lr: 0.012494
2022-08-28 04:03:48 - train: epoch 0078, iter [00100, 05004], lr: 0.012474, loss: 1.0412
2022-08-28 04:04:21 - train: epoch 0078, iter [00200, 05004], lr: 0.012453, loss: 1.2367
2022-08-28 04:04:55 - train: epoch 0078, iter [00300, 05004], lr: 0.012432, loss: 1.2116
2022-08-28 04:05:29 - train: epoch 0078, iter [00400, 05004], lr: 0.012412, loss: 1.1287
2022-08-28 04:06:02 - train: epoch 0078, iter [00500, 05004], lr: 0.012391, loss: 1.0721
2022-08-28 04:06:36 - train: epoch 0078, iter [00600, 05004], lr: 0.012370, loss: 1.1308
2022-08-28 04:07:09 - train: epoch 0078, iter [00700, 05004], lr: 0.012349, loss: 1.1360
2022-08-28 04:07:43 - train: epoch 0078, iter [00800, 05004], lr: 0.012329, loss: 1.3231
2022-08-28 04:08:18 - train: epoch 0078, iter [00900, 05004], lr: 0.012308, loss: 1.2392
2022-08-28 04:08:51 - train: epoch 0078, iter [01000, 05004], lr: 0.012288, loss: 1.1516
2022-08-28 04:09:25 - train: epoch 0078, iter [01100, 05004], lr: 0.012267, loss: 0.9240
2022-08-28 04:09:58 - train: epoch 0078, iter [01200, 05004], lr: 0.012246, loss: 0.9217
2022-08-28 04:10:32 - train: epoch 0078, iter [01300, 05004], lr: 0.012226, loss: 1.1540
2022-08-28 04:11:06 - train: epoch 0078, iter [01400, 05004], lr: 0.012205, loss: 1.0236
2022-08-28 04:11:39 - train: epoch 0078, iter [01500, 05004], lr: 0.012185, loss: 1.0464
2022-08-28 04:12:13 - train: epoch 0078, iter [01600, 05004], lr: 0.012164, loss: 1.2080
2022-08-28 04:12:47 - train: epoch 0078, iter [01700, 05004], lr: 0.012144, loss: 1.1206
2022-08-28 04:13:22 - train: epoch 0078, iter [01800, 05004], lr: 0.012123, loss: 1.1516
2022-08-28 04:13:56 - train: epoch 0078, iter [01900, 05004], lr: 0.012103, loss: 0.9114
2022-08-28 04:14:30 - train: epoch 0078, iter [02000, 05004], lr: 0.012082, loss: 1.0530
2022-08-28 04:15:04 - train: epoch 0078, iter [02100, 05004], lr: 0.012062, loss: 1.3502
2022-08-28 04:15:38 - train: epoch 0078, iter [02200, 05004], lr: 0.012041, loss: 1.0829
2022-08-28 04:16:12 - train: epoch 0078, iter [02300, 05004], lr: 0.012021, loss: 1.1998
2022-08-28 04:16:47 - train: epoch 0078, iter [02400, 05004], lr: 0.012001, loss: 1.0913
2022-08-28 04:17:21 - train: epoch 0078, iter [02500, 05004], lr: 0.011980, loss: 1.1519
2022-08-28 04:17:55 - train: epoch 0078, iter [02600, 05004], lr: 0.011960, loss: 1.0942
2022-08-28 04:18:29 - train: epoch 0078, iter [02700, 05004], lr: 0.011939, loss: 1.1062
2022-08-28 04:19:03 - train: epoch 0078, iter [02800, 05004], lr: 0.011919, loss: 1.1301
2022-08-28 04:19:37 - train: epoch 0078, iter [02900, 05004], lr: 0.011899, loss: 1.1064
2022-08-28 04:20:11 - train: epoch 0078, iter [03000, 05004], lr: 0.011878, loss: 1.1109
2022-08-28 04:20:45 - train: epoch 0078, iter [03100, 05004], lr: 0.011858, loss: 1.2556
2022-08-28 04:21:19 - train: epoch 0078, iter [03200, 05004], lr: 0.011838, loss: 1.1345
2022-08-28 04:21:53 - train: epoch 0078, iter [03300, 05004], lr: 0.011817, loss: 1.2340
2022-08-28 04:22:27 - train: epoch 0078, iter [03400, 05004], lr: 0.011797, loss: 1.1499
2022-08-28 04:23:01 - train: epoch 0078, iter [03500, 05004], lr: 0.011777, loss: 1.1145
2022-08-28 04:23:35 - train: epoch 0078, iter [03600, 05004], lr: 0.011757, loss: 1.1689
2022-08-28 04:24:09 - train: epoch 0078, iter [03700, 05004], lr: 0.011737, loss: 1.0473
2022-08-28 04:24:43 - train: epoch 0078, iter [03800, 05004], lr: 0.011716, loss: 1.1526
2022-08-28 04:25:17 - train: epoch 0078, iter [03900, 05004], lr: 0.011696, loss: 1.1999
2022-08-28 04:25:51 - train: epoch 0078, iter [04000, 05004], lr: 0.011676, loss: 1.1477
2022-08-28 04:26:25 - train: epoch 0078, iter [04100, 05004], lr: 0.011656, loss: 1.1712
2022-08-28 04:26:59 - train: epoch 0078, iter [04200, 05004], lr: 0.011636, loss: 1.2339
2022-08-28 04:27:32 - train: epoch 0078, iter [04300, 05004], lr: 0.011616, loss: 1.1948
2022-08-28 04:28:07 - train: epoch 0078, iter [04400, 05004], lr: 0.011595, loss: 1.1659
2022-08-28 04:28:41 - train: epoch 0078, iter [04500, 05004], lr: 0.011575, loss: 1.2296
2022-08-28 04:29:15 - train: epoch 0078, iter [04600, 05004], lr: 0.011555, loss: 0.9085
2022-08-28 04:29:49 - train: epoch 0078, iter [04700, 05004], lr: 0.011535, loss: 1.2002
2022-08-28 04:30:23 - train: epoch 0078, iter [04800, 05004], lr: 0.011515, loss: 1.3279
2022-08-28 04:30:57 - train: epoch 0078, iter [04900, 05004], lr: 0.011495, loss: 1.1848
2022-08-28 04:31:29 - train: epoch 0078, iter [05000, 05004], lr: 0.011475, loss: 1.1265
2022-08-28 04:31:30 - train: epoch 078, train_loss: 1.1155
2022-08-28 04:32:46 - eval: epoch: 078, acc1: 71.618%, acc5: 90.752%, test_loss: 1.1341, per_image_load_time: 2.415ms, per_image_inference_time: 0.506ms
2022-08-28 04:32:46 - until epoch: 078, best_acc1: 72.204%
2022-08-28 04:32:46 - epoch 079 lr: 0.011474
2022-08-28 04:33:25 - train: epoch 0079, iter [00100, 05004], lr: 0.011454, loss: 1.0606
2022-08-28 04:33:59 - train: epoch 0079, iter [00200, 05004], lr: 0.011434, loss: 1.0028
2022-08-28 04:34:32 - train: epoch 0079, iter [00300, 05004], lr: 0.011414, loss: 1.1182
2022-08-28 04:35:06 - train: epoch 0079, iter [00400, 05004], lr: 0.011394, loss: 1.1290
2022-08-28 04:35:39 - train: epoch 0079, iter [00500, 05004], lr: 0.011374, loss: 1.1745
2022-08-28 04:36:13 - train: epoch 0079, iter [00600, 05004], lr: 0.011355, loss: 1.0092
2022-08-28 04:36:47 - train: epoch 0079, iter [00700, 05004], lr: 0.011335, loss: 0.9571
2022-08-28 04:37:21 - train: epoch 0079, iter [00800, 05004], lr: 0.011315, loss: 1.0254
2022-08-28 04:37:54 - train: epoch 0079, iter [00900, 05004], lr: 0.011295, loss: 1.0291
2022-08-28 04:38:28 - train: epoch 0079, iter [01000, 05004], lr: 0.011275, loss: 0.9743
2022-08-28 04:39:01 - train: epoch 0079, iter [01100, 05004], lr: 0.011255, loss: 1.2411
2022-08-28 04:39:35 - train: epoch 0079, iter [01200, 05004], lr: 0.011235, loss: 1.3891
2022-08-28 04:40:09 - train: epoch 0079, iter [01300, 05004], lr: 0.011216, loss: 0.9123
2022-08-28 04:40:43 - train: epoch 0079, iter [01400, 05004], lr: 0.011196, loss: 1.1058
2022-08-28 04:41:17 - train: epoch 0079, iter [01500, 05004], lr: 0.011176, loss: 0.9403
2022-08-28 04:41:52 - train: epoch 0079, iter [01600, 05004], lr: 0.011156, loss: 0.8932
2022-08-28 04:42:26 - train: epoch 0079, iter [01700, 05004], lr: 0.011136, loss: 1.0394
2022-08-28 04:43:00 - train: epoch 0079, iter [01800, 05004], lr: 0.011117, loss: 1.0446
2022-08-28 04:43:34 - train: epoch 0079, iter [01900, 05004], lr: 0.011097, loss: 1.1083
2022-08-28 04:44:07 - train: epoch 0079, iter [02000, 05004], lr: 0.011077, loss: 1.3378
2022-08-28 04:44:41 - train: epoch 0079, iter [02100, 05004], lr: 0.011058, loss: 0.9260
2022-08-28 04:45:15 - train: epoch 0079, iter [02200, 05004], lr: 0.011038, loss: 1.1787
2022-08-28 04:45:49 - train: epoch 0079, iter [02300, 05004], lr: 0.011018, loss: 1.0194
2022-08-28 04:46:22 - train: epoch 0079, iter [02400, 05004], lr: 0.010999, loss: 1.1160
2022-08-28 04:46:56 - train: epoch 0079, iter [02500, 05004], lr: 0.010979, loss: 1.0344
2022-08-28 04:47:31 - train: epoch 0079, iter [02600, 05004], lr: 0.010959, loss: 1.0084
2022-08-28 04:48:04 - train: epoch 0079, iter [02700, 05004], lr: 0.010940, loss: 0.9369
2022-08-28 04:48:38 - train: epoch 0079, iter [02800, 05004], lr: 0.010920, loss: 0.9854
2022-08-28 04:49:11 - train: epoch 0079, iter [02900, 05004], lr: 0.010900, loss: 0.9214
2022-08-28 04:49:46 - train: epoch 0079, iter [03000, 05004], lr: 0.010881, loss: 1.1042
2022-08-28 04:50:20 - train: epoch 0079, iter [03100, 05004], lr: 0.010861, loss: 1.0886
2022-08-28 04:50:54 - train: epoch 0079, iter [03200, 05004], lr: 0.010842, loss: 1.2608
2022-08-28 04:51:27 - train: epoch 0079, iter [03300, 05004], lr: 0.010822, loss: 1.0380
2022-08-28 04:52:00 - train: epoch 0079, iter [03400, 05004], lr: 0.010803, loss: 0.8924
2022-08-28 04:52:35 - train: epoch 0079, iter [03500, 05004], lr: 0.010783, loss: 1.2644
2022-08-28 04:53:09 - train: epoch 0079, iter [03600, 05004], lr: 0.010764, loss: 0.9956
2022-08-28 04:53:43 - train: epoch 0079, iter [03700, 05004], lr: 0.010744, loss: 0.9774
2022-08-28 04:54:16 - train: epoch 0079, iter [03800, 05004], lr: 0.010725, loss: 1.0934
2022-08-28 04:54:50 - train: epoch 0079, iter [03900, 05004], lr: 0.010706, loss: 0.9960
2022-08-28 04:55:25 - train: epoch 0079, iter [04000, 05004], lr: 0.010686, loss: 0.9245
2022-08-28 04:55:59 - train: epoch 0079, iter [04100, 05004], lr: 0.010667, loss: 1.2441
2022-08-28 04:56:33 - train: epoch 0079, iter [04200, 05004], lr: 0.010647, loss: 1.0058
2022-08-28 04:57:07 - train: epoch 0079, iter [04300, 05004], lr: 0.010628, loss: 0.8171
2022-08-28 04:57:41 - train: epoch 0079, iter [04400, 05004], lr: 0.010609, loss: 1.2429
2022-08-28 04:58:16 - train: epoch 0079, iter [04500, 05004], lr: 0.010589, loss: 1.3054
2022-08-28 04:58:50 - train: epoch 0079, iter [04600, 05004], lr: 0.010570, loss: 1.2955
2022-08-28 04:59:24 - train: epoch 0079, iter [04700, 05004], lr: 0.010551, loss: 1.0823
2022-08-28 04:59:57 - train: epoch 0079, iter [04800, 05004], lr: 0.010532, loss: 1.1940
2022-08-28 05:00:32 - train: epoch 0079, iter [04900, 05004], lr: 0.010512, loss: 1.2100
2022-08-28 05:01:05 - train: epoch 0079, iter [05000, 05004], lr: 0.010493, loss: 1.0294
2022-08-28 05:01:06 - train: epoch 079, train_loss: 1.0928
2022-08-28 05:02:21 - eval: epoch: 079, acc1: 72.672%, acc5: 91.294%, test_loss: 1.0996, per_image_load_time: 2.269ms, per_image_inference_time: 0.533ms
2022-08-28 05:02:21 - until epoch: 079, best_acc1: 72.672%
2022-08-28 05:02:21 - epoch 080 lr: 0.010492
2022-08-28 05:03:00 - train: epoch 0080, iter [00100, 05004], lr: 0.010473, loss: 0.9370
2022-08-28 05:03:34 - train: epoch 0080, iter [00200, 05004], lr: 0.010454, loss: 1.0245
2022-08-28 05:04:07 - train: epoch 0080, iter [00300, 05004], lr: 0.010435, loss: 1.1959
2022-08-28 05:04:41 - train: epoch 0080, iter [00400, 05004], lr: 0.010415, loss: 0.8806
2022-08-28 05:05:15 - train: epoch 0080, iter [00500, 05004], lr: 0.010396, loss: 1.0721
2022-08-28 05:05:48 - train: epoch 0080, iter [00600, 05004], lr: 0.010377, loss: 0.9834
2022-08-28 05:06:21 - train: epoch 0080, iter [00700, 05004], lr: 0.010358, loss: 1.0378
2022-08-28 05:06:55 - train: epoch 0080, iter [00800, 05004], lr: 0.010339, loss: 0.9382
2022-08-28 05:07:28 - train: epoch 0080, iter [00900, 05004], lr: 0.010320, loss: 1.1422
2022-08-28 05:08:02 - train: epoch 0080, iter [01000, 05004], lr: 0.010301, loss: 1.0210
2022-08-28 05:08:36 - train: epoch 0080, iter [01100, 05004], lr: 0.010282, loss: 1.0795
2022-08-28 05:09:09 - train: epoch 0080, iter [01200, 05004], lr: 0.010262, loss: 0.9782
2022-08-28 05:09:43 - train: epoch 0080, iter [01300, 05004], lr: 0.010243, loss: 1.0168
2022-08-28 05:10:16 - train: epoch 0080, iter [01400, 05004], lr: 0.010224, loss: 1.1118
2022-08-28 05:10:51 - train: epoch 0080, iter [01500, 05004], lr: 0.010205, loss: 1.0794
2022-08-28 05:11:24 - train: epoch 0080, iter [01600, 05004], lr: 0.010186, loss: 0.9759
2022-08-28 05:11:58 - train: epoch 0080, iter [01700, 05004], lr: 0.010167, loss: 1.1210
2022-08-28 05:12:31 - train: epoch 0080, iter [01800, 05004], lr: 0.010148, loss: 1.1346
2022-08-28 05:13:06 - train: epoch 0080, iter [01900, 05004], lr: 0.010130, loss: 1.1333
2022-08-28 05:13:40 - train: epoch 0080, iter [02000, 05004], lr: 0.010111, loss: 1.1266
2022-08-28 05:14:14 - train: epoch 0080, iter [02100, 05004], lr: 0.010092, loss: 1.1845
2022-08-28 05:14:47 - train: epoch 0080, iter [02200, 05004], lr: 0.010073, loss: 1.1728
2022-08-28 05:15:22 - train: epoch 0080, iter [02300, 05004], lr: 0.010054, loss: 0.9175
2022-08-28 05:15:55 - train: epoch 0080, iter [02400, 05004], lr: 0.010035, loss: 1.0432
2022-08-28 05:16:30 - train: epoch 0080, iter [02500, 05004], lr: 0.010016, loss: 0.9237
2022-08-28 05:17:04 - train: epoch 0080, iter [02600, 05004], lr: 0.009997, loss: 1.0334
2022-08-28 05:17:38 - train: epoch 0080, iter [02700, 05004], lr: 0.009978, loss: 1.1856
2022-08-28 05:18:12 - train: epoch 0080, iter [02800, 05004], lr: 0.009960, loss: 1.2333
2022-08-28 05:18:46 - train: epoch 0080, iter [02900, 05004], lr: 0.009941, loss: 0.9657
2022-08-28 05:19:20 - train: epoch 0080, iter [03000, 05004], lr: 0.009922, loss: 1.0462
2022-08-28 05:19:54 - train: epoch 0080, iter [03100, 05004], lr: 0.009903, loss: 1.3165
2022-08-28 05:20:28 - train: epoch 0080, iter [03200, 05004], lr: 0.009885, loss: 0.7799
2022-08-28 05:21:02 - train: epoch 0080, iter [03300, 05004], lr: 0.009866, loss: 1.0570
2022-08-28 05:21:35 - train: epoch 0080, iter [03400, 05004], lr: 0.009847, loss: 0.9539
2022-08-28 05:22:10 - train: epoch 0080, iter [03500, 05004], lr: 0.009828, loss: 0.8989
2022-08-28 05:22:45 - train: epoch 0080, iter [03600, 05004], lr: 0.009810, loss: 1.0716
2022-08-28 05:23:18 - train: epoch 0080, iter [03700, 05004], lr: 0.009791, loss: 1.0424
2022-08-28 05:23:52 - train: epoch 0080, iter [03800, 05004], lr: 0.009772, loss: 1.0590
2022-08-28 05:24:26 - train: epoch 0080, iter [03900, 05004], lr: 0.009754, loss: 1.1067
2022-08-28 05:25:00 - train: epoch 0080, iter [04000, 05004], lr: 0.009735, loss: 1.1303
2022-08-28 05:25:34 - train: epoch 0080, iter [04100, 05004], lr: 0.009717, loss: 1.1743
2022-08-28 05:26:08 - train: epoch 0080, iter [04200, 05004], lr: 0.009698, loss: 1.0651
2022-08-28 05:26:42 - train: epoch 0080, iter [04300, 05004], lr: 0.009679, loss: 1.1421
2022-08-28 05:27:16 - train: epoch 0080, iter [04400, 05004], lr: 0.009661, loss: 1.0921
2022-08-28 05:27:51 - train: epoch 0080, iter [04500, 05004], lr: 0.009642, loss: 1.1177
2022-08-28 05:28:24 - train: epoch 0080, iter [04600, 05004], lr: 0.009624, loss: 1.1179
2022-08-28 05:28:59 - train: epoch 0080, iter [04700, 05004], lr: 0.009605, loss: 1.1195
2022-08-28 05:29:32 - train: epoch 0080, iter [04800, 05004], lr: 0.009587, loss: 1.0664
2022-08-28 05:30:07 - train: epoch 0080, iter [04900, 05004], lr: 0.009568, loss: 1.1733
2022-08-28 05:30:39 - train: epoch 0080, iter [05000, 05004], lr: 0.009550, loss: 0.8870
2022-08-28 05:30:40 - train: epoch 080, train_loss: 1.0704
2022-08-28 05:31:56 - eval: epoch: 080, acc1: 73.198%, acc5: 91.508%, test_loss: 1.0801, per_image_load_time: 2.409ms, per_image_inference_time: 0.507ms
2022-08-28 05:31:56 - until epoch: 080, best_acc1: 73.198%
2022-08-28 05:31:56 - epoch 081 lr: 0.009549
2022-08-28 05:32:35 - train: epoch 0081, iter [00100, 05004], lr: 0.009531, loss: 0.8477
2022-08-28 05:33:08 - train: epoch 0081, iter [00200, 05004], lr: 0.009512, loss: 1.0934
2022-08-28 05:33:42 - train: epoch 0081, iter [00300, 05004], lr: 0.009494, loss: 0.9757
2022-08-28 05:34:16 - train: epoch 0081, iter [00400, 05004], lr: 0.009475, loss: 1.1561
2022-08-28 05:34:50 - train: epoch 0081, iter [00500, 05004], lr: 0.009457, loss: 1.0707
2022-08-28 05:35:23 - train: epoch 0081, iter [00600, 05004], lr: 0.009439, loss: 1.0272
2022-08-28 05:35:57 - train: epoch 0081, iter [00700, 05004], lr: 0.009420, loss: 0.9561
2022-08-28 05:36:31 - train: epoch 0081, iter [00800, 05004], lr: 0.009402, loss: 1.1451
2022-08-28 05:37:05 - train: epoch 0081, iter [00900, 05004], lr: 0.009384, loss: 0.9238
2022-08-28 05:37:38 - train: epoch 0081, iter [01000, 05004], lr: 0.009365, loss: 0.9672
2022-08-28 05:38:12 - train: epoch 0081, iter [01100, 05004], lr: 0.009347, loss: 1.0704
2022-08-28 05:38:46 - train: epoch 0081, iter [01200, 05004], lr: 0.009329, loss: 1.0344
2022-08-28 05:39:20 - train: epoch 0081, iter [01300, 05004], lr: 0.009311, loss: 1.0089
2022-08-28 05:39:54 - train: epoch 0081, iter [01400, 05004], lr: 0.009292, loss: 0.7069
2022-08-28 05:40:28 - train: epoch 0081, iter [01500, 05004], lr: 0.009274, loss: 1.0644
2022-08-28 05:41:01 - train: epoch 0081, iter [01600, 05004], lr: 0.009256, loss: 0.9700
2022-08-28 05:41:36 - train: epoch 0081, iter [01700, 05004], lr: 0.009238, loss: 1.1073
2022-08-28 05:42:09 - train: epoch 0081, iter [01800, 05004], lr: 0.009220, loss: 0.9409
2022-08-28 05:42:43 - train: epoch 0081, iter [01900, 05004], lr: 0.009201, loss: 1.0603
2022-08-28 05:43:16 - train: epoch 0081, iter [02000, 05004], lr: 0.009183, loss: 1.1210
2022-08-28 05:43:51 - train: epoch 0081, iter [02100, 05004], lr: 0.009165, loss: 1.0052
2022-08-28 05:44:25 - train: epoch 0081, iter [02200, 05004], lr: 0.009147, loss: 1.0616
2022-08-28 05:44:58 - train: epoch 0081, iter [02300, 05004], lr: 0.009129, loss: 0.9623
2022-08-28 05:45:32 - train: epoch 0081, iter [02400, 05004], lr: 0.009111, loss: 1.0116
2022-08-28 05:46:05 - train: epoch 0081, iter [02500, 05004], lr: 0.009093, loss: 1.2717
2022-08-28 05:46:40 - train: epoch 0081, iter [02600, 05004], lr: 0.009075, loss: 1.1369
2022-08-28 05:47:13 - train: epoch 0081, iter [02700, 05004], lr: 0.009057, loss: 1.0434
2022-08-28 05:47:47 - train: epoch 0081, iter [02800, 05004], lr: 0.009039, loss: 1.0013
2022-08-28 05:48:22 - train: epoch 0081, iter [02900, 05004], lr: 0.009021, loss: 0.9879
2022-08-28 05:48:55 - train: epoch 0081, iter [03000, 05004], lr: 0.009003, loss: 1.0434
2022-08-28 05:49:29 - train: epoch 0081, iter [03100, 05004], lr: 0.008985, loss: 1.0159
2022-08-28 05:50:03 - train: epoch 0081, iter [03200, 05004], lr: 0.008967, loss: 0.9836
2022-08-28 05:50:38 - train: epoch 0081, iter [03300, 05004], lr: 0.008949, loss: 1.0431
2022-08-28 05:51:12 - train: epoch 0081, iter [03400, 05004], lr: 0.008931, loss: 1.1848
2022-08-28 05:51:45 - train: epoch 0081, iter [03500, 05004], lr: 0.008913, loss: 1.0718
2022-08-28 05:52:19 - train: epoch 0081, iter [03600, 05004], lr: 0.008895, loss: 0.8701
2022-08-28 05:52:53 - train: epoch 0081, iter [03700, 05004], lr: 0.008877, loss: 1.1680
2022-08-28 05:53:27 - train: epoch 0081, iter [03800, 05004], lr: 0.008860, loss: 0.9115
2022-08-28 05:54:02 - train: epoch 0081, iter [03900, 05004], lr: 0.008842, loss: 1.0991
2022-08-28 05:54:36 - train: epoch 0081, iter [04000, 05004], lr: 0.008824, loss: 0.8003
2022-08-28 05:55:10 - train: epoch 0081, iter [04100, 05004], lr: 0.008806, loss: 1.1173
2022-08-28 05:55:43 - train: epoch 0081, iter [04200, 05004], lr: 0.008788, loss: 1.0605
2022-08-28 05:56:20 - train: epoch 0081, iter [04300, 05004], lr: 0.008771, loss: 1.0593
2022-08-28 05:56:53 - train: epoch 0081, iter [04400, 05004], lr: 0.008753, loss: 1.1734
2022-08-28 05:57:26 - train: epoch 0081, iter [04500, 05004], lr: 0.008735, loss: 1.0960
2022-08-28 05:58:01 - train: epoch 0081, iter [04600, 05004], lr: 0.008717, loss: 0.9121
2022-08-28 05:58:36 - train: epoch 0081, iter [04700, 05004], lr: 0.008700, loss: 1.0790
2022-08-28 05:59:10 - train: epoch 0081, iter [04800, 05004], lr: 0.008682, loss: 1.0713
2022-08-28 05:59:43 - train: epoch 0081, iter [04900, 05004], lr: 0.008664, loss: 1.0791
2022-08-28 06:00:16 - train: epoch 0081, iter [05000, 05004], lr: 0.008647, loss: 0.8897
2022-08-28 06:00:18 - train: epoch 081, train_loss: 1.0452
2022-08-28 06:01:33 - eval: epoch: 081, acc1: 72.918%, acc5: 91.344%, test_loss: 1.0842, per_image_load_time: 2.376ms, per_image_inference_time: 0.513ms
2022-08-28 06:01:33 - until epoch: 081, best_acc1: 73.198%
2022-08-28 06:01:33 - epoch 082 lr: 0.008646
2022-08-28 06:02:12 - train: epoch 0082, iter [00100, 05004], lr: 0.008628, loss: 0.8299
2022-08-28 06:02:46 - train: epoch 0082, iter [00200, 05004], lr: 0.008611, loss: 0.8213
2022-08-28 06:03:21 - train: epoch 0082, iter [00300, 05004], lr: 0.008593, loss: 1.1222
2022-08-28 06:03:55 - train: epoch 0082, iter [00400, 05004], lr: 0.008576, loss: 1.0839
2022-08-28 06:04:29 - train: epoch 0082, iter [00500, 05004], lr: 0.008558, loss: 0.9967
2022-08-28 06:05:03 - train: epoch 0082, iter [00600, 05004], lr: 0.008540, loss: 0.8518
2022-08-28 06:05:37 - train: epoch 0082, iter [00700, 05004], lr: 0.008523, loss: 1.1825
2022-08-28 06:06:11 - train: epoch 0082, iter [00800, 05004], lr: 0.008505, loss: 1.0708
2022-08-28 06:06:45 - train: epoch 0082, iter [00900, 05004], lr: 0.008488, loss: 1.1920
2022-08-28 06:07:19 - train: epoch 0082, iter [01000, 05004], lr: 0.008470, loss: 1.0861
2022-08-28 06:07:53 - train: epoch 0082, iter [01100, 05004], lr: 0.008453, loss: 1.1188
2022-08-28 06:08:26 - train: epoch 0082, iter [01200, 05004], lr: 0.008435, loss: 1.0180
2022-08-28 06:09:01 - train: epoch 0082, iter [01300, 05004], lr: 0.008418, loss: 1.2060
2022-08-28 06:09:34 - train: epoch 0082, iter [01400, 05004], lr: 0.008401, loss: 1.0658
2022-08-28 06:10:09 - train: epoch 0082, iter [01500, 05004], lr: 0.008383, loss: 0.9241
2022-08-28 06:10:44 - train: epoch 0082, iter [01600, 05004], lr: 0.008366, loss: 1.1299
2022-08-28 06:11:17 - train: epoch 0082, iter [01700, 05004], lr: 0.008348, loss: 0.9426
2022-08-28 06:11:51 - train: epoch 0082, iter [01800, 05004], lr: 0.008331, loss: 0.9299
2022-08-28 06:12:25 - train: epoch 0082, iter [01900, 05004], lr: 0.008314, loss: 0.9298
2022-08-28 06:13:00 - train: epoch 0082, iter [02000, 05004], lr: 0.008296, loss: 0.8214
2022-08-28 06:13:34 - train: epoch 0082, iter [02100, 05004], lr: 0.008279, loss: 0.9297
2022-08-28 06:14:09 - train: epoch 0082, iter [02200, 05004], lr: 0.008262, loss: 1.1046
2022-08-28 06:14:43 - train: epoch 0082, iter [02300, 05004], lr: 0.008244, loss: 0.9968
2022-08-28 06:15:17 - train: epoch 0082, iter [02400, 05004], lr: 0.008227, loss: 1.0011
2022-08-28 06:15:52 - train: epoch 0082, iter [02500, 05004], lr: 0.008210, loss: 0.9551
2022-08-28 06:16:26 - train: epoch 0082, iter [02600, 05004], lr: 0.008193, loss: 0.9244
2022-08-28 06:17:00 - train: epoch 0082, iter [02700, 05004], lr: 0.008176, loss: 0.9869
2022-08-28 06:17:35 - train: epoch 0082, iter [02800, 05004], lr: 0.008158, loss: 0.9241
2022-08-28 06:18:09 - train: epoch 0082, iter [02900, 05004], lr: 0.008141, loss: 0.8791
2022-08-28 06:18:44 - train: epoch 0082, iter [03000, 05004], lr: 0.008124, loss: 0.9684
2022-08-28 06:19:17 - train: epoch 0082, iter [03100, 05004], lr: 0.008107, loss: 0.9411
2022-08-28 06:19:52 - train: epoch 0082, iter [03200, 05004], lr: 0.008090, loss: 1.2171
2022-08-28 06:20:25 - train: epoch 0082, iter [03300, 05004], lr: 0.008073, loss: 0.9682
2022-08-28 06:21:01 - train: epoch 0082, iter [03400, 05004], lr: 0.008056, loss: 0.9835
2022-08-28 06:21:35 - train: epoch 0082, iter [03500, 05004], lr: 0.008038, loss: 0.9036
2022-08-28 06:22:10 - train: epoch 0082, iter [03600, 05004], lr: 0.008021, loss: 0.8103
2022-08-28 06:22:43 - train: epoch 0082, iter [03700, 05004], lr: 0.008004, loss: 0.9111
2022-08-28 06:23:18 - train: epoch 0082, iter [03800, 05004], lr: 0.007987, loss: 1.2183
2022-08-28 06:23:52 - train: epoch 0082, iter [03900, 05004], lr: 0.007970, loss: 1.0638
2022-08-28 06:24:27 - train: epoch 0082, iter [04000, 05004], lr: 0.007953, loss: 1.0562
2022-08-28 06:25:01 - train: epoch 0082, iter [04100, 05004], lr: 0.007936, loss: 0.9907
2022-08-28 06:25:35 - train: epoch 0082, iter [04200, 05004], lr: 0.007919, loss: 1.1488
2022-08-28 06:26:10 - train: epoch 0082, iter [04300, 05004], lr: 0.007902, loss: 0.8286
2022-08-28 06:26:44 - train: epoch 0082, iter [04400, 05004], lr: 0.007886, loss: 0.9050
2022-08-28 06:27:19 - train: epoch 0082, iter [04500, 05004], lr: 0.007869, loss: 1.1809
2022-08-28 06:27:52 - train: epoch 0082, iter [04600, 05004], lr: 0.007852, loss: 1.0014
2022-08-28 06:28:26 - train: epoch 0082, iter [04700, 05004], lr: 0.007835, loss: 0.9579
2022-08-28 06:29:01 - train: epoch 0082, iter [04800, 05004], lr: 0.007818, loss: 0.9713
2022-08-28 06:29:34 - train: epoch 0082, iter [04900, 05004], lr: 0.007801, loss: 0.9575
2022-08-28 06:30:08 - train: epoch 0082, iter [05000, 05004], lr: 0.007784, loss: 1.0837
2022-08-28 06:30:09 - train: epoch 082, train_loss: 1.0180
2022-08-28 06:31:25 - eval: epoch: 082, acc1: 73.388%, acc5: 91.624%, test_loss: 1.0726, per_image_load_time: 2.371ms, per_image_inference_time: 0.506ms
2022-08-28 06:31:25 - until epoch: 082, best_acc1: 73.388%
2022-08-28 06:31:25 - epoch 083 lr: 0.007783
2022-08-28 06:32:04 - train: epoch 0083, iter [00100, 05004], lr: 0.007767, loss: 0.9033
2022-08-28 06:32:39 - train: epoch 0083, iter [00200, 05004], lr: 0.007750, loss: 0.8459
2022-08-28 06:33:12 - train: epoch 0083, iter [00300, 05004], lr: 0.007733, loss: 1.0175
2022-08-28 06:33:46 - train: epoch 0083, iter [00400, 05004], lr: 0.007716, loss: 1.1025
2022-08-28 06:34:20 - train: epoch 0083, iter [00500, 05004], lr: 0.007700, loss: 0.9436
2022-08-28 06:34:53 - train: epoch 0083, iter [00600, 05004], lr: 0.007683, loss: 0.9866
2022-08-28 06:35:27 - train: epoch 0083, iter [00700, 05004], lr: 0.007666, loss: 0.9355
2022-08-28 06:36:02 - train: epoch 0083, iter [00800, 05004], lr: 0.007650, loss: 0.9979
2022-08-28 06:36:36 - train: epoch 0083, iter [00900, 05004], lr: 0.007633, loss: 1.0190
2022-08-28 06:37:09 - train: epoch 0083, iter [01000, 05004], lr: 0.007616, loss: 1.0693
2022-08-28 06:37:44 - train: epoch 0083, iter [01100, 05004], lr: 0.007600, loss: 0.9553
2022-08-28 06:38:17 - train: epoch 0083, iter [01200, 05004], lr: 0.007583, loss: 0.9739
2022-08-28 06:38:52 - train: epoch 0083, iter [01300, 05004], lr: 0.007566, loss: 0.9183
2022-08-28 06:39:25 - train: epoch 0083, iter [01400, 05004], lr: 0.007550, loss: 1.1269
2022-08-28 06:40:00 - train: epoch 0083, iter [01500, 05004], lr: 0.007533, loss: 0.9034
2022-08-28 06:40:34 - train: epoch 0083, iter [01600, 05004], lr: 0.007517, loss: 0.9508
2022-08-28 06:41:08 - train: epoch 0083, iter [01700, 05004], lr: 0.007500, loss: 1.1464
2022-08-28 06:41:42 - train: epoch 0083, iter [01800, 05004], lr: 0.007484, loss: 1.0755
2022-08-28 06:42:16 - train: epoch 0083, iter [01900, 05004], lr: 0.007467, loss: 0.8092
2022-08-28 06:42:50 - train: epoch 0083, iter [02000, 05004], lr: 0.007451, loss: 0.7627
2022-08-28 06:43:24 - train: epoch 0083, iter [02100, 05004], lr: 0.007434, loss: 0.9447
2022-08-28 06:43:58 - train: epoch 0083, iter [02200, 05004], lr: 0.007418, loss: 0.9655
2022-08-28 06:44:32 - train: epoch 0083, iter [02300, 05004], lr: 0.007401, loss: 1.0104
2022-08-28 06:45:06 - train: epoch 0083, iter [02400, 05004], lr: 0.007385, loss: 1.0164
2022-08-28 06:45:40 - train: epoch 0083, iter [02500, 05004], lr: 0.007368, loss: 0.9852
2022-08-28 06:46:14 - train: epoch 0083, iter [02600, 05004], lr: 0.007352, loss: 0.9098
2022-08-28 06:46:49 - train: epoch 0083, iter [02700, 05004], lr: 0.007336, loss: 0.9496
2022-08-28 06:47:22 - train: epoch 0083, iter [02800, 05004], lr: 0.007319, loss: 0.9401
2022-08-28 06:47:57 - train: epoch 0083, iter [02900, 05004], lr: 0.007303, loss: 0.9447
2022-08-28 06:48:31 - train: epoch 0083, iter [03000, 05004], lr: 0.007287, loss: 1.1657
2022-08-28 06:49:05 - train: epoch 0083, iter [03100, 05004], lr: 0.007270, loss: 0.8257
2022-08-28 06:49:39 - train: epoch 0083, iter [03200, 05004], lr: 0.007254, loss: 0.8590
2022-08-28 06:50:13 - train: epoch 0083, iter [03300, 05004], lr: 0.007238, loss: 1.1416
2022-08-28 06:50:48 - train: epoch 0083, iter [03400, 05004], lr: 0.007221, loss: 1.1077
2022-08-28 06:51:22 - train: epoch 0083, iter [03500, 05004], lr: 0.007205, loss: 1.1127
2022-08-28 06:51:56 - train: epoch 0083, iter [03600, 05004], lr: 0.007189, loss: 0.9955
2022-08-28 06:52:30 - train: epoch 0083, iter [03700, 05004], lr: 0.007173, loss: 1.0535
2022-08-28 06:53:04 - train: epoch 0083, iter [03800, 05004], lr: 0.007157, loss: 0.9057
2022-08-28 06:53:39 - train: epoch 0083, iter [03900, 05004], lr: 0.007140, loss: 0.9916
2022-08-28 06:54:13 - train: epoch 0083, iter [04000, 05004], lr: 0.007124, loss: 1.1652
2022-08-28 06:54:47 - train: epoch 0083, iter [04100, 05004], lr: 0.007108, loss: 0.9577
2022-08-28 06:55:20 - train: epoch 0083, iter [04200, 05004], lr: 0.007092, loss: 1.3064
2022-08-28 06:55:55 - train: epoch 0083, iter [04300, 05004], lr: 0.007076, loss: 0.9688
2022-08-28 06:56:29 - train: epoch 0083, iter [04400, 05004], lr: 0.007060, loss: 0.8442
2022-08-28 06:57:03 - train: epoch 0083, iter [04500, 05004], lr: 0.007044, loss: 1.0244
2022-08-28 06:57:38 - train: epoch 0083, iter [04600, 05004], lr: 0.007028, loss: 1.1128
2022-08-28 06:58:12 - train: epoch 0083, iter [04700, 05004], lr: 0.007012, loss: 1.0725
2022-08-28 06:58:46 - train: epoch 0083, iter [04800, 05004], lr: 0.006996, loss: 0.9895
2022-08-28 06:59:21 - train: epoch 0083, iter [04900, 05004], lr: 0.006980, loss: 1.2076
2022-08-28 06:59:54 - train: epoch 0083, iter [05000, 05004], lr: 0.006964, loss: 1.1611
2022-08-28 06:59:55 - train: epoch 083, train_loss: 0.9943
2022-08-28 07:01:11 - eval: epoch: 083, acc1: 73.980%, acc5: 91.870%, test_loss: 1.0534, per_image_load_time: 2.472ms, per_image_inference_time: 0.489ms
2022-08-28 07:01:12 - until epoch: 083, best_acc1: 73.980%
2022-08-28 07:01:12 - epoch 084 lr: 0.006963
2022-08-28 07:01:51 - train: epoch 0084, iter [00100, 05004], lr: 0.006947, loss: 0.9265
2022-08-28 07:02:24 - train: epoch 0084, iter [00200, 05004], lr: 0.006931, loss: 1.0318
2022-08-28 07:02:58 - train: epoch 0084, iter [00300, 05004], lr: 0.006915, loss: 0.8129
2022-08-28 07:03:32 - train: epoch 0084, iter [00400, 05004], lr: 0.006899, loss: 0.9760
2022-08-28 07:04:05 - train: epoch 0084, iter [00500, 05004], lr: 0.006883, loss: 0.9422
2022-08-28 07:04:39 - train: epoch 0084, iter [00600, 05004], lr: 0.006867, loss: 1.0985
2022-08-28 07:05:13 - train: epoch 0084, iter [00700, 05004], lr: 0.006851, loss: 0.9465
2022-08-28 07:05:47 - train: epoch 0084, iter [00800, 05004], lr: 0.006836, loss: 1.0851
2022-08-28 07:06:20 - train: epoch 0084, iter [00900, 05004], lr: 0.006820, loss: 0.9383
2022-08-28 07:06:54 - train: epoch 0084, iter [01000, 05004], lr: 0.006804, loss: 0.9966
2022-08-28 07:07:28 - train: epoch 0084, iter [01100, 05004], lr: 0.006788, loss: 0.9417
2022-08-28 07:08:02 - train: epoch 0084, iter [01200, 05004], lr: 0.006772, loss: 1.1830
2022-08-28 07:08:36 - train: epoch 0084, iter [01300, 05004], lr: 0.006757, loss: 0.9324
2022-08-28 07:09:10 - train: epoch 0084, iter [01400, 05004], lr: 0.006741, loss: 1.0041
2022-08-28 07:09:43 - train: epoch 0084, iter [01500, 05004], lr: 0.006725, loss: 0.9093
2022-08-28 07:10:17 - train: epoch 0084, iter [01600, 05004], lr: 0.006709, loss: 0.8087
2022-08-28 07:10:51 - train: epoch 0084, iter [01700, 05004], lr: 0.006694, loss: 0.9858
2022-08-28 07:11:25 - train: epoch 0084, iter [01800, 05004], lr: 0.006678, loss: 0.9728
2022-08-28 07:11:58 - train: epoch 0084, iter [01900, 05004], lr: 0.006662, loss: 0.9692
2022-08-28 07:12:32 - train: epoch 0084, iter [02000, 05004], lr: 0.006647, loss: 1.0201
2022-08-28 07:13:07 - train: epoch 0084, iter [02100, 05004], lr: 0.006631, loss: 0.9167
2022-08-28 07:13:40 - train: epoch 0084, iter [02200, 05004], lr: 0.006615, loss: 0.7312
2022-08-28 07:14:15 - train: epoch 0084, iter [02300, 05004], lr: 0.006600, loss: 0.9471
2022-08-28 07:14:49 - train: epoch 0084, iter [02400, 05004], lr: 0.006584, loss: 0.8228
2022-08-28 07:15:23 - train: epoch 0084, iter [02500, 05004], lr: 0.006569, loss: 1.1223
2022-08-28 07:15:56 - train: epoch 0084, iter [02600, 05004], lr: 0.006553, loss: 1.0914
2022-08-28 07:16:30 - train: epoch 0084, iter [02700, 05004], lr: 0.006538, loss: 0.9842
2022-08-28 07:17:04 - train: epoch 0084, iter [02800, 05004], lr: 0.006522, loss: 1.1064
2022-08-28 07:17:38 - train: epoch 0084, iter [02900, 05004], lr: 0.006507, loss: 0.9590
2022-08-28 07:18:13 - train: epoch 0084, iter [03000, 05004], lr: 0.006491, loss: 0.9913
2022-08-28 07:18:46 - train: epoch 0084, iter [03100, 05004], lr: 0.006476, loss: 0.9248
2022-08-28 07:19:21 - train: epoch 0084, iter [03200, 05004], lr: 0.006460, loss: 0.8272
2022-08-28 07:19:54 - train: epoch 0084, iter [03300, 05004], lr: 0.006445, loss: 0.9759
2022-08-28 07:20:28 - train: epoch 0084, iter [03400, 05004], lr: 0.006429, loss: 0.9071
2022-08-28 07:21:03 - train: epoch 0084, iter [03500, 05004], lr: 0.006414, loss: 0.8170
2022-08-28 07:21:36 - train: epoch 0084, iter [03600, 05004], lr: 0.006399, loss: 0.9727
2022-08-28 07:22:10 - train: epoch 0084, iter [03700, 05004], lr: 0.006383, loss: 1.0410
2022-08-28 07:22:45 - train: epoch 0084, iter [03800, 05004], lr: 0.006368, loss: 1.1755
2022-08-28 07:23:18 - train: epoch 0084, iter [03900, 05004], lr: 0.006353, loss: 1.0854
2022-08-28 07:23:52 - train: epoch 0084, iter [04000, 05004], lr: 0.006337, loss: 1.0854
2022-08-28 07:24:26 - train: epoch 0084, iter [04100, 05004], lr: 0.006322, loss: 1.0016
2022-08-28 07:24:59 - train: epoch 0084, iter [04200, 05004], lr: 0.006307, loss: 0.9836
2022-08-28 07:25:33 - train: epoch 0084, iter [04300, 05004], lr: 0.006292, loss: 0.9916
2022-08-28 07:26:07 - train: epoch 0084, iter [04400, 05004], lr: 0.006276, loss: 1.1857
2022-08-28 07:26:40 - train: epoch 0084, iter [04500, 05004], lr: 0.006261, loss: 0.9415
2022-08-28 07:27:14 - train: epoch 0084, iter [04600, 05004], lr: 0.006246, loss: 0.9835
2022-08-28 07:27:48 - train: epoch 0084, iter [04700, 05004], lr: 0.006231, loss: 1.2734
2022-08-28 07:28:23 - train: epoch 0084, iter [04800, 05004], lr: 0.006216, loss: 0.8577
2022-08-28 07:28:57 - train: epoch 0084, iter [04900, 05004], lr: 0.006200, loss: 0.8103
2022-08-28 07:29:29 - train: epoch 0084, iter [05000, 05004], lr: 0.006185, loss: 1.0256
2022-08-28 07:29:31 - train: epoch 084, train_loss: 0.9684
2022-08-28 07:30:46 - eval: epoch: 084, acc1: 74.120%, acc5: 92.178%, test_loss: 1.0329, per_image_load_time: 2.386ms, per_image_inference_time: 0.506ms
2022-08-28 07:30:46 - until epoch: 084, best_acc1: 74.120%
2022-08-28 07:30:46 - epoch 085 lr: 0.006185
2022-08-28 07:31:25 - train: epoch 0085, iter [00100, 05004], lr: 0.006170, loss: 0.8469
2022-08-28 07:31:59 - train: epoch 0085, iter [00200, 05004], lr: 0.006154, loss: 0.7294
2022-08-28 07:32:32 - train: epoch 0085, iter [00300, 05004], lr: 0.006139, loss: 1.0167
2022-08-28 07:33:06 - train: epoch 0085, iter [00400, 05004], lr: 0.006124, loss: 0.8191
2022-08-28 07:33:40 - train: epoch 0085, iter [00500, 05004], lr: 0.006109, loss: 1.0658
2022-08-28 07:34:13 - train: epoch 0085, iter [00600, 05004], lr: 0.006094, loss: 0.7496
2022-08-28 07:34:48 - train: epoch 0085, iter [00700, 05004], lr: 0.006079, loss: 0.8952
2022-08-28 07:35:22 - train: epoch 0085, iter [00800, 05004], lr: 0.006064, loss: 0.8624
2022-08-28 07:35:55 - train: epoch 0085, iter [00900, 05004], lr: 0.006049, loss: 0.9359
2022-08-28 07:36:29 - train: epoch 0085, iter [01000, 05004], lr: 0.006034, loss: 1.0123
2022-08-28 07:37:03 - train: epoch 0085, iter [01100, 05004], lr: 0.006019, loss: 0.9318
2022-08-28 07:37:36 - train: epoch 0085, iter [01200, 05004], lr: 0.006004, loss: 0.8760
2022-08-28 07:38:11 - train: epoch 0085, iter [01300, 05004], lr: 0.005990, loss: 0.8655
2022-08-28 07:38:44 - train: epoch 0085, iter [01400, 05004], lr: 0.005975, loss: 0.9618
2022-08-28 07:39:19 - train: epoch 0085, iter [01500, 05004], lr: 0.005960, loss: 0.8478
2022-08-28 07:39:53 - train: epoch 0085, iter [01600, 05004], lr: 0.005945, loss: 0.9499
2022-08-28 07:40:27 - train: epoch 0085, iter [01700, 05004], lr: 0.005930, loss: 1.0859
2022-08-28 07:41:02 - train: epoch 0085, iter [01800, 05004], lr: 0.005915, loss: 0.8728
2022-08-28 07:41:35 - train: epoch 0085, iter [01900, 05004], lr: 0.005900, loss: 0.8313
2022-08-28 07:42:09 - train: epoch 0085, iter [02000, 05004], lr: 0.005886, loss: 0.9811
2022-08-28 07:42:43 - train: epoch 0085, iter [02100, 05004], lr: 0.005871, loss: 0.7434
2022-08-28 07:43:18 - train: epoch 0085, iter [02200, 05004], lr: 0.005856, loss: 0.9503
2022-08-28 07:43:51 - train: epoch 0085, iter [02300, 05004], lr: 0.005841, loss: 0.6987
2022-08-28 07:44:25 - train: epoch 0085, iter [02400, 05004], lr: 0.005827, loss: 1.0381
2022-08-28 07:44:59 - train: epoch 0085, iter [02500, 05004], lr: 0.005812, loss: 1.2002
2022-08-28 07:45:33 - train: epoch 0085, iter [02600, 05004], lr: 0.005797, loss: 1.0381
2022-08-28 07:46:07 - train: epoch 0085, iter [02700, 05004], lr: 0.005783, loss: 0.8789
2022-08-28 07:46:42 - train: epoch 0085, iter [02800, 05004], lr: 0.005768, loss: 1.0091
2022-08-28 07:47:16 - train: epoch 0085, iter [02900, 05004], lr: 0.005753, loss: 1.1706
2022-08-28 07:47:49 - train: epoch 0085, iter [03000, 05004], lr: 0.005739, loss: 1.0432
2022-08-28 07:48:23 - train: epoch 0085, iter [03100, 05004], lr: 0.005724, loss: 0.8730
2022-08-28 07:48:57 - train: epoch 0085, iter [03200, 05004], lr: 0.005710, loss: 0.8761
2022-08-28 07:49:31 - train: epoch 0085, iter [03300, 05004], lr: 0.005695, loss: 1.0251
2022-08-28 07:50:05 - train: epoch 0085, iter [03400, 05004], lr: 0.005681, loss: 1.0401
2022-08-28 07:50:40 - train: epoch 0085, iter [03500, 05004], lr: 0.005666, loss: 1.0414
2022-08-28 07:51:13 - train: epoch 0085, iter [03600, 05004], lr: 0.005651, loss: 0.9073
2022-08-28 07:51:47 - train: epoch 0085, iter [03700, 05004], lr: 0.005637, loss: 0.8725
2022-08-28 07:52:21 - train: epoch 0085, iter [03800, 05004], lr: 0.005623, loss: 0.8744
2022-08-28 07:52:56 - train: epoch 0085, iter [03900, 05004], lr: 0.005608, loss: 0.8503
2022-08-28 07:53:30 - train: epoch 0085, iter [04000, 05004], lr: 0.005594, loss: 1.1899
2022-08-28 07:54:03 - train: epoch 0085, iter [04100, 05004], lr: 0.005579, loss: 0.9630
2022-08-28 07:54:37 - train: epoch 0085, iter [04200, 05004], lr: 0.005565, loss: 1.0822
2022-08-28 07:55:12 - train: epoch 0085, iter [04300, 05004], lr: 0.005550, loss: 0.9677
2022-08-28 07:55:45 - train: epoch 0085, iter [04400, 05004], lr: 0.005536, loss: 0.9440
2022-08-28 07:56:20 - train: epoch 0085, iter [04500, 05004], lr: 0.005522, loss: 1.0499
2022-08-28 07:56:54 - train: epoch 0085, iter [04600, 05004], lr: 0.005507, loss: 0.9506
2022-08-28 07:57:27 - train: epoch 0085, iter [04700, 05004], lr: 0.005493, loss: 1.1472
2022-08-28 07:58:01 - train: epoch 0085, iter [04800, 05004], lr: 0.005479, loss: 0.8722
2022-08-28 07:58:36 - train: epoch 0085, iter [04900, 05004], lr: 0.005465, loss: 0.9381
2022-08-28 07:59:08 - train: epoch 0085, iter [05000, 05004], lr: 0.005450, loss: 0.8905
2022-08-28 07:59:09 - train: epoch 085, train_loss: 0.9455
2022-08-28 08:00:25 - eval: epoch: 085, acc1: 74.674%, acc5: 92.186%, test_loss: 1.0221, per_image_load_time: 2.432ms, per_image_inference_time: 0.506ms
2022-08-28 08:00:26 - until epoch: 085, best_acc1: 74.674%
2022-08-28 08:00:26 - epoch 086 lr: 0.005450
2022-08-28 08:01:05 - train: epoch 0086, iter [00100, 05004], lr: 0.005435, loss: 0.7625
2022-08-28 08:01:38 - train: epoch 0086, iter [00200, 05004], lr: 0.005421, loss: 0.7896
2022-08-28 08:02:12 - train: epoch 0086, iter [00300, 05004], lr: 0.005407, loss: 1.0722
2022-08-28 08:02:46 - train: epoch 0086, iter [00400, 05004], lr: 0.005393, loss: 1.0028
2022-08-28 08:03:19 - train: epoch 0086, iter [00500, 05004], lr: 0.005379, loss: 0.8587
2022-08-28 08:03:53 - train: epoch 0086, iter [00600, 05004], lr: 0.005364, loss: 0.9393
2022-08-28 08:04:27 - train: epoch 0086, iter [00700, 05004], lr: 0.005350, loss: 0.8395
2022-08-28 08:05:01 - train: epoch 0086, iter [00800, 05004], lr: 0.005336, loss: 1.1353
2022-08-28 08:05:34 - train: epoch 0086, iter [00900, 05004], lr: 0.005322, loss: 0.9899
2022-08-28 08:06:08 - train: epoch 0086, iter [01000, 05004], lr: 0.005308, loss: 0.9261
2022-08-28 08:06:43 - train: epoch 0086, iter [01100, 05004], lr: 0.005294, loss: 0.9422
2022-08-28 08:07:17 - train: epoch 0086, iter [01200, 05004], lr: 0.005280, loss: 0.8272
2022-08-28 08:07:50 - train: epoch 0086, iter [01300, 05004], lr: 0.005266, loss: 0.8965
2022-08-28 08:08:25 - train: epoch 0086, iter [01400, 05004], lr: 0.005252, loss: 0.7998
2022-08-28 08:08:57 - train: epoch 0086, iter [01500, 05004], lr: 0.005238, loss: 0.8143
2022-08-28 08:09:31 - train: epoch 0086, iter [01600, 05004], lr: 0.005224, loss: 1.0470
2022-08-28 08:10:05 - train: epoch 0086, iter [01700, 05004], lr: 0.005210, loss: 0.8723
2022-08-28 08:10:39 - train: epoch 0086, iter [01800, 05004], lr: 0.005196, loss: 0.9183
2022-08-28 08:11:13 - train: epoch 0086, iter [01900, 05004], lr: 0.005182, loss: 1.0932
2022-08-28 08:11:47 - train: epoch 0086, iter [02000, 05004], lr: 0.005168, loss: 0.9125
2022-08-28 08:12:21 - train: epoch 0086, iter [02100, 05004], lr: 0.005154, loss: 0.8148
2022-08-28 08:12:55 - train: epoch 0086, iter [02200, 05004], lr: 0.005140, loss: 0.9350
2022-08-28 08:13:29 - train: epoch 0086, iter [02300, 05004], lr: 0.005127, loss: 0.8157
2022-08-28 08:14:03 - train: epoch 0086, iter [02400, 05004], lr: 0.005113, loss: 0.7369
2022-08-28 08:14:38 - train: epoch 0086, iter [02500, 05004], lr: 0.005099, loss: 0.8679
2022-08-28 08:15:11 - train: epoch 0086, iter [02600, 05004], lr: 0.005085, loss: 0.9152
2022-08-28 08:15:45 - train: epoch 0086, iter [02700, 05004], lr: 0.005071, loss: 0.8182
2022-08-28 08:16:20 - train: epoch 0086, iter [02800, 05004], lr: 0.005058, loss: 1.0905
2022-08-28 08:16:53 - train: epoch 0086, iter [02900, 05004], lr: 0.005044, loss: 0.8447
2022-08-28 08:17:28 - train: epoch 0086, iter [03000, 05004], lr: 0.005030, loss: 0.9846
2022-08-28 08:18:03 - train: epoch 0086, iter [03100, 05004], lr: 0.005016, loss: 0.9623
2022-08-28 08:18:37 - train: epoch 0086, iter [03200, 05004], lr: 0.005003, loss: 1.0304
2022-08-28 08:19:10 - train: epoch 0086, iter [03300, 05004], lr: 0.004989, loss: 0.9945
2022-08-28 08:19:44 - train: epoch 0086, iter [03400, 05004], lr: 0.004975, loss: 0.9608
2022-08-28 08:20:19 - train: epoch 0086, iter [03500, 05004], lr: 0.004962, loss: 0.9375
2022-08-28 08:20:53 - train: epoch 0086, iter [03600, 05004], lr: 0.004948, loss: 0.9601
2022-08-28 08:21:27 - train: epoch 0086, iter [03700, 05004], lr: 0.004934, loss: 1.0268
2022-08-28 08:22:00 - train: epoch 0086, iter [03800, 05004], lr: 0.004921, loss: 0.9255
2022-08-28 08:22:34 - train: epoch 0086, iter [03900, 05004], lr: 0.004907, loss: 0.9753
2022-08-28 08:23:09 - train: epoch 0086, iter [04000, 05004], lr: 0.004894, loss: 1.0025
2022-08-28 08:23:43 - train: epoch 0086, iter [04100, 05004], lr: 0.004880, loss: 0.9915
2022-08-28 08:24:17 - train: epoch 0086, iter [04200, 05004], lr: 0.004867, loss: 1.0597
2022-08-28 08:24:52 - train: epoch 0086, iter [04300, 05004], lr: 0.004853, loss: 0.9528
2022-08-28 08:25:26 - train: epoch 0086, iter [04400, 05004], lr: 0.004840, loss: 0.8318
2022-08-28 08:25:59 - train: epoch 0086, iter [04500, 05004], lr: 0.004826, loss: 0.8377
2022-08-28 08:26:33 - train: epoch 0086, iter [04600, 05004], lr: 0.004813, loss: 0.8350
2022-08-28 08:27:08 - train: epoch 0086, iter [04700, 05004], lr: 0.004799, loss: 0.8845
2022-08-28 08:27:42 - train: epoch 0086, iter [04800, 05004], lr: 0.004786, loss: 0.9401
2022-08-28 08:28:16 - train: epoch 0086, iter [04900, 05004], lr: 0.004773, loss: 0.8895
2022-08-28 08:28:49 - train: epoch 0086, iter [05000, 05004], lr: 0.004759, loss: 0.9362
2022-08-28 08:28:50 - train: epoch 086, train_loss: 0.9175
2022-08-28 08:30:05 - eval: epoch: 086, acc1: 74.772%, acc5: 92.420%, test_loss: 1.0104, per_image_load_time: 2.417ms, per_image_inference_time: 0.504ms
2022-08-28 08:30:06 - until epoch: 086, best_acc1: 74.772%
2022-08-28 08:30:06 - epoch 087 lr: 0.004759
2022-08-28 08:30:44 - train: epoch 0087, iter [00100, 05004], lr: 0.004745, loss: 0.9710
2022-08-28 08:31:17 - train: epoch 0087, iter [00200, 05004], lr: 0.004732, loss: 0.7048
2022-08-28 08:31:51 - train: epoch 0087, iter [00300, 05004], lr: 0.004719, loss: 1.1107
2022-08-28 08:32:25 - train: epoch 0087, iter [00400, 05004], lr: 0.004705, loss: 0.9900
2022-08-28 08:32:59 - train: epoch 0087, iter [00500, 05004], lr: 0.004692, loss: 0.7249
2022-08-28 08:33:32 - train: epoch 0087, iter [00600, 05004], lr: 0.004679, loss: 0.9176
2022-08-28 08:34:06 - train: epoch 0087, iter [00700, 05004], lr: 0.004666, loss: 0.7149
2022-08-28 08:34:40 - train: epoch 0087, iter [00800, 05004], lr: 0.004652, loss: 1.0136
2022-08-28 08:35:14 - train: epoch 0087, iter [00900, 05004], lr: 0.004639, loss: 1.0062
2022-08-28 08:35:49 - train: epoch 0087, iter [01000, 05004], lr: 0.004626, loss: 0.8549
2022-08-28 08:36:22 - train: epoch 0087, iter [01100, 05004], lr: 0.004613, loss: 0.9315
2022-08-28 08:36:56 - train: epoch 0087, iter [01200, 05004], lr: 0.004600, loss: 0.8837
2022-08-28 08:37:30 - train: epoch 0087, iter [01300, 05004], lr: 0.004586, loss: 0.9264
2022-08-28 08:38:04 - train: epoch 0087, iter [01400, 05004], lr: 0.004573, loss: 0.8952
2022-08-28 08:38:38 - train: epoch 0087, iter [01500, 05004], lr: 0.004560, loss: 0.8206
2022-08-28 08:39:12 - train: epoch 0087, iter [01600, 05004], lr: 0.004547, loss: 0.7945
2022-08-28 08:39:46 - train: epoch 0087, iter [01700, 05004], lr: 0.004534, loss: 0.9051
2022-08-28 08:40:19 - train: epoch 0087, iter [01800, 05004], lr: 0.004521, loss: 0.9707
2022-08-28 08:40:54 - train: epoch 0087, iter [01900, 05004], lr: 0.004508, loss: 0.9496
2022-08-28 08:41:28 - train: epoch 0087, iter [02000, 05004], lr: 0.004495, loss: 0.8557
2022-08-28 08:42:02 - train: epoch 0087, iter [02100, 05004], lr: 0.004482, loss: 1.0284
2022-08-28 08:42:37 - train: epoch 0087, iter [02200, 05004], lr: 0.004469, loss: 0.8854
2022-08-28 08:43:11 - train: epoch 0087, iter [02300, 05004], lr: 0.004456, loss: 0.9234
2022-08-28 08:43:45 - train: epoch 0087, iter [02400, 05004], lr: 0.004443, loss: 0.8836
2022-08-28 08:44:19 - train: epoch 0087, iter [02500, 05004], lr: 0.004430, loss: 0.8688
2022-08-28 08:44:53 - train: epoch 0087, iter [02600, 05004], lr: 0.004417, loss: 0.9883
2022-08-28 08:45:27 - train: epoch 0087, iter [02700, 05004], lr: 0.004404, loss: 0.8795
2022-08-28 08:46:01 - train: epoch 0087, iter [02800, 05004], lr: 0.004391, loss: 0.8104
2022-08-28 08:46:35 - train: epoch 0087, iter [02900, 05004], lr: 0.004379, loss: 0.7851
2022-08-28 08:47:10 - train: epoch 0087, iter [03000, 05004], lr: 0.004366, loss: 0.9052
2022-08-28 08:47:44 - train: epoch 0087, iter [03100, 05004], lr: 0.004353, loss: 0.9000
2022-08-28 08:48:17 - train: epoch 0087, iter [03200, 05004], lr: 0.004340, loss: 0.8628
2022-08-28 08:48:51 - train: epoch 0087, iter [03300, 05004], lr: 0.004327, loss: 0.8645
2022-08-28 08:49:26 - train: epoch 0087, iter [03400, 05004], lr: 0.004315, loss: 0.9458
2022-08-28 08:50:00 - train: epoch 0087, iter [03500, 05004], lr: 0.004302, loss: 0.8191
2022-08-28 08:50:34 - train: epoch 0087, iter [03600, 05004], lr: 0.004289, loss: 0.7350
2022-08-28 08:51:08 - train: epoch 0087, iter [03700, 05004], lr: 0.004276, loss: 0.8381
2022-08-28 08:51:42 - train: epoch 0087, iter [03800, 05004], lr: 0.004264, loss: 0.8675
2022-08-28 08:52:16 - train: epoch 0087, iter [03900, 05004], lr: 0.004251, loss: 0.8683
2022-08-28 08:52:50 - train: epoch 0087, iter [04000, 05004], lr: 0.004238, loss: 0.9788
2022-08-28 08:53:24 - train: epoch 0087, iter [04100, 05004], lr: 0.004226, loss: 1.0425
2022-08-28 08:53:58 - train: epoch 0087, iter [04200, 05004], lr: 0.004213, loss: 1.0107
2022-08-28 08:54:32 - train: epoch 0087, iter [04300, 05004], lr: 0.004200, loss: 0.9918
2022-08-28 08:55:06 - train: epoch 0087, iter [04400, 05004], lr: 0.004188, loss: 0.9162
2022-08-28 08:55:41 - train: epoch 0087, iter [04500, 05004], lr: 0.004175, loss: 0.9369
2022-08-28 08:56:15 - train: epoch 0087, iter [04600, 05004], lr: 0.004163, loss: 0.9025
2022-08-28 08:56:49 - train: epoch 0087, iter [04700, 05004], lr: 0.004150, loss: 0.8464
2022-08-28 08:57:23 - train: epoch 0087, iter [04800, 05004], lr: 0.004138, loss: 0.8686
2022-08-28 08:57:57 - train: epoch 0087, iter [04900, 05004], lr: 0.004125, loss: 0.8082
2022-08-28 08:58:29 - train: epoch 0087, iter [05000, 05004], lr: 0.004113, loss: 0.9162
2022-08-28 08:58:30 - train: epoch 087, train_loss: 0.8916
2022-08-28 08:59:46 - eval: epoch: 087, acc1: 75.164%, acc5: 92.552%, test_loss: 0.9952, per_image_load_time: 2.382ms, per_image_inference_time: 0.507ms
2022-08-28 08:59:46 - until epoch: 087, best_acc1: 75.164%
2022-08-28 08:59:46 - epoch 088 lr: 0.004112
2022-08-28 09:00:25 - train: epoch 0088, iter [00100, 05004], lr: 0.004100, loss: 0.8531
2022-08-28 09:00:59 - train: epoch 0088, iter [00200, 05004], lr: 0.004087, loss: 0.8061
2022-08-28 09:01:32 - train: epoch 0088, iter [00300, 05004], lr: 0.004075, loss: 0.8991
2022-08-28 09:02:06 - train: epoch 0088, iter [00400, 05004], lr: 0.004063, loss: 0.8732
2022-08-28 09:02:39 - train: epoch 0088, iter [00500, 05004], lr: 0.004050, loss: 0.7557
2022-08-28 09:03:14 - train: epoch 0088, iter [00600, 05004], lr: 0.004038, loss: 0.8375
2022-08-28 09:03:47 - train: epoch 0088, iter [00700, 05004], lr: 0.004025, loss: 0.8458
2022-08-28 09:04:21 - train: epoch 0088, iter [00800, 05004], lr: 0.004013, loss: 0.8312
2022-08-28 09:04:54 - train: epoch 0088, iter [00900, 05004], lr: 0.004001, loss: 0.9354
2022-08-28 09:05:28 - train: epoch 0088, iter [01000, 05004], lr: 0.003989, loss: 0.8071
2022-08-28 09:06:01 - train: epoch 0088, iter [01100, 05004], lr: 0.003976, loss: 0.8406
2022-08-28 09:06:35 - train: epoch 0088, iter [01200, 05004], lr: 0.003964, loss: 0.8263
2022-08-28 09:07:09 - train: epoch 0088, iter [01300, 05004], lr: 0.003952, loss: 0.8870
2022-08-28 09:07:43 - train: epoch 0088, iter [01400, 05004], lr: 0.003940, loss: 0.8621
2022-08-28 09:08:17 - train: epoch 0088, iter [01500, 05004], lr: 0.003927, loss: 0.8368
2022-08-28 09:08:51 - train: epoch 0088, iter [01600, 05004], lr: 0.003915, loss: 0.7624
2022-08-28 09:09:24 - train: epoch 0088, iter [01700, 05004], lr: 0.003903, loss: 0.9412
2022-08-28 09:09:58 - train: epoch 0088, iter [01800, 05004], lr: 0.003891, loss: 0.8790
2022-08-28 09:10:32 - train: epoch 0088, iter [01900, 05004], lr: 0.003879, loss: 0.9154
2022-08-28 09:11:06 - train: epoch 0088, iter [02000, 05004], lr: 0.003867, loss: 0.7746
2022-08-28 09:11:40 - train: epoch 0088, iter [02100, 05004], lr: 0.003854, loss: 0.8683
2022-08-28 09:12:14 - train: epoch 0088, iter [02200, 05004], lr: 0.003842, loss: 0.7876
2022-08-28 09:12:48 - train: epoch 0088, iter [02300, 05004], lr: 0.003830, loss: 0.7227
2022-08-28 09:13:23 - train: epoch 0088, iter [02400, 05004], lr: 0.003818, loss: 0.8580
2022-08-28 09:13:57 - train: epoch 0088, iter [02500, 05004], lr: 0.003806, loss: 0.8354
2022-08-28 09:14:32 - train: epoch 0088, iter [02600, 05004], lr: 0.003794, loss: 0.8854
2022-08-28 09:15:05 - train: epoch 0088, iter [02700, 05004], lr: 0.003782, loss: 0.8583
2022-08-28 09:15:39 - train: epoch 0088, iter [02800, 05004], lr: 0.003770, loss: 0.8394
2022-08-28 09:16:12 - train: epoch 0088, iter [02900, 05004], lr: 0.003758, loss: 0.7944
2022-08-28 09:16:47 - train: epoch 0088, iter [03000, 05004], lr: 0.003746, loss: 0.9763
2022-08-28 09:17:21 - train: epoch 0088, iter [03100, 05004], lr: 0.003735, loss: 0.7566
2022-08-28 09:17:54 - train: epoch 0088, iter [03200, 05004], lr: 0.003723, loss: 0.7517
2022-08-28 09:18:28 - train: epoch 0088, iter [03300, 05004], lr: 0.003711, loss: 0.7778
2022-08-28 09:19:02 - train: epoch 0088, iter [03400, 05004], lr: 0.003699, loss: 0.7431
2022-08-28 09:19:36 - train: epoch 0088, iter [03500, 05004], lr: 0.003687, loss: 0.7553
2022-08-28 09:20:10 - train: epoch 0088, iter [03600, 05004], lr: 0.003675, loss: 0.9039
2022-08-28 09:20:44 - train: epoch 0088, iter [03700, 05004], lr: 0.003663, loss: 0.8869
2022-08-28 09:21:19 - train: epoch 0088, iter [03800, 05004], lr: 0.003652, loss: 0.8614
2022-08-28 09:21:53 - train: epoch 0088, iter [03900, 05004], lr: 0.003640, loss: 0.9350
2022-08-28 09:22:28 - train: epoch 0088, iter [04000, 05004], lr: 0.003628, loss: 0.7621
2022-08-28 09:23:01 - train: epoch 0088, iter [04100, 05004], lr: 0.003616, loss: 0.9631
2022-08-28 09:23:35 - train: epoch 0088, iter [04200, 05004], lr: 0.003605, loss: 0.9783
2022-08-28 09:24:10 - train: epoch 0088, iter [04300, 05004], lr: 0.003593, loss: 0.7860
2022-08-28 09:24:43 - train: epoch 0088, iter [04400, 05004], lr: 0.003581, loss: 0.8236
2022-08-28 09:25:18 - train: epoch 0088, iter [04500, 05004], lr: 0.003570, loss: 0.8709
2022-08-28 09:25:52 - train: epoch 0088, iter [04600, 05004], lr: 0.003558, loss: 0.9710
2022-08-28 09:26:26 - train: epoch 0088, iter [04700, 05004], lr: 0.003546, loss: 0.8795
2022-08-28 09:27:00 - train: epoch 0088, iter [04800, 05004], lr: 0.003535, loss: 0.7135
2022-08-28 09:27:34 - train: epoch 0088, iter [04900, 05004], lr: 0.003523, loss: 0.7284
2022-08-28 09:28:07 - train: epoch 0088, iter [05000, 05004], lr: 0.003512, loss: 0.8681
2022-08-28 09:28:08 - train: epoch 088, train_loss: 0.8680
2022-08-28 09:29:24 - eval: epoch: 088, acc1: 75.558%, acc5: 92.694%, test_loss: 0.9780, per_image_load_time: 2.412ms, per_image_inference_time: 0.522ms
2022-08-28 09:29:24 - until epoch: 088, best_acc1: 75.558%
2022-08-28 09:29:24 - epoch 089 lr: 0.003511
2022-08-28 09:30:02 - train: epoch 0089, iter [00100, 05004], lr: 0.003500, loss: 1.0136
2022-08-28 09:30:36 - train: epoch 0089, iter [00200, 05004], lr: 0.003488, loss: 0.6636
2022-08-28 09:31:09 - train: epoch 0089, iter [00300, 05004], lr: 0.003477, loss: 0.8641
2022-08-28 09:31:43 - train: epoch 0089, iter [00400, 05004], lr: 0.003465, loss: 0.8476
2022-08-28 09:32:17 - train: epoch 0089, iter [00500, 05004], lr: 0.003454, loss: 0.8338
2022-08-28 09:32:50 - train: epoch 0089, iter [00600, 05004], lr: 0.003442, loss: 0.8293
2022-08-28 09:33:24 - train: epoch 0089, iter [00700, 05004], lr: 0.003431, loss: 0.8761
2022-08-28 09:33:58 - train: epoch 0089, iter [00800, 05004], lr: 0.003419, loss: 0.9458
2022-08-28 09:34:31 - train: epoch 0089, iter [00900, 05004], lr: 0.003408, loss: 0.8991
2022-08-28 09:35:04 - train: epoch 0089, iter [01000, 05004], lr: 0.003397, loss: 0.9675
2022-08-28 09:35:39 - train: epoch 0089, iter [01100, 05004], lr: 0.003385, loss: 0.7780
2022-08-28 09:36:12 - train: epoch 0089, iter [01200, 05004], lr: 0.003374, loss: 0.9589
2022-08-28 09:36:46 - train: epoch 0089, iter [01300, 05004], lr: 0.003363, loss: 0.8523
2022-08-28 09:37:19 - train: epoch 0089, iter [01400, 05004], lr: 0.003351, loss: 0.9043
2022-08-28 09:37:53 - train: epoch 0089, iter [01500, 05004], lr: 0.003340, loss: 0.8462
2022-08-28 09:38:28 - train: epoch 0089, iter [01600, 05004], lr: 0.003329, loss: 0.8534
2022-08-28 09:39:01 - train: epoch 0089, iter [01700, 05004], lr: 0.003317, loss: 0.8704
2022-08-28 09:39:35 - train: epoch 0089, iter [01800, 05004], lr: 0.003306, loss: 0.8525
2022-08-28 09:40:09 - train: epoch 0089, iter [01900, 05004], lr: 0.003295, loss: 0.7829
2022-08-28 09:40:44 - train: epoch 0089, iter [02000, 05004], lr: 0.003284, loss: 0.7485
2022-08-28 09:41:16 - train: epoch 0089, iter [02100, 05004], lr: 0.003273, loss: 0.9507
2022-08-28 09:41:50 - train: epoch 0089, iter [02200, 05004], lr: 0.003261, loss: 0.8564
2022-08-28 09:42:24 - train: epoch 0089, iter [02300, 05004], lr: 0.003250, loss: 0.7946
2022-08-28 09:42:58 - train: epoch 0089, iter [02400, 05004], lr: 0.003239, loss: 0.8637
2022-08-28 09:43:32 - train: epoch 0089, iter [02500, 05004], lr: 0.003228, loss: 0.8007
2022-08-28 09:44:06 - train: epoch 0089, iter [02600, 05004], lr: 0.003217, loss: 0.8753
2022-08-28 09:44:40 - train: epoch 0089, iter [02700, 05004], lr: 0.003206, loss: 0.7602
2022-08-28 09:45:14 - train: epoch 0089, iter [02800, 05004], lr: 0.003195, loss: 0.8745
2022-08-28 09:45:47 - train: epoch 0089, iter [02900, 05004], lr: 0.003184, loss: 0.8412
2022-08-28 09:46:21 - train: epoch 0089, iter [03000, 05004], lr: 0.003173, loss: 0.9524
2022-08-28 09:46:56 - train: epoch 0089, iter [03100, 05004], lr: 0.003162, loss: 0.9176
2022-08-28 09:47:29 - train: epoch 0089, iter [03200, 05004], lr: 0.003151, loss: 0.9107
2022-08-28 09:48:02 - train: epoch 0089, iter [03300, 05004], lr: 0.003140, loss: 1.0068
2022-08-28 09:48:37 - train: epoch 0089, iter [03400, 05004], lr: 0.003129, loss: 0.8771
2022-08-28 09:49:11 - train: epoch 0089, iter [03500, 05004], lr: 0.003118, loss: 0.7476
2022-08-28 09:49:45 - train: epoch 0089, iter [03600, 05004], lr: 0.003107, loss: 0.7373
2022-08-28 09:50:19 - train: epoch 0089, iter [03700, 05004], lr: 0.003096, loss: 1.0422
2022-08-28 09:50:53 - train: epoch 0089, iter [03800, 05004], lr: 0.003085, loss: 0.7503
2022-08-28 09:51:27 - train: epoch 0089, iter [03900, 05004], lr: 0.003074, loss: 0.9098
2022-08-28 09:52:01 - train: epoch 0089, iter [04000, 05004], lr: 0.003064, loss: 0.6620
2022-08-28 09:52:36 - train: epoch 0089, iter [04100, 05004], lr: 0.003053, loss: 0.9972
2022-08-28 09:53:09 - train: epoch 0089, iter [04200, 05004], lr: 0.003042, loss: 0.7595
2022-08-28 09:53:44 - train: epoch 0089, iter [04300, 05004], lr: 0.003031, loss: 0.8643
2022-08-28 09:54:17 - train: epoch 0089, iter [04400, 05004], lr: 0.003021, loss: 0.8238
2022-08-28 09:54:51 - train: epoch 0089, iter [04500, 05004], lr: 0.003010, loss: 0.7825
2022-08-28 09:55:25 - train: epoch 0089, iter [04600, 05004], lr: 0.002999, loss: 0.8579
2022-08-28 09:55:59 - train: epoch 0089, iter [04700, 05004], lr: 0.002988, loss: 0.8526
2022-08-28 09:56:33 - train: epoch 0089, iter [04800, 05004], lr: 0.002978, loss: 0.8120
2022-08-28 09:57:07 - train: epoch 0089, iter [04900, 05004], lr: 0.002967, loss: 0.7888
2022-08-28 09:57:40 - train: epoch 0089, iter [05000, 05004], lr: 0.002956, loss: 0.6197
2022-08-28 09:57:41 - train: epoch 089, train_loss: 0.8460
2022-08-28 09:58:55 - eval: epoch: 089, acc1: 75.618%, acc5: 92.720%, test_loss: 0.9728, per_image_load_time: 2.069ms, per_image_inference_time: 0.515ms
2022-08-28 09:58:56 - until epoch: 089, best_acc1: 75.618%
2022-08-28 09:58:56 - epoch 090 lr: 0.002956
2022-08-28 09:59:34 - train: epoch 0090, iter [00100, 05004], lr: 0.002945, loss: 0.8281
2022-08-28 10:00:08 - train: epoch 0090, iter [00200, 05004], lr: 0.002935, loss: 0.7986
2022-08-28 10:00:42 - train: epoch 0090, iter [00300, 05004], lr: 0.002924, loss: 0.7992
2022-08-28 10:01:16 - train: epoch 0090, iter [00400, 05004], lr: 0.002914, loss: 0.7719
2022-08-28 10:01:49 - train: epoch 0090, iter [00500, 05004], lr: 0.002903, loss: 0.9023
2022-08-28 10:02:23 - train: epoch 0090, iter [00600, 05004], lr: 0.002892, loss: 0.9510
2022-08-28 10:02:57 - train: epoch 0090, iter [00700, 05004], lr: 0.002882, loss: 0.8714
2022-08-28 10:03:31 - train: epoch 0090, iter [00800, 05004], lr: 0.002871, loss: 0.7575
2022-08-28 10:04:05 - train: epoch 0090, iter [00900, 05004], lr: 0.002861, loss: 0.7166
2022-08-28 10:04:38 - train: epoch 0090, iter [01000, 05004], lr: 0.002851, loss: 0.7411
2022-08-28 10:05:13 - train: epoch 0090, iter [01100, 05004], lr: 0.002840, loss: 0.8732
2022-08-28 10:05:47 - train: epoch 0090, iter [01200, 05004], lr: 0.002830, loss: 0.7923
2022-08-28 10:06:21 - train: epoch 0090, iter [01300, 05004], lr: 0.002819, loss: 1.0211
2022-08-28 10:06:54 - train: epoch 0090, iter [01400, 05004], lr: 0.002809, loss: 0.7479
2022-08-28 10:07:28 - train: epoch 0090, iter [01500, 05004], lr: 0.002799, loss: 1.0233
2022-08-28 10:08:02 - train: epoch 0090, iter [01600, 05004], lr: 0.002788, loss: 0.7981
2022-08-28 10:08:37 - train: epoch 0090, iter [01700, 05004], lr: 0.002778, loss: 0.6963
2022-08-28 10:09:10 - train: epoch 0090, iter [01800, 05004], lr: 0.002768, loss: 0.6610
2022-08-28 10:09:44 - train: epoch 0090, iter [01900, 05004], lr: 0.002757, loss: 0.7490
2022-08-28 10:10:18 - train: epoch 0090, iter [02000, 05004], lr: 0.002747, loss: 0.9479
2022-08-28 10:10:51 - train: epoch 0090, iter [02100, 05004], lr: 0.002737, loss: 0.9596
2022-08-28 10:11:26 - train: epoch 0090, iter [02200, 05004], lr: 0.002727, loss: 0.8245
2022-08-28 10:11:59 - train: epoch 0090, iter [02300, 05004], lr: 0.002716, loss: 1.0141
2022-08-28 10:12:34 - train: epoch 0090, iter [02400, 05004], lr: 0.002706, loss: 0.8461
2022-08-28 10:13:07 - train: epoch 0090, iter [02500, 05004], lr: 0.002696, loss: 0.9367
2022-08-28 10:13:42 - train: epoch 0090, iter [02600, 05004], lr: 0.002686, loss: 0.8919
2022-08-28 10:14:15 - train: epoch 0090, iter [02700, 05004], lr: 0.002676, loss: 0.7649
2022-08-28 10:14:50 - train: epoch 0090, iter [02800, 05004], lr: 0.002666, loss: 0.8172
2022-08-28 10:15:25 - train: epoch 0090, iter [02900, 05004], lr: 0.002655, loss: 0.7934
2022-08-28 10:15:59 - train: epoch 0090, iter [03000, 05004], lr: 0.002645, loss: 0.8553
2022-08-28 10:16:32 - train: epoch 0090, iter [03100, 05004], lr: 0.002635, loss: 0.7427
2022-08-28 10:17:06 - train: epoch 0090, iter [03200, 05004], lr: 0.002625, loss: 0.7641
2022-08-28 10:17:40 - train: epoch 0090, iter [03300, 05004], lr: 0.002615, loss: 0.9187
2022-08-28 10:18:13 - train: epoch 0090, iter [03400, 05004], lr: 0.002605, loss: 0.6727
2022-08-28 10:18:47 - train: epoch 0090, iter [03500, 05004], lr: 0.002595, loss: 0.7964
2022-08-28 10:19:22 - train: epoch 0090, iter [03600, 05004], lr: 0.002585, loss: 0.7580
2022-08-28 10:19:56 - train: epoch 0090, iter [03700, 05004], lr: 0.002575, loss: 0.8120
2022-08-28 10:20:30 - train: epoch 0090, iter [03800, 05004], lr: 0.002565, loss: 0.8205
2022-08-28 10:21:04 - train: epoch 0090, iter [03900, 05004], lr: 0.002555, loss: 0.6579
2022-08-28 10:21:38 - train: epoch 0090, iter [04000, 05004], lr: 0.002546, loss: 0.8462
2022-08-28 10:22:12 - train: epoch 0090, iter [04100, 05004], lr: 0.002536, loss: 0.9977
2022-08-28 10:22:47 - train: epoch 0090, iter [04200, 05004], lr: 0.002526, loss: 0.8621
2022-08-28 10:23:20 - train: epoch 0090, iter [04300, 05004], lr: 0.002516, loss: 0.8982
2022-08-28 10:23:54 - train: epoch 0090, iter [04400, 05004], lr: 0.002506, loss: 0.7791
2022-08-28 10:24:28 - train: epoch 0090, iter [04500, 05004], lr: 0.002496, loss: 0.7520
2022-08-28 10:25:02 - train: epoch 0090, iter [04600, 05004], lr: 0.002487, loss: 0.8615
2022-08-28 10:25:37 - train: epoch 0090, iter [04700, 05004], lr: 0.002477, loss: 0.7723
2022-08-28 10:26:10 - train: epoch 0090, iter [04800, 05004], lr: 0.002467, loss: 0.8438
2022-08-28 10:26:46 - train: epoch 0090, iter [04900, 05004], lr: 0.002457, loss: 0.6825
2022-08-28 10:27:18 - train: epoch 0090, iter [05000, 05004], lr: 0.002448, loss: 0.8214
2022-08-28 10:27:19 - train: epoch 090, train_loss: 0.8222
2022-08-28 10:28:34 - eval: epoch: 090, acc1: 76.004%, acc5: 92.878%, test_loss: 0.9662, per_image_load_time: 2.351ms, per_image_inference_time: 0.522ms
2022-08-28 10:28:34 - until epoch: 090, best_acc1: 76.004%
2022-08-28 10:28:34 - epoch 091 lr: 0.002447
2022-08-28 10:29:13 - train: epoch 0091, iter [00100, 05004], lr: 0.002437, loss: 0.6811
2022-08-28 10:29:47 - train: epoch 0091, iter [00200, 05004], lr: 0.002428, loss: 0.7369
2022-08-28 10:30:21 - train: epoch 0091, iter [00300, 05004], lr: 0.002418, loss: 0.7694
2022-08-28 10:30:55 - train: epoch 0091, iter [00400, 05004], lr: 0.002409, loss: 0.6587
2022-08-28 10:31:28 - train: epoch 0091, iter [00500, 05004], lr: 0.002399, loss: 0.8123
2022-08-28 10:32:02 - train: epoch 0091, iter [00600, 05004], lr: 0.002389, loss: 0.8154
2022-08-28 10:32:36 - train: epoch 0091, iter [00700, 05004], lr: 0.002380, loss: 0.8219
2022-08-28 10:33:10 - train: epoch 0091, iter [00800, 05004], lr: 0.002370, loss: 0.7296
2022-08-28 10:33:44 - train: epoch 0091, iter [00900, 05004], lr: 0.002361, loss: 0.8533
2022-08-28 10:34:17 - train: epoch 0091, iter [01000, 05004], lr: 0.002351, loss: 0.7514
2022-08-28 10:34:51 - train: epoch 0091, iter [01100, 05004], lr: 0.002342, loss: 0.6470
2022-08-28 10:35:25 - train: epoch 0091, iter [01200, 05004], lr: 0.002332, loss: 0.9299
2022-08-28 10:35:59 - train: epoch 0091, iter [01300, 05004], lr: 0.002323, loss: 0.7764
2022-08-28 10:36:32 - train: epoch 0091, iter [01400, 05004], lr: 0.002313, loss: 0.8177
2022-08-28 10:37:07 - train: epoch 0091, iter [01500, 05004], lr: 0.002304, loss: 0.8242
2022-08-28 10:37:41 - train: epoch 0091, iter [01600, 05004], lr: 0.002294, loss: 0.7726
2022-08-28 10:38:15 - train: epoch 0091, iter [01700, 05004], lr: 0.002285, loss: 0.8784
2022-08-28 10:38:49 - train: epoch 0091, iter [01800, 05004], lr: 0.002276, loss: 0.7738
2022-08-28 10:39:22 - train: epoch 0091, iter [01900, 05004], lr: 0.002266, loss: 0.7093
2022-08-28 10:39:57 - train: epoch 0091, iter [02000, 05004], lr: 0.002257, loss: 0.7912
2022-08-28 10:40:30 - train: epoch 0091, iter [02100, 05004], lr: 0.002248, loss: 0.5892
2022-08-28 10:41:03 - train: epoch 0091, iter [02200, 05004], lr: 0.002238, loss: 0.8446
2022-08-28 10:41:38 - train: epoch 0091, iter [02300, 05004], lr: 0.002229, loss: 0.8348
2022-08-28 10:42:12 - train: epoch 0091, iter [02400, 05004], lr: 0.002220, loss: 0.7819
2022-08-28 10:42:45 - train: epoch 0091, iter [02500, 05004], lr: 0.002211, loss: 0.7373
2022-08-28 10:43:19 - train: epoch 0091, iter [02600, 05004], lr: 0.002201, loss: 0.7495
2022-08-28 10:43:53 - train: epoch 0091, iter [02700, 05004], lr: 0.002192, loss: 0.6741
2022-08-28 10:44:27 - train: epoch 0091, iter [02800, 05004], lr: 0.002183, loss: 0.8279
2022-08-28 10:45:01 - train: epoch 0091, iter [02900, 05004], lr: 0.002174, loss: 0.8973
2022-08-28 10:45:36 - train: epoch 0091, iter [03000, 05004], lr: 0.002165, loss: 0.9368
2022-08-28 10:46:09 - train: epoch 0091, iter [03100, 05004], lr: 0.002155, loss: 0.7139
2022-08-28 10:46:43 - train: epoch 0091, iter [03200, 05004], lr: 0.002146, loss: 0.9098
2022-08-28 10:47:17 - train: epoch 0091, iter [03300, 05004], lr: 0.002137, loss: 0.7196
2022-08-28 10:47:51 - train: epoch 0091, iter [03400, 05004], lr: 0.002128, loss: 0.6938
2022-08-28 10:48:25 - train: epoch 0091, iter [03500, 05004], lr: 0.002119, loss: 0.8396
2022-08-28 10:48:59 - train: epoch 0091, iter [03600, 05004], lr: 0.002110, loss: 0.7711
2022-08-28 10:49:33 - train: epoch 0091, iter [03700, 05004], lr: 0.002101, loss: 0.7699
2022-08-28 10:50:08 - train: epoch 0091, iter [03800, 05004], lr: 0.002092, loss: 0.7741
2022-08-28 10:50:41 - train: epoch 0091, iter [03900, 05004], lr: 0.002083, loss: 0.7839
2022-08-28 10:51:16 - train: epoch 0091, iter [04000, 05004], lr: 0.002074, loss: 0.8648
2022-08-28 10:51:50 - train: epoch 0091, iter [04100, 05004], lr: 0.002065, loss: 0.8603
2022-08-28 10:52:24 - train: epoch 0091, iter [04200, 05004], lr: 0.002056, loss: 0.8738
2022-08-28 10:52:59 - train: epoch 0091, iter [04300, 05004], lr: 0.002047, loss: 0.9258
2022-08-28 10:53:32 - train: epoch 0091, iter [04400, 05004], lr: 0.002039, loss: 0.6799
2022-08-28 10:54:06 - train: epoch 0091, iter [04500, 05004], lr: 0.002030, loss: 0.7858
2022-08-28 10:54:39 - train: epoch 0091, iter [04600, 05004], lr: 0.002021, loss: 0.7960
2022-08-28 10:55:13 - train: epoch 0091, iter [04700, 05004], lr: 0.002012, loss: 0.8847
2022-08-28 10:55:48 - train: epoch 0091, iter [04800, 05004], lr: 0.002003, loss: 0.7972
2022-08-28 10:56:21 - train: epoch 0091, iter [04900, 05004], lr: 0.001994, loss: 0.7970
2022-08-28 10:56:54 - train: epoch 0091, iter [05000, 05004], lr: 0.001986, loss: 0.9534
2022-08-28 10:56:56 - train: epoch 091, train_loss: 0.8006
2022-08-28 10:58:11 - eval: epoch: 091, acc1: 76.050%, acc5: 93.044%, test_loss: 0.9607, per_image_load_time: 2.402ms, per_image_inference_time: 0.522ms
2022-08-28 10:58:12 - until epoch: 091, best_acc1: 76.050%
2022-08-28 10:58:12 - epoch 092 lr: 0.001985
2022-08-28 10:58:50 - train: epoch 0092, iter [00100, 05004], lr: 0.001977, loss: 0.7956
2022-08-28 10:59:24 - train: epoch 0092, iter [00200, 05004], lr: 0.001968, loss: 0.8500
2022-08-28 10:59:58 - train: epoch 0092, iter [00300, 05004], lr: 0.001959, loss: 0.7876
2022-08-28 11:00:32 - train: epoch 0092, iter [00400, 05004], lr: 0.001950, loss: 0.6222
2022-08-28 11:01:06 - train: epoch 0092, iter [00500, 05004], lr: 0.001942, loss: 0.8673
2022-08-28 11:01:39 - train: epoch 0092, iter [00600, 05004], lr: 0.001933, loss: 0.8459
2022-08-28 11:02:13 - train: epoch 0092, iter [00700, 05004], lr: 0.001924, loss: 0.8024
2022-08-28 11:02:48 - train: epoch 0092, iter [00800, 05004], lr: 0.001916, loss: 0.8492
2022-08-28 11:03:22 - train: epoch 0092, iter [00900, 05004], lr: 0.001907, loss: 0.7684
2022-08-28 11:03:55 - train: epoch 0092, iter [01000, 05004], lr: 0.001899, loss: 0.9385
2022-08-28 11:04:29 - train: epoch 0092, iter [01100, 05004], lr: 0.001890, loss: 0.6795
2022-08-28 11:05:02 - train: epoch 0092, iter [01200, 05004], lr: 0.001882, loss: 0.7064
2022-08-28 11:05:37 - train: epoch 0092, iter [01300, 05004], lr: 0.001873, loss: 0.7270
2022-08-28 11:06:11 - train: epoch 0092, iter [01400, 05004], lr: 0.001865, loss: 0.8907
2022-08-28 11:06:44 - train: epoch 0092, iter [01500, 05004], lr: 0.001856, loss: 0.6745
2022-08-28 11:07:18 - train: epoch 0092, iter [01600, 05004], lr: 0.001848, loss: 0.7744
2022-08-28 11:07:52 - train: epoch 0092, iter [01700, 05004], lr: 0.001839, loss: 0.8241
2022-08-28 11:08:25 - train: epoch 0092, iter [01800, 05004], lr: 0.001831, loss: 0.6767
2022-08-28 11:08:59 - train: epoch 0092, iter [01900, 05004], lr: 0.001822, loss: 0.6933
2022-08-28 11:09:33 - train: epoch 0092, iter [02000, 05004], lr: 0.001814, loss: 0.7427
2022-08-28 11:10:07 - train: epoch 0092, iter [02100, 05004], lr: 0.001806, loss: 0.7709
2022-08-28 11:10:41 - train: epoch 0092, iter [02200, 05004], lr: 0.001797, loss: 0.8077
2022-08-28 11:11:15 - train: epoch 0092, iter [02300, 05004], lr: 0.001789, loss: 0.6536
2022-08-28 11:11:49 - train: epoch 0092, iter [02400, 05004], lr: 0.001781, loss: 0.6855
2022-08-28 11:12:23 - train: epoch 0092, iter [02500, 05004], lr: 0.001772, loss: 0.6608
2022-08-28 11:12:57 - train: epoch 0092, iter [02600, 05004], lr: 0.001764, loss: 0.8766
2022-08-28 11:13:31 - train: epoch 0092, iter [02700, 05004], lr: 0.001756, loss: 0.7701
2022-08-28 11:14:06 - train: epoch 0092, iter [02800, 05004], lr: 0.001748, loss: 0.6734
2022-08-28 11:14:40 - train: epoch 0092, iter [02900, 05004], lr: 0.001739, loss: 0.7631
2022-08-28 11:15:13 - train: epoch 0092, iter [03000, 05004], lr: 0.001731, loss: 0.6573
2022-08-28 11:15:48 - train: epoch 0092, iter [03100, 05004], lr: 0.001723, loss: 0.7521
2022-08-28 11:16:21 - train: epoch 0092, iter [03200, 05004], lr: 0.001715, loss: 0.6459
2022-08-28 11:16:55 - train: epoch 0092, iter [03300, 05004], lr: 0.001707, loss: 0.8904
2022-08-28 11:17:29 - train: epoch 0092, iter [03400, 05004], lr: 0.001699, loss: 0.9841
2022-08-28 11:18:03 - train: epoch 0092, iter [03500, 05004], lr: 0.001690, loss: 0.6315
2022-08-28 11:18:36 - train: epoch 0092, iter [03600, 05004], lr: 0.001682, loss: 0.7323
2022-08-28 11:19:11 - train: epoch 0092, iter [03700, 05004], lr: 0.001674, loss: 0.7177
2022-08-28 11:19:44 - train: epoch 0092, iter [03800, 05004], lr: 0.001666, loss: 0.9045
2022-08-28 11:20:18 - train: epoch 0092, iter [03900, 05004], lr: 0.001658, loss: 0.7435
2022-08-28 11:20:52 - train: epoch 0092, iter [04000, 05004], lr: 0.001650, loss: 0.8816
2022-08-28 11:21:27 - train: epoch 0092, iter [04100, 05004], lr: 0.001642, loss: 0.6605
2022-08-28 11:22:01 - train: epoch 0092, iter [04200, 05004], lr: 0.001634, loss: 0.8039
2022-08-28 11:22:34 - train: epoch 0092, iter [04300, 05004], lr: 0.001626, loss: 0.8438
2022-08-28 11:23:08 - train: epoch 0092, iter [04400, 05004], lr: 0.001618, loss: 0.6606
2022-08-28 11:23:43 - train: epoch 0092, iter [04500, 05004], lr: 0.001610, loss: 0.7207
2022-08-28 11:24:17 - train: epoch 0092, iter [04600, 05004], lr: 0.001603, loss: 0.7242
2022-08-28 11:24:50 - train: epoch 0092, iter [04700, 05004], lr: 0.001595, loss: 0.5168
2022-08-28 11:25:23 - train: epoch 0092, iter [04800, 05004], lr: 0.001587, loss: 0.7162
2022-08-28 11:25:57 - train: epoch 0092, iter [04900, 05004], lr: 0.001579, loss: 0.8306
2022-08-28 11:26:30 - train: epoch 0092, iter [05000, 05004], lr: 0.001571, loss: 0.9092
2022-08-28 11:26:31 - train: epoch 092, train_loss: 0.7798
2022-08-28 11:27:46 - eval: epoch: 092, acc1: 76.158%, acc5: 93.094%, test_loss: 0.9585, per_image_load_time: 2.406ms, per_image_inference_time: 0.513ms
2022-08-28 11:27:47 - until epoch: 092, best_acc1: 76.158%
2022-08-28 11:27:47 - epoch 093 lr: 0.001571
2022-08-28 11:28:25 - train: epoch 0093, iter [00100, 05004], lr: 0.001563, loss: 0.6965
2022-08-28 11:28:59 - train: epoch 0093, iter [00200, 05004], lr: 0.001555, loss: 0.7463
2022-08-28 11:29:33 - train: epoch 0093, iter [00300, 05004], lr: 0.001548, loss: 0.7173
2022-08-28 11:30:07 - train: epoch 0093, iter [00400, 05004], lr: 0.001540, loss: 0.7453
2022-08-28 11:30:41 - train: epoch 0093, iter [00500, 05004], lr: 0.001532, loss: 0.8046
2022-08-28 11:31:15 - train: epoch 0093, iter [00600, 05004], lr: 0.001524, loss: 0.7551
2022-08-28 11:31:49 - train: epoch 0093, iter [00700, 05004], lr: 0.001517, loss: 0.8146
2022-08-28 11:32:22 - train: epoch 0093, iter [00800, 05004], lr: 0.001509, loss: 0.6966
2022-08-28 11:32:55 - train: epoch 0093, iter [00900, 05004], lr: 0.001501, loss: 0.6997
2022-08-28 11:33:29 - train: epoch 0093, iter [01000, 05004], lr: 0.001494, loss: 0.6415
2022-08-28 11:34:03 - train: epoch 0093, iter [01100, 05004], lr: 0.001486, loss: 0.6325
2022-08-28 11:34:37 - train: epoch 0093, iter [01200, 05004], lr: 0.001479, loss: 0.5966
2022-08-28 11:35:11 - train: epoch 0093, iter [01300, 05004], lr: 0.001471, loss: 0.8162
2022-08-28 11:35:44 - train: epoch 0093, iter [01400, 05004], lr: 0.001463, loss: 0.8028
2022-08-28 11:36:17 - train: epoch 0093, iter [01500, 05004], lr: 0.001456, loss: 0.9584
2022-08-28 11:36:51 - train: epoch 0093, iter [01600, 05004], lr: 0.001448, loss: 0.8507
2022-08-28 11:37:25 - train: epoch 0093, iter [01700, 05004], lr: 0.001441, loss: 0.7231
2022-08-28 11:37:59 - train: epoch 0093, iter [01800, 05004], lr: 0.001433, loss: 0.7545
2022-08-28 11:38:33 - train: epoch 0093, iter [01900, 05004], lr: 0.001426, loss: 0.7231
2022-08-28 11:39:07 - train: epoch 0093, iter [02000, 05004], lr: 0.001419, loss: 0.6390
2022-08-28 11:39:41 - train: epoch 0093, iter [02100, 05004], lr: 0.001411, loss: 0.7200
2022-08-28 11:40:15 - train: epoch 0093, iter [02200, 05004], lr: 0.001404, loss: 0.9460
2022-08-28 11:40:50 - train: epoch 0093, iter [02300, 05004], lr: 0.001396, loss: 0.7397
2022-08-28 11:41:23 - train: epoch 0093, iter [02400, 05004], lr: 0.001389, loss: 0.7208
2022-08-28 11:41:57 - train: epoch 0093, iter [02500, 05004], lr: 0.001382, loss: 0.7869
2022-08-28 11:42:31 - train: epoch 0093, iter [02600, 05004], lr: 0.001374, loss: 0.8315
2022-08-28 11:43:06 - train: epoch 0093, iter [02700, 05004], lr: 0.001367, loss: 0.6486
2022-08-28 11:43:39 - train: epoch 0093, iter [02800, 05004], lr: 0.001360, loss: 0.7423
2022-08-28 11:44:13 - train: epoch 0093, iter [02900, 05004], lr: 0.001352, loss: 0.7694
2022-08-28 11:44:47 - train: epoch 0093, iter [03000, 05004], lr: 0.001345, loss: 0.7126
2022-08-28 11:45:21 - train: epoch 0093, iter [03100, 05004], lr: 0.001338, loss: 0.9449
2022-08-28 11:45:55 - train: epoch 0093, iter [03200, 05004], lr: 0.001331, loss: 0.6748
2022-08-28 11:46:29 - train: epoch 0093, iter [03300, 05004], lr: 0.001324, loss: 0.8201
2022-08-28 11:47:04 - train: epoch 0093, iter [03400, 05004], lr: 0.001316, loss: 0.9439
2022-08-28 11:47:38 - train: epoch 0093, iter [03500, 05004], lr: 0.001309, loss: 0.5941
2022-08-28 11:48:11 - train: epoch 0093, iter [03600, 05004], lr: 0.001302, loss: 0.8805
2022-08-28 11:48:44 - train: epoch 0093, iter [03700, 05004], lr: 0.001295, loss: 0.6813
2022-08-28 11:49:18 - train: epoch 0093, iter [03800, 05004], lr: 0.001288, loss: 0.6028
2022-08-28 11:49:52 - train: epoch 0093, iter [03900, 05004], lr: 0.001281, loss: 0.8248
2022-08-28 11:50:26 - train: epoch 0093, iter [04000, 05004], lr: 0.001274, loss: 1.0275
2022-08-28 11:51:00 - train: epoch 0093, iter [04100, 05004], lr: 0.001267, loss: 0.8591
2022-08-28 11:51:34 - train: epoch 0093, iter [04200, 05004], lr: 0.001260, loss: 0.7752
2022-08-28 11:52:08 - train: epoch 0093, iter [04300, 05004], lr: 0.001253, loss: 0.9246
2022-08-28 11:52:42 - train: epoch 0093, iter [04400, 05004], lr: 0.001246, loss: 0.7045
2022-08-28 11:53:16 - train: epoch 0093, iter [04500, 05004], lr: 0.001239, loss: 0.7486
2022-08-28 11:53:50 - train: epoch 0093, iter [04600, 05004], lr: 0.001232, loss: 0.6231
2022-08-28 11:54:24 - train: epoch 0093, iter [04700, 05004], lr: 0.001225, loss: 0.8939
2022-08-28 11:54:59 - train: epoch 0093, iter [04800, 05004], lr: 0.001218, loss: 0.7343
2022-08-28 11:55:32 - train: epoch 0093, iter [04900, 05004], lr: 0.001211, loss: 0.8832
2022-08-28 11:56:05 - train: epoch 0093, iter [05000, 05004], lr: 0.001204, loss: 0.7286
2022-08-28 11:56:06 - train: epoch 093, train_loss: 0.7626
2022-08-28 11:57:21 - eval: epoch: 093, acc1: 76.390%, acc5: 93.132%, test_loss: 0.9480, per_image_load_time: 2.401ms, per_image_inference_time: 0.531ms
2022-08-28 11:57:22 - until epoch: 093, best_acc1: 76.390%
2022-08-28 11:57:22 - epoch 094 lr: 0.001204
2022-08-28 11:58:00 - train: epoch 0094, iter [00100, 05004], lr: 0.001197, loss: 0.7860
2022-08-28 11:58:34 - train: epoch 0094, iter [00200, 05004], lr: 0.001191, loss: 0.7817
2022-08-28 11:59:08 - train: epoch 0094, iter [00300, 05004], lr: 0.001184, loss: 0.8318
2022-08-28 11:59:42 - train: epoch 0094, iter [00400, 05004], lr: 0.001177, loss: 0.9449
2022-08-28 12:00:17 - train: epoch 0094, iter [00500, 05004], lr: 0.001170, loss: 0.7252
2022-08-28 12:00:50 - train: epoch 0094, iter [00600, 05004], lr: 0.001163, loss: 0.6443
2022-08-28 12:01:23 - train: epoch 0094, iter [00700, 05004], lr: 0.001157, loss: 0.7513
2022-08-28 12:01:57 - train: epoch 0094, iter [00800, 05004], lr: 0.001150, loss: 0.6684
2022-08-28 12:02:30 - train: epoch 0094, iter [00900, 05004], lr: 0.001143, loss: 0.6993
2022-08-28 12:03:04 - train: epoch 0094, iter [01000, 05004], lr: 0.001137, loss: 0.8046
2022-08-28 12:03:37 - train: epoch 0094, iter [01100, 05004], lr: 0.001130, loss: 0.8100
2022-08-28 12:04:10 - train: epoch 0094, iter [01200, 05004], lr: 0.001123, loss: 0.7530
2022-08-28 12:04:44 - train: epoch 0094, iter [01300, 05004], lr: 0.001117, loss: 0.8332
2022-08-28 12:05:18 - train: epoch 0094, iter [01400, 05004], lr: 0.001110, loss: 0.6872
2022-08-28 12:05:52 - train: epoch 0094, iter [01500, 05004], lr: 0.001104, loss: 0.8851
2022-08-28 12:06:26 - train: epoch 0094, iter [01600, 05004], lr: 0.001097, loss: 1.0801
2022-08-28 12:06:59 - train: epoch 0094, iter [01700, 05004], lr: 0.001091, loss: 0.7059
2022-08-28 12:07:33 - train: epoch 0094, iter [01800, 05004], lr: 0.001084, loss: 0.6975
2022-08-28 12:08:07 - train: epoch 0094, iter [01900, 05004], lr: 0.001078, loss: 0.7855
2022-08-28 12:08:41 - train: epoch 0094, iter [02000, 05004], lr: 0.001071, loss: 0.6600
2022-08-28 12:09:14 - train: epoch 0094, iter [02100, 05004], lr: 0.001065, loss: 0.8081
2022-08-28 12:09:49 - train: epoch 0094, iter [02200, 05004], lr: 0.001058, loss: 0.6069
2022-08-28 12:10:23 - train: epoch 0094, iter [02300, 05004], lr: 0.001052, loss: 0.5604
2022-08-28 12:10:57 - train: epoch 0094, iter [02400, 05004], lr: 0.001045, loss: 0.7697
2022-08-28 12:11:30 - train: epoch 0094, iter [02500, 05004], lr: 0.001039, loss: 0.7275
2022-08-28 12:12:04 - train: epoch 0094, iter [02600, 05004], lr: 0.001033, loss: 0.6296
2022-08-28 12:12:39 - train: epoch 0094, iter [02700, 05004], lr: 0.001026, loss: 0.6971
2022-08-28 12:13:12 - train: epoch 0094, iter [02800, 05004], lr: 0.001020, loss: 0.6966
2022-08-28 12:13:47 - train: epoch 0094, iter [02900, 05004], lr: 0.001014, loss: 0.8020
2022-08-28 12:14:20 - train: epoch 0094, iter [03000, 05004], lr: 0.001007, loss: 0.6996
2022-08-28 12:14:54 - train: epoch 0094, iter [03100, 05004], lr: 0.001001, loss: 0.8885
2022-08-28 12:15:28 - train: epoch 0094, iter [03200, 05004], lr: 0.000995, loss: 0.7126
2022-08-28 12:16:02 - train: epoch 0094, iter [03300, 05004], lr: 0.000989, loss: 0.7293
2022-08-28 12:16:37 - train: epoch 0094, iter [03400, 05004], lr: 0.000982, loss: 0.8494
2022-08-28 12:17:11 - train: epoch 0094, iter [03500, 05004], lr: 0.000976, loss: 0.8995
2022-08-28 12:17:46 - train: epoch 0094, iter [03600, 05004], lr: 0.000970, loss: 0.7166
2022-08-28 12:18:19 - train: epoch 0094, iter [03700, 05004], lr: 0.000964, loss: 0.8642
2022-08-28 12:18:53 - train: epoch 0094, iter [03800, 05004], lr: 0.000958, loss: 0.7385
2022-08-28 12:19:27 - train: epoch 0094, iter [03900, 05004], lr: 0.000952, loss: 0.7366
2022-08-28 12:20:02 - train: epoch 0094, iter [04000, 05004], lr: 0.000946, loss: 0.8508
2022-08-28 12:20:36 - train: epoch 0094, iter [04100, 05004], lr: 0.000940, loss: 1.0038
2022-08-28 12:21:09 - train: epoch 0094, iter [04200, 05004], lr: 0.000934, loss: 0.5952
2022-08-28 12:21:44 - train: epoch 0094, iter [04300, 05004], lr: 0.000928, loss: 0.8321
2022-08-28 12:22:18 - train: epoch 0094, iter [04400, 05004], lr: 0.000922, loss: 0.8095
2022-08-28 12:22:52 - train: epoch 0094, iter [04500, 05004], lr: 0.000916, loss: 0.8080
2022-08-28 12:23:25 - train: epoch 0094, iter [04600, 05004], lr: 0.000910, loss: 0.7560
2022-08-28 12:24:00 - train: epoch 0094, iter [04700, 05004], lr: 0.000904, loss: 0.7774
2022-08-28 12:24:34 - train: epoch 0094, iter [04800, 05004], lr: 0.000898, loss: 0.6910
2022-08-28 12:25:09 - train: epoch 0094, iter [04900, 05004], lr: 0.000892, loss: 0.7665
2022-08-28 12:25:41 - train: epoch 0094, iter [05000, 05004], lr: 0.000886, loss: 0.6488
2022-08-28 12:25:42 - train: epoch 094, train_loss: 0.7481
2022-08-28 12:26:58 - eval: epoch: 094, acc1: 76.476%, acc5: 93.206%, test_loss: 0.9441, per_image_load_time: 2.426ms, per_image_inference_time: 0.498ms
2022-08-28 12:26:58 - until epoch: 094, best_acc1: 76.476%
2022-08-28 12:26:58 - epoch 095 lr: 0.000886
2022-08-28 12:27:37 - train: epoch 0095, iter [00100, 05004], lr: 0.000880, loss: 0.6134
2022-08-28 12:28:12 - train: epoch 0095, iter [00200, 05004], lr: 0.000874, loss: 0.7901
2022-08-28 12:28:45 - train: epoch 0095, iter [00300, 05004], lr: 0.000868, loss: 0.6339
2022-08-28 12:29:19 - train: epoch 0095, iter [00400, 05004], lr: 0.000862, loss: 0.7771
2022-08-28 12:29:53 - train: epoch 0095, iter [00500, 05004], lr: 0.000856, loss: 0.8417
2022-08-28 12:30:26 - train: epoch 0095, iter [00600, 05004], lr: 0.000851, loss: 0.7265
2022-08-28 12:30:59 - train: epoch 0095, iter [00700, 05004], lr: 0.000845, loss: 0.7730
2022-08-28 12:31:33 - train: epoch 0095, iter [00800, 05004], lr: 0.000839, loss: 0.9212
2022-08-28 12:32:07 - train: epoch 0095, iter [00900, 05004], lr: 0.000833, loss: 0.8828
2022-08-28 12:32:42 - train: epoch 0095, iter [01000, 05004], lr: 0.000828, loss: 0.8253
2022-08-28 12:33:14 - train: epoch 0095, iter [01100, 05004], lr: 0.000822, loss: 0.6709
2022-08-28 12:33:49 - train: epoch 0095, iter [01200, 05004], lr: 0.000816, loss: 0.5823
2022-08-28 12:34:23 - train: epoch 0095, iter [01300, 05004], lr: 0.000811, loss: 0.7495
2022-08-28 12:34:57 - train: epoch 0095, iter [01400, 05004], lr: 0.000805, loss: 0.7728
2022-08-28 12:35:30 - train: epoch 0095, iter [01500, 05004], lr: 0.000800, loss: 0.7380
2022-08-28 12:36:04 - train: epoch 0095, iter [01600, 05004], lr: 0.000794, loss: 0.4952
2022-08-28 12:36:38 - train: epoch 0095, iter [01700, 05004], lr: 0.000788, loss: 0.8427
2022-08-28 12:37:12 - train: epoch 0095, iter [01800, 05004], lr: 0.000783, loss: 0.7563
2022-08-28 12:37:45 - train: epoch 0095, iter [01900, 05004], lr: 0.000777, loss: 0.6606
2022-08-28 12:38:19 - train: epoch 0095, iter [02000, 05004], lr: 0.000772, loss: 0.8114
2022-08-28 12:38:53 - train: epoch 0095, iter [02100, 05004], lr: 0.000766, loss: 0.6821
2022-08-28 12:39:27 - train: epoch 0095, iter [02200, 05004], lr: 0.000761, loss: 0.6059
2022-08-28 12:40:01 - train: epoch 0095, iter [02300, 05004], lr: 0.000755, loss: 0.7140
2022-08-28 12:40:35 - train: epoch 0095, iter [02400, 05004], lr: 0.000750, loss: 0.8373
2022-08-28 12:41:09 - train: epoch 0095, iter [02500, 05004], lr: 0.000745, loss: 0.6151
2022-08-28 12:41:43 - train: epoch 0095, iter [02600, 05004], lr: 0.000739, loss: 0.6797
2022-08-28 12:42:17 - train: epoch 0095, iter [02700, 05004], lr: 0.000734, loss: 0.7696
2022-08-28 12:42:51 - train: epoch 0095, iter [02800, 05004], lr: 0.000729, loss: 0.6670
2022-08-28 12:43:26 - train: epoch 0095, iter [02900, 05004], lr: 0.000723, loss: 0.6324
2022-08-28 12:44:00 - train: epoch 0095, iter [03000, 05004], lr: 0.000718, loss: 0.8250
2022-08-28 12:44:33 - train: epoch 0095, iter [03100, 05004], lr: 0.000713, loss: 0.9633
2022-08-28 12:45:08 - train: epoch 0095, iter [03200, 05004], lr: 0.000707, loss: 0.7066
2022-08-28 12:45:41 - train: epoch 0095, iter [03300, 05004], lr: 0.000702, loss: 0.7935
2022-08-28 12:46:14 - train: epoch 0095, iter [03400, 05004], lr: 0.000697, loss: 0.7546
2022-08-28 12:46:49 - train: epoch 0095, iter [03500, 05004], lr: 0.000692, loss: 0.6769
2022-08-28 12:47:23 - train: epoch 0095, iter [03600, 05004], lr: 0.000686, loss: 0.7585
2022-08-28 12:47:57 - train: epoch 0095, iter [03700, 05004], lr: 0.000681, loss: 0.7815
2022-08-28 12:48:32 - train: epoch 0095, iter [03800, 05004], lr: 0.000676, loss: 0.5447
2022-08-28 12:49:06 - train: epoch 0095, iter [03900, 05004], lr: 0.000671, loss: 0.7697
2022-08-28 12:49:40 - train: epoch 0095, iter [04000, 05004], lr: 0.000666, loss: 0.5764
2022-08-28 12:50:14 - train: epoch 0095, iter [04100, 05004], lr: 0.000661, loss: 0.7676
2022-08-28 12:50:48 - train: epoch 0095, iter [04200, 05004], lr: 0.000656, loss: 0.6162
2022-08-28 12:51:23 - train: epoch 0095, iter [04300, 05004], lr: 0.000651, loss: 0.7171
2022-08-28 12:51:57 - train: epoch 0095, iter [04400, 05004], lr: 0.000646, loss: 0.7453
2022-08-28 12:52:32 - train: epoch 0095, iter [04500, 05004], lr: 0.000641, loss: 0.6089
2022-08-28 12:53:05 - train: epoch 0095, iter [04600, 05004], lr: 0.000636, loss: 0.7181
2022-08-28 12:53:39 - train: epoch 0095, iter [04700, 05004], lr: 0.000631, loss: 0.6570
2022-08-28 12:54:14 - train: epoch 0095, iter [04800, 05004], lr: 0.000626, loss: 0.7943
2022-08-28 12:54:48 - train: epoch 0095, iter [04900, 05004], lr: 0.000621, loss: 0.6948
2022-08-28 12:55:21 - train: epoch 0095, iter [05000, 05004], lr: 0.000616, loss: 0.5628
2022-08-28 12:55:22 - train: epoch 095, train_loss: 0.7361
2022-08-28 12:56:38 - eval: epoch: 095, acc1: 76.666%, acc5: 93.230%, test_loss: 0.9400, per_image_load_time: 2.402ms, per_image_inference_time: 0.530ms
2022-08-28 12:56:38 - until epoch: 095, best_acc1: 76.666%
2022-08-28 12:56:38 - epoch 096 lr: 0.000616
2022-08-28 12:57:17 - train: epoch 0096, iter [00100, 05004], lr: 0.000611, loss: 0.8079
2022-08-28 12:57:50 - train: epoch 0096, iter [00200, 05004], lr: 0.000606, loss: 0.7917
2022-08-28 12:58:24 - train: epoch 0096, iter [00300, 05004], lr: 0.000601, loss: 0.8209
2022-08-28 12:58:58 - train: epoch 0096, iter [00400, 05004], lr: 0.000596, loss: 0.6279
2022-08-28 12:59:31 - train: epoch 0096, iter [00500, 05004], lr: 0.000591, loss: 0.7077
2022-08-28 13:00:05 - train: epoch 0096, iter [00600, 05004], lr: 0.000586, loss: 0.7853
2022-08-28 13:00:38 - train: epoch 0096, iter [00700, 05004], lr: 0.000582, loss: 0.6090
2022-08-28 13:01:12 - train: epoch 0096, iter [00800, 05004], lr: 0.000577, loss: 0.6685
2022-08-28 13:01:46 - train: epoch 0096, iter [00900, 05004], lr: 0.000572, loss: 0.6761
2022-08-28 13:02:20 - train: epoch 0096, iter [01000, 05004], lr: 0.000567, loss: 0.7390
2022-08-28 13:02:53 - train: epoch 0096, iter [01100, 05004], lr: 0.000563, loss: 0.7898
2022-08-28 13:03:28 - train: epoch 0096, iter [01200, 05004], lr: 0.000558, loss: 0.7230
2022-08-28 13:04:01 - train: epoch 0096, iter [01300, 05004], lr: 0.000553, loss: 0.7276
2022-08-28 13:04:35 - train: epoch 0096, iter [01400, 05004], lr: 0.000549, loss: 0.7047
2022-08-28 13:05:08 - train: epoch 0096, iter [01500, 05004], lr: 0.000544, loss: 0.6815
2022-08-28 13:05:42 - train: epoch 0096, iter [01600, 05004], lr: 0.000540, loss: 0.5299
2022-08-28 13:06:15 - train: epoch 0096, iter [01700, 05004], lr: 0.000535, loss: 0.8173
2022-08-28 13:06:49 - train: epoch 0096, iter [01800, 05004], lr: 0.000530, loss: 0.7671
2022-08-28 13:07:22 - train: epoch 0096, iter [01900, 05004], lr: 0.000526, loss: 0.7886
2022-08-28 13:07:57 - train: epoch 0096, iter [02000, 05004], lr: 0.000521, loss: 0.6669
2022-08-28 13:08:32 - train: epoch 0096, iter [02100, 05004], lr: 0.000517, loss: 0.7856
2022-08-28 13:09:05 - train: epoch 0096, iter [02200, 05004], lr: 0.000512, loss: 0.5648
2022-08-28 13:09:39 - train: epoch 0096, iter [02300, 05004], lr: 0.000508, loss: 0.5670
2022-08-28 13:10:12 - train: epoch 0096, iter [02400, 05004], lr: 0.000503, loss: 0.6023
2022-08-28 13:10:45 - train: epoch 0096, iter [02500, 05004], lr: 0.000499, loss: 0.6587
2022-08-28 13:11:19 - train: epoch 0096, iter [02600, 05004], lr: 0.000494, loss: 0.6943
2022-08-28 13:11:54 - train: epoch 0096, iter [02700, 05004], lr: 0.000490, loss: 0.7117
2022-08-28 13:12:27 - train: epoch 0096, iter [02800, 05004], lr: 0.000486, loss: 0.8234
2022-08-28 13:13:02 - train: epoch 0096, iter [02900, 05004], lr: 0.000481, loss: 0.6617
2022-08-28 13:13:36 - train: epoch 0096, iter [03000, 05004], lr: 0.000477, loss: 0.7299
2022-08-28 13:14:10 - train: epoch 0096, iter [03100, 05004], lr: 0.000473, loss: 0.7164
2022-08-28 13:14:44 - train: epoch 0096, iter [03200, 05004], lr: 0.000468, loss: 0.7414
2022-08-28 13:15:17 - train: epoch 0096, iter [03300, 05004], lr: 0.000464, loss: 0.8535
2022-08-28 13:15:51 - train: epoch 0096, iter [03400, 05004], lr: 0.000460, loss: 0.5240
2022-08-28 13:16:26 - train: epoch 0096, iter [03500, 05004], lr: 0.000456, loss: 0.6257
2022-08-28 13:16:59 - train: epoch 0096, iter [03600, 05004], lr: 0.000451, loss: 0.6119
2022-08-28 13:17:34 - train: epoch 0096, iter [03700, 05004], lr: 0.000447, loss: 0.7627
2022-08-28 13:18:07 - train: epoch 0096, iter [03800, 05004], lr: 0.000443, loss: 0.7129
2022-08-28 13:18:41 - train: epoch 0096, iter [03900, 05004], lr: 0.000439, loss: 0.6173
2022-08-28 13:19:15 - train: epoch 0096, iter [04000, 05004], lr: 0.000435, loss: 0.5351
2022-08-28 13:19:49 - train: epoch 0096, iter [04100, 05004], lr: 0.000431, loss: 0.7359
2022-08-28 13:20:23 - train: epoch 0096, iter [04200, 05004], lr: 0.000427, loss: 0.7082
2022-08-28 13:20:56 - train: epoch 0096, iter [04300, 05004], lr: 0.000422, loss: 0.4681
2022-08-28 13:21:30 - train: epoch 0096, iter [04400, 05004], lr: 0.000418, loss: 0.7444
2022-08-28 13:22:05 - train: epoch 0096, iter [04500, 05004], lr: 0.000414, loss: 0.6615
2022-08-28 13:22:38 - train: epoch 0096, iter [04600, 05004], lr: 0.000410, loss: 0.7331
2022-08-28 13:23:12 - train: epoch 0096, iter [04700, 05004], lr: 0.000406, loss: 0.6651
2022-08-28 13:23:46 - train: epoch 0096, iter [04800, 05004], lr: 0.000402, loss: 0.8474
2022-08-28 13:24:18 - train: epoch 0096, iter [04900, 05004], lr: 0.000398, loss: 0.8544
2022-08-28 13:24:52 - train: epoch 0096, iter [05000, 05004], lr: 0.000394, loss: 0.6783
2022-08-28 13:24:53 - train: epoch 096, train_loss: 0.7228
2022-08-28 13:26:08 - eval: epoch: 096, acc1: 76.670%, acc5: 93.224%, test_loss: 0.9388, per_image_load_time: 2.396ms, per_image_inference_time: 0.533ms
2022-08-28 13:26:09 - until epoch: 096, best_acc1: 76.670%
2022-08-28 13:26:09 - epoch 097 lr: 0.000394
2022-08-28 13:26:47 - train: epoch 0097, iter [00100, 05004], lr: 0.000390, loss: 0.7994
2022-08-28 13:27:21 - train: epoch 0097, iter [00200, 05004], lr: 0.000386, loss: 0.5970
2022-08-28 13:27:56 - train: epoch 0097, iter [00300, 05004], lr: 0.000383, loss: 0.7494
2022-08-28 13:28:29 - train: epoch 0097, iter [00400, 05004], lr: 0.000379, loss: 0.8874
2022-08-28 13:29:03 - train: epoch 0097, iter [00500, 05004], lr: 0.000375, loss: 0.6997
2022-08-28 13:29:37 - train: epoch 0097, iter [00600, 05004], lr: 0.000371, loss: 0.7868
2022-08-28 13:30:11 - train: epoch 0097, iter [00700, 05004], lr: 0.000367, loss: 0.7375
2022-08-28 13:30:44 - train: epoch 0097, iter [00800, 05004], lr: 0.000363, loss: 0.6632
2022-08-28 13:31:18 - train: epoch 0097, iter [00900, 05004], lr: 0.000360, loss: 0.6176
2022-08-28 13:31:52 - train: epoch 0097, iter [01000, 05004], lr: 0.000356, loss: 0.6248
2022-08-28 13:32:26 - train: epoch 0097, iter [01100, 05004], lr: 0.000352, loss: 0.6359
2022-08-28 13:33:00 - train: epoch 0097, iter [01200, 05004], lr: 0.000348, loss: 0.6616
2022-08-28 13:33:33 - train: epoch 0097, iter [01300, 05004], lr: 0.000345, loss: 0.6612
2022-08-28 13:34:07 - train: epoch 0097, iter [01400, 05004], lr: 0.000341, loss: 0.6524
2022-08-28 13:34:41 - train: epoch 0097, iter [01500, 05004], lr: 0.000337, loss: 0.7151
2022-08-28 13:35:16 - train: epoch 0097, iter [01600, 05004], lr: 0.000334, loss: 0.7046
2022-08-28 13:35:49 - train: epoch 0097, iter [01700, 05004], lr: 0.000330, loss: 0.6478
2022-08-28 13:36:23 - train: epoch 0097, iter [01800, 05004], lr: 0.000327, loss: 0.7332
2022-08-28 13:36:58 - train: epoch 0097, iter [01900, 05004], lr: 0.000323, loss: 0.7483
2022-08-28 13:37:32 - train: epoch 0097, iter [02000, 05004], lr: 0.000319, loss: 0.6361
2022-08-28 13:38:06 - train: epoch 0097, iter [02100, 05004], lr: 0.000316, loss: 0.7755
2022-08-28 13:38:40 - train: epoch 0097, iter [02200, 05004], lr: 0.000312, loss: 0.8773
2022-08-28 13:39:13 - train: epoch 0097, iter [02300, 05004], lr: 0.000309, loss: 0.6075
2022-08-28 13:39:47 - train: epoch 0097, iter [02400, 05004], lr: 0.000305, loss: 0.6752
2022-08-28 13:40:21 - train: epoch 0097, iter [02500, 05004], lr: 0.000302, loss: 0.7842
2022-08-28 13:40:54 - train: epoch 0097, iter [02600, 05004], lr: 0.000299, loss: 0.7471
2022-08-28 13:41:28 - train: epoch 0097, iter [02700, 05004], lr: 0.000295, loss: 0.6273
2022-08-28 13:42:03 - train: epoch 0097, iter [02800, 05004], lr: 0.000292, loss: 0.6336
2022-08-28 13:42:36 - train: epoch 0097, iter [02900, 05004], lr: 0.000288, loss: 0.8133
2022-08-28 13:43:10 - train: epoch 0097, iter [03000, 05004], lr: 0.000285, loss: 0.6855
2022-08-28 13:43:43 - train: epoch 0097, iter [03100, 05004], lr: 0.000282, loss: 0.6888
2022-08-28 13:44:18 - train: epoch 0097, iter [03200, 05004], lr: 0.000278, loss: 0.8050
2022-08-28 13:44:53 - train: epoch 0097, iter [03300, 05004], lr: 0.000275, loss: 0.7983
2022-08-28 13:45:25 - train: epoch 0097, iter [03400, 05004], lr: 0.000272, loss: 0.8774
2022-08-28 13:45:59 - train: epoch 0097, iter [03500, 05004], lr: 0.000269, loss: 0.8112
2022-08-28 13:46:34 - train: epoch 0097, iter [03600, 05004], lr: 0.000265, loss: 0.6745
2022-08-28 13:47:08 - train: epoch 0097, iter [03700, 05004], lr: 0.000262, loss: 0.6051
2022-08-28 13:47:42 - train: epoch 0097, iter [03800, 05004], lr: 0.000259, loss: 0.6215
2022-08-28 13:48:15 - train: epoch 0097, iter [03900, 05004], lr: 0.000256, loss: 0.7082
2022-08-28 13:48:49 - train: epoch 0097, iter [04000, 05004], lr: 0.000253, loss: 0.6951
2022-08-28 13:49:23 - train: epoch 0097, iter [04100, 05004], lr: 0.000249, loss: 0.6118
2022-08-28 13:49:56 - train: epoch 0097, iter [04200, 05004], lr: 0.000246, loss: 0.7553
2022-08-28 13:50:31 - train: epoch 0097, iter [04300, 05004], lr: 0.000243, loss: 0.5603
2022-08-28 13:51:05 - train: epoch 0097, iter [04400, 05004], lr: 0.000240, loss: 0.6377
2022-08-28 13:51:39 - train: epoch 0097, iter [04500, 05004], lr: 0.000237, loss: 0.7124
2022-08-28 13:52:13 - train: epoch 0097, iter [04600, 05004], lr: 0.000234, loss: 0.8946
2022-08-28 13:52:47 - train: epoch 0097, iter [04700, 05004], lr: 0.000231, loss: 0.7447
2022-08-28 13:53:22 - train: epoch 0097, iter [04800, 05004], lr: 0.000228, loss: 0.7066
2022-08-28 13:53:56 - train: epoch 0097, iter [04900, 05004], lr: 0.000225, loss: 0.8475
2022-08-28 13:54:29 - train: epoch 0097, iter [05000, 05004], lr: 0.000222, loss: 0.7357
2022-08-28 13:54:30 - train: epoch 097, train_loss: 0.7154
2022-08-28 13:55:46 - eval: epoch: 097, acc1: 76.828%, acc5: 93.300%, test_loss: 0.9333, per_image_load_time: 2.371ms, per_image_inference_time: 0.537ms
2022-08-28 13:55:46 - until epoch: 097, best_acc1: 76.828%
2022-08-28 13:55:46 - epoch 098 lr: 0.000222
2022-08-28 13:56:25 - train: epoch 0098, iter [00100, 05004], lr: 0.000219, loss: 0.7145
2022-08-28 13:56:59 - train: epoch 0098, iter [00200, 05004], lr: 0.000216, loss: 0.7588
2022-08-28 13:57:33 - train: epoch 0098, iter [00300, 05004], lr: 0.000213, loss: 0.8266
2022-08-28 13:58:06 - train: epoch 0098, iter [00400, 05004], lr: 0.000210, loss: 0.6019
2022-08-28 13:58:41 - train: epoch 0098, iter [00500, 05004], lr: 0.000207, loss: 0.7418
2022-08-28 13:59:14 - train: epoch 0098, iter [00600, 05004], lr: 0.000205, loss: 0.7821
2022-08-28 13:59:48 - train: epoch 0098, iter [00700, 05004], lr: 0.000202, loss: 0.6761
2022-08-28 14:00:21 - train: epoch 0098, iter [00800, 05004], lr: 0.000199, loss: 0.7756
2022-08-28 14:00:55 - train: epoch 0098, iter [00900, 05004], lr: 0.000196, loss: 0.7028
2022-08-28 14:01:29 - train: epoch 0098, iter [01000, 05004], lr: 0.000193, loss: 0.6304
2022-08-28 14:02:03 - train: epoch 0098, iter [01100, 05004], lr: 0.000191, loss: 0.5657
2022-08-28 14:02:36 - train: epoch 0098, iter [01200, 05004], lr: 0.000188, loss: 0.5936
2022-08-28 14:03:10 - train: epoch 0098, iter [01300, 05004], lr: 0.000185, loss: 0.7911
2022-08-28 14:03:43 - train: epoch 0098, iter [01400, 05004], lr: 0.000182, loss: 0.6999
2022-08-28 14:04:18 - train: epoch 0098, iter [01500, 05004], lr: 0.000180, loss: 0.6181
2022-08-28 14:04:52 - train: epoch 0098, iter [01600, 05004], lr: 0.000177, loss: 0.7002
2022-08-28 14:05:26 - train: epoch 0098, iter [01700, 05004], lr: 0.000175, loss: 0.7103
2022-08-28 14:05:59 - train: epoch 0098, iter [01800, 05004], lr: 0.000172, loss: 0.5901
2022-08-28 14:06:33 - train: epoch 0098, iter [01900, 05004], lr: 0.000169, loss: 0.5973
2022-08-28 14:07:07 - train: epoch 0098, iter [02000, 05004], lr: 0.000167, loss: 0.8212
2022-08-28 14:07:41 - train: epoch 0098, iter [02100, 05004], lr: 0.000164, loss: 0.6320
2022-08-28 14:08:15 - train: epoch 0098, iter [02200, 05004], lr: 0.000162, loss: 0.6284
2022-08-28 14:08:48 - train: epoch 0098, iter [02300, 05004], lr: 0.000159, loss: 0.7362
2022-08-28 14:09:22 - train: epoch 0098, iter [02400, 05004], lr: 0.000157, loss: 0.6764
2022-08-28 14:09:56 - train: epoch 0098, iter [02500, 05004], lr: 0.000154, loss: 0.8404
2022-08-28 14:10:30 - train: epoch 0098, iter [02600, 05004], lr: 0.000152, loss: 0.6752
2022-08-28 14:11:05 - train: epoch 0098, iter [02700, 05004], lr: 0.000149, loss: 0.7516
2022-08-28 14:11:38 - train: epoch 0098, iter [02800, 05004], lr: 0.000147, loss: 0.6369
2022-08-28 14:12:11 - train: epoch 0098, iter [02900, 05004], lr: 0.000144, loss: 0.5863
2022-08-28 14:12:47 - train: epoch 0098, iter [03000, 05004], lr: 0.000142, loss: 0.7249
2022-08-28 14:13:20 - train: epoch 0098, iter [03100, 05004], lr: 0.000140, loss: 0.5996
2022-08-28 14:13:55 - train: epoch 0098, iter [03200, 05004], lr: 0.000137, loss: 0.6563
2022-08-28 14:14:29 - train: epoch 0098, iter [03300, 05004], lr: 0.000135, loss: 0.5649
2022-08-28 14:15:02 - train: epoch 0098, iter [03400, 05004], lr: 0.000133, loss: 0.7346
2022-08-28 14:15:36 - train: epoch 0098, iter [03500, 05004], lr: 0.000131, loss: 0.7449
2022-08-28 14:16:11 - train: epoch 0098, iter [03600, 05004], lr: 0.000128, loss: 0.9102
2022-08-28 14:16:45 - train: epoch 0098, iter [03700, 05004], lr: 0.000126, loss: 0.8123
2022-08-28 14:17:19 - train: epoch 0098, iter [03800, 05004], lr: 0.000124, loss: 0.7983
2022-08-28 14:17:53 - train: epoch 0098, iter [03900, 05004], lr: 0.000122, loss: 0.7084
2022-08-28 14:18:26 - train: epoch 0098, iter [04000, 05004], lr: 0.000119, loss: 0.7557
2022-08-28 14:19:01 - train: epoch 0098, iter [04100, 05004], lr: 0.000117, loss: 0.8432
2022-08-28 14:19:35 - train: epoch 0098, iter [04200, 05004], lr: 0.000115, loss: 0.6792
2022-08-28 14:20:09 - train: epoch 0098, iter [04300, 05004], lr: 0.000113, loss: 0.6038
2022-08-28 14:20:43 - train: epoch 0098, iter [04400, 05004], lr: 0.000111, loss: 0.8205
2022-08-28 14:21:17 - train: epoch 0098, iter [04500, 05004], lr: 0.000109, loss: 0.8429
2022-08-28 14:21:52 - train: epoch 0098, iter [04600, 05004], lr: 0.000107, loss: 0.7837
2022-08-28 14:22:25 - train: epoch 0098, iter [04700, 05004], lr: 0.000105, loss: 0.7609
2022-08-28 14:22:59 - train: epoch 0098, iter [04800, 05004], lr: 0.000103, loss: 0.4851
2022-08-28 14:23:33 - train: epoch 0098, iter [04900, 05004], lr: 0.000101, loss: 0.7528
2022-08-28 14:24:06 - train: epoch 0098, iter [05000, 05004], lr: 0.000099, loss: 0.8132
2022-08-28 14:24:07 - train: epoch 098, train_loss: 0.7088
2022-08-28 14:25:22 - eval: epoch: 098, acc1: 76.754%, acc5: 93.314%, test_loss: 0.9343, per_image_load_time: 2.396ms, per_image_inference_time: 0.512ms
2022-08-28 14:25:23 - until epoch: 098, best_acc1: 76.828%
2022-08-28 14:25:23 - epoch 099 lr: 0.000099
2022-08-28 14:26:02 - train: epoch 0099, iter [00100, 05004], lr: 0.000097, loss: 0.6080
2022-08-28 14:26:35 - train: epoch 0099, iter [00200, 05004], lr: 0.000095, loss: 0.5955
2022-08-28 14:27:09 - train: epoch 0099, iter [00300, 05004], lr: 0.000093, loss: 0.7143
2022-08-28 14:27:43 - train: epoch 0099, iter [00400, 05004], lr: 0.000091, loss: 0.6978
2022-08-28 14:28:17 - train: epoch 0099, iter [00500, 05004], lr: 0.000089, loss: 0.7943
2022-08-28 14:28:50 - train: epoch 0099, iter [00600, 05004], lr: 0.000087, loss: 0.6747
2022-08-28 14:29:23 - train: epoch 0099, iter [00700, 05004], lr: 0.000085, loss: 0.6997
2022-08-28 14:29:57 - train: epoch 0099, iter [00800, 05004], lr: 0.000084, loss: 0.7180
2022-08-28 14:30:31 - train: epoch 0099, iter [00900, 05004], lr: 0.000082, loss: 0.6778
2022-08-28 14:31:04 - train: epoch 0099, iter [01000, 05004], lr: 0.000080, loss: 0.7414
2022-08-28 14:31:38 - train: epoch 0099, iter [01100, 05004], lr: 0.000078, loss: 0.6371
2022-08-28 14:32:13 - train: epoch 0099, iter [01200, 05004], lr: 0.000076, loss: 0.6714
2022-08-28 14:32:46 - train: epoch 0099, iter [01300, 05004], lr: 0.000075, loss: 0.8268
2022-08-28 14:33:20 - train: epoch 0099, iter [01400, 05004], lr: 0.000073, loss: 0.7538
2022-08-28 14:33:53 - train: epoch 0099, iter [01500, 05004], lr: 0.000071, loss: 0.5830
2022-08-28 14:34:27 - train: epoch 0099, iter [01600, 05004], lr: 0.000070, loss: 0.9959
2022-08-28 14:35:00 - train: epoch 0099, iter [01700, 05004], lr: 0.000068, loss: 0.5658
2022-08-28 14:35:35 - train: epoch 0099, iter [01800, 05004], lr: 0.000066, loss: 0.6886
2022-08-28 14:36:08 - train: epoch 0099, iter [01900, 05004], lr: 0.000065, loss: 0.6745
2022-08-28 14:36:42 - train: epoch 0099, iter [02000, 05004], lr: 0.000063, loss: 0.6691
2022-08-28 14:37:16 - train: epoch 0099, iter [02100, 05004], lr: 0.000062, loss: 0.6006
2022-08-28 14:37:49 - train: epoch 0099, iter [02200, 05004], lr: 0.000060, loss: 0.7882
2022-08-28 14:38:24 - train: epoch 0099, iter [02300, 05004], lr: 0.000059, loss: 0.7574
2022-08-28 14:38:58 - train: epoch 0099, iter [02400, 05004], lr: 0.000057, loss: 0.7712
2022-08-28 14:39:31 - train: epoch 0099, iter [02500, 05004], lr: 0.000056, loss: 0.6201
2022-08-28 14:40:06 - train: epoch 0099, iter [02600, 05004], lr: 0.000054, loss: 0.5537
2022-08-28 14:40:40 - train: epoch 0099, iter [02700, 05004], lr: 0.000053, loss: 0.7213
2022-08-28 14:41:15 - train: epoch 0099, iter [02800, 05004], lr: 0.000051, loss: 0.8948
2022-08-28 14:41:48 - train: epoch 0099, iter [02900, 05004], lr: 0.000050, loss: 0.7357
2022-08-28 14:42:22 - train: epoch 0099, iter [03000, 05004], lr: 0.000048, loss: 0.7427
2022-08-28 14:42:57 - train: epoch 0099, iter [03100, 05004], lr: 0.000047, loss: 0.5529
2022-08-28 14:43:30 - train: epoch 0099, iter [03200, 05004], lr: 0.000046, loss: 0.7782
2022-08-28 14:44:04 - train: epoch 0099, iter [03300, 05004], lr: 0.000044, loss: 0.6198
2022-08-28 14:44:38 - train: epoch 0099, iter [03400, 05004], lr: 0.000043, loss: 0.7565
2022-08-28 14:45:12 - train: epoch 0099, iter [03500, 05004], lr: 0.000042, loss: 0.7328
2022-08-28 14:45:46 - train: epoch 0099, iter [03600, 05004], lr: 0.000040, loss: 0.6846
2022-08-28 14:46:20 - train: epoch 0099, iter [03700, 05004], lr: 0.000039, loss: 0.5981
2022-08-28 14:46:54 - train: epoch 0099, iter [03800, 05004], lr: 0.000038, loss: 0.8879
2022-08-28 14:47:29 - train: epoch 0099, iter [03900, 05004], lr: 0.000037, loss: 0.7487
2022-08-28 14:48:01 - train: epoch 0099, iter [04000, 05004], lr: 0.000036, loss: 0.7767
2022-08-28 14:48:35 - train: epoch 0099, iter [04100, 05004], lr: 0.000034, loss: 0.6144
2022-08-28 14:49:09 - train: epoch 0099, iter [04200, 05004], lr: 0.000033, loss: 0.6629
2022-08-28 14:49:43 - train: epoch 0099, iter [04300, 05004], lr: 0.000032, loss: 0.6804
2022-08-28 14:50:17 - train: epoch 0099, iter [04400, 05004], lr: 0.000031, loss: 0.7939
2022-08-28 14:50:51 - train: epoch 0099, iter [04500, 05004], lr: 0.000030, loss: 0.7802
2022-08-28 14:51:24 - train: epoch 0099, iter [04600, 05004], lr: 0.000029, loss: 0.7984
2022-08-28 14:51:59 - train: epoch 0099, iter [04700, 05004], lr: 0.000028, loss: 0.5935
2022-08-28 14:52:33 - train: epoch 0099, iter [04800, 05004], lr: 0.000027, loss: 0.8743
2022-08-28 14:53:07 - train: epoch 0099, iter [04900, 05004], lr: 0.000026, loss: 0.7553
2022-08-28 14:53:40 - train: epoch 0099, iter [05000, 05004], lr: 0.000025, loss: 0.6690
2022-08-28 14:53:41 - train: epoch 099, train_loss: 0.7059
2022-08-28 14:54:56 - eval: epoch: 099, acc1: 76.856%, acc5: 93.354%, test_loss: 0.9334, per_image_load_time: 2.395ms, per_image_inference_time: 0.507ms
2022-08-28 14:54:57 - until epoch: 099, best_acc1: 76.856%
2022-08-28 14:54:57 - epoch 100 lr: 0.000025
2022-08-28 14:55:36 - train: epoch 0100, iter [00100, 05004], lr: 0.000024, loss: 0.6832
2022-08-28 14:56:09 - train: epoch 0100, iter [00200, 05004], lr: 0.000023, loss: 0.7956
2022-08-28 14:56:42 - train: epoch 0100, iter [00300, 05004], lr: 0.000022, loss: 0.6528
2022-08-28 14:57:16 - train: epoch 0100, iter [00400, 05004], lr: 0.000021, loss: 0.5430
2022-08-28 14:57:49 - train: epoch 0100, iter [00500, 05004], lr: 0.000020, loss: 0.6876
2022-08-28 14:58:23 - train: epoch 0100, iter [00600, 05004], lr: 0.000019, loss: 0.9087
2022-08-28 14:58:58 - train: epoch 0100, iter [00700, 05004], lr: 0.000018, loss: 0.6315
2022-08-28 14:59:32 - train: epoch 0100, iter [00800, 05004], lr: 0.000017, loss: 0.7534
2022-08-28 15:00:05 - train: epoch 0100, iter [00900, 05004], lr: 0.000017, loss: 0.5350
2022-08-28 15:00:39 - train: epoch 0100, iter [01000, 05004], lr: 0.000016, loss: 0.7430
2022-08-28 15:01:12 - train: epoch 0100, iter [01100, 05004], lr: 0.000015, loss: 0.6179
2022-08-28 15:01:46 - train: epoch 0100, iter [01200, 05004], lr: 0.000014, loss: 0.6986
2022-08-28 15:02:20 - train: epoch 0100, iter [01300, 05004], lr: 0.000014, loss: 0.7508
2022-08-28 15:02:54 - train: epoch 0100, iter [01400, 05004], lr: 0.000013, loss: 0.7483
2022-08-28 15:03:27 - train: epoch 0100, iter [01500, 05004], lr: 0.000012, loss: 0.6778
2022-08-28 15:04:01 - train: epoch 0100, iter [01600, 05004], lr: 0.000011, loss: 0.6639
2022-08-28 15:04:35 - train: epoch 0100, iter [01700, 05004], lr: 0.000011, loss: 0.7515
2022-08-28 15:05:09 - train: epoch 0100, iter [01800, 05004], lr: 0.000010, loss: 0.7604
2022-08-28 15:05:43 - train: epoch 0100, iter [01900, 05004], lr: 0.000009, loss: 0.6255
2022-08-28 15:06:17 - train: epoch 0100, iter [02000, 05004], lr: 0.000009, loss: 0.6759
2022-08-28 15:06:51 - train: epoch 0100, iter [02100, 05004], lr: 0.000008, loss: 0.5672
2022-08-28 15:07:25 - train: epoch 0100, iter [02200, 05004], lr: 0.000008, loss: 0.7319
2022-08-28 15:07:59 - train: epoch 0100, iter [02300, 05004], lr: 0.000007, loss: 0.6755
2022-08-28 15:08:32 - train: epoch 0100, iter [02400, 05004], lr: 0.000007, loss: 0.6838
2022-08-28 15:09:06 - train: epoch 0100, iter [02500, 05004], lr: 0.000006, loss: 0.6438
2022-08-28 15:09:40 - train: epoch 0100, iter [02600, 05004], lr: 0.000006, loss: 0.7549
2022-08-28 15:10:14 - train: epoch 0100, iter [02700, 05004], lr: 0.000005, loss: 0.6033
2022-08-28 15:10:48 - train: epoch 0100, iter [02800, 05004], lr: 0.000005, loss: 0.7126
2022-08-28 15:11:21 - train: epoch 0100, iter [02900, 05004], lr: 0.000004, loss: 0.8260
2022-08-28 15:11:55 - train: epoch 0100, iter [03000, 05004], lr: 0.000004, loss: 0.6055
2022-08-28 15:12:29 - train: epoch 0100, iter [03100, 05004], lr: 0.000004, loss: 0.6549
2022-08-28 15:13:03 - train: epoch 0100, iter [03200, 05004], lr: 0.000003, loss: 0.7500
2022-08-28 15:13:37 - train: epoch 0100, iter [03300, 05004], lr: 0.000003, loss: 0.7468
2022-08-28 15:14:11 - train: epoch 0100, iter [03400, 05004], lr: 0.000003, loss: 0.8400
2022-08-28 15:14:46 - train: epoch 0100, iter [03500, 05004], lr: 0.000002, loss: 0.5832
2022-08-28 15:15:18 - train: epoch 0100, iter [03600, 05004], lr: 0.000002, loss: 0.7859
2022-08-28 15:15:53 - train: epoch 0100, iter [03700, 05004], lr: 0.000002, loss: 0.7398
2022-08-28 15:16:27 - train: epoch 0100, iter [03800, 05004], lr: 0.000001, loss: 0.6913
2022-08-28 15:17:01 - train: epoch 0100, iter [03900, 05004], lr: 0.000001, loss: 0.7245
2022-08-28 15:17:34 - train: epoch 0100, iter [04000, 05004], lr: 0.000001, loss: 0.5993
2022-08-28 15:18:08 - train: epoch 0100, iter [04100, 05004], lr: 0.000001, loss: 0.8523
2022-08-28 15:18:42 - train: epoch 0100, iter [04200, 05004], lr: 0.000001, loss: 0.5935
2022-08-28 15:19:16 - train: epoch 0100, iter [04300, 05004], lr: 0.000000, loss: 0.7150
2022-08-28 15:19:50 - train: epoch 0100, iter [04400, 05004], lr: 0.000000, loss: 0.8054
2022-08-28 15:20:24 - train: epoch 0100, iter [04500, 05004], lr: 0.000000, loss: 0.6621
2022-08-28 15:20:59 - train: epoch 0100, iter [04600, 05004], lr: 0.000000, loss: 0.6153
2022-08-28 15:21:32 - train: epoch 0100, iter [04700, 05004], lr: 0.000000, loss: 0.7969
2022-08-28 15:22:06 - train: epoch 0100, iter [04800, 05004], lr: 0.000000, loss: 0.5206
2022-08-28 15:22:40 - train: epoch 0100, iter [04900, 05004], lr: 0.000000, loss: 0.6349
2022-08-28 15:23:12 - train: epoch 0100, iter [05000, 05004], lr: 0.000000, loss: 0.7815
2022-08-28 15:23:13 - train: epoch 100, train_loss: 0.7050
2022-08-28 15:24:28 - eval: epoch: 100, acc1: 76.756%, acc5: 93.358%, test_loss: 0.9339, per_image_load_time: 2.366ms, per_image_inference_time: 0.491ms
2022-08-28 15:24:28 - until epoch: 100, best_acc1: 76.856%
2022-08-28 15:24:28 - train done. model: resnet50, train time: 58.277 hours, best_acc1: 76.856%
